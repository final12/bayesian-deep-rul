Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18545
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-25 17:47:10.592822 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 16, 11]           1,120
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
    BayesianConv2d-4            [-1, 14, 7, 11]             448
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
    BayesianLinear-8                    [-1, 1]             924
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,492
Trainable params: 2,492
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 17:47:10.604078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4273.02
 ---- batch: 020 ----
mean loss: 4078.68
 ---- batch: 030 ----
mean loss: 3943.09
 ---- batch: 040 ----
mean loss: 3677.90
 ---- batch: 050 ----
mean loss: 3383.88
 ---- batch: 060 ----
mean loss: 3232.97
 ---- batch: 070 ----
mean loss: 2956.53
 ---- batch: 080 ----
mean loss: 2785.84
 ---- batch: 090 ----
mean loss: 2569.85
train mean loss: 3367.99
epoch train time: 0:00:35.344862
elapsed time: 0:00:35.359384
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 17:47:45.952248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2198.40
 ---- batch: 020 ----
mean loss: 2061.02
 ---- batch: 030 ----
mean loss: 1871.87
 ---- batch: 040 ----
mean loss: 1708.90
 ---- batch: 050 ----
mean loss: 1524.31
 ---- batch: 060 ----
mean loss: 1428.92
 ---- batch: 070 ----
mean loss: 1316.77
 ---- batch: 080 ----
mean loss: 1211.99
 ---- batch: 090 ----
mean loss: 1159.60
train mean loss: 1575.95
epoch train time: 0:00:02.643045
elapsed time: 0:00:38.002768
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 17:47:48.595777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1046.39
 ---- batch: 020 ----
mean loss: 995.13
 ---- batch: 030 ----
mean loss: 974.27
 ---- batch: 040 ----
mean loss: 962.94
 ---- batch: 050 ----
mean loss: 946.11
 ---- batch: 060 ----
mean loss: 902.66
 ---- batch: 070 ----
mean loss: 920.28
 ---- batch: 080 ----
mean loss: 887.93
 ---- batch: 090 ----
mean loss: 902.36
train mean loss: 945.36
epoch train time: 0:00:02.692899
elapsed time: 0:00:40.696244
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 17:47:51.289256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.87
 ---- batch: 020 ----
mean loss: 882.54
 ---- batch: 030 ----
mean loss: 885.05
 ---- batch: 040 ----
mean loss: 897.51
 ---- batch: 050 ----
mean loss: 878.73
 ---- batch: 060 ----
mean loss: 881.28
 ---- batch: 070 ----
mean loss: 902.03
 ---- batch: 080 ----
mean loss: 894.74
 ---- batch: 090 ----
mean loss: 881.21
train mean loss: 889.32
epoch train time: 0:00:02.690648
elapsed time: 0:00:43.387446
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 17:47:53.980452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.40
 ---- batch: 020 ----
mean loss: 872.19
 ---- batch: 030 ----
mean loss: 887.87
 ---- batch: 040 ----
mean loss: 884.77
 ---- batch: 050 ----
mean loss: 863.82
 ---- batch: 060 ----
mean loss: 883.06
 ---- batch: 070 ----
mean loss: 897.16
 ---- batch: 080 ----
mean loss: 884.90
 ---- batch: 090 ----
mean loss: 871.81
train mean loss: 880.96
epoch train time: 0:00:02.726909
elapsed time: 0:00:46.114860
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 17:47:56.707861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.89
 ---- batch: 020 ----
mean loss: 880.58
 ---- batch: 030 ----
mean loss: 894.96
 ---- batch: 040 ----
mean loss: 884.86
 ---- batch: 050 ----
mean loss: 874.65
 ---- batch: 060 ----
mean loss: 854.73
 ---- batch: 070 ----
mean loss: 891.00
 ---- batch: 080 ----
mean loss: 885.11
 ---- batch: 090 ----
mean loss: 870.51
train mean loss: 880.40
epoch train time: 0:00:02.662684
elapsed time: 0:00:48.778066
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 17:47:59.371106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.58
 ---- batch: 020 ----
mean loss: 881.26
 ---- batch: 030 ----
mean loss: 886.67
 ---- batch: 040 ----
mean loss: 890.54
 ---- batch: 050 ----
mean loss: 877.31
 ---- batch: 060 ----
mean loss: 872.65
 ---- batch: 070 ----
mean loss: 871.10
 ---- batch: 080 ----
mean loss: 869.59
 ---- batch: 090 ----
mean loss: 866.90
train mean loss: 875.08
epoch train time: 0:00:02.677800
elapsed time: 0:00:51.456377
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 17:48:02.049369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.57
 ---- batch: 020 ----
mean loss: 880.37
 ---- batch: 030 ----
mean loss: 913.26
 ---- batch: 040 ----
mean loss: 869.18
 ---- batch: 050 ----
mean loss: 868.49
 ---- batch: 060 ----
mean loss: 861.45
 ---- batch: 070 ----
mean loss: 882.81
 ---- batch: 080 ----
mean loss: 869.07
 ---- batch: 090 ----
mean loss: 850.92
train mean loss: 871.58
epoch train time: 0:00:02.676761
elapsed time: 0:00:54.133695
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 17:48:04.726698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.77
 ---- batch: 020 ----
mean loss: 859.26
 ---- batch: 030 ----
mean loss: 872.29
 ---- batch: 040 ----
mean loss: 894.47
 ---- batch: 050 ----
mean loss: 849.12
 ---- batch: 060 ----
mean loss: 863.72
 ---- batch: 070 ----
mean loss: 855.61
 ---- batch: 080 ----
mean loss: 851.45
 ---- batch: 090 ----
mean loss: 883.11
train mean loss: 866.57
epoch train time: 0:00:02.673942
elapsed time: 0:00:56.808109
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 17:48:07.401157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.04
 ---- batch: 020 ----
mean loss: 868.33
 ---- batch: 030 ----
mean loss: 841.84
 ---- batch: 040 ----
mean loss: 854.54
 ---- batch: 050 ----
mean loss: 874.19
 ---- batch: 060 ----
mean loss: 880.11
 ---- batch: 070 ----
mean loss: 865.87
 ---- batch: 080 ----
mean loss: 849.99
 ---- batch: 090 ----
mean loss: 853.40
train mean loss: 861.95
epoch train time: 0:00:02.695164
elapsed time: 0:00:59.503794
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 17:48:10.096790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.92
 ---- batch: 020 ----
mean loss: 872.54
 ---- batch: 030 ----
mean loss: 859.74
 ---- batch: 040 ----
mean loss: 862.53
 ---- batch: 050 ----
mean loss: 854.22
 ---- batch: 060 ----
mean loss: 864.61
 ---- batch: 070 ----
mean loss: 858.21
 ---- batch: 080 ----
mean loss: 844.80
 ---- batch: 090 ----
mean loss: 861.49
train mean loss: 857.11
epoch train time: 0:00:02.695862
elapsed time: 0:01:02.200175
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 17:48:12.793189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 812.10
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 854.51
 ---- batch: 040 ----
mean loss: 878.52
 ---- batch: 050 ----
mean loss: 870.36
 ---- batch: 060 ----
mean loss: 857.01
 ---- batch: 070 ----
mean loss: 866.23
 ---- batch: 080 ----
mean loss: 847.64
 ---- batch: 090 ----
mean loss: 841.31
train mean loss: 851.19
epoch train time: 0:00:02.651447
elapsed time: 0:01:04.852106
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 17:48:15.445101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.82
 ---- batch: 020 ----
mean loss: 852.92
 ---- batch: 030 ----
mean loss: 852.57
 ---- batch: 040 ----
mean loss: 833.52
 ---- batch: 050 ----
mean loss: 854.40
 ---- batch: 060 ----
mean loss: 855.54
 ---- batch: 070 ----
mean loss: 827.71
 ---- batch: 080 ----
mean loss: 843.20
 ---- batch: 090 ----
mean loss: 862.73
train mean loss: 847.55
epoch train time: 0:00:02.673095
elapsed time: 0:01:07.525664
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 17:48:18.118659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.40
 ---- batch: 020 ----
mean loss: 852.22
 ---- batch: 030 ----
mean loss: 839.77
 ---- batch: 040 ----
mean loss: 825.96
 ---- batch: 050 ----
mean loss: 850.93
 ---- batch: 060 ----
mean loss: 834.74
 ---- batch: 070 ----
mean loss: 861.68
 ---- batch: 080 ----
mean loss: 839.67
 ---- batch: 090 ----
mean loss: 843.07
train mean loss: 844.05
epoch train time: 0:00:02.614206
elapsed time: 0:01:10.140340
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 17:48:20.733330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.09
 ---- batch: 020 ----
mean loss: 840.25
 ---- batch: 030 ----
mean loss: 843.94
 ---- batch: 040 ----
mean loss: 839.69
 ---- batch: 050 ----
mean loss: 835.62
 ---- batch: 060 ----
mean loss: 824.42
 ---- batch: 070 ----
mean loss: 829.04
 ---- batch: 080 ----
mean loss: 851.11
 ---- batch: 090 ----
mean loss: 840.89
train mean loss: 840.28
epoch train time: 0:00:02.675735
elapsed time: 0:01:12.816547
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 17:48:23.409577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.13
 ---- batch: 020 ----
mean loss: 839.67
 ---- batch: 030 ----
mean loss: 828.38
 ---- batch: 040 ----
mean loss: 844.82
 ---- batch: 050 ----
mean loss: 841.25
 ---- batch: 060 ----
mean loss: 827.71
 ---- batch: 070 ----
mean loss: 818.96
 ---- batch: 080 ----
mean loss: 833.27
 ---- batch: 090 ----
mean loss: 843.97
train mean loss: 834.81
epoch train time: 0:00:02.695089
elapsed time: 0:01:15.512219
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 17:48:26.105225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.47
 ---- batch: 020 ----
mean loss: 812.07
 ---- batch: 030 ----
mean loss: 828.55
 ---- batch: 040 ----
mean loss: 837.31
 ---- batch: 050 ----
mean loss: 814.87
 ---- batch: 060 ----
mean loss: 839.52
 ---- batch: 070 ----
mean loss: 841.00
 ---- batch: 080 ----
mean loss: 847.26
 ---- batch: 090 ----
mean loss: 809.70
train mean loss: 830.61
epoch train time: 0:00:02.632953
elapsed time: 0:01:18.145682
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 17:48:28.738696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.37
 ---- batch: 020 ----
mean loss: 823.41
 ---- batch: 030 ----
mean loss: 840.25
 ---- batch: 040 ----
mean loss: 834.32
 ---- batch: 050 ----
mean loss: 808.71
 ---- batch: 060 ----
mean loss: 810.02
 ---- batch: 070 ----
mean loss: 825.27
 ---- batch: 080 ----
mean loss: 820.08
 ---- batch: 090 ----
mean loss: 820.62
train mean loss: 825.30
epoch train time: 0:00:02.675433
elapsed time: 0:01:20.821594
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 17:48:31.414599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.09
 ---- batch: 020 ----
mean loss: 829.96
 ---- batch: 030 ----
mean loss: 804.99
 ---- batch: 040 ----
mean loss: 821.32
 ---- batch: 050 ----
mean loss: 820.02
 ---- batch: 060 ----
mean loss: 819.48
 ---- batch: 070 ----
mean loss: 803.78
 ---- batch: 080 ----
mean loss: 831.01
 ---- batch: 090 ----
mean loss: 827.27
train mean loss: 820.01
epoch train time: 0:00:02.655419
elapsed time: 0:01:23.477489
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 17:48:34.070490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.32
 ---- batch: 020 ----
mean loss: 829.22
 ---- batch: 030 ----
mean loss: 818.05
 ---- batch: 040 ----
mean loss: 820.47
 ---- batch: 050 ----
mean loss: 803.74
 ---- batch: 060 ----
mean loss: 814.04
 ---- batch: 070 ----
mean loss: 817.78
 ---- batch: 080 ----
mean loss: 805.15
 ---- batch: 090 ----
mean loss: 809.52
train mean loss: 814.89
epoch train time: 0:00:02.656061
elapsed time: 0:01:26.134033
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 17:48:36.727026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 815.74
 ---- batch: 020 ----
mean loss: 806.79
 ---- batch: 030 ----
mean loss: 801.02
 ---- batch: 040 ----
mean loss: 811.60
 ---- batch: 050 ----
mean loss: 820.80
 ---- batch: 060 ----
mean loss: 805.22
 ---- batch: 070 ----
mean loss: 794.89
 ---- batch: 080 ----
mean loss: 817.89
 ---- batch: 090 ----
mean loss: 812.02
train mean loss: 808.28
epoch train time: 0:00:02.665790
elapsed time: 0:01:28.800345
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 17:48:39.393296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.28
 ---- batch: 020 ----
mean loss: 813.48
 ---- batch: 030 ----
mean loss: 795.98
 ---- batch: 040 ----
mean loss: 784.72
 ---- batch: 050 ----
mean loss: 795.86
 ---- batch: 060 ----
mean loss: 817.56
 ---- batch: 070 ----
mean loss: 803.24
 ---- batch: 080 ----
mean loss: 799.83
 ---- batch: 090 ----
mean loss: 805.15
train mean loss: 802.78
epoch train time: 0:00:02.705250
elapsed time: 0:01:31.506017
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 17:48:42.099006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 805.73
 ---- batch: 020 ----
mean loss: 789.22
 ---- batch: 030 ----
mean loss: 784.80
 ---- batch: 040 ----
mean loss: 791.92
 ---- batch: 050 ----
mean loss: 795.58
 ---- batch: 060 ----
mean loss: 794.21
 ---- batch: 070 ----
mean loss: 795.21
 ---- batch: 080 ----
mean loss: 797.81
 ---- batch: 090 ----
mean loss: 802.24
train mean loss: 795.08
epoch train time: 0:00:02.661098
elapsed time: 0:01:34.167594
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 17:48:44.760583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.91
 ---- batch: 020 ----
mean loss: 784.91
 ---- batch: 030 ----
mean loss: 789.26
 ---- batch: 040 ----
mean loss: 779.32
 ---- batch: 050 ----
mean loss: 787.72
 ---- batch: 060 ----
mean loss: 775.98
 ---- batch: 070 ----
mean loss: 793.72
 ---- batch: 080 ----
mean loss: 796.35
 ---- batch: 090 ----
mean loss: 784.56
train mean loss: 789.54
epoch train time: 0:00:02.666138
elapsed time: 0:01:36.834201
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 17:48:47.427217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 775.43
 ---- batch: 020 ----
mean loss: 795.39
 ---- batch: 030 ----
mean loss: 782.43
 ---- batch: 040 ----
mean loss: 786.71
 ---- batch: 050 ----
mean loss: 787.16
 ---- batch: 060 ----
mean loss: 780.08
 ---- batch: 070 ----
mean loss: 777.09
 ---- batch: 080 ----
mean loss: 773.14
 ---- batch: 090 ----
mean loss: 782.96
train mean loss: 780.51
epoch train time: 0:00:02.635546
elapsed time: 0:01:39.470256
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 17:48:50.063242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 771.08
 ---- batch: 020 ----
mean loss: 772.44
 ---- batch: 030 ----
mean loss: 766.19
 ---- batch: 040 ----
mean loss: 764.86
 ---- batch: 050 ----
mean loss: 763.11
 ---- batch: 060 ----
mean loss: 794.84
 ---- batch: 070 ----
mean loss: 779.30
 ---- batch: 080 ----
mean loss: 769.55
 ---- batch: 090 ----
mean loss: 763.69
train mean loss: 772.30
epoch train time: 0:00:02.681258
elapsed time: 0:01:42.152002
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 17:48:52.745033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 769.76
 ---- batch: 020 ----
mean loss: 773.42
 ---- batch: 030 ----
mean loss: 758.97
 ---- batch: 040 ----
mean loss: 761.38
 ---- batch: 050 ----
mean loss: 751.19
 ---- batch: 060 ----
mean loss: 761.59
 ---- batch: 070 ----
mean loss: 767.06
 ---- batch: 080 ----
mean loss: 769.33
 ---- batch: 090 ----
mean loss: 769.09
train mean loss: 763.27
epoch train time: 0:00:02.713695
elapsed time: 0:01:44.866246
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 17:48:55.459241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 772.30
 ---- batch: 020 ----
mean loss: 753.95
 ---- batch: 030 ----
mean loss: 760.07
 ---- batch: 040 ----
mean loss: 765.17
 ---- batch: 050 ----
mean loss: 756.46
 ---- batch: 060 ----
mean loss: 743.74
 ---- batch: 070 ----
mean loss: 743.52
 ---- batch: 080 ----
mean loss: 769.96
 ---- batch: 090 ----
mean loss: 736.86
train mean loss: 754.68
epoch train time: 0:00:02.661442
elapsed time: 0:01:47.528230
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 17:48:58.121229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 749.64
 ---- batch: 020 ----
mean loss: 743.63
 ---- batch: 030 ----
mean loss: 737.73
 ---- batch: 040 ----
mean loss: 744.18
 ---- batch: 050 ----
mean loss: 753.81
 ---- batch: 060 ----
mean loss: 751.61
 ---- batch: 070 ----
mean loss: 756.61
 ---- batch: 080 ----
mean loss: 738.00
 ---- batch: 090 ----
mean loss: 729.42
train mean loss: 745.03
epoch train time: 0:00:02.653729
elapsed time: 0:01:50.182479
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 17:49:00.775476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 753.41
 ---- batch: 020 ----
mean loss: 745.58
 ---- batch: 030 ----
mean loss: 731.94
 ---- batch: 040 ----
mean loss: 736.91
 ---- batch: 050 ----
mean loss: 737.23
 ---- batch: 060 ----
mean loss: 742.66
 ---- batch: 070 ----
mean loss: 732.72
 ---- batch: 080 ----
mean loss: 730.07
 ---- batch: 090 ----
mean loss: 711.77
train mean loss: 734.93
epoch train time: 0:00:02.666326
elapsed time: 0:01:52.849349
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 17:49:03.442375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 722.69
 ---- batch: 020 ----
mean loss: 725.35
 ---- batch: 030 ----
mean loss: 721.05
 ---- batch: 040 ----
mean loss: 730.85
 ---- batch: 050 ----
mean loss: 744.56
 ---- batch: 060 ----
mean loss: 717.97
 ---- batch: 070 ----
mean loss: 733.99
 ---- batch: 080 ----
mean loss: 718.67
 ---- batch: 090 ----
mean loss: 709.34
train mean loss: 724.51
epoch train time: 0:00:02.673396
elapsed time: 0:01:55.523253
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 17:49:06.116250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 724.28
 ---- batch: 020 ----
mean loss: 721.95
 ---- batch: 030 ----
mean loss: 717.38
 ---- batch: 040 ----
mean loss: 705.73
 ---- batch: 050 ----
mean loss: 710.79
 ---- batch: 060 ----
mean loss: 721.98
 ---- batch: 070 ----
mean loss: 697.16
 ---- batch: 080 ----
mean loss: 725.15
 ---- batch: 090 ----
mean loss: 706.43
train mean loss: 713.83
epoch train time: 0:00:02.675834
elapsed time: 0:01:58.199569
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 17:49:08.792614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.19
 ---- batch: 020 ----
mean loss: 712.33
 ---- batch: 030 ----
mean loss: 707.43
 ---- batch: 040 ----
mean loss: 702.01
 ---- batch: 050 ----
mean loss: 693.96
 ---- batch: 060 ----
mean loss: 698.09
 ---- batch: 070 ----
mean loss: 701.71
 ---- batch: 080 ----
mean loss: 702.49
 ---- batch: 090 ----
mean loss: 697.90
train mean loss: 703.86
epoch train time: 0:00:02.692461
elapsed time: 0:02:00.892554
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 17:49:11.485560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.06
 ---- batch: 020 ----
mean loss: 702.36
 ---- batch: 030 ----
mean loss: 699.57
 ---- batch: 040 ----
mean loss: 693.58
 ---- batch: 050 ----
mean loss: 699.12
 ---- batch: 060 ----
mean loss: 690.71
 ---- batch: 070 ----
mean loss: 692.88
 ---- batch: 080 ----
mean loss: 676.90
 ---- batch: 090 ----
mean loss: 682.93
train mean loss: 692.83
epoch train time: 0:00:02.700343
elapsed time: 0:02:03.593424
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 17:49:14.186413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.96
 ---- batch: 020 ----
mean loss: 685.04
 ---- batch: 030 ----
mean loss: 674.86
 ---- batch: 040 ----
mean loss: 684.37
 ---- batch: 050 ----
mean loss: 679.01
 ---- batch: 060 ----
mean loss: 689.09
 ---- batch: 070 ----
mean loss: 683.49
 ---- batch: 080 ----
mean loss: 673.55
 ---- batch: 090 ----
mean loss: 686.60
train mean loss: 681.84
epoch train time: 0:00:02.652653
elapsed time: 0:02:06.246561
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 17:49:16.839562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 676.23
 ---- batch: 020 ----
mean loss: 672.13
 ---- batch: 030 ----
mean loss: 681.79
 ---- batch: 040 ----
mean loss: 681.67
 ---- batch: 050 ----
mean loss: 677.03
 ---- batch: 060 ----
mean loss: 655.12
 ---- batch: 070 ----
mean loss: 658.32
 ---- batch: 080 ----
mean loss: 660.85
 ---- batch: 090 ----
mean loss: 687.85
train mean loss: 671.30
epoch train time: 0:00:02.659035
elapsed time: 0:02:08.906142
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 17:49:19.499132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 674.68
 ---- batch: 020 ----
mean loss: 662.79
 ---- batch: 030 ----
mean loss: 660.18
 ---- batch: 040 ----
mean loss: 675.21
 ---- batch: 050 ----
mean loss: 668.90
 ---- batch: 060 ----
mean loss: 654.39
 ---- batch: 070 ----
mean loss: 657.92
 ---- batch: 080 ----
mean loss: 636.97
 ---- batch: 090 ----
mean loss: 646.39
train mean loss: 659.68
epoch train time: 0:00:02.677339
elapsed time: 0:02:11.583950
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 17:49:22.176949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 645.32
 ---- batch: 020 ----
mean loss: 661.81
 ---- batch: 030 ----
mean loss: 652.23
 ---- batch: 040 ----
mean loss: 659.27
 ---- batch: 050 ----
mean loss: 646.56
 ---- batch: 060 ----
mean loss: 650.33
 ---- batch: 070 ----
mean loss: 650.42
 ---- batch: 080 ----
mean loss: 652.11
 ---- batch: 090 ----
mean loss: 634.26
train mean loss: 650.67
epoch train time: 0:00:02.675104
elapsed time: 0:02:14.259552
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 17:49:24.852555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 646.29
 ---- batch: 020 ----
mean loss: 643.16
 ---- batch: 030 ----
mean loss: 639.66
 ---- batch: 040 ----
mean loss: 644.15
 ---- batch: 050 ----
mean loss: 633.30
 ---- batch: 060 ----
mean loss: 644.79
 ---- batch: 070 ----
mean loss: 647.00
 ---- batch: 080 ----
mean loss: 633.63
 ---- batch: 090 ----
mean loss: 637.54
train mean loss: 640.71
epoch train time: 0:00:02.663330
elapsed time: 0:02:16.923403
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 17:49:27.516419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 636.85
 ---- batch: 020 ----
mean loss: 621.32
 ---- batch: 030 ----
mean loss: 624.58
 ---- batch: 040 ----
mean loss: 638.30
 ---- batch: 050 ----
mean loss: 632.75
 ---- batch: 060 ----
mean loss: 629.24
 ---- batch: 070 ----
mean loss: 624.89
 ---- batch: 080 ----
mean loss: 623.56
 ---- batch: 090 ----
mean loss: 630.41
train mean loss: 629.61
epoch train time: 0:00:02.685141
elapsed time: 0:02:19.609086
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 17:49:30.202080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 632.66
 ---- batch: 020 ----
mean loss: 628.48
 ---- batch: 030 ----
mean loss: 619.21
 ---- batch: 040 ----
mean loss: 614.58
 ---- batch: 050 ----
mean loss: 620.08
 ---- batch: 060 ----
mean loss: 622.99
 ---- batch: 070 ----
mean loss: 620.10
 ---- batch: 080 ----
mean loss: 612.15
 ---- batch: 090 ----
mean loss: 610.32
train mean loss: 619.59
epoch train time: 0:00:02.683472
elapsed time: 0:02:22.293061
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 17:49:32.886064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 604.84
 ---- batch: 020 ----
mean loss: 614.56
 ---- batch: 030 ----
mean loss: 615.61
 ---- batch: 040 ----
mean loss: 613.99
 ---- batch: 050 ----
mean loss: 601.49
 ---- batch: 060 ----
mean loss: 619.56
 ---- batch: 070 ----
mean loss: 605.35
 ---- batch: 080 ----
mean loss: 603.21
 ---- batch: 090 ----
mean loss: 613.12
train mean loss: 610.46
epoch train time: 0:00:02.660740
elapsed time: 0:02:24.954301
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 17:49:35.547298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 597.10
 ---- batch: 020 ----
mean loss: 592.41
 ---- batch: 030 ----
mean loss: 602.21
 ---- batch: 040 ----
mean loss: 582.07
 ---- batch: 050 ----
mean loss: 595.76
 ---- batch: 060 ----
mean loss: 601.92
 ---- batch: 070 ----
mean loss: 602.20
 ---- batch: 080 ----
mean loss: 607.42
 ---- batch: 090 ----
mean loss: 606.88
train mean loss: 600.37
epoch train time: 0:00:02.663877
elapsed time: 0:02:27.618646
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 17:49:38.211644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 592.61
 ---- batch: 020 ----
mean loss: 595.57
 ---- batch: 030 ----
mean loss: 600.14
 ---- batch: 040 ----
mean loss: 596.11
 ---- batch: 050 ----
mean loss: 585.95
 ---- batch: 060 ----
mean loss: 587.03
 ---- batch: 070 ----
mean loss: 586.14
 ---- batch: 080 ----
mean loss: 588.59
 ---- batch: 090 ----
mean loss: 594.06
train mean loss: 591.02
epoch train time: 0:00:02.645955
elapsed time: 0:02:30.265127
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 17:49:40.858120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 583.37
 ---- batch: 020 ----
mean loss: 580.25
 ---- batch: 030 ----
mean loss: 603.30
 ---- batch: 040 ----
mean loss: 579.89
 ---- batch: 050 ----
mean loss: 575.27
 ---- batch: 060 ----
mean loss: 585.56
 ---- batch: 070 ----
mean loss: 585.82
 ---- batch: 080 ----
mean loss: 571.14
 ---- batch: 090 ----
mean loss: 584.28
train mean loss: 583.17
epoch train time: 0:00:02.650393
elapsed time: 0:02:32.916013
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 17:49:43.509021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.42
 ---- batch: 020 ----
mean loss: 581.57
 ---- batch: 030 ----
mean loss: 576.97
 ---- batch: 040 ----
mean loss: 577.69
 ---- batch: 050 ----
mean loss: 570.44
 ---- batch: 060 ----
mean loss: 569.17
 ---- batch: 070 ----
mean loss: 579.53
 ---- batch: 080 ----
mean loss: 563.40
 ---- batch: 090 ----
mean loss: 569.75
train mean loss: 573.43
epoch train time: 0:00:02.666986
elapsed time: 0:02:35.583545
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 17:49:46.176560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.63
 ---- batch: 020 ----
mean loss: 569.70
 ---- batch: 030 ----
mean loss: 571.35
 ---- batch: 040 ----
mean loss: 563.52
 ---- batch: 050 ----
mean loss: 571.60
 ---- batch: 060 ----
mean loss: 570.37
 ---- batch: 070 ----
mean loss: 568.03
 ---- batch: 080 ----
mean loss: 547.87
 ---- batch: 090 ----
mean loss: 557.58
train mean loss: 565.53
epoch train time: 0:00:02.665195
elapsed time: 0:02:38.249328
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 17:49:48.842339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 561.94
 ---- batch: 020 ----
mean loss: 557.46
 ---- batch: 030 ----
mean loss: 557.09
 ---- batch: 040 ----
mean loss: 549.88
 ---- batch: 050 ----
mean loss: 578.45
 ---- batch: 060 ----
mean loss: 544.44
 ---- batch: 070 ----
mean loss: 560.41
 ---- batch: 080 ----
mean loss: 551.21
 ---- batch: 090 ----
mean loss: 560.27
train mean loss: 557.65
epoch train time: 0:00:02.635942
elapsed time: 0:02:40.885763
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 17:49:51.478755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 548.25
 ---- batch: 020 ----
mean loss: 544.27
 ---- batch: 030 ----
mean loss: 543.02
 ---- batch: 040 ----
mean loss: 542.79
 ---- batch: 050 ----
mean loss: 556.29
 ---- batch: 060 ----
mean loss: 544.00
 ---- batch: 070 ----
mean loss: 557.82
 ---- batch: 080 ----
mean loss: 555.01
 ---- batch: 090 ----
mean loss: 556.77
train mean loss: 549.64
epoch train time: 0:00:02.659843
elapsed time: 0:02:43.546102
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 17:49:54.139104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 547.19
 ---- batch: 020 ----
mean loss: 539.63
 ---- batch: 030 ----
mean loss: 547.42
 ---- batch: 040 ----
mean loss: 549.05
 ---- batch: 050 ----
mean loss: 532.90
 ---- batch: 060 ----
mean loss: 549.99
 ---- batch: 070 ----
mean loss: 532.90
 ---- batch: 080 ----
mean loss: 542.68
 ---- batch: 090 ----
mean loss: 536.74
train mean loss: 541.65
epoch train time: 0:00:02.638024
elapsed time: 0:02:46.184648
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 17:49:56.777671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.74
 ---- batch: 020 ----
mean loss: 536.32
 ---- batch: 030 ----
mean loss: 538.60
 ---- batch: 040 ----
mean loss: 531.57
 ---- batch: 050 ----
mean loss: 520.97
 ---- batch: 060 ----
mean loss: 526.00
 ---- batch: 070 ----
mean loss: 527.70
 ---- batch: 080 ----
mean loss: 522.34
 ---- batch: 090 ----
mean loss: 522.24
train mean loss: 528.39
epoch train time: 0:00:02.629687
elapsed time: 0:02:48.814876
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 17:49:59.407868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 517.08
 ---- batch: 020 ----
mean loss: 514.65
 ---- batch: 030 ----
mean loss: 519.14
 ---- batch: 040 ----
mean loss: 510.55
 ---- batch: 050 ----
mean loss: 499.58
 ---- batch: 060 ----
mean loss: 503.82
 ---- batch: 070 ----
mean loss: 506.82
 ---- batch: 080 ----
mean loss: 507.33
 ---- batch: 090 ----
mean loss: 510.18
train mean loss: 509.35
epoch train time: 0:00:02.690767
elapsed time: 0:02:51.506134
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 17:50:02.099125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.36
 ---- batch: 020 ----
mean loss: 494.81
 ---- batch: 030 ----
mean loss: 498.06
 ---- batch: 040 ----
mean loss: 483.29
 ---- batch: 050 ----
mean loss: 497.63
 ---- batch: 060 ----
mean loss: 493.57
 ---- batch: 070 ----
mean loss: 478.82
 ---- batch: 080 ----
mean loss: 485.41
 ---- batch: 090 ----
mean loss: 481.22
train mean loss: 489.82
epoch train time: 0:00:02.644934
elapsed time: 0:02:54.151576
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 17:50:04.744575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.38
 ---- batch: 020 ----
mean loss: 470.96
 ---- batch: 030 ----
mean loss: 492.16
 ---- batch: 040 ----
mean loss: 472.74
 ---- batch: 050 ----
mean loss: 472.49
 ---- batch: 060 ----
mean loss: 472.21
 ---- batch: 070 ----
mean loss: 460.11
 ---- batch: 080 ----
mean loss: 473.62
 ---- batch: 090 ----
mean loss: 448.00
train mean loss: 470.52
epoch train time: 0:00:02.667793
elapsed time: 0:02:56.819846
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 17:50:07.412806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.97
 ---- batch: 020 ----
mean loss: 450.37
 ---- batch: 030 ----
mean loss: 462.51
 ---- batch: 040 ----
mean loss: 448.85
 ---- batch: 050 ----
mean loss: 452.44
 ---- batch: 060 ----
mean loss: 459.73
 ---- batch: 070 ----
mean loss: 452.34
 ---- batch: 080 ----
mean loss: 439.92
 ---- batch: 090 ----
mean loss: 454.57
train mean loss: 454.41
epoch train time: 0:00:02.637237
elapsed time: 0:02:59.457522
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 17:50:10.050509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.71
 ---- batch: 020 ----
mean loss: 440.08
 ---- batch: 030 ----
mean loss: 442.22
 ---- batch: 040 ----
mean loss: 445.98
 ---- batch: 050 ----
mean loss: 440.24
 ---- batch: 060 ----
mean loss: 440.75
 ---- batch: 070 ----
mean loss: 447.73
 ---- batch: 080 ----
mean loss: 418.32
 ---- batch: 090 ----
mean loss: 429.24
train mean loss: 439.01
epoch train time: 0:00:02.655318
elapsed time: 0:03:02.113345
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 17:50:12.706345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.27
 ---- batch: 020 ----
mean loss: 423.91
 ---- batch: 030 ----
mean loss: 438.17
 ---- batch: 040 ----
mean loss: 418.92
 ---- batch: 050 ----
mean loss: 427.83
 ---- batch: 060 ----
mean loss: 416.79
 ---- batch: 070 ----
mean loss: 422.94
 ---- batch: 080 ----
mean loss: 420.70
 ---- batch: 090 ----
mean loss: 415.40
train mean loss: 424.51
epoch train time: 0:00:02.617401
elapsed time: 0:03:04.731222
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 17:50:15.324221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.10
 ---- batch: 020 ----
mean loss: 425.56
 ---- batch: 030 ----
mean loss: 419.67
 ---- batch: 040 ----
mean loss: 402.44
 ---- batch: 050 ----
mean loss: 420.74
 ---- batch: 060 ----
mean loss: 409.08
 ---- batch: 070 ----
mean loss: 403.06
 ---- batch: 080 ----
mean loss: 402.19
 ---- batch: 090 ----
mean loss: 401.01
train mean loss: 409.63
epoch train time: 0:00:02.683814
elapsed time: 0:03:07.415539
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 17:50:18.008537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.38
 ---- batch: 020 ----
mean loss: 394.04
 ---- batch: 030 ----
mean loss: 400.49
 ---- batch: 040 ----
mean loss: 398.74
 ---- batch: 050 ----
mean loss: 396.57
 ---- batch: 060 ----
mean loss: 387.46
 ---- batch: 070 ----
mean loss: 394.92
 ---- batch: 080 ----
mean loss: 392.53
 ---- batch: 090 ----
mean loss: 395.30
train mean loss: 395.87
epoch train time: 0:00:02.679018
elapsed time: 0:03:10.095073
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 17:50:20.688073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.31
 ---- batch: 020 ----
mean loss: 385.64
 ---- batch: 030 ----
mean loss: 380.12
 ---- batch: 040 ----
mean loss: 382.86
 ---- batch: 050 ----
mean loss: 381.63
 ---- batch: 060 ----
mean loss: 384.67
 ---- batch: 070 ----
mean loss: 379.56
 ---- batch: 080 ----
mean loss: 377.44
 ---- batch: 090 ----
mean loss: 380.67
train mean loss: 382.41
epoch train time: 0:00:02.661556
elapsed time: 0:03:12.757118
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 17:50:23.350111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.49
 ---- batch: 020 ----
mean loss: 366.32
 ---- batch: 030 ----
mean loss: 360.22
 ---- batch: 040 ----
mean loss: 377.74
 ---- batch: 050 ----
mean loss: 365.84
 ---- batch: 060 ----
mean loss: 363.81
 ---- batch: 070 ----
mean loss: 373.34
 ---- batch: 080 ----
mean loss: 374.27
 ---- batch: 090 ----
mean loss: 370.08
train mean loss: 368.35
epoch train time: 0:00:02.649165
elapsed time: 0:03:15.406742
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 17:50:25.999742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.08
 ---- batch: 020 ----
mean loss: 357.58
 ---- batch: 030 ----
mean loss: 360.87
 ---- batch: 040 ----
mean loss: 344.79
 ---- batch: 050 ----
mean loss: 354.66
 ---- batch: 060 ----
mean loss: 360.47
 ---- batch: 070 ----
mean loss: 355.53
 ---- batch: 080 ----
mean loss: 353.65
 ---- batch: 090 ----
mean loss: 366.71
train mean loss: 356.84
epoch train time: 0:00:02.629506
elapsed time: 0:03:18.036702
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 17:50:28.629813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.86
 ---- batch: 020 ----
mean loss: 341.86
 ---- batch: 030 ----
mean loss: 347.11
 ---- batch: 040 ----
mean loss: 343.56
 ---- batch: 050 ----
mean loss: 353.30
 ---- batch: 060 ----
mean loss: 349.92
 ---- batch: 070 ----
mean loss: 350.12
 ---- batch: 080 ----
mean loss: 346.50
 ---- batch: 090 ----
mean loss: 342.12
train mean loss: 347.20
epoch train time: 0:00:02.653266
elapsed time: 0:03:20.690563
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 17:50:31.283557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.78
 ---- batch: 020 ----
mean loss: 341.63
 ---- batch: 030 ----
mean loss: 341.78
 ---- batch: 040 ----
mean loss: 339.71
 ---- batch: 050 ----
mean loss: 354.23
 ---- batch: 060 ----
mean loss: 332.82
 ---- batch: 070 ----
mean loss: 330.42
 ---- batch: 080 ----
mean loss: 335.04
 ---- batch: 090 ----
mean loss: 333.47
train mean loss: 338.98
epoch train time: 0:00:02.665589
elapsed time: 0:03:23.356660
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 17:50:33.949690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.94
 ---- batch: 020 ----
mean loss: 334.71
 ---- batch: 030 ----
mean loss: 332.93
 ---- batch: 040 ----
mean loss: 336.62
 ---- batch: 050 ----
mean loss: 342.70
 ---- batch: 060 ----
mean loss: 329.14
 ---- batch: 070 ----
mean loss: 323.09
 ---- batch: 080 ----
mean loss: 328.39
 ---- batch: 090 ----
mean loss: 328.66
train mean loss: 332.09
epoch train time: 0:00:02.675233
elapsed time: 0:03:26.032421
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 17:50:36.625452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.78
 ---- batch: 020 ----
mean loss: 324.49
 ---- batch: 030 ----
mean loss: 332.09
 ---- batch: 040 ----
mean loss: 319.17
 ---- batch: 050 ----
mean loss: 324.55
 ---- batch: 060 ----
mean loss: 334.66
 ---- batch: 070 ----
mean loss: 320.70
 ---- batch: 080 ----
mean loss: 327.06
 ---- batch: 090 ----
mean loss: 324.57
train mean loss: 325.33
epoch train time: 0:00:02.679446
elapsed time: 0:03:28.712401
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 17:50:39.305389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.08
 ---- batch: 020 ----
mean loss: 317.65
 ---- batch: 030 ----
mean loss: 332.27
 ---- batch: 040 ----
mean loss: 326.72
 ---- batch: 050 ----
mean loss: 323.86
 ---- batch: 060 ----
mean loss: 318.57
 ---- batch: 070 ----
mean loss: 312.82
 ---- batch: 080 ----
mean loss: 316.16
 ---- batch: 090 ----
mean loss: 318.25
train mean loss: 320.34
epoch train time: 0:00:02.655112
elapsed time: 0:03:31.367986
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 17:50:41.960976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.10
 ---- batch: 020 ----
mean loss: 313.40
 ---- batch: 030 ----
mean loss: 305.80
 ---- batch: 040 ----
mean loss: 308.04
 ---- batch: 050 ----
mean loss: 315.07
 ---- batch: 060 ----
mean loss: 325.36
 ---- batch: 070 ----
mean loss: 327.86
 ---- batch: 080 ----
mean loss: 317.98
 ---- batch: 090 ----
mean loss: 306.92
train mean loss: 315.09
epoch train time: 0:00:02.637276
elapsed time: 0:03:34.005730
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 17:50:44.598714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.84
 ---- batch: 020 ----
mean loss: 312.07
 ---- batch: 030 ----
mean loss: 310.64
 ---- batch: 040 ----
mean loss: 306.97
 ---- batch: 050 ----
mean loss: 304.53
 ---- batch: 060 ----
mean loss: 313.09
 ---- batch: 070 ----
mean loss: 314.92
 ---- batch: 080 ----
mean loss: 305.35
 ---- batch: 090 ----
mean loss: 311.49
train mean loss: 310.97
epoch train time: 0:00:02.692646
elapsed time: 0:03:36.698906
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 17:50:47.291925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.91
 ---- batch: 020 ----
mean loss: 299.65
 ---- batch: 030 ----
mean loss: 306.08
 ---- batch: 040 ----
mean loss: 308.19
 ---- batch: 050 ----
mean loss: 301.57
 ---- batch: 060 ----
mean loss: 297.63
 ---- batch: 070 ----
mean loss: 295.22
 ---- batch: 080 ----
mean loss: 317.44
 ---- batch: 090 ----
mean loss: 313.60
train mean loss: 306.50
epoch train time: 0:00:02.691524
elapsed time: 0:03:39.390920
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 17:50:49.983911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.78
 ---- batch: 020 ----
mean loss: 301.89
 ---- batch: 030 ----
mean loss: 310.54
 ---- batch: 040 ----
mean loss: 312.55
 ---- batch: 050 ----
mean loss: 306.03
 ---- batch: 060 ----
mean loss: 294.62
 ---- batch: 070 ----
mean loss: 299.84
 ---- batch: 080 ----
mean loss: 295.16
 ---- batch: 090 ----
mean loss: 305.62
train mean loss: 301.92
epoch train time: 0:00:02.641299
elapsed time: 0:03:42.032661
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 17:50:52.625683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.60
 ---- batch: 020 ----
mean loss: 294.60
 ---- batch: 030 ----
mean loss: 307.45
 ---- batch: 040 ----
mean loss: 299.00
 ---- batch: 050 ----
mean loss: 304.56
 ---- batch: 060 ----
mean loss: 303.74
 ---- batch: 070 ----
mean loss: 284.53
 ---- batch: 080 ----
mean loss: 295.82
 ---- batch: 090 ----
mean loss: 295.54
train mean loss: 297.74
epoch train time: 0:00:02.671073
elapsed time: 0:03:44.704224
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 17:50:55.297234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.17
 ---- batch: 020 ----
mean loss: 283.35
 ---- batch: 030 ----
mean loss: 289.01
 ---- batch: 040 ----
mean loss: 297.28
 ---- batch: 050 ----
mean loss: 295.16
 ---- batch: 060 ----
mean loss: 296.23
 ---- batch: 070 ----
mean loss: 292.12
 ---- batch: 080 ----
mean loss: 295.78
 ---- batch: 090 ----
mean loss: 295.73
train mean loss: 292.66
epoch train time: 0:00:02.615787
elapsed time: 0:03:47.320562
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 17:50:57.913563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.78
 ---- batch: 020 ----
mean loss: 285.30
 ---- batch: 030 ----
mean loss: 297.48
 ---- batch: 040 ----
mean loss: 280.19
 ---- batch: 050 ----
mean loss: 281.58
 ---- batch: 060 ----
mean loss: 286.06
 ---- batch: 070 ----
mean loss: 285.74
 ---- batch: 080 ----
mean loss: 289.64
 ---- batch: 090 ----
mean loss: 282.60
train mean loss: 285.53
epoch train time: 0:00:02.647494
elapsed time: 0:03:49.968521
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 17:51:00.561513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.90
 ---- batch: 020 ----
mean loss: 282.50
 ---- batch: 030 ----
mean loss: 285.62
 ---- batch: 040 ----
mean loss: 275.31
 ---- batch: 050 ----
mean loss: 276.54
 ---- batch: 060 ----
mean loss: 280.34
 ---- batch: 070 ----
mean loss: 280.05
 ---- batch: 080 ----
mean loss: 278.52
 ---- batch: 090 ----
mean loss: 275.39
train mean loss: 279.02
epoch train time: 0:00:02.677695
elapsed time: 0:03:52.646698
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 17:51:03.239705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.80
 ---- batch: 020 ----
mean loss: 276.62
 ---- batch: 030 ----
mean loss: 275.03
 ---- batch: 040 ----
mean loss: 278.67
 ---- batch: 050 ----
mean loss: 266.16
 ---- batch: 060 ----
mean loss: 275.14
 ---- batch: 070 ----
mean loss: 278.17
 ---- batch: 080 ----
mean loss: 275.73
 ---- batch: 090 ----
mean loss: 279.56
train mean loss: 274.45
epoch train time: 0:00:02.655541
elapsed time: 0:03:55.302750
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 17:51:05.895746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.19
 ---- batch: 020 ----
mean loss: 275.07
 ---- batch: 030 ----
mean loss: 255.01
 ---- batch: 040 ----
mean loss: 271.88
 ---- batch: 050 ----
mean loss: 278.62
 ---- batch: 060 ----
mean loss: 262.45
 ---- batch: 070 ----
mean loss: 272.27
 ---- batch: 080 ----
mean loss: 272.61
 ---- batch: 090 ----
mean loss: 270.61
train mean loss: 270.93
epoch train time: 0:00:02.639859
elapsed time: 0:03:57.943068
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 17:51:08.536094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.03
 ---- batch: 020 ----
mean loss: 270.80
 ---- batch: 030 ----
mean loss: 264.85
 ---- batch: 040 ----
mean loss: 277.66
 ---- batch: 050 ----
mean loss: 261.09
 ---- batch: 060 ----
mean loss: 266.30
 ---- batch: 070 ----
mean loss: 265.34
 ---- batch: 080 ----
mean loss: 267.26
 ---- batch: 090 ----
mean loss: 267.23
train mean loss: 266.95
epoch train time: 0:00:02.622452
elapsed time: 0:04:00.566056
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 17:51:11.159044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.76
 ---- batch: 020 ----
mean loss: 260.34
 ---- batch: 030 ----
mean loss: 262.64
 ---- batch: 040 ----
mean loss: 275.29
 ---- batch: 050 ----
mean loss: 267.23
 ---- batch: 060 ----
mean loss: 259.44
 ---- batch: 070 ----
mean loss: 275.04
 ---- batch: 080 ----
mean loss: 258.77
 ---- batch: 090 ----
mean loss: 265.94
train mean loss: 264.03
epoch train time: 0:00:02.695114
elapsed time: 0:04:03.261685
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 17:51:13.854682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.45
 ---- batch: 020 ----
mean loss: 263.90
 ---- batch: 030 ----
mean loss: 260.86
 ---- batch: 040 ----
mean loss: 266.54
 ---- batch: 050 ----
mean loss: 260.58
 ---- batch: 060 ----
mean loss: 269.50
 ---- batch: 070 ----
mean loss: 249.64
 ---- batch: 080 ----
mean loss: 261.42
 ---- batch: 090 ----
mean loss: 260.14
train mean loss: 261.23
epoch train time: 0:00:02.665927
elapsed time: 0:04:05.928118
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 17:51:16.521130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.94
 ---- batch: 020 ----
mean loss: 257.35
 ---- batch: 030 ----
mean loss: 256.98
 ---- batch: 040 ----
mean loss: 255.66
 ---- batch: 050 ----
mean loss: 258.23
 ---- batch: 060 ----
mean loss: 257.52
 ---- batch: 070 ----
mean loss: 257.58
 ---- batch: 080 ----
mean loss: 258.66
 ---- batch: 090 ----
mean loss: 263.55
train mean loss: 258.98
epoch train time: 0:00:02.663350
elapsed time: 0:04:08.591995
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 17:51:19.184997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.87
 ---- batch: 020 ----
mean loss: 263.47
 ---- batch: 030 ----
mean loss: 262.94
 ---- batch: 040 ----
mean loss: 254.53
 ---- batch: 050 ----
mean loss: 264.53
 ---- batch: 060 ----
mean loss: 253.27
 ---- batch: 070 ----
mean loss: 253.09
 ---- batch: 080 ----
mean loss: 246.14
 ---- batch: 090 ----
mean loss: 255.12
train mean loss: 255.82
epoch train time: 0:00:02.656902
elapsed time: 0:04:11.249433
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 17:51:21.842426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.47
 ---- batch: 020 ----
mean loss: 250.06
 ---- batch: 030 ----
mean loss: 247.97
 ---- batch: 040 ----
mean loss: 254.27
 ---- batch: 050 ----
mean loss: 255.42
 ---- batch: 060 ----
mean loss: 260.61
 ---- batch: 070 ----
mean loss: 250.43
 ---- batch: 080 ----
mean loss: 263.28
 ---- batch: 090 ----
mean loss: 252.97
train mean loss: 254.38
epoch train time: 0:00:02.630666
elapsed time: 0:04:13.880559
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 17:51:24.473532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.90
 ---- batch: 020 ----
mean loss: 245.09
 ---- batch: 030 ----
mean loss: 256.03
 ---- batch: 040 ----
mean loss: 250.96
 ---- batch: 050 ----
mean loss: 255.08
 ---- batch: 060 ----
mean loss: 254.32
 ---- batch: 070 ----
mean loss: 249.09
 ---- batch: 080 ----
mean loss: 250.27
 ---- batch: 090 ----
mean loss: 250.60
train mean loss: 252.39
epoch train time: 0:00:02.675262
elapsed time: 0:04:16.556289
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 17:51:27.149278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.98
 ---- batch: 020 ----
mean loss: 246.81
 ---- batch: 030 ----
mean loss: 250.97
 ---- batch: 040 ----
mean loss: 241.98
 ---- batch: 050 ----
mean loss: 251.71
 ---- batch: 060 ----
mean loss: 253.13
 ---- batch: 070 ----
mean loss: 247.51
 ---- batch: 080 ----
mean loss: 249.21
 ---- batch: 090 ----
mean loss: 257.59
train mean loss: 249.90
epoch train time: 0:00:02.683181
elapsed time: 0:04:19.240008
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 17:51:29.833094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.87
 ---- batch: 020 ----
mean loss: 256.45
 ---- batch: 030 ----
mean loss: 244.00
 ---- batch: 040 ----
mean loss: 252.79
 ---- batch: 050 ----
mean loss: 243.07
 ---- batch: 060 ----
mean loss: 240.21
 ---- batch: 070 ----
mean loss: 236.23
 ---- batch: 080 ----
mean loss: 242.77
 ---- batch: 090 ----
mean loss: 257.07
train mean loss: 248.08
epoch train time: 0:00:02.664466
elapsed time: 0:04:21.905069
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 17:51:32.498063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.00
 ---- batch: 020 ----
mean loss: 246.30
 ---- batch: 030 ----
mean loss: 249.38
 ---- batch: 040 ----
mean loss: 249.41
 ---- batch: 050 ----
mean loss: 244.43
 ---- batch: 060 ----
mean loss: 244.00
 ---- batch: 070 ----
mean loss: 243.37
 ---- batch: 080 ----
mean loss: 246.76
 ---- batch: 090 ----
mean loss: 239.67
train mean loss: 245.86
epoch train time: 0:00:02.656385
elapsed time: 0:04:24.561940
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 17:51:35.154931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.18
 ---- batch: 020 ----
mean loss: 246.58
 ---- batch: 030 ----
mean loss: 249.12
 ---- batch: 040 ----
mean loss: 247.67
 ---- batch: 050 ----
mean loss: 231.74
 ---- batch: 060 ----
mean loss: 252.91
 ---- batch: 070 ----
mean loss: 241.90
 ---- batch: 080 ----
mean loss: 247.50
 ---- batch: 090 ----
mean loss: 239.85
train mean loss: 244.30
epoch train time: 0:00:02.633611
elapsed time: 0:04:27.196073
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 17:51:37.789024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.29
 ---- batch: 020 ----
mean loss: 243.20
 ---- batch: 030 ----
mean loss: 244.83
 ---- batch: 040 ----
mean loss: 242.29
 ---- batch: 050 ----
mean loss: 240.77
 ---- batch: 060 ----
mean loss: 249.97
 ---- batch: 070 ----
mean loss: 239.26
 ---- batch: 080 ----
mean loss: 240.11
 ---- batch: 090 ----
mean loss: 239.93
train mean loss: 243.19
epoch train time: 0:00:02.688867
elapsed time: 0:04:29.885417
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 17:51:40.478393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.74
 ---- batch: 020 ----
mean loss: 248.42
 ---- batch: 030 ----
mean loss: 243.78
 ---- batch: 040 ----
mean loss: 238.28
 ---- batch: 050 ----
mean loss: 243.68
 ---- batch: 060 ----
mean loss: 238.50
 ---- batch: 070 ----
mean loss: 241.97
 ---- batch: 080 ----
mean loss: 238.13
 ---- batch: 090 ----
mean loss: 240.68
train mean loss: 241.17
epoch train time: 0:00:02.672622
elapsed time: 0:04:32.558571
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 17:51:43.151563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.84
 ---- batch: 020 ----
mean loss: 238.95
 ---- batch: 030 ----
mean loss: 227.04
 ---- batch: 040 ----
mean loss: 243.61
 ---- batch: 050 ----
mean loss: 245.64
 ---- batch: 060 ----
mean loss: 249.76
 ---- batch: 070 ----
mean loss: 238.63
 ---- batch: 080 ----
mean loss: 241.69
 ---- batch: 090 ----
mean loss: 234.41
train mean loss: 239.51
epoch train time: 0:00:02.652163
elapsed time: 0:04:35.211188
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 17:51:45.804180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.41
 ---- batch: 020 ----
mean loss: 243.25
 ---- batch: 030 ----
mean loss: 237.01
 ---- batch: 040 ----
mean loss: 235.24
 ---- batch: 050 ----
mean loss: 241.63
 ---- batch: 060 ----
mean loss: 248.28
 ---- batch: 070 ----
mean loss: 240.78
 ---- batch: 080 ----
mean loss: 237.66
 ---- batch: 090 ----
mean loss: 228.72
train mean loss: 238.53
epoch train time: 0:00:02.660500
elapsed time: 0:04:37.872198
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 17:51:48.465272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.52
 ---- batch: 020 ----
mean loss: 244.73
 ---- batch: 030 ----
mean loss: 235.20
 ---- batch: 040 ----
mean loss: 238.79
 ---- batch: 050 ----
mean loss: 234.57
 ---- batch: 060 ----
mean loss: 240.90
 ---- batch: 070 ----
mean loss: 235.48
 ---- batch: 080 ----
mean loss: 239.36
 ---- batch: 090 ----
mean loss: 237.29
train mean loss: 237.66
epoch train time: 0:00:02.702909
elapsed time: 0:04:40.575666
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 17:51:51.168661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.04
 ---- batch: 020 ----
mean loss: 243.15
 ---- batch: 030 ----
mean loss: 238.19
 ---- batch: 040 ----
mean loss: 239.02
 ---- batch: 050 ----
mean loss: 234.26
 ---- batch: 060 ----
mean loss: 241.90
 ---- batch: 070 ----
mean loss: 241.38
 ---- batch: 080 ----
mean loss: 239.58
 ---- batch: 090 ----
mean loss: 226.33
train mean loss: 236.69
epoch train time: 0:00:02.706835
elapsed time: 0:04:43.283014
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 17:51:53.876061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.28
 ---- batch: 020 ----
mean loss: 231.19
 ---- batch: 030 ----
mean loss: 242.84
 ---- batch: 040 ----
mean loss: 226.67
 ---- batch: 050 ----
mean loss: 230.39
 ---- batch: 060 ----
mean loss: 244.69
 ---- batch: 070 ----
mean loss: 237.16
 ---- batch: 080 ----
mean loss: 229.83
 ---- batch: 090 ----
mean loss: 241.29
train mean loss: 234.97
epoch train time: 0:00:02.728474
elapsed time: 0:04:46.012026
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 17:51:56.605020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.63
 ---- batch: 020 ----
mean loss: 229.09
 ---- batch: 030 ----
mean loss: 235.67
 ---- batch: 040 ----
mean loss: 234.53
 ---- batch: 050 ----
mean loss: 233.99
 ---- batch: 060 ----
mean loss: 236.03
 ---- batch: 070 ----
mean loss: 228.32
 ---- batch: 080 ----
mean loss: 232.82
 ---- batch: 090 ----
mean loss: 235.95
train mean loss: 234.06
epoch train time: 0:00:02.693006
elapsed time: 0:04:48.705546
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 17:51:59.298519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.87
 ---- batch: 020 ----
mean loss: 227.78
 ---- batch: 030 ----
mean loss: 232.92
 ---- batch: 040 ----
mean loss: 233.56
 ---- batch: 050 ----
mean loss: 231.66
 ---- batch: 060 ----
mean loss: 233.68
 ---- batch: 070 ----
mean loss: 237.57
 ---- batch: 080 ----
mean loss: 230.73
 ---- batch: 090 ----
mean loss: 233.13
train mean loss: 233.43
epoch train time: 0:00:02.693877
elapsed time: 0:04:51.399907
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 17:52:01.992906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.30
 ---- batch: 020 ----
mean loss: 227.77
 ---- batch: 030 ----
mean loss: 230.92
 ---- batch: 040 ----
mean loss: 235.87
 ---- batch: 050 ----
mean loss: 234.34
 ---- batch: 060 ----
mean loss: 228.56
 ---- batch: 070 ----
mean loss: 235.03
 ---- batch: 080 ----
mean loss: 235.34
 ---- batch: 090 ----
mean loss: 236.73
train mean loss: 232.60
epoch train time: 0:00:02.704891
elapsed time: 0:04:54.105301
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 17:52:04.698323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.43
 ---- batch: 020 ----
mean loss: 236.05
 ---- batch: 030 ----
mean loss: 226.56
 ---- batch: 040 ----
mean loss: 232.55
 ---- batch: 050 ----
mean loss: 227.18
 ---- batch: 060 ----
mean loss: 234.49
 ---- batch: 070 ----
mean loss: 236.34
 ---- batch: 080 ----
mean loss: 231.02
 ---- batch: 090 ----
mean loss: 228.88
train mean loss: 231.44
epoch train time: 0:00:02.698712
elapsed time: 0:04:56.804522
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 17:52:07.397523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.56
 ---- batch: 020 ----
mean loss: 224.05
 ---- batch: 030 ----
mean loss: 231.27
 ---- batch: 040 ----
mean loss: 228.98
 ---- batch: 050 ----
mean loss: 234.21
 ---- batch: 060 ----
mean loss: 233.51
 ---- batch: 070 ----
mean loss: 226.93
 ---- batch: 080 ----
mean loss: 233.00
 ---- batch: 090 ----
mean loss: 233.31
train mean loss: 231.30
epoch train time: 0:00:02.709012
elapsed time: 0:04:59.514000
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 17:52:10.107012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.47
 ---- batch: 020 ----
mean loss: 224.36
 ---- batch: 030 ----
mean loss: 227.20
 ---- batch: 040 ----
mean loss: 231.03
 ---- batch: 050 ----
mean loss: 241.85
 ---- batch: 060 ----
mean loss: 221.99
 ---- batch: 070 ----
mean loss: 222.50
 ---- batch: 080 ----
mean loss: 229.48
 ---- batch: 090 ----
mean loss: 231.41
train mean loss: 230.18
epoch train time: 0:00:02.711500
elapsed time: 0:05:02.225995
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 17:52:12.819006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.84
 ---- batch: 020 ----
mean loss: 231.54
 ---- batch: 030 ----
mean loss: 227.69
 ---- batch: 040 ----
mean loss: 229.54
 ---- batch: 050 ----
mean loss: 237.21
 ---- batch: 060 ----
mean loss: 234.77
 ---- batch: 070 ----
mean loss: 226.99
 ---- batch: 080 ----
mean loss: 224.78
 ---- batch: 090 ----
mean loss: 226.45
train mean loss: 229.90
epoch train time: 0:00:02.694942
elapsed time: 0:05:04.921487
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 17:52:15.514486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.09
 ---- batch: 020 ----
mean loss: 224.90
 ---- batch: 030 ----
mean loss: 228.35
 ---- batch: 040 ----
mean loss: 228.31
 ---- batch: 050 ----
mean loss: 224.07
 ---- batch: 060 ----
mean loss: 233.85
 ---- batch: 070 ----
mean loss: 224.79
 ---- batch: 080 ----
mean loss: 229.23
 ---- batch: 090 ----
mean loss: 233.85
train mean loss: 228.92
epoch train time: 0:00:02.688975
elapsed time: 0:05:07.610957
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 17:52:18.203954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.29
 ---- batch: 020 ----
mean loss: 233.26
 ---- batch: 030 ----
mean loss: 221.51
 ---- batch: 040 ----
mean loss: 225.66
 ---- batch: 050 ----
mean loss: 224.05
 ---- batch: 060 ----
mean loss: 230.11
 ---- batch: 070 ----
mean loss: 233.96
 ---- batch: 080 ----
mean loss: 222.07
 ---- batch: 090 ----
mean loss: 227.00
train mean loss: 228.41
epoch train time: 0:00:02.726272
elapsed time: 0:05:10.337698
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 17:52:20.930705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.75
 ---- batch: 020 ----
mean loss: 232.62
 ---- batch: 030 ----
mean loss: 222.53
 ---- batch: 040 ----
mean loss: 228.73
 ---- batch: 050 ----
mean loss: 227.14
 ---- batch: 060 ----
mean loss: 233.34
 ---- batch: 070 ----
mean loss: 219.34
 ---- batch: 080 ----
mean loss: 225.27
 ---- batch: 090 ----
mean loss: 222.87
train mean loss: 227.75
epoch train time: 0:00:02.706386
elapsed time: 0:05:13.044530
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 17:52:23.637539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.91
 ---- batch: 020 ----
mean loss: 224.30
 ---- batch: 030 ----
mean loss: 236.71
 ---- batch: 040 ----
mean loss: 223.07
 ---- batch: 050 ----
mean loss: 218.27
 ---- batch: 060 ----
mean loss: 229.89
 ---- batch: 070 ----
mean loss: 226.57
 ---- batch: 080 ----
mean loss: 227.59
 ---- batch: 090 ----
mean loss: 229.46
train mean loss: 226.80
epoch train time: 0:00:02.705949
elapsed time: 0:05:15.751016
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 17:52:26.344024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.95
 ---- batch: 020 ----
mean loss: 229.73
 ---- batch: 030 ----
mean loss: 226.79
 ---- batch: 040 ----
mean loss: 217.85
 ---- batch: 050 ----
mean loss: 230.55
 ---- batch: 060 ----
mean loss: 229.67
 ---- batch: 070 ----
mean loss: 224.72
 ---- batch: 080 ----
mean loss: 225.88
 ---- batch: 090 ----
mean loss: 225.36
train mean loss: 226.67
epoch train time: 0:00:02.722231
elapsed time: 0:05:18.473905
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 17:52:29.066881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.53
 ---- batch: 020 ----
mean loss: 229.85
 ---- batch: 030 ----
mean loss: 226.10
 ---- batch: 040 ----
mean loss: 226.35
 ---- batch: 050 ----
mean loss: 223.13
 ---- batch: 060 ----
mean loss: 227.84
 ---- batch: 070 ----
mean loss: 228.86
 ---- batch: 080 ----
mean loss: 231.21
 ---- batch: 090 ----
mean loss: 222.98
train mean loss: 225.73
epoch train time: 0:00:02.713245
elapsed time: 0:05:21.187636
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 17:52:31.780632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.05
 ---- batch: 020 ----
mean loss: 224.87
 ---- batch: 030 ----
mean loss: 222.67
 ---- batch: 040 ----
mean loss: 225.25
 ---- batch: 050 ----
mean loss: 219.59
 ---- batch: 060 ----
mean loss: 226.22
 ---- batch: 070 ----
mean loss: 228.67
 ---- batch: 080 ----
mean loss: 225.44
 ---- batch: 090 ----
mean loss: 224.21
train mean loss: 225.29
epoch train time: 0:00:02.717917
elapsed time: 0:05:23.906099
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 17:52:34.499158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.39
 ---- batch: 020 ----
mean loss: 226.38
 ---- batch: 030 ----
mean loss: 228.23
 ---- batch: 040 ----
mean loss: 222.51
 ---- batch: 050 ----
mean loss: 222.83
 ---- batch: 060 ----
mean loss: 226.77
 ---- batch: 070 ----
mean loss: 228.36
 ---- batch: 080 ----
mean loss: 222.35
 ---- batch: 090 ----
mean loss: 221.24
train mean loss: 225.70
epoch train time: 0:00:02.713223
elapsed time: 0:05:26.619865
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 17:52:37.212859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.62
 ---- batch: 020 ----
mean loss: 224.42
 ---- batch: 030 ----
mean loss: 228.77
 ---- batch: 040 ----
mean loss: 223.24
 ---- batch: 050 ----
mean loss: 220.25
 ---- batch: 060 ----
mean loss: 227.76
 ---- batch: 070 ----
mean loss: 221.27
 ---- batch: 080 ----
mean loss: 222.87
 ---- batch: 090 ----
mean loss: 225.72
train mean loss: 224.78
epoch train time: 0:00:02.704194
elapsed time: 0:05:29.324566
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 17:52:39.917577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.49
 ---- batch: 020 ----
mean loss: 223.71
 ---- batch: 030 ----
mean loss: 229.40
 ---- batch: 040 ----
mean loss: 230.60
 ---- batch: 050 ----
mean loss: 227.97
 ---- batch: 060 ----
mean loss: 227.64
 ---- batch: 070 ----
mean loss: 227.83
 ---- batch: 080 ----
mean loss: 222.02
 ---- batch: 090 ----
mean loss: 218.36
train mean loss: 224.20
epoch train time: 0:00:02.695744
elapsed time: 0:05:32.020815
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 17:52:42.613811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.53
 ---- batch: 020 ----
mean loss: 225.97
 ---- batch: 030 ----
mean loss: 220.22
 ---- batch: 040 ----
mean loss: 218.66
 ---- batch: 050 ----
mean loss: 219.60
 ---- batch: 060 ----
mean loss: 229.15
 ---- batch: 070 ----
mean loss: 233.82
 ---- batch: 080 ----
mean loss: 224.42
 ---- batch: 090 ----
mean loss: 224.76
train mean loss: 224.30
epoch train time: 0:00:02.689410
elapsed time: 0:05:34.710690
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 17:52:45.303683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.75
 ---- batch: 020 ----
mean loss: 219.33
 ---- batch: 030 ----
mean loss: 219.40
 ---- batch: 040 ----
mean loss: 225.62
 ---- batch: 050 ----
mean loss: 222.06
 ---- batch: 060 ----
mean loss: 230.07
 ---- batch: 070 ----
mean loss: 214.81
 ---- batch: 080 ----
mean loss: 224.98
 ---- batch: 090 ----
mean loss: 225.15
train mean loss: 223.89
epoch train time: 0:00:02.715737
elapsed time: 0:05:37.426930
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 17:52:48.019925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.82
 ---- batch: 020 ----
mean loss: 222.24
 ---- batch: 030 ----
mean loss: 216.50
 ---- batch: 040 ----
mean loss: 227.03
 ---- batch: 050 ----
mean loss: 221.01
 ---- batch: 060 ----
mean loss: 224.82
 ---- batch: 070 ----
mean loss: 224.43
 ---- batch: 080 ----
mean loss: 214.84
 ---- batch: 090 ----
mean loss: 226.31
train mean loss: 222.97
epoch train time: 0:00:02.709264
elapsed time: 0:05:40.136650
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 17:52:50.729704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.69
 ---- batch: 020 ----
mean loss: 226.43
 ---- batch: 030 ----
mean loss: 219.88
 ---- batch: 040 ----
mean loss: 215.65
 ---- batch: 050 ----
mean loss: 233.38
 ---- batch: 060 ----
mean loss: 223.24
 ---- batch: 070 ----
mean loss: 226.94
 ---- batch: 080 ----
mean loss: 219.85
 ---- batch: 090 ----
mean loss: 219.05
train mean loss: 222.93
epoch train time: 0:00:02.688428
elapsed time: 0:05:42.825673
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 17:52:53.418681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.91
 ---- batch: 020 ----
mean loss: 216.36
 ---- batch: 030 ----
mean loss: 222.19
 ---- batch: 040 ----
mean loss: 220.37
 ---- batch: 050 ----
mean loss: 222.40
 ---- batch: 060 ----
mean loss: 225.60
 ---- batch: 070 ----
mean loss: 229.94
 ---- batch: 080 ----
mean loss: 231.11
 ---- batch: 090 ----
mean loss: 229.28
train mean loss: 221.98
epoch train time: 0:00:02.703284
elapsed time: 0:05:45.529465
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 17:52:56.122438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.42
 ---- batch: 020 ----
mean loss: 217.65
 ---- batch: 030 ----
mean loss: 222.82
 ---- batch: 040 ----
mean loss: 223.26
 ---- batch: 050 ----
mean loss: 221.91
 ---- batch: 060 ----
mean loss: 214.78
 ---- batch: 070 ----
mean loss: 224.77
 ---- batch: 080 ----
mean loss: 226.03
 ---- batch: 090 ----
mean loss: 231.59
train mean loss: 222.24
epoch train time: 0:00:02.697306
elapsed time: 0:05:48.227212
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 17:52:58.820206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.06
 ---- batch: 020 ----
mean loss: 223.71
 ---- batch: 030 ----
mean loss: 224.28
 ---- batch: 040 ----
mean loss: 227.92
 ---- batch: 050 ----
mean loss: 226.15
 ---- batch: 060 ----
mean loss: 223.67
 ---- batch: 070 ----
mean loss: 219.15
 ---- batch: 080 ----
mean loss: 225.40
 ---- batch: 090 ----
mean loss: 209.43
train mean loss: 221.81
epoch train time: 0:00:02.700841
elapsed time: 0:05:50.928513
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 17:53:01.521501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.92
 ---- batch: 020 ----
mean loss: 219.15
 ---- batch: 030 ----
mean loss: 218.10
 ---- batch: 040 ----
mean loss: 222.56
 ---- batch: 050 ----
mean loss: 212.87
 ---- batch: 060 ----
mean loss: 225.56
 ---- batch: 070 ----
mean loss: 221.35
 ---- batch: 080 ----
mean loss: 227.92
 ---- batch: 090 ----
mean loss: 220.18
train mean loss: 221.82
epoch train time: 0:00:02.703471
elapsed time: 0:05:53.632470
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 17:53:04.225462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.23
 ---- batch: 020 ----
mean loss: 220.90
 ---- batch: 030 ----
mean loss: 219.84
 ---- batch: 040 ----
mean loss: 229.13
 ---- batch: 050 ----
mean loss: 224.02
 ---- batch: 060 ----
mean loss: 222.36
 ---- batch: 070 ----
mean loss: 215.40
 ---- batch: 080 ----
mean loss: 222.59
 ---- batch: 090 ----
mean loss: 220.33
train mean loss: 221.39
epoch train time: 0:00:02.718393
elapsed time: 0:05:56.351431
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 17:53:06.944452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.71
 ---- batch: 020 ----
mean loss: 221.02
 ---- batch: 030 ----
mean loss: 217.92
 ---- batch: 040 ----
mean loss: 221.18
 ---- batch: 050 ----
mean loss: 223.48
 ---- batch: 060 ----
mean loss: 219.39
 ---- batch: 070 ----
mean loss: 226.74
 ---- batch: 080 ----
mean loss: 228.16
 ---- batch: 090 ----
mean loss: 219.89
train mean loss: 220.97
epoch train time: 0:00:02.699328
elapsed time: 0:05:59.051281
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 17:53:09.644327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.90
 ---- batch: 020 ----
mean loss: 223.79
 ---- batch: 030 ----
mean loss: 216.07
 ---- batch: 040 ----
mean loss: 225.19
 ---- batch: 050 ----
mean loss: 223.55
 ---- batch: 060 ----
mean loss: 219.75
 ---- batch: 070 ----
mean loss: 212.78
 ---- batch: 080 ----
mean loss: 222.94
 ---- batch: 090 ----
mean loss: 216.92
train mean loss: 220.80
epoch train time: 0:00:02.728371
elapsed time: 0:06:01.780171
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 17:53:12.373165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.24
 ---- batch: 020 ----
mean loss: 213.50
 ---- batch: 030 ----
mean loss: 216.12
 ---- batch: 040 ----
mean loss: 224.63
 ---- batch: 050 ----
mean loss: 223.97
 ---- batch: 060 ----
mean loss: 220.18
 ---- batch: 070 ----
mean loss: 218.20
 ---- batch: 080 ----
mean loss: 224.50
 ---- batch: 090 ----
mean loss: 219.28
train mean loss: 220.45
epoch train time: 0:00:02.673276
elapsed time: 0:06:04.453942
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 17:53:15.046932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.84
 ---- batch: 020 ----
mean loss: 214.47
 ---- batch: 030 ----
mean loss: 216.05
 ---- batch: 040 ----
mean loss: 221.88
 ---- batch: 050 ----
mean loss: 218.96
 ---- batch: 060 ----
mean loss: 228.22
 ---- batch: 070 ----
mean loss: 219.88
 ---- batch: 080 ----
mean loss: 221.49
 ---- batch: 090 ----
mean loss: 223.36
train mean loss: 219.83
epoch train time: 0:00:02.633015
elapsed time: 0:06:07.087462
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 17:53:17.680456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.73
 ---- batch: 020 ----
mean loss: 218.51
 ---- batch: 030 ----
mean loss: 211.14
 ---- batch: 040 ----
mean loss: 214.45
 ---- batch: 050 ----
mean loss: 222.65
 ---- batch: 060 ----
mean loss: 221.60
 ---- batch: 070 ----
mean loss: 221.85
 ---- batch: 080 ----
mean loss: 226.67
 ---- batch: 090 ----
mean loss: 224.72
train mean loss: 219.62
epoch train time: 0:00:02.669966
elapsed time: 0:06:09.757940
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 17:53:20.350930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.84
 ---- batch: 020 ----
mean loss: 217.99
 ---- batch: 030 ----
mean loss: 223.17
 ---- batch: 040 ----
mean loss: 220.58
 ---- batch: 050 ----
mean loss: 218.74
 ---- batch: 060 ----
mean loss: 220.23
 ---- batch: 070 ----
mean loss: 217.93
 ---- batch: 080 ----
mean loss: 223.19
 ---- batch: 090 ----
mean loss: 223.72
train mean loss: 219.66
epoch train time: 0:00:02.682592
elapsed time: 0:06:12.441090
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 17:53:23.034033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.30
 ---- batch: 020 ----
mean loss: 216.96
 ---- batch: 030 ----
mean loss: 218.13
 ---- batch: 040 ----
mean loss: 225.77
 ---- batch: 050 ----
mean loss: 219.48
 ---- batch: 060 ----
mean loss: 219.24
 ---- batch: 070 ----
mean loss: 211.72
 ---- batch: 080 ----
mean loss: 217.91
 ---- batch: 090 ----
mean loss: 229.07
train mean loss: 219.38
epoch train time: 0:00:02.672061
elapsed time: 0:06:15.113583
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 17:53:25.706584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.89
 ---- batch: 020 ----
mean loss: 209.58
 ---- batch: 030 ----
mean loss: 215.77
 ---- batch: 040 ----
mean loss: 214.62
 ---- batch: 050 ----
mean loss: 215.91
 ---- batch: 060 ----
mean loss: 217.86
 ---- batch: 070 ----
mean loss: 222.06
 ---- batch: 080 ----
mean loss: 225.21
 ---- batch: 090 ----
mean loss: 227.93
train mean loss: 218.87
epoch train time: 0:00:02.628339
elapsed time: 0:06:17.742441
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 17:53:28.335436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.90
 ---- batch: 020 ----
mean loss: 216.03
 ---- batch: 030 ----
mean loss: 210.55
 ---- batch: 040 ----
mean loss: 218.13
 ---- batch: 050 ----
mean loss: 215.16
 ---- batch: 060 ----
mean loss: 216.13
 ---- batch: 070 ----
mean loss: 218.91
 ---- batch: 080 ----
mean loss: 226.83
 ---- batch: 090 ----
mean loss: 223.24
train mean loss: 218.89
epoch train time: 0:00:02.643839
elapsed time: 0:06:20.386775
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 17:53:30.979788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.22
 ---- batch: 020 ----
mean loss: 218.24
 ---- batch: 030 ----
mean loss: 218.01
 ---- batch: 040 ----
mean loss: 224.20
 ---- batch: 050 ----
mean loss: 219.45
 ---- batch: 060 ----
mean loss: 215.85
 ---- batch: 070 ----
mean loss: 215.24
 ---- batch: 080 ----
mean loss: 217.51
 ---- batch: 090 ----
mean loss: 221.56
train mean loss: 218.45
epoch train time: 0:00:02.646633
elapsed time: 0:06:23.033905
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 17:53:33.626899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.49
 ---- batch: 020 ----
mean loss: 212.28
 ---- batch: 030 ----
mean loss: 216.58
 ---- batch: 040 ----
mean loss: 217.20
 ---- batch: 050 ----
mean loss: 218.78
 ---- batch: 060 ----
mean loss: 214.43
 ---- batch: 070 ----
mean loss: 217.76
 ---- batch: 080 ----
mean loss: 227.05
 ---- batch: 090 ----
mean loss: 218.64
train mean loss: 218.33
epoch train time: 0:00:02.659233
elapsed time: 0:06:25.693625
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 17:53:36.286617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.74
 ---- batch: 020 ----
mean loss: 218.27
 ---- batch: 030 ----
mean loss: 217.13
 ---- batch: 040 ----
mean loss: 213.06
 ---- batch: 050 ----
mean loss: 216.92
 ---- batch: 060 ----
mean loss: 220.72
 ---- batch: 070 ----
mean loss: 217.99
 ---- batch: 080 ----
mean loss: 219.38
 ---- batch: 090 ----
mean loss: 215.92
train mean loss: 218.02
epoch train time: 0:00:02.672113
elapsed time: 0:06:28.366228
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 17:53:38.959214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.19
 ---- batch: 020 ----
mean loss: 208.94
 ---- batch: 030 ----
mean loss: 213.12
 ---- batch: 040 ----
mean loss: 220.29
 ---- batch: 050 ----
mean loss: 222.16
 ---- batch: 060 ----
mean loss: 211.47
 ---- batch: 070 ----
mean loss: 222.94
 ---- batch: 080 ----
mean loss: 221.59
 ---- batch: 090 ----
mean loss: 217.64
train mean loss: 218.00
epoch train time: 0:00:02.647568
elapsed time: 0:06:31.014258
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 17:53:41.607248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.37
 ---- batch: 020 ----
mean loss: 217.13
 ---- batch: 030 ----
mean loss: 219.42
 ---- batch: 040 ----
mean loss: 216.11
 ---- batch: 050 ----
mean loss: 214.77
 ---- batch: 060 ----
mean loss: 224.33
 ---- batch: 070 ----
mean loss: 218.01
 ---- batch: 080 ----
mean loss: 217.92
 ---- batch: 090 ----
mean loss: 213.67
train mean loss: 217.97
epoch train time: 0:00:02.661608
elapsed time: 0:06:33.676330
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 17:53:44.269318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.70
 ---- batch: 020 ----
mean loss: 215.25
 ---- batch: 030 ----
mean loss: 213.97
 ---- batch: 040 ----
mean loss: 217.50
 ---- batch: 050 ----
mean loss: 213.25
 ---- batch: 060 ----
mean loss: 215.94
 ---- batch: 070 ----
mean loss: 214.67
 ---- batch: 080 ----
mean loss: 220.56
 ---- batch: 090 ----
mean loss: 221.81
train mean loss: 217.22
epoch train time: 0:00:02.646085
elapsed time: 0:06:36.322912
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 17:53:46.915906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.35
 ---- batch: 020 ----
mean loss: 216.15
 ---- batch: 030 ----
mean loss: 220.23
 ---- batch: 040 ----
mean loss: 213.54
 ---- batch: 050 ----
mean loss: 216.04
 ---- batch: 060 ----
mean loss: 213.51
 ---- batch: 070 ----
mean loss: 212.11
 ---- batch: 080 ----
mean loss: 218.08
 ---- batch: 090 ----
mean loss: 220.49
train mean loss: 217.00
epoch train time: 0:00:02.644168
elapsed time: 0:06:38.967561
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 17:53:49.560574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.65
 ---- batch: 020 ----
mean loss: 217.02
 ---- batch: 030 ----
mean loss: 214.68
 ---- batch: 040 ----
mean loss: 221.48
 ---- batch: 050 ----
mean loss: 216.81
 ---- batch: 060 ----
mean loss: 220.38
 ---- batch: 070 ----
mean loss: 219.84
 ---- batch: 080 ----
mean loss: 219.17
 ---- batch: 090 ----
mean loss: 212.73
train mean loss: 216.87
epoch train time: 0:00:02.662426
elapsed time: 0:06:41.630485
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 17:53:52.223492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.17
 ---- batch: 020 ----
mean loss: 215.26
 ---- batch: 030 ----
mean loss: 222.54
 ---- batch: 040 ----
mean loss: 212.52
 ---- batch: 050 ----
mean loss: 222.08
 ---- batch: 060 ----
mean loss: 210.97
 ---- batch: 070 ----
mean loss: 214.73
 ---- batch: 080 ----
mean loss: 218.53
 ---- batch: 090 ----
mean loss: 223.68
train mean loss: 216.92
epoch train time: 0:00:02.664562
elapsed time: 0:06:44.295646
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 17:53:54.888644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.49
 ---- batch: 020 ----
mean loss: 213.40
 ---- batch: 030 ----
mean loss: 221.30
 ---- batch: 040 ----
mean loss: 210.89
 ---- batch: 050 ----
mean loss: 213.73
 ---- batch: 060 ----
mean loss: 216.75
 ---- batch: 070 ----
mean loss: 217.46
 ---- batch: 080 ----
mean loss: 224.75
 ---- batch: 090 ----
mean loss: 211.50
train mean loss: 215.97
epoch train time: 0:00:02.646246
elapsed time: 0:06:46.942361
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 17:53:57.535359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.84
 ---- batch: 020 ----
mean loss: 214.97
 ---- batch: 030 ----
mean loss: 215.77
 ---- batch: 040 ----
mean loss: 218.97
 ---- batch: 050 ----
mean loss: 219.90
 ---- batch: 060 ----
mean loss: 215.57
 ---- batch: 070 ----
mean loss: 212.12
 ---- batch: 080 ----
mean loss: 213.02
 ---- batch: 090 ----
mean loss: 221.70
train mean loss: 216.65
epoch train time: 0:00:02.627912
elapsed time: 0:06:49.570745
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 17:54:00.163735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.47
 ---- batch: 020 ----
mean loss: 217.85
 ---- batch: 030 ----
mean loss: 220.01
 ---- batch: 040 ----
mean loss: 206.59
 ---- batch: 050 ----
mean loss: 220.95
 ---- batch: 060 ----
mean loss: 214.73
 ---- batch: 070 ----
mean loss: 217.46
 ---- batch: 080 ----
mean loss: 213.76
 ---- batch: 090 ----
mean loss: 216.82
train mean loss: 216.19
epoch train time: 0:00:02.653952
elapsed time: 0:06:52.225192
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 17:54:02.818183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.83
 ---- batch: 020 ----
mean loss: 212.85
 ---- batch: 030 ----
mean loss: 214.77
 ---- batch: 040 ----
mean loss: 217.05
 ---- batch: 050 ----
mean loss: 222.78
 ---- batch: 060 ----
mean loss: 220.53
 ---- batch: 070 ----
mean loss: 213.33
 ---- batch: 080 ----
mean loss: 216.62
 ---- batch: 090 ----
mean loss: 212.70
train mean loss: 215.88
epoch train time: 0:00:02.666489
elapsed time: 0:06:54.892157
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 17:54:05.485152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.11
 ---- batch: 020 ----
mean loss: 220.04
 ---- batch: 030 ----
mean loss: 207.07
 ---- batch: 040 ----
mean loss: 219.64
 ---- batch: 050 ----
mean loss: 212.92
 ---- batch: 060 ----
mean loss: 209.25
 ---- batch: 070 ----
mean loss: 216.74
 ---- batch: 080 ----
mean loss: 212.33
 ---- batch: 090 ----
mean loss: 211.18
train mean loss: 215.54
epoch train time: 0:00:02.669841
elapsed time: 0:06:57.562517
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 17:54:08.155511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.69
 ---- batch: 020 ----
mean loss: 220.31
 ---- batch: 030 ----
mean loss: 217.39
 ---- batch: 040 ----
mean loss: 217.14
 ---- batch: 050 ----
mean loss: 205.66
 ---- batch: 060 ----
mean loss: 222.56
 ---- batch: 070 ----
mean loss: 218.69
 ---- batch: 080 ----
mean loss: 212.09
 ---- batch: 090 ----
mean loss: 218.29
train mean loss: 215.21
epoch train time: 0:00:02.681311
elapsed time: 0:07:00.244420
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 17:54:10.837507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.37
 ---- batch: 020 ----
mean loss: 215.45
 ---- batch: 030 ----
mean loss: 212.51
 ---- batch: 040 ----
mean loss: 218.38
 ---- batch: 050 ----
mean loss: 218.25
 ---- batch: 060 ----
mean loss: 220.13
 ---- batch: 070 ----
mean loss: 209.52
 ---- batch: 080 ----
mean loss: 220.33
 ---- batch: 090 ----
mean loss: 215.19
train mean loss: 215.10
epoch train time: 0:00:02.645744
elapsed time: 0:07:02.890774
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 17:54:13.483823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.03
 ---- batch: 020 ----
mean loss: 217.10
 ---- batch: 030 ----
mean loss: 216.70
 ---- batch: 040 ----
mean loss: 215.53
 ---- batch: 050 ----
mean loss: 216.16
 ---- batch: 060 ----
mean loss: 212.75
 ---- batch: 070 ----
mean loss: 212.81
 ---- batch: 080 ----
mean loss: 210.76
 ---- batch: 090 ----
mean loss: 214.06
train mean loss: 215.15
epoch train time: 0:00:02.638347
elapsed time: 0:07:05.529663
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 17:54:16.122653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.67
 ---- batch: 020 ----
mean loss: 220.14
 ---- batch: 030 ----
mean loss: 212.65
 ---- batch: 040 ----
mean loss: 212.01
 ---- batch: 050 ----
mean loss: 213.48
 ---- batch: 060 ----
mean loss: 216.17
 ---- batch: 070 ----
mean loss: 206.81
 ---- batch: 080 ----
mean loss: 220.50
 ---- batch: 090 ----
mean loss: 219.92
train mean loss: 214.88
epoch train time: 0:00:02.660340
elapsed time: 0:07:08.190455
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 17:54:18.783482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.63
 ---- batch: 020 ----
mean loss: 211.26
 ---- batch: 030 ----
mean loss: 213.22
 ---- batch: 040 ----
mean loss: 217.59
 ---- batch: 050 ----
mean loss: 209.86
 ---- batch: 060 ----
mean loss: 211.76
 ---- batch: 070 ----
mean loss: 215.41
 ---- batch: 080 ----
mean loss: 215.33
 ---- batch: 090 ----
mean loss: 216.40
train mean loss: 214.50
epoch train time: 0:00:02.657487
elapsed time: 0:07:10.848533
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 17:54:21.441471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.67
 ---- batch: 020 ----
mean loss: 213.66
 ---- batch: 030 ----
mean loss: 213.34
 ---- batch: 040 ----
mean loss: 215.58
 ---- batch: 050 ----
mean loss: 218.33
 ---- batch: 060 ----
mean loss: 220.94
 ---- batch: 070 ----
mean loss: 214.90
 ---- batch: 080 ----
mean loss: 208.90
 ---- batch: 090 ----
mean loss: 218.15
train mean loss: 214.64
epoch train time: 0:00:02.610058
elapsed time: 0:07:13.459009
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 17:54:24.052012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.77
 ---- batch: 020 ----
mean loss: 220.86
 ---- batch: 030 ----
mean loss: 208.29
 ---- batch: 040 ----
mean loss: 220.34
 ---- batch: 050 ----
mean loss: 208.07
 ---- batch: 060 ----
mean loss: 213.84
 ---- batch: 070 ----
mean loss: 207.79
 ---- batch: 080 ----
mean loss: 218.23
 ---- batch: 090 ----
mean loss: 212.43
train mean loss: 214.15
epoch train time: 0:00:02.700061
elapsed time: 0:07:16.159561
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 17:54:26.752579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.70
 ---- batch: 020 ----
mean loss: 217.89
 ---- batch: 030 ----
mean loss: 213.96
 ---- batch: 040 ----
mean loss: 210.68
 ---- batch: 050 ----
mean loss: 212.76
 ---- batch: 060 ----
mean loss: 217.08
 ---- batch: 070 ----
mean loss: 210.04
 ---- batch: 080 ----
mean loss: 214.50
 ---- batch: 090 ----
mean loss: 222.92
train mean loss: 214.04
epoch train time: 0:00:02.673773
elapsed time: 0:07:18.833891
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 17:54:29.426935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.64
 ---- batch: 020 ----
mean loss: 221.54
 ---- batch: 030 ----
mean loss: 214.39
 ---- batch: 040 ----
mean loss: 208.36
 ---- batch: 050 ----
mean loss: 215.41
 ---- batch: 060 ----
mean loss: 213.38
 ---- batch: 070 ----
mean loss: 214.68
 ---- batch: 080 ----
mean loss: 213.79
 ---- batch: 090 ----
mean loss: 215.87
train mean loss: 213.56
epoch train time: 0:00:02.671662
elapsed time: 0:07:21.506077
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 17:54:32.099074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.71
 ---- batch: 020 ----
mean loss: 207.27
 ---- batch: 030 ----
mean loss: 218.97
 ---- batch: 040 ----
mean loss: 221.69
 ---- batch: 050 ----
mean loss: 213.92
 ---- batch: 060 ----
mean loss: 210.84
 ---- batch: 070 ----
mean loss: 210.44
 ---- batch: 080 ----
mean loss: 211.66
 ---- batch: 090 ----
mean loss: 209.58
train mean loss: 213.93
epoch train time: 0:00:02.656191
elapsed time: 0:07:24.162765
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 17:54:34.755760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.63
 ---- batch: 020 ----
mean loss: 211.82
 ---- batch: 030 ----
mean loss: 208.43
 ---- batch: 040 ----
mean loss: 216.85
 ---- batch: 050 ----
mean loss: 214.93
 ---- batch: 060 ----
mean loss: 217.93
 ---- batch: 070 ----
mean loss: 209.39
 ---- batch: 080 ----
mean loss: 215.99
 ---- batch: 090 ----
mean loss: 210.79
train mean loss: 213.41
epoch train time: 0:00:02.640883
elapsed time: 0:07:26.804183
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 17:54:37.397187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.09
 ---- batch: 020 ----
mean loss: 219.24
 ---- batch: 030 ----
mean loss: 219.74
 ---- batch: 040 ----
mean loss: 206.11
 ---- batch: 050 ----
mean loss: 212.95
 ---- batch: 060 ----
mean loss: 219.83
 ---- batch: 070 ----
mean loss: 211.98
 ---- batch: 080 ----
mean loss: 208.88
 ---- batch: 090 ----
mean loss: 213.13
train mean loss: 213.60
epoch train time: 0:00:02.637935
elapsed time: 0:07:29.442626
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 17:54:40.035626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.92
 ---- batch: 020 ----
mean loss: 207.55
 ---- batch: 030 ----
mean loss: 216.15
 ---- batch: 040 ----
mean loss: 217.20
 ---- batch: 050 ----
mean loss: 206.55
 ---- batch: 060 ----
mean loss: 212.89
 ---- batch: 070 ----
mean loss: 218.92
 ---- batch: 080 ----
mean loss: 224.19
 ---- batch: 090 ----
mean loss: 209.50
train mean loss: 213.20
epoch train time: 0:00:02.662842
elapsed time: 0:07:32.105944
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 17:54:42.698939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.87
 ---- batch: 020 ----
mean loss: 217.90
 ---- batch: 030 ----
mean loss: 211.86
 ---- batch: 040 ----
mean loss: 212.33
 ---- batch: 050 ----
mean loss: 220.68
 ---- batch: 060 ----
mean loss: 213.27
 ---- batch: 070 ----
mean loss: 211.16
 ---- batch: 080 ----
mean loss: 208.53
 ---- batch: 090 ----
mean loss: 211.57
train mean loss: 212.92
epoch train time: 0:00:02.654042
elapsed time: 0:07:34.760478
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 17:54:45.353473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.89
 ---- batch: 020 ----
mean loss: 208.38
 ---- batch: 030 ----
mean loss: 215.80
 ---- batch: 040 ----
mean loss: 208.17
 ---- batch: 050 ----
mean loss: 206.89
 ---- batch: 060 ----
mean loss: 208.83
 ---- batch: 070 ----
mean loss: 213.25
 ---- batch: 080 ----
mean loss: 219.71
 ---- batch: 090 ----
mean loss: 208.40
train mean loss: 213.14
epoch train time: 0:00:02.682243
elapsed time: 0:07:37.443281
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 17:54:48.036287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.23
 ---- batch: 020 ----
mean loss: 204.42
 ---- batch: 030 ----
mean loss: 214.69
 ---- batch: 040 ----
mean loss: 220.00
 ---- batch: 050 ----
mean loss: 217.77
 ---- batch: 060 ----
mean loss: 211.38
 ---- batch: 070 ----
mean loss: 214.50
 ---- batch: 080 ----
mean loss: 208.39
 ---- batch: 090 ----
mean loss: 213.11
train mean loss: 212.56
epoch train time: 0:00:02.640507
elapsed time: 0:07:40.084253
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 17:54:50.677267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.72
 ---- batch: 020 ----
mean loss: 211.56
 ---- batch: 030 ----
mean loss: 209.03
 ---- batch: 040 ----
mean loss: 215.11
 ---- batch: 050 ----
mean loss: 211.39
 ---- batch: 060 ----
mean loss: 211.90
 ---- batch: 070 ----
mean loss: 219.26
 ---- batch: 080 ----
mean loss: 215.05
 ---- batch: 090 ----
mean loss: 210.41
train mean loss: 212.56
epoch train time: 0:00:02.657623
elapsed time: 0:07:42.742376
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 17:54:53.335380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.18
 ---- batch: 020 ----
mean loss: 207.95
 ---- batch: 030 ----
mean loss: 215.64
 ---- batch: 040 ----
mean loss: 207.47
 ---- batch: 050 ----
mean loss: 205.57
 ---- batch: 060 ----
mean loss: 216.98
 ---- batch: 070 ----
mean loss: 219.18
 ---- batch: 080 ----
mean loss: 210.91
 ---- batch: 090 ----
mean loss: 213.45
train mean loss: 212.26
epoch train time: 0:00:02.645335
elapsed time: 0:07:45.388213
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 17:54:55.981210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.17
 ---- batch: 020 ----
mean loss: 217.11
 ---- batch: 030 ----
mean loss: 220.48
 ---- batch: 040 ----
mean loss: 215.77
 ---- batch: 050 ----
mean loss: 205.51
 ---- batch: 060 ----
mean loss: 213.95
 ---- batch: 070 ----
mean loss: 210.69
 ---- batch: 080 ----
mean loss: 212.55
 ---- batch: 090 ----
mean loss: 207.25
train mean loss: 212.18
epoch train time: 0:00:02.669014
elapsed time: 0:07:48.057706
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 17:54:58.650717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.28
 ---- batch: 020 ----
mean loss: 206.66
 ---- batch: 030 ----
mean loss: 212.22
 ---- batch: 040 ----
mean loss: 214.04
 ---- batch: 050 ----
mean loss: 213.73
 ---- batch: 060 ----
mean loss: 215.83
 ---- batch: 070 ----
mean loss: 215.04
 ---- batch: 080 ----
mean loss: 206.97
 ---- batch: 090 ----
mean loss: 212.09
train mean loss: 212.27
epoch train time: 0:00:02.656824
elapsed time: 0:07:50.715027
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 17:55:01.308047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.46
 ---- batch: 020 ----
mean loss: 208.74
 ---- batch: 030 ----
mean loss: 213.20
 ---- batch: 040 ----
mean loss: 204.84
 ---- batch: 050 ----
mean loss: 221.67
 ---- batch: 060 ----
mean loss: 216.21
 ---- batch: 070 ----
mean loss: 213.62
 ---- batch: 080 ----
mean loss: 205.31
 ---- batch: 090 ----
mean loss: 215.83
train mean loss: 211.92
epoch train time: 0:00:02.684272
elapsed time: 0:07:53.399855
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 17:55:03.992849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.35
 ---- batch: 020 ----
mean loss: 210.71
 ---- batch: 030 ----
mean loss: 206.71
 ---- batch: 040 ----
mean loss: 219.48
 ---- batch: 050 ----
mean loss: 212.22
 ---- batch: 060 ----
mean loss: 220.40
 ---- batch: 070 ----
mean loss: 204.53
 ---- batch: 080 ----
mean loss: 206.03
 ---- batch: 090 ----
mean loss: 214.83
train mean loss: 211.89
epoch train time: 0:00:02.633225
elapsed time: 0:07:56.033540
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 17:55:06.626547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.23
 ---- batch: 020 ----
mean loss: 210.89
 ---- batch: 030 ----
mean loss: 213.55
 ---- batch: 040 ----
mean loss: 209.54
 ---- batch: 050 ----
mean loss: 214.88
 ---- batch: 060 ----
mean loss: 204.67
 ---- batch: 070 ----
mean loss: 202.27
 ---- batch: 080 ----
mean loss: 211.55
 ---- batch: 090 ----
mean loss: 217.95
train mean loss: 211.76
epoch train time: 0:00:02.637065
elapsed time: 0:07:58.671119
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 17:55:09.264128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.06
 ---- batch: 020 ----
mean loss: 208.67
 ---- batch: 030 ----
mean loss: 215.60
 ---- batch: 040 ----
mean loss: 202.72
 ---- batch: 050 ----
mean loss: 211.94
 ---- batch: 060 ----
mean loss: 207.38
 ---- batch: 070 ----
mean loss: 214.54
 ---- batch: 080 ----
mean loss: 210.93
 ---- batch: 090 ----
mean loss: 212.07
train mean loss: 211.53
epoch train time: 0:00:02.661430
elapsed time: 0:08:01.333098
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 17:55:11.926126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.49
 ---- batch: 020 ----
mean loss: 212.73
 ---- batch: 030 ----
mean loss: 206.66
 ---- batch: 040 ----
mean loss: 210.42
 ---- batch: 050 ----
mean loss: 211.83
 ---- batch: 060 ----
mean loss: 217.43
 ---- batch: 070 ----
mean loss: 205.86
 ---- batch: 080 ----
mean loss: 214.69
 ---- batch: 090 ----
mean loss: 210.46
train mean loss: 211.51
epoch train time: 0:00:02.669126
elapsed time: 0:08:04.002748
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 17:55:14.595751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.82
 ---- batch: 020 ----
mean loss: 206.25
 ---- batch: 030 ----
mean loss: 208.20
 ---- batch: 040 ----
mean loss: 215.91
 ---- batch: 050 ----
mean loss: 205.81
 ---- batch: 060 ----
mean loss: 214.65
 ---- batch: 070 ----
mean loss: 217.76
 ---- batch: 080 ----
mean loss: 210.92
 ---- batch: 090 ----
mean loss: 210.77
train mean loss: 211.20
epoch train time: 0:00:02.689469
elapsed time: 0:08:06.692701
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 17:55:17.285747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.72
 ---- batch: 020 ----
mean loss: 212.00
 ---- batch: 030 ----
mean loss: 209.77
 ---- batch: 040 ----
mean loss: 215.57
 ---- batch: 050 ----
mean loss: 215.91
 ---- batch: 060 ----
mean loss: 211.63
 ---- batch: 070 ----
mean loss: 212.89
 ---- batch: 080 ----
mean loss: 200.40
 ---- batch: 090 ----
mean loss: 207.73
train mean loss: 210.81
epoch train time: 0:00:02.638643
elapsed time: 0:08:09.331891
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 17:55:19.924879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.65
 ---- batch: 020 ----
mean loss: 208.85
 ---- batch: 030 ----
mean loss: 202.54
 ---- batch: 040 ----
mean loss: 211.58
 ---- batch: 050 ----
mean loss: 208.08
 ---- batch: 060 ----
mean loss: 214.90
 ---- batch: 070 ----
mean loss: 219.24
 ---- batch: 080 ----
mean loss: 207.13
 ---- batch: 090 ----
mean loss: 216.98
train mean loss: 210.95
epoch train time: 0:00:02.675868
elapsed time: 0:08:12.008315
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 17:55:22.601329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.72
 ---- batch: 020 ----
mean loss: 203.26
 ---- batch: 030 ----
mean loss: 213.23
 ---- batch: 040 ----
mean loss: 214.96
 ---- batch: 050 ----
mean loss: 210.05
 ---- batch: 060 ----
mean loss: 205.02
 ---- batch: 070 ----
mean loss: 212.49
 ---- batch: 080 ----
mean loss: 212.76
 ---- batch: 090 ----
mean loss: 207.67
train mean loss: 210.79
epoch train time: 0:00:02.690740
elapsed time: 0:08:14.699553
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 17:55:25.292580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.51
 ---- batch: 020 ----
mean loss: 205.43
 ---- batch: 030 ----
mean loss: 219.02
 ---- batch: 040 ----
mean loss: 204.63
 ---- batch: 050 ----
mean loss: 204.92
 ---- batch: 060 ----
mean loss: 217.61
 ---- batch: 070 ----
mean loss: 214.29
 ---- batch: 080 ----
mean loss: 216.54
 ---- batch: 090 ----
mean loss: 207.52
train mean loss: 210.58
epoch train time: 0:00:02.671754
elapsed time: 0:08:17.371965
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 17:55:27.964886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.95
 ---- batch: 020 ----
mean loss: 213.68
 ---- batch: 030 ----
mean loss: 221.82
 ---- batch: 040 ----
mean loss: 203.79
 ---- batch: 050 ----
mean loss: 208.56
 ---- batch: 060 ----
mean loss: 215.13
 ---- batch: 070 ----
mean loss: 202.39
 ---- batch: 080 ----
mean loss: 208.54
 ---- batch: 090 ----
mean loss: 211.59
train mean loss: 210.39
epoch train time: 0:00:02.653669
elapsed time: 0:08:20.026007
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 17:55:30.619021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.95
 ---- batch: 020 ----
mean loss: 201.92
 ---- batch: 030 ----
mean loss: 210.67
 ---- batch: 040 ----
mean loss: 214.83
 ---- batch: 050 ----
mean loss: 210.32
 ---- batch: 060 ----
mean loss: 211.32
 ---- batch: 070 ----
mean loss: 212.98
 ---- batch: 080 ----
mean loss: 204.49
 ---- batch: 090 ----
mean loss: 210.35
train mean loss: 210.10
epoch train time: 0:00:02.675665
elapsed time: 0:08:22.702173
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 17:55:33.295197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.74
 ---- batch: 020 ----
mean loss: 210.33
 ---- batch: 030 ----
mean loss: 206.98
 ---- batch: 040 ----
mean loss: 212.53
 ---- batch: 050 ----
mean loss: 213.40
 ---- batch: 060 ----
mean loss: 211.68
 ---- batch: 070 ----
mean loss: 205.86
 ---- batch: 080 ----
mean loss: 201.39
 ---- batch: 090 ----
mean loss: 215.35
train mean loss: 209.85
epoch train time: 0:00:02.710429
elapsed time: 0:08:25.413171
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 17:55:36.006176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.86
 ---- batch: 020 ----
mean loss: 213.83
 ---- batch: 030 ----
mean loss: 211.89
 ---- batch: 040 ----
mean loss: 206.25
 ---- batch: 050 ----
mean loss: 201.91
 ---- batch: 060 ----
mean loss: 213.94
 ---- batch: 070 ----
mean loss: 211.70
 ---- batch: 080 ----
mean loss: 209.88
 ---- batch: 090 ----
mean loss: 203.35
train mean loss: 209.66
epoch train time: 0:00:02.707531
elapsed time: 0:08:28.121219
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 17:55:38.714217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.18
 ---- batch: 020 ----
mean loss: 211.65
 ---- batch: 030 ----
mean loss: 211.88
 ---- batch: 040 ----
mean loss: 210.28
 ---- batch: 050 ----
mean loss: 205.26
 ---- batch: 060 ----
mean loss: 213.15
 ---- batch: 070 ----
mean loss: 206.82
 ---- batch: 080 ----
mean loss: 204.45
 ---- batch: 090 ----
mean loss: 209.57
train mean loss: 209.95
epoch train time: 0:00:02.636566
elapsed time: 0:08:30.758235
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 17:55:41.351228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.40
 ---- batch: 020 ----
mean loss: 210.68
 ---- batch: 030 ----
mean loss: 215.59
 ---- batch: 040 ----
mean loss: 209.78
 ---- batch: 050 ----
mean loss: 213.75
 ---- batch: 060 ----
mean loss: 207.45
 ---- batch: 070 ----
mean loss: 199.69
 ---- batch: 080 ----
mean loss: 208.27
 ---- batch: 090 ----
mean loss: 209.22
train mean loss: 209.86
epoch train time: 0:00:02.683775
elapsed time: 0:08:33.442511
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 17:55:44.035506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.17
 ---- batch: 020 ----
mean loss: 209.87
 ---- batch: 030 ----
mean loss: 210.79
 ---- batch: 040 ----
mean loss: 219.32
 ---- batch: 050 ----
mean loss: 209.81
 ---- batch: 060 ----
mean loss: 205.03
 ---- batch: 070 ----
mean loss: 204.88
 ---- batch: 080 ----
mean loss: 210.06
 ---- batch: 090 ----
mean loss: 205.50
train mean loss: 209.78
epoch train time: 0:00:02.685914
elapsed time: 0:08:36.128928
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 17:55:46.721938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.25
 ---- batch: 020 ----
mean loss: 212.22
 ---- batch: 030 ----
mean loss: 209.36
 ---- batch: 040 ----
mean loss: 210.20
 ---- batch: 050 ----
mean loss: 206.32
 ---- batch: 060 ----
mean loss: 206.61
 ---- batch: 070 ----
mean loss: 211.52
 ---- batch: 080 ----
mean loss: 211.17
 ---- batch: 090 ----
mean loss: 208.07
train mean loss: 209.51
epoch train time: 0:00:02.668172
elapsed time: 0:08:38.797611
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 17:55:49.390600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.82
 ---- batch: 020 ----
mean loss: 207.43
 ---- batch: 030 ----
mean loss: 215.21
 ---- batch: 040 ----
mean loss: 210.08
 ---- batch: 050 ----
mean loss: 206.46
 ---- batch: 060 ----
mean loss: 213.06
 ---- batch: 070 ----
mean loss: 206.48
 ---- batch: 080 ----
mean loss: 210.63
 ---- batch: 090 ----
mean loss: 198.41
train mean loss: 209.05
epoch train time: 0:00:02.730242
elapsed time: 0:08:41.528332
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 17:55:52.121354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.08
 ---- batch: 020 ----
mean loss: 207.88
 ---- batch: 030 ----
mean loss: 203.95
 ---- batch: 040 ----
mean loss: 214.93
 ---- batch: 050 ----
mean loss: 213.00
 ---- batch: 060 ----
mean loss: 206.98
 ---- batch: 070 ----
mean loss: 211.20
 ---- batch: 080 ----
mean loss: 210.29
 ---- batch: 090 ----
mean loss: 206.37
train mean loss: 209.21
epoch train time: 0:00:02.713520
elapsed time: 0:08:44.242398
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 17:55:54.835396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.44
 ---- batch: 020 ----
mean loss: 208.01
 ---- batch: 030 ----
mean loss: 208.51
 ---- batch: 040 ----
mean loss: 210.49
 ---- batch: 050 ----
mean loss: 214.55
 ---- batch: 060 ----
mean loss: 213.99
 ---- batch: 070 ----
mean loss: 207.30
 ---- batch: 080 ----
mean loss: 202.27
 ---- batch: 090 ----
mean loss: 207.72
train mean loss: 209.41
epoch train time: 0:00:02.690392
elapsed time: 0:08:46.933346
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 17:55:57.526359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.53
 ---- batch: 020 ----
mean loss: 204.07
 ---- batch: 030 ----
mean loss: 204.64
 ---- batch: 040 ----
mean loss: 213.49
 ---- batch: 050 ----
mean loss: 219.78
 ---- batch: 060 ----
mean loss: 208.83
 ---- batch: 070 ----
mean loss: 205.64
 ---- batch: 080 ----
mean loss: 205.10
 ---- batch: 090 ----
mean loss: 212.45
train mean loss: 208.96
epoch train time: 0:00:02.648818
elapsed time: 0:08:49.582688
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 17:56:00.175680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.34
 ---- batch: 020 ----
mean loss: 203.70
 ---- batch: 030 ----
mean loss: 213.16
 ---- batch: 040 ----
mean loss: 209.39
 ---- batch: 050 ----
mean loss: 205.37
 ---- batch: 060 ----
mean loss: 212.63
 ---- batch: 070 ----
mean loss: 211.46
 ---- batch: 080 ----
mean loss: 205.55
 ---- batch: 090 ----
mean loss: 212.77
train mean loss: 208.93
epoch train time: 0:00:02.671760
elapsed time: 0:08:52.254964
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 17:56:02.847973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.59
 ---- batch: 020 ----
mean loss: 210.92
 ---- batch: 030 ----
mean loss: 206.02
 ---- batch: 040 ----
mean loss: 203.26
 ---- batch: 050 ----
mean loss: 210.74
 ---- batch: 060 ----
mean loss: 207.80
 ---- batch: 070 ----
mean loss: 208.34
 ---- batch: 080 ----
mean loss: 209.06
 ---- batch: 090 ----
mean loss: 211.28
train mean loss: 208.39
epoch train time: 0:00:02.696632
elapsed time: 0:08:54.952197
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 17:56:05.545186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.68
 ---- batch: 020 ----
mean loss: 213.23
 ---- batch: 030 ----
mean loss: 202.10
 ---- batch: 040 ----
mean loss: 206.15
 ---- batch: 050 ----
mean loss: 211.67
 ---- batch: 060 ----
mean loss: 206.47
 ---- batch: 070 ----
mean loss: 209.84
 ---- batch: 080 ----
mean loss: 209.47
 ---- batch: 090 ----
mean loss: 210.17
train mean loss: 208.60
epoch train time: 0:00:02.683890
elapsed time: 0:08:57.636621
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 17:56:08.229639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.71
 ---- batch: 020 ----
mean loss: 210.08
 ---- batch: 030 ----
mean loss: 217.97
 ---- batch: 040 ----
mean loss: 202.28
 ---- batch: 050 ----
mean loss: 207.41
 ---- batch: 060 ----
mean loss: 199.46
 ---- batch: 070 ----
mean loss: 200.34
 ---- batch: 080 ----
mean loss: 216.94
 ---- batch: 090 ----
mean loss: 212.95
train mean loss: 208.29
epoch train time: 0:00:02.700531
elapsed time: 0:09:00.337656
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 17:56:10.930672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.66
 ---- batch: 020 ----
mean loss: 205.87
 ---- batch: 030 ----
mean loss: 214.07
 ---- batch: 040 ----
mean loss: 208.39
 ---- batch: 050 ----
mean loss: 212.19
 ---- batch: 060 ----
mean loss: 209.32
 ---- batch: 070 ----
mean loss: 210.59
 ---- batch: 080 ----
mean loss: 214.82
 ---- batch: 090 ----
mean loss: 199.47
train mean loss: 208.41
epoch train time: 0:00:02.688569
elapsed time: 0:09:03.026702
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 17:56:13.619699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.45
 ---- batch: 020 ----
mean loss: 209.98
 ---- batch: 030 ----
mean loss: 209.36
 ---- batch: 040 ----
mean loss: 201.97
 ---- batch: 050 ----
mean loss: 210.40
 ---- batch: 060 ----
mean loss: 208.47
 ---- batch: 070 ----
mean loss: 211.85
 ---- batch: 080 ----
mean loss: 207.71
 ---- batch: 090 ----
mean loss: 204.42
train mean loss: 207.92
epoch train time: 0:00:02.661598
elapsed time: 0:09:05.688761
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 17:56:16.281820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.99
 ---- batch: 020 ----
mean loss: 208.66
 ---- batch: 030 ----
mean loss: 204.99
 ---- batch: 040 ----
mean loss: 209.85
 ---- batch: 050 ----
mean loss: 207.68
 ---- batch: 060 ----
mean loss: 211.62
 ---- batch: 070 ----
mean loss: 202.63
 ---- batch: 080 ----
mean loss: 204.20
 ---- batch: 090 ----
mean loss: 207.26
train mean loss: 207.59
epoch train time: 0:00:02.654254
elapsed time: 0:09:08.343548
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 17:56:18.936549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.59
 ---- batch: 020 ----
mean loss: 203.44
 ---- batch: 030 ----
mean loss: 211.92
 ---- batch: 040 ----
mean loss: 206.76
 ---- batch: 050 ----
mean loss: 203.06
 ---- batch: 060 ----
mean loss: 198.57
 ---- batch: 070 ----
mean loss: 213.30
 ---- batch: 080 ----
mean loss: 210.36
 ---- batch: 090 ----
mean loss: 213.58
train mean loss: 208.00
epoch train time: 0:00:02.706435
elapsed time: 0:09:11.050462
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 17:56:21.643465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.84
 ---- batch: 020 ----
mean loss: 206.33
 ---- batch: 030 ----
mean loss: 206.31
 ---- batch: 040 ----
mean loss: 207.72
 ---- batch: 050 ----
mean loss: 202.88
 ---- batch: 060 ----
mean loss: 211.48
 ---- batch: 070 ----
mean loss: 211.90
 ---- batch: 080 ----
mean loss: 208.11
 ---- batch: 090 ----
mean loss: 205.99
train mean loss: 208.15
epoch train time: 0:00:02.674404
elapsed time: 0:09:13.725377
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 17:56:24.318370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.83
 ---- batch: 020 ----
mean loss: 207.89
 ---- batch: 030 ----
mean loss: 214.96
 ---- batch: 040 ----
mean loss: 204.52
 ---- batch: 050 ----
mean loss: 205.79
 ---- batch: 060 ----
mean loss: 210.09
 ---- batch: 070 ----
mean loss: 210.56
 ---- batch: 080 ----
mean loss: 207.47
 ---- batch: 090 ----
mean loss: 203.81
train mean loss: 207.96
epoch train time: 0:00:02.691912
elapsed time: 0:09:16.417801
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 17:56:27.010793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.02
 ---- batch: 020 ----
mean loss: 206.90
 ---- batch: 030 ----
mean loss: 208.03
 ---- batch: 040 ----
mean loss: 208.74
 ---- batch: 050 ----
mean loss: 205.23
 ---- batch: 060 ----
mean loss: 205.41
 ---- batch: 070 ----
mean loss: 204.64
 ---- batch: 080 ----
mean loss: 202.99
 ---- batch: 090 ----
mean loss: 213.82
train mean loss: 207.46
epoch train time: 0:00:02.663004
elapsed time: 0:09:19.081289
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 17:56:29.674284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.44
 ---- batch: 020 ----
mean loss: 210.65
 ---- batch: 030 ----
mean loss: 208.10
 ---- batch: 040 ----
mean loss: 206.38
 ---- batch: 050 ----
mean loss: 207.12
 ---- batch: 060 ----
mean loss: 208.63
 ---- batch: 070 ----
mean loss: 203.26
 ---- batch: 080 ----
mean loss: 203.94
 ---- batch: 090 ----
mean loss: 214.67
train mean loss: 207.31
epoch train time: 0:00:02.652105
elapsed time: 0:09:21.733896
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 17:56:32.326908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.11
 ---- batch: 020 ----
mean loss: 202.64
 ---- batch: 030 ----
mean loss: 206.32
 ---- batch: 040 ----
mean loss: 212.07
 ---- batch: 050 ----
mean loss: 214.50
 ---- batch: 060 ----
mean loss: 208.51
 ---- batch: 070 ----
mean loss: 209.72
 ---- batch: 080 ----
mean loss: 209.58
 ---- batch: 090 ----
mean loss: 203.01
train mean loss: 207.46
epoch train time: 0:00:02.661199
elapsed time: 0:09:24.395693
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 17:56:34.988691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.55
 ---- batch: 020 ----
mean loss: 202.52
 ---- batch: 030 ----
mean loss: 210.06
 ---- batch: 040 ----
mean loss: 206.37
 ---- batch: 050 ----
mean loss: 208.14
 ---- batch: 060 ----
mean loss: 211.06
 ---- batch: 070 ----
mean loss: 206.57
 ---- batch: 080 ----
mean loss: 211.91
 ---- batch: 090 ----
mean loss: 204.78
train mean loss: 207.06
epoch train time: 0:00:02.671147
elapsed time: 0:09:27.067304
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 17:56:37.660298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.15
 ---- batch: 020 ----
mean loss: 207.55
 ---- batch: 030 ----
mean loss: 201.60
 ---- batch: 040 ----
mean loss: 209.62
 ---- batch: 050 ----
mean loss: 207.31
 ---- batch: 060 ----
mean loss: 201.36
 ---- batch: 070 ----
mean loss: 208.09
 ---- batch: 080 ----
mean loss: 212.52
 ---- batch: 090 ----
mean loss: 211.22
train mean loss: 207.39
epoch train time: 0:00:02.677377
elapsed time: 0:09:29.745148
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 17:56:40.338144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.08
 ---- batch: 020 ----
mean loss: 204.53
 ---- batch: 030 ----
mean loss: 203.72
 ---- batch: 040 ----
mean loss: 214.80
 ---- batch: 050 ----
mean loss: 206.31
 ---- batch: 060 ----
mean loss: 198.47
 ---- batch: 070 ----
mean loss: 207.97
 ---- batch: 080 ----
mean loss: 208.25
 ---- batch: 090 ----
mean loss: 208.38
train mean loss: 206.73
epoch train time: 0:00:02.697227
elapsed time: 0:09:32.443024
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 17:56:43.035899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.01
 ---- batch: 020 ----
mean loss: 204.87
 ---- batch: 030 ----
mean loss: 206.68
 ---- batch: 040 ----
mean loss: 200.91
 ---- batch: 050 ----
mean loss: 211.52
 ---- batch: 060 ----
mean loss: 201.31
 ---- batch: 070 ----
mean loss: 208.50
 ---- batch: 080 ----
mean loss: 208.57
 ---- batch: 090 ----
mean loss: 202.32
train mean loss: 206.79
epoch train time: 0:00:02.666470
elapsed time: 0:09:35.109877
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 17:56:45.702895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.75
 ---- batch: 020 ----
mean loss: 207.36
 ---- batch: 030 ----
mean loss: 200.60
 ---- batch: 040 ----
mean loss: 209.31
 ---- batch: 050 ----
mean loss: 210.34
 ---- batch: 060 ----
mean loss: 199.05
 ---- batch: 070 ----
mean loss: 215.12
 ---- batch: 080 ----
mean loss: 204.15
 ---- batch: 090 ----
mean loss: 206.40
train mean loss: 206.61
epoch train time: 0:00:02.706327
elapsed time: 0:09:37.816708
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 17:56:48.409731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.86
 ---- batch: 020 ----
mean loss: 208.52
 ---- batch: 030 ----
mean loss: 207.83
 ---- batch: 040 ----
mean loss: 207.96
 ---- batch: 050 ----
mean loss: 204.63
 ---- batch: 060 ----
mean loss: 209.46
 ---- batch: 070 ----
mean loss: 201.99
 ---- batch: 080 ----
mean loss: 216.83
 ---- batch: 090 ----
mean loss: 201.04
train mean loss: 206.54
epoch train time: 0:00:02.672214
elapsed time: 0:09:40.489447
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 17:56:51.082471
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.51
 ---- batch: 020 ----
mean loss: 206.49
 ---- batch: 030 ----
mean loss: 216.00
 ---- batch: 040 ----
mean loss: 195.20
 ---- batch: 050 ----
mean loss: 209.33
 ---- batch: 060 ----
mean loss: 208.29
 ---- batch: 070 ----
mean loss: 204.86
 ---- batch: 080 ----
mean loss: 215.89
 ---- batch: 090 ----
mean loss: 203.68
train mean loss: 206.43
epoch train time: 0:00:02.655445
elapsed time: 0:09:43.145387
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 17:56:53.738383
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.14
 ---- batch: 020 ----
mean loss: 204.52
 ---- batch: 030 ----
mean loss: 203.31
 ---- batch: 040 ----
mean loss: 210.59
 ---- batch: 050 ----
mean loss: 202.99
 ---- batch: 060 ----
mean loss: 205.14
 ---- batch: 070 ----
mean loss: 211.57
 ---- batch: 080 ----
mean loss: 212.75
 ---- batch: 090 ----
mean loss: 206.27
train mean loss: 206.59
epoch train time: 0:00:02.701860
elapsed time: 0:09:45.847708
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 17:56:56.440730
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.36
 ---- batch: 020 ----
mean loss: 202.21
 ---- batch: 030 ----
mean loss: 207.77
 ---- batch: 040 ----
mean loss: 214.07
 ---- batch: 050 ----
mean loss: 206.00
 ---- batch: 060 ----
mean loss: 199.51
 ---- batch: 070 ----
mean loss: 208.86
 ---- batch: 080 ----
mean loss: 207.70
 ---- batch: 090 ----
mean loss: 205.94
train mean loss: 206.62
epoch train time: 0:00:02.689520
elapsed time: 0:09:48.537730
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 17:56:59.130726
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.44
 ---- batch: 020 ----
mean loss: 208.41
 ---- batch: 030 ----
mean loss: 204.73
 ---- batch: 040 ----
mean loss: 215.90
 ---- batch: 050 ----
mean loss: 209.26
 ---- batch: 060 ----
mean loss: 203.03
 ---- batch: 070 ----
mean loss: 207.20
 ---- batch: 080 ----
mean loss: 207.80
 ---- batch: 090 ----
mean loss: 200.61
train mean loss: 206.50
epoch train time: 0:00:02.675953
elapsed time: 0:09:51.214228
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 17:57:01.807264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.61
 ---- batch: 020 ----
mean loss: 208.55
 ---- batch: 030 ----
mean loss: 204.95
 ---- batch: 040 ----
mean loss: 201.28
 ---- batch: 050 ----
mean loss: 213.50
 ---- batch: 060 ----
mean loss: 212.29
 ---- batch: 070 ----
mean loss: 206.80
 ---- batch: 080 ----
mean loss: 204.57
 ---- batch: 090 ----
mean loss: 204.11
train mean loss: 206.52
epoch train time: 0:00:02.656071
elapsed time: 0:09:53.870799
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 17:57:04.463799
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.16
 ---- batch: 020 ----
mean loss: 208.50
 ---- batch: 030 ----
mean loss: 205.17
 ---- batch: 040 ----
mean loss: 205.71
 ---- batch: 050 ----
mean loss: 201.81
 ---- batch: 060 ----
mean loss: 215.45
 ---- batch: 070 ----
mean loss: 201.97
 ---- batch: 080 ----
mean loss: 207.29
 ---- batch: 090 ----
mean loss: 205.36
train mean loss: 206.41
epoch train time: 0:00:02.647623
elapsed time: 0:09:56.518876
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 17:57:07.111867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.99
 ---- batch: 020 ----
mean loss: 204.07
 ---- batch: 030 ----
mean loss: 207.97
 ---- batch: 040 ----
mean loss: 204.02
 ---- batch: 050 ----
mean loss: 207.26
 ---- batch: 060 ----
mean loss: 205.61
 ---- batch: 070 ----
mean loss: 211.20
 ---- batch: 080 ----
mean loss: 204.13
 ---- batch: 090 ----
mean loss: 205.70
train mean loss: 206.75
epoch train time: 0:00:02.638007
elapsed time: 0:09:59.157364
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 17:57:09.750360
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.18
 ---- batch: 020 ----
mean loss: 207.19
 ---- batch: 030 ----
mean loss: 204.79
 ---- batch: 040 ----
mean loss: 209.90
 ---- batch: 050 ----
mean loss: 203.07
 ---- batch: 060 ----
mean loss: 199.67
 ---- batch: 070 ----
mean loss: 207.95
 ---- batch: 080 ----
mean loss: 207.02
 ---- batch: 090 ----
mean loss: 207.55
train mean loss: 206.49
epoch train time: 0:00:02.681457
elapsed time: 0:10:01.839291
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 17:57:12.432310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.75
 ---- batch: 020 ----
mean loss: 209.08
 ---- batch: 030 ----
mean loss: 201.56
 ---- batch: 040 ----
mean loss: 208.86
 ---- batch: 050 ----
mean loss: 212.53
 ---- batch: 060 ----
mean loss: 209.85
 ---- batch: 070 ----
mean loss: 201.74
 ---- batch: 080 ----
mean loss: 203.32
 ---- batch: 090 ----
mean loss: 202.91
train mean loss: 206.41
epoch train time: 0:00:02.689528
elapsed time: 0:10:04.529320
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 17:57:15.122321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.91
 ---- batch: 020 ----
mean loss: 194.47
 ---- batch: 030 ----
mean loss: 202.17
 ---- batch: 040 ----
mean loss: 204.49
 ---- batch: 050 ----
mean loss: 205.81
 ---- batch: 060 ----
mean loss: 213.09
 ---- batch: 070 ----
mean loss: 213.35
 ---- batch: 080 ----
mean loss: 214.28
 ---- batch: 090 ----
mean loss: 210.67
train mean loss: 206.90
epoch train time: 0:00:02.713685
elapsed time: 0:10:07.243499
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 17:57:17.836518
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.58
 ---- batch: 020 ----
mean loss: 205.62
 ---- batch: 030 ----
mean loss: 205.80
 ---- batch: 040 ----
mean loss: 205.97
 ---- batch: 050 ----
mean loss: 203.09
 ---- batch: 060 ----
mean loss: 209.78
 ---- batch: 070 ----
mean loss: 203.39
 ---- batch: 080 ----
mean loss: 209.01
 ---- batch: 090 ----
mean loss: 200.55
train mean loss: 206.46
epoch train time: 0:00:02.681080
elapsed time: 0:10:09.925161
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 17:57:20.518163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.59
 ---- batch: 020 ----
mean loss: 204.24
 ---- batch: 030 ----
mean loss: 208.99
 ---- batch: 040 ----
mean loss: 206.07
 ---- batch: 050 ----
mean loss: 203.03
 ---- batch: 060 ----
mean loss: 209.65
 ---- batch: 070 ----
mean loss: 203.85
 ---- batch: 080 ----
mean loss: 211.43
 ---- batch: 090 ----
mean loss: 203.59
train mean loss: 206.24
epoch train time: 0:00:02.658993
elapsed time: 0:10:12.584644
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 17:57:23.177664
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.88
 ---- batch: 020 ----
mean loss: 206.50
 ---- batch: 030 ----
mean loss: 207.29
 ---- batch: 040 ----
mean loss: 199.77
 ---- batch: 050 ----
mean loss: 210.38
 ---- batch: 060 ----
mean loss: 204.97
 ---- batch: 070 ----
mean loss: 202.67
 ---- batch: 080 ----
mean loss: 214.57
 ---- batch: 090 ----
mean loss: 207.55
train mean loss: 206.47
epoch train time: 0:00:02.644897
elapsed time: 0:10:15.230080
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 17:57:25.823082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.69
 ---- batch: 020 ----
mean loss: 207.08
 ---- batch: 030 ----
mean loss: 206.78
 ---- batch: 040 ----
mean loss: 209.70
 ---- batch: 050 ----
mean loss: 202.12
 ---- batch: 060 ----
mean loss: 206.64
 ---- batch: 070 ----
mean loss: 203.96
 ---- batch: 080 ----
mean loss: 211.44
 ---- batch: 090 ----
mean loss: 208.75
train mean loss: 206.54
epoch train time: 0:00:02.707085
elapsed time: 0:10:17.937629
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 17:57:28.530627
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.82
 ---- batch: 020 ----
mean loss: 203.16
 ---- batch: 030 ----
mean loss: 210.20
 ---- batch: 040 ----
mean loss: 210.32
 ---- batch: 050 ----
mean loss: 215.17
 ---- batch: 060 ----
mean loss: 197.19
 ---- batch: 070 ----
mean loss: 201.03
 ---- batch: 080 ----
mean loss: 213.63
 ---- batch: 090 ----
mean loss: 205.78
train mean loss: 206.32
epoch train time: 0:00:02.677063
elapsed time: 0:10:20.615215
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 17:57:31.208262
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.57
 ---- batch: 020 ----
mean loss: 200.84
 ---- batch: 030 ----
mean loss: 207.41
 ---- batch: 040 ----
mean loss: 205.57
 ---- batch: 050 ----
mean loss: 209.23
 ---- batch: 060 ----
mean loss: 206.86
 ---- batch: 070 ----
mean loss: 204.75
 ---- batch: 080 ----
mean loss: 214.80
 ---- batch: 090 ----
mean loss: 204.71
train mean loss: 206.48
epoch train time: 0:00:02.676347
elapsed time: 0:10:23.292099
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 17:57:33.885099
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.34
 ---- batch: 020 ----
mean loss: 204.14
 ---- batch: 030 ----
mean loss: 205.66
 ---- batch: 040 ----
mean loss: 205.43
 ---- batch: 050 ----
mean loss: 206.87
 ---- batch: 060 ----
mean loss: 212.62
 ---- batch: 070 ----
mean loss: 206.78
 ---- batch: 080 ----
mean loss: 214.27
 ---- batch: 090 ----
mean loss: 195.33
train mean loss: 206.25
epoch train time: 0:00:02.651585
elapsed time: 0:10:25.944156
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 17:57:36.537182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.18
 ---- batch: 020 ----
mean loss: 206.46
 ---- batch: 030 ----
mean loss: 203.68
 ---- batch: 040 ----
mean loss: 205.04
 ---- batch: 050 ----
mean loss: 201.59
 ---- batch: 060 ----
mean loss: 207.97
 ---- batch: 070 ----
mean loss: 206.08
 ---- batch: 080 ----
mean loss: 215.48
 ---- batch: 090 ----
mean loss: 209.57
train mean loss: 206.28
epoch train time: 0:00:02.652039
elapsed time: 0:10:28.596736
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 17:57:39.189744
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.81
 ---- batch: 020 ----
mean loss: 204.97
 ---- batch: 030 ----
mean loss: 206.61
 ---- batch: 040 ----
mean loss: 205.46
 ---- batch: 050 ----
mean loss: 208.37
 ---- batch: 060 ----
mean loss: 208.05
 ---- batch: 070 ----
mean loss: 207.48
 ---- batch: 080 ----
mean loss: 200.86
 ---- batch: 090 ----
mean loss: 214.79
train mean loss: 206.31
epoch train time: 0:00:02.634853
elapsed time: 0:10:31.232113
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 17:57:41.825113
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.81
 ---- batch: 020 ----
mean loss: 200.38
 ---- batch: 030 ----
mean loss: 206.67
 ---- batch: 040 ----
mean loss: 204.86
 ---- batch: 050 ----
mean loss: 203.83
 ---- batch: 060 ----
mean loss: 202.57
 ---- batch: 070 ----
mean loss: 206.00
 ---- batch: 080 ----
mean loss: 219.72
 ---- batch: 090 ----
mean loss: 209.25
train mean loss: 206.26
epoch train time: 0:00:02.681073
elapsed time: 0:10:33.913675
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 17:57:44.506666
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.01
 ---- batch: 020 ----
mean loss: 212.01
 ---- batch: 030 ----
mean loss: 201.19
 ---- batch: 040 ----
mean loss: 207.78
 ---- batch: 050 ----
mean loss: 209.20
 ---- batch: 060 ----
mean loss: 203.08
 ---- batch: 070 ----
mean loss: 203.73
 ---- batch: 080 ----
mean loss: 202.52
 ---- batch: 090 ----
mean loss: 208.15
train mean loss: 206.43
epoch train time: 0:00:02.668882
elapsed time: 0:10:36.583019
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 17:57:47.176095
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.70
 ---- batch: 020 ----
mean loss: 209.13
 ---- batch: 030 ----
mean loss: 208.39
 ---- batch: 040 ----
mean loss: 202.76
 ---- batch: 050 ----
mean loss: 202.62
 ---- batch: 060 ----
mean loss: 207.23
 ---- batch: 070 ----
mean loss: 201.93
 ---- batch: 080 ----
mean loss: 210.66
 ---- batch: 090 ----
mean loss: 210.86
train mean loss: 206.38
epoch train time: 0:00:02.649806
elapsed time: 0:10:39.233358
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 17:57:49.826349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.37
 ---- batch: 020 ----
mean loss: 210.23
 ---- batch: 030 ----
mean loss: 206.31
 ---- batch: 040 ----
mean loss: 209.55
 ---- batch: 050 ----
mean loss: 212.23
 ---- batch: 060 ----
mean loss: 202.19
 ---- batch: 070 ----
mean loss: 205.69
 ---- batch: 080 ----
mean loss: 209.87
 ---- batch: 090 ----
mean loss: 203.20
train mean loss: 206.28
epoch train time: 0:00:02.644898
elapsed time: 0:10:41.878784
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 17:57:52.471814
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.07
 ---- batch: 020 ----
mean loss: 196.74
 ---- batch: 030 ----
mean loss: 208.28
 ---- batch: 040 ----
mean loss: 208.94
 ---- batch: 050 ----
mean loss: 203.44
 ---- batch: 060 ----
mean loss: 207.26
 ---- batch: 070 ----
mean loss: 211.19
 ---- batch: 080 ----
mean loss: 201.38
 ---- batch: 090 ----
mean loss: 205.50
train mean loss: 206.24
epoch train time: 0:00:02.660065
elapsed time: 0:10:44.539335
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 17:57:55.132327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.88
 ---- batch: 020 ----
mean loss: 205.81
 ---- batch: 030 ----
mean loss: 205.02
 ---- batch: 040 ----
mean loss: 210.54
 ---- batch: 050 ----
mean loss: 212.15
 ---- batch: 060 ----
mean loss: 201.84
 ---- batch: 070 ----
mean loss: 209.15
 ---- batch: 080 ----
mean loss: 199.49
 ---- batch: 090 ----
mean loss: 213.34
train mean loss: 206.18
epoch train time: 0:00:02.638642
elapsed time: 0:10:47.178509
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 17:57:57.771520
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.69
 ---- batch: 020 ----
mean loss: 210.64
 ---- batch: 030 ----
mean loss: 211.29
 ---- batch: 040 ----
mean loss: 203.49
 ---- batch: 050 ----
mean loss: 202.36
 ---- batch: 060 ----
mean loss: 200.03
 ---- batch: 070 ----
mean loss: 207.46
 ---- batch: 080 ----
mean loss: 209.79
 ---- batch: 090 ----
mean loss: 203.72
train mean loss: 206.24
epoch train time: 0:00:02.596290
elapsed time: 0:10:49.775288
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 17:58:00.368290
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.45
 ---- batch: 020 ----
mean loss: 204.88
 ---- batch: 030 ----
mean loss: 206.74
 ---- batch: 040 ----
mean loss: 203.22
 ---- batch: 050 ----
mean loss: 201.92
 ---- batch: 060 ----
mean loss: 199.83
 ---- batch: 070 ----
mean loss: 204.85
 ---- batch: 080 ----
mean loss: 205.48
 ---- batch: 090 ----
mean loss: 214.91
train mean loss: 206.16
epoch train time: 0:00:02.657903
elapsed time: 0:10:52.433712
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 17:58:03.026705
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.86
 ---- batch: 020 ----
mean loss: 208.53
 ---- batch: 030 ----
mean loss: 208.26
 ---- batch: 040 ----
mean loss: 201.84
 ---- batch: 050 ----
mean loss: 198.38
 ---- batch: 060 ----
mean loss: 213.26
 ---- batch: 070 ----
mean loss: 207.00
 ---- batch: 080 ----
mean loss: 203.02
 ---- batch: 090 ----
mean loss: 207.54
train mean loss: 206.16
epoch train time: 0:00:02.654093
elapsed time: 0:10:55.088284
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 17:58:05.681280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.96
 ---- batch: 020 ----
mean loss: 204.42
 ---- batch: 030 ----
mean loss: 201.17
 ---- batch: 040 ----
mean loss: 201.83
 ---- batch: 050 ----
mean loss: 208.69
 ---- batch: 060 ----
mean loss: 208.60
 ---- batch: 070 ----
mean loss: 199.21
 ---- batch: 080 ----
mean loss: 209.10
 ---- batch: 090 ----
mean loss: 205.45
train mean loss: 206.33
epoch train time: 0:00:02.638955
elapsed time: 0:10:57.727817
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 17:58:08.320686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.42
 ---- batch: 020 ----
mean loss: 211.00
 ---- batch: 030 ----
mean loss: 200.88
 ---- batch: 040 ----
mean loss: 212.78
 ---- batch: 050 ----
mean loss: 205.89
 ---- batch: 060 ----
mean loss: 204.71
 ---- batch: 070 ----
mean loss: 210.40
 ---- batch: 080 ----
mean loss: 203.77
 ---- batch: 090 ----
mean loss: 204.82
train mean loss: 206.33
epoch train time: 0:00:02.656741
elapsed time: 0:11:00.384914
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 17:58:10.977923
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.22
 ---- batch: 020 ----
mean loss: 205.77
 ---- batch: 030 ----
mean loss: 203.97
 ---- batch: 040 ----
mean loss: 200.78
 ---- batch: 050 ----
mean loss: 211.48
 ---- batch: 060 ----
mean loss: 211.90
 ---- batch: 070 ----
mean loss: 206.39
 ---- batch: 080 ----
mean loss: 207.20
 ---- batch: 090 ----
mean loss: 202.32
train mean loss: 206.26
epoch train time: 0:00:02.620188
elapsed time: 0:11:03.005582
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 17:58:13.598572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.38
 ---- batch: 020 ----
mean loss: 207.15
 ---- batch: 030 ----
mean loss: 205.90
 ---- batch: 040 ----
mean loss: 211.78
 ---- batch: 050 ----
mean loss: 205.89
 ---- batch: 060 ----
mean loss: 197.73
 ---- batch: 070 ----
mean loss: 209.56
 ---- batch: 080 ----
mean loss: 208.48
 ---- batch: 090 ----
mean loss: 206.41
train mean loss: 206.03
epoch train time: 0:00:02.616152
elapsed time: 0:11:05.622201
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 17:58:16.215220
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.60
 ---- batch: 020 ----
mean loss: 202.25
 ---- batch: 030 ----
mean loss: 212.62
 ---- batch: 040 ----
mean loss: 204.52
 ---- batch: 050 ----
mean loss: 205.30
 ---- batch: 060 ----
mean loss: 199.62
 ---- batch: 070 ----
mean loss: 205.13
 ---- batch: 080 ----
mean loss: 211.93
 ---- batch: 090 ----
mean loss: 204.42
train mean loss: 206.29
epoch train time: 0:00:02.647013
elapsed time: 0:11:08.269703
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 17:58:18.862699
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.25
 ---- batch: 020 ----
mean loss: 208.36
 ---- batch: 030 ----
mean loss: 211.66
 ---- batch: 040 ----
mean loss: 201.88
 ---- batch: 050 ----
mean loss: 205.81
 ---- batch: 060 ----
mean loss: 201.83
 ---- batch: 070 ----
mean loss: 204.83
 ---- batch: 080 ----
mean loss: 205.73
 ---- batch: 090 ----
mean loss: 205.27
train mean loss: 206.44
epoch train time: 0:00:02.636311
elapsed time: 0:11:10.906505
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 17:58:21.499500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.89
 ---- batch: 020 ----
mean loss: 205.53
 ---- batch: 030 ----
mean loss: 207.24
 ---- batch: 040 ----
mean loss: 199.89
 ---- batch: 050 ----
mean loss: 205.83
 ---- batch: 060 ----
mean loss: 212.47
 ---- batch: 070 ----
mean loss: 198.89
 ---- batch: 080 ----
mean loss: 206.68
 ---- batch: 090 ----
mean loss: 211.31
train mean loss: 206.18
epoch train time: 0:00:02.643734
elapsed time: 0:11:13.550720
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 17:58:24.143708
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.26
 ---- batch: 020 ----
mean loss: 207.66
 ---- batch: 030 ----
mean loss: 205.93
 ---- batch: 040 ----
mean loss: 201.27
 ---- batch: 050 ----
mean loss: 203.57
 ---- batch: 060 ----
mean loss: 202.06
 ---- batch: 070 ----
mean loss: 208.47
 ---- batch: 080 ----
mean loss: 211.06
 ---- batch: 090 ----
mean loss: 201.27
train mean loss: 206.36
epoch train time: 0:00:02.614454
elapsed time: 0:11:16.165628
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 17:58:26.758624
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.31
 ---- batch: 020 ----
mean loss: 201.03
 ---- batch: 030 ----
mean loss: 211.74
 ---- batch: 040 ----
mean loss: 199.30
 ---- batch: 050 ----
mean loss: 206.00
 ---- batch: 060 ----
mean loss: 201.20
 ---- batch: 070 ----
mean loss: 210.41
 ---- batch: 080 ----
mean loss: 203.51
 ---- batch: 090 ----
mean loss: 210.03
train mean loss: 206.38
epoch train time: 0:00:02.617420
elapsed time: 0:11:18.783549
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 17:58:29.376551
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.73
 ---- batch: 020 ----
mean loss: 200.68
 ---- batch: 030 ----
mean loss: 211.18
 ---- batch: 040 ----
mean loss: 203.74
 ---- batch: 050 ----
mean loss: 202.73
 ---- batch: 060 ----
mean loss: 203.74
 ---- batch: 070 ----
mean loss: 205.14
 ---- batch: 080 ----
mean loss: 204.36
 ---- batch: 090 ----
mean loss: 210.45
train mean loss: 206.09
epoch train time: 0:00:02.656823
elapsed time: 0:11:21.440904
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 17:58:32.033946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.79
 ---- batch: 020 ----
mean loss: 208.86
 ---- batch: 030 ----
mean loss: 203.87
 ---- batch: 040 ----
mean loss: 199.06
 ---- batch: 050 ----
mean loss: 202.23
 ---- batch: 060 ----
mean loss: 210.67
 ---- batch: 070 ----
mean loss: 212.72
 ---- batch: 080 ----
mean loss: 200.32
 ---- batch: 090 ----
mean loss: 211.57
train mean loss: 205.89
epoch train time: 0:00:02.654276
elapsed time: 0:11:24.095715
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 17:58:34.688718
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.22
 ---- batch: 020 ----
mean loss: 212.94
 ---- batch: 030 ----
mean loss: 202.62
 ---- batch: 040 ----
mean loss: 202.31
 ---- batch: 050 ----
mean loss: 207.32
 ---- batch: 060 ----
mean loss: 203.52
 ---- batch: 070 ----
mean loss: 206.61
 ---- batch: 080 ----
mean loss: 202.37
 ---- batch: 090 ----
mean loss: 207.14
train mean loss: 205.99
epoch train time: 0:00:02.697965
elapsed time: 0:11:26.794167
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 17:58:37.387177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.56
 ---- batch: 020 ----
mean loss: 204.87
 ---- batch: 030 ----
mean loss: 202.56
 ---- batch: 040 ----
mean loss: 208.44
 ---- batch: 050 ----
mean loss: 208.35
 ---- batch: 060 ----
mean loss: 208.78
 ---- batch: 070 ----
mean loss: 201.22
 ---- batch: 080 ----
mean loss: 211.72
 ---- batch: 090 ----
mean loss: 208.56
train mean loss: 206.04
epoch train time: 0:00:02.649779
elapsed time: 0:11:29.444486
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 17:58:40.037487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.82
 ---- batch: 020 ----
mean loss: 205.43
 ---- batch: 030 ----
mean loss: 201.33
 ---- batch: 040 ----
mean loss: 213.42
 ---- batch: 050 ----
mean loss: 206.32
 ---- batch: 060 ----
mean loss: 198.22
 ---- batch: 070 ----
mean loss: 205.01
 ---- batch: 080 ----
mean loss: 201.90
 ---- batch: 090 ----
mean loss: 209.05
train mean loss: 206.15
epoch train time: 0:00:02.625329
elapsed time: 0:11:32.070323
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 17:58:42.663321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.14
 ---- batch: 020 ----
mean loss: 204.78
 ---- batch: 030 ----
mean loss: 207.41
 ---- batch: 040 ----
mean loss: 213.10
 ---- batch: 050 ----
mean loss: 207.31
 ---- batch: 060 ----
mean loss: 207.88
 ---- batch: 070 ----
mean loss: 198.23
 ---- batch: 080 ----
mean loss: 198.99
 ---- batch: 090 ----
mean loss: 211.09
train mean loss: 205.96
epoch train time: 0:00:02.653843
elapsed time: 0:11:34.724655
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 17:58:45.317674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.19
 ---- batch: 020 ----
mean loss: 207.39
 ---- batch: 030 ----
mean loss: 205.45
 ---- batch: 040 ----
mean loss: 209.74
 ---- batch: 050 ----
mean loss: 199.97
 ---- batch: 060 ----
mean loss: 208.21
 ---- batch: 070 ----
mean loss: 215.76
 ---- batch: 080 ----
mean loss: 197.80
 ---- batch: 090 ----
mean loss: 203.15
train mean loss: 206.15
epoch train time: 0:00:02.671033
elapsed time: 0:11:37.396220
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 17:58:47.989217
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.11
 ---- batch: 020 ----
mean loss: 210.55
 ---- batch: 030 ----
mean loss: 203.41
 ---- batch: 040 ----
mean loss: 200.11
 ---- batch: 050 ----
mean loss: 213.40
 ---- batch: 060 ----
mean loss: 207.33
 ---- batch: 070 ----
mean loss: 199.74
 ---- batch: 080 ----
mean loss: 212.83
 ---- batch: 090 ----
mean loss: 210.63
train mean loss: 205.88
epoch train time: 0:00:02.649830
elapsed time: 0:11:40.050542
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_2/checkpoint.pth.tar
**** end time: 2019-09-25 17:58:50.643381 ****
