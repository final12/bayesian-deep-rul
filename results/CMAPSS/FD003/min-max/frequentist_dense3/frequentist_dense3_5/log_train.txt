Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_5', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8686
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistDense3...
Done.
**** start time: 2019-09-20 23:44:26.371271 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
            Linear-2                  [-1, 100]          42,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 62,100
Trainable params: 62,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:44:26.374363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4762.64
 ---- batch: 020 ----
mean loss: 4669.09
 ---- batch: 030 ----
mean loss: 4638.97
 ---- batch: 040 ----
mean loss: 4497.99
train mean loss: 4626.37
epoch train time: 0:00:14.841134
elapsed time: 0:00:14.846935
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:44:41.218256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4443.83
 ---- batch: 020 ----
mean loss: 4314.67
 ---- batch: 030 ----
mean loss: 4230.39
 ---- batch: 040 ----
mean loss: 4264.35
train mean loss: 4301.42
epoch train time: 0:00:00.200714
elapsed time: 0:00:15.047785
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:44:41.419112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4085.28
 ---- batch: 020 ----
mean loss: 4056.71
 ---- batch: 030 ----
mean loss: 4054.66
 ---- batch: 040 ----
mean loss: 3927.55
train mean loss: 4015.53
epoch train time: 0:00:00.200850
elapsed time: 0:00:15.248779
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:44:41.620092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3811.92
 ---- batch: 020 ----
mean loss: 3863.92
 ---- batch: 030 ----
mean loss: 3604.21
 ---- batch: 040 ----
mean loss: 3680.13
train mean loss: 3740.28
epoch train time: 0:00:00.200406
elapsed time: 0:00:15.449307
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:44:41.820620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3601.26
 ---- batch: 020 ----
mean loss: 3500.67
 ---- batch: 030 ----
mean loss: 3478.80
 ---- batch: 040 ----
mean loss: 3393.44
train mean loss: 3484.79
epoch train time: 0:00:00.199474
elapsed time: 0:00:15.648899
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:44:42.020212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3313.03
 ---- batch: 020 ----
mean loss: 3329.30
 ---- batch: 030 ----
mean loss: 3229.06
 ---- batch: 040 ----
mean loss: 3163.03
train mean loss: 3253.89
epoch train time: 0:00:00.198996
elapsed time: 0:00:15.848023
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:44:42.219335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3093.56
 ---- batch: 020 ----
mean loss: 3113.24
 ---- batch: 030 ----
mean loss: 3035.96
 ---- batch: 040 ----
mean loss: 2908.43
train mean loss: 3032.28
epoch train time: 0:00:00.201393
elapsed time: 0:00:16.049532
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:44:42.420856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2917.26
 ---- batch: 020 ----
mean loss: 2807.06
 ---- batch: 030 ----
mean loss: 2839.96
 ---- batch: 040 ----
mean loss: 2758.02
train mean loss: 2826.63
epoch train time: 0:00:00.197123
elapsed time: 0:00:16.246782
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:44:42.618095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2770.29
 ---- batch: 020 ----
mean loss: 2643.78
 ---- batch: 030 ----
mean loss: 2609.82
 ---- batch: 040 ----
mean loss: 2557.45
train mean loss: 2639.33
epoch train time: 0:00:00.199524
elapsed time: 0:00:16.446447
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:44:42.817780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2513.78
 ---- batch: 020 ----
mean loss: 2460.77
 ---- batch: 030 ----
mean loss: 2430.92
 ---- batch: 040 ----
mean loss: 2421.84
train mean loss: 2448.71
epoch train time: 0:00:00.192915
elapsed time: 0:00:16.639492
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:44:43.010802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2325.38
 ---- batch: 020 ----
mean loss: 2301.36
 ---- batch: 030 ----
mean loss: 2273.38
 ---- batch: 040 ----
mean loss: 2184.82
train mean loss: 2272.45
epoch train time: 0:00:00.194007
elapsed time: 0:00:16.833608
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:44:43.204917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2211.08
 ---- batch: 020 ----
mean loss: 2114.86
 ---- batch: 030 ----
mean loss: 2083.57
 ---- batch: 040 ----
mean loss: 2070.12
train mean loss: 2116.75
epoch train time: 0:00:00.190839
elapsed time: 0:00:17.024556
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:44:43.395876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2016.69
 ---- batch: 020 ----
mean loss: 2001.64
 ---- batch: 030 ----
mean loss: 1951.86
 ---- batch: 040 ----
mean loss: 1952.95
train mean loss: 1976.60
epoch train time: 0:00:00.190868
elapsed time: 0:00:17.215558
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:44:43.586884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1891.54
 ---- batch: 020 ----
mean loss: 1866.39
 ---- batch: 030 ----
mean loss: 1831.06
 ---- batch: 040 ----
mean loss: 1820.64
train mean loss: 1851.06
epoch train time: 0:00:00.194192
elapsed time: 0:00:17.409880
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:44:43.781191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1786.76
 ---- batch: 020 ----
mean loss: 1741.49
 ---- batch: 030 ----
mean loss: 1712.57
 ---- batch: 040 ----
mean loss: 1713.01
train mean loss: 1732.46
epoch train time: 0:00:00.197626
elapsed time: 0:00:17.607631
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:44:43.978956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1645.64
 ---- batch: 020 ----
mean loss: 1656.64
 ---- batch: 030 ----
mean loss: 1624.95
 ---- batch: 040 ----
mean loss: 1586.75
train mean loss: 1624.48
epoch train time: 0:00:00.196982
elapsed time: 0:00:17.804740
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:44:44.176053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1541.64
 ---- batch: 020 ----
mean loss: 1532.70
 ---- batch: 030 ----
mean loss: 1521.36
 ---- batch: 040 ----
mean loss: 1507.88
train mean loss: 1525.83
epoch train time: 0:00:00.198849
elapsed time: 0:00:18.003722
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:44:44.375036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1458.74
 ---- batch: 020 ----
mean loss: 1447.27
 ---- batch: 030 ----
mean loss: 1426.89
 ---- batch: 040 ----
mean loss: 1408.68
train mean loss: 1433.28
epoch train time: 0:00:00.194850
elapsed time: 0:00:18.198683
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:44:44.569992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1394.34
 ---- batch: 020 ----
mean loss: 1333.22
 ---- batch: 030 ----
mean loss: 1356.88
 ---- batch: 040 ----
mean loss: 1319.79
train mean loss: 1347.19
epoch train time: 0:00:00.195552
elapsed time: 0:00:18.394368
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:44:44.765684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1284.03
 ---- batch: 020 ----
mean loss: 1286.40
 ---- batch: 030 ----
mean loss: 1280.94
 ---- batch: 040 ----
mean loss: 1229.19
train mean loss: 1269.85
epoch train time: 0:00:00.205972
elapsed time: 0:00:18.600463
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:44:44.971773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1217.77
 ---- batch: 020 ----
mean loss: 1212.70
 ---- batch: 030 ----
mean loss: 1191.26
 ---- batch: 040 ----
mean loss: 1176.97
train mean loss: 1197.79
epoch train time: 0:00:00.202470
elapsed time: 0:00:18.803070
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:44:45.174386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1159.93
 ---- batch: 020 ----
mean loss: 1128.68
 ---- batch: 030 ----
mean loss: 1128.29
 ---- batch: 040 ----
mean loss: 1123.14
train mean loss: 1132.37
epoch train time: 0:00:00.202139
elapsed time: 0:00:19.005363
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:44:45.376676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1093.30
 ---- batch: 020 ----
mean loss: 1029.61
 ---- batch: 030 ----
mean loss: 979.99
 ---- batch: 040 ----
mean loss: 943.76
train mean loss: 1007.06
epoch train time: 0:00:00.203584
elapsed time: 0:00:19.209062
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:44:45.580375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.16
 ---- batch: 020 ----
mean loss: 922.22
 ---- batch: 030 ----
mean loss: 888.17
 ---- batch: 040 ----
mean loss: 865.84
train mean loss: 896.61
epoch train time: 0:00:00.207175
elapsed time: 0:00:19.416357
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:44:45.787671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.11
 ---- batch: 020 ----
mean loss: 827.63
 ---- batch: 030 ----
mean loss: 817.87
 ---- batch: 040 ----
mean loss: 799.58
train mean loss: 823.93
epoch train time: 0:00:00.206550
elapsed time: 0:00:19.623053
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:44:45.994393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 785.57
 ---- batch: 020 ----
mean loss: 780.39
 ---- batch: 030 ----
mean loss: 749.30
 ---- batch: 040 ----
mean loss: 727.41
train mean loss: 756.28
epoch train time: 0:00:00.205073
elapsed time: 0:00:19.828281
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:44:46.199596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 726.08
 ---- batch: 020 ----
mean loss: 704.03
 ---- batch: 030 ----
mean loss: 676.09
 ---- batch: 040 ----
mean loss: 674.14
train mean loss: 692.97
epoch train time: 0:00:00.209260
elapsed time: 0:00:20.037665
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:44:46.408980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 661.85
 ---- batch: 020 ----
mean loss: 639.11
 ---- batch: 030 ----
mean loss: 636.94
 ---- batch: 040 ----
mean loss: 616.68
train mean loss: 635.37
epoch train time: 0:00:00.202604
elapsed time: 0:00:20.240390
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:44:46.611704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.86
 ---- batch: 020 ----
mean loss: 590.69
 ---- batch: 030 ----
mean loss: 577.19
 ---- batch: 040 ----
mean loss: 566.59
train mean loss: 583.85
epoch train time: 0:00:00.204966
elapsed time: 0:00:20.445497
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:44:46.816812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 554.22
 ---- batch: 020 ----
mean loss: 532.12
 ---- batch: 030 ----
mean loss: 540.19
 ---- batch: 040 ----
mean loss: 524.82
train mean loss: 535.72
epoch train time: 0:00:00.201088
elapsed time: 0:00:20.646715
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:44:47.018026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.82
 ---- batch: 020 ----
mean loss: 505.38
 ---- batch: 030 ----
mean loss: 493.82
 ---- batch: 040 ----
mean loss: 481.89
train mean loss: 491.84
epoch train time: 0:00:00.198525
elapsed time: 0:00:20.845366
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:44:47.216679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.04
 ---- batch: 020 ----
mean loss: 464.34
 ---- batch: 030 ----
mean loss: 447.93
 ---- batch: 040 ----
mean loss: 438.92
train mean loss: 452.41
epoch train time: 0:00:00.196218
elapsed time: 0:00:21.041728
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:44:47.413054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.39
 ---- batch: 020 ----
mean loss: 425.20
 ---- batch: 030 ----
mean loss: 404.55
 ---- batch: 040 ----
mean loss: 414.81
train mean loss: 414.60
epoch train time: 0:00:00.192368
elapsed time: 0:00:21.234238
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:44:47.605546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.80
 ---- batch: 020 ----
mean loss: 379.33
 ---- batch: 030 ----
mean loss: 373.96
 ---- batch: 040 ----
mean loss: 370.38
train mean loss: 378.63
epoch train time: 0:00:00.196525
elapsed time: 0:00:21.430882
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:44:47.802195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.62
 ---- batch: 020 ----
mean loss: 351.85
 ---- batch: 030 ----
mean loss: 344.35
 ---- batch: 040 ----
mean loss: 331.81
train mean loss: 347.03
epoch train time: 0:00:00.197264
elapsed time: 0:00:21.628256
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:44:47.999565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.38
 ---- batch: 020 ----
mean loss: 322.30
 ---- batch: 030 ----
mean loss: 316.84
 ---- batch: 040 ----
mean loss: 315.23
train mean loss: 318.53
epoch train time: 0:00:00.197556
elapsed time: 0:00:21.825928
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:44:48.197261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.39
 ---- batch: 020 ----
mean loss: 298.39
 ---- batch: 030 ----
mean loss: 291.38
 ---- batch: 040 ----
mean loss: 285.69
train mean loss: 293.12
epoch train time: 0:00:00.198916
elapsed time: 0:00:22.024986
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:44:48.396308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.36
 ---- batch: 020 ----
mean loss: 272.70
 ---- batch: 030 ----
mean loss: 266.78
 ---- batch: 040 ----
mean loss: 262.87
train mean loss: 269.38
epoch train time: 0:00:00.195352
elapsed time: 0:00:22.220483
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:44:48.591796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.96
 ---- batch: 020 ----
mean loss: 252.39
 ---- batch: 030 ----
mean loss: 244.68
 ---- batch: 040 ----
mean loss: 243.42
train mean loss: 248.79
epoch train time: 0:00:00.200563
elapsed time: 0:00:22.421190
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:44:48.792521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.73
 ---- batch: 020 ----
mean loss: 231.76
 ---- batch: 030 ----
mean loss: 227.79
 ---- batch: 040 ----
mean loss: 222.61
train mean loss: 229.75
epoch train time: 0:00:00.199266
elapsed time: 0:00:22.620586
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:44:48.991895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.46
 ---- batch: 020 ----
mean loss: 214.13
 ---- batch: 030 ----
mean loss: 213.02
 ---- batch: 040 ----
mean loss: 207.88
train mean loss: 212.98
epoch train time: 0:00:00.202098
elapsed time: 0:00:22.822801
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:44:49.194115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.15
 ---- batch: 020 ----
mean loss: 193.15
 ---- batch: 030 ----
mean loss: 200.45
 ---- batch: 040 ----
mean loss: 193.71
train mean loss: 197.35
epoch train time: 0:00:00.201440
elapsed time: 0:00:23.024370
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:44:49.395695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.37
 ---- batch: 020 ----
mean loss: 187.39
 ---- batch: 030 ----
mean loss: 183.41
 ---- batch: 040 ----
mean loss: 177.48
train mean loss: 183.73
epoch train time: 0:00:00.200112
elapsed time: 0:00:23.224606
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:44:49.595917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.84
 ---- batch: 020 ----
mean loss: 178.14
 ---- batch: 030 ----
mean loss: 167.08
 ---- batch: 040 ----
mean loss: 163.58
train mean loss: 170.87
epoch train time: 0:00:00.209902
elapsed time: 0:00:23.434626
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:44:49.805941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.80
 ---- batch: 020 ----
mean loss: 162.66
 ---- batch: 030 ----
mean loss: 160.10
 ---- batch: 040 ----
mean loss: 157.36
train mean loss: 160.60
epoch train time: 0:00:00.201183
elapsed time: 0:00:23.635929
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:44:50.007241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.38
 ---- batch: 020 ----
mean loss: 151.64
 ---- batch: 030 ----
mean loss: 149.57
 ---- batch: 040 ----
mean loss: 146.78
train mean loss: 150.04
epoch train time: 0:00:00.200016
elapsed time: 0:00:23.836062
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:44:50.207376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.35
 ---- batch: 020 ----
mean loss: 140.09
 ---- batch: 030 ----
mean loss: 143.49
 ---- batch: 040 ----
mean loss: 138.19
train mean loss: 140.59
epoch train time: 0:00:00.204420
elapsed time: 0:00:24.040600
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:44:50.411928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.85
 ---- batch: 020 ----
mean loss: 132.95
 ---- batch: 030 ----
mean loss: 130.13
 ---- batch: 040 ----
mean loss: 131.97
train mean loss: 132.21
epoch train time: 0:00:00.201413
elapsed time: 0:00:24.242144
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:44:50.613457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.83
 ---- batch: 020 ----
mean loss: 124.67
 ---- batch: 030 ----
mean loss: 123.24
 ---- batch: 040 ----
mean loss: 125.90
train mean loss: 124.52
epoch train time: 0:00:00.204480
elapsed time: 0:00:24.446775
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:44:50.818126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.05
 ---- batch: 020 ----
mean loss: 119.00
 ---- batch: 030 ----
mean loss: 114.27
 ---- batch: 040 ----
mean loss: 116.07
train mean loss: 118.00
epoch train time: 0:00:00.202408
elapsed time: 0:00:24.649340
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:44:51.020669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.88
 ---- batch: 020 ----
mean loss: 114.12
 ---- batch: 030 ----
mean loss: 111.01
 ---- batch: 040 ----
mean loss: 111.45
train mean loss: 112.32
epoch train time: 0:00:00.200483
elapsed time: 0:00:24.849959
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:44:51.221275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.18
 ---- batch: 020 ----
mean loss: 105.81
 ---- batch: 030 ----
mean loss: 105.44
 ---- batch: 040 ----
mean loss: 104.67
train mean loss: 106.50
epoch train time: 0:00:00.197463
elapsed time: 0:00:25.047540
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:44:51.418852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.93
 ---- batch: 020 ----
mean loss: 101.14
 ---- batch: 030 ----
mean loss: 103.38
 ---- batch: 040 ----
mean loss: 100.21
train mean loss: 102.09
epoch train time: 0:00:00.196198
elapsed time: 0:00:25.243879
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:44:51.615193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.79
 ---- batch: 020 ----
mean loss: 96.62
 ---- batch: 030 ----
mean loss: 96.37
 ---- batch: 040 ----
mean loss: 97.21
train mean loss: 97.30
epoch train time: 0:00:00.207027
elapsed time: 0:00:25.451054
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:44:51.822364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.31
 ---- batch: 020 ----
mean loss: 91.31
 ---- batch: 030 ----
mean loss: 95.10
 ---- batch: 040 ----
mean loss: 92.08
train mean loss: 93.61
epoch train time: 0:00:00.202621
elapsed time: 0:00:25.653790
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:44:52.025129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.47
 ---- batch: 020 ----
mean loss: 91.65
 ---- batch: 030 ----
mean loss: 92.45
 ---- batch: 040 ----
mean loss: 87.36
train mean loss: 90.68
epoch train time: 0:00:00.197008
elapsed time: 0:00:25.850943
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:44:52.222272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.05
 ---- batch: 020 ----
mean loss: 86.69
 ---- batch: 030 ----
mean loss: 84.83
 ---- batch: 040 ----
mean loss: 86.60
train mean loss: 86.96
epoch train time: 0:00:00.198029
elapsed time: 0:00:26.049103
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:44:52.420466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.35
 ---- batch: 020 ----
mean loss: 83.18
 ---- batch: 030 ----
mean loss: 85.25
 ---- batch: 040 ----
mean loss: 82.40
train mean loss: 83.86
epoch train time: 0:00:00.192080
elapsed time: 0:00:26.241363
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:44:52.612674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.33
 ---- batch: 020 ----
mean loss: 79.78
 ---- batch: 030 ----
mean loss: 79.05
 ---- batch: 040 ----
mean loss: 82.04
train mean loss: 80.87
epoch train time: 0:00:00.194961
elapsed time: 0:00:26.436438
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:44:52.807749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.47
 ---- batch: 020 ----
mean loss: 79.78
 ---- batch: 030 ----
mean loss: 77.61
 ---- batch: 040 ----
mean loss: 77.94
train mean loss: 78.96
epoch train time: 0:00:00.195695
elapsed time: 0:00:26.632251
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:44:53.003565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.98
 ---- batch: 020 ----
mean loss: 76.46
 ---- batch: 030 ----
mean loss: 78.01
 ---- batch: 040 ----
mean loss: 75.44
train mean loss: 76.53
epoch train time: 0:00:00.202455
elapsed time: 0:00:26.834829
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:44:53.206158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.08
 ---- batch: 020 ----
mean loss: 75.05
 ---- batch: 030 ----
mean loss: 72.82
 ---- batch: 040 ----
mean loss: 74.66
train mean loss: 74.76
epoch train time: 0:00:00.206295
elapsed time: 0:00:27.041260
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:44:53.412574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.29
 ---- batch: 020 ----
mean loss: 74.58
 ---- batch: 030 ----
mean loss: 74.78
 ---- batch: 040 ----
mean loss: 75.69
train mean loss: 73.75
epoch train time: 0:00:00.206592
elapsed time: 0:00:27.247992
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:44:53.619306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.24
 ---- batch: 020 ----
mean loss: 70.51
 ---- batch: 030 ----
mean loss: 73.82
 ---- batch: 040 ----
mean loss: 70.74
train mean loss: 71.50
epoch train time: 0:00:00.204520
elapsed time: 0:00:27.452645
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:44:53.823958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.45
 ---- batch: 020 ----
mean loss: 67.77
 ---- batch: 030 ----
mean loss: 68.96
 ---- batch: 040 ----
mean loss: 71.30
train mean loss: 69.16
epoch train time: 0:00:00.205205
elapsed time: 0:00:27.657971
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:44:54.029285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.96
 ---- batch: 020 ----
mean loss: 69.05
 ---- batch: 030 ----
mean loss: 66.14
 ---- batch: 040 ----
mean loss: 67.45
train mean loss: 68.09
epoch train time: 0:00:00.207615
elapsed time: 0:00:27.865725
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:44:54.237055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.71
 ---- batch: 020 ----
mean loss: 66.17
 ---- batch: 030 ----
mean loss: 67.17
 ---- batch: 040 ----
mean loss: 65.64
train mean loss: 66.46
epoch train time: 0:00:00.208194
elapsed time: 0:00:28.074059
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:44:54.445374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.20
 ---- batch: 020 ----
mean loss: 66.80
 ---- batch: 030 ----
mean loss: 67.27
 ---- batch: 040 ----
mean loss: 63.18
train mean loss: 65.62
epoch train time: 0:00:00.210899
elapsed time: 0:00:28.285082
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:44:54.656419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.87
 ---- batch: 020 ----
mean loss: 63.34
 ---- batch: 030 ----
mean loss: 66.07
 ---- batch: 040 ----
mean loss: 66.70
train mean loss: 65.67
epoch train time: 0:00:00.210099
elapsed time: 0:00:28.495337
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:44:54.866649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.65
 ---- batch: 020 ----
mean loss: 61.58
 ---- batch: 030 ----
mean loss: 60.78
 ---- batch: 040 ----
mean loss: 64.92
train mean loss: 63.12
epoch train time: 0:00:00.202180
elapsed time: 0:00:28.697633
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:44:55.068945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.56
 ---- batch: 020 ----
mean loss: 59.97
 ---- batch: 030 ----
mean loss: 62.28
 ---- batch: 040 ----
mean loss: 63.62
train mean loss: 62.60
epoch train time: 0:00:00.199932
elapsed time: 0:00:28.897677
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:44:55.268987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.89
 ---- batch: 020 ----
mean loss: 63.16
 ---- batch: 030 ----
mean loss: 62.65
 ---- batch: 040 ----
mean loss: 60.27
train mean loss: 61.55
epoch train time: 0:00:00.198629
elapsed time: 0:00:29.096419
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:44:55.467732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.96
 ---- batch: 020 ----
mean loss: 63.38
 ---- batch: 030 ----
mean loss: 61.07
 ---- batch: 040 ----
mean loss: 60.94
train mean loss: 60.55
epoch train time: 0:00:00.192929
elapsed time: 0:00:29.289497
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:44:55.660823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.81
 ---- batch: 020 ----
mean loss: 57.33
 ---- batch: 030 ----
mean loss: 60.42
 ---- batch: 040 ----
mean loss: 61.14
train mean loss: 59.85
epoch train time: 0:00:00.204796
elapsed time: 0:00:29.494417
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:44:55.865726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.37
 ---- batch: 020 ----
mean loss: 59.85
 ---- batch: 030 ----
mean loss: 59.39
 ---- batch: 040 ----
mean loss: 58.76
train mean loss: 59.22
epoch train time: 0:00:00.191830
elapsed time: 0:00:29.686371
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:44:56.057682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.86
 ---- batch: 020 ----
mean loss: 61.71
 ---- batch: 030 ----
mean loss: 59.09
 ---- batch: 040 ----
mean loss: 55.66
train mean loss: 59.23
epoch train time: 0:00:00.194518
elapsed time: 0:00:29.880999
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:44:56.252339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.00
 ---- batch: 020 ----
mean loss: 53.87
 ---- batch: 030 ----
mean loss: 59.15
 ---- batch: 040 ----
mean loss: 59.70
train mean loss: 57.67
epoch train time: 0:00:00.197544
elapsed time: 0:00:30.078682
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:44:56.449993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.50
 ---- batch: 020 ----
mean loss: 59.51
 ---- batch: 030 ----
mean loss: 57.65
 ---- batch: 040 ----
mean loss: 56.25
train mean loss: 57.40
epoch train time: 0:00:00.195242
elapsed time: 0:00:30.274040
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:44:56.645353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.38
 ---- batch: 020 ----
mean loss: 53.47
 ---- batch: 030 ----
mean loss: 58.65
 ---- batch: 040 ----
mean loss: 56.70
train mean loss: 56.41
epoch train time: 0:00:00.198764
elapsed time: 0:00:30.472914
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:44:56.844224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.96
 ---- batch: 020 ----
mean loss: 55.01
 ---- batch: 030 ----
mean loss: 57.51
 ---- batch: 040 ----
mean loss: 57.15
train mean loss: 56.50
epoch train time: 0:00:00.199086
elapsed time: 0:00:30.672113
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:44:57.043426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.95
 ---- batch: 020 ----
mean loss: 54.27
 ---- batch: 030 ----
mean loss: 56.67
 ---- batch: 040 ----
mean loss: 55.61
train mean loss: 56.15
epoch train time: 0:00:00.205790
elapsed time: 0:00:30.878021
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:44:57.249335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.23
 ---- batch: 020 ----
mean loss: 54.53
 ---- batch: 030 ----
mean loss: 56.56
 ---- batch: 040 ----
mean loss: 58.56
train mean loss: 55.69
epoch train time: 0:00:00.205716
elapsed time: 0:00:31.083892
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:44:57.455208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.25
 ---- batch: 020 ----
mean loss: 53.42
 ---- batch: 030 ----
mean loss: 53.00
 ---- batch: 040 ----
mean loss: 55.19
train mean loss: 54.81
epoch train time: 0:00:00.206246
elapsed time: 0:00:31.290261
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:44:57.661574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.51
 ---- batch: 020 ----
mean loss: 56.27
 ---- batch: 030 ----
mean loss: 56.03
 ---- batch: 040 ----
mean loss: 56.07
train mean loss: 55.22
epoch train time: 0:00:00.206000
elapsed time: 0:00:31.496382
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:44:57.867697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.92
 ---- batch: 020 ----
mean loss: 52.51
 ---- batch: 030 ----
mean loss: 54.20
 ---- batch: 040 ----
mean loss: 53.15
train mean loss: 53.85
epoch train time: 0:00:00.203707
elapsed time: 0:00:31.700209
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:44:58.071553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.80
 ---- batch: 020 ----
mean loss: 53.58
 ---- batch: 030 ----
mean loss: 51.95
 ---- batch: 040 ----
mean loss: 54.91
train mean loss: 53.39
epoch train time: 0:00:00.203465
elapsed time: 0:00:31.903836
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:44:58.275152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.24
 ---- batch: 020 ----
mean loss: 52.97
 ---- batch: 030 ----
mean loss: 52.28
 ---- batch: 040 ----
mean loss: 56.54
train mean loss: 53.44
epoch train time: 0:00:00.204476
elapsed time: 0:00:32.108452
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:44:58.479767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.30
 ---- batch: 020 ----
mean loss: 52.89
 ---- batch: 030 ----
mean loss: 53.26
 ---- batch: 040 ----
mean loss: 51.52
train mean loss: 53.00
epoch train time: 0:00:00.203405
elapsed time: 0:00:32.312007
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:44:58.683321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.81
 ---- batch: 020 ----
mean loss: 51.77
 ---- batch: 030 ----
mean loss: 50.06
 ---- batch: 040 ----
mean loss: 51.33
train mean loss: 52.22
epoch train time: 0:00:00.202508
elapsed time: 0:00:32.514671
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:44:58.885986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.81
 ---- batch: 020 ----
mean loss: 49.97
 ---- batch: 030 ----
mean loss: 52.20
 ---- batch: 040 ----
mean loss: 55.41
train mean loss: 52.32
epoch train time: 0:00:00.196166
elapsed time: 0:00:32.710957
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:44:59.082284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.41
 ---- batch: 020 ----
mean loss: 55.55
 ---- batch: 030 ----
mean loss: 51.03
 ---- batch: 040 ----
mean loss: 52.40
train mean loss: 52.83
epoch train time: 0:00:00.197679
elapsed time: 0:00:32.908763
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:44:59.280073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.82
 ---- batch: 020 ----
mean loss: 52.64
 ---- batch: 030 ----
mean loss: 50.10
 ---- batch: 040 ----
mean loss: 49.27
train mean loss: 51.03
epoch train time: 0:00:00.197796
elapsed time: 0:00:33.106673
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:44:59.477984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.44
 ---- batch: 020 ----
mean loss: 50.27
 ---- batch: 030 ----
mean loss: 51.93
 ---- batch: 040 ----
mean loss: 53.27
train mean loss: 51.40
epoch train time: 0:00:00.192247
elapsed time: 0:00:33.299027
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:44:59.670335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.14
 ---- batch: 020 ----
mean loss: 53.30
 ---- batch: 030 ----
mean loss: 50.24
 ---- batch: 040 ----
mean loss: 51.12
train mean loss: 51.53
epoch train time: 0:00:00.194399
elapsed time: 0:00:33.493536
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:44:59.864848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.24
 ---- batch: 020 ----
mean loss: 52.73
 ---- batch: 030 ----
mean loss: 50.36
 ---- batch: 040 ----
mean loss: 52.89
train mean loss: 51.18
epoch train time: 0:00:00.196171
elapsed time: 0:00:33.689821
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:45:00.061132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.52
 ---- batch: 020 ----
mean loss: 52.00
 ---- batch: 030 ----
mean loss: 51.50
 ---- batch: 040 ----
mean loss: 48.88
train mean loss: 51.24
epoch train time: 0:00:00.198196
elapsed time: 0:00:33.888127
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:45:00.259439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.12
 ---- batch: 020 ----
mean loss: 51.40
 ---- batch: 030 ----
mean loss: 46.31
 ---- batch: 040 ----
mean loss: 49.33
train mean loss: 49.49
epoch train time: 0:00:00.197139
elapsed time: 0:00:34.085384
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:45:00.456716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.11
 ---- batch: 020 ----
mean loss: 48.73
 ---- batch: 030 ----
mean loss: 46.30
 ---- batch: 040 ----
mean loss: 50.06
train mean loss: 49.12
epoch train time: 0:00:00.200707
elapsed time: 0:00:34.286223
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:45:00.657535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.51
 ---- batch: 020 ----
mean loss: 49.33
 ---- batch: 030 ----
mean loss: 49.52
 ---- batch: 040 ----
mean loss: 51.79
train mean loss: 50.27
epoch train time: 0:00:00.205787
elapsed time: 0:00:34.492122
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:45:00.863480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.43
 ---- batch: 020 ----
mean loss: 53.28
 ---- batch: 030 ----
mean loss: 52.92
 ---- batch: 040 ----
mean loss: 52.96
train mean loss: 52.62
epoch train time: 0:00:00.192008
elapsed time: 0:00:34.684288
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:45:01.055599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.11
 ---- batch: 020 ----
mean loss: 48.21
 ---- batch: 030 ----
mean loss: 49.35
 ---- batch: 040 ----
mean loss: 49.43
train mean loss: 48.96
epoch train time: 0:00:00.194881
elapsed time: 0:00:34.879285
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:45:01.250648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.33
 ---- batch: 020 ----
mean loss: 46.82
 ---- batch: 030 ----
mean loss: 48.07
 ---- batch: 040 ----
mean loss: 49.39
train mean loss: 48.49
epoch train time: 0:00:00.200987
elapsed time: 0:00:35.080439
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:45:01.451755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.09
 ---- batch: 020 ----
mean loss: 46.05
 ---- batch: 030 ----
mean loss: 49.28
 ---- batch: 040 ----
mean loss: 49.89
train mean loss: 48.17
epoch train time: 0:00:00.199433
elapsed time: 0:00:35.280025
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:45:01.651354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.88
 ---- batch: 020 ----
mean loss: 47.72
 ---- batch: 030 ----
mean loss: 48.15
 ---- batch: 040 ----
mean loss: 46.30
train mean loss: 47.60
epoch train time: 0:00:00.205565
elapsed time: 0:00:35.485731
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:45:01.857047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.19
 ---- batch: 020 ----
mean loss: 48.92
 ---- batch: 030 ----
mean loss: 44.69
 ---- batch: 040 ----
mean loss: 48.32
train mean loss: 47.56
epoch train time: 0:00:00.199798
elapsed time: 0:00:35.685650
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:45:02.056974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.51
 ---- batch: 020 ----
mean loss: 48.60
 ---- batch: 030 ----
mean loss: 47.08
 ---- batch: 040 ----
mean loss: 51.98
train mean loss: 48.40
epoch train time: 0:00:00.199797
elapsed time: 0:00:35.885577
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:45:02.256893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.80
 ---- batch: 020 ----
mean loss: 46.19
 ---- batch: 030 ----
mean loss: 47.79
 ---- batch: 040 ----
mean loss: 47.78
train mean loss: 47.21
epoch train time: 0:00:00.196956
elapsed time: 0:00:36.082666
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:45:02.453974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.47
 ---- batch: 020 ----
mean loss: 46.65
 ---- batch: 030 ----
mean loss: 46.38
 ---- batch: 040 ----
mean loss: 46.97
train mean loss: 47.23
epoch train time: 0:00:00.197537
elapsed time: 0:00:36.280313
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:45:02.651629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.98
 ---- batch: 020 ----
mean loss: 45.53
 ---- batch: 030 ----
mean loss: 46.37
 ---- batch: 040 ----
mean loss: 48.70
train mean loss: 46.75
epoch train time: 0:00:00.201938
elapsed time: 0:00:36.482373
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:45:02.853704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.78
 ---- batch: 020 ----
mean loss: 44.88
 ---- batch: 030 ----
mean loss: 48.29
 ---- batch: 040 ----
mean loss: 43.46
train mean loss: 46.29
epoch train time: 0:00:00.202802
elapsed time: 0:00:36.685327
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:45:03.056641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.40
 ---- batch: 020 ----
mean loss: 45.31
 ---- batch: 030 ----
mean loss: 47.50
 ---- batch: 040 ----
mean loss: 46.10
train mean loss: 46.22
epoch train time: 0:00:00.202660
elapsed time: 0:00:36.888104
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:45:03.259416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.48
 ---- batch: 020 ----
mean loss: 47.01
 ---- batch: 030 ----
mean loss: 45.36
 ---- batch: 040 ----
mean loss: 46.63
train mean loss: 46.78
epoch train time: 0:00:00.198235
elapsed time: 0:00:37.086455
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:45:03.457780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.53
 ---- batch: 020 ----
mean loss: 47.88
 ---- batch: 030 ----
mean loss: 45.36
 ---- batch: 040 ----
mean loss: 44.68
train mean loss: 46.35
epoch train time: 0:00:00.195134
elapsed time: 0:00:37.281723
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:45:03.653048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.02
 ---- batch: 020 ----
mean loss: 46.46
 ---- batch: 030 ----
mean loss: 45.13
 ---- batch: 040 ----
mean loss: 43.67
train mean loss: 45.32
epoch train time: 0:00:00.197180
elapsed time: 0:00:37.479027
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:45:03.850337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.35
 ---- batch: 020 ----
mean loss: 43.96
 ---- batch: 030 ----
mean loss: 46.32
 ---- batch: 040 ----
mean loss: 45.95
train mean loss: 45.46
epoch train time: 0:00:00.190537
elapsed time: 0:00:37.669705
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:45:04.041045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.36
 ---- batch: 020 ----
mean loss: 48.84
 ---- batch: 030 ----
mean loss: 48.27
 ---- batch: 040 ----
mean loss: 49.72
train mean loss: 48.55
epoch train time: 0:00:00.196444
elapsed time: 0:00:37.866290
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:45:04.237600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.84
 ---- batch: 020 ----
mean loss: 45.76
 ---- batch: 030 ----
mean loss: 45.14
 ---- batch: 040 ----
mean loss: 51.07
train mean loss: 46.81
epoch train time: 0:00:00.194140
elapsed time: 0:00:38.060539
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:45:04.431851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.17
 ---- batch: 020 ----
mean loss: 49.90
 ---- batch: 030 ----
mean loss: 44.33
 ---- batch: 040 ----
mean loss: 44.21
train mean loss: 47.30
epoch train time: 0:00:00.192870
elapsed time: 0:00:38.253524
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:45:04.624847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.39
 ---- batch: 020 ----
mean loss: 43.56
 ---- batch: 030 ----
mean loss: 45.30
 ---- batch: 040 ----
mean loss: 47.48
train mean loss: 45.70
epoch train time: 0:00:00.200981
elapsed time: 0:00:38.454625
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:45:04.825943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.62
 ---- batch: 020 ----
mean loss: 45.53
 ---- batch: 030 ----
mean loss: 46.79
 ---- batch: 040 ----
mean loss: 42.98
train mean loss: 45.43
epoch train time: 0:00:00.197389
elapsed time: 0:00:38.652141
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:45:05.023459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.93
 ---- batch: 020 ----
mean loss: 44.96
 ---- batch: 030 ----
mean loss: 45.55
 ---- batch: 040 ----
mean loss: 44.42
train mean loss: 44.42
epoch train time: 0:00:00.208753
elapsed time: 0:00:38.861039
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:45:05.232365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.28
 ---- batch: 020 ----
mean loss: 44.92
 ---- batch: 030 ----
mean loss: 41.39
 ---- batch: 040 ----
mean loss: 46.85
train mean loss: 43.96
epoch train time: 0:00:00.204220
elapsed time: 0:00:39.065391
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:45:05.436705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.62
 ---- batch: 020 ----
mean loss: 44.37
 ---- batch: 030 ----
mean loss: 43.87
 ---- batch: 040 ----
mean loss: 44.80
train mean loss: 44.09
epoch train time: 0:00:00.205240
elapsed time: 0:00:39.270781
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:45:05.642097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.38
 ---- batch: 020 ----
mean loss: 43.33
 ---- batch: 030 ----
mean loss: 44.98
 ---- batch: 040 ----
mean loss: 42.74
train mean loss: 43.72
epoch train time: 0:00:00.206814
elapsed time: 0:00:39.477717
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:45:05.849029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.89
 ---- batch: 020 ----
mean loss: 41.88
 ---- batch: 030 ----
mean loss: 42.51
 ---- batch: 040 ----
mean loss: 43.29
train mean loss: 43.28
epoch train time: 0:00:00.202736
elapsed time: 0:00:39.680568
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:45:06.051879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.77
 ---- batch: 020 ----
mean loss: 42.79
 ---- batch: 030 ----
mean loss: 46.02
 ---- batch: 040 ----
mean loss: 42.55
train mean loss: 43.37
epoch train time: 0:00:00.205962
elapsed time: 0:00:39.886664
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:45:06.257993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.71
 ---- batch: 020 ----
mean loss: 44.45
 ---- batch: 030 ----
mean loss: 43.56
 ---- batch: 040 ----
mean loss: 43.53
train mean loss: 44.03
epoch train time: 0:00:00.205394
elapsed time: 0:00:40.092233
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:45:06.463541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.17
 ---- batch: 020 ----
mean loss: 42.36
 ---- batch: 030 ----
mean loss: 42.12
 ---- batch: 040 ----
mean loss: 43.62
train mean loss: 43.53
epoch train time: 0:00:00.204032
elapsed time: 0:00:40.296376
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:45:06.667687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.87
 ---- batch: 020 ----
mean loss: 42.77
 ---- batch: 030 ----
mean loss: 42.06
 ---- batch: 040 ----
mean loss: 43.96
train mean loss: 42.77
epoch train time: 0:00:00.202240
elapsed time: 0:00:40.498731
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:45:06.870043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.64
 ---- batch: 020 ----
mean loss: 44.52
 ---- batch: 030 ----
mean loss: 44.90
 ---- batch: 040 ----
mean loss: 43.11
train mean loss: 44.21
epoch train time: 0:00:00.200523
elapsed time: 0:00:40.699367
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:45:07.070680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.89
 ---- batch: 020 ----
mean loss: 43.81
 ---- batch: 030 ----
mean loss: 40.74
 ---- batch: 040 ----
mean loss: 43.56
train mean loss: 43.25
epoch train time: 0:00:00.197916
elapsed time: 0:00:40.897401
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:45:07.268714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.86
 ---- batch: 020 ----
mean loss: 43.77
 ---- batch: 030 ----
mean loss: 41.24
 ---- batch: 040 ----
mean loss: 42.07
train mean loss: 43.38
epoch train time: 0:00:00.195807
elapsed time: 0:00:41.093351
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:45:07.464662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.71
 ---- batch: 020 ----
mean loss: 43.22
 ---- batch: 030 ----
mean loss: 42.29
 ---- batch: 040 ----
mean loss: 41.15
train mean loss: 42.77
epoch train time: 0:00:00.193610
elapsed time: 0:00:41.287081
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:45:07.658404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.19
 ---- batch: 020 ----
mean loss: 41.70
 ---- batch: 030 ----
mean loss: 42.79
 ---- batch: 040 ----
mean loss: 42.93
train mean loss: 42.01
epoch train time: 0:00:00.196718
elapsed time: 0:00:41.483929
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:45:07.855244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.78
 ---- batch: 020 ----
mean loss: 44.00
 ---- batch: 030 ----
mean loss: 42.76
 ---- batch: 040 ----
mean loss: 42.48
train mean loss: 43.06
epoch train time: 0:00:00.203083
elapsed time: 0:00:41.687147
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:45:08.058458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.48
 ---- batch: 020 ----
mean loss: 41.32
 ---- batch: 030 ----
mean loss: 43.03
 ---- batch: 040 ----
mean loss: 43.26
train mean loss: 42.02
epoch train time: 0:00:00.197226
elapsed time: 0:00:41.884484
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:45:08.255794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.63
 ---- batch: 020 ----
mean loss: 42.99
 ---- batch: 030 ----
mean loss: 42.05
 ---- batch: 040 ----
mean loss: 41.08
train mean loss: 41.94
epoch train time: 0:00:00.195193
elapsed time: 0:00:42.079784
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:45:08.451094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.03
 ---- batch: 020 ----
mean loss: 41.78
 ---- batch: 030 ----
mean loss: 44.63
 ---- batch: 040 ----
mean loss: 42.92
train mean loss: 42.63
epoch train time: 0:00:00.194766
elapsed time: 0:00:42.274658
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:45:08.645984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.31
 ---- batch: 020 ----
mean loss: 44.78
 ---- batch: 030 ----
mean loss: 43.78
 ---- batch: 040 ----
mean loss: 42.18
train mean loss: 42.91
epoch train time: 0:00:00.197444
elapsed time: 0:00:42.472225
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:45:08.843535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.89
 ---- batch: 020 ----
mean loss: 40.62
 ---- batch: 030 ----
mean loss: 41.36
 ---- batch: 040 ----
mean loss: 40.92
train mean loss: 41.36
epoch train time: 0:00:00.194639
elapsed time: 0:00:42.666984
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:45:09.038312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.16
 ---- batch: 020 ----
mean loss: 39.14
 ---- batch: 030 ----
mean loss: 40.94
 ---- batch: 040 ----
mean loss: 41.47
train mean loss: 40.97
epoch train time: 0:00:00.197295
elapsed time: 0:00:42.864409
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:45:09.235721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.16
 ---- batch: 020 ----
mean loss: 42.11
 ---- batch: 030 ----
mean loss: 43.31
 ---- batch: 040 ----
mean loss: 41.24
train mean loss: 41.31
epoch train time: 0:00:00.198332
elapsed time: 0:00:43.062871
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:45:09.434184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.05
 ---- batch: 020 ----
mean loss: 42.56
 ---- batch: 030 ----
mean loss: 41.34
 ---- batch: 040 ----
mean loss: 39.78
train mean loss: 40.61
epoch train time: 0:00:00.203969
elapsed time: 0:00:43.266958
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:45:09.638281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.42
 ---- batch: 020 ----
mean loss: 41.74
 ---- batch: 030 ----
mean loss: 41.48
 ---- batch: 040 ----
mean loss: 41.61
train mean loss: 41.00
epoch train time: 0:00:00.207530
elapsed time: 0:00:43.474613
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:45:09.845926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.37
 ---- batch: 020 ----
mean loss: 42.43
 ---- batch: 030 ----
mean loss: 40.73
 ---- batch: 040 ----
mean loss: 39.74
train mean loss: 40.60
epoch train time: 0:00:00.204864
elapsed time: 0:00:43.679597
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:45:10.050925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.95
 ---- batch: 020 ----
mean loss: 37.81
 ---- batch: 030 ----
mean loss: 40.50
 ---- batch: 040 ----
mean loss: 41.31
train mean loss: 40.37
epoch train time: 0:00:00.204080
elapsed time: 0:00:43.883835
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:45:10.255151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.49
 ---- batch: 020 ----
mean loss: 42.74
 ---- batch: 030 ----
mean loss: 40.68
 ---- batch: 040 ----
mean loss: 41.09
train mean loss: 41.22
epoch train time: 0:00:00.205007
elapsed time: 0:00:44.088981
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:45:10.460306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.14
 ---- batch: 020 ----
mean loss: 38.67
 ---- batch: 030 ----
mean loss: 41.04
 ---- batch: 040 ----
mean loss: 39.88
train mean loss: 40.34
epoch train time: 0:00:00.202373
elapsed time: 0:00:44.291533
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:45:10.662857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.12
 ---- batch: 020 ----
mean loss: 42.46
 ---- batch: 030 ----
mean loss: 39.08
 ---- batch: 040 ----
mean loss: 38.22
train mean loss: 39.75
epoch train time: 0:00:00.205399
elapsed time: 0:00:44.497073
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:45:10.868379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.18
 ---- batch: 020 ----
mean loss: 40.25
 ---- batch: 030 ----
mean loss: 39.97
 ---- batch: 040 ----
mean loss: 39.89
train mean loss: 40.20
epoch train time: 0:00:00.202555
elapsed time: 0:00:44.699737
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:45:11.071048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.10
 ---- batch: 020 ----
mean loss: 40.50
 ---- batch: 030 ----
mean loss: 41.29
 ---- batch: 040 ----
mean loss: 41.56
train mean loss: 40.60
epoch train time: 0:00:00.204946
elapsed time: 0:00:44.904798
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:45:11.276112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.19
 ---- batch: 020 ----
mean loss: 40.02
 ---- batch: 030 ----
mean loss: 39.17
 ---- batch: 040 ----
mean loss: 38.96
train mean loss: 39.36
epoch train time: 0:00:00.202282
elapsed time: 0:00:45.107243
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:45:11.478571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.73
 ---- batch: 020 ----
mean loss: 38.71
 ---- batch: 030 ----
mean loss: 40.55
 ---- batch: 040 ----
mean loss: 40.76
train mean loss: 39.98
epoch train time: 0:00:00.198124
elapsed time: 0:00:45.305513
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:45:11.676824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.50
 ---- batch: 020 ----
mean loss: 40.20
 ---- batch: 030 ----
mean loss: 37.52
 ---- batch: 040 ----
mean loss: 38.40
train mean loss: 39.59
epoch train time: 0:00:00.199735
elapsed time: 0:00:45.505379
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:45:11.876704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.25
 ---- batch: 020 ----
mean loss: 39.09
 ---- batch: 030 ----
mean loss: 40.05
 ---- batch: 040 ----
mean loss: 40.28
train mean loss: 39.88
epoch train time: 0:00:00.196813
elapsed time: 0:00:45.702317
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:45:12.073643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.49
 ---- batch: 020 ----
mean loss: 39.24
 ---- batch: 030 ----
mean loss: 41.15
 ---- batch: 040 ----
mean loss: 38.95
train mean loss: 39.84
epoch train time: 0:00:00.196096
elapsed time: 0:00:45.898549
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:45:12.269861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.43
 ---- batch: 020 ----
mean loss: 40.34
 ---- batch: 030 ----
mean loss: 40.72
 ---- batch: 040 ----
mean loss: 38.58
train mean loss: 39.36
epoch train time: 0:00:00.198657
elapsed time: 0:00:46.097318
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:45:12.468656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.78
 ---- batch: 020 ----
mean loss: 39.80
 ---- batch: 030 ----
mean loss: 38.56
 ---- batch: 040 ----
mean loss: 39.04
train mean loss: 38.60
epoch train time: 0:00:00.196775
elapsed time: 0:00:46.294253
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:45:12.665566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.05
 ---- batch: 020 ----
mean loss: 38.14
 ---- batch: 030 ----
mean loss: 38.18
 ---- batch: 040 ----
mean loss: 37.59
train mean loss: 38.35
epoch train time: 0:00:00.216369
elapsed time: 0:00:46.510772
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:45:12.882088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.50
 ---- batch: 020 ----
mean loss: 37.28
 ---- batch: 030 ----
mean loss: 39.30
 ---- batch: 040 ----
mean loss: 42.19
train mean loss: 39.08
epoch train time: 0:00:00.196934
elapsed time: 0:00:46.707820
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:45:13.079131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.91
 ---- batch: 020 ----
mean loss: 38.32
 ---- batch: 030 ----
mean loss: 37.39
 ---- batch: 040 ----
mean loss: 39.46
train mean loss: 38.68
epoch train time: 0:00:00.197562
elapsed time: 0:00:46.905534
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:45:13.276862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.09
 ---- batch: 020 ----
mean loss: 35.72
 ---- batch: 030 ----
mean loss: 39.32
 ---- batch: 040 ----
mean loss: 38.96
train mean loss: 38.28
epoch train time: 0:00:00.196197
elapsed time: 0:00:47.101861
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:45:13.473172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.60
 ---- batch: 020 ----
mean loss: 39.08
 ---- batch: 030 ----
mean loss: 37.20
 ---- batch: 040 ----
mean loss: 40.21
train mean loss: 38.13
epoch train time: 0:00:00.205066
elapsed time: 0:00:47.307044
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:45:13.678359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.24
 ---- batch: 020 ----
mean loss: 37.10
 ---- batch: 030 ----
mean loss: 37.79
 ---- batch: 040 ----
mean loss: 38.76
train mean loss: 37.68
epoch train time: 0:00:00.208398
elapsed time: 0:00:47.515563
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:45:13.886878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.16
 ---- batch: 020 ----
mean loss: 37.83
 ---- batch: 030 ----
mean loss: 38.40
 ---- batch: 040 ----
mean loss: 37.51
train mean loss: 37.75
epoch train time: 0:00:00.210134
elapsed time: 0:00:47.725819
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:45:14.097131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.56
 ---- batch: 020 ----
mean loss: 38.46
 ---- batch: 030 ----
mean loss: 38.13
 ---- batch: 040 ----
mean loss: 40.28
train mean loss: 38.74
epoch train time: 0:00:00.208013
elapsed time: 0:00:47.933949
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:45:14.305263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.18
 ---- batch: 020 ----
mean loss: 37.32
 ---- batch: 030 ----
mean loss: 38.74
 ---- batch: 040 ----
mean loss: 35.64
train mean loss: 37.70
epoch train time: 0:00:00.205401
elapsed time: 0:00:48.139476
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:45:14.510820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.39
 ---- batch: 020 ----
mean loss: 38.70
 ---- batch: 030 ----
mean loss: 37.72
 ---- batch: 040 ----
mean loss: 35.74
train mean loss: 37.33
epoch train time: 0:00:00.205780
elapsed time: 0:00:48.345403
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:45:14.716716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.28
 ---- batch: 020 ----
mean loss: 38.43
 ---- batch: 030 ----
mean loss: 43.07
 ---- batch: 040 ----
mean loss: 38.02
train mean loss: 39.10
epoch train time: 0:00:00.222064
elapsed time: 0:00:48.567635
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:45:14.938965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.93
 ---- batch: 020 ----
mean loss: 35.97
 ---- batch: 030 ----
mean loss: 39.75
 ---- batch: 040 ----
mean loss: 38.68
train mean loss: 37.60
epoch train time: 0:00:00.211884
elapsed time: 0:00:48.779669
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:45:15.151006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.12
 ---- batch: 020 ----
mean loss: 35.87
 ---- batch: 030 ----
mean loss: 36.89
 ---- batch: 040 ----
mean loss: 37.38
train mean loss: 37.04
epoch train time: 0:00:00.213767
elapsed time: 0:00:48.993582
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:45:15.364907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.74
 ---- batch: 020 ----
mean loss: 36.03
 ---- batch: 030 ----
mean loss: 39.32
 ---- batch: 040 ----
mean loss: 38.58
train mean loss: 37.52
epoch train time: 0:00:00.217142
elapsed time: 0:00:49.210860
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:45:15.582175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.73
 ---- batch: 020 ----
mean loss: 38.77
 ---- batch: 030 ----
mean loss: 38.22
 ---- batch: 040 ----
mean loss: 39.72
train mean loss: 38.52
epoch train time: 0:00:00.219171
elapsed time: 0:00:49.430164
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:45:15.801477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.30
 ---- batch: 020 ----
mean loss: 40.04
 ---- batch: 030 ----
mean loss: 37.59
 ---- batch: 040 ----
mean loss: 37.84
train mean loss: 37.86
epoch train time: 0:00:00.222361
elapsed time: 0:00:49.652682
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:45:16.023987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.34
 ---- batch: 020 ----
mean loss: 37.18
 ---- batch: 030 ----
mean loss: 37.46
 ---- batch: 040 ----
mean loss: 36.74
train mean loss: 37.05
epoch train time: 0:00:00.215566
elapsed time: 0:00:49.868354
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:45:16.239665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.46
 ---- batch: 020 ----
mean loss: 38.22
 ---- batch: 030 ----
mean loss: 37.28
 ---- batch: 040 ----
mean loss: 37.66
train mean loss: 37.74
epoch train time: 0:00:00.207970
elapsed time: 0:00:50.076435
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:45:16.447799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.29
 ---- batch: 020 ----
mean loss: 36.33
 ---- batch: 030 ----
mean loss: 36.54
 ---- batch: 040 ----
mean loss: 39.16
train mean loss: 36.64
epoch train time: 0:00:00.208733
elapsed time: 0:00:50.285350
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:45:16.656661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.79
 ---- batch: 020 ----
mean loss: 35.36
 ---- batch: 030 ----
mean loss: 34.59
 ---- batch: 040 ----
mean loss: 37.25
train mean loss: 36.23
epoch train time: 0:00:00.211745
elapsed time: 0:00:50.497225
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:45:16.868536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.51
 ---- batch: 020 ----
mean loss: 38.65
 ---- batch: 030 ----
mean loss: 36.07
 ---- batch: 040 ----
mean loss: 37.64
train mean loss: 37.62
epoch train time: 0:00:00.203997
elapsed time: 0:00:50.701335
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:45:17.072645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.64
 ---- batch: 020 ----
mean loss: 43.26
 ---- batch: 030 ----
mean loss: 36.32
 ---- batch: 040 ----
mean loss: 34.50
train mean loss: 38.93
epoch train time: 0:00:00.202304
elapsed time: 0:00:50.903749
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:45:17.275088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.71
 ---- batch: 020 ----
mean loss: 37.78
 ---- batch: 030 ----
mean loss: 35.88
 ---- batch: 040 ----
mean loss: 35.38
train mean loss: 36.37
epoch train time: 0:00:00.203270
elapsed time: 0:00:51.107156
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:45:17.478466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.32
 ---- batch: 020 ----
mean loss: 34.88
 ---- batch: 030 ----
mean loss: 35.33
 ---- batch: 040 ----
mean loss: 36.24
train mean loss: 35.78
epoch train time: 0:00:00.212650
elapsed time: 0:00:51.319944
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:45:17.691299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.14
 ---- batch: 020 ----
mean loss: 35.48
 ---- batch: 030 ----
mean loss: 34.09
 ---- batch: 040 ----
mean loss: 37.08
train mean loss: 35.78
epoch train time: 0:00:00.218154
elapsed time: 0:00:51.538254
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:45:17.909580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.79
 ---- batch: 020 ----
mean loss: 35.82
 ---- batch: 030 ----
mean loss: 40.97
 ---- batch: 040 ----
mean loss: 38.22
train mean loss: 37.53
epoch train time: 0:00:00.213566
elapsed time: 0:00:51.751970
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:45:18.123283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.08
 ---- batch: 020 ----
mean loss: 39.57
 ---- batch: 030 ----
mean loss: 35.06
 ---- batch: 040 ----
mean loss: 39.05
train mean loss: 37.08
epoch train time: 0:00:00.216759
elapsed time: 0:00:51.968849
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:45:18.340165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.60
 ---- batch: 020 ----
mean loss: 34.12
 ---- batch: 030 ----
mean loss: 38.50
 ---- batch: 040 ----
mean loss: 35.40
train mean loss: 36.10
epoch train time: 0:00:00.214373
elapsed time: 0:00:52.183345
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:45:18.554660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.65
 ---- batch: 020 ----
mean loss: 36.67
 ---- batch: 030 ----
mean loss: 36.17
 ---- batch: 040 ----
mean loss: 31.46
train mean loss: 35.75
epoch train time: 0:00:00.220405
elapsed time: 0:00:52.403876
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:45:18.775193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.70
 ---- batch: 020 ----
mean loss: 39.30
 ---- batch: 030 ----
mean loss: 36.98
 ---- batch: 040 ----
mean loss: 38.19
train mean loss: 37.12
epoch train time: 0:00:00.210929
elapsed time: 0:00:52.614936
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:45:18.986251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.84
 ---- batch: 020 ----
mean loss: 34.79
 ---- batch: 030 ----
mean loss: 35.66
 ---- batch: 040 ----
mean loss: 36.95
train mean loss: 35.39
epoch train time: 0:00:00.206292
elapsed time: 0:00:52.821350
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:45:19.192662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.35
 ---- batch: 020 ----
mean loss: 35.25
 ---- batch: 030 ----
mean loss: 36.89
 ---- batch: 040 ----
mean loss: 38.66
train mean loss: 36.15
epoch train time: 0:00:00.205885
elapsed time: 0:00:53.027347
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:45:19.398659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.49
 ---- batch: 020 ----
mean loss: 36.41
 ---- batch: 030 ----
mean loss: 33.26
 ---- batch: 040 ----
mean loss: 35.68
train mean loss: 35.11
epoch train time: 0:00:00.206136
elapsed time: 0:00:53.233601
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:45:19.604915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.96
 ---- batch: 020 ----
mean loss: 33.69
 ---- batch: 030 ----
mean loss: 35.98
 ---- batch: 040 ----
mean loss: 34.65
train mean loss: 34.77
epoch train time: 0:00:00.214900
elapsed time: 0:00:53.448614
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:45:19.819924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.37
 ---- batch: 020 ----
mean loss: 34.23
 ---- batch: 030 ----
mean loss: 35.85
 ---- batch: 040 ----
mean loss: 33.52
train mean loss: 35.40
epoch train time: 0:00:00.199886
elapsed time: 0:00:53.648619
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:45:20.019932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.78
 ---- batch: 020 ----
mean loss: 34.06
 ---- batch: 030 ----
mean loss: 37.20
 ---- batch: 040 ----
mean loss: 34.67
train mean loss: 35.34
epoch train time: 0:00:00.199484
elapsed time: 0:00:53.848244
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:45:20.219566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.76
 ---- batch: 020 ----
mean loss: 36.15
 ---- batch: 030 ----
mean loss: 34.66
 ---- batch: 040 ----
mean loss: 33.41
train mean loss: 35.22
epoch train time: 0:00:00.198722
elapsed time: 0:00:54.047086
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:45:20.418397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.07
 ---- batch: 020 ----
mean loss: 34.24
 ---- batch: 030 ----
mean loss: 35.87
 ---- batch: 040 ----
mean loss: 35.53
train mean loss: 34.90
epoch train time: 0:00:00.198042
elapsed time: 0:00:54.245238
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:45:20.616547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.49
 ---- batch: 020 ----
mean loss: 35.73
 ---- batch: 030 ----
mean loss: 34.39
 ---- batch: 040 ----
mean loss: 35.25
train mean loss: 34.92
epoch train time: 0:00:00.204441
elapsed time: 0:00:54.449816
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:45:20.821131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.68
 ---- batch: 020 ----
mean loss: 34.53
 ---- batch: 030 ----
mean loss: 33.85
 ---- batch: 040 ----
mean loss: 34.22
train mean loss: 34.57
epoch train time: 0:00:00.203982
elapsed time: 0:00:54.653922
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:45:21.025235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.59
 ---- batch: 020 ----
mean loss: 33.84
 ---- batch: 030 ----
mean loss: 35.17
 ---- batch: 040 ----
mean loss: 35.31
train mean loss: 34.33
epoch train time: 0:00:00.205086
elapsed time: 0:00:54.859122
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:45:21.230438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.16
 ---- batch: 020 ----
mean loss: 34.53
 ---- batch: 030 ----
mean loss: 34.80
 ---- batch: 040 ----
mean loss: 34.68
train mean loss: 34.41
epoch train time: 0:00:00.203863
elapsed time: 0:00:55.063900
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:45:21.435242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.10
 ---- batch: 020 ----
mean loss: 34.35
 ---- batch: 030 ----
mean loss: 35.78
 ---- batch: 040 ----
mean loss: 35.56
train mean loss: 35.05
epoch train time: 0:00:00.202214
elapsed time: 0:00:55.266255
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:45:21.637582
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.17
 ---- batch: 020 ----
mean loss: 32.33
 ---- batch: 030 ----
mean loss: 33.15
 ---- batch: 040 ----
mean loss: 34.38
train mean loss: 33.00
epoch train time: 0:00:00.213038
elapsed time: 0:00:55.479436
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:45:21.850742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.72
 ---- batch: 020 ----
mean loss: 33.52
 ---- batch: 030 ----
mean loss: 33.65
 ---- batch: 040 ----
mean loss: 32.03
train mean loss: 32.73
epoch train time: 0:00:00.208850
elapsed time: 0:00:55.688398
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:45:22.059739
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.31
 ---- batch: 020 ----
mean loss: 32.92
 ---- batch: 030 ----
mean loss: 31.51
 ---- batch: 040 ----
mean loss: 31.44
train mean loss: 32.66
epoch train time: 0:00:00.209621
elapsed time: 0:00:55.898161
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:45:22.269472
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.12
 ---- batch: 020 ----
mean loss: 32.99
 ---- batch: 030 ----
mean loss: 32.99
 ---- batch: 040 ----
mean loss: 33.27
train mean loss: 32.65
epoch train time: 0:00:00.204732
elapsed time: 0:00:56.103005
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:45:22.474317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.18
 ---- batch: 020 ----
mean loss: 32.50
 ---- batch: 030 ----
mean loss: 32.75
 ---- batch: 040 ----
mean loss: 32.61
train mean loss: 32.73
epoch train time: 0:00:00.205237
elapsed time: 0:00:56.308359
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:45:22.679690
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.69
 ---- batch: 020 ----
mean loss: 32.40
 ---- batch: 030 ----
mean loss: 33.26
 ---- batch: 040 ----
mean loss: 32.16
train mean loss: 32.71
epoch train time: 0:00:00.208228
elapsed time: 0:00:56.516725
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:45:22.888039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.20
 ---- batch: 020 ----
mean loss: 31.66
 ---- batch: 030 ----
mean loss: 32.47
 ---- batch: 040 ----
mean loss: 32.41
train mean loss: 32.74
epoch train time: 0:00:00.206971
elapsed time: 0:00:56.723842
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:45:23.095159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.92
 ---- batch: 020 ----
mean loss: 32.73
 ---- batch: 030 ----
mean loss: 31.12
 ---- batch: 040 ----
mean loss: 31.35
train mean loss: 32.57
epoch train time: 0:00:00.206514
elapsed time: 0:00:56.930490
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:45:23.301818
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.80
 ---- batch: 020 ----
mean loss: 33.39
 ---- batch: 030 ----
mean loss: 31.48
 ---- batch: 040 ----
mean loss: 34.32
train mean loss: 32.63
epoch train time: 0:00:00.207918
elapsed time: 0:00:57.138566
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:45:23.509882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.44
 ---- batch: 020 ----
mean loss: 33.54
 ---- batch: 030 ----
mean loss: 32.93
 ---- batch: 040 ----
mean loss: 31.12
train mean loss: 32.63
epoch train time: 0:00:00.212904
elapsed time: 0:00:57.351596
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:45:23.722932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.90
 ---- batch: 020 ----
mean loss: 32.41
 ---- batch: 030 ----
mean loss: 31.84
 ---- batch: 040 ----
mean loss: 32.20
train mean loss: 32.61
epoch train time: 0:00:00.215348
elapsed time: 0:00:57.567083
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:45:23.938394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.65
 ---- batch: 020 ----
mean loss: 31.39
 ---- batch: 030 ----
mean loss: 31.59
 ---- batch: 040 ----
mean loss: 33.89
train mean loss: 32.60
epoch train time: 0:00:00.203314
elapsed time: 0:00:57.770521
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:45:24.141834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.07
 ---- batch: 020 ----
mean loss: 30.78
 ---- batch: 030 ----
mean loss: 31.50
 ---- batch: 040 ----
mean loss: 33.20
train mean loss: 32.57
epoch train time: 0:00:00.205231
elapsed time: 0:00:57.975870
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:45:24.347184
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.97
 ---- batch: 020 ----
mean loss: 32.72
 ---- batch: 030 ----
mean loss: 32.90
 ---- batch: 040 ----
mean loss: 32.72
train mean loss: 32.53
epoch train time: 0:00:00.203502
elapsed time: 0:00:58.179488
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:45:24.550801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.14
 ---- batch: 020 ----
mean loss: 31.80
 ---- batch: 030 ----
mean loss: 33.25
 ---- batch: 040 ----
mean loss: 33.31
train mean loss: 32.52
epoch train time: 0:00:00.202524
elapsed time: 0:00:58.382126
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:45:24.753437
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.08
 ---- batch: 020 ----
mean loss: 32.29
 ---- batch: 030 ----
mean loss: 32.57
 ---- batch: 040 ----
mean loss: 30.40
train mean loss: 32.56
epoch train time: 0:00:00.219169
elapsed time: 0:00:58.601426
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:45:24.972770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.59
 ---- batch: 020 ----
mean loss: 32.61
 ---- batch: 030 ----
mean loss: 33.08
 ---- batch: 040 ----
mean loss: 31.72
train mean loss: 32.53
epoch train time: 0:00:00.203913
elapsed time: 0:00:58.805495
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:45:25.176824
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.28
 ---- batch: 020 ----
mean loss: 34.15
 ---- batch: 030 ----
mean loss: 31.94
 ---- batch: 040 ----
mean loss: 33.28
train mean loss: 32.51
epoch train time: 0:00:00.205482
elapsed time: 0:00:59.011136
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:45:25.382450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.28
 ---- batch: 020 ----
mean loss: 33.49
 ---- batch: 030 ----
mean loss: 32.10
 ---- batch: 040 ----
mean loss: 31.62
train mean loss: 32.57
epoch train time: 0:00:00.205404
elapsed time: 0:00:59.216660
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:45:25.587974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.15
 ---- batch: 020 ----
mean loss: 32.60
 ---- batch: 030 ----
mean loss: 33.41
 ---- batch: 040 ----
mean loss: 32.59
train mean loss: 32.51
epoch train time: 0:00:00.204550
elapsed time: 0:00:59.421331
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:45:25.792658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.64
 ---- batch: 020 ----
mean loss: 31.88
 ---- batch: 030 ----
mean loss: 32.57
 ---- batch: 040 ----
mean loss: 33.82
train mean loss: 32.56
epoch train time: 0:00:00.207054
elapsed time: 0:00:59.628523
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:45:25.999834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.36
 ---- batch: 020 ----
mean loss: 33.55
 ---- batch: 030 ----
mean loss: 32.13
 ---- batch: 040 ----
mean loss: 32.90
train mean loss: 32.57
epoch train time: 0:00:00.205443
elapsed time: 0:00:59.834086
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:45:26.205401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.71
 ---- batch: 020 ----
mean loss: 31.31
 ---- batch: 030 ----
mean loss: 32.32
 ---- batch: 040 ----
mean loss: 33.32
train mean loss: 32.43
epoch train time: 0:00:00.204927
elapsed time: 0:01:00.039136
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:45:26.410450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.75
 ---- batch: 020 ----
mean loss: 33.14
 ---- batch: 030 ----
mean loss: 32.04
 ---- batch: 040 ----
mean loss: 32.12
train mean loss: 32.52
epoch train time: 0:00:00.206027
elapsed time: 0:01:00.245315
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:45:26.616631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.16
 ---- batch: 020 ----
mean loss: 32.98
 ---- batch: 030 ----
mean loss: 32.54
 ---- batch: 040 ----
mean loss: 31.87
train mean loss: 32.48
epoch train time: 0:00:00.201133
elapsed time: 0:01:00.446572
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:45:26.817887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.36
 ---- batch: 020 ----
mean loss: 32.60
 ---- batch: 030 ----
mean loss: 32.80
 ---- batch: 040 ----
mean loss: 32.83
train mean loss: 32.42
epoch train time: 0:00:00.206149
elapsed time: 0:01:00.652840
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:45:27.024153
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.51
 ---- batch: 020 ----
mean loss: 33.08
 ---- batch: 030 ----
mean loss: 32.79
 ---- batch: 040 ----
mean loss: 31.38
train mean loss: 32.32
epoch train time: 0:00:00.200564
elapsed time: 0:01:00.853522
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:45:27.224842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.08
 ---- batch: 020 ----
mean loss: 30.67
 ---- batch: 030 ----
mean loss: 33.46
 ---- batch: 040 ----
mean loss: 32.77
train mean loss: 32.43
epoch train time: 0:00:00.197321
elapsed time: 0:01:01.050970
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:45:27.422284
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.29
 ---- batch: 020 ----
mean loss: 33.21
 ---- batch: 030 ----
mean loss: 32.86
 ---- batch: 040 ----
mean loss: 33.47
train mean loss: 32.37
epoch train time: 0:00:00.198193
elapsed time: 0:01:01.249285
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:45:27.620616
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.03
 ---- batch: 020 ----
mean loss: 32.60
 ---- batch: 030 ----
mean loss: 32.63
 ---- batch: 040 ----
mean loss: 31.02
train mean loss: 32.40
epoch train time: 0:00:00.204173
elapsed time: 0:01:01.453625
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:45:27.824939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.46
 ---- batch: 020 ----
mean loss: 31.08
 ---- batch: 030 ----
mean loss: 32.01
 ---- batch: 040 ----
mean loss: 33.43
train mean loss: 32.49
epoch train time: 0:00:00.199877
elapsed time: 0:01:01.653623
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:45:28.024936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.55
 ---- batch: 020 ----
mean loss: 33.03
 ---- batch: 030 ----
mean loss: 32.98
 ---- batch: 040 ----
mean loss: 30.17
train mean loss: 32.32
epoch train time: 0:00:00.192958
elapsed time: 0:01:01.846692
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:45:28.218005
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.21
 ---- batch: 020 ----
mean loss: 34.20
 ---- batch: 030 ----
mean loss: 31.38
 ---- batch: 040 ----
mean loss: 31.43
train mean loss: 32.58
epoch train time: 0:00:00.194965
elapsed time: 0:01:02.041790
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:45:28.413102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.88
 ---- batch: 020 ----
mean loss: 32.30
 ---- batch: 030 ----
mean loss: 33.16
 ---- batch: 040 ----
mean loss: 32.33
train mean loss: 32.36
epoch train time: 0:00:00.203467
elapsed time: 0:01:02.245405
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:45:28.616759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.79
 ---- batch: 020 ----
mean loss: 31.20
 ---- batch: 030 ----
mean loss: 31.05
 ---- batch: 040 ----
mean loss: 33.26
train mean loss: 32.30
epoch train time: 0:00:00.197235
elapsed time: 0:01:02.442796
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:45:28.814108
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.12
 ---- batch: 020 ----
mean loss: 32.19
 ---- batch: 030 ----
mean loss: 32.38
 ---- batch: 040 ----
mean loss: 31.69
train mean loss: 32.20
epoch train time: 0:00:00.193843
elapsed time: 0:01:02.636765
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:45:29.008106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.40
 ---- batch: 020 ----
mean loss: 30.45
 ---- batch: 030 ----
mean loss: 33.40
 ---- batch: 040 ----
mean loss: 31.78
train mean loss: 32.23
epoch train time: 0:00:00.192422
elapsed time: 0:01:02.829361
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:45:29.200671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.75
 ---- batch: 020 ----
mean loss: 32.91
 ---- batch: 030 ----
mean loss: 31.78
 ---- batch: 040 ----
mean loss: 32.52
train mean loss: 32.26
epoch train time: 0:00:00.196468
elapsed time: 0:01:03.025936
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:45:29.397246
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.55
 ---- batch: 020 ----
mean loss: 32.19
 ---- batch: 030 ----
mean loss: 33.64
 ---- batch: 040 ----
mean loss: 33.07
train mean loss: 32.16
epoch train time: 0:00:00.191547
elapsed time: 0:01:03.217601
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:45:29.588912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.10
 ---- batch: 020 ----
mean loss: 33.61
 ---- batch: 030 ----
mean loss: 30.25
 ---- batch: 040 ----
mean loss: 32.67
train mean loss: 32.26
epoch train time: 0:00:00.192071
elapsed time: 0:01:03.409781
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:45:29.781105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.01
 ---- batch: 020 ----
mean loss: 31.76
 ---- batch: 030 ----
mean loss: 32.36
 ---- batch: 040 ----
mean loss: 33.33
train mean loss: 32.16
epoch train time: 0:00:00.201341
elapsed time: 0:01:03.611252
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:45:29.982582
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.63
 ---- batch: 020 ----
mean loss: 32.35
 ---- batch: 030 ----
mean loss: 32.37
 ---- batch: 040 ----
mean loss: 31.86
train mean loss: 32.18
epoch train time: 0:00:00.210038
elapsed time: 0:01:03.821431
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:45:30.192747
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.70
 ---- batch: 020 ----
mean loss: 31.07
 ---- batch: 030 ----
mean loss: 33.05
 ---- batch: 040 ----
mean loss: 32.16
train mean loss: 32.21
epoch train time: 0:00:00.207909
elapsed time: 0:01:04.029462
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:45:30.400788
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.80
 ---- batch: 020 ----
mean loss: 32.20
 ---- batch: 030 ----
mean loss: 33.48
 ---- batch: 040 ----
mean loss: 31.43
train mean loss: 32.09
epoch train time: 0:00:00.205622
elapsed time: 0:01:04.235246
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:45:30.606562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.75
 ---- batch: 020 ----
mean loss: 33.07
 ---- batch: 030 ----
mean loss: 31.98
 ---- batch: 040 ----
mean loss: 32.94
train mean loss: 32.16
epoch train time: 0:00:00.210793
elapsed time: 0:01:04.446204
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:45:30.817536
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.33
 ---- batch: 020 ----
mean loss: 30.77
 ---- batch: 030 ----
mean loss: 33.85
 ---- batch: 040 ----
mean loss: 34.06
train mean loss: 32.26
epoch train time: 0:00:00.203082
elapsed time: 0:01:04.649426
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:45:31.020740
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.48
 ---- batch: 020 ----
mean loss: 32.65
 ---- batch: 030 ----
mean loss: 31.71
 ---- batch: 040 ----
mean loss: 32.68
train mean loss: 32.16
epoch train time: 0:00:00.200971
elapsed time: 0:01:04.850527
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:45:31.221843
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.75
 ---- batch: 020 ----
mean loss: 31.67
 ---- batch: 030 ----
mean loss: 32.51
 ---- batch: 040 ----
mean loss: 30.83
train mean loss: 32.22
epoch train time: 0:00:00.199263
elapsed time: 0:01:05.049932
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:45:31.421247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.86
 ---- batch: 020 ----
mean loss: 32.68
 ---- batch: 030 ----
mean loss: 32.05
 ---- batch: 040 ----
mean loss: 31.10
train mean loss: 32.09
epoch train time: 0:00:00.201162
elapsed time: 0:01:05.254730
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_5/checkpoint.pth.tar
**** end time: 2019-09-20 23:45:31.626020 ****
