Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_0', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18187
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-25 17:23:27.332049 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 16, 11]           1,120
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
    BayesianConv2d-4            [-1, 14, 7, 11]             448
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
    BayesianLinear-8                    [-1, 1]             924
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,492
Trainable params: 2,492
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 17:23:27.354110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4164.29
 ---- batch: 020 ----
mean loss: 3915.09
 ---- batch: 030 ----
mean loss: 3770.53
 ---- batch: 040 ----
mean loss: 3501.72
 ---- batch: 050 ----
mean loss: 3216.09
 ---- batch: 060 ----
mean loss: 3079.48
 ---- batch: 070 ----
mean loss: 2808.92
 ---- batch: 080 ----
mean loss: 2635.66
 ---- batch: 090 ----
mean loss: 2439.26
train mean loss: 3218.12
epoch train time: 0:00:35.991512
elapsed time: 0:00:36.016809
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 17:24:03.348900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2083.43
 ---- batch: 020 ----
mean loss: 1977.81
 ---- batch: 030 ----
mean loss: 1805.23
 ---- batch: 040 ----
mean loss: 1662.34
 ---- batch: 050 ----
mean loss: 1495.74
 ---- batch: 060 ----
mean loss: 1415.42
 ---- batch: 070 ----
mean loss: 1307.64
 ---- batch: 080 ----
mean loss: 1224.23
 ---- batch: 090 ----
mean loss: 1176.70
train mean loss: 1542.82
epoch train time: 0:00:02.630750
elapsed time: 0:00:38.647922
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 17:24:05.980141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1067.00
 ---- batch: 020 ----
mean loss: 1010.13
 ---- batch: 030 ----
mean loss: 995.51
 ---- batch: 040 ----
mean loss: 980.29
 ---- batch: 050 ----
mean loss: 955.36
 ---- batch: 060 ----
mean loss: 924.32
 ---- batch: 070 ----
mean loss: 926.55
 ---- batch: 080 ----
mean loss: 896.63
 ---- batch: 090 ----
mean loss: 908.55
train mean loss: 958.93
epoch train time: 0:00:02.637164
elapsed time: 0:00:41.285582
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 17:24:08.617810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.31
 ---- batch: 020 ----
mean loss: 879.79
 ---- batch: 030 ----
mean loss: 890.30
 ---- batch: 040 ----
mean loss: 895.34
 ---- batch: 050 ----
mean loss: 871.69
 ---- batch: 060 ----
mean loss: 880.45
 ---- batch: 070 ----
mean loss: 900.51
 ---- batch: 080 ----
mean loss: 895.26
 ---- batch: 090 ----
mean loss: 880.74
train mean loss: 889.55
epoch train time: 0:00:02.619814
elapsed time: 0:00:43.905898
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 17:24:11.238126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 904.73
 ---- batch: 020 ----
mean loss: 873.96
 ---- batch: 030 ----
mean loss: 888.17
 ---- batch: 040 ----
mean loss: 884.44
 ---- batch: 050 ----
mean loss: 868.31
 ---- batch: 060 ----
mean loss: 888.60
 ---- batch: 070 ----
mean loss: 894.37
 ---- batch: 080 ----
mean loss: 893.71
 ---- batch: 090 ----
mean loss: 876.27
train mean loss: 883.86
epoch train time: 0:00:02.619883
elapsed time: 0:00:46.526347
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 17:24:13.858652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.60
 ---- batch: 020 ----
mean loss: 881.45
 ---- batch: 030 ----
mean loss: 893.35
 ---- batch: 040 ----
mean loss: 885.84
 ---- batch: 050 ----
mean loss: 878.11
 ---- batch: 060 ----
mean loss: 859.95
 ---- batch: 070 ----
mean loss: 892.46
 ---- batch: 080 ----
mean loss: 878.63
 ---- batch: 090 ----
mean loss: 870.71
train mean loss: 881.59
epoch train time: 0:00:02.618144
elapsed time: 0:00:49.145047
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 17:24:16.477271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.77
 ---- batch: 020 ----
mean loss: 881.36
 ---- batch: 030 ----
mean loss: 893.61
 ---- batch: 040 ----
mean loss: 892.57
 ---- batch: 050 ----
mean loss: 887.69
 ---- batch: 060 ----
mean loss: 875.17
 ---- batch: 070 ----
mean loss: 880.23
 ---- batch: 080 ----
mean loss: 876.43
 ---- batch: 090 ----
mean loss: 865.41
train mean loss: 879.49
epoch train time: 0:00:02.632141
elapsed time: 0:00:51.777661
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 17:24:19.110052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.85
 ---- batch: 020 ----
mean loss: 883.97
 ---- batch: 030 ----
mean loss: 912.94
 ---- batch: 040 ----
mean loss: 874.29
 ---- batch: 050 ----
mean loss: 875.41
 ---- batch: 060 ----
mean loss: 868.92
 ---- batch: 070 ----
mean loss: 890.02
 ---- batch: 080 ----
mean loss: 866.97
 ---- batch: 090 ----
mean loss: 851.49
train mean loss: 875.31
epoch train time: 0:00:02.639350
elapsed time: 0:00:54.417746
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 17:24:21.749966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.48
 ---- batch: 020 ----
mean loss: 870.95
 ---- batch: 030 ----
mean loss: 873.56
 ---- batch: 040 ----
mean loss: 901.79
 ---- batch: 050 ----
mean loss: 853.40
 ---- batch: 060 ----
mean loss: 865.52
 ---- batch: 070 ----
mean loss: 867.26
 ---- batch: 080 ----
mean loss: 860.75
 ---- batch: 090 ----
mean loss: 885.21
train mean loss: 872.52
epoch train time: 0:00:02.631168
elapsed time: 0:00:57.049375
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 17:24:24.381622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.48
 ---- batch: 020 ----
mean loss: 877.36
 ---- batch: 030 ----
mean loss: 845.72
 ---- batch: 040 ----
mean loss: 869.31
 ---- batch: 050 ----
mean loss: 881.23
 ---- batch: 060 ----
mean loss: 884.99
 ---- batch: 070 ----
mean loss: 876.37
 ---- batch: 080 ----
mean loss: 858.79
 ---- batch: 090 ----
mean loss: 864.13
train mean loss: 870.22
epoch train time: 0:00:02.628310
elapsed time: 0:00:59.678162
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 17:24:27.010414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.02
 ---- batch: 020 ----
mean loss: 875.98
 ---- batch: 030 ----
mean loss: 870.88
 ---- batch: 040 ----
mean loss: 871.73
 ---- batch: 050 ----
mean loss: 858.72
 ---- batch: 060 ----
mean loss: 869.90
 ---- batch: 070 ----
mean loss: 871.30
 ---- batch: 080 ----
mean loss: 853.07
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 865.27
epoch train time: 0:00:02.631725
elapsed time: 0:01:02.310365
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 17:24:29.642570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 825.20
 ---- batch: 020 ----
mean loss: 853.53
 ---- batch: 030 ----
mean loss: 864.44
 ---- batch: 040 ----
mean loss: 884.05
 ---- batch: 050 ----
mean loss: 884.08
 ---- batch: 060 ----
mean loss: 866.24
 ---- batch: 070 ----
mean loss: 879.14
 ---- batch: 080 ----
mean loss: 859.41
 ---- batch: 090 ----
mean loss: 855.20
train mean loss: 861.94
epoch train time: 0:00:02.626319
elapsed time: 0:01:04.937135
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 17:24:32.269353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.24
 ---- batch: 020 ----
mean loss: 863.19
 ---- batch: 030 ----
mean loss: 863.06
 ---- batch: 040 ----
mean loss: 846.53
 ---- batch: 050 ----
mean loss: 869.14
 ---- batch: 060 ----
mean loss: 867.73
 ---- batch: 070 ----
mean loss: 838.89
 ---- batch: 080 ----
mean loss: 853.88
 ---- batch: 090 ----
mean loss: 870.05
train mean loss: 858.67
epoch train time: 0:00:02.616772
elapsed time: 0:01:07.554434
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 17:24:34.886678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.00
 ---- batch: 020 ----
mean loss: 857.18
 ---- batch: 030 ----
mean loss: 851.52
 ---- batch: 040 ----
mean loss: 829.87
 ---- batch: 050 ----
mean loss: 860.05
 ---- batch: 060 ----
mean loss: 848.98
 ---- batch: 070 ----
mean loss: 875.51
 ---- batch: 080 ----
mean loss: 849.96
 ---- batch: 090 ----
mean loss: 854.08
train mean loss: 854.09
epoch train time: 0:00:02.636811
elapsed time: 0:01:10.191738
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 17:24:37.523973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.45
 ---- batch: 020 ----
mean loss: 849.12
 ---- batch: 030 ----
mean loss: 855.94
 ---- batch: 040 ----
mean loss: 850.87
 ---- batch: 050 ----
mean loss: 839.45
 ---- batch: 060 ----
mean loss: 835.62
 ---- batch: 070 ----
mean loss: 839.98
 ---- batch: 080 ----
mean loss: 864.27
 ---- batch: 090 ----
mean loss: 851.11
train mean loss: 850.44
epoch train time: 0:00:02.627525
elapsed time: 0:01:12.819741
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 17:24:40.151976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.66
 ---- batch: 020 ----
mean loss: 848.08
 ---- batch: 030 ----
mean loss: 841.21
 ---- batch: 040 ----
mean loss: 860.17
 ---- batch: 050 ----
mean loss: 853.22
 ---- batch: 060 ----
mean loss: 839.73
 ---- batch: 070 ----
mean loss: 827.05
 ---- batch: 080 ----
mean loss: 843.39
 ---- batch: 090 ----
mean loss: 849.56
train mean loss: 845.76
epoch train time: 0:00:02.648476
elapsed time: 0:01:15.468736
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 17:24:42.800972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.02
 ---- batch: 020 ----
mean loss: 823.70
 ---- batch: 030 ----
mean loss: 839.95
 ---- batch: 040 ----
mean loss: 852.18
 ---- batch: 050 ----
mean loss: 828.90
 ---- batch: 060 ----
mean loss: 850.44
 ---- batch: 070 ----
mean loss: 852.92
 ---- batch: 080 ----
mean loss: 859.37
 ---- batch: 090 ----
mean loss: 825.73
train mean loss: 843.12
epoch train time: 0:00:02.635725
elapsed time: 0:01:18.104986
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 17:24:45.437219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.91
 ---- batch: 020 ----
mean loss: 835.44
 ---- batch: 030 ----
mean loss: 849.68
 ---- batch: 040 ----
mean loss: 846.98
 ---- batch: 050 ----
mean loss: 822.82
 ---- batch: 060 ----
mean loss: 822.07
 ---- batch: 070 ----
mean loss: 839.06
 ---- batch: 080 ----
mean loss: 830.46
 ---- batch: 090 ----
mean loss: 835.45
train mean loss: 837.42
epoch train time: 0:00:02.654835
elapsed time: 0:01:20.760289
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 17:24:48.092523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.01
 ---- batch: 020 ----
mean loss: 840.68
 ---- batch: 030 ----
mean loss: 821.13
 ---- batch: 040 ----
mean loss: 836.68
 ---- batch: 050 ----
mean loss: 833.63
 ---- batch: 060 ----
mean loss: 832.46
 ---- batch: 070 ----
mean loss: 817.59
 ---- batch: 080 ----
mean loss: 840.28
 ---- batch: 090 ----
mean loss: 839.09
train mean loss: 832.89
epoch train time: 0:00:02.658560
elapsed time: 0:01:23.419352
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 17:24:50.751576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.02
 ---- batch: 020 ----
mean loss: 843.06
 ---- batch: 030 ----
mean loss: 832.72
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 817.87
 ---- batch: 060 ----
mean loss: 833.82
 ---- batch: 070 ----
mean loss: 832.27
 ---- batch: 080 ----
mean loss: 816.87
 ---- batch: 090 ----
mean loss: 819.25
train mean loss: 829.50
epoch train time: 0:00:02.674405
elapsed time: 0:01:26.094272
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 17:24:53.426575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.61
 ---- batch: 020 ----
mean loss: 821.56
 ---- batch: 030 ----
mean loss: 818.77
 ---- batch: 040 ----
mean loss: 824.78
 ---- batch: 050 ----
mean loss: 838.11
 ---- batch: 060 ----
mean loss: 822.24
 ---- batch: 070 ----
mean loss: 807.74
 ---- batch: 080 ----
mean loss: 836.31
 ---- batch: 090 ----
mean loss: 826.30
train mean loss: 824.18
epoch train time: 0:00:02.640476
elapsed time: 0:01:28.735320
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 17:24:56.067552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.80
 ---- batch: 020 ----
mean loss: 827.53
 ---- batch: 030 ----
mean loss: 813.75
 ---- batch: 040 ----
mean loss: 798.10
 ---- batch: 050 ----
mean loss: 810.82
 ---- batch: 060 ----
mean loss: 833.78
 ---- batch: 070 ----
mean loss: 821.80
 ---- batch: 080 ----
mean loss: 814.51
 ---- batch: 090 ----
mean loss: 818.05
train mean loss: 818.02
epoch train time: 0:00:02.648404
elapsed time: 0:01:31.384184
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 17:24:58.716458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.05
 ---- batch: 020 ----
mean loss: 809.50
 ---- batch: 030 ----
mean loss: 800.87
 ---- batch: 040 ----
mean loss: 805.31
 ---- batch: 050 ----
mean loss: 815.04
 ---- batch: 060 ----
mean loss: 814.78
 ---- batch: 070 ----
mean loss: 811.96
 ---- batch: 080 ----
mean loss: 814.82
 ---- batch: 090 ----
mean loss: 819.17
train mean loss: 812.42
epoch train time: 0:00:02.637868
elapsed time: 0:01:34.022561
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 17:25:01.354788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.05
 ---- batch: 020 ----
mean loss: 800.81
 ---- batch: 030 ----
mean loss: 807.32
 ---- batch: 040 ----
mean loss: 792.07
 ---- batch: 050 ----
mean loss: 803.61
 ---- batch: 060 ----
mean loss: 793.27
 ---- batch: 070 ----
mean loss: 810.89
 ---- batch: 080 ----
mean loss: 815.07
 ---- batch: 090 ----
mean loss: 802.84
train mean loss: 806.56
epoch train time: 0:00:02.639037
elapsed time: 0:01:36.662089
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 17:25:03.994319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 790.96
 ---- batch: 020 ----
mean loss: 815.96
 ---- batch: 030 ----
mean loss: 801.56
 ---- batch: 040 ----
mean loss: 805.19
 ---- batch: 050 ----
mean loss: 803.37
 ---- batch: 060 ----
mean loss: 798.50
 ---- batch: 070 ----
mean loss: 796.12
 ---- batch: 080 ----
mean loss: 786.90
 ---- batch: 090 ----
mean loss: 804.07
train mean loss: 798.40
epoch train time: 0:00:02.633264
elapsed time: 0:01:39.295865
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 17:25:06.628122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 786.70
 ---- batch: 020 ----
mean loss: 788.17
 ---- batch: 030 ----
mean loss: 786.14
 ---- batch: 040 ----
mean loss: 783.37
 ---- batch: 050 ----
mean loss: 786.45
 ---- batch: 060 ----
mean loss: 810.12
 ---- batch: 070 ----
mean loss: 798.68
 ---- batch: 080 ----
mean loss: 790.71
 ---- batch: 090 ----
mean loss: 784.16
train mean loss: 791.06
epoch train time: 0:00:02.641849
elapsed time: 0:01:41.938202
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 17:25:09.270452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 785.94
 ---- batch: 020 ----
mean loss: 792.46
 ---- batch: 030 ----
mean loss: 776.65
 ---- batch: 040 ----
mean loss: 780.05
 ---- batch: 050 ----
mean loss: 770.79
 ---- batch: 060 ----
mean loss: 780.05
 ---- batch: 070 ----
mean loss: 786.59
 ---- batch: 080 ----
mean loss: 787.30
 ---- batch: 090 ----
mean loss: 785.77
train mean loss: 781.70
epoch train time: 0:00:02.628428
elapsed time: 0:01:44.567158
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 17:25:11.899404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 790.48
 ---- batch: 020 ----
mean loss: 770.52
 ---- batch: 030 ----
mean loss: 775.71
 ---- batch: 040 ----
mean loss: 783.15
 ---- batch: 050 ----
mean loss: 778.58
 ---- batch: 060 ----
mean loss: 764.51
 ---- batch: 070 ----
mean loss: 757.41
 ---- batch: 080 ----
mean loss: 789.47
 ---- batch: 090 ----
mean loss: 754.03
train mean loss: 772.89
epoch train time: 0:00:02.619914
elapsed time: 0:01:47.187673
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 17:25:14.519904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 771.75
 ---- batch: 020 ----
mean loss: 763.86
 ---- batch: 030 ----
mean loss: 752.23
 ---- batch: 040 ----
mean loss: 764.04
 ---- batch: 050 ----
mean loss: 770.37
 ---- batch: 060 ----
mean loss: 771.45
 ---- batch: 070 ----
mean loss: 778.71
 ---- batch: 080 ----
mean loss: 756.50
 ---- batch: 090 ----
mean loss: 746.21
train mean loss: 763.84
epoch train time: 0:00:02.641562
elapsed time: 0:01:49.829707
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 17:25:17.161951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 768.35
 ---- batch: 020 ----
mean loss: 760.55
 ---- batch: 030 ----
mean loss: 754.89
 ---- batch: 040 ----
mean loss: 752.44
 ---- batch: 050 ----
mean loss: 758.36
 ---- batch: 060 ----
mean loss: 762.40
 ---- batch: 070 ----
mean loss: 749.63
 ---- batch: 080 ----
mean loss: 746.57
 ---- batch: 090 ----
mean loss: 729.23
train mean loss: 752.51
epoch train time: 0:00:02.641497
elapsed time: 0:01:52.471681
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 17:25:19.803943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 738.12
 ---- batch: 020 ----
mean loss: 740.83
 ---- batch: 030 ----
mean loss: 738.31
 ---- batch: 040 ----
mean loss: 748.82
 ---- batch: 050 ----
mean loss: 758.42
 ---- batch: 060 ----
mean loss: 731.37
 ---- batch: 070 ----
mean loss: 754.07
 ---- batch: 080 ----
mean loss: 735.30
 ---- batch: 090 ----
mean loss: 722.65
train mean loss: 740.67
epoch train time: 0:00:02.635577
elapsed time: 0:01:55.107763
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 17:25:22.440037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.96
 ---- batch: 020 ----
mean loss: 739.80
 ---- batch: 030 ----
mean loss: 732.98
 ---- batch: 040 ----
mean loss: 722.31
 ---- batch: 050 ----
mean loss: 723.79
 ---- batch: 060 ----
mean loss: 736.32
 ---- batch: 070 ----
mean loss: 714.93
 ---- batch: 080 ----
mean loss: 740.35
 ---- batch: 090 ----
mean loss: 722.14
train mean loss: 729.23
epoch train time: 0:00:02.634165
elapsed time: 0:01:57.742463
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 17:25:25.074699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 723.28
 ---- batch: 020 ----
mean loss: 723.50
 ---- batch: 030 ----
mean loss: 720.58
 ---- batch: 040 ----
mean loss: 717.26
 ---- batch: 050 ----
mean loss: 706.59
 ---- batch: 060 ----
mean loss: 712.99
 ---- batch: 070 ----
mean loss: 713.50
 ---- batch: 080 ----
mean loss: 715.84
 ---- batch: 090 ----
mean loss: 708.72
train mean loss: 716.97
epoch train time: 0:00:02.635816
elapsed time: 0:02:00.378754
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 17:25:27.710986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.19
 ---- batch: 020 ----
mean loss: 715.01
 ---- batch: 030 ----
mean loss: 708.73
 ---- batch: 040 ----
mean loss: 705.59
 ---- batch: 050 ----
mean loss: 712.13
 ---- batch: 060 ----
mean loss: 702.83
 ---- batch: 070 ----
mean loss: 704.13
 ---- batch: 080 ----
mean loss: 687.06
 ---- batch: 090 ----
mean loss: 698.07
train mean loss: 704.87
epoch train time: 0:00:02.626300
elapsed time: 0:02:03.005628
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 17:25:30.337886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.81
 ---- batch: 020 ----
mean loss: 697.62
 ---- batch: 030 ----
mean loss: 685.87
 ---- batch: 040 ----
mean loss: 692.54
 ---- batch: 050 ----
mean loss: 688.59
 ---- batch: 060 ----
mean loss: 697.93
 ---- batch: 070 ----
mean loss: 689.26
 ---- batch: 080 ----
mean loss: 686.39
 ---- batch: 090 ----
mean loss: 698.09
train mean loss: 692.44
epoch train time: 0:00:02.625067
elapsed time: 0:02:05.631210
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 17:25:32.963460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 683.05
 ---- batch: 020 ----
mean loss: 680.76
 ---- batch: 030 ----
mean loss: 688.91
 ---- batch: 040 ----
mean loss: 689.75
 ---- batch: 050 ----
mean loss: 685.74
 ---- batch: 060 ----
mean loss: 659.50
 ---- batch: 070 ----
mean loss: 664.75
 ---- batch: 080 ----
mean loss: 668.73
 ---- batch: 090 ----
mean loss: 694.37
train mean loss: 678.42
epoch train time: 0:00:02.658092
elapsed time: 0:02:08.289805
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 17:25:35.622046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 677.75
 ---- batch: 020 ----
mean loss: 670.38
 ---- batch: 030 ----
mean loss: 666.37
 ---- batch: 040 ----
mean loss: 676.80
 ---- batch: 050 ----
mean loss: 673.81
 ---- batch: 060 ----
mean loss: 661.26
 ---- batch: 070 ----
mean loss: 666.30
 ---- batch: 080 ----
mean loss: 641.91
 ---- batch: 090 ----
mean loss: 649.62
train mean loss: 664.93
epoch train time: 0:00:02.636008
elapsed time: 0:02:10.926293
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 17:25:38.258539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 647.54
 ---- batch: 020 ----
mean loss: 663.27
 ---- batch: 030 ----
mean loss: 653.25
 ---- batch: 040 ----
mean loss: 664.95
 ---- batch: 050 ----
mean loss: 651.43
 ---- batch: 060 ----
mean loss: 650.94
 ---- batch: 070 ----
mean loss: 653.66
 ---- batch: 080 ----
mean loss: 651.51
 ---- batch: 090 ----
mean loss: 631.43
train mean loss: 652.18
epoch train time: 0:00:02.618900
elapsed time: 0:02:13.545712
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 17:25:40.877944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 647.02
 ---- batch: 020 ----
mean loss: 642.38
 ---- batch: 030 ----
mean loss: 635.98
 ---- batch: 040 ----
mean loss: 643.11
 ---- batch: 050 ----
mean loss: 631.39
 ---- batch: 060 ----
mean loss: 643.77
 ---- batch: 070 ----
mean loss: 647.15
 ---- batch: 080 ----
mean loss: 632.26
 ---- batch: 090 ----
mean loss: 635.00
train mean loss: 639.65
epoch train time: 0:00:02.612861
elapsed time: 0:02:16.159056
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 17:25:43.491329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 636.27
 ---- batch: 020 ----
mean loss: 618.87
 ---- batch: 030 ----
mean loss: 621.75
 ---- batch: 040 ----
mean loss: 632.71
 ---- batch: 050 ----
mean loss: 626.27
 ---- batch: 060 ----
mean loss: 621.37
 ---- batch: 070 ----
mean loss: 617.87
 ---- batch: 080 ----
mean loss: 611.87
 ---- batch: 090 ----
mean loss: 622.93
train mean loss: 623.81
epoch train time: 0:00:02.606727
elapsed time: 0:02:18.766282
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 17:25:46.098562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 622.23
 ---- batch: 020 ----
mean loss: 612.91
 ---- batch: 030 ----
mean loss: 604.39
 ---- batch: 040 ----
mean loss: 604.28
 ---- batch: 050 ----
mean loss: 605.87
 ---- batch: 060 ----
mean loss: 606.69
 ---- batch: 070 ----
mean loss: 604.26
 ---- batch: 080 ----
mean loss: 596.63
 ---- batch: 090 ----
mean loss: 587.60
train mean loss: 604.80
epoch train time: 0:00:02.633645
elapsed time: 0:02:21.400464
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 17:25:48.732688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 583.06
 ---- batch: 020 ----
mean loss: 591.82
 ---- batch: 030 ----
mean loss: 590.50
 ---- batch: 040 ----
mean loss: 589.50
 ---- batch: 050 ----
mean loss: 576.78
 ---- batch: 060 ----
mean loss: 596.07
 ---- batch: 070 ----
mean loss: 575.87
 ---- batch: 080 ----
mean loss: 572.34
 ---- batch: 090 ----
mean loss: 583.10
train mean loss: 584.54
epoch train time: 0:00:02.627856
elapsed time: 0:02:24.028771
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 17:25:51.361041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 566.38
 ---- batch: 020 ----
mean loss: 557.57
 ---- batch: 030 ----
mean loss: 569.73
 ---- batch: 040 ----
mean loss: 545.09
 ---- batch: 050 ----
mean loss: 564.94
 ---- batch: 060 ----
mean loss: 567.62
 ---- batch: 070 ----
mean loss: 559.29
 ---- batch: 080 ----
mean loss: 569.86
 ---- batch: 090 ----
mean loss: 560.54
train mean loss: 563.50
epoch train time: 0:00:02.615858
elapsed time: 0:02:26.645172
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 17:25:53.977393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 555.85
 ---- batch: 020 ----
mean loss: 556.14
 ---- batch: 030 ----
mean loss: 552.61
 ---- batch: 040 ----
mean loss: 545.73
 ---- batch: 050 ----
mean loss: 539.16
 ---- batch: 060 ----
mean loss: 538.42
 ---- batch: 070 ----
mean loss: 538.98
 ---- batch: 080 ----
mean loss: 535.70
 ---- batch: 090 ----
mean loss: 539.93
train mean loss: 544.12
epoch train time: 0:00:02.633180
elapsed time: 0:02:29.278791
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 17:25:56.611058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.47
 ---- batch: 020 ----
mean loss: 520.49
 ---- batch: 030 ----
mean loss: 547.98
 ---- batch: 040 ----
mean loss: 521.25
 ---- batch: 050 ----
mean loss: 520.36
 ---- batch: 060 ----
mean loss: 524.70
 ---- batch: 070 ----
mean loss: 525.36
 ---- batch: 080 ----
mean loss: 509.45
 ---- batch: 090 ----
mean loss: 522.23
train mean loss: 524.46
epoch train time: 0:00:02.618241
elapsed time: 0:02:31.897515
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 17:25:59.229790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.17
 ---- batch: 020 ----
mean loss: 519.08
 ---- batch: 030 ----
mean loss: 510.25
 ---- batch: 040 ----
mean loss: 509.52
 ---- batch: 050 ----
mean loss: 499.16
 ---- batch: 060 ----
mean loss: 499.99
 ---- batch: 070 ----
mean loss: 508.85
 ---- batch: 080 ----
mean loss: 490.08
 ---- batch: 090 ----
mean loss: 500.79
train mean loss: 504.76
epoch train time: 0:00:02.638455
elapsed time: 0:02:34.536484
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 17:26:01.868728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.14
 ---- batch: 020 ----
mean loss: 493.24
 ---- batch: 030 ----
mean loss: 492.36
 ---- batch: 040 ----
mean loss: 481.94
 ---- batch: 050 ----
mean loss: 491.92
 ---- batch: 060 ----
mean loss: 492.37
 ---- batch: 070 ----
mean loss: 482.86
 ---- batch: 080 ----
mean loss: 468.46
 ---- batch: 090 ----
mean loss: 474.61
train mean loss: 486.09
epoch train time: 0:00:02.628477
elapsed time: 0:02:37.165444
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 17:26:04.497691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.20
 ---- batch: 020 ----
mean loss: 475.70
 ---- batch: 030 ----
mean loss: 474.80
 ---- batch: 040 ----
mean loss: 460.22
 ---- batch: 050 ----
mean loss: 488.03
 ---- batch: 060 ----
mean loss: 457.34
 ---- batch: 070 ----
mean loss: 468.35
 ---- batch: 080 ----
mean loss: 460.01
 ---- batch: 090 ----
mean loss: 461.57
train mean loss: 468.90
epoch train time: 0:00:02.648191
elapsed time: 0:02:39.814169
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 17:26:07.146416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.98
 ---- batch: 020 ----
mean loss: 448.40
 ---- batch: 030 ----
mean loss: 454.09
 ---- batch: 040 ----
mean loss: 444.02
 ---- batch: 050 ----
mean loss: 454.12
 ---- batch: 060 ----
mean loss: 446.95
 ---- batch: 070 ----
mean loss: 453.26
 ---- batch: 080 ----
mean loss: 452.12
 ---- batch: 090 ----
mean loss: 454.53
train mean loss: 451.33
epoch train time: 0:00:02.640928
elapsed time: 0:02:42.455622
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 17:26:09.787850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.14
 ---- batch: 020 ----
mean loss: 431.08
 ---- batch: 030 ----
mean loss: 439.32
 ---- batch: 040 ----
mean loss: 440.99
 ---- batch: 050 ----
mean loss: 424.66
 ---- batch: 060 ----
mean loss: 443.41
 ---- batch: 070 ----
mean loss: 424.01
 ---- batch: 080 ----
mean loss: 429.86
 ---- batch: 090 ----
mean loss: 425.48
train mean loss: 432.87
epoch train time: 0:00:02.650280
elapsed time: 0:02:45.106454
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 17:26:12.438687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.61
 ---- batch: 020 ----
mean loss: 420.42
 ---- batch: 030 ----
mean loss: 424.40
 ---- batch: 040 ----
mean loss: 416.80
 ---- batch: 050 ----
mean loss: 408.86
 ---- batch: 060 ----
mean loss: 411.09
 ---- batch: 070 ----
mean loss: 413.28
 ---- batch: 080 ----
mean loss: 404.70
 ---- batch: 090 ----
mean loss: 412.09
train mean loss: 414.47
epoch train time: 0:00:02.637526
elapsed time: 0:02:47.744446
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 17:26:15.076671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.87
 ---- batch: 020 ----
mean loss: 399.09
 ---- batch: 030 ----
mean loss: 405.13
 ---- batch: 040 ----
mean loss: 403.62
 ---- batch: 050 ----
mean loss: 388.66
 ---- batch: 060 ----
mean loss: 391.45
 ---- batch: 070 ----
mean loss: 391.89
 ---- batch: 080 ----
mean loss: 397.12
 ---- batch: 090 ----
mean loss: 399.11
train mean loss: 397.46
epoch train time: 0:00:02.618613
elapsed time: 0:02:50.363537
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 17:26:17.695794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.71
 ---- batch: 020 ----
mean loss: 384.91
 ---- batch: 030 ----
mean loss: 385.41
 ---- batch: 040 ----
mean loss: 373.73
 ---- batch: 050 ----
mean loss: 389.49
 ---- batch: 060 ----
mean loss: 386.05
 ---- batch: 070 ----
mean loss: 368.34
 ---- batch: 080 ----
mean loss: 378.82
 ---- batch: 090 ----
mean loss: 377.28
train mean loss: 381.05
epoch train time: 0:00:02.622731
elapsed time: 0:02:52.986751
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 17:26:20.318985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.31
 ---- batch: 020 ----
mean loss: 368.45
 ---- batch: 030 ----
mean loss: 385.48
 ---- batch: 040 ----
mean loss: 366.94
 ---- batch: 050 ----
mean loss: 366.07
 ---- batch: 060 ----
mean loss: 368.51
 ---- batch: 070 ----
mean loss: 355.88
 ---- batch: 080 ----
mean loss: 371.29
 ---- batch: 090 ----
mean loss: 349.65
train mean loss: 367.16
epoch train time: 0:00:02.613581
elapsed time: 0:02:55.600805
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 17:26:22.932971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.11
 ---- batch: 020 ----
mean loss: 349.61
 ---- batch: 030 ----
mean loss: 361.18
 ---- batch: 040 ----
mean loss: 347.15
 ---- batch: 050 ----
mean loss: 356.37
 ---- batch: 060 ----
mean loss: 360.73
 ---- batch: 070 ----
mean loss: 361.32
 ---- batch: 080 ----
mean loss: 337.95
 ---- batch: 090 ----
mean loss: 353.05
train mean loss: 353.94
epoch train time: 0:00:02.640709
elapsed time: 0:02:58.241918
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 17:26:25.574156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.51
 ---- batch: 020 ----
mean loss: 342.65
 ---- batch: 030 ----
mean loss: 345.10
 ---- batch: 040 ----
mean loss: 349.59
 ---- batch: 050 ----
mean loss: 344.31
 ---- batch: 060 ----
mean loss: 343.43
 ---- batch: 070 ----
mean loss: 352.58
 ---- batch: 080 ----
mean loss: 322.80
 ---- batch: 090 ----
mean loss: 335.12
train mean loss: 343.03
epoch train time: 0:00:02.607892
elapsed time: 0:03:00.850307
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 17:26:28.182540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.19
 ---- batch: 020 ----
mean loss: 330.28
 ---- batch: 030 ----
mean loss: 346.56
 ---- batch: 040 ----
mean loss: 331.06
 ---- batch: 050 ----
mean loss: 336.69
 ---- batch: 060 ----
mean loss: 331.39
 ---- batch: 070 ----
mean loss: 332.95
 ---- batch: 080 ----
mean loss: 329.48
 ---- batch: 090 ----
mean loss: 326.21
train mean loss: 333.29
epoch train time: 0:00:02.628586
elapsed time: 0:03:03.479469
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 17:26:30.811693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.63
 ---- batch: 020 ----
mean loss: 334.78
 ---- batch: 030 ----
mean loss: 332.88
 ---- batch: 040 ----
mean loss: 320.05
 ---- batch: 050 ----
mean loss: 332.41
 ---- batch: 060 ----
mean loss: 322.90
 ---- batch: 070 ----
mean loss: 314.10
 ---- batch: 080 ----
mean loss: 319.29
 ---- batch: 090 ----
mean loss: 318.70
train mean loss: 323.99
epoch train time: 0:00:02.618323
elapsed time: 0:03:06.098279
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 17:26:33.430508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.12
 ---- batch: 020 ----
mean loss: 311.24
 ---- batch: 030 ----
mean loss: 318.05
 ---- batch: 040 ----
mean loss: 325.06
 ---- batch: 050 ----
mean loss: 311.34
 ---- batch: 060 ----
mean loss: 312.33
 ---- batch: 070 ----
mean loss: 314.89
 ---- batch: 080 ----
mean loss: 318.71
 ---- batch: 090 ----
mean loss: 319.80
train mean loss: 316.71
epoch train time: 0:00:02.632913
elapsed time: 0:03:08.731745
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 17:26:36.063970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.25
 ---- batch: 020 ----
mean loss: 312.39
 ---- batch: 030 ----
mean loss: 306.56
 ---- batch: 040 ----
mean loss: 313.98
 ---- batch: 050 ----
mean loss: 310.16
 ---- batch: 060 ----
mean loss: 318.23
 ---- batch: 070 ----
mean loss: 305.27
 ---- batch: 080 ----
mean loss: 308.62
 ---- batch: 090 ----
mean loss: 308.33
train mean loss: 310.76
epoch train time: 0:00:02.644715
elapsed time: 0:03:11.376970
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 17:26:38.709223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.03
 ---- batch: 020 ----
mean loss: 304.15
 ---- batch: 030 ----
mean loss: 297.11
 ---- batch: 040 ----
mean loss: 311.64
 ---- batch: 050 ----
mean loss: 302.67
 ---- batch: 060 ----
mean loss: 296.65
 ---- batch: 070 ----
mean loss: 310.89
 ---- batch: 080 ----
mean loss: 310.62
 ---- batch: 090 ----
mean loss: 310.57
train mean loss: 304.56
epoch train time: 0:00:02.637598
elapsed time: 0:03:14.015045
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 17:26:41.347266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.73
 ---- batch: 020 ----
mean loss: 297.33
 ---- batch: 030 ----
mean loss: 301.80
 ---- batch: 040 ----
mean loss: 287.38
 ---- batch: 050 ----
mean loss: 300.04
 ---- batch: 060 ----
mean loss: 306.45
 ---- batch: 070 ----
mean loss: 296.32
 ---- batch: 080 ----
mean loss: 292.47
 ---- batch: 090 ----
mean loss: 309.02
train mean loss: 299.31
epoch train time: 0:00:02.632442
elapsed time: 0:03:16.647983
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 17:26:43.980210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.80
 ---- batch: 020 ----
mean loss: 285.90
 ---- batch: 030 ----
mean loss: 298.61
 ---- batch: 040 ----
mean loss: 288.78
 ---- batch: 050 ----
mean loss: 298.36
 ---- batch: 060 ----
mean loss: 295.39
 ---- batch: 070 ----
mean loss: 296.70
 ---- batch: 080 ----
mean loss: 299.07
 ---- batch: 090 ----
mean loss: 291.22
train mean loss: 294.39
epoch train time: 0:00:02.619071
elapsed time: 0:03:19.267563
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 17:26:46.599836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.54
 ---- batch: 020 ----
mean loss: 290.91
 ---- batch: 030 ----
mean loss: 291.14
 ---- batch: 040 ----
mean loss: 290.59
 ---- batch: 050 ----
mean loss: 302.06
 ---- batch: 060 ----
mean loss: 284.68
 ---- batch: 070 ----
mean loss: 284.53
 ---- batch: 080 ----
mean loss: 287.02
 ---- batch: 090 ----
mean loss: 287.64
train mean loss: 290.17
epoch train time: 0:00:02.647640
elapsed time: 0:03:21.915695
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 17:26:49.247976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.84
 ---- batch: 020 ----
mean loss: 289.83
 ---- batch: 030 ----
mean loss: 288.17
 ---- batch: 040 ----
mean loss: 293.71
 ---- batch: 050 ----
mean loss: 296.12
 ---- batch: 060 ----
mean loss: 283.65
 ---- batch: 070 ----
mean loss: 278.67
 ---- batch: 080 ----
mean loss: 283.31
 ---- batch: 090 ----
mean loss: 280.62
train mean loss: 287.07
epoch train time: 0:00:02.659086
elapsed time: 0:03:24.575314
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 17:26:51.907548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.29
 ---- batch: 020 ----
mean loss: 284.06
 ---- batch: 030 ----
mean loss: 289.90
 ---- batch: 040 ----
mean loss: 277.85
 ---- batch: 050 ----
mean loss: 281.06
 ---- batch: 060 ----
mean loss: 290.38
 ---- batch: 070 ----
mean loss: 277.70
 ---- batch: 080 ----
mean loss: 286.15
 ---- batch: 090 ----
mean loss: 282.12
train mean loss: 282.89
epoch train time: 0:00:02.623486
elapsed time: 0:03:27.199327
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 17:26:54.531550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.83
 ---- batch: 020 ----
mean loss: 274.17
 ---- batch: 030 ----
mean loss: 292.69
 ---- batch: 040 ----
mean loss: 287.97
 ---- batch: 050 ----
mean loss: 286.28
 ---- batch: 060 ----
mean loss: 279.97
 ---- batch: 070 ----
mean loss: 276.36
 ---- batch: 080 ----
mean loss: 275.60
 ---- batch: 090 ----
mean loss: 276.70
train mean loss: 280.99
epoch train time: 0:00:02.627245
elapsed time: 0:03:29.827049
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 17:26:57.159297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.80
 ---- batch: 020 ----
mean loss: 277.05
 ---- batch: 030 ----
mean loss: 268.97
 ---- batch: 040 ----
mean loss: 271.03
 ---- batch: 050 ----
mean loss: 277.28
 ---- batch: 060 ----
mean loss: 284.12
 ---- batch: 070 ----
mean loss: 288.09
 ---- batch: 080 ----
mean loss: 282.99
 ---- batch: 090 ----
mean loss: 270.52
train mean loss: 277.51
epoch train time: 0:00:02.632310
elapsed time: 0:03:32.459886
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 17:26:59.792115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.86
 ---- batch: 020 ----
mean loss: 278.26
 ---- batch: 030 ----
mean loss: 270.83
 ---- batch: 040 ----
mean loss: 271.73
 ---- batch: 050 ----
mean loss: 270.35
 ---- batch: 060 ----
mean loss: 273.93
 ---- batch: 070 ----
mean loss: 280.68
 ---- batch: 080 ----
mean loss: 272.98
 ---- batch: 090 ----
mean loss: 277.28
train mean loss: 275.37
epoch train time: 0:00:02.638529
elapsed time: 0:03:35.098906
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 17:27:02.431133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.75
 ---- batch: 020 ----
mean loss: 264.98
 ---- batch: 030 ----
mean loss: 271.09
 ---- batch: 040 ----
mean loss: 274.92
 ---- batch: 050 ----
mean loss: 271.68
 ---- batch: 060 ----
mean loss: 264.97
 ---- batch: 070 ----
mean loss: 264.23
 ---- batch: 080 ----
mean loss: 283.58
 ---- batch: 090 ----
mean loss: 280.24
train mean loss: 273.27
epoch train time: 0:00:02.628301
elapsed time: 0:03:37.727687
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 17:27:05.059968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.76
 ---- batch: 020 ----
mean loss: 268.32
 ---- batch: 030 ----
mean loss: 278.97
 ---- batch: 040 ----
mean loss: 281.93
 ---- batch: 050 ----
mean loss: 277.01
 ---- batch: 060 ----
mean loss: 266.97
 ---- batch: 070 ----
mean loss: 268.67
 ---- batch: 080 ----
mean loss: 266.88
 ---- batch: 090 ----
mean loss: 269.20
train mean loss: 270.92
epoch train time: 0:00:02.628742
elapsed time: 0:03:40.356965
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 17:27:07.689186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.60
 ---- batch: 020 ----
mean loss: 263.46
 ---- batch: 030 ----
mean loss: 278.18
 ---- batch: 040 ----
mean loss: 269.89
 ---- batch: 050 ----
mean loss: 274.05
 ---- batch: 060 ----
mean loss: 275.93
 ---- batch: 070 ----
mean loss: 258.72
 ---- batch: 080 ----
mean loss: 266.09
 ---- batch: 090 ----
mean loss: 268.50
train mean loss: 268.76
epoch train time: 0:00:02.636768
elapsed time: 0:03:42.994184
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 17:27:10.326409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.63
 ---- batch: 020 ----
mean loss: 253.68
 ---- batch: 030 ----
mean loss: 264.76
 ---- batch: 040 ----
mean loss: 269.71
 ---- batch: 050 ----
mean loss: 274.52
 ---- batch: 060 ----
mean loss: 268.13
 ---- batch: 070 ----
mean loss: 268.06
 ---- batch: 080 ----
mean loss: 271.23
 ---- batch: 090 ----
mean loss: 270.94
train mean loss: 267.23
epoch train time: 0:00:02.657131
elapsed time: 0:03:45.651789
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 17:27:12.984054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.54
 ---- batch: 020 ----
mean loss: 268.75
 ---- batch: 030 ----
mean loss: 276.15
 ---- batch: 040 ----
mean loss: 259.06
 ---- batch: 050 ----
mean loss: 260.38
 ---- batch: 060 ----
mean loss: 266.12
 ---- batch: 070 ----
mean loss: 267.37
 ---- batch: 080 ----
mean loss: 267.14
 ---- batch: 090 ----
mean loss: 264.49
train mean loss: 265.23
epoch train time: 0:00:02.632143
elapsed time: 0:03:48.284434
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 17:27:15.616702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.29
 ---- batch: 020 ----
mean loss: 264.66
 ---- batch: 030 ----
mean loss: 270.25
 ---- batch: 040 ----
mean loss: 259.16
 ---- batch: 050 ----
mean loss: 260.58
 ---- batch: 060 ----
mean loss: 265.91
 ---- batch: 070 ----
mean loss: 262.13
 ---- batch: 080 ----
mean loss: 262.76
 ---- batch: 090 ----
mean loss: 261.10
train mean loss: 263.10
epoch train time: 0:00:02.635498
elapsed time: 0:03:50.920420
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 17:27:18.252659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.24
 ---- batch: 020 ----
mean loss: 262.18
 ---- batch: 030 ----
mean loss: 263.39
 ---- batch: 040 ----
mean loss: 265.97
 ---- batch: 050 ----
mean loss: 256.63
 ---- batch: 060 ----
mean loss: 262.56
 ---- batch: 070 ----
mean loss: 265.08
 ---- batch: 080 ----
mean loss: 264.66
 ---- batch: 090 ----
mean loss: 265.01
train mean loss: 262.32
epoch train time: 0:00:02.631387
elapsed time: 0:03:53.552310
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 17:27:20.884535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.69
 ---- batch: 020 ----
mean loss: 261.62
 ---- batch: 030 ----
mean loss: 244.03
 ---- batch: 040 ----
mean loss: 261.32
 ---- batch: 050 ----
mean loss: 267.26
 ---- batch: 060 ----
mean loss: 255.01
 ---- batch: 070 ----
mean loss: 264.97
 ---- batch: 080 ----
mean loss: 267.19
 ---- batch: 090 ----
mean loss: 258.27
train mean loss: 260.78
epoch train time: 0:00:02.649686
elapsed time: 0:03:56.202450
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 17:27:23.534682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.73
 ---- batch: 020 ----
mean loss: 261.64
 ---- batch: 030 ----
mean loss: 252.98
 ---- batch: 040 ----
mean loss: 271.03
 ---- batch: 050 ----
mean loss: 256.09
 ---- batch: 060 ----
mean loss: 258.05
 ---- batch: 070 ----
mean loss: 261.43
 ---- batch: 080 ----
mean loss: 260.25
 ---- batch: 090 ----
mean loss: 258.90
train mean loss: 259.14
epoch train time: 0:00:02.621506
elapsed time: 0:03:58.824439
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 17:27:26.156654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.93
 ---- batch: 020 ----
mean loss: 254.49
 ---- batch: 030 ----
mean loss: 254.28
 ---- batch: 040 ----
mean loss: 269.45
 ---- batch: 050 ----
mean loss: 262.23
 ---- batch: 060 ----
mean loss: 254.58
 ---- batch: 070 ----
mean loss: 273.46
 ---- batch: 080 ----
mean loss: 255.47
 ---- batch: 090 ----
mean loss: 256.45
train mean loss: 258.56
epoch train time: 0:00:02.623621
elapsed time: 0:04:01.448532
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 17:27:28.780759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.12
 ---- batch: 020 ----
mean loss: 257.32
 ---- batch: 030 ----
mean loss: 256.38
 ---- batch: 040 ----
mean loss: 263.04
 ---- batch: 050 ----
mean loss: 252.89
 ---- batch: 060 ----
mean loss: 267.04
 ---- batch: 070 ----
mean loss: 243.81
 ---- batch: 080 ----
mean loss: 258.30
 ---- batch: 090 ----
mean loss: 257.36
train mean loss: 256.35
epoch train time: 0:00:02.610876
elapsed time: 0:04:04.059908
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 17:27:31.392126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.95
 ---- batch: 020 ----
mean loss: 250.46
 ---- batch: 030 ----
mean loss: 254.40
 ---- batch: 040 ----
mean loss: 253.70
 ---- batch: 050 ----
mean loss: 255.42
 ---- batch: 060 ----
mean loss: 254.91
 ---- batch: 070 ----
mean loss: 254.42
 ---- batch: 080 ----
mean loss: 256.86
 ---- batch: 090 ----
mean loss: 259.09
train mean loss: 255.32
epoch train time: 0:00:02.619690
elapsed time: 0:04:06.680088
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 17:27:34.012320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.35
 ---- batch: 020 ----
mean loss: 261.96
 ---- batch: 030 ----
mean loss: 264.19
 ---- batch: 040 ----
mean loss: 253.94
 ---- batch: 050 ----
mean loss: 262.53
 ---- batch: 060 ----
mean loss: 252.68
 ---- batch: 070 ----
mean loss: 249.70
 ---- batch: 080 ----
mean loss: 242.25
 ---- batch: 090 ----
mean loss: 254.07
train mean loss: 254.42
epoch train time: 0:00:02.617555
elapsed time: 0:04:09.298172
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 17:27:36.630417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.03
 ---- batch: 020 ----
mean loss: 246.10
 ---- batch: 030 ----
mean loss: 246.77
 ---- batch: 040 ----
mean loss: 250.80
 ---- batch: 050 ----
mean loss: 255.28
 ---- batch: 060 ----
mean loss: 260.99
 ---- batch: 070 ----
mean loss: 249.46
 ---- batch: 080 ----
mean loss: 261.01
 ---- batch: 090 ----
mean loss: 250.91
train mean loss: 252.84
epoch train time: 0:00:02.617203
elapsed time: 0:04:11.915877
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 17:27:39.248109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.93
 ---- batch: 020 ----
mean loss: 244.80
 ---- batch: 030 ----
mean loss: 254.92
 ---- batch: 040 ----
mean loss: 248.92
 ---- batch: 050 ----
mean loss: 256.31
 ---- batch: 060 ----
mean loss: 253.34
 ---- batch: 070 ----
mean loss: 251.90
 ---- batch: 080 ----
mean loss: 251.42
 ---- batch: 090 ----
mean loss: 250.32
train mean loss: 252.35
epoch train time: 0:00:02.647837
elapsed time: 0:04:14.564237
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 17:27:41.896462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.60
 ---- batch: 020 ----
mean loss: 248.08
 ---- batch: 030 ----
mean loss: 252.99
 ---- batch: 040 ----
mean loss: 239.89
 ---- batch: 050 ----
mean loss: 254.98
 ---- batch: 060 ----
mean loss: 253.64
 ---- batch: 070 ----
mean loss: 247.77
 ---- batch: 080 ----
mean loss: 250.21
 ---- batch: 090 ----
mean loss: 258.28
train mean loss: 251.04
epoch train time: 0:00:02.635290
elapsed time: 0:04:17.200037
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 17:27:44.532263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.99
 ---- batch: 020 ----
mean loss: 257.43
 ---- batch: 030 ----
mean loss: 245.39
 ---- batch: 040 ----
mean loss: 255.44
 ---- batch: 050 ----
mean loss: 247.48
 ---- batch: 060 ----
mean loss: 241.50
 ---- batch: 070 ----
mean loss: 238.77
 ---- batch: 080 ----
mean loss: 243.80
 ---- batch: 090 ----
mean loss: 258.97
train mean loss: 249.69
epoch train time: 0:00:02.629750
elapsed time: 0:04:19.830240
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 17:27:47.162475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.81
 ---- batch: 020 ----
mean loss: 249.95
 ---- batch: 030 ----
mean loss: 251.52
 ---- batch: 040 ----
mean loss: 251.27
 ---- batch: 050 ----
mean loss: 245.47
 ---- batch: 060 ----
mean loss: 246.99
 ---- batch: 070 ----
mean loss: 247.97
 ---- batch: 080 ----
mean loss: 251.62
 ---- batch: 090 ----
mean loss: 241.15
train mean loss: 248.66
epoch train time: 0:00:02.643983
elapsed time: 0:04:22.474703
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 17:27:49.806970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.35
 ---- batch: 020 ----
mean loss: 245.75
 ---- batch: 030 ----
mean loss: 253.77
 ---- batch: 040 ----
mean loss: 251.61
 ---- batch: 050 ----
mean loss: 234.44
 ---- batch: 060 ----
mean loss: 256.80
 ---- batch: 070 ----
mean loss: 245.19
 ---- batch: 080 ----
mean loss: 251.91
 ---- batch: 090 ----
mean loss: 243.88
train mean loss: 247.71
epoch train time: 0:00:02.630430
elapsed time: 0:04:25.105660
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 17:27:52.437934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.47
 ---- batch: 020 ----
mean loss: 249.27
 ---- batch: 030 ----
mean loss: 246.15
 ---- batch: 040 ----
mean loss: 243.73
 ---- batch: 050 ----
mean loss: 244.49
 ---- batch: 060 ----
mean loss: 252.19
 ---- batch: 070 ----
mean loss: 243.11
 ---- batch: 080 ----
mean loss: 246.28
 ---- batch: 090 ----
mean loss: 240.65
train mean loss: 246.81
epoch train time: 0:00:02.619460
elapsed time: 0:04:27.725659
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 17:27:55.057948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.44
 ---- batch: 020 ----
mean loss: 253.12
 ---- batch: 030 ----
mean loss: 247.55
 ---- batch: 040 ----
mean loss: 244.03
 ---- batch: 050 ----
mean loss: 247.21
 ---- batch: 060 ----
mean loss: 246.43
 ---- batch: 070 ----
mean loss: 242.70
 ---- batch: 080 ----
mean loss: 243.39
 ---- batch: 090 ----
mean loss: 245.78
train mean loss: 245.92
epoch train time: 0:00:02.618363
elapsed time: 0:04:30.344556
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 17:27:57.676794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.26
 ---- batch: 020 ----
mean loss: 242.55
 ---- batch: 030 ----
mean loss: 232.49
 ---- batch: 040 ----
mean loss: 245.47
 ---- batch: 050 ----
mean loss: 251.78
 ---- batch: 060 ----
mean loss: 255.61
 ---- batch: 070 ----
mean loss: 243.63
 ---- batch: 080 ----
mean loss: 246.87
 ---- batch: 090 ----
mean loss: 242.26
train mean loss: 244.96
epoch train time: 0:00:02.637000
elapsed time: 0:04:32.982050
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 17:28:00.314278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.36
 ---- batch: 020 ----
mean loss: 250.95
 ---- batch: 030 ----
mean loss: 243.34
 ---- batch: 040 ----
mean loss: 240.98
 ---- batch: 050 ----
mean loss: 249.58
 ---- batch: 060 ----
mean loss: 253.26
 ---- batch: 070 ----
mean loss: 242.65
 ---- batch: 080 ----
mean loss: 244.89
 ---- batch: 090 ----
mean loss: 237.81
train mean loss: 244.53
epoch train time: 0:00:02.651099
elapsed time: 0:04:35.633648
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 17:28:02.965899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.50
 ---- batch: 020 ----
mean loss: 248.93
 ---- batch: 030 ----
mean loss: 241.13
 ---- batch: 040 ----
mean loss: 245.31
 ---- batch: 050 ----
mean loss: 238.21
 ---- batch: 060 ----
mean loss: 248.24
 ---- batch: 070 ----
mean loss: 244.06
 ---- batch: 080 ----
mean loss: 247.20
 ---- batch: 090 ----
mean loss: 241.78
train mean loss: 243.98
epoch train time: 0:00:02.636105
elapsed time: 0:04:38.270240
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 17:28:05.602478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.12
 ---- batch: 020 ----
mean loss: 249.88
 ---- batch: 030 ----
mean loss: 245.17
 ---- batch: 040 ----
mean loss: 245.49
 ---- batch: 050 ----
mean loss: 239.51
 ---- batch: 060 ----
mean loss: 248.10
 ---- batch: 070 ----
mean loss: 249.06
 ---- batch: 080 ----
mean loss: 248.34
 ---- batch: 090 ----
mean loss: 232.28
train mean loss: 243.39
epoch train time: 0:00:02.642570
elapsed time: 0:04:40.913325
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 17:28:08.245586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.87
 ---- batch: 020 ----
mean loss: 236.89
 ---- batch: 030 ----
mean loss: 253.21
 ---- batch: 040 ----
mean loss: 232.38
 ---- batch: 050 ----
mean loss: 236.48
 ---- batch: 060 ----
mean loss: 249.76
 ---- batch: 070 ----
mean loss: 241.73
 ---- batch: 080 ----
mean loss: 237.82
 ---- batch: 090 ----
mean loss: 250.33
train mean loss: 242.06
epoch train time: 0:00:02.643725
elapsed time: 0:04:43.557603
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 17:28:10.889832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.25
 ---- batch: 020 ----
mean loss: 237.46
 ---- batch: 030 ----
mean loss: 243.75
 ---- batch: 040 ----
mean loss: 240.91
 ---- batch: 050 ----
mean loss: 240.92
 ---- batch: 060 ----
mean loss: 242.31
 ---- batch: 070 ----
mean loss: 237.16
 ---- batch: 080 ----
mean loss: 239.99
 ---- batch: 090 ----
mean loss: 244.39
train mean loss: 241.25
epoch train time: 0:00:02.660135
elapsed time: 0:04:46.218223
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 17:28:13.550380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.25
 ---- batch: 020 ----
mean loss: 237.08
 ---- batch: 030 ----
mean loss: 240.24
 ---- batch: 040 ----
mean loss: 241.39
 ---- batch: 050 ----
mean loss: 238.92
 ---- batch: 060 ----
mean loss: 239.59
 ---- batch: 070 ----
mean loss: 247.87
 ---- batch: 080 ----
mean loss: 238.03
 ---- batch: 090 ----
mean loss: 241.88
train mean loss: 240.84
epoch train time: 0:00:02.619312
elapsed time: 0:04:48.837926
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 17:28:16.170169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.79
 ---- batch: 020 ----
mean loss: 238.89
 ---- batch: 030 ----
mean loss: 237.78
 ---- batch: 040 ----
mean loss: 243.38
 ---- batch: 050 ----
mean loss: 239.10
 ---- batch: 060 ----
mean loss: 233.21
 ---- batch: 070 ----
mean loss: 242.54
 ---- batch: 080 ----
mean loss: 243.31
 ---- batch: 090 ----
mean loss: 244.87
train mean loss: 239.80
epoch train time: 0:00:02.636861
elapsed time: 0:04:51.475387
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 17:28:18.807611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.10
 ---- batch: 020 ----
mean loss: 244.84
 ---- batch: 030 ----
mean loss: 234.99
 ---- batch: 040 ----
mean loss: 239.98
 ---- batch: 050 ----
mean loss: 234.61
 ---- batch: 060 ----
mean loss: 244.17
 ---- batch: 070 ----
mean loss: 244.91
 ---- batch: 080 ----
mean loss: 241.18
 ---- batch: 090 ----
mean loss: 236.80
train mean loss: 239.53
epoch train time: 0:00:02.641495
elapsed time: 0:04:54.117345
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 17:28:21.449573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.81
 ---- batch: 020 ----
mean loss: 231.31
 ---- batch: 030 ----
mean loss: 238.35
 ---- batch: 040 ----
mean loss: 237.30
 ---- batch: 050 ----
mean loss: 242.14
 ---- batch: 060 ----
mean loss: 240.93
 ---- batch: 070 ----
mean loss: 236.89
 ---- batch: 080 ----
mean loss: 236.99
 ---- batch: 090 ----
mean loss: 240.22
train mean loss: 238.80
epoch train time: 0:00:02.646968
elapsed time: 0:04:56.764769
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 17:28:24.096987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.10
 ---- batch: 020 ----
mean loss: 233.91
 ---- batch: 030 ----
mean loss: 236.84
 ---- batch: 040 ----
mean loss: 235.41
 ---- batch: 050 ----
mean loss: 248.78
 ---- batch: 060 ----
mean loss: 232.46
 ---- batch: 070 ----
mean loss: 231.49
 ---- batch: 080 ----
mean loss: 237.11
 ---- batch: 090 ----
mean loss: 238.66
train mean loss: 238.24
epoch train time: 0:00:02.626827
elapsed time: 0:04:59.392123
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 17:28:26.724440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.75
 ---- batch: 020 ----
mean loss: 238.41
 ---- batch: 030 ----
mean loss: 236.20
 ---- batch: 040 ----
mean loss: 236.59
 ---- batch: 050 ----
mean loss: 243.73
 ---- batch: 060 ----
mean loss: 242.49
 ---- batch: 070 ----
mean loss: 236.40
 ---- batch: 080 ----
mean loss: 232.11
 ---- batch: 090 ----
mean loss: 234.82
train mean loss: 237.51
epoch train time: 0:00:02.659036
elapsed time: 0:05:02.051711
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 17:28:29.383936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.64
 ---- batch: 020 ----
mean loss: 234.72
 ---- batch: 030 ----
mean loss: 235.36
 ---- batch: 040 ----
mean loss: 235.45
 ---- batch: 050 ----
mean loss: 234.76
 ---- batch: 060 ----
mean loss: 241.23
 ---- batch: 070 ----
mean loss: 233.96
 ---- batch: 080 ----
mean loss: 236.60
 ---- batch: 090 ----
mean loss: 242.09
train mean loss: 237.22
epoch train time: 0:00:02.611842
elapsed time: 0:05:04.664100
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 17:28:31.996362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.56
 ---- batch: 020 ----
mean loss: 239.17
 ---- batch: 030 ----
mean loss: 234.14
 ---- batch: 040 ----
mean loss: 233.08
 ---- batch: 050 ----
mean loss: 230.91
 ---- batch: 060 ----
mean loss: 236.69
 ---- batch: 070 ----
mean loss: 243.79
 ---- batch: 080 ----
mean loss: 228.48
 ---- batch: 090 ----
mean loss: 235.79
train mean loss: 236.38
epoch train time: 0:00:02.637167
elapsed time: 0:05:07.301785
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 17:28:34.634017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.26
 ---- batch: 020 ----
mean loss: 241.28
 ---- batch: 030 ----
mean loss: 231.76
 ---- batch: 040 ----
mean loss: 236.87
 ---- batch: 050 ----
mean loss: 235.21
 ---- batch: 060 ----
mean loss: 242.07
 ---- batch: 070 ----
mean loss: 228.57
 ---- batch: 080 ----
mean loss: 234.37
 ---- batch: 090 ----
mean loss: 230.34
train mean loss: 236.18
epoch train time: 0:00:02.644359
elapsed time: 0:05:09.946650
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 17:28:37.278895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.20
 ---- batch: 020 ----
mean loss: 233.07
 ---- batch: 030 ----
mean loss: 243.24
 ---- batch: 040 ----
mean loss: 232.06
 ---- batch: 050 ----
mean loss: 225.47
 ---- batch: 060 ----
mean loss: 240.40
 ---- batch: 070 ----
mean loss: 233.26
 ---- batch: 080 ----
mean loss: 237.95
 ---- batch: 090 ----
mean loss: 238.12
train mean loss: 235.14
epoch train time: 0:00:02.632379
elapsed time: 0:05:12.579509
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 17:28:39.911734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.19
 ---- batch: 020 ----
mean loss: 238.90
 ---- batch: 030 ----
mean loss: 233.45
 ---- batch: 040 ----
mean loss: 225.14
 ---- batch: 050 ----
mean loss: 239.05
 ---- batch: 060 ----
mean loss: 238.00
 ---- batch: 070 ----
mean loss: 233.87
 ---- batch: 080 ----
mean loss: 232.88
 ---- batch: 090 ----
mean loss: 234.09
train mean loss: 234.60
epoch train time: 0:00:02.631856
elapsed time: 0:05:15.211861
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 17:28:42.544044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.18
 ---- batch: 020 ----
mean loss: 237.27
 ---- batch: 030 ----
mean loss: 232.18
 ---- batch: 040 ----
mean loss: 233.58
 ---- batch: 050 ----
mean loss: 231.55
 ---- batch: 060 ----
mean loss: 238.18
 ---- batch: 070 ----
mean loss: 235.86
 ---- batch: 080 ----
mean loss: 240.65
 ---- batch: 090 ----
mean loss: 231.67
train mean loss: 233.92
epoch train time: 0:00:02.648507
elapsed time: 0:05:17.860876
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 17:28:45.193186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.61
 ---- batch: 020 ----
mean loss: 233.75
 ---- batch: 030 ----
mean loss: 228.49
 ---- batch: 040 ----
mean loss: 235.30
 ---- batch: 050 ----
mean loss: 229.19
 ---- batch: 060 ----
mean loss: 234.21
 ---- batch: 070 ----
mean loss: 233.21
 ---- batch: 080 ----
mean loss: 235.85
 ---- batch: 090 ----
mean loss: 232.18
train mean loss: 233.52
epoch train time: 0:00:02.631734
elapsed time: 0:05:20.493160
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 17:28:47.825441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.44
 ---- batch: 020 ----
mean loss: 231.55
 ---- batch: 030 ----
mean loss: 233.96
 ---- batch: 040 ----
mean loss: 230.01
 ---- batch: 050 ----
mean loss: 232.51
 ---- batch: 060 ----
mean loss: 236.60
 ---- batch: 070 ----
mean loss: 236.04
 ---- batch: 080 ----
mean loss: 229.54
 ---- batch: 090 ----
mean loss: 230.17
train mean loss: 233.97
epoch train time: 0:00:02.631191
elapsed time: 0:05:23.124931
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 17:28:50.457165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.68
 ---- batch: 020 ----
mean loss: 231.95
 ---- batch: 030 ----
mean loss: 237.15
 ---- batch: 040 ----
mean loss: 230.92
 ---- batch: 050 ----
mean loss: 231.13
 ---- batch: 060 ----
mean loss: 238.06
 ---- batch: 070 ----
mean loss: 228.08
 ---- batch: 080 ----
mean loss: 229.23
 ---- batch: 090 ----
mean loss: 231.89
train mean loss: 232.80
epoch train time: 0:00:02.617867
elapsed time: 0:05:25.743312
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 17:28:53.075553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.23
 ---- batch: 020 ----
mean loss: 231.27
 ---- batch: 030 ----
mean loss: 237.31
 ---- batch: 040 ----
mean loss: 239.49
 ---- batch: 050 ----
mean loss: 234.37
 ---- batch: 060 ----
mean loss: 238.32
 ---- batch: 070 ----
mean loss: 238.50
 ---- batch: 080 ----
mean loss: 228.37
 ---- batch: 090 ----
mean loss: 226.17
train mean loss: 232.34
epoch train time: 0:00:02.633761
elapsed time: 0:05:28.377615
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 17:28:55.709863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.28
 ---- batch: 020 ----
mean loss: 234.74
 ---- batch: 030 ----
mean loss: 226.34
 ---- batch: 040 ----
mean loss: 224.29
 ---- batch: 050 ----
mean loss: 229.00
 ---- batch: 060 ----
mean loss: 238.74
 ---- batch: 070 ----
mean loss: 242.54
 ---- batch: 080 ----
mean loss: 231.29
 ---- batch: 090 ----
mean loss: 234.85
train mean loss: 232.42
epoch train time: 0:00:02.618991
elapsed time: 0:05:30.997159
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 17:28:58.329391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.85
 ---- batch: 020 ----
mean loss: 228.41
 ---- batch: 030 ----
mean loss: 229.40
 ---- batch: 040 ----
mean loss: 231.90
 ---- batch: 050 ----
mean loss: 228.86
 ---- batch: 060 ----
mean loss: 238.04
 ---- batch: 070 ----
mean loss: 222.88
 ---- batch: 080 ----
mean loss: 233.96
 ---- batch: 090 ----
mean loss: 232.74
train mean loss: 231.84
epoch train time: 0:00:02.637147
elapsed time: 0:05:33.634758
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 17:29:00.966998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.69
 ---- batch: 020 ----
mean loss: 230.64
 ---- batch: 030 ----
mean loss: 224.74
 ---- batch: 040 ----
mean loss: 234.35
 ---- batch: 050 ----
mean loss: 226.97
 ---- batch: 060 ----
mean loss: 232.40
 ---- batch: 070 ----
mean loss: 232.62
 ---- batch: 080 ----
mean loss: 224.53
 ---- batch: 090 ----
mean loss: 231.91
train mean loss: 230.77
epoch train time: 0:00:02.623630
elapsed time: 0:05:36.258891
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 17:29:03.591125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.89
 ---- batch: 020 ----
mean loss: 230.97
 ---- batch: 030 ----
mean loss: 226.09
 ---- batch: 040 ----
mean loss: 222.69
 ---- batch: 050 ----
mean loss: 241.36
 ---- batch: 060 ----
mean loss: 229.18
 ---- batch: 070 ----
mean loss: 238.80
 ---- batch: 080 ----
mean loss: 226.80
 ---- batch: 090 ----
mean loss: 228.98
train mean loss: 230.74
epoch train time: 0:00:02.650824
elapsed time: 0:05:38.910177
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 17:29:06.242428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.22
 ---- batch: 020 ----
mean loss: 222.26
 ---- batch: 030 ----
mean loss: 230.70
 ---- batch: 040 ----
mean loss: 225.29
 ---- batch: 050 ----
mean loss: 231.10
 ---- batch: 060 ----
mean loss: 234.22
 ---- batch: 070 ----
mean loss: 241.15
 ---- batch: 080 ----
mean loss: 239.25
 ---- batch: 090 ----
mean loss: 236.29
train mean loss: 229.85
epoch train time: 0:00:02.631308
elapsed time: 0:05:41.542033
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 17:29:08.874219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.31
 ---- batch: 020 ----
mean loss: 223.35
 ---- batch: 030 ----
mean loss: 228.66
 ---- batch: 040 ----
mean loss: 230.32
 ---- batch: 050 ----
mean loss: 231.82
 ---- batch: 060 ----
mean loss: 222.95
 ---- batch: 070 ----
mean loss: 232.37
 ---- batch: 080 ----
mean loss: 232.72
 ---- batch: 090 ----
mean loss: 238.23
train mean loss: 229.69
epoch train time: 0:00:02.631027
elapsed time: 0:05:44.173520
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 17:29:11.505737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.69
 ---- batch: 020 ----
mean loss: 233.04
 ---- batch: 030 ----
mean loss: 232.65
 ---- batch: 040 ----
mean loss: 235.16
 ---- batch: 050 ----
mean loss: 233.12
 ---- batch: 060 ----
mean loss: 230.25
 ---- batch: 070 ----
mean loss: 227.56
 ---- batch: 080 ----
mean loss: 232.13
 ---- batch: 090 ----
mean loss: 217.81
train mean loss: 229.81
epoch train time: 0:00:02.620225
elapsed time: 0:05:46.794244
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 17:29:14.126442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.38
 ---- batch: 020 ----
mean loss: 227.02
 ---- batch: 030 ----
mean loss: 225.00
 ---- batch: 040 ----
mean loss: 229.32
 ---- batch: 050 ----
mean loss: 220.81
 ---- batch: 060 ----
mean loss: 235.08
 ---- batch: 070 ----
mean loss: 226.97
 ---- batch: 080 ----
mean loss: 235.61
 ---- batch: 090 ----
mean loss: 229.84
train mean loss: 229.46
epoch train time: 0:00:02.626075
elapsed time: 0:05:49.420784
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 17:29:16.753089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.12
 ---- batch: 020 ----
mean loss: 226.54
 ---- batch: 030 ----
mean loss: 228.34
 ---- batch: 040 ----
mean loss: 236.95
 ---- batch: 050 ----
mean loss: 230.28
 ---- batch: 060 ----
mean loss: 230.58
 ---- batch: 070 ----
mean loss: 221.60
 ---- batch: 080 ----
mean loss: 231.77
 ---- batch: 090 ----
mean loss: 229.76
train mean loss: 228.68
epoch train time: 0:00:02.643361
elapsed time: 0:05:52.064689
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 17:29:19.396917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.26
 ---- batch: 020 ----
mean loss: 230.65
 ---- batch: 030 ----
mean loss: 226.23
 ---- batch: 040 ----
mean loss: 228.47
 ---- batch: 050 ----
mean loss: 230.83
 ---- batch: 060 ----
mean loss: 226.00
 ---- batch: 070 ----
mean loss: 230.23
 ---- batch: 080 ----
mean loss: 234.92
 ---- batch: 090 ----
mean loss: 229.37
train mean loss: 228.21
epoch train time: 0:00:02.641624
elapsed time: 0:05:54.706787
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 17:29:22.039009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.97
 ---- batch: 020 ----
mean loss: 230.66
 ---- batch: 030 ----
mean loss: 222.07
 ---- batch: 040 ----
mean loss: 233.56
 ---- batch: 050 ----
mean loss: 232.08
 ---- batch: 060 ----
mean loss: 226.38
 ---- batch: 070 ----
mean loss: 221.57
 ---- batch: 080 ----
mean loss: 226.82
 ---- batch: 090 ----
mean loss: 225.23
train mean loss: 227.90
epoch train time: 0:00:02.645267
elapsed time: 0:05:57.352589
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 17:29:24.684809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.27
 ---- batch: 020 ----
mean loss: 219.81
 ---- batch: 030 ----
mean loss: 223.48
 ---- batch: 040 ----
mean loss: 230.33
 ---- batch: 050 ----
mean loss: 229.30
 ---- batch: 060 ----
mean loss: 226.30
 ---- batch: 070 ----
mean loss: 230.07
 ---- batch: 080 ----
mean loss: 230.80
 ---- batch: 090 ----
mean loss: 227.18
train mean loss: 227.65
epoch train time: 0:00:02.640571
elapsed time: 0:05:59.993619
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 17:29:27.325846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.10
 ---- batch: 020 ----
mean loss: 223.17
 ---- batch: 030 ----
mean loss: 221.10
 ---- batch: 040 ----
mean loss: 228.99
 ---- batch: 050 ----
mean loss: 224.55
 ---- batch: 060 ----
mean loss: 234.89
 ---- batch: 070 ----
mean loss: 229.03
 ---- batch: 080 ----
mean loss: 227.75
 ---- batch: 090 ----
mean loss: 231.73
train mean loss: 227.05
epoch train time: 0:00:02.613648
elapsed time: 0:06:02.607791
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 17:29:29.940012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.59
 ---- batch: 020 ----
mean loss: 226.45
 ---- batch: 030 ----
mean loss: 218.63
 ---- batch: 040 ----
mean loss: 221.75
 ---- batch: 050 ----
mean loss: 228.66
 ---- batch: 060 ----
mean loss: 230.78
 ---- batch: 070 ----
mean loss: 226.16
 ---- batch: 080 ----
mean loss: 233.63
 ---- batch: 090 ----
mean loss: 232.72
train mean loss: 226.67
epoch train time: 0:00:02.635932
elapsed time: 0:06:05.244190
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 17:29:32.576411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.90
 ---- batch: 020 ----
mean loss: 222.10
 ---- batch: 030 ----
mean loss: 232.35
 ---- batch: 040 ----
mean loss: 229.51
 ---- batch: 050 ----
mean loss: 226.60
 ---- batch: 060 ----
mean loss: 224.15
 ---- batch: 070 ----
mean loss: 225.87
 ---- batch: 080 ----
mean loss: 231.71
 ---- batch: 090 ----
mean loss: 232.72
train mean loss: 226.78
epoch train time: 0:00:02.642518
elapsed time: 0:06:07.887262
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 17:29:35.219437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.94
 ---- batch: 020 ----
mean loss: 220.58
 ---- batch: 030 ----
mean loss: 224.02
 ---- batch: 040 ----
mean loss: 231.90
 ---- batch: 050 ----
mean loss: 228.02
 ---- batch: 060 ----
mean loss: 227.98
 ---- batch: 070 ----
mean loss: 217.85
 ---- batch: 080 ----
mean loss: 224.92
 ---- batch: 090 ----
mean loss: 235.71
train mean loss: 226.35
epoch train time: 0:00:02.631387
elapsed time: 0:06:10.519064
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 17:29:37.851319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.70
 ---- batch: 020 ----
mean loss: 216.93
 ---- batch: 030 ----
mean loss: 223.54
 ---- batch: 040 ----
mean loss: 224.09
 ---- batch: 050 ----
mean loss: 224.46
 ---- batch: 060 ----
mean loss: 222.32
 ---- batch: 070 ----
mean loss: 229.79
 ---- batch: 080 ----
mean loss: 232.32
 ---- batch: 090 ----
mean loss: 231.20
train mean loss: 225.73
epoch train time: 0:00:02.653110
elapsed time: 0:06:13.172682
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 17:29:40.504899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.43
 ---- batch: 020 ----
mean loss: 223.27
 ---- batch: 030 ----
mean loss: 217.57
 ---- batch: 040 ----
mean loss: 224.96
 ---- batch: 050 ----
mean loss: 219.98
 ---- batch: 060 ----
mean loss: 221.61
 ---- batch: 070 ----
mean loss: 228.33
 ---- batch: 080 ----
mean loss: 233.79
 ---- batch: 090 ----
mean loss: 228.45
train mean loss: 225.70
epoch train time: 0:00:02.653125
elapsed time: 0:06:15.826237
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 17:29:43.158499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.59
 ---- batch: 020 ----
mean loss: 226.53
 ---- batch: 030 ----
mean loss: 225.20
 ---- batch: 040 ----
mean loss: 231.69
 ---- batch: 050 ----
mean loss: 225.35
 ---- batch: 060 ----
mean loss: 222.46
 ---- batch: 070 ----
mean loss: 222.52
 ---- batch: 080 ----
mean loss: 223.83
 ---- batch: 090 ----
mean loss: 227.72
train mean loss: 225.19
epoch train time: 0:00:02.629872
elapsed time: 0:06:18.456619
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 17:29:45.788858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.61
 ---- batch: 020 ----
mean loss: 219.48
 ---- batch: 030 ----
mean loss: 223.47
 ---- batch: 040 ----
mean loss: 225.22
 ---- batch: 050 ----
mean loss: 224.47
 ---- batch: 060 ----
mean loss: 222.46
 ---- batch: 070 ----
mean loss: 223.43
 ---- batch: 080 ----
mean loss: 234.76
 ---- batch: 090 ----
mean loss: 224.36
train mean loss: 225.13
epoch train time: 0:00:02.640579
elapsed time: 0:06:21.097699
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 17:29:48.429946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.60
 ---- batch: 020 ----
mean loss: 224.80
 ---- batch: 030 ----
mean loss: 221.90
 ---- batch: 040 ----
mean loss: 220.86
 ---- batch: 050 ----
mean loss: 224.07
 ---- batch: 060 ----
mean loss: 229.18
 ---- batch: 070 ----
mean loss: 223.98
 ---- batch: 080 ----
mean loss: 227.47
 ---- batch: 090 ----
mean loss: 221.81
train mean loss: 224.77
epoch train time: 0:00:02.635297
elapsed time: 0:06:23.733522
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 17:29:51.065776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.89
 ---- batch: 020 ----
mean loss: 215.08
 ---- batch: 030 ----
mean loss: 218.46
 ---- batch: 040 ----
mean loss: 226.94
 ---- batch: 050 ----
mean loss: 228.29
 ---- batch: 060 ----
mean loss: 217.40
 ---- batch: 070 ----
mean loss: 232.17
 ---- batch: 080 ----
mean loss: 226.61
 ---- batch: 090 ----
mean loss: 223.79
train mean loss: 224.66
epoch train time: 0:00:02.647181
elapsed time: 0:06:26.381216
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 17:29:53.713439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.49
 ---- batch: 020 ----
mean loss: 226.14
 ---- batch: 030 ----
mean loss: 227.04
 ---- batch: 040 ----
mean loss: 220.55
 ---- batch: 050 ----
mean loss: 220.63
 ---- batch: 060 ----
mean loss: 232.78
 ---- batch: 070 ----
mean loss: 226.63
 ---- batch: 080 ----
mean loss: 220.99
 ---- batch: 090 ----
mean loss: 219.95
train mean loss: 224.37
epoch train time: 0:00:02.644499
elapsed time: 0:06:29.026166
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 17:29:56.358406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.42
 ---- batch: 020 ----
mean loss: 220.93
 ---- batch: 030 ----
mean loss: 219.39
 ---- batch: 040 ----
mean loss: 224.72
 ---- batch: 050 ----
mean loss: 221.91
 ---- batch: 060 ----
mean loss: 221.79
 ---- batch: 070 ----
mean loss: 224.46
 ---- batch: 080 ----
mean loss: 224.98
 ---- batch: 090 ----
mean loss: 226.87
train mean loss: 223.76
epoch train time: 0:00:02.652363
elapsed time: 0:06:31.679008
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 17:29:59.011232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.67
 ---- batch: 020 ----
mean loss: 223.09
 ---- batch: 030 ----
mean loss: 225.70
 ---- batch: 040 ----
mean loss: 221.82
 ---- batch: 050 ----
mean loss: 222.69
 ---- batch: 060 ----
mean loss: 219.83
 ---- batch: 070 ----
mean loss: 218.02
 ---- batch: 080 ----
mean loss: 226.06
 ---- batch: 090 ----
mean loss: 227.58
train mean loss: 223.36
epoch train time: 0:00:02.620193
elapsed time: 0:06:34.299694
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 17:30:01.631916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.80
 ---- batch: 020 ----
mean loss: 220.73
 ---- batch: 030 ----
mean loss: 223.31
 ---- batch: 040 ----
mean loss: 230.57
 ---- batch: 050 ----
mean loss: 222.61
 ---- batch: 060 ----
mean loss: 228.55
 ---- batch: 070 ----
mean loss: 227.18
 ---- batch: 080 ----
mean loss: 225.20
 ---- batch: 090 ----
mean loss: 218.05
train mean loss: 223.63
epoch train time: 0:00:02.619293
elapsed time: 0:06:36.919420
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 17:30:04.251640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.28
 ---- batch: 020 ----
mean loss: 222.53
 ---- batch: 030 ----
mean loss: 227.13
 ---- batch: 040 ----
mean loss: 221.20
 ---- batch: 050 ----
mean loss: 226.87
 ---- batch: 060 ----
mean loss: 218.36
 ---- batch: 070 ----
mean loss: 219.83
 ---- batch: 080 ----
mean loss: 224.65
 ---- batch: 090 ----
mean loss: 230.74
train mean loss: 223.38
epoch train time: 0:00:02.626207
elapsed time: 0:06:39.546130
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 17:30:06.878409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.90
 ---- batch: 020 ----
mean loss: 220.06
 ---- batch: 030 ----
mean loss: 228.32
 ---- batch: 040 ----
mean loss: 216.86
 ---- batch: 050 ----
mean loss: 219.08
 ---- batch: 060 ----
mean loss: 223.34
 ---- batch: 070 ----
mean loss: 225.36
 ---- batch: 080 ----
mean loss: 231.05
 ---- batch: 090 ----
mean loss: 219.48
train mean loss: 222.47
epoch train time: 0:00:02.605912
elapsed time: 0:06:42.152551
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 17:30:09.484769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.17
 ---- batch: 020 ----
mean loss: 222.57
 ---- batch: 030 ----
mean loss: 221.12
 ---- batch: 040 ----
mean loss: 226.36
 ---- batch: 050 ----
mean loss: 224.54
 ---- batch: 060 ----
mean loss: 224.27
 ---- batch: 070 ----
mean loss: 215.58
 ---- batch: 080 ----
mean loss: 221.76
 ---- batch: 090 ----
mean loss: 229.45
train mean loss: 222.81
epoch train time: 0:00:02.624383
elapsed time: 0:06:44.777397
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 17:30:12.109640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.21
 ---- batch: 020 ----
mean loss: 223.41
 ---- batch: 030 ----
mean loss: 227.23
 ---- batch: 040 ----
mean loss: 213.87
 ---- batch: 050 ----
mean loss: 225.90
 ---- batch: 060 ----
mean loss: 221.00
 ---- batch: 070 ----
mean loss: 221.29
 ---- batch: 080 ----
mean loss: 220.00
 ---- batch: 090 ----
mean loss: 224.32
train mean loss: 222.42
epoch train time: 0:00:02.601650
elapsed time: 0:06:47.379570
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 17:30:14.711788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.02
 ---- batch: 020 ----
mean loss: 218.99
 ---- batch: 030 ----
mean loss: 221.86
 ---- batch: 040 ----
mean loss: 224.87
 ---- batch: 050 ----
mean loss: 227.44
 ---- batch: 060 ----
mean loss: 225.46
 ---- batch: 070 ----
mean loss: 221.00
 ---- batch: 080 ----
mean loss: 221.30
 ---- batch: 090 ----
mean loss: 218.30
train mean loss: 222.22
epoch train time: 0:00:02.618069
elapsed time: 0:06:49.998087
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 17:30:17.330336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.77
 ---- batch: 020 ----
mean loss: 225.43
 ---- batch: 030 ----
mean loss: 213.23
 ---- batch: 040 ----
mean loss: 227.63
 ---- batch: 050 ----
mean loss: 217.45
 ---- batch: 060 ----
mean loss: 217.59
 ---- batch: 070 ----
mean loss: 223.25
 ---- batch: 080 ----
mean loss: 219.34
 ---- batch: 090 ----
mean loss: 217.32
train mean loss: 221.97
epoch train time: 0:00:02.635730
elapsed time: 0:06:52.634335
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 17:30:19.966574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.10
 ---- batch: 020 ----
mean loss: 228.66
 ---- batch: 030 ----
mean loss: 221.07
 ---- batch: 040 ----
mean loss: 223.94
 ---- batch: 050 ----
mean loss: 212.15
 ---- batch: 060 ----
mean loss: 228.42
 ---- batch: 070 ----
mean loss: 225.24
 ---- batch: 080 ----
mean loss: 217.91
 ---- batch: 090 ----
mean loss: 222.00
train mean loss: 221.65
epoch train time: 0:00:02.648385
elapsed time: 0:06:55.283216
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 17:30:22.615455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.98
 ---- batch: 020 ----
mean loss: 225.36
 ---- batch: 030 ----
mean loss: 216.40
 ---- batch: 040 ----
mean loss: 223.60
 ---- batch: 050 ----
mean loss: 223.77
 ---- batch: 060 ----
mean loss: 226.22
 ---- batch: 070 ----
mean loss: 217.66
 ---- batch: 080 ----
mean loss: 226.74
 ---- batch: 090 ----
mean loss: 221.30
train mean loss: 221.39
epoch train time: 0:00:02.624338
elapsed time: 0:06:57.908092
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 17:30:25.240362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.14
 ---- batch: 020 ----
mean loss: 222.83
 ---- batch: 030 ----
mean loss: 224.56
 ---- batch: 040 ----
mean loss: 221.14
 ---- batch: 050 ----
mean loss: 219.86
 ---- batch: 060 ----
mean loss: 219.05
 ---- batch: 070 ----
mean loss: 218.21
 ---- batch: 080 ----
mean loss: 216.91
 ---- batch: 090 ----
mean loss: 221.85
train mean loss: 221.48
epoch train time: 0:00:02.631812
elapsed time: 0:07:00.540406
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 17:30:27.872630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.78
 ---- batch: 020 ----
mean loss: 227.05
 ---- batch: 030 ----
mean loss: 220.60
 ---- batch: 040 ----
mean loss: 219.96
 ---- batch: 050 ----
mean loss: 220.76
 ---- batch: 060 ----
mean loss: 221.18
 ---- batch: 070 ----
mean loss: 211.31
 ---- batch: 080 ----
mean loss: 226.73
 ---- batch: 090 ----
mean loss: 225.14
train mean loss: 221.31
epoch train time: 0:00:02.625738
elapsed time: 0:07:03.166598
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 17:30:30.498877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.77
 ---- batch: 020 ----
mean loss: 219.98
 ---- batch: 030 ----
mean loss: 220.81
 ---- batch: 040 ----
mean loss: 220.79
 ---- batch: 050 ----
mean loss: 216.52
 ---- batch: 060 ----
mean loss: 218.62
 ---- batch: 070 ----
mean loss: 222.49
 ---- batch: 080 ----
mean loss: 222.01
 ---- batch: 090 ----
mean loss: 221.98
train mean loss: 220.69
epoch train time: 0:00:02.638978
elapsed time: 0:07:05.806149
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 17:30:33.138310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.91
 ---- batch: 020 ----
mean loss: 218.29
 ---- batch: 030 ----
mean loss: 218.53
 ---- batch: 040 ----
mean loss: 221.92
 ---- batch: 050 ----
mean loss: 224.17
 ---- batch: 060 ----
mean loss: 226.98
 ---- batch: 070 ----
mean loss: 220.92
 ---- batch: 080 ----
mean loss: 215.17
 ---- batch: 090 ----
mean loss: 224.33
train mean loss: 220.94
epoch train time: 0:00:02.627053
elapsed time: 0:07:08.433646
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 17:30:35.765888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.19
 ---- batch: 020 ----
mean loss: 223.51
 ---- batch: 030 ----
mean loss: 218.62
 ---- batch: 040 ----
mean loss: 226.17
 ---- batch: 050 ----
mean loss: 213.92
 ---- batch: 060 ----
mean loss: 220.65
 ---- batch: 070 ----
mean loss: 212.47
 ---- batch: 080 ----
mean loss: 223.60
 ---- batch: 090 ----
mean loss: 217.77
train mean loss: 220.62
epoch train time: 0:00:02.625449
elapsed time: 0:07:11.059600
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 17:30:38.391831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.52
 ---- batch: 020 ----
mean loss: 221.87
 ---- batch: 030 ----
mean loss: 218.63
 ---- batch: 040 ----
mean loss: 219.59
 ---- batch: 050 ----
mean loss: 217.54
 ---- batch: 060 ----
mean loss: 223.42
 ---- batch: 070 ----
mean loss: 219.39
 ---- batch: 080 ----
mean loss: 220.30
 ---- batch: 090 ----
mean loss: 228.93
train mean loss: 220.48
epoch train time: 0:00:02.649618
elapsed time: 0:07:13.709888
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 17:30:41.042121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.19
 ---- batch: 020 ----
mean loss: 227.34
 ---- batch: 030 ----
mean loss: 221.40
 ---- batch: 040 ----
mean loss: 214.18
 ---- batch: 050 ----
mean loss: 221.77
 ---- batch: 060 ----
mean loss: 218.15
 ---- batch: 070 ----
mean loss: 219.38
 ---- batch: 080 ----
mean loss: 219.08
 ---- batch: 090 ----
mean loss: 221.57
train mean loss: 219.67
epoch train time: 0:00:02.631589
elapsed time: 0:07:16.341980
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 17:30:43.674215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.39
 ---- batch: 020 ----
mean loss: 212.07
 ---- batch: 030 ----
mean loss: 227.16
 ---- batch: 040 ----
mean loss: 228.29
 ---- batch: 050 ----
mean loss: 219.54
 ---- batch: 060 ----
mean loss: 216.86
 ---- batch: 070 ----
mean loss: 217.77
 ---- batch: 080 ----
mean loss: 215.55
 ---- batch: 090 ----
mean loss: 215.97
train mean loss: 220.06
epoch train time: 0:00:02.635352
elapsed time: 0:07:18.977840
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 17:30:46.310073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.28
 ---- batch: 020 ----
mean loss: 217.74
 ---- batch: 030 ----
mean loss: 216.44
 ---- batch: 040 ----
mean loss: 222.60
 ---- batch: 050 ----
mean loss: 219.19
 ---- batch: 060 ----
mean loss: 223.03
 ---- batch: 070 ----
mean loss: 216.78
 ---- batch: 080 ----
mean loss: 223.43
 ---- batch: 090 ----
mean loss: 218.17
train mean loss: 219.73
epoch train time: 0:00:02.634676
elapsed time: 0:07:21.613018
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 17:30:48.945245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.59
 ---- batch: 020 ----
mean loss: 224.39
 ---- batch: 030 ----
mean loss: 222.55
 ---- batch: 040 ----
mean loss: 211.81
 ---- batch: 050 ----
mean loss: 219.32
 ---- batch: 060 ----
mean loss: 225.76
 ---- batch: 070 ----
mean loss: 218.06
 ---- batch: 080 ----
mean loss: 217.26
 ---- batch: 090 ----
mean loss: 221.88
train mean loss: 219.89
epoch train time: 0:00:02.643301
elapsed time: 0:07:24.256778
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 17:30:51.589011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.89
 ---- batch: 020 ----
mean loss: 213.59
 ---- batch: 030 ----
mean loss: 221.89
 ---- batch: 040 ----
mean loss: 226.27
 ---- batch: 050 ----
mean loss: 212.42
 ---- batch: 060 ----
mean loss: 219.58
 ---- batch: 070 ----
mean loss: 226.17
 ---- batch: 080 ----
mean loss: 230.56
 ---- batch: 090 ----
mean loss: 215.40
train mean loss: 219.40
epoch train time: 0:00:02.620510
elapsed time: 0:07:26.877761
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 17:30:54.210086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.26
 ---- batch: 020 ----
mean loss: 225.16
 ---- batch: 030 ----
mean loss: 216.11
 ---- batch: 040 ----
mean loss: 219.30
 ---- batch: 050 ----
mean loss: 225.44
 ---- batch: 060 ----
mean loss: 216.74
 ---- batch: 070 ----
mean loss: 218.40
 ---- batch: 080 ----
mean loss: 215.12
 ---- batch: 090 ----
mean loss: 217.35
train mean loss: 219.32
epoch train time: 0:00:02.615642
elapsed time: 0:07:29.494030
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 17:30:56.826278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.34
 ---- batch: 020 ----
mean loss: 213.99
 ---- batch: 030 ----
mean loss: 219.50
 ---- batch: 040 ----
mean loss: 215.25
 ---- batch: 050 ----
mean loss: 213.00
 ---- batch: 060 ----
mean loss: 214.63
 ---- batch: 070 ----
mean loss: 220.72
 ---- batch: 080 ----
mean loss: 227.65
 ---- batch: 090 ----
mean loss: 215.72
train mean loss: 219.41
epoch train time: 0:00:02.622043
elapsed time: 0:07:32.116562
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 17:30:59.448787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.36
 ---- batch: 020 ----
mean loss: 212.53
 ---- batch: 030 ----
mean loss: 225.27
 ---- batch: 040 ----
mean loss: 223.08
 ---- batch: 050 ----
mean loss: 223.23
 ---- batch: 060 ----
mean loss: 215.78
 ---- batch: 070 ----
mean loss: 220.16
 ---- batch: 080 ----
mean loss: 214.75
 ---- batch: 090 ----
mean loss: 219.69
train mean loss: 218.85
epoch train time: 0:00:02.617345
elapsed time: 0:07:34.734386
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 17:31:02.066626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.28
 ---- batch: 020 ----
mean loss: 217.12
 ---- batch: 030 ----
mean loss: 214.16
 ---- batch: 040 ----
mean loss: 221.78
 ---- batch: 050 ----
mean loss: 217.63
 ---- batch: 060 ----
mean loss: 217.80
 ---- batch: 070 ----
mean loss: 223.76
 ---- batch: 080 ----
mean loss: 221.61
 ---- batch: 090 ----
mean loss: 219.23
train mean loss: 218.95
epoch train time: 0:00:02.626558
elapsed time: 0:07:37.361424
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 17:31:04.693703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.91
 ---- batch: 020 ----
mean loss: 213.12
 ---- batch: 030 ----
mean loss: 223.08
 ---- batch: 040 ----
mean loss: 214.99
 ---- batch: 050 ----
mean loss: 212.11
 ---- batch: 060 ----
mean loss: 223.30
 ---- batch: 070 ----
mean loss: 225.85
 ---- batch: 080 ----
mean loss: 216.92
 ---- batch: 090 ----
mean loss: 216.27
train mean loss: 218.49
epoch train time: 0:00:02.607398
elapsed time: 0:07:39.969325
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 17:31:07.301546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.84
 ---- batch: 020 ----
mean loss: 221.94
 ---- batch: 030 ----
mean loss: 227.68
 ---- batch: 040 ----
mean loss: 221.19
 ---- batch: 050 ----
mean loss: 212.50
 ---- batch: 060 ----
mean loss: 221.02
 ---- batch: 070 ----
mean loss: 218.63
 ---- batch: 080 ----
mean loss: 220.97
 ---- batch: 090 ----
mean loss: 212.88
train mean loss: 218.24
epoch train time: 0:00:02.637589
elapsed time: 0:07:42.607405
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 17:31:09.939627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.69
 ---- batch: 020 ----
mean loss: 210.77
 ---- batch: 030 ----
mean loss: 218.90
 ---- batch: 040 ----
mean loss: 220.55
 ---- batch: 050 ----
mean loss: 219.65
 ---- batch: 060 ----
mean loss: 222.76
 ---- batch: 070 ----
mean loss: 218.53
 ---- batch: 080 ----
mean loss: 213.69
 ---- batch: 090 ----
mean loss: 220.46
train mean loss: 218.44
epoch train time: 0:00:02.635010
elapsed time: 0:07:45.242890
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 17:31:12.575189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.78
 ---- batch: 020 ----
mean loss: 216.92
 ---- batch: 030 ----
mean loss: 217.81
 ---- batch: 040 ----
mean loss: 210.48
 ---- batch: 050 ----
mean loss: 224.03
 ---- batch: 060 ----
mean loss: 224.32
 ---- batch: 070 ----
mean loss: 217.02
 ---- batch: 080 ----
mean loss: 213.25
 ---- batch: 090 ----
mean loss: 223.59
train mean loss: 218.12
epoch train time: 0:00:02.650962
elapsed time: 0:07:47.894398
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 17:31:15.226657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.62
 ---- batch: 020 ----
mean loss: 216.92
 ---- batch: 030 ----
mean loss: 208.97
 ---- batch: 040 ----
mean loss: 225.89
 ---- batch: 050 ----
mean loss: 219.44
 ---- batch: 060 ----
mean loss: 227.30
 ---- batch: 070 ----
mean loss: 211.09
 ---- batch: 080 ----
mean loss: 212.16
 ---- batch: 090 ----
mean loss: 221.09
train mean loss: 217.99
epoch train time: 0:00:02.649113
elapsed time: 0:07:50.544024
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 17:31:17.876248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.16
 ---- batch: 020 ----
mean loss: 216.74
 ---- batch: 030 ----
mean loss: 220.69
 ---- batch: 040 ----
mean loss: 214.80
 ---- batch: 050 ----
mean loss: 218.77
 ---- batch: 060 ----
mean loss: 213.51
 ---- batch: 070 ----
mean loss: 208.45
 ---- batch: 080 ----
mean loss: 218.55
 ---- batch: 090 ----
mean loss: 225.03
train mean loss: 217.97
epoch train time: 0:00:02.642633
elapsed time: 0:07:53.187121
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 17:31:20.519352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.76
 ---- batch: 020 ----
mean loss: 215.52
 ---- batch: 030 ----
mean loss: 223.49
 ---- batch: 040 ----
mean loss: 210.57
 ---- batch: 050 ----
mean loss: 217.92
 ---- batch: 060 ----
mean loss: 211.76
 ---- batch: 070 ----
mean loss: 221.64
 ---- batch: 080 ----
mean loss: 216.17
 ---- batch: 090 ----
mean loss: 218.71
train mean loss: 217.75
epoch train time: 0:00:02.648913
elapsed time: 0:07:55.836486
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 17:31:23.168716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.90
 ---- batch: 020 ----
mean loss: 217.90
 ---- batch: 030 ----
mean loss: 210.43
 ---- batch: 040 ----
mean loss: 215.91
 ---- batch: 050 ----
mean loss: 219.98
 ---- batch: 060 ----
mean loss: 222.46
 ---- batch: 070 ----
mean loss: 213.01
 ---- batch: 080 ----
mean loss: 220.34
 ---- batch: 090 ----
mean loss: 217.24
train mean loss: 217.74
epoch train time: 0:00:02.672290
elapsed time: 0:07:58.509256
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 17:31:25.841485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.44
 ---- batch: 020 ----
mean loss: 213.51
 ---- batch: 030 ----
mean loss: 215.79
 ---- batch: 040 ----
mean loss: 221.54
 ---- batch: 050 ----
mean loss: 213.79
 ---- batch: 060 ----
mean loss: 219.78
 ---- batch: 070 ----
mean loss: 223.28
 ---- batch: 080 ----
mean loss: 217.31
 ---- batch: 090 ----
mean loss: 215.75
train mean loss: 217.51
epoch train time: 0:00:02.660696
elapsed time: 0:08:01.170436
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 17:31:28.502670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.54
 ---- batch: 020 ----
mean loss: 217.23
 ---- batch: 030 ----
mean loss: 216.83
 ---- batch: 040 ----
mean loss: 218.57
 ---- batch: 050 ----
mean loss: 224.04
 ---- batch: 060 ----
mean loss: 217.11
 ---- batch: 070 ----
mean loss: 218.03
 ---- batch: 080 ----
mean loss: 207.21
 ---- batch: 090 ----
mean loss: 214.82
train mean loss: 217.01
epoch train time: 0:00:02.630610
elapsed time: 0:08:03.801636
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 17:31:31.133872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.48
 ---- batch: 020 ----
mean loss: 213.46
 ---- batch: 030 ----
mean loss: 210.26
 ---- batch: 040 ----
mean loss: 217.97
 ---- batch: 050 ----
mean loss: 216.77
 ---- batch: 060 ----
mean loss: 219.52
 ---- batch: 070 ----
mean loss: 225.63
 ---- batch: 080 ----
mean loss: 212.30
 ---- batch: 090 ----
mean loss: 221.32
train mean loss: 217.13
epoch train time: 0:00:02.639578
elapsed time: 0:08:06.441802
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 17:31:33.774076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.20
 ---- batch: 020 ----
mean loss: 207.63
 ---- batch: 030 ----
mean loss: 220.34
 ---- batch: 040 ----
mean loss: 220.22
 ---- batch: 050 ----
mean loss: 215.37
 ---- batch: 060 ----
mean loss: 208.74
 ---- batch: 070 ----
mean loss: 217.56
 ---- batch: 080 ----
mean loss: 220.43
 ---- batch: 090 ----
mean loss: 214.71
train mean loss: 216.93
epoch train time: 0:00:02.643553
elapsed time: 0:08:09.085864
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 17:31:36.418099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.56
 ---- batch: 020 ----
mean loss: 212.46
 ---- batch: 030 ----
mean loss: 225.09
 ---- batch: 040 ----
mean loss: 212.73
 ---- batch: 050 ----
mean loss: 213.41
 ---- batch: 060 ----
mean loss: 219.66
 ---- batch: 070 ----
mean loss: 217.44
 ---- batch: 080 ----
mean loss: 223.03
 ---- batch: 090 ----
mean loss: 214.27
train mean loss: 216.88
epoch train time: 0:00:02.634566
elapsed time: 0:08:11.721125
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 17:31:39.053236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.01
 ---- batch: 020 ----
mean loss: 219.98
 ---- batch: 030 ----
mean loss: 227.26
 ---- batch: 040 ----
mean loss: 210.78
 ---- batch: 050 ----
mean loss: 212.42
 ---- batch: 060 ----
mean loss: 219.19
 ---- batch: 070 ----
mean loss: 208.87
 ---- batch: 080 ----
mean loss: 217.53
 ---- batch: 090 ----
mean loss: 218.18
train mean loss: 216.57
epoch train time: 0:00:02.645991
elapsed time: 0:08:14.367500
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 17:31:41.699715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.23
 ---- batch: 020 ----
mean loss: 210.31
 ---- batch: 030 ----
mean loss: 215.79
 ---- batch: 040 ----
mean loss: 220.88
 ---- batch: 050 ----
mean loss: 216.66
 ---- batch: 060 ----
mean loss: 218.20
 ---- batch: 070 ----
mean loss: 222.27
 ---- batch: 080 ----
mean loss: 210.36
 ---- batch: 090 ----
mean loss: 215.12
train mean loss: 216.35
epoch train time: 0:00:02.625633
elapsed time: 0:08:16.993634
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 17:31:44.325873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.83
 ---- batch: 020 ----
mean loss: 217.54
 ---- batch: 030 ----
mean loss: 214.87
 ---- batch: 040 ----
mean loss: 218.57
 ---- batch: 050 ----
mean loss: 223.23
 ---- batch: 060 ----
mean loss: 214.24
 ---- batch: 070 ----
mean loss: 210.07
 ---- batch: 080 ----
mean loss: 208.49
 ---- batch: 090 ----
mean loss: 222.27
train mean loss: 216.14
epoch train time: 0:00:02.665243
elapsed time: 0:08:19.659371
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 17:31:46.991611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.45
 ---- batch: 020 ----
mean loss: 221.63
 ---- batch: 030 ----
mean loss: 216.98
 ---- batch: 040 ----
mean loss: 210.68
 ---- batch: 050 ----
mean loss: 211.54
 ---- batch: 060 ----
mean loss: 219.02
 ---- batch: 070 ----
mean loss: 219.16
 ---- batch: 080 ----
mean loss: 216.01
 ---- batch: 090 ----
mean loss: 209.49
train mean loss: 215.97
epoch train time: 0:00:02.650808
elapsed time: 0:08:22.310674
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 17:31:49.642892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.49
 ---- batch: 020 ----
mean loss: 219.15
 ---- batch: 030 ----
mean loss: 218.17
 ---- batch: 040 ----
mean loss: 218.59
 ---- batch: 050 ----
mean loss: 210.07
 ---- batch: 060 ----
mean loss: 220.08
 ---- batch: 070 ----
mean loss: 211.97
 ---- batch: 080 ----
mean loss: 209.83
 ---- batch: 090 ----
mean loss: 213.71
train mean loss: 215.96
epoch train time: 0:00:02.625647
elapsed time: 0:08:24.936787
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 17:31:52.269018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.97
 ---- batch: 020 ----
mean loss: 215.83
 ---- batch: 030 ----
mean loss: 218.36
 ---- batch: 040 ----
mean loss: 215.61
 ---- batch: 050 ----
mean loss: 220.61
 ---- batch: 060 ----
mean loss: 216.09
 ---- batch: 070 ----
mean loss: 210.24
 ---- batch: 080 ----
mean loss: 216.31
 ---- batch: 090 ----
mean loss: 214.10
train mean loss: 216.09
epoch train time: 0:00:02.651924
elapsed time: 0:08:27.589228
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 17:31:54.921482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.97
 ---- batch: 020 ----
mean loss: 215.03
 ---- batch: 030 ----
mean loss: 214.63
 ---- batch: 040 ----
mean loss: 226.37
 ---- batch: 050 ----
mean loss: 214.27
 ---- batch: 060 ----
mean loss: 212.11
 ---- batch: 070 ----
mean loss: 212.87
 ---- batch: 080 ----
mean loss: 216.67
 ---- batch: 090 ----
mean loss: 214.35
train mean loss: 216.06
epoch train time: 0:00:02.643811
elapsed time: 0:08:30.233528
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 17:31:57.565773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.18
 ---- batch: 020 ----
mean loss: 222.37
 ---- batch: 030 ----
mean loss: 216.68
 ---- batch: 040 ----
mean loss: 215.57
 ---- batch: 050 ----
mean loss: 210.67
 ---- batch: 060 ----
mean loss: 214.60
 ---- batch: 070 ----
mean loss: 217.41
 ---- batch: 080 ----
mean loss: 214.77
 ---- batch: 090 ----
mean loss: 213.11
train mean loss: 215.52
epoch train time: 0:00:02.615456
elapsed time: 0:08:32.849480
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 17:32:00.181718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.44
 ---- batch: 020 ----
mean loss: 216.11
 ---- batch: 030 ----
mean loss: 221.39
 ---- batch: 040 ----
mean loss: 216.60
 ---- batch: 050 ----
mean loss: 210.62
 ---- batch: 060 ----
mean loss: 217.93
 ---- batch: 070 ----
mean loss: 213.77
 ---- batch: 080 ----
mean loss: 218.11
 ---- batch: 090 ----
mean loss: 203.38
train mean loss: 215.20
epoch train time: 0:00:02.635999
elapsed time: 0:08:35.485985
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 17:32:02.818214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.34
 ---- batch: 020 ----
mean loss: 215.12
 ---- batch: 030 ----
mean loss: 209.96
 ---- batch: 040 ----
mean loss: 218.61
 ---- batch: 050 ----
mean loss: 217.60
 ---- batch: 060 ----
mean loss: 217.39
 ---- batch: 070 ----
mean loss: 216.32
 ---- batch: 080 ----
mean loss: 217.59
 ---- batch: 090 ----
mean loss: 212.23
train mean loss: 215.40
epoch train time: 0:00:02.663014
elapsed time: 0:08:38.149461
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 17:32:05.481718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.62
 ---- batch: 020 ----
mean loss: 214.19
 ---- batch: 030 ----
mean loss: 214.75
 ---- batch: 040 ----
mean loss: 216.80
 ---- batch: 050 ----
mean loss: 217.27
 ---- batch: 060 ----
mean loss: 220.74
 ---- batch: 070 ----
mean loss: 213.22
 ---- batch: 080 ----
mean loss: 209.03
 ---- batch: 090 ----
mean loss: 214.42
train mean loss: 215.50
epoch train time: 0:00:02.658602
elapsed time: 0:08:40.808550
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 17:32:08.140792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.96
 ---- batch: 020 ----
mean loss: 209.61
 ---- batch: 030 ----
mean loss: 210.88
 ---- batch: 040 ----
mean loss: 222.66
 ---- batch: 050 ----
mean loss: 226.19
 ---- batch: 060 ----
mean loss: 214.11
 ---- batch: 070 ----
mean loss: 212.29
 ---- batch: 080 ----
mean loss: 213.21
 ---- batch: 090 ----
mean loss: 216.95
train mean loss: 215.17
epoch train time: 0:00:02.680566
elapsed time: 0:08:43.489649
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 17:32:10.821872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.30
 ---- batch: 020 ----
mean loss: 211.06
 ---- batch: 030 ----
mean loss: 217.65
 ---- batch: 040 ----
mean loss: 218.20
 ---- batch: 050 ----
mean loss: 209.88
 ---- batch: 060 ----
mean loss: 216.83
 ---- batch: 070 ----
mean loss: 218.13
 ---- batch: 080 ----
mean loss: 210.86
 ---- batch: 090 ----
mean loss: 220.47
train mean loss: 215.12
epoch train time: 0:00:02.645660
elapsed time: 0:08:46.135790
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 17:32:13.468022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.55
 ---- batch: 020 ----
mean loss: 217.64
 ---- batch: 030 ----
mean loss: 211.10
 ---- batch: 040 ----
mean loss: 209.00
 ---- batch: 050 ----
mean loss: 219.00
 ---- batch: 060 ----
mean loss: 215.34
 ---- batch: 070 ----
mean loss: 212.53
 ---- batch: 080 ----
mean loss: 217.43
 ---- batch: 090 ----
mean loss: 215.38
train mean loss: 214.52
epoch train time: 0:00:02.662901
elapsed time: 0:08:48.799192
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 17:32:16.131446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.67
 ---- batch: 020 ----
mean loss: 218.17
 ---- batch: 030 ----
mean loss: 211.22
 ---- batch: 040 ----
mean loss: 211.79
 ---- batch: 050 ----
mean loss: 215.83
 ---- batch: 060 ----
mean loss: 212.55
 ---- batch: 070 ----
mean loss: 216.56
 ---- batch: 080 ----
mean loss: 219.31
 ---- batch: 090 ----
mean loss: 213.56
train mean loss: 214.62
epoch train time: 0:00:02.649305
elapsed time: 0:08:51.449010
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 17:32:18.781234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.57
 ---- batch: 020 ----
mean loss: 212.67
 ---- batch: 030 ----
mean loss: 222.54
 ---- batch: 040 ----
mean loss: 209.62
 ---- batch: 050 ----
mean loss: 214.08
 ---- batch: 060 ----
mean loss: 205.52
 ---- batch: 070 ----
mean loss: 207.93
 ---- batch: 080 ----
mean loss: 222.02
 ---- batch: 090 ----
mean loss: 220.35
train mean loss: 214.41
epoch train time: 0:00:02.653079
elapsed time: 0:08:54.102543
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 17:32:21.434768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.53
 ---- batch: 020 ----
mean loss: 213.92
 ---- batch: 030 ----
mean loss: 219.82
 ---- batch: 040 ----
mean loss: 216.91
 ---- batch: 050 ----
mean loss: 218.34
 ---- batch: 060 ----
mean loss: 212.41
 ---- batch: 070 ----
mean loss: 214.13
 ---- batch: 080 ----
mean loss: 221.21
 ---- batch: 090 ----
mean loss: 205.85
train mean loss: 214.59
epoch train time: 0:00:02.649616
elapsed time: 0:08:56.752616
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 17:32:24.084845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.58
 ---- batch: 020 ----
mean loss: 215.82
 ---- batch: 030 ----
mean loss: 215.78
 ---- batch: 040 ----
mean loss: 209.63
 ---- batch: 050 ----
mean loss: 219.46
 ---- batch: 060 ----
mean loss: 214.58
 ---- batch: 070 ----
mean loss: 215.52
 ---- batch: 080 ----
mean loss: 211.91
 ---- batch: 090 ----
mean loss: 210.67
train mean loss: 214.09
epoch train time: 0:00:02.654650
elapsed time: 0:08:59.407743
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 17:32:26.739982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.54
 ---- batch: 020 ----
mean loss: 215.11
 ---- batch: 030 ----
mean loss: 210.24
 ---- batch: 040 ----
mean loss: 214.94
 ---- batch: 050 ----
mean loss: 213.33
 ---- batch: 060 ----
mean loss: 216.67
 ---- batch: 070 ----
mean loss: 209.53
 ---- batch: 080 ----
mean loss: 210.71
 ---- batch: 090 ----
mean loss: 215.09
train mean loss: 213.77
epoch train time: 0:00:02.656730
elapsed time: 0:09:02.064971
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 17:32:29.397203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.05
 ---- batch: 020 ----
mean loss: 210.11
 ---- batch: 030 ----
mean loss: 215.40
 ---- batch: 040 ----
mean loss: 212.55
 ---- batch: 050 ----
mean loss: 211.11
 ---- batch: 060 ----
mean loss: 205.48
 ---- batch: 070 ----
mean loss: 219.37
 ---- batch: 080 ----
mean loss: 217.83
 ---- batch: 090 ----
mean loss: 217.90
train mean loss: 214.06
epoch train time: 0:00:02.641625
elapsed time: 0:09:04.707099
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 17:32:32.039339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.17
 ---- batch: 020 ----
mean loss: 210.04
 ---- batch: 030 ----
mean loss: 212.57
 ---- batch: 040 ----
mean loss: 216.48
 ---- batch: 050 ----
mean loss: 208.04
 ---- batch: 060 ----
mean loss: 217.78
 ---- batch: 070 ----
mean loss: 218.94
 ---- batch: 080 ----
mean loss: 213.28
 ---- batch: 090 ----
mean loss: 212.02
train mean loss: 214.12
epoch train time: 0:00:02.640495
elapsed time: 0:09:07.348063
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 17:32:34.680303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.43
 ---- batch: 020 ----
mean loss: 214.27
 ---- batch: 030 ----
mean loss: 221.86
 ---- batch: 040 ----
mean loss: 211.16
 ---- batch: 050 ----
mean loss: 213.10
 ---- batch: 060 ----
mean loss: 217.09
 ---- batch: 070 ----
mean loss: 216.41
 ---- batch: 080 ----
mean loss: 214.63
 ---- batch: 090 ----
mean loss: 209.85
train mean loss: 214.17
epoch train time: 0:00:02.666818
elapsed time: 0:09:10.015389
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 17:32:37.347609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.27
 ---- batch: 020 ----
mean loss: 213.11
 ---- batch: 030 ----
mean loss: 212.10
 ---- batch: 040 ----
mean loss: 217.03
 ---- batch: 050 ----
mean loss: 211.89
 ---- batch: 060 ----
mean loss: 211.92
 ---- batch: 070 ----
mean loss: 212.16
 ---- batch: 080 ----
mean loss: 208.65
 ---- batch: 090 ----
mean loss: 217.99
train mean loss: 213.59
epoch train time: 0:00:02.646460
elapsed time: 0:09:12.662343
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 17:32:39.994579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.36
 ---- batch: 020 ----
mean loss: 216.54
 ---- batch: 030 ----
mean loss: 214.10
 ---- batch: 040 ----
mean loss: 212.20
 ---- batch: 050 ----
mean loss: 212.58
 ---- batch: 060 ----
mean loss: 217.23
 ---- batch: 070 ----
mean loss: 208.16
 ---- batch: 080 ----
mean loss: 209.89
 ---- batch: 090 ----
mean loss: 219.86
train mean loss: 213.48
epoch train time: 0:00:02.645875
elapsed time: 0:09:15.308732
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 17:32:42.640962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.59
 ---- batch: 020 ----
mean loss: 208.90
 ---- batch: 030 ----
mean loss: 210.88
 ---- batch: 040 ----
mean loss: 217.62
 ---- batch: 050 ----
mean loss: 221.46
 ---- batch: 060 ----
mean loss: 213.83
 ---- batch: 070 ----
mean loss: 215.84
 ---- batch: 080 ----
mean loss: 218.37
 ---- batch: 090 ----
mean loss: 206.32
train mean loss: 213.49
epoch train time: 0:00:02.634265
elapsed time: 0:09:17.943486
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 17:32:45.275724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.53
 ---- batch: 020 ----
mean loss: 202.94
 ---- batch: 030 ----
mean loss: 216.98
 ---- batch: 040 ----
mean loss: 212.99
 ---- batch: 050 ----
mean loss: 216.24
 ---- batch: 060 ----
mean loss: 218.00
 ---- batch: 070 ----
mean loss: 210.52
 ---- batch: 080 ----
mean loss: 217.94
 ---- batch: 090 ----
mean loss: 209.91
train mean loss: 213.11
epoch train time: 0:00:02.665353
elapsed time: 0:09:20.609347
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 17:32:47.941579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.88
 ---- batch: 020 ----
mean loss: 213.72
 ---- batch: 030 ----
mean loss: 208.05
 ---- batch: 040 ----
mean loss: 212.57
 ---- batch: 050 ----
mean loss: 213.39
 ---- batch: 060 ----
mean loss: 206.02
 ---- batch: 070 ----
mean loss: 216.52
 ---- batch: 080 ----
mean loss: 220.68
 ---- batch: 090 ----
mean loss: 218.22
train mean loss: 213.39
epoch train time: 0:00:02.644763
elapsed time: 0:09:23.254579
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 17:32:50.586801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.06
 ---- batch: 020 ----
mean loss: 208.17
 ---- batch: 030 ----
mean loss: 209.97
 ---- batch: 040 ----
mean loss: 221.75
 ---- batch: 050 ----
mean loss: 212.24
 ---- batch: 060 ----
mean loss: 203.98
 ---- batch: 070 ----
mean loss: 212.37
 ---- batch: 080 ----
mean loss: 216.49
 ---- batch: 090 ----
mean loss: 213.82
train mean loss: 212.77
epoch train time: 0:00:02.647581
elapsed time: 0:09:25.902764
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 17:32:53.234864
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.22
 ---- batch: 020 ----
mean loss: 210.10
 ---- batch: 030 ----
mean loss: 215.00
 ---- batch: 040 ----
mean loss: 208.93
 ---- batch: 050 ----
mean loss: 215.94
 ---- batch: 060 ----
mean loss: 207.35
 ---- batch: 070 ----
mean loss: 211.34
 ---- batch: 080 ----
mean loss: 213.22
 ---- batch: 090 ----
mean loss: 210.53
train mean loss: 212.60
epoch train time: 0:00:02.666326
elapsed time: 0:09:28.569472
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 17:32:55.901770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.20
 ---- batch: 020 ----
mean loss: 212.86
 ---- batch: 030 ----
mean loss: 205.91
 ---- batch: 040 ----
mean loss: 216.58
 ---- batch: 050 ----
mean loss: 213.07
 ---- batch: 060 ----
mean loss: 207.72
 ---- batch: 070 ----
mean loss: 220.71
 ---- batch: 080 ----
mean loss: 211.40
 ---- batch: 090 ----
mean loss: 210.26
train mean loss: 212.48
epoch train time: 0:00:02.642813
elapsed time: 0:09:31.212817
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 17:32:58.545078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.96
 ---- batch: 020 ----
mean loss: 214.00
 ---- batch: 030 ----
mean loss: 216.36
 ---- batch: 040 ----
mean loss: 213.05
 ---- batch: 050 ----
mean loss: 211.11
 ---- batch: 060 ----
mean loss: 214.68
 ---- batch: 070 ----
mean loss: 208.95
 ---- batch: 080 ----
mean loss: 223.17
 ---- batch: 090 ----
mean loss: 206.94
train mean loss: 212.56
epoch train time: 0:00:02.635201
elapsed time: 0:09:33.848527
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 17:33:01.180757
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.88
 ---- batch: 020 ----
mean loss: 214.51
 ---- batch: 030 ----
mean loss: 223.67
 ---- batch: 040 ----
mean loss: 200.25
 ---- batch: 050 ----
mean loss: 213.84
 ---- batch: 060 ----
mean loss: 211.30
 ---- batch: 070 ----
mean loss: 211.77
 ---- batch: 080 ----
mean loss: 222.91
 ---- batch: 090 ----
mean loss: 208.72
train mean loss: 212.48
epoch train time: 0:00:02.640305
elapsed time: 0:09:36.489376
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 17:33:03.821622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.73
 ---- batch: 020 ----
mean loss: 208.88
 ---- batch: 030 ----
mean loss: 210.71
 ---- batch: 040 ----
mean loss: 219.78
 ---- batch: 050 ----
mean loss: 210.68
 ---- batch: 060 ----
mean loss: 211.03
 ---- batch: 070 ----
mean loss: 217.01
 ---- batch: 080 ----
mean loss: 216.87
 ---- batch: 090 ----
mean loss: 210.40
train mean loss: 212.57
epoch train time: 0:00:02.644094
elapsed time: 0:09:39.133954
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 17:33:06.466183
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.62
 ---- batch: 020 ----
mean loss: 207.47
 ---- batch: 030 ----
mean loss: 211.37
 ---- batch: 040 ----
mean loss: 219.00
 ---- batch: 050 ----
mean loss: 213.26
 ---- batch: 060 ----
mean loss: 207.74
 ---- batch: 070 ----
mean loss: 214.56
 ---- batch: 080 ----
mean loss: 213.06
 ---- batch: 090 ----
mean loss: 211.40
train mean loss: 212.61
epoch train time: 0:00:02.601973
elapsed time: 0:09:41.736468
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 17:33:09.068693
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.13
 ---- batch: 020 ----
mean loss: 214.75
 ---- batch: 030 ----
mean loss: 210.29
 ---- batch: 040 ----
mean loss: 222.99
 ---- batch: 050 ----
mean loss: 214.33
 ---- batch: 060 ----
mean loss: 206.87
 ---- batch: 070 ----
mean loss: 212.92
 ---- batch: 080 ----
mean loss: 212.85
 ---- batch: 090 ----
mean loss: 209.78
train mean loss: 212.40
epoch train time: 0:00:02.593181
elapsed time: 0:09:44.330214
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 17:33:11.662461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.33
 ---- batch: 020 ----
mean loss: 216.83
 ---- batch: 030 ----
mean loss: 211.08
 ---- batch: 040 ----
mean loss: 208.76
 ---- batch: 050 ----
mean loss: 216.79
 ---- batch: 060 ----
mean loss: 218.12
 ---- batch: 070 ----
mean loss: 212.04
 ---- batch: 080 ----
mean loss: 211.10
 ---- batch: 090 ----
mean loss: 208.60
train mean loss: 212.44
epoch train time: 0:00:02.604162
elapsed time: 0:09:46.934868
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 17:33:14.267110
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.60
 ---- batch: 020 ----
mean loss: 214.72
 ---- batch: 030 ----
mean loss: 214.08
 ---- batch: 040 ----
mean loss: 210.35
 ---- batch: 050 ----
mean loss: 208.42
 ---- batch: 060 ----
mean loss: 220.49
 ---- batch: 070 ----
mean loss: 208.84
 ---- batch: 080 ----
mean loss: 212.11
 ---- batch: 090 ----
mean loss: 209.25
train mean loss: 212.36
epoch train time: 0:00:02.592874
elapsed time: 0:09:49.528230
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 17:33:16.860474
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.67
 ---- batch: 020 ----
mean loss: 210.15
 ---- batch: 030 ----
mean loss: 214.72
 ---- batch: 040 ----
mean loss: 210.45
 ---- batch: 050 ----
mean loss: 211.94
 ---- batch: 060 ----
mean loss: 208.28
 ---- batch: 070 ----
mean loss: 218.10
 ---- batch: 080 ----
mean loss: 211.76
 ---- batch: 090 ----
mean loss: 212.26
train mean loss: 212.71
epoch train time: 0:00:02.596508
elapsed time: 0:09:52.125234
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 17:33:19.457469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.62
 ---- batch: 020 ----
mean loss: 214.42
 ---- batch: 030 ----
mean loss: 211.41
 ---- batch: 040 ----
mean loss: 217.57
 ---- batch: 050 ----
mean loss: 209.50
 ---- batch: 060 ----
mean loss: 205.78
 ---- batch: 070 ----
mean loss: 215.03
 ---- batch: 080 ----
mean loss: 211.77
 ---- batch: 090 ----
mean loss: 211.58
train mean loss: 212.58
epoch train time: 0:00:02.597446
elapsed time: 0:09:54.723142
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 17:33:22.055364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.52
 ---- batch: 020 ----
mean loss: 216.30
 ---- batch: 030 ----
mean loss: 209.12
 ---- batch: 040 ----
mean loss: 213.17
 ---- batch: 050 ----
mean loss: 217.95
 ---- batch: 060 ----
mean loss: 215.16
 ---- batch: 070 ----
mean loss: 208.28
 ---- batch: 080 ----
mean loss: 210.32
 ---- batch: 090 ----
mean loss: 208.43
train mean loss: 212.31
epoch train time: 0:00:02.624808
elapsed time: 0:09:57.348482
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 17:33:24.680742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.28
 ---- batch: 020 ----
mean loss: 200.04
 ---- batch: 030 ----
mean loss: 209.47
 ---- batch: 040 ----
mean loss: 210.48
 ---- batch: 050 ----
mean loss: 211.96
 ---- batch: 060 ----
mean loss: 218.91
 ---- batch: 070 ----
mean loss: 217.92
 ---- batch: 080 ----
mean loss: 221.79
 ---- batch: 090 ----
mean loss: 215.69
train mean loss: 212.92
epoch train time: 0:00:02.613277
elapsed time: 0:09:59.962239
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 17:33:27.294465
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.26
 ---- batch: 020 ----
mean loss: 211.89
 ---- batch: 030 ----
mean loss: 210.41
 ---- batch: 040 ----
mean loss: 210.70
 ---- batch: 050 ----
mean loss: 213.57
 ---- batch: 060 ----
mean loss: 216.44
 ---- batch: 070 ----
mean loss: 209.38
 ---- batch: 080 ----
mean loss: 212.97
 ---- batch: 090 ----
mean loss: 208.63
train mean loss: 212.51
epoch train time: 0:00:02.625926
elapsed time: 0:10:02.588700
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 17:33:29.920935
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.71
 ---- batch: 020 ----
mean loss: 208.67
 ---- batch: 030 ----
mean loss: 213.31
 ---- batch: 040 ----
mean loss: 212.59
 ---- batch: 050 ----
mean loss: 212.16
 ---- batch: 060 ----
mean loss: 216.07
 ---- batch: 070 ----
mean loss: 209.52
 ---- batch: 080 ----
mean loss: 216.30
 ---- batch: 090 ----
mean loss: 209.90
train mean loss: 212.14
epoch train time: 0:00:02.621891
elapsed time: 0:10:05.211050
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 17:33:32.543282
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.03
 ---- batch: 020 ----
mean loss: 211.60
 ---- batch: 030 ----
mean loss: 214.92
 ---- batch: 040 ----
mean loss: 208.26
 ---- batch: 050 ----
mean loss: 215.31
 ---- batch: 060 ----
mean loss: 210.67
 ---- batch: 070 ----
mean loss: 207.51
 ---- batch: 080 ----
mean loss: 221.20
 ---- batch: 090 ----
mean loss: 211.24
train mean loss: 212.51
epoch train time: 0:00:02.595258
elapsed time: 0:10:07.806784
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 17:33:35.139039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.26
 ---- batch: 020 ----
mean loss: 215.74
 ---- batch: 030 ----
mean loss: 214.65
 ---- batch: 040 ----
mean loss: 214.29
 ---- batch: 050 ----
mean loss: 210.66
 ---- batch: 060 ----
mean loss: 211.87
 ---- batch: 070 ----
mean loss: 208.29
 ---- batch: 080 ----
mean loss: 214.65
 ---- batch: 090 ----
mean loss: 213.77
train mean loss: 212.54
epoch train time: 0:00:02.597803
elapsed time: 0:10:10.405187
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 17:33:37.737411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.80
 ---- batch: 020 ----
mean loss: 209.33
 ---- batch: 030 ----
mean loss: 216.96
 ---- batch: 040 ----
mean loss: 214.66
 ---- batch: 050 ----
mean loss: 219.86
 ---- batch: 060 ----
mean loss: 204.20
 ---- batch: 070 ----
mean loss: 206.50
 ---- batch: 080 ----
mean loss: 220.16
 ---- batch: 090 ----
mean loss: 212.79
train mean loss: 212.27
epoch train time: 0:00:02.605063
elapsed time: 0:10:13.010750
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 17:33:40.342980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.25
 ---- batch: 020 ----
mean loss: 207.38
 ---- batch: 030 ----
mean loss: 211.77
 ---- batch: 040 ----
mean loss: 211.33
 ---- batch: 050 ----
mean loss: 216.00
 ---- batch: 060 ----
mean loss: 212.32
 ---- batch: 070 ----
mean loss: 213.06
 ---- batch: 080 ----
mean loss: 218.46
 ---- batch: 090 ----
mean loss: 211.08
train mean loss: 212.48
epoch train time: 0:00:02.579378
elapsed time: 0:10:15.590627
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 17:33:42.922846
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.39
 ---- batch: 020 ----
mean loss: 208.46
 ---- batch: 030 ----
mean loss: 212.68
 ---- batch: 040 ----
mean loss: 213.31
 ---- batch: 050 ----
mean loss: 213.50
 ---- batch: 060 ----
mean loss: 219.34
 ---- batch: 070 ----
mean loss: 212.91
 ---- batch: 080 ----
mean loss: 219.31
 ---- batch: 090 ----
mean loss: 202.88
train mean loss: 212.31
epoch train time: 0:00:02.578334
elapsed time: 0:10:18.169427
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 17:33:45.501720
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.76
 ---- batch: 020 ----
mean loss: 212.31
 ---- batch: 030 ----
mean loss: 211.38
 ---- batch: 040 ----
mean loss: 207.95
 ---- batch: 050 ----
mean loss: 208.31
 ---- batch: 060 ----
mean loss: 212.28
 ---- batch: 070 ----
mean loss: 211.20
 ---- batch: 080 ----
mean loss: 220.92
 ---- batch: 090 ----
mean loss: 216.15
train mean loss: 212.26
epoch train time: 0:00:02.632894
elapsed time: 0:10:20.802867
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 17:33:48.135085
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.25
 ---- batch: 020 ----
mean loss: 212.27
 ---- batch: 030 ----
mean loss: 213.76
 ---- batch: 040 ----
mean loss: 209.32
 ---- batch: 050 ----
mean loss: 211.90
 ---- batch: 060 ----
mean loss: 212.53
 ---- batch: 070 ----
mean loss: 212.98
 ---- batch: 080 ----
mean loss: 206.49
 ---- batch: 090 ----
mean loss: 220.12
train mean loss: 212.27
epoch train time: 0:00:02.642001
elapsed time: 0:10:23.445368
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 17:33:50.777574
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.27
 ---- batch: 020 ----
mean loss: 206.39
 ---- batch: 030 ----
mean loss: 210.05
 ---- batch: 040 ----
mean loss: 210.53
 ---- batch: 050 ----
mean loss: 211.20
 ---- batch: 060 ----
mean loss: 207.98
 ---- batch: 070 ----
mean loss: 214.00
 ---- batch: 080 ----
mean loss: 225.49
 ---- batch: 090 ----
mean loss: 215.46
train mean loss: 212.28
epoch train time: 0:00:02.620072
elapsed time: 0:10:26.065887
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 17:33:53.398171
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.47
 ---- batch: 020 ----
mean loss: 216.62
 ---- batch: 030 ----
mean loss: 207.65
 ---- batch: 040 ----
mean loss: 212.79
 ---- batch: 050 ----
mean loss: 214.76
 ---- batch: 060 ----
mean loss: 210.19
 ---- batch: 070 ----
mean loss: 211.22
 ---- batch: 080 ----
mean loss: 208.46
 ---- batch: 090 ----
mean loss: 212.82
train mean loss: 212.20
epoch train time: 0:00:02.636788
elapsed time: 0:10:28.703218
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 17:33:56.035480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.59
 ---- batch: 020 ----
mean loss: 218.79
 ---- batch: 030 ----
mean loss: 213.54
 ---- batch: 040 ----
mean loss: 207.14
 ---- batch: 050 ----
mean loss: 208.19
 ---- batch: 060 ----
mean loss: 213.12
 ---- batch: 070 ----
mean loss: 206.54
 ---- batch: 080 ----
mean loss: 214.58
 ---- batch: 090 ----
mean loss: 218.23
train mean loss: 212.38
epoch train time: 0:00:02.608255
elapsed time: 0:10:31.311979
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 17:33:58.644195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.59
 ---- batch: 020 ----
mean loss: 217.15
 ---- batch: 030 ----
mean loss: 210.86
 ---- batch: 040 ----
mean loss: 215.98
 ---- batch: 050 ----
mean loss: 218.45
 ---- batch: 060 ----
mean loss: 210.39
 ---- batch: 070 ----
mean loss: 213.61
 ---- batch: 080 ----
mean loss: 214.37
 ---- batch: 090 ----
mean loss: 207.48
train mean loss: 212.32
epoch train time: 0:00:02.593681
elapsed time: 0:10:33.906130
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 17:34:01.238353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.71
 ---- batch: 020 ----
mean loss: 204.08
 ---- batch: 030 ----
mean loss: 217.98
 ---- batch: 040 ----
mean loss: 212.46
 ---- batch: 050 ----
mean loss: 207.32
 ---- batch: 060 ----
mean loss: 212.48
 ---- batch: 070 ----
mean loss: 217.72
 ---- batch: 080 ----
mean loss: 206.09
 ---- batch: 090 ----
mean loss: 212.73
train mean loss: 212.23
epoch train time: 0:00:02.618907
elapsed time: 0:10:36.525525
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 17:34:03.857778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.87
 ---- batch: 020 ----
mean loss: 212.49
 ---- batch: 030 ----
mean loss: 208.35
 ---- batch: 040 ----
mean loss: 220.53
 ---- batch: 050 ----
mean loss: 217.11
 ---- batch: 060 ----
mean loss: 206.78
 ---- batch: 070 ----
mean loss: 213.71
 ---- batch: 080 ----
mean loss: 205.98
 ---- batch: 090 ----
mean loss: 221.17
train mean loss: 212.17
epoch train time: 0:00:02.621852
elapsed time: 0:10:39.147884
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 17:34:06.480103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.12
 ---- batch: 020 ----
mean loss: 214.86
 ---- batch: 030 ----
mean loss: 219.75
 ---- batch: 040 ----
mean loss: 207.34
 ---- batch: 050 ----
mean loss: 207.95
 ---- batch: 060 ----
mean loss: 205.62
 ---- batch: 070 ----
mean loss: 212.60
 ---- batch: 080 ----
mean loss: 215.71
 ---- batch: 090 ----
mean loss: 211.77
train mean loss: 212.33
epoch train time: 0:00:02.612389
elapsed time: 0:10:41.760738
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 17:34:09.092961
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.43
 ---- batch: 020 ----
mean loss: 208.79
 ---- batch: 030 ----
mean loss: 213.51
 ---- batch: 040 ----
mean loss: 210.82
 ---- batch: 050 ----
mean loss: 206.99
 ---- batch: 060 ----
mean loss: 204.32
 ---- batch: 070 ----
mean loss: 209.11
 ---- batch: 080 ----
mean loss: 212.17
 ---- batch: 090 ----
mean loss: 220.72
train mean loss: 212.14
epoch train time: 0:00:02.641486
elapsed time: 0:10:44.402794
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 17:34:11.735039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.03
 ---- batch: 020 ----
mean loss: 213.73
 ---- batch: 030 ----
mean loss: 216.26
 ---- batch: 040 ----
mean loss: 207.27
 ---- batch: 050 ----
mean loss: 206.33
 ---- batch: 060 ----
mean loss: 216.25
 ---- batch: 070 ----
mean loss: 210.54
 ---- batch: 080 ----
mean loss: 210.45
 ---- batch: 090 ----
mean loss: 213.64
train mean loss: 212.07
epoch train time: 0:00:02.639780
elapsed time: 0:10:47.043049
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 17:34:14.375310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.28
 ---- batch: 020 ----
mean loss: 210.77
 ---- batch: 030 ----
mean loss: 208.13
 ---- batch: 040 ----
mean loss: 209.99
 ---- batch: 050 ----
mean loss: 214.20
 ---- batch: 060 ----
mean loss: 213.25
 ---- batch: 070 ----
mean loss: 204.98
 ---- batch: 080 ----
mean loss: 214.18
 ---- batch: 090 ----
mean loss: 212.47
train mean loss: 212.29
epoch train time: 0:00:02.621013
elapsed time: 0:10:49.664742
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 17:34:16.996848
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.30
 ---- batch: 020 ----
mean loss: 213.93
 ---- batch: 030 ----
mean loss: 208.23
 ---- batch: 040 ----
mean loss: 218.56
 ---- batch: 050 ----
mean loss: 213.87
 ---- batch: 060 ----
mean loss: 211.76
 ---- batch: 070 ----
mean loss: 217.15
 ---- batch: 080 ----
mean loss: 210.08
 ---- batch: 090 ----
mean loss: 208.73
train mean loss: 212.25
epoch train time: 0:00:02.645980
elapsed time: 0:10:52.311042
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 17:34:19.643264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.90
 ---- batch: 020 ----
mean loss: 209.45
 ---- batch: 030 ----
mean loss: 210.79
 ---- batch: 040 ----
mean loss: 208.75
 ---- batch: 050 ----
mean loss: 216.30
 ---- batch: 060 ----
mean loss: 217.86
 ---- batch: 070 ----
mean loss: 212.24
 ---- batch: 080 ----
mean loss: 212.47
 ---- batch: 090 ----
mean loss: 209.78
train mean loss: 212.08
epoch train time: 0:00:02.638504
elapsed time: 0:10:54.950038
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 17:34:22.282260
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.84
 ---- batch: 020 ----
mean loss: 214.19
 ---- batch: 030 ----
mean loss: 211.78
 ---- batch: 040 ----
mean loss: 217.92
 ---- batch: 050 ----
mean loss: 211.71
 ---- batch: 060 ----
mean loss: 204.13
 ---- batch: 070 ----
mean loss: 216.87
 ---- batch: 080 ----
mean loss: 212.75
 ---- batch: 090 ----
mean loss: 213.83
train mean loss: 212.04
epoch train time: 0:00:02.640369
elapsed time: 0:10:57.590856
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 17:34:24.923077
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.20
 ---- batch: 020 ----
mean loss: 205.66
 ---- batch: 030 ----
mean loss: 218.16
 ---- batch: 040 ----
mean loss: 213.83
 ---- batch: 050 ----
mean loss: 209.94
 ---- batch: 060 ----
mean loss: 204.85
 ---- batch: 070 ----
mean loss: 209.98
 ---- batch: 080 ----
mean loss: 216.30
 ---- batch: 090 ----
mean loss: 212.84
train mean loss: 212.29
epoch train time: 0:00:02.629811
elapsed time: 0:11:00.221158
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 17:34:27.553382
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.05
 ---- batch: 020 ----
mean loss: 212.30
 ---- batch: 030 ----
mean loss: 214.38
 ---- batch: 040 ----
mean loss: 209.44
 ---- batch: 050 ----
mean loss: 213.64
 ---- batch: 060 ----
mean loss: 207.27
 ---- batch: 070 ----
mean loss: 212.25
 ---- batch: 080 ----
mean loss: 209.52
 ---- batch: 090 ----
mean loss: 214.18
train mean loss: 212.31
epoch train time: 0:00:02.625744
elapsed time: 0:11:02.847383
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 17:34:30.179617
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.46
 ---- batch: 020 ----
mean loss: 214.38
 ---- batch: 030 ----
mean loss: 210.05
 ---- batch: 040 ----
mean loss: 206.54
 ---- batch: 050 ----
mean loss: 213.02
 ---- batch: 060 ----
mean loss: 216.50
 ---- batch: 070 ----
mean loss: 206.55
 ---- batch: 080 ----
mean loss: 213.05
 ---- batch: 090 ----
mean loss: 215.34
train mean loss: 212.18
epoch train time: 0:00:02.657347
elapsed time: 0:11:05.505234
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 17:34:32.837457
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.82
 ---- batch: 020 ----
mean loss: 211.36
 ---- batch: 030 ----
mean loss: 213.77
 ---- batch: 040 ----
mean loss: 208.10
 ---- batch: 050 ----
mean loss: 213.20
 ---- batch: 060 ----
mean loss: 208.25
 ---- batch: 070 ----
mean loss: 213.10
 ---- batch: 080 ----
mean loss: 213.27
 ---- batch: 090 ----
mean loss: 207.87
train mean loss: 212.22
epoch train time: 0:00:02.633809
elapsed time: 0:11:08.139523
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 17:34:35.471766
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.21
 ---- batch: 020 ----
mean loss: 208.86
 ---- batch: 030 ----
mean loss: 214.72
 ---- batch: 040 ----
mean loss: 206.82
 ---- batch: 050 ----
mean loss: 212.69
 ---- batch: 060 ----
mean loss: 207.46
 ---- batch: 070 ----
mean loss: 214.77
 ---- batch: 080 ----
mean loss: 208.33
 ---- batch: 090 ----
mean loss: 216.10
train mean loss: 212.28
epoch train time: 0:00:02.607095
elapsed time: 0:11:10.747121
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 17:34:38.079333
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.29
 ---- batch: 020 ----
mean loss: 206.29
 ---- batch: 030 ----
mean loss: 217.68
 ---- batch: 040 ----
mean loss: 209.68
 ---- batch: 050 ----
mean loss: 210.27
 ---- batch: 060 ----
mean loss: 208.91
 ---- batch: 070 ----
mean loss: 210.96
 ---- batch: 080 ----
mean loss: 209.24
 ---- batch: 090 ----
mean loss: 217.55
train mean loss: 212.14
epoch train time: 0:00:02.632780
elapsed time: 0:11:13.380421
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 17:34:40.712649
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.64
 ---- batch: 020 ----
mean loss: 213.04
 ---- batch: 030 ----
mean loss: 209.75
 ---- batch: 040 ----
mean loss: 206.13
 ---- batch: 050 ----
mean loss: 209.18
 ---- batch: 060 ----
mean loss: 218.06
 ---- batch: 070 ----
mean loss: 215.67
 ---- batch: 080 ----
mean loss: 205.90
 ---- batch: 090 ----
mean loss: 219.29
train mean loss: 211.90
epoch train time: 0:00:02.630975
elapsed time: 0:11:16.011985
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 17:34:43.344249
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.83
 ---- batch: 020 ----
mean loss: 220.41
 ---- batch: 030 ----
mean loss: 208.17
 ---- batch: 040 ----
mean loss: 208.98
 ---- batch: 050 ----
mean loss: 216.55
 ---- batch: 060 ----
mean loss: 207.63
 ---- batch: 070 ----
mean loss: 212.61
 ---- batch: 080 ----
mean loss: 208.58
 ---- batch: 090 ----
mean loss: 214.24
train mean loss: 211.95
epoch train time: 0:00:02.627380
elapsed time: 0:11:18.639905
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 17:34:45.972169
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.33
 ---- batch: 020 ----
mean loss: 212.06
 ---- batch: 030 ----
mean loss: 210.69
 ---- batch: 040 ----
mean loss: 214.55
 ---- batch: 050 ----
mean loss: 215.06
 ---- batch: 060 ----
mean loss: 213.42
 ---- batch: 070 ----
mean loss: 205.62
 ---- batch: 080 ----
mean loss: 214.94
 ---- batch: 090 ----
mean loss: 215.84
train mean loss: 211.92
epoch train time: 0:00:02.633551
elapsed time: 0:11:21.273970
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 17:34:48.606193
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.99
 ---- batch: 020 ----
mean loss: 212.08
 ---- batch: 030 ----
mean loss: 205.50
 ---- batch: 040 ----
mean loss: 218.85
 ---- batch: 050 ----
mean loss: 213.08
 ---- batch: 060 ----
mean loss: 204.71
 ---- batch: 070 ----
mean loss: 213.17
 ---- batch: 080 ----
mean loss: 205.37
 ---- batch: 090 ----
mean loss: 215.67
train mean loss: 212.09
epoch train time: 0:00:02.614231
elapsed time: 0:11:23.888685
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 17:34:51.220858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.12
 ---- batch: 020 ----
mean loss: 210.33
 ---- batch: 030 ----
mean loss: 212.41
 ---- batch: 040 ----
mean loss: 219.70
 ---- batch: 050 ----
mean loss: 215.76
 ---- batch: 060 ----
mean loss: 215.50
 ---- batch: 070 ----
mean loss: 202.76
 ---- batch: 080 ----
mean loss: 204.19
 ---- batch: 090 ----
mean loss: 215.38
train mean loss: 211.85
epoch train time: 0:00:02.627190
elapsed time: 0:11:26.516314
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 17:34:53.848560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.82
 ---- batch: 020 ----
mean loss: 214.29
 ---- batch: 030 ----
mean loss: 208.03
 ---- batch: 040 ----
mean loss: 217.56
 ---- batch: 050 ----
mean loss: 209.23
 ---- batch: 060 ----
mean loss: 213.65
 ---- batch: 070 ----
mean loss: 219.68
 ---- batch: 080 ----
mean loss: 205.45
 ---- batch: 090 ----
mean loss: 207.36
train mean loss: 211.94
epoch train time: 0:00:02.659048
elapsed time: 0:11:29.175861
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 17:34:56.508091
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.18
 ---- batch: 020 ----
mean loss: 215.97
 ---- batch: 030 ----
mean loss: 211.82
 ---- batch: 040 ----
mean loss: 204.73
 ---- batch: 050 ----
mean loss: 221.06
 ---- batch: 060 ----
mean loss: 211.92
 ---- batch: 070 ----
mean loss: 207.46
 ---- batch: 080 ----
mean loss: 214.74
 ---- batch: 090 ----
mean loss: 215.33
train mean loss: 211.76
epoch train time: 0:00:02.622291
elapsed time: 0:11:31.802592
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_0/checkpoint.pth.tar
**** end time: 2019-09-25 17:34:59.134658 ****
