Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_3', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 11217
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 08:37:44.184227 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 08:37:44.202641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2249.05
 ---- batch: 020 ----
mean loss: 1689.41
 ---- batch: 030 ----
mean loss: 1446.98
 ---- batch: 040 ----
mean loss: 1317.21
train mean loss: 1605.34
epoch train time: 0:00:23.825156
elapsed time: 0:00:23.852698
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 08:38:08.036966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1196.60
 ---- batch: 020 ----
mean loss: 1184.77
 ---- batch: 030 ----
mean loss: 1142.99
 ---- batch: 040 ----
mean loss: 1123.63
train mean loss: 1149.26
epoch train time: 0:00:08.602807
elapsed time: 0:00:32.456694
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 08:38:16.641265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1060.71
 ---- batch: 020 ----
mean loss: 1116.62
 ---- batch: 030 ----
mean loss: 1074.66
 ---- batch: 040 ----
mean loss: 1066.51
train mean loss: 1075.37
epoch train time: 0:00:08.662799
elapsed time: 0:00:41.121038
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 08:38:25.305563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1068.37
 ---- batch: 020 ----
mean loss: 1060.92
 ---- batch: 030 ----
mean loss: 1031.43
 ---- batch: 040 ----
mean loss: 1062.61
train mean loss: 1054.86
epoch train time: 0:00:08.653811
elapsed time: 0:00:49.776181
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 08:38:33.960757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1069.92
 ---- batch: 020 ----
mean loss: 1034.55
 ---- batch: 030 ----
mean loss: 1058.53
 ---- batch: 040 ----
mean loss: 1037.80
train mean loss: 1048.70
epoch train time: 0:00:08.666853
elapsed time: 0:00:58.444327
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 08:38:42.628876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1048.53
 ---- batch: 020 ----
mean loss: 1026.62
 ---- batch: 030 ----
mean loss: 1061.34
 ---- batch: 040 ----
mean loss: 1041.06
train mean loss: 1046.23
epoch train time: 0:00:08.663900
elapsed time: 0:01:07.109627
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 08:38:51.294196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1030.85
 ---- batch: 020 ----
mean loss: 1038.63
 ---- batch: 030 ----
mean loss: 1028.71
 ---- batch: 040 ----
mean loss: 1026.17
train mean loss: 1033.62
epoch train time: 0:00:08.613350
elapsed time: 0:01:15.724389
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 08:38:59.909126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1021.50
 ---- batch: 020 ----
mean loss: 989.43
 ---- batch: 030 ----
mean loss: 1004.67
 ---- batch: 040 ----
mean loss: 1037.56
train mean loss: 1015.15
epoch train time: 0:00:08.657753
elapsed time: 0:01:24.383651
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 08:39:08.568212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1011.96
 ---- batch: 020 ----
mean loss: 1026.32
 ---- batch: 030 ----
mean loss: 1003.90
 ---- batch: 040 ----
mean loss: 1001.11
train mean loss: 1008.40
epoch train time: 0:00:08.619868
elapsed time: 0:01:33.005005
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 08:39:17.189554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 966.75
 ---- batch: 020 ----
mean loss: 1007.75
 ---- batch: 030 ----
mean loss: 981.59
 ---- batch: 040 ----
mean loss: 1018.66
train mean loss: 996.51
epoch train time: 0:00:08.619499
elapsed time: 0:01:41.625871
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 08:39:25.810410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 994.71
 ---- batch: 020 ----
mean loss: 1001.62
 ---- batch: 030 ----
mean loss: 982.36
 ---- batch: 040 ----
mean loss: 998.80
train mean loss: 993.05
epoch train time: 0:00:08.639618
elapsed time: 0:01:50.266859
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 08:39:34.451393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1001.48
 ---- batch: 020 ----
mean loss: 1002.80
 ---- batch: 030 ----
mean loss: 978.84
 ---- batch: 040 ----
mean loss: 981.14
train mean loss: 987.41
epoch train time: 0:00:08.560220
elapsed time: 0:01:58.828372
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 08:39:43.012884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 998.80
 ---- batch: 020 ----
mean loss: 966.73
 ---- batch: 030 ----
mean loss: 991.18
 ---- batch: 040 ----
mean loss: 989.02
train mean loss: 982.03
epoch train time: 0:00:08.524684
elapsed time: 0:02:07.354253
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 08:39:51.538790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 982.79
 ---- batch: 020 ----
mean loss: 970.23
 ---- batch: 030 ----
mean loss: 981.62
 ---- batch: 040 ----
mean loss: 965.29
train mean loss: 976.62
epoch train time: 0:00:08.472440
elapsed time: 0:02:15.827955
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 08:40:00.012501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.02
 ---- batch: 020 ----
mean loss: 985.52
 ---- batch: 030 ----
mean loss: 976.66
 ---- batch: 040 ----
mean loss: 982.06
train mean loss: 978.40
epoch train time: 0:00:08.579086
elapsed time: 0:02:24.408326
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 08:40:08.592884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 971.76
 ---- batch: 020 ----
mean loss: 969.04
 ---- batch: 030 ----
mean loss: 982.88
 ---- batch: 040 ----
mean loss: 977.57
train mean loss: 974.72
epoch train time: 0:00:08.674413
elapsed time: 0:02:33.084271
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 08:40:17.268859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.40
 ---- batch: 020 ----
mean loss: 1000.44
 ---- batch: 030 ----
mean loss: 981.88
 ---- batch: 040 ----
mean loss: 971.18
train mean loss: 969.95
epoch train time: 0:00:08.615496
elapsed time: 0:02:41.701211
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 08:40:25.885791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.28
 ---- batch: 020 ----
mean loss: 969.12
 ---- batch: 030 ----
mean loss: 959.27
 ---- batch: 040 ----
mean loss: 965.26
train mean loss: 962.11
epoch train time: 0:00:08.569802
elapsed time: 0:02:50.272371
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 08:40:34.456904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 968.69
 ---- batch: 020 ----
mean loss: 967.89
 ---- batch: 030 ----
mean loss: 950.41
 ---- batch: 040 ----
mean loss: 963.72
train mean loss: 963.93
epoch train time: 0:00:08.529035
elapsed time: 0:02:58.802620
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 08:40:42.987174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 967.48
 ---- batch: 020 ----
mean loss: 939.78
 ---- batch: 030 ----
mean loss: 955.34
 ---- batch: 040 ----
mean loss: 997.19
train mean loss: 960.85
epoch train time: 0:00:08.521087
elapsed time: 0:03:07.324941
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 08:40:51.509474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 945.61
 ---- batch: 020 ----
mean loss: 974.08
 ---- batch: 030 ----
mean loss: 946.95
 ---- batch: 040 ----
mean loss: 955.73
train mean loss: 954.95
epoch train time: 0:00:08.513758
elapsed time: 0:03:15.840184
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 08:41:00.024768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.97
 ---- batch: 020 ----
mean loss: 952.87
 ---- batch: 030 ----
mean loss: 940.00
 ---- batch: 040 ----
mean loss: 970.35
train mean loss: 957.11
epoch train time: 0:00:08.541906
elapsed time: 0:03:24.383495
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 08:41:08.567964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.52
 ---- batch: 020 ----
mean loss: 965.94
 ---- batch: 030 ----
mean loss: 953.89
 ---- batch: 040 ----
mean loss: 969.08
train mean loss: 959.04
epoch train time: 0:00:08.527437
elapsed time: 0:03:32.912101
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 08:41:17.096645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.55
 ---- batch: 020 ----
mean loss: 953.64
 ---- batch: 030 ----
mean loss: 925.72
 ---- batch: 040 ----
mean loss: 952.32
train mean loss: 946.22
epoch train time: 0:00:08.553177
elapsed time: 0:03:41.466639
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 08:41:25.651190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.65
 ---- batch: 020 ----
mean loss: 932.07
 ---- batch: 030 ----
mean loss: 934.37
 ---- batch: 040 ----
mean loss: 961.61
train mean loss: 941.45
epoch train time: 0:00:08.527209
elapsed time: 0:03:49.995070
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 08:41:34.179615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.86
 ---- batch: 020 ----
mean loss: 939.47
 ---- batch: 030 ----
mean loss: 945.56
 ---- batch: 040 ----
mean loss: 940.27
train mean loss: 939.44
epoch train time: 0:00:08.582474
elapsed time: 0:03:58.578790
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 08:41:42.763359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.98
 ---- batch: 020 ----
mean loss: 929.19
 ---- batch: 030 ----
mean loss: 941.51
 ---- batch: 040 ----
mean loss: 938.31
train mean loss: 935.38
epoch train time: 0:00:08.482213
elapsed time: 0:04:07.062294
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 08:41:51.246815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.20
 ---- batch: 020 ----
mean loss: 932.44
 ---- batch: 030 ----
mean loss: 915.59
 ---- batch: 040 ----
mean loss: 936.73
train mean loss: 929.83
epoch train time: 0:00:08.505115
elapsed time: 0:04:15.568620
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 08:41:59.753194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.86
 ---- batch: 020 ----
mean loss: 934.54
 ---- batch: 030 ----
mean loss: 920.49
 ---- batch: 040 ----
mean loss: 923.53
train mean loss: 925.66
epoch train time: 0:00:08.498417
elapsed time: 0:04:24.068265
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 08:42:08.252916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 938.42
 ---- batch: 020 ----
mean loss: 932.69
 ---- batch: 030 ----
mean loss: 933.22
 ---- batch: 040 ----
mean loss: 909.68
train mean loss: 928.69
epoch train time: 0:00:08.488643
elapsed time: 0:04:32.558361
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 08:42:16.742905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.07
 ---- batch: 020 ----
mean loss: 923.68
 ---- batch: 030 ----
mean loss: 925.72
 ---- batch: 040 ----
mean loss: 898.27
train mean loss: 920.98
epoch train time: 0:00:08.493506
elapsed time: 0:04:41.053060
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 08:42:25.237627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 904.06
 ---- batch: 020 ----
mean loss: 917.63
 ---- batch: 030 ----
mean loss: 884.73
 ---- batch: 040 ----
mean loss: 901.30
train mean loss: 904.93
epoch train time: 0:00:08.502527
elapsed time: 0:04:49.556838
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 08:42:33.741371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.59
 ---- batch: 020 ----
mean loss: 906.56
 ---- batch: 030 ----
mean loss: 891.74
 ---- batch: 040 ----
mean loss: 888.16
train mean loss: 891.09
epoch train time: 0:00:08.484791
elapsed time: 0:04:58.042920
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 08:42:42.227463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.55
 ---- batch: 020 ----
mean loss: 876.60
 ---- batch: 030 ----
mean loss: 845.22
 ---- batch: 040 ----
mean loss: 830.55
train mean loss: 849.81
epoch train time: 0:00:08.495219
elapsed time: 0:05:06.539462
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 08:42:50.723981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 810.68
 ---- batch: 020 ----
mean loss: 779.47
 ---- batch: 030 ----
mean loss: 745.66
 ---- batch: 040 ----
mean loss: 762.38
train mean loss: 764.56
epoch train time: 0:00:08.493032
elapsed time: 0:05:15.033676
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 08:42:59.218223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 712.14
 ---- batch: 020 ----
mean loss: 729.09
 ---- batch: 030 ----
mean loss: 707.62
 ---- batch: 040 ----
mean loss: 706.80
train mean loss: 710.36
epoch train time: 0:00:08.501981
elapsed time: 0:05:23.537012
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 08:43:07.721683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 685.31
 ---- batch: 020 ----
mean loss: 681.52
 ---- batch: 030 ----
mean loss: 669.58
 ---- batch: 040 ----
mean loss: 674.96
train mean loss: 678.16
epoch train time: 0:00:08.523604
elapsed time: 0:05:32.061979
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 08:43:16.246546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.02
 ---- batch: 020 ----
mean loss: 649.64
 ---- batch: 030 ----
mean loss: 655.11
 ---- batch: 040 ----
mean loss: 639.27
train mean loss: 648.88
epoch train time: 0:00:08.495979
elapsed time: 0:05:40.559210
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 08:43:24.743743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 612.62
 ---- batch: 020 ----
mean loss: 610.63
 ---- batch: 030 ----
mean loss: 614.34
 ---- batch: 040 ----
mean loss: 614.24
train mean loss: 612.80
epoch train time: 0:00:08.490030
elapsed time: 0:05:49.050493
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 08:43:33.235031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 603.04
 ---- batch: 020 ----
mean loss: 586.64
 ---- batch: 030 ----
mean loss: 588.36
 ---- batch: 040 ----
mean loss: 585.39
train mean loss: 588.67
epoch train time: 0:00:08.476242
elapsed time: 0:05:57.527977
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 08:43:41.712535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 579.79
 ---- batch: 020 ----
mean loss: 561.78
 ---- batch: 030 ----
mean loss: 575.32
 ---- batch: 040 ----
mean loss: 570.10
train mean loss: 569.53
epoch train time: 0:00:08.495244
elapsed time: 0:06:06.024752
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 08:43:50.209177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 559.71
 ---- batch: 020 ----
mean loss: 553.12
 ---- batch: 030 ----
mean loss: 545.14
 ---- batch: 040 ----
mean loss: 520.15
train mean loss: 542.93
epoch train time: 0:00:08.500939
elapsed time: 0:06:14.526877
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 08:43:58.711518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 542.36
 ---- batch: 020 ----
mean loss: 517.15
 ---- batch: 030 ----
mean loss: 528.17
 ---- batch: 040 ----
mean loss: 520.19
train mean loss: 525.92
epoch train time: 0:00:08.487846
elapsed time: 0:06:23.016048
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 08:44:07.200589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.20
 ---- batch: 020 ----
mean loss: 512.76
 ---- batch: 030 ----
mean loss: 501.88
 ---- batch: 040 ----
mean loss: 506.15
train mean loss: 504.33
epoch train time: 0:00:08.509605
elapsed time: 0:06:31.526872
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 08:44:15.711325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.92
 ---- batch: 020 ----
mean loss: 481.61
 ---- batch: 030 ----
mean loss: 498.53
 ---- batch: 040 ----
mean loss: 502.52
train mean loss: 491.14
epoch train time: 0:00:08.504758
elapsed time: 0:06:40.032755
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 08:44:24.217314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.63
 ---- batch: 020 ----
mean loss: 490.32
 ---- batch: 030 ----
mean loss: 484.78
 ---- batch: 040 ----
mean loss: 461.25
train mean loss: 479.55
epoch train time: 0:00:08.498481
elapsed time: 0:06:48.532487
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 08:44:32.717020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.87
 ---- batch: 020 ----
mean loss: 465.90
 ---- batch: 030 ----
mean loss: 466.27
 ---- batch: 040 ----
mean loss: 458.05
train mean loss: 462.60
epoch train time: 0:00:08.490691
elapsed time: 0:06:57.024468
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 08:44:41.209047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.89
 ---- batch: 020 ----
mean loss: 455.79
 ---- batch: 030 ----
mean loss: 458.71
 ---- batch: 040 ----
mean loss: 449.04
train mean loss: 453.67
epoch train time: 0:00:08.461952
elapsed time: 0:07:05.487693
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 08:44:49.672233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.55
 ---- batch: 020 ----
mean loss: 442.37
 ---- batch: 030 ----
mean loss: 436.58
 ---- batch: 040 ----
mean loss: 453.24
train mean loss: 445.31
epoch train time: 0:00:08.447991
elapsed time: 0:07:13.937049
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 08:44:58.121600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.37
 ---- batch: 020 ----
mean loss: 433.48
 ---- batch: 030 ----
mean loss: 438.74
 ---- batch: 040 ----
mean loss: 433.10
train mean loss: 434.34
epoch train time: 0:00:08.474776
elapsed time: 0:07:22.413064
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 08:45:06.597623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.29
 ---- batch: 020 ----
mean loss: 428.93
 ---- batch: 030 ----
mean loss: 431.25
 ---- batch: 040 ----
mean loss: 428.78
train mean loss: 426.88
epoch train time: 0:00:08.456994
elapsed time: 0:07:30.871346
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 08:45:15.055864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.81
 ---- batch: 020 ----
mean loss: 427.67
 ---- batch: 030 ----
mean loss: 422.12
 ---- batch: 040 ----
mean loss: 420.86
train mean loss: 424.26
epoch train time: 0:00:08.467553
elapsed time: 0:07:39.340127
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 08:45:23.524666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.53
 ---- batch: 020 ----
mean loss: 420.41
 ---- batch: 030 ----
mean loss: 413.35
 ---- batch: 040 ----
mean loss: 419.24
train mean loss: 416.59
epoch train time: 0:00:08.472101
elapsed time: 0:07:47.813511
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 08:45:31.998058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.38
 ---- batch: 020 ----
mean loss: 392.19
 ---- batch: 030 ----
mean loss: 407.73
 ---- batch: 040 ----
mean loss: 406.77
train mean loss: 404.21
epoch train time: 0:00:08.451837
elapsed time: 0:07:56.266579
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 08:45:40.451116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.06
 ---- batch: 020 ----
mean loss: 401.40
 ---- batch: 030 ----
mean loss: 400.80
 ---- batch: 040 ----
mean loss: 402.85
train mean loss: 401.05
epoch train time: 0:00:08.426906
elapsed time: 0:08:04.694699
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 08:45:48.879239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.69
 ---- batch: 020 ----
mean loss: 391.32
 ---- batch: 030 ----
mean loss: 388.29
 ---- batch: 040 ----
mean loss: 401.66
train mean loss: 391.99
epoch train time: 0:00:08.453068
elapsed time: 0:08:13.149110
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 08:45:57.333665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.35
 ---- batch: 020 ----
mean loss: 388.26
 ---- batch: 030 ----
mean loss: 382.57
 ---- batch: 040 ----
mean loss: 387.56
train mean loss: 386.16
epoch train time: 0:00:08.467569
elapsed time: 0:08:21.617991
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 08:46:05.802553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.55
 ---- batch: 020 ----
mean loss: 384.26
 ---- batch: 030 ----
mean loss: 385.17
 ---- batch: 040 ----
mean loss: 376.13
train mean loss: 384.93
epoch train time: 0:00:08.438441
elapsed time: 0:08:30.057714
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 08:46:14.242294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.28
 ---- batch: 020 ----
mean loss: 375.37
 ---- batch: 030 ----
mean loss: 377.66
 ---- batch: 040 ----
mean loss: 367.50
train mean loss: 378.48
epoch train time: 0:00:08.460562
elapsed time: 0:08:38.519564
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 08:46:22.704094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.66
 ---- batch: 020 ----
mean loss: 374.03
 ---- batch: 030 ----
mean loss: 382.78
 ---- batch: 040 ----
mean loss: 369.62
train mean loss: 375.01
epoch train time: 0:00:08.489019
elapsed time: 0:08:47.009821
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 08:46:31.194350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.53
 ---- batch: 020 ----
mean loss: 372.80
 ---- batch: 030 ----
mean loss: 367.45
 ---- batch: 040 ----
mean loss: 372.76
train mean loss: 367.34
epoch train time: 0:00:08.434769
elapsed time: 0:08:55.445859
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 08:46:39.630411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.64
 ---- batch: 020 ----
mean loss: 371.19
 ---- batch: 030 ----
mean loss: 375.37
 ---- batch: 040 ----
mean loss: 350.26
train mean loss: 367.43
epoch train time: 0:00:08.488791
elapsed time: 0:09:03.935938
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 08:46:48.120477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.93
 ---- batch: 020 ----
mean loss: 374.68
 ---- batch: 030 ----
mean loss: 360.05
 ---- batch: 040 ----
mean loss: 353.69
train mean loss: 362.25
epoch train time: 0:00:08.514514
elapsed time: 0:09:12.451700
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 08:46:56.636244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.99
 ---- batch: 020 ----
mean loss: 352.20
 ---- batch: 030 ----
mean loss: 362.70
 ---- batch: 040 ----
mean loss: 364.36
train mean loss: 356.63
epoch train time: 0:00:08.481408
elapsed time: 0:09:20.934378
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 08:47:05.118947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.58
 ---- batch: 020 ----
mean loss: 350.97
 ---- batch: 030 ----
mean loss: 334.36
 ---- batch: 040 ----
mean loss: 348.05
train mean loss: 351.05
epoch train time: 0:00:08.501356
elapsed time: 0:09:29.437031
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 08:47:13.621665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.60
 ---- batch: 020 ----
mean loss: 348.60
 ---- batch: 030 ----
mean loss: 344.67
 ---- batch: 040 ----
mean loss: 351.62
train mean loss: 347.97
epoch train time: 0:00:08.475257
elapsed time: 0:09:37.913641
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 08:47:22.098160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.85
 ---- batch: 020 ----
mean loss: 352.19
 ---- batch: 030 ----
mean loss: 337.42
 ---- batch: 040 ----
mean loss: 352.18
train mean loss: 346.21
epoch train time: 0:00:08.473605
elapsed time: 0:09:46.388528
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 08:47:30.573117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.79
 ---- batch: 020 ----
mean loss: 340.87
 ---- batch: 030 ----
mean loss: 345.14
 ---- batch: 040 ----
mean loss: 340.92
train mean loss: 338.34
epoch train time: 0:00:08.488571
elapsed time: 0:09:54.878418
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 08:47:39.062984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.40
 ---- batch: 020 ----
mean loss: 348.69
 ---- batch: 030 ----
mean loss: 332.22
 ---- batch: 040 ----
mean loss: 339.22
train mean loss: 339.16
epoch train time: 0:00:08.503563
elapsed time: 0:10:03.383227
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 08:47:47.567772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.46
 ---- batch: 020 ----
mean loss: 344.39
 ---- batch: 030 ----
mean loss: 343.64
 ---- batch: 040 ----
mean loss: 337.03
train mean loss: 336.47
epoch train time: 0:00:08.474873
elapsed time: 0:10:11.859337
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 08:47:56.043915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.91
 ---- batch: 020 ----
mean loss: 321.34
 ---- batch: 030 ----
mean loss: 327.37
 ---- batch: 040 ----
mean loss: 325.35
train mean loss: 327.38
epoch train time: 0:00:08.476917
elapsed time: 0:10:20.337561
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 08:48:04.522100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.12
 ---- batch: 020 ----
mean loss: 325.53
 ---- batch: 030 ----
mean loss: 340.49
 ---- batch: 040 ----
mean loss: 324.83
train mean loss: 326.29
epoch train time: 0:00:08.473789
elapsed time: 0:10:28.812541
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 08:48:12.997097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.28
 ---- batch: 020 ----
mean loss: 327.75
 ---- batch: 030 ----
mean loss: 327.73
 ---- batch: 040 ----
mean loss: 317.48
train mean loss: 327.05
epoch train time: 0:00:08.477972
elapsed time: 0:10:37.291787
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 08:48:21.476327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.71
 ---- batch: 020 ----
mean loss: 321.26
 ---- batch: 030 ----
mean loss: 323.50
 ---- batch: 040 ----
mean loss: 318.10
train mean loss: 321.08
epoch train time: 0:00:08.488273
elapsed time: 0:10:45.781346
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 08:48:29.965917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.46
 ---- batch: 020 ----
mean loss: 323.01
 ---- batch: 030 ----
mean loss: 317.84
 ---- batch: 040 ----
mean loss: 321.97
train mean loss: 320.20
epoch train time: 0:00:08.454679
elapsed time: 0:10:54.237291
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 08:48:38.421846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.15
 ---- batch: 020 ----
mean loss: 307.61
 ---- batch: 030 ----
mean loss: 312.94
 ---- batch: 040 ----
mean loss: 317.15
train mean loss: 314.75
epoch train time: 0:00:08.482241
elapsed time: 0:11:02.720844
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 08:48:46.905411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.91
 ---- batch: 020 ----
mean loss: 306.79
 ---- batch: 030 ----
mean loss: 315.10
 ---- batch: 040 ----
mean loss: 317.26
train mean loss: 313.41
epoch train time: 0:00:08.478500
elapsed time: 0:11:11.200633
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 08:48:55.385197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.87
 ---- batch: 020 ----
mean loss: 323.29
 ---- batch: 030 ----
mean loss: 315.55
 ---- batch: 040 ----
mean loss: 306.12
train mean loss: 313.77
epoch train time: 0:00:08.480666
elapsed time: 0:11:19.682573
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 08:49:03.867149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.78
 ---- batch: 020 ----
mean loss: 311.00
 ---- batch: 030 ----
mean loss: 314.65
 ---- batch: 040 ----
mean loss: 301.56
train mean loss: 308.59
epoch train time: 0:00:08.504338
elapsed time: 0:11:28.188217
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 08:49:12.372740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.79
 ---- batch: 020 ----
mean loss: 319.63
 ---- batch: 030 ----
mean loss: 316.65
 ---- batch: 040 ----
mean loss: 311.69
train mean loss: 316.03
epoch train time: 0:00:08.478290
elapsed time: 0:11:36.667702
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 08:49:20.852217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.52
 ---- batch: 020 ----
mean loss: 300.34
 ---- batch: 030 ----
mean loss: 302.26
 ---- batch: 040 ----
mean loss: 308.41
train mean loss: 308.07
epoch train time: 0:00:08.473693
elapsed time: 0:11:45.142572
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 08:49:29.327167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.38
 ---- batch: 020 ----
mean loss: 301.72
 ---- batch: 030 ----
mean loss: 301.73
 ---- batch: 040 ----
mean loss: 311.84
train mean loss: 304.73
epoch train time: 0:00:08.474060
elapsed time: 0:11:53.618015
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 08:49:37.802557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.23
 ---- batch: 020 ----
mean loss: 301.56
 ---- batch: 030 ----
mean loss: 307.64
 ---- batch: 040 ----
mean loss: 308.95
train mean loss: 306.01
epoch train time: 0:00:08.489071
elapsed time: 0:12:02.108249
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 08:49:46.292775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.45
 ---- batch: 020 ----
mean loss: 308.24
 ---- batch: 030 ----
mean loss: 304.97
 ---- batch: 040 ----
mean loss: 292.62
train mean loss: 304.56
epoch train time: 0:00:08.459626
elapsed time: 0:12:10.569463
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 08:49:54.754131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.38
 ---- batch: 020 ----
mean loss: 294.14
 ---- batch: 030 ----
mean loss: 299.19
 ---- batch: 040 ----
mean loss: 295.97
train mean loss: 294.65
epoch train time: 0:00:08.500341
elapsed time: 0:12:19.071162
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 08:50:03.255713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.69
 ---- batch: 020 ----
mean loss: 293.74
 ---- batch: 030 ----
mean loss: 297.94
 ---- batch: 040 ----
mean loss: 294.28
train mean loss: 294.40
epoch train time: 0:00:08.456881
elapsed time: 0:12:27.529373
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 08:50:11.713862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.42
 ---- batch: 020 ----
mean loss: 293.66
 ---- batch: 030 ----
mean loss: 298.02
 ---- batch: 040 ----
mean loss: 285.05
train mean loss: 293.55
epoch train time: 0:00:08.477523
elapsed time: 0:12:36.008102
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 08:50:20.192646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.65
 ---- batch: 020 ----
mean loss: 299.62
 ---- batch: 030 ----
mean loss: 294.55
 ---- batch: 040 ----
mean loss: 285.42
train mean loss: 294.78
epoch train time: 0:00:08.444260
elapsed time: 0:12:44.453587
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 08:50:28.638197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.01
 ---- batch: 020 ----
mean loss: 289.97
 ---- batch: 030 ----
mean loss: 289.64
 ---- batch: 040 ----
mean loss: 287.40
train mean loss: 289.85
epoch train time: 0:00:08.460043
elapsed time: 0:12:52.915014
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 08:50:37.099546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.90
 ---- batch: 020 ----
mean loss: 280.84
 ---- batch: 030 ----
mean loss: 287.66
 ---- batch: 040 ----
mean loss: 288.37
train mean loss: 287.53
epoch train time: 0:00:08.462406
elapsed time: 0:13:01.378642
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 08:50:45.563243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.36
 ---- batch: 020 ----
mean loss: 286.42
 ---- batch: 030 ----
mean loss: 280.91
 ---- batch: 040 ----
mean loss: 291.93
train mean loss: 288.25
epoch train time: 0:00:08.464258
elapsed time: 0:13:09.844446
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 08:50:54.029154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.00
 ---- batch: 020 ----
mean loss: 291.66
 ---- batch: 030 ----
mean loss: 286.38
 ---- batch: 040 ----
mean loss: 290.48
train mean loss: 287.35
epoch train time: 0:00:08.485726
elapsed time: 0:13:18.331541
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 08:51:02.516097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.47
 ---- batch: 020 ----
mean loss: 282.49
 ---- batch: 030 ----
mean loss: 289.29
 ---- batch: 040 ----
mean loss: 283.96
train mean loss: 284.18
epoch train time: 0:00:08.479477
elapsed time: 0:13:26.812310
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 08:51:10.996830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.80
 ---- batch: 020 ----
mean loss: 290.09
 ---- batch: 030 ----
mean loss: 281.15
 ---- batch: 040 ----
mean loss: 279.38
train mean loss: 282.12
epoch train time: 0:00:08.476224
elapsed time: 0:13:35.289929
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 08:51:19.474501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.74
 ---- batch: 020 ----
mean loss: 284.37
 ---- batch: 030 ----
mean loss: 266.26
 ---- batch: 040 ----
mean loss: 275.33
train mean loss: 281.08
epoch train time: 0:00:08.472576
elapsed time: 0:13:43.763949
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 08:51:27.948548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.64
 ---- batch: 020 ----
mean loss: 272.38
 ---- batch: 030 ----
mean loss: 277.93
 ---- batch: 040 ----
mean loss: 275.71
train mean loss: 278.84
epoch train time: 0:00:08.519246
elapsed time: 0:13:52.284511
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 08:51:36.469102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.78
 ---- batch: 020 ----
mean loss: 273.88
 ---- batch: 030 ----
mean loss: 278.36
 ---- batch: 040 ----
mean loss: 278.71
train mean loss: 282.35
epoch train time: 0:00:08.517887
elapsed time: 0:14:00.803725
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 08:51:44.988270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.05
 ---- batch: 020 ----
mean loss: 283.53
 ---- batch: 030 ----
mean loss: 275.25
 ---- batch: 040 ----
mean loss: 278.87
train mean loss: 278.77
epoch train time: 0:00:08.476364
elapsed time: 0:14:09.281366
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 08:51:53.465902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.78
 ---- batch: 020 ----
mean loss: 279.63
 ---- batch: 030 ----
mean loss: 285.50
 ---- batch: 040 ----
mean loss: 273.26
train mean loss: 279.32
epoch train time: 0:00:08.576251
elapsed time: 0:14:17.858812
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 08:52:02.043426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.92
 ---- batch: 020 ----
mean loss: 271.05
 ---- batch: 030 ----
mean loss: 283.66
 ---- batch: 040 ----
mean loss: 279.45
train mean loss: 275.30
epoch train time: 0:00:08.484798
elapsed time: 0:14:26.344868
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 08:52:10.529401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.00
 ---- batch: 020 ----
mean loss: 268.75
 ---- batch: 030 ----
mean loss: 269.22
 ---- batch: 040 ----
mean loss: 276.70
train mean loss: 271.70
epoch train time: 0:00:08.473959
elapsed time: 0:14:34.820342
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 08:52:19.004958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.61
 ---- batch: 020 ----
mean loss: 278.05
 ---- batch: 030 ----
mean loss: 272.25
 ---- batch: 040 ----
mean loss: 264.59
train mean loss: 271.35
epoch train time: 0:00:08.462213
elapsed time: 0:14:43.283838
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 08:52:27.468358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.78
 ---- batch: 020 ----
mean loss: 270.49
 ---- batch: 030 ----
mean loss: 271.52
 ---- batch: 040 ----
mean loss: 271.73
train mean loss: 273.24
epoch train time: 0:00:08.459712
elapsed time: 0:14:51.744748
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 08:52:35.929280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.39
 ---- batch: 020 ----
mean loss: 270.00
 ---- batch: 030 ----
mean loss: 268.71
 ---- batch: 040 ----
mean loss: 274.44
train mean loss: 270.83
epoch train time: 0:00:08.469523
elapsed time: 0:15:00.215533
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 08:52:44.400193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.80
 ---- batch: 020 ----
mean loss: 272.35
 ---- batch: 030 ----
mean loss: 275.88
 ---- batch: 040 ----
mean loss: 269.88
train mean loss: 269.59
epoch train time: 0:00:08.441480
elapsed time: 0:15:08.658381
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 08:52:52.842972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.60
 ---- batch: 020 ----
mean loss: 270.64
 ---- batch: 030 ----
mean loss: 265.97
 ---- batch: 040 ----
mean loss: 264.38
train mean loss: 268.58
epoch train time: 0:00:08.473736
elapsed time: 0:15:17.133426
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 08:53:01.317987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.19
 ---- batch: 020 ----
mean loss: 278.37
 ---- batch: 030 ----
mean loss: 267.42
 ---- batch: 040 ----
mean loss: 262.53
train mean loss: 267.91
epoch train time: 0:00:08.497181
elapsed time: 0:15:25.632047
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 08:53:09.816412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.45
 ---- batch: 020 ----
mean loss: 262.87
 ---- batch: 030 ----
mean loss: 267.17
 ---- batch: 040 ----
mean loss: 266.00
train mean loss: 266.12
epoch train time: 0:00:08.502445
elapsed time: 0:15:34.135612
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 08:53:18.320157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.30
 ---- batch: 020 ----
mean loss: 262.10
 ---- batch: 030 ----
mean loss: 265.43
 ---- batch: 040 ----
mean loss: 265.68
train mean loss: 263.13
epoch train time: 0:00:08.500092
elapsed time: 0:15:42.636937
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 08:53:26.821505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.08
 ---- batch: 020 ----
mean loss: 260.95
 ---- batch: 030 ----
mean loss: 254.73
 ---- batch: 040 ----
mean loss: 264.46
train mean loss: 263.14
epoch train time: 0:00:08.482467
elapsed time: 0:15:51.120629
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 08:53:35.305170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.54
 ---- batch: 020 ----
mean loss: 264.96
 ---- batch: 030 ----
mean loss: 263.80
 ---- batch: 040 ----
mean loss: 263.90
train mean loss: 261.11
epoch train time: 0:00:08.479722
elapsed time: 0:15:59.602027
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 08:53:43.786656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.05
 ---- batch: 020 ----
mean loss: 259.08
 ---- batch: 030 ----
mean loss: 263.07
 ---- batch: 040 ----
mean loss: 256.55
train mean loss: 261.25
epoch train time: 0:00:08.489066
elapsed time: 0:16:08.092486
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 08:53:52.277060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.51
 ---- batch: 020 ----
mean loss: 255.85
 ---- batch: 030 ----
mean loss: 261.00
 ---- batch: 040 ----
mean loss: 258.37
train mean loss: 259.36
epoch train time: 0:00:08.470906
elapsed time: 0:16:16.564988
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 08:54:00.749632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.01
 ---- batch: 020 ----
mean loss: 252.19
 ---- batch: 030 ----
mean loss: 256.51
 ---- batch: 040 ----
mean loss: 267.74
train mean loss: 258.98
epoch train time: 0:00:08.468874
elapsed time: 0:16:25.035293
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 08:54:09.219845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.07
 ---- batch: 020 ----
mean loss: 264.07
 ---- batch: 030 ----
mean loss: 260.53
 ---- batch: 040 ----
mean loss: 254.48
train mean loss: 258.28
epoch train time: 0:00:08.463891
elapsed time: 0:16:33.500463
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 08:54:17.685016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.05
 ---- batch: 020 ----
mean loss: 257.67
 ---- batch: 030 ----
mean loss: 256.95
 ---- batch: 040 ----
mean loss: 259.36
train mean loss: 258.39
epoch train time: 0:00:08.457530
elapsed time: 0:16:41.959303
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 08:54:26.143890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.92
 ---- batch: 020 ----
mean loss: 249.83
 ---- batch: 030 ----
mean loss: 254.09
 ---- batch: 040 ----
mean loss: 257.57
train mean loss: 255.63
epoch train time: 0:00:08.468754
elapsed time: 0:16:50.429422
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 08:54:34.613974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.93
 ---- batch: 020 ----
mean loss: 252.26
 ---- batch: 030 ----
mean loss: 260.48
 ---- batch: 040 ----
mean loss: 258.27
train mean loss: 257.53
epoch train time: 0:00:08.455903
elapsed time: 0:16:58.886580
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 08:54:43.071120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.14
 ---- batch: 020 ----
mean loss: 256.81
 ---- batch: 030 ----
mean loss: 266.80
 ---- batch: 040 ----
mean loss: 253.01
train mean loss: 256.38
epoch train time: 0:00:08.465061
elapsed time: 0:17:07.352910
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 08:54:51.537556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.58
 ---- batch: 020 ----
mean loss: 251.71
 ---- batch: 030 ----
mean loss: 254.03
 ---- batch: 040 ----
mean loss: 261.67
train mean loss: 255.33
epoch train time: 0:00:08.450632
elapsed time: 0:17:15.804964
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 08:54:59.989520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.61
 ---- batch: 020 ----
mean loss: 255.14
 ---- batch: 030 ----
mean loss: 258.52
 ---- batch: 040 ----
mean loss: 257.83
train mean loss: 254.76
epoch train time: 0:00:08.477630
elapsed time: 0:17:24.283851
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 08:55:08.468413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.38
 ---- batch: 020 ----
mean loss: 246.12
 ---- batch: 030 ----
mean loss: 251.13
 ---- batch: 040 ----
mean loss: 250.24
train mean loss: 251.21
epoch train time: 0:00:08.464949
elapsed time: 0:17:32.750138
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 08:55:16.934728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.17
 ---- batch: 020 ----
mean loss: 249.14
 ---- batch: 030 ----
mean loss: 250.92
 ---- batch: 040 ----
mean loss: 242.88
train mean loss: 248.83
epoch train time: 0:00:08.464312
elapsed time: 0:17:41.215802
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 08:55:25.400421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.87
 ---- batch: 020 ----
mean loss: 250.35
 ---- batch: 030 ----
mean loss: 253.07
 ---- batch: 040 ----
mean loss: 251.12
train mean loss: 249.64
epoch train time: 0:00:08.484959
elapsed time: 0:17:49.702094
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 08:55:33.886689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.93
 ---- batch: 020 ----
mean loss: 250.46
 ---- batch: 030 ----
mean loss: 245.32
 ---- batch: 040 ----
mean loss: 256.59
train mean loss: 249.37
epoch train time: 0:00:08.477069
elapsed time: 0:17:58.180435
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 08:55:42.364996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.53
 ---- batch: 020 ----
mean loss: 241.30
 ---- batch: 030 ----
mean loss: 245.12
 ---- batch: 040 ----
mean loss: 252.24
train mean loss: 247.47
epoch train time: 0:00:08.475269
elapsed time: 0:18:06.657065
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 08:55:50.841633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.05
 ---- batch: 020 ----
mean loss: 250.18
 ---- batch: 030 ----
mean loss: 256.91
 ---- batch: 040 ----
mean loss: 238.83
train mean loss: 247.86
epoch train time: 0:00:08.453862
elapsed time: 0:18:15.112442
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 08:55:59.296795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.32
 ---- batch: 020 ----
mean loss: 259.18
 ---- batch: 030 ----
mean loss: 237.85
 ---- batch: 040 ----
mean loss: 243.47
train mean loss: 247.17
epoch train time: 0:00:08.482972
elapsed time: 0:18:23.596534
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 08:56:07.781026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.50
 ---- batch: 020 ----
mean loss: 243.28
 ---- batch: 030 ----
mean loss: 238.83
 ---- batch: 040 ----
mean loss: 246.72
train mean loss: 244.28
epoch train time: 0:00:08.473891
elapsed time: 0:18:32.071591
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 08:56:16.256183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.76
 ---- batch: 020 ----
mean loss: 242.42
 ---- batch: 030 ----
mean loss: 241.86
 ---- batch: 040 ----
mean loss: 244.94
train mean loss: 243.79
epoch train time: 0:00:08.477798
elapsed time: 0:18:40.550692
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 08:56:24.735235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.28
 ---- batch: 020 ----
mean loss: 240.39
 ---- batch: 030 ----
mean loss: 240.12
 ---- batch: 040 ----
mean loss: 239.97
train mean loss: 244.23
epoch train time: 0:00:08.458298
elapsed time: 0:18:49.010197
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 08:56:33.194742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.06
 ---- batch: 020 ----
mean loss: 243.70
 ---- batch: 030 ----
mean loss: 248.36
 ---- batch: 040 ----
mean loss: 236.29
train mean loss: 243.14
epoch train time: 0:00:08.506645
elapsed time: 0:18:57.518121
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 08:56:41.702702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.17
 ---- batch: 020 ----
mean loss: 246.53
 ---- batch: 030 ----
mean loss: 239.36
 ---- batch: 040 ----
mean loss: 247.75
train mean loss: 243.41
epoch train time: 0:00:08.466939
elapsed time: 0:19:05.986403
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 08:56:50.170934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.96
 ---- batch: 020 ----
mean loss: 242.17
 ---- batch: 030 ----
mean loss: 242.91
 ---- batch: 040 ----
mean loss: 238.71
train mean loss: 241.05
epoch train time: 0:00:08.457092
elapsed time: 0:19:14.444686
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 08:56:58.629228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.53
 ---- batch: 020 ----
mean loss: 241.00
 ---- batch: 030 ----
mean loss: 235.79
 ---- batch: 040 ----
mean loss: 239.99
train mean loss: 240.10
epoch train time: 0:00:08.527008
elapsed time: 0:19:22.972934
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 08:57:07.157509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.57
 ---- batch: 020 ----
mean loss: 237.10
 ---- batch: 030 ----
mean loss: 249.26
 ---- batch: 040 ----
mean loss: 242.16
train mean loss: 240.43
epoch train time: 0:00:08.541307
elapsed time: 0:19:31.515724
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 08:57:15.700319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.88
 ---- batch: 020 ----
mean loss: 242.24
 ---- batch: 030 ----
mean loss: 234.61
 ---- batch: 040 ----
mean loss: 240.58
train mean loss: 239.79
epoch train time: 0:00:08.485785
elapsed time: 0:19:40.002787
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 08:57:24.187361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.43
 ---- batch: 020 ----
mean loss: 245.47
 ---- batch: 030 ----
mean loss: 237.41
 ---- batch: 040 ----
mean loss: 232.87
train mean loss: 238.08
epoch train time: 0:00:08.468402
elapsed time: 0:19:48.472471
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 08:57:32.657046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.30
 ---- batch: 020 ----
mean loss: 237.55
 ---- batch: 030 ----
mean loss: 244.91
 ---- batch: 040 ----
mean loss: 237.50
train mean loss: 237.93
epoch train time: 0:00:08.486835
elapsed time: 0:19:56.960554
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 08:57:41.145103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.14
 ---- batch: 020 ----
mean loss: 241.86
 ---- batch: 030 ----
mean loss: 232.93
 ---- batch: 040 ----
mean loss: 246.02
train mean loss: 237.95
epoch train time: 0:00:08.473060
elapsed time: 0:20:05.434881
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 08:57:49.619462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.19
 ---- batch: 020 ----
mean loss: 234.77
 ---- batch: 030 ----
mean loss: 230.79
 ---- batch: 040 ----
mean loss: 243.26
train mean loss: 236.76
epoch train time: 0:00:08.502577
elapsed time: 0:20:13.938814
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 08:57:58.123359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.91
 ---- batch: 020 ----
mean loss: 234.62
 ---- batch: 030 ----
mean loss: 232.50
 ---- batch: 040 ----
mean loss: 237.40
train mean loss: 235.88
epoch train time: 0:00:08.492138
elapsed time: 0:20:22.432234
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 08:58:06.616776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.40
 ---- batch: 020 ----
mean loss: 239.23
 ---- batch: 030 ----
mean loss: 235.99
 ---- batch: 040 ----
mean loss: 240.62
train mean loss: 236.72
epoch train time: 0:00:08.457370
elapsed time: 0:20:30.890816
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 08:58:15.075365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.17
 ---- batch: 020 ----
mean loss: 230.93
 ---- batch: 030 ----
mean loss: 240.88
 ---- batch: 040 ----
mean loss: 237.15
train mean loss: 235.74
epoch train time: 0:00:08.455899
elapsed time: 0:20:39.347971
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 08:58:23.532517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.27
 ---- batch: 020 ----
mean loss: 234.34
 ---- batch: 030 ----
mean loss: 229.20
 ---- batch: 040 ----
mean loss: 242.39
train mean loss: 236.33
epoch train time: 0:00:08.463952
elapsed time: 0:20:47.813148
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 08:58:31.997710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.17
 ---- batch: 020 ----
mean loss: 228.58
 ---- batch: 030 ----
mean loss: 237.22
 ---- batch: 040 ----
mean loss: 229.46
train mean loss: 233.39
epoch train time: 0:00:08.471716
elapsed time: 0:20:56.286217
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 08:58:40.470826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.89
 ---- batch: 020 ----
mean loss: 242.03
 ---- batch: 030 ----
mean loss: 240.52
 ---- batch: 040 ----
mean loss: 228.39
train mean loss: 235.08
epoch train time: 0:00:08.465308
elapsed time: 0:21:04.752873
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 08:58:48.937486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.11
 ---- batch: 020 ----
mean loss: 237.04
 ---- batch: 030 ----
mean loss: 237.10
 ---- batch: 040 ----
mean loss: 237.80
train mean loss: 233.87
epoch train time: 0:00:08.473265
elapsed time: 0:21:13.227599
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 08:58:57.412171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.37
 ---- batch: 020 ----
mean loss: 223.52
 ---- batch: 030 ----
mean loss: 236.29
 ---- batch: 040 ----
mean loss: 231.39
train mean loss: 233.18
epoch train time: 0:00:08.480932
elapsed time: 0:21:21.710159
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 08:59:05.894455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.25
 ---- batch: 020 ----
mean loss: 235.28
 ---- batch: 030 ----
mean loss: 224.95
 ---- batch: 040 ----
mean loss: 234.11
train mean loss: 231.70
epoch train time: 0:00:08.529899
elapsed time: 0:21:30.241001
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 08:59:14.425513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.17
 ---- batch: 020 ----
mean loss: 238.82
 ---- batch: 030 ----
mean loss: 235.83
 ---- batch: 040 ----
mean loss: 225.52
train mean loss: 232.64
epoch train time: 0:00:08.504895
elapsed time: 0:21:38.747169
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 08:59:22.931687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.86
 ---- batch: 020 ----
mean loss: 227.81
 ---- batch: 030 ----
mean loss: 229.00
 ---- batch: 040 ----
mean loss: 232.75
train mean loss: 230.18
epoch train time: 0:00:08.525799
elapsed time: 0:21:47.274330
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 08:59:31.458872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.46
 ---- batch: 020 ----
mean loss: 231.65
 ---- batch: 030 ----
mean loss: 229.65
 ---- batch: 040 ----
mean loss: 234.82
train mean loss: 230.57
epoch train time: 0:00:08.497717
elapsed time: 0:21:55.773269
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 08:59:39.957831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.04
 ---- batch: 020 ----
mean loss: 232.55
 ---- batch: 030 ----
mean loss: 231.61
 ---- batch: 040 ----
mean loss: 225.96
train mean loss: 230.45
epoch train time: 0:00:08.497236
elapsed time: 0:22:04.271857
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 08:59:48.456401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.15
 ---- batch: 020 ----
mean loss: 238.40
 ---- batch: 030 ----
mean loss: 229.33
 ---- batch: 040 ----
mean loss: 220.23
train mean loss: 230.70
epoch train time: 0:00:08.489825
elapsed time: 0:22:12.762912
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 08:59:56.947463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.40
 ---- batch: 020 ----
mean loss: 224.32
 ---- batch: 030 ----
mean loss: 226.19
 ---- batch: 040 ----
mean loss: 232.56
train mean loss: 227.67
epoch train time: 0:00:08.501375
elapsed time: 0:22:21.265507
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 09:00:05.450089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.81
 ---- batch: 020 ----
mean loss: 226.16
 ---- batch: 030 ----
mean loss: 228.47
 ---- batch: 040 ----
mean loss: 225.80
train mean loss: 228.67
epoch train time: 0:00:08.496311
elapsed time: 0:22:29.763236
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 09:00:13.947633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.51
 ---- batch: 020 ----
mean loss: 228.38
 ---- batch: 030 ----
mean loss: 226.85
 ---- batch: 040 ----
mean loss: 233.09
train mean loss: 230.12
epoch train time: 0:00:08.484932
elapsed time: 0:22:38.249264
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 09:00:22.433861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.14
 ---- batch: 020 ----
mean loss: 224.40
 ---- batch: 030 ----
mean loss: 224.57
 ---- batch: 040 ----
mean loss: 232.97
train mean loss: 227.26
epoch train time: 0:00:08.489188
elapsed time: 0:22:46.739885
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 09:00:30.924464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.00
 ---- batch: 020 ----
mean loss: 225.73
 ---- batch: 030 ----
mean loss: 228.97
 ---- batch: 040 ----
mean loss: 220.69
train mean loss: 226.85
epoch train time: 0:00:08.484332
elapsed time: 0:22:55.225516
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 09:00:39.410040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.38
 ---- batch: 020 ----
mean loss: 230.42
 ---- batch: 030 ----
mean loss: 225.24
 ---- batch: 040 ----
mean loss: 223.09
train mean loss: 226.56
epoch train time: 0:00:08.466670
elapsed time: 0:23:03.693374
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 09:00:47.877929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.65
 ---- batch: 020 ----
mean loss: 226.53
 ---- batch: 030 ----
mean loss: 227.51
 ---- batch: 040 ----
mean loss: 231.23
train mean loss: 227.04
epoch train time: 0:00:08.505671
elapsed time: 0:23:12.200293
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 09:00:56.384870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.60
 ---- batch: 020 ----
mean loss: 219.92
 ---- batch: 030 ----
mean loss: 225.04
 ---- batch: 040 ----
mean loss: 227.74
train mean loss: 225.05
epoch train time: 0:00:08.494947
elapsed time: 0:23:20.696589
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 09:01:04.881101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.73
 ---- batch: 020 ----
mean loss: 225.63
 ---- batch: 030 ----
mean loss: 229.98
 ---- batch: 040 ----
mean loss: 230.57
train mean loss: 228.37
epoch train time: 0:00:08.510458
elapsed time: 0:23:29.208217
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 09:01:13.392799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.90
 ---- batch: 020 ----
mean loss: 231.60
 ---- batch: 030 ----
mean loss: 218.68
 ---- batch: 040 ----
mean loss: 226.25
train mean loss: 226.93
epoch train time: 0:00:08.489539
elapsed time: 0:23:37.699141
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 09:01:21.883693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.78
 ---- batch: 020 ----
mean loss: 218.31
 ---- batch: 030 ----
mean loss: 224.14
 ---- batch: 040 ----
mean loss: 230.75
train mean loss: 224.49
epoch train time: 0:00:08.491276
elapsed time: 0:23:46.191794
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 09:01:30.376389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.87
 ---- batch: 020 ----
mean loss: 228.74
 ---- batch: 030 ----
mean loss: 224.25
 ---- batch: 040 ----
mean loss: 220.96
train mean loss: 225.35
epoch train time: 0:00:08.510987
elapsed time: 0:23:54.704182
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 09:01:38.888749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.63
 ---- batch: 020 ----
mean loss: 228.47
 ---- batch: 030 ----
mean loss: 216.92
 ---- batch: 040 ----
mean loss: 219.42
train mean loss: 223.64
epoch train time: 0:00:08.526935
elapsed time: 0:24:03.232354
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 09:01:47.416901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.79
 ---- batch: 020 ----
mean loss: 223.94
 ---- batch: 030 ----
mean loss: 226.28
 ---- batch: 040 ----
mean loss: 216.96
train mean loss: 224.60
epoch train time: 0:00:08.497387
elapsed time: 0:24:11.731050
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 09:01:55.915620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.46
 ---- batch: 020 ----
mean loss: 214.37
 ---- batch: 030 ----
mean loss: 225.15
 ---- batch: 040 ----
mean loss: 230.03
train mean loss: 224.35
epoch train time: 0:00:08.512022
elapsed time: 0:24:20.244273
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 09:02:04.428803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.26
 ---- batch: 020 ----
mean loss: 219.81
 ---- batch: 030 ----
mean loss: 211.41
 ---- batch: 040 ----
mean loss: 229.50
train mean loss: 222.45
epoch train time: 0:00:08.499038
elapsed time: 0:24:28.744615
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 09:02:12.929233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.80
 ---- batch: 020 ----
mean loss: 217.63
 ---- batch: 030 ----
mean loss: 222.72
 ---- batch: 040 ----
mean loss: 226.17
train mean loss: 221.04
epoch train time: 0:00:08.566314
elapsed time: 0:24:37.312216
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 09:02:21.496742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.90
 ---- batch: 020 ----
mean loss: 218.90
 ---- batch: 030 ----
mean loss: 227.85
 ---- batch: 040 ----
mean loss: 223.88
train mean loss: 222.13
epoch train time: 0:00:08.522372
elapsed time: 0:24:45.835902
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 09:02:30.020458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.53
 ---- batch: 020 ----
mean loss: 221.95
 ---- batch: 030 ----
mean loss: 219.91
 ---- batch: 040 ----
mean loss: 217.88
train mean loss: 222.74
epoch train time: 0:00:08.495971
elapsed time: 0:24:54.333396
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 09:02:38.517711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.26
 ---- batch: 020 ----
mean loss: 224.90
 ---- batch: 030 ----
mean loss: 217.31
 ---- batch: 040 ----
mean loss: 224.78
train mean loss: 222.78
epoch train time: 0:00:08.485093
elapsed time: 0:25:02.819480
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 09:02:47.004052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.27
 ---- batch: 020 ----
mean loss: 213.27
 ---- batch: 030 ----
mean loss: 223.46
 ---- batch: 040 ----
mean loss: 223.83
train mean loss: 218.66
epoch train time: 0:00:08.489460
elapsed time: 0:25:11.310233
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 09:02:55.494786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.56
 ---- batch: 020 ----
mean loss: 222.48
 ---- batch: 030 ----
mean loss: 220.82
 ---- batch: 040 ----
mean loss: 219.54
train mean loss: 221.16
epoch train time: 0:00:08.496825
elapsed time: 0:25:19.808273
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 09:03:03.992836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.25
 ---- batch: 020 ----
mean loss: 215.11
 ---- batch: 030 ----
mean loss: 214.20
 ---- batch: 040 ----
mean loss: 218.83
train mean loss: 218.43
epoch train time: 0:00:08.504944
elapsed time: 0:25:28.314430
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 09:03:12.498976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.75
 ---- batch: 020 ----
mean loss: 218.67
 ---- batch: 030 ----
mean loss: 220.45
 ---- batch: 040 ----
mean loss: 215.78
train mean loss: 220.78
epoch train time: 0:00:08.486410
elapsed time: 0:25:36.802086
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 09:03:20.986691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.83
 ---- batch: 020 ----
mean loss: 213.50
 ---- batch: 030 ----
mean loss: 219.33
 ---- batch: 040 ----
mean loss: 224.52
train mean loss: 217.93
epoch train time: 0:00:08.466061
elapsed time: 0:25:45.269538
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 09:03:29.454091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.00
 ---- batch: 020 ----
mean loss: 223.71
 ---- batch: 030 ----
mean loss: 223.24
 ---- batch: 040 ----
mean loss: 210.97
train mean loss: 218.05
epoch train time: 0:00:08.477035
elapsed time: 0:25:53.748056
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 09:03:37.932505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.85
 ---- batch: 020 ----
mean loss: 213.03
 ---- batch: 030 ----
mean loss: 214.46
 ---- batch: 040 ----
mean loss: 218.90
train mean loss: 215.84
epoch train time: 0:00:08.476444
elapsed time: 0:26:02.225625
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 09:03:46.410204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.31
 ---- batch: 020 ----
mean loss: 214.68
 ---- batch: 030 ----
mean loss: 217.26
 ---- batch: 040 ----
mean loss: 216.74
train mean loss: 217.02
epoch train time: 0:00:08.492449
elapsed time: 0:26:10.719510
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 09:03:54.904095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.04
 ---- batch: 020 ----
mean loss: 221.07
 ---- batch: 030 ----
mean loss: 220.72
 ---- batch: 040 ----
mean loss: 215.30
train mean loss: 216.77
epoch train time: 0:00:08.479423
elapsed time: 0:26:19.200204
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 09:04:03.384760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.55
 ---- batch: 020 ----
mean loss: 220.17
 ---- batch: 030 ----
mean loss: 215.32
 ---- batch: 040 ----
mean loss: 217.09
train mean loss: 216.84
epoch train time: 0:00:08.474422
elapsed time: 0:26:27.675868
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 09:04:11.860585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.23
 ---- batch: 020 ----
mean loss: 218.77
 ---- batch: 030 ----
mean loss: 216.62
 ---- batch: 040 ----
mean loss: 217.27
train mean loss: 218.45
epoch train time: 0:00:08.458318
elapsed time: 0:26:36.135584
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 09:04:20.320160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.86
 ---- batch: 020 ----
mean loss: 230.13
 ---- batch: 030 ----
mean loss: 216.51
 ---- batch: 040 ----
mean loss: 215.30
train mean loss: 218.36
epoch train time: 0:00:08.454843
elapsed time: 0:26:44.591742
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 09:04:28.776313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.26
 ---- batch: 020 ----
mean loss: 218.22
 ---- batch: 030 ----
mean loss: 221.75
 ---- batch: 040 ----
mean loss: 212.25
train mean loss: 216.93
epoch train time: 0:00:08.469664
elapsed time: 0:26:53.062740
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 09:04:37.247340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.77
 ---- batch: 020 ----
mean loss: 217.99
 ---- batch: 030 ----
mean loss: 215.12
 ---- batch: 040 ----
mean loss: 212.24
train mean loss: 216.43
epoch train time: 0:00:08.456336
elapsed time: 0:27:01.520367
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 09:04:45.704916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.97
 ---- batch: 020 ----
mean loss: 217.83
 ---- batch: 030 ----
mean loss: 219.88
 ---- batch: 040 ----
mean loss: 215.96
train mean loss: 214.26
epoch train time: 0:00:08.458434
elapsed time: 0:27:09.980086
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 09:04:54.164631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.00
 ---- batch: 020 ----
mean loss: 218.75
 ---- batch: 030 ----
mean loss: 208.38
 ---- batch: 040 ----
mean loss: 211.12
train mean loss: 215.40
epoch train time: 0:00:08.462577
elapsed time: 0:27:18.443896
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 09:05:02.628394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.46
 ---- batch: 020 ----
mean loss: 219.02
 ---- batch: 030 ----
mean loss: 216.40
 ---- batch: 040 ----
mean loss: 212.04
train mean loss: 214.31
epoch train time: 0:00:08.482774
elapsed time: 0:27:26.927880
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 09:05:11.112465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.36
 ---- batch: 020 ----
mean loss: 219.12
 ---- batch: 030 ----
mean loss: 217.53
 ---- batch: 040 ----
mean loss: 212.07
train mean loss: 215.22
epoch train time: 0:00:08.466125
elapsed time: 0:27:35.395268
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 09:05:19.579822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.94
 ---- batch: 020 ----
mean loss: 208.53
 ---- batch: 030 ----
mean loss: 215.97
 ---- batch: 040 ----
mean loss: 219.32
train mean loss: 216.64
epoch train time: 0:00:08.470300
elapsed time: 0:27:43.866950
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 09:05:28.051493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.79
 ---- batch: 020 ----
mean loss: 215.18
 ---- batch: 030 ----
mean loss: 217.55
 ---- batch: 040 ----
mean loss: 215.12
train mean loss: 215.53
epoch train time: 0:00:08.466875
elapsed time: 0:27:52.335135
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 09:05:36.519748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.93
 ---- batch: 020 ----
mean loss: 207.66
 ---- batch: 030 ----
mean loss: 216.30
 ---- batch: 040 ----
mean loss: 218.83
train mean loss: 214.50
epoch train time: 0:00:08.460930
elapsed time: 0:28:00.797406
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 09:05:44.981988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.14
 ---- batch: 020 ----
mean loss: 216.81
 ---- batch: 030 ----
mean loss: 211.28
 ---- batch: 040 ----
mean loss: 212.01
train mean loss: 214.32
epoch train time: 0:00:08.440860
elapsed time: 0:28:09.239601
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 09:05:53.424236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.43
 ---- batch: 020 ----
mean loss: 214.68
 ---- batch: 030 ----
mean loss: 209.34
 ---- batch: 040 ----
mean loss: 217.78
train mean loss: 212.74
epoch train time: 0:00:08.449513
elapsed time: 0:28:17.690489
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 09:06:01.875053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.62
 ---- batch: 020 ----
mean loss: 219.81
 ---- batch: 030 ----
mean loss: 211.32
 ---- batch: 040 ----
mean loss: 212.79
train mean loss: 213.11
epoch train time: 0:00:08.445495
elapsed time: 0:28:26.137252
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 09:06:10.321826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.22
 ---- batch: 020 ----
mean loss: 213.94
 ---- batch: 030 ----
mean loss: 216.53
 ---- batch: 040 ----
mean loss: 212.97
train mean loss: 213.06
epoch train time: 0:00:08.465925
elapsed time: 0:28:34.604456
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 09:06:18.789080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.73
 ---- batch: 020 ----
mean loss: 223.00
 ---- batch: 030 ----
mean loss: 210.15
 ---- batch: 040 ----
mean loss: 216.24
train mean loss: 213.03
epoch train time: 0:00:08.449600
elapsed time: 0:28:43.055348
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 09:06:27.239906
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.23
 ---- batch: 020 ----
mean loss: 211.48
 ---- batch: 030 ----
mean loss: 208.38
 ---- batch: 040 ----
mean loss: 215.72
train mean loss: 210.49
epoch train time: 0:00:08.471856
elapsed time: 0:28:51.529031
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 09:06:35.713348
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.94
 ---- batch: 020 ----
mean loss: 203.59
 ---- batch: 030 ----
mean loss: 210.11
 ---- batch: 040 ----
mean loss: 210.27
train mean loss: 209.46
epoch train time: 0:00:08.478789
elapsed time: 0:29:00.009015
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 09:06:44.193552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.74
 ---- batch: 020 ----
mean loss: 208.11
 ---- batch: 030 ----
mean loss: 212.62
 ---- batch: 040 ----
mean loss: 200.31
train mean loss: 209.56
epoch train time: 0:00:08.505684
elapsed time: 0:29:08.515929
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 09:06:52.700474
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.13
 ---- batch: 020 ----
mean loss: 208.28
 ---- batch: 030 ----
mean loss: 212.71
 ---- batch: 040 ----
mean loss: 211.42
train mean loss: 209.45
epoch train time: 0:00:08.442248
elapsed time: 0:29:16.959385
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 09:07:01.143931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.41
 ---- batch: 020 ----
mean loss: 208.92
 ---- batch: 030 ----
mean loss: 203.49
 ---- batch: 040 ----
mean loss: 210.93
train mean loss: 209.48
epoch train time: 0:00:08.451415
elapsed time: 0:29:25.412119
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 09:07:09.596674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.54
 ---- batch: 020 ----
mean loss: 206.39
 ---- batch: 030 ----
mean loss: 210.68
 ---- batch: 040 ----
mean loss: 208.32
train mean loss: 210.16
epoch train time: 0:00:08.455702
elapsed time: 0:29:33.869050
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 09:07:18.053714
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.22
 ---- batch: 020 ----
mean loss: 204.79
 ---- batch: 030 ----
mean loss: 206.19
 ---- batch: 040 ----
mean loss: 208.22
train mean loss: 208.27
epoch train time: 0:00:08.456699
elapsed time: 0:29:42.327109
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 09:07:26.511652
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.46
 ---- batch: 020 ----
mean loss: 215.21
 ---- batch: 030 ----
mean loss: 210.20
 ---- batch: 040 ----
mean loss: 207.33
train mean loss: 210.02
epoch train time: 0:00:08.518577
elapsed time: 0:29:50.846919
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 09:07:35.031498
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.56
 ---- batch: 020 ----
mean loss: 213.13
 ---- batch: 030 ----
mean loss: 204.91
 ---- batch: 040 ----
mean loss: 212.64
train mean loss: 208.80
epoch train time: 0:00:08.455889
elapsed time: 0:29:59.304014
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 09:07:43.488521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.68
 ---- batch: 020 ----
mean loss: 217.11
 ---- batch: 030 ----
mean loss: 211.58
 ---- batch: 040 ----
mean loss: 203.70
train mean loss: 210.39
epoch train time: 0:00:08.454730
elapsed time: 0:30:07.759962
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 09:07:51.944514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.07
 ---- batch: 020 ----
mean loss: 213.58
 ---- batch: 030 ----
mean loss: 212.31
 ---- batch: 040 ----
mean loss: 200.63
train mean loss: 209.15
epoch train time: 0:00:08.433716
elapsed time: 0:30:16.194869
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 09:08:00.379396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.90
 ---- batch: 020 ----
mean loss: 204.69
 ---- batch: 030 ----
mean loss: 212.38
 ---- batch: 040 ----
mean loss: 205.55
train mean loss: 210.57
epoch train time: 0:00:08.440846
elapsed time: 0:30:24.636910
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 09:08:08.821441
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.86
 ---- batch: 020 ----
mean loss: 212.59
 ---- batch: 030 ----
mean loss: 206.09
 ---- batch: 040 ----
mean loss: 204.86
train mean loss: 209.24
epoch train time: 0:00:08.436096
elapsed time: 0:30:33.074345
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 09:08:17.258885
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.70
 ---- batch: 020 ----
mean loss: 210.93
 ---- batch: 030 ----
mean loss: 204.77
 ---- batch: 040 ----
mean loss: 213.20
train mean loss: 209.19
epoch train time: 0:00:08.422702
elapsed time: 0:30:41.498301
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 09:08:25.682814
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.71
 ---- batch: 020 ----
mean loss: 205.25
 ---- batch: 030 ----
mean loss: 209.50
 ---- batch: 040 ----
mean loss: 213.22
train mean loss: 209.46
epoch train time: 0:00:08.480241
elapsed time: 0:30:49.979758
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 09:08:34.164299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.86
 ---- batch: 020 ----
mean loss: 205.93
 ---- batch: 030 ----
mean loss: 208.55
 ---- batch: 040 ----
mean loss: 208.01
train mean loss: 209.60
epoch train time: 0:00:08.449497
elapsed time: 0:30:58.430511
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 09:08:42.615093
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.85
 ---- batch: 020 ----
mean loss: 205.39
 ---- batch: 030 ----
mean loss: 210.90
 ---- batch: 040 ----
mean loss: 212.92
train mean loss: 209.18
epoch train time: 0:00:08.439352
elapsed time: 0:31:06.871223
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 09:08:51.055769
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.39
 ---- batch: 020 ----
mean loss: 209.29
 ---- batch: 030 ----
mean loss: 215.81
 ---- batch: 040 ----
mean loss: 207.63
train mean loss: 210.46
epoch train time: 0:00:08.425562
elapsed time: 0:31:15.298003
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 09:08:59.482556
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.45
 ---- batch: 020 ----
mean loss: 216.00
 ---- batch: 030 ----
mean loss: 210.82
 ---- batch: 040 ----
mean loss: 210.66
train mean loss: 209.15
epoch train time: 0:00:08.420173
elapsed time: 0:31:23.719470
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 09:09:07.904067
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.99
 ---- batch: 020 ----
mean loss: 209.86
 ---- batch: 030 ----
mean loss: 207.13
 ---- batch: 040 ----
mean loss: 215.49
train mean loss: 209.98
epoch train time: 0:00:08.461752
elapsed time: 0:31:32.182473
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 09:09:16.367010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.45
 ---- batch: 020 ----
mean loss: 204.60
 ---- batch: 030 ----
mean loss: 209.04
 ---- batch: 040 ----
mean loss: 208.81
train mean loss: 208.99
epoch train time: 0:00:08.425552
elapsed time: 0:31:40.609333
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 09:09:24.793899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.54
 ---- batch: 020 ----
mean loss: 205.11
 ---- batch: 030 ----
mean loss: 202.34
 ---- batch: 040 ----
mean loss: 212.76
train mean loss: 208.68
epoch train time: 0:00:08.443211
elapsed time: 0:31:49.053865
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 09:09:33.238417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.06
 ---- batch: 020 ----
mean loss: 211.90
 ---- batch: 030 ----
mean loss: 209.07
 ---- batch: 040 ----
mean loss: 207.92
train mean loss: 209.38
epoch train time: 0:00:08.439574
elapsed time: 0:31:57.494741
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 09:09:41.679266
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.18
 ---- batch: 020 ----
mean loss: 207.14
 ---- batch: 030 ----
mean loss: 210.23
 ---- batch: 040 ----
mean loss: 207.94
train mean loss: 208.73
epoch train time: 0:00:08.449404
elapsed time: 0:32:05.945398
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 09:09:50.129947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.21
 ---- batch: 020 ----
mean loss: 216.92
 ---- batch: 030 ----
mean loss: 207.37
 ---- batch: 040 ----
mean loss: 202.44
train mean loss: 209.97
epoch train time: 0:00:08.444702
elapsed time: 0:32:14.391392
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 09:09:58.575944
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.62
 ---- batch: 020 ----
mean loss: 205.76
 ---- batch: 030 ----
mean loss: 212.74
 ---- batch: 040 ----
mean loss: 212.04
train mean loss: 208.90
epoch train time: 0:00:08.444138
elapsed time: 0:32:22.836807
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 09:10:07.021355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.87
 ---- batch: 020 ----
mean loss: 210.47
 ---- batch: 030 ----
mean loss: 207.72
 ---- batch: 040 ----
mean loss: 210.50
train mean loss: 209.40
epoch train time: 0:00:08.453048
elapsed time: 0:32:31.291099
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 09:10:15.475650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.44
 ---- batch: 020 ----
mean loss: 212.10
 ---- batch: 030 ----
mean loss: 210.40
 ---- batch: 040 ----
mean loss: 207.77
train mean loss: 208.90
epoch train time: 0:00:08.430583
elapsed time: 0:32:39.722988
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 09:10:23.907519
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.27
 ---- batch: 020 ----
mean loss: 206.85
 ---- batch: 030 ----
mean loss: 219.98
 ---- batch: 040 ----
mean loss: 202.45
train mean loss: 208.48
epoch train time: 0:00:08.423154
elapsed time: 0:32:48.147539
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 09:10:32.332124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.07
 ---- batch: 020 ----
mean loss: 203.79
 ---- batch: 030 ----
mean loss: 211.19
 ---- batch: 040 ----
mean loss: 210.05
train mean loss: 209.05
epoch train time: 0:00:08.424046
elapsed time: 0:32:56.572890
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 09:10:40.757469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.08
 ---- batch: 020 ----
mean loss: 206.95
 ---- batch: 030 ----
mean loss: 203.72
 ---- batch: 040 ----
mean loss: 208.19
train mean loss: 209.28
epoch train time: 0:00:08.427751
elapsed time: 0:33:05.001946
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 09:10:49.186545
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.10
 ---- batch: 020 ----
mean loss: 212.87
 ---- batch: 030 ----
mean loss: 205.67
 ---- batch: 040 ----
mean loss: 212.85
train mean loss: 208.89
epoch train time: 0:00:08.417322
elapsed time: 0:33:13.420580
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 09:10:57.605172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.22
 ---- batch: 020 ----
mean loss: 208.85
 ---- batch: 030 ----
mean loss: 209.12
 ---- batch: 040 ----
mean loss: 204.65
train mean loss: 208.94
epoch train time: 0:00:08.412450
elapsed time: 0:33:21.834665
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 09:11:06.018947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.28
 ---- batch: 020 ----
mean loss: 214.34
 ---- batch: 030 ----
mean loss: 208.24
 ---- batch: 040 ----
mean loss: 210.19
train mean loss: 208.85
epoch train time: 0:00:08.452463
elapsed time: 0:33:30.288089
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 09:11:14.472634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.09
 ---- batch: 020 ----
mean loss: 208.04
 ---- batch: 030 ----
mean loss: 208.78
 ---- batch: 040 ----
mean loss: 203.11
train mean loss: 208.70
epoch train time: 0:00:08.423922
elapsed time: 0:33:38.713228
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 09:11:22.897802
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.83
 ---- batch: 020 ----
mean loss: 204.56
 ---- batch: 030 ----
mean loss: 210.50
 ---- batch: 040 ----
mean loss: 205.73
train mean loss: 208.55
epoch train time: 0:00:08.432542
elapsed time: 0:33:47.147106
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 09:11:31.331658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.65
 ---- batch: 020 ----
mean loss: 214.37
 ---- batch: 030 ----
mean loss: 203.15
 ---- batch: 040 ----
mean loss: 202.32
train mean loss: 208.64
epoch train time: 0:00:08.411315
elapsed time: 0:33:55.559745
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 09:11:39.744298
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.46
 ---- batch: 020 ----
mean loss: 209.78
 ---- batch: 030 ----
mean loss: 207.31
 ---- batch: 040 ----
mean loss: 213.45
train mean loss: 208.98
epoch train time: 0:00:08.415969
elapsed time: 0:34:03.977082
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 09:11:48.161645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.57
 ---- batch: 020 ----
mean loss: 212.17
 ---- batch: 030 ----
mean loss: 206.18
 ---- batch: 040 ----
mean loss: 208.87
train mean loss: 207.97
epoch train time: 0:00:08.443933
elapsed time: 0:34:12.422304
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 09:11:56.606820
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.27
 ---- batch: 020 ----
mean loss: 212.79
 ---- batch: 030 ----
mean loss: 205.61
 ---- batch: 040 ----
mean loss: 203.43
train mean loss: 209.03
epoch train time: 0:00:08.427715
elapsed time: 0:34:20.851198
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 09:12:05.035764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.60
 ---- batch: 020 ----
mean loss: 212.04
 ---- batch: 030 ----
mean loss: 209.61
 ---- batch: 040 ----
mean loss: 206.15
train mean loss: 208.82
epoch train time: 0:00:08.387531
elapsed time: 0:34:29.239956
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 09:12:13.424562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.85
 ---- batch: 020 ----
mean loss: 218.15
 ---- batch: 030 ----
mean loss: 207.48
 ---- batch: 040 ----
mean loss: 204.79
train mean loss: 208.95
epoch train time: 0:00:08.412928
elapsed time: 0:34:37.654206
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 09:12:21.838780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.84
 ---- batch: 020 ----
mean loss: 209.15
 ---- batch: 030 ----
mean loss: 205.23
 ---- batch: 040 ----
mean loss: 216.19
train mean loss: 208.60
epoch train time: 0:00:08.390582
elapsed time: 0:34:46.046032
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 09:12:30.230566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.75
 ---- batch: 020 ----
mean loss: 211.46
 ---- batch: 030 ----
mean loss: 206.81
 ---- batch: 040 ----
mean loss: 214.55
train mean loss: 208.05
epoch train time: 0:00:08.416119
elapsed time: 0:34:54.463488
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 09:12:38.648037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.44
 ---- batch: 020 ----
mean loss: 206.31
 ---- batch: 030 ----
mean loss: 206.71
 ---- batch: 040 ----
mean loss: 207.53
train mean loss: 208.73
epoch train time: 0:00:08.428552
elapsed time: 0:35:02.893452
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 09:12:47.078001
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.96
 ---- batch: 020 ----
mean loss: 210.50
 ---- batch: 030 ----
mean loss: 209.21
 ---- batch: 040 ----
mean loss: 211.26
train mean loss: 209.14
epoch train time: 0:00:08.371636
elapsed time: 0:35:11.266387
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 09:12:55.450955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.60
 ---- batch: 020 ----
mean loss: 207.57
 ---- batch: 030 ----
mean loss: 204.45
 ---- batch: 040 ----
mean loss: 207.74
train mean loss: 208.19
epoch train time: 0:00:08.370361
elapsed time: 0:35:19.637996
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 09:13:03.822538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.88
 ---- batch: 020 ----
mean loss: 207.45
 ---- batch: 030 ----
mean loss: 205.02
 ---- batch: 040 ----
mean loss: 209.48
train mean loss: 208.38
epoch train time: 0:00:08.387014
elapsed time: 0:35:28.026536
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 09:13:12.211115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.62
 ---- batch: 020 ----
mean loss: 212.24
 ---- batch: 030 ----
mean loss: 204.07
 ---- batch: 040 ----
mean loss: 205.31
train mean loss: 208.38
epoch train time: 0:00:08.402093
elapsed time: 0:35:36.439736
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_3/checkpoint.pth.tar
**** end time: 2019-09-25 09:13:20.623981 ****
