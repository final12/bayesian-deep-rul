Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_6', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 6639
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 13:53:20.515549 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 13:53:20.533739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2716.81
 ---- batch: 020 ----
mean loss: 1400.91
 ---- batch: 030 ----
mean loss: 1252.60
 ---- batch: 040 ----
mean loss: 1177.19
 ---- batch: 050 ----
mean loss: 1164.95
train mean loss: 1488.35
epoch train time: 0:00:23.079674
elapsed time: 0:00:23.106135
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 13:53:43.621761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1093.08
 ---- batch: 020 ----
mean loss: 1071.68
 ---- batch: 030 ----
mean loss: 1020.03
 ---- batch: 040 ----
mean loss: 1030.19
 ---- batch: 050 ----
mean loss: 994.60
train mean loss: 1042.80
epoch train time: 0:00:07.988830
elapsed time: 0:00:31.095981
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 13:53:51.611976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 998.65
 ---- batch: 020 ----
mean loss: 1028.47
 ---- batch: 030 ----
mean loss: 992.73
 ---- batch: 040 ----
mean loss: 985.66
 ---- batch: 050 ----
mean loss: 972.91
train mean loss: 994.11
epoch train time: 0:00:08.006166
elapsed time: 0:00:39.103363
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 13:53:59.619267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 987.09
 ---- batch: 020 ----
mean loss: 954.75
 ---- batch: 030 ----
mean loss: 961.96
 ---- batch: 040 ----
mean loss: 953.40
 ---- batch: 050 ----
mean loss: 961.09
train mean loss: 961.86
epoch train time: 0:00:07.978393
elapsed time: 0:00:47.082997
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 13:54:07.598935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 965.89
 ---- batch: 020 ----
mean loss: 983.02
 ---- batch: 030 ----
mean loss: 905.81
 ---- batch: 040 ----
mean loss: 942.79
 ---- batch: 050 ----
mean loss: 928.82
train mean loss: 941.02
epoch train time: 0:00:07.979185
elapsed time: 0:00:55.063388
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 13:54:15.579309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.45
 ---- batch: 020 ----
mean loss: 934.65
 ---- batch: 030 ----
mean loss: 907.53
 ---- batch: 040 ----
mean loss: 923.19
 ---- batch: 050 ----
mean loss: 918.81
train mean loss: 930.62
epoch train time: 0:00:07.987879
elapsed time: 0:01:03.052399
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 13:54:23.568329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.75
 ---- batch: 020 ----
mean loss: 933.16
 ---- batch: 030 ----
mean loss: 916.83
 ---- batch: 040 ----
mean loss: 930.19
 ---- batch: 050 ----
mean loss: 893.30
train mean loss: 920.00
epoch train time: 0:00:08.007211
elapsed time: 0:01:11.060894
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 13:54:31.576847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.98
 ---- batch: 020 ----
mean loss: 926.04
 ---- batch: 030 ----
mean loss: 910.66
 ---- batch: 040 ----
mean loss: 920.48
 ---- batch: 050 ----
mean loss: 917.95
train mean loss: 915.55
epoch train time: 0:00:08.008100
elapsed time: 0:01:19.070244
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 13:54:39.586172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.93
 ---- batch: 020 ----
mean loss: 893.58
 ---- batch: 030 ----
mean loss: 891.41
 ---- batch: 040 ----
mean loss: 914.20
 ---- batch: 050 ----
mean loss: 918.98
train mean loss: 912.11
epoch train time: 0:00:07.998682
elapsed time: 0:01:27.070188
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 13:54:47.586128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.32
 ---- batch: 020 ----
mean loss: 903.49
 ---- batch: 030 ----
mean loss: 919.94
 ---- batch: 040 ----
mean loss: 919.73
 ---- batch: 050 ----
mean loss: 919.50
train mean loss: 910.81
epoch train time: 0:00:07.975794
elapsed time: 0:01:35.047133
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 13:54:55.563044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.31
 ---- batch: 020 ----
mean loss: 905.24
 ---- batch: 030 ----
mean loss: 911.51
 ---- batch: 040 ----
mean loss: 880.21
 ---- batch: 050 ----
mean loss: 914.32
train mean loss: 903.28
epoch train time: 0:00:07.988419
elapsed time: 0:01:43.036776
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 13:55:03.552720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.96
 ---- batch: 020 ----
mean loss: 883.21
 ---- batch: 030 ----
mean loss: 894.65
 ---- batch: 040 ----
mean loss: 898.62
 ---- batch: 050 ----
mean loss: 914.59
train mean loss: 897.53
epoch train time: 0:00:07.963893
elapsed time: 0:01:51.001837
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 13:55:11.517693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.57
 ---- batch: 020 ----
mean loss: 898.97
 ---- batch: 030 ----
mean loss: 911.53
 ---- batch: 040 ----
mean loss: 894.11
 ---- batch: 050 ----
mean loss: 892.23
train mean loss: 896.44
epoch train time: 0:00:07.976883
elapsed time: 0:01:58.979835
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 13:55:19.495752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.30
 ---- batch: 020 ----
mean loss: 886.92
 ---- batch: 030 ----
mean loss: 883.71
 ---- batch: 040 ----
mean loss: 902.28
 ---- batch: 050 ----
mean loss: 875.81
train mean loss: 892.08
epoch train time: 0:00:07.996393
elapsed time: 0:02:06.977441
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 13:55:27.493340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.25
 ---- batch: 020 ----
mean loss: 891.86
 ---- batch: 030 ----
mean loss: 895.52
 ---- batch: 040 ----
mean loss: 868.73
 ---- batch: 050 ----
mean loss: 885.93
train mean loss: 886.99
epoch train time: 0:00:08.008662
elapsed time: 0:02:14.987434
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 13:55:35.503496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.65
 ---- batch: 020 ----
mean loss: 878.07
 ---- batch: 030 ----
mean loss: 884.12
 ---- batch: 040 ----
mean loss: 895.95
 ---- batch: 050 ----
mean loss: 890.99
train mean loss: 888.48
epoch train time: 0:00:07.989132
elapsed time: 0:02:22.977879
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 13:55:43.493830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 897.73
 ---- batch: 020 ----
mean loss: 873.14
 ---- batch: 030 ----
mean loss: 876.72
 ---- batch: 040 ----
mean loss: 886.96
 ---- batch: 050 ----
mean loss: 877.67
train mean loss: 882.20
epoch train time: 0:00:08.000408
elapsed time: 0:02:30.979560
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 13:55:51.495503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.33
 ---- batch: 020 ----
mean loss: 882.34
 ---- batch: 030 ----
mean loss: 890.63
 ---- batch: 040 ----
mean loss: 856.13
 ---- batch: 050 ----
mean loss: 861.33
train mean loss: 879.07
epoch train time: 0:00:07.999027
elapsed time: 0:02:38.979890
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 13:55:59.495794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.07
 ---- batch: 020 ----
mean loss: 859.34
 ---- batch: 030 ----
mean loss: 899.04
 ---- batch: 040 ----
mean loss: 901.84
 ---- batch: 050 ----
mean loss: 862.19
train mean loss: 878.09
epoch train time: 0:00:07.974136
elapsed time: 0:02:46.955186
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 13:56:07.471157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.85
 ---- batch: 020 ----
mean loss: 894.12
 ---- batch: 030 ----
mean loss: 884.87
 ---- batch: 040 ----
mean loss: 842.13
 ---- batch: 050 ----
mean loss: 878.68
train mean loss: 874.87
epoch train time: 0:00:07.971360
elapsed time: 0:02:54.927845
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 13:56:15.443844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.77
 ---- batch: 020 ----
mean loss: 898.14
 ---- batch: 030 ----
mean loss: 850.00
 ---- batch: 040 ----
mean loss: 902.23
 ---- batch: 050 ----
mean loss: 852.53
train mean loss: 872.67
epoch train time: 0:00:07.956773
elapsed time: 0:03:02.885922
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 13:56:23.401889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.04
 ---- batch: 020 ----
mean loss: 866.88
 ---- batch: 030 ----
mean loss: 860.25
 ---- batch: 040 ----
mean loss: 864.20
 ---- batch: 050 ----
mean loss: 865.43
train mean loss: 867.10
epoch train time: 0:00:07.950517
elapsed time: 0:03:10.837756
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 13:56:31.353578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.13
 ---- batch: 020 ----
mean loss: 862.53
 ---- batch: 030 ----
mean loss: 858.83
 ---- batch: 040 ----
mean loss: 876.59
 ---- batch: 050 ----
mean loss: 862.08
train mean loss: 865.40
epoch train time: 0:00:07.928703
elapsed time: 0:03:18.767481
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 13:56:39.283442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.10
 ---- batch: 020 ----
mean loss: 866.42
 ---- batch: 030 ----
mean loss: 858.91
 ---- batch: 040 ----
mean loss: 839.28
 ---- batch: 050 ----
mean loss: 862.41
train mean loss: 858.09
epoch train time: 0:00:07.990419
elapsed time: 0:03:26.759137
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 13:56:47.275062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.31
 ---- batch: 020 ----
mean loss: 853.05
 ---- batch: 030 ----
mean loss: 847.38
 ---- batch: 040 ----
mean loss: 858.90
 ---- batch: 050 ----
mean loss: 859.96
train mean loss: 854.78
epoch train time: 0:00:07.977754
elapsed time: 0:03:34.738084
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 13:56:55.254000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.56
 ---- batch: 020 ----
mean loss: 838.95
 ---- batch: 030 ----
mean loss: 856.29
 ---- batch: 040 ----
mean loss: 856.97
 ---- batch: 050 ----
mean loss: 842.87
train mean loss: 847.16
epoch train time: 0:00:07.967019
elapsed time: 0:03:42.706269
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 13:57:03.222206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 827.56
 ---- batch: 020 ----
mean loss: 847.36
 ---- batch: 030 ----
mean loss: 832.71
 ---- batch: 040 ----
mean loss: 846.34
 ---- batch: 050 ----
mean loss: 855.58
train mean loss: 845.13
epoch train time: 0:00:07.997455
elapsed time: 0:03:50.704893
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 13:57:11.220822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.82
 ---- batch: 020 ----
mean loss: 835.27
 ---- batch: 030 ----
mean loss: 827.36
 ---- batch: 040 ----
mean loss: 820.61
 ---- batch: 050 ----
mean loss: 865.69
train mean loss: 838.77
epoch train time: 0:00:08.027797
elapsed time: 0:03:58.733875
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 13:57:19.249840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.25
 ---- batch: 020 ----
mean loss: 837.17
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 831.95
 ---- batch: 050 ----
mean loss: 821.93
train mean loss: 831.23
epoch train time: 0:00:07.979290
elapsed time: 0:04:06.714377
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 13:57:27.230372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.46
 ---- batch: 020 ----
mean loss: 838.54
 ---- batch: 030 ----
mean loss: 829.12
 ---- batch: 040 ----
mean loss: 806.29
 ---- batch: 050 ----
mean loss: 804.07
train mean loss: 822.99
epoch train time: 0:00:08.004285
elapsed time: 0:04:14.719931
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 13:57:35.235852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 804.84
 ---- batch: 020 ----
mean loss: 815.68
 ---- batch: 030 ----
mean loss: 811.63
 ---- batch: 040 ----
mean loss: 796.68
 ---- batch: 050 ----
mean loss: 817.07
train mean loss: 808.70
epoch train time: 0:00:07.996187
elapsed time: 0:04:22.717330
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 13:57:43.233352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 804.16
 ---- batch: 020 ----
mean loss: 800.90
 ---- batch: 030 ----
mean loss: 783.38
 ---- batch: 040 ----
mean loss: 783.22
 ---- batch: 050 ----
mean loss: 747.13
train mean loss: 783.43
epoch train time: 0:00:08.029342
elapsed time: 0:04:30.748011
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 13:57:51.263976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 757.92
 ---- batch: 020 ----
mean loss: 753.66
 ---- batch: 030 ----
mean loss: 752.79
 ---- batch: 040 ----
mean loss: 745.52
 ---- batch: 050 ----
mean loss: 748.49
train mean loss: 747.02
epoch train time: 0:00:08.000530
elapsed time: 0:04:38.749740
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 13:57:59.265673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 731.17
 ---- batch: 020 ----
mean loss: 750.54
 ---- batch: 030 ----
mean loss: 722.13
 ---- batch: 040 ----
mean loss: 706.34
 ---- batch: 050 ----
mean loss: 703.37
train mean loss: 720.00
epoch train time: 0:00:08.019457
elapsed time: 0:04:46.770395
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 13:58:07.286315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 716.37
 ---- batch: 020 ----
mean loss: 705.91
 ---- batch: 030 ----
mean loss: 704.14
 ---- batch: 040 ----
mean loss: 683.19
 ---- batch: 050 ----
mean loss: 657.93
train mean loss: 691.03
epoch train time: 0:00:08.015460
elapsed time: 0:04:54.787064
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 13:58:15.302992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 683.23
 ---- batch: 020 ----
mean loss: 666.67
 ---- batch: 030 ----
mean loss: 662.11
 ---- batch: 040 ----
mean loss: 668.31
 ---- batch: 050 ----
mean loss: 661.33
train mean loss: 667.75
epoch train time: 0:00:08.038038
elapsed time: 0:05:02.826368
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 13:58:23.342298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 661.06
 ---- batch: 020 ----
mean loss: 652.56
 ---- batch: 030 ----
mean loss: 650.77
 ---- batch: 040 ----
mean loss: 657.16
 ---- batch: 050 ----
mean loss: 646.50
train mean loss: 652.54
epoch train time: 0:00:07.997111
elapsed time: 0:05:10.824952
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 13:58:31.340929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 624.24
 ---- batch: 020 ----
mean loss: 641.79
 ---- batch: 030 ----
mean loss: 638.67
 ---- batch: 040 ----
mean loss: 614.83
 ---- batch: 050 ----
mean loss: 626.90
train mean loss: 629.56
epoch train time: 0:00:07.974820
elapsed time: 0:05:18.801010
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 13:58:39.316917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 638.30
 ---- batch: 020 ----
mean loss: 612.25
 ---- batch: 030 ----
mean loss: 605.83
 ---- batch: 040 ----
mean loss: 610.54
 ---- batch: 050 ----
mean loss: 608.23
train mean loss: 611.61
epoch train time: 0:00:07.984312
elapsed time: 0:05:26.786476
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 13:58:47.302421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 605.19
 ---- batch: 020 ----
mean loss: 592.43
 ---- batch: 030 ----
mean loss: 583.91
 ---- batch: 040 ----
mean loss: 604.92
 ---- batch: 050 ----
mean loss: 588.99
train mean loss: 594.17
epoch train time: 0:00:07.983904
elapsed time: 0:05:34.771613
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 13:58:55.287532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 613.44
 ---- batch: 020 ----
mean loss: 593.32
 ---- batch: 030 ----
mean loss: 581.79
 ---- batch: 040 ----
mean loss: 559.02
 ---- batch: 050 ----
mean loss: 552.22
train mean loss: 579.89
epoch train time: 0:00:07.988844
elapsed time: 0:05:42.761723
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 13:59:03.277722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.78
 ---- batch: 020 ----
mean loss: 582.93
 ---- batch: 030 ----
mean loss: 562.88
 ---- batch: 040 ----
mean loss: 559.87
 ---- batch: 050 ----
mean loss: 562.22
train mean loss: 565.21
epoch train time: 0:00:07.992836
elapsed time: 0:05:50.755759
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 13:59:11.271663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 560.41
 ---- batch: 020 ----
mean loss: 555.91
 ---- batch: 030 ----
mean loss: 539.88
 ---- batch: 040 ----
mean loss: 555.97
 ---- batch: 050 ----
mean loss: 548.94
train mean loss: 548.62
epoch train time: 0:00:07.985815
elapsed time: 0:05:58.742724
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 13:59:19.258657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 565.54
 ---- batch: 020 ----
mean loss: 534.91
 ---- batch: 030 ----
mean loss: 522.06
 ---- batch: 040 ----
mean loss: 536.03
 ---- batch: 050 ----
mean loss: 544.25
train mean loss: 538.50
epoch train time: 0:00:07.997360
elapsed time: 0:06:06.741253
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 13:59:27.257019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 531.91
 ---- batch: 020 ----
mean loss: 524.14
 ---- batch: 030 ----
mean loss: 535.31
 ---- batch: 040 ----
mean loss: 523.02
 ---- batch: 050 ----
mean loss: 523.62
train mean loss: 525.47
epoch train time: 0:00:07.999683
elapsed time: 0:06:14.741961
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 13:59:35.257896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 521.75
 ---- batch: 020 ----
mean loss: 514.58
 ---- batch: 030 ----
mean loss: 516.14
 ---- batch: 040 ----
mean loss: 511.73
 ---- batch: 050 ----
mean loss: 517.93
train mean loss: 516.66
epoch train time: 0:00:07.975355
elapsed time: 0:06:22.718530
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 13:59:43.234449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.47
 ---- batch: 020 ----
mean loss: 507.58
 ---- batch: 030 ----
mean loss: 499.42
 ---- batch: 040 ----
mean loss: 500.36
 ---- batch: 050 ----
mean loss: 487.30
train mean loss: 497.35
epoch train time: 0:00:07.964019
elapsed time: 0:06:30.683791
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 13:59:51.199685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.80
 ---- batch: 020 ----
mean loss: 481.21
 ---- batch: 030 ----
mean loss: 485.97
 ---- batch: 040 ----
mean loss: 484.99
 ---- batch: 050 ----
mean loss: 490.52
train mean loss: 486.41
epoch train time: 0:00:07.967431
elapsed time: 0:06:38.652312
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 13:59:59.168238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.69
 ---- batch: 020 ----
mean loss: 475.27
 ---- batch: 030 ----
mean loss: 482.97
 ---- batch: 040 ----
mean loss: 474.82
 ---- batch: 050 ----
mean loss: 470.22
train mean loss: 478.56
epoch train time: 0:00:07.973789
elapsed time: 0:06:46.627240
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 14:00:07.143225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.22
 ---- batch: 020 ----
mean loss: 473.46
 ---- batch: 030 ----
mean loss: 463.09
 ---- batch: 040 ----
mean loss: 463.64
 ---- batch: 050 ----
mean loss: 462.54
train mean loss: 463.70
epoch train time: 0:00:07.988080
elapsed time: 0:06:54.616517
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 14:00:15.132440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.28
 ---- batch: 020 ----
mean loss: 451.36
 ---- batch: 030 ----
mean loss: 457.14
 ---- batch: 040 ----
mean loss: 458.78
 ---- batch: 050 ----
mean loss: 445.07
train mean loss: 455.75
epoch train time: 0:00:07.991280
elapsed time: 0:07:02.609031
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 14:00:23.125033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.92
 ---- batch: 020 ----
mean loss: 441.10
 ---- batch: 030 ----
mean loss: 440.76
 ---- batch: 040 ----
mean loss: 434.46
 ---- batch: 050 ----
mean loss: 435.26
train mean loss: 439.22
epoch train time: 0:00:07.971216
elapsed time: 0:07:10.581453
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 14:00:31.097375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.16
 ---- batch: 020 ----
mean loss: 438.36
 ---- batch: 030 ----
mean loss: 428.77
 ---- batch: 040 ----
mean loss: 437.58
 ---- batch: 050 ----
mean loss: 446.47
train mean loss: 435.82
epoch train time: 0:00:07.983887
elapsed time: 0:07:18.566517
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 14:00:39.082510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.94
 ---- batch: 020 ----
mean loss: 421.62
 ---- batch: 030 ----
mean loss: 433.94
 ---- batch: 040 ----
mean loss: 422.81
 ---- batch: 050 ----
mean loss: 421.64
train mean loss: 425.96
epoch train time: 0:00:07.976730
elapsed time: 0:07:26.544455
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 14:00:47.060366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.94
 ---- batch: 020 ----
mean loss: 440.42
 ---- batch: 030 ----
mean loss: 435.03
 ---- batch: 040 ----
mean loss: 426.41
 ---- batch: 050 ----
mean loss: 417.30
train mean loss: 426.24
epoch train time: 0:00:07.995021
elapsed time: 0:07:34.540629
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 14:00:55.056532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.12
 ---- batch: 020 ----
mean loss: 419.77
 ---- batch: 030 ----
mean loss: 427.69
 ---- batch: 040 ----
mean loss: 400.55
 ---- batch: 050 ----
mean loss: 415.09
train mean loss: 413.52
epoch train time: 0:00:07.986358
elapsed time: 0:07:42.528125
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 14:01:03.044027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.25
 ---- batch: 020 ----
mean loss: 402.24
 ---- batch: 030 ----
mean loss: 400.75
 ---- batch: 040 ----
mean loss: 408.19
 ---- batch: 050 ----
mean loss: 409.43
train mean loss: 406.08
epoch train time: 0:00:07.991783
elapsed time: 0:07:50.521087
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 14:01:11.036953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.92
 ---- batch: 020 ----
mean loss: 400.04
 ---- batch: 030 ----
mean loss: 398.45
 ---- batch: 040 ----
mean loss: 397.60
 ---- batch: 050 ----
mean loss: 398.87
train mean loss: 399.73
epoch train time: 0:00:07.957457
elapsed time: 0:07:58.479677
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 14:01:18.995602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.10
 ---- batch: 020 ----
mean loss: 397.60
 ---- batch: 030 ----
mean loss: 392.49
 ---- batch: 040 ----
mean loss: 389.54
 ---- batch: 050 ----
mean loss: 401.08
train mean loss: 393.52
epoch train time: 0:00:07.964040
elapsed time: 0:08:06.444941
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 14:01:26.960917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.86
 ---- batch: 020 ----
mean loss: 382.21
 ---- batch: 030 ----
mean loss: 389.33
 ---- batch: 040 ----
mean loss: 390.44
 ---- batch: 050 ----
mean loss: 378.59
train mean loss: 386.35
epoch train time: 0:00:07.968028
elapsed time: 0:08:14.414202
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 14:01:34.930105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.90
 ---- batch: 020 ----
mean loss: 380.50
 ---- batch: 030 ----
mean loss: 375.74
 ---- batch: 040 ----
mean loss: 382.01
 ---- batch: 050 ----
mean loss: 375.96
train mean loss: 379.38
epoch train time: 0:00:07.973654
elapsed time: 0:08:22.389047
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 14:01:42.904942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.67
 ---- batch: 020 ----
mean loss: 376.66
 ---- batch: 030 ----
mean loss: 379.79
 ---- batch: 040 ----
mean loss: 372.85
 ---- batch: 050 ----
mean loss: 389.57
train mean loss: 378.43
epoch train time: 0:00:07.981640
elapsed time: 0:08:30.371809
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 14:01:50.887745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.80
 ---- batch: 020 ----
mean loss: 357.67
 ---- batch: 030 ----
mean loss: 359.27
 ---- batch: 040 ----
mean loss: 372.06
 ---- batch: 050 ----
mean loss: 391.84
train mean loss: 370.33
epoch train time: 0:00:08.011846
elapsed time: 0:08:38.384942
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 14:01:58.900849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.74
 ---- batch: 020 ----
mean loss: 364.96
 ---- batch: 030 ----
mean loss: 367.90
 ---- batch: 040 ----
mean loss: 363.74
 ---- batch: 050 ----
mean loss: 360.77
train mean loss: 365.62
epoch train time: 0:00:07.980236
elapsed time: 0:08:46.366385
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 14:02:06.882334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.53
 ---- batch: 020 ----
mean loss: 357.41
 ---- batch: 030 ----
mean loss: 371.44
 ---- batch: 040 ----
mean loss: 357.46
 ---- batch: 050 ----
mean loss: 366.88
train mean loss: 361.89
epoch train time: 0:00:07.966915
elapsed time: 0:08:54.334584
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 14:02:14.850570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.89
 ---- batch: 020 ----
mean loss: 363.01
 ---- batch: 030 ----
mean loss: 348.81
 ---- batch: 040 ----
mean loss: 348.88
 ---- batch: 050 ----
mean loss: 359.22
train mean loss: 355.47
epoch train time: 0:00:07.946551
elapsed time: 0:09:02.282356
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 14:02:22.798291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.62
 ---- batch: 020 ----
mean loss: 355.10
 ---- batch: 030 ----
mean loss: 359.92
 ---- batch: 040 ----
mean loss: 356.60
 ---- batch: 050 ----
mean loss: 366.78
train mean loss: 355.53
epoch train time: 0:00:07.968832
elapsed time: 0:09:10.252322
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 14:02:30.768235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.56
 ---- batch: 020 ----
mean loss: 348.34
 ---- batch: 030 ----
mean loss: 343.83
 ---- batch: 040 ----
mean loss: 349.97
 ---- batch: 050 ----
mean loss: 344.95
train mean loss: 347.56
epoch train time: 0:00:07.951415
elapsed time: 0:09:18.204872
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 14:02:38.720814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.62
 ---- batch: 020 ----
mean loss: 346.62
 ---- batch: 030 ----
mean loss: 354.30
 ---- batch: 040 ----
mean loss: 342.99
 ---- batch: 050 ----
mean loss: 347.26
train mean loss: 345.79
epoch train time: 0:00:07.970447
elapsed time: 0:09:26.176579
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 14:02:46.692566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.74
 ---- batch: 020 ----
mean loss: 354.74
 ---- batch: 030 ----
mean loss: 342.15
 ---- batch: 040 ----
mean loss: 342.35
 ---- batch: 050 ----
mean loss: 345.26
train mean loss: 344.31
epoch train time: 0:00:07.956619
elapsed time: 0:09:34.134467
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 14:02:54.650494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.20
 ---- batch: 020 ----
mean loss: 331.67
 ---- batch: 030 ----
mean loss: 331.48
 ---- batch: 040 ----
mean loss: 339.96
 ---- batch: 050 ----
mean loss: 333.26
train mean loss: 336.64
epoch train time: 0:00:07.999289
elapsed time: 0:09:42.135145
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 14:03:02.651119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.73
 ---- batch: 020 ----
mean loss: 337.52
 ---- batch: 030 ----
mean loss: 340.67
 ---- batch: 040 ----
mean loss: 335.85
 ---- batch: 050 ----
mean loss: 329.95
train mean loss: 334.28
epoch train time: 0:00:08.002555
elapsed time: 0:09:50.138964
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 14:03:10.654964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.16
 ---- batch: 020 ----
mean loss: 340.38
 ---- batch: 030 ----
mean loss: 323.06
 ---- batch: 040 ----
mean loss: 340.34
 ---- batch: 050 ----
mean loss: 338.18
train mean loss: 335.86
epoch train time: 0:00:08.003185
elapsed time: 0:09:58.143424
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 14:03:18.659386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.03
 ---- batch: 020 ----
mean loss: 329.02
 ---- batch: 030 ----
mean loss: 327.40
 ---- batch: 040 ----
mean loss: 327.33
 ---- batch: 050 ----
mean loss: 325.29
train mean loss: 327.82
epoch train time: 0:00:08.018078
elapsed time: 0:10:06.162765
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 14:03:26.678716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.63
 ---- batch: 020 ----
mean loss: 323.36
 ---- batch: 030 ----
mean loss: 327.06
 ---- batch: 040 ----
mean loss: 329.67
 ---- batch: 050 ----
mean loss: 319.49
train mean loss: 324.43
epoch train time: 0:00:07.992492
elapsed time: 0:10:14.156821
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 14:03:34.672787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.91
 ---- batch: 020 ----
mean loss: 329.67
 ---- batch: 030 ----
mean loss: 318.49
 ---- batch: 040 ----
mean loss: 324.23
 ---- batch: 050 ----
mean loss: 321.77
train mean loss: 322.93
epoch train time: 0:00:07.980537
elapsed time: 0:10:22.138599
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 14:03:42.654568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.04
 ---- batch: 020 ----
mean loss: 327.67
 ---- batch: 030 ----
mean loss: 328.36
 ---- batch: 040 ----
mean loss: 319.72
 ---- batch: 050 ----
mean loss: 324.15
train mean loss: 319.37
epoch train time: 0:00:08.002151
elapsed time: 0:10:30.141947
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 14:03:50.657881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.75
 ---- batch: 020 ----
mean loss: 320.09
 ---- batch: 030 ----
mean loss: 327.12
 ---- batch: 040 ----
mean loss: 317.61
 ---- batch: 050 ----
mean loss: 318.66
train mean loss: 317.05
epoch train time: 0:00:07.981659
elapsed time: 0:10:38.124813
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 14:03:58.640749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.90
 ---- batch: 020 ----
mean loss: 312.45
 ---- batch: 030 ----
mean loss: 317.59
 ---- batch: 040 ----
mean loss: 316.04
 ---- batch: 050 ----
mean loss: 319.67
train mean loss: 315.91
epoch train time: 0:00:07.988767
elapsed time: 0:10:46.114731
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 14:04:06.630744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.20
 ---- batch: 020 ----
mean loss: 309.29
 ---- batch: 030 ----
mean loss: 319.21
 ---- batch: 040 ----
mean loss: 314.62
 ---- batch: 050 ----
mean loss: 304.66
train mean loss: 312.77
epoch train time: 0:00:07.955417
elapsed time: 0:10:54.071454
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 14:04:14.587394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.80
 ---- batch: 020 ----
mean loss: 308.10
 ---- batch: 030 ----
mean loss: 311.70
 ---- batch: 040 ----
mean loss: 316.85
 ---- batch: 050 ----
mean loss: 320.42
train mean loss: 313.22
epoch train time: 0:00:07.988081
elapsed time: 0:11:02.060776
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 14:04:22.576777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.84
 ---- batch: 020 ----
mean loss: 293.58
 ---- batch: 030 ----
mean loss: 301.90
 ---- batch: 040 ----
mean loss: 312.63
 ---- batch: 050 ----
mean loss: 316.66
train mean loss: 305.32
epoch train time: 0:00:07.973391
elapsed time: 0:11:10.035461
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 14:04:30.551389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.56
 ---- batch: 020 ----
mean loss: 304.35
 ---- batch: 030 ----
mean loss: 299.04
 ---- batch: 040 ----
mean loss: 300.37
 ---- batch: 050 ----
mean loss: 302.64
train mean loss: 303.52
epoch train time: 0:00:07.952152
elapsed time: 0:11:17.988852
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 14:04:38.504761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.91
 ---- batch: 020 ----
mean loss: 295.12
 ---- batch: 030 ----
mean loss: 297.48
 ---- batch: 040 ----
mean loss: 307.61
 ---- batch: 050 ----
mean loss: 303.55
train mean loss: 304.22
epoch train time: 0:00:07.964393
elapsed time: 0:11:25.954401
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 14:04:46.470316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.18
 ---- batch: 020 ----
mean loss: 302.14
 ---- batch: 030 ----
mean loss: 301.12
 ---- batch: 040 ----
mean loss: 300.16
 ---- batch: 050 ----
mean loss: 305.45
train mean loss: 302.58
epoch train time: 0:00:07.976430
elapsed time: 0:11:33.932015
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 14:04:54.447944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.79
 ---- batch: 020 ----
mean loss: 319.07
 ---- batch: 030 ----
mean loss: 285.94
 ---- batch: 040 ----
mean loss: 307.63
 ---- batch: 050 ----
mean loss: 300.87
train mean loss: 300.07
epoch train time: 0:00:07.970415
elapsed time: 0:11:41.903683
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 14:05:02.419435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.31
 ---- batch: 020 ----
mean loss: 294.40
 ---- batch: 030 ----
mean loss: 297.21
 ---- batch: 040 ----
mean loss: 293.99
 ---- batch: 050 ----
mean loss: 298.02
train mean loss: 297.34
epoch train time: 0:00:07.962570
elapsed time: 0:11:49.867220
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 14:05:10.383155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.02
 ---- batch: 020 ----
mean loss: 297.94
 ---- batch: 030 ----
mean loss: 299.16
 ---- batch: 040 ----
mean loss: 285.48
 ---- batch: 050 ----
mean loss: 302.93
train mean loss: 296.22
epoch train time: 0:00:07.982490
elapsed time: 0:11:57.850891
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 14:05:18.366823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.61
 ---- batch: 020 ----
mean loss: 284.78
 ---- batch: 030 ----
mean loss: 302.01
 ---- batch: 040 ----
mean loss: 295.61
 ---- batch: 050 ----
mean loss: 295.07
train mean loss: 292.65
epoch train time: 0:00:07.974218
elapsed time: 0:12:05.826376
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 14:05:26.342297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.39
 ---- batch: 020 ----
mean loss: 301.69
 ---- batch: 030 ----
mean loss: 298.71
 ---- batch: 040 ----
mean loss: 285.09
 ---- batch: 050 ----
mean loss: 295.23
train mean loss: 292.50
epoch train time: 0:00:07.939704
elapsed time: 0:12:13.767376
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 14:05:34.283294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.59
 ---- batch: 020 ----
mean loss: 293.80
 ---- batch: 030 ----
mean loss: 289.34
 ---- batch: 040 ----
mean loss: 277.70
 ---- batch: 050 ----
mean loss: 297.89
train mean loss: 289.66
epoch train time: 0:00:07.860884
elapsed time: 0:12:21.629376
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 14:05:42.145367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.08
 ---- batch: 020 ----
mean loss: 283.72
 ---- batch: 030 ----
mean loss: 290.47
 ---- batch: 040 ----
mean loss: 292.99
 ---- batch: 050 ----
mean loss: 293.02
train mean loss: 288.31
epoch train time: 0:00:07.833383
elapsed time: 0:12:29.463963
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 14:05:49.979882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.64
 ---- batch: 020 ----
mean loss: 286.63
 ---- batch: 030 ----
mean loss: 285.81
 ---- batch: 040 ----
mean loss: 282.35
 ---- batch: 050 ----
mean loss: 285.07
train mean loss: 285.16
epoch train time: 0:00:07.853260
elapsed time: 0:12:37.318367
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 14:05:57.834214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.68
 ---- batch: 020 ----
mean loss: 285.42
 ---- batch: 030 ----
mean loss: 286.89
 ---- batch: 040 ----
mean loss: 281.93
 ---- batch: 050 ----
mean loss: 285.46
train mean loss: 284.80
epoch train time: 0:00:07.836581
elapsed time: 0:12:45.156071
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 14:06:05.672000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.72
 ---- batch: 020 ----
mean loss: 286.57
 ---- batch: 030 ----
mean loss: 284.13
 ---- batch: 040 ----
mean loss: 279.73
 ---- batch: 050 ----
mean loss: 277.47
train mean loss: 281.26
epoch train time: 0:00:07.883472
elapsed time: 0:12:53.040700
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 14:06:13.556666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.49
 ---- batch: 020 ----
mean loss: 281.77
 ---- batch: 030 ----
mean loss: 282.63
 ---- batch: 040 ----
mean loss: 286.44
 ---- batch: 050 ----
mean loss: 288.40
train mean loss: 282.16
epoch train time: 0:00:07.829363
elapsed time: 0:13:00.871321
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 14:06:21.387267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.62
 ---- batch: 020 ----
mean loss: 270.13
 ---- batch: 030 ----
mean loss: 286.18
 ---- batch: 040 ----
mean loss: 280.62
 ---- batch: 050 ----
mean loss: 280.92
train mean loss: 276.55
epoch train time: 0:00:07.852356
elapsed time: 0:13:08.724922
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 14:06:29.240865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.12
 ---- batch: 020 ----
mean loss: 285.87
 ---- batch: 030 ----
mean loss: 275.58
 ---- batch: 040 ----
mean loss: 271.66
 ---- batch: 050 ----
mean loss: 283.60
train mean loss: 278.73
epoch train time: 0:00:07.855752
elapsed time: 0:13:16.581841
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 14:06:37.097813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.38
 ---- batch: 020 ----
mean loss: 283.05
 ---- batch: 030 ----
mean loss: 272.25
 ---- batch: 040 ----
mean loss: 277.83
 ---- batch: 050 ----
mean loss: 272.79
train mean loss: 275.57
epoch train time: 0:00:07.829155
elapsed time: 0:13:24.412287
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 14:06:44.928205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.02
 ---- batch: 020 ----
mean loss: 275.00
 ---- batch: 030 ----
mean loss: 278.13
 ---- batch: 040 ----
mean loss: 282.53
 ---- batch: 050 ----
mean loss: 266.67
train mean loss: 276.61
epoch train time: 0:00:07.822452
elapsed time: 0:13:32.235942
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 14:06:52.752143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.21
 ---- batch: 020 ----
mean loss: 286.29
 ---- batch: 030 ----
mean loss: 278.41
 ---- batch: 040 ----
mean loss: 265.64
 ---- batch: 050 ----
mean loss: 271.38
train mean loss: 273.83
epoch train time: 0:00:07.832935
elapsed time: 0:13:40.070405
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 14:07:00.586350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.61
 ---- batch: 020 ----
mean loss: 278.41
 ---- batch: 030 ----
mean loss: 274.00
 ---- batch: 040 ----
mean loss: 269.75
 ---- batch: 050 ----
mean loss: 265.27
train mean loss: 272.70
epoch train time: 0:00:07.875953
elapsed time: 0:13:47.947508
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 14:07:08.463408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.24
 ---- batch: 020 ----
mean loss: 268.09
 ---- batch: 030 ----
mean loss: 272.61
 ---- batch: 040 ----
mean loss: 260.37
 ---- batch: 050 ----
mean loss: 269.72
train mean loss: 272.18
epoch train time: 0:00:07.849785
elapsed time: 0:13:55.798401
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 14:07:16.314385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.43
 ---- batch: 020 ----
mean loss: 257.00
 ---- batch: 030 ----
mean loss: 272.77
 ---- batch: 040 ----
mean loss: 268.08
 ---- batch: 050 ----
mean loss: 268.39
train mean loss: 268.22
epoch train time: 0:00:07.819065
elapsed time: 0:14:03.618700
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 14:07:24.134624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.59
 ---- batch: 020 ----
mean loss: 267.34
 ---- batch: 030 ----
mean loss: 275.20
 ---- batch: 040 ----
mean loss: 264.14
 ---- batch: 050 ----
mean loss: 270.62
train mean loss: 268.37
epoch train time: 0:00:07.835803
elapsed time: 0:14:11.455648
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 14:07:31.971566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.38
 ---- batch: 020 ----
mean loss: 265.91
 ---- batch: 030 ----
mean loss: 258.72
 ---- batch: 040 ----
mean loss: 264.09
 ---- batch: 050 ----
mean loss: 283.92
train mean loss: 264.80
epoch train time: 0:00:07.852364
elapsed time: 0:14:19.309154
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 14:07:39.825079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.26
 ---- batch: 020 ----
mean loss: 264.01
 ---- batch: 030 ----
mean loss: 262.93
 ---- batch: 040 ----
mean loss: 265.79
 ---- batch: 050 ----
mean loss: 263.80
train mean loss: 265.78
epoch train time: 0:00:07.825413
elapsed time: 0:14:27.135900
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 14:07:47.651578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.85
 ---- batch: 020 ----
mean loss: 259.49
 ---- batch: 030 ----
mean loss: 268.18
 ---- batch: 040 ----
mean loss: 260.50
 ---- batch: 050 ----
mean loss: 266.04
train mean loss: 262.96
epoch train time: 0:00:07.935896
elapsed time: 0:14:35.072758
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 14:07:55.588611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.27
 ---- batch: 020 ----
mean loss: 262.64
 ---- batch: 030 ----
mean loss: 251.71
 ---- batch: 040 ----
mean loss: 268.22
 ---- batch: 050 ----
mean loss: 260.35
train mean loss: 261.69
epoch train time: 0:00:07.840568
elapsed time: 0:14:42.914408
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 14:08:03.430347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.52
 ---- batch: 020 ----
mean loss: 257.61
 ---- batch: 030 ----
mean loss: 245.47
 ---- batch: 040 ----
mean loss: 269.63
 ---- batch: 050 ----
mean loss: 271.35
train mean loss: 262.20
epoch train time: 0:00:07.919693
elapsed time: 0:14:50.835414
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 14:08:11.351308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.23
 ---- batch: 020 ----
mean loss: 262.83
 ---- batch: 030 ----
mean loss: 263.27
 ---- batch: 040 ----
mean loss: 269.32
 ---- batch: 050 ----
mean loss: 266.17
train mean loss: 262.84
epoch train time: 0:00:07.875252
elapsed time: 0:14:58.711820
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 14:08:19.227748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.21
 ---- batch: 020 ----
mean loss: 256.34
 ---- batch: 030 ----
mean loss: 263.89
 ---- batch: 040 ----
mean loss: 260.87
 ---- batch: 050 ----
mean loss: 259.00
train mean loss: 259.95
epoch train time: 0:00:07.865787
elapsed time: 0:15:06.578964
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 14:08:27.094889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.30
 ---- batch: 020 ----
mean loss: 251.48
 ---- batch: 030 ----
mean loss: 259.12
 ---- batch: 040 ----
mean loss: 252.26
 ---- batch: 050 ----
mean loss: 258.42
train mean loss: 257.30
epoch train time: 0:00:07.830042
elapsed time: 0:15:14.410158
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 14:08:34.926083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.44
 ---- batch: 020 ----
mean loss: 261.85
 ---- batch: 030 ----
mean loss: 262.07
 ---- batch: 040 ----
mean loss: 257.68
 ---- batch: 050 ----
mean loss: 248.06
train mean loss: 257.00
epoch train time: 0:00:07.857300
elapsed time: 0:15:22.268598
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 14:08:42.784541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.76
 ---- batch: 020 ----
mean loss: 267.55
 ---- batch: 030 ----
mean loss: 263.92
 ---- batch: 040 ----
mean loss: 258.27
 ---- batch: 050 ----
mean loss: 253.16
train mean loss: 257.59
epoch train time: 0:00:07.850954
elapsed time: 0:15:30.120705
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 14:08:50.636629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.85
 ---- batch: 020 ----
mean loss: 253.79
 ---- batch: 030 ----
mean loss: 252.57
 ---- batch: 040 ----
mean loss: 264.48
 ---- batch: 050 ----
mean loss: 246.70
train mean loss: 252.25
epoch train time: 0:00:07.862874
elapsed time: 0:15:37.984767
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 14:08:58.500737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.78
 ---- batch: 020 ----
mean loss: 253.37
 ---- batch: 030 ----
mean loss: 250.87
 ---- batch: 040 ----
mean loss: 255.07
 ---- batch: 050 ----
mean loss: 252.69
train mean loss: 252.05
epoch train time: 0:00:07.843247
elapsed time: 0:15:45.829250
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 14:09:06.345221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.43
 ---- batch: 020 ----
mean loss: 252.70
 ---- batch: 030 ----
mean loss: 249.30
 ---- batch: 040 ----
mean loss: 241.13
 ---- batch: 050 ----
mean loss: 252.68
train mean loss: 251.87
epoch train time: 0:00:07.839542
elapsed time: 0:15:53.669964
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 14:09:14.185856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.35
 ---- batch: 020 ----
mean loss: 242.86
 ---- batch: 030 ----
mean loss: 249.47
 ---- batch: 040 ----
mean loss: 248.83
 ---- batch: 050 ----
mean loss: 257.48
train mean loss: 251.26
epoch train time: 0:00:07.869179
elapsed time: 0:16:01.540231
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 14:09:22.056135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.27
 ---- batch: 020 ----
mean loss: 251.58
 ---- batch: 030 ----
mean loss: 243.47
 ---- batch: 040 ----
mean loss: 251.32
 ---- batch: 050 ----
mean loss: 250.00
train mean loss: 250.76
epoch train time: 0:00:07.852595
elapsed time: 0:16:09.393976
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 14:09:29.909917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.18
 ---- batch: 020 ----
mean loss: 245.67
 ---- batch: 030 ----
mean loss: 255.78
 ---- batch: 040 ----
mean loss: 247.01
 ---- batch: 050 ----
mean loss: 260.67
train mean loss: 251.72
epoch train time: 0:00:07.850742
elapsed time: 0:16:17.245902
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 14:09:37.761857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.51
 ---- batch: 020 ----
mean loss: 251.99
 ---- batch: 030 ----
mean loss: 249.46
 ---- batch: 040 ----
mean loss: 242.85
 ---- batch: 050 ----
mean loss: 238.71
train mean loss: 247.66
epoch train time: 0:00:07.832146
elapsed time: 0:16:25.079180
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 14:09:45.595101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.63
 ---- batch: 020 ----
mean loss: 246.63
 ---- batch: 030 ----
mean loss: 248.34
 ---- batch: 040 ----
mean loss: 253.04
 ---- batch: 050 ----
mean loss: 239.31
train mean loss: 246.63
epoch train time: 0:00:07.848411
elapsed time: 0:16:32.928706
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 14:09:53.444582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.39
 ---- batch: 020 ----
mean loss: 248.96
 ---- batch: 030 ----
mean loss: 244.95
 ---- batch: 040 ----
mean loss: 248.14
 ---- batch: 050 ----
mean loss: 243.79
train mean loss: 245.25
epoch train time: 0:00:07.844875
elapsed time: 0:16:40.774775
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 14:10:01.290755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.35
 ---- batch: 020 ----
mean loss: 249.24
 ---- batch: 030 ----
mean loss: 244.36
 ---- batch: 040 ----
mean loss: 246.12
 ---- batch: 050 ----
mean loss: 252.44
train mean loss: 246.12
epoch train time: 0:00:07.814887
elapsed time: 0:16:48.590820
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 14:10:09.106731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.78
 ---- batch: 020 ----
mean loss: 242.97
 ---- batch: 030 ----
mean loss: 236.55
 ---- batch: 040 ----
mean loss: 246.31
 ---- batch: 050 ----
mean loss: 239.07
train mean loss: 242.67
epoch train time: 0:00:07.837153
elapsed time: 0:16:56.429184
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 14:10:16.945109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.62
 ---- batch: 020 ----
mean loss: 237.98
 ---- batch: 030 ----
mean loss: 246.91
 ---- batch: 040 ----
mean loss: 244.64
 ---- batch: 050 ----
mean loss: 249.49
train mean loss: 243.38
epoch train time: 0:00:07.834782
elapsed time: 0:17:04.265475
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 14:10:24.781213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.52
 ---- batch: 020 ----
mean loss: 237.68
 ---- batch: 030 ----
mean loss: 245.54
 ---- batch: 040 ----
mean loss: 235.14
 ---- batch: 050 ----
mean loss: 245.98
train mean loss: 241.87
epoch train time: 0:00:07.838854
elapsed time: 0:17:12.105250
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 14:10:32.621180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.19
 ---- batch: 020 ----
mean loss: 233.59
 ---- batch: 030 ----
mean loss: 237.39
 ---- batch: 040 ----
mean loss: 248.00
 ---- batch: 050 ----
mean loss: 237.78
train mean loss: 240.63
epoch train time: 0:00:07.801001
elapsed time: 0:17:19.907421
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 14:10:40.423307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.82
 ---- batch: 020 ----
mean loss: 241.29
 ---- batch: 030 ----
mean loss: 236.70
 ---- batch: 040 ----
mean loss: 237.11
 ---- batch: 050 ----
mean loss: 235.62
train mean loss: 239.56
epoch train time: 0:00:07.816287
elapsed time: 0:17:27.724849
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 14:10:48.240817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.66
 ---- batch: 020 ----
mean loss: 240.91
 ---- batch: 030 ----
mean loss: 242.03
 ---- batch: 040 ----
mean loss: 236.54
 ---- batch: 050 ----
mean loss: 240.98
train mean loss: 237.41
epoch train time: 0:00:07.833499
elapsed time: 0:17:35.559530
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 14:10:56.075480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.74
 ---- batch: 020 ----
mean loss: 236.11
 ---- batch: 030 ----
mean loss: 241.28
 ---- batch: 040 ----
mean loss: 238.80
 ---- batch: 050 ----
mean loss: 233.43
train mean loss: 237.92
epoch train time: 0:00:07.824083
elapsed time: 0:17:43.384840
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 14:11:03.900748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.25
 ---- batch: 020 ----
mean loss: 230.43
 ---- batch: 030 ----
mean loss: 236.79
 ---- batch: 040 ----
mean loss: 243.82
 ---- batch: 050 ----
mean loss: 234.66
train mean loss: 236.40
epoch train time: 0:00:07.806841
elapsed time: 0:17:51.192800
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 14:11:11.708708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.03
 ---- batch: 020 ----
mean loss: 237.37
 ---- batch: 030 ----
mean loss: 242.76
 ---- batch: 040 ----
mean loss: 242.81
 ---- batch: 050 ----
mean loss: 231.19
train mean loss: 236.21
epoch train time: 0:00:07.863238
elapsed time: 0:17:59.057102
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 14:11:19.573081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.18
 ---- batch: 020 ----
mean loss: 241.87
 ---- batch: 030 ----
mean loss: 234.25
 ---- batch: 040 ----
mean loss: 227.19
 ---- batch: 050 ----
mean loss: 230.91
train mean loss: 234.80
epoch train time: 0:00:07.812913
elapsed time: 0:18:06.871204
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 14:11:27.387137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.96
 ---- batch: 020 ----
mean loss: 232.87
 ---- batch: 030 ----
mean loss: 230.39
 ---- batch: 040 ----
mean loss: 234.91
 ---- batch: 050 ----
mean loss: 233.18
train mean loss: 233.26
epoch train time: 0:00:07.815562
elapsed time: 0:18:14.687901
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 14:11:35.203837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.88
 ---- batch: 020 ----
mean loss: 234.13
 ---- batch: 030 ----
mean loss: 237.34
 ---- batch: 040 ----
mean loss: 238.72
 ---- batch: 050 ----
mean loss: 235.63
train mean loss: 233.91
epoch train time: 0:00:07.803467
elapsed time: 0:18:22.492486
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 14:11:43.008503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.18
 ---- batch: 020 ----
mean loss: 243.11
 ---- batch: 030 ----
mean loss: 226.70
 ---- batch: 040 ----
mean loss: 241.39
 ---- batch: 050 ----
mean loss: 234.53
train mean loss: 233.03
epoch train time: 0:00:07.810046
elapsed time: 0:18:30.303777
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 14:11:50.819769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.58
 ---- batch: 020 ----
mean loss: 241.21
 ---- batch: 030 ----
mean loss: 222.23
 ---- batch: 040 ----
mean loss: 231.30
 ---- batch: 050 ----
mean loss: 233.05
train mean loss: 232.17
epoch train time: 0:00:07.798411
elapsed time: 0:18:38.103386
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 14:11:58.619340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.35
 ---- batch: 020 ----
mean loss: 229.76
 ---- batch: 030 ----
mean loss: 230.11
 ---- batch: 040 ----
mean loss: 235.12
 ---- batch: 050 ----
mean loss: 226.51
train mean loss: 230.88
epoch train time: 0:00:07.804317
elapsed time: 0:18:45.908864
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 14:12:06.424799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.73
 ---- batch: 020 ----
mean loss: 233.70
 ---- batch: 030 ----
mean loss: 237.35
 ---- batch: 040 ----
mean loss: 224.82
 ---- batch: 050 ----
mean loss: 236.10
train mean loss: 231.99
epoch train time: 0:00:07.795786
elapsed time: 0:18:53.705732
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 14:12:14.221650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.60
 ---- batch: 020 ----
mean loss: 241.81
 ---- batch: 030 ----
mean loss: 228.51
 ---- batch: 040 ----
mean loss: 229.81
 ---- batch: 050 ----
mean loss: 223.97
train mean loss: 229.81
epoch train time: 0:00:07.849481
elapsed time: 0:19:01.556419
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 14:12:22.072319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.46
 ---- batch: 020 ----
mean loss: 224.47
 ---- batch: 030 ----
mean loss: 230.90
 ---- batch: 040 ----
mean loss: 226.42
 ---- batch: 050 ----
mean loss: 230.16
train mean loss: 228.82
epoch train time: 0:00:07.801910
elapsed time: 0:19:09.359487
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 14:12:29.875449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.75
 ---- batch: 020 ----
mean loss: 229.93
 ---- batch: 030 ----
mean loss: 224.38
 ---- batch: 040 ----
mean loss: 223.88
 ---- batch: 050 ----
mean loss: 227.60
train mean loss: 227.34
epoch train time: 0:00:07.800332
elapsed time: 0:19:17.161011
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 14:12:37.676944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.25
 ---- batch: 020 ----
mean loss: 224.57
 ---- batch: 030 ----
mean loss: 224.16
 ---- batch: 040 ----
mean loss: 221.17
 ---- batch: 050 ----
mean loss: 235.21
train mean loss: 226.12
epoch train time: 0:00:07.794219
elapsed time: 0:19:24.956419
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 14:12:45.472364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.45
 ---- batch: 020 ----
mean loss: 229.65
 ---- batch: 030 ----
mean loss: 226.03
 ---- batch: 040 ----
mean loss: 235.74
 ---- batch: 050 ----
mean loss: 217.40
train mean loss: 227.71
epoch train time: 0:00:07.805183
elapsed time: 0:19:32.762765
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 14:12:53.278657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.35
 ---- batch: 020 ----
mean loss: 222.57
 ---- batch: 030 ----
mean loss: 216.39
 ---- batch: 040 ----
mean loss: 230.98
 ---- batch: 050 ----
mean loss: 227.84
train mean loss: 224.77
epoch train time: 0:00:07.802117
elapsed time: 0:19:40.565988
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 14:13:01.081966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.24
 ---- batch: 020 ----
mean loss: 215.19
 ---- batch: 030 ----
mean loss: 227.45
 ---- batch: 040 ----
mean loss: 231.20
 ---- batch: 050 ----
mean loss: 222.20
train mean loss: 223.38
epoch train time: 0:00:07.823855
elapsed time: 0:19:48.391197
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 14:13:08.907148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.88
 ---- batch: 020 ----
mean loss: 215.92
 ---- batch: 030 ----
mean loss: 227.53
 ---- batch: 040 ----
mean loss: 227.26
 ---- batch: 050 ----
mean loss: 222.10
train mean loss: 224.49
epoch train time: 0:00:07.821617
elapsed time: 0:19:56.214329
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 14:13:16.730029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.68
 ---- batch: 020 ----
mean loss: 222.59
 ---- batch: 030 ----
mean loss: 223.78
 ---- batch: 040 ----
mean loss: 215.83
 ---- batch: 050 ----
mean loss: 233.26
train mean loss: 222.82
epoch train time: 0:00:07.830893
elapsed time: 0:20:04.046187
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 14:13:24.562118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.56
 ---- batch: 020 ----
mean loss: 224.75
 ---- batch: 030 ----
mean loss: 228.84
 ---- batch: 040 ----
mean loss: 219.49
 ---- batch: 050 ----
mean loss: 222.74
train mean loss: 223.44
epoch train time: 0:00:07.809630
elapsed time: 0:20:11.856999
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 14:13:32.372909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.83
 ---- batch: 020 ----
mean loss: 221.89
 ---- batch: 030 ----
mean loss: 219.41
 ---- batch: 040 ----
mean loss: 214.69
 ---- batch: 050 ----
mean loss: 218.57
train mean loss: 221.06
epoch train time: 0:00:07.795290
elapsed time: 0:20:19.653360
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 14:13:40.169299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.99
 ---- batch: 020 ----
mean loss: 227.88
 ---- batch: 030 ----
mean loss: 216.18
 ---- batch: 040 ----
mean loss: 223.40
 ---- batch: 050 ----
mean loss: 222.40
train mean loss: 221.05
epoch train time: 0:00:07.811505
elapsed time: 0:20:27.466169
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 14:13:47.982018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.38
 ---- batch: 020 ----
mean loss: 225.23
 ---- batch: 030 ----
mean loss: 227.89
 ---- batch: 040 ----
mean loss: 211.68
 ---- batch: 050 ----
mean loss: 214.47
train mean loss: 220.45
epoch train time: 0:00:07.827106
elapsed time: 0:20:35.294496
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 14:13:55.810424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.97
 ---- batch: 020 ----
mean loss: 228.37
 ---- batch: 030 ----
mean loss: 212.81
 ---- batch: 040 ----
mean loss: 225.10
 ---- batch: 050 ----
mean loss: 216.11
train mean loss: 220.19
epoch train time: 0:00:07.844340
elapsed time: 0:20:43.140012
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 14:14:03.656019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.61
 ---- batch: 020 ----
mean loss: 214.77
 ---- batch: 030 ----
mean loss: 229.99
 ---- batch: 040 ----
mean loss: 214.96
 ---- batch: 050 ----
mean loss: 223.73
train mean loss: 219.31
epoch train time: 0:00:07.839725
elapsed time: 0:20:50.981011
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 14:14:11.496953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.58
 ---- batch: 020 ----
mean loss: 217.21
 ---- batch: 030 ----
mean loss: 212.84
 ---- batch: 040 ----
mean loss: 222.36
 ---- batch: 050 ----
mean loss: 220.12
train mean loss: 219.11
epoch train time: 0:00:07.829599
elapsed time: 0:20:58.811812
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 14:14:19.327757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.05
 ---- batch: 020 ----
mean loss: 220.66
 ---- batch: 030 ----
mean loss: 216.80
 ---- batch: 040 ----
mean loss: 212.34
 ---- batch: 050 ----
mean loss: 219.69
train mean loss: 216.99
epoch train time: 0:00:07.813289
elapsed time: 0:21:06.626285
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 14:14:27.142203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.28
 ---- batch: 020 ----
mean loss: 212.40
 ---- batch: 030 ----
mean loss: 226.30
 ---- batch: 040 ----
mean loss: 216.98
 ---- batch: 050 ----
mean loss: 210.98
train mean loss: 217.37
epoch train time: 0:00:07.806948
elapsed time: 0:21:14.434381
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 14:14:34.950354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.89
 ---- batch: 020 ----
mean loss: 220.88
 ---- batch: 030 ----
mean loss: 209.05
 ---- batch: 040 ----
mean loss: 212.75
 ---- batch: 050 ----
mean loss: 216.46
train mean loss: 215.48
epoch train time: 0:00:07.804161
elapsed time: 0:21:22.239810
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 14:14:42.755744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.09
 ---- batch: 020 ----
mean loss: 211.62
 ---- batch: 030 ----
mean loss: 215.21
 ---- batch: 040 ----
mean loss: 219.53
 ---- batch: 050 ----
mean loss: 209.94
train mean loss: 217.16
epoch train time: 0:00:07.810329
elapsed time: 0:21:30.051400
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 14:14:50.567342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.61
 ---- batch: 020 ----
mean loss: 223.55
 ---- batch: 030 ----
mean loss: 212.54
 ---- batch: 040 ----
mean loss: 207.98
 ---- batch: 050 ----
mean loss: 219.43
train mean loss: 215.99
epoch train time: 0:00:07.823246
elapsed time: 0:21:37.875827
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 14:14:58.391747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.04
 ---- batch: 020 ----
mean loss: 210.42
 ---- batch: 030 ----
mean loss: 217.70
 ---- batch: 040 ----
mean loss: 212.07
 ---- batch: 050 ----
mean loss: 210.87
train mean loss: 213.06
epoch train time: 0:00:07.831905
elapsed time: 0:21:45.708820
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 14:15:06.224729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.59
 ---- batch: 020 ----
mean loss: 213.29
 ---- batch: 030 ----
mean loss: 210.65
 ---- batch: 040 ----
mean loss: 212.47
 ---- batch: 050 ----
mean loss: 216.57
train mean loss: 213.39
epoch train time: 0:00:07.833236
elapsed time: 0:21:53.543173
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 14:15:14.059097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.27
 ---- batch: 020 ----
mean loss: 212.09
 ---- batch: 030 ----
mean loss: 223.13
 ---- batch: 040 ----
mean loss: 213.51
 ---- batch: 050 ----
mean loss: 207.99
train mean loss: 213.22
epoch train time: 0:00:07.846140
elapsed time: 0:22:01.390494
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 14:15:21.906392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.41
 ---- batch: 020 ----
mean loss: 216.46
 ---- batch: 030 ----
mean loss: 218.26
 ---- batch: 040 ----
mean loss: 201.50
 ---- batch: 050 ----
mean loss: 210.42
train mean loss: 210.30
epoch train time: 0:00:07.861644
elapsed time: 0:22:09.253228
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 14:15:29.769158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.91
 ---- batch: 020 ----
mean loss: 203.91
 ---- batch: 030 ----
mean loss: 215.97
 ---- batch: 040 ----
mean loss: 213.55
 ---- batch: 050 ----
mean loss: 209.33
train mean loss: 211.18
epoch train time: 0:00:07.832987
elapsed time: 0:22:17.087389
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 14:15:37.603307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.13
 ---- batch: 020 ----
mean loss: 215.89
 ---- batch: 030 ----
mean loss: 216.39
 ---- batch: 040 ----
mean loss: 207.09
 ---- batch: 050 ----
mean loss: 201.98
train mean loss: 211.31
epoch train time: 0:00:07.848022
elapsed time: 0:22:24.936625
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 14:15:45.452522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.23
 ---- batch: 020 ----
mean loss: 218.85
 ---- batch: 030 ----
mean loss: 208.16
 ---- batch: 040 ----
mean loss: 210.03
 ---- batch: 050 ----
mean loss: 202.67
train mean loss: 208.81
epoch train time: 0:00:07.862312
elapsed time: 0:22:32.800033
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 14:15:53.315990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.95
 ---- batch: 020 ----
mean loss: 216.16
 ---- batch: 030 ----
mean loss: 206.80
 ---- batch: 040 ----
mean loss: 215.76
 ---- batch: 050 ----
mean loss: 205.95
train mean loss: 211.09
epoch train time: 0:00:07.821741
elapsed time: 0:22:40.622949
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 14:16:01.138876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.33
 ---- batch: 020 ----
mean loss: 215.40
 ---- batch: 030 ----
mean loss: 219.29
 ---- batch: 040 ----
mean loss: 213.88
 ---- batch: 050 ----
mean loss: 203.16
train mean loss: 213.35
epoch train time: 0:00:07.809658
elapsed time: 0:22:48.433731
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 14:16:08.949683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.44
 ---- batch: 020 ----
mean loss: 210.83
 ---- batch: 030 ----
mean loss: 212.66
 ---- batch: 040 ----
mean loss: 224.07
 ---- batch: 050 ----
mean loss: 204.82
train mean loss: 211.49
epoch train time: 0:00:07.816215
elapsed time: 0:22:56.251202
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 14:16:16.767116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.70
 ---- batch: 020 ----
mean loss: 212.90
 ---- batch: 030 ----
mean loss: 211.37
 ---- batch: 040 ----
mean loss: 210.55
 ---- batch: 050 ----
mean loss: 204.99
train mean loss: 209.81
epoch train time: 0:00:07.842726
elapsed time: 0:23:04.095091
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 14:16:24.610968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.91
 ---- batch: 020 ----
mean loss: 208.99
 ---- batch: 030 ----
mean loss: 214.67
 ---- batch: 040 ----
mean loss: 207.24
 ---- batch: 050 ----
mean loss: 211.74
train mean loss: 208.38
epoch train time: 0:00:07.821127
elapsed time: 0:23:11.917592
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 14:16:32.433238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.46
 ---- batch: 020 ----
mean loss: 205.41
 ---- batch: 030 ----
mean loss: 211.24
 ---- batch: 040 ----
mean loss: 206.69
 ---- batch: 050 ----
mean loss: 211.66
train mean loss: 208.13
epoch train time: 0:00:07.803445
elapsed time: 0:23:19.721952
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 14:16:40.237902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.48
 ---- batch: 020 ----
mean loss: 205.47
 ---- batch: 030 ----
mean loss: 205.94
 ---- batch: 040 ----
mean loss: 205.13
 ---- batch: 050 ----
mean loss: 209.53
train mean loss: 208.06
epoch train time: 0:00:07.816650
elapsed time: 0:23:27.539782
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 14:16:48.055711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.43
 ---- batch: 020 ----
mean loss: 204.10
 ---- batch: 030 ----
mean loss: 214.89
 ---- batch: 040 ----
mean loss: 215.31
 ---- batch: 050 ----
mean loss: 209.46
train mean loss: 206.70
epoch train time: 0:00:07.800696
elapsed time: 0:23:35.341649
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 14:16:55.857558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.46
 ---- batch: 020 ----
mean loss: 211.89
 ---- batch: 030 ----
mean loss: 207.09
 ---- batch: 040 ----
mean loss: 200.78
 ---- batch: 050 ----
mean loss: 201.86
train mean loss: 205.59
epoch train time: 0:00:07.798983
elapsed time: 0:23:43.141778
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 14:17:03.657766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.21
 ---- batch: 020 ----
mean loss: 197.53
 ---- batch: 030 ----
mean loss: 206.21
 ---- batch: 040 ----
mean loss: 210.88
 ---- batch: 050 ----
mean loss: 208.11
train mean loss: 204.45
epoch train time: 0:00:07.813088
elapsed time: 0:23:50.956084
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 14:17:11.472000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.37
 ---- batch: 020 ----
mean loss: 204.51
 ---- batch: 030 ----
mean loss: 202.66
 ---- batch: 040 ----
mean loss: 207.81
 ---- batch: 050 ----
mean loss: 202.39
train mean loss: 203.68
epoch train time: 0:00:07.827911
elapsed time: 0:23:58.785169
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 14:17:19.301131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.10
 ---- batch: 020 ----
mean loss: 208.60
 ---- batch: 030 ----
mean loss: 210.56
 ---- batch: 040 ----
mean loss: 205.32
 ---- batch: 050 ----
mean loss: 200.76
train mean loss: 204.23
epoch train time: 0:00:07.856258
elapsed time: 0:24:06.642630
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 14:17:27.158577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.89
 ---- batch: 020 ----
mean loss: 207.24
 ---- batch: 030 ----
mean loss: 197.81
 ---- batch: 040 ----
mean loss: 207.76
 ---- batch: 050 ----
mean loss: 215.22
train mean loss: 205.53
epoch train time: 0:00:07.919881
elapsed time: 0:24:14.563658
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 14:17:35.079579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.35
 ---- batch: 020 ----
mean loss: 207.74
 ---- batch: 030 ----
mean loss: 200.91
 ---- batch: 040 ----
mean loss: 209.15
 ---- batch: 050 ----
mean loss: 210.09
train mean loss: 205.17
epoch train time: 0:00:07.816032
elapsed time: 0:24:22.380945
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 14:17:42.896904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.24
 ---- batch: 020 ----
mean loss: 203.00
 ---- batch: 030 ----
mean loss: 210.52
 ---- batch: 040 ----
mean loss: 202.30
 ---- batch: 050 ----
mean loss: 204.47
train mean loss: 204.97
epoch train time: 0:00:07.816520
elapsed time: 0:24:30.198634
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 14:17:50.714589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.86
 ---- batch: 020 ----
mean loss: 204.32
 ---- batch: 030 ----
mean loss: 198.04
 ---- batch: 040 ----
mean loss: 207.11
 ---- batch: 050 ----
mean loss: 196.39
train mean loss: 201.88
epoch train time: 0:00:07.840935
elapsed time: 0:24:38.040735
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 14:17:58.556668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.46
 ---- batch: 020 ----
mean loss: 211.35
 ---- batch: 030 ----
mean loss: 198.56
 ---- batch: 040 ----
mean loss: 197.53
 ---- batch: 050 ----
mean loss: 207.77
train mean loss: 203.21
epoch train time: 0:00:07.852489
elapsed time: 0:24:45.894454
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 14:18:06.410385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.77
 ---- batch: 020 ----
mean loss: 195.85
 ---- batch: 030 ----
mean loss: 193.42
 ---- batch: 040 ----
mean loss: 208.29
 ---- batch: 050 ----
mean loss: 208.99
train mean loss: 201.30
epoch train time: 0:00:07.803275
elapsed time: 0:24:53.698842
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 14:18:14.214749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.20
 ---- batch: 020 ----
mean loss: 201.77
 ---- batch: 030 ----
mean loss: 196.78
 ---- batch: 040 ----
mean loss: 204.16
 ---- batch: 050 ----
mean loss: 203.57
train mean loss: 203.10
epoch train time: 0:00:07.817733
elapsed time: 0:25:01.517828
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 14:18:22.033808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.68
 ---- batch: 020 ----
mean loss: 198.11
 ---- batch: 030 ----
mean loss: 197.95
 ---- batch: 040 ----
mean loss: 195.04
 ---- batch: 050 ----
mean loss: 203.18
train mean loss: 199.71
epoch train time: 0:00:07.806183
elapsed time: 0:25:09.325222
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 14:18:29.841168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.67
 ---- batch: 020 ----
mean loss: 201.87
 ---- batch: 030 ----
mean loss: 206.72
 ---- batch: 040 ----
mean loss: 200.15
 ---- batch: 050 ----
mean loss: 196.93
train mean loss: 201.36
epoch train time: 0:00:07.797479
elapsed time: 0:25:17.123816
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 14:18:37.639720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.65
 ---- batch: 020 ----
mean loss: 202.23
 ---- batch: 030 ----
mean loss: 191.89
 ---- batch: 040 ----
mean loss: 202.22
 ---- batch: 050 ----
mean loss: 194.58
train mean loss: 200.49
epoch train time: 0:00:07.796029
elapsed time: 0:25:24.920973
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 14:18:45.436960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.11
 ---- batch: 020 ----
mean loss: 204.73
 ---- batch: 030 ----
mean loss: 195.65
 ---- batch: 040 ----
mean loss: 204.19
 ---- batch: 050 ----
mean loss: 206.24
train mean loss: 200.67
epoch train time: 0:00:07.811076
elapsed time: 0:25:32.733293
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 14:18:53.249254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.83
 ---- batch: 020 ----
mean loss: 199.27
 ---- batch: 030 ----
mean loss: 202.69
 ---- batch: 040 ----
mean loss: 201.83
 ---- batch: 050 ----
mean loss: 188.38
train mean loss: 199.05
epoch train time: 0:00:07.844095
elapsed time: 0:25:40.578629
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 14:19:01.094569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.77
 ---- batch: 020 ----
mean loss: 200.66
 ---- batch: 030 ----
mean loss: 196.87
 ---- batch: 040 ----
mean loss: 210.97
 ---- batch: 050 ----
mean loss: 206.99
train mean loss: 200.55
epoch train time: 0:00:07.840627
elapsed time: 0:25:48.420408
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 14:19:08.936312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.47
 ---- batch: 020 ----
mean loss: 200.37
 ---- batch: 030 ----
mean loss: 196.45
 ---- batch: 040 ----
mean loss: 198.42
 ---- batch: 050 ----
mean loss: 202.19
train mean loss: 200.37
epoch train time: 0:00:07.841316
elapsed time: 0:25:56.263056
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 14:19:16.779010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.62
 ---- batch: 020 ----
mean loss: 199.48
 ---- batch: 030 ----
mean loss: 196.50
 ---- batch: 040 ----
mean loss: 200.54
 ---- batch: 050 ----
mean loss: 200.66
train mean loss: 199.64
epoch train time: 0:00:07.833216
elapsed time: 0:26:04.097489
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 14:19:24.613414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.67
 ---- batch: 020 ----
mean loss: 199.09
 ---- batch: 030 ----
mean loss: 196.30
 ---- batch: 040 ----
mean loss: 197.56
 ---- batch: 050 ----
mean loss: 197.15
train mean loss: 198.22
epoch train time: 0:00:07.830230
elapsed time: 0:26:11.928869
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 14:19:32.444789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.96
 ---- batch: 020 ----
mean loss: 192.64
 ---- batch: 030 ----
mean loss: 205.33
 ---- batch: 040 ----
mean loss: 201.16
 ---- batch: 050 ----
mean loss: 198.54
train mean loss: 198.96
epoch train time: 0:00:07.818230
elapsed time: 0:26:19.748249
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 14:19:40.264201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.57
 ---- batch: 020 ----
mean loss: 200.25
 ---- batch: 030 ----
mean loss: 207.08
 ---- batch: 040 ----
mean loss: 196.41
 ---- batch: 050 ----
mean loss: 197.89
train mean loss: 199.26
epoch train time: 0:00:07.776717
elapsed time: 0:26:27.526216
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 14:19:48.042109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.77
 ---- batch: 020 ----
mean loss: 199.07
 ---- batch: 030 ----
mean loss: 200.89
 ---- batch: 040 ----
mean loss: 204.65
 ---- batch: 050 ----
mean loss: 192.79
train mean loss: 198.37
epoch train time: 0:00:07.804123
elapsed time: 0:26:35.331459
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 14:19:55.847400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.99
 ---- batch: 020 ----
mean loss: 203.16
 ---- batch: 030 ----
mean loss: 194.63
 ---- batch: 040 ----
mean loss: 195.26
 ---- batch: 050 ----
mean loss: 198.41
train mean loss: 197.58
epoch train time: 0:00:07.818164
elapsed time: 0:26:43.150786
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 14:20:03.666741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.25
 ---- batch: 020 ----
mean loss: 194.45
 ---- batch: 030 ----
mean loss: 193.12
 ---- batch: 040 ----
mean loss: 193.50
 ---- batch: 050 ----
mean loss: 198.74
train mean loss: 195.57
epoch train time: 0:00:07.820338
elapsed time: 0:26:50.972861
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 14:20:11.488493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.10
 ---- batch: 020 ----
mean loss: 195.10
 ---- batch: 030 ----
mean loss: 190.77
 ---- batch: 040 ----
mean loss: 187.35
 ---- batch: 050 ----
mean loss: 199.33
train mean loss: 195.40
epoch train time: 0:00:07.830737
elapsed time: 0:26:58.804476
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 14:20:19.320391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.19
 ---- batch: 020 ----
mean loss: 194.86
 ---- batch: 030 ----
mean loss: 188.24
 ---- batch: 040 ----
mean loss: 194.16
 ---- batch: 050 ----
mean loss: 199.41
train mean loss: 194.68
epoch train time: 0:00:07.827950
elapsed time: 0:27:06.633648
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 14:20:27.149586
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.06
 ---- batch: 020 ----
mean loss: 190.72
 ---- batch: 030 ----
mean loss: 193.00
 ---- batch: 040 ----
mean loss: 193.01
 ---- batch: 050 ----
mean loss: 194.86
train mean loss: 194.15
epoch train time: 0:00:07.830927
elapsed time: 0:27:14.465810
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 14:20:34.981828
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.34
 ---- batch: 020 ----
mean loss: 201.02
 ---- batch: 030 ----
mean loss: 194.29
 ---- batch: 040 ----
mean loss: 196.41
 ---- batch: 050 ----
mean loss: 201.18
train mean loss: 195.33
epoch train time: 0:00:07.815642
elapsed time: 0:27:22.282777
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 14:20:42.798689
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.85
 ---- batch: 020 ----
mean loss: 195.90
 ---- batch: 030 ----
mean loss: 199.81
 ---- batch: 040 ----
mean loss: 201.39
 ---- batch: 050 ----
mean loss: 196.94
train mean loss: 194.53
epoch train time: 0:00:07.863790
elapsed time: 0:27:30.147751
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 14:20:50.663703
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.18
 ---- batch: 020 ----
mean loss: 191.75
 ---- batch: 030 ----
mean loss: 192.31
 ---- batch: 040 ----
mean loss: 202.80
 ---- batch: 050 ----
mean loss: 190.33
train mean loss: 194.48
epoch train time: 0:00:07.813086
elapsed time: 0:27:37.961970
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 14:20:58.477884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.44
 ---- batch: 020 ----
mean loss: 198.66
 ---- batch: 030 ----
mean loss: 197.78
 ---- batch: 040 ----
mean loss: 186.33
 ---- batch: 050 ----
mean loss: 195.25
train mean loss: 194.27
epoch train time: 0:00:07.846042
elapsed time: 0:27:45.809051
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 14:21:06.324950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.36
 ---- batch: 020 ----
mean loss: 195.54
 ---- batch: 030 ----
mean loss: 190.11
 ---- batch: 040 ----
mean loss: 199.53
 ---- batch: 050 ----
mean loss: 193.38
train mean loss: 195.10
epoch train time: 0:00:07.840462
elapsed time: 0:27:53.650654
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 14:21:14.166542
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.97
 ---- batch: 020 ----
mean loss: 200.31
 ---- batch: 030 ----
mean loss: 194.78
 ---- batch: 040 ----
mean loss: 195.56
 ---- batch: 050 ----
mean loss: 192.78
train mean loss: 194.12
epoch train time: 0:00:07.853026
elapsed time: 0:28:01.504979
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 14:21:22.020928
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.20
 ---- batch: 020 ----
mean loss: 203.51
 ---- batch: 030 ----
mean loss: 196.12
 ---- batch: 040 ----
mean loss: 185.36
 ---- batch: 050 ----
mean loss: 197.81
train mean loss: 195.26
epoch train time: 0:00:07.888252
elapsed time: 0:28:09.394462
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 14:21:29.910382
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.09
 ---- batch: 020 ----
mean loss: 193.58
 ---- batch: 030 ----
mean loss: 198.26
 ---- batch: 040 ----
mean loss: 192.68
 ---- batch: 050 ----
mean loss: 193.35
train mean loss: 194.57
epoch train time: 0:00:07.824437
elapsed time: 0:28:17.220041
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 14:21:37.736049
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.24
 ---- batch: 020 ----
mean loss: 193.04
 ---- batch: 030 ----
mean loss: 196.94
 ---- batch: 040 ----
mean loss: 185.55
 ---- batch: 050 ----
mean loss: 196.03
train mean loss: 194.25
epoch train time: 0:00:07.826054
elapsed time: 0:28:25.047323
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 14:21:45.563247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.42
 ---- batch: 020 ----
mean loss: 196.75
 ---- batch: 030 ----
mean loss: 191.36
 ---- batch: 040 ----
mean loss: 191.24
 ---- batch: 050 ----
mean loss: 191.62
train mean loss: 194.51
epoch train time: 0:00:07.832451
elapsed time: 0:28:32.880851
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 14:21:53.396633
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.29
 ---- batch: 020 ----
mean loss: 191.63
 ---- batch: 030 ----
mean loss: 188.08
 ---- batch: 040 ----
mean loss: 208.29
 ---- batch: 050 ----
mean loss: 196.85
train mean loss: 195.28
epoch train time: 0:00:07.844596
elapsed time: 0:28:40.726557
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 14:22:01.242476
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.89
 ---- batch: 020 ----
mean loss: 199.52
 ---- batch: 030 ----
mean loss: 198.76
 ---- batch: 040 ----
mean loss: 196.34
 ---- batch: 050 ----
mean loss: 190.99
train mean loss: 195.50
epoch train time: 0:00:07.831492
elapsed time: 0:28:48.559221
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 14:22:09.075158
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.36
 ---- batch: 020 ----
mean loss: 195.40
 ---- batch: 030 ----
mean loss: 198.41
 ---- batch: 040 ----
mean loss: 194.98
 ---- batch: 050 ----
mean loss: 187.77
train mean loss: 195.13
epoch train time: 0:00:07.839340
elapsed time: 0:28:56.399699
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 14:22:16.915612
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.86
 ---- batch: 020 ----
mean loss: 186.20
 ---- batch: 030 ----
mean loss: 194.09
 ---- batch: 040 ----
mean loss: 187.78
 ---- batch: 050 ----
mean loss: 193.68
train mean loss: 193.83
epoch train time: 0:00:07.836155
elapsed time: 0:29:04.236955
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 14:22:24.752889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.12
 ---- batch: 020 ----
mean loss: 188.25
 ---- batch: 030 ----
mean loss: 192.40
 ---- batch: 040 ----
mean loss: 194.41
 ---- batch: 050 ----
mean loss: 194.90
train mean loss: 194.65
epoch train time: 0:00:07.841710
elapsed time: 0:29:12.079843
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 14:22:32.595764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.94
 ---- batch: 020 ----
mean loss: 201.34
 ---- batch: 030 ----
mean loss: 187.12
 ---- batch: 040 ----
mean loss: 203.12
 ---- batch: 050 ----
mean loss: 190.39
train mean loss: 193.99
epoch train time: 0:00:07.892612
elapsed time: 0:29:19.973559
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 14:22:40.489467
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.24
 ---- batch: 020 ----
mean loss: 198.65
 ---- batch: 030 ----
mean loss: 197.41
 ---- batch: 040 ----
mean loss: 195.79
 ---- batch: 050 ----
mean loss: 202.16
train mean loss: 193.88
epoch train time: 0:00:07.862794
elapsed time: 0:29:27.837489
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 14:22:48.353405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.90
 ---- batch: 020 ----
mean loss: 200.42
 ---- batch: 030 ----
mean loss: 188.88
 ---- batch: 040 ----
mean loss: 194.38
 ---- batch: 050 ----
mean loss: 200.09
train mean loss: 194.17
epoch train time: 0:00:07.824461
elapsed time: 0:29:35.663151
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 14:22:56.179119
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.82
 ---- batch: 020 ----
mean loss: 190.01
 ---- batch: 030 ----
mean loss: 195.39
 ---- batch: 040 ----
mean loss: 197.02
 ---- batch: 050 ----
mean loss: 197.53
train mean loss: 194.90
epoch train time: 0:00:07.874447
elapsed time: 0:29:43.538882
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 14:23:04.054796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.90
 ---- batch: 020 ----
mean loss: 202.53
 ---- batch: 030 ----
mean loss: 199.40
 ---- batch: 040 ----
mean loss: 188.94
 ---- batch: 050 ----
mean loss: 193.81
train mean loss: 193.93
epoch train time: 0:00:07.827489
elapsed time: 0:29:51.367762
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 14:23:11.883686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.02
 ---- batch: 020 ----
mean loss: 194.93
 ---- batch: 030 ----
mean loss: 194.17
 ---- batch: 040 ----
mean loss: 196.65
 ---- batch: 050 ----
mean loss: 188.36
train mean loss: 194.02
epoch train time: 0:00:07.858450
elapsed time: 0:29:59.227413
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 14:23:19.743350
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.09
 ---- batch: 020 ----
mean loss: 191.32
 ---- batch: 030 ----
mean loss: 197.82
 ---- batch: 040 ----
mean loss: 190.56
 ---- batch: 050 ----
mean loss: 194.82
train mean loss: 193.88
epoch train time: 0:00:07.847836
elapsed time: 0:30:07.076722
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 14:23:27.592666
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.89
 ---- batch: 020 ----
mean loss: 185.24
 ---- batch: 030 ----
mean loss: 197.36
 ---- batch: 040 ----
mean loss: 197.47
 ---- batch: 050 ----
mean loss: 189.31
train mean loss: 194.09
epoch train time: 0:00:07.822734
elapsed time: 0:30:14.900699
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 14:23:35.416621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.06
 ---- batch: 020 ----
mean loss: 194.87
 ---- batch: 030 ----
mean loss: 193.52
 ---- batch: 040 ----
mean loss: 194.14
 ---- batch: 050 ----
mean loss: 192.71
train mean loss: 193.70
epoch train time: 0:00:07.814679
elapsed time: 0:30:22.716550
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 14:23:43.232496
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.55
 ---- batch: 020 ----
mean loss: 197.00
 ---- batch: 030 ----
mean loss: 195.86
 ---- batch: 040 ----
mean loss: 182.75
 ---- batch: 050 ----
mean loss: 200.27
train mean loss: 194.51
epoch train time: 0:00:07.830577
elapsed time: 0:30:30.548299
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 14:23:51.064234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.98
 ---- batch: 020 ----
mean loss: 192.24
 ---- batch: 030 ----
mean loss: 189.26
 ---- batch: 040 ----
mean loss: 196.33
 ---- batch: 050 ----
mean loss: 202.35
train mean loss: 193.62
epoch train time: 0:00:07.852740
elapsed time: 0:30:38.402224
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 14:23:58.918152
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.86
 ---- batch: 020 ----
mean loss: 193.50
 ---- batch: 030 ----
mean loss: 194.94
 ---- batch: 040 ----
mean loss: 188.79
 ---- batch: 050 ----
mean loss: 197.03
train mean loss: 193.96
epoch train time: 0:00:07.823437
elapsed time: 0:30:46.226805
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 14:24:06.742732
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.16
 ---- batch: 020 ----
mean loss: 193.63
 ---- batch: 030 ----
mean loss: 202.54
 ---- batch: 040 ----
mean loss: 190.83
 ---- batch: 050 ----
mean loss: 187.48
train mean loss: 193.68
epoch train time: 0:00:07.800340
elapsed time: 0:30:54.028293
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 14:24:14.544269
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.16
 ---- batch: 020 ----
mean loss: 190.87
 ---- batch: 030 ----
mean loss: 188.30
 ---- batch: 040 ----
mean loss: 196.48
 ---- batch: 050 ----
mean loss: 193.00
train mean loss: 193.86
epoch train time: 0:00:07.808861
elapsed time: 0:31:01.838689
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 14:24:22.354297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.50
 ---- batch: 020 ----
mean loss: 191.08
 ---- batch: 030 ----
mean loss: 195.02
 ---- batch: 040 ----
mean loss: 191.78
 ---- batch: 050 ----
mean loss: 197.77
train mean loss: 193.75
epoch train time: 0:00:07.816608
elapsed time: 0:31:09.656085
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 14:24:30.171995
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.67
 ---- batch: 020 ----
mean loss: 195.32
 ---- batch: 030 ----
mean loss: 195.68
 ---- batch: 040 ----
mean loss: 190.99
 ---- batch: 050 ----
mean loss: 188.16
train mean loss: 193.92
epoch train time: 0:00:07.797849
elapsed time: 0:31:17.455083
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 14:24:37.971028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.01
 ---- batch: 020 ----
mean loss: 190.19
 ---- batch: 030 ----
mean loss: 189.19
 ---- batch: 040 ----
mean loss: 200.18
 ---- batch: 050 ----
mean loss: 200.23
train mean loss: 194.17
epoch train time: 0:00:07.794209
elapsed time: 0:31:25.250518
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 14:24:45.766458
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.27
 ---- batch: 020 ----
mean loss: 197.90
 ---- batch: 030 ----
mean loss: 194.54
 ---- batch: 040 ----
mean loss: 196.47
 ---- batch: 050 ----
mean loss: 183.58
train mean loss: 193.40
epoch train time: 0:00:07.798887
elapsed time: 0:31:33.050778
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 14:24:53.566692
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.63
 ---- batch: 020 ----
mean loss: 191.88
 ---- batch: 030 ----
mean loss: 198.03
 ---- batch: 040 ----
mean loss: 194.84
 ---- batch: 050 ----
mean loss: 195.25
train mean loss: 193.83
epoch train time: 0:00:07.806160
elapsed time: 0:31:40.858144
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 14:25:01.374079
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.36
 ---- batch: 020 ----
mean loss: 194.66
 ---- batch: 030 ----
mean loss: 196.25
 ---- batch: 040 ----
mean loss: 186.55
 ---- batch: 050 ----
mean loss: 200.29
train mean loss: 193.34
epoch train time: 0:00:07.805050
elapsed time: 0:31:48.664408
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 14:25:09.180397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.25
 ---- batch: 020 ----
mean loss: 186.23
 ---- batch: 030 ----
mean loss: 186.40
 ---- batch: 040 ----
mean loss: 198.98
 ---- batch: 050 ----
mean loss: 193.54
train mean loss: 193.71
epoch train time: 0:00:07.823796
elapsed time: 0:31:56.489377
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 14:25:17.005304
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.66
 ---- batch: 020 ----
mean loss: 196.85
 ---- batch: 030 ----
mean loss: 197.60
 ---- batch: 040 ----
mean loss: 204.33
 ---- batch: 050 ----
mean loss: 186.48
train mean loss: 193.86
epoch train time: 0:00:07.815536
elapsed time: 0:32:04.306112
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 14:25:24.822042
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.01
 ---- batch: 020 ----
mean loss: 196.22
 ---- batch: 030 ----
mean loss: 188.03
 ---- batch: 040 ----
mean loss: 195.68
 ---- batch: 050 ----
mean loss: 193.00
train mean loss: 194.09
epoch train time: 0:00:07.817231
elapsed time: 0:32:12.124555
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 14:25:32.640471
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.29
 ---- batch: 020 ----
mean loss: 197.34
 ---- batch: 030 ----
mean loss: 188.57
 ---- batch: 040 ----
mean loss: 191.32
 ---- batch: 050 ----
mean loss: 192.00
train mean loss: 194.05
epoch train time: 0:00:07.805590
elapsed time: 0:32:19.931272
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 14:25:40.447250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.57
 ---- batch: 020 ----
mean loss: 190.94
 ---- batch: 030 ----
mean loss: 188.37
 ---- batch: 040 ----
mean loss: 193.89
 ---- batch: 050 ----
mean loss: 202.74
train mean loss: 194.21
epoch train time: 0:00:07.842469
elapsed time: 0:32:27.774927
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 14:25:48.290845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.49
 ---- batch: 020 ----
mean loss: 194.57
 ---- batch: 030 ----
mean loss: 190.41
 ---- batch: 040 ----
mean loss: 195.11
 ---- batch: 050 ----
mean loss: 197.53
train mean loss: 192.90
epoch train time: 0:00:07.843387
elapsed time: 0:32:35.619532
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 14:25:56.135475
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.44
 ---- batch: 020 ----
mean loss: 195.11
 ---- batch: 030 ----
mean loss: 194.11
 ---- batch: 040 ----
mean loss: 195.58
 ---- batch: 050 ----
mean loss: 190.82
train mean loss: 193.25
epoch train time: 0:00:07.811939
elapsed time: 0:32:43.432608
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 14:26:03.948514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.67
 ---- batch: 020 ----
mean loss: 186.95
 ---- batch: 030 ----
mean loss: 191.65
 ---- batch: 040 ----
mean loss: 195.52
 ---- batch: 050 ----
mean loss: 203.73
train mean loss: 193.23
epoch train time: 0:00:07.800992
elapsed time: 0:32:51.234769
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 14:26:11.750671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.36
 ---- batch: 020 ----
mean loss: 192.87
 ---- batch: 030 ----
mean loss: 192.63
 ---- batch: 040 ----
mean loss: 195.85
 ---- batch: 050 ----
mean loss: 191.26
train mean loss: 193.12
epoch train time: 0:00:07.819941
elapsed time: 0:32:59.055799
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 14:26:19.571723
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.24
 ---- batch: 020 ----
mean loss: 189.91
 ---- batch: 030 ----
mean loss: 200.33
 ---- batch: 040 ----
mean loss: 189.64
 ---- batch: 050 ----
mean loss: 193.49
train mean loss: 192.96
epoch train time: 0:00:07.825760
elapsed time: 0:33:06.891528
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_6/checkpoint.pth.tar
**** end time: 2019-09-26 14:26:27.407094 ****
