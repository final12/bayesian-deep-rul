Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.25, resume=False, step_size=200, visualize_step=50)
pid: 16541
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 15:31:36.120176 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 15:31:36.137466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2991.22
 ---- batch: 020 ----
mean loss: 1575.90
train mean loss: 2128.28
epoch train time: 0:00:11.822876
elapsed time: 0:00:11.848609
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 15:31:47.968826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1264.77
 ---- batch: 020 ----
mean loss: 1221.95
train mean loss: 1221.46
epoch train time: 0:00:04.226468
elapsed time: 0:00:16.075543
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 15:31:52.195862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1168.39
 ---- batch: 020 ----
mean loss: 1141.15
train mean loss: 1144.90
epoch train time: 0:00:04.213875
elapsed time: 0:00:20.290080
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 15:31:56.410373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1095.30
 ---- batch: 020 ----
mean loss: 1109.77
train mean loss: 1100.63
epoch train time: 0:00:04.226890
elapsed time: 0:00:24.517535
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 15:32:00.637836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1097.31
 ---- batch: 020 ----
mean loss: 1070.12
train mean loss: 1080.75
epoch train time: 0:00:04.214400
elapsed time: 0:00:28.732507
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 15:32:04.852795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1062.30
 ---- batch: 020 ----
mean loss: 1023.87
train mean loss: 1040.97
epoch train time: 0:00:04.211798
elapsed time: 0:00:32.944873
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 15:32:09.065172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1019.06
 ---- batch: 020 ----
mean loss: 1045.05
train mean loss: 1027.45
epoch train time: 0:00:04.204726
elapsed time: 0:00:37.150208
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 15:32:13.270506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1018.54
 ---- batch: 020 ----
mean loss: 1005.31
train mean loss: 1017.21
epoch train time: 0:00:04.210319
elapsed time: 0:00:41.361075
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 15:32:17.481376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1017.18
 ---- batch: 020 ----
mean loss: 1009.74
train mean loss: 1014.24
epoch train time: 0:00:04.225652
elapsed time: 0:00:45.587300
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 15:32:21.707591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1014.21
 ---- batch: 020 ----
mean loss: 1004.88
train mean loss: 1010.51
epoch train time: 0:00:04.207445
elapsed time: 0:00:49.795300
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 15:32:25.915605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1004.74
 ---- batch: 020 ----
mean loss: 993.48
train mean loss: 1008.01
epoch train time: 0:00:04.209954
elapsed time: 0:00:54.005833
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 15:32:30.126136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1010.18
 ---- batch: 020 ----
mean loss: 996.59
train mean loss: 998.62
epoch train time: 0:00:04.216112
elapsed time: 0:00:58.222538
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 15:32:34.342837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 986.30
 ---- batch: 020 ----
mean loss: 985.75
train mean loss: 995.38
epoch train time: 0:00:04.215164
elapsed time: 0:01:02.438280
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 15:32:38.558572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 999.60
 ---- batch: 020 ----
mean loss: 992.42
train mean loss: 993.97
epoch train time: 0:00:04.222445
elapsed time: 0:01:06.661344
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 15:32:42.781680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 992.47
 ---- batch: 020 ----
mean loss: 973.49
train mean loss: 980.10
epoch train time: 0:00:04.207667
elapsed time: 0:01:10.869601
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 15:32:46.989922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 981.69
 ---- batch: 020 ----
mean loss: 980.78
train mean loss: 982.21
epoch train time: 0:00:04.199715
elapsed time: 0:01:15.069879
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 15:32:51.190174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 979.60
 ---- batch: 020 ----
mean loss: 983.04
train mean loss: 984.94
epoch train time: 0:00:04.217339
elapsed time: 0:01:19.287772
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 15:32:55.408090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 994.36
 ---- batch: 020 ----
mean loss: 989.73
train mean loss: 989.49
epoch train time: 0:00:04.225770
elapsed time: 0:01:23.514211
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 15:32:59.634575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 969.62
 ---- batch: 020 ----
mean loss: 971.11
train mean loss: 965.45
epoch train time: 0:00:04.209470
elapsed time: 0:01:27.724306
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 15:33:03.844609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 989.54
 ---- batch: 020 ----
mean loss: 977.10
train mean loss: 984.36
epoch train time: 0:00:04.208822
elapsed time: 0:01:31.933755
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 15:33:08.054072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 973.92
 ---- batch: 020 ----
mean loss: 986.77
train mean loss: 973.62
epoch train time: 0:00:04.213401
elapsed time: 0:01:36.148055
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 15:33:12.268372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 984.10
 ---- batch: 020 ----
mean loss: 969.48
train mean loss: 979.99
epoch train time: 0:00:04.227380
elapsed time: 0:01:40.375998
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 15:33:16.496335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 981.86
 ---- batch: 020 ----
mean loss: 988.13
train mean loss: 979.68
epoch train time: 0:00:04.223104
elapsed time: 0:01:44.599743
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 15:33:20.720074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 957.87
 ---- batch: 020 ----
mean loss: 964.55
train mean loss: 965.39
epoch train time: 0:00:04.223176
elapsed time: 0:01:48.823524
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 15:33:24.943840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 974.85
 ---- batch: 020 ----
mean loss: 971.65
train mean loss: 967.51
epoch train time: 0:00:04.217202
elapsed time: 0:01:53.041295
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 15:33:29.161596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 966.05
 ---- batch: 020 ----
mean loss: 969.45
train mean loss: 965.62
epoch train time: 0:00:04.211926
elapsed time: 0:01:57.253775
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 15:33:33.374061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.59
 ---- batch: 020 ----
mean loss: 959.88
train mean loss: 956.73
epoch train time: 0:00:04.207242
elapsed time: 0:02:01.461538
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 15:33:37.581838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 956.32
 ---- batch: 020 ----
mean loss: 959.44
train mean loss: 961.48
epoch train time: 0:00:04.204006
elapsed time: 0:02:05.666172
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 15:33:41.786491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 958.50
 ---- batch: 020 ----
mean loss: 961.80
train mean loss: 958.52
epoch train time: 0:00:04.221696
elapsed time: 0:02:09.888473
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 15:33:46.008766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.74
 ---- batch: 020 ----
mean loss: 948.57
train mean loss: 950.83
epoch train time: 0:00:04.213129
elapsed time: 0:02:14.102133
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 15:33:50.222457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.25
 ---- batch: 020 ----
mean loss: 936.76
train mean loss: 951.85
epoch train time: 0:00:04.216490
elapsed time: 0:02:18.319204
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 15:33:54.439512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 950.40
 ---- batch: 020 ----
mean loss: 953.32
train mean loss: 948.69
epoch train time: 0:00:04.220702
elapsed time: 0:02:22.540461
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 15:33:58.660753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.52
 ---- batch: 020 ----
mean loss: 938.89
train mean loss: 952.49
epoch train time: 0:00:04.230347
elapsed time: 0:02:26.771385
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 15:34:02.891696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.80
 ---- batch: 020 ----
mean loss: 956.31
train mean loss: 947.39
epoch train time: 0:00:04.237711
elapsed time: 0:02:31.009707
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 15:34:07.130003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 958.64
 ---- batch: 020 ----
mean loss: 926.77
train mean loss: 948.46
epoch train time: 0:00:04.204636
elapsed time: 0:02:35.214916
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 15:34:11.335257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.01
 ---- batch: 020 ----
mean loss: 940.65
train mean loss: 933.72
epoch train time: 0:00:04.211493
elapsed time: 0:02:39.426970
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 15:34:15.547286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.48
 ---- batch: 020 ----
mean loss: 939.02
train mean loss: 938.42
epoch train time: 0:00:04.210131
elapsed time: 0:02:43.637762
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 15:34:19.758070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.25
 ---- batch: 020 ----
mean loss: 947.14
train mean loss: 942.18
epoch train time: 0:00:04.222990
elapsed time: 0:02:47.861364
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 15:34:23.981693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.27
 ---- batch: 020 ----
mean loss: 922.76
train mean loss: 935.22
epoch train time: 0:00:04.261115
elapsed time: 0:02:52.123039
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 15:34:28.243352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.84
 ---- batch: 020 ----
mean loss: 939.00
train mean loss: 931.65
epoch train time: 0:00:04.214875
elapsed time: 0:02:56.338494
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 15:34:32.458814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.45
 ---- batch: 020 ----
mean loss: 915.85
train mean loss: 930.89
epoch train time: 0:00:04.207037
elapsed time: 0:03:00.546174
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 15:34:36.666486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.66
 ---- batch: 020 ----
mean loss: 936.33
train mean loss: 924.64
epoch train time: 0:00:04.184645
elapsed time: 0:03:04.731387
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 15:34:40.851682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.20
 ---- batch: 020 ----
mean loss: 928.32
train mean loss: 921.05
epoch train time: 0:00:04.196825
elapsed time: 0:03:08.928820
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 15:34:45.049172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.27
 ---- batch: 020 ----
mean loss: 916.49
train mean loss: 919.38
epoch train time: 0:00:04.190408
elapsed time: 0:03:13.119804
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 15:34:49.240104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.77
 ---- batch: 020 ----
mean loss: 916.57
train mean loss: 923.07
epoch train time: 0:00:04.206975
elapsed time: 0:03:17.327566
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 15:34:53.447871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 920.00
 ---- batch: 020 ----
mean loss: 915.38
train mean loss: 916.92
epoch train time: 0:00:04.191298
elapsed time: 0:03:21.519426
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 15:34:57.639726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.93
 ---- batch: 020 ----
mean loss: 925.02
train mean loss: 917.08
epoch train time: 0:00:04.208094
elapsed time: 0:03:25.728168
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 15:35:01.848463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.34
 ---- batch: 020 ----
mean loss: 912.07
train mean loss: 913.64
epoch train time: 0:00:04.218522
elapsed time: 0:03:29.947267
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 15:35:06.067559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.48
 ---- batch: 020 ----
mean loss: 906.06
train mean loss: 901.66
epoch train time: 0:00:04.201363
elapsed time: 0:03:34.149137
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 15:35:10.269439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.32
 ---- batch: 020 ----
mean loss: 895.15
train mean loss: 902.07
epoch train time: 0:00:04.210805
elapsed time: 0:03:38.360544
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 15:35:14.480832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.80
 ---- batch: 020 ----
mean loss: 903.55
train mean loss: 902.78
epoch train time: 0:00:04.232433
elapsed time: 0:03:42.593531
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 15:35:18.713826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.04
 ---- batch: 020 ----
mean loss: 896.14
train mean loss: 899.70
epoch train time: 0:00:04.209042
elapsed time: 0:03:46.803131
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 15:35:22.923454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.20
 ---- batch: 020 ----
mean loss: 883.59
train mean loss: 898.64
epoch train time: 0:00:04.214811
elapsed time: 0:03:51.018469
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 15:35:27.138753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.92
 ---- batch: 020 ----
mean loss: 883.73
train mean loss: 890.92
epoch train time: 0:00:04.226890
elapsed time: 0:03:55.245921
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 15:35:31.366215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.48
 ---- batch: 020 ----
mean loss: 901.29
train mean loss: 882.71
epoch train time: 0:00:04.205108
elapsed time: 0:03:59.451601
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 15:35:35.571950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.99
 ---- batch: 020 ----
mean loss: 886.28
train mean loss: 874.72
epoch train time: 0:00:04.240008
elapsed time: 0:04:03.692204
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 15:35:39.812526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.60
 ---- batch: 020 ----
mean loss: 869.49
train mean loss: 864.20
epoch train time: 0:00:04.253118
elapsed time: 0:04:07.945917
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 15:35:44.066213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.03
 ---- batch: 020 ----
mean loss: 873.44
train mean loss: 856.32
epoch train time: 0:00:04.214273
elapsed time: 0:04:12.160731
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 15:35:48.281043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.18
 ---- batch: 020 ----
mean loss: 837.35
train mean loss: 834.27
epoch train time: 0:00:04.205077
elapsed time: 0:04:16.366406
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 15:35:52.486699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.36
 ---- batch: 020 ----
mean loss: 820.94
train mean loss: 824.72
epoch train time: 0:00:04.307994
elapsed time: 0:04:20.675192
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 15:35:56.795555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 809.55
 ---- batch: 020 ----
mean loss: 801.42
train mean loss: 802.19
epoch train time: 0:00:04.242192
elapsed time: 0:04:24.917990
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 15:36:01.038287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 770.57
 ---- batch: 020 ----
mean loss: 751.85
train mean loss: 760.93
epoch train time: 0:00:04.258614
elapsed time: 0:04:29.177187
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 15:36:05.297498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.39
 ---- batch: 020 ----
mean loss: 717.33
train mean loss: 722.27
epoch train time: 0:00:04.199312
elapsed time: 0:04:33.377063
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 15:36:09.497386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 687.61
 ---- batch: 020 ----
mean loss: 702.62
train mean loss: 698.97
epoch train time: 0:00:04.230585
elapsed time: 0:04:37.608229
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 15:36:13.728560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 694.68
 ---- batch: 020 ----
mean loss: 687.23
train mean loss: 685.75
epoch train time: 0:00:04.237230
elapsed time: 0:04:41.846046
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 15:36:17.966336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 679.63
 ---- batch: 020 ----
mean loss: 671.03
train mean loss: 673.96
epoch train time: 0:00:04.256470
elapsed time: 0:04:46.103069
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 15:36:22.223367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 661.98
 ---- batch: 020 ----
mean loss: 671.88
train mean loss: 664.76
epoch train time: 0:00:04.262489
elapsed time: 0:04:50.366117
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 15:36:26.486488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 648.59
 ---- batch: 020 ----
mean loss: 637.91
train mean loss: 649.04
epoch train time: 0:00:04.255659
elapsed time: 0:04:54.622408
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 15:36:30.742713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 637.03
 ---- batch: 020 ----
mean loss: 626.72
train mean loss: 632.97
epoch train time: 0:00:04.252206
elapsed time: 0:04:58.875183
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 15:36:34.995497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 625.92
 ---- batch: 020 ----
mean loss: 622.94
train mean loss: 618.55
epoch train time: 0:00:04.230289
elapsed time: 0:05:03.106232
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 15:36:39.226532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 600.54
 ---- batch: 020 ----
mean loss: 605.05
train mean loss: 605.63
epoch train time: 0:00:04.255921
elapsed time: 0:05:07.362713
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 15:36:43.483031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.81
 ---- batch: 020 ----
mean loss: 588.23
train mean loss: 592.64
epoch train time: 0:00:04.255233
elapsed time: 0:05:11.618575
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 15:36:47.738919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.75
 ---- batch: 020 ----
mean loss: 570.83
train mean loss: 576.82
epoch train time: 0:00:04.227850
elapsed time: 0:05:15.847076
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 15:36:51.967375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 586.08
 ---- batch: 020 ----
mean loss: 576.21
train mean loss: 578.92
epoch train time: 0:00:04.213276
elapsed time: 0:05:20.060937
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 15:36:56.181234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 551.40
 ---- batch: 020 ----
mean loss: 548.30
train mean loss: 548.59
epoch train time: 0:00:04.232174
elapsed time: 0:05:24.293679
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 15:37:00.413973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.44
 ---- batch: 020 ----
mean loss: 538.57
train mean loss: 542.70
epoch train time: 0:00:04.201109
elapsed time: 0:05:28.495319
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 15:37:04.615620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.86
 ---- batch: 020 ----
mean loss: 538.27
train mean loss: 532.62
epoch train time: 0:00:04.221671
elapsed time: 0:05:32.717717
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 15:37:08.838055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.01
 ---- batch: 020 ----
mean loss: 529.88
train mean loss: 525.53
epoch train time: 0:00:04.211530
elapsed time: 0:05:36.929848
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 15:37:13.050147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.05
 ---- batch: 020 ----
mean loss: 512.21
train mean loss: 512.58
epoch train time: 0:00:04.231920
elapsed time: 0:05:41.162339
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 15:37:17.282655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 510.88
 ---- batch: 020 ----
mean loss: 500.92
train mean loss: 502.68
epoch train time: 0:00:04.209184
elapsed time: 0:05:45.372159
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 15:37:21.492477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.77
 ---- batch: 020 ----
mean loss: 490.15
train mean loss: 491.16
epoch train time: 0:00:04.209544
elapsed time: 0:05:49.582355
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 15:37:25.702651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.45
 ---- batch: 020 ----
mean loss: 496.28
train mean loss: 489.69
epoch train time: 0:00:04.206613
elapsed time: 0:05:53.789587
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 15:37:29.909900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.26
 ---- batch: 020 ----
mean loss: 477.05
train mean loss: 486.52
epoch train time: 0:00:04.204737
elapsed time: 0:05:57.994874
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 15:37:34.115183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.84
 ---- batch: 020 ----
mean loss: 473.48
train mean loss: 475.55
epoch train time: 0:00:04.213883
elapsed time: 0:06:02.209305
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 15:37:38.329624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.60
 ---- batch: 020 ----
mean loss: 459.19
train mean loss: 464.56
epoch train time: 0:00:04.224997
elapsed time: 0:06:06.434879
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 15:37:42.555203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.51
 ---- batch: 020 ----
mean loss: 476.37
train mean loss: 470.27
epoch train time: 0:00:04.207962
elapsed time: 0:06:10.643436
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 15:37:46.763762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.81
 ---- batch: 020 ----
mean loss: 450.41
train mean loss: 454.84
epoch train time: 0:00:04.210397
elapsed time: 0:06:14.854458
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 15:37:50.974763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.15
 ---- batch: 020 ----
mean loss: 446.96
train mean loss: 450.79
epoch train time: 0:00:04.207276
elapsed time: 0:06:19.062338
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 15:37:55.182643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.06
 ---- batch: 020 ----
mean loss: 451.93
train mean loss: 445.57
epoch train time: 0:00:04.224922
elapsed time: 0:06:23.287785
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 15:37:59.408079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.37
 ---- batch: 020 ----
mean loss: 430.80
train mean loss: 440.01
epoch train time: 0:00:04.216777
elapsed time: 0:06:27.505102
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 15:38:03.625401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.90
 ---- batch: 020 ----
mean loss: 434.90
train mean loss: 440.03
epoch train time: 0:00:04.223127
elapsed time: 0:06:31.728795
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 15:38:07.849078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.84
 ---- batch: 020 ----
mean loss: 445.79
train mean loss: 443.28
epoch train time: 0:00:04.219745
elapsed time: 0:06:35.949068
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 15:38:12.069425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.20
 ---- batch: 020 ----
mean loss: 426.67
train mean loss: 427.16
epoch train time: 0:00:04.235261
elapsed time: 0:06:40.184998
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 15:38:16.305320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.05
 ---- batch: 020 ----
mean loss: 414.03
train mean loss: 422.34
epoch train time: 0:00:04.214753
elapsed time: 0:06:44.400402
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 15:38:20.520759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.28
 ---- batch: 020 ----
mean loss: 421.57
train mean loss: 420.15
epoch train time: 0:00:04.209962
elapsed time: 0:06:48.610972
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 15:38:24.731269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.50
 ---- batch: 020 ----
mean loss: 414.30
train mean loss: 415.18
epoch train time: 0:00:04.220595
elapsed time: 0:06:52.832153
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 15:38:28.952459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.71
 ---- batch: 020 ----
mean loss: 414.01
train mean loss: 416.30
epoch train time: 0:00:04.211637
elapsed time: 0:06:57.044372
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 15:38:33.164666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.14
 ---- batch: 020 ----
mean loss: 414.52
train mean loss: 411.90
epoch train time: 0:00:04.206153
elapsed time: 0:07:01.251089
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 15:38:37.371382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.46
 ---- batch: 020 ----
mean loss: 401.47
train mean loss: 408.76
epoch train time: 0:00:04.204105
elapsed time: 0:07:05.455750
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 15:38:41.576053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.37
 ---- batch: 020 ----
mean loss: 410.00
train mean loss: 401.91
epoch train time: 0:00:04.231998
elapsed time: 0:07:09.688308
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 15:38:45.808604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.77
 ---- batch: 020 ----
mean loss: 404.93
train mean loss: 398.06
epoch train time: 0:00:04.220910
elapsed time: 0:07:13.909749
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 15:38:50.030051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.50
 ---- batch: 020 ----
mean loss: 397.22
train mean loss: 396.72
epoch train time: 0:00:04.219989
elapsed time: 0:07:18.130279
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 15:38:54.250593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.67
 ---- batch: 020 ----
mean loss: 395.94
train mean loss: 398.34
epoch train time: 0:00:04.220677
elapsed time: 0:07:22.351518
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 15:38:58.471807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.98
 ---- batch: 020 ----
mean loss: 388.65
train mean loss: 390.36
epoch train time: 0:00:04.225350
elapsed time: 0:07:26.577409
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 15:39:02.697735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.94
 ---- batch: 020 ----
mean loss: 390.22
train mean loss: 388.80
epoch train time: 0:00:04.213255
elapsed time: 0:07:30.791247
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 15:39:06.911549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.14
 ---- batch: 020 ----
mean loss: 383.88
train mean loss: 390.45
epoch train time: 0:00:04.248067
elapsed time: 0:07:35.039836
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 15:39:11.160158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.67
 ---- batch: 020 ----
mean loss: 381.76
train mean loss: 383.34
epoch train time: 0:00:04.238408
elapsed time: 0:07:39.278992
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 15:39:15.399223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.86
 ---- batch: 020 ----
mean loss: 379.07
train mean loss: 381.73
epoch train time: 0:00:04.222423
elapsed time: 0:07:43.501904
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 15:39:19.622214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.13
 ---- batch: 020 ----
mean loss: 385.36
train mean loss: 382.75
epoch train time: 0:00:04.204625
elapsed time: 0:07:47.707104
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 15:39:23.827396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.77
 ---- batch: 020 ----
mean loss: 387.16
train mean loss: 383.73
epoch train time: 0:00:04.212312
elapsed time: 0:07:51.919995
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 15:39:28.040289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.00
 ---- batch: 020 ----
mean loss: 364.59
train mean loss: 366.89
epoch train time: 0:00:04.231787
elapsed time: 0:07:56.152321
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 15:39:32.272614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.40
 ---- batch: 020 ----
mean loss: 367.26
train mean loss: 369.00
epoch train time: 0:00:04.205679
elapsed time: 0:08:00.358626
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 15:39:36.478937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.05
 ---- batch: 020 ----
mean loss: 374.93
train mean loss: 372.88
epoch train time: 0:00:04.187118
elapsed time: 0:08:04.546324
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 15:39:40.666612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.91
 ---- batch: 020 ----
mean loss: 369.47
train mean loss: 367.59
epoch train time: 0:00:04.212300
elapsed time: 0:08:08.759154
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 15:39:44.879440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.88
 ---- batch: 020 ----
mean loss: 358.85
train mean loss: 367.36
epoch train time: 0:00:04.206630
elapsed time: 0:08:12.966311
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 15:39:49.086606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.16
 ---- batch: 020 ----
mean loss: 361.11
train mean loss: 363.02
epoch train time: 0:00:04.220014
elapsed time: 0:08:17.186870
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 15:39:53.307173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.93
 ---- batch: 020 ----
mean loss: 361.34
train mean loss: 361.17
epoch train time: 0:00:04.208622
elapsed time: 0:08:21.396034
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 15:39:57.516376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.35
 ---- batch: 020 ----
mean loss: 353.62
train mean loss: 353.24
epoch train time: 0:00:04.221388
elapsed time: 0:08:25.618003
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 15:40:01.738306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.78
 ---- batch: 020 ----
mean loss: 359.11
train mean loss: 357.86
epoch train time: 0:00:04.206826
elapsed time: 0:08:29.825463
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 15:40:05.945773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.73
 ---- batch: 020 ----
mean loss: 347.98
train mean loss: 355.75
epoch train time: 0:00:04.197795
elapsed time: 0:08:34.023813
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 15:40:10.144111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.54
 ---- batch: 020 ----
mean loss: 354.44
train mean loss: 351.45
epoch train time: 0:00:04.205119
elapsed time: 0:08:38.229512
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 15:40:14.349825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.06
 ---- batch: 020 ----
mean loss: 341.49
train mean loss: 351.25
epoch train time: 0:00:04.208466
elapsed time: 0:08:42.438575
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 15:40:18.558879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.81
 ---- batch: 020 ----
mean loss: 351.84
train mean loss: 353.13
epoch train time: 0:00:04.207399
elapsed time: 0:08:46.646622
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 15:40:22.766928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.03
 ---- batch: 020 ----
mean loss: 347.83
train mean loss: 347.49
epoch train time: 0:00:04.205864
elapsed time: 0:08:50.853158
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 15:40:26.973455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.82
 ---- batch: 020 ----
mean loss: 349.32
train mean loss: 347.11
epoch train time: 0:00:04.209150
elapsed time: 0:08:55.062860
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 15:40:31.183151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.36
 ---- batch: 020 ----
mean loss: 349.08
train mean loss: 347.77
epoch train time: 0:00:04.213752
elapsed time: 0:08:59.277182
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 15:40:35.397485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.45
 ---- batch: 020 ----
mean loss: 335.78
train mean loss: 336.35
epoch train time: 0:00:04.218321
elapsed time: 0:09:03.496171
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 15:40:39.616417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.46
 ---- batch: 020 ----
mean loss: 339.20
train mean loss: 340.21
epoch train time: 0:00:04.225589
elapsed time: 0:09:07.722259
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 15:40:43.842558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.27
 ---- batch: 020 ----
mean loss: 339.97
train mean loss: 337.44
epoch train time: 0:00:04.207724
elapsed time: 0:09:11.930582
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 15:40:48.050874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.79
 ---- batch: 020 ----
mean loss: 326.45
train mean loss: 332.45
epoch train time: 0:00:04.224590
elapsed time: 0:09:16.155802
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 15:40:52.276120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.86
 ---- batch: 020 ----
mean loss: 332.19
train mean loss: 334.86
epoch train time: 0:00:04.229045
elapsed time: 0:09:20.385537
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 15:40:56.505838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.03
 ---- batch: 020 ----
mean loss: 339.86
train mean loss: 335.80
epoch train time: 0:00:04.219826
elapsed time: 0:09:24.605911
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 15:41:00.726206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.21
 ---- batch: 020 ----
mean loss: 334.58
train mean loss: 333.42
epoch train time: 0:00:04.233872
elapsed time: 0:09:28.840397
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 15:41:04.960773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.90
 ---- batch: 020 ----
mean loss: 325.29
train mean loss: 325.29
epoch train time: 0:00:04.253671
elapsed time: 0:09:33.094687
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 15:41:09.214982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.31
 ---- batch: 020 ----
mean loss: 331.94
train mean loss: 327.31
epoch train time: 0:00:04.255874
elapsed time: 0:09:37.351098
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 15:41:13.471388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.82
 ---- batch: 020 ----
mean loss: 331.04
train mean loss: 328.31
epoch train time: 0:00:04.228001
elapsed time: 0:09:41.579780
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 15:41:17.700092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.57
 ---- batch: 020 ----
mean loss: 319.45
train mean loss: 327.39
epoch train time: 0:00:04.229965
elapsed time: 0:09:45.810384
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 15:41:21.930619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.06
 ---- batch: 020 ----
mean loss: 317.90
train mean loss: 323.68
epoch train time: 0:00:04.213762
elapsed time: 0:09:50.024643
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 15:41:26.144947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.13
 ---- batch: 020 ----
mean loss: 322.04
train mean loss: 323.11
epoch train time: 0:00:04.216498
elapsed time: 0:09:54.241741
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 15:41:30.362047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.41
 ---- batch: 020 ----
mean loss: 325.31
train mean loss: 321.39
epoch train time: 0:00:04.232294
elapsed time: 0:09:58.474616
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 15:41:34.594912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.74
 ---- batch: 020 ----
mean loss: 317.92
train mean loss: 319.66
epoch train time: 0:00:04.223583
elapsed time: 0:10:02.698765
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 15:41:38.819087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.24
 ---- batch: 020 ----
mean loss: 328.94
train mean loss: 317.66
epoch train time: 0:00:04.233535
elapsed time: 0:10:06.932941
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 15:41:43.053234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.39
 ---- batch: 020 ----
mean loss: 320.25
train mean loss: 317.98
epoch train time: 0:00:04.229890
elapsed time: 0:10:11.163366
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 15:41:47.283676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.40
 ---- batch: 020 ----
mean loss: 310.78
train mean loss: 310.62
epoch train time: 0:00:04.208838
elapsed time: 0:10:15.372849
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 15:41:51.493089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.42
 ---- batch: 020 ----
mean loss: 313.66
train mean loss: 314.07
epoch train time: 0:00:04.211393
elapsed time: 0:10:19.584725
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 15:41:55.705021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.66
 ---- batch: 020 ----
mean loss: 319.88
train mean loss: 317.74
epoch train time: 0:00:04.214977
elapsed time: 0:10:23.800427
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 15:41:59.920764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.06
 ---- batch: 020 ----
mean loss: 316.02
train mean loss: 312.82
epoch train time: 0:00:04.228232
elapsed time: 0:10:28.029299
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 15:42:04.149596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.01
 ---- batch: 020 ----
mean loss: 309.74
train mean loss: 314.41
epoch train time: 0:00:04.205356
elapsed time: 0:10:32.235264
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 15:42:08.355587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.03
 ---- batch: 020 ----
mean loss: 314.11
train mean loss: 310.73
epoch train time: 0:00:04.211163
elapsed time: 0:10:36.447058
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 15:42:12.567301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.15
 ---- batch: 020 ----
mean loss: 301.12
train mean loss: 305.45
epoch train time: 0:00:04.230790
elapsed time: 0:10:40.678364
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 15:42:16.798657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.34
 ---- batch: 020 ----
mean loss: 314.85
train mean loss: 308.33
epoch train time: 0:00:04.219145
elapsed time: 0:10:44.898061
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 15:42:21.018370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.97
 ---- batch: 020 ----
mean loss: 307.34
train mean loss: 311.71
epoch train time: 0:00:04.190847
elapsed time: 0:10:49.089501
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 15:42:25.209807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.49
 ---- batch: 020 ----
mean loss: 304.59
train mean loss: 303.68
epoch train time: 0:00:04.198173
elapsed time: 0:10:53.288283
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 15:42:29.408624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.52
 ---- batch: 020 ----
mean loss: 301.84
train mean loss: 301.94
epoch train time: 0:00:04.210855
elapsed time: 0:10:57.499754
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 15:42:33.620068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.79
 ---- batch: 020 ----
mean loss: 309.91
train mean loss: 309.69
epoch train time: 0:00:04.212572
elapsed time: 0:11:01.712898
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 15:42:37.833209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.94
 ---- batch: 020 ----
mean loss: 295.15
train mean loss: 299.90
epoch train time: 0:00:04.209461
elapsed time: 0:11:05.922904
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 15:42:42.043194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.87
 ---- batch: 020 ----
mean loss: 297.85
train mean loss: 296.81
epoch train time: 0:00:04.193784
elapsed time: 0:11:10.117185
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 15:42:46.237474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.37
 ---- batch: 020 ----
mean loss: 298.05
train mean loss: 297.18
epoch train time: 0:00:04.212158
elapsed time: 0:11:14.329857
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 15:42:50.450167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.09
 ---- batch: 020 ----
mean loss: 303.50
train mean loss: 299.05
epoch train time: 0:00:04.209192
elapsed time: 0:11:18.539608
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 15:42:54.659900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.60
 ---- batch: 020 ----
mean loss: 299.09
train mean loss: 297.60
epoch train time: 0:00:04.200579
elapsed time: 0:11:22.740803
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 15:42:58.861137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.64
 ---- batch: 020 ----
mean loss: 296.38
train mean loss: 299.08
epoch train time: 0:00:04.211342
elapsed time: 0:11:26.952746
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 15:43:03.073072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.35
 ---- batch: 020 ----
mean loss: 305.55
train mean loss: 299.11
epoch train time: 0:00:04.213594
elapsed time: 0:11:31.167004
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 15:43:07.287320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.73
 ---- batch: 020 ----
mean loss: 289.73
train mean loss: 295.69
epoch train time: 0:00:04.202780
elapsed time: 0:11:35.370341
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 15:43:11.490633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.32
 ---- batch: 020 ----
mean loss: 281.75
train mean loss: 287.55
epoch train time: 0:00:04.206454
elapsed time: 0:11:39.577354
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 15:43:15.697716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.03
 ---- batch: 020 ----
mean loss: 289.30
train mean loss: 294.86
epoch train time: 0:00:04.207988
elapsed time: 0:11:43.786021
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 15:43:19.906328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.47
 ---- batch: 020 ----
mean loss: 296.04
train mean loss: 292.07
epoch train time: 0:00:04.235843
elapsed time: 0:11:48.022550
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 15:43:24.142858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.95
 ---- batch: 020 ----
mean loss: 298.32
train mean loss: 292.88
epoch train time: 0:00:04.211582
elapsed time: 0:11:52.234726
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 15:43:28.355027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.65
 ---- batch: 020 ----
mean loss: 287.05
train mean loss: 290.83
epoch train time: 0:00:04.211374
elapsed time: 0:11:56.446708
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 15:43:32.567003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.14
 ---- batch: 020 ----
mean loss: 288.82
train mean loss: 287.11
epoch train time: 0:00:04.215934
elapsed time: 0:12:00.663201
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 15:43:36.783494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.90
 ---- batch: 020 ----
mean loss: 288.49
train mean loss: 289.80
epoch train time: 0:00:04.217264
elapsed time: 0:12:04.881050
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 15:43:41.001373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.83
 ---- batch: 020 ----
mean loss: 291.81
train mean loss: 288.67
epoch train time: 0:00:04.201547
elapsed time: 0:12:09.083148
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 15:43:45.203474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.49
 ---- batch: 020 ----
mean loss: 291.01
train mean loss: 289.94
epoch train time: 0:00:04.198496
elapsed time: 0:12:13.282240
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 15:43:49.402538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.91
 ---- batch: 020 ----
mean loss: 285.22
train mean loss: 285.74
epoch train time: 0:00:04.225977
elapsed time: 0:12:17.508744
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 15:43:53.629036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.87
 ---- batch: 020 ----
mean loss: 285.43
train mean loss: 287.25
epoch train time: 0:00:04.220561
elapsed time: 0:12:21.729921
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 15:43:57.850147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.02
 ---- batch: 020 ----
mean loss: 286.42
train mean loss: 284.61
epoch train time: 0:00:04.230617
elapsed time: 0:12:25.961046
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 15:44:02.081344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.01
 ---- batch: 020 ----
mean loss: 281.61
train mean loss: 284.41
epoch train time: 0:00:04.208806
elapsed time: 0:12:30.170466
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 15:44:06.290787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.93
 ---- batch: 020 ----
mean loss: 280.62
train mean loss: 283.56
epoch train time: 0:00:04.221573
elapsed time: 0:12:34.392635
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 15:44:10.512953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.54
 ---- batch: 020 ----
mean loss: 281.02
train mean loss: 283.17
epoch train time: 0:00:04.215504
elapsed time: 0:12:38.608695
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 15:44:14.729002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.75
 ---- batch: 020 ----
mean loss: 279.29
train mean loss: 281.43
epoch train time: 0:00:04.240086
elapsed time: 0:12:42.849330
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 15:44:18.969652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.10
 ---- batch: 020 ----
mean loss: 288.26
train mean loss: 284.16
epoch train time: 0:00:04.233796
elapsed time: 0:12:47.083686
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 15:44:23.204013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.03
 ---- batch: 020 ----
mean loss: 285.31
train mean loss: 287.70
epoch train time: 0:00:04.226236
elapsed time: 0:12:51.310543
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 15:44:27.430872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.02
 ---- batch: 020 ----
mean loss: 277.03
train mean loss: 281.31
epoch train time: 0:00:04.220621
elapsed time: 0:12:55.531752
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 15:44:31.652102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.41
 ---- batch: 020 ----
mean loss: 277.12
train mean loss: 278.16
epoch train time: 0:00:04.201933
elapsed time: 0:12:59.734373
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 15:44:35.854745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.03
 ---- batch: 020 ----
mean loss: 277.42
train mean loss: 276.67
epoch train time: 0:00:04.207766
elapsed time: 0:13:03.942726
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 15:44:40.063049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.89
 ---- batch: 020 ----
mean loss: 277.77
train mean loss: 276.66
epoch train time: 0:00:04.227149
elapsed time: 0:13:08.170471
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 15:44:44.290760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.43
 ---- batch: 020 ----
mean loss: 286.73
train mean loss: 279.50
epoch train time: 0:00:04.216729
elapsed time: 0:13:12.387731
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 15:44:48.508028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.35
 ---- batch: 020 ----
mean loss: 274.11
train mean loss: 277.67
epoch train time: 0:00:04.219972
elapsed time: 0:13:16.608293
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 15:44:52.728622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.07
 ---- batch: 020 ----
mean loss: 282.61
train mean loss: 276.61
epoch train time: 0:00:04.220085
elapsed time: 0:13:20.828944
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 15:44:56.949247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.76
 ---- batch: 020 ----
mean loss: 281.95
train mean loss: 277.20
epoch train time: 0:00:04.222089
elapsed time: 0:13:25.051631
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 15:45:01.171935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.14
 ---- batch: 020 ----
mean loss: 269.88
train mean loss: 274.44
epoch train time: 0:00:04.220410
elapsed time: 0:13:29.272687
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 15:45:05.393003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.84
 ---- batch: 020 ----
mean loss: 270.99
train mean loss: 274.58
epoch train time: 0:00:04.226708
elapsed time: 0:13:33.499995
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 15:45:09.620298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.68
 ---- batch: 020 ----
mean loss: 267.22
train mean loss: 269.53
epoch train time: 0:00:04.214225
elapsed time: 0:13:37.714782
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 15:45:13.835078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.61
 ---- batch: 020 ----
mean loss: 271.84
train mean loss: 270.55
epoch train time: 0:00:04.224553
elapsed time: 0:13:41.939865
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 15:45:18.060160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.70
 ---- batch: 020 ----
mean loss: 273.61
train mean loss: 272.68
epoch train time: 0:00:04.223296
elapsed time: 0:13:46.163775
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 15:45:22.284092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.82
 ---- batch: 020 ----
mean loss: 270.10
train mean loss: 275.42
epoch train time: 0:00:04.238442
elapsed time: 0:13:50.402790
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 15:45:26.523174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.82
 ---- batch: 020 ----
mean loss: 265.01
train mean loss: 271.34
epoch train time: 0:00:04.214646
elapsed time: 0:13:54.618069
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 15:45:30.738362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.96
 ---- batch: 020 ----
mean loss: 268.28
train mean loss: 269.04
epoch train time: 0:00:04.216149
elapsed time: 0:13:58.834863
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 15:45:34.955206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.04
 ---- batch: 020 ----
mean loss: 272.66
train mean loss: 270.29
epoch train time: 0:00:04.219633
elapsed time: 0:14:03.055071
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 15:45:39.175370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.96
 ---- batch: 020 ----
mean loss: 258.67
train mean loss: 263.61
epoch train time: 0:00:04.213782
elapsed time: 0:14:07.269413
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 15:45:43.389725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.00
 ---- batch: 020 ----
mean loss: 269.65
train mean loss: 268.74
epoch train time: 0:00:04.228662
elapsed time: 0:14:11.498645
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 15:45:47.618969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.43
 ---- batch: 020 ----
mean loss: 269.51
train mean loss: 270.49
epoch train time: 0:00:04.224880
elapsed time: 0:14:15.724113
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 15:45:51.844414
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.91
 ---- batch: 020 ----
mean loss: 268.43
train mean loss: 266.19
epoch train time: 0:00:04.235687
elapsed time: 0:14:19.960587
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 15:45:56.080809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.42
 ---- batch: 020 ----
mean loss: 268.56
train mean loss: 264.65
epoch train time: 0:00:04.236062
elapsed time: 0:14:24.197153
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 15:46:00.317466
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.16
 ---- batch: 020 ----
mean loss: 265.88
train mean loss: 264.16
epoch train time: 0:00:04.225950
elapsed time: 0:14:28.423714
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 15:46:04.544019
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.21
 ---- batch: 020 ----
mean loss: 263.59
train mean loss: 262.82
epoch train time: 0:00:04.224049
elapsed time: 0:14:32.648367
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 15:46:08.768603
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.08
 ---- batch: 020 ----
mean loss: 272.52
train mean loss: 265.56
epoch train time: 0:00:04.233239
elapsed time: 0:14:36.882090
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 15:46:13.002397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 269.92
 ---- batch: 020 ----
mean loss: 258.05
train mean loss: 265.51
epoch train time: 0:00:04.269324
elapsed time: 0:14:41.152045
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 15:46:17.272337
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.20
 ---- batch: 020 ----
mean loss: 270.45
train mean loss: 266.41
epoch train time: 0:00:04.262866
elapsed time: 0:14:45.415447
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 15:46:21.535743
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.59
 ---- batch: 020 ----
mean loss: 262.62
train mean loss: 265.78
epoch train time: 0:00:04.260800
elapsed time: 0:14:49.676841
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 15:46:25.797167
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.20
 ---- batch: 020 ----
mean loss: 268.21
train mean loss: 265.56
epoch train time: 0:00:04.225596
elapsed time: 0:14:53.903036
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 15:46:30.023344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.29
 ---- batch: 020 ----
mean loss: 266.38
train mean loss: 266.29
epoch train time: 0:00:04.217807
elapsed time: 0:14:58.121505
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 15:46:34.241812
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 263.10
 ---- batch: 020 ----
mean loss: 261.56
train mean loss: 263.33
epoch train time: 0:00:04.226146
elapsed time: 0:15:02.348196
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 15:46:38.468490
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.25
 ---- batch: 020 ----
mean loss: 261.41
train mean loss: 263.48
epoch train time: 0:00:04.212024
elapsed time: 0:15:06.560820
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 15:46:42.681059
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.70
 ---- batch: 020 ----
mean loss: 265.44
train mean loss: 265.59
epoch train time: 0:00:04.218349
elapsed time: 0:15:10.779677
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 15:46:46.899995
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 271.01
 ---- batch: 020 ----
mean loss: 262.60
train mean loss: 265.20
epoch train time: 0:00:04.227346
elapsed time: 0:15:15.007683
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 15:46:51.128050
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.63
 ---- batch: 020 ----
mean loss: 264.17
train mean loss: 264.91
epoch train time: 0:00:04.213032
elapsed time: 0:15:19.221294
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 15:46:55.341589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.20
 ---- batch: 020 ----
mean loss: 274.24
train mean loss: 263.36
epoch train time: 0:00:04.221245
elapsed time: 0:15:23.443081
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 15:46:59.563390
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.62
 ---- batch: 020 ----
mean loss: 262.73
train mean loss: 263.05
epoch train time: 0:00:04.224156
elapsed time: 0:15:27.667818
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 15:47:03.788105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.88
 ---- batch: 020 ----
mean loss: 263.36
train mean loss: 265.80
epoch train time: 0:00:04.223988
elapsed time: 0:15:31.892338
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 15:47:08.012673
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.14
 ---- batch: 020 ----
mean loss: 264.18
train mean loss: 264.38
epoch train time: 0:00:04.280173
elapsed time: 0:15:36.173110
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 15:47:12.293406
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.14
 ---- batch: 020 ----
mean loss: 268.89
train mean loss: 264.80
epoch train time: 0:00:04.229159
elapsed time: 0:15:40.402876
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 15:47:16.523184
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.54
 ---- batch: 020 ----
mean loss: 267.22
train mean loss: 265.08
epoch train time: 0:00:04.217094
elapsed time: 0:15:44.620583
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 15:47:20.740921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.87
 ---- batch: 020 ----
mean loss: 263.13
train mean loss: 264.64
epoch train time: 0:00:04.223492
elapsed time: 0:15:48.844644
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 15:47:24.964959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.92
 ---- batch: 020 ----
mean loss: 263.33
train mean loss: 263.87
epoch train time: 0:00:04.218766
elapsed time: 0:15:53.063954
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 15:47:29.184263
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.50
 ---- batch: 020 ----
mean loss: 268.64
train mean loss: 263.35
epoch train time: 0:00:04.229790
elapsed time: 0:15:57.294359
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 15:47:33.414650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.86
 ---- batch: 020 ----
mean loss: 259.38
train mean loss: 262.77
epoch train time: 0:00:04.218092
elapsed time: 0:16:01.512971
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 15:47:37.633306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.04
 ---- batch: 020 ----
mean loss: 261.19
train mean loss: 264.09
epoch train time: 0:00:04.221366
elapsed time: 0:16:05.734967
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 15:47:41.855264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 267.78
 ---- batch: 020 ----
mean loss: 263.27
train mean loss: 263.54
epoch train time: 0:00:04.217092
elapsed time: 0:16:09.952636
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 15:47:46.072957
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.29
 ---- batch: 020 ----
mean loss: 268.66
train mean loss: 262.21
epoch train time: 0:00:04.204382
elapsed time: 0:16:14.157621
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 15:47:50.277939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 267.03
 ---- batch: 020 ----
mean loss: 261.01
train mean loss: 263.23
epoch train time: 0:00:04.227852
elapsed time: 0:16:18.386036
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 15:47:54.506330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 270.93
 ---- batch: 020 ----
mean loss: 262.80
train mean loss: 266.98
epoch train time: 0:00:04.225305
elapsed time: 0:16:22.611935
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 15:47:58.732233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.33
 ---- batch: 020 ----
mean loss: 271.84
train mean loss: 263.27
epoch train time: 0:00:04.210761
elapsed time: 0:16:26.823289
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 15:48:02.943586
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.05
 ---- batch: 020 ----
mean loss: 265.06
train mean loss: 262.54
epoch train time: 0:00:04.212373
elapsed time: 0:16:31.036297
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 15:48:07.156618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.03
 ---- batch: 020 ----
mean loss: 263.23
train mean loss: 262.85
epoch train time: 0:00:04.228119
elapsed time: 0:16:35.265145
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 15:48:11.385366
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.08
 ---- batch: 020 ----
mean loss: 260.81
train mean loss: 263.44
epoch train time: 0:00:04.204550
elapsed time: 0:16:39.470201
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 15:48:15.590496
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.57
 ---- batch: 020 ----
mean loss: 254.01
train mean loss: 261.26
epoch train time: 0:00:04.221456
elapsed time: 0:16:43.692213
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 15:48:19.812515
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 274.88
 ---- batch: 020 ----
mean loss: 255.46
train mean loss: 264.50
epoch train time: 0:00:04.218931
elapsed time: 0:16:47.911712
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 15:48:24.032004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.64
 ---- batch: 020 ----
mean loss: 261.54
train mean loss: 261.50
epoch train time: 0:00:04.234521
elapsed time: 0:16:52.146777
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 15:48:28.267075
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.08
 ---- batch: 020 ----
mean loss: 267.10
train mean loss: 264.04
epoch train time: 0:00:04.324532
elapsed time: 0:16:56.471895
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 15:48:32.592188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.54
 ---- batch: 020 ----
mean loss: 266.30
train mean loss: 264.61
epoch train time: 0:00:04.211780
elapsed time: 0:17:00.684270
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 15:48:36.804581
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.92
 ---- batch: 020 ----
mean loss: 266.38
train mean loss: 264.34
epoch train time: 0:00:04.236171
elapsed time: 0:17:04.921042
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 15:48:41.041344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 267.15
 ---- batch: 020 ----
mean loss: 262.74
train mean loss: 265.45
epoch train time: 0:00:04.245006
elapsed time: 0:17:09.166699
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 15:48:45.287020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.68
 ---- batch: 020 ----
mean loss: 263.40
train mean loss: 261.22
epoch train time: 0:00:04.246908
elapsed time: 0:17:13.414206
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 15:48:49.534511
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.15
 ---- batch: 020 ----
mean loss: 260.82
train mean loss: 261.94
epoch train time: 0:00:04.214544
elapsed time: 0:17:17.629367
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 15:48:53.749700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.63
 ---- batch: 020 ----
mean loss: 259.60
train mean loss: 261.96
epoch train time: 0:00:04.246707
elapsed time: 0:17:21.876734
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 15:48:57.997028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.73
 ---- batch: 020 ----
mean loss: 265.94
train mean loss: 262.95
epoch train time: 0:00:04.182820
elapsed time: 0:17:26.060174
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 15:49:02.180477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.66
 ---- batch: 020 ----
mean loss: 265.61
train mean loss: 263.65
epoch train time: 0:00:04.197685
elapsed time: 0:17:30.258431
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 15:49:06.378731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.06
 ---- batch: 020 ----
mean loss: 261.71
train mean loss: 262.58
epoch train time: 0:00:04.250867
elapsed time: 0:17:34.509943
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 15:49:10.630235
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.63
 ---- batch: 020 ----
mean loss: 264.20
train mean loss: 260.82
epoch train time: 0:00:04.239545
elapsed time: 0:17:38.750023
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 15:49:14.870347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.35
 ---- batch: 020 ----
mean loss: 260.88
train mean loss: 260.77
epoch train time: 0:00:04.230787
elapsed time: 0:17:42.990948
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_9/checkpoint.pth.tar
**** end time: 2019-09-25 15:49:19.111140 ****
