Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 23196
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-10-01 14:38:38.125967 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-10-01 14:38:38.135457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4646.29
 ---- batch: 020 ----
mean loss: 4428.84
train mean loss: 4528.07
epoch train time: 0:00:07.798832
elapsed time: 0:00:07.814500
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-10-01 14:38:45.940506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4214.75
 ---- batch: 020 ----
mean loss: 3995.17
train mean loss: 4085.44
epoch train time: 0:00:00.356904
elapsed time: 0:00:08.171637
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-10-01 14:38:46.297690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3748.99
 ---- batch: 020 ----
mean loss: 3717.31
train mean loss: 3728.33
epoch train time: 0:00:00.353352
elapsed time: 0:00:08.525209
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-10-01 14:38:46.651263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3508.29
 ---- batch: 020 ----
mean loss: 3368.21
train mean loss: 3412.39
epoch train time: 0:00:00.351699
elapsed time: 0:00:08.877196
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-10-01 14:38:47.003228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3249.96
 ---- batch: 020 ----
mean loss: 3121.44
train mean loss: 3189.72
epoch train time: 0:00:00.354446
elapsed time: 0:00:09.231862
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-10-01 14:38:47.357906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3042.13
 ---- batch: 020 ----
mean loss: 2942.11
train mean loss: 2979.59
epoch train time: 0:00:00.354908
elapsed time: 0:00:09.587002
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-10-01 14:38:47.713036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2881.78
 ---- batch: 020 ----
mean loss: 2795.95
train mean loss: 2827.69
epoch train time: 0:00:00.356505
elapsed time: 0:00:09.943695
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-10-01 14:38:48.069725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2750.61
 ---- batch: 020 ----
mean loss: 2658.29
train mean loss: 2702.15
epoch train time: 0:00:00.348647
elapsed time: 0:00:10.292527
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-10-01 14:38:48.418556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2630.45
 ---- batch: 020 ----
mean loss: 2562.46
train mean loss: 2583.52
epoch train time: 0:00:00.352826
elapsed time: 0:00:10.645548
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-10-01 14:38:48.771578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2527.19
 ---- batch: 020 ----
mean loss: 2478.07
train mean loss: 2491.60
epoch train time: 0:00:00.358996
elapsed time: 0:00:11.004726
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-10-01 14:38:49.130767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2408.79
 ---- batch: 020 ----
mean loss: 2428.69
train mean loss: 2425.57
epoch train time: 0:00:00.354665
elapsed time: 0:00:11.359584
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-10-01 14:38:49.485617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2381.86
 ---- batch: 020 ----
mean loss: 2310.30
train mean loss: 2349.84
epoch train time: 0:00:00.353746
elapsed time: 0:00:11.713532
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-10-01 14:38:49.839563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2268.28
 ---- batch: 020 ----
mean loss: 2292.53
train mean loss: 2272.76
epoch train time: 0:00:00.354874
elapsed time: 0:00:12.068584
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-10-01 14:38:50.194609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2210.04
 ---- batch: 020 ----
mean loss: 2187.92
train mean loss: 2198.09
epoch train time: 0:00:00.354005
elapsed time: 0:00:12.422761
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-10-01 14:38:50.548803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2172.32
 ---- batch: 020 ----
mean loss: 2109.21
train mean loss: 2148.23
epoch train time: 0:00:00.353296
elapsed time: 0:00:12.776266
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-10-01 14:38:50.902303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2115.10
 ---- batch: 020 ----
mean loss: 2048.54
train mean loss: 2077.86
epoch train time: 0:00:00.347186
elapsed time: 0:00:13.123640
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-10-01 14:38:51.249669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2049.35
 ---- batch: 020 ----
mean loss: 2015.47
train mean loss: 2023.33
epoch train time: 0:00:00.342359
elapsed time: 0:00:13.466187
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-10-01 14:38:51.592229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1992.97
 ---- batch: 020 ----
mean loss: 1941.46
train mean loss: 1971.22
epoch train time: 0:00:00.357465
elapsed time: 0:00:13.823860
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-10-01 14:38:51.949897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1914.52
 ---- batch: 020 ----
mean loss: 1918.17
train mean loss: 1914.56
epoch train time: 0:00:00.338629
elapsed time: 0:00:14.162665
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-10-01 14:38:52.288689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1856.84
 ---- batch: 020 ----
mean loss: 1871.20
train mean loss: 1854.03
epoch train time: 0:00:00.342505
elapsed time: 0:00:14.505435
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-10-01 14:38:52.631498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1830.99
 ---- batch: 020 ----
mean loss: 1786.29
train mean loss: 1800.58
epoch train time: 0:00:00.344901
elapsed time: 0:00:14.850545
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-10-01 14:38:52.976572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1782.46
 ---- batch: 020 ----
mean loss: 1759.61
train mean loss: 1762.77
epoch train time: 0:00:00.338205
elapsed time: 0:00:15.188953
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-10-01 14:38:53.314980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1724.71
 ---- batch: 020 ----
mean loss: 1710.97
train mean loss: 1705.24
epoch train time: 0:00:00.341260
elapsed time: 0:00:15.530401
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-10-01 14:38:53.656429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1676.74
 ---- batch: 020 ----
mean loss: 1685.11
train mean loss: 1677.40
epoch train time: 0:00:00.350444
elapsed time: 0:00:15.881021
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-10-01 14:38:54.007054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1633.61
 ---- batch: 020 ----
mean loss: 1606.55
train mean loss: 1629.45
epoch train time: 0:00:00.354633
elapsed time: 0:00:16.235913
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-10-01 14:38:54.361947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1599.82
 ---- batch: 020 ----
mean loss: 1586.79
train mean loss: 1581.52
epoch train time: 0:00:00.356315
elapsed time: 0:00:16.592427
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-10-01 14:38:54.718457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1553.44
 ---- batch: 020 ----
mean loss: 1541.92
train mean loss: 1536.77
epoch train time: 0:00:00.356891
elapsed time: 0:00:16.949499
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-10-01 14:38:55.075559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1521.07
 ---- batch: 020 ----
mean loss: 1465.30
train mean loss: 1490.57
epoch train time: 0:00:00.358649
elapsed time: 0:00:17.308371
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-10-01 14:38:55.434408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1458.80
 ---- batch: 020 ----
mean loss: 1419.44
train mean loss: 1432.11
epoch train time: 0:00:00.353845
elapsed time: 0:00:17.662403
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-10-01 14:38:55.788436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1393.79
 ---- batch: 020 ----
mean loss: 1372.07
train mean loss: 1384.49
epoch train time: 0:00:00.356098
elapsed time: 0:00:18.018703
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-10-01 14:38:56.144734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1334.66
 ---- batch: 020 ----
mean loss: 1334.96
train mean loss: 1335.41
epoch train time: 0:00:00.350014
elapsed time: 0:00:18.368915
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-10-01 14:38:56.494945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1300.30
 ---- batch: 020 ----
mean loss: 1297.47
train mean loss: 1296.24
epoch train time: 0:00:00.360332
elapsed time: 0:00:18.729434
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-10-01 14:38:56.855525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1289.49
 ---- batch: 020 ----
mean loss: 1255.15
train mean loss: 1270.54
epoch train time: 0:00:00.359272
elapsed time: 0:00:19.088963
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-10-01 14:38:57.215017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1221.41
 ---- batch: 020 ----
mean loss: 1234.60
train mean loss: 1227.12
epoch train time: 0:00:00.351527
elapsed time: 0:00:19.440689
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-10-01 14:38:57.566757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1188.27
 ---- batch: 020 ----
mean loss: 1195.76
train mean loss: 1195.43
epoch train time: 0:00:00.350738
elapsed time: 0:00:19.791644
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-10-01 14:38:57.917677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1158.64
 ---- batch: 020 ----
mean loss: 1152.43
train mean loss: 1154.16
epoch train time: 0:00:00.357437
elapsed time: 0:00:20.149270
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-10-01 14:38:58.275305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1139.78
 ---- batch: 020 ----
mean loss: 1106.90
train mean loss: 1122.57
epoch train time: 0:00:00.355955
elapsed time: 0:00:20.505423
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-10-01 14:38:58.631504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1106.08
 ---- batch: 020 ----
mean loss: 1077.47
train mean loss: 1087.16
epoch train time: 0:00:00.356081
elapsed time: 0:00:20.861737
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-10-01 14:38:58.987775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1061.74
 ---- batch: 020 ----
mean loss: 1041.94
train mean loss: 1053.44
epoch train time: 0:00:00.343795
elapsed time: 0:00:21.205717
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-10-01 14:38:59.331743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1015.08
 ---- batch: 020 ----
mean loss: 1021.66
train mean loss: 1021.73
epoch train time: 0:00:00.350830
elapsed time: 0:00:21.556732
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-10-01 14:38:59.682776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1009.65
 ---- batch: 020 ----
mean loss: 975.80
train mean loss: 990.76
epoch train time: 0:00:00.349879
elapsed time: 0:00:21.906803
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-10-01 14:39:00.032833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 956.64
 ---- batch: 020 ----
mean loss: 968.28
train mean loss: 962.28
epoch train time: 0:00:00.341423
elapsed time: 0:00:22.248415
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-10-01 14:39:00.374446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.87
 ---- batch: 020 ----
mean loss: 912.27
train mean loss: 926.32
epoch train time: 0:00:00.356623
elapsed time: 0:00:22.605230
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-10-01 14:39:00.731314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.04
 ---- batch: 020 ----
mean loss: 892.69
train mean loss: 903.49
epoch train time: 0:00:00.354279
elapsed time: 0:00:22.959744
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-10-01 14:39:01.085801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.93
 ---- batch: 020 ----
mean loss: 866.41
train mean loss: 877.00
epoch train time: 0:00:00.351574
elapsed time: 0:00:23.311571
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-10-01 14:39:01.437600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.89
 ---- batch: 020 ----
mean loss: 834.28
train mean loss: 842.50
epoch train time: 0:00:00.349848
elapsed time: 0:00:23.661673
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-10-01 14:39:01.787703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.81
 ---- batch: 020 ----
mean loss: 808.76
train mean loss: 813.05
epoch train time: 0:00:00.362313
elapsed time: 0:00:24.024167
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-10-01 14:39:02.150197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 798.50
 ---- batch: 020 ----
mean loss: 782.61
train mean loss: 790.57
epoch train time: 0:00:00.358027
elapsed time: 0:00:24.382376
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-10-01 14:39:02.508434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.35
 ---- batch: 020 ----
mean loss: 772.82
train mean loss: 766.88
epoch train time: 0:00:00.356389
elapsed time: 0:00:24.738976
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-10-01 14:39:02.865038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 747.53
 ---- batch: 020 ----
mean loss: 732.27
train mean loss: 737.16
epoch train time: 0:00:00.348684
elapsed time: 0:00:25.087910
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-10-01 14:39:03.213946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 723.90
 ---- batch: 020 ----
mean loss: 706.56
train mean loss: 715.72
epoch train time: 0:00:00.354552
elapsed time: 0:00:25.442650
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-10-01 14:39:03.568675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 709.65
 ---- batch: 020 ----
mean loss: 678.08
train mean loss: 694.51
epoch train time: 0:00:00.353341
elapsed time: 0:00:25.796171
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-10-01 14:39:03.922199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 686.47
 ---- batch: 020 ----
mean loss: 657.23
train mean loss: 665.09
epoch train time: 0:00:00.343938
elapsed time: 0:00:26.140340
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-10-01 14:39:04.266389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 652.77
 ---- batch: 020 ----
mean loss: 641.96
train mean loss: 644.63
epoch train time: 0:00:00.346634
elapsed time: 0:00:26.487169
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-10-01 14:39:04.613195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 625.55
 ---- batch: 020 ----
mean loss: 615.82
train mean loss: 621.56
epoch train time: 0:00:00.348932
elapsed time: 0:00:26.836291
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-10-01 14:39:04.962351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 620.67
 ---- batch: 020 ----
mean loss: 588.78
train mean loss: 603.28
epoch train time: 0:00:00.340333
elapsed time: 0:00:27.176866
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-10-01 14:39:05.302892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.68
 ---- batch: 020 ----
mean loss: 573.40
train mean loss: 573.71
epoch train time: 0:00:00.346154
elapsed time: 0:00:27.523192
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-10-01 14:39:05.649221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 559.93
 ---- batch: 020 ----
mean loss: 556.19
train mean loss: 556.25
epoch train time: 0:00:00.366362
elapsed time: 0:00:27.889733
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-10-01 14:39:06.015774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 543.30
 ---- batch: 020 ----
mean loss: 540.26
train mean loss: 541.71
epoch train time: 0:00:00.350186
elapsed time: 0:00:28.240137
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-10-01 14:39:06.366174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 534.49
 ---- batch: 020 ----
mean loss: 520.17
train mean loss: 523.47
epoch train time: 0:00:00.351563
elapsed time: 0:00:28.591928
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-10-01 14:39:06.717961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.80
 ---- batch: 020 ----
mean loss: 507.23
train mean loss: 508.45
epoch train time: 0:00:00.376912
elapsed time: 0:00:28.969043
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-10-01 14:39:07.095071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.13
 ---- batch: 020 ----
mean loss: 481.97
train mean loss: 486.34
epoch train time: 0:00:00.358457
elapsed time: 0:00:29.327684
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-10-01 14:39:07.453735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.34
 ---- batch: 020 ----
mean loss: 462.73
train mean loss: 469.42
epoch train time: 0:00:00.365341
elapsed time: 0:00:29.693229
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-10-01 14:39:07.819261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.07
 ---- batch: 020 ----
mean loss: 454.47
train mean loss: 455.44
epoch train time: 0:00:00.347443
elapsed time: 0:00:30.040866
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-10-01 14:39:08.166917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.24
 ---- batch: 020 ----
mean loss: 444.66
train mean loss: 444.67
epoch train time: 0:00:00.348426
elapsed time: 0:00:30.389491
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-10-01 14:39:08.515518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.60
 ---- batch: 020 ----
mean loss: 426.11
train mean loss: 429.05
epoch train time: 0:00:00.351661
elapsed time: 0:00:30.741332
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-10-01 14:39:08.867378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.01
 ---- batch: 020 ----
mean loss: 414.19
train mean loss: 413.72
epoch train time: 0:00:00.348728
elapsed time: 0:00:31.090254
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-10-01 14:39:09.216285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.33
 ---- batch: 020 ----
mean loss: 398.62
train mean loss: 403.44
epoch train time: 0:00:00.351221
elapsed time: 0:00:31.441651
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-10-01 14:39:09.567680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.23
 ---- batch: 020 ----
mean loss: 388.46
train mean loss: 390.54
epoch train time: 0:00:00.373113
elapsed time: 0:00:31.814957
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-10-01 14:39:09.941009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.80
 ---- batch: 020 ----
mean loss: 371.45
train mean loss: 377.78
epoch train time: 0:00:00.354453
elapsed time: 0:00:32.169629
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-10-01 14:39:10.295656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.83
 ---- batch: 020 ----
mean loss: 363.08
train mean loss: 367.24
epoch train time: 0:00:00.356385
elapsed time: 0:00:32.526196
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-10-01 14:39:10.652229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.13
 ---- batch: 020 ----
mean loss: 353.31
train mean loss: 354.06
epoch train time: 0:00:00.361126
elapsed time: 0:00:32.887512
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-10-01 14:39:11.013550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.37
 ---- batch: 020 ----
mean loss: 337.62
train mean loss: 343.62
epoch train time: 0:00:00.353232
elapsed time: 0:00:33.240989
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-10-01 14:39:11.367074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.42
 ---- batch: 020 ----
mean loss: 327.55
train mean loss: 327.94
epoch train time: 0:00:00.376334
elapsed time: 0:00:33.617639
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-10-01 14:39:11.743667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.98
 ---- batch: 020 ----
mean loss: 319.01
train mean loss: 318.95
epoch train time: 0:00:00.355452
elapsed time: 0:00:33.973355
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-10-01 14:39:12.099427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.14
 ---- batch: 020 ----
mean loss: 304.87
train mean loss: 308.98
epoch train time: 0:00:00.340639
elapsed time: 0:00:34.314232
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-10-01 14:39:12.440331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.30
 ---- batch: 020 ----
mean loss: 298.07
train mean loss: 299.04
epoch train time: 0:00:00.356968
elapsed time: 0:00:34.671465
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-10-01 14:39:12.797501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.62
 ---- batch: 020 ----
mean loss: 288.11
train mean loss: 287.14
epoch train time: 0:00:00.358824
elapsed time: 0:00:35.030486
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-10-01 14:39:13.156515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.84
 ---- batch: 020 ----
mean loss: 276.24
train mean loss: 276.94
epoch train time: 0:00:00.354839
elapsed time: 0:00:35.385521
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-10-01 14:39:13.511553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.68
 ---- batch: 020 ----
mean loss: 263.40
train mean loss: 266.14
epoch train time: 0:00:00.362413
elapsed time: 0:00:35.748120
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-10-01 14:39:13.874197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.78
 ---- batch: 020 ----
mean loss: 260.45
train mean loss: 258.91
epoch train time: 0:00:00.348166
elapsed time: 0:00:36.096550
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-10-01 14:39:14.222579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.36
 ---- batch: 020 ----
mean loss: 253.56
train mean loss: 254.58
epoch train time: 0:00:00.351782
elapsed time: 0:00:36.448514
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-10-01 14:39:14.574546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.32
 ---- batch: 020 ----
mean loss: 244.45
train mean loss: 243.85
epoch train time: 0:00:00.365210
elapsed time: 0:00:36.813925
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-10-01 14:39:14.939955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.17
 ---- batch: 020 ----
mean loss: 233.43
train mean loss: 235.99
epoch train time: 0:00:00.348703
elapsed time: 0:00:37.162807
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-10-01 14:39:15.288885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.46
 ---- batch: 020 ----
mean loss: 226.85
train mean loss: 227.37
epoch train time: 0:00:00.349099
elapsed time: 0:00:37.512147
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-10-01 14:39:15.638174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.17
 ---- batch: 020 ----
mean loss: 224.98
train mean loss: 224.45
epoch train time: 0:00:00.354224
elapsed time: 0:00:37.867276
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-10-01 14:39:15.993325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.99
 ---- batch: 020 ----
mean loss: 213.92
train mean loss: 213.81
epoch train time: 0:00:00.347717
elapsed time: 0:00:38.215195
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-10-01 14:39:16.341260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.03
 ---- batch: 020 ----
mean loss: 206.88
train mean loss: 209.52
epoch train time: 0:00:00.350215
elapsed time: 0:00:38.565638
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-10-01 14:39:16.691669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.26
 ---- batch: 020 ----
mean loss: 201.87
train mean loss: 205.08
epoch train time: 0:00:00.358392
elapsed time: 0:00:38.924220
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-10-01 14:39:17.050256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.04
 ---- batch: 020 ----
mean loss: 197.09
train mean loss: 198.01
epoch train time: 0:00:00.354192
elapsed time: 0:00:39.278627
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-10-01 14:39:17.404677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.98
 ---- batch: 020 ----
mean loss: 191.23
train mean loss: 194.09
epoch train time: 0:00:00.369377
elapsed time: 0:00:39.648202
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-10-01 14:39:17.774233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.07
 ---- batch: 020 ----
mean loss: 187.45
train mean loss: 186.67
epoch train time: 0:00:00.350908
elapsed time: 0:00:39.999291
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-10-01 14:39:18.125323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.70
 ---- batch: 020 ----
mean loss: 181.43
train mean loss: 181.74
epoch train time: 0:00:00.349144
elapsed time: 0:00:40.348618
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-10-01 14:39:18.474649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.07
 ---- batch: 020 ----
mean loss: 172.74
train mean loss: 175.13
epoch train time: 0:00:00.357807
elapsed time: 0:00:40.706605
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-10-01 14:39:18.832634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.54
 ---- batch: 020 ----
mean loss: 170.14
train mean loss: 171.09
epoch train time: 0:00:00.342807
elapsed time: 0:00:41.049650
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-10-01 14:39:19.175677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.63
 ---- batch: 020 ----
mean loss: 173.04
train mean loss: 172.73
epoch train time: 0:00:00.337244
elapsed time: 0:00:41.387085
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-10-01 14:39:19.513132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.68
 ---- batch: 020 ----
mean loss: 166.08
train mean loss: 165.09
epoch train time: 0:00:00.350696
elapsed time: 0:00:41.737994
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-10-01 14:39:19.864034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.43
 ---- batch: 020 ----
mean loss: 161.51
train mean loss: 160.43
epoch train time: 0:00:00.350605
elapsed time: 0:00:42.088787
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-10-01 14:39:20.214833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.46
 ---- batch: 020 ----
mean loss: 153.69
train mean loss: 155.46
epoch train time: 0:00:00.357011
elapsed time: 0:00:42.446007
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-10-01 14:39:20.572040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.80
 ---- batch: 020 ----
mean loss: 152.80
train mean loss: 153.37
epoch train time: 0:00:00.370819
elapsed time: 0:00:42.817021
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-10-01 14:39:20.943050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.92
 ---- batch: 020 ----
mean loss: 148.71
train mean loss: 150.13
epoch train time: 0:00:00.352969
elapsed time: 0:00:43.170196
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-10-01 14:39:21.296227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.64
 ---- batch: 020 ----
mean loss: 144.64
train mean loss: 147.13
epoch train time: 0:00:00.355003
elapsed time: 0:00:43.525395
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-10-01 14:39:21.651412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.69
 ---- batch: 020 ----
mean loss: 146.66
train mean loss: 144.84
epoch train time: 0:00:00.351562
elapsed time: 0:00:43.877121
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-10-01 14:39:22.003158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.08
 ---- batch: 020 ----
mean loss: 141.29
train mean loss: 140.66
epoch train time: 0:00:00.340545
elapsed time: 0:00:44.217847
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-10-01 14:39:22.343906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.78
 ---- batch: 020 ----
mean loss: 141.65
train mean loss: 138.36
epoch train time: 0:00:00.347114
elapsed time: 0:00:44.565169
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-10-01 14:39:22.691198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.46
 ---- batch: 020 ----
mean loss: 137.26
train mean loss: 134.92
epoch train time: 0:00:00.358169
elapsed time: 0:00:44.923520
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-10-01 14:39:23.049578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.30
 ---- batch: 020 ----
mean loss: 134.81
train mean loss: 133.31
epoch train time: 0:00:00.356067
elapsed time: 0:00:45.279842
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-10-01 14:39:23.405849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.46
 ---- batch: 020 ----
mean loss: 133.09
train mean loss: 130.53
epoch train time: 0:00:00.352174
elapsed time: 0:00:45.632174
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-10-01 14:39:23.758205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.67
 ---- batch: 020 ----
mean loss: 128.88
train mean loss: 130.12
epoch train time: 0:00:00.350101
elapsed time: 0:00:45.982463
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-10-01 14:39:24.108492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.26
 ---- batch: 020 ----
mean loss: 125.92
train mean loss: 126.67
epoch train time: 0:00:00.349900
elapsed time: 0:00:46.332577
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-10-01 14:39:24.458639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.84
 ---- batch: 020 ----
mean loss: 120.81
train mean loss: 123.15
epoch train time: 0:00:00.362037
elapsed time: 0:00:46.694865
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-10-01 14:39:24.820910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.93
 ---- batch: 020 ----
mean loss: 118.25
train mean loss: 122.31
epoch train time: 0:00:00.359157
elapsed time: 0:00:47.054261
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-10-01 14:39:25.180328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.30
 ---- batch: 020 ----
mean loss: 116.69
train mean loss: 117.43
epoch train time: 0:00:00.350539
elapsed time: 0:00:47.405018
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-10-01 14:39:25.531048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.98
 ---- batch: 020 ----
mean loss: 119.78
train mean loss: 116.87
epoch train time: 0:00:00.353557
elapsed time: 0:00:47.758752
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-10-01 14:39:25.884775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.38
 ---- batch: 020 ----
mean loss: 113.55
train mean loss: 116.29
epoch train time: 0:00:00.354987
elapsed time: 0:00:48.113933
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-10-01 14:39:26.239958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.93
 ---- batch: 020 ----
mean loss: 117.19
train mean loss: 113.71
epoch train time: 0:00:00.350298
elapsed time: 0:00:48.464424
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-10-01 14:39:26.590450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.23
 ---- batch: 020 ----
mean loss: 107.43
train mean loss: 113.17
epoch train time: 0:00:00.364727
elapsed time: 0:00:48.829334
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-10-01 14:39:26.955372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.17
 ---- batch: 020 ----
mean loss: 111.30
train mean loss: 109.84
epoch train time: 0:00:00.353811
elapsed time: 0:00:49.183331
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-10-01 14:39:27.309397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.55
 ---- batch: 020 ----
mean loss: 108.04
train mean loss: 108.50
epoch train time: 0:00:00.351935
elapsed time: 0:00:49.535486
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-10-01 14:39:27.661513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.54
 ---- batch: 020 ----
mean loss: 111.21
train mean loss: 109.65
epoch train time: 0:00:00.379575
elapsed time: 0:00:49.915299
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-10-01 14:39:28.041345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.72
 ---- batch: 020 ----
mean loss: 108.13
train mean loss: 106.10
epoch train time: 0:00:00.355102
elapsed time: 0:00:50.270596
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-10-01 14:39:28.396648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.72
 ---- batch: 020 ----
mean loss: 105.30
train mean loss: 104.96
epoch train time: 0:00:00.351482
elapsed time: 0:00:50.622307
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-10-01 14:39:28.748341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.62
 ---- batch: 020 ----
mean loss: 105.61
train mean loss: 105.29
epoch train time: 0:00:00.357108
elapsed time: 0:00:50.979609
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-10-01 14:39:29.105639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.29
 ---- batch: 020 ----
mean loss: 100.30
train mean loss: 102.13
epoch train time: 0:00:00.353864
elapsed time: 0:00:51.333653
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-10-01 14:39:29.459677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.01
 ---- batch: 020 ----
mean loss: 102.14
train mean loss: 101.82
epoch train time: 0:00:00.349388
elapsed time: 0:00:51.683213
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-10-01 14:39:29.809237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.46
 ---- batch: 020 ----
mean loss: 99.89
train mean loss: 99.46
epoch train time: 0:00:00.351313
elapsed time: 0:00:52.034710
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-10-01 14:39:30.160750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.36
 ---- batch: 020 ----
mean loss: 97.21
train mean loss: 97.94
epoch train time: 0:00:00.352963
elapsed time: 0:00:52.387910
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-10-01 14:39:30.513933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.07
 ---- batch: 020 ----
mean loss: 95.31
train mean loss: 97.54
epoch train time: 0:00:00.358052
elapsed time: 0:00:52.746167
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-10-01 14:39:30.872196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.05
 ---- batch: 020 ----
mean loss: 94.11
train mean loss: 96.66
epoch train time: 0:00:00.346873
elapsed time: 0:00:53.093676
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-10-01 14:39:31.219721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.53
 ---- batch: 020 ----
mean loss: 98.25
train mean loss: 95.98
epoch train time: 0:00:00.346741
elapsed time: 0:00:53.440625
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-10-01 14:39:31.566651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.13
 ---- batch: 020 ----
mean loss: 95.06
train mean loss: 93.95
epoch train time: 0:00:00.360977
elapsed time: 0:00:53.801791
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-10-01 14:39:31.927821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.23
 ---- batch: 020 ----
mean loss: 91.85
train mean loss: 92.55
epoch train time: 0:00:00.350539
elapsed time: 0:00:54.152511
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-10-01 14:39:32.278539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.33
 ---- batch: 020 ----
mean loss: 94.67
train mean loss: 91.59
epoch train time: 0:00:00.357950
elapsed time: 0:00:54.510694
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-10-01 14:39:32.636723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.59
 ---- batch: 020 ----
mean loss: 92.44
train mean loss: 91.42
epoch train time: 0:00:00.357281
elapsed time: 0:00:54.868176
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-10-01 14:39:32.994207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.26
 ---- batch: 020 ----
mean loss: 89.70
train mean loss: 90.36
epoch train time: 0:00:00.358297
elapsed time: 0:00:55.226692
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-10-01 14:39:33.352744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.87
 ---- batch: 020 ----
mean loss: 92.90
train mean loss: 87.71
epoch train time: 0:00:00.357686
elapsed time: 0:00:55.584592
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-10-01 14:39:33.710628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.77
 ---- batch: 020 ----
mean loss: 86.89
train mean loss: 88.68
epoch train time: 0:00:00.365084
elapsed time: 0:00:55.949884
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-10-01 14:39:34.075933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.13
 ---- batch: 020 ----
mean loss: 90.22
train mean loss: 88.69
epoch train time: 0:00:00.357264
elapsed time: 0:00:56.307362
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-10-01 14:39:34.433389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.24
 ---- batch: 020 ----
mean loss: 86.62
train mean loss: 88.05
epoch train time: 0:00:00.357526
elapsed time: 0:00:56.665076
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-10-01 14:39:34.791104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.84
 ---- batch: 020 ----
mean loss: 85.58
train mean loss: 85.52
epoch train time: 0:00:00.341239
elapsed time: 0:00:57.006488
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-10-01 14:39:35.132517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.10
 ---- batch: 020 ----
mean loss: 90.12
train mean loss: 86.63
epoch train time: 0:00:00.340052
elapsed time: 0:00:57.346734
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-10-01 14:39:35.472781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.11
 ---- batch: 020 ----
mean loss: 82.43
train mean loss: 85.96
epoch train time: 0:00:00.350237
elapsed time: 0:00:57.697171
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-10-01 14:39:35.823220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.48
 ---- batch: 020 ----
mean loss: 81.80
train mean loss: 84.84
epoch train time: 0:00:00.351445
elapsed time: 0:00:58.048851
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-10-01 14:39:36.174885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.03
 ---- batch: 020 ----
mean loss: 80.42
train mean loss: 82.72
epoch train time: 0:00:00.355645
elapsed time: 0:00:58.404684
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-10-01 14:39:36.530719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.37
 ---- batch: 020 ----
mean loss: 86.08
train mean loss: 83.50
epoch train time: 0:00:00.366529
elapsed time: 0:00:58.771406
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-10-01 14:39:36.897438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.88
 ---- batch: 020 ----
mean loss: 81.05
train mean loss: 82.39
epoch train time: 0:00:00.348195
elapsed time: 0:00:59.119780
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-10-01 14:39:37.245840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.13
 ---- batch: 020 ----
mean loss: 78.56
train mean loss: 80.04
epoch train time: 0:00:00.355124
elapsed time: 0:00:59.475136
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-10-01 14:39:37.601168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.69
 ---- batch: 020 ----
mean loss: 76.96
train mean loss: 80.38
epoch train time: 0:00:00.356214
elapsed time: 0:00:59.831532
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-10-01 14:39:37.957563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.44
 ---- batch: 020 ----
mean loss: 78.94
train mean loss: 80.66
epoch train time: 0:00:00.345269
elapsed time: 0:01:00.177026
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-10-01 14:39:38.303044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.10
 ---- batch: 020 ----
mean loss: 78.62
train mean loss: 79.86
epoch train time: 0:00:00.349460
elapsed time: 0:01:00.526670
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-10-01 14:39:38.652702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.46
 ---- batch: 020 ----
mean loss: 78.94
train mean loss: 79.29
epoch train time: 0:00:00.355226
elapsed time: 0:01:00.882075
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-10-01 14:39:39.008118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.94
 ---- batch: 020 ----
mean loss: 78.41
train mean loss: 78.51
epoch train time: 0:00:00.341058
elapsed time: 0:01:01.223339
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-10-01 14:39:39.349371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.58
 ---- batch: 020 ----
mean loss: 78.88
train mean loss: 77.82
epoch train time: 0:00:00.347659
elapsed time: 0:01:01.571227
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-10-01 14:39:39.697319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.41
 ---- batch: 020 ----
mean loss: 75.30
train mean loss: 76.30
epoch train time: 0:00:00.355094
elapsed time: 0:01:01.926610
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-10-01 14:39:40.052638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.38
 ---- batch: 020 ----
mean loss: 75.30
train mean loss: 76.83
epoch train time: 0:00:00.352094
elapsed time: 0:01:02.278878
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-10-01 14:39:40.404951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.79
 ---- batch: 020 ----
mean loss: 77.42
train mean loss: 76.74
epoch train time: 0:00:00.360439
elapsed time: 0:01:02.639547
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-10-01 14:39:40.765586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.06
 ---- batch: 020 ----
mean loss: 75.13
train mean loss: 75.15
epoch train time: 0:00:00.361646
elapsed time: 0:01:03.001459
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-10-01 14:39:41.127491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.18
 ---- batch: 020 ----
mean loss: 72.11
train mean loss: 74.38
epoch train time: 0:00:00.351558
elapsed time: 0:01:03.353198
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-10-01 14:39:41.479248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.95
 ---- batch: 020 ----
mean loss: 72.88
train mean loss: 74.78
epoch train time: 0:00:00.354640
elapsed time: 0:01:03.708076
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-10-01 14:39:41.834105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.00
 ---- batch: 020 ----
mean loss: 74.46
train mean loss: 74.71
epoch train time: 0:00:00.356492
elapsed time: 0:01:04.064745
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-10-01 14:39:42.190774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.62
 ---- batch: 020 ----
mean loss: 73.29
train mean loss: 73.46
epoch train time: 0:00:00.353853
elapsed time: 0:01:04.418779
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-10-01 14:39:42.544811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.38
 ---- batch: 020 ----
mean loss: 74.10
train mean loss: 73.41
epoch train time: 0:00:00.376606
elapsed time: 0:01:04.795563
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-10-01 14:39:42.921591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.41
 ---- batch: 020 ----
mean loss: 75.01
train mean loss: 72.22
epoch train time: 0:00:00.350265
elapsed time: 0:01:05.146014
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-10-01 14:39:43.272068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.41
 ---- batch: 020 ----
mean loss: 70.25
train mean loss: 71.58
epoch train time: 0:00:00.352308
elapsed time: 0:01:05.498543
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-10-01 14:39:43.624576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.35
 ---- batch: 020 ----
mean loss: 73.26
train mean loss: 72.49
epoch train time: 0:00:00.352668
elapsed time: 0:01:05.851404
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-10-01 14:39:43.977448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.51
 ---- batch: 020 ----
mean loss: 70.27
train mean loss: 71.94
epoch train time: 0:00:00.339399
elapsed time: 0:01:06.191019
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-10-01 14:39:44.317064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.77
 ---- batch: 020 ----
mean loss: 67.69
train mean loss: 71.26
epoch train time: 0:00:00.343722
elapsed time: 0:01:06.534978
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-10-01 14:39:44.661029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.76
 ---- batch: 020 ----
mean loss: 68.72
train mean loss: 71.28
epoch train time: 0:00:00.377243
elapsed time: 0:01:06.912426
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-10-01 14:39:45.038456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.10
 ---- batch: 020 ----
mean loss: 70.79
train mean loss: 70.31
epoch train time: 0:00:00.349871
elapsed time: 0:01:07.262472
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-10-01 14:39:45.388500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.72
 ---- batch: 020 ----
mean loss: 70.48
train mean loss: 70.59
epoch train time: 0:00:00.347565
elapsed time: 0:01:07.610265
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-10-01 14:39:45.736343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.09
 ---- batch: 020 ----
mean loss: 70.87
train mean loss: 70.33
epoch train time: 0:00:00.344358
elapsed time: 0:01:07.954923
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-10-01 14:39:46.080968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.24
 ---- batch: 020 ----
mean loss: 70.69
train mean loss: 69.20
epoch train time: 0:00:00.338907
elapsed time: 0:01:08.294013
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-10-01 14:39:46.420035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.10
 ---- batch: 020 ----
mean loss: 69.85
train mean loss: 68.80
epoch train time: 0:00:00.365702
elapsed time: 0:01:08.659922
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-10-01 14:39:46.785953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.69
 ---- batch: 020 ----
mean loss: 69.07
train mean loss: 68.39
epoch train time: 0:00:00.357438
elapsed time: 0:01:09.017597
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-10-01 14:39:47.143616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.51
 ---- batch: 020 ----
mean loss: 69.35
train mean loss: 68.34
epoch train time: 0:00:00.349395
elapsed time: 0:01:09.367235
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-10-01 14:39:47.493279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.72
 ---- batch: 020 ----
mean loss: 69.67
train mean loss: 69.08
epoch train time: 0:00:00.351751
elapsed time: 0:01:09.719179
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-10-01 14:39:47.845203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.95
 ---- batch: 020 ----
mean loss: 65.91
train mean loss: 67.22
epoch train time: 0:00:00.339058
elapsed time: 0:01:10.058404
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-10-01 14:39:48.184429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.28
 ---- batch: 020 ----
mean loss: 68.46
train mean loss: 66.12
epoch train time: 0:00:00.342614
elapsed time: 0:01:10.401196
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-10-01 14:39:48.527236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.14
 ---- batch: 020 ----
mean loss: 68.57
train mean loss: 66.89
epoch train time: 0:00:00.342831
elapsed time: 0:01:10.744217
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-10-01 14:39:48.870246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.18
 ---- batch: 020 ----
mean loss: 66.99
train mean loss: 66.11
epoch train time: 0:00:00.339118
elapsed time: 0:01:11.083509
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-10-01 14:39:49.209537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.52
 ---- batch: 020 ----
mean loss: 66.80
train mean loss: 65.59
epoch train time: 0:00:00.338256
elapsed time: 0:01:11.421936
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-10-01 14:39:49.547988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.84
 ---- batch: 020 ----
mean loss: 64.40
train mean loss: 65.49
epoch train time: 0:00:00.355816
elapsed time: 0:01:11.777964
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-10-01 14:39:49.904002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.27
 ---- batch: 020 ----
mean loss: 67.68
train mean loss: 66.66
epoch train time: 0:00:00.368551
elapsed time: 0:01:12.146733
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-10-01 14:39:50.272770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.58
 ---- batch: 020 ----
mean loss: 63.59
train mean loss: 64.89
epoch train time: 0:00:00.353435
elapsed time: 0:01:12.500355
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-10-01 14:39:50.626387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.78
 ---- batch: 020 ----
mean loss: 65.38
train mean loss: 66.31
epoch train time: 0:00:00.352152
elapsed time: 0:01:12.852680
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-10-01 14:39:50.978704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.22
 ---- batch: 020 ----
mean loss: 64.61
train mean loss: 64.03
epoch train time: 0:00:00.339742
elapsed time: 0:01:13.192588
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-10-01 14:39:51.318611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.26
 ---- batch: 020 ----
mean loss: 66.21
train mean loss: 64.66
epoch train time: 0:00:00.340994
elapsed time: 0:01:13.533760
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-10-01 14:39:51.659797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.46
 ---- batch: 020 ----
mean loss: 63.77
train mean loss: 63.70
epoch train time: 0:00:00.360701
elapsed time: 0:01:13.894645
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-10-01 14:39:52.020679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.25
 ---- batch: 020 ----
mean loss: 64.24
train mean loss: 64.61
epoch train time: 0:00:00.336798
elapsed time: 0:01:14.231621
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-10-01 14:39:52.357647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.15
 ---- batch: 020 ----
mean loss: 61.49
train mean loss: 62.05
epoch train time: 0:00:00.335387
elapsed time: 0:01:14.567307
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-10-01 14:39:52.693383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.94
 ---- batch: 020 ----
mean loss: 64.49
train mean loss: 63.65
epoch train time: 0:00:00.372025
elapsed time: 0:01:14.939601
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-10-01 14:39:53.065643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.04
 ---- batch: 020 ----
mean loss: 63.31
train mean loss: 64.22
epoch train time: 0:00:00.347188
elapsed time: 0:01:15.287003
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-10-01 14:39:53.413098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.25
 ---- batch: 020 ----
mean loss: 62.64
train mean loss: 63.79
epoch train time: 0:00:00.350578
elapsed time: 0:01:15.637866
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-10-01 14:39:53.763893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.73
 ---- batch: 020 ----
mean loss: 62.38
train mean loss: 62.10
epoch train time: 0:00:00.354091
elapsed time: 0:01:15.992138
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-10-01 14:39:54.118170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.10
 ---- batch: 020 ----
mean loss: 63.37
train mean loss: 61.71
epoch train time: 0:00:00.353891
elapsed time: 0:01:16.346226
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-10-01 14:39:54.472272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.49
 ---- batch: 020 ----
mean loss: 62.25
train mean loss: 62.18
epoch train time: 0:00:00.366119
elapsed time: 0:01:16.712543
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-10-01 14:39:54.838574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.31
 ---- batch: 020 ----
mean loss: 63.08
train mean loss: 61.56
epoch train time: 0:00:00.345462
elapsed time: 0:01:17.058200
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-10-01 14:39:55.184229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.30
 ---- batch: 020 ----
mean loss: 60.98
train mean loss: 60.82
epoch train time: 0:00:00.343561
elapsed time: 0:01:17.401955
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-10-01 14:39:55.527986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.21
 ---- batch: 020 ----
mean loss: 61.04
train mean loss: 60.86
epoch train time: 0:00:00.355545
elapsed time: 0:01:17.757685
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-10-01 14:39:55.883719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.87
 ---- batch: 020 ----
mean loss: 62.13
train mean loss: 61.08
epoch train time: 0:00:00.355014
elapsed time: 0:01:18.112893
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-10-01 14:39:56.238940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.54
 ---- batch: 020 ----
mean loss: 62.18
train mean loss: 60.85
epoch train time: 0:00:00.348488
elapsed time: 0:01:18.461608
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-10-01 14:39:56.587639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.62
 ---- batch: 020 ----
mean loss: 58.65
train mean loss: 59.73
epoch train time: 0:00:00.357950
elapsed time: 0:01:18.819809
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-10-01 14:39:56.945830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.95
 ---- batch: 020 ----
mean loss: 61.70
train mean loss: 58.70
epoch train time: 0:00:00.351909
elapsed time: 0:01:19.171909
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-10-01 14:39:57.297939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.92
 ---- batch: 020 ----
mean loss: 59.29
train mean loss: 59.07
epoch train time: 0:00:00.350409
elapsed time: 0:01:19.522535
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-10-01 14:39:57.648583
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.23
 ---- batch: 020 ----
mean loss: 59.11
train mean loss: 58.53
epoch train time: 0:00:00.349109
elapsed time: 0:01:19.871864
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-10-01 14:39:57.997926
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.60
 ---- batch: 020 ----
mean loss: 59.28
train mean loss: 59.53
epoch train time: 0:00:00.341798
elapsed time: 0:01:20.213886
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-10-01 14:39:58.339914
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.65
 ---- batch: 020 ----
mean loss: 61.85
train mean loss: 60.31
epoch train time: 0:00:00.343390
elapsed time: 0:01:20.557499
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-10-01 14:39:58.683546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.79
 ---- batch: 020 ----
mean loss: 59.72
train mean loss: 59.45
epoch train time: 0:00:00.356994
elapsed time: 0:01:20.914737
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-10-01 14:39:59.040794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.49
 ---- batch: 020 ----
mean loss: 59.06
train mean loss: 59.14
epoch train time: 0:00:00.349877
elapsed time: 0:01:21.264827
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-10-01 14:39:59.390856
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.12
 ---- batch: 020 ----
mean loss: 58.29
train mean loss: 59.45
epoch train time: 0:00:00.357819
elapsed time: 0:01:21.622835
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-10-01 14:39:59.748892
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.89
 ---- batch: 020 ----
mean loss: 61.01
train mean loss: 59.17
epoch train time: 0:00:00.359179
elapsed time: 0:01:21.982232
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-10-01 14:40:00.108277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.31
 ---- batch: 020 ----
mean loss: 56.88
train mean loss: 56.91
epoch train time: 0:00:00.352085
elapsed time: 0:01:22.334506
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-10-01 14:40:00.460534
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.77
 ---- batch: 020 ----
mean loss: 58.38
train mean loss: 59.52
epoch train time: 0:00:00.370433
elapsed time: 0:01:22.705109
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-10-01 14:40:00.831134
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.90
 ---- batch: 020 ----
mean loss: 60.76
train mean loss: 58.36
epoch train time: 0:00:00.343950
elapsed time: 0:01:23.049253
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-10-01 14:40:01.175286
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.19
 ---- batch: 020 ----
mean loss: 59.04
train mean loss: 58.22
epoch train time: 0:00:00.348007
elapsed time: 0:01:23.397459
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-10-01 14:40:01.523500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.66
 ---- batch: 020 ----
mean loss: 58.70
train mean loss: 58.65
epoch train time: 0:00:00.367195
elapsed time: 0:01:23.764853
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-10-01 14:40:01.890907
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.17
 ---- batch: 020 ----
mean loss: 60.14
train mean loss: 58.87
epoch train time: 0:00:00.343355
elapsed time: 0:01:24.108409
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-10-01 14:40:02.234436
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.20
 ---- batch: 020 ----
mean loss: 59.57
train mean loss: 58.36
epoch train time: 0:00:00.343816
elapsed time: 0:01:24.452401
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-10-01 14:40:02.578427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.27
 ---- batch: 020 ----
mean loss: 58.55
train mean loss: 58.77
epoch train time: 0:00:00.356634
elapsed time: 0:01:24.809211
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-10-01 14:40:02.935238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.99
 ---- batch: 020 ----
mean loss: 59.12
train mean loss: 58.25
epoch train time: 0:00:00.352891
elapsed time: 0:01:25.162346
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-10-01 14:40:03.288376
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.86
 ---- batch: 020 ----
mean loss: 56.51
train mean loss: 57.71
epoch train time: 0:00:00.355055
elapsed time: 0:01:25.517587
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-10-01 14:40:03.643635
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.07
 ---- batch: 020 ----
mean loss: 58.47
train mean loss: 59.04
epoch train time: 0:00:00.352190
elapsed time: 0:01:25.869976
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-10-01 14:40:03.996001
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.77
 ---- batch: 020 ----
mean loss: 58.91
train mean loss: 59.89
epoch train time: 0:00:00.347414
elapsed time: 0:01:26.217575
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-10-01 14:40:04.343619
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.39
 ---- batch: 020 ----
mean loss: 58.34
train mean loss: 58.40
epoch train time: 0:00:00.346655
elapsed time: 0:01:26.564426
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-10-01 14:40:04.690450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.41
 ---- batch: 020 ----
mean loss: 58.78
train mean loss: 59.73
epoch train time: 0:00:00.364854
elapsed time: 0:01:26.929520
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-10-01 14:40:05.055550
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.49
 ---- batch: 020 ----
mean loss: 60.96
train mean loss: 58.92
epoch train time: 0:00:00.358927
elapsed time: 0:01:27.288631
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-10-01 14:40:05.414658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.20
 ---- batch: 020 ----
mean loss: 60.08
train mean loss: 58.85
epoch train time: 0:00:00.353854
elapsed time: 0:01:27.642659
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-10-01 14:40:05.768684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.25
 ---- batch: 020 ----
mean loss: 57.23
train mean loss: 57.95
epoch train time: 0:00:00.349481
elapsed time: 0:01:27.992313
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-10-01 14:40:06.118343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.40
 ---- batch: 020 ----
mean loss: 61.03
train mean loss: 59.74
epoch train time: 0:00:00.349852
elapsed time: 0:01:28.342347
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-10-01 14:40:06.468374
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.70
 ---- batch: 020 ----
mean loss: 60.55
train mean loss: 58.86
epoch train time: 0:00:00.368576
elapsed time: 0:01:28.711171
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-10-01 14:40:06.837212
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.99
 ---- batch: 020 ----
mean loss: 58.20
train mean loss: 57.65
epoch train time: 0:00:00.345632
elapsed time: 0:01:29.057010
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-10-01 14:40:07.183036
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.28
 ---- batch: 020 ----
mean loss: 60.22
train mean loss: 57.91
epoch train time: 0:00:00.343436
elapsed time: 0:01:29.400620
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-10-01 14:40:07.526648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.30
 ---- batch: 020 ----
mean loss: 58.58
train mean loss: 59.65
epoch train time: 0:00:00.345914
elapsed time: 0:01:29.746711
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-10-01 14:40:07.872745
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.25
 ---- batch: 020 ----
mean loss: 56.71
train mean loss: 58.51
epoch train time: 0:00:00.341682
elapsed time: 0:01:30.088638
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-10-01 14:40:08.214646
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.25
 ---- batch: 020 ----
mean loss: 60.37
train mean loss: 59.00
epoch train time: 0:00:00.343877
elapsed time: 0:01:30.432666
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-10-01 14:40:08.558691
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.87
 ---- batch: 020 ----
mean loss: 57.22
train mean loss: 58.05
epoch train time: 0:00:00.373354
elapsed time: 0:01:30.806228
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-10-01 14:40:08.932293
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.25
 ---- batch: 020 ----
mean loss: 59.20
train mean loss: 58.64
epoch train time: 0:00:00.342384
elapsed time: 0:01:31.148820
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-10-01 14:40:09.274876
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.16
 ---- batch: 020 ----
mean loss: 59.05
train mean loss: 58.52
epoch train time: 0:00:00.344862
elapsed time: 0:01:31.493892
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-10-01 14:40:09.619939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.61
 ---- batch: 020 ----
mean loss: 55.83
train mean loss: 58.27
epoch train time: 0:00:00.351105
elapsed time: 0:01:31.845199
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-10-01 14:40:09.971233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.03
 ---- batch: 020 ----
mean loss: 59.10
train mean loss: 57.67
epoch train time: 0:00:00.347450
elapsed time: 0:01:32.192842
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-10-01 14:40:10.318869
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.63
 ---- batch: 020 ----
mean loss: 57.22
train mean loss: 59.11
epoch train time: 0:00:00.353599
elapsed time: 0:01:32.546622
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-10-01 14:40:10.672662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.01
 ---- batch: 020 ----
mean loss: 59.79
train mean loss: 58.92
epoch train time: 0:00:00.372539
elapsed time: 0:01:32.919365
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-10-01 14:40:11.045397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.35
 ---- batch: 020 ----
mean loss: 60.21
train mean loss: 58.35
epoch train time: 0:00:00.348520
elapsed time: 0:01:33.268070
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-10-01 14:40:11.394102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.28
 ---- batch: 020 ----
mean loss: 58.22
train mean loss: 57.90
epoch train time: 0:00:00.353243
elapsed time: 0:01:33.621494
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-10-01 14:40:11.747542
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.87
 ---- batch: 020 ----
mean loss: 60.00
train mean loss: 57.67
epoch train time: 0:00:00.357693
elapsed time: 0:01:33.979413
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-10-01 14:40:12.105462
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.08
 ---- batch: 020 ----
mean loss: 60.30
train mean loss: 58.15
epoch train time: 0:00:00.347562
elapsed time: 0:01:34.327217
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-10-01 14:40:12.453258
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.14
 ---- batch: 020 ----
mean loss: 58.16
train mean loss: 58.79
epoch train time: 0:00:00.357372
elapsed time: 0:01:34.684805
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-10-01 14:40:12.810834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.56
 ---- batch: 020 ----
mean loss: 58.17
train mean loss: 58.74
epoch train time: 0:00:00.349341
elapsed time: 0:01:35.034323
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-10-01 14:40:13.160367
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.93
 ---- batch: 020 ----
mean loss: 57.43
train mean loss: 56.66
epoch train time: 0:00:00.346526
elapsed time: 0:01:35.381046
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-10-01 14:40:13.507074
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.63
 ---- batch: 020 ----
mean loss: 58.61
train mean loss: 58.15
epoch train time: 0:00:00.354732
elapsed time: 0:01:35.743554
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_9/checkpoint.pth.tar
**** end time: 2019-10-01 14:40:13.869538 ****
