Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_4', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18893
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-25 18:10:57.205176 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 16, 11]           1,120
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
    BayesianConv2d-4            [-1, 14, 7, 11]             448
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
    BayesianLinear-8                    [-1, 1]             924
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,492
Trainable params: 2,492
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 18:10:57.216441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4066.36
 ---- batch: 020 ----
mean loss: 3817.05
 ---- batch: 030 ----
mean loss: 3680.55
 ---- batch: 040 ----
mean loss: 3435.50
 ---- batch: 050 ----
mean loss: 3164.93
 ---- batch: 060 ----
mean loss: 3027.12
 ---- batch: 070 ----
mean loss: 2764.61
 ---- batch: 080 ----
mean loss: 2595.22
 ---- batch: 090 ----
mean loss: 2388.57
train mean loss: 3152.39
epoch train time: 0:00:35.987465
elapsed time: 0:00:36.001895
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 18:11:33.207111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2036.77
 ---- batch: 020 ----
mean loss: 1911.47
 ---- batch: 030 ----
mean loss: 1738.22
 ---- batch: 040 ----
mean loss: 1591.10
 ---- batch: 050 ----
mean loss: 1424.86
 ---- batch: 060 ----
mean loss: 1331.27
 ---- batch: 070 ----
mean loss: 1225.87
 ---- batch: 080 ----
mean loss: 1154.99
 ---- batch: 090 ----
mean loss: 1106.95
train mean loss: 1473.83
epoch train time: 0:00:02.571247
elapsed time: 0:00:38.573528
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 18:11:35.778873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1018.56
 ---- batch: 020 ----
mean loss: 965.44
 ---- batch: 030 ----
mean loss: 962.02
 ---- batch: 040 ----
mean loss: 950.37
 ---- batch: 050 ----
mean loss: 941.42
 ---- batch: 060 ----
mean loss: 909.29
 ---- batch: 070 ----
mean loss: 913.67
 ---- batch: 080 ----
mean loss: 891.60
 ---- batch: 090 ----
mean loss: 899.60
train mean loss: 936.71
epoch train time: 0:00:02.631363
elapsed time: 0:00:41.205388
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 18:11:38.410740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.28
 ---- batch: 020 ----
mean loss: 884.94
 ---- batch: 030 ----
mean loss: 892.98
 ---- batch: 040 ----
mean loss: 891.15
 ---- batch: 050 ----
mean loss: 876.84
 ---- batch: 060 ----
mean loss: 886.41
 ---- batch: 070 ----
mean loss: 904.08
 ---- batch: 080 ----
mean loss: 899.75
 ---- batch: 090 ----
mean loss: 886.45
train mean loss: 891.27
epoch train time: 0:00:02.625729
elapsed time: 0:00:43.831591
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 18:11:41.036959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.42
 ---- batch: 020 ----
mean loss: 881.03
 ---- batch: 030 ----
mean loss: 885.76
 ---- batch: 040 ----
mean loss: 891.58
 ---- batch: 050 ----
mean loss: 869.53
 ---- batch: 060 ----
mean loss: 882.63
 ---- batch: 070 ----
mean loss: 896.94
 ---- batch: 080 ----
mean loss: 891.97
 ---- batch: 090 ----
mean loss: 877.29
train mean loss: 884.71
epoch train time: 0:00:02.615101
elapsed time: 0:00:46.447176
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 18:11:43.652520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.30
 ---- batch: 020 ----
mean loss: 877.42
 ---- batch: 030 ----
mean loss: 894.94
 ---- batch: 040 ----
mean loss: 881.87
 ---- batch: 050 ----
mean loss: 873.66
 ---- batch: 060 ----
mean loss: 860.39
 ---- batch: 070 ----
mean loss: 889.49
 ---- batch: 080 ----
mean loss: 881.37
 ---- batch: 090 ----
mean loss: 872.95
train mean loss: 879.95
epoch train time: 0:00:02.606272
elapsed time: 0:00:49.053901
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 18:11:46.259246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.92
 ---- batch: 020 ----
mean loss: 877.83
 ---- batch: 030 ----
mean loss: 888.42
 ---- batch: 040 ----
mean loss: 894.86
 ---- batch: 050 ----
mean loss: 887.38
 ---- batch: 060 ----
mean loss: 872.94
 ---- batch: 070 ----
mean loss: 864.27
 ---- batch: 080 ----
mean loss: 869.92
 ---- batch: 090 ----
mean loss: 864.16
train mean loss: 875.54
epoch train time: 0:00:02.598355
elapsed time: 0:00:51.652745
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 18:11:48.858104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.68
 ---- batch: 020 ----
mean loss: 876.85
 ---- batch: 030 ----
mean loss: 912.53
 ---- batch: 040 ----
mean loss: 873.77
 ---- batch: 050 ----
mean loss: 859.33
 ---- batch: 060 ----
mean loss: 865.36
 ---- batch: 070 ----
mean loss: 884.61
 ---- batch: 080 ----
mean loss: 861.89
 ---- batch: 090 ----
mean loss: 848.07
train mean loss: 869.69
epoch train time: 0:00:02.632944
elapsed time: 0:00:54.286174
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 18:11:51.491526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.35
 ---- batch: 020 ----
mean loss: 861.61
 ---- batch: 030 ----
mean loss: 867.96
 ---- batch: 040 ----
mean loss: 894.84
 ---- batch: 050 ----
mean loss: 851.79
 ---- batch: 060 ----
mean loss: 854.35
 ---- batch: 070 ----
mean loss: 856.52
 ---- batch: 080 ----
mean loss: 852.17
 ---- batch: 090 ----
mean loss: 875.01
train mean loss: 864.31
epoch train time: 0:00:02.614651
elapsed time: 0:00:56.901333
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 18:11:54.106683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.87
 ---- batch: 020 ----
mean loss: 864.41
 ---- batch: 030 ----
mean loss: 834.87
 ---- batch: 040 ----
mean loss: 850.88
 ---- batch: 050 ----
mean loss: 870.92
 ---- batch: 060 ----
mean loss: 876.86
 ---- batch: 070 ----
mean loss: 862.27
 ---- batch: 080 ----
mean loss: 847.14
 ---- batch: 090 ----
mean loss: 852.17
train mean loss: 858.24
epoch train time: 0:00:02.572125
elapsed time: 0:00:59.473942
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 18:11:56.679288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.87
 ---- batch: 020 ----
mean loss: 865.83
 ---- batch: 030 ----
mean loss: 854.95
 ---- batch: 040 ----
mean loss: 858.94
 ---- batch: 050 ----
mean loss: 847.50
 ---- batch: 060 ----
mean loss: 853.45
 ---- batch: 070 ----
mean loss: 858.06
 ---- batch: 080 ----
mean loss: 845.34
 ---- batch: 090 ----
mean loss: 857.42
train mean loss: 852.48
epoch train time: 0:00:02.623617
elapsed time: 0:01:02.098131
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 18:11:59.303493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 813.62
 ---- batch: 020 ----
mean loss: 837.64
 ---- batch: 030 ----
mean loss: 850.75
 ---- batch: 040 ----
mean loss: 865.96
 ---- batch: 050 ----
mean loss: 866.52
 ---- batch: 060 ----
mean loss: 852.53
 ---- batch: 070 ----
mean loss: 861.24
 ---- batch: 080 ----
mean loss: 845.21
 ---- batch: 090 ----
mean loss: 834.68
train mean loss: 845.61
epoch train time: 0:00:02.629140
elapsed time: 0:01:04.727768
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 18:12:01.933114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.09
 ---- batch: 020 ----
mean loss: 848.74
 ---- batch: 030 ----
mean loss: 847.27
 ---- batch: 040 ----
mean loss: 825.28
 ---- batch: 050 ----
mean loss: 844.68
 ---- batch: 060 ----
mean loss: 845.42
 ---- batch: 070 ----
mean loss: 823.42
 ---- batch: 080 ----
mean loss: 836.22
 ---- batch: 090 ----
mean loss: 847.08
train mean loss: 839.38
epoch train time: 0:00:02.633957
elapsed time: 0:01:07.362245
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 18:12:04.567607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.89
 ---- batch: 020 ----
mean loss: 837.99
 ---- batch: 030 ----
mean loss: 837.44
 ---- batch: 040 ----
mean loss: 813.37
 ---- batch: 050 ----
mean loss: 841.38
 ---- batch: 060 ----
mean loss: 825.38
 ---- batch: 070 ----
mean loss: 853.82
 ---- batch: 080 ----
mean loss: 834.37
 ---- batch: 090 ----
mean loss: 837.63
train mean loss: 836.29
epoch train time: 0:00:02.635003
elapsed time: 0:01:09.997725
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 18:12:07.203088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.52
 ---- batch: 020 ----
mean loss: 829.49
 ---- batch: 030 ----
mean loss: 839.57
 ---- batch: 040 ----
mean loss: 829.31
 ---- batch: 050 ----
mean loss: 825.40
 ---- batch: 060 ----
mean loss: 819.68
 ---- batch: 070 ----
mean loss: 819.32
 ---- batch: 080 ----
mean loss: 841.69
 ---- batch: 090 ----
mean loss: 831.12
train mean loss: 831.16
epoch train time: 0:00:02.604379
elapsed time: 0:01:12.602589
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 18:12:09.807938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.16
 ---- batch: 020 ----
mean loss: 831.05
 ---- batch: 030 ----
mean loss: 822.42
 ---- batch: 040 ----
mean loss: 834.56
 ---- batch: 050 ----
mean loss: 833.93
 ---- batch: 060 ----
mean loss: 818.51
 ---- batch: 070 ----
mean loss: 810.99
 ---- batch: 080 ----
mean loss: 818.74
 ---- batch: 090 ----
mean loss: 827.80
train mean loss: 824.85
epoch train time: 0:00:02.611269
elapsed time: 0:01:15.214340
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 18:12:12.419695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.46
 ---- batch: 020 ----
mean loss: 800.30
 ---- batch: 030 ----
mean loss: 815.80
 ---- batch: 040 ----
mean loss: 824.59
 ---- batch: 050 ----
mean loss: 801.82
 ---- batch: 060 ----
mean loss: 826.82
 ---- batch: 070 ----
mean loss: 833.29
 ---- batch: 080 ----
mean loss: 834.94
 ---- batch: 090 ----
mean loss: 805.34
train mean loss: 820.11
epoch train time: 0:00:02.610853
elapsed time: 0:01:17.825677
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 18:12:15.031070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 827.23
 ---- batch: 020 ----
mean loss: 806.77
 ---- batch: 030 ----
mean loss: 828.23
 ---- batch: 040 ----
mean loss: 819.70
 ---- batch: 050 ----
mean loss: 796.41
 ---- batch: 060 ----
mean loss: 800.89
 ---- batch: 070 ----
mean loss: 810.88
 ---- batch: 080 ----
mean loss: 808.26
 ---- batch: 090 ----
mean loss: 808.95
train mean loss: 812.31
epoch train time: 0:00:02.641234
elapsed time: 0:01:20.467421
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 18:12:17.672770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.27
 ---- batch: 020 ----
mean loss: 822.59
 ---- batch: 030 ----
mean loss: 792.03
 ---- batch: 040 ----
mean loss: 813.11
 ---- batch: 050 ----
mean loss: 810.19
 ---- batch: 060 ----
mean loss: 805.83
 ---- batch: 070 ----
mean loss: 790.51
 ---- batch: 080 ----
mean loss: 815.67
 ---- batch: 090 ----
mean loss: 815.34
train mean loss: 808.19
epoch train time: 0:00:02.634721
elapsed time: 0:01:23.102613
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 18:12:20.307990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.80
 ---- batch: 020 ----
mean loss: 811.63
 ---- batch: 030 ----
mean loss: 803.01
 ---- batch: 040 ----
mean loss: 809.41
 ---- batch: 050 ----
mean loss: 791.24
 ---- batch: 060 ----
mean loss: 802.84
 ---- batch: 070 ----
mean loss: 803.63
 ---- batch: 080 ----
mean loss: 792.06
 ---- batch: 090 ----
mean loss: 793.72
train mean loss: 801.63
epoch train time: 0:00:02.616004
elapsed time: 0:01:25.719128
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 18:12:22.924482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 799.75
 ---- batch: 020 ----
mean loss: 791.56
 ---- batch: 030 ----
mean loss: 786.55
 ---- batch: 040 ----
mean loss: 796.62
 ---- batch: 050 ----
mean loss: 804.76
 ---- batch: 060 ----
mean loss: 790.50
 ---- batch: 070 ----
mean loss: 779.30
 ---- batch: 080 ----
mean loss: 803.80
 ---- batch: 090 ----
mean loss: 794.27
train mean loss: 792.61
epoch train time: 0:00:02.564077
elapsed time: 0:01:28.283648
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 18:12:25.488990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 789.05
 ---- batch: 020 ----
mean loss: 797.21
 ---- batch: 030 ----
mean loss: 779.75
 ---- batch: 040 ----
mean loss: 774.88
 ---- batch: 050 ----
mean loss: 783.02
 ---- batch: 060 ----
mean loss: 801.16
 ---- batch: 070 ----
mean loss: 787.33
 ---- batch: 080 ----
mean loss: 782.80
 ---- batch: 090 ----
mean loss: 786.87
train mean loss: 787.50
epoch train time: 0:00:02.578900
elapsed time: 0:01:30.863026
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 18:12:28.068392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 789.21
 ---- batch: 020 ----
mean loss: 773.83
 ---- batch: 030 ----
mean loss: 767.62
 ---- batch: 040 ----
mean loss: 773.94
 ---- batch: 050 ----
mean loss: 780.08
 ---- batch: 060 ----
mean loss: 779.09
 ---- batch: 070 ----
mean loss: 780.10
 ---- batch: 080 ----
mean loss: 782.16
 ---- batch: 090 ----
mean loss: 786.61
train mean loss: 779.21
epoch train time: 0:00:02.599553
elapsed time: 0:01:33.463098
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 18:12:30.668456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 787.11
 ---- batch: 020 ----
mean loss: 764.86
 ---- batch: 030 ----
mean loss: 772.55
 ---- batch: 040 ----
mean loss: 764.04
 ---- batch: 050 ----
mean loss: 770.31
 ---- batch: 060 ----
mean loss: 763.63
 ---- batch: 070 ----
mean loss: 775.24
 ---- batch: 080 ----
mean loss: 779.52
 ---- batch: 090 ----
mean loss: 769.78
train mean loss: 773.23
epoch train time: 0:00:02.632017
elapsed time: 0:01:36.095598
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 18:12:33.300941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 757.73
 ---- batch: 020 ----
mean loss: 780.06
 ---- batch: 030 ----
mean loss: 764.53
 ---- batch: 040 ----
mean loss: 773.15
 ---- batch: 050 ----
mean loss: 766.63
 ---- batch: 060 ----
mean loss: 764.07
 ---- batch: 070 ----
mean loss: 760.51
 ---- batch: 080 ----
mean loss: 753.86
 ---- batch: 090 ----
mean loss: 765.43
train mean loss: 763.11
epoch train time: 0:00:02.667181
elapsed time: 0:01:38.763281
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 18:12:35.968660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 751.66
 ---- batch: 020 ----
mean loss: 755.95
 ---- batch: 030 ----
mean loss: 751.70
 ---- batch: 040 ----
mean loss: 744.84
 ---- batch: 050 ----
mean loss: 748.73
 ---- batch: 060 ----
mean loss: 771.82
 ---- batch: 070 ----
mean loss: 763.52
 ---- batch: 080 ----
mean loss: 753.37
 ---- batch: 090 ----
mean loss: 750.56
train mean loss: 755.35
epoch train time: 0:00:02.597806
elapsed time: 0:01:41.361576
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 18:12:38.566921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 752.25
 ---- batch: 020 ----
mean loss: 754.87
 ---- batch: 030 ----
mean loss: 742.92
 ---- batch: 040 ----
mean loss: 741.43
 ---- batch: 050 ----
mean loss: 733.60
 ---- batch: 060 ----
mean loss: 745.00
 ---- batch: 070 ----
mean loss: 750.08
 ---- batch: 080 ----
mean loss: 750.04
 ---- batch: 090 ----
mean loss: 752.46
train mean loss: 745.88
epoch train time: 0:00:02.615639
elapsed time: 0:01:43.977698
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 18:12:41.183059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 751.84
 ---- batch: 020 ----
mean loss: 735.23
 ---- batch: 030 ----
mean loss: 738.88
 ---- batch: 040 ----
mean loss: 745.76
 ---- batch: 050 ----
mean loss: 737.87
 ---- batch: 060 ----
mean loss: 726.83
 ---- batch: 070 ----
mean loss: 719.49
 ---- batch: 080 ----
mean loss: 751.66
 ---- batch: 090 ----
mean loss: 722.11
train mean loss: 735.49
epoch train time: 0:00:02.592336
elapsed time: 0:01:46.570501
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 18:12:43.775846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 734.66
 ---- batch: 020 ----
mean loss: 728.86
 ---- batch: 030 ----
mean loss: 714.26
 ---- batch: 040 ----
mean loss: 725.28
 ---- batch: 050 ----
mean loss: 732.28
 ---- batch: 060 ----
mean loss: 734.22
 ---- batch: 070 ----
mean loss: 742.06
 ---- batch: 080 ----
mean loss: 719.24
 ---- batch: 090 ----
mean loss: 714.22
train mean loss: 727.01
epoch train time: 0:00:02.656187
elapsed time: 0:01:49.227159
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 18:12:46.432612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 731.12
 ---- batch: 020 ----
mean loss: 725.67
 ---- batch: 030 ----
mean loss: 713.77
 ---- batch: 040 ----
mean loss: 717.67
 ---- batch: 050 ----
mean loss: 720.35
 ---- batch: 060 ----
mean loss: 725.53
 ---- batch: 070 ----
mean loss: 712.37
 ---- batch: 080 ----
mean loss: 712.04
 ---- batch: 090 ----
mean loss: 693.55
train mean loss: 715.63
epoch train time: 0:00:02.634359
elapsed time: 0:01:51.862133
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 18:12:49.067477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 699.88
 ---- batch: 020 ----
mean loss: 707.50
 ---- batch: 030 ----
mean loss: 703.65
 ---- batch: 040 ----
mean loss: 712.85
 ---- batch: 050 ----
mean loss: 724.56
 ---- batch: 060 ----
mean loss: 697.18
 ---- batch: 070 ----
mean loss: 717.95
 ---- batch: 080 ----
mean loss: 699.33
 ---- batch: 090 ----
mean loss: 688.86
train mean loss: 705.53
epoch train time: 0:00:02.630705
elapsed time: 0:01:54.493344
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 18:12:51.698699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 704.08
 ---- batch: 020 ----
mean loss: 705.07
 ---- batch: 030 ----
mean loss: 696.54
 ---- batch: 040 ----
mean loss: 686.86
 ---- batch: 050 ----
mean loss: 688.08
 ---- batch: 060 ----
mean loss: 701.36
 ---- batch: 070 ----
mean loss: 678.61
 ---- batch: 080 ----
mean loss: 708.44
 ---- batch: 090 ----
mean loss: 685.93
train mean loss: 694.32
epoch train time: 0:00:02.612031
elapsed time: 0:01:57.105834
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 18:12:54.311177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.66
 ---- batch: 020 ----
mean loss: 690.85
 ---- batch: 030 ----
mean loss: 690.06
 ---- batch: 040 ----
mean loss: 682.44
 ---- batch: 050 ----
mean loss: 675.31
 ---- batch: 060 ----
mean loss: 683.87
 ---- batch: 070 ----
mean loss: 681.96
 ---- batch: 080 ----
mean loss: 682.35
 ---- batch: 090 ----
mean loss: 679.46
train mean loss: 685.10
epoch train time: 0:00:02.575411
elapsed time: 0:01:59.681711
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 18:12:56.887069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 680.85
 ---- batch: 020 ----
mean loss: 682.52
 ---- batch: 030 ----
mean loss: 678.18
 ---- batch: 040 ----
mean loss: 675.39
 ---- batch: 050 ----
mean loss: 683.35
 ---- batch: 060 ----
mean loss: 674.33
 ---- batch: 070 ----
mean loss: 671.66
 ---- batch: 080 ----
mean loss: 657.69
 ---- batch: 090 ----
mean loss: 666.97
train mean loss: 674.44
epoch train time: 0:00:02.671800
elapsed time: 0:02:02.354036
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 18:12:59.559470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.94
 ---- batch: 020 ----
mean loss: 666.35
 ---- batch: 030 ----
mean loss: 654.06
 ---- batch: 040 ----
mean loss: 662.81
 ---- batch: 050 ----
mean loss: 657.71
 ---- batch: 060 ----
mean loss: 670.17
 ---- batch: 070 ----
mean loss: 663.76
 ---- batch: 080 ----
mean loss: 659.03
 ---- batch: 090 ----
mean loss: 671.67
train mean loss: 663.54
epoch train time: 0:00:02.665884
elapsed time: 0:02:05.020510
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 18:13:02.225865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.23
 ---- batch: 020 ----
mean loss: 656.27
 ---- batch: 030 ----
mean loss: 663.57
 ---- batch: 040 ----
mean loss: 661.47
 ---- batch: 050 ----
mean loss: 660.37
 ---- batch: 060 ----
mean loss: 638.69
 ---- batch: 070 ----
mean loss: 641.95
 ---- batch: 080 ----
mean loss: 642.83
 ---- batch: 090 ----
mean loss: 669.77
train mean loss: 653.74
epoch train time: 0:00:02.658310
elapsed time: 0:02:07.679307
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 18:13:04.884654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.65
 ---- batch: 020 ----
mean loss: 649.10
 ---- batch: 030 ----
mean loss: 644.35
 ---- batch: 040 ----
mean loss: 658.29
 ---- batch: 050 ----
mean loss: 652.29
 ---- batch: 060 ----
mean loss: 637.86
 ---- batch: 070 ----
mean loss: 643.77
 ---- batch: 080 ----
mean loss: 622.69
 ---- batch: 090 ----
mean loss: 630.73
train mean loss: 644.28
epoch train time: 0:00:02.637349
elapsed time: 0:02:10.317104
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 18:13:07.522462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.94
 ---- batch: 020 ----
mean loss: 642.31
 ---- batch: 030 ----
mean loss: 632.69
 ---- batch: 040 ----
mean loss: 646.86
 ---- batch: 050 ----
mean loss: 631.09
 ---- batch: 060 ----
mean loss: 633.65
 ---- batch: 070 ----
mean loss: 636.44
 ---- batch: 080 ----
mean loss: 634.49
 ---- batch: 090 ----
mean loss: 614.65
train mean loss: 633.84
epoch train time: 0:00:02.614242
elapsed time: 0:02:12.931833
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 18:13:10.137181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 631.70
 ---- batch: 020 ----
mean loss: 624.75
 ---- batch: 030 ----
mean loss: 622.62
 ---- batch: 040 ----
mean loss: 626.40
 ---- batch: 050 ----
mean loss: 617.89
 ---- batch: 060 ----
mean loss: 628.69
 ---- batch: 070 ----
mean loss: 629.94
 ---- batch: 080 ----
mean loss: 619.10
 ---- batch: 090 ----
mean loss: 621.73
train mean loss: 624.39
epoch train time: 0:00:02.622102
elapsed time: 0:02:15.554386
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 18:13:12.759739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 624.19
 ---- batch: 020 ----
mean loss: 610.78
 ---- batch: 030 ----
mean loss: 612.33
 ---- batch: 040 ----
mean loss: 623.68
 ---- batch: 050 ----
mean loss: 616.53
 ---- batch: 060 ----
mean loss: 613.28
 ---- batch: 070 ----
mean loss: 613.29
 ---- batch: 080 ----
mean loss: 607.01
 ---- batch: 090 ----
mean loss: 615.52
train mean loss: 615.60
epoch train time: 0:00:02.636869
elapsed time: 0:02:18.191706
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 18:13:15.397054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 619.15
 ---- batch: 020 ----
mean loss: 613.21
 ---- batch: 030 ----
mean loss: 605.30
 ---- batch: 040 ----
mean loss: 603.17
 ---- batch: 050 ----
mean loss: 607.11
 ---- batch: 060 ----
mean loss: 610.40
 ---- batch: 070 ----
mean loss: 607.29
 ---- batch: 080 ----
mean loss: 597.47
 ---- batch: 090 ----
mean loss: 593.39
train mean loss: 606.13
epoch train time: 0:00:02.653967
elapsed time: 0:02:20.846134
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 18:13:18.051490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.54
 ---- batch: 020 ----
mean loss: 599.82
 ---- batch: 030 ----
mean loss: 600.87
 ---- batch: 040 ----
mean loss: 599.63
 ---- batch: 050 ----
mean loss: 587.31
 ---- batch: 060 ----
mean loss: 603.97
 ---- batch: 070 ----
mean loss: 593.09
 ---- batch: 080 ----
mean loss: 588.78
 ---- batch: 090 ----
mean loss: 599.10
train mean loss: 596.05
epoch train time: 0:00:02.672363
elapsed time: 0:02:23.518957
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 18:13:20.724334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.81
 ---- batch: 020 ----
mean loss: 579.58
 ---- batch: 030 ----
mean loss: 593.87
 ---- batch: 040 ----
mean loss: 569.53
 ---- batch: 050 ----
mean loss: 585.39
 ---- batch: 060 ----
mean loss: 589.41
 ---- batch: 070 ----
mean loss: 586.25
 ---- batch: 080 ----
mean loss: 596.74
 ---- batch: 090 ----
mean loss: 592.79
train mean loss: 588.34
epoch train time: 0:00:02.678665
elapsed time: 0:02:26.198146
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 18:13:23.403493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.35
 ---- batch: 020 ----
mean loss: 586.57
 ---- batch: 030 ----
mean loss: 585.65
 ---- batch: 040 ----
mean loss: 586.90
 ---- batch: 050 ----
mean loss: 573.31
 ---- batch: 060 ----
mean loss: 576.88
 ---- batch: 070 ----
mean loss: 576.08
 ---- batch: 080 ----
mean loss: 576.63
 ---- batch: 090 ----
mean loss: 578.71
train mean loss: 580.10
epoch train time: 0:00:02.628484
elapsed time: 0:02:28.827104
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 18:13:26.032464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 572.89
 ---- batch: 020 ----
mean loss: 568.56
 ---- batch: 030 ----
mean loss: 591.91
 ---- batch: 040 ----
mean loss: 569.39
 ---- batch: 050 ----
mean loss: 564.43
 ---- batch: 060 ----
mean loss: 575.39
 ---- batch: 070 ----
mean loss: 575.16
 ---- batch: 080 ----
mean loss: 561.98
 ---- batch: 090 ----
mean loss: 572.21
train mean loss: 572.09
epoch train time: 0:00:02.641659
elapsed time: 0:02:31.469300
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 18:13:28.674671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 569.50
 ---- batch: 020 ----
mean loss: 574.67
 ---- batch: 030 ----
mean loss: 568.39
 ---- batch: 040 ----
mean loss: 567.08
 ---- batch: 050 ----
mean loss: 560.98
 ---- batch: 060 ----
mean loss: 561.88
 ---- batch: 070 ----
mean loss: 571.49
 ---- batch: 080 ----
mean loss: 555.45
 ---- batch: 090 ----
mean loss: 562.93
train mean loss: 565.35
epoch train time: 0:00:02.623634
elapsed time: 0:02:34.093436
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 18:13:31.298785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 560.87
 ---- batch: 020 ----
mean loss: 555.88
 ---- batch: 030 ----
mean loss: 565.41
 ---- batch: 040 ----
mean loss: 553.53
 ---- batch: 050 ----
mean loss: 565.19
 ---- batch: 060 ----
mean loss: 559.36
 ---- batch: 070 ----
mean loss: 561.29
 ---- batch: 080 ----
mean loss: 543.25
 ---- batch: 090 ----
mean loss: 547.68
train mean loss: 557.09
epoch train time: 0:00:02.628582
elapsed time: 0:02:36.722519
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 18:13:33.927898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 553.54
 ---- batch: 020 ----
mean loss: 550.83
 ---- batch: 030 ----
mean loss: 554.23
 ---- batch: 040 ----
mean loss: 541.40
 ---- batch: 050 ----
mean loss: 568.42
 ---- batch: 060 ----
mean loss: 538.55
 ---- batch: 070 ----
mean loss: 551.50
 ---- batch: 080 ----
mean loss: 542.67
 ---- batch: 090 ----
mean loss: 554.41
train mean loss: 550.74
epoch train time: 0:00:02.663971
elapsed time: 0:02:39.387032
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 18:13:36.592426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 542.16
 ---- batch: 020 ----
mean loss: 539.14
 ---- batch: 030 ----
mean loss: 538.87
 ---- batch: 040 ----
mean loss: 532.38
 ---- batch: 050 ----
mean loss: 550.25
 ---- batch: 060 ----
mean loss: 535.37
 ---- batch: 070 ----
mean loss: 551.61
 ---- batch: 080 ----
mean loss: 551.68
 ---- batch: 090 ----
mean loss: 550.78
train mean loss: 543.51
epoch train time: 0:00:02.649453
elapsed time: 0:02:42.036989
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 18:13:39.242360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 541.90
 ---- batch: 020 ----
mean loss: 532.74
 ---- batch: 030 ----
mean loss: 541.65
 ---- batch: 040 ----
mean loss: 546.88
 ---- batch: 050 ----
mean loss: 525.78
 ---- batch: 060 ----
mean loss: 547.06
 ---- batch: 070 ----
mean loss: 526.60
 ---- batch: 080 ----
mean loss: 541.10
 ---- batch: 090 ----
mean loss: 535.89
train mean loss: 537.48
epoch train time: 0:00:02.606140
elapsed time: 0:02:44.643677
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 18:13:41.849052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 540.52
 ---- batch: 020 ----
mean loss: 536.11
 ---- batch: 030 ----
mean loss: 538.73
 ---- batch: 040 ----
mean loss: 531.15
 ---- batch: 050 ----
mean loss: 520.67
 ---- batch: 060 ----
mean loss: 529.82
 ---- batch: 070 ----
mean loss: 534.56
 ---- batch: 080 ----
mean loss: 525.70
 ---- batch: 090 ----
mean loss: 532.09
train mean loss: 531.39
epoch train time: 0:00:02.630274
elapsed time: 0:02:47.274456
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 18:13:44.479805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.12
 ---- batch: 020 ----
mean loss: 522.16
 ---- batch: 030 ----
mean loss: 531.06
 ---- batch: 040 ----
mean loss: 527.13
 ---- batch: 050 ----
mean loss: 513.75
 ---- batch: 060 ----
mean loss: 517.23
 ---- batch: 070 ----
mean loss: 529.14
 ---- batch: 080 ----
mean loss: 528.91
 ---- batch: 090 ----
mean loss: 530.53
train mean loss: 525.21
epoch train time: 0:00:02.660877
elapsed time: 0:02:49.935791
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 18:13:47.141147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.79
 ---- batch: 020 ----
mean loss: 516.59
 ---- batch: 030 ----
mean loss: 520.58
 ---- batch: 040 ----
mean loss: 511.69
 ---- batch: 050 ----
mean loss: 525.46
 ---- batch: 060 ----
mean loss: 526.13
 ---- batch: 070 ----
mean loss: 510.49
 ---- batch: 080 ----
mean loss: 520.22
 ---- batch: 090 ----
mean loss: 514.08
train mean loss: 518.48
epoch train time: 0:00:02.666492
elapsed time: 0:02:52.602743
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 18:13:49.808137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.52
 ---- batch: 020 ----
mean loss: 509.38
 ---- batch: 030 ----
mean loss: 529.19
 ---- batch: 040 ----
mean loss: 512.74
 ---- batch: 050 ----
mean loss: 513.50
 ---- batch: 060 ----
mean loss: 512.96
 ---- batch: 070 ----
mean loss: 504.29
 ---- batch: 080 ----
mean loss: 518.69
 ---- batch: 090 ----
mean loss: 500.47
train mean loss: 512.59
epoch train time: 0:00:02.642189
elapsed time: 0:02:55.245453
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 18:13:52.450741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.68
 ---- batch: 020 ----
mean loss: 501.66
 ---- batch: 030 ----
mean loss: 511.94
 ---- batch: 040 ----
mean loss: 496.40
 ---- batch: 050 ----
mean loss: 506.12
 ---- batch: 060 ----
mean loss: 507.72
 ---- batch: 070 ----
mean loss: 509.06
 ---- batch: 080 ----
mean loss: 497.95
 ---- batch: 090 ----
mean loss: 503.74
train mean loss: 506.55
epoch train time: 0:00:02.605568
elapsed time: 0:02:57.851426
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 18:13:55.056783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.07
 ---- batch: 020 ----
mean loss: 496.78
 ---- batch: 030 ----
mean loss: 500.56
 ---- batch: 040 ----
mean loss: 502.87
 ---- batch: 050 ----
mean loss: 502.98
 ---- batch: 060 ----
mean loss: 501.62
 ---- batch: 070 ----
mean loss: 513.33
 ---- batch: 080 ----
mean loss: 484.08
 ---- batch: 090 ----
mean loss: 495.12
train mean loss: 500.53
epoch train time: 0:00:02.644498
elapsed time: 0:03:00.496413
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 18:13:57.701792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.81
 ---- batch: 020 ----
mean loss: 488.23
 ---- batch: 030 ----
mean loss: 506.68
 ---- batch: 040 ----
mean loss: 487.68
 ---- batch: 050 ----
mean loss: 496.43
 ---- batch: 060 ----
mean loss: 488.80
 ---- batch: 070 ----
mean loss: 494.11
 ---- batch: 080 ----
mean loss: 490.75
 ---- batch: 090 ----
mean loss: 486.70
train mean loss: 493.13
epoch train time: 0:00:02.598586
elapsed time: 0:03:03.095532
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 18:14:00.300904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.21
 ---- batch: 020 ----
mean loss: 499.80
 ---- batch: 030 ----
mean loss: 494.57
 ---- batch: 040 ----
mean loss: 476.17
 ---- batch: 050 ----
mean loss: 499.82
 ---- batch: 060 ----
mean loss: 486.26
 ---- batch: 070 ----
mean loss: 485.11
 ---- batch: 080 ----
mean loss: 482.00
 ---- batch: 090 ----
mean loss: 477.20
train mean loss: 486.39
epoch train time: 0:00:02.623098
elapsed time: 0:03:05.719094
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 18:14:02.924468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.36
 ---- batch: 020 ----
mean loss: 477.25
 ---- batch: 030 ----
mean loss: 484.12
 ---- batch: 040 ----
mean loss: 478.55
 ---- batch: 050 ----
mean loss: 476.59
 ---- batch: 060 ----
mean loss: 469.54
 ---- batch: 070 ----
mean loss: 476.44
 ---- batch: 080 ----
mean loss: 477.70
 ---- batch: 090 ----
mean loss: 481.04
train mean loss: 478.34
epoch train time: 0:00:02.597586
elapsed time: 0:03:08.317192
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 18:14:05.522551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.57
 ---- batch: 020 ----
mean loss: 470.11
 ---- batch: 030 ----
mean loss: 467.80
 ---- batch: 040 ----
mean loss: 470.60
 ---- batch: 050 ----
mean loss: 468.87
 ---- batch: 060 ----
mean loss: 475.01
 ---- batch: 070 ----
mean loss: 470.92
 ---- batch: 080 ----
mean loss: 467.18
 ---- batch: 090 ----
mean loss: 472.03
train mean loss: 471.08
epoch train time: 0:00:02.621633
elapsed time: 0:03:10.939287
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 18:14:08.144688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.30
 ---- batch: 020 ----
mean loss: 457.71
 ---- batch: 030 ----
mean loss: 456.68
 ---- batch: 040 ----
mean loss: 472.43
 ---- batch: 050 ----
mean loss: 461.68
 ---- batch: 060 ----
mean loss: 458.57
 ---- batch: 070 ----
mean loss: 460.59
 ---- batch: 080 ----
mean loss: 472.55
 ---- batch: 090 ----
mean loss: 465.86
train mean loss: 462.80
epoch train time: 0:00:02.637232
elapsed time: 0:03:13.577042
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 18:14:10.782394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.23
 ---- batch: 020 ----
mean loss: 446.31
 ---- batch: 030 ----
mean loss: 463.73
 ---- batch: 040 ----
mean loss: 445.29
 ---- batch: 050 ----
mean loss: 448.26
 ---- batch: 060 ----
mean loss: 456.46
 ---- batch: 070 ----
mean loss: 449.31
 ---- batch: 080 ----
mean loss: 451.59
 ---- batch: 090 ----
mean loss: 466.20
train mean loss: 453.28
epoch train time: 0:00:02.656133
elapsed time: 0:03:16.233754
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 18:14:13.439132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.31
 ---- batch: 020 ----
mean loss: 442.76
 ---- batch: 030 ----
mean loss: 443.74
 ---- batch: 040 ----
mean loss: 436.52
 ---- batch: 050 ----
mean loss: 448.68
 ---- batch: 060 ----
mean loss: 441.46
 ---- batch: 070 ----
mean loss: 446.52
 ---- batch: 080 ----
mean loss: 439.94
 ---- batch: 090 ----
mean loss: 433.19
train mean loss: 441.74
epoch train time: 0:00:02.615003
elapsed time: 0:03:18.849279
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 18:14:16.054635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.44
 ---- batch: 020 ----
mean loss: 428.96
 ---- batch: 030 ----
mean loss: 434.85
 ---- batch: 040 ----
mean loss: 425.58
 ---- batch: 050 ----
mean loss: 437.10
 ---- batch: 060 ----
mean loss: 413.04
 ---- batch: 070 ----
mean loss: 412.49
 ---- batch: 080 ----
mean loss: 410.29
 ---- batch: 090 ----
mean loss: 408.73
train mean loss: 421.65
epoch train time: 0:00:02.614519
elapsed time: 0:03:21.464340
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 18:14:18.669738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.92
 ---- batch: 020 ----
mean loss: 407.94
 ---- batch: 030 ----
mean loss: 397.37
 ---- batch: 040 ----
mean loss: 408.00
 ---- batch: 050 ----
mean loss: 401.05
 ---- batch: 060 ----
mean loss: 391.09
 ---- batch: 070 ----
mean loss: 384.53
 ---- batch: 080 ----
mean loss: 390.58
 ---- batch: 090 ----
mean loss: 383.20
train mean loss: 396.59
epoch train time: 0:00:02.626113
elapsed time: 0:03:24.091034
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 18:14:21.296436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.53
 ---- batch: 020 ----
mean loss: 380.66
 ---- batch: 030 ----
mean loss: 381.33
 ---- batch: 040 ----
mean loss: 362.65
 ---- batch: 050 ----
mean loss: 373.30
 ---- batch: 060 ----
mean loss: 382.09
 ---- batch: 070 ----
mean loss: 368.72
 ---- batch: 080 ----
mean loss: 369.06
 ---- batch: 090 ----
mean loss: 369.74
train mean loss: 373.59
epoch train time: 0:00:02.592548
elapsed time: 0:03:26.684101
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 18:14:23.889455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.43
 ---- batch: 020 ----
mean loss: 356.09
 ---- batch: 030 ----
mean loss: 368.79
 ---- batch: 040 ----
mean loss: 361.14
 ---- batch: 050 ----
mean loss: 359.37
 ---- batch: 060 ----
mean loss: 353.71
 ---- batch: 070 ----
mean loss: 349.37
 ---- batch: 080 ----
mean loss: 348.69
 ---- batch: 090 ----
mean loss: 346.38
train mean loss: 355.79
epoch train time: 0:00:02.617624
elapsed time: 0:03:29.302196
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 18:14:26.507539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.51
 ---- batch: 020 ----
mean loss: 341.82
 ---- batch: 030 ----
mean loss: 332.89
 ---- batch: 040 ----
mean loss: 333.67
 ---- batch: 050 ----
mean loss: 344.67
 ---- batch: 060 ----
mean loss: 345.22
 ---- batch: 070 ----
mean loss: 347.56
 ---- batch: 080 ----
mean loss: 339.32
 ---- batch: 090 ----
mean loss: 330.96
train mean loss: 339.93
epoch train time: 0:00:02.641424
elapsed time: 0:03:31.944158
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 18:14:29.149542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.62
 ---- batch: 020 ----
mean loss: 332.86
 ---- batch: 030 ----
mean loss: 329.22
 ---- batch: 040 ----
mean loss: 324.36
 ---- batch: 050 ----
mean loss: 328.10
 ---- batch: 060 ----
mean loss: 327.50
 ---- batch: 070 ----
mean loss: 334.11
 ---- batch: 080 ----
mean loss: 324.75
 ---- batch: 090 ----
mean loss: 324.23
train mean loss: 329.28
epoch train time: 0:00:02.629394
elapsed time: 0:03:34.574109
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 18:14:31.779473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.67
 ---- batch: 020 ----
mean loss: 313.22
 ---- batch: 030 ----
mean loss: 323.65
 ---- batch: 040 ----
mean loss: 321.19
 ---- batch: 050 ----
mean loss: 315.56
 ---- batch: 060 ----
mean loss: 309.72
 ---- batch: 070 ----
mean loss: 303.55
 ---- batch: 080 ----
mean loss: 334.68
 ---- batch: 090 ----
mean loss: 326.19
train mean loss: 320.30
epoch train time: 0:00:02.604220
elapsed time: 0:03:37.178811
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 18:14:34.384161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.81
 ---- batch: 020 ----
mean loss: 314.57
 ---- batch: 030 ----
mean loss: 321.79
 ---- batch: 040 ----
mean loss: 318.83
 ---- batch: 050 ----
mean loss: 313.61
 ---- batch: 060 ----
mean loss: 307.04
 ---- batch: 070 ----
mean loss: 310.50
 ---- batch: 080 ----
mean loss: 306.99
 ---- batch: 090 ----
mean loss: 314.39
train mean loss: 312.59
epoch train time: 0:00:02.589544
elapsed time: 0:03:39.768819
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 18:14:36.974162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.51
 ---- batch: 020 ----
mean loss: 301.97
 ---- batch: 030 ----
mean loss: 312.76
 ---- batch: 040 ----
mean loss: 307.15
 ---- batch: 050 ----
mean loss: 312.97
 ---- batch: 060 ----
mean loss: 311.42
 ---- batch: 070 ----
mean loss: 294.77
 ---- batch: 080 ----
mean loss: 299.85
 ---- batch: 090 ----
mean loss: 306.55
train mean loss: 305.47
epoch train time: 0:00:02.639047
elapsed time: 0:03:42.408321
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 18:14:39.613717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.98
 ---- batch: 020 ----
mean loss: 288.03
 ---- batch: 030 ----
mean loss: 296.29
 ---- batch: 040 ----
mean loss: 304.05
 ---- batch: 050 ----
mean loss: 304.66
 ---- batch: 060 ----
mean loss: 302.01
 ---- batch: 070 ----
mean loss: 299.62
 ---- batch: 080 ----
mean loss: 303.73
 ---- batch: 090 ----
mean loss: 300.10
train mean loss: 299.67
epoch train time: 0:00:02.635371
elapsed time: 0:03:45.044215
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 18:14:42.249573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.13
 ---- batch: 020 ----
mean loss: 296.59
 ---- batch: 030 ----
mean loss: 307.48
 ---- batch: 040 ----
mean loss: 291.99
 ---- batch: 050 ----
mean loss: 291.27
 ---- batch: 060 ----
mean loss: 293.99
 ---- batch: 070 ----
mean loss: 297.53
 ---- batch: 080 ----
mean loss: 298.34
 ---- batch: 090 ----
mean loss: 295.88
train mean loss: 295.47
epoch train time: 0:00:02.629535
elapsed time: 0:03:47.674394
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 18:14:44.879751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.18
 ---- batch: 020 ----
mean loss: 292.04
 ---- batch: 030 ----
mean loss: 300.39
 ---- batch: 040 ----
mean loss: 287.32
 ---- batch: 050 ----
mean loss: 287.97
 ---- batch: 060 ----
mean loss: 294.49
 ---- batch: 070 ----
mean loss: 293.69
 ---- batch: 080 ----
mean loss: 289.01
 ---- batch: 090 ----
mean loss: 286.95
train mean loss: 290.94
epoch train time: 0:00:02.608442
elapsed time: 0:03:50.283318
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 18:14:47.488664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.23
 ---- batch: 020 ----
mean loss: 290.57
 ---- batch: 030 ----
mean loss: 288.74
 ---- batch: 040 ----
mean loss: 291.89
 ---- batch: 050 ----
mean loss: 282.41
 ---- batch: 060 ----
mean loss: 288.32
 ---- batch: 070 ----
mean loss: 293.55
 ---- batch: 080 ----
mean loss: 289.25
 ---- batch: 090 ----
mean loss: 292.26
train mean loss: 288.33
epoch train time: 0:00:02.607666
elapsed time: 0:03:52.891445
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 18:14:50.096799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.77
 ---- batch: 020 ----
mean loss: 283.49
 ---- batch: 030 ----
mean loss: 268.70
 ---- batch: 040 ----
mean loss: 288.45
 ---- batch: 050 ----
mean loss: 291.23
 ---- batch: 060 ----
mean loss: 277.48
 ---- batch: 070 ----
mean loss: 285.04
 ---- batch: 080 ----
mean loss: 292.79
 ---- batch: 090 ----
mean loss: 284.01
train mean loss: 284.94
epoch train time: 0:00:02.639682
elapsed time: 0:03:55.531627
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 18:14:52.736987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.11
 ---- batch: 020 ----
mean loss: 286.92
 ---- batch: 030 ----
mean loss: 278.21
 ---- batch: 040 ----
mean loss: 294.03
 ---- batch: 050 ----
mean loss: 278.00
 ---- batch: 060 ----
mean loss: 278.36
 ---- batch: 070 ----
mean loss: 278.44
 ---- batch: 080 ----
mean loss: 284.96
 ---- batch: 090 ----
mean loss: 280.62
train mean loss: 282.03
epoch train time: 0:00:02.634223
elapsed time: 0:03:58.166379
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 18:14:55.371733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.11
 ---- batch: 020 ----
mean loss: 276.29
 ---- batch: 030 ----
mean loss: 277.44
 ---- batch: 040 ----
mean loss: 292.73
 ---- batch: 050 ----
mean loss: 281.50
 ---- batch: 060 ----
mean loss: 276.02
 ---- batch: 070 ----
mean loss: 291.85
 ---- batch: 080 ----
mean loss: 274.79
 ---- batch: 090 ----
mean loss: 280.72
train mean loss: 279.68
epoch train time: 0:00:02.623595
elapsed time: 0:04:00.790439
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 18:14:57.995809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.46
 ---- batch: 020 ----
mean loss: 279.61
 ---- batch: 030 ----
mean loss: 277.84
 ---- batch: 040 ----
mean loss: 283.42
 ---- batch: 050 ----
mean loss: 274.16
 ---- batch: 060 ----
mean loss: 287.48
 ---- batch: 070 ----
mean loss: 267.70
 ---- batch: 080 ----
mean loss: 277.90
 ---- batch: 090 ----
mean loss: 279.91
train mean loss: 277.76
epoch train time: 0:00:02.601896
elapsed time: 0:04:03.392838
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 18:15:00.598189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.78
 ---- batch: 020 ----
mean loss: 271.10
 ---- batch: 030 ----
mean loss: 276.96
 ---- batch: 040 ----
mean loss: 272.72
 ---- batch: 050 ----
mean loss: 275.61
 ---- batch: 060 ----
mean loss: 274.25
 ---- batch: 070 ----
mean loss: 275.29
 ---- batch: 080 ----
mean loss: 275.26
 ---- batch: 090 ----
mean loss: 277.28
train mean loss: 275.37
epoch train time: 0:00:02.636749
elapsed time: 0:04:06.030096
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 18:15:03.235450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.02
 ---- batch: 020 ----
mean loss: 281.91
 ---- batch: 030 ----
mean loss: 279.83
 ---- batch: 040 ----
mean loss: 269.35
 ---- batch: 050 ----
mean loss: 283.11
 ---- batch: 060 ----
mean loss: 268.68
 ---- batch: 070 ----
mean loss: 271.57
 ---- batch: 080 ----
mean loss: 264.06
 ---- batch: 090 ----
mean loss: 272.94
train mean loss: 273.14
epoch train time: 0:00:02.655890
elapsed time: 0:04:08.686494
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 18:15:05.891891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.45
 ---- batch: 020 ----
mean loss: 265.64
 ---- batch: 030 ----
mean loss: 265.29
 ---- batch: 040 ----
mean loss: 271.28
 ---- batch: 050 ----
mean loss: 271.51
 ---- batch: 060 ----
mean loss: 280.98
 ---- batch: 070 ----
mean loss: 265.51
 ---- batch: 080 ----
mean loss: 281.14
 ---- batch: 090 ----
mean loss: 270.56
train mean loss: 271.10
epoch train time: 0:00:02.622494
elapsed time: 0:04:11.309542
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 18:15:08.514893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.65
 ---- batch: 020 ----
mean loss: 262.88
 ---- batch: 030 ----
mean loss: 272.23
 ---- batch: 040 ----
mean loss: 269.27
 ---- batch: 050 ----
mean loss: 273.49
 ---- batch: 060 ----
mean loss: 274.06
 ---- batch: 070 ----
mean loss: 266.69
 ---- batch: 080 ----
mean loss: 266.03
 ---- batch: 090 ----
mean loss: 268.08
train mean loss: 269.98
epoch train time: 0:00:02.581300
elapsed time: 0:04:13.891309
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 18:15:11.096668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.14
 ---- batch: 020 ----
mean loss: 266.27
 ---- batch: 030 ----
mean loss: 268.10
 ---- batch: 040 ----
mean loss: 256.50
 ---- batch: 050 ----
mean loss: 275.15
 ---- batch: 060 ----
mean loss: 269.43
 ---- batch: 070 ----
mean loss: 263.84
 ---- batch: 080 ----
mean loss: 267.68
 ---- batch: 090 ----
mean loss: 276.12
train mean loss: 267.65
epoch train time: 0:00:02.594207
elapsed time: 0:04:16.485998
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 18:15:13.691344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.81
 ---- batch: 020 ----
mean loss: 276.41
 ---- batch: 030 ----
mean loss: 263.84
 ---- batch: 040 ----
mean loss: 271.82
 ---- batch: 050 ----
mean loss: 265.22
 ---- batch: 060 ----
mean loss: 254.93
 ---- batch: 070 ----
mean loss: 253.89
 ---- batch: 080 ----
mean loss: 261.61
 ---- batch: 090 ----
mean loss: 276.01
train mean loss: 266.80
epoch train time: 0:00:02.616819
elapsed time: 0:04:19.103274
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 18:15:16.308626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.26
 ---- batch: 020 ----
mean loss: 263.28
 ---- batch: 030 ----
mean loss: 266.07
 ---- batch: 040 ----
mean loss: 269.82
 ---- batch: 050 ----
mean loss: 265.04
 ---- batch: 060 ----
mean loss: 262.20
 ---- batch: 070 ----
mean loss: 263.88
 ---- batch: 080 ----
mean loss: 265.71
 ---- batch: 090 ----
mean loss: 254.94
train mean loss: 264.56
epoch train time: 0:00:02.625318
elapsed time: 0:04:21.729140
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 18:15:18.934519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.82
 ---- batch: 020 ----
mean loss: 267.72
 ---- batch: 030 ----
mean loss: 267.13
 ---- batch: 040 ----
mean loss: 267.58
 ---- batch: 050 ----
mean loss: 249.54
 ---- batch: 060 ----
mean loss: 269.17
 ---- batch: 070 ----
mean loss: 260.11
 ---- batch: 080 ----
mean loss: 268.58
 ---- batch: 090 ----
mean loss: 261.45
train mean loss: 263.19
epoch train time: 0:00:02.624075
elapsed time: 0:04:24.353720
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 18:15:21.559069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.12
 ---- batch: 020 ----
mean loss: 262.36
 ---- batch: 030 ----
mean loss: 262.08
 ---- batch: 040 ----
mean loss: 259.12
 ---- batch: 050 ----
mean loss: 260.18
 ---- batch: 060 ----
mean loss: 268.52
 ---- batch: 070 ----
mean loss: 255.53
 ---- batch: 080 ----
mean loss: 262.33
 ---- batch: 090 ----
mean loss: 257.36
train mean loss: 261.89
epoch train time: 0:00:02.599256
elapsed time: 0:04:26.953457
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 18:15:24.158803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.00
 ---- batch: 020 ----
mean loss: 267.05
 ---- batch: 030 ----
mean loss: 264.04
 ---- batch: 040 ----
mean loss: 256.59
 ---- batch: 050 ----
mean loss: 263.33
 ---- batch: 060 ----
mean loss: 256.88
 ---- batch: 070 ----
mean loss: 259.50
 ---- batch: 080 ----
mean loss: 257.54
 ---- batch: 090 ----
mean loss: 261.43
train mean loss: 260.28
epoch train time: 0:00:02.613608
elapsed time: 0:04:29.567533
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 18:15:26.772883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.04
 ---- batch: 020 ----
mean loss: 257.36
 ---- batch: 030 ----
mean loss: 246.56
 ---- batch: 040 ----
mean loss: 259.54
 ---- batch: 050 ----
mean loss: 265.65
 ---- batch: 060 ----
mean loss: 270.11
 ---- batch: 070 ----
mean loss: 262.49
 ---- batch: 080 ----
mean loss: 258.15
 ---- batch: 090 ----
mean loss: 253.15
train mean loss: 259.01
epoch train time: 0:00:02.615444
elapsed time: 0:04:32.183448
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 18:15:29.388821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.82
 ---- batch: 020 ----
mean loss: 261.94
 ---- batch: 030 ----
mean loss: 257.96
 ---- batch: 040 ----
mean loss: 252.16
 ---- batch: 050 ----
mean loss: 264.55
 ---- batch: 060 ----
mean loss: 268.56
 ---- batch: 070 ----
mean loss: 255.46
 ---- batch: 080 ----
mean loss: 256.27
 ---- batch: 090 ----
mean loss: 250.88
train mean loss: 257.36
epoch train time: 0:00:02.592526
elapsed time: 0:04:34.776493
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 18:15:31.981845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.01
 ---- batch: 020 ----
mean loss: 261.26
 ---- batch: 030 ----
mean loss: 253.58
 ---- batch: 040 ----
mean loss: 259.76
 ---- batch: 050 ----
mean loss: 254.32
 ---- batch: 060 ----
mean loss: 257.82
 ---- batch: 070 ----
mean loss: 255.72
 ---- batch: 080 ----
mean loss: 258.35
 ---- batch: 090 ----
mean loss: 257.89
train mean loss: 256.87
epoch train time: 0:00:02.624628
elapsed time: 0:04:37.401635
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 18:15:34.607013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.90
 ---- batch: 020 ----
mean loss: 264.27
 ---- batch: 030 ----
mean loss: 255.29
 ---- batch: 040 ----
mean loss: 258.84
 ---- batch: 050 ----
mean loss: 251.80
 ---- batch: 060 ----
mean loss: 259.95
 ---- batch: 070 ----
mean loss: 259.91
 ---- batch: 080 ----
mean loss: 257.90
 ---- batch: 090 ----
mean loss: 248.20
train mean loss: 255.96
epoch train time: 0:00:02.638953
elapsed time: 0:04:40.041119
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 18:15:37.246489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.64
 ---- batch: 020 ----
mean loss: 250.87
 ---- batch: 030 ----
mean loss: 263.22
 ---- batch: 040 ----
mean loss: 245.52
 ---- batch: 050 ----
mean loss: 248.69
 ---- batch: 060 ----
mean loss: 264.03
 ---- batch: 070 ----
mean loss: 255.94
 ---- batch: 080 ----
mean loss: 249.91
 ---- batch: 090 ----
mean loss: 259.69
train mean loss: 254.10
epoch train time: 0:00:02.632639
elapsed time: 0:04:42.674263
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 18:15:39.879626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.43
 ---- batch: 020 ----
mean loss: 248.82
 ---- batch: 030 ----
mean loss: 252.72
 ---- batch: 040 ----
mean loss: 254.53
 ---- batch: 050 ----
mean loss: 255.52
 ---- batch: 060 ----
mean loss: 252.17
 ---- batch: 070 ----
mean loss: 248.43
 ---- batch: 080 ----
mean loss: 252.84
 ---- batch: 090 ----
mean loss: 254.91
train mean loss: 253.32
epoch train time: 0:00:02.595420
elapsed time: 0:04:45.270156
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 18:15:42.475446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.26
 ---- batch: 020 ----
mean loss: 247.44
 ---- batch: 030 ----
mean loss: 253.28
 ---- batch: 040 ----
mean loss: 251.95
 ---- batch: 050 ----
mean loss: 250.75
 ---- batch: 060 ----
mean loss: 253.94
 ---- batch: 070 ----
mean loss: 258.42
 ---- batch: 080 ----
mean loss: 247.24
 ---- batch: 090 ----
mean loss: 255.08
train mean loss: 252.59
epoch train time: 0:00:02.585235
elapsed time: 0:04:47.855822
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 18:15:45.061192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.01
 ---- batch: 020 ----
mean loss: 248.53
 ---- batch: 030 ----
mean loss: 246.46
 ---- batch: 040 ----
mean loss: 253.34
 ---- batch: 050 ----
mean loss: 249.27
 ---- batch: 060 ----
mean loss: 245.33
 ---- batch: 070 ----
mean loss: 253.05
 ---- batch: 080 ----
mean loss: 255.67
 ---- batch: 090 ----
mean loss: 259.47
train mean loss: 251.06
epoch train time: 0:00:02.640834
elapsed time: 0:04:50.497153
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 18:15:47.702506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.68
 ---- batch: 020 ----
mean loss: 255.04
 ---- batch: 030 ----
mean loss: 246.82
 ---- batch: 040 ----
mean loss: 252.79
 ---- batch: 050 ----
mean loss: 246.21
 ---- batch: 060 ----
mean loss: 254.45
 ---- batch: 070 ----
mean loss: 258.12
 ---- batch: 080 ----
mean loss: 248.61
 ---- batch: 090 ----
mean loss: 244.31
train mean loss: 250.46
epoch train time: 0:00:02.642110
elapsed time: 0:04:53.139728
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 18:15:50.345072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.97
 ---- batch: 020 ----
mean loss: 242.83
 ---- batch: 030 ----
mean loss: 246.82
 ---- batch: 040 ----
mean loss: 251.19
 ---- batch: 050 ----
mean loss: 252.42
 ---- batch: 060 ----
mean loss: 252.38
 ---- batch: 070 ----
mean loss: 247.23
 ---- batch: 080 ----
mean loss: 248.96
 ---- batch: 090 ----
mean loss: 250.36
train mean loss: 249.56
epoch train time: 0:00:02.638184
elapsed time: 0:04:55.778362
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 18:15:52.983731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.50
 ---- batch: 020 ----
mean loss: 244.02
 ---- batch: 030 ----
mean loss: 248.24
 ---- batch: 040 ----
mean loss: 245.61
 ---- batch: 050 ----
mean loss: 260.04
 ---- batch: 060 ----
mean loss: 242.09
 ---- batch: 070 ----
mean loss: 239.05
 ---- batch: 080 ----
mean loss: 245.53
 ---- batch: 090 ----
mean loss: 249.98
train mean loss: 248.39
epoch train time: 0:00:02.616719
elapsed time: 0:04:58.395553
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 18:15:55.600897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.05
 ---- batch: 020 ----
mean loss: 249.04
 ---- batch: 030 ----
mean loss: 244.58
 ---- batch: 040 ----
mean loss: 248.05
 ---- batch: 050 ----
mean loss: 255.09
 ---- batch: 060 ----
mean loss: 252.85
 ---- batch: 070 ----
mean loss: 245.20
 ---- batch: 080 ----
mean loss: 243.15
 ---- batch: 090 ----
mean loss: 244.76
train mean loss: 247.71
epoch train time: 0:00:02.601777
elapsed time: 0:05:00.997870
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 18:15:58.203239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.92
 ---- batch: 020 ----
mean loss: 244.23
 ---- batch: 030 ----
mean loss: 246.17
 ---- batch: 040 ----
mean loss: 247.59
 ---- batch: 050 ----
mean loss: 242.56
 ---- batch: 060 ----
mean loss: 251.75
 ---- batch: 070 ----
mean loss: 243.63
 ---- batch: 080 ----
mean loss: 246.17
 ---- batch: 090 ----
mean loss: 251.30
train mean loss: 246.82
epoch train time: 0:00:02.628294
elapsed time: 0:05:03.626651
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 18:16:00.831998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.51
 ---- batch: 020 ----
mean loss: 249.86
 ---- batch: 030 ----
mean loss: 240.47
 ---- batch: 040 ----
mean loss: 245.98
 ---- batch: 050 ----
mean loss: 240.89
 ---- batch: 060 ----
mean loss: 246.91
 ---- batch: 070 ----
mean loss: 250.99
 ---- batch: 080 ----
mean loss: 238.12
 ---- batch: 090 ----
mean loss: 245.77
train mean loss: 246.09
epoch train time: 0:00:02.648473
elapsed time: 0:05:06.275610
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 18:16:03.480970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.37
 ---- batch: 020 ----
mean loss: 248.52
 ---- batch: 030 ----
mean loss: 244.37
 ---- batch: 040 ----
mean loss: 243.29
 ---- batch: 050 ----
mean loss: 243.96
 ---- batch: 060 ----
mean loss: 252.39
 ---- batch: 070 ----
mean loss: 236.42
 ---- batch: 080 ----
mean loss: 244.94
 ---- batch: 090 ----
mean loss: 240.38
train mean loss: 245.17
epoch train time: 0:00:02.642818
elapsed time: 0:05:08.918909
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 18:16:06.124284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.30
 ---- batch: 020 ----
mean loss: 241.71
 ---- batch: 030 ----
mean loss: 252.10
 ---- batch: 040 ----
mean loss: 241.84
 ---- batch: 050 ----
mean loss: 237.75
 ---- batch: 060 ----
mean loss: 250.53
 ---- batch: 070 ----
mean loss: 241.64
 ---- batch: 080 ----
mean loss: 244.60
 ---- batch: 090 ----
mean loss: 248.29
train mean loss: 244.34
epoch train time: 0:00:02.631067
elapsed time: 0:05:11.550518
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 18:16:08.755867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.11
 ---- batch: 020 ----
mean loss: 250.40
 ---- batch: 030 ----
mean loss: 244.10
 ---- batch: 040 ----
mean loss: 234.45
 ---- batch: 050 ----
mean loss: 247.09
 ---- batch: 060 ----
mean loss: 245.35
 ---- batch: 070 ----
mean loss: 240.90
 ---- batch: 080 ----
mean loss: 242.18
 ---- batch: 090 ----
mean loss: 241.11
train mean loss: 243.56
epoch train time: 0:00:02.631839
elapsed time: 0:05:14.182912
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 18:16:11.388288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.84
 ---- batch: 020 ----
mean loss: 243.59
 ---- batch: 030 ----
mean loss: 241.96
 ---- batch: 040 ----
mean loss: 245.05
 ---- batch: 050 ----
mean loss: 238.40
 ---- batch: 060 ----
mean loss: 246.07
 ---- batch: 070 ----
mean loss: 245.72
 ---- batch: 080 ----
mean loss: 248.30
 ---- batch: 090 ----
mean loss: 239.73
train mean loss: 242.52
epoch train time: 0:00:02.650926
elapsed time: 0:05:16.834342
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 18:16:14.039695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.44
 ---- batch: 020 ----
mean loss: 240.26
 ---- batch: 030 ----
mean loss: 239.13
 ---- batch: 040 ----
mean loss: 239.47
 ---- batch: 050 ----
mean loss: 234.67
 ---- batch: 060 ----
mean loss: 245.10
 ---- batch: 070 ----
mean loss: 244.29
 ---- batch: 080 ----
mean loss: 243.00
 ---- batch: 090 ----
mean loss: 243.06
train mean loss: 242.07
epoch train time: 0:00:02.628152
elapsed time: 0:05:19.462972
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 18:16:16.668345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.97
 ---- batch: 020 ----
mean loss: 240.34
 ---- batch: 030 ----
mean loss: 243.00
 ---- batch: 040 ----
mean loss: 237.14
 ---- batch: 050 ----
mean loss: 243.31
 ---- batch: 060 ----
mean loss: 246.24
 ---- batch: 070 ----
mean loss: 244.95
 ---- batch: 080 ----
mean loss: 238.09
 ---- batch: 090 ----
mean loss: 237.49
train mean loss: 242.12
epoch train time: 0:00:02.622158
elapsed time: 0:05:22.085636
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 18:16:19.290989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.15
 ---- batch: 020 ----
mean loss: 242.78
 ---- batch: 030 ----
mean loss: 245.14
 ---- batch: 040 ----
mean loss: 238.33
 ---- batch: 050 ----
mean loss: 237.84
 ---- batch: 060 ----
mean loss: 245.03
 ---- batch: 070 ----
mean loss: 236.60
 ---- batch: 080 ----
mean loss: 239.13
 ---- batch: 090 ----
mean loss: 240.11
train mean loss: 241.21
epoch train time: 0:00:02.657318
elapsed time: 0:05:24.743434
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 18:16:21.948788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.14
 ---- batch: 020 ----
mean loss: 242.49
 ---- batch: 030 ----
mean loss: 244.92
 ---- batch: 040 ----
mean loss: 247.36
 ---- batch: 050 ----
mean loss: 244.55
 ---- batch: 060 ----
mean loss: 244.96
 ---- batch: 070 ----
mean loss: 246.42
 ---- batch: 080 ----
mean loss: 234.29
 ---- batch: 090 ----
mean loss: 232.96
train mean loss: 240.27
epoch train time: 0:00:02.662729
elapsed time: 0:05:27.406637
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 18:16:24.611986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.70
 ---- batch: 020 ----
mean loss: 241.68
 ---- batch: 030 ----
mean loss: 231.91
 ---- batch: 040 ----
mean loss: 235.15
 ---- batch: 050 ----
mean loss: 234.17
 ---- batch: 060 ----
mean loss: 245.16
 ---- batch: 070 ----
mean loss: 250.87
 ---- batch: 080 ----
mean loss: 240.72
 ---- batch: 090 ----
mean loss: 241.63
train mean loss: 240.26
epoch train time: 0:00:02.673585
elapsed time: 0:05:30.080834
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 18:16:27.286235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.49
 ---- batch: 020 ----
mean loss: 233.95
 ---- batch: 030 ----
mean loss: 239.78
 ---- batch: 040 ----
mean loss: 240.28
 ---- batch: 050 ----
mean loss: 238.15
 ---- batch: 060 ----
mean loss: 245.75
 ---- batch: 070 ----
mean loss: 230.30
 ---- batch: 080 ----
mean loss: 243.80
 ---- batch: 090 ----
mean loss: 243.85
train mean loss: 240.08
epoch train time: 0:00:02.660513
elapsed time: 0:05:32.741929
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 18:16:29.947310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.33
 ---- batch: 020 ----
mean loss: 238.54
 ---- batch: 030 ----
mean loss: 232.74
 ---- batch: 040 ----
mean loss: 240.93
 ---- batch: 050 ----
mean loss: 234.11
 ---- batch: 060 ----
mean loss: 241.45
 ---- batch: 070 ----
mean loss: 242.72
 ---- batch: 080 ----
mean loss: 232.66
 ---- batch: 090 ----
mean loss: 238.78
train mean loss: 238.87
epoch train time: 0:00:02.651059
elapsed time: 0:05:35.393507
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 18:16:32.598866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.19
 ---- batch: 020 ----
mean loss: 240.80
 ---- batch: 030 ----
mean loss: 232.34
 ---- batch: 040 ----
mean loss: 229.35
 ---- batch: 050 ----
mean loss: 247.88
 ---- batch: 060 ----
mean loss: 237.56
 ---- batch: 070 ----
mean loss: 245.51
 ---- batch: 080 ----
mean loss: 235.28
 ---- batch: 090 ----
mean loss: 235.57
train mean loss: 238.47
epoch train time: 0:00:02.701980
elapsed time: 0:05:38.095987
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 18:16:35.301388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.98
 ---- batch: 020 ----
mean loss: 229.65
 ---- batch: 030 ----
mean loss: 239.47
 ---- batch: 040 ----
mean loss: 236.03
 ---- batch: 050 ----
mean loss: 237.30
 ---- batch: 060 ----
mean loss: 242.29
 ---- batch: 070 ----
mean loss: 248.65
 ---- batch: 080 ----
mean loss: 246.63
 ---- batch: 090 ----
mean loss: 244.06
train mean loss: 237.56
epoch train time: 0:00:02.666535
elapsed time: 0:05:40.762997
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 18:16:37.968416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.97
 ---- batch: 020 ----
mean loss: 230.25
 ---- batch: 030 ----
mean loss: 239.08
 ---- batch: 040 ----
mean loss: 237.20
 ---- batch: 050 ----
mean loss: 237.22
 ---- batch: 060 ----
mean loss: 232.59
 ---- batch: 070 ----
mean loss: 243.17
 ---- batch: 080 ----
mean loss: 238.69
 ---- batch: 090 ----
mean loss: 246.10
train mean loss: 237.75
epoch train time: 0:00:02.624096
elapsed time: 0:05:43.387661
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 18:16:40.593007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.02
 ---- batch: 020 ----
mean loss: 239.45
 ---- batch: 030 ----
mean loss: 238.57
 ---- batch: 040 ----
mean loss: 242.55
 ---- batch: 050 ----
mean loss: 241.39
 ---- batch: 060 ----
mean loss: 240.41
 ---- batch: 070 ----
mean loss: 232.85
 ---- batch: 080 ----
mean loss: 238.27
 ---- batch: 090 ----
mean loss: 226.24
train mean loss: 237.51
epoch train time: 0:00:02.640123
elapsed time: 0:05:46.028244
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 18:16:43.233638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.17
 ---- batch: 020 ----
mean loss: 235.96
 ---- batch: 030 ----
mean loss: 232.06
 ---- batch: 040 ----
mean loss: 237.50
 ---- batch: 050 ----
mean loss: 226.68
 ---- batch: 060 ----
mean loss: 239.51
 ---- batch: 070 ----
mean loss: 237.53
 ---- batch: 080 ----
mean loss: 245.85
 ---- batch: 090 ----
mean loss: 235.48
train mean loss: 236.92
epoch train time: 0:00:02.658959
elapsed time: 0:05:48.687714
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 18:16:45.893115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.63
 ---- batch: 020 ----
mean loss: 231.86
 ---- batch: 030 ----
mean loss: 234.23
 ---- batch: 040 ----
mean loss: 243.79
 ---- batch: 050 ----
mean loss: 236.47
 ---- batch: 060 ----
mean loss: 239.68
 ---- batch: 070 ----
mean loss: 231.69
 ---- batch: 080 ----
mean loss: 240.46
 ---- batch: 090 ----
mean loss: 238.09
train mean loss: 236.21
epoch train time: 0:00:02.655207
elapsed time: 0:05:51.343475
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 18:16:48.548820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.82
 ---- batch: 020 ----
mean loss: 236.81
 ---- batch: 030 ----
mean loss: 233.38
 ---- batch: 040 ----
mean loss: 236.52
 ---- batch: 050 ----
mean loss: 236.52
 ---- batch: 060 ----
mean loss: 235.76
 ---- batch: 070 ----
mean loss: 238.38
 ---- batch: 080 ----
mean loss: 244.75
 ---- batch: 090 ----
mean loss: 235.93
train mean loss: 235.75
epoch train time: 0:00:02.633077
elapsed time: 0:05:53.977005
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 18:16:51.182353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.01
 ---- batch: 020 ----
mean loss: 236.50
 ---- batch: 030 ----
mean loss: 229.26
 ---- batch: 040 ----
mean loss: 238.86
 ---- batch: 050 ----
mean loss: 241.11
 ---- batch: 060 ----
mean loss: 236.14
 ---- batch: 070 ----
mean loss: 227.20
 ---- batch: 080 ----
mean loss: 237.00
 ---- batch: 090 ----
mean loss: 230.20
train mean loss: 235.33
epoch train time: 0:00:02.624660
elapsed time: 0:05:56.602147
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 18:16:53.807507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.04
 ---- batch: 020 ----
mean loss: 228.06
 ---- batch: 030 ----
mean loss: 230.77
 ---- batch: 040 ----
mean loss: 237.36
 ---- batch: 050 ----
mean loss: 233.95
 ---- batch: 060 ----
mean loss: 236.56
 ---- batch: 070 ----
mean loss: 233.43
 ---- batch: 080 ----
mean loss: 238.19
 ---- batch: 090 ----
mean loss: 234.13
train mean loss: 234.99
epoch train time: 0:00:02.599597
elapsed time: 0:05:59.202212
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 18:16:56.407555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.74
 ---- batch: 020 ----
mean loss: 229.47
 ---- batch: 030 ----
mean loss: 230.82
 ---- batch: 040 ----
mean loss: 238.92
 ---- batch: 050 ----
mean loss: 232.99
 ---- batch: 060 ----
mean loss: 240.96
 ---- batch: 070 ----
mean loss: 233.79
 ---- batch: 080 ----
mean loss: 236.28
 ---- batch: 090 ----
mean loss: 237.67
train mean loss: 234.55
epoch train time: 0:00:02.662072
elapsed time: 0:06:01.864769
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 18:16:59.070148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.05
 ---- batch: 020 ----
mean loss: 234.93
 ---- batch: 030 ----
mean loss: 223.88
 ---- batch: 040 ----
mean loss: 228.22
 ---- batch: 050 ----
mean loss: 236.71
 ---- batch: 060 ----
mean loss: 235.28
 ---- batch: 070 ----
mean loss: 235.49
 ---- batch: 080 ----
mean loss: 242.59
 ---- batch: 090 ----
mean loss: 238.16
train mean loss: 234.10
epoch train time: 0:00:02.667604
elapsed time: 0:06:04.532925
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 18:17:01.738275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.12
 ---- batch: 020 ----
mean loss: 232.35
 ---- batch: 030 ----
mean loss: 239.81
 ---- batch: 040 ----
mean loss: 234.73
 ---- batch: 050 ----
mean loss: 234.36
 ---- batch: 060 ----
mean loss: 229.53
 ---- batch: 070 ----
mean loss: 233.44
 ---- batch: 080 ----
mean loss: 237.56
 ---- batch: 090 ----
mean loss: 241.16
train mean loss: 234.17
epoch train time: 0:00:02.636833
elapsed time: 0:06:07.170328
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 18:17:04.375630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.22
 ---- batch: 020 ----
mean loss: 226.42
 ---- batch: 030 ----
mean loss: 231.53
 ---- batch: 040 ----
mean loss: 237.32
 ---- batch: 050 ----
mean loss: 235.66
 ---- batch: 060 ----
mean loss: 238.01
 ---- batch: 070 ----
mean loss: 224.16
 ---- batch: 080 ----
mean loss: 232.21
 ---- batch: 090 ----
mean loss: 240.32
train mean loss: 233.46
epoch train time: 0:00:02.623719
elapsed time: 0:06:09.794482
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 18:17:06.999823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.84
 ---- batch: 020 ----
mean loss: 225.57
 ---- batch: 030 ----
mean loss: 230.31
 ---- batch: 040 ----
mean loss: 232.44
 ---- batch: 050 ----
mean loss: 231.02
 ---- batch: 060 ----
mean loss: 230.31
 ---- batch: 070 ----
mean loss: 235.79
 ---- batch: 080 ----
mean loss: 236.70
 ---- batch: 090 ----
mean loss: 240.50
train mean loss: 233.13
epoch train time: 0:00:02.653417
elapsed time: 0:06:12.448401
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 18:17:09.653772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.54
 ---- batch: 020 ----
mean loss: 232.16
 ---- batch: 030 ----
mean loss: 225.73
 ---- batch: 040 ----
mean loss: 228.57
 ---- batch: 050 ----
mean loss: 227.33
 ---- batch: 060 ----
mean loss: 226.35
 ---- batch: 070 ----
mean loss: 235.95
 ---- batch: 080 ----
mean loss: 241.05
 ---- batch: 090 ----
mean loss: 236.47
train mean loss: 232.68
epoch train time: 0:00:02.663520
elapsed time: 0:06:15.112425
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 18:17:12.317796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.79
 ---- batch: 020 ----
mean loss: 232.00
 ---- batch: 030 ----
mean loss: 232.92
 ---- batch: 040 ----
mean loss: 241.76
 ---- batch: 050 ----
mean loss: 233.29
 ---- batch: 060 ----
mean loss: 228.01
 ---- batch: 070 ----
mean loss: 230.61
 ---- batch: 080 ----
mean loss: 230.62
 ---- batch: 090 ----
mean loss: 233.64
train mean loss: 232.58
epoch train time: 0:00:02.660213
elapsed time: 0:06:17.773230
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 18:17:14.978610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.63
 ---- batch: 020 ----
mean loss: 226.93
 ---- batch: 030 ----
mean loss: 229.46
 ---- batch: 040 ----
mean loss: 233.07
 ---- batch: 050 ----
mean loss: 231.38
 ---- batch: 060 ----
mean loss: 228.78
 ---- batch: 070 ----
mean loss: 233.54
 ---- batch: 080 ----
mean loss: 240.21
 ---- batch: 090 ----
mean loss: 231.68
train mean loss: 232.23
epoch train time: 0:00:02.630478
elapsed time: 0:06:20.404237
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 18:17:17.609622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.37
 ---- batch: 020 ----
mean loss: 230.32
 ---- batch: 030 ----
mean loss: 228.15
 ---- batch: 040 ----
mean loss: 226.66
 ---- batch: 050 ----
mean loss: 233.12
 ---- batch: 060 ----
mean loss: 234.32
 ---- batch: 070 ----
mean loss: 229.26
 ---- batch: 080 ----
mean loss: 237.15
 ---- batch: 090 ----
mean loss: 231.80
train mean loss: 231.80
epoch train time: 0:00:02.600636
elapsed time: 0:06:23.005380
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 18:17:20.210731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.70
 ---- batch: 020 ----
mean loss: 221.14
 ---- batch: 030 ----
mean loss: 226.47
 ---- batch: 040 ----
mean loss: 232.42
 ---- batch: 050 ----
mean loss: 235.49
 ---- batch: 060 ----
mean loss: 225.74
 ---- batch: 070 ----
mean loss: 239.91
 ---- batch: 080 ----
mean loss: 234.17
 ---- batch: 090 ----
mean loss: 232.75
train mean loss: 231.93
epoch train time: 0:00:02.649148
elapsed time: 0:06:25.655030
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 18:17:22.860402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.08
 ---- batch: 020 ----
mean loss: 229.53
 ---- batch: 030 ----
mean loss: 233.84
 ---- batch: 040 ----
mean loss: 229.18
 ---- batch: 050 ----
mean loss: 228.06
 ---- batch: 060 ----
mean loss: 238.68
 ---- batch: 070 ----
mean loss: 232.78
 ---- batch: 080 ----
mean loss: 231.41
 ---- batch: 090 ----
mean loss: 227.68
train mean loss: 231.57
epoch train time: 0:00:02.650470
elapsed time: 0:06:28.305975
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 18:17:25.511327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.58
 ---- batch: 020 ----
mean loss: 229.53
 ---- batch: 030 ----
mean loss: 224.12
 ---- batch: 040 ----
mean loss: 234.23
 ---- batch: 050 ----
mean loss: 228.31
 ---- batch: 060 ----
mean loss: 228.45
 ---- batch: 070 ----
mean loss: 229.14
 ---- batch: 080 ----
mean loss: 232.45
 ---- batch: 090 ----
mean loss: 233.42
train mean loss: 230.86
epoch train time: 0:00:02.640560
elapsed time: 0:06:30.947081
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 18:17:28.152437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.89
 ---- batch: 020 ----
mean loss: 227.27
 ---- batch: 030 ----
mean loss: 234.21
 ---- batch: 040 ----
mean loss: 225.18
 ---- batch: 050 ----
mean loss: 229.12
 ---- batch: 060 ----
mean loss: 226.88
 ---- batch: 070 ----
mean loss: 228.01
 ---- batch: 080 ----
mean loss: 232.55
 ---- batch: 090 ----
mean loss: 236.55
train mean loss: 230.63
epoch train time: 0:00:02.615149
elapsed time: 0:06:33.562698
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 18:17:30.768042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.06
 ---- batch: 020 ----
mean loss: 229.39
 ---- batch: 030 ----
mean loss: 229.43
 ---- batch: 040 ----
mean loss: 238.80
 ---- batch: 050 ----
mean loss: 230.13
 ---- batch: 060 ----
mean loss: 234.83
 ---- batch: 070 ----
mean loss: 233.84
 ---- batch: 080 ----
mean loss: 229.73
 ---- batch: 090 ----
mean loss: 223.33
train mean loss: 230.55
epoch train time: 0:00:02.604015
elapsed time: 0:06:36.167166
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 18:17:33.372523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.57
 ---- batch: 020 ----
mean loss: 229.05
 ---- batch: 030 ----
mean loss: 236.07
 ---- batch: 040 ----
mean loss: 227.30
 ---- batch: 050 ----
mean loss: 237.49
 ---- batch: 060 ----
mean loss: 226.63
 ---- batch: 070 ----
mean loss: 226.53
 ---- batch: 080 ----
mean loss: 230.75
 ---- batch: 090 ----
mean loss: 235.28
train mean loss: 230.37
epoch train time: 0:00:02.636703
elapsed time: 0:06:38.804355
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 18:17:36.009731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.86
 ---- batch: 020 ----
mean loss: 227.92
 ---- batch: 030 ----
mean loss: 234.06
 ---- batch: 040 ----
mean loss: 225.07
 ---- batch: 050 ----
mean loss: 226.67
 ---- batch: 060 ----
mean loss: 230.03
 ---- batch: 070 ----
mean loss: 231.94
 ---- batch: 080 ----
mean loss: 239.45
 ---- batch: 090 ----
mean loss: 224.84
train mean loss: 229.19
epoch train time: 0:00:02.652569
elapsed time: 0:06:41.457456
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 18:17:38.662834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.46
 ---- batch: 020 ----
mean loss: 229.98
 ---- batch: 030 ----
mean loss: 230.76
 ---- batch: 040 ----
mean loss: 231.33
 ---- batch: 050 ----
mean loss: 230.06
 ---- batch: 060 ----
mean loss: 230.30
 ---- batch: 070 ----
mean loss: 223.76
 ---- batch: 080 ----
mean loss: 228.39
 ---- batch: 090 ----
mean loss: 237.28
train mean loss: 229.76
epoch train time: 0:00:02.645131
elapsed time: 0:06:44.103070
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 18:17:41.308415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.61
 ---- batch: 020 ----
mean loss: 231.99
 ---- batch: 030 ----
mean loss: 234.61
 ---- batch: 040 ----
mean loss: 221.82
 ---- batch: 050 ----
mean loss: 234.31
 ---- batch: 060 ----
mean loss: 226.98
 ---- batch: 070 ----
mean loss: 229.22
 ---- batch: 080 ----
mean loss: 225.42
 ---- batch: 090 ----
mean loss: 227.77
train mean loss: 229.45
epoch train time: 0:00:02.600746
elapsed time: 0:06:46.704326
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 18:17:43.909701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.75
 ---- batch: 020 ----
mean loss: 229.17
 ---- batch: 030 ----
mean loss: 230.85
 ---- batch: 040 ----
mean loss: 230.43
 ---- batch: 050 ----
mean loss: 233.19
 ---- batch: 060 ----
mean loss: 229.40
 ---- batch: 070 ----
mean loss: 226.72
 ---- batch: 080 ----
mean loss: 227.37
 ---- batch: 090 ----
mean loss: 224.62
train mean loss: 228.96
epoch train time: 0:00:02.641105
elapsed time: 0:06:49.345966
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 18:17:46.551310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.68
 ---- batch: 020 ----
mean loss: 233.21
 ---- batch: 030 ----
mean loss: 218.84
 ---- batch: 040 ----
mean loss: 231.95
 ---- batch: 050 ----
mean loss: 224.81
 ---- batch: 060 ----
mean loss: 223.56
 ---- batch: 070 ----
mean loss: 231.18
 ---- batch: 080 ----
mean loss: 225.06
 ---- batch: 090 ----
mean loss: 223.63
train mean loss: 228.44
epoch train time: 0:00:02.601892
elapsed time: 0:06:51.948361
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 18:17:49.153749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.93
 ---- batch: 020 ----
mean loss: 237.38
 ---- batch: 030 ----
mean loss: 226.25
 ---- batch: 040 ----
mean loss: 228.79
 ---- batch: 050 ----
mean loss: 218.39
 ---- batch: 060 ----
mean loss: 235.73
 ---- batch: 070 ----
mean loss: 234.96
 ---- batch: 080 ----
mean loss: 224.06
 ---- batch: 090 ----
mean loss: 228.58
train mean loss: 228.29
epoch train time: 0:00:02.656254
elapsed time: 0:06:54.605158
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 18:17:51.810521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.59
 ---- batch: 020 ----
mean loss: 231.39
 ---- batch: 030 ----
mean loss: 222.49
 ---- batch: 040 ----
mean loss: 232.11
 ---- batch: 050 ----
mean loss: 228.23
 ---- batch: 060 ----
mean loss: 234.44
 ---- batch: 070 ----
mean loss: 223.05
 ---- batch: 080 ----
mean loss: 233.44
 ---- batch: 090 ----
mean loss: 227.44
train mean loss: 228.05
epoch train time: 0:00:02.659745
elapsed time: 0:06:57.265460
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 18:17:54.470849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.20
 ---- batch: 020 ----
mean loss: 228.59
 ---- batch: 030 ----
mean loss: 228.07
 ---- batch: 040 ----
mean loss: 228.84
 ---- batch: 050 ----
mean loss: 226.47
 ---- batch: 060 ----
mean loss: 227.91
 ---- batch: 070 ----
mean loss: 225.03
 ---- batch: 080 ----
mean loss: 225.43
 ---- batch: 090 ----
mean loss: 225.97
train mean loss: 227.90
epoch train time: 0:00:02.659072
elapsed time: 0:06:59.925070
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 18:17:57.130447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.25
 ---- batch: 020 ----
mean loss: 232.86
 ---- batch: 030 ----
mean loss: 225.76
 ---- batch: 040 ----
mean loss: 226.20
 ---- batch: 050 ----
mean loss: 227.89
 ---- batch: 060 ----
mean loss: 230.40
 ---- batch: 070 ----
mean loss: 218.21
 ---- batch: 080 ----
mean loss: 231.69
 ---- batch: 090 ----
mean loss: 230.75
train mean loss: 227.85
epoch train time: 0:00:02.597575
elapsed time: 0:07:02.523137
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 18:17:59.728523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.30
 ---- batch: 020 ----
mean loss: 225.04
 ---- batch: 030 ----
mean loss: 227.90
 ---- batch: 040 ----
mean loss: 230.89
 ---- batch: 050 ----
mean loss: 223.62
 ---- batch: 060 ----
mean loss: 222.77
 ---- batch: 070 ----
mean loss: 228.44
 ---- batch: 080 ----
mean loss: 227.95
 ---- batch: 090 ----
mean loss: 227.62
train mean loss: 227.12
epoch train time: 0:00:02.596236
elapsed time: 0:07:05.120018
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 18:18:02.325265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.31
 ---- batch: 020 ----
mean loss: 225.80
 ---- batch: 030 ----
mean loss: 226.41
 ---- batch: 040 ----
mean loss: 225.95
 ---- batch: 050 ----
mean loss: 232.56
 ---- batch: 060 ----
mean loss: 234.50
 ---- batch: 070 ----
mean loss: 230.14
 ---- batch: 080 ----
mean loss: 221.04
 ---- batch: 090 ----
mean loss: 231.14
train mean loss: 227.39
epoch train time: 0:00:02.615466
elapsed time: 0:07:07.735894
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 18:18:04.941248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.64
 ---- batch: 020 ----
mean loss: 233.69
 ---- batch: 030 ----
mean loss: 222.31
 ---- batch: 040 ----
mean loss: 232.81
 ---- batch: 050 ----
mean loss: 219.84
 ---- batch: 060 ----
mean loss: 229.21
 ---- batch: 070 ----
mean loss: 218.02
 ---- batch: 080 ----
mean loss: 229.07
 ---- batch: 090 ----
mean loss: 222.92
train mean loss: 226.69
epoch train time: 0:00:02.663654
elapsed time: 0:07:10.400014
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 18:18:07.605418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.56
 ---- batch: 020 ----
mean loss: 228.37
 ---- batch: 030 ----
mean loss: 224.52
 ---- batch: 040 ----
mean loss: 224.68
 ---- batch: 050 ----
mean loss: 222.98
 ---- batch: 060 ----
mean loss: 230.41
 ---- batch: 070 ----
mean loss: 224.77
 ---- batch: 080 ----
mean loss: 226.82
 ---- batch: 090 ----
mean loss: 238.21
train mean loss: 226.64
epoch train time: 0:00:02.667519
elapsed time: 0:07:13.068160
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 18:18:10.273563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.34
 ---- batch: 020 ----
mean loss: 234.96
 ---- batch: 030 ----
mean loss: 228.16
 ---- batch: 040 ----
mean loss: 222.64
 ---- batch: 050 ----
mean loss: 227.25
 ---- batch: 060 ----
mean loss: 223.20
 ---- batch: 070 ----
mean loss: 226.25
 ---- batch: 080 ----
mean loss: 224.47
 ---- batch: 090 ----
mean loss: 228.70
train mean loss: 226.07
epoch train time: 0:00:02.651628
elapsed time: 0:07:15.720297
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 18:18:12.925687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.61
 ---- batch: 020 ----
mean loss: 219.03
 ---- batch: 030 ----
mean loss: 231.75
 ---- batch: 040 ----
mean loss: 234.02
 ---- batch: 050 ----
mean loss: 227.64
 ---- batch: 060 ----
mean loss: 222.88
 ---- batch: 070 ----
mean loss: 221.70
 ---- batch: 080 ----
mean loss: 223.73
 ---- batch: 090 ----
mean loss: 221.48
train mean loss: 226.21
epoch train time: 0:00:02.641317
elapsed time: 0:07:18.362138
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 18:18:15.567491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.32
 ---- batch: 020 ----
mean loss: 225.63
 ---- batch: 030 ----
mean loss: 220.73
 ---- batch: 040 ----
mean loss: 227.67
 ---- batch: 050 ----
mean loss: 226.94
 ---- batch: 060 ----
mean loss: 230.70
 ---- batch: 070 ----
mean loss: 223.34
 ---- batch: 080 ----
mean loss: 227.69
 ---- batch: 090 ----
mean loss: 224.85
train mean loss: 225.77
epoch train time: 0:00:02.591211
elapsed time: 0:07:20.953801
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 18:18:18.159192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.57
 ---- batch: 020 ----
mean loss: 228.55
 ---- batch: 030 ----
mean loss: 226.68
 ---- batch: 040 ----
mean loss: 218.87
 ---- batch: 050 ----
mean loss: 226.47
 ---- batch: 060 ----
mean loss: 234.07
 ---- batch: 070 ----
mean loss: 223.91
 ---- batch: 080 ----
mean loss: 223.17
 ---- batch: 090 ----
mean loss: 229.06
train mean loss: 225.88
epoch train time: 0:00:02.661590
elapsed time: 0:07:23.615914
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 18:18:20.821275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.63
 ---- batch: 020 ----
mean loss: 219.06
 ---- batch: 030 ----
mean loss: 228.44
 ---- batch: 040 ----
mean loss: 230.90
 ---- batch: 050 ----
mean loss: 216.26
 ---- batch: 060 ----
mean loss: 227.57
 ---- batch: 070 ----
mean loss: 230.92
 ---- batch: 080 ----
mean loss: 236.04
 ---- batch: 090 ----
mean loss: 222.51
train mean loss: 225.67
epoch train time: 0:00:02.650991
elapsed time: 0:07:26.267392
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 18:18:23.472745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.39
 ---- batch: 020 ----
mean loss: 231.81
 ---- batch: 030 ----
mean loss: 224.07
 ---- batch: 040 ----
mean loss: 226.18
 ---- batch: 050 ----
mean loss: 229.85
 ---- batch: 060 ----
mean loss: 223.36
 ---- batch: 070 ----
mean loss: 223.90
 ---- batch: 080 ----
mean loss: 221.60
 ---- batch: 090 ----
mean loss: 222.52
train mean loss: 225.30
epoch train time: 0:00:02.669244
elapsed time: 0:07:28.937226
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 18:18:26.142575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.66
 ---- batch: 020 ----
mean loss: 218.87
 ---- batch: 030 ----
mean loss: 225.53
 ---- batch: 040 ----
mean loss: 221.28
 ---- batch: 050 ----
mean loss: 219.13
 ---- batch: 060 ----
mean loss: 221.74
 ---- batch: 070 ----
mean loss: 225.42
 ---- batch: 080 ----
mean loss: 232.17
 ---- batch: 090 ----
mean loss: 223.57
train mean loss: 225.43
epoch train time: 0:00:02.618880
elapsed time: 0:07:31.556606
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 18:18:28.761970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.64
 ---- batch: 020 ----
mean loss: 219.48
 ---- batch: 030 ----
mean loss: 231.25
 ---- batch: 040 ----
mean loss: 229.34
 ---- batch: 050 ----
mean loss: 229.12
 ---- batch: 060 ----
mean loss: 220.22
 ---- batch: 070 ----
mean loss: 224.65
 ---- batch: 080 ----
mean loss: 220.95
 ---- batch: 090 ----
mean loss: 225.01
train mean loss: 224.85
epoch train time: 0:00:02.583023
elapsed time: 0:07:34.140137
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 18:18:31.345496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.72
 ---- batch: 020 ----
mean loss: 223.74
 ---- batch: 030 ----
mean loss: 222.01
 ---- batch: 040 ----
mean loss: 226.10
 ---- batch: 050 ----
mean loss: 221.28
 ---- batch: 060 ----
mean loss: 224.17
 ---- batch: 070 ----
mean loss: 228.00
 ---- batch: 080 ----
mean loss: 228.90
 ---- batch: 090 ----
mean loss: 223.41
train mean loss: 224.83
epoch train time: 0:00:02.633700
elapsed time: 0:07:36.774338
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 18:18:33.979713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.95
 ---- batch: 020 ----
mean loss: 220.15
 ---- batch: 030 ----
mean loss: 230.64
 ---- batch: 040 ----
mean loss: 219.31
 ---- batch: 050 ----
mean loss: 217.58
 ---- batch: 060 ----
mean loss: 227.33
 ---- batch: 070 ----
mean loss: 231.90
 ---- batch: 080 ----
mean loss: 222.17
 ---- batch: 090 ----
mean loss: 225.90
train mean loss: 224.33
epoch train time: 0:00:02.635346
elapsed time: 0:07:39.410187
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 18:18:36.615561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.01
 ---- batch: 020 ----
mean loss: 227.81
 ---- batch: 030 ----
mean loss: 233.27
 ---- batch: 040 ----
mean loss: 227.48
 ---- batch: 050 ----
mean loss: 218.06
 ---- batch: 060 ----
mean loss: 227.80
 ---- batch: 070 ----
mean loss: 225.08
 ---- batch: 080 ----
mean loss: 225.55
 ---- batch: 090 ----
mean loss: 217.14
train mean loss: 224.16
epoch train time: 0:00:02.636811
elapsed time: 0:07:42.047489
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 18:18:39.252840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.34
 ---- batch: 020 ----
mean loss: 215.97
 ---- batch: 030 ----
mean loss: 224.84
 ---- batch: 040 ----
mean loss: 225.69
 ---- batch: 050 ----
mean loss: 226.79
 ---- batch: 060 ----
mean loss: 226.81
 ---- batch: 070 ----
mean loss: 226.86
 ---- batch: 080 ----
mean loss: 218.61
 ---- batch: 090 ----
mean loss: 224.96
train mean loss: 224.30
epoch train time: 0:00:02.619619
elapsed time: 0:07:44.667570
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 18:18:41.872924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.74
 ---- batch: 020 ----
mean loss: 221.96
 ---- batch: 030 ----
mean loss: 223.39
 ---- batch: 040 ----
mean loss: 217.79
 ---- batch: 050 ----
mean loss: 231.72
 ---- batch: 060 ----
mean loss: 227.30
 ---- batch: 070 ----
mean loss: 224.04
 ---- batch: 080 ----
mean loss: 215.90
 ---- batch: 090 ----
mean loss: 228.42
train mean loss: 223.79
epoch train time: 0:00:02.603153
elapsed time: 0:07:47.271213
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 18:18:44.476572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.77
 ---- batch: 020 ----
mean loss: 220.99
 ---- batch: 030 ----
mean loss: 216.67
 ---- batch: 040 ----
mean loss: 234.13
 ---- batch: 050 ----
mean loss: 224.59
 ---- batch: 060 ----
mean loss: 233.94
 ---- batch: 070 ----
mean loss: 218.67
 ---- batch: 080 ----
mean loss: 213.73
 ---- batch: 090 ----
mean loss: 226.83
train mean loss: 223.73
epoch train time: 0:00:02.637312
elapsed time: 0:07:49.909006
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 18:18:47.114362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.13
 ---- batch: 020 ----
mean loss: 222.14
 ---- batch: 030 ----
mean loss: 225.82
 ---- batch: 040 ----
mean loss: 218.39
 ---- batch: 050 ----
mean loss: 224.62
 ---- batch: 060 ----
mean loss: 219.43
 ---- batch: 070 ----
mean loss: 213.92
 ---- batch: 080 ----
mean loss: 225.64
 ---- batch: 090 ----
mean loss: 228.16
train mean loss: 223.71
epoch train time: 0:00:02.633025
elapsed time: 0:07:52.542632
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 18:18:49.748860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.67
 ---- batch: 020 ----
mean loss: 222.95
 ---- batch: 030 ----
mean loss: 228.49
 ---- batch: 040 ----
mean loss: 213.51
 ---- batch: 050 ----
mean loss: 223.43
 ---- batch: 060 ----
mean loss: 220.04
 ---- batch: 070 ----
mean loss: 226.24
 ---- batch: 080 ----
mean loss: 222.10
 ---- batch: 090 ----
mean loss: 224.31
train mean loss: 223.42
epoch train time: 0:00:02.624642
elapsed time: 0:07:55.168677
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 18:18:52.374032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.91
 ---- batch: 020 ----
mean loss: 221.20
 ---- batch: 030 ----
mean loss: 214.15
 ---- batch: 040 ----
mean loss: 221.20
 ---- batch: 050 ----
mean loss: 225.72
 ---- batch: 060 ----
mean loss: 229.46
 ---- batch: 070 ----
mean loss: 219.55
 ---- batch: 080 ----
mean loss: 227.16
 ---- batch: 090 ----
mean loss: 219.62
train mean loss: 223.32
epoch train time: 0:00:02.601033
elapsed time: 0:07:57.770226
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 18:18:54.975593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.89
 ---- batch: 020 ----
mean loss: 219.89
 ---- batch: 030 ----
mean loss: 220.13
 ---- batch: 040 ----
mean loss: 226.64
 ---- batch: 050 ----
mean loss: 218.77
 ---- batch: 060 ----
mean loss: 225.49
 ---- batch: 070 ----
mean loss: 227.62
 ---- batch: 080 ----
mean loss: 226.23
 ---- batch: 090 ----
mean loss: 217.81
train mean loss: 222.87
epoch train time: 0:00:02.608701
elapsed time: 0:08:00.379411
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 18:18:57.584781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.93
 ---- batch: 020 ----
mean loss: 224.70
 ---- batch: 030 ----
mean loss: 220.53
 ---- batch: 040 ----
mean loss: 225.29
 ---- batch: 050 ----
mean loss: 227.57
 ---- batch: 060 ----
mean loss: 220.73
 ---- batch: 070 ----
mean loss: 226.35
 ---- batch: 080 ----
mean loss: 213.60
 ---- batch: 090 ----
mean loss: 219.93
train mean loss: 222.58
epoch train time: 0:00:02.596587
elapsed time: 0:08:02.976493
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 18:19:00.181849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.94
 ---- batch: 020 ----
mean loss: 218.08
 ---- batch: 030 ----
mean loss: 213.19
 ---- batch: 040 ----
mean loss: 222.64
 ---- batch: 050 ----
mean loss: 220.65
 ---- batch: 060 ----
mean loss: 226.41
 ---- batch: 070 ----
mean loss: 231.52
 ---- batch: 080 ----
mean loss: 219.06
 ---- batch: 090 ----
mean loss: 228.77
train mean loss: 222.53
epoch train time: 0:00:02.630208
elapsed time: 0:08:05.607216
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 18:19:02.812561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.12
 ---- batch: 020 ----
mean loss: 211.36
 ---- batch: 030 ----
mean loss: 225.51
 ---- batch: 040 ----
mean loss: 228.83
 ---- batch: 050 ----
mean loss: 219.88
 ---- batch: 060 ----
mean loss: 216.02
 ---- batch: 070 ----
mean loss: 221.84
 ---- batch: 080 ----
mean loss: 224.24
 ---- batch: 090 ----
mean loss: 219.95
train mean loss: 222.42
epoch train time: 0:00:02.634414
elapsed time: 0:08:08.242077
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 18:19:05.447428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.64
 ---- batch: 020 ----
mean loss: 218.09
 ---- batch: 030 ----
mean loss: 229.74
 ---- batch: 040 ----
mean loss: 216.09
 ---- batch: 050 ----
mean loss: 217.71
 ---- batch: 060 ----
mean loss: 226.32
 ---- batch: 070 ----
mean loss: 224.96
 ---- batch: 080 ----
mean loss: 226.82
 ---- batch: 090 ----
mean loss: 221.01
train mean loss: 222.13
epoch train time: 0:00:02.649680
elapsed time: 0:08:10.892368
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 18:19:08.097594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.59
 ---- batch: 020 ----
mean loss: 225.54
 ---- batch: 030 ----
mean loss: 232.85
 ---- batch: 040 ----
mean loss: 216.52
 ---- batch: 050 ----
mean loss: 217.37
 ---- batch: 060 ----
mean loss: 226.29
 ---- batch: 070 ----
mean loss: 216.50
 ---- batch: 080 ----
mean loss: 223.26
 ---- batch: 090 ----
mean loss: 222.61
train mean loss: 221.98
epoch train time: 0:00:02.593442
elapsed time: 0:08:13.486158
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 18:19:10.691551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.22
 ---- batch: 020 ----
mean loss: 213.94
 ---- batch: 030 ----
mean loss: 221.54
 ---- batch: 040 ----
mean loss: 225.57
 ---- batch: 050 ----
mean loss: 224.50
 ---- batch: 060 ----
mean loss: 224.07
 ---- batch: 070 ----
mean loss: 226.59
 ---- batch: 080 ----
mean loss: 215.93
 ---- batch: 090 ----
mean loss: 218.27
train mean loss: 221.66
epoch train time: 0:00:02.599828
elapsed time: 0:08:16.086508
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 18:19:13.291852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.47
 ---- batch: 020 ----
mean loss: 223.02
 ---- batch: 030 ----
mean loss: 217.08
 ---- batch: 040 ----
mean loss: 224.51
 ---- batch: 050 ----
mean loss: 228.37
 ---- batch: 060 ----
mean loss: 221.02
 ---- batch: 070 ----
mean loss: 218.01
 ---- batch: 080 ----
mean loss: 215.76
 ---- batch: 090 ----
mean loss: 226.87
train mean loss: 221.61
epoch train time: 0:00:02.601211
elapsed time: 0:08:18.688211
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 18:19:15.893580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.39
 ---- batch: 020 ----
mean loss: 227.83
 ---- batch: 030 ----
mean loss: 220.98
 ---- batch: 040 ----
mean loss: 217.33
 ---- batch: 050 ----
mean loss: 214.28
 ---- batch: 060 ----
mean loss: 224.21
 ---- batch: 070 ----
mean loss: 224.77
 ---- batch: 080 ----
mean loss: 222.31
 ---- batch: 090 ----
mean loss: 214.89
train mean loss: 221.24
epoch train time: 0:00:02.603701
elapsed time: 0:08:21.292403
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 18:19:18.498108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.95
 ---- batch: 020 ----
mean loss: 222.38
 ---- batch: 030 ----
mean loss: 226.04
 ---- batch: 040 ----
mean loss: 224.33
 ---- batch: 050 ----
mean loss: 214.58
 ---- batch: 060 ----
mean loss: 222.76
 ---- batch: 070 ----
mean loss: 218.76
 ---- batch: 080 ----
mean loss: 213.54
 ---- batch: 090 ----
mean loss: 220.51
train mean loss: 221.19
epoch train time: 0:00:02.634127
elapsed time: 0:08:23.927422
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 18:19:21.132776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.30
 ---- batch: 020 ----
mean loss: 221.42
 ---- batch: 030 ----
mean loss: 223.77
 ---- batch: 040 ----
mean loss: 220.06
 ---- batch: 050 ----
mean loss: 226.79
 ---- batch: 060 ----
mean loss: 221.55
 ---- batch: 070 ----
mean loss: 212.98
 ---- batch: 080 ----
mean loss: 218.87
 ---- batch: 090 ----
mean loss: 220.41
train mean loss: 221.31
epoch train time: 0:00:02.660600
elapsed time: 0:08:26.588523
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 18:19:23.793886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.13
 ---- batch: 020 ----
mean loss: 220.86
 ---- batch: 030 ----
mean loss: 222.25
 ---- batch: 040 ----
mean loss: 229.13
 ---- batch: 050 ----
mean loss: 218.56
 ---- batch: 060 ----
mean loss: 216.70
 ---- batch: 070 ----
mean loss: 217.17
 ---- batch: 080 ----
mean loss: 221.62
 ---- batch: 090 ----
mean loss: 220.64
train mean loss: 221.33
epoch train time: 0:00:02.652531
elapsed time: 0:08:29.241519
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 18:19:26.446875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.10
 ---- batch: 020 ----
mean loss: 227.36
 ---- batch: 030 ----
mean loss: 222.47
 ---- batch: 040 ----
mean loss: 221.41
 ---- batch: 050 ----
mean loss: 215.15
 ---- batch: 060 ----
mean loss: 217.71
 ---- batch: 070 ----
mean loss: 223.48
 ---- batch: 080 ----
mean loss: 219.18
 ---- batch: 090 ----
mean loss: 219.37
train mean loss: 220.83
epoch train time: 0:00:02.633246
elapsed time: 0:08:31.875233
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 18:19:29.080586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.65
 ---- batch: 020 ----
mean loss: 219.65
 ---- batch: 030 ----
mean loss: 228.07
 ---- batch: 040 ----
mean loss: 220.60
 ---- batch: 050 ----
mean loss: 218.53
 ---- batch: 060 ----
mean loss: 222.66
 ---- batch: 070 ----
mean loss: 219.49
 ---- batch: 080 ----
mean loss: 222.91
 ---- batch: 090 ----
mean loss: 208.54
train mean loss: 220.50
epoch train time: 0:00:02.624641
elapsed time: 0:08:34.500351
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 18:19:31.705729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.56
 ---- batch: 020 ----
mean loss: 221.87
 ---- batch: 030 ----
mean loss: 217.61
 ---- batch: 040 ----
mean loss: 226.36
 ---- batch: 050 ----
mean loss: 223.37
 ---- batch: 060 ----
mean loss: 221.58
 ---- batch: 070 ----
mean loss: 220.58
 ---- batch: 080 ----
mean loss: 220.33
 ---- batch: 090 ----
mean loss: 215.44
train mean loss: 220.62
epoch train time: 0:00:02.640653
elapsed time: 0:08:37.141487
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 18:19:34.346834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.86
 ---- batch: 020 ----
mean loss: 215.58
 ---- batch: 030 ----
mean loss: 221.61
 ---- batch: 040 ----
mean loss: 221.33
 ---- batch: 050 ----
mean loss: 221.89
 ---- batch: 060 ----
mean loss: 226.43
 ---- batch: 070 ----
mean loss: 219.05
 ---- batch: 080 ----
mean loss: 216.25
 ---- batch: 090 ----
mean loss: 220.18
train mean loss: 220.53
epoch train time: 0:00:02.623772
elapsed time: 0:08:39.765866
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 18:19:36.971238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.76
 ---- batch: 020 ----
mean loss: 217.35
 ---- batch: 030 ----
mean loss: 216.95
 ---- batch: 040 ----
mean loss: 227.18
 ---- batch: 050 ----
mean loss: 232.56
 ---- batch: 060 ----
mean loss: 220.53
 ---- batch: 070 ----
mean loss: 216.38
 ---- batch: 080 ----
mean loss: 215.61
 ---- batch: 090 ----
mean loss: 221.55
train mean loss: 220.35
epoch train time: 0:00:02.586335
elapsed time: 0:08:42.352758
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 18:19:39.558106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.66
 ---- batch: 020 ----
mean loss: 215.97
 ---- batch: 030 ----
mean loss: 224.93
 ---- batch: 040 ----
mean loss: 223.00
 ---- batch: 050 ----
mean loss: 214.75
 ---- batch: 060 ----
mean loss: 222.50
 ---- batch: 070 ----
mean loss: 224.08
 ---- batch: 080 ----
mean loss: 214.89
 ---- batch: 090 ----
mean loss: 222.59
train mean loss: 220.31
epoch train time: 0:00:02.638489
elapsed time: 0:08:44.991700
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 18:19:42.197063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.24
 ---- batch: 020 ----
mean loss: 223.43
 ---- batch: 030 ----
mean loss: 216.58
 ---- batch: 040 ----
mean loss: 217.19
 ---- batch: 050 ----
mean loss: 222.55
 ---- batch: 060 ----
mean loss: 221.33
 ---- batch: 070 ----
mean loss: 217.76
 ---- batch: 080 ----
mean loss: 220.36
 ---- batch: 090 ----
mean loss: 220.69
train mean loss: 219.64
epoch train time: 0:00:02.651873
elapsed time: 0:08:47.644062
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 18:19:44.849412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.09
 ---- batch: 020 ----
mean loss: 223.83
 ---- batch: 030 ----
mean loss: 214.62
 ---- batch: 040 ----
mean loss: 215.02
 ---- batch: 050 ----
mean loss: 224.09
 ---- batch: 060 ----
mean loss: 215.85
 ---- batch: 070 ----
mean loss: 220.26
 ---- batch: 080 ----
mean loss: 226.02
 ---- batch: 090 ----
mean loss: 219.47
train mean loss: 219.98
epoch train time: 0:00:02.655604
elapsed time: 0:08:50.300144
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 18:19:47.505492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.39
 ---- batch: 020 ----
mean loss: 221.60
 ---- batch: 030 ----
mean loss: 229.00
 ---- batch: 040 ----
mean loss: 213.69
 ---- batch: 050 ----
mean loss: 218.11
 ---- batch: 060 ----
mean loss: 212.37
 ---- batch: 070 ----
mean loss: 211.37
 ---- batch: 080 ----
mean loss: 225.50
 ---- batch: 090 ----
mean loss: 223.92
train mean loss: 219.56
epoch train time: 0:00:02.640976
elapsed time: 0:08:52.941645
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 18:19:50.147002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.85
 ---- batch: 020 ----
mean loss: 218.79
 ---- batch: 030 ----
mean loss: 223.16
 ---- batch: 040 ----
mean loss: 222.41
 ---- batch: 050 ----
mean loss: 222.80
 ---- batch: 060 ----
mean loss: 218.56
 ---- batch: 070 ----
mean loss: 217.38
 ---- batch: 080 ----
mean loss: 226.54
 ---- batch: 090 ----
mean loss: 209.91
train mean loss: 219.59
epoch train time: 0:00:02.616496
elapsed time: 0:08:55.558629
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 18:19:52.763977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.60
 ---- batch: 020 ----
mean loss: 221.93
 ---- batch: 030 ----
mean loss: 222.75
 ---- batch: 040 ----
mean loss: 213.83
 ---- batch: 050 ----
mean loss: 221.83
 ---- batch: 060 ----
mean loss: 217.33
 ---- batch: 070 ----
mean loss: 219.27
 ---- batch: 080 ----
mean loss: 217.08
 ---- batch: 090 ----
mean loss: 215.54
train mean loss: 219.23
epoch train time: 0:00:02.623627
elapsed time: 0:08:58.182744
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 18:19:55.388144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.29
 ---- batch: 020 ----
mean loss: 221.51
 ---- batch: 030 ----
mean loss: 213.97
 ---- batch: 040 ----
mean loss: 223.76
 ---- batch: 050 ----
mean loss: 219.63
 ---- batch: 060 ----
mean loss: 224.88
 ---- batch: 070 ----
mean loss: 212.26
 ---- batch: 080 ----
mean loss: 216.63
 ---- batch: 090 ----
mean loss: 218.09
train mean loss: 218.91
epoch train time: 0:00:02.658497
elapsed time: 0:09:00.841839
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 18:19:58.047196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.59
 ---- batch: 020 ----
mean loss: 214.37
 ---- batch: 030 ----
mean loss: 219.35
 ---- batch: 040 ----
mean loss: 217.52
 ---- batch: 050 ----
mean loss: 218.41
 ---- batch: 060 ----
mean loss: 209.24
 ---- batch: 070 ----
mean loss: 223.83
 ---- batch: 080 ----
mean loss: 223.09
 ---- batch: 090 ----
mean loss: 221.49
train mean loss: 219.16
epoch train time: 0:00:02.678493
elapsed time: 0:09:03.520844
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 18:20:00.726214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.44
 ---- batch: 020 ----
mean loss: 217.05
 ---- batch: 030 ----
mean loss: 218.39
 ---- batch: 040 ----
mean loss: 219.22
 ---- batch: 050 ----
mean loss: 214.53
 ---- batch: 060 ----
mean loss: 222.14
 ---- batch: 070 ----
mean loss: 221.60
 ---- batch: 080 ----
mean loss: 219.06
 ---- batch: 090 ----
mean loss: 216.73
train mean loss: 219.28
epoch train time: 0:00:02.675813
elapsed time: 0:09:06.197180
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 18:20:03.402535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.84
 ---- batch: 020 ----
mean loss: 215.77
 ---- batch: 030 ----
mean loss: 225.45
 ---- batch: 040 ----
mean loss: 214.93
 ---- batch: 050 ----
mean loss: 218.37
 ---- batch: 060 ----
mean loss: 222.45
 ---- batch: 070 ----
mean loss: 221.78
 ---- batch: 080 ----
mean loss: 220.85
 ---- batch: 090 ----
mean loss: 217.98
train mean loss: 219.26
epoch train time: 0:00:02.631411
elapsed time: 0:09:08.829139
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 18:20:06.034570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.36
 ---- batch: 020 ----
mean loss: 218.22
 ---- batch: 030 ----
mean loss: 217.41
 ---- batch: 040 ----
mean loss: 220.53
 ---- batch: 050 ----
mean loss: 215.77
 ---- batch: 060 ----
mean loss: 217.43
 ---- batch: 070 ----
mean loss: 218.01
 ---- batch: 080 ----
mean loss: 214.58
 ---- batch: 090 ----
mean loss: 226.21
train mean loss: 218.55
epoch train time: 0:00:02.578867
elapsed time: 0:09:11.408605
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 18:20:08.614009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.38
 ---- batch: 020 ----
mean loss: 222.86
 ---- batch: 030 ----
mean loss: 218.50
 ---- batch: 040 ----
mean loss: 216.53
 ---- batch: 050 ----
mean loss: 218.81
 ---- batch: 060 ----
mean loss: 220.02
 ---- batch: 070 ----
mean loss: 214.39
 ---- batch: 080 ----
mean loss: 215.17
 ---- batch: 090 ----
mean loss: 224.65
train mean loss: 218.50
epoch train time: 0:00:02.642686
elapsed time: 0:09:14.051798
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 18:20:11.257105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.79
 ---- batch: 020 ----
mean loss: 214.25
 ---- batch: 030 ----
mean loss: 215.79
 ---- batch: 040 ----
mean loss: 222.93
 ---- batch: 050 ----
mean loss: 226.63
 ---- batch: 060 ----
mean loss: 219.90
 ---- batch: 070 ----
mean loss: 221.07
 ---- batch: 080 ----
mean loss: 220.99
 ---- batch: 090 ----
mean loss: 213.81
train mean loss: 218.57
epoch train time: 0:00:02.644535
elapsed time: 0:09:16.696764
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 18:20:13.902117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.02
 ---- batch: 020 ----
mean loss: 209.57
 ---- batch: 030 ----
mean loss: 223.15
 ---- batch: 040 ----
mean loss: 216.46
 ---- batch: 050 ----
mean loss: 223.45
 ---- batch: 060 ----
mean loss: 221.07
 ---- batch: 070 ----
mean loss: 216.13
 ---- batch: 080 ----
mean loss: 222.58
 ---- batch: 090 ----
mean loss: 212.66
train mean loss: 218.16
epoch train time: 0:00:02.677274
elapsed time: 0:09:19.374562
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 18:20:16.579940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.11
 ---- batch: 020 ----
mean loss: 220.12
 ---- batch: 030 ----
mean loss: 211.52
 ---- batch: 040 ----
mean loss: 220.41
 ---- batch: 050 ----
mean loss: 217.23
 ---- batch: 060 ----
mean loss: 212.95
 ---- batch: 070 ----
mean loss: 219.65
 ---- batch: 080 ----
mean loss: 223.59
 ---- batch: 090 ----
mean loss: 221.03
train mean loss: 218.44
epoch train time: 0:00:02.646049
elapsed time: 0:09:22.021097
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 18:20:19.226466
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.95
 ---- batch: 020 ----
mean loss: 215.16
 ---- batch: 030 ----
mean loss: 216.14
 ---- batch: 040 ----
mean loss: 226.91
 ---- batch: 050 ----
mean loss: 215.91
 ---- batch: 060 ----
mean loss: 207.41
 ---- batch: 070 ----
mean loss: 215.82
 ---- batch: 080 ----
mean loss: 220.21
 ---- batch: 090 ----
mean loss: 222.59
train mean loss: 217.89
epoch train time: 0:00:02.622908
elapsed time: 0:09:24.644646
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 18:20:21.849873
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.58
 ---- batch: 020 ----
mean loss: 215.72
 ---- batch: 030 ----
mean loss: 219.51
 ---- batch: 040 ----
mean loss: 212.60
 ---- batch: 050 ----
mean loss: 222.95
 ---- batch: 060 ----
mean loss: 210.90
 ---- batch: 070 ----
mean loss: 216.82
 ---- batch: 080 ----
mean loss: 220.10
 ---- batch: 090 ----
mean loss: 213.88
train mean loss: 217.70
epoch train time: 0:00:02.631668
elapsed time: 0:09:27.276711
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 18:20:24.482076
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.96
 ---- batch: 020 ----
mean loss: 219.09
 ---- batch: 030 ----
mean loss: 212.42
 ---- batch: 040 ----
mean loss: 218.18
 ---- batch: 050 ----
mean loss: 219.30
 ---- batch: 060 ----
mean loss: 212.84
 ---- batch: 070 ----
mean loss: 225.48
 ---- batch: 080 ----
mean loss: 215.01
 ---- batch: 090 ----
mean loss: 218.46
train mean loss: 217.47
epoch train time: 0:00:02.650602
elapsed time: 0:09:29.927798
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 18:20:27.133181
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.03
 ---- batch: 020 ----
mean loss: 218.95
 ---- batch: 030 ----
mean loss: 220.45
 ---- batch: 040 ----
mean loss: 215.97
 ---- batch: 050 ----
mean loss: 218.60
 ---- batch: 060 ----
mean loss: 218.89
 ---- batch: 070 ----
mean loss: 212.46
 ---- batch: 080 ----
mean loss: 226.84
 ---- batch: 090 ----
mean loss: 212.00
train mean loss: 217.51
epoch train time: 0:00:02.624942
elapsed time: 0:09:32.553305
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 18:20:29.758756
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.11
 ---- batch: 020 ----
mean loss: 219.21
 ---- batch: 030 ----
mean loss: 226.27
 ---- batch: 040 ----
mean loss: 206.12
 ---- batch: 050 ----
mean loss: 219.33
 ---- batch: 060 ----
mean loss: 218.79
 ---- batch: 070 ----
mean loss: 214.71
 ---- batch: 080 ----
mean loss: 227.83
 ---- batch: 090 ----
mean loss: 215.75
train mean loss: 217.42
epoch train time: 0:00:02.660520
elapsed time: 0:09:35.214407
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 18:20:32.419759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.65
 ---- batch: 020 ----
mean loss: 211.27
 ---- batch: 030 ----
mean loss: 215.19
 ---- batch: 040 ----
mean loss: 226.16
 ---- batch: 050 ----
mean loss: 215.92
 ---- batch: 060 ----
mean loss: 216.13
 ---- batch: 070 ----
mean loss: 223.57
 ---- batch: 080 ----
mean loss: 224.16
 ---- batch: 090 ----
mean loss: 215.21
train mean loss: 217.52
epoch train time: 0:00:02.653964
elapsed time: 0:09:37.868952
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 18:20:35.074336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.20
 ---- batch: 020 ----
mean loss: 213.79
 ---- batch: 030 ----
mean loss: 217.77
 ---- batch: 040 ----
mean loss: 224.53
 ---- batch: 050 ----
mean loss: 218.89
 ---- batch: 060 ----
mean loss: 209.16
 ---- batch: 070 ----
mean loss: 221.81
 ---- batch: 080 ----
mean loss: 219.26
 ---- batch: 090 ----
mean loss: 217.34
train mean loss: 217.63
epoch train time: 0:00:02.633571
elapsed time: 0:09:40.503110
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 18:20:37.708569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.28
 ---- batch: 020 ----
mean loss: 220.03
 ---- batch: 030 ----
mean loss: 212.72
 ---- batch: 040 ----
mean loss: 229.42
 ---- batch: 050 ----
mean loss: 219.34
 ---- batch: 060 ----
mean loss: 212.58
 ---- batch: 070 ----
mean loss: 216.45
 ---- batch: 080 ----
mean loss: 219.33
 ---- batch: 090 ----
mean loss: 214.06
train mean loss: 217.47
epoch train time: 0:00:02.609527
elapsed time: 0:09:43.113208
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 18:20:40.318575
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.42
 ---- batch: 020 ----
mean loss: 220.68
 ---- batch: 030 ----
mean loss: 217.26
 ---- batch: 040 ----
mean loss: 214.03
 ---- batch: 050 ----
mean loss: 222.20
 ---- batch: 060 ----
mean loss: 222.97
 ---- batch: 070 ----
mean loss: 217.09
 ---- batch: 080 ----
mean loss: 216.14
 ---- batch: 090 ----
mean loss: 216.30
train mean loss: 217.53
epoch train time: 0:00:02.603669
elapsed time: 0:09:45.717364
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 18:20:42.922718
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.30
 ---- batch: 020 ----
mean loss: 219.26
 ---- batch: 030 ----
mean loss: 220.49
 ---- batch: 040 ----
mean loss: 214.36
 ---- batch: 050 ----
mean loss: 211.91
 ---- batch: 060 ----
mean loss: 227.36
 ---- batch: 070 ----
mean loss: 213.10
 ---- batch: 080 ----
mean loss: 217.66
 ---- batch: 090 ----
mean loss: 214.53
train mean loss: 217.31
epoch train time: 0:00:02.635284
elapsed time: 0:09:48.353167
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 18:20:45.558552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.27
 ---- batch: 020 ----
mean loss: 215.99
 ---- batch: 030 ----
mean loss: 218.28
 ---- batch: 040 ----
mean loss: 214.89
 ---- batch: 050 ----
mean loss: 216.14
 ---- batch: 060 ----
mean loss: 217.07
 ---- batch: 070 ----
mean loss: 222.16
 ---- batch: 080 ----
mean loss: 216.15
 ---- batch: 090 ----
mean loss: 215.69
train mean loss: 217.79
epoch train time: 0:00:02.638427
elapsed time: 0:09:50.992116
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 18:20:48.197521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.18
 ---- batch: 020 ----
mean loss: 220.36
 ---- batch: 030 ----
mean loss: 216.06
 ---- batch: 040 ----
mean loss: 221.43
 ---- batch: 050 ----
mean loss: 214.18
 ---- batch: 060 ----
mean loss: 207.12
 ---- batch: 070 ----
mean loss: 220.96
 ---- batch: 080 ----
mean loss: 218.75
 ---- batch: 090 ----
mean loss: 217.03
train mean loss: 217.46
epoch train time: 0:00:02.646221
elapsed time: 0:09:53.638962
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 18:20:50.844318
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.88
 ---- batch: 020 ----
mean loss: 219.20
 ---- batch: 030 ----
mean loss: 213.50
 ---- batch: 040 ----
mean loss: 220.12
 ---- batch: 050 ----
mean loss: 223.44
 ---- batch: 060 ----
mean loss: 219.57
 ---- batch: 070 ----
mean loss: 212.18
 ---- batch: 080 ----
mean loss: 215.36
 ---- batch: 090 ----
mean loss: 215.22
train mean loss: 217.31
epoch train time: 0:00:02.627177
elapsed time: 0:09:56.266579
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 18:20:53.471930
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.25
 ---- batch: 020 ----
mean loss: 204.30
 ---- batch: 030 ----
mean loss: 214.10
 ---- batch: 040 ----
mean loss: 214.67
 ---- batch: 050 ----
mean loss: 218.27
 ---- batch: 060 ----
mean loss: 224.87
 ---- batch: 070 ----
mean loss: 223.77
 ---- batch: 080 ----
mean loss: 224.56
 ---- batch: 090 ----
mean loss: 221.75
train mean loss: 217.76
epoch train time: 0:00:02.595771
elapsed time: 0:09:58.862785
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 18:20:56.068172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.69
 ---- batch: 020 ----
mean loss: 218.79
 ---- batch: 030 ----
mean loss: 215.53
 ---- batch: 040 ----
mean loss: 216.18
 ---- batch: 050 ----
mean loss: 215.19
 ---- batch: 060 ----
mean loss: 224.40
 ---- batch: 070 ----
mean loss: 214.94
 ---- batch: 080 ----
mean loss: 216.78
 ---- batch: 090 ----
mean loss: 213.27
train mean loss: 217.54
epoch train time: 0:00:02.632665
elapsed time: 0:10:01.495975
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 18:20:58.701482
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.48
 ---- batch: 020 ----
mean loss: 215.43
 ---- batch: 030 ----
mean loss: 217.78
 ---- batch: 040 ----
mean loss: 214.41
 ---- batch: 050 ----
mean loss: 216.90
 ---- batch: 060 ----
mean loss: 219.28
 ---- batch: 070 ----
mean loss: 214.75
 ---- batch: 080 ----
mean loss: 223.15
 ---- batch: 090 ----
mean loss: 217.66
train mean loss: 217.20
epoch train time: 0:00:02.659296
elapsed time: 0:10:04.155901
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 18:21:01.361252
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.05
 ---- batch: 020 ----
mean loss: 218.83
 ---- batch: 030 ----
mean loss: 219.77
 ---- batch: 040 ----
mean loss: 213.19
 ---- batch: 050 ----
mean loss: 220.49
 ---- batch: 060 ----
mean loss: 214.81
 ---- batch: 070 ----
mean loss: 211.27
 ---- batch: 080 ----
mean loss: 225.50
 ---- batch: 090 ----
mean loss: 216.90
train mean loss: 217.39
epoch train time: 0:00:02.673736
elapsed time: 0:10:06.830133
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 18:21:04.035499
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.98
 ---- batch: 020 ----
mean loss: 220.32
 ---- batch: 030 ----
mean loss: 220.23
 ---- batch: 040 ----
mean loss: 218.02
 ---- batch: 050 ----
mean loss: 214.69
 ---- batch: 060 ----
mean loss: 218.80
 ---- batch: 070 ----
mean loss: 214.86
 ---- batch: 080 ----
mean loss: 219.91
 ---- batch: 090 ----
mean loss: 217.54
train mean loss: 217.49
epoch train time: 0:00:02.615941
elapsed time: 0:10:09.446565
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 18:21:06.651916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.15
 ---- batch: 020 ----
mean loss: 213.00
 ---- batch: 030 ----
mean loss: 223.19
 ---- batch: 040 ----
mean loss: 220.48
 ---- batch: 050 ----
mean loss: 224.40
 ---- batch: 060 ----
mean loss: 208.83
 ---- batch: 070 ----
mean loss: 214.56
 ---- batch: 080 ----
mean loss: 222.37
 ---- batch: 090 ----
mean loss: 217.70
train mean loss: 217.30
epoch train time: 0:00:02.630953
elapsed time: 0:10:12.077981
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 18:21:09.283339
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.17
 ---- batch: 020 ----
mean loss: 213.31
 ---- batch: 030 ----
mean loss: 217.97
 ---- batch: 040 ----
mean loss: 215.10
 ---- batch: 050 ----
mean loss: 220.24
 ---- batch: 060 ----
mean loss: 216.16
 ---- batch: 070 ----
mean loss: 217.95
 ---- batch: 080 ----
mean loss: 225.97
 ---- batch: 090 ----
mean loss: 215.82
train mean loss: 217.43
epoch train time: 0:00:02.599495
elapsed time: 0:10:14.677961
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 18:21:11.883311
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.66
 ---- batch: 020 ----
mean loss: 212.09
 ---- batch: 030 ----
mean loss: 216.62
 ---- batch: 040 ----
mean loss: 218.90
 ---- batch: 050 ----
mean loss: 218.57
 ---- batch: 060 ----
mean loss: 223.88
 ---- batch: 070 ----
mean loss: 216.26
 ---- batch: 080 ----
mean loss: 227.43
 ---- batch: 090 ----
mean loss: 210.15
train mean loss: 217.24
epoch train time: 0:00:02.633561
elapsed time: 0:10:17.312019
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 18:21:14.517405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.72
 ---- batch: 020 ----
mean loss: 217.72
 ---- batch: 030 ----
mean loss: 216.57
 ---- batch: 040 ----
mean loss: 214.28
 ---- batch: 050 ----
mean loss: 211.26
 ---- batch: 060 ----
mean loss: 218.10
 ---- batch: 070 ----
mean loss: 218.48
 ---- batch: 080 ----
mean loss: 227.25
 ---- batch: 090 ----
mean loss: 220.75
train mean loss: 217.31
epoch train time: 0:00:02.673883
elapsed time: 0:10:19.986405
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 18:21:17.191801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.65
 ---- batch: 020 ----
mean loss: 215.01
 ---- batch: 030 ----
mean loss: 220.59
 ---- batch: 040 ----
mean loss: 216.01
 ---- batch: 050 ----
mean loss: 216.67
 ---- batch: 060 ----
mean loss: 215.16
 ---- batch: 070 ----
mean loss: 221.74
 ---- batch: 080 ----
mean loss: 209.51
 ---- batch: 090 ----
mean loss: 224.69
train mean loss: 217.28
epoch train time: 0:00:02.659951
elapsed time: 0:10:22.646849
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 18:21:19.852202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.33
 ---- batch: 020 ----
mean loss: 211.64
 ---- batch: 030 ----
mean loss: 214.93
 ---- batch: 040 ----
mean loss: 217.98
 ---- batch: 050 ----
mean loss: 217.35
 ---- batch: 060 ----
mean loss: 214.15
 ---- batch: 070 ----
mean loss: 218.53
 ---- batch: 080 ----
mean loss: 230.19
 ---- batch: 090 ----
mean loss: 217.87
train mean loss: 217.29
epoch train time: 0:00:02.639544
elapsed time: 0:10:25.286882
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 18:21:22.492231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.44
 ---- batch: 020 ----
mean loss: 224.76
 ---- batch: 030 ----
mean loss: 213.95
 ---- batch: 040 ----
mean loss: 216.10
 ---- batch: 050 ----
mean loss: 220.43
 ---- batch: 060 ----
mean loss: 213.04
 ---- batch: 070 ----
mean loss: 216.63
 ---- batch: 080 ----
mean loss: 212.32
 ---- batch: 090 ----
mean loss: 219.16
train mean loss: 217.29
epoch train time: 0:00:02.625198
elapsed time: 0:10:27.912581
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 18:21:25.117958
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.77
 ---- batch: 020 ----
mean loss: 222.75
 ---- batch: 030 ----
mean loss: 219.75
 ---- batch: 040 ----
mean loss: 211.11
 ---- batch: 050 ----
mean loss: 213.56
 ---- batch: 060 ----
mean loss: 217.91
 ---- batch: 070 ----
mean loss: 209.98
 ---- batch: 080 ----
mean loss: 221.93
 ---- batch: 090 ----
mean loss: 223.14
train mean loss: 217.42
epoch train time: 0:00:02.670189
elapsed time: 0:10:30.583270
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 18:21:27.788634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.22
 ---- batch: 020 ----
mean loss: 224.79
 ---- batch: 030 ----
mean loss: 214.83
 ---- batch: 040 ----
mean loss: 220.61
 ---- batch: 050 ----
mean loss: 224.41
 ---- batch: 060 ----
mean loss: 212.17
 ---- batch: 070 ----
mean loss: 219.12
 ---- batch: 080 ----
mean loss: 217.28
 ---- batch: 090 ----
mean loss: 215.26
train mean loss: 217.27
epoch train time: 0:00:02.628228
elapsed time: 0:10:33.211966
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 18:21:30.417310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.03
 ---- batch: 020 ----
mean loss: 209.37
 ---- batch: 030 ----
mean loss: 221.31
 ---- batch: 040 ----
mean loss: 218.31
 ---- batch: 050 ----
mean loss: 213.57
 ---- batch: 060 ----
mean loss: 217.28
 ---- batch: 070 ----
mean loss: 221.29
 ---- batch: 080 ----
mean loss: 213.26
 ---- batch: 090 ----
mean loss: 216.94
train mean loss: 217.16
epoch train time: 0:00:02.662794
elapsed time: 0:10:35.875202
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 18:21:33.080549
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.17
 ---- batch: 020 ----
mean loss: 218.49
 ---- batch: 030 ----
mean loss: 214.14
 ---- batch: 040 ----
mean loss: 222.92
 ---- batch: 050 ----
mean loss: 220.74
 ---- batch: 060 ----
mean loss: 211.05
 ---- batch: 070 ----
mean loss: 222.10
 ---- batch: 080 ----
mean loss: 212.09
 ---- batch: 090 ----
mean loss: 224.01
train mean loss: 217.22
epoch train time: 0:00:02.637341
elapsed time: 0:10:38.513033
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 18:21:35.718414
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.23
 ---- batch: 020 ----
mean loss: 221.43
 ---- batch: 030 ----
mean loss: 222.46
 ---- batch: 040 ----
mean loss: 213.31
 ---- batch: 050 ----
mean loss: 210.24
 ---- batch: 060 ----
mean loss: 211.80
 ---- batch: 070 ----
mean loss: 214.32
 ---- batch: 080 ----
mean loss: 221.50
 ---- batch: 090 ----
mean loss: 218.36
train mean loss: 217.34
epoch train time: 0:00:02.630882
elapsed time: 0:10:41.144418
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 18:21:38.349828
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.68
 ---- batch: 020 ----
mean loss: 213.06
 ---- batch: 030 ----
mean loss: 216.66
 ---- batch: 040 ----
mean loss: 216.14
 ---- batch: 050 ----
mean loss: 215.32
 ---- batch: 060 ----
mean loss: 208.78
 ---- batch: 070 ----
mean loss: 212.83
 ---- batch: 080 ----
mean loss: 218.92
 ---- batch: 090 ----
mean loss: 225.90
train mean loss: 217.08
epoch train time: 0:00:02.657925
elapsed time: 0:10:43.802969
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 18:21:41.008323
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.04
 ---- batch: 020 ----
mean loss: 218.99
 ---- batch: 030 ----
mean loss: 219.82
 ---- batch: 040 ----
mean loss: 214.81
 ---- batch: 050 ----
mean loss: 210.37
 ---- batch: 060 ----
mean loss: 220.47
 ---- batch: 070 ----
mean loss: 215.31
 ---- batch: 080 ----
mean loss: 215.01
 ---- batch: 090 ----
mean loss: 220.58
train mean loss: 217.18
epoch train time: 0:00:02.646483
elapsed time: 0:10:46.449968
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 18:21:43.655366
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.89
 ---- batch: 020 ----
mean loss: 215.08
 ---- batch: 030 ----
mean loss: 211.84
 ---- batch: 040 ----
mean loss: 213.28
 ---- batch: 050 ----
mean loss: 216.71
 ---- batch: 060 ----
mean loss: 219.25
 ---- batch: 070 ----
mean loss: 211.49
 ---- batch: 080 ----
mean loss: 221.71
 ---- batch: 090 ----
mean loss: 218.25
train mean loss: 217.25
epoch train time: 0:00:02.632698
elapsed time: 0:10:49.083251
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 18:21:46.288476
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.71
 ---- batch: 020 ----
mean loss: 219.60
 ---- batch: 030 ----
mean loss: 215.87
 ---- batch: 040 ----
mean loss: 222.28
 ---- batch: 050 ----
mean loss: 217.65
 ---- batch: 060 ----
mean loss: 215.18
 ---- batch: 070 ----
mean loss: 222.71
 ---- batch: 080 ----
mean loss: 214.84
 ---- batch: 090 ----
mean loss: 214.03
train mean loss: 217.23
epoch train time: 0:00:02.618578
elapsed time: 0:10:51.702239
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 18:21:48.907595
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.43
 ---- batch: 020 ----
mean loss: 212.74
 ---- batch: 030 ----
mean loss: 213.57
 ---- batch: 040 ----
mean loss: 212.46
 ---- batch: 050 ----
mean loss: 223.45
 ---- batch: 060 ----
mean loss: 223.35
 ---- batch: 070 ----
mean loss: 217.49
 ---- batch: 080 ----
mean loss: 218.05
 ---- batch: 090 ----
mean loss: 217.14
train mean loss: 217.16
epoch train time: 0:00:02.610880
elapsed time: 0:10:54.313592
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 18:21:51.518936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.31
 ---- batch: 020 ----
mean loss: 219.00
 ---- batch: 030 ----
mean loss: 215.56
 ---- batch: 040 ----
mean loss: 223.26
 ---- batch: 050 ----
mean loss: 215.73
 ---- batch: 060 ----
mean loss: 209.82
 ---- batch: 070 ----
mean loss: 221.35
 ---- batch: 080 ----
mean loss: 219.36
 ---- batch: 090 ----
mean loss: 218.23
train mean loss: 217.06
epoch train time: 0:00:02.653130
elapsed time: 0:10:56.967177
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 18:21:54.172525
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.06
 ---- batch: 020 ----
mean loss: 212.07
 ---- batch: 030 ----
mean loss: 222.28
 ---- batch: 040 ----
mean loss: 217.64
 ---- batch: 050 ----
mean loss: 214.22
 ---- batch: 060 ----
mean loss: 210.23
 ---- batch: 070 ----
mean loss: 217.42
 ---- batch: 080 ----
mean loss: 221.11
 ---- batch: 090 ----
mean loss: 218.04
train mean loss: 217.25
epoch train time: 0:00:02.639909
elapsed time: 0:10:59.607607
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 18:21:56.812969
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.66
 ---- batch: 020 ----
mean loss: 216.79
 ---- batch: 030 ----
mean loss: 220.08
 ---- batch: 040 ----
mean loss: 212.69
 ---- batch: 050 ----
mean loss: 219.12
 ---- batch: 060 ----
mean loss: 210.14
 ---- batch: 070 ----
mean loss: 218.02
 ---- batch: 080 ----
mean loss: 217.19
 ---- batch: 090 ----
mean loss: 219.04
train mean loss: 217.43
epoch train time: 0:00:02.624263
elapsed time: 0:11:02.232331
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 18:21:59.437706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.77
 ---- batch: 020 ----
mean loss: 216.93
 ---- batch: 030 ----
mean loss: 216.57
 ---- batch: 040 ----
mean loss: 210.69
 ---- batch: 050 ----
mean loss: 215.25
 ---- batch: 060 ----
mean loss: 224.14
 ---- batch: 070 ----
mean loss: 212.94
 ---- batch: 080 ----
mean loss: 217.94
 ---- batch: 090 ----
mean loss: 220.38
train mean loss: 217.24
epoch train time: 0:00:02.610146
elapsed time: 0:11:04.843014
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 18:22:02.048362
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.36
 ---- batch: 020 ----
mean loss: 214.60
 ---- batch: 030 ----
mean loss: 220.02
 ---- batch: 040 ----
mean loss: 213.62
 ---- batch: 050 ----
mean loss: 217.85
 ---- batch: 060 ----
mean loss: 212.57
 ---- batch: 070 ----
mean loss: 219.42
 ---- batch: 080 ----
mean loss: 221.95
 ---- batch: 090 ----
mean loss: 212.48
train mean loss: 217.25
epoch train time: 0:00:02.635153
elapsed time: 0:11:07.478677
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 18:22:04.684046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.37
 ---- batch: 020 ----
mean loss: 212.62
 ---- batch: 030 ----
mean loss: 222.05
 ---- batch: 040 ----
mean loss: 210.72
 ---- batch: 050 ----
mean loss: 216.69
 ---- batch: 060 ----
mean loss: 211.76
 ---- batch: 070 ----
mean loss: 220.28
 ---- batch: 080 ----
mean loss: 214.01
 ---- batch: 090 ----
mean loss: 222.29
train mean loss: 217.29
epoch train time: 0:00:02.656690
elapsed time: 0:11:10.135858
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 18:22:07.341211
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.33
 ---- batch: 020 ----
mean loss: 212.51
 ---- batch: 030 ----
mean loss: 221.70
 ---- batch: 040 ----
mean loss: 212.86
 ---- batch: 050 ----
mean loss: 212.84
 ---- batch: 060 ----
mean loss: 214.18
 ---- batch: 070 ----
mean loss: 217.10
 ---- batch: 080 ----
mean loss: 215.40
 ---- batch: 090 ----
mean loss: 223.94
train mean loss: 217.10
epoch train time: 0:00:02.655283
elapsed time: 0:11:12.791590
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 18:22:09.996948
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.81
 ---- batch: 020 ----
mean loss: 217.30
 ---- batch: 030 ----
mean loss: 214.55
 ---- batch: 040 ----
mean loss: 210.03
 ---- batch: 050 ----
mean loss: 214.73
 ---- batch: 060 ----
mean loss: 223.66
 ---- batch: 070 ----
mean loss: 222.98
 ---- batch: 080 ----
mean loss: 209.82
 ---- batch: 090 ----
mean loss: 223.74
train mean loss: 216.88
epoch train time: 0:00:02.638419
elapsed time: 0:11:15.430482
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 18:22:12.635829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.34
 ---- batch: 020 ----
mean loss: 226.66
 ---- batch: 030 ----
mean loss: 211.93
 ---- batch: 040 ----
mean loss: 213.70
 ---- batch: 050 ----
mean loss: 221.21
 ---- batch: 060 ----
mean loss: 211.94
 ---- batch: 070 ----
mean loss: 214.83
 ---- batch: 080 ----
mean loss: 215.21
 ---- batch: 090 ----
mean loss: 220.28
train mean loss: 216.87
epoch train time: 0:00:02.663284
elapsed time: 0:11:18.094262
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 18:22:15.299655
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.38
 ---- batch: 020 ----
mean loss: 213.92
 ---- batch: 030 ----
mean loss: 214.45
 ---- batch: 040 ----
mean loss: 220.34
 ---- batch: 050 ----
mean loss: 221.14
 ---- batch: 060 ----
mean loss: 215.30
 ---- batch: 070 ----
mean loss: 212.58
 ---- batch: 080 ----
mean loss: 224.34
 ---- batch: 090 ----
mean loss: 218.76
train mean loss: 216.93
epoch train time: 0:00:02.612646
elapsed time: 0:11:20.707449
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 18:22:17.912809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.36
 ---- batch: 020 ----
mean loss: 217.78
 ---- batch: 030 ----
mean loss: 212.03
 ---- batch: 040 ----
mean loss: 221.59
 ---- batch: 050 ----
mean loss: 216.58
 ---- batch: 060 ----
mean loss: 211.12
 ---- batch: 070 ----
mean loss: 217.04
 ---- batch: 080 ----
mean loss: 212.01
 ---- batch: 090 ----
mean loss: 219.22
train mean loss: 217.13
epoch train time: 0:00:02.642264
elapsed time: 0:11:23.350222
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 18:22:20.555582
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.20
 ---- batch: 020 ----
mean loss: 214.51
 ---- batch: 030 ----
mean loss: 219.14
 ---- batch: 040 ----
mean loss: 223.99
 ---- batch: 050 ----
mean loss: 221.50
 ---- batch: 060 ----
mean loss: 221.68
 ---- batch: 070 ----
mean loss: 204.70
 ---- batch: 080 ----
mean loss: 208.33
 ---- batch: 090 ----
mean loss: 220.80
train mean loss: 217.02
epoch train time: 0:00:02.647678
elapsed time: 0:11:25.998372
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 18:22:23.203741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.43
 ---- batch: 020 ----
mean loss: 218.76
 ---- batch: 030 ----
mean loss: 213.80
 ---- batch: 040 ----
mean loss: 220.68
 ---- batch: 050 ----
mean loss: 210.66
 ---- batch: 060 ----
mean loss: 218.36
 ---- batch: 070 ----
mean loss: 227.26
 ---- batch: 080 ----
mean loss: 211.20
 ---- batch: 090 ----
mean loss: 214.69
train mean loss: 216.93
epoch train time: 0:00:02.625468
elapsed time: 0:11:28.624356
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 18:22:25.829748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.34
 ---- batch: 020 ----
mean loss: 219.35
 ---- batch: 030 ----
mean loss: 216.35
 ---- batch: 040 ----
mean loss: 208.43
 ---- batch: 050 ----
mean loss: 225.88
 ---- batch: 060 ----
mean loss: 218.20
 ---- batch: 070 ----
mean loss: 212.51
 ---- batch: 080 ----
mean loss: 222.32
 ---- batch: 090 ----
mean loss: 220.78
train mean loss: 216.83
epoch train time: 0:00:02.584836
elapsed time: 0:11:31.213536
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_4/checkpoint.pth.tar
**** end time: 2019-09-25 18:22:28.418729 ****
