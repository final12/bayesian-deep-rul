Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_4', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 4764
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-24 23:42:48.991773 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-24 23:42:49.010313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2302.85
 ---- batch: 020 ----
mean loss: 1528.40
 ---- batch: 030 ----
mean loss: 1302.44
 ---- batch: 040 ----
mean loss: 1232.28
 ---- batch: 050 ----
mean loss: 1202.77
 ---- batch: 060 ----
mean loss: 1117.27
 ---- batch: 070 ----
mean loss: 1099.73
 ---- batch: 080 ----
mean loss: 1081.36
 ---- batch: 090 ----
mean loss: 1068.40
train mean loss: 1309.99
epoch train time: 0:00:47.606089
elapsed time: 0:00:47.633724
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-24 23:43:36.625547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1060.59
 ---- batch: 020 ----
mean loss: 1039.01
 ---- batch: 030 ----
mean loss: 1036.66
 ---- batch: 040 ----
mean loss: 1022.56
 ---- batch: 050 ----
mean loss: 1000.31
 ---- batch: 060 ----
mean loss: 1001.06
 ---- batch: 070 ----
mean loss: 1002.07
 ---- batch: 080 ----
mean loss: 1006.74
 ---- batch: 090 ----
mean loss: 1030.16
train mean loss: 1019.85
epoch train time: 0:00:16.851103
elapsed time: 0:01:04.485518
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-24 23:43:53.477855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 998.01
 ---- batch: 020 ----
mean loss: 995.18
 ---- batch: 030 ----
mean loss: 997.19
 ---- batch: 040 ----
mean loss: 1015.57
 ---- batch: 050 ----
mean loss: 998.67
 ---- batch: 060 ----
mean loss: 1000.42
 ---- batch: 070 ----
mean loss: 990.61
 ---- batch: 080 ----
mean loss: 964.28
 ---- batch: 090 ----
mean loss: 1000.25
train mean loss: 993.89
epoch train time: 0:00:16.917895
elapsed time: 0:01:21.404601
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-24 23:44:10.396942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1017.83
 ---- batch: 020 ----
mean loss: 981.27
 ---- batch: 030 ----
mean loss: 966.60
 ---- batch: 040 ----
mean loss: 977.50
 ---- batch: 050 ----
mean loss: 964.16
 ---- batch: 060 ----
mean loss: 978.04
 ---- batch: 070 ----
mean loss: 981.07
 ---- batch: 080 ----
mean loss: 978.86
 ---- batch: 090 ----
mean loss: 961.42
train mean loss: 977.75
epoch train time: 0:00:16.916964
elapsed time: 0:01:38.322785
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-24 23:44:27.315127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 984.64
 ---- batch: 020 ----
mean loss: 959.13
 ---- batch: 030 ----
mean loss: 968.56
 ---- batch: 040 ----
mean loss: 967.94
 ---- batch: 050 ----
mean loss: 942.94
 ---- batch: 060 ----
mean loss: 967.70
 ---- batch: 070 ----
mean loss: 975.39
 ---- batch: 080 ----
mean loss: 976.81
 ---- batch: 090 ----
mean loss: 956.07
train mean loss: 965.41
epoch train time: 0:00:16.911799
elapsed time: 0:01:55.235892
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-24 23:44:44.228279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 969.08
 ---- batch: 020 ----
mean loss: 964.73
 ---- batch: 030 ----
mean loss: 970.41
 ---- batch: 040 ----
mean loss: 958.63
 ---- batch: 050 ----
mean loss: 953.19
 ---- batch: 060 ----
mean loss: 936.49
 ---- batch: 070 ----
mean loss: 986.50
 ---- batch: 080 ----
mean loss: 962.33
 ---- batch: 090 ----
mean loss: 949.36
train mean loss: 961.38
epoch train time: 0:00:16.890257
elapsed time: 0:02:12.127371
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-24 23:45:01.119708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.70
 ---- batch: 020 ----
mean loss: 953.61
 ---- batch: 030 ----
mean loss: 964.61
 ---- batch: 040 ----
mean loss: 979.19
 ---- batch: 050 ----
mean loss: 968.57
 ---- batch: 060 ----
mean loss: 948.75
 ---- batch: 070 ----
mean loss: 955.10
 ---- batch: 080 ----
mean loss: 947.10
 ---- batch: 090 ----
mean loss: 941.22
train mean loss: 953.93
epoch train time: 0:00:16.912322
elapsed time: 0:02:29.040970
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-24 23:45:18.033334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.52
 ---- batch: 020 ----
mean loss: 965.75
 ---- batch: 030 ----
mean loss: 978.13
 ---- batch: 040 ----
mean loss: 946.12
 ---- batch: 050 ----
mean loss: 953.43
 ---- batch: 060 ----
mean loss: 948.74
 ---- batch: 070 ----
mean loss: 955.94
 ---- batch: 080 ----
mean loss: 946.77
 ---- batch: 090 ----
mean loss: 921.96
train mean loss: 949.08
epoch train time: 0:00:16.883218
elapsed time: 0:02:45.925537
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-24 23:45:34.917869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.65
 ---- batch: 020 ----
mean loss: 929.37
 ---- batch: 030 ----
mean loss: 938.67
 ---- batch: 040 ----
mean loss: 978.39
 ---- batch: 050 ----
mean loss: 933.36
 ---- batch: 060 ----
mean loss: 947.33
 ---- batch: 070 ----
mean loss: 941.87
 ---- batch: 080 ----
mean loss: 929.71
 ---- batch: 090 ----
mean loss: 953.98
train mean loss: 942.44
epoch train time: 0:00:16.916616
elapsed time: 0:03:02.843422
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-24 23:45:51.835723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 945.00
 ---- batch: 020 ----
mean loss: 940.00
 ---- batch: 030 ----
mean loss: 917.98
 ---- batch: 040 ----
mean loss: 947.79
 ---- batch: 050 ----
mean loss: 937.17
 ---- batch: 060 ----
mean loss: 952.38
 ---- batch: 070 ----
mean loss: 944.98
 ---- batch: 080 ----
mean loss: 925.24
 ---- batch: 090 ----
mean loss: 928.54
train mean loss: 937.35
epoch train time: 0:00:16.869794
elapsed time: 0:03:19.714395
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-24 23:46:08.706726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.08
 ---- batch: 020 ----
mean loss: 934.45
 ---- batch: 030 ----
mean loss: 934.79
 ---- batch: 040 ----
mean loss: 937.67
 ---- batch: 050 ----
mean loss: 913.78
 ---- batch: 060 ----
mean loss: 938.47
 ---- batch: 070 ----
mean loss: 935.73
 ---- batch: 080 ----
mean loss: 914.76
 ---- batch: 090 ----
mean loss: 934.06
train mean loss: 927.39
epoch train time: 0:00:16.960246
elapsed time: 0:03:36.675812
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-24 23:46:25.668133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.55
 ---- batch: 020 ----
mean loss: 918.76
 ---- batch: 030 ----
mean loss: 920.71
 ---- batch: 040 ----
mean loss: 929.96
 ---- batch: 050 ----
mean loss: 950.44
 ---- batch: 060 ----
mean loss: 922.50
 ---- batch: 070 ----
mean loss: 930.82
 ---- batch: 080 ----
mean loss: 927.15
 ---- batch: 090 ----
mean loss: 907.25
train mean loss: 921.04
epoch train time: 0:00:16.852162
elapsed time: 0:03:53.529278
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-24 23:46:42.521672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 910.94
 ---- batch: 020 ----
mean loss: 920.93
 ---- batch: 030 ----
mean loss: 913.39
 ---- batch: 040 ----
mean loss: 893.87
 ---- batch: 050 ----
mean loss: 917.06
 ---- batch: 060 ----
mean loss: 913.12
 ---- batch: 070 ----
mean loss: 890.73
 ---- batch: 080 ----
mean loss: 902.81
 ---- batch: 090 ----
mean loss: 921.45
train mean loss: 909.62
epoch train time: 0:00:16.879990
elapsed time: 0:04:10.410539
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-24 23:46:59.402871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 897.25
 ---- batch: 020 ----
mean loss: 914.77
 ---- batch: 030 ----
mean loss: 899.85
 ---- batch: 040 ----
mean loss: 873.56
 ---- batch: 050 ----
mean loss: 901.16
 ---- batch: 060 ----
mean loss: 883.15
 ---- batch: 070 ----
mean loss: 908.30
 ---- batch: 080 ----
mean loss: 894.57
 ---- batch: 090 ----
mean loss: 887.95
train mean loss: 894.26
epoch train time: 0:00:16.874947
elapsed time: 0:04:27.286746
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-24 23:47:16.279131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.09
 ---- batch: 020 ----
mean loss: 870.86
 ---- batch: 030 ----
mean loss: 866.91
 ---- batch: 040 ----
mean loss: 843.16
 ---- batch: 050 ----
mean loss: 815.30
 ---- batch: 060 ----
mean loss: 812.73
 ---- batch: 070 ----
mean loss: 818.39
 ---- batch: 080 ----
mean loss: 809.95
 ---- batch: 090 ----
mean loss: 798.35
train mean loss: 830.17
epoch train time: 0:00:16.862402
elapsed time: 0:04:44.150391
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-24 23:47:33.142770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 765.94
 ---- batch: 020 ----
mean loss: 778.96
 ---- batch: 030 ----
mean loss: 750.33
 ---- batch: 040 ----
mean loss: 758.37
 ---- batch: 050 ----
mean loss: 750.51
 ---- batch: 060 ----
mean loss: 729.45
 ---- batch: 070 ----
mean loss: 708.29
 ---- batch: 080 ----
mean loss: 700.82
 ---- batch: 090 ----
mean loss: 723.62
train mean loss: 739.37
epoch train time: 0:00:16.879313
elapsed time: 0:05:01.031044
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-24 23:47:50.023373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.95
 ---- batch: 020 ----
mean loss: 680.50
 ---- batch: 030 ----
mean loss: 684.60
 ---- batch: 040 ----
mean loss: 679.90
 ---- batch: 050 ----
mean loss: 670.19
 ---- batch: 060 ----
mean loss: 670.17
 ---- batch: 070 ----
mean loss: 661.71
 ---- batch: 080 ----
mean loss: 679.70
 ---- batch: 090 ----
mean loss: 654.72
train mean loss: 676.21
epoch train time: 0:00:16.961169
elapsed time: 0:05:17.993502
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-24 23:48:06.985880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 624.31
 ---- batch: 020 ----
mean loss: 637.91
 ---- batch: 030 ----
mean loss: 624.57
 ---- batch: 040 ----
mean loss: 611.33
 ---- batch: 050 ----
mean loss: 601.02
 ---- batch: 060 ----
mean loss: 592.33
 ---- batch: 070 ----
mean loss: 589.52
 ---- batch: 080 ----
mean loss: 592.39
 ---- batch: 090 ----
mean loss: 580.13
train mean loss: 602.75
epoch train time: 0:00:16.995358
elapsed time: 0:05:34.990143
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-24 23:48:23.982467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.39
 ---- batch: 020 ----
mean loss: 576.36
 ---- batch: 030 ----
mean loss: 543.09
 ---- batch: 040 ----
mean loss: 546.11
 ---- batch: 050 ----
mean loss: 547.36
 ---- batch: 060 ----
mean loss: 517.26
 ---- batch: 070 ----
mean loss: 524.26
 ---- batch: 080 ----
mean loss: 515.67
 ---- batch: 090 ----
mean loss: 521.85
train mean loss: 538.17
epoch train time: 0:00:16.987872
elapsed time: 0:05:51.979250
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-24 23:48:40.971580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 510.38
 ---- batch: 020 ----
mean loss: 510.06
 ---- batch: 030 ----
mean loss: 509.20
 ---- batch: 040 ----
mean loss: 513.05
 ---- batch: 050 ----
mean loss: 506.85
 ---- batch: 060 ----
mean loss: 478.16
 ---- batch: 070 ----
mean loss: 500.40
 ---- batch: 080 ----
mean loss: 486.09
 ---- batch: 090 ----
mean loss: 498.92
train mean loss: 500.68
epoch train time: 0:00:17.102311
elapsed time: 0:06:09.082921
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-24 23:48:58.075301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.34
 ---- batch: 020 ----
mean loss: 482.74
 ---- batch: 030 ----
mean loss: 472.59
 ---- batch: 040 ----
mean loss: 470.39
 ---- batch: 050 ----
mean loss: 470.13
 ---- batch: 060 ----
mean loss: 468.28
 ---- batch: 070 ----
mean loss: 470.07
 ---- batch: 080 ----
mean loss: 465.37
 ---- batch: 090 ----
mean loss: 453.25
train mean loss: 469.18
epoch train time: 0:00:17.015841
elapsed time: 0:06:26.099998
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-24 23:49:15.092332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.71
 ---- batch: 020 ----
mean loss: 453.93
 ---- batch: 030 ----
mean loss: 463.32
 ---- batch: 040 ----
mean loss: 458.71
 ---- batch: 050 ----
mean loss: 441.99
 ---- batch: 060 ----
mean loss: 452.84
 ---- batch: 070 ----
mean loss: 445.47
 ---- batch: 080 ----
mean loss: 430.59
 ---- batch: 090 ----
mean loss: 449.02
train mean loss: 448.51
epoch train time: 0:00:16.957172
elapsed time: 0:06:43.058403
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-24 23:49:32.050589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.63
 ---- batch: 020 ----
mean loss: 436.87
 ---- batch: 030 ----
mean loss: 426.48
 ---- batch: 040 ----
mean loss: 427.07
 ---- batch: 050 ----
mean loss: 424.87
 ---- batch: 060 ----
mean loss: 423.24
 ---- batch: 070 ----
mean loss: 432.52
 ---- batch: 080 ----
mean loss: 435.69
 ---- batch: 090 ----
mean loss: 428.14
train mean loss: 430.78
epoch train time: 0:00:16.948702
elapsed time: 0:07:00.008259
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-24 23:49:49.000614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.10
 ---- batch: 020 ----
mean loss: 423.89
 ---- batch: 030 ----
mean loss: 408.85
 ---- batch: 040 ----
mean loss: 405.27
 ---- batch: 050 ----
mean loss: 409.22
 ---- batch: 060 ----
mean loss: 409.06
 ---- batch: 070 ----
mean loss: 395.82
 ---- batch: 080 ----
mean loss: 395.47
 ---- batch: 090 ----
mean loss: 400.57
train mean loss: 408.63
epoch train time: 0:00:16.981259
elapsed time: 0:07:16.990853
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-24 23:50:05.983222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.34
 ---- batch: 020 ----
mean loss: 409.20
 ---- batch: 030 ----
mean loss: 403.44
 ---- batch: 040 ----
mean loss: 404.22
 ---- batch: 050 ----
mean loss: 389.84
 ---- batch: 060 ----
mean loss: 394.18
 ---- batch: 070 ----
mean loss: 389.22
 ---- batch: 080 ----
mean loss: 392.20
 ---- batch: 090 ----
mean loss: 396.09
train mean loss: 397.52
epoch train time: 0:00:17.009692
elapsed time: 0:07:34.001821
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-24 23:50:22.994221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.79
 ---- batch: 020 ----
mean loss: 386.92
 ---- batch: 030 ----
mean loss: 386.14
 ---- batch: 040 ----
mean loss: 378.59
 ---- batch: 050 ----
mean loss: 380.63
 ---- batch: 060 ----
mean loss: 391.20
 ---- batch: 070 ----
mean loss: 370.27
 ---- batch: 080 ----
mean loss: 375.66
 ---- batch: 090 ----
mean loss: 368.84
train mean loss: 381.08
epoch train time: 0:00:17.003191
elapsed time: 0:07:51.006355
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-24 23:50:39.998653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.39
 ---- batch: 020 ----
mean loss: 379.03
 ---- batch: 030 ----
mean loss: 379.26
 ---- batch: 040 ----
mean loss: 392.83
 ---- batch: 050 ----
mean loss: 367.45
 ---- batch: 060 ----
mean loss: 375.00
 ---- batch: 070 ----
mean loss: 365.78
 ---- batch: 080 ----
mean loss: 369.66
 ---- batch: 090 ----
mean loss: 361.40
train mean loss: 373.78
epoch train time: 0:00:16.963294
elapsed time: 0:08:07.970860
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-24 23:50:56.963267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.86
 ---- batch: 020 ----
mean loss: 370.70
 ---- batch: 030 ----
mean loss: 362.45
 ---- batch: 040 ----
mean loss: 369.14
 ---- batch: 050 ----
mean loss: 373.74
 ---- batch: 060 ----
mean loss: 359.82
 ---- batch: 070 ----
mean loss: 348.98
 ---- batch: 080 ----
mean loss: 370.20
 ---- batch: 090 ----
mean loss: 358.93
train mean loss: 365.03
epoch train time: 0:00:16.977634
elapsed time: 0:08:24.949979
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-24 23:51:13.942355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.63
 ---- batch: 020 ----
mean loss: 342.66
 ---- batch: 030 ----
mean loss: 349.93
 ---- batch: 040 ----
mean loss: 352.24
 ---- batch: 050 ----
mean loss: 350.75
 ---- batch: 060 ----
mean loss: 344.11
 ---- batch: 070 ----
mean loss: 358.76
 ---- batch: 080 ----
mean loss: 353.55
 ---- batch: 090 ----
mean loss: 346.73
train mean loss: 349.40
epoch train time: 0:00:17.029046
elapsed time: 0:08:41.980326
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-24 23:51:30.972663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.44
 ---- batch: 020 ----
mean loss: 347.15
 ---- batch: 030 ----
mean loss: 342.13
 ---- batch: 040 ----
mean loss: 332.45
 ---- batch: 050 ----
mean loss: 352.19
 ---- batch: 060 ----
mean loss: 353.46
 ---- batch: 070 ----
mean loss: 340.22
 ---- batch: 080 ----
mean loss: 351.33
 ---- batch: 090 ----
mean loss: 337.43
train mean loss: 346.08
epoch train time: 0:00:16.977248
elapsed time: 0:08:58.958846
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-24 23:51:47.951173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.79
 ---- batch: 020 ----
mean loss: 346.00
 ---- batch: 030 ----
mean loss: 339.50
 ---- batch: 040 ----
mean loss: 344.77
 ---- batch: 050 ----
mean loss: 349.79
 ---- batch: 060 ----
mean loss: 343.09
 ---- batch: 070 ----
mean loss: 334.43
 ---- batch: 080 ----
mean loss: 327.13
 ---- batch: 090 ----
mean loss: 324.43
train mean loss: 338.20
epoch train time: 0:00:16.935310
elapsed time: 0:09:15.895466
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-24 23:52:04.887760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.06
 ---- batch: 020 ----
mean loss: 337.21
 ---- batch: 030 ----
mean loss: 328.73
 ---- batch: 040 ----
mean loss: 324.92
 ---- batch: 050 ----
mean loss: 319.58
 ---- batch: 060 ----
mean loss: 337.16
 ---- batch: 070 ----
mean loss: 331.49
 ---- batch: 080 ----
mean loss: 335.87
 ---- batch: 090 ----
mean loss: 334.69
train mean loss: 331.57
epoch train time: 0:00:16.933518
elapsed time: 0:09:32.830253
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-24 23:52:21.822583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.62
 ---- batch: 020 ----
mean loss: 337.72
 ---- batch: 030 ----
mean loss: 322.71
 ---- batch: 040 ----
mean loss: 328.09
 ---- batch: 050 ----
mean loss: 333.94
 ---- batch: 060 ----
mean loss: 331.02
 ---- batch: 070 ----
mean loss: 336.55
 ---- batch: 080 ----
mean loss: 321.49
 ---- batch: 090 ----
mean loss: 311.28
train mean loss: 327.59
epoch train time: 0:00:16.962094
elapsed time: 0:09:49.793627
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-24 23:52:38.785964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.62
 ---- batch: 020 ----
mean loss: 328.46
 ---- batch: 030 ----
mean loss: 328.38
 ---- batch: 040 ----
mean loss: 311.25
 ---- batch: 050 ----
mean loss: 316.19
 ---- batch: 060 ----
mean loss: 312.56
 ---- batch: 070 ----
mean loss: 311.68
 ---- batch: 080 ----
mean loss: 328.36
 ---- batch: 090 ----
mean loss: 317.43
train mean loss: 319.18
epoch train time: 0:00:16.982180
elapsed time: 0:10:06.777052
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-24 23:52:55.769431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.47
 ---- batch: 020 ----
mean loss: 317.31
 ---- batch: 030 ----
mean loss: 319.20
 ---- batch: 040 ----
mean loss: 317.42
 ---- batch: 050 ----
mean loss: 312.80
 ---- batch: 060 ----
mean loss: 316.43
 ---- batch: 070 ----
mean loss: 319.38
 ---- batch: 080 ----
mean loss: 309.50
 ---- batch: 090 ----
mean loss: 317.19
train mean loss: 314.91
epoch train time: 0:00:16.992874
elapsed time: 0:10:23.771252
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-24 23:53:12.763625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.78
 ---- batch: 020 ----
mean loss: 319.82
 ---- batch: 030 ----
mean loss: 300.96
 ---- batch: 040 ----
mean loss: 307.67
 ---- batch: 050 ----
mean loss: 302.62
 ---- batch: 060 ----
mean loss: 295.90
 ---- batch: 070 ----
mean loss: 303.30
 ---- batch: 080 ----
mean loss: 309.95
 ---- batch: 090 ----
mean loss: 295.93
train mean loss: 307.85
epoch train time: 0:00:17.071843
elapsed time: 0:10:40.844386
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-24 23:53:29.836736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.47
 ---- batch: 020 ----
mean loss: 311.63
 ---- batch: 030 ----
mean loss: 299.25
 ---- batch: 040 ----
mean loss: 306.30
 ---- batch: 050 ----
mean loss: 305.17
 ---- batch: 060 ----
mean loss: 301.32
 ---- batch: 070 ----
mean loss: 300.90
 ---- batch: 080 ----
mean loss: 297.85
 ---- batch: 090 ----
mean loss: 293.84
train mean loss: 304.58
epoch train time: 0:00:16.981166
elapsed time: 0:10:57.826806
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-24 23:53:46.819155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.53
 ---- batch: 020 ----
mean loss: 298.55
 ---- batch: 030 ----
mean loss: 301.14
 ---- batch: 040 ----
mean loss: 309.57
 ---- batch: 050 ----
mean loss: 302.08
 ---- batch: 060 ----
mean loss: 296.19
 ---- batch: 070 ----
mean loss: 302.02
 ---- batch: 080 ----
mean loss: 295.45
 ---- batch: 090 ----
mean loss: 299.65
train mean loss: 300.07
epoch train time: 0:00:16.985477
elapsed time: 0:11:14.813585
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-24 23:54:03.806004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.97
 ---- batch: 020 ----
mean loss: 302.87
 ---- batch: 030 ----
mean loss: 301.03
 ---- batch: 040 ----
mean loss: 296.92
 ---- batch: 050 ----
mean loss: 299.65
 ---- batch: 060 ----
mean loss: 296.22
 ---- batch: 070 ----
mean loss: 295.83
 ---- batch: 080 ----
mean loss: 294.53
 ---- batch: 090 ----
mean loss: 290.16
train mean loss: 295.60
epoch train time: 0:00:16.987101
elapsed time: 0:11:31.802232
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-24 23:54:20.794617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.05
 ---- batch: 020 ----
mean loss: 284.87
 ---- batch: 030 ----
mean loss: 294.67
 ---- batch: 040 ----
mean loss: 298.76
 ---- batch: 050 ----
mean loss: 297.01
 ---- batch: 060 ----
mean loss: 288.97
 ---- batch: 070 ----
mean loss: 291.96
 ---- batch: 080 ----
mean loss: 285.07
 ---- batch: 090 ----
mean loss: 288.72
train mean loss: 291.17
epoch train time: 0:00:16.975389
elapsed time: 0:11:48.778873
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-24 23:54:37.771227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.28
 ---- batch: 020 ----
mean loss: 293.63
 ---- batch: 030 ----
mean loss: 288.98
 ---- batch: 040 ----
mean loss: 288.98
 ---- batch: 050 ----
mean loss: 283.66
 ---- batch: 060 ----
mean loss: 286.88
 ---- batch: 070 ----
mean loss: 283.82
 ---- batch: 080 ----
mean loss: 289.21
 ---- batch: 090 ----
mean loss: 277.08
train mean loss: 286.76
epoch train time: 0:00:16.952751
elapsed time: 0:12:05.732887
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-24 23:54:54.725206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.33
 ---- batch: 020 ----
mean loss: 272.19
 ---- batch: 030 ----
mean loss: 286.54
 ---- batch: 040 ----
mean loss: 291.32
 ---- batch: 050 ----
mean loss: 284.26
 ---- batch: 060 ----
mean loss: 286.59
 ---- batch: 070 ----
mean loss: 283.61
 ---- batch: 080 ----
mean loss: 286.90
 ---- batch: 090 ----
mean loss: 284.69
train mean loss: 283.46
epoch train time: 0:00:16.944757
elapsed time: 0:12:22.678858
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-24 23:55:11.671172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.12
 ---- batch: 020 ----
mean loss: 274.88
 ---- batch: 030 ----
mean loss: 277.71
 ---- batch: 040 ----
mean loss: 273.65
 ---- batch: 050 ----
mean loss: 282.81
 ---- batch: 060 ----
mean loss: 284.24
 ---- batch: 070 ----
mean loss: 279.51
 ---- batch: 080 ----
mean loss: 293.14
 ---- batch: 090 ----
mean loss: 275.37
train mean loss: 280.84
epoch train time: 0:00:17.001114
elapsed time: 0:12:39.681150
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-24 23:55:28.673496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.36
 ---- batch: 020 ----
mean loss: 290.99
 ---- batch: 030 ----
mean loss: 278.96
 ---- batch: 040 ----
mean loss: 280.15
 ---- batch: 050 ----
mean loss: 271.43
 ---- batch: 060 ----
mean loss: 272.27
 ---- batch: 070 ----
mean loss: 274.23
 ---- batch: 080 ----
mean loss: 277.04
 ---- batch: 090 ----
mean loss: 278.79
train mean loss: 277.08
epoch train time: 0:00:17.033320
elapsed time: 0:12:56.715717
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-24 23:55:45.707901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.37
 ---- batch: 020 ----
mean loss: 263.13
 ---- batch: 030 ----
mean loss: 281.84
 ---- batch: 040 ----
mean loss: 268.64
 ---- batch: 050 ----
mean loss: 269.87
 ---- batch: 060 ----
mean loss: 281.56
 ---- batch: 070 ----
mean loss: 273.94
 ---- batch: 080 ----
mean loss: 267.64
 ---- batch: 090 ----
mean loss: 275.23
train mean loss: 273.47
epoch train time: 0:00:17.039339
elapsed time: 0:13:13.756111
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-24 23:56:02.748430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.00
 ---- batch: 020 ----
mean loss: 282.80
 ---- batch: 030 ----
mean loss: 269.31
 ---- batch: 040 ----
mean loss: 274.66
 ---- batch: 050 ----
mean loss: 263.59
 ---- batch: 060 ----
mean loss: 267.24
 ---- batch: 070 ----
mean loss: 273.74
 ---- batch: 080 ----
mean loss: 266.21
 ---- batch: 090 ----
mean loss: 275.01
train mean loss: 271.19
epoch train time: 0:00:17.031699
elapsed time: 0:13:30.788995
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-24 23:56:19.781328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.55
 ---- batch: 020 ----
mean loss: 272.42
 ---- batch: 030 ----
mean loss: 278.64
 ---- batch: 040 ----
mean loss: 271.06
 ---- batch: 050 ----
mean loss: 263.91
 ---- batch: 060 ----
mean loss: 267.82
 ---- batch: 070 ----
mean loss: 264.76
 ---- batch: 080 ----
mean loss: 263.56
 ---- batch: 090 ----
mean loss: 264.97
train mean loss: 269.46
epoch train time: 0:00:17.088857
elapsed time: 0:13:47.879076
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-24 23:56:36.871415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.79
 ---- batch: 020 ----
mean loss: 269.04
 ---- batch: 030 ----
mean loss: 263.17
 ---- batch: 040 ----
mean loss: 259.73
 ---- batch: 050 ----
mean loss: 264.79
 ---- batch: 060 ----
mean loss: 269.38
 ---- batch: 070 ----
mean loss: 266.97
 ---- batch: 080 ----
mean loss: 261.15
 ---- batch: 090 ----
mean loss: 262.16
train mean loss: 265.27
epoch train time: 0:00:17.042321
elapsed time: 0:14:04.922678
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-24 23:56:53.915091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.71
 ---- batch: 020 ----
mean loss: 258.34
 ---- batch: 030 ----
mean loss: 267.12
 ---- batch: 040 ----
mean loss: 260.20
 ---- batch: 050 ----
mean loss: 263.74
 ---- batch: 060 ----
mean loss: 263.64
 ---- batch: 070 ----
mean loss: 269.68
 ---- batch: 080 ----
mean loss: 269.02
 ---- batch: 090 ----
mean loss: 264.73
train mean loss: 264.39
epoch train time: 0:00:17.018898
elapsed time: 0:14:21.942893
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-24 23:57:10.935242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.33
 ---- batch: 020 ----
mean loss: 249.49
 ---- batch: 030 ----
mean loss: 265.42
 ---- batch: 040 ----
mean loss: 258.68
 ---- batch: 050 ----
mean loss: 265.94
 ---- batch: 060 ----
mean loss: 268.49
 ---- batch: 070 ----
mean loss: 253.05
 ---- batch: 080 ----
mean loss: 260.07
 ---- batch: 090 ----
mean loss: 259.85
train mean loss: 259.81
epoch train time: 0:00:17.031848
elapsed time: 0:14:38.975954
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-24 23:57:27.968247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.17
 ---- batch: 020 ----
mean loss: 255.90
 ---- batch: 030 ----
mean loss: 259.35
 ---- batch: 040 ----
mean loss: 261.99
 ---- batch: 050 ----
mean loss: 264.22
 ---- batch: 060 ----
mean loss: 262.96
 ---- batch: 070 ----
mean loss: 258.18
 ---- batch: 080 ----
mean loss: 250.03
 ---- batch: 090 ----
mean loss: 252.48
train mean loss: 258.05
epoch train time: 0:00:17.013464
elapsed time: 0:14:55.990684
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-24 23:57:44.983038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.71
 ---- batch: 020 ----
mean loss: 251.14
 ---- batch: 030 ----
mean loss: 258.58
 ---- batch: 040 ----
mean loss: 259.90
 ---- batch: 050 ----
mean loss: 250.65
 ---- batch: 060 ----
mean loss: 256.50
 ---- batch: 070 ----
mean loss: 247.06
 ---- batch: 080 ----
mean loss: 254.98
 ---- batch: 090 ----
mean loss: 255.96
train mean loss: 254.82
epoch train time: 0:00:17.066768
elapsed time: 0:15:13.058674
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-24 23:58:02.051054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.75
 ---- batch: 020 ----
mean loss: 257.12
 ---- batch: 030 ----
mean loss: 255.55
 ---- batch: 040 ----
mean loss: 250.28
 ---- batch: 050 ----
mean loss: 258.97
 ---- batch: 060 ----
mean loss: 260.97
 ---- batch: 070 ----
mean loss: 244.55
 ---- batch: 080 ----
mean loss: 242.29
 ---- batch: 090 ----
mean loss: 258.11
train mean loss: 253.62
epoch train time: 0:00:17.026769
elapsed time: 0:15:30.086716
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-24 23:58:19.079053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.35
 ---- batch: 020 ----
mean loss: 247.87
 ---- batch: 030 ----
mean loss: 260.00
 ---- batch: 040 ----
mean loss: 252.93
 ---- batch: 050 ----
mean loss: 248.25
 ---- batch: 060 ----
mean loss: 246.83
 ---- batch: 070 ----
mean loss: 245.43
 ---- batch: 080 ----
mean loss: 258.43
 ---- batch: 090 ----
mean loss: 243.47
train mean loss: 250.18
epoch train time: 0:00:17.130492
elapsed time: 0:15:47.218496
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-24 23:58:36.210851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.97
 ---- batch: 020 ----
mean loss: 252.75
 ---- batch: 030 ----
mean loss: 251.18
 ---- batch: 040 ----
mean loss: 247.96
 ---- batch: 050 ----
mean loss: 250.71
 ---- batch: 060 ----
mean loss: 251.84
 ---- batch: 070 ----
mean loss: 250.62
 ---- batch: 080 ----
mean loss: 235.71
 ---- batch: 090 ----
mean loss: 250.79
train mean loss: 249.27
epoch train time: 0:00:17.024906
elapsed time: 0:16:04.244625
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-24 23:58:53.237023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.87
 ---- batch: 020 ----
mean loss: 248.45
 ---- batch: 030 ----
mean loss: 248.96
 ---- batch: 040 ----
mean loss: 250.19
 ---- batch: 050 ----
mean loss: 243.91
 ---- batch: 060 ----
mean loss: 249.58
 ---- batch: 070 ----
mean loss: 254.40
 ---- batch: 080 ----
mean loss: 242.93
 ---- batch: 090 ----
mean loss: 240.40
train mean loss: 248.10
epoch train time: 0:00:17.044510
elapsed time: 0:16:21.290541
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-24 23:59:10.282869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.99
 ---- batch: 020 ----
mean loss: 244.85
 ---- batch: 030 ----
mean loss: 251.88
 ---- batch: 040 ----
mean loss: 249.08
 ---- batch: 050 ----
mean loss: 243.62
 ---- batch: 060 ----
mean loss: 244.34
 ---- batch: 070 ----
mean loss: 249.51
 ---- batch: 080 ----
mean loss: 243.39
 ---- batch: 090 ----
mean loss: 250.03
train mean loss: 247.14
epoch train time: 0:00:17.018031
elapsed time: 0:16:38.309773
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-24 23:59:27.302124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.53
 ---- batch: 020 ----
mean loss: 245.95
 ---- batch: 030 ----
mean loss: 244.45
 ---- batch: 040 ----
mean loss: 238.03
 ---- batch: 050 ----
mean loss: 245.96
 ---- batch: 060 ----
mean loss: 247.69
 ---- batch: 070 ----
mean loss: 235.22
 ---- batch: 080 ----
mean loss: 241.05
 ---- batch: 090 ----
mean loss: 245.59
train mean loss: 243.14
epoch train time: 0:00:17.012957
elapsed time: 0:16:55.323991
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-24 23:59:44.316408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.38
 ---- batch: 020 ----
mean loss: 248.02
 ---- batch: 030 ----
mean loss: 241.16
 ---- batch: 040 ----
mean loss: 245.19
 ---- batch: 050 ----
mean loss: 242.67
 ---- batch: 060 ----
mean loss: 242.55
 ---- batch: 070 ----
mean loss: 242.11
 ---- batch: 080 ----
mean loss: 245.14
 ---- batch: 090 ----
mean loss: 244.96
train mean loss: 244.64
epoch train time: 0:00:17.021423
elapsed time: 0:17:12.346713
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 00:00:01.339032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.27
 ---- batch: 020 ----
mean loss: 244.87
 ---- batch: 030 ----
mean loss: 234.43
 ---- batch: 040 ----
mean loss: 242.03
 ---- batch: 050 ----
mean loss: 244.87
 ---- batch: 060 ----
mean loss: 243.09
 ---- batch: 070 ----
mean loss: 237.16
 ---- batch: 080 ----
mean loss: 239.85
 ---- batch: 090 ----
mean loss: 239.68
train mean loss: 240.31
epoch train time: 0:00:17.008517
elapsed time: 0:17:29.356725
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 00:00:18.348997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.42
 ---- batch: 020 ----
mean loss: 238.51
 ---- batch: 030 ----
mean loss: 237.89
 ---- batch: 040 ----
mean loss: 238.18
 ---- batch: 050 ----
mean loss: 234.88
 ---- batch: 060 ----
mean loss: 236.29
 ---- batch: 070 ----
mean loss: 242.54
 ---- batch: 080 ----
mean loss: 248.17
 ---- batch: 090 ----
mean loss: 242.09
train mean loss: 239.51
epoch train time: 0:00:17.002873
elapsed time: 0:17:46.360720
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 00:00:35.353105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.40
 ---- batch: 020 ----
mean loss: 235.85
 ---- batch: 030 ----
mean loss: 234.92
 ---- batch: 040 ----
mean loss: 228.09
 ---- batch: 050 ----
mean loss: 236.74
 ---- batch: 060 ----
mean loss: 246.99
 ---- batch: 070 ----
mean loss: 235.80
 ---- batch: 080 ----
mean loss: 233.23
 ---- batch: 090 ----
mean loss: 249.54
train mean loss: 238.31
epoch train time: 0:00:17.030720
elapsed time: 0:18:03.392757
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 00:00:52.385056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.78
 ---- batch: 020 ----
mean loss: 236.38
 ---- batch: 030 ----
mean loss: 238.45
 ---- batch: 040 ----
mean loss: 231.97
 ---- batch: 050 ----
mean loss: 243.83
 ---- batch: 060 ----
mean loss: 238.16
 ---- batch: 070 ----
mean loss: 237.50
 ---- batch: 080 ----
mean loss: 240.38
 ---- batch: 090 ----
mean loss: 235.51
train mean loss: 237.66
epoch train time: 0:00:17.038354
elapsed time: 0:18:20.432300
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 00:01:09.424632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.31
 ---- batch: 020 ----
mean loss: 234.12
 ---- batch: 030 ----
mean loss: 230.46
 ---- batch: 040 ----
mean loss: 233.34
 ---- batch: 050 ----
mean loss: 244.84
 ---- batch: 060 ----
mean loss: 234.72
 ---- batch: 070 ----
mean loss: 232.08
 ---- batch: 080 ----
mean loss: 228.00
 ---- batch: 090 ----
mean loss: 227.39
train mean loss: 233.81
epoch train time: 0:00:17.054417
elapsed time: 0:18:37.488045
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 00:01:26.480368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.08
 ---- batch: 020 ----
mean loss: 226.27
 ---- batch: 030 ----
mean loss: 241.32
 ---- batch: 040 ----
mean loss: 239.47
 ---- batch: 050 ----
mean loss: 243.45
 ---- batch: 060 ----
mean loss: 232.67
 ---- batch: 070 ----
mean loss: 226.85
 ---- batch: 080 ----
mean loss: 231.94
 ---- batch: 090 ----
mean loss: 229.64
train mean loss: 234.65
epoch train time: 0:00:17.102275
elapsed time: 0:18:54.591604
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 00:01:43.583930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.64
 ---- batch: 020 ----
mean loss: 235.60
 ---- batch: 030 ----
mean loss: 236.63
 ---- batch: 040 ----
mean loss: 231.18
 ---- batch: 050 ----
mean loss: 234.77
 ---- batch: 060 ----
mean loss: 234.63
 ---- batch: 070 ----
mean loss: 232.35
 ---- batch: 080 ----
mean loss: 231.89
 ---- batch: 090 ----
mean loss: 232.07
train mean loss: 233.10
epoch train time: 0:00:17.088598
elapsed time: 0:19:11.681472
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 00:02:00.673863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.67
 ---- batch: 020 ----
mean loss: 230.91
 ---- batch: 030 ----
mean loss: 235.21
 ---- batch: 040 ----
mean loss: 241.25
 ---- batch: 050 ----
mean loss: 232.28
 ---- batch: 060 ----
mean loss: 235.71
 ---- batch: 070 ----
mean loss: 227.38
 ---- batch: 080 ----
mean loss: 229.83
 ---- batch: 090 ----
mean loss: 223.17
train mean loss: 231.68
epoch train time: 0:00:17.060659
elapsed time: 0:19:28.743435
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 00:02:17.735751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.41
 ---- batch: 020 ----
mean loss: 230.23
 ---- batch: 030 ----
mean loss: 226.17
 ---- batch: 040 ----
mean loss: 229.94
 ---- batch: 050 ----
mean loss: 229.14
 ---- batch: 060 ----
mean loss: 227.98
 ---- batch: 070 ----
mean loss: 231.31
 ---- batch: 080 ----
mean loss: 231.95
 ---- batch: 090 ----
mean loss: 224.47
train mean loss: 229.21
epoch train time: 0:00:17.046611
elapsed time: 0:19:45.791234
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 00:02:34.783614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.36
 ---- batch: 020 ----
mean loss: 230.09
 ---- batch: 030 ----
mean loss: 221.95
 ---- batch: 040 ----
mean loss: 228.30
 ---- batch: 050 ----
mean loss: 228.71
 ---- batch: 060 ----
mean loss: 224.23
 ---- batch: 070 ----
mean loss: 234.78
 ---- batch: 080 ----
mean loss: 228.52
 ---- batch: 090 ----
mean loss: 232.12
train mean loss: 229.25
epoch train time: 0:00:17.011705
elapsed time: 0:20:02.804373
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 00:02:51.796737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.59
 ---- batch: 020 ----
mean loss: 226.47
 ---- batch: 030 ----
mean loss: 222.86
 ---- batch: 040 ----
mean loss: 225.77
 ---- batch: 050 ----
mean loss: 224.96
 ---- batch: 060 ----
mean loss: 220.43
 ---- batch: 070 ----
mean loss: 216.72
 ---- batch: 080 ----
mean loss: 236.07
 ---- batch: 090 ----
mean loss: 233.33
train mean loss: 227.62
epoch train time: 0:00:16.969964
elapsed time: 0:20:19.775598
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 00:03:08.767913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.44
 ---- batch: 020 ----
mean loss: 222.94
 ---- batch: 030 ----
mean loss: 225.45
 ---- batch: 040 ----
mean loss: 233.39
 ---- batch: 050 ----
mean loss: 237.18
 ---- batch: 060 ----
mean loss: 228.56
 ---- batch: 070 ----
mean loss: 229.93
 ---- batch: 080 ----
mean loss: 228.42
 ---- batch: 090 ----
mean loss: 228.82
train mean loss: 228.13
epoch train time: 0:00:17.008921
elapsed time: 0:20:36.785719
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 00:03:25.778056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.65
 ---- batch: 020 ----
mean loss: 224.06
 ---- batch: 030 ----
mean loss: 231.55
 ---- batch: 040 ----
mean loss: 228.07
 ---- batch: 050 ----
mean loss: 225.07
 ---- batch: 060 ----
mean loss: 232.24
 ---- batch: 070 ----
mean loss: 219.68
 ---- batch: 080 ----
mean loss: 224.25
 ---- batch: 090 ----
mean loss: 224.31
train mean loss: 225.74
epoch train time: 0:00:17.112177
elapsed time: 0:20:53.899175
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 00:03:42.891547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.59
 ---- batch: 020 ----
mean loss: 210.54
 ---- batch: 030 ----
mean loss: 225.63
 ---- batch: 040 ----
mean loss: 224.27
 ---- batch: 050 ----
mean loss: 227.68
 ---- batch: 060 ----
mean loss: 224.46
 ---- batch: 070 ----
mean loss: 228.10
 ---- batch: 080 ----
mean loss: 231.51
 ---- batch: 090 ----
mean loss: 226.02
train mean loss: 223.66
epoch train time: 0:00:17.057322
elapsed time: 0:21:10.957791
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 00:03:59.950203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.13
 ---- batch: 020 ----
mean loss: 227.51
 ---- batch: 030 ----
mean loss: 229.11
 ---- batch: 040 ----
mean loss: 222.85
 ---- batch: 050 ----
mean loss: 218.01
 ---- batch: 060 ----
mean loss: 225.97
 ---- batch: 070 ----
mean loss: 225.03
 ---- batch: 080 ----
mean loss: 225.47
 ---- batch: 090 ----
mean loss: 220.08
train mean loss: 224.15
epoch train time: 0:00:17.001796
elapsed time: 0:21:27.960970
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 00:04:16.953219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.53
 ---- batch: 020 ----
mean loss: 227.72
 ---- batch: 030 ----
mean loss: 230.94
 ---- batch: 040 ----
mean loss: 230.02
 ---- batch: 050 ----
mean loss: 223.53
 ---- batch: 060 ----
mean loss: 220.81
 ---- batch: 070 ----
mean loss: 221.77
 ---- batch: 080 ----
mean loss: 217.60
 ---- batch: 090 ----
mean loss: 221.07
train mean loss: 224.19
epoch train time: 0:00:17.007381
elapsed time: 0:21:44.969515
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 00:04:33.961898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.23
 ---- batch: 020 ----
mean loss: 220.78
 ---- batch: 030 ----
mean loss: 227.16
 ---- batch: 040 ----
mean loss: 228.89
 ---- batch: 050 ----
mean loss: 216.63
 ---- batch: 060 ----
mean loss: 220.10
 ---- batch: 070 ----
mean loss: 222.68
 ---- batch: 080 ----
mean loss: 224.88
 ---- batch: 090 ----
mean loss: 223.73
train mean loss: 222.52
epoch train time: 0:00:17.009807
elapsed time: 0:22:01.980722
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 00:04:50.972941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.39
 ---- batch: 020 ----
mean loss: 222.96
 ---- batch: 030 ----
mean loss: 219.67
 ---- batch: 040 ----
mean loss: 223.11
 ---- batch: 050 ----
mean loss: 233.37
 ---- batch: 060 ----
mean loss: 220.05
 ---- batch: 070 ----
mean loss: 223.09
 ---- batch: 080 ----
mean loss: 224.68
 ---- batch: 090 ----
mean loss: 225.51
train mean loss: 223.92
epoch train time: 0:00:17.005919
elapsed time: 0:22:18.987770
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 00:05:07.980121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.81
 ---- batch: 020 ----
mean loss: 223.38
 ---- batch: 030 ----
mean loss: 221.87
 ---- batch: 040 ----
mean loss: 235.65
 ---- batch: 050 ----
mean loss: 216.41
 ---- batch: 060 ----
mean loss: 217.52
 ---- batch: 070 ----
mean loss: 222.08
 ---- batch: 080 ----
mean loss: 221.30
 ---- batch: 090 ----
mean loss: 220.85
train mean loss: 222.15
epoch train time: 0:00:17.001151
elapsed time: 0:22:35.990165
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 00:05:24.982511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.19
 ---- batch: 020 ----
mean loss: 221.93
 ---- batch: 030 ----
mean loss: 220.58
 ---- batch: 040 ----
mean loss: 228.46
 ---- batch: 050 ----
mean loss: 215.13
 ---- batch: 060 ----
mean loss: 215.05
 ---- batch: 070 ----
mean loss: 230.48
 ---- batch: 080 ----
mean loss: 216.18
 ---- batch: 090 ----
mean loss: 216.92
train mean loss: 219.71
epoch train time: 0:00:16.983511
elapsed time: 0:22:52.974929
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 00:05:41.967138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.76
 ---- batch: 020 ----
mean loss: 216.66
 ---- batch: 030 ----
mean loss: 217.15
 ---- batch: 040 ----
mean loss: 224.76
 ---- batch: 050 ----
mean loss: 217.50
 ---- batch: 060 ----
mean loss: 228.03
 ---- batch: 070 ----
mean loss: 211.86
 ---- batch: 080 ----
mean loss: 220.72
 ---- batch: 090 ----
mean loss: 221.11
train mean loss: 218.90
epoch train time: 0:00:17.009799
elapsed time: 0:23:09.985850
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 00:05:58.978147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.95
 ---- batch: 020 ----
mean loss: 212.44
 ---- batch: 030 ----
mean loss: 215.22
 ---- batch: 040 ----
mean loss: 216.99
 ---- batch: 050 ----
mean loss: 215.21
 ---- batch: 060 ----
mean loss: 215.96
 ---- batch: 070 ----
mean loss: 216.49
 ---- batch: 080 ----
mean loss: 218.48
 ---- batch: 090 ----
mean loss: 225.87
train mean loss: 217.15
epoch train time: 0:00:16.993026
elapsed time: 0:23:26.980031
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 00:06:15.972388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.27
 ---- batch: 020 ----
mean loss: 222.28
 ---- batch: 030 ----
mean loss: 225.50
 ---- batch: 040 ----
mean loss: 215.55
 ---- batch: 050 ----
mean loss: 220.52
 ---- batch: 060 ----
mean loss: 217.74
 ---- batch: 070 ----
mean loss: 219.45
 ---- batch: 080 ----
mean loss: 209.70
 ---- batch: 090 ----
mean loss: 218.72
train mean loss: 218.19
epoch train time: 0:00:17.006189
elapsed time: 0:23:43.987570
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 00:06:32.979959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.41
 ---- batch: 020 ----
mean loss: 215.79
 ---- batch: 030 ----
mean loss: 214.88
 ---- batch: 040 ----
mean loss: 215.54
 ---- batch: 050 ----
mean loss: 218.75
 ---- batch: 060 ----
mean loss: 224.36
 ---- batch: 070 ----
mean loss: 210.75
 ---- batch: 080 ----
mean loss: 224.84
 ---- batch: 090 ----
mean loss: 212.27
train mean loss: 216.71
epoch train time: 0:00:17.009474
elapsed time: 0:24:00.998367
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 00:06:49.990690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.33
 ---- batch: 020 ----
mean loss: 212.28
 ---- batch: 030 ----
mean loss: 217.48
 ---- batch: 040 ----
mean loss: 212.01
 ---- batch: 050 ----
mean loss: 214.07
 ---- batch: 060 ----
mean loss: 217.50
 ---- batch: 070 ----
mean loss: 215.44
 ---- batch: 080 ----
mean loss: 215.18
 ---- batch: 090 ----
mean loss: 216.73
train mean loss: 215.52
epoch train time: 0:00:16.996340
elapsed time: 0:24:17.995954
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 00:07:06.988296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.73
 ---- batch: 020 ----
mean loss: 209.52
 ---- batch: 030 ----
mean loss: 216.38
 ---- batch: 040 ----
mean loss: 210.32
 ---- batch: 050 ----
mean loss: 218.91
 ---- batch: 060 ----
mean loss: 214.74
 ---- batch: 070 ----
mean loss: 207.36
 ---- batch: 080 ----
mean loss: 215.73
 ---- batch: 090 ----
mean loss: 223.96
train mean loss: 214.58
epoch train time: 0:00:16.971294
elapsed time: 0:24:34.968477
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 00:07:23.960826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.79
 ---- batch: 020 ----
mean loss: 224.85
 ---- batch: 030 ----
mean loss: 216.28
 ---- batch: 040 ----
mean loss: 223.96
 ---- batch: 050 ----
mean loss: 213.97
 ---- batch: 060 ----
mean loss: 206.65
 ---- batch: 070 ----
mean loss: 203.70
 ---- batch: 080 ----
mean loss: 211.79
 ---- batch: 090 ----
mean loss: 223.29
train mean loss: 216.73
epoch train time: 0:00:16.999457
elapsed time: 0:24:51.969187
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 00:07:40.961568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.59
 ---- batch: 020 ----
mean loss: 211.49
 ---- batch: 030 ----
mean loss: 221.77
 ---- batch: 040 ----
mean loss: 216.82
 ---- batch: 050 ----
mean loss: 211.12
 ---- batch: 060 ----
mean loss: 217.21
 ---- batch: 070 ----
mean loss: 209.93
 ---- batch: 080 ----
mean loss: 215.20
 ---- batch: 090 ----
mean loss: 205.96
train mean loss: 214.08
epoch train time: 0:00:16.980569
elapsed time: 0:25:08.951054
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 00:07:57.943405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.87
 ---- batch: 020 ----
mean loss: 214.41
 ---- batch: 030 ----
mean loss: 213.51
 ---- batch: 040 ----
mean loss: 216.60
 ---- batch: 050 ----
mean loss: 200.94
 ---- batch: 060 ----
mean loss: 217.78
 ---- batch: 070 ----
mean loss: 217.17
 ---- batch: 080 ----
mean loss: 212.35
 ---- batch: 090 ----
mean loss: 213.76
train mean loss: 213.62
epoch train time: 0:00:16.970908
elapsed time: 0:25:25.923252
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 00:08:14.915594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.08
 ---- batch: 020 ----
mean loss: 214.33
 ---- batch: 030 ----
mean loss: 212.98
 ---- batch: 040 ----
mean loss: 215.83
 ---- batch: 050 ----
mean loss: 208.19
 ---- batch: 060 ----
mean loss: 223.90
 ---- batch: 070 ----
mean loss: 210.06
 ---- batch: 080 ----
mean loss: 213.08
 ---- batch: 090 ----
mean loss: 208.79
train mean loss: 214.10
epoch train time: 0:00:16.982811
elapsed time: 0:25:42.907290
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 00:08:31.899609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.37
 ---- batch: 020 ----
mean loss: 215.57
 ---- batch: 030 ----
mean loss: 216.60
 ---- batch: 040 ----
mean loss: 211.77
 ---- batch: 050 ----
mean loss: 215.94
 ---- batch: 060 ----
mean loss: 213.42
 ---- batch: 070 ----
mean loss: 210.80
 ---- batch: 080 ----
mean loss: 211.28
 ---- batch: 090 ----
mean loss: 214.28
train mean loss: 213.56
epoch train time: 0:00:16.977817
elapsed time: 0:25:59.886379
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 00:08:48.878682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.68
 ---- batch: 020 ----
mean loss: 211.01
 ---- batch: 030 ----
mean loss: 203.94
 ---- batch: 040 ----
mean loss: 211.85
 ---- batch: 050 ----
mean loss: 218.86
 ---- batch: 060 ----
mean loss: 218.99
 ---- batch: 070 ----
mean loss: 210.82
 ---- batch: 080 ----
mean loss: 220.81
 ---- batch: 090 ----
mean loss: 211.49
train mean loss: 212.86
epoch train time: 0:00:17.059940
elapsed time: 0:26:16.947537
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 00:09:05.939876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.90
 ---- batch: 020 ----
mean loss: 220.72
 ---- batch: 030 ----
mean loss: 206.84
 ---- batch: 040 ----
mean loss: 214.45
 ---- batch: 050 ----
mean loss: 222.36
 ---- batch: 060 ----
mean loss: 218.44
 ---- batch: 070 ----
mean loss: 218.42
 ---- batch: 080 ----
mean loss: 211.23
 ---- batch: 090 ----
mean loss: 206.15
train mean loss: 214.03
epoch train time: 0:00:16.988098
elapsed time: 0:26:33.936888
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 00:09:22.929271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.53
 ---- batch: 020 ----
mean loss: 219.29
 ---- batch: 030 ----
mean loss: 213.58
 ---- batch: 040 ----
mean loss: 211.33
 ---- batch: 050 ----
mean loss: 207.86
 ---- batch: 060 ----
mean loss: 212.22
 ---- batch: 070 ----
mean loss: 209.24
 ---- batch: 080 ----
mean loss: 216.92
 ---- batch: 090 ----
mean loss: 201.34
train mean loss: 211.09
epoch train time: 0:00:17.001352
elapsed time: 0:26:50.939558
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 00:09:39.931889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.62
 ---- batch: 020 ----
mean loss: 214.87
 ---- batch: 030 ----
mean loss: 210.71
 ---- batch: 040 ----
mean loss: 215.01
 ---- batch: 050 ----
mean loss: 209.86
 ---- batch: 060 ----
mean loss: 219.84
 ---- batch: 070 ----
mean loss: 220.50
 ---- batch: 080 ----
mean loss: 211.08
 ---- batch: 090 ----
mean loss: 205.87
train mean loss: 212.77
epoch train time: 0:00:17.020915
elapsed time: 0:27:07.961767
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 00:09:56.954182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.63
 ---- batch: 020 ----
mean loss: 201.94
 ---- batch: 030 ----
mean loss: 217.36
 ---- batch: 040 ----
mean loss: 203.05
 ---- batch: 050 ----
mean loss: 208.15
 ---- batch: 060 ----
mean loss: 218.69
 ---- batch: 070 ----
mean loss: 210.42
 ---- batch: 080 ----
mean loss: 207.85
 ---- batch: 090 ----
mean loss: 212.89
train mean loss: 209.75
epoch train time: 0:00:17.002564
elapsed time: 0:27:24.965614
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 00:10:13.957982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.66
 ---- batch: 020 ----
mean loss: 210.29
 ---- batch: 030 ----
mean loss: 208.74
 ---- batch: 040 ----
mean loss: 204.00
 ---- batch: 050 ----
mean loss: 213.37
 ---- batch: 060 ----
mean loss: 209.64
 ---- batch: 070 ----
mean loss: 206.62
 ---- batch: 080 ----
mean loss: 212.04
 ---- batch: 090 ----
mean loss: 209.93
train mean loss: 209.75
epoch train time: 0:00:17.035760
elapsed time: 0:27:42.002561
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 00:10:30.994942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.41
 ---- batch: 020 ----
mean loss: 204.09
 ---- batch: 030 ----
mean loss: 207.52
 ---- batch: 040 ----
mean loss: 216.07
 ---- batch: 050 ----
mean loss: 211.29
 ---- batch: 060 ----
mean loss: 202.55
 ---- batch: 070 ----
mean loss: 215.86
 ---- batch: 080 ----
mean loss: 205.74
 ---- batch: 090 ----
mean loss: 208.13
train mean loss: 209.46
epoch train time: 0:00:16.982367
elapsed time: 0:27:58.986275
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 00:10:47.978669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.35
 ---- batch: 020 ----
mean loss: 205.39
 ---- batch: 030 ----
mean loss: 206.98
 ---- batch: 040 ----
mean loss: 210.51
 ---- batch: 050 ----
mean loss: 207.71
 ---- batch: 060 ----
mean loss: 207.80
 ---- batch: 070 ----
mean loss: 216.48
 ---- batch: 080 ----
mean loss: 213.96
 ---- batch: 090 ----
mean loss: 215.06
train mean loss: 209.71
epoch train time: 0:00:17.017255
elapsed time: 0:28:16.004817
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 00:11:04.997149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.51
 ---- batch: 020 ----
mean loss: 215.46
 ---- batch: 030 ----
mean loss: 203.55
 ---- batch: 040 ----
mean loss: 207.06
 ---- batch: 050 ----
mean loss: 202.74
 ---- batch: 060 ----
mean loss: 213.56
 ---- batch: 070 ----
mean loss: 211.11
 ---- batch: 080 ----
mean loss: 214.24
 ---- batch: 090 ----
mean loss: 206.11
train mean loss: 209.25
epoch train time: 0:00:17.021991
elapsed time: 0:28:33.028043
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 00:11:22.020430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.18
 ---- batch: 020 ----
mean loss: 200.54
 ---- batch: 030 ----
mean loss: 202.61
 ---- batch: 040 ----
mean loss: 204.01
 ---- batch: 050 ----
mean loss: 213.33
 ---- batch: 060 ----
mean loss: 209.53
 ---- batch: 070 ----
mean loss: 203.85
 ---- batch: 080 ----
mean loss: 206.74
 ---- batch: 090 ----
mean loss: 207.65
train mean loss: 207.28
epoch train time: 0:00:16.996743
elapsed time: 0:28:50.026126
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 00:11:39.018482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.45
 ---- batch: 020 ----
mean loss: 203.10
 ---- batch: 030 ----
mean loss: 206.32
 ---- batch: 040 ----
mean loss: 209.01
 ---- batch: 050 ----
mean loss: 216.21
 ---- batch: 060 ----
mean loss: 197.38
 ---- batch: 070 ----
mean loss: 199.77
 ---- batch: 080 ----
mean loss: 207.07
 ---- batch: 090 ----
mean loss: 208.35
train mean loss: 206.41
epoch train time: 0:00:17.005085
elapsed time: 0:29:07.032495
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 00:11:56.024844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.58
 ---- batch: 020 ----
mean loss: 209.39
 ---- batch: 030 ----
mean loss: 208.02
 ---- batch: 040 ----
mean loss: 204.43
 ---- batch: 050 ----
mean loss: 211.37
 ---- batch: 060 ----
mean loss: 208.83
 ---- batch: 070 ----
mean loss: 200.88
 ---- batch: 080 ----
mean loss: 199.29
 ---- batch: 090 ----
mean loss: 209.12
train mean loss: 206.65
epoch train time: 0:00:16.931096
elapsed time: 0:29:23.964799
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 00:12:12.957192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.02
 ---- batch: 020 ----
mean loss: 201.10
 ---- batch: 030 ----
mean loss: 203.06
 ---- batch: 040 ----
mean loss: 204.17
 ---- batch: 050 ----
mean loss: 202.09
 ---- batch: 060 ----
mean loss: 208.45
 ---- batch: 070 ----
mean loss: 206.09
 ---- batch: 080 ----
mean loss: 203.04
 ---- batch: 090 ----
mean loss: 208.59
train mean loss: 205.89
epoch train time: 0:00:16.927427
elapsed time: 0:29:40.893512
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 00:12:29.885901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.50
 ---- batch: 020 ----
mean loss: 206.27
 ---- batch: 030 ----
mean loss: 201.40
 ---- batch: 040 ----
mean loss: 205.83
 ---- batch: 050 ----
mean loss: 209.34
 ---- batch: 060 ----
mean loss: 206.79
 ---- batch: 070 ----
mean loss: 212.86
 ---- batch: 080 ----
mean loss: 200.59
 ---- batch: 090 ----
mean loss: 207.12
train mean loss: 207.08
epoch train time: 0:00:16.923413
elapsed time: 0:29:57.818393
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 00:12:46.810740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.19
 ---- batch: 020 ----
mean loss: 212.87
 ---- batch: 030 ----
mean loss: 205.79
 ---- batch: 040 ----
mean loss: 206.32
 ---- batch: 050 ----
mean loss: 209.37
 ---- batch: 060 ----
mean loss: 208.66
 ---- batch: 070 ----
mean loss: 195.55
 ---- batch: 080 ----
mean loss: 200.31
 ---- batch: 090 ----
mean loss: 198.94
train mean loss: 205.30
epoch train time: 0:00:16.965531
elapsed time: 0:30:14.785159
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 00:13:03.777543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.66
 ---- batch: 020 ----
mean loss: 200.29
 ---- batch: 030 ----
mean loss: 212.63
 ---- batch: 040 ----
mean loss: 204.26
 ---- batch: 050 ----
mean loss: 198.22
 ---- batch: 060 ----
mean loss: 207.73
 ---- batch: 070 ----
mean loss: 204.92
 ---- batch: 080 ----
mean loss: 209.72
 ---- batch: 090 ----
mean loss: 210.69
train mean loss: 205.41
epoch train time: 0:00:16.956360
elapsed time: 0:30:31.742789
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 00:13:20.735250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.38
 ---- batch: 020 ----
mean loss: 209.66
 ---- batch: 030 ----
mean loss: 204.17
 ---- batch: 040 ----
mean loss: 199.06
 ---- batch: 050 ----
mean loss: 207.71
 ---- batch: 060 ----
mean loss: 208.18
 ---- batch: 070 ----
mean loss: 202.80
 ---- batch: 080 ----
mean loss: 205.51
 ---- batch: 090 ----
mean loss: 203.87
train mean loss: 204.80
epoch train time: 0:00:16.949417
elapsed time: 0:30:48.694009
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 00:13:37.686284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.05
 ---- batch: 020 ----
mean loss: 207.01
 ---- batch: 030 ----
mean loss: 201.15
 ---- batch: 040 ----
mean loss: 207.85
 ---- batch: 050 ----
mean loss: 198.48
 ---- batch: 060 ----
mean loss: 206.41
 ---- batch: 070 ----
mean loss: 202.96
 ---- batch: 080 ----
mean loss: 213.63
 ---- batch: 090 ----
mean loss: 208.22
train mean loss: 204.54
epoch train time: 0:00:16.933975
elapsed time: 0:31:05.629105
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 00:13:54.621449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.78
 ---- batch: 020 ----
mean loss: 204.92
 ---- batch: 030 ----
mean loss: 198.96
 ---- batch: 040 ----
mean loss: 204.55
 ---- batch: 050 ----
mean loss: 198.68
 ---- batch: 060 ----
mean loss: 212.08
 ---- batch: 070 ----
mean loss: 210.71
 ---- batch: 080 ----
mean loss: 208.05
 ---- batch: 090 ----
mean loss: 202.98
train mean loss: 206.20
epoch train time: 0:00:17.036318
elapsed time: 0:31:22.666650
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 00:14:11.659019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.43
 ---- batch: 020 ----
mean loss: 206.37
 ---- batch: 030 ----
mean loss: 207.54
 ---- batch: 040 ----
mean loss: 198.71
 ---- batch: 050 ----
mean loss: 200.88
 ---- batch: 060 ----
mean loss: 206.74
 ---- batch: 070 ----
mean loss: 207.48
 ---- batch: 080 ----
mean loss: 204.96
 ---- batch: 090 ----
mean loss: 204.94
train mean loss: 205.28
epoch train time: 0:00:16.915940
elapsed time: 0:31:39.583844
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 00:14:28.576156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.72
 ---- batch: 020 ----
mean loss: 202.50
 ---- batch: 030 ----
mean loss: 208.63
 ---- batch: 040 ----
mean loss: 205.27
 ---- batch: 050 ----
mean loss: 201.39
 ---- batch: 060 ----
mean loss: 208.42
 ---- batch: 070 ----
mean loss: 200.54
 ---- batch: 080 ----
mean loss: 199.69
 ---- batch: 090 ----
mean loss: 202.54
train mean loss: 204.65
epoch train time: 0:00:16.927433
elapsed time: 0:31:56.512460
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 00:14:45.504771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.22
 ---- batch: 020 ----
mean loss: 200.78
 ---- batch: 030 ----
mean loss: 212.81
 ---- batch: 040 ----
mean loss: 207.41
 ---- batch: 050 ----
mean loss: 207.09
 ---- batch: 060 ----
mean loss: 202.14
 ---- batch: 070 ----
mean loss: 206.21
 ---- batch: 080 ----
mean loss: 196.81
 ---- batch: 090 ----
mean loss: 197.68
train mean loss: 202.35
epoch train time: 0:00:16.913152
elapsed time: 0:32:13.426778
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 00:15:02.419098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.34
 ---- batch: 020 ----
mean loss: 206.64
 ---- batch: 030 ----
mean loss: 198.87
 ---- batch: 040 ----
mean loss: 199.48
 ---- batch: 050 ----
mean loss: 196.73
 ---- batch: 060 ----
mean loss: 206.83
 ---- batch: 070 ----
mean loss: 212.71
 ---- batch: 080 ----
mean loss: 203.94
 ---- batch: 090 ----
mean loss: 208.15
train mean loss: 203.28
epoch train time: 0:00:16.921947
elapsed time: 0:32:30.349934
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 00:15:19.342252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.06
 ---- batch: 020 ----
mean loss: 205.16
 ---- batch: 030 ----
mean loss: 201.49
 ---- batch: 040 ----
mean loss: 209.46
 ---- batch: 050 ----
mean loss: 205.88
 ---- batch: 060 ----
mean loss: 209.60
 ---- batch: 070 ----
mean loss: 194.32
 ---- batch: 080 ----
mean loss: 202.85
 ---- batch: 090 ----
mean loss: 200.17
train mean loss: 204.23
epoch train time: 0:00:16.961119
elapsed time: 0:32:47.312241
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 00:15:36.304609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.81
 ---- batch: 020 ----
mean loss: 205.43
 ---- batch: 030 ----
mean loss: 197.09
 ---- batch: 040 ----
mean loss: 201.96
 ---- batch: 050 ----
mean loss: 203.89
 ---- batch: 060 ----
mean loss: 204.96
 ---- batch: 070 ----
mean loss: 206.15
 ---- batch: 080 ----
mean loss: 190.76
 ---- batch: 090 ----
mean loss: 205.16
train mean loss: 202.15
epoch train time: 0:00:16.933447
elapsed time: 0:33:04.246919
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 00:15:53.239254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.91
 ---- batch: 020 ----
mean loss: 218.02
 ---- batch: 030 ----
mean loss: 216.46
 ---- batch: 040 ----
mean loss: 205.20
 ---- batch: 050 ----
mean loss: 213.45
 ---- batch: 060 ----
mean loss: 204.21
 ---- batch: 070 ----
mean loss: 204.63
 ---- batch: 080 ----
mean loss: 199.23
 ---- batch: 090 ----
mean loss: 200.40
train mean loss: 207.00
epoch train time: 0:00:16.972734
elapsed time: 0:33:21.220894
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 00:16:10.213228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.72
 ---- batch: 020 ----
mean loss: 198.24
 ---- batch: 030 ----
mean loss: 200.56
 ---- batch: 040 ----
mean loss: 197.72
 ---- batch: 050 ----
mean loss: 202.55
 ---- batch: 060 ----
mean loss: 205.90
 ---- batch: 070 ----
mean loss: 208.50
 ---- batch: 080 ----
mean loss: 207.65
 ---- batch: 090 ----
mean loss: 207.67
train mean loss: 201.91
epoch train time: 0:00:16.940418
elapsed time: 0:33:38.162566
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 00:16:27.154912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.18
 ---- batch: 020 ----
mean loss: 197.32
 ---- batch: 030 ----
mean loss: 203.64
 ---- batch: 040 ----
mean loss: 202.68
 ---- batch: 050 ----
mean loss: 199.88
 ---- batch: 060 ----
mean loss: 199.67
 ---- batch: 070 ----
mean loss: 219.19
 ---- batch: 080 ----
mean loss: 212.98
 ---- batch: 090 ----
mean loss: 219.36
train mean loss: 205.83
epoch train time: 0:00:16.989849
elapsed time: 0:33:55.153686
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 00:16:44.146062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.30
 ---- batch: 020 ----
mean loss: 205.49
 ---- batch: 030 ----
mean loss: 202.41
 ---- batch: 040 ----
mean loss: 207.02
 ---- batch: 050 ----
mean loss: 206.69
 ---- batch: 060 ----
mean loss: 201.83
 ---- batch: 070 ----
mean loss: 199.89
 ---- batch: 080 ----
mean loss: 204.67
 ---- batch: 090 ----
mean loss: 194.81
train mean loss: 202.03
epoch train time: 0:00:17.018543
elapsed time: 0:34:12.173550
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 00:17:01.165921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.81
 ---- batch: 020 ----
mean loss: 201.32
 ---- batch: 030 ----
mean loss: 198.09
 ---- batch: 040 ----
mean loss: 206.09
 ---- batch: 050 ----
mean loss: 198.52
 ---- batch: 060 ----
mean loss: 204.77
 ---- batch: 070 ----
mean loss: 200.59
 ---- batch: 080 ----
mean loss: 205.17
 ---- batch: 090 ----
mean loss: 200.07
train mean loss: 202.15
epoch train time: 0:00:16.967098
elapsed time: 0:34:29.141919
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 00:17:18.134285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.16
 ---- batch: 020 ----
mean loss: 203.90
 ---- batch: 030 ----
mean loss: 197.89
 ---- batch: 040 ----
mean loss: 208.38
 ---- batch: 050 ----
mean loss: 205.73
 ---- batch: 060 ----
mean loss: 198.72
 ---- batch: 070 ----
mean loss: 192.99
 ---- batch: 080 ----
mean loss: 204.99
 ---- batch: 090 ----
mean loss: 195.50
train mean loss: 201.34
epoch train time: 0:00:17.019266
elapsed time: 0:34:46.162452
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 00:17:35.154659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.26
 ---- batch: 020 ----
mean loss: 201.65
 ---- batch: 030 ----
mean loss: 208.92
 ---- batch: 040 ----
mean loss: 204.53
 ---- batch: 050 ----
mean loss: 212.79
 ---- batch: 060 ----
mean loss: 202.05
 ---- batch: 070 ----
mean loss: 205.51
 ---- batch: 080 ----
mean loss: 207.63
 ---- batch: 090 ----
mean loss: 197.62
train mean loss: 203.32
epoch train time: 0:00:16.961896
elapsed time: 0:35:03.125423
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 00:17:52.117772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.28
 ---- batch: 020 ----
mean loss: 202.57
 ---- batch: 030 ----
mean loss: 198.17
 ---- batch: 040 ----
mean loss: 204.63
 ---- batch: 050 ----
mean loss: 198.53
 ---- batch: 060 ----
mean loss: 199.28
 ---- batch: 070 ----
mean loss: 193.30
 ---- batch: 080 ----
mean loss: 197.12
 ---- batch: 090 ----
mean loss: 196.77
train mean loss: 199.64
epoch train time: 0:00:16.970420
elapsed time: 0:35:20.097080
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 00:18:09.089407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.19
 ---- batch: 020 ----
mean loss: 191.71
 ---- batch: 030 ----
mean loss: 194.73
 ---- batch: 040 ----
mean loss: 201.93
 ---- batch: 050 ----
mean loss: 202.49
 ---- batch: 060 ----
mean loss: 200.36
 ---- batch: 070 ----
mean loss: 202.04
 ---- batch: 080 ----
mean loss: 200.73
 ---- batch: 090 ----
mean loss: 198.55
train mean loss: 199.45
epoch train time: 0:00:16.921766
elapsed time: 0:35:37.020111
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 00:18:26.012428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.25
 ---- batch: 020 ----
mean loss: 197.25
 ---- batch: 030 ----
mean loss: 193.31
 ---- batch: 040 ----
mean loss: 199.96
 ---- batch: 050 ----
mean loss: 199.38
 ---- batch: 060 ----
mean loss: 201.35
 ---- batch: 070 ----
mean loss: 201.83
 ---- batch: 080 ----
mean loss: 199.25
 ---- batch: 090 ----
mean loss: 203.22
train mean loss: 198.92
epoch train time: 0:00:16.938984
elapsed time: 0:35:53.960410
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 00:18:42.952762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.29
 ---- batch: 020 ----
mean loss: 200.48
 ---- batch: 030 ----
mean loss: 195.06
 ---- batch: 040 ----
mean loss: 196.52
 ---- batch: 050 ----
mean loss: 198.76
 ---- batch: 060 ----
mean loss: 195.67
 ---- batch: 070 ----
mean loss: 201.06
 ---- batch: 080 ----
mean loss: 203.59
 ---- batch: 090 ----
mean loss: 206.55
train mean loss: 199.17
epoch train time: 0:00:16.996480
elapsed time: 0:36:10.958134
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 00:18:59.950562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.19
 ---- batch: 020 ----
mean loss: 200.19
 ---- batch: 030 ----
mean loss: 203.08
 ---- batch: 040 ----
mean loss: 203.12
 ---- batch: 050 ----
mean loss: 198.09
 ---- batch: 060 ----
mean loss: 196.66
 ---- batch: 070 ----
mean loss: 195.21
 ---- batch: 080 ----
mean loss: 198.13
 ---- batch: 090 ----
mean loss: 201.91
train mean loss: 198.89
epoch train time: 0:00:17.036760
elapsed time: 0:36:27.996503
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 00:19:16.988732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.93
 ---- batch: 020 ----
mean loss: 200.70
 ---- batch: 030 ----
mean loss: 196.81
 ---- batch: 040 ----
mean loss: 201.28
 ---- batch: 050 ----
mean loss: 198.15
 ---- batch: 060 ----
mean loss: 199.39
 ---- batch: 070 ----
mean loss: 191.27
 ---- batch: 080 ----
mean loss: 194.93
 ---- batch: 090 ----
mean loss: 204.57
train mean loss: 198.12
epoch train time: 0:00:16.959160
elapsed time: 0:36:44.956903
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 00:19:33.949340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.06
 ---- batch: 020 ----
mean loss: 197.17
 ---- batch: 030 ----
mean loss: 201.96
 ---- batch: 040 ----
mean loss: 205.12
 ---- batch: 050 ----
mean loss: 198.79
 ---- batch: 060 ----
mean loss: 194.74
 ---- batch: 070 ----
mean loss: 201.12
 ---- batch: 080 ----
mean loss: 202.26
 ---- batch: 090 ----
mean loss: 206.43
train mean loss: 201.17
epoch train time: 0:00:16.999321
elapsed time: 0:37:01.957591
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 00:19:50.949942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.75
 ---- batch: 020 ----
mean loss: 196.04
 ---- batch: 030 ----
mean loss: 196.75
 ---- batch: 040 ----
mean loss: 203.20
 ---- batch: 050 ----
mean loss: 197.52
 ---- batch: 060 ----
mean loss: 193.76
 ---- batch: 070 ----
mean loss: 200.09
 ---- batch: 080 ----
mean loss: 204.04
 ---- batch: 090 ----
mean loss: 200.10
train mean loss: 198.93
epoch train time: 0:00:17.029745
elapsed time: 0:37:18.988564
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 00:20:07.980986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.10
 ---- batch: 020 ----
mean loss: 195.51
 ---- batch: 030 ----
mean loss: 196.76
 ---- batch: 040 ----
mean loss: 202.32
 ---- batch: 050 ----
mean loss: 198.12
 ---- batch: 060 ----
mean loss: 195.12
 ---- batch: 070 ----
mean loss: 198.62
 ---- batch: 080 ----
mean loss: 196.07
 ---- batch: 090 ----
mean loss: 206.31
train mean loss: 198.35
epoch train time: 0:00:16.991982
elapsed time: 0:37:35.981925
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 00:20:24.974295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.68
 ---- batch: 020 ----
mean loss: 195.08
 ---- batch: 030 ----
mean loss: 206.48
 ---- batch: 040 ----
mean loss: 208.40
 ---- batch: 050 ----
mean loss: 199.67
 ---- batch: 060 ----
mean loss: 188.18
 ---- batch: 070 ----
mean loss: 198.39
 ---- batch: 080 ----
mean loss: 205.30
 ---- batch: 090 ----
mean loss: 199.49
train mean loss: 200.61
epoch train time: 0:00:17.027297
elapsed time: 0:37:53.010496
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 00:20:42.002846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.78
 ---- batch: 020 ----
mean loss: 197.03
 ---- batch: 030 ----
mean loss: 193.93
 ---- batch: 040 ----
mean loss: 194.00
 ---- batch: 050 ----
mean loss: 199.74
 ---- batch: 060 ----
mean loss: 198.79
 ---- batch: 070 ----
mean loss: 194.60
 ---- batch: 080 ----
mean loss: 203.35
 ---- batch: 090 ----
mean loss: 193.84
train mean loss: 197.31
epoch train time: 0:00:17.013257
elapsed time: 0:38:10.025078
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 00:20:59.017418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.02
 ---- batch: 020 ----
mean loss: 190.20
 ---- batch: 030 ----
mean loss: 193.56
 ---- batch: 040 ----
mean loss: 198.65
 ---- batch: 050 ----
mean loss: 199.27
 ---- batch: 060 ----
mean loss: 193.75
 ---- batch: 070 ----
mean loss: 197.24
 ---- batch: 080 ----
mean loss: 200.74
 ---- batch: 090 ----
mean loss: 201.21
train mean loss: 198.11
epoch train time: 0:00:17.018419
elapsed time: 0:38:27.044735
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 00:21:16.037088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.62
 ---- batch: 020 ----
mean loss: 197.87
 ---- batch: 030 ----
mean loss: 194.75
 ---- batch: 040 ----
mean loss: 194.88
 ---- batch: 050 ----
mean loss: 198.99
 ---- batch: 060 ----
mean loss: 208.00
 ---- batch: 070 ----
mean loss: 196.83
 ---- batch: 080 ----
mean loss: 193.99
 ---- batch: 090 ----
mean loss: 192.02
train mean loss: 197.15
epoch train time: 0:00:17.015078
elapsed time: 0:38:44.061020
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 00:21:33.053403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.94
 ---- batch: 020 ----
mean loss: 200.49
 ---- batch: 030 ----
mean loss: 198.24
 ---- batch: 040 ----
mean loss: 201.67
 ---- batch: 050 ----
mean loss: 191.95
 ---- batch: 060 ----
mean loss: 192.82
 ---- batch: 070 ----
mean loss: 193.27
 ---- batch: 080 ----
mean loss: 198.16
 ---- batch: 090 ----
mean loss: 201.92
train mean loss: 198.69
epoch train time: 0:00:16.996048
elapsed time: 0:39:01.058436
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 00:21:50.050770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.75
 ---- batch: 020 ----
mean loss: 194.42
 ---- batch: 030 ----
mean loss: 201.02
 ---- batch: 040 ----
mean loss: 197.17
 ---- batch: 050 ----
mean loss: 194.85
 ---- batch: 060 ----
mean loss: 199.16
 ---- batch: 070 ----
mean loss: 195.91
 ---- batch: 080 ----
mean loss: 195.26
 ---- batch: 090 ----
mean loss: 196.80
train mean loss: 197.19
epoch train time: 0:00:17.023932
elapsed time: 0:39:18.083583
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 00:22:07.075959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.05
 ---- batch: 020 ----
mean loss: 197.38
 ---- batch: 030 ----
mean loss: 195.60
 ---- batch: 040 ----
mean loss: 198.22
 ---- batch: 050 ----
mean loss: 193.08
 ---- batch: 060 ----
mean loss: 198.41
 ---- batch: 070 ----
mean loss: 205.53
 ---- batch: 080 ----
mean loss: 200.79
 ---- batch: 090 ----
mean loss: 191.73
train mean loss: 197.69
epoch train time: 0:00:16.946000
elapsed time: 0:39:35.030931
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 00:22:24.023303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.17
 ---- batch: 020 ----
mean loss: 204.66
 ---- batch: 030 ----
mean loss: 203.03
 ---- batch: 040 ----
mean loss: 194.31
 ---- batch: 050 ----
mean loss: 202.46
 ---- batch: 060 ----
mean loss: 194.87
 ---- batch: 070 ----
mean loss: 193.56
 ---- batch: 080 ----
mean loss: 196.78
 ---- batch: 090 ----
mean loss: 203.02
train mean loss: 199.60
epoch train time: 0:00:16.975448
elapsed time: 0:39:52.007624
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 00:22:40.999943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.09
 ---- batch: 020 ----
mean loss: 194.06
 ---- batch: 030 ----
mean loss: 199.32
 ---- batch: 040 ----
mean loss: 193.89
 ---- batch: 050 ----
mean loss: 190.47
 ---- batch: 060 ----
mean loss: 198.82
 ---- batch: 070 ----
mean loss: 198.63
 ---- batch: 080 ----
mean loss: 203.31
 ---- batch: 090 ----
mean loss: 191.05
train mean loss: 195.44
epoch train time: 0:00:16.988198
elapsed time: 0:40:08.997151
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 00:22:57.989479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.22
 ---- batch: 020 ----
mean loss: 194.01
 ---- batch: 030 ----
mean loss: 193.08
 ---- batch: 040 ----
mean loss: 197.47
 ---- batch: 050 ----
mean loss: 197.41
 ---- batch: 060 ----
mean loss: 196.73
 ---- batch: 070 ----
mean loss: 197.54
 ---- batch: 080 ----
mean loss: 198.96
 ---- batch: 090 ----
mean loss: 199.04
train mean loss: 195.96
epoch train time: 0:00:16.985752
elapsed time: 0:40:25.984383
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 00:23:14.976804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.14
 ---- batch: 020 ----
mean loss: 198.02
 ---- batch: 030 ----
mean loss: 205.08
 ---- batch: 040 ----
mean loss: 182.25
 ---- batch: 050 ----
mean loss: 201.39
 ---- batch: 060 ----
mean loss: 194.81
 ---- batch: 070 ----
mean loss: 197.85
 ---- batch: 080 ----
mean loss: 192.89
 ---- batch: 090 ----
mean loss: 201.26
train mean loss: 196.11
epoch train time: 0:00:16.986303
elapsed time: 0:40:42.971969
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 00:23:31.964280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.25
 ---- batch: 020 ----
mean loss: 193.26
 ---- batch: 030 ----
mean loss: 200.99
 ---- batch: 040 ----
mean loss: 202.74
 ---- batch: 050 ----
mean loss: 204.72
 ---- batch: 060 ----
mean loss: 199.13
 ---- batch: 070 ----
mean loss: 203.49
 ---- batch: 080 ----
mean loss: 209.08
 ---- batch: 090 ----
mean loss: 195.22
train mean loss: 200.72
epoch train time: 0:00:17.035386
elapsed time: 0:41:00.008610
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 00:23:49.000908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.80
 ---- batch: 020 ----
mean loss: 203.55
 ---- batch: 030 ----
mean loss: 189.33
 ---- batch: 040 ----
mean loss: 200.14
 ---- batch: 050 ----
mean loss: 191.60
 ---- batch: 060 ----
mean loss: 187.24
 ---- batch: 070 ----
mean loss: 196.27
 ---- batch: 080 ----
mean loss: 189.86
 ---- batch: 090 ----
mean loss: 191.35
train mean loss: 195.64
epoch train time: 0:00:17.058277
elapsed time: 0:41:17.068069
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 00:24:06.060416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.44
 ---- batch: 020 ----
mean loss: 202.77
 ---- batch: 030 ----
mean loss: 195.36
 ---- batch: 040 ----
mean loss: 196.55
 ---- batch: 050 ----
mean loss: 186.73
 ---- batch: 060 ----
mean loss: 201.67
 ---- batch: 070 ----
mean loss: 195.90
 ---- batch: 080 ----
mean loss: 191.80
 ---- batch: 090 ----
mean loss: 196.25
train mean loss: 195.34
epoch train time: 0:00:17.061597
elapsed time: 0:41:34.130888
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 00:24:23.123346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.18
 ---- batch: 020 ----
mean loss: 197.75
 ---- batch: 030 ----
mean loss: 188.71
 ---- batch: 040 ----
mean loss: 199.59
 ---- batch: 050 ----
mean loss: 200.15
 ---- batch: 060 ----
mean loss: 197.50
 ---- batch: 070 ----
mean loss: 191.64
 ---- batch: 080 ----
mean loss: 198.29
 ---- batch: 090 ----
mean loss: 194.09
train mean loss: 194.47
epoch train time: 0:00:17.047805
elapsed time: 0:41:51.179978
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 00:24:40.172322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.71
 ---- batch: 020 ----
mean loss: 196.63
 ---- batch: 030 ----
mean loss: 200.97
 ---- batch: 040 ----
mean loss: 198.34
 ---- batch: 050 ----
mean loss: 194.08
 ---- batch: 060 ----
mean loss: 195.62
 ---- batch: 070 ----
mean loss: 194.76
 ---- batch: 080 ----
mean loss: 189.70
 ---- batch: 090 ----
mean loss: 190.92
train mean loss: 194.94
epoch train time: 0:00:16.996369
elapsed time: 0:42:08.177591
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 00:24:57.169937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.14
 ---- batch: 020 ----
mean loss: 199.23
 ---- batch: 030 ----
mean loss: 198.04
 ---- batch: 040 ----
mean loss: 193.24
 ---- batch: 050 ----
mean loss: 194.76
 ---- batch: 060 ----
mean loss: 198.56
 ---- batch: 070 ----
mean loss: 189.25
 ---- batch: 080 ----
mean loss: 198.26
 ---- batch: 090 ----
mean loss: 200.31
train mean loss: 195.57
epoch train time: 0:00:17.021785
elapsed time: 0:42:25.200674
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 00:25:14.193124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.44
 ---- batch: 020 ----
mean loss: 194.85
 ---- batch: 030 ----
mean loss: 194.97
 ---- batch: 040 ----
mean loss: 195.48
 ---- batch: 050 ----
mean loss: 186.38
 ---- batch: 060 ----
mean loss: 190.85
 ---- batch: 070 ----
mean loss: 197.14
 ---- batch: 080 ----
mean loss: 199.37
 ---- batch: 090 ----
mean loss: 195.21
train mean loss: 194.57
epoch train time: 0:00:17.029486
elapsed time: 0:42:42.232140
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 00:25:31.224113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.21
 ---- batch: 020 ----
mean loss: 197.25
 ---- batch: 030 ----
mean loss: 192.53
 ---- batch: 040 ----
mean loss: 196.69
 ---- batch: 050 ----
mean loss: 195.08
 ---- batch: 060 ----
mean loss: 199.31
 ---- batch: 070 ----
mean loss: 195.65
 ---- batch: 080 ----
mean loss: 190.86
 ---- batch: 090 ----
mean loss: 195.28
train mean loss: 194.80
epoch train time: 0:00:17.009399
elapsed time: 0:42:59.242434
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 00:25:48.234890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.06
 ---- batch: 020 ----
mean loss: 199.64
 ---- batch: 030 ----
mean loss: 191.45
 ---- batch: 040 ----
mean loss: 203.70
 ---- batch: 050 ----
mean loss: 196.19
 ---- batch: 060 ----
mean loss: 197.99
 ---- batch: 070 ----
mean loss: 185.12
 ---- batch: 080 ----
mean loss: 201.16
 ---- batch: 090 ----
mean loss: 195.19
train mean loss: 196.64
epoch train time: 0:00:17.028195
elapsed time: 0:43:16.271956
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 00:26:05.264508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.45
 ---- batch: 020 ----
mean loss: 193.22
 ---- batch: 030 ----
mean loss: 191.78
 ---- batch: 040 ----
mean loss: 192.73
 ---- batch: 050 ----
mean loss: 194.47
 ---- batch: 060 ----
mean loss: 207.91
 ---- batch: 070 ----
mean loss: 198.75
 ---- batch: 080 ----
mean loss: 196.43
 ---- batch: 090 ----
mean loss: 201.53
train mean loss: 196.21
epoch train time: 0:00:17.018450
elapsed time: 0:43:33.291915
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 00:26:22.284372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.41
 ---- batch: 020 ----
mean loss: 197.33
 ---- batch: 030 ----
mean loss: 198.20
 ---- batch: 040 ----
mean loss: 189.59
 ---- batch: 050 ----
mean loss: 192.39
 ---- batch: 060 ----
mean loss: 193.49
 ---- batch: 070 ----
mean loss: 197.29
 ---- batch: 080 ----
mean loss: 189.27
 ---- batch: 090 ----
mean loss: 194.54
train mean loss: 193.98
epoch train time: 0:00:17.029426
elapsed time: 0:43:50.322688
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 00:26:39.315070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.78
 ---- batch: 020 ----
mean loss: 189.86
 ---- batch: 030 ----
mean loss: 198.36
 ---- batch: 040 ----
mean loss: 198.71
 ---- batch: 050 ----
mean loss: 192.96
 ---- batch: 060 ----
mean loss: 188.45
 ---- batch: 070 ----
mean loss: 188.65
 ---- batch: 080 ----
mean loss: 189.31
 ---- batch: 090 ----
mean loss: 192.50
train mean loss: 192.96
epoch train time: 0:00:17.034377
elapsed time: 0:44:07.358348
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 00:26:56.350649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.01
 ---- batch: 020 ----
mean loss: 195.45
 ---- batch: 030 ----
mean loss: 189.14
 ---- batch: 040 ----
mean loss: 194.04
 ---- batch: 050 ----
mean loss: 194.58
 ---- batch: 060 ----
mean loss: 204.39
 ---- batch: 070 ----
mean loss: 196.65
 ---- batch: 080 ----
mean loss: 195.77
 ---- batch: 090 ----
mean loss: 193.81
train mean loss: 194.91
epoch train time: 0:00:17.087700
elapsed time: 0:44:24.447444
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 00:27:13.439707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.38
 ---- batch: 020 ----
mean loss: 196.13
 ---- batch: 030 ----
mean loss: 195.80
 ---- batch: 040 ----
mean loss: 190.14
 ---- batch: 050 ----
mean loss: 187.01
 ---- batch: 060 ----
mean loss: 199.41
 ---- batch: 070 ----
mean loss: 192.50
 ---- batch: 080 ----
mean loss: 189.28
 ---- batch: 090 ----
mean loss: 189.09
train mean loss: 192.87
epoch train time: 0:00:17.022805
elapsed time: 0:44:41.471495
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 00:27:30.463850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.32
 ---- batch: 020 ----
mean loss: 188.54
 ---- batch: 030 ----
mean loss: 201.30
 ---- batch: 040 ----
mean loss: 200.78
 ---- batch: 050 ----
mean loss: 185.43
 ---- batch: 060 ----
mean loss: 194.11
 ---- batch: 070 ----
mean loss: 199.96
 ---- batch: 080 ----
mean loss: 212.27
 ---- batch: 090 ----
mean loss: 194.75
train mean loss: 196.23
epoch train time: 0:00:17.036706
elapsed time: 0:44:58.509457
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 00:27:47.501779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.76
 ---- batch: 020 ----
mean loss: 202.14
 ---- batch: 030 ----
mean loss: 194.56
 ---- batch: 040 ----
mean loss: 196.80
 ---- batch: 050 ----
mean loss: 197.93
 ---- batch: 060 ----
mean loss: 196.47
 ---- batch: 070 ----
mean loss: 190.00
 ---- batch: 080 ----
mean loss: 188.22
 ---- batch: 090 ----
mean loss: 190.05
train mean loss: 194.29
epoch train time: 0:00:17.019034
elapsed time: 0:45:15.529765
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 00:28:04.522194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.64
 ---- batch: 020 ----
mean loss: 191.23
 ---- batch: 030 ----
mean loss: 191.42
 ---- batch: 040 ----
mean loss: 187.39
 ---- batch: 050 ----
mean loss: 187.29
 ---- batch: 060 ----
mean loss: 188.67
 ---- batch: 070 ----
mean loss: 194.89
 ---- batch: 080 ----
mean loss: 196.64
 ---- batch: 090 ----
mean loss: 191.68
train mean loss: 192.49
epoch train time: 0:00:17.009079
elapsed time: 0:45:32.540119
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 00:28:21.532453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.08
 ---- batch: 020 ----
mean loss: 183.87
 ---- batch: 030 ----
mean loss: 196.56
 ---- batch: 040 ----
mean loss: 195.23
 ---- batch: 050 ----
mean loss: 197.56
 ---- batch: 060 ----
mean loss: 191.08
 ---- batch: 070 ----
mean loss: 195.78
 ---- batch: 080 ----
mean loss: 194.14
 ---- batch: 090 ----
mean loss: 192.65
train mean loss: 192.62
epoch train time: 0:00:17.008990
elapsed time: 0:45:49.550346
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 00:28:38.542665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.98
 ---- batch: 020 ----
mean loss: 193.81
 ---- batch: 030 ----
mean loss: 189.14
 ---- batch: 040 ----
mean loss: 197.60
 ---- batch: 050 ----
mean loss: 194.48
 ---- batch: 060 ----
mean loss: 191.52
 ---- batch: 070 ----
mean loss: 200.77
 ---- batch: 080 ----
mean loss: 190.67
 ---- batch: 090 ----
mean loss: 190.33
train mean loss: 192.80
epoch train time: 0:00:17.016942
elapsed time: 0:46:06.568501
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 00:28:55.560854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.24
 ---- batch: 020 ----
mean loss: 187.49
 ---- batch: 030 ----
mean loss: 194.65
 ---- batch: 040 ----
mean loss: 191.60
 ---- batch: 050 ----
mean loss: 193.78
 ---- batch: 060 ----
mean loss: 201.69
 ---- batch: 070 ----
mean loss: 199.00
 ---- batch: 080 ----
mean loss: 192.96
 ---- batch: 090 ----
mean loss: 193.99
train mean loss: 194.12
epoch train time: 0:00:17.024208
elapsed time: 0:46:23.594053
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 00:29:12.586404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.08
 ---- batch: 020 ----
mean loss: 201.72
 ---- batch: 030 ----
mean loss: 198.73
 ---- batch: 040 ----
mean loss: 202.08
 ---- batch: 050 ----
mean loss: 188.17
 ---- batch: 060 ----
mean loss: 194.52
 ---- batch: 070 ----
mean loss: 194.20
 ---- batch: 080 ----
mean loss: 196.13
 ---- batch: 090 ----
mean loss: 187.89
train mean loss: 194.56
epoch train time: 0:00:17.010303
elapsed time: 0:46:40.605598
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 00:29:29.597950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.27
 ---- batch: 020 ----
mean loss: 182.66
 ---- batch: 030 ----
mean loss: 190.78
 ---- batch: 040 ----
mean loss: 192.54
 ---- batch: 050 ----
mean loss: 193.04
 ---- batch: 060 ----
mean loss: 193.82
 ---- batch: 070 ----
mean loss: 193.00
 ---- batch: 080 ----
mean loss: 186.78
 ---- batch: 090 ----
mean loss: 192.55
train mean loss: 191.53
epoch train time: 0:00:17.082101
elapsed time: 0:46:57.688932
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 00:29:46.681322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.41
 ---- batch: 020 ----
mean loss: 187.93
 ---- batch: 030 ----
mean loss: 190.09
 ---- batch: 040 ----
mean loss: 187.17
 ---- batch: 050 ----
mean loss: 196.59
 ---- batch: 060 ----
mean loss: 193.55
 ---- batch: 070 ----
mean loss: 195.12
 ---- batch: 080 ----
mean loss: 190.89
 ---- batch: 090 ----
mean loss: 199.20
train mean loss: 192.30
epoch train time: 0:00:16.994545
elapsed time: 0:47:14.684745
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 00:30:03.677065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.98
 ---- batch: 020 ----
mean loss: 192.70
 ---- batch: 030 ----
mean loss: 186.56
 ---- batch: 040 ----
mean loss: 202.02
 ---- batch: 050 ----
mean loss: 192.07
 ---- batch: 060 ----
mean loss: 202.82
 ---- batch: 070 ----
mean loss: 190.93
 ---- batch: 080 ----
mean loss: 190.46
 ---- batch: 090 ----
mean loss: 195.32
train mean loss: 193.32
epoch train time: 0:00:16.982674
elapsed time: 0:47:31.668605
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 00:30:20.660940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.39
 ---- batch: 020 ----
mean loss: 191.63
 ---- batch: 030 ----
mean loss: 195.07
 ---- batch: 040 ----
mean loss: 192.25
 ---- batch: 050 ----
mean loss: 189.99
 ---- batch: 060 ----
mean loss: 190.04
 ---- batch: 070 ----
mean loss: 181.07
 ---- batch: 080 ----
mean loss: 196.61
 ---- batch: 090 ----
mean loss: 200.40
train mean loss: 192.61
epoch train time: 0:00:17.012961
elapsed time: 0:47:48.682862
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 00:30:37.675229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.87
 ---- batch: 020 ----
mean loss: 190.14
 ---- batch: 030 ----
mean loss: 195.36
 ---- batch: 040 ----
mean loss: 185.48
 ---- batch: 050 ----
mean loss: 192.05
 ---- batch: 060 ----
mean loss: 188.98
 ---- batch: 070 ----
mean loss: 194.53
 ---- batch: 080 ----
mean loss: 194.64
 ---- batch: 090 ----
mean loss: 189.97
train mean loss: 191.48
epoch train time: 0:00:16.999170
elapsed time: 0:48:05.683257
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 00:30:54.675590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.79
 ---- batch: 020 ----
mean loss: 195.25
 ---- batch: 030 ----
mean loss: 187.23
 ---- batch: 040 ----
mean loss: 195.96
 ---- batch: 050 ----
mean loss: 198.91
 ---- batch: 060 ----
mean loss: 203.03
 ---- batch: 070 ----
mean loss: 186.84
 ---- batch: 080 ----
mean loss: 194.40
 ---- batch: 090 ----
mean loss: 191.68
train mean loss: 194.40
epoch train time: 0:00:17.036024
elapsed time: 0:48:22.720541
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 00:31:11.712896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.90
 ---- batch: 020 ----
mean loss: 188.89
 ---- batch: 030 ----
mean loss: 187.72
 ---- batch: 040 ----
mean loss: 190.18
 ---- batch: 050 ----
mean loss: 191.53
 ---- batch: 060 ----
mean loss: 195.03
 ---- batch: 070 ----
mean loss: 193.96
 ---- batch: 080 ----
mean loss: 194.17
 ---- batch: 090 ----
mean loss: 188.03
train mean loss: 191.40
epoch train time: 0:00:17.026217
elapsed time: 0:48:39.747979
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 00:31:28.740360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.86
 ---- batch: 020 ----
mean loss: 191.70
 ---- batch: 030 ----
mean loss: 189.99
 ---- batch: 040 ----
mean loss: 192.34
 ---- batch: 050 ----
mean loss: 201.03
 ---- batch: 060 ----
mean loss: 189.31
 ---- batch: 070 ----
mean loss: 191.36
 ---- batch: 080 ----
mean loss: 183.51
 ---- batch: 090 ----
mean loss: 196.08
train mean loss: 192.80
epoch train time: 0:00:17.009085
elapsed time: 0:48:56.758393
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 00:31:45.750711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.72
 ---- batch: 020 ----
mean loss: 190.17
 ---- batch: 030 ----
mean loss: 187.78
 ---- batch: 040 ----
mean loss: 192.60
 ---- batch: 050 ----
mean loss: 189.28
 ---- batch: 060 ----
mean loss: 193.50
 ---- batch: 070 ----
mean loss: 196.91
 ---- batch: 080 ----
mean loss: 188.19
 ---- batch: 090 ----
mean loss: 197.09
train mean loss: 191.51
epoch train time: 0:00:17.019710
elapsed time: 0:49:13.779359
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 00:32:02.771691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.67
 ---- batch: 020 ----
mean loss: 189.27
 ---- batch: 030 ----
mean loss: 191.03
 ---- batch: 040 ----
mean loss: 194.80
 ---- batch: 050 ----
mean loss: 187.87
 ---- batch: 060 ----
mean loss: 189.76
 ---- batch: 070 ----
mean loss: 195.03
 ---- batch: 080 ----
mean loss: 189.95
 ---- batch: 090 ----
mean loss: 186.88
train mean loss: 191.67
epoch train time: 0:00:17.063396
elapsed time: 0:49:30.844009
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 00:32:19.836487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.49
 ---- batch: 020 ----
mean loss: 185.51
 ---- batch: 030 ----
mean loss: 194.35
 ---- batch: 040 ----
mean loss: 184.08
 ---- batch: 050 ----
mean loss: 186.47
 ---- batch: 060 ----
mean loss: 195.58
 ---- batch: 070 ----
mean loss: 193.30
 ---- batch: 080 ----
mean loss: 201.61
 ---- batch: 090 ----
mean loss: 187.88
train mean loss: 190.47
epoch train time: 0:00:16.982226
elapsed time: 0:49:47.827907
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 00:32:36.819877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.12
 ---- batch: 020 ----
mean loss: 198.89
 ---- batch: 030 ----
mean loss: 198.81
 ---- batch: 040 ----
mean loss: 182.82
 ---- batch: 050 ----
mean loss: 188.53
 ---- batch: 060 ----
mean loss: 197.65
 ---- batch: 070 ----
mean loss: 189.87
 ---- batch: 080 ----
mean loss: 191.40
 ---- batch: 090 ----
mean loss: 189.56
train mean loss: 191.71
epoch train time: 0:00:16.972743
elapsed time: 0:50:04.801590
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 00:32:53.793946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.41
 ---- batch: 020 ----
mean loss: 185.83
 ---- batch: 030 ----
mean loss: 188.76
 ---- batch: 040 ----
mean loss: 191.60
 ---- batch: 050 ----
mean loss: 191.01
 ---- batch: 060 ----
mean loss: 193.15
 ---- batch: 070 ----
mean loss: 194.38
 ---- batch: 080 ----
mean loss: 184.44
 ---- batch: 090 ----
mean loss: 191.25
train mean loss: 190.56
epoch train time: 0:00:16.975562
elapsed time: 0:50:21.778440
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 00:33:10.770769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.08
 ---- batch: 020 ----
mean loss: 191.50
 ---- batch: 030 ----
mean loss: 188.49
 ---- batch: 040 ----
mean loss: 191.40
 ---- batch: 050 ----
mean loss: 194.32
 ---- batch: 060 ----
mean loss: 190.31
 ---- batch: 070 ----
mean loss: 183.57
 ---- batch: 080 ----
mean loss: 183.89
 ---- batch: 090 ----
mean loss: 195.61
train mean loss: 189.48
epoch train time: 0:00:16.940027
elapsed time: 0:50:38.719694
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 00:33:27.712066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.49
 ---- batch: 020 ----
mean loss: 191.50
 ---- batch: 030 ----
mean loss: 191.99
 ---- batch: 040 ----
mean loss: 185.71
 ---- batch: 050 ----
mean loss: 185.44
 ---- batch: 060 ----
mean loss: 191.07
 ---- batch: 070 ----
mean loss: 194.13
 ---- batch: 080 ----
mean loss: 185.16
 ---- batch: 090 ----
mean loss: 186.01
train mean loss: 189.24
epoch train time: 0:00:16.902691
elapsed time: 0:50:55.623671
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 00:33:44.616007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.40
 ---- batch: 020 ----
mean loss: 190.18
 ---- batch: 030 ----
mean loss: 193.10
 ---- batch: 040 ----
mean loss: 189.25
 ---- batch: 050 ----
mean loss: 187.69
 ---- batch: 060 ----
mean loss: 191.49
 ---- batch: 070 ----
mean loss: 189.72
 ---- batch: 080 ----
mean loss: 179.64
 ---- batch: 090 ----
mean loss: 190.70
train mean loss: 189.27
epoch train time: 0:00:16.994791
elapsed time: 0:51:12.619692
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 00:34:01.612020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.18
 ---- batch: 020 ----
mean loss: 189.37
 ---- batch: 030 ----
mean loss: 191.20
 ---- batch: 040 ----
mean loss: 185.23
 ---- batch: 050 ----
mean loss: 193.75
 ---- batch: 060 ----
mean loss: 190.84
 ---- batch: 070 ----
mean loss: 185.96
 ---- batch: 080 ----
mean loss: 190.96
 ---- batch: 090 ----
mean loss: 191.68
train mean loss: 190.01
epoch train time: 0:00:16.916436
elapsed time: 0:51:29.537343
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 00:34:18.529698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.23
 ---- batch: 020 ----
mean loss: 190.19
 ---- batch: 030 ----
mean loss: 190.32
 ---- batch: 040 ----
mean loss: 192.01
 ---- batch: 050 ----
mean loss: 190.20
 ---- batch: 060 ----
mean loss: 182.14
 ---- batch: 070 ----
mean loss: 185.28
 ---- batch: 080 ----
mean loss: 187.62
 ---- batch: 090 ----
mean loss: 187.58
train mean loss: 189.05
epoch train time: 0:00:16.964824
elapsed time: 0:51:46.503433
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 00:34:35.495746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.85
 ---- batch: 020 ----
mean loss: 192.36
 ---- batch: 030 ----
mean loss: 187.68
 ---- batch: 040 ----
mean loss: 189.22
 ---- batch: 050 ----
mean loss: 185.58
 ---- batch: 060 ----
mean loss: 192.25
 ---- batch: 070 ----
mean loss: 189.57
 ---- batch: 080 ----
mean loss: 189.41
 ---- batch: 090 ----
mean loss: 186.58
train mean loss: 189.14
epoch train time: 0:00:17.043413
elapsed time: 0:52:03.548063
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 00:34:52.540409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.96
 ---- batch: 020 ----
mean loss: 187.73
 ---- batch: 030 ----
mean loss: 195.05
 ---- batch: 040 ----
mean loss: 191.87
 ---- batch: 050 ----
mean loss: 185.19
 ---- batch: 060 ----
mean loss: 194.81
 ---- batch: 070 ----
mean loss: 186.65
 ---- batch: 080 ----
mean loss: 191.29
 ---- batch: 090 ----
mean loss: 183.41
train mean loss: 189.81
epoch train time: 0:00:17.036125
elapsed time: 0:52:20.585430
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 00:35:09.577782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.55
 ---- batch: 020 ----
mean loss: 192.49
 ---- batch: 030 ----
mean loss: 187.51
 ---- batch: 040 ----
mean loss: 194.53
 ---- batch: 050 ----
mean loss: 191.88
 ---- batch: 060 ----
mean loss: 188.41
 ---- batch: 070 ----
mean loss: 187.91
 ---- batch: 080 ----
mean loss: 192.22
 ---- batch: 090 ----
mean loss: 187.08
train mean loss: 190.41
epoch train time: 0:00:17.044065
elapsed time: 0:52:37.630722
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 00:35:26.623078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.04
 ---- batch: 020 ----
mean loss: 190.94
 ---- batch: 030 ----
mean loss: 186.78
 ---- batch: 040 ----
mean loss: 192.42
 ---- batch: 050 ----
mean loss: 188.99
 ---- batch: 060 ----
mean loss: 192.55
 ---- batch: 070 ----
mean loss: 187.61
 ---- batch: 080 ----
mean loss: 185.61
 ---- batch: 090 ----
mean loss: 191.04
train mean loss: 189.74
epoch train time: 0:00:17.005094
elapsed time: 0:52:54.637039
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 00:35:43.629353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.60
 ---- batch: 020 ----
mean loss: 188.83
 ---- batch: 030 ----
mean loss: 185.76
 ---- batch: 040 ----
mean loss: 196.17
 ---- batch: 050 ----
mean loss: 197.96
 ---- batch: 060 ----
mean loss: 191.24
 ---- batch: 070 ----
mean loss: 191.71
 ---- batch: 080 ----
mean loss: 187.59
 ---- batch: 090 ----
mean loss: 191.04
train mean loss: 191.43
epoch train time: 0:00:17.022465
elapsed time: 0:53:11.660682
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 00:36:00.652996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.24
 ---- batch: 020 ----
mean loss: 182.98
 ---- batch: 030 ----
mean loss: 192.66
 ---- batch: 040 ----
mean loss: 186.73
 ---- batch: 050 ----
mean loss: 185.00
 ---- batch: 060 ----
mean loss: 191.35
 ---- batch: 070 ----
mean loss: 195.37
 ---- batch: 080 ----
mean loss: 187.43
 ---- batch: 090 ----
mean loss: 196.25
train mean loss: 189.39
epoch train time: 0:00:16.986997
elapsed time: 0:53:28.648990
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 00:36:17.641303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.23
 ---- batch: 020 ----
mean loss: 195.79
 ---- batch: 030 ----
mean loss: 187.45
 ---- batch: 040 ----
mean loss: 189.16
 ---- batch: 050 ----
mean loss: 189.77
 ---- batch: 060 ----
mean loss: 188.90
 ---- batch: 070 ----
mean loss: 190.61
 ---- batch: 080 ----
mean loss: 189.77
 ---- batch: 090 ----
mean loss: 189.30
train mean loss: 189.81
epoch train time: 0:00:16.983860
elapsed time: 0:53:45.634043
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 00:36:34.626385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.49
 ---- batch: 020 ----
mean loss: 195.11
 ---- batch: 030 ----
mean loss: 184.00
 ---- batch: 040 ----
mean loss: 183.60
 ---- batch: 050 ----
mean loss: 188.55
 ---- batch: 060 ----
mean loss: 190.29
 ---- batch: 070 ----
mean loss: 190.47
 ---- batch: 080 ----
mean loss: 190.94
 ---- batch: 090 ----
mean loss: 190.25
train mean loss: 189.22
epoch train time: 0:00:16.977341
elapsed time: 0:54:02.612729
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 00:36:51.605065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.78
 ---- batch: 020 ----
mean loss: 189.73
 ---- batch: 030 ----
mean loss: 207.60
 ---- batch: 040 ----
mean loss: 192.46
 ---- batch: 050 ----
mean loss: 192.50
 ---- batch: 060 ----
mean loss: 178.80
 ---- batch: 070 ----
mean loss: 185.05
 ---- batch: 080 ----
mean loss: 198.68
 ---- batch: 090 ----
mean loss: 196.86
train mean loss: 191.86
epoch train time: 0:00:16.967464
elapsed time: 0:54:19.581538
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 00:37:08.573984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.71
 ---- batch: 020 ----
mean loss: 185.44
 ---- batch: 030 ----
mean loss: 192.75
 ---- batch: 040 ----
mean loss: 202.15
 ---- batch: 050 ----
mean loss: 199.66
 ---- batch: 060 ----
mean loss: 189.35
 ---- batch: 070 ----
mean loss: 189.96
 ---- batch: 080 ----
mean loss: 191.94
 ---- batch: 090 ----
mean loss: 181.03
train mean loss: 190.79
epoch train time: 0:00:17.008151
elapsed time: 0:54:36.591000
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 00:37:25.583334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.01
 ---- batch: 020 ----
mean loss: 190.07
 ---- batch: 030 ----
mean loss: 190.63
 ---- batch: 040 ----
mean loss: 183.19
 ---- batch: 050 ----
mean loss: 187.39
 ---- batch: 060 ----
mean loss: 195.66
 ---- batch: 070 ----
mean loss: 188.44
 ---- batch: 080 ----
mean loss: 184.22
 ---- batch: 090 ----
mean loss: 190.18
train mean loss: 188.65
epoch train time: 0:00:17.025195
elapsed time: 0:54:53.617391
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 00:37:42.609743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.56
 ---- batch: 020 ----
mean loss: 192.19
 ---- batch: 030 ----
mean loss: 186.18
 ---- batch: 040 ----
mean loss: 186.48
 ---- batch: 050 ----
mean loss: 188.31
 ---- batch: 060 ----
mean loss: 192.84
 ---- batch: 070 ----
mean loss: 181.28
 ---- batch: 080 ----
mean loss: 182.33
 ---- batch: 090 ----
mean loss: 188.44
train mean loss: 187.37
epoch train time: 0:00:17.033998
elapsed time: 0:55:10.652615
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 00:37:59.644934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.14
 ---- batch: 020 ----
mean loss: 183.24
 ---- batch: 030 ----
mean loss: 187.57
 ---- batch: 040 ----
mean loss: 187.86
 ---- batch: 050 ----
mean loss: 183.71
 ---- batch: 060 ----
mean loss: 179.01
 ---- batch: 070 ----
mean loss: 194.69
 ---- batch: 080 ----
mean loss: 192.13
 ---- batch: 090 ----
mean loss: 192.28
train mean loss: 187.73
epoch train time: 0:00:17.006172
elapsed time: 0:55:27.659990
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 00:38:16.652339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.95
 ---- batch: 020 ----
mean loss: 189.43
 ---- batch: 030 ----
mean loss: 188.82
 ---- batch: 040 ----
mean loss: 189.26
 ---- batch: 050 ----
mean loss: 182.28
 ---- batch: 060 ----
mean loss: 190.52
 ---- batch: 070 ----
mean loss: 187.97
 ---- batch: 080 ----
mean loss: 189.75
 ---- batch: 090 ----
mean loss: 184.37
train mean loss: 188.08
epoch train time: 0:00:17.011357
elapsed time: 0:55:44.672604
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 00:38:33.664933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.90
 ---- batch: 020 ----
mean loss: 191.22
 ---- batch: 030 ----
mean loss: 192.31
 ---- batch: 040 ----
mean loss: 184.29
 ---- batch: 050 ----
mean loss: 185.62
 ---- batch: 060 ----
mean loss: 188.92
 ---- batch: 070 ----
mean loss: 189.22
 ---- batch: 080 ----
mean loss: 186.85
 ---- batch: 090 ----
mean loss: 184.86
train mean loss: 187.20
epoch train time: 0:00:17.002173
elapsed time: 0:56:01.676178
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 00:38:50.668502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.32
 ---- batch: 020 ----
mean loss: 192.37
 ---- batch: 030 ----
mean loss: 187.40
 ---- batch: 040 ----
mean loss: 185.89
 ---- batch: 050 ----
mean loss: 187.79
 ---- batch: 060 ----
mean loss: 186.76
 ---- batch: 070 ----
mean loss: 186.99
 ---- batch: 080 ----
mean loss: 180.19
 ---- batch: 090 ----
mean loss: 191.40
train mean loss: 187.65
epoch train time: 0:00:17.019055
elapsed time: 0:56:18.696636
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 00:39:07.688991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.05
 ---- batch: 020 ----
mean loss: 187.99
 ---- batch: 030 ----
mean loss: 187.34
 ---- batch: 040 ----
mean loss: 184.74
 ---- batch: 050 ----
mean loss: 186.64
 ---- batch: 060 ----
mean loss: 194.53
 ---- batch: 070 ----
mean loss: 184.43
 ---- batch: 080 ----
mean loss: 184.57
 ---- batch: 090 ----
mean loss: 193.66
train mean loss: 187.49
epoch train time: 0:00:17.010682
elapsed time: 0:56:35.708566
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 00:39:24.700968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.80
 ---- batch: 020 ----
mean loss: 183.07
 ---- batch: 030 ----
mean loss: 186.42
 ---- batch: 040 ----
mean loss: 193.67
 ---- batch: 050 ----
mean loss: 194.60
 ---- batch: 060 ----
mean loss: 187.39
 ---- batch: 070 ----
mean loss: 188.50
 ---- batch: 080 ----
mean loss: 193.16
 ---- batch: 090 ----
mean loss: 183.21
train mean loss: 187.92
epoch train time: 0:00:16.937147
elapsed time: 0:56:52.646964
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 00:39:41.639289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.62
 ---- batch: 020 ----
mean loss: 180.48
 ---- batch: 030 ----
mean loss: 189.13
 ---- batch: 040 ----
mean loss: 184.25
 ---- batch: 050 ----
mean loss: 186.93
 ---- batch: 060 ----
mean loss: 189.53
 ---- batch: 070 ----
mean loss: 185.77
 ---- batch: 080 ----
mean loss: 188.97
 ---- batch: 090 ----
mean loss: 186.65
train mean loss: 186.26
epoch train time: 0:00:16.967636
elapsed time: 0:57:09.615793
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 00:39:58.608145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.02
 ---- batch: 020 ----
mean loss: 186.96
 ---- batch: 030 ----
mean loss: 178.57
 ---- batch: 040 ----
mean loss: 186.70
 ---- batch: 050 ----
mean loss: 190.28
 ---- batch: 060 ----
mean loss: 186.30
 ---- batch: 070 ----
mean loss: 192.08
 ---- batch: 080 ----
mean loss: 191.03
 ---- batch: 090 ----
mean loss: 192.58
train mean loss: 187.76
epoch train time: 0:00:16.972080
elapsed time: 0:57:26.589089
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 00:40:15.581535
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.13
 ---- batch: 020 ----
mean loss: 183.31
 ---- batch: 030 ----
mean loss: 183.19
 ---- batch: 040 ----
mean loss: 191.59
 ---- batch: 050 ----
mean loss: 183.99
 ---- batch: 060 ----
mean loss: 177.45
 ---- batch: 070 ----
mean loss: 184.12
 ---- batch: 080 ----
mean loss: 186.19
 ---- batch: 090 ----
mean loss: 187.22
train mean loss: 185.74
epoch train time: 0:00:16.981043
elapsed time: 0:57:43.572106
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 00:40:32.564032
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.05
 ---- batch: 020 ----
mean loss: 179.47
 ---- batch: 030 ----
mean loss: 185.45
 ---- batch: 040 ----
mean loss: 178.18
 ---- batch: 050 ----
mean loss: 186.20
 ---- batch: 060 ----
mean loss: 185.30
 ---- batch: 070 ----
mean loss: 181.25
 ---- batch: 080 ----
mean loss: 185.50
 ---- batch: 090 ----
mean loss: 180.66
train mean loss: 183.76
epoch train time: 0:00:17.005900
elapsed time: 0:58:00.578907
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 00:40:49.571279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.81
 ---- batch: 020 ----
mean loss: 186.63
 ---- batch: 030 ----
mean loss: 180.54
 ---- batch: 040 ----
mean loss: 186.48
 ---- batch: 050 ----
mean loss: 187.31
 ---- batch: 060 ----
mean loss: 175.46
 ---- batch: 070 ----
mean loss: 187.08
 ---- batch: 080 ----
mean loss: 182.63
 ---- batch: 090 ----
mean loss: 183.17
train mean loss: 183.49
epoch train time: 0:00:16.993702
elapsed time: 0:58:17.574003
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 00:41:06.566356
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.02
 ---- batch: 020 ----
mean loss: 182.50
 ---- batch: 030 ----
mean loss: 189.02
 ---- batch: 040 ----
mean loss: 180.87
 ---- batch: 050 ----
mean loss: 180.04
 ---- batch: 060 ----
mean loss: 182.00
 ---- batch: 070 ----
mean loss: 181.91
 ---- batch: 080 ----
mean loss: 195.08
 ---- batch: 090 ----
mean loss: 179.65
train mean loss: 183.09
epoch train time: 0:00:17.012485
elapsed time: 0:58:34.588006
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 00:41:23.580372
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.77
 ---- batch: 020 ----
mean loss: 185.25
 ---- batch: 030 ----
mean loss: 192.45
 ---- batch: 040 ----
mean loss: 173.02
 ---- batch: 050 ----
mean loss: 184.81
 ---- batch: 060 ----
mean loss: 186.23
 ---- batch: 070 ----
mean loss: 179.02
 ---- batch: 080 ----
mean loss: 192.63
 ---- batch: 090 ----
mean loss: 179.72
train mean loss: 183.51
epoch train time: 0:00:17.029765
elapsed time: 0:58:51.618970
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 00:41:40.611368
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.55
 ---- batch: 020 ----
mean loss: 179.23
 ---- batch: 030 ----
mean loss: 180.54
 ---- batch: 040 ----
mean loss: 189.05
 ---- batch: 050 ----
mean loss: 181.30
 ---- batch: 060 ----
mean loss: 183.80
 ---- batch: 070 ----
mean loss: 190.54
 ---- batch: 080 ----
mean loss: 183.02
 ---- batch: 090 ----
mean loss: 185.19
train mean loss: 183.65
epoch train time: 0:00:17.008563
elapsed time: 0:59:08.628820
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 00:41:57.621148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.43
 ---- batch: 020 ----
mean loss: 176.71
 ---- batch: 030 ----
mean loss: 185.72
 ---- batch: 040 ----
mean loss: 186.99
 ---- batch: 050 ----
mean loss: 185.67
 ---- batch: 060 ----
mean loss: 180.98
 ---- batch: 070 ----
mean loss: 183.78
 ---- batch: 080 ----
mean loss: 178.94
 ---- batch: 090 ----
mean loss: 184.55
train mean loss: 183.62
epoch train time: 0:00:17.019825
elapsed time: 0:59:25.649952
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 00:42:14.642307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.69
 ---- batch: 020 ----
mean loss: 184.66
 ---- batch: 030 ----
mean loss: 185.14
 ---- batch: 040 ----
mean loss: 192.89
 ---- batch: 050 ----
mean loss: 185.92
 ---- batch: 060 ----
mean loss: 179.53
 ---- batch: 070 ----
mean loss: 180.88
 ---- batch: 080 ----
mean loss: 185.66
 ---- batch: 090 ----
mean loss: 178.12
train mean loss: 183.63
epoch train time: 0:00:17.060504
elapsed time: 0:59:42.711711
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 00:42:31.704058
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.62
 ---- batch: 020 ----
mean loss: 189.37
 ---- batch: 030 ----
mean loss: 185.91
 ---- batch: 040 ----
mean loss: 174.06
 ---- batch: 050 ----
mean loss: 188.95
 ---- batch: 060 ----
mean loss: 189.84
 ---- batch: 070 ----
mean loss: 180.62
 ---- batch: 080 ----
mean loss: 180.32
 ---- batch: 090 ----
mean loss: 179.84
train mean loss: 183.44
epoch train time: 0:00:17.019001
elapsed time: 0:59:59.732061
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 00:42:48.724501
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.50
 ---- batch: 020 ----
mean loss: 185.80
 ---- batch: 030 ----
mean loss: 184.64
 ---- batch: 040 ----
mean loss: 182.68
 ---- batch: 050 ----
mean loss: 177.80
 ---- batch: 060 ----
mean loss: 189.77
 ---- batch: 070 ----
mean loss: 181.97
 ---- batch: 080 ----
mean loss: 182.85
 ---- batch: 090 ----
mean loss: 180.36
train mean loss: 183.17
epoch train time: 0:00:17.025700
elapsed time: 1:00:16.759055
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 00:43:05.751378
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.40
 ---- batch: 020 ----
mean loss: 176.92
 ---- batch: 030 ----
mean loss: 186.52
 ---- batch: 040 ----
mean loss: 182.72
 ---- batch: 050 ----
mean loss: 180.12
 ---- batch: 060 ----
mean loss: 181.49
 ---- batch: 070 ----
mean loss: 187.55
 ---- batch: 080 ----
mean loss: 184.53
 ---- batch: 090 ----
mean loss: 185.96
train mean loss: 183.27
epoch train time: 0:00:17.045104
elapsed time: 1:00:33.805361
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 00:43:22.797762
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.31
 ---- batch: 020 ----
mean loss: 182.07
 ---- batch: 030 ----
mean loss: 181.15
 ---- batch: 040 ----
mean loss: 189.64
 ---- batch: 050 ----
mean loss: 182.48
 ---- batch: 060 ----
mean loss: 176.22
 ---- batch: 070 ----
mean loss: 184.94
 ---- batch: 080 ----
mean loss: 186.23
 ---- batch: 090 ----
mean loss: 182.24
train mean loss: 183.25
epoch train time: 0:00:17.012813
elapsed time: 1:00:50.819548
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 00:43:39.811970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.22
 ---- batch: 020 ----
mean loss: 186.52
 ---- batch: 030 ----
mean loss: 179.41
 ---- batch: 040 ----
mean loss: 187.23
 ---- batch: 050 ----
mean loss: 184.18
 ---- batch: 060 ----
mean loss: 187.02
 ---- batch: 070 ----
mean loss: 179.74
 ---- batch: 080 ----
mean loss: 182.39
 ---- batch: 090 ----
mean loss: 181.95
train mean loss: 183.07
epoch train time: 0:00:17.039515
elapsed time: 1:01:07.860470
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 00:43:56.852798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.27
 ---- batch: 020 ----
mean loss: 171.30
 ---- batch: 030 ----
mean loss: 180.57
 ---- batch: 040 ----
mean loss: 183.14
 ---- batch: 050 ----
mean loss: 183.63
 ---- batch: 060 ----
mean loss: 186.09
 ---- batch: 070 ----
mean loss: 190.56
 ---- batch: 080 ----
mean loss: 191.41
 ---- batch: 090 ----
mean loss: 184.23
train mean loss: 183.56
epoch train time: 0:00:17.010299
elapsed time: 1:01:24.872043
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 00:44:13.864385
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.25
 ---- batch: 020 ----
mean loss: 186.87
 ---- batch: 030 ----
mean loss: 181.39
 ---- batch: 040 ----
mean loss: 181.10
 ---- batch: 050 ----
mean loss: 182.14
 ---- batch: 060 ----
mean loss: 184.21
 ---- batch: 070 ----
mean loss: 182.15
 ---- batch: 080 ----
mean loss: 186.54
 ---- batch: 090 ----
mean loss: 180.55
train mean loss: 183.75
epoch train time: 0:00:17.001571
elapsed time: 1:01:41.875049
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 00:44:30.867503
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.33
 ---- batch: 020 ----
mean loss: 179.58
 ---- batch: 030 ----
mean loss: 188.85
 ---- batch: 040 ----
mean loss: 183.38
 ---- batch: 050 ----
mean loss: 183.22
 ---- batch: 060 ----
mean loss: 185.07
 ---- batch: 070 ----
mean loss: 181.03
 ---- batch: 080 ----
mean loss: 186.46
 ---- batch: 090 ----
mean loss: 180.33
train mean loss: 183.48
epoch train time: 0:00:16.946393
elapsed time: 1:01:58.822772
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 00:44:47.815210
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.09
 ---- batch: 020 ----
mean loss: 181.39
 ---- batch: 030 ----
mean loss: 185.13
 ---- batch: 040 ----
mean loss: 183.66
 ---- batch: 050 ----
mean loss: 189.26
 ---- batch: 060 ----
mean loss: 180.06
 ---- batch: 070 ----
mean loss: 179.34
 ---- batch: 080 ----
mean loss: 186.72
 ---- batch: 090 ----
mean loss: 183.44
train mean loss: 183.47
epoch train time: 0:00:16.963686
elapsed time: 1:02:15.787790
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 00:45:04.780169
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.65
 ---- batch: 020 ----
mean loss: 183.95
 ---- batch: 030 ----
mean loss: 186.29
 ---- batch: 040 ----
mean loss: 184.05
 ---- batch: 050 ----
mean loss: 181.50
 ---- batch: 060 ----
mean loss: 184.91
 ---- batch: 070 ----
mean loss: 178.75
 ---- batch: 080 ----
mean loss: 186.31
 ---- batch: 090 ----
mean loss: 186.05
train mean loss: 183.24
epoch train time: 0:00:17.044815
elapsed time: 1:02:32.833967
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 00:45:21.826305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.63
 ---- batch: 020 ----
mean loss: 183.07
 ---- batch: 030 ----
mean loss: 185.87
 ---- batch: 040 ----
mean loss: 183.72
 ---- batch: 050 ----
mean loss: 187.81
 ---- batch: 060 ----
mean loss: 178.63
 ---- batch: 070 ----
mean loss: 177.85
 ---- batch: 080 ----
mean loss: 189.15
 ---- batch: 090 ----
mean loss: 179.39
train mean loss: 182.86
epoch train time: 0:00:17.003568
elapsed time: 1:02:49.838778
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 00:45:38.831128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.31
 ---- batch: 020 ----
mean loss: 178.37
 ---- batch: 030 ----
mean loss: 182.26
 ---- batch: 040 ----
mean loss: 185.76
 ---- batch: 050 ----
mean loss: 185.87
 ---- batch: 060 ----
mean loss: 180.77
 ---- batch: 070 ----
mean loss: 182.16
 ---- batch: 080 ----
mean loss: 185.10
 ---- batch: 090 ----
mean loss: 183.16
train mean loss: 183.04
epoch train time: 0:00:16.988752
elapsed time: 1:03:06.828826
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 00:45:55.821145
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.56
 ---- batch: 020 ----
mean loss: 182.62
 ---- batch: 030 ----
mean loss: 180.19
 ---- batch: 040 ----
mean loss: 183.84
 ---- batch: 050 ----
mean loss: 182.03
 ---- batch: 060 ----
mean loss: 191.26
 ---- batch: 070 ----
mean loss: 182.17
 ---- batch: 080 ----
mean loss: 188.34
 ---- batch: 090 ----
mean loss: 177.21
train mean loss: 183.17
epoch train time: 0:00:16.930423
elapsed time: 1:03:23.760435
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 00:46:12.752868
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.73
 ---- batch: 020 ----
mean loss: 188.84
 ---- batch: 030 ----
mean loss: 182.22
 ---- batch: 040 ----
mean loss: 179.12
 ---- batch: 050 ----
mean loss: 178.61
 ---- batch: 060 ----
mean loss: 185.43
 ---- batch: 070 ----
mean loss: 181.41
 ---- batch: 080 ----
mean loss: 186.89
 ---- batch: 090 ----
mean loss: 185.23
train mean loss: 183.09
epoch train time: 0:00:16.944134
elapsed time: 1:03:40.705993
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 00:46:29.698300
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.47
 ---- batch: 020 ----
mean loss: 182.63
 ---- batch: 030 ----
mean loss: 182.02
 ---- batch: 040 ----
mean loss: 180.27
 ---- batch: 050 ----
mean loss: 184.91
 ---- batch: 060 ----
mean loss: 185.63
 ---- batch: 070 ----
mean loss: 183.02
 ---- batch: 080 ----
mean loss: 177.13
 ---- batch: 090 ----
mean loss: 188.96
train mean loss: 183.09
epoch train time: 0:00:16.914894
elapsed time: 1:03:57.622302
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 00:46:46.614764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.89
 ---- batch: 020 ----
mean loss: 179.91
 ---- batch: 030 ----
mean loss: 179.58
 ---- batch: 040 ----
mean loss: 181.33
 ---- batch: 050 ----
mean loss: 180.84
 ---- batch: 060 ----
mean loss: 178.99
 ---- batch: 070 ----
mean loss: 184.54
 ---- batch: 080 ----
mean loss: 196.81
 ---- batch: 090 ----
mean loss: 186.45
train mean loss: 183.39
epoch train time: 0:00:16.948215
elapsed time: 1:04:14.571963
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 00:47:03.564190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.31
 ---- batch: 020 ----
mean loss: 185.72
 ---- batch: 030 ----
mean loss: 180.68
 ---- batch: 040 ----
mean loss: 186.65
 ---- batch: 050 ----
mean loss: 181.82
 ---- batch: 060 ----
mean loss: 182.22
 ---- batch: 070 ----
mean loss: 178.16
 ---- batch: 080 ----
mean loss: 179.90
 ---- batch: 090 ----
mean loss: 184.40
train mean loss: 182.94
epoch train time: 0:00:16.971057
elapsed time: 1:04:31.544090
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 00:47:20.536408
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.85
 ---- batch: 020 ----
mean loss: 188.57
 ---- batch: 030 ----
mean loss: 184.20
 ---- batch: 040 ----
mean loss: 178.29
 ---- batch: 050 ----
mean loss: 178.72
 ---- batch: 060 ----
mean loss: 183.63
 ---- batch: 070 ----
mean loss: 176.73
 ---- batch: 080 ----
mean loss: 186.38
 ---- batch: 090 ----
mean loss: 189.22
train mean loss: 183.00
epoch train time: 0:00:17.020480
elapsed time: 1:04:48.565768
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 00:47:37.558051
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.06
 ---- batch: 020 ----
mean loss: 184.36
 ---- batch: 030 ----
mean loss: 180.98
 ---- batch: 040 ----
mean loss: 188.43
 ---- batch: 050 ----
mean loss: 190.09
 ---- batch: 060 ----
mean loss: 180.21
 ---- batch: 070 ----
mean loss: 182.85
 ---- batch: 080 ----
mean loss: 181.64
 ---- batch: 090 ----
mean loss: 180.61
train mean loss: 182.96
epoch train time: 0:00:16.977290
elapsed time: 1:05:05.544204
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 00:47:54.536564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.06
 ---- batch: 020 ----
mean loss: 175.63
 ---- batch: 030 ----
mean loss: 185.77
 ---- batch: 040 ----
mean loss: 185.89
 ---- batch: 050 ----
mean loss: 179.41
 ---- batch: 060 ----
mean loss: 183.88
 ---- batch: 070 ----
mean loss: 186.11
 ---- batch: 080 ----
mean loss: 178.30
 ---- batch: 090 ----
mean loss: 185.32
train mean loss: 182.91
epoch train time: 0:00:17.007129
elapsed time: 1:05:22.552603
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 00:48:11.544990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.00
 ---- batch: 020 ----
mean loss: 181.02
 ---- batch: 030 ----
mean loss: 183.43
 ---- batch: 040 ----
mean loss: 189.61
 ---- batch: 050 ----
mean loss: 186.98
 ---- batch: 060 ----
mean loss: 179.83
 ---- batch: 070 ----
mean loss: 183.40
 ---- batch: 080 ----
mean loss: 176.30
 ---- batch: 090 ----
mean loss: 185.70
train mean loss: 182.91
epoch train time: 0:00:16.990320
elapsed time: 1:05:39.544247
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 00:48:28.536599
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.81
 ---- batch: 020 ----
mean loss: 184.55
 ---- batch: 030 ----
mean loss: 184.88
 ---- batch: 040 ----
mean loss: 181.37
 ---- batch: 050 ----
mean loss: 180.33
 ---- batch: 060 ----
mean loss: 182.26
 ---- batch: 070 ----
mean loss: 183.41
 ---- batch: 080 ----
mean loss: 183.72
 ---- batch: 090 ----
mean loss: 181.27
train mean loss: 183.00
epoch train time: 0:00:16.973546
elapsed time: 1:05:56.519001
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 00:48:45.511321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.06
 ---- batch: 020 ----
mean loss: 182.91
 ---- batch: 030 ----
mean loss: 183.34
 ---- batch: 040 ----
mean loss: 183.75
 ---- batch: 050 ----
mean loss: 180.22
 ---- batch: 060 ----
mean loss: 176.86
 ---- batch: 070 ----
mean loss: 181.19
 ---- batch: 080 ----
mean loss: 186.14
 ---- batch: 090 ----
mean loss: 185.53
train mean loss: 182.86
epoch train time: 0:00:16.998727
elapsed time: 1:06:13.518914
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 00:49:02.511346
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.32
 ---- batch: 020 ----
mean loss: 180.34
 ---- batch: 030 ----
mean loss: 183.80
 ---- batch: 040 ----
mean loss: 178.68
 ---- batch: 050 ----
mean loss: 179.57
 ---- batch: 060 ----
mean loss: 186.49
 ---- batch: 070 ----
mean loss: 183.13
 ---- batch: 080 ----
mean loss: 180.85
 ---- batch: 090 ----
mean loss: 185.65
train mean loss: 182.51
epoch train time: 0:00:16.984870
elapsed time: 1:06:30.505154
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 00:49:19.497633
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.73
 ---- batch: 020 ----
mean loss: 181.34
 ---- batch: 030 ----
mean loss: 179.38
 ---- batch: 040 ----
mean loss: 178.53
 ---- batch: 050 ----
mean loss: 185.09
 ---- batch: 060 ----
mean loss: 182.76
 ---- batch: 070 ----
mean loss: 179.07
 ---- batch: 080 ----
mean loss: 185.43
 ---- batch: 090 ----
mean loss: 184.77
train mean loss: 183.26
epoch train time: 0:00:16.992953
elapsed time: 1:06:47.500222
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 00:49:36.492066
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.03
 ---- batch: 020 ----
mean loss: 186.11
 ---- batch: 030 ----
mean loss: 179.78
 ---- batch: 040 ----
mean loss: 185.44
 ---- batch: 050 ----
mean loss: 183.45
 ---- batch: 060 ----
mean loss: 183.66
 ---- batch: 070 ----
mean loss: 185.95
 ---- batch: 080 ----
mean loss: 182.47
 ---- batch: 090 ----
mean loss: 180.22
train mean loss: 182.65
epoch train time: 0:00:16.975455
elapsed time: 1:07:04.476424
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 00:49:53.468805
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.80
 ---- batch: 020 ----
mean loss: 182.83
 ---- batch: 030 ----
mean loss: 178.92
 ---- batch: 040 ----
mean loss: 178.97
 ---- batch: 050 ----
mean loss: 186.66
 ---- batch: 060 ----
mean loss: 184.59
 ---- batch: 070 ----
mean loss: 181.92
 ---- batch: 080 ----
mean loss: 185.65
 ---- batch: 090 ----
mean loss: 182.29
train mean loss: 182.79
epoch train time: 0:00:16.989725
elapsed time: 1:07:21.467507
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 00:50:10.459938
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.17
 ---- batch: 020 ----
mean loss: 183.48
 ---- batch: 030 ----
mean loss: 184.73
 ---- batch: 040 ----
mean loss: 186.64
 ---- batch: 050 ----
mean loss: 177.38
 ---- batch: 060 ----
mean loss: 177.83
 ---- batch: 070 ----
mean loss: 185.23
 ---- batch: 080 ----
mean loss: 185.06
 ---- batch: 090 ----
mean loss: 185.57
train mean loss: 182.83
epoch train time: 0:00:17.048442
elapsed time: 1:07:38.517239
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 00:50:27.509573
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.43
 ---- batch: 020 ----
mean loss: 179.02
 ---- batch: 030 ----
mean loss: 191.68
 ---- batch: 040 ----
mean loss: 181.45
 ---- batch: 050 ----
mean loss: 181.68
 ---- batch: 060 ----
mean loss: 176.21
 ---- batch: 070 ----
mean loss: 183.37
 ---- batch: 080 ----
mean loss: 188.53
 ---- batch: 090 ----
mean loss: 183.27
train mean loss: 183.06
epoch train time: 0:00:16.915657
elapsed time: 1:07:55.434242
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 00:50:44.426451
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.09
 ---- batch: 020 ----
mean loss: 182.94
 ---- batch: 030 ----
mean loss: 184.00
 ---- batch: 040 ----
mean loss: 180.89
 ---- batch: 050 ----
mean loss: 185.07
 ---- batch: 060 ----
mean loss: 178.52
 ---- batch: 070 ----
mean loss: 182.54
 ---- batch: 080 ----
mean loss: 181.17
 ---- batch: 090 ----
mean loss: 183.52
train mean loss: 183.07
epoch train time: 0:00:16.922200
elapsed time: 1:08:12.357512
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 00:51:01.349926
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.32
 ---- batch: 020 ----
mean loss: 184.48
 ---- batch: 030 ----
mean loss: 182.55
 ---- batch: 040 ----
mean loss: 175.54
 ---- batch: 050 ----
mean loss: 185.44
 ---- batch: 060 ----
mean loss: 183.47
 ---- batch: 070 ----
mean loss: 178.80
 ---- batch: 080 ----
mean loss: 180.23
 ---- batch: 090 ----
mean loss: 186.19
train mean loss: 182.69
epoch train time: 0:00:16.937124
elapsed time: 1:08:29.295909
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 00:51:18.288231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.85
 ---- batch: 020 ----
mean loss: 185.81
 ---- batch: 030 ----
mean loss: 183.33
 ---- batch: 040 ----
mean loss: 174.06
 ---- batch: 050 ----
mean loss: 183.32
 ---- batch: 060 ----
mean loss: 181.86
 ---- batch: 070 ----
mean loss: 186.01
 ---- batch: 080 ----
mean loss: 183.22
 ---- batch: 090 ----
mean loss: 178.30
train mean loss: 182.77
epoch train time: 0:00:16.945952
elapsed time: 1:08:46.243101
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 00:51:35.235579
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.59
 ---- batch: 020 ----
mean loss: 179.73
 ---- batch: 030 ----
mean loss: 184.91
 ---- batch: 040 ----
mean loss: 179.49
 ---- batch: 050 ----
mean loss: 182.92
 ---- batch: 060 ----
mean loss: 176.14
 ---- batch: 070 ----
mean loss: 182.87
 ---- batch: 080 ----
mean loss: 178.87
 ---- batch: 090 ----
mean loss: 187.70
train mean loss: 182.69
epoch train time: 0:00:16.944954
elapsed time: 1:09:03.189364
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 00:51:52.181814
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.98
 ---- batch: 020 ----
mean loss: 179.09
 ---- batch: 030 ----
mean loss: 185.09
 ---- batch: 040 ----
mean loss: 183.78
 ---- batch: 050 ----
mean loss: 178.40
 ---- batch: 060 ----
mean loss: 181.44
 ---- batch: 070 ----
mean loss: 179.22
 ---- batch: 080 ----
mean loss: 181.36
 ---- batch: 090 ----
mean loss: 187.47
train mean loss: 182.51
epoch train time: 0:00:16.932659
elapsed time: 1:09:20.123336
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 00:52:09.115674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.84
 ---- batch: 020 ----
mean loss: 183.25
 ---- batch: 030 ----
mean loss: 183.63
 ---- batch: 040 ----
mean loss: 179.21
 ---- batch: 050 ----
mean loss: 177.86
 ---- batch: 060 ----
mean loss: 184.97
 ---- batch: 070 ----
mean loss: 188.03
 ---- batch: 080 ----
mean loss: 176.54
 ---- batch: 090 ----
mean loss: 186.98
train mean loss: 182.76
epoch train time: 0:00:16.931756
elapsed time: 1:09:37.056313
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 00:52:26.048665
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.37
 ---- batch: 020 ----
mean loss: 192.62
 ---- batch: 030 ----
mean loss: 181.38
 ---- batch: 040 ----
mean loss: 174.54
 ---- batch: 050 ----
mean loss: 183.34
 ---- batch: 060 ----
mean loss: 181.97
 ---- batch: 070 ----
mean loss: 183.48
 ---- batch: 080 ----
mean loss: 177.97
 ---- batch: 090 ----
mean loss: 185.06
train mean loss: 182.72
epoch train time: 0:00:16.968777
elapsed time: 1:09:54.026411
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 00:52:43.018761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.43
 ---- batch: 020 ----
mean loss: 182.87
 ---- batch: 030 ----
mean loss: 179.24
 ---- batch: 040 ----
mean loss: 181.79
 ---- batch: 050 ----
mean loss: 188.48
 ---- batch: 060 ----
mean loss: 184.21
 ---- batch: 070 ----
mean loss: 180.62
 ---- batch: 080 ----
mean loss: 184.40
 ---- batch: 090 ----
mean loss: 186.67
train mean loss: 182.17
epoch train time: 0:00:16.927138
elapsed time: 1:10:10.954793
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 00:52:59.947171
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.28
 ---- batch: 020 ----
mean loss: 183.16
 ---- batch: 030 ----
mean loss: 178.77
 ---- batch: 040 ----
mean loss: 186.44
 ---- batch: 050 ----
mean loss: 182.22
 ---- batch: 060 ----
mean loss: 179.04
 ---- batch: 070 ----
mean loss: 182.21
 ---- batch: 080 ----
mean loss: 177.98
 ---- batch: 090 ----
mean loss: 183.12
train mean loss: 182.58
epoch train time: 0:00:16.937785
elapsed time: 1:10:27.893874
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 00:53:16.886196
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.78
 ---- batch: 020 ----
mean loss: 181.87
 ---- batch: 030 ----
mean loss: 180.48
 ---- batch: 040 ----
mean loss: 187.34
 ---- batch: 050 ----
mean loss: 185.91
 ---- batch: 060 ----
mean loss: 186.13
 ---- batch: 070 ----
mean loss: 176.42
 ---- batch: 080 ----
mean loss: 175.01
 ---- batch: 090 ----
mean loss: 185.56
train mean loss: 182.20
epoch train time: 0:00:16.920369
elapsed time: 1:10:44.815483
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 00:53:33.807809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.24
 ---- batch: 020 ----
mean loss: 182.17
 ---- batch: 030 ----
mean loss: 178.64
 ---- batch: 040 ----
mean loss: 186.27
 ---- batch: 050 ----
mean loss: 178.20
 ---- batch: 060 ----
mean loss: 186.19
 ---- batch: 070 ----
mean loss: 194.25
 ---- batch: 080 ----
mean loss: 176.56
 ---- batch: 090 ----
mean loss: 180.13
train mean loss: 182.41
epoch train time: 0:00:16.934700
elapsed time: 1:11:01.751520
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 00:53:50.743880
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.11
 ---- batch: 020 ----
mean loss: 183.24
 ---- batch: 030 ----
mean loss: 182.33
 ---- batch: 040 ----
mean loss: 173.62
 ---- batch: 050 ----
mean loss: 188.24
 ---- batch: 060 ----
mean loss: 185.87
 ---- batch: 070 ----
mean loss: 180.39
 ---- batch: 080 ----
mean loss: 185.42
 ---- batch: 090 ----
mean loss: 184.11
train mean loss: 182.34
epoch train time: 0:00:16.941133
elapsed time: 1:11:18.749102
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_4/checkpoint.pth.tar
**** end time: 2019-09-25 00:54:07.740900 ****
