Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_7', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 16517
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-27 00:56:17.736191 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 16, 24]             100
              Tanh-2           [-1, 10, 16, 24]               0
            Conv2d-3           [-1, 10, 15, 24]           1,000
              Tanh-4           [-1, 10, 15, 24]               0
            Conv2d-5           [-1, 10, 16, 24]           1,000
              Tanh-6           [-1, 10, 16, 24]               0
            Conv2d-7           [-1, 10, 15, 24]           1,000
              Tanh-8           [-1, 10, 15, 24]               0
            Conv2d-9            [-1, 1, 15, 24]              30
             Tanh-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
          Dropout-12                  [-1, 360]               0
           Linear-13                  [-1, 100]          36,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 39,230
Trainable params: 39,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 00:56:17.744931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4467.58
 ---- batch: 020 ----
mean loss: 2387.35
 ---- batch: 030 ----
mean loss: 1416.06
 ---- batch: 040 ----
mean loss: 1331.60
 ---- batch: 050 ----
mean loss: 1171.71
 ---- batch: 060 ----
mean loss: 1103.69
 ---- batch: 070 ----
mean loss: 1071.19
 ---- batch: 080 ----
mean loss: 1040.05
 ---- batch: 090 ----
mean loss: 982.45
 ---- batch: 100 ----
mean loss: 970.81
 ---- batch: 110 ----
mean loss: 931.36
train mean loss: 1517.94
epoch train time: 0:00:32.412816
elapsed time: 0:00:32.423782
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 00:56:50.160009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.39
 ---- batch: 020 ----
mean loss: 882.03
 ---- batch: 030 ----
mean loss: 845.79
 ---- batch: 040 ----
mean loss: 844.18
 ---- batch: 050 ----
mean loss: 806.06
 ---- batch: 060 ----
mean loss: 795.25
 ---- batch: 070 ----
mean loss: 799.83
 ---- batch: 080 ----
mean loss: 794.71
 ---- batch: 090 ----
mean loss: 782.28
 ---- batch: 100 ----
mean loss: 780.02
 ---- batch: 110 ----
mean loss: 805.63
train mean loss: 819.26
epoch train time: 0:00:02.228380
elapsed time: 0:00:34.652323
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 00:56:52.388590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 726.60
 ---- batch: 020 ----
mean loss: 754.08
 ---- batch: 030 ----
mean loss: 715.64
 ---- batch: 040 ----
mean loss: 721.64
 ---- batch: 050 ----
mean loss: 715.60
 ---- batch: 060 ----
mean loss: 705.73
 ---- batch: 070 ----
mean loss: 722.76
 ---- batch: 080 ----
mean loss: 715.94
 ---- batch: 090 ----
mean loss: 704.48
 ---- batch: 100 ----
mean loss: 686.60
 ---- batch: 110 ----
mean loss: 689.41
train mean loss: 713.09
epoch train time: 0:00:02.124552
elapsed time: 0:00:36.777044
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 00:56:54.513318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 693.66
 ---- batch: 020 ----
mean loss: 677.86
 ---- batch: 030 ----
mean loss: 677.24
 ---- batch: 040 ----
mean loss: 656.37
 ---- batch: 050 ----
mean loss: 659.33
 ---- batch: 060 ----
mean loss: 665.26
 ---- batch: 070 ----
mean loss: 664.08
 ---- batch: 080 ----
mean loss: 629.88
 ---- batch: 090 ----
mean loss: 637.22
 ---- batch: 100 ----
mean loss: 648.20
 ---- batch: 110 ----
mean loss: 638.45
train mean loss: 658.00
epoch train time: 0:00:02.121052
elapsed time: 0:00:38.898284
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 00:56:56.634524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 615.91
 ---- batch: 020 ----
mean loss: 633.81
 ---- batch: 030 ----
mean loss: 636.24
 ---- batch: 040 ----
mean loss: 644.52
 ---- batch: 050 ----
mean loss: 628.02
 ---- batch: 060 ----
mean loss: 620.72
 ---- batch: 070 ----
mean loss: 624.03
 ---- batch: 080 ----
mean loss: 628.30
 ---- batch: 090 ----
mean loss: 606.03
 ---- batch: 100 ----
mean loss: 604.35
 ---- batch: 110 ----
mean loss: 605.36
train mean loss: 621.78
epoch train time: 0:00:02.129320
elapsed time: 0:00:41.027740
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 00:56:58.763977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 591.74
 ---- batch: 020 ----
mean loss: 584.44
 ---- batch: 030 ----
mean loss: 586.69
 ---- batch: 040 ----
mean loss: 586.14
 ---- batch: 050 ----
mean loss: 601.58
 ---- batch: 060 ----
mean loss: 583.18
 ---- batch: 070 ----
mean loss: 572.29
 ---- batch: 080 ----
mean loss: 585.82
 ---- batch: 090 ----
mean loss: 562.41
 ---- batch: 100 ----
mean loss: 577.18
 ---- batch: 110 ----
mean loss: 565.33
train mean loss: 580.58
epoch train time: 0:00:02.131082
elapsed time: 0:00:43.158968
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 00:57:00.895211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 557.34
 ---- batch: 020 ----
mean loss: 560.65
 ---- batch: 030 ----
mean loss: 562.21
 ---- batch: 040 ----
mean loss: 554.92
 ---- batch: 050 ----
mean loss: 555.22
 ---- batch: 060 ----
mean loss: 549.69
 ---- batch: 070 ----
mean loss: 567.61
 ---- batch: 080 ----
mean loss: 571.62
 ---- batch: 090 ----
mean loss: 556.58
 ---- batch: 100 ----
mean loss: 565.04
 ---- batch: 110 ----
mean loss: 546.72
train mean loss: 559.03
epoch train time: 0:00:02.130208
elapsed time: 0:00:45.289318
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 00:57:03.025557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 546.24
 ---- batch: 020 ----
mean loss: 532.77
 ---- batch: 030 ----
mean loss: 548.72
 ---- batch: 040 ----
mean loss: 544.74
 ---- batch: 050 ----
mean loss: 532.55
 ---- batch: 060 ----
mean loss: 536.96
 ---- batch: 070 ----
mean loss: 540.63
 ---- batch: 080 ----
mean loss: 533.82
 ---- batch: 090 ----
mean loss: 521.35
 ---- batch: 100 ----
mean loss: 547.02
 ---- batch: 110 ----
mean loss: 542.52
train mean loss: 538.56
epoch train time: 0:00:02.137145
elapsed time: 0:00:47.426604
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 00:57:05.162861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.95
 ---- batch: 020 ----
mean loss: 530.41
 ---- batch: 030 ----
mean loss: 532.77
 ---- batch: 040 ----
mean loss: 527.55
 ---- batch: 050 ----
mean loss: 528.72
 ---- batch: 060 ----
mean loss: 527.40
 ---- batch: 070 ----
mean loss: 517.97
 ---- batch: 080 ----
mean loss: 525.38
 ---- batch: 090 ----
mean loss: 553.04
 ---- batch: 100 ----
mean loss: 566.71
 ---- batch: 110 ----
mean loss: 543.43
train mean loss: 534.59
epoch train time: 0:00:02.128008
elapsed time: 0:00:49.554803
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 00:57:07.291056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.19
 ---- batch: 020 ----
mean loss: 529.37
 ---- batch: 030 ----
mean loss: 521.41
 ---- batch: 040 ----
mean loss: 531.11
 ---- batch: 050 ----
mean loss: 524.87
 ---- batch: 060 ----
mean loss: 528.76
 ---- batch: 070 ----
mean loss: 516.40
 ---- batch: 080 ----
mean loss: 525.06
 ---- batch: 090 ----
mean loss: 511.93
 ---- batch: 100 ----
mean loss: 510.47
 ---- batch: 110 ----
mean loss: 530.03
train mean loss: 522.33
epoch train time: 0:00:02.139249
elapsed time: 0:00:51.694229
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 00:57:09.430487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.17
 ---- batch: 020 ----
mean loss: 517.35
 ---- batch: 030 ----
mean loss: 507.08
 ---- batch: 040 ----
mean loss: 529.07
 ---- batch: 050 ----
mean loss: 524.93
 ---- batch: 060 ----
mean loss: 531.28
 ---- batch: 070 ----
mean loss: 535.95
 ---- batch: 080 ----
mean loss: 526.21
 ---- batch: 090 ----
mean loss: 540.69
 ---- batch: 100 ----
mean loss: 519.53
 ---- batch: 110 ----
mean loss: 515.19
train mean loss: 525.46
epoch train time: 0:00:02.135021
elapsed time: 0:00:53.829454
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 00:57:11.565700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.21
 ---- batch: 020 ----
mean loss: 519.57
 ---- batch: 030 ----
mean loss: 515.32
 ---- batch: 040 ----
mean loss: 521.20
 ---- batch: 050 ----
mean loss: 524.20
 ---- batch: 060 ----
mean loss: 504.33
 ---- batch: 070 ----
mean loss: 516.93
 ---- batch: 080 ----
mean loss: 524.03
 ---- batch: 090 ----
mean loss: 523.39
 ---- batch: 100 ----
mean loss: 518.10
 ---- batch: 110 ----
mean loss: 509.55
train mean loss: 517.41
epoch train time: 0:00:02.136029
elapsed time: 0:00:55.965648
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 00:57:13.701892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.73
 ---- batch: 020 ----
mean loss: 504.31
 ---- batch: 030 ----
mean loss: 507.41
 ---- batch: 040 ----
mean loss: 502.25
 ---- batch: 050 ----
mean loss: 505.98
 ---- batch: 060 ----
mean loss: 536.37
 ---- batch: 070 ----
mean loss: 501.83
 ---- batch: 080 ----
mean loss: 500.90
 ---- batch: 090 ----
mean loss: 517.11
 ---- batch: 100 ----
mean loss: 528.45
 ---- batch: 110 ----
mean loss: 523.37
train mean loss: 514.97
epoch train time: 0:00:02.135565
elapsed time: 0:00:58.101389
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 00:57:15.837650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.60
 ---- batch: 020 ----
mean loss: 527.23
 ---- batch: 030 ----
mean loss: 507.08
 ---- batch: 040 ----
mean loss: 508.82
 ---- batch: 050 ----
mean loss: 519.01
 ---- batch: 060 ----
mean loss: 523.48
 ---- batch: 070 ----
mean loss: 518.51
 ---- batch: 080 ----
mean loss: 504.60
 ---- batch: 090 ----
mean loss: 512.84
 ---- batch: 100 ----
mean loss: 523.33
 ---- batch: 110 ----
mean loss: 501.29
train mean loss: 513.60
epoch train time: 0:00:02.134101
elapsed time: 0:01:00.235647
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 00:57:17.971883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.26
 ---- batch: 020 ----
mean loss: 521.76
 ---- batch: 030 ----
mean loss: 512.37
 ---- batch: 040 ----
mean loss: 502.35
 ---- batch: 050 ----
mean loss: 502.43
 ---- batch: 060 ----
mean loss: 505.86
 ---- batch: 070 ----
mean loss: 505.88
 ---- batch: 080 ----
mean loss: 508.30
 ---- batch: 090 ----
mean loss: 514.10
 ---- batch: 100 ----
mean loss: 506.85
 ---- batch: 110 ----
mean loss: 524.50
train mean loss: 510.58
epoch train time: 0:00:02.133179
elapsed time: 0:01:02.368962
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 00:57:20.105221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.06
 ---- batch: 020 ----
mean loss: 512.43
 ---- batch: 030 ----
mean loss: 519.27
 ---- batch: 040 ----
mean loss: 504.44
 ---- batch: 050 ----
mean loss: 501.85
 ---- batch: 060 ----
mean loss: 521.80
 ---- batch: 070 ----
mean loss: 498.24
 ---- batch: 080 ----
mean loss: 513.64
 ---- batch: 090 ----
mean loss: 513.17
 ---- batch: 100 ----
mean loss: 506.25
 ---- batch: 110 ----
mean loss: 504.42
train mean loss: 508.85
epoch train time: 0:00:02.129114
elapsed time: 0:01:04.498236
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 00:57:22.234482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.40
 ---- batch: 020 ----
mean loss: 504.35
 ---- batch: 030 ----
mean loss: 491.78
 ---- batch: 040 ----
mean loss: 505.02
 ---- batch: 050 ----
mean loss: 505.02
 ---- batch: 060 ----
mean loss: 498.44
 ---- batch: 070 ----
mean loss: 508.87
 ---- batch: 080 ----
mean loss: 508.85
 ---- batch: 090 ----
mean loss: 492.10
 ---- batch: 100 ----
mean loss: 514.84
 ---- batch: 110 ----
mean loss: 514.09
train mean loss: 503.87
epoch train time: 0:00:02.133162
elapsed time: 0:01:06.631546
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 00:57:24.367785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.25
 ---- batch: 020 ----
mean loss: 508.46
 ---- batch: 030 ----
mean loss: 497.32
 ---- batch: 040 ----
mean loss: 503.61
 ---- batch: 050 ----
mean loss: 514.73
 ---- batch: 060 ----
mean loss: 502.42
 ---- batch: 070 ----
mean loss: 504.51
 ---- batch: 080 ----
mean loss: 502.38
 ---- batch: 090 ----
mean loss: 497.61
 ---- batch: 100 ----
mean loss: 525.00
 ---- batch: 110 ----
mean loss: 525.45
train mean loss: 505.21
epoch train time: 0:00:02.130116
elapsed time: 0:01:08.761810
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 00:57:26.498052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.44
 ---- batch: 020 ----
mean loss: 507.59
 ---- batch: 030 ----
mean loss: 502.65
 ---- batch: 040 ----
mean loss: 489.97
 ---- batch: 050 ----
mean loss: 498.25
 ---- batch: 060 ----
mean loss: 507.48
 ---- batch: 070 ----
mean loss: 504.62
 ---- batch: 080 ----
mean loss: 506.89
 ---- batch: 090 ----
mean loss: 495.25
 ---- batch: 100 ----
mean loss: 497.48
 ---- batch: 110 ----
mean loss: 512.47
train mean loss: 501.48
epoch train time: 0:00:02.131922
elapsed time: 0:01:10.893873
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 00:57:28.630111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.51
 ---- batch: 020 ----
mean loss: 502.80
 ---- batch: 030 ----
mean loss: 527.36
 ---- batch: 040 ----
mean loss: 518.39
 ---- batch: 050 ----
mean loss: 488.38
 ---- batch: 060 ----
mean loss: 500.33
 ---- batch: 070 ----
mean loss: 506.19
 ---- batch: 080 ----
mean loss: 501.19
 ---- batch: 090 ----
mean loss: 512.89
 ---- batch: 100 ----
mean loss: 503.83
 ---- batch: 110 ----
mean loss: 500.09
train mean loss: 505.66
epoch train time: 0:00:02.135205
elapsed time: 0:01:13.029213
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 00:57:30.765481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.91
 ---- batch: 020 ----
mean loss: 503.86
 ---- batch: 030 ----
mean loss: 523.95
 ---- batch: 040 ----
mean loss: 505.07
 ---- batch: 050 ----
mean loss: 511.51
 ---- batch: 060 ----
mean loss: 499.95
 ---- batch: 070 ----
mean loss: 510.28
 ---- batch: 080 ----
mean loss: 521.54
 ---- batch: 090 ----
mean loss: 499.37
 ---- batch: 100 ----
mean loss: 498.83
 ---- batch: 110 ----
mean loss: 501.90
train mean loss: 506.96
epoch train time: 0:00:02.129150
elapsed time: 0:01:15.158532
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 00:57:32.894771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.92
 ---- batch: 020 ----
mean loss: 497.76
 ---- batch: 030 ----
mean loss: 499.69
 ---- batch: 040 ----
mean loss: 492.60
 ---- batch: 050 ----
mean loss: 496.26
 ---- batch: 060 ----
mean loss: 492.56
 ---- batch: 070 ----
mean loss: 496.48
 ---- batch: 080 ----
mean loss: 489.76
 ---- batch: 090 ----
mean loss: 495.19
 ---- batch: 100 ----
mean loss: 503.66
 ---- batch: 110 ----
mean loss: 485.94
train mean loss: 496.30
epoch train time: 0:00:02.133426
elapsed time: 0:01:17.292108
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 00:57:35.028367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.67
 ---- batch: 020 ----
mean loss: 488.93
 ---- batch: 030 ----
mean loss: 497.81
 ---- batch: 040 ----
mean loss: 493.84
 ---- batch: 050 ----
mean loss: 496.67
 ---- batch: 060 ----
mean loss: 516.86
 ---- batch: 070 ----
mean loss: 516.34
 ---- batch: 080 ----
mean loss: 484.26
 ---- batch: 090 ----
mean loss: 496.77
 ---- batch: 100 ----
mean loss: 514.42
 ---- batch: 110 ----
mean loss: 508.50
train mean loss: 500.37
epoch train time: 0:00:02.131846
elapsed time: 0:01:19.424110
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 00:57:37.160350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.13
 ---- batch: 020 ----
mean loss: 492.73
 ---- batch: 030 ----
mean loss: 498.32
 ---- batch: 040 ----
mean loss: 492.40
 ---- batch: 050 ----
mean loss: 505.29
 ---- batch: 060 ----
mean loss: 500.66
 ---- batch: 070 ----
mean loss: 502.82
 ---- batch: 080 ----
mean loss: 505.66
 ---- batch: 090 ----
mean loss: 490.09
 ---- batch: 100 ----
mean loss: 509.83
 ---- batch: 110 ----
mean loss: 497.18
train mean loss: 499.58
epoch train time: 0:00:02.134041
elapsed time: 0:01:21.558295
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 00:57:39.294536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.72
 ---- batch: 020 ----
mean loss: 497.40
 ---- batch: 030 ----
mean loss: 496.78
 ---- batch: 040 ----
mean loss: 494.08
 ---- batch: 050 ----
mean loss: 501.68
 ---- batch: 060 ----
mean loss: 492.62
 ---- batch: 070 ----
mean loss: 495.33
 ---- batch: 080 ----
mean loss: 497.14
 ---- batch: 090 ----
mean loss: 491.82
 ---- batch: 100 ----
mean loss: 496.19
 ---- batch: 110 ----
mean loss: 491.17
train mean loss: 495.15
epoch train time: 0:00:02.138955
elapsed time: 0:01:23.697429
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 00:57:41.433692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.27
 ---- batch: 020 ----
mean loss: 497.88
 ---- batch: 030 ----
mean loss: 498.62
 ---- batch: 040 ----
mean loss: 494.40
 ---- batch: 050 ----
mean loss: 498.30
 ---- batch: 060 ----
mean loss: 499.57
 ---- batch: 070 ----
mean loss: 514.20
 ---- batch: 080 ----
mean loss: 502.87
 ---- batch: 090 ----
mean loss: 488.58
 ---- batch: 100 ----
mean loss: 489.79
 ---- batch: 110 ----
mean loss: 510.42
train mean loss: 498.68
epoch train time: 0:00:02.134737
elapsed time: 0:01:25.832352
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 00:57:43.568608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.45
 ---- batch: 020 ----
mean loss: 489.33
 ---- batch: 030 ----
mean loss: 488.18
 ---- batch: 040 ----
mean loss: 487.47
 ---- batch: 050 ----
mean loss: 476.09
 ---- batch: 060 ----
mean loss: 491.37
 ---- batch: 070 ----
mean loss: 478.18
 ---- batch: 080 ----
mean loss: 508.25
 ---- batch: 090 ----
mean loss: 498.87
 ---- batch: 100 ----
mean loss: 487.93
 ---- batch: 110 ----
mean loss: 483.25
train mean loss: 490.71
epoch train time: 0:00:02.132329
elapsed time: 0:01:27.964839
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 00:57:45.701079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.62
 ---- batch: 020 ----
mean loss: 502.21
 ---- batch: 030 ----
mean loss: 498.31
 ---- batch: 040 ----
mean loss: 476.40
 ---- batch: 050 ----
mean loss: 479.75
 ---- batch: 060 ----
mean loss: 479.45
 ---- batch: 070 ----
mean loss: 492.37
 ---- batch: 080 ----
mean loss: 488.34
 ---- batch: 090 ----
mean loss: 501.58
 ---- batch: 100 ----
mean loss: 494.40
 ---- batch: 110 ----
mean loss: 496.10
train mean loss: 491.66
epoch train time: 0:00:02.132010
elapsed time: 0:01:30.096989
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 00:57:47.833248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.93
 ---- batch: 020 ----
mean loss: 489.04
 ---- batch: 030 ----
mean loss: 482.71
 ---- batch: 040 ----
mean loss: 494.23
 ---- batch: 050 ----
mean loss: 503.45
 ---- batch: 060 ----
mean loss: 495.99
 ---- batch: 070 ----
mean loss: 498.22
 ---- batch: 080 ----
mean loss: 504.28
 ---- batch: 090 ----
mean loss: 470.39
 ---- batch: 100 ----
mean loss: 487.02
 ---- batch: 110 ----
mean loss: 493.30
train mean loss: 490.23
epoch train time: 0:00:02.134202
elapsed time: 0:01:32.231349
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 00:57:49.967590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.92
 ---- batch: 020 ----
mean loss: 480.13
 ---- batch: 030 ----
mean loss: 479.87
 ---- batch: 040 ----
mean loss: 485.47
 ---- batch: 050 ----
mean loss: 492.54
 ---- batch: 060 ----
mean loss: 492.57
 ---- batch: 070 ----
mean loss: 481.82
 ---- batch: 080 ----
mean loss: 491.46
 ---- batch: 090 ----
mean loss: 486.54
 ---- batch: 100 ----
mean loss: 504.05
 ---- batch: 110 ----
mean loss: 490.54
train mean loss: 488.36
epoch train time: 0:00:02.139565
elapsed time: 0:01:34.371058
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 00:57:52.107297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.51
 ---- batch: 020 ----
mean loss: 482.38
 ---- batch: 030 ----
mean loss: 483.45
 ---- batch: 040 ----
mean loss: 494.00
 ---- batch: 050 ----
mean loss: 481.69
 ---- batch: 060 ----
mean loss: 506.84
 ---- batch: 070 ----
mean loss: 474.69
 ---- batch: 080 ----
mean loss: 498.91
 ---- batch: 090 ----
mean loss: 487.92
 ---- batch: 100 ----
mean loss: 494.72
 ---- batch: 110 ----
mean loss: 493.50
train mean loss: 488.37
epoch train time: 0:00:02.135336
elapsed time: 0:01:36.506529
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 00:57:54.242778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.73
 ---- batch: 020 ----
mean loss: 496.25
 ---- batch: 030 ----
mean loss: 493.77
 ---- batch: 040 ----
mean loss: 510.27
 ---- batch: 050 ----
mean loss: 510.12
 ---- batch: 060 ----
mean loss: 494.83
 ---- batch: 070 ----
mean loss: 483.86
 ---- batch: 080 ----
mean loss: 487.56
 ---- batch: 090 ----
mean loss: 488.24
 ---- batch: 100 ----
mean loss: 476.02
 ---- batch: 110 ----
mean loss: 491.17
train mean loss: 492.17
epoch train time: 0:00:02.131415
elapsed time: 0:01:38.638108
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 00:57:56.374348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.32
 ---- batch: 020 ----
mean loss: 489.67
 ---- batch: 030 ----
mean loss: 492.87
 ---- batch: 040 ----
mean loss: 479.07
 ---- batch: 050 ----
mean loss: 480.78
 ---- batch: 060 ----
mean loss: 492.57
 ---- batch: 070 ----
mean loss: 486.03
 ---- batch: 080 ----
mean loss: 503.80
 ---- batch: 090 ----
mean loss: 476.65
 ---- batch: 100 ----
mean loss: 484.22
 ---- batch: 110 ----
mean loss: 485.91
train mean loss: 486.52
epoch train time: 0:00:02.132396
elapsed time: 0:01:40.770659
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 00:57:58.506900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.55
 ---- batch: 020 ----
mean loss: 488.43
 ---- batch: 030 ----
mean loss: 490.10
 ---- batch: 040 ----
mean loss: 485.54
 ---- batch: 050 ----
mean loss: 475.18
 ---- batch: 060 ----
mean loss: 476.01
 ---- batch: 070 ----
mean loss: 511.40
 ---- batch: 080 ----
mean loss: 491.55
 ---- batch: 090 ----
mean loss: 485.74
 ---- batch: 100 ----
mean loss: 484.17
 ---- batch: 110 ----
mean loss: 490.98
train mean loss: 487.75
epoch train time: 0:00:02.134560
elapsed time: 0:01:42.905364
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 00:58:00.641627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.89
 ---- batch: 020 ----
mean loss: 469.87
 ---- batch: 030 ----
mean loss: 472.16
 ---- batch: 040 ----
mean loss: 482.77
 ---- batch: 050 ----
mean loss: 483.77
 ---- batch: 060 ----
mean loss: 492.18
 ---- batch: 070 ----
mean loss: 469.63
 ---- batch: 080 ----
mean loss: 486.29
 ---- batch: 090 ----
mean loss: 481.17
 ---- batch: 100 ----
mean loss: 493.56
 ---- batch: 110 ----
mean loss: 488.89
train mean loss: 482.13
epoch train time: 0:00:02.131517
elapsed time: 0:01:45.037075
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 00:58:02.773333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 510.51
 ---- batch: 020 ----
mean loss: 485.12
 ---- batch: 030 ----
mean loss: 477.21
 ---- batch: 040 ----
mean loss: 481.49
 ---- batch: 050 ----
mean loss: 486.30
 ---- batch: 060 ----
mean loss: 483.63
 ---- batch: 070 ----
mean loss: 479.40
 ---- batch: 080 ----
mean loss: 487.43
 ---- batch: 090 ----
mean loss: 469.66
 ---- batch: 100 ----
mean loss: 485.54
 ---- batch: 110 ----
mean loss: 478.79
train mean loss: 484.18
epoch train time: 0:00:02.132646
elapsed time: 0:01:47.169878
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 00:58:04.906116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.40
 ---- batch: 020 ----
mean loss: 466.28
 ---- batch: 030 ----
mean loss: 481.71
 ---- batch: 040 ----
mean loss: 475.86
 ---- batch: 050 ----
mean loss: 477.47
 ---- batch: 060 ----
mean loss: 480.28
 ---- batch: 070 ----
mean loss: 494.58
 ---- batch: 080 ----
mean loss: 492.16
 ---- batch: 090 ----
mean loss: 473.63
 ---- batch: 100 ----
mean loss: 480.34
 ---- batch: 110 ----
mean loss: 488.34
train mean loss: 480.99
epoch train time: 0:00:02.134299
elapsed time: 0:01:49.304321
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 00:58:07.040565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.31
 ---- batch: 020 ----
mean loss: 496.79
 ---- batch: 030 ----
mean loss: 482.92
 ---- batch: 040 ----
mean loss: 481.03
 ---- batch: 050 ----
mean loss: 476.20
 ---- batch: 060 ----
mean loss: 472.71
 ---- batch: 070 ----
mean loss: 469.85
 ---- batch: 080 ----
mean loss: 485.02
 ---- batch: 090 ----
mean loss: 452.43
 ---- batch: 100 ----
mean loss: 482.69
 ---- batch: 110 ----
mean loss: 499.94
train mean loss: 479.68
epoch train time: 0:00:02.131703
elapsed time: 0:01:51.436169
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 00:58:09.172410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.95
 ---- batch: 020 ----
mean loss: 481.04
 ---- batch: 030 ----
mean loss: 479.24
 ---- batch: 040 ----
mean loss: 483.59
 ---- batch: 050 ----
mean loss: 471.19
 ---- batch: 060 ----
mean loss: 486.66
 ---- batch: 070 ----
mean loss: 469.11
 ---- batch: 080 ----
mean loss: 483.35
 ---- batch: 090 ----
mean loss: 463.05
 ---- batch: 100 ----
mean loss: 474.32
 ---- batch: 110 ----
mean loss: 487.24
train mean loss: 478.37
epoch train time: 0:00:02.135554
elapsed time: 0:01:53.571861
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 00:58:11.308101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.14
 ---- batch: 020 ----
mean loss: 468.16
 ---- batch: 030 ----
mean loss: 475.39
 ---- batch: 040 ----
mean loss: 469.32
 ---- batch: 050 ----
mean loss: 477.77
 ---- batch: 060 ----
mean loss: 478.20
 ---- batch: 070 ----
mean loss: 483.58
 ---- batch: 080 ----
mean loss: 509.45
 ---- batch: 090 ----
mean loss: 528.72
 ---- batch: 100 ----
mean loss: 495.27
 ---- batch: 110 ----
mean loss: 483.22
train mean loss: 484.29
epoch train time: 0:00:02.135478
elapsed time: 0:01:55.707475
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 00:58:13.443712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.56
 ---- batch: 020 ----
mean loss: 474.16
 ---- batch: 030 ----
mean loss: 466.34
 ---- batch: 040 ----
mean loss: 467.81
 ---- batch: 050 ----
mean loss: 475.21
 ---- batch: 060 ----
mean loss: 483.02
 ---- batch: 070 ----
mean loss: 473.02
 ---- batch: 080 ----
mean loss: 472.85
 ---- batch: 090 ----
mean loss: 472.50
 ---- batch: 100 ----
mean loss: 475.72
 ---- batch: 110 ----
mean loss: 491.42
train mean loss: 474.29
epoch train time: 0:00:02.131051
elapsed time: 0:01:57.838667
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 00:58:15.574935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.78
 ---- batch: 020 ----
mean loss: 479.51
 ---- batch: 030 ----
mean loss: 475.48
 ---- batch: 040 ----
mean loss: 463.69
 ---- batch: 050 ----
mean loss: 459.30
 ---- batch: 060 ----
mean loss: 471.51
 ---- batch: 070 ----
mean loss: 459.83
 ---- batch: 080 ----
mean loss: 480.71
 ---- batch: 090 ----
mean loss: 465.45
 ---- batch: 100 ----
mean loss: 461.79
 ---- batch: 110 ----
mean loss: 468.44
train mean loss: 469.34
epoch train time: 0:00:02.137204
elapsed time: 0:01:59.976036
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 00:58:17.712274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.24
 ---- batch: 020 ----
mean loss: 461.88
 ---- batch: 030 ----
mean loss: 475.27
 ---- batch: 040 ----
mean loss: 479.47
 ---- batch: 050 ----
mean loss: 467.23
 ---- batch: 060 ----
mean loss: 479.25
 ---- batch: 070 ----
mean loss: 473.40
 ---- batch: 080 ----
mean loss: 475.93
 ---- batch: 090 ----
mean loss: 459.45
 ---- batch: 100 ----
mean loss: 461.79
 ---- batch: 110 ----
mean loss: 488.30
train mean loss: 473.52
epoch train time: 0:00:02.142183
elapsed time: 0:02:02.118365
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 00:58:19.854623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.64
 ---- batch: 020 ----
mean loss: 476.26
 ---- batch: 030 ----
mean loss: 475.00
 ---- batch: 040 ----
mean loss: 474.96
 ---- batch: 050 ----
mean loss: 461.18
 ---- batch: 060 ----
mean loss: 470.09
 ---- batch: 070 ----
mean loss: 469.49
 ---- batch: 080 ----
mean loss: 469.01
 ---- batch: 090 ----
mean loss: 457.84
 ---- batch: 100 ----
mean loss: 471.10
 ---- batch: 110 ----
mean loss: 460.34
train mean loss: 471.20
epoch train time: 0:00:02.142237
elapsed time: 0:02:04.260779
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 00:58:21.997024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.10
 ---- batch: 020 ----
mean loss: 461.64
 ---- batch: 030 ----
mean loss: 448.77
 ---- batch: 040 ----
mean loss: 472.16
 ---- batch: 050 ----
mean loss: 476.62
 ---- batch: 060 ----
mean loss: 490.50
 ---- batch: 070 ----
mean loss: 467.53
 ---- batch: 080 ----
mean loss: 463.50
 ---- batch: 090 ----
mean loss: 476.12
 ---- batch: 100 ----
mean loss: 469.19
 ---- batch: 110 ----
mean loss: 455.97
train mean loss: 469.11
epoch train time: 0:00:02.135810
elapsed time: 0:02:06.396733
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 00:58:24.132971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.86
 ---- batch: 020 ----
mean loss: 472.94
 ---- batch: 030 ----
mean loss: 468.92
 ---- batch: 040 ----
mean loss: 467.29
 ---- batch: 050 ----
mean loss: 459.19
 ---- batch: 060 ----
mean loss: 455.35
 ---- batch: 070 ----
mean loss: 466.64
 ---- batch: 080 ----
mean loss: 465.66
 ---- batch: 090 ----
mean loss: 466.96
 ---- batch: 100 ----
mean loss: 464.43
 ---- batch: 110 ----
mean loss: 462.19
train mean loss: 464.84
epoch train time: 0:00:02.132513
elapsed time: 0:02:08.529385
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 00:58:26.265647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.75
 ---- batch: 020 ----
mean loss: 465.70
 ---- batch: 030 ----
mean loss: 466.62
 ---- batch: 040 ----
mean loss: 462.14
 ---- batch: 050 ----
mean loss: 459.62
 ---- batch: 060 ----
mean loss: 470.67
 ---- batch: 070 ----
mean loss: 463.27
 ---- batch: 080 ----
mean loss: 450.18
 ---- batch: 090 ----
mean loss: 455.09
 ---- batch: 100 ----
mean loss: 454.90
 ---- batch: 110 ----
mean loss: 462.08
train mean loss: 460.23
epoch train time: 0:00:02.134710
elapsed time: 0:02:10.664283
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 00:58:28.400525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.39
 ---- batch: 020 ----
mean loss: 461.20
 ---- batch: 030 ----
mean loss: 468.91
 ---- batch: 040 ----
mean loss: 469.26
 ---- batch: 050 ----
mean loss: 464.60
 ---- batch: 060 ----
mean loss: 454.84
 ---- batch: 070 ----
mean loss: 463.36
 ---- batch: 080 ----
mean loss: 461.28
 ---- batch: 090 ----
mean loss: 468.31
 ---- batch: 100 ----
mean loss: 451.24
 ---- batch: 110 ----
mean loss: 450.07
train mean loss: 460.68
epoch train time: 0:00:02.138021
elapsed time: 0:02:12.802478
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 00:58:30.538738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.41
 ---- batch: 020 ----
mean loss: 463.75
 ---- batch: 030 ----
mean loss: 464.75
 ---- batch: 040 ----
mean loss: 459.87
 ---- batch: 050 ----
mean loss: 463.40
 ---- batch: 060 ----
mean loss: 447.65
 ---- batch: 070 ----
mean loss: 473.56
 ---- batch: 080 ----
mean loss: 459.14
 ---- batch: 090 ----
mean loss: 445.86
 ---- batch: 100 ----
mean loss: 438.66
 ---- batch: 110 ----
mean loss: 445.56
train mean loss: 456.31
epoch train time: 0:00:02.130297
elapsed time: 0:02:14.932951
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 00:58:32.669191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.88
 ---- batch: 020 ----
mean loss: 459.41
 ---- batch: 030 ----
mean loss: 445.42
 ---- batch: 040 ----
mean loss: 463.03
 ---- batch: 050 ----
mean loss: 460.23
 ---- batch: 060 ----
mean loss: 446.49
 ---- batch: 070 ----
mean loss: 447.19
 ---- batch: 080 ----
mean loss: 442.44
 ---- batch: 090 ----
mean loss: 451.85
 ---- batch: 100 ----
mean loss: 437.94
 ---- batch: 110 ----
mean loss: 434.08
train mean loss: 449.55
epoch train time: 0:00:02.132898
elapsed time: 0:02:17.066009
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 00:58:34.802250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.16
 ---- batch: 020 ----
mean loss: 442.42
 ---- batch: 030 ----
mean loss: 450.29
 ---- batch: 040 ----
mean loss: 442.31
 ---- batch: 050 ----
mean loss: 433.07
 ---- batch: 060 ----
mean loss: 443.60
 ---- batch: 070 ----
mean loss: 461.91
 ---- batch: 080 ----
mean loss: 432.79
 ---- batch: 090 ----
mean loss: 443.47
 ---- batch: 100 ----
mean loss: 430.27
 ---- batch: 110 ----
mean loss: 452.24
train mean loss: 442.83
epoch train time: 0:00:02.137872
elapsed time: 0:02:19.204021
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 00:58:36.940261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.99
 ---- batch: 020 ----
mean loss: 457.11
 ---- batch: 030 ----
mean loss: 437.82
 ---- batch: 040 ----
mean loss: 432.71
 ---- batch: 050 ----
mean loss: 438.75
 ---- batch: 060 ----
mean loss: 445.09
 ---- batch: 070 ----
mean loss: 446.62
 ---- batch: 080 ----
mean loss: 428.51
 ---- batch: 090 ----
mean loss: 442.32
 ---- batch: 100 ----
mean loss: 453.84
 ---- batch: 110 ----
mean loss: 441.88
train mean loss: 441.51
epoch train time: 0:00:02.130449
elapsed time: 0:02:21.334621
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 00:58:39.070866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.09
 ---- batch: 020 ----
mean loss: 437.08
 ---- batch: 030 ----
mean loss: 441.60
 ---- batch: 040 ----
mean loss: 440.97
 ---- batch: 050 ----
mean loss: 446.74
 ---- batch: 060 ----
mean loss: 438.02
 ---- batch: 070 ----
mean loss: 432.92
 ---- batch: 080 ----
mean loss: 440.02
 ---- batch: 090 ----
mean loss: 431.83
 ---- batch: 100 ----
mean loss: 421.20
 ---- batch: 110 ----
mean loss: 431.63
train mean loss: 437.45
epoch train time: 0:00:02.130003
elapsed time: 0:02:23.464775
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 00:58:41.201018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.26
 ---- batch: 020 ----
mean loss: 449.74
 ---- batch: 030 ----
mean loss: 433.51
 ---- batch: 040 ----
mean loss: 440.53
 ---- batch: 050 ----
mean loss: 441.83
 ---- batch: 060 ----
mean loss: 431.97
 ---- batch: 070 ----
mean loss: 423.15
 ---- batch: 080 ----
mean loss: 420.57
 ---- batch: 090 ----
mean loss: 446.94
 ---- batch: 100 ----
mean loss: 448.25
 ---- batch: 110 ----
mean loss: 431.45
train mean loss: 436.64
epoch train time: 0:00:02.131087
elapsed time: 0:02:25.596006
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 00:58:43.332262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.89
 ---- batch: 020 ----
mean loss: 426.36
 ---- batch: 030 ----
mean loss: 434.06
 ---- batch: 040 ----
mean loss: 432.13
 ---- batch: 050 ----
mean loss: 427.31
 ---- batch: 060 ----
mean loss: 402.48
 ---- batch: 070 ----
mean loss: 414.90
 ---- batch: 080 ----
mean loss: 432.35
 ---- batch: 090 ----
mean loss: 420.87
 ---- batch: 100 ----
mean loss: 421.16
 ---- batch: 110 ----
mean loss: 434.15
train mean loss: 425.07
epoch train time: 0:00:02.133361
elapsed time: 0:02:27.729557
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 00:58:45.465828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.75
 ---- batch: 020 ----
mean loss: 422.94
 ---- batch: 030 ----
mean loss: 444.48
 ---- batch: 040 ----
mean loss: 432.77
 ---- batch: 050 ----
mean loss: 433.37
 ---- batch: 060 ----
mean loss: 417.69
 ---- batch: 070 ----
mean loss: 423.57
 ---- batch: 080 ----
mean loss: 417.14
 ---- batch: 090 ----
mean loss: 428.73
 ---- batch: 100 ----
mean loss: 419.93
 ---- batch: 110 ----
mean loss: 427.27
train mean loss: 427.47
epoch train time: 0:00:02.130304
elapsed time: 0:02:29.860028
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 00:58:47.596264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.69
 ---- batch: 020 ----
mean loss: 422.63
 ---- batch: 030 ----
mean loss: 454.92
 ---- batch: 040 ----
mean loss: 427.36
 ---- batch: 050 ----
mean loss: 429.58
 ---- batch: 060 ----
mean loss: 422.46
 ---- batch: 070 ----
mean loss: 422.23
 ---- batch: 080 ----
mean loss: 403.82
 ---- batch: 090 ----
mean loss: 424.21
 ---- batch: 100 ----
mean loss: 416.45
 ---- batch: 110 ----
mean loss: 417.00
train mean loss: 424.21
epoch train time: 0:00:02.130282
elapsed time: 0:02:31.990509
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 00:58:49.726748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.55
 ---- batch: 020 ----
mean loss: 413.43
 ---- batch: 030 ----
mean loss: 425.23
 ---- batch: 040 ----
mean loss: 407.38
 ---- batch: 050 ----
mean loss: 425.53
 ---- batch: 060 ----
mean loss: 414.33
 ---- batch: 070 ----
mean loss: 420.91
 ---- batch: 080 ----
mean loss: 413.83
 ---- batch: 090 ----
mean loss: 422.53
 ---- batch: 100 ----
mean loss: 409.90
 ---- batch: 110 ----
mean loss: 427.96
train mean loss: 418.12
epoch train time: 0:00:02.134618
elapsed time: 0:02:34.125286
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 00:58:51.861529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.74
 ---- batch: 020 ----
mean loss: 432.47
 ---- batch: 030 ----
mean loss: 418.73
 ---- batch: 040 ----
mean loss: 409.87
 ---- batch: 050 ----
mean loss: 417.54
 ---- batch: 060 ----
mean loss: 407.55
 ---- batch: 070 ----
mean loss: 410.73
 ---- batch: 080 ----
mean loss: 414.24
 ---- batch: 090 ----
mean loss: 420.73
 ---- batch: 100 ----
mean loss: 419.52
 ---- batch: 110 ----
mean loss: 419.14
train mean loss: 416.94
epoch train time: 0:00:02.135766
elapsed time: 0:02:36.261199
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 00:58:53.997439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.92
 ---- batch: 020 ----
mean loss: 410.26
 ---- batch: 030 ----
mean loss: 412.29
 ---- batch: 040 ----
mean loss: 410.10
 ---- batch: 050 ----
mean loss: 408.90
 ---- batch: 060 ----
mean loss: 419.30
 ---- batch: 070 ----
mean loss: 410.86
 ---- batch: 080 ----
mean loss: 417.49
 ---- batch: 090 ----
mean loss: 416.13
 ---- batch: 100 ----
mean loss: 404.62
 ---- batch: 110 ----
mean loss: 438.65
train mean loss: 414.97
epoch train time: 0:00:02.129452
elapsed time: 0:02:38.390830
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 00:58:56.127091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.02
 ---- batch: 020 ----
mean loss: 415.11
 ---- batch: 030 ----
mean loss: 412.22
 ---- batch: 040 ----
mean loss: 408.29
 ---- batch: 050 ----
mean loss: 412.03
 ---- batch: 060 ----
mean loss: 402.96
 ---- batch: 070 ----
mean loss: 407.52
 ---- batch: 080 ----
mean loss: 417.04
 ---- batch: 090 ----
mean loss: 422.35
 ---- batch: 100 ----
mean loss: 415.10
 ---- batch: 110 ----
mean loss: 417.34
train mean loss: 414.69
epoch train time: 0:00:02.132496
elapsed time: 0:02:40.523494
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 00:58:58.259758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.64
 ---- batch: 020 ----
mean loss: 405.64
 ---- batch: 030 ----
mean loss: 415.98
 ---- batch: 040 ----
mean loss: 416.44
 ---- batch: 050 ----
mean loss: 403.45
 ---- batch: 060 ----
mean loss: 397.40
 ---- batch: 070 ----
mean loss: 433.79
 ---- batch: 080 ----
mean loss: 409.18
 ---- batch: 090 ----
mean loss: 410.49
 ---- batch: 100 ----
mean loss: 409.74
 ---- batch: 110 ----
mean loss: 410.17
train mean loss: 410.59
epoch train time: 0:00:02.128852
elapsed time: 0:02:42.652507
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 00:59:00.388744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.79
 ---- batch: 020 ----
mean loss: 405.38
 ---- batch: 030 ----
mean loss: 401.02
 ---- batch: 040 ----
mean loss: 404.53
 ---- batch: 050 ----
mean loss: 408.22
 ---- batch: 060 ----
mean loss: 402.96
 ---- batch: 070 ----
mean loss: 393.21
 ---- batch: 080 ----
mean loss: 409.67
 ---- batch: 090 ----
mean loss: 412.47
 ---- batch: 100 ----
mean loss: 410.93
 ---- batch: 110 ----
mean loss: 404.60
train mean loss: 406.13
epoch train time: 0:00:02.131953
elapsed time: 0:02:44.784598
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 00:59:02.520842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.87
 ---- batch: 020 ----
mean loss: 395.98
 ---- batch: 030 ----
mean loss: 393.21
 ---- batch: 040 ----
mean loss: 398.99
 ---- batch: 050 ----
mean loss: 394.57
 ---- batch: 060 ----
mean loss: 399.48
 ---- batch: 070 ----
mean loss: 410.38
 ---- batch: 080 ----
mean loss: 401.62
 ---- batch: 090 ----
mean loss: 416.30
 ---- batch: 100 ----
mean loss: 412.16
 ---- batch: 110 ----
mean loss: 424.42
train mean loss: 405.32
epoch train time: 0:00:02.133912
elapsed time: 0:02:46.918654
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 00:59:04.654894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.97
 ---- batch: 020 ----
mean loss: 408.59
 ---- batch: 030 ----
mean loss: 409.67
 ---- batch: 040 ----
mean loss: 389.32
 ---- batch: 050 ----
mean loss: 395.68
 ---- batch: 060 ----
mean loss: 400.67
 ---- batch: 070 ----
mean loss: 404.61
 ---- batch: 080 ----
mean loss: 399.86
 ---- batch: 090 ----
mean loss: 391.27
 ---- batch: 100 ----
mean loss: 407.40
 ---- batch: 110 ----
mean loss: 383.14
train mean loss: 398.68
epoch train time: 0:00:02.132171
elapsed time: 0:02:49.050975
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 00:59:06.787222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.46
 ---- batch: 020 ----
mean loss: 404.98
 ---- batch: 030 ----
mean loss: 402.35
 ---- batch: 040 ----
mean loss: 414.35
 ---- batch: 050 ----
mean loss: 411.79
 ---- batch: 060 ----
mean loss: 394.83
 ---- batch: 070 ----
mean loss: 409.43
 ---- batch: 080 ----
mean loss: 400.37
 ---- batch: 090 ----
mean loss: 384.80
 ---- batch: 100 ----
mean loss: 400.43
 ---- batch: 110 ----
mean loss: 395.25
train mean loss: 402.44
epoch train time: 0:00:02.140022
elapsed time: 0:02:51.191166
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 00:59:08.927408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.42
 ---- batch: 020 ----
mean loss: 407.27
 ---- batch: 030 ----
mean loss: 401.47
 ---- batch: 040 ----
mean loss: 400.98
 ---- batch: 050 ----
mean loss: 393.94
 ---- batch: 060 ----
mean loss: 401.56
 ---- batch: 070 ----
mean loss: 392.81
 ---- batch: 080 ----
mean loss: 406.22
 ---- batch: 090 ----
mean loss: 386.43
 ---- batch: 100 ----
mean loss: 394.90
 ---- batch: 110 ----
mean loss: 390.70
train mean loss: 399.07
epoch train time: 0:00:02.134932
elapsed time: 0:02:53.326265
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 00:59:11.062541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.29
 ---- batch: 020 ----
mean loss: 388.82
 ---- batch: 030 ----
mean loss: 380.09
 ---- batch: 040 ----
mean loss: 395.43
 ---- batch: 050 ----
mean loss: 390.70
 ---- batch: 060 ----
mean loss: 396.76
 ---- batch: 070 ----
mean loss: 390.61
 ---- batch: 080 ----
mean loss: 407.03
 ---- batch: 090 ----
mean loss: 407.90
 ---- batch: 100 ----
mean loss: 397.65
 ---- batch: 110 ----
mean loss: 409.25
train mean loss: 397.50
epoch train time: 0:00:02.132761
elapsed time: 0:02:55.459270
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 00:59:13.195548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.53
 ---- batch: 020 ----
mean loss: 398.99
 ---- batch: 030 ----
mean loss: 393.24
 ---- batch: 040 ----
mean loss: 395.45
 ---- batch: 050 ----
mean loss: 402.87
 ---- batch: 060 ----
mean loss: 410.88
 ---- batch: 070 ----
mean loss: 396.55
 ---- batch: 080 ----
mean loss: 388.57
 ---- batch: 090 ----
mean loss: 386.20
 ---- batch: 100 ----
mean loss: 391.14
 ---- batch: 110 ----
mean loss: 395.85
train mean loss: 396.03
epoch train time: 0:00:02.132532
elapsed time: 0:02:57.591982
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 00:59:15.328223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.91
 ---- batch: 020 ----
mean loss: 402.76
 ---- batch: 030 ----
mean loss: 397.69
 ---- batch: 040 ----
mean loss: 394.67
 ---- batch: 050 ----
mean loss: 395.41
 ---- batch: 060 ----
mean loss: 398.51
 ---- batch: 070 ----
mean loss: 375.59
 ---- batch: 080 ----
mean loss: 399.96
 ---- batch: 090 ----
mean loss: 393.44
 ---- batch: 100 ----
mean loss: 382.36
 ---- batch: 110 ----
mean loss: 395.50
train mean loss: 394.64
epoch train time: 0:00:02.133936
elapsed time: 0:02:59.726078
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 00:59:17.462324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.61
 ---- batch: 020 ----
mean loss: 391.39
 ---- batch: 030 ----
mean loss: 393.97
 ---- batch: 040 ----
mean loss: 386.75
 ---- batch: 050 ----
mean loss: 386.49
 ---- batch: 060 ----
mean loss: 384.16
 ---- batch: 070 ----
mean loss: 393.11
 ---- batch: 080 ----
mean loss: 382.64
 ---- batch: 090 ----
mean loss: 388.45
 ---- batch: 100 ----
mean loss: 397.31
 ---- batch: 110 ----
mean loss: 404.72
train mean loss: 391.36
epoch train time: 0:00:02.136589
elapsed time: 0:03:01.862831
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 00:59:19.599102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.36
 ---- batch: 020 ----
mean loss: 383.53
 ---- batch: 030 ----
mean loss: 401.89
 ---- batch: 040 ----
mean loss: 380.74
 ---- batch: 050 ----
mean loss: 373.94
 ---- batch: 060 ----
mean loss: 392.85
 ---- batch: 070 ----
mean loss: 405.06
 ---- batch: 080 ----
mean loss: 401.89
 ---- batch: 090 ----
mean loss: 384.67
 ---- batch: 100 ----
mean loss: 393.89
 ---- batch: 110 ----
mean loss: 403.32
train mean loss: 392.12
epoch train time: 0:00:02.133707
elapsed time: 0:03:03.996708
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 00:59:21.732951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.98
 ---- batch: 020 ----
mean loss: 387.91
 ---- batch: 030 ----
mean loss: 389.40
 ---- batch: 040 ----
mean loss: 405.40
 ---- batch: 050 ----
mean loss: 403.07
 ---- batch: 060 ----
mean loss: 386.22
 ---- batch: 070 ----
mean loss: 388.11
 ---- batch: 080 ----
mean loss: 403.92
 ---- batch: 090 ----
mean loss: 387.82
 ---- batch: 100 ----
mean loss: 386.29
 ---- batch: 110 ----
mean loss: 386.79
train mean loss: 390.83
epoch train time: 0:00:02.132454
elapsed time: 0:03:06.129316
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 00:59:23.865564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.13
 ---- batch: 020 ----
mean loss: 389.67
 ---- batch: 030 ----
mean loss: 385.28
 ---- batch: 040 ----
mean loss: 371.53
 ---- batch: 050 ----
mean loss: 390.21
 ---- batch: 060 ----
mean loss: 400.74
 ---- batch: 070 ----
mean loss: 383.86
 ---- batch: 080 ----
mean loss: 392.36
 ---- batch: 090 ----
mean loss: 394.39
 ---- batch: 100 ----
mean loss: 389.75
 ---- batch: 110 ----
mean loss: 385.12
train mean loss: 388.71
epoch train time: 0:00:02.137053
elapsed time: 0:03:08.266522
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 00:59:26.002782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.31
 ---- batch: 020 ----
mean loss: 377.14
 ---- batch: 030 ----
mean loss: 380.72
 ---- batch: 040 ----
mean loss: 382.35
 ---- batch: 050 ----
mean loss: 376.86
 ---- batch: 060 ----
mean loss: 402.88
 ---- batch: 070 ----
mean loss: 395.00
 ---- batch: 080 ----
mean loss: 396.96
 ---- batch: 090 ----
mean loss: 393.22
 ---- batch: 100 ----
mean loss: 375.69
 ---- batch: 110 ----
mean loss: 385.86
train mean loss: 387.49
epoch train time: 0:00:02.134642
elapsed time: 0:03:10.401328
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 00:59:28.137568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.65
 ---- batch: 020 ----
mean loss: 381.41
 ---- batch: 030 ----
mean loss: 392.64
 ---- batch: 040 ----
mean loss: 385.31
 ---- batch: 050 ----
mean loss: 395.73
 ---- batch: 060 ----
mean loss: 380.43
 ---- batch: 070 ----
mean loss: 373.57
 ---- batch: 080 ----
mean loss: 391.58
 ---- batch: 090 ----
mean loss: 384.64
 ---- batch: 100 ----
mean loss: 387.39
 ---- batch: 110 ----
mean loss: 387.15
train mean loss: 385.07
epoch train time: 0:00:02.131408
elapsed time: 0:03:12.532889
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 00:59:30.269132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.86
 ---- batch: 020 ----
mean loss: 383.90
 ---- batch: 030 ----
mean loss: 384.31
 ---- batch: 040 ----
mean loss: 389.10
 ---- batch: 050 ----
mean loss: 379.74
 ---- batch: 060 ----
mean loss: 391.95
 ---- batch: 070 ----
mean loss: 381.54
 ---- batch: 080 ----
mean loss: 378.20
 ---- batch: 090 ----
mean loss: 381.08
 ---- batch: 100 ----
mean loss: 376.00
 ---- batch: 110 ----
mean loss: 387.65
train mean loss: 382.16
epoch train time: 0:00:02.133893
elapsed time: 0:03:14.666941
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 00:59:32.403213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.25
 ---- batch: 020 ----
mean loss: 384.49
 ---- batch: 030 ----
mean loss: 373.53
 ---- batch: 040 ----
mean loss: 384.50
 ---- batch: 050 ----
mean loss: 371.07
 ---- batch: 060 ----
mean loss: 380.82
 ---- batch: 070 ----
mean loss: 376.14
 ---- batch: 080 ----
mean loss: 379.42
 ---- batch: 090 ----
mean loss: 384.57
 ---- batch: 100 ----
mean loss: 388.07
 ---- batch: 110 ----
mean loss: 368.44
train mean loss: 380.51
epoch train time: 0:00:02.133134
elapsed time: 0:03:16.800279
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 00:59:34.536529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.99
 ---- batch: 020 ----
mean loss: 379.91
 ---- batch: 030 ----
mean loss: 381.02
 ---- batch: 040 ----
mean loss: 397.87
 ---- batch: 050 ----
mean loss: 401.22
 ---- batch: 060 ----
mean loss: 406.82
 ---- batch: 070 ----
mean loss: 399.09
 ---- batch: 080 ----
mean loss: 389.41
 ---- batch: 090 ----
mean loss: 376.32
 ---- batch: 100 ----
mean loss: 379.77
 ---- batch: 110 ----
mean loss: 371.48
train mean loss: 387.40
epoch train time: 0:00:02.131861
elapsed time: 0:03:18.932287
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 00:59:36.668528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.06
 ---- batch: 020 ----
mean loss: 372.31
 ---- batch: 030 ----
mean loss: 379.39
 ---- batch: 040 ----
mean loss: 387.87
 ---- batch: 050 ----
mean loss: 384.78
 ---- batch: 060 ----
mean loss: 388.52
 ---- batch: 070 ----
mean loss: 385.64
 ---- batch: 080 ----
mean loss: 370.17
 ---- batch: 090 ----
mean loss: 395.95
 ---- batch: 100 ----
mean loss: 371.24
 ---- batch: 110 ----
mean loss: 389.22
train mean loss: 381.99
epoch train time: 0:00:02.144099
elapsed time: 0:03:21.076529
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 00:59:38.812771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.75
 ---- batch: 020 ----
mean loss: 381.23
 ---- batch: 030 ----
mean loss: 367.39
 ---- batch: 040 ----
mean loss: 375.47
 ---- batch: 050 ----
mean loss: 383.24
 ---- batch: 060 ----
mean loss: 378.25
 ---- batch: 070 ----
mean loss: 381.17
 ---- batch: 080 ----
mean loss: 382.44
 ---- batch: 090 ----
mean loss: 374.20
 ---- batch: 100 ----
mean loss: 374.93
 ---- batch: 110 ----
mean loss: 370.58
train mean loss: 375.79
epoch train time: 0:00:02.141630
elapsed time: 0:03:23.218338
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 00:59:40.954636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.51
 ---- batch: 020 ----
mean loss: 369.17
 ---- batch: 030 ----
mean loss: 372.92
 ---- batch: 040 ----
mean loss: 372.83
 ---- batch: 050 ----
mean loss: 369.99
 ---- batch: 060 ----
mean loss: 378.66
 ---- batch: 070 ----
mean loss: 374.26
 ---- batch: 080 ----
mean loss: 386.66
 ---- batch: 090 ----
mean loss: 375.50
 ---- batch: 100 ----
mean loss: 367.51
 ---- batch: 110 ----
mean loss: 392.20
train mean loss: 375.85
epoch train time: 0:00:02.149150
elapsed time: 0:03:25.367685
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 00:59:43.103923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.53
 ---- batch: 020 ----
mean loss: 385.31
 ---- batch: 030 ----
mean loss: 376.83
 ---- batch: 040 ----
mean loss: 374.08
 ---- batch: 050 ----
mean loss: 362.72
 ---- batch: 060 ----
mean loss: 371.08
 ---- batch: 070 ----
mean loss: 381.28
 ---- batch: 080 ----
mean loss: 372.74
 ---- batch: 090 ----
mean loss: 372.93
 ---- batch: 100 ----
mean loss: 376.67
 ---- batch: 110 ----
mean loss: 362.46
train mean loss: 374.05
epoch train time: 0:00:02.145525
elapsed time: 0:03:27.513371
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 00:59:45.249632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.00
 ---- batch: 020 ----
mean loss: 371.07
 ---- batch: 030 ----
mean loss: 377.34
 ---- batch: 040 ----
mean loss: 364.71
 ---- batch: 050 ----
mean loss: 376.59
 ---- batch: 060 ----
mean loss: 380.57
 ---- batch: 070 ----
mean loss: 388.52
 ---- batch: 080 ----
mean loss: 373.85
 ---- batch: 090 ----
mean loss: 385.99
 ---- batch: 100 ----
mean loss: 375.67
 ---- batch: 110 ----
mean loss: 383.27
train mean loss: 378.44
epoch train time: 0:00:02.145718
elapsed time: 0:03:29.659263
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 00:59:47.395508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.77
 ---- batch: 020 ----
mean loss: 369.38
 ---- batch: 030 ----
mean loss: 360.00
 ---- batch: 040 ----
mean loss: 378.35
 ---- batch: 050 ----
mean loss: 366.47
 ---- batch: 060 ----
mean loss: 370.49
 ---- batch: 070 ----
mean loss: 374.03
 ---- batch: 080 ----
mean loss: 374.28
 ---- batch: 090 ----
mean loss: 369.39
 ---- batch: 100 ----
mean loss: 375.26
 ---- batch: 110 ----
mean loss: 387.74
train mean loss: 372.11
epoch train time: 0:00:02.132948
elapsed time: 0:03:31.792354
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 00:59:49.528597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.73
 ---- batch: 020 ----
mean loss: 397.34
 ---- batch: 030 ----
mean loss: 368.87
 ---- batch: 040 ----
mean loss: 372.73
 ---- batch: 050 ----
mean loss: 377.61
 ---- batch: 060 ----
mean loss: 366.95
 ---- batch: 070 ----
mean loss: 364.53
 ---- batch: 080 ----
mean loss: 382.21
 ---- batch: 090 ----
mean loss: 368.81
 ---- batch: 100 ----
mean loss: 370.41
 ---- batch: 110 ----
mean loss: 367.78
train mean loss: 372.08
epoch train time: 0:00:02.132250
elapsed time: 0:03:33.924745
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 00:59:51.660988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.84
 ---- batch: 020 ----
mean loss: 380.96
 ---- batch: 030 ----
mean loss: 361.88
 ---- batch: 040 ----
mean loss: 373.76
 ---- batch: 050 ----
mean loss: 367.28
 ---- batch: 060 ----
mean loss: 363.70
 ---- batch: 070 ----
mean loss: 363.37
 ---- batch: 080 ----
mean loss: 361.39
 ---- batch: 090 ----
mean loss: 382.61
 ---- batch: 100 ----
mean loss: 371.26
 ---- batch: 110 ----
mean loss: 367.95
train mean loss: 367.88
epoch train time: 0:00:02.138177
elapsed time: 0:03:36.063072
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 00:59:53.799334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.85
 ---- batch: 020 ----
mean loss: 366.22
 ---- batch: 030 ----
mean loss: 377.60
 ---- batch: 040 ----
mean loss: 364.43
 ---- batch: 050 ----
mean loss: 376.65
 ---- batch: 060 ----
mean loss: 370.66
 ---- batch: 070 ----
mean loss: 359.58
 ---- batch: 080 ----
mean loss: 378.50
 ---- batch: 090 ----
mean loss: 372.66
 ---- batch: 100 ----
mean loss: 354.58
 ---- batch: 110 ----
mean loss: 372.16
train mean loss: 369.56
epoch train time: 0:00:02.132678
elapsed time: 0:03:38.195912
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 00:59:55.932151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.42
 ---- batch: 020 ----
mean loss: 365.83
 ---- batch: 030 ----
mean loss: 371.40
 ---- batch: 040 ----
mean loss: 368.48
 ---- batch: 050 ----
mean loss: 364.17
 ---- batch: 060 ----
mean loss: 356.45
 ---- batch: 070 ----
mean loss: 372.58
 ---- batch: 080 ----
mean loss: 364.38
 ---- batch: 090 ----
mean loss: 373.23
 ---- batch: 100 ----
mean loss: 369.90
 ---- batch: 110 ----
mean loss: 373.94
train mean loss: 367.32
epoch train time: 0:00:02.133534
elapsed time: 0:03:40.329599
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 00:59:58.065893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.54
 ---- batch: 020 ----
mean loss: 372.92
 ---- batch: 030 ----
mean loss: 370.79
 ---- batch: 040 ----
mean loss: 362.26
 ---- batch: 050 ----
mean loss: 359.43
 ---- batch: 060 ----
mean loss: 364.81
 ---- batch: 070 ----
mean loss: 364.02
 ---- batch: 080 ----
mean loss: 369.68
 ---- batch: 090 ----
mean loss: 357.39
 ---- batch: 100 ----
mean loss: 368.01
 ---- batch: 110 ----
mean loss: 366.27
train mean loss: 366.33
epoch train time: 0:00:02.138459
elapsed time: 0:03:42.468259
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 01:00:00.204499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.35
 ---- batch: 020 ----
mean loss: 368.52
 ---- batch: 030 ----
mean loss: 363.75
 ---- batch: 040 ----
mean loss: 365.86
 ---- batch: 050 ----
mean loss: 358.51
 ---- batch: 060 ----
mean loss: 379.66
 ---- batch: 070 ----
mean loss: 370.07
 ---- batch: 080 ----
mean loss: 379.45
 ---- batch: 090 ----
mean loss: 370.80
 ---- batch: 100 ----
mean loss: 381.67
 ---- batch: 110 ----
mean loss: 369.88
train mean loss: 369.96
epoch train time: 0:00:02.132828
elapsed time: 0:03:44.601229
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 01:00:02.337471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.80
 ---- batch: 020 ----
mean loss: 373.79
 ---- batch: 030 ----
mean loss: 364.19
 ---- batch: 040 ----
mean loss: 351.77
 ---- batch: 050 ----
mean loss: 364.96
 ---- batch: 060 ----
mean loss: 372.44
 ---- batch: 070 ----
mean loss: 370.96
 ---- batch: 080 ----
mean loss: 366.13
 ---- batch: 090 ----
mean loss: 369.96
 ---- batch: 100 ----
mean loss: 359.83
 ---- batch: 110 ----
mean loss: 366.28
train mean loss: 366.21
epoch train time: 0:00:02.131935
elapsed time: 0:03:46.733334
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 01:00:04.469573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.24
 ---- batch: 020 ----
mean loss: 367.46
 ---- batch: 030 ----
mean loss: 366.73
 ---- batch: 040 ----
mean loss: 370.13
 ---- batch: 050 ----
mean loss: 358.05
 ---- batch: 060 ----
mean loss: 364.23
 ---- batch: 070 ----
mean loss: 371.34
 ---- batch: 080 ----
mean loss: 356.46
 ---- batch: 090 ----
mean loss: 342.01
 ---- batch: 100 ----
mean loss: 359.75
 ---- batch: 110 ----
mean loss: 361.19
train mean loss: 363.40
epoch train time: 0:00:02.130937
elapsed time: 0:03:48.864428
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 01:00:06.600704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.12
 ---- batch: 020 ----
mean loss: 370.48
 ---- batch: 030 ----
mean loss: 362.96
 ---- batch: 040 ----
mean loss: 371.62
 ---- batch: 050 ----
mean loss: 351.99
 ---- batch: 060 ----
mean loss: 355.78
 ---- batch: 070 ----
mean loss: 352.37
 ---- batch: 080 ----
mean loss: 355.46
 ---- batch: 090 ----
mean loss: 371.52
 ---- batch: 100 ----
mean loss: 371.57
 ---- batch: 110 ----
mean loss: 356.82
train mean loss: 361.47
epoch train time: 0:00:02.133576
elapsed time: 0:03:50.998200
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 01:00:08.734467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.50
 ---- batch: 020 ----
mean loss: 369.13
 ---- batch: 030 ----
mean loss: 366.37
 ---- batch: 040 ----
mean loss: 359.36
 ---- batch: 050 ----
mean loss: 360.47
 ---- batch: 060 ----
mean loss: 351.82
 ---- batch: 070 ----
mean loss: 374.40
 ---- batch: 080 ----
mean loss: 358.53
 ---- batch: 090 ----
mean loss: 357.80
 ---- batch: 100 ----
mean loss: 358.76
 ---- batch: 110 ----
mean loss: 365.87
train mean loss: 362.22
epoch train time: 0:00:02.135468
elapsed time: 0:03:53.133840
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 01:00:10.870085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.37
 ---- batch: 020 ----
mean loss: 362.20
 ---- batch: 030 ----
mean loss: 361.66
 ---- batch: 040 ----
mean loss: 358.84
 ---- batch: 050 ----
mean loss: 355.71
 ---- batch: 060 ----
mean loss: 355.11
 ---- batch: 070 ----
mean loss: 374.77
 ---- batch: 080 ----
mean loss: 355.21
 ---- batch: 090 ----
mean loss: 365.58
 ---- batch: 100 ----
mean loss: 356.45
 ---- batch: 110 ----
mean loss: 362.99
train mean loss: 361.07
epoch train time: 0:00:02.132748
elapsed time: 0:03:55.266744
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 01:00:13.002986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.18
 ---- batch: 020 ----
mean loss: 361.30
 ---- batch: 030 ----
mean loss: 347.86
 ---- batch: 040 ----
mean loss: 357.88
 ---- batch: 050 ----
mean loss: 367.49
 ---- batch: 060 ----
mean loss: 344.35
 ---- batch: 070 ----
mean loss: 351.89
 ---- batch: 080 ----
mean loss: 355.57
 ---- batch: 090 ----
mean loss: 359.95
 ---- batch: 100 ----
mean loss: 349.45
 ---- batch: 110 ----
mean loss: 355.74
train mean loss: 355.96
epoch train time: 0:00:02.135191
elapsed time: 0:03:57.402117
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 01:00:15.138359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.14
 ---- batch: 020 ----
mean loss: 354.67
 ---- batch: 030 ----
mean loss: 345.75
 ---- batch: 040 ----
mean loss: 360.01
 ---- batch: 050 ----
mean loss: 366.54
 ---- batch: 060 ----
mean loss: 357.39
 ---- batch: 070 ----
mean loss: 355.83
 ---- batch: 080 ----
mean loss: 347.21
 ---- batch: 090 ----
mean loss: 345.07
 ---- batch: 100 ----
mean loss: 352.84
 ---- batch: 110 ----
mean loss: 362.89
train mean loss: 356.09
epoch train time: 0:00:02.135602
elapsed time: 0:03:59.537866
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 01:00:17.274110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.98
 ---- batch: 020 ----
mean loss: 350.15
 ---- batch: 030 ----
mean loss: 356.35
 ---- batch: 040 ----
mean loss: 350.59
 ---- batch: 050 ----
mean loss: 353.26
 ---- batch: 060 ----
mean loss: 358.56
 ---- batch: 070 ----
mean loss: 356.46
 ---- batch: 080 ----
mean loss: 365.60
 ---- batch: 090 ----
mean loss: 358.63
 ---- batch: 100 ----
mean loss: 362.18
 ---- batch: 110 ----
mean loss: 357.93
train mean loss: 356.02
epoch train time: 0:00:02.133050
elapsed time: 0:04:01.671081
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 01:00:19.407354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.05
 ---- batch: 020 ----
mean loss: 364.32
 ---- batch: 030 ----
mean loss: 365.78
 ---- batch: 040 ----
mean loss: 350.52
 ---- batch: 050 ----
mean loss: 350.70
 ---- batch: 060 ----
mean loss: 366.42
 ---- batch: 070 ----
mean loss: 346.61
 ---- batch: 080 ----
mean loss: 358.11
 ---- batch: 090 ----
mean loss: 355.93
 ---- batch: 100 ----
mean loss: 350.29
 ---- batch: 110 ----
mean loss: 352.39
train mean loss: 356.18
epoch train time: 0:00:02.130482
elapsed time: 0:04:03.801744
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 01:00:21.538002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.67
 ---- batch: 020 ----
mean loss: 359.31
 ---- batch: 030 ----
mean loss: 349.39
 ---- batch: 040 ----
mean loss: 356.88
 ---- batch: 050 ----
mean loss: 342.39
 ---- batch: 060 ----
mean loss: 355.07
 ---- batch: 070 ----
mean loss: 356.78
 ---- batch: 080 ----
mean loss: 351.22
 ---- batch: 090 ----
mean loss: 357.09
 ---- batch: 100 ----
mean loss: 359.38
 ---- batch: 110 ----
mean loss: 368.34
train mean loss: 355.77
epoch train time: 0:00:02.131969
elapsed time: 0:04:05.933870
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 01:00:23.670110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.84
 ---- batch: 020 ----
mean loss: 354.38
 ---- batch: 030 ----
mean loss: 357.29
 ---- batch: 040 ----
mean loss: 354.42
 ---- batch: 050 ----
mean loss: 350.00
 ---- batch: 060 ----
mean loss: 356.15
 ---- batch: 070 ----
mean loss: 345.84
 ---- batch: 080 ----
mean loss: 356.69
 ---- batch: 090 ----
mean loss: 353.72
 ---- batch: 100 ----
mean loss: 360.37
 ---- batch: 110 ----
mean loss: 358.26
train mean loss: 354.93
epoch train time: 0:00:02.133695
elapsed time: 0:04:08.067714
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 01:00:25.803958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.52
 ---- batch: 020 ----
mean loss: 360.66
 ---- batch: 030 ----
mean loss: 366.50
 ---- batch: 040 ----
mean loss: 348.75
 ---- batch: 050 ----
mean loss: 357.07
 ---- batch: 060 ----
mean loss: 347.44
 ---- batch: 070 ----
mean loss: 347.85
 ---- batch: 080 ----
mean loss: 352.57
 ---- batch: 090 ----
mean loss: 344.92
 ---- batch: 100 ----
mean loss: 342.66
 ---- batch: 110 ----
mean loss: 354.07
train mean loss: 353.14
epoch train time: 0:00:02.129906
elapsed time: 0:04:10.197767
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 01:00:27.934008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.12
 ---- batch: 020 ----
mean loss: 341.90
 ---- batch: 030 ----
mean loss: 360.59
 ---- batch: 040 ----
mean loss: 353.83
 ---- batch: 050 ----
mean loss: 339.35
 ---- batch: 060 ----
mean loss: 366.09
 ---- batch: 070 ----
mean loss: 340.21
 ---- batch: 080 ----
mean loss: 373.27
 ---- batch: 090 ----
mean loss: 353.25
 ---- batch: 100 ----
mean loss: 348.05
 ---- batch: 110 ----
mean loss: 343.35
train mean loss: 351.65
epoch train time: 0:00:02.134872
elapsed time: 0:04:12.332827
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 01:00:30.069063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.40
 ---- batch: 020 ----
mean loss: 354.90
 ---- batch: 030 ----
mean loss: 357.97
 ---- batch: 040 ----
mean loss: 354.73
 ---- batch: 050 ----
mean loss: 352.36
 ---- batch: 060 ----
mean loss: 360.82
 ---- batch: 070 ----
mean loss: 348.54
 ---- batch: 080 ----
mean loss: 342.77
 ---- batch: 090 ----
mean loss: 341.93
 ---- batch: 100 ----
mean loss: 351.14
 ---- batch: 110 ----
mean loss: 358.75
train mean loss: 353.30
epoch train time: 0:00:02.134814
elapsed time: 0:04:14.467780
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 01:00:32.204022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.38
 ---- batch: 020 ----
mean loss: 351.27
 ---- batch: 030 ----
mean loss: 355.12
 ---- batch: 040 ----
mean loss: 353.07
 ---- batch: 050 ----
mean loss: 351.00
 ---- batch: 060 ----
mean loss: 348.93
 ---- batch: 070 ----
mean loss: 357.31
 ---- batch: 080 ----
mean loss: 360.69
 ---- batch: 090 ----
mean loss: 356.02
 ---- batch: 100 ----
mean loss: 356.31
 ---- batch: 110 ----
mean loss: 356.49
train mean loss: 353.50
epoch train time: 0:00:02.131349
elapsed time: 0:04:16.599286
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 01:00:34.335532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.99
 ---- batch: 020 ----
mean loss: 345.31
 ---- batch: 030 ----
mean loss: 350.83
 ---- batch: 040 ----
mean loss: 333.68
 ---- batch: 050 ----
mean loss: 343.33
 ---- batch: 060 ----
mean loss: 357.94
 ---- batch: 070 ----
mean loss: 350.15
 ---- batch: 080 ----
mean loss: 355.93
 ---- batch: 090 ----
mean loss: 345.47
 ---- batch: 100 ----
mean loss: 354.75
 ---- batch: 110 ----
mean loss: 358.34
train mean loss: 348.76
epoch train time: 0:00:02.133995
elapsed time: 0:04:18.733493
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 01:00:36.469742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.84
 ---- batch: 020 ----
mean loss: 360.85
 ---- batch: 030 ----
mean loss: 348.75
 ---- batch: 040 ----
mean loss: 356.85
 ---- batch: 050 ----
mean loss: 358.67
 ---- batch: 060 ----
mean loss: 353.18
 ---- batch: 070 ----
mean loss: 348.01
 ---- batch: 080 ----
mean loss: 357.01
 ---- batch: 090 ----
mean loss: 356.38
 ---- batch: 100 ----
mean loss: 334.95
 ---- batch: 110 ----
mean loss: 350.27
train mean loss: 352.13
epoch train time: 0:00:02.134255
elapsed time: 0:04:20.867896
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 01:00:38.604135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.19
 ---- batch: 020 ----
mean loss: 350.49
 ---- batch: 030 ----
mean loss: 341.41
 ---- batch: 040 ----
mean loss: 351.07
 ---- batch: 050 ----
mean loss: 352.38
 ---- batch: 060 ----
mean loss: 345.43
 ---- batch: 070 ----
mean loss: 347.01
 ---- batch: 080 ----
mean loss: 347.83
 ---- batch: 090 ----
mean loss: 346.13
 ---- batch: 100 ----
mean loss: 332.20
 ---- batch: 110 ----
mean loss: 332.04
train mean loss: 345.03
epoch train time: 0:00:02.133965
elapsed time: 0:04:23.002116
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 01:00:40.738384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.17
 ---- batch: 020 ----
mean loss: 351.07
 ---- batch: 030 ----
mean loss: 332.17
 ---- batch: 040 ----
mean loss: 343.35
 ---- batch: 050 ----
mean loss: 367.50
 ---- batch: 060 ----
mean loss: 340.44
 ---- batch: 070 ----
mean loss: 345.15
 ---- batch: 080 ----
mean loss: 349.77
 ---- batch: 090 ----
mean loss: 348.54
 ---- batch: 100 ----
mean loss: 361.21
 ---- batch: 110 ----
mean loss: 346.33
train mean loss: 348.08
epoch train time: 0:00:02.133589
elapsed time: 0:04:25.135910
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 01:00:42.872153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.44
 ---- batch: 020 ----
mean loss: 331.04
 ---- batch: 030 ----
mean loss: 343.90
 ---- batch: 040 ----
mean loss: 335.73
 ---- batch: 050 ----
mean loss: 357.39
 ---- batch: 060 ----
mean loss: 341.78
 ---- batch: 070 ----
mean loss: 350.47
 ---- batch: 080 ----
mean loss: 352.38
 ---- batch: 090 ----
mean loss: 342.95
 ---- batch: 100 ----
mean loss: 342.45
 ---- batch: 110 ----
mean loss: 337.90
train mean loss: 343.37
epoch train time: 0:00:02.130169
elapsed time: 0:04:27.266231
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 01:00:45.002473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.55
 ---- batch: 020 ----
mean loss: 331.73
 ---- batch: 030 ----
mean loss: 344.76
 ---- batch: 040 ----
mean loss: 338.68
 ---- batch: 050 ----
mean loss: 357.75
 ---- batch: 060 ----
mean loss: 347.53
 ---- batch: 070 ----
mean loss: 340.32
 ---- batch: 080 ----
mean loss: 350.64
 ---- batch: 090 ----
mean loss: 353.05
 ---- batch: 100 ----
mean loss: 347.02
 ---- batch: 110 ----
mean loss: 336.53
train mean loss: 344.61
epoch train time: 0:00:02.134996
elapsed time: 0:04:29.401368
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 01:00:47.137628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.90
 ---- batch: 020 ----
mean loss: 336.65
 ---- batch: 030 ----
mean loss: 356.10
 ---- batch: 040 ----
mean loss: 348.18
 ---- batch: 050 ----
mean loss: 345.11
 ---- batch: 060 ----
mean loss: 353.76
 ---- batch: 070 ----
mean loss: 345.41
 ---- batch: 080 ----
mean loss: 352.60
 ---- batch: 090 ----
mean loss: 365.51
 ---- batch: 100 ----
mean loss: 348.89
 ---- batch: 110 ----
mean loss: 351.66
train mean loss: 349.82
epoch train time: 0:00:02.130846
elapsed time: 0:04:31.532376
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 01:00:49.268618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.78
 ---- batch: 020 ----
mean loss: 338.92
 ---- batch: 030 ----
mean loss: 345.83
 ---- batch: 040 ----
mean loss: 339.51
 ---- batch: 050 ----
mean loss: 345.77
 ---- batch: 060 ----
mean loss: 339.31
 ---- batch: 070 ----
mean loss: 346.32
 ---- batch: 080 ----
mean loss: 358.67
 ---- batch: 090 ----
mean loss: 345.76
 ---- batch: 100 ----
mean loss: 339.57
 ---- batch: 110 ----
mean loss: 341.32
train mean loss: 344.58
epoch train time: 0:00:02.134146
elapsed time: 0:04:33.666697
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 01:00:51.402964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.80
 ---- batch: 020 ----
mean loss: 345.29
 ---- batch: 030 ----
mean loss: 337.86
 ---- batch: 040 ----
mean loss: 332.70
 ---- batch: 050 ----
mean loss: 343.24
 ---- batch: 060 ----
mean loss: 353.06
 ---- batch: 070 ----
mean loss: 337.60
 ---- batch: 080 ----
mean loss: 330.56
 ---- batch: 090 ----
mean loss: 344.10
 ---- batch: 100 ----
mean loss: 338.15
 ---- batch: 110 ----
mean loss: 345.29
train mean loss: 341.88
epoch train time: 0:00:02.131539
elapsed time: 0:04:35.798404
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 01:00:53.534693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.83
 ---- batch: 020 ----
mean loss: 361.45
 ---- batch: 030 ----
mean loss: 341.25
 ---- batch: 040 ----
mean loss: 340.39
 ---- batch: 050 ----
mean loss: 341.81
 ---- batch: 060 ----
mean loss: 342.34
 ---- batch: 070 ----
mean loss: 343.16
 ---- batch: 080 ----
mean loss: 354.83
 ---- batch: 090 ----
mean loss: 346.96
 ---- batch: 100 ----
mean loss: 335.12
 ---- batch: 110 ----
mean loss: 342.11
train mean loss: 345.44
epoch train time: 0:00:02.135391
elapsed time: 0:04:37.934003
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 01:00:55.670293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.70
 ---- batch: 020 ----
mean loss: 344.90
 ---- batch: 030 ----
mean loss: 331.21
 ---- batch: 040 ----
mean loss: 336.27
 ---- batch: 050 ----
mean loss: 342.04
 ---- batch: 060 ----
mean loss: 342.05
 ---- batch: 070 ----
mean loss: 351.45
 ---- batch: 080 ----
mean loss: 342.70
 ---- batch: 090 ----
mean loss: 342.68
 ---- batch: 100 ----
mean loss: 344.90
 ---- batch: 110 ----
mean loss: 338.40
train mean loss: 342.87
epoch train time: 0:00:02.134990
elapsed time: 0:04:40.069183
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 01:00:57.805423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.20
 ---- batch: 020 ----
mean loss: 346.14
 ---- batch: 030 ----
mean loss: 341.41
 ---- batch: 040 ----
mean loss: 334.98
 ---- batch: 050 ----
mean loss: 340.93
 ---- batch: 060 ----
mean loss: 338.10
 ---- batch: 070 ----
mean loss: 334.92
 ---- batch: 080 ----
mean loss: 348.26
 ---- batch: 090 ----
mean loss: 334.31
 ---- batch: 100 ----
mean loss: 341.86
 ---- batch: 110 ----
mean loss: 342.69
train mean loss: 341.30
epoch train time: 0:00:02.131022
elapsed time: 0:04:42.200345
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 01:00:59.936584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.80
 ---- batch: 020 ----
mean loss: 349.89
 ---- batch: 030 ----
mean loss: 332.64
 ---- batch: 040 ----
mean loss: 344.71
 ---- batch: 050 ----
mean loss: 346.21
 ---- batch: 060 ----
mean loss: 348.62
 ---- batch: 070 ----
mean loss: 332.58
 ---- batch: 080 ----
mean loss: 359.01
 ---- batch: 090 ----
mean loss: 345.80
 ---- batch: 100 ----
mean loss: 339.14
 ---- batch: 110 ----
mean loss: 339.99
train mean loss: 342.53
epoch train time: 0:00:02.132416
elapsed time: 0:04:44.332930
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 01:01:02.069172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.22
 ---- batch: 020 ----
mean loss: 342.44
 ---- batch: 030 ----
mean loss: 352.26
 ---- batch: 040 ----
mean loss: 337.27
 ---- batch: 050 ----
mean loss: 342.37
 ---- batch: 060 ----
mean loss: 344.68
 ---- batch: 070 ----
mean loss: 342.00
 ---- batch: 080 ----
mean loss: 345.06
 ---- batch: 090 ----
mean loss: 337.56
 ---- batch: 100 ----
mean loss: 336.38
 ---- batch: 110 ----
mean loss: 343.57
train mean loss: 341.76
epoch train time: 0:00:02.134550
elapsed time: 0:04:46.467624
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 01:01:04.203883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.10
 ---- batch: 020 ----
mean loss: 341.70
 ---- batch: 030 ----
mean loss: 357.72
 ---- batch: 040 ----
mean loss: 344.58
 ---- batch: 050 ----
mean loss: 355.55
 ---- batch: 060 ----
mean loss: 342.90
 ---- batch: 070 ----
mean loss: 342.62
 ---- batch: 080 ----
mean loss: 332.99
 ---- batch: 090 ----
mean loss: 343.65
 ---- batch: 100 ----
mean loss: 351.86
 ---- batch: 110 ----
mean loss: 342.09
train mean loss: 345.74
epoch train time: 0:00:02.136448
elapsed time: 0:04:48.604233
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 01:01:06.340506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.70
 ---- batch: 020 ----
mean loss: 337.50
 ---- batch: 030 ----
mean loss: 338.11
 ---- batch: 040 ----
mean loss: 338.13
 ---- batch: 050 ----
mean loss: 329.25
 ---- batch: 060 ----
mean loss: 344.69
 ---- batch: 070 ----
mean loss: 335.96
 ---- batch: 080 ----
mean loss: 334.83
 ---- batch: 090 ----
mean loss: 340.23
 ---- batch: 100 ----
mean loss: 340.89
 ---- batch: 110 ----
mean loss: 345.33
train mean loss: 338.12
epoch train time: 0:00:02.132785
elapsed time: 0:04:50.737190
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 01:01:08.473448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.88
 ---- batch: 020 ----
mean loss: 348.18
 ---- batch: 030 ----
mean loss: 336.27
 ---- batch: 040 ----
mean loss: 341.54
 ---- batch: 050 ----
mean loss: 334.22
 ---- batch: 060 ----
mean loss: 346.94
 ---- batch: 070 ----
mean loss: 343.90
 ---- batch: 080 ----
mean loss: 345.27
 ---- batch: 090 ----
mean loss: 327.88
 ---- batch: 100 ----
mean loss: 348.87
 ---- batch: 110 ----
mean loss: 333.73
train mean loss: 340.57
epoch train time: 0:00:02.133004
elapsed time: 0:04:52.870355
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 01:01:10.606594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.67
 ---- batch: 020 ----
mean loss: 336.15
 ---- batch: 030 ----
mean loss: 331.81
 ---- batch: 040 ----
mean loss: 338.03
 ---- batch: 050 ----
mean loss: 339.10
 ---- batch: 060 ----
mean loss: 340.91
 ---- batch: 070 ----
mean loss: 342.05
 ---- batch: 080 ----
mean loss: 341.32
 ---- batch: 090 ----
mean loss: 352.75
 ---- batch: 100 ----
mean loss: 333.61
 ---- batch: 110 ----
mean loss: 333.96
train mean loss: 339.35
epoch train time: 0:00:02.133867
elapsed time: 0:04:55.004358
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 01:01:12.740599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.19
 ---- batch: 020 ----
mean loss: 333.31
 ---- batch: 030 ----
mean loss: 351.13
 ---- batch: 040 ----
mean loss: 339.51
 ---- batch: 050 ----
mean loss: 350.27
 ---- batch: 060 ----
mean loss: 340.10
 ---- batch: 070 ----
mean loss: 336.82
 ---- batch: 080 ----
mean loss: 334.84
 ---- batch: 090 ----
mean loss: 344.82
 ---- batch: 100 ----
mean loss: 331.80
 ---- batch: 110 ----
mean loss: 337.21
train mean loss: 339.44
epoch train time: 0:00:02.131559
elapsed time: 0:04:57.136062
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 01:01:14.872305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.42
 ---- batch: 020 ----
mean loss: 327.80
 ---- batch: 030 ----
mean loss: 337.62
 ---- batch: 040 ----
mean loss: 338.48
 ---- batch: 050 ----
mean loss: 329.97
 ---- batch: 060 ----
mean loss: 330.89
 ---- batch: 070 ----
mean loss: 347.71
 ---- batch: 080 ----
mean loss: 356.94
 ---- batch: 090 ----
mean loss: 328.83
 ---- batch: 100 ----
mean loss: 329.95
 ---- batch: 110 ----
mean loss: 342.36
train mean loss: 337.59
epoch train time: 0:00:02.133141
elapsed time: 0:04:59.269360
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 01:01:17.005619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.56
 ---- batch: 020 ----
mean loss: 334.90
 ---- batch: 030 ----
mean loss: 344.28
 ---- batch: 040 ----
mean loss: 348.12
 ---- batch: 050 ----
mean loss: 338.99
 ---- batch: 060 ----
mean loss: 325.18
 ---- batch: 070 ----
mean loss: 336.80
 ---- batch: 080 ----
mean loss: 342.25
 ---- batch: 090 ----
mean loss: 332.09
 ---- batch: 100 ----
mean loss: 331.08
 ---- batch: 110 ----
mean loss: 333.69
train mean loss: 336.72
epoch train time: 0:00:02.134937
elapsed time: 0:05:01.404471
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 01:01:19.140731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.25
 ---- batch: 020 ----
mean loss: 333.48
 ---- batch: 030 ----
mean loss: 345.86
 ---- batch: 040 ----
mean loss: 331.12
 ---- batch: 050 ----
mean loss: 341.74
 ---- batch: 060 ----
mean loss: 334.28
 ---- batch: 070 ----
mean loss: 345.91
 ---- batch: 080 ----
mean loss: 339.33
 ---- batch: 090 ----
mean loss: 347.71
 ---- batch: 100 ----
mean loss: 336.85
 ---- batch: 110 ----
mean loss: 346.69
train mean loss: 339.56
epoch train time: 0:00:02.134259
elapsed time: 0:05:03.538896
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 01:01:21.275171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.48
 ---- batch: 020 ----
mean loss: 341.70
 ---- batch: 030 ----
mean loss: 341.96
 ---- batch: 040 ----
mean loss: 346.87
 ---- batch: 050 ----
mean loss: 340.99
 ---- batch: 060 ----
mean loss: 336.09
 ---- batch: 070 ----
mean loss: 341.23
 ---- batch: 080 ----
mean loss: 332.91
 ---- batch: 090 ----
mean loss: 322.84
 ---- batch: 100 ----
mean loss: 338.23
 ---- batch: 110 ----
mean loss: 336.84
train mean loss: 339.46
epoch train time: 0:00:02.134261
elapsed time: 0:05:05.673328
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 01:01:23.409568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.53
 ---- batch: 020 ----
mean loss: 341.87
 ---- batch: 030 ----
mean loss: 337.29
 ---- batch: 040 ----
mean loss: 336.42
 ---- batch: 050 ----
mean loss: 339.91
 ---- batch: 060 ----
mean loss: 326.46
 ---- batch: 070 ----
mean loss: 330.28
 ---- batch: 080 ----
mean loss: 333.38
 ---- batch: 090 ----
mean loss: 339.48
 ---- batch: 100 ----
mean loss: 333.49
 ---- batch: 110 ----
mean loss: 334.09
train mean loss: 335.72
epoch train time: 0:00:02.129333
elapsed time: 0:05:07.802799
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 01:01:25.539035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.24
 ---- batch: 020 ----
mean loss: 332.36
 ---- batch: 030 ----
mean loss: 332.33
 ---- batch: 040 ----
mean loss: 337.48
 ---- batch: 050 ----
mean loss: 326.99
 ---- batch: 060 ----
mean loss: 333.39
 ---- batch: 070 ----
mean loss: 346.35
 ---- batch: 080 ----
mean loss: 338.39
 ---- batch: 090 ----
mean loss: 341.41
 ---- batch: 100 ----
mean loss: 339.70
 ---- batch: 110 ----
mean loss: 332.33
train mean loss: 335.25
epoch train time: 0:00:02.133205
elapsed time: 0:05:09.936138
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 01:01:27.672394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.70
 ---- batch: 020 ----
mean loss: 333.42
 ---- batch: 030 ----
mean loss: 336.46
 ---- batch: 040 ----
mean loss: 326.03
 ---- batch: 050 ----
mean loss: 330.12
 ---- batch: 060 ----
mean loss: 330.90
 ---- batch: 070 ----
mean loss: 335.30
 ---- batch: 080 ----
mean loss: 330.05
 ---- batch: 090 ----
mean loss: 338.18
 ---- batch: 100 ----
mean loss: 331.66
 ---- batch: 110 ----
mean loss: 331.50
train mean loss: 332.45
epoch train time: 0:00:02.133561
elapsed time: 0:05:12.069859
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 01:01:29.806099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.91
 ---- batch: 020 ----
mean loss: 332.41
 ---- batch: 030 ----
mean loss: 337.48
 ---- batch: 040 ----
mean loss: 336.50
 ---- batch: 050 ----
mean loss: 331.47
 ---- batch: 060 ----
mean loss: 334.90
 ---- batch: 070 ----
mean loss: 327.05
 ---- batch: 080 ----
mean loss: 340.16
 ---- batch: 090 ----
mean loss: 330.81
 ---- batch: 100 ----
mean loss: 329.60
 ---- batch: 110 ----
mean loss: 330.27
train mean loss: 332.67
epoch train time: 0:00:02.134039
elapsed time: 0:05:14.204045
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 01:01:31.940308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.11
 ---- batch: 020 ----
mean loss: 332.43
 ---- batch: 030 ----
mean loss: 330.90
 ---- batch: 040 ----
mean loss: 338.55
 ---- batch: 050 ----
mean loss: 342.64
 ---- batch: 060 ----
mean loss: 329.85
 ---- batch: 070 ----
mean loss: 334.74
 ---- batch: 080 ----
mean loss: 332.90
 ---- batch: 090 ----
mean loss: 336.75
 ---- batch: 100 ----
mean loss: 319.16
 ---- batch: 110 ----
mean loss: 336.96
train mean loss: 333.74
epoch train time: 0:00:02.137359
elapsed time: 0:05:16.341570
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 01:01:34.077810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.72
 ---- batch: 020 ----
mean loss: 326.87
 ---- batch: 030 ----
mean loss: 345.54
 ---- batch: 040 ----
mean loss: 336.66
 ---- batch: 050 ----
mean loss: 327.23
 ---- batch: 060 ----
mean loss: 330.74
 ---- batch: 070 ----
mean loss: 325.34
 ---- batch: 080 ----
mean loss: 331.13
 ---- batch: 090 ----
mean loss: 338.18
 ---- batch: 100 ----
mean loss: 324.70
 ---- batch: 110 ----
mean loss: 334.33
train mean loss: 331.70
epoch train time: 0:00:02.130824
elapsed time: 0:05:18.472530
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 01:01:36.208785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.44
 ---- batch: 020 ----
mean loss: 344.29
 ---- batch: 030 ----
mean loss: 335.45
 ---- batch: 040 ----
mean loss: 327.70
 ---- batch: 050 ----
mean loss: 334.53
 ---- batch: 060 ----
mean loss: 339.29
 ---- batch: 070 ----
mean loss: 339.42
 ---- batch: 080 ----
mean loss: 329.48
 ---- batch: 090 ----
mean loss: 338.64
 ---- batch: 100 ----
mean loss: 335.15
 ---- batch: 110 ----
mean loss: 335.49
train mean loss: 334.85
epoch train time: 0:00:02.134011
elapsed time: 0:05:20.606715
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 01:01:38.342956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.93
 ---- batch: 020 ----
mean loss: 336.40
 ---- batch: 030 ----
mean loss: 329.91
 ---- batch: 040 ----
mean loss: 338.13
 ---- batch: 050 ----
mean loss: 326.65
 ---- batch: 060 ----
mean loss: 326.88
 ---- batch: 070 ----
mean loss: 335.53
 ---- batch: 080 ----
mean loss: 340.75
 ---- batch: 090 ----
mean loss: 334.73
 ---- batch: 100 ----
mean loss: 322.76
 ---- batch: 110 ----
mean loss: 329.60
train mean loss: 332.99
epoch train time: 0:00:02.134588
elapsed time: 0:05:22.741454
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 01:01:40.477711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.45
 ---- batch: 020 ----
mean loss: 340.77
 ---- batch: 030 ----
mean loss: 336.95
 ---- batch: 040 ----
mean loss: 326.35
 ---- batch: 050 ----
mean loss: 340.95
 ---- batch: 060 ----
mean loss: 350.67
 ---- batch: 070 ----
mean loss: 337.65
 ---- batch: 080 ----
mean loss: 342.28
 ---- batch: 090 ----
mean loss: 318.97
 ---- batch: 100 ----
mean loss: 324.65
 ---- batch: 110 ----
mean loss: 336.84
train mean loss: 334.95
epoch train time: 0:00:02.133978
elapsed time: 0:05:24.875583
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 01:01:42.611821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.83
 ---- batch: 020 ----
mean loss: 346.46
 ---- batch: 030 ----
mean loss: 329.05
 ---- batch: 040 ----
mean loss: 333.41
 ---- batch: 050 ----
mean loss: 327.20
 ---- batch: 060 ----
mean loss: 333.39
 ---- batch: 070 ----
mean loss: 322.04
 ---- batch: 080 ----
mean loss: 333.39
 ---- batch: 090 ----
mean loss: 321.83
 ---- batch: 100 ----
mean loss: 335.75
 ---- batch: 110 ----
mean loss: 333.64
train mean loss: 332.81
epoch train time: 0:00:02.132814
elapsed time: 0:05:27.008531
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 01:01:44.744768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.20
 ---- batch: 020 ----
mean loss: 346.08
 ---- batch: 030 ----
mean loss: 331.00
 ---- batch: 040 ----
mean loss: 332.83
 ---- batch: 050 ----
mean loss: 337.02
 ---- batch: 060 ----
mean loss: 326.81
 ---- batch: 070 ----
mean loss: 321.44
 ---- batch: 080 ----
mean loss: 327.92
 ---- batch: 090 ----
mean loss: 334.68
 ---- batch: 100 ----
mean loss: 326.23
 ---- batch: 110 ----
mean loss: 338.80
train mean loss: 331.87
epoch train time: 0:00:02.133432
elapsed time: 0:05:29.142126
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 01:01:46.878397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.80
 ---- batch: 020 ----
mean loss: 330.09
 ---- batch: 030 ----
mean loss: 342.75
 ---- batch: 040 ----
mean loss: 331.08
 ---- batch: 050 ----
mean loss: 335.77
 ---- batch: 060 ----
mean loss: 315.93
 ---- batch: 070 ----
mean loss: 339.73
 ---- batch: 080 ----
mean loss: 324.57
 ---- batch: 090 ----
mean loss: 334.12
 ---- batch: 100 ----
mean loss: 334.69
 ---- batch: 110 ----
mean loss: 326.49
train mean loss: 330.36
epoch train time: 0:00:02.131040
elapsed time: 0:05:31.273335
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 01:01:49.009583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.74
 ---- batch: 020 ----
mean loss: 331.88
 ---- batch: 030 ----
mean loss: 331.43
 ---- batch: 040 ----
mean loss: 332.11
 ---- batch: 050 ----
mean loss: 323.05
 ---- batch: 060 ----
mean loss: 337.20
 ---- batch: 070 ----
mean loss: 332.97
 ---- batch: 080 ----
mean loss: 331.06
 ---- batch: 090 ----
mean loss: 333.28
 ---- batch: 100 ----
mean loss: 322.56
 ---- batch: 110 ----
mean loss: 324.94
train mean loss: 330.37
epoch train time: 0:00:02.131418
elapsed time: 0:05:33.404901
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 01:01:51.141141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.10
 ---- batch: 020 ----
mean loss: 326.27
 ---- batch: 030 ----
mean loss: 331.84
 ---- batch: 040 ----
mean loss: 332.96
 ---- batch: 050 ----
mean loss: 335.89
 ---- batch: 060 ----
mean loss: 339.88
 ---- batch: 070 ----
mean loss: 326.49
 ---- batch: 080 ----
mean loss: 340.07
 ---- batch: 090 ----
mean loss: 329.04
 ---- batch: 100 ----
mean loss: 332.26
 ---- batch: 110 ----
mean loss: 345.18
train mean loss: 332.92
epoch train time: 0:00:02.135001
elapsed time: 0:05:35.540043
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 01:01:53.276285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.36
 ---- batch: 020 ----
mean loss: 334.06
 ---- batch: 030 ----
mean loss: 324.80
 ---- batch: 040 ----
mean loss: 338.99
 ---- batch: 050 ----
mean loss: 324.71
 ---- batch: 060 ----
mean loss: 328.84
 ---- batch: 070 ----
mean loss: 327.73
 ---- batch: 080 ----
mean loss: 334.56
 ---- batch: 090 ----
mean loss: 334.76
 ---- batch: 100 ----
mean loss: 334.52
 ---- batch: 110 ----
mean loss: 320.98
train mean loss: 329.58
epoch train time: 0:00:02.135740
elapsed time: 0:05:37.675925
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 01:01:55.412180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.57
 ---- batch: 020 ----
mean loss: 337.94
 ---- batch: 030 ----
mean loss: 335.11
 ---- batch: 040 ----
mean loss: 331.12
 ---- batch: 050 ----
mean loss: 327.65
 ---- batch: 060 ----
mean loss: 333.10
 ---- batch: 070 ----
mean loss: 324.41
 ---- batch: 080 ----
mean loss: 332.01
 ---- batch: 090 ----
mean loss: 324.05
 ---- batch: 100 ----
mean loss: 327.65
 ---- batch: 110 ----
mean loss: 322.34
train mean loss: 329.44
epoch train time: 0:00:02.129804
elapsed time: 0:05:39.805881
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 01:01:57.542121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.59
 ---- batch: 020 ----
mean loss: 328.63
 ---- batch: 030 ----
mean loss: 338.26
 ---- batch: 040 ----
mean loss: 325.64
 ---- batch: 050 ----
mean loss: 333.21
 ---- batch: 060 ----
mean loss: 331.17
 ---- batch: 070 ----
mean loss: 325.89
 ---- batch: 080 ----
mean loss: 320.26
 ---- batch: 090 ----
mean loss: 319.16
 ---- batch: 100 ----
mean loss: 316.31
 ---- batch: 110 ----
mean loss: 325.96
train mean loss: 325.80
epoch train time: 0:00:02.135331
elapsed time: 0:05:41.941350
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 01:01:59.677589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.86
 ---- batch: 020 ----
mean loss: 332.67
 ---- batch: 030 ----
mean loss: 335.39
 ---- batch: 040 ----
mean loss: 330.58
 ---- batch: 050 ----
mean loss: 322.65
 ---- batch: 060 ----
mean loss: 328.50
 ---- batch: 070 ----
mean loss: 327.37
 ---- batch: 080 ----
mean loss: 325.56
 ---- batch: 090 ----
mean loss: 329.85
 ---- batch: 100 ----
mean loss: 326.37
 ---- batch: 110 ----
mean loss: 326.63
train mean loss: 329.92
epoch train time: 0:00:02.138377
elapsed time: 0:05:44.079903
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 01:02:01.816169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.02
 ---- batch: 020 ----
mean loss: 325.19
 ---- batch: 030 ----
mean loss: 322.02
 ---- batch: 040 ----
mean loss: 325.29
 ---- batch: 050 ----
mean loss: 332.55
 ---- batch: 060 ----
mean loss: 322.16
 ---- batch: 070 ----
mean loss: 335.82
 ---- batch: 080 ----
mean loss: 333.49
 ---- batch: 090 ----
mean loss: 336.16
 ---- batch: 100 ----
mean loss: 335.75
 ---- batch: 110 ----
mean loss: 320.19
train mean loss: 328.82
epoch train time: 0:00:02.128970
elapsed time: 0:05:46.209042
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 01:02:03.945287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.84
 ---- batch: 020 ----
mean loss: 331.30
 ---- batch: 030 ----
mean loss: 332.73
 ---- batch: 040 ----
mean loss: 318.83
 ---- batch: 050 ----
mean loss: 334.36
 ---- batch: 060 ----
mean loss: 320.55
 ---- batch: 070 ----
mean loss: 334.98
 ---- batch: 080 ----
mean loss: 334.90
 ---- batch: 090 ----
mean loss: 323.81
 ---- batch: 100 ----
mean loss: 325.20
 ---- batch: 110 ----
mean loss: 323.19
train mean loss: 327.25
epoch train time: 0:00:02.134307
elapsed time: 0:05:48.343513
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 01:02:06.079747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.09
 ---- batch: 020 ----
mean loss: 315.38
 ---- batch: 030 ----
mean loss: 327.18
 ---- batch: 040 ----
mean loss: 322.15
 ---- batch: 050 ----
mean loss: 321.55
 ---- batch: 060 ----
mean loss: 329.79
 ---- batch: 070 ----
mean loss: 332.80
 ---- batch: 080 ----
mean loss: 320.83
 ---- batch: 090 ----
mean loss: 330.50
 ---- batch: 100 ----
mean loss: 337.63
 ---- batch: 110 ----
mean loss: 326.34
train mean loss: 326.87
epoch train time: 0:00:02.133091
elapsed time: 0:05:50.476749
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 01:02:08.212995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.38
 ---- batch: 020 ----
mean loss: 322.68
 ---- batch: 030 ----
mean loss: 321.95
 ---- batch: 040 ----
mean loss: 325.28
 ---- batch: 050 ----
mean loss: 329.72
 ---- batch: 060 ----
mean loss: 335.48
 ---- batch: 070 ----
mean loss: 318.99
 ---- batch: 080 ----
mean loss: 327.42
 ---- batch: 090 ----
mean loss: 327.15
 ---- batch: 100 ----
mean loss: 318.85
 ---- batch: 110 ----
mean loss: 329.72
train mean loss: 325.51
epoch train time: 0:00:02.133096
elapsed time: 0:05:52.610011
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 01:02:10.346255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.12
 ---- batch: 020 ----
mean loss: 326.79
 ---- batch: 030 ----
mean loss: 331.21
 ---- batch: 040 ----
mean loss: 327.10
 ---- batch: 050 ----
mean loss: 330.13
 ---- batch: 060 ----
mean loss: 331.15
 ---- batch: 070 ----
mean loss: 336.71
 ---- batch: 080 ----
mean loss: 330.44
 ---- batch: 090 ----
mean loss: 325.23
 ---- batch: 100 ----
mean loss: 337.70
 ---- batch: 110 ----
mean loss: 319.69
train mean loss: 329.17
epoch train time: 0:00:02.138244
elapsed time: 0:05:54.748404
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 01:02:12.484650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.03
 ---- batch: 020 ----
mean loss: 325.69
 ---- batch: 030 ----
mean loss: 319.82
 ---- batch: 040 ----
mean loss: 322.36
 ---- batch: 050 ----
mean loss: 328.57
 ---- batch: 060 ----
mean loss: 315.98
 ---- batch: 070 ----
mean loss: 326.16
 ---- batch: 080 ----
mean loss: 324.99
 ---- batch: 090 ----
mean loss: 334.89
 ---- batch: 100 ----
mean loss: 315.35
 ---- batch: 110 ----
mean loss: 314.50
train mean loss: 324.80
epoch train time: 0:00:02.132401
elapsed time: 0:05:56.880957
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 01:02:14.617220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.18
 ---- batch: 020 ----
mean loss: 315.38
 ---- batch: 030 ----
mean loss: 330.09
 ---- batch: 040 ----
mean loss: 326.46
 ---- batch: 050 ----
mean loss: 334.88
 ---- batch: 060 ----
mean loss: 325.36
 ---- batch: 070 ----
mean loss: 323.74
 ---- batch: 080 ----
mean loss: 326.61
 ---- batch: 090 ----
mean loss: 322.28
 ---- batch: 100 ----
mean loss: 315.50
 ---- batch: 110 ----
mean loss: 330.81
train mean loss: 326.14
epoch train time: 0:00:02.137825
elapsed time: 0:05:59.018974
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 01:02:16.755229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.93
 ---- batch: 020 ----
mean loss: 328.08
 ---- batch: 030 ----
mean loss: 324.87
 ---- batch: 040 ----
mean loss: 329.37
 ---- batch: 050 ----
mean loss: 338.99
 ---- batch: 060 ----
mean loss: 324.71
 ---- batch: 070 ----
mean loss: 320.58
 ---- batch: 080 ----
mean loss: 330.79
 ---- batch: 090 ----
mean loss: 321.70
 ---- batch: 100 ----
mean loss: 318.03
 ---- batch: 110 ----
mean loss: 325.96
train mean loss: 326.01
epoch train time: 0:00:02.133658
elapsed time: 0:06:01.152797
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 01:02:18.889055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.00
 ---- batch: 020 ----
mean loss: 334.71
 ---- batch: 030 ----
mean loss: 313.08
 ---- batch: 040 ----
mean loss: 321.47
 ---- batch: 050 ----
mean loss: 324.63
 ---- batch: 060 ----
mean loss: 325.17
 ---- batch: 070 ----
mean loss: 334.05
 ---- batch: 080 ----
mean loss: 329.52
 ---- batch: 090 ----
mean loss: 321.62
 ---- batch: 100 ----
mean loss: 325.02
 ---- batch: 110 ----
mean loss: 321.83
train mean loss: 325.15
epoch train time: 0:00:02.134192
elapsed time: 0:06:03.287154
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 01:02:21.023396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.99
 ---- batch: 020 ----
mean loss: 321.07
 ---- batch: 030 ----
mean loss: 326.78
 ---- batch: 040 ----
mean loss: 326.24
 ---- batch: 050 ----
mean loss: 326.90
 ---- batch: 060 ----
mean loss: 321.59
 ---- batch: 070 ----
mean loss: 316.50
 ---- batch: 080 ----
mean loss: 327.31
 ---- batch: 090 ----
mean loss: 319.55
 ---- batch: 100 ----
mean loss: 318.29
 ---- batch: 110 ----
mean loss: 320.46
train mean loss: 323.42
epoch train time: 0:00:02.139535
elapsed time: 0:06:05.426836
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 01:02:23.163079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.46
 ---- batch: 020 ----
mean loss: 318.47
 ---- batch: 030 ----
mean loss: 326.99
 ---- batch: 040 ----
mean loss: 322.51
 ---- batch: 050 ----
mean loss: 318.85
 ---- batch: 060 ----
mean loss: 330.89
 ---- batch: 070 ----
mean loss: 311.73
 ---- batch: 080 ----
mean loss: 326.01
 ---- batch: 090 ----
mean loss: 327.42
 ---- batch: 100 ----
mean loss: 324.37
 ---- batch: 110 ----
mean loss: 319.49
train mean loss: 323.05
epoch train time: 0:00:02.172233
elapsed time: 0:06:07.599233
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 01:02:25.335482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.38
 ---- batch: 020 ----
mean loss: 332.94
 ---- batch: 030 ----
mean loss: 327.45
 ---- batch: 040 ----
mean loss: 329.51
 ---- batch: 050 ----
mean loss: 338.12
 ---- batch: 060 ----
mean loss: 325.43
 ---- batch: 070 ----
mean loss: 327.35
 ---- batch: 080 ----
mean loss: 314.05
 ---- batch: 090 ----
mean loss: 316.35
 ---- batch: 100 ----
mean loss: 330.77
 ---- batch: 110 ----
mean loss: 334.87
train mean loss: 327.91
epoch train time: 0:00:02.176033
elapsed time: 0:06:09.775464
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 01:02:27.511718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.39
 ---- batch: 020 ----
mean loss: 326.40
 ---- batch: 030 ----
mean loss: 319.99
 ---- batch: 040 ----
mean loss: 332.28
 ---- batch: 050 ----
mean loss: 323.48
 ---- batch: 060 ----
mean loss: 312.44
 ---- batch: 070 ----
mean loss: 323.55
 ---- batch: 080 ----
mean loss: 317.65
 ---- batch: 090 ----
mean loss: 318.73
 ---- batch: 100 ----
mean loss: 332.01
 ---- batch: 110 ----
mean loss: 325.93
train mean loss: 323.17
epoch train time: 0:00:02.134861
elapsed time: 0:06:11.910491
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 01:02:29.646756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.22
 ---- batch: 020 ----
mean loss: 311.46
 ---- batch: 030 ----
mean loss: 317.68
 ---- batch: 040 ----
mean loss: 325.84
 ---- batch: 050 ----
mean loss: 340.08
 ---- batch: 060 ----
mean loss: 319.62
 ---- batch: 070 ----
mean loss: 307.28
 ---- batch: 080 ----
mean loss: 342.74
 ---- batch: 090 ----
mean loss: 331.93
 ---- batch: 100 ----
mean loss: 317.05
 ---- batch: 110 ----
mean loss: 304.14
train mean loss: 322.87
epoch train time: 0:00:02.137448
elapsed time: 0:06:14.048117
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 01:02:31.784361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.73
 ---- batch: 020 ----
mean loss: 328.88
 ---- batch: 030 ----
mean loss: 321.40
 ---- batch: 040 ----
mean loss: 320.22
 ---- batch: 050 ----
mean loss: 335.30
 ---- batch: 060 ----
mean loss: 328.86
 ---- batch: 070 ----
mean loss: 314.81
 ---- batch: 080 ----
mean loss: 314.50
 ---- batch: 090 ----
mean loss: 322.74
 ---- batch: 100 ----
mean loss: 334.77
 ---- batch: 110 ----
mean loss: 329.39
train mean loss: 324.60
epoch train time: 0:00:02.132714
elapsed time: 0:06:16.180987
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 01:02:33.917235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.44
 ---- batch: 020 ----
mean loss: 324.24
 ---- batch: 030 ----
mean loss: 325.72
 ---- batch: 040 ----
mean loss: 311.75
 ---- batch: 050 ----
mean loss: 340.08
 ---- batch: 060 ----
mean loss: 326.91
 ---- batch: 070 ----
mean loss: 317.11
 ---- batch: 080 ----
mean loss: 305.07
 ---- batch: 090 ----
mean loss: 313.98
 ---- batch: 100 ----
mean loss: 331.40
 ---- batch: 110 ----
mean loss: 327.05
train mean loss: 322.37
epoch train time: 0:00:02.141313
elapsed time: 0:06:18.322461
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 01:02:36.058707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.08
 ---- batch: 020 ----
mean loss: 315.66
 ---- batch: 030 ----
mean loss: 317.58
 ---- batch: 040 ----
mean loss: 320.88
 ---- batch: 050 ----
mean loss: 320.22
 ---- batch: 060 ----
mean loss: 326.71
 ---- batch: 070 ----
mean loss: 319.12
 ---- batch: 080 ----
mean loss: 319.51
 ---- batch: 090 ----
mean loss: 313.61
 ---- batch: 100 ----
mean loss: 319.00
 ---- batch: 110 ----
mean loss: 319.35
train mean loss: 320.11
epoch train time: 0:00:02.133319
elapsed time: 0:06:20.455935
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 01:02:38.192191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.70
 ---- batch: 020 ----
mean loss: 319.96
 ---- batch: 030 ----
mean loss: 322.36
 ---- batch: 040 ----
mean loss: 332.30
 ---- batch: 050 ----
mean loss: 327.99
 ---- batch: 060 ----
mean loss: 317.36
 ---- batch: 070 ----
mean loss: 331.22
 ---- batch: 080 ----
mean loss: 315.98
 ---- batch: 090 ----
mean loss: 313.22
 ---- batch: 100 ----
mean loss: 326.55
 ---- batch: 110 ----
mean loss: 311.63
train mean loss: 320.99
epoch train time: 0:00:02.136779
elapsed time: 0:06:22.592879
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 01:02:40.329125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.87
 ---- batch: 020 ----
mean loss: 310.95
 ---- batch: 030 ----
mean loss: 321.94
 ---- batch: 040 ----
mean loss: 312.64
 ---- batch: 050 ----
mean loss: 323.82
 ---- batch: 060 ----
mean loss: 318.10
 ---- batch: 070 ----
mean loss: 324.04
 ---- batch: 080 ----
mean loss: 323.78
 ---- batch: 090 ----
mean loss: 322.29
 ---- batch: 100 ----
mean loss: 308.51
 ---- batch: 110 ----
mean loss: 315.01
train mean loss: 318.64
epoch train time: 0:00:02.141068
elapsed time: 0:06:24.734109
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 01:02:42.470376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.14
 ---- batch: 020 ----
mean loss: 323.63
 ---- batch: 030 ----
mean loss: 318.45
 ---- batch: 040 ----
mean loss: 315.56
 ---- batch: 050 ----
mean loss: 316.43
 ---- batch: 060 ----
mean loss: 327.67
 ---- batch: 070 ----
mean loss: 311.30
 ---- batch: 080 ----
mean loss: 322.31
 ---- batch: 090 ----
mean loss: 324.23
 ---- batch: 100 ----
mean loss: 331.22
 ---- batch: 110 ----
mean loss: 306.47
train mean loss: 320.00
epoch train time: 0:00:02.145824
elapsed time: 0:06:26.880106
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 01:02:44.616353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.87
 ---- batch: 020 ----
mean loss: 306.38
 ---- batch: 030 ----
mean loss: 312.64
 ---- batch: 040 ----
mean loss: 313.05
 ---- batch: 050 ----
mean loss: 319.13
 ---- batch: 060 ----
mean loss: 318.65
 ---- batch: 070 ----
mean loss: 322.63
 ---- batch: 080 ----
mean loss: 321.07
 ---- batch: 090 ----
mean loss: 325.62
 ---- batch: 100 ----
mean loss: 322.69
 ---- batch: 110 ----
mean loss: 326.99
train mean loss: 318.07
epoch train time: 0:00:02.136227
elapsed time: 0:06:29.016488
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 01:02:46.752731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.59
 ---- batch: 020 ----
mean loss: 312.64
 ---- batch: 030 ----
mean loss: 305.05
 ---- batch: 040 ----
mean loss: 329.92
 ---- batch: 050 ----
mean loss: 335.80
 ---- batch: 060 ----
mean loss: 315.96
 ---- batch: 070 ----
mean loss: 312.31
 ---- batch: 080 ----
mean loss: 321.38
 ---- batch: 090 ----
mean loss: 312.08
 ---- batch: 100 ----
mean loss: 313.93
 ---- batch: 110 ----
mean loss: 327.13
train mean loss: 318.45
epoch train time: 0:00:02.141591
elapsed time: 0:06:31.158242
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 01:02:48.894490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.87
 ---- batch: 020 ----
mean loss: 314.89
 ---- batch: 030 ----
mean loss: 304.22
 ---- batch: 040 ----
mean loss: 317.82
 ---- batch: 050 ----
mean loss: 321.44
 ---- batch: 060 ----
mean loss: 325.84
 ---- batch: 070 ----
mean loss: 326.61
 ---- batch: 080 ----
mean loss: 318.91
 ---- batch: 090 ----
mean loss: 318.22
 ---- batch: 100 ----
mean loss: 316.48
 ---- batch: 110 ----
mean loss: 308.18
train mean loss: 316.49
epoch train time: 0:00:02.141575
elapsed time: 0:06:33.299985
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 01:02:51.036232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.14
 ---- batch: 020 ----
mean loss: 338.18
 ---- batch: 030 ----
mean loss: 319.79
 ---- batch: 040 ----
mean loss: 309.84
 ---- batch: 050 ----
mean loss: 313.24
 ---- batch: 060 ----
mean loss: 323.39
 ---- batch: 070 ----
mean loss: 313.92
 ---- batch: 080 ----
mean loss: 310.76
 ---- batch: 090 ----
mean loss: 322.53
 ---- batch: 100 ----
mean loss: 307.20
 ---- batch: 110 ----
mean loss: 301.41
train mean loss: 316.77
epoch train time: 0:00:02.144926
elapsed time: 0:06:35.445072
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 01:02:53.181318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.06
 ---- batch: 020 ----
mean loss: 321.04
 ---- batch: 030 ----
mean loss: 305.11
 ---- batch: 040 ----
mean loss: 306.49
 ---- batch: 050 ----
mean loss: 316.06
 ---- batch: 060 ----
mean loss: 314.80
 ---- batch: 070 ----
mean loss: 326.58
 ---- batch: 080 ----
mean loss: 312.93
 ---- batch: 090 ----
mean loss: 321.83
 ---- batch: 100 ----
mean loss: 314.99
 ---- batch: 110 ----
mean loss: 307.71
train mean loss: 313.62
epoch train time: 0:00:02.153476
elapsed time: 0:06:37.598769
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 01:02:55.335015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.48
 ---- batch: 020 ----
mean loss: 311.86
 ---- batch: 030 ----
mean loss: 314.98
 ---- batch: 040 ----
mean loss: 302.76
 ---- batch: 050 ----
mean loss: 311.94
 ---- batch: 060 ----
mean loss: 320.62
 ---- batch: 070 ----
mean loss: 307.19
 ---- batch: 080 ----
mean loss: 314.22
 ---- batch: 090 ----
mean loss: 322.33
 ---- batch: 100 ----
mean loss: 306.68
 ---- batch: 110 ----
mean loss: 310.05
train mean loss: 312.40
epoch train time: 0:00:02.153651
elapsed time: 0:06:39.752573
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 01:02:57.488834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.23
 ---- batch: 020 ----
mean loss: 316.25
 ---- batch: 030 ----
mean loss: 324.40
 ---- batch: 040 ----
mean loss: 307.27
 ---- batch: 050 ----
mean loss: 307.84
 ---- batch: 060 ----
mean loss: 315.04
 ---- batch: 070 ----
mean loss: 320.16
 ---- batch: 080 ----
mean loss: 301.60
 ---- batch: 090 ----
mean loss: 328.00
 ---- batch: 100 ----
mean loss: 303.46
 ---- batch: 110 ----
mean loss: 323.91
train mean loss: 314.82
epoch train time: 0:00:02.152339
elapsed time: 0:06:41.905105
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 01:02:59.641340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.69
 ---- batch: 020 ----
mean loss: 311.08
 ---- batch: 030 ----
mean loss: 324.49
 ---- batch: 040 ----
mean loss: 317.45
 ---- batch: 050 ----
mean loss: 315.83
 ---- batch: 060 ----
mean loss: 310.91
 ---- batch: 070 ----
mean loss: 303.25
 ---- batch: 080 ----
mean loss: 309.49
 ---- batch: 090 ----
mean loss: 321.35
 ---- batch: 100 ----
mean loss: 316.30
 ---- batch: 110 ----
mean loss: 311.99
train mean loss: 314.33
epoch train time: 0:00:02.140799
elapsed time: 0:06:44.046056
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 01:03:01.782302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.17
 ---- batch: 020 ----
mean loss: 305.23
 ---- batch: 030 ----
mean loss: 320.84
 ---- batch: 040 ----
mean loss: 314.66
 ---- batch: 050 ----
mean loss: 313.22
 ---- batch: 060 ----
mean loss: 302.31
 ---- batch: 070 ----
mean loss: 325.90
 ---- batch: 080 ----
mean loss: 301.69
 ---- batch: 090 ----
mean loss: 313.23
 ---- batch: 100 ----
mean loss: 314.78
 ---- batch: 110 ----
mean loss: 310.62
train mean loss: 313.11
epoch train time: 0:00:02.141289
elapsed time: 0:06:46.187497
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 01:03:03.923758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.71
 ---- batch: 020 ----
mean loss: 311.53
 ---- batch: 030 ----
mean loss: 297.01
 ---- batch: 040 ----
mean loss: 318.40
 ---- batch: 050 ----
mean loss: 310.18
 ---- batch: 060 ----
mean loss: 310.44
 ---- batch: 070 ----
mean loss: 309.92
 ---- batch: 080 ----
mean loss: 312.71
 ---- batch: 090 ----
mean loss: 302.77
 ---- batch: 100 ----
mean loss: 311.82
 ---- batch: 110 ----
mean loss: 320.64
train mean loss: 310.65
epoch train time: 0:00:02.139915
elapsed time: 0:06:48.327579
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 01:03:06.063821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.69
 ---- batch: 020 ----
mean loss: 322.85
 ---- batch: 030 ----
mean loss: 309.77
 ---- batch: 040 ----
mean loss: 305.87
 ---- batch: 050 ----
mean loss: 316.76
 ---- batch: 060 ----
mean loss: 313.17
 ---- batch: 070 ----
mean loss: 302.42
 ---- batch: 080 ----
mean loss: 300.77
 ---- batch: 090 ----
mean loss: 315.62
 ---- batch: 100 ----
mean loss: 312.44
 ---- batch: 110 ----
mean loss: 294.55
train mean loss: 309.66
epoch train time: 0:00:02.146981
elapsed time: 0:06:50.474710
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 01:03:08.210964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.64
 ---- batch: 020 ----
mean loss: 318.14
 ---- batch: 030 ----
mean loss: 314.40
 ---- batch: 040 ----
mean loss: 300.49
 ---- batch: 050 ----
mean loss: 298.89
 ---- batch: 060 ----
mean loss: 307.24
 ---- batch: 070 ----
mean loss: 301.34
 ---- batch: 080 ----
mean loss: 298.51
 ---- batch: 090 ----
mean loss: 323.05
 ---- batch: 100 ----
mean loss: 318.20
 ---- batch: 110 ----
mean loss: 312.11
train mean loss: 310.56
epoch train time: 0:00:02.142114
elapsed time: 0:06:52.617042
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 01:03:10.353317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.50
 ---- batch: 020 ----
mean loss: 308.63
 ---- batch: 030 ----
mean loss: 312.09
 ---- batch: 040 ----
mean loss: 317.35
 ---- batch: 050 ----
mean loss: 307.92
 ---- batch: 060 ----
mean loss: 309.72
 ---- batch: 070 ----
mean loss: 310.76
 ---- batch: 080 ----
mean loss: 310.15
 ---- batch: 090 ----
mean loss: 302.66
 ---- batch: 100 ----
mean loss: 306.75
 ---- batch: 110 ----
mean loss: 315.46
train mean loss: 309.24
epoch train time: 0:00:02.148087
elapsed time: 0:06:54.765391
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 01:03:12.501685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.00
 ---- batch: 020 ----
mean loss: 318.44
 ---- batch: 030 ----
mean loss: 301.60
 ---- batch: 040 ----
mean loss: 303.98
 ---- batch: 050 ----
mean loss: 311.12
 ---- batch: 060 ----
mean loss: 310.89
 ---- batch: 070 ----
mean loss: 315.50
 ---- batch: 080 ----
mean loss: 306.64
 ---- batch: 090 ----
mean loss: 311.40
 ---- batch: 100 ----
mean loss: 304.08
 ---- batch: 110 ----
mean loss: 309.64
train mean loss: 309.38
epoch train time: 0:00:02.151535
elapsed time: 0:06:56.917151
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 01:03:14.653435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.90
 ---- batch: 020 ----
mean loss: 312.52
 ---- batch: 030 ----
mean loss: 312.40
 ---- batch: 040 ----
mean loss: 301.60
 ---- batch: 050 ----
mean loss: 311.90
 ---- batch: 060 ----
mean loss: 296.40
 ---- batch: 070 ----
mean loss: 312.52
 ---- batch: 080 ----
mean loss: 319.96
 ---- batch: 090 ----
mean loss: 319.12
 ---- batch: 100 ----
mean loss: 313.90
 ---- batch: 110 ----
mean loss: 316.15
train mean loss: 310.71
epoch train time: 0:00:02.151279
elapsed time: 0:06:59.068627
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 01:03:16.804878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.83
 ---- batch: 020 ----
mean loss: 318.70
 ---- batch: 030 ----
mean loss: 314.56
 ---- batch: 040 ----
mean loss: 315.94
 ---- batch: 050 ----
mean loss: 303.84
 ---- batch: 060 ----
mean loss: 316.09
 ---- batch: 070 ----
mean loss: 302.79
 ---- batch: 080 ----
mean loss: 306.51
 ---- batch: 090 ----
mean loss: 306.94
 ---- batch: 100 ----
mean loss: 312.06
 ---- batch: 110 ----
mean loss: 299.42
train mean loss: 309.79
epoch train time: 0:00:02.144025
elapsed time: 0:07:01.212804
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 01:03:18.949045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.39
 ---- batch: 020 ----
mean loss: 306.77
 ---- batch: 030 ----
mean loss: 298.10
 ---- batch: 040 ----
mean loss: 306.13
 ---- batch: 050 ----
mean loss: 308.88
 ---- batch: 060 ----
mean loss: 304.50
 ---- batch: 070 ----
mean loss: 312.62
 ---- batch: 080 ----
mean loss: 306.70
 ---- batch: 090 ----
mean loss: 314.39
 ---- batch: 100 ----
mean loss: 309.60
 ---- batch: 110 ----
mean loss: 301.98
train mean loss: 306.74
epoch train time: 0:00:02.137308
elapsed time: 0:07:03.350259
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 01:03:21.086502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.77
 ---- batch: 020 ----
mean loss: 318.28
 ---- batch: 030 ----
mean loss: 310.68
 ---- batch: 040 ----
mean loss: 313.47
 ---- batch: 050 ----
mean loss: 314.56
 ---- batch: 060 ----
mean loss: 312.28
 ---- batch: 070 ----
mean loss: 299.78
 ---- batch: 080 ----
mean loss: 298.96
 ---- batch: 090 ----
mean loss: 296.90
 ---- batch: 100 ----
mean loss: 311.60
 ---- batch: 110 ----
mean loss: 307.50
train mean loss: 308.36
epoch train time: 0:00:02.144501
elapsed time: 0:07:05.494960
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 01:03:23.231201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.49
 ---- batch: 020 ----
mean loss: 304.29
 ---- batch: 030 ----
mean loss: 307.81
 ---- batch: 040 ----
mean loss: 298.93
 ---- batch: 050 ----
mean loss: 303.96
 ---- batch: 060 ----
mean loss: 306.82
 ---- batch: 070 ----
mean loss: 308.68
 ---- batch: 080 ----
mean loss: 300.22
 ---- batch: 090 ----
mean loss: 295.17
 ---- batch: 100 ----
mean loss: 306.72
 ---- batch: 110 ----
mean loss: 298.14
train mean loss: 304.23
epoch train time: 0:00:02.152546
elapsed time: 0:07:07.647650
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 01:03:25.383914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.86
 ---- batch: 020 ----
mean loss: 310.35
 ---- batch: 030 ----
mean loss: 308.70
 ---- batch: 040 ----
mean loss: 301.26
 ---- batch: 050 ----
mean loss: 311.92
 ---- batch: 060 ----
mean loss: 306.76
 ---- batch: 070 ----
mean loss: 306.05
 ---- batch: 080 ----
mean loss: 298.07
 ---- batch: 090 ----
mean loss: 286.57
 ---- batch: 100 ----
mean loss: 298.57
 ---- batch: 110 ----
mean loss: 291.25
train mean loss: 303.13
epoch train time: 0:00:02.150430
elapsed time: 0:07:09.798248
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 01:03:27.534489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.97
 ---- batch: 020 ----
mean loss: 311.25
 ---- batch: 030 ----
mean loss: 311.07
 ---- batch: 040 ----
mean loss: 300.98
 ---- batch: 050 ----
mean loss: 307.45
 ---- batch: 060 ----
mean loss: 304.99
 ---- batch: 070 ----
mean loss: 299.88
 ---- batch: 080 ----
mean loss: 299.83
 ---- batch: 090 ----
mean loss: 313.15
 ---- batch: 100 ----
mean loss: 301.76
 ---- batch: 110 ----
mean loss: 303.46
train mean loss: 305.28
epoch train time: 0:00:02.144102
elapsed time: 0:07:11.942500
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 01:03:29.678744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.80
 ---- batch: 020 ----
mean loss: 310.84
 ---- batch: 030 ----
mean loss: 317.52
 ---- batch: 040 ----
mean loss: 302.01
 ---- batch: 050 ----
mean loss: 303.61
 ---- batch: 060 ----
mean loss: 304.26
 ---- batch: 070 ----
mean loss: 300.91
 ---- batch: 080 ----
mean loss: 298.39
 ---- batch: 090 ----
mean loss: 308.73
 ---- batch: 100 ----
mean loss: 302.63
 ---- batch: 110 ----
mean loss: 299.78
train mean loss: 305.72
epoch train time: 0:00:02.143618
elapsed time: 0:07:14.086284
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 01:03:31.822536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.49
 ---- batch: 020 ----
mean loss: 306.03
 ---- batch: 030 ----
mean loss: 324.72
 ---- batch: 040 ----
mean loss: 307.84
 ---- batch: 050 ----
mean loss: 314.31
 ---- batch: 060 ----
mean loss: 305.44
 ---- batch: 070 ----
mean loss: 291.48
 ---- batch: 080 ----
mean loss: 303.47
 ---- batch: 090 ----
mean loss: 305.69
 ---- batch: 100 ----
mean loss: 291.22
 ---- batch: 110 ----
mean loss: 294.03
train mean loss: 303.94
epoch train time: 0:00:02.143206
elapsed time: 0:07:16.229646
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 01:03:33.965890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.54
 ---- batch: 020 ----
mean loss: 300.88
 ---- batch: 030 ----
mean loss: 316.54
 ---- batch: 040 ----
mean loss: 299.45
 ---- batch: 050 ----
mean loss: 295.05
 ---- batch: 060 ----
mean loss: 305.76
 ---- batch: 070 ----
mean loss: 308.00
 ---- batch: 080 ----
mean loss: 306.22
 ---- batch: 090 ----
mean loss: 309.64
 ---- batch: 100 ----
mean loss: 313.24
 ---- batch: 110 ----
mean loss: 317.97
train mean loss: 306.38
epoch train time: 0:00:02.134877
elapsed time: 0:07:18.364712
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 01:03:36.100959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.02
 ---- batch: 020 ----
mean loss: 293.32
 ---- batch: 030 ----
mean loss: 299.13
 ---- batch: 040 ----
mean loss: 301.75
 ---- batch: 050 ----
mean loss: 313.03
 ---- batch: 060 ----
mean loss: 303.24
 ---- batch: 070 ----
mean loss: 302.29
 ---- batch: 080 ----
mean loss: 300.40
 ---- batch: 090 ----
mean loss: 299.03
 ---- batch: 100 ----
mean loss: 302.96
 ---- batch: 110 ----
mean loss: 311.38
train mean loss: 303.11
epoch train time: 0:00:02.138619
elapsed time: 0:07:20.503481
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 01:03:38.239723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.45
 ---- batch: 020 ----
mean loss: 299.47
 ---- batch: 030 ----
mean loss: 302.07
 ---- batch: 040 ----
mean loss: 308.81
 ---- batch: 050 ----
mean loss: 303.09
 ---- batch: 060 ----
mean loss: 318.62
 ---- batch: 070 ----
mean loss: 299.57
 ---- batch: 080 ----
mean loss: 304.30
 ---- batch: 090 ----
mean loss: 296.10
 ---- batch: 100 ----
mean loss: 294.52
 ---- batch: 110 ----
mean loss: 300.36
train mean loss: 303.31
epoch train time: 0:00:02.136272
elapsed time: 0:07:22.639902
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 01:03:40.376164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.47
 ---- batch: 020 ----
mean loss: 298.14
 ---- batch: 030 ----
mean loss: 303.07
 ---- batch: 040 ----
mean loss: 306.47
 ---- batch: 050 ----
mean loss: 302.63
 ---- batch: 060 ----
mean loss: 280.88
 ---- batch: 070 ----
mean loss: 289.59
 ---- batch: 080 ----
mean loss: 311.61
 ---- batch: 090 ----
mean loss: 288.02
 ---- batch: 100 ----
mean loss: 303.33
 ---- batch: 110 ----
mean loss: 301.75
train mean loss: 299.05
epoch train time: 0:00:02.134002
elapsed time: 0:07:24.774070
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 01:03:42.510311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.92
 ---- batch: 020 ----
mean loss: 312.71
 ---- batch: 030 ----
mean loss: 308.50
 ---- batch: 040 ----
mean loss: 300.77
 ---- batch: 050 ----
mean loss: 287.53
 ---- batch: 060 ----
mean loss: 309.18
 ---- batch: 070 ----
mean loss: 317.82
 ---- batch: 080 ----
mean loss: 293.32
 ---- batch: 090 ----
mean loss: 302.78
 ---- batch: 100 ----
mean loss: 304.77
 ---- batch: 110 ----
mean loss: 292.24
train mean loss: 302.34
epoch train time: 0:00:02.142601
elapsed time: 0:07:26.916822
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 01:03:44.653071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.00
 ---- batch: 020 ----
mean loss: 309.53
 ---- batch: 030 ----
mean loss: 303.99
 ---- batch: 040 ----
mean loss: 315.21
 ---- batch: 050 ----
mean loss: 308.04
 ---- batch: 060 ----
mean loss: 299.09
 ---- batch: 070 ----
mean loss: 292.52
 ---- batch: 080 ----
mean loss: 295.73
 ---- batch: 090 ----
mean loss: 302.70
 ---- batch: 100 ----
mean loss: 290.14
 ---- batch: 110 ----
mean loss: 313.27
train mean loss: 302.23
epoch train time: 0:00:02.140488
elapsed time: 0:07:29.057487
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 01:03:46.793739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.93
 ---- batch: 020 ----
mean loss: 299.20
 ---- batch: 030 ----
mean loss: 308.63
 ---- batch: 040 ----
mean loss: 310.54
 ---- batch: 050 ----
mean loss: 301.38
 ---- batch: 060 ----
mean loss: 295.36
 ---- batch: 070 ----
mean loss: 309.97
 ---- batch: 080 ----
mean loss: 302.28
 ---- batch: 090 ----
mean loss: 307.62
 ---- batch: 100 ----
mean loss: 294.15
 ---- batch: 110 ----
mean loss: 295.52
train mean loss: 301.84
epoch train time: 0:00:02.142486
elapsed time: 0:07:31.200132
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 01:03:48.936376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.88
 ---- batch: 020 ----
mean loss: 304.19
 ---- batch: 030 ----
mean loss: 314.98
 ---- batch: 040 ----
mean loss: 291.23
 ---- batch: 050 ----
mean loss: 307.43
 ---- batch: 060 ----
mean loss: 294.43
 ---- batch: 070 ----
mean loss: 302.99
 ---- batch: 080 ----
mean loss: 297.69
 ---- batch: 090 ----
mean loss: 303.49
 ---- batch: 100 ----
mean loss: 300.13
 ---- batch: 110 ----
mean loss: 287.50
train mean loss: 300.30
epoch train time: 0:00:02.141450
elapsed time: 0:07:33.341743
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 01:03:51.077990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.43
 ---- batch: 020 ----
mean loss: 291.53
 ---- batch: 030 ----
mean loss: 295.29
 ---- batch: 040 ----
mean loss: 300.83
 ---- batch: 050 ----
mean loss: 303.56
 ---- batch: 060 ----
mean loss: 314.76
 ---- batch: 070 ----
mean loss: 294.82
 ---- batch: 080 ----
mean loss: 299.43
 ---- batch: 090 ----
mean loss: 298.75
 ---- batch: 100 ----
mean loss: 295.29
 ---- batch: 110 ----
mean loss: 302.44
train mean loss: 300.38
epoch train time: 0:00:02.142883
elapsed time: 0:07:35.484782
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 01:03:53.221048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.07
 ---- batch: 020 ----
mean loss: 298.53
 ---- batch: 030 ----
mean loss: 300.57
 ---- batch: 040 ----
mean loss: 293.58
 ---- batch: 050 ----
mean loss: 293.47
 ---- batch: 060 ----
mean loss: 306.06
 ---- batch: 070 ----
mean loss: 300.96
 ---- batch: 080 ----
mean loss: 304.68
 ---- batch: 090 ----
mean loss: 296.64
 ---- batch: 100 ----
mean loss: 295.32
 ---- batch: 110 ----
mean loss: 296.92
train mean loss: 298.90
epoch train time: 0:00:02.136753
elapsed time: 0:07:37.621707
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 01:03:55.357966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.12
 ---- batch: 020 ----
mean loss: 299.15
 ---- batch: 030 ----
mean loss: 297.49
 ---- batch: 040 ----
mean loss: 298.23
 ---- batch: 050 ----
mean loss: 304.84
 ---- batch: 060 ----
mean loss: 305.14
 ---- batch: 070 ----
mean loss: 294.91
 ---- batch: 080 ----
mean loss: 287.44
 ---- batch: 090 ----
mean loss: 292.09
 ---- batch: 100 ----
mean loss: 294.56
 ---- batch: 110 ----
mean loss: 311.95
train mean loss: 298.40
epoch train time: 0:00:02.137981
elapsed time: 0:07:39.759858
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 01:03:57.496110
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.73
 ---- batch: 020 ----
mean loss: 289.68
 ---- batch: 030 ----
mean loss: 299.52
 ---- batch: 040 ----
mean loss: 291.63
 ---- batch: 050 ----
mean loss: 284.98
 ---- batch: 060 ----
mean loss: 298.52
 ---- batch: 070 ----
mean loss: 291.85
 ---- batch: 080 ----
mean loss: 295.89
 ---- batch: 090 ----
mean loss: 295.44
 ---- batch: 100 ----
mean loss: 286.98
 ---- batch: 110 ----
mean loss: 286.42
train mean loss: 292.35
epoch train time: 0:00:02.140219
elapsed time: 0:07:41.900259
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 01:03:59.636489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.51
 ---- batch: 020 ----
mean loss: 286.55
 ---- batch: 030 ----
mean loss: 285.06
 ---- batch: 040 ----
mean loss: 295.13
 ---- batch: 050 ----
mean loss: 290.17
 ---- batch: 060 ----
mean loss: 292.48
 ---- batch: 070 ----
mean loss: 285.59
 ---- batch: 080 ----
mean loss: 297.78
 ---- batch: 090 ----
mean loss: 305.74
 ---- batch: 100 ----
mean loss: 280.55
 ---- batch: 110 ----
mean loss: 286.04
train mean loss: 291.68
epoch train time: 0:00:02.143453
elapsed time: 0:07:44.043851
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 01:04:01.780093
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.19
 ---- batch: 020 ----
mean loss: 289.78
 ---- batch: 030 ----
mean loss: 293.56
 ---- batch: 040 ----
mean loss: 295.41
 ---- batch: 050 ----
mean loss: 293.21
 ---- batch: 060 ----
mean loss: 294.42
 ---- batch: 070 ----
mean loss: 285.08
 ---- batch: 080 ----
mean loss: 292.92
 ---- batch: 090 ----
mean loss: 288.20
 ---- batch: 100 ----
mean loss: 294.28
 ---- batch: 110 ----
mean loss: 288.16
train mean loss: 290.84
epoch train time: 0:00:02.140866
elapsed time: 0:07:46.184884
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 01:04:03.921128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.20
 ---- batch: 020 ----
mean loss: 286.60
 ---- batch: 030 ----
mean loss: 288.04
 ---- batch: 040 ----
mean loss: 287.14
 ---- batch: 050 ----
mean loss: 290.21
 ---- batch: 060 ----
mean loss: 292.90
 ---- batch: 070 ----
mean loss: 302.26
 ---- batch: 080 ----
mean loss: 288.86
 ---- batch: 090 ----
mean loss: 294.29
 ---- batch: 100 ----
mean loss: 284.65
 ---- batch: 110 ----
mean loss: 285.51
train mean loss: 289.56
epoch train time: 0:00:02.140062
elapsed time: 0:07:48.325113
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 01:04:06.061353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.32
 ---- batch: 020 ----
mean loss: 278.07
 ---- batch: 030 ----
mean loss: 294.72
 ---- batch: 040 ----
mean loss: 293.97
 ---- batch: 050 ----
mean loss: 286.53
 ---- batch: 060 ----
mean loss: 293.04
 ---- batch: 070 ----
mean loss: 294.67
 ---- batch: 080 ----
mean loss: 292.96
 ---- batch: 090 ----
mean loss: 294.42
 ---- batch: 100 ----
mean loss: 277.53
 ---- batch: 110 ----
mean loss: 296.34
train mean loss: 291.07
epoch train time: 0:00:02.138160
elapsed time: 0:07:50.463419
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 01:04:08.199677
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 292.54
 ---- batch: 020 ----
mean loss: 281.58
 ---- batch: 030 ----
mean loss: 287.33
 ---- batch: 040 ----
mean loss: 295.06
 ---- batch: 050 ----
mean loss: 288.87
 ---- batch: 060 ----
mean loss: 297.45
 ---- batch: 070 ----
mean loss: 296.97
 ---- batch: 080 ----
mean loss: 297.31
 ---- batch: 090 ----
mean loss: 289.37
 ---- batch: 100 ----
mean loss: 285.89
 ---- batch: 110 ----
mean loss: 289.88
train mean loss: 291.43
epoch train time: 0:00:02.145269
elapsed time: 0:07:52.608849
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 01:04:10.345092
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.60
 ---- batch: 020 ----
mean loss: 290.56
 ---- batch: 030 ----
mean loss: 288.98
 ---- batch: 040 ----
mean loss: 299.04
 ---- batch: 050 ----
mean loss: 282.56
 ---- batch: 060 ----
mean loss: 289.72
 ---- batch: 070 ----
mean loss: 283.96
 ---- batch: 080 ----
mean loss: 298.04
 ---- batch: 090 ----
mean loss: 288.50
 ---- batch: 100 ----
mean loss: 300.30
 ---- batch: 110 ----
mean loss: 285.76
train mean loss: 290.40
epoch train time: 0:00:02.138246
elapsed time: 0:07:54.747242
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 01:04:12.483483
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 293.00
 ---- batch: 020 ----
mean loss: 292.33
 ---- batch: 030 ----
mean loss: 294.08
 ---- batch: 040 ----
mean loss: 296.54
 ---- batch: 050 ----
mean loss: 282.34
 ---- batch: 060 ----
mean loss: 296.85
 ---- batch: 070 ----
mean loss: 295.34
 ---- batch: 080 ----
mean loss: 290.24
 ---- batch: 090 ----
mean loss: 288.03
 ---- batch: 100 ----
mean loss: 286.07
 ---- batch: 110 ----
mean loss: 290.38
train mean loss: 290.94
epoch train time: 0:00:02.132060
elapsed time: 0:07:56.879475
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 01:04:14.615729
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.52
 ---- batch: 020 ----
mean loss: 279.13
 ---- batch: 030 ----
mean loss: 295.42
 ---- batch: 040 ----
mean loss: 281.45
 ---- batch: 050 ----
mean loss: 291.24
 ---- batch: 060 ----
mean loss: 289.99
 ---- batch: 070 ----
mean loss: 293.22
 ---- batch: 080 ----
mean loss: 286.97
 ---- batch: 090 ----
mean loss: 298.98
 ---- batch: 100 ----
mean loss: 291.43
 ---- batch: 110 ----
mean loss: 303.30
train mean loss: 290.62
epoch train time: 0:00:02.137902
elapsed time: 0:07:59.017545
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 01:04:16.753795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.20
 ---- batch: 020 ----
mean loss: 285.17
 ---- batch: 030 ----
mean loss: 297.07
 ---- batch: 040 ----
mean loss: 299.64
 ---- batch: 050 ----
mean loss: 295.12
 ---- batch: 060 ----
mean loss: 289.23
 ---- batch: 070 ----
mean loss: 283.51
 ---- batch: 080 ----
mean loss: 285.89
 ---- batch: 090 ----
mean loss: 283.67
 ---- batch: 100 ----
mean loss: 288.22
 ---- batch: 110 ----
mean loss: 286.95
train mean loss: 290.50
epoch train time: 0:00:02.137168
elapsed time: 0:08:01.154879
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 01:04:18.891123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.85
 ---- batch: 020 ----
mean loss: 293.35
 ---- batch: 030 ----
mean loss: 297.66
 ---- batch: 040 ----
mean loss: 295.33
 ---- batch: 050 ----
mean loss: 292.91
 ---- batch: 060 ----
mean loss: 282.93
 ---- batch: 070 ----
mean loss: 277.61
 ---- batch: 080 ----
mean loss: 281.31
 ---- batch: 090 ----
mean loss: 291.87
 ---- batch: 100 ----
mean loss: 275.13
 ---- batch: 110 ----
mean loss: 287.44
train mean loss: 288.86
epoch train time: 0:00:02.144902
elapsed time: 0:08:03.299934
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 01:04:21.036202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.61
 ---- batch: 020 ----
mean loss: 284.31
 ---- batch: 030 ----
mean loss: 288.49
 ---- batch: 040 ----
mean loss: 293.60
 ---- batch: 050 ----
mean loss: 286.37
 ---- batch: 060 ----
mean loss: 297.13
 ---- batch: 070 ----
mean loss: 292.57
 ---- batch: 080 ----
mean loss: 292.01
 ---- batch: 090 ----
mean loss: 284.94
 ---- batch: 100 ----
mean loss: 293.03
 ---- batch: 110 ----
mean loss: 297.82
train mean loss: 290.90
epoch train time: 0:00:02.141538
elapsed time: 0:08:05.441648
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 01:04:23.177900
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.76
 ---- batch: 020 ----
mean loss: 285.65
 ---- batch: 030 ----
mean loss: 284.13
 ---- batch: 040 ----
mean loss: 291.28
 ---- batch: 050 ----
mean loss: 280.39
 ---- batch: 060 ----
mean loss: 301.73
 ---- batch: 070 ----
mean loss: 277.78
 ---- batch: 080 ----
mean loss: 283.18
 ---- batch: 090 ----
mean loss: 281.01
 ---- batch: 100 ----
mean loss: 299.74
 ---- batch: 110 ----
mean loss: 295.96
train mean loss: 288.66
epoch train time: 0:00:02.144811
elapsed time: 0:08:07.586619
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 01:04:25.322862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.56
 ---- batch: 020 ----
mean loss: 296.44
 ---- batch: 030 ----
mean loss: 283.61
 ---- batch: 040 ----
mean loss: 278.59
 ---- batch: 050 ----
mean loss: 293.05
 ---- batch: 060 ----
mean loss: 284.15
 ---- batch: 070 ----
mean loss: 291.97
 ---- batch: 080 ----
mean loss: 292.32
 ---- batch: 090 ----
mean loss: 285.17
 ---- batch: 100 ----
mean loss: 283.30
 ---- batch: 110 ----
mean loss: 301.36
train mean loss: 289.35
epoch train time: 0:00:02.139880
elapsed time: 0:08:09.726645
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 01:04:27.462886
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.03
 ---- batch: 020 ----
mean loss: 295.26
 ---- batch: 030 ----
mean loss: 297.26
 ---- batch: 040 ----
mean loss: 293.45
 ---- batch: 050 ----
mean loss: 287.33
 ---- batch: 060 ----
mean loss: 282.23
 ---- batch: 070 ----
mean loss: 301.54
 ---- batch: 080 ----
mean loss: 282.14
 ---- batch: 090 ----
mean loss: 281.13
 ---- batch: 100 ----
mean loss: 288.14
 ---- batch: 110 ----
mean loss: 300.29
train mean loss: 290.86
epoch train time: 0:00:02.139268
elapsed time: 0:08:11.866058
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 01:04:29.602317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.98
 ---- batch: 020 ----
mean loss: 289.63
 ---- batch: 030 ----
mean loss: 293.33
 ---- batch: 040 ----
mean loss: 285.82
 ---- batch: 050 ----
mean loss: 292.14
 ---- batch: 060 ----
mean loss: 288.76
 ---- batch: 070 ----
mean loss: 293.54
 ---- batch: 080 ----
mean loss: 287.51
 ---- batch: 090 ----
mean loss: 291.17
 ---- batch: 100 ----
mean loss: 289.01
 ---- batch: 110 ----
mean loss: 288.84
train mean loss: 289.78
epoch train time: 0:00:02.138400
elapsed time: 0:08:14.004667
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 01:04:31.740941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.83
 ---- batch: 020 ----
mean loss: 284.31
 ---- batch: 030 ----
mean loss: 286.76
 ---- batch: 040 ----
mean loss: 291.50
 ---- batch: 050 ----
mean loss: 289.66
 ---- batch: 060 ----
mean loss: 290.47
 ---- batch: 070 ----
mean loss: 287.55
 ---- batch: 080 ----
mean loss: 285.42
 ---- batch: 090 ----
mean loss: 294.57
 ---- batch: 100 ----
mean loss: 295.51
 ---- batch: 110 ----
mean loss: 297.99
train mean loss: 290.04
epoch train time: 0:00:02.144289
elapsed time: 0:08:16.149170
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 01:04:33.885484
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.13
 ---- batch: 020 ----
mean loss: 294.05
 ---- batch: 030 ----
mean loss: 297.00
 ---- batch: 040 ----
mean loss: 288.08
 ---- batch: 050 ----
mean loss: 288.26
 ---- batch: 060 ----
mean loss: 294.22
 ---- batch: 070 ----
mean loss: 294.68
 ---- batch: 080 ----
mean loss: 295.67
 ---- batch: 090 ----
mean loss: 278.76
 ---- batch: 100 ----
mean loss: 287.57
 ---- batch: 110 ----
mean loss: 281.87
train mean loss: 290.11
epoch train time: 0:00:02.144061
elapsed time: 0:08:18.293470
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 01:04:36.029732
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.96
 ---- batch: 020 ----
mean loss: 297.38
 ---- batch: 030 ----
mean loss: 293.91
 ---- batch: 040 ----
mean loss: 295.66
 ---- batch: 050 ----
mean loss: 286.51
 ---- batch: 060 ----
mean loss: 286.86
 ---- batch: 070 ----
mean loss: 289.24
 ---- batch: 080 ----
mean loss: 297.14
 ---- batch: 090 ----
mean loss: 294.76
 ---- batch: 100 ----
mean loss: 289.98
 ---- batch: 110 ----
mean loss: 287.76
train mean loss: 290.85
epoch train time: 0:00:02.135385
elapsed time: 0:08:20.429036
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 01:04:38.165277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.31
 ---- batch: 020 ----
mean loss: 290.98
 ---- batch: 030 ----
mean loss: 298.11
 ---- batch: 040 ----
mean loss: 294.04
 ---- batch: 050 ----
mean loss: 291.19
 ---- batch: 060 ----
mean loss: 287.67
 ---- batch: 070 ----
mean loss: 291.27
 ---- batch: 080 ----
mean loss: 290.77
 ---- batch: 090 ----
mean loss: 285.87
 ---- batch: 100 ----
mean loss: 286.22
 ---- batch: 110 ----
mean loss: 280.75
train mean loss: 289.19
epoch train time: 0:00:02.131109
elapsed time: 0:08:22.560305
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 01:04:40.296550
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.15
 ---- batch: 020 ----
mean loss: 291.96
 ---- batch: 030 ----
mean loss: 302.98
 ---- batch: 040 ----
mean loss: 293.93
 ---- batch: 050 ----
mean loss: 280.89
 ---- batch: 060 ----
mean loss: 278.42
 ---- batch: 070 ----
mean loss: 299.14
 ---- batch: 080 ----
mean loss: 298.35
 ---- batch: 090 ----
mean loss: 289.60
 ---- batch: 100 ----
mean loss: 285.55
 ---- batch: 110 ----
mean loss: 295.37
train mean loss: 290.96
epoch train time: 0:00:02.139407
elapsed time: 0:08:24.699870
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 01:04:42.436113
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.55
 ---- batch: 020 ----
mean loss: 284.85
 ---- batch: 030 ----
mean loss: 293.46
 ---- batch: 040 ----
mean loss: 286.14
 ---- batch: 050 ----
mean loss: 287.70
 ---- batch: 060 ----
mean loss: 302.67
 ---- batch: 070 ----
mean loss: 280.09
 ---- batch: 080 ----
mean loss: 291.17
 ---- batch: 090 ----
mean loss: 280.60
 ---- batch: 100 ----
mean loss: 284.58
 ---- batch: 110 ----
mean loss: 283.93
train mean loss: 288.38
epoch train time: 0:00:02.147122
elapsed time: 0:08:26.847162
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 01:04:44.583433
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.60
 ---- batch: 020 ----
mean loss: 281.41
 ---- batch: 030 ----
mean loss: 286.19
 ---- batch: 040 ----
mean loss: 287.22
 ---- batch: 050 ----
mean loss: 297.35
 ---- batch: 060 ----
mean loss: 287.44
 ---- batch: 070 ----
mean loss: 285.57
 ---- batch: 080 ----
mean loss: 294.79
 ---- batch: 090 ----
mean loss: 291.97
 ---- batch: 100 ----
mean loss: 293.79
 ---- batch: 110 ----
mean loss: 280.02
train mean loss: 288.99
epoch train time: 0:00:02.149798
elapsed time: 0:08:28.997143
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 01:04:46.733387
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.47
 ---- batch: 020 ----
mean loss: 296.28
 ---- batch: 030 ----
mean loss: 289.15
 ---- batch: 040 ----
mean loss: 287.69
 ---- batch: 050 ----
mean loss: 291.15
 ---- batch: 060 ----
mean loss: 292.23
 ---- batch: 070 ----
mean loss: 287.28
 ---- batch: 080 ----
mean loss: 288.09
 ---- batch: 090 ----
mean loss: 289.84
 ---- batch: 100 ----
mean loss: 272.00
 ---- batch: 110 ----
mean loss: 288.74
train mean loss: 288.39
epoch train time: 0:00:02.150085
elapsed time: 0:08:31.147377
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 01:04:48.883638
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 293.22
 ---- batch: 020 ----
mean loss: 274.62
 ---- batch: 030 ----
mean loss: 294.93
 ---- batch: 040 ----
mean loss: 297.63
 ---- batch: 050 ----
mean loss: 286.96
 ---- batch: 060 ----
mean loss: 294.69
 ---- batch: 070 ----
mean loss: 284.58
 ---- batch: 080 ----
mean loss: 277.09
 ---- batch: 090 ----
mean loss: 291.80
 ---- batch: 100 ----
mean loss: 289.63
 ---- batch: 110 ----
mean loss: 279.11
train mean loss: 288.25
epoch train time: 0:00:02.162229
elapsed time: 0:08:33.309803
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 01:04:51.046047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.91
 ---- batch: 020 ----
mean loss: 297.36
 ---- batch: 030 ----
mean loss: 289.94
 ---- batch: 040 ----
mean loss: 278.97
 ---- batch: 050 ----
mean loss: 292.77
 ---- batch: 060 ----
mean loss: 272.39
 ---- batch: 070 ----
mean loss: 298.89
 ---- batch: 080 ----
mean loss: 285.56
 ---- batch: 090 ----
mean loss: 305.98
 ---- batch: 100 ----
mean loss: 292.87
 ---- batch: 110 ----
mean loss: 287.90
train mean loss: 289.85
epoch train time: 0:00:02.151314
elapsed time: 0:08:35.461272
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 01:04:53.197540
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.96
 ---- batch: 020 ----
mean loss: 279.92
 ---- batch: 030 ----
mean loss: 287.29
 ---- batch: 040 ----
mean loss: 285.74
 ---- batch: 050 ----
mean loss: 284.02
 ---- batch: 060 ----
mean loss: 292.00
 ---- batch: 070 ----
mean loss: 296.45
 ---- batch: 080 ----
mean loss: 288.56
 ---- batch: 090 ----
mean loss: 287.09
 ---- batch: 100 ----
mean loss: 289.25
 ---- batch: 110 ----
mean loss: 285.60
train mean loss: 286.88
epoch train time: 0:00:02.152280
elapsed time: 0:08:37.613730
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 01:04:55.349975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.25
 ---- batch: 020 ----
mean loss: 288.53
 ---- batch: 030 ----
mean loss: 282.21
 ---- batch: 040 ----
mean loss: 293.49
 ---- batch: 050 ----
mean loss: 293.20
 ---- batch: 060 ----
mean loss: 298.68
 ---- batch: 070 ----
mean loss: 292.31
 ---- batch: 080 ----
mean loss: 289.32
 ---- batch: 090 ----
mean loss: 280.11
 ---- batch: 100 ----
mean loss: 288.44
 ---- batch: 110 ----
mean loss: 283.79
train mean loss: 289.29
epoch train time: 0:00:02.214199
elapsed time: 0:08:39.828141
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 01:04:57.564425
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.77
 ---- batch: 020 ----
mean loss: 292.41
 ---- batch: 030 ----
mean loss: 301.52
 ---- batch: 040 ----
mean loss: 291.65
 ---- batch: 050 ----
mean loss: 290.98
 ---- batch: 060 ----
mean loss: 293.05
 ---- batch: 070 ----
mean loss: 301.48
 ---- batch: 080 ----
mean loss: 275.15
 ---- batch: 090 ----
mean loss: 290.91
 ---- batch: 100 ----
mean loss: 291.82
 ---- batch: 110 ----
mean loss: 290.71
train mean loss: 290.90
epoch train time: 0:00:02.230450
elapsed time: 0:08:42.058811
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 01:04:59.795058
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.01
 ---- batch: 020 ----
mean loss: 289.52
 ---- batch: 030 ----
mean loss: 288.20
 ---- batch: 040 ----
mean loss: 296.57
 ---- batch: 050 ----
mean loss: 286.91
 ---- batch: 060 ----
mean loss: 286.06
 ---- batch: 070 ----
mean loss: 289.75
 ---- batch: 080 ----
mean loss: 288.76
 ---- batch: 090 ----
mean loss: 282.14
 ---- batch: 100 ----
mean loss: 284.11
 ---- batch: 110 ----
mean loss: 293.14
train mean loss: 289.64
epoch train time: 0:00:02.136948
elapsed time: 0:08:44.195910
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 01:05:01.932152
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.76
 ---- batch: 020 ----
mean loss: 276.25
 ---- batch: 030 ----
mean loss: 283.29
 ---- batch: 040 ----
mean loss: 298.95
 ---- batch: 050 ----
mean loss: 295.49
 ---- batch: 060 ----
mean loss: 288.36
 ---- batch: 070 ----
mean loss: 288.64
 ---- batch: 080 ----
mean loss: 288.09
 ---- batch: 090 ----
mean loss: 288.41
 ---- batch: 100 ----
mean loss: 291.12
 ---- batch: 110 ----
mean loss: 291.40
train mean loss: 288.33
epoch train time: 0:00:02.135116
elapsed time: 0:08:46.331177
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 01:05:04.067466
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.92
 ---- batch: 020 ----
mean loss: 286.73
 ---- batch: 030 ----
mean loss: 293.42
 ---- batch: 040 ----
mean loss: 288.40
 ---- batch: 050 ----
mean loss: 282.32
 ---- batch: 060 ----
mean loss: 290.27
 ---- batch: 070 ----
mean loss: 284.81
 ---- batch: 080 ----
mean loss: 292.28
 ---- batch: 090 ----
mean loss: 293.29
 ---- batch: 100 ----
mean loss: 290.98
 ---- batch: 110 ----
mean loss: 296.20
train mean loss: 289.22
epoch train time: 0:00:02.135078
elapsed time: 0:08:48.466462
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 01:05:06.202706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.41
 ---- batch: 020 ----
mean loss: 286.89
 ---- batch: 030 ----
mean loss: 289.78
 ---- batch: 040 ----
mean loss: 296.90
 ---- batch: 050 ----
mean loss: 290.88
 ---- batch: 060 ----
mean loss: 290.52
 ---- batch: 070 ----
mean loss: 290.70
 ---- batch: 080 ----
mean loss: 287.22
 ---- batch: 090 ----
mean loss: 289.13
 ---- batch: 100 ----
mean loss: 297.81
 ---- batch: 110 ----
mean loss: 290.60
train mean loss: 289.87
epoch train time: 0:00:02.140501
elapsed time: 0:08:50.607139
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 01:05:08.343371
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.21
 ---- batch: 020 ----
mean loss: 286.35
 ---- batch: 030 ----
mean loss: 278.46
 ---- batch: 040 ----
mean loss: 287.66
 ---- batch: 050 ----
mean loss: 289.20
 ---- batch: 060 ----
mean loss: 293.87
 ---- batch: 070 ----
mean loss: 292.00
 ---- batch: 080 ----
mean loss: 290.57
 ---- batch: 090 ----
mean loss: 298.09
 ---- batch: 100 ----
mean loss: 294.20
 ---- batch: 110 ----
mean loss: 293.76
train mean loss: 291.31
epoch train time: 0:00:02.147069
elapsed time: 0:08:52.754371
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 01:05:10.490618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.92
 ---- batch: 020 ----
mean loss: 280.90
 ---- batch: 030 ----
mean loss: 290.35
 ---- batch: 040 ----
mean loss: 296.07
 ---- batch: 050 ----
mean loss: 282.26
 ---- batch: 060 ----
mean loss: 294.60
 ---- batch: 070 ----
mean loss: 297.99
 ---- batch: 080 ----
mean loss: 283.36
 ---- batch: 090 ----
mean loss: 284.56
 ---- batch: 100 ----
mean loss: 291.57
 ---- batch: 110 ----
mean loss: 284.58
train mean loss: 288.19
epoch train time: 0:00:02.141890
elapsed time: 0:08:54.896417
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 01:05:12.632658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.81
 ---- batch: 020 ----
mean loss: 284.52
 ---- batch: 030 ----
mean loss: 285.85
 ---- batch: 040 ----
mean loss: 288.50
 ---- batch: 050 ----
mean loss: 281.27
 ---- batch: 060 ----
mean loss: 293.79
 ---- batch: 070 ----
mean loss: 298.43
 ---- batch: 080 ----
mean loss: 286.02
 ---- batch: 090 ----
mean loss: 287.86
 ---- batch: 100 ----
mean loss: 282.35
 ---- batch: 110 ----
mean loss: 292.97
train mean loss: 288.42
epoch train time: 0:00:02.141782
elapsed time: 0:08:57.038363
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 01:05:14.774609
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.96
 ---- batch: 020 ----
mean loss: 293.53
 ---- batch: 030 ----
mean loss: 285.24
 ---- batch: 040 ----
mean loss: 286.59
 ---- batch: 050 ----
mean loss: 287.42
 ---- batch: 060 ----
mean loss: 293.33
 ---- batch: 070 ----
mean loss: 281.18
 ---- batch: 080 ----
mean loss: 280.12
 ---- batch: 090 ----
mean loss: 291.02
 ---- batch: 100 ----
mean loss: 287.95
 ---- batch: 110 ----
mean loss: 281.11
train mean loss: 286.84
epoch train time: 0:00:02.137377
elapsed time: 0:08:59.175913
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 01:05:16.912175
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 293.04
 ---- batch: 020 ----
mean loss: 284.27
 ---- batch: 030 ----
mean loss: 293.29
 ---- batch: 040 ----
mean loss: 285.68
 ---- batch: 050 ----
mean loss: 285.16
 ---- batch: 060 ----
mean loss: 288.26
 ---- batch: 070 ----
mean loss: 295.28
 ---- batch: 080 ----
mean loss: 281.20
 ---- batch: 090 ----
mean loss: 278.99
 ---- batch: 100 ----
mean loss: 289.26
 ---- batch: 110 ----
mean loss: 284.62
train mean loss: 286.95
epoch train time: 0:00:02.135238
elapsed time: 0:09:01.311340
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 01:05:19.047583
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.30
 ---- batch: 020 ----
mean loss: 288.49
 ---- batch: 030 ----
mean loss: 291.33
 ---- batch: 040 ----
mean loss: 284.00
 ---- batch: 050 ----
mean loss: 290.96
 ---- batch: 060 ----
mean loss: 300.68
 ---- batch: 070 ----
mean loss: 285.62
 ---- batch: 080 ----
mean loss: 299.62
 ---- batch: 090 ----
mean loss: 288.49
 ---- batch: 100 ----
mean loss: 290.42
 ---- batch: 110 ----
mean loss: 282.41
train mean loss: 290.48
epoch train time: 0:00:02.135662
elapsed time: 0:09:03.447151
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 01:05:21.183395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.54
 ---- batch: 020 ----
mean loss: 279.87
 ---- batch: 030 ----
mean loss: 282.15
 ---- batch: 040 ----
mean loss: 278.67
 ---- batch: 050 ----
mean loss: 285.15
 ---- batch: 060 ----
mean loss: 288.63
 ---- batch: 070 ----
mean loss: 289.76
 ---- batch: 080 ----
mean loss: 290.84
 ---- batch: 090 ----
mean loss: 307.17
 ---- batch: 100 ----
mean loss: 287.33
 ---- batch: 110 ----
mean loss: 303.29
train mean loss: 288.84
epoch train time: 0:00:02.133623
elapsed time: 0:09:05.580924
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 01:05:23.317181
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.01
 ---- batch: 020 ----
mean loss: 286.36
 ---- batch: 030 ----
mean loss: 292.89
 ---- batch: 040 ----
mean loss: 283.98
 ---- batch: 050 ----
mean loss: 286.43
 ---- batch: 060 ----
mean loss: 299.62
 ---- batch: 070 ----
mean loss: 290.28
 ---- batch: 080 ----
mean loss: 288.68
 ---- batch: 090 ----
mean loss: 292.00
 ---- batch: 100 ----
mean loss: 300.16
 ---- batch: 110 ----
mean loss: 285.87
train mean loss: 289.76
epoch train time: 0:00:02.141080
elapsed time: 0:09:07.722170
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 01:05:25.458422
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.02
 ---- batch: 020 ----
mean loss: 300.73
 ---- batch: 030 ----
mean loss: 300.59
 ---- batch: 040 ----
mean loss: 282.30
 ---- batch: 050 ----
mean loss: 285.80
 ---- batch: 060 ----
mean loss: 285.96
 ---- batch: 070 ----
mean loss: 296.53
 ---- batch: 080 ----
mean loss: 273.10
 ---- batch: 090 ----
mean loss: 281.82
 ---- batch: 100 ----
mean loss: 284.19
 ---- batch: 110 ----
mean loss: 286.08
train mean loss: 288.08
epoch train time: 0:00:02.131053
elapsed time: 0:09:09.853381
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 01:05:27.589640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.24
 ---- batch: 020 ----
mean loss: 298.39
 ---- batch: 030 ----
mean loss: 281.99
 ---- batch: 040 ----
mean loss: 305.94
 ---- batch: 050 ----
mean loss: 301.50
 ---- batch: 060 ----
mean loss: 290.01
 ---- batch: 070 ----
mean loss: 297.23
 ---- batch: 080 ----
mean loss: 282.46
 ---- batch: 090 ----
mean loss: 282.19
 ---- batch: 100 ----
mean loss: 287.52
 ---- batch: 110 ----
mean loss: 278.52
train mean loss: 290.01
epoch train time: 0:00:02.137245
elapsed time: 0:09:11.990796
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 01:05:29.727038
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.44
 ---- batch: 020 ----
mean loss: 281.43
 ---- batch: 030 ----
mean loss: 282.43
 ---- batch: 040 ----
mean loss: 289.39
 ---- batch: 050 ----
mean loss: 291.50
 ---- batch: 060 ----
mean loss: 283.11
 ---- batch: 070 ----
mean loss: 290.99
 ---- batch: 080 ----
mean loss: 286.94
 ---- batch: 090 ----
mean loss: 282.65
 ---- batch: 100 ----
mean loss: 287.76
 ---- batch: 110 ----
mean loss: 285.13
train mean loss: 285.29
epoch train time: 0:00:02.135842
elapsed time: 0:09:14.126788
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 01:05:31.863056
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.92
 ---- batch: 020 ----
mean loss: 282.49
 ---- batch: 030 ----
mean loss: 286.91
 ---- batch: 040 ----
mean loss: 294.67
 ---- batch: 050 ----
mean loss: 282.36
 ---- batch: 060 ----
mean loss: 287.98
 ---- batch: 070 ----
mean loss: 285.17
 ---- batch: 080 ----
mean loss: 283.32
 ---- batch: 090 ----
mean loss: 300.18
 ---- batch: 100 ----
mean loss: 282.55
 ---- batch: 110 ----
mean loss: 290.40
train mean loss: 286.68
epoch train time: 0:00:02.150658
elapsed time: 0:09:16.277652
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 01:05:34.013895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.54
 ---- batch: 020 ----
mean loss: 291.13
 ---- batch: 030 ----
mean loss: 291.67
 ---- batch: 040 ----
mean loss: 291.97
 ---- batch: 050 ----
mean loss: 286.84
 ---- batch: 060 ----
mean loss: 282.57
 ---- batch: 070 ----
mean loss: 284.08
 ---- batch: 080 ----
mean loss: 287.61
 ---- batch: 090 ----
mean loss: 286.52
 ---- batch: 100 ----
mean loss: 290.60
 ---- batch: 110 ----
mean loss: 284.55
train mean loss: 288.55
epoch train time: 0:00:02.143784
elapsed time: 0:09:18.421587
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 01:05:36.157832
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 274.55
 ---- batch: 020 ----
mean loss: 288.31
 ---- batch: 030 ----
mean loss: 288.29
 ---- batch: 040 ----
mean loss: 285.71
 ---- batch: 050 ----
mean loss: 289.54
 ---- batch: 060 ----
mean loss: 289.84
 ---- batch: 070 ----
mean loss: 289.13
 ---- batch: 080 ----
mean loss: 281.98
 ---- batch: 090 ----
mean loss: 277.23
 ---- batch: 100 ----
mean loss: 292.49
 ---- batch: 110 ----
mean loss: 290.37
train mean loss: 285.56
epoch train time: 0:00:02.144037
elapsed time: 0:09:20.565785
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 01:05:38.302028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.61
 ---- batch: 020 ----
mean loss: 285.00
 ---- batch: 030 ----
mean loss: 285.69
 ---- batch: 040 ----
mean loss: 288.88
 ---- batch: 050 ----
mean loss: 290.47
 ---- batch: 060 ----
mean loss: 285.54
 ---- batch: 070 ----
mean loss: 285.56
 ---- batch: 080 ----
mean loss: 288.20
 ---- batch: 090 ----
mean loss: 284.56
 ---- batch: 100 ----
mean loss: 292.83
 ---- batch: 110 ----
mean loss: 289.69
train mean loss: 287.34
epoch train time: 0:00:02.143478
elapsed time: 0:09:22.709409
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 01:05:40.445696
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.92
 ---- batch: 020 ----
mean loss: 283.01
 ---- batch: 030 ----
mean loss: 284.57
 ---- batch: 040 ----
mean loss: 290.32
 ---- batch: 050 ----
mean loss: 286.99
 ---- batch: 060 ----
mean loss: 293.64
 ---- batch: 070 ----
mean loss: 289.60
 ---- batch: 080 ----
mean loss: 289.10
 ---- batch: 090 ----
mean loss: 292.66
 ---- batch: 100 ----
mean loss: 299.42
 ---- batch: 110 ----
mean loss: 285.36
train mean loss: 288.45
epoch train time: 0:00:02.142565
elapsed time: 0:09:24.855559
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_7/checkpoint.pth.tar
**** end time: 2019-09-27 01:05:42.591767 ****
