Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 19778
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-25 19:10:36.066340 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 16, 11]           1,120
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
    BayesianConv2d-4            [-1, 14, 7, 11]             448
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
    BayesianLinear-8                    [-1, 1]             924
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,492
Trainable params: 2,492
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 19:10:36.077887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4212.03
 ---- batch: 020 ----
mean loss: 3972.69
 ---- batch: 030 ----
mean loss: 3829.29
 ---- batch: 040 ----
mean loss: 3578.36
 ---- batch: 050 ----
mean loss: 3276.06
 ---- batch: 060 ----
mean loss: 3128.03
 ---- batch: 070 ----
mean loss: 2847.53
 ---- batch: 080 ----
mean loss: 2671.46
 ---- batch: 090 ----
mean loss: 2443.13
train mean loss: 3262.71
epoch train time: 0:00:36.614445
elapsed time: 0:00:36.629323
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 19:11:12.695704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2083.99
 ---- batch: 020 ----
mean loss: 1948.90
 ---- batch: 030 ----
mean loss: 1772.85
 ---- batch: 040 ----
mean loss: 1625.47
 ---- batch: 050 ----
mean loss: 1453.91
 ---- batch: 060 ----
mean loss: 1356.09
 ---- batch: 070 ----
mean loss: 1252.80
 ---- batch: 080 ----
mean loss: 1170.18
 ---- batch: 090 ----
mean loss: 1126.15
train mean loss: 1500.99
epoch train time: 0:00:02.655788
elapsed time: 0:00:39.285474
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 19:11:15.352003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1027.65
 ---- batch: 020 ----
mean loss: 973.83
 ---- batch: 030 ----
mean loss: 961.38
 ---- batch: 040 ----
mean loss: 951.38
 ---- batch: 050 ----
mean loss: 941.36
 ---- batch: 060 ----
mean loss: 908.11
 ---- batch: 070 ----
mean loss: 914.17
 ---- batch: 080 ----
mean loss: 882.93
 ---- batch: 090 ----
mean loss: 898.84
train mean loss: 937.09
epoch train time: 0:00:02.699254
elapsed time: 0:00:41.985183
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 19:11:18.051703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.26
 ---- batch: 020 ----
mean loss: 883.43
 ---- batch: 030 ----
mean loss: 886.81
 ---- batch: 040 ----
mean loss: 895.95
 ---- batch: 050 ----
mean loss: 879.36
 ---- batch: 060 ----
mean loss: 881.87
 ---- batch: 070 ----
mean loss: 901.22
 ---- batch: 080 ----
mean loss: 894.98
 ---- batch: 090 ----
mean loss: 875.82
train mean loss: 889.43
epoch train time: 0:00:02.678644
elapsed time: 0:00:44.664334
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 19:11:20.730845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.87
 ---- batch: 020 ----
mean loss: 872.62
 ---- batch: 030 ----
mean loss: 883.86
 ---- batch: 040 ----
mean loss: 888.29
 ---- batch: 050 ----
mean loss: 874.11
 ---- batch: 060 ----
mean loss: 884.13
 ---- batch: 070 ----
mean loss: 899.18
 ---- batch: 080 ----
mean loss: 887.76
 ---- batch: 090 ----
mean loss: 875.49
train mean loss: 882.77
epoch train time: 0:00:02.699011
elapsed time: 0:00:47.363812
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 19:11:23.430369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.71
 ---- batch: 020 ----
mean loss: 881.50
 ---- batch: 030 ----
mean loss: 890.62
 ---- batch: 040 ----
mean loss: 877.54
 ---- batch: 050 ----
mean loss: 875.76
 ---- batch: 060 ----
mean loss: 857.47
 ---- batch: 070 ----
mean loss: 889.85
 ---- batch: 080 ----
mean loss: 880.95
 ---- batch: 090 ----
mean loss: 868.90
train mean loss: 879.71
epoch train time: 0:00:02.680602
elapsed time: 0:00:50.044973
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 19:11:26.111591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.07
 ---- batch: 020 ----
mean loss: 880.44
 ---- batch: 030 ----
mean loss: 884.85
 ---- batch: 040 ----
mean loss: 889.02
 ---- batch: 050 ----
mean loss: 885.93
 ---- batch: 060 ----
mean loss: 874.00
 ---- batch: 070 ----
mean loss: 872.61
 ---- batch: 080 ----
mean loss: 867.65
 ---- batch: 090 ----
mean loss: 864.56
train mean loss: 875.96
epoch train time: 0:00:02.671481
elapsed time: 0:00:52.717046
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 19:11:28.783562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.10
 ---- batch: 020 ----
mean loss: 878.28
 ---- batch: 030 ----
mean loss: 907.52
 ---- batch: 040 ----
mean loss: 878.49
 ---- batch: 050 ----
mean loss: 867.91
 ---- batch: 060 ----
mean loss: 862.16
 ---- batch: 070 ----
mean loss: 884.31
 ---- batch: 080 ----
mean loss: 865.23
 ---- batch: 090 ----
mean loss: 845.56
train mean loss: 870.58
epoch train time: 0:00:02.669743
elapsed time: 0:00:55.387251
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 19:11:31.453854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.88
 ---- batch: 020 ----
mean loss: 861.13
 ---- batch: 030 ----
mean loss: 869.03
 ---- batch: 040 ----
mean loss: 893.75
 ---- batch: 050 ----
mean loss: 847.12
 ---- batch: 060 ----
mean loss: 860.33
 ---- batch: 070 ----
mean loss: 854.20
 ---- batch: 080 ----
mean loss: 852.00
 ---- batch: 090 ----
mean loss: 871.21
train mean loss: 864.13
epoch train time: 0:00:02.657455
elapsed time: 0:00:58.045313
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 19:11:34.111827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.97
 ---- batch: 020 ----
mean loss: 864.20
 ---- batch: 030 ----
mean loss: 836.30
 ---- batch: 040 ----
mean loss: 853.99
 ---- batch: 050 ----
mean loss: 872.05
 ---- batch: 060 ----
mean loss: 871.59
 ---- batch: 070 ----
mean loss: 860.96
 ---- batch: 080 ----
mean loss: 846.78
 ---- batch: 090 ----
mean loss: 852.86
train mean loss: 858.02
epoch train time: 0:00:02.668965
elapsed time: 0:01:00.714721
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 19:11:36.781232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.48
 ---- batch: 020 ----
mean loss: 867.31
 ---- batch: 030 ----
mean loss: 856.85
 ---- batch: 040 ----
mean loss: 861.53
 ---- batch: 050 ----
mean loss: 848.12
 ---- batch: 060 ----
mean loss: 857.90
 ---- batch: 070 ----
mean loss: 853.01
 ---- batch: 080 ----
mean loss: 838.01
 ---- batch: 090 ----
mean loss: 853.89
train mean loss: 852.13
epoch train time: 0:00:02.664731
elapsed time: 0:01:03.379930
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 19:11:39.446429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.84
 ---- batch: 020 ----
mean loss: 839.37
 ---- batch: 030 ----
mean loss: 854.38
 ---- batch: 040 ----
mean loss: 870.70
 ---- batch: 050 ----
mean loss: 864.48
 ---- batch: 060 ----
mean loss: 850.18
 ---- batch: 070 ----
mean loss: 858.85
 ---- batch: 080 ----
mean loss: 843.77
 ---- batch: 090 ----
mean loss: 835.64
train mean loss: 845.86
epoch train time: 0:00:02.659803
elapsed time: 0:01:06.040222
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 19:11:42.106762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.15
 ---- batch: 020 ----
mean loss: 843.45
 ---- batch: 030 ----
mean loss: 848.80
 ---- batch: 040 ----
mean loss: 830.27
 ---- batch: 050 ----
mean loss: 848.47
 ---- batch: 060 ----
mean loss: 845.77
 ---- batch: 070 ----
mean loss: 818.43
 ---- batch: 080 ----
mean loss: 834.57
 ---- batch: 090 ----
mean loss: 852.47
train mean loss: 840.63
epoch train time: 0:00:02.668661
elapsed time: 0:01:08.709343
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 19:11:44.775872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.31
 ---- batch: 020 ----
mean loss: 840.29
 ---- batch: 030 ----
mean loss: 838.77
 ---- batch: 040 ----
mean loss: 811.98
 ---- batch: 050 ----
mean loss: 840.38
 ---- batch: 060 ----
mean loss: 826.52
 ---- batch: 070 ----
mean loss: 850.70
 ---- batch: 080 ----
mean loss: 829.73
 ---- batch: 090 ----
mean loss: 830.72
train mean loss: 834.41
epoch train time: 0:00:02.649532
elapsed time: 0:01:11.359363
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 19:11:47.425873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.50
 ---- batch: 020 ----
mean loss: 831.36
 ---- batch: 030 ----
mean loss: 835.19
 ---- batch: 040 ----
mean loss: 825.85
 ---- batch: 050 ----
mean loss: 819.09
 ---- batch: 060 ----
mean loss: 819.50
 ---- batch: 070 ----
mean loss: 820.95
 ---- batch: 080 ----
mean loss: 839.16
 ---- batch: 090 ----
mean loss: 834.10
train mean loss: 829.97
epoch train time: 0:00:02.666175
elapsed time: 0:01:14.026017
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 19:11:50.092559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 819.82
 ---- batch: 020 ----
mean loss: 828.89
 ---- batch: 030 ----
mean loss: 819.44
 ---- batch: 040 ----
mean loss: 834.82
 ---- batch: 050 ----
mean loss: 833.29
 ---- batch: 060 ----
mean loss: 815.25
 ---- batch: 070 ----
mean loss: 804.79
 ---- batch: 080 ----
mean loss: 817.37
 ---- batch: 090 ----
mean loss: 823.60
train mean loss: 822.75
epoch train time: 0:00:02.664459
elapsed time: 0:01:16.691042
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 19:11:52.757559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.02
 ---- batch: 020 ----
mean loss: 802.19
 ---- batch: 030 ----
mean loss: 816.39
 ---- batch: 040 ----
mean loss: 826.45
 ---- batch: 050 ----
mean loss: 803.45
 ---- batch: 060 ----
mean loss: 823.44
 ---- batch: 070 ----
mean loss: 834.39
 ---- batch: 080 ----
mean loss: 832.57
 ---- batch: 090 ----
mean loss: 802.65
train mean loss: 819.48
epoch train time: 0:00:02.661583
elapsed time: 0:01:19.353133
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 19:11:55.419650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.07
 ---- batch: 020 ----
mean loss: 808.10
 ---- batch: 030 ----
mean loss: 825.81
 ---- batch: 040 ----
mean loss: 824.91
 ---- batch: 050 ----
mean loss: 796.14
 ---- batch: 060 ----
mean loss: 797.04
 ---- batch: 070 ----
mean loss: 814.25
 ---- batch: 080 ----
mean loss: 808.05
 ---- batch: 090 ----
mean loss: 806.29
train mean loss: 812.36
epoch train time: 0:00:02.665497
elapsed time: 0:01:22.019113
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 19:11:58.085661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 818.12
 ---- batch: 020 ----
mean loss: 817.01
 ---- batch: 030 ----
mean loss: 793.11
 ---- batch: 040 ----
mean loss: 811.00
 ---- batch: 050 ----
mean loss: 809.05
 ---- batch: 060 ----
mean loss: 804.91
 ---- batch: 070 ----
mean loss: 788.87
 ---- batch: 080 ----
mean loss: 815.31
 ---- batch: 090 ----
mean loss: 812.89
train mean loss: 806.76
epoch train time: 0:00:02.685681
elapsed time: 0:01:24.705277
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 19:12:00.771796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 801.07
 ---- batch: 020 ----
mean loss: 815.65
 ---- batch: 030 ----
mean loss: 804.32
 ---- batch: 040 ----
mean loss: 807.76
 ---- batch: 050 ----
mean loss: 788.32
 ---- batch: 060 ----
mean loss: 803.61
 ---- batch: 070 ----
mean loss: 802.38
 ---- batch: 080 ----
mean loss: 788.27
 ---- batch: 090 ----
mean loss: 790.74
train mean loss: 800.41
epoch train time: 0:00:02.654707
elapsed time: 0:01:27.360455
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 19:12:03.426963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 799.35
 ---- batch: 020 ----
mean loss: 791.12
 ---- batch: 030 ----
mean loss: 785.45
 ---- batch: 040 ----
mean loss: 796.10
 ---- batch: 050 ----
mean loss: 800.49
 ---- batch: 060 ----
mean loss: 789.04
 ---- batch: 070 ----
mean loss: 779.73
 ---- batch: 080 ----
mean loss: 798.29
 ---- batch: 090 ----
mean loss: 795.61
train mean loss: 791.80
epoch train time: 0:00:02.677711
elapsed time: 0:01:30.038783
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 19:12:06.105256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 787.23
 ---- batch: 020 ----
mean loss: 797.47
 ---- batch: 030 ----
mean loss: 777.79
 ---- batch: 040 ----
mean loss: 768.45
 ---- batch: 050 ----
mean loss: 779.95
 ---- batch: 060 ----
mean loss: 801.04
 ---- batch: 070 ----
mean loss: 786.48
 ---- batch: 080 ----
mean loss: 782.56
 ---- batch: 090 ----
mean loss: 788.44
train mean loss: 785.88
epoch train time: 0:00:02.663634
elapsed time: 0:01:32.702828
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 19:12:08.769335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 785.89
 ---- batch: 020 ----
mean loss: 772.85
 ---- batch: 030 ----
mean loss: 764.85
 ---- batch: 040 ----
mean loss: 774.60
 ---- batch: 050 ----
mean loss: 780.65
 ---- batch: 060 ----
mean loss: 777.25
 ---- batch: 070 ----
mean loss: 778.01
 ---- batch: 080 ----
mean loss: 784.44
 ---- batch: 090 ----
mean loss: 784.84
train mean loss: 778.06
epoch train time: 0:00:02.678047
elapsed time: 0:01:35.381317
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 19:12:11.447824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 785.61
 ---- batch: 020 ----
mean loss: 766.05
 ---- batch: 030 ----
mean loss: 771.87
 ---- batch: 040 ----
mean loss: 760.02
 ---- batch: 050 ----
mean loss: 768.07
 ---- batch: 060 ----
mean loss: 759.08
 ---- batch: 070 ----
mean loss: 776.12
 ---- batch: 080 ----
mean loss: 775.32
 ---- batch: 090 ----
mean loss: 767.78
train mean loss: 771.29
epoch train time: 0:00:02.668447
elapsed time: 0:01:38.050212
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 19:12:14.116729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 756.07
 ---- batch: 020 ----
mean loss: 774.38
 ---- batch: 030 ----
mean loss: 767.24
 ---- batch: 040 ----
mean loss: 766.96
 ---- batch: 050 ----
mean loss: 768.30
 ---- batch: 060 ----
mean loss: 763.45
 ---- batch: 070 ----
mean loss: 755.44
 ---- batch: 080 ----
mean loss: 751.98
 ---- batch: 090 ----
mean loss: 763.67
train mean loss: 761.22
epoch train time: 0:00:02.645189
elapsed time: 0:01:40.695884
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 19:12:16.762396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 750.85
 ---- batch: 020 ----
mean loss: 750.31
 ---- batch: 030 ----
mean loss: 747.60
 ---- batch: 040 ----
mean loss: 745.73
 ---- batch: 050 ----
mean loss: 743.53
 ---- batch: 060 ----
mean loss: 771.39
 ---- batch: 070 ----
mean loss: 756.13
 ---- batch: 080 ----
mean loss: 752.41
 ---- batch: 090 ----
mean loss: 745.81
train mean loss: 752.26
epoch train time: 0:00:02.654369
elapsed time: 0:01:43.350753
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 19:12:19.417300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 748.44
 ---- batch: 020 ----
mean loss: 750.01
 ---- batch: 030 ----
mean loss: 739.83
 ---- batch: 040 ----
mean loss: 743.52
 ---- batch: 050 ----
mean loss: 729.26
 ---- batch: 060 ----
mean loss: 741.47
 ---- batch: 070 ----
mean loss: 746.80
 ---- batch: 080 ----
mean loss: 748.91
 ---- batch: 090 ----
mean loss: 745.35
train mean loss: 742.37
epoch train time: 0:00:02.667193
elapsed time: 0:01:46.018415
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 19:12:22.085018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 750.83
 ---- batch: 020 ----
mean loss: 731.65
 ---- batch: 030 ----
mean loss: 734.31
 ---- batch: 040 ----
mean loss: 742.77
 ---- batch: 050 ----
mean loss: 736.46
 ---- batch: 060 ----
mean loss: 723.65
 ---- batch: 070 ----
mean loss: 718.71
 ---- batch: 080 ----
mean loss: 747.48
 ---- batch: 090 ----
mean loss: 713.58
train mean loss: 732.16
epoch train time: 0:00:02.670717
elapsed time: 0:01:48.689664
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 19:12:24.756193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 730.16
 ---- batch: 020 ----
mean loss: 722.66
 ---- batch: 030 ----
mean loss: 713.30
 ---- batch: 040 ----
mean loss: 721.65
 ---- batch: 050 ----
mean loss: 730.00
 ---- batch: 060 ----
mean loss: 729.44
 ---- batch: 070 ----
mean loss: 735.23
 ---- batch: 080 ----
mean loss: 714.26
 ---- batch: 090 ----
mean loss: 705.86
train mean loss: 722.49
epoch train time: 0:00:02.672924
elapsed time: 0:01:51.363084
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 19:12:27.429634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 728.23
 ---- batch: 020 ----
mean loss: 722.18
 ---- batch: 030 ----
mean loss: 711.85
 ---- batch: 040 ----
mean loss: 712.80
 ---- batch: 050 ----
mean loss: 713.49
 ---- batch: 060 ----
mean loss: 721.68
 ---- batch: 070 ----
mean loss: 708.15
 ---- batch: 080 ----
mean loss: 709.27
 ---- batch: 090 ----
mean loss: 686.58
train mean loss: 711.59
epoch train time: 0:00:02.681087
elapsed time: 0:01:54.044624
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 19:12:30.111123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 693.00
 ---- batch: 020 ----
mean loss: 702.01
 ---- batch: 030 ----
mean loss: 700.26
 ---- batch: 040 ----
mean loss: 705.16
 ---- batch: 050 ----
mean loss: 717.50
 ---- batch: 060 ----
mean loss: 692.47
 ---- batch: 070 ----
mean loss: 713.94
 ---- batch: 080 ----
mean loss: 696.64
 ---- batch: 090 ----
mean loss: 685.38
train mean loss: 700.53
epoch train time: 0:00:02.660020
elapsed time: 0:01:56.705075
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 19:12:32.771598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 698.21
 ---- batch: 020 ----
mean loss: 700.92
 ---- batch: 030 ----
mean loss: 694.90
 ---- batch: 040 ----
mean loss: 685.75
 ---- batch: 050 ----
mean loss: 684.17
 ---- batch: 060 ----
mean loss: 697.15
 ---- batch: 070 ----
mean loss: 674.99
 ---- batch: 080 ----
mean loss: 705.80
 ---- batch: 090 ----
mean loss: 682.19
train mean loss: 690.92
epoch train time: 0:00:02.657423
elapsed time: 0:01:59.362963
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 19:12:35.429464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 686.67
 ---- batch: 020 ----
mean loss: 687.47
 ---- batch: 030 ----
mean loss: 683.32
 ---- batch: 040 ----
mean loss: 679.38
 ---- batch: 050 ----
mean loss: 673.26
 ---- batch: 060 ----
mean loss: 675.33
 ---- batch: 070 ----
mean loss: 679.34
 ---- batch: 080 ----
mean loss: 677.03
 ---- batch: 090 ----
mean loss: 674.20
train mean loss: 680.82
epoch train time: 0:00:02.664303
elapsed time: 0:02:02.027709
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 19:12:38.094256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 676.40
 ---- batch: 020 ----
mean loss: 677.32
 ---- batch: 030 ----
mean loss: 673.31
 ---- batch: 040 ----
mean loss: 669.91
 ---- batch: 050 ----
mean loss: 677.78
 ---- batch: 060 ----
mean loss: 668.03
 ---- batch: 070 ----
mean loss: 668.56
 ---- batch: 080 ----
mean loss: 656.22
 ---- batch: 090 ----
mean loss: 660.59
train mean loss: 669.54
epoch train time: 0:00:02.669227
elapsed time: 0:02:04.697431
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 19:12:40.764012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 662.19
 ---- batch: 020 ----
mean loss: 661.11
 ---- batch: 030 ----
mean loss: 653.29
 ---- batch: 040 ----
mean loss: 660.50
 ---- batch: 050 ----
mean loss: 654.10
 ---- batch: 060 ----
mean loss: 662.84
 ---- batch: 070 ----
mean loss: 659.23
 ---- batch: 080 ----
mean loss: 651.13
 ---- batch: 090 ----
mean loss: 665.71
train mean loss: 658.94
epoch train time: 0:00:02.664369
elapsed time: 0:02:07.362355
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 19:12:43.428867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 653.34
 ---- batch: 020 ----
mean loss: 652.24
 ---- batch: 030 ----
mean loss: 658.22
 ---- batch: 040 ----
mean loss: 657.40
 ---- batch: 050 ----
mean loss: 652.39
 ---- batch: 060 ----
mean loss: 631.21
 ---- batch: 070 ----
mean loss: 636.46
 ---- batch: 080 ----
mean loss: 638.90
 ---- batch: 090 ----
mean loss: 663.83
train mean loss: 648.60
epoch train time: 0:00:02.671131
elapsed time: 0:02:10.033963
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 19:12:46.100545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 650.30
 ---- batch: 020 ----
mean loss: 641.06
 ---- batch: 030 ----
mean loss: 637.96
 ---- batch: 040 ----
mean loss: 648.40
 ---- batch: 050 ----
mean loss: 645.85
 ---- batch: 060 ----
mean loss: 631.72
 ---- batch: 070 ----
mean loss: 635.91
 ---- batch: 080 ----
mean loss: 617.06
 ---- batch: 090 ----
mean loss: 626.04
train mean loss: 637.43
epoch train time: 0:00:02.635962
elapsed time: 0:02:12.670455
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 19:12:48.736970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 622.41
 ---- batch: 020 ----
mean loss: 637.07
 ---- batch: 030 ----
mean loss: 627.30
 ---- batch: 040 ----
mean loss: 640.68
 ---- batch: 050 ----
mean loss: 625.42
 ---- batch: 060 ----
mean loss: 627.10
 ---- batch: 070 ----
mean loss: 629.06
 ---- batch: 080 ----
mean loss: 628.48
 ---- batch: 090 ----
mean loss: 611.06
train mean loss: 628.04
epoch train time: 0:00:02.660471
elapsed time: 0:02:15.331400
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 19:12:51.397923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 625.05
 ---- batch: 020 ----
mean loss: 621.50
 ---- batch: 030 ----
mean loss: 616.58
 ---- batch: 040 ----
mean loss: 621.36
 ---- batch: 050 ----
mean loss: 612.26
 ---- batch: 060 ----
mean loss: 623.45
 ---- batch: 070 ----
mean loss: 623.45
 ---- batch: 080 ----
mean loss: 611.47
 ---- batch: 090 ----
mean loss: 612.62
train mean loss: 618.40
epoch train time: 0:00:02.704460
elapsed time: 0:02:18.036324
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 19:12:54.102837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 615.42
 ---- batch: 020 ----
mean loss: 601.87
 ---- batch: 030 ----
mean loss: 603.82
 ---- batch: 040 ----
mean loss: 617.17
 ---- batch: 050 ----
mean loss: 611.44
 ---- batch: 060 ----
mean loss: 608.21
 ---- batch: 070 ----
mean loss: 602.85
 ---- batch: 080 ----
mean loss: 600.46
 ---- batch: 090 ----
mean loss: 609.87
train mean loss: 608.31
epoch train time: 0:00:02.685538
elapsed time: 0:02:20.722378
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 19:12:56.788972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 612.18
 ---- batch: 020 ----
mean loss: 605.06
 ---- batch: 030 ----
mean loss: 597.28
 ---- batch: 040 ----
mean loss: 594.60
 ---- batch: 050 ----
mean loss: 601.30
 ---- batch: 060 ----
mean loss: 602.93
 ---- batch: 070 ----
mean loss: 599.15
 ---- batch: 080 ----
mean loss: 594.00
 ---- batch: 090 ----
mean loss: 585.46
train mean loss: 598.77
epoch train time: 0:00:02.681136
elapsed time: 0:02:23.404029
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 19:12:59.470552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 584.38
 ---- batch: 020 ----
mean loss: 592.45
 ---- batch: 030 ----
mean loss: 594.73
 ---- batch: 040 ----
mean loss: 594.73
 ---- batch: 050 ----
mean loss: 580.67
 ---- batch: 060 ----
mean loss: 599.08
 ---- batch: 070 ----
mean loss: 581.07
 ---- batch: 080 ----
mean loss: 581.82
 ---- batch: 090 ----
mean loss: 590.73
train mean loss: 589.08
epoch train time: 0:00:02.646588
elapsed time: 0:02:26.051107
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 19:13:02.117649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.92
 ---- batch: 020 ----
mean loss: 573.71
 ---- batch: 030 ----
mean loss: 581.73
 ---- batch: 040 ----
mean loss: 562.41
 ---- batch: 050 ----
mean loss: 578.04
 ---- batch: 060 ----
mean loss: 581.99
 ---- batch: 070 ----
mean loss: 577.00
 ---- batch: 080 ----
mean loss: 585.40
 ---- batch: 090 ----
mean loss: 583.45
train mean loss: 579.34
epoch train time: 0:00:02.643432
elapsed time: 0:02:28.695040
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 19:13:04.761669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 574.29
 ---- batch: 020 ----
mean loss: 576.16
 ---- batch: 030 ----
mean loss: 574.48
 ---- batch: 040 ----
mean loss: 572.27
 ---- batch: 050 ----
mean loss: 565.16
 ---- batch: 060 ----
mean loss: 563.91
 ---- batch: 070 ----
mean loss: 563.19
 ---- batch: 080 ----
mean loss: 564.35
 ---- batch: 090 ----
mean loss: 565.65
train mean loss: 567.80
epoch train time: 0:00:02.683030
elapsed time: 0:02:31.378609
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 19:13:07.445140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 557.05
 ---- batch: 020 ----
mean loss: 554.40
 ---- batch: 030 ----
mean loss: 573.04
 ---- batch: 040 ----
mean loss: 550.72
 ---- batch: 050 ----
mean loss: 547.72
 ---- batch: 060 ----
mean loss: 552.91
 ---- batch: 070 ----
mean loss: 553.78
 ---- batch: 080 ----
mean loss: 538.08
 ---- batch: 090 ----
mean loss: 550.01
train mean loss: 552.39
epoch train time: 0:00:02.678890
elapsed time: 0:02:34.057956
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 19:13:10.124469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 542.02
 ---- batch: 020 ----
mean loss: 546.02
 ---- batch: 030 ----
mean loss: 537.54
 ---- batch: 040 ----
mean loss: 537.13
 ---- batch: 050 ----
mean loss: 527.77
 ---- batch: 060 ----
mean loss: 526.21
 ---- batch: 070 ----
mean loss: 532.90
 ---- batch: 080 ----
mean loss: 516.75
 ---- batch: 090 ----
mean loss: 524.74
train mean loss: 531.51
epoch train time: 0:00:02.708590
elapsed time: 0:02:36.767256
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 19:13:12.833822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.48
 ---- batch: 020 ----
mean loss: 519.30
 ---- batch: 030 ----
mean loss: 518.33
 ---- batch: 040 ----
mean loss: 507.52
 ---- batch: 050 ----
mean loss: 514.16
 ---- batch: 060 ----
mean loss: 515.03
 ---- batch: 070 ----
mean loss: 508.72
 ---- batch: 080 ----
mean loss: 493.82
 ---- batch: 090 ----
mean loss: 499.32
train mean loss: 510.35
epoch train time: 0:00:02.681890
elapsed time: 0:02:39.449685
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 19:13:15.516233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.83
 ---- batch: 020 ----
mean loss: 498.86
 ---- batch: 030 ----
mean loss: 494.61
 ---- batch: 040 ----
mean loss: 484.22
 ---- batch: 050 ----
mean loss: 509.87
 ---- batch: 060 ----
mean loss: 475.21
 ---- batch: 070 ----
mean loss: 488.74
 ---- batch: 080 ----
mean loss: 477.93
 ---- batch: 090 ----
mean loss: 484.75
train mean loss: 489.81
epoch train time: 0:00:02.657891
elapsed time: 0:02:42.108049
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 19:13:18.174555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.59
 ---- batch: 020 ----
mean loss: 465.90
 ---- batch: 030 ----
mean loss: 470.97
 ---- batch: 040 ----
mean loss: 463.50
 ---- batch: 050 ----
mean loss: 473.51
 ---- batch: 060 ----
mean loss: 463.18
 ---- batch: 070 ----
mean loss: 476.62
 ---- batch: 080 ----
mean loss: 474.02
 ---- batch: 090 ----
mean loss: 473.71
train mean loss: 470.13
epoch train time: 0:00:02.669001
elapsed time: 0:02:44.777497
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 19:13:20.844022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.43
 ---- batch: 020 ----
mean loss: 450.21
 ---- batch: 030 ----
mean loss: 458.47
 ---- batch: 040 ----
mean loss: 460.02
 ---- batch: 050 ----
mean loss: 441.30
 ---- batch: 060 ----
mean loss: 461.84
 ---- batch: 070 ----
mean loss: 441.62
 ---- batch: 080 ----
mean loss: 448.12
 ---- batch: 090 ----
mean loss: 442.53
train mean loss: 451.16
epoch train time: 0:00:02.684740
elapsed time: 0:02:47.462716
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 19:13:23.529237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.10
 ---- batch: 020 ----
mean loss: 438.40
 ---- batch: 030 ----
mean loss: 446.73
 ---- batch: 040 ----
mean loss: 432.60
 ---- batch: 050 ----
mean loss: 428.36
 ---- batch: 060 ----
mean loss: 431.45
 ---- batch: 070 ----
mean loss: 431.83
 ---- batch: 080 ----
mean loss: 424.83
 ---- batch: 090 ----
mean loss: 426.29
train mean loss: 432.95
epoch train time: 0:00:02.696690
elapsed time: 0:02:50.159864
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 19:13:26.226398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.68
 ---- batch: 020 ----
mean loss: 416.17
 ---- batch: 030 ----
mean loss: 422.16
 ---- batch: 040 ----
mean loss: 419.15
 ---- batch: 050 ----
mean loss: 406.05
 ---- batch: 060 ----
mean loss: 406.33
 ---- batch: 070 ----
mean loss: 408.83
 ---- batch: 080 ----
mean loss: 411.09
 ---- batch: 090 ----
mean loss: 416.27
train mean loss: 413.93
epoch train time: 0:00:02.689389
elapsed time: 0:02:52.849791
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 19:13:28.916302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.16
 ---- batch: 020 ----
mean loss: 399.48
 ---- batch: 030 ----
mean loss: 401.38
 ---- batch: 040 ----
mean loss: 386.47
 ---- batch: 050 ----
mean loss: 407.67
 ---- batch: 060 ----
mean loss: 398.14
 ---- batch: 070 ----
mean loss: 382.62
 ---- batch: 080 ----
mean loss: 392.06
 ---- batch: 090 ----
mean loss: 388.38
train mean loss: 394.82
epoch train time: 0:00:02.675959
elapsed time: 0:02:55.526216
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 19:13:31.592733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.92
 ---- batch: 020 ----
mean loss: 379.58
 ---- batch: 030 ----
mean loss: 395.16
 ---- batch: 040 ----
mean loss: 378.80
 ---- batch: 050 ----
mean loss: 376.21
 ---- batch: 060 ----
mean loss: 377.89
 ---- batch: 070 ----
mean loss: 364.95
 ---- batch: 080 ----
mean loss: 382.28
 ---- batch: 090 ----
mean loss: 360.77
train mean loss: 377.42
epoch train time: 0:00:02.690780
elapsed time: 0:02:58.217500
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 19:13:34.283953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.97
 ---- batch: 020 ----
mean loss: 354.27
 ---- batch: 030 ----
mean loss: 371.08
 ---- batch: 040 ----
mean loss: 357.24
 ---- batch: 050 ----
mean loss: 362.18
 ---- batch: 060 ----
mean loss: 365.63
 ---- batch: 070 ----
mean loss: 369.54
 ---- batch: 080 ----
mean loss: 345.60
 ---- batch: 090 ----
mean loss: 359.42
train mean loss: 361.76
epoch train time: 0:00:02.656948
elapsed time: 0:03:00.874910
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 19:13:36.941473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.59
 ---- batch: 020 ----
mean loss: 343.21
 ---- batch: 030 ----
mean loss: 350.81
 ---- batch: 040 ----
mean loss: 356.50
 ---- batch: 050 ----
mean loss: 349.44
 ---- batch: 060 ----
mean loss: 349.84
 ---- batch: 070 ----
mean loss: 359.23
 ---- batch: 080 ----
mean loss: 327.99
 ---- batch: 090 ----
mean loss: 338.29
train mean loss: 348.13
epoch train time: 0:00:02.675101
elapsed time: 0:03:03.550504
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 19:13:39.617022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.09
 ---- batch: 020 ----
mean loss: 335.14
 ---- batch: 030 ----
mean loss: 352.45
 ---- batch: 040 ----
mean loss: 333.96
 ---- batch: 050 ----
mean loss: 341.64
 ---- batch: 060 ----
mean loss: 332.02
 ---- batch: 070 ----
mean loss: 335.31
 ---- batch: 080 ----
mean loss: 333.11
 ---- batch: 090 ----
mean loss: 331.51
train mean loss: 337.16
epoch train time: 0:00:02.671613
elapsed time: 0:03:06.222601
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 19:13:42.289113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.17
 ---- batch: 020 ----
mean loss: 338.81
 ---- batch: 030 ----
mean loss: 337.67
 ---- batch: 040 ----
mean loss: 323.32
 ---- batch: 050 ----
mean loss: 337.53
 ---- batch: 060 ----
mean loss: 324.33
 ---- batch: 070 ----
mean loss: 318.81
 ---- batch: 080 ----
mean loss: 322.56
 ---- batch: 090 ----
mean loss: 320.93
train mean loss: 327.37
epoch train time: 0:00:02.664800
elapsed time: 0:03:08.887922
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 19:13:44.954433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.05
 ---- batch: 020 ----
mean loss: 313.71
 ---- batch: 030 ----
mean loss: 318.35
 ---- batch: 040 ----
mean loss: 325.16
 ---- batch: 050 ----
mean loss: 316.86
 ---- batch: 060 ----
mean loss: 317.47
 ---- batch: 070 ----
mean loss: 318.29
 ---- batch: 080 ----
mean loss: 320.62
 ---- batch: 090 ----
mean loss: 321.48
train mean loss: 319.19
epoch train time: 0:00:02.635018
elapsed time: 0:03:11.523451
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 19:13:47.590018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.81
 ---- batch: 020 ----
mean loss: 314.85
 ---- batch: 030 ----
mean loss: 308.12
 ---- batch: 040 ----
mean loss: 317.00
 ---- batch: 050 ----
mean loss: 310.79
 ---- batch: 060 ----
mean loss: 317.37
 ---- batch: 070 ----
mean loss: 310.85
 ---- batch: 080 ----
mean loss: 310.08
 ---- batch: 090 ----
mean loss: 311.10
train mean loss: 312.64
epoch train time: 0:00:02.654942
elapsed time: 0:03:14.178871
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 19:13:50.245379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.21
 ---- batch: 020 ----
mean loss: 303.61
 ---- batch: 030 ----
mean loss: 297.80
 ---- batch: 040 ----
mean loss: 314.88
 ---- batch: 050 ----
mean loss: 302.03
 ---- batch: 060 ----
mean loss: 299.58
 ---- batch: 070 ----
mean loss: 312.10
 ---- batch: 080 ----
mean loss: 313.15
 ---- batch: 090 ----
mean loss: 313.86
train mean loss: 306.62
epoch train time: 0:00:02.679662
elapsed time: 0:03:16.859009
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 19:13:52.925536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.58
 ---- batch: 020 ----
mean loss: 300.95
 ---- batch: 030 ----
mean loss: 300.22
 ---- batch: 040 ----
mean loss: 289.42
 ---- batch: 050 ----
mean loss: 301.93
 ---- batch: 060 ----
mean loss: 307.69
 ---- batch: 070 ----
mean loss: 299.35
 ---- batch: 080 ----
mean loss: 295.41
 ---- batch: 090 ----
mean loss: 310.58
train mean loss: 301.02
epoch train time: 0:00:02.697766
elapsed time: 0:03:19.557306
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 19:13:55.623824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.36
 ---- batch: 020 ----
mean loss: 288.33
 ---- batch: 030 ----
mean loss: 304.31
 ---- batch: 040 ----
mean loss: 288.35
 ---- batch: 050 ----
mean loss: 301.29
 ---- batch: 060 ----
mean loss: 299.65
 ---- batch: 070 ----
mean loss: 296.93
 ---- batch: 080 ----
mean loss: 298.93
 ---- batch: 090 ----
mean loss: 293.43
train mean loss: 296.51
epoch train time: 0:00:02.680274
elapsed time: 0:03:22.238016
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 19:13:58.304546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.87
 ---- batch: 020 ----
mean loss: 293.10
 ---- batch: 030 ----
mean loss: 294.75
 ---- batch: 040 ----
mean loss: 291.62
 ---- batch: 050 ----
mean loss: 304.08
 ---- batch: 060 ----
mean loss: 285.12
 ---- batch: 070 ----
mean loss: 285.68
 ---- batch: 080 ----
mean loss: 290.26
 ---- batch: 090 ----
mean loss: 289.96
train mean loss: 292.32
epoch train time: 0:00:02.661330
elapsed time: 0:03:24.899862
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 19:14:00.966382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.69
 ---- batch: 020 ----
mean loss: 288.08
 ---- batch: 030 ----
mean loss: 292.66
 ---- batch: 040 ----
mean loss: 293.19
 ---- batch: 050 ----
mean loss: 297.26
 ---- batch: 060 ----
mean loss: 284.13
 ---- batch: 070 ----
mean loss: 277.85
 ---- batch: 080 ----
mean loss: 284.58
 ---- batch: 090 ----
mean loss: 285.42
train mean loss: 288.57
epoch train time: 0:00:02.683648
elapsed time: 0:03:27.584015
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 19:14:03.650584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.90
 ---- batch: 020 ----
mean loss: 285.40
 ---- batch: 030 ----
mean loss: 290.40
 ---- batch: 040 ----
mean loss: 280.60
 ---- batch: 050 ----
mean loss: 283.94
 ---- batch: 060 ----
mean loss: 293.49
 ---- batch: 070 ----
mean loss: 278.85
 ---- batch: 080 ----
mean loss: 288.36
 ---- batch: 090 ----
mean loss: 281.74
train mean loss: 284.89
epoch train time: 0:00:02.676050
elapsed time: 0:03:30.260519
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 19:14:06.327055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.67
 ---- batch: 020 ----
mean loss: 276.20
 ---- batch: 030 ----
mean loss: 294.26
 ---- batch: 040 ----
mean loss: 291.66
 ---- batch: 050 ----
mean loss: 287.44
 ---- batch: 060 ----
mean loss: 285.30
 ---- batch: 070 ----
mean loss: 275.46
 ---- batch: 080 ----
mean loss: 277.96
 ---- batch: 090 ----
mean loss: 278.53
train mean loss: 282.63
epoch train time: 0:00:02.683045
elapsed time: 0:03:32.944046
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 19:14:09.010554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.97
 ---- batch: 020 ----
mean loss: 279.94
 ---- batch: 030 ----
mean loss: 271.75
 ---- batch: 040 ----
mean loss: 269.29
 ---- batch: 050 ----
mean loss: 277.24
 ---- batch: 060 ----
mean loss: 284.14
 ---- batch: 070 ----
mean loss: 287.02
 ---- batch: 080 ----
mean loss: 284.26
 ---- batch: 090 ----
mean loss: 271.11
train mean loss: 278.29
epoch train time: 0:00:02.649185
elapsed time: 0:03:35.593656
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 19:14:11.660174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.84
 ---- batch: 020 ----
mean loss: 279.65
 ---- batch: 030 ----
mean loss: 272.04
 ---- batch: 040 ----
mean loss: 273.84
 ---- batch: 050 ----
mean loss: 272.14
 ---- batch: 060 ----
mean loss: 273.67
 ---- batch: 070 ----
mean loss: 281.13
 ---- batch: 080 ----
mean loss: 274.68
 ---- batch: 090 ----
mean loss: 280.31
train mean loss: 276.81
epoch train time: 0:00:02.675770
elapsed time: 0:03:38.269904
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 19:14:14.336409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.50
 ---- batch: 020 ----
mean loss: 266.52
 ---- batch: 030 ----
mean loss: 273.05
 ---- batch: 040 ----
mean loss: 277.55
 ---- batch: 050 ----
mean loss: 271.65
 ---- batch: 060 ----
mean loss: 263.82
 ---- batch: 070 ----
mean loss: 265.98
 ---- batch: 080 ----
mean loss: 286.41
 ---- batch: 090 ----
mean loss: 282.20
train mean loss: 275.03
epoch train time: 0:00:02.651492
elapsed time: 0:03:40.921825
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 19:14:16.988327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.91
 ---- batch: 020 ----
mean loss: 273.29
 ---- batch: 030 ----
mean loss: 278.28
 ---- batch: 040 ----
mean loss: 281.26
 ---- batch: 050 ----
mean loss: 276.42
 ---- batch: 060 ----
mean loss: 270.16
 ---- batch: 070 ----
mean loss: 268.79
 ---- batch: 080 ----
mean loss: 267.13
 ---- batch: 090 ----
mean loss: 270.58
train mean loss: 271.90
epoch train time: 0:00:02.672098
elapsed time: 0:03:43.594363
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 19:14:19.660864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.99
 ---- batch: 020 ----
mean loss: 265.42
 ---- batch: 030 ----
mean loss: 276.45
 ---- batch: 040 ----
mean loss: 268.77
 ---- batch: 050 ----
mean loss: 275.95
 ---- batch: 060 ----
mean loss: 278.62
 ---- batch: 070 ----
mean loss: 259.13
 ---- batch: 080 ----
mean loss: 270.42
 ---- batch: 090 ----
mean loss: 268.02
train mean loss: 270.13
epoch train time: 0:00:02.663621
elapsed time: 0:03:46.258420
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 19:14:22.324926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.89
 ---- batch: 020 ----
mean loss: 255.47
 ---- batch: 030 ----
mean loss: 261.58
 ---- batch: 040 ----
mean loss: 272.23
 ---- batch: 050 ----
mean loss: 274.98
 ---- batch: 060 ----
mean loss: 268.30
 ---- batch: 070 ----
mean loss: 268.46
 ---- batch: 080 ----
mean loss: 273.35
 ---- batch: 090 ----
mean loss: 271.68
train mean loss: 268.04
epoch train time: 0:00:02.664226
elapsed time: 0:03:48.923077
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 19:14:24.989593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.92
 ---- batch: 020 ----
mean loss: 268.10
 ---- batch: 030 ----
mean loss: 281.28
 ---- batch: 040 ----
mean loss: 261.20
 ---- batch: 050 ----
mean loss: 260.94
 ---- batch: 060 ----
mean loss: 267.63
 ---- batch: 070 ----
mean loss: 265.58
 ---- batch: 080 ----
mean loss: 272.80
 ---- batch: 090 ----
mean loss: 263.70
train mean loss: 266.47
epoch train time: 0:00:02.650285
elapsed time: 0:03:51.573796
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 19:14:27.640360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.66
 ---- batch: 020 ----
mean loss: 267.60
 ---- batch: 030 ----
mean loss: 271.57
 ---- batch: 040 ----
mean loss: 261.80
 ---- batch: 050 ----
mean loss: 263.18
 ---- batch: 060 ----
mean loss: 266.00
 ---- batch: 070 ----
mean loss: 264.15
 ---- batch: 080 ----
mean loss: 262.78
 ---- batch: 090 ----
mean loss: 264.12
train mean loss: 264.51
epoch train time: 0:00:02.647057
elapsed time: 0:03:54.221346
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 19:14:30.287862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.36
 ---- batch: 020 ----
mean loss: 263.38
 ---- batch: 030 ----
mean loss: 262.64
 ---- batch: 040 ----
mean loss: 266.89
 ---- batch: 050 ----
mean loss: 257.76
 ---- batch: 060 ----
mean loss: 262.99
 ---- batch: 070 ----
mean loss: 270.69
 ---- batch: 080 ----
mean loss: 266.25
 ---- batch: 090 ----
mean loss: 264.62
train mean loss: 263.35
epoch train time: 0:00:02.659616
elapsed time: 0:03:56.881396
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 19:14:32.947919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.68
 ---- batch: 020 ----
mean loss: 261.53
 ---- batch: 030 ----
mean loss: 245.39
 ---- batch: 040 ----
mean loss: 261.94
 ---- batch: 050 ----
mean loss: 269.17
 ---- batch: 060 ----
mean loss: 257.75
 ---- batch: 070 ----
mean loss: 267.18
 ---- batch: 080 ----
mean loss: 269.94
 ---- batch: 090 ----
mean loss: 259.79
train mean loss: 262.45
epoch train time: 0:00:02.658897
elapsed time: 0:03:59.540742
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 19:14:35.607254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.65
 ---- batch: 020 ----
mean loss: 265.21
 ---- batch: 030 ----
mean loss: 255.26
 ---- batch: 040 ----
mean loss: 272.93
 ---- batch: 050 ----
mean loss: 255.96
 ---- batch: 060 ----
mean loss: 259.55
 ---- batch: 070 ----
mean loss: 259.48
 ---- batch: 080 ----
mean loss: 263.23
 ---- batch: 090 ----
mean loss: 259.72
train mean loss: 260.57
epoch train time: 0:00:02.681857
elapsed time: 0:04:02.223069
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 19:14:38.289576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.09
 ---- batch: 020 ----
mean loss: 256.86
 ---- batch: 030 ----
mean loss: 254.42
 ---- batch: 040 ----
mean loss: 273.61
 ---- batch: 050 ----
mean loss: 263.59
 ---- batch: 060 ----
mean loss: 257.93
 ---- batch: 070 ----
mean loss: 274.34
 ---- batch: 080 ----
mean loss: 254.87
 ---- batch: 090 ----
mean loss: 259.83
train mean loss: 260.04
epoch train time: 0:00:02.690575
elapsed time: 0:04:04.914092
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 19:14:40.980604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.89
 ---- batch: 020 ----
mean loss: 259.07
 ---- batch: 030 ----
mean loss: 260.32
 ---- batch: 040 ----
mean loss: 264.90
 ---- batch: 050 ----
mean loss: 254.65
 ---- batch: 060 ----
mean loss: 268.61
 ---- batch: 070 ----
mean loss: 248.19
 ---- batch: 080 ----
mean loss: 258.79
 ---- batch: 090 ----
mean loss: 260.69
train mean loss: 258.52
epoch train time: 0:00:02.650346
elapsed time: 0:04:07.564883
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 19:14:43.631385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.51
 ---- batch: 020 ----
mean loss: 256.15
 ---- batch: 030 ----
mean loss: 257.28
 ---- batch: 040 ----
mean loss: 253.55
 ---- batch: 050 ----
mean loss: 257.49
 ---- batch: 060 ----
mean loss: 256.77
 ---- batch: 070 ----
mean loss: 256.36
 ---- batch: 080 ----
mean loss: 258.47
 ---- batch: 090 ----
mean loss: 262.60
train mean loss: 257.58
epoch train time: 0:00:02.647502
elapsed time: 0:04:10.212837
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 19:14:46.279370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.69
 ---- batch: 020 ----
mean loss: 262.67
 ---- batch: 030 ----
mean loss: 266.74
 ---- batch: 040 ----
mean loss: 256.11
 ---- batch: 050 ----
mean loss: 267.38
 ---- batch: 060 ----
mean loss: 254.27
 ---- batch: 070 ----
mean loss: 252.14
 ---- batch: 080 ----
mean loss: 246.74
 ---- batch: 090 ----
mean loss: 254.63
train mean loss: 256.58
epoch train time: 0:00:02.657447
elapsed time: 0:04:12.870763
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 19:14:48.937287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.52
 ---- batch: 020 ----
mean loss: 250.57
 ---- batch: 030 ----
mean loss: 248.77
 ---- batch: 040 ----
mean loss: 254.37
 ---- batch: 050 ----
mean loss: 256.25
 ---- batch: 060 ----
mean loss: 264.09
 ---- batch: 070 ----
mean loss: 251.06
 ---- batch: 080 ----
mean loss: 266.75
 ---- batch: 090 ----
mean loss: 253.10
train mean loss: 255.39
epoch train time: 0:00:02.656144
elapsed time: 0:04:15.527462
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 19:14:51.593986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.28
 ---- batch: 020 ----
mean loss: 246.94
 ---- batch: 030 ----
mean loss: 256.07
 ---- batch: 040 ----
mean loss: 250.87
 ---- batch: 050 ----
mean loss: 260.95
 ---- batch: 060 ----
mean loss: 258.20
 ---- batch: 070 ----
mean loss: 253.11
 ---- batch: 080 ----
mean loss: 253.18
 ---- batch: 090 ----
mean loss: 253.09
train mean loss: 254.93
epoch train time: 0:00:02.694747
elapsed time: 0:04:18.222677
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 19:14:54.289187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.02
 ---- batch: 020 ----
mean loss: 251.77
 ---- batch: 030 ----
mean loss: 253.69
 ---- batch: 040 ----
mean loss: 244.32
 ---- batch: 050 ----
mean loss: 257.32
 ---- batch: 060 ----
mean loss: 255.69
 ---- batch: 070 ----
mean loss: 249.42
 ---- batch: 080 ----
mean loss: 254.36
 ---- batch: 090 ----
mean loss: 260.14
train mean loss: 253.22
epoch train time: 0:00:02.661858
elapsed time: 0:04:20.884993
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 19:14:56.951565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.74
 ---- batch: 020 ----
mean loss: 263.48
 ---- batch: 030 ----
mean loss: 247.16
 ---- batch: 040 ----
mean loss: 256.89
 ---- batch: 050 ----
mean loss: 253.80
 ---- batch: 060 ----
mean loss: 243.02
 ---- batch: 070 ----
mean loss: 242.38
 ---- batch: 080 ----
mean loss: 246.74
 ---- batch: 090 ----
mean loss: 262.25
train mean loss: 252.89
epoch train time: 0:00:02.641003
elapsed time: 0:04:23.526556
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 19:14:59.593102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.70
 ---- batch: 020 ----
mean loss: 252.90
 ---- batch: 030 ----
mean loss: 255.36
 ---- batch: 040 ----
mean loss: 255.45
 ---- batch: 050 ----
mean loss: 252.76
 ---- batch: 060 ----
mean loss: 247.84
 ---- batch: 070 ----
mean loss: 249.01
 ---- batch: 080 ----
mean loss: 253.08
 ---- batch: 090 ----
mean loss: 244.93
train mean loss: 252.07
epoch train time: 0:00:02.684559
elapsed time: 0:04:26.211630
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 19:15:02.278174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.05
 ---- batch: 020 ----
mean loss: 254.03
 ---- batch: 030 ----
mean loss: 255.99
 ---- batch: 040 ----
mean loss: 256.18
 ---- batch: 050 ----
mean loss: 241.15
 ---- batch: 060 ----
mean loss: 257.04
 ---- batch: 070 ----
mean loss: 247.21
 ---- batch: 080 ----
mean loss: 257.05
 ---- batch: 090 ----
mean loss: 248.40
train mean loss: 251.46
epoch train time: 0:00:02.682253
elapsed time: 0:04:28.894379
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 19:15:04.960856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.87
 ---- batch: 020 ----
mean loss: 251.81
 ---- batch: 030 ----
mean loss: 251.01
 ---- batch: 040 ----
mean loss: 247.41
 ---- batch: 050 ----
mean loss: 247.74
 ---- batch: 060 ----
mean loss: 257.08
 ---- batch: 070 ----
mean loss: 246.02
 ---- batch: 080 ----
mean loss: 246.43
 ---- batch: 090 ----
mean loss: 244.90
train mean loss: 250.35
epoch train time: 0:00:02.652910
elapsed time: 0:04:31.547709
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 19:15:07.614254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.90
 ---- batch: 020 ----
mean loss: 256.46
 ---- batch: 030 ----
mean loss: 252.95
 ---- batch: 040 ----
mean loss: 247.62
 ---- batch: 050 ----
mean loss: 251.99
 ---- batch: 060 ----
mean loss: 247.43
 ---- batch: 070 ----
mean loss: 247.37
 ---- batch: 080 ----
mean loss: 247.49
 ---- batch: 090 ----
mean loss: 248.68
train mean loss: 249.75
epoch train time: 0:00:02.659820
elapsed time: 0:04:34.208012
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 19:15:10.274526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.50
 ---- batch: 020 ----
mean loss: 245.45
 ---- batch: 030 ----
mean loss: 238.27
 ---- batch: 040 ----
mean loss: 250.47
 ---- batch: 050 ----
mean loss: 252.95
 ---- batch: 060 ----
mean loss: 261.65
 ---- batch: 070 ----
mean loss: 248.41
 ---- batch: 080 ----
mean loss: 251.55
 ---- batch: 090 ----
mean loss: 244.72
train mean loss: 248.89
epoch train time: 0:00:02.682177
elapsed time: 0:04:36.890710
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 19:15:12.957283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.33
 ---- batch: 020 ----
mean loss: 252.86
 ---- batch: 030 ----
mean loss: 247.73
 ---- batch: 040 ----
mean loss: 245.00
 ---- batch: 050 ----
mean loss: 253.44
 ---- batch: 060 ----
mean loss: 257.54
 ---- batch: 070 ----
mean loss: 248.32
 ---- batch: 080 ----
mean loss: 250.95
 ---- batch: 090 ----
mean loss: 240.39
train mean loss: 248.45
epoch train time: 0:00:02.679775
elapsed time: 0:04:39.571021
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 19:15:15.637582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.14
 ---- batch: 020 ----
mean loss: 252.44
 ---- batch: 030 ----
mean loss: 247.55
 ---- batch: 040 ----
mean loss: 248.00
 ---- batch: 050 ----
mean loss: 244.44
 ---- batch: 060 ----
mean loss: 249.08
 ---- batch: 070 ----
mean loss: 246.50
 ---- batch: 080 ----
mean loss: 250.40
 ---- batch: 090 ----
mean loss: 250.94
train mean loss: 248.00
epoch train time: 0:00:02.674857
elapsed time: 0:04:42.246390
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 19:15:18.312910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.03
 ---- batch: 020 ----
mean loss: 254.90
 ---- batch: 030 ----
mean loss: 247.56
 ---- batch: 040 ----
mean loss: 249.64
 ---- batch: 050 ----
mean loss: 246.91
 ---- batch: 060 ----
mean loss: 251.72
 ---- batch: 070 ----
mean loss: 251.40
 ---- batch: 080 ----
mean loss: 252.66
 ---- batch: 090 ----
mean loss: 237.60
train mean loss: 247.53
epoch train time: 0:00:02.671746
elapsed time: 0:04:44.918633
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 19:15:20.985142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.19
 ---- batch: 020 ----
mean loss: 244.36
 ---- batch: 030 ----
mean loss: 257.14
 ---- batch: 040 ----
mean loss: 235.29
 ---- batch: 050 ----
mean loss: 240.43
 ---- batch: 060 ----
mean loss: 254.88
 ---- batch: 070 ----
mean loss: 247.73
 ---- batch: 080 ----
mean loss: 240.22
 ---- batch: 090 ----
mean loss: 252.68
train mean loss: 246.17
epoch train time: 0:00:02.684057
elapsed time: 0:04:47.603134
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 19:15:23.669700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.40
 ---- batch: 020 ----
mean loss: 240.79
 ---- batch: 030 ----
mean loss: 245.70
 ---- batch: 040 ----
mean loss: 247.00
 ---- batch: 050 ----
mean loss: 246.54
 ---- batch: 060 ----
mean loss: 247.95
 ---- batch: 070 ----
mean loss: 240.29
 ---- batch: 080 ----
mean loss: 244.35
 ---- batch: 090 ----
mean loss: 249.03
train mean loss: 245.78
epoch train time: 0:00:02.658626
elapsed time: 0:04:50.262267
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 19:15:26.328730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.14
 ---- batch: 020 ----
mean loss: 241.86
 ---- batch: 030 ----
mean loss: 246.44
 ---- batch: 040 ----
mean loss: 244.87
 ---- batch: 050 ----
mean loss: 242.13
 ---- batch: 060 ----
mean loss: 246.26
 ---- batch: 070 ----
mean loss: 251.83
 ---- batch: 080 ----
mean loss: 243.53
 ---- batch: 090 ----
mean loss: 247.89
train mean loss: 245.65
epoch train time: 0:00:02.674671
elapsed time: 0:04:52.937418
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 19:15:29.003959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.65
 ---- batch: 020 ----
mean loss: 239.50
 ---- batch: 030 ----
mean loss: 241.03
 ---- batch: 040 ----
mean loss: 247.99
 ---- batch: 050 ----
mean loss: 244.49
 ---- batch: 060 ----
mean loss: 237.45
 ---- batch: 070 ----
mean loss: 246.60
 ---- batch: 080 ----
mean loss: 249.79
 ---- batch: 090 ----
mean loss: 252.74
train mean loss: 244.57
epoch train time: 0:00:02.681469
elapsed time: 0:04:55.619436
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 19:15:31.685948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.59
 ---- batch: 020 ----
mean loss: 248.39
 ---- batch: 030 ----
mean loss: 240.29
 ---- batch: 040 ----
mean loss: 246.43
 ---- batch: 050 ----
mean loss: 236.81
 ---- batch: 060 ----
mean loss: 247.24
 ---- batch: 070 ----
mean loss: 250.17
 ---- batch: 080 ----
mean loss: 246.28
 ---- batch: 090 ----
mean loss: 242.02
train mean loss: 244.31
epoch train time: 0:00:02.666981
elapsed time: 0:04:58.286933
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 19:15:34.353443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.63
 ---- batch: 020 ----
mean loss: 236.66
 ---- batch: 030 ----
mean loss: 241.37
 ---- batch: 040 ----
mean loss: 242.92
 ---- batch: 050 ----
mean loss: 246.61
 ---- batch: 060 ----
mean loss: 245.34
 ---- batch: 070 ----
mean loss: 242.72
 ---- batch: 080 ----
mean loss: 244.89
 ---- batch: 090 ----
mean loss: 245.71
train mean loss: 244.26
epoch train time: 0:00:02.641674
elapsed time: 0:05:00.929125
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 19:15:36.995697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.63
 ---- batch: 020 ----
mean loss: 239.24
 ---- batch: 030 ----
mean loss: 241.88
 ---- batch: 040 ----
mean loss: 242.45
 ---- batch: 050 ----
mean loss: 253.27
 ---- batch: 060 ----
mean loss: 235.44
 ---- batch: 070 ----
mean loss: 235.20
 ---- batch: 080 ----
mean loss: 242.56
 ---- batch: 090 ----
mean loss: 243.51
train mean loss: 243.06
epoch train time: 0:00:02.654586
elapsed time: 0:05:03.584203
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 19:15:39.650711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.24
 ---- batch: 020 ----
mean loss: 245.91
 ---- batch: 030 ----
mean loss: 242.05
 ---- batch: 040 ----
mean loss: 243.32
 ---- batch: 050 ----
mean loss: 250.30
 ---- batch: 060 ----
mean loss: 250.44
 ---- batch: 070 ----
mean loss: 241.84
 ---- batch: 080 ----
mean loss: 237.20
 ---- batch: 090 ----
mean loss: 238.74
train mean loss: 243.15
epoch train time: 0:00:02.691890
elapsed time: 0:05:06.276535
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 19:15:42.343071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.77
 ---- batch: 020 ----
mean loss: 238.26
 ---- batch: 030 ----
mean loss: 235.72
 ---- batch: 040 ----
mean loss: 241.82
 ---- batch: 050 ----
mean loss: 239.81
 ---- batch: 060 ----
mean loss: 246.67
 ---- batch: 070 ----
mean loss: 240.19
 ---- batch: 080 ----
mean loss: 242.75
 ---- batch: 090 ----
mean loss: 250.43
train mean loss: 242.23
epoch train time: 0:00:02.652921
elapsed time: 0:05:08.929940
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 19:15:44.996447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.17
 ---- batch: 020 ----
mean loss: 245.36
 ---- batch: 030 ----
mean loss: 238.92
 ---- batch: 040 ----
mean loss: 241.52
 ---- batch: 050 ----
mean loss: 235.51
 ---- batch: 060 ----
mean loss: 243.23
 ---- batch: 070 ----
mean loss: 248.83
 ---- batch: 080 ----
mean loss: 233.20
 ---- batch: 090 ----
mean loss: 241.70
train mean loss: 242.14
epoch train time: 0:00:02.684188
elapsed time: 0:05:11.614585
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 19:15:47.681126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.13
 ---- batch: 020 ----
mean loss: 243.30
 ---- batch: 030 ----
mean loss: 240.80
 ---- batch: 040 ----
mean loss: 241.53
 ---- batch: 050 ----
mean loss: 242.26
 ---- batch: 060 ----
mean loss: 246.91
 ---- batch: 070 ----
mean loss: 233.37
 ---- batch: 080 ----
mean loss: 239.89
 ---- batch: 090 ----
mean loss: 236.31
train mean loss: 241.32
epoch train time: 0:00:02.665305
elapsed time: 0:05:14.280418
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 19:15:50.346942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.05
 ---- batch: 020 ----
mean loss: 240.50
 ---- batch: 030 ----
mean loss: 251.00
 ---- batch: 040 ----
mean loss: 238.65
 ---- batch: 050 ----
mean loss: 230.89
 ---- batch: 060 ----
mean loss: 246.93
 ---- batch: 070 ----
mean loss: 238.41
 ---- batch: 080 ----
mean loss: 240.31
 ---- batch: 090 ----
mean loss: 243.59
train mean loss: 240.92
epoch train time: 0:00:02.671218
elapsed time: 0:05:16.952175
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 19:15:53.018714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.08
 ---- batch: 020 ----
mean loss: 245.23
 ---- batch: 030 ----
mean loss: 239.17
 ---- batch: 040 ----
mean loss: 230.87
 ---- batch: 050 ----
mean loss: 244.48
 ---- batch: 060 ----
mean loss: 242.82
 ---- batch: 070 ----
mean loss: 237.43
 ---- batch: 080 ----
mean loss: 239.74
 ---- batch: 090 ----
mean loss: 238.68
train mean loss: 240.21
epoch train time: 0:00:02.688235
elapsed time: 0:05:19.640987
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 19:15:55.707477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.51
 ---- batch: 020 ----
mean loss: 240.76
 ---- batch: 030 ----
mean loss: 239.69
 ---- batch: 040 ----
mean loss: 240.45
 ---- batch: 050 ----
mean loss: 237.82
 ---- batch: 060 ----
mean loss: 245.10
 ---- batch: 070 ----
mean loss: 244.07
 ---- batch: 080 ----
mean loss: 243.91
 ---- batch: 090 ----
mean loss: 235.74
train mean loss: 239.69
epoch train time: 0:00:02.657947
elapsed time: 0:05:22.299365
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 19:15:58.365887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.55
 ---- batch: 020 ----
mean loss: 237.49
 ---- batch: 030 ----
mean loss: 237.22
 ---- batch: 040 ----
mean loss: 237.69
 ---- batch: 050 ----
mean loss: 231.99
 ---- batch: 060 ----
mean loss: 239.80
 ---- batch: 070 ----
mean loss: 240.79
 ---- batch: 080 ----
mean loss: 241.56
 ---- batch: 090 ----
mean loss: 239.25
train mean loss: 239.36
epoch train time: 0:00:02.663259
elapsed time: 0:05:24.963069
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 19:16:01.029633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.28
 ---- batch: 020 ----
mean loss: 237.46
 ---- batch: 030 ----
mean loss: 240.62
 ---- batch: 040 ----
mean loss: 233.94
 ---- batch: 050 ----
mean loss: 238.16
 ---- batch: 060 ----
mean loss: 243.77
 ---- batch: 070 ----
mean loss: 244.12
 ---- batch: 080 ----
mean loss: 236.21
 ---- batch: 090 ----
mean loss: 236.47
train mean loss: 239.80
epoch train time: 0:00:02.638190
elapsed time: 0:05:27.601748
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 19:16:03.668264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.21
 ---- batch: 020 ----
mean loss: 238.41
 ---- batch: 030 ----
mean loss: 244.31
 ---- batch: 040 ----
mean loss: 235.20
 ---- batch: 050 ----
mean loss: 236.76
 ---- batch: 060 ----
mean loss: 245.33
 ---- batch: 070 ----
mean loss: 235.75
 ---- batch: 080 ----
mean loss: 235.19
 ---- batch: 090 ----
mean loss: 236.98
train mean loss: 238.92
epoch train time: 0:00:02.701559
elapsed time: 0:05:30.303747
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 19:16:06.370278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.93
 ---- batch: 020 ----
mean loss: 237.59
 ---- batch: 030 ----
mean loss: 240.41
 ---- batch: 040 ----
mean loss: 246.07
 ---- batch: 050 ----
mean loss: 241.86
 ---- batch: 060 ----
mean loss: 243.67
 ---- batch: 070 ----
mean loss: 243.73
 ---- batch: 080 ----
mean loss: 234.37
 ---- batch: 090 ----
mean loss: 232.80
train mean loss: 238.14
epoch train time: 0:00:02.702773
elapsed time: 0:05:33.006988
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 19:16:09.073541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.14
 ---- batch: 020 ----
mean loss: 241.15
 ---- batch: 030 ----
mean loss: 230.87
 ---- batch: 040 ----
mean loss: 230.93
 ---- batch: 050 ----
mean loss: 232.25
 ---- batch: 060 ----
mean loss: 245.92
 ---- batch: 070 ----
mean loss: 250.50
 ---- batch: 080 ----
mean loss: 237.79
 ---- batch: 090 ----
mean loss: 240.14
train mean loss: 238.62
epoch train time: 0:00:02.669728
elapsed time: 0:05:35.677248
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 19:16:11.743788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.50
 ---- batch: 020 ----
mean loss: 230.76
 ---- batch: 030 ----
mean loss: 236.09
 ---- batch: 040 ----
mean loss: 240.71
 ---- batch: 050 ----
mean loss: 236.91
 ---- batch: 060 ----
mean loss: 244.17
 ---- batch: 070 ----
mean loss: 229.08
 ---- batch: 080 ----
mean loss: 239.43
 ---- batch: 090 ----
mean loss: 241.43
train mean loss: 237.90
epoch train time: 0:00:02.679173
elapsed time: 0:05:38.356942
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 19:16:14.423481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.61
 ---- batch: 020 ----
mean loss: 237.30
 ---- batch: 030 ----
mean loss: 230.84
 ---- batch: 040 ----
mean loss: 238.13
 ---- batch: 050 ----
mean loss: 233.53
 ---- batch: 060 ----
mean loss: 239.05
 ---- batch: 070 ----
mean loss: 239.64
 ---- batch: 080 ----
mean loss: 229.99
 ---- batch: 090 ----
mean loss: 239.28
train mean loss: 237.20
epoch train time: 0:00:02.659644
elapsed time: 0:05:41.017042
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 19:16:17.083555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.21
 ---- batch: 020 ----
mean loss: 238.27
 ---- batch: 030 ----
mean loss: 232.04
 ---- batch: 040 ----
mean loss: 224.41
 ---- batch: 050 ----
mean loss: 249.65
 ---- batch: 060 ----
mean loss: 236.22
 ---- batch: 070 ----
mean loss: 245.53
 ---- batch: 080 ----
mean loss: 235.23
 ---- batch: 090 ----
mean loss: 233.86
train mean loss: 237.17
epoch train time: 0:00:02.664076
elapsed time: 0:05:43.681602
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 19:16:19.748132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.89
 ---- batch: 020 ----
mean loss: 226.81
 ---- batch: 030 ----
mean loss: 237.63
 ---- batch: 040 ----
mean loss: 234.95
 ---- batch: 050 ----
mean loss: 236.29
 ---- batch: 060 ----
mean loss: 243.14
 ---- batch: 070 ----
mean loss: 246.70
 ---- batch: 080 ----
mean loss: 244.30
 ---- batch: 090 ----
mean loss: 245.07
train mean loss: 236.32
epoch train time: 0:00:02.678776
elapsed time: 0:05:46.360810
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 19:16:22.427315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.04
 ---- batch: 020 ----
mean loss: 229.79
 ---- batch: 030 ----
mean loss: 237.33
 ---- batch: 040 ----
mean loss: 236.96
 ---- batch: 050 ----
mean loss: 236.11
 ---- batch: 060 ----
mean loss: 228.62
 ---- batch: 070 ----
mean loss: 239.74
 ---- batch: 080 ----
mean loss: 240.49
 ---- batch: 090 ----
mean loss: 244.67
train mean loss: 236.46
epoch train time: 0:00:02.672515
elapsed time: 0:05:49.033770
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 19:16:25.100283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.57
 ---- batch: 020 ----
mean loss: 240.07
 ---- batch: 030 ----
mean loss: 238.33
 ---- batch: 040 ----
mean loss: 241.57
 ---- batch: 050 ----
mean loss: 236.74
 ---- batch: 060 ----
mean loss: 237.35
 ---- batch: 070 ----
mean loss: 233.49
 ---- batch: 080 ----
mean loss: 237.82
 ---- batch: 090 ----
mean loss: 223.77
train mean loss: 236.10
epoch train time: 0:00:02.671559
elapsed time: 0:05:51.705805
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 19:16:27.772347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.61
 ---- batch: 020 ----
mean loss: 235.47
 ---- batch: 030 ----
mean loss: 230.59
 ---- batch: 040 ----
mean loss: 234.54
 ---- batch: 050 ----
mean loss: 227.14
 ---- batch: 060 ----
mean loss: 239.53
 ---- batch: 070 ----
mean loss: 233.17
 ---- batch: 080 ----
mean loss: 246.36
 ---- batch: 090 ----
mean loss: 234.52
train mean loss: 236.00
epoch train time: 0:00:02.672894
elapsed time: 0:05:54.379160
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 19:16:30.445697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.94
 ---- batch: 020 ----
mean loss: 233.16
 ---- batch: 030 ----
mean loss: 235.47
 ---- batch: 040 ----
mean loss: 241.77
 ---- batch: 050 ----
mean loss: 232.66
 ---- batch: 060 ----
mean loss: 239.40
 ---- batch: 070 ----
mean loss: 229.50
 ---- batch: 080 ----
mean loss: 240.03
 ---- batch: 090 ----
mean loss: 235.81
train mean loss: 235.14
epoch train time: 0:00:02.667657
elapsed time: 0:05:57.047310
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 19:16:33.113829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.37
 ---- batch: 020 ----
mean loss: 235.99
 ---- batch: 030 ----
mean loss: 232.39
 ---- batch: 040 ----
mean loss: 232.32
 ---- batch: 050 ----
mean loss: 237.86
 ---- batch: 060 ----
mean loss: 234.36
 ---- batch: 070 ----
mean loss: 236.01
 ---- batch: 080 ----
mean loss: 244.39
 ---- batch: 090 ----
mean loss: 236.03
train mean loss: 234.79
epoch train time: 0:00:02.660694
elapsed time: 0:05:59.708494
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 19:16:35.775009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.22
 ---- batch: 020 ----
mean loss: 237.17
 ---- batch: 030 ----
mean loss: 229.45
 ---- batch: 040 ----
mean loss: 239.87
 ---- batch: 050 ----
mean loss: 237.47
 ---- batch: 060 ----
mean loss: 234.75
 ---- batch: 070 ----
mean loss: 225.41
 ---- batch: 080 ----
mean loss: 235.21
 ---- batch: 090 ----
mean loss: 232.68
train mean loss: 234.75
epoch train time: 0:00:02.679729
elapsed time: 0:06:02.388692
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 19:16:38.455219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.25
 ---- batch: 020 ----
mean loss: 227.70
 ---- batch: 030 ----
mean loss: 231.68
 ---- batch: 040 ----
mean loss: 235.74
 ---- batch: 050 ----
mean loss: 233.25
 ---- batch: 060 ----
mean loss: 234.38
 ---- batch: 070 ----
mean loss: 235.27
 ---- batch: 080 ----
mean loss: 238.48
 ---- batch: 090 ----
mean loss: 234.27
train mean loss: 234.46
epoch train time: 0:00:02.645159
elapsed time: 0:06:05.034314
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 19:16:41.100819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.83
 ---- batch: 020 ----
mean loss: 227.42
 ---- batch: 030 ----
mean loss: 230.07
 ---- batch: 040 ----
mean loss: 236.14
 ---- batch: 050 ----
mean loss: 232.32
 ---- batch: 060 ----
mean loss: 242.90
 ---- batch: 070 ----
mean loss: 235.42
 ---- batch: 080 ----
mean loss: 234.98
 ---- batch: 090 ----
mean loss: 238.55
train mean loss: 233.77
epoch train time: 0:00:02.670379
elapsed time: 0:06:07.705138
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 19:16:43.771647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.76
 ---- batch: 020 ----
mean loss: 230.68
 ---- batch: 030 ----
mean loss: 221.94
 ---- batch: 040 ----
mean loss: 225.31
 ---- batch: 050 ----
mean loss: 237.80
 ---- batch: 060 ----
mean loss: 236.03
 ---- batch: 070 ----
mean loss: 234.50
 ---- batch: 080 ----
mean loss: 243.09
 ---- batch: 090 ----
mean loss: 242.08
train mean loss: 233.46
epoch train time: 0:00:02.658443
elapsed time: 0:06:10.364001
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 19:16:46.430555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.37
 ---- batch: 020 ----
mean loss: 227.21
 ---- batch: 030 ----
mean loss: 238.72
 ---- batch: 040 ----
mean loss: 238.09
 ---- batch: 050 ----
mean loss: 237.24
 ---- batch: 060 ----
mean loss: 229.54
 ---- batch: 070 ----
mean loss: 231.72
 ---- batch: 080 ----
mean loss: 239.16
 ---- batch: 090 ----
mean loss: 239.38
train mean loss: 233.47
epoch train time: 0:00:02.663113
elapsed time: 0:06:13.027687
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 19:16:49.094162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.16
 ---- batch: 020 ----
mean loss: 226.97
 ---- batch: 030 ----
mean loss: 229.76
 ---- batch: 040 ----
mean loss: 240.77
 ---- batch: 050 ----
mean loss: 234.64
 ---- batch: 060 ----
mean loss: 237.18
 ---- batch: 070 ----
mean loss: 222.31
 ---- batch: 080 ----
mean loss: 232.35
 ---- batch: 090 ----
mean loss: 239.85
train mean loss: 233.26
epoch train time: 0:00:02.647053
elapsed time: 0:06:15.675161
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 19:16:51.741709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.52
 ---- batch: 020 ----
mean loss: 222.22
 ---- batch: 030 ----
mean loss: 231.24
 ---- batch: 040 ----
mean loss: 230.93
 ---- batch: 050 ----
mean loss: 229.33
 ---- batch: 060 ----
mean loss: 229.91
 ---- batch: 070 ----
mean loss: 232.60
 ---- batch: 080 ----
mean loss: 238.01
 ---- batch: 090 ----
mean loss: 239.95
train mean loss: 232.44
epoch train time: 0:00:02.652271
elapsed time: 0:06:18.328011
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 19:16:54.394559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.80
 ---- batch: 020 ----
mean loss: 231.34
 ---- batch: 030 ----
mean loss: 223.48
 ---- batch: 040 ----
mean loss: 229.51
 ---- batch: 050 ----
mean loss: 227.71
 ---- batch: 060 ----
mean loss: 224.21
 ---- batch: 070 ----
mean loss: 236.35
 ---- batch: 080 ----
mean loss: 243.37
 ---- batch: 090 ----
mean loss: 233.43
train mean loss: 232.49
epoch train time: 0:00:02.680076
elapsed time: 0:06:21.008581
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 19:16:57.075125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.53
 ---- batch: 020 ----
mean loss: 235.10
 ---- batch: 030 ----
mean loss: 231.42
 ---- batch: 040 ----
mean loss: 239.01
 ---- batch: 050 ----
mean loss: 232.31
 ---- batch: 060 ----
mean loss: 229.91
 ---- batch: 070 ----
mean loss: 230.24
 ---- batch: 080 ----
mean loss: 230.44
 ---- batch: 090 ----
mean loss: 231.13
train mean loss: 232.32
epoch train time: 0:00:02.681071
elapsed time: 0:06:23.690125
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 19:16:59.756638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.91
 ---- batch: 020 ----
mean loss: 227.70
 ---- batch: 030 ----
mean loss: 230.73
 ---- batch: 040 ----
mean loss: 233.29
 ---- batch: 050 ----
mean loss: 230.90
 ---- batch: 060 ----
mean loss: 227.21
 ---- batch: 070 ----
mean loss: 230.70
 ---- batch: 080 ----
mean loss: 241.19
 ---- batch: 090 ----
mean loss: 232.23
train mean loss: 232.16
epoch train time: 0:00:02.656845
elapsed time: 0:06:26.347610
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 19:17:02.414239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.45
 ---- batch: 020 ----
mean loss: 231.95
 ---- batch: 030 ----
mean loss: 226.38
 ---- batch: 040 ----
mean loss: 223.52
 ---- batch: 050 ----
mean loss: 232.26
 ---- batch: 060 ----
mean loss: 236.15
 ---- batch: 070 ----
mean loss: 228.15
 ---- batch: 080 ----
mean loss: 238.97
 ---- batch: 090 ----
mean loss: 230.26
train mean loss: 231.69
epoch train time: 0:00:02.664456
elapsed time: 0:06:29.012614
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 19:17:05.079119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.62
 ---- batch: 020 ----
mean loss: 218.05
 ---- batch: 030 ----
mean loss: 226.68
 ---- batch: 040 ----
mean loss: 234.72
 ---- batch: 050 ----
mean loss: 237.66
 ---- batch: 060 ----
mean loss: 226.67
 ---- batch: 070 ----
mean loss: 240.33
 ---- batch: 080 ----
mean loss: 234.97
 ---- batch: 090 ----
mean loss: 231.44
train mean loss: 232.00
epoch train time: 0:00:02.670937
elapsed time: 0:06:31.684066
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 19:17:07.750613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.68
 ---- batch: 020 ----
mean loss: 229.82
 ---- batch: 030 ----
mean loss: 232.83
 ---- batch: 040 ----
mean loss: 226.82
 ---- batch: 050 ----
mean loss: 225.87
 ---- batch: 060 ----
mean loss: 241.55
 ---- batch: 070 ----
mean loss: 234.92
 ---- batch: 080 ----
mean loss: 228.93
 ---- batch: 090 ----
mean loss: 228.35
train mean loss: 231.38
epoch train time: 0:00:02.656389
elapsed time: 0:06:34.340920
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 19:17:10.407429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.43
 ---- batch: 020 ----
mean loss: 229.63
 ---- batch: 030 ----
mean loss: 225.01
 ---- batch: 040 ----
mean loss: 235.43
 ---- batch: 050 ----
mean loss: 225.46
 ---- batch: 060 ----
mean loss: 230.67
 ---- batch: 070 ----
mean loss: 230.18
 ---- batch: 080 ----
mean loss: 231.16
 ---- batch: 090 ----
mean loss: 231.92
train mean loss: 230.96
epoch train time: 0:00:02.664155
elapsed time: 0:06:37.005523
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 19:17:13.072081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.93
 ---- batch: 020 ----
mean loss: 229.69
 ---- batch: 030 ----
mean loss: 231.70
 ---- batch: 040 ----
mean loss: 227.14
 ---- batch: 050 ----
mean loss: 228.53
 ---- batch: 060 ----
mean loss: 226.79
 ---- batch: 070 ----
mean loss: 226.18
 ---- batch: 080 ----
mean loss: 234.40
 ---- batch: 090 ----
mean loss: 234.14
train mean loss: 230.49
epoch train time: 0:00:02.663200
elapsed time: 0:06:39.669214
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 19:17:15.735722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.10
 ---- batch: 020 ----
mean loss: 230.81
 ---- batch: 030 ----
mean loss: 230.91
 ---- batch: 040 ----
mean loss: 238.20
 ---- batch: 050 ----
mean loss: 228.36
 ---- batch: 060 ----
mean loss: 236.34
 ---- batch: 070 ----
mean loss: 234.26
 ---- batch: 080 ----
mean loss: 230.69
 ---- batch: 090 ----
mean loss: 222.83
train mean loss: 230.30
epoch train time: 0:00:02.681114
elapsed time: 0:06:42.350747
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 19:17:18.417253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.07
 ---- batch: 020 ----
mean loss: 229.51
 ---- batch: 030 ----
mean loss: 234.41
 ---- batch: 040 ----
mean loss: 227.72
 ---- batch: 050 ----
mean loss: 239.05
 ---- batch: 060 ----
mean loss: 223.26
 ---- batch: 070 ----
mean loss: 226.96
 ---- batch: 080 ----
mean loss: 232.65
 ---- batch: 090 ----
mean loss: 233.76
train mean loss: 230.40
epoch train time: 0:00:02.652927
elapsed time: 0:06:45.004109
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 19:17:21.070640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.08
 ---- batch: 020 ----
mean loss: 227.05
 ---- batch: 030 ----
mean loss: 234.74
 ---- batch: 040 ----
mean loss: 222.49
 ---- batch: 050 ----
mean loss: 226.25
 ---- batch: 060 ----
mean loss: 230.92
 ---- batch: 070 ----
mean loss: 230.62
 ---- batch: 080 ----
mean loss: 239.75
 ---- batch: 090 ----
mean loss: 225.93
train mean loss: 229.21
epoch train time: 0:00:02.683546
elapsed time: 0:06:47.688136
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 19:17:23.754682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.29
 ---- batch: 020 ----
mean loss: 229.80
 ---- batch: 030 ----
mean loss: 227.38
 ---- batch: 040 ----
mean loss: 232.35
 ---- batch: 050 ----
mean loss: 230.31
 ---- batch: 060 ----
mean loss: 231.53
 ---- batch: 070 ----
mean loss: 222.33
 ---- batch: 080 ----
mean loss: 228.84
 ---- batch: 090 ----
mean loss: 237.88
train mean loss: 229.80
epoch train time: 0:00:02.670980
elapsed time: 0:06:50.359598
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 19:17:26.426100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.13
 ---- batch: 020 ----
mean loss: 229.75
 ---- batch: 030 ----
mean loss: 234.57
 ---- batch: 040 ----
mean loss: 221.47
 ---- batch: 050 ----
mean loss: 234.08
 ---- batch: 060 ----
mean loss: 228.42
 ---- batch: 070 ----
mean loss: 229.31
 ---- batch: 080 ----
mean loss: 224.05
 ---- batch: 090 ----
mean loss: 230.23
train mean loss: 229.43
epoch train time: 0:00:02.662977
elapsed time: 0:06:53.022999
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 19:17:29.089537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.11
 ---- batch: 020 ----
mean loss: 228.84
 ---- batch: 030 ----
mean loss: 232.75
 ---- batch: 040 ----
mean loss: 227.09
 ---- batch: 050 ----
mean loss: 233.04
 ---- batch: 060 ----
mean loss: 231.45
 ---- batch: 070 ----
mean loss: 227.69
 ---- batch: 080 ----
mean loss: 226.08
 ---- batch: 090 ----
mean loss: 225.03
train mean loss: 229.12
epoch train time: 0:00:02.692295
elapsed time: 0:06:55.715832
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 19:17:31.782343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.49
 ---- batch: 020 ----
mean loss: 233.56
 ---- batch: 030 ----
mean loss: 220.10
 ---- batch: 040 ----
mean loss: 233.73
 ---- batch: 050 ----
mean loss: 225.61
 ---- batch: 060 ----
mean loss: 222.48
 ---- batch: 070 ----
mean loss: 228.73
 ---- batch: 080 ----
mean loss: 225.70
 ---- batch: 090 ----
mean loss: 223.36
train mean loss: 228.68
epoch train time: 0:00:02.657643
elapsed time: 0:06:58.373961
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 19:17:34.440476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.74
 ---- batch: 020 ----
mean loss: 236.23
 ---- batch: 030 ----
mean loss: 227.79
 ---- batch: 040 ----
mean loss: 227.49
 ---- batch: 050 ----
mean loss: 218.02
 ---- batch: 060 ----
mean loss: 234.64
 ---- batch: 070 ----
mean loss: 232.91
 ---- batch: 080 ----
mean loss: 226.94
 ---- batch: 090 ----
mean loss: 229.65
train mean loss: 228.38
epoch train time: 0:00:02.664239
elapsed time: 0:07:01.038620
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 19:17:37.105131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.79
 ---- batch: 020 ----
mean loss: 232.37
 ---- batch: 030 ----
mean loss: 222.19
 ---- batch: 040 ----
mean loss: 230.61
 ---- batch: 050 ----
mean loss: 229.02
 ---- batch: 060 ----
mean loss: 235.04
 ---- batch: 070 ----
mean loss: 221.51
 ---- batch: 080 ----
mean loss: 235.11
 ---- batch: 090 ----
mean loss: 228.35
train mean loss: 228.02
epoch train time: 0:00:02.673059
elapsed time: 0:07:03.712096
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 19:17:39.778609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.08
 ---- batch: 020 ----
mean loss: 228.50
 ---- batch: 030 ----
mean loss: 230.30
 ---- batch: 040 ----
mean loss: 228.92
 ---- batch: 050 ----
mean loss: 225.26
 ---- batch: 060 ----
mean loss: 227.82
 ---- batch: 070 ----
mean loss: 226.82
 ---- batch: 080 ----
mean loss: 223.97
 ---- batch: 090 ----
mean loss: 226.85
train mean loss: 228.13
epoch train time: 0:00:02.676012
elapsed time: 0:07:06.388588
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 19:17:42.455099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.45
 ---- batch: 020 ----
mean loss: 237.02
 ---- batch: 030 ----
mean loss: 226.78
 ---- batch: 040 ----
mean loss: 227.31
 ---- batch: 050 ----
mean loss: 225.81
 ---- batch: 060 ----
mean loss: 228.57
 ---- batch: 070 ----
mean loss: 217.14
 ---- batch: 080 ----
mean loss: 235.04
 ---- batch: 090 ----
mean loss: 229.69
train mean loss: 227.90
epoch train time: 0:00:02.671585
elapsed time: 0:07:09.060612
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 19:17:45.127145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.44
 ---- batch: 020 ----
mean loss: 223.93
 ---- batch: 030 ----
mean loss: 228.16
 ---- batch: 040 ----
mean loss: 231.70
 ---- batch: 050 ----
mean loss: 222.05
 ---- batch: 060 ----
mean loss: 223.46
 ---- batch: 070 ----
mean loss: 226.79
 ---- batch: 080 ----
mean loss: 232.10
 ---- batch: 090 ----
mean loss: 226.43
train mean loss: 227.45
epoch train time: 0:00:02.661501
elapsed time: 0:07:11.722748
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 19:17:47.789184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.90
 ---- batch: 020 ----
mean loss: 225.19
 ---- batch: 030 ----
mean loss: 226.15
 ---- batch: 040 ----
mean loss: 226.91
 ---- batch: 050 ----
mean loss: 230.94
 ---- batch: 060 ----
mean loss: 232.74
 ---- batch: 070 ----
mean loss: 228.69
 ---- batch: 080 ----
mean loss: 223.23
 ---- batch: 090 ----
mean loss: 232.45
train mean loss: 227.25
epoch train time: 0:00:02.674917
elapsed time: 0:07:14.398034
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 19:17:50.464571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.27
 ---- batch: 020 ----
mean loss: 234.33
 ---- batch: 030 ----
mean loss: 224.14
 ---- batch: 040 ----
mean loss: 231.52
 ---- batch: 050 ----
mean loss: 218.89
 ---- batch: 060 ----
mean loss: 227.99
 ---- batch: 070 ----
mean loss: 219.42
 ---- batch: 080 ----
mean loss: 231.32
 ---- batch: 090 ----
mean loss: 221.46
train mean loss: 227.14
epoch train time: 0:00:02.660185
elapsed time: 0:07:17.058715
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 19:17:53.125242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.62
 ---- batch: 020 ----
mean loss: 229.22
 ---- batch: 030 ----
mean loss: 224.78
 ---- batch: 040 ----
mean loss: 225.64
 ---- batch: 050 ----
mean loss: 222.88
 ---- batch: 060 ----
mean loss: 228.23
 ---- batch: 070 ----
mean loss: 224.14
 ---- batch: 080 ----
mean loss: 226.91
 ---- batch: 090 ----
mean loss: 239.13
train mean loss: 226.73
epoch train time: 0:00:02.685853
elapsed time: 0:07:19.745117
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 19:17:55.811651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.56
 ---- batch: 020 ----
mean loss: 234.51
 ---- batch: 030 ----
mean loss: 228.35
 ---- batch: 040 ----
mean loss: 220.19
 ---- batch: 050 ----
mean loss: 228.80
 ---- batch: 060 ----
mean loss: 221.54
 ---- batch: 070 ----
mean loss: 225.90
 ---- batch: 080 ----
mean loss: 225.42
 ---- batch: 090 ----
mean loss: 229.27
train mean loss: 226.23
epoch train time: 0:00:02.665689
elapsed time: 0:07:22.411293
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 19:17:58.477805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.12
 ---- batch: 020 ----
mean loss: 215.16
 ---- batch: 030 ----
mean loss: 233.60
 ---- batch: 040 ----
mean loss: 236.78
 ---- batch: 050 ----
mean loss: 229.63
 ---- batch: 060 ----
mean loss: 222.87
 ---- batch: 070 ----
mean loss: 223.86
 ---- batch: 080 ----
mean loss: 220.17
 ---- batch: 090 ----
mean loss: 221.86
train mean loss: 226.51
epoch train time: 0:00:02.662524
elapsed time: 0:07:25.074313
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 19:18:01.140820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.12
 ---- batch: 020 ----
mean loss: 222.83
 ---- batch: 030 ----
mean loss: 220.62
 ---- batch: 040 ----
mean loss: 231.54
 ---- batch: 050 ----
mean loss: 227.36
 ---- batch: 060 ----
mean loss: 231.23
 ---- batch: 070 ----
mean loss: 223.33
 ---- batch: 080 ----
mean loss: 229.39
 ---- batch: 090 ----
mean loss: 223.32
train mean loss: 226.11
epoch train time: 0:00:02.650577
elapsed time: 0:07:27.725387
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 19:18:03.791914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.61
 ---- batch: 020 ----
mean loss: 229.65
 ---- batch: 030 ----
mean loss: 229.12
 ---- batch: 040 ----
mean loss: 218.45
 ---- batch: 050 ----
mean loss: 226.76
 ---- batch: 060 ----
mean loss: 233.45
 ---- batch: 070 ----
mean loss: 223.86
 ---- batch: 080 ----
mean loss: 223.19
 ---- batch: 090 ----
mean loss: 227.74
train mean loss: 226.14
epoch train time: 0:00:02.674353
elapsed time: 0:07:30.400271
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 19:18:06.466817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.17
 ---- batch: 020 ----
mean loss: 218.76
 ---- batch: 030 ----
mean loss: 229.24
 ---- batch: 040 ----
mean loss: 231.26
 ---- batch: 050 ----
mean loss: 218.13
 ---- batch: 060 ----
mean loss: 227.03
 ---- batch: 070 ----
mean loss: 233.34
 ---- batch: 080 ----
mean loss: 236.89
 ---- batch: 090 ----
mean loss: 219.13
train mean loss: 225.64
epoch train time: 0:00:02.641446
elapsed time: 0:07:33.042178
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 19:18:09.108723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.58
 ---- batch: 020 ----
mean loss: 231.99
 ---- batch: 030 ----
mean loss: 223.97
 ---- batch: 040 ----
mean loss: 225.42
 ---- batch: 050 ----
mean loss: 229.08
 ---- batch: 060 ----
mean loss: 225.01
 ---- batch: 070 ----
mean loss: 223.28
 ---- batch: 080 ----
mean loss: 222.20
 ---- batch: 090 ----
mean loss: 227.02
train mean loss: 225.46
epoch train time: 0:00:02.678316
elapsed time: 0:07:35.720962
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 19:18:11.787473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.09
 ---- batch: 020 ----
mean loss: 220.56
 ---- batch: 030 ----
mean loss: 228.25
 ---- batch: 040 ----
mean loss: 223.66
 ---- batch: 050 ----
mean loss: 217.67
 ---- batch: 060 ----
mean loss: 222.49
 ---- batch: 070 ----
mean loss: 223.51
 ---- batch: 080 ----
mean loss: 231.47
 ---- batch: 090 ----
mean loss: 221.02
train mean loss: 225.42
epoch train time: 0:00:02.653559
elapsed time: 0:07:38.374976
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 19:18:14.441519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.01
 ---- batch: 020 ----
mean loss: 218.81
 ---- batch: 030 ----
mean loss: 231.14
 ---- batch: 040 ----
mean loss: 229.05
 ---- batch: 050 ----
mean loss: 229.46
 ---- batch: 060 ----
mean loss: 222.05
 ---- batch: 070 ----
mean loss: 225.48
 ---- batch: 080 ----
mean loss: 220.37
 ---- batch: 090 ----
mean loss: 224.34
train mean loss: 224.89
epoch train time: 0:00:02.654064
elapsed time: 0:07:41.029509
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 19:18:17.096016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.75
 ---- batch: 020 ----
mean loss: 222.31
 ---- batch: 030 ----
mean loss: 218.46
 ---- batch: 040 ----
mean loss: 227.27
 ---- batch: 050 ----
mean loss: 222.23
 ---- batch: 060 ----
mean loss: 223.13
 ---- batch: 070 ----
mean loss: 230.65
 ---- batch: 080 ----
mean loss: 228.77
 ---- batch: 090 ----
mean loss: 225.09
train mean loss: 224.78
epoch train time: 0:00:02.695903
elapsed time: 0:07:43.725873
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 19:18:19.792432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.02
 ---- batch: 020 ----
mean loss: 218.86
 ---- batch: 030 ----
mean loss: 231.97
 ---- batch: 040 ----
mean loss: 219.24
 ---- batch: 050 ----
mean loss: 217.67
 ---- batch: 060 ----
mean loss: 230.21
 ---- batch: 070 ----
mean loss: 233.55
 ---- batch: 080 ----
mean loss: 219.43
 ---- batch: 090 ----
mean loss: 223.67
train mean loss: 224.45
epoch train time: 0:00:02.647138
elapsed time: 0:07:46.373552
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 19:18:22.440062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.50
 ---- batch: 020 ----
mean loss: 228.45
 ---- batch: 030 ----
mean loss: 234.14
 ---- batch: 040 ----
mean loss: 224.33
 ---- batch: 050 ----
mean loss: 218.72
 ---- batch: 060 ----
mean loss: 230.48
 ---- batch: 070 ----
mean loss: 221.18
 ---- batch: 080 ----
mean loss: 226.61
 ---- batch: 090 ----
mean loss: 220.97
train mean loss: 224.18
epoch train time: 0:00:02.654709
elapsed time: 0:07:49.028686
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 19:18:25.095295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.63
 ---- batch: 020 ----
mean loss: 216.27
 ---- batch: 030 ----
mean loss: 226.26
 ---- batch: 040 ----
mean loss: 225.65
 ---- batch: 050 ----
mean loss: 223.68
 ---- batch: 060 ----
mean loss: 225.55
 ---- batch: 070 ----
mean loss: 226.74
 ---- batch: 080 ----
mean loss: 221.00
 ---- batch: 090 ----
mean loss: 227.72
train mean loss: 224.33
epoch train time: 0:00:02.665070
elapsed time: 0:07:51.694368
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 19:18:27.760895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.70
 ---- batch: 020 ----
mean loss: 221.65
 ---- batch: 030 ----
mean loss: 224.02
 ---- batch: 040 ----
mean loss: 217.24
 ---- batch: 050 ----
mean loss: 231.85
 ---- batch: 060 ----
mean loss: 228.44
 ---- batch: 070 ----
mean loss: 223.58
 ---- batch: 080 ----
mean loss: 218.16
 ---- batch: 090 ----
mean loss: 231.14
train mean loss: 223.94
epoch train time: 0:00:02.686118
elapsed time: 0:07:54.380959
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 19:18:30.447490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.68
 ---- batch: 020 ----
mean loss: 221.51
 ---- batch: 030 ----
mean loss: 215.63
 ---- batch: 040 ----
mean loss: 232.47
 ---- batch: 050 ----
mean loss: 227.31
 ---- batch: 060 ----
mean loss: 232.07
 ---- batch: 070 ----
mean loss: 216.53
 ---- batch: 080 ----
mean loss: 213.84
 ---- batch: 090 ----
mean loss: 227.22
train mean loss: 223.68
epoch train time: 0:00:02.668794
elapsed time: 0:07:57.050194
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 19:18:33.116818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.14
 ---- batch: 020 ----
mean loss: 222.17
 ---- batch: 030 ----
mean loss: 226.64
 ---- batch: 040 ----
mean loss: 220.49
 ---- batch: 050 ----
mean loss: 223.07
 ---- batch: 060 ----
mean loss: 217.41
 ---- batch: 070 ----
mean loss: 213.31
 ---- batch: 080 ----
mean loss: 226.42
 ---- batch: 090 ----
mean loss: 230.00
train mean loss: 223.35
epoch train time: 0:00:02.667297
elapsed time: 0:07:59.718125
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 19:18:35.784644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.82
 ---- batch: 020 ----
mean loss: 222.23
 ---- batch: 030 ----
mean loss: 227.74
 ---- batch: 040 ----
mean loss: 215.43
 ---- batch: 050 ----
mean loss: 223.99
 ---- batch: 060 ----
mean loss: 218.65
 ---- batch: 070 ----
mean loss: 226.64
 ---- batch: 080 ----
mean loss: 222.50
 ---- batch: 090 ----
mean loss: 223.86
train mean loss: 223.33
epoch train time: 0:00:02.672683
elapsed time: 0:08:02.391258
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 19:18:38.457800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.86
 ---- batch: 020 ----
mean loss: 223.86
 ---- batch: 030 ----
mean loss: 215.28
 ---- batch: 040 ----
mean loss: 218.92
 ---- batch: 050 ----
mean loss: 224.59
 ---- batch: 060 ----
mean loss: 230.37
 ---- batch: 070 ----
mean loss: 219.24
 ---- batch: 080 ----
mean loss: 229.30
 ---- batch: 090 ----
mean loss: 220.41
train mean loss: 223.26
epoch train time: 0:00:02.680266
elapsed time: 0:08:05.072015
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 19:18:41.138544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.50
 ---- batch: 020 ----
mean loss: 218.21
 ---- batch: 030 ----
mean loss: 219.13
 ---- batch: 040 ----
mean loss: 226.77
 ---- batch: 050 ----
mean loss: 219.44
 ---- batch: 060 ----
mean loss: 226.45
 ---- batch: 070 ----
mean loss: 229.48
 ---- batch: 080 ----
mean loss: 226.02
 ---- batch: 090 ----
mean loss: 219.26
train mean loss: 222.78
epoch train time: 0:00:02.709030
elapsed time: 0:08:07.781524
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 19:18:43.848024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.45
 ---- batch: 020 ----
mean loss: 224.21
 ---- batch: 030 ----
mean loss: 222.06
 ---- batch: 040 ----
mean loss: 224.20
 ---- batch: 050 ----
mean loss: 228.87
 ---- batch: 060 ----
mean loss: 223.54
 ---- batch: 070 ----
mean loss: 223.26
 ---- batch: 080 ----
mean loss: 210.66
 ---- batch: 090 ----
mean loss: 218.73
train mean loss: 222.08
epoch train time: 0:00:02.660394
elapsed time: 0:08:10.442382
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 19:18:46.508891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.84
 ---- batch: 020 ----
mean loss: 218.11
 ---- batch: 030 ----
mean loss: 212.27
 ---- batch: 040 ----
mean loss: 223.79
 ---- batch: 050 ----
mean loss: 221.54
 ---- batch: 060 ----
mean loss: 226.04
 ---- batch: 070 ----
mean loss: 232.01
 ---- batch: 080 ----
mean loss: 216.93
 ---- batch: 090 ----
mean loss: 227.83
train mean loss: 222.35
epoch train time: 0:00:02.634345
elapsed time: 0:08:13.077169
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 19:18:49.143795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.21
 ---- batch: 020 ----
mean loss: 211.92
 ---- batch: 030 ----
mean loss: 226.22
 ---- batch: 040 ----
mean loss: 229.60
 ---- batch: 050 ----
mean loss: 219.74
 ---- batch: 060 ----
mean loss: 214.41
 ---- batch: 070 ----
mean loss: 221.81
 ---- batch: 080 ----
mean loss: 223.84
 ---- batch: 090 ----
mean loss: 220.03
train mean loss: 222.03
epoch train time: 0:00:02.641179
elapsed time: 0:08:15.718924
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 19:18:51.785434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.11
 ---- batch: 020 ----
mean loss: 214.34
 ---- batch: 030 ----
mean loss: 230.02
 ---- batch: 040 ----
mean loss: 216.53
 ---- batch: 050 ----
mean loss: 216.30
 ---- batch: 060 ----
mean loss: 226.66
 ---- batch: 070 ----
mean loss: 221.98
 ---- batch: 080 ----
mean loss: 226.48
 ---- batch: 090 ----
mean loss: 220.00
train mean loss: 221.73
epoch train time: 0:00:02.675965
elapsed time: 0:08:18.395485
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 19:18:54.461895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.13
 ---- batch: 020 ----
mean loss: 224.16
 ---- batch: 030 ----
mean loss: 233.70
 ---- batch: 040 ----
mean loss: 214.37
 ---- batch: 050 ----
mean loss: 217.23
 ---- batch: 060 ----
mean loss: 224.75
 ---- batch: 070 ----
mean loss: 212.64
 ---- batch: 080 ----
mean loss: 221.94
 ---- batch: 090 ----
mean loss: 223.08
train mean loss: 221.37
epoch train time: 0:00:02.676064
elapsed time: 0:08:21.071945
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 19:18:57.138466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.44
 ---- batch: 020 ----
mean loss: 213.87
 ---- batch: 030 ----
mean loss: 220.06
 ---- batch: 040 ----
mean loss: 226.16
 ---- batch: 050 ----
mean loss: 221.69
 ---- batch: 060 ----
mean loss: 222.65
 ---- batch: 070 ----
mean loss: 225.11
 ---- batch: 080 ----
mean loss: 217.01
 ---- batch: 090 ----
mean loss: 220.36
train mean loss: 221.16
epoch train time: 0:00:02.679357
elapsed time: 0:08:23.751791
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 19:18:59.818358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.13
 ---- batch: 020 ----
mean loss: 219.11
 ---- batch: 030 ----
mean loss: 220.15
 ---- batch: 040 ----
mean loss: 221.95
 ---- batch: 050 ----
mean loss: 228.25
 ---- batch: 060 ----
mean loss: 221.75
 ---- batch: 070 ----
mean loss: 216.04
 ---- batch: 080 ----
mean loss: 214.15
 ---- batch: 090 ----
mean loss: 227.24
train mean loss: 221.04
epoch train time: 0:00:02.647863
elapsed time: 0:08:26.400176
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 19:19:02.466693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.71
 ---- batch: 020 ----
mean loss: 227.14
 ---- batch: 030 ----
mean loss: 219.72
 ---- batch: 040 ----
mean loss: 213.94
 ---- batch: 050 ----
mean loss: 213.76
 ---- batch: 060 ----
mean loss: 223.73
 ---- batch: 070 ----
mean loss: 224.53
 ---- batch: 080 ----
mean loss: 222.33
 ---- batch: 090 ----
mean loss: 216.45
train mean loss: 220.60
epoch train time: 0:00:02.693931
elapsed time: 0:08:29.094553
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 19:19:05.161065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.77
 ---- batch: 020 ----
mean loss: 223.33
 ---- batch: 030 ----
mean loss: 223.27
 ---- batch: 040 ----
mean loss: 222.56
 ---- batch: 050 ----
mean loss: 212.65
 ---- batch: 060 ----
mean loss: 225.44
 ---- batch: 070 ----
mean loss: 216.78
 ---- batch: 080 ----
mean loss: 213.72
 ---- batch: 090 ----
mean loss: 219.18
train mean loss: 220.58
epoch train time: 0:00:02.658069
elapsed time: 0:08:31.753060
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 19:19:07.819610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.70
 ---- batch: 020 ----
mean loss: 221.33
 ---- batch: 030 ----
mean loss: 225.27
 ---- batch: 040 ----
mean loss: 220.80
 ---- batch: 050 ----
mean loss: 225.54
 ---- batch: 060 ----
mean loss: 220.09
 ---- batch: 070 ----
mean loss: 213.91
 ---- batch: 080 ----
mean loss: 218.10
 ---- batch: 090 ----
mean loss: 218.71
train mean loss: 220.51
epoch train time: 0:00:02.636685
elapsed time: 0:08:34.390226
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 19:19:10.456733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.99
 ---- batch: 020 ----
mean loss: 219.41
 ---- batch: 030 ----
mean loss: 220.94
 ---- batch: 040 ----
mean loss: 229.36
 ---- batch: 050 ----
mean loss: 217.34
 ---- batch: 060 ----
mean loss: 215.58
 ---- batch: 070 ----
mean loss: 216.26
 ---- batch: 080 ----
mean loss: 219.98
 ---- batch: 090 ----
mean loss: 219.55
train mean loss: 220.48
epoch train time: 0:00:02.655647
elapsed time: 0:08:37.046335
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 19:19:13.112841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.45
 ---- batch: 020 ----
mean loss: 224.45
 ---- batch: 030 ----
mean loss: 221.20
 ---- batch: 040 ----
mean loss: 222.41
 ---- batch: 050 ----
mean loss: 213.68
 ---- batch: 060 ----
mean loss: 216.93
 ---- batch: 070 ----
mean loss: 223.79
 ---- batch: 080 ----
mean loss: 220.18
 ---- batch: 090 ----
mean loss: 216.95
train mean loss: 220.00
epoch train time: 0:00:02.684885
elapsed time: 0:08:39.731719
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 19:19:15.798224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.35
 ---- batch: 020 ----
mean loss: 219.54
 ---- batch: 030 ----
mean loss: 227.05
 ---- batch: 040 ----
mean loss: 218.57
 ---- batch: 050 ----
mean loss: 214.83
 ---- batch: 060 ----
mean loss: 224.74
 ---- batch: 070 ----
mean loss: 217.09
 ---- batch: 080 ----
mean loss: 220.78
 ---- batch: 090 ----
mean loss: 207.94
train mean loss: 219.48
epoch train time: 0:00:02.677374
elapsed time: 0:08:42.409564
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 19:19:18.476149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.59
 ---- batch: 020 ----
mean loss: 219.42
 ---- batch: 030 ----
mean loss: 214.00
 ---- batch: 040 ----
mean loss: 225.46
 ---- batch: 050 ----
mean loss: 221.91
 ---- batch: 060 ----
mean loss: 217.71
 ---- batch: 070 ----
mean loss: 220.00
 ---- batch: 080 ----
mean loss: 220.82
 ---- batch: 090 ----
mean loss: 217.52
train mean loss: 219.58
epoch train time: 0:00:02.661645
elapsed time: 0:08:45.071711
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 19:19:21.138238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.05
 ---- batch: 020 ----
mean loss: 217.58
 ---- batch: 030 ----
mean loss: 219.32
 ---- batch: 040 ----
mean loss: 219.87
 ---- batch: 050 ----
mean loss: 223.25
 ---- batch: 060 ----
mean loss: 227.58
 ---- batch: 070 ----
mean loss: 216.94
 ---- batch: 080 ----
mean loss: 212.05
 ---- batch: 090 ----
mean loss: 216.70
train mean loss: 219.55
epoch train time: 0:00:02.674024
elapsed time: 0:08:47.746224
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 19:19:23.812734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.46
 ---- batch: 020 ----
mean loss: 213.35
 ---- batch: 030 ----
mean loss: 218.37
 ---- batch: 040 ----
mean loss: 226.95
 ---- batch: 050 ----
mean loss: 230.49
 ---- batch: 060 ----
mean loss: 219.06
 ---- batch: 070 ----
mean loss: 214.78
 ---- batch: 080 ----
mean loss: 216.02
 ---- batch: 090 ----
mean loss: 218.63
train mean loss: 219.03
epoch train time: 0:00:02.683743
elapsed time: 0:08:50.430463
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 19:19:26.496990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.51
 ---- batch: 020 ----
mean loss: 214.35
 ---- batch: 030 ----
mean loss: 222.46
 ---- batch: 040 ----
mean loss: 220.49
 ---- batch: 050 ----
mean loss: 214.27
 ---- batch: 060 ----
mean loss: 221.12
 ---- batch: 070 ----
mean loss: 222.92
 ---- batch: 080 ----
mean loss: 216.41
 ---- batch: 090 ----
mean loss: 222.39
train mean loss: 218.99
epoch train time: 0:00:02.683423
elapsed time: 0:08:53.114376
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 19:19:29.180902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.49
 ---- batch: 020 ----
mean loss: 221.22
 ---- batch: 030 ----
mean loss: 214.84
 ---- batch: 040 ----
mean loss: 216.26
 ---- batch: 050 ----
mean loss: 222.73
 ---- batch: 060 ----
mean loss: 219.71
 ---- batch: 070 ----
mean loss: 216.96
 ---- batch: 080 ----
mean loss: 220.30
 ---- batch: 090 ----
mean loss: 220.53
train mean loss: 218.49
epoch train time: 0:00:02.649054
elapsed time: 0:08:55.763914
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 19:19:31.830440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.04
 ---- batch: 020 ----
mean loss: 221.18
 ---- batch: 030 ----
mean loss: 214.97
 ---- batch: 040 ----
mean loss: 215.45
 ---- batch: 050 ----
mean loss: 222.12
 ---- batch: 060 ----
mean loss: 214.74
 ---- batch: 070 ----
mean loss: 220.34
 ---- batch: 080 ----
mean loss: 221.41
 ---- batch: 090 ----
mean loss: 219.37
train mean loss: 218.66
epoch train time: 0:00:02.662337
elapsed time: 0:08:58.426738
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 19:19:34.493258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.56
 ---- batch: 020 ----
mean loss: 217.73
 ---- batch: 030 ----
mean loss: 227.22
 ---- batch: 040 ----
mean loss: 211.42
 ---- batch: 050 ----
mean loss: 219.34
 ---- batch: 060 ----
mean loss: 207.41
 ---- batch: 070 ----
mean loss: 212.18
 ---- batch: 080 ----
mean loss: 223.57
 ---- batch: 090 ----
mean loss: 224.91
train mean loss: 218.21
epoch train time: 0:00:02.666038
elapsed time: 0:09:01.093205
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 19:19:37.159730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.95
 ---- batch: 020 ----
mean loss: 216.33
 ---- batch: 030 ----
mean loss: 220.00
 ---- batch: 040 ----
mean loss: 222.55
 ---- batch: 050 ----
mean loss: 221.71
 ---- batch: 060 ----
mean loss: 216.02
 ---- batch: 070 ----
mean loss: 216.84
 ---- batch: 080 ----
mean loss: 227.36
 ---- batch: 090 ----
mean loss: 207.93
train mean loss: 218.09
epoch train time: 0:00:02.637174
elapsed time: 0:09:03.730911
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 19:19:39.797414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.72
 ---- batch: 020 ----
mean loss: 221.90
 ---- batch: 030 ----
mean loss: 220.04
 ---- batch: 040 ----
mean loss: 212.69
 ---- batch: 050 ----
mean loss: 223.93
 ---- batch: 060 ----
mean loss: 217.24
 ---- batch: 070 ----
mean loss: 220.76
 ---- batch: 080 ----
mean loss: 213.84
 ---- batch: 090 ----
mean loss: 212.16
train mean loss: 217.82
epoch train time: 0:00:02.698934
elapsed time: 0:09:06.430277
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 19:19:42.496791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.91
 ---- batch: 020 ----
mean loss: 219.20
 ---- batch: 030 ----
mean loss: 212.50
 ---- batch: 040 ----
mean loss: 220.36
 ---- batch: 050 ----
mean loss: 216.95
 ---- batch: 060 ----
mean loss: 220.86
 ---- batch: 070 ----
mean loss: 210.18
 ---- batch: 080 ----
mean loss: 215.40
 ---- batch: 090 ----
mean loss: 216.36
train mean loss: 217.20
epoch train time: 0:00:02.686553
elapsed time: 0:09:09.117313
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 19:19:45.183875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.40
 ---- batch: 020 ----
mean loss: 211.47
 ---- batch: 030 ----
mean loss: 219.15
 ---- batch: 040 ----
mean loss: 217.40
 ---- batch: 050 ----
mean loss: 214.66
 ---- batch: 060 ----
mean loss: 206.08
 ---- batch: 070 ----
mean loss: 222.63
 ---- batch: 080 ----
mean loss: 222.50
 ---- batch: 090 ----
mean loss: 220.80
train mean loss: 217.42
epoch train time: 0:00:02.693273
elapsed time: 0:09:11.811098
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 19:19:47.877652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.39
 ---- batch: 020 ----
mean loss: 214.77
 ---- batch: 030 ----
mean loss: 216.46
 ---- batch: 040 ----
mean loss: 216.29
 ---- batch: 050 ----
mean loss: 213.95
 ---- batch: 060 ----
mean loss: 219.62
 ---- batch: 070 ----
mean loss: 222.55
 ---- batch: 080 ----
mean loss: 215.19
 ---- batch: 090 ----
mean loss: 215.60
train mean loss: 217.46
epoch train time: 0:00:02.647591
elapsed time: 0:09:14.459214
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 19:19:50.525744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.38
 ---- batch: 020 ----
mean loss: 214.79
 ---- batch: 030 ----
mean loss: 224.54
 ---- batch: 040 ----
mean loss: 214.84
 ---- batch: 050 ----
mean loss: 214.69
 ---- batch: 060 ----
mean loss: 222.01
 ---- batch: 070 ----
mean loss: 220.30
 ---- batch: 080 ----
mean loss: 215.79
 ---- batch: 090 ----
mean loss: 212.64
train mean loss: 217.22
epoch train time: 0:00:02.667733
elapsed time: 0:09:17.127433
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 19:19:53.193945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.02
 ---- batch: 020 ----
mean loss: 215.37
 ---- batch: 030 ----
mean loss: 215.27
 ---- batch: 040 ----
mean loss: 216.96
 ---- batch: 050 ----
mean loss: 214.32
 ---- batch: 060 ----
mean loss: 213.42
 ---- batch: 070 ----
mean loss: 216.47
 ---- batch: 080 ----
mean loss: 212.55
 ---- batch: 090 ----
mean loss: 222.73
train mean loss: 216.68
epoch train time: 0:00:02.649032
elapsed time: 0:09:19.776920
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 19:19:55.843442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.43
 ---- batch: 020 ----
mean loss: 220.94
 ---- batch: 030 ----
mean loss: 219.71
 ---- batch: 040 ----
mean loss: 211.29
 ---- batch: 050 ----
mean loss: 215.72
 ---- batch: 060 ----
mean loss: 221.45
 ---- batch: 070 ----
mean loss: 211.10
 ---- batch: 080 ----
mean loss: 210.90
 ---- batch: 090 ----
mean loss: 224.64
train mean loss: 216.48
epoch train time: 0:00:02.661239
elapsed time: 0:09:22.438652
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 19:19:58.505168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.98
 ---- batch: 020 ----
mean loss: 212.43
 ---- batch: 030 ----
mean loss: 215.51
 ---- batch: 040 ----
mean loss: 223.07
 ---- batch: 050 ----
mean loss: 225.18
 ---- batch: 060 ----
mean loss: 216.62
 ---- batch: 070 ----
mean loss: 219.29
 ---- batch: 080 ----
mean loss: 220.10
 ---- batch: 090 ----
mean loss: 208.90
train mean loss: 216.55
epoch train time: 0:00:02.651901
elapsed time: 0:09:25.091035
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 19:20:01.157635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.40
 ---- batch: 020 ----
mean loss: 206.26
 ---- batch: 030 ----
mean loss: 220.18
 ---- batch: 040 ----
mean loss: 215.32
 ---- batch: 050 ----
mean loss: 219.61
 ---- batch: 060 ----
mean loss: 217.50
 ---- batch: 070 ----
mean loss: 211.71
 ---- batch: 080 ----
mean loss: 222.17
 ---- batch: 090 ----
mean loss: 213.42
train mean loss: 216.01
epoch train time: 0:00:02.650089
elapsed time: 0:09:27.741651
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 19:20:03.808165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.44
 ---- batch: 020 ----
mean loss: 215.72
 ---- batch: 030 ----
mean loss: 207.52
 ---- batch: 040 ----
mean loss: 218.58
 ---- batch: 050 ----
mean loss: 215.88
 ---- batch: 060 ----
mean loss: 210.95
 ---- batch: 070 ----
mean loss: 217.35
 ---- batch: 080 ----
mean loss: 219.96
 ---- batch: 090 ----
mean loss: 219.51
train mean loss: 216.11
epoch train time: 0:00:02.651777
elapsed time: 0:09:30.393974
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 19:20:06.460504
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.31
 ---- batch: 020 ----
mean loss: 211.73
 ---- batch: 030 ----
mean loss: 213.08
 ---- batch: 040 ----
mean loss: 226.08
 ---- batch: 050 ----
mean loss: 214.73
 ---- batch: 060 ----
mean loss: 206.67
 ---- batch: 070 ----
mean loss: 215.31
 ---- batch: 080 ----
mean loss: 217.19
 ---- batch: 090 ----
mean loss: 218.03
train mean loss: 215.62
epoch train time: 0:00:02.646794
elapsed time: 0:09:33.041393
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 19:20:09.107814
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.32
 ---- batch: 020 ----
mean loss: 214.02
 ---- batch: 030 ----
mean loss: 216.41
 ---- batch: 040 ----
mean loss: 212.00
 ---- batch: 050 ----
mean loss: 218.87
 ---- batch: 060 ----
mean loss: 210.13
 ---- batch: 070 ----
mean loss: 213.96
 ---- batch: 080 ----
mean loss: 215.91
 ---- batch: 090 ----
mean loss: 213.35
train mean loss: 215.64
epoch train time: 0:00:02.644094
elapsed time: 0:09:35.685898
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 19:20:11.752411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.14
 ---- batch: 020 ----
mean loss: 216.06
 ---- batch: 030 ----
mean loss: 210.24
 ---- batch: 040 ----
mean loss: 214.10
 ---- batch: 050 ----
mean loss: 217.04
 ---- batch: 060 ----
mean loss: 210.01
 ---- batch: 070 ----
mean loss: 227.63
 ---- batch: 080 ----
mean loss: 214.75
 ---- batch: 090 ----
mean loss: 216.50
train mean loss: 215.37
epoch train time: 0:00:02.672592
elapsed time: 0:09:38.359015
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 19:20:14.425582
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.66
 ---- batch: 020 ----
mean loss: 218.99
 ---- batch: 030 ----
mean loss: 219.96
 ---- batch: 040 ----
mean loss: 214.72
 ---- batch: 050 ----
mean loss: 215.62
 ---- batch: 060 ----
mean loss: 216.99
 ---- batch: 070 ----
mean loss: 210.35
 ---- batch: 080 ----
mean loss: 223.18
 ---- batch: 090 ----
mean loss: 210.57
train mean loss: 215.19
epoch train time: 0:00:02.663936
elapsed time: 0:09:41.023486
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 19:20:17.090007
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.62
 ---- batch: 020 ----
mean loss: 217.13
 ---- batch: 030 ----
mean loss: 223.83
 ---- batch: 040 ----
mean loss: 206.99
 ---- batch: 050 ----
mean loss: 216.57
 ---- batch: 060 ----
mean loss: 214.71
 ---- batch: 070 ----
mean loss: 215.00
 ---- batch: 080 ----
mean loss: 226.55
 ---- batch: 090 ----
mean loss: 210.69
train mean loss: 215.33
epoch train time: 0:00:02.663033
elapsed time: 0:09:43.686962
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 19:20:19.753480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.40
 ---- batch: 020 ----
mean loss: 212.48
 ---- batch: 030 ----
mean loss: 212.01
 ---- batch: 040 ----
mean loss: 224.83
 ---- batch: 050 ----
mean loss: 212.65
 ---- batch: 060 ----
mean loss: 211.03
 ---- batch: 070 ----
mean loss: 218.53
 ---- batch: 080 ----
mean loss: 221.67
 ---- batch: 090 ----
mean loss: 213.76
train mean loss: 215.48
epoch train time: 0:00:02.669861
elapsed time: 0:09:46.357269
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 19:20:22.423787
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.32
 ---- batch: 020 ----
mean loss: 211.46
 ---- batch: 030 ----
mean loss: 214.28
 ---- batch: 040 ----
mean loss: 223.26
 ---- batch: 050 ----
mean loss: 216.39
 ---- batch: 060 ----
mean loss: 206.48
 ---- batch: 070 ----
mean loss: 219.45
 ---- batch: 080 ----
mean loss: 214.24
 ---- batch: 090 ----
mean loss: 215.53
train mean loss: 215.37
epoch train time: 0:00:02.653533
elapsed time: 0:09:49.011258
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 19:20:25.077794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.66
 ---- batch: 020 ----
mean loss: 218.26
 ---- batch: 030 ----
mean loss: 212.28
 ---- batch: 040 ----
mean loss: 225.73
 ---- batch: 050 ----
mean loss: 220.74
 ---- batch: 060 ----
mean loss: 210.17
 ---- batch: 070 ----
mean loss: 212.42
 ---- batch: 080 ----
mean loss: 216.08
 ---- batch: 090 ----
mean loss: 212.10
train mean loss: 215.33
epoch train time: 0:00:02.657353
elapsed time: 0:09:51.669111
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 19:20:27.735669
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.71
 ---- batch: 020 ----
mean loss: 219.58
 ---- batch: 030 ----
mean loss: 213.80
 ---- batch: 040 ----
mean loss: 210.82
 ---- batch: 050 ----
mean loss: 219.14
 ---- batch: 060 ----
mean loss: 218.98
 ---- batch: 070 ----
mean loss: 216.32
 ---- batch: 080 ----
mean loss: 212.64
 ---- batch: 090 ----
mean loss: 213.48
train mean loss: 215.41
epoch train time: 0:00:02.646804
elapsed time: 0:09:54.316459
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 19:20:30.382971
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.44
 ---- batch: 020 ----
mean loss: 216.66
 ---- batch: 030 ----
mean loss: 215.88
 ---- batch: 040 ----
mean loss: 212.28
 ---- batch: 050 ----
mean loss: 211.27
 ---- batch: 060 ----
mean loss: 224.31
 ---- batch: 070 ----
mean loss: 211.76
 ---- batch: 080 ----
mean loss: 217.23
 ---- batch: 090 ----
mean loss: 211.82
train mean loss: 215.19
epoch train time: 0:00:02.659608
elapsed time: 0:09:56.976532
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 19:20:33.043055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.77
 ---- batch: 020 ----
mean loss: 215.25
 ---- batch: 030 ----
mean loss: 215.93
 ---- batch: 040 ----
mean loss: 213.99
 ---- batch: 050 ----
mean loss: 212.19
 ---- batch: 060 ----
mean loss: 214.21
 ---- batch: 070 ----
mean loss: 218.11
 ---- batch: 080 ----
mean loss: 215.12
 ---- batch: 090 ----
mean loss: 214.21
train mean loss: 215.57
epoch train time: 0:00:02.663772
elapsed time: 0:09:59.640808
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 19:20:35.707323
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.24
 ---- batch: 020 ----
mean loss: 217.71
 ---- batch: 030 ----
mean loss: 214.71
 ---- batch: 040 ----
mean loss: 221.53
 ---- batch: 050 ----
mean loss: 211.61
 ---- batch: 060 ----
mean loss: 205.44
 ---- batch: 070 ----
mean loss: 216.92
 ---- batch: 080 ----
mean loss: 215.48
 ---- batch: 090 ----
mean loss: 215.21
train mean loss: 215.41
epoch train time: 0:00:02.644687
elapsed time: 0:10:02.286004
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 19:20:38.352567
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.43
 ---- batch: 020 ----
mean loss: 217.81
 ---- batch: 030 ----
mean loss: 213.12
 ---- batch: 040 ----
mean loss: 216.18
 ---- batch: 050 ----
mean loss: 221.22
 ---- batch: 060 ----
mean loss: 219.12
 ---- batch: 070 ----
mean loss: 207.46
 ---- batch: 080 ----
mean loss: 213.92
 ---- batch: 090 ----
mean loss: 212.20
train mean loss: 215.06
epoch train time: 0:00:02.673132
elapsed time: 0:10:04.959676
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 19:20:41.026220
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.84
 ---- batch: 020 ----
mean loss: 199.93
 ---- batch: 030 ----
mean loss: 210.65
 ---- batch: 040 ----
mean loss: 213.85
 ---- batch: 050 ----
mean loss: 216.36
 ---- batch: 060 ----
mean loss: 222.55
 ---- batch: 070 ----
mean loss: 222.38
 ---- batch: 080 ----
mean loss: 223.38
 ---- batch: 090 ----
mean loss: 217.82
train mean loss: 215.68
epoch train time: 0:00:02.655633
elapsed time: 0:10:07.615804
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 19:20:43.682376
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.55
 ---- batch: 020 ----
mean loss: 215.66
 ---- batch: 030 ----
mean loss: 212.36
 ---- batch: 040 ----
mean loss: 214.31
 ---- batch: 050 ----
mean loss: 213.29
 ---- batch: 060 ----
mean loss: 221.65
 ---- batch: 070 ----
mean loss: 210.72
 ---- batch: 080 ----
mean loss: 216.76
 ---- batch: 090 ----
mean loss: 210.40
train mean loss: 215.24
epoch train time: 0:00:02.672313
elapsed time: 0:10:10.288628
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 19:20:46.355147
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.46
 ---- batch: 020 ----
mean loss: 212.14
 ---- batch: 030 ----
mean loss: 216.55
 ---- batch: 040 ----
mean loss: 214.78
 ---- batch: 050 ----
mean loss: 213.38
 ---- batch: 060 ----
mean loss: 215.77
 ---- batch: 070 ----
mean loss: 210.89
 ---- batch: 080 ----
mean loss: 219.45
 ---- batch: 090 ----
mean loss: 215.03
train mean loss: 215.01
epoch train time: 0:00:02.663349
elapsed time: 0:10:12.952478
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 19:20:49.019044
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.49
 ---- batch: 020 ----
mean loss: 215.75
 ---- batch: 030 ----
mean loss: 219.10
 ---- batch: 040 ----
mean loss: 211.04
 ---- batch: 050 ----
mean loss: 217.82
 ---- batch: 060 ----
mean loss: 211.68
 ---- batch: 070 ----
mean loss: 208.21
 ---- batch: 080 ----
mean loss: 224.79
 ---- batch: 090 ----
mean loss: 214.41
train mean loss: 215.35
epoch train time: 0:00:02.659158
elapsed time: 0:10:15.612173
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 19:20:51.678692
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.89
 ---- batch: 020 ----
mean loss: 219.44
 ---- batch: 030 ----
mean loss: 218.21
 ---- batch: 040 ----
mean loss: 216.73
 ---- batch: 050 ----
mean loss: 209.84
 ---- batch: 060 ----
mean loss: 214.11
 ---- batch: 070 ----
mean loss: 211.69
 ---- batch: 080 ----
mean loss: 219.25
 ---- batch: 090 ----
mean loss: 216.52
train mean loss: 215.25
epoch train time: 0:00:02.659012
elapsed time: 0:10:18.271617
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 19:20:54.338128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.98
 ---- batch: 020 ----
mean loss: 212.90
 ---- batch: 030 ----
mean loss: 219.14
 ---- batch: 040 ----
mean loss: 217.92
 ---- batch: 050 ----
mean loss: 222.62
 ---- batch: 060 ----
mean loss: 207.97
 ---- batch: 070 ----
mean loss: 211.08
 ---- batch: 080 ----
mean loss: 221.43
 ---- batch: 090 ----
mean loss: 215.96
train mean loss: 214.98
epoch train time: 0:00:02.697838
elapsed time: 0:10:20.969993
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 19:20:57.036520
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.86
 ---- batch: 020 ----
mean loss: 210.45
 ---- batch: 030 ----
mean loss: 214.44
 ---- batch: 040 ----
mean loss: 213.21
 ---- batch: 050 ----
mean loss: 219.05
 ---- batch: 060 ----
mean loss: 213.23
 ---- batch: 070 ----
mean loss: 215.39
 ---- batch: 080 ----
mean loss: 222.11
 ---- batch: 090 ----
mean loss: 214.43
train mean loss: 215.42
epoch train time: 0:00:02.652787
elapsed time: 0:10:23.623244
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 19:20:59.689779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.02
 ---- batch: 020 ----
mean loss: 211.74
 ---- batch: 030 ----
mean loss: 213.69
 ---- batch: 040 ----
mean loss: 216.03
 ---- batch: 050 ----
mean loss: 217.04
 ---- batch: 060 ----
mean loss: 222.77
 ---- batch: 070 ----
mean loss: 215.25
 ---- batch: 080 ----
mean loss: 223.32
 ---- batch: 090 ----
mean loss: 204.60
train mean loss: 215.11
epoch train time: 0:00:02.672486
elapsed time: 0:10:26.296197
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 19:21:02.362743
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.93
 ---- batch: 020 ----
mean loss: 214.05
 ---- batch: 030 ----
mean loss: 213.71
 ---- batch: 040 ----
mean loss: 211.39
 ---- batch: 050 ----
mean loss: 210.62
 ---- batch: 060 ----
mean loss: 215.47
 ---- batch: 070 ----
mean loss: 215.13
 ---- batch: 080 ----
mean loss: 226.17
 ---- batch: 090 ----
mean loss: 219.51
train mean loss: 215.19
epoch train time: 0:00:02.678280
elapsed time: 0:10:28.974965
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 19:21:05.041487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.46
 ---- batch: 020 ----
mean loss: 214.23
 ---- batch: 030 ----
mean loss: 215.75
 ---- batch: 040 ----
mean loss: 214.20
 ---- batch: 050 ----
mean loss: 214.78
 ---- batch: 060 ----
mean loss: 213.80
 ---- batch: 070 ----
mean loss: 216.52
 ---- batch: 080 ----
mean loss: 208.69
 ---- batch: 090 ----
mean loss: 223.13
train mean loss: 215.06
epoch train time: 0:00:02.657181
elapsed time: 0:10:31.632671
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 19:21:07.699222
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.42
 ---- batch: 020 ----
mean loss: 208.40
 ---- batch: 030 ----
mean loss: 214.27
 ---- batch: 040 ----
mean loss: 215.19
 ---- batch: 050 ----
mean loss: 214.92
 ---- batch: 060 ----
mean loss: 211.82
 ---- batch: 070 ----
mean loss: 215.65
 ---- batch: 080 ----
mean loss: 227.76
 ---- batch: 090 ----
mean loss: 216.93
train mean loss: 215.10
epoch train time: 0:00:02.673376
elapsed time: 0:10:34.306513
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 19:21:10.373036
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.69
 ---- batch: 020 ----
mean loss: 221.63
 ---- batch: 030 ----
mean loss: 210.42
 ---- batch: 040 ----
mean loss: 216.67
 ---- batch: 050 ----
mean loss: 217.36
 ---- batch: 060 ----
mean loss: 211.39
 ---- batch: 070 ----
mean loss: 215.58
 ---- batch: 080 ----
mean loss: 210.72
 ---- batch: 090 ----
mean loss: 215.58
train mean loss: 215.04
epoch train time: 0:00:02.660757
elapsed time: 0:10:36.967737
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 19:21:13.034248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.34
 ---- batch: 020 ----
mean loss: 221.92
 ---- batch: 030 ----
mean loss: 217.45
 ---- batch: 040 ----
mean loss: 209.04
 ---- batch: 050 ----
mean loss: 209.91
 ---- batch: 060 ----
mean loss: 216.18
 ---- batch: 070 ----
mean loss: 210.27
 ---- batch: 080 ----
mean loss: 215.73
 ---- batch: 090 ----
mean loss: 220.82
train mean loss: 215.05
epoch train time: 0:00:02.644388
elapsed time: 0:10:39.612571
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 19:21:15.679094
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.46
 ---- batch: 020 ----
mean loss: 220.50
 ---- batch: 030 ----
mean loss: 213.27
 ---- batch: 040 ----
mean loss: 218.86
 ---- batch: 050 ----
mean loss: 221.95
 ---- batch: 060 ----
mean loss: 212.08
 ---- batch: 070 ----
mean loss: 215.59
 ---- batch: 080 ----
mean loss: 215.19
 ---- batch: 090 ----
mean loss: 211.51
train mean loss: 215.04
epoch train time: 0:00:02.671413
elapsed time: 0:10:42.284431
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 19:21:18.350950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.67
 ---- batch: 020 ----
mean loss: 208.62
 ---- batch: 030 ----
mean loss: 220.19
 ---- batch: 040 ----
mean loss: 215.92
 ---- batch: 050 ----
mean loss: 207.93
 ---- batch: 060 ----
mean loss: 214.17
 ---- batch: 070 ----
mean loss: 219.15
 ---- batch: 080 ----
mean loss: 210.21
 ---- batch: 090 ----
mean loss: 215.07
train mean loss: 214.87
epoch train time: 0:00:02.646947
elapsed time: 0:10:44.931836
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 19:21:20.998341
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.94
 ---- batch: 020 ----
mean loss: 215.31
 ---- batch: 030 ----
mean loss: 213.16
 ---- batch: 040 ----
mean loss: 221.37
 ---- batch: 050 ----
mean loss: 220.40
 ---- batch: 060 ----
mean loss: 208.62
 ---- batch: 070 ----
mean loss: 218.56
 ---- batch: 080 ----
mean loss: 206.97
 ---- batch: 090 ----
mean loss: 223.94
train mean loss: 214.81
epoch train time: 0:00:02.697320
elapsed time: 0:10:47.629681
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 19:21:23.696200
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.17
 ---- batch: 020 ----
mean loss: 218.79
 ---- batch: 030 ----
mean loss: 223.48
 ---- batch: 040 ----
mean loss: 210.81
 ---- batch: 050 ----
mean loss: 209.71
 ---- batch: 060 ----
mean loss: 207.75
 ---- batch: 070 ----
mean loss: 216.43
 ---- batch: 080 ----
mean loss: 219.43
 ---- batch: 090 ----
mean loss: 213.52
train mean loss: 214.97
epoch train time: 0:00:02.712521
elapsed time: 0:10:50.342696
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 19:21:26.409233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.96
 ---- batch: 020 ----
mean loss: 212.70
 ---- batch: 030 ----
mean loss: 215.49
 ---- batch: 040 ----
mean loss: 211.99
 ---- batch: 050 ----
mean loss: 208.63
 ---- batch: 060 ----
mean loss: 206.82
 ---- batch: 070 ----
mean loss: 212.06
 ---- batch: 080 ----
mean loss: 216.66
 ---- batch: 090 ----
mean loss: 222.46
train mean loss: 214.72
epoch train time: 0:00:02.671300
elapsed time: 0:10:53.014452
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 19:21:29.080992
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.71
 ---- batch: 020 ----
mean loss: 216.18
 ---- batch: 030 ----
mean loss: 219.11
 ---- batch: 040 ----
mean loss: 209.67
 ---- batch: 050 ----
mean loss: 208.65
 ---- batch: 060 ----
mean loss: 218.22
 ---- batch: 070 ----
mean loss: 215.05
 ---- batch: 080 ----
mean loss: 211.59
 ---- batch: 090 ----
mean loss: 216.15
train mean loss: 214.72
epoch train time: 0:00:02.702202
elapsed time: 0:10:55.717159
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 19:21:31.783671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.78
 ---- batch: 020 ----
mean loss: 213.19
 ---- batch: 030 ----
mean loss: 210.98
 ---- batch: 040 ----
mean loss: 209.09
 ---- batch: 050 ----
mean loss: 217.52
 ---- batch: 060 ----
mean loss: 215.38
 ---- batch: 070 ----
mean loss: 209.03
 ---- batch: 080 ----
mean loss: 218.16
 ---- batch: 090 ----
mean loss: 215.07
train mean loss: 214.98
epoch train time: 0:00:02.680992
elapsed time: 0:10:58.398696
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 19:21:34.465084
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.16
 ---- batch: 020 ----
mean loss: 215.19
 ---- batch: 030 ----
mean loss: 213.38
 ---- batch: 040 ----
mean loss: 222.00
 ---- batch: 050 ----
mean loss: 215.07
 ---- batch: 060 ----
mean loss: 215.38
 ---- batch: 070 ----
mean loss: 221.83
 ---- batch: 080 ----
mean loss: 210.81
 ---- batch: 090 ----
mean loss: 210.83
train mean loss: 214.97
epoch train time: 0:00:02.710757
elapsed time: 0:11:01.109840
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 19:21:37.176361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.94
 ---- batch: 020 ----
mean loss: 211.46
 ---- batch: 030 ----
mean loss: 212.90
 ---- batch: 040 ----
mean loss: 211.04
 ---- batch: 050 ----
mean loss: 222.08
 ---- batch: 060 ----
mean loss: 225.15
 ---- batch: 070 ----
mean loss: 214.46
 ---- batch: 080 ----
mean loss: 211.84
 ---- batch: 090 ----
mean loss: 211.42
train mean loss: 214.86
epoch train time: 0:00:02.683486
elapsed time: 0:11:03.793819
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 19:21:39.860328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.44
 ---- batch: 020 ----
mean loss: 217.68
 ---- batch: 030 ----
mean loss: 215.63
 ---- batch: 040 ----
mean loss: 220.75
 ---- batch: 050 ----
mean loss: 213.53
 ---- batch: 060 ----
mean loss: 206.84
 ---- batch: 070 ----
mean loss: 220.88
 ---- batch: 080 ----
mean loss: 215.24
 ---- batch: 090 ----
mean loss: 213.20
train mean loss: 214.60
epoch train time: 0:00:02.663479
elapsed time: 0:11:06.457798
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 19:21:42.524317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.85
 ---- batch: 020 ----
mean loss: 209.00
 ---- batch: 030 ----
mean loss: 219.45
 ---- batch: 040 ----
mean loss: 215.39
 ---- batch: 050 ----
mean loss: 214.25
 ---- batch: 060 ----
mean loss: 207.58
 ---- batch: 070 ----
mean loss: 215.49
 ---- batch: 080 ----
mean loss: 218.57
 ---- batch: 090 ----
mean loss: 214.16
train mean loss: 214.92
epoch train time: 0:00:02.677088
elapsed time: 0:11:09.135367
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 19:21:45.201876
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.93
 ---- batch: 020 ----
mean loss: 216.12
 ---- batch: 030 ----
mean loss: 217.48
 ---- batch: 040 ----
mean loss: 210.30
 ---- batch: 050 ----
mean loss: 214.45
 ---- batch: 060 ----
mean loss: 209.40
 ---- batch: 070 ----
mean loss: 214.54
 ---- batch: 080 ----
mean loss: 213.72
 ---- batch: 090 ----
mean loss: 217.39
train mean loss: 214.95
epoch train time: 0:00:02.673381
elapsed time: 0:11:11.809234
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 19:21:47.875751
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.04
 ---- batch: 020 ----
mean loss: 217.94
 ---- batch: 030 ----
mean loss: 211.30
 ---- batch: 040 ----
mean loss: 207.14
 ---- batch: 050 ----
mean loss: 214.73
 ---- batch: 060 ----
mean loss: 222.26
 ---- batch: 070 ----
mean loss: 209.18
 ---- batch: 080 ----
mean loss: 215.26
 ---- batch: 090 ----
mean loss: 219.24
train mean loss: 214.94
epoch train time: 0:00:02.696456
elapsed time: 0:11:14.506158
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 19:21:50.572688
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.46
 ---- batch: 020 ----
mean loss: 212.22
 ---- batch: 030 ----
mean loss: 215.80
 ---- batch: 040 ----
mean loss: 212.12
 ---- batch: 050 ----
mean loss: 212.89
 ---- batch: 060 ----
mean loss: 211.19
 ---- batch: 070 ----
mean loss: 215.05
 ---- batch: 080 ----
mean loss: 218.62
 ---- batch: 090 ----
mean loss: 210.53
train mean loss: 214.98
epoch train time: 0:00:02.729225
elapsed time: 0:11:17.235873
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 19:21:53.302381
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.85
 ---- batch: 020 ----
mean loss: 213.67
 ---- batch: 030 ----
mean loss: 217.89
 ---- batch: 040 ----
mean loss: 207.51
 ---- batch: 050 ----
mean loss: 214.97
 ---- batch: 060 ----
mean loss: 209.06
 ---- batch: 070 ----
mean loss: 218.62
 ---- batch: 080 ----
mean loss: 210.60
 ---- batch: 090 ----
mean loss: 220.76
train mean loss: 214.88
epoch train time: 0:00:02.683809
elapsed time: 0:11:19.920191
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 19:21:55.986698
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.36
 ---- batch: 020 ----
mean loss: 209.72
 ---- batch: 030 ----
mean loss: 219.61
 ---- batch: 040 ----
mean loss: 210.36
 ---- batch: 050 ----
mean loss: 213.13
 ---- batch: 060 ----
mean loss: 211.32
 ---- batch: 070 ----
mean loss: 213.45
 ---- batch: 080 ----
mean loss: 210.75
 ---- batch: 090 ----
mean loss: 222.94
train mean loss: 214.77
epoch train time: 0:00:02.684867
elapsed time: 0:11:22.605543
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 19:21:58.672061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.64
 ---- batch: 020 ----
mean loss: 216.64
 ---- batch: 030 ----
mean loss: 212.55
 ---- batch: 040 ----
mean loss: 209.50
 ---- batch: 050 ----
mean loss: 213.22
 ---- batch: 060 ----
mean loss: 220.99
 ---- batch: 070 ----
mean loss: 219.14
 ---- batch: 080 ----
mean loss: 204.84
 ---- batch: 090 ----
mean loss: 222.09
train mean loss: 214.62
epoch train time: 0:00:02.694921
elapsed time: 0:11:25.300915
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 19:22:01.367421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.05
 ---- batch: 020 ----
mean loss: 224.82
 ---- batch: 030 ----
mean loss: 207.43
 ---- batch: 040 ----
mean loss: 214.54
 ---- batch: 050 ----
mean loss: 217.87
 ---- batch: 060 ----
mean loss: 209.22
 ---- batch: 070 ----
mean loss: 212.89
 ---- batch: 080 ----
mean loss: 209.84
 ---- batch: 090 ----
mean loss: 219.32
train mean loss: 214.54
epoch train time: 0:00:02.687735
elapsed time: 0:11:27.989184
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 19:22:04.055700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.77
 ---- batch: 020 ----
mean loss: 213.52
 ---- batch: 030 ----
mean loss: 212.15
 ---- batch: 040 ----
mean loss: 217.16
 ---- batch: 050 ----
mean loss: 217.17
 ---- batch: 060 ----
mean loss: 214.20
 ---- batch: 070 ----
mean loss: 208.82
 ---- batch: 080 ----
mean loss: 219.96
 ---- batch: 090 ----
mean loss: 218.50
train mean loss: 214.56
epoch train time: 0:00:02.658984
elapsed time: 0:11:30.648694
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 19:22:06.715220
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.98
 ---- batch: 020 ----
mean loss: 217.45
 ---- batch: 030 ----
mean loss: 210.09
 ---- batch: 040 ----
mean loss: 220.05
 ---- batch: 050 ----
mean loss: 215.23
 ---- batch: 060 ----
mean loss: 205.57
 ---- batch: 070 ----
mean loss: 213.42
 ---- batch: 080 ----
mean loss: 209.25
 ---- batch: 090 ----
mean loss: 219.85
train mean loss: 214.70
epoch train time: 0:00:02.663029
elapsed time: 0:11:33.312327
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 19:22:09.378869
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.81
 ---- batch: 020 ----
mean loss: 211.85
 ---- batch: 030 ----
mean loss: 214.63
 ---- batch: 040 ----
mean loss: 222.39
 ---- batch: 050 ----
mean loss: 215.18
 ---- batch: 060 ----
mean loss: 221.60
 ---- batch: 070 ----
mean loss: 206.54
 ---- batch: 080 ----
mean loss: 206.70
 ---- batch: 090 ----
mean loss: 218.74
train mean loss: 214.54
epoch train time: 0:00:02.683489
elapsed time: 0:11:35.996351
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 19:22:12.062920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.45
 ---- batch: 020 ----
mean loss: 216.45
 ---- batch: 030 ----
mean loss: 212.81
 ---- batch: 040 ----
mean loss: 220.44
 ---- batch: 050 ----
mean loss: 210.83
 ---- batch: 060 ----
mean loss: 215.73
 ---- batch: 070 ----
mean loss: 223.38
 ---- batch: 080 ----
mean loss: 207.80
 ---- batch: 090 ----
mean loss: 211.24
train mean loss: 214.59
epoch train time: 0:00:02.694304
elapsed time: 0:11:38.691191
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 19:22:14.757750
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.42
 ---- batch: 020 ----
mean loss: 218.24
 ---- batch: 030 ----
mean loss: 211.06
 ---- batch: 040 ----
mean loss: 207.65
 ---- batch: 050 ----
mean loss: 223.85
 ---- batch: 060 ----
mean loss: 214.07
 ---- batch: 070 ----
mean loss: 210.87
 ---- batch: 080 ----
mean loss: 219.81
 ---- batch: 090 ----
mean loss: 220.65
train mean loss: 214.34
epoch train time: 0:00:02.683717
elapsed time: 0:11:41.379552
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_9/checkpoint.pth.tar
**** end time: 2019-09-25 19:22:17.445910 ****
