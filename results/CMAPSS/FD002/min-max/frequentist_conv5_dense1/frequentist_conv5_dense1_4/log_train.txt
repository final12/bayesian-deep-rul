Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_4', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 21877
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-25 21:22:18.814833 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 21, 24]             100
              Tanh-2           [-1, 10, 21, 24]               0
            Conv2d-3           [-1, 10, 20, 24]           1,000
              Tanh-4           [-1, 10, 20, 24]               0
            Conv2d-5           [-1, 10, 21, 24]           1,000
              Tanh-6           [-1, 10, 21, 24]               0
            Conv2d-7           [-1, 10, 20, 24]           1,000
              Tanh-8           [-1, 10, 20, 24]               0
            Conv2d-9            [-1, 1, 20, 24]              30
             Tanh-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
          Dropout-12                  [-1, 480]               0
           Linear-13                  [-1, 100]          48,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 51,230
Trainable params: 51,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 21:22:18.823883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3805.44
 ---- batch: 020 ----
mean loss: 1720.84
 ---- batch: 030 ----
mean loss: 1221.56
 ---- batch: 040 ----
mean loss: 1091.96
 ---- batch: 050 ----
mean loss: 1015.40
 ---- batch: 060 ----
mean loss: 992.32
 ---- batch: 070 ----
mean loss: 952.11
 ---- batch: 080 ----
mean loss: 931.48
 ---- batch: 090 ----
mean loss: 919.23
train mean loss: 1374.52
epoch train time: 0:00:35.452159
elapsed time: 0:00:35.463852
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 21:22:54.278725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.95
 ---- batch: 020 ----
mean loss: 861.10
 ---- batch: 030 ----
mean loss: 850.97
 ---- batch: 040 ----
mean loss: 845.26
 ---- batch: 050 ----
mean loss: 828.50
 ---- batch: 060 ----
mean loss: 807.19
 ---- batch: 070 ----
mean loss: 788.77
 ---- batch: 080 ----
mean loss: 787.32
 ---- batch: 090 ----
mean loss: 786.96
train mean loss: 821.01
epoch train time: 0:00:02.434692
elapsed time: 0:00:37.898712
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 21:22:56.713623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 762.70
 ---- batch: 020 ----
mean loss: 735.52
 ---- batch: 030 ----
mean loss: 728.94
 ---- batch: 040 ----
mean loss: 739.36
 ---- batch: 050 ----
mean loss: 718.15
 ---- batch: 060 ----
mean loss: 694.54
 ---- batch: 070 ----
mean loss: 687.50
 ---- batch: 080 ----
mean loss: 679.09
 ---- batch: 090 ----
mean loss: 664.63
train mean loss: 709.61
epoch train time: 0:00:02.338294
elapsed time: 0:00:40.237222
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 21:22:59.052118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.01
 ---- batch: 020 ----
mean loss: 654.04
 ---- batch: 030 ----
mean loss: 649.01
 ---- batch: 040 ----
mean loss: 625.86
 ---- batch: 050 ----
mean loss: 619.55
 ---- batch: 060 ----
mean loss: 627.20
 ---- batch: 070 ----
mean loss: 623.52
 ---- batch: 080 ----
mean loss: 602.42
 ---- batch: 090 ----
mean loss: 602.37
train mean loss: 627.52
epoch train time: 0:00:02.347684
elapsed time: 0:00:42.585126
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 21:23:01.400042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 600.68
 ---- batch: 020 ----
mean loss: 597.10
 ---- batch: 030 ----
mean loss: 580.18
 ---- batch: 040 ----
mean loss: 577.07
 ---- batch: 050 ----
mean loss: 575.90
 ---- batch: 060 ----
mean loss: 569.05
 ---- batch: 070 ----
mean loss: 580.20
 ---- batch: 080 ----
mean loss: 567.15
 ---- batch: 090 ----
mean loss: 571.61
train mean loss: 578.16
epoch train time: 0:00:02.353759
elapsed time: 0:00:44.939110
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 21:23:03.754012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 577.01
 ---- batch: 020 ----
mean loss: 561.50
 ---- batch: 030 ----
mean loss: 539.05
 ---- batch: 040 ----
mean loss: 561.91
 ---- batch: 050 ----
mean loss: 556.97
 ---- batch: 060 ----
mean loss: 524.09
 ---- batch: 070 ----
mean loss: 536.69
 ---- batch: 080 ----
mean loss: 534.93
 ---- batch: 090 ----
mean loss: 534.58
train mean loss: 545.60
epoch train time: 0:00:02.349205
elapsed time: 0:00:47.288520
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 21:23:06.103457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 542.54
 ---- batch: 020 ----
mean loss: 535.85
 ---- batch: 030 ----
mean loss: 521.11
 ---- batch: 040 ----
mean loss: 532.48
 ---- batch: 050 ----
mean loss: 518.40
 ---- batch: 060 ----
mean loss: 549.40
 ---- batch: 070 ----
mean loss: 528.42
 ---- batch: 080 ----
mean loss: 521.40
 ---- batch: 090 ----
mean loss: 520.15
train mean loss: 529.32
epoch train time: 0:00:02.350218
elapsed time: 0:00:49.638957
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 21:23:08.453872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.49
 ---- batch: 020 ----
mean loss: 515.33
 ---- batch: 030 ----
mean loss: 501.25
 ---- batch: 040 ----
mean loss: 521.13
 ---- batch: 050 ----
mean loss: 518.39
 ---- batch: 060 ----
mean loss: 494.08
 ---- batch: 070 ----
mean loss: 504.21
 ---- batch: 080 ----
mean loss: 490.19
 ---- batch: 090 ----
mean loss: 486.28
train mean loss: 504.07
epoch train time: 0:00:02.345727
elapsed time: 0:00:51.984883
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 21:23:10.799781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.19
 ---- batch: 020 ----
mean loss: 489.43
 ---- batch: 030 ----
mean loss: 505.48
 ---- batch: 040 ----
mean loss: 483.78
 ---- batch: 050 ----
mean loss: 469.65
 ---- batch: 060 ----
mean loss: 486.17
 ---- batch: 070 ----
mean loss: 464.06
 ---- batch: 080 ----
mean loss: 488.45
 ---- batch: 090 ----
mean loss: 474.24
train mean loss: 484.92
epoch train time: 0:00:02.345682
elapsed time: 0:00:54.330751
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 21:23:13.145684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.74
 ---- batch: 020 ----
mean loss: 482.44
 ---- batch: 030 ----
mean loss: 454.52
 ---- batch: 040 ----
mean loss: 460.52
 ---- batch: 050 ----
mean loss: 487.48
 ---- batch: 060 ----
mean loss: 468.27
 ---- batch: 070 ----
mean loss: 456.54
 ---- batch: 080 ----
mean loss: 448.40
 ---- batch: 090 ----
mean loss: 466.83
train mean loss: 467.85
epoch train time: 0:00:02.338515
elapsed time: 0:00:56.669478
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 21:23:15.484371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.27
 ---- batch: 020 ----
mean loss: 472.53
 ---- batch: 030 ----
mean loss: 454.58
 ---- batch: 040 ----
mean loss: 451.55
 ---- batch: 050 ----
mean loss: 471.57
 ---- batch: 060 ----
mean loss: 468.29
 ---- batch: 070 ----
mean loss: 452.48
 ---- batch: 080 ----
mean loss: 457.06
 ---- batch: 090 ----
mean loss: 453.27
train mean loss: 461.38
epoch train time: 0:00:02.342889
elapsed time: 0:00:59.012621
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 21:23:17.827527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.25
 ---- batch: 020 ----
mean loss: 474.55
 ---- batch: 030 ----
mean loss: 488.89
 ---- batch: 040 ----
mean loss: 473.34
 ---- batch: 050 ----
mean loss: 471.53
 ---- batch: 060 ----
mean loss: 455.88
 ---- batch: 070 ----
mean loss: 456.41
 ---- batch: 080 ----
mean loss: 455.77
 ---- batch: 090 ----
mean loss: 439.92
train mean loss: 464.12
epoch train time: 0:00:02.338674
elapsed time: 0:01:01.351501
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 21:23:20.166403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.65
 ---- batch: 020 ----
mean loss: 443.18
 ---- batch: 030 ----
mean loss: 448.09
 ---- batch: 040 ----
mean loss: 446.00
 ---- batch: 050 ----
mean loss: 448.59
 ---- batch: 060 ----
mean loss: 455.50
 ---- batch: 070 ----
mean loss: 434.70
 ---- batch: 080 ----
mean loss: 450.33
 ---- batch: 090 ----
mean loss: 445.90
train mean loss: 444.07
epoch train time: 0:00:02.332225
elapsed time: 0:01:03.683911
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 21:23:22.498802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.80
 ---- batch: 020 ----
mean loss: 448.11
 ---- batch: 030 ----
mean loss: 438.38
 ---- batch: 040 ----
mean loss: 431.12
 ---- batch: 050 ----
mean loss: 432.42
 ---- batch: 060 ----
mean loss: 419.82
 ---- batch: 070 ----
mean loss: 429.28
 ---- batch: 080 ----
mean loss: 438.61
 ---- batch: 090 ----
mean loss: 433.59
train mean loss: 436.32
epoch train time: 0:00:02.337787
elapsed time: 0:01:06.021896
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 21:23:24.836793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.57
 ---- batch: 020 ----
mean loss: 432.69
 ---- batch: 030 ----
mean loss: 431.01
 ---- batch: 040 ----
mean loss: 449.60
 ---- batch: 050 ----
mean loss: 435.93
 ---- batch: 060 ----
mean loss: 441.08
 ---- batch: 070 ----
mean loss: 442.42
 ---- batch: 080 ----
mean loss: 429.00
 ---- batch: 090 ----
mean loss: 428.38
train mean loss: 434.68
epoch train time: 0:00:02.334292
elapsed time: 0:01:08.356363
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 21:23:27.171277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.25
 ---- batch: 020 ----
mean loss: 446.86
 ---- batch: 030 ----
mean loss: 438.43
 ---- batch: 040 ----
mean loss: 427.12
 ---- batch: 050 ----
mean loss: 430.43
 ---- batch: 060 ----
mean loss: 423.24
 ---- batch: 070 ----
mean loss: 419.68
 ---- batch: 080 ----
mean loss: 406.11
 ---- batch: 090 ----
mean loss: 427.36
train mean loss: 430.21
epoch train time: 0:00:02.336250
elapsed time: 0:01:10.692823
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 21:23:29.507720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.44
 ---- batch: 020 ----
mean loss: 424.52
 ---- batch: 030 ----
mean loss: 423.15
 ---- batch: 040 ----
mean loss: 411.66
 ---- batch: 050 ----
mean loss: 415.83
 ---- batch: 060 ----
mean loss: 428.82
 ---- batch: 070 ----
mean loss: 420.78
 ---- batch: 080 ----
mean loss: 425.37
 ---- batch: 090 ----
mean loss: 413.00
train mean loss: 421.55
epoch train time: 0:00:02.334918
elapsed time: 0:01:13.027922
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 21:23:31.842817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.45
 ---- batch: 020 ----
mean loss: 414.18
 ---- batch: 030 ----
mean loss: 418.01
 ---- batch: 040 ----
mean loss: 418.82
 ---- batch: 050 ----
mean loss: 414.19
 ---- batch: 060 ----
mean loss: 415.46
 ---- batch: 070 ----
mean loss: 418.01
 ---- batch: 080 ----
mean loss: 424.48
 ---- batch: 090 ----
mean loss: 431.71
train mean loss: 420.28
epoch train time: 0:00:02.325899
elapsed time: 0:01:15.354031
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 21:23:34.168941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.75
 ---- batch: 020 ----
mean loss: 406.25
 ---- batch: 030 ----
mean loss: 409.33
 ---- batch: 040 ----
mean loss: 423.22
 ---- batch: 050 ----
mean loss: 399.83
 ---- batch: 060 ----
mean loss: 423.79
 ---- batch: 070 ----
mean loss: 422.48
 ---- batch: 080 ----
mean loss: 425.53
 ---- batch: 090 ----
mean loss: 456.74
train mean loss: 422.72
epoch train time: 0:00:02.334904
elapsed time: 0:01:17.689141
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 21:23:36.504042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.25
 ---- batch: 020 ----
mean loss: 416.96
 ---- batch: 030 ----
mean loss: 414.69
 ---- batch: 040 ----
mean loss: 425.72
 ---- batch: 050 ----
mean loss: 408.28
 ---- batch: 060 ----
mean loss: 419.67
 ---- batch: 070 ----
mean loss: 403.83
 ---- batch: 080 ----
mean loss: 419.00
 ---- batch: 090 ----
mean loss: 416.78
train mean loss: 416.68
epoch train time: 0:00:02.333343
elapsed time: 0:01:20.022698
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 21:23:38.837621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.08
 ---- batch: 020 ----
mean loss: 413.99
 ---- batch: 030 ----
mean loss: 419.06
 ---- batch: 040 ----
mean loss: 425.47
 ---- batch: 050 ----
mean loss: 410.05
 ---- batch: 060 ----
mean loss: 440.95
 ---- batch: 070 ----
mean loss: 412.88
 ---- batch: 080 ----
mean loss: 410.10
 ---- batch: 090 ----
mean loss: 424.40
train mean loss: 419.29
epoch train time: 0:00:02.328729
elapsed time: 0:01:22.351654
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 21:23:41.166544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.59
 ---- batch: 020 ----
mean loss: 419.46
 ---- batch: 030 ----
mean loss: 409.78
 ---- batch: 040 ----
mean loss: 410.27
 ---- batch: 050 ----
mean loss: 410.37
 ---- batch: 060 ----
mean loss: 418.26
 ---- batch: 070 ----
mean loss: 421.76
 ---- batch: 080 ----
mean loss: 417.71
 ---- batch: 090 ----
mean loss: 412.18
train mean loss: 415.80
epoch train time: 0:00:02.336335
elapsed time: 0:01:24.688168
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 21:23:43.503084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.80
 ---- batch: 020 ----
mean loss: 414.92
 ---- batch: 030 ----
mean loss: 396.63
 ---- batch: 040 ----
mean loss: 422.02
 ---- batch: 050 ----
mean loss: 396.87
 ---- batch: 060 ----
mean loss: 413.91
 ---- batch: 070 ----
mean loss: 417.65
 ---- batch: 080 ----
mean loss: 422.84
 ---- batch: 090 ----
mean loss: 410.34
train mean loss: 411.55
epoch train time: 0:00:02.332238
elapsed time: 0:01:27.020620
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 21:23:45.835518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.82
 ---- batch: 020 ----
mean loss: 407.87
 ---- batch: 030 ----
mean loss: 412.94
 ---- batch: 040 ----
mean loss: 416.18
 ---- batch: 050 ----
mean loss: 393.14
 ---- batch: 060 ----
mean loss: 415.17
 ---- batch: 070 ----
mean loss: 406.87
 ---- batch: 080 ----
mean loss: 402.21
 ---- batch: 090 ----
mean loss: 430.84
train mean loss: 410.80
epoch train time: 0:00:02.323894
elapsed time: 0:01:29.344692
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 21:23:48.159622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.69
 ---- batch: 020 ----
mean loss: 421.92
 ---- batch: 030 ----
mean loss: 410.28
 ---- batch: 040 ----
mean loss: 406.27
 ---- batch: 050 ----
mean loss: 415.12
 ---- batch: 060 ----
mean loss: 413.40
 ---- batch: 070 ----
mean loss: 411.81
 ---- batch: 080 ----
mean loss: 397.80
 ---- batch: 090 ----
mean loss: 413.25
train mean loss: 412.73
epoch train time: 0:00:02.316852
elapsed time: 0:01:31.661765
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 21:23:50.476661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.83
 ---- batch: 020 ----
mean loss: 399.77
 ---- batch: 030 ----
mean loss: 407.67
 ---- batch: 040 ----
mean loss: 410.96
 ---- batch: 050 ----
mean loss: 404.93
 ---- batch: 060 ----
mean loss: 409.70
 ---- batch: 070 ----
mean loss: 395.01
 ---- batch: 080 ----
mean loss: 412.98
 ---- batch: 090 ----
mean loss: 391.24
train mean loss: 406.22
epoch train time: 0:00:02.325102
elapsed time: 0:01:33.987058
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 21:23:52.801960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.24
 ---- batch: 020 ----
mean loss: 406.55
 ---- batch: 030 ----
mean loss: 416.21
 ---- batch: 040 ----
mean loss: 414.36
 ---- batch: 050 ----
mean loss: 400.21
 ---- batch: 060 ----
mean loss: 406.12
 ---- batch: 070 ----
mean loss: 407.97
 ---- batch: 080 ----
mean loss: 404.51
 ---- batch: 090 ----
mean loss: 399.81
train mean loss: 407.98
epoch train time: 0:00:02.324460
elapsed time: 0:01:36.311708
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 21:23:55.126605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.17
 ---- batch: 020 ----
mean loss: 404.44
 ---- batch: 030 ----
mean loss: 407.87
 ---- batch: 040 ----
mean loss: 413.64
 ---- batch: 050 ----
mean loss: 418.50
 ---- batch: 060 ----
mean loss: 404.42
 ---- batch: 070 ----
mean loss: 395.06
 ---- batch: 080 ----
mean loss: 407.68
 ---- batch: 090 ----
mean loss: 397.94
train mean loss: 409.57
epoch train time: 0:00:02.326818
elapsed time: 0:01:38.638721
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 21:23:57.453635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.82
 ---- batch: 020 ----
mean loss: 406.26
 ---- batch: 030 ----
mean loss: 398.84
 ---- batch: 040 ----
mean loss: 414.21
 ---- batch: 050 ----
mean loss: 405.74
 ---- batch: 060 ----
mean loss: 407.13
 ---- batch: 070 ----
mean loss: 415.87
 ---- batch: 080 ----
mean loss: 409.40
 ---- batch: 090 ----
mean loss: 405.23
train mean loss: 407.36
epoch train time: 0:00:02.324806
elapsed time: 0:01:40.963729
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 21:23:59.778625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.91
 ---- batch: 020 ----
mean loss: 411.81
 ---- batch: 030 ----
mean loss: 425.03
 ---- batch: 040 ----
mean loss: 405.64
 ---- batch: 050 ----
mean loss: 428.17
 ---- batch: 060 ----
mean loss: 417.77
 ---- batch: 070 ----
mean loss: 416.57
 ---- batch: 080 ----
mean loss: 417.52
 ---- batch: 090 ----
mean loss: 402.87
train mean loss: 415.02
epoch train time: 0:00:02.327335
elapsed time: 0:01:43.291301
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 21:24:02.106225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.50
 ---- batch: 020 ----
mean loss: 401.44
 ---- batch: 030 ----
mean loss: 398.85
 ---- batch: 040 ----
mean loss: 410.95
 ---- batch: 050 ----
mean loss: 409.92
 ---- batch: 060 ----
mean loss: 407.09
 ---- batch: 070 ----
mean loss: 402.09
 ---- batch: 080 ----
mean loss: 411.05
 ---- batch: 090 ----
mean loss: 423.22
train mean loss: 407.38
epoch train time: 0:00:02.324852
elapsed time: 0:01:45.616362
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 21:24:04.431255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.04
 ---- batch: 020 ----
mean loss: 397.59
 ---- batch: 030 ----
mean loss: 393.01
 ---- batch: 040 ----
mean loss: 391.41
 ---- batch: 050 ----
mean loss: 404.88
 ---- batch: 060 ----
mean loss: 400.78
 ---- batch: 070 ----
mean loss: 397.83
 ---- batch: 080 ----
mean loss: 417.86
 ---- batch: 090 ----
mean loss: 406.77
train mean loss: 399.99
epoch train time: 0:00:02.318185
elapsed time: 0:01:47.934806
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 21:24:06.749715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.08
 ---- batch: 020 ----
mean loss: 411.02
 ---- batch: 030 ----
mean loss: 392.51
 ---- batch: 040 ----
mean loss: 400.51
 ---- batch: 050 ----
mean loss: 406.38
 ---- batch: 060 ----
mean loss: 413.07
 ---- batch: 070 ----
mean loss: 419.78
 ---- batch: 080 ----
mean loss: 397.83
 ---- batch: 090 ----
mean loss: 390.79
train mean loss: 403.58
epoch train time: 0:00:02.320738
elapsed time: 0:01:50.255742
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 21:24:09.070641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.06
 ---- batch: 020 ----
mean loss: 435.75
 ---- batch: 030 ----
mean loss: 403.49
 ---- batch: 040 ----
mean loss: 388.05
 ---- batch: 050 ----
mean loss: 410.16
 ---- batch: 060 ----
mean loss: 377.49
 ---- batch: 070 ----
mean loss: 398.85
 ---- batch: 080 ----
mean loss: 402.15
 ---- batch: 090 ----
mean loss: 422.73
train mean loss: 407.55
epoch train time: 0:00:02.320796
elapsed time: 0:01:52.576744
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 21:24:11.391639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.97
 ---- batch: 020 ----
mean loss: 401.72
 ---- batch: 030 ----
mean loss: 414.76
 ---- batch: 040 ----
mean loss: 408.67
 ---- batch: 050 ----
mean loss: 407.64
 ---- batch: 060 ----
mean loss: 400.42
 ---- batch: 070 ----
mean loss: 401.35
 ---- batch: 080 ----
mean loss: 407.46
 ---- batch: 090 ----
mean loss: 413.44
train mean loss: 406.67
epoch train time: 0:00:02.321465
elapsed time: 0:01:54.898388
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 21:24:13.713311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.37
 ---- batch: 020 ----
mean loss: 406.03
 ---- batch: 030 ----
mean loss: 402.98
 ---- batch: 040 ----
mean loss: 411.30
 ---- batch: 050 ----
mean loss: 398.48
 ---- batch: 060 ----
mean loss: 395.58
 ---- batch: 070 ----
mean loss: 395.05
 ---- batch: 080 ----
mean loss: 405.71
 ---- batch: 090 ----
mean loss: 388.65
train mean loss: 400.56
epoch train time: 0:00:02.327356
elapsed time: 0:01:57.225953
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 21:24:16.040847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.71
 ---- batch: 020 ----
mean loss: 395.30
 ---- batch: 030 ----
mean loss: 385.58
 ---- batch: 040 ----
mean loss: 402.16
 ---- batch: 050 ----
mean loss: 396.47
 ---- batch: 060 ----
mean loss: 396.73
 ---- batch: 070 ----
mean loss: 390.49
 ---- batch: 080 ----
mean loss: 389.63
 ---- batch: 090 ----
mean loss: 397.38
train mean loss: 395.56
epoch train time: 0:00:02.323914
elapsed time: 0:01:59.550061
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 21:24:18.364953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.98
 ---- batch: 020 ----
mean loss: 381.92
 ---- batch: 030 ----
mean loss: 401.93
 ---- batch: 040 ----
mean loss: 414.86
 ---- batch: 050 ----
mean loss: 395.80
 ---- batch: 060 ----
mean loss: 395.62
 ---- batch: 070 ----
mean loss: 422.80
 ---- batch: 080 ----
mean loss: 409.46
 ---- batch: 090 ----
mean loss: 400.48
train mean loss: 401.04
epoch train time: 0:00:02.322711
elapsed time: 0:02:01.872971
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 21:24:20.687926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.72
 ---- batch: 020 ----
mean loss: 390.23
 ---- batch: 030 ----
mean loss: 406.41
 ---- batch: 040 ----
mean loss: 401.81
 ---- batch: 050 ----
mean loss: 402.67
 ---- batch: 060 ----
mean loss: 397.67
 ---- batch: 070 ----
mean loss: 407.16
 ---- batch: 080 ----
mean loss: 396.65
 ---- batch: 090 ----
mean loss: 387.98
train mean loss: 397.36
epoch train time: 0:00:02.327893
elapsed time: 0:02:04.201115
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 21:24:23.016006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.26
 ---- batch: 020 ----
mean loss: 429.04
 ---- batch: 030 ----
mean loss: 405.41
 ---- batch: 040 ----
mean loss: 411.19
 ---- batch: 050 ----
mean loss: 396.98
 ---- batch: 060 ----
mean loss: 393.59
 ---- batch: 070 ----
mean loss: 405.24
 ---- batch: 080 ----
mean loss: 401.06
 ---- batch: 090 ----
mean loss: 398.60
train mean loss: 404.52
epoch train time: 0:00:02.326196
elapsed time: 0:02:06.527487
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 21:24:25.342379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.98
 ---- batch: 020 ----
mean loss: 401.13
 ---- batch: 030 ----
mean loss: 391.36
 ---- batch: 040 ----
mean loss: 388.60
 ---- batch: 050 ----
mean loss: 403.37
 ---- batch: 060 ----
mean loss: 393.88
 ---- batch: 070 ----
mean loss: 393.58
 ---- batch: 080 ----
mean loss: 415.10
 ---- batch: 090 ----
mean loss: 391.31
train mean loss: 396.24
epoch train time: 0:00:02.326069
elapsed time: 0:02:08.853737
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 21:24:27.668628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.45
 ---- batch: 020 ----
mean loss: 411.57
 ---- batch: 030 ----
mean loss: 412.90
 ---- batch: 040 ----
mean loss: 391.83
 ---- batch: 050 ----
mean loss: 404.72
 ---- batch: 060 ----
mean loss: 401.88
 ---- batch: 070 ----
mean loss: 390.77
 ---- batch: 080 ----
mean loss: 397.06
 ---- batch: 090 ----
mean loss: 391.69
train mean loss: 398.08
epoch train time: 0:00:02.316243
elapsed time: 0:02:11.170204
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 21:24:29.985099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.69
 ---- batch: 020 ----
mean loss: 389.52
 ---- batch: 030 ----
mean loss: 413.48
 ---- batch: 040 ----
mean loss: 395.38
 ---- batch: 050 ----
mean loss: 391.85
 ---- batch: 060 ----
mean loss: 393.79
 ---- batch: 070 ----
mean loss: 393.99
 ---- batch: 080 ----
mean loss: 406.99
 ---- batch: 090 ----
mean loss: 395.93
train mean loss: 400.27
epoch train time: 0:00:02.323104
elapsed time: 0:02:13.493494
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 21:24:32.308404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.46
 ---- batch: 020 ----
mean loss: 410.57
 ---- batch: 030 ----
mean loss: 386.12
 ---- batch: 040 ----
mean loss: 399.80
 ---- batch: 050 ----
mean loss: 395.81
 ---- batch: 060 ----
mean loss: 391.84
 ---- batch: 070 ----
mean loss: 395.34
 ---- batch: 080 ----
mean loss: 397.07
 ---- batch: 090 ----
mean loss: 393.68
train mean loss: 399.31
epoch train time: 0:00:02.335154
elapsed time: 0:02:15.828838
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 21:24:34.643733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.00
 ---- batch: 020 ----
mean loss: 389.42
 ---- batch: 030 ----
mean loss: 411.84
 ---- batch: 040 ----
mean loss: 379.64
 ---- batch: 050 ----
mean loss: 398.84
 ---- batch: 060 ----
mean loss: 407.53
 ---- batch: 070 ----
mean loss: 392.37
 ---- batch: 080 ----
mean loss: 417.21
 ---- batch: 090 ----
mean loss: 395.01
train mean loss: 400.29
epoch train time: 0:00:02.323400
elapsed time: 0:02:18.152415
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 21:24:36.967338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.76
 ---- batch: 020 ----
mean loss: 411.84
 ---- batch: 030 ----
mean loss: 391.24
 ---- batch: 040 ----
mean loss: 394.69
 ---- batch: 050 ----
mean loss: 402.00
 ---- batch: 060 ----
mean loss: 397.92
 ---- batch: 070 ----
mean loss: 424.25
 ---- batch: 080 ----
mean loss: 415.74
 ---- batch: 090 ----
mean loss: 403.68
train mean loss: 404.59
epoch train time: 0:00:02.322156
elapsed time: 0:02:20.474814
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 21:24:39.289712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.77
 ---- batch: 020 ----
mean loss: 390.54
 ---- batch: 030 ----
mean loss: 399.06
 ---- batch: 040 ----
mean loss: 385.13
 ---- batch: 050 ----
mean loss: 386.36
 ---- batch: 060 ----
mean loss: 405.15
 ---- batch: 070 ----
mean loss: 404.68
 ---- batch: 080 ----
mean loss: 389.94
 ---- batch: 090 ----
mean loss: 399.49
train mean loss: 394.57
epoch train time: 0:00:02.329652
elapsed time: 0:02:22.804685
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 21:24:41.619581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.44
 ---- batch: 020 ----
mean loss: 411.07
 ---- batch: 030 ----
mean loss: 419.69
 ---- batch: 040 ----
mean loss: 391.44
 ---- batch: 050 ----
mean loss: 407.99
 ---- batch: 060 ----
mean loss: 402.76
 ---- batch: 070 ----
mean loss: 406.78
 ---- batch: 080 ----
mean loss: 410.75
 ---- batch: 090 ----
mean loss: 405.58
train mean loss: 405.56
epoch train time: 0:00:02.326223
elapsed time: 0:02:25.131119
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 21:24:43.946027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.04
 ---- batch: 020 ----
mean loss: 393.75
 ---- batch: 030 ----
mean loss: 383.92
 ---- batch: 040 ----
mean loss: 391.25
 ---- batch: 050 ----
mean loss: 386.61
 ---- batch: 060 ----
mean loss: 398.12
 ---- batch: 070 ----
mean loss: 408.79
 ---- batch: 080 ----
mean loss: 407.89
 ---- batch: 090 ----
mean loss: 389.98
train mean loss: 395.60
epoch train time: 0:00:02.315221
elapsed time: 0:02:27.446543
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 21:24:46.261443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.65
 ---- batch: 020 ----
mean loss: 386.80
 ---- batch: 030 ----
mean loss: 393.48
 ---- batch: 040 ----
mean loss: 395.54
 ---- batch: 050 ----
mean loss: 410.13
 ---- batch: 060 ----
mean loss: 424.53
 ---- batch: 070 ----
mean loss: 396.66
 ---- batch: 080 ----
mean loss: 395.39
 ---- batch: 090 ----
mean loss: 393.83
train mean loss: 397.41
epoch train time: 0:00:02.321317
elapsed time: 0:02:29.768054
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 21:24:48.582949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.64
 ---- batch: 020 ----
mean loss: 390.30
 ---- batch: 030 ----
mean loss: 395.89
 ---- batch: 040 ----
mean loss: 396.10
 ---- batch: 050 ----
mean loss: 400.01
 ---- batch: 060 ----
mean loss: 407.12
 ---- batch: 070 ----
mean loss: 399.67
 ---- batch: 080 ----
mean loss: 386.89
 ---- batch: 090 ----
mean loss: 384.02
train mean loss: 395.02
epoch train time: 0:00:02.325202
elapsed time: 0:02:32.093451
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 21:24:50.908358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.72
 ---- batch: 020 ----
mean loss: 393.92
 ---- batch: 030 ----
mean loss: 396.62
 ---- batch: 040 ----
mean loss: 402.75
 ---- batch: 050 ----
mean loss: 384.96
 ---- batch: 060 ----
mean loss: 406.98
 ---- batch: 070 ----
mean loss: 391.37
 ---- batch: 080 ----
mean loss: 386.41
 ---- batch: 090 ----
mean loss: 395.43
train mean loss: 394.79
epoch train time: 0:00:02.322218
elapsed time: 0:02:34.415875
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 21:24:53.230799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.57
 ---- batch: 020 ----
mean loss: 393.12
 ---- batch: 030 ----
mean loss: 395.43
 ---- batch: 040 ----
mean loss: 382.90
 ---- batch: 050 ----
mean loss: 390.83
 ---- batch: 060 ----
mean loss: 398.41
 ---- batch: 070 ----
mean loss: 378.02
 ---- batch: 080 ----
mean loss: 389.43
 ---- batch: 090 ----
mean loss: 408.57
train mean loss: 391.40
epoch train time: 0:00:02.324187
elapsed time: 0:02:36.740292
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 21:24:55.555212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.14
 ---- batch: 020 ----
mean loss: 403.44
 ---- batch: 030 ----
mean loss: 408.81
 ---- batch: 040 ----
mean loss: 397.67
 ---- batch: 050 ----
mean loss: 377.92
 ---- batch: 060 ----
mean loss: 390.28
 ---- batch: 070 ----
mean loss: 386.67
 ---- batch: 080 ----
mean loss: 408.07
 ---- batch: 090 ----
mean loss: 377.48
train mean loss: 393.61
epoch train time: 0:00:02.324216
elapsed time: 0:02:39.064741
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 21:24:57.879635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.02
 ---- batch: 020 ----
mean loss: 385.92
 ---- batch: 030 ----
mean loss: 387.94
 ---- batch: 040 ----
mean loss: 387.27
 ---- batch: 050 ----
mean loss: 392.94
 ---- batch: 060 ----
mean loss: 403.98
 ---- batch: 070 ----
mean loss: 420.63
 ---- batch: 080 ----
mean loss: 387.96
 ---- batch: 090 ----
mean loss: 389.12
train mean loss: 394.96
epoch train time: 0:00:02.321717
elapsed time: 0:02:41.386644
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 21:25:00.201554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.34
 ---- batch: 020 ----
mean loss: 403.73
 ---- batch: 030 ----
mean loss: 389.85
 ---- batch: 040 ----
mean loss: 385.60
 ---- batch: 050 ----
mean loss: 385.51
 ---- batch: 060 ----
mean loss: 387.95
 ---- batch: 070 ----
mean loss: 401.32
 ---- batch: 080 ----
mean loss: 381.81
 ---- batch: 090 ----
mean loss: 377.67
train mean loss: 390.57
epoch train time: 0:00:02.317444
elapsed time: 0:02:43.704298
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 21:25:02.519193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.72
 ---- batch: 020 ----
mean loss: 386.68
 ---- batch: 030 ----
mean loss: 397.04
 ---- batch: 040 ----
mean loss: 405.47
 ---- batch: 050 ----
mean loss: 389.62
 ---- batch: 060 ----
mean loss: 401.19
 ---- batch: 070 ----
mean loss: 409.36
 ---- batch: 080 ----
mean loss: 415.02
 ---- batch: 090 ----
mean loss: 384.07
train mean loss: 398.28
epoch train time: 0:00:02.325204
elapsed time: 0:02:46.029685
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 21:25:04.844580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.94
 ---- batch: 020 ----
mean loss: 405.55
 ---- batch: 030 ----
mean loss: 397.51
 ---- batch: 040 ----
mean loss: 399.53
 ---- batch: 050 ----
mean loss: 408.67
 ---- batch: 060 ----
mean loss: 398.39
 ---- batch: 070 ----
mean loss: 380.31
 ---- batch: 080 ----
mean loss: 390.41
 ---- batch: 090 ----
mean loss: 404.70
train mean loss: 396.18
epoch train time: 0:00:02.321515
elapsed time: 0:02:48.351388
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 21:25:07.166280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.77
 ---- batch: 020 ----
mean loss: 425.68
 ---- batch: 030 ----
mean loss: 396.11
 ---- batch: 040 ----
mean loss: 398.18
 ---- batch: 050 ----
mean loss: 391.01
 ---- batch: 060 ----
mean loss: 391.54
 ---- batch: 070 ----
mean loss: 392.62
 ---- batch: 080 ----
mean loss: 396.43
 ---- batch: 090 ----
mean loss: 389.50
train mean loss: 398.11
epoch train time: 0:00:02.321255
elapsed time: 0:02:50.672822
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 21:25:09.487716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.07
 ---- batch: 020 ----
mean loss: 403.46
 ---- batch: 030 ----
mean loss: 380.56
 ---- batch: 040 ----
mean loss: 389.92
 ---- batch: 050 ----
mean loss: 385.49
 ---- batch: 060 ----
mean loss: 382.86
 ---- batch: 070 ----
mean loss: 390.76
 ---- batch: 080 ----
mean loss: 385.42
 ---- batch: 090 ----
mean loss: 393.76
train mean loss: 389.70
epoch train time: 0:00:02.320978
elapsed time: 0:02:52.993982
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 21:25:11.808928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.52
 ---- batch: 020 ----
mean loss: 381.54
 ---- batch: 030 ----
mean loss: 394.20
 ---- batch: 040 ----
mean loss: 397.26
 ---- batch: 050 ----
mean loss: 382.35
 ---- batch: 060 ----
mean loss: 386.78
 ---- batch: 070 ----
mean loss: 387.33
 ---- batch: 080 ----
mean loss: 402.68
 ---- batch: 090 ----
mean loss: 385.98
train mean loss: 389.07
epoch train time: 0:00:02.326047
elapsed time: 0:02:55.320275
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 21:25:14.135230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.94
 ---- batch: 020 ----
mean loss: 388.22
 ---- batch: 030 ----
mean loss: 394.60
 ---- batch: 040 ----
mean loss: 396.27
 ---- batch: 050 ----
mean loss: 388.50
 ---- batch: 060 ----
mean loss: 410.50
 ---- batch: 070 ----
mean loss: 393.73
 ---- batch: 080 ----
mean loss: 410.03
 ---- batch: 090 ----
mean loss: 422.13
train mean loss: 400.33
epoch train time: 0:00:02.321607
elapsed time: 0:02:57.642205
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 21:25:16.457110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.89
 ---- batch: 020 ----
mean loss: 375.86
 ---- batch: 030 ----
mean loss: 387.14
 ---- batch: 040 ----
mean loss: 386.57
 ---- batch: 050 ----
mean loss: 412.29
 ---- batch: 060 ----
mean loss: 390.44
 ---- batch: 070 ----
mean loss: 385.00
 ---- batch: 080 ----
mean loss: 385.17
 ---- batch: 090 ----
mean loss: 378.49
train mean loss: 387.52
epoch train time: 0:00:02.311894
elapsed time: 0:02:59.954293
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 21:25:18.769186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.32
 ---- batch: 020 ----
mean loss: 395.07
 ---- batch: 030 ----
mean loss: 400.79
 ---- batch: 040 ----
mean loss: 397.45
 ---- batch: 050 ----
mean loss: 418.22
 ---- batch: 060 ----
mean loss: 391.96
 ---- batch: 070 ----
mean loss: 403.78
 ---- batch: 080 ----
mean loss: 388.27
 ---- batch: 090 ----
mean loss: 375.53
train mean loss: 395.40
epoch train time: 0:00:02.316574
elapsed time: 0:03:02.271044
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 21:25:21.085955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.85
 ---- batch: 020 ----
mean loss: 390.49
 ---- batch: 030 ----
mean loss: 402.97
 ---- batch: 040 ----
mean loss: 388.06
 ---- batch: 050 ----
mean loss: 391.77
 ---- batch: 060 ----
mean loss: 395.99
 ---- batch: 070 ----
mean loss: 391.34
 ---- batch: 080 ----
mean loss: 405.05
 ---- batch: 090 ----
mean loss: 391.38
train mean loss: 394.83
epoch train time: 0:00:02.325411
elapsed time: 0:03:04.596648
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 21:25:23.411542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.68
 ---- batch: 020 ----
mean loss: 394.64
 ---- batch: 030 ----
mean loss: 386.81
 ---- batch: 040 ----
mean loss: 391.35
 ---- batch: 050 ----
mean loss: 384.26
 ---- batch: 060 ----
mean loss: 389.36
 ---- batch: 070 ----
mean loss: 385.14
 ---- batch: 080 ----
mean loss: 396.66
 ---- batch: 090 ----
mean loss: 387.82
train mean loss: 388.83
epoch train time: 0:00:02.324065
elapsed time: 0:03:06.920908
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 21:25:25.735805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.41
 ---- batch: 020 ----
mean loss: 395.14
 ---- batch: 030 ----
mean loss: 400.17
 ---- batch: 040 ----
mean loss: 401.16
 ---- batch: 050 ----
mean loss: 394.22
 ---- batch: 060 ----
mean loss: 397.41
 ---- batch: 070 ----
mean loss: 390.79
 ---- batch: 080 ----
mean loss: 389.35
 ---- batch: 090 ----
mean loss: 387.08
train mean loss: 393.92
epoch train time: 0:00:02.324294
elapsed time: 0:03:09.245396
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 21:25:28.060322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.17
 ---- batch: 020 ----
mean loss: 390.21
 ---- batch: 030 ----
mean loss: 381.75
 ---- batch: 040 ----
mean loss: 380.52
 ---- batch: 050 ----
mean loss: 390.67
 ---- batch: 060 ----
mean loss: 383.20
 ---- batch: 070 ----
mean loss: 387.70
 ---- batch: 080 ----
mean loss: 400.39
 ---- batch: 090 ----
mean loss: 386.19
train mean loss: 387.36
epoch train time: 0:00:02.323182
elapsed time: 0:03:11.568800
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 21:25:30.383719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.71
 ---- batch: 020 ----
mean loss: 393.79
 ---- batch: 030 ----
mean loss: 378.82
 ---- batch: 040 ----
mean loss: 390.04
 ---- batch: 050 ----
mean loss: 385.73
 ---- batch: 060 ----
mean loss: 391.77
 ---- batch: 070 ----
mean loss: 390.64
 ---- batch: 080 ----
mean loss: 384.58
 ---- batch: 090 ----
mean loss: 380.89
train mean loss: 388.73
epoch train time: 0:00:02.317661
elapsed time: 0:03:13.886668
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 21:25:32.701560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.98
 ---- batch: 020 ----
mean loss: 394.80
 ---- batch: 030 ----
mean loss: 410.59
 ---- batch: 040 ----
mean loss: 390.40
 ---- batch: 050 ----
mean loss: 382.81
 ---- batch: 060 ----
mean loss: 375.69
 ---- batch: 070 ----
mean loss: 395.03
 ---- batch: 080 ----
mean loss: 401.08
 ---- batch: 090 ----
mean loss: 406.97
train mean loss: 396.05
epoch train time: 0:00:02.320488
elapsed time: 0:03:16.207335
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 21:25:35.022231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.24
 ---- batch: 020 ----
mean loss: 376.86
 ---- batch: 030 ----
mean loss: 388.86
 ---- batch: 040 ----
mean loss: 397.37
 ---- batch: 050 ----
mean loss: 392.67
 ---- batch: 060 ----
mean loss: 385.40
 ---- batch: 070 ----
mean loss: 392.68
 ---- batch: 080 ----
mean loss: 386.73
 ---- batch: 090 ----
mean loss: 388.59
train mean loss: 388.72
epoch train time: 0:00:02.316437
elapsed time: 0:03:18.523954
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 21:25:37.338848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.93
 ---- batch: 020 ----
mean loss: 393.58
 ---- batch: 030 ----
mean loss: 408.47
 ---- batch: 040 ----
mean loss: 387.02
 ---- batch: 050 ----
mean loss: 387.84
 ---- batch: 060 ----
mean loss: 394.20
 ---- batch: 070 ----
mean loss: 371.17
 ---- batch: 080 ----
mean loss: 375.81
 ---- batch: 090 ----
mean loss: 393.19
train mean loss: 387.37
epoch train time: 0:00:02.327508
elapsed time: 0:03:20.851711
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 21:25:39.666612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.88
 ---- batch: 020 ----
mean loss: 368.22
 ---- batch: 030 ----
mean loss: 382.65
 ---- batch: 040 ----
mean loss: 390.13
 ---- batch: 050 ----
mean loss: 393.58
 ---- batch: 060 ----
mean loss: 381.03
 ---- batch: 070 ----
mean loss: 396.65
 ---- batch: 080 ----
mean loss: 405.72
 ---- batch: 090 ----
mean loss: 385.80
train mean loss: 386.75
epoch train time: 0:00:02.327177
elapsed time: 0:03:23.179093
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 21:25:41.993973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.89
 ---- batch: 020 ----
mean loss: 378.42
 ---- batch: 030 ----
mean loss: 406.77
 ---- batch: 040 ----
mean loss: 398.20
 ---- batch: 050 ----
mean loss: 400.35
 ---- batch: 060 ----
mean loss: 384.87
 ---- batch: 070 ----
mean loss: 387.39
 ---- batch: 080 ----
mean loss: 392.78
 ---- batch: 090 ----
mean loss: 383.57
train mean loss: 390.38
epoch train time: 0:00:02.317462
elapsed time: 0:03:25.496723
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 21:25:44.311612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.98
 ---- batch: 020 ----
mean loss: 403.71
 ---- batch: 030 ----
mean loss: 396.15
 ---- batch: 040 ----
mean loss: 389.49
 ---- batch: 050 ----
mean loss: 389.74
 ---- batch: 060 ----
mean loss: 374.42
 ---- batch: 070 ----
mean loss: 387.90
 ---- batch: 080 ----
mean loss: 384.49
 ---- batch: 090 ----
mean loss: 387.73
train mean loss: 388.32
epoch train time: 0:00:02.320928
elapsed time: 0:03:27.817875
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 21:25:46.632776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.02
 ---- batch: 020 ----
mean loss: 381.94
 ---- batch: 030 ----
mean loss: 380.67
 ---- batch: 040 ----
mean loss: 394.44
 ---- batch: 050 ----
mean loss: 381.81
 ---- batch: 060 ----
mean loss: 369.11
 ---- batch: 070 ----
mean loss: 384.38
 ---- batch: 080 ----
mean loss: 388.16
 ---- batch: 090 ----
mean loss: 397.58
train mean loss: 385.10
epoch train time: 0:00:02.321850
elapsed time: 0:03:30.139953
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 21:25:48.954848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.59
 ---- batch: 020 ----
mean loss: 398.94
 ---- batch: 030 ----
mean loss: 388.72
 ---- batch: 040 ----
mean loss: 397.27
 ---- batch: 050 ----
mean loss: 393.67
 ---- batch: 060 ----
mean loss: 380.84
 ---- batch: 070 ----
mean loss: 384.59
 ---- batch: 080 ----
mean loss: 382.23
 ---- batch: 090 ----
mean loss: 386.29
train mean loss: 392.36
epoch train time: 0:00:02.324352
elapsed time: 0:03:32.464551
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 21:25:51.279462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.46
 ---- batch: 020 ----
mean loss: 383.85
 ---- batch: 030 ----
mean loss: 400.59
 ---- batch: 040 ----
mean loss: 393.33
 ---- batch: 050 ----
mean loss: 402.30
 ---- batch: 060 ----
mean loss: 401.61
 ---- batch: 070 ----
mean loss: 393.18
 ---- batch: 080 ----
mean loss: 389.49
 ---- batch: 090 ----
mean loss: 382.43
train mean loss: 392.42
epoch train time: 0:00:02.330327
elapsed time: 0:03:34.795113
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 21:25:53.610062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.71
 ---- batch: 020 ----
mean loss: 398.24
 ---- batch: 030 ----
mean loss: 374.89
 ---- batch: 040 ----
mean loss: 401.48
 ---- batch: 050 ----
mean loss: 376.00
 ---- batch: 060 ----
mean loss: 383.22
 ---- batch: 070 ----
mean loss: 377.97
 ---- batch: 080 ----
mean loss: 391.15
 ---- batch: 090 ----
mean loss: 388.42
train mean loss: 388.05
epoch train time: 0:00:02.327117
elapsed time: 0:03:37.122476
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 21:25:55.937376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.50
 ---- batch: 020 ----
mean loss: 400.93
 ---- batch: 030 ----
mean loss: 386.23
 ---- batch: 040 ----
mean loss: 392.34
 ---- batch: 050 ----
mean loss: 383.61
 ---- batch: 060 ----
mean loss: 395.00
 ---- batch: 070 ----
mean loss: 381.18
 ---- batch: 080 ----
mean loss: 400.89
 ---- batch: 090 ----
mean loss: 419.73
train mean loss: 395.61
epoch train time: 0:00:02.322368
elapsed time: 0:03:39.445029
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 21:25:58.259923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.62
 ---- batch: 020 ----
mean loss: 402.15
 ---- batch: 030 ----
mean loss: 388.84
 ---- batch: 040 ----
mean loss: 400.37
 ---- batch: 050 ----
mean loss: 383.41
 ---- batch: 060 ----
mean loss: 375.71
 ---- batch: 070 ----
mean loss: 380.41
 ---- batch: 080 ----
mean loss: 383.95
 ---- batch: 090 ----
mean loss: 375.11
train mean loss: 386.37
epoch train time: 0:00:02.328884
elapsed time: 0:03:41.774204
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 21:26:00.589112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.50
 ---- batch: 020 ----
mean loss: 391.12
 ---- batch: 030 ----
mean loss: 386.77
 ---- batch: 040 ----
mean loss: 382.67
 ---- batch: 050 ----
mean loss: 391.05
 ---- batch: 060 ----
mean loss: 377.32
 ---- batch: 070 ----
mean loss: 375.61
 ---- batch: 080 ----
mean loss: 376.51
 ---- batch: 090 ----
mean loss: 394.80
train mean loss: 386.50
epoch train time: 0:00:02.325408
elapsed time: 0:03:44.099806
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 21:26:02.914697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.43
 ---- batch: 020 ----
mean loss: 386.15
 ---- batch: 030 ----
mean loss: 399.34
 ---- batch: 040 ----
mean loss: 383.77
 ---- batch: 050 ----
mean loss: 390.26
 ---- batch: 060 ----
mean loss: 380.37
 ---- batch: 070 ----
mean loss: 367.85
 ---- batch: 080 ----
mean loss: 395.47
 ---- batch: 090 ----
mean loss: 383.84
train mean loss: 386.28
epoch train time: 0:00:02.320445
elapsed time: 0:03:46.420441
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 21:26:05.235374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.96
 ---- batch: 020 ----
mean loss: 366.46
 ---- batch: 030 ----
mean loss: 390.91
 ---- batch: 040 ----
mean loss: 375.81
 ---- batch: 050 ----
mean loss: 383.72
 ---- batch: 060 ----
mean loss: 391.79
 ---- batch: 070 ----
mean loss: 392.89
 ---- batch: 080 ----
mean loss: 379.03
 ---- batch: 090 ----
mean loss: 377.56
train mean loss: 382.61
epoch train time: 0:00:02.321252
elapsed time: 0:03:48.741913
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 21:26:07.556806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.56
 ---- batch: 020 ----
mean loss: 373.87
 ---- batch: 030 ----
mean loss: 394.34
 ---- batch: 040 ----
mean loss: 386.77
 ---- batch: 050 ----
mean loss: 397.83
 ---- batch: 060 ----
mean loss: 384.85
 ---- batch: 070 ----
mean loss: 382.21
 ---- batch: 080 ----
mean loss: 377.27
 ---- batch: 090 ----
mean loss: 392.95
train mean loss: 384.55
epoch train time: 0:00:02.323068
elapsed time: 0:03:51.065172
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 21:26:09.880064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.49
 ---- batch: 020 ----
mean loss: 394.61
 ---- batch: 030 ----
mean loss: 390.58
 ---- batch: 040 ----
mean loss: 380.98
 ---- batch: 050 ----
mean loss: 381.71
 ---- batch: 060 ----
mean loss: 375.37
 ---- batch: 070 ----
mean loss: 376.97
 ---- batch: 080 ----
mean loss: 382.99
 ---- batch: 090 ----
mean loss: 384.47
train mean loss: 386.09
epoch train time: 0:00:02.327164
elapsed time: 0:03:53.392514
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 21:26:12.207409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.17
 ---- batch: 020 ----
mean loss: 368.48
 ---- batch: 030 ----
mean loss: 382.18
 ---- batch: 040 ----
mean loss: 381.20
 ---- batch: 050 ----
mean loss: 387.77
 ---- batch: 060 ----
mean loss: 385.43
 ---- batch: 070 ----
mean loss: 383.21
 ---- batch: 080 ----
mean loss: 396.21
 ---- batch: 090 ----
mean loss: 377.97
train mean loss: 384.56
epoch train time: 0:00:02.316192
elapsed time: 0:03:55.708924
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 21:26:14.523855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.65
 ---- batch: 020 ----
mean loss: 383.43
 ---- batch: 030 ----
mean loss: 379.67
 ---- batch: 040 ----
mean loss: 376.25
 ---- batch: 050 ----
mean loss: 366.75
 ---- batch: 060 ----
mean loss: 389.67
 ---- batch: 070 ----
mean loss: 377.91
 ---- batch: 080 ----
mean loss: 390.93
 ---- batch: 090 ----
mean loss: 378.17
train mean loss: 380.57
epoch train time: 0:00:02.318185
elapsed time: 0:03:58.027351
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 21:26:16.842269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.49
 ---- batch: 020 ----
mean loss: 376.72
 ---- batch: 030 ----
mean loss: 389.76
 ---- batch: 040 ----
mean loss: 403.21
 ---- batch: 050 ----
mean loss: 384.19
 ---- batch: 060 ----
mean loss: 388.60
 ---- batch: 070 ----
mean loss: 382.01
 ---- batch: 080 ----
mean loss: 369.31
 ---- batch: 090 ----
mean loss: 367.89
train mean loss: 383.77
epoch train time: 0:00:02.328398
elapsed time: 0:04:00.355958
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 21:26:19.170849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.43
 ---- batch: 020 ----
mean loss: 382.72
 ---- batch: 030 ----
mean loss: 371.97
 ---- batch: 040 ----
mean loss: 377.24
 ---- batch: 050 ----
mean loss: 367.52
 ---- batch: 060 ----
mean loss: 377.00
 ---- batch: 070 ----
mean loss: 371.70
 ---- batch: 080 ----
mean loss: 373.34
 ---- batch: 090 ----
mean loss: 378.83
train mean loss: 376.96
epoch train time: 0:00:02.313332
elapsed time: 0:04:02.669463
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 21:26:21.484374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.68
 ---- batch: 020 ----
mean loss: 379.58
 ---- batch: 030 ----
mean loss: 383.18
 ---- batch: 040 ----
mean loss: 382.89
 ---- batch: 050 ----
mean loss: 415.38
 ---- batch: 060 ----
mean loss: 433.75
 ---- batch: 070 ----
mean loss: 391.76
 ---- batch: 080 ----
mean loss: 406.09
 ---- batch: 090 ----
mean loss: 389.43
train mean loss: 395.39
epoch train time: 0:00:02.321394
elapsed time: 0:04:04.991068
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 21:26:23.805963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.17
 ---- batch: 020 ----
mean loss: 391.10
 ---- batch: 030 ----
mean loss: 378.39
 ---- batch: 040 ----
mean loss: 386.51
 ---- batch: 050 ----
mean loss: 386.27
 ---- batch: 060 ----
mean loss: 412.36
 ---- batch: 070 ----
mean loss: 373.63
 ---- batch: 080 ----
mean loss: 378.21
 ---- batch: 090 ----
mean loss: 367.74
train mean loss: 382.38
epoch train time: 0:00:02.316719
elapsed time: 0:04:07.307984
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 21:26:26.122883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.08
 ---- batch: 020 ----
mean loss: 387.04
 ---- batch: 030 ----
mean loss: 388.16
 ---- batch: 040 ----
mean loss: 399.44
 ---- batch: 050 ----
mean loss: 388.37
 ---- batch: 060 ----
mean loss: 379.55
 ---- batch: 070 ----
mean loss: 382.48
 ---- batch: 080 ----
mean loss: 379.76
 ---- batch: 090 ----
mean loss: 372.56
train mean loss: 382.72
epoch train time: 0:00:02.331214
elapsed time: 0:04:09.639381
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 21:26:28.454273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.00
 ---- batch: 020 ----
mean loss: 386.10
 ---- batch: 030 ----
mean loss: 390.15
 ---- batch: 040 ----
mean loss: 378.58
 ---- batch: 050 ----
mean loss: 388.88
 ---- batch: 060 ----
mean loss: 388.65
 ---- batch: 070 ----
mean loss: 388.88
 ---- batch: 080 ----
mean loss: 377.87
 ---- batch: 090 ----
mean loss: 374.93
train mean loss: 383.25
epoch train time: 0:00:02.319904
elapsed time: 0:04:11.959473
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 21:26:30.774370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.25
 ---- batch: 020 ----
mean loss: 384.39
 ---- batch: 030 ----
mean loss: 382.83
 ---- batch: 040 ----
mean loss: 394.52
 ---- batch: 050 ----
mean loss: 386.46
 ---- batch: 060 ----
mean loss: 385.31
 ---- batch: 070 ----
mean loss: 383.33
 ---- batch: 080 ----
mean loss: 375.45
 ---- batch: 090 ----
mean loss: 385.93
train mean loss: 382.84
epoch train time: 0:00:02.319068
elapsed time: 0:04:14.278724
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 21:26:33.093648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.75
 ---- batch: 020 ----
mean loss: 374.41
 ---- batch: 030 ----
mean loss: 389.21
 ---- batch: 040 ----
mean loss: 375.47
 ---- batch: 050 ----
mean loss: 381.37
 ---- batch: 060 ----
mean loss: 368.02
 ---- batch: 070 ----
mean loss: 375.98
 ---- batch: 080 ----
mean loss: 395.39
 ---- batch: 090 ----
mean loss: 391.25
train mean loss: 380.64
epoch train time: 0:00:02.317713
elapsed time: 0:04:16.596663
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 21:26:35.411557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.17
 ---- batch: 020 ----
mean loss: 375.94
 ---- batch: 030 ----
mean loss: 373.14
 ---- batch: 040 ----
mean loss: 387.45
 ---- batch: 050 ----
mean loss: 372.34
 ---- batch: 060 ----
mean loss: 380.98
 ---- batch: 070 ----
mean loss: 380.02
 ---- batch: 080 ----
mean loss: 373.42
 ---- batch: 090 ----
mean loss: 376.71
train mean loss: 380.57
epoch train time: 0:00:02.328395
elapsed time: 0:04:18.925244
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 21:26:37.740155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.65
 ---- batch: 020 ----
mean loss: 381.21
 ---- batch: 030 ----
mean loss: 371.97
 ---- batch: 040 ----
mean loss: 389.57
 ---- batch: 050 ----
mean loss: 388.15
 ---- batch: 060 ----
mean loss: 381.42
 ---- batch: 070 ----
mean loss: 393.35
 ---- batch: 080 ----
mean loss: 380.00
 ---- batch: 090 ----
mean loss: 386.08
train mean loss: 383.98
epoch train time: 0:00:02.322265
elapsed time: 0:04:21.247707
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 21:26:40.062623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.16
 ---- batch: 020 ----
mean loss: 381.18
 ---- batch: 030 ----
mean loss: 386.18
 ---- batch: 040 ----
mean loss: 375.21
 ---- batch: 050 ----
mean loss: 379.97
 ---- batch: 060 ----
mean loss: 378.15
 ---- batch: 070 ----
mean loss: 383.48
 ---- batch: 080 ----
mean loss: 388.48
 ---- batch: 090 ----
mean loss: 376.73
train mean loss: 381.77
epoch train time: 0:00:02.316043
elapsed time: 0:04:23.563952
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 21:26:42.378846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.34
 ---- batch: 020 ----
mean loss: 388.32
 ---- batch: 030 ----
mean loss: 399.43
 ---- batch: 040 ----
mean loss: 383.00
 ---- batch: 050 ----
mean loss: 386.52
 ---- batch: 060 ----
mean loss: 377.59
 ---- batch: 070 ----
mean loss: 390.85
 ---- batch: 080 ----
mean loss: 378.09
 ---- batch: 090 ----
mean loss: 383.00
train mean loss: 384.93
epoch train time: 0:00:02.319646
elapsed time: 0:04:25.883795
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 21:26:44.698715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.77
 ---- batch: 020 ----
mean loss: 369.54
 ---- batch: 030 ----
mean loss: 371.88
 ---- batch: 040 ----
mean loss: 372.08
 ---- batch: 050 ----
mean loss: 389.32
 ---- batch: 060 ----
mean loss: 362.07
 ---- batch: 070 ----
mean loss: 383.70
 ---- batch: 080 ----
mean loss: 393.56
 ---- batch: 090 ----
mean loss: 386.42
train mean loss: 380.32
epoch train time: 0:00:02.321286
elapsed time: 0:04:28.205284
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 21:26:47.020179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.96
 ---- batch: 020 ----
mean loss: 383.64
 ---- batch: 030 ----
mean loss: 385.85
 ---- batch: 040 ----
mean loss: 374.82
 ---- batch: 050 ----
mean loss: 391.75
 ---- batch: 060 ----
mean loss: 385.01
 ---- batch: 070 ----
mean loss: 386.47
 ---- batch: 080 ----
mean loss: 374.88
 ---- batch: 090 ----
mean loss: 377.45
train mean loss: 382.45
epoch train time: 0:00:02.321602
elapsed time: 0:04:30.527071
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 21:26:49.341963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.50
 ---- batch: 020 ----
mean loss: 366.32
 ---- batch: 030 ----
mean loss: 372.22
 ---- batch: 040 ----
mean loss: 368.13
 ---- batch: 050 ----
mean loss: 367.55
 ---- batch: 060 ----
mean loss: 381.26
 ---- batch: 070 ----
mean loss: 371.98
 ---- batch: 080 ----
mean loss: 383.55
 ---- batch: 090 ----
mean loss: 394.21
train mean loss: 377.59
epoch train time: 0:00:02.322426
elapsed time: 0:04:32.849682
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 21:26:51.664575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.82
 ---- batch: 020 ----
mean loss: 377.36
 ---- batch: 030 ----
mean loss: 376.48
 ---- batch: 040 ----
mean loss: 372.59
 ---- batch: 050 ----
mean loss: 374.85
 ---- batch: 060 ----
mean loss: 377.19
 ---- batch: 070 ----
mean loss: 385.59
 ---- batch: 080 ----
mean loss: 377.20
 ---- batch: 090 ----
mean loss: 397.10
train mean loss: 380.79
epoch train time: 0:00:02.331271
elapsed time: 0:04:35.181160
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 21:26:53.996053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.82
 ---- batch: 020 ----
mean loss: 383.89
 ---- batch: 030 ----
mean loss: 380.95
 ---- batch: 040 ----
mean loss: 375.60
 ---- batch: 050 ----
mean loss: 377.08
 ---- batch: 060 ----
mean loss: 371.90
 ---- batch: 070 ----
mean loss: 364.48
 ---- batch: 080 ----
mean loss: 368.67
 ---- batch: 090 ----
mean loss: 379.70
train mean loss: 376.65
epoch train time: 0:00:02.323291
elapsed time: 0:04:37.504643
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 21:26:56.319562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.68
 ---- batch: 020 ----
mean loss: 383.69
 ---- batch: 030 ----
mean loss: 381.02
 ---- batch: 040 ----
mean loss: 377.39
 ---- batch: 050 ----
mean loss: 380.32
 ---- batch: 060 ----
mean loss: 390.57
 ---- batch: 070 ----
mean loss: 365.59
 ---- batch: 080 ----
mean loss: 369.41
 ---- batch: 090 ----
mean loss: 382.84
train mean loss: 379.18
epoch train time: 0:00:02.323049
elapsed time: 0:04:39.827893
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 21:26:58.642803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.54
 ---- batch: 020 ----
mean loss: 380.41
 ---- batch: 030 ----
mean loss: 376.11
 ---- batch: 040 ----
mean loss: 375.49
 ---- batch: 050 ----
mean loss: 376.52
 ---- batch: 060 ----
mean loss: 384.54
 ---- batch: 070 ----
mean loss: 375.11
 ---- batch: 080 ----
mean loss: 388.37
 ---- batch: 090 ----
mean loss: 381.86
train mean loss: 377.91
epoch train time: 0:00:02.327330
elapsed time: 0:04:42.155442
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 21:27:00.970317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.58
 ---- batch: 020 ----
mean loss: 384.11
 ---- batch: 030 ----
mean loss: 369.22
 ---- batch: 040 ----
mean loss: 364.94
 ---- batch: 050 ----
mean loss: 363.88
 ---- batch: 060 ----
mean loss: 378.30
 ---- batch: 070 ----
mean loss: 370.90
 ---- batch: 080 ----
mean loss: 382.90
 ---- batch: 090 ----
mean loss: 368.94
train mean loss: 371.96
epoch train time: 0:00:02.324113
elapsed time: 0:04:44.479724
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 21:27:03.294619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.54
 ---- batch: 020 ----
mean loss: 377.94
 ---- batch: 030 ----
mean loss: 372.26
 ---- batch: 040 ----
mean loss: 372.57
 ---- batch: 050 ----
mean loss: 382.53
 ---- batch: 060 ----
mean loss: 378.08
 ---- batch: 070 ----
mean loss: 387.01
 ---- batch: 080 ----
mean loss: 371.61
 ---- batch: 090 ----
mean loss: 384.37
train mean loss: 377.23
epoch train time: 0:00:02.323705
elapsed time: 0:04:46.803680
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 21:27:05.618604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.14
 ---- batch: 020 ----
mean loss: 376.91
 ---- batch: 030 ----
mean loss: 378.70
 ---- batch: 040 ----
mean loss: 391.74
 ---- batch: 050 ----
mean loss: 363.35
 ---- batch: 060 ----
mean loss: 380.93
 ---- batch: 070 ----
mean loss: 387.12
 ---- batch: 080 ----
mean loss: 365.65
 ---- batch: 090 ----
mean loss: 374.89
train mean loss: 377.92
epoch train time: 0:00:02.329979
elapsed time: 0:04:49.133884
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 21:27:07.948784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.56
 ---- batch: 020 ----
mean loss: 383.14
 ---- batch: 030 ----
mean loss: 374.85
 ---- batch: 040 ----
mean loss: 367.84
 ---- batch: 050 ----
mean loss: 378.86
 ---- batch: 060 ----
mean loss: 382.66
 ---- batch: 070 ----
mean loss: 385.86
 ---- batch: 080 ----
mean loss: 390.80
 ---- batch: 090 ----
mean loss: 377.67
train mean loss: 380.23
epoch train time: 0:00:02.323121
elapsed time: 0:04:51.457236
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 21:27:10.272146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.66
 ---- batch: 020 ----
mean loss: 371.55
 ---- batch: 030 ----
mean loss: 375.52
 ---- batch: 040 ----
mean loss: 388.21
 ---- batch: 050 ----
mean loss: 391.28
 ---- batch: 060 ----
mean loss: 370.18
 ---- batch: 070 ----
mean loss: 372.96
 ---- batch: 080 ----
mean loss: 367.91
 ---- batch: 090 ----
mean loss: 381.78
train mean loss: 376.41
epoch train time: 0:00:02.323190
elapsed time: 0:04:53.780621
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 21:27:12.595525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.81
 ---- batch: 020 ----
mean loss: 369.09
 ---- batch: 030 ----
mean loss: 366.16
 ---- batch: 040 ----
mean loss: 371.37
 ---- batch: 050 ----
mean loss: 361.41
 ---- batch: 060 ----
mean loss: 374.20
 ---- batch: 070 ----
mean loss: 374.22
 ---- batch: 080 ----
mean loss: 377.46
 ---- batch: 090 ----
mean loss: 370.77
train mean loss: 372.88
epoch train time: 0:00:02.325268
elapsed time: 0:04:56.106087
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 21:27:14.921000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.49
 ---- batch: 020 ----
mean loss: 365.96
 ---- batch: 030 ----
mean loss: 369.15
 ---- batch: 040 ----
mean loss: 392.18
 ---- batch: 050 ----
mean loss: 380.85
 ---- batch: 060 ----
mean loss: 374.53
 ---- batch: 070 ----
mean loss: 365.46
 ---- batch: 080 ----
mean loss: 365.20
 ---- batch: 090 ----
mean loss: 371.23
train mean loss: 373.37
epoch train time: 0:00:02.327971
elapsed time: 0:04:58.434270
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 21:27:17.249163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.45
 ---- batch: 020 ----
mean loss: 370.08
 ---- batch: 030 ----
mean loss: 375.62
 ---- batch: 040 ----
mean loss: 373.03
 ---- batch: 050 ----
mean loss: 359.77
 ---- batch: 060 ----
mean loss: 370.53
 ---- batch: 070 ----
mean loss: 373.76
 ---- batch: 080 ----
mean loss: 354.32
 ---- batch: 090 ----
mean loss: 365.94
train mean loss: 367.64
epoch train time: 0:00:02.321715
elapsed time: 0:05:00.756187
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 21:27:19.571081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.41
 ---- batch: 020 ----
mean loss: 377.37
 ---- batch: 030 ----
mean loss: 371.13
 ---- batch: 040 ----
mean loss: 368.24
 ---- batch: 050 ----
mean loss: 366.86
 ---- batch: 060 ----
mean loss: 376.17
 ---- batch: 070 ----
mean loss: 379.81
 ---- batch: 080 ----
mean loss: 373.31
 ---- batch: 090 ----
mean loss: 375.98
train mean loss: 373.14
epoch train time: 0:00:02.321238
elapsed time: 0:05:03.077614
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 21:27:21.892506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.00
 ---- batch: 020 ----
mean loss: 352.38
 ---- batch: 030 ----
mean loss: 363.37
 ---- batch: 040 ----
mean loss: 368.72
 ---- batch: 050 ----
mean loss: 384.53
 ---- batch: 060 ----
mean loss: 403.80
 ---- batch: 070 ----
mean loss: 393.04
 ---- batch: 080 ----
mean loss: 391.08
 ---- batch: 090 ----
mean loss: 393.52
train mean loss: 377.78
epoch train time: 0:00:02.326484
elapsed time: 0:05:05.404297
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 21:27:24.219192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.22
 ---- batch: 020 ----
mean loss: 371.90
 ---- batch: 030 ----
mean loss: 363.23
 ---- batch: 040 ----
mean loss: 362.87
 ---- batch: 050 ----
mean loss: 366.69
 ---- batch: 060 ----
mean loss: 360.62
 ---- batch: 070 ----
mean loss: 369.30
 ---- batch: 080 ----
mean loss: 380.30
 ---- batch: 090 ----
mean loss: 377.13
train mean loss: 368.99
epoch train time: 0:00:02.336458
elapsed time: 0:05:07.740940
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 21:27:26.555849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.15
 ---- batch: 020 ----
mean loss: 390.19
 ---- batch: 030 ----
mean loss: 382.05
 ---- batch: 040 ----
mean loss: 371.90
 ---- batch: 050 ----
mean loss: 371.08
 ---- batch: 060 ----
mean loss: 366.21
 ---- batch: 070 ----
mean loss: 372.82
 ---- batch: 080 ----
mean loss: 378.01
 ---- batch: 090 ----
mean loss: 359.59
train mean loss: 375.17
epoch train time: 0:00:02.337564
elapsed time: 0:05:10.078707
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 21:27:28.893636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.94
 ---- batch: 020 ----
mean loss: 362.44
 ---- batch: 030 ----
mean loss: 362.58
 ---- batch: 040 ----
mean loss: 367.28
 ---- batch: 050 ----
mean loss: 359.49
 ---- batch: 060 ----
mean loss: 366.53
 ---- batch: 070 ----
mean loss: 371.30
 ---- batch: 080 ----
mean loss: 364.46
 ---- batch: 090 ----
mean loss: 357.33
train mean loss: 364.42
epoch train time: 0:00:02.331213
elapsed time: 0:05:12.410152
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 21:27:31.225044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.84
 ---- batch: 020 ----
mean loss: 381.29
 ---- batch: 030 ----
mean loss: 384.89
 ---- batch: 040 ----
mean loss: 383.45
 ---- batch: 050 ----
mean loss: 377.68
 ---- batch: 060 ----
mean loss: 368.28
 ---- batch: 070 ----
mean loss: 358.55
 ---- batch: 080 ----
mean loss: 374.43
 ---- batch: 090 ----
mean loss: 354.97
train mean loss: 372.29
epoch train time: 0:00:02.330502
elapsed time: 0:05:14.740835
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 21:27:33.555749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.42
 ---- batch: 020 ----
mean loss: 352.20
 ---- batch: 030 ----
mean loss: 358.55
 ---- batch: 040 ----
mean loss: 377.40
 ---- batch: 050 ----
mean loss: 370.23
 ---- batch: 060 ----
mean loss: 356.42
 ---- batch: 070 ----
mean loss: 357.38
 ---- batch: 080 ----
mean loss: 383.09
 ---- batch: 090 ----
mean loss: 389.34
train mean loss: 367.96
epoch train time: 0:00:02.321911
elapsed time: 0:05:17.062985
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 21:27:35.877898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.69
 ---- batch: 020 ----
mean loss: 360.22
 ---- batch: 030 ----
mean loss: 361.23
 ---- batch: 040 ----
mean loss: 363.22
 ---- batch: 050 ----
mean loss: 368.79
 ---- batch: 060 ----
mean loss: 386.94
 ---- batch: 070 ----
mean loss: 352.53
 ---- batch: 080 ----
mean loss: 362.88
 ---- batch: 090 ----
mean loss: 360.38
train mean loss: 365.40
epoch train time: 0:00:02.328040
elapsed time: 0:05:19.391225
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 21:27:38.206123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.08
 ---- batch: 020 ----
mean loss: 369.07
 ---- batch: 030 ----
mean loss: 368.86
 ---- batch: 040 ----
mean loss: 373.54
 ---- batch: 050 ----
mean loss: 356.94
 ---- batch: 060 ----
mean loss: 363.67
 ---- batch: 070 ----
mean loss: 367.90
 ---- batch: 080 ----
mean loss: 352.59
 ---- batch: 090 ----
mean loss: 348.03
train mean loss: 364.08
epoch train time: 0:00:02.320471
elapsed time: 0:05:21.711902
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 21:27:40.526796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.37
 ---- batch: 020 ----
mean loss: 367.10
 ---- batch: 030 ----
mean loss: 357.90
 ---- batch: 040 ----
mean loss: 363.89
 ---- batch: 050 ----
mean loss: 365.92
 ---- batch: 060 ----
mean loss: 368.14
 ---- batch: 070 ----
mean loss: 353.73
 ---- batch: 080 ----
mean loss: 365.65
 ---- batch: 090 ----
mean loss: 366.14
train mean loss: 363.64
epoch train time: 0:00:02.321475
elapsed time: 0:05:24.033574
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 21:27:42.848460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.48
 ---- batch: 020 ----
mean loss: 369.28
 ---- batch: 030 ----
mean loss: 363.92
 ---- batch: 040 ----
mean loss: 361.41
 ---- batch: 050 ----
mean loss: 364.99
 ---- batch: 060 ----
mean loss: 365.50
 ---- batch: 070 ----
mean loss: 360.75
 ---- batch: 080 ----
mean loss: 370.73
 ---- batch: 090 ----
mean loss: 367.83
train mean loss: 365.21
epoch train time: 0:00:02.328563
elapsed time: 0:05:26.362319
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 21:27:45.177213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.85
 ---- batch: 020 ----
mean loss: 363.35
 ---- batch: 030 ----
mean loss: 365.50
 ---- batch: 040 ----
mean loss: 359.69
 ---- batch: 050 ----
mean loss: 363.75
 ---- batch: 060 ----
mean loss: 351.09
 ---- batch: 070 ----
mean loss: 356.35
 ---- batch: 080 ----
mean loss: 361.58
 ---- batch: 090 ----
mean loss: 380.81
train mean loss: 360.44
epoch train time: 0:00:02.328859
elapsed time: 0:05:28.691425
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 21:27:47.506306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.64
 ---- batch: 020 ----
mean loss: 368.75
 ---- batch: 030 ----
mean loss: 382.95
 ---- batch: 040 ----
mean loss: 360.51
 ---- batch: 050 ----
mean loss: 362.65
 ---- batch: 060 ----
mean loss: 361.55
 ---- batch: 070 ----
mean loss: 348.15
 ---- batch: 080 ----
mean loss: 356.63
 ---- batch: 090 ----
mean loss: 366.43
train mean loss: 364.42
epoch train time: 0:00:02.323108
elapsed time: 0:05:31.014716
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 21:27:49.829653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.46
 ---- batch: 020 ----
mean loss: 348.73
 ---- batch: 030 ----
mean loss: 347.19
 ---- batch: 040 ----
mean loss: 362.46
 ---- batch: 050 ----
mean loss: 363.04
 ---- batch: 060 ----
mean loss: 363.01
 ---- batch: 070 ----
mean loss: 366.78
 ---- batch: 080 ----
mean loss: 385.22
 ---- batch: 090 ----
mean loss: 367.39
train mean loss: 362.05
epoch train time: 0:00:02.320470
elapsed time: 0:05:33.335424
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 21:27:52.150316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.82
 ---- batch: 020 ----
mean loss: 366.79
 ---- batch: 030 ----
mean loss: 358.51
 ---- batch: 040 ----
mean loss: 350.48
 ---- batch: 050 ----
mean loss: 346.88
 ---- batch: 060 ----
mean loss: 346.51
 ---- batch: 070 ----
mean loss: 356.47
 ---- batch: 080 ----
mean loss: 373.25
 ---- batch: 090 ----
mean loss: 361.58
train mean loss: 358.59
epoch train time: 0:00:02.326494
elapsed time: 0:05:35.662121
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 21:27:54.477015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.79
 ---- batch: 020 ----
mean loss: 347.61
 ---- batch: 030 ----
mean loss: 355.86
 ---- batch: 040 ----
mean loss: 351.99
 ---- batch: 050 ----
mean loss: 361.16
 ---- batch: 060 ----
mean loss: 356.46
 ---- batch: 070 ----
mean loss: 348.24
 ---- batch: 080 ----
mean loss: 350.23
 ---- batch: 090 ----
mean loss: 365.18
train mean loss: 354.78
epoch train time: 0:00:02.330526
elapsed time: 0:05:37.992836
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 21:27:56.807742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.01
 ---- batch: 020 ----
mean loss: 353.59
 ---- batch: 030 ----
mean loss: 369.07
 ---- batch: 040 ----
mean loss: 366.70
 ---- batch: 050 ----
mean loss: 374.00
 ---- batch: 060 ----
mean loss: 369.35
 ---- batch: 070 ----
mean loss: 370.10
 ---- batch: 080 ----
mean loss: 384.84
 ---- batch: 090 ----
mean loss: 370.30
train mean loss: 368.80
epoch train time: 0:00:02.333922
elapsed time: 0:05:40.326971
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 21:27:59.141883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.78
 ---- batch: 020 ----
mean loss: 353.30
 ---- batch: 030 ----
mean loss: 355.31
 ---- batch: 040 ----
mean loss: 359.31
 ---- batch: 050 ----
mean loss: 371.20
 ---- batch: 060 ----
mean loss: 358.05
 ---- batch: 070 ----
mean loss: 342.38
 ---- batch: 080 ----
mean loss: 355.55
 ---- batch: 090 ----
mean loss: 353.28
train mean loss: 357.08
epoch train time: 0:00:02.330720
elapsed time: 0:05:42.657899
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 21:28:01.472792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.81
 ---- batch: 020 ----
mean loss: 344.21
 ---- batch: 030 ----
mean loss: 357.10
 ---- batch: 040 ----
mean loss: 372.83
 ---- batch: 050 ----
mean loss: 369.05
 ---- batch: 060 ----
mean loss: 359.72
 ---- batch: 070 ----
mean loss: 362.68
 ---- batch: 080 ----
mean loss: 373.62
 ---- batch: 090 ----
mean loss: 365.33
train mean loss: 364.57
epoch train time: 0:00:02.327151
elapsed time: 0:05:44.985250
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 21:28:03.800147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.40
 ---- batch: 020 ----
mean loss: 384.92
 ---- batch: 030 ----
mean loss: 368.90
 ---- batch: 040 ----
mean loss: 360.53
 ---- batch: 050 ----
mean loss: 356.29
 ---- batch: 060 ----
mean loss: 351.39
 ---- batch: 070 ----
mean loss: 344.29
 ---- batch: 080 ----
mean loss: 349.23
 ---- batch: 090 ----
mean loss: 367.15
train mean loss: 361.50
epoch train time: 0:00:02.317543
elapsed time: 0:05:47.303008
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 21:28:06.117900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.62
 ---- batch: 020 ----
mean loss: 361.61
 ---- batch: 030 ----
mean loss: 334.23
 ---- batch: 040 ----
mean loss: 351.84
 ---- batch: 050 ----
mean loss: 361.61
 ---- batch: 060 ----
mean loss: 357.79
 ---- batch: 070 ----
mean loss: 351.68
 ---- batch: 080 ----
mean loss: 343.39
 ---- batch: 090 ----
mean loss: 353.58
train mean loss: 355.34
epoch train time: 0:00:02.330513
elapsed time: 0:05:49.633703
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 21:28:08.448621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.52
 ---- batch: 020 ----
mean loss: 345.60
 ---- batch: 030 ----
mean loss: 350.15
 ---- batch: 040 ----
mean loss: 353.73
 ---- batch: 050 ----
mean loss: 348.71
 ---- batch: 060 ----
mean loss: 341.60
 ---- batch: 070 ----
mean loss: 346.87
 ---- batch: 080 ----
mean loss: 359.26
 ---- batch: 090 ----
mean loss: 356.67
train mean loss: 350.88
epoch train time: 0:00:02.321827
elapsed time: 0:05:51.955738
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 21:28:10.770646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.50
 ---- batch: 020 ----
mean loss: 351.46
 ---- batch: 030 ----
mean loss: 341.70
 ---- batch: 040 ----
mean loss: 358.36
 ---- batch: 050 ----
mean loss: 351.17
 ---- batch: 060 ----
mean loss: 344.71
 ---- batch: 070 ----
mean loss: 345.15
 ---- batch: 080 ----
mean loss: 351.43
 ---- batch: 090 ----
mean loss: 341.04
train mean loss: 348.46
epoch train time: 0:00:02.322220
elapsed time: 0:05:54.278159
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 21:28:13.093051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.63
 ---- batch: 020 ----
mean loss: 353.90
 ---- batch: 030 ----
mean loss: 366.61
 ---- batch: 040 ----
mean loss: 356.49
 ---- batch: 050 ----
mean loss: 346.10
 ---- batch: 060 ----
mean loss: 343.68
 ---- batch: 070 ----
mean loss: 356.65
 ---- batch: 080 ----
mean loss: 350.62
 ---- batch: 090 ----
mean loss: 362.89
train mean loss: 354.63
epoch train time: 0:00:02.324585
elapsed time: 0:05:56.602920
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 21:28:15.417814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.36
 ---- batch: 020 ----
mean loss: 356.22
 ---- batch: 030 ----
mean loss: 370.54
 ---- batch: 040 ----
mean loss: 359.34
 ---- batch: 050 ----
mean loss: 336.49
 ---- batch: 060 ----
mean loss: 343.95
 ---- batch: 070 ----
mean loss: 354.52
 ---- batch: 080 ----
mean loss: 370.62
 ---- batch: 090 ----
mean loss: 341.22
train mean loss: 354.35
epoch train time: 0:00:02.319606
elapsed time: 0:05:58.922712
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 21:28:17.737633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.29
 ---- batch: 020 ----
mean loss: 354.44
 ---- batch: 030 ----
mean loss: 341.72
 ---- batch: 040 ----
mean loss: 343.92
 ---- batch: 050 ----
mean loss: 352.63
 ---- batch: 060 ----
mean loss: 342.81
 ---- batch: 070 ----
mean loss: 344.56
 ---- batch: 080 ----
mean loss: 346.23
 ---- batch: 090 ----
mean loss: 354.26
train mean loss: 346.60
epoch train time: 0:00:02.324106
elapsed time: 0:06:01.247072
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 21:28:20.061970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.47
 ---- batch: 020 ----
mean loss: 350.18
 ---- batch: 030 ----
mean loss: 362.20
 ---- batch: 040 ----
mean loss: 368.02
 ---- batch: 050 ----
mean loss: 348.27
 ---- batch: 060 ----
mean loss: 350.83
 ---- batch: 070 ----
mean loss: 346.14
 ---- batch: 080 ----
mean loss: 344.91
 ---- batch: 090 ----
mean loss: 350.92
train mean loss: 350.94
epoch train time: 0:00:02.327371
elapsed time: 0:06:03.574631
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 21:28:22.389525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.48
 ---- batch: 020 ----
mean loss: 352.23
 ---- batch: 030 ----
mean loss: 347.03
 ---- batch: 040 ----
mean loss: 338.92
 ---- batch: 050 ----
mean loss: 351.18
 ---- batch: 060 ----
mean loss: 351.37
 ---- batch: 070 ----
mean loss: 365.68
 ---- batch: 080 ----
mean loss: 351.62
 ---- batch: 090 ----
mean loss: 345.09
train mean loss: 351.41
epoch train time: 0:00:02.321509
elapsed time: 0:06:05.896329
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 21:28:24.711246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.64
 ---- batch: 020 ----
mean loss: 361.96
 ---- batch: 030 ----
mean loss: 345.93
 ---- batch: 040 ----
mean loss: 340.01
 ---- batch: 050 ----
mean loss: 334.62
 ---- batch: 060 ----
mean loss: 349.24
 ---- batch: 070 ----
mean loss: 363.66
 ---- batch: 080 ----
mean loss: 346.14
 ---- batch: 090 ----
mean loss: 346.22
train mean loss: 347.88
epoch train time: 0:00:02.321757
elapsed time: 0:06:08.218289
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 21:28:27.033181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.19
 ---- batch: 020 ----
mean loss: 350.29
 ---- batch: 030 ----
mean loss: 359.45
 ---- batch: 040 ----
mean loss: 353.29
 ---- batch: 050 ----
mean loss: 355.96
 ---- batch: 060 ----
mean loss: 356.58
 ---- batch: 070 ----
mean loss: 348.55
 ---- batch: 080 ----
mean loss: 352.40
 ---- batch: 090 ----
mean loss: 352.69
train mean loss: 351.98
epoch train time: 0:00:02.322991
elapsed time: 0:06:10.541460
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 21:28:29.356354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.00
 ---- batch: 020 ----
mean loss: 358.77
 ---- batch: 030 ----
mean loss: 337.66
 ---- batch: 040 ----
mean loss: 349.26
 ---- batch: 050 ----
mean loss: 351.80
 ---- batch: 060 ----
mean loss: 356.94
 ---- batch: 070 ----
mean loss: 347.14
 ---- batch: 080 ----
mean loss: 365.30
 ---- batch: 090 ----
mean loss: 356.88
train mean loss: 351.24
epoch train time: 0:00:02.322608
elapsed time: 0:06:12.864258
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 21:28:31.679160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.54
 ---- batch: 020 ----
mean loss: 346.90
 ---- batch: 030 ----
mean loss: 333.25
 ---- batch: 040 ----
mean loss: 345.44
 ---- batch: 050 ----
mean loss: 337.29
 ---- batch: 060 ----
mean loss: 349.39
 ---- batch: 070 ----
mean loss: 349.94
 ---- batch: 080 ----
mean loss: 380.52
 ---- batch: 090 ----
mean loss: 359.44
train mean loss: 351.57
epoch train time: 0:00:02.335132
elapsed time: 0:06:15.199599
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 21:28:34.014498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.61
 ---- batch: 020 ----
mean loss: 358.82
 ---- batch: 030 ----
mean loss: 345.98
 ---- batch: 040 ----
mean loss: 344.69
 ---- batch: 050 ----
mean loss: 331.42
 ---- batch: 060 ----
mean loss: 350.84
 ---- batch: 070 ----
mean loss: 329.38
 ---- batch: 080 ----
mean loss: 341.88
 ---- batch: 090 ----
mean loss: 351.85
train mean loss: 344.58
epoch train time: 0:00:02.322965
elapsed time: 0:06:17.522755
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 21:28:36.337670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.81
 ---- batch: 020 ----
mean loss: 351.15
 ---- batch: 030 ----
mean loss: 336.67
 ---- batch: 040 ----
mean loss: 338.67
 ---- batch: 050 ----
mean loss: 341.76
 ---- batch: 060 ----
mean loss: 342.57
 ---- batch: 070 ----
mean loss: 352.78
 ---- batch: 080 ----
mean loss: 334.11
 ---- batch: 090 ----
mean loss: 350.76
train mean loss: 343.44
epoch train time: 0:00:02.321923
elapsed time: 0:06:19.844926
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 21:28:38.659803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.26
 ---- batch: 020 ----
mean loss: 342.06
 ---- batch: 030 ----
mean loss: 337.94
 ---- batch: 040 ----
mean loss: 349.62
 ---- batch: 050 ----
mean loss: 359.97
 ---- batch: 060 ----
mean loss: 354.43
 ---- batch: 070 ----
mean loss: 354.29
 ---- batch: 080 ----
mean loss: 355.10
 ---- batch: 090 ----
mean loss: 367.53
train mean loss: 352.72
epoch train time: 0:00:02.317389
elapsed time: 0:06:22.162473
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 21:28:40.977364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.88
 ---- batch: 020 ----
mean loss: 348.94
 ---- batch: 030 ----
mean loss: 339.17
 ---- batch: 040 ----
mean loss: 359.54
 ---- batch: 050 ----
mean loss: 335.62
 ---- batch: 060 ----
mean loss: 344.05
 ---- batch: 070 ----
mean loss: 334.92
 ---- batch: 080 ----
mean loss: 337.12
 ---- batch: 090 ----
mean loss: 337.22
train mean loss: 342.23
epoch train time: 0:00:02.321463
elapsed time: 0:06:24.484125
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 21:28:43.299036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.38
 ---- batch: 020 ----
mean loss: 341.70
 ---- batch: 030 ----
mean loss: 354.32
 ---- batch: 040 ----
mean loss: 352.79
 ---- batch: 050 ----
mean loss: 337.24
 ---- batch: 060 ----
mean loss: 356.01
 ---- batch: 070 ----
mean loss: 343.27
 ---- batch: 080 ----
mean loss: 344.75
 ---- batch: 090 ----
mean loss: 348.88
train mean loss: 345.81
epoch train time: 0:00:02.318495
elapsed time: 0:06:26.802848
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 21:28:45.617748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.10
 ---- batch: 020 ----
mean loss: 344.52
 ---- batch: 030 ----
mean loss: 351.46
 ---- batch: 040 ----
mean loss: 337.08
 ---- batch: 050 ----
mean loss: 353.32
 ---- batch: 060 ----
mean loss: 349.62
 ---- batch: 070 ----
mean loss: 360.75
 ---- batch: 080 ----
mean loss: 357.45
 ---- batch: 090 ----
mean loss: 347.74
train mean loss: 348.36
epoch train time: 0:00:02.325323
elapsed time: 0:06:29.128393
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 21:28:47.943286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.15
 ---- batch: 020 ----
mean loss: 336.52
 ---- batch: 030 ----
mean loss: 335.00
 ---- batch: 040 ----
mean loss: 341.96
 ---- batch: 050 ----
mean loss: 345.32
 ---- batch: 060 ----
mean loss: 326.70
 ---- batch: 070 ----
mean loss: 354.26
 ---- batch: 080 ----
mean loss: 347.99
 ---- batch: 090 ----
mean loss: 347.04
train mean loss: 343.23
epoch train time: 0:00:02.328464
elapsed time: 0:06:31.457040
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 21:28:50.271941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.82
 ---- batch: 020 ----
mean loss: 325.99
 ---- batch: 030 ----
mean loss: 337.23
 ---- batch: 040 ----
mean loss: 340.51
 ---- batch: 050 ----
mean loss: 344.81
 ---- batch: 060 ----
mean loss: 344.49
 ---- batch: 070 ----
mean loss: 352.97
 ---- batch: 080 ----
mean loss: 342.54
 ---- batch: 090 ----
mean loss: 340.89
train mean loss: 339.99
epoch train time: 0:00:02.324597
elapsed time: 0:06:33.781869
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 21:28:52.596762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.96
 ---- batch: 020 ----
mean loss: 344.86
 ---- batch: 030 ----
mean loss: 349.20
 ---- batch: 040 ----
mean loss: 344.58
 ---- batch: 050 ----
mean loss: 367.32
 ---- batch: 060 ----
mean loss: 361.75
 ---- batch: 070 ----
mean loss: 330.24
 ---- batch: 080 ----
mean loss: 324.19
 ---- batch: 090 ----
mean loss: 330.84
train mean loss: 344.28
epoch train time: 0:00:02.325386
elapsed time: 0:06:36.107443
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 21:28:54.922340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.66
 ---- batch: 020 ----
mean loss: 345.74
 ---- batch: 030 ----
mean loss: 333.57
 ---- batch: 040 ----
mean loss: 346.92
 ---- batch: 050 ----
mean loss: 325.13
 ---- batch: 060 ----
mean loss: 340.26
 ---- batch: 070 ----
mean loss: 352.69
 ---- batch: 080 ----
mean loss: 352.40
 ---- batch: 090 ----
mean loss: 336.20
train mean loss: 341.01
epoch train time: 0:00:02.324243
elapsed time: 0:06:38.431872
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 21:28:57.246765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.39
 ---- batch: 020 ----
mean loss: 342.26
 ---- batch: 030 ----
mean loss: 345.56
 ---- batch: 040 ----
mean loss: 350.55
 ---- batch: 050 ----
mean loss: 344.58
 ---- batch: 060 ----
mean loss: 333.89
 ---- batch: 070 ----
mean loss: 336.52
 ---- batch: 080 ----
mean loss: 344.46
 ---- batch: 090 ----
mean loss: 346.59
train mean loss: 342.04
epoch train time: 0:00:02.321287
elapsed time: 0:06:40.753341
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 21:28:59.568258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.80
 ---- batch: 020 ----
mean loss: 341.43
 ---- batch: 030 ----
mean loss: 354.20
 ---- batch: 040 ----
mean loss: 344.44
 ---- batch: 050 ----
mean loss: 344.08
 ---- batch: 060 ----
mean loss: 341.35
 ---- batch: 070 ----
mean loss: 336.19
 ---- batch: 080 ----
mean loss: 349.47
 ---- batch: 090 ----
mean loss: 344.89
train mean loss: 345.18
epoch train time: 0:00:02.321850
elapsed time: 0:06:43.075406
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 21:29:01.890302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.95
 ---- batch: 020 ----
mean loss: 333.15
 ---- batch: 030 ----
mean loss: 338.71
 ---- batch: 040 ----
mean loss: 351.27
 ---- batch: 050 ----
mean loss: 345.83
 ---- batch: 060 ----
mean loss: 337.71
 ---- batch: 070 ----
mean loss: 338.95
 ---- batch: 080 ----
mean loss: 338.97
 ---- batch: 090 ----
mean loss: 347.43
train mean loss: 340.71
epoch train time: 0:00:02.323540
elapsed time: 0:06:45.399151
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 21:29:04.214085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.52
 ---- batch: 020 ----
mean loss: 342.48
 ---- batch: 030 ----
mean loss: 337.14
 ---- batch: 040 ----
mean loss: 338.51
 ---- batch: 050 ----
mean loss: 336.09
 ---- batch: 060 ----
mean loss: 333.10
 ---- batch: 070 ----
mean loss: 346.69
 ---- batch: 080 ----
mean loss: 340.62
 ---- batch: 090 ----
mean loss: 342.06
train mean loss: 340.62
epoch train time: 0:00:02.331909
elapsed time: 0:06:47.731273
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 21:29:06.546165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.45
 ---- batch: 020 ----
mean loss: 333.79
 ---- batch: 030 ----
mean loss: 335.63
 ---- batch: 040 ----
mean loss: 338.80
 ---- batch: 050 ----
mean loss: 337.04
 ---- batch: 060 ----
mean loss: 339.75
 ---- batch: 070 ----
mean loss: 351.47
 ---- batch: 080 ----
mean loss: 330.17
 ---- batch: 090 ----
mean loss: 335.31
train mean loss: 338.40
epoch train time: 0:00:02.322175
elapsed time: 0:06:50.053631
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 21:29:08.868523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.66
 ---- batch: 020 ----
mean loss: 354.07
 ---- batch: 030 ----
mean loss: 340.95
 ---- batch: 040 ----
mean loss: 344.64
 ---- batch: 050 ----
mean loss: 339.83
 ---- batch: 060 ----
mean loss: 342.74
 ---- batch: 070 ----
mean loss: 346.32
 ---- batch: 080 ----
mean loss: 351.94
 ---- batch: 090 ----
mean loss: 337.06
train mean loss: 343.00
epoch train time: 0:00:02.322158
elapsed time: 0:06:52.375963
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 21:29:11.190858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.97
 ---- batch: 020 ----
mean loss: 338.40
 ---- batch: 030 ----
mean loss: 351.80
 ---- batch: 040 ----
mean loss: 351.41
 ---- batch: 050 ----
mean loss: 339.59
 ---- batch: 060 ----
mean loss: 358.22
 ---- batch: 070 ----
mean loss: 353.01
 ---- batch: 080 ----
mean loss: 344.51
 ---- batch: 090 ----
mean loss: 340.76
train mean loss: 345.74
epoch train time: 0:00:02.327211
elapsed time: 0:06:54.703360
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 21:29:13.518253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.21
 ---- batch: 020 ----
mean loss: 333.74
 ---- batch: 030 ----
mean loss: 340.33
 ---- batch: 040 ----
mean loss: 338.73
 ---- batch: 050 ----
mean loss: 341.17
 ---- batch: 060 ----
mean loss: 338.32
 ---- batch: 070 ----
mean loss: 335.12
 ---- batch: 080 ----
mean loss: 339.42
 ---- batch: 090 ----
mean loss: 348.60
train mean loss: 338.48
epoch train time: 0:00:02.325806
elapsed time: 0:06:57.029344
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 21:29:15.844240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.54
 ---- batch: 020 ----
mean loss: 350.49
 ---- batch: 030 ----
mean loss: 343.58
 ---- batch: 040 ----
mean loss: 346.43
 ---- batch: 050 ----
mean loss: 342.21
 ---- batch: 060 ----
mean loss: 341.76
 ---- batch: 070 ----
mean loss: 335.93
 ---- batch: 080 ----
mean loss: 335.10
 ---- batch: 090 ----
mean loss: 346.12
train mean loss: 342.15
epoch train time: 0:00:02.320039
elapsed time: 0:06:59.349565
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 21:29:18.164484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.47
 ---- batch: 020 ----
mean loss: 343.96
 ---- batch: 030 ----
mean loss: 342.13
 ---- batch: 040 ----
mean loss: 352.59
 ---- batch: 050 ----
mean loss: 354.47
 ---- batch: 060 ----
mean loss: 342.76
 ---- batch: 070 ----
mean loss: 346.39
 ---- batch: 080 ----
mean loss: 338.13
 ---- batch: 090 ----
mean loss: 349.26
train mean loss: 346.68
epoch train time: 0:00:02.326351
elapsed time: 0:07:01.676123
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 21:29:20.491017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.21
 ---- batch: 020 ----
mean loss: 337.21
 ---- batch: 030 ----
mean loss: 336.81
 ---- batch: 040 ----
mean loss: 336.04
 ---- batch: 050 ----
mean loss: 333.05
 ---- batch: 060 ----
mean loss: 342.35
 ---- batch: 070 ----
mean loss: 348.82
 ---- batch: 080 ----
mean loss: 336.05
 ---- batch: 090 ----
mean loss: 347.16
train mean loss: 338.69
epoch train time: 0:00:02.329847
elapsed time: 0:07:04.006161
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 21:29:22.821052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.06
 ---- batch: 020 ----
mean loss: 324.73
 ---- batch: 030 ----
mean loss: 324.61
 ---- batch: 040 ----
mean loss: 347.16
 ---- batch: 050 ----
mean loss: 332.24
 ---- batch: 060 ----
mean loss: 350.70
 ---- batch: 070 ----
mean loss: 343.08
 ---- batch: 080 ----
mean loss: 349.07
 ---- batch: 090 ----
mean loss: 337.58
train mean loss: 339.18
epoch train time: 0:00:02.327808
elapsed time: 0:07:06.334150
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 21:29:25.149048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.38
 ---- batch: 020 ----
mean loss: 335.91
 ---- batch: 030 ----
mean loss: 331.02
 ---- batch: 040 ----
mean loss: 333.22
 ---- batch: 050 ----
mean loss: 339.58
 ---- batch: 060 ----
mean loss: 346.59
 ---- batch: 070 ----
mean loss: 361.47
 ---- batch: 080 ----
mean loss: 348.86
 ---- batch: 090 ----
mean loss: 324.05
train mean loss: 341.50
epoch train time: 0:00:02.320371
elapsed time: 0:07:08.654711
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 21:29:27.469658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.92
 ---- batch: 020 ----
mean loss: 353.03
 ---- batch: 030 ----
mean loss: 351.37
 ---- batch: 040 ----
mean loss: 347.74
 ---- batch: 050 ----
mean loss: 351.43
 ---- batch: 060 ----
mean loss: 344.76
 ---- batch: 070 ----
mean loss: 345.53
 ---- batch: 080 ----
mean loss: 333.70
 ---- batch: 090 ----
mean loss: 331.64
train mean loss: 345.46
epoch train time: 0:00:02.330022
elapsed time: 0:07:10.985041
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 21:29:29.799964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.78
 ---- batch: 020 ----
mean loss: 361.77
 ---- batch: 030 ----
mean loss: 339.05
 ---- batch: 040 ----
mean loss: 332.66
 ---- batch: 050 ----
mean loss: 336.13
 ---- batch: 060 ----
mean loss: 345.09
 ---- batch: 070 ----
mean loss: 354.09
 ---- batch: 080 ----
mean loss: 332.41
 ---- batch: 090 ----
mean loss: 352.39
train mean loss: 343.96
epoch train time: 0:00:02.328334
elapsed time: 0:07:13.313588
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 21:29:32.128501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.97
 ---- batch: 020 ----
mean loss: 335.71
 ---- batch: 030 ----
mean loss: 339.94
 ---- batch: 040 ----
mean loss: 345.88
 ---- batch: 050 ----
mean loss: 343.89
 ---- batch: 060 ----
mean loss: 336.18
 ---- batch: 070 ----
mean loss: 338.31
 ---- batch: 080 ----
mean loss: 334.86
 ---- batch: 090 ----
mean loss: 328.95
train mean loss: 337.78
epoch train time: 0:00:02.327006
elapsed time: 0:07:15.640788
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 21:29:34.455703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.25
 ---- batch: 020 ----
mean loss: 347.29
 ---- batch: 030 ----
mean loss: 350.80
 ---- batch: 040 ----
mean loss: 335.07
 ---- batch: 050 ----
mean loss: 332.54
 ---- batch: 060 ----
mean loss: 355.59
 ---- batch: 070 ----
mean loss: 335.55
 ---- batch: 080 ----
mean loss: 342.43
 ---- batch: 090 ----
mean loss: 335.38
train mean loss: 343.32
epoch train time: 0:00:02.328173
elapsed time: 0:07:17.969190
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 21:29:36.784068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.90
 ---- batch: 020 ----
mean loss: 356.29
 ---- batch: 030 ----
mean loss: 350.57
 ---- batch: 040 ----
mean loss: 338.55
 ---- batch: 050 ----
mean loss: 335.63
 ---- batch: 060 ----
mean loss: 352.01
 ---- batch: 070 ----
mean loss: 336.45
 ---- batch: 080 ----
mean loss: 343.64
 ---- batch: 090 ----
mean loss: 335.09
train mean loss: 344.91
epoch train time: 0:00:02.324359
elapsed time: 0:07:20.293727
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 21:29:39.108625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.84
 ---- batch: 020 ----
mean loss: 340.33
 ---- batch: 030 ----
mean loss: 331.61
 ---- batch: 040 ----
mean loss: 334.98
 ---- batch: 050 ----
mean loss: 323.95
 ---- batch: 060 ----
mean loss: 342.18
 ---- batch: 070 ----
mean loss: 344.44
 ---- batch: 080 ----
mean loss: 333.23
 ---- batch: 090 ----
mean loss: 355.16
train mean loss: 337.42
epoch train time: 0:00:02.325118
elapsed time: 0:07:22.619084
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 21:29:41.433979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.12
 ---- batch: 020 ----
mean loss: 332.85
 ---- batch: 030 ----
mean loss: 334.67
 ---- batch: 040 ----
mean loss: 341.49
 ---- batch: 050 ----
mean loss: 342.37
 ---- batch: 060 ----
mean loss: 335.36
 ---- batch: 070 ----
mean loss: 335.48
 ---- batch: 080 ----
mean loss: 321.00
 ---- batch: 090 ----
mean loss: 337.70
train mean loss: 333.44
epoch train time: 0:00:02.329589
elapsed time: 0:07:24.948862
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 21:29:43.763754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.62
 ---- batch: 020 ----
mean loss: 338.42
 ---- batch: 030 ----
mean loss: 324.38
 ---- batch: 040 ----
mean loss: 330.48
 ---- batch: 050 ----
mean loss: 323.17
 ---- batch: 060 ----
mean loss: 335.84
 ---- batch: 070 ----
mean loss: 344.04
 ---- batch: 080 ----
mean loss: 330.10
 ---- batch: 090 ----
mean loss: 339.86
train mean loss: 333.83
epoch train time: 0:00:02.324371
elapsed time: 0:07:27.273415
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 21:29:46.088310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.83
 ---- batch: 020 ----
mean loss: 336.74
 ---- batch: 030 ----
mean loss: 334.94
 ---- batch: 040 ----
mean loss: 334.31
 ---- batch: 050 ----
mean loss: 331.54
 ---- batch: 060 ----
mean loss: 331.32
 ---- batch: 070 ----
mean loss: 327.51
 ---- batch: 080 ----
mean loss: 333.30
 ---- batch: 090 ----
mean loss: 330.86
train mean loss: 334.53
epoch train time: 0:00:02.321861
elapsed time: 0:07:29.595478
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 21:29:48.410360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.73
 ---- batch: 020 ----
mean loss: 337.05
 ---- batch: 030 ----
mean loss: 329.14
 ---- batch: 040 ----
mean loss: 324.56
 ---- batch: 050 ----
mean loss: 338.24
 ---- batch: 060 ----
mean loss: 333.92
 ---- batch: 070 ----
mean loss: 324.20
 ---- batch: 080 ----
mean loss: 346.26
 ---- batch: 090 ----
mean loss: 340.51
train mean loss: 333.35
epoch train time: 0:00:02.333663
elapsed time: 0:07:31.929308
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 21:29:50.744204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.54
 ---- batch: 020 ----
mean loss: 328.93
 ---- batch: 030 ----
mean loss: 331.50
 ---- batch: 040 ----
mean loss: 335.58
 ---- batch: 050 ----
mean loss: 356.00
 ---- batch: 060 ----
mean loss: 345.25
 ---- batch: 070 ----
mean loss: 339.31
 ---- batch: 080 ----
mean loss: 335.16
 ---- batch: 090 ----
mean loss: 330.37
train mean loss: 338.11
epoch train time: 0:00:02.329716
elapsed time: 0:07:34.259229
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 21:29:53.074139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.78
 ---- batch: 020 ----
mean loss: 333.01
 ---- batch: 030 ----
mean loss: 341.74
 ---- batch: 040 ----
mean loss: 337.69
 ---- batch: 050 ----
mean loss: 330.77
 ---- batch: 060 ----
mean loss: 322.04
 ---- batch: 070 ----
mean loss: 334.97
 ---- batch: 080 ----
mean loss: 347.27
 ---- batch: 090 ----
mean loss: 329.13
train mean loss: 333.31
epoch train time: 0:00:02.324289
elapsed time: 0:07:36.583727
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 21:29:55.398652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.69
 ---- batch: 020 ----
mean loss: 344.52
 ---- batch: 030 ----
mean loss: 333.19
 ---- batch: 040 ----
mean loss: 329.64
 ---- batch: 050 ----
mean loss: 319.52
 ---- batch: 060 ----
mean loss: 342.19
 ---- batch: 070 ----
mean loss: 334.29
 ---- batch: 080 ----
mean loss: 341.05
 ---- batch: 090 ----
mean loss: 331.36
train mean loss: 336.06
epoch train time: 0:00:02.320111
elapsed time: 0:07:38.904090
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 21:29:57.718996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.75
 ---- batch: 020 ----
mean loss: 336.55
 ---- batch: 030 ----
mean loss: 323.34
 ---- batch: 040 ----
mean loss: 333.31
 ---- batch: 050 ----
mean loss: 335.93
 ---- batch: 060 ----
mean loss: 348.13
 ---- batch: 070 ----
mean loss: 330.68
 ---- batch: 080 ----
mean loss: 331.87
 ---- batch: 090 ----
mean loss: 340.07
train mean loss: 338.75
epoch train time: 0:00:02.322944
elapsed time: 0:07:41.227247
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 21:30:00.042147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.50
 ---- batch: 020 ----
mean loss: 344.32
 ---- batch: 030 ----
mean loss: 339.20
 ---- batch: 040 ----
mean loss: 333.05
 ---- batch: 050 ----
mean loss: 344.93
 ---- batch: 060 ----
mean loss: 353.31
 ---- batch: 070 ----
mean loss: 331.45
 ---- batch: 080 ----
mean loss: 327.58
 ---- batch: 090 ----
mean loss: 340.93
train mean loss: 338.08
epoch train time: 0:00:02.329438
elapsed time: 0:07:43.556904
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 21:30:02.371801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.96
 ---- batch: 020 ----
mean loss: 323.02
 ---- batch: 030 ----
mean loss: 325.22
 ---- batch: 040 ----
mean loss: 335.96
 ---- batch: 050 ----
mean loss: 339.50
 ---- batch: 060 ----
mean loss: 331.39
 ---- batch: 070 ----
mean loss: 331.31
 ---- batch: 080 ----
mean loss: 338.40
 ---- batch: 090 ----
mean loss: 342.53
train mean loss: 332.60
epoch train time: 0:00:02.323115
elapsed time: 0:07:45.880206
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 21:30:04.695100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.97
 ---- batch: 020 ----
mean loss: 335.02
 ---- batch: 030 ----
mean loss: 339.50
 ---- batch: 040 ----
mean loss: 339.88
 ---- batch: 050 ----
mean loss: 325.39
 ---- batch: 060 ----
mean loss: 340.36
 ---- batch: 070 ----
mean loss: 332.28
 ---- batch: 080 ----
mean loss: 328.15
 ---- batch: 090 ----
mean loss: 334.83
train mean loss: 334.01
epoch train time: 0:00:02.318020
elapsed time: 0:07:48.198418
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 21:30:07.013308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.48
 ---- batch: 020 ----
mean loss: 336.69
 ---- batch: 030 ----
mean loss: 329.50
 ---- batch: 040 ----
mean loss: 339.84
 ---- batch: 050 ----
mean loss: 332.09
 ---- batch: 060 ----
mean loss: 333.48
 ---- batch: 070 ----
mean loss: 336.02
 ---- batch: 080 ----
mean loss: 338.56
 ---- batch: 090 ----
mean loss: 332.53
train mean loss: 334.43
epoch train time: 0:00:02.318440
elapsed time: 0:07:50.517046
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 21:30:09.331957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.90
 ---- batch: 020 ----
mean loss: 367.87
 ---- batch: 030 ----
mean loss: 356.62
 ---- batch: 040 ----
mean loss: 337.91
 ---- batch: 050 ----
mean loss: 340.21
 ---- batch: 060 ----
mean loss: 329.94
 ---- batch: 070 ----
mean loss: 332.70
 ---- batch: 080 ----
mean loss: 334.62
 ---- batch: 090 ----
mean loss: 329.55
train mean loss: 340.31
epoch train time: 0:00:02.331209
elapsed time: 0:07:52.848455
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 21:30:11.663351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.36
 ---- batch: 020 ----
mean loss: 325.99
 ---- batch: 030 ----
mean loss: 346.01
 ---- batch: 040 ----
mean loss: 329.82
 ---- batch: 050 ----
mean loss: 332.58
 ---- batch: 060 ----
mean loss: 322.46
 ---- batch: 070 ----
mean loss: 344.62
 ---- batch: 080 ----
mean loss: 359.41
 ---- batch: 090 ----
mean loss: 352.95
train mean loss: 339.73
epoch train time: 0:00:02.326300
elapsed time: 0:07:55.174962
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 21:30:13.989853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.95
 ---- batch: 020 ----
mean loss: 338.36
 ---- batch: 030 ----
mean loss: 327.88
 ---- batch: 040 ----
mean loss: 338.49
 ---- batch: 050 ----
mean loss: 332.24
 ---- batch: 060 ----
mean loss: 322.32
 ---- batch: 070 ----
mean loss: 335.00
 ---- batch: 080 ----
mean loss: 336.85
 ---- batch: 090 ----
mean loss: 320.06
train mean loss: 330.02
epoch train time: 0:00:02.329847
elapsed time: 0:07:57.505042
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 21:30:16.319937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.57
 ---- batch: 020 ----
mean loss: 335.51
 ---- batch: 030 ----
mean loss: 337.73
 ---- batch: 040 ----
mean loss: 326.68
 ---- batch: 050 ----
mean loss: 341.85
 ---- batch: 060 ----
mean loss: 328.70
 ---- batch: 070 ----
mean loss: 338.32
 ---- batch: 080 ----
mean loss: 329.03
 ---- batch: 090 ----
mean loss: 323.68
train mean loss: 332.37
epoch train time: 0:00:02.333092
elapsed time: 0:07:59.838325
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 21:30:18.653230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.79
 ---- batch: 020 ----
mean loss: 329.75
 ---- batch: 030 ----
mean loss: 335.33
 ---- batch: 040 ----
mean loss: 332.15
 ---- batch: 050 ----
mean loss: 333.70
 ---- batch: 060 ----
mean loss: 344.91
 ---- batch: 070 ----
mean loss: 332.87
 ---- batch: 080 ----
mean loss: 330.77
 ---- batch: 090 ----
mean loss: 327.25
train mean loss: 332.83
epoch train time: 0:00:02.326030
elapsed time: 0:08:02.164590
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 21:30:20.979485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.99
 ---- batch: 020 ----
mean loss: 335.63
 ---- batch: 030 ----
mean loss: 329.70
 ---- batch: 040 ----
mean loss: 327.76
 ---- batch: 050 ----
mean loss: 324.55
 ---- batch: 060 ----
mean loss: 320.10
 ---- batch: 070 ----
mean loss: 330.97
 ---- batch: 080 ----
mean loss: 336.15
 ---- batch: 090 ----
mean loss: 333.72
train mean loss: 332.22
epoch train time: 0:00:02.333886
elapsed time: 0:08:04.498660
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 21:30:23.313552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.77
 ---- batch: 020 ----
mean loss: 339.25
 ---- batch: 030 ----
mean loss: 336.02
 ---- batch: 040 ----
mean loss: 329.89
 ---- batch: 050 ----
mean loss: 333.32
 ---- batch: 060 ----
mean loss: 331.96
 ---- batch: 070 ----
mean loss: 339.23
 ---- batch: 080 ----
mean loss: 326.55
 ---- batch: 090 ----
mean loss: 321.73
train mean loss: 331.98
epoch train time: 0:00:02.327601
elapsed time: 0:08:06.826448
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 21:30:25.641343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.90
 ---- batch: 020 ----
mean loss: 329.57
 ---- batch: 030 ----
mean loss: 343.36
 ---- batch: 040 ----
mean loss: 332.36
 ---- batch: 050 ----
mean loss: 340.90
 ---- batch: 060 ----
mean loss: 350.38
 ---- batch: 070 ----
mean loss: 344.00
 ---- batch: 080 ----
mean loss: 347.91
 ---- batch: 090 ----
mean loss: 334.12
train mean loss: 338.35
epoch train time: 0:00:02.325794
elapsed time: 0:08:09.152423
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 21:30:27.967340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.92
 ---- batch: 020 ----
mean loss: 335.88
 ---- batch: 030 ----
mean loss: 335.42
 ---- batch: 040 ----
mean loss: 335.55
 ---- batch: 050 ----
mean loss: 349.99
 ---- batch: 060 ----
mean loss: 362.58
 ---- batch: 070 ----
mean loss: 361.83
 ---- batch: 080 ----
mean loss: 357.81
 ---- batch: 090 ----
mean loss: 335.10
train mean loss: 344.42
epoch train time: 0:00:02.324751
elapsed time: 0:08:11.477376
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 21:30:30.292268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.39
 ---- batch: 020 ----
mean loss: 338.46
 ---- batch: 030 ----
mean loss: 326.37
 ---- batch: 040 ----
mean loss: 328.96
 ---- batch: 050 ----
mean loss: 331.25
 ---- batch: 060 ----
mean loss: 334.72
 ---- batch: 070 ----
mean loss: 331.96
 ---- batch: 080 ----
mean loss: 326.43
 ---- batch: 090 ----
mean loss: 354.54
train mean loss: 333.45
epoch train time: 0:00:02.333712
elapsed time: 0:08:13.811284
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 21:30:32.626185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.67
 ---- batch: 020 ----
mean loss: 329.68
 ---- batch: 030 ----
mean loss: 324.97
 ---- batch: 040 ----
mean loss: 338.78
 ---- batch: 050 ----
mean loss: 335.10
 ---- batch: 060 ----
mean loss: 330.09
 ---- batch: 070 ----
mean loss: 336.29
 ---- batch: 080 ----
mean loss: 349.31
 ---- batch: 090 ----
mean loss: 320.84
train mean loss: 332.16
epoch train time: 0:00:02.326607
elapsed time: 0:08:16.138086
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 21:30:34.952982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.74
 ---- batch: 020 ----
mean loss: 322.67
 ---- batch: 030 ----
mean loss: 346.79
 ---- batch: 040 ----
mean loss: 336.65
 ---- batch: 050 ----
mean loss: 330.22
 ---- batch: 060 ----
mean loss: 335.08
 ---- batch: 070 ----
mean loss: 329.97
 ---- batch: 080 ----
mean loss: 333.69
 ---- batch: 090 ----
mean loss: 334.55
train mean loss: 333.84
epoch train time: 0:00:02.328490
elapsed time: 0:08:18.466748
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 21:30:37.281660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.11
 ---- batch: 020 ----
mean loss: 324.61
 ---- batch: 030 ----
mean loss: 331.19
 ---- batch: 040 ----
mean loss: 327.86
 ---- batch: 050 ----
mean loss: 339.04
 ---- batch: 060 ----
mean loss: 320.34
 ---- batch: 070 ----
mean loss: 335.44
 ---- batch: 080 ----
mean loss: 351.08
 ---- batch: 090 ----
mean loss: 343.21
train mean loss: 335.39
epoch train time: 0:00:02.330794
elapsed time: 0:08:20.797738
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 21:30:39.612631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.05
 ---- batch: 020 ----
mean loss: 319.14
 ---- batch: 030 ----
mean loss: 325.55
 ---- batch: 040 ----
mean loss: 331.39
 ---- batch: 050 ----
mean loss: 315.17
 ---- batch: 060 ----
mean loss: 321.50
 ---- batch: 070 ----
mean loss: 320.44
 ---- batch: 080 ----
mean loss: 325.32
 ---- batch: 090 ----
mean loss: 324.78
train mean loss: 323.83
epoch train time: 0:00:02.323669
elapsed time: 0:08:23.121615
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 21:30:41.936500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.73
 ---- batch: 020 ----
mean loss: 331.21
 ---- batch: 030 ----
mean loss: 332.56
 ---- batch: 040 ----
mean loss: 316.65
 ---- batch: 050 ----
mean loss: 330.76
 ---- batch: 060 ----
mean loss: 316.13
 ---- batch: 070 ----
mean loss: 324.16
 ---- batch: 080 ----
mean loss: 314.20
 ---- batch: 090 ----
mean loss: 315.45
train mean loss: 323.13
epoch train time: 0:00:02.330007
elapsed time: 0:08:25.451794
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 21:30:44.266703
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 328.25
 ---- batch: 020 ----
mean loss: 321.05
 ---- batch: 030 ----
mean loss: 313.63
 ---- batch: 040 ----
mean loss: 316.83
 ---- batch: 050 ----
mean loss: 325.54
 ---- batch: 060 ----
mean loss: 318.47
 ---- batch: 070 ----
mean loss: 327.98
 ---- batch: 080 ----
mean loss: 332.35
 ---- batch: 090 ----
mean loss: 311.79
train mean loss: 320.88
epoch train time: 0:00:02.322824
elapsed time: 0:08:27.774846
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 21:30:46.589787
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.01
 ---- batch: 020 ----
mean loss: 333.10
 ---- batch: 030 ----
mean loss: 322.67
 ---- batch: 040 ----
mean loss: 317.98
 ---- batch: 050 ----
mean loss: 320.54
 ---- batch: 060 ----
mean loss: 327.38
 ---- batch: 070 ----
mean loss: 318.21
 ---- batch: 080 ----
mean loss: 330.29
 ---- batch: 090 ----
mean loss: 323.72
train mean loss: 323.76
epoch train time: 0:00:02.325098
elapsed time: 0:08:30.100174
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 21:30:48.915066
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.06
 ---- batch: 020 ----
mean loss: 331.30
 ---- batch: 030 ----
mean loss: 336.69
 ---- batch: 040 ----
mean loss: 321.43
 ---- batch: 050 ----
mean loss: 323.85
 ---- batch: 060 ----
mean loss: 316.47
 ---- batch: 070 ----
mean loss: 310.96
 ---- batch: 080 ----
mean loss: 335.77
 ---- batch: 090 ----
mean loss: 326.10
train mean loss: 324.02
epoch train time: 0:00:02.331418
elapsed time: 0:08:32.431776
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 21:30:51.246671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.24
 ---- batch: 020 ----
mean loss: 322.10
 ---- batch: 030 ----
mean loss: 315.35
 ---- batch: 040 ----
mean loss: 330.09
 ---- batch: 050 ----
mean loss: 326.67
 ---- batch: 060 ----
mean loss: 329.66
 ---- batch: 070 ----
mean loss: 329.45
 ---- batch: 080 ----
mean loss: 333.01
 ---- batch: 090 ----
mean loss: 316.04
train mean loss: 324.49
epoch train time: 0:00:02.327488
elapsed time: 0:08:34.759442
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 21:30:53.574367
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.40
 ---- batch: 020 ----
mean loss: 308.09
 ---- batch: 030 ----
mean loss: 324.09
 ---- batch: 040 ----
mean loss: 325.01
 ---- batch: 050 ----
mean loss: 321.53
 ---- batch: 060 ----
mean loss: 320.95
 ---- batch: 070 ----
mean loss: 320.96
 ---- batch: 080 ----
mean loss: 322.65
 ---- batch: 090 ----
mean loss: 310.67
train mean loss: 319.96
epoch train time: 0:00:02.324753
elapsed time: 0:08:37.084420
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 21:30:55.899312
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 314.53
 ---- batch: 020 ----
mean loss: 321.24
 ---- batch: 030 ----
mean loss: 321.87
 ---- batch: 040 ----
mean loss: 329.67
 ---- batch: 050 ----
mean loss: 326.50
 ---- batch: 060 ----
mean loss: 322.20
 ---- batch: 070 ----
mean loss: 324.28
 ---- batch: 080 ----
mean loss: 317.65
 ---- batch: 090 ----
mean loss: 322.63
train mean loss: 322.42
epoch train time: 0:00:02.334639
elapsed time: 0:08:39.419253
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 21:30:58.234150
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 312.02
 ---- batch: 020 ----
mean loss: 323.92
 ---- batch: 030 ----
mean loss: 328.01
 ---- batch: 040 ----
mean loss: 310.61
 ---- batch: 050 ----
mean loss: 321.08
 ---- batch: 060 ----
mean loss: 318.53
 ---- batch: 070 ----
mean loss: 331.76
 ---- batch: 080 ----
mean loss: 321.70
 ---- batch: 090 ----
mean loss: 323.51
train mean loss: 321.62
epoch train time: 0:00:02.336896
elapsed time: 0:08:41.756346
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 21:31:00.571237
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 336.77
 ---- batch: 020 ----
mean loss: 318.00
 ---- batch: 030 ----
mean loss: 320.34
 ---- batch: 040 ----
mean loss: 317.10
 ---- batch: 050 ----
mean loss: 325.02
 ---- batch: 060 ----
mean loss: 326.30
 ---- batch: 070 ----
mean loss: 318.05
 ---- batch: 080 ----
mean loss: 324.02
 ---- batch: 090 ----
mean loss: 318.62
train mean loss: 321.13
epoch train time: 0:00:02.327648
elapsed time: 0:08:44.084204
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 21:31:02.899135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 332.15
 ---- batch: 020 ----
mean loss: 320.28
 ---- batch: 030 ----
mean loss: 322.24
 ---- batch: 040 ----
mean loss: 322.28
 ---- batch: 050 ----
mean loss: 314.70
 ---- batch: 060 ----
mean loss: 324.70
 ---- batch: 070 ----
mean loss: 323.84
 ---- batch: 080 ----
mean loss: 321.70
 ---- batch: 090 ----
mean loss: 319.09
train mean loss: 322.09
epoch train time: 0:00:02.333185
elapsed time: 0:08:46.417607
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 21:31:05.232500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.84
 ---- batch: 020 ----
mean loss: 317.97
 ---- batch: 030 ----
mean loss: 322.71
 ---- batch: 040 ----
mean loss: 331.78
 ---- batch: 050 ----
mean loss: 320.60
 ---- batch: 060 ----
mean loss: 309.62
 ---- batch: 070 ----
mean loss: 321.01
 ---- batch: 080 ----
mean loss: 323.07
 ---- batch: 090 ----
mean loss: 321.21
train mean loss: 321.48
epoch train time: 0:00:02.329682
elapsed time: 0:08:48.747498
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 21:31:07.562395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.86
 ---- batch: 020 ----
mean loss: 324.62
 ---- batch: 030 ----
mean loss: 326.33
 ---- batch: 040 ----
mean loss: 321.27
 ---- batch: 050 ----
mean loss: 326.15
 ---- batch: 060 ----
mean loss: 324.54
 ---- batch: 070 ----
mean loss: 323.87
 ---- batch: 080 ----
mean loss: 323.48
 ---- batch: 090 ----
mean loss: 320.42
train mean loss: 322.53
epoch train time: 0:00:02.334648
elapsed time: 0:08:51.082340
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 21:31:09.897234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.16
 ---- batch: 020 ----
mean loss: 312.48
 ---- batch: 030 ----
mean loss: 316.80
 ---- batch: 040 ----
mean loss: 326.01
 ---- batch: 050 ----
mean loss: 322.32
 ---- batch: 060 ----
mean loss: 314.26
 ---- batch: 070 ----
mean loss: 328.25
 ---- batch: 080 ----
mean loss: 329.19
 ---- batch: 090 ----
mean loss: 322.78
train mean loss: 321.29
epoch train time: 0:00:02.329723
elapsed time: 0:08:53.412286
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 21:31:12.227179
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.36
 ---- batch: 020 ----
mean loss: 321.58
 ---- batch: 030 ----
mean loss: 312.46
 ---- batch: 040 ----
mean loss: 315.75
 ---- batch: 050 ----
mean loss: 320.44
 ---- batch: 060 ----
mean loss: 321.99
 ---- batch: 070 ----
mean loss: 330.08
 ---- batch: 080 ----
mean loss: 324.86
 ---- batch: 090 ----
mean loss: 314.20
train mean loss: 321.30
epoch train time: 0:00:02.330156
elapsed time: 0:08:55.742617
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 21:31:14.557509
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.08
 ---- batch: 020 ----
mean loss: 320.47
 ---- batch: 030 ----
mean loss: 322.54
 ---- batch: 040 ----
mean loss: 324.87
 ---- batch: 050 ----
mean loss: 327.33
 ---- batch: 060 ----
mean loss: 326.73
 ---- batch: 070 ----
mean loss: 311.07
 ---- batch: 080 ----
mean loss: 325.25
 ---- batch: 090 ----
mean loss: 318.30
train mean loss: 321.14
epoch train time: 0:00:02.338205
elapsed time: 0:08:58.081010
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 21:31:16.895914
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 326.15
 ---- batch: 020 ----
mean loss: 324.30
 ---- batch: 030 ----
mean loss: 324.34
 ---- batch: 040 ----
mean loss: 311.30
 ---- batch: 050 ----
mean loss: 323.81
 ---- batch: 060 ----
mean loss: 329.59
 ---- batch: 070 ----
mean loss: 324.82
 ---- batch: 080 ----
mean loss: 331.07
 ---- batch: 090 ----
mean loss: 323.26
train mean loss: 324.02
epoch train time: 0:00:02.334135
elapsed time: 0:09:00.415369
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 21:31:19.230281
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.79
 ---- batch: 020 ----
mean loss: 319.15
 ---- batch: 030 ----
mean loss: 323.12
 ---- batch: 040 ----
mean loss: 315.37
 ---- batch: 050 ----
mean loss: 315.44
 ---- batch: 060 ----
mean loss: 316.90
 ---- batch: 070 ----
mean loss: 320.91
 ---- batch: 080 ----
mean loss: 317.03
 ---- batch: 090 ----
mean loss: 321.38
train mean loss: 319.05
epoch train time: 0:00:02.335158
elapsed time: 0:09:02.750727
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 21:31:21.565660
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.06
 ---- batch: 020 ----
mean loss: 317.30
 ---- batch: 030 ----
mean loss: 318.73
 ---- batch: 040 ----
mean loss: 317.70
 ---- batch: 050 ----
mean loss: 324.67
 ---- batch: 060 ----
mean loss: 318.08
 ---- batch: 070 ----
mean loss: 307.32
 ---- batch: 080 ----
mean loss: 328.04
 ---- batch: 090 ----
mean loss: 322.94
train mean loss: 317.27
epoch train time: 0:00:02.334387
elapsed time: 0:09:05.085382
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 21:31:23.900278
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.63
 ---- batch: 020 ----
mean loss: 312.88
 ---- batch: 030 ----
mean loss: 325.93
 ---- batch: 040 ----
mean loss: 321.60
 ---- batch: 050 ----
mean loss: 322.67
 ---- batch: 060 ----
mean loss: 323.00
 ---- batch: 070 ----
mean loss: 315.47
 ---- batch: 080 ----
mean loss: 329.77
 ---- batch: 090 ----
mean loss: 323.28
train mean loss: 321.48
epoch train time: 0:00:02.339007
elapsed time: 0:09:07.424576
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 21:31:26.239472
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 322.89
 ---- batch: 020 ----
mean loss: 319.83
 ---- batch: 030 ----
mean loss: 318.21
 ---- batch: 040 ----
mean loss: 316.49
 ---- batch: 050 ----
mean loss: 321.98
 ---- batch: 060 ----
mean loss: 327.26
 ---- batch: 070 ----
mean loss: 315.24
 ---- batch: 080 ----
mean loss: 321.35
 ---- batch: 090 ----
mean loss: 305.81
train mean loss: 318.79
epoch train time: 0:00:02.338073
elapsed time: 0:09:09.762900
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 21:31:28.577829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 312.94
 ---- batch: 020 ----
mean loss: 324.33
 ---- batch: 030 ----
mean loss: 318.12
 ---- batch: 040 ----
mean loss: 315.89
 ---- batch: 050 ----
mean loss: 315.95
 ---- batch: 060 ----
mean loss: 320.65
 ---- batch: 070 ----
mean loss: 322.28
 ---- batch: 080 ----
mean loss: 320.75
 ---- batch: 090 ----
mean loss: 330.93
train mean loss: 319.90
epoch train time: 0:00:02.335794
elapsed time: 0:09:12.098923
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 21:31:30.913815
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.16
 ---- batch: 020 ----
mean loss: 323.13
 ---- batch: 030 ----
mean loss: 323.55
 ---- batch: 040 ----
mean loss: 324.93
 ---- batch: 050 ----
mean loss: 319.80
 ---- batch: 060 ----
mean loss: 314.34
 ---- batch: 070 ----
mean loss: 318.56
 ---- batch: 080 ----
mean loss: 316.76
 ---- batch: 090 ----
mean loss: 335.80
train mean loss: 321.66
epoch train time: 0:00:02.341971
elapsed time: 0:09:14.441100
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 21:31:33.256009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 319.42
 ---- batch: 020 ----
mean loss: 326.96
 ---- batch: 030 ----
mean loss: 325.13
 ---- batch: 040 ----
mean loss: 323.23
 ---- batch: 050 ----
mean loss: 325.46
 ---- batch: 060 ----
mean loss: 319.62
 ---- batch: 070 ----
mean loss: 319.16
 ---- batch: 080 ----
mean loss: 328.27
 ---- batch: 090 ----
mean loss: 324.36
train mean loss: 323.35
epoch train time: 0:00:02.339763
elapsed time: 0:09:16.781050
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 21:31:35.595947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 327.33
 ---- batch: 020 ----
mean loss: 319.42
 ---- batch: 030 ----
mean loss: 313.86
 ---- batch: 040 ----
mean loss: 319.73
 ---- batch: 050 ----
mean loss: 320.45
 ---- batch: 060 ----
mean loss: 320.52
 ---- batch: 070 ----
mean loss: 319.49
 ---- batch: 080 ----
mean loss: 309.39
 ---- batch: 090 ----
mean loss: 321.76
train mean loss: 320.02
epoch train time: 0:00:02.329902
elapsed time: 0:09:19.111163
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 21:31:37.926090
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 324.32
 ---- batch: 020 ----
mean loss: 320.71
 ---- batch: 030 ----
mean loss: 324.79
 ---- batch: 040 ----
mean loss: 317.04
 ---- batch: 050 ----
mean loss: 314.93
 ---- batch: 060 ----
mean loss: 309.47
 ---- batch: 070 ----
mean loss: 310.20
 ---- batch: 080 ----
mean loss: 319.21
 ---- batch: 090 ----
mean loss: 327.34
train mean loss: 319.96
epoch train time: 0:00:02.335735
elapsed time: 0:09:21.447115
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 21:31:40.262010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.19
 ---- batch: 020 ----
mean loss: 319.12
 ---- batch: 030 ----
mean loss: 325.48
 ---- batch: 040 ----
mean loss: 321.95
 ---- batch: 050 ----
mean loss: 314.02
 ---- batch: 060 ----
mean loss: 314.04
 ---- batch: 070 ----
mean loss: 324.23
 ---- batch: 080 ----
mean loss: 315.55
 ---- batch: 090 ----
mean loss: 322.15
train mean loss: 318.23
epoch train time: 0:00:02.337510
elapsed time: 0:09:23.784804
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 21:31:42.599696
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.26
 ---- batch: 020 ----
mean loss: 316.67
 ---- batch: 030 ----
mean loss: 328.71
 ---- batch: 040 ----
mean loss: 326.76
 ---- batch: 050 ----
mean loss: 324.14
 ---- batch: 060 ----
mean loss: 322.07
 ---- batch: 070 ----
mean loss: 319.52
 ---- batch: 080 ----
mean loss: 313.35
 ---- batch: 090 ----
mean loss: 314.34
train mean loss: 320.08
epoch train time: 0:00:02.335874
elapsed time: 0:09:26.120866
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 21:31:44.935763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.75
 ---- batch: 020 ----
mean loss: 325.15
 ---- batch: 030 ----
mean loss: 312.44
 ---- batch: 040 ----
mean loss: 332.09
 ---- batch: 050 ----
mean loss: 332.16
 ---- batch: 060 ----
mean loss: 311.79
 ---- batch: 070 ----
mean loss: 321.08
 ---- batch: 080 ----
mean loss: 316.12
 ---- batch: 090 ----
mean loss: 322.30
train mean loss: 319.97
epoch train time: 0:00:02.335319
elapsed time: 0:09:28.456382
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 21:31:47.271276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 320.58
 ---- batch: 020 ----
mean loss: 320.74
 ---- batch: 030 ----
mean loss: 331.43
 ---- batch: 040 ----
mean loss: 320.06
 ---- batch: 050 ----
mean loss: 310.38
 ---- batch: 060 ----
mean loss: 318.48
 ---- batch: 070 ----
mean loss: 317.88
 ---- batch: 080 ----
mean loss: 322.95
 ---- batch: 090 ----
mean loss: 323.97
train mean loss: 320.84
epoch train time: 0:00:02.346272
elapsed time: 0:09:30.802881
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 21:31:49.617773
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 312.92
 ---- batch: 020 ----
mean loss: 310.29
 ---- batch: 030 ----
mean loss: 322.23
 ---- batch: 040 ----
mean loss: 313.93
 ---- batch: 050 ----
mean loss: 324.18
 ---- batch: 060 ----
mean loss: 316.85
 ---- batch: 070 ----
mean loss: 315.33
 ---- batch: 080 ----
mean loss: 324.22
 ---- batch: 090 ----
mean loss: 326.00
train mean loss: 318.61
epoch train time: 0:00:02.339317
elapsed time: 0:09:33.142389
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 21:31:51.957284
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 324.14
 ---- batch: 020 ----
mean loss: 319.47
 ---- batch: 030 ----
mean loss: 326.94
 ---- batch: 040 ----
mean loss: 307.28
 ---- batch: 050 ----
mean loss: 320.94
 ---- batch: 060 ----
mean loss: 326.57
 ---- batch: 070 ----
mean loss: 324.05
 ---- batch: 080 ----
mean loss: 321.49
 ---- batch: 090 ----
mean loss: 319.28
train mean loss: 320.61
epoch train time: 0:00:02.337605
elapsed time: 0:09:35.480181
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 21:31:54.295080
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 314.69
 ---- batch: 020 ----
mean loss: 315.22
 ---- batch: 030 ----
mean loss: 315.72
 ---- batch: 040 ----
mean loss: 316.86
 ---- batch: 050 ----
mean loss: 313.81
 ---- batch: 060 ----
mean loss: 328.43
 ---- batch: 070 ----
mean loss: 320.69
 ---- batch: 080 ----
mean loss: 330.55
 ---- batch: 090 ----
mean loss: 323.08
train mean loss: 320.21
epoch train time: 0:00:02.342815
elapsed time: 0:09:37.823218
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 21:31:56.638122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 314.07
 ---- batch: 020 ----
mean loss: 316.63
 ---- batch: 030 ----
mean loss: 312.44
 ---- batch: 040 ----
mean loss: 320.89
 ---- batch: 050 ----
mean loss: 324.79
 ---- batch: 060 ----
mean loss: 316.59
 ---- batch: 070 ----
mean loss: 327.99
 ---- batch: 080 ----
mean loss: 318.02
 ---- batch: 090 ----
mean loss: 321.24
train mean loss: 318.83
epoch train time: 0:00:02.336113
elapsed time: 0:09:40.159524
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 21:31:58.974419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 324.23
 ---- batch: 020 ----
mean loss: 309.85
 ---- batch: 030 ----
mean loss: 321.70
 ---- batch: 040 ----
mean loss: 318.38
 ---- batch: 050 ----
mean loss: 332.72
 ---- batch: 060 ----
mean loss: 314.11
 ---- batch: 070 ----
mean loss: 326.19
 ---- batch: 080 ----
mean loss: 320.11
 ---- batch: 090 ----
mean loss: 316.16
train mean loss: 321.14
epoch train time: 0:00:02.339078
elapsed time: 0:09:42.498812
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 21:32:01.313709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 319.96
 ---- batch: 020 ----
mean loss: 326.72
 ---- batch: 030 ----
mean loss: 323.60
 ---- batch: 040 ----
mean loss: 324.95
 ---- batch: 050 ----
mean loss: 314.04
 ---- batch: 060 ----
mean loss: 312.82
 ---- batch: 070 ----
mean loss: 320.96
 ---- batch: 080 ----
mean loss: 322.05
 ---- batch: 090 ----
mean loss: 315.86
train mean loss: 319.38
epoch train time: 0:00:02.332645
elapsed time: 0:09:44.831672
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 21:32:03.646569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.88
 ---- batch: 020 ----
mean loss: 315.70
 ---- batch: 030 ----
mean loss: 320.90
 ---- batch: 040 ----
mean loss: 316.89
 ---- batch: 050 ----
mean loss: 321.98
 ---- batch: 060 ----
mean loss: 314.01
 ---- batch: 070 ----
mean loss: 315.27
 ---- batch: 080 ----
mean loss: 324.73
 ---- batch: 090 ----
mean loss: 323.02
train mean loss: 319.89
epoch train time: 0:00:02.344628
elapsed time: 0:09:47.176484
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 21:32:05.991373
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 324.29
 ---- batch: 020 ----
mean loss: 312.18
 ---- batch: 030 ----
mean loss: 321.97
 ---- batch: 040 ----
mean loss: 311.21
 ---- batch: 050 ----
mean loss: 326.46
 ---- batch: 060 ----
mean loss: 319.45
 ---- batch: 070 ----
mean loss: 319.89
 ---- batch: 080 ----
mean loss: 316.57
 ---- batch: 090 ----
mean loss: 326.61
train mean loss: 319.58
epoch train time: 0:00:02.334678
elapsed time: 0:09:49.511365
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 21:32:08.326257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.53
 ---- batch: 020 ----
mean loss: 318.60
 ---- batch: 030 ----
mean loss: 317.06
 ---- batch: 040 ----
mean loss: 309.25
 ---- batch: 050 ----
mean loss: 316.13
 ---- batch: 060 ----
mean loss: 330.12
 ---- batch: 070 ----
mean loss: 313.32
 ---- batch: 080 ----
mean loss: 322.91
 ---- batch: 090 ----
mean loss: 327.71
train mean loss: 318.95
epoch train time: 0:00:02.339652
elapsed time: 0:09:51.851200
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 21:32:10.666124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.13
 ---- batch: 020 ----
mean loss: 320.52
 ---- batch: 030 ----
mean loss: 319.94
 ---- batch: 040 ----
mean loss: 322.17
 ---- batch: 050 ----
mean loss: 314.72
 ---- batch: 060 ----
mean loss: 314.41
 ---- batch: 070 ----
mean loss: 323.08
 ---- batch: 080 ----
mean loss: 315.94
 ---- batch: 090 ----
mean loss: 313.74
train mean loss: 317.88
epoch train time: 0:00:02.333205
elapsed time: 0:09:54.184615
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 21:32:12.999523
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 331.45
 ---- batch: 020 ----
mean loss: 323.03
 ---- batch: 030 ----
mean loss: 327.66
 ---- batch: 040 ----
mean loss: 315.76
 ---- batch: 050 ----
mean loss: 312.57
 ---- batch: 060 ----
mean loss: 318.20
 ---- batch: 070 ----
mean loss: 316.89
 ---- batch: 080 ----
mean loss: 319.70
 ---- batch: 090 ----
mean loss: 325.56
train mean loss: 321.27
epoch train time: 0:00:02.337232
elapsed time: 0:09:56.522041
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 21:32:15.336943
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 326.08
 ---- batch: 020 ----
mean loss: 321.90
 ---- batch: 030 ----
mean loss: 321.53
 ---- batch: 040 ----
mean loss: 319.71
 ---- batch: 050 ----
mean loss: 314.79
 ---- batch: 060 ----
mean loss: 315.77
 ---- batch: 070 ----
mean loss: 312.69
 ---- batch: 080 ----
mean loss: 307.95
 ---- batch: 090 ----
mean loss: 327.77
train mean loss: 318.79
epoch train time: 0:00:02.331881
elapsed time: 0:09:58.854132
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 21:32:17.669037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.36
 ---- batch: 020 ----
mean loss: 329.30
 ---- batch: 030 ----
mean loss: 321.21
 ---- batch: 040 ----
mean loss: 318.50
 ---- batch: 050 ----
mean loss: 321.83
 ---- batch: 060 ----
mean loss: 325.82
 ---- batch: 070 ----
mean loss: 324.69
 ---- batch: 080 ----
mean loss: 315.58
 ---- batch: 090 ----
mean loss: 323.63
train mean loss: 321.33
epoch train time: 0:00:02.336607
elapsed time: 0:10:01.190933
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 21:32:20.005830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 320.44
 ---- batch: 020 ----
mean loss: 324.25
 ---- batch: 030 ----
mean loss: 311.28
 ---- batch: 040 ----
mean loss: 318.46
 ---- batch: 050 ----
mean loss: 330.93
 ---- batch: 060 ----
mean loss: 324.69
 ---- batch: 070 ----
mean loss: 313.95
 ---- batch: 080 ----
mean loss: 313.43
 ---- batch: 090 ----
mean loss: 325.62
train mean loss: 320.70
epoch train time: 0:00:02.341889
elapsed time: 0:10:03.532998
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 21:32:22.347891
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 311.46
 ---- batch: 020 ----
mean loss: 317.46
 ---- batch: 030 ----
mean loss: 321.17
 ---- batch: 040 ----
mean loss: 319.06
 ---- batch: 050 ----
mean loss: 327.66
 ---- batch: 060 ----
mean loss: 321.15
 ---- batch: 070 ----
mean loss: 315.46
 ---- batch: 080 ----
mean loss: 313.88
 ---- batch: 090 ----
mean loss: 326.98
train mean loss: 319.82
epoch train time: 0:00:02.340942
elapsed time: 0:10:05.874146
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 21:32:24.689060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 319.16
 ---- batch: 020 ----
mean loss: 320.17
 ---- batch: 030 ----
mean loss: 316.69
 ---- batch: 040 ----
mean loss: 326.65
 ---- batch: 050 ----
mean loss: 318.98
 ---- batch: 060 ----
mean loss: 314.88
 ---- batch: 070 ----
mean loss: 322.38
 ---- batch: 080 ----
mean loss: 314.97
 ---- batch: 090 ----
mean loss: 315.40
train mean loss: 319.99
epoch train time: 0:00:02.328646
elapsed time: 0:10:08.202988
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 21:32:27.017878
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 320.59
 ---- batch: 020 ----
mean loss: 314.06
 ---- batch: 030 ----
mean loss: 314.97
 ---- batch: 040 ----
mean loss: 333.44
 ---- batch: 050 ----
mean loss: 324.98
 ---- batch: 060 ----
mean loss: 321.32
 ---- batch: 070 ----
mean loss: 307.01
 ---- batch: 080 ----
mean loss: 314.84
 ---- batch: 090 ----
mean loss: 327.32
train mean loss: 318.75
epoch train time: 0:00:02.344973
elapsed time: 0:10:10.548149
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 21:32:29.363042
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.71
 ---- batch: 020 ----
mean loss: 318.53
 ---- batch: 030 ----
mean loss: 322.76
 ---- batch: 040 ----
mean loss: 314.49
 ---- batch: 050 ----
mean loss: 320.67
 ---- batch: 060 ----
mean loss: 325.10
 ---- batch: 070 ----
mean loss: 327.46
 ---- batch: 080 ----
mean loss: 323.09
 ---- batch: 090 ----
mean loss: 310.89
train mean loss: 319.80
epoch train time: 0:00:02.336133
elapsed time: 0:10:12.884474
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 21:32:31.699431
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 314.26
 ---- batch: 020 ----
mean loss: 314.18
 ---- batch: 030 ----
mean loss: 313.17
 ---- batch: 040 ----
mean loss: 306.06
 ---- batch: 050 ----
mean loss: 330.22
 ---- batch: 060 ----
mean loss: 327.74
 ---- batch: 070 ----
mean loss: 314.07
 ---- batch: 080 ----
mean loss: 326.97
 ---- batch: 090 ----
mean loss: 328.32
train mean loss: 319.41
epoch train time: 0:00:02.336718
elapsed time: 0:10:15.224885
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_4/checkpoint.pth.tar
**** end time: 2019-09-25 21:32:34.039735 ****
