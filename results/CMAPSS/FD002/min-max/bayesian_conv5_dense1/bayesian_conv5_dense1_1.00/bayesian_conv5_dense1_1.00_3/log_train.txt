Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_3', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 3922
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-24 22:30:50.909945 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-24 22:30:50.927928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2476.71
 ---- batch: 020 ----
mean loss: 1789.11
 ---- batch: 030 ----
mean loss: 1447.65
 ---- batch: 040 ----
mean loss: 1351.74
 ---- batch: 050 ----
mean loss: 1272.11
 ---- batch: 060 ----
mean loss: 1202.11
 ---- batch: 070 ----
mean loss: 1177.67
 ---- batch: 080 ----
mean loss: 1125.53
 ---- batch: 090 ----
mean loss: 1140.89
train mean loss: 1422.53
epoch train time: 0:00:48.774134
elapsed time: 0:00:48.800918
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-24 22:31:39.710906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1116.79
 ---- batch: 020 ----
mean loss: 1129.48
 ---- batch: 030 ----
mean loss: 1108.24
 ---- batch: 040 ----
mean loss: 1086.04
 ---- batch: 050 ----
mean loss: 1058.85
 ---- batch: 060 ----
mean loss: 1079.52
 ---- batch: 070 ----
mean loss: 1082.90
 ---- batch: 080 ----
mean loss: 1079.86
 ---- batch: 090 ----
mean loss: 1101.33
train mean loss: 1090.70
epoch train time: 0:00:17.330653
elapsed time: 0:01:06.132232
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-24 22:31:57.042714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1070.36
 ---- batch: 020 ----
mean loss: 1060.04
 ---- batch: 030 ----
mean loss: 1068.64
 ---- batch: 040 ----
mean loss: 1087.40
 ---- batch: 050 ----
mean loss: 1075.07
 ---- batch: 060 ----
mean loss: 1036.73
 ---- batch: 070 ----
mean loss: 1055.42
 ---- batch: 080 ----
mean loss: 1035.05
 ---- batch: 090 ----
mean loss: 1057.95
train mean loss: 1060.71
epoch train time: 0:00:17.268902
elapsed time: 0:01:23.402274
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-24 22:32:14.312827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1072.22
 ---- batch: 020 ----
mean loss: 1034.18
 ---- batch: 030 ----
mean loss: 1022.04
 ---- batch: 040 ----
mean loss: 1062.99
 ---- batch: 050 ----
mean loss: 1025.18
 ---- batch: 060 ----
mean loss: 1015.33
 ---- batch: 070 ----
mean loss: 1033.63
 ---- batch: 080 ----
mean loss: 1029.17
 ---- batch: 090 ----
mean loss: 1023.48
train mean loss: 1036.61
epoch train time: 0:00:17.411373
elapsed time: 0:01:40.814859
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-24 22:32:31.725390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1038.24
 ---- batch: 020 ----
mean loss: 1019.08
 ---- batch: 030 ----
mean loss: 1024.41
 ---- batch: 040 ----
mean loss: 1029.33
 ---- batch: 050 ----
mean loss: 1020.92
 ---- batch: 060 ----
mean loss: 1020.61
 ---- batch: 070 ----
mean loss: 1009.08
 ---- batch: 080 ----
mean loss: 1048.06
 ---- batch: 090 ----
mean loss: 1007.30
train mean loss: 1021.23
epoch train time: 0:00:17.195295
elapsed time: 0:01:58.011376
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-24 22:32:48.921890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1014.33
 ---- batch: 020 ----
mean loss: 1013.52
 ---- batch: 030 ----
mean loss: 1034.63
 ---- batch: 040 ----
mean loss: 1023.84
 ---- batch: 050 ----
mean loss: 990.25
 ---- batch: 060 ----
mean loss: 991.68
 ---- batch: 070 ----
mean loss: 1038.73
 ---- batch: 080 ----
mean loss: 1001.14
 ---- batch: 090 ----
mean loss: 1009.95
train mean loss: 1013.58
epoch train time: 0:00:17.192120
elapsed time: 0:02:15.204722
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-24 22:33:06.115245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 985.19
 ---- batch: 020 ----
mean loss: 1000.70
 ---- batch: 030 ----
mean loss: 993.17
 ---- batch: 040 ----
mean loss: 1006.28
 ---- batch: 050 ----
mean loss: 999.02
 ---- batch: 060 ----
mean loss: 1001.90
 ---- batch: 070 ----
mean loss: 990.45
 ---- batch: 080 ----
mean loss: 981.11
 ---- batch: 090 ----
mean loss: 988.27
train mean loss: 994.60
epoch train time: 0:00:17.254465
elapsed time: 0:02:32.460432
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-24 22:33:23.370925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 984.14
 ---- batch: 020 ----
mean loss: 989.06
 ---- batch: 030 ----
mean loss: 1033.25
 ---- batch: 040 ----
mean loss: 987.58
 ---- batch: 050 ----
mean loss: 983.24
 ---- batch: 060 ----
mean loss: 992.35
 ---- batch: 070 ----
mean loss: 992.53
 ---- batch: 080 ----
mean loss: 986.45
 ---- batch: 090 ----
mean loss: 972.06
train mean loss: 989.60
epoch train time: 0:00:17.188014
elapsed time: 0:02:49.649634
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-24 22:33:40.560182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 982.77
 ---- batch: 020 ----
mean loss: 968.39
 ---- batch: 030 ----
mean loss: 989.20
 ---- batch: 040 ----
mean loss: 994.36
 ---- batch: 050 ----
mean loss: 985.93
 ---- batch: 060 ----
mean loss: 975.14
 ---- batch: 070 ----
mean loss: 970.87
 ---- batch: 080 ----
mean loss: 973.02
 ---- batch: 090 ----
mean loss: 982.22
train mean loss: 979.53
epoch train time: 0:00:17.082084
elapsed time: 0:03:06.733098
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-24 22:33:57.643643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 972.07
 ---- batch: 020 ----
mean loss: 986.21
 ---- batch: 030 ----
mean loss: 953.65
 ---- batch: 040 ----
mean loss: 962.53
 ---- batch: 050 ----
mean loss: 976.47
 ---- batch: 060 ----
mean loss: 981.66
 ---- batch: 070 ----
mean loss: 980.86
 ---- batch: 080 ----
mean loss: 955.16
 ---- batch: 090 ----
mean loss: 974.32
train mean loss: 972.36
epoch train time: 0:00:17.003643
elapsed time: 0:03:23.737965
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-24 22:34:14.648478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.61
 ---- batch: 020 ----
mean loss: 988.08
 ---- batch: 030 ----
mean loss: 974.56
 ---- batch: 040 ----
mean loss: 979.25
 ---- batch: 050 ----
mean loss: 962.95
 ---- batch: 060 ----
mean loss: 965.99
 ---- batch: 070 ----
mean loss: 961.23
 ---- batch: 080 ----
mean loss: 959.84
 ---- batch: 090 ----
mean loss: 971.44
train mean loss: 969.47
epoch train time: 0:00:17.050953
elapsed time: 0:03:40.790208
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-24 22:34:31.700810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 918.80
 ---- batch: 020 ----
mean loss: 955.09
 ---- batch: 030 ----
mean loss: 977.46
 ---- batch: 040 ----
mean loss: 985.82
 ---- batch: 050 ----
mean loss: 979.75
 ---- batch: 060 ----
mean loss: 945.92
 ---- batch: 070 ----
mean loss: 964.74
 ---- batch: 080 ----
mean loss: 954.58
 ---- batch: 090 ----
mean loss: 950.46
train mean loss: 957.90
epoch train time: 0:00:16.984817
elapsed time: 0:03:57.776598
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-24 22:34:48.687147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 961.10
 ---- batch: 020 ----
mean loss: 953.19
 ---- batch: 030 ----
mean loss: 956.58
 ---- batch: 040 ----
mean loss: 937.59
 ---- batch: 050 ----
mean loss: 952.69
 ---- batch: 060 ----
mean loss: 971.52
 ---- batch: 070 ----
mean loss: 942.50
 ---- batch: 080 ----
mean loss: 947.19
 ---- batch: 090 ----
mean loss: 958.88
train mean loss: 953.31
epoch train time: 0:00:17.024164
elapsed time: 0:04:14.802147
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-24 22:35:05.712612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.53
 ---- batch: 020 ----
mean loss: 940.18
 ---- batch: 030 ----
mean loss: 936.33
 ---- batch: 040 ----
mean loss: 919.98
 ---- batch: 050 ----
mean loss: 950.89
 ---- batch: 060 ----
mean loss: 942.01
 ---- batch: 070 ----
mean loss: 960.77
 ---- batch: 080 ----
mean loss: 937.65
 ---- batch: 090 ----
mean loss: 953.98
train mean loss: 942.67
epoch train time: 0:00:16.984035
elapsed time: 0:04:31.787310
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-24 22:35:22.697921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 953.44
 ---- batch: 020 ----
mean loss: 935.97
 ---- batch: 030 ----
mean loss: 939.18
 ---- batch: 040 ----
mean loss: 927.48
 ---- batch: 050 ----
mean loss: 931.40
 ---- batch: 060 ----
mean loss: 926.48
 ---- batch: 070 ----
mean loss: 931.00
 ---- batch: 080 ----
mean loss: 955.14
 ---- batch: 090 ----
mean loss: 941.66
train mean loss: 938.61
epoch train time: 0:00:17.064828
elapsed time: 0:04:48.853428
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-24 22:35:39.763941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.60
 ---- batch: 020 ----
mean loss: 929.71
 ---- batch: 030 ----
mean loss: 920.61
 ---- batch: 040 ----
mean loss: 948.01
 ---- batch: 050 ----
mean loss: 920.12
 ---- batch: 060 ----
mean loss: 902.72
 ---- batch: 070 ----
mean loss: 886.05
 ---- batch: 080 ----
mean loss: 915.24
 ---- batch: 090 ----
mean loss: 931.60
train mean loss: 918.08
epoch train time: 0:00:17.355569
elapsed time: 0:05:06.210462
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-24 22:35:57.121007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.67
 ---- batch: 020 ----
mean loss: 863.67
 ---- batch: 030 ----
mean loss: 896.25
 ---- batch: 040 ----
mean loss: 883.29
 ---- batch: 050 ----
mean loss: 851.96
 ---- batch: 060 ----
mean loss: 842.65
 ---- batch: 070 ----
mean loss: 847.56
 ---- batch: 080 ----
mean loss: 817.93
 ---- batch: 090 ----
mean loss: 772.81
train mean loss: 849.45
epoch train time: 0:00:17.304986
elapsed time: 0:05:23.516980
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-24 22:36:14.427610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 759.51
 ---- batch: 020 ----
mean loss: 748.04
 ---- batch: 030 ----
mean loss: 731.52
 ---- batch: 040 ----
mean loss: 717.24
 ---- batch: 050 ----
mean loss: 710.69
 ---- batch: 060 ----
mean loss: 710.30
 ---- batch: 070 ----
mean loss: 706.13
 ---- batch: 080 ----
mean loss: 701.48
 ---- batch: 090 ----
mean loss: 692.33
train mean loss: 717.43
epoch train time: 0:00:17.301553
elapsed time: 0:05:40.820015
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-24 22:36:31.730689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 698.49
 ---- batch: 020 ----
mean loss: 684.40
 ---- batch: 030 ----
mean loss: 673.52
 ---- batch: 040 ----
mean loss: 675.37
 ---- batch: 050 ----
mean loss: 671.56
 ---- batch: 060 ----
mean loss: 657.77
 ---- batch: 070 ----
mean loss: 636.17
 ---- batch: 080 ----
mean loss: 638.96
 ---- batch: 090 ----
mean loss: 646.22
train mean loss: 663.53
epoch train time: 0:00:17.317435
elapsed time: 0:05:58.138802
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-24 22:36:49.049317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 630.36
 ---- batch: 020 ----
mean loss: 634.40
 ---- batch: 030 ----
mean loss: 621.08
 ---- batch: 040 ----
mean loss: 628.58
 ---- batch: 050 ----
mean loss: 604.99
 ---- batch: 060 ----
mean loss: 590.30
 ---- batch: 070 ----
mean loss: 612.28
 ---- batch: 080 ----
mean loss: 598.98
 ---- batch: 090 ----
mean loss: 590.31
train mean loss: 613.34
epoch train time: 0:00:17.279265
elapsed time: 0:06:15.419363
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-24 22:37:06.329875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 583.27
 ---- batch: 020 ----
mean loss: 577.29
 ---- batch: 030 ----
mean loss: 574.96
 ---- batch: 040 ----
mean loss: 568.68
 ---- batch: 050 ----
mean loss: 562.56
 ---- batch: 060 ----
mean loss: 569.80
 ---- batch: 070 ----
mean loss: 550.52
 ---- batch: 080 ----
mean loss: 566.56
 ---- batch: 090 ----
mean loss: 544.67
train mean loss: 565.36
epoch train time: 0:00:17.456063
elapsed time: 0:06:32.876761
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-24 22:37:23.787317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.00
 ---- batch: 020 ----
mean loss: 538.17
 ---- batch: 030 ----
mean loss: 542.41
 ---- batch: 040 ----
mean loss: 520.44
 ---- batch: 050 ----
mean loss: 520.63
 ---- batch: 060 ----
mean loss: 523.99
 ---- batch: 070 ----
mean loss: 518.18
 ---- batch: 080 ----
mean loss: 513.27
 ---- batch: 090 ----
mean loss: 518.42
train mean loss: 523.61
epoch train time: 0:00:17.297591
elapsed time: 0:06:50.175680
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-24 22:37:41.086108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.95
 ---- batch: 020 ----
mean loss: 491.74
 ---- batch: 030 ----
mean loss: 496.79
 ---- batch: 040 ----
mean loss: 484.06
 ---- batch: 050 ----
mean loss: 488.14
 ---- batch: 060 ----
mean loss: 490.58
 ---- batch: 070 ----
mean loss: 481.00
 ---- batch: 080 ----
mean loss: 500.36
 ---- batch: 090 ----
mean loss: 481.78
train mean loss: 491.73
epoch train time: 0:00:17.339671
elapsed time: 0:07:07.516430
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-24 22:37:58.426945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.80
 ---- batch: 020 ----
mean loss: 478.92
 ---- batch: 030 ----
mean loss: 470.58
 ---- batch: 040 ----
mean loss: 471.85
 ---- batch: 050 ----
mean loss: 474.57
 ---- batch: 060 ----
mean loss: 464.36
 ---- batch: 070 ----
mean loss: 469.55
 ---- batch: 080 ----
mean loss: 470.59
 ---- batch: 090 ----
mean loss: 452.68
train mean loss: 470.62
epoch train time: 0:00:17.227008
elapsed time: 0:07:24.744615
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-24 22:38:15.655119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.15
 ---- batch: 020 ----
mean loss: 479.88
 ---- batch: 030 ----
mean loss: 468.83
 ---- batch: 040 ----
mean loss: 451.91
 ---- batch: 050 ----
mean loss: 467.66
 ---- batch: 060 ----
mean loss: 448.56
 ---- batch: 070 ----
mean loss: 447.73
 ---- batch: 080 ----
mean loss: 451.50
 ---- batch: 090 ----
mean loss: 453.34
train mean loss: 458.24
epoch train time: 0:00:17.433604
elapsed time: 0:07:42.179502
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-24 22:38:33.090151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.16
 ---- batch: 020 ----
mean loss: 443.49
 ---- batch: 030 ----
mean loss: 439.79
 ---- batch: 040 ----
mean loss: 440.61
 ---- batch: 050 ----
mean loss: 431.28
 ---- batch: 060 ----
mean loss: 447.21
 ---- batch: 070 ----
mean loss: 429.95
 ---- batch: 080 ----
mean loss: 435.51
 ---- batch: 090 ----
mean loss: 426.52
train mean loss: 437.12
epoch train time: 0:00:17.328587
elapsed time: 0:07:59.509566
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-24 22:38:50.420164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.46
 ---- batch: 020 ----
mean loss: 437.98
 ---- batch: 030 ----
mean loss: 436.94
 ---- batch: 040 ----
mean loss: 438.15
 ---- batch: 050 ----
mean loss: 412.39
 ---- batch: 060 ----
mean loss: 425.17
 ---- batch: 070 ----
mean loss: 433.49
 ---- batch: 080 ----
mean loss: 414.68
 ---- batch: 090 ----
mean loss: 429.73
train mean loss: 427.91
epoch train time: 0:00:17.349090
elapsed time: 0:08:16.860054
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-24 22:39:07.770563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.73
 ---- batch: 020 ----
mean loss: 423.74
 ---- batch: 030 ----
mean loss: 420.40
 ---- batch: 040 ----
mean loss: 418.59
 ---- batch: 050 ----
mean loss: 416.71
 ---- batch: 060 ----
mean loss: 407.24
 ---- batch: 070 ----
mean loss: 400.16
 ---- batch: 080 ----
mean loss: 411.98
 ---- batch: 090 ----
mean loss: 410.32
train mean loss: 415.40
epoch train time: 0:00:17.375680
elapsed time: 0:08:34.237135
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-24 22:39:25.147823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.92
 ---- batch: 020 ----
mean loss: 396.58
 ---- batch: 030 ----
mean loss: 400.42
 ---- batch: 040 ----
mean loss: 407.80
 ---- batch: 050 ----
mean loss: 396.28
 ---- batch: 060 ----
mean loss: 399.64
 ---- batch: 070 ----
mean loss: 412.48
 ---- batch: 080 ----
mean loss: 407.04
 ---- batch: 090 ----
mean loss: 398.23
train mean loss: 402.51
epoch train time: 0:00:17.262461
elapsed time: 0:08:51.501091
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-24 22:39:42.411648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.94
 ---- batch: 020 ----
mean loss: 398.60
 ---- batch: 030 ----
mean loss: 396.16
 ---- batch: 040 ----
mean loss: 386.93
 ---- batch: 050 ----
mean loss: 396.45
 ---- batch: 060 ----
mean loss: 395.67
 ---- batch: 070 ----
mean loss: 395.91
 ---- batch: 080 ----
mean loss: 392.90
 ---- batch: 090 ----
mean loss: 377.57
train mean loss: 394.05
epoch train time: 0:00:17.452687
elapsed time: 0:09:08.955116
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-24 22:39:59.865714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.44
 ---- batch: 020 ----
mean loss: 384.49
 ---- batch: 030 ----
mean loss: 388.44
 ---- batch: 040 ----
mean loss: 382.40
 ---- batch: 050 ----
mean loss: 392.62
 ---- batch: 060 ----
mean loss: 388.46
 ---- batch: 070 ----
mean loss: 377.35
 ---- batch: 080 ----
mean loss: 379.72
 ---- batch: 090 ----
mean loss: 369.10
train mean loss: 382.33
epoch train time: 0:00:17.286036
elapsed time: 0:09:26.242497
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-24 22:40:17.153012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.90
 ---- batch: 020 ----
mean loss: 370.72
 ---- batch: 030 ----
mean loss: 379.08
 ---- batch: 040 ----
mean loss: 356.60
 ---- batch: 050 ----
mean loss: 364.89
 ---- batch: 060 ----
mean loss: 383.16
 ---- batch: 070 ----
mean loss: 371.83
 ---- batch: 080 ----
mean loss: 383.44
 ---- batch: 090 ----
mean loss: 369.42
train mean loss: 373.55
epoch train time: 0:00:17.332468
elapsed time: 0:09:43.576223
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-24 22:40:34.486850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.29
 ---- batch: 020 ----
mean loss: 384.12
 ---- batch: 030 ----
mean loss: 375.42
 ---- batch: 040 ----
mean loss: 365.48
 ---- batch: 050 ----
mean loss: 377.98
 ---- batch: 060 ----
mean loss: 373.60
 ---- batch: 070 ----
mean loss: 374.22
 ---- batch: 080 ----
mean loss: 369.58
 ---- batch: 090 ----
mean loss: 349.29
train mean loss: 370.11
epoch train time: 0:00:17.366295
elapsed time: 0:10:00.943927
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-24 22:40:51.854462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.01
 ---- batch: 020 ----
mean loss: 367.93
 ---- batch: 030 ----
mean loss: 363.21
 ---- batch: 040 ----
mean loss: 355.69
 ---- batch: 050 ----
mean loss: 355.42
 ---- batch: 060 ----
mean loss: 358.94
 ---- batch: 070 ----
mean loss: 357.77
 ---- batch: 080 ----
mean loss: 361.21
 ---- batch: 090 ----
mean loss: 353.36
train mean loss: 359.06
epoch train time: 0:00:17.264761
elapsed time: 0:10:18.210071
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-24 22:41:09.120676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.53
 ---- batch: 020 ----
mean loss: 350.86
 ---- batch: 030 ----
mean loss: 355.50
 ---- batch: 040 ----
mean loss: 359.54
 ---- batch: 050 ----
mean loss: 348.21
 ---- batch: 060 ----
mean loss: 355.99
 ---- batch: 070 ----
mean loss: 363.18
 ---- batch: 080 ----
mean loss: 339.88
 ---- batch: 090 ----
mean loss: 352.69
train mean loss: 353.27
epoch train time: 0:00:17.390312
elapsed time: 0:10:35.601796
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-24 22:41:26.512374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.07
 ---- batch: 020 ----
mean loss: 349.73
 ---- batch: 030 ----
mean loss: 347.64
 ---- batch: 040 ----
mean loss: 344.32
 ---- batch: 050 ----
mean loss: 340.12
 ---- batch: 060 ----
mean loss: 333.25
 ---- batch: 070 ----
mean loss: 349.35
 ---- batch: 080 ----
mean loss: 338.61
 ---- batch: 090 ----
mean loss: 332.58
train mean loss: 344.06
epoch train time: 0:00:17.348734
elapsed time: 0:10:52.951929
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-24 22:41:43.862465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.79
 ---- batch: 020 ----
mean loss: 345.61
 ---- batch: 030 ----
mean loss: 338.61
 ---- batch: 040 ----
mean loss: 342.79
 ---- batch: 050 ----
mean loss: 342.92
 ---- batch: 060 ----
mean loss: 329.56
 ---- batch: 070 ----
mean loss: 336.03
 ---- batch: 080 ----
mean loss: 335.36
 ---- batch: 090 ----
mean loss: 335.56
train mean loss: 339.79
epoch train time: 0:00:17.350668
elapsed time: 0:11:10.303821
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-24 22:42:01.214296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.90
 ---- batch: 020 ----
mean loss: 342.03
 ---- batch: 030 ----
mean loss: 337.81
 ---- batch: 040 ----
mean loss: 348.00
 ---- batch: 050 ----
mean loss: 337.50
 ---- batch: 060 ----
mean loss: 330.86
 ---- batch: 070 ----
mean loss: 332.94
 ---- batch: 080 ----
mean loss: 328.46
 ---- batch: 090 ----
mean loss: 326.12
train mean loss: 337.05
epoch train time: 0:00:17.261116
elapsed time: 0:11:27.566046
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-24 22:42:18.476550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.49
 ---- batch: 020 ----
mean loss: 332.22
 ---- batch: 030 ----
mean loss: 336.30
 ---- batch: 040 ----
mean loss: 339.97
 ---- batch: 050 ----
mean loss: 332.20
 ---- batch: 060 ----
mean loss: 331.48
 ---- batch: 070 ----
mean loss: 334.45
 ---- batch: 080 ----
mean loss: 331.18
 ---- batch: 090 ----
mean loss: 316.68
train mean loss: 330.66
epoch train time: 0:00:17.408961
elapsed time: 0:11:44.976280
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-24 22:42:35.886820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.33
 ---- batch: 020 ----
mean loss: 318.65
 ---- batch: 030 ----
mean loss: 328.96
 ---- batch: 040 ----
mean loss: 327.23
 ---- batch: 050 ----
mean loss: 327.65
 ---- batch: 060 ----
mean loss: 320.90
 ---- batch: 070 ----
mean loss: 316.46
 ---- batch: 080 ----
mean loss: 325.57
 ---- batch: 090 ----
mean loss: 321.54
train mean loss: 323.48
epoch train time: 0:00:17.394055
elapsed time: 0:12:02.371725
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-24 22:42:53.282303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.68
 ---- batch: 020 ----
mean loss: 323.11
 ---- batch: 030 ----
mean loss: 314.56
 ---- batch: 040 ----
mean loss: 319.67
 ---- batch: 050 ----
mean loss: 315.93
 ---- batch: 060 ----
mean loss: 324.68
 ---- batch: 070 ----
mean loss: 316.45
 ---- batch: 080 ----
mean loss: 325.18
 ---- batch: 090 ----
mean loss: 313.99
train mean loss: 320.08
epoch train time: 0:00:17.338916
elapsed time: 0:12:19.711926
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-24 22:43:10.622432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.11
 ---- batch: 020 ----
mean loss: 308.56
 ---- batch: 030 ----
mean loss: 321.16
 ---- batch: 040 ----
mean loss: 319.11
 ---- batch: 050 ----
mean loss: 314.22
 ---- batch: 060 ----
mean loss: 319.47
 ---- batch: 070 ----
mean loss: 308.59
 ---- batch: 080 ----
mean loss: 319.23
 ---- batch: 090 ----
mean loss: 310.85
train mean loss: 313.84
epoch train time: 0:00:17.354620
elapsed time: 0:12:37.067914
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-24 22:43:27.978524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.16
 ---- batch: 020 ----
mean loss: 306.92
 ---- batch: 030 ----
mean loss: 308.71
 ---- batch: 040 ----
mean loss: 307.40
 ---- batch: 050 ----
mean loss: 316.65
 ---- batch: 060 ----
mean loss: 314.07
 ---- batch: 070 ----
mean loss: 303.78
 ---- batch: 080 ----
mean loss: 315.06
 ---- batch: 090 ----
mean loss: 305.47
train mean loss: 310.42
epoch train time: 0:00:17.231663
elapsed time: 0:12:54.300994
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-24 22:43:45.211598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.09
 ---- batch: 020 ----
mean loss: 321.98
 ---- batch: 030 ----
mean loss: 307.13
 ---- batch: 040 ----
mean loss: 309.38
 ---- batch: 050 ----
mean loss: 297.03
 ---- batch: 060 ----
mean loss: 300.04
 ---- batch: 070 ----
mean loss: 301.21
 ---- batch: 080 ----
mean loss: 301.40
 ---- batch: 090 ----
mean loss: 303.02
train mean loss: 305.25
epoch train time: 0:00:17.208028
elapsed time: 0:13:11.510478
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-24 22:44:02.420949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.74
 ---- batch: 020 ----
mean loss: 285.69
 ---- batch: 030 ----
mean loss: 318.79
 ---- batch: 040 ----
mean loss: 288.98
 ---- batch: 050 ----
mean loss: 296.88
 ---- batch: 060 ----
mean loss: 302.67
 ---- batch: 070 ----
mean loss: 306.51
 ---- batch: 080 ----
mean loss: 304.22
 ---- batch: 090 ----
mean loss: 304.08
train mean loss: 301.15
epoch train time: 0:00:17.026267
elapsed time: 0:13:28.537983
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-24 22:44:19.448477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.45
 ---- batch: 020 ----
mean loss: 306.20
 ---- batch: 030 ----
mean loss: 292.64
 ---- batch: 040 ----
mean loss: 297.29
 ---- batch: 050 ----
mean loss: 287.81
 ---- batch: 060 ----
mean loss: 283.77
 ---- batch: 070 ----
mean loss: 298.97
 ---- batch: 080 ----
mean loss: 294.03
 ---- batch: 090 ----
mean loss: 303.51
train mean loss: 295.40
epoch train time: 0:00:17.061707
elapsed time: 0:13:45.600894
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-24 22:44:36.511444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.74
 ---- batch: 020 ----
mean loss: 304.09
 ---- batch: 030 ----
mean loss: 301.63
 ---- batch: 040 ----
mean loss: 294.11
 ---- batch: 050 ----
mean loss: 292.17
 ---- batch: 060 ----
mean loss: 295.25
 ---- batch: 070 ----
mean loss: 293.31
 ---- batch: 080 ----
mean loss: 284.76
 ---- batch: 090 ----
mean loss: 291.24
train mean loss: 295.48
epoch train time: 0:00:17.058704
elapsed time: 0:14:02.660887
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-24 22:44:53.571483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.94
 ---- batch: 020 ----
mean loss: 293.41
 ---- batch: 030 ----
mean loss: 297.04
 ---- batch: 040 ----
mean loss: 283.04
 ---- batch: 050 ----
mean loss: 298.74
 ---- batch: 060 ----
mean loss: 292.48
 ---- batch: 070 ----
mean loss: 289.45
 ---- batch: 080 ----
mean loss: 281.34
 ---- batch: 090 ----
mean loss: 284.22
train mean loss: 291.02
epoch train time: 0:00:17.083242
elapsed time: 0:14:19.745524
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-24 22:45:10.656075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.56
 ---- batch: 020 ----
mean loss: 282.24
 ---- batch: 030 ----
mean loss: 287.88
 ---- batch: 040 ----
mean loss: 287.99
 ---- batch: 050 ----
mean loss: 288.69
 ---- batch: 060 ----
mean loss: 286.12
 ---- batch: 070 ----
mean loss: 292.80
 ---- batch: 080 ----
mean loss: 293.59
 ---- batch: 090 ----
mean loss: 289.35
train mean loss: 288.08
epoch train time: 0:00:17.138602
elapsed time: 0:14:36.885381
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-24 22:45:27.796018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.11
 ---- batch: 020 ----
mean loss: 277.57
 ---- batch: 030 ----
mean loss: 292.55
 ---- batch: 040 ----
mean loss: 287.99
 ---- batch: 050 ----
mean loss: 288.09
 ---- batch: 060 ----
mean loss: 293.80
 ---- batch: 070 ----
mean loss: 279.30
 ---- batch: 080 ----
mean loss: 285.32
 ---- batch: 090 ----
mean loss: 275.91
train mean loss: 284.69
epoch train time: 0:00:17.183708
elapsed time: 0:14:54.070399
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-24 22:45:44.980884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.71
 ---- batch: 020 ----
mean loss: 282.71
 ---- batch: 030 ----
mean loss: 282.14
 ---- batch: 040 ----
mean loss: 281.94
 ---- batch: 050 ----
mean loss: 281.41
 ---- batch: 060 ----
mean loss: 288.34
 ---- batch: 070 ----
mean loss: 283.51
 ---- batch: 080 ----
mean loss: 273.19
 ---- batch: 090 ----
mean loss: 272.13
train mean loss: 280.81
epoch train time: 0:00:17.121781
elapsed time: 0:15:11.193349
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-24 22:46:02.103868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.14
 ---- batch: 020 ----
mean loss: 279.27
 ---- batch: 030 ----
mean loss: 285.01
 ---- batch: 040 ----
mean loss: 286.16
 ---- batch: 050 ----
mean loss: 273.22
 ---- batch: 060 ----
mean loss: 280.78
 ---- batch: 070 ----
mean loss: 267.86
 ---- batch: 080 ----
mean loss: 276.83
 ---- batch: 090 ----
mean loss: 278.91
train mean loss: 279.49
epoch train time: 0:00:16.980892
elapsed time: 0:15:28.175422
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-24 22:46:19.085940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.56
 ---- batch: 020 ----
mean loss: 275.01
 ---- batch: 030 ----
mean loss: 276.66
 ---- batch: 040 ----
mean loss: 270.54
 ---- batch: 050 ----
mean loss: 275.21
 ---- batch: 060 ----
mean loss: 277.53
 ---- batch: 070 ----
mean loss: 268.61
 ---- batch: 080 ----
mean loss: 261.74
 ---- batch: 090 ----
mean loss: 279.87
train mean loss: 274.61
epoch train time: 0:00:16.956596
elapsed time: 0:15:45.133195
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-24 22:46:36.043737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.67
 ---- batch: 020 ----
mean loss: 274.33
 ---- batch: 030 ----
mean loss: 282.79
 ---- batch: 040 ----
mean loss: 273.72
 ---- batch: 050 ----
mean loss: 268.65
 ---- batch: 060 ----
mean loss: 268.33
 ---- batch: 070 ----
mean loss: 268.27
 ---- batch: 080 ----
mean loss: 282.33
 ---- batch: 090 ----
mean loss: 267.15
train mean loss: 273.57
epoch train time: 0:00:16.978850
elapsed time: 0:16:02.113263
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-24 22:46:53.023798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.30
 ---- batch: 020 ----
mean loss: 268.67
 ---- batch: 030 ----
mean loss: 265.96
 ---- batch: 040 ----
mean loss: 268.97
 ---- batch: 050 ----
mean loss: 270.06
 ---- batch: 060 ----
mean loss: 270.60
 ---- batch: 070 ----
mean loss: 275.69
 ---- batch: 080 ----
mean loss: 259.80
 ---- batch: 090 ----
mean loss: 273.99
train mean loss: 269.39
epoch train time: 0:00:17.176665
elapsed time: 0:16:19.291245
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-24 22:47:10.201859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.96
 ---- batch: 020 ----
mean loss: 261.30
 ---- batch: 030 ----
mean loss: 262.84
 ---- batch: 040 ----
mean loss: 271.00
 ---- batch: 050 ----
mean loss: 262.23
 ---- batch: 060 ----
mean loss: 266.68
 ---- batch: 070 ----
mean loss: 278.62
 ---- batch: 080 ----
mean loss: 262.35
 ---- batch: 090 ----
mean loss: 260.28
train mean loss: 267.24
epoch train time: 0:00:16.963569
elapsed time: 0:16:36.256125
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-24 22:47:27.166614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.59
 ---- batch: 020 ----
mean loss: 261.57
 ---- batch: 030 ----
mean loss: 274.00
 ---- batch: 040 ----
mean loss: 266.88
 ---- batch: 050 ----
mean loss: 262.73
 ---- batch: 060 ----
mean loss: 260.53
 ---- batch: 070 ----
mean loss: 269.75
 ---- batch: 080 ----
mean loss: 263.76
 ---- batch: 090 ----
mean loss: 268.02
train mean loss: 265.91
epoch train time: 0:00:17.008085
elapsed time: 0:16:53.265640
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-24 22:47:44.176230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.50
 ---- batch: 020 ----
mean loss: 263.68
 ---- batch: 030 ----
mean loss: 265.32
 ---- batch: 040 ----
mean loss: 259.29
 ---- batch: 050 ----
mean loss: 267.87
 ---- batch: 060 ----
mean loss: 268.66
 ---- batch: 070 ----
mean loss: 258.02
 ---- batch: 080 ----
mean loss: 260.02
 ---- batch: 090 ----
mean loss: 264.59
train mean loss: 263.75
epoch train time: 0:00:17.363602
elapsed time: 0:17:10.630529
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-24 22:48:01.541098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.17
 ---- batch: 020 ----
mean loss: 262.52
 ---- batch: 030 ----
mean loss: 256.07
 ---- batch: 040 ----
mean loss: 262.24
 ---- batch: 050 ----
mean loss: 258.13
 ---- batch: 060 ----
mean loss: 262.24
 ---- batch: 070 ----
mean loss: 262.84
 ---- batch: 080 ----
mean loss: 263.80
 ---- batch: 090 ----
mean loss: 263.34
train mean loss: 261.82
epoch train time: 0:00:17.248472
elapsed time: 0:17:27.880313
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-24 22:48:18.790835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.20
 ---- batch: 020 ----
mean loss: 265.35
 ---- batch: 030 ----
mean loss: 255.58
 ---- batch: 040 ----
mean loss: 263.77
 ---- batch: 050 ----
mean loss: 262.68
 ---- batch: 060 ----
mean loss: 263.87
 ---- batch: 070 ----
mean loss: 255.30
 ---- batch: 080 ----
mean loss: 257.89
 ---- batch: 090 ----
mean loss: 253.25
train mean loss: 259.43
epoch train time: 0:00:17.196465
elapsed time: 0:17:45.078002
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-24 22:48:35.988561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.18
 ---- batch: 020 ----
mean loss: 254.72
 ---- batch: 030 ----
mean loss: 251.33
 ---- batch: 040 ----
mean loss: 261.19
 ---- batch: 050 ----
mean loss: 256.73
 ---- batch: 060 ----
mean loss: 251.24
 ---- batch: 070 ----
mean loss: 263.52
 ---- batch: 080 ----
mean loss: 264.42
 ---- batch: 090 ----
mean loss: 262.48
train mean loss: 257.51
epoch train time: 0:00:17.081118
elapsed time: 0:18:02.160347
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-24 22:48:53.070842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.42
 ---- batch: 020 ----
mean loss: 254.29
 ---- batch: 030 ----
mean loss: 250.22
 ---- batch: 040 ----
mean loss: 248.62
 ---- batch: 050 ----
mean loss: 256.45
 ---- batch: 060 ----
mean loss: 265.28
 ---- batch: 070 ----
mean loss: 253.50
 ---- batch: 080 ----
mean loss: 252.87
 ---- batch: 090 ----
mean loss: 267.53
train mean loss: 256.28
epoch train time: 0:00:17.351732
elapsed time: 0:18:19.513217
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-24 22:49:10.423727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.37
 ---- batch: 020 ----
mean loss: 254.44
 ---- batch: 030 ----
mean loss: 262.25
 ---- batch: 040 ----
mean loss: 250.03
 ---- batch: 050 ----
mean loss: 258.22
 ---- batch: 060 ----
mean loss: 256.46
 ---- batch: 070 ----
mean loss: 256.52
 ---- batch: 080 ----
mean loss: 258.24
 ---- batch: 090 ----
mean loss: 251.93
train mean loss: 256.16
epoch train time: 0:00:17.424150
elapsed time: 0:18:36.938567
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-24 22:49:27.849069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.92
 ---- batch: 020 ----
mean loss: 249.99
 ---- batch: 030 ----
mean loss: 253.29
 ---- batch: 040 ----
mean loss: 251.54
 ---- batch: 050 ----
mean loss: 260.02
 ---- batch: 060 ----
mean loss: 246.00
 ---- batch: 070 ----
mean loss: 248.65
 ---- batch: 080 ----
mean loss: 245.74
 ---- batch: 090 ----
mean loss: 246.25
train mean loss: 250.58
epoch train time: 0:00:17.322406
elapsed time: 0:18:54.262111
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-24 22:49:45.172619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.42
 ---- batch: 020 ----
mean loss: 242.16
 ---- batch: 030 ----
mean loss: 255.64
 ---- batch: 040 ----
mean loss: 259.77
 ---- batch: 050 ----
mean loss: 254.88
 ---- batch: 060 ----
mean loss: 250.47
 ---- batch: 070 ----
mean loss: 245.33
 ---- batch: 080 ----
mean loss: 246.19
 ---- batch: 090 ----
mean loss: 245.00
train mean loss: 251.00
epoch train time: 0:00:17.404041
elapsed time: 0:19:11.667477
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-24 22:50:02.578036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.28
 ---- batch: 020 ----
mean loss: 249.44
 ---- batch: 030 ----
mean loss: 250.99
 ---- batch: 040 ----
mean loss: 251.40
 ---- batch: 050 ----
mean loss: 252.73
 ---- batch: 060 ----
mean loss: 252.61
 ---- batch: 070 ----
mean loss: 249.85
 ---- batch: 080 ----
mean loss: 252.98
 ---- batch: 090 ----
mean loss: 245.57
train mean loss: 249.57
epoch train time: 0:00:17.389922
elapsed time: 0:19:29.058639
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-24 22:50:19.969154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.51
 ---- batch: 020 ----
mean loss: 242.69
 ---- batch: 030 ----
mean loss: 252.92
 ---- batch: 040 ----
mean loss: 252.26
 ---- batch: 050 ----
mean loss: 250.67
 ---- batch: 060 ----
mean loss: 248.84
 ---- batch: 070 ----
mean loss: 241.71
 ---- batch: 080 ----
mean loss: 240.34
 ---- batch: 090 ----
mean loss: 241.94
train mean loss: 245.35
epoch train time: 0:00:17.396330
elapsed time: 0:19:46.456184
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-24 22:50:37.366667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.25
 ---- batch: 020 ----
mean loss: 245.95
 ---- batch: 030 ----
mean loss: 235.67
 ---- batch: 040 ----
mean loss: 241.44
 ---- batch: 050 ----
mean loss: 244.92
 ---- batch: 060 ----
mean loss: 245.43
 ---- batch: 070 ----
mean loss: 242.22
 ---- batch: 080 ----
mean loss: 248.72
 ---- batch: 090 ----
mean loss: 242.35
train mean loss: 243.67
epoch train time: 0:00:17.267901
elapsed time: 0:20:03.725319
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-24 22:50:54.635827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.55
 ---- batch: 020 ----
mean loss: 249.08
 ---- batch: 030 ----
mean loss: 233.58
 ---- batch: 040 ----
mean loss: 239.02
 ---- batch: 050 ----
mean loss: 238.86
 ---- batch: 060 ----
mean loss: 240.21
 ---- batch: 070 ----
mean loss: 252.78
 ---- batch: 080 ----
mean loss: 243.34
 ---- batch: 090 ----
mean loss: 244.44
train mean loss: 243.25
epoch train time: 0:00:17.473825
elapsed time: 0:20:21.200508
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-24 22:51:12.111066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.51
 ---- batch: 020 ----
mean loss: 240.35
 ---- batch: 030 ----
mean loss: 236.87
 ---- batch: 040 ----
mean loss: 241.52
 ---- batch: 050 ----
mean loss: 239.72
 ---- batch: 060 ----
mean loss: 236.01
 ---- batch: 070 ----
mean loss: 231.39
 ---- batch: 080 ----
mean loss: 252.19
 ---- batch: 090 ----
mean loss: 248.26
train mean loss: 242.58
epoch train time: 0:00:17.362599
elapsed time: 0:20:38.564376
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-24 22:51:29.474958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.61
 ---- batch: 020 ----
mean loss: 234.87
 ---- batch: 030 ----
mean loss: 240.67
 ---- batch: 040 ----
mean loss: 246.19
 ---- batch: 050 ----
mean loss: 251.99
 ---- batch: 060 ----
mean loss: 239.86
 ---- batch: 070 ----
mean loss: 243.30
 ---- batch: 080 ----
mean loss: 242.27
 ---- batch: 090 ----
mean loss: 240.65
train mean loss: 241.16
epoch train time: 0:00:17.341139
elapsed time: 0:20:55.906836
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-24 22:51:46.817384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.09
 ---- batch: 020 ----
mean loss: 237.01
 ---- batch: 030 ----
mean loss: 245.01
 ---- batch: 040 ----
mean loss: 241.18
 ---- batch: 050 ----
mean loss: 239.41
 ---- batch: 060 ----
mean loss: 245.81
 ---- batch: 070 ----
mean loss: 233.63
 ---- batch: 080 ----
mean loss: 242.41
 ---- batch: 090 ----
mean loss: 241.16
train mean loss: 240.29
epoch train time: 0:00:17.346249
elapsed time: 0:21:13.254308
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-24 22:52:04.164825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.53
 ---- batch: 020 ----
mean loss: 224.49
 ---- batch: 030 ----
mean loss: 240.59
 ---- batch: 040 ----
mean loss: 237.60
 ---- batch: 050 ----
mean loss: 243.52
 ---- batch: 060 ----
mean loss: 234.58
 ---- batch: 070 ----
mean loss: 238.06
 ---- batch: 080 ----
mean loss: 244.13
 ---- batch: 090 ----
mean loss: 239.49
train mean loss: 237.06
epoch train time: 0:00:17.396581
elapsed time: 0:21:30.652149
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-24 22:52:21.562669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.03
 ---- batch: 020 ----
mean loss: 244.58
 ---- batch: 030 ----
mean loss: 245.13
 ---- batch: 040 ----
mean loss: 233.98
 ---- batch: 050 ----
mean loss: 230.54
 ---- batch: 060 ----
mean loss: 239.71
 ---- batch: 070 ----
mean loss: 235.80
 ---- batch: 080 ----
mean loss: 239.31
 ---- batch: 090 ----
mean loss: 233.48
train mean loss: 236.89
epoch train time: 0:00:17.328690
elapsed time: 0:21:47.982070
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-24 22:52:38.892563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.26
 ---- batch: 020 ----
mean loss: 240.00
 ---- batch: 030 ----
mean loss: 238.53
 ---- batch: 040 ----
mean loss: 242.10
 ---- batch: 050 ----
mean loss: 231.16
 ---- batch: 060 ----
mean loss: 232.54
 ---- batch: 070 ----
mean loss: 234.23
 ---- batch: 080 ----
mean loss: 231.14
 ---- batch: 090 ----
mean loss: 234.47
train mean loss: 235.42
epoch train time: 0:00:16.955058
elapsed time: 0:22:04.938339
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-24 22:52:55.848836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.90
 ---- batch: 020 ----
mean loss: 231.14
 ---- batch: 030 ----
mean loss: 240.11
 ---- batch: 040 ----
mean loss: 241.39
 ---- batch: 050 ----
mean loss: 229.54
 ---- batch: 060 ----
mean loss: 231.26
 ---- batch: 070 ----
mean loss: 235.88
 ---- batch: 080 ----
mean loss: 236.42
 ---- batch: 090 ----
mean loss: 235.99
train mean loss: 234.85
epoch train time: 0:00:16.888244
elapsed time: 0:22:21.827783
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-24 22:53:12.738260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.59
 ---- batch: 020 ----
mean loss: 232.72
 ---- batch: 030 ----
mean loss: 230.64
 ---- batch: 040 ----
mean loss: 238.01
 ---- batch: 050 ----
mean loss: 246.72
 ---- batch: 060 ----
mean loss: 237.75
 ---- batch: 070 ----
mean loss: 235.50
 ---- batch: 080 ----
mean loss: 237.56
 ---- batch: 090 ----
mean loss: 235.97
train mean loss: 236.84
epoch train time: 0:00:16.905821
elapsed time: 0:22:38.734805
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-24 22:53:29.645304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.18
 ---- batch: 020 ----
mean loss: 233.00
 ---- batch: 030 ----
mean loss: 230.98
 ---- batch: 040 ----
mean loss: 244.77
 ---- batch: 050 ----
mean loss: 225.05
 ---- batch: 060 ----
mean loss: 230.46
 ---- batch: 070 ----
mean loss: 229.94
 ---- batch: 080 ----
mean loss: 233.14
 ---- batch: 090 ----
mean loss: 232.90
train mean loss: 231.99
epoch train time: 0:00:16.855991
elapsed time: 0:22:55.591957
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-24 22:53:46.502444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.55
 ---- batch: 020 ----
mean loss: 228.96
 ---- batch: 030 ----
mean loss: 230.54
 ---- batch: 040 ----
mean loss: 240.70
 ---- batch: 050 ----
mean loss: 229.59
 ---- batch: 060 ----
mean loss: 226.17
 ---- batch: 070 ----
mean loss: 242.92
 ---- batch: 080 ----
mean loss: 229.24
 ---- batch: 090 ----
mean loss: 230.05
train mean loss: 231.24
epoch train time: 0:00:16.972468
elapsed time: 0:23:12.565605
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-24 22:54:03.475967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.04
 ---- batch: 020 ----
mean loss: 230.89
 ---- batch: 030 ----
mean loss: 229.34
 ---- batch: 040 ----
mean loss: 238.26
 ---- batch: 050 ----
mean loss: 228.61
 ---- batch: 060 ----
mean loss: 235.68
 ---- batch: 070 ----
mean loss: 222.01
 ---- batch: 080 ----
mean loss: 231.62
 ---- batch: 090 ----
mean loss: 231.99
train mean loss: 230.48
epoch train time: 0:00:16.958981
elapsed time: 0:23:29.525794
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-24 22:54:20.436292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.08
 ---- batch: 020 ----
mean loss: 223.78
 ---- batch: 030 ----
mean loss: 226.21
 ---- batch: 040 ----
mean loss: 225.81
 ---- batch: 050 ----
mean loss: 227.07
 ---- batch: 060 ----
mean loss: 228.69
 ---- batch: 070 ----
mean loss: 230.07
 ---- batch: 080 ----
mean loss: 229.90
 ---- batch: 090 ----
mean loss: 235.88
train mean loss: 228.73
epoch train time: 0:00:17.068078
elapsed time: 0:23:46.595116
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-24 22:54:37.505706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.82
 ---- batch: 020 ----
mean loss: 234.33
 ---- batch: 030 ----
mean loss: 236.49
 ---- batch: 040 ----
mean loss: 228.97
 ---- batch: 050 ----
mean loss: 233.16
 ---- batch: 060 ----
mean loss: 226.20
 ---- batch: 070 ----
mean loss: 229.69
 ---- batch: 080 ----
mean loss: 222.14
 ---- batch: 090 ----
mean loss: 227.43
train mean loss: 229.35
epoch train time: 0:00:17.071620
elapsed time: 0:24:03.668096
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-24 22:54:54.578637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.68
 ---- batch: 020 ----
mean loss: 227.25
 ---- batch: 030 ----
mean loss: 227.11
 ---- batch: 040 ----
mean loss: 227.58
 ---- batch: 050 ----
mean loss: 228.19
 ---- batch: 060 ----
mean loss: 234.96
 ---- batch: 070 ----
mean loss: 221.31
 ---- batch: 080 ----
mean loss: 233.09
 ---- batch: 090 ----
mean loss: 219.73
train mean loss: 227.20
epoch train time: 0:00:17.020723
elapsed time: 0:24:20.690157
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-24 22:55:11.600701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.56
 ---- batch: 020 ----
mean loss: 223.40
 ---- batch: 030 ----
mean loss: 228.20
 ---- batch: 040 ----
mean loss: 223.04
 ---- batch: 050 ----
mean loss: 224.76
 ---- batch: 060 ----
mean loss: 224.74
 ---- batch: 070 ----
mean loss: 226.79
 ---- batch: 080 ----
mean loss: 223.58
 ---- batch: 090 ----
mean loss: 226.43
train mean loss: 225.93
epoch train time: 0:00:17.066672
elapsed time: 0:24:37.758136
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-24 22:55:28.668570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.24
 ---- batch: 020 ----
mean loss: 222.56
 ---- batch: 030 ----
mean loss: 225.23
 ---- batch: 040 ----
mean loss: 222.97
 ---- batch: 050 ----
mean loss: 227.33
 ---- batch: 060 ----
mean loss: 223.50
 ---- batch: 070 ----
mean loss: 217.28
 ---- batch: 080 ----
mean loss: 226.97
 ---- batch: 090 ----
mean loss: 236.48
train mean loss: 225.21
epoch train time: 0:00:16.986306
elapsed time: 0:24:54.745626
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-24 22:55:45.656135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.05
 ---- batch: 020 ----
mean loss: 234.40
 ---- batch: 030 ----
mean loss: 226.04
 ---- batch: 040 ----
mean loss: 233.74
 ---- batch: 050 ----
mean loss: 224.09
 ---- batch: 060 ----
mean loss: 215.29
 ---- batch: 070 ----
mean loss: 215.13
 ---- batch: 080 ----
mean loss: 220.07
 ---- batch: 090 ----
mean loss: 232.67
train mean loss: 225.74
epoch train time: 0:00:17.102984
elapsed time: 0:25:11.849780
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-24 22:56:02.760322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.40
 ---- batch: 020 ----
mean loss: 221.33
 ---- batch: 030 ----
mean loss: 231.22
 ---- batch: 040 ----
mean loss: 228.86
 ---- batch: 050 ----
mean loss: 221.80
 ---- batch: 060 ----
mean loss: 226.39
 ---- batch: 070 ----
mean loss: 219.88
 ---- batch: 080 ----
mean loss: 221.04
 ---- batch: 090 ----
mean loss: 216.94
train mean loss: 223.32
epoch train time: 0:00:16.963844
elapsed time: 0:25:28.814969
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-24 22:56:19.725499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.94
 ---- batch: 020 ----
mean loss: 226.03
 ---- batch: 030 ----
mean loss: 224.68
 ---- batch: 040 ----
mean loss: 222.37
 ---- batch: 050 ----
mean loss: 207.89
 ---- batch: 060 ----
mean loss: 227.61
 ---- batch: 070 ----
mean loss: 228.27
 ---- batch: 080 ----
mean loss: 222.24
 ---- batch: 090 ----
mean loss: 223.39
train mean loss: 223.25
epoch train time: 0:00:17.007097
elapsed time: 0:25:45.823297
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-24 22:56:36.733831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.93
 ---- batch: 020 ----
mean loss: 221.84
 ---- batch: 030 ----
mean loss: 220.86
 ---- batch: 040 ----
mean loss: 223.81
 ---- batch: 050 ----
mean loss: 217.13
 ---- batch: 060 ----
mean loss: 227.44
 ---- batch: 070 ----
mean loss: 219.19
 ---- batch: 080 ----
mean loss: 220.98
 ---- batch: 090 ----
mean loss: 216.79
train mean loss: 222.00
epoch train time: 0:00:16.936645
elapsed time: 0:26:02.761193
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-24 22:56:53.671699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.68
 ---- batch: 020 ----
mean loss: 225.21
 ---- batch: 030 ----
mean loss: 224.30
 ---- batch: 040 ----
mean loss: 221.81
 ---- batch: 050 ----
mean loss: 222.04
 ---- batch: 060 ----
mean loss: 220.38
 ---- batch: 070 ----
mean loss: 217.67
 ---- batch: 080 ----
mean loss: 219.70
 ---- batch: 090 ----
mean loss: 222.50
train mean loss: 221.44
epoch train time: 0:00:16.980520
elapsed time: 0:26:19.742905
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-24 22:57:10.653415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.35
 ---- batch: 020 ----
mean loss: 216.92
 ---- batch: 030 ----
mean loss: 207.88
 ---- batch: 040 ----
mean loss: 217.99
 ---- batch: 050 ----
mean loss: 228.05
 ---- batch: 060 ----
mean loss: 225.86
 ---- batch: 070 ----
mean loss: 217.21
 ---- batch: 080 ----
mean loss: 224.34
 ---- batch: 090 ----
mean loss: 220.38
train mean loss: 219.61
epoch train time: 0:00:16.985160
elapsed time: 0:26:36.729221
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-24 22:57:27.639746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.75
 ---- batch: 020 ----
mean loss: 226.91
 ---- batch: 030 ----
mean loss: 217.06
 ---- batch: 040 ----
mean loss: 221.81
 ---- batch: 050 ----
mean loss: 226.23
 ---- batch: 060 ----
mean loss: 228.53
 ---- batch: 070 ----
mean loss: 222.07
 ---- batch: 080 ----
mean loss: 218.87
 ---- batch: 090 ----
mean loss: 214.01
train mean loss: 221.11
epoch train time: 0:00:16.953772
elapsed time: 0:26:53.684195
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-24 22:57:44.594688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.54
 ---- batch: 020 ----
mean loss: 227.08
 ---- batch: 030 ----
mean loss: 221.29
 ---- batch: 040 ----
mean loss: 218.71
 ---- batch: 050 ----
mean loss: 214.87
 ---- batch: 060 ----
mean loss: 217.86
 ---- batch: 070 ----
mean loss: 216.27
 ---- batch: 080 ----
mean loss: 226.88
 ---- batch: 090 ----
mean loss: 217.14
train mean loss: 219.57
epoch train time: 0:00:16.961867
elapsed time: 0:27:10.647221
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-24 22:58:01.557809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.72
 ---- batch: 020 ----
mean loss: 225.77
 ---- batch: 030 ----
mean loss: 220.81
 ---- batch: 040 ----
mean loss: 224.01
 ---- batch: 050 ----
mean loss: 214.69
 ---- batch: 060 ----
mean loss: 224.48
 ---- batch: 070 ----
mean loss: 225.25
 ---- batch: 080 ----
mean loss: 217.20
 ---- batch: 090 ----
mean loss: 214.20
train mean loss: 220.28
epoch train time: 0:00:16.942043
elapsed time: 0:27:27.590493
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-24 22:58:18.500993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.64
 ---- batch: 020 ----
mean loss: 212.32
 ---- batch: 030 ----
mean loss: 227.22
 ---- batch: 040 ----
mean loss: 209.53
 ---- batch: 050 ----
mean loss: 215.32
 ---- batch: 060 ----
mean loss: 227.75
 ---- batch: 070 ----
mean loss: 222.31
 ---- batch: 080 ----
mean loss: 218.36
 ---- batch: 090 ----
mean loss: 220.96
train mean loss: 218.63
epoch train time: 0:00:16.976219
elapsed time: 0:27:44.567954
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-24 22:58:35.478465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.04
 ---- batch: 020 ----
mean loss: 218.50
 ---- batch: 030 ----
mean loss: 219.32
 ---- batch: 040 ----
mean loss: 210.76
 ---- batch: 050 ----
mean loss: 215.79
 ---- batch: 060 ----
mean loss: 216.55
 ---- batch: 070 ----
mean loss: 213.60
 ---- batch: 080 ----
mean loss: 219.66
 ---- batch: 090 ----
mean loss: 220.18
train mean loss: 217.04
epoch train time: 0:00:16.974711
elapsed time: 0:28:01.543975
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-24 22:58:52.454503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.34
 ---- batch: 020 ----
mean loss: 215.53
 ---- batch: 030 ----
mean loss: 215.58
 ---- batch: 040 ----
mean loss: 221.56
 ---- batch: 050 ----
mean loss: 214.90
 ---- batch: 060 ----
mean loss: 213.07
 ---- batch: 070 ----
mean loss: 225.45
 ---- batch: 080 ----
mean loss: 212.84
 ---- batch: 090 ----
mean loss: 218.06
train mean loss: 218.18
epoch train time: 0:00:17.014357
elapsed time: 0:28:18.559503
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-24 22:59:09.470039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.12
 ---- batch: 020 ----
mean loss: 212.49
 ---- batch: 030 ----
mean loss: 215.71
 ---- batch: 040 ----
mean loss: 215.02
 ---- batch: 050 ----
mean loss: 212.67
 ---- batch: 060 ----
mean loss: 214.70
 ---- batch: 070 ----
mean loss: 225.30
 ---- batch: 080 ----
mean loss: 220.72
 ---- batch: 090 ----
mean loss: 223.31
train mean loss: 216.81
epoch train time: 0:00:17.013062
elapsed time: 0:28:35.573770
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-24 22:59:26.484245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.03
 ---- batch: 020 ----
mean loss: 219.50
 ---- batch: 030 ----
mean loss: 208.85
 ---- batch: 040 ----
mean loss: 211.52
 ---- batch: 050 ----
mean loss: 211.21
 ---- batch: 060 ----
mean loss: 219.41
 ---- batch: 070 ----
mean loss: 218.26
 ---- batch: 080 ----
mean loss: 221.55
 ---- batch: 090 ----
mean loss: 210.53
train mean loss: 215.33
epoch train time: 0:00:16.999564
elapsed time: 0:28:52.574452
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-24 22:59:43.485020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.05
 ---- batch: 020 ----
mean loss: 210.19
 ---- batch: 030 ----
mean loss: 213.47
 ---- batch: 040 ----
mean loss: 208.68
 ---- batch: 050 ----
mean loss: 222.24
 ---- batch: 060 ----
mean loss: 218.45
 ---- batch: 070 ----
mean loss: 212.58
 ---- batch: 080 ----
mean loss: 214.54
 ---- batch: 090 ----
mean loss: 213.18
train mean loss: 215.09
epoch train time: 0:00:16.990215
elapsed time: 0:29:09.565946
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-24 23:00:00.476465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.92
 ---- batch: 020 ----
mean loss: 211.92
 ---- batch: 030 ----
mean loss: 213.93
 ---- batch: 040 ----
mean loss: 216.34
 ---- batch: 050 ----
mean loss: 226.30
 ---- batch: 060 ----
mean loss: 209.48
 ---- batch: 070 ----
mean loss: 210.04
 ---- batch: 080 ----
mean loss: 215.87
 ---- batch: 090 ----
mean loss: 217.93
train mean loss: 215.89
epoch train time: 0:00:16.948617
elapsed time: 0:29:26.515772
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-24 23:00:17.426274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.67
 ---- batch: 020 ----
mean loss: 212.92
 ---- batch: 030 ----
mean loss: 216.91
 ---- batch: 040 ----
mean loss: 211.48
 ---- batch: 050 ----
mean loss: 216.32
 ---- batch: 060 ----
mean loss: 216.12
 ---- batch: 070 ----
mean loss: 209.77
 ---- batch: 080 ----
mean loss: 209.25
 ---- batch: 090 ----
mean loss: 215.07
train mean loss: 213.66
epoch train time: 0:00:17.002395
elapsed time: 0:29:43.519333
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-24 23:00:34.429858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.32
 ---- batch: 020 ----
mean loss: 209.09
 ---- batch: 030 ----
mean loss: 211.42
 ---- batch: 040 ----
mean loss: 211.80
 ---- batch: 050 ----
mean loss: 210.57
 ---- batch: 060 ----
mean loss: 214.46
 ---- batch: 070 ----
mean loss: 212.99
 ---- batch: 080 ----
mean loss: 210.68
 ---- batch: 090 ----
mean loss: 219.42
train mean loss: 213.47
epoch train time: 0:00:16.961547
elapsed time: 0:30:00.482048
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-24 23:00:51.392562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.06
 ---- batch: 020 ----
mean loss: 214.28
 ---- batch: 030 ----
mean loss: 211.84
 ---- batch: 040 ----
mean loss: 212.41
 ---- batch: 050 ----
mean loss: 211.76
 ---- batch: 060 ----
mean loss: 212.12
 ---- batch: 070 ----
mean loss: 218.12
 ---- batch: 080 ----
mean loss: 200.74
 ---- batch: 090 ----
mean loss: 214.01
train mean loss: 212.88
epoch train time: 0:00:16.972817
elapsed time: 0:30:17.456351
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-24 23:01:08.366865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.86
 ---- batch: 020 ----
mean loss: 218.72
 ---- batch: 030 ----
mean loss: 209.97
 ---- batch: 040 ----
mean loss: 215.19
 ---- batch: 050 ----
mean loss: 215.31
 ---- batch: 060 ----
mean loss: 214.09
 ---- batch: 070 ----
mean loss: 201.09
 ---- batch: 080 ----
mean loss: 208.01
 ---- batch: 090 ----
mean loss: 206.83
train mean loss: 212.22
epoch train time: 0:00:16.995633
elapsed time: 0:30:34.453193
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-24 23:01:25.363700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.36
 ---- batch: 020 ----
mean loss: 212.81
 ---- batch: 030 ----
mean loss: 223.35
 ---- batch: 040 ----
mean loss: 213.09
 ---- batch: 050 ----
mean loss: 207.75
 ---- batch: 060 ----
mean loss: 215.48
 ---- batch: 070 ----
mean loss: 209.36
 ---- batch: 080 ----
mean loss: 214.55
 ---- batch: 090 ----
mean loss: 217.85
train mean loss: 213.57
epoch train time: 0:00:17.024014
elapsed time: 0:30:51.478465
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-24 23:01:42.389039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.08
 ---- batch: 020 ----
mean loss: 217.62
 ---- batch: 030 ----
mean loss: 210.56
 ---- batch: 040 ----
mean loss: 204.67
 ---- batch: 050 ----
mean loss: 213.68
 ---- batch: 060 ----
mean loss: 212.89
 ---- batch: 070 ----
mean loss: 209.34
 ---- batch: 080 ----
mean loss: 211.97
 ---- batch: 090 ----
mean loss: 208.43
train mean loss: 210.91
epoch train time: 0:00:16.934818
elapsed time: 0:31:08.414808
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-24 23:01:59.325212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.93
 ---- batch: 020 ----
mean loss: 214.26
 ---- batch: 030 ----
mean loss: 208.59
 ---- batch: 040 ----
mean loss: 212.99
 ---- batch: 050 ----
mean loss: 204.18
 ---- batch: 060 ----
mean loss: 213.73
 ---- batch: 070 ----
mean loss: 210.25
 ---- batch: 080 ----
mean loss: 217.72
 ---- batch: 090 ----
mean loss: 212.96
train mean loss: 210.73
epoch train time: 0:00:16.912772
elapsed time: 0:31:25.328669
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-24 23:02:16.239199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.26
 ---- batch: 020 ----
mean loss: 213.58
 ---- batch: 030 ----
mean loss: 204.76
 ---- batch: 040 ----
mean loss: 209.57
 ---- batch: 050 ----
mean loss: 206.52
 ---- batch: 060 ----
mean loss: 216.81
 ---- batch: 070 ----
mean loss: 216.10
 ---- batch: 080 ----
mean loss: 216.27
 ---- batch: 090 ----
mean loss: 213.90
train mean loss: 213.18
epoch train time: 0:00:16.901178
elapsed time: 0:31:42.231051
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-24 23:02:33.141544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.20
 ---- batch: 020 ----
mean loss: 208.72
 ---- batch: 030 ----
mean loss: 211.91
 ---- batch: 040 ----
mean loss: 205.22
 ---- batch: 050 ----
mean loss: 207.96
 ---- batch: 060 ----
mean loss: 212.16
 ---- batch: 070 ----
mean loss: 212.66
 ---- batch: 080 ----
mean loss: 208.46
 ---- batch: 090 ----
mean loss: 207.76
train mean loss: 210.20
epoch train time: 0:00:16.905022
elapsed time: 0:31:59.137239
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-24 23:02:50.047820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.44
 ---- batch: 020 ----
mean loss: 208.33
 ---- batch: 030 ----
mean loss: 214.04
 ---- batch: 040 ----
mean loss: 210.83
 ---- batch: 050 ----
mean loss: 211.62
 ---- batch: 060 ----
mean loss: 218.49
 ---- batch: 070 ----
mean loss: 205.85
 ---- batch: 080 ----
mean loss: 206.57
 ---- batch: 090 ----
mean loss: 209.52
train mean loss: 211.35
epoch train time: 0:00:16.899313
elapsed time: 0:32:16.037986
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-24 23:03:06.948629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.80
 ---- batch: 020 ----
mean loss: 210.24
 ---- batch: 030 ----
mean loss: 216.62
 ---- batch: 040 ----
mean loss: 213.07
 ---- batch: 050 ----
mean loss: 211.94
 ---- batch: 060 ----
mean loss: 207.96
 ---- batch: 070 ----
mean loss: 213.92
 ---- batch: 080 ----
mean loss: 204.97
 ---- batch: 090 ----
mean loss: 207.14
train mean loss: 209.22
epoch train time: 0:00:16.926222
elapsed time: 0:32:32.965535
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-24 23:03:23.876073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.95
 ---- batch: 020 ----
mean loss: 211.18
 ---- batch: 030 ----
mean loss: 205.78
 ---- batch: 040 ----
mean loss: 204.98
 ---- batch: 050 ----
mean loss: 206.17
 ---- batch: 060 ----
mean loss: 212.33
 ---- batch: 070 ----
mean loss: 221.08
 ---- batch: 080 ----
mean loss: 213.06
 ---- batch: 090 ----
mean loss: 212.35
train mean loss: 210.16
epoch train time: 0:00:16.936694
elapsed time: 0:32:49.903530
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-24 23:03:40.814053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.78
 ---- batch: 020 ----
mean loss: 214.30
 ---- batch: 030 ----
mean loss: 208.37
 ---- batch: 040 ----
mean loss: 215.04
 ---- batch: 050 ----
mean loss: 212.95
 ---- batch: 060 ----
mean loss: 218.35
 ---- batch: 070 ----
mean loss: 202.82
 ---- batch: 080 ----
mean loss: 208.36
 ---- batch: 090 ----
mean loss: 207.36
train mean loss: 211.86
epoch train time: 0:00:16.950916
elapsed time: 0:33:06.855643
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-24 23:03:57.766164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.43
 ---- batch: 020 ----
mean loss: 210.50
 ---- batch: 030 ----
mean loss: 204.54
 ---- batch: 040 ----
mean loss: 208.71
 ---- batch: 050 ----
mean loss: 210.23
 ---- batch: 060 ----
mean loss: 211.23
 ---- batch: 070 ----
mean loss: 212.29
 ---- batch: 080 ----
mean loss: 197.44
 ---- batch: 090 ----
mean loss: 212.49
train mean loss: 208.61
epoch train time: 0:00:16.938238
elapsed time: 0:33:23.795060
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-24 23:04:14.705640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.37
 ---- batch: 020 ----
mean loss: 220.11
 ---- batch: 030 ----
mean loss: 214.33
 ---- batch: 040 ----
mean loss: 203.08
 ---- batch: 050 ----
mean loss: 217.28
 ---- batch: 060 ----
mean loss: 211.54
 ---- batch: 070 ----
mean loss: 212.13
 ---- batch: 080 ----
mean loss: 205.08
 ---- batch: 090 ----
mean loss: 205.24
train mean loss: 210.75
epoch train time: 0:00:16.949571
elapsed time: 0:33:40.745925
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-24 23:04:31.656423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.43
 ---- batch: 020 ----
mean loss: 205.83
 ---- batch: 030 ----
mean loss: 206.44
 ---- batch: 040 ----
mean loss: 203.75
 ---- batch: 050 ----
mean loss: 209.20
 ---- batch: 060 ----
mean loss: 212.45
 ---- batch: 070 ----
mean loss: 217.45
 ---- batch: 080 ----
mean loss: 213.71
 ---- batch: 090 ----
mean loss: 214.16
train mean loss: 208.82
epoch train time: 0:00:16.902292
elapsed time: 0:33:57.649364
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-24 23:04:48.559851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.00
 ---- batch: 020 ----
mean loss: 203.07
 ---- batch: 030 ----
mean loss: 210.32
 ---- batch: 040 ----
mean loss: 208.12
 ---- batch: 050 ----
mean loss: 207.21
 ---- batch: 060 ----
mean loss: 207.57
 ---- batch: 070 ----
mean loss: 220.31
 ---- batch: 080 ----
mean loss: 213.56
 ---- batch: 090 ----
mean loss: 221.44
train mean loss: 210.90
epoch train time: 0:00:16.940438
elapsed time: 0:34:14.590968
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-24 23:05:05.501509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.68
 ---- batch: 020 ----
mean loss: 211.58
 ---- batch: 030 ----
mean loss: 209.76
 ---- batch: 040 ----
mean loss: 213.67
 ---- batch: 050 ----
mean loss: 209.91
 ---- batch: 060 ----
mean loss: 206.37
 ---- batch: 070 ----
mean loss: 205.90
 ---- batch: 080 ----
mean loss: 210.27
 ---- batch: 090 ----
mean loss: 201.13
train mean loss: 207.77
epoch train time: 0:00:16.971931
elapsed time: 0:34:31.564122
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-24 23:05:22.474614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.04
 ---- batch: 020 ----
mean loss: 206.54
 ---- batch: 030 ----
mean loss: 205.09
 ---- batch: 040 ----
mean loss: 214.63
 ---- batch: 050 ----
mean loss: 203.24
 ---- batch: 060 ----
mean loss: 210.23
 ---- batch: 070 ----
mean loss: 204.72
 ---- batch: 080 ----
mean loss: 209.77
 ---- batch: 090 ----
mean loss: 203.12
train mean loss: 207.48
epoch train time: 0:00:16.963615
elapsed time: 0:34:48.528972
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-24 23:05:39.439533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.13
 ---- batch: 020 ----
mean loss: 207.06
 ---- batch: 030 ----
mean loss: 202.88
 ---- batch: 040 ----
mean loss: 211.83
 ---- batch: 050 ----
mean loss: 211.96
 ---- batch: 060 ----
mean loss: 205.86
 ---- batch: 070 ----
mean loss: 200.64
 ---- batch: 080 ----
mean loss: 211.88
 ---- batch: 090 ----
mean loss: 201.82
train mean loss: 206.81
epoch train time: 0:00:16.918225
elapsed time: 0:35:05.448506
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-24 23:05:56.358891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.82
 ---- batch: 020 ----
mean loss: 207.69
 ---- batch: 030 ----
mean loss: 213.17
 ---- batch: 040 ----
mean loss: 211.84
 ---- batch: 050 ----
mean loss: 216.02
 ---- batch: 060 ----
mean loss: 204.16
 ---- batch: 070 ----
mean loss: 209.92
 ---- batch: 080 ----
mean loss: 212.65
 ---- batch: 090 ----
mean loss: 205.01
train mean loss: 208.28
epoch train time: 0:00:16.948089
elapsed time: 0:35:22.397712
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-24 23:06:13.308228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.62
 ---- batch: 020 ----
mean loss: 209.57
 ---- batch: 030 ----
mean loss: 203.89
 ---- batch: 040 ----
mean loss: 212.17
 ---- batch: 050 ----
mean loss: 203.76
 ---- batch: 060 ----
mean loss: 205.42
 ---- batch: 070 ----
mean loss: 197.94
 ---- batch: 080 ----
mean loss: 201.75
 ---- batch: 090 ----
mean loss: 202.13
train mean loss: 205.12
epoch train time: 0:00:16.974127
elapsed time: 0:35:39.373020
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-24 23:06:30.283585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.04
 ---- batch: 020 ----
mean loss: 196.25
 ---- batch: 030 ----
mean loss: 198.80
 ---- batch: 040 ----
mean loss: 210.14
 ---- batch: 050 ----
mean loss: 210.63
 ---- batch: 060 ----
mean loss: 208.87
 ---- batch: 070 ----
mean loss: 208.40
 ---- batch: 080 ----
mean loss: 207.47
 ---- batch: 090 ----
mean loss: 204.00
train mean loss: 205.93
epoch train time: 0:00:17.053608
elapsed time: 0:35:56.427892
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-24 23:06:47.338388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.37
 ---- batch: 020 ----
mean loss: 203.42
 ---- batch: 030 ----
mean loss: 199.95
 ---- batch: 040 ----
mean loss: 207.71
 ---- batch: 050 ----
mean loss: 205.06
 ---- batch: 060 ----
mean loss: 208.32
 ---- batch: 070 ----
mean loss: 209.22
 ---- batch: 080 ----
mean loss: 207.19
 ---- batch: 090 ----
mean loss: 209.31
train mean loss: 205.54
epoch train time: 0:00:16.935193
elapsed time: 0:36:13.364276
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-24 23:07:04.274869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.74
 ---- batch: 020 ----
mean loss: 206.58
 ---- batch: 030 ----
mean loss: 202.07
 ---- batch: 040 ----
mean loss: 202.29
 ---- batch: 050 ----
mean loss: 205.63
 ---- batch: 060 ----
mean loss: 202.81
 ---- batch: 070 ----
mean loss: 204.54
 ---- batch: 080 ----
mean loss: 211.33
 ---- batch: 090 ----
mean loss: 209.02
train mean loss: 205.32
epoch train time: 0:00:16.964239
elapsed time: 0:36:30.329846
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-24 23:07:21.240561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.63
 ---- batch: 020 ----
mean loss: 206.70
 ---- batch: 030 ----
mean loss: 209.48
 ---- batch: 040 ----
mean loss: 207.09
 ---- batch: 050 ----
mean loss: 201.73
 ---- batch: 060 ----
mean loss: 203.72
 ---- batch: 070 ----
mean loss: 202.81
 ---- batch: 080 ----
mean loss: 206.83
 ---- batch: 090 ----
mean loss: 208.26
train mean loss: 204.87
epoch train time: 0:00:16.953118
elapsed time: 0:36:47.284726
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-24 23:07:38.195156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.06
 ---- batch: 020 ----
mean loss: 206.41
 ---- batch: 030 ----
mean loss: 203.70
 ---- batch: 040 ----
mean loss: 207.25
 ---- batch: 050 ----
mean loss: 203.64
 ---- batch: 060 ----
mean loss: 203.45
 ---- batch: 070 ----
mean loss: 199.67
 ---- batch: 080 ----
mean loss: 201.75
 ---- batch: 090 ----
mean loss: 214.32
train mean loss: 204.70
epoch train time: 0:00:16.962944
elapsed time: 0:37:04.248764
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-24 23:07:55.159337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.05
 ---- batch: 020 ----
mean loss: 197.66
 ---- batch: 030 ----
mean loss: 203.12
 ---- batch: 040 ----
mean loss: 208.93
 ---- batch: 050 ----
mean loss: 202.76
 ---- batch: 060 ----
mean loss: 200.84
 ---- batch: 070 ----
mean loss: 205.64
 ---- batch: 080 ----
mean loss: 208.22
 ---- batch: 090 ----
mean loss: 211.22
train mean loss: 205.28
epoch train time: 0:00:16.969470
elapsed time: 0:37:21.219634
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-24 23:08:12.130163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.52
 ---- batch: 020 ----
mean loss: 203.62
 ---- batch: 030 ----
mean loss: 202.20
 ---- batch: 040 ----
mean loss: 207.60
 ---- batch: 050 ----
mean loss: 202.69
 ---- batch: 060 ----
mean loss: 200.45
 ---- batch: 070 ----
mean loss: 208.03
 ---- batch: 080 ----
mean loss: 210.73
 ---- batch: 090 ----
mean loss: 207.37
train mean loss: 205.69
epoch train time: 0:00:16.945240
elapsed time: 0:37:38.166097
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-24 23:08:29.076608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.11
 ---- batch: 020 ----
mean loss: 201.53
 ---- batch: 030 ----
mean loss: 201.83
 ---- batch: 040 ----
mean loss: 205.58
 ---- batch: 050 ----
mean loss: 203.29
 ---- batch: 060 ----
mean loss: 201.86
 ---- batch: 070 ----
mean loss: 203.32
 ---- batch: 080 ----
mean loss: 201.67
 ---- batch: 090 ----
mean loss: 212.97
train mean loss: 203.68
epoch train time: 0:00:16.957052
elapsed time: 0:37:55.124322
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-24 23:08:46.034893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.82
 ---- batch: 020 ----
mean loss: 202.29
 ---- batch: 030 ----
mean loss: 212.52
 ---- batch: 040 ----
mean loss: 214.40
 ---- batch: 050 ----
mean loss: 207.67
 ---- batch: 060 ----
mean loss: 196.27
 ---- batch: 070 ----
mean loss: 204.97
 ---- batch: 080 ----
mean loss: 210.86
 ---- batch: 090 ----
mean loss: 204.77
train mean loss: 207.36
epoch train time: 0:00:16.970773
elapsed time: 0:38:12.096384
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-24 23:09:03.007065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.19
 ---- batch: 020 ----
mean loss: 202.92
 ---- batch: 030 ----
mean loss: 200.67
 ---- batch: 040 ----
mean loss: 198.04
 ---- batch: 050 ----
mean loss: 205.00
 ---- batch: 060 ----
mean loss: 204.77
 ---- batch: 070 ----
mean loss: 201.91
 ---- batch: 080 ----
mean loss: 209.58
 ---- batch: 090 ----
mean loss: 199.00
train mean loss: 203.11
epoch train time: 0:00:16.979518
elapsed time: 0:38:29.077243
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-24 23:09:19.987751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.21
 ---- batch: 020 ----
mean loss: 195.66
 ---- batch: 030 ----
mean loss: 199.11
 ---- batch: 040 ----
mean loss: 205.14
 ---- batch: 050 ----
mean loss: 206.10
 ---- batch: 060 ----
mean loss: 198.43
 ---- batch: 070 ----
mean loss: 201.73
 ---- batch: 080 ----
mean loss: 203.99
 ---- batch: 090 ----
mean loss: 208.25
train mean loss: 203.60
epoch train time: 0:00:16.969007
elapsed time: 0:38:46.047489
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-24 23:09:36.958045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.55
 ---- batch: 020 ----
mean loss: 204.41
 ---- batch: 030 ----
mean loss: 200.09
 ---- batch: 040 ----
mean loss: 202.35
 ---- batch: 050 ----
mean loss: 204.59
 ---- batch: 060 ----
mean loss: 212.02
 ---- batch: 070 ----
mean loss: 202.88
 ---- batch: 080 ----
mean loss: 201.15
 ---- batch: 090 ----
mean loss: 200.23
train mean loss: 203.41
epoch train time: 0:00:16.983034
elapsed time: 0:39:03.031851
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-24 23:09:53.942373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.77
 ---- batch: 020 ----
mean loss: 206.17
 ---- batch: 030 ----
mean loss: 202.64
 ---- batch: 040 ----
mean loss: 205.51
 ---- batch: 050 ----
mean loss: 197.81
 ---- batch: 060 ----
mean loss: 198.68
 ---- batch: 070 ----
mean loss: 198.89
 ---- batch: 080 ----
mean loss: 202.87
 ---- batch: 090 ----
mean loss: 209.79
train mean loss: 203.79
epoch train time: 0:00:17.037490
elapsed time: 0:39:20.070679
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-24 23:10:10.981066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.88
 ---- batch: 020 ----
mean loss: 199.83
 ---- batch: 030 ----
mean loss: 205.69
 ---- batch: 040 ----
mean loss: 202.73
 ---- batch: 050 ----
mean loss: 200.54
 ---- batch: 060 ----
mean loss: 204.99
 ---- batch: 070 ----
mean loss: 202.41
 ---- batch: 080 ----
mean loss: 199.96
 ---- batch: 090 ----
mean loss: 203.85
train mean loss: 202.59
epoch train time: 0:00:17.024406
elapsed time: 0:39:37.096235
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-24 23:10:28.006786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.62
 ---- batch: 020 ----
mean loss: 202.77
 ---- batch: 030 ----
mean loss: 199.49
 ---- batch: 040 ----
mean loss: 203.75
 ---- batch: 050 ----
mean loss: 199.66
 ---- batch: 060 ----
mean loss: 203.63
 ---- batch: 070 ----
mean loss: 208.17
 ---- batch: 080 ----
mean loss: 205.29
 ---- batch: 090 ----
mean loss: 198.72
train mean loss: 203.04
epoch train time: 0:00:17.059512
elapsed time: 0:39:54.157018
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-24 23:10:45.067539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.36
 ---- batch: 020 ----
mean loss: 208.44
 ---- batch: 030 ----
mean loss: 208.26
 ---- batch: 040 ----
mean loss: 201.39
 ---- batch: 050 ----
mean loss: 208.07
 ---- batch: 060 ----
mean loss: 201.17
 ---- batch: 070 ----
mean loss: 203.21
 ---- batch: 080 ----
mean loss: 202.84
 ---- batch: 090 ----
mean loss: 209.29
train mean loss: 205.46
epoch train time: 0:00:16.978944
elapsed time: 0:40:11.137191
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-24 23:11:02.047763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.50
 ---- batch: 020 ----
mean loss: 198.58
 ---- batch: 030 ----
mean loss: 204.63
 ---- batch: 040 ----
mean loss: 201.79
 ---- batch: 050 ----
mean loss: 197.11
 ---- batch: 060 ----
mean loss: 205.18
 ---- batch: 070 ----
mean loss: 203.22
 ---- batch: 080 ----
mean loss: 210.41
 ---- batch: 090 ----
mean loss: 197.68
train mean loss: 201.57
epoch train time: 0:00:16.995079
elapsed time: 0:40:28.133516
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-24 23:11:19.044037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.07
 ---- batch: 020 ----
mean loss: 202.35
 ---- batch: 030 ----
mean loss: 198.42
 ---- batch: 040 ----
mean loss: 201.40
 ---- batch: 050 ----
mean loss: 204.97
 ---- batch: 060 ----
mean loss: 202.14
 ---- batch: 070 ----
mean loss: 202.73
 ---- batch: 080 ----
mean loss: 201.67
 ---- batch: 090 ----
mean loss: 208.51
train mean loss: 202.11
epoch train time: 0:00:16.968120
elapsed time: 0:40:45.102826
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-24 23:11:36.013313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.31
 ---- batch: 020 ----
mean loss: 203.50
 ---- batch: 030 ----
mean loss: 209.07
 ---- batch: 040 ----
mean loss: 189.55
 ---- batch: 050 ----
mean loss: 205.20
 ---- batch: 060 ----
mean loss: 200.06
 ---- batch: 070 ----
mean loss: 202.55
 ---- batch: 080 ----
mean loss: 199.42
 ---- batch: 090 ----
mean loss: 205.81
train mean loss: 201.60
epoch train time: 0:00:17.064333
elapsed time: 0:41:02.168410
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-24 23:11:53.078922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.42
 ---- batch: 020 ----
mean loss: 199.80
 ---- batch: 030 ----
mean loss: 206.04
 ---- batch: 040 ----
mean loss: 207.22
 ---- batch: 050 ----
mean loss: 207.94
 ---- batch: 060 ----
mean loss: 203.52
 ---- batch: 070 ----
mean loss: 206.71
 ---- batch: 080 ----
mean loss: 214.42
 ---- batch: 090 ----
mean loss: 200.18
train mean loss: 205.22
epoch train time: 0:00:17.027397
elapsed time: 0:41:19.196993
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-24 23:12:10.107597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.99
 ---- batch: 020 ----
mean loss: 206.37
 ---- batch: 030 ----
mean loss: 194.41
 ---- batch: 040 ----
mean loss: 205.44
 ---- batch: 050 ----
mean loss: 196.55
 ---- batch: 060 ----
mean loss: 192.83
 ---- batch: 070 ----
mean loss: 203.33
 ---- batch: 080 ----
mean loss: 196.79
 ---- batch: 090 ----
mean loss: 197.36
train mean loss: 200.96
epoch train time: 0:00:16.958405
elapsed time: 0:41:36.156752
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-24 23:12:27.067362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.51
 ---- batch: 020 ----
mean loss: 209.74
 ---- batch: 030 ----
mean loss: 200.20
 ---- batch: 040 ----
mean loss: 200.57
 ---- batch: 050 ----
mean loss: 192.03
 ---- batch: 060 ----
mean loss: 208.03
 ---- batch: 070 ----
mean loss: 202.84
 ---- batch: 080 ----
mean loss: 198.93
 ---- batch: 090 ----
mean loss: 204.43
train mean loss: 201.91
epoch train time: 0:00:16.929338
elapsed time: 0:41:53.087414
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-24 23:12:43.997908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.10
 ---- batch: 020 ----
mean loss: 205.36
 ---- batch: 030 ----
mean loss: 195.74
 ---- batch: 040 ----
mean loss: 202.97
 ---- batch: 050 ----
mean loss: 204.71
 ---- batch: 060 ----
mean loss: 204.16
 ---- batch: 070 ----
mean loss: 195.44
 ---- batch: 080 ----
mean loss: 205.03
 ---- batch: 090 ----
mean loss: 200.39
train mean loss: 200.43
epoch train time: 0:00:16.925994
elapsed time: 0:42:10.014552
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-24 23:13:00.925053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.06
 ---- batch: 020 ----
mean loss: 202.88
 ---- batch: 030 ----
mean loss: 203.65
 ---- batch: 040 ----
mean loss: 201.81
 ---- batch: 050 ----
mean loss: 200.05
 ---- batch: 060 ----
mean loss: 200.66
 ---- batch: 070 ----
mean loss: 201.46
 ---- batch: 080 ----
mean loss: 193.03
 ---- batch: 090 ----
mean loss: 197.57
train mean loss: 200.15
epoch train time: 0:00:17.039414
elapsed time: 0:42:27.055145
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-24 23:13:17.965718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.49
 ---- batch: 020 ----
mean loss: 204.86
 ---- batch: 030 ----
mean loss: 202.05
 ---- batch: 040 ----
mean loss: 199.63
 ---- batch: 050 ----
mean loss: 202.59
 ---- batch: 060 ----
mean loss: 204.92
 ---- batch: 070 ----
mean loss: 194.82
 ---- batch: 080 ----
mean loss: 201.54
 ---- batch: 090 ----
mean loss: 203.09
train mean loss: 200.81
epoch train time: 0:00:16.979519
elapsed time: 0:42:44.036028
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-24 23:13:34.946665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.46
 ---- batch: 020 ----
mean loss: 199.65
 ---- batch: 030 ----
mean loss: 200.84
 ---- batch: 040 ----
mean loss: 201.04
 ---- batch: 050 ----
mean loss: 192.46
 ---- batch: 060 ----
mean loss: 196.64
 ---- batch: 070 ----
mean loss: 201.87
 ---- batch: 080 ----
mean loss: 205.41
 ---- batch: 090 ----
mean loss: 201.13
train mean loss: 200.11
epoch train time: 0:00:16.952271
elapsed time: 0:43:00.990203
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-24 23:13:51.900353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.92
 ---- batch: 020 ----
mean loss: 203.40
 ---- batch: 030 ----
mean loss: 200.32
 ---- batch: 040 ----
mean loss: 203.74
 ---- batch: 050 ----
mean loss: 199.64
 ---- batch: 060 ----
mean loss: 207.24
 ---- batch: 070 ----
mean loss: 203.57
 ---- batch: 080 ----
mean loss: 197.87
 ---- batch: 090 ----
mean loss: 199.10
train mean loss: 201.21
epoch train time: 0:00:16.965475
elapsed time: 0:43:17.956527
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-24 23:14:08.867043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.32
 ---- batch: 020 ----
mean loss: 207.03
 ---- batch: 030 ----
mean loss: 197.50
 ---- batch: 040 ----
mean loss: 205.46
 ---- batch: 050 ----
mean loss: 199.18
 ---- batch: 060 ----
mean loss: 203.46
 ---- batch: 070 ----
mean loss: 192.35
 ---- batch: 080 ----
mean loss: 206.01
 ---- batch: 090 ----
mean loss: 198.93
train mean loss: 202.03
epoch train time: 0:00:16.963174
elapsed time: 0:43:34.920879
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-24 23:14:25.831438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.60
 ---- batch: 020 ----
mean loss: 198.80
 ---- batch: 030 ----
mean loss: 197.37
 ---- batch: 040 ----
mean loss: 199.20
 ---- batch: 050 ----
mean loss: 199.71
 ---- batch: 060 ----
mean loss: 213.68
 ---- batch: 070 ----
mean loss: 204.76
 ---- batch: 080 ----
mean loss: 201.87
 ---- batch: 090 ----
mean loss: 207.12
train mean loss: 201.89
epoch train time: 0:00:16.942217
elapsed time: 0:43:51.864321
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-24 23:14:42.774822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.26
 ---- batch: 020 ----
mean loss: 202.96
 ---- batch: 030 ----
mean loss: 203.10
 ---- batch: 040 ----
mean loss: 194.32
 ---- batch: 050 ----
mean loss: 199.80
 ---- batch: 060 ----
mean loss: 201.23
 ---- batch: 070 ----
mean loss: 203.12
 ---- batch: 080 ----
mean loss: 194.18
 ---- batch: 090 ----
mean loss: 199.13
train mean loss: 199.74
epoch train time: 0:00:16.996236
elapsed time: 0:44:08.861720
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-24 23:14:59.772219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.56
 ---- batch: 020 ----
mean loss: 193.90
 ---- batch: 030 ----
mean loss: 206.58
 ---- batch: 040 ----
mean loss: 206.39
 ---- batch: 050 ----
mean loss: 199.58
 ---- batch: 060 ----
mean loss: 196.33
 ---- batch: 070 ----
mean loss: 195.21
 ---- batch: 080 ----
mean loss: 193.71
 ---- batch: 090 ----
mean loss: 196.65
train mean loss: 198.98
epoch train time: 0:00:16.986770
elapsed time: 0:44:25.849691
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-24 23:15:16.760219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.05
 ---- batch: 020 ----
mean loss: 199.52
 ---- batch: 030 ----
mean loss: 195.02
 ---- batch: 040 ----
mean loss: 199.85
 ---- batch: 050 ----
mean loss: 200.16
 ---- batch: 060 ----
mean loss: 209.19
 ---- batch: 070 ----
mean loss: 201.84
 ---- batch: 080 ----
mean loss: 202.49
 ---- batch: 090 ----
mean loss: 197.50
train mean loss: 200.20
epoch train time: 0:00:17.002834
elapsed time: 0:44:42.853690
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-24 23:15:33.764189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.96
 ---- batch: 020 ----
mean loss: 200.18
 ---- batch: 030 ----
mean loss: 202.22
 ---- batch: 040 ----
mean loss: 194.83
 ---- batch: 050 ----
mean loss: 192.18
 ---- batch: 060 ----
mean loss: 205.37
 ---- batch: 070 ----
mean loss: 197.06
 ---- batch: 080 ----
mean loss: 194.36
 ---- batch: 090 ----
mean loss: 195.33
train mean loss: 198.06
epoch train time: 0:00:17.041689
elapsed time: 0:44:59.896573
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-24 23:15:50.807118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.65
 ---- batch: 020 ----
mean loss: 195.29
 ---- batch: 030 ----
mean loss: 210.38
 ---- batch: 040 ----
mean loss: 204.64
 ---- batch: 050 ----
mean loss: 191.20
 ---- batch: 060 ----
mean loss: 198.71
 ---- batch: 070 ----
mean loss: 206.89
 ---- batch: 080 ----
mean loss: 220.64
 ---- batch: 090 ----
mean loss: 201.38
train mean loss: 202.59
epoch train time: 0:00:16.982121
elapsed time: 0:45:16.880063
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-24 23:16:07.790564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.02
 ---- batch: 020 ----
mean loss: 205.71
 ---- batch: 030 ----
mean loss: 197.85
 ---- batch: 040 ----
mean loss: 202.20
 ---- batch: 050 ----
mean loss: 202.87
 ---- batch: 060 ----
mean loss: 203.91
 ---- batch: 070 ----
mean loss: 196.13
 ---- batch: 080 ----
mean loss: 194.02
 ---- batch: 090 ----
mean loss: 194.85
train mean loss: 199.54
epoch train time: 0:00:16.972900
elapsed time: 0:45:33.854124
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-24 23:16:24.764603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.34
 ---- batch: 020 ----
mean loss: 196.88
 ---- batch: 030 ----
mean loss: 193.99
 ---- batch: 040 ----
mean loss: 194.94
 ---- batch: 050 ----
mean loss: 194.32
 ---- batch: 060 ----
mean loss: 194.74
 ---- batch: 070 ----
mean loss: 200.52
 ---- batch: 080 ----
mean loss: 203.57
 ---- batch: 090 ----
mean loss: 196.33
train mean loss: 198.28
epoch train time: 0:00:16.974873
elapsed time: 0:45:50.830224
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-24 23:16:41.740728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.29
 ---- batch: 020 ----
mean loss: 188.98
 ---- batch: 030 ----
mean loss: 202.31
 ---- batch: 040 ----
mean loss: 202.57
 ---- batch: 050 ----
mean loss: 203.44
 ---- batch: 060 ----
mean loss: 197.05
 ---- batch: 070 ----
mean loss: 201.49
 ---- batch: 080 ----
mean loss: 199.51
 ---- batch: 090 ----
mean loss: 197.13
train mean loss: 198.48
epoch train time: 0:00:16.966207
elapsed time: 0:46:07.797617
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-24 23:16:58.708184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.01
 ---- batch: 020 ----
mean loss: 199.04
 ---- batch: 030 ----
mean loss: 193.22
 ---- batch: 040 ----
mean loss: 200.77
 ---- batch: 050 ----
mean loss: 197.59
 ---- batch: 060 ----
mean loss: 196.14
 ---- batch: 070 ----
mean loss: 208.24
 ---- batch: 080 ----
mean loss: 195.64
 ---- batch: 090 ----
mean loss: 194.94
train mean loss: 197.77
epoch train time: 0:00:17.052750
elapsed time: 0:46:24.851609
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-24 23:17:15.762161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.00
 ---- batch: 020 ----
mean loss: 193.99
 ---- batch: 030 ----
mean loss: 202.03
 ---- batch: 040 ----
mean loss: 195.74
 ---- batch: 050 ----
mean loss: 196.98
 ---- batch: 060 ----
mean loss: 206.77
 ---- batch: 070 ----
mean loss: 204.24
 ---- batch: 080 ----
mean loss: 198.36
 ---- batch: 090 ----
mean loss: 198.72
train mean loss: 199.37
epoch train time: 0:00:16.954591
elapsed time: 0:46:41.807499
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-24 23:17:32.718068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.40
 ---- batch: 020 ----
mean loss: 206.57
 ---- batch: 030 ----
mean loss: 203.34
 ---- batch: 040 ----
mean loss: 205.00
 ---- batch: 050 ----
mean loss: 193.03
 ---- batch: 060 ----
mean loss: 199.27
 ---- batch: 070 ----
mean loss: 200.05
 ---- batch: 080 ----
mean loss: 200.03
 ---- batch: 090 ----
mean loss: 192.11
train mean loss: 198.85
epoch train time: 0:00:16.973124
elapsed time: 0:46:58.781896
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-24 23:17:49.692414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.14
 ---- batch: 020 ----
mean loss: 189.47
 ---- batch: 030 ----
mean loss: 195.90
 ---- batch: 040 ----
mean loss: 197.44
 ---- batch: 050 ----
mean loss: 198.74
 ---- batch: 060 ----
mean loss: 200.13
 ---- batch: 070 ----
mean loss: 198.07
 ---- batch: 080 ----
mean loss: 191.31
 ---- batch: 090 ----
mean loss: 198.41
train mean loss: 197.06
epoch train time: 0:00:16.964059
elapsed time: 0:47:15.747114
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-24 23:18:06.657601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.51
 ---- batch: 020 ----
mean loss: 193.58
 ---- batch: 030 ----
mean loss: 195.33
 ---- batch: 040 ----
mean loss: 191.52
 ---- batch: 050 ----
mean loss: 203.35
 ---- batch: 060 ----
mean loss: 199.99
 ---- batch: 070 ----
mean loss: 202.16
 ---- batch: 080 ----
mean loss: 197.60
 ---- batch: 090 ----
mean loss: 206.19
train mean loss: 198.43
epoch train time: 0:00:16.995113
elapsed time: 0:47:32.743403
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-24 23:18:23.653902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.22
 ---- batch: 020 ----
mean loss: 198.70
 ---- batch: 030 ----
mean loss: 191.91
 ---- batch: 040 ----
mean loss: 206.88
 ---- batch: 050 ----
mean loss: 197.44
 ---- batch: 060 ----
mean loss: 207.63
 ---- batch: 070 ----
mean loss: 194.39
 ---- batch: 080 ----
mean loss: 193.88
 ---- batch: 090 ----
mean loss: 200.33
train mean loss: 198.63
epoch train time: 0:00:16.968300
elapsed time: 0:47:49.712921
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-24 23:18:40.623468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.91
 ---- batch: 020 ----
mean loss: 196.33
 ---- batch: 030 ----
mean loss: 200.50
 ---- batch: 040 ----
mean loss: 196.73
 ---- batch: 050 ----
mean loss: 195.48
 ---- batch: 060 ----
mean loss: 193.44
 ---- batch: 070 ----
mean loss: 186.63
 ---- batch: 080 ----
mean loss: 201.87
 ---- batch: 090 ----
mean loss: 204.35
train mean loss: 197.56
epoch train time: 0:00:16.938446
elapsed time: 0:48:06.652558
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-24 23:18:57.563053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.86
 ---- batch: 020 ----
mean loss: 195.35
 ---- batch: 030 ----
mean loss: 201.00
 ---- batch: 040 ----
mean loss: 191.30
 ---- batch: 050 ----
mean loss: 197.61
 ---- batch: 060 ----
mean loss: 193.73
 ---- batch: 070 ----
mean loss: 201.56
 ---- batch: 080 ----
mean loss: 204.47
 ---- batch: 090 ----
mean loss: 196.51
train mean loss: 197.78
epoch train time: 0:00:16.943745
elapsed time: 0:48:23.597506
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-24 23:19:14.508003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.36
 ---- batch: 020 ----
mean loss: 200.96
 ---- batch: 030 ----
mean loss: 192.99
 ---- batch: 040 ----
mean loss: 201.78
 ---- batch: 050 ----
mean loss: 203.83
 ---- batch: 060 ----
mean loss: 207.05
 ---- batch: 070 ----
mean loss: 191.82
 ---- batch: 080 ----
mean loss: 199.61
 ---- batch: 090 ----
mean loss: 194.85
train mean loss: 199.48
epoch train time: 0:00:16.926520
elapsed time: 0:48:40.525207
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-24 23:19:31.435700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.29
 ---- batch: 020 ----
mean loss: 194.03
 ---- batch: 030 ----
mean loss: 191.75
 ---- batch: 040 ----
mean loss: 195.92
 ---- batch: 050 ----
mean loss: 195.17
 ---- batch: 060 ----
mean loss: 202.86
 ---- batch: 070 ----
mean loss: 199.40
 ---- batch: 080 ----
mean loss: 200.86
 ---- batch: 090 ----
mean loss: 192.82
train mean loss: 196.97
epoch train time: 0:00:16.952103
elapsed time: 0:48:57.478449
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-24 23:19:48.388994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.23
 ---- batch: 020 ----
mean loss: 196.24
 ---- batch: 030 ----
mean loss: 195.32
 ---- batch: 040 ----
mean loss: 196.27
 ---- batch: 050 ----
mean loss: 206.98
 ---- batch: 060 ----
mean loss: 196.01
 ---- batch: 070 ----
mean loss: 196.93
 ---- batch: 080 ----
mean loss: 187.02
 ---- batch: 090 ----
mean loss: 198.28
train mean loss: 197.69
epoch train time: 0:00:16.958571
elapsed time: 0:49:14.438386
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-24 23:20:05.348960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.40
 ---- batch: 020 ----
mean loss: 194.44
 ---- batch: 030 ----
mean loss: 192.26
 ---- batch: 040 ----
mean loss: 197.39
 ---- batch: 050 ----
mean loss: 194.94
 ---- batch: 060 ----
mean loss: 199.39
 ---- batch: 070 ----
mean loss: 202.71
 ---- batch: 080 ----
mean loss: 192.39
 ---- batch: 090 ----
mean loss: 202.67
train mean loss: 196.45
epoch train time: 0:00:16.939513
elapsed time: 0:49:31.379156
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-24 23:20:22.289647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.02
 ---- batch: 020 ----
mean loss: 195.25
 ---- batch: 030 ----
mean loss: 196.22
 ---- batch: 040 ----
mean loss: 198.57
 ---- batch: 050 ----
mean loss: 193.01
 ---- batch: 060 ----
mean loss: 194.43
 ---- batch: 070 ----
mean loss: 199.05
 ---- batch: 080 ----
mean loss: 195.28
 ---- batch: 090 ----
mean loss: 193.85
train mean loss: 196.70
epoch train time: 0:00:16.932989
elapsed time: 0:49:48.313287
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-24 23:20:39.223868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.80
 ---- batch: 020 ----
mean loss: 190.95
 ---- batch: 030 ----
mean loss: 198.68
 ---- batch: 040 ----
mean loss: 191.27
 ---- batch: 050 ----
mean loss: 193.51
 ---- batch: 060 ----
mean loss: 200.30
 ---- batch: 070 ----
mean loss: 200.54
 ---- batch: 080 ----
mean loss: 206.96
 ---- batch: 090 ----
mean loss: 191.65
train mean loss: 196.18
epoch train time: 0:00:16.987222
elapsed time: 0:50:05.301970
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-24 23:20:56.212111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.57
 ---- batch: 020 ----
mean loss: 201.51
 ---- batch: 030 ----
mean loss: 203.41
 ---- batch: 040 ----
mean loss: 188.33
 ---- batch: 050 ----
mean loss: 193.74
 ---- batch: 060 ----
mean loss: 202.98
 ---- batch: 070 ----
mean loss: 193.88
 ---- batch: 080 ----
mean loss: 197.45
 ---- batch: 090 ----
mean loss: 195.64
train mean loss: 196.78
epoch train time: 0:00:16.921438
elapsed time: 0:50:22.224272
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-24 23:21:13.134781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.20
 ---- batch: 020 ----
mean loss: 192.05
 ---- batch: 030 ----
mean loss: 193.76
 ---- batch: 040 ----
mean loss: 196.46
 ---- batch: 050 ----
mean loss: 199.07
 ---- batch: 060 ----
mean loss: 198.99
 ---- batch: 070 ----
mean loss: 201.34
 ---- batch: 080 ----
mean loss: 188.89
 ---- batch: 090 ----
mean loss: 195.96
train mean loss: 196.30
epoch train time: 0:00:16.956551
elapsed time: 0:50:39.181992
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-24 23:21:30.092476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.94
 ---- batch: 020 ----
mean loss: 196.14
 ---- batch: 030 ----
mean loss: 194.72
 ---- batch: 040 ----
mean loss: 197.00
 ---- batch: 050 ----
mean loss: 200.09
 ---- batch: 060 ----
mean loss: 193.50
 ---- batch: 070 ----
mean loss: 187.51
 ---- batch: 080 ----
mean loss: 188.72
 ---- batch: 090 ----
mean loss: 201.02
train mean loss: 194.44
epoch train time: 0:00:16.925366
elapsed time: 0:50:56.108493
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-24 23:21:47.019016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.59
 ---- batch: 020 ----
mean loss: 198.99
 ---- batch: 030 ----
mean loss: 196.30
 ---- batch: 040 ----
mean loss: 190.90
 ---- batch: 050 ----
mean loss: 189.53
 ---- batch: 060 ----
mean loss: 196.86
 ---- batch: 070 ----
mean loss: 200.68
 ---- batch: 080 ----
mean loss: 192.28
 ---- batch: 090 ----
mean loss: 192.13
train mean loss: 194.70
epoch train time: 0:00:16.921193
elapsed time: 0:51:13.030898
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-24 23:22:03.941437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.26
 ---- batch: 020 ----
mean loss: 196.94
 ---- batch: 030 ----
mean loss: 199.25
 ---- batch: 040 ----
mean loss: 196.12
 ---- batch: 050 ----
mean loss: 191.82
 ---- batch: 060 ----
mean loss: 197.65
 ---- batch: 070 ----
mean loss: 192.95
 ---- batch: 080 ----
mean loss: 185.21
 ---- batch: 090 ----
mean loss: 196.43
train mean loss: 194.80
epoch train time: 0:00:16.988435
elapsed time: 0:51:30.020555
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-24 23:22:20.931038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.79
 ---- batch: 020 ----
mean loss: 193.46
 ---- batch: 030 ----
mean loss: 197.61
 ---- batch: 040 ----
mean loss: 191.73
 ---- batch: 050 ----
mean loss: 198.11
 ---- batch: 060 ----
mean loss: 194.49
 ---- batch: 070 ----
mean loss: 191.35
 ---- batch: 080 ----
mean loss: 195.04
 ---- batch: 090 ----
mean loss: 196.84
train mean loss: 194.95
epoch train time: 0:00:16.882088
elapsed time: 0:51:46.903872
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-24 23:22:37.814388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.97
 ---- batch: 020 ----
mean loss: 193.45
 ---- batch: 030 ----
mean loss: 195.31
 ---- batch: 040 ----
mean loss: 198.17
 ---- batch: 050 ----
mean loss: 193.79
 ---- batch: 060 ----
mean loss: 187.01
 ---- batch: 070 ----
mean loss: 191.32
 ---- batch: 080 ----
mean loss: 193.08
 ---- batch: 090 ----
mean loss: 193.25
train mean loss: 194.07
epoch train time: 0:00:16.881183
elapsed time: 0:52:03.786395
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-24 23:22:54.696924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.47
 ---- batch: 020 ----
mean loss: 200.26
 ---- batch: 030 ----
mean loss: 195.44
 ---- batch: 040 ----
mean loss: 196.48
 ---- batch: 050 ----
mean loss: 191.79
 ---- batch: 060 ----
mean loss: 198.89
 ---- batch: 070 ----
mean loss: 194.25
 ---- batch: 080 ----
mean loss: 194.52
 ---- batch: 090 ----
mean loss: 191.57
train mean loss: 195.57
epoch train time: 0:00:16.880660
elapsed time: 0:52:20.668269
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-24 23:23:11.578823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.39
 ---- batch: 020 ----
mean loss: 194.31
 ---- batch: 030 ----
mean loss: 201.67
 ---- batch: 040 ----
mean loss: 197.98
 ---- batch: 050 ----
mean loss: 189.26
 ---- batch: 060 ----
mean loss: 200.03
 ---- batch: 070 ----
mean loss: 192.04
 ---- batch: 080 ----
mean loss: 196.84
 ---- batch: 090 ----
mean loss: 188.45
train mean loss: 195.11
epoch train time: 0:00:16.917618
elapsed time: 0:52:37.587178
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-24 23:23:28.497691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.31
 ---- batch: 020 ----
mean loss: 198.76
 ---- batch: 030 ----
mean loss: 192.82
 ---- batch: 040 ----
mean loss: 198.98
 ---- batch: 050 ----
mean loss: 198.08
 ---- batch: 060 ----
mean loss: 193.90
 ---- batch: 070 ----
mean loss: 191.69
 ---- batch: 080 ----
mean loss: 197.60
 ---- batch: 090 ----
mean loss: 190.66
train mean loss: 195.46
epoch train time: 0:00:16.952296
elapsed time: 0:52:54.540725
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-24 23:23:45.451247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.43
 ---- batch: 020 ----
mean loss: 195.39
 ---- batch: 030 ----
mean loss: 190.60
 ---- batch: 040 ----
mean loss: 197.47
 ---- batch: 050 ----
mean loss: 193.28
 ---- batch: 060 ----
mean loss: 198.34
 ---- batch: 070 ----
mean loss: 192.44
 ---- batch: 080 ----
mean loss: 191.43
 ---- batch: 090 ----
mean loss: 195.78
train mean loss: 194.56
epoch train time: 0:00:16.983730
elapsed time: 0:53:11.525670
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-24 23:24:02.436191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.50
 ---- batch: 020 ----
mean loss: 191.10
 ---- batch: 030 ----
mean loss: 189.59
 ---- batch: 040 ----
mean loss: 200.31
 ---- batch: 050 ----
mean loss: 205.11
 ---- batch: 060 ----
mean loss: 196.10
 ---- batch: 070 ----
mean loss: 194.98
 ---- batch: 080 ----
mean loss: 192.38
 ---- batch: 090 ----
mean loss: 195.93
train mean loss: 195.65
epoch train time: 0:00:16.941067
elapsed time: 0:53:28.468074
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-24 23:24:19.378579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.44
 ---- batch: 020 ----
mean loss: 188.59
 ---- batch: 030 ----
mean loss: 196.58
 ---- batch: 040 ----
mean loss: 193.14
 ---- batch: 050 ----
mean loss: 190.29
 ---- batch: 060 ----
mean loss: 196.92
 ---- batch: 070 ----
mean loss: 200.01
 ---- batch: 080 ----
mean loss: 189.94
 ---- batch: 090 ----
mean loss: 198.06
train mean loss: 194.05
epoch train time: 0:00:16.965699
elapsed time: 0:53:45.435055
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-24 23:24:36.345696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.68
 ---- batch: 020 ----
mean loss: 200.67
 ---- batch: 030 ----
mean loss: 191.81
 ---- batch: 040 ----
mean loss: 194.74
 ---- batch: 050 ----
mean loss: 195.71
 ---- batch: 060 ----
mean loss: 193.96
 ---- batch: 070 ----
mean loss: 194.24
 ---- batch: 080 ----
mean loss: 196.33
 ---- batch: 090 ----
mean loss: 193.47
train mean loss: 195.10
epoch train time: 0:00:16.939185
elapsed time: 0:54:02.375540
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-24 23:24:53.286078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.19
 ---- batch: 020 ----
mean loss: 201.38
 ---- batch: 030 ----
mean loss: 190.51
 ---- batch: 040 ----
mean loss: 189.58
 ---- batch: 050 ----
mean loss: 194.97
 ---- batch: 060 ----
mean loss: 197.34
 ---- batch: 070 ----
mean loss: 197.51
 ---- batch: 080 ----
mean loss: 197.19
 ---- batch: 090 ----
mean loss: 194.38
train mean loss: 195.45
epoch train time: 0:00:16.965081
elapsed time: 0:54:19.341810
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-24 23:25:10.252316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.15
 ---- batch: 020 ----
mean loss: 196.70
 ---- batch: 030 ----
mean loss: 208.76
 ---- batch: 040 ----
mean loss: 192.92
 ---- batch: 050 ----
mean loss: 192.12
 ---- batch: 060 ----
mean loss: 182.16
 ---- batch: 070 ----
mean loss: 190.39
 ---- batch: 080 ----
mean loss: 201.32
 ---- batch: 090 ----
mean loss: 201.70
train mean loss: 195.44
epoch train time: 0:00:16.944499
elapsed time: 0:54:36.287638
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-24 23:25:27.198259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.01
 ---- batch: 020 ----
mean loss: 191.15
 ---- batch: 030 ----
mean loss: 196.29
 ---- batch: 040 ----
mean loss: 207.04
 ---- batch: 050 ----
mean loss: 204.83
 ---- batch: 060 ----
mean loss: 194.34
 ---- batch: 070 ----
mean loss: 194.68
 ---- batch: 080 ----
mean loss: 196.90
 ---- batch: 090 ----
mean loss: 184.65
train mean loss: 195.43
epoch train time: 0:00:16.927429
elapsed time: 0:54:53.216344
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-24 23:25:44.126854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.23
 ---- batch: 020 ----
mean loss: 195.01
 ---- batch: 030 ----
mean loss: 196.16
 ---- batch: 040 ----
mean loss: 186.60
 ---- batch: 050 ----
mean loss: 191.33
 ---- batch: 060 ----
mean loss: 199.40
 ---- batch: 070 ----
mean loss: 193.59
 ---- batch: 080 ----
mean loss: 188.98
 ---- batch: 090 ----
mean loss: 193.74
train mean loss: 193.29
epoch train time: 0:00:16.971842
elapsed time: 0:55:10.189363
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-24 23:26:01.099849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.49
 ---- batch: 020 ----
mean loss: 197.32
 ---- batch: 030 ----
mean loss: 190.26
 ---- batch: 040 ----
mean loss: 192.45
 ---- batch: 050 ----
mean loss: 194.24
 ---- batch: 060 ----
mean loss: 196.68
 ---- batch: 070 ----
mean loss: 187.29
 ---- batch: 080 ----
mean loss: 186.80
 ---- batch: 090 ----
mean loss: 190.92
train mean loss: 192.21
epoch train time: 0:00:16.950520
elapsed time: 0:55:27.141039
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-24 23:26:18.051563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.92
 ---- batch: 020 ----
mean loss: 186.87
 ---- batch: 030 ----
mean loss: 193.27
 ---- batch: 040 ----
mean loss: 190.42
 ---- batch: 050 ----
mean loss: 187.77
 ---- batch: 060 ----
mean loss: 182.71
 ---- batch: 070 ----
mean loss: 200.29
 ---- batch: 080 ----
mean loss: 196.65
 ---- batch: 090 ----
mean loss: 200.13
train mean loss: 192.58
epoch train time: 0:00:16.926265
elapsed time: 0:55:44.068498
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-24 23:26:34.978968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.13
 ---- batch: 020 ----
mean loss: 193.45
 ---- batch: 030 ----
mean loss: 191.78
 ---- batch: 040 ----
mean loss: 192.91
 ---- batch: 050 ----
mean loss: 185.73
 ---- batch: 060 ----
mean loss: 194.55
 ---- batch: 070 ----
mean loss: 192.66
 ---- batch: 080 ----
mean loss: 194.62
 ---- batch: 090 ----
mean loss: 195.17
train mean loss: 192.87
epoch train time: 0:00:16.906080
elapsed time: 0:56:00.975739
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-24 23:26:51.886285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.08
 ---- batch: 020 ----
mean loss: 196.68
 ---- batch: 030 ----
mean loss: 200.07
 ---- batch: 040 ----
mean loss: 190.54
 ---- batch: 050 ----
mean loss: 191.60
 ---- batch: 060 ----
mean loss: 194.99
 ---- batch: 070 ----
mean loss: 194.21
 ---- batch: 080 ----
mean loss: 192.56
 ---- batch: 090 ----
mean loss: 190.93
train mean loss: 193.19
epoch train time: 0:00:16.912519
elapsed time: 0:56:17.889641
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-24 23:27:08.800223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.48
 ---- batch: 020 ----
mean loss: 194.79
 ---- batch: 030 ----
mean loss: 191.17
 ---- batch: 040 ----
mean loss: 191.70
 ---- batch: 050 ----
mean loss: 192.43
 ---- batch: 060 ----
mean loss: 191.93
 ---- batch: 070 ----
mean loss: 190.79
 ---- batch: 080 ----
mean loss: 186.74
 ---- batch: 090 ----
mean loss: 195.88
train mean loss: 192.62
epoch train time: 0:00:17.006052
elapsed time: 0:56:34.897034
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-24 23:27:25.807598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.32
 ---- batch: 020 ----
mean loss: 193.91
 ---- batch: 030 ----
mean loss: 193.69
 ---- batch: 040 ----
mean loss: 190.60
 ---- batch: 050 ----
mean loss: 189.79
 ---- batch: 060 ----
mean loss: 198.68
 ---- batch: 070 ----
mean loss: 186.93
 ---- batch: 080 ----
mean loss: 189.54
 ---- batch: 090 ----
mean loss: 198.90
train mean loss: 192.40
epoch train time: 0:00:16.955289
elapsed time: 0:56:51.853647
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-24 23:27:42.764158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.79
 ---- batch: 020 ----
mean loss: 187.59
 ---- batch: 030 ----
mean loss: 191.33
 ---- batch: 040 ----
mean loss: 199.27
 ---- batch: 050 ----
mean loss: 199.53
 ---- batch: 060 ----
mean loss: 192.90
 ---- batch: 070 ----
mean loss: 194.27
 ---- batch: 080 ----
mean loss: 196.68
 ---- batch: 090 ----
mean loss: 186.50
train mean loss: 192.77
epoch train time: 0:00:16.900871
elapsed time: 0:57:08.755770
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-24 23:27:59.666395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.23
 ---- batch: 020 ----
mean loss: 185.23
 ---- batch: 030 ----
mean loss: 195.33
 ---- batch: 040 ----
mean loss: 190.71
 ---- batch: 050 ----
mean loss: 191.18
 ---- batch: 060 ----
mean loss: 194.34
 ---- batch: 070 ----
mean loss: 191.28
 ---- batch: 080 ----
mean loss: 191.88
 ---- batch: 090 ----
mean loss: 190.41
train mean loss: 191.16
epoch train time: 0:00:16.895106
elapsed time: 0:57:25.652191
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-24 23:28:16.562790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.36
 ---- batch: 020 ----
mean loss: 192.06
 ---- batch: 030 ----
mean loss: 184.29
 ---- batch: 040 ----
mean loss: 192.22
 ---- batch: 050 ----
mean loss: 195.77
 ---- batch: 060 ----
mean loss: 189.57
 ---- batch: 070 ----
mean loss: 197.13
 ---- batch: 080 ----
mean loss: 194.31
 ---- batch: 090 ----
mean loss: 197.37
train mean loss: 192.71
epoch train time: 0:00:16.920439
elapsed time: 0:57:42.573920
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-24 23:28:33.484524
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.32
 ---- batch: 020 ----
mean loss: 189.45
 ---- batch: 030 ----
mean loss: 188.64
 ---- batch: 040 ----
mean loss: 196.51
 ---- batch: 050 ----
mean loss: 188.78
 ---- batch: 060 ----
mean loss: 182.89
 ---- batch: 070 ----
mean loss: 189.21
 ---- batch: 080 ----
mean loss: 192.47
 ---- batch: 090 ----
mean loss: 192.45
train mean loss: 191.02
epoch train time: 0:00:16.872166
elapsed time: 0:57:59.447987
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-24 23:28:50.358137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.13
 ---- batch: 020 ----
mean loss: 185.13
 ---- batch: 030 ----
mean loss: 190.20
 ---- batch: 040 ----
mean loss: 182.21
 ---- batch: 050 ----
mean loss: 192.30
 ---- batch: 060 ----
mean loss: 189.72
 ---- batch: 070 ----
mean loss: 185.49
 ---- batch: 080 ----
mean loss: 191.69
 ---- batch: 090 ----
mean loss: 188.34
train mean loss: 188.94
epoch train time: 0:00:16.918108
elapsed time: 0:58:16.366902
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-24 23:29:07.277443
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.34
 ---- batch: 020 ----
mean loss: 192.21
 ---- batch: 030 ----
mean loss: 184.78
 ---- batch: 040 ----
mean loss: 190.46
 ---- batch: 050 ----
mean loss: 192.62
 ---- batch: 060 ----
mean loss: 182.71
 ---- batch: 070 ----
mean loss: 193.94
 ---- batch: 080 ----
mean loss: 188.07
 ---- batch: 090 ----
mean loss: 186.93
train mean loss: 188.64
epoch train time: 0:00:16.925351
elapsed time: 0:58:33.293430
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-24 23:29:24.203920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.33
 ---- batch: 020 ----
mean loss: 189.95
 ---- batch: 030 ----
mean loss: 194.58
 ---- batch: 040 ----
mean loss: 186.64
 ---- batch: 050 ----
mean loss: 183.40
 ---- batch: 060 ----
mean loss: 187.07
 ---- batch: 070 ----
mean loss: 188.42
 ---- batch: 080 ----
mean loss: 199.02
 ---- batch: 090 ----
mean loss: 185.44
train mean loss: 188.54
epoch train time: 0:00:16.943413
elapsed time: 0:58:50.237983
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-24 23:29:41.148517
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.16
 ---- batch: 020 ----
mean loss: 191.76
 ---- batch: 030 ----
mean loss: 196.60
 ---- batch: 040 ----
mean loss: 179.03
 ---- batch: 050 ----
mean loss: 190.09
 ---- batch: 060 ----
mean loss: 189.55
 ---- batch: 070 ----
mean loss: 184.04
 ---- batch: 080 ----
mean loss: 198.08
 ---- batch: 090 ----
mean loss: 184.32
train mean loss: 188.73
epoch train time: 0:00:16.951046
elapsed time: 0:59:07.190368
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-24 23:29:58.100912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.23
 ---- batch: 020 ----
mean loss: 184.49
 ---- batch: 030 ----
mean loss: 185.70
 ---- batch: 040 ----
mean loss: 194.53
 ---- batch: 050 ----
mean loss: 187.93
 ---- batch: 060 ----
mean loss: 189.39
 ---- batch: 070 ----
mean loss: 195.34
 ---- batch: 080 ----
mean loss: 188.40
 ---- batch: 090 ----
mean loss: 190.62
train mean loss: 188.86
epoch train time: 0:00:16.893244
elapsed time: 0:59:24.084795
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-24 23:30:14.995305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.28
 ---- batch: 020 ----
mean loss: 183.05
 ---- batch: 030 ----
mean loss: 190.77
 ---- batch: 040 ----
mean loss: 192.67
 ---- batch: 050 ----
mean loss: 190.24
 ---- batch: 060 ----
mean loss: 184.99
 ---- batch: 070 ----
mean loss: 188.35
 ---- batch: 080 ----
mean loss: 185.66
 ---- batch: 090 ----
mean loss: 189.18
train mean loss: 189.01
epoch train time: 0:00:16.928323
elapsed time: 0:59:41.014353
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-24 23:30:31.924941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.79
 ---- batch: 020 ----
mean loss: 189.45
 ---- batch: 030 ----
mean loss: 188.70
 ---- batch: 040 ----
mean loss: 197.12
 ---- batch: 050 ----
mean loss: 189.67
 ---- batch: 060 ----
mean loss: 186.59
 ---- batch: 070 ----
mean loss: 187.69
 ---- batch: 080 ----
mean loss: 192.48
 ---- batch: 090 ----
mean loss: 184.21
train mean loss: 188.97
epoch train time: 0:00:16.881708
elapsed time: 0:59:57.897306
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-24 23:30:48.807778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.70
 ---- batch: 020 ----
mean loss: 193.95
 ---- batch: 030 ----
mean loss: 190.19
 ---- batch: 040 ----
mean loss: 180.37
 ---- batch: 050 ----
mean loss: 196.17
 ---- batch: 060 ----
mean loss: 194.77
 ---- batch: 070 ----
mean loss: 185.90
 ---- batch: 080 ----
mean loss: 185.60
 ---- batch: 090 ----
mean loss: 186.39
train mean loss: 188.93
epoch train time: 0:00:16.945777
elapsed time: 1:00:14.844212
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-24 23:31:05.754779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.75
 ---- batch: 020 ----
mean loss: 190.70
 ---- batch: 030 ----
mean loss: 191.16
 ---- batch: 040 ----
mean loss: 186.85
 ---- batch: 050 ----
mean loss: 183.43
 ---- batch: 060 ----
mean loss: 195.64
 ---- batch: 070 ----
mean loss: 186.87
 ---- batch: 080 ----
mean loss: 187.19
 ---- batch: 090 ----
mean loss: 186.67
train mean loss: 188.50
epoch train time: 0:00:16.950582
elapsed time: 1:00:31.796261
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-24 23:31:22.706763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.75
 ---- batch: 020 ----
mean loss: 184.94
 ---- batch: 030 ----
mean loss: 189.97
 ---- batch: 040 ----
mean loss: 187.30
 ---- batch: 050 ----
mean loss: 185.82
 ---- batch: 060 ----
mean loss: 186.52
 ---- batch: 070 ----
mean loss: 192.78
 ---- batch: 080 ----
mean loss: 190.27
 ---- batch: 090 ----
mean loss: 191.80
train mean loss: 188.62
epoch train time: 0:00:16.915833
elapsed time: 1:00:48.713240
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-24 23:31:39.623770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.54
 ---- batch: 020 ----
mean loss: 187.72
 ---- batch: 030 ----
mean loss: 186.47
 ---- batch: 040 ----
mean loss: 194.72
 ---- batch: 050 ----
mean loss: 186.72
 ---- batch: 060 ----
mean loss: 182.34
 ---- batch: 070 ----
mean loss: 191.04
 ---- batch: 080 ----
mean loss: 191.06
 ---- batch: 090 ----
mean loss: 186.62
train mean loss: 188.69
epoch train time: 0:00:16.941452
elapsed time: 1:01:05.655968
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-24 23:31:56.566500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.43
 ---- batch: 020 ----
mean loss: 192.44
 ---- batch: 030 ----
mean loss: 183.95
 ---- batch: 040 ----
mean loss: 191.40
 ---- batch: 050 ----
mean loss: 188.70
 ---- batch: 060 ----
mean loss: 191.44
 ---- batch: 070 ----
mean loss: 185.62
 ---- batch: 080 ----
mean loss: 187.89
 ---- batch: 090 ----
mean loss: 188.92
train mean loss: 188.33
epoch train time: 0:00:16.898475
elapsed time: 1:01:22.555736
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-24 23:32:13.466274
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.73
 ---- batch: 020 ----
mean loss: 177.76
 ---- batch: 030 ----
mean loss: 184.89
 ---- batch: 040 ----
mean loss: 189.70
 ---- batch: 050 ----
mean loss: 187.80
 ---- batch: 060 ----
mean loss: 192.77
 ---- batch: 070 ----
mean loss: 195.63
 ---- batch: 080 ----
mean loss: 196.01
 ---- batch: 090 ----
mean loss: 188.66
train mean loss: 188.99
epoch train time: 0:00:16.912427
elapsed time: 1:01:39.469382
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-24 23:32:30.379889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.49
 ---- batch: 020 ----
mean loss: 189.79
 ---- batch: 030 ----
mean loss: 184.40
 ---- batch: 040 ----
mean loss: 187.65
 ---- batch: 050 ----
mean loss: 188.49
 ---- batch: 060 ----
mean loss: 190.15
 ---- batch: 070 ----
mean loss: 187.56
 ---- batch: 080 ----
mean loss: 190.24
 ---- batch: 090 ----
mean loss: 186.35
train mean loss: 188.86
epoch train time: 0:00:16.995426
elapsed time: 1:01:56.466006
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-24 23:32:47.376556
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.76
 ---- batch: 020 ----
mean loss: 186.27
 ---- batch: 030 ----
mean loss: 192.81
 ---- batch: 040 ----
mean loss: 189.33
 ---- batch: 050 ----
mean loss: 188.21
 ---- batch: 060 ----
mean loss: 189.78
 ---- batch: 070 ----
mean loss: 185.38
 ---- batch: 080 ----
mean loss: 191.61
 ---- batch: 090 ----
mean loss: 184.64
train mean loss: 188.63
epoch train time: 0:00:16.916446
elapsed time: 1:02:13.383679
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-24 23:33:04.294188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.87
 ---- batch: 020 ----
mean loss: 187.60
 ---- batch: 030 ----
mean loss: 190.18
 ---- batch: 040 ----
mean loss: 188.34
 ---- batch: 050 ----
mean loss: 192.93
 ---- batch: 060 ----
mean loss: 185.39
 ---- batch: 070 ----
mean loss: 185.60
 ---- batch: 080 ----
mean loss: 194.27
 ---- batch: 090 ----
mean loss: 188.18
train mean loss: 188.83
epoch train time: 0:00:16.942263
elapsed time: 1:02:30.327193
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-24 23:33:21.237722
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.16
 ---- batch: 020 ----
mean loss: 190.28
 ---- batch: 030 ----
mean loss: 192.14
 ---- batch: 040 ----
mean loss: 189.81
 ---- batch: 050 ----
mean loss: 185.40
 ---- batch: 060 ----
mean loss: 190.88
 ---- batch: 070 ----
mean loss: 183.47
 ---- batch: 080 ----
mean loss: 191.52
 ---- batch: 090 ----
mean loss: 189.83
train mean loss: 188.44
epoch train time: 0:00:16.991793
elapsed time: 1:02:47.320252
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-24 23:33:38.230801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.74
 ---- batch: 020 ----
mean loss: 187.45
 ---- batch: 030 ----
mean loss: 191.82
 ---- batch: 040 ----
mean loss: 189.48
 ---- batch: 050 ----
mean loss: 194.21
 ---- batch: 060 ----
mean loss: 183.37
 ---- batch: 070 ----
mean loss: 182.16
 ---- batch: 080 ----
mean loss: 194.93
 ---- batch: 090 ----
mean loss: 184.41
train mean loss: 188.16
epoch train time: 0:00:17.025414
elapsed time: 1:03:04.346863
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-24 23:33:55.257400
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.58
 ---- batch: 020 ----
mean loss: 183.64
 ---- batch: 030 ----
mean loss: 188.68
 ---- batch: 040 ----
mean loss: 189.14
 ---- batch: 050 ----
mean loss: 193.55
 ---- batch: 060 ----
mean loss: 185.91
 ---- batch: 070 ----
mean loss: 186.53
 ---- batch: 080 ----
mean loss: 191.48
 ---- batch: 090 ----
mean loss: 188.63
train mean loss: 188.32
epoch train time: 0:00:16.907084
elapsed time: 1:03:21.255168
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-24 23:34:12.165724
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.22
 ---- batch: 020 ----
mean loss: 188.03
 ---- batch: 030 ----
mean loss: 185.14
 ---- batch: 040 ----
mean loss: 189.06
 ---- batch: 050 ----
mean loss: 186.62
 ---- batch: 060 ----
mean loss: 196.76
 ---- batch: 070 ----
mean loss: 187.92
 ---- batch: 080 ----
mean loss: 192.75
 ---- batch: 090 ----
mean loss: 183.01
train mean loss: 188.37
epoch train time: 0:00:16.920208
elapsed time: 1:03:38.176604
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-24 23:34:29.087105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.80
 ---- batch: 020 ----
mean loss: 194.14
 ---- batch: 030 ----
mean loss: 186.73
 ---- batch: 040 ----
mean loss: 185.81
 ---- batch: 050 ----
mean loss: 183.85
 ---- batch: 060 ----
mean loss: 189.84
 ---- batch: 070 ----
mean loss: 187.30
 ---- batch: 080 ----
mean loss: 193.03
 ---- batch: 090 ----
mean loss: 190.35
train mean loss: 188.28
epoch train time: 0:00:16.946135
elapsed time: 1:03:55.123954
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-24 23:34:46.034484
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.04
 ---- batch: 020 ----
mean loss: 188.77
 ---- batch: 030 ----
mean loss: 186.43
 ---- batch: 040 ----
mean loss: 185.77
 ---- batch: 050 ----
mean loss: 189.69
 ---- batch: 060 ----
mean loss: 189.82
 ---- batch: 070 ----
mean loss: 189.50
 ---- batch: 080 ----
mean loss: 181.38
 ---- batch: 090 ----
mean loss: 193.68
train mean loss: 188.12
epoch train time: 0:00:16.939882
elapsed time: 1:04:12.065064
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-24 23:35:02.975655
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.86
 ---- batch: 020 ----
mean loss: 184.50
 ---- batch: 030 ----
mean loss: 186.16
 ---- batch: 040 ----
mean loss: 185.38
 ---- batch: 050 ----
mean loss: 185.68
 ---- batch: 060 ----
mean loss: 184.97
 ---- batch: 070 ----
mean loss: 188.68
 ---- batch: 080 ----
mean loss: 203.33
 ---- batch: 090 ----
mean loss: 192.64
train mean loss: 188.60
epoch train time: 0:00:16.931847
elapsed time: 1:04:28.998253
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-24 23:35:19.908748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.83
 ---- batch: 020 ----
mean loss: 191.69
 ---- batch: 030 ----
mean loss: 185.92
 ---- batch: 040 ----
mean loss: 189.55
 ---- batch: 050 ----
mean loss: 188.23
 ---- batch: 060 ----
mean loss: 187.37
 ---- batch: 070 ----
mean loss: 183.93
 ---- batch: 080 ----
mean loss: 184.47
 ---- batch: 090 ----
mean loss: 191.03
train mean loss: 188.10
epoch train time: 0:00:16.956819
elapsed time: 1:04:45.956294
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-24 23:35:36.866843
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.67
 ---- batch: 020 ----
mean loss: 193.34
 ---- batch: 030 ----
mean loss: 189.28
 ---- batch: 040 ----
mean loss: 183.92
 ---- batch: 050 ----
mean loss: 184.26
 ---- batch: 060 ----
mean loss: 187.70
 ---- batch: 070 ----
mean loss: 182.97
 ---- batch: 080 ----
mean loss: 191.40
 ---- batch: 090 ----
mean loss: 195.47
train mean loss: 188.13
epoch train time: 0:00:16.903759
elapsed time: 1:05:02.861335
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-24 23:35:53.771730
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.03
 ---- batch: 020 ----
mean loss: 189.99
 ---- batch: 030 ----
mean loss: 187.46
 ---- batch: 040 ----
mean loss: 192.68
 ---- batch: 050 ----
mean loss: 196.08
 ---- batch: 060 ----
mean loss: 184.83
 ---- batch: 070 ----
mean loss: 188.23
 ---- batch: 080 ----
mean loss: 186.64
 ---- batch: 090 ----
mean loss: 185.98
train mean loss: 188.35
epoch train time: 0:00:17.048801
elapsed time: 1:05:19.911294
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-24 23:36:10.821967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.99
 ---- batch: 020 ----
mean loss: 182.47
 ---- batch: 030 ----
mean loss: 191.06
 ---- batch: 040 ----
mean loss: 189.68
 ---- batch: 050 ----
mean loss: 183.27
 ---- batch: 060 ----
mean loss: 189.72
 ---- batch: 070 ----
mean loss: 191.63
 ---- batch: 080 ----
mean loss: 184.09
 ---- batch: 090 ----
mean loss: 189.84
train mean loss: 188.04
epoch train time: 0:00:17.054722
elapsed time: 1:05:36.967414
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-24 23:36:27.877956
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.98
 ---- batch: 020 ----
mean loss: 186.20
 ---- batch: 030 ----
mean loss: 187.52
 ---- batch: 040 ----
mean loss: 195.23
 ---- batch: 050 ----
mean loss: 191.61
 ---- batch: 060 ----
mean loss: 184.94
 ---- batch: 070 ----
mean loss: 188.14
 ---- batch: 080 ----
mean loss: 182.59
 ---- batch: 090 ----
mean loss: 192.26
train mean loss: 188.18
epoch train time: 0:00:16.905994
elapsed time: 1:05:53.874656
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-24 23:36:44.785226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.00
 ---- batch: 020 ----
mean loss: 190.16
 ---- batch: 030 ----
mean loss: 191.31
 ---- batch: 040 ----
mean loss: 186.81
 ---- batch: 050 ----
mean loss: 185.39
 ---- batch: 060 ----
mean loss: 186.11
 ---- batch: 070 ----
mean loss: 189.19
 ---- batch: 080 ----
mean loss: 188.92
 ---- batch: 090 ----
mean loss: 187.12
train mean loss: 188.53
epoch train time: 0:00:16.911379
elapsed time: 1:06:10.787291
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-24 23:37:01.697837
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.43
 ---- batch: 020 ----
mean loss: 189.28
 ---- batch: 030 ----
mean loss: 188.08
 ---- batch: 040 ----
mean loss: 190.57
 ---- batch: 050 ----
mean loss: 186.54
 ---- batch: 060 ----
mean loss: 181.76
 ---- batch: 070 ----
mean loss: 185.40
 ---- batch: 080 ----
mean loss: 191.74
 ---- batch: 090 ----
mean loss: 190.02
train mean loss: 188.25
epoch train time: 0:00:16.907094
elapsed time: 1:06:27.695585
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-24 23:37:18.606122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.84
 ---- batch: 020 ----
mean loss: 187.93
 ---- batch: 030 ----
mean loss: 189.73
 ---- batch: 040 ----
mean loss: 184.95
 ---- batch: 050 ----
mean loss: 184.08
 ---- batch: 060 ----
mean loss: 191.65
 ---- batch: 070 ----
mean loss: 189.10
 ---- batch: 080 ----
mean loss: 184.69
 ---- batch: 090 ----
mean loss: 189.72
train mean loss: 187.91
epoch train time: 0:00:16.872314
elapsed time: 1:06:44.569184
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-24 23:37:35.479844
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.55
 ---- batch: 020 ----
mean loss: 187.73
 ---- batch: 030 ----
mean loss: 184.16
 ---- batch: 040 ----
mean loss: 184.69
 ---- batch: 050 ----
mean loss: 189.43
 ---- batch: 060 ----
mean loss: 189.70
 ---- batch: 070 ----
mean loss: 182.44
 ---- batch: 080 ----
mean loss: 190.90
 ---- batch: 090 ----
mean loss: 188.34
train mean loss: 188.45
epoch train time: 0:00:16.950909
elapsed time: 1:07:01.522251
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-24 23:37:52.432267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.97
 ---- batch: 020 ----
mean loss: 191.22
 ---- batch: 030 ----
mean loss: 186.31
 ---- batch: 040 ----
mean loss: 191.23
 ---- batch: 050 ----
mean loss: 188.73
 ---- batch: 060 ----
mean loss: 187.51
 ---- batch: 070 ----
mean loss: 192.42
 ---- batch: 080 ----
mean loss: 188.87
 ---- batch: 090 ----
mean loss: 184.33
train mean loss: 187.95
epoch train time: 0:00:16.895508
elapsed time: 1:07:18.418518
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-24 23:38:09.329141
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.47
 ---- batch: 020 ----
mean loss: 188.20
 ---- batch: 030 ----
mean loss: 184.79
 ---- batch: 040 ----
mean loss: 183.53
 ---- batch: 050 ----
mean loss: 191.13
 ---- batch: 060 ----
mean loss: 190.75
 ---- batch: 070 ----
mean loss: 187.29
 ---- batch: 080 ----
mean loss: 189.04
 ---- batch: 090 ----
mean loss: 187.93
train mean loss: 188.03
epoch train time: 0:00:16.887004
elapsed time: 1:07:35.306830
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-24 23:38:26.217395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.61
 ---- batch: 020 ----
mean loss: 187.94
 ---- batch: 030 ----
mean loss: 188.65
 ---- batch: 040 ----
mean loss: 191.54
 ---- batch: 050 ----
mean loss: 181.94
 ---- batch: 060 ----
mean loss: 183.28
 ---- batch: 070 ----
mean loss: 191.67
 ---- batch: 080 ----
mean loss: 189.72
 ---- batch: 090 ----
mean loss: 190.62
train mean loss: 187.81
epoch train time: 0:00:16.896389
elapsed time: 1:07:52.204453
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-24 23:38:43.114939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.67
 ---- batch: 020 ----
mean loss: 183.35
 ---- batch: 030 ----
mean loss: 198.33
 ---- batch: 040 ----
mean loss: 187.48
 ---- batch: 050 ----
mean loss: 187.34
 ---- batch: 060 ----
mean loss: 181.64
 ---- batch: 070 ----
mean loss: 189.17
 ---- batch: 080 ----
mean loss: 194.34
 ---- batch: 090 ----
mean loss: 188.51
train mean loss: 188.62
epoch train time: 0:00:16.910996
elapsed time: 1:08:09.116764
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-24 23:39:00.027264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.84
 ---- batch: 020 ----
mean loss: 187.67
 ---- batch: 030 ----
mean loss: 188.52
 ---- batch: 040 ----
mean loss: 187.98
 ---- batch: 050 ----
mean loss: 189.04
 ---- batch: 060 ----
mean loss: 183.38
 ---- batch: 070 ----
mean loss: 188.08
 ---- batch: 080 ----
mean loss: 185.72
 ---- batch: 090 ----
mean loss: 188.95
train mean loss: 188.09
epoch train time: 0:00:16.902637
elapsed time: 1:08:26.020587
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-24 23:39:16.931176
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.49
 ---- batch: 020 ----
mean loss: 189.98
 ---- batch: 030 ----
mean loss: 185.52
 ---- batch: 040 ----
mean loss: 180.59
 ---- batch: 050 ----
mean loss: 190.70
 ---- batch: 060 ----
mean loss: 189.14
 ---- batch: 070 ----
mean loss: 184.93
 ---- batch: 080 ----
mean loss: 186.02
 ---- batch: 090 ----
mean loss: 192.08
train mean loss: 187.99
epoch train time: 0:00:16.930224
elapsed time: 1:08:42.952084
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-24 23:39:33.862637
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.31
 ---- batch: 020 ----
mean loss: 191.01
 ---- batch: 030 ----
mean loss: 189.67
 ---- batch: 040 ----
mean loss: 180.05
 ---- batch: 050 ----
mean loss: 187.07
 ---- batch: 060 ----
mean loss: 185.16
 ---- batch: 070 ----
mean loss: 191.80
 ---- batch: 080 ----
mean loss: 189.09
 ---- batch: 090 ----
mean loss: 183.62
train mean loss: 187.95
epoch train time: 0:00:16.893373
elapsed time: 1:08:59.846701
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-24 23:39:50.757179
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.05
 ---- batch: 020 ----
mean loss: 184.87
 ---- batch: 030 ----
mean loss: 190.25
 ---- batch: 040 ----
mean loss: 184.42
 ---- batch: 050 ----
mean loss: 187.83
 ---- batch: 060 ----
mean loss: 181.31
 ---- batch: 070 ----
mean loss: 190.69
 ---- batch: 080 ----
mean loss: 183.52
 ---- batch: 090 ----
mean loss: 193.01
train mean loss: 188.14
epoch train time: 0:00:16.881889
elapsed time: 1:09:16.729738
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-24 23:40:07.640244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.38
 ---- batch: 020 ----
mean loss: 183.41
 ---- batch: 030 ----
mean loss: 191.70
 ---- batch: 040 ----
mean loss: 189.69
 ---- batch: 050 ----
mean loss: 182.34
 ---- batch: 060 ----
mean loss: 185.61
 ---- batch: 070 ----
mean loss: 185.18
 ---- batch: 080 ----
mean loss: 186.37
 ---- batch: 090 ----
mean loss: 193.35
train mean loss: 187.84
epoch train time: 0:00:16.906162
elapsed time: 1:09:33.637090
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-24 23:40:24.547602
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.77
 ---- batch: 020 ----
mean loss: 189.62
 ---- batch: 030 ----
mean loss: 188.47
 ---- batch: 040 ----
mean loss: 185.52
 ---- batch: 050 ----
mean loss: 183.61
 ---- batch: 060 ----
mean loss: 190.63
 ---- batch: 070 ----
mean loss: 193.36
 ---- batch: 080 ----
mean loss: 180.89
 ---- batch: 090 ----
mean loss: 193.60
train mean loss: 188.18
epoch train time: 0:00:16.883800
elapsed time: 1:09:50.522067
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-24 23:40:41.432561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.33
 ---- batch: 020 ----
mean loss: 197.37
 ---- batch: 030 ----
mean loss: 187.76
 ---- batch: 040 ----
mean loss: 179.62
 ---- batch: 050 ----
mean loss: 188.69
 ---- batch: 060 ----
mean loss: 186.42
 ---- batch: 070 ----
mean loss: 189.42
 ---- batch: 080 ----
mean loss: 182.22
 ---- batch: 090 ----
mean loss: 190.24
train mean loss: 188.01
epoch train time: 0:00:16.925488
elapsed time: 1:10:07.448734
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-24 23:40:58.359277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.76
 ---- batch: 020 ----
mean loss: 189.73
 ---- batch: 030 ----
mean loss: 182.72
 ---- batch: 040 ----
mean loss: 186.79
 ---- batch: 050 ----
mean loss: 193.54
 ---- batch: 060 ----
mean loss: 190.29
 ---- batch: 070 ----
mean loss: 185.45
 ---- batch: 080 ----
mean loss: 189.26
 ---- batch: 090 ----
mean loss: 191.76
train mean loss: 187.33
epoch train time: 0:00:16.921890
elapsed time: 1:10:24.371921
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-24 23:41:15.282478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.70
 ---- batch: 020 ----
mean loss: 187.61
 ---- batch: 030 ----
mean loss: 183.77
 ---- batch: 040 ----
mean loss: 192.97
 ---- batch: 050 ----
mean loss: 186.91
 ---- batch: 060 ----
mean loss: 182.56
 ---- batch: 070 ----
mean loss: 188.85
 ---- batch: 080 ----
mean loss: 182.08
 ---- batch: 090 ----
mean loss: 189.54
train mean loss: 187.88
epoch train time: 0:00:16.884996
elapsed time: 1:10:41.258193
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-24 23:41:32.168681
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.72
 ---- batch: 020 ----
mean loss: 186.48
 ---- batch: 030 ----
mean loss: 186.40
 ---- batch: 040 ----
mean loss: 193.15
 ---- batch: 050 ----
mean loss: 190.73
 ---- batch: 060 ----
mean loss: 192.01
 ---- batch: 070 ----
mean loss: 182.06
 ---- batch: 080 ----
mean loss: 180.18
 ---- batch: 090 ----
mean loss: 191.47
train mean loss: 187.42
epoch train time: 0:00:16.891092
elapsed time: 1:10:58.150539
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-24 23:41:49.061163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.49
 ---- batch: 020 ----
mean loss: 187.07
 ---- batch: 030 ----
mean loss: 185.42
 ---- batch: 040 ----
mean loss: 189.93
 ---- batch: 050 ----
mean loss: 183.85
 ---- batch: 060 ----
mean loss: 191.31
 ---- batch: 070 ----
mean loss: 201.42
 ---- batch: 080 ----
mean loss: 182.11
 ---- batch: 090 ----
mean loss: 184.02
train mean loss: 187.84
epoch train time: 0:00:16.887713
elapsed time: 1:11:15.039593
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-24 23:42:05.950123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.02
 ---- batch: 020 ----
mean loss: 188.85
 ---- batch: 030 ----
mean loss: 186.46
 ---- batch: 040 ----
mean loss: 178.19
 ---- batch: 050 ----
mean loss: 194.35
 ---- batch: 060 ----
mean loss: 191.08
 ---- batch: 070 ----
mean loss: 186.76
 ---- batch: 080 ----
mean loss: 188.58
 ---- batch: 090 ----
mean loss: 191.14
train mean loss: 187.59
epoch train time: 0:00:16.856397
elapsed time: 1:11:31.908006
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_3/checkpoint.pth.tar
**** end time: 2019-09-24 23:42:22.817990 ****
