Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_1', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 21407
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-25 20:50:40.627769 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 21, 24]             100
              Tanh-2           [-1, 10, 21, 24]               0
            Conv2d-3           [-1, 10, 20, 24]           1,000
              Tanh-4           [-1, 10, 20, 24]               0
            Conv2d-5           [-1, 10, 21, 24]           1,000
              Tanh-6           [-1, 10, 21, 24]               0
            Conv2d-7           [-1, 10, 20, 24]           1,000
              Tanh-8           [-1, 10, 20, 24]               0
            Conv2d-9            [-1, 1, 20, 24]              30
             Tanh-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
          Dropout-12                  [-1, 480]               0
           Linear-13                  [-1, 100]          48,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 51,230
Trainable params: 51,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 20:50:40.636693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3757.45
 ---- batch: 020 ----
mean loss: 1707.88
 ---- batch: 030 ----
mean loss: 1235.31
 ---- batch: 040 ----
mean loss: 1105.54
 ---- batch: 050 ----
mean loss: 1026.63
 ---- batch: 060 ----
mean loss: 1005.62
 ---- batch: 070 ----
mean loss: 953.24
 ---- batch: 080 ----
mean loss: 939.15
 ---- batch: 090 ----
mean loss: 927.81
train mean loss: 1375.50
epoch train time: 0:00:35.145011
elapsed time: 0:00:35.156660
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 20:51:15.784469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.20
 ---- batch: 020 ----
mean loss: 859.88
 ---- batch: 030 ----
mean loss: 849.91
 ---- batch: 040 ----
mean loss: 837.14
 ---- batch: 050 ----
mean loss: 828.91
 ---- batch: 060 ----
mean loss: 814.54
 ---- batch: 070 ----
mean loss: 774.60
 ---- batch: 080 ----
mean loss: 792.58
 ---- batch: 090 ----
mean loss: 782.56
train mean loss: 819.78
epoch train time: 0:00:02.422301
elapsed time: 0:00:37.579118
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 20:51:18.207000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 744.35
 ---- batch: 020 ----
mean loss: 722.30
 ---- batch: 030 ----
mean loss: 720.43
 ---- batch: 040 ----
mean loss: 721.52
 ---- batch: 050 ----
mean loss: 708.50
 ---- batch: 060 ----
mean loss: 678.78
 ---- batch: 070 ----
mean loss: 684.29
 ---- batch: 080 ----
mean loss: 681.92
 ---- batch: 090 ----
mean loss: 660.58
train mean loss: 699.04
epoch train time: 0:00:02.369152
elapsed time: 0:00:39.948534
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 20:51:20.576387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 670.98
 ---- batch: 020 ----
mean loss: 648.85
 ---- batch: 030 ----
mean loss: 641.95
 ---- batch: 040 ----
mean loss: 636.53
 ---- batch: 050 ----
mean loss: 632.34
 ---- batch: 060 ----
mean loss: 603.93
 ---- batch: 070 ----
mean loss: 612.60
 ---- batch: 080 ----
mean loss: 602.07
 ---- batch: 090 ----
mean loss: 593.29
train mean loss: 625.15
epoch train time: 0:00:02.359606
elapsed time: 0:00:42.308370
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 20:51:22.936225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 594.90
 ---- batch: 020 ----
mean loss: 606.30
 ---- batch: 030 ----
mean loss: 578.98
 ---- batch: 040 ----
mean loss: 564.53
 ---- batch: 050 ----
mean loss: 579.04
 ---- batch: 060 ----
mean loss: 570.00
 ---- batch: 070 ----
mean loss: 561.96
 ---- batch: 080 ----
mean loss: 579.59
 ---- batch: 090 ----
mean loss: 550.37
train mean loss: 574.67
epoch train time: 0:00:02.365497
elapsed time: 0:00:44.674089
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 20:51:25.301926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 548.52
 ---- batch: 020 ----
mean loss: 563.38
 ---- batch: 030 ----
mean loss: 525.28
 ---- batch: 040 ----
mean loss: 552.74
 ---- batch: 050 ----
mean loss: 547.14
 ---- batch: 060 ----
mean loss: 528.43
 ---- batch: 070 ----
mean loss: 542.61
 ---- batch: 080 ----
mean loss: 534.03
 ---- batch: 090 ----
mean loss: 519.95
train mean loss: 537.79
epoch train time: 0:00:02.369190
elapsed time: 0:00:47.043465
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 20:51:27.671291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 539.50
 ---- batch: 020 ----
mean loss: 513.14
 ---- batch: 030 ----
mean loss: 516.39
 ---- batch: 040 ----
mean loss: 523.74
 ---- batch: 050 ----
mean loss: 497.30
 ---- batch: 060 ----
mean loss: 504.40
 ---- batch: 070 ----
mean loss: 513.20
 ---- batch: 080 ----
mean loss: 511.87
 ---- batch: 090 ----
mean loss: 503.09
train mean loss: 513.91
epoch train time: 0:00:02.367494
elapsed time: 0:00:49.411140
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 20:51:30.038970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.10
 ---- batch: 020 ----
mean loss: 496.15
 ---- batch: 030 ----
mean loss: 498.50
 ---- batch: 040 ----
mean loss: 501.21
 ---- batch: 050 ----
mean loss: 505.68
 ---- batch: 060 ----
mean loss: 486.15
 ---- batch: 070 ----
mean loss: 493.20
 ---- batch: 080 ----
mean loss: 464.84
 ---- batch: 090 ----
mean loss: 471.63
train mean loss: 488.48
epoch train time: 0:00:02.365268
elapsed time: 0:00:51.776588
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 20:51:32.404417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.03
 ---- batch: 020 ----
mean loss: 463.93
 ---- batch: 030 ----
mean loss: 464.84
 ---- batch: 040 ----
mean loss: 447.68
 ---- batch: 050 ----
mean loss: 440.75
 ---- batch: 060 ----
mean loss: 456.12
 ---- batch: 070 ----
mean loss: 438.76
 ---- batch: 080 ----
mean loss: 447.87
 ---- batch: 090 ----
mean loss: 438.59
train mean loss: 453.47
epoch train time: 0:00:02.361469
elapsed time: 0:00:54.138246
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 20:51:34.766092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.82
 ---- batch: 020 ----
mean loss: 495.46
 ---- batch: 030 ----
mean loss: 436.10
 ---- batch: 040 ----
mean loss: 432.72
 ---- batch: 050 ----
mean loss: 446.60
 ---- batch: 060 ----
mean loss: 442.65
 ---- batch: 070 ----
mean loss: 435.61
 ---- batch: 080 ----
mean loss: 455.01
 ---- batch: 090 ----
mean loss: 448.57
train mean loss: 447.86
epoch train time: 0:00:02.371263
elapsed time: 0:00:56.509752
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 20:51:37.137570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.34
 ---- batch: 020 ----
mean loss: 429.11
 ---- batch: 030 ----
mean loss: 417.87
 ---- batch: 040 ----
mean loss: 424.27
 ---- batch: 050 ----
mean loss: 440.60
 ---- batch: 060 ----
mean loss: 422.95
 ---- batch: 070 ----
mean loss: 424.15
 ---- batch: 080 ----
mean loss: 430.74
 ---- batch: 090 ----
mean loss: 436.11
train mean loss: 430.21
epoch train time: 0:00:02.357070
elapsed time: 0:00:58.867005
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 20:51:39.494851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.45
 ---- batch: 020 ----
mean loss: 418.14
 ---- batch: 030 ----
mean loss: 413.74
 ---- batch: 040 ----
mean loss: 427.54
 ---- batch: 050 ----
mean loss: 422.39
 ---- batch: 060 ----
mean loss: 425.77
 ---- batch: 070 ----
mean loss: 418.63
 ---- batch: 080 ----
mean loss: 425.77
 ---- batch: 090 ----
mean loss: 443.31
train mean loss: 426.57
epoch train time: 0:00:02.369093
elapsed time: 0:01:01.236298
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 20:51:41.864131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.83
 ---- batch: 020 ----
mean loss: 415.24
 ---- batch: 030 ----
mean loss: 415.48
 ---- batch: 040 ----
mean loss: 419.87
 ---- batch: 050 ----
mean loss: 442.28
 ---- batch: 060 ----
mean loss: 436.05
 ---- batch: 070 ----
mean loss: 420.84
 ---- batch: 080 ----
mean loss: 444.92
 ---- batch: 090 ----
mean loss: 444.76
train mean loss: 429.64
epoch train time: 0:00:02.351497
elapsed time: 0:01:03.587978
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 20:51:44.215805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.72
 ---- batch: 020 ----
mean loss: 436.96
 ---- batch: 030 ----
mean loss: 431.13
 ---- batch: 040 ----
mean loss: 434.04
 ---- batch: 050 ----
mean loss: 418.69
 ---- batch: 060 ----
mean loss: 410.55
 ---- batch: 070 ----
mean loss: 408.85
 ---- batch: 080 ----
mean loss: 412.84
 ---- batch: 090 ----
mean loss: 400.52
train mean loss: 422.28
epoch train time: 0:00:02.357105
elapsed time: 0:01:05.945259
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 20:51:46.573091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.47
 ---- batch: 020 ----
mean loss: 410.05
 ---- batch: 030 ----
mean loss: 418.73
 ---- batch: 040 ----
mean loss: 421.65
 ---- batch: 050 ----
mean loss: 415.03
 ---- batch: 060 ----
mean loss: 419.18
 ---- batch: 070 ----
mean loss: 412.35
 ---- batch: 080 ----
mean loss: 420.46
 ---- batch: 090 ----
mean loss: 408.46
train mean loss: 415.71
epoch train time: 0:00:02.353925
elapsed time: 0:01:08.299381
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 20:51:48.927215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.93
 ---- batch: 020 ----
mean loss: 438.23
 ---- batch: 030 ----
mean loss: 419.15
 ---- batch: 040 ----
mean loss: 409.30
 ---- batch: 050 ----
mean loss: 414.61
 ---- batch: 060 ----
mean loss: 406.48
 ---- batch: 070 ----
mean loss: 410.59
 ---- batch: 080 ----
mean loss: 404.67
 ---- batch: 090 ----
mean loss: 411.14
train mean loss: 413.75
epoch train time: 0:00:02.349329
elapsed time: 0:01:10.648887
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 20:51:51.276719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.63
 ---- batch: 020 ----
mean loss: 420.10
 ---- batch: 030 ----
mean loss: 402.56
 ---- batch: 040 ----
mean loss: 396.91
 ---- batch: 050 ----
mean loss: 400.46
 ---- batch: 060 ----
mean loss: 411.73
 ---- batch: 070 ----
mean loss: 412.08
 ---- batch: 080 ----
mean loss: 422.72
 ---- batch: 090 ----
mean loss: 412.54
train mean loss: 409.45
epoch train time: 0:00:02.353092
elapsed time: 0:01:13.002169
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 20:51:53.629997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.96
 ---- batch: 020 ----
mean loss: 404.23
 ---- batch: 030 ----
mean loss: 411.22
 ---- batch: 040 ----
mean loss: 403.16
 ---- batch: 050 ----
mean loss: 407.96
 ---- batch: 060 ----
mean loss: 416.62
 ---- batch: 070 ----
mean loss: 411.95
 ---- batch: 080 ----
mean loss: 411.70
 ---- batch: 090 ----
mean loss: 400.14
train mean loss: 409.11
epoch train time: 0:00:02.350953
elapsed time: 0:01:15.353340
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 20:51:55.981183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.26
 ---- batch: 020 ----
mean loss: 410.61
 ---- batch: 030 ----
mean loss: 415.30
 ---- batch: 040 ----
mean loss: 416.33
 ---- batch: 050 ----
mean loss: 406.90
 ---- batch: 060 ----
mean loss: 411.94
 ---- batch: 070 ----
mean loss: 411.04
 ---- batch: 080 ----
mean loss: 407.66
 ---- batch: 090 ----
mean loss: 424.08
train mean loss: 411.93
epoch train time: 0:00:02.340841
elapsed time: 0:01:17.694382
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 20:51:58.322238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.78
 ---- batch: 020 ----
mean loss: 396.31
 ---- batch: 030 ----
mean loss: 413.41
 ---- batch: 040 ----
mean loss: 410.87
 ---- batch: 050 ----
mean loss: 420.45
 ---- batch: 060 ----
mean loss: 406.77
 ---- batch: 070 ----
mean loss: 403.84
 ---- batch: 080 ----
mean loss: 407.43
 ---- batch: 090 ----
mean loss: 407.38
train mean loss: 408.89
epoch train time: 0:00:02.351925
elapsed time: 0:01:20.046542
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 20:52:00.674356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.27
 ---- batch: 020 ----
mean loss: 411.94
 ---- batch: 030 ----
mean loss: 395.52
 ---- batch: 040 ----
mean loss: 401.85
 ---- batch: 050 ----
mean loss: 411.95
 ---- batch: 060 ----
mean loss: 407.41
 ---- batch: 070 ----
mean loss: 416.56
 ---- batch: 080 ----
mean loss: 399.10
 ---- batch: 090 ----
mean loss: 410.34
train mean loss: 406.32
epoch train time: 0:00:02.338861
elapsed time: 0:01:22.385591
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 20:52:03.013404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.24
 ---- batch: 020 ----
mean loss: 401.01
 ---- batch: 030 ----
mean loss: 417.84
 ---- batch: 040 ----
mean loss: 401.47
 ---- batch: 050 ----
mean loss: 408.76
 ---- batch: 060 ----
mean loss: 434.35
 ---- batch: 070 ----
mean loss: 423.51
 ---- batch: 080 ----
mean loss: 420.63
 ---- batch: 090 ----
mean loss: 415.85
train mean loss: 413.02
epoch train time: 0:00:02.346019
elapsed time: 0:01:24.731788
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 20:52:05.359602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.49
 ---- batch: 020 ----
mean loss: 399.17
 ---- batch: 030 ----
mean loss: 400.28
 ---- batch: 040 ----
mean loss: 430.76
 ---- batch: 050 ----
mean loss: 410.39
 ---- batch: 060 ----
mean loss: 413.32
 ---- batch: 070 ----
mean loss: 397.07
 ---- batch: 080 ----
mean loss: 405.92
 ---- batch: 090 ----
mean loss: 405.27
train mean loss: 406.70
epoch train time: 0:00:02.341634
elapsed time: 0:01:27.073673
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 20:52:07.701557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.08
 ---- batch: 020 ----
mean loss: 393.59
 ---- batch: 030 ----
mean loss: 399.92
 ---- batch: 040 ----
mean loss: 393.37
 ---- batch: 050 ----
mean loss: 423.09
 ---- batch: 060 ----
mean loss: 413.91
 ---- batch: 070 ----
mean loss: 398.60
 ---- batch: 080 ----
mean loss: 403.78
 ---- batch: 090 ----
mean loss: 402.15
train mean loss: 406.88
epoch train time: 0:00:02.339483
elapsed time: 0:01:29.413400
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 20:52:10.041228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.41
 ---- batch: 020 ----
mean loss: 420.80
 ---- batch: 030 ----
mean loss: 412.01
 ---- batch: 040 ----
mean loss: 401.33
 ---- batch: 050 ----
mean loss: 401.03
 ---- batch: 060 ----
mean loss: 413.41
 ---- batch: 070 ----
mean loss: 398.16
 ---- batch: 080 ----
mean loss: 407.86
 ---- batch: 090 ----
mean loss: 402.10
train mean loss: 409.68
epoch train time: 0:00:02.338591
elapsed time: 0:01:31.752172
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 20:52:12.380002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.23
 ---- batch: 020 ----
mean loss: 402.48
 ---- batch: 030 ----
mean loss: 409.64
 ---- batch: 040 ----
mean loss: 420.03
 ---- batch: 050 ----
mean loss: 415.17
 ---- batch: 060 ----
mean loss: 401.22
 ---- batch: 070 ----
mean loss: 387.11
 ---- batch: 080 ----
mean loss: 397.71
 ---- batch: 090 ----
mean loss: 394.85
train mean loss: 403.16
epoch train time: 0:00:02.331927
elapsed time: 0:01:34.084280
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 20:52:14.712122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.65
 ---- batch: 020 ----
mean loss: 428.74
 ---- batch: 030 ----
mean loss: 415.64
 ---- batch: 040 ----
mean loss: 412.30
 ---- batch: 050 ----
mean loss: 401.24
 ---- batch: 060 ----
mean loss: 413.83
 ---- batch: 070 ----
mean loss: 393.86
 ---- batch: 080 ----
mean loss: 398.88
 ---- batch: 090 ----
mean loss: 400.73
train mean loss: 407.18
epoch train time: 0:00:02.336579
elapsed time: 0:01:36.421079
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 20:52:17.048907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.17
 ---- batch: 020 ----
mean loss: 403.07
 ---- batch: 030 ----
mean loss: 410.80
 ---- batch: 040 ----
mean loss: 410.52
 ---- batch: 050 ----
mean loss: 409.45
 ---- batch: 060 ----
mean loss: 391.53
 ---- batch: 070 ----
mean loss: 391.10
 ---- batch: 080 ----
mean loss: 388.57
 ---- batch: 090 ----
mean loss: 393.72
train mean loss: 401.63
epoch train time: 0:00:02.337740
elapsed time: 0:01:38.759008
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 20:52:19.386838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.46
 ---- batch: 020 ----
mean loss: 405.13
 ---- batch: 030 ----
mean loss: 410.22
 ---- batch: 040 ----
mean loss: 413.82
 ---- batch: 050 ----
mean loss: 404.49
 ---- batch: 060 ----
mean loss: 396.44
 ---- batch: 070 ----
mean loss: 410.82
 ---- batch: 080 ----
mean loss: 399.72
 ---- batch: 090 ----
mean loss: 406.01
train mean loss: 405.36
epoch train time: 0:00:02.336403
elapsed time: 0:01:41.095592
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 20:52:21.723429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.12
 ---- batch: 020 ----
mean loss: 409.00
 ---- batch: 030 ----
mean loss: 397.59
 ---- batch: 040 ----
mean loss: 391.75
 ---- batch: 050 ----
mean loss: 410.84
 ---- batch: 060 ----
mean loss: 425.02
 ---- batch: 070 ----
mean loss: 441.16
 ---- batch: 080 ----
mean loss: 428.21
 ---- batch: 090 ----
mean loss: 390.18
train mean loss: 415.31
epoch train time: 0:00:02.334246
elapsed time: 0:01:43.430028
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 20:52:24.057861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.94
 ---- batch: 020 ----
mean loss: 391.36
 ---- batch: 030 ----
mean loss: 395.49
 ---- batch: 040 ----
mean loss: 407.91
 ---- batch: 050 ----
mean loss: 395.99
 ---- batch: 060 ----
mean loss: 403.74
 ---- batch: 070 ----
mean loss: 405.50
 ---- batch: 080 ----
mean loss: 392.81
 ---- batch: 090 ----
mean loss: 405.37
train mean loss: 400.44
epoch train time: 0:00:02.329859
elapsed time: 0:01:45.760123
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 20:52:26.387988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.81
 ---- batch: 020 ----
mean loss: 398.72
 ---- batch: 030 ----
mean loss: 408.82
 ---- batch: 040 ----
mean loss: 407.65
 ---- batch: 050 ----
mean loss: 410.54
 ---- batch: 060 ----
mean loss: 405.54
 ---- batch: 070 ----
mean loss: 394.31
 ---- batch: 080 ----
mean loss: 407.57
 ---- batch: 090 ----
mean loss: 422.15
train mean loss: 404.26
epoch train time: 0:00:02.326919
elapsed time: 0:01:48.087275
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 20:52:28.715105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.29
 ---- batch: 020 ----
mean loss: 407.71
 ---- batch: 030 ----
mean loss: 397.53
 ---- batch: 040 ----
mean loss: 397.87
 ---- batch: 050 ----
mean loss: 401.30
 ---- batch: 060 ----
mean loss: 397.12
 ---- batch: 070 ----
mean loss: 405.08
 ---- batch: 080 ----
mean loss: 408.98
 ---- batch: 090 ----
mean loss: 407.51
train mean loss: 404.76
epoch train time: 0:00:02.335556
elapsed time: 0:01:50.423080
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 20:52:31.050902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.60
 ---- batch: 020 ----
mean loss: 403.83
 ---- batch: 030 ----
mean loss: 397.54
 ---- batch: 040 ----
mean loss: 380.97
 ---- batch: 050 ----
mean loss: 391.25
 ---- batch: 060 ----
mean loss: 389.59
 ---- batch: 070 ----
mean loss: 385.35
 ---- batch: 080 ----
mean loss: 395.18
 ---- batch: 090 ----
mean loss: 401.30
train mean loss: 394.82
epoch train time: 0:00:02.336036
elapsed time: 0:01:52.759292
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 20:52:33.387145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.42
 ---- batch: 020 ----
mean loss: 402.11
 ---- batch: 030 ----
mean loss: 390.23
 ---- batch: 040 ----
mean loss: 394.40
 ---- batch: 050 ----
mean loss: 400.44
 ---- batch: 060 ----
mean loss: 401.19
 ---- batch: 070 ----
mean loss: 390.80
 ---- batch: 080 ----
mean loss: 401.32
 ---- batch: 090 ----
mean loss: 407.49
train mean loss: 397.00
epoch train time: 0:00:02.333454
elapsed time: 0:01:55.092980
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 20:52:35.720808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.01
 ---- batch: 020 ----
mean loss: 409.78
 ---- batch: 030 ----
mean loss: 393.00
 ---- batch: 040 ----
mean loss: 401.53
 ---- batch: 050 ----
mean loss: 401.91
 ---- batch: 060 ----
mean loss: 392.02
 ---- batch: 070 ----
mean loss: 394.24
 ---- batch: 080 ----
mean loss: 408.87
 ---- batch: 090 ----
mean loss: 399.93
train mean loss: 399.03
epoch train time: 0:00:02.332897
elapsed time: 0:01:57.426054
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 20:52:38.053882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.17
 ---- batch: 020 ----
mean loss: 389.86
 ---- batch: 030 ----
mean loss: 375.14
 ---- batch: 040 ----
mean loss: 393.99
 ---- batch: 050 ----
mean loss: 404.29
 ---- batch: 060 ----
mean loss: 398.61
 ---- batch: 070 ----
mean loss: 396.68
 ---- batch: 080 ----
mean loss: 382.29
 ---- batch: 090 ----
mean loss: 386.73
train mean loss: 393.41
epoch train time: 0:00:02.337740
elapsed time: 0:01:59.763967
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 20:52:40.391794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.21
 ---- batch: 020 ----
mean loss: 394.06
 ---- batch: 030 ----
mean loss: 398.17
 ---- batch: 040 ----
mean loss: 394.12
 ---- batch: 050 ----
mean loss: 387.27
 ---- batch: 060 ----
mean loss: 392.23
 ---- batch: 070 ----
mean loss: 402.23
 ---- batch: 080 ----
mean loss: 412.45
 ---- batch: 090 ----
mean loss: 397.70
train mean loss: 396.50
epoch train time: 0:00:02.332246
elapsed time: 0:02:02.096407
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 20:52:42.724239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.05
 ---- batch: 020 ----
mean loss: 387.66
 ---- batch: 030 ----
mean loss: 398.13
 ---- batch: 040 ----
mean loss: 403.56
 ---- batch: 050 ----
mean loss: 401.07
 ---- batch: 060 ----
mean loss: 393.87
 ---- batch: 070 ----
mean loss: 396.64
 ---- batch: 080 ----
mean loss: 408.22
 ---- batch: 090 ----
mean loss: 406.73
train mean loss: 399.58
epoch train time: 0:00:02.333463
elapsed time: 0:02:04.430048
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 20:52:45.057914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.44
 ---- batch: 020 ----
mean loss: 399.66
 ---- batch: 030 ----
mean loss: 403.87
 ---- batch: 040 ----
mean loss: 400.16
 ---- batch: 050 ----
mean loss: 413.85
 ---- batch: 060 ----
mean loss: 413.02
 ---- batch: 070 ----
mean loss: 396.75
 ---- batch: 080 ----
mean loss: 388.29
 ---- batch: 090 ----
mean loss: 393.41
train mean loss: 403.16
epoch train time: 0:00:02.336429
elapsed time: 0:02:06.766745
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 20:52:47.394578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.64
 ---- batch: 020 ----
mean loss: 392.74
 ---- batch: 030 ----
mean loss: 401.65
 ---- batch: 040 ----
mean loss: 401.39
 ---- batch: 050 ----
mean loss: 398.13
 ---- batch: 060 ----
mean loss: 390.35
 ---- batch: 070 ----
mean loss: 389.42
 ---- batch: 080 ----
mean loss: 417.34
 ---- batch: 090 ----
mean loss: 412.25
train mean loss: 401.03
epoch train time: 0:00:02.335833
elapsed time: 0:02:09.102769
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 20:52:49.730633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.25
 ---- batch: 020 ----
mean loss: 409.69
 ---- batch: 030 ----
mean loss: 403.90
 ---- batch: 040 ----
mean loss: 407.99
 ---- batch: 050 ----
mean loss: 406.52
 ---- batch: 060 ----
mean loss: 410.96
 ---- batch: 070 ----
mean loss: 396.47
 ---- batch: 080 ----
mean loss: 401.04
 ---- batch: 090 ----
mean loss: 394.84
train mean loss: 403.32
epoch train time: 0:00:02.336427
elapsed time: 0:02:11.439426
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 20:52:52.067256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.80
 ---- batch: 020 ----
mean loss: 392.74
 ---- batch: 030 ----
mean loss: 399.74
 ---- batch: 040 ----
mean loss: 397.42
 ---- batch: 050 ----
mean loss: 396.49
 ---- batch: 060 ----
mean loss: 391.49
 ---- batch: 070 ----
mean loss: 402.24
 ---- batch: 080 ----
mean loss: 403.44
 ---- batch: 090 ----
mean loss: 384.73
train mean loss: 396.20
epoch train time: 0:00:02.343056
elapsed time: 0:02:13.782682
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 20:52:54.410587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.95
 ---- batch: 020 ----
mean loss: 394.40
 ---- batch: 030 ----
mean loss: 396.73
 ---- batch: 040 ----
mean loss: 392.01
 ---- batch: 050 ----
mean loss: 404.97
 ---- batch: 060 ----
mean loss: 385.36
 ---- batch: 070 ----
mean loss: 395.26
 ---- batch: 080 ----
mean loss: 397.53
 ---- batch: 090 ----
mean loss: 390.75
train mean loss: 396.07
epoch train time: 0:00:02.332503
elapsed time: 0:02:16.115444
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 20:52:56.743273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.54
 ---- batch: 020 ----
mean loss: 399.97
 ---- batch: 030 ----
mean loss: 401.64
 ---- batch: 040 ----
mean loss: 385.45
 ---- batch: 050 ----
mean loss: 394.70
 ---- batch: 060 ----
mean loss: 398.66
 ---- batch: 070 ----
mean loss: 394.61
 ---- batch: 080 ----
mean loss: 422.56
 ---- batch: 090 ----
mean loss: 408.55
train mean loss: 401.54
epoch train time: 0:00:02.335020
elapsed time: 0:02:18.450647
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 20:52:59.078480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.50
 ---- batch: 020 ----
mean loss: 404.07
 ---- batch: 030 ----
mean loss: 397.08
 ---- batch: 040 ----
mean loss: 408.52
 ---- batch: 050 ----
mean loss: 381.08
 ---- batch: 060 ----
mean loss: 385.15
 ---- batch: 070 ----
mean loss: 395.84
 ---- batch: 080 ----
mean loss: 394.62
 ---- batch: 090 ----
mean loss: 403.41
train mean loss: 396.00
epoch train time: 0:00:02.335313
elapsed time: 0:02:20.786188
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 20:53:01.414049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.58
 ---- batch: 020 ----
mean loss: 388.23
 ---- batch: 030 ----
mean loss: 398.28
 ---- batch: 040 ----
mean loss: 391.52
 ---- batch: 050 ----
mean loss: 396.79
 ---- batch: 060 ----
mean loss: 396.23
 ---- batch: 070 ----
mean loss: 391.05
 ---- batch: 080 ----
mean loss: 394.32
 ---- batch: 090 ----
mean loss: 407.24
train mean loss: 395.11
epoch train time: 0:00:02.345365
elapsed time: 0:02:23.131812
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 20:53:03.759652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.67
 ---- batch: 020 ----
mean loss: 417.00
 ---- batch: 030 ----
mean loss: 402.92
 ---- batch: 040 ----
mean loss: 385.25
 ---- batch: 050 ----
mean loss: 398.04
 ---- batch: 060 ----
mean loss: 413.63
 ---- batch: 070 ----
mean loss: 402.04
 ---- batch: 080 ----
mean loss: 404.69
 ---- batch: 090 ----
mean loss: 393.22
train mean loss: 403.11
epoch train time: 0:00:02.323106
elapsed time: 0:02:25.455147
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 20:53:06.082980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.43
 ---- batch: 020 ----
mean loss: 377.11
 ---- batch: 030 ----
mean loss: 384.10
 ---- batch: 040 ----
mean loss: 395.29
 ---- batch: 050 ----
mean loss: 400.29
 ---- batch: 060 ----
mean loss: 394.53
 ---- batch: 070 ----
mean loss: 423.12
 ---- batch: 080 ----
mean loss: 419.79
 ---- batch: 090 ----
mean loss: 396.92
train mean loss: 397.03
epoch train time: 0:00:02.329749
elapsed time: 0:02:27.785082
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 20:53:08.412932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.19
 ---- batch: 020 ----
mean loss: 378.91
 ---- batch: 030 ----
mean loss: 409.45
 ---- batch: 040 ----
mean loss: 401.73
 ---- batch: 050 ----
mean loss: 406.48
 ---- batch: 060 ----
mean loss: 419.73
 ---- batch: 070 ----
mean loss: 406.32
 ---- batch: 080 ----
mean loss: 400.99
 ---- batch: 090 ----
mean loss: 402.02
train mean loss: 401.37
epoch train time: 0:00:02.328434
elapsed time: 0:02:30.113807
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 20:53:10.741655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.43
 ---- batch: 020 ----
mean loss: 387.26
 ---- batch: 030 ----
mean loss: 400.94
 ---- batch: 040 ----
mean loss: 391.81
 ---- batch: 050 ----
mean loss: 393.54
 ---- batch: 060 ----
mean loss: 390.38
 ---- batch: 070 ----
mean loss: 388.75
 ---- batch: 080 ----
mean loss: 392.00
 ---- batch: 090 ----
mean loss: 394.85
train mean loss: 392.71
epoch train time: 0:00:02.331658
elapsed time: 0:02:32.445662
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 20:53:13.073491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.50
 ---- batch: 020 ----
mean loss: 393.55
 ---- batch: 030 ----
mean loss: 382.78
 ---- batch: 040 ----
mean loss: 408.63
 ---- batch: 050 ----
mean loss: 377.72
 ---- batch: 060 ----
mean loss: 398.75
 ---- batch: 070 ----
mean loss: 416.00
 ---- batch: 080 ----
mean loss: 411.66
 ---- batch: 090 ----
mean loss: 397.12
train mean loss: 398.28
epoch train time: 0:00:02.332588
elapsed time: 0:02:34.778480
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 20:53:15.406308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.82
 ---- batch: 020 ----
mean loss: 393.85
 ---- batch: 030 ----
mean loss: 397.74
 ---- batch: 040 ----
mean loss: 389.68
 ---- batch: 050 ----
mean loss: 400.79
 ---- batch: 060 ----
mean loss: 407.32
 ---- batch: 070 ----
mean loss: 387.08
 ---- batch: 080 ----
mean loss: 387.40
 ---- batch: 090 ----
mean loss: 387.13
train mean loss: 393.88
epoch train time: 0:00:02.323525
elapsed time: 0:02:37.102198
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 20:53:17.730034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.26
 ---- batch: 020 ----
mean loss: 400.42
 ---- batch: 030 ----
mean loss: 414.84
 ---- batch: 040 ----
mean loss: 398.74
 ---- batch: 050 ----
mean loss: 401.87
 ---- batch: 060 ----
mean loss: 392.09
 ---- batch: 070 ----
mean loss: 392.59
 ---- batch: 080 ----
mean loss: 406.14
 ---- batch: 090 ----
mean loss: 383.35
train mean loss: 398.55
epoch train time: 0:00:02.331653
elapsed time: 0:02:39.434060
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 20:53:20.061890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.18
 ---- batch: 020 ----
mean loss: 400.13
 ---- batch: 030 ----
mean loss: 395.88
 ---- batch: 040 ----
mean loss: 380.97
 ---- batch: 050 ----
mean loss: 389.35
 ---- batch: 060 ----
mean loss: 398.38
 ---- batch: 070 ----
mean loss: 399.98
 ---- batch: 080 ----
mean loss: 391.73
 ---- batch: 090 ----
mean loss: 389.18
train mean loss: 393.15
epoch train time: 0:00:02.325597
elapsed time: 0:02:41.759852
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 20:53:22.387691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.77
 ---- batch: 020 ----
mean loss: 388.10
 ---- batch: 030 ----
mean loss: 387.42
 ---- batch: 040 ----
mean loss: 396.93
 ---- batch: 050 ----
mean loss: 395.13
 ---- batch: 060 ----
mean loss: 407.41
 ---- batch: 070 ----
mean loss: 387.47
 ---- batch: 080 ----
mean loss: 380.34
 ---- batch: 090 ----
mean loss: 411.90
train mean loss: 394.79
epoch train time: 0:00:02.324521
elapsed time: 0:02:44.084582
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 20:53:24.712410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.03
 ---- batch: 020 ----
mean loss: 385.76
 ---- batch: 030 ----
mean loss: 396.38
 ---- batch: 040 ----
mean loss: 399.25
 ---- batch: 050 ----
mean loss: 405.69
 ---- batch: 060 ----
mean loss: 413.21
 ---- batch: 070 ----
mean loss: 400.66
 ---- batch: 080 ----
mean loss: 421.72
 ---- batch: 090 ----
mean loss: 400.71
train mean loss: 401.17
epoch train time: 0:00:02.330579
elapsed time: 0:02:46.415350
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 20:53:27.043198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.40
 ---- batch: 020 ----
mean loss: 392.19
 ---- batch: 030 ----
mean loss: 388.90
 ---- batch: 040 ----
mean loss: 379.07
 ---- batch: 050 ----
mean loss: 400.64
 ---- batch: 060 ----
mean loss: 386.00
 ---- batch: 070 ----
mean loss: 381.68
 ---- batch: 080 ----
mean loss: 398.64
 ---- batch: 090 ----
mean loss: 413.28
train mean loss: 390.91
epoch train time: 0:00:02.336451
elapsed time: 0:02:48.752004
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 20:53:29.379837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.30
 ---- batch: 020 ----
mean loss: 431.38
 ---- batch: 030 ----
mean loss: 400.87
 ---- batch: 040 ----
mean loss: 388.20
 ---- batch: 050 ----
mean loss: 390.96
 ---- batch: 060 ----
mean loss: 388.73
 ---- batch: 070 ----
mean loss: 395.61
 ---- batch: 080 ----
mean loss: 411.33
 ---- batch: 090 ----
mean loss: 405.57
train mean loss: 400.91
epoch train time: 0:00:02.324656
elapsed time: 0:02:51.076866
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 20:53:31.704708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.37
 ---- batch: 020 ----
mean loss: 382.24
 ---- batch: 030 ----
mean loss: 386.38
 ---- batch: 040 ----
mean loss: 393.67
 ---- batch: 050 ----
mean loss: 392.21
 ---- batch: 060 ----
mean loss: 393.37
 ---- batch: 070 ----
mean loss: 396.00
 ---- batch: 080 ----
mean loss: 389.07
 ---- batch: 090 ----
mean loss: 392.13
train mean loss: 390.02
epoch train time: 0:00:02.334032
elapsed time: 0:02:53.411096
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 20:53:34.038960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.64
 ---- batch: 020 ----
mean loss: 377.79
 ---- batch: 030 ----
mean loss: 388.16
 ---- batch: 040 ----
mean loss: 406.72
 ---- batch: 050 ----
mean loss: 391.93
 ---- batch: 060 ----
mean loss: 385.31
 ---- batch: 070 ----
mean loss: 384.52
 ---- batch: 080 ----
mean loss: 412.86
 ---- batch: 090 ----
mean loss: 383.60
train mean loss: 390.32
epoch train time: 0:00:02.329609
elapsed time: 0:02:55.740915
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 20:53:36.368749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.66
 ---- batch: 020 ----
mean loss: 393.43
 ---- batch: 030 ----
mean loss: 394.30
 ---- batch: 040 ----
mean loss: 379.59
 ---- batch: 050 ----
mean loss: 385.55
 ---- batch: 060 ----
mean loss: 417.50
 ---- batch: 070 ----
mean loss: 400.41
 ---- batch: 080 ----
mean loss: 388.75
 ---- batch: 090 ----
mean loss: 397.43
train mean loss: 393.80
epoch train time: 0:00:02.329470
elapsed time: 0:02:58.070571
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 20:53:38.698401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.95
 ---- batch: 020 ----
mean loss: 402.15
 ---- batch: 030 ----
mean loss: 395.34
 ---- batch: 040 ----
mean loss: 386.84
 ---- batch: 050 ----
mean loss: 399.12
 ---- batch: 060 ----
mean loss: 404.56
 ---- batch: 070 ----
mean loss: 401.38
 ---- batch: 080 ----
mean loss: 415.17
 ---- batch: 090 ----
mean loss: 400.08
train mean loss: 400.10
epoch train time: 0:00:02.328993
elapsed time: 0:03:00.399754
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 20:53:41.027582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.27
 ---- batch: 020 ----
mean loss: 393.42
 ---- batch: 030 ----
mean loss: 384.36
 ---- batch: 040 ----
mean loss: 396.14
 ---- batch: 050 ----
mean loss: 415.20
 ---- batch: 060 ----
mean loss: 389.42
 ---- batch: 070 ----
mean loss: 386.94
 ---- batch: 080 ----
mean loss: 385.72
 ---- batch: 090 ----
mean loss: 395.23
train mean loss: 393.18
epoch train time: 0:00:02.331091
elapsed time: 0:03:02.731018
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 20:53:43.358867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.14
 ---- batch: 020 ----
mean loss: 394.46
 ---- batch: 030 ----
mean loss: 401.97
 ---- batch: 040 ----
mean loss: 394.36
 ---- batch: 050 ----
mean loss: 392.38
 ---- batch: 060 ----
mean loss: 384.80
 ---- batch: 070 ----
mean loss: 391.07
 ---- batch: 080 ----
mean loss: 395.97
 ---- batch: 090 ----
mean loss: 399.28
train mean loss: 395.27
epoch train time: 0:00:02.324758
elapsed time: 0:03:05.055968
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 20:53:45.683794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.27
 ---- batch: 020 ----
mean loss: 391.03
 ---- batch: 030 ----
mean loss: 399.78
 ---- batch: 040 ----
mean loss: 383.18
 ---- batch: 050 ----
mean loss: 386.15
 ---- batch: 060 ----
mean loss: 394.36
 ---- batch: 070 ----
mean loss: 390.38
 ---- batch: 080 ----
mean loss: 398.52
 ---- batch: 090 ----
mean loss: 390.45
train mean loss: 390.97
epoch train time: 0:00:02.328521
elapsed time: 0:03:07.384681
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 20:53:48.012512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.72
 ---- batch: 020 ----
mean loss: 379.08
 ---- batch: 030 ----
mean loss: 397.87
 ---- batch: 040 ----
mean loss: 394.91
 ---- batch: 050 ----
mean loss: 390.26
 ---- batch: 060 ----
mean loss: 402.17
 ---- batch: 070 ----
mean loss: 388.29
 ---- batch: 080 ----
mean loss: 390.04
 ---- batch: 090 ----
mean loss: 386.38
train mean loss: 391.05
epoch train time: 0:00:02.329380
elapsed time: 0:03:09.714243
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 20:53:50.342075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.39
 ---- batch: 020 ----
mean loss: 412.44
 ---- batch: 030 ----
mean loss: 396.85
 ---- batch: 040 ----
mean loss: 400.54
 ---- batch: 050 ----
mean loss: 391.84
 ---- batch: 060 ----
mean loss: 399.16
 ---- batch: 070 ----
mean loss: 397.50
 ---- batch: 080 ----
mean loss: 400.27
 ---- batch: 090 ----
mean loss: 395.60
train mean loss: 398.57
epoch train time: 0:00:02.333643
elapsed time: 0:03:12.048070
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 20:53:52.675898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.34
 ---- batch: 020 ----
mean loss: 409.55
 ---- batch: 030 ----
mean loss: 397.19
 ---- batch: 040 ----
mean loss: 397.04
 ---- batch: 050 ----
mean loss: 384.68
 ---- batch: 060 ----
mean loss: 375.59
 ---- batch: 070 ----
mean loss: 395.09
 ---- batch: 080 ----
mean loss: 391.96
 ---- batch: 090 ----
mean loss: 395.13
train mean loss: 395.24
epoch train time: 0:00:02.330442
elapsed time: 0:03:14.378699
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 20:53:55.006530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.55
 ---- batch: 020 ----
mean loss: 397.42
 ---- batch: 030 ----
mean loss: 398.89
 ---- batch: 040 ----
mean loss: 393.38
 ---- batch: 050 ----
mean loss: 386.09
 ---- batch: 060 ----
mean loss: 382.94
 ---- batch: 070 ----
mean loss: 376.99
 ---- batch: 080 ----
mean loss: 394.53
 ---- batch: 090 ----
mean loss: 406.30
train mean loss: 393.84
epoch train time: 0:00:02.328241
elapsed time: 0:03:16.707119
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 20:53:57.334964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.75
 ---- batch: 020 ----
mean loss: 367.61
 ---- batch: 030 ----
mean loss: 394.39
 ---- batch: 040 ----
mean loss: 402.11
 ---- batch: 050 ----
mean loss: 400.98
 ---- batch: 060 ----
mean loss: 385.35
 ---- batch: 070 ----
mean loss: 390.80
 ---- batch: 080 ----
mean loss: 396.05
 ---- batch: 090 ----
mean loss: 391.95
train mean loss: 390.78
epoch train time: 0:00:02.328835
elapsed time: 0:03:19.036165
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 20:53:59.663995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.49
 ---- batch: 020 ----
mean loss: 384.86
 ---- batch: 030 ----
mean loss: 416.32
 ---- batch: 040 ----
mean loss: 381.84
 ---- batch: 050 ----
mean loss: 393.07
 ---- batch: 060 ----
mean loss: 399.69
 ---- batch: 070 ----
mean loss: 378.17
 ---- batch: 080 ----
mean loss: 383.58
 ---- batch: 090 ----
mean loss: 410.11
train mean loss: 394.11
epoch train time: 0:00:02.338775
elapsed time: 0:03:21.375178
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 20:54:02.003001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.68
 ---- batch: 020 ----
mean loss: 381.97
 ---- batch: 030 ----
mean loss: 393.40
 ---- batch: 040 ----
mean loss: 392.92
 ---- batch: 050 ----
mean loss: 389.88
 ---- batch: 060 ----
mean loss: 407.01
 ---- batch: 070 ----
mean loss: 401.80
 ---- batch: 080 ----
mean loss: 418.36
 ---- batch: 090 ----
mean loss: 408.76
train mean loss: 398.45
epoch train time: 0:00:02.324855
elapsed time: 0:03:23.700228
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 20:54:04.328053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.56
 ---- batch: 020 ----
mean loss: 388.77
 ---- batch: 030 ----
mean loss: 402.76
 ---- batch: 040 ----
mean loss: 402.43
 ---- batch: 050 ----
mean loss: 388.84
 ---- batch: 060 ----
mean loss: 403.03
 ---- batch: 070 ----
mean loss: 406.49
 ---- batch: 080 ----
mean loss: 382.72
 ---- batch: 090 ----
mean loss: 387.47
train mean loss: 398.65
epoch train time: 0:00:02.334500
elapsed time: 0:03:26.034904
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 20:54:06.662735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.44
 ---- batch: 020 ----
mean loss: 390.24
 ---- batch: 030 ----
mean loss: 391.05
 ---- batch: 040 ----
mean loss: 395.37
 ---- batch: 050 ----
mean loss: 383.09
 ---- batch: 060 ----
mean loss: 394.54
 ---- batch: 070 ----
mean loss: 376.04
 ---- batch: 080 ----
mean loss: 392.27
 ---- batch: 090 ----
mean loss: 389.18
train mean loss: 389.58
epoch train time: 0:00:02.328119
elapsed time: 0:03:28.363218
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 20:54:08.991054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.30
 ---- batch: 020 ----
mean loss: 408.42
 ---- batch: 030 ----
mean loss: 397.66
 ---- batch: 040 ----
mean loss: 411.45
 ---- batch: 050 ----
mean loss: 398.05
 ---- batch: 060 ----
mean loss: 385.95
 ---- batch: 070 ----
mean loss: 386.04
 ---- batch: 080 ----
mean loss: 405.62
 ---- batch: 090 ----
mean loss: 416.23
train mean loss: 398.97
epoch train time: 0:00:02.335294
elapsed time: 0:03:30.698705
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 20:54:11.326533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.13
 ---- batch: 020 ----
mean loss: 384.15
 ---- batch: 030 ----
mean loss: 381.98
 ---- batch: 040 ----
mean loss: 393.84
 ---- batch: 050 ----
mean loss: 393.89
 ---- batch: 060 ----
mean loss: 404.94
 ---- batch: 070 ----
mean loss: 388.96
 ---- batch: 080 ----
mean loss: 397.30
 ---- batch: 090 ----
mean loss: 385.39
train mean loss: 393.19
epoch train time: 0:00:02.331965
elapsed time: 0:03:33.030851
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 20:54:13.658677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.20
 ---- batch: 020 ----
mean loss: 396.06
 ---- batch: 030 ----
mean loss: 392.94
 ---- batch: 040 ----
mean loss: 390.95
 ---- batch: 050 ----
mean loss: 378.85
 ---- batch: 060 ----
mean loss: 392.89
 ---- batch: 070 ----
mean loss: 393.75
 ---- batch: 080 ----
mean loss: 395.06
 ---- batch: 090 ----
mean loss: 376.98
train mean loss: 388.19
epoch train time: 0:00:02.324682
elapsed time: 0:03:35.355716
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 20:54:15.983548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.51
 ---- batch: 020 ----
mean loss: 388.34
 ---- batch: 030 ----
mean loss: 415.30
 ---- batch: 040 ----
mean loss: 424.45
 ---- batch: 050 ----
mean loss: 390.07
 ---- batch: 060 ----
mean loss: 387.52
 ---- batch: 070 ----
mean loss: 406.79
 ---- batch: 080 ----
mean loss: 381.42
 ---- batch: 090 ----
mean loss: 384.61
train mean loss: 395.33
epoch train time: 0:00:02.335722
elapsed time: 0:03:37.691619
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 20:54:18.319470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.76
 ---- batch: 020 ----
mean loss: 383.46
 ---- batch: 030 ----
mean loss: 401.14
 ---- batch: 040 ----
mean loss: 401.15
 ---- batch: 050 ----
mean loss: 381.05
 ---- batch: 060 ----
mean loss: 381.39
 ---- batch: 070 ----
mean loss: 380.84
 ---- batch: 080 ----
mean loss: 396.92
 ---- batch: 090 ----
mean loss: 403.12
train mean loss: 390.97
epoch train time: 0:00:02.335985
elapsed time: 0:03:40.027825
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 20:54:20.655655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.67
 ---- batch: 020 ----
mean loss: 380.06
 ---- batch: 030 ----
mean loss: 401.48
 ---- batch: 040 ----
mean loss: 400.28
 ---- batch: 050 ----
mean loss: 405.14
 ---- batch: 060 ----
mean loss: 399.84
 ---- batch: 070 ----
mean loss: 398.31
 ---- batch: 080 ----
mean loss: 389.81
 ---- batch: 090 ----
mean loss: 383.17
train mean loss: 392.99
epoch train time: 0:00:02.333877
elapsed time: 0:03:42.361928
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 20:54:22.989762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.56
 ---- batch: 020 ----
mean loss: 403.13
 ---- batch: 030 ----
mean loss: 386.83
 ---- batch: 040 ----
mean loss: 389.38
 ---- batch: 050 ----
mean loss: 392.19
 ---- batch: 060 ----
mean loss: 373.77
 ---- batch: 070 ----
mean loss: 382.16
 ---- batch: 080 ----
mean loss: 393.90
 ---- batch: 090 ----
mean loss: 419.56
train mean loss: 394.02
epoch train time: 0:00:02.329320
elapsed time: 0:03:44.691446
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 20:54:25.319277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.68
 ---- batch: 020 ----
mean loss: 402.15
 ---- batch: 030 ----
mean loss: 398.22
 ---- batch: 040 ----
mean loss: 401.91
 ---- batch: 050 ----
mean loss: 392.00
 ---- batch: 060 ----
mean loss: 392.90
 ---- batch: 070 ----
mean loss: 394.39
 ---- batch: 080 ----
mean loss: 397.53
 ---- batch: 090 ----
mean loss: 376.90
train mean loss: 395.21
epoch train time: 0:00:02.331252
elapsed time: 0:03:47.022899
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 20:54:27.650739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.64
 ---- batch: 020 ----
mean loss: 386.69
 ---- batch: 030 ----
mean loss: 392.98
 ---- batch: 040 ----
mean loss: 389.13
 ---- batch: 050 ----
mean loss: 387.47
 ---- batch: 060 ----
mean loss: 380.58
 ---- batch: 070 ----
mean loss: 390.68
 ---- batch: 080 ----
mean loss: 387.56
 ---- batch: 090 ----
mean loss: 403.89
train mean loss: 393.54
epoch train time: 0:00:02.329189
elapsed time: 0:03:49.352285
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 20:54:29.980125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.41
 ---- batch: 020 ----
mean loss: 381.02
 ---- batch: 030 ----
mean loss: 398.78
 ---- batch: 040 ----
mean loss: 390.59
 ---- batch: 050 ----
mean loss: 395.56
 ---- batch: 060 ----
mean loss: 388.30
 ---- batch: 070 ----
mean loss: 384.80
 ---- batch: 080 ----
mean loss: 398.76
 ---- batch: 090 ----
mean loss: 399.42
train mean loss: 391.36
epoch train time: 0:00:02.326716
elapsed time: 0:03:51.679210
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 20:54:32.307040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.80
 ---- batch: 020 ----
mean loss: 393.65
 ---- batch: 030 ----
mean loss: 392.06
 ---- batch: 040 ----
mean loss: 395.80
 ---- batch: 050 ----
mean loss: 371.04
 ---- batch: 060 ----
mean loss: 380.83
 ---- batch: 070 ----
mean loss: 383.64
 ---- batch: 080 ----
mean loss: 371.02
 ---- batch: 090 ----
mean loss: 401.50
train mean loss: 388.02
epoch train time: 0:00:02.338501
elapsed time: 0:03:54.017915
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 20:54:34.645755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.70
 ---- batch: 020 ----
mean loss: 387.07
 ---- batch: 030 ----
mean loss: 388.21
 ---- batch: 040 ----
mean loss: 395.70
 ---- batch: 050 ----
mean loss: 402.71
 ---- batch: 060 ----
mean loss: 409.43
 ---- batch: 070 ----
mean loss: 406.85
 ---- batch: 080 ----
mean loss: 387.01
 ---- batch: 090 ----
mean loss: 380.74
train mean loss: 394.42
epoch train time: 0:00:02.337746
elapsed time: 0:03:56.355863
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 20:54:36.983722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.30
 ---- batch: 020 ----
mean loss: 398.01
 ---- batch: 030 ----
mean loss: 416.72
 ---- batch: 040 ----
mean loss: 396.54
 ---- batch: 050 ----
mean loss: 386.80
 ---- batch: 060 ----
mean loss: 406.45
 ---- batch: 070 ----
mean loss: 393.01
 ---- batch: 080 ----
mean loss: 384.53
 ---- batch: 090 ----
mean loss: 388.78
train mean loss: 397.88
epoch train time: 0:00:02.334292
elapsed time: 0:03:58.690384
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 20:54:39.318210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.06
 ---- batch: 020 ----
mean loss: 399.70
 ---- batch: 030 ----
mean loss: 387.58
 ---- batch: 040 ----
mean loss: 388.88
 ---- batch: 050 ----
mean loss: 379.57
 ---- batch: 060 ----
mean loss: 393.68
 ---- batch: 070 ----
mean loss: 382.91
 ---- batch: 080 ----
mean loss: 386.21
 ---- batch: 090 ----
mean loss: 377.14
train mean loss: 387.00
epoch train time: 0:00:02.337018
elapsed time: 0:04:01.027585
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 20:54:41.655419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.33
 ---- batch: 020 ----
mean loss: 400.70
 ---- batch: 030 ----
mean loss: 390.50
 ---- batch: 040 ----
mean loss: 388.46
 ---- batch: 050 ----
mean loss: 394.60
 ---- batch: 060 ----
mean loss: 391.13
 ---- batch: 070 ----
mean loss: 398.27
 ---- batch: 080 ----
mean loss: 393.24
 ---- batch: 090 ----
mean loss: 380.74
train mean loss: 392.06
epoch train time: 0:00:02.333653
elapsed time: 0:04:03.361428
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 20:54:43.989296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.04
 ---- batch: 020 ----
mean loss: 395.84
 ---- batch: 030 ----
mean loss: 383.31
 ---- batch: 040 ----
mean loss: 391.93
 ---- batch: 050 ----
mean loss: 411.63
 ---- batch: 060 ----
mean loss: 412.90
 ---- batch: 070 ----
mean loss: 390.72
 ---- batch: 080 ----
mean loss: 388.36
 ---- batch: 090 ----
mean loss: 370.32
train mean loss: 391.27
epoch train time: 0:00:02.325963
elapsed time: 0:04:05.687603
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 20:54:46.315432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.17
 ---- batch: 020 ----
mean loss: 394.19
 ---- batch: 030 ----
mean loss: 396.73
 ---- batch: 040 ----
mean loss: 385.53
 ---- batch: 050 ----
mean loss: 395.98
 ---- batch: 060 ----
mean loss: 421.16
 ---- batch: 070 ----
mean loss: 397.59
 ---- batch: 080 ----
mean loss: 402.95
 ---- batch: 090 ----
mean loss: 401.96
train mean loss: 397.64
epoch train time: 0:00:02.339802
elapsed time: 0:04:08.027584
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 20:54:48.655436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.79
 ---- batch: 020 ----
mean loss: 401.94
 ---- batch: 030 ----
mean loss: 399.11
 ---- batch: 040 ----
mean loss: 424.67
 ---- batch: 050 ----
mean loss: 409.80
 ---- batch: 060 ----
mean loss: 406.54
 ---- batch: 070 ----
mean loss: 389.34
 ---- batch: 080 ----
mean loss: 399.33
 ---- batch: 090 ----
mean loss: 386.99
train mean loss: 398.95
epoch train time: 0:00:02.333774
elapsed time: 0:04:10.361633
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 20:54:50.989478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.91
 ---- batch: 020 ----
mean loss: 396.60
 ---- batch: 030 ----
mean loss: 404.26
 ---- batch: 040 ----
mean loss: 400.94
 ---- batch: 050 ----
mean loss: 392.07
 ---- batch: 060 ----
mean loss: 404.88
 ---- batch: 070 ----
mean loss: 403.52
 ---- batch: 080 ----
mean loss: 385.18
 ---- batch: 090 ----
mean loss: 389.10
train mean loss: 395.15
epoch train time: 0:00:02.326696
elapsed time: 0:04:12.688541
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 20:54:53.316419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.39
 ---- batch: 020 ----
mean loss: 391.00
 ---- batch: 030 ----
mean loss: 405.10
 ---- batch: 040 ----
mean loss: 408.88
 ---- batch: 050 ----
mean loss: 425.32
 ---- batch: 060 ----
mean loss: 402.22
 ---- batch: 070 ----
mean loss: 398.56
 ---- batch: 080 ----
mean loss: 396.28
 ---- batch: 090 ----
mean loss: 388.11
train mean loss: 398.92
epoch train time: 0:00:02.336644
elapsed time: 0:04:15.025414
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 20:54:55.653243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.17
 ---- batch: 020 ----
mean loss: 376.56
 ---- batch: 030 ----
mean loss: 380.25
 ---- batch: 040 ----
mean loss: 386.42
 ---- batch: 050 ----
mean loss: 394.92
 ---- batch: 060 ----
mean loss: 384.73
 ---- batch: 070 ----
mean loss: 388.61
 ---- batch: 080 ----
mean loss: 387.94
 ---- batch: 090 ----
mean loss: 387.90
train mean loss: 386.93
epoch train time: 0:00:02.331789
elapsed time: 0:04:17.357392
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 20:54:57.985222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.09
 ---- batch: 020 ----
mean loss: 377.05
 ---- batch: 030 ----
mean loss: 388.33
 ---- batch: 040 ----
mean loss: 381.65
 ---- batch: 050 ----
mean loss: 378.83
 ---- batch: 060 ----
mean loss: 386.54
 ---- batch: 070 ----
mean loss: 383.97
 ---- batch: 080 ----
mean loss: 380.24
 ---- batch: 090 ----
mean loss: 386.57
train mean loss: 387.29
epoch train time: 0:00:02.334782
elapsed time: 0:04:19.692346
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 20:55:00.320182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.38
 ---- batch: 020 ----
mean loss: 374.98
 ---- batch: 030 ----
mean loss: 385.31
 ---- batch: 040 ----
mean loss: 399.31
 ---- batch: 050 ----
mean loss: 384.27
 ---- batch: 060 ----
mean loss: 393.54
 ---- batch: 070 ----
mean loss: 392.37
 ---- batch: 080 ----
mean loss: 390.75
 ---- batch: 090 ----
mean loss: 388.79
train mean loss: 390.26
epoch train time: 0:00:02.330882
elapsed time: 0:04:22.023499
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 20:55:02.651336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.08
 ---- batch: 020 ----
mean loss: 396.54
 ---- batch: 030 ----
mean loss: 385.86
 ---- batch: 040 ----
mean loss: 376.86
 ---- batch: 050 ----
mean loss: 387.38
 ---- batch: 060 ----
mean loss: 392.18
 ---- batch: 070 ----
mean loss: 388.52
 ---- batch: 080 ----
mean loss: 393.60
 ---- batch: 090 ----
mean loss: 391.86
train mean loss: 389.74
epoch train time: 0:00:02.331540
elapsed time: 0:04:24.355254
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 20:55:04.983122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.20
 ---- batch: 020 ----
mean loss: 398.90
 ---- batch: 030 ----
mean loss: 394.09
 ---- batch: 040 ----
mean loss: 388.86
 ---- batch: 050 ----
mean loss: 395.89
 ---- batch: 060 ----
mean loss: 393.66
 ---- batch: 070 ----
mean loss: 397.10
 ---- batch: 080 ----
mean loss: 383.79
 ---- batch: 090 ----
mean loss: 393.54
train mean loss: 392.12
epoch train time: 0:00:02.333603
elapsed time: 0:04:26.689076
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 20:55:07.316908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.95
 ---- batch: 020 ----
mean loss: 384.11
 ---- batch: 030 ----
mean loss: 386.58
 ---- batch: 040 ----
mean loss: 390.77
 ---- batch: 050 ----
mean loss: 396.55
 ---- batch: 060 ----
mean loss: 360.88
 ---- batch: 070 ----
mean loss: 384.16
 ---- batch: 080 ----
mean loss: 401.72
 ---- batch: 090 ----
mean loss: 396.38
train mean loss: 390.27
epoch train time: 0:00:02.331290
elapsed time: 0:04:29.020578
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 20:55:09.648406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.19
 ---- batch: 020 ----
mean loss: 396.10
 ---- batch: 030 ----
mean loss: 388.53
 ---- batch: 040 ----
mean loss: 373.61
 ---- batch: 050 ----
mean loss: 392.69
 ---- batch: 060 ----
mean loss: 387.82
 ---- batch: 070 ----
mean loss: 387.83
 ---- batch: 080 ----
mean loss: 379.47
 ---- batch: 090 ----
mean loss: 375.29
train mean loss: 386.44
epoch train time: 0:00:02.336092
elapsed time: 0:04:31.356864
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 20:55:11.984714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.29
 ---- batch: 020 ----
mean loss: 390.76
 ---- batch: 030 ----
mean loss: 403.42
 ---- batch: 040 ----
mean loss: 400.21
 ---- batch: 050 ----
mean loss: 392.95
 ---- batch: 060 ----
mean loss: 395.50
 ---- batch: 070 ----
mean loss: 390.08
 ---- batch: 080 ----
mean loss: 384.93
 ---- batch: 090 ----
mean loss: 392.90
train mean loss: 393.33
epoch train time: 0:00:02.322885
elapsed time: 0:04:33.679953
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 20:55:14.307785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.17
 ---- batch: 020 ----
mean loss: 394.23
 ---- batch: 030 ----
mean loss: 383.77
 ---- batch: 040 ----
mean loss: 385.26
 ---- batch: 050 ----
mean loss: 387.72
 ---- batch: 060 ----
mean loss: 390.53
 ---- batch: 070 ----
mean loss: 385.00
 ---- batch: 080 ----
mean loss: 396.17
 ---- batch: 090 ----
mean loss: 397.46
train mean loss: 389.34
epoch train time: 0:00:02.328478
elapsed time: 0:04:36.008642
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 20:55:16.636472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.19
 ---- batch: 020 ----
mean loss: 387.59
 ---- batch: 030 ----
mean loss: 397.53
 ---- batch: 040 ----
mean loss: 399.40
 ---- batch: 050 ----
mean loss: 386.05
 ---- batch: 060 ----
mean loss: 385.36
 ---- batch: 070 ----
mean loss: 376.89
 ---- batch: 080 ----
mean loss: 393.08
 ---- batch: 090 ----
mean loss: 397.70
train mean loss: 390.67
epoch train time: 0:00:02.334623
elapsed time: 0:04:38.343533
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 20:55:18.971391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.69
 ---- batch: 020 ----
mean loss: 383.53
 ---- batch: 030 ----
mean loss: 396.99
 ---- batch: 040 ----
mean loss: 388.22
 ---- batch: 050 ----
mean loss: 368.58
 ---- batch: 060 ----
mean loss: 382.63
 ---- batch: 070 ----
mean loss: 387.21
 ---- batch: 080 ----
mean loss: 390.95
 ---- batch: 090 ----
mean loss: 403.39
train mean loss: 387.82
epoch train time: 0:00:02.330077
elapsed time: 0:04:40.673822
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 20:55:21.301674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.75
 ---- batch: 020 ----
mean loss: 407.74
 ---- batch: 030 ----
mean loss: 387.33
 ---- batch: 040 ----
mean loss: 391.27
 ---- batch: 050 ----
mean loss: 398.00
 ---- batch: 060 ----
mean loss: 395.18
 ---- batch: 070 ----
mean loss: 384.96
 ---- batch: 080 ----
mean loss: 390.51
 ---- batch: 090 ----
mean loss: 382.56
train mean loss: 394.04
epoch train time: 0:00:02.325506
elapsed time: 0:04:42.999547
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 20:55:23.627357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.00
 ---- batch: 020 ----
mean loss: 395.97
 ---- batch: 030 ----
mean loss: 393.13
 ---- batch: 040 ----
mean loss: 395.55
 ---- batch: 050 ----
mean loss: 377.47
 ---- batch: 060 ----
mean loss: 382.46
 ---- batch: 070 ----
mean loss: 389.02
 ---- batch: 080 ----
mean loss: 390.59
 ---- batch: 090 ----
mean loss: 389.97
train mean loss: 386.56
epoch train time: 0:00:02.323768
elapsed time: 0:04:45.323484
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 20:55:25.951316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.30
 ---- batch: 020 ----
mean loss: 386.20
 ---- batch: 030 ----
mean loss: 376.23
 ---- batch: 040 ----
mean loss: 372.41
 ---- batch: 050 ----
mean loss: 384.08
 ---- batch: 060 ----
mean loss: 403.47
 ---- batch: 070 ----
mean loss: 402.12
 ---- batch: 080 ----
mean loss: 409.37
 ---- batch: 090 ----
mean loss: 382.73
train mean loss: 390.15
epoch train time: 0:00:02.337442
elapsed time: 0:04:47.661112
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 20:55:28.288962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.64
 ---- batch: 020 ----
mean loss: 379.24
 ---- batch: 030 ----
mean loss: 384.19
 ---- batch: 040 ----
mean loss: 403.28
 ---- batch: 050 ----
mean loss: 399.89
 ---- batch: 060 ----
mean loss: 398.87
 ---- batch: 070 ----
mean loss: 396.89
 ---- batch: 080 ----
mean loss: 382.65
 ---- batch: 090 ----
mean loss: 391.51
train mean loss: 392.72
epoch train time: 0:00:02.328972
elapsed time: 0:04:49.990286
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 20:55:30.618115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.30
 ---- batch: 020 ----
mean loss: 392.07
 ---- batch: 030 ----
mean loss: 420.63
 ---- batch: 040 ----
mean loss: 400.99
 ---- batch: 050 ----
mean loss: 391.35
 ---- batch: 060 ----
mean loss: 390.53
 ---- batch: 070 ----
mean loss: 381.65
 ---- batch: 080 ----
mean loss: 387.63
 ---- batch: 090 ----
mean loss: 385.34
train mean loss: 392.20
epoch train time: 0:00:02.332502
elapsed time: 0:04:52.322969
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 20:55:32.950797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.44
 ---- batch: 020 ----
mean loss: 380.71
 ---- batch: 030 ----
mean loss: 392.66
 ---- batch: 040 ----
mean loss: 412.03
 ---- batch: 050 ----
mean loss: 400.72
 ---- batch: 060 ----
mean loss: 396.93
 ---- batch: 070 ----
mean loss: 403.21
 ---- batch: 080 ----
mean loss: 378.20
 ---- batch: 090 ----
mean loss: 386.72
train mean loss: 391.13
epoch train time: 0:00:02.329278
elapsed time: 0:04:54.652440
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 20:55:35.280269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.26
 ---- batch: 020 ----
mean loss: 389.86
 ---- batch: 030 ----
mean loss: 378.28
 ---- batch: 040 ----
mean loss: 379.35
 ---- batch: 050 ----
mean loss: 387.35
 ---- batch: 060 ----
mean loss: 394.47
 ---- batch: 070 ----
mean loss: 390.50
 ---- batch: 080 ----
mean loss: 381.89
 ---- batch: 090 ----
mean loss: 387.55
train mean loss: 386.93
epoch train time: 0:00:02.335869
elapsed time: 0:04:56.988518
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 20:55:37.616359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.94
 ---- batch: 020 ----
mean loss: 382.07
 ---- batch: 030 ----
mean loss: 384.10
 ---- batch: 040 ----
mean loss: 386.74
 ---- batch: 050 ----
mean loss: 414.20
 ---- batch: 060 ----
mean loss: 405.49
 ---- batch: 070 ----
mean loss: 382.78
 ---- batch: 080 ----
mean loss: 390.56
 ---- batch: 090 ----
mean loss: 396.99
train mean loss: 391.79
epoch train time: 0:00:02.334824
elapsed time: 0:04:59.323544
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 20:55:39.951371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.40
 ---- batch: 020 ----
mean loss: 384.51
 ---- batch: 030 ----
mean loss: 388.41
 ---- batch: 040 ----
mean loss: 383.01
 ---- batch: 050 ----
mean loss: 377.64
 ---- batch: 060 ----
mean loss: 376.94
 ---- batch: 070 ----
mean loss: 393.78
 ---- batch: 080 ----
mean loss: 383.00
 ---- batch: 090 ----
mean loss: 369.94
train mean loss: 383.94
epoch train time: 0:00:02.325771
elapsed time: 0:05:01.649505
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 20:55:42.277335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.11
 ---- batch: 020 ----
mean loss: 383.20
 ---- batch: 030 ----
mean loss: 375.12
 ---- batch: 040 ----
mean loss: 373.80
 ---- batch: 050 ----
mean loss: 398.41
 ---- batch: 060 ----
mean loss: 393.50
 ---- batch: 070 ----
mean loss: 400.81
 ---- batch: 080 ----
mean loss: 383.71
 ---- batch: 090 ----
mean loss: 396.27
train mean loss: 390.42
epoch train time: 0:00:02.329513
elapsed time: 0:05:03.979194
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 20:55:44.607025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.78
 ---- batch: 020 ----
mean loss: 380.83
 ---- batch: 030 ----
mean loss: 385.81
 ---- batch: 040 ----
mean loss: 385.29
 ---- batch: 050 ----
mean loss: 404.99
 ---- batch: 060 ----
mean loss: 400.81
 ---- batch: 070 ----
mean loss: 411.22
 ---- batch: 080 ----
mean loss: 376.22
 ---- batch: 090 ----
mean loss: 389.67
train mean loss: 389.45
epoch train time: 0:00:02.335086
elapsed time: 0:05:06.314514
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 20:55:46.942389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.24
 ---- batch: 020 ----
mean loss: 365.72
 ---- batch: 030 ----
mean loss: 382.75
 ---- batch: 040 ----
mean loss: 374.23
 ---- batch: 050 ----
mean loss: 386.20
 ---- batch: 060 ----
mean loss: 391.60
 ---- batch: 070 ----
mean loss: 390.85
 ---- batch: 080 ----
mean loss: 393.11
 ---- batch: 090 ----
mean loss: 392.45
train mean loss: 384.40
epoch train time: 0:00:02.345774
elapsed time: 0:05:08.660530
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 20:55:49.288358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.17
 ---- batch: 020 ----
mean loss: 398.92
 ---- batch: 030 ----
mean loss: 394.77
 ---- batch: 040 ----
mean loss: 386.55
 ---- batch: 050 ----
mean loss: 401.03
 ---- batch: 060 ----
mean loss: 386.86
 ---- batch: 070 ----
mean loss: 393.08
 ---- batch: 080 ----
mean loss: 402.90
 ---- batch: 090 ----
mean loss: 374.11
train mean loss: 391.75
epoch train time: 0:00:02.343385
elapsed time: 0:05:11.004091
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 20:55:51.631921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.64
 ---- batch: 020 ----
mean loss: 374.54
 ---- batch: 030 ----
mean loss: 385.17
 ---- batch: 040 ----
mean loss: 387.70
 ---- batch: 050 ----
mean loss: 381.20
 ---- batch: 060 ----
mean loss: 380.75
 ---- batch: 070 ----
mean loss: 388.93
 ---- batch: 080 ----
mean loss: 392.45
 ---- batch: 090 ----
mean loss: 394.56
train mean loss: 385.87
epoch train time: 0:00:02.337351
elapsed time: 0:05:13.341632
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 20:55:53.969466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.01
 ---- batch: 020 ----
mean loss: 406.00
 ---- batch: 030 ----
mean loss: 416.68
 ---- batch: 040 ----
mean loss: 397.61
 ---- batch: 050 ----
mean loss: 406.00
 ---- batch: 060 ----
mean loss: 392.08
 ---- batch: 070 ----
mean loss: 380.35
 ---- batch: 080 ----
mean loss: 384.83
 ---- batch: 090 ----
mean loss: 382.91
train mean loss: 395.27
epoch train time: 0:00:02.329661
elapsed time: 0:05:15.671475
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 20:55:56.299304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.39
 ---- batch: 020 ----
mean loss: 381.91
 ---- batch: 030 ----
mean loss: 380.35
 ---- batch: 040 ----
mean loss: 401.99
 ---- batch: 050 ----
mean loss: 412.29
 ---- batch: 060 ----
mean loss: 397.41
 ---- batch: 070 ----
mean loss: 404.86
 ---- batch: 080 ----
mean loss: 413.00
 ---- batch: 090 ----
mean loss: 400.18
train mean loss: 396.39
epoch train time: 0:00:02.336969
elapsed time: 0:05:18.008646
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 20:55:58.636480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.66
 ---- batch: 020 ----
mean loss: 393.40
 ---- batch: 030 ----
mean loss: 387.58
 ---- batch: 040 ----
mean loss: 397.79
 ---- batch: 050 ----
mean loss: 392.88
 ---- batch: 060 ----
mean loss: 390.15
 ---- batch: 070 ----
mean loss: 385.52
 ---- batch: 080 ----
mean loss: 385.62
 ---- batch: 090 ----
mean loss: 376.61
train mean loss: 391.45
epoch train time: 0:00:02.331889
elapsed time: 0:05:20.340747
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 20:56:00.968576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.25
 ---- batch: 020 ----
mean loss: 393.44
 ---- batch: 030 ----
mean loss: 382.10
 ---- batch: 040 ----
mean loss: 391.41
 ---- batch: 050 ----
mean loss: 383.54
 ---- batch: 060 ----
mean loss: 390.76
 ---- batch: 070 ----
mean loss: 381.69
 ---- batch: 080 ----
mean loss: 388.19
 ---- batch: 090 ----
mean loss: 362.20
train mean loss: 384.57
epoch train time: 0:00:02.334815
elapsed time: 0:05:22.675746
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 20:56:03.303593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.00
 ---- batch: 020 ----
mean loss: 385.23
 ---- batch: 030 ----
mean loss: 371.25
 ---- batch: 040 ----
mean loss: 379.94
 ---- batch: 050 ----
mean loss: 380.84
 ---- batch: 060 ----
mean loss: 384.13
 ---- batch: 070 ----
mean loss: 396.89
 ---- batch: 080 ----
mean loss: 390.60
 ---- batch: 090 ----
mean loss: 415.23
train mean loss: 388.18
epoch train time: 0:00:02.334478
elapsed time: 0:05:25.010424
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 20:56:05.638248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.61
 ---- batch: 020 ----
mean loss: 399.41
 ---- batch: 030 ----
mean loss: 375.76
 ---- batch: 040 ----
mean loss: 373.19
 ---- batch: 050 ----
mean loss: 379.41
 ---- batch: 060 ----
mean loss: 380.40
 ---- batch: 070 ----
mean loss: 390.43
 ---- batch: 080 ----
mean loss: 387.59
 ---- batch: 090 ----
mean loss: 381.28
train mean loss: 384.20
epoch train time: 0:00:02.332304
elapsed time: 0:05:27.342909
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 20:56:07.970739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.94
 ---- batch: 020 ----
mean loss: 390.25
 ---- batch: 030 ----
mean loss: 396.90
 ---- batch: 040 ----
mean loss: 381.91
 ---- batch: 050 ----
mean loss: 388.69
 ---- batch: 060 ----
mean loss: 387.57
 ---- batch: 070 ----
mean loss: 388.33
 ---- batch: 080 ----
mean loss: 397.83
 ---- batch: 090 ----
mean loss: 403.94
train mean loss: 389.44
epoch train time: 0:00:02.331521
elapsed time: 0:05:29.674624
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 20:56:10.302432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.37
 ---- batch: 020 ----
mean loss: 408.81
 ---- batch: 030 ----
mean loss: 374.35
 ---- batch: 040 ----
mean loss: 383.70
 ---- batch: 050 ----
mean loss: 382.53
 ---- batch: 060 ----
mean loss: 387.61
 ---- batch: 070 ----
mean loss: 376.52
 ---- batch: 080 ----
mean loss: 376.98
 ---- batch: 090 ----
mean loss: 387.46
train mean loss: 385.03
epoch train time: 0:00:02.332937
elapsed time: 0:05:32.007723
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 20:56:12.635554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.71
 ---- batch: 020 ----
mean loss: 368.82
 ---- batch: 030 ----
mean loss: 385.20
 ---- batch: 040 ----
mean loss: 387.25
 ---- batch: 050 ----
mean loss: 382.81
 ---- batch: 060 ----
mean loss: 397.09
 ---- batch: 070 ----
mean loss: 386.26
 ---- batch: 080 ----
mean loss: 392.12
 ---- batch: 090 ----
mean loss: 389.27
train mean loss: 386.65
epoch train time: 0:00:02.333896
elapsed time: 0:05:34.341796
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 20:56:14.969645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.33
 ---- batch: 020 ----
mean loss: 385.39
 ---- batch: 030 ----
mean loss: 383.74
 ---- batch: 040 ----
mean loss: 384.49
 ---- batch: 050 ----
mean loss: 383.59
 ---- batch: 060 ----
mean loss: 397.94
 ---- batch: 070 ----
mean loss: 395.82
 ---- batch: 080 ----
mean loss: 396.34
 ---- batch: 090 ----
mean loss: 402.77
train mean loss: 391.20
epoch train time: 0:00:02.330315
elapsed time: 0:05:36.672304
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 20:56:17.300133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.72
 ---- batch: 020 ----
mean loss: 384.99
 ---- batch: 030 ----
mean loss: 399.11
 ---- batch: 040 ----
mean loss: 377.86
 ---- batch: 050 ----
mean loss: 369.20
 ---- batch: 060 ----
mean loss: 382.54
 ---- batch: 070 ----
mean loss: 384.92
 ---- batch: 080 ----
mean loss: 373.13
 ---- batch: 090 ----
mean loss: 395.54
train mean loss: 385.67
epoch train time: 0:00:02.329527
elapsed time: 0:05:39.002014
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 20:56:19.629842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.69
 ---- batch: 020 ----
mean loss: 393.16
 ---- batch: 030 ----
mean loss: 391.72
 ---- batch: 040 ----
mean loss: 400.70
 ---- batch: 050 ----
mean loss: 398.18
 ---- batch: 060 ----
mean loss: 393.10
 ---- batch: 070 ----
mean loss: 390.20
 ---- batch: 080 ----
mean loss: 400.48
 ---- batch: 090 ----
mean loss: 394.81
train mean loss: 394.39
epoch train time: 0:00:02.339592
elapsed time: 0:05:41.341797
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 20:56:21.969667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.85
 ---- batch: 020 ----
mean loss: 386.60
 ---- batch: 030 ----
mean loss: 375.20
 ---- batch: 040 ----
mean loss: 389.79
 ---- batch: 050 ----
mean loss: 396.68
 ---- batch: 060 ----
mean loss: 391.37
 ---- batch: 070 ----
mean loss: 390.36
 ---- batch: 080 ----
mean loss: 398.07
 ---- batch: 090 ----
mean loss: 386.32
train mean loss: 388.29
epoch train time: 0:00:02.332535
elapsed time: 0:05:43.674552
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 20:56:24.302384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.35
 ---- batch: 020 ----
mean loss: 380.99
 ---- batch: 030 ----
mean loss: 385.07
 ---- batch: 040 ----
mean loss: 401.13
 ---- batch: 050 ----
mean loss: 393.65
 ---- batch: 060 ----
mean loss: 383.93
 ---- batch: 070 ----
mean loss: 379.39
 ---- batch: 080 ----
mean loss: 379.97
 ---- batch: 090 ----
mean loss: 387.27
train mean loss: 387.90
epoch train time: 0:00:02.323520
elapsed time: 0:05:45.998247
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 20:56:26.626073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.86
 ---- batch: 020 ----
mean loss: 384.35
 ---- batch: 030 ----
mean loss: 379.92
 ---- batch: 040 ----
mean loss: 378.84
 ---- batch: 050 ----
mean loss: 393.77
 ---- batch: 060 ----
mean loss: 399.81
 ---- batch: 070 ----
mean loss: 387.49
 ---- batch: 080 ----
mean loss: 384.04
 ---- batch: 090 ----
mean loss: 394.67
train mean loss: 387.58
epoch train time: 0:00:02.336152
elapsed time: 0:05:48.334579
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 20:56:28.962408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.41
 ---- batch: 020 ----
mean loss: 386.87
 ---- batch: 030 ----
mean loss: 371.44
 ---- batch: 040 ----
mean loss: 378.78
 ---- batch: 050 ----
mean loss: 390.17
 ---- batch: 060 ----
mean loss: 381.12
 ---- batch: 070 ----
mean loss: 388.47
 ---- batch: 080 ----
mean loss: 392.30
 ---- batch: 090 ----
mean loss: 387.92
train mean loss: 383.38
epoch train time: 0:00:02.336172
elapsed time: 0:05:50.670935
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 20:56:31.298779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.50
 ---- batch: 020 ----
mean loss: 373.14
 ---- batch: 030 ----
mean loss: 397.05
 ---- batch: 040 ----
mean loss: 388.81
 ---- batch: 050 ----
mean loss: 385.77
 ---- batch: 060 ----
mean loss: 383.72
 ---- batch: 070 ----
mean loss: 382.38
 ---- batch: 080 ----
mean loss: 372.41
 ---- batch: 090 ----
mean loss: 385.38
train mean loss: 382.20
epoch train time: 0:00:02.331624
elapsed time: 0:05:53.002758
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 20:56:33.630599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.24
 ---- batch: 020 ----
mean loss: 381.03
 ---- batch: 030 ----
mean loss: 378.92
 ---- batch: 040 ----
mean loss: 380.21
 ---- batch: 050 ----
mean loss: 392.20
 ---- batch: 060 ----
mean loss: 378.77
 ---- batch: 070 ----
mean loss: 376.07
 ---- batch: 080 ----
mean loss: 381.39
 ---- batch: 090 ----
mean loss: 388.08
train mean loss: 381.84
epoch train time: 0:00:02.333493
elapsed time: 0:05:55.336445
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 20:56:35.964273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.35
 ---- batch: 020 ----
mean loss: 387.84
 ---- batch: 030 ----
mean loss: 388.90
 ---- batch: 040 ----
mean loss: 367.97
 ---- batch: 050 ----
mean loss: 387.14
 ---- batch: 060 ----
mean loss: 384.38
 ---- batch: 070 ----
mean loss: 399.73
 ---- batch: 080 ----
mean loss: 398.40
 ---- batch: 090 ----
mean loss: 377.37
train mean loss: 387.00
epoch train time: 0:00:02.330469
elapsed time: 0:05:57.667103
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 20:56:38.294935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.92
 ---- batch: 020 ----
mean loss: 381.01
 ---- batch: 030 ----
mean loss: 393.27
 ---- batch: 040 ----
mean loss: 406.07
 ---- batch: 050 ----
mean loss: 390.80
 ---- batch: 060 ----
mean loss: 396.10
 ---- batch: 070 ----
mean loss: 407.58
 ---- batch: 080 ----
mean loss: 426.00
 ---- batch: 090 ----
mean loss: 390.11
train mean loss: 397.45
epoch train time: 0:00:02.327099
elapsed time: 0:05:59.994406
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 20:56:40.622248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.23
 ---- batch: 020 ----
mean loss: 384.09
 ---- batch: 030 ----
mean loss: 376.00
 ---- batch: 040 ----
mean loss: 375.69
 ---- batch: 050 ----
mean loss: 384.35
 ---- batch: 060 ----
mean loss: 372.57
 ---- batch: 070 ----
mean loss: 384.11
 ---- batch: 080 ----
mean loss: 378.34
 ---- batch: 090 ----
mean loss: 397.72
train mean loss: 381.37
epoch train time: 0:00:02.341590
elapsed time: 0:06:02.336201
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 20:56:42.964052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.47
 ---- batch: 020 ----
mean loss: 393.61
 ---- batch: 030 ----
mean loss: 388.28
 ---- batch: 040 ----
mean loss: 375.64
 ---- batch: 050 ----
mean loss: 393.17
 ---- batch: 060 ----
mean loss: 382.92
 ---- batch: 070 ----
mean loss: 376.31
 ---- batch: 080 ----
mean loss: 387.51
 ---- batch: 090 ----
mean loss: 387.23
train mean loss: 386.96
epoch train time: 0:00:02.339984
elapsed time: 0:06:04.676406
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 20:56:45.304233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.38
 ---- batch: 020 ----
mean loss: 376.81
 ---- batch: 030 ----
mean loss: 373.41
 ---- batch: 040 ----
mean loss: 390.61
 ---- batch: 050 ----
mean loss: 404.62
 ---- batch: 060 ----
mean loss: 382.76
 ---- batch: 070 ----
mean loss: 386.19
 ---- batch: 080 ----
mean loss: 401.62
 ---- batch: 090 ----
mean loss: 407.00
train mean loss: 390.09
epoch train time: 0:00:02.325012
elapsed time: 0:06:07.001607
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 20:56:47.629445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.04
 ---- batch: 020 ----
mean loss: 385.33
 ---- batch: 030 ----
mean loss: 375.96
 ---- batch: 040 ----
mean loss: 379.78
 ---- batch: 050 ----
mean loss: 386.51
 ---- batch: 060 ----
mean loss: 371.66
 ---- batch: 070 ----
mean loss: 378.56
 ---- batch: 080 ----
mean loss: 386.90
 ---- batch: 090 ----
mean loss: 380.66
train mean loss: 381.94
epoch train time: 0:00:02.335027
elapsed time: 0:06:09.336817
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 20:56:49.964647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.29
 ---- batch: 020 ----
mean loss: 394.09
 ---- batch: 030 ----
mean loss: 408.12
 ---- batch: 040 ----
mean loss: 383.48
 ---- batch: 050 ----
mean loss: 374.00
 ---- batch: 060 ----
mean loss: 385.85
 ---- batch: 070 ----
mean loss: 380.06
 ---- batch: 080 ----
mean loss: 392.80
 ---- batch: 090 ----
mean loss: 390.02
train mean loss: 388.01
epoch train time: 0:00:02.332712
elapsed time: 0:06:11.669720
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 20:56:52.297563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.24
 ---- batch: 020 ----
mean loss: 392.00
 ---- batch: 030 ----
mean loss: 374.51
 ---- batch: 040 ----
mean loss: 376.51
 ---- batch: 050 ----
mean loss: 384.72
 ---- batch: 060 ----
mean loss: 393.69
 ---- batch: 070 ----
mean loss: 383.19
 ---- batch: 080 ----
mean loss: 401.77
 ---- batch: 090 ----
mean loss: 387.43
train mean loss: 384.03
epoch train time: 0:00:02.328304
elapsed time: 0:06:13.998211
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 20:56:54.626042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.71
 ---- batch: 020 ----
mean loss: 402.05
 ---- batch: 030 ----
mean loss: 386.30
 ---- batch: 040 ----
mean loss: 392.01
 ---- batch: 050 ----
mean loss: 386.10
 ---- batch: 060 ----
mean loss: 382.37
 ---- batch: 070 ----
mean loss: 376.87
 ---- batch: 080 ----
mean loss: 399.36
 ---- batch: 090 ----
mean loss: 375.95
train mean loss: 389.58
epoch train time: 0:00:02.335076
elapsed time: 0:06:16.333483
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 20:56:56.961342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.63
 ---- batch: 020 ----
mean loss: 393.24
 ---- batch: 030 ----
mean loss: 378.17
 ---- batch: 040 ----
mean loss: 381.36
 ---- batch: 050 ----
mean loss: 384.31
 ---- batch: 060 ----
mean loss: 378.24
 ---- batch: 070 ----
mean loss: 365.41
 ---- batch: 080 ----
mean loss: 386.40
 ---- batch: 090 ----
mean loss: 393.13
train mean loss: 382.96
epoch train time: 0:00:02.336860
elapsed time: 0:06:18.670569
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 20:56:59.298413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.63
 ---- batch: 020 ----
mean loss: 379.71
 ---- batch: 030 ----
mean loss: 383.01
 ---- batch: 040 ----
mean loss: 376.98
 ---- batch: 050 ----
mean loss: 366.75
 ---- batch: 060 ----
mean loss: 382.83
 ---- batch: 070 ----
mean loss: 380.92
 ---- batch: 080 ----
mean loss: 390.48
 ---- batch: 090 ----
mean loss: 401.52
train mean loss: 381.98
epoch train time: 0:00:02.328175
elapsed time: 0:06:20.998959
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 20:57:01.626768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.37
 ---- batch: 020 ----
mean loss: 391.58
 ---- batch: 030 ----
mean loss: 376.72
 ---- batch: 040 ----
mean loss: 387.11
 ---- batch: 050 ----
mean loss: 398.24
 ---- batch: 060 ----
mean loss: 395.80
 ---- batch: 070 ----
mean loss: 385.67
 ---- batch: 080 ----
mean loss: 392.59
 ---- batch: 090 ----
mean loss: 388.48
train mean loss: 390.08
epoch train time: 0:00:02.337046
elapsed time: 0:06:23.336220
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 20:57:03.964050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.78
 ---- batch: 020 ----
mean loss: 379.57
 ---- batch: 030 ----
mean loss: 395.69
 ---- batch: 040 ----
mean loss: 387.46
 ---- batch: 050 ----
mean loss: 377.92
 ---- batch: 060 ----
mean loss: 386.90
 ---- batch: 070 ----
mean loss: 376.58
 ---- batch: 080 ----
mean loss: 378.09
 ---- batch: 090 ----
mean loss: 407.23
train mean loss: 386.24
epoch train time: 0:00:02.335666
elapsed time: 0:06:25.672068
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 20:57:06.299900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.28
 ---- batch: 020 ----
mean loss: 374.35
 ---- batch: 030 ----
mean loss: 388.38
 ---- batch: 040 ----
mean loss: 379.48
 ---- batch: 050 ----
mean loss: 371.45
 ---- batch: 060 ----
mean loss: 394.77
 ---- batch: 070 ----
mean loss: 388.83
 ---- batch: 080 ----
mean loss: 381.59
 ---- batch: 090 ----
mean loss: 380.48
train mean loss: 381.18
epoch train time: 0:00:02.329327
elapsed time: 0:06:28.001581
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 20:57:08.629411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.44
 ---- batch: 020 ----
mean loss: 387.67
 ---- batch: 030 ----
mean loss: 392.14
 ---- batch: 040 ----
mean loss: 386.71
 ---- batch: 050 ----
mean loss: 391.48
 ---- batch: 060 ----
mean loss: 370.49
 ---- batch: 070 ----
mean loss: 381.87
 ---- batch: 080 ----
mean loss: 380.17
 ---- batch: 090 ----
mean loss: 372.37
train mean loss: 382.44
epoch train time: 0:00:02.331517
elapsed time: 0:06:30.333314
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 20:57:10.961164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.01
 ---- batch: 020 ----
mean loss: 373.78
 ---- batch: 030 ----
mean loss: 390.52
 ---- batch: 040 ----
mean loss: 389.73
 ---- batch: 050 ----
mean loss: 389.28
 ---- batch: 060 ----
mean loss: 373.93
 ---- batch: 070 ----
mean loss: 377.41
 ---- batch: 080 ----
mean loss: 372.35
 ---- batch: 090 ----
mean loss: 381.48
train mean loss: 380.89
epoch train time: 0:00:02.327238
elapsed time: 0:06:32.660752
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 20:57:13.288586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.77
 ---- batch: 020 ----
mean loss: 374.04
 ---- batch: 030 ----
mean loss: 379.31
 ---- batch: 040 ----
mean loss: 400.81
 ---- batch: 050 ----
mean loss: 370.17
 ---- batch: 060 ----
mean loss: 409.67
 ---- batch: 070 ----
mean loss: 377.62
 ---- batch: 080 ----
mean loss: 375.84
 ---- batch: 090 ----
mean loss: 378.35
train mean loss: 381.91
epoch train time: 0:00:02.330105
elapsed time: 0:06:34.991052
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 20:57:15.618887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.15
 ---- batch: 020 ----
mean loss: 381.50
 ---- batch: 030 ----
mean loss: 386.08
 ---- batch: 040 ----
mean loss: 385.02
 ---- batch: 050 ----
mean loss: 404.50
 ---- batch: 060 ----
mean loss: 391.79
 ---- batch: 070 ----
mean loss: 385.40
 ---- batch: 080 ----
mean loss: 378.38
 ---- batch: 090 ----
mean loss: 378.39
train mean loss: 386.50
epoch train time: 0:00:02.336210
elapsed time: 0:06:37.327469
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 20:57:17.955301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.69
 ---- batch: 020 ----
mean loss: 380.87
 ---- batch: 030 ----
mean loss: 382.29
 ---- batch: 040 ----
mean loss: 386.40
 ---- batch: 050 ----
mean loss: 366.27
 ---- batch: 060 ----
mean loss: 372.89
 ---- batch: 070 ----
mean loss: 388.58
 ---- batch: 080 ----
mean loss: 391.78
 ---- batch: 090 ----
mean loss: 385.07
train mean loss: 380.88
epoch train time: 0:00:02.341375
elapsed time: 0:06:39.669032
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 20:57:20.296869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.99
 ---- batch: 020 ----
mean loss: 374.67
 ---- batch: 030 ----
mean loss: 384.52
 ---- batch: 040 ----
mean loss: 389.59
 ---- batch: 050 ----
mean loss: 388.88
 ---- batch: 060 ----
mean loss: 380.50
 ---- batch: 070 ----
mean loss: 370.52
 ---- batch: 080 ----
mean loss: 388.15
 ---- batch: 090 ----
mean loss: 389.25
train mean loss: 382.06
epoch train time: 0:00:02.329630
elapsed time: 0:06:41.998902
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 20:57:22.626743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.83
 ---- batch: 020 ----
mean loss: 386.97
 ---- batch: 030 ----
mean loss: 388.30
 ---- batch: 040 ----
mean loss: 403.13
 ---- batch: 050 ----
mean loss: 378.17
 ---- batch: 060 ----
mean loss: 376.52
 ---- batch: 070 ----
mean loss: 375.60
 ---- batch: 080 ----
mean loss: 380.42
 ---- batch: 090 ----
mean loss: 378.42
train mean loss: 383.87
epoch train time: 0:00:02.340027
elapsed time: 0:06:44.339156
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 20:57:24.966984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.45
 ---- batch: 020 ----
mean loss: 382.81
 ---- batch: 030 ----
mean loss: 388.28
 ---- batch: 040 ----
mean loss: 377.89
 ---- batch: 050 ----
mean loss: 377.05
 ---- batch: 060 ----
mean loss: 369.65
 ---- batch: 070 ----
mean loss: 377.44
 ---- batch: 080 ----
mean loss: 371.85
 ---- batch: 090 ----
mean loss: 370.12
train mean loss: 378.42
epoch train time: 0:00:02.335418
elapsed time: 0:06:46.674768
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 20:57:27.302595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.45
 ---- batch: 020 ----
mean loss: 386.65
 ---- batch: 030 ----
mean loss: 382.86
 ---- batch: 040 ----
mean loss: 376.63
 ---- batch: 050 ----
mean loss: 375.75
 ---- batch: 060 ----
mean loss: 379.08
 ---- batch: 070 ----
mean loss: 382.67
 ---- batch: 080 ----
mean loss: 381.62
 ---- batch: 090 ----
mean loss: 374.36
train mean loss: 381.68
epoch train time: 0:00:02.334470
elapsed time: 0:06:49.009415
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 20:57:29.637248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.99
 ---- batch: 020 ----
mean loss: 375.95
 ---- batch: 030 ----
mean loss: 377.54
 ---- batch: 040 ----
mean loss: 385.47
 ---- batch: 050 ----
mean loss: 378.42
 ---- batch: 060 ----
mean loss: 388.93
 ---- batch: 070 ----
mean loss: 381.09
 ---- batch: 080 ----
mean loss: 371.52
 ---- batch: 090 ----
mean loss: 373.62
train mean loss: 379.15
epoch train time: 0:00:02.335368
elapsed time: 0:06:51.344969
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 20:57:31.972819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.37
 ---- batch: 020 ----
mean loss: 387.04
 ---- batch: 030 ----
mean loss: 396.81
 ---- batch: 040 ----
mean loss: 389.00
 ---- batch: 050 ----
mean loss: 376.90
 ---- batch: 060 ----
mean loss: 390.43
 ---- batch: 070 ----
mean loss: 372.08
 ---- batch: 080 ----
mean loss: 372.34
 ---- batch: 090 ----
mean loss: 377.28
train mean loss: 382.87
epoch train time: 0:00:02.327051
elapsed time: 0:06:53.672226
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 20:57:34.300056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.31
 ---- batch: 020 ----
mean loss: 387.65
 ---- batch: 030 ----
mean loss: 413.24
 ---- batch: 040 ----
mean loss: 376.10
 ---- batch: 050 ----
mean loss: 384.71
 ---- batch: 060 ----
mean loss: 398.09
 ---- batch: 070 ----
mean loss: 391.18
 ---- batch: 080 ----
mean loss: 377.33
 ---- batch: 090 ----
mean loss: 380.67
train mean loss: 388.27
epoch train time: 0:00:02.338933
elapsed time: 0:06:56.011347
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 20:57:36.639173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.59
 ---- batch: 020 ----
mean loss: 392.44
 ---- batch: 030 ----
mean loss: 376.36
 ---- batch: 040 ----
mean loss: 368.49
 ---- batch: 050 ----
mean loss: 372.00
 ---- batch: 060 ----
mean loss: 377.20
 ---- batch: 070 ----
mean loss: 380.24
 ---- batch: 080 ----
mean loss: 381.53
 ---- batch: 090 ----
mean loss: 381.11
train mean loss: 378.75
epoch train time: 0:00:02.333024
elapsed time: 0:06:58.344569
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 20:57:38.972398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.57
 ---- batch: 020 ----
mean loss: 409.04
 ---- batch: 030 ----
mean loss: 389.40
 ---- batch: 040 ----
mean loss: 391.33
 ---- batch: 050 ----
mean loss: 381.12
 ---- batch: 060 ----
mean loss: 376.41
 ---- batch: 070 ----
mean loss: 371.01
 ---- batch: 080 ----
mean loss: 371.36
 ---- batch: 090 ----
mean loss: 382.26
train mean loss: 383.17
epoch train time: 0:00:02.331699
elapsed time: 0:07:00.676442
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 20:57:41.304272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.05
 ---- batch: 020 ----
mean loss: 378.27
 ---- batch: 030 ----
mean loss: 381.29
 ---- batch: 040 ----
mean loss: 371.80
 ---- batch: 050 ----
mean loss: 395.74
 ---- batch: 060 ----
mean loss: 398.40
 ---- batch: 070 ----
mean loss: 395.43
 ---- batch: 080 ----
mean loss: 392.81
 ---- batch: 090 ----
mean loss: 407.16
train mean loss: 389.20
epoch train time: 0:00:02.335167
elapsed time: 0:07:03.011782
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 20:57:43.639620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.21
 ---- batch: 020 ----
mean loss: 374.74
 ---- batch: 030 ----
mean loss: 376.92
 ---- batch: 040 ----
mean loss: 370.59
 ---- batch: 050 ----
mean loss: 384.85
 ---- batch: 060 ----
mean loss: 379.07
 ---- batch: 070 ----
mean loss: 392.79
 ---- batch: 080 ----
mean loss: 376.02
 ---- batch: 090 ----
mean loss: 390.36
train mean loss: 381.95
epoch train time: 0:00:02.332331
elapsed time: 0:07:05.344310
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 20:57:45.972141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.62
 ---- batch: 020 ----
mean loss: 383.54
 ---- batch: 030 ----
mean loss: 378.81
 ---- batch: 040 ----
mean loss: 374.57
 ---- batch: 050 ----
mean loss: 387.44
 ---- batch: 060 ----
mean loss: 401.34
 ---- batch: 070 ----
mean loss: 381.98
 ---- batch: 080 ----
mean loss: 387.98
 ---- batch: 090 ----
mean loss: 388.76
train mean loss: 384.82
epoch train time: 0:00:02.329169
elapsed time: 0:07:07.673669
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 20:57:48.301499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.50
 ---- batch: 020 ----
mean loss: 378.39
 ---- batch: 030 ----
mean loss: 371.52
 ---- batch: 040 ----
mean loss: 376.42
 ---- batch: 050 ----
mean loss: 385.45
 ---- batch: 060 ----
mean loss: 377.84
 ---- batch: 070 ----
mean loss: 374.00
 ---- batch: 080 ----
mean loss: 391.67
 ---- batch: 090 ----
mean loss: 378.21
train mean loss: 380.92
epoch train time: 0:00:02.337386
elapsed time: 0:07:10.011235
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 20:57:50.639090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.73
 ---- batch: 020 ----
mean loss: 388.22
 ---- batch: 030 ----
mean loss: 392.15
 ---- batch: 040 ----
mean loss: 386.91
 ---- batch: 050 ----
mean loss: 379.77
 ---- batch: 060 ----
mean loss: 369.53
 ---- batch: 070 ----
mean loss: 391.17
 ---- batch: 080 ----
mean loss: 372.23
 ---- batch: 090 ----
mean loss: 380.75
train mean loss: 385.10
epoch train time: 0:00:02.337367
elapsed time: 0:07:12.348814
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 20:57:52.976643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.98
 ---- batch: 020 ----
mean loss: 391.60
 ---- batch: 030 ----
mean loss: 386.04
 ---- batch: 040 ----
mean loss: 370.78
 ---- batch: 050 ----
mean loss: 384.47
 ---- batch: 060 ----
mean loss: 378.24
 ---- batch: 070 ----
mean loss: 392.35
 ---- batch: 080 ----
mean loss: 374.60
 ---- batch: 090 ----
mean loss: 380.57
train mean loss: 383.64
epoch train time: 0:00:02.341892
elapsed time: 0:07:14.690883
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 20:57:55.318712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.55
 ---- batch: 020 ----
mean loss: 387.16
 ---- batch: 030 ----
mean loss: 369.38
 ---- batch: 040 ----
mean loss: 375.13
 ---- batch: 050 ----
mean loss: 366.25
 ---- batch: 060 ----
mean loss: 370.31
 ---- batch: 070 ----
mean loss: 376.61
 ---- batch: 080 ----
mean loss: 377.19
 ---- batch: 090 ----
mean loss: 370.52
train mean loss: 375.01
epoch train time: 0:00:02.326695
elapsed time: 0:07:17.017756
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 20:57:57.645625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.88
 ---- batch: 020 ----
mean loss: 373.62
 ---- batch: 030 ----
mean loss: 373.97
 ---- batch: 040 ----
mean loss: 359.90
 ---- batch: 050 ----
mean loss: 374.29
 ---- batch: 060 ----
mean loss: 378.95
 ---- batch: 070 ----
mean loss: 368.27
 ---- batch: 080 ----
mean loss: 380.08
 ---- batch: 090 ----
mean loss: 371.89
train mean loss: 371.88
epoch train time: 0:00:02.340119
elapsed time: 0:07:19.358149
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 20:57:59.985962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.11
 ---- batch: 020 ----
mean loss: 378.04
 ---- batch: 030 ----
mean loss: 378.29
 ---- batch: 040 ----
mean loss: 375.48
 ---- batch: 050 ----
mean loss: 375.02
 ---- batch: 060 ----
mean loss: 388.79
 ---- batch: 070 ----
mean loss: 396.02
 ---- batch: 080 ----
mean loss: 401.14
 ---- batch: 090 ----
mean loss: 394.71
train mean loss: 386.02
epoch train time: 0:00:02.333101
elapsed time: 0:07:21.691421
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 20:58:02.319247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.25
 ---- batch: 020 ----
mean loss: 388.03
 ---- batch: 030 ----
mean loss: 372.37
 ---- batch: 040 ----
mean loss: 364.35
 ---- batch: 050 ----
mean loss: 370.98
 ---- batch: 060 ----
mean loss: 378.97
 ---- batch: 070 ----
mean loss: 381.81
 ---- batch: 080 ----
mean loss: 384.42
 ---- batch: 090 ----
mean loss: 379.91
train mean loss: 380.74
epoch train time: 0:00:02.333436
elapsed time: 0:07:24.025029
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 20:58:04.652857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.14
 ---- batch: 020 ----
mean loss: 365.23
 ---- batch: 030 ----
mean loss: 381.82
 ---- batch: 040 ----
mean loss: 381.02
 ---- batch: 050 ----
mean loss: 371.42
 ---- batch: 060 ----
mean loss: 373.60
 ---- batch: 070 ----
mean loss: 377.31
 ---- batch: 080 ----
mean loss: 354.05
 ---- batch: 090 ----
mean loss: 385.55
train mean loss: 374.93
epoch train time: 0:00:02.329687
elapsed time: 0:07:26.354899
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 20:58:06.982757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.10
 ---- batch: 020 ----
mean loss: 382.70
 ---- batch: 030 ----
mean loss: 379.35
 ---- batch: 040 ----
mean loss: 374.61
 ---- batch: 050 ----
mean loss: 362.39
 ---- batch: 060 ----
mean loss: 373.74
 ---- batch: 070 ----
mean loss: 379.56
 ---- batch: 080 ----
mean loss: 376.82
 ---- batch: 090 ----
mean loss: 378.79
train mean loss: 375.54
epoch train time: 0:00:02.338373
elapsed time: 0:07:28.693491
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 20:58:09.321304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.40
 ---- batch: 020 ----
mean loss: 382.07
 ---- batch: 030 ----
mean loss: 386.13
 ---- batch: 040 ----
mean loss: 379.10
 ---- batch: 050 ----
mean loss: 368.03
 ---- batch: 060 ----
mean loss: 375.81
 ---- batch: 070 ----
mean loss: 377.06
 ---- batch: 080 ----
mean loss: 397.86
 ---- batch: 090 ----
mean loss: 391.14
train mean loss: 383.05
epoch train time: 0:00:02.337211
elapsed time: 0:07:31.030870
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 20:58:11.658697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.32
 ---- batch: 020 ----
mean loss: 383.87
 ---- batch: 030 ----
mean loss: 381.05
 ---- batch: 040 ----
mean loss: 374.55
 ---- batch: 050 ----
mean loss: 385.60
 ---- batch: 060 ----
mean loss: 365.82
 ---- batch: 070 ----
mean loss: 365.03
 ---- batch: 080 ----
mean loss: 378.12
 ---- batch: 090 ----
mean loss: 381.70
train mean loss: 378.41
epoch train time: 0:00:02.336883
elapsed time: 0:07:33.367955
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 20:58:13.995781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.77
 ---- batch: 020 ----
mean loss: 364.82
 ---- batch: 030 ----
mean loss: 379.59
 ---- batch: 040 ----
mean loss: 376.94
 ---- batch: 050 ----
mean loss: 380.93
 ---- batch: 060 ----
mean loss: 360.23
 ---- batch: 070 ----
mean loss: 370.37
 ---- batch: 080 ----
mean loss: 376.30
 ---- batch: 090 ----
mean loss: 370.12
train mean loss: 373.53
epoch train time: 0:00:02.335405
elapsed time: 0:07:35.703543
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 20:58:16.331360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.71
 ---- batch: 020 ----
mean loss: 388.91
 ---- batch: 030 ----
mean loss: 378.83
 ---- batch: 040 ----
mean loss: 374.72
 ---- batch: 050 ----
mean loss: 367.88
 ---- batch: 060 ----
mean loss: 380.60
 ---- batch: 070 ----
mean loss: 386.81
 ---- batch: 080 ----
mean loss: 383.19
 ---- batch: 090 ----
mean loss: 388.32
train mean loss: 382.47
epoch train time: 0:00:02.326856
elapsed time: 0:07:38.030593
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 20:58:18.658469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.49
 ---- batch: 020 ----
mean loss: 381.19
 ---- batch: 030 ----
mean loss: 410.32
 ---- batch: 040 ----
mean loss: 378.75
 ---- batch: 050 ----
mean loss: 361.05
 ---- batch: 060 ----
mean loss: 378.40
 ---- batch: 070 ----
mean loss: 378.38
 ---- batch: 080 ----
mean loss: 386.00
 ---- batch: 090 ----
mean loss: 369.27
train mean loss: 380.03
epoch train time: 0:00:02.338709
elapsed time: 0:07:40.369527
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 20:58:20.997357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.27
 ---- batch: 020 ----
mean loss: 375.31
 ---- batch: 030 ----
mean loss: 366.56
 ---- batch: 040 ----
mean loss: 388.34
 ---- batch: 050 ----
mean loss: 374.93
 ---- batch: 060 ----
mean loss: 378.41
 ---- batch: 070 ----
mean loss: 368.49
 ---- batch: 080 ----
mean loss: 369.42
 ---- batch: 090 ----
mean loss: 368.88
train mean loss: 373.05
epoch train time: 0:00:02.336825
elapsed time: 0:07:42.706563
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 20:58:23.334389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.89
 ---- batch: 020 ----
mean loss: 409.90
 ---- batch: 030 ----
mean loss: 396.15
 ---- batch: 040 ----
mean loss: 385.66
 ---- batch: 050 ----
mean loss: 369.75
 ---- batch: 060 ----
mean loss: 384.03
 ---- batch: 070 ----
mean loss: 373.51
 ---- batch: 080 ----
mean loss: 374.18
 ---- batch: 090 ----
mean loss: 381.18
train mean loss: 384.85
epoch train time: 0:00:02.332160
elapsed time: 0:07:45.038917
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 20:58:25.666770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.24
 ---- batch: 020 ----
mean loss: 363.90
 ---- batch: 030 ----
mean loss: 370.83
 ---- batch: 040 ----
mean loss: 384.59
 ---- batch: 050 ----
mean loss: 393.05
 ---- batch: 060 ----
mean loss: 382.17
 ---- batch: 070 ----
mean loss: 384.32
 ---- batch: 080 ----
mean loss: 387.26
 ---- batch: 090 ----
mean loss: 383.14
train mean loss: 379.55
epoch train time: 0:00:02.331435
elapsed time: 0:07:47.370624
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 20:58:27.998453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.71
 ---- batch: 020 ----
mean loss: 374.89
 ---- batch: 030 ----
mean loss: 376.12
 ---- batch: 040 ----
mean loss: 377.47
 ---- batch: 050 ----
mean loss: 369.79
 ---- batch: 060 ----
mean loss: 394.45
 ---- batch: 070 ----
mean loss: 369.55
 ---- batch: 080 ----
mean loss: 363.88
 ---- batch: 090 ----
mean loss: 382.88
train mean loss: 377.45
epoch train time: 0:00:02.334045
elapsed time: 0:07:49.704858
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 20:58:30.332687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.58
 ---- batch: 020 ----
mean loss: 374.96
 ---- batch: 030 ----
mean loss: 379.66
 ---- batch: 040 ----
mean loss: 378.05
 ---- batch: 050 ----
mean loss: 387.98
 ---- batch: 060 ----
mean loss: 385.94
 ---- batch: 070 ----
mean loss: 374.77
 ---- batch: 080 ----
mean loss: 369.46
 ---- batch: 090 ----
mean loss: 375.96
train mean loss: 377.53
epoch train time: 0:00:02.328065
elapsed time: 0:07:52.033105
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 20:58:32.660950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.65
 ---- batch: 020 ----
mean loss: 386.14
 ---- batch: 030 ----
mean loss: 389.74
 ---- batch: 040 ----
mean loss: 373.31
 ---- batch: 050 ----
mean loss: 386.90
 ---- batch: 060 ----
mean loss: 380.80
 ---- batch: 070 ----
mean loss: 371.31
 ---- batch: 080 ----
mean loss: 370.13
 ---- batch: 090 ----
mean loss: 373.06
train mean loss: 377.92
epoch train time: 0:00:02.331762
elapsed time: 0:07:54.365069
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 20:58:34.992900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.51
 ---- batch: 020 ----
mean loss: 379.14
 ---- batch: 030 ----
mean loss: 388.85
 ---- batch: 040 ----
mean loss: 369.81
 ---- batch: 050 ----
mean loss: 367.38
 ---- batch: 060 ----
mean loss: 370.93
 ---- batch: 070 ----
mean loss: 380.77
 ---- batch: 080 ----
mean loss: 392.71
 ---- batch: 090 ----
mean loss: 375.15
train mean loss: 377.12
epoch train time: 0:00:02.339163
elapsed time: 0:07:56.704409
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 20:58:37.332246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.04
 ---- batch: 020 ----
mean loss: 387.17
 ---- batch: 030 ----
mean loss: 390.60
 ---- batch: 040 ----
mean loss: 371.32
 ---- batch: 050 ----
mean loss: 378.36
 ---- batch: 060 ----
mean loss: 380.88
 ---- batch: 070 ----
mean loss: 368.12
 ---- batch: 080 ----
mean loss: 377.61
 ---- batch: 090 ----
mean loss: 372.64
train mean loss: 380.21
epoch train time: 0:00:02.327153
elapsed time: 0:07:59.031757
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 20:58:39.659585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.04
 ---- batch: 020 ----
mean loss: 377.49
 ---- batch: 030 ----
mean loss: 372.93
 ---- batch: 040 ----
mean loss: 377.05
 ---- batch: 050 ----
mean loss: 367.40
 ---- batch: 060 ----
mean loss: 373.04
 ---- batch: 070 ----
mean loss: 379.02
 ---- batch: 080 ----
mean loss: 383.12
 ---- batch: 090 ----
mean loss: 373.32
train mean loss: 377.62
epoch train time: 0:00:02.329931
elapsed time: 0:08:01.361882
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 20:58:41.989737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.39
 ---- batch: 020 ----
mean loss: 362.40
 ---- batch: 030 ----
mean loss: 362.09
 ---- batch: 040 ----
mean loss: 383.09
 ---- batch: 050 ----
mean loss: 370.29
 ---- batch: 060 ----
mean loss: 370.93
 ---- batch: 070 ----
mean loss: 370.49
 ---- batch: 080 ----
mean loss: 392.72
 ---- batch: 090 ----
mean loss: 382.26
train mean loss: 374.22
epoch train time: 0:00:02.338600
elapsed time: 0:08:03.700718
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 20:58:44.328544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.52
 ---- batch: 020 ----
mean loss: 377.46
 ---- batch: 030 ----
mean loss: 371.71
 ---- batch: 040 ----
mean loss: 380.86
 ---- batch: 050 ----
mean loss: 386.53
 ---- batch: 060 ----
mean loss: 356.36
 ---- batch: 070 ----
mean loss: 375.42
 ---- batch: 080 ----
mean loss: 378.79
 ---- batch: 090 ----
mean loss: 388.70
train mean loss: 378.76
epoch train time: 0:00:02.337290
elapsed time: 0:08:06.038182
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 20:58:46.666027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.91
 ---- batch: 020 ----
mean loss: 372.38
 ---- batch: 030 ----
mean loss: 367.81
 ---- batch: 040 ----
mean loss: 370.86
 ---- batch: 050 ----
mean loss: 373.65
 ---- batch: 060 ----
mean loss: 377.12
 ---- batch: 070 ----
mean loss: 381.17
 ---- batch: 080 ----
mean loss: 381.72
 ---- batch: 090 ----
mean loss: 370.90
train mean loss: 374.23
epoch train time: 0:00:02.326432
elapsed time: 0:08:08.364830
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 20:58:48.992667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.48
 ---- batch: 020 ----
mean loss: 372.40
 ---- batch: 030 ----
mean loss: 372.78
 ---- batch: 040 ----
mean loss: 362.74
 ---- batch: 050 ----
mean loss: 378.84
 ---- batch: 060 ----
mean loss: 373.04
 ---- batch: 070 ----
mean loss: 372.13
 ---- batch: 080 ----
mean loss: 379.61
 ---- batch: 090 ----
mean loss: 372.55
train mean loss: 372.38
epoch train time: 0:00:02.333613
elapsed time: 0:08:10.698668
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 20:58:51.326509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.87
 ---- batch: 020 ----
mean loss: 367.53
 ---- batch: 030 ----
mean loss: 374.76
 ---- batch: 040 ----
mean loss: 373.47
 ---- batch: 050 ----
mean loss: 377.25
 ---- batch: 060 ----
mean loss: 368.19
 ---- batch: 070 ----
mean loss: 380.11
 ---- batch: 080 ----
mean loss: 379.58
 ---- batch: 090 ----
mean loss: 377.09
train mean loss: 374.35
epoch train time: 0:00:02.333773
elapsed time: 0:08:13.032648
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 20:58:53.660477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.38
 ---- batch: 020 ----
mean loss: 374.90
 ---- batch: 030 ----
mean loss: 390.90
 ---- batch: 040 ----
mean loss: 395.77
 ---- batch: 050 ----
mean loss: 376.08
 ---- batch: 060 ----
mean loss: 383.53
 ---- batch: 070 ----
mean loss: 372.87
 ---- batch: 080 ----
mean loss: 370.81
 ---- batch: 090 ----
mean loss: 374.20
train mean loss: 377.44
epoch train time: 0:00:02.329306
elapsed time: 0:08:15.362149
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 20:58:55.989977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.02
 ---- batch: 020 ----
mean loss: 367.47
 ---- batch: 030 ----
mean loss: 365.12
 ---- batch: 040 ----
mean loss: 384.92
 ---- batch: 050 ----
mean loss: 392.68
 ---- batch: 060 ----
mean loss: 359.36
 ---- batch: 070 ----
mean loss: 378.24
 ---- batch: 080 ----
mean loss: 381.71
 ---- batch: 090 ----
mean loss: 366.57
train mean loss: 373.50
epoch train time: 0:00:02.341113
elapsed time: 0:08:17.703464
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 20:58:58.331294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.70
 ---- batch: 020 ----
mean loss: 372.23
 ---- batch: 030 ----
mean loss: 372.06
 ---- batch: 040 ----
mean loss: 364.45
 ---- batch: 050 ----
mean loss: 359.91
 ---- batch: 060 ----
mean loss: 386.54
 ---- batch: 070 ----
mean loss: 367.40
 ---- batch: 080 ----
mean loss: 363.92
 ---- batch: 090 ----
mean loss: 371.90
train mean loss: 371.59
epoch train time: 0:00:02.336901
elapsed time: 0:08:20.040546
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 20:59:00.668392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.09
 ---- batch: 020 ----
mean loss: 369.47
 ---- batch: 030 ----
mean loss: 370.12
 ---- batch: 040 ----
mean loss: 369.81
 ---- batch: 050 ----
mean loss: 379.78
 ---- batch: 060 ----
mean loss: 371.08
 ---- batch: 070 ----
mean loss: 371.77
 ---- batch: 080 ----
mean loss: 384.50
 ---- batch: 090 ----
mean loss: 384.61
train mean loss: 376.18
epoch train time: 0:00:02.329570
elapsed time: 0:08:22.370314
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 20:59:02.998145
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 372.01
 ---- batch: 020 ----
mean loss: 362.55
 ---- batch: 030 ----
mean loss: 357.19
 ---- batch: 040 ----
mean loss: 376.02
 ---- batch: 050 ----
mean loss: 358.78
 ---- batch: 060 ----
mean loss: 355.31
 ---- batch: 070 ----
mean loss: 373.76
 ---- batch: 080 ----
mean loss: 370.63
 ---- batch: 090 ----
mean loss: 373.84
train mean loss: 366.32
epoch train time: 0:00:02.333848
elapsed time: 0:08:24.704368
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 20:59:05.332178
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 374.04
 ---- batch: 020 ----
mean loss: 364.40
 ---- batch: 030 ----
mean loss: 373.43
 ---- batch: 040 ----
mean loss: 362.47
 ---- batch: 050 ----
mean loss: 373.64
 ---- batch: 060 ----
mean loss: 357.45
 ---- batch: 070 ----
mean loss: 369.43
 ---- batch: 080 ----
mean loss: 365.07
 ---- batch: 090 ----
mean loss: 357.72
train mean loss: 367.15
epoch train time: 0:00:02.338793
elapsed time: 0:08:27.043324
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 20:59:07.671157
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 371.94
 ---- batch: 020 ----
mean loss: 374.28
 ---- batch: 030 ----
mean loss: 353.71
 ---- batch: 040 ----
mean loss: 362.53
 ---- batch: 050 ----
mean loss: 367.41
 ---- batch: 060 ----
mean loss: 361.48
 ---- batch: 070 ----
mean loss: 376.73
 ---- batch: 080 ----
mean loss: 370.83
 ---- batch: 090 ----
mean loss: 360.26
train mean loss: 366.03
epoch train time: 0:00:02.331791
elapsed time: 0:08:29.375297
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 20:59:10.003125
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.95
 ---- batch: 020 ----
mean loss: 364.83
 ---- batch: 030 ----
mean loss: 367.62
 ---- batch: 040 ----
mean loss: 361.11
 ---- batch: 050 ----
mean loss: 356.26
 ---- batch: 060 ----
mean loss: 362.36
 ---- batch: 070 ----
mean loss: 365.77
 ---- batch: 080 ----
mean loss: 365.56
 ---- batch: 090 ----
mean loss: 366.46
train mean loss: 362.62
epoch train time: 0:00:02.333412
elapsed time: 0:08:31.708888
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 20:59:12.336719
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.68
 ---- batch: 020 ----
mean loss: 369.50
 ---- batch: 030 ----
mean loss: 374.48
 ---- batch: 040 ----
mean loss: 364.54
 ---- batch: 050 ----
mean loss: 365.42
 ---- batch: 060 ----
mean loss: 362.89
 ---- batch: 070 ----
mean loss: 360.20
 ---- batch: 080 ----
mean loss: 375.41
 ---- batch: 090 ----
mean loss: 367.35
train mean loss: 365.92
epoch train time: 0:00:02.333800
elapsed time: 0:08:34.042866
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 20:59:14.670695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 372.66
 ---- batch: 020 ----
mean loss: 361.39
 ---- batch: 030 ----
mean loss: 357.22
 ---- batch: 040 ----
mean loss: 362.74
 ---- batch: 050 ----
mean loss: 362.00
 ---- batch: 060 ----
mean loss: 370.61
 ---- batch: 070 ----
mean loss: 374.11
 ---- batch: 080 ----
mean loss: 369.36
 ---- batch: 090 ----
mean loss: 357.91
train mean loss: 364.62
epoch train time: 0:00:02.332424
elapsed time: 0:08:36.375474
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 20:59:17.003327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.78
 ---- batch: 020 ----
mean loss: 344.38
 ---- batch: 030 ----
mean loss: 362.57
 ---- batch: 040 ----
mean loss: 367.99
 ---- batch: 050 ----
mean loss: 364.60
 ---- batch: 060 ----
mean loss: 360.30
 ---- batch: 070 ----
mean loss: 363.63
 ---- batch: 080 ----
mean loss: 374.27
 ---- batch: 090 ----
mean loss: 354.28
train mean loss: 363.21
epoch train time: 0:00:02.347008
elapsed time: 0:08:38.722698
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 20:59:19.350526
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.51
 ---- batch: 020 ----
mean loss: 368.39
 ---- batch: 030 ----
mean loss: 351.10
 ---- batch: 040 ----
mean loss: 369.76
 ---- batch: 050 ----
mean loss: 351.56
 ---- batch: 060 ----
mean loss: 369.32
 ---- batch: 070 ----
mean loss: 359.47
 ---- batch: 080 ----
mean loss: 360.98
 ---- batch: 090 ----
mean loss: 368.89
train mean loss: 360.77
epoch train time: 0:00:02.325019
elapsed time: 0:08:41.047936
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 20:59:21.675774
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.77
 ---- batch: 020 ----
mean loss: 365.42
 ---- batch: 030 ----
mean loss: 362.10
 ---- batch: 040 ----
mean loss: 358.23
 ---- batch: 050 ----
mean loss: 360.76
 ---- batch: 060 ----
mean loss: 361.97
 ---- batch: 070 ----
mean loss: 370.05
 ---- batch: 080 ----
mean loss: 369.30
 ---- batch: 090 ----
mean loss: 375.39
train mean loss: 364.77
epoch train time: 0:00:02.332481
elapsed time: 0:08:43.380633
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 20:59:24.008469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.69
 ---- batch: 020 ----
mean loss: 360.89
 ---- batch: 030 ----
mean loss: 372.59
 ---- batch: 040 ----
mean loss: 358.04
 ---- batch: 050 ----
mean loss: 347.49
 ---- batch: 060 ----
mean loss: 375.11
 ---- batch: 070 ----
mean loss: 360.70
 ---- batch: 080 ----
mean loss: 360.36
 ---- batch: 090 ----
mean loss: 358.50
train mean loss: 361.51
epoch train time: 0:00:02.340381
elapsed time: 0:08:45.721217
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 20:59:26.349059
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 374.02
 ---- batch: 020 ----
mean loss: 368.18
 ---- batch: 030 ----
mean loss: 363.39
 ---- batch: 040 ----
mean loss: 360.75
 ---- batch: 050 ----
mean loss: 374.57
 ---- batch: 060 ----
mean loss: 358.13
 ---- batch: 070 ----
mean loss: 364.45
 ---- batch: 080 ----
mean loss: 358.16
 ---- batch: 090 ----
mean loss: 362.30
train mean loss: 365.28
epoch train time: 0:00:02.330793
elapsed time: 0:08:48.052199
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 20:59:28.680029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.08
 ---- batch: 020 ----
mean loss: 367.18
 ---- batch: 030 ----
mean loss: 363.54
 ---- batch: 040 ----
mean loss: 385.42
 ---- batch: 050 ----
mean loss: 355.94
 ---- batch: 060 ----
mean loss: 363.19
 ---- batch: 070 ----
mean loss: 368.80
 ---- batch: 080 ----
mean loss: 353.76
 ---- batch: 090 ----
mean loss: 357.16
train mean loss: 365.11
epoch train time: 0:00:02.331095
elapsed time: 0:08:50.383472
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 20:59:31.011302
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.27
 ---- batch: 020 ----
mean loss: 364.48
 ---- batch: 030 ----
mean loss: 374.00
 ---- batch: 040 ----
mean loss: 370.08
 ---- batch: 050 ----
mean loss: 374.12
 ---- batch: 060 ----
mean loss: 362.62
 ---- batch: 070 ----
mean loss: 348.15
 ---- batch: 080 ----
mean loss: 361.42
 ---- batch: 090 ----
mean loss: 373.52
train mean loss: 365.49
epoch train time: 0:00:02.341533
elapsed time: 0:08:52.725196
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 20:59:33.353027
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.74
 ---- batch: 020 ----
mean loss: 352.90
 ---- batch: 030 ----
mean loss: 357.92
 ---- batch: 040 ----
mean loss: 364.53
 ---- batch: 050 ----
mean loss: 362.54
 ---- batch: 060 ----
mean loss: 364.42
 ---- batch: 070 ----
mean loss: 365.30
 ---- batch: 080 ----
mean loss: 367.67
 ---- batch: 090 ----
mean loss: 367.73
train mean loss: 363.92
epoch train time: 0:00:02.331444
elapsed time: 0:08:55.056822
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 20:59:35.684671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.63
 ---- batch: 020 ----
mean loss: 367.84
 ---- batch: 030 ----
mean loss: 356.11
 ---- batch: 040 ----
mean loss: 366.36
 ---- batch: 050 ----
mean loss: 367.97
 ---- batch: 060 ----
mean loss: 360.80
 ---- batch: 070 ----
mean loss: 369.21
 ---- batch: 080 ----
mean loss: 355.37
 ---- batch: 090 ----
mean loss: 354.77
train mean loss: 363.35
epoch train time: 0:00:02.334381
elapsed time: 0:08:57.391400
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 20:59:38.019231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.67
 ---- batch: 020 ----
mean loss: 371.46
 ---- batch: 030 ----
mean loss: 361.44
 ---- batch: 040 ----
mean loss: 363.25
 ---- batch: 050 ----
mean loss: 357.38
 ---- batch: 060 ----
mean loss: 365.18
 ---- batch: 070 ----
mean loss: 340.08
 ---- batch: 080 ----
mean loss: 375.04
 ---- batch: 090 ----
mean loss: 369.64
train mean loss: 361.52
epoch train time: 0:00:02.336277
elapsed time: 0:08:59.727864
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 20:59:40.355698
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.77
 ---- batch: 020 ----
mean loss: 367.26
 ---- batch: 030 ----
mean loss: 371.14
 ---- batch: 040 ----
mean loss: 357.56
 ---- batch: 050 ----
mean loss: 365.33
 ---- batch: 060 ----
mean loss: 355.13
 ---- batch: 070 ----
mean loss: 365.34
 ---- batch: 080 ----
mean loss: 374.33
 ---- batch: 090 ----
mean loss: 360.58
train mean loss: 364.85
epoch train time: 0:00:02.335021
elapsed time: 0:09:02.063071
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 20:59:42.690911
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.90
 ---- batch: 020 ----
mean loss: 369.38
 ---- batch: 030 ----
mean loss: 367.88
 ---- batch: 040 ----
mean loss: 367.04
 ---- batch: 050 ----
mean loss: 369.44
 ---- batch: 060 ----
mean loss: 351.30
 ---- batch: 070 ----
mean loss: 362.38
 ---- batch: 080 ----
mean loss: 362.70
 ---- batch: 090 ----
mean loss: 379.03
train mean loss: 365.23
epoch train time: 0:00:02.335440
elapsed time: 0:09:04.398713
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 20:59:45.026543
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.76
 ---- batch: 020 ----
mean loss: 366.41
 ---- batch: 030 ----
mean loss: 362.90
 ---- batch: 040 ----
mean loss: 363.52
 ---- batch: 050 ----
mean loss: 367.16
 ---- batch: 060 ----
mean loss: 366.09
 ---- batch: 070 ----
mean loss: 370.40
 ---- batch: 080 ----
mean loss: 374.14
 ---- batch: 090 ----
mean loss: 365.86
train mean loss: 365.07
epoch train time: 0:00:02.341493
elapsed time: 0:09:06.740418
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 20:59:47.368252
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.01
 ---- batch: 020 ----
mean loss: 354.83
 ---- batch: 030 ----
mean loss: 365.92
 ---- batch: 040 ----
mean loss: 347.84
 ---- batch: 050 ----
mean loss: 362.69
 ---- batch: 060 ----
mean loss: 371.80
 ---- batch: 070 ----
mean loss: 360.51
 ---- batch: 080 ----
mean loss: 361.31
 ---- batch: 090 ----
mean loss: 361.35
train mean loss: 361.13
epoch train time: 0:00:02.329539
elapsed time: 0:09:09.070166
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 20:59:49.698002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.63
 ---- batch: 020 ----
mean loss: 370.71
 ---- batch: 030 ----
mean loss: 367.09
 ---- batch: 040 ----
mean loss: 359.11
 ---- batch: 050 ----
mean loss: 365.08
 ---- batch: 060 ----
mean loss: 360.58
 ---- batch: 070 ----
mean loss: 354.86
 ---- batch: 080 ----
mean loss: 362.02
 ---- batch: 090 ----
mean loss: 363.90
train mean loss: 362.33
epoch train time: 0:00:02.338316
elapsed time: 0:09:11.408675
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 20:59:52.036527
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.74
 ---- batch: 020 ----
mean loss: 357.34
 ---- batch: 030 ----
mean loss: 368.65
 ---- batch: 040 ----
mean loss: 374.16
 ---- batch: 050 ----
mean loss: 357.74
 ---- batch: 060 ----
mean loss: 366.98
 ---- batch: 070 ----
mean loss: 356.61
 ---- batch: 080 ----
mean loss: 375.60
 ---- batch: 090 ----
mean loss: 375.07
train mean loss: 366.56
epoch train time: 0:00:02.340613
elapsed time: 0:09:13.749493
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 20:59:54.377323
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.16
 ---- batch: 020 ----
mean loss: 356.38
 ---- batch: 030 ----
mean loss: 358.52
 ---- batch: 040 ----
mean loss: 359.72
 ---- batch: 050 ----
mean loss: 364.87
 ---- batch: 060 ----
mean loss: 365.78
 ---- batch: 070 ----
mean loss: 359.04
 ---- batch: 080 ----
mean loss: 357.69
 ---- batch: 090 ----
mean loss: 381.11
train mean loss: 363.50
epoch train time: 0:00:02.336064
elapsed time: 0:09:16.085773
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 20:59:56.713601
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.76
 ---- batch: 020 ----
mean loss: 366.50
 ---- batch: 030 ----
mean loss: 370.53
 ---- batch: 040 ----
mean loss: 365.71
 ---- batch: 050 ----
mean loss: 366.13
 ---- batch: 060 ----
mean loss: 359.52
 ---- batch: 070 ----
mean loss: 368.65
 ---- batch: 080 ----
mean loss: 369.45
 ---- batch: 090 ----
mean loss: 356.15
train mean loss: 363.68
epoch train time: 0:00:02.334811
elapsed time: 0:09:18.420776
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 20:59:59.048614
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.84
 ---- batch: 020 ----
mean loss: 365.78
 ---- batch: 030 ----
mean loss: 361.17
 ---- batch: 040 ----
mean loss: 363.97
 ---- batch: 050 ----
mean loss: 364.43
 ---- batch: 060 ----
mean loss: 357.59
 ---- batch: 070 ----
mean loss: 361.40
 ---- batch: 080 ----
mean loss: 350.41
 ---- batch: 090 ----
mean loss: 362.97
train mean loss: 359.84
epoch train time: 0:00:02.332472
elapsed time: 0:09:20.753492
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 21:00:01.381321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.12
 ---- batch: 020 ----
mean loss: 367.49
 ---- batch: 030 ----
mean loss: 370.39
 ---- batch: 040 ----
mean loss: 365.43
 ---- batch: 050 ----
mean loss: 356.12
 ---- batch: 060 ----
mean loss: 358.68
 ---- batch: 070 ----
mean loss: 355.98
 ---- batch: 080 ----
mean loss: 379.57
 ---- batch: 090 ----
mean loss: 382.99
train mean loss: 366.27
epoch train time: 0:00:02.335107
elapsed time: 0:09:23.088781
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 21:00:03.716612
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.05
 ---- batch: 020 ----
mean loss: 368.26
 ---- batch: 030 ----
mean loss: 354.20
 ---- batch: 040 ----
mean loss: 364.88
 ---- batch: 050 ----
mean loss: 369.66
 ---- batch: 060 ----
mean loss: 363.08
 ---- batch: 070 ----
mean loss: 356.77
 ---- batch: 080 ----
mean loss: 368.46
 ---- batch: 090 ----
mean loss: 360.80
train mean loss: 362.61
epoch train time: 0:00:02.335587
elapsed time: 0:09:25.424632
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 21:00:06.052519
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 370.22
 ---- batch: 020 ----
mean loss: 352.77
 ---- batch: 030 ----
mean loss: 358.16
 ---- batch: 040 ----
mean loss: 367.26
 ---- batch: 050 ----
mean loss: 359.90
 ---- batch: 060 ----
mean loss: 365.44
 ---- batch: 070 ----
mean loss: 365.37
 ---- batch: 080 ----
mean loss: 350.42
 ---- batch: 090 ----
mean loss: 364.97
train mean loss: 361.81
epoch train time: 0:00:02.342699
elapsed time: 0:09:27.767569
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 21:00:08.395407
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.50
 ---- batch: 020 ----
mean loss: 357.62
 ---- batch: 030 ----
mean loss: 355.21
 ---- batch: 040 ----
mean loss: 370.34
 ---- batch: 050 ----
mean loss: 367.08
 ---- batch: 060 ----
mean loss: 361.74
 ---- batch: 070 ----
mean loss: 365.72
 ---- batch: 080 ----
mean loss: 362.10
 ---- batch: 090 ----
mean loss: 372.61
train mean loss: 363.66
epoch train time: 0:00:02.334480
elapsed time: 0:09:30.102260
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 21:00:10.730104
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.10
 ---- batch: 020 ----
mean loss: 354.17
 ---- batch: 030 ----
mean loss: 368.80
 ---- batch: 040 ----
mean loss: 369.37
 ---- batch: 050 ----
mean loss: 357.72
 ---- batch: 060 ----
mean loss: 360.39
 ---- batch: 070 ----
mean loss: 359.84
 ---- batch: 080 ----
mean loss: 369.75
 ---- batch: 090 ----
mean loss: 359.32
train mean loss: 362.99
epoch train time: 0:00:02.342862
elapsed time: 0:09:32.445341
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 21:00:13.073161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.71
 ---- batch: 020 ----
mean loss: 363.73
 ---- batch: 030 ----
mean loss: 366.79
 ---- batch: 040 ----
mean loss: 349.18
 ---- batch: 050 ----
mean loss: 368.24
 ---- batch: 060 ----
mean loss: 360.56
 ---- batch: 070 ----
mean loss: 352.85
 ---- batch: 080 ----
mean loss: 363.85
 ---- batch: 090 ----
mean loss: 355.30
train mean loss: 362.53
epoch train time: 0:00:02.334114
elapsed time: 0:09:34.779633
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 21:00:15.407464
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.98
 ---- batch: 020 ----
mean loss: 365.76
 ---- batch: 030 ----
mean loss: 367.74
 ---- batch: 040 ----
mean loss: 354.02
 ---- batch: 050 ----
mean loss: 369.48
 ---- batch: 060 ----
mean loss: 365.15
 ---- batch: 070 ----
mean loss: 365.02
 ---- batch: 080 ----
mean loss: 367.72
 ---- batch: 090 ----
mean loss: 369.80
train mean loss: 364.03
epoch train time: 0:00:02.328852
elapsed time: 0:09:37.108719
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 21:00:17.736546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.74
 ---- batch: 020 ----
mean loss: 355.65
 ---- batch: 030 ----
mean loss: 351.80
 ---- batch: 040 ----
mean loss: 366.70
 ---- batch: 050 ----
mean loss: 357.04
 ---- batch: 060 ----
mean loss: 372.98
 ---- batch: 070 ----
mean loss: 360.67
 ---- batch: 080 ----
mean loss: 361.30
 ---- batch: 090 ----
mean loss: 353.87
train mean loss: 360.95
epoch train time: 0:00:02.338603
elapsed time: 0:09:39.447531
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 21:00:20.075345
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.22
 ---- batch: 020 ----
mean loss: 356.55
 ---- batch: 030 ----
mean loss: 347.84
 ---- batch: 040 ----
mean loss: 358.16
 ---- batch: 050 ----
mean loss: 365.58
 ---- batch: 060 ----
mean loss: 362.23
 ---- batch: 070 ----
mean loss: 364.69
 ---- batch: 080 ----
mean loss: 358.49
 ---- batch: 090 ----
mean loss: 361.10
train mean loss: 358.95
epoch train time: 0:00:02.342222
elapsed time: 0:09:41.789954
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 21:00:22.417774
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.15
 ---- batch: 020 ----
mean loss: 355.09
 ---- batch: 030 ----
mean loss: 367.96
 ---- batch: 040 ----
mean loss: 359.05
 ---- batch: 050 ----
mean loss: 363.98
 ---- batch: 060 ----
mean loss: 360.15
 ---- batch: 070 ----
mean loss: 372.60
 ---- batch: 080 ----
mean loss: 359.70
 ---- batch: 090 ----
mean loss: 362.00
train mean loss: 362.22
epoch train time: 0:00:02.329220
elapsed time: 0:09:44.119352
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 21:00:24.747183
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.51
 ---- batch: 020 ----
mean loss: 368.30
 ---- batch: 030 ----
mean loss: 358.17
 ---- batch: 040 ----
mean loss: 374.49
 ---- batch: 050 ----
mean loss: 369.40
 ---- batch: 060 ----
mean loss: 363.08
 ---- batch: 070 ----
mean loss: 369.93
 ---- batch: 080 ----
mean loss: 371.27
 ---- batch: 090 ----
mean loss: 355.59
train mean loss: 365.09
epoch train time: 0:00:02.331456
elapsed time: 0:09:46.450999
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 21:00:27.078852
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.16
 ---- batch: 020 ----
mean loss: 355.48
 ---- batch: 030 ----
mean loss: 365.79
 ---- batch: 040 ----
mean loss: 357.50
 ---- batch: 050 ----
mean loss: 351.11
 ---- batch: 060 ----
mean loss: 355.30
 ---- batch: 070 ----
mean loss: 355.40
 ---- batch: 080 ----
mean loss: 366.97
 ---- batch: 090 ----
mean loss: 359.70
train mean loss: 358.66
epoch train time: 0:00:02.335921
elapsed time: 0:09:48.787131
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 21:00:29.414982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 379.87
 ---- batch: 020 ----
mean loss: 364.53
 ---- batch: 030 ----
mean loss: 376.43
 ---- batch: 040 ----
mean loss: 359.82
 ---- batch: 050 ----
mean loss: 369.67
 ---- batch: 060 ----
mean loss: 357.01
 ---- batch: 070 ----
mean loss: 361.53
 ---- batch: 080 ----
mean loss: 352.98
 ---- batch: 090 ----
mean loss: 358.97
train mean loss: 364.74
epoch train time: 0:00:02.335195
elapsed time: 0:09:51.122535
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 21:00:31.750359
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 371.08
 ---- batch: 020 ----
mean loss: 361.57
 ---- batch: 030 ----
mean loss: 354.54
 ---- batch: 040 ----
mean loss: 361.45
 ---- batch: 050 ----
mean loss: 367.97
 ---- batch: 060 ----
mean loss: 368.37
 ---- batch: 070 ----
mean loss: 360.99
 ---- batch: 080 ----
mean loss: 361.66
 ---- batch: 090 ----
mean loss: 368.80
train mean loss: 363.62
epoch train time: 0:00:02.338486
elapsed time: 0:09:53.461207
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 21:00:34.089046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.89
 ---- batch: 020 ----
mean loss: 366.36
 ---- batch: 030 ----
mean loss: 365.06
 ---- batch: 040 ----
mean loss: 364.31
 ---- batch: 050 ----
mean loss: 363.16
 ---- batch: 060 ----
mean loss: 368.00
 ---- batch: 070 ----
mean loss: 372.56
 ---- batch: 080 ----
mean loss: 365.98
 ---- batch: 090 ----
mean loss: 343.91
train mean loss: 361.94
epoch train time: 0:00:02.332522
elapsed time: 0:09:55.793967
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 21:00:36.421800
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.90
 ---- batch: 020 ----
mean loss: 354.43
 ---- batch: 030 ----
mean loss: 364.71
 ---- batch: 040 ----
mean loss: 358.63
 ---- batch: 050 ----
mean loss: 358.56
 ---- batch: 060 ----
mean loss: 341.71
 ---- batch: 070 ----
mean loss: 360.54
 ---- batch: 080 ----
mean loss: 361.72
 ---- batch: 090 ----
mean loss: 368.61
train mean loss: 358.78
epoch train time: 0:00:02.337455
elapsed time: 0:09:58.131621
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 21:00:38.759466
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.92
 ---- batch: 020 ----
mean loss: 364.40
 ---- batch: 030 ----
mean loss: 366.33
 ---- batch: 040 ----
mean loss: 369.30
 ---- batch: 050 ----
mean loss: 363.67
 ---- batch: 060 ----
mean loss: 359.27
 ---- batch: 070 ----
mean loss: 366.89
 ---- batch: 080 ----
mean loss: 347.28
 ---- batch: 090 ----
mean loss: 370.17
train mean loss: 362.69
epoch train time: 0:00:02.339067
elapsed time: 0:10:00.470962
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 21:00:41.098793
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 361.89
 ---- batch: 020 ----
mean loss: 376.49
 ---- batch: 030 ----
mean loss: 367.74
 ---- batch: 040 ----
mean loss: 363.58
 ---- batch: 050 ----
mean loss: 366.24
 ---- batch: 060 ----
mean loss: 364.51
 ---- batch: 070 ----
mean loss: 366.68
 ---- batch: 080 ----
mean loss: 356.31
 ---- batch: 090 ----
mean loss: 374.29
train mean loss: 365.58
epoch train time: 0:00:02.338533
elapsed time: 0:10:02.809679
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 21:00:43.437508
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.93
 ---- batch: 020 ----
mean loss: 379.87
 ---- batch: 030 ----
mean loss: 358.55
 ---- batch: 040 ----
mean loss: 359.22
 ---- batch: 050 ----
mean loss: 369.60
 ---- batch: 060 ----
mean loss: 350.85
 ---- batch: 070 ----
mean loss: 364.23
 ---- batch: 080 ----
mean loss: 365.75
 ---- batch: 090 ----
mean loss: 357.71
train mean loss: 363.33
epoch train time: 0:00:02.328686
elapsed time: 0:10:05.138548
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 21:00:45.766401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.50
 ---- batch: 020 ----
mean loss: 358.82
 ---- batch: 030 ----
mean loss: 366.69
 ---- batch: 040 ----
mean loss: 368.17
 ---- batch: 050 ----
mean loss: 369.06
 ---- batch: 060 ----
mean loss: 363.41
 ---- batch: 070 ----
mean loss: 374.51
 ---- batch: 080 ----
mean loss: 366.60
 ---- batch: 090 ----
mean loss: 374.30
train mean loss: 365.64
epoch train time: 0:00:02.329913
elapsed time: 0:10:07.468685
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 21:00:48.096513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.87
 ---- batch: 020 ----
mean loss: 368.94
 ---- batch: 030 ----
mean loss: 354.89
 ---- batch: 040 ----
mean loss: 369.90
 ---- batch: 050 ----
mean loss: 364.36
 ---- batch: 060 ----
mean loss: 355.67
 ---- batch: 070 ----
mean loss: 361.74
 ---- batch: 080 ----
mean loss: 359.47
 ---- batch: 090 ----
mean loss: 362.57
train mean loss: 362.65
epoch train time: 0:00:02.332632
elapsed time: 0:10:09.801493
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 21:00:50.429323
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.95
 ---- batch: 020 ----
mean loss: 359.30
 ---- batch: 030 ----
mean loss: 369.78
 ---- batch: 040 ----
mean loss: 373.53
 ---- batch: 050 ----
mean loss: 363.10
 ---- batch: 060 ----
mean loss: 363.82
 ---- batch: 070 ----
mean loss: 340.22
 ---- batch: 080 ----
mean loss: 355.69
 ---- batch: 090 ----
mean loss: 371.50
train mean loss: 361.20
epoch train time: 0:00:02.335974
elapsed time: 0:10:12.137656
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 21:00:52.765489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.52
 ---- batch: 020 ----
mean loss: 362.14
 ---- batch: 030 ----
mean loss: 352.70
 ---- batch: 040 ----
mean loss: 370.98
 ---- batch: 050 ----
mean loss: 361.27
 ---- batch: 060 ----
mean loss: 362.15
 ---- batch: 070 ----
mean loss: 363.62
 ---- batch: 080 ----
mean loss: 356.52
 ---- batch: 090 ----
mean loss: 366.92
train mean loss: 361.51
epoch train time: 0:00:02.335804
elapsed time: 0:10:14.473643
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 21:00:55.101473
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.53
 ---- batch: 020 ----
mean loss: 367.28
 ---- batch: 030 ----
mean loss: 359.56
 ---- batch: 040 ----
mean loss: 362.97
 ---- batch: 050 ----
mean loss: 373.65
 ---- batch: 060 ----
mean loss: 355.12
 ---- batch: 070 ----
mean loss: 350.82
 ---- batch: 080 ----
mean loss: 355.68
 ---- batch: 090 ----
mean loss: 362.16
train mean loss: 360.07
epoch train time: 0:00:02.327915
elapsed time: 0:10:16.805092
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_1/checkpoint.pth.tar
**** end time: 2019-09-25 21:00:57.432878 ****
