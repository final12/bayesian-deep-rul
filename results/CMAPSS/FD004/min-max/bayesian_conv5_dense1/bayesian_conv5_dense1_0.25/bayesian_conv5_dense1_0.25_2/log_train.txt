Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.25, resume=False, step_size=200, visualize_step=50)
pid: 8854
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 16:40:55.740566 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 16:40:55.758033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2992.22
 ---- batch: 020 ----
mean loss: 1883.67
train mean loss: 2171.46
epoch train time: 0:00:11.523913
elapsed time: 0:00:11.549605
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 16:41:07.290214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1435.25
 ---- batch: 020 ----
mean loss: 1321.86
train mean loss: 1375.00
epoch train time: 0:00:03.905994
elapsed time: 0:00:15.456819
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 16:41:11.197470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1282.23
 ---- batch: 020 ----
mean loss: 1228.09
train mean loss: 1245.49
epoch train time: 0:00:03.906375
elapsed time: 0:00:19.364333
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 16:41:15.105129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1192.25
 ---- batch: 020 ----
mean loss: 1134.38
train mean loss: 1161.50
epoch train time: 0:00:03.915774
elapsed time: 0:00:23.281333
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 16:41:19.022179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1120.84
 ---- batch: 020 ----
mean loss: 1140.64
train mean loss: 1117.22
epoch train time: 0:00:03.907112
elapsed time: 0:00:27.189806
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 16:41:22.930618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1127.54
 ---- batch: 020 ----
mean loss: 1101.47
train mean loss: 1120.07
epoch train time: 0:00:03.908766
elapsed time: 0:00:31.099790
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 16:41:26.840607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1097.86
 ---- batch: 020 ----
mean loss: 1101.94
train mean loss: 1099.92
epoch train time: 0:00:03.908174
elapsed time: 0:00:35.009203
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 16:41:30.750028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1035.27
 ---- batch: 020 ----
mean loss: 1093.43
train mean loss: 1070.26
epoch train time: 0:00:03.893851
elapsed time: 0:00:38.904375
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 16:41:34.645168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1050.41
 ---- batch: 020 ----
mean loss: 1066.89
train mean loss: 1062.69
epoch train time: 0:00:03.900493
elapsed time: 0:00:42.806090
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 16:41:38.546891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1064.11
 ---- batch: 020 ----
mean loss: 1081.64
train mean loss: 1063.97
epoch train time: 0:00:03.893190
elapsed time: 0:00:46.700465
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 16:41:42.441274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1056.89
 ---- batch: 020 ----
mean loss: 1050.68
train mean loss: 1053.51
epoch train time: 0:00:03.885408
elapsed time: 0:00:50.587058
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 16:41:46.327920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1061.03
 ---- batch: 020 ----
mean loss: 1053.35
train mean loss: 1058.53
epoch train time: 0:00:03.902595
elapsed time: 0:00:54.490951
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 16:41:50.231738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1045.05
 ---- batch: 020 ----
mean loss: 1030.68
train mean loss: 1039.85
epoch train time: 0:00:03.900686
elapsed time: 0:00:58.392815
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 16:41:54.133643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1030.75
 ---- batch: 020 ----
mean loss: 1034.57
train mean loss: 1027.65
epoch train time: 0:00:03.920052
elapsed time: 0:01:02.314211
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 16:41:58.054997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1054.36
 ---- batch: 020 ----
mean loss: 1011.66
train mean loss: 1033.47
epoch train time: 0:00:03.923163
elapsed time: 0:01:06.238526
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 16:42:01.979308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1030.94
 ---- batch: 020 ----
mean loss: 991.23
train mean loss: 1016.44
epoch train time: 0:00:03.895936
elapsed time: 0:01:10.135695
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 16:42:05.876547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1033.47
 ---- batch: 020 ----
mean loss: 1003.34
train mean loss: 1010.78
epoch train time: 0:00:03.899482
elapsed time: 0:01:14.036598
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 16:42:09.777404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1011.95
 ---- batch: 020 ----
mean loss: 1003.55
train mean loss: 1016.54
epoch train time: 0:00:03.894954
elapsed time: 0:01:17.932736
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 16:42:13.673538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1010.74
 ---- batch: 020 ----
mean loss: 996.27
train mean loss: 998.34
epoch train time: 0:00:03.901862
elapsed time: 0:01:21.835818
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 16:42:17.576636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 999.73
 ---- batch: 020 ----
mean loss: 998.80
train mean loss: 985.23
epoch train time: 0:00:03.879516
elapsed time: 0:01:25.716619
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 16:42:21.457493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1018.21
 ---- batch: 020 ----
mean loss: 994.05
train mean loss: 1003.83
epoch train time: 0:00:03.894902
elapsed time: 0:01:29.612990
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 16:42:25.353802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 975.97
 ---- batch: 020 ----
mean loss: 997.63
train mean loss: 996.45
epoch train time: 0:00:03.918465
elapsed time: 0:01:33.532697
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 16:42:29.273513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 983.21
 ---- batch: 020 ----
mean loss: 987.56
train mean loss: 987.18
epoch train time: 0:00:03.931628
elapsed time: 0:01:37.465617
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 16:42:33.206423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 983.45
 ---- batch: 020 ----
mean loss: 965.26
train mean loss: 972.85
epoch train time: 0:00:03.904925
elapsed time: 0:01:41.371779
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 16:42:37.112581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.71
 ---- batch: 020 ----
mean loss: 947.62
train mean loss: 973.10
epoch train time: 0:00:03.902244
elapsed time: 0:01:45.275360
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 16:42:41.016178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 967.51
 ---- batch: 020 ----
mean loss: 995.41
train mean loss: 977.74
epoch train time: 0:00:03.895673
elapsed time: 0:01:49.172253
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 16:42:44.913062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 983.99
 ---- batch: 020 ----
mean loss: 935.52
train mean loss: 959.30
epoch train time: 0:00:03.892232
elapsed time: 0:01:53.065815
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 16:42:48.806612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.90
 ---- batch: 020 ----
mean loss: 964.87
train mean loss: 965.99
epoch train time: 0:00:03.915096
elapsed time: 0:01:56.982106
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 16:42:52.722910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 958.53
 ---- batch: 020 ----
mean loss: 959.33
train mean loss: 955.30
epoch train time: 0:00:03.917067
elapsed time: 0:02:00.900389
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 16:42:56.641187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 965.04
 ---- batch: 020 ----
mean loss: 937.66
train mean loss: 952.32
epoch train time: 0:00:03.909137
elapsed time: 0:02:04.810763
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 16:43:00.551594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 969.38
 ---- batch: 020 ----
mean loss: 937.98
train mean loss: 946.02
epoch train time: 0:00:03.898882
elapsed time: 0:02:08.710834
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 16:43:04.451655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 938.74
 ---- batch: 020 ----
mean loss: 943.94
train mean loss: 948.86
epoch train time: 0:00:03.925146
elapsed time: 0:02:12.637253
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 16:43:08.378064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 961.11
 ---- batch: 020 ----
mean loss: 924.80
train mean loss: 949.26
epoch train time: 0:00:03.912575
elapsed time: 0:02:16.551052
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 16:43:12.291838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.64
 ---- batch: 020 ----
mean loss: 941.75
train mean loss: 934.43
epoch train time: 0:00:03.899294
elapsed time: 0:02:20.451490
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 16:43:16.192301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.62
 ---- batch: 020 ----
mean loss: 941.76
train mean loss: 939.61
epoch train time: 0:00:03.911350
elapsed time: 0:02:24.364012
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 16:43:20.104869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.84
 ---- batch: 020 ----
mean loss: 946.61
train mean loss: 930.53
epoch train time: 0:00:03.904953
elapsed time: 0:02:28.270177
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 16:43:24.010958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.11
 ---- batch: 020 ----
mean loss: 927.09
train mean loss: 933.43
epoch train time: 0:00:03.900103
elapsed time: 0:02:32.171593
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 16:43:27.912417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.64
 ---- batch: 020 ----
mean loss: 917.46
train mean loss: 921.84
epoch train time: 0:00:03.892941
elapsed time: 0:02:36.065854
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 16:43:31.806664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.03
 ---- batch: 020 ----
mean loss: 918.08
train mean loss: 916.27
epoch train time: 0:00:03.893505
elapsed time: 0:02:39.960617
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 16:43:35.701458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.53
 ---- batch: 020 ----
mean loss: 919.20
train mean loss: 917.05
epoch train time: 0:00:03.906139
elapsed time: 0:02:43.867999
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 16:43:39.608822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.89
 ---- batch: 020 ----
mean loss: 923.99
train mean loss: 912.70
epoch train time: 0:00:03.887267
elapsed time: 0:02:47.756581
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 16:43:43.497373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.37
 ---- batch: 020 ----
mean loss: 905.22
train mean loss: 897.08
epoch train time: 0:00:03.889610
elapsed time: 0:02:51.647475
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 16:43:47.388264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.12
 ---- batch: 020 ----
mean loss: 901.88
train mean loss: 904.19
epoch train time: 0:00:03.886791
elapsed time: 0:02:55.535502
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 16:43:51.276312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.34
 ---- batch: 020 ----
mean loss: 912.13
train mean loss: 903.52
epoch train time: 0:00:03.894754
elapsed time: 0:02:59.431470
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 16:43:55.172283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.27
 ---- batch: 020 ----
mean loss: 919.99
train mean loss: 899.34
epoch train time: 0:00:03.922986
elapsed time: 0:03:03.356029
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 16:43:59.096826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.40
 ---- batch: 020 ----
mean loss: 872.71
train mean loss: 891.78
epoch train time: 0:00:03.903513
elapsed time: 0:03:07.260697
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 16:44:03.001503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.33
 ---- batch: 020 ----
mean loss: 872.96
train mean loss: 892.74
epoch train time: 0:00:03.904866
elapsed time: 0:03:11.166807
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 16:44:06.907624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.50
 ---- batch: 020 ----
mean loss: 879.88
train mean loss: 893.24
epoch train time: 0:00:03.895769
elapsed time: 0:03:15.063817
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 16:44:10.804640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.47
 ---- batch: 020 ----
mean loss: 901.02
train mean loss: 880.84
epoch train time: 0:00:03.885044
elapsed time: 0:03:18.950113
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 16:44:14.690925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.24
 ---- batch: 020 ----
mean loss: 882.09
train mean loss: 875.08
epoch train time: 0:00:03.904020
elapsed time: 0:03:22.855300
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 16:44:18.596106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.25
 ---- batch: 020 ----
mean loss: 876.35
train mean loss: 870.66
epoch train time: 0:00:03.892597
elapsed time: 0:03:26.749183
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 16:44:22.489975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.13
 ---- batch: 020 ----
mean loss: 864.57
train mean loss: 876.89
epoch train time: 0:00:03.899206
elapsed time: 0:03:30.649683
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 16:44:26.390487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.40
 ---- batch: 020 ----
mean loss: 867.17
train mean loss: 875.79
epoch train time: 0:00:03.889772
elapsed time: 0:03:34.540676
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 16:44:30.281501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.54
 ---- batch: 020 ----
mean loss: 866.68
train mean loss: 862.02
epoch train time: 0:00:03.902868
elapsed time: 0:03:38.444692
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 16:44:34.185492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.90
 ---- batch: 020 ----
mean loss: 864.35
train mean loss: 860.12
epoch train time: 0:00:03.901068
elapsed time: 0:03:42.346901
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 16:44:38.087712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.11
 ---- batch: 020 ----
mean loss: 867.13
train mean loss: 853.50
epoch train time: 0:00:03.904265
elapsed time: 0:03:46.252466
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 16:44:41.993347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.48
 ---- batch: 020 ----
mean loss: 853.11
train mean loss: 840.29
epoch train time: 0:00:03.902196
elapsed time: 0:03:50.155995
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 16:44:45.896806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.90
 ---- batch: 020 ----
mean loss: 848.33
train mean loss: 836.42
epoch train time: 0:00:03.905073
elapsed time: 0:03:54.062312
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 16:44:49.803163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.51
 ---- batch: 020 ----
mean loss: 801.03
train mean loss: 816.63
epoch train time: 0:00:03.897717
elapsed time: 0:03:57.961490
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 16:44:53.702325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 813.00
 ---- batch: 020 ----
mean loss: 804.43
train mean loss: 810.88
epoch train time: 0:00:03.907005
elapsed time: 0:04:01.869792
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 16:44:57.610624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 801.48
 ---- batch: 020 ----
mean loss: 800.55
train mean loss: 798.43
epoch train time: 0:00:03.898956
elapsed time: 0:04:05.769958
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 16:45:01.510749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 796.45
 ---- batch: 020 ----
mean loss: 782.23
train mean loss: 777.58
epoch train time: 0:00:03.915866
elapsed time: 0:04:09.687100
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 16:45:05.427909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 746.86
 ---- batch: 020 ----
mean loss: 757.63
train mean loss: 758.57
epoch train time: 0:00:03.944406
elapsed time: 0:04:13.632808
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 16:45:09.373663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 746.46
 ---- batch: 020 ----
mean loss: 733.82
train mean loss: 734.35
epoch train time: 0:00:03.926756
elapsed time: 0:04:17.560865
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 16:45:13.301678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 745.11
 ---- batch: 020 ----
mean loss: 720.50
train mean loss: 732.57
epoch train time: 0:00:03.954500
elapsed time: 0:04:21.517214
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 16:45:17.258183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 731.79
 ---- batch: 020 ----
mean loss: 707.22
train mean loss: 714.94
epoch train time: 0:00:03.929743
elapsed time: 0:04:25.448308
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 16:45:21.189117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.10
 ---- batch: 020 ----
mean loss: 685.20
train mean loss: 698.56
epoch train time: 0:00:03.900385
elapsed time: 0:04:29.349913
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 16:45:25.090708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 692.09
 ---- batch: 020 ----
mean loss: 693.51
train mean loss: 687.18
epoch train time: 0:00:03.939018
elapsed time: 0:04:33.290223
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 16:45:29.031022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 672.45
 ---- batch: 020 ----
mean loss: 695.15
train mean loss: 688.81
epoch train time: 0:00:03.939214
elapsed time: 0:04:37.230756
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 16:45:32.971560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 657.09
 ---- batch: 020 ----
mean loss: 667.91
train mean loss: 666.66
epoch train time: 0:00:03.909716
elapsed time: 0:04:41.141779
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 16:45:36.882617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 680.36
 ---- batch: 020 ----
mean loss: 659.53
train mean loss: 666.29
epoch train time: 0:00:03.941319
elapsed time: 0:04:45.084546
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 16:45:40.825379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.68
 ---- batch: 020 ----
mean loss: 651.98
train mean loss: 662.66
epoch train time: 0:00:03.937458
elapsed time: 0:04:49.023578
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 16:45:44.764400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 651.26
 ---- batch: 020 ----
mean loss: 662.59
train mean loss: 649.49
epoch train time: 0:00:03.883999
elapsed time: 0:04:52.908922
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 16:45:48.649743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 636.40
 ---- batch: 020 ----
mean loss: 648.66
train mean loss: 639.01
epoch train time: 0:00:03.949548
elapsed time: 0:04:56.860016
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 16:45:52.600841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 622.42
 ---- batch: 020 ----
mean loss: 637.72
train mean loss: 632.06
epoch train time: 0:00:03.926943
elapsed time: 0:05:00.788211
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 16:45:56.529051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 637.62
 ---- batch: 020 ----
mean loss: 624.94
train mean loss: 623.23
epoch train time: 0:00:03.912191
elapsed time: 0:05:04.701656
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 16:46:00.442459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 633.45
 ---- batch: 020 ----
mean loss: 616.39
train mean loss: 620.14
epoch train time: 0:00:03.956511
elapsed time: 0:05:08.659462
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 16:46:04.400270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 606.00
 ---- batch: 020 ----
mean loss: 619.32
train mean loss: 611.94
epoch train time: 0:00:03.929882
elapsed time: 0:05:12.590548
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 16:46:08.331352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 603.54
 ---- batch: 020 ----
mean loss: 614.17
train mean loss: 609.77
epoch train time: 0:00:03.950463
elapsed time: 0:05:16.542220
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 16:46:12.283045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 602.01
 ---- batch: 020 ----
mean loss: 615.90
train mean loss: 599.43
epoch train time: 0:00:03.917365
elapsed time: 0:05:20.460821
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 16:46:16.201632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 603.31
 ---- batch: 020 ----
mean loss: 584.78
train mean loss: 584.89
epoch train time: 0:00:03.907985
elapsed time: 0:05:24.370066
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 16:46:20.110862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.47
 ---- batch: 020 ----
mean loss: 596.59
train mean loss: 585.41
epoch train time: 0:00:03.909984
elapsed time: 0:05:28.281273
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 16:46:24.022063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.38
 ---- batch: 020 ----
mean loss: 567.99
train mean loss: 575.65
epoch train time: 0:00:03.943427
elapsed time: 0:05:32.225900
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 16:46:27.966710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 573.44
 ---- batch: 020 ----
mean loss: 568.20
train mean loss: 567.64
epoch train time: 0:00:03.907124
elapsed time: 0:05:36.134278
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 16:46:31.875111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 574.79
 ---- batch: 020 ----
mean loss: 555.53
train mean loss: 560.85
epoch train time: 0:00:03.907574
elapsed time: 0:05:40.043401
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 16:46:35.784247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.60
 ---- batch: 020 ----
mean loss: 559.10
train mean loss: 558.37
epoch train time: 0:00:03.925095
elapsed time: 0:05:43.969777
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 16:46:39.710591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 544.58
 ---- batch: 020 ----
mean loss: 545.46
train mean loss: 548.06
epoch train time: 0:00:03.922971
elapsed time: 0:05:47.893918
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 16:46:43.634736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.56
 ---- batch: 020 ----
mean loss: 543.70
train mean loss: 543.63
epoch train time: 0:00:03.918270
elapsed time: 0:05:51.813386
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 16:46:47.554186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 552.08
 ---- batch: 020 ----
mean loss: 527.78
train mean loss: 540.63
epoch train time: 0:00:03.929135
elapsed time: 0:05:55.744018
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 16:46:51.484818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.07
 ---- batch: 020 ----
mean loss: 534.59
train mean loss: 537.56
epoch train time: 0:00:03.921536
elapsed time: 0:05:59.666834
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 16:46:55.407622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 537.24
 ---- batch: 020 ----
mean loss: 535.96
train mean loss: 532.85
epoch train time: 0:00:03.920245
elapsed time: 0:06:03.588324
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 16:46:59.329105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.55
 ---- batch: 020 ----
mean loss: 520.99
train mean loss: 522.07
epoch train time: 0:00:03.916234
elapsed time: 0:06:07.505749
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 16:47:03.246570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.35
 ---- batch: 020 ----
mean loss: 520.10
train mean loss: 519.22
epoch train time: 0:00:03.911379
elapsed time: 0:06:11.418435
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 16:47:07.159239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.07
 ---- batch: 020 ----
mean loss: 511.80
train mean loss: 510.18
epoch train time: 0:00:03.933149
elapsed time: 0:06:15.352884
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 16:47:11.093718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.39
 ---- batch: 020 ----
mean loss: 509.93
train mean loss: 515.42
epoch train time: 0:00:03.910751
elapsed time: 0:06:19.264821
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 16:47:15.005670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.29
 ---- batch: 020 ----
mean loss: 504.33
train mean loss: 507.29
epoch train time: 0:00:03.915983
elapsed time: 0:06:23.182047
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 16:47:18.922846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.89
 ---- batch: 020 ----
mean loss: 493.47
train mean loss: 493.26
epoch train time: 0:00:03.908744
elapsed time: 0:06:27.092050
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 16:47:22.832883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.02
 ---- batch: 020 ----
mean loss: 490.69
train mean loss: 488.38
epoch train time: 0:00:03.916779
elapsed time: 0:06:31.010058
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 16:47:26.750853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.35
 ---- batch: 020 ----
mean loss: 508.44
train mean loss: 493.62
epoch train time: 0:00:03.927631
elapsed time: 0:06:34.938868
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 16:47:30.679652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.91
 ---- batch: 020 ----
mean loss: 480.95
train mean loss: 483.66
epoch train time: 0:00:03.929756
elapsed time: 0:06:38.869888
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 16:47:34.610732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.54
 ---- batch: 020 ----
mean loss: 471.82
train mean loss: 478.07
epoch train time: 0:00:03.933641
elapsed time: 0:06:42.805166
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 16:47:38.545958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.99
 ---- batch: 020 ----
mean loss: 473.49
train mean loss: 476.23
epoch train time: 0:00:03.965540
elapsed time: 0:06:46.771928
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 16:47:42.512731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.66
 ---- batch: 020 ----
mean loss: 459.23
train mean loss: 467.72
epoch train time: 0:00:03.958159
elapsed time: 0:06:50.731615
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 16:47:46.472461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.04
 ---- batch: 020 ----
mean loss: 466.72
train mean loss: 456.42
epoch train time: 0:00:03.927862
elapsed time: 0:06:54.660750
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 16:47:50.401560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.52
 ---- batch: 020 ----
mean loss: 458.29
train mean loss: 462.28
epoch train time: 0:00:03.955558
elapsed time: 0:06:58.617688
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 16:47:54.358549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.79
 ---- batch: 020 ----
mean loss: 444.70
train mean loss: 452.82
epoch train time: 0:00:03.937693
elapsed time: 0:07:02.556747
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 16:47:58.297621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.65
 ---- batch: 020 ----
mean loss: 456.39
train mean loss: 451.86
epoch train time: 0:00:03.933610
elapsed time: 0:07:06.491834
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 16:48:02.232527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.59
 ---- batch: 020 ----
mean loss: 450.55
train mean loss: 443.81
epoch train time: 0:00:03.929610
elapsed time: 0:07:10.422623
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 16:48:06.163426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.61
 ---- batch: 020 ----
mean loss: 441.25
train mean loss: 439.36
epoch train time: 0:00:03.936671
elapsed time: 0:07:14.360632
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 16:48:10.101461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.35
 ---- batch: 020 ----
mean loss: 440.13
train mean loss: 437.70
epoch train time: 0:00:03.941420
elapsed time: 0:07:18.303217
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 16:48:14.044028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.43
 ---- batch: 020 ----
mean loss: 438.56
train mean loss: 435.72
epoch train time: 0:00:03.939533
elapsed time: 0:07:22.243982
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 16:48:17.984781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.81
 ---- batch: 020 ----
mean loss: 428.79
train mean loss: 431.39
epoch train time: 0:00:03.939517
elapsed time: 0:07:26.184749
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 16:48:21.925558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.59
 ---- batch: 020 ----
mean loss: 421.38
train mean loss: 421.41
epoch train time: 0:00:03.930402
elapsed time: 0:07:30.116474
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 16:48:25.857303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.37
 ---- batch: 020 ----
mean loss: 415.72
train mean loss: 419.07
epoch train time: 0:00:03.942035
elapsed time: 0:07:34.059769
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 16:48:29.800569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.38
 ---- batch: 020 ----
mean loss: 422.12
train mean loss: 421.18
epoch train time: 0:00:03.925461
elapsed time: 0:07:37.986533
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 16:48:33.727380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.42
 ---- batch: 020 ----
mean loss: 413.67
train mean loss: 411.10
epoch train time: 0:00:03.937946
elapsed time: 0:07:41.925736
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 16:48:37.666555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.73
 ---- batch: 020 ----
mean loss: 415.47
train mean loss: 412.09
epoch train time: 0:00:03.942019
elapsed time: 0:07:45.869195
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 16:48:41.610005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.81
 ---- batch: 020 ----
mean loss: 409.87
train mean loss: 412.07
epoch train time: 0:00:03.937199
elapsed time: 0:07:49.807690
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 16:48:45.548509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.71
 ---- batch: 020 ----
mean loss: 413.61
train mean loss: 408.79
epoch train time: 0:00:03.926112
elapsed time: 0:07:53.735022
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 16:48:49.475856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.23
 ---- batch: 020 ----
mean loss: 409.04
train mean loss: 408.97
epoch train time: 0:00:03.941425
elapsed time: 0:07:57.677665
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 16:48:53.418470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.95
 ---- batch: 020 ----
mean loss: 403.65
train mean loss: 401.21
epoch train time: 0:00:03.942647
elapsed time: 0:08:01.621552
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 16:48:57.362343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.67
 ---- batch: 020 ----
mean loss: 401.52
train mean loss: 394.45
epoch train time: 0:00:03.947666
elapsed time: 0:08:05.570490
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 16:49:01.311304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.44
 ---- batch: 020 ----
mean loss: 397.87
train mean loss: 394.23
epoch train time: 0:00:03.933944
elapsed time: 0:08:09.505745
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 16:49:05.246549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.84
 ---- batch: 020 ----
mean loss: 403.72
train mean loss: 390.94
epoch train time: 0:00:03.931131
elapsed time: 0:08:13.438045
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 16:49:09.178817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.16
 ---- batch: 020 ----
mean loss: 400.09
train mean loss: 387.42
epoch train time: 0:00:03.955584
elapsed time: 0:08:17.395088
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 16:49:13.135908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.43
 ---- batch: 020 ----
mean loss: 380.60
train mean loss: 382.78
epoch train time: 0:00:03.955405
elapsed time: 0:08:21.351709
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 16:49:17.092546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.57
 ---- batch: 020 ----
mean loss: 392.05
train mean loss: 383.83
epoch train time: 0:00:03.935294
elapsed time: 0:08:25.288636
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 16:49:21.029265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.08
 ---- batch: 020 ----
mean loss: 382.42
train mean loss: 381.87
epoch train time: 0:00:03.929713
elapsed time: 0:08:29.219438
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 16:49:24.960232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.36
 ---- batch: 020 ----
mean loss: 372.10
train mean loss: 379.75
epoch train time: 0:00:03.936007
elapsed time: 0:08:33.156748
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 16:49:28.897589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.79
 ---- batch: 020 ----
mean loss: 375.59
train mean loss: 373.46
epoch train time: 0:00:03.924505
elapsed time: 0:08:37.082665
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 16:49:32.823299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.22
 ---- batch: 020 ----
mean loss: 366.79
train mean loss: 371.03
epoch train time: 0:00:03.917954
elapsed time: 0:08:41.001696
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 16:49:36.742500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.55
 ---- batch: 020 ----
mean loss: 379.79
train mean loss: 374.66
epoch train time: 0:00:03.933792
elapsed time: 0:08:44.936749
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 16:49:40.677541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.16
 ---- batch: 020 ----
mean loss: 368.18
train mean loss: 370.52
epoch train time: 0:00:03.941275
elapsed time: 0:08:48.879405
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 16:49:44.620208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.55
 ---- batch: 020 ----
mean loss: 368.15
train mean loss: 372.56
epoch train time: 0:00:03.929873
elapsed time: 0:08:52.810465
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 16:49:48.551269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.53
 ---- batch: 020 ----
mean loss: 358.04
train mean loss: 366.39
epoch train time: 0:00:03.945219
elapsed time: 0:08:56.756942
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 16:49:52.497754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.15
 ---- batch: 020 ----
mean loss: 364.67
train mean loss: 372.77
epoch train time: 0:00:03.939614
elapsed time: 0:09:00.697804
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 16:49:56.438608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.41
 ---- batch: 020 ----
mean loss: 363.87
train mean loss: 365.90
epoch train time: 0:00:03.946581
elapsed time: 0:09:04.645886
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 16:50:00.386687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.96
 ---- batch: 020 ----
mean loss: 364.93
train mean loss: 368.30
epoch train time: 0:00:03.931877
elapsed time: 0:09:08.578949
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 16:50:04.319779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.98
 ---- batch: 020 ----
mean loss: 357.51
train mean loss: 356.13
epoch train time: 0:00:03.928822
elapsed time: 0:09:12.508993
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 16:50:08.249815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.85
 ---- batch: 020 ----
mean loss: 348.68
train mean loss: 359.23
epoch train time: 0:00:03.941997
elapsed time: 0:09:16.452277
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 16:50:12.193123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.77
 ---- batch: 020 ----
mean loss: 369.76
train mean loss: 357.08
epoch train time: 0:00:03.953199
elapsed time: 0:09:20.406772
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 16:50:16.147599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.25
 ---- batch: 020 ----
mean loss: 354.77
train mean loss: 355.61
epoch train time: 0:00:03.935703
elapsed time: 0:09:24.343847
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 16:50:20.084630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.60
 ---- batch: 020 ----
mean loss: 351.91
train mean loss: 349.76
epoch train time: 0:00:03.940689
elapsed time: 0:09:28.285736
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 16:50:24.026536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.42
 ---- batch: 020 ----
mean loss: 349.71
train mean loss: 349.91
epoch train time: 0:00:03.934346
elapsed time: 0:09:32.221287
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 16:50:27.962080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.29
 ---- batch: 020 ----
mean loss: 345.17
train mean loss: 350.77
epoch train time: 0:00:03.936567
elapsed time: 0:09:36.159099
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 16:50:31.899880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.78
 ---- batch: 020 ----
mean loss: 343.74
train mean loss: 344.46
epoch train time: 0:00:03.920020
elapsed time: 0:09:40.080275
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 16:50:35.821080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.83
 ---- batch: 020 ----
mean loss: 354.51
train mean loss: 344.91
epoch train time: 0:00:03.938889
elapsed time: 0:09:44.020389
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 16:50:39.761172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.83
 ---- batch: 020 ----
mean loss: 347.79
train mean loss: 342.58
epoch train time: 0:00:03.957119
elapsed time: 0:09:47.978729
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 16:50:43.719537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.65
 ---- batch: 020 ----
mean loss: 341.99
train mean loss: 342.41
epoch train time: 0:00:03.917787
elapsed time: 0:09:51.898075
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 16:50:47.638700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.89
 ---- batch: 020 ----
mean loss: 335.41
train mean loss: 340.56
epoch train time: 0:00:03.944441
elapsed time: 0:09:55.843507
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 16:50:51.584348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.86
 ---- batch: 020 ----
mean loss: 331.48
train mean loss: 342.98
epoch train time: 0:00:03.931400
elapsed time: 0:09:59.776141
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 16:50:55.516961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.71
 ---- batch: 020 ----
mean loss: 341.54
train mean loss: 336.03
epoch train time: 0:00:03.921380
elapsed time: 0:10:03.698681
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 16:50:59.439474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.06
 ---- batch: 020 ----
mean loss: 343.36
train mean loss: 339.18
epoch train time: 0:00:03.937626
elapsed time: 0:10:07.637744
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 16:51:03.378573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.62
 ---- batch: 020 ----
mean loss: 334.55
train mean loss: 341.07
epoch train time: 0:00:03.931264
elapsed time: 0:10:11.570512
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 16:51:07.311323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.99
 ---- batch: 020 ----
mean loss: 333.61
train mean loss: 332.98
epoch train time: 0:00:03.926596
elapsed time: 0:10:15.498277
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 16:51:11.239077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.70
 ---- batch: 020 ----
mean loss: 333.51
train mean loss: 335.00
epoch train time: 0:00:03.944873
elapsed time: 0:10:19.444373
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 16:51:15.185193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.96
 ---- batch: 020 ----
mean loss: 342.70
train mean loss: 335.24
epoch train time: 0:00:03.951260
elapsed time: 0:10:23.396968
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 16:51:19.137798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.28
 ---- batch: 020 ----
mean loss: 335.07
train mean loss: 335.13
epoch train time: 0:00:03.937635
elapsed time: 0:10:27.335796
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 16:51:23.076639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.82
 ---- batch: 020 ----
mean loss: 337.60
train mean loss: 329.46
epoch train time: 0:00:03.942599
elapsed time: 0:10:31.279609
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 16:51:27.020442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.11
 ---- batch: 020 ----
mean loss: 325.57
train mean loss: 328.14
epoch train time: 0:00:03.941401
elapsed time: 0:10:35.222403
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 16:51:30.963226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.38
 ---- batch: 020 ----
mean loss: 324.15
train mean loss: 327.65
epoch train time: 0:00:03.934166
elapsed time: 0:10:39.157970
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 16:51:34.898857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.15
 ---- batch: 020 ----
mean loss: 323.49
train mean loss: 324.95
epoch train time: 0:00:03.941810
elapsed time: 0:10:43.101129
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 16:51:38.841951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.74
 ---- batch: 020 ----
mean loss: 328.36
train mean loss: 325.19
epoch train time: 0:00:03.942722
elapsed time: 0:10:47.045209
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 16:51:42.786051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.80
 ---- batch: 020 ----
mean loss: 325.93
train mean loss: 325.55
epoch train time: 0:00:03.935636
elapsed time: 0:10:50.982130
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 16:51:46.723040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.21
 ---- batch: 020 ----
mean loss: 313.67
train mean loss: 319.83
epoch train time: 0:00:03.932997
elapsed time: 0:10:54.916546
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 16:51:50.657378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.16
 ---- batch: 020 ----
mean loss: 318.13
train mean loss: 323.05
epoch train time: 0:00:03.952504
elapsed time: 0:10:58.870360
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 16:51:54.611168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.91
 ---- batch: 020 ----
mean loss: 311.28
train mean loss: 321.25
epoch train time: 0:00:03.942915
elapsed time: 0:11:02.814424
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 16:51:58.555240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.13
 ---- batch: 020 ----
mean loss: 327.28
train mean loss: 321.55
epoch train time: 0:00:03.917484
elapsed time: 0:11:06.733269
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 16:52:02.474175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.00
 ---- batch: 020 ----
mean loss: 320.26
train mean loss: 326.29
epoch train time: 0:00:03.921239
elapsed time: 0:11:10.655870
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 16:52:06.396678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.48
 ---- batch: 020 ----
mean loss: 314.62
train mean loss: 318.17
epoch train time: 0:00:03.908135
elapsed time: 0:11:14.565209
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 16:52:10.305990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.60
 ---- batch: 020 ----
mean loss: 310.06
train mean loss: 317.91
epoch train time: 0:00:03.938645
elapsed time: 0:11:18.505071
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 16:52:14.245909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.26
 ---- batch: 020 ----
mean loss: 321.68
train mean loss: 320.45
epoch train time: 0:00:03.930832
elapsed time: 0:11:22.437127
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 16:52:18.177926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.16
 ---- batch: 020 ----
mean loss: 316.87
train mean loss: 310.07
epoch train time: 0:00:03.935105
elapsed time: 0:11:26.373750
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 16:52:22.114573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.06
 ---- batch: 020 ----
mean loss: 313.37
train mean loss: 311.47
epoch train time: 0:00:03.941692
elapsed time: 0:11:30.316866
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 16:52:26.057483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.50
 ---- batch: 020 ----
mean loss: 322.28
train mean loss: 311.17
epoch train time: 0:00:03.921287
elapsed time: 0:11:34.239250
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 16:52:29.980064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.43
 ---- batch: 020 ----
mean loss: 312.92
train mean loss: 310.73
epoch train time: 0:00:03.909370
elapsed time: 0:11:38.149934
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 16:52:33.890724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.62
 ---- batch: 020 ----
mean loss: 311.35
train mean loss: 311.32
epoch train time: 0:00:03.910379
elapsed time: 0:11:42.061624
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 16:52:37.802510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.85
 ---- batch: 020 ----
mean loss: 316.47
train mean loss: 312.62
epoch train time: 0:00:03.903187
elapsed time: 0:11:45.966120
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 16:52:41.706936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.23
 ---- batch: 020 ----
mean loss: 309.79
train mean loss: 308.68
epoch train time: 0:00:03.925271
elapsed time: 0:11:49.892655
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 16:52:45.633483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.32
 ---- batch: 020 ----
mean loss: 297.38
train mean loss: 310.45
epoch train time: 0:00:03.952949
elapsed time: 0:11:53.847221
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 16:52:49.588118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.19
 ---- batch: 020 ----
mean loss: 297.88
train mean loss: 305.12
epoch train time: 0:00:03.954437
elapsed time: 0:11:57.803350
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 16:52:53.544156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.81
 ---- batch: 020 ----
mean loss: 314.83
train mean loss: 308.31
epoch train time: 0:00:03.982908
elapsed time: 0:12:01.787604
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 16:52:57.528463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.89
 ---- batch: 020 ----
mean loss: 309.11
train mean loss: 302.44
epoch train time: 0:00:03.928014
elapsed time: 0:12:05.716984
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 16:53:01.457803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.99
 ---- batch: 020 ----
mean loss: 299.02
train mean loss: 305.28
epoch train time: 0:00:03.938736
elapsed time: 0:12:09.657058
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 16:53:05.397856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.68
 ---- batch: 020 ----
mean loss: 304.59
train mean loss: 302.81
epoch train time: 0:00:03.952028
elapsed time: 0:12:13.610513
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 16:53:09.351306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.63
 ---- batch: 020 ----
mean loss: 306.82
train mean loss: 302.02
epoch train time: 0:00:03.949639
elapsed time: 0:12:17.561350
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 16:53:13.302147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.68
 ---- batch: 020 ----
mean loss: 294.50
train mean loss: 299.29
epoch train time: 0:00:03.948523
elapsed time: 0:12:21.511128
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 16:53:17.251947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.29
 ---- batch: 020 ----
mean loss: 304.39
train mean loss: 303.99
epoch train time: 0:00:03.962287
elapsed time: 0:12:25.474637
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 16:53:21.215432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.88
 ---- batch: 020 ----
mean loss: 312.20
train mean loss: 305.33
epoch train time: 0:00:03.939769
elapsed time: 0:12:29.415682
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 16:53:25.156478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.35
 ---- batch: 020 ----
mean loss: 304.45
train mean loss: 302.67
epoch train time: 0:00:03.945818
elapsed time: 0:12:33.362709
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 16:53:29.103563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.61
 ---- batch: 020 ----
mean loss: 295.52
train mean loss: 299.04
epoch train time: 0:00:03.940532
elapsed time: 0:12:37.304451
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 16:53:33.045248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.64
 ---- batch: 020 ----
mean loss: 293.65
train mean loss: 298.12
epoch train time: 0:00:03.944673
elapsed time: 0:12:41.250334
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 16:53:36.991128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.80
 ---- batch: 020 ----
mean loss: 296.34
train mean loss: 294.63
epoch train time: 0:00:03.942697
elapsed time: 0:12:45.194267
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 16:53:40.935063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.31
 ---- batch: 020 ----
mean loss: 292.80
train mean loss: 297.73
epoch train time: 0:00:03.924449
elapsed time: 0:12:49.120038
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 16:53:44.860841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.99
 ---- batch: 020 ----
mean loss: 287.55
train mean loss: 291.04
epoch train time: 0:00:03.926023
elapsed time: 0:12:53.047325
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 16:53:48.788162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.78
 ---- batch: 020 ----
mean loss: 291.38
train mean loss: 294.96
epoch train time: 0:00:03.974768
elapsed time: 0:12:57.023314
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 16:53:52.764135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.12
 ---- batch: 020 ----
mean loss: 299.28
train mean loss: 294.63
epoch train time: 0:00:03.994193
elapsed time: 0:13:01.018759
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 16:53:56.759565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.43
 ---- batch: 020 ----
mean loss: 290.31
train mean loss: 294.39
epoch train time: 0:00:03.957471
elapsed time: 0:13:04.977865
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 16:54:00.718689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.20
 ---- batch: 020 ----
mean loss: 294.13
train mean loss: 292.87
epoch train time: 0:00:03.975756
elapsed time: 0:13:08.954864
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 16:54:04.695720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.27
 ---- batch: 020 ----
mean loss: 288.01
train mean loss: 292.24
epoch train time: 0:00:03.956647
elapsed time: 0:13:12.912816
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 16:54:08.653696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.81
 ---- batch: 020 ----
mean loss: 287.92
train mean loss: 293.59
epoch train time: 0:00:03.945506
elapsed time: 0:13:16.859582
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 16:54:12.600428
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.04
 ---- batch: 020 ----
mean loss: 296.27
train mean loss: 290.26
epoch train time: 0:00:03.945529
elapsed time: 0:13:20.806786
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 16:54:16.547438
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.18
 ---- batch: 020 ----
mean loss: 290.49
train mean loss: 285.64
epoch train time: 0:00:03.972226
elapsed time: 0:13:24.780110
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 16:54:20.520931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.47
 ---- batch: 020 ----
mean loss: 302.08
train mean loss: 289.46
epoch train time: 0:00:03.946088
elapsed time: 0:13:28.727594
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 16:54:24.468391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.57
 ---- batch: 020 ----
mean loss: 280.62
train mean loss: 286.12
epoch train time: 0:00:03.969021
elapsed time: 0:13:32.697947
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 16:54:28.438753
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.82
 ---- batch: 020 ----
mean loss: 281.84
train mean loss: 286.61
epoch train time: 0:00:03.894060
elapsed time: 0:13:36.593286
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 16:54:32.334093
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.15
 ---- batch: 020 ----
mean loss: 295.43
train mean loss: 288.67
epoch train time: 0:00:03.944629
elapsed time: 0:13:40.539085
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 16:54:36.279895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.62
 ---- batch: 020 ----
mean loss: 291.19
train mean loss: 291.19
epoch train time: 0:00:03.952757
elapsed time: 0:13:44.493108
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 16:54:40.233928
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.06
 ---- batch: 020 ----
mean loss: 298.75
train mean loss: 291.57
epoch train time: 0:00:03.951117
elapsed time: 0:13:48.445533
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 16:54:44.186331
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.26
 ---- batch: 020 ----
mean loss: 282.67
train mean loss: 289.13
epoch train time: 0:00:03.953188
elapsed time: 0:13:52.400004
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 16:54:48.140797
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.65
 ---- batch: 020 ----
mean loss: 290.16
train mean loss: 287.41
epoch train time: 0:00:03.934386
elapsed time: 0:13:56.335553
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 16:54:52.076357
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.46
 ---- batch: 020 ----
mean loss: 286.15
train mean loss: 287.34
epoch train time: 0:00:03.946696
elapsed time: 0:14:00.283443
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 16:54:56.024250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.48
 ---- batch: 020 ----
mean loss: 287.75
train mean loss: 290.00
epoch train time: 0:00:03.963512
elapsed time: 0:14:04.248238
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 16:54:59.989029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.70
 ---- batch: 020 ----
mean loss: 283.94
train mean loss: 287.71
epoch train time: 0:00:03.959094
elapsed time: 0:14:08.208616
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 16:55:03.949420
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.18
 ---- batch: 020 ----
mean loss: 284.05
train mean loss: 286.47
epoch train time: 0:00:03.959557
elapsed time: 0:14:12.169465
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 16:55:07.910326
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 293.59
 ---- batch: 020 ----
mean loss: 285.17
train mean loss: 288.56
epoch train time: 0:00:03.951321
elapsed time: 0:14:16.122064
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 16:55:11.862855
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 294.08
 ---- batch: 020 ----
mean loss: 284.31
train mean loss: 286.08
epoch train time: 0:00:03.953252
elapsed time: 0:14:20.076516
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 16:55:15.817330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 293.95
 ---- batch: 020 ----
mean loss: 290.03
train mean loss: 288.74
epoch train time: 0:00:03.973484
elapsed time: 0:14:24.051317
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 16:55:19.792173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.29
 ---- batch: 020 ----
mean loss: 281.69
train mean loss: 285.52
epoch train time: 0:00:03.951280
elapsed time: 0:14:28.003884
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 16:55:23.744677
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.85
 ---- batch: 020 ----
mean loss: 291.43
train mean loss: 284.82
epoch train time: 0:00:03.973123
elapsed time: 0:14:31.978279
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 16:55:27.719094
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.03
 ---- batch: 020 ----
mean loss: 288.62
train mean loss: 288.80
epoch train time: 0:00:03.960298
elapsed time: 0:14:35.939904
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 16:55:31.680704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.03
 ---- batch: 020 ----
mean loss: 294.77
train mean loss: 285.98
epoch train time: 0:00:03.971169
elapsed time: 0:14:39.912441
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 16:55:35.653241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.61
 ---- batch: 020 ----
mean loss: 279.86
train mean loss: 286.61
epoch train time: 0:00:03.972547
elapsed time: 0:14:43.886198
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 16:55:39.627002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.86
 ---- batch: 020 ----
mean loss: 285.78
train mean loss: 283.37
epoch train time: 0:00:03.983636
elapsed time: 0:14:47.871025
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 16:55:43.611811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.02
 ---- batch: 020 ----
mean loss: 290.35
train mean loss: 290.74
epoch train time: 0:00:03.983455
elapsed time: 0:14:51.855723
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 16:55:47.596562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.01
 ---- batch: 020 ----
mean loss: 289.50
train mean loss: 287.92
epoch train time: 0:00:03.995691
elapsed time: 0:14:55.852875
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 16:55:51.593787
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.55
 ---- batch: 020 ----
mean loss: 287.59
train mean loss: 286.30
epoch train time: 0:00:04.088582
elapsed time: 0:14:59.942808
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 16:55:55.683613
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.02
 ---- batch: 020 ----
mean loss: 282.75
train mean loss: 284.45
epoch train time: 0:00:04.014521
elapsed time: 0:15:03.958650
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 16:55:59.699453
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.54
 ---- batch: 020 ----
mean loss: 284.08
train mean loss: 284.13
epoch train time: 0:00:03.976280
elapsed time: 0:15:07.936299
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 16:56:03.677144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.82
 ---- batch: 020 ----
mean loss: 289.93
train mean loss: 287.45
epoch train time: 0:00:03.932858
elapsed time: 0:15:11.870405
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 16:56:07.611198
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.76
 ---- batch: 020 ----
mean loss: 282.48
train mean loss: 285.31
epoch train time: 0:00:03.938897
elapsed time: 0:15:15.810693
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 16:56:11.551479
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.07
 ---- batch: 020 ----
mean loss: 279.85
train mean loss: 286.92
epoch train time: 0:00:03.934630
elapsed time: 0:15:19.746449
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 16:56:15.487223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.34
 ---- batch: 020 ----
mean loss: 282.91
train mean loss: 288.00
epoch train time: 0:00:03.911757
elapsed time: 0:15:23.659364
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 16:56:19.400186
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.58
 ---- batch: 020 ----
mean loss: 287.99
train mean loss: 288.60
epoch train time: 0:00:03.932022
elapsed time: 0:15:27.592916
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 16:56:23.333567
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.00
 ---- batch: 020 ----
mean loss: 287.73
train mean loss: 285.12
epoch train time: 0:00:03.942462
elapsed time: 0:15:31.536519
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 16:56:27.277327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.53
 ---- batch: 020 ----
mean loss: 287.96
train mean loss: 281.69
epoch train time: 0:00:03.934336
elapsed time: 0:15:35.472125
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 16:56:31.212940
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.45
 ---- batch: 020 ----
mean loss: 284.52
train mean loss: 286.01
epoch train time: 0:00:03.921775
elapsed time: 0:15:39.395147
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 16:56:35.135964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.06
 ---- batch: 020 ----
mean loss: 288.06
train mean loss: 285.24
epoch train time: 0:00:03.920886
elapsed time: 0:15:43.317363
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 16:56:39.058161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.57
 ---- batch: 020 ----
mean loss: 286.51
train mean loss: 287.02
epoch train time: 0:00:03.920359
elapsed time: 0:15:47.238985
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 16:56:42.979771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.58
 ---- batch: 020 ----
mean loss: 288.31
train mean loss: 284.34
epoch train time: 0:00:03.932464
elapsed time: 0:15:51.172607
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 16:56:46.913402
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 294.68
 ---- batch: 020 ----
mean loss: 279.22
train mean loss: 285.13
epoch train time: 0:00:03.928526
elapsed time: 0:15:55.102362
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 16:56:50.843197
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.89
 ---- batch: 020 ----
mean loss: 290.17
train mean loss: 288.62
epoch train time: 0:00:03.933347
elapsed time: 0:15:59.037072
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 16:56:54.777895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.15
 ---- batch: 020 ----
mean loss: 277.70
train mean loss: 284.85
epoch train time: 0:00:03.922153
elapsed time: 0:16:02.960544
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 16:56:58.701366
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.37
 ---- batch: 020 ----
mean loss: 288.45
train mean loss: 286.21
epoch train time: 0:00:03.925489
elapsed time: 0:16:06.887312
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 16:57:02.628140
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.71
 ---- batch: 020 ----
mean loss: 285.19
train mean loss: 287.06
epoch train time: 0:00:03.937973
elapsed time: 0:16:10.826598
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 16:57:06.567401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.39
 ---- batch: 020 ----
mean loss: 285.73
train mean loss: 285.99
epoch train time: 0:00:03.932275
elapsed time: 0:16:14.760206
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 16:57:10.501005
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.12
 ---- batch: 020 ----
mean loss: 279.34
train mean loss: 285.31
epoch train time: 0:00:03.936507
elapsed time: 0:16:18.697975
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 16:57:14.438782
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.66
 ---- batch: 020 ----
mean loss: 280.02
train mean loss: 284.81
epoch train time: 0:00:03.939926
elapsed time: 0:16:22.639301
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 16:57:18.380137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.21
 ---- batch: 020 ----
mean loss: 277.00
train mean loss: 284.48
epoch train time: 0:00:03.929760
elapsed time: 0:16:26.570316
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 16:57:22.311182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.96
 ---- batch: 020 ----
mean loss: 293.20
train mean loss: 287.74
epoch train time: 0:00:03.957905
elapsed time: 0:16:30.538483
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_2/checkpoint.pth.tar
**** end time: 2019-09-26 16:57:26.279065 ****
