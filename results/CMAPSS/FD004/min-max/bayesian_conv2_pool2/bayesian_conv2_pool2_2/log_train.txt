Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 12583
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-26 20:48:46.512002 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 11, 11]           1,120
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
    BayesianConv2d-4            [-1, 14, 4, 11]             448
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
    BayesianLinear-8                    [-1, 1]             616
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,184
Trainable params: 2,184
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 20:48:46.523794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4785.54
 ---- batch: 020 ----
mean loss: 4639.02
 ---- batch: 030 ----
mean loss: 4479.70
 ---- batch: 040 ----
mean loss: 4279.46
 ---- batch: 050 ----
mean loss: 4088.50
 ---- batch: 060 ----
mean loss: 3867.87
 ---- batch: 070 ----
mean loss: 3695.56
 ---- batch: 080 ----
mean loss: 3490.81
 ---- batch: 090 ----
mean loss: 3294.94
 ---- batch: 100 ----
mean loss: 3126.34
 ---- batch: 110 ----
mean loss: 2958.77
train mean loss: 3853.88
epoch train time: 0:00:34.728187
elapsed time: 0:00:34.743186
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 20:49:21.255229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2691.85
 ---- batch: 020 ----
mean loss: 2508.93
 ---- batch: 030 ----
mean loss: 2350.28
 ---- batch: 040 ----
mean loss: 2201.99
 ---- batch: 050 ----
mean loss: 2077.80
 ---- batch: 060 ----
mean loss: 1936.19
 ---- batch: 070 ----
mean loss: 1796.85
 ---- batch: 080 ----
mean loss: 1709.48
 ---- batch: 090 ----
mean loss: 1603.72
 ---- batch: 100 ----
mean loss: 1493.58
 ---- batch: 110 ----
mean loss: 1406.97
train mean loss: 1964.40
epoch train time: 0:00:02.533105
elapsed time: 0:00:37.276535
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 20:49:23.788780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1325.83
 ---- batch: 020 ----
mean loss: 1275.90
 ---- batch: 030 ----
mean loss: 1223.22
 ---- batch: 040 ----
mean loss: 1165.36
 ---- batch: 050 ----
mean loss: 1111.85
 ---- batch: 060 ----
mean loss: 1070.42
 ---- batch: 070 ----
mean loss: 1067.61
 ---- batch: 080 ----
mean loss: 1021.35
 ---- batch: 090 ----
mean loss: 995.61
 ---- batch: 100 ----
mean loss: 967.97
 ---- batch: 110 ----
mean loss: 956.06
train mean loss: 1102.93
epoch train time: 0:00:02.515517
elapsed time: 0:00:39.792467
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 20:49:26.304684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.28
 ---- batch: 020 ----
mean loss: 934.16
 ---- batch: 030 ----
mean loss: 914.27
 ---- batch: 040 ----
mean loss: 904.31
 ---- batch: 050 ----
mean loss: 887.79
 ---- batch: 060 ----
mean loss: 880.69
 ---- batch: 070 ----
mean loss: 887.53
 ---- batch: 080 ----
mean loss: 858.72
 ---- batch: 090 ----
mean loss: 890.75
 ---- batch: 100 ----
mean loss: 889.99
 ---- batch: 110 ----
mean loss: 867.30
train mean loss: 895.64
epoch train time: 0:00:02.519130
elapsed time: 0:00:42.312008
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 20:49:28.824259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.91
 ---- batch: 020 ----
mean loss: 872.06
 ---- batch: 030 ----
mean loss: 868.67
 ---- batch: 040 ----
mean loss: 878.50
 ---- batch: 050 ----
mean loss: 867.07
 ---- batch: 060 ----
mean loss: 854.83
 ---- batch: 070 ----
mean loss: 860.90
 ---- batch: 080 ----
mean loss: 858.80
 ---- batch: 090 ----
mean loss: 854.75
 ---- batch: 100 ----
mean loss: 871.76
 ---- batch: 110 ----
mean loss: 870.51
train mean loss: 865.73
epoch train time: 0:00:02.515302
elapsed time: 0:00:44.827755
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 20:49:31.339996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.01
 ---- batch: 020 ----
mean loss: 859.52
 ---- batch: 030 ----
mean loss: 858.43
 ---- batch: 040 ----
mean loss: 840.79
 ---- batch: 050 ----
mean loss: 859.06
 ---- batch: 060 ----
mean loss: 873.00
 ---- batch: 070 ----
mean loss: 851.44
 ---- batch: 080 ----
mean loss: 870.64
 ---- batch: 090 ----
mean loss: 850.73
 ---- batch: 100 ----
mean loss: 859.86
 ---- batch: 110 ----
mean loss: 858.02
train mean loss: 857.46
epoch train time: 0:00:02.524023
elapsed time: 0:00:47.352211
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 20:49:33.864445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.85
 ---- batch: 020 ----
mean loss: 840.22
 ---- batch: 030 ----
mean loss: 845.11
 ---- batch: 040 ----
mean loss: 853.46
 ---- batch: 050 ----
mean loss: 825.75
 ---- batch: 060 ----
mean loss: 854.19
 ---- batch: 070 ----
mean loss: 844.22
 ---- batch: 080 ----
mean loss: 868.22
 ---- batch: 090 ----
mean loss: 863.05
 ---- batch: 100 ----
mean loss: 867.93
 ---- batch: 110 ----
mean loss: 857.10
train mean loss: 853.57
epoch train time: 0:00:02.553672
elapsed time: 0:00:49.906313
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 20:49:36.418558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.02
 ---- batch: 020 ----
mean loss: 833.52
 ---- batch: 030 ----
mean loss: 868.76
 ---- batch: 040 ----
mean loss: 850.29
 ---- batch: 050 ----
mean loss: 851.04
 ---- batch: 060 ----
mean loss: 856.83
 ---- batch: 070 ----
mean loss: 827.15
 ---- batch: 080 ----
mean loss: 849.55
 ---- batch: 090 ----
mean loss: 844.20
 ---- batch: 100 ----
mean loss: 859.01
 ---- batch: 110 ----
mean loss: 851.53
train mean loss: 847.94
epoch train time: 0:00:02.560416
elapsed time: 0:00:52.467142
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 20:49:38.979393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.22
 ---- batch: 020 ----
mean loss: 847.42
 ---- batch: 030 ----
mean loss: 834.90
 ---- batch: 040 ----
mean loss: 838.16
 ---- batch: 050 ----
mean loss: 849.25
 ---- batch: 060 ----
mean loss: 829.26
 ---- batch: 070 ----
mean loss: 852.66
 ---- batch: 080 ----
mean loss: 838.24
 ---- batch: 090 ----
mean loss: 837.78
 ---- batch: 100 ----
mean loss: 865.84
 ---- batch: 110 ----
mean loss: 856.11
train mean loss: 845.53
epoch train time: 0:00:02.522588
elapsed time: 0:00:54.990138
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 20:49:41.502349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.97
 ---- batch: 020 ----
mean loss: 855.17
 ---- batch: 030 ----
mean loss: 826.73
 ---- batch: 040 ----
mean loss: 853.80
 ---- batch: 050 ----
mean loss: 832.85
 ---- batch: 060 ----
mean loss: 840.71
 ---- batch: 070 ----
mean loss: 837.59
 ---- batch: 080 ----
mean loss: 867.72
 ---- batch: 090 ----
mean loss: 833.08
 ---- batch: 100 ----
mean loss: 824.34
 ---- batch: 110 ----
mean loss: 849.59
train mean loss: 841.38
epoch train time: 0:00:02.523744
elapsed time: 0:00:57.514263
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 20:49:44.026593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.35
 ---- batch: 020 ----
mean loss: 822.68
 ---- batch: 030 ----
mean loss: 836.63
 ---- batch: 040 ----
mean loss: 845.19
 ---- batch: 050 ----
mean loss: 825.98
 ---- batch: 060 ----
mean loss: 829.28
 ---- batch: 070 ----
mean loss: 838.62
 ---- batch: 080 ----
mean loss: 830.61
 ---- batch: 090 ----
mean loss: 845.01
 ---- batch: 100 ----
mean loss: 828.36
 ---- batch: 110 ----
mean loss: 828.10
train mean loss: 834.11
epoch train time: 0:00:02.508161
elapsed time: 0:01:00.022916
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 20:49:46.535131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.60
 ---- batch: 020 ----
mean loss: 818.32
 ---- batch: 030 ----
mean loss: 846.48
 ---- batch: 040 ----
mean loss: 842.92
 ---- batch: 050 ----
mean loss: 830.56
 ---- batch: 060 ----
mean loss: 823.81
 ---- batch: 070 ----
mean loss: 831.20
 ---- batch: 080 ----
mean loss: 817.71
 ---- batch: 090 ----
mean loss: 837.14
 ---- batch: 100 ----
mean loss: 824.08
 ---- batch: 110 ----
mean loss: 802.53
train mean loss: 831.20
epoch train time: 0:00:02.531134
elapsed time: 0:01:02.554417
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 20:49:49.066632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.96
 ---- batch: 020 ----
mean loss: 834.18
 ---- batch: 030 ----
mean loss: 828.11
 ---- batch: 040 ----
mean loss: 820.55
 ---- batch: 050 ----
mean loss: 824.73
 ---- batch: 060 ----
mean loss: 820.82
 ---- batch: 070 ----
mean loss: 836.73
 ---- batch: 080 ----
mean loss: 797.60
 ---- batch: 090 ----
mean loss: 818.58
 ---- batch: 100 ----
mean loss: 833.68
 ---- batch: 110 ----
mean loss: 809.19
train mean loss: 825.41
epoch train time: 0:00:02.519188
elapsed time: 0:01:05.074005
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 20:49:51.586311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 806.87
 ---- batch: 020 ----
mean loss: 822.52
 ---- batch: 030 ----
mean loss: 812.17
 ---- batch: 040 ----
mean loss: 804.80
 ---- batch: 050 ----
mean loss: 808.70
 ---- batch: 060 ----
mean loss: 829.47
 ---- batch: 070 ----
mean loss: 824.29
 ---- batch: 080 ----
mean loss: 824.39
 ---- batch: 090 ----
mean loss: 810.20
 ---- batch: 100 ----
mean loss: 826.21
 ---- batch: 110 ----
mean loss: 834.19
train mean loss: 819.42
epoch train time: 0:00:02.538388
elapsed time: 0:01:07.612853
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 20:49:54.125076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.76
 ---- batch: 020 ----
mean loss: 804.68
 ---- batch: 030 ----
mean loss: 825.05
 ---- batch: 040 ----
mean loss: 823.64
 ---- batch: 050 ----
mean loss: 815.13
 ---- batch: 060 ----
mean loss: 806.09
 ---- batch: 070 ----
mean loss: 813.39
 ---- batch: 080 ----
mean loss: 800.89
 ---- batch: 090 ----
mean loss: 810.70
 ---- batch: 100 ----
mean loss: 814.58
 ---- batch: 110 ----
mean loss: 830.01
train mean loss: 813.83
epoch train time: 0:00:02.518157
elapsed time: 0:01:10.131384
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 20:49:56.643642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 813.22
 ---- batch: 020 ----
mean loss: 813.27
 ---- batch: 030 ----
mean loss: 804.90
 ---- batch: 040 ----
mean loss: 790.64
 ---- batch: 050 ----
mean loss: 804.62
 ---- batch: 060 ----
mean loss: 815.70
 ---- batch: 070 ----
mean loss: 821.42
 ---- batch: 080 ----
mean loss: 795.78
 ---- batch: 090 ----
mean loss: 797.12
 ---- batch: 100 ----
mean loss: 820.33
 ---- batch: 110 ----
mean loss: 796.19
train mean loss: 807.49
epoch train time: 0:00:02.547543
elapsed time: 0:01:12.679400
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 20:49:59.191635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 798.08
 ---- batch: 020 ----
mean loss: 771.74
 ---- batch: 030 ----
mean loss: 802.46
 ---- batch: 040 ----
mean loss: 822.47
 ---- batch: 050 ----
mean loss: 820.68
 ---- batch: 060 ----
mean loss: 818.32
 ---- batch: 070 ----
mean loss: 813.40
 ---- batch: 080 ----
mean loss: 798.21
 ---- batch: 090 ----
mean loss: 790.27
 ---- batch: 100 ----
mean loss: 798.56
 ---- batch: 110 ----
mean loss: 793.21
train mean loss: 802.45
epoch train time: 0:00:02.524405
elapsed time: 0:01:15.204287
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 20:50:01.716619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 779.83
 ---- batch: 020 ----
mean loss: 805.83
 ---- batch: 030 ----
mean loss: 784.17
 ---- batch: 040 ----
mean loss: 805.59
 ---- batch: 050 ----
mean loss: 813.61
 ---- batch: 060 ----
mean loss: 780.67
 ---- batch: 070 ----
mean loss: 809.40
 ---- batch: 080 ----
mean loss: 792.77
 ---- batch: 090 ----
mean loss: 797.92
 ---- batch: 100 ----
mean loss: 810.09
 ---- batch: 110 ----
mean loss: 803.89
train mean loss: 798.04
epoch train time: 0:00:02.533771
elapsed time: 0:01:17.738553
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 20:50:04.250771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 796.63
 ---- batch: 020 ----
mean loss: 806.83
 ---- batch: 030 ----
mean loss: 797.48
 ---- batch: 040 ----
mean loss: 773.45
 ---- batch: 050 ----
mean loss: 789.09
 ---- batch: 060 ----
mean loss: 803.99
 ---- batch: 070 ----
mean loss: 794.32
 ---- batch: 080 ----
mean loss: 794.48
 ---- batch: 090 ----
mean loss: 800.34
 ---- batch: 100 ----
mean loss: 786.38
 ---- batch: 110 ----
mean loss: 811.94
train mean loss: 794.97
epoch train time: 0:00:02.521975
elapsed time: 0:01:20.260994
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 20:50:06.773226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 789.07
 ---- batch: 020 ----
mean loss: 797.56
 ---- batch: 030 ----
mean loss: 802.30
 ---- batch: 040 ----
mean loss: 785.30
 ---- batch: 050 ----
mean loss: 777.09
 ---- batch: 060 ----
mean loss: 800.94
 ---- batch: 070 ----
mean loss: 791.95
 ---- batch: 080 ----
mean loss: 794.48
 ---- batch: 090 ----
mean loss: 785.59
 ---- batch: 100 ----
mean loss: 787.86
 ---- batch: 110 ----
mean loss: 792.33
train mean loss: 790.47
epoch train time: 0:00:02.546624
elapsed time: 0:01:22.808041
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 20:50:09.320273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 753.43
 ---- batch: 020 ----
mean loss: 828.04
 ---- batch: 030 ----
mean loss: 789.82
 ---- batch: 040 ----
mean loss: 808.88
 ---- batch: 050 ----
mean loss: 790.62
 ---- batch: 060 ----
mean loss: 792.88
 ---- batch: 070 ----
mean loss: 783.96
 ---- batch: 080 ----
mean loss: 790.08
 ---- batch: 090 ----
mean loss: 772.49
 ---- batch: 100 ----
mean loss: 771.00
 ---- batch: 110 ----
mean loss: 767.01
train mean loss: 785.99
epoch train time: 0:00:02.525933
elapsed time: 0:01:25.334391
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 20:50:11.846572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 770.54
 ---- batch: 020 ----
mean loss: 798.66
 ---- batch: 030 ----
mean loss: 773.36
 ---- batch: 040 ----
mean loss: 799.81
 ---- batch: 050 ----
mean loss: 784.49
 ---- batch: 060 ----
mean loss: 778.71
 ---- batch: 070 ----
mean loss: 788.13
 ---- batch: 080 ----
mean loss: 779.92
 ---- batch: 090 ----
mean loss: 771.06
 ---- batch: 100 ----
mean loss: 770.97
 ---- batch: 110 ----
mean loss: 787.68
train mean loss: 781.50
epoch train time: 0:00:02.537843
elapsed time: 0:01:27.872605
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 20:50:14.384857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 759.00
 ---- batch: 020 ----
mean loss: 763.81
 ---- batch: 030 ----
mean loss: 755.31
 ---- batch: 040 ----
mean loss: 784.97
 ---- batch: 050 ----
mean loss: 798.82
 ---- batch: 060 ----
mean loss: 775.40
 ---- batch: 070 ----
mean loss: 793.97
 ---- batch: 080 ----
mean loss: 767.57
 ---- batch: 090 ----
mean loss: 769.14
 ---- batch: 100 ----
mean loss: 800.22
 ---- batch: 110 ----
mean loss: 768.39
train mean loss: 776.73
epoch train time: 0:00:02.523299
elapsed time: 0:01:30.396439
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 20:50:16.908710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.53
 ---- batch: 020 ----
mean loss: 775.35
 ---- batch: 030 ----
mean loss: 785.33
 ---- batch: 040 ----
mean loss: 769.63
 ---- batch: 050 ----
mean loss: 787.23
 ---- batch: 060 ----
mean loss: 764.27
 ---- batch: 070 ----
mean loss: 769.49
 ---- batch: 080 ----
mean loss: 780.22
 ---- batch: 090 ----
mean loss: 768.20
 ---- batch: 100 ----
mean loss: 779.92
 ---- batch: 110 ----
mean loss: 760.28
train mean loss: 772.45
epoch train time: 0:00:02.526006
elapsed time: 0:01:32.922884
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 20:50:19.435113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 768.20
 ---- batch: 020 ----
mean loss: 769.88
 ---- batch: 030 ----
mean loss: 760.43
 ---- batch: 040 ----
mean loss: 778.94
 ---- batch: 050 ----
mean loss: 767.92
 ---- batch: 060 ----
mean loss: 780.06
 ---- batch: 070 ----
mean loss: 750.35
 ---- batch: 080 ----
mean loss: 770.43
 ---- batch: 090 ----
mean loss: 779.70
 ---- batch: 100 ----
mean loss: 753.94
 ---- batch: 110 ----
mean loss: 774.34
train mean loss: 768.93
epoch train time: 0:00:02.539903
elapsed time: 0:01:35.463215
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 20:50:21.975476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 766.14
 ---- batch: 020 ----
mean loss: 762.59
 ---- batch: 030 ----
mean loss: 755.36
 ---- batch: 040 ----
mean loss: 764.44
 ---- batch: 050 ----
mean loss: 766.69
 ---- batch: 060 ----
mean loss: 769.09
 ---- batch: 070 ----
mean loss: 776.11
 ---- batch: 080 ----
mean loss: 747.81
 ---- batch: 090 ----
mean loss: 771.01
 ---- batch: 100 ----
mean loss: 761.35
 ---- batch: 110 ----
mean loss: 760.39
train mean loss: 763.02
epoch train time: 0:00:02.542351
elapsed time: 0:01:38.006027
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 20:50:24.518259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 765.46
 ---- batch: 020 ----
mean loss: 764.47
 ---- batch: 030 ----
mean loss: 768.43
 ---- batch: 040 ----
mean loss: 763.39
 ---- batch: 050 ----
mean loss: 753.94
 ---- batch: 060 ----
mean loss: 743.21
 ---- batch: 070 ----
mean loss: 739.97
 ---- batch: 080 ----
mean loss: 773.89
 ---- batch: 090 ----
mean loss: 768.45
 ---- batch: 100 ----
mean loss: 742.95
 ---- batch: 110 ----
mean loss: 750.41
train mean loss: 757.12
epoch train time: 0:00:02.552253
elapsed time: 0:01:40.558674
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 20:50:27.070905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 753.27
 ---- batch: 020 ----
mean loss: 745.96
 ---- batch: 030 ----
mean loss: 770.65
 ---- batch: 040 ----
mean loss: 770.08
 ---- batch: 050 ----
mean loss: 747.00
 ---- batch: 060 ----
mean loss: 749.19
 ---- batch: 070 ----
mean loss: 745.49
 ---- batch: 080 ----
mean loss: 747.64
 ---- batch: 090 ----
mean loss: 757.20
 ---- batch: 100 ----
mean loss: 740.97
 ---- batch: 110 ----
mean loss: 750.77
train mean loss: 751.65
epoch train time: 0:00:02.503256
elapsed time: 0:01:43.062324
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 20:50:29.574565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 710.81
 ---- batch: 020 ----
mean loss: 746.04
 ---- batch: 030 ----
mean loss: 758.08
 ---- batch: 040 ----
mean loss: 768.81
 ---- batch: 050 ----
mean loss: 768.18
 ---- batch: 060 ----
mean loss: 737.86
 ---- batch: 070 ----
mean loss: 745.41
 ---- batch: 080 ----
mean loss: 744.95
 ---- batch: 090 ----
mean loss: 732.70
 ---- batch: 100 ----
mean loss: 749.69
 ---- batch: 110 ----
mean loss: 736.97
train mean loss: 745.78
epoch train time: 0:00:02.552611
elapsed time: 0:01:45.615342
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 20:50:32.127630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 745.65
 ---- batch: 020 ----
mean loss: 724.02
 ---- batch: 030 ----
mean loss: 734.80
 ---- batch: 040 ----
mean loss: 738.67
 ---- batch: 050 ----
mean loss: 749.48
 ---- batch: 060 ----
mean loss: 735.37
 ---- batch: 070 ----
mean loss: 740.68
 ---- batch: 080 ----
mean loss: 748.98
 ---- batch: 090 ----
mean loss: 741.11
 ---- batch: 100 ----
mean loss: 747.41
 ---- batch: 110 ----
mean loss: 736.53
train mean loss: 739.63
epoch train time: 0:00:02.512936
elapsed time: 0:01:48.128861
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 20:50:34.641100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 734.83
 ---- batch: 020 ----
mean loss: 728.68
 ---- batch: 030 ----
mean loss: 743.87
 ---- batch: 040 ----
mean loss: 740.81
 ---- batch: 050 ----
mean loss: 737.97
 ---- batch: 060 ----
mean loss: 736.46
 ---- batch: 070 ----
mean loss: 698.79
 ---- batch: 080 ----
mean loss: 742.88
 ---- batch: 090 ----
mean loss: 734.68
 ---- batch: 100 ----
mean loss: 732.62
 ---- batch: 110 ----
mean loss: 737.96
train mean loss: 733.55
epoch train time: 0:00:02.532147
elapsed time: 0:01:50.661404
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 20:50:37.173670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.63
 ---- batch: 020 ----
mean loss: 711.91
 ---- batch: 030 ----
mean loss: 742.70
 ---- batch: 040 ----
mean loss: 747.17
 ---- batch: 050 ----
mean loss: 711.45
 ---- batch: 060 ----
mean loss: 732.83
 ---- batch: 070 ----
mean loss: 729.70
 ---- batch: 080 ----
mean loss: 728.19
 ---- batch: 090 ----
mean loss: 719.93
 ---- batch: 100 ----
mean loss: 728.42
 ---- batch: 110 ----
mean loss: 711.96
train mean loss: 727.14
epoch train time: 0:00:02.516278
elapsed time: 0:01:53.178100
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 20:50:39.690321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.66
 ---- batch: 020 ----
mean loss: 711.63
 ---- batch: 030 ----
mean loss: 737.02
 ---- batch: 040 ----
mean loss: 735.24
 ---- batch: 050 ----
mean loss: 723.07
 ---- batch: 060 ----
mean loss: 730.93
 ---- batch: 070 ----
mean loss: 693.98
 ---- batch: 080 ----
mean loss: 727.95
 ---- batch: 090 ----
mean loss: 717.72
 ---- batch: 100 ----
mean loss: 725.72
 ---- batch: 110 ----
mean loss: 708.34
train mean loss: 720.22
epoch train time: 0:00:02.538895
elapsed time: 0:01:55.717385
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 20:50:42.229700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 710.88
 ---- batch: 020 ----
mean loss: 717.23
 ---- batch: 030 ----
mean loss: 708.52
 ---- batch: 040 ----
mean loss: 714.55
 ---- batch: 050 ----
mean loss: 698.95
 ---- batch: 060 ----
mean loss: 706.61
 ---- batch: 070 ----
mean loss: 719.61
 ---- batch: 080 ----
mean loss: 720.71
 ---- batch: 090 ----
mean loss: 717.75
 ---- batch: 100 ----
mean loss: 719.08
 ---- batch: 110 ----
mean loss: 712.85
train mean loss: 712.86
epoch train time: 0:00:02.562573
elapsed time: 0:01:58.280471
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 20:50:44.792726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 705.46
 ---- batch: 020 ----
mean loss: 682.39
 ---- batch: 030 ----
mean loss: 702.36
 ---- batch: 040 ----
mean loss: 719.19
 ---- batch: 050 ----
mean loss: 708.76
 ---- batch: 060 ----
mean loss: 716.80
 ---- batch: 070 ----
mean loss: 690.41
 ---- batch: 080 ----
mean loss: 715.26
 ---- batch: 090 ----
mean loss: 713.03
 ---- batch: 100 ----
mean loss: 714.10
 ---- batch: 110 ----
mean loss: 706.22
train mean loss: 706.19
epoch train time: 0:00:02.544215
elapsed time: 0:02:00.825114
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 20:50:47.337268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.89
 ---- batch: 020 ----
mean loss: 709.93
 ---- batch: 030 ----
mean loss: 696.68
 ---- batch: 040 ----
mean loss: 695.06
 ---- batch: 050 ----
mean loss: 694.27
 ---- batch: 060 ----
mean loss: 691.84
 ---- batch: 070 ----
mean loss: 693.75
 ---- batch: 080 ----
mean loss: 696.99
 ---- batch: 090 ----
mean loss: 686.24
 ---- batch: 100 ----
mean loss: 695.07
 ---- batch: 110 ----
mean loss: 687.64
train mean loss: 697.50
epoch train time: 0:00:02.523371
elapsed time: 0:02:03.348867
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 20:50:49.861118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 705.59
 ---- batch: 020 ----
mean loss: 688.34
 ---- batch: 030 ----
mean loss: 701.71
 ---- batch: 040 ----
mean loss: 693.81
 ---- batch: 050 ----
mean loss: 681.20
 ---- batch: 060 ----
mean loss: 698.79
 ---- batch: 070 ----
mean loss: 680.27
 ---- batch: 080 ----
mean loss: 678.23
 ---- batch: 090 ----
mean loss: 699.76
 ---- batch: 100 ----
mean loss: 692.34
 ---- batch: 110 ----
mean loss: 671.94
train mean loss: 690.25
epoch train time: 0:00:02.543023
elapsed time: 0:02:05.892321
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 20:50:52.404566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 687.86
 ---- batch: 020 ----
mean loss: 693.80
 ---- batch: 030 ----
mean loss: 685.20
 ---- batch: 040 ----
mean loss: 685.06
 ---- batch: 050 ----
mean loss: 702.08
 ---- batch: 060 ----
mean loss: 665.86
 ---- batch: 070 ----
mean loss: 676.39
 ---- batch: 080 ----
mean loss: 673.01
 ---- batch: 090 ----
mean loss: 671.00
 ---- batch: 100 ----
mean loss: 676.60
 ---- batch: 110 ----
mean loss: 694.81
train mean loss: 682.55
epoch train time: 0:00:02.541086
elapsed time: 0:02:08.433861
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 20:50:54.946109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 686.52
 ---- batch: 020 ----
mean loss: 679.25
 ---- batch: 030 ----
mean loss: 682.21
 ---- batch: 040 ----
mean loss: 679.07
 ---- batch: 050 ----
mean loss: 653.17
 ---- batch: 060 ----
mean loss: 668.63
 ---- batch: 070 ----
mean loss: 675.05
 ---- batch: 080 ----
mean loss: 682.47
 ---- batch: 090 ----
mean loss: 665.52
 ---- batch: 100 ----
mean loss: 664.53
 ---- batch: 110 ----
mean loss: 673.81
train mean loss: 674.02
epoch train time: 0:00:02.531569
elapsed time: 0:02:10.965857
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 20:50:57.478099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 668.51
 ---- batch: 020 ----
mean loss: 673.74
 ---- batch: 030 ----
mean loss: 656.31
 ---- batch: 040 ----
mean loss: 665.83
 ---- batch: 050 ----
mean loss: 667.62
 ---- batch: 060 ----
mean loss: 662.13
 ---- batch: 070 ----
mean loss: 662.08
 ---- batch: 080 ----
mean loss: 671.30
 ---- batch: 090 ----
mean loss: 661.45
 ---- batch: 100 ----
mean loss: 671.08
 ---- batch: 110 ----
mean loss: 668.52
train mean loss: 665.43
epoch train time: 0:00:02.553661
elapsed time: 0:02:13.519937
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 20:51:00.032162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.82
 ---- batch: 020 ----
mean loss: 672.78
 ---- batch: 030 ----
mean loss: 650.78
 ---- batch: 040 ----
mean loss: 666.92
 ---- batch: 050 ----
mean loss: 664.36
 ---- batch: 060 ----
mean loss: 656.72
 ---- batch: 070 ----
mean loss: 657.26
 ---- batch: 080 ----
mean loss: 661.26
 ---- batch: 090 ----
mean loss: 643.22
 ---- batch: 100 ----
mean loss: 650.62
 ---- batch: 110 ----
mean loss: 655.94
train mean loss: 657.47
epoch train time: 0:00:02.526366
elapsed time: 0:02:16.046750
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 20:51:02.559080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 660.04
 ---- batch: 020 ----
mean loss: 641.16
 ---- batch: 030 ----
mean loss: 649.94
 ---- batch: 040 ----
mean loss: 651.66
 ---- batch: 050 ----
mean loss: 644.08
 ---- batch: 060 ----
mean loss: 639.05
 ---- batch: 070 ----
mean loss: 660.94
 ---- batch: 080 ----
mean loss: 661.67
 ---- batch: 090 ----
mean loss: 649.43
 ---- batch: 100 ----
mean loss: 645.79
 ---- batch: 110 ----
mean loss: 632.94
train mean loss: 648.10
epoch train time: 0:00:02.599644
elapsed time: 0:02:18.646945
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 20:51:05.159178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 643.15
 ---- batch: 020 ----
mean loss: 637.28
 ---- batch: 030 ----
mean loss: 644.91
 ---- batch: 040 ----
mean loss: 647.00
 ---- batch: 050 ----
mean loss: 644.15
 ---- batch: 060 ----
mean loss: 638.80
 ---- batch: 070 ----
mean loss: 643.67
 ---- batch: 080 ----
mean loss: 639.92
 ---- batch: 090 ----
mean loss: 634.87
 ---- batch: 100 ----
mean loss: 623.55
 ---- batch: 110 ----
mean loss: 638.70
train mean loss: 639.56
epoch train time: 0:00:02.557627
elapsed time: 0:02:21.205053
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 20:51:07.717343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 628.66
 ---- batch: 020 ----
mean loss: 625.08
 ---- batch: 030 ----
mean loss: 641.99
 ---- batch: 040 ----
mean loss: 632.13
 ---- batch: 050 ----
mean loss: 630.67
 ---- batch: 060 ----
mean loss: 628.36
 ---- batch: 070 ----
mean loss: 614.07
 ---- batch: 080 ----
mean loss: 639.07
 ---- batch: 090 ----
mean loss: 626.04
 ---- batch: 100 ----
mean loss: 635.50
 ---- batch: 110 ----
mean loss: 624.82
train mean loss: 630.17
epoch train time: 0:00:02.599576
elapsed time: 0:02:23.805103
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 20:51:10.317348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 619.30
 ---- batch: 020 ----
mean loss: 622.84
 ---- batch: 030 ----
mean loss: 619.85
 ---- batch: 040 ----
mean loss: 625.12
 ---- batch: 050 ----
mean loss: 618.48
 ---- batch: 060 ----
mean loss: 649.60
 ---- batch: 070 ----
mean loss: 629.54
 ---- batch: 080 ----
mean loss: 612.82
 ---- batch: 090 ----
mean loss: 611.88
 ---- batch: 100 ----
mean loss: 600.22
 ---- batch: 110 ----
mean loss: 616.42
train mean loss: 620.28
epoch train time: 0:00:02.581432
elapsed time: 0:02:26.387043
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 20:51:12.899330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 598.73
 ---- batch: 020 ----
mean loss: 630.93
 ---- batch: 030 ----
mean loss: 623.09
 ---- batch: 040 ----
mean loss: 619.29
 ---- batch: 050 ----
mean loss: 604.07
 ---- batch: 060 ----
mean loss: 601.02
 ---- batch: 070 ----
mean loss: 604.61
 ---- batch: 080 ----
mean loss: 605.67
 ---- batch: 090 ----
mean loss: 599.06
 ---- batch: 100 ----
mean loss: 613.83
 ---- batch: 110 ----
mean loss: 612.43
train mean loss: 609.72
epoch train time: 0:00:02.550172
elapsed time: 0:02:28.937710
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 20:51:15.449957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 600.17
 ---- batch: 020 ----
mean loss: 603.31
 ---- batch: 030 ----
mean loss: 605.39
 ---- batch: 040 ----
mean loss: 602.03
 ---- batch: 050 ----
mean loss: 596.15
 ---- batch: 060 ----
mean loss: 584.86
 ---- batch: 070 ----
mean loss: 615.90
 ---- batch: 080 ----
mean loss: 591.99
 ---- batch: 090 ----
mean loss: 603.80
 ---- batch: 100 ----
mean loss: 591.22
 ---- batch: 110 ----
mean loss: 605.55
train mean loss: 599.88
epoch train time: 0:00:02.509617
elapsed time: 0:02:31.447744
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 20:51:17.960024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 593.10
 ---- batch: 020 ----
mean loss: 600.96
 ---- batch: 030 ----
mean loss: 586.87
 ---- batch: 040 ----
mean loss: 579.07
 ---- batch: 050 ----
mean loss: 595.35
 ---- batch: 060 ----
mean loss: 585.25
 ---- batch: 070 ----
mean loss: 607.18
 ---- batch: 080 ----
mean loss: 587.64
 ---- batch: 090 ----
mean loss: 595.45
 ---- batch: 100 ----
mean loss: 569.83
 ---- batch: 110 ----
mean loss: 582.03
train mean loss: 589.04
epoch train time: 0:00:02.524621
elapsed time: 0:02:33.972810
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 20:51:20.485068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.05
 ---- batch: 020 ----
mean loss: 577.02
 ---- batch: 030 ----
mean loss: 587.41
 ---- batch: 040 ----
mean loss: 568.28
 ---- batch: 050 ----
mean loss: 568.79
 ---- batch: 060 ----
mean loss: 583.15
 ---- batch: 070 ----
mean loss: 586.21
 ---- batch: 080 ----
mean loss: 580.24
 ---- batch: 090 ----
mean loss: 568.94
 ---- batch: 100 ----
mean loss: 574.63
 ---- batch: 110 ----
mean loss: 566.17
train mean loss: 577.09
epoch train time: 0:00:02.544429
elapsed time: 0:02:36.517748
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 20:51:23.030251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.07
 ---- batch: 020 ----
mean loss: 571.46
 ---- batch: 030 ----
mean loss: 568.45
 ---- batch: 040 ----
mean loss: 573.52
 ---- batch: 050 ----
mean loss: 556.00
 ---- batch: 060 ----
mean loss: 566.70
 ---- batch: 070 ----
mean loss: 558.53
 ---- batch: 080 ----
mean loss: 568.00
 ---- batch: 090 ----
mean loss: 568.01
 ---- batch: 100 ----
mean loss: 554.40
 ---- batch: 110 ----
mean loss: 550.11
train mean loss: 565.21
epoch train time: 0:00:02.554558
elapsed time: 0:02:39.073028
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 20:51:25.585256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.64
 ---- batch: 020 ----
mean loss: 566.79
 ---- batch: 030 ----
mean loss: 551.79
 ---- batch: 040 ----
mean loss: 561.46
 ---- batch: 050 ----
mean loss: 559.44
 ---- batch: 060 ----
mean loss: 555.72
 ---- batch: 070 ----
mean loss: 551.58
 ---- batch: 080 ----
mean loss: 549.89
 ---- batch: 090 ----
mean loss: 545.65
 ---- batch: 100 ----
mean loss: 542.99
 ---- batch: 110 ----
mean loss: 543.70
train mean loss: 553.28
epoch train time: 0:00:02.530083
elapsed time: 0:02:41.603511
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 20:51:28.115773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.18
 ---- batch: 020 ----
mean loss: 545.97
 ---- batch: 030 ----
mean loss: 555.45
 ---- batch: 040 ----
mean loss: 537.80
 ---- batch: 050 ----
mean loss: 532.36
 ---- batch: 060 ----
mean loss: 544.01
 ---- batch: 070 ----
mean loss: 538.94
 ---- batch: 080 ----
mean loss: 527.93
 ---- batch: 090 ----
mean loss: 524.87
 ---- batch: 100 ----
mean loss: 543.80
 ---- batch: 110 ----
mean loss: 536.80
train mean loss: 539.24
epoch train time: 0:00:02.555435
elapsed time: 0:02:44.159396
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 20:51:30.671670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 534.24
 ---- batch: 020 ----
mean loss: 535.19
 ---- batch: 030 ----
mean loss: 524.90
 ---- batch: 040 ----
mean loss: 517.42
 ---- batch: 050 ----
mean loss: 516.07
 ---- batch: 060 ----
mean loss: 526.42
 ---- batch: 070 ----
mean loss: 531.18
 ---- batch: 080 ----
mean loss: 534.75
 ---- batch: 090 ----
mean loss: 514.40
 ---- batch: 100 ----
mean loss: 516.40
 ---- batch: 110 ----
mean loss: 534.32
train mean loss: 525.83
epoch train time: 0:00:02.533302
elapsed time: 0:02:46.693142
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 20:51:33.205395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 522.00
 ---- batch: 020 ----
mean loss: 516.70
 ---- batch: 030 ----
mean loss: 520.40
 ---- batch: 040 ----
mean loss: 502.99
 ---- batch: 050 ----
mean loss: 500.10
 ---- batch: 060 ----
mean loss: 517.78
 ---- batch: 070 ----
mean loss: 518.84
 ---- batch: 080 ----
mean loss: 516.25
 ---- batch: 090 ----
mean loss: 496.57
 ---- batch: 100 ----
mean loss: 506.84
 ---- batch: 110 ----
mean loss: 503.32
train mean loss: 510.60
epoch train time: 0:00:02.526390
elapsed time: 0:02:49.219991
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 20:51:35.732266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.65
 ---- batch: 020 ----
mean loss: 499.36
 ---- batch: 030 ----
mean loss: 506.72
 ---- batch: 040 ----
mean loss: 507.76
 ---- batch: 050 ----
mean loss: 504.05
 ---- batch: 060 ----
mean loss: 477.60
 ---- batch: 070 ----
mean loss: 489.50
 ---- batch: 080 ----
mean loss: 479.88
 ---- batch: 090 ----
mean loss: 488.44
 ---- batch: 100 ----
mean loss: 501.12
 ---- batch: 110 ----
mean loss: 489.17
train mean loss: 495.28
epoch train time: 0:00:02.560185
elapsed time: 0:02:51.780620
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 20:51:38.292853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.33
 ---- batch: 020 ----
mean loss: 486.37
 ---- batch: 030 ----
mean loss: 498.28
 ---- batch: 040 ----
mean loss: 478.66
 ---- batch: 050 ----
mean loss: 484.37
 ---- batch: 060 ----
mean loss: 472.98
 ---- batch: 070 ----
mean loss: 463.07
 ---- batch: 080 ----
mean loss: 475.09
 ---- batch: 090 ----
mean loss: 479.78
 ---- batch: 100 ----
mean loss: 468.08
 ---- batch: 110 ----
mean loss: 476.03
train mean loss: 480.43
epoch train time: 0:00:02.557496
elapsed time: 0:02:54.338548
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 20:51:40.850790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.64
 ---- batch: 020 ----
mean loss: 469.19
 ---- batch: 030 ----
mean loss: 472.77
 ---- batch: 040 ----
mean loss: 465.12
 ---- batch: 050 ----
mean loss: 475.96
 ---- batch: 060 ----
mean loss: 479.21
 ---- batch: 070 ----
mean loss: 450.28
 ---- batch: 080 ----
mean loss: 454.33
 ---- batch: 090 ----
mean loss: 464.29
 ---- batch: 100 ----
mean loss: 459.58
 ---- batch: 110 ----
mean loss: 460.67
train mean loss: 466.30
epoch train time: 0:00:02.537593
elapsed time: 0:02:56.876597
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 20:51:43.388700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.77
 ---- batch: 020 ----
mean loss: 464.48
 ---- batch: 030 ----
mean loss: 443.87
 ---- batch: 040 ----
mean loss: 446.73
 ---- batch: 050 ----
mean loss: 448.00
 ---- batch: 060 ----
mean loss: 445.49
 ---- batch: 070 ----
mean loss: 451.42
 ---- batch: 080 ----
mean loss: 436.16
 ---- batch: 090 ----
mean loss: 458.03
 ---- batch: 100 ----
mean loss: 457.95
 ---- batch: 110 ----
mean loss: 453.53
train mean loss: 451.87
epoch train time: 0:00:02.568889
elapsed time: 0:02:59.445785
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 20:51:45.958027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.98
 ---- batch: 020 ----
mean loss: 443.46
 ---- batch: 030 ----
mean loss: 443.54
 ---- batch: 040 ----
mean loss: 444.47
 ---- batch: 050 ----
mean loss: 439.50
 ---- batch: 060 ----
mean loss: 419.31
 ---- batch: 070 ----
mean loss: 437.53
 ---- batch: 080 ----
mean loss: 441.98
 ---- batch: 090 ----
mean loss: 436.08
 ---- batch: 100 ----
mean loss: 436.26
 ---- batch: 110 ----
mean loss: 436.89
train mean loss: 438.36
epoch train time: 0:00:02.548372
elapsed time: 0:03:01.994581
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 20:51:48.506847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.64
 ---- batch: 020 ----
mean loss: 428.70
 ---- batch: 030 ----
mean loss: 437.88
 ---- batch: 040 ----
mean loss: 424.06
 ---- batch: 050 ----
mean loss: 418.96
 ---- batch: 060 ----
mean loss: 423.38
 ---- batch: 070 ----
mean loss: 412.33
 ---- batch: 080 ----
mean loss: 422.60
 ---- batch: 090 ----
mean loss: 434.70
 ---- batch: 100 ----
mean loss: 424.02
 ---- batch: 110 ----
mean loss: 417.65
train mean loss: 425.52
epoch train time: 0:00:02.521390
elapsed time: 0:03:04.516456
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 20:51:51.028696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.52
 ---- batch: 020 ----
mean loss: 410.62
 ---- batch: 030 ----
mean loss: 424.85
 ---- batch: 040 ----
mean loss: 416.07
 ---- batch: 050 ----
mean loss: 425.14
 ---- batch: 060 ----
mean loss: 399.11
 ---- batch: 070 ----
mean loss: 406.60
 ---- batch: 080 ----
mean loss: 411.65
 ---- batch: 090 ----
mean loss: 404.93
 ---- batch: 100 ----
mean loss: 413.19
 ---- batch: 110 ----
mean loss: 409.29
train mean loss: 413.26
epoch train time: 0:00:02.538119
elapsed time: 0:03:07.055020
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 20:51:53.567265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.42
 ---- batch: 020 ----
mean loss: 408.65
 ---- batch: 030 ----
mean loss: 410.78
 ---- batch: 040 ----
mean loss: 396.98
 ---- batch: 050 ----
mean loss: 404.56
 ---- batch: 060 ----
mean loss: 403.29
 ---- batch: 070 ----
mean loss: 410.20
 ---- batch: 080 ----
mean loss: 397.33
 ---- batch: 090 ----
mean loss: 407.13
 ---- batch: 100 ----
mean loss: 397.32
 ---- batch: 110 ----
mean loss: 405.06
train mean loss: 402.93
epoch train time: 0:00:02.523268
elapsed time: 0:03:09.578699
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 20:51:56.090927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.28
 ---- batch: 020 ----
mean loss: 388.33
 ---- batch: 030 ----
mean loss: 388.25
 ---- batch: 040 ----
mean loss: 404.17
 ---- batch: 050 ----
mean loss: 399.37
 ---- batch: 060 ----
mean loss: 393.14
 ---- batch: 070 ----
mean loss: 376.98
 ---- batch: 080 ----
mean loss: 391.34
 ---- batch: 090 ----
mean loss: 393.08
 ---- batch: 100 ----
mean loss: 390.24
 ---- batch: 110 ----
mean loss: 394.97
train mean loss: 393.27
epoch train time: 0:00:02.529349
elapsed time: 0:03:12.108488
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 20:51:58.620742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.92
 ---- batch: 020 ----
mean loss: 373.97
 ---- batch: 030 ----
mean loss: 376.62
 ---- batch: 040 ----
mean loss: 389.77
 ---- batch: 050 ----
mean loss: 376.79
 ---- batch: 060 ----
mean loss: 382.36
 ---- batch: 070 ----
mean loss: 391.51
 ---- batch: 080 ----
mean loss: 380.29
 ---- batch: 090 ----
mean loss: 377.61
 ---- batch: 100 ----
mean loss: 387.72
 ---- batch: 110 ----
mean loss: 403.45
train mean loss: 383.61
epoch train time: 0:00:02.550538
elapsed time: 0:03:14.659471
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 20:52:01.171714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.75
 ---- batch: 020 ----
mean loss: 381.59
 ---- batch: 030 ----
mean loss: 373.44
 ---- batch: 040 ----
mean loss: 359.84
 ---- batch: 050 ----
mean loss: 371.94
 ---- batch: 060 ----
mean loss: 377.61
 ---- batch: 070 ----
mean loss: 375.54
 ---- batch: 080 ----
mean loss: 374.96
 ---- batch: 090 ----
mean loss: 390.32
 ---- batch: 100 ----
mean loss: 371.38
 ---- batch: 110 ----
mean loss: 367.96
train mean loss: 374.64
epoch train time: 0:00:02.541253
elapsed time: 0:03:17.201150
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 20:52:03.713395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.09
 ---- batch: 020 ----
mean loss: 363.87
 ---- batch: 030 ----
mean loss: 368.08
 ---- batch: 040 ----
mean loss: 372.39
 ---- batch: 050 ----
mean loss: 375.54
 ---- batch: 060 ----
mean loss: 363.90
 ---- batch: 070 ----
mean loss: 362.73
 ---- batch: 080 ----
mean loss: 367.92
 ---- batch: 090 ----
mean loss: 359.15
 ---- batch: 100 ----
mean loss: 355.04
 ---- batch: 110 ----
mean loss: 366.07
train mean loss: 366.96
epoch train time: 0:00:02.522886
elapsed time: 0:03:19.724499
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 20:52:06.236730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.78
 ---- batch: 020 ----
mean loss: 365.65
 ---- batch: 030 ----
mean loss: 360.91
 ---- batch: 040 ----
mean loss: 362.28
 ---- batch: 050 ----
mean loss: 359.17
 ---- batch: 060 ----
mean loss: 367.80
 ---- batch: 070 ----
mean loss: 363.12
 ---- batch: 080 ----
mean loss: 359.03
 ---- batch: 090 ----
mean loss: 357.42
 ---- batch: 100 ----
mean loss: 353.79
 ---- batch: 110 ----
mean loss: 346.40
train mean loss: 359.22
epoch train time: 0:00:02.508389
elapsed time: 0:03:22.233300
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 20:52:08.745540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.44
 ---- batch: 020 ----
mean loss: 355.43
 ---- batch: 030 ----
mean loss: 348.86
 ---- batch: 040 ----
mean loss: 358.20
 ---- batch: 050 ----
mean loss: 347.05
 ---- batch: 060 ----
mean loss: 347.16
 ---- batch: 070 ----
mean loss: 342.79
 ---- batch: 080 ----
mean loss: 344.62
 ---- batch: 090 ----
mean loss: 352.18
 ---- batch: 100 ----
mean loss: 352.57
 ---- batch: 110 ----
mean loss: 354.99
train mean loss: 351.34
epoch train time: 0:00:02.519828
elapsed time: 0:03:24.753530
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 20:52:11.265839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.89
 ---- batch: 020 ----
mean loss: 350.32
 ---- batch: 030 ----
mean loss: 339.35
 ---- batch: 040 ----
mean loss: 344.83
 ---- batch: 050 ----
mean loss: 345.26
 ---- batch: 060 ----
mean loss: 356.49
 ---- batch: 070 ----
mean loss: 347.21
 ---- batch: 080 ----
mean loss: 339.30
 ---- batch: 090 ----
mean loss: 337.10
 ---- batch: 100 ----
mean loss: 331.42
 ---- batch: 110 ----
mean loss: 349.12
train mean loss: 343.71
epoch train time: 0:00:02.541852
elapsed time: 0:03:27.295869
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 20:52:13.808090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.06
 ---- batch: 020 ----
mean loss: 333.68
 ---- batch: 030 ----
mean loss: 336.99
 ---- batch: 040 ----
mean loss: 325.89
 ---- batch: 050 ----
mean loss: 330.22
 ---- batch: 060 ----
mean loss: 346.83
 ---- batch: 070 ----
mean loss: 340.63
 ---- batch: 080 ----
mean loss: 329.17
 ---- batch: 090 ----
mean loss: 338.31
 ---- batch: 100 ----
mean loss: 324.59
 ---- batch: 110 ----
mean loss: 338.19
train mean loss: 335.38
epoch train time: 0:00:02.519359
elapsed time: 0:03:29.815632
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 20:52:16.327852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.08
 ---- batch: 020 ----
mean loss: 330.50
 ---- batch: 030 ----
mean loss: 335.67
 ---- batch: 040 ----
mean loss: 333.28
 ---- batch: 050 ----
mean loss: 326.93
 ---- batch: 060 ----
mean loss: 313.26
 ---- batch: 070 ----
mean loss: 333.04
 ---- batch: 080 ----
mean loss: 326.40
 ---- batch: 090 ----
mean loss: 326.02
 ---- batch: 100 ----
mean loss: 324.06
 ---- batch: 110 ----
mean loss: 327.08
train mean loss: 327.35
epoch train time: 0:00:02.539547
elapsed time: 0:03:32.355592
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 20:52:18.867818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.80
 ---- batch: 020 ----
mean loss: 316.30
 ---- batch: 030 ----
mean loss: 326.07
 ---- batch: 040 ----
mean loss: 320.72
 ---- batch: 050 ----
mean loss: 302.30
 ---- batch: 060 ----
mean loss: 312.87
 ---- batch: 070 ----
mean loss: 323.59
 ---- batch: 080 ----
mean loss: 331.97
 ---- batch: 090 ----
mean loss: 318.37
 ---- batch: 100 ----
mean loss: 323.72
 ---- batch: 110 ----
mean loss: 314.23
train mean loss: 319.62
epoch train time: 0:00:02.543089
elapsed time: 0:03:34.899107
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 20:52:21.411405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.25
 ---- batch: 020 ----
mean loss: 310.93
 ---- batch: 030 ----
mean loss: 312.89
 ---- batch: 040 ----
mean loss: 319.86
 ---- batch: 050 ----
mean loss: 314.61
 ---- batch: 060 ----
mean loss: 310.35
 ---- batch: 070 ----
mean loss: 309.02
 ---- batch: 080 ----
mean loss: 314.95
 ---- batch: 090 ----
mean loss: 312.94
 ---- batch: 100 ----
mean loss: 310.51
 ---- batch: 110 ----
mean loss: 305.50
train mean loss: 311.19
epoch train time: 0:00:02.545358
elapsed time: 0:03:37.444947
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 20:52:23.957178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.54
 ---- batch: 020 ----
mean loss: 312.47
 ---- batch: 030 ----
mean loss: 297.32
 ---- batch: 040 ----
mean loss: 299.90
 ---- batch: 050 ----
mean loss: 298.33
 ---- batch: 060 ----
mean loss: 304.83
 ---- batch: 070 ----
mean loss: 301.77
 ---- batch: 080 ----
mean loss: 312.34
 ---- batch: 090 ----
mean loss: 299.42
 ---- batch: 100 ----
mean loss: 300.24
 ---- batch: 110 ----
mean loss: 299.01
train mean loss: 303.21
epoch train time: 0:00:02.550274
elapsed time: 0:03:39.995646
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 20:52:26.507904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.42
 ---- batch: 020 ----
mean loss: 292.33
 ---- batch: 030 ----
mean loss: 291.41
 ---- batch: 040 ----
mean loss: 293.21
 ---- batch: 050 ----
mean loss: 288.49
 ---- batch: 060 ----
mean loss: 304.73
 ---- batch: 070 ----
mean loss: 301.85
 ---- batch: 080 ----
mean loss: 295.45
 ---- batch: 090 ----
mean loss: 291.22
 ---- batch: 100 ----
mean loss: 296.09
 ---- batch: 110 ----
mean loss: 290.93
train mean loss: 295.43
epoch train time: 0:00:02.548261
elapsed time: 0:03:42.544336
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 20:52:29.056565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.76
 ---- batch: 020 ----
mean loss: 289.28
 ---- batch: 030 ----
mean loss: 302.59
 ---- batch: 040 ----
mean loss: 288.29
 ---- batch: 050 ----
mean loss: 294.39
 ---- batch: 060 ----
mean loss: 283.04
 ---- batch: 070 ----
mean loss: 278.98
 ---- batch: 080 ----
mean loss: 277.85
 ---- batch: 090 ----
mean loss: 284.55
 ---- batch: 100 ----
mean loss: 281.64
 ---- batch: 110 ----
mean loss: 281.03
train mean loss: 286.48
epoch train time: 0:00:02.555313
elapsed time: 0:03:45.100084
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 20:52:31.612342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.69
 ---- batch: 020 ----
mean loss: 280.89
 ---- batch: 030 ----
mean loss: 284.64
 ---- batch: 040 ----
mean loss: 280.10
 ---- batch: 050 ----
mean loss: 278.30
 ---- batch: 060 ----
mean loss: 284.26
 ---- batch: 070 ----
mean loss: 280.06
 ---- batch: 080 ----
mean loss: 278.95
 ---- batch: 090 ----
mean loss: 278.96
 ---- batch: 100 ----
mean loss: 277.65
 ---- batch: 110 ----
mean loss: 280.32
train mean loss: 279.92
epoch train time: 0:00:02.564221
elapsed time: 0:03:47.664732
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 20:52:34.176966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.23
 ---- batch: 020 ----
mean loss: 280.74
 ---- batch: 030 ----
mean loss: 274.38
 ---- batch: 040 ----
mean loss: 276.07
 ---- batch: 050 ----
mean loss: 282.16
 ---- batch: 060 ----
mean loss: 276.01
 ---- batch: 070 ----
mean loss: 268.02
 ---- batch: 080 ----
mean loss: 271.17
 ---- batch: 090 ----
mean loss: 280.44
 ---- batch: 100 ----
mean loss: 270.22
 ---- batch: 110 ----
mean loss: 262.07
train mean loss: 273.89
epoch train time: 0:00:02.524515
elapsed time: 0:03:50.189714
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 20:52:36.702002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.59
 ---- batch: 020 ----
mean loss: 271.16
 ---- batch: 030 ----
mean loss: 262.45
 ---- batch: 040 ----
mean loss: 269.43
 ---- batch: 050 ----
mean loss: 271.97
 ---- batch: 060 ----
mean loss: 276.11
 ---- batch: 070 ----
mean loss: 268.67
 ---- batch: 080 ----
mean loss: 274.89
 ---- batch: 090 ----
mean loss: 258.98
 ---- batch: 100 ----
mean loss: 263.17
 ---- batch: 110 ----
mean loss: 266.10
train mean loss: 268.34
epoch train time: 0:00:02.526552
elapsed time: 0:03:52.716733
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 20:52:39.228956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.11
 ---- batch: 020 ----
mean loss: 266.93
 ---- batch: 030 ----
mean loss: 262.31
 ---- batch: 040 ----
mean loss: 269.08
 ---- batch: 050 ----
mean loss: 264.74
 ---- batch: 060 ----
mean loss: 264.67
 ---- batch: 070 ----
mean loss: 259.94
 ---- batch: 080 ----
mean loss: 260.36
 ---- batch: 090 ----
mean loss: 270.86
 ---- batch: 100 ----
mean loss: 247.99
 ---- batch: 110 ----
mean loss: 270.26
train mean loss: 263.78
epoch train time: 0:00:02.536909
elapsed time: 0:03:55.254037
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 20:52:41.766263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.74
 ---- batch: 020 ----
mean loss: 264.48
 ---- batch: 030 ----
mean loss: 255.62
 ---- batch: 040 ----
mean loss: 253.16
 ---- batch: 050 ----
mean loss: 264.64
 ---- batch: 060 ----
mean loss: 259.62
 ---- batch: 070 ----
mean loss: 266.69
 ---- batch: 080 ----
mean loss: 256.12
 ---- batch: 090 ----
mean loss: 256.69
 ---- batch: 100 ----
mean loss: 262.34
 ---- batch: 110 ----
mean loss: 252.65
train mean loss: 259.35
epoch train time: 0:00:02.535061
elapsed time: 0:03:57.789528
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 20:52:44.301795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.23
 ---- batch: 020 ----
mean loss: 257.79
 ---- batch: 030 ----
mean loss: 253.54
 ---- batch: 040 ----
mean loss: 256.40
 ---- batch: 050 ----
mean loss: 255.21
 ---- batch: 060 ----
mean loss: 253.20
 ---- batch: 070 ----
mean loss: 250.01
 ---- batch: 080 ----
mean loss: 267.88
 ---- batch: 090 ----
mean loss: 261.17
 ---- batch: 100 ----
mean loss: 247.94
 ---- batch: 110 ----
mean loss: 258.91
train mean loss: 255.91
epoch train time: 0:00:02.542760
elapsed time: 0:04:00.332744
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 20:52:46.844977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.20
 ---- batch: 020 ----
mean loss: 259.80
 ---- batch: 030 ----
mean loss: 258.11
 ---- batch: 040 ----
mean loss: 256.29
 ---- batch: 050 ----
mean loss: 251.21
 ---- batch: 060 ----
mean loss: 250.43
 ---- batch: 070 ----
mean loss: 255.93
 ---- batch: 080 ----
mean loss: 248.05
 ---- batch: 090 ----
mean loss: 247.51
 ---- batch: 100 ----
mean loss: 254.11
 ---- batch: 110 ----
mean loss: 250.91
train mean loss: 252.74
epoch train time: 0:00:02.541215
elapsed time: 0:04:02.874419
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 20:52:49.386733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.15
 ---- batch: 020 ----
mean loss: 253.92
 ---- batch: 030 ----
mean loss: 245.91
 ---- batch: 040 ----
mean loss: 244.43
 ---- batch: 050 ----
mean loss: 254.27
 ---- batch: 060 ----
mean loss: 249.97
 ---- batch: 070 ----
mean loss: 237.93
 ---- batch: 080 ----
mean loss: 246.70
 ---- batch: 090 ----
mean loss: 252.60
 ---- batch: 100 ----
mean loss: 250.55
 ---- batch: 110 ----
mean loss: 251.36
train mean loss: 249.34
epoch train time: 0:00:02.550044
elapsed time: 0:04:05.424939
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 20:52:51.937172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.97
 ---- batch: 020 ----
mean loss: 251.48
 ---- batch: 030 ----
mean loss: 247.95
 ---- batch: 040 ----
mean loss: 242.54
 ---- batch: 050 ----
mean loss: 238.07
 ---- batch: 060 ----
mean loss: 244.03
 ---- batch: 070 ----
mean loss: 252.16
 ---- batch: 080 ----
mean loss: 256.43
 ---- batch: 090 ----
mean loss: 245.86
 ---- batch: 100 ----
mean loss: 250.42
 ---- batch: 110 ----
mean loss: 241.86
train mean loss: 246.33
epoch train time: 0:00:02.565882
elapsed time: 0:04:07.991272
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 20:52:54.503553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.55
 ---- batch: 020 ----
mean loss: 251.41
 ---- batch: 030 ----
mean loss: 237.05
 ---- batch: 040 ----
mean loss: 248.30
 ---- batch: 050 ----
mean loss: 249.03
 ---- batch: 060 ----
mean loss: 242.02
 ---- batch: 070 ----
mean loss: 247.88
 ---- batch: 080 ----
mean loss: 246.79
 ---- batch: 090 ----
mean loss: 241.83
 ---- batch: 100 ----
mean loss: 241.78
 ---- batch: 110 ----
mean loss: 239.59
train mean loss: 243.79
epoch train time: 0:00:02.533933
elapsed time: 0:04:10.525693
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 20:52:57.037940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.61
 ---- batch: 020 ----
mean loss: 253.81
 ---- batch: 030 ----
mean loss: 239.79
 ---- batch: 040 ----
mean loss: 241.96
 ---- batch: 050 ----
mean loss: 242.78
 ---- batch: 060 ----
mean loss: 238.71
 ---- batch: 070 ----
mean loss: 233.02
 ---- batch: 080 ----
mean loss: 235.75
 ---- batch: 090 ----
mean loss: 246.59
 ---- batch: 100 ----
mean loss: 241.14
 ---- batch: 110 ----
mean loss: 244.73
train mean loss: 241.37
epoch train time: 0:00:02.540620
elapsed time: 0:04:13.066719
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 20:52:59.578958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.49
 ---- batch: 020 ----
mean loss: 244.91
 ---- batch: 030 ----
mean loss: 238.13
 ---- batch: 040 ----
mean loss: 229.57
 ---- batch: 050 ----
mean loss: 241.30
 ---- batch: 060 ----
mean loss: 240.47
 ---- batch: 070 ----
mean loss: 237.04
 ---- batch: 080 ----
mean loss: 248.68
 ---- batch: 090 ----
mean loss: 238.30
 ---- batch: 100 ----
mean loss: 231.45
 ---- batch: 110 ----
mean loss: 235.95
train mean loss: 239.12
epoch train time: 0:00:02.531922
elapsed time: 0:04:15.599093
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 20:53:02.111320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.94
 ---- batch: 020 ----
mean loss: 234.73
 ---- batch: 030 ----
mean loss: 239.69
 ---- batch: 040 ----
mean loss: 239.23
 ---- batch: 050 ----
mean loss: 241.11
 ---- batch: 060 ----
mean loss: 233.73
 ---- batch: 070 ----
mean loss: 236.73
 ---- batch: 080 ----
mean loss: 242.45
 ---- batch: 090 ----
mean loss: 241.22
 ---- batch: 100 ----
mean loss: 239.05
 ---- batch: 110 ----
mean loss: 237.20
train mean loss: 237.15
epoch train time: 0:00:02.522415
elapsed time: 0:04:18.121953
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 20:53:04.634217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.01
 ---- batch: 020 ----
mean loss: 233.87
 ---- batch: 030 ----
mean loss: 240.93
 ---- batch: 040 ----
mean loss: 243.51
 ---- batch: 050 ----
mean loss: 228.73
 ---- batch: 060 ----
mean loss: 232.47
 ---- batch: 070 ----
mean loss: 234.48
 ---- batch: 080 ----
mean loss: 235.75
 ---- batch: 090 ----
mean loss: 234.76
 ---- batch: 100 ----
mean loss: 235.15
 ---- batch: 110 ----
mean loss: 229.47
train mean loss: 235.77
epoch train time: 0:00:02.542361
elapsed time: 0:04:20.664811
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 20:53:07.177153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.15
 ---- batch: 020 ----
mean loss: 231.64
 ---- batch: 030 ----
mean loss: 225.11
 ---- batch: 040 ----
mean loss: 237.78
 ---- batch: 050 ----
mean loss: 229.00
 ---- batch: 060 ----
mean loss: 242.14
 ---- batch: 070 ----
mean loss: 238.52
 ---- batch: 080 ----
mean loss: 239.40
 ---- batch: 090 ----
mean loss: 228.64
 ---- batch: 100 ----
mean loss: 232.74
 ---- batch: 110 ----
mean loss: 236.96
train mean loss: 233.71
epoch train time: 0:00:02.511793
elapsed time: 0:04:23.177151
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 20:53:09.689375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.47
 ---- batch: 020 ----
mean loss: 241.20
 ---- batch: 030 ----
mean loss: 241.15
 ---- batch: 040 ----
mean loss: 227.34
 ---- batch: 050 ----
mean loss: 230.18
 ---- batch: 060 ----
mean loss: 234.46
 ---- batch: 070 ----
mean loss: 230.81
 ---- batch: 080 ----
mean loss: 227.79
 ---- batch: 090 ----
mean loss: 229.40
 ---- batch: 100 ----
mean loss: 224.01
 ---- batch: 110 ----
mean loss: 237.59
train mean loss: 232.43
epoch train time: 0:00:02.534799
elapsed time: 0:04:25.712409
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 20:53:12.224637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.71
 ---- batch: 020 ----
mean loss: 232.81
 ---- batch: 030 ----
mean loss: 233.62
 ---- batch: 040 ----
mean loss: 238.74
 ---- batch: 050 ----
mean loss: 224.10
 ---- batch: 060 ----
mean loss: 228.98
 ---- batch: 070 ----
mean loss: 235.29
 ---- batch: 080 ----
mean loss: 227.95
 ---- batch: 090 ----
mean loss: 226.71
 ---- batch: 100 ----
mean loss: 224.67
 ---- batch: 110 ----
mean loss: 234.38
train mean loss: 230.96
epoch train time: 0:00:02.543070
elapsed time: 0:04:28.255935
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 20:53:14.768172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.11
 ---- batch: 020 ----
mean loss: 234.20
 ---- batch: 030 ----
mean loss: 231.02
 ---- batch: 040 ----
mean loss: 237.12
 ---- batch: 050 ----
mean loss: 224.68
 ---- batch: 060 ----
mean loss: 217.56
 ---- batch: 070 ----
mean loss: 233.51
 ---- batch: 080 ----
mean loss: 222.62
 ---- batch: 090 ----
mean loss: 236.18
 ---- batch: 100 ----
mean loss: 228.37
 ---- batch: 110 ----
mean loss: 227.31
train mean loss: 229.35
epoch train time: 0:00:02.533708
elapsed time: 0:04:30.790043
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 20:53:17.302301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.42
 ---- batch: 020 ----
mean loss: 238.45
 ---- batch: 030 ----
mean loss: 227.62
 ---- batch: 040 ----
mean loss: 226.23
 ---- batch: 050 ----
mean loss: 225.97
 ---- batch: 060 ----
mean loss: 227.91
 ---- batch: 070 ----
mean loss: 232.49
 ---- batch: 080 ----
mean loss: 231.53
 ---- batch: 090 ----
mean loss: 226.85
 ---- batch: 100 ----
mean loss: 225.56
 ---- batch: 110 ----
mean loss: 224.10
train mean loss: 228.30
epoch train time: 0:00:02.559081
elapsed time: 0:04:33.349587
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 20:53:19.861886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.20
 ---- batch: 020 ----
mean loss: 230.27
 ---- batch: 030 ----
mean loss: 228.42
 ---- batch: 040 ----
mean loss: 226.12
 ---- batch: 050 ----
mean loss: 229.96
 ---- batch: 060 ----
mean loss: 225.37
 ---- batch: 070 ----
mean loss: 229.06
 ---- batch: 080 ----
mean loss: 224.03
 ---- batch: 090 ----
mean loss: 228.21
 ---- batch: 100 ----
mean loss: 226.03
 ---- batch: 110 ----
mean loss: 219.40
train mean loss: 226.47
epoch train time: 0:00:02.530256
elapsed time: 0:04:35.880328
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 20:53:22.392923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.67
 ---- batch: 020 ----
mean loss: 226.27
 ---- batch: 030 ----
mean loss: 229.33
 ---- batch: 040 ----
mean loss: 229.24
 ---- batch: 050 ----
mean loss: 234.34
 ---- batch: 060 ----
mean loss: 222.23
 ---- batch: 070 ----
mean loss: 217.12
 ---- batch: 080 ----
mean loss: 218.54
 ---- batch: 090 ----
mean loss: 226.55
 ---- batch: 100 ----
mean loss: 223.60
 ---- batch: 110 ----
mean loss: 225.05
train mean loss: 225.52
epoch train time: 0:00:02.525776
elapsed time: 0:04:38.406890
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 20:53:24.919120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.98
 ---- batch: 020 ----
mean loss: 218.84
 ---- batch: 030 ----
mean loss: 207.87
 ---- batch: 040 ----
mean loss: 230.74
 ---- batch: 050 ----
mean loss: 237.68
 ---- batch: 060 ----
mean loss: 228.99
 ---- batch: 070 ----
mean loss: 220.70
 ---- batch: 080 ----
mean loss: 223.91
 ---- batch: 090 ----
mean loss: 221.89
 ---- batch: 100 ----
mean loss: 223.60
 ---- batch: 110 ----
mean loss: 229.99
train mean loss: 224.17
epoch train time: 0:00:02.519705
elapsed time: 0:04:40.927009
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 20:53:27.439228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.15
 ---- batch: 020 ----
mean loss: 218.56
 ---- batch: 030 ----
mean loss: 223.79
 ---- batch: 040 ----
mean loss: 225.58
 ---- batch: 050 ----
mean loss: 234.06
 ---- batch: 060 ----
mean loss: 218.05
 ---- batch: 070 ----
mean loss: 220.71
 ---- batch: 080 ----
mean loss: 229.91
 ---- batch: 090 ----
mean loss: 226.53
 ---- batch: 100 ----
mean loss: 226.12
 ---- batch: 110 ----
mean loss: 220.64
train mean loss: 223.45
epoch train time: 0:00:02.550403
elapsed time: 0:04:43.477800
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 20:53:29.989885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.95
 ---- batch: 020 ----
mean loss: 226.49
 ---- batch: 030 ----
mean loss: 222.61
 ---- batch: 040 ----
mean loss: 214.48
 ---- batch: 050 ----
mean loss: 219.70
 ---- batch: 060 ----
mean loss: 227.05
 ---- batch: 070 ----
mean loss: 221.58
 ---- batch: 080 ----
mean loss: 233.12
 ---- batch: 090 ----
mean loss: 223.17
 ---- batch: 100 ----
mean loss: 224.17
 ---- batch: 110 ----
mean loss: 217.72
train mean loss: 222.26
epoch train time: 0:00:02.526313
elapsed time: 0:04:46.004359
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 20:53:32.516611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.96
 ---- batch: 020 ----
mean loss: 227.40
 ---- batch: 030 ----
mean loss: 212.92
 ---- batch: 040 ----
mean loss: 226.62
 ---- batch: 050 ----
mean loss: 211.15
 ---- batch: 060 ----
mean loss: 222.50
 ---- batch: 070 ----
mean loss: 214.20
 ---- batch: 080 ----
mean loss: 213.04
 ---- batch: 090 ----
mean loss: 220.36
 ---- batch: 100 ----
mean loss: 222.71
 ---- batch: 110 ----
mean loss: 231.30
train mean loss: 221.07
epoch train time: 0:00:02.530153
elapsed time: 0:04:48.534948
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 20:53:35.047184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.16
 ---- batch: 020 ----
mean loss: 221.32
 ---- batch: 030 ----
mean loss: 227.38
 ---- batch: 040 ----
mean loss: 217.71
 ---- batch: 050 ----
mean loss: 223.09
 ---- batch: 060 ----
mean loss: 222.38
 ---- batch: 070 ----
mean loss: 218.67
 ---- batch: 080 ----
mean loss: 215.42
 ---- batch: 090 ----
mean loss: 217.31
 ---- batch: 100 ----
mean loss: 224.73
 ---- batch: 110 ----
mean loss: 221.04
train mean loss: 220.03
epoch train time: 0:00:02.511933
elapsed time: 0:04:51.047289
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 20:53:37.559570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.18
 ---- batch: 020 ----
mean loss: 218.19
 ---- batch: 030 ----
mean loss: 224.52
 ---- batch: 040 ----
mean loss: 215.27
 ---- batch: 050 ----
mean loss: 224.45
 ---- batch: 060 ----
mean loss: 214.85
 ---- batch: 070 ----
mean loss: 216.63
 ---- batch: 080 ----
mean loss: 222.47
 ---- batch: 090 ----
mean loss: 220.91
 ---- batch: 100 ----
mean loss: 213.31
 ---- batch: 110 ----
mean loss: 219.19
train mean loss: 219.20
epoch train time: 0:00:02.537793
elapsed time: 0:04:53.585544
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 20:53:40.097792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.48
 ---- batch: 020 ----
mean loss: 217.11
 ---- batch: 030 ----
mean loss: 225.41
 ---- batch: 040 ----
mean loss: 216.86
 ---- batch: 050 ----
mean loss: 215.53
 ---- batch: 060 ----
mean loss: 220.20
 ---- batch: 070 ----
mean loss: 212.86
 ---- batch: 080 ----
mean loss: 223.83
 ---- batch: 090 ----
mean loss: 213.11
 ---- batch: 100 ----
mean loss: 217.47
 ---- batch: 110 ----
mean loss: 214.90
train mean loss: 218.12
epoch train time: 0:00:02.515140
elapsed time: 0:04:56.101136
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 20:53:42.613389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.42
 ---- batch: 020 ----
mean loss: 211.82
 ---- batch: 030 ----
mean loss: 221.50
 ---- batch: 040 ----
mean loss: 216.22
 ---- batch: 050 ----
mean loss: 214.18
 ---- batch: 060 ----
mean loss: 219.10
 ---- batch: 070 ----
mean loss: 213.08
 ---- batch: 080 ----
mean loss: 213.16
 ---- batch: 090 ----
mean loss: 212.40
 ---- batch: 100 ----
mean loss: 219.07
 ---- batch: 110 ----
mean loss: 225.25
train mean loss: 217.39
epoch train time: 0:00:02.533398
elapsed time: 0:04:58.634961
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 20:53:45.147230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.97
 ---- batch: 020 ----
mean loss: 218.34
 ---- batch: 030 ----
mean loss: 218.26
 ---- batch: 040 ----
mean loss: 213.70
 ---- batch: 050 ----
mean loss: 216.80
 ---- batch: 060 ----
mean loss: 210.69
 ---- batch: 070 ----
mean loss: 221.38
 ---- batch: 080 ----
mean loss: 208.96
 ---- batch: 090 ----
mean loss: 216.78
 ---- batch: 100 ----
mean loss: 219.12
 ---- batch: 110 ----
mean loss: 227.66
train mean loss: 216.48
epoch train time: 0:00:02.584237
elapsed time: 0:05:01.219661
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 20:53:47.731898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.02
 ---- batch: 020 ----
mean loss: 211.64
 ---- batch: 030 ----
mean loss: 215.39
 ---- batch: 040 ----
mean loss: 210.51
 ---- batch: 050 ----
mean loss: 211.04
 ---- batch: 060 ----
mean loss: 223.77
 ---- batch: 070 ----
mean loss: 215.37
 ---- batch: 080 ----
mean loss: 211.85
 ---- batch: 090 ----
mean loss: 215.68
 ---- batch: 100 ----
mean loss: 222.14
 ---- batch: 110 ----
mean loss: 213.88
train mean loss: 215.77
epoch train time: 0:00:02.588179
elapsed time: 0:05:03.808391
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 20:53:50.320564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.94
 ---- batch: 020 ----
mean loss: 223.95
 ---- batch: 030 ----
mean loss: 211.65
 ---- batch: 040 ----
mean loss: 215.59
 ---- batch: 050 ----
mean loss: 214.76
 ---- batch: 060 ----
mean loss: 213.95
 ---- batch: 070 ----
mean loss: 209.09
 ---- batch: 080 ----
mean loss: 220.26
 ---- batch: 090 ----
mean loss: 221.92
 ---- batch: 100 ----
mean loss: 205.99
 ---- batch: 110 ----
mean loss: 216.55
train mean loss: 215.05
epoch train time: 0:00:02.523373
elapsed time: 0:05:06.332133
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 20:53:52.844364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.11
 ---- batch: 020 ----
mean loss: 215.03
 ---- batch: 030 ----
mean loss: 218.75
 ---- batch: 040 ----
mean loss: 210.37
 ---- batch: 050 ----
mean loss: 225.02
 ---- batch: 060 ----
mean loss: 204.82
 ---- batch: 070 ----
mean loss: 221.82
 ---- batch: 080 ----
mean loss: 217.29
 ---- batch: 090 ----
mean loss: 218.44
 ---- batch: 100 ----
mean loss: 202.67
 ---- batch: 110 ----
mean loss: 208.27
train mean loss: 214.32
epoch train time: 0:00:02.544112
elapsed time: 0:05:08.876674
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 20:53:55.388917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.74
 ---- batch: 020 ----
mean loss: 217.59
 ---- batch: 030 ----
mean loss: 203.30
 ---- batch: 040 ----
mean loss: 208.08
 ---- batch: 050 ----
mean loss: 215.97
 ---- batch: 060 ----
mean loss: 214.00
 ---- batch: 070 ----
mean loss: 215.72
 ---- batch: 080 ----
mean loss: 212.44
 ---- batch: 090 ----
mean loss: 219.27
 ---- batch: 100 ----
mean loss: 218.96
 ---- batch: 110 ----
mean loss: 214.59
train mean loss: 213.77
epoch train time: 0:00:02.533299
elapsed time: 0:05:11.410413
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 20:53:57.922667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.70
 ---- batch: 020 ----
mean loss: 203.41
 ---- batch: 030 ----
mean loss: 219.28
 ---- batch: 040 ----
mean loss: 207.59
 ---- batch: 050 ----
mean loss: 216.29
 ---- batch: 060 ----
mean loss: 210.79
 ---- batch: 070 ----
mean loss: 218.42
 ---- batch: 080 ----
mean loss: 213.13
 ---- batch: 090 ----
mean loss: 207.95
 ---- batch: 100 ----
mean loss: 214.69
 ---- batch: 110 ----
mean loss: 217.63
train mean loss: 212.92
epoch train time: 0:00:02.541255
elapsed time: 0:05:13.952161
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 20:54:00.464453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.14
 ---- batch: 020 ----
mean loss: 211.79
 ---- batch: 030 ----
mean loss: 210.60
 ---- batch: 040 ----
mean loss: 201.98
 ---- batch: 050 ----
mean loss: 219.73
 ---- batch: 060 ----
mean loss: 211.95
 ---- batch: 070 ----
mean loss: 212.86
 ---- batch: 080 ----
mean loss: 220.29
 ---- batch: 090 ----
mean loss: 208.44
 ---- batch: 100 ----
mean loss: 207.03
 ---- batch: 110 ----
mean loss: 215.96
train mean loss: 212.27
epoch train time: 0:00:02.536131
elapsed time: 0:05:16.488786
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 20:54:03.001054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.44
 ---- batch: 020 ----
mean loss: 207.84
 ---- batch: 030 ----
mean loss: 213.38
 ---- batch: 040 ----
mean loss: 212.00
 ---- batch: 050 ----
mean loss: 202.99
 ---- batch: 060 ----
mean loss: 213.86
 ---- batch: 070 ----
mean loss: 212.32
 ---- batch: 080 ----
mean loss: 216.27
 ---- batch: 090 ----
mean loss: 221.36
 ---- batch: 100 ----
mean loss: 202.72
 ---- batch: 110 ----
mean loss: 212.31
train mean loss: 211.65
epoch train time: 0:00:02.524938
elapsed time: 0:05:19.014162
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 20:54:05.526405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.49
 ---- batch: 020 ----
mean loss: 201.44
 ---- batch: 030 ----
mean loss: 211.25
 ---- batch: 040 ----
mean loss: 200.68
 ---- batch: 050 ----
mean loss: 213.13
 ---- batch: 060 ----
mean loss: 215.33
 ---- batch: 070 ----
mean loss: 211.97
 ---- batch: 080 ----
mean loss: 216.43
 ---- batch: 090 ----
mean loss: 211.48
 ---- batch: 100 ----
mean loss: 212.98
 ---- batch: 110 ----
mean loss: 214.36
train mean loss: 211.06
epoch train time: 0:00:02.524529
elapsed time: 0:05:21.539114
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 20:54:08.051387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.06
 ---- batch: 020 ----
mean loss: 219.92
 ---- batch: 030 ----
mean loss: 208.69
 ---- batch: 040 ----
mean loss: 212.30
 ---- batch: 050 ----
mean loss: 217.95
 ---- batch: 060 ----
mean loss: 224.87
 ---- batch: 070 ----
mean loss: 210.97
 ---- batch: 080 ----
mean loss: 202.18
 ---- batch: 090 ----
mean loss: 211.51
 ---- batch: 100 ----
mean loss: 205.90
 ---- batch: 110 ----
mean loss: 202.95
train mean loss: 210.95
epoch train time: 0:00:02.524489
elapsed time: 0:05:24.064077
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 20:54:10.576391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.23
 ---- batch: 020 ----
mean loss: 215.02
 ---- batch: 030 ----
mean loss: 211.50
 ---- batch: 040 ----
mean loss: 198.20
 ---- batch: 050 ----
mean loss: 212.85
 ---- batch: 060 ----
mean loss: 212.95
 ---- batch: 070 ----
mean loss: 208.90
 ---- batch: 080 ----
mean loss: 215.73
 ---- batch: 090 ----
mean loss: 211.22
 ---- batch: 100 ----
mean loss: 201.71
 ---- batch: 110 ----
mean loss: 205.22
train mean loss: 210.06
epoch train time: 0:00:02.522920
elapsed time: 0:05:26.587538
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 20:54:13.099773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.00
 ---- batch: 020 ----
mean loss: 209.38
 ---- batch: 030 ----
mean loss: 204.00
 ---- batch: 040 ----
mean loss: 207.53
 ---- batch: 050 ----
mean loss: 208.49
 ---- batch: 060 ----
mean loss: 209.48
 ---- batch: 070 ----
mean loss: 215.82
 ---- batch: 080 ----
mean loss: 215.46
 ---- batch: 090 ----
mean loss: 208.68
 ---- batch: 100 ----
mean loss: 216.03
 ---- batch: 110 ----
mean loss: 205.54
train mean loss: 209.72
epoch train time: 0:00:02.529671
elapsed time: 0:05:29.117637
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 20:54:15.629875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.09
 ---- batch: 020 ----
mean loss: 212.59
 ---- batch: 030 ----
mean loss: 210.07
 ---- batch: 040 ----
mean loss: 206.26
 ---- batch: 050 ----
mean loss: 203.51
 ---- batch: 060 ----
mean loss: 207.97
 ---- batch: 070 ----
mean loss: 209.24
 ---- batch: 080 ----
mean loss: 217.56
 ---- batch: 090 ----
mean loss: 207.45
 ---- batch: 100 ----
mean loss: 209.55
 ---- batch: 110 ----
mean loss: 203.62
train mean loss: 209.21
epoch train time: 0:00:02.529358
elapsed time: 0:05:31.647417
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 20:54:18.159663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.00
 ---- batch: 020 ----
mean loss: 207.46
 ---- batch: 030 ----
mean loss: 212.50
 ---- batch: 040 ----
mean loss: 207.57
 ---- batch: 050 ----
mean loss: 206.82
 ---- batch: 060 ----
mean loss: 203.34
 ---- batch: 070 ----
mean loss: 207.82
 ---- batch: 080 ----
mean loss: 213.64
 ---- batch: 090 ----
mean loss: 212.27
 ---- batch: 100 ----
mean loss: 209.81
 ---- batch: 110 ----
mean loss: 206.73
train mean loss: 208.52
epoch train time: 0:00:02.501896
elapsed time: 0:05:34.149758
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 20:54:20.661995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.46
 ---- batch: 020 ----
mean loss: 204.75
 ---- batch: 030 ----
mean loss: 203.46
 ---- batch: 040 ----
mean loss: 211.58
 ---- batch: 050 ----
mean loss: 209.39
 ---- batch: 060 ----
mean loss: 218.46
 ---- batch: 070 ----
mean loss: 206.82
 ---- batch: 080 ----
mean loss: 207.19
 ---- batch: 090 ----
mean loss: 217.46
 ---- batch: 100 ----
mean loss: 202.72
 ---- batch: 110 ----
mean loss: 208.79
train mean loss: 208.39
epoch train time: 0:00:02.511563
elapsed time: 0:05:36.661744
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 20:54:23.173998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.94
 ---- batch: 020 ----
mean loss: 197.63
 ---- batch: 030 ----
mean loss: 211.96
 ---- batch: 040 ----
mean loss: 208.01
 ---- batch: 050 ----
mean loss: 203.85
 ---- batch: 060 ----
mean loss: 209.15
 ---- batch: 070 ----
mean loss: 211.30
 ---- batch: 080 ----
mean loss: 197.74
 ---- batch: 090 ----
mean loss: 216.21
 ---- batch: 100 ----
mean loss: 207.06
 ---- batch: 110 ----
mean loss: 211.18
train mean loss: 207.81
epoch train time: 0:00:02.523160
elapsed time: 0:05:39.185356
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 20:54:25.697592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.78
 ---- batch: 020 ----
mean loss: 205.56
 ---- batch: 030 ----
mean loss: 205.06
 ---- batch: 040 ----
mean loss: 212.09
 ---- batch: 050 ----
mean loss: 199.90
 ---- batch: 060 ----
mean loss: 206.57
 ---- batch: 070 ----
mean loss: 208.76
 ---- batch: 080 ----
mean loss: 207.86
 ---- batch: 090 ----
mean loss: 205.39
 ---- batch: 100 ----
mean loss: 213.85
 ---- batch: 110 ----
mean loss: 206.47
train mean loss: 207.24
epoch train time: 0:00:02.514925
elapsed time: 0:05:41.700684
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 20:54:28.212905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.54
 ---- batch: 020 ----
mean loss: 211.58
 ---- batch: 030 ----
mean loss: 203.46
 ---- batch: 040 ----
mean loss: 209.00
 ---- batch: 050 ----
mean loss: 212.05
 ---- batch: 060 ----
mean loss: 206.52
 ---- batch: 070 ----
mean loss: 209.76
 ---- batch: 080 ----
mean loss: 211.29
 ---- batch: 090 ----
mean loss: 197.47
 ---- batch: 100 ----
mean loss: 207.57
 ---- batch: 110 ----
mean loss: 200.75
train mean loss: 206.89
epoch train time: 0:00:02.515245
elapsed time: 0:05:44.216321
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 20:54:30.728574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.71
 ---- batch: 020 ----
mean loss: 199.99
 ---- batch: 030 ----
mean loss: 206.09
 ---- batch: 040 ----
mean loss: 209.29
 ---- batch: 050 ----
mean loss: 205.49
 ---- batch: 060 ----
mean loss: 210.69
 ---- batch: 070 ----
mean loss: 206.69
 ---- batch: 080 ----
mean loss: 207.38
 ---- batch: 090 ----
mean loss: 213.26
 ---- batch: 100 ----
mean loss: 205.41
 ---- batch: 110 ----
mean loss: 198.60
train mean loss: 206.48
epoch train time: 0:00:02.526471
elapsed time: 0:05:46.743257
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 20:54:33.255490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.53
 ---- batch: 020 ----
mean loss: 202.74
 ---- batch: 030 ----
mean loss: 207.38
 ---- batch: 040 ----
mean loss: 211.59
 ---- batch: 050 ----
mean loss: 213.91
 ---- batch: 060 ----
mean loss: 208.85
 ---- batch: 070 ----
mean loss: 209.22
 ---- batch: 080 ----
mean loss: 204.35
 ---- batch: 090 ----
mean loss: 207.01
 ---- batch: 100 ----
mean loss: 196.35
 ---- batch: 110 ----
mean loss: 201.31
train mean loss: 206.13
epoch train time: 0:00:02.528610
elapsed time: 0:05:49.272277
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 20:54:35.784501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.27
 ---- batch: 020 ----
mean loss: 203.27
 ---- batch: 030 ----
mean loss: 205.37
 ---- batch: 040 ----
mean loss: 209.38
 ---- batch: 050 ----
mean loss: 198.88
 ---- batch: 060 ----
mean loss: 208.87
 ---- batch: 070 ----
mean loss: 214.08
 ---- batch: 080 ----
mean loss: 210.76
 ---- batch: 090 ----
mean loss: 200.38
 ---- batch: 100 ----
mean loss: 198.57
 ---- batch: 110 ----
mean loss: 209.56
train mean loss: 205.72
epoch train time: 0:00:02.550095
elapsed time: 0:05:51.822760
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 20:54:38.335004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.31
 ---- batch: 020 ----
mean loss: 198.13
 ---- batch: 030 ----
mean loss: 200.69
 ---- batch: 040 ----
mean loss: 208.89
 ---- batch: 050 ----
mean loss: 203.33
 ---- batch: 060 ----
mean loss: 202.03
 ---- batch: 070 ----
mean loss: 204.52
 ---- batch: 080 ----
mean loss: 213.03
 ---- batch: 090 ----
mean loss: 204.19
 ---- batch: 100 ----
mean loss: 207.37
 ---- batch: 110 ----
mean loss: 206.03
train mean loss: 205.31
epoch train time: 0:00:02.514766
elapsed time: 0:05:54.338065
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 20:54:40.850211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.62
 ---- batch: 020 ----
mean loss: 206.81
 ---- batch: 030 ----
mean loss: 205.90
 ---- batch: 040 ----
mean loss: 190.20
 ---- batch: 050 ----
mean loss: 217.13
 ---- batch: 060 ----
mean loss: 205.89
 ---- batch: 070 ----
mean loss: 203.37
 ---- batch: 080 ----
mean loss: 204.86
 ---- batch: 090 ----
mean loss: 204.75
 ---- batch: 100 ----
mean loss: 203.51
 ---- batch: 110 ----
mean loss: 203.26
train mean loss: 204.96
epoch train time: 0:00:02.509050
elapsed time: 0:05:56.847426
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 20:54:43.359655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.58
 ---- batch: 020 ----
mean loss: 198.96
 ---- batch: 030 ----
mean loss: 205.74
 ---- batch: 040 ----
mean loss: 202.16
 ---- batch: 050 ----
mean loss: 213.98
 ---- batch: 060 ----
mean loss: 204.54
 ---- batch: 070 ----
mean loss: 204.21
 ---- batch: 080 ----
mean loss: 206.29
 ---- batch: 090 ----
mean loss: 196.93
 ---- batch: 100 ----
mean loss: 210.39
 ---- batch: 110 ----
mean loss: 195.76
train mean loss: 204.66
epoch train time: 0:00:02.537548
elapsed time: 0:05:59.385415
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 20:54:45.897685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.09
 ---- batch: 020 ----
mean loss: 212.29
 ---- batch: 030 ----
mean loss: 204.51
 ---- batch: 040 ----
mean loss: 203.30
 ---- batch: 050 ----
mean loss: 209.06
 ---- batch: 060 ----
mean loss: 206.99
 ---- batch: 070 ----
mean loss: 197.12
 ---- batch: 080 ----
mean loss: 204.26
 ---- batch: 090 ----
mean loss: 202.14
 ---- batch: 100 ----
mean loss: 200.40
 ---- batch: 110 ----
mean loss: 207.85
train mean loss: 204.30
epoch train time: 0:00:02.507170
elapsed time: 0:06:01.893023
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 20:54:48.405252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.07
 ---- batch: 020 ----
mean loss: 201.59
 ---- batch: 030 ----
mean loss: 206.15
 ---- batch: 040 ----
mean loss: 214.11
 ---- batch: 050 ----
mean loss: 196.42
 ---- batch: 060 ----
mean loss: 201.10
 ---- batch: 070 ----
mean loss: 209.98
 ---- batch: 080 ----
mean loss: 206.14
 ---- batch: 090 ----
mean loss: 207.33
 ---- batch: 100 ----
mean loss: 203.38
 ---- batch: 110 ----
mean loss: 198.80
train mean loss: 203.84
epoch train time: 0:00:02.505666
elapsed time: 0:06:04.399089
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 20:54:50.911326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.41
 ---- batch: 020 ----
mean loss: 204.14
 ---- batch: 030 ----
mean loss: 206.54
 ---- batch: 040 ----
mean loss: 203.56
 ---- batch: 050 ----
mean loss: 203.12
 ---- batch: 060 ----
mean loss: 202.29
 ---- batch: 070 ----
mean loss: 206.29
 ---- batch: 080 ----
mean loss: 202.90
 ---- batch: 090 ----
mean loss: 200.37
 ---- batch: 100 ----
mean loss: 207.93
 ---- batch: 110 ----
mean loss: 199.33
train mean loss: 203.62
epoch train time: 0:00:02.514635
elapsed time: 0:06:06.914118
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 20:54:53.426334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.61
 ---- batch: 020 ----
mean loss: 207.82
 ---- batch: 030 ----
mean loss: 207.38
 ---- batch: 040 ----
mean loss: 205.40
 ---- batch: 050 ----
mean loss: 200.90
 ---- batch: 060 ----
mean loss: 209.48
 ---- batch: 070 ----
mean loss: 206.88
 ---- batch: 080 ----
mean loss: 200.36
 ---- batch: 090 ----
mean loss: 193.05
 ---- batch: 100 ----
mean loss: 197.95
 ---- batch: 110 ----
mean loss: 200.47
train mean loss: 203.22
epoch train time: 0:00:02.536800
elapsed time: 0:06:09.451294
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 20:54:55.963584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.27
 ---- batch: 020 ----
mean loss: 201.97
 ---- batch: 030 ----
mean loss: 205.17
 ---- batch: 040 ----
mean loss: 198.06
 ---- batch: 050 ----
mean loss: 202.09
 ---- batch: 060 ----
mean loss: 197.05
 ---- batch: 070 ----
mean loss: 200.36
 ---- batch: 080 ----
mean loss: 201.76
 ---- batch: 090 ----
mean loss: 210.43
 ---- batch: 100 ----
mean loss: 193.11
 ---- batch: 110 ----
mean loss: 208.65
train mean loss: 203.12
epoch train time: 0:00:02.523562
elapsed time: 0:06:11.975318
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 20:54:58.487613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.43
 ---- batch: 020 ----
mean loss: 203.77
 ---- batch: 030 ----
mean loss: 212.89
 ---- batch: 040 ----
mean loss: 204.73
 ---- batch: 050 ----
mean loss: 199.58
 ---- batch: 060 ----
mean loss: 207.44
 ---- batch: 070 ----
mean loss: 201.41
 ---- batch: 080 ----
mean loss: 195.00
 ---- batch: 090 ----
mean loss: 200.93
 ---- batch: 100 ----
mean loss: 204.14
 ---- batch: 110 ----
mean loss: 198.47
train mean loss: 202.71
epoch train time: 0:00:02.517159
elapsed time: 0:06:14.492949
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 20:55:01.005182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.39
 ---- batch: 020 ----
mean loss: 209.57
 ---- batch: 030 ----
mean loss: 207.27
 ---- batch: 040 ----
mean loss: 205.32
 ---- batch: 050 ----
mean loss: 198.45
 ---- batch: 060 ----
mean loss: 201.83
 ---- batch: 070 ----
mean loss: 198.93
 ---- batch: 080 ----
mean loss: 197.92
 ---- batch: 090 ----
mean loss: 209.07
 ---- batch: 100 ----
mean loss: 197.99
 ---- batch: 110 ----
mean loss: 202.19
train mean loss: 202.56
epoch train time: 0:00:02.512402
elapsed time: 0:06:17.005755
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 20:55:03.517965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.81
 ---- batch: 020 ----
mean loss: 204.79
 ---- batch: 030 ----
mean loss: 203.98
 ---- batch: 040 ----
mean loss: 201.86
 ---- batch: 050 ----
mean loss: 196.50
 ---- batch: 060 ----
mean loss: 195.22
 ---- batch: 070 ----
mean loss: 205.06
 ---- batch: 080 ----
mean loss: 198.00
 ---- batch: 090 ----
mean loss: 205.91
 ---- batch: 100 ----
mean loss: 195.66
 ---- batch: 110 ----
mean loss: 199.40
train mean loss: 202.17
epoch train time: 0:00:02.526405
elapsed time: 0:06:19.532573
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 20:55:06.044807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.01
 ---- batch: 020 ----
mean loss: 206.19
 ---- batch: 030 ----
mean loss: 203.63
 ---- batch: 040 ----
mean loss: 204.93
 ---- batch: 050 ----
mean loss: 204.18
 ---- batch: 060 ----
mean loss: 205.94
 ---- batch: 070 ----
mean loss: 196.66
 ---- batch: 080 ----
mean loss: 201.78
 ---- batch: 090 ----
mean loss: 190.12
 ---- batch: 100 ----
mean loss: 208.44
 ---- batch: 110 ----
mean loss: 205.32
train mean loss: 201.74
epoch train time: 0:00:02.533715
elapsed time: 0:06:22.066684
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 20:55:08.578901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.36
 ---- batch: 020 ----
mean loss: 209.18
 ---- batch: 030 ----
mean loss: 195.42
 ---- batch: 040 ----
mean loss: 210.04
 ---- batch: 050 ----
mean loss: 191.23
 ---- batch: 060 ----
mean loss: 206.54
 ---- batch: 070 ----
mean loss: 193.48
 ---- batch: 080 ----
mean loss: 202.00
 ---- batch: 090 ----
mean loss: 192.74
 ---- batch: 100 ----
mean loss: 206.13
 ---- batch: 110 ----
mean loss: 197.48
train mean loss: 201.39
epoch train time: 0:00:02.519064
elapsed time: 0:06:24.586162
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 20:55:11.098399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.41
 ---- batch: 020 ----
mean loss: 206.79
 ---- batch: 030 ----
mean loss: 202.15
 ---- batch: 040 ----
mean loss: 206.96
 ---- batch: 050 ----
mean loss: 203.84
 ---- batch: 060 ----
mean loss: 204.07
 ---- batch: 070 ----
mean loss: 197.73
 ---- batch: 080 ----
mean loss: 196.25
 ---- batch: 090 ----
mean loss: 201.34
 ---- batch: 100 ----
mean loss: 191.90
 ---- batch: 110 ----
mean loss: 204.54
train mean loss: 201.25
epoch train time: 0:00:02.507841
elapsed time: 0:06:27.094427
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 20:55:13.606682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.86
 ---- batch: 020 ----
mean loss: 201.11
 ---- batch: 030 ----
mean loss: 210.41
 ---- batch: 040 ----
mean loss: 209.32
 ---- batch: 050 ----
mean loss: 202.77
 ---- batch: 060 ----
mean loss: 194.32
 ---- batch: 070 ----
mean loss: 204.08
 ---- batch: 080 ----
mean loss: 202.14
 ---- batch: 090 ----
mean loss: 206.41
 ---- batch: 100 ----
mean loss: 200.28
 ---- batch: 110 ----
mean loss: 194.65
train mean loss: 201.14
epoch train time: 0:00:02.544741
elapsed time: 0:06:29.639591
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 20:55:16.151812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.69
 ---- batch: 020 ----
mean loss: 206.68
 ---- batch: 030 ----
mean loss: 199.10
 ---- batch: 040 ----
mean loss: 198.57
 ---- batch: 050 ----
mean loss: 196.41
 ---- batch: 060 ----
mean loss: 203.70
 ---- batch: 070 ----
mean loss: 198.92
 ---- batch: 080 ----
mean loss: 206.17
 ---- batch: 090 ----
mean loss: 202.62
 ---- batch: 100 ----
mean loss: 197.29
 ---- batch: 110 ----
mean loss: 195.91
train mean loss: 200.83
epoch train time: 0:00:02.525819
elapsed time: 0:06:32.165841
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 20:55:18.678105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.25
 ---- batch: 020 ----
mean loss: 209.41
 ---- batch: 030 ----
mean loss: 198.88
 ---- batch: 040 ----
mean loss: 200.71
 ---- batch: 050 ----
mean loss: 197.59
 ---- batch: 060 ----
mean loss: 198.55
 ---- batch: 070 ----
mean loss: 199.06
 ---- batch: 080 ----
mean loss: 196.29
 ---- batch: 090 ----
mean loss: 193.14
 ---- batch: 100 ----
mean loss: 206.59
 ---- batch: 110 ----
mean loss: 210.09
train mean loss: 200.53
epoch train time: 0:00:02.529087
elapsed time: 0:06:34.695358
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 20:55:21.207585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.74
 ---- batch: 020 ----
mean loss: 195.09
 ---- batch: 030 ----
mean loss: 193.41
 ---- batch: 040 ----
mean loss: 202.29
 ---- batch: 050 ----
mean loss: 209.10
 ---- batch: 060 ----
mean loss: 197.62
 ---- batch: 070 ----
mean loss: 202.42
 ---- batch: 080 ----
mean loss: 207.98
 ---- batch: 090 ----
mean loss: 206.11
 ---- batch: 100 ----
mean loss: 203.00
 ---- batch: 110 ----
mean loss: 187.62
train mean loss: 200.37
epoch train time: 0:00:02.505353
elapsed time: 0:06:37.201136
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 20:55:23.713398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.21
 ---- batch: 020 ----
mean loss: 200.60
 ---- batch: 030 ----
mean loss: 199.98
 ---- batch: 040 ----
mean loss: 207.33
 ---- batch: 050 ----
mean loss: 198.08
 ---- batch: 060 ----
mean loss: 200.70
 ---- batch: 070 ----
mean loss: 193.52
 ---- batch: 080 ----
mean loss: 207.97
 ---- batch: 090 ----
mean loss: 198.79
 ---- batch: 100 ----
mean loss: 200.87
 ---- batch: 110 ----
mean loss: 196.25
train mean loss: 200.02
epoch train time: 0:00:02.545502
elapsed time: 0:06:39.747099
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 20:55:26.259333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.38
 ---- batch: 020 ----
mean loss: 203.41
 ---- batch: 030 ----
mean loss: 209.00
 ---- batch: 040 ----
mean loss: 197.63
 ---- batch: 050 ----
mean loss: 205.75
 ---- batch: 060 ----
mean loss: 202.11
 ---- batch: 070 ----
mean loss: 198.17
 ---- batch: 080 ----
mean loss: 197.14
 ---- batch: 090 ----
mean loss: 197.36
 ---- batch: 100 ----
mean loss: 196.49
 ---- batch: 110 ----
mean loss: 199.39
train mean loss: 199.64
epoch train time: 0:00:02.536529
elapsed time: 0:06:42.284094
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 20:55:28.796379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.65
 ---- batch: 020 ----
mean loss: 203.82
 ---- batch: 030 ----
mean loss: 196.18
 ---- batch: 040 ----
mean loss: 201.63
 ---- batch: 050 ----
mean loss: 195.66
 ---- batch: 060 ----
mean loss: 197.76
 ---- batch: 070 ----
mean loss: 201.57
 ---- batch: 080 ----
mean loss: 195.12
 ---- batch: 090 ----
mean loss: 203.02
 ---- batch: 100 ----
mean loss: 194.33
 ---- batch: 110 ----
mean loss: 199.81
train mean loss: 199.56
epoch train time: 0:00:02.517476
elapsed time: 0:06:44.802046
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 20:55:31.314330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.88
 ---- batch: 020 ----
mean loss: 202.09
 ---- batch: 030 ----
mean loss: 198.44
 ---- batch: 040 ----
mean loss: 198.21
 ---- batch: 050 ----
mean loss: 207.94
 ---- batch: 060 ----
mean loss: 196.41
 ---- batch: 070 ----
mean loss: 198.32
 ---- batch: 080 ----
mean loss: 198.55
 ---- batch: 090 ----
mean loss: 196.05
 ---- batch: 100 ----
mean loss: 207.62
 ---- batch: 110 ----
mean loss: 189.75
train mean loss: 199.07
epoch train time: 0:00:02.553065
elapsed time: 0:06:47.355668
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 20:55:33.867904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.72
 ---- batch: 020 ----
mean loss: 205.46
 ---- batch: 030 ----
mean loss: 204.01
 ---- batch: 040 ----
mean loss: 192.17
 ---- batch: 050 ----
mean loss: 199.66
 ---- batch: 060 ----
mean loss: 189.54
 ---- batch: 070 ----
mean loss: 198.72
 ---- batch: 080 ----
mean loss: 207.29
 ---- batch: 090 ----
mean loss: 201.52
 ---- batch: 100 ----
mean loss: 203.31
 ---- batch: 110 ----
mean loss: 192.17
train mean loss: 199.09
epoch train time: 0:00:02.489961
elapsed time: 0:06:49.846291
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 20:55:36.358371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.28
 ---- batch: 020 ----
mean loss: 193.93
 ---- batch: 030 ----
mean loss: 193.23
 ---- batch: 040 ----
mean loss: 185.36
 ---- batch: 050 ----
mean loss: 200.88
 ---- batch: 060 ----
mean loss: 200.26
 ---- batch: 070 ----
mean loss: 201.58
 ---- batch: 080 ----
mean loss: 203.09
 ---- batch: 090 ----
mean loss: 204.55
 ---- batch: 100 ----
mean loss: 203.39
 ---- batch: 110 ----
mean loss: 195.40
train mean loss: 198.77
epoch train time: 0:00:02.531609
elapsed time: 0:06:52.378213
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 20:55:38.890461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.59
 ---- batch: 020 ----
mean loss: 195.89
 ---- batch: 030 ----
mean loss: 204.68
 ---- batch: 040 ----
mean loss: 192.33
 ---- batch: 050 ----
mean loss: 197.62
 ---- batch: 060 ----
mean loss: 207.72
 ---- batch: 070 ----
mean loss: 192.46
 ---- batch: 080 ----
mean loss: 205.64
 ---- batch: 090 ----
mean loss: 197.46
 ---- batch: 100 ----
mean loss: 194.23
 ---- batch: 110 ----
mean loss: 196.10
train mean loss: 198.55
epoch train time: 0:00:02.544566
elapsed time: 0:06:54.923265
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 20:55:41.435531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.07
 ---- batch: 020 ----
mean loss: 195.89
 ---- batch: 030 ----
mean loss: 201.78
 ---- batch: 040 ----
mean loss: 200.20
 ---- batch: 050 ----
mean loss: 193.12
 ---- batch: 060 ----
mean loss: 200.82
 ---- batch: 070 ----
mean loss: 196.96
 ---- batch: 080 ----
mean loss: 197.66
 ---- batch: 090 ----
mean loss: 199.24
 ---- batch: 100 ----
mean loss: 202.58
 ---- batch: 110 ----
mean loss: 194.39
train mean loss: 198.30
epoch train time: 0:00:02.503684
elapsed time: 0:06:57.427419
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 20:55:43.939669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.76
 ---- batch: 020 ----
mean loss: 195.62
 ---- batch: 030 ----
mean loss: 196.94
 ---- batch: 040 ----
mean loss: 191.48
 ---- batch: 050 ----
mean loss: 200.67
 ---- batch: 060 ----
mean loss: 194.72
 ---- batch: 070 ----
mean loss: 192.91
 ---- batch: 080 ----
mean loss: 206.33
 ---- batch: 090 ----
mean loss: 202.48
 ---- batch: 100 ----
mean loss: 196.66
 ---- batch: 110 ----
mean loss: 193.45
train mean loss: 198.09
epoch train time: 0:00:02.508270
elapsed time: 0:06:59.936121
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 20:55:46.448403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.55
 ---- batch: 020 ----
mean loss: 188.53
 ---- batch: 030 ----
mean loss: 197.76
 ---- batch: 040 ----
mean loss: 196.76
 ---- batch: 050 ----
mean loss: 206.19
 ---- batch: 060 ----
mean loss: 200.44
 ---- batch: 070 ----
mean loss: 201.88
 ---- batch: 080 ----
mean loss: 198.84
 ---- batch: 090 ----
mean loss: 192.26
 ---- batch: 100 ----
mean loss: 194.59
 ---- batch: 110 ----
mean loss: 200.31
train mean loss: 197.75
epoch train time: 0:00:02.544680
elapsed time: 0:07:02.481260
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 20:55:48.993486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.77
 ---- batch: 020 ----
mean loss: 194.41
 ---- batch: 030 ----
mean loss: 198.96
 ---- batch: 040 ----
mean loss: 194.02
 ---- batch: 050 ----
mean loss: 198.66
 ---- batch: 060 ----
mean loss: 195.10
 ---- batch: 070 ----
mean loss: 199.16
 ---- batch: 080 ----
mean loss: 196.33
 ---- batch: 090 ----
mean loss: 203.11
 ---- batch: 100 ----
mean loss: 193.93
 ---- batch: 110 ----
mean loss: 202.54
train mean loss: 197.41
epoch train time: 0:00:02.550374
elapsed time: 0:07:05.032082
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 20:55:51.544384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.18
 ---- batch: 020 ----
mean loss: 195.44
 ---- batch: 030 ----
mean loss: 195.04
 ---- batch: 040 ----
mean loss: 200.21
 ---- batch: 050 ----
mean loss: 191.86
 ---- batch: 060 ----
mean loss: 195.03
 ---- batch: 070 ----
mean loss: 201.82
 ---- batch: 080 ----
mean loss: 194.44
 ---- batch: 090 ----
mean loss: 199.73
 ---- batch: 100 ----
mean loss: 198.12
 ---- batch: 110 ----
mean loss: 202.13
train mean loss: 197.38
epoch train time: 0:00:02.543878
elapsed time: 0:07:07.576431
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 20:55:54.088672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.88
 ---- batch: 020 ----
mean loss: 193.35
 ---- batch: 030 ----
mean loss: 204.57
 ---- batch: 040 ----
mean loss: 206.17
 ---- batch: 050 ----
mean loss: 191.91
 ---- batch: 060 ----
mean loss: 194.92
 ---- batch: 070 ----
mean loss: 199.38
 ---- batch: 080 ----
mean loss: 192.75
 ---- batch: 090 ----
mean loss: 193.07
 ---- batch: 100 ----
mean loss: 200.84
 ---- batch: 110 ----
mean loss: 194.70
train mean loss: 197.24
epoch train time: 0:00:02.556430
elapsed time: 0:07:10.133271
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 20:55:56.645556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.30
 ---- batch: 020 ----
mean loss: 194.69
 ---- batch: 030 ----
mean loss: 200.85
 ---- batch: 040 ----
mean loss: 194.87
 ---- batch: 050 ----
mean loss: 186.10
 ---- batch: 060 ----
mean loss: 203.81
 ---- batch: 070 ----
mean loss: 190.14
 ---- batch: 080 ----
mean loss: 198.32
 ---- batch: 090 ----
mean loss: 198.77
 ---- batch: 100 ----
mean loss: 201.83
 ---- batch: 110 ----
mean loss: 203.08
train mean loss: 197.03
epoch train time: 0:00:02.542900
elapsed time: 0:07:12.676639
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 20:55:59.188874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.60
 ---- batch: 020 ----
mean loss: 195.67
 ---- batch: 030 ----
mean loss: 198.68
 ---- batch: 040 ----
mean loss: 190.99
 ---- batch: 050 ----
mean loss: 195.92
 ---- batch: 060 ----
mean loss: 192.76
 ---- batch: 070 ----
mean loss: 199.45
 ---- batch: 080 ----
mean loss: 191.54
 ---- batch: 090 ----
mean loss: 189.38
 ---- batch: 100 ----
mean loss: 196.87
 ---- batch: 110 ----
mean loss: 206.82
train mean loss: 196.65
epoch train time: 0:00:02.536641
elapsed time: 0:07:15.213731
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 20:56:01.726005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.23
 ---- batch: 020 ----
mean loss: 194.26
 ---- batch: 030 ----
mean loss: 195.31
 ---- batch: 040 ----
mean loss: 203.29
 ---- batch: 050 ----
mean loss: 198.55
 ---- batch: 060 ----
mean loss: 188.52
 ---- batch: 070 ----
mean loss: 201.80
 ---- batch: 080 ----
mean loss: 194.37
 ---- batch: 090 ----
mean loss: 190.69
 ---- batch: 100 ----
mean loss: 204.29
 ---- batch: 110 ----
mean loss: 193.19
train mean loss: 196.65
epoch train time: 0:00:02.544512
elapsed time: 0:07:17.758729
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 20:56:04.271008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.68
 ---- batch: 020 ----
mean loss: 194.01
 ---- batch: 030 ----
mean loss: 188.71
 ---- batch: 040 ----
mean loss: 200.03
 ---- batch: 050 ----
mean loss: 210.43
 ---- batch: 060 ----
mean loss: 194.24
 ---- batch: 070 ----
mean loss: 189.93
 ---- batch: 080 ----
mean loss: 203.39
 ---- batch: 090 ----
mean loss: 201.74
 ---- batch: 100 ----
mean loss: 185.88
 ---- batch: 110 ----
mean loss: 193.80
train mean loss: 196.18
epoch train time: 0:00:02.525566
elapsed time: 0:07:20.284759
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 20:56:06.797007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.08
 ---- batch: 020 ----
mean loss: 197.88
 ---- batch: 030 ----
mean loss: 199.41
 ---- batch: 040 ----
mean loss: 198.72
 ---- batch: 050 ----
mean loss: 207.90
 ---- batch: 060 ----
mean loss: 195.05
 ---- batch: 070 ----
mean loss: 192.94
 ---- batch: 080 ----
mean loss: 190.77
 ---- batch: 090 ----
mean loss: 195.26
 ---- batch: 100 ----
mean loss: 204.83
 ---- batch: 110 ----
mean loss: 194.57
train mean loss: 196.16
epoch train time: 0:00:02.521929
elapsed time: 0:07:22.807128
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 20:56:09.319396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.94
 ---- batch: 020 ----
mean loss: 204.05
 ---- batch: 030 ----
mean loss: 198.12
 ---- batch: 040 ----
mean loss: 189.77
 ---- batch: 050 ----
mean loss: 206.17
 ---- batch: 060 ----
mean loss: 194.18
 ---- batch: 070 ----
mean loss: 188.96
 ---- batch: 080 ----
mean loss: 182.34
 ---- batch: 090 ----
mean loss: 191.82
 ---- batch: 100 ----
mean loss: 198.05
 ---- batch: 110 ----
mean loss: 204.70
train mean loss: 196.02
epoch train time: 0:00:02.541387
elapsed time: 0:07:25.348997
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 20:56:11.861253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.78
 ---- batch: 020 ----
mean loss: 199.99
 ---- batch: 030 ----
mean loss: 200.63
 ---- batch: 040 ----
mean loss: 204.59
 ---- batch: 050 ----
mean loss: 197.06
 ---- batch: 060 ----
mean loss: 194.24
 ---- batch: 070 ----
mean loss: 195.49
 ---- batch: 080 ----
mean loss: 193.66
 ---- batch: 090 ----
mean loss: 189.44
 ---- batch: 100 ----
mean loss: 185.49
 ---- batch: 110 ----
mean loss: 191.88
train mean loss: 195.50
epoch train time: 0:00:02.523971
elapsed time: 0:07:27.873429
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 20:56:14.385756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.50
 ---- batch: 020 ----
mean loss: 201.12
 ---- batch: 030 ----
mean loss: 193.19
 ---- batch: 040 ----
mean loss: 196.22
 ---- batch: 050 ----
mean loss: 200.29
 ---- batch: 060 ----
mean loss: 191.24
 ---- batch: 070 ----
mean loss: 207.81
 ---- batch: 080 ----
mean loss: 190.26
 ---- batch: 090 ----
mean loss: 186.38
 ---- batch: 100 ----
mean loss: 193.52
 ---- batch: 110 ----
mean loss: 195.78
train mean loss: 195.43
epoch train time: 0:00:02.605908
elapsed time: 0:07:30.479847
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 20:56:16.992084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.52
 ---- batch: 020 ----
mean loss: 188.73
 ---- batch: 030 ----
mean loss: 200.57
 ---- batch: 040 ----
mean loss: 196.33
 ---- batch: 050 ----
mean loss: 206.70
 ---- batch: 060 ----
mean loss: 194.10
 ---- batch: 070 ----
mean loss: 193.00
 ---- batch: 080 ----
mean loss: 195.57
 ---- batch: 090 ----
mean loss: 198.84
 ---- batch: 100 ----
mean loss: 187.74
 ---- batch: 110 ----
mean loss: 185.78
train mean loss: 195.39
epoch train time: 0:00:02.594822
elapsed time: 0:07:33.075123
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 20:56:19.587445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.31
 ---- batch: 020 ----
mean loss: 198.19
 ---- batch: 030 ----
mean loss: 193.62
 ---- batch: 040 ----
mean loss: 190.45
 ---- batch: 050 ----
mean loss: 186.35
 ---- batch: 060 ----
mean loss: 194.36
 ---- batch: 070 ----
mean loss: 183.58
 ---- batch: 080 ----
mean loss: 206.05
 ---- batch: 090 ----
mean loss: 198.72
 ---- batch: 100 ----
mean loss: 212.88
 ---- batch: 110 ----
mean loss: 187.18
train mean loss: 195.15
epoch train time: 0:00:02.571958
elapsed time: 0:07:35.647609
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 20:56:22.159838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.92
 ---- batch: 020 ----
mean loss: 193.08
 ---- batch: 030 ----
mean loss: 188.04
 ---- batch: 040 ----
mean loss: 189.20
 ---- batch: 050 ----
mean loss: 195.07
 ---- batch: 060 ----
mean loss: 192.27
 ---- batch: 070 ----
mean loss: 193.42
 ---- batch: 080 ----
mean loss: 192.22
 ---- batch: 090 ----
mean loss: 202.91
 ---- batch: 100 ----
mean loss: 200.78
 ---- batch: 110 ----
mean loss: 199.22
train mean loss: 194.74
epoch train time: 0:00:02.568170
elapsed time: 0:07:38.216211
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 20:56:24.728436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.97
 ---- batch: 020 ----
mean loss: 200.93
 ---- batch: 030 ----
mean loss: 195.48
 ---- batch: 040 ----
mean loss: 204.85
 ---- batch: 050 ----
mean loss: 205.12
 ---- batch: 060 ----
mean loss: 188.82
 ---- batch: 070 ----
mean loss: 185.13
 ---- batch: 080 ----
mean loss: 195.26
 ---- batch: 090 ----
mean loss: 189.17
 ---- batch: 100 ----
mean loss: 189.15
 ---- batch: 110 ----
mean loss: 192.74
train mean loss: 194.72
epoch train time: 0:00:02.546781
elapsed time: 0:07:40.763378
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 20:56:27.275612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.25
 ---- batch: 020 ----
mean loss: 196.72
 ---- batch: 030 ----
mean loss: 183.87
 ---- batch: 040 ----
mean loss: 203.29
 ---- batch: 050 ----
mean loss: 200.10
 ---- batch: 060 ----
mean loss: 196.86
 ---- batch: 070 ----
mean loss: 191.23
 ---- batch: 080 ----
mean loss: 200.05
 ---- batch: 090 ----
mean loss: 192.03
 ---- batch: 100 ----
mean loss: 193.41
 ---- batch: 110 ----
mean loss: 189.33
train mean loss: 194.51
epoch train time: 0:00:02.526469
elapsed time: 0:07:43.290286
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 20:56:29.802550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.37
 ---- batch: 020 ----
mean loss: 199.78
 ---- batch: 030 ----
mean loss: 200.36
 ---- batch: 040 ----
mean loss: 190.13
 ---- batch: 050 ----
mean loss: 191.19
 ---- batch: 060 ----
mean loss: 201.40
 ---- batch: 070 ----
mean loss: 189.06
 ---- batch: 080 ----
mean loss: 194.77
 ---- batch: 090 ----
mean loss: 197.00
 ---- batch: 100 ----
mean loss: 188.26
 ---- batch: 110 ----
mean loss: 189.11
train mean loss: 194.28
epoch train time: 0:00:02.519081
elapsed time: 0:07:45.809797
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 20:56:32.322052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.81
 ---- batch: 020 ----
mean loss: 205.14
 ---- batch: 030 ----
mean loss: 201.25
 ---- batch: 040 ----
mean loss: 191.43
 ---- batch: 050 ----
mean loss: 194.76
 ---- batch: 060 ----
mean loss: 188.21
 ---- batch: 070 ----
mean loss: 200.37
 ---- batch: 080 ----
mean loss: 192.63
 ---- batch: 090 ----
mean loss: 193.86
 ---- batch: 100 ----
mean loss: 193.64
 ---- batch: 110 ----
mean loss: 191.82
train mean loss: 194.06
epoch train time: 0:00:02.516549
elapsed time: 0:07:48.326786
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 20:56:34.839022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.34
 ---- batch: 020 ----
mean loss: 201.82
 ---- batch: 030 ----
mean loss: 191.90
 ---- batch: 040 ----
mean loss: 187.06
 ---- batch: 050 ----
mean loss: 188.70
 ---- batch: 060 ----
mean loss: 197.97
 ---- batch: 070 ----
mean loss: 186.69
 ---- batch: 080 ----
mean loss: 197.27
 ---- batch: 090 ----
mean loss: 198.73
 ---- batch: 100 ----
mean loss: 192.49
 ---- batch: 110 ----
mean loss: 198.81
train mean loss: 193.90
epoch train time: 0:00:02.532650
elapsed time: 0:07:50.859833
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 20:56:37.372091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.09
 ---- batch: 020 ----
mean loss: 191.43
 ---- batch: 030 ----
mean loss: 202.04
 ---- batch: 040 ----
mean loss: 195.46
 ---- batch: 050 ----
mean loss: 185.14
 ---- batch: 060 ----
mean loss: 191.13
 ---- batch: 070 ----
mean loss: 195.13
 ---- batch: 080 ----
mean loss: 192.40
 ---- batch: 090 ----
mean loss: 198.13
 ---- batch: 100 ----
mean loss: 184.49
 ---- batch: 110 ----
mean loss: 199.26
train mean loss: 193.73
epoch train time: 0:00:02.526668
elapsed time: 0:07:53.387159
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 20:56:39.899248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.39
 ---- batch: 020 ----
mean loss: 200.54
 ---- batch: 030 ----
mean loss: 189.91
 ---- batch: 040 ----
mean loss: 195.26
 ---- batch: 050 ----
mean loss: 198.65
 ---- batch: 060 ----
mean loss: 192.46
 ---- batch: 070 ----
mean loss: 189.98
 ---- batch: 080 ----
mean loss: 188.23
 ---- batch: 090 ----
mean loss: 190.67
 ---- batch: 100 ----
mean loss: 195.91
 ---- batch: 110 ----
mean loss: 194.51
train mean loss: 193.45
epoch train time: 0:00:02.515696
elapsed time: 0:07:55.903194
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 20:56:42.415454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.10
 ---- batch: 020 ----
mean loss: 195.50
 ---- batch: 030 ----
mean loss: 191.17
 ---- batch: 040 ----
mean loss: 190.97
 ---- batch: 050 ----
mean loss: 187.85
 ---- batch: 060 ----
mean loss: 194.44
 ---- batch: 070 ----
mean loss: 195.48
 ---- batch: 080 ----
mean loss: 188.41
 ---- batch: 090 ----
mean loss: 198.08
 ---- batch: 100 ----
mean loss: 194.77
 ---- batch: 110 ----
mean loss: 187.69
train mean loss: 193.36
epoch train time: 0:00:02.550147
elapsed time: 0:07:58.453833
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 20:56:44.966100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.76
 ---- batch: 020 ----
mean loss: 192.90
 ---- batch: 030 ----
mean loss: 182.35
 ---- batch: 040 ----
mean loss: 202.97
 ---- batch: 050 ----
mean loss: 197.18
 ---- batch: 060 ----
mean loss: 193.18
 ---- batch: 070 ----
mean loss: 195.92
 ---- batch: 080 ----
mean loss: 193.99
 ---- batch: 090 ----
mean loss: 189.16
 ---- batch: 100 ----
mean loss: 193.23
 ---- batch: 110 ----
mean loss: 191.21
train mean loss: 193.28
epoch train time: 0:00:02.544058
elapsed time: 0:08:00.998335
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 20:56:47.510561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.04
 ---- batch: 020 ----
mean loss: 201.57
 ---- batch: 030 ----
mean loss: 198.12
 ---- batch: 040 ----
mean loss: 183.43
 ---- batch: 050 ----
mean loss: 192.56
 ---- batch: 060 ----
mean loss: 194.01
 ---- batch: 070 ----
mean loss: 182.51
 ---- batch: 080 ----
mean loss: 195.02
 ---- batch: 090 ----
mean loss: 195.40
 ---- batch: 100 ----
mean loss: 191.42
 ---- batch: 110 ----
mean loss: 187.00
train mean loss: 193.02
epoch train time: 0:00:02.519815
elapsed time: 0:08:03.518547
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 20:56:50.030770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.70
 ---- batch: 020 ----
mean loss: 199.69
 ---- batch: 030 ----
mean loss: 196.49
 ---- batch: 040 ----
mean loss: 190.98
 ---- batch: 050 ----
mean loss: 183.83
 ---- batch: 060 ----
mean loss: 194.16
 ---- batch: 070 ----
mean loss: 192.03
 ---- batch: 080 ----
mean loss: 188.69
 ---- batch: 090 ----
mean loss: 197.09
 ---- batch: 100 ----
mean loss: 190.55
 ---- batch: 110 ----
mean loss: 195.91
train mean loss: 192.89
epoch train time: 0:00:02.515642
elapsed time: 0:08:06.034600
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 20:56:52.546858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.83
 ---- batch: 020 ----
mean loss: 187.09
 ---- batch: 030 ----
mean loss: 194.73
 ---- batch: 040 ----
mean loss: 194.88
 ---- batch: 050 ----
mean loss: 199.40
 ---- batch: 060 ----
mean loss: 188.34
 ---- batch: 070 ----
mean loss: 198.78
 ---- batch: 080 ----
mean loss: 191.76
 ---- batch: 090 ----
mean loss: 191.48
 ---- batch: 100 ----
mean loss: 186.92
 ---- batch: 110 ----
mean loss: 200.20
train mean loss: 192.82
epoch train time: 0:00:02.526086
elapsed time: 0:08:08.561106
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 20:56:55.073332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.48
 ---- batch: 020 ----
mean loss: 201.79
 ---- batch: 030 ----
mean loss: 189.36
 ---- batch: 040 ----
mean loss: 185.79
 ---- batch: 050 ----
mean loss: 200.87
 ---- batch: 060 ----
mean loss: 195.95
 ---- batch: 070 ----
mean loss: 193.01
 ---- batch: 080 ----
mean loss: 191.62
 ---- batch: 090 ----
mean loss: 185.51
 ---- batch: 100 ----
mean loss: 190.51
 ---- batch: 110 ----
mean loss: 195.99
train mean loss: 192.52
epoch train time: 0:00:02.519168
elapsed time: 0:08:11.080670
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 20:56:57.592897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.52
 ---- batch: 020 ----
mean loss: 191.57
 ---- batch: 030 ----
mean loss: 206.49
 ---- batch: 040 ----
mean loss: 183.72
 ---- batch: 050 ----
mean loss: 189.50
 ---- batch: 060 ----
mean loss: 184.92
 ---- batch: 070 ----
mean loss: 188.51
 ---- batch: 080 ----
mean loss: 192.44
 ---- batch: 090 ----
mean loss: 193.96
 ---- batch: 100 ----
mean loss: 196.79
 ---- batch: 110 ----
mean loss: 196.56
train mean loss: 192.46
epoch train time: 0:00:02.536832
elapsed time: 0:08:13.617917
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 20:57:00.130238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.63
 ---- batch: 020 ----
mean loss: 196.22
 ---- batch: 030 ----
mean loss: 191.17
 ---- batch: 040 ----
mean loss: 197.02
 ---- batch: 050 ----
mean loss: 190.67
 ---- batch: 060 ----
mean loss: 197.17
 ---- batch: 070 ----
mean loss: 193.20
 ---- batch: 080 ----
mean loss: 187.00
 ---- batch: 090 ----
mean loss: 193.35
 ---- batch: 100 ----
mean loss: 188.23
 ---- batch: 110 ----
mean loss: 184.97
train mean loss: 192.30
epoch train time: 0:00:02.526367
elapsed time: 0:08:16.144800
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 20:57:02.657038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.75
 ---- batch: 020 ----
mean loss: 196.02
 ---- batch: 030 ----
mean loss: 187.02
 ---- batch: 040 ----
mean loss: 195.77
 ---- batch: 050 ----
mean loss: 183.05
 ---- batch: 060 ----
mean loss: 190.69
 ---- batch: 070 ----
mean loss: 190.96
 ---- batch: 080 ----
mean loss: 193.22
 ---- batch: 090 ----
mean loss: 201.69
 ---- batch: 100 ----
mean loss: 194.34
 ---- batch: 110 ----
mean loss: 187.39
train mean loss: 191.87
epoch train time: 0:00:02.531564
elapsed time: 0:08:18.676822
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 20:57:05.189079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.67
 ---- batch: 020 ----
mean loss: 192.96
 ---- batch: 030 ----
mean loss: 195.23
 ---- batch: 040 ----
mean loss: 188.93
 ---- batch: 050 ----
mean loss: 195.36
 ---- batch: 060 ----
mean loss: 198.21
 ---- batch: 070 ----
mean loss: 191.82
 ---- batch: 080 ----
mean loss: 192.99
 ---- batch: 090 ----
mean loss: 184.35
 ---- batch: 100 ----
mean loss: 187.80
 ---- batch: 110 ----
mean loss: 196.68
train mean loss: 191.82
epoch train time: 0:00:02.529738
elapsed time: 0:08:21.206991
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 20:57:07.719221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.83
 ---- batch: 020 ----
mean loss: 180.61
 ---- batch: 030 ----
mean loss: 197.15
 ---- batch: 040 ----
mean loss: 189.80
 ---- batch: 050 ----
mean loss: 197.82
 ---- batch: 060 ----
mean loss: 193.40
 ---- batch: 070 ----
mean loss: 196.35
 ---- batch: 080 ----
mean loss: 184.93
 ---- batch: 090 ----
mean loss: 190.02
 ---- batch: 100 ----
mean loss: 196.59
 ---- batch: 110 ----
mean loss: 194.53
train mean loss: 191.89
epoch train time: 0:00:02.567103
elapsed time: 0:08:23.774553
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 20:57:10.286867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.07
 ---- batch: 020 ----
mean loss: 200.43
 ---- batch: 030 ----
mean loss: 195.60
 ---- batch: 040 ----
mean loss: 189.43
 ---- batch: 050 ----
mean loss: 191.27
 ---- batch: 060 ----
mean loss: 190.38
 ---- batch: 070 ----
mean loss: 191.46
 ---- batch: 080 ----
mean loss: 188.41
 ---- batch: 090 ----
mean loss: 185.16
 ---- batch: 100 ----
mean loss: 186.12
 ---- batch: 110 ----
mean loss: 190.36
train mean loss: 191.58
epoch train time: 0:00:02.535316
elapsed time: 0:08:26.310372
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 20:57:12.822644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.60
 ---- batch: 020 ----
mean loss: 190.53
 ---- batch: 030 ----
mean loss: 192.20
 ---- batch: 040 ----
mean loss: 193.79
 ---- batch: 050 ----
mean loss: 194.35
 ---- batch: 060 ----
mean loss: 200.67
 ---- batch: 070 ----
mean loss: 185.91
 ---- batch: 080 ----
mean loss: 185.81
 ---- batch: 090 ----
mean loss: 196.43
 ---- batch: 100 ----
mean loss: 193.35
 ---- batch: 110 ----
mean loss: 190.11
train mean loss: 191.50
epoch train time: 0:00:02.523868
elapsed time: 0:08:28.834668
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 20:57:15.346892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.20
 ---- batch: 020 ----
mean loss: 193.08
 ---- batch: 030 ----
mean loss: 200.66
 ---- batch: 040 ----
mean loss: 183.40
 ---- batch: 050 ----
mean loss: 191.28
 ---- batch: 060 ----
mean loss: 188.87
 ---- batch: 070 ----
mean loss: 191.86
 ---- batch: 080 ----
mean loss: 184.42
 ---- batch: 090 ----
mean loss: 195.06
 ---- batch: 100 ----
mean loss: 194.14
 ---- batch: 110 ----
mean loss: 186.33
train mean loss: 191.18
epoch train time: 0:00:02.546119
elapsed time: 0:08:31.381202
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 20:57:17.893437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.65
 ---- batch: 020 ----
mean loss: 191.34
 ---- batch: 030 ----
mean loss: 202.92
 ---- batch: 040 ----
mean loss: 192.79
 ---- batch: 050 ----
mean loss: 189.67
 ---- batch: 060 ----
mean loss: 191.47
 ---- batch: 070 ----
mean loss: 188.81
 ---- batch: 080 ----
mean loss: 197.43
 ---- batch: 090 ----
mean loss: 190.66
 ---- batch: 100 ----
mean loss: 183.85
 ---- batch: 110 ----
mean loss: 186.37
train mean loss: 191.17
epoch train time: 0:00:02.521612
elapsed time: 0:08:33.903287
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 20:57:20.415570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.75
 ---- batch: 020 ----
mean loss: 190.67
 ---- batch: 030 ----
mean loss: 198.78
 ---- batch: 040 ----
mean loss: 186.79
 ---- batch: 050 ----
mean loss: 184.76
 ---- batch: 060 ----
mean loss: 195.19
 ---- batch: 070 ----
mean loss: 195.16
 ---- batch: 080 ----
mean loss: 190.99
 ---- batch: 090 ----
mean loss: 186.29
 ---- batch: 100 ----
mean loss: 193.26
 ---- batch: 110 ----
mean loss: 197.53
train mean loss: 191.02
epoch train time: 0:00:02.537667
elapsed time: 0:08:36.441406
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 20:57:22.953636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.07
 ---- batch: 020 ----
mean loss: 183.80
 ---- batch: 030 ----
mean loss: 189.85
 ---- batch: 040 ----
mean loss: 190.78
 ---- batch: 050 ----
mean loss: 196.30
 ---- batch: 060 ----
mean loss: 195.40
 ---- batch: 070 ----
mean loss: 187.45
 ---- batch: 080 ----
mean loss: 187.76
 ---- batch: 090 ----
mean loss: 194.41
 ---- batch: 100 ----
mean loss: 194.04
 ---- batch: 110 ----
mean loss: 186.09
train mean loss: 190.82
epoch train time: 0:00:02.523271
elapsed time: 0:08:38.965057
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 20:57:25.477290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.08
 ---- batch: 020 ----
mean loss: 187.75
 ---- batch: 030 ----
mean loss: 188.85
 ---- batch: 040 ----
mean loss: 194.35
 ---- batch: 050 ----
mean loss: 194.38
 ---- batch: 060 ----
mean loss: 200.86
 ---- batch: 070 ----
mean loss: 183.22
 ---- batch: 080 ----
mean loss: 190.18
 ---- batch: 090 ----
mean loss: 186.32
 ---- batch: 100 ----
mean loss: 182.73
 ---- batch: 110 ----
mean loss: 191.38
train mean loss: 190.59
epoch train time: 0:00:02.539018
elapsed time: 0:08:41.504489
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 20:57:28.016742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.42
 ---- batch: 020 ----
mean loss: 194.05
 ---- batch: 030 ----
mean loss: 190.19
 ---- batch: 040 ----
mean loss: 193.66
 ---- batch: 050 ----
mean loss: 196.46
 ---- batch: 060 ----
mean loss: 183.57
 ---- batch: 070 ----
mean loss: 182.96
 ---- batch: 080 ----
mean loss: 188.13
 ---- batch: 090 ----
mean loss: 183.83
 ---- batch: 100 ----
mean loss: 196.74
 ---- batch: 110 ----
mean loss: 196.91
train mean loss: 190.48
epoch train time: 0:00:02.553500
elapsed time: 0:08:44.058420
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 20:57:30.570651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.97
 ---- batch: 020 ----
mean loss: 203.72
 ---- batch: 030 ----
mean loss: 192.71
 ---- batch: 040 ----
mean loss: 192.53
 ---- batch: 050 ----
mean loss: 180.99
 ---- batch: 060 ----
mean loss: 186.44
 ---- batch: 070 ----
mean loss: 195.63
 ---- batch: 080 ----
mean loss: 187.46
 ---- batch: 090 ----
mean loss: 195.38
 ---- batch: 100 ----
mean loss: 196.06
 ---- batch: 110 ----
mean loss: 180.29
train mean loss: 190.24
epoch train time: 0:00:02.536754
elapsed time: 0:08:46.595610
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 20:57:33.107847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.93
 ---- batch: 020 ----
mean loss: 197.07
 ---- batch: 030 ----
mean loss: 197.20
 ---- batch: 040 ----
mean loss: 194.40
 ---- batch: 050 ----
mean loss: 182.83
 ---- batch: 060 ----
mean loss: 184.34
 ---- batch: 070 ----
mean loss: 190.10
 ---- batch: 080 ----
mean loss: 186.87
 ---- batch: 090 ----
mean loss: 193.56
 ---- batch: 100 ----
mean loss: 188.18
 ---- batch: 110 ----
mean loss: 194.10
train mean loss: 190.17
epoch train time: 0:00:02.532569
elapsed time: 0:08:49.128607
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 20:57:35.640841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.32
 ---- batch: 020 ----
mean loss: 193.12
 ---- batch: 030 ----
mean loss: 184.57
 ---- batch: 040 ----
mean loss: 198.56
 ---- batch: 050 ----
mean loss: 185.16
 ---- batch: 060 ----
mean loss: 186.83
 ---- batch: 070 ----
mean loss: 192.20
 ---- batch: 080 ----
mean loss: 190.34
 ---- batch: 090 ----
mean loss: 185.90
 ---- batch: 100 ----
mean loss: 180.30
 ---- batch: 110 ----
mean loss: 197.68
train mean loss: 189.97
epoch train time: 0:00:02.527677
elapsed time: 0:08:51.656689
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 20:57:38.168915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.16
 ---- batch: 020 ----
mean loss: 185.65
 ---- batch: 030 ----
mean loss: 193.60
 ---- batch: 040 ----
mean loss: 179.01
 ---- batch: 050 ----
mean loss: 196.36
 ---- batch: 060 ----
mean loss: 183.59
 ---- batch: 070 ----
mean loss: 204.18
 ---- batch: 080 ----
mean loss: 194.51
 ---- batch: 090 ----
mean loss: 191.18
 ---- batch: 100 ----
mean loss: 186.71
 ---- batch: 110 ----
mean loss: 184.47
train mean loss: 189.86
epoch train time: 0:00:02.535286
elapsed time: 0:08:54.192399
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 20:57:40.704658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.28
 ---- batch: 020 ----
mean loss: 187.68
 ---- batch: 030 ----
mean loss: 186.99
 ---- batch: 040 ----
mean loss: 188.45
 ---- batch: 050 ----
mean loss: 186.64
 ---- batch: 060 ----
mean loss: 195.81
 ---- batch: 070 ----
mean loss: 182.58
 ---- batch: 080 ----
mean loss: 186.05
 ---- batch: 090 ----
mean loss: 189.64
 ---- batch: 100 ----
mean loss: 183.83
 ---- batch: 110 ----
mean loss: 196.38
train mean loss: 189.70
epoch train time: 0:00:02.537084
elapsed time: 0:08:56.730075
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 20:57:43.242347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.09
 ---- batch: 020 ----
mean loss: 182.98
 ---- batch: 030 ----
mean loss: 196.54
 ---- batch: 040 ----
mean loss: 184.87
 ---- batch: 050 ----
mean loss: 188.95
 ---- batch: 060 ----
mean loss: 203.65
 ---- batch: 070 ----
mean loss: 189.33
 ---- batch: 080 ----
mean loss: 191.28
 ---- batch: 090 ----
mean loss: 180.53
 ---- batch: 100 ----
mean loss: 185.18
 ---- batch: 110 ----
mean loss: 190.47
train mean loss: 189.55
epoch train time: 0:00:02.517316
elapsed time: 0:08:59.247839
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 20:57:45.760076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.95
 ---- batch: 020 ----
mean loss: 185.30
 ---- batch: 030 ----
mean loss: 189.74
 ---- batch: 040 ----
mean loss: 183.67
 ---- batch: 050 ----
mean loss: 192.30
 ---- batch: 060 ----
mean loss: 193.98
 ---- batch: 070 ----
mean loss: 190.19
 ---- batch: 080 ----
mean loss: 182.83
 ---- batch: 090 ----
mean loss: 187.06
 ---- batch: 100 ----
mean loss: 193.04
 ---- batch: 110 ----
mean loss: 195.17
train mean loss: 189.56
epoch train time: 0:00:02.529698
elapsed time: 0:09:01.777959
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 20:57:48.290193
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.49
 ---- batch: 020 ----
mean loss: 188.38
 ---- batch: 030 ----
mean loss: 194.90
 ---- batch: 040 ----
mean loss: 191.62
 ---- batch: 050 ----
mean loss: 184.03
 ---- batch: 060 ----
mean loss: 190.35
 ---- batch: 070 ----
mean loss: 181.90
 ---- batch: 080 ----
mean loss: 188.60
 ---- batch: 090 ----
mean loss: 191.16
 ---- batch: 100 ----
mean loss: 184.53
 ---- batch: 110 ----
mean loss: 189.01
train mean loss: 188.98
epoch train time: 0:00:02.535341
elapsed time: 0:09:04.313917
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 20:57:50.825980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.95
 ---- batch: 020 ----
mean loss: 183.72
 ---- batch: 030 ----
mean loss: 182.70
 ---- batch: 040 ----
mean loss: 192.19
 ---- batch: 050 ----
mean loss: 190.12
 ---- batch: 060 ----
mean loss: 188.77
 ---- batch: 070 ----
mean loss: 183.58
 ---- batch: 080 ----
mean loss: 197.52
 ---- batch: 090 ----
mean loss: 193.56
 ---- batch: 100 ----
mean loss: 188.71
 ---- batch: 110 ----
mean loss: 183.64
train mean loss: 189.00
epoch train time: 0:00:02.538350
elapsed time: 0:09:06.852516
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 20:57:53.364743
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.42
 ---- batch: 020 ----
mean loss: 190.83
 ---- batch: 030 ----
mean loss: 189.72
 ---- batch: 040 ----
mean loss: 190.76
 ---- batch: 050 ----
mean loss: 193.54
 ---- batch: 060 ----
mean loss: 188.38
 ---- batch: 070 ----
mean loss: 185.80
 ---- batch: 080 ----
mean loss: 194.96
 ---- batch: 090 ----
mean loss: 188.19
 ---- batch: 100 ----
mean loss: 185.87
 ---- batch: 110 ----
mean loss: 182.20
train mean loss: 188.91
epoch train time: 0:00:02.524492
elapsed time: 0:09:09.377399
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 20:57:55.889641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.12
 ---- batch: 020 ----
mean loss: 188.91
 ---- batch: 030 ----
mean loss: 181.63
 ---- batch: 040 ----
mean loss: 188.41
 ---- batch: 050 ----
mean loss: 193.40
 ---- batch: 060 ----
mean loss: 189.76
 ---- batch: 070 ----
mean loss: 193.51
 ---- batch: 080 ----
mean loss: 192.01
 ---- batch: 090 ----
mean loss: 192.06
 ---- batch: 100 ----
mean loss: 184.34
 ---- batch: 110 ----
mean loss: 186.79
train mean loss: 188.88
epoch train time: 0:00:02.518925
elapsed time: 0:09:11.896729
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 20:57:58.408961
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.31
 ---- batch: 020 ----
mean loss: 184.89
 ---- batch: 030 ----
mean loss: 189.97
 ---- batch: 040 ----
mean loss: 185.73
 ---- batch: 050 ----
mean loss: 191.44
 ---- batch: 060 ----
mean loss: 191.80
 ---- batch: 070 ----
mean loss: 191.15
 ---- batch: 080 ----
mean loss: 194.01
 ---- batch: 090 ----
mean loss: 184.47
 ---- batch: 100 ----
mean loss: 181.31
 ---- batch: 110 ----
mean loss: 194.05
train mean loss: 188.90
epoch train time: 0:00:02.528453
elapsed time: 0:09:14.425594
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 20:58:00.937867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.83
 ---- batch: 020 ----
mean loss: 188.30
 ---- batch: 030 ----
mean loss: 187.04
 ---- batch: 040 ----
mean loss: 192.72
 ---- batch: 050 ----
mean loss: 183.60
 ---- batch: 060 ----
mean loss: 191.40
 ---- batch: 070 ----
mean loss: 192.38
 ---- batch: 080 ----
mean loss: 190.42
 ---- batch: 090 ----
mean loss: 188.21
 ---- batch: 100 ----
mean loss: 179.28
 ---- batch: 110 ----
mean loss: 194.46
train mean loss: 188.94
epoch train time: 0:00:02.531279
elapsed time: 0:09:16.957308
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 20:58:03.469658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.05
 ---- batch: 020 ----
mean loss: 189.78
 ---- batch: 030 ----
mean loss: 191.65
 ---- batch: 040 ----
mean loss: 198.90
 ---- batch: 050 ----
mean loss: 178.61
 ---- batch: 060 ----
mean loss: 191.24
 ---- batch: 070 ----
mean loss: 180.43
 ---- batch: 080 ----
mean loss: 194.46
 ---- batch: 090 ----
mean loss: 185.70
 ---- batch: 100 ----
mean loss: 195.56
 ---- batch: 110 ----
mean loss: 184.54
train mean loss: 188.94
epoch train time: 0:00:02.552475
elapsed time: 0:09:19.510342
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 20:58:06.022591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.71
 ---- batch: 020 ----
mean loss: 189.49
 ---- batch: 030 ----
mean loss: 183.71
 ---- batch: 040 ----
mean loss: 183.84
 ---- batch: 050 ----
mean loss: 190.04
 ---- batch: 060 ----
mean loss: 195.14
 ---- batch: 070 ----
mean loss: 194.14
 ---- batch: 080 ----
mean loss: 195.09
 ---- batch: 090 ----
mean loss: 185.91
 ---- batch: 100 ----
mean loss: 183.36
 ---- batch: 110 ----
mean loss: 191.77
train mean loss: 188.93
epoch train time: 0:00:02.537483
elapsed time: 0:09:22.048263
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 20:58:08.560500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.86
 ---- batch: 020 ----
mean loss: 172.08
 ---- batch: 030 ----
mean loss: 193.78
 ---- batch: 040 ----
mean loss: 183.02
 ---- batch: 050 ----
mean loss: 186.61
 ---- batch: 060 ----
mean loss: 196.65
 ---- batch: 070 ----
mean loss: 190.38
 ---- batch: 080 ----
mean loss: 186.94
 ---- batch: 090 ----
mean loss: 188.20
 ---- batch: 100 ----
mean loss: 187.78
 ---- batch: 110 ----
mean loss: 202.39
train mean loss: 188.87
epoch train time: 0:00:02.528024
elapsed time: 0:09:24.576695
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 20:58:11.088920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.96
 ---- batch: 020 ----
mean loss: 184.28
 ---- batch: 030 ----
mean loss: 192.11
 ---- batch: 040 ----
mean loss: 192.44
 ---- batch: 050 ----
mean loss: 192.46
 ---- batch: 060 ----
mean loss: 194.90
 ---- batch: 070 ----
mean loss: 181.17
 ---- batch: 080 ----
mean loss: 185.86
 ---- batch: 090 ----
mean loss: 181.90
 ---- batch: 100 ----
mean loss: 183.66
 ---- batch: 110 ----
mean loss: 187.97
train mean loss: 188.82
epoch train time: 0:00:02.531809
elapsed time: 0:09:27.108903
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 20:58:13.621137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.69
 ---- batch: 020 ----
mean loss: 194.27
 ---- batch: 030 ----
mean loss: 188.45
 ---- batch: 040 ----
mean loss: 191.18
 ---- batch: 050 ----
mean loss: 194.37
 ---- batch: 060 ----
mean loss: 189.47
 ---- batch: 070 ----
mean loss: 183.10
 ---- batch: 080 ----
mean loss: 182.47
 ---- batch: 090 ----
mean loss: 194.17
 ---- batch: 100 ----
mean loss: 181.09
 ---- batch: 110 ----
mean loss: 186.09
train mean loss: 188.87
epoch train time: 0:00:02.503384
elapsed time: 0:09:29.612689
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 20:58:16.124939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.41
 ---- batch: 020 ----
mean loss: 185.23
 ---- batch: 030 ----
mean loss: 189.50
 ---- batch: 040 ----
mean loss: 193.02
 ---- batch: 050 ----
mean loss: 194.62
 ---- batch: 060 ----
mean loss: 191.87
 ---- batch: 070 ----
mean loss: 190.38
 ---- batch: 080 ----
mean loss: 186.16
 ---- batch: 090 ----
mean loss: 189.05
 ---- batch: 100 ----
mean loss: 186.83
 ---- batch: 110 ----
mean loss: 186.94
train mean loss: 188.77
epoch train time: 0:00:02.531791
elapsed time: 0:09:32.144942
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 20:58:18.657195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.33
 ---- batch: 020 ----
mean loss: 183.52
 ---- batch: 030 ----
mean loss: 179.43
 ---- batch: 040 ----
mean loss: 195.02
 ---- batch: 050 ----
mean loss: 189.64
 ---- batch: 060 ----
mean loss: 194.92
 ---- batch: 070 ----
mean loss: 181.26
 ---- batch: 080 ----
mean loss: 188.19
 ---- batch: 090 ----
mean loss: 182.37
 ---- batch: 100 ----
mean loss: 193.70
 ---- batch: 110 ----
mean loss: 198.76
train mean loss: 188.80
epoch train time: 0:00:02.531070
elapsed time: 0:09:34.676467
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 20:58:21.188776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.19
 ---- batch: 020 ----
mean loss: 189.57
 ---- batch: 030 ----
mean loss: 189.37
 ---- batch: 040 ----
mean loss: 181.84
 ---- batch: 050 ----
mean loss: 194.45
 ---- batch: 060 ----
mean loss: 184.36
 ---- batch: 070 ----
mean loss: 196.47
 ---- batch: 080 ----
mean loss: 188.40
 ---- batch: 090 ----
mean loss: 185.78
 ---- batch: 100 ----
mean loss: 189.81
 ---- batch: 110 ----
mean loss: 193.74
train mean loss: 188.81
epoch train time: 0:00:02.490807
elapsed time: 0:09:37.167773
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 20:58:23.680082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.51
 ---- batch: 020 ----
mean loss: 194.13
 ---- batch: 030 ----
mean loss: 192.45
 ---- batch: 040 ----
mean loss: 195.19
 ---- batch: 050 ----
mean loss: 184.51
 ---- batch: 060 ----
mean loss: 182.25
 ---- batch: 070 ----
mean loss: 197.43
 ---- batch: 080 ----
mean loss: 180.19
 ---- batch: 090 ----
mean loss: 177.42
 ---- batch: 100 ----
mean loss: 186.82
 ---- batch: 110 ----
mean loss: 187.93
train mean loss: 188.74
epoch train time: 0:00:02.559303
elapsed time: 0:09:39.727566
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 20:58:26.239796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.79
 ---- batch: 020 ----
mean loss: 190.50
 ---- batch: 030 ----
mean loss: 188.83
 ---- batch: 040 ----
mean loss: 189.05
 ---- batch: 050 ----
mean loss: 186.55
 ---- batch: 060 ----
mean loss: 185.95
 ---- batch: 070 ----
mean loss: 184.68
 ---- batch: 080 ----
mean loss: 192.55
 ---- batch: 090 ----
mean loss: 192.24
 ---- batch: 100 ----
mean loss: 189.37
 ---- batch: 110 ----
mean loss: 184.74
train mean loss: 188.76
epoch train time: 0:00:02.543496
elapsed time: 0:09:42.271572
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 20:58:28.783853
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.67
 ---- batch: 020 ----
mean loss: 184.51
 ---- batch: 030 ----
mean loss: 189.64
 ---- batch: 040 ----
mean loss: 186.35
 ---- batch: 050 ----
mean loss: 186.01
 ---- batch: 060 ----
mean loss: 192.25
 ---- batch: 070 ----
mean loss: 185.95
 ---- batch: 080 ----
mean loss: 191.36
 ---- batch: 090 ----
mean loss: 188.98
 ---- batch: 100 ----
mean loss: 195.91
 ---- batch: 110 ----
mean loss: 192.03
train mean loss: 188.81
epoch train time: 0:00:02.528937
elapsed time: 0:09:44.801021
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 20:58:31.313270
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.91
 ---- batch: 020 ----
mean loss: 191.85
 ---- batch: 030 ----
mean loss: 197.67
 ---- batch: 040 ----
mean loss: 191.82
 ---- batch: 050 ----
mean loss: 186.99
 ---- batch: 060 ----
mean loss: 192.83
 ---- batch: 070 ----
mean loss: 189.06
 ---- batch: 080 ----
mean loss: 176.44
 ---- batch: 090 ----
mean loss: 188.35
 ---- batch: 100 ----
mean loss: 189.22
 ---- batch: 110 ----
mean loss: 184.24
train mean loss: 188.68
epoch train time: 0:00:02.530787
elapsed time: 0:09:47.332257
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 20:58:33.844523
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.56
 ---- batch: 020 ----
mean loss: 192.82
 ---- batch: 030 ----
mean loss: 183.89
 ---- batch: 040 ----
mean loss: 187.42
 ---- batch: 050 ----
mean loss: 188.26
 ---- batch: 060 ----
mean loss: 181.79
 ---- batch: 070 ----
mean loss: 186.00
 ---- batch: 080 ----
mean loss: 197.21
 ---- batch: 090 ----
mean loss: 188.68
 ---- batch: 100 ----
mean loss: 190.83
 ---- batch: 110 ----
mean loss: 191.51
train mean loss: 188.75
epoch train time: 0:00:02.519151
elapsed time: 0:09:49.851894
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 20:58:36.364177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.19
 ---- batch: 020 ----
mean loss: 194.15
 ---- batch: 030 ----
mean loss: 188.99
 ---- batch: 040 ----
mean loss: 186.95
 ---- batch: 050 ----
mean loss: 192.50
 ---- batch: 060 ----
mean loss: 186.28
 ---- batch: 070 ----
mean loss: 190.90
 ---- batch: 080 ----
mean loss: 190.32
 ---- batch: 090 ----
mean loss: 187.70
 ---- batch: 100 ----
mean loss: 193.57
 ---- batch: 110 ----
mean loss: 185.06
train mean loss: 188.71
epoch train time: 0:00:02.525229
elapsed time: 0:09:52.377589
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 20:58:38.889867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.12
 ---- batch: 020 ----
mean loss: 180.54
 ---- batch: 030 ----
mean loss: 194.42
 ---- batch: 040 ----
mean loss: 197.15
 ---- batch: 050 ----
mean loss: 179.33
 ---- batch: 060 ----
mean loss: 188.13
 ---- batch: 070 ----
mean loss: 193.90
 ---- batch: 080 ----
mean loss: 194.84
 ---- batch: 090 ----
mean loss: 187.46
 ---- batch: 100 ----
mean loss: 187.10
 ---- batch: 110 ----
mean loss: 188.76
train mean loss: 188.60
epoch train time: 0:00:02.517224
elapsed time: 0:09:54.895332
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 20:58:41.407561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.61
 ---- batch: 020 ----
mean loss: 194.61
 ---- batch: 030 ----
mean loss: 200.30
 ---- batch: 040 ----
mean loss: 182.59
 ---- batch: 050 ----
mean loss: 185.83
 ---- batch: 060 ----
mean loss: 185.73
 ---- batch: 070 ----
mean loss: 187.04
 ---- batch: 080 ----
mean loss: 184.44
 ---- batch: 090 ----
mean loss: 184.56
 ---- batch: 100 ----
mean loss: 191.46
 ---- batch: 110 ----
mean loss: 183.16
train mean loss: 188.69
epoch train time: 0:00:02.536440
elapsed time: 0:09:57.432237
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 20:58:43.944477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.93
 ---- batch: 020 ----
mean loss: 186.92
 ---- batch: 030 ----
mean loss: 186.10
 ---- batch: 040 ----
mean loss: 193.77
 ---- batch: 050 ----
mean loss: 194.12
 ---- batch: 060 ----
mean loss: 185.01
 ---- batch: 070 ----
mean loss: 187.56
 ---- batch: 080 ----
mean loss: 183.98
 ---- batch: 090 ----
mean loss: 190.59
 ---- batch: 100 ----
mean loss: 191.88
 ---- batch: 110 ----
mean loss: 187.98
train mean loss: 188.66
epoch train time: 0:00:02.536572
elapsed time: 0:09:59.969231
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 20:58:46.481497
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.27
 ---- batch: 020 ----
mean loss: 191.64
 ---- batch: 030 ----
mean loss: 184.96
 ---- batch: 040 ----
mean loss: 190.14
 ---- batch: 050 ----
mean loss: 185.78
 ---- batch: 060 ----
mean loss: 198.07
 ---- batch: 070 ----
mean loss: 190.15
 ---- batch: 080 ----
mean loss: 194.70
 ---- batch: 090 ----
mean loss: 191.96
 ---- batch: 100 ----
mean loss: 176.09
 ---- batch: 110 ----
mean loss: 185.62
train mean loss: 188.62
epoch train time: 0:00:02.535273
elapsed time: 0:10:02.504935
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 20:58:49.017253
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.17
 ---- batch: 020 ----
mean loss: 184.78
 ---- batch: 030 ----
mean loss: 192.96
 ---- batch: 040 ----
mean loss: 194.47
 ---- batch: 050 ----
mean loss: 180.30
 ---- batch: 060 ----
mean loss: 191.61
 ---- batch: 070 ----
mean loss: 180.63
 ---- batch: 080 ----
mean loss: 182.62
 ---- batch: 090 ----
mean loss: 193.27
 ---- batch: 100 ----
mean loss: 196.34
 ---- batch: 110 ----
mean loss: 184.98
train mean loss: 188.65
epoch train time: 0:00:02.516834
elapsed time: 0:10:05.022256
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 20:58:51.534476
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.06
 ---- batch: 020 ----
mean loss: 194.03
 ---- batch: 030 ----
mean loss: 189.59
 ---- batch: 040 ----
mean loss: 185.64
 ---- batch: 050 ----
mean loss: 191.11
 ---- batch: 060 ----
mean loss: 177.96
 ---- batch: 070 ----
mean loss: 192.64
 ---- batch: 080 ----
mean loss: 184.24
 ---- batch: 090 ----
mean loss: 194.64
 ---- batch: 100 ----
mean loss: 193.30
 ---- batch: 110 ----
mean loss: 192.35
train mean loss: 188.61
epoch train time: 0:00:02.568767
elapsed time: 0:10:07.591422
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 20:58:54.103659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.90
 ---- batch: 020 ----
mean loss: 188.89
 ---- batch: 030 ----
mean loss: 194.66
 ---- batch: 040 ----
mean loss: 181.50
 ---- batch: 050 ----
mean loss: 185.41
 ---- batch: 060 ----
mean loss: 191.43
 ---- batch: 070 ----
mean loss: 196.90
 ---- batch: 080 ----
mean loss: 196.53
 ---- batch: 090 ----
mean loss: 188.59
 ---- batch: 100 ----
mean loss: 191.08
 ---- batch: 110 ----
mean loss: 184.62
train mean loss: 188.59
epoch train time: 0:00:02.563937
elapsed time: 0:10:10.155788
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 20:58:56.668037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.86
 ---- batch: 020 ----
mean loss: 184.42
 ---- batch: 030 ----
mean loss: 185.81
 ---- batch: 040 ----
mean loss: 189.93
 ---- batch: 050 ----
mean loss: 190.33
 ---- batch: 060 ----
mean loss: 189.62
 ---- batch: 070 ----
mean loss: 195.75
 ---- batch: 080 ----
mean loss: 192.10
 ---- batch: 090 ----
mean loss: 185.70
 ---- batch: 100 ----
mean loss: 182.43
 ---- batch: 110 ----
mean loss: 190.81
train mean loss: 188.62
epoch train time: 0:00:02.539041
elapsed time: 0:10:12.695240
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 20:58:59.207470
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.55
 ---- batch: 020 ----
mean loss: 194.32
 ---- batch: 030 ----
mean loss: 196.93
 ---- batch: 040 ----
mean loss: 190.10
 ---- batch: 050 ----
mean loss: 188.40
 ---- batch: 060 ----
mean loss: 187.73
 ---- batch: 070 ----
mean loss: 188.59
 ---- batch: 080 ----
mean loss: 176.39
 ---- batch: 090 ----
mean loss: 192.64
 ---- batch: 100 ----
mean loss: 191.06
 ---- batch: 110 ----
mean loss: 187.60
train mean loss: 188.54
epoch train time: 0:00:02.519933
elapsed time: 0:10:15.215581
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 20:59:01.727808
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.20
 ---- batch: 020 ----
mean loss: 183.74
 ---- batch: 030 ----
mean loss: 184.27
 ---- batch: 040 ----
mean loss: 198.54
 ---- batch: 050 ----
mean loss: 183.57
 ---- batch: 060 ----
mean loss: 188.44
 ---- batch: 070 ----
mean loss: 189.45
 ---- batch: 080 ----
mean loss: 188.35
 ---- batch: 090 ----
mean loss: 185.85
 ---- batch: 100 ----
mean loss: 182.11
 ---- batch: 110 ----
mean loss: 192.37
train mean loss: 188.60
epoch train time: 0:00:02.516216
elapsed time: 0:10:17.732267
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 20:59:04.244493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.28
 ---- batch: 020 ----
mean loss: 178.18
 ---- batch: 030 ----
mean loss: 181.27
 ---- batch: 040 ----
mean loss: 194.35
 ---- batch: 050 ----
mean loss: 191.22
 ---- batch: 060 ----
mean loss: 182.67
 ---- batch: 070 ----
mean loss: 189.86
 ---- batch: 080 ----
mean loss: 195.16
 ---- batch: 090 ----
mean loss: 186.82
 ---- batch: 100 ----
mean loss: 197.62
 ---- batch: 110 ----
mean loss: 190.57
train mean loss: 188.49
epoch train time: 0:00:02.540640
elapsed time: 0:10:20.273326
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 20:59:06.785580
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.42
 ---- batch: 020 ----
mean loss: 185.49
 ---- batch: 030 ----
mean loss: 193.09
 ---- batch: 040 ----
mean loss: 188.39
 ---- batch: 050 ----
mean loss: 179.10
 ---- batch: 060 ----
mean loss: 194.06
 ---- batch: 070 ----
mean loss: 188.37
 ---- batch: 080 ----
mean loss: 195.69
 ---- batch: 090 ----
mean loss: 188.82
 ---- batch: 100 ----
mean loss: 186.46
 ---- batch: 110 ----
mean loss: 189.55
train mean loss: 188.46
epoch train time: 0:00:02.536111
elapsed time: 0:10:22.809869
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 20:59:09.322093
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.49
 ---- batch: 020 ----
mean loss: 180.49
 ---- batch: 030 ----
mean loss: 184.01
 ---- batch: 040 ----
mean loss: 190.63
 ---- batch: 050 ----
mean loss: 187.18
 ---- batch: 060 ----
mean loss: 198.02
 ---- batch: 070 ----
mean loss: 192.28
 ---- batch: 080 ----
mean loss: 189.69
 ---- batch: 090 ----
mean loss: 183.82
 ---- batch: 100 ----
mean loss: 193.45
 ---- batch: 110 ----
mean loss: 186.81
train mean loss: 188.50
epoch train time: 0:00:02.522400
elapsed time: 0:10:25.332849
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 20:59:11.844900
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.80
 ---- batch: 020 ----
mean loss: 188.87
 ---- batch: 030 ----
mean loss: 177.26
 ---- batch: 040 ----
mean loss: 187.22
 ---- batch: 050 ----
mean loss: 185.21
 ---- batch: 060 ----
mean loss: 190.63
 ---- batch: 070 ----
mean loss: 186.15
 ---- batch: 080 ----
mean loss: 195.44
 ---- batch: 090 ----
mean loss: 192.43
 ---- batch: 100 ----
mean loss: 189.41
 ---- batch: 110 ----
mean loss: 189.78
train mean loss: 188.44
epoch train time: 0:00:02.565682
elapsed time: 0:10:27.898761
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 20:59:14.410991
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.64
 ---- batch: 020 ----
mean loss: 182.67
 ---- batch: 030 ----
mean loss: 184.20
 ---- batch: 040 ----
mean loss: 198.30
 ---- batch: 050 ----
mean loss: 183.19
 ---- batch: 060 ----
mean loss: 193.84
 ---- batch: 070 ----
mean loss: 189.09
 ---- batch: 080 ----
mean loss: 184.10
 ---- batch: 090 ----
mean loss: 187.56
 ---- batch: 100 ----
mean loss: 191.39
 ---- batch: 110 ----
mean loss: 187.24
train mean loss: 188.46
epoch train time: 0:00:02.533132
elapsed time: 0:10:30.432297
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 20:59:16.944522
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.87
 ---- batch: 020 ----
mean loss: 187.84
 ---- batch: 030 ----
mean loss: 184.04
 ---- batch: 040 ----
mean loss: 196.28
 ---- batch: 050 ----
mean loss: 189.24
 ---- batch: 060 ----
mean loss: 197.26
 ---- batch: 070 ----
mean loss: 192.09
 ---- batch: 080 ----
mean loss: 181.28
 ---- batch: 090 ----
mean loss: 186.43
 ---- batch: 100 ----
mean loss: 185.81
 ---- batch: 110 ----
mean loss: 184.11
train mean loss: 188.48
epoch train time: 0:00:02.571385
elapsed time: 0:10:33.004093
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 20:59:19.516175
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.88
 ---- batch: 020 ----
mean loss: 200.33
 ---- batch: 030 ----
mean loss: 188.25
 ---- batch: 040 ----
mean loss: 183.05
 ---- batch: 050 ----
mean loss: 187.12
 ---- batch: 060 ----
mean loss: 192.79
 ---- batch: 070 ----
mean loss: 177.98
 ---- batch: 080 ----
mean loss: 182.58
 ---- batch: 090 ----
mean loss: 193.68
 ---- batch: 100 ----
mean loss: 192.07
 ---- batch: 110 ----
mean loss: 183.52
train mean loss: 188.39
epoch train time: 0:00:02.564427
elapsed time: 0:10:35.568779
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 20:59:22.081070
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.83
 ---- batch: 020 ----
mean loss: 191.33
 ---- batch: 030 ----
mean loss: 186.21
 ---- batch: 040 ----
mean loss: 188.09
 ---- batch: 050 ----
mean loss: 185.05
 ---- batch: 060 ----
mean loss: 188.97
 ---- batch: 070 ----
mean loss: 196.80
 ---- batch: 080 ----
mean loss: 186.44
 ---- batch: 090 ----
mean loss: 181.36
 ---- batch: 100 ----
mean loss: 197.26
 ---- batch: 110 ----
mean loss: 185.49
train mean loss: 188.42
epoch train time: 0:00:02.530364
elapsed time: 0:10:38.099612
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 20:59:24.611838
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.67
 ---- batch: 020 ----
mean loss: 190.22
 ---- batch: 030 ----
mean loss: 191.67
 ---- batch: 040 ----
mean loss: 191.99
 ---- batch: 050 ----
mean loss: 183.84
 ---- batch: 060 ----
mean loss: 197.22
 ---- batch: 070 ----
mean loss: 189.99
 ---- batch: 080 ----
mean loss: 198.34
 ---- batch: 090 ----
mean loss: 181.63
 ---- batch: 100 ----
mean loss: 186.67
 ---- batch: 110 ----
mean loss: 177.55
train mean loss: 188.40
epoch train time: 0:00:02.526297
elapsed time: 0:10:40.626300
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 20:59:27.138546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.63
 ---- batch: 020 ----
mean loss: 186.25
 ---- batch: 030 ----
mean loss: 191.40
 ---- batch: 040 ----
mean loss: 181.17
 ---- batch: 050 ----
mean loss: 183.92
 ---- batch: 060 ----
mean loss: 190.99
 ---- batch: 070 ----
mean loss: 190.86
 ---- batch: 080 ----
mean loss: 184.99
 ---- batch: 090 ----
mean loss: 202.60
 ---- batch: 100 ----
mean loss: 186.08
 ---- batch: 110 ----
mean loss: 193.96
train mean loss: 188.49
epoch train time: 0:00:02.539006
elapsed time: 0:10:43.165756
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 20:59:29.677987
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.59
 ---- batch: 020 ----
mean loss: 185.89
 ---- batch: 030 ----
mean loss: 184.63
 ---- batch: 040 ----
mean loss: 188.47
 ---- batch: 050 ----
mean loss: 192.81
 ---- batch: 060 ----
mean loss: 193.75
 ---- batch: 070 ----
mean loss: 189.82
 ---- batch: 080 ----
mean loss: 191.05
 ---- batch: 090 ----
mean loss: 187.29
 ---- batch: 100 ----
mean loss: 195.44
 ---- batch: 110 ----
mean loss: 182.21
train mean loss: 188.41
epoch train time: 0:00:02.537988
elapsed time: 0:10:45.704154
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 20:59:32.216384
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.10
 ---- batch: 020 ----
mean loss: 202.29
 ---- batch: 030 ----
mean loss: 196.10
 ---- batch: 040 ----
mean loss: 182.28
 ---- batch: 050 ----
mean loss: 181.49
 ---- batch: 060 ----
mean loss: 187.27
 ---- batch: 070 ----
mean loss: 192.42
 ---- batch: 080 ----
mean loss: 171.67
 ---- batch: 090 ----
mean loss: 186.59
 ---- batch: 100 ----
mean loss: 196.22
 ---- batch: 110 ----
mean loss: 188.65
train mean loss: 188.31
epoch train time: 0:00:02.519366
elapsed time: 0:10:48.223926
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 20:59:34.736173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.22
 ---- batch: 020 ----
mean loss: 193.71
 ---- batch: 030 ----
mean loss: 181.89
 ---- batch: 040 ----
mean loss: 200.29
 ---- batch: 050 ----
mean loss: 193.25
 ---- batch: 060 ----
mean loss: 184.18
 ---- batch: 070 ----
mean loss: 195.24
 ---- batch: 080 ----
mean loss: 180.83
 ---- batch: 090 ----
mean loss: 181.28
 ---- batch: 100 ----
mean loss: 191.97
 ---- batch: 110 ----
mean loss: 185.01
train mean loss: 188.35
epoch train time: 0:00:02.520601
elapsed time: 0:10:50.744934
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 20:59:37.257156
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.91
 ---- batch: 020 ----
mean loss: 186.75
 ---- batch: 030 ----
mean loss: 189.24
 ---- batch: 040 ----
mean loss: 188.76
 ---- batch: 050 ----
mean loss: 191.30
 ---- batch: 060 ----
mean loss: 181.36
 ---- batch: 070 ----
mean loss: 194.76
 ---- batch: 080 ----
mean loss: 193.53
 ---- batch: 090 ----
mean loss: 184.73
 ---- batch: 100 ----
mean loss: 191.55
 ---- batch: 110 ----
mean loss: 190.44
train mean loss: 188.30
epoch train time: 0:00:02.559125
elapsed time: 0:10:53.304462
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 20:59:39.816706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.43
 ---- batch: 020 ----
mean loss: 186.35
 ---- batch: 030 ----
mean loss: 189.17
 ---- batch: 040 ----
mean loss: 189.46
 ---- batch: 050 ----
mean loss: 180.52
 ---- batch: 060 ----
mean loss: 189.95
 ---- batch: 070 ----
mean loss: 193.34
 ---- batch: 080 ----
mean loss: 181.32
 ---- batch: 090 ----
mean loss: 191.77
 ---- batch: 100 ----
mean loss: 192.08
 ---- batch: 110 ----
mean loss: 194.31
train mean loss: 188.27
epoch train time: 0:00:02.555945
elapsed time: 0:10:55.860821
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 20:59:42.373052
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.46
 ---- batch: 020 ----
mean loss: 188.98
 ---- batch: 030 ----
mean loss: 191.42
 ---- batch: 040 ----
mean loss: 189.41
 ---- batch: 050 ----
mean loss: 188.78
 ---- batch: 060 ----
mean loss: 183.83
 ---- batch: 070 ----
mean loss: 189.71
 ---- batch: 080 ----
mean loss: 184.98
 ---- batch: 090 ----
mean loss: 183.38
 ---- batch: 100 ----
mean loss: 188.18
 ---- batch: 110 ----
mean loss: 189.37
train mean loss: 188.34
epoch train time: 0:00:02.536957
elapsed time: 0:10:58.398219
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 20:59:44.910452
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.93
 ---- batch: 020 ----
mean loss: 194.71
 ---- batch: 030 ----
mean loss: 190.20
 ---- batch: 040 ----
mean loss: 189.71
 ---- batch: 050 ----
mean loss: 184.06
 ---- batch: 060 ----
mean loss: 190.39
 ---- batch: 070 ----
mean loss: 192.48
 ---- batch: 080 ----
mean loss: 184.32
 ---- batch: 090 ----
mean loss: 185.85
 ---- batch: 100 ----
mean loss: 189.47
 ---- batch: 110 ----
mean loss: 196.90
train mean loss: 188.28
epoch train time: 0:00:02.524374
elapsed time: 0:11:00.923001
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 20:59:47.435266
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.23
 ---- batch: 020 ----
mean loss: 191.51
 ---- batch: 030 ----
mean loss: 187.52
 ---- batch: 040 ----
mean loss: 187.54
 ---- batch: 050 ----
mean loss: 182.44
 ---- batch: 060 ----
mean loss: 187.07
 ---- batch: 070 ----
mean loss: 183.05
 ---- batch: 080 ----
mean loss: 190.76
 ---- batch: 090 ----
mean loss: 191.33
 ---- batch: 100 ----
mean loss: 195.42
 ---- batch: 110 ----
mean loss: 189.67
train mean loss: 188.20
epoch train time: 0:00:02.529382
elapsed time: 0:11:03.452842
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 20:59:49.965087
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.95
 ---- batch: 020 ----
mean loss: 190.64
 ---- batch: 030 ----
mean loss: 184.21
 ---- batch: 040 ----
mean loss: 186.50
 ---- batch: 050 ----
mean loss: 182.10
 ---- batch: 060 ----
mean loss: 196.27
 ---- batch: 070 ----
mean loss: 188.33
 ---- batch: 080 ----
mean loss: 190.08
 ---- batch: 090 ----
mean loss: 189.34
 ---- batch: 100 ----
mean loss: 192.27
 ---- batch: 110 ----
mean loss: 187.52
train mean loss: 188.25
epoch train time: 0:00:02.557251
elapsed time: 0:11:06.014414
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_2/checkpoint.pth.tar
**** end time: 2019-09-26 20:59:52.526432 ****
