Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_6', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17457
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 01:48:37.078561 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 11, 11]             560
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
            Conv2d-4            [-1, 14, 4, 11]             224
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
            Linear-8                    [-1, 1]             308
================================================================
Total params: 1,092
Trainable params: 1,092
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 01:48:37.083896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4974.14
 ---- batch: 020 ----
mean loss: 4840.12
 ---- batch: 030 ----
mean loss: 4708.58
 ---- batch: 040 ----
mean loss: 4544.59
 ---- batch: 050 ----
mean loss: 4388.33
 ---- batch: 060 ----
mean loss: 4174.38
 ---- batch: 070 ----
mean loss: 4016.07
 ---- batch: 080 ----
mean loss: 3808.33
 ---- batch: 090 ----
mean loss: 3593.84
 ---- batch: 100 ----
mean loss: 3404.40
 ---- batch: 110 ----
mean loss: 3200.26
train mean loss: 4120.59
epoch train time: 0:00:32.994634
elapsed time: 0:00:33.001400
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 01:49:10.080002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2888.72
 ---- batch: 020 ----
mean loss: 2658.39
 ---- batch: 030 ----
mean loss: 2479.53
 ---- batch: 040 ----
mean loss: 2292.23
 ---- batch: 050 ----
mean loss: 2139.60
 ---- batch: 060 ----
mean loss: 1969.59
 ---- batch: 070 ----
mean loss: 1821.66
 ---- batch: 080 ----
mean loss: 1709.70
 ---- batch: 090 ----
mean loss: 1591.01
 ---- batch: 100 ----
mean loss: 1480.02
 ---- batch: 110 ----
mean loss: 1387.80
train mean loss: 2020.18
epoch train time: 0:00:00.715479
elapsed time: 0:00:33.717111
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 01:49:10.795761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1295.87
 ---- batch: 020 ----
mean loss: 1242.21
 ---- batch: 030 ----
mean loss: 1186.07
 ---- batch: 040 ----
mean loss: 1137.93
 ---- batch: 050 ----
mean loss: 1080.80
 ---- batch: 060 ----
mean loss: 1036.25
 ---- batch: 070 ----
mean loss: 1033.36
 ---- batch: 080 ----
mean loss: 992.56
 ---- batch: 090 ----
mean loss: 969.83
 ---- batch: 100 ----
mean loss: 947.93
 ---- batch: 110 ----
mean loss: 934.07
train mean loss: 1073.47
epoch train time: 0:00:00.722545
elapsed time: 0:00:34.439835
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 01:49:11.518463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.33
 ---- batch: 020 ----
mean loss: 907.21
 ---- batch: 030 ----
mean loss: 904.74
 ---- batch: 040 ----
mean loss: 880.68
 ---- batch: 050 ----
mean loss: 864.49
 ---- batch: 060 ----
mean loss: 867.08
 ---- batch: 070 ----
mean loss: 868.84
 ---- batch: 080 ----
mean loss: 842.13
 ---- batch: 090 ----
mean loss: 867.61
 ---- batch: 100 ----
mean loss: 870.07
 ---- batch: 110 ----
mean loss: 850.01
train mean loss: 876.26
epoch train time: 0:00:00.722853
elapsed time: 0:00:35.162849
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 01:49:12.241461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.92
 ---- batch: 020 ----
mean loss: 846.22
 ---- batch: 030 ----
mean loss: 853.07
 ---- batch: 040 ----
mean loss: 859.58
 ---- batch: 050 ----
mean loss: 852.65
 ---- batch: 060 ----
mean loss: 838.43
 ---- batch: 070 ----
mean loss: 841.89
 ---- batch: 080 ----
mean loss: 839.15
 ---- batch: 090 ----
mean loss: 840.18
 ---- batch: 100 ----
mean loss: 855.14
 ---- batch: 110 ----
mean loss: 853.42
train mean loss: 847.68
epoch train time: 0:00:00.723161
elapsed time: 0:00:35.886149
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 01:49:12.964756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.83
 ---- batch: 020 ----
mean loss: 842.47
 ---- batch: 030 ----
mean loss: 847.45
 ---- batch: 040 ----
mean loss: 821.98
 ---- batch: 050 ----
mean loss: 836.39
 ---- batch: 060 ----
mean loss: 857.65
 ---- batch: 070 ----
mean loss: 835.68
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 836.71
 ---- batch: 100 ----
mean loss: 844.25
 ---- batch: 110 ----
mean loss: 842.35
train mean loss: 841.78
epoch train time: 0:00:00.727141
elapsed time: 0:00:36.613439
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 01:49:13.692062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.28
 ---- batch: 020 ----
mean loss: 824.88
 ---- batch: 030 ----
mean loss: 828.16
 ---- batch: 040 ----
mean loss: 837.84
 ---- batch: 050 ----
mean loss: 810.72
 ---- batch: 060 ----
mean loss: 839.46
 ---- batch: 070 ----
mean loss: 829.91
 ---- batch: 080 ----
mean loss: 853.44
 ---- batch: 090 ----
mean loss: 848.79
 ---- batch: 100 ----
mean loss: 853.25
 ---- batch: 110 ----
mean loss: 840.32
train mean loss: 837.89
epoch train time: 0:00:00.727204
elapsed time: 0:00:37.340797
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 01:49:14.419409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.42
 ---- batch: 020 ----
mean loss: 820.08
 ---- batch: 030 ----
mean loss: 855.34
 ---- batch: 040 ----
mean loss: 836.14
 ---- batch: 050 ----
mean loss: 836.55
 ---- batch: 060 ----
mean loss: 842.18
 ---- batch: 070 ----
mean loss: 812.79
 ---- batch: 080 ----
mean loss: 832.12
 ---- batch: 090 ----
mean loss: 835.58
 ---- batch: 100 ----
mean loss: 842.89
 ---- batch: 110 ----
mean loss: 838.20
train mean loss: 834.12
epoch train time: 0:00:00.754336
elapsed time: 0:00:38.095274
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 01:49:15.173889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.39
 ---- batch: 020 ----
mean loss: 836.94
 ---- batch: 030 ----
mean loss: 818.10
 ---- batch: 040 ----
mean loss: 817.52
 ---- batch: 050 ----
mean loss: 834.41
 ---- batch: 060 ----
mean loss: 817.14
 ---- batch: 070 ----
mean loss: 840.38
 ---- batch: 080 ----
mean loss: 824.78
 ---- batch: 090 ----
mean loss: 823.81
 ---- batch: 100 ----
mean loss: 848.53
 ---- batch: 110 ----
mean loss: 837.38
train mean loss: 830.52
epoch train time: 0:00:00.744895
elapsed time: 0:00:38.840313
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 01:49:15.918953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 822.92
 ---- batch: 020 ----
mean loss: 836.33
 ---- batch: 030 ----
mean loss: 807.80
 ---- batch: 040 ----
mean loss: 840.91
 ---- batch: 050 ----
mean loss: 821.11
 ---- batch: 060 ----
mean loss: 828.12
 ---- batch: 070 ----
mean loss: 822.26
 ---- batch: 080 ----
mean loss: 856.84
 ---- batch: 090 ----
mean loss: 817.49
 ---- batch: 100 ----
mean loss: 811.27
 ---- batch: 110 ----
mean loss: 833.17
train mean loss: 826.49
epoch train time: 0:00:00.740804
elapsed time: 0:00:39.581296
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 01:49:16.659914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.09
 ---- batch: 020 ----
mean loss: 808.03
 ---- batch: 030 ----
mean loss: 823.50
 ---- batch: 040 ----
mean loss: 840.27
 ---- batch: 050 ----
mean loss: 814.89
 ---- batch: 060 ----
mean loss: 819.48
 ---- batch: 070 ----
mean loss: 824.57
 ---- batch: 080 ----
mean loss: 816.57
 ---- batch: 090 ----
mean loss: 832.24
 ---- batch: 100 ----
mean loss: 816.28
 ---- batch: 110 ----
mean loss: 818.77
train mean loss: 822.13
epoch train time: 0:00:00.741997
elapsed time: 0:00:40.323445
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 01:49:17.402054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.98
 ---- batch: 020 ----
mean loss: 799.47
 ---- batch: 030 ----
mean loss: 834.33
 ---- batch: 040 ----
mean loss: 827.41
 ---- batch: 050 ----
mean loss: 817.95
 ---- batch: 060 ----
mean loss: 811.04
 ---- batch: 070 ----
mean loss: 815.01
 ---- batch: 080 ----
mean loss: 807.96
 ---- batch: 090 ----
mean loss: 823.27
 ---- batch: 100 ----
mean loss: 812.28
 ---- batch: 110 ----
mean loss: 785.48
train mean loss: 816.99
epoch train time: 0:00:00.749391
elapsed time: 0:00:41.073006
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 01:49:18.151656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.34
 ---- batch: 020 ----
mean loss: 820.58
 ---- batch: 030 ----
mean loss: 818.13
 ---- batch: 040 ----
mean loss: 806.98
 ---- batch: 050 ----
mean loss: 810.13
 ---- batch: 060 ----
mean loss: 805.58
 ---- batch: 070 ----
mean loss: 821.30
 ---- batch: 080 ----
mean loss: 787.18
 ---- batch: 090 ----
mean loss: 808.15
 ---- batch: 100 ----
mean loss: 820.70
 ---- batch: 110 ----
mean loss: 793.60
train mean loss: 811.62
epoch train time: 0:00:00.753241
elapsed time: 0:00:41.826434
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 01:49:18.905046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 795.71
 ---- batch: 020 ----
mean loss: 810.24
 ---- batch: 030 ----
mean loss: 802.29
 ---- batch: 040 ----
mean loss: 791.28
 ---- batch: 050 ----
mean loss: 796.96
 ---- batch: 060 ----
mean loss: 812.95
 ---- batch: 070 ----
mean loss: 809.83
 ---- batch: 080 ----
mean loss: 812.33
 ---- batch: 090 ----
mean loss: 798.56
 ---- batch: 100 ----
mean loss: 814.37
 ---- batch: 110 ----
mean loss: 816.51
train mean loss: 806.29
epoch train time: 0:00:00.741183
elapsed time: 0:00:42.567796
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 01:49:19.646418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 806.06
 ---- batch: 020 ----
mean loss: 795.87
 ---- batch: 030 ----
mean loss: 812.59
 ---- batch: 040 ----
mean loss: 813.34
 ---- batch: 050 ----
mean loss: 804.73
 ---- batch: 060 ----
mean loss: 793.02
 ---- batch: 070 ----
mean loss: 797.81
 ---- batch: 080 ----
mean loss: 788.82
 ---- batch: 090 ----
mean loss: 797.61
 ---- batch: 100 ----
mean loss: 796.98
 ---- batch: 110 ----
mean loss: 817.04
train mean loss: 801.15
epoch train time: 0:00:00.733232
elapsed time: 0:00:43.301181
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 01:49:20.379792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 801.93
 ---- batch: 020 ----
mean loss: 804.62
 ---- batch: 030 ----
mean loss: 793.97
 ---- batch: 040 ----
mean loss: 781.70
 ---- batch: 050 ----
mean loss: 794.06
 ---- batch: 060 ----
mean loss: 804.41
 ---- batch: 070 ----
mean loss: 807.19
 ---- batch: 080 ----
mean loss: 783.46
 ---- batch: 090 ----
mean loss: 786.65
 ---- batch: 100 ----
mean loss: 807.22
 ---- batch: 110 ----
mean loss: 783.25
train mean loss: 796.25
epoch train time: 0:00:00.732480
elapsed time: 0:00:44.033814
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 01:49:21.112438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 787.92
 ---- batch: 020 ----
mean loss: 762.61
 ---- batch: 030 ----
mean loss: 789.55
 ---- batch: 040 ----
mean loss: 812.64
 ---- batch: 050 ----
mean loss: 810.38
 ---- batch: 060 ----
mean loss: 806.83
 ---- batch: 070 ----
mean loss: 802.91
 ---- batch: 080 ----
mean loss: 789.83
 ---- batch: 090 ----
mean loss: 776.63
 ---- batch: 100 ----
mean loss: 784.26
 ---- batch: 110 ----
mean loss: 781.84
train mean loss: 791.42
epoch train time: 0:00:00.733020
elapsed time: 0:00:44.767003
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 01:49:21.845649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 768.40
 ---- batch: 020 ----
mean loss: 793.40
 ---- batch: 030 ----
mean loss: 774.47
 ---- batch: 040 ----
mean loss: 792.15
 ---- batch: 050 ----
mean loss: 802.96
 ---- batch: 060 ----
mean loss: 769.33
 ---- batch: 070 ----
mean loss: 798.19
 ---- batch: 080 ----
mean loss: 781.39
 ---- batch: 090 ----
mean loss: 781.66
 ---- batch: 100 ----
mean loss: 799.00
 ---- batch: 110 ----
mean loss: 794.31
train mean loss: 786.41
epoch train time: 0:00:00.717993
elapsed time: 0:00:45.485170
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 01:49:22.563779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 783.31
 ---- batch: 020 ----
mean loss: 794.56
 ---- batch: 030 ----
mean loss: 781.92
 ---- batch: 040 ----
mean loss: 763.32
 ---- batch: 050 ----
mean loss: 772.38
 ---- batch: 060 ----
mean loss: 787.42
 ---- batch: 070 ----
mean loss: 780.37
 ---- batch: 080 ----
mean loss: 780.55
 ---- batch: 090 ----
mean loss: 786.32
 ---- batch: 100 ----
mean loss: 776.52
 ---- batch: 110 ----
mean loss: 798.44
train mean loss: 781.37
epoch train time: 0:00:00.725445
elapsed time: 0:00:46.210750
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 01:49:23.289358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 777.40
 ---- batch: 020 ----
mean loss: 784.67
 ---- batch: 030 ----
mean loss: 788.12
 ---- batch: 040 ----
mean loss: 771.40
 ---- batch: 050 ----
mean loss: 763.57
 ---- batch: 060 ----
mean loss: 784.12
 ---- batch: 070 ----
mean loss: 777.60
 ---- batch: 080 ----
mean loss: 779.67
 ---- batch: 090 ----
mean loss: 769.67
 ---- batch: 100 ----
mean loss: 775.60
 ---- batch: 110 ----
mean loss: 778.03
train mean loss: 776.27
epoch train time: 0:00:00.722357
elapsed time: 0:00:46.933273
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 01:49:24.011881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 739.43
 ---- batch: 020 ----
mean loss: 809.15
 ---- batch: 030 ----
mean loss: 774.07
 ---- batch: 040 ----
mean loss: 794.43
 ---- batch: 050 ----
mean loss: 777.12
 ---- batch: 060 ----
mean loss: 778.89
 ---- batch: 070 ----
mean loss: 767.58
 ---- batch: 080 ----
mean loss: 777.30
 ---- batch: 090 ----
mean loss: 758.55
 ---- batch: 100 ----
mean loss: 755.22
 ---- batch: 110 ----
mean loss: 749.82
train mean loss: 770.82
epoch train time: 0:00:00.715896
elapsed time: 0:00:47.649315
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 01:49:24.727925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 759.20
 ---- batch: 020 ----
mean loss: 780.80
 ---- batch: 030 ----
mean loss: 756.28
 ---- batch: 040 ----
mean loss: 781.28
 ---- batch: 050 ----
mean loss: 768.67
 ---- batch: 060 ----
mean loss: 761.11
 ---- batch: 070 ----
mean loss: 773.19
 ---- batch: 080 ----
mean loss: 758.65
 ---- batch: 090 ----
mean loss: 755.14
 ---- batch: 100 ----
mean loss: 752.53
 ---- batch: 110 ----
mean loss: 766.93
train mean loss: 764.36
epoch train time: 0:00:00.718432
elapsed time: 0:00:48.367916
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 01:49:25.446564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.86
 ---- batch: 020 ----
mean loss: 746.71
 ---- batch: 030 ----
mean loss: 737.93
 ---- batch: 040 ----
mean loss: 763.16
 ---- batch: 050 ----
mean loss: 776.64
 ---- batch: 060 ----
mean loss: 754.04
 ---- batch: 070 ----
mean loss: 777.10
 ---- batch: 080 ----
mean loss: 746.95
 ---- batch: 090 ----
mean loss: 750.67
 ---- batch: 100 ----
mean loss: 779.62
 ---- batch: 110 ----
mean loss: 749.03
train mean loss: 757.42
epoch train time: 0:00:00.724168
elapsed time: 0:00:49.092271
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 01:49:26.170882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 735.73
 ---- batch: 020 ----
mean loss: 753.33
 ---- batch: 030 ----
mean loss: 767.05
 ---- batch: 040 ----
mean loss: 747.84
 ---- batch: 050 ----
mean loss: 765.29
 ---- batch: 060 ----
mean loss: 744.64
 ---- batch: 070 ----
mean loss: 747.39
 ---- batch: 080 ----
mean loss: 760.45
 ---- batch: 090 ----
mean loss: 746.20
 ---- batch: 100 ----
mean loss: 755.85
 ---- batch: 110 ----
mean loss: 734.59
train mean loss: 750.88
epoch train time: 0:00:00.721890
elapsed time: 0:00:49.814297
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 01:49:26.892908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 745.06
 ---- batch: 020 ----
mean loss: 746.04
 ---- batch: 030 ----
mean loss: 733.11
 ---- batch: 040 ----
mean loss: 751.92
 ---- batch: 050 ----
mean loss: 745.48
 ---- batch: 060 ----
mean loss: 753.73
 ---- batch: 070 ----
mean loss: 727.46
 ---- batch: 080 ----
mean loss: 746.39
 ---- batch: 090 ----
mean loss: 752.23
 ---- batch: 100 ----
mean loss: 730.97
 ---- batch: 110 ----
mean loss: 751.29
train mean loss: 744.24
epoch train time: 0:00:00.716402
elapsed time: 0:00:50.530830
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 01:49:27.609437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.02
 ---- batch: 020 ----
mean loss: 738.95
 ---- batch: 030 ----
mean loss: 731.31
 ---- batch: 040 ----
mean loss: 738.23
 ---- batch: 050 ----
mean loss: 737.94
 ---- batch: 060 ----
mean loss: 744.74
 ---- batch: 070 ----
mean loss: 749.96
 ---- batch: 080 ----
mean loss: 722.98
 ---- batch: 090 ----
mean loss: 745.48
 ---- batch: 100 ----
mean loss: 730.63
 ---- batch: 110 ----
mean loss: 732.99
train mean loss: 737.13
epoch train time: 0:00:00.732127
elapsed time: 0:00:51.263127
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 01:49:28.341743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.73
 ---- batch: 020 ----
mean loss: 736.79
 ---- batch: 030 ----
mean loss: 742.31
 ---- batch: 040 ----
mean loss: 736.82
 ---- batch: 050 ----
mean loss: 725.74
 ---- batch: 060 ----
mean loss: 715.39
 ---- batch: 070 ----
mean loss: 712.97
 ---- batch: 080 ----
mean loss: 747.31
 ---- batch: 090 ----
mean loss: 741.27
 ---- batch: 100 ----
mean loss: 715.92
 ---- batch: 110 ----
mean loss: 720.41
train mean loss: 729.69
epoch train time: 0:00:00.731764
elapsed time: 0:00:51.995037
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 01:49:29.073676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 723.11
 ---- batch: 020 ----
mean loss: 714.36
 ---- batch: 030 ----
mean loss: 739.98
 ---- batch: 040 ----
mean loss: 739.12
 ---- batch: 050 ----
mean loss: 719.28
 ---- batch: 060 ----
mean loss: 719.17
 ---- batch: 070 ----
mean loss: 717.00
 ---- batch: 080 ----
mean loss: 717.36
 ---- batch: 090 ----
mean loss: 725.17
 ---- batch: 100 ----
mean loss: 710.66
 ---- batch: 110 ----
mean loss: 719.47
train mean loss: 721.39
epoch train time: 0:00:00.726522
elapsed time: 0:00:52.721724
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 01:49:29.800336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.54
 ---- batch: 020 ----
mean loss: 712.62
 ---- batch: 030 ----
mean loss: 724.39
 ---- batch: 040 ----
mean loss: 734.05
 ---- batch: 050 ----
mean loss: 733.27
 ---- batch: 060 ----
mean loss: 703.97
 ---- batch: 070 ----
mean loss: 712.01
 ---- batch: 080 ----
mean loss: 712.74
 ---- batch: 090 ----
mean loss: 697.91
 ---- batch: 100 ----
mean loss: 714.35
 ---- batch: 110 ----
mean loss: 707.17
train mean loss: 712.51
epoch train time: 0:00:00.732192
elapsed time: 0:00:53.454057
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 01:49:30.532685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.37
 ---- batch: 020 ----
mean loss: 690.44
 ---- batch: 030 ----
mean loss: 700.54
 ---- batch: 040 ----
mean loss: 698.72
 ---- batch: 050 ----
mean loss: 711.63
 ---- batch: 060 ----
mean loss: 699.34
 ---- batch: 070 ----
mean loss: 700.58
 ---- batch: 080 ----
mean loss: 710.24
 ---- batch: 090 ----
mean loss: 701.44
 ---- batch: 100 ----
mean loss: 707.26
 ---- batch: 110 ----
mean loss: 699.60
train mean loss: 702.15
epoch train time: 0:00:00.727372
elapsed time: 0:00:54.181607
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 01:49:31.260251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.37
 ---- batch: 020 ----
mean loss: 686.87
 ---- batch: 030 ----
mean loss: 696.58
 ---- batch: 040 ----
mean loss: 696.01
 ---- batch: 050 ----
mean loss: 692.81
 ---- batch: 060 ----
mean loss: 691.45
 ---- batch: 070 ----
mean loss: 657.56
 ---- batch: 080 ----
mean loss: 697.12
 ---- batch: 090 ----
mean loss: 687.40
 ---- batch: 100 ----
mean loss: 686.18
 ---- batch: 110 ----
mean loss: 689.09
train mean loss: 688.63
epoch train time: 0:00:00.739046
elapsed time: 0:00:54.920825
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 01:49:31.999443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.92
 ---- batch: 020 ----
mean loss: 662.93
 ---- batch: 030 ----
mean loss: 688.63
 ---- batch: 040 ----
mean loss: 691.43
 ---- batch: 050 ----
mean loss: 657.71
 ---- batch: 060 ----
mean loss: 677.98
 ---- batch: 070 ----
mean loss: 668.10
 ---- batch: 080 ----
mean loss: 668.41
 ---- batch: 090 ----
mean loss: 662.21
 ---- batch: 100 ----
mean loss: 664.84
 ---- batch: 110 ----
mean loss: 651.89
train mean loss: 670.34
epoch train time: 0:00:00.723799
elapsed time: 0:00:55.644766
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 01:49:32.723374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 647.92
 ---- batch: 020 ----
mean loss: 648.75
 ---- batch: 030 ----
mean loss: 662.57
 ---- batch: 040 ----
mean loss: 663.20
 ---- batch: 050 ----
mean loss: 651.88
 ---- batch: 060 ----
mean loss: 656.77
 ---- batch: 070 ----
mean loss: 621.28
 ---- batch: 080 ----
mean loss: 654.66
 ---- batch: 090 ----
mean loss: 641.27
 ---- batch: 100 ----
mean loss: 648.57
 ---- batch: 110 ----
mean loss: 632.94
train mean loss: 647.84
epoch train time: 0:00:00.719899
elapsed time: 0:00:56.364845
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 01:49:33.443453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 628.63
 ---- batch: 020 ----
mean loss: 632.76
 ---- batch: 030 ----
mean loss: 622.10
 ---- batch: 040 ----
mean loss: 627.94
 ---- batch: 050 ----
mean loss: 613.64
 ---- batch: 060 ----
mean loss: 619.13
 ---- batch: 070 ----
mean loss: 633.74
 ---- batch: 080 ----
mean loss: 627.09
 ---- batch: 090 ----
mean loss: 626.02
 ---- batch: 100 ----
mean loss: 624.11
 ---- batch: 110 ----
mean loss: 616.47
train mean loss: 624.05
epoch train time: 0:00:00.724638
elapsed time: 0:00:57.089632
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 01:49:34.168243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 607.02
 ---- batch: 020 ----
mean loss: 583.30
 ---- batch: 030 ----
mean loss: 601.46
 ---- batch: 040 ----
mean loss: 617.16
 ---- batch: 050 ----
mean loss: 600.71
 ---- batch: 060 ----
mean loss: 610.42
 ---- batch: 070 ----
mean loss: 581.46
 ---- batch: 080 ----
mean loss: 606.78
 ---- batch: 090 ----
mean loss: 602.03
 ---- batch: 100 ----
mean loss: 602.92
 ---- batch: 110 ----
mean loss: 600.77
train mean loss: 600.41
epoch train time: 0:00:00.718231
elapsed time: 0:00:57.808004
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 01:49:34.886636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 612.80
 ---- batch: 020 ----
mean loss: 592.49
 ---- batch: 030 ----
mean loss: 581.44
 ---- batch: 040 ----
mean loss: 577.00
 ---- batch: 050 ----
mean loss: 582.27
 ---- batch: 060 ----
mean loss: 571.90
 ---- batch: 070 ----
mean loss: 572.82
 ---- batch: 080 ----
mean loss: 570.59
 ---- batch: 090 ----
mean loss: 557.48
 ---- batch: 100 ----
mean loss: 576.33
 ---- batch: 110 ----
mean loss: 564.36
train mean loss: 577.55
epoch train time: 0:00:00.720769
elapsed time: 0:00:58.528941
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 01:49:35.607553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 574.48
 ---- batch: 020 ----
mean loss: 561.11
 ---- batch: 030 ----
mean loss: 566.44
 ---- batch: 040 ----
mean loss: 559.91
 ---- batch: 050 ----
mean loss: 555.56
 ---- batch: 060 ----
mean loss: 564.11
 ---- batch: 070 ----
mean loss: 549.85
 ---- batch: 080 ----
mean loss: 546.82
 ---- batch: 090 ----
mean loss: 556.56
 ---- batch: 100 ----
mean loss: 553.65
 ---- batch: 110 ----
mean loss: 533.34
train mean loss: 556.20
epoch train time: 0:00:00.729904
elapsed time: 0:00:59.258983
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 01:49:36.337647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 541.41
 ---- batch: 020 ----
mean loss: 545.25
 ---- batch: 030 ----
mean loss: 551.80
 ---- batch: 040 ----
mean loss: 541.93
 ---- batch: 050 ----
mean loss: 549.95
 ---- batch: 060 ----
mean loss: 522.62
 ---- batch: 070 ----
mean loss: 525.78
 ---- batch: 080 ----
mean loss: 531.59
 ---- batch: 090 ----
mean loss: 522.10
 ---- batch: 100 ----
mean loss: 528.76
 ---- batch: 110 ----
mean loss: 539.70
train mean loss: 536.05
epoch train time: 0:00:00.731095
elapsed time: 0:00:59.990270
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 01:49:37.068878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.56
 ---- batch: 020 ----
mean loss: 529.37
 ---- batch: 030 ----
mean loss: 525.85
 ---- batch: 040 ----
mean loss: 514.88
 ---- batch: 050 ----
mean loss: 507.34
 ---- batch: 060 ----
mean loss: 511.64
 ---- batch: 070 ----
mean loss: 505.65
 ---- batch: 080 ----
mean loss: 525.13
 ---- batch: 090 ----
mean loss: 512.86
 ---- batch: 100 ----
mean loss: 502.82
 ---- batch: 110 ----
mean loss: 513.49
train mean loss: 516.45
epoch train time: 0:00:00.727947
elapsed time: 0:01:00.718349
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 01:49:37.796956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.16
 ---- batch: 020 ----
mean loss: 507.31
 ---- batch: 030 ----
mean loss: 487.54
 ---- batch: 040 ----
mean loss: 500.70
 ---- batch: 050 ----
mean loss: 506.15
 ---- batch: 060 ----
mean loss: 491.68
 ---- batch: 070 ----
mean loss: 496.31
 ---- batch: 080 ----
mean loss: 500.53
 ---- batch: 090 ----
mean loss: 491.64
 ---- batch: 100 ----
mean loss: 497.30
 ---- batch: 110 ----
mean loss: 493.10
train mean loss: 496.39
epoch train time: 0:00:00.726255
elapsed time: 0:01:01.444734
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 01:49:38.523339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.21
 ---- batch: 020 ----
mean loss: 487.26
 ---- batch: 030 ----
mean loss: 474.74
 ---- batch: 040 ----
mean loss: 482.28
 ---- batch: 050 ----
mean loss: 477.59
 ---- batch: 060 ----
mean loss: 480.15
 ---- batch: 070 ----
mean loss: 478.43
 ---- batch: 080 ----
mean loss: 478.57
 ---- batch: 090 ----
mean loss: 459.95
 ---- batch: 100 ----
mean loss: 470.14
 ---- batch: 110 ----
mean loss: 473.63
train mean loss: 476.07
epoch train time: 0:00:00.718687
elapsed time: 0:01:02.163552
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 01:49:39.242159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.51
 ---- batch: 020 ----
mean loss: 457.11
 ---- batch: 030 ----
mean loss: 469.50
 ---- batch: 040 ----
mean loss: 463.31
 ---- batch: 050 ----
mean loss: 452.35
 ---- batch: 060 ----
mean loss: 445.07
 ---- batch: 070 ----
mean loss: 470.88
 ---- batch: 080 ----
mean loss: 466.14
 ---- batch: 090 ----
mean loss: 456.95
 ---- batch: 100 ----
mean loss: 447.89
 ---- batch: 110 ----
mean loss: 445.27
train mean loss: 457.30
epoch train time: 0:00:00.718363
elapsed time: 0:01:02.882047
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 01:49:39.960678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.96
 ---- batch: 020 ----
mean loss: 442.28
 ---- batch: 030 ----
mean loss: 442.93
 ---- batch: 040 ----
mean loss: 452.49
 ---- batch: 050 ----
mean loss: 441.02
 ---- batch: 060 ----
mean loss: 446.55
 ---- batch: 070 ----
mean loss: 442.61
 ---- batch: 080 ----
mean loss: 429.57
 ---- batch: 090 ----
mean loss: 434.26
 ---- batch: 100 ----
mean loss: 423.09
 ---- batch: 110 ----
mean loss: 440.48
train mean loss: 439.99
epoch train time: 0:00:00.714019
elapsed time: 0:01:03.596233
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 01:49:40.674859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.82
 ---- batch: 020 ----
mean loss: 422.43
 ---- batch: 030 ----
mean loss: 435.53
 ---- batch: 040 ----
mean loss: 435.43
 ---- batch: 050 ----
mean loss: 424.80
 ---- batch: 060 ----
mean loss: 427.70
 ---- batch: 070 ----
mean loss: 417.75
 ---- batch: 080 ----
mean loss: 424.11
 ---- batch: 090 ----
mean loss: 413.35
 ---- batch: 100 ----
mean loss: 420.70
 ---- batch: 110 ----
mean loss: 409.65
train mean loss: 423.73
epoch train time: 0:00:00.716956
elapsed time: 0:01:04.313344
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 01:49:41.391954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.84
 ---- batch: 020 ----
mean loss: 413.72
 ---- batch: 030 ----
mean loss: 404.63
 ---- batch: 040 ----
mean loss: 412.47
 ---- batch: 050 ----
mean loss: 410.44
 ---- batch: 060 ----
mean loss: 434.37
 ---- batch: 070 ----
mean loss: 412.69
 ---- batch: 080 ----
mean loss: 401.65
 ---- batch: 090 ----
mean loss: 411.35
 ---- batch: 100 ----
mean loss: 395.15
 ---- batch: 110 ----
mean loss: 396.63
train mean loss: 408.64
epoch train time: 0:00:00.720900
elapsed time: 0:01:05.034376
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 01:49:42.112984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.51
 ---- batch: 020 ----
mean loss: 409.66
 ---- batch: 030 ----
mean loss: 402.88
 ---- batch: 040 ----
mean loss: 401.53
 ---- batch: 050 ----
mean loss: 395.60
 ---- batch: 060 ----
mean loss: 390.09
 ---- batch: 070 ----
mean loss: 392.48
 ---- batch: 080 ----
mean loss: 392.99
 ---- batch: 090 ----
mean loss: 383.94
 ---- batch: 100 ----
mean loss: 391.07
 ---- batch: 110 ----
mean loss: 389.14
train mean loss: 394.66
epoch train time: 0:00:00.721971
elapsed time: 0:01:05.756525
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 01:49:42.835152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.90
 ---- batch: 020 ----
mean loss: 393.14
 ---- batch: 030 ----
mean loss: 388.25
 ---- batch: 040 ----
mean loss: 374.65
 ---- batch: 050 ----
mean loss: 387.42
 ---- batch: 060 ----
mean loss: 371.24
 ---- batch: 070 ----
mean loss: 390.78
 ---- batch: 080 ----
mean loss: 381.76
 ---- batch: 090 ----
mean loss: 371.77
 ---- batch: 100 ----
mean loss: 374.78
 ---- batch: 110 ----
mean loss: 382.62
train mean loss: 381.84
epoch train time: 0:00:00.722160
elapsed time: 0:01:06.478846
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 01:49:43.557457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.52
 ---- batch: 020 ----
mean loss: 383.77
 ---- batch: 030 ----
mean loss: 372.64
 ---- batch: 040 ----
mean loss: 369.07
 ---- batch: 050 ----
mean loss: 377.62
 ---- batch: 060 ----
mean loss: 369.15
 ---- batch: 070 ----
mean loss: 376.63
 ---- batch: 080 ----
mean loss: 366.49
 ---- batch: 090 ----
mean loss: 371.55
 ---- batch: 100 ----
mean loss: 348.61
 ---- batch: 110 ----
mean loss: 364.70
train mean loss: 369.98
epoch train time: 0:00:00.727174
elapsed time: 0:01:07.206179
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 01:49:44.284790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.09
 ---- batch: 020 ----
mean loss: 362.19
 ---- batch: 030 ----
mean loss: 361.48
 ---- batch: 040 ----
mean loss: 349.94
 ---- batch: 050 ----
mean loss: 355.21
 ---- batch: 060 ----
mean loss: 364.47
 ---- batch: 070 ----
mean loss: 361.35
 ---- batch: 080 ----
mean loss: 365.09
 ---- batch: 090 ----
mean loss: 356.91
 ---- batch: 100 ----
mean loss: 354.63
 ---- batch: 110 ----
mean loss: 351.81
train mean loss: 359.02
epoch train time: 0:00:00.723412
elapsed time: 0:01:07.929725
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 01:49:45.008358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.33
 ---- batch: 020 ----
mean loss: 356.25
 ---- batch: 030 ----
mean loss: 344.27
 ---- batch: 040 ----
mean loss: 357.86
 ---- batch: 050 ----
mean loss: 349.21
 ---- batch: 060 ----
mean loss: 345.99
 ---- batch: 070 ----
mean loss: 347.28
 ---- batch: 080 ----
mean loss: 351.50
 ---- batch: 090 ----
mean loss: 344.47
 ---- batch: 100 ----
mean loss: 344.44
 ---- batch: 110 ----
mean loss: 331.01
train mean loss: 348.45
epoch train time: 0:00:00.721606
elapsed time: 0:01:08.651491
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 01:49:45.730099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.54
 ---- batch: 020 ----
mean loss: 338.37
 ---- batch: 030 ----
mean loss: 336.00
 ---- batch: 040 ----
mean loss: 341.35
 ---- batch: 050 ----
mean loss: 343.57
 ---- batch: 060 ----
mean loss: 337.79
 ---- batch: 070 ----
mean loss: 337.85
 ---- batch: 080 ----
mean loss: 333.06
 ---- batch: 090 ----
mean loss: 340.17
 ---- batch: 100 ----
mean loss: 336.43
 ---- batch: 110 ----
mean loss: 341.02
train mean loss: 338.93
epoch train time: 0:00:00.727870
elapsed time: 0:01:09.379499
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 01:49:46.458119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.01
 ---- batch: 020 ----
mean loss: 339.93
 ---- batch: 030 ----
mean loss: 346.46
 ---- batch: 040 ----
mean loss: 323.03
 ---- batch: 050 ----
mean loss: 323.28
 ---- batch: 060 ----
mean loss: 328.18
 ---- batch: 070 ----
mean loss: 323.51
 ---- batch: 080 ----
mean loss: 335.35
 ---- batch: 090 ----
mean loss: 325.64
 ---- batch: 100 ----
mean loss: 331.86
 ---- batch: 110 ----
mean loss: 326.52
train mean loss: 329.67
epoch train time: 0:00:00.723376
elapsed time: 0:01:10.103017
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 01:49:47.181649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.13
 ---- batch: 020 ----
mean loss: 333.94
 ---- batch: 030 ----
mean loss: 325.67
 ---- batch: 040 ----
mean loss: 317.76
 ---- batch: 050 ----
mean loss: 312.84
 ---- batch: 060 ----
mean loss: 321.39
 ---- batch: 070 ----
mean loss: 318.22
 ---- batch: 080 ----
mean loss: 319.71
 ---- batch: 090 ----
mean loss: 313.71
 ---- batch: 100 ----
mean loss: 313.74
 ---- batch: 110 ----
mean loss: 322.23
train mean loss: 321.07
epoch train time: 0:00:00.726549
elapsed time: 0:01:10.829730
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 01:49:47.908342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.01
 ---- batch: 020 ----
mean loss: 317.23
 ---- batch: 030 ----
mean loss: 320.10
 ---- batch: 040 ----
mean loss: 309.35
 ---- batch: 050 ----
mean loss: 314.93
 ---- batch: 060 ----
mean loss: 314.77
 ---- batch: 070 ----
mean loss: 313.32
 ---- batch: 080 ----
mean loss: 307.46
 ---- batch: 090 ----
mean loss: 301.59
 ---- batch: 100 ----
mean loss: 314.02
 ---- batch: 110 ----
mean loss: 309.68
train mean loss: 312.80
epoch train time: 0:00:00.726789
elapsed time: 0:01:11.556674
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 01:49:48.635283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.68
 ---- batch: 020 ----
mean loss: 303.95
 ---- batch: 030 ----
mean loss: 318.99
 ---- batch: 040 ----
mean loss: 309.16
 ---- batch: 050 ----
mean loss: 309.44
 ---- batch: 060 ----
mean loss: 291.00
 ---- batch: 070 ----
mean loss: 294.78
 ---- batch: 080 ----
mean loss: 300.72
 ---- batch: 090 ----
mean loss: 300.79
 ---- batch: 100 ----
mean loss: 317.65
 ---- batch: 110 ----
mean loss: 302.83
train mean loss: 305.09
epoch train time: 0:00:00.723965
elapsed time: 0:01:12.280790
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 01:49:49.359428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.02
 ---- batch: 020 ----
mean loss: 297.46
 ---- batch: 030 ----
mean loss: 307.20
 ---- batch: 040 ----
mean loss: 300.90
 ---- batch: 050 ----
mean loss: 297.97
 ---- batch: 060 ----
mean loss: 293.20
 ---- batch: 070 ----
mean loss: 290.38
 ---- batch: 080 ----
mean loss: 296.95
 ---- batch: 090 ----
mean loss: 300.20
 ---- batch: 100 ----
mean loss: 290.09
 ---- batch: 110 ----
mean loss: 297.42
train mean loss: 298.20
epoch train time: 0:00:00.723841
elapsed time: 0:01:13.004799
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 01:49:50.083412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.90
 ---- batch: 020 ----
mean loss: 295.58
 ---- batch: 030 ----
mean loss: 301.09
 ---- batch: 040 ----
mean loss: 292.09
 ---- batch: 050 ----
mean loss: 301.54
 ---- batch: 060 ----
mean loss: 297.86
 ---- batch: 070 ----
mean loss: 285.17
 ---- batch: 080 ----
mean loss: 281.70
 ---- batch: 090 ----
mean loss: 287.58
 ---- batch: 100 ----
mean loss: 283.50
 ---- batch: 110 ----
mean loss: 285.79
train mean loss: 291.69
epoch train time: 0:00:00.717000
elapsed time: 0:01:13.721948
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 01:49:50.800559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.46
 ---- batch: 020 ----
mean loss: 285.95
 ---- batch: 030 ----
mean loss: 282.39
 ---- batch: 040 ----
mean loss: 280.51
 ---- batch: 050 ----
mean loss: 287.94
 ---- batch: 060 ----
mean loss: 287.18
 ---- batch: 070 ----
mean loss: 285.75
 ---- batch: 080 ----
mean loss: 280.19
 ---- batch: 090 ----
mean loss: 289.34
 ---- batch: 100 ----
mean loss: 279.20
 ---- batch: 110 ----
mean loss: 293.35
train mean loss: 286.16
epoch train time: 0:00:00.723171
elapsed time: 0:01:14.445259
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 01:49:51.523870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.54
 ---- batch: 020 ----
mean loss: 288.28
 ---- batch: 030 ----
mean loss: 290.42
 ---- batch: 040 ----
mean loss: 280.86
 ---- batch: 050 ----
mean loss: 275.57
 ---- batch: 060 ----
mean loss: 270.64
 ---- batch: 070 ----
mean loss: 275.24
 ---- batch: 080 ----
mean loss: 287.50
 ---- batch: 090 ----
mean loss: 276.88
 ---- batch: 100 ----
mean loss: 288.10
 ---- batch: 110 ----
mean loss: 278.48
train mean loss: 281.20
epoch train time: 0:00:00.723261
elapsed time: 0:01:15.168661
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 01:49:52.247270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.26
 ---- batch: 020 ----
mean loss: 276.09
 ---- batch: 030 ----
mean loss: 286.19
 ---- batch: 040 ----
mean loss: 275.92
 ---- batch: 050 ----
mean loss: 271.76
 ---- batch: 060 ----
mean loss: 273.12
 ---- batch: 070 ----
mean loss: 264.58
 ---- batch: 080 ----
mean loss: 279.55
 ---- batch: 090 ----
mean loss: 283.06
 ---- batch: 100 ----
mean loss: 278.68
 ---- batch: 110 ----
mean loss: 281.39
train mean loss: 277.22
epoch train time: 0:00:00.721485
elapsed time: 0:01:15.890299
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 01:49:52.968923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.75
 ---- batch: 020 ----
mean loss: 263.75
 ---- batch: 030 ----
mean loss: 279.98
 ---- batch: 040 ----
mean loss: 272.75
 ---- batch: 050 ----
mean loss: 278.00
 ---- batch: 060 ----
mean loss: 271.83
 ---- batch: 070 ----
mean loss: 267.77
 ---- batch: 080 ----
mean loss: 273.31
 ---- batch: 090 ----
mean loss: 277.24
 ---- batch: 100 ----
mean loss: 272.60
 ---- batch: 110 ----
mean loss: 271.41
train mean loss: 273.52
epoch train time: 0:00:00.718555
elapsed time: 0:01:16.609018
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 01:49:53.687627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.26
 ---- batch: 020 ----
mean loss: 276.40
 ---- batch: 030 ----
mean loss: 268.73
 ---- batch: 040 ----
mean loss: 260.27
 ---- batch: 050 ----
mean loss: 272.26
 ---- batch: 060 ----
mean loss: 269.19
 ---- batch: 070 ----
mean loss: 278.81
 ---- batch: 080 ----
mean loss: 266.71
 ---- batch: 090 ----
mean loss: 279.71
 ---- batch: 100 ----
mean loss: 263.99
 ---- batch: 110 ----
mean loss: 278.54
train mean loss: 270.38
epoch train time: 0:00:00.716138
elapsed time: 0:01:17.325322
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 01:49:54.403949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.70
 ---- batch: 020 ----
mean loss: 256.83
 ---- batch: 030 ----
mean loss: 264.80
 ---- batch: 040 ----
mean loss: 274.56
 ---- batch: 050 ----
mean loss: 264.28
 ---- batch: 060 ----
mean loss: 266.73
 ---- batch: 070 ----
mean loss: 254.59
 ---- batch: 080 ----
mean loss: 273.91
 ---- batch: 090 ----
mean loss: 268.66
 ---- batch: 100 ----
mean loss: 271.10
 ---- batch: 110 ----
mean loss: 269.55
train mean loss: 267.26
epoch train time: 0:00:00.720298
elapsed time: 0:01:18.045770
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 01:49:55.124411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.91
 ---- batch: 020 ----
mean loss: 260.82
 ---- batch: 030 ----
mean loss: 268.05
 ---- batch: 040 ----
mean loss: 268.58
 ---- batch: 050 ----
mean loss: 258.57
 ---- batch: 060 ----
mean loss: 261.01
 ---- batch: 070 ----
mean loss: 272.83
 ---- batch: 080 ----
mean loss: 262.73
 ---- batch: 090 ----
mean loss: 261.66
 ---- batch: 100 ----
mean loss: 264.03
 ---- batch: 110 ----
mean loss: 275.02
train mean loss: 264.60
epoch train time: 0:00:00.719831
elapsed time: 0:01:18.765785
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 01:49:55.844396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.00
 ---- batch: 020 ----
mean loss: 265.59
 ---- batch: 030 ----
mean loss: 261.76
 ---- batch: 040 ----
mean loss: 248.77
 ---- batch: 050 ----
mean loss: 262.35
 ---- batch: 060 ----
mean loss: 264.33
 ---- batch: 070 ----
mean loss: 264.47
 ---- batch: 080 ----
mean loss: 265.12
 ---- batch: 090 ----
mean loss: 271.00
 ---- batch: 100 ----
mean loss: 259.23
 ---- batch: 110 ----
mean loss: 254.67
train mean loss: 262.17
epoch train time: 0:00:00.723797
elapsed time: 0:01:19.489766
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 01:49:56.568400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.28
 ---- batch: 020 ----
mean loss: 252.66
 ---- batch: 030 ----
mean loss: 260.89
 ---- batch: 040 ----
mean loss: 263.96
 ---- batch: 050 ----
mean loss: 266.38
 ---- batch: 060 ----
mean loss: 255.63
 ---- batch: 070 ----
mean loss: 265.06
 ---- batch: 080 ----
mean loss: 256.98
 ---- batch: 090 ----
mean loss: 252.59
 ---- batch: 100 ----
mean loss: 252.94
 ---- batch: 110 ----
mean loss: 263.38
train mean loss: 259.83
epoch train time: 0:00:00.729139
elapsed time: 0:01:20.219105
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 01:49:57.297722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.41
 ---- batch: 020 ----
mean loss: 258.64
 ---- batch: 030 ----
mean loss: 260.07
 ---- batch: 040 ----
mean loss: 254.28
 ---- batch: 050 ----
mean loss: 253.44
 ---- batch: 060 ----
mean loss: 263.11
 ---- batch: 070 ----
mean loss: 256.03
 ---- batch: 080 ----
mean loss: 262.12
 ---- batch: 090 ----
mean loss: 254.76
 ---- batch: 100 ----
mean loss: 258.68
 ---- batch: 110 ----
mean loss: 255.12
train mean loss: 257.72
epoch train time: 0:00:00.733043
elapsed time: 0:01:20.952293
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 01:49:58.030922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.86
 ---- batch: 020 ----
mean loss: 255.86
 ---- batch: 030 ----
mean loss: 245.84
 ---- batch: 040 ----
mean loss: 259.07
 ---- batch: 050 ----
mean loss: 257.55
 ---- batch: 060 ----
mean loss: 247.78
 ---- batch: 070 ----
mean loss: 251.13
 ---- batch: 080 ----
mean loss: 253.32
 ---- batch: 090 ----
mean loss: 258.26
 ---- batch: 100 ----
mean loss: 256.65
 ---- batch: 110 ----
mean loss: 264.20
train mean loss: 255.80
epoch train time: 0:00:00.717809
elapsed time: 0:01:21.670255
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 01:49:58.748886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.60
 ---- batch: 020 ----
mean loss: 258.93
 ---- batch: 030 ----
mean loss: 246.25
 ---- batch: 040 ----
mean loss: 257.02
 ---- batch: 050 ----
mean loss: 257.11
 ---- batch: 060 ----
mean loss: 261.39
 ---- batch: 070 ----
mean loss: 259.22
 ---- batch: 080 ----
mean loss: 246.89
 ---- batch: 090 ----
mean loss: 253.62
 ---- batch: 100 ----
mean loss: 241.51
 ---- batch: 110 ----
mean loss: 257.96
train mean loss: 253.69
epoch train time: 0:00:00.716821
elapsed time: 0:01:22.387266
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 01:49:59.467310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.00
 ---- batch: 020 ----
mean loss: 251.54
 ---- batch: 030 ----
mean loss: 254.26
 ---- batch: 040 ----
mean loss: 243.54
 ---- batch: 050 ----
mean loss: 250.24
 ---- batch: 060 ----
mean loss: 258.96
 ---- batch: 070 ----
mean loss: 251.58
 ---- batch: 080 ----
mean loss: 248.79
 ---- batch: 090 ----
mean loss: 252.18
 ---- batch: 100 ----
mean loss: 244.70
 ---- batch: 110 ----
mean loss: 255.78
train mean loss: 251.93
epoch train time: 0:00:00.725554
elapsed time: 0:01:23.114389
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 01:50:00.193004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.28
 ---- batch: 020 ----
mean loss: 252.14
 ---- batch: 030 ----
mean loss: 255.65
 ---- batch: 040 ----
mean loss: 254.65
 ---- batch: 050 ----
mean loss: 244.56
 ---- batch: 060 ----
mean loss: 241.68
 ---- batch: 070 ----
mean loss: 251.80
 ---- batch: 080 ----
mean loss: 251.36
 ---- batch: 090 ----
mean loss: 252.82
 ---- batch: 100 ----
mean loss: 251.73
 ---- batch: 110 ----
mean loss: 250.77
train mean loss: 250.04
epoch train time: 0:00:00.717397
elapsed time: 0:01:23.831992
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 01:50:00.910600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.16
 ---- batch: 020 ----
mean loss: 247.74
 ---- batch: 030 ----
mean loss: 250.82
 ---- batch: 040 ----
mean loss: 248.66
 ---- batch: 050 ----
mean loss: 232.41
 ---- batch: 060 ----
mean loss: 250.07
 ---- batch: 070 ----
mean loss: 253.17
 ---- batch: 080 ----
mean loss: 257.90
 ---- batch: 090 ----
mean loss: 246.32
 ---- batch: 100 ----
mean loss: 251.64
 ---- batch: 110 ----
mean loss: 247.82
train mean loss: 248.34
epoch train time: 0:00:00.721649
elapsed time: 0:01:24.553778
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 01:50:01.632393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.41
 ---- batch: 020 ----
mean loss: 247.62
 ---- batch: 030 ----
mean loss: 247.06
 ---- batch: 040 ----
mean loss: 251.76
 ---- batch: 050 ----
mean loss: 252.23
 ---- batch: 060 ----
mean loss: 244.04
 ---- batch: 070 ----
mean loss: 245.62
 ---- batch: 080 ----
mean loss: 246.08
 ---- batch: 090 ----
mean loss: 249.59
 ---- batch: 100 ----
mean loss: 247.62
 ---- batch: 110 ----
mean loss: 243.63
train mean loss: 246.66
epoch train time: 0:00:00.734168
elapsed time: 0:01:25.288092
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 01:50:02.366719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.81
 ---- batch: 020 ----
mean loss: 247.56
 ---- batch: 030 ----
mean loss: 241.50
 ---- batch: 040 ----
mean loss: 242.79
 ---- batch: 050 ----
mean loss: 238.92
 ---- batch: 060 ----
mean loss: 252.08
 ---- batch: 070 ----
mean loss: 243.74
 ---- batch: 080 ----
mean loss: 251.04
 ---- batch: 090 ----
mean loss: 246.05
 ---- batch: 100 ----
mean loss: 243.24
 ---- batch: 110 ----
mean loss: 243.04
train mean loss: 245.18
epoch train time: 0:00:00.717131
elapsed time: 0:01:26.005389
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 01:50:03.083999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.47
 ---- batch: 020 ----
mean loss: 239.01
 ---- batch: 030 ----
mean loss: 237.63
 ---- batch: 040 ----
mean loss: 241.02
 ---- batch: 050 ----
mean loss: 241.25
 ---- batch: 060 ----
mean loss: 250.54
 ---- batch: 070 ----
mean loss: 248.46
 ---- batch: 080 ----
mean loss: 246.84
 ---- batch: 090 ----
mean loss: 241.47
 ---- batch: 100 ----
mean loss: 244.46
 ---- batch: 110 ----
mean loss: 240.67
train mean loss: 243.79
epoch train time: 0:00:00.722863
elapsed time: 0:01:26.728402
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 01:50:03.807037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.44
 ---- batch: 020 ----
mean loss: 244.30
 ---- batch: 030 ----
mean loss: 250.99
 ---- batch: 040 ----
mean loss: 246.42
 ---- batch: 050 ----
mean loss: 250.94
 ---- batch: 060 ----
mean loss: 236.23
 ---- batch: 070 ----
mean loss: 232.02
 ---- batch: 080 ----
mean loss: 237.25
 ---- batch: 090 ----
mean loss: 241.68
 ---- batch: 100 ----
mean loss: 239.48
 ---- batch: 110 ----
mean loss: 243.81
train mean loss: 242.21
epoch train time: 0:00:00.722340
elapsed time: 0:01:27.450911
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 01:50:04.529523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.65
 ---- batch: 020 ----
mean loss: 236.86
 ---- batch: 030 ----
mean loss: 244.31
 ---- batch: 040 ----
mean loss: 245.72
 ---- batch: 050 ----
mean loss: 239.97
 ---- batch: 060 ----
mean loss: 244.10
 ---- batch: 070 ----
mean loss: 243.46
 ---- batch: 080 ----
mean loss: 237.61
 ---- batch: 090 ----
mean loss: 240.26
 ---- batch: 100 ----
mean loss: 232.84
 ---- batch: 110 ----
mean loss: 243.93
train mean loss: 240.67
epoch train time: 0:00:00.723149
elapsed time: 0:01:28.174194
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 01:50:05.252803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.28
 ---- batch: 020 ----
mean loss: 245.17
 ---- batch: 030 ----
mean loss: 234.04
 ---- batch: 040 ----
mean loss: 240.52
 ---- batch: 050 ----
mean loss: 245.92
 ---- batch: 060 ----
mean loss: 241.66
 ---- batch: 070 ----
mean loss: 234.32
 ---- batch: 080 ----
mean loss: 237.51
 ---- batch: 090 ----
mean loss: 242.79
 ---- batch: 100 ----
mean loss: 236.26
 ---- batch: 110 ----
mean loss: 233.26
train mean loss: 239.45
epoch train time: 0:00:00.720177
elapsed time: 0:01:28.894510
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 01:50:05.973116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.60
 ---- batch: 020 ----
mean loss: 237.19
 ---- batch: 030 ----
mean loss: 232.60
 ---- batch: 040 ----
mean loss: 237.74
 ---- batch: 050 ----
mean loss: 242.03
 ---- batch: 060 ----
mean loss: 246.08
 ---- batch: 070 ----
mean loss: 239.96
 ---- batch: 080 ----
mean loss: 244.95
 ---- batch: 090 ----
mean loss: 233.99
 ---- batch: 100 ----
mean loss: 235.59
 ---- batch: 110 ----
mean loss: 235.93
train mean loss: 238.07
epoch train time: 0:00:00.722595
elapsed time: 0:01:29.617249
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 01:50:06.695870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.78
 ---- batch: 020 ----
mean loss: 235.05
 ---- batch: 030 ----
mean loss: 236.17
 ---- batch: 040 ----
mean loss: 243.87
 ---- batch: 050 ----
mean loss: 238.52
 ---- batch: 060 ----
mean loss: 233.87
 ---- batch: 070 ----
mean loss: 237.13
 ---- batch: 080 ----
mean loss: 232.72
 ---- batch: 090 ----
mean loss: 246.54
 ---- batch: 100 ----
mean loss: 221.84
 ---- batch: 110 ----
mean loss: 246.33
train mean loss: 236.83
epoch train time: 0:00:00.718116
elapsed time: 0:01:30.335513
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 01:50:07.414124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.91
 ---- batch: 020 ----
mean loss: 242.68
 ---- batch: 030 ----
mean loss: 231.01
 ---- batch: 040 ----
mean loss: 229.22
 ---- batch: 050 ----
mean loss: 238.94
 ---- batch: 060 ----
mean loss: 236.56
 ---- batch: 070 ----
mean loss: 245.26
 ---- batch: 080 ----
mean loss: 231.73
 ---- batch: 090 ----
mean loss: 236.72
 ---- batch: 100 ----
mean loss: 240.58
 ---- batch: 110 ----
mean loss: 228.61
train mean loss: 235.62
epoch train time: 0:00:00.721425
elapsed time: 0:01:31.057087
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 01:50:08.135713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.53
 ---- batch: 020 ----
mean loss: 235.53
 ---- batch: 030 ----
mean loss: 230.52
 ---- batch: 040 ----
mean loss: 233.39
 ---- batch: 050 ----
mean loss: 235.43
 ---- batch: 060 ----
mean loss: 233.21
 ---- batch: 070 ----
mean loss: 229.94
 ---- batch: 080 ----
mean loss: 246.33
 ---- batch: 090 ----
mean loss: 238.04
 ---- batch: 100 ----
mean loss: 225.32
 ---- batch: 110 ----
mean loss: 241.91
train mean loss: 234.36
epoch train time: 0:00:00.719019
elapsed time: 0:01:31.776257
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 01:50:08.854864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.74
 ---- batch: 020 ----
mean loss: 238.69
 ---- batch: 030 ----
mean loss: 239.98
 ---- batch: 040 ----
mean loss: 237.63
 ---- batch: 050 ----
mean loss: 228.42
 ---- batch: 060 ----
mean loss: 231.71
 ---- batch: 070 ----
mean loss: 234.74
 ---- batch: 080 ----
mean loss: 231.96
 ---- batch: 090 ----
mean loss: 230.14
 ---- batch: 100 ----
mean loss: 230.65
 ---- batch: 110 ----
mean loss: 235.62
train mean loss: 233.47
epoch train time: 0:00:00.724507
elapsed time: 0:01:32.500913
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 01:50:09.579541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.21
 ---- batch: 020 ----
mean loss: 236.15
 ---- batch: 030 ----
mean loss: 227.71
 ---- batch: 040 ----
mean loss: 228.84
 ---- batch: 050 ----
mean loss: 234.58
 ---- batch: 060 ----
mean loss: 230.21
 ---- batch: 070 ----
mean loss: 222.52
 ---- batch: 080 ----
mean loss: 232.39
 ---- batch: 090 ----
mean loss: 231.81
 ---- batch: 100 ----
mean loss: 236.42
 ---- batch: 110 ----
mean loss: 231.71
train mean loss: 231.96
epoch train time: 0:00:00.724879
elapsed time: 0:01:33.225943
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 01:50:10.304550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.53
 ---- batch: 020 ----
mean loss: 235.79
 ---- batch: 030 ----
mean loss: 229.08
 ---- batch: 040 ----
mean loss: 224.81
 ---- batch: 050 ----
mean loss: 222.32
 ---- batch: 060 ----
mean loss: 234.65
 ---- batch: 070 ----
mean loss: 239.88
 ---- batch: 080 ----
mean loss: 240.02
 ---- batch: 090 ----
mean loss: 230.51
 ---- batch: 100 ----
mean loss: 232.90
 ---- batch: 110 ----
mean loss: 226.20
train mean loss: 231.00
epoch train time: 0:00:00.725305
elapsed time: 0:01:33.951385
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 01:50:11.029994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.87
 ---- batch: 020 ----
mean loss: 236.17
 ---- batch: 030 ----
mean loss: 224.65
 ---- batch: 040 ----
mean loss: 235.23
 ---- batch: 050 ----
mean loss: 235.35
 ---- batch: 060 ----
mean loss: 228.89
 ---- batch: 070 ----
mean loss: 229.08
 ---- batch: 080 ----
mean loss: 235.78
 ---- batch: 090 ----
mean loss: 231.58
 ---- batch: 100 ----
mean loss: 227.28
 ---- batch: 110 ----
mean loss: 223.59
train mean loss: 229.82
epoch train time: 0:00:00.734266
elapsed time: 0:01:34.685783
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 01:50:11.764393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.46
 ---- batch: 020 ----
mean loss: 240.46
 ---- batch: 030 ----
mean loss: 224.25
 ---- batch: 040 ----
mean loss: 227.81
 ---- batch: 050 ----
mean loss: 228.38
 ---- batch: 060 ----
mean loss: 223.96
 ---- batch: 070 ----
mean loss: 222.45
 ---- batch: 080 ----
mean loss: 222.46
 ---- batch: 090 ----
mean loss: 231.94
 ---- batch: 100 ----
mean loss: 231.16
 ---- batch: 110 ----
mean loss: 233.65
train mean loss: 228.83
epoch train time: 0:00:00.712053
elapsed time: 0:01:35.397972
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 01:50:12.476582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.81
 ---- batch: 020 ----
mean loss: 231.61
 ---- batch: 030 ----
mean loss: 224.43
 ---- batch: 040 ----
mean loss: 220.51
 ---- batch: 050 ----
mean loss: 228.59
 ---- batch: 060 ----
mean loss: 226.82
 ---- batch: 070 ----
mean loss: 227.17
 ---- batch: 080 ----
mean loss: 235.42
 ---- batch: 090 ----
mean loss: 226.63
 ---- batch: 100 ----
mean loss: 220.92
 ---- batch: 110 ----
mean loss: 227.34
train mean loss: 227.78
epoch train time: 0:00:00.723137
elapsed time: 0:01:36.121261
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 01:50:13.199910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.83
 ---- batch: 020 ----
mean loss: 223.35
 ---- batch: 030 ----
mean loss: 231.73
 ---- batch: 040 ----
mean loss: 227.04
 ---- batch: 050 ----
mean loss: 234.33
 ---- batch: 060 ----
mean loss: 220.04
 ---- batch: 070 ----
mean loss: 225.20
 ---- batch: 080 ----
mean loss: 231.62
 ---- batch: 090 ----
mean loss: 229.34
 ---- batch: 100 ----
mean loss: 230.80
 ---- batch: 110 ----
mean loss: 225.38
train mean loss: 226.69
epoch train time: 0:00:00.722846
elapsed time: 0:01:36.844291
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 01:50:13.922901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.59
 ---- batch: 020 ----
mean loss: 225.17
 ---- batch: 030 ----
mean loss: 230.25
 ---- batch: 040 ----
mean loss: 231.36
 ---- batch: 050 ----
mean loss: 222.14
 ---- batch: 060 ----
mean loss: 222.32
 ---- batch: 070 ----
mean loss: 229.09
 ---- batch: 080 ----
mean loss: 225.24
 ---- batch: 090 ----
mean loss: 226.92
 ---- batch: 100 ----
mean loss: 221.72
 ---- batch: 110 ----
mean loss: 219.98
train mean loss: 225.96
epoch train time: 0:00:00.723096
elapsed time: 0:01:37.567526
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 01:50:14.646137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.73
 ---- batch: 020 ----
mean loss: 221.20
 ---- batch: 030 ----
mean loss: 216.63
 ---- batch: 040 ----
mean loss: 224.07
 ---- batch: 050 ----
mean loss: 221.01
 ---- batch: 060 ----
mean loss: 231.76
 ---- batch: 070 ----
mean loss: 233.05
 ---- batch: 080 ----
mean loss: 229.33
 ---- batch: 090 ----
mean loss: 222.31
 ---- batch: 100 ----
mean loss: 223.77
 ---- batch: 110 ----
mean loss: 226.88
train mean loss: 224.99
epoch train time: 0:00:00.722289
elapsed time: 0:01:38.289949
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 01:50:15.368557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.13
 ---- batch: 020 ----
mean loss: 232.20
 ---- batch: 030 ----
mean loss: 230.45
 ---- batch: 040 ----
mean loss: 220.64
 ---- batch: 050 ----
mean loss: 226.15
 ---- batch: 060 ----
mean loss: 225.48
 ---- batch: 070 ----
mean loss: 224.96
 ---- batch: 080 ----
mean loss: 219.40
 ---- batch: 090 ----
mean loss: 221.10
 ---- batch: 100 ----
mean loss: 214.96
 ---- batch: 110 ----
mean loss: 226.49
train mean loss: 224.19
epoch train time: 0:00:00.723530
elapsed time: 0:01:39.013612
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 01:50:16.092220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.18
 ---- batch: 020 ----
mean loss: 228.49
 ---- batch: 030 ----
mean loss: 223.69
 ---- batch: 040 ----
mean loss: 230.23
 ---- batch: 050 ----
mean loss: 212.73
 ---- batch: 060 ----
mean loss: 220.22
 ---- batch: 070 ----
mean loss: 228.29
 ---- batch: 080 ----
mean loss: 221.75
 ---- batch: 090 ----
mean loss: 217.50
 ---- batch: 100 ----
mean loss: 218.01
 ---- batch: 110 ----
mean loss: 227.73
train mean loss: 223.39
epoch train time: 0:00:00.714245
elapsed time: 0:01:39.728001
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 01:50:16.806614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.89
 ---- batch: 020 ----
mean loss: 225.39
 ---- batch: 030 ----
mean loss: 223.39
 ---- batch: 040 ----
mean loss: 227.23
 ---- batch: 050 ----
mean loss: 220.39
 ---- batch: 060 ----
mean loss: 213.46
 ---- batch: 070 ----
mean loss: 222.80
 ---- batch: 080 ----
mean loss: 216.74
 ---- batch: 090 ----
mean loss: 227.39
 ---- batch: 100 ----
mean loss: 225.26
 ---- batch: 110 ----
mean loss: 223.38
train mean loss: 222.54
epoch train time: 0:00:00.721010
elapsed time: 0:01:40.449179
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 01:50:17.527789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.30
 ---- batch: 020 ----
mean loss: 230.48
 ---- batch: 030 ----
mean loss: 217.77
 ---- batch: 040 ----
mean loss: 220.28
 ---- batch: 050 ----
mean loss: 216.39
 ---- batch: 060 ----
mean loss: 218.92
 ---- batch: 070 ----
mean loss: 228.23
 ---- batch: 080 ----
mean loss: 228.48
 ---- batch: 090 ----
mean loss: 220.39
 ---- batch: 100 ----
mean loss: 221.63
 ---- batch: 110 ----
mean loss: 219.93
train mean loss: 221.95
epoch train time: 0:00:00.719865
elapsed time: 0:01:41.169175
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 01:50:18.247801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.85
 ---- batch: 020 ----
mean loss: 222.33
 ---- batch: 030 ----
mean loss: 223.54
 ---- batch: 040 ----
mean loss: 220.24
 ---- batch: 050 ----
mean loss: 225.59
 ---- batch: 060 ----
mean loss: 221.79
 ---- batch: 070 ----
mean loss: 224.91
 ---- batch: 080 ----
mean loss: 217.50
 ---- batch: 090 ----
mean loss: 225.29
 ---- batch: 100 ----
mean loss: 217.91
 ---- batch: 110 ----
mean loss: 215.61
train mean loss: 221.11
epoch train time: 0:00:00.722455
elapsed time: 0:01:41.891782
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 01:50:18.970390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.93
 ---- batch: 020 ----
mean loss: 222.13
 ---- batch: 030 ----
mean loss: 222.81
 ---- batch: 040 ----
mean loss: 226.03
 ---- batch: 050 ----
mean loss: 228.11
 ---- batch: 060 ----
mean loss: 215.27
 ---- batch: 070 ----
mean loss: 211.52
 ---- batch: 080 ----
mean loss: 214.50
 ---- batch: 090 ----
mean loss: 219.08
 ---- batch: 100 ----
mean loss: 220.15
 ---- batch: 110 ----
mean loss: 221.30
train mean loss: 220.35
epoch train time: 0:00:00.723329
elapsed time: 0:01:42.615250
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 01:50:19.693861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.53
 ---- batch: 020 ----
mean loss: 215.25
 ---- batch: 030 ----
mean loss: 199.24
 ---- batch: 040 ----
mean loss: 223.50
 ---- batch: 050 ----
mean loss: 234.18
 ---- batch: 060 ----
mean loss: 224.82
 ---- batch: 070 ----
mean loss: 221.77
 ---- batch: 080 ----
mean loss: 218.63
 ---- batch: 090 ----
mean loss: 217.53
 ---- batch: 100 ----
mean loss: 219.62
 ---- batch: 110 ----
mean loss: 225.04
train mean loss: 219.58
epoch train time: 0:00:00.716736
elapsed time: 0:01:43.332124
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 01:50:20.410732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.58
 ---- batch: 020 ----
mean loss: 211.01
 ---- batch: 030 ----
mean loss: 217.42
 ---- batch: 040 ----
mean loss: 217.01
 ---- batch: 050 ----
mean loss: 231.74
 ---- batch: 060 ----
mean loss: 214.39
 ---- batch: 070 ----
mean loss: 217.64
 ---- batch: 080 ----
mean loss: 226.54
 ---- batch: 090 ----
mean loss: 219.61
 ---- batch: 100 ----
mean loss: 222.08
 ---- batch: 110 ----
mean loss: 217.81
train mean loss: 218.91
epoch train time: 0:00:00.712954
elapsed time: 0:01:44.045209
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 01:50:21.123832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.89
 ---- batch: 020 ----
mean loss: 217.46
 ---- batch: 030 ----
mean loss: 218.34
 ---- batch: 040 ----
mean loss: 210.27
 ---- batch: 050 ----
mean loss: 217.82
 ---- batch: 060 ----
mean loss: 223.91
 ---- batch: 070 ----
mean loss: 218.47
 ---- batch: 080 ----
mean loss: 229.99
 ---- batch: 090 ----
mean loss: 220.99
 ---- batch: 100 ----
mean loss: 212.48
 ---- batch: 110 ----
mean loss: 214.68
train mean loss: 218.16
epoch train time: 0:00:00.727920
elapsed time: 0:01:44.773290
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 01:50:21.851917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.79
 ---- batch: 020 ----
mean loss: 224.67
 ---- batch: 030 ----
mean loss: 209.53
 ---- batch: 040 ----
mean loss: 226.10
 ---- batch: 050 ----
mean loss: 211.19
 ---- batch: 060 ----
mean loss: 220.82
 ---- batch: 070 ----
mean loss: 209.83
 ---- batch: 080 ----
mean loss: 206.64
 ---- batch: 090 ----
mean loss: 214.76
 ---- batch: 100 ----
mean loss: 220.67
 ---- batch: 110 ----
mean loss: 226.64
train mean loss: 217.48
epoch train time: 0:00:00.722387
elapsed time: 0:01:45.495878
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 01:50:22.574505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.41
 ---- batch: 020 ----
mean loss: 216.91
 ---- batch: 030 ----
mean loss: 225.58
 ---- batch: 040 ----
mean loss: 213.42
 ---- batch: 050 ----
mean loss: 217.80
 ---- batch: 060 ----
mean loss: 220.71
 ---- batch: 070 ----
mean loss: 215.33
 ---- batch: 080 ----
mean loss: 216.87
 ---- batch: 090 ----
mean loss: 208.54
 ---- batch: 100 ----
mean loss: 224.24
 ---- batch: 110 ----
mean loss: 216.50
train mean loss: 216.83
epoch train time: 0:00:00.726075
elapsed time: 0:01:46.222109
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 01:50:23.300720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.97
 ---- batch: 020 ----
mean loss: 218.71
 ---- batch: 030 ----
mean loss: 217.94
 ---- batch: 040 ----
mean loss: 214.26
 ---- batch: 050 ----
mean loss: 219.87
 ---- batch: 060 ----
mean loss: 208.60
 ---- batch: 070 ----
mean loss: 217.26
 ---- batch: 080 ----
mean loss: 219.57
 ---- batch: 090 ----
mean loss: 215.11
 ---- batch: 100 ----
mean loss: 208.58
 ---- batch: 110 ----
mean loss: 215.79
train mean loss: 216.11
epoch train time: 0:00:00.729950
elapsed time: 0:01:46.952197
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 01:50:24.030805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.82
 ---- batch: 020 ----
mean loss: 214.24
 ---- batch: 030 ----
mean loss: 220.88
 ---- batch: 040 ----
mean loss: 219.50
 ---- batch: 050 ----
mean loss: 212.02
 ---- batch: 060 ----
mean loss: 216.92
 ---- batch: 070 ----
mean loss: 212.12
 ---- batch: 080 ----
mean loss: 220.00
 ---- batch: 090 ----
mean loss: 208.06
 ---- batch: 100 ----
mean loss: 212.48
 ---- batch: 110 ----
mean loss: 212.65
train mean loss: 215.46
epoch train time: 0:00:00.722892
elapsed time: 0:01:47.675222
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 01:50:24.753862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.21
 ---- batch: 020 ----
mean loss: 211.63
 ---- batch: 030 ----
mean loss: 214.57
 ---- batch: 040 ----
mean loss: 212.34
 ---- batch: 050 ----
mean loss: 205.86
 ---- batch: 060 ----
mean loss: 218.16
 ---- batch: 070 ----
mean loss: 210.31
 ---- batch: 080 ----
mean loss: 210.46
 ---- batch: 090 ----
mean loss: 211.09
 ---- batch: 100 ----
mean loss: 217.67
 ---- batch: 110 ----
mean loss: 224.68
train mean loss: 214.74
epoch train time: 0:00:00.725597
elapsed time: 0:01:48.400984
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 01:50:25.479591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.05
 ---- batch: 020 ----
mean loss: 216.28
 ---- batch: 030 ----
mean loss: 214.76
 ---- batch: 040 ----
mean loss: 210.90
 ---- batch: 050 ----
mean loss: 213.61
 ---- batch: 060 ----
mean loss: 209.17
 ---- batch: 070 ----
mean loss: 218.52
 ---- batch: 080 ----
mean loss: 207.80
 ---- batch: 090 ----
mean loss: 213.50
 ---- batch: 100 ----
mean loss: 220.48
 ---- batch: 110 ----
mean loss: 221.88
train mean loss: 214.08
epoch train time: 0:00:00.727122
elapsed time: 0:01:49.128244
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 01:50:26.206856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.55
 ---- batch: 020 ----
mean loss: 213.51
 ---- batch: 030 ----
mean loss: 210.61
 ---- batch: 040 ----
mean loss: 207.55
 ---- batch: 050 ----
mean loss: 205.71
 ---- batch: 060 ----
mean loss: 219.21
 ---- batch: 070 ----
mean loss: 213.94
 ---- batch: 080 ----
mean loss: 210.66
 ---- batch: 090 ----
mean loss: 211.75
 ---- batch: 100 ----
mean loss: 222.69
 ---- batch: 110 ----
mean loss: 211.57
train mean loss: 213.35
epoch train time: 0:00:00.723021
elapsed time: 0:01:49.851418
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 01:50:26.930017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.41
 ---- batch: 020 ----
mean loss: 223.55
 ---- batch: 030 ----
mean loss: 204.75
 ---- batch: 040 ----
mean loss: 211.84
 ---- batch: 050 ----
mean loss: 216.62
 ---- batch: 060 ----
mean loss: 210.20
 ---- batch: 070 ----
mean loss: 207.99
 ---- batch: 080 ----
mean loss: 216.56
 ---- batch: 090 ----
mean loss: 216.59
 ---- batch: 100 ----
mean loss: 207.21
 ---- batch: 110 ----
mean loss: 216.22
train mean loss: 212.76
epoch train time: 0:00:00.723161
elapsed time: 0:01:50.574704
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 01:50:27.653329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.56
 ---- batch: 020 ----
mean loss: 215.58
 ---- batch: 030 ----
mean loss: 216.97
 ---- batch: 040 ----
mean loss: 208.58
 ---- batch: 050 ----
mean loss: 219.74
 ---- batch: 060 ----
mean loss: 206.90
 ---- batch: 070 ----
mean loss: 221.22
 ---- batch: 080 ----
mean loss: 212.82
 ---- batch: 090 ----
mean loss: 210.85
 ---- batch: 100 ----
mean loss: 201.91
 ---- batch: 110 ----
mean loss: 204.12
train mean loss: 211.95
epoch train time: 0:00:00.721948
elapsed time: 0:01:51.296805
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 01:50:28.375422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.92
 ---- batch: 020 ----
mean loss: 216.93
 ---- batch: 030 ----
mean loss: 202.15
 ---- batch: 040 ----
mean loss: 205.63
 ---- batch: 050 ----
mean loss: 213.94
 ---- batch: 060 ----
mean loss: 211.10
 ---- batch: 070 ----
mean loss: 211.73
 ---- batch: 080 ----
mean loss: 211.73
 ---- batch: 090 ----
mean loss: 216.32
 ---- batch: 100 ----
mean loss: 216.23
 ---- batch: 110 ----
mean loss: 211.23
train mean loss: 211.44
epoch train time: 0:00:00.722830
elapsed time: 0:01:52.019779
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 01:50:29.098389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.40
 ---- batch: 020 ----
mean loss: 205.85
 ---- batch: 030 ----
mean loss: 215.65
 ---- batch: 040 ----
mean loss: 204.07
 ---- batch: 050 ----
mean loss: 213.04
 ---- batch: 060 ----
mean loss: 206.62
 ---- batch: 070 ----
mean loss: 217.60
 ---- batch: 080 ----
mean loss: 210.98
 ---- batch: 090 ----
mean loss: 205.60
 ---- batch: 100 ----
mean loss: 213.54
 ---- batch: 110 ----
mean loss: 215.24
train mean loss: 210.71
epoch train time: 0:00:00.727770
elapsed time: 0:01:52.747705
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 01:50:29.826315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.63
 ---- batch: 020 ----
mean loss: 208.46
 ---- batch: 030 ----
mean loss: 208.51
 ---- batch: 040 ----
mean loss: 202.19
 ---- batch: 050 ----
mean loss: 217.17
 ---- batch: 060 ----
mean loss: 207.86
 ---- batch: 070 ----
mean loss: 211.05
 ---- batch: 080 ----
mean loss: 219.44
 ---- batch: 090 ----
mean loss: 207.47
 ---- batch: 100 ----
mean loss: 206.59
 ---- batch: 110 ----
mean loss: 213.86
train mean loss: 210.22
epoch train time: 0:00:00.721320
elapsed time: 0:01:53.469163
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 01:50:30.547773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.05
 ---- batch: 020 ----
mean loss: 208.08
 ---- batch: 030 ----
mean loss: 210.71
 ---- batch: 040 ----
mean loss: 208.20
 ---- batch: 050 ----
mean loss: 202.71
 ---- batch: 060 ----
mean loss: 214.21
 ---- batch: 070 ----
mean loss: 212.12
 ---- batch: 080 ----
mean loss: 214.20
 ---- batch: 090 ----
mean loss: 217.24
 ---- batch: 100 ----
mean loss: 198.37
 ---- batch: 110 ----
mean loss: 209.83
train mean loss: 209.55
epoch train time: 0:00:00.720003
elapsed time: 0:01:54.189301
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 01:50:31.267910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.30
 ---- batch: 020 ----
mean loss: 198.40
 ---- batch: 030 ----
mean loss: 209.80
 ---- batch: 040 ----
mean loss: 197.13
 ---- batch: 050 ----
mean loss: 209.42
 ---- batch: 060 ----
mean loss: 213.98
 ---- batch: 070 ----
mean loss: 209.99
 ---- batch: 080 ----
mean loss: 213.41
 ---- batch: 090 ----
mean loss: 211.15
 ---- batch: 100 ----
mean loss: 208.96
 ---- batch: 110 ----
mean loss: 216.90
train mean loss: 208.79
epoch train time: 0:00:00.718739
elapsed time: 0:01:54.908176
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 01:50:31.986800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.49
 ---- batch: 020 ----
mean loss: 217.82
 ---- batch: 030 ----
mean loss: 205.59
 ---- batch: 040 ----
mean loss: 206.51
 ---- batch: 050 ----
mean loss: 215.34
 ---- batch: 060 ----
mean loss: 223.57
 ---- batch: 070 ----
mean loss: 211.10
 ---- batch: 080 ----
mean loss: 197.32
 ---- batch: 090 ----
mean loss: 205.12
 ---- batch: 100 ----
mean loss: 208.12
 ---- batch: 110 ----
mean loss: 203.69
train mean loss: 208.73
epoch train time: 0:00:00.723241
elapsed time: 0:01:55.631572
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 01:50:32.710182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.82
 ---- batch: 020 ----
mean loss: 216.90
 ---- batch: 030 ----
mean loss: 211.18
 ---- batch: 040 ----
mean loss: 199.14
 ---- batch: 050 ----
mean loss: 210.00
 ---- batch: 060 ----
mean loss: 206.89
 ---- batch: 070 ----
mean loss: 202.99
 ---- batch: 080 ----
mean loss: 213.49
 ---- batch: 090 ----
mean loss: 210.92
 ---- batch: 100 ----
mean loss: 198.38
 ---- batch: 110 ----
mean loss: 200.73
train mean loss: 207.79
epoch train time: 0:00:00.718996
elapsed time: 0:01:56.350704
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 01:50:33.429313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.50
 ---- batch: 020 ----
mean loss: 207.80
 ---- batch: 030 ----
mean loss: 203.67
 ---- batch: 040 ----
mean loss: 204.67
 ---- batch: 050 ----
mean loss: 205.54
 ---- batch: 060 ----
mean loss: 207.43
 ---- batch: 070 ----
mean loss: 214.14
 ---- batch: 080 ----
mean loss: 212.13
 ---- batch: 090 ----
mean loss: 206.83
 ---- batch: 100 ----
mean loss: 212.88
 ---- batch: 110 ----
mean loss: 203.97
train mean loss: 207.32
epoch train time: 0:00:00.722661
elapsed time: 0:01:57.073500
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 01:50:34.152109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.34
 ---- batch: 020 ----
mean loss: 210.54
 ---- batch: 030 ----
mean loss: 207.43
 ---- batch: 040 ----
mean loss: 205.50
 ---- batch: 050 ----
mean loss: 203.77
 ---- batch: 060 ----
mean loss: 204.39
 ---- batch: 070 ----
mean loss: 204.37
 ---- batch: 080 ----
mean loss: 217.27
 ---- batch: 090 ----
mean loss: 204.73
 ---- batch: 100 ----
mean loss: 204.74
 ---- batch: 110 ----
mean loss: 198.65
train mean loss: 206.65
epoch train time: 0:00:00.720011
elapsed time: 0:01:57.793647
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 01:50:34.872259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.56
 ---- batch: 020 ----
mean loss: 205.22
 ---- batch: 030 ----
mean loss: 208.92
 ---- batch: 040 ----
mean loss: 202.74
 ---- batch: 050 ----
mean loss: 207.57
 ---- batch: 060 ----
mean loss: 203.74
 ---- batch: 070 ----
mean loss: 203.17
 ---- batch: 080 ----
mean loss: 210.38
 ---- batch: 090 ----
mean loss: 211.85
 ---- batch: 100 ----
mean loss: 207.61
 ---- batch: 110 ----
mean loss: 203.04
train mean loss: 206.19
epoch train time: 0:00:00.728146
elapsed time: 0:01:58.521936
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 01:50:35.600544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.14
 ---- batch: 020 ----
mean loss: 199.82
 ---- batch: 030 ----
mean loss: 205.20
 ---- batch: 040 ----
mean loss: 208.02
 ---- batch: 050 ----
mean loss: 209.28
 ---- batch: 060 ----
mean loss: 213.02
 ---- batch: 070 ----
mean loss: 204.00
 ---- batch: 080 ----
mean loss: 204.79
 ---- batch: 090 ----
mean loss: 212.18
 ---- batch: 100 ----
mean loss: 203.30
 ---- batch: 110 ----
mean loss: 205.74
train mean loss: 206.01
epoch train time: 0:00:00.725938
elapsed time: 0:01:59.248010
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 01:50:36.326617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.04
 ---- batch: 020 ----
mean loss: 192.00
 ---- batch: 030 ----
mean loss: 211.27
 ---- batch: 040 ----
mean loss: 208.45
 ---- batch: 050 ----
mean loss: 202.36
 ---- batch: 060 ----
mean loss: 203.86
 ---- batch: 070 ----
mean loss: 209.15
 ---- batch: 080 ----
mean loss: 196.58
 ---- batch: 090 ----
mean loss: 212.07
 ---- batch: 100 ----
mean loss: 204.91
 ---- batch: 110 ----
mean loss: 207.69
train mean loss: 205.42
epoch train time: 0:00:00.732067
elapsed time: 0:01:59.980214
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 01:50:37.058840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.66
 ---- batch: 020 ----
mean loss: 203.49
 ---- batch: 030 ----
mean loss: 203.82
 ---- batch: 040 ----
mean loss: 208.23
 ---- batch: 050 ----
mean loss: 198.12
 ---- batch: 060 ----
mean loss: 203.69
 ---- batch: 070 ----
mean loss: 203.01
 ---- batch: 080 ----
mean loss: 207.19
 ---- batch: 090 ----
mean loss: 205.29
 ---- batch: 100 ----
mean loss: 209.88
 ---- batch: 110 ----
mean loss: 207.24
train mean loss: 204.93
epoch train time: 0:00:00.730599
elapsed time: 0:02:00.710977
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 01:50:37.789596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.57
 ---- batch: 020 ----
mean loss: 211.76
 ---- batch: 030 ----
mean loss: 200.69
 ---- batch: 040 ----
mean loss: 208.65
 ---- batch: 050 ----
mean loss: 210.83
 ---- batch: 060 ----
mean loss: 204.78
 ---- batch: 070 ----
mean loss: 209.43
 ---- batch: 080 ----
mean loss: 208.86
 ---- batch: 090 ----
mean loss: 187.45
 ---- batch: 100 ----
mean loss: 209.64
 ---- batch: 110 ----
mean loss: 195.35
train mean loss: 204.64
epoch train time: 0:00:00.724883
elapsed time: 0:02:01.436005
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 01:50:38.514612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.76
 ---- batch: 020 ----
mean loss: 198.46
 ---- batch: 030 ----
mean loss: 202.02
 ---- batch: 040 ----
mean loss: 205.44
 ---- batch: 050 ----
mean loss: 204.06
 ---- batch: 060 ----
mean loss: 204.98
 ---- batch: 070 ----
mean loss: 203.72
 ---- batch: 080 ----
mean loss: 208.14
 ---- batch: 090 ----
mean loss: 212.07
 ---- batch: 100 ----
mean loss: 202.66
 ---- batch: 110 ----
mean loss: 196.72
train mean loss: 204.13
epoch train time: 0:00:00.717760
elapsed time: 0:02:02.153895
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 01:50:39.232502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.81
 ---- batch: 020 ----
mean loss: 199.92
 ---- batch: 030 ----
mean loss: 205.23
 ---- batch: 040 ----
mean loss: 211.04
 ---- batch: 050 ----
mean loss: 212.37
 ---- batch: 060 ----
mean loss: 206.53
 ---- batch: 070 ----
mean loss: 206.21
 ---- batch: 080 ----
mean loss: 201.52
 ---- batch: 090 ----
mean loss: 202.33
 ---- batch: 100 ----
mean loss: 194.68
 ---- batch: 110 ----
mean loss: 198.69
train mean loss: 203.83
epoch train time: 0:00:00.725191
elapsed time: 0:02:02.879216
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 01:50:39.957823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.76
 ---- batch: 020 ----
mean loss: 200.22
 ---- batch: 030 ----
mean loss: 207.08
 ---- batch: 040 ----
mean loss: 211.34
 ---- batch: 050 ----
mean loss: 196.78
 ---- batch: 060 ----
mean loss: 205.92
 ---- batch: 070 ----
mean loss: 211.38
 ---- batch: 080 ----
mean loss: 207.20
 ---- batch: 090 ----
mean loss: 197.28
 ---- batch: 100 ----
mean loss: 194.76
 ---- batch: 110 ----
mean loss: 204.16
train mean loss: 203.58
epoch train time: 0:00:00.723337
elapsed time: 0:02:03.602685
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 01:50:40.681294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.89
 ---- batch: 020 ----
mean loss: 199.60
 ---- batch: 030 ----
mean loss: 201.51
 ---- batch: 040 ----
mean loss: 206.30
 ---- batch: 050 ----
mean loss: 202.52
 ---- batch: 060 ----
mean loss: 200.59
 ---- batch: 070 ----
mean loss: 205.64
 ---- batch: 080 ----
mean loss: 209.11
 ---- batch: 090 ----
mean loss: 203.63
 ---- batch: 100 ----
mean loss: 202.57
 ---- batch: 110 ----
mean loss: 200.34
train mean loss: 203.41
epoch train time: 0:00:00.715867
elapsed time: 0:02:04.318701
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 01:50:41.397319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.55
 ---- batch: 020 ----
mean loss: 203.95
 ---- batch: 030 ----
mean loss: 202.52
 ---- batch: 040 ----
mean loss: 189.51
 ---- batch: 050 ----
mean loss: 215.54
 ---- batch: 060 ----
mean loss: 203.34
 ---- batch: 070 ----
mean loss: 201.80
 ---- batch: 080 ----
mean loss: 201.51
 ---- batch: 090 ----
mean loss: 205.46
 ---- batch: 100 ----
mean loss: 199.94
 ---- batch: 110 ----
mean loss: 201.14
train mean loss: 202.97
epoch train time: 0:00:00.722782
elapsed time: 0:02:05.041626
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 01:50:42.120233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.81
 ---- batch: 020 ----
mean loss: 197.62
 ---- batch: 030 ----
mean loss: 201.58
 ---- batch: 040 ----
mean loss: 202.73
 ---- batch: 050 ----
mean loss: 209.49
 ---- batch: 060 ----
mean loss: 202.62
 ---- batch: 070 ----
mean loss: 203.03
 ---- batch: 080 ----
mean loss: 204.22
 ---- batch: 090 ----
mean loss: 195.77
 ---- batch: 100 ----
mean loss: 208.93
 ---- batch: 110 ----
mean loss: 195.25
train mean loss: 202.71
epoch train time: 0:00:00.722980
elapsed time: 0:02:05.764741
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 01:50:42.843371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.17
 ---- batch: 020 ----
mean loss: 210.84
 ---- batch: 030 ----
mean loss: 201.14
 ---- batch: 040 ----
mean loss: 202.47
 ---- batch: 050 ----
mean loss: 207.06
 ---- batch: 060 ----
mean loss: 205.53
 ---- batch: 070 ----
mean loss: 195.31
 ---- batch: 080 ----
mean loss: 202.70
 ---- batch: 090 ----
mean loss: 201.01
 ---- batch: 100 ----
mean loss: 201.68
 ---- batch: 110 ----
mean loss: 202.12
train mean loss: 202.35
epoch train time: 0:00:00.732637
elapsed time: 0:02:06.497544
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 01:50:43.576155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.95
 ---- batch: 020 ----
mean loss: 200.36
 ---- batch: 030 ----
mean loss: 202.04
 ---- batch: 040 ----
mean loss: 210.89
 ---- batch: 050 ----
mean loss: 195.35
 ---- batch: 060 ----
mean loss: 198.36
 ---- batch: 070 ----
mean loss: 208.44
 ---- batch: 080 ----
mean loss: 205.51
 ---- batch: 090 ----
mean loss: 204.73
 ---- batch: 100 ----
mean loss: 198.12
 ---- batch: 110 ----
mean loss: 199.11
train mean loss: 202.03
epoch train time: 0:00:00.727609
elapsed time: 0:02:07.225308
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 01:50:44.303919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.70
 ---- batch: 020 ----
mean loss: 201.55
 ---- batch: 030 ----
mean loss: 206.04
 ---- batch: 040 ----
mean loss: 202.30
 ---- batch: 050 ----
mean loss: 204.94
 ---- batch: 060 ----
mean loss: 203.55
 ---- batch: 070 ----
mean loss: 201.40
 ---- batch: 080 ----
mean loss: 198.92
 ---- batch: 090 ----
mean loss: 194.67
 ---- batch: 100 ----
mean loss: 205.45
 ---- batch: 110 ----
mean loss: 201.48
train mean loss: 202.17
epoch train time: 0:00:00.719189
elapsed time: 0:02:07.944631
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 01:50:45.023239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.87
 ---- batch: 020 ----
mean loss: 206.65
 ---- batch: 030 ----
mean loss: 204.73
 ---- batch: 040 ----
mean loss: 204.15
 ---- batch: 050 ----
mean loss: 200.02
 ---- batch: 060 ----
mean loss: 204.22
 ---- batch: 070 ----
mean loss: 208.40
 ---- batch: 080 ----
mean loss: 198.18
 ---- batch: 090 ----
mean loss: 191.53
 ---- batch: 100 ----
mean loss: 198.53
 ---- batch: 110 ----
mean loss: 196.84
train mean loss: 201.50
epoch train time: 0:00:00.717356
elapsed time: 0:02:08.662116
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 01:50:45.740723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.33
 ---- batch: 020 ----
mean loss: 200.73
 ---- batch: 030 ----
mean loss: 201.13
 ---- batch: 040 ----
mean loss: 194.57
 ---- batch: 050 ----
mean loss: 201.47
 ---- batch: 060 ----
mean loss: 193.12
 ---- batch: 070 ----
mean loss: 199.38
 ---- batch: 080 ----
mean loss: 199.16
 ---- batch: 090 ----
mean loss: 207.79
 ---- batch: 100 ----
mean loss: 192.32
 ---- batch: 110 ----
mean loss: 205.44
train mean loss: 201.26
epoch train time: 0:00:00.715815
elapsed time: 0:02:09.378064
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 01:50:46.456693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.72
 ---- batch: 020 ----
mean loss: 204.31
 ---- batch: 030 ----
mean loss: 210.14
 ---- batch: 040 ----
mean loss: 202.30
 ---- batch: 050 ----
mean loss: 202.09
 ---- batch: 060 ----
mean loss: 203.93
 ---- batch: 070 ----
mean loss: 200.57
 ---- batch: 080 ----
mean loss: 194.17
 ---- batch: 090 ----
mean loss: 197.52
 ---- batch: 100 ----
mean loss: 200.29
 ---- batch: 110 ----
mean loss: 196.23
train mean loss: 201.15
epoch train time: 0:00:00.725380
elapsed time: 0:02:10.103606
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 01:50:47.182215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.11
 ---- batch: 020 ----
mean loss: 207.14
 ---- batch: 030 ----
mean loss: 208.42
 ---- batch: 040 ----
mean loss: 200.83
 ---- batch: 050 ----
mean loss: 197.59
 ---- batch: 060 ----
mean loss: 197.33
 ---- batch: 070 ----
mean loss: 198.88
 ---- batch: 080 ----
mean loss: 197.34
 ---- batch: 090 ----
mean loss: 206.17
 ---- batch: 100 ----
mean loss: 196.04
 ---- batch: 110 ----
mean loss: 200.72
train mean loss: 200.91
epoch train time: 0:00:00.732627
elapsed time: 0:02:10.836375
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 01:50:47.914988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.53
 ---- batch: 020 ----
mean loss: 205.08
 ---- batch: 030 ----
mean loss: 201.16
 ---- batch: 040 ----
mean loss: 202.29
 ---- batch: 050 ----
mean loss: 194.57
 ---- batch: 060 ----
mean loss: 194.57
 ---- batch: 070 ----
mean loss: 203.95
 ---- batch: 080 ----
mean loss: 197.55
 ---- batch: 090 ----
mean loss: 206.07
 ---- batch: 100 ----
mean loss: 192.89
 ---- batch: 110 ----
mean loss: 194.20
train mean loss: 200.62
epoch train time: 0:00:00.724595
elapsed time: 0:02:11.561109
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 01:50:48.639717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.72
 ---- batch: 020 ----
mean loss: 203.16
 ---- batch: 030 ----
mean loss: 201.99
 ---- batch: 040 ----
mean loss: 203.55
 ---- batch: 050 ----
mean loss: 203.65
 ---- batch: 060 ----
mean loss: 205.35
 ---- batch: 070 ----
mean loss: 190.81
 ---- batch: 080 ----
mean loss: 204.40
 ---- batch: 090 ----
mean loss: 191.33
 ---- batch: 100 ----
mean loss: 205.63
 ---- batch: 110 ----
mean loss: 202.86
train mean loss: 200.28
epoch train time: 0:00:00.724974
elapsed time: 0:02:12.286231
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 01:50:49.364838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.18
 ---- batch: 020 ----
mean loss: 207.90
 ---- batch: 030 ----
mean loss: 195.20
 ---- batch: 040 ----
mean loss: 204.50
 ---- batch: 050 ----
mean loss: 191.36
 ---- batch: 060 ----
mean loss: 205.68
 ---- batch: 070 ----
mean loss: 190.89
 ---- batch: 080 ----
mean loss: 200.39
 ---- batch: 090 ----
mean loss: 192.49
 ---- batch: 100 ----
mean loss: 207.41
 ---- batch: 110 ----
mean loss: 196.24
train mean loss: 200.05
epoch train time: 0:00:00.720948
elapsed time: 0:02:13.007331
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 01:50:50.085940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.68
 ---- batch: 020 ----
mean loss: 208.57
 ---- batch: 030 ----
mean loss: 201.19
 ---- batch: 040 ----
mean loss: 206.73
 ---- batch: 050 ----
mean loss: 202.30
 ---- batch: 060 ----
mean loss: 203.48
 ---- batch: 070 ----
mean loss: 196.62
 ---- batch: 080 ----
mean loss: 193.42
 ---- batch: 090 ----
mean loss: 201.11
 ---- batch: 100 ----
mean loss: 188.45
 ---- batch: 110 ----
mean loss: 202.88
train mean loss: 199.93
epoch train time: 0:00:00.720021
elapsed time: 0:02:13.727487
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 01:50:50.806094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.03
 ---- batch: 020 ----
mean loss: 199.48
 ---- batch: 030 ----
mean loss: 208.77
 ---- batch: 040 ----
mean loss: 204.21
 ---- batch: 050 ----
mean loss: 205.70
 ---- batch: 060 ----
mean loss: 190.28
 ---- batch: 070 ----
mean loss: 203.40
 ---- batch: 080 ----
mean loss: 200.27
 ---- batch: 090 ----
mean loss: 208.29
 ---- batch: 100 ----
mean loss: 195.69
 ---- batch: 110 ----
mean loss: 191.75
train mean loss: 199.82
epoch train time: 0:00:00.720063
elapsed time: 0:02:14.447684
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 01:50:51.526315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.67
 ---- batch: 020 ----
mean loss: 206.99
 ---- batch: 030 ----
mean loss: 197.94
 ---- batch: 040 ----
mean loss: 194.92
 ---- batch: 050 ----
mean loss: 194.51
 ---- batch: 060 ----
mean loss: 203.43
 ---- batch: 070 ----
mean loss: 199.49
 ---- batch: 080 ----
mean loss: 201.46
 ---- batch: 090 ----
mean loss: 198.81
 ---- batch: 100 ----
mean loss: 197.62
 ---- batch: 110 ----
mean loss: 194.75
train mean loss: 199.40
epoch train time: 0:00:00.715741
elapsed time: 0:02:15.163580
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 01:50:52.242186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.11
 ---- batch: 020 ----
mean loss: 209.28
 ---- batch: 030 ----
mean loss: 194.67
 ---- batch: 040 ----
mean loss: 198.69
 ---- batch: 050 ----
mean loss: 196.11
 ---- batch: 060 ----
mean loss: 199.39
 ---- batch: 070 ----
mean loss: 196.70
 ---- batch: 080 ----
mean loss: 197.72
 ---- batch: 090 ----
mean loss: 192.00
 ---- batch: 100 ----
mean loss: 204.76
 ---- batch: 110 ----
mean loss: 207.30
train mean loss: 199.29
epoch train time: 0:00:00.715546
elapsed time: 0:02:15.879277
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 01:50:52.957886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.21
 ---- batch: 020 ----
mean loss: 193.01
 ---- batch: 030 ----
mean loss: 191.47
 ---- batch: 040 ----
mean loss: 197.35
 ---- batch: 050 ----
mean loss: 207.52
 ---- batch: 060 ----
mean loss: 190.36
 ---- batch: 070 ----
mean loss: 202.53
 ---- batch: 080 ----
mean loss: 207.42
 ---- batch: 090 ----
mean loss: 202.57
 ---- batch: 100 ----
mean loss: 204.67
 ---- batch: 110 ----
mean loss: 190.39
train mean loss: 199.00
epoch train time: 0:00:00.714801
elapsed time: 0:02:16.594220
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 01:50:53.672835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.23
 ---- batch: 020 ----
mean loss: 195.34
 ---- batch: 030 ----
mean loss: 201.08
 ---- batch: 040 ----
mean loss: 204.87
 ---- batch: 050 ----
mean loss: 196.36
 ---- batch: 060 ----
mean loss: 199.65
 ---- batch: 070 ----
mean loss: 192.74
 ---- batch: 080 ----
mean loss: 206.40
 ---- batch: 090 ----
mean loss: 198.04
 ---- batch: 100 ----
mean loss: 202.07
 ---- batch: 110 ----
mean loss: 196.55
train mean loss: 198.83
epoch train time: 0:00:00.725198
elapsed time: 0:02:17.319559
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 01:50:54.398166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.64
 ---- batch: 020 ----
mean loss: 205.44
 ---- batch: 030 ----
mean loss: 205.17
 ---- batch: 040 ----
mean loss: 197.33
 ---- batch: 050 ----
mean loss: 203.81
 ---- batch: 060 ----
mean loss: 202.36
 ---- batch: 070 ----
mean loss: 196.54
 ---- batch: 080 ----
mean loss: 198.05
 ---- batch: 090 ----
mean loss: 195.53
 ---- batch: 100 ----
mean loss: 198.02
 ---- batch: 110 ----
mean loss: 198.36
train mean loss: 198.53
epoch train time: 0:00:00.725315
elapsed time: 0:02:18.045002
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 01:50:55.123610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.74
 ---- batch: 020 ----
mean loss: 202.95
 ---- batch: 030 ----
mean loss: 192.74
 ---- batch: 040 ----
mean loss: 201.64
 ---- batch: 050 ----
mean loss: 193.97
 ---- batch: 060 ----
mean loss: 199.76
 ---- batch: 070 ----
mean loss: 202.05
 ---- batch: 080 ----
mean loss: 193.16
 ---- batch: 090 ----
mean loss: 200.00
 ---- batch: 100 ----
mean loss: 195.11
 ---- batch: 110 ----
mean loss: 198.89
train mean loss: 198.50
epoch train time: 0:00:00.724946
elapsed time: 0:02:18.770085
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 01:50:55.848713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.85
 ---- batch: 020 ----
mean loss: 203.33
 ---- batch: 030 ----
mean loss: 197.36
 ---- batch: 040 ----
mean loss: 196.93
 ---- batch: 050 ----
mean loss: 205.26
 ---- batch: 060 ----
mean loss: 195.30
 ---- batch: 070 ----
mean loss: 198.38
 ---- batch: 080 ----
mean loss: 196.07
 ---- batch: 090 ----
mean loss: 197.27
 ---- batch: 100 ----
mean loss: 206.43
 ---- batch: 110 ----
mean loss: 187.54
train mean loss: 198.13
epoch train time: 0:00:00.721952
elapsed time: 0:02:19.492189
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 01:50:56.570800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.67
 ---- batch: 020 ----
mean loss: 200.40
 ---- batch: 030 ----
mean loss: 203.68
 ---- batch: 040 ----
mean loss: 193.01
 ---- batch: 050 ----
mean loss: 199.51
 ---- batch: 060 ----
mean loss: 188.84
 ---- batch: 070 ----
mean loss: 196.61
 ---- batch: 080 ----
mean loss: 204.78
 ---- batch: 090 ----
mean loss: 202.82
 ---- batch: 100 ----
mean loss: 203.03
 ---- batch: 110 ----
mean loss: 190.73
train mean loss: 197.92
epoch train time: 0:00:00.719046
elapsed time: 0:02:20.211381
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 01:50:57.289979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.44
 ---- batch: 020 ----
mean loss: 195.85
 ---- batch: 030 ----
mean loss: 189.19
 ---- batch: 040 ----
mean loss: 185.47
 ---- batch: 050 ----
mean loss: 194.77
 ---- batch: 060 ----
mean loss: 200.24
 ---- batch: 070 ----
mean loss: 200.87
 ---- batch: 080 ----
mean loss: 200.81
 ---- batch: 090 ----
mean loss: 201.91
 ---- batch: 100 ----
mean loss: 203.17
 ---- batch: 110 ----
mean loss: 196.36
train mean loss: 197.69
epoch train time: 0:00:00.722734
elapsed time: 0:02:20.934241
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 01:50:58.012856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.73
 ---- batch: 020 ----
mean loss: 193.16
 ---- batch: 030 ----
mean loss: 202.85
 ---- batch: 040 ----
mean loss: 189.46
 ---- batch: 050 ----
mean loss: 199.11
 ---- batch: 060 ----
mean loss: 206.48
 ---- batch: 070 ----
mean loss: 192.16
 ---- batch: 080 ----
mean loss: 201.09
 ---- batch: 090 ----
mean loss: 199.44
 ---- batch: 100 ----
mean loss: 191.36
 ---- batch: 110 ----
mean loss: 195.15
train mean loss: 197.66
epoch train time: 0:00:00.723342
elapsed time: 0:02:21.657723
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 01:50:58.736333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.38
 ---- batch: 020 ----
mean loss: 193.14
 ---- batch: 030 ----
mean loss: 200.62
 ---- batch: 040 ----
mean loss: 197.92
 ---- batch: 050 ----
mean loss: 194.65
 ---- batch: 060 ----
mean loss: 201.36
 ---- batch: 070 ----
mean loss: 200.04
 ---- batch: 080 ----
mean loss: 195.46
 ---- batch: 090 ----
mean loss: 198.42
 ---- batch: 100 ----
mean loss: 202.18
 ---- batch: 110 ----
mean loss: 192.33
train mean loss: 197.60
epoch train time: 0:00:00.710919
elapsed time: 0:02:22.368790
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 01:50:59.447410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.63
 ---- batch: 020 ----
mean loss: 194.38
 ---- batch: 030 ----
mean loss: 195.48
 ---- batch: 040 ----
mean loss: 187.09
 ---- batch: 050 ----
mean loss: 198.76
 ---- batch: 060 ----
mean loss: 195.25
 ---- batch: 070 ----
mean loss: 189.98
 ---- batch: 080 ----
mean loss: 206.04
 ---- batch: 090 ----
mean loss: 202.65
 ---- batch: 100 ----
mean loss: 196.82
 ---- batch: 110 ----
mean loss: 191.80
train mean loss: 197.08
epoch train time: 0:00:00.722739
elapsed time: 0:02:23.091675
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 01:51:00.170286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.05
 ---- batch: 020 ----
mean loss: 186.88
 ---- batch: 030 ----
mean loss: 197.74
 ---- batch: 040 ----
mean loss: 195.35
 ---- batch: 050 ----
mean loss: 203.14
 ---- batch: 060 ----
mean loss: 199.87
 ---- batch: 070 ----
mean loss: 200.32
 ---- batch: 080 ----
mean loss: 195.09
 ---- batch: 090 ----
mean loss: 193.19
 ---- batch: 100 ----
mean loss: 194.84
 ---- batch: 110 ----
mean loss: 198.72
train mean loss: 196.91
epoch train time: 0:00:00.721978
elapsed time: 0:02:23.813787
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 01:51:00.892458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.56
 ---- batch: 020 ----
mean loss: 192.91
 ---- batch: 030 ----
mean loss: 199.15
 ---- batch: 040 ----
mean loss: 194.82
 ---- batch: 050 ----
mean loss: 198.06
 ---- batch: 060 ----
mean loss: 192.64
 ---- batch: 070 ----
mean loss: 197.63
 ---- batch: 080 ----
mean loss: 195.51
 ---- batch: 090 ----
mean loss: 200.74
 ---- batch: 100 ----
mean loss: 192.40
 ---- batch: 110 ----
mean loss: 201.87
train mean loss: 196.54
epoch train time: 0:00:00.721376
elapsed time: 0:02:24.535374
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 01:51:01.613980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.61
 ---- batch: 020 ----
mean loss: 193.04
 ---- batch: 030 ----
mean loss: 194.76
 ---- batch: 040 ----
mean loss: 197.64
 ---- batch: 050 ----
mean loss: 190.90
 ---- batch: 060 ----
mean loss: 194.03
 ---- batch: 070 ----
mean loss: 202.23
 ---- batch: 080 ----
mean loss: 194.48
 ---- batch: 090 ----
mean loss: 201.52
 ---- batch: 100 ----
mean loss: 198.11
 ---- batch: 110 ----
mean loss: 199.60
train mean loss: 196.61
epoch train time: 0:00:00.714509
elapsed time: 0:02:25.250014
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 01:51:02.328621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.48
 ---- batch: 020 ----
mean loss: 190.74
 ---- batch: 030 ----
mean loss: 202.09
 ---- batch: 040 ----
mean loss: 207.43
 ---- batch: 050 ----
mean loss: 193.59
 ---- batch: 060 ----
mean loss: 198.24
 ---- batch: 070 ----
mean loss: 196.64
 ---- batch: 080 ----
mean loss: 193.48
 ---- batch: 090 ----
mean loss: 190.05
 ---- batch: 100 ----
mean loss: 198.90
 ---- batch: 110 ----
mean loss: 192.41
train mean loss: 196.44
epoch train time: 0:00:00.717221
elapsed time: 0:02:25.967392
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 01:51:03.046003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.16
 ---- batch: 020 ----
mean loss: 195.99
 ---- batch: 030 ----
mean loss: 198.77
 ---- batch: 040 ----
mean loss: 194.23
 ---- batch: 050 ----
mean loss: 188.75
 ---- batch: 060 ----
mean loss: 202.69
 ---- batch: 070 ----
mean loss: 189.65
 ---- batch: 080 ----
mean loss: 195.76
 ---- batch: 090 ----
mean loss: 199.31
 ---- batch: 100 ----
mean loss: 199.30
 ---- batch: 110 ----
mean loss: 200.12
train mean loss: 196.21
epoch train time: 0:00:00.719280
elapsed time: 0:02:26.686805
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 01:51:03.765429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.35
 ---- batch: 020 ----
mean loss: 191.81
 ---- batch: 030 ----
mean loss: 200.03
 ---- batch: 040 ----
mean loss: 188.81
 ---- batch: 050 ----
mean loss: 194.61
 ---- batch: 060 ----
mean loss: 193.67
 ---- batch: 070 ----
mean loss: 195.45
 ---- batch: 080 ----
mean loss: 191.20
 ---- batch: 090 ----
mean loss: 190.09
 ---- batch: 100 ----
mean loss: 199.35
 ---- batch: 110 ----
mean loss: 205.41
train mean loss: 195.84
epoch train time: 0:00:00.721250
elapsed time: 0:02:27.408209
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 01:51:04.486847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.86
 ---- batch: 020 ----
mean loss: 193.79
 ---- batch: 030 ----
mean loss: 190.03
 ---- batch: 040 ----
mean loss: 199.73
 ---- batch: 050 ----
mean loss: 197.35
 ---- batch: 060 ----
mean loss: 191.83
 ---- batch: 070 ----
mean loss: 200.52
 ---- batch: 080 ----
mean loss: 194.68
 ---- batch: 090 ----
mean loss: 191.26
 ---- batch: 100 ----
mean loss: 204.97
 ---- batch: 110 ----
mean loss: 192.65
train mean loss: 195.86
epoch train time: 0:00:00.720246
elapsed time: 0:02:28.128631
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 01:51:05.207256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.40
 ---- batch: 020 ----
mean loss: 193.16
 ---- batch: 030 ----
mean loss: 190.31
 ---- batch: 040 ----
mean loss: 195.56
 ---- batch: 050 ----
mean loss: 211.94
 ---- batch: 060 ----
mean loss: 188.49
 ---- batch: 070 ----
mean loss: 192.45
 ---- batch: 080 ----
mean loss: 203.99
 ---- batch: 090 ----
mean loss: 200.58
 ---- batch: 100 ----
mean loss: 185.13
 ---- batch: 110 ----
mean loss: 188.42
train mean loss: 195.35
epoch train time: 0:00:00.724823
elapsed time: 0:02:28.853621
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 01:51:05.932228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.51
 ---- batch: 020 ----
mean loss: 199.08
 ---- batch: 030 ----
mean loss: 197.90
 ---- batch: 040 ----
mean loss: 197.17
 ---- batch: 050 ----
mean loss: 204.86
 ---- batch: 060 ----
mean loss: 193.44
 ---- batch: 070 ----
mean loss: 193.74
 ---- batch: 080 ----
mean loss: 192.75
 ---- batch: 090 ----
mean loss: 195.05
 ---- batch: 100 ----
mean loss: 201.41
 ---- batch: 110 ----
mean loss: 193.58
train mean loss: 195.25
epoch train time: 0:00:00.720284
elapsed time: 0:02:29.574071
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 01:51:06.652681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.93
 ---- batch: 020 ----
mean loss: 198.38
 ---- batch: 030 ----
mean loss: 199.23
 ---- batch: 040 ----
mean loss: 188.11
 ---- batch: 050 ----
mean loss: 208.53
 ---- batch: 060 ----
mean loss: 196.62
 ---- batch: 070 ----
mean loss: 186.72
 ---- batch: 080 ----
mean loss: 177.66
 ---- batch: 090 ----
mean loss: 192.81
 ---- batch: 100 ----
mean loss: 199.27
 ---- batch: 110 ----
mean loss: 201.09
train mean loss: 195.37
epoch train time: 0:00:00.720152
elapsed time: 0:02:30.294358
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 01:51:07.372966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.55
 ---- batch: 020 ----
mean loss: 201.00
 ---- batch: 030 ----
mean loss: 199.79
 ---- batch: 040 ----
mean loss: 201.40
 ---- batch: 050 ----
mean loss: 195.74
 ---- batch: 060 ----
mean loss: 193.46
 ---- batch: 070 ----
mean loss: 196.44
 ---- batch: 080 ----
mean loss: 194.54
 ---- batch: 090 ----
mean loss: 186.60
 ---- batch: 100 ----
mean loss: 184.44
 ---- batch: 110 ----
mean loss: 192.87
train mean loss: 194.84
epoch train time: 0:00:00.719876
elapsed time: 0:02:31.014371
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 01:51:08.092980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.83
 ---- batch: 020 ----
mean loss: 196.82
 ---- batch: 030 ----
mean loss: 191.54
 ---- batch: 040 ----
mean loss: 195.24
 ---- batch: 050 ----
mean loss: 198.03
 ---- batch: 060 ----
mean loss: 191.10
 ---- batch: 070 ----
mean loss: 208.48
 ---- batch: 080 ----
mean loss: 191.91
 ---- batch: 090 ----
mean loss: 186.78
 ---- batch: 100 ----
mean loss: 196.01
 ---- batch: 110 ----
mean loss: 193.67
train mean loss: 194.77
epoch train time: 0:00:00.720718
elapsed time: 0:02:31.735241
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 01:51:08.813850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.50
 ---- batch: 020 ----
mean loss: 187.93
 ---- batch: 030 ----
mean loss: 198.75
 ---- batch: 040 ----
mean loss: 194.08
 ---- batch: 050 ----
mean loss: 206.53
 ---- batch: 060 ----
mean loss: 193.05
 ---- batch: 070 ----
mean loss: 194.32
 ---- batch: 080 ----
mean loss: 195.35
 ---- batch: 090 ----
mean loss: 196.92
 ---- batch: 100 ----
mean loss: 189.69
 ---- batch: 110 ----
mean loss: 184.69
train mean loss: 194.53
epoch train time: 0:00:00.718296
elapsed time: 0:02:32.453667
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 01:51:09.532273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.35
 ---- batch: 020 ----
mean loss: 194.39
 ---- batch: 030 ----
mean loss: 193.52
 ---- batch: 040 ----
mean loss: 191.03
 ---- batch: 050 ----
mean loss: 189.06
 ---- batch: 060 ----
mean loss: 195.14
 ---- batch: 070 ----
mean loss: 181.92
 ---- batch: 080 ----
mean loss: 203.70
 ---- batch: 090 ----
mean loss: 198.77
 ---- batch: 100 ----
mean loss: 210.27
 ---- batch: 110 ----
mean loss: 188.22
train mean loss: 194.44
epoch train time: 0:00:00.719815
elapsed time: 0:02:33.173614
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 01:51:10.252245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.32
 ---- batch: 020 ----
mean loss: 192.90
 ---- batch: 030 ----
mean loss: 190.15
 ---- batch: 040 ----
mean loss: 186.66
 ---- batch: 050 ----
mean loss: 197.46
 ---- batch: 060 ----
mean loss: 190.43
 ---- batch: 070 ----
mean loss: 190.91
 ---- batch: 080 ----
mean loss: 190.67
 ---- batch: 090 ----
mean loss: 202.27
 ---- batch: 100 ----
mean loss: 199.39
 ---- batch: 110 ----
mean loss: 198.42
train mean loss: 194.02
epoch train time: 0:00:00.714230
elapsed time: 0:02:33.888002
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 01:51:10.966610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.01
 ---- batch: 020 ----
mean loss: 199.71
 ---- batch: 030 ----
mean loss: 195.23
 ---- batch: 040 ----
mean loss: 206.29
 ---- batch: 050 ----
mean loss: 203.39
 ---- batch: 060 ----
mean loss: 189.60
 ---- batch: 070 ----
mean loss: 185.75
 ---- batch: 080 ----
mean loss: 190.45
 ---- batch: 090 ----
mean loss: 187.40
 ---- batch: 100 ----
mean loss: 187.99
 ---- batch: 110 ----
mean loss: 190.07
train mean loss: 193.96
epoch train time: 0:00:00.717954
elapsed time: 0:02:34.606102
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 01:51:11.684727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.52
 ---- batch: 020 ----
mean loss: 194.74
 ---- batch: 030 ----
mean loss: 184.22
 ---- batch: 040 ----
mean loss: 204.91
 ---- batch: 050 ----
mean loss: 196.67
 ---- batch: 060 ----
mean loss: 196.40
 ---- batch: 070 ----
mean loss: 192.07
 ---- batch: 080 ----
mean loss: 201.46
 ---- batch: 090 ----
mean loss: 188.67
 ---- batch: 100 ----
mean loss: 195.50
 ---- batch: 110 ----
mean loss: 187.28
train mean loss: 193.81
epoch train time: 0:00:00.715256
elapsed time: 0:02:35.321507
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 01:51:12.400113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.90
 ---- batch: 020 ----
mean loss: 199.28
 ---- batch: 030 ----
mean loss: 197.24
 ---- batch: 040 ----
mean loss: 189.65
 ---- batch: 050 ----
mean loss: 190.15
 ---- batch: 060 ----
mean loss: 197.95
 ---- batch: 070 ----
mean loss: 191.71
 ---- batch: 080 ----
mean loss: 196.59
 ---- batch: 090 ----
mean loss: 196.27
 ---- batch: 100 ----
mean loss: 188.83
 ---- batch: 110 ----
mean loss: 187.27
train mean loss: 193.40
epoch train time: 0:00:00.717526
elapsed time: 0:02:36.039164
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 01:51:13.117769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.67
 ---- batch: 020 ----
mean loss: 205.23
 ---- batch: 030 ----
mean loss: 200.37
 ---- batch: 040 ----
mean loss: 191.68
 ---- batch: 050 ----
mean loss: 190.76
 ---- batch: 060 ----
mean loss: 186.77
 ---- batch: 070 ----
mean loss: 201.14
 ---- batch: 080 ----
mean loss: 188.78
 ---- batch: 090 ----
mean loss: 195.29
 ---- batch: 100 ----
mean loss: 191.09
 ---- batch: 110 ----
mean loss: 191.89
train mean loss: 193.15
epoch train time: 0:00:00.723946
elapsed time: 0:02:36.763237
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 01:51:13.841844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.56
 ---- batch: 020 ----
mean loss: 200.99
 ---- batch: 030 ----
mean loss: 190.24
 ---- batch: 040 ----
mean loss: 184.39
 ---- batch: 050 ----
mean loss: 192.58
 ---- batch: 060 ----
mean loss: 191.25
 ---- batch: 070 ----
mean loss: 184.32
 ---- batch: 080 ----
mean loss: 196.23
 ---- batch: 090 ----
mean loss: 200.42
 ---- batch: 100 ----
mean loss: 189.66
 ---- batch: 110 ----
mean loss: 197.89
train mean loss: 192.83
epoch train time: 0:00:00.714791
elapsed time: 0:02:37.478160
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 01:51:14.556788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.82
 ---- batch: 020 ----
mean loss: 195.32
 ---- batch: 030 ----
mean loss: 199.13
 ---- batch: 040 ----
mean loss: 194.24
 ---- batch: 050 ----
mean loss: 185.64
 ---- batch: 060 ----
mean loss: 189.71
 ---- batch: 070 ----
mean loss: 193.01
 ---- batch: 080 ----
mean loss: 188.58
 ---- batch: 090 ----
mean loss: 201.84
 ---- batch: 100 ----
mean loss: 182.58
 ---- batch: 110 ----
mean loss: 195.09
train mean loss: 192.80
epoch train time: 0:00:00.716259
elapsed time: 0:02:38.194584
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 01:51:15.273187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.26
 ---- batch: 020 ----
mean loss: 199.61
 ---- batch: 030 ----
mean loss: 191.98
 ---- batch: 040 ----
mean loss: 193.69
 ---- batch: 050 ----
mean loss: 196.98
 ---- batch: 060 ----
mean loss: 193.35
 ---- batch: 070 ----
mean loss: 190.75
 ---- batch: 080 ----
mean loss: 183.27
 ---- batch: 090 ----
mean loss: 189.14
 ---- batch: 100 ----
mean loss: 197.11
 ---- batch: 110 ----
mean loss: 191.67
train mean loss: 192.35
epoch train time: 0:00:00.725293
elapsed time: 0:02:38.920007
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 01:51:15.998614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.46
 ---- batch: 020 ----
mean loss: 195.10
 ---- batch: 030 ----
mean loss: 189.70
 ---- batch: 040 ----
mean loss: 190.21
 ---- batch: 050 ----
mean loss: 187.81
 ---- batch: 060 ----
mean loss: 193.50
 ---- batch: 070 ----
mean loss: 194.73
 ---- batch: 080 ----
mean loss: 189.95
 ---- batch: 090 ----
mean loss: 197.09
 ---- batch: 100 ----
mean loss: 192.43
 ---- batch: 110 ----
mean loss: 184.67
train mean loss: 192.34
epoch train time: 0:00:00.719334
elapsed time: 0:02:39.639481
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 01:51:16.718103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.73
 ---- batch: 020 ----
mean loss: 193.50
 ---- batch: 030 ----
mean loss: 183.58
 ---- batch: 040 ----
mean loss: 203.13
 ---- batch: 050 ----
mean loss: 194.41
 ---- batch: 060 ----
mean loss: 194.36
 ---- batch: 070 ----
mean loss: 190.19
 ---- batch: 080 ----
mean loss: 192.27
 ---- batch: 090 ----
mean loss: 186.70
 ---- batch: 100 ----
mean loss: 191.25
 ---- batch: 110 ----
mean loss: 187.71
train mean loss: 191.98
epoch train time: 0:00:00.732576
elapsed time: 0:02:40.372227
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 01:51:17.450853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.96
 ---- batch: 020 ----
mean loss: 201.35
 ---- batch: 030 ----
mean loss: 196.83
 ---- batch: 040 ----
mean loss: 184.25
 ---- batch: 050 ----
mean loss: 192.54
 ---- batch: 060 ----
mean loss: 188.17
 ---- batch: 070 ----
mean loss: 185.71
 ---- batch: 080 ----
mean loss: 193.13
 ---- batch: 090 ----
mean loss: 195.69
 ---- batch: 100 ----
mean loss: 191.62
 ---- batch: 110 ----
mean loss: 182.80
train mean loss: 191.78
epoch train time: 0:00:00.722696
elapsed time: 0:02:41.095114
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 01:51:18.173725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.02
 ---- batch: 020 ----
mean loss: 199.73
 ---- batch: 030 ----
mean loss: 193.40
 ---- batch: 040 ----
mean loss: 187.90
 ---- batch: 050 ----
mean loss: 184.42
 ---- batch: 060 ----
mean loss: 195.42
 ---- batch: 070 ----
mean loss: 190.16
 ---- batch: 080 ----
mean loss: 186.42
 ---- batch: 090 ----
mean loss: 194.31
 ---- batch: 100 ----
mean loss: 188.12
 ---- batch: 110 ----
mean loss: 196.58
train mean loss: 191.64
epoch train time: 0:00:00.717475
elapsed time: 0:02:41.812725
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 01:51:18.891349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.54
 ---- batch: 020 ----
mean loss: 185.39
 ---- batch: 030 ----
mean loss: 195.21
 ---- batch: 040 ----
mean loss: 197.07
 ---- batch: 050 ----
mean loss: 197.26
 ---- batch: 060 ----
mean loss: 186.25
 ---- batch: 070 ----
mean loss: 196.17
 ---- batch: 080 ----
mean loss: 191.10
 ---- batch: 090 ----
mean loss: 186.72
 ---- batch: 100 ----
mean loss: 189.13
 ---- batch: 110 ----
mean loss: 198.68
train mean loss: 191.54
epoch train time: 0:00:00.713485
elapsed time: 0:02:42.526364
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 01:51:19.604991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.89
 ---- batch: 020 ----
mean loss: 198.66
 ---- batch: 030 ----
mean loss: 184.74
 ---- batch: 040 ----
mean loss: 184.21
 ---- batch: 050 ----
mean loss: 199.53
 ---- batch: 060 ----
mean loss: 195.52
 ---- batch: 070 ----
mean loss: 191.56
 ---- batch: 080 ----
mean loss: 192.51
 ---- batch: 090 ----
mean loss: 184.94
 ---- batch: 100 ----
mean loss: 192.17
 ---- batch: 110 ----
mean loss: 193.73
train mean loss: 191.10
epoch train time: 0:00:00.717550
elapsed time: 0:02:43.244067
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 01:51:20.322675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.44
 ---- batch: 020 ----
mean loss: 192.26
 ---- batch: 030 ----
mean loss: 202.93
 ---- batch: 040 ----
mean loss: 181.74
 ---- batch: 050 ----
mean loss: 188.56
 ---- batch: 060 ----
mean loss: 182.27
 ---- batch: 070 ----
mean loss: 188.03
 ---- batch: 080 ----
mean loss: 190.61
 ---- batch: 090 ----
mean loss: 194.59
 ---- batch: 100 ----
mean loss: 194.67
 ---- batch: 110 ----
mean loss: 191.44
train mean loss: 191.01
epoch train time: 0:00:00.718900
elapsed time: 0:02:43.963125
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 01:51:21.041740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.94
 ---- batch: 020 ----
mean loss: 196.74
 ---- batch: 030 ----
mean loss: 190.92
 ---- batch: 040 ----
mean loss: 196.43
 ---- batch: 050 ----
mean loss: 191.31
 ---- batch: 060 ----
mean loss: 193.70
 ---- batch: 070 ----
mean loss: 190.38
 ---- batch: 080 ----
mean loss: 186.63
 ---- batch: 090 ----
mean loss: 190.38
 ---- batch: 100 ----
mean loss: 185.76
 ---- batch: 110 ----
mean loss: 181.98
train mean loss: 190.75
epoch train time: 0:00:00.726126
elapsed time: 0:02:44.689408
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 01:51:21.768062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.52
 ---- batch: 020 ----
mean loss: 192.75
 ---- batch: 030 ----
mean loss: 183.10
 ---- batch: 040 ----
mean loss: 194.34
 ---- batch: 050 ----
mean loss: 181.57
 ---- batch: 060 ----
mean loss: 186.99
 ---- batch: 070 ----
mean loss: 191.61
 ---- batch: 080 ----
mean loss: 194.12
 ---- batch: 090 ----
mean loss: 200.77
 ---- batch: 100 ----
mean loss: 193.70
 ---- batch: 110 ----
mean loss: 186.05
train mean loss: 190.33
epoch train time: 0:00:00.723632
elapsed time: 0:02:45.413234
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 01:51:22.491842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.74
 ---- batch: 020 ----
mean loss: 194.74
 ---- batch: 030 ----
mean loss: 193.56
 ---- batch: 040 ----
mean loss: 187.30
 ---- batch: 050 ----
mean loss: 194.99
 ---- batch: 060 ----
mean loss: 195.65
 ---- batch: 070 ----
mean loss: 190.12
 ---- batch: 080 ----
mean loss: 187.85
 ---- batch: 090 ----
mean loss: 182.47
 ---- batch: 100 ----
mean loss: 186.09
 ---- batch: 110 ----
mean loss: 195.18
train mean loss: 190.17
epoch train time: 0:00:00.717681
elapsed time: 0:02:46.131064
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 01:51:23.209675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.21
 ---- batch: 020 ----
mean loss: 181.55
 ---- batch: 030 ----
mean loss: 198.49
 ---- batch: 040 ----
mean loss: 188.12
 ---- batch: 050 ----
mean loss: 191.44
 ---- batch: 060 ----
mean loss: 191.41
 ---- batch: 070 ----
mean loss: 190.43
 ---- batch: 080 ----
mean loss: 184.98
 ---- batch: 090 ----
mean loss: 188.39
 ---- batch: 100 ----
mean loss: 193.58
 ---- batch: 110 ----
mean loss: 194.99
train mean loss: 190.10
epoch train time: 0:00:00.718970
elapsed time: 0:02:46.850202
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 01:51:23.928813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.89
 ---- batch: 020 ----
mean loss: 198.80
 ---- batch: 030 ----
mean loss: 195.26
 ---- batch: 040 ----
mean loss: 184.89
 ---- batch: 050 ----
mean loss: 189.30
 ---- batch: 060 ----
mean loss: 189.61
 ---- batch: 070 ----
mean loss: 193.49
 ---- batch: 080 ----
mean loss: 187.19
 ---- batch: 090 ----
mean loss: 181.78
 ---- batch: 100 ----
mean loss: 182.53
 ---- batch: 110 ----
mean loss: 189.87
train mean loss: 189.86
epoch train time: 0:00:00.723552
elapsed time: 0:02:47.573901
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 01:51:24.652535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.52
 ---- batch: 020 ----
mean loss: 186.21
 ---- batch: 030 ----
mean loss: 189.97
 ---- batch: 040 ----
mean loss: 193.52
 ---- batch: 050 ----
mean loss: 193.66
 ---- batch: 060 ----
mean loss: 196.44
 ---- batch: 070 ----
mean loss: 187.59
 ---- batch: 080 ----
mean loss: 184.27
 ---- batch: 090 ----
mean loss: 195.17
 ---- batch: 100 ----
mean loss: 187.26
 ---- batch: 110 ----
mean loss: 189.98
train mean loss: 189.91
epoch train time: 0:00:00.736183
elapsed time: 0:02:48.310241
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 01:51:25.388850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.33
 ---- batch: 020 ----
mean loss: 192.29
 ---- batch: 030 ----
mean loss: 200.76
 ---- batch: 040 ----
mean loss: 177.59
 ---- batch: 050 ----
mean loss: 190.14
 ---- batch: 060 ----
mean loss: 185.10
 ---- batch: 070 ----
mean loss: 190.98
 ---- batch: 080 ----
mean loss: 186.04
 ---- batch: 090 ----
mean loss: 193.72
 ---- batch: 100 ----
mean loss: 190.04
 ---- batch: 110 ----
mean loss: 185.72
train mean loss: 189.53
epoch train time: 0:00:00.740488
elapsed time: 0:02:49.050862
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 01:51:26.129480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.37
 ---- batch: 020 ----
mean loss: 189.66
 ---- batch: 030 ----
mean loss: 201.03
 ---- batch: 040 ----
mean loss: 189.63
 ---- batch: 050 ----
mean loss: 186.76
 ---- batch: 060 ----
mean loss: 191.88
 ---- batch: 070 ----
mean loss: 186.78
 ---- batch: 080 ----
mean loss: 197.22
 ---- batch: 090 ----
mean loss: 189.55
 ---- batch: 100 ----
mean loss: 179.86
 ---- batch: 110 ----
mean loss: 182.71
train mean loss: 189.41
epoch train time: 0:00:00.747361
elapsed time: 0:02:49.798379
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 01:51:26.876990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.31
 ---- batch: 020 ----
mean loss: 186.93
 ---- batch: 030 ----
mean loss: 196.99
 ---- batch: 040 ----
mean loss: 185.97
 ---- batch: 050 ----
mean loss: 183.86
 ---- batch: 060 ----
mean loss: 195.89
 ---- batch: 070 ----
mean loss: 196.04
 ---- batch: 080 ----
mean loss: 185.41
 ---- batch: 090 ----
mean loss: 185.59
 ---- batch: 100 ----
mean loss: 191.69
 ---- batch: 110 ----
mean loss: 194.81
train mean loss: 189.44
epoch train time: 0:00:00.732457
elapsed time: 0:02:50.530967
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 01:51:27.609572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.23
 ---- batch: 020 ----
mean loss: 183.79
 ---- batch: 030 ----
mean loss: 188.41
 ---- batch: 040 ----
mean loss: 191.35
 ---- batch: 050 ----
mean loss: 197.39
 ---- batch: 060 ----
mean loss: 192.03
 ---- batch: 070 ----
mean loss: 186.03
 ---- batch: 080 ----
mean loss: 184.15
 ---- batch: 090 ----
mean loss: 193.98
 ---- batch: 100 ----
mean loss: 190.69
 ---- batch: 110 ----
mean loss: 185.58
train mean loss: 189.12
epoch train time: 0:00:00.745126
elapsed time: 0:02:51.276224
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 01:51:28.354837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.17
 ---- batch: 020 ----
mean loss: 187.17
 ---- batch: 030 ----
mean loss: 186.88
 ---- batch: 040 ----
mean loss: 191.78
 ---- batch: 050 ----
mean loss: 193.50
 ---- batch: 060 ----
mean loss: 197.41
 ---- batch: 070 ----
mean loss: 182.87
 ---- batch: 080 ----
mean loss: 190.55
 ---- batch: 090 ----
mean loss: 183.46
 ---- batch: 100 ----
mean loss: 184.17
 ---- batch: 110 ----
mean loss: 185.57
train mean loss: 189.03
epoch train time: 0:00:00.743358
elapsed time: 0:02:52.019755
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 01:51:29.098386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.89
 ---- batch: 020 ----
mean loss: 191.05
 ---- batch: 030 ----
mean loss: 188.03
 ---- batch: 040 ----
mean loss: 190.88
 ---- batch: 050 ----
mean loss: 194.30
 ---- batch: 060 ----
mean loss: 181.54
 ---- batch: 070 ----
mean loss: 182.30
 ---- batch: 080 ----
mean loss: 185.90
 ---- batch: 090 ----
mean loss: 182.76
 ---- batch: 100 ----
mean loss: 195.81
 ---- batch: 110 ----
mean loss: 196.60
train mean loss: 188.96
epoch train time: 0:00:00.743782
elapsed time: 0:02:52.763694
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 01:51:29.842301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.00
 ---- batch: 020 ----
mean loss: 199.07
 ---- batch: 030 ----
mean loss: 195.35
 ---- batch: 040 ----
mean loss: 188.18
 ---- batch: 050 ----
mean loss: 180.72
 ---- batch: 060 ----
mean loss: 184.89
 ---- batch: 070 ----
mean loss: 194.12
 ---- batch: 080 ----
mean loss: 185.99
 ---- batch: 090 ----
mean loss: 192.22
 ---- batch: 100 ----
mean loss: 195.30
 ---- batch: 110 ----
mean loss: 178.05
train mean loss: 188.64
epoch train time: 0:00:00.740305
elapsed time: 0:02:53.504140
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 01:51:30.582753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.93
 ---- batch: 020 ----
mean loss: 195.18
 ---- batch: 030 ----
mean loss: 198.07
 ---- batch: 040 ----
mean loss: 190.93
 ---- batch: 050 ----
mean loss: 180.01
 ---- batch: 060 ----
mean loss: 182.87
 ---- batch: 070 ----
mean loss: 188.02
 ---- batch: 080 ----
mean loss: 186.60
 ---- batch: 090 ----
mean loss: 191.34
 ---- batch: 100 ----
mean loss: 186.05
 ---- batch: 110 ----
mean loss: 193.60
train mean loss: 188.61
epoch train time: 0:00:00.737353
elapsed time: 0:02:54.241645
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 01:51:31.320268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.28
 ---- batch: 020 ----
mean loss: 190.59
 ---- batch: 030 ----
mean loss: 185.20
 ---- batch: 040 ----
mean loss: 198.13
 ---- batch: 050 ----
mean loss: 183.22
 ---- batch: 060 ----
mean loss: 184.63
 ---- batch: 070 ----
mean loss: 187.67
 ---- batch: 080 ----
mean loss: 189.06
 ---- batch: 090 ----
mean loss: 187.53
 ---- batch: 100 ----
mean loss: 179.39
 ---- batch: 110 ----
mean loss: 195.49
train mean loss: 188.38
epoch train time: 0:00:00.745473
elapsed time: 0:02:54.987276
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 01:51:32.065886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.15
 ---- batch: 020 ----
mean loss: 186.95
 ---- batch: 030 ----
mean loss: 192.32
 ---- batch: 040 ----
mean loss: 181.36
 ---- batch: 050 ----
mean loss: 193.26
 ---- batch: 060 ----
mean loss: 180.59
 ---- batch: 070 ----
mean loss: 202.25
 ---- batch: 080 ----
mean loss: 194.43
 ---- batch: 090 ----
mean loss: 186.42
 ---- batch: 100 ----
mean loss: 185.51
 ---- batch: 110 ----
mean loss: 185.15
train mean loss: 188.38
epoch train time: 0:00:00.732004
elapsed time: 0:02:55.719452
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 01:51:32.798079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.00
 ---- batch: 020 ----
mean loss: 185.59
 ---- batch: 030 ----
mean loss: 189.17
 ---- batch: 040 ----
mean loss: 188.49
 ---- batch: 050 ----
mean loss: 187.01
 ---- batch: 060 ----
mean loss: 191.65
 ---- batch: 070 ----
mean loss: 181.97
 ---- batch: 080 ----
mean loss: 185.64
 ---- batch: 090 ----
mean loss: 186.44
 ---- batch: 100 ----
mean loss: 180.82
 ---- batch: 110 ----
mean loss: 196.99
train mean loss: 188.36
epoch train time: 0:00:00.735575
elapsed time: 0:02:56.455179
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 01:51:33.533789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.39
 ---- batch: 020 ----
mean loss: 181.45
 ---- batch: 030 ----
mean loss: 198.38
 ---- batch: 040 ----
mean loss: 180.22
 ---- batch: 050 ----
mean loss: 189.93
 ---- batch: 060 ----
mean loss: 201.03
 ---- batch: 070 ----
mean loss: 190.40
 ---- batch: 080 ----
mean loss: 186.01
 ---- batch: 090 ----
mean loss: 177.17
 ---- batch: 100 ----
mean loss: 184.00
 ---- batch: 110 ----
mean loss: 190.48
train mean loss: 188.12
epoch train time: 0:00:00.737030
elapsed time: 0:02:57.192349
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 01:51:34.270992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.79
 ---- batch: 020 ----
mean loss: 184.30
 ---- batch: 030 ----
mean loss: 187.47
 ---- batch: 040 ----
mean loss: 185.05
 ---- batch: 050 ----
mean loss: 189.07
 ---- batch: 060 ----
mean loss: 193.51
 ---- batch: 070 ----
mean loss: 188.21
 ---- batch: 080 ----
mean loss: 177.01
 ---- batch: 090 ----
mean loss: 189.28
 ---- batch: 100 ----
mean loss: 188.63
 ---- batch: 110 ----
mean loss: 197.60
train mean loss: 188.16
epoch train time: 0:00:00.734815
elapsed time: 0:02:57.927353
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 01:51:35.005962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.43
 ---- batch: 020 ----
mean loss: 184.85
 ---- batch: 030 ----
mean loss: 197.28
 ---- batch: 040 ----
mean loss: 188.88
 ---- batch: 050 ----
mean loss: 184.31
 ---- batch: 060 ----
mean loss: 188.25
 ---- batch: 070 ----
mean loss: 182.41
 ---- batch: 080 ----
mean loss: 187.01
 ---- batch: 090 ----
mean loss: 190.63
 ---- batch: 100 ----
mean loss: 183.17
 ---- batch: 110 ----
mean loss: 185.98
train mean loss: 187.64
epoch train time: 0:00:00.719194
elapsed time: 0:02:58.646738
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 01:51:35.725338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.02
 ---- batch: 020 ----
mean loss: 182.17
 ---- batch: 030 ----
mean loss: 179.06
 ---- batch: 040 ----
mean loss: 193.01
 ---- batch: 050 ----
mean loss: 190.92
 ---- batch: 060 ----
mean loss: 187.69
 ---- batch: 070 ----
mean loss: 180.94
 ---- batch: 080 ----
mean loss: 195.43
 ---- batch: 090 ----
mean loss: 193.02
 ---- batch: 100 ----
mean loss: 186.78
 ---- batch: 110 ----
mean loss: 181.19
train mean loss: 187.60
epoch train time: 0:00:00.718243
elapsed time: 0:02:59.365106
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 01:51:36.443712
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.75
 ---- batch: 020 ----
mean loss: 189.21
 ---- batch: 030 ----
mean loss: 192.74
 ---- batch: 040 ----
mean loss: 190.62
 ---- batch: 050 ----
mean loss: 190.92
 ---- batch: 060 ----
mean loss: 187.59
 ---- batch: 070 ----
mean loss: 181.67
 ---- batch: 080 ----
mean loss: 194.08
 ---- batch: 090 ----
mean loss: 186.46
 ---- batch: 100 ----
mean loss: 184.23
 ---- batch: 110 ----
mean loss: 182.56
train mean loss: 187.54
epoch train time: 0:00:00.721930
elapsed time: 0:03:00.087175
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 01:51:37.165784
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.67
 ---- batch: 020 ----
mean loss: 188.70
 ---- batch: 030 ----
mean loss: 183.77
 ---- batch: 040 ----
mean loss: 186.38
 ---- batch: 050 ----
mean loss: 191.11
 ---- batch: 060 ----
mean loss: 188.06
 ---- batch: 070 ----
mean loss: 192.44
 ---- batch: 080 ----
mean loss: 193.35
 ---- batch: 090 ----
mean loss: 189.99
 ---- batch: 100 ----
mean loss: 181.06
 ---- batch: 110 ----
mean loss: 182.44
train mean loss: 187.52
epoch train time: 0:00:00.719204
elapsed time: 0:03:00.806513
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 01:51:37.885136
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.63
 ---- batch: 020 ----
mean loss: 184.65
 ---- batch: 030 ----
mean loss: 187.69
 ---- batch: 040 ----
mean loss: 184.38
 ---- batch: 050 ----
mean loss: 190.85
 ---- batch: 060 ----
mean loss: 190.50
 ---- batch: 070 ----
mean loss: 187.74
 ---- batch: 080 ----
mean loss: 193.03
 ---- batch: 090 ----
mean loss: 180.81
 ---- batch: 100 ----
mean loss: 178.77
 ---- batch: 110 ----
mean loss: 196.44
train mean loss: 187.54
epoch train time: 0:00:00.714047
elapsed time: 0:03:01.520708
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 01:51:38.599338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.30
 ---- batch: 020 ----
mean loss: 184.70
 ---- batch: 030 ----
mean loss: 188.65
 ---- batch: 040 ----
mean loss: 189.61
 ---- batch: 050 ----
mean loss: 182.63
 ---- batch: 060 ----
mean loss: 189.34
 ---- batch: 070 ----
mean loss: 190.05
 ---- batch: 080 ----
mean loss: 192.12
 ---- batch: 090 ----
mean loss: 185.82
 ---- batch: 100 ----
mean loss: 178.22
 ---- batch: 110 ----
mean loss: 194.23
train mean loss: 187.60
epoch train time: 0:00:00.714422
elapsed time: 0:03:02.235286
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 01:51:39.313895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.97
 ---- batch: 020 ----
mean loss: 190.05
 ---- batch: 030 ----
mean loss: 187.58
 ---- batch: 040 ----
mean loss: 199.98
 ---- batch: 050 ----
mean loss: 174.66
 ---- batch: 060 ----
mean loss: 187.73
 ---- batch: 070 ----
mean loss: 176.32
 ---- batch: 080 ----
mean loss: 193.27
 ---- batch: 090 ----
mean loss: 183.39
 ---- batch: 100 ----
mean loss: 196.84
 ---- batch: 110 ----
mean loss: 185.65
train mean loss: 187.58
epoch train time: 0:00:00.718841
elapsed time: 0:03:02.954278
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 01:51:40.032886
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.95
 ---- batch: 020 ----
mean loss: 186.61
 ---- batch: 030 ----
mean loss: 181.30
 ---- batch: 040 ----
mean loss: 183.67
 ---- batch: 050 ----
mean loss: 190.69
 ---- batch: 060 ----
mean loss: 196.65
 ---- batch: 070 ----
mean loss: 194.25
 ---- batch: 080 ----
mean loss: 190.70
 ---- batch: 090 ----
mean loss: 184.89
 ---- batch: 100 ----
mean loss: 182.85
 ---- batch: 110 ----
mean loss: 190.50
train mean loss: 187.63
epoch train time: 0:00:00.721807
elapsed time: 0:03:03.676225
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 01:51:40.754836
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.66
 ---- batch: 020 ----
mean loss: 171.45
 ---- batch: 030 ----
mean loss: 191.84
 ---- batch: 040 ----
mean loss: 183.62
 ---- batch: 050 ----
mean loss: 182.85
 ---- batch: 060 ----
mean loss: 193.07
 ---- batch: 070 ----
mean loss: 186.52
 ---- batch: 080 ----
mean loss: 184.95
 ---- batch: 090 ----
mean loss: 188.59
 ---- batch: 100 ----
mean loss: 189.06
 ---- batch: 110 ----
mean loss: 202.41
train mean loss: 187.52
epoch train time: 0:00:00.721750
elapsed time: 0:03:04.398129
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 01:51:41.476738
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.59
 ---- batch: 020 ----
mean loss: 185.25
 ---- batch: 030 ----
mean loss: 190.16
 ---- batch: 040 ----
mean loss: 194.07
 ---- batch: 050 ----
mean loss: 190.79
 ---- batch: 060 ----
mean loss: 193.07
 ---- batch: 070 ----
mean loss: 181.53
 ---- batch: 080 ----
mean loss: 179.20
 ---- batch: 090 ----
mean loss: 179.58
 ---- batch: 100 ----
mean loss: 180.35
 ---- batch: 110 ----
mean loss: 187.07
train mean loss: 187.52
epoch train time: 0:00:00.714290
elapsed time: 0:03:05.112566
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 01:51:42.191173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.35
 ---- batch: 020 ----
mean loss: 191.10
 ---- batch: 030 ----
mean loss: 188.06
 ---- batch: 040 ----
mean loss: 192.37
 ---- batch: 050 ----
mean loss: 191.55
 ---- batch: 060 ----
mean loss: 190.57
 ---- batch: 070 ----
mean loss: 178.79
 ---- batch: 080 ----
mean loss: 179.53
 ---- batch: 090 ----
mean loss: 193.43
 ---- batch: 100 ----
mean loss: 179.60
 ---- batch: 110 ----
mean loss: 188.27
train mean loss: 187.51
epoch train time: 0:00:00.717374
elapsed time: 0:03:05.830124
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 01:51:42.908734
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.11
 ---- batch: 020 ----
mean loss: 184.72
 ---- batch: 030 ----
mean loss: 186.47
 ---- batch: 040 ----
mean loss: 190.54
 ---- batch: 050 ----
mean loss: 192.14
 ---- batch: 060 ----
mean loss: 190.98
 ---- batch: 070 ----
mean loss: 191.15
 ---- batch: 080 ----
mean loss: 184.72
 ---- batch: 090 ----
mean loss: 186.91
 ---- batch: 100 ----
mean loss: 186.73
 ---- batch: 110 ----
mean loss: 185.45
train mean loss: 187.44
epoch train time: 0:00:00.714423
elapsed time: 0:03:06.544678
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 01:51:43.623306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.11
 ---- batch: 020 ----
mean loss: 184.49
 ---- batch: 030 ----
mean loss: 177.40
 ---- batch: 040 ----
mean loss: 195.38
 ---- batch: 050 ----
mean loss: 190.42
 ---- batch: 060 ----
mean loss: 191.11
 ---- batch: 070 ----
mean loss: 181.95
 ---- batch: 080 ----
mean loss: 186.04
 ---- batch: 090 ----
mean loss: 176.39
 ---- batch: 100 ----
mean loss: 193.00
 ---- batch: 110 ----
mean loss: 196.05
train mean loss: 187.44
epoch train time: 0:00:00.722419
elapsed time: 0:03:07.267254
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 01:51:44.345864
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.53
 ---- batch: 020 ----
mean loss: 186.56
 ---- batch: 030 ----
mean loss: 188.06
 ---- batch: 040 ----
mean loss: 177.87
 ---- batch: 050 ----
mean loss: 193.18
 ---- batch: 060 ----
mean loss: 185.05
 ---- batch: 070 ----
mean loss: 196.17
 ---- batch: 080 ----
mean loss: 190.59
 ---- batch: 090 ----
mean loss: 184.49
 ---- batch: 100 ----
mean loss: 188.38
 ---- batch: 110 ----
mean loss: 188.89
train mean loss: 187.53
epoch train time: 0:00:00.716626
elapsed time: 0:03:07.984034
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 01:51:45.062673
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.86
 ---- batch: 020 ----
mean loss: 193.03
 ---- batch: 030 ----
mean loss: 189.82
 ---- batch: 040 ----
mean loss: 194.09
 ---- batch: 050 ----
mean loss: 182.73
 ---- batch: 060 ----
mean loss: 181.73
 ---- batch: 070 ----
mean loss: 193.20
 ---- batch: 080 ----
mean loss: 179.60
 ---- batch: 090 ----
mean loss: 174.71
 ---- batch: 100 ----
mean loss: 184.88
 ---- batch: 110 ----
mean loss: 188.91
train mean loss: 187.48
epoch train time: 0:00:00.723749
elapsed time: 0:03:08.707977
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 01:51:45.786592
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.92
 ---- batch: 020 ----
mean loss: 188.37
 ---- batch: 030 ----
mean loss: 189.34
 ---- batch: 040 ----
mean loss: 187.68
 ---- batch: 050 ----
mean loss: 184.13
 ---- batch: 060 ----
mean loss: 185.14
 ---- batch: 070 ----
mean loss: 180.30
 ---- batch: 080 ----
mean loss: 191.49
 ---- batch: 090 ----
mean loss: 187.78
 ---- batch: 100 ----
mean loss: 190.57
 ---- batch: 110 ----
mean loss: 187.07
train mean loss: 187.49
epoch train time: 0:00:00.717080
elapsed time: 0:03:09.425196
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 01:51:46.503820
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.42
 ---- batch: 020 ----
mean loss: 184.44
 ---- batch: 030 ----
mean loss: 187.61
 ---- batch: 040 ----
mean loss: 185.60
 ---- batch: 050 ----
mean loss: 186.17
 ---- batch: 060 ----
mean loss: 190.78
 ---- batch: 070 ----
mean loss: 186.00
 ---- batch: 080 ----
mean loss: 190.94
 ---- batch: 090 ----
mean loss: 186.28
 ---- batch: 100 ----
mean loss: 195.49
 ---- batch: 110 ----
mean loss: 189.43
train mean loss: 187.50
epoch train time: 0:00:00.717043
elapsed time: 0:03:10.142387
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 01:51:47.221002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.22
 ---- batch: 020 ----
mean loss: 191.78
 ---- batch: 030 ----
mean loss: 193.72
 ---- batch: 040 ----
mean loss: 189.47
 ---- batch: 050 ----
mean loss: 183.23
 ---- batch: 060 ----
mean loss: 191.08
 ---- batch: 070 ----
mean loss: 188.45
 ---- batch: 080 ----
mean loss: 177.15
 ---- batch: 090 ----
mean loss: 186.64
 ---- batch: 100 ----
mean loss: 189.39
 ---- batch: 110 ----
mean loss: 183.81
train mean loss: 187.43
epoch train time: 0:00:00.714810
elapsed time: 0:03:10.857351
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 01:51:47.935984
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.48
 ---- batch: 020 ----
mean loss: 192.81
 ---- batch: 030 ----
mean loss: 182.33
 ---- batch: 040 ----
mean loss: 183.84
 ---- batch: 050 ----
mean loss: 190.15
 ---- batch: 060 ----
mean loss: 181.56
 ---- batch: 070 ----
mean loss: 186.40
 ---- batch: 080 ----
mean loss: 194.35
 ---- batch: 090 ----
mean loss: 188.18
 ---- batch: 100 ----
mean loss: 187.69
 ---- batch: 110 ----
mean loss: 190.03
train mean loss: 187.44
epoch train time: 0:00:00.721483
elapsed time: 0:03:11.578991
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 01:51:48.657600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.23
 ---- batch: 020 ----
mean loss: 191.70
 ---- batch: 030 ----
mean loss: 189.48
 ---- batch: 040 ----
mean loss: 187.94
 ---- batch: 050 ----
mean loss: 194.52
 ---- batch: 060 ----
mean loss: 181.75
 ---- batch: 070 ----
mean loss: 189.74
 ---- batch: 080 ----
mean loss: 188.22
 ---- batch: 090 ----
mean loss: 187.85
 ---- batch: 100 ----
mean loss: 189.30
 ---- batch: 110 ----
mean loss: 186.33
train mean loss: 187.40
epoch train time: 0:00:00.719707
elapsed time: 0:03:12.298840
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 01:51:49.377450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.10
 ---- batch: 020 ----
mean loss: 184.57
 ---- batch: 030 ----
mean loss: 191.14
 ---- batch: 040 ----
mean loss: 198.12
 ---- batch: 050 ----
mean loss: 178.85
 ---- batch: 060 ----
mean loss: 186.17
 ---- batch: 070 ----
mean loss: 193.94
 ---- batch: 080 ----
mean loss: 192.44
 ---- batch: 090 ----
mean loss: 180.88
 ---- batch: 100 ----
mean loss: 186.53
 ---- batch: 110 ----
mean loss: 189.11
train mean loss: 187.37
epoch train time: 0:00:00.725639
elapsed time: 0:03:13.024646
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 01:51:50.103256
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.92
 ---- batch: 020 ----
mean loss: 192.83
 ---- batch: 030 ----
mean loss: 198.68
 ---- batch: 040 ----
mean loss: 184.34
 ---- batch: 050 ----
mean loss: 185.42
 ---- batch: 060 ----
mean loss: 187.20
 ---- batch: 070 ----
mean loss: 189.11
 ---- batch: 080 ----
mean loss: 180.06
 ---- batch: 090 ----
mean loss: 182.44
 ---- batch: 100 ----
mean loss: 189.31
 ---- batch: 110 ----
mean loss: 182.98
train mean loss: 187.46
epoch train time: 0:00:00.721044
elapsed time: 0:03:13.745827
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 01:51:50.824437
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.70
 ---- batch: 020 ----
mean loss: 185.81
 ---- batch: 030 ----
mean loss: 185.09
 ---- batch: 040 ----
mean loss: 190.67
 ---- batch: 050 ----
mean loss: 195.76
 ---- batch: 060 ----
mean loss: 184.74
 ---- batch: 070 ----
mean loss: 185.56
 ---- batch: 080 ----
mean loss: 185.64
 ---- batch: 090 ----
mean loss: 190.83
 ---- batch: 100 ----
mean loss: 188.18
 ---- batch: 110 ----
mean loss: 184.39
train mean loss: 187.42
epoch train time: 0:00:00.714115
elapsed time: 0:03:14.460077
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 01:51:51.538684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.58
 ---- batch: 020 ----
mean loss: 189.14
 ---- batch: 030 ----
mean loss: 188.28
 ---- batch: 040 ----
mean loss: 189.28
 ---- batch: 050 ----
mean loss: 185.43
 ---- batch: 060 ----
mean loss: 196.45
 ---- batch: 070 ----
mean loss: 184.75
 ---- batch: 080 ----
mean loss: 191.08
 ---- batch: 090 ----
mean loss: 189.37
 ---- batch: 100 ----
mean loss: 177.08
 ---- batch: 110 ----
mean loss: 184.34
train mean loss: 187.41
epoch train time: 0:00:00.714036
elapsed time: 0:03:15.174241
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 01:51:52.252847
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.02
 ---- batch: 020 ----
mean loss: 182.31
 ---- batch: 030 ----
mean loss: 192.41
 ---- batch: 040 ----
mean loss: 193.17
 ---- batch: 050 ----
mean loss: 176.08
 ---- batch: 060 ----
mean loss: 190.76
 ---- batch: 070 ----
mean loss: 182.90
 ---- batch: 080 ----
mean loss: 184.75
 ---- batch: 090 ----
mean loss: 190.69
 ---- batch: 100 ----
mean loss: 191.77
 ---- batch: 110 ----
mean loss: 186.77
train mean loss: 187.41
epoch train time: 0:00:00.721638
elapsed time: 0:03:15.896012
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 01:51:52.974643
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.01
 ---- batch: 020 ----
mean loss: 194.28
 ---- batch: 030 ----
mean loss: 191.17
 ---- batch: 040 ----
mean loss: 183.34
 ---- batch: 050 ----
mean loss: 188.48
 ---- batch: 060 ----
mean loss: 174.75
 ---- batch: 070 ----
mean loss: 194.42
 ---- batch: 080 ----
mean loss: 183.75
 ---- batch: 090 ----
mean loss: 192.90
 ---- batch: 100 ----
mean loss: 190.33
 ---- batch: 110 ----
mean loss: 187.62
train mean loss: 187.32
epoch train time: 0:00:00.723717
elapsed time: 0:03:16.619886
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 01:51:53.698494
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.47
 ---- batch: 020 ----
mean loss: 186.31
 ---- batch: 030 ----
mean loss: 191.04
 ---- batch: 040 ----
mean loss: 181.64
 ---- batch: 050 ----
mean loss: 186.85
 ---- batch: 060 ----
mean loss: 189.46
 ---- batch: 070 ----
mean loss: 196.48
 ---- batch: 080 ----
mean loss: 193.74
 ---- batch: 090 ----
mean loss: 189.71
 ---- batch: 100 ----
mean loss: 188.59
 ---- batch: 110 ----
mean loss: 180.63
train mean loss: 187.34
epoch train time: 0:00:00.720417
elapsed time: 0:03:17.340434
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 01:51:54.419041
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.71
 ---- batch: 020 ----
mean loss: 181.80
 ---- batch: 030 ----
mean loss: 184.41
 ---- batch: 040 ----
mean loss: 190.15
 ---- batch: 050 ----
mean loss: 192.08
 ---- batch: 060 ----
mean loss: 187.89
 ---- batch: 070 ----
mean loss: 193.92
 ---- batch: 080 ----
mean loss: 188.89
 ---- batch: 090 ----
mean loss: 180.93
 ---- batch: 100 ----
mean loss: 183.37
 ---- batch: 110 ----
mean loss: 188.46
train mean loss: 187.37
epoch train time: 0:00:00.724484
elapsed time: 0:03:18.065063
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 01:51:55.143672
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.44
 ---- batch: 020 ----
mean loss: 190.47
 ---- batch: 030 ----
mean loss: 197.08
 ---- batch: 040 ----
mean loss: 188.00
 ---- batch: 050 ----
mean loss: 186.13
 ---- batch: 060 ----
mean loss: 189.38
 ---- batch: 070 ----
mean loss: 184.58
 ---- batch: 080 ----
mean loss: 175.24
 ---- batch: 090 ----
mean loss: 193.41
 ---- batch: 100 ----
mean loss: 191.44
 ---- batch: 110 ----
mean loss: 186.27
train mean loss: 187.32
epoch train time: 0:00:00.725543
elapsed time: 0:03:18.790741
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 01:51:55.869352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.61
 ---- batch: 020 ----
mean loss: 181.85
 ---- batch: 030 ----
mean loss: 181.78
 ---- batch: 040 ----
mean loss: 201.99
 ---- batch: 050 ----
mean loss: 181.29
 ---- batch: 060 ----
mean loss: 185.82
 ---- batch: 070 ----
mean loss: 187.19
 ---- batch: 080 ----
mean loss: 188.30
 ---- batch: 090 ----
mean loss: 185.93
 ---- batch: 100 ----
mean loss: 180.40
 ---- batch: 110 ----
mean loss: 191.71
train mean loss: 187.39
epoch train time: 0:00:00.723660
elapsed time: 0:03:19.514551
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 01:51:56.593175
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.68
 ---- batch: 020 ----
mean loss: 177.04
 ---- batch: 030 ----
mean loss: 177.56
 ---- batch: 040 ----
mean loss: 194.60
 ---- batch: 050 ----
mean loss: 189.52
 ---- batch: 060 ----
mean loss: 179.00
 ---- batch: 070 ----
mean loss: 190.78
 ---- batch: 080 ----
mean loss: 195.76
 ---- batch: 090 ----
mean loss: 182.26
 ---- batch: 100 ----
mean loss: 197.60
 ---- batch: 110 ----
mean loss: 192.04
train mean loss: 187.28
epoch train time: 0:00:00.723676
elapsed time: 0:03:20.238378
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 01:51:57.316989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.39
 ---- batch: 020 ----
mean loss: 182.78
 ---- batch: 030 ----
mean loss: 190.29
 ---- batch: 040 ----
mean loss: 191.52
 ---- batch: 050 ----
mean loss: 175.73
 ---- batch: 060 ----
mean loss: 193.84
 ---- batch: 070 ----
mean loss: 190.66
 ---- batch: 080 ----
mean loss: 191.54
 ---- batch: 090 ----
mean loss: 189.77
 ---- batch: 100 ----
mean loss: 182.67
 ---- batch: 110 ----
mean loss: 186.55
train mean loss: 187.26
epoch train time: 0:00:00.729097
elapsed time: 0:03:20.967633
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 01:51:58.046280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.65
 ---- batch: 020 ----
mean loss: 179.73
 ---- batch: 030 ----
mean loss: 185.70
 ---- batch: 040 ----
mean loss: 189.56
 ---- batch: 050 ----
mean loss: 185.98
 ---- batch: 060 ----
mean loss: 196.60
 ---- batch: 070 ----
mean loss: 188.87
 ---- batch: 080 ----
mean loss: 187.90
 ---- batch: 090 ----
mean loss: 183.87
 ---- batch: 100 ----
mean loss: 190.49
 ---- batch: 110 ----
mean loss: 186.11
train mean loss: 187.28
epoch train time: 0:00:00.730898
elapsed time: 0:03:21.698720
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 01:51:58.777321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.38
 ---- batch: 020 ----
mean loss: 186.51
 ---- batch: 030 ----
mean loss: 177.10
 ---- batch: 040 ----
mean loss: 187.12
 ---- batch: 050 ----
mean loss: 181.59
 ---- batch: 060 ----
mean loss: 190.80
 ---- batch: 070 ----
mean loss: 184.55
 ---- batch: 080 ----
mean loss: 194.25
 ---- batch: 090 ----
mean loss: 192.11
 ---- batch: 100 ----
mean loss: 191.34
 ---- batch: 110 ----
mean loss: 188.38
train mean loss: 187.25
epoch train time: 0:00:00.718999
elapsed time: 0:03:22.417846
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 01:51:59.496459
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.41
 ---- batch: 020 ----
mean loss: 182.93
 ---- batch: 030 ----
mean loss: 184.37
 ---- batch: 040 ----
mean loss: 193.69
 ---- batch: 050 ----
mean loss: 182.49
 ---- batch: 060 ----
mean loss: 192.44
 ---- batch: 070 ----
mean loss: 188.09
 ---- batch: 080 ----
mean loss: 180.78
 ---- batch: 090 ----
mean loss: 186.20
 ---- batch: 100 ----
mean loss: 189.04
 ---- batch: 110 ----
mean loss: 185.29
train mean loss: 187.28
epoch train time: 0:00:00.728391
elapsed time: 0:03:23.146378
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 01:52:00.224987
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.47
 ---- batch: 020 ----
mean loss: 187.89
 ---- batch: 030 ----
mean loss: 182.45
 ---- batch: 040 ----
mean loss: 190.56
 ---- batch: 050 ----
mean loss: 187.92
 ---- batch: 060 ----
mean loss: 198.69
 ---- batch: 070 ----
mean loss: 190.12
 ---- batch: 080 ----
mean loss: 183.09
 ---- batch: 090 ----
mean loss: 183.93
 ---- batch: 100 ----
mean loss: 183.98
 ---- batch: 110 ----
mean loss: 183.75
train mean loss: 187.28
epoch train time: 0:00:00.725520
elapsed time: 0:03:23.872035
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 01:52:00.950648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.22
 ---- batch: 020 ----
mean loss: 199.16
 ---- batch: 030 ----
mean loss: 186.24
 ---- batch: 040 ----
mean loss: 184.10
 ---- batch: 050 ----
mean loss: 189.30
 ---- batch: 060 ----
mean loss: 190.04
 ---- batch: 070 ----
mean loss: 177.45
 ---- batch: 080 ----
mean loss: 183.33
 ---- batch: 090 ----
mean loss: 192.99
 ---- batch: 100 ----
mean loss: 187.93
 ---- batch: 110 ----
mean loss: 180.16
train mean loss: 187.24
epoch train time: 0:00:00.721958
elapsed time: 0:03:24.594151
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 01:52:01.672777
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.13
 ---- batch: 020 ----
mean loss: 190.44
 ---- batch: 030 ----
mean loss: 186.33
 ---- batch: 040 ----
mean loss: 189.93
 ---- batch: 050 ----
mean loss: 184.56
 ---- batch: 060 ----
mean loss: 187.24
 ---- batch: 070 ----
mean loss: 197.67
 ---- batch: 080 ----
mean loss: 183.66
 ---- batch: 090 ----
mean loss: 178.33
 ---- batch: 100 ----
mean loss: 193.79
 ---- batch: 110 ----
mean loss: 182.87
train mean loss: 187.22
epoch train time: 0:00:00.721319
elapsed time: 0:03:25.315641
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 01:52:02.394272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.56
 ---- batch: 020 ----
mean loss: 189.29
 ---- batch: 030 ----
mean loss: 189.51
 ---- batch: 040 ----
mean loss: 192.43
 ---- batch: 050 ----
mean loss: 183.63
 ---- batch: 060 ----
mean loss: 196.11
 ---- batch: 070 ----
mean loss: 188.92
 ---- batch: 080 ----
mean loss: 194.07
 ---- batch: 090 ----
mean loss: 180.53
 ---- batch: 100 ----
mean loss: 186.53
 ---- batch: 110 ----
mean loss: 178.16
train mean loss: 187.28
epoch train time: 0:00:00.725537
elapsed time: 0:03:26.041343
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 01:52:03.119957
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.05
 ---- batch: 020 ----
mean loss: 184.85
 ---- batch: 030 ----
mean loss: 192.02
 ---- batch: 040 ----
mean loss: 179.06
 ---- batch: 050 ----
mean loss: 186.13
 ---- batch: 060 ----
mean loss: 188.40
 ---- batch: 070 ----
mean loss: 187.55
 ---- batch: 080 ----
mean loss: 186.23
 ---- batch: 090 ----
mean loss: 198.13
 ---- batch: 100 ----
mean loss: 185.34
 ---- batch: 110 ----
mean loss: 190.42
train mean loss: 187.27
epoch train time: 0:00:00.726583
elapsed time: 0:03:26.768096
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 01:52:03.846717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.94
 ---- batch: 020 ----
mean loss: 185.68
 ---- batch: 030 ----
mean loss: 184.74
 ---- batch: 040 ----
mean loss: 185.39
 ---- batch: 050 ----
mean loss: 190.14
 ---- batch: 060 ----
mean loss: 190.52
 ---- batch: 070 ----
mean loss: 190.08
 ---- batch: 080 ----
mean loss: 188.46
 ---- batch: 090 ----
mean loss: 188.25
 ---- batch: 100 ----
mean loss: 196.75
 ---- batch: 110 ----
mean loss: 182.59
train mean loss: 187.20
epoch train time: 0:00:00.720027
elapsed time: 0:03:27.488290
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 01:52:04.566914
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.56
 ---- batch: 020 ----
mean loss: 201.98
 ---- batch: 030 ----
mean loss: 194.68
 ---- batch: 040 ----
mean loss: 178.61
 ---- batch: 050 ----
mean loss: 180.47
 ---- batch: 060 ----
mean loss: 189.02
 ---- batch: 070 ----
mean loss: 191.96
 ---- batch: 080 ----
mean loss: 172.62
 ---- batch: 090 ----
mean loss: 185.47
 ---- batch: 100 ----
mean loss: 190.86
 ---- batch: 110 ----
mean loss: 186.57
train mean loss: 187.18
epoch train time: 0:00:00.722318
elapsed time: 0:03:28.210795
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 01:52:05.289411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.49
 ---- batch: 020 ----
mean loss: 192.73
 ---- batch: 030 ----
mean loss: 180.70
 ---- batch: 040 ----
mean loss: 198.68
 ---- batch: 050 ----
mean loss: 192.56
 ---- batch: 060 ----
mean loss: 184.50
 ---- batch: 070 ----
mean loss: 192.78
 ---- batch: 080 ----
mean loss: 178.76
 ---- batch: 090 ----
mean loss: 178.55
 ---- batch: 100 ----
mean loss: 193.62
 ---- batch: 110 ----
mean loss: 181.85
train mean loss: 187.20
epoch train time: 0:00:00.720059
elapsed time: 0:03:28.930995
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 01:52:06.009640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.90
 ---- batch: 020 ----
mean loss: 188.83
 ---- batch: 030 ----
mean loss: 185.29
 ---- batch: 040 ----
mean loss: 186.18
 ---- batch: 050 ----
mean loss: 186.28
 ---- batch: 060 ----
mean loss: 183.42
 ---- batch: 070 ----
mean loss: 193.21
 ---- batch: 080 ----
mean loss: 193.86
 ---- batch: 090 ----
mean loss: 183.58
 ---- batch: 100 ----
mean loss: 189.17
 ---- batch: 110 ----
mean loss: 191.94
train mean loss: 187.15
epoch train time: 0:00:00.720307
elapsed time: 0:03:29.651493
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 01:52:06.730113
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.89
 ---- batch: 020 ----
mean loss: 185.37
 ---- batch: 030 ----
mean loss: 191.05
 ---- batch: 040 ----
mean loss: 185.00
 ---- batch: 050 ----
mean loss: 183.13
 ---- batch: 060 ----
mean loss: 191.03
 ---- batch: 070 ----
mean loss: 189.55
 ---- batch: 080 ----
mean loss: 183.27
 ---- batch: 090 ----
mean loss: 185.48
 ---- batch: 100 ----
mean loss: 192.43
 ---- batch: 110 ----
mean loss: 193.31
train mean loss: 187.18
epoch train time: 0:00:00.729265
elapsed time: 0:03:30.380906
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 01:52:07.459536
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.83
 ---- batch: 020 ----
mean loss: 186.90
 ---- batch: 030 ----
mean loss: 185.71
 ---- batch: 040 ----
mean loss: 185.37
 ---- batch: 050 ----
mean loss: 188.49
 ---- batch: 060 ----
mean loss: 185.74
 ---- batch: 070 ----
mean loss: 187.98
 ---- batch: 080 ----
mean loss: 182.81
 ---- batch: 090 ----
mean loss: 186.16
 ---- batch: 100 ----
mean loss: 185.39
 ---- batch: 110 ----
mean loss: 188.94
train mean loss: 187.22
epoch train time: 0:00:00.731624
elapsed time: 0:03:31.112690
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 01:52:08.191299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.94
 ---- batch: 020 ----
mean loss: 190.07
 ---- batch: 030 ----
mean loss: 188.12
 ---- batch: 040 ----
mean loss: 190.06
 ---- batch: 050 ----
mean loss: 184.57
 ---- batch: 060 ----
mean loss: 189.05
 ---- batch: 070 ----
mean loss: 189.20
 ---- batch: 080 ----
mean loss: 183.96
 ---- batch: 090 ----
mean loss: 187.65
 ---- batch: 100 ----
mean loss: 192.12
 ---- batch: 110 ----
mean loss: 193.56
train mean loss: 187.12
epoch train time: 0:00:00.719318
elapsed time: 0:03:31.832143
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 01:52:08.910752
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.64
 ---- batch: 020 ----
mean loss: 187.86
 ---- batch: 030 ----
mean loss: 183.14
 ---- batch: 040 ----
mean loss: 186.09
 ---- batch: 050 ----
mean loss: 184.39
 ---- batch: 060 ----
mean loss: 182.06
 ---- batch: 070 ----
mean loss: 182.27
 ---- batch: 080 ----
mean loss: 189.73
 ---- batch: 090 ----
mean loss: 190.92
 ---- batch: 100 ----
mean loss: 195.58
 ---- batch: 110 ----
mean loss: 187.94
train mean loss: 187.09
epoch train time: 0:00:00.722790
elapsed time: 0:03:32.555093
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 01:52:09.633705
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.44
 ---- batch: 020 ----
mean loss: 189.17
 ---- batch: 030 ----
mean loss: 183.67
 ---- batch: 040 ----
mean loss: 186.56
 ---- batch: 050 ----
mean loss: 183.40
 ---- batch: 060 ----
mean loss: 192.82
 ---- batch: 070 ----
mean loss: 186.62
 ---- batch: 080 ----
mean loss: 186.27
 ---- batch: 090 ----
mean loss: 186.41
 ---- batch: 100 ----
mean loss: 191.66
 ---- batch: 110 ----
mean loss: 187.49
train mean loss: 187.10
epoch train time: 0:00:00.717702
elapsed time: 0:03:33.275414
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_6/checkpoint.pth.tar
**** end time: 2019-09-27 01:52:10.353991 ****
