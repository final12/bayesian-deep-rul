Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_6', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.25, resume=False, step_size=200, visualize_step=50)
pid: 9852
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 17:48:30.193701 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 17:48:30.210266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3732.84
 ---- batch: 020 ----
mean loss: 1801.64
train mean loss: 2394.01
epoch train time: 0:00:11.269778
elapsed time: 0:00:11.294488
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 17:48:41.488228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1380.38
 ---- batch: 020 ----
mean loss: 1288.15
train mean loss: 1317.76
epoch train time: 0:00:03.849518
elapsed time: 0:00:15.145068
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 17:48:45.338983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1192.79
 ---- batch: 020 ----
mean loss: 1156.54
train mean loss: 1169.68
epoch train time: 0:00:03.830878
elapsed time: 0:00:18.977088
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 17:48:49.171009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1117.65
 ---- batch: 020 ----
mean loss: 1125.50
train mean loss: 1121.97
epoch train time: 0:00:03.830262
elapsed time: 0:00:22.808592
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 17:48:53.002428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1113.31
 ---- batch: 020 ----
mean loss: 1093.42
train mean loss: 1090.87
epoch train time: 0:00:03.836730
elapsed time: 0:00:26.646434
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 17:48:56.840484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1076.63
 ---- batch: 020 ----
mean loss: 1033.44
train mean loss: 1061.08
epoch train time: 0:00:03.841581
elapsed time: 0:00:30.489311
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 17:49:00.683314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1061.50
 ---- batch: 020 ----
mean loss: 1062.37
train mean loss: 1062.61
epoch train time: 0:00:03.830185
elapsed time: 0:00:34.320753
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 17:49:04.514676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1045.07
 ---- batch: 020 ----
mean loss: 1061.73
train mean loss: 1047.48
epoch train time: 0:00:03.819154
elapsed time: 0:00:38.141098
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 17:49:08.335059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1029.83
 ---- batch: 020 ----
mean loss: 1002.92
train mean loss: 1032.30
epoch train time: 0:00:03.827131
elapsed time: 0:00:41.969572
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 17:49:12.163544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1016.19
 ---- batch: 020 ----
mean loss: 1021.21
train mean loss: 1021.18
epoch train time: 0:00:03.850218
elapsed time: 0:00:45.821010
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 17:49:16.014972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1015.02
 ---- batch: 020 ----
mean loss: 1009.65
train mean loss: 1004.21
epoch train time: 0:00:03.848185
elapsed time: 0:00:49.670626
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 17:49:19.864562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 987.13
 ---- batch: 020 ----
mean loss: 996.14
train mean loss: 1003.21
epoch train time: 0:00:03.833952
elapsed time: 0:00:53.505894
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 17:49:23.699820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 972.23
 ---- batch: 020 ----
mean loss: 1018.66
train mean loss: 994.09
epoch train time: 0:00:03.823917
elapsed time: 0:00:57.331058
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 17:49:27.525000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 994.17
 ---- batch: 020 ----
mean loss: 982.48
train mean loss: 994.21
epoch train time: 0:00:03.821299
elapsed time: 0:01:01.153478
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 17:49:31.347431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1001.75
 ---- batch: 020 ----
mean loss: 991.80
train mean loss: 991.81
epoch train time: 0:00:03.870918
elapsed time: 0:01:05.025627
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 17:49:35.219569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 988.99
 ---- batch: 020 ----
mean loss: 994.83
train mean loss: 984.36
epoch train time: 0:00:03.813841
elapsed time: 0:01:08.840725
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 17:49:39.034695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 973.39
 ---- batch: 020 ----
mean loss: 968.05
train mean loss: 973.01
epoch train time: 0:00:03.830730
elapsed time: 0:01:12.672722
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 17:49:42.866698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 982.72
 ---- batch: 020 ----
mean loss: 960.06
train mean loss: 973.63
epoch train time: 0:00:03.856628
elapsed time: 0:01:16.530585
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 17:49:46.724518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 994.93
 ---- batch: 020 ----
mean loss: 958.87
train mean loss: 969.46
epoch train time: 0:00:03.892839
elapsed time: 0:01:20.424771
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 17:49:50.618717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1003.70
 ---- batch: 020 ----
mean loss: 975.16
train mean loss: 973.69
epoch train time: 0:00:03.895052
elapsed time: 0:01:24.321038
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 17:49:54.515054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 990.13
 ---- batch: 020 ----
mean loss: 968.71
train mean loss: 976.08
epoch train time: 0:00:03.908013
elapsed time: 0:01:28.230316
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 17:49:58.424257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 958.52
 ---- batch: 020 ----
mean loss: 962.33
train mean loss: 971.90
epoch train time: 0:00:03.921229
elapsed time: 0:01:32.152741
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 17:50:02.346671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.40
 ---- batch: 020 ----
mean loss: 969.77
train mean loss: 961.51
epoch train time: 0:00:03.886182
elapsed time: 0:01:36.040243
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 17:50:06.234209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 972.20
 ---- batch: 020 ----
mean loss: 946.81
train mean loss: 958.92
epoch train time: 0:00:03.875638
elapsed time: 0:01:39.917113
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 17:50:10.111067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 974.02
 ---- batch: 020 ----
mean loss: 930.53
train mean loss: 957.16
epoch train time: 0:00:03.847740
elapsed time: 0:01:43.766090
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 17:50:13.960062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.25
 ---- batch: 020 ----
mean loss: 967.97
train mean loss: 951.16
epoch train time: 0:00:03.821427
elapsed time: 0:01:47.589171
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 17:50:17.783262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.76
 ---- batch: 020 ----
mean loss: 929.31
train mean loss: 957.71
epoch train time: 0:00:03.849971
elapsed time: 0:01:51.440589
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 17:50:21.634525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.37
 ---- batch: 020 ----
mean loss: 926.89
train mean loss: 933.32
epoch train time: 0:00:03.850009
elapsed time: 0:01:55.291846
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 17:50:25.485847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.24
 ---- batch: 020 ----
mean loss: 933.95
train mean loss: 934.74
epoch train time: 0:00:03.828777
elapsed time: 0:01:59.121965
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 17:50:29.315905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.85
 ---- batch: 020 ----
mean loss: 943.97
train mean loss: 936.23
epoch train time: 0:00:03.830664
elapsed time: 0:02:02.953870
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 17:50:33.147792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.03
 ---- batch: 020 ----
mean loss: 920.65
train mean loss: 924.05
epoch train time: 0:00:03.815512
elapsed time: 0:02:06.770675
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 17:50:36.964611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.38
 ---- batch: 020 ----
mean loss: 953.86
train mean loss: 945.43
epoch train time: 0:00:03.825892
elapsed time: 0:02:10.597805
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 17:50:40.791743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.16
 ---- batch: 020 ----
mean loss: 911.02
train mean loss: 927.26
epoch train time: 0:00:03.821897
elapsed time: 0:02:14.420977
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 17:50:44.614924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.26
 ---- batch: 020 ----
mean loss: 935.72
train mean loss: 925.47
epoch train time: 0:00:03.815673
elapsed time: 0:02:18.237808
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 17:50:48.431750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.31
 ---- batch: 020 ----
mean loss: 925.83
train mean loss: 925.32
epoch train time: 0:00:03.827798
elapsed time: 0:02:22.066794
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 17:50:52.260731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 933.86
 ---- batch: 020 ----
mean loss: 926.60
train mean loss: 913.71
epoch train time: 0:00:03.835290
elapsed time: 0:02:25.903308
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 17:50:56.097282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.80
 ---- batch: 020 ----
mean loss: 914.05
train mean loss: 914.58
epoch train time: 0:00:03.833094
elapsed time: 0:02:29.737708
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 17:50:59.931706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.72
 ---- batch: 020 ----
mean loss: 920.82
train mean loss: 909.92
epoch train time: 0:00:03.799342
elapsed time: 0:02:33.538235
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 17:51:03.732146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.08
 ---- batch: 020 ----
mean loss: 920.88
train mean loss: 911.35
epoch train time: 0:00:03.817286
elapsed time: 0:02:37.356831
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 17:51:07.550777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.54
 ---- batch: 020 ----
mean loss: 899.08
train mean loss: 897.69
epoch train time: 0:00:03.817606
elapsed time: 0:02:41.175702
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 17:51:11.369722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.33
 ---- batch: 020 ----
mean loss: 910.92
train mean loss: 903.68
epoch train time: 0:00:03.816177
elapsed time: 0:02:44.993339
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 17:51:15.187321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.29
 ---- batch: 020 ----
mean loss: 905.94
train mean loss: 899.64
epoch train time: 0:00:03.829664
elapsed time: 0:02:48.824252
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 17:51:19.018180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.70
 ---- batch: 020 ----
mean loss: 887.34
train mean loss: 888.40
epoch train time: 0:00:03.820998
elapsed time: 0:02:52.646456
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 17:51:22.840385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.82
 ---- batch: 020 ----
mean loss: 891.46
train mean loss: 891.80
epoch train time: 0:00:03.813978
elapsed time: 0:02:56.461550
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 17:51:26.655493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.64
 ---- batch: 020 ----
mean loss: 903.23
train mean loss: 881.63
epoch train time: 0:00:03.805418
elapsed time: 0:03:00.268198
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 17:51:30.462124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.73
 ---- batch: 020 ----
mean loss: 861.06
train mean loss: 876.86
epoch train time: 0:00:03.826124
elapsed time: 0:03:04.095522
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 17:51:34.289477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.33
 ---- batch: 020 ----
mean loss: 855.38
train mean loss: 877.87
epoch train time: 0:00:03.830881
elapsed time: 0:03:07.927609
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 17:51:38.121549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.83
 ---- batch: 020 ----
mean loss: 865.59
train mean loss: 870.84
epoch train time: 0:00:03.816362
elapsed time: 0:03:11.745249
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 17:51:41.939222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.53
 ---- batch: 020 ----
mean loss: 885.12
train mean loss: 866.08
epoch train time: 0:00:03.831081
elapsed time: 0:03:15.577746
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 17:51:45.771727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.76
 ---- batch: 020 ----
mean loss: 865.89
train mean loss: 863.23
epoch train time: 0:00:03.830701
elapsed time: 0:03:19.409611
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 17:51:49.603535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.27
 ---- batch: 020 ----
mean loss: 849.30
train mean loss: 857.03
epoch train time: 0:00:03.855451
elapsed time: 0:03:23.266269
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 17:51:53.460268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.91
 ---- batch: 020 ----
mean loss: 854.63
train mean loss: 856.50
epoch train time: 0:00:03.856330
elapsed time: 0:03:27.124856
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 17:51:57.318843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.12
 ---- batch: 020 ----
mean loss: 837.27
train mean loss: 852.09
epoch train time: 0:00:03.835042
elapsed time: 0:03:30.961085
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 17:52:01.155038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.57
 ---- batch: 020 ----
mean loss: 842.22
train mean loss: 846.64
epoch train time: 0:00:03.815974
elapsed time: 0:03:34.778222
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 17:52:04.972161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.89
 ---- batch: 020 ----
mean loss: 827.95
train mean loss: 829.52
epoch train time: 0:00:03.818704
elapsed time: 0:03:38.598166
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 17:52:08.792124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.13
 ---- batch: 020 ----
mean loss: 853.16
train mean loss: 841.11
epoch train time: 0:00:03.825727
elapsed time: 0:03:42.425054
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 17:52:12.618984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.67
 ---- batch: 020 ----
mean loss: 839.52
train mean loss: 823.02
epoch train time: 0:00:03.817314
elapsed time: 0:03:46.243483
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 17:52:16.437425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.88
 ---- batch: 020 ----
mean loss: 829.93
train mean loss: 827.93
epoch train time: 0:00:03.813115
elapsed time: 0:03:50.057707
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 17:52:20.251621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 822.74
 ---- batch: 020 ----
mean loss: 810.15
train mean loss: 817.19
epoch train time: 0:00:03.831048
elapsed time: 0:03:53.889925
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 17:52:24.083851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 815.03
 ---- batch: 020 ----
mean loss: 800.57
train mean loss: 813.14
epoch train time: 0:00:03.843415
elapsed time: 0:03:57.734545
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 17:52:27.928483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 800.62
 ---- batch: 020 ----
mean loss: 803.99
train mean loss: 804.74
epoch train time: 0:00:03.862124
elapsed time: 0:04:01.597849
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 17:52:31.791784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.71
 ---- batch: 020 ----
mean loss: 801.63
train mean loss: 796.05
epoch train time: 0:00:03.810267
elapsed time: 0:04:05.409259
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 17:52:35.603217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 775.79
 ---- batch: 020 ----
mean loss: 787.71
train mean loss: 788.12
epoch train time: 0:00:03.835719
elapsed time: 0:04:09.246200
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 17:52:39.440154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 782.78
 ---- batch: 020 ----
mean loss: 765.70
train mean loss: 768.13
epoch train time: 0:00:03.827471
elapsed time: 0:04:13.074881
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 17:52:43.268854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 777.76
 ---- batch: 020 ----
mean loss: 745.35
train mean loss: 765.45
epoch train time: 0:00:03.883308
elapsed time: 0:04:16.959542
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 17:52:47.153506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 761.42
 ---- batch: 020 ----
mean loss: 742.27
train mean loss: 753.26
epoch train time: 0:00:03.841590
elapsed time: 0:04:20.802356
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 17:52:50.996288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 762.68
 ---- batch: 020 ----
mean loss: 736.15
train mean loss: 738.37
epoch train time: 0:00:03.825706
elapsed time: 0:04:24.629284
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 17:52:54.823269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.47
 ---- batch: 020 ----
mean loss: 728.34
train mean loss: 729.27
epoch train time: 0:00:03.845618
elapsed time: 0:04:28.476193
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 17:52:58.670129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 697.81
 ---- batch: 020 ----
mean loss: 726.15
train mean loss: 717.14
epoch train time: 0:00:03.831958
elapsed time: 0:04:32.309335
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 17:53:02.503320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 704.62
 ---- batch: 020 ----
mean loss: 707.31
train mean loss: 704.87
epoch train time: 0:00:03.837748
elapsed time: 0:04:36.148296
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 17:53:06.342247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 691.99
 ---- batch: 020 ----
mean loss: 701.55
train mean loss: 692.92
epoch train time: 0:00:03.844594
elapsed time: 0:04:39.994082
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 17:53:10.188002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 688.87
 ---- batch: 020 ----
mean loss: 681.92
train mean loss: 687.93
epoch train time: 0:00:03.855894
elapsed time: 0:04:43.851176
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 17:53:14.045117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.77
 ---- batch: 020 ----
mean loss: 684.61
train mean loss: 677.04
epoch train time: 0:00:03.880759
elapsed time: 0:04:47.733180
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 17:53:17.927106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 672.74
 ---- batch: 020 ----
mean loss: 675.88
train mean loss: 668.16
epoch train time: 0:00:03.844920
elapsed time: 0:04:51.579253
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 17:53:21.773220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.68
 ---- batch: 020 ----
mean loss: 659.40
train mean loss: 667.43
epoch train time: 0:00:03.841110
elapsed time: 0:04:55.421613
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 17:53:25.615696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 644.25
 ---- batch: 020 ----
mean loss: 659.39
train mean loss: 647.44
epoch train time: 0:00:03.865055
elapsed time: 0:04:59.288109
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 17:53:29.482034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 654.11
 ---- batch: 020 ----
mean loss: 656.11
train mean loss: 647.68
epoch train time: 0:00:03.842097
elapsed time: 0:05:03.131348
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 17:53:33.325288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 640.47
 ---- batch: 020 ----
mean loss: 648.06
train mean loss: 639.37
epoch train time: 0:00:03.857932
elapsed time: 0:05:06.990546
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 17:53:37.184489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.88
 ---- batch: 020 ----
mean loss: 650.79
train mean loss: 639.09
epoch train time: 0:00:03.865244
elapsed time: 0:05:10.857098
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 17:53:41.051038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 628.85
 ---- batch: 020 ----
mean loss: 648.91
train mean loss: 633.90
epoch train time: 0:00:03.881008
elapsed time: 0:05:14.739465
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 17:53:44.933407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 634.63
 ---- batch: 020 ----
mean loss: 605.94
train mean loss: 611.07
epoch train time: 0:00:03.896435
elapsed time: 0:05:18.637267
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 17:53:48.831230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 606.65
 ---- batch: 020 ----
mean loss: 621.37
train mean loss: 605.74
epoch train time: 0:00:03.841255
elapsed time: 0:05:22.479851
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 17:53:52.673799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 602.92
 ---- batch: 020 ----
mean loss: 589.68
train mean loss: 602.82
epoch train time: 0:00:03.874283
elapsed time: 0:05:26.355475
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 17:53:56.549429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.13
 ---- batch: 020 ----
mean loss: 592.44
train mean loss: 594.70
epoch train time: 0:00:03.875570
elapsed time: 0:05:30.232274
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 17:54:00.426197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 603.72
 ---- batch: 020 ----
mean loss: 580.80
train mean loss: 587.47
epoch train time: 0:00:03.840361
elapsed time: 0:05:34.073820
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 17:54:04.267749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 588.55
 ---- batch: 020 ----
mean loss: 566.64
train mean loss: 577.20
epoch train time: 0:00:03.845929
elapsed time: 0:05:37.920930
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 17:54:08.114867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.36
 ---- batch: 020 ----
mean loss: 569.04
train mean loss: 568.09
epoch train time: 0:00:03.842335
elapsed time: 0:05:41.764411
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 17:54:11.958375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.44
 ---- batch: 020 ----
mean loss: 568.05
train mean loss: 561.03
epoch train time: 0:00:03.855344
elapsed time: 0:05:45.620966
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 17:54:15.814893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.35
 ---- batch: 020 ----
mean loss: 549.05
train mean loss: 560.55
epoch train time: 0:00:03.838839
elapsed time: 0:05:49.461162
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 17:54:19.655085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 552.32
 ---- batch: 020 ----
mean loss: 550.40
train mean loss: 545.74
epoch train time: 0:00:03.839232
elapsed time: 0:05:53.301844
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 17:54:23.495775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 532.35
 ---- batch: 020 ----
mean loss: 537.93
train mean loss: 530.08
epoch train time: 0:00:03.847178
elapsed time: 0:05:57.150284
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 17:54:27.344231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 535.23
 ---- batch: 020 ----
mean loss: 538.77
train mean loss: 539.29
epoch train time: 0:00:03.855949
elapsed time: 0:06:01.007387
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 17:54:31.201316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.68
 ---- batch: 020 ----
mean loss: 538.77
train mean loss: 527.52
epoch train time: 0:00:03.857345
elapsed time: 0:06:04.866003
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 17:54:35.059975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.09
 ---- batch: 020 ----
mean loss: 518.35
train mean loss: 514.29
epoch train time: 0:00:03.914988
elapsed time: 0:06:08.782193
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 17:54:38.976118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.35
 ---- batch: 020 ----
mean loss: 509.78
train mean loss: 512.80
epoch train time: 0:00:03.834309
elapsed time: 0:06:12.617696
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 17:54:42.811626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 517.66
 ---- batch: 020 ----
mean loss: 491.68
train mean loss: 503.77
epoch train time: 0:00:03.848356
elapsed time: 0:06:16.467356
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 17:54:46.661314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.71
 ---- batch: 020 ----
mean loss: 494.81
train mean loss: 498.66
epoch train time: 0:00:03.841715
elapsed time: 0:06:20.310208
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 17:54:50.504146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.92
 ---- batch: 020 ----
mean loss: 489.55
train mean loss: 488.64
epoch train time: 0:00:03.852136
elapsed time: 0:06:24.163760
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 17:54:54.357730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.11
 ---- batch: 020 ----
mean loss: 499.62
train mean loss: 489.60
epoch train time: 0:00:03.868248
elapsed time: 0:06:28.033199
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 17:54:58.227123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.16
 ---- batch: 020 ----
mean loss: 486.25
train mean loss: 484.56
epoch train time: 0:00:03.880105
elapsed time: 0:06:31.914848
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 17:55:02.108792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.71
 ---- batch: 020 ----
mean loss: 481.14
train mean loss: 476.87
epoch train time: 0:00:03.873986
elapsed time: 0:06:35.790011
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 17:55:05.983931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.14
 ---- batch: 020 ----
mean loss: 465.39
train mean loss: 466.82
epoch train time: 0:00:03.846453
elapsed time: 0:06:39.637678
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 17:55:09.831638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.88
 ---- batch: 020 ----
mean loss: 460.93
train mean loss: 460.07
epoch train time: 0:00:03.859625
elapsed time: 0:06:43.498554
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 17:55:13.692471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.64
 ---- batch: 020 ----
mean loss: 465.04
train mean loss: 460.17
epoch train time: 0:00:03.864524
elapsed time: 0:06:47.364303
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 17:55:17.558240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.43
 ---- batch: 020 ----
mean loss: 453.78
train mean loss: 456.88
epoch train time: 0:00:03.846921
elapsed time: 0:06:51.212573
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 17:55:21.406538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.52
 ---- batch: 020 ----
mean loss: 442.72
train mean loss: 450.90
epoch train time: 0:00:03.858085
elapsed time: 0:06:55.071884
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 17:55:25.265833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.54
 ---- batch: 020 ----
mean loss: 450.67
train mean loss: 441.80
epoch train time: 0:00:03.863201
elapsed time: 0:06:58.936541
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 17:55:29.130305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.45
 ---- batch: 020 ----
mean loss: 443.21
train mean loss: 440.05
epoch train time: 0:00:03.899194
elapsed time: 0:07:02.836873
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 17:55:33.030813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.48
 ---- batch: 020 ----
mean loss: 445.95
train mean loss: 441.53
epoch train time: 0:00:03.858386
elapsed time: 0:07:06.696538
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 17:55:36.890519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.62
 ---- batch: 020 ----
mean loss: 433.28
train mean loss: 430.85
epoch train time: 0:00:03.900764
elapsed time: 0:07:10.598593
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 17:55:40.792541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.41
 ---- batch: 020 ----
mean loss: 431.14
train mean loss: 433.61
epoch train time: 0:00:03.939222
elapsed time: 0:07:14.539108
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 17:55:44.733069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.06
 ---- batch: 020 ----
mean loss: 422.06
train mean loss: 420.73
epoch train time: 0:00:03.895748
elapsed time: 0:07:18.436155
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 17:55:48.630117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.37
 ---- batch: 020 ----
mean loss: 417.27
train mean loss: 419.67
epoch train time: 0:00:03.918583
elapsed time: 0:07:22.356053
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 17:55:52.550006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.83
 ---- batch: 020 ----
mean loss: 421.91
train mean loss: 417.95
epoch train time: 0:00:03.879030
elapsed time: 0:07:26.236297
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 17:55:56.430252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.96
 ---- batch: 020 ----
mean loss: 417.91
train mean loss: 413.72
epoch train time: 0:00:03.850439
elapsed time: 0:07:30.087990
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 17:56:00.281958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.80
 ---- batch: 020 ----
mean loss: 406.49
train mean loss: 405.92
epoch train time: 0:00:03.839567
elapsed time: 0:07:33.928827
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 17:56:04.122759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.93
 ---- batch: 020 ----
mean loss: 410.60
train mean loss: 407.76
epoch train time: 0:00:03.837772
elapsed time: 0:07:37.767767
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 17:56:07.961712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.95
 ---- batch: 020 ----
mean loss: 398.93
train mean loss: 400.27
epoch train time: 0:00:03.852953
elapsed time: 0:07:41.621945
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 17:56:11.815875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.20
 ---- batch: 020 ----
mean loss: 408.73
train mean loss: 402.60
epoch train time: 0:00:03.849424
elapsed time: 0:07:45.472590
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 17:56:15.666527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.70
 ---- batch: 020 ----
mean loss: 401.15
train mean loss: 395.58
epoch train time: 0:00:03.855245
elapsed time: 0:07:49.329009
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 17:56:19.522940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.75
 ---- batch: 020 ----
mean loss: 400.47
train mean loss: 390.78
epoch train time: 0:00:03.858519
elapsed time: 0:07:53.188711
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 17:56:23.382660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.85
 ---- batch: 020 ----
mean loss: 392.46
train mean loss: 394.02
epoch train time: 0:00:03.863981
elapsed time: 0:07:57.053866
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 17:56:27.247800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.11
 ---- batch: 020 ----
mean loss: 401.45
train mean loss: 388.52
epoch train time: 0:00:03.846190
elapsed time: 0:08:00.901403
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 17:56:31.095468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.16
 ---- batch: 020 ----
mean loss: 387.99
train mean loss: 381.91
epoch train time: 0:00:03.845399
elapsed time: 0:08:04.748208
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 17:56:34.942152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.57
 ---- batch: 020 ----
mean loss: 381.80
train mean loss: 376.78
epoch train time: 0:00:03.836491
elapsed time: 0:08:08.586141
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 17:56:38.780081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.96
 ---- batch: 020 ----
mean loss: 381.51
train mean loss: 380.72
epoch train time: 0:00:03.850725
elapsed time: 0:08:12.438123
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 17:56:42.632077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.47
 ---- batch: 020 ----
mean loss: 385.90
train mean loss: 378.41
epoch train time: 0:00:03.854750
elapsed time: 0:08:16.294489
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 17:56:46.488247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.63
 ---- batch: 020 ----
mean loss: 383.84
train mean loss: 381.54
epoch train time: 0:00:03.860052
elapsed time: 0:08:20.155565
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 17:56:50.349531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.44
 ---- batch: 020 ----
mean loss: 373.39
train mean loss: 374.00
epoch train time: 0:00:03.882853
elapsed time: 0:08:24.039929
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 17:56:54.233885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.21
 ---- batch: 020 ----
mean loss: 372.70
train mean loss: 373.43
epoch train time: 0:00:03.874878
elapsed time: 0:08:27.916307
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 17:56:58.110310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.80
 ---- batch: 020 ----
mean loss: 360.57
train mean loss: 370.05
epoch train time: 0:00:03.853915
elapsed time: 0:08:31.771658
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 17:57:01.965584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.81
 ---- batch: 020 ----
mean loss: 371.36
train mean loss: 367.63
epoch train time: 0:00:03.852914
elapsed time: 0:08:35.625748
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 17:57:05.819689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.32
 ---- batch: 020 ----
mean loss: 363.67
train mean loss: 361.75
epoch train time: 0:00:03.836895
elapsed time: 0:08:39.463782
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 17:57:09.657740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.86
 ---- batch: 020 ----
mean loss: 357.09
train mean loss: 361.63
epoch train time: 0:00:03.861269
elapsed time: 0:08:43.326338
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 17:57:13.520270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.41
 ---- batch: 020 ----
mean loss: 357.68
train mean loss: 361.85
epoch train time: 0:00:03.882680
elapsed time: 0:08:47.210176
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 17:57:17.404107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.75
 ---- batch: 020 ----
mean loss: 344.92
train mean loss: 361.15
epoch train time: 0:00:03.862844
elapsed time: 0:08:51.074294
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 17:57:21.268230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.47
 ---- batch: 020 ----
mean loss: 357.24
train mean loss: 359.94
epoch train time: 0:00:03.857470
elapsed time: 0:08:54.933164
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 17:57:25.127104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.65
 ---- batch: 020 ----
mean loss: 350.57
train mean loss: 354.21
epoch train time: 0:00:03.854914
elapsed time: 0:08:58.789348
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 17:57:28.983267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.48
 ---- batch: 020 ----
mean loss: 356.39
train mean loss: 352.21
epoch train time: 0:00:03.858376
elapsed time: 0:09:02.648939
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 17:57:32.842873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.70
 ---- batch: 020 ----
mean loss: 347.24
train mean loss: 350.20
epoch train time: 0:00:03.843275
elapsed time: 0:09:06.493462
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 17:57:36.687404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.65
 ---- batch: 020 ----
mean loss: 359.89
train mean loss: 350.16
epoch train time: 0:00:03.858018
elapsed time: 0:09:10.352810
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 17:57:40.546737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.05
 ---- batch: 020 ----
mean loss: 352.66
train mean loss: 350.59
epoch train time: 0:00:03.867562
elapsed time: 0:09:14.221690
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 17:57:44.415668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.18
 ---- batch: 020 ----
mean loss: 337.45
train mean loss: 340.00
epoch train time: 0:00:03.847585
elapsed time: 0:09:18.070630
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 17:57:48.264584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.42
 ---- batch: 020 ----
mean loss: 344.08
train mean loss: 345.68
epoch train time: 0:00:03.852608
elapsed time: 0:09:21.924578
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 17:57:52.118495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.06
 ---- batch: 020 ----
mean loss: 336.85
train mean loss: 341.51
epoch train time: 0:00:03.856164
elapsed time: 0:09:25.782033
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 17:57:55.975981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.01
 ---- batch: 020 ----
mean loss: 345.13
train mean loss: 341.48
epoch train time: 0:00:03.858966
elapsed time: 0:09:29.642164
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 17:57:59.836089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.31
 ---- batch: 020 ----
mean loss: 336.51
train mean loss: 332.46
epoch train time: 0:00:03.845543
elapsed time: 0:09:33.488867
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 17:58:03.682792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.09
 ---- batch: 020 ----
mean loss: 337.94
train mean loss: 336.12
epoch train time: 0:00:03.838976
elapsed time: 0:09:37.329020
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 17:58:07.522989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.12
 ---- batch: 020 ----
mean loss: 346.80
train mean loss: 339.15
epoch train time: 0:00:03.855912
elapsed time: 0:09:41.186484
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 17:58:11.380257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.35
 ---- batch: 020 ----
mean loss: 322.62
train mean loss: 331.72
epoch train time: 0:00:03.865246
elapsed time: 0:09:45.052784
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 17:58:15.246743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.40
 ---- batch: 020 ----
mean loss: 327.44
train mean loss: 333.12
epoch train time: 0:00:03.858760
elapsed time: 0:09:48.913064
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 17:58:19.107007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.94
 ---- batch: 020 ----
mean loss: 334.14
train mean loss: 329.14
epoch train time: 0:00:03.858619
elapsed time: 0:09:52.772951
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 17:58:22.966867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.32
 ---- batch: 020 ----
mean loss: 336.81
train mean loss: 329.48
epoch train time: 0:00:03.851256
elapsed time: 0:09:56.625368
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 17:58:26.819283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.00
 ---- batch: 020 ----
mean loss: 319.39
train mean loss: 328.21
epoch train time: 0:00:03.849057
elapsed time: 0:10:00.475540
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 17:58:30.669508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.44
 ---- batch: 020 ----
mean loss: 324.50
train mean loss: 328.67
epoch train time: 0:00:03.840293
elapsed time: 0:10:04.317058
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 17:58:34.510983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.36
 ---- batch: 020 ----
mean loss: 326.81
train mean loss: 325.08
epoch train time: 0:00:03.850306
elapsed time: 0:10:08.168501
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 17:58:38.362425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.22
 ---- batch: 020 ----
mean loss: 324.61
train mean loss: 319.63
epoch train time: 0:00:03.843996
elapsed time: 0:10:12.013790
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 17:58:42.207744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.37
 ---- batch: 020 ----
mean loss: 328.03
train mean loss: 329.13
epoch train time: 0:00:03.846279
elapsed time: 0:10:15.861299
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 17:58:46.055262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.53
 ---- batch: 020 ----
mean loss: 328.46
train mean loss: 321.72
epoch train time: 0:00:03.847760
elapsed time: 0:10:19.710380
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 17:58:49.904307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.79
 ---- batch: 020 ----
mean loss: 313.86
train mean loss: 320.03
epoch train time: 0:00:03.847643
elapsed time: 0:10:23.559329
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 17:58:53.753314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.36
 ---- batch: 020 ----
mean loss: 313.21
train mean loss: 317.61
epoch train time: 0:00:03.854514
elapsed time: 0:10:27.415148
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 17:58:57.609126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.85
 ---- batch: 020 ----
mean loss: 309.75
train mean loss: 317.11
epoch train time: 0:00:03.856425
elapsed time: 0:10:31.272802
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 17:59:01.466749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.19
 ---- batch: 020 ----
mean loss: 317.59
train mean loss: 316.52
epoch train time: 0:00:03.821397
elapsed time: 0:10:35.095460
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 17:59:05.289402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.29
 ---- batch: 020 ----
mean loss: 313.56
train mean loss: 315.95
epoch train time: 0:00:03.827599
elapsed time: 0:10:38.924220
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 17:59:09.118155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.36
 ---- batch: 020 ----
mean loss: 301.41
train mean loss: 314.24
epoch train time: 0:00:03.847914
elapsed time: 0:10:42.773424
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 17:59:12.967349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.58
 ---- batch: 020 ----
mean loss: 307.87
train mean loss: 314.73
epoch train time: 0:00:03.857263
elapsed time: 0:10:46.631815
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 17:59:16.825766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.17
 ---- batch: 020 ----
mean loss: 309.86
train mean loss: 310.53
epoch train time: 0:00:03.846436
elapsed time: 0:10:50.479575
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 17:59:20.673528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.15
 ---- batch: 020 ----
mean loss: 317.53
train mean loss: 310.31
epoch train time: 0:00:03.829189
elapsed time: 0:10:54.309993
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 17:59:24.503977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.83
 ---- batch: 020 ----
mean loss: 305.79
train mean loss: 314.62
epoch train time: 0:00:03.856156
elapsed time: 0:10:58.167359
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 17:59:28.361291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.71
 ---- batch: 020 ----
mean loss: 309.50
train mean loss: 317.81
epoch train time: 0:00:03.844833
elapsed time: 0:11:02.013447
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 17:59:32.207423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.82
 ---- batch: 020 ----
mean loss: 295.01
train mean loss: 307.03
epoch train time: 0:00:03.865249
elapsed time: 0:11:05.880270
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 17:59:36.074219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.45
 ---- batch: 020 ----
mean loss: 308.99
train mean loss: 311.93
epoch train time: 0:00:03.846355
elapsed time: 0:11:09.727882
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 17:59:39.921821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.12
 ---- batch: 020 ----
mean loss: 311.01
train mean loss: 305.42
epoch train time: 0:00:03.845361
elapsed time: 0:11:13.574448
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 17:59:43.768392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.64
 ---- batch: 020 ----
mean loss: 310.98
train mean loss: 309.20
epoch train time: 0:00:03.857682
elapsed time: 0:11:17.433478
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 17:59:47.627233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.02
 ---- batch: 020 ----
mean loss: 310.20
train mean loss: 302.70
epoch train time: 0:00:03.819777
elapsed time: 0:11:21.254404
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 17:59:51.448343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.64
 ---- batch: 020 ----
mean loss: 311.29
train mean loss: 305.53
epoch train time: 0:00:03.830081
elapsed time: 0:11:25.085726
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 17:59:55.279669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.56
 ---- batch: 020 ----
mean loss: 313.08
train mean loss: 308.60
epoch train time: 0:00:03.830286
elapsed time: 0:11:28.917303
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 17:59:59.111255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.94
 ---- batch: 020 ----
mean loss: 304.26
train mean loss: 306.47
epoch train time: 0:00:03.831001
elapsed time: 0:11:32.749613
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 18:00:02.943515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.68
 ---- batch: 020 ----
mean loss: 302.45
train mean loss: 299.14
epoch train time: 0:00:03.843968
elapsed time: 0:11:36.594851
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 18:00:06.788775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.44
 ---- batch: 020 ----
mean loss: 290.01
train mean loss: 301.07
epoch train time: 0:00:03.846465
elapsed time: 0:11:40.442464
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 18:00:10.636390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.48
 ---- batch: 020 ----
mean loss: 292.35
train mean loss: 296.43
epoch train time: 0:00:03.863705
elapsed time: 0:11:44.307412
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 18:00:14.501345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.55
 ---- batch: 020 ----
mean loss: 309.74
train mean loss: 300.90
epoch train time: 0:00:03.857634
elapsed time: 0:11:48.166201
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 18:00:18.360128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.88
 ---- batch: 020 ----
mean loss: 299.34
train mean loss: 295.22
epoch train time: 0:00:03.808310
elapsed time: 0:11:51.975725
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 18:00:22.169658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.68
 ---- batch: 020 ----
mean loss: 295.67
train mean loss: 298.96
epoch train time: 0:00:03.813520
elapsed time: 0:11:55.790481
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 18:00:25.984426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.22
 ---- batch: 020 ----
mean loss: 300.19
train mean loss: 294.37
epoch train time: 0:00:03.816786
elapsed time: 0:11:59.608517
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 18:00:29.802475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.92
 ---- batch: 020 ----
mean loss: 298.84
train mean loss: 294.70
epoch train time: 0:00:03.822741
elapsed time: 0:12:03.432550
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 18:00:33.626496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.01
 ---- batch: 020 ----
mean loss: 289.17
train mean loss: 291.23
epoch train time: 0:00:03.845622
elapsed time: 0:12:07.279299
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 18:00:37.473237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.98
 ---- batch: 020 ----
mean loss: 296.68
train mean loss: 291.21
epoch train time: 0:00:03.813633
elapsed time: 0:12:11.094073
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 18:00:41.287999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.41
 ---- batch: 020 ----
mean loss: 298.78
train mean loss: 294.28
epoch train time: 0:00:03.836720
elapsed time: 0:12:14.932039
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 18:00:45.125968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.63
 ---- batch: 020 ----
mean loss: 295.98
train mean loss: 296.20
epoch train time: 0:00:03.815372
elapsed time: 0:12:18.748657
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 18:00:48.942593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.45
 ---- batch: 020 ----
mean loss: 293.13
train mean loss: 290.45
epoch train time: 0:00:03.837328
elapsed time: 0:12:22.587095
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 18:00:52.781019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.64
 ---- batch: 020 ----
mean loss: 285.36
train mean loss: 291.71
epoch train time: 0:00:03.828915
elapsed time: 0:12:26.417166
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 18:00:56.611123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.07
 ---- batch: 020 ----
mean loss: 288.81
train mean loss: 287.57
epoch train time: 0:00:03.825682
elapsed time: 0:12:30.244073
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 18:01:00.438000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.65
 ---- batch: 020 ----
mean loss: 283.81
train mean loss: 287.24
epoch train time: 0:00:03.824015
elapsed time: 0:12:34.069213
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 18:01:04.263142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.89
 ---- batch: 020 ----
mean loss: 289.46
train mean loss: 290.20
epoch train time: 0:00:03.816731
elapsed time: 0:12:37.887116
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 18:01:08.081081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.51
 ---- batch: 020 ----
mean loss: 283.21
train mean loss: 288.46
epoch train time: 0:00:03.835366
elapsed time: 0:12:41.723732
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 18:01:11.917702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.05
 ---- batch: 020 ----
mean loss: 286.96
train mean loss: 288.94
epoch train time: 0:00:03.851349
elapsed time: 0:12:45.576373
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 18:01:15.770325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.19
 ---- batch: 020 ----
mean loss: 290.19
train mean loss: 287.56
epoch train time: 0:00:03.832512
elapsed time: 0:12:49.410168
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 18:01:19.604083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.67
 ---- batch: 020 ----
mean loss: 287.66
train mean loss: 287.81
epoch train time: 0:00:03.841085
elapsed time: 0:12:53.252406
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 18:01:23.446364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.68
 ---- batch: 020 ----
mean loss: 287.85
train mean loss: 286.63
epoch train time: 0:00:03.830924
elapsed time: 0:12:57.084503
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 18:01:27.278441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.92
 ---- batch: 020 ----
mean loss: 281.59
train mean loss: 285.07
epoch train time: 0:00:03.835299
elapsed time: 0:13:00.921009
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 18:01:31.114954
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.94
 ---- batch: 020 ----
mean loss: 287.26
train mean loss: 284.54
epoch train time: 0:00:03.838013
elapsed time: 0:13:04.760488
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 18:01:34.954248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 279.79
 ---- batch: 020 ----
mean loss: 281.17
train mean loss: 279.47
epoch train time: 0:00:03.840547
elapsed time: 0:13:08.602445
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 18:01:38.796450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 274.78
 ---- batch: 020 ----
mean loss: 294.68
train mean loss: 281.98
epoch train time: 0:00:03.835983
elapsed time: 0:13:12.439658
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 18:01:42.633589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.68
 ---- batch: 020 ----
mean loss: 278.08
train mean loss: 284.22
epoch train time: 0:00:03.831302
elapsed time: 0:13:16.272188
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 18:01:46.466123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.92
 ---- batch: 020 ----
mean loss: 278.87
train mean loss: 280.13
epoch train time: 0:00:03.826048
elapsed time: 0:13:20.099432
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 18:01:50.293349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.82
 ---- batch: 020 ----
mean loss: 282.99
train mean loss: 282.23
epoch train time: 0:00:03.821750
elapsed time: 0:13:23.922412
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 18:01:54.116374
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.54
 ---- batch: 020 ----
mean loss: 279.64
train mean loss: 282.22
epoch train time: 0:00:03.831137
elapsed time: 0:13:27.754779
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 18:01:57.948728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.98
 ---- batch: 020 ----
mean loss: 285.13
train mean loss: 279.58
epoch train time: 0:00:03.841521
elapsed time: 0:13:31.597560
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 18:02:01.791602
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.41
 ---- batch: 020 ----
mean loss: 281.61
train mean loss: 286.99
epoch train time: 0:00:03.819521
elapsed time: 0:13:35.418358
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 18:02:05.612340
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.59
 ---- batch: 020 ----
mean loss: 285.19
train mean loss: 280.20
epoch train time: 0:00:03.830116
elapsed time: 0:13:39.249708
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 18:02:09.443643
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.93
 ---- batch: 020 ----
mean loss: 280.36
train mean loss: 279.56
epoch train time: 0:00:03.838007
elapsed time: 0:13:43.088895
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 18:02:13.282831
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.33
 ---- batch: 020 ----
mean loss: 283.31
train mean loss: 283.36
epoch train time: 0:00:03.855540
elapsed time: 0:13:46.945599
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 18:02:17.139532
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.34
 ---- batch: 020 ----
mean loss: 277.19
train mean loss: 279.88
epoch train time: 0:00:03.826894
elapsed time: 0:13:50.773902
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 18:02:20.967868
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.38
 ---- batch: 020 ----
mean loss: 281.56
train mean loss: 280.40
epoch train time: 0:00:03.828044
elapsed time: 0:13:54.603169
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 18:02:24.797142
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.87
 ---- batch: 020 ----
mean loss: 278.39
train mean loss: 282.44
epoch train time: 0:00:03.830023
elapsed time: 0:13:58.434333
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 18:02:28.628253
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.61
 ---- batch: 020 ----
mean loss: 279.75
train mean loss: 278.60
epoch train time: 0:00:03.825003
elapsed time: 0:14:02.260511
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 18:02:32.454486
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.63
 ---- batch: 020 ----
mean loss: 285.83
train mean loss: 280.77
epoch train time: 0:00:03.815593
elapsed time: 0:14:06.077397
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 18:02:36.271331
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.65
 ---- batch: 020 ----
mean loss: 277.77
train mean loss: 280.13
epoch train time: 0:00:03.813514
elapsed time: 0:14:09.892190
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 18:02:40.086129
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.97
 ---- batch: 020 ----
mean loss: 284.36
train mean loss: 279.23
epoch train time: 0:00:03.812979
elapsed time: 0:14:13.706352
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 18:02:43.900298
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.93
 ---- batch: 020 ----
mean loss: 281.34
train mean loss: 282.05
epoch train time: 0:00:03.810022
elapsed time: 0:14:17.517536
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 18:02:47.711474
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 277.08
 ---- batch: 020 ----
mean loss: 288.26
train mean loss: 276.60
epoch train time: 0:00:03.814661
elapsed time: 0:14:21.333433
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 18:02:51.527355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.14
 ---- batch: 020 ----
mean loss: 271.90
train mean loss: 279.29
epoch train time: 0:00:03.813303
elapsed time: 0:14:25.147860
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 18:02:55.341809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.64
 ---- batch: 020 ----
mean loss: 281.63
train mean loss: 279.95
epoch train time: 0:00:03.816534
elapsed time: 0:14:28.965616
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 18:02:59.159572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 271.52
 ---- batch: 020 ----
mean loss: 281.64
train mean loss: 281.36
epoch train time: 0:00:03.805656
elapsed time: 0:14:32.772568
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 18:03:02.966489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.42
 ---- batch: 020 ----
mean loss: 278.79
train mean loss: 279.26
epoch train time: 0:00:03.827207
elapsed time: 0:14:36.601212
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 18:03:06.795180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.29
 ---- batch: 020 ----
mean loss: 281.22
train mean loss: 281.35
epoch train time: 0:00:03.825920
elapsed time: 0:14:40.428282
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 18:03:10.622272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.64
 ---- batch: 020 ----
mean loss: 279.22
train mean loss: 279.72
epoch train time: 0:00:03.820261
elapsed time: 0:14:44.249777
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 18:03:14.443696
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.91
 ---- batch: 020 ----
mean loss: 277.74
train mean loss: 278.36
epoch train time: 0:00:03.855674
elapsed time: 0:14:48.106609
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 18:03:18.300585
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.05
 ---- batch: 020 ----
mean loss: 285.13
train mean loss: 280.93
epoch train time: 0:00:03.821805
elapsed time: 0:14:51.929638
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 18:03:22.123567
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 279.96
 ---- batch: 020 ----
mean loss: 281.01
train mean loss: 280.18
epoch train time: 0:00:03.829868
elapsed time: 0:14:55.760675
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 18:03:25.954594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.62
 ---- batch: 020 ----
mean loss: 275.92
train mean loss: 281.40
epoch train time: 0:00:03.818979
elapsed time: 0:14:59.580854
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 18:03:29.774781
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 272.20
 ---- batch: 020 ----
mean loss: 276.12
train mean loss: 280.72
epoch train time: 0:00:03.823055
elapsed time: 0:15:03.405058
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 18:03:33.599048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.21
 ---- batch: 020 ----
mean loss: 281.11
train mean loss: 277.81
epoch train time: 0:00:03.823329
elapsed time: 0:15:07.229841
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 18:03:37.423589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 279.44
 ---- batch: 020 ----
mean loss: 282.73
train mean loss: 279.64
epoch train time: 0:00:03.832367
elapsed time: 0:15:11.063160
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 18:03:41.257096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.90
 ---- batch: 020 ----
mean loss: 281.86
train mean loss: 277.48
epoch train time: 0:00:03.829127
elapsed time: 0:15:14.893441
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 18:03:45.087371
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.18
 ---- batch: 020 ----
mean loss: 279.61
train mean loss: 281.13
epoch train time: 0:00:03.830632
elapsed time: 0:15:18.725275
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 18:03:48.919219
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.04
 ---- batch: 020 ----
mean loss: 287.50
train mean loss: 282.65
epoch train time: 0:00:03.832888
elapsed time: 0:15:22.559351
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 18:03:52.753288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.19
 ---- batch: 020 ----
mean loss: 279.30
train mean loss: 279.32
epoch train time: 0:00:03.853635
elapsed time: 0:15:26.414236
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 18:03:56.608204
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 270.29
 ---- batch: 020 ----
mean loss: 283.46
train mean loss: 279.40
epoch train time: 0:00:03.852351
elapsed time: 0:15:30.267818
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 18:04:00.461791
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.78
 ---- batch: 020 ----
mean loss: 275.04
train mean loss: 279.33
epoch train time: 0:00:03.842607
elapsed time: 0:15:34.111540
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 18:04:04.305505
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 273.88
 ---- batch: 020 ----
mean loss: 284.11
train mean loss: 279.18
epoch train time: 0:00:03.817317
elapsed time: 0:15:37.930018
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 18:04:08.124025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 279.95
 ---- batch: 020 ----
mean loss: 270.73
train mean loss: 279.31
epoch train time: 0:00:03.852010
elapsed time: 0:15:41.783316
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 18:04:11.977252
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.38
 ---- batch: 020 ----
mean loss: 278.18
train mean loss: 278.70
epoch train time: 0:00:03.892128
elapsed time: 0:15:45.676819
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 18:04:15.870755
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.06
 ---- batch: 020 ----
mean loss: 274.08
train mean loss: 278.38
epoch train time: 0:00:03.878960
elapsed time: 0:15:49.556922
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 18:04:19.750851
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.11
 ---- batch: 020 ----
mean loss: 281.65
train mean loss: 278.64
epoch train time: 0:00:03.877917
elapsed time: 0:15:53.436105
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 18:04:23.630036
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 272.96
 ---- batch: 020 ----
mean loss: 268.62
train mean loss: 276.91
epoch train time: 0:00:03.877691
elapsed time: 0:15:57.314947
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 18:04:27.508892
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 279.21
 ---- batch: 020 ----
mean loss: 272.03
train mean loss: 278.70
epoch train time: 0:00:03.902119
elapsed time: 0:16:01.218476
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 18:04:31.412411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.20
 ---- batch: 020 ----
mean loss: 270.77
train mean loss: 277.17
epoch train time: 0:00:03.897039
elapsed time: 0:16:05.116768
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 18:04:35.310714
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.72
 ---- batch: 020 ----
mean loss: 284.70
train mean loss: 278.22
epoch train time: 0:00:03.879135
elapsed time: 0:16:09.005803
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_6/checkpoint.pth.tar
**** end time: 2019-09-26 18:04:39.199521 ****
