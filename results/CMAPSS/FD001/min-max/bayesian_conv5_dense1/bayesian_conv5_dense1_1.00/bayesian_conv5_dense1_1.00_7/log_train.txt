Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 23585
use_cuda: True
Dataset: CMAPSS/FD001
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-27 09:07:08.781765 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 31, 14]             200
           Sigmoid-2           [-1, 10, 31, 14]               0
    BayesianConv2d-3           [-1, 10, 30, 14]           2,000
           Sigmoid-4           [-1, 10, 30, 14]               0
    BayesianConv2d-5           [-1, 10, 31, 14]           2,000
           Sigmoid-6           [-1, 10, 31, 14]               0
    BayesianConv2d-7           [-1, 10, 30, 14]           2,000
           Sigmoid-8           [-1, 10, 30, 14]               0
    BayesianConv2d-9            [-1, 1, 30, 14]              60
         Softplus-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
   BayesianLinear-12                  [-1, 100]          84,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 90,460
Trainable params: 90,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 09:07:08.799406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2616.07
 ---- batch: 020 ----
mean loss: 1463.92
 ---- batch: 030 ----
mean loss: 1277.07
train mean loss: 1706.47
epoch train time: 0:00:17.116172
elapsed time: 0:00:17.142270
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 09:07:25.924094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1141.76
 ---- batch: 020 ----
mean loss: 1135.88
 ---- batch: 030 ----
mean loss: 1104.57
train mean loss: 1126.56
epoch train time: 0:00:07.050663
elapsed time: 0:00:24.193638
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 09:07:32.975579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1069.08
 ---- batch: 020 ----
mean loss: 1050.61
 ---- batch: 030 ----
mean loss: 1061.97
train mean loss: 1060.93
epoch train time: 0:00:06.954618
elapsed time: 0:00:31.149067
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 09:07:39.931009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1057.69
 ---- batch: 020 ----
mean loss: 1025.29
 ---- batch: 030 ----
mean loss: 1006.73
train mean loss: 1025.88
epoch train time: 0:00:06.978995
elapsed time: 0:00:38.128952
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 09:07:46.910960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1028.89
 ---- batch: 020 ----
mean loss: 1012.05
 ---- batch: 030 ----
mean loss: 1008.76
train mean loss: 1014.49
epoch train time: 0:00:06.934829
elapsed time: 0:00:45.064682
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 09:07:53.846615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 989.32
 ---- batch: 020 ----
mean loss: 1002.68
 ---- batch: 030 ----
mean loss: 996.56
train mean loss: 995.01
epoch train time: 0:00:06.960785
elapsed time: 0:00:52.026248
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 09:08:00.808195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 995.01
 ---- batch: 020 ----
mean loss: 981.61
 ---- batch: 030 ----
mean loss: 997.06
train mean loss: 996.08
epoch train time: 0:00:06.934913
elapsed time: 0:00:58.961987
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 09:08:07.743909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.89
 ---- batch: 020 ----
mean loss: 985.80
 ---- batch: 030 ----
mean loss: 964.01
train mean loss: 973.17
epoch train time: 0:00:06.911742
elapsed time: 0:01:05.874609
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 09:08:14.656546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.18
 ---- batch: 020 ----
mean loss: 937.63
 ---- batch: 030 ----
mean loss: 931.13
train mean loss: 943.22
epoch train time: 0:00:06.927708
elapsed time: 0:01:12.803102
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 09:08:21.585029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.88
 ---- batch: 020 ----
mean loss: 835.48
 ---- batch: 030 ----
mean loss: 763.32
train mean loss: 816.34
epoch train time: 0:00:06.940830
elapsed time: 0:01:19.744736
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 09:08:28.526669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 683.87
 ---- batch: 020 ----
mean loss: 629.95
 ---- batch: 030 ----
mean loss: 543.18
train mean loss: 603.44
epoch train time: 0:00:06.955316
elapsed time: 0:01:26.700812
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 09:08:35.482727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.90
 ---- batch: 020 ----
mean loss: 459.94
 ---- batch: 030 ----
mean loss: 434.73
train mean loss: 457.05
epoch train time: 0:00:06.827913
elapsed time: 0:01:33.529530
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 09:08:42.311468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.84
 ---- batch: 020 ----
mean loss: 413.87
 ---- batch: 030 ----
mean loss: 403.61
train mean loss: 409.95
epoch train time: 0:00:06.873605
elapsed time: 0:01:40.404018
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 09:08:49.185981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.48
 ---- batch: 020 ----
mean loss: 403.98
 ---- batch: 030 ----
mean loss: 389.27
train mean loss: 398.23
epoch train time: 0:00:06.800141
elapsed time: 0:01:47.204955
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 09:08:55.986928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.62
 ---- batch: 020 ----
mean loss: 393.43
 ---- batch: 030 ----
mean loss: 385.24
train mean loss: 389.64
epoch train time: 0:00:06.835830
elapsed time: 0:01:54.041589
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 09:09:02.823526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.60
 ---- batch: 020 ----
mean loss: 377.31
 ---- batch: 030 ----
mean loss: 364.66
train mean loss: 371.39
epoch train time: 0:00:06.884500
elapsed time: 0:02:00.926871
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 09:09:09.708782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.62
 ---- batch: 020 ----
mean loss: 373.30
 ---- batch: 030 ----
mean loss: 372.33
train mean loss: 366.09
epoch train time: 0:00:06.767439
elapsed time: 0:02:07.695035
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 09:09:16.476964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.14
 ---- batch: 020 ----
mean loss: 350.84
 ---- batch: 030 ----
mean loss: 346.25
train mean loss: 349.86
epoch train time: 0:00:06.876630
elapsed time: 0:02:14.572389
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 09:09:23.354314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.68
 ---- batch: 020 ----
mean loss: 333.63
 ---- batch: 030 ----
mean loss: 334.24
train mean loss: 332.74
epoch train time: 0:00:06.721242
elapsed time: 0:02:21.294362
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 09:09:30.076292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.97
 ---- batch: 020 ----
mean loss: 318.19
 ---- batch: 030 ----
mean loss: 318.63
train mean loss: 320.62
epoch train time: 0:00:06.763274
elapsed time: 0:02:28.058390
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 09:09:36.840325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.12
 ---- batch: 020 ----
mean loss: 305.19
 ---- batch: 030 ----
mean loss: 289.87
train mean loss: 297.06
epoch train time: 0:00:06.706681
elapsed time: 0:02:34.765853
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 09:09:43.547778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.63
 ---- batch: 020 ----
mean loss: 287.00
 ---- batch: 030 ----
mean loss: 270.85
train mean loss: 278.55
epoch train time: 0:00:06.874444
elapsed time: 0:02:41.641064
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 09:09:50.422985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.87
 ---- batch: 020 ----
mean loss: 263.37
 ---- batch: 030 ----
mean loss: 260.38
train mean loss: 265.28
epoch train time: 0:00:06.855511
elapsed time: 0:02:48.497410
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 09:09:57.279325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.32
 ---- batch: 020 ----
mean loss: 245.77
 ---- batch: 030 ----
mean loss: 237.06
train mean loss: 245.77
epoch train time: 0:00:06.904151
elapsed time: 0:02:55.402314
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 09:10:04.184242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.18
 ---- batch: 020 ----
mean loss: 235.92
 ---- batch: 030 ----
mean loss: 227.31
train mean loss: 232.26
epoch train time: 0:00:06.778275
elapsed time: 0:03:02.181374
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 09:10:10.963295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.92
 ---- batch: 020 ----
mean loss: 232.71
 ---- batch: 030 ----
mean loss: 227.46
train mean loss: 230.60
epoch train time: 0:00:06.991055
elapsed time: 0:03:09.173233
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 09:10:17.955192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.54
 ---- batch: 020 ----
mean loss: 231.15
 ---- batch: 030 ----
mean loss: 210.90
train mean loss: 224.99
epoch train time: 0:00:06.882221
elapsed time: 0:03:16.056258
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 09:10:24.838182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.57
 ---- batch: 020 ----
mean loss: 211.91
 ---- batch: 030 ----
mean loss: 220.92
train mean loss: 215.64
epoch train time: 0:00:06.909382
elapsed time: 0:03:22.966386
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 09:10:31.748308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.02
 ---- batch: 020 ----
mean loss: 213.96
 ---- batch: 030 ----
mean loss: 208.79
train mean loss: 211.93
epoch train time: 0:00:06.919162
elapsed time: 0:03:29.886325
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 09:10:38.668257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.36
 ---- batch: 020 ----
mean loss: 207.85
 ---- batch: 030 ----
mean loss: 206.10
train mean loss: 205.46
epoch train time: 0:00:06.862879
elapsed time: 0:03:36.750020
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 09:10:45.531995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.47
 ---- batch: 020 ----
mean loss: 199.28
 ---- batch: 030 ----
mean loss: 199.08
train mean loss: 196.94
epoch train time: 0:00:06.913180
elapsed time: 0:03:43.664046
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 09:10:52.445970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.98
 ---- batch: 020 ----
mean loss: 199.65
 ---- batch: 030 ----
mean loss: 198.70
train mean loss: 198.25
epoch train time: 0:00:06.838142
elapsed time: 0:03:50.503008
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 09:10:59.284939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.68
 ---- batch: 020 ----
mean loss: 202.18
 ---- batch: 030 ----
mean loss: 201.07
train mean loss: 198.79
epoch train time: 0:00:06.941458
elapsed time: 0:03:57.445234
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 09:11:06.227175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.65
 ---- batch: 020 ----
mean loss: 193.67
 ---- batch: 030 ----
mean loss: 196.66
train mean loss: 193.80
epoch train time: 0:00:06.802942
elapsed time: 0:04:04.249077
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 09:11:13.031007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.70
 ---- batch: 020 ----
mean loss: 193.56
 ---- batch: 030 ----
mean loss: 194.09
train mean loss: 191.90
epoch train time: 0:00:06.892704
elapsed time: 0:04:11.142536
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 09:11:19.924492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.00
 ---- batch: 020 ----
mean loss: 191.34
 ---- batch: 030 ----
mean loss: 191.19
train mean loss: 191.93
epoch train time: 0:00:06.908323
elapsed time: 0:04:18.051690
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 09:11:26.833620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.14
 ---- batch: 020 ----
mean loss: 191.65
 ---- batch: 030 ----
mean loss: 177.52
train mean loss: 183.97
epoch train time: 0:00:06.865518
elapsed time: 0:04:24.918073
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 09:11:33.699993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.62
 ---- batch: 020 ----
mean loss: 178.27
 ---- batch: 030 ----
mean loss: 183.72
train mean loss: 181.01
epoch train time: 0:00:06.945395
elapsed time: 0:04:31.864293
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 09:11:40.646210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.12
 ---- batch: 020 ----
mean loss: 184.58
 ---- batch: 030 ----
mean loss: 185.63
train mean loss: 182.43
epoch train time: 0:00:06.963487
elapsed time: 0:04:38.828512
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 09:11:47.610452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.31
 ---- batch: 020 ----
mean loss: 176.23
 ---- batch: 030 ----
mean loss: 180.55
train mean loss: 177.27
epoch train time: 0:00:06.966659
elapsed time: 0:04:45.796086
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 09:11:54.578055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.77
 ---- batch: 020 ----
mean loss: 166.76
 ---- batch: 030 ----
mean loss: 175.29
train mean loss: 174.93
epoch train time: 0:00:06.954724
elapsed time: 0:04:52.751621
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 09:12:01.533551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.82
 ---- batch: 020 ----
mean loss: 173.99
 ---- batch: 030 ----
mean loss: 172.52
train mean loss: 173.40
epoch train time: 0:00:06.912595
elapsed time: 0:04:59.664975
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 09:12:08.446912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.27
 ---- batch: 020 ----
mean loss: 168.65
 ---- batch: 030 ----
mean loss: 170.31
train mean loss: 171.65
epoch train time: 0:00:06.956385
elapsed time: 0:05:06.622277
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 09:12:15.404203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.82
 ---- batch: 020 ----
mean loss: 167.84
 ---- batch: 030 ----
mean loss: 167.66
train mean loss: 168.48
epoch train time: 0:00:06.842706
elapsed time: 0:05:13.465837
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 09:12:22.247793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.77
 ---- batch: 020 ----
mean loss: 174.87
 ---- batch: 030 ----
mean loss: 170.25
train mean loss: 169.45
epoch train time: 0:00:06.945086
elapsed time: 0:05:20.411718
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 09:12:29.193675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.57
 ---- batch: 020 ----
mean loss: 173.69
 ---- batch: 030 ----
mean loss: 169.82
train mean loss: 168.78
epoch train time: 0:00:06.733625
elapsed time: 0:05:27.146298
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 09:12:35.928297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.75
 ---- batch: 020 ----
mean loss: 168.21
 ---- batch: 030 ----
mean loss: 165.75
train mean loss: 166.00
epoch train time: 0:00:06.860555
elapsed time: 0:05:34.007862
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 09:12:42.789801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.64
 ---- batch: 020 ----
mean loss: 168.07
 ---- batch: 030 ----
mean loss: 162.30
train mean loss: 163.38
epoch train time: 0:00:06.834909
elapsed time: 0:05:40.843559
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 09:12:49.625476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.84
 ---- batch: 020 ----
mean loss: 166.15
 ---- batch: 030 ----
mean loss: 164.54
train mean loss: 162.58
epoch train time: 0:00:06.831827
elapsed time: 0:05:47.676237
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 09:12:56.458156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.29
 ---- batch: 020 ----
mean loss: 162.46
 ---- batch: 030 ----
mean loss: 166.73
train mean loss: 161.80
epoch train time: 0:00:06.864868
elapsed time: 0:05:54.541828
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 09:13:03.323771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.07
 ---- batch: 020 ----
mean loss: 157.27
 ---- batch: 030 ----
mean loss: 159.48
train mean loss: 160.48
epoch train time: 0:00:06.727147
elapsed time: 0:06:01.269855
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 09:13:10.051784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.58
 ---- batch: 020 ----
mean loss: 158.14
 ---- batch: 030 ----
mean loss: 159.22
train mean loss: 157.63
epoch train time: 0:00:06.931349
elapsed time: 0:06:08.201982
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 09:13:16.983886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.89
 ---- batch: 020 ----
mean loss: 152.57
 ---- batch: 030 ----
mean loss: 156.79
train mean loss: 155.00
epoch train time: 0:00:06.796541
elapsed time: 0:06:14.999246
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 09:13:23.781164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.53
 ---- batch: 020 ----
mean loss: 159.03
 ---- batch: 030 ----
mean loss: 151.31
train mean loss: 156.46
epoch train time: 0:00:06.841564
elapsed time: 0:06:21.841601
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 09:13:30.623529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.40
 ---- batch: 020 ----
mean loss: 154.04
 ---- batch: 030 ----
mean loss: 159.79
train mean loss: 155.32
epoch train time: 0:00:06.774243
elapsed time: 0:06:28.616626
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 09:13:37.398548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.01
 ---- batch: 020 ----
mean loss: 155.89
 ---- batch: 030 ----
mean loss: 152.70
train mean loss: 153.51
epoch train time: 0:00:06.807526
elapsed time: 0:06:35.424994
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 09:13:44.206906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.38
 ---- batch: 020 ----
mean loss: 151.62
 ---- batch: 030 ----
mean loss: 151.83
train mean loss: 152.97
epoch train time: 0:00:06.720700
elapsed time: 0:06:42.146498
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 09:13:50.928467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.92
 ---- batch: 020 ----
mean loss: 148.83
 ---- batch: 030 ----
mean loss: 150.91
train mean loss: 149.20
epoch train time: 0:00:06.821116
elapsed time: 0:06:48.968454
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 09:13:57.750384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.80
 ---- batch: 020 ----
mean loss: 146.81
 ---- batch: 030 ----
mean loss: 148.79
train mean loss: 147.52
epoch train time: 0:00:06.764333
elapsed time: 0:06:55.733610
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 09:14:04.515534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.17
 ---- batch: 020 ----
mean loss: 146.31
 ---- batch: 030 ----
mean loss: 143.91
train mean loss: 147.89
epoch train time: 0:00:06.802983
elapsed time: 0:07:02.537363
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 09:14:11.319300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.97
 ---- batch: 020 ----
mean loss: 144.86
 ---- batch: 030 ----
mean loss: 148.01
train mean loss: 146.39
epoch train time: 0:00:06.818921
elapsed time: 0:07:09.357117
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 09:14:18.139047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.45
 ---- batch: 020 ----
mean loss: 148.00
 ---- batch: 030 ----
mean loss: 148.22
train mean loss: 147.61
epoch train time: 0:00:06.814943
elapsed time: 0:07:16.172814
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 09:14:24.954729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.95
 ---- batch: 020 ----
mean loss: 145.29
 ---- batch: 030 ----
mean loss: 145.52
train mean loss: 146.02
epoch train time: 0:00:06.694737
elapsed time: 0:07:22.868316
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 09:14:31.650238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.54
 ---- batch: 020 ----
mean loss: 146.36
 ---- batch: 030 ----
mean loss: 146.91
train mean loss: 144.67
epoch train time: 0:00:06.796229
elapsed time: 0:07:29.665399
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 09:14:38.447332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.56
 ---- batch: 020 ----
mean loss: 145.01
 ---- batch: 030 ----
mean loss: 141.88
train mean loss: 143.37
epoch train time: 0:00:06.696978
elapsed time: 0:07:36.363185
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 09:14:45.145119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.17
 ---- batch: 020 ----
mean loss: 140.06
 ---- batch: 030 ----
mean loss: 147.05
train mean loss: 144.12
epoch train time: 0:00:06.777591
elapsed time: 0:07:43.141576
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 09:14:51.923496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.83
 ---- batch: 020 ----
mean loss: 142.56
 ---- batch: 030 ----
mean loss: 141.65
train mean loss: 142.27
epoch train time: 0:00:06.720362
elapsed time: 0:07:49.862702
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 09:14:58.644718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.29
 ---- batch: 020 ----
mean loss: 144.44
 ---- batch: 030 ----
mean loss: 144.21
train mean loss: 142.57
epoch train time: 0:00:06.787366
elapsed time: 0:07:56.650920
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 09:15:05.432836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.29
 ---- batch: 020 ----
mean loss: 137.07
 ---- batch: 030 ----
mean loss: 133.29
train mean loss: 138.38
epoch train time: 0:00:06.737621
elapsed time: 0:08:03.389315
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 09:15:12.171228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.53
 ---- batch: 020 ----
mean loss: 136.29
 ---- batch: 030 ----
mean loss: 139.35
train mean loss: 138.44
epoch train time: 0:00:06.790486
elapsed time: 0:08:10.180675
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 09:15:18.962597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.53
 ---- batch: 020 ----
mean loss: 137.47
 ---- batch: 030 ----
mean loss: 138.16
train mean loss: 138.20
epoch train time: 0:00:06.688684
elapsed time: 0:08:16.870086
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 09:15:25.652045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.52
 ---- batch: 020 ----
mean loss: 137.00
 ---- batch: 030 ----
mean loss: 135.86
train mean loss: 136.67
epoch train time: 0:00:06.647050
elapsed time: 0:08:23.517957
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 09:15:32.299951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.82
 ---- batch: 020 ----
mean loss: 133.43
 ---- batch: 030 ----
mean loss: 133.84
train mean loss: 134.11
epoch train time: 0:00:06.770155
elapsed time: 0:08:30.288964
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 09:15:39.070914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.77
 ---- batch: 020 ----
mean loss: 137.06
 ---- batch: 030 ----
mean loss: 139.87
train mean loss: 136.97
epoch train time: 0:00:06.723844
elapsed time: 0:08:37.013694
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 09:15:45.795622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.13
 ---- batch: 020 ----
mean loss: 136.07
 ---- batch: 030 ----
mean loss: 132.37
train mean loss: 133.78
epoch train time: 0:00:06.825674
elapsed time: 0:08:43.840152
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 09:15:52.622076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.12
 ---- batch: 020 ----
mean loss: 137.18
 ---- batch: 030 ----
mean loss: 136.43
train mean loss: 135.61
epoch train time: 0:00:06.816876
elapsed time: 0:08:50.657854
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 09:15:59.439762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.21
 ---- batch: 020 ----
mean loss: 129.92
 ---- batch: 030 ----
mean loss: 125.91
train mean loss: 130.91
epoch train time: 0:00:06.679974
elapsed time: 0:08:57.338580
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 09:16:06.120510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.99
 ---- batch: 020 ----
mean loss: 128.18
 ---- batch: 030 ----
mean loss: 137.25
train mean loss: 132.57
epoch train time: 0:00:06.841028
elapsed time: 0:09:04.180480
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 09:16:12.962408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.76
 ---- batch: 020 ----
mean loss: 128.31
 ---- batch: 030 ----
mean loss: 134.25
train mean loss: 130.80
epoch train time: 0:00:06.746228
elapsed time: 0:09:10.927456
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 09:16:19.709386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.73
 ---- batch: 020 ----
mean loss: 137.09
 ---- batch: 030 ----
mean loss: 133.49
train mean loss: 132.96
epoch train time: 0:00:06.794198
elapsed time: 0:09:17.722401
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 09:16:26.504325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.87
 ---- batch: 020 ----
mean loss: 127.69
 ---- batch: 030 ----
mean loss: 129.13
train mean loss: 130.13
epoch train time: 0:00:06.681561
elapsed time: 0:09:24.404714
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 09:16:33.186638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.35
 ---- batch: 020 ----
mean loss: 130.58
 ---- batch: 030 ----
mean loss: 126.58
train mean loss: 127.63
epoch train time: 0:00:06.687447
elapsed time: 0:09:31.092948
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 09:16:39.874875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.98
 ---- batch: 020 ----
mean loss: 129.02
 ---- batch: 030 ----
mean loss: 127.53
train mean loss: 128.62
epoch train time: 0:00:06.743653
elapsed time: 0:09:37.837350
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 09:16:46.619275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.97
 ---- batch: 020 ----
mean loss: 127.88
 ---- batch: 030 ----
mean loss: 127.55
train mean loss: 129.34
epoch train time: 0:00:06.796229
elapsed time: 0:09:44.634334
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 09:16:53.416263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.60
 ---- batch: 020 ----
mean loss: 132.03
 ---- batch: 030 ----
mean loss: 129.13
train mean loss: 128.13
epoch train time: 0:00:06.868526
elapsed time: 0:09:51.503629
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 09:17:00.285556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.21
 ---- batch: 020 ----
mean loss: 125.29
 ---- batch: 030 ----
mean loss: 124.75
train mean loss: 125.79
epoch train time: 0:00:06.708565
elapsed time: 0:09:58.213013
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 09:17:06.994951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.72
 ---- batch: 020 ----
mean loss: 129.32
 ---- batch: 030 ----
mean loss: 129.00
train mean loss: 127.03
epoch train time: 0:00:06.793161
elapsed time: 0:10:05.006978
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 09:17:13.788926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.11
 ---- batch: 020 ----
mean loss: 130.01
 ---- batch: 030 ----
mean loss: 123.82
train mean loss: 127.67
epoch train time: 0:00:06.813903
elapsed time: 0:10:11.821690
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 09:17:20.603639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.22
 ---- batch: 020 ----
mean loss: 125.66
 ---- batch: 030 ----
mean loss: 120.37
train mean loss: 125.80
epoch train time: 0:00:06.772711
elapsed time: 0:10:18.595239
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 09:17:27.377205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.97
 ---- batch: 020 ----
mean loss: 122.83
 ---- batch: 030 ----
mean loss: 127.77
train mean loss: 125.38
epoch train time: 0:00:06.721763
elapsed time: 0:10:25.317875
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 09:17:34.099845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.62
 ---- batch: 020 ----
mean loss: 120.57
 ---- batch: 030 ----
mean loss: 124.05
train mean loss: 122.86
epoch train time: 0:00:06.825043
elapsed time: 0:10:32.143722
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 09:17:40.925685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.63
 ---- batch: 020 ----
mean loss: 124.70
 ---- batch: 030 ----
mean loss: 124.11
train mean loss: 124.43
epoch train time: 0:00:06.795414
elapsed time: 0:10:38.939982
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 09:17:47.721938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.88
 ---- batch: 020 ----
mean loss: 123.44
 ---- batch: 030 ----
mean loss: 124.00
train mean loss: 123.83
epoch train time: 0:00:06.741496
elapsed time: 0:10:45.682254
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 09:17:54.464176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.39
 ---- batch: 020 ----
mean loss: 120.84
 ---- batch: 030 ----
mean loss: 125.91
train mean loss: 122.72
epoch train time: 0:00:06.798541
elapsed time: 0:10:52.481617
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 09:18:01.263591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.40
 ---- batch: 020 ----
mean loss: 126.80
 ---- batch: 030 ----
mean loss: 124.37
train mean loss: 123.77
epoch train time: 0:00:06.867188
elapsed time: 0:10:59.349613
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 09:18:08.131542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.58
 ---- batch: 020 ----
mean loss: 119.84
 ---- batch: 030 ----
mean loss: 127.56
train mean loss: 122.74
epoch train time: 0:00:06.914242
elapsed time: 0:11:06.264706
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 09:18:15.046664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.23
 ---- batch: 020 ----
mean loss: 122.79
 ---- batch: 030 ----
mean loss: 122.46
train mean loss: 122.26
epoch train time: 0:00:06.832856
elapsed time: 0:11:13.098383
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 09:18:21.880305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.11
 ---- batch: 020 ----
mean loss: 119.31
 ---- batch: 030 ----
mean loss: 123.22
train mean loss: 120.75
epoch train time: 0:00:06.921449
elapsed time: 0:11:20.020611
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 09:18:28.802542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.66
 ---- batch: 020 ----
mean loss: 121.03
 ---- batch: 030 ----
mean loss: 123.97
train mean loss: 122.23
epoch train time: 0:00:06.887188
elapsed time: 0:11:26.908649
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 09:18:35.690578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.22
 ---- batch: 020 ----
mean loss: 119.05
 ---- batch: 030 ----
mean loss: 115.80
train mean loss: 121.00
epoch train time: 0:00:06.804379
elapsed time: 0:11:33.713824
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 09:18:42.495775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.72
 ---- batch: 020 ----
mean loss: 116.03
 ---- batch: 030 ----
mean loss: 117.07
train mean loss: 117.44
epoch train time: 0:00:06.830390
elapsed time: 0:11:40.545051
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 09:18:49.326993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.17
 ---- batch: 020 ----
mean loss: 118.07
 ---- batch: 030 ----
mean loss: 117.12
train mean loss: 117.67
epoch train time: 0:00:06.812106
elapsed time: 0:11:47.357933
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 09:18:56.139867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.98
 ---- batch: 020 ----
mean loss: 120.70
 ---- batch: 030 ----
mean loss: 118.13
train mean loss: 117.60
epoch train time: 0:00:06.859608
elapsed time: 0:11:54.218457
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 09:19:03.000394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.16
 ---- batch: 020 ----
mean loss: 117.47
 ---- batch: 030 ----
mean loss: 116.04
train mean loss: 117.09
epoch train time: 0:00:06.808226
elapsed time: 0:12:01.027464
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 09:19:09.809382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.96
 ---- batch: 020 ----
mean loss: 116.50
 ---- batch: 030 ----
mean loss: 115.93
train mean loss: 117.47
epoch train time: 0:00:06.841710
elapsed time: 0:12:07.869901
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 09:19:16.651820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.24
 ---- batch: 020 ----
mean loss: 115.93
 ---- batch: 030 ----
mean loss: 116.40
train mean loss: 115.14
epoch train time: 0:00:06.828475
elapsed time: 0:12:14.699084
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 09:19:23.481004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.46
 ---- batch: 020 ----
mean loss: 118.12
 ---- batch: 030 ----
mean loss: 113.57
train mean loss: 116.34
epoch train time: 0:00:06.667932
elapsed time: 0:12:21.367922
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 09:19:30.149773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.66
 ---- batch: 020 ----
mean loss: 114.82
 ---- batch: 030 ----
mean loss: 114.97
train mean loss: 115.51
epoch train time: 0:00:06.879471
elapsed time: 0:12:28.248085
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 09:19:37.030058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.99
 ---- batch: 020 ----
mean loss: 118.31
 ---- batch: 030 ----
mean loss: 111.96
train mean loss: 115.46
epoch train time: 0:00:06.718616
elapsed time: 0:12:34.967497
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 09:19:43.749410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.95
 ---- batch: 020 ----
mean loss: 117.20
 ---- batch: 030 ----
mean loss: 116.70
train mean loss: 115.82
epoch train time: 0:00:06.801371
elapsed time: 0:12:41.769703
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 09:19:50.551657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.44
 ---- batch: 020 ----
mean loss: 112.88
 ---- batch: 030 ----
mean loss: 116.85
train mean loss: 114.49
epoch train time: 0:00:06.823431
elapsed time: 0:12:48.594019
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 09:19:57.375949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.98
 ---- batch: 020 ----
mean loss: 113.36
 ---- batch: 030 ----
mean loss: 110.14
train mean loss: 113.82
epoch train time: 0:00:06.639209
elapsed time: 0:12:55.234007
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 09:20:04.015932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.69
 ---- batch: 020 ----
mean loss: 114.04
 ---- batch: 030 ----
mean loss: 114.41
train mean loss: 114.00
epoch train time: 0:00:06.714574
elapsed time: 0:13:01.949368
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 09:20:10.731319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.01
 ---- batch: 020 ----
mean loss: 108.21
 ---- batch: 030 ----
mean loss: 112.15
train mean loss: 111.26
epoch train time: 0:00:06.725769
elapsed time: 0:13:08.675963
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 09:20:17.457904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.83
 ---- batch: 020 ----
mean loss: 112.86
 ---- batch: 030 ----
mean loss: 111.14
train mean loss: 112.31
epoch train time: 0:00:06.829797
elapsed time: 0:13:15.506528
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 09:20:24.288486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.59
 ---- batch: 020 ----
mean loss: 111.60
 ---- batch: 030 ----
mean loss: 111.68
train mean loss: 111.32
epoch train time: 0:00:06.851566
elapsed time: 0:13:22.358898
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 09:20:31.140815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.77
 ---- batch: 020 ----
mean loss: 112.44
 ---- batch: 030 ----
mean loss: 113.97
train mean loss: 112.52
epoch train time: 0:00:06.755063
elapsed time: 0:13:29.114714
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 09:20:37.896659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.34
 ---- batch: 020 ----
mean loss: 111.83
 ---- batch: 030 ----
mean loss: 113.06
train mean loss: 113.24
epoch train time: 0:00:06.779330
elapsed time: 0:13:35.894840
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 09:20:44.676769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.37
 ---- batch: 020 ----
mean loss: 110.85
 ---- batch: 030 ----
mean loss: 112.90
train mean loss: 112.29
epoch train time: 0:00:06.784316
elapsed time: 0:13:42.680039
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 09:20:51.461986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.18
 ---- batch: 020 ----
mean loss: 112.75
 ---- batch: 030 ----
mean loss: 110.75
train mean loss: 111.06
epoch train time: 0:00:06.739895
elapsed time: 0:13:49.420715
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 09:20:58.202638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.62
 ---- batch: 020 ----
mean loss: 112.86
 ---- batch: 030 ----
mean loss: 106.32
train mean loss: 110.72
epoch train time: 0:00:06.778568
elapsed time: 0:13:56.200050
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 09:21:04.981973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.83
 ---- batch: 020 ----
mean loss: 107.76
 ---- batch: 030 ----
mean loss: 112.84
train mean loss: 109.86
epoch train time: 0:00:06.990426
elapsed time: 0:14:03.191293
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 09:21:11.973251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.65
 ---- batch: 020 ----
mean loss: 109.94
 ---- batch: 030 ----
mean loss: 112.86
train mean loss: 110.13
epoch train time: 0:00:06.905653
elapsed time: 0:14:10.097786
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 09:21:18.879723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.62
 ---- batch: 020 ----
mean loss: 107.90
 ---- batch: 030 ----
mean loss: 110.85
train mean loss: 110.44
epoch train time: 0:00:06.980893
elapsed time: 0:14:17.079515
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 09:21:25.861443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.42
 ---- batch: 020 ----
mean loss: 108.59
 ---- batch: 030 ----
mean loss: 110.42
train mean loss: 108.92
epoch train time: 0:00:06.942217
elapsed time: 0:14:24.022478
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 09:21:32.804398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.73
 ---- batch: 020 ----
mean loss: 109.05
 ---- batch: 030 ----
mean loss: 107.24
train mean loss: 109.09
epoch train time: 0:00:06.784143
elapsed time: 0:14:30.807362
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 09:21:39.589314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.61
 ---- batch: 020 ----
mean loss: 106.50
 ---- batch: 030 ----
mean loss: 109.32
train mean loss: 108.56
epoch train time: 0:00:06.833608
elapsed time: 0:14:37.641882
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 09:21:46.423699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.24
 ---- batch: 020 ----
mean loss: 107.64
 ---- batch: 030 ----
mean loss: 111.86
train mean loss: 109.56
epoch train time: 0:00:06.799718
elapsed time: 0:14:44.442235
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 09:21:53.224176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.82
 ---- batch: 020 ----
mean loss: 107.93
 ---- batch: 030 ----
mean loss: 106.14
train mean loss: 108.25
epoch train time: 0:00:06.795869
elapsed time: 0:14:51.238952
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 09:22:00.020974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.88
 ---- batch: 020 ----
mean loss: 105.80
 ---- batch: 030 ----
mean loss: 105.68
train mean loss: 107.07
epoch train time: 0:00:06.796444
elapsed time: 0:14:58.036344
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 09:22:06.818292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.54
 ---- batch: 020 ----
mean loss: 109.25
 ---- batch: 030 ----
mean loss: 108.46
train mean loss: 108.07
epoch train time: 0:00:06.786079
elapsed time: 0:15:04.823268
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 09:22:13.605204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.88
 ---- batch: 020 ----
mean loss: 104.93
 ---- batch: 030 ----
mean loss: 109.24
train mean loss: 108.28
epoch train time: 0:00:06.788765
elapsed time: 0:15:11.612857
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 09:22:20.394809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.49
 ---- batch: 020 ----
mean loss: 106.77
 ---- batch: 030 ----
mean loss: 103.84
train mean loss: 107.90
epoch train time: 0:00:06.838482
elapsed time: 0:15:18.452265
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 09:22:27.234244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.13
 ---- batch: 020 ----
mean loss: 107.77
 ---- batch: 030 ----
mean loss: 108.84
train mean loss: 107.52
epoch train time: 0:00:06.735553
elapsed time: 0:15:25.188662
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 09:22:33.970592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.63
 ---- batch: 020 ----
mean loss: 103.99
 ---- batch: 030 ----
mean loss: 105.61
train mean loss: 104.24
epoch train time: 0:00:06.834103
elapsed time: 0:15:32.023547
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 09:22:40.805466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.39
 ---- batch: 020 ----
mean loss: 101.25
 ---- batch: 030 ----
mean loss: 108.07
train mean loss: 104.94
epoch train time: 0:00:06.807758
elapsed time: 0:15:38.832065
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 09:22:47.613987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.66
 ---- batch: 020 ----
mean loss: 107.37
 ---- batch: 030 ----
mean loss: 104.97
train mean loss: 106.43
epoch train time: 0:00:06.862370
elapsed time: 0:15:45.695232
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 09:22:54.477211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.65
 ---- batch: 020 ----
mean loss: 104.47
 ---- batch: 030 ----
mean loss: 103.19
train mean loss: 104.56
epoch train time: 0:00:06.888293
elapsed time: 0:15:52.584395
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 09:23:01.366338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.33
 ---- batch: 020 ----
mean loss: 107.07
 ---- batch: 030 ----
mean loss: 103.76
train mean loss: 105.55
epoch train time: 0:00:06.800012
elapsed time: 0:15:59.385195
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 09:23:08.167111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.05
 ---- batch: 020 ----
mean loss: 105.49
 ---- batch: 030 ----
mean loss: 105.91
train mean loss: 104.70
epoch train time: 0:00:06.847193
elapsed time: 0:16:06.233195
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 09:23:15.015205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.71
 ---- batch: 020 ----
mean loss: 105.17
 ---- batch: 030 ----
mean loss: 106.39
train mean loss: 106.19
epoch train time: 0:00:06.912135
elapsed time: 0:16:13.146172
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 09:23:21.928140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.76
 ---- batch: 020 ----
mean loss: 106.84
 ---- batch: 030 ----
mean loss: 108.02
train mean loss: 108.07
epoch train time: 0:00:06.760076
elapsed time: 0:16:19.907170
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 09:23:28.689109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.45
 ---- batch: 020 ----
mean loss: 107.79
 ---- batch: 030 ----
mean loss: 107.68
train mean loss: 106.11
epoch train time: 0:00:06.846511
elapsed time: 0:16:26.754530
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 09:23:35.536465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.20
 ---- batch: 020 ----
mean loss: 102.84
 ---- batch: 030 ----
mean loss: 104.79
train mean loss: 103.14
epoch train time: 0:00:06.821387
elapsed time: 0:16:33.576672
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 09:23:42.358596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.58
 ---- batch: 020 ----
mean loss: 104.15
 ---- batch: 030 ----
mean loss: 104.57
train mean loss: 103.83
epoch train time: 0:00:06.707866
elapsed time: 0:16:40.285330
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 09:23:49.067268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.41
 ---- batch: 020 ----
mean loss: 106.58
 ---- batch: 030 ----
mean loss: 100.41
train mean loss: 103.74
epoch train time: 0:00:06.757668
elapsed time: 0:16:47.043895
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 09:23:55.825876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.64
 ---- batch: 020 ----
mean loss: 105.76
 ---- batch: 030 ----
mean loss: 101.20
train mean loss: 103.14
epoch train time: 0:00:06.874516
elapsed time: 0:16:53.919213
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 09:24:02.701136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.70
 ---- batch: 020 ----
mean loss: 100.06
 ---- batch: 030 ----
mean loss: 101.69
train mean loss: 101.52
epoch train time: 0:00:06.775489
elapsed time: 0:17:00.695460
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 09:24:09.477418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.88
 ---- batch: 020 ----
mean loss: 102.62
 ---- batch: 030 ----
mean loss: 101.16
train mean loss: 103.16
epoch train time: 0:00:06.852360
elapsed time: 0:17:07.548771
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 09:24:16.330586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.82
 ---- batch: 020 ----
mean loss: 100.43
 ---- batch: 030 ----
mean loss: 102.41
train mean loss: 102.24
epoch train time: 0:00:06.788084
elapsed time: 0:17:14.337575
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 09:24:23.119527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.78
 ---- batch: 020 ----
mean loss: 104.85
 ---- batch: 030 ----
mean loss: 103.20
train mean loss: 102.50
epoch train time: 0:00:06.816653
elapsed time: 0:17:21.155123
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 09:24:29.937056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.49
 ---- batch: 020 ----
mean loss: 101.96
 ---- batch: 030 ----
mean loss: 104.76
train mean loss: 102.08
epoch train time: 0:00:06.858056
elapsed time: 0:17:28.014008
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 09:24:36.795983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.09
 ---- batch: 020 ----
mean loss: 105.90
 ---- batch: 030 ----
mean loss: 98.43
train mean loss: 102.03
epoch train time: 0:00:06.824632
elapsed time: 0:17:34.839461
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 09:24:43.621377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.51
 ---- batch: 020 ----
mean loss: 100.79
 ---- batch: 030 ----
mean loss: 100.94
train mean loss: 100.92
epoch train time: 0:00:06.906300
elapsed time: 0:17:41.746586
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 09:24:50.528544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.29
 ---- batch: 020 ----
mean loss: 101.74
 ---- batch: 030 ----
mean loss: 100.64
train mean loss: 101.51
epoch train time: 0:00:06.883608
elapsed time: 0:17:48.631286
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 09:24:57.413293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.11
 ---- batch: 020 ----
mean loss: 102.48
 ---- batch: 030 ----
mean loss: 101.42
train mean loss: 100.96
epoch train time: 0:00:06.653672
elapsed time: 0:17:55.285800
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 09:25:04.067719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.42
 ---- batch: 020 ----
mean loss: 98.27
 ---- batch: 030 ----
mean loss: 99.11
train mean loss: 100.42
epoch train time: 0:00:06.755291
elapsed time: 0:18:02.041833
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 09:25:10.823742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.16
 ---- batch: 020 ----
mean loss: 97.78
 ---- batch: 030 ----
mean loss: 103.18
train mean loss: 100.00
epoch train time: 0:00:06.735491
elapsed time: 0:18:08.778053
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 09:25:17.559981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.38
 ---- batch: 020 ----
mean loss: 100.21
 ---- batch: 030 ----
mean loss: 100.68
train mean loss: 99.73
epoch train time: 0:00:06.636843
elapsed time: 0:18:15.415801
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 09:25:24.197732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.72
 ---- batch: 020 ----
mean loss: 98.50
 ---- batch: 030 ----
mean loss: 96.71
train mean loss: 98.92
epoch train time: 0:00:06.733253
elapsed time: 0:18:22.149835
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 09:25:30.931768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.27
 ---- batch: 020 ----
mean loss: 98.36
 ---- batch: 030 ----
mean loss: 100.47
train mean loss: 99.17
epoch train time: 0:00:06.680336
elapsed time: 0:18:28.830965
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 09:25:37.612876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.40
 ---- batch: 020 ----
mean loss: 103.25
 ---- batch: 030 ----
mean loss: 99.36
train mean loss: 101.55
epoch train time: 0:00:06.959082
elapsed time: 0:18:35.790802
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 09:25:44.572759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.40
 ---- batch: 020 ----
mean loss: 100.22
 ---- batch: 030 ----
mean loss: 98.57
train mean loss: 98.31
epoch train time: 0:00:06.917220
elapsed time: 0:18:42.708789
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 09:25:51.490706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.00
 ---- batch: 020 ----
mean loss: 98.91
 ---- batch: 030 ----
mean loss: 95.54
train mean loss: 98.35
epoch train time: 0:00:06.926248
elapsed time: 0:18:49.635858
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 09:25:58.417787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.54
 ---- batch: 020 ----
mean loss: 97.76
 ---- batch: 030 ----
mean loss: 98.04
train mean loss: 99.06
epoch train time: 0:00:06.846060
elapsed time: 0:18:56.482666
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 09:26:05.264591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.31
 ---- batch: 020 ----
mean loss: 96.65
 ---- batch: 030 ----
mean loss: 101.39
train mean loss: 98.78
epoch train time: 0:00:06.900651
elapsed time: 0:19:03.384098
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 09:26:12.166036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.00
 ---- batch: 020 ----
mean loss: 98.38
 ---- batch: 030 ----
mean loss: 98.23
train mean loss: 99.05
epoch train time: 0:00:06.936685
elapsed time: 0:19:10.321645
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 09:26:19.103557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.38
 ---- batch: 020 ----
mean loss: 97.76
 ---- batch: 030 ----
mean loss: 101.08
train mean loss: 99.37
epoch train time: 0:00:06.862362
elapsed time: 0:19:17.185077
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 09:26:25.967030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.45
 ---- batch: 020 ----
mean loss: 96.98
 ---- batch: 030 ----
mean loss: 95.98
train mean loss: 98.57
epoch train time: 0:00:06.933251
elapsed time: 0:19:24.119108
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 09:26:32.901057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.56
 ---- batch: 020 ----
mean loss: 97.94
 ---- batch: 030 ----
mean loss: 98.60
train mean loss: 98.09
epoch train time: 0:00:06.917609
elapsed time: 0:19:31.037565
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 09:26:39.819489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.71
 ---- batch: 020 ----
mean loss: 97.70
 ---- batch: 030 ----
mean loss: 95.29
train mean loss: 97.74
epoch train time: 0:00:06.843793
elapsed time: 0:19:37.882092
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 09:26:46.664008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.79
 ---- batch: 020 ----
mean loss: 99.51
 ---- batch: 030 ----
mean loss: 97.64
train mean loss: 100.28
epoch train time: 0:00:06.896490
elapsed time: 0:19:44.779592
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 09:26:53.561555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.89
 ---- batch: 020 ----
mean loss: 94.19
 ---- batch: 030 ----
mean loss: 97.91
train mean loss: 96.92
epoch train time: 0:00:06.826198
elapsed time: 0:19:51.606606
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 09:27:00.388528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.67
 ---- batch: 020 ----
mean loss: 99.65
 ---- batch: 030 ----
mean loss: 98.54
train mean loss: 98.35
epoch train time: 0:00:06.810155
elapsed time: 0:19:58.417666
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 09:27:07.199486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.83
 ---- batch: 020 ----
mean loss: 98.24
 ---- batch: 030 ----
mean loss: 98.71
train mean loss: 97.62
epoch train time: 0:00:06.819595
elapsed time: 0:20:05.237885
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 09:27:14.019810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.39
 ---- batch: 020 ----
mean loss: 97.43
 ---- batch: 030 ----
mean loss: 98.48
train mean loss: 97.43
epoch train time: 0:00:06.736683
elapsed time: 0:20:11.975340
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 09:27:20.757259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.75
 ---- batch: 020 ----
mean loss: 96.62
 ---- batch: 030 ----
mean loss: 96.87
train mean loss: 96.71
epoch train time: 0:00:06.791035
elapsed time: 0:20:18.767155
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 09:27:27.549071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.76
 ---- batch: 020 ----
mean loss: 95.21
 ---- batch: 030 ----
mean loss: 99.45
train mean loss: 97.13
epoch train time: 0:00:06.806598
elapsed time: 0:20:25.574503
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 09:27:34.356423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.31
 ---- batch: 020 ----
mean loss: 99.81
 ---- batch: 030 ----
mean loss: 92.38
train mean loss: 95.96
epoch train time: 0:00:06.862885
elapsed time: 0:20:32.438182
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 09:27:41.220104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.53
 ---- batch: 020 ----
mean loss: 93.82
 ---- batch: 030 ----
mean loss: 95.47
train mean loss: 95.34
epoch train time: 0:00:06.987414
elapsed time: 0:20:39.426450
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 09:27:48.208383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.98
 ---- batch: 020 ----
mean loss: 99.38
 ---- batch: 030 ----
mean loss: 94.82
train mean loss: 95.29
epoch train time: 0:00:06.981815
elapsed time: 0:20:46.409231
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 09:27:55.191165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.03
 ---- batch: 020 ----
mean loss: 95.32
 ---- batch: 030 ----
mean loss: 96.29
train mean loss: 95.01
epoch train time: 0:00:06.987146
elapsed time: 0:20:53.397191
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 09:28:02.179137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.85
 ---- batch: 020 ----
mean loss: 94.70
 ---- batch: 030 ----
mean loss: 95.51
train mean loss: 95.08
epoch train time: 0:00:07.011102
elapsed time: 0:21:00.409155
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 09:28:09.191101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.25
 ---- batch: 020 ----
mean loss: 95.53
 ---- batch: 030 ----
mean loss: 97.35
train mean loss: 96.40
epoch train time: 0:00:07.004478
elapsed time: 0:21:07.414497
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 09:28:16.196451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.24
 ---- batch: 020 ----
mean loss: 93.30
 ---- batch: 030 ----
mean loss: 95.64
train mean loss: 94.86
epoch train time: 0:00:06.929462
elapsed time: 0:21:14.344744
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 09:28:23.126662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.60
 ---- batch: 020 ----
mean loss: 98.21
 ---- batch: 030 ----
mean loss: 91.03
train mean loss: 95.38
epoch train time: 0:00:06.902963
elapsed time: 0:21:21.248464
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 09:28:30.030382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.78
 ---- batch: 020 ----
mean loss: 93.35
 ---- batch: 030 ----
mean loss: 93.09
train mean loss: 94.69
epoch train time: 0:00:06.932571
elapsed time: 0:21:28.181813
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 09:28:36.963734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.20
 ---- batch: 020 ----
mean loss: 95.37
 ---- batch: 030 ----
mean loss: 96.35
train mean loss: 94.06
epoch train time: 0:00:06.910073
elapsed time: 0:21:35.092777
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 09:28:43.874718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.09
 ---- batch: 020 ----
mean loss: 93.32
 ---- batch: 030 ----
mean loss: 99.81
train mean loss: 95.51
epoch train time: 0:00:06.827006
elapsed time: 0:21:41.920591
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 09:28:50.702543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.96
 ---- batch: 020 ----
mean loss: 94.60
 ---- batch: 030 ----
mean loss: 95.81
train mean loss: 94.53
epoch train time: 0:00:06.854437
elapsed time: 0:21:48.775821
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 09:28:57.557770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.32
 ---- batch: 020 ----
mean loss: 91.93
 ---- batch: 030 ----
mean loss: 92.27
train mean loss: 93.09
epoch train time: 0:00:06.846218
elapsed time: 0:21:55.622925
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 09:29:04.404859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.00
 ---- batch: 020 ----
mean loss: 94.19
 ---- batch: 030 ----
mean loss: 94.39
train mean loss: 94.47
epoch train time: 0:00:06.774560
elapsed time: 0:22:02.398353
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 09:29:11.180268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.51
 ---- batch: 020 ----
mean loss: 92.05
 ---- batch: 030 ----
mean loss: 94.04
train mean loss: 93.70
epoch train time: 0:00:06.812853
elapsed time: 0:22:09.212017
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 09:29:17.993975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.96
 ---- batch: 020 ----
mean loss: 95.76
 ---- batch: 030 ----
mean loss: 90.75
train mean loss: 93.50
epoch train time: 0:00:06.839502
elapsed time: 0:22:16.052329
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 09:29:24.834259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.42
 ---- batch: 020 ----
mean loss: 93.93
 ---- batch: 030 ----
mean loss: 92.53
train mean loss: 92.93
epoch train time: 0:00:06.756475
elapsed time: 0:22:22.809566
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 09:29:31.591485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.35
 ---- batch: 020 ----
mean loss: 96.80
 ---- batch: 030 ----
mean loss: 88.85
train mean loss: 93.68
epoch train time: 0:00:06.764291
elapsed time: 0:22:29.574734
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 09:29:38.356725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.71
 ---- batch: 020 ----
mean loss: 96.20
 ---- batch: 030 ----
mean loss: 90.04
train mean loss: 93.32
epoch train time: 0:00:06.720795
elapsed time: 0:22:36.296412
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 09:29:45.078321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.74
 ---- batch: 020 ----
mean loss: 94.92
 ---- batch: 030 ----
mean loss: 93.19
train mean loss: 93.48
epoch train time: 0:00:06.986017
elapsed time: 0:22:43.283312
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 09:29:52.065236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.88
 ---- batch: 020 ----
mean loss: 91.48
 ---- batch: 030 ----
mean loss: 94.89
train mean loss: 92.41
epoch train time: 0:00:06.866711
elapsed time: 0:22:50.150756
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 09:29:58.932677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.82
 ---- batch: 020 ----
mean loss: 91.24
 ---- batch: 030 ----
mean loss: 93.80
train mean loss: 92.60
epoch train time: 0:00:06.932173
elapsed time: 0:22:57.083702
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 09:30:05.865671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.10
 ---- batch: 020 ----
mean loss: 90.13
 ---- batch: 030 ----
mean loss: 94.08
train mean loss: 91.71
epoch train time: 0:00:06.942838
elapsed time: 0:23:04.027421
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 09:30:12.809366
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.85
 ---- batch: 020 ----
mean loss: 90.38
 ---- batch: 030 ----
mean loss: 92.74
train mean loss: 91.87
epoch train time: 0:00:06.955769
elapsed time: 0:23:10.984108
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 09:30:19.765924
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.93
 ---- batch: 020 ----
mean loss: 90.33
 ---- batch: 030 ----
mean loss: 91.28
train mean loss: 91.62
epoch train time: 0:00:06.828134
elapsed time: 0:23:17.812980
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 09:30:26.594912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.18
 ---- batch: 020 ----
mean loss: 91.56
 ---- batch: 030 ----
mean loss: 92.25
train mean loss: 91.28
epoch train time: 0:00:06.906784
elapsed time: 0:23:24.720537
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 09:30:33.502456
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.56
 ---- batch: 020 ----
mean loss: 88.80
 ---- batch: 030 ----
mean loss: 91.56
train mean loss: 91.24
epoch train time: 0:00:06.819134
elapsed time: 0:23:31.540458
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 09:30:40.322388
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.68
 ---- batch: 020 ----
mean loss: 91.39
 ---- batch: 030 ----
mean loss: 89.66
train mean loss: 91.35
epoch train time: 0:00:06.891016
elapsed time: 0:23:38.432320
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 09:30:47.214261
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.80
 ---- batch: 020 ----
mean loss: 93.20
 ---- batch: 030 ----
mean loss: 89.44
train mean loss: 91.33
epoch train time: 0:00:06.876581
elapsed time: 0:23:45.309710
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 09:30:54.091642
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.59
 ---- batch: 020 ----
mean loss: 90.01
 ---- batch: 030 ----
mean loss: 91.61
train mean loss: 91.25
epoch train time: 0:00:06.887101
elapsed time: 0:23:52.197604
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 09:31:00.979505
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.01
 ---- batch: 020 ----
mean loss: 87.08
 ---- batch: 030 ----
mean loss: 92.07
train mean loss: 90.82
epoch train time: 0:00:06.898690
elapsed time: 0:23:59.097048
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 09:31:07.878989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 94.24
 ---- batch: 020 ----
mean loss: 91.34
 ---- batch: 030 ----
mean loss: 89.49
train mean loss: 91.18
epoch train time: 0:00:06.911756
elapsed time: 0:24:06.009662
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 09:31:14.791621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.66
 ---- batch: 020 ----
mean loss: 89.50
 ---- batch: 030 ----
mean loss: 92.91
train mean loss: 91.10
epoch train time: 0:00:06.799554
elapsed time: 0:24:12.810030
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 09:31:21.591953
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.84
 ---- batch: 020 ----
mean loss: 90.36
 ---- batch: 030 ----
mean loss: 89.02
train mean loss: 90.82
epoch train time: 0:00:06.872370
elapsed time: 0:24:19.683237
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 09:31:28.465157
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.81
 ---- batch: 020 ----
mean loss: 93.00
 ---- batch: 030 ----
mean loss: 90.57
train mean loss: 90.47
epoch train time: 0:00:06.810440
elapsed time: 0:24:26.494471
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 09:31:35.276394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 93.42
 ---- batch: 020 ----
mean loss: 89.21
 ---- batch: 030 ----
mean loss: 92.44
train mean loss: 91.45
epoch train time: 0:00:06.832674
elapsed time: 0:24:33.327905
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 09:31:42.109863
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.00
 ---- batch: 020 ----
mean loss: 91.84
 ---- batch: 030 ----
mean loss: 88.86
train mean loss: 91.56
epoch train time: 0:00:06.847012
elapsed time: 0:24:40.175699
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 09:31:48.957628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.46
 ---- batch: 020 ----
mean loss: 87.24
 ---- batch: 030 ----
mean loss: 96.07
train mean loss: 91.05
epoch train time: 0:00:06.833016
elapsed time: 0:24:47.009464
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 09:31:55.791383
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.64
 ---- batch: 020 ----
mean loss: 93.24
 ---- batch: 030 ----
mean loss: 89.64
train mean loss: 91.12
epoch train time: 0:00:06.879145
elapsed time: 0:24:53.889336
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 09:32:02.671251
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.05
 ---- batch: 020 ----
mean loss: 90.14
 ---- batch: 030 ----
mean loss: 93.55
train mean loss: 90.40
epoch train time: 0:00:06.808511
elapsed time: 0:25:00.698660
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 09:32:09.480618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.15
 ---- batch: 020 ----
mean loss: 89.89
 ---- batch: 030 ----
mean loss: 91.34
train mean loss: 91.17
epoch train time: 0:00:06.840434
elapsed time: 0:25:07.539886
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 09:32:16.321809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.40
 ---- batch: 020 ----
mean loss: 93.35
 ---- batch: 030 ----
mean loss: 90.37
train mean loss: 90.63
epoch train time: 0:00:06.833898
elapsed time: 0:25:14.374537
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 09:32:23.156480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.85
 ---- batch: 020 ----
mean loss: 91.57
 ---- batch: 030 ----
mean loss: 90.88
train mean loss: 90.81
epoch train time: 0:00:06.900116
elapsed time: 0:25:21.275433
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 09:32:30.057392
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.96
 ---- batch: 020 ----
mean loss: 91.22
 ---- batch: 030 ----
mean loss: 91.72
train mean loss: 91.03
epoch train time: 0:00:06.767871
elapsed time: 0:25:28.044193
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 09:32:36.826129
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.64
 ---- batch: 020 ----
mean loss: 91.25
 ---- batch: 030 ----
mean loss: 87.80
train mean loss: 91.20
epoch train time: 0:00:06.820074
elapsed time: 0:25:34.865099
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 09:32:43.647037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.53
 ---- batch: 020 ----
mean loss: 90.89
 ---- batch: 030 ----
mean loss: 92.95
train mean loss: 91.40
epoch train time: 0:00:06.710864
elapsed time: 0:25:41.576704
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 09:32:50.358665
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 93.69
 ---- batch: 020 ----
mean loss: 89.94
 ---- batch: 030 ----
mean loss: 87.76
train mean loss: 90.26
epoch train time: 0:00:06.890387
elapsed time: 0:25:48.467901
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 09:32:57.249839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.95
 ---- batch: 020 ----
mean loss: 91.48
 ---- batch: 030 ----
mean loss: 91.54
train mean loss: 91.12
epoch train time: 0:00:06.801070
elapsed time: 0:25:55.269790
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 09:33:04.051712
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.18
 ---- batch: 020 ----
mean loss: 92.38
 ---- batch: 030 ----
mean loss: 90.68
train mean loss: 90.67
epoch train time: 0:00:06.789727
elapsed time: 0:26:02.060269
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 09:33:10.842185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.87
 ---- batch: 020 ----
mean loss: 95.09
 ---- batch: 030 ----
mean loss: 88.60
train mean loss: 90.80
epoch train time: 0:00:06.930531
elapsed time: 0:26:08.991596
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 09:33:17.773539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.04
 ---- batch: 020 ----
mean loss: 91.78
 ---- batch: 030 ----
mean loss: 91.77
train mean loss: 90.72
epoch train time: 0:00:06.844585
elapsed time: 0:26:15.837053
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 09:33:24.618997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.18
 ---- batch: 020 ----
mean loss: 91.12
 ---- batch: 030 ----
mean loss: 89.86
train mean loss: 91.23
epoch train time: 0:00:06.935405
elapsed time: 0:26:22.773281
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 09:33:31.555215
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.03
 ---- batch: 020 ----
mean loss: 91.95
 ---- batch: 030 ----
mean loss: 90.01
train mean loss: 90.94
epoch train time: 0:00:06.794884
elapsed time: 0:26:29.568908
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 09:33:38.350829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 93.50
 ---- batch: 020 ----
mean loss: 90.15
 ---- batch: 030 ----
mean loss: 88.79
train mean loss: 90.76
epoch train time: 0:00:06.863814
elapsed time: 0:26:36.433548
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 09:33:45.215486
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.84
 ---- batch: 020 ----
mean loss: 91.36
 ---- batch: 030 ----
mean loss: 90.34
train mean loss: 90.88
epoch train time: 0:00:06.862946
elapsed time: 0:26:43.297275
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 09:33:52.079202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.37
 ---- batch: 020 ----
mean loss: 91.06
 ---- batch: 030 ----
mean loss: 89.54
train mean loss: 91.23
epoch train time: 0:00:06.832601
elapsed time: 0:26:50.130755
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 09:33:58.912574
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 94.97
 ---- batch: 020 ----
mean loss: 89.61
 ---- batch: 030 ----
mean loss: 88.99
train mean loss: 91.02
epoch train time: 0:00:06.813686
elapsed time: 0:26:56.945093
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 09:34:05.727009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.47
 ---- batch: 020 ----
mean loss: 88.98
 ---- batch: 030 ----
mean loss: 89.83
train mean loss: 90.37
epoch train time: 0:00:06.855548
elapsed time: 0:27:03.801424
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 09:34:12.583335
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.94
 ---- batch: 020 ----
mean loss: 89.09
 ---- batch: 030 ----
mean loss: 90.52
train mean loss: 90.89
epoch train time: 0:00:06.751224
elapsed time: 0:27:10.553430
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 09:34:19.335379
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.53
 ---- batch: 020 ----
mean loss: 90.75
 ---- batch: 030 ----
mean loss: 90.30
train mean loss: 91.15
epoch train time: 0:00:06.819914
elapsed time: 0:27:17.374136
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 09:34:26.156073
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.32
 ---- batch: 020 ----
mean loss: 91.18
 ---- batch: 030 ----
mean loss: 90.90
train mean loss: 91.46
epoch train time: 0:00:06.873445
elapsed time: 0:27:24.248379
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 09:34:33.030308
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.09
 ---- batch: 020 ----
mean loss: 90.14
 ---- batch: 030 ----
mean loss: 88.22
train mean loss: 90.77
epoch train time: 0:00:06.859053
elapsed time: 0:27:31.108260
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 09:34:39.890189
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.57
 ---- batch: 020 ----
mean loss: 90.92
 ---- batch: 030 ----
mean loss: 93.58
train mean loss: 91.04
epoch train time: 0:00:06.894163
elapsed time: 0:27:38.003212
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 09:34:46.785171
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.51
 ---- batch: 020 ----
mean loss: 89.37
 ---- batch: 030 ----
mean loss: 92.59
train mean loss: 90.40
epoch train time: 0:00:06.878610
elapsed time: 0:27:44.882625
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 09:34:53.664548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.09
 ---- batch: 020 ----
mean loss: 91.31
 ---- batch: 030 ----
mean loss: 88.97
train mean loss: 90.78
epoch train time: 0:00:06.825223
elapsed time: 0:27:51.708625
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 09:35:00.490547
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.89
 ---- batch: 020 ----
mean loss: 90.47
 ---- batch: 030 ----
mean loss: 89.45
train mean loss: 90.73
epoch train time: 0:00:06.874838
elapsed time: 0:27:58.584296
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 09:35:07.366219
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.52
 ---- batch: 020 ----
mean loss: 89.07
 ---- batch: 030 ----
mean loss: 89.15
train mean loss: 90.61
epoch train time: 0:00:06.880046
elapsed time: 0:28:05.465091
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 09:35:14.247026
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.73
 ---- batch: 020 ----
mean loss: 88.60
 ---- batch: 030 ----
mean loss: 90.38
train mean loss: 90.21
epoch train time: 0:00:06.784997
elapsed time: 0:28:12.250850
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 09:35:21.032769
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.24
 ---- batch: 020 ----
mean loss: 91.39
 ---- batch: 030 ----
mean loss: 89.32
train mean loss: 90.40
epoch train time: 0:00:06.957376
elapsed time: 0:28:19.209024
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 09:35:27.990974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.14
 ---- batch: 020 ----
mean loss: 88.49
 ---- batch: 030 ----
mean loss: 88.37
train mean loss: 90.11
epoch train time: 0:00:06.848412
elapsed time: 0:28:26.058390
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 09:35:34.840311
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 93.30
 ---- batch: 020 ----
mean loss: 89.84
 ---- batch: 030 ----
mean loss: 88.37
train mean loss: 90.49
epoch train time: 0:00:06.882723
elapsed time: 0:28:32.941907
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 09:35:41.723834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 85.99
 ---- batch: 020 ----
mean loss: 91.34
 ---- batch: 030 ----
mean loss: 92.56
train mean loss: 89.98
epoch train time: 0:00:06.816336
elapsed time: 0:28:39.768358
checkpoint saved in file: log/CMAPSS/FD001/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_7/checkpoint.pth.tar
**** end time: 2019-09-27 09:35:48.550148 ****
