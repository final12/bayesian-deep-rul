Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_7', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 7406
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-20 23:00:54.185825 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 31, 14]             100
              Tanh-2           [-1, 10, 31, 14]               0
            Conv2d-3           [-1, 10, 30, 14]           1,000
              Tanh-4           [-1, 10, 30, 14]               0
            Conv2d-5           [-1, 10, 31, 14]           1,000
              Tanh-6           [-1, 10, 31, 14]               0
            Conv2d-7           [-1, 10, 30, 14]           1,000
              Tanh-8           [-1, 10, 30, 14]               0
            Conv2d-9            [-1, 1, 30, 14]              30
             Tanh-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
          Dropout-12                  [-1, 420]               0
           Linear-13                  [-1, 100]          42,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 45,230
Trainable params: 45,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:00:54.193245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4480.14
 ---- batch: 020 ----
mean loss: 2142.87
 ---- batch: 030 ----
mean loss: 482.40
 ---- batch: 040 ----
mean loss: 466.63
train mean loss: 1789.78
epoch train time: 0:00:15.610735
elapsed time: 0:00:15.620832
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:01:09.806697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.56
 ---- batch: 020 ----
mean loss: 349.90
 ---- batch: 030 ----
mean loss: 320.16
 ---- batch: 040 ----
mean loss: 311.18
train mean loss: 331.86
epoch train time: 0:00:01.810484
elapsed time: 0:00:17.431451
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:01:11.617348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.52
 ---- batch: 020 ----
mean loss: 308.42
 ---- batch: 030 ----
mean loss: 297.61
 ---- batch: 040 ----
mean loss: 296.52
train mean loss: 300.51
epoch train time: 0:00:01.691265
elapsed time: 0:00:19.122877
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:01:13.308751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.70
 ---- batch: 020 ----
mean loss: 271.46
 ---- batch: 030 ----
mean loss: 275.83
 ---- batch: 040 ----
mean loss: 267.39
train mean loss: 272.53
epoch train time: 0:00:01.682345
elapsed time: 0:00:20.805360
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:01:14.991247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.21
 ---- batch: 020 ----
mean loss: 257.52
 ---- batch: 030 ----
mean loss: 243.74
 ---- batch: 040 ----
mean loss: 235.86
train mean loss: 247.42
epoch train time: 0:00:01.657431
elapsed time: 0:00:22.462960
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:01:16.648853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.12
 ---- batch: 020 ----
mean loss: 223.82
 ---- batch: 030 ----
mean loss: 229.57
 ---- batch: 040 ----
mean loss: 228.02
train mean loss: 229.81
epoch train time: 0:00:01.760130
elapsed time: 0:00:24.223266
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:01:18.409156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.07
 ---- batch: 020 ----
mean loss: 227.87
 ---- batch: 030 ----
mean loss: 219.45
 ---- batch: 040 ----
mean loss: 221.40
train mean loss: 222.40
epoch train time: 0:00:01.665601
elapsed time: 0:00:25.889036
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:01:20.074910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.67
 ---- batch: 020 ----
mean loss: 219.13
 ---- batch: 030 ----
mean loss: 209.14
 ---- batch: 040 ----
mean loss: 207.11
train mean loss: 214.33
epoch train time: 0:00:01.768387
elapsed time: 0:00:27.657572
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:01:21.843456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.68
 ---- batch: 020 ----
mean loss: 211.58
 ---- batch: 030 ----
mean loss: 206.88
 ---- batch: 040 ----
mean loss: 216.61
train mean loss: 212.75
epoch train time: 0:00:01.673002
elapsed time: 0:00:29.330727
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:01:23.516602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.58
 ---- batch: 020 ----
mean loss: 206.00
 ---- batch: 030 ----
mean loss: 206.65
 ---- batch: 040 ----
mean loss: 214.76
train mean loss: 209.48
epoch train time: 0:00:01.770506
elapsed time: 0:00:31.101381
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:01:25.287256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.89
 ---- batch: 020 ----
mean loss: 211.36
 ---- batch: 030 ----
mean loss: 204.79
 ---- batch: 040 ----
mean loss: 207.97
train mean loss: 207.54
epoch train time: 0:00:01.666848
elapsed time: 0:00:32.768370
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:01:26.954246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.29
 ---- batch: 020 ----
mean loss: 205.50
 ---- batch: 030 ----
mean loss: 201.62
 ---- batch: 040 ----
mean loss: 204.32
train mean loss: 204.21
epoch train time: 0:00:01.783476
elapsed time: 0:00:34.551985
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:01:28.737861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.84
 ---- batch: 020 ----
mean loss: 207.63
 ---- batch: 030 ----
mean loss: 198.52
 ---- batch: 040 ----
mean loss: 195.52
train mean loss: 202.62
epoch train time: 0:00:01.666692
elapsed time: 0:00:36.218833
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:01:30.404710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.79
 ---- batch: 020 ----
mean loss: 199.06
 ---- batch: 030 ----
mean loss: 199.54
 ---- batch: 040 ----
mean loss: 198.91
train mean loss: 202.07
epoch train time: 0:00:01.777901
elapsed time: 0:00:37.996920
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:01:32.182795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.70
 ---- batch: 020 ----
mean loss: 209.52
 ---- batch: 030 ----
mean loss: 200.03
 ---- batch: 040 ----
mean loss: 200.61
train mean loss: 201.59
epoch train time: 0:00:01.673874
elapsed time: 0:00:39.670952
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:01:33.856850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.37
 ---- batch: 020 ----
mean loss: 199.16
 ---- batch: 030 ----
mean loss: 199.16
 ---- batch: 040 ----
mean loss: 202.30
train mean loss: 201.50
epoch train time: 0:00:01.767338
elapsed time: 0:00:41.438443
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:01:35.624331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.49
 ---- batch: 020 ----
mean loss: 203.75
 ---- batch: 030 ----
mean loss: 196.76
 ---- batch: 040 ----
mean loss: 202.42
train mean loss: 202.56
epoch train time: 0:00:01.680536
elapsed time: 0:00:43.119158
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:01:37.305035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.66
 ---- batch: 020 ----
mean loss: 206.35
 ---- batch: 030 ----
mean loss: 200.69
 ---- batch: 040 ----
mean loss: 202.98
train mean loss: 203.13
epoch train time: 0:00:01.669173
elapsed time: 0:00:44.788465
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:01:38.974349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.35
 ---- batch: 020 ----
mean loss: 204.92
 ---- batch: 030 ----
mean loss: 199.31
 ---- batch: 040 ----
mean loss: 190.35
train mean loss: 197.58
epoch train time: 0:00:01.772040
elapsed time: 0:00:46.560645
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:01:40.746516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.87
 ---- batch: 020 ----
mean loss: 195.22
 ---- batch: 030 ----
mean loss: 198.23
 ---- batch: 040 ----
mean loss: 205.76
train mean loss: 197.64
epoch train time: 0:00:01.670660
elapsed time: 0:00:48.231455
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:01:42.417333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.33
 ---- batch: 020 ----
mean loss: 198.91
 ---- batch: 030 ----
mean loss: 195.49
 ---- batch: 040 ----
mean loss: 196.34
train mean loss: 196.24
epoch train time: 0:00:01.782343
elapsed time: 0:00:50.013943
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:01:44.199811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.51
 ---- batch: 020 ----
mean loss: 205.06
 ---- batch: 030 ----
mean loss: 190.32
 ---- batch: 040 ----
mean loss: 199.10
train mean loss: 197.02
epoch train time: 0:00:01.673181
elapsed time: 0:00:51.687267
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:01:45.873142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.52
 ---- batch: 020 ----
mean loss: 196.99
 ---- batch: 030 ----
mean loss: 193.66
 ---- batch: 040 ----
mean loss: 193.40
train mean loss: 195.56
epoch train time: 0:00:01.772267
elapsed time: 0:00:53.459690
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:01:47.645566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.18
 ---- batch: 020 ----
mean loss: 187.26
 ---- batch: 030 ----
mean loss: 194.13
 ---- batch: 040 ----
mean loss: 194.58
train mean loss: 192.63
epoch train time: 0:00:01.686150
elapsed time: 0:00:55.145979
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:01:49.331853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.30
 ---- batch: 020 ----
mean loss: 201.94
 ---- batch: 030 ----
mean loss: 199.20
 ---- batch: 040 ----
mean loss: 195.77
train mean loss: 195.72
epoch train time: 0:00:01.774276
elapsed time: 0:00:56.920384
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:01:51.106255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.01
 ---- batch: 020 ----
mean loss: 193.88
 ---- batch: 030 ----
mean loss: 189.32
 ---- batch: 040 ----
mean loss: 198.42
train mean loss: 194.47
epoch train time: 0:00:01.675215
elapsed time: 0:00:58.595765
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:01:52.781648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.12
 ---- batch: 020 ----
mean loss: 187.91
 ---- batch: 030 ----
mean loss: 195.86
 ---- batch: 040 ----
mean loss: 185.73
train mean loss: 190.25
epoch train time: 0:00:01.771913
elapsed time: 0:01:00.367817
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:01:54.553689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.87
 ---- batch: 020 ----
mean loss: 196.05
 ---- batch: 030 ----
mean loss: 194.89
 ---- batch: 040 ----
mean loss: 188.85
train mean loss: 191.75
epoch train time: 0:00:01.679249
elapsed time: 0:01:02.047214
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:01:56.233109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.28
 ---- batch: 020 ----
mean loss: 191.01
 ---- batch: 030 ----
mean loss: 200.09
 ---- batch: 040 ----
mean loss: 193.60
train mean loss: 194.46
epoch train time: 0:00:01.664594
elapsed time: 0:01:03.712036
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:01:57.897914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.89
 ---- batch: 020 ----
mean loss: 196.93
 ---- batch: 030 ----
mean loss: 186.12
 ---- batch: 040 ----
mean loss: 184.01
train mean loss: 189.46
epoch train time: 0:00:01.784090
elapsed time: 0:01:05.496286
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:01:59.682168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.05
 ---- batch: 020 ----
mean loss: 187.24
 ---- batch: 030 ----
mean loss: 192.51
 ---- batch: 040 ----
mean loss: 190.05
train mean loss: 191.18
epoch train time: 0:00:01.677801
elapsed time: 0:01:07.174272
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:02:01.360151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.89
 ---- batch: 020 ----
mean loss: 189.55
 ---- batch: 030 ----
mean loss: 190.25
 ---- batch: 040 ----
mean loss: 187.42
train mean loss: 190.39
epoch train time: 0:00:01.782533
elapsed time: 0:01:08.956942
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:02:03.142816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.43
 ---- batch: 020 ----
mean loss: 182.90
 ---- batch: 030 ----
mean loss: 184.60
 ---- batch: 040 ----
mean loss: 184.52
train mean loss: 187.68
epoch train time: 0:00:01.679134
elapsed time: 0:01:10.636267
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:02:04.822142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.10
 ---- batch: 020 ----
mean loss: 186.29
 ---- batch: 030 ----
mean loss: 180.50
 ---- batch: 040 ----
mean loss: 187.89
train mean loss: 186.36
epoch train time: 0:00:01.722742
elapsed time: 0:01:12.359172
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:02:06.545048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.48
 ---- batch: 020 ----
mean loss: 190.00
 ---- batch: 030 ----
mean loss: 181.48
 ---- batch: 040 ----
mean loss: 181.59
train mean loss: 183.76
epoch train time: 0:00:01.721530
elapsed time: 0:01:14.080875
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:02:08.266752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.97
 ---- batch: 020 ----
mean loss: 180.13
 ---- batch: 030 ----
mean loss: 176.66
 ---- batch: 040 ----
mean loss: 187.63
train mean loss: 184.37
epoch train time: 0:00:01.669699
elapsed time: 0:01:15.750721
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:02:09.936598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.36
 ---- batch: 020 ----
mean loss: 182.89
 ---- batch: 030 ----
mean loss: 188.65
 ---- batch: 040 ----
mean loss: 184.65
train mean loss: 183.45
epoch train time: 0:00:01.772201
elapsed time: 0:01:17.523082
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:02:11.708950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.64
 ---- batch: 020 ----
mean loss: 182.58
 ---- batch: 030 ----
mean loss: 180.90
 ---- batch: 040 ----
mean loss: 180.32
train mean loss: 183.51
epoch train time: 0:00:01.671122
elapsed time: 0:01:19.194364
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:02:13.380237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.15
 ---- batch: 020 ----
mean loss: 180.08
 ---- batch: 030 ----
mean loss: 181.47
 ---- batch: 040 ----
mean loss: 186.46
train mean loss: 182.64
epoch train time: 0:00:01.772003
elapsed time: 0:01:20.966499
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:02:15.152404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.75
 ---- batch: 020 ----
mean loss: 180.50
 ---- batch: 030 ----
mean loss: 181.47
 ---- batch: 040 ----
mean loss: 178.85
train mean loss: 180.95
epoch train time: 0:00:01.675229
elapsed time: 0:01:22.641902
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:02:16.827791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.38
 ---- batch: 020 ----
mean loss: 179.09
 ---- batch: 030 ----
mean loss: 183.95
 ---- batch: 040 ----
mean loss: 177.28
train mean loss: 179.34
epoch train time: 0:00:01.666417
elapsed time: 0:01:24.308469
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:02:18.494373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.64
 ---- batch: 020 ----
mean loss: 176.67
 ---- batch: 030 ----
mean loss: 184.19
 ---- batch: 040 ----
mean loss: 175.47
train mean loss: 178.90
epoch train time: 0:00:01.788943
elapsed time: 0:01:26.097567
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:02:20.283435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.93
 ---- batch: 020 ----
mean loss: 176.74
 ---- batch: 030 ----
mean loss: 183.25
 ---- batch: 040 ----
mean loss: 180.86
train mean loss: 181.55
epoch train time: 0:00:01.672805
elapsed time: 0:01:27.770519
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:02:21.956414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.30
 ---- batch: 020 ----
mean loss: 178.10
 ---- batch: 030 ----
mean loss: 174.38
 ---- batch: 040 ----
mean loss: 174.68
train mean loss: 180.11
epoch train time: 0:00:01.779130
elapsed time: 0:01:29.549849
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:02:23.735746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.65
 ---- batch: 020 ----
mean loss: 182.60
 ---- batch: 030 ----
mean loss: 181.82
 ---- batch: 040 ----
mean loss: 181.13
train mean loss: 182.88
epoch train time: 0:00:01.678808
elapsed time: 0:01:31.228853
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:02:25.414724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.63
 ---- batch: 020 ----
mean loss: 175.22
 ---- batch: 030 ----
mean loss: 177.46
 ---- batch: 040 ----
mean loss: 182.02
train mean loss: 178.43
epoch train time: 0:00:01.777599
elapsed time: 0:01:33.006600
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:02:27.192474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.37
 ---- batch: 020 ----
mean loss: 172.98
 ---- batch: 030 ----
mean loss: 178.57
 ---- batch: 040 ----
mean loss: 176.91
train mean loss: 177.10
epoch train time: 0:00:01.692292
elapsed time: 0:01:34.699057
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:02:28.884933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.72
 ---- batch: 020 ----
mean loss: 178.81
 ---- batch: 030 ----
mean loss: 184.26
 ---- batch: 040 ----
mean loss: 175.65
train mean loss: 180.41
epoch train time: 0:00:01.671587
elapsed time: 0:01:36.370784
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:02:30.556656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.21
 ---- batch: 020 ----
mean loss: 183.19
 ---- batch: 030 ----
mean loss: 178.81
 ---- batch: 040 ----
mean loss: 178.57
train mean loss: 180.02
epoch train time: 0:00:01.793954
elapsed time: 0:01:38.164905
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:02:32.350820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.37
 ---- batch: 020 ----
mean loss: 177.47
 ---- batch: 030 ----
mean loss: 172.96
 ---- batch: 040 ----
mean loss: 178.80
train mean loss: 178.37
epoch train time: 0:00:01.676153
elapsed time: 0:01:39.841251
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:02:34.027146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.46
 ---- batch: 020 ----
mean loss: 176.38
 ---- batch: 030 ----
mean loss: 173.47
 ---- batch: 040 ----
mean loss: 179.26
train mean loss: 176.51
epoch train time: 0:00:01.786621
elapsed time: 0:01:41.628050
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:02:35.813942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.69
 ---- batch: 020 ----
mean loss: 168.84
 ---- batch: 030 ----
mean loss: 179.06
 ---- batch: 040 ----
mean loss: 166.97
train mean loss: 173.01
epoch train time: 0:00:01.688540
elapsed time: 0:01:43.316764
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:02:37.502654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.04
 ---- batch: 020 ----
mean loss: 172.57
 ---- batch: 030 ----
mean loss: 176.79
 ---- batch: 040 ----
mean loss: 173.33
train mean loss: 175.89
epoch train time: 0:00:01.673845
elapsed time: 0:01:44.990769
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:02:39.176643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.47
 ---- batch: 020 ----
mean loss: 170.86
 ---- batch: 030 ----
mean loss: 173.32
 ---- batch: 040 ----
mean loss: 175.80
train mean loss: 173.16
epoch train time: 0:00:01.790481
elapsed time: 0:01:46.781403
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:02:40.967281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.86
 ---- batch: 020 ----
mean loss: 172.16
 ---- batch: 030 ----
mean loss: 177.23
 ---- batch: 040 ----
mean loss: 171.78
train mean loss: 174.05
epoch train time: 0:00:01.677445
elapsed time: 0:01:48.459003
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:02:42.644876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.95
 ---- batch: 020 ----
mean loss: 181.62
 ---- batch: 030 ----
mean loss: 177.46
 ---- batch: 040 ----
mean loss: 170.69
train mean loss: 179.94
epoch train time: 0:00:01.727366
elapsed time: 0:01:50.186504
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:02:44.372377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.77
 ---- batch: 020 ----
mean loss: 170.86
 ---- batch: 030 ----
mean loss: 169.91
 ---- batch: 040 ----
mean loss: 173.69
train mean loss: 173.07
epoch train time: 0:00:01.683357
elapsed time: 0:01:51.870010
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:02:46.055884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.67
 ---- batch: 020 ----
mean loss: 171.29
 ---- batch: 030 ----
mean loss: 176.34
 ---- batch: 040 ----
mean loss: 177.73
train mean loss: 173.82
epoch train time: 0:00:01.775313
elapsed time: 0:01:53.645467
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:02:47.831342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.12
 ---- batch: 020 ----
mean loss: 181.61
 ---- batch: 030 ----
mean loss: 177.66
 ---- batch: 040 ----
mean loss: 176.92
train mean loss: 176.67
epoch train time: 0:00:01.684070
elapsed time: 0:01:55.329698
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:02:49.515573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.88
 ---- batch: 020 ----
mean loss: 176.25
 ---- batch: 030 ----
mean loss: 172.99
 ---- batch: 040 ----
mean loss: 170.02
train mean loss: 172.46
epoch train time: 0:00:01.670945
elapsed time: 0:01:57.000791
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:02:51.186664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.74
 ---- batch: 020 ----
mean loss: 169.21
 ---- batch: 030 ----
mean loss: 178.66
 ---- batch: 040 ----
mean loss: 178.03
train mean loss: 175.32
epoch train time: 0:00:01.795880
elapsed time: 0:01:58.796804
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:02:52.982675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.30
 ---- batch: 020 ----
mean loss: 175.27
 ---- batch: 030 ----
mean loss: 167.78
 ---- batch: 040 ----
mean loss: 177.23
train mean loss: 173.52
epoch train time: 0:00:01.683946
elapsed time: 0:02:00.480883
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:02:54.666755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.63
 ---- batch: 020 ----
mean loss: 171.53
 ---- batch: 030 ----
mean loss: 169.78
 ---- batch: 040 ----
mean loss: 179.50
train mean loss: 172.53
epoch train time: 0:00:01.779906
elapsed time: 0:02:02.260963
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:02:56.446868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.39
 ---- batch: 020 ----
mean loss: 168.59
 ---- batch: 030 ----
mean loss: 176.68
 ---- batch: 040 ----
mean loss: 165.25
train mean loss: 170.64
epoch train time: 0:00:01.684250
elapsed time: 0:02:03.945394
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:02:58.131285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.40
 ---- batch: 020 ----
mean loss: 172.99
 ---- batch: 030 ----
mean loss: 173.65
 ---- batch: 040 ----
mean loss: 166.23
train mean loss: 171.58
epoch train time: 0:00:01.670794
elapsed time: 0:02:05.616357
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:02:59.802244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.70
 ---- batch: 020 ----
mean loss: 175.26
 ---- batch: 030 ----
mean loss: 168.59
 ---- batch: 040 ----
mean loss: 173.78
train mean loss: 172.52
epoch train time: 0:00:01.793747
elapsed time: 0:02:07.410256
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:03:01.596131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.21
 ---- batch: 020 ----
mean loss: 163.36
 ---- batch: 030 ----
mean loss: 169.58
 ---- batch: 040 ----
mean loss: 176.38
train mean loss: 170.06
epoch train time: 0:00:01.681536
elapsed time: 0:02:09.091934
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:03:03.277807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.03
 ---- batch: 020 ----
mean loss: 171.40
 ---- batch: 030 ----
mean loss: 174.71
 ---- batch: 040 ----
mean loss: 167.93
train mean loss: 170.39
epoch train time: 0:00:01.774595
elapsed time: 0:02:10.866685
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:03:05.052558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.58
 ---- batch: 020 ----
mean loss: 164.65
 ---- batch: 030 ----
mean loss: 172.55
 ---- batch: 040 ----
mean loss: 176.56
train mean loss: 172.44
epoch train time: 0:00:01.677442
elapsed time: 0:02:12.544272
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:03:06.730147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.56
 ---- batch: 020 ----
mean loss: 169.91
 ---- batch: 030 ----
mean loss: 174.98
 ---- batch: 040 ----
mean loss: 176.07
train mean loss: 174.25
epoch train time: 0:00:01.772854
elapsed time: 0:02:14.317268
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:03:08.503154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.58
 ---- batch: 020 ----
mean loss: 165.84
 ---- batch: 030 ----
mean loss: 173.62
 ---- batch: 040 ----
mean loss: 172.30
train mean loss: 171.86
epoch train time: 0:00:01.682617
elapsed time: 0:02:16.000048
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:03:10.185927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.14
 ---- batch: 020 ----
mean loss: 180.59
 ---- batch: 030 ----
mean loss: 176.39
 ---- batch: 040 ----
mean loss: 167.79
train mean loss: 173.13
epoch train time: 0:00:01.671479
elapsed time: 0:02:17.671695
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:03:11.857568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.19
 ---- batch: 020 ----
mean loss: 175.85
 ---- batch: 030 ----
mean loss: 171.78
 ---- batch: 040 ----
mean loss: 167.64
train mean loss: 170.22
epoch train time: 0:00:01.786712
elapsed time: 0:02:19.458537
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:03:13.644427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.61
 ---- batch: 020 ----
mean loss: 166.10
 ---- batch: 030 ----
mean loss: 174.43
 ---- batch: 040 ----
mean loss: 177.17
train mean loss: 172.60
epoch train time: 0:00:01.683199
elapsed time: 0:02:21.141890
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:03:15.327774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.41
 ---- batch: 020 ----
mean loss: 162.96
 ---- batch: 030 ----
mean loss: 164.93
 ---- batch: 040 ----
mean loss: 174.53
train mean loss: 169.38
epoch train time: 0:00:01.784153
elapsed time: 0:02:22.926191
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:03:17.112083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.38
 ---- batch: 020 ----
mean loss: 172.21
 ---- batch: 030 ----
mean loss: 170.21
 ---- batch: 040 ----
mean loss: 168.38
train mean loss: 169.58
epoch train time: 0:00:01.692976
elapsed time: 0:02:24.619335
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:03:18.805211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.35
 ---- batch: 020 ----
mean loss: 167.24
 ---- batch: 030 ----
mean loss: 165.86
 ---- batch: 040 ----
mean loss: 170.88
train mean loss: 168.54
epoch train time: 0:00:01.763279
elapsed time: 0:02:26.382801
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:03:20.568675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.28
 ---- batch: 020 ----
mean loss: 179.29
 ---- batch: 030 ----
mean loss: 172.70
 ---- batch: 040 ----
mean loss: 170.33
train mean loss: 173.86
epoch train time: 0:00:01.687522
elapsed time: 0:02:28.070478
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:03:22.256406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.93
 ---- batch: 020 ----
mean loss: 172.61
 ---- batch: 030 ----
mean loss: 167.90
 ---- batch: 040 ----
mean loss: 169.43
train mean loss: 173.33
epoch train time: 0:00:01.676766
elapsed time: 0:02:29.747449
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:03:23.933334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.44
 ---- batch: 020 ----
mean loss: 163.26
 ---- batch: 030 ----
mean loss: 175.98
 ---- batch: 040 ----
mean loss: 171.32
train mean loss: 170.31
epoch train time: 0:00:01.788839
elapsed time: 0:02:31.536450
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:03:25.722337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.72
 ---- batch: 020 ----
mean loss: 171.43
 ---- batch: 030 ----
mean loss: 175.97
 ---- batch: 040 ----
mean loss: 166.74
train mean loss: 171.86
epoch train time: 0:00:01.675075
elapsed time: 0:02:33.211677
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:03:27.397551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.11
 ---- batch: 020 ----
mean loss: 168.18
 ---- batch: 030 ----
mean loss: 167.30
 ---- batch: 040 ----
mean loss: 162.18
train mean loss: 167.27
epoch train time: 0:00:01.788839
elapsed time: 0:02:35.000647
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:03:29.186526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.34
 ---- batch: 020 ----
mean loss: 173.72
 ---- batch: 030 ----
mean loss: 169.93
 ---- batch: 040 ----
mean loss: 169.71
train mean loss: 169.22
epoch train time: 0:00:01.675945
elapsed time: 0:02:36.676738
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:03:30.862614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.13
 ---- batch: 020 ----
mean loss: 168.21
 ---- batch: 030 ----
mean loss: 166.36
 ---- batch: 040 ----
mean loss: 169.03
train mean loss: 166.09
epoch train time: 0:00:01.669292
elapsed time: 0:02:38.346201
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:03:32.532074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.13
 ---- batch: 020 ----
mean loss: 170.12
 ---- batch: 030 ----
mean loss: 175.00
 ---- batch: 040 ----
mean loss: 163.87
train mean loss: 170.85
epoch train time: 0:00:01.716464
elapsed time: 0:02:40.062826
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:03:34.248702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.44
 ---- batch: 020 ----
mean loss: 171.25
 ---- batch: 030 ----
mean loss: 174.72
 ---- batch: 040 ----
mean loss: 166.59
train mean loss: 169.57
epoch train time: 0:00:01.677863
elapsed time: 0:02:41.740832
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:03:35.926708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.46
 ---- batch: 020 ----
mean loss: 170.73
 ---- batch: 030 ----
mean loss: 163.73
 ---- batch: 040 ----
mean loss: 168.18
train mean loss: 168.13
epoch train time: 0:00:01.769431
elapsed time: 0:02:43.510397
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:03:37.696281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.77
 ---- batch: 020 ----
mean loss: 167.04
 ---- batch: 030 ----
mean loss: 172.96
 ---- batch: 040 ----
mean loss: 168.08
train mean loss: 169.39
epoch train time: 0:00:01.672308
elapsed time: 0:02:45.182878
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:03:39.368785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.09
 ---- batch: 020 ----
mean loss: 179.82
 ---- batch: 030 ----
mean loss: 162.90
 ---- batch: 040 ----
mean loss: 167.84
train mean loss: 171.25
epoch train time: 0:00:01.779975
elapsed time: 0:02:46.963057
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:03:41.148933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.36
 ---- batch: 020 ----
mean loss: 163.22
 ---- batch: 030 ----
mean loss: 175.09
 ---- batch: 040 ----
mean loss: 171.91
train mean loss: 169.89
epoch train time: 0:00:01.685434
elapsed time: 0:02:48.648648
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:03:42.834542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.30
 ---- batch: 020 ----
mean loss: 171.64
 ---- batch: 030 ----
mean loss: 161.13
 ---- batch: 040 ----
mean loss: 170.02
train mean loss: 167.64
epoch train time: 0:00:01.669412
elapsed time: 0:02:50.318230
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:03:44.504105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.44
 ---- batch: 020 ----
mean loss: 172.55
 ---- batch: 030 ----
mean loss: 164.53
 ---- batch: 040 ----
mean loss: 167.22
train mean loss: 168.70
epoch train time: 0:00:01.792478
elapsed time: 0:02:52.110844
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:03:46.296717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.47
 ---- batch: 020 ----
mean loss: 166.54
 ---- batch: 030 ----
mean loss: 165.32
 ---- batch: 040 ----
mean loss: 169.47
train mean loss: 166.10
epoch train time: 0:00:01.676104
elapsed time: 0:02:53.787103
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:03:47.972981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.56
 ---- batch: 020 ----
mean loss: 161.21
 ---- batch: 030 ----
mean loss: 164.41
 ---- batch: 040 ----
mean loss: 161.58
train mean loss: 164.76
epoch train time: 0:00:01.770334
elapsed time: 0:02:55.557583
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:03:49.743458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.82
 ---- batch: 020 ----
mean loss: 170.23
 ---- batch: 030 ----
mean loss: 163.01
 ---- batch: 040 ----
mean loss: 165.82
train mean loss: 164.63
epoch train time: 0:00:01.685851
elapsed time: 0:02:57.243583
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:03:51.429459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.65
 ---- batch: 020 ----
mean loss: 169.35
 ---- batch: 030 ----
mean loss: 157.97
 ---- batch: 040 ----
mean loss: 165.52
train mean loss: 165.33
epoch train time: 0:00:01.704411
elapsed time: 0:02:58.948131
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:03:53.134011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.14
 ---- batch: 020 ----
mean loss: 167.56
 ---- batch: 030 ----
mean loss: 155.83
 ---- batch: 040 ----
mean loss: 167.98
train mean loss: 166.74
epoch train time: 0:00:01.771214
elapsed time: 0:03:00.719494
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:03:54.905390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.13
 ---- batch: 020 ----
mean loss: 163.01
 ---- batch: 030 ----
mean loss: 168.91
 ---- batch: 040 ----
mean loss: 164.56
train mean loss: 165.49
epoch train time: 0:00:01.674009
elapsed time: 0:03:02.393670
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:03:56.579547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.81
 ---- batch: 020 ----
mean loss: 165.53
 ---- batch: 030 ----
mean loss: 167.34
 ---- batch: 040 ----
mean loss: 170.09
train mean loss: 168.86
epoch train time: 0:00:01.780634
elapsed time: 0:03:04.174461
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:03:58.360349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.72
 ---- batch: 020 ----
mean loss: 160.81
 ---- batch: 030 ----
mean loss: 162.80
 ---- batch: 040 ----
mean loss: 167.39
train mean loss: 164.76
epoch train time: 0:00:01.684621
elapsed time: 0:03:05.859252
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:04:00.045126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.64
 ---- batch: 020 ----
mean loss: 165.69
 ---- batch: 030 ----
mean loss: 167.22
 ---- batch: 040 ----
mean loss: 164.39
train mean loss: 163.91
epoch train time: 0:00:01.769731
elapsed time: 0:03:07.629131
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:04:01.815008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.31
 ---- batch: 020 ----
mean loss: 167.51
 ---- batch: 030 ----
mean loss: 164.78
 ---- batch: 040 ----
mean loss: 170.26
train mean loss: 168.14
epoch train time: 0:00:01.681721
elapsed time: 0:03:09.311017
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:04:03.496910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.78
 ---- batch: 020 ----
mean loss: 164.72
 ---- batch: 030 ----
mean loss: 168.75
 ---- batch: 040 ----
mean loss: 172.60
train mean loss: 168.24
epoch train time: 0:00:01.669389
elapsed time: 0:03:10.980558
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:04:05.166430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.35
 ---- batch: 020 ----
mean loss: 164.36
 ---- batch: 030 ----
mean loss: 165.04
 ---- batch: 040 ----
mean loss: 164.26
train mean loss: 166.77
epoch train time: 0:00:01.785138
elapsed time: 0:03:12.765836
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:04:06.951711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.16
 ---- batch: 020 ----
mean loss: 170.28
 ---- batch: 030 ----
mean loss: 160.48
 ---- batch: 040 ----
mean loss: 168.93
train mean loss: 164.78
epoch train time: 0:00:01.677579
elapsed time: 0:03:14.443562
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:04:08.629437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.39
 ---- batch: 020 ----
mean loss: 163.93
 ---- batch: 030 ----
mean loss: 167.75
 ---- batch: 040 ----
mean loss: 166.19
train mean loss: 166.06
epoch train time: 0:00:01.789530
elapsed time: 0:03:16.233230
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:04:10.419110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.28
 ---- batch: 020 ----
mean loss: 164.72
 ---- batch: 030 ----
mean loss: 164.29
 ---- batch: 040 ----
mean loss: 167.68
train mean loss: 165.74
epoch train time: 0:00:01.678214
elapsed time: 0:03:17.911637
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:04:12.097504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.72
 ---- batch: 020 ----
mean loss: 166.83
 ---- batch: 030 ----
mean loss: 167.62
 ---- batch: 040 ----
mean loss: 167.07
train mean loss: 168.01
epoch train time: 0:00:01.668739
elapsed time: 0:03:19.580520
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:04:13.766395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.90
 ---- batch: 020 ----
mean loss: 165.71
 ---- batch: 030 ----
mean loss: 160.50
 ---- batch: 040 ----
mean loss: 173.49
train mean loss: 167.25
epoch train time: 0:00:01.719429
elapsed time: 0:03:21.300103
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:04:15.485991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.53
 ---- batch: 020 ----
mean loss: 169.08
 ---- batch: 030 ----
mean loss: 167.55
 ---- batch: 040 ----
mean loss: 158.04
train mean loss: 167.38
epoch train time: 0:00:01.669928
elapsed time: 0:03:22.970188
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:04:17.156066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.08
 ---- batch: 020 ----
mean loss: 161.74
 ---- batch: 030 ----
mean loss: 168.82
 ---- batch: 040 ----
mean loss: 162.50
train mean loss: 165.62
epoch train time: 0:00:01.794620
elapsed time: 0:03:24.764958
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:04:18.950868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.74
 ---- batch: 020 ----
mean loss: 169.69
 ---- batch: 030 ----
mean loss: 159.66
 ---- batch: 040 ----
mean loss: 164.83
train mean loss: 164.70
epoch train time: 0:00:01.677315
elapsed time: 0:03:26.442465
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:04:20.628341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.01
 ---- batch: 020 ----
mean loss: 163.82
 ---- batch: 030 ----
mean loss: 165.37
 ---- batch: 040 ----
mean loss: 157.77
train mean loss: 163.61
epoch train time: 0:00:01.787880
elapsed time: 0:03:28.230507
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:04:22.416381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.75
 ---- batch: 020 ----
mean loss: 165.26
 ---- batch: 030 ----
mean loss: 166.22
 ---- batch: 040 ----
mean loss: 162.48
train mean loss: 164.93
epoch train time: 0:00:01.674170
elapsed time: 0:03:29.904836
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:04:24.090709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.78
 ---- batch: 020 ----
mean loss: 165.49
 ---- batch: 030 ----
mean loss: 161.26
 ---- batch: 040 ----
mean loss: 163.42
train mean loss: 163.47
epoch train time: 0:00:01.666906
elapsed time: 0:03:31.571870
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:04:25.757741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.18
 ---- batch: 020 ----
mean loss: 163.98
 ---- batch: 030 ----
mean loss: 163.67
 ---- batch: 040 ----
mean loss: 159.27
train mean loss: 162.82
epoch train time: 0:00:01.792686
elapsed time: 0:03:33.364689
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:04:27.550563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.05
 ---- batch: 020 ----
mean loss: 161.50
 ---- batch: 030 ----
mean loss: 159.79
 ---- batch: 040 ----
mean loss: 163.52
train mean loss: 162.84
epoch train time: 0:00:01.677389
elapsed time: 0:03:35.042217
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:04:29.228091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.63
 ---- batch: 020 ----
mean loss: 163.84
 ---- batch: 030 ----
mean loss: 163.52
 ---- batch: 040 ----
mean loss: 161.23
train mean loss: 162.62
epoch train time: 0:00:01.715484
elapsed time: 0:03:36.757840
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:04:30.943807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.58
 ---- batch: 020 ----
mean loss: 161.88
 ---- batch: 030 ----
mean loss: 163.08
 ---- batch: 040 ----
mean loss: 163.01
train mean loss: 162.53
epoch train time: 0:00:01.684149
elapsed time: 0:03:38.442231
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:04:32.628106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.27
 ---- batch: 020 ----
mean loss: 166.51
 ---- batch: 030 ----
mean loss: 170.22
 ---- batch: 040 ----
mean loss: 163.29
train mean loss: 166.41
epoch train time: 0:00:01.769643
elapsed time: 0:03:40.212011
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:04:34.397882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.30
 ---- batch: 020 ----
mean loss: 159.26
 ---- batch: 030 ----
mean loss: 161.30
 ---- batch: 040 ----
mean loss: 158.13
train mean loss: 160.43
epoch train time: 0:00:01.691303
elapsed time: 0:03:41.903454
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:04:36.089329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.71
 ---- batch: 020 ----
mean loss: 165.63
 ---- batch: 030 ----
mean loss: 158.28
 ---- batch: 040 ----
mean loss: 159.42
train mean loss: 159.90
epoch train time: 0:00:01.671075
elapsed time: 0:03:43.574686
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:04:37.760562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.09
 ---- batch: 020 ----
mean loss: 157.60
 ---- batch: 030 ----
mean loss: 160.42
 ---- batch: 040 ----
mean loss: 161.43
train mean loss: 159.24
epoch train time: 0:00:01.710737
elapsed time: 0:03:45.285559
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:04:39.471450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.08
 ---- batch: 020 ----
mean loss: 161.83
 ---- batch: 030 ----
mean loss: 162.84
 ---- batch: 040 ----
mean loss: 159.28
train mean loss: 161.34
epoch train time: 0:00:01.680912
elapsed time: 0:03:46.966635
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:04:41.152509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.18
 ---- batch: 020 ----
mean loss: 156.91
 ---- batch: 030 ----
mean loss: 157.06
 ---- batch: 040 ----
mean loss: 153.08
train mean loss: 156.72
epoch train time: 0:00:01.779201
elapsed time: 0:03:48.745976
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:04:42.931854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.81
 ---- batch: 020 ----
mean loss: 156.85
 ---- batch: 030 ----
mean loss: 163.58
 ---- batch: 040 ----
mean loss: 157.97
train mean loss: 159.22
epoch train time: 0:00:01.678400
elapsed time: 0:03:50.424530
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:04:44.610408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.53
 ---- batch: 020 ----
mean loss: 157.07
 ---- batch: 030 ----
mean loss: 165.13
 ---- batch: 040 ----
mean loss: 153.70
train mean loss: 157.62
epoch train time: 0:00:01.671788
elapsed time: 0:03:52.096489
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:04:46.282351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.40
 ---- batch: 020 ----
mean loss: 153.65
 ---- batch: 030 ----
mean loss: 157.28
 ---- batch: 040 ----
mean loss: 159.95
train mean loss: 158.32
epoch train time: 0:00:01.781973
elapsed time: 0:03:53.878624
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:04:48.064503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.44
 ---- batch: 020 ----
mean loss: 163.11
 ---- batch: 030 ----
mean loss: 160.26
 ---- batch: 040 ----
mean loss: 154.42
train mean loss: 156.99
epoch train time: 0:00:01.670851
elapsed time: 0:03:55.549619
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:04:49.735492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.62
 ---- batch: 020 ----
mean loss: 158.98
 ---- batch: 030 ----
mean loss: 155.45
 ---- batch: 040 ----
mean loss: 157.09
train mean loss: 156.10
epoch train time: 0:00:01.709699
elapsed time: 0:03:57.259466
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:04:51.445337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.51
 ---- batch: 020 ----
mean loss: 154.48
 ---- batch: 030 ----
mean loss: 154.18
 ---- batch: 040 ----
mean loss: 151.76
train mean loss: 154.29
epoch train time: 0:00:01.682953
elapsed time: 0:03:58.942551
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:04:53.128443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.80
 ---- batch: 020 ----
mean loss: 150.96
 ---- batch: 030 ----
mean loss: 151.70
 ---- batch: 040 ----
mean loss: 153.40
train mean loss: 151.97
epoch train time: 0:00:01.771827
elapsed time: 0:04:00.714579
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:04:54.900457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.98
 ---- batch: 020 ----
mean loss: 152.33
 ---- batch: 030 ----
mean loss: 153.27
 ---- batch: 040 ----
mean loss: 151.82
train mean loss: 153.62
epoch train time: 0:00:01.678658
elapsed time: 0:04:02.393411
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:04:56.579291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.12
 ---- batch: 020 ----
mean loss: 151.25
 ---- batch: 030 ----
mean loss: 154.39
 ---- batch: 040 ----
mean loss: 151.73
train mean loss: 152.55
epoch train time: 0:00:01.676757
elapsed time: 0:04:04.070311
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:04:58.256187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.25
 ---- batch: 020 ----
mean loss: 159.69
 ---- batch: 030 ----
mean loss: 156.85
 ---- batch: 040 ----
mean loss: 153.95
train mean loss: 155.16
epoch train time: 0:00:01.795285
elapsed time: 0:04:05.865763
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:05:00.051652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.63
 ---- batch: 020 ----
mean loss: 151.69
 ---- batch: 030 ----
mean loss: 150.01
 ---- batch: 040 ----
mean loss: 156.63
train mean loss: 152.90
epoch train time: 0:00:01.670986
elapsed time: 0:04:07.536922
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:05:01.722813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.73
 ---- batch: 020 ----
mean loss: 156.36
 ---- batch: 030 ----
mean loss: 148.71
 ---- batch: 040 ----
mean loss: 146.32
train mean loss: 151.52
epoch train time: 0:00:01.782726
elapsed time: 0:04:09.319797
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:05:03.505670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.69
 ---- batch: 020 ----
mean loss: 146.22
 ---- batch: 030 ----
mean loss: 151.88
 ---- batch: 040 ----
mean loss: 149.02
train mean loss: 150.38
epoch train time: 0:00:01.676481
elapsed time: 0:04:10.996421
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:05:05.182310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.99
 ---- batch: 020 ----
mean loss: 157.17
 ---- batch: 030 ----
mean loss: 149.85
 ---- batch: 040 ----
mean loss: 146.75
train mean loss: 151.06
epoch train time: 0:00:01.697006
elapsed time: 0:04:12.693599
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:05:06.879473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.65
 ---- batch: 020 ----
mean loss: 149.87
 ---- batch: 030 ----
mean loss: 144.65
 ---- batch: 040 ----
mean loss: 148.20
train mean loss: 148.08
epoch train time: 0:00:01.742322
elapsed time: 0:04:14.436074
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:05:08.621953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.34
 ---- batch: 020 ----
mean loss: 147.19
 ---- batch: 030 ----
mean loss: 148.59
 ---- batch: 040 ----
mean loss: 156.30
train mean loss: 150.47
epoch train time: 0:00:01.675225
elapsed time: 0:04:16.111439
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:05:10.297310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.45
 ---- batch: 020 ----
mean loss: 144.78
 ---- batch: 030 ----
mean loss: 144.55
 ---- batch: 040 ----
mean loss: 153.92
train mean loss: 147.09
epoch train time: 0:00:01.780089
elapsed time: 0:04:17.891689
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:05:12.077577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.16
 ---- batch: 020 ----
mean loss: 150.97
 ---- batch: 030 ----
mean loss: 154.42
 ---- batch: 040 ----
mean loss: 144.04
train mean loss: 148.26
epoch train time: 0:00:01.678982
elapsed time: 0:04:19.570825
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:05:13.756730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.36
 ---- batch: 020 ----
mean loss: 150.95
 ---- batch: 030 ----
mean loss: 153.07
 ---- batch: 040 ----
mean loss: 146.53
train mean loss: 149.84
epoch train time: 0:00:01.785930
elapsed time: 0:04:21.356971
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:05:15.542881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.46
 ---- batch: 020 ----
mean loss: 147.20
 ---- batch: 030 ----
mean loss: 143.09
 ---- batch: 040 ----
mean loss: 146.00
train mean loss: 146.00
epoch train time: 0:00:01.685465
elapsed time: 0:04:23.042627
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:05:17.228505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.59
 ---- batch: 020 ----
mean loss: 139.89
 ---- batch: 030 ----
mean loss: 141.19
 ---- batch: 040 ----
mean loss: 137.76
train mean loss: 141.00
epoch train time: 0:00:01.665928
elapsed time: 0:04:24.708716
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:05:18.894592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.21
 ---- batch: 020 ----
mean loss: 142.18
 ---- batch: 030 ----
mean loss: 142.37
 ---- batch: 040 ----
mean loss: 148.99
train mean loss: 143.16
epoch train time: 0:00:01.781476
elapsed time: 0:04:26.490339
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:05:20.676230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.61
 ---- batch: 020 ----
mean loss: 142.84
 ---- batch: 030 ----
mean loss: 145.38
 ---- batch: 040 ----
mean loss: 142.62
train mean loss: 143.28
epoch train time: 0:00:01.678075
elapsed time: 0:04:28.168569
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:05:22.354446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.07
 ---- batch: 020 ----
mean loss: 148.91
 ---- batch: 030 ----
mean loss: 146.05
 ---- batch: 040 ----
mean loss: 138.32
train mean loss: 145.04
epoch train time: 0:00:01.797574
elapsed time: 0:04:29.966288
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:05:24.152151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.62
 ---- batch: 020 ----
mean loss: 142.82
 ---- batch: 030 ----
mean loss: 150.21
 ---- batch: 040 ----
mean loss: 139.02
train mean loss: 144.62
epoch train time: 0:00:01.680705
elapsed time: 0:04:31.647131
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:05:25.833004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.25
 ---- batch: 020 ----
mean loss: 143.85
 ---- batch: 030 ----
mean loss: 141.90
 ---- batch: 040 ----
mean loss: 139.57
train mean loss: 143.76
epoch train time: 0:00:01.759149
elapsed time: 0:04:33.406405
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:05:27.592301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.89
 ---- batch: 020 ----
mean loss: 141.43
 ---- batch: 030 ----
mean loss: 142.06
 ---- batch: 040 ----
mean loss: 144.22
train mean loss: 142.00
epoch train time: 0:00:01.693747
elapsed time: 0:04:35.100319
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:05:29.286196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.61
 ---- batch: 020 ----
mean loss: 140.14
 ---- batch: 030 ----
mean loss: 137.16
 ---- batch: 040 ----
mean loss: 141.30
train mean loss: 140.69
epoch train time: 0:00:01.669176
elapsed time: 0:04:36.769655
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:05:30.955531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.25
 ---- batch: 020 ----
mean loss: 139.59
 ---- batch: 030 ----
mean loss: 137.27
 ---- batch: 040 ----
mean loss: 140.86
train mean loss: 140.60
epoch train time: 0:00:01.781269
elapsed time: 0:04:38.551069
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:05:32.736945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.81
 ---- batch: 020 ----
mean loss: 147.29
 ---- batch: 030 ----
mean loss: 139.17
 ---- batch: 040 ----
mean loss: 144.94
train mean loss: 143.08
epoch train time: 0:00:01.676615
elapsed time: 0:04:40.227826
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:05:34.413732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.40
 ---- batch: 020 ----
mean loss: 140.72
 ---- batch: 030 ----
mean loss: 143.90
 ---- batch: 040 ----
mean loss: 136.66
train mean loss: 138.35
epoch train time: 0:00:01.779927
elapsed time: 0:04:42.007955
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:05:36.193874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.76
 ---- batch: 020 ----
mean loss: 142.70
 ---- batch: 030 ----
mean loss: 134.07
 ---- batch: 040 ----
mean loss: 143.42
train mean loss: 139.83
epoch train time: 0:00:01.687903
elapsed time: 0:04:43.696067
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:05:37.882017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.21
 ---- batch: 020 ----
mean loss: 134.98
 ---- batch: 030 ----
mean loss: 132.93
 ---- batch: 040 ----
mean loss: 144.58
train mean loss: 137.57
epoch train time: 0:00:01.670649
elapsed time: 0:04:45.366929
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:05:39.552817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.19
 ---- batch: 020 ----
mean loss: 138.57
 ---- batch: 030 ----
mean loss: 140.62
 ---- batch: 040 ----
mean loss: 139.13
train mean loss: 139.13
epoch train time: 0:00:01.781783
elapsed time: 0:04:47.148854
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:05:41.334742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.55
 ---- batch: 020 ----
mean loss: 133.30
 ---- batch: 030 ----
mean loss: 134.15
 ---- batch: 040 ----
mean loss: 139.13
train mean loss: 136.68
epoch train time: 0:00:01.678855
elapsed time: 0:04:48.827869
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:05:43.013746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.51
 ---- batch: 020 ----
mean loss: 135.48
 ---- batch: 030 ----
mean loss: 132.95
 ---- batch: 040 ----
mean loss: 138.98
train mean loss: 135.56
epoch train time: 0:00:01.787118
elapsed time: 0:04:50.615132
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:05:44.801006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.20
 ---- batch: 020 ----
mean loss: 140.33
 ---- batch: 030 ----
mean loss: 134.15
 ---- batch: 040 ----
mean loss: 142.41
train mean loss: 138.81
epoch train time: 0:00:01.687300
elapsed time: 0:04:52.302600
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:05:46.488476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.67
 ---- batch: 020 ----
mean loss: 139.52
 ---- batch: 030 ----
mean loss: 144.10
 ---- batch: 040 ----
mean loss: 140.11
train mean loss: 141.16
epoch train time: 0:00:01.773595
elapsed time: 0:04:54.076337
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:05:48.262211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.13
 ---- batch: 020 ----
mean loss: 134.30
 ---- batch: 030 ----
mean loss: 140.26
 ---- batch: 040 ----
mean loss: 134.94
train mean loss: 135.46
epoch train time: 0:00:01.688168
elapsed time: 0:04:55.764675
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:05:49.950549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.37
 ---- batch: 020 ----
mean loss: 138.01
 ---- batch: 030 ----
mean loss: 140.23
 ---- batch: 040 ----
mean loss: 138.66
train mean loss: 138.20
epoch train time: 0:00:01.673174
elapsed time: 0:04:57.438008
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:05:51.623881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.43
 ---- batch: 020 ----
mean loss: 139.04
 ---- batch: 030 ----
mean loss: 137.66
 ---- batch: 040 ----
mean loss: 131.18
train mean loss: 137.02
epoch train time: 0:00:01.780617
elapsed time: 0:04:59.218768
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:05:53.404638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.36
 ---- batch: 020 ----
mean loss: 132.75
 ---- batch: 030 ----
mean loss: 138.54
 ---- batch: 040 ----
mean loss: 134.75
train mean loss: 135.63
epoch train time: 0:00:01.674200
elapsed time: 0:05:00.893100
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:05:55.078975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.38
 ---- batch: 020 ----
mean loss: 135.67
 ---- batch: 030 ----
mean loss: 135.31
 ---- batch: 040 ----
mean loss: 137.36
train mean loss: 136.67
epoch train time: 0:00:01.780580
elapsed time: 0:05:02.673820
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:05:56.859698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.34
 ---- batch: 020 ----
mean loss: 137.54
 ---- batch: 030 ----
mean loss: 137.96
 ---- batch: 040 ----
mean loss: 135.59
train mean loss: 136.23
epoch train time: 0:00:01.682946
elapsed time: 0:05:04.356918
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:05:58.542795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.07
 ---- batch: 020 ----
mean loss: 134.72
 ---- batch: 030 ----
mean loss: 142.15
 ---- batch: 040 ----
mean loss: 137.17
train mean loss: 137.94
epoch train time: 0:00:01.771893
elapsed time: 0:05:06.128964
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:06:00.314840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.44
 ---- batch: 020 ----
mean loss: 134.63
 ---- batch: 030 ----
mean loss: 136.80
 ---- batch: 040 ----
mean loss: 134.31
train mean loss: 135.35
epoch train time: 0:00:01.692927
elapsed time: 0:05:07.822038
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:06:02.007932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.53
 ---- batch: 020 ----
mean loss: 131.43
 ---- batch: 030 ----
mean loss: 135.50
 ---- batch: 040 ----
mean loss: 133.92
train mean loss: 132.60
epoch train time: 0:00:01.668714
elapsed time: 0:05:09.490935
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:06:03.676815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.75
 ---- batch: 020 ----
mean loss: 142.80
 ---- batch: 030 ----
mean loss: 135.23
 ---- batch: 040 ----
mean loss: 140.61
train mean loss: 139.14
epoch train time: 0:00:01.786580
elapsed time: 0:05:11.277672
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:06:05.463543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.79
 ---- batch: 020 ----
mean loss: 135.50
 ---- batch: 030 ----
mean loss: 133.47
 ---- batch: 040 ----
mean loss: 131.49
train mean loss: 133.37
epoch train time: 0:00:01.679710
elapsed time: 0:05:12.957568
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:06:07.143432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.22
 ---- batch: 020 ----
mean loss: 134.71
 ---- batch: 030 ----
mean loss: 140.22
 ---- batch: 040 ----
mean loss: 135.01
train mean loss: 135.87
epoch train time: 0:00:01.779765
elapsed time: 0:05:14.737465
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:06:08.923339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.01
 ---- batch: 020 ----
mean loss: 131.68
 ---- batch: 030 ----
mean loss: 133.87
 ---- batch: 040 ----
mean loss: 135.40
train mean loss: 133.05
epoch train time: 0:00:01.684848
elapsed time: 0:05:16.422462
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:06:10.608339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.22
 ---- batch: 020 ----
mean loss: 134.04
 ---- batch: 030 ----
mean loss: 136.01
 ---- batch: 040 ----
mean loss: 131.92
train mean loss: 133.29
epoch train time: 0:00:01.668436
elapsed time: 0:05:18.091039
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:06:12.276914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.06
 ---- batch: 020 ----
mean loss: 131.52
 ---- batch: 030 ----
mean loss: 130.27
 ---- batch: 040 ----
mean loss: 132.83
train mean loss: 131.78
epoch train time: 0:00:01.797245
elapsed time: 0:05:19.888422
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:06:14.074295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.12
 ---- batch: 020 ----
mean loss: 134.48
 ---- batch: 030 ----
mean loss: 129.55
 ---- batch: 040 ----
mean loss: 129.20
train mean loss: 132.34
epoch train time: 0:00:01.676472
elapsed time: 0:05:21.565041
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:06:15.750918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.61
 ---- batch: 020 ----
mean loss: 141.40
 ---- batch: 030 ----
mean loss: 152.28
 ---- batch: 040 ----
mean loss: 145.07
train mean loss: 142.49
epoch train time: 0:00:01.779430
elapsed time: 0:05:23.344608
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:06:17.530481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.75
 ---- batch: 020 ----
mean loss: 137.39
 ---- batch: 030 ----
mean loss: 130.26
 ---- batch: 040 ----
mean loss: 128.29
train mean loss: 131.71
epoch train time: 0:00:01.680205
elapsed time: 0:05:25.025000
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:06:19.210898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.80
 ---- batch: 020 ----
mean loss: 130.00
 ---- batch: 030 ----
mean loss: 130.45
 ---- batch: 040 ----
mean loss: 136.32
train mean loss: 132.12
epoch train time: 0:00:01.724193
elapsed time: 0:05:26.749363
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:06:20.935255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.97
 ---- batch: 020 ----
mean loss: 139.57
 ---- batch: 030 ----
mean loss: 130.55
 ---- batch: 040 ----
mean loss: 132.07
train mean loss: 134.45
epoch train time: 0:00:01.728288
elapsed time: 0:05:28.477838
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:06:22.663723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.56
 ---- batch: 020 ----
mean loss: 131.20
 ---- batch: 030 ----
mean loss: 132.63
 ---- batch: 040 ----
mean loss: 136.20
train mean loss: 133.09
epoch train time: 0:00:01.671069
elapsed time: 0:05:30.149055
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:06:24.334928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.63
 ---- batch: 020 ----
mean loss: 131.03
 ---- batch: 030 ----
mean loss: 130.13
 ---- batch: 040 ----
mean loss: 135.59
train mean loss: 132.50
epoch train time: 0:00:01.810206
elapsed time: 0:05:31.959392
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:06:26.145265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.39
 ---- batch: 020 ----
mean loss: 127.82
 ---- batch: 030 ----
mean loss: 134.56
 ---- batch: 040 ----
mean loss: 130.97
train mean loss: 131.47
epoch train time: 0:00:01.680603
elapsed time: 0:05:33.640156
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:06:27.826034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.23
 ---- batch: 020 ----
mean loss: 131.51
 ---- batch: 030 ----
mean loss: 128.05
 ---- batch: 040 ----
mean loss: 131.96
train mean loss: 131.98
epoch train time: 0:00:01.780495
elapsed time: 0:05:35.421513
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:06:29.607398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.75
 ---- batch: 020 ----
mean loss: 128.67
 ---- batch: 030 ----
mean loss: 132.01
 ---- batch: 040 ----
mean loss: 133.57
train mean loss: 130.92
epoch train time: 0:00:01.688935
elapsed time: 0:05:37.110630
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:06:31.296509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.23
 ---- batch: 020 ----
mean loss: 127.40
 ---- batch: 030 ----
mean loss: 128.81
 ---- batch: 040 ----
mean loss: 128.05
train mean loss: 129.62
epoch train time: 0:00:01.667740
elapsed time: 0:05:38.778512
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:06:32.964414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.07
 ---- batch: 020 ----
mean loss: 131.03
 ---- batch: 030 ----
mean loss: 134.77
 ---- batch: 040 ----
mean loss: 134.20
train mean loss: 133.15
epoch train time: 0:00:01.774273
elapsed time: 0:05:40.552953
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:06:34.738830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.29
 ---- batch: 020 ----
mean loss: 132.81
 ---- batch: 030 ----
mean loss: 132.12
 ---- batch: 040 ----
mean loss: 130.91
train mean loss: 131.37
epoch train time: 0:00:01.676773
elapsed time: 0:05:42.229883
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:06:36.415761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.80
 ---- batch: 020 ----
mean loss: 139.62
 ---- batch: 030 ----
mean loss: 137.18
 ---- batch: 040 ----
mean loss: 129.90
train mean loss: 134.33
epoch train time: 0:00:01.777867
elapsed time: 0:05:44.007911
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:06:38.193781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.73
 ---- batch: 020 ----
mean loss: 130.13
 ---- batch: 030 ----
mean loss: 128.63
 ---- batch: 040 ----
mean loss: 131.33
train mean loss: 130.31
epoch train time: 0:00:01.677065
elapsed time: 0:05:45.685144
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:06:39.871024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.19
 ---- batch: 020 ----
mean loss: 134.16
 ---- batch: 030 ----
mean loss: 128.76
 ---- batch: 040 ----
mean loss: 129.32
train mean loss: 129.43
epoch train time: 0:00:01.689196
elapsed time: 0:05:47.374476
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:06:41.560379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.73
 ---- batch: 020 ----
mean loss: 138.66
 ---- batch: 030 ----
mean loss: 131.07
 ---- batch: 040 ----
mean loss: 128.39
train mean loss: 130.88
epoch train time: 0:00:01.768614
elapsed time: 0:05:49.143258
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:06:43.329149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.34
 ---- batch: 020 ----
mean loss: 134.16
 ---- batch: 030 ----
mean loss: 134.09
 ---- batch: 040 ----
mean loss: 128.74
train mean loss: 130.09
epoch train time: 0:00:01.668224
elapsed time: 0:05:50.811652
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:06:44.997530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.55
 ---- batch: 020 ----
mean loss: 132.99
 ---- batch: 030 ----
mean loss: 124.19
 ---- batch: 040 ----
mean loss: 130.13
train mean loss: 128.06
epoch train time: 0:00:01.779140
elapsed time: 0:05:52.590949
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:06:46.776825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.63
 ---- batch: 020 ----
mean loss: 130.43
 ---- batch: 030 ----
mean loss: 134.34
 ---- batch: 040 ----
mean loss: 127.33
train mean loss: 130.16
epoch train time: 0:00:01.679989
elapsed time: 0:05:54.271083
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:06:48.456959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.41
 ---- batch: 020 ----
mean loss: 131.29
 ---- batch: 030 ----
mean loss: 127.74
 ---- batch: 040 ----
mean loss: 123.91
train mean loss: 128.64
epoch train time: 0:00:01.771341
elapsed time: 0:05:56.042573
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:06:50.228451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.52
 ---- batch: 020 ----
mean loss: 130.22
 ---- batch: 030 ----
mean loss: 128.70
 ---- batch: 040 ----
mean loss: 124.64
train mean loss: 128.31
epoch train time: 0:00:01.687255
elapsed time: 0:05:57.729973
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:06:51.915847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.18
 ---- batch: 020 ----
mean loss: 131.88
 ---- batch: 030 ----
mean loss: 127.56
 ---- batch: 040 ----
mean loss: 123.86
train mean loss: 128.91
epoch train time: 0:00:01.671962
elapsed time: 0:05:59.402072
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:06:53.587946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.33
 ---- batch: 020 ----
mean loss: 126.58
 ---- batch: 030 ----
mean loss: 122.80
 ---- batch: 040 ----
mean loss: 124.86
train mean loss: 124.77
epoch train time: 0:00:01.800879
elapsed time: 0:06:01.203132
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:06:55.389028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.53
 ---- batch: 020 ----
mean loss: 125.79
 ---- batch: 030 ----
mean loss: 123.21
 ---- batch: 040 ----
mean loss: 120.01
train mean loss: 123.19
epoch train time: 0:00:01.675658
elapsed time: 0:06:02.878990
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:06:57.064894
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.03
 ---- batch: 020 ----
mean loss: 125.81
 ---- batch: 030 ----
mean loss: 128.19
 ---- batch: 040 ----
mean loss: 124.69
train mean loss: 125.15
epoch train time: 0:00:01.785452
elapsed time: 0:06:04.664615
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:06:58.850515
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.02
 ---- batch: 020 ----
mean loss: 122.38
 ---- batch: 030 ----
mean loss: 126.39
 ---- batch: 040 ----
mean loss: 128.83
train mean loss: 124.78
epoch train time: 0:00:01.674854
elapsed time: 0:06:06.339634
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:07:00.525510
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.66
 ---- batch: 020 ----
mean loss: 125.71
 ---- batch: 030 ----
mean loss: 122.60
 ---- batch: 040 ----
mean loss: 126.46
train mean loss: 126.05
epoch train time: 0:00:01.668084
elapsed time: 0:06:08.007857
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:07:02.193728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.16
 ---- batch: 020 ----
mean loss: 126.64
 ---- batch: 030 ----
mean loss: 127.59
 ---- batch: 040 ----
mean loss: 119.85
train mean loss: 124.99
epoch train time: 0:00:01.781081
elapsed time: 0:06:09.789068
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:07:03.974955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.96
 ---- batch: 020 ----
mean loss: 125.38
 ---- batch: 030 ----
mean loss: 126.14
 ---- batch: 040 ----
mean loss: 125.38
train mean loss: 125.90
epoch train time: 0:00:01.673346
elapsed time: 0:06:11.462616
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:07:05.648511
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.21
 ---- batch: 020 ----
mean loss: 129.85
 ---- batch: 030 ----
mean loss: 118.83
 ---- batch: 040 ----
mean loss: 122.79
train mean loss: 123.94
epoch train time: 0:00:01.779026
elapsed time: 0:06:13.241801
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:07:07.427673
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.48
 ---- batch: 020 ----
mean loss: 128.87
 ---- batch: 030 ----
mean loss: 123.34
 ---- batch: 040 ----
mean loss: 125.83
train mean loss: 123.85
epoch train time: 0:00:01.677814
elapsed time: 0:06:14.919759
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:07:09.105634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.56
 ---- batch: 020 ----
mean loss: 128.00
 ---- batch: 030 ----
mean loss: 127.96
 ---- batch: 040 ----
mean loss: 125.33
train mean loss: 126.18
epoch train time: 0:00:01.769555
elapsed time: 0:06:16.689451
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:07:10.875327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.17
 ---- batch: 020 ----
mean loss: 124.69
 ---- batch: 030 ----
mean loss: 120.31
 ---- batch: 040 ----
mean loss: 122.97
train mean loss: 123.30
epoch train time: 0:00:01.679563
elapsed time: 0:06:18.369158
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:07:12.555034
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.37
 ---- batch: 020 ----
mean loss: 124.58
 ---- batch: 030 ----
mean loss: 123.74
 ---- batch: 040 ----
mean loss: 120.92
train mean loss: 123.48
epoch train time: 0:00:01.671744
elapsed time: 0:06:20.041052
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:07:14.226944
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.63
 ---- batch: 020 ----
mean loss: 119.83
 ---- batch: 030 ----
mean loss: 124.77
 ---- batch: 040 ----
mean loss: 125.30
train mean loss: 124.45
epoch train time: 0:00:01.786582
elapsed time: 0:06:21.827791
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:07:16.013678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.81
 ---- batch: 020 ----
mean loss: 125.97
 ---- batch: 030 ----
mean loss: 124.30
 ---- batch: 040 ----
mean loss: 124.13
train mean loss: 123.67
epoch train time: 0:00:01.680208
elapsed time: 0:06:23.508163
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:07:17.694036
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.91
 ---- batch: 020 ----
mean loss: 128.36
 ---- batch: 030 ----
mean loss: 124.23
 ---- batch: 040 ----
mean loss: 121.41
train mean loss: 124.13
epoch train time: 0:00:01.782184
elapsed time: 0:06:25.290493
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:07:19.476367
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.99
 ---- batch: 020 ----
mean loss: 124.32
 ---- batch: 030 ----
mean loss: 122.97
 ---- batch: 040 ----
mean loss: 121.63
train mean loss: 122.53
epoch train time: 0:00:01.680876
elapsed time: 0:06:26.971512
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:07:21.157391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.47
 ---- batch: 020 ----
mean loss: 120.19
 ---- batch: 030 ----
mean loss: 127.92
 ---- batch: 040 ----
mean loss: 129.54
train mean loss: 125.23
epoch train time: 0:00:01.669900
elapsed time: 0:06:28.641558
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:07:22.827432
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.04
 ---- batch: 020 ----
mean loss: 125.85
 ---- batch: 030 ----
mean loss: 122.44
 ---- batch: 040 ----
mean loss: 123.14
train mean loss: 123.73
epoch train time: 0:00:01.792459
elapsed time: 0:06:30.434157
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:07:24.620046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.89
 ---- batch: 020 ----
mean loss: 126.78
 ---- batch: 030 ----
mean loss: 125.04
 ---- batch: 040 ----
mean loss: 122.29
train mean loss: 125.38
epoch train time: 0:00:01.680510
elapsed time: 0:06:32.114858
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:07:26.300736
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.96
 ---- batch: 020 ----
mean loss: 126.20
 ---- batch: 030 ----
mean loss: 123.79
 ---- batch: 040 ----
mean loss: 124.05
train mean loss: 123.88
epoch train time: 0:00:01.774919
elapsed time: 0:06:33.889926
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:07:28.075800
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.05
 ---- batch: 020 ----
mean loss: 126.47
 ---- batch: 030 ----
mean loss: 115.15
 ---- batch: 040 ----
mean loss: 124.05
train mean loss: 122.86
epoch train time: 0:00:01.683312
elapsed time: 0:06:35.573400
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:07:29.759286
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.21
 ---- batch: 020 ----
mean loss: 123.81
 ---- batch: 030 ----
mean loss: 124.91
 ---- batch: 040 ----
mean loss: 122.65
train mean loss: 124.12
epoch train time: 0:00:01.774356
elapsed time: 0:06:37.347942
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:07:31.533832
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.13
 ---- batch: 020 ----
mean loss: 124.82
 ---- batch: 030 ----
mean loss: 124.69
 ---- batch: 040 ----
mean loss: 120.24
train mean loss: 122.96
epoch train time: 0:00:01.689077
elapsed time: 0:06:39.037195
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:07:33.223072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.67
 ---- batch: 020 ----
mean loss: 123.66
 ---- batch: 030 ----
mean loss: 118.52
 ---- batch: 040 ----
mean loss: 125.18
train mean loss: 123.36
epoch train time: 0:00:01.673747
elapsed time: 0:06:40.711089
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:07:34.896968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.14
 ---- batch: 020 ----
mean loss: 122.86
 ---- batch: 030 ----
mean loss: 128.04
 ---- batch: 040 ----
mean loss: 125.78
train mean loss: 125.50
epoch train time: 0:00:01.784737
elapsed time: 0:06:42.495986
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:07:36.681858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.55
 ---- batch: 020 ----
mean loss: 123.22
 ---- batch: 030 ----
mean loss: 120.56
 ---- batch: 040 ----
mean loss: 124.67
train mean loss: 123.39
epoch train time: 0:00:01.674184
elapsed time: 0:06:44.170313
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:07:38.356187
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.62
 ---- batch: 020 ----
mean loss: 122.12
 ---- batch: 030 ----
mean loss: 129.60
 ---- batch: 040 ----
mean loss: 124.60
train mean loss: 124.62
epoch train time: 0:00:01.772159
elapsed time: 0:06:45.942634
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:07:40.128551
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.81
 ---- batch: 020 ----
mean loss: 121.56
 ---- batch: 030 ----
mean loss: 124.89
 ---- batch: 040 ----
mean loss: 126.09
train mean loss: 124.63
epoch train time: 0:00:01.678744
elapsed time: 0:06:47.621567
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:07:41.807446
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.21
 ---- batch: 020 ----
mean loss: 119.63
 ---- batch: 030 ----
mean loss: 124.29
 ---- batch: 040 ----
mean loss: 130.11
train mean loss: 123.79
epoch train time: 0:00:01.680522
elapsed time: 0:06:49.302233
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:07:43.488124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.37
 ---- batch: 020 ----
mean loss: 127.81
 ---- batch: 030 ----
mean loss: 126.34
 ---- batch: 040 ----
mean loss: 120.13
train mean loss: 124.56
epoch train time: 0:00:01.764593
elapsed time: 0:06:51.066995
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:07:45.252874
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.80
 ---- batch: 020 ----
mean loss: 122.77
 ---- batch: 030 ----
mean loss: 125.35
 ---- batch: 040 ----
mean loss: 124.88
train mean loss: 124.12
epoch train time: 0:00:01.669032
elapsed time: 0:06:52.736205
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:07:46.922084
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.00
 ---- batch: 020 ----
mean loss: 125.89
 ---- batch: 030 ----
mean loss: 127.39
 ---- batch: 040 ----
mean loss: 118.48
train mean loss: 123.65
epoch train time: 0:00:01.783601
elapsed time: 0:06:54.520013
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:07:48.705893
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.40
 ---- batch: 020 ----
mean loss: 123.28
 ---- batch: 030 ----
mean loss: 121.83
 ---- batch: 040 ----
mean loss: 124.51
train mean loss: 123.22
epoch train time: 0:00:01.681313
elapsed time: 0:06:56.201497
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:07:50.387360
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.77
 ---- batch: 020 ----
mean loss: 122.96
 ---- batch: 030 ----
mean loss: 127.21
 ---- batch: 040 ----
mean loss: 121.75
train mean loss: 124.31
epoch train time: 0:00:01.774273
elapsed time: 0:06:57.975899
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:07:52.161790
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.93
 ---- batch: 020 ----
mean loss: 122.83
 ---- batch: 030 ----
mean loss: 122.03
 ---- batch: 040 ----
mean loss: 123.91
train mean loss: 123.76
epoch train time: 0:00:01.678622
elapsed time: 0:06:59.654693
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:07:53.840593
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.88
 ---- batch: 020 ----
mean loss: 121.41
 ---- batch: 030 ----
mean loss: 122.78
 ---- batch: 040 ----
mean loss: 122.28
train mean loss: 122.97
epoch train time: 0:00:01.672486
elapsed time: 0:07:01.327357
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:07:55.513243
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.06
 ---- batch: 020 ----
mean loss: 122.04
 ---- batch: 030 ----
mean loss: 122.38
 ---- batch: 040 ----
mean loss: 124.83
train mean loss: 124.28
epoch train time: 0:00:01.786152
elapsed time: 0:07:03.113668
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:07:57.299542
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.26
 ---- batch: 020 ----
mean loss: 124.45
 ---- batch: 030 ----
mean loss: 125.40
 ---- batch: 040 ----
mean loss: 126.14
train mean loss: 125.01
epoch train time: 0:00:01.675698
elapsed time: 0:07:04.789507
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:07:58.975382
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.41
 ---- batch: 020 ----
mean loss: 124.81
 ---- batch: 030 ----
mean loss: 124.16
 ---- batch: 040 ----
mean loss: 127.98
train mean loss: 125.51
epoch train time: 0:00:01.774126
elapsed time: 0:07:06.563775
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:08:00.749647
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.56
 ---- batch: 020 ----
mean loss: 121.13
 ---- batch: 030 ----
mean loss: 121.29
 ---- batch: 040 ----
mean loss: 123.87
train mean loss: 122.75
epoch train time: 0:00:01.673303
elapsed time: 0:07:08.237214
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:08:02.423087
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.93
 ---- batch: 020 ----
mean loss: 120.60
 ---- batch: 030 ----
mean loss: 124.20
 ---- batch: 040 ----
mean loss: 125.00
train mean loss: 123.16
epoch train time: 0:00:01.673477
elapsed time: 0:07:09.910826
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:08:04.096717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.63
 ---- batch: 020 ----
mean loss: 122.00
 ---- batch: 030 ----
mean loss: 128.58
 ---- batch: 040 ----
mean loss: 122.84
train mean loss: 123.26
epoch train time: 0:00:01.784541
elapsed time: 0:07:11.695518
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:08:05.881389
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.73
 ---- batch: 020 ----
mean loss: 120.11
 ---- batch: 030 ----
mean loss: 125.00
 ---- batch: 040 ----
mean loss: 123.45
train mean loss: 123.29
epoch train time: 0:00:01.675525
elapsed time: 0:07:13.371188
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:08:07.557065
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.80
 ---- batch: 020 ----
mean loss: 121.16
 ---- batch: 030 ----
mean loss: 125.78
 ---- batch: 040 ----
mean loss: 119.41
train mean loss: 122.29
epoch train time: 0:00:01.783712
elapsed time: 0:07:15.155086
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:08:09.340954
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.81
 ---- batch: 020 ----
mean loss: 125.19
 ---- batch: 030 ----
mean loss: 126.67
 ---- batch: 040 ----
mean loss: 120.33
train mean loss: 122.95
epoch train time: 0:00:01.677970
elapsed time: 0:07:16.833196
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:08:11.019071
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.97
 ---- batch: 020 ----
mean loss: 124.02
 ---- batch: 030 ----
mean loss: 123.26
 ---- batch: 040 ----
mean loss: 120.24
train mean loss: 123.39
epoch train time: 0:00:01.779329
elapsed time: 0:07:18.612679
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:08:12.798572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.56
 ---- batch: 020 ----
mean loss: 120.65
 ---- batch: 030 ----
mean loss: 120.31
 ---- batch: 040 ----
mean loss: 120.42
train mean loss: 120.63
epoch train time: 0:00:01.678738
elapsed time: 0:07:20.291587
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:08:14.477460
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.38
 ---- batch: 020 ----
mean loss: 123.00
 ---- batch: 030 ----
mean loss: 131.40
 ---- batch: 040 ----
mean loss: 119.47
train mean loss: 124.22
epoch train time: 0:00:01.671016
elapsed time: 0:07:21.962735
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:08:16.148609
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.60
 ---- batch: 020 ----
mean loss: 119.29
 ---- batch: 030 ----
mean loss: 126.37
 ---- batch: 040 ----
mean loss: 122.02
train mean loss: 123.17
epoch train time: 0:00:01.783400
elapsed time: 0:07:23.749314
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_7/checkpoint.pth.tar
**** end time: 2019-09-20 23:08:17.935155 ****
