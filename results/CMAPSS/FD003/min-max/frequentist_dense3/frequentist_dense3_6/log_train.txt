Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_6', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8751
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistDense3...
Done.
**** start time: 2019-09-20 23:45:47.605113 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
            Linear-2                  [-1, 100]          42,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 62,100
Trainable params: 62,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:45:47.608462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4728.92
 ---- batch: 020 ----
mean loss: 4628.47
 ---- batch: 030 ----
mean loss: 4587.27
 ---- batch: 040 ----
mean loss: 4437.11
train mean loss: 4578.42
epoch train time: 0:00:14.714853
elapsed time: 0:00:14.720383
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:46:02.325538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4377.91
 ---- batch: 020 ----
mean loss: 4251.43
 ---- batch: 030 ----
mean loss: 4173.35
 ---- batch: 040 ----
mean loss: 4212.46
train mean loss: 4242.73
epoch train time: 0:00:00.200423
elapsed time: 0:00:14.920938
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:46:02.526102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4041.17
 ---- batch: 020 ----
mean loss: 4020.50
 ---- batch: 030 ----
mean loss: 4029.70
 ---- batch: 040 ----
mean loss: 3913.91
train mean loss: 3987.36
epoch train time: 0:00:00.199678
elapsed time: 0:00:15.120739
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:46:02.725891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3810.11
 ---- batch: 020 ----
mean loss: 3869.46
 ---- batch: 030 ----
mean loss: 3615.43
 ---- batch: 040 ----
mean loss: 3695.90
train mean loss: 3748.70
epoch train time: 0:00:00.198763
elapsed time: 0:00:15.319619
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:46:02.924775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3621.81
 ---- batch: 020 ----
mean loss: 3524.49
 ---- batch: 030 ----
mean loss: 3501.93
 ---- batch: 040 ----
mean loss: 3414.29
train mean loss: 3506.63
epoch train time: 0:00:00.207836
elapsed time: 0:00:15.527590
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:46:03.132787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3328.23
 ---- batch: 020 ----
mean loss: 3340.26
 ---- batch: 030 ----
mean loss: 3236.50
 ---- batch: 040 ----
mean loss: 3168.14
train mean loss: 3263.22
epoch train time: 0:00:00.206182
elapsed time: 0:00:15.733929
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:46:03.339085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3099.39
 ---- batch: 020 ----
mean loss: 3121.75
 ---- batch: 030 ----
mean loss: 3045.13
 ---- batch: 040 ----
mean loss: 2916.07
train mean loss: 3039.97
epoch train time: 0:00:00.203293
elapsed time: 0:00:15.937341
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:46:03.542496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2922.34
 ---- batch: 020 ----
mean loss: 2810.37
 ---- batch: 030 ----
mean loss: 2841.86
 ---- batch: 040 ----
mean loss: 2758.56
train mean loss: 2829.11
epoch train time: 0:00:00.210180
elapsed time: 0:00:16.147660
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:46:03.752826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2767.89
 ---- batch: 020 ----
mean loss: 2637.81
 ---- batch: 030 ----
mean loss: 2599.97
 ---- batch: 040 ----
mean loss: 2545.90
train mean loss: 2631.54
epoch train time: 0:00:00.204927
elapsed time: 0:00:16.352717
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:46:03.957873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2501.54
 ---- batch: 020 ----
mean loss: 2449.54
 ---- batch: 030 ----
mean loss: 2420.39
 ---- batch: 040 ----
mean loss: 2411.86
train mean loss: 2437.82
epoch train time: 0:00:00.204486
elapsed time: 0:00:16.557326
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:46:04.162484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2316.25
 ---- batch: 020 ----
mean loss: 2292.67
 ---- batch: 030 ----
mean loss: 2265.06
 ---- batch: 040 ----
mean loss: 2176.99
train mean loss: 2263.97
epoch train time: 0:00:00.205634
elapsed time: 0:00:16.763083
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:46:04.368248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2203.40
 ---- batch: 020 ----
mean loss: 2107.67
 ---- batch: 030 ----
mean loss: 2076.75
 ---- batch: 040 ----
mean loss: 2063.47
train mean loss: 2109.71
epoch train time: 0:00:00.203798
elapsed time: 0:00:16.967006
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:46:04.572161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2010.32
 ---- batch: 020 ----
mean loss: 1995.37
 ---- batch: 030 ----
mean loss: 1945.76
 ---- batch: 040 ----
mean loss: 1946.72
train mean loss: 1970.33
epoch train time: 0:00:00.206119
elapsed time: 0:00:17.173246
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:46:04.778426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1884.23
 ---- batch: 020 ----
mean loss: 1857.33
 ---- batch: 030 ----
mean loss: 1820.39
 ---- batch: 040 ----
mean loss: 1808.12
train mean loss: 1840.89
epoch train time: 0:00:00.219461
elapsed time: 0:00:17.392847
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:46:04.998000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1772.38
 ---- batch: 020 ----
mean loss: 1725.73
 ---- batch: 030 ----
mean loss: 1695.67
 ---- batch: 040 ----
mean loss: 1694.43
train mean loss: 1715.89
epoch train time: 0:00:00.201978
elapsed time: 0:00:17.594937
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:46:05.200090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1625.93
 ---- batch: 020 ----
mean loss: 1635.89
 ---- batch: 030 ----
mean loss: 1602.44
 ---- batch: 040 ----
mean loss: 1561.14
train mean loss: 1601.93
epoch train time: 0:00:00.198804
elapsed time: 0:00:17.793853
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:46:05.399007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1511.45
 ---- batch: 020 ----
mean loss: 1498.80
 ---- batch: 030 ----
mean loss: 1484.26
 ---- batch: 040 ----
mean loss: 1465.99
train mean loss: 1489.41
epoch train time: 0:00:00.194837
elapsed time: 0:00:17.988800
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:46:05.593950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1412.64
 ---- batch: 020 ----
mean loss: 1398.07
 ---- batch: 030 ----
mean loss: 1375.00
 ---- batch: 040 ----
mean loss: 1355.11
train mean loss: 1382.80
epoch train time: 0:00:00.204671
elapsed time: 0:00:18.193591
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:46:05.798744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1337.89
 ---- batch: 020 ----
mean loss: 1277.46
 ---- batch: 030 ----
mean loss: 1298.42
 ---- batch: 040 ----
mean loss: 1261.89
train mean loss: 1290.01
epoch train time: 0:00:00.198386
elapsed time: 0:00:18.392098
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:46:05.997260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1225.67
 ---- batch: 020 ----
mean loss: 1226.78
 ---- batch: 030 ----
mean loss: 1220.12
 ---- batch: 040 ----
mean loss: 1170.61
train mean loss: 1210.34
epoch train time: 0:00:00.198436
elapsed time: 0:00:18.590665
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:46:06.195818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1158.95
 ---- batch: 020 ----
mean loss: 1151.83
 ---- batch: 030 ----
mean loss: 1130.78
 ---- batch: 040 ----
mean loss: 1115.64
train mean loss: 1137.29
epoch train time: 0:00:00.198078
elapsed time: 0:00:18.788855
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:46:06.394008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1096.72
 ---- batch: 020 ----
mean loss: 1033.69
 ---- batch: 030 ----
mean loss: 988.61
 ---- batch: 040 ----
mean loss: 970.83
train mean loss: 1016.69
epoch train time: 0:00:00.195878
elapsed time: 0:00:18.984846
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:46:06.589998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.68
 ---- batch: 020 ----
mean loss: 912.98
 ---- batch: 030 ----
mean loss: 893.33
 ---- batch: 040 ----
mean loss: 864.66
train mean loss: 899.24
epoch train time: 0:00:00.194538
elapsed time: 0:00:19.179495
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:46:06.784647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.78
 ---- batch: 020 ----
mean loss: 844.03
 ---- batch: 030 ----
mean loss: 814.35
 ---- batch: 040 ----
mean loss: 792.39
train mean loss: 821.41
epoch train time: 0:00:00.198624
elapsed time: 0:00:19.378250
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:46:06.983400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 790.41
 ---- batch: 020 ----
mean loss: 753.81
 ---- batch: 030 ----
mean loss: 745.51
 ---- batch: 040 ----
mean loss: 726.72
train mean loss: 750.60
epoch train time: 0:00:00.207478
elapsed time: 0:00:19.585851
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:46:07.191019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.95
 ---- batch: 020 ----
mean loss: 705.97
 ---- batch: 030 ----
mean loss: 676.46
 ---- batch: 040 ----
mean loss: 655.42
train mean loss: 683.28
epoch train time: 0:00:00.205770
elapsed time: 0:00:19.791759
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:46:07.396913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 652.45
 ---- batch: 020 ----
mean loss: 630.17
 ---- batch: 030 ----
mean loss: 604.70
 ---- batch: 040 ----
mean loss: 602.33
train mean loss: 620.36
epoch train time: 0:00:00.205276
elapsed time: 0:00:19.997173
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:46:07.602343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 590.05
 ---- batch: 020 ----
mean loss: 569.58
 ---- batch: 030 ----
mean loss: 566.43
 ---- batch: 040 ----
mean loss: 548.14
train mean loss: 565.46
epoch train time: 0:00:00.204185
elapsed time: 0:00:20.201493
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:46:07.806652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 540.64
 ---- batch: 020 ----
mean loss: 522.44
 ---- batch: 030 ----
mean loss: 511.36
 ---- batch: 040 ----
mean loss: 500.72
train mean loss: 516.83
epoch train time: 0:00:00.205312
elapsed time: 0:00:20.406944
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:46:08.012099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.55
 ---- batch: 020 ----
mean loss: 469.61
 ---- batch: 030 ----
mean loss: 475.81
 ---- batch: 040 ----
mean loss: 462.25
train mean loss: 472.28
epoch train time: 0:00:00.206779
elapsed time: 0:00:20.613841
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:46:08.218994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.02
 ---- batch: 020 ----
mean loss: 443.81
 ---- batch: 030 ----
mean loss: 433.24
 ---- batch: 040 ----
mean loss: 422.79
train mean loss: 431.88
epoch train time: 0:00:00.204883
elapsed time: 0:00:20.818871
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:46:08.424042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.61
 ---- batch: 020 ----
mean loss: 406.67
 ---- batch: 030 ----
mean loss: 391.82
 ---- batch: 040 ----
mean loss: 383.82
train mean loss: 396.17
epoch train time: 0:00:00.202077
elapsed time: 0:00:21.021082
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:46:08.626249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.63
 ---- batch: 020 ----
mean loss: 372.54
 ---- batch: 030 ----
mean loss: 354.43
 ---- batch: 040 ----
mean loss: 364.27
train mean loss: 363.49
epoch train time: 0:00:00.207784
elapsed time: 0:00:21.229055
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:46:08.834234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.94
 ---- batch: 020 ----
mean loss: 333.91
 ---- batch: 030 ----
mean loss: 329.14
 ---- batch: 040 ----
mean loss: 326.68
train mean loss: 333.33
epoch train time: 0:00:00.205626
elapsed time: 0:00:21.434822
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:46:09.039975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.89
 ---- batch: 020 ----
mean loss: 310.71
 ---- batch: 030 ----
mean loss: 304.03
 ---- batch: 040 ----
mean loss: 292.90
train mean loss: 306.44
epoch train time: 0:00:00.197943
elapsed time: 0:00:21.632878
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:46:09.238032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.97
 ---- batch: 020 ----
mean loss: 285.09
 ---- batch: 030 ----
mean loss: 280.65
 ---- batch: 040 ----
mean loss: 279.46
train mean loss: 281.99
epoch train time: 0:00:00.190101
elapsed time: 0:00:21.823100
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:46:09.428255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.96
 ---- batch: 020 ----
mean loss: 265.07
 ---- batch: 030 ----
mean loss: 258.40
 ---- batch: 040 ----
mean loss: 253.58
train mean loss: 260.06
epoch train time: 0:00:00.188225
elapsed time: 0:00:22.011446
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:46:09.616625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.16
 ---- batch: 020 ----
mean loss: 241.64
 ---- batch: 030 ----
mean loss: 237.38
 ---- batch: 040 ----
mean loss: 233.69
train mean loss: 239.39
epoch train time: 0:00:00.190076
elapsed time: 0:00:22.201661
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:46:09.806814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.93
 ---- batch: 020 ----
mean loss: 224.15
 ---- batch: 030 ----
mean loss: 218.02
 ---- batch: 040 ----
mean loss: 217.00
train mean loss: 221.50
epoch train time: 0:00:00.206838
elapsed time: 0:00:22.408611
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:46:10.013856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.57
 ---- batch: 020 ----
mean loss: 206.70
 ---- batch: 030 ----
mean loss: 203.29
 ---- batch: 040 ----
mean loss: 198.81
train mean loss: 204.91
epoch train time: 0:00:00.197982
elapsed time: 0:00:22.606802
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:46:10.211958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.97
 ---- batch: 020 ----
mean loss: 191.48
 ---- batch: 030 ----
mean loss: 190.89
 ---- batch: 040 ----
mean loss: 185.77
train mean loss: 190.44
epoch train time: 0:00:00.200212
elapsed time: 0:00:22.807134
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:46:10.412287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.12
 ---- batch: 020 ----
mean loss: 172.66
 ---- batch: 030 ----
mean loss: 179.34
 ---- batch: 040 ----
mean loss: 173.26
train mean loss: 176.69
epoch train time: 0:00:00.193908
elapsed time: 0:00:23.001166
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:46:10.606320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.65
 ---- batch: 020 ----
mean loss: 167.70
 ---- batch: 030 ----
mean loss: 164.68
 ---- batch: 040 ----
mean loss: 159.07
train mean loss: 164.83
epoch train time: 0:00:00.196029
elapsed time: 0:00:23.197309
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:46:10.802460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.14
 ---- batch: 020 ----
mean loss: 159.80
 ---- batch: 030 ----
mean loss: 150.29
 ---- batch: 040 ----
mean loss: 147.53
train mean loss: 153.99
epoch train time: 0:00:00.203286
elapsed time: 0:00:23.400707
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:46:11.005878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.94
 ---- batch: 020 ----
mean loss: 147.39
 ---- batch: 030 ----
mean loss: 144.48
 ---- batch: 040 ----
mean loss: 142.69
train mean loss: 145.46
epoch train time: 0:00:00.201160
elapsed time: 0:00:23.602004
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:46:11.207160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.96
 ---- batch: 020 ----
mean loss: 136.73
 ---- batch: 030 ----
mean loss: 135.32
 ---- batch: 040 ----
mean loss: 133.92
train mean loss: 135.99
epoch train time: 0:00:00.202920
elapsed time: 0:00:23.805043
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:46:11.410212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.62
 ---- batch: 020 ----
mean loss: 126.80
 ---- batch: 030 ----
mean loss: 130.03
 ---- batch: 040 ----
mean loss: 125.52
train mean loss: 127.60
epoch train time: 0:00:00.203126
elapsed time: 0:00:24.008301
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:46:11.613455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.77
 ---- batch: 020 ----
mean loss: 120.70
 ---- batch: 030 ----
mean loss: 119.33
 ---- batch: 040 ----
mean loss: 120.45
train mean loss: 120.62
epoch train time: 0:00:00.205059
elapsed time: 0:00:24.213478
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:46:11.818632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.01
 ---- batch: 020 ----
mean loss: 113.68
 ---- batch: 030 ----
mean loss: 112.40
 ---- batch: 040 ----
mean loss: 115.69
train mean loss: 113.84
epoch train time: 0:00:00.198853
elapsed time: 0:00:24.412444
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:46:12.017611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.40
 ---- batch: 020 ----
mean loss: 109.29
 ---- batch: 030 ----
mean loss: 104.30
 ---- batch: 040 ----
mean loss: 107.14
train mean loss: 108.42
epoch train time: 0:00:00.209883
elapsed time: 0:00:24.622458
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:46:12.227615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.66
 ---- batch: 020 ----
mean loss: 105.05
 ---- batch: 030 ----
mean loss: 102.16
 ---- batch: 040 ----
mean loss: 102.83
train mean loss: 103.46
epoch train time: 0:00:00.205078
elapsed time: 0:00:24.827689
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:46:12.432843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.63
 ---- batch: 020 ----
mean loss: 97.61
 ---- batch: 030 ----
mean loss: 97.97
 ---- batch: 040 ----
mean loss: 96.75
train mean loss: 98.47
epoch train time: 0:00:00.204106
elapsed time: 0:00:25.031916
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:46:12.637073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.40
 ---- batch: 020 ----
mean loss: 93.76
 ---- batch: 030 ----
mean loss: 96.40
 ---- batch: 040 ----
mean loss: 92.75
train mean loss: 94.74
epoch train time: 0:00:00.208662
elapsed time: 0:00:25.240713
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:46:12.845868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.61
 ---- batch: 020 ----
mean loss: 90.04
 ---- batch: 030 ----
mean loss: 89.60
 ---- batch: 040 ----
mean loss: 90.50
train mean loss: 90.51
epoch train time: 0:00:00.203451
elapsed time: 0:00:25.444297
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:46:13.049461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.50
 ---- batch: 020 ----
mean loss: 85.24
 ---- batch: 030 ----
mean loss: 89.04
 ---- batch: 040 ----
mean loss: 85.74
train mean loss: 87.47
epoch train time: 0:00:00.203511
elapsed time: 0:00:25.647937
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:46:13.253093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.38
 ---- batch: 020 ----
mean loss: 85.43
 ---- batch: 030 ----
mean loss: 86.29
 ---- batch: 040 ----
mean loss: 81.60
train mean loss: 84.87
epoch train time: 0:00:00.200104
elapsed time: 0:00:25.848163
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:46:13.453316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.53
 ---- batch: 020 ----
mean loss: 81.60
 ---- batch: 030 ----
mean loss: 79.69
 ---- batch: 040 ----
mean loss: 81.36
train mean loss: 81.69
epoch train time: 0:00:00.195221
elapsed time: 0:00:26.043541
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:46:13.648691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.70
 ---- batch: 020 ----
mean loss: 78.26
 ---- batch: 030 ----
mean loss: 80.49
 ---- batch: 040 ----
mean loss: 77.95
train mean loss: 79.17
epoch train time: 0:00:00.196147
elapsed time: 0:00:26.239795
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:46:13.844963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.54
 ---- batch: 020 ----
mean loss: 75.62
 ---- batch: 030 ----
mean loss: 74.40
 ---- batch: 040 ----
mean loss: 77.70
train mean loss: 76.45
epoch train time: 0:00:00.196286
elapsed time: 0:00:26.436208
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:46:14.041359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.21
 ---- batch: 020 ----
mean loss: 76.18
 ---- batch: 030 ----
mean loss: 73.61
 ---- batch: 040 ----
mean loss: 74.05
train mean loss: 75.07
epoch train time: 0:00:00.198522
elapsed time: 0:00:26.634839
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:46:14.239992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.90
 ---- batch: 020 ----
mean loss: 72.93
 ---- batch: 030 ----
mean loss: 74.74
 ---- batch: 040 ----
mean loss: 72.18
train mean loss: 72.95
epoch train time: 0:00:00.198891
elapsed time: 0:00:26.833842
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:46:14.439009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.29
 ---- batch: 020 ----
mean loss: 71.54
 ---- batch: 030 ----
mean loss: 69.34
 ---- batch: 040 ----
mean loss: 71.34
train mean loss: 71.24
epoch train time: 0:00:00.193097
elapsed time: 0:00:27.027064
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:46:14.632221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.91
 ---- batch: 020 ----
mean loss: 71.42
 ---- batch: 030 ----
mean loss: 71.58
 ---- batch: 040 ----
mean loss: 71.69
train mean loss: 70.34
epoch train time: 0:00:00.196278
elapsed time: 0:00:27.223462
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:46:14.828613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.79
 ---- batch: 020 ----
mean loss: 68.01
 ---- batch: 030 ----
mean loss: 70.98
 ---- batch: 040 ----
mean loss: 67.01
train mean loss: 68.60
epoch train time: 0:00:00.194330
elapsed time: 0:00:27.417934
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:46:15.023095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.38
 ---- batch: 020 ----
mean loss: 64.74
 ---- batch: 030 ----
mean loss: 66.34
 ---- batch: 040 ----
mean loss: 68.91
train mean loss: 66.42
epoch train time: 0:00:00.199039
elapsed time: 0:00:27.617096
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:46:15.222261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.27
 ---- batch: 020 ----
mean loss: 66.23
 ---- batch: 030 ----
mean loss: 63.73
 ---- batch: 040 ----
mean loss: 65.08
train mean loss: 65.44
epoch train time: 0:00:00.203261
elapsed time: 0:00:27.820497
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:46:15.425664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.41
 ---- batch: 020 ----
mean loss: 63.66
 ---- batch: 030 ----
mean loss: 64.70
 ---- batch: 040 ----
mean loss: 62.77
train mean loss: 63.99
epoch train time: 0:00:00.205947
elapsed time: 0:00:28.026606
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:46:15.631770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.51
 ---- batch: 020 ----
mean loss: 65.01
 ---- batch: 030 ----
mean loss: 65.51
 ---- batch: 040 ----
mean loss: 60.47
train mean loss: 63.63
epoch train time: 0:00:00.202716
elapsed time: 0:00:28.229444
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:46:15.834597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.48
 ---- batch: 020 ----
mean loss: 61.08
 ---- batch: 030 ----
mean loss: 63.34
 ---- batch: 040 ----
mean loss: 64.72
train mean loss: 63.40
epoch train time: 0:00:00.201783
elapsed time: 0:00:28.431409
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:46:16.036565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.79
 ---- batch: 020 ----
mean loss: 59.41
 ---- batch: 030 ----
mean loss: 58.88
 ---- batch: 040 ----
mean loss: 63.44
train mean loss: 61.50
epoch train time: 0:00:00.201077
elapsed time: 0:00:28.632607
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:46:16.237762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.50
 ---- batch: 020 ----
mean loss: 58.25
 ---- batch: 030 ----
mean loss: 60.50
 ---- batch: 040 ----
mean loss: 61.94
train mean loss: 60.74
epoch train time: 0:00:00.202082
elapsed time: 0:00:28.834806
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:46:16.439960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.90
 ---- batch: 020 ----
mean loss: 61.60
 ---- batch: 030 ----
mean loss: 60.47
 ---- batch: 040 ----
mean loss: 58.51
train mean loss: 59.93
epoch train time: 0:00:00.198937
elapsed time: 0:00:29.033855
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:46:16.639023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.24
 ---- batch: 020 ----
mean loss: 62.48
 ---- batch: 030 ----
mean loss: 59.17
 ---- batch: 040 ----
mean loss: 59.36
train mean loss: 59.03
epoch train time: 0:00:00.200963
elapsed time: 0:00:29.234950
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:46:16.840117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.24
 ---- batch: 020 ----
mean loss: 55.94
 ---- batch: 030 ----
mean loss: 59.12
 ---- batch: 040 ----
mean loss: 60.18
train mean loss: 58.53
epoch train time: 0:00:00.199573
elapsed time: 0:00:29.434650
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:46:17.039802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.78
 ---- batch: 020 ----
mean loss: 58.88
 ---- batch: 030 ----
mean loss: 58.15
 ---- batch: 040 ----
mean loss: 56.87
train mean loss: 58.06
epoch train time: 0:00:00.205714
elapsed time: 0:00:29.640482
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:46:17.245639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.47
 ---- batch: 020 ----
mean loss: 59.67
 ---- batch: 030 ----
mean loss: 58.13
 ---- batch: 040 ----
mean loss: 55.28
train mean loss: 58.05
epoch train time: 0:00:00.207626
elapsed time: 0:00:29.848227
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:46:17.453389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.37
 ---- batch: 020 ----
mean loss: 52.73
 ---- batch: 030 ----
mean loss: 58.11
 ---- batch: 040 ----
mean loss: 58.77
train mean loss: 56.71
epoch train time: 0:00:00.202271
elapsed time: 0:00:30.050634
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:46:17.655796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.51
 ---- batch: 020 ----
mean loss: 58.50
 ---- batch: 030 ----
mean loss: 57.05
 ---- batch: 040 ----
mean loss: 55.07
train mean loss: 56.52
epoch train time: 0:00:00.205537
elapsed time: 0:00:30.256306
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:46:17.861462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.46
 ---- batch: 020 ----
mean loss: 52.47
 ---- batch: 030 ----
mean loss: 57.77
 ---- batch: 040 ----
mean loss: 56.21
train mean loss: 55.62
epoch train time: 0:00:00.201538
elapsed time: 0:00:30.457965
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:46:18.063122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.61
 ---- batch: 020 ----
mean loss: 54.44
 ---- batch: 030 ----
mean loss: 56.28
 ---- batch: 040 ----
mean loss: 56.11
train mean loss: 55.71
epoch train time: 0:00:00.201985
elapsed time: 0:00:30.660075
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:46:18.265234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.25
 ---- batch: 020 ----
mean loss: 52.34
 ---- batch: 030 ----
mean loss: 55.26
 ---- batch: 040 ----
mean loss: 55.08
train mean loss: 55.04
epoch train time: 0:00:00.199820
elapsed time: 0:00:30.860021
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:46:18.465177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.75
 ---- batch: 020 ----
mean loss: 53.98
 ---- batch: 030 ----
mean loss: 55.22
 ---- batch: 040 ----
mean loss: 57.60
train mean loss: 54.81
epoch train time: 0:00:00.197170
elapsed time: 0:00:31.057340
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:46:18.662494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.18
 ---- batch: 020 ----
mean loss: 53.46
 ---- batch: 030 ----
mean loss: 53.09
 ---- batch: 040 ----
mean loss: 55.67
train mean loss: 54.85
epoch train time: 0:00:00.200225
elapsed time: 0:00:31.257680
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:46:18.862851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.58
 ---- batch: 020 ----
mean loss: 55.62
 ---- batch: 030 ----
mean loss: 55.38
 ---- batch: 040 ----
mean loss: 55.74
train mean loss: 55.03
epoch train time: 0:00:00.194841
elapsed time: 0:00:31.452658
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:46:19.057814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.76
 ---- batch: 020 ----
mean loss: 51.05
 ---- batch: 030 ----
mean loss: 53.58
 ---- batch: 040 ----
mean loss: 52.78
train mean loss: 53.41
epoch train time: 0:00:00.194775
elapsed time: 0:00:31.647585
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:46:19.252750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.58
 ---- batch: 020 ----
mean loss: 53.06
 ---- batch: 030 ----
mean loss: 51.25
 ---- batch: 040 ----
mean loss: 54.69
train mean loss: 53.00
epoch train time: 0:00:00.196130
elapsed time: 0:00:31.843868
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:46:19.449040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.26
 ---- batch: 020 ----
mean loss: 52.37
 ---- batch: 030 ----
mean loss: 51.86
 ---- batch: 040 ----
mean loss: 56.13
train mean loss: 53.00
epoch train time: 0:00:00.201885
elapsed time: 0:00:32.045887
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:46:19.651061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.15
 ---- batch: 020 ----
mean loss: 52.53
 ---- batch: 030 ----
mean loss: 53.01
 ---- batch: 040 ----
mean loss: 51.27
train mean loss: 52.67
epoch train time: 0:00:00.208441
elapsed time: 0:00:32.254462
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:46:19.859629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.85
 ---- batch: 020 ----
mean loss: 51.55
 ---- batch: 030 ----
mean loss: 50.05
 ---- batch: 040 ----
mean loss: 50.83
train mean loss: 51.77
epoch train time: 0:00:00.205056
elapsed time: 0:00:32.459703
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:46:20.064862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.87
 ---- batch: 020 ----
mean loss: 49.90
 ---- batch: 030 ----
mean loss: 52.22
 ---- batch: 040 ----
mean loss: 54.47
train mean loss: 51.79
epoch train time: 0:00:00.204853
elapsed time: 0:00:32.664696
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:46:20.269866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.36
 ---- batch: 020 ----
mean loss: 54.30
 ---- batch: 030 ----
mean loss: 49.72
 ---- batch: 040 ----
mean loss: 52.47
train mean loss: 51.98
epoch train time: 0:00:00.206057
elapsed time: 0:00:32.870902
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:46:20.476059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.88
 ---- batch: 020 ----
mean loss: 51.89
 ---- batch: 030 ----
mean loss: 50.87
 ---- batch: 040 ----
mean loss: 49.56
train mean loss: 50.92
epoch train time: 0:00:00.204318
elapsed time: 0:00:33.075361
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:46:20.680515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.36
 ---- batch: 020 ----
mean loss: 50.60
 ---- batch: 030 ----
mean loss: 52.00
 ---- batch: 040 ----
mean loss: 52.23
train mean loss: 50.97
epoch train time: 0:00:00.203807
elapsed time: 0:00:33.279305
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:46:20.884463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.33
 ---- batch: 020 ----
mean loss: 51.96
 ---- batch: 030 ----
mean loss: 49.59
 ---- batch: 040 ----
mean loss: 50.36
train mean loss: 50.71
epoch train time: 0:00:00.201065
elapsed time: 0:00:33.480491
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:46:21.085647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.31
 ---- batch: 020 ----
mean loss: 52.70
 ---- batch: 030 ----
mean loss: 49.83
 ---- batch: 040 ----
mean loss: 52.53
train mean loss: 50.92
epoch train time: 0:00:00.200689
elapsed time: 0:00:33.681314
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:46:21.286470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.56
 ---- batch: 020 ----
mean loss: 52.41
 ---- batch: 030 ----
mean loss: 51.94
 ---- batch: 040 ----
mean loss: 49.46
train mean loss: 51.55
epoch train time: 0:00:00.200933
elapsed time: 0:00:33.882381
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:46:21.487536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.49
 ---- batch: 020 ----
mean loss: 51.39
 ---- batch: 030 ----
mean loss: 46.36
 ---- batch: 040 ----
mean loss: 49.41
train mean loss: 49.60
epoch train time: 0:00:00.197925
elapsed time: 0:00:34.080432
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:46:21.685626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.63
 ---- batch: 020 ----
mean loss: 49.12
 ---- batch: 030 ----
mean loss: 46.22
 ---- batch: 040 ----
mean loss: 49.89
train mean loss: 49.25
epoch train time: 0:00:00.204439
elapsed time: 0:00:34.285022
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:46:21.890175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.03
 ---- batch: 020 ----
mean loss: 49.78
 ---- batch: 030 ----
mean loss: 49.27
 ---- batch: 040 ----
mean loss: 50.60
train mean loss: 49.67
epoch train time: 0:00:00.190119
elapsed time: 0:00:34.475265
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:46:22.080444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.93
 ---- batch: 020 ----
mean loss: 52.81
 ---- batch: 030 ----
mean loss: 51.90
 ---- batch: 040 ----
mean loss: 50.68
train mean loss: 51.48
epoch train time: 0:00:00.192598
elapsed time: 0:00:34.668004
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:46:22.273158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.46
 ---- batch: 020 ----
mean loss: 48.14
 ---- batch: 030 ----
mean loss: 50.48
 ---- batch: 040 ----
mean loss: 49.35
train mean loss: 49.00
epoch train time: 0:00:00.192249
elapsed time: 0:00:34.860372
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:46:22.465527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.64
 ---- batch: 020 ----
mean loss: 46.93
 ---- batch: 030 ----
mean loss: 48.12
 ---- batch: 040 ----
mean loss: 49.94
train mean loss: 48.69
epoch train time: 0:00:00.193818
elapsed time: 0:00:35.054329
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:46:22.659481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.43
 ---- batch: 020 ----
mean loss: 46.19
 ---- batch: 030 ----
mean loss: 49.86
 ---- batch: 040 ----
mean loss: 50.14
train mean loss: 48.43
epoch train time: 0:00:00.196010
elapsed time: 0:00:35.250451
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:46:22.855604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.70
 ---- batch: 020 ----
mean loss: 48.83
 ---- batch: 030 ----
mean loss: 48.30
 ---- batch: 040 ----
mean loss: 45.80
train mean loss: 47.80
epoch train time: 0:00:00.194315
elapsed time: 0:00:35.445544
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:46:23.050715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.20
 ---- batch: 020 ----
mean loss: 48.60
 ---- batch: 030 ----
mean loss: 45.00
 ---- batch: 040 ----
mean loss: 48.64
train mean loss: 47.64
epoch train time: 0:00:00.195047
elapsed time: 0:00:35.640723
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:46:23.245874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.33
 ---- batch: 020 ----
mean loss: 48.80
 ---- batch: 030 ----
mean loss: 48.07
 ---- batch: 040 ----
mean loss: 52.54
train mean loss: 48.79
epoch train time: 0:00:00.194931
elapsed time: 0:00:35.835765
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:46:23.440921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.16
 ---- batch: 020 ----
mean loss: 46.29
 ---- batch: 030 ----
mean loss: 47.36
 ---- batch: 040 ----
mean loss: 48.09
train mean loss: 47.32
epoch train time: 0:00:00.196922
elapsed time: 0:00:36.032824
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:46:23.637971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.58
 ---- batch: 020 ----
mean loss: 47.54
 ---- batch: 030 ----
mean loss: 46.49
 ---- batch: 040 ----
mean loss: 47.79
train mean loss: 47.67
epoch train time: 0:00:00.203771
elapsed time: 0:00:36.236704
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:46:23.841874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.79
 ---- batch: 020 ----
mean loss: 45.35
 ---- batch: 030 ----
mean loss: 46.33
 ---- batch: 040 ----
mean loss: 49.63
train mean loss: 47.22
epoch train time: 0:00:00.206548
elapsed time: 0:00:36.443406
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:46:24.048575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.15
 ---- batch: 020 ----
mean loss: 45.20
 ---- batch: 030 ----
mean loss: 48.11
 ---- batch: 040 ----
mean loss: 44.26
train mean loss: 46.71
epoch train time: 0:00:00.206363
elapsed time: 0:00:36.649945
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:46:24.255104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.44
 ---- batch: 020 ----
mean loss: 45.88
 ---- batch: 030 ----
mean loss: 48.06
 ---- batch: 040 ----
mean loss: 46.11
train mean loss: 46.48
epoch train time: 0:00:00.204229
elapsed time: 0:00:36.854292
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:46:24.459445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.14
 ---- batch: 020 ----
mean loss: 47.53
 ---- batch: 030 ----
mean loss: 46.01
 ---- batch: 040 ----
mean loss: 47.07
train mean loss: 47.30
epoch train time: 0:00:00.199710
elapsed time: 0:00:37.054115
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:46:24.659270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.27
 ---- batch: 020 ----
mean loss: 47.72
 ---- batch: 030 ----
mean loss: 45.87
 ---- batch: 040 ----
mean loss: 45.12
train mean loss: 46.79
epoch train time: 0:00:00.200870
elapsed time: 0:00:37.255104
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:46:24.860258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.55
 ---- batch: 020 ----
mean loss: 47.42
 ---- batch: 030 ----
mean loss: 45.21
 ---- batch: 040 ----
mean loss: 43.98
train mean loss: 45.83
epoch train time: 0:00:00.200856
elapsed time: 0:00:37.456074
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:46:25.061228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.64
 ---- batch: 020 ----
mean loss: 43.92
 ---- batch: 030 ----
mean loss: 47.58
 ---- batch: 040 ----
mean loss: 46.35
train mean loss: 45.96
epoch train time: 0:00:00.201200
elapsed time: 0:00:37.657391
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:46:25.262544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.85
 ---- batch: 020 ----
mean loss: 48.79
 ---- batch: 030 ----
mean loss: 47.16
 ---- batch: 040 ----
mean loss: 47.75
train mean loss: 47.56
epoch train time: 0:00:00.204478
elapsed time: 0:00:37.861997
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:46:25.467161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.14
 ---- batch: 020 ----
mean loss: 45.89
 ---- batch: 030 ----
mean loss: 44.84
 ---- batch: 040 ----
mean loss: 50.35
train mean loss: 46.53
epoch train time: 0:00:00.200646
elapsed time: 0:00:38.062780
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:46:25.667935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.53
 ---- batch: 020 ----
mean loss: 49.44
 ---- batch: 030 ----
mean loss: 45.68
 ---- batch: 040 ----
mean loss: 44.45
train mean loss: 47.65
epoch train time: 0:00:00.200548
elapsed time: 0:00:38.263441
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:46:25.868592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.79
 ---- batch: 020 ----
mean loss: 44.43
 ---- batch: 030 ----
mean loss: 45.65
 ---- batch: 040 ----
mean loss: 46.69
train mean loss: 45.85
epoch train time: 0:00:00.198545
elapsed time: 0:00:38.462098
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:46:26.067277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.87
 ---- batch: 020 ----
mean loss: 45.99
 ---- batch: 030 ----
mean loss: 47.31
 ---- batch: 040 ----
mean loss: 44.13
train mean loss: 45.81
epoch train time: 0:00:00.196704
elapsed time: 0:00:38.658941
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:46:26.264112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.80
 ---- batch: 020 ----
mean loss: 46.17
 ---- batch: 030 ----
mean loss: 46.43
 ---- batch: 040 ----
mean loss: 45.31
train mean loss: 45.32
epoch train time: 0:00:00.196574
elapsed time: 0:00:38.855679
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:46:26.460893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.41
 ---- batch: 020 ----
mean loss: 46.22
 ---- batch: 030 ----
mean loss: 42.68
 ---- batch: 040 ----
mean loss: 47.84
train mean loss: 44.91
epoch train time: 0:00:00.196531
elapsed time: 0:00:39.052382
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:46:26.657537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.05
 ---- batch: 020 ----
mean loss: 44.88
 ---- batch: 030 ----
mean loss: 45.13
 ---- batch: 040 ----
mean loss: 45.00
train mean loss: 44.69
epoch train time: 0:00:00.194724
elapsed time: 0:00:39.247235
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:46:26.852406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.95
 ---- batch: 020 ----
mean loss: 44.62
 ---- batch: 030 ----
mean loss: 45.30
 ---- batch: 040 ----
mean loss: 42.83
train mean loss: 44.31
epoch train time: 0:00:00.194987
elapsed time: 0:00:39.442350
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:46:27.047502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.36
 ---- batch: 020 ----
mean loss: 42.90
 ---- batch: 030 ----
mean loss: 42.93
 ---- batch: 040 ----
mean loss: 43.52
train mean loss: 44.15
epoch train time: 0:00:00.191552
elapsed time: 0:00:39.634012
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:46:27.239164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.13
 ---- batch: 020 ----
mean loss: 43.54
 ---- batch: 030 ----
mean loss: 46.96
 ---- batch: 040 ----
mean loss: 43.34
train mean loss: 44.03
epoch train time: 0:00:00.191820
elapsed time: 0:00:39.825938
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:46:27.431089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.85
 ---- batch: 020 ----
mean loss: 45.07
 ---- batch: 030 ----
mean loss: 44.38
 ---- batch: 040 ----
mean loss: 43.58
train mean loss: 44.88
epoch train time: 0:00:00.188040
elapsed time: 0:00:40.014105
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:46:27.619249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.42
 ---- batch: 020 ----
mean loss: 42.79
 ---- batch: 030 ----
mean loss: 42.80
 ---- batch: 040 ----
mean loss: 44.02
train mean loss: 43.96
epoch train time: 0:00:00.209011
elapsed time: 0:00:40.223237
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:46:27.828445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.76
 ---- batch: 020 ----
mean loss: 43.85
 ---- batch: 030 ----
mean loss: 42.63
 ---- batch: 040 ----
mean loss: 44.78
train mean loss: 43.62
epoch train time: 0:00:00.204615
elapsed time: 0:00:40.428029
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:46:28.033183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.72
 ---- batch: 020 ----
mean loss: 43.58
 ---- batch: 030 ----
mean loss: 45.40
 ---- batch: 040 ----
mean loss: 42.20
train mean loss: 44.15
epoch train time: 0:00:00.202610
elapsed time: 0:00:40.630756
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:46:28.235911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.71
 ---- batch: 020 ----
mean loss: 43.87
 ---- batch: 030 ----
mean loss: 41.44
 ---- batch: 040 ----
mean loss: 44.66
train mean loss: 43.49
epoch train time: 0:00:00.202020
elapsed time: 0:00:40.832895
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:46:28.438061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.58
 ---- batch: 020 ----
mean loss: 45.51
 ---- batch: 030 ----
mean loss: 42.54
 ---- batch: 040 ----
mean loss: 43.01
train mean loss: 44.48
epoch train time: 0:00:00.201232
elapsed time: 0:00:41.034255
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:46:28.639410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.60
 ---- batch: 020 ----
mean loss: 44.71
 ---- batch: 030 ----
mean loss: 42.51
 ---- batch: 040 ----
mean loss: 40.80
train mean loss: 43.34
epoch train time: 0:00:00.209634
elapsed time: 0:00:41.244005
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:46:28.849158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.48
 ---- batch: 020 ----
mean loss: 42.18
 ---- batch: 030 ----
mean loss: 43.67
 ---- batch: 040 ----
mean loss: 43.20
train mean loss: 42.57
epoch train time: 0:00:00.206533
elapsed time: 0:00:41.450675
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:46:29.055861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.11
 ---- batch: 020 ----
mean loss: 44.17
 ---- batch: 030 ----
mean loss: 43.34
 ---- batch: 040 ----
mean loss: 42.39
train mean loss: 43.26
epoch train time: 0:00:00.203305
elapsed time: 0:00:41.654139
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:46:29.259298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.91
 ---- batch: 020 ----
mean loss: 42.50
 ---- batch: 030 ----
mean loss: 42.93
 ---- batch: 040 ----
mean loss: 43.48
train mean loss: 42.67
epoch train time: 0:00:00.208442
elapsed time: 0:00:41.862704
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:46:29.467858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.48
 ---- batch: 020 ----
mean loss: 43.32
 ---- batch: 030 ----
mean loss: 42.93
 ---- batch: 040 ----
mean loss: 41.18
train mean loss: 42.44
epoch train time: 0:00:00.205161
elapsed time: 0:00:42.067987
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:46:29.673144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.64
 ---- batch: 020 ----
mean loss: 42.78
 ---- batch: 030 ----
mean loss: 44.40
 ---- batch: 040 ----
mean loss: 42.82
train mean loss: 42.88
epoch train time: 0:00:00.205550
elapsed time: 0:00:42.273761
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:46:29.878917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.22
 ---- batch: 020 ----
mean loss: 46.99
 ---- batch: 030 ----
mean loss: 43.85
 ---- batch: 040 ----
mean loss: 42.12
train mean loss: 43.66
epoch train time: 0:00:00.199237
elapsed time: 0:00:42.473114
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:46:30.078267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.48
 ---- batch: 020 ----
mean loss: 41.30
 ---- batch: 030 ----
mean loss: 41.36
 ---- batch: 040 ----
mean loss: 41.51
train mean loss: 41.87
epoch train time: 0:00:00.199058
elapsed time: 0:00:42.672287
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:46:30.277484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.94
 ---- batch: 020 ----
mean loss: 39.06
 ---- batch: 030 ----
mean loss: 40.90
 ---- batch: 040 ----
mean loss: 42.58
train mean loss: 41.38
epoch train time: 0:00:00.200814
elapsed time: 0:00:42.873269
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:46:30.478453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.39
 ---- batch: 020 ----
mean loss: 42.36
 ---- batch: 030 ----
mean loss: 44.14
 ---- batch: 040 ----
mean loss: 42.18
train mean loss: 41.93
epoch train time: 0:00:00.197862
elapsed time: 0:00:43.071312
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:46:30.676484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.59
 ---- batch: 020 ----
mean loss: 43.59
 ---- batch: 030 ----
mean loss: 42.47
 ---- batch: 040 ----
mean loss: 41.20
train mean loss: 41.63
epoch train time: 0:00:00.198971
elapsed time: 0:00:43.270428
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:46:30.875582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.55
 ---- batch: 020 ----
mean loss: 43.49
 ---- batch: 030 ----
mean loss: 42.08
 ---- batch: 040 ----
mean loss: 41.80
train mean loss: 42.13
epoch train time: 0:00:00.199197
elapsed time: 0:00:43.469743
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:46:31.074905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.56
 ---- batch: 020 ----
mean loss: 42.93
 ---- batch: 030 ----
mean loss: 41.42
 ---- batch: 040 ----
mean loss: 40.33
train mean loss: 41.17
epoch train time: 0:00:00.198137
elapsed time: 0:00:43.668010
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:46:31.273163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.25
 ---- batch: 020 ----
mean loss: 38.37
 ---- batch: 030 ----
mean loss: 41.13
 ---- batch: 040 ----
mean loss: 41.80
train mean loss: 40.84
epoch train time: 0:00:00.210247
elapsed time: 0:00:43.878402
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:46:31.483589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.88
 ---- batch: 020 ----
mean loss: 43.13
 ---- batch: 030 ----
mean loss: 40.40
 ---- batch: 040 ----
mean loss: 42.40
train mean loss: 41.71
epoch train time: 0:00:00.197395
elapsed time: 0:00:44.075946
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:46:31.681099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.31
 ---- batch: 020 ----
mean loss: 40.02
 ---- batch: 030 ----
mean loss: 42.35
 ---- batch: 040 ----
mean loss: 40.70
train mean loss: 41.48
epoch train time: 0:00:00.203354
elapsed time: 0:00:44.279414
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:46:31.884569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.45
 ---- batch: 020 ----
mean loss: 43.66
 ---- batch: 030 ----
mean loss: 39.83
 ---- batch: 040 ----
mean loss: 38.79
train mean loss: 40.49
epoch train time: 0:00:00.201048
elapsed time: 0:00:44.480593
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:46:32.085740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.99
 ---- batch: 020 ----
mean loss: 40.26
 ---- batch: 030 ----
mean loss: 41.46
 ---- batch: 040 ----
mean loss: 40.91
train mean loss: 40.72
epoch train time: 0:00:00.201977
elapsed time: 0:00:44.682678
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:46:32.287840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.95
 ---- batch: 020 ----
mean loss: 41.38
 ---- batch: 030 ----
mean loss: 41.49
 ---- batch: 040 ----
mean loss: 42.34
train mean loss: 41.05
epoch train time: 0:00:00.204179
elapsed time: 0:00:44.887032
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:46:32.492196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.87
 ---- batch: 020 ----
mean loss: 40.53
 ---- batch: 030 ----
mean loss: 39.77
 ---- batch: 040 ----
mean loss: 39.35
train mean loss: 39.88
epoch train time: 0:00:00.200648
elapsed time: 0:00:45.087809
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:46:32.692963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.98
 ---- batch: 020 ----
mean loss: 39.93
 ---- batch: 030 ----
mean loss: 41.06
 ---- batch: 040 ----
mean loss: 40.07
train mean loss: 40.19
epoch train time: 0:00:00.203489
elapsed time: 0:00:45.291421
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:46:32.896576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.69
 ---- batch: 020 ----
mean loss: 40.95
 ---- batch: 030 ----
mean loss: 36.77
 ---- batch: 040 ----
mean loss: 39.30
train mean loss: 39.70
epoch train time: 0:00:00.205167
elapsed time: 0:00:45.496707
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:46:33.101863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.60
 ---- batch: 020 ----
mean loss: 39.68
 ---- batch: 030 ----
mean loss: 39.27
 ---- batch: 040 ----
mean loss: 40.35
train mean loss: 39.86
epoch train time: 0:00:00.205976
elapsed time: 0:00:45.702845
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:46:33.308000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.21
 ---- batch: 020 ----
mean loss: 40.20
 ---- batch: 030 ----
mean loss: 40.98
 ---- batch: 040 ----
mean loss: 38.62
train mean loss: 39.73
epoch train time: 0:00:00.204340
elapsed time: 0:00:45.907337
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:46:33.512497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.03
 ---- batch: 020 ----
mean loss: 40.60
 ---- batch: 030 ----
mean loss: 40.22
 ---- batch: 040 ----
mean loss: 39.28
train mean loss: 39.49
epoch train time: 0:00:00.212371
elapsed time: 0:00:46.119840
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:46:33.725013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.86
 ---- batch: 020 ----
mean loss: 40.43
 ---- batch: 030 ----
mean loss: 39.73
 ---- batch: 040 ----
mean loss: 40.16
train mean loss: 39.33
epoch train time: 0:00:00.208445
elapsed time: 0:00:46.328428
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:46:33.933618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.34
 ---- batch: 020 ----
mean loss: 39.06
 ---- batch: 030 ----
mean loss: 39.77
 ---- batch: 040 ----
mean loss: 38.55
train mean loss: 39.18
epoch train time: 0:00:00.204223
elapsed time: 0:00:46.532801
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:46:34.137951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.80
 ---- batch: 020 ----
mean loss: 37.91
 ---- batch: 030 ----
mean loss: 39.49
 ---- batch: 040 ----
mean loss: 42.66
train mean loss: 39.47
epoch train time: 0:00:00.198740
elapsed time: 0:00:46.731659
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:46:34.336812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.09
 ---- batch: 020 ----
mean loss: 38.06
 ---- batch: 030 ----
mean loss: 38.60
 ---- batch: 040 ----
mean loss: 39.48
train mean loss: 38.70
epoch train time: 0:00:00.196310
elapsed time: 0:00:46.928078
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:46:34.533229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.92
 ---- batch: 020 ----
mean loss: 36.74
 ---- batch: 030 ----
mean loss: 38.87
 ---- batch: 040 ----
mean loss: 39.31
train mean loss: 38.48
epoch train time: 0:00:00.196023
elapsed time: 0:00:47.124209
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:46:34.729360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.44
 ---- batch: 020 ----
mean loss: 39.21
 ---- batch: 030 ----
mean loss: 36.99
 ---- batch: 040 ----
mean loss: 39.79
train mean loss: 38.14
epoch train time: 0:00:00.202647
elapsed time: 0:00:47.326968
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:46:34.932120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.41
 ---- batch: 020 ----
mean loss: 37.27
 ---- batch: 030 ----
mean loss: 37.90
 ---- batch: 040 ----
mean loss: 39.45
train mean loss: 37.91
epoch train time: 0:00:00.195905
elapsed time: 0:00:47.522988
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:46:35.128141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.76
 ---- batch: 020 ----
mean loss: 39.01
 ---- batch: 030 ----
mean loss: 38.76
 ---- batch: 040 ----
mean loss: 37.94
train mean loss: 38.09
epoch train time: 0:00:00.198085
elapsed time: 0:00:47.721191
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:46:35.326345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.67
 ---- batch: 020 ----
mean loss: 38.25
 ---- batch: 030 ----
mean loss: 37.60
 ---- batch: 040 ----
mean loss: 39.93
train mean loss: 38.58
epoch train time: 0:00:00.198704
elapsed time: 0:00:47.920018
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:46:35.525173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.12
 ---- batch: 020 ----
mean loss: 38.16
 ---- batch: 030 ----
mean loss: 38.88
 ---- batch: 040 ----
mean loss: 35.92
train mean loss: 37.89
epoch train time: 0:00:00.202289
elapsed time: 0:00:48.122421
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:46:35.727571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.45
 ---- batch: 020 ----
mean loss: 38.96
 ---- batch: 030 ----
mean loss: 37.08
 ---- batch: 040 ----
mean loss: 36.15
train mean loss: 37.45
epoch train time: 0:00:00.200485
elapsed time: 0:00:48.323030
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:46:35.928183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.50
 ---- batch: 020 ----
mean loss: 38.31
 ---- batch: 030 ----
mean loss: 42.10
 ---- batch: 040 ----
mean loss: 36.80
train mean loss: 38.74
epoch train time: 0:00:00.205336
elapsed time: 0:00:48.528505
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:46:36.133668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.44
 ---- batch: 020 ----
mean loss: 36.09
 ---- batch: 030 ----
mean loss: 40.36
 ---- batch: 040 ----
mean loss: 38.76
train mean loss: 37.87
epoch train time: 0:00:00.207409
elapsed time: 0:00:48.736036
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:46:36.341207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.19
 ---- batch: 020 ----
mean loss: 36.26
 ---- batch: 030 ----
mean loss: 37.54
 ---- batch: 040 ----
mean loss: 37.13
train mean loss: 37.17
epoch train time: 0:00:00.206562
elapsed time: 0:00:48.942736
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:46:36.547892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.70
 ---- batch: 020 ----
mean loss: 35.41
 ---- batch: 030 ----
mean loss: 38.87
 ---- batch: 040 ----
mean loss: 38.17
train mean loss: 37.13
epoch train time: 0:00:00.203493
elapsed time: 0:00:49.146346
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:46:36.751499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.09
 ---- batch: 020 ----
mean loss: 39.50
 ---- batch: 030 ----
mean loss: 37.68
 ---- batch: 040 ----
mean loss: 39.69
train mean loss: 38.58
epoch train time: 0:00:00.206949
elapsed time: 0:00:49.353432
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:46:36.958585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.47
 ---- batch: 020 ----
mean loss: 39.98
 ---- batch: 030 ----
mean loss: 37.49
 ---- batch: 040 ----
mean loss: 37.38
train mean loss: 37.75
epoch train time: 0:00:00.207781
elapsed time: 0:00:49.561344
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:46:37.166493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.48
 ---- batch: 020 ----
mean loss: 36.55
 ---- batch: 030 ----
mean loss: 37.14
 ---- batch: 040 ----
mean loss: 36.50
train mean loss: 36.84
epoch train time: 0:00:00.208118
elapsed time: 0:00:49.769578
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:46:37.374734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.45
 ---- batch: 020 ----
mean loss: 38.45
 ---- batch: 030 ----
mean loss: 37.15
 ---- batch: 040 ----
mean loss: 36.56
train mean loss: 37.49
epoch train time: 0:00:00.206976
elapsed time: 0:00:49.976674
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:46:37.581828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.57
 ---- batch: 020 ----
mean loss: 36.79
 ---- batch: 030 ----
mean loss: 36.83
 ---- batch: 040 ----
mean loss: 38.86
train mean loss: 36.89
epoch train time: 0:00:00.204101
elapsed time: 0:00:50.180907
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:46:37.786062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.34
 ---- batch: 020 ----
mean loss: 34.88
 ---- batch: 030 ----
mean loss: 34.77
 ---- batch: 040 ----
mean loss: 37.42
train mean loss: 36.31
epoch train time: 0:00:00.206292
elapsed time: 0:00:50.387338
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:46:37.992495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.96
 ---- batch: 020 ----
mean loss: 37.91
 ---- batch: 030 ----
mean loss: 35.28
 ---- batch: 040 ----
mean loss: 37.20
train mean loss: 37.28
epoch train time: 0:00:00.203983
elapsed time: 0:00:50.591444
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:46:38.196600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.92
 ---- batch: 020 ----
mean loss: 42.58
 ---- batch: 030 ----
mean loss: 36.20
 ---- batch: 040 ----
mean loss: 35.29
train mean loss: 38.48
epoch train time: 0:00:00.196192
elapsed time: 0:00:50.787753
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:46:38.392906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.70
 ---- batch: 020 ----
mean loss: 38.22
 ---- batch: 030 ----
mean loss: 35.88
 ---- batch: 040 ----
mean loss: 35.06
train mean loss: 36.38
epoch train time: 0:00:00.196222
elapsed time: 0:00:50.984129
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:46:38.589286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.26
 ---- batch: 020 ----
mean loss: 34.48
 ---- batch: 030 ----
mean loss: 35.31
 ---- batch: 040 ----
mean loss: 36.03
train mean loss: 35.62
epoch train time: 0:00:00.196335
elapsed time: 0:00:51.180598
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:46:38.785779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.33
 ---- batch: 020 ----
mean loss: 35.06
 ---- batch: 030 ----
mean loss: 33.39
 ---- batch: 040 ----
mean loss: 36.89
train mean loss: 35.49
epoch train time: 0:00:00.195189
elapsed time: 0:00:51.375932
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:46:38.981084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.75
 ---- batch: 020 ----
mean loss: 36.22
 ---- batch: 030 ----
mean loss: 38.81
 ---- batch: 040 ----
mean loss: 36.75
train mean loss: 36.73
epoch train time: 0:00:00.189370
elapsed time: 0:00:51.565411
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:46:39.170563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.78
 ---- batch: 020 ----
mean loss: 37.52
 ---- batch: 030 ----
mean loss: 34.36
 ---- batch: 040 ----
mean loss: 37.48
train mean loss: 35.99
epoch train time: 0:00:00.190826
elapsed time: 0:00:51.756359
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:46:39.361524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.66
 ---- batch: 020 ----
mean loss: 33.71
 ---- batch: 030 ----
mean loss: 38.48
 ---- batch: 040 ----
mean loss: 35.60
train mean loss: 35.85
epoch train time: 0:00:00.191160
elapsed time: 0:00:51.947646
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:46:39.552799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.46
 ---- batch: 020 ----
mean loss: 36.14
 ---- batch: 030 ----
mean loss: 36.68
 ---- batch: 040 ----
mean loss: 30.99
train mean loss: 35.27
epoch train time: 0:00:00.189623
elapsed time: 0:00:52.137399
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:46:39.742550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.97
 ---- batch: 020 ----
mean loss: 38.35
 ---- batch: 030 ----
mean loss: 36.79
 ---- batch: 040 ----
mean loss: 37.24
train mean loss: 36.47
epoch train time: 0:00:00.195864
elapsed time: 0:00:52.333396
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:46:39.938547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.95
 ---- batch: 020 ----
mean loss: 34.64
 ---- batch: 030 ----
mean loss: 34.88
 ---- batch: 040 ----
mean loss: 36.66
train mean loss: 35.16
epoch train time: 0:00:00.200759
elapsed time: 0:00:52.534270
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:46:40.139427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.40
 ---- batch: 020 ----
mean loss: 34.75
 ---- batch: 030 ----
mean loss: 36.25
 ---- batch: 040 ----
mean loss: 37.78
train mean loss: 35.43
epoch train time: 0:00:00.208154
elapsed time: 0:00:52.742544
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:46:40.347698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.27
 ---- batch: 020 ----
mean loss: 36.51
 ---- batch: 030 ----
mean loss: 33.05
 ---- batch: 040 ----
mean loss: 35.93
train mean loss: 35.02
epoch train time: 0:00:00.208374
elapsed time: 0:00:52.951036
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:46:40.556189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.05
 ---- batch: 020 ----
mean loss: 32.99
 ---- batch: 030 ----
mean loss: 35.41
 ---- batch: 040 ----
mean loss: 33.80
train mean loss: 34.21
epoch train time: 0:00:00.202534
elapsed time: 0:00:53.153694
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:46:40.758849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.27
 ---- batch: 020 ----
mean loss: 33.73
 ---- batch: 030 ----
mean loss: 36.47
 ---- batch: 040 ----
mean loss: 33.49
train mean loss: 35.20
epoch train time: 0:00:00.214421
elapsed time: 0:00:53.368234
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:46:40.973388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.77
 ---- batch: 020 ----
mean loss: 33.98
 ---- batch: 030 ----
mean loss: 37.22
 ---- batch: 040 ----
mean loss: 34.55
train mean loss: 35.33
epoch train time: 0:00:00.206302
elapsed time: 0:00:53.574664
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:46:41.179837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.79
 ---- batch: 020 ----
mean loss: 37.19
 ---- batch: 030 ----
mean loss: 34.49
 ---- batch: 040 ----
mean loss: 33.93
train mean loss: 35.53
epoch train time: 0:00:00.206048
elapsed time: 0:00:53.780848
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:46:41.386003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.05
 ---- batch: 020 ----
mean loss: 34.05
 ---- batch: 030 ----
mean loss: 36.05
 ---- batch: 040 ----
mean loss: 36.35
train mean loss: 34.92
epoch train time: 0:00:00.206282
elapsed time: 0:00:53.987249
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:46:41.592428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.43
 ---- batch: 020 ----
mean loss: 35.88
 ---- batch: 030 ----
mean loss: 35.35
 ---- batch: 040 ----
mean loss: 37.00
train mean loss: 35.64
epoch train time: 0:00:00.205572
elapsed time: 0:00:54.192966
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:46:41.798150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.66
 ---- batch: 020 ----
mean loss: 34.93
 ---- batch: 030 ----
mean loss: 34.42
 ---- batch: 040 ----
mean loss: 33.21
train mean loss: 34.78
epoch train time: 0:00:00.206548
elapsed time: 0:00:54.399665
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:46:42.004819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.34
 ---- batch: 020 ----
mean loss: 33.20
 ---- batch: 030 ----
mean loss: 33.99
 ---- batch: 040 ----
mean loss: 34.37
train mean loss: 33.57
epoch train time: 0:00:00.202429
elapsed time: 0:00:54.602211
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:46:42.207365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 32.20
 ---- batch: 020 ----
mean loss: 33.35
 ---- batch: 030 ----
mean loss: 34.20
 ---- batch: 040 ----
mean loss: 34.65
train mean loss: 33.71
epoch train time: 0:00:00.200413
elapsed time: 0:00:54.802743
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:46:42.407897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.44
 ---- batch: 020 ----
mean loss: 32.96
 ---- batch: 030 ----
mean loss: 33.83
 ---- batch: 040 ----
mean loss: 34.72
train mean loss: 33.91
epoch train time: 0:00:00.193963
elapsed time: 0:00:54.996816
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:46:42.601998
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.74
 ---- batch: 020 ----
mean loss: 31.63
 ---- batch: 030 ----
mean loss: 32.96
 ---- batch: 040 ----
mean loss: 33.54
train mean loss: 32.36
epoch train time: 0:00:00.196537
elapsed time: 0:00:55.193537
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:46:42.798715
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.99
 ---- batch: 020 ----
mean loss: 33.28
 ---- batch: 030 ----
mean loss: 32.26
 ---- batch: 040 ----
mean loss: 31.75
train mean loss: 32.14
epoch train time: 0:00:00.197470
elapsed time: 0:00:55.391153
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:46:42.996306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.22
 ---- batch: 020 ----
mean loss: 32.50
 ---- batch: 030 ----
mean loss: 30.91
 ---- batch: 040 ----
mean loss: 31.53
train mean loss: 32.07
epoch train time: 0:00:00.197482
elapsed time: 0:00:55.588748
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:46:43.193902
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.33
 ---- batch: 020 ----
mean loss: 32.31
 ---- batch: 030 ----
mean loss: 31.62
 ---- batch: 040 ----
mean loss: 33.44
train mean loss: 32.06
epoch train time: 0:00:00.193813
elapsed time: 0:00:55.782672
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:46:43.387823
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.78
 ---- batch: 020 ----
mean loss: 31.66
 ---- batch: 030 ----
mean loss: 32.71
 ---- batch: 040 ----
mean loss: 32.21
train mean loss: 32.08
epoch train time: 0:00:00.191933
elapsed time: 0:00:55.974715
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:46:43.579882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.54
 ---- batch: 020 ----
mean loss: 31.30
 ---- batch: 030 ----
mean loss: 32.77
 ---- batch: 040 ----
mean loss: 31.32
train mean loss: 32.10
epoch train time: 0:00:00.194415
elapsed time: 0:00:56.169260
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:46:43.774432
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.41
 ---- batch: 020 ----
mean loss: 31.37
 ---- batch: 030 ----
mean loss: 31.22
 ---- batch: 040 ----
mean loss: 32.26
train mean loss: 32.18
epoch train time: 0:00:00.198940
elapsed time: 0:00:56.368346
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:46:43.973498
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.89
 ---- batch: 020 ----
mean loss: 32.06
 ---- batch: 030 ----
mean loss: 30.87
 ---- batch: 040 ----
mean loss: 30.87
train mean loss: 31.97
epoch train time: 0:00:00.198495
elapsed time: 0:00:56.566956
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:46:44.172111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.53
 ---- batch: 020 ----
mean loss: 31.50
 ---- batch: 030 ----
mean loss: 30.64
 ---- batch: 040 ----
mean loss: 34.53
train mean loss: 32.03
epoch train time: 0:00:00.206783
elapsed time: 0:00:56.773859
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:46:44.379016
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.45
 ---- batch: 020 ----
mean loss: 33.33
 ---- batch: 030 ----
mean loss: 32.25
 ---- batch: 040 ----
mean loss: 30.46
train mean loss: 32.01
epoch train time: 0:00:00.207128
elapsed time: 0:00:56.981118
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:46:44.586276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.70
 ---- batch: 020 ----
mean loss: 31.45
 ---- batch: 030 ----
mean loss: 31.43
 ---- batch: 040 ----
mean loss: 31.35
train mean loss: 31.97
epoch train time: 0:00:00.202310
elapsed time: 0:00:57.183551
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:46:44.788706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.93
 ---- batch: 020 ----
mean loss: 30.90
 ---- batch: 030 ----
mean loss: 30.97
 ---- batch: 040 ----
mean loss: 33.33
train mean loss: 31.95
epoch train time: 0:00:00.201267
elapsed time: 0:00:57.384931
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:46:44.990083
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.48
 ---- batch: 020 ----
mean loss: 29.48
 ---- batch: 030 ----
mean loss: 31.05
 ---- batch: 040 ----
mean loss: 32.92
train mean loss: 31.97
epoch train time: 0:00:00.200424
elapsed time: 0:00:57.585482
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:46:45.190636
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.46
 ---- batch: 020 ----
mean loss: 31.74
 ---- batch: 030 ----
mean loss: 32.45
 ---- batch: 040 ----
mean loss: 32.35
train mean loss: 31.91
epoch train time: 0:00:00.201153
elapsed time: 0:00:57.786763
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:46:45.391915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.56
 ---- batch: 020 ----
mean loss: 31.40
 ---- batch: 030 ----
mean loss: 32.55
 ---- batch: 040 ----
mean loss: 32.26
train mean loss: 31.90
epoch train time: 0:00:00.203368
elapsed time: 0:00:57.990264
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:46:45.595422
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.66
 ---- batch: 020 ----
mean loss: 31.36
 ---- batch: 030 ----
mean loss: 32.06
 ---- batch: 040 ----
mean loss: 29.74
train mean loss: 31.97
epoch train time: 0:00:00.202896
elapsed time: 0:00:58.193281
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:46:45.798437
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.28
 ---- batch: 020 ----
mean loss: 31.80
 ---- batch: 030 ----
mean loss: 32.36
 ---- batch: 040 ----
mean loss: 31.17
train mean loss: 31.93
epoch train time: 0:00:00.203378
elapsed time: 0:00:58.396779
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:46:46.001951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.69
 ---- batch: 020 ----
mean loss: 33.80
 ---- batch: 030 ----
mean loss: 31.06
 ---- batch: 040 ----
mean loss: 32.54
train mean loss: 31.85
epoch train time: 0:00:00.203286
elapsed time: 0:00:58.600204
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:46:46.205362
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.23
 ---- batch: 020 ----
mean loss: 32.51
 ---- batch: 030 ----
mean loss: 31.62
 ---- batch: 040 ----
mean loss: 30.54
train mean loss: 31.91
epoch train time: 0:00:00.202930
elapsed time: 0:00:58.803270
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:46:46.408448
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.65
 ---- batch: 020 ----
mean loss: 31.87
 ---- batch: 030 ----
mean loss: 32.81
 ---- batch: 040 ----
mean loss: 31.90
train mean loss: 31.84
epoch train time: 0:00:00.193689
elapsed time: 0:00:58.997101
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:46:46.602286
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.23
 ---- batch: 020 ----
mean loss: 31.91
 ---- batch: 030 ----
mean loss: 31.61
 ---- batch: 040 ----
mean loss: 32.49
train mean loss: 31.88
epoch train time: 0:00:00.194205
elapsed time: 0:00:59.191492
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:46:46.796677
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.91
 ---- batch: 020 ----
mean loss: 32.84
 ---- batch: 030 ----
mean loss: 31.41
 ---- batch: 040 ----
mean loss: 32.13
train mean loss: 31.90
epoch train time: 0:00:00.195139
elapsed time: 0:00:59.386783
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:46:46.991936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.78
 ---- batch: 020 ----
mean loss: 30.91
 ---- batch: 030 ----
mean loss: 31.52
 ---- batch: 040 ----
mean loss: 32.85
train mean loss: 31.79
epoch train time: 0:00:00.195808
elapsed time: 0:00:59.582707
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:46:47.187876
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.65
 ---- batch: 020 ----
mean loss: 32.48
 ---- batch: 030 ----
mean loss: 31.27
 ---- batch: 040 ----
mean loss: 32.05
train mean loss: 31.84
epoch train time: 0:00:00.192986
elapsed time: 0:00:59.775822
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:46:47.380973
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.39
 ---- batch: 020 ----
mean loss: 32.23
 ---- batch: 030 ----
mean loss: 31.84
 ---- batch: 040 ----
mean loss: 31.34
train mean loss: 31.81
epoch train time: 0:00:00.190368
elapsed time: 0:00:59.966311
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:46:47.571483
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.68
 ---- batch: 020 ----
mean loss: 32.08
 ---- batch: 030 ----
mean loss: 32.14
 ---- batch: 040 ----
mean loss: 32.07
train mean loss: 31.79
epoch train time: 0:00:00.197849
elapsed time: 0:01:00.164292
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:46:47.769444
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.13
 ---- batch: 020 ----
mean loss: 32.55
 ---- batch: 030 ----
mean loss: 31.83
 ---- batch: 040 ----
mean loss: 30.83
train mean loss: 31.68
epoch train time: 0:00:00.193310
elapsed time: 0:01:00.357711
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:46:47.962877
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.26
 ---- batch: 020 ----
mean loss: 30.16
 ---- batch: 030 ----
mean loss: 33.13
 ---- batch: 040 ----
mean loss: 31.98
train mean loss: 31.78
epoch train time: 0:00:00.195178
elapsed time: 0:01:00.553012
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:46:48.158166
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.57
 ---- batch: 020 ----
mean loss: 32.71
 ---- batch: 030 ----
mean loss: 32.28
 ---- batch: 040 ----
mean loss: 32.59
train mean loss: 31.72
epoch train time: 0:00:00.197440
elapsed time: 0:01:00.750568
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:46:48.355735
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.17
 ---- batch: 020 ----
mean loss: 32.08
 ---- batch: 030 ----
mean loss: 31.51
 ---- batch: 040 ----
mean loss: 30.80
train mean loss: 31.73
epoch train time: 0:00:00.202565
elapsed time: 0:01:00.953278
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:46:48.558434
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.63
 ---- batch: 020 ----
mean loss: 29.55
 ---- batch: 030 ----
mean loss: 31.55
 ---- batch: 040 ----
mean loss: 32.78
train mean loss: 31.83
epoch train time: 0:00:00.202820
elapsed time: 0:01:01.156218
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:46:48.761372
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.01
 ---- batch: 020 ----
mean loss: 31.83
 ---- batch: 030 ----
mean loss: 32.62
 ---- batch: 040 ----
mean loss: 29.61
train mean loss: 31.67
epoch train time: 0:00:00.214533
elapsed time: 0:01:01.370866
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:46:48.976024
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.23
 ---- batch: 020 ----
mean loss: 33.74
 ---- batch: 030 ----
mean loss: 30.14
 ---- batch: 040 ----
mean loss: 30.83
train mean loss: 31.85
epoch train time: 0:00:00.201805
elapsed time: 0:01:01.572821
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:46:49.177970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.39
 ---- batch: 020 ----
mean loss: 31.90
 ---- batch: 030 ----
mean loss: 32.27
 ---- batch: 040 ----
mean loss: 31.81
train mean loss: 31.71
epoch train time: 0:00:00.202214
elapsed time: 0:01:01.775148
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:46:49.380306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.39
 ---- batch: 020 ----
mean loss: 30.92
 ---- batch: 030 ----
mean loss: 30.17
 ---- batch: 040 ----
mean loss: 32.09
train mean loss: 31.66
epoch train time: 0:00:00.202099
elapsed time: 0:01:01.977370
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:46:49.582526
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.23
 ---- batch: 020 ----
mean loss: 31.46
 ---- batch: 030 ----
mean loss: 31.89
 ---- batch: 040 ----
mean loss: 31.36
train mean loss: 31.52
epoch train time: 0:00:00.207894
elapsed time: 0:01:02.185398
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:46:49.790554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.31
 ---- batch: 020 ----
mean loss: 29.68
 ---- batch: 030 ----
mean loss: 32.32
 ---- batch: 040 ----
mean loss: 31.03
train mean loss: 31.55
epoch train time: 0:00:00.200576
elapsed time: 0:01:02.386091
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:46:49.991246
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.71
 ---- batch: 020 ----
mean loss: 31.84
 ---- batch: 030 ----
mean loss: 30.77
 ---- batch: 040 ----
mean loss: 31.56
train mean loss: 31.59
epoch train time: 0:00:00.200447
elapsed time: 0:01:02.586693
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:46:50.191882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.22
 ---- batch: 020 ----
mean loss: 31.55
 ---- batch: 030 ----
mean loss: 33.07
 ---- batch: 040 ----
mean loss: 32.40
train mean loss: 31.51
epoch train time: 0:00:00.200240
elapsed time: 0:01:02.787083
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:46:50.392238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.62
 ---- batch: 020 ----
mean loss: 32.24
 ---- batch: 030 ----
mean loss: 29.63
 ---- batch: 040 ----
mean loss: 32.05
train mean loss: 31.53
epoch train time: 0:00:00.196510
elapsed time: 0:01:02.983741
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:46:50.588895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.08
 ---- batch: 020 ----
mean loss: 31.20
 ---- batch: 030 ----
mean loss: 32.07
 ---- batch: 040 ----
mean loss: 32.40
train mean loss: 31.48
epoch train time: 0:00:00.191744
elapsed time: 0:01:03.175598
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:46:50.780763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.52
 ---- batch: 020 ----
mean loss: 31.59
 ---- batch: 030 ----
mean loss: 31.29
 ---- batch: 040 ----
mean loss: 31.31
train mean loss: 31.43
epoch train time: 0:00:00.204740
elapsed time: 0:01:03.380462
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:46:50.985646
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.85
 ---- batch: 020 ----
mean loss: 30.04
 ---- batch: 030 ----
mean loss: 32.06
 ---- batch: 040 ----
mean loss: 31.84
train mean loss: 31.51
epoch train time: 0:00:00.196704
elapsed time: 0:01:03.577315
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:46:51.182487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.17
 ---- batch: 020 ----
mean loss: 31.24
 ---- batch: 030 ----
mean loss: 32.23
 ---- batch: 040 ----
mean loss: 31.33
train mean loss: 31.41
epoch train time: 0:00:00.198718
elapsed time: 0:01:03.776182
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:46:51.381353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.81
 ---- batch: 020 ----
mean loss: 32.34
 ---- batch: 030 ----
mean loss: 31.31
 ---- batch: 040 ----
mean loss: 32.51
train mean loss: 31.46
epoch train time: 0:00:00.197372
elapsed time: 0:01:03.973685
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:46:51.578838
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.94
 ---- batch: 020 ----
mean loss: 30.04
 ---- batch: 030 ----
mean loss: 32.98
 ---- batch: 040 ----
mean loss: 33.21
train mean loss: 31.49
epoch train time: 0:00:00.194260
elapsed time: 0:01:04.168064
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:46:51.773217
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.10
 ---- batch: 020 ----
mean loss: 32.14
 ---- batch: 030 ----
mean loss: 30.24
 ---- batch: 040 ----
mean loss: 31.66
train mean loss: 31.46
epoch train time: 0:00:00.193997
elapsed time: 0:01:04.362201
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:46:51.967352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.48
 ---- batch: 020 ----
mean loss: 31.52
 ---- batch: 030 ----
mean loss: 31.62
 ---- batch: 040 ----
mean loss: 30.61
train mean loss: 31.48
epoch train time: 0:00:00.192638
elapsed time: 0:01:04.554978
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:46:52.160130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.31
 ---- batch: 020 ----
mean loss: 32.04
 ---- batch: 030 ----
mean loss: 30.82
 ---- batch: 040 ----
mean loss: 30.74
train mean loss: 31.37
epoch train time: 0:00:00.193706
elapsed time: 0:01:04.752098
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_6/checkpoint.pth.tar
**** end time: 2019-09-20 23:46:52.357227 ****
