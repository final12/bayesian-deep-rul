Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_5', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 23571
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-25 22:43:38.460739 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 16, 11]             560
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
            Conv2d-4            [-1, 14, 7, 11]             224
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
            Linear-8                    [-1, 1]             462
================================================================
Total params: 1,246
Trainable params: 1,246
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 22:43:38.466032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4221.45
 ---- batch: 020 ----
mean loss: 3950.52
 ---- batch: 030 ----
mean loss: 3810.38
 ---- batch: 040 ----
mean loss: 3549.46
 ---- batch: 050 ----
mean loss: 3254.22
 ---- batch: 060 ----
mean loss: 3094.45
 ---- batch: 070 ----
mean loss: 2792.53
 ---- batch: 080 ----
mean loss: 2594.40
 ---- batch: 090 ----
mean loss: 2359.28
train mean loss: 3221.91
epoch train time: 0:00:33.174953
elapsed time: 0:00:33.181574
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 22:44:11.642351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1975.53
 ---- batch: 020 ----
mean loss: 1840.79
 ---- batch: 030 ----
mean loss: 1663.07
 ---- batch: 040 ----
mean loss: 1518.42
 ---- batch: 050 ----
mean loss: 1361.61
 ---- batch: 060 ----
mean loss: 1279.51
 ---- batch: 070 ----
mean loss: 1189.95
 ---- batch: 080 ----
mean loss: 1117.77
 ---- batch: 090 ----
mean loss: 1086.23
train mean loss: 1421.20
epoch train time: 0:00:00.707911
elapsed time: 0:00:33.889679
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 22:44:12.350496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 996.86
 ---- batch: 020 ----
mean loss: 957.07
 ---- batch: 030 ----
mean loss: 945.84
 ---- batch: 040 ----
mean loss: 943.19
 ---- batch: 050 ----
mean loss: 929.22
 ---- batch: 060 ----
mean loss: 898.25
 ---- batch: 070 ----
mean loss: 906.20
 ---- batch: 080 ----
mean loss: 876.48
 ---- batch: 090 ----
mean loss: 892.48
train mean loss: 924.55
epoch train time: 0:00:00.694212
elapsed time: 0:00:34.584075
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 22:44:13.044875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.67
 ---- batch: 020 ----
mean loss: 873.82
 ---- batch: 030 ----
mean loss: 879.65
 ---- batch: 040 ----
mean loss: 886.96
 ---- batch: 050 ----
mean loss: 868.32
 ---- batch: 060 ----
mean loss: 872.44
 ---- batch: 070 ----
mean loss: 891.24
 ---- batch: 080 ----
mean loss: 885.36
 ---- batch: 090 ----
mean loss: 870.35
train mean loss: 880.47
epoch train time: 0:00:00.671225
elapsed time: 0:00:35.255460
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 22:44:13.716247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.27
 ---- batch: 020 ----
mean loss: 864.01
 ---- batch: 030 ----
mean loss: 881.12
 ---- batch: 040 ----
mean loss: 880.58
 ---- batch: 050 ----
mean loss: 860.86
 ---- batch: 060 ----
mean loss: 874.71
 ---- batch: 070 ----
mean loss: 890.01
 ---- batch: 080 ----
mean loss: 883.04
 ---- batch: 090 ----
mean loss: 865.43
train mean loss: 874.90
epoch train time: 0:00:00.683393
elapsed time: 0:00:35.939010
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 22:44:14.399803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.63
 ---- batch: 020 ----
mean loss: 874.18
 ---- batch: 030 ----
mean loss: 884.09
 ---- batch: 040 ----
mean loss: 873.55
 ---- batch: 050 ----
mean loss: 869.23
 ---- batch: 060 ----
mean loss: 848.54
 ---- batch: 070 ----
mean loss: 885.13
 ---- batch: 080 ----
mean loss: 875.07
 ---- batch: 090 ----
mean loss: 863.16
train mean loss: 872.63
epoch train time: 0:00:00.678075
elapsed time: 0:00:36.617251
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 22:44:15.078045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.35
 ---- batch: 020 ----
mean loss: 873.11
 ---- batch: 030 ----
mean loss: 880.44
 ---- batch: 040 ----
mean loss: 880.51
 ---- batch: 050 ----
mean loss: 878.16
 ---- batch: 060 ----
mean loss: 867.60
 ---- batch: 070 ----
mean loss: 864.89
 ---- batch: 080 ----
mean loss: 864.00
 ---- batch: 090 ----
mean loss: 860.68
train mean loss: 869.30
epoch train time: 0:00:00.678248
elapsed time: 0:00:37.295651
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 22:44:15.756442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.47
 ---- batch: 020 ----
mean loss: 871.17
 ---- batch: 030 ----
mean loss: 903.27
 ---- batch: 040 ----
mean loss: 867.80
 ---- batch: 050 ----
mean loss: 861.76
 ---- batch: 060 ----
mean loss: 857.73
 ---- batch: 070 ----
mean loss: 879.17
 ---- batch: 080 ----
mean loss: 860.48
 ---- batch: 090 ----
mean loss: 841.91
train mean loss: 864.77
epoch train time: 0:00:00.676626
elapsed time: 0:00:37.972426
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 22:44:16.433217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.61
 ---- batch: 020 ----
mean loss: 856.38
 ---- batch: 030 ----
mean loss: 865.23
 ---- batch: 040 ----
mean loss: 888.43
 ---- batch: 050 ----
mean loss: 846.12
 ---- batch: 060 ----
mean loss: 854.47
 ---- batch: 070 ----
mean loss: 853.15
 ---- batch: 080 ----
mean loss: 846.55
 ---- batch: 090 ----
mean loss: 872.05
train mean loss: 860.60
epoch train time: 0:00:00.672438
elapsed time: 0:00:38.645021
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 22:44:17.105811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.45
 ---- batch: 020 ----
mean loss: 861.36
 ---- batch: 030 ----
mean loss: 833.75
 ---- batch: 040 ----
mean loss: 853.69
 ---- batch: 050 ----
mean loss: 869.36
 ---- batch: 060 ----
mean loss: 871.17
 ---- batch: 070 ----
mean loss: 858.88
 ---- batch: 080 ----
mean loss: 844.86
 ---- batch: 090 ----
mean loss: 848.41
train mean loss: 855.98
epoch train time: 0:00:00.671182
elapsed time: 0:00:39.316375
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 22:44:17.777180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.51
 ---- batch: 020 ----
mean loss: 862.70
 ---- batch: 030 ----
mean loss: 853.49
 ---- batch: 040 ----
mean loss: 856.46
 ---- batch: 050 ----
mean loss: 844.75
 ---- batch: 060 ----
mean loss: 854.81
 ---- batch: 070 ----
mean loss: 853.15
 ---- batch: 080 ----
mean loss: 837.88
 ---- batch: 090 ----
mean loss: 852.83
train mean loss: 849.78
epoch train time: 0:00:00.675470
elapsed time: 0:00:39.992008
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 22:44:18.452819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 809.23
 ---- batch: 020 ----
mean loss: 835.90
 ---- batch: 030 ----
mean loss: 846.87
 ---- batch: 040 ----
mean loss: 866.49
 ---- batch: 050 ----
mean loss: 861.66
 ---- batch: 060 ----
mean loss: 848.17
 ---- batch: 070 ----
mean loss: 857.63
 ---- batch: 080 ----
mean loss: 839.58
 ---- batch: 090 ----
mean loss: 831.97
train mean loss: 842.61
epoch train time: 0:00:00.679563
elapsed time: 0:00:40.671749
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 22:44:19.132541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.77
 ---- batch: 020 ----
mean loss: 841.22
 ---- batch: 030 ----
mean loss: 842.42
 ---- batch: 040 ----
mean loss: 826.19
 ---- batch: 050 ----
mean loss: 844.32
 ---- batch: 060 ----
mean loss: 842.51
 ---- batch: 070 ----
mean loss: 816.36
 ---- batch: 080 ----
mean loss: 831.25
 ---- batch: 090 ----
mean loss: 846.65
train mean loss: 836.21
epoch train time: 0:00:00.678643
elapsed time: 0:00:41.350544
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 22:44:19.811336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.93
 ---- batch: 020 ----
mean loss: 833.91
 ---- batch: 030 ----
mean loss: 829.23
 ---- batch: 040 ----
mean loss: 806.12
 ---- batch: 050 ----
mean loss: 834.47
 ---- batch: 060 ----
mean loss: 819.93
 ---- batch: 070 ----
mean loss: 846.37
 ---- batch: 080 ----
mean loss: 824.37
 ---- batch: 090 ----
mean loss: 824.33
train mean loss: 828.11
epoch train time: 0:00:00.672774
elapsed time: 0:00:42.023490
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 22:44:20.484283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.26
 ---- batch: 020 ----
mean loss: 822.42
 ---- batch: 030 ----
mean loss: 825.78
 ---- batch: 040 ----
mean loss: 818.25
 ---- batch: 050 ----
mean loss: 812.60
 ---- batch: 060 ----
mean loss: 808.73
 ---- batch: 070 ----
mean loss: 810.42
 ---- batch: 080 ----
mean loss: 829.71
 ---- batch: 090 ----
mean loss: 819.28
train mean loss: 820.76
epoch train time: 0:00:00.669958
elapsed time: 0:00:42.693633
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 22:44:21.154493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 809.15
 ---- batch: 020 ----
mean loss: 819.27
 ---- batch: 030 ----
mean loss: 808.47
 ---- batch: 040 ----
mean loss: 824.51
 ---- batch: 050 ----
mean loss: 820.93
 ---- batch: 060 ----
mean loss: 804.46
 ---- batch: 070 ----
mean loss: 794.81
 ---- batch: 080 ----
mean loss: 806.40
 ---- batch: 090 ----
mean loss: 814.33
train mean loss: 811.90
epoch train time: 0:00:00.675646
elapsed time: 0:00:43.369496
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 22:44:21.830285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 814.16
 ---- batch: 020 ----
mean loss: 788.57
 ---- batch: 030 ----
mean loss: 802.61
 ---- batch: 040 ----
mean loss: 811.73
 ---- batch: 050 ----
mean loss: 789.24
 ---- batch: 060 ----
mean loss: 810.00
 ---- batch: 070 ----
mean loss: 815.27
 ---- batch: 080 ----
mean loss: 819.86
 ---- batch: 090 ----
mean loss: 785.74
train mean loss: 804.55
epoch train time: 0:00:00.667376
elapsed time: 0:00:44.037021
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 22:44:22.497815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 812.89
 ---- batch: 020 ----
mean loss: 793.24
 ---- batch: 030 ----
mean loss: 810.04
 ---- batch: 040 ----
mean loss: 805.68
 ---- batch: 050 ----
mean loss: 781.67
 ---- batch: 060 ----
mean loss: 780.79
 ---- batch: 070 ----
mean loss: 796.49
 ---- batch: 080 ----
mean loss: 791.87
 ---- batch: 090 ----
mean loss: 791.03
train mean loss: 796.06
epoch train time: 0:00:00.685111
elapsed time: 0:00:44.722317
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 22:44:23.183141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 797.48
 ---- batch: 020 ----
mean loss: 799.44
 ---- batch: 030 ----
mean loss: 775.64
 ---- batch: 040 ----
mean loss: 792.29
 ---- batch: 050 ----
mean loss: 789.75
 ---- batch: 060 ----
mean loss: 787.01
 ---- batch: 070 ----
mean loss: 770.94
 ---- batch: 080 ----
mean loss: 794.32
 ---- batch: 090 ----
mean loss: 793.20
train mean loss: 788.00
epoch train time: 0:00:00.684490
elapsed time: 0:00:45.406986
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 22:44:23.867775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 782.49
 ---- batch: 020 ----
mean loss: 792.61
 ---- batch: 030 ----
mean loss: 784.13
 ---- batch: 040 ----
mean loss: 786.70
 ---- batch: 050 ----
mean loss: 769.59
 ---- batch: 060 ----
mean loss: 780.79
 ---- batch: 070 ----
mean loss: 783.17
 ---- batch: 080 ----
mean loss: 768.36
 ---- batch: 090 ----
mean loss: 770.75
train mean loss: 780.04
epoch train time: 0:00:00.672247
elapsed time: 0:00:46.079456
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 22:44:24.540260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 778.78
 ---- batch: 020 ----
mean loss: 770.66
 ---- batch: 030 ----
mean loss: 765.31
 ---- batch: 040 ----
mean loss: 773.09
 ---- batch: 050 ----
mean loss: 782.28
 ---- batch: 060 ----
mean loss: 767.64
 ---- batch: 070 ----
mean loss: 757.07
 ---- batch: 080 ----
mean loss: 778.10
 ---- batch: 090 ----
mean loss: 773.41
train mean loss: 770.65
epoch train time: 0:00:00.680499
elapsed time: 0:00:46.760142
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 22:44:25.220930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 763.76
 ---- batch: 020 ----
mean loss: 773.47
 ---- batch: 030 ----
mean loss: 757.82
 ---- batch: 040 ----
mean loss: 747.92
 ---- batch: 050 ----
mean loss: 755.75
 ---- batch: 060 ----
mean loss: 775.67
 ---- batch: 070 ----
mean loss: 762.74
 ---- batch: 080 ----
mean loss: 759.98
 ---- batch: 090 ----
mean loss: 764.60
train mean loss: 762.72
epoch train time: 0:00:00.679008
elapsed time: 0:00:47.439306
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 22:44:25.900097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 764.52
 ---- batch: 020 ----
mean loss: 750.36
 ---- batch: 030 ----
mean loss: 741.63
 ---- batch: 040 ----
mean loss: 749.65
 ---- batch: 050 ----
mean loss: 753.63
 ---- batch: 060 ----
mean loss: 752.70
 ---- batch: 070 ----
mean loss: 753.88
 ---- batch: 080 ----
mean loss: 756.79
 ---- batch: 090 ----
mean loss: 757.08
train mean loss: 753.30
epoch train time: 0:00:00.673792
elapsed time: 0:00:48.113253
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 22:44:26.574049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 761.00
 ---- batch: 020 ----
mean loss: 739.81
 ---- batch: 030 ----
mean loss: 745.29
 ---- batch: 040 ----
mean loss: 734.39
 ---- batch: 050 ----
mean loss: 741.73
 ---- batch: 060 ----
mean loss: 732.92
 ---- batch: 070 ----
mean loss: 748.42
 ---- batch: 080 ----
mean loss: 750.78
 ---- batch: 090 ----
mean loss: 739.48
train mean loss: 744.87
epoch train time: 0:00:00.672949
elapsed time: 0:00:48.786363
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 22:44:27.247153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 730.96
 ---- batch: 020 ----
mean loss: 749.50
 ---- batch: 030 ----
mean loss: 740.51
 ---- batch: 040 ----
mean loss: 741.09
 ---- batch: 050 ----
mean loss: 740.60
 ---- batch: 060 ----
mean loss: 733.84
 ---- batch: 070 ----
mean loss: 729.62
 ---- batch: 080 ----
mean loss: 726.72
 ---- batch: 090 ----
mean loss: 736.72
train mean loss: 734.80
epoch train time: 0:00:00.670965
elapsed time: 0:00:49.457474
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 22:44:27.918261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 721.71
 ---- batch: 020 ----
mean loss: 724.00
 ---- batch: 030 ----
mean loss: 720.35
 ---- batch: 040 ----
mean loss: 716.41
 ---- batch: 050 ----
mean loss: 718.86
 ---- batch: 060 ----
mean loss: 744.48
 ---- batch: 070 ----
mean loss: 729.05
 ---- batch: 080 ----
mean loss: 724.30
 ---- batch: 090 ----
mean loss: 718.01
train mean loss: 724.90
epoch train time: 0:00:00.657502
elapsed time: 0:00:50.115123
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 22:44:28.575929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 723.04
 ---- batch: 020 ----
mean loss: 724.46
 ---- batch: 030 ----
mean loss: 711.64
 ---- batch: 040 ----
mean loss: 714.00
 ---- batch: 050 ----
mean loss: 702.47
 ---- batch: 060 ----
mean loss: 713.53
 ---- batch: 070 ----
mean loss: 718.55
 ---- batch: 080 ----
mean loss: 720.73
 ---- batch: 090 ----
mean loss: 718.30
train mean loss: 714.97
epoch train time: 0:00:00.672875
elapsed time: 0:00:50.788166
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 22:44:29.248960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 721.66
 ---- batch: 020 ----
mean loss: 703.67
 ---- batch: 030 ----
mean loss: 706.36
 ---- batch: 040 ----
mean loss: 714.95
 ---- batch: 050 ----
mean loss: 708.91
 ---- batch: 060 ----
mean loss: 696.65
 ---- batch: 070 ----
mean loss: 691.41
 ---- batch: 080 ----
mean loss: 718.00
 ---- batch: 090 ----
mean loss: 688.19
train mean loss: 704.60
epoch train time: 0:00:00.681547
elapsed time: 0:00:51.469878
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 22:44:29.930670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.09
 ---- batch: 020 ----
mean loss: 694.12
 ---- batch: 030 ----
mean loss: 685.54
 ---- batch: 040 ----
mean loss: 696.14
 ---- batch: 050 ----
mean loss: 700.58
 ---- batch: 060 ----
mean loss: 701.36
 ---- batch: 070 ----
mean loss: 709.75
 ---- batch: 080 ----
mean loss: 688.16
 ---- batch: 090 ----
mean loss: 680.08
train mean loss: 695.04
epoch train time: 0:00:00.667722
elapsed time: 0:00:52.137756
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 22:44:30.598557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.18
 ---- batch: 020 ----
mean loss: 693.47
 ---- batch: 030 ----
mean loss: 684.45
 ---- batch: 040 ----
mean loss: 683.73
 ---- batch: 050 ----
mean loss: 688.84
 ---- batch: 060 ----
mean loss: 694.74
 ---- batch: 070 ----
mean loss: 680.18
 ---- batch: 080 ----
mean loss: 680.61
 ---- batch: 090 ----
mean loss: 658.86
train mean loss: 684.10
epoch train time: 0:00:00.675116
elapsed time: 0:00:52.813032
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 22:44:31.273821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 668.38
 ---- batch: 020 ----
mean loss: 675.71
 ---- batch: 030 ----
mean loss: 672.58
 ---- batch: 040 ----
mean loss: 679.61
 ---- batch: 050 ----
mean loss: 690.87
 ---- batch: 060 ----
mean loss: 666.60
 ---- batch: 070 ----
mean loss: 684.34
 ---- batch: 080 ----
mean loss: 669.86
 ---- batch: 090 ----
mean loss: 658.97
train mean loss: 673.81
epoch train time: 0:00:00.685917
elapsed time: 0:00:53.499095
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 22:44:31.959889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.66
 ---- batch: 020 ----
mean loss: 672.94
 ---- batch: 030 ----
mean loss: 666.83
 ---- batch: 040 ----
mean loss: 656.04
 ---- batch: 050 ----
mean loss: 656.77
 ---- batch: 060 ----
mean loss: 670.57
 ---- batch: 070 ----
mean loss: 647.80
 ---- batch: 080 ----
mean loss: 676.31
 ---- batch: 090 ----
mean loss: 656.71
train mean loss: 663.54
epoch train time: 0:00:00.670564
elapsed time: 0:00:54.169837
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 22:44:32.630625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 658.97
 ---- batch: 020 ----
mean loss: 660.78
 ---- batch: 030 ----
mean loss: 656.63
 ---- batch: 040 ----
mean loss: 653.80
 ---- batch: 050 ----
mean loss: 646.09
 ---- batch: 060 ----
mean loss: 649.07
 ---- batch: 070 ----
mean loss: 651.69
 ---- batch: 080 ----
mean loss: 650.89
 ---- batch: 090 ----
mean loss: 645.98
train mean loss: 653.70
epoch train time: 0:00:00.678557
elapsed time: 0:00:54.848540
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 22:44:33.309355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 650.69
 ---- batch: 020 ----
mean loss: 651.66
 ---- batch: 030 ----
mean loss: 648.30
 ---- batch: 040 ----
mean loss: 644.01
 ---- batch: 050 ----
mean loss: 650.42
 ---- batch: 060 ----
mean loss: 640.94
 ---- batch: 070 ----
mean loss: 641.36
 ---- batch: 080 ----
mean loss: 628.26
 ---- batch: 090 ----
mean loss: 635.21
train mean loss: 643.17
epoch train time: 0:00:00.683901
elapsed time: 0:00:55.532611
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 22:44:33.993400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 635.96
 ---- batch: 020 ----
mean loss: 635.76
 ---- batch: 030 ----
mean loss: 626.68
 ---- batch: 040 ----
mean loss: 632.69
 ---- batch: 050 ----
mean loss: 629.79
 ---- batch: 060 ----
mean loss: 638.12
 ---- batch: 070 ----
mean loss: 634.61
 ---- batch: 080 ----
mean loss: 626.09
 ---- batch: 090 ----
mean loss: 640.70
train mean loss: 633.15
epoch train time: 0:00:00.665732
elapsed time: 0:00:56.198507
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 22:44:34.659296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.37
 ---- batch: 020 ----
mean loss: 625.47
 ---- batch: 030 ----
mean loss: 632.16
 ---- batch: 040 ----
mean loss: 631.54
 ---- batch: 050 ----
mean loss: 628.41
 ---- batch: 060 ----
mean loss: 606.96
 ---- batch: 070 ----
mean loss: 611.12
 ---- batch: 080 ----
mean loss: 613.94
 ---- batch: 090 ----
mean loss: 636.37
train mean loss: 622.87
epoch train time: 0:00:00.676782
elapsed time: 0:00:56.875518
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 22:44:35.336325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.76
 ---- batch: 020 ----
mean loss: 617.57
 ---- batch: 030 ----
mean loss: 612.15
 ---- batch: 040 ----
mean loss: 624.11
 ---- batch: 050 ----
mean loss: 619.89
 ---- batch: 060 ----
mean loss: 607.22
 ---- batch: 070 ----
mean loss: 613.97
 ---- batch: 080 ----
mean loss: 592.24
 ---- batch: 090 ----
mean loss: 600.31
train mean loss: 613.12
epoch train time: 0:00:00.671666
elapsed time: 0:00:57.547350
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 22:44:36.008141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 598.53
 ---- batch: 020 ----
mean loss: 612.75
 ---- batch: 030 ----
mean loss: 602.18
 ---- batch: 040 ----
mean loss: 614.86
 ---- batch: 050 ----
mean loss: 600.26
 ---- batch: 060 ----
mean loss: 601.47
 ---- batch: 070 ----
mean loss: 604.66
 ---- batch: 080 ----
mean loss: 604.46
 ---- batch: 090 ----
mean loss: 588.21
train mean loss: 603.43
epoch train time: 0:00:00.681454
elapsed time: 0:00:58.229011
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 22:44:36.689804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 599.75
 ---- batch: 020 ----
mean loss: 595.74
 ---- batch: 030 ----
mean loss: 592.07
 ---- batch: 040 ----
mean loss: 597.88
 ---- batch: 050 ----
mean loss: 588.86
 ---- batch: 060 ----
mean loss: 597.97
 ---- batch: 070 ----
mean loss: 599.59
 ---- batch: 080 ----
mean loss: 589.23
 ---- batch: 090 ----
mean loss: 590.04
train mean loss: 594.48
epoch train time: 0:00:00.678803
elapsed time: 0:00:58.907969
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 22:44:37.368760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 592.12
 ---- batch: 020 ----
mean loss: 578.78
 ---- batch: 030 ----
mean loss: 580.76
 ---- batch: 040 ----
mean loss: 593.48
 ---- batch: 050 ----
mean loss: 587.23
 ---- batch: 060 ----
mean loss: 584.34
 ---- batch: 070 ----
mean loss: 579.80
 ---- batch: 080 ----
mean loss: 576.82
 ---- batch: 090 ----
mean loss: 586.05
train mean loss: 584.84
epoch train time: 0:00:00.679090
elapsed time: 0:00:59.587219
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 22:44:38.048012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 587.80
 ---- batch: 020 ----
mean loss: 581.21
 ---- batch: 030 ----
mean loss: 573.15
 ---- batch: 040 ----
mean loss: 572.75
 ---- batch: 050 ----
mean loss: 575.74
 ---- batch: 060 ----
mean loss: 579.89
 ---- batch: 070 ----
mean loss: 577.88
 ---- batch: 080 ----
mean loss: 571.40
 ---- batch: 090 ----
mean loss: 563.89
train mean loss: 575.78
epoch train time: 0:00:00.665317
elapsed time: 0:01:00.252681
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 22:44:38.713545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 562.45
 ---- batch: 020 ----
mean loss: 570.43
 ---- batch: 030 ----
mean loss: 570.44
 ---- batch: 040 ----
mean loss: 570.05
 ---- batch: 050 ----
mean loss: 558.81
 ---- batch: 060 ----
mean loss: 577.45
 ---- batch: 070 ----
mean loss: 561.28
 ---- batch: 080 ----
mean loss: 559.48
 ---- batch: 090 ----
mean loss: 569.54
train mean loss: 567.01
epoch train time: 0:00:00.675767
elapsed time: 0:01:00.928670
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 22:44:39.389461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.21
 ---- batch: 020 ----
mean loss: 550.86
 ---- batch: 030 ----
mean loss: 562.34
 ---- batch: 040 ----
mean loss: 540.97
 ---- batch: 050 ----
mean loss: 556.29
 ---- batch: 060 ----
mean loss: 560.93
 ---- batch: 070 ----
mean loss: 556.90
 ---- batch: 080 ----
mean loss: 565.81
 ---- batch: 090 ----
mean loss: 561.64
train mean loss: 558.51
epoch train time: 0:00:00.677439
elapsed time: 0:01:01.606275
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 22:44:40.067066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 554.05
 ---- batch: 020 ----
mean loss: 556.72
 ---- batch: 030 ----
mean loss: 556.85
 ---- batch: 040 ----
mean loss: 552.47
 ---- batch: 050 ----
mean loss: 545.56
 ---- batch: 060 ----
mean loss: 546.80
 ---- batch: 070 ----
mean loss: 545.50
 ---- batch: 080 ----
mean loss: 547.21
 ---- batch: 090 ----
mean loss: 548.65
train mean loss: 549.90
epoch train time: 0:00:00.678008
elapsed time: 0:01:02.284430
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 22:44:40.745217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 542.97
 ---- batch: 020 ----
mean loss: 536.70
 ---- batch: 030 ----
mean loss: 561.34
 ---- batch: 040 ----
mean loss: 537.69
 ---- batch: 050 ----
mean loss: 535.78
 ---- batch: 060 ----
mean loss: 542.58
 ---- batch: 070 ----
mean loss: 545.00
 ---- batch: 080 ----
mean loss: 531.07
 ---- batch: 090 ----
mean loss: 543.14
train mean loss: 541.60
epoch train time: 0:00:00.673752
elapsed time: 0:01:02.958326
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 22:44:41.419115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 536.86
 ---- batch: 020 ----
mean loss: 543.18
 ---- batch: 030 ----
mean loss: 534.04
 ---- batch: 040 ----
mean loss: 536.92
 ---- batch: 050 ----
mean loss: 529.70
 ---- batch: 060 ----
mean loss: 527.65
 ---- batch: 070 ----
mean loss: 540.01
 ---- batch: 080 ----
mean loss: 524.70
 ---- batch: 090 ----
mean loss: 529.29
train mean loss: 533.21
epoch train time: 0:00:00.674685
elapsed time: 0:01:03.633160
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 22:44:42.093951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 528.49
 ---- batch: 020 ----
mean loss: 526.44
 ---- batch: 030 ----
mean loss: 532.21
 ---- batch: 040 ----
mean loss: 520.76
 ---- batch: 050 ----
mean loss: 531.91
 ---- batch: 060 ----
mean loss: 529.88
 ---- batch: 070 ----
mean loss: 527.81
 ---- batch: 080 ----
mean loss: 509.76
 ---- batch: 090 ----
mean loss: 516.77
train mean loss: 524.93
epoch train time: 0:00:00.672066
elapsed time: 0:01:04.305374
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 22:44:42.766161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.40
 ---- batch: 020 ----
mean loss: 515.98
 ---- batch: 030 ----
mean loss: 518.60
 ---- batch: 040 ----
mean loss: 510.05
 ---- batch: 050 ----
mean loss: 534.07
 ---- batch: 060 ----
mean loss: 508.69
 ---- batch: 070 ----
mean loss: 520.14
 ---- batch: 080 ----
mean loss: 508.21
 ---- batch: 090 ----
mean loss: 519.03
train mean loss: 517.18
epoch train time: 0:00:00.672648
elapsed time: 0:01:04.978175
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 22:44:43.438982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.70
 ---- batch: 020 ----
mean loss: 505.08
 ---- batch: 030 ----
mean loss: 505.31
 ---- batch: 040 ----
mean loss: 499.91
 ---- batch: 050 ----
mean loss: 514.77
 ---- batch: 060 ----
mean loss: 503.61
 ---- batch: 070 ----
mean loss: 516.50
 ---- batch: 080 ----
mean loss: 514.80
 ---- batch: 090 ----
mean loss: 515.30
train mean loss: 509.21
epoch train time: 0:00:00.674560
elapsed time: 0:01:05.652955
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 22:44:44.113763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.04
 ---- batch: 020 ----
mean loss: 496.49
 ---- batch: 030 ----
mean loss: 504.85
 ---- batch: 040 ----
mean loss: 509.74
 ---- batch: 050 ----
mean loss: 491.33
 ---- batch: 060 ----
mean loss: 509.92
 ---- batch: 070 ----
mean loss: 494.51
 ---- batch: 080 ----
mean loss: 503.62
 ---- batch: 090 ----
mean loss: 497.64
train mean loss: 501.46
epoch train time: 0:00:00.673922
elapsed time: 0:01:06.327043
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 22:44:44.787855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.00
 ---- batch: 020 ----
mean loss: 498.41
 ---- batch: 030 ----
mean loss: 500.28
 ---- batch: 040 ----
mean loss: 494.05
 ---- batch: 050 ----
mean loss: 486.63
 ---- batch: 060 ----
mean loss: 492.41
 ---- batch: 070 ----
mean loss: 495.85
 ---- batch: 080 ----
mean loss: 487.97
 ---- batch: 090 ----
mean loss: 492.59
train mean loss: 493.78
epoch train time: 0:00:00.667965
elapsed time: 0:01:06.995186
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 22:44:45.455977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.15
 ---- batch: 020 ----
mean loss: 485.35
 ---- batch: 030 ----
mean loss: 492.78
 ---- batch: 040 ----
mean loss: 486.64
 ---- batch: 050 ----
mean loss: 475.42
 ---- batch: 060 ----
mean loss: 479.86
 ---- batch: 070 ----
mean loss: 487.45
 ---- batch: 080 ----
mean loss: 489.72
 ---- batch: 090 ----
mean loss: 492.07
train mean loss: 486.62
epoch train time: 0:00:00.679123
elapsed time: 0:01:07.674476
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 22:44:46.135269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.44
 ---- batch: 020 ----
mean loss: 478.99
 ---- batch: 030 ----
mean loss: 481.78
 ---- batch: 040 ----
mean loss: 471.13
 ---- batch: 050 ----
mean loss: 485.36
 ---- batch: 060 ----
mean loss: 487.12
 ---- batch: 070 ----
mean loss: 471.54
 ---- batch: 080 ----
mean loss: 480.04
 ---- batch: 090 ----
mean loss: 474.38
train mean loss: 478.99
epoch train time: 0:00:00.673983
elapsed time: 0:01:08.348611
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 22:44:46.809420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.65
 ---- batch: 020 ----
mean loss: 471.52
 ---- batch: 030 ----
mean loss: 487.90
 ---- batch: 040 ----
mean loss: 472.22
 ---- batch: 050 ----
mean loss: 472.97
 ---- batch: 060 ----
mean loss: 471.03
 ---- batch: 070 ----
mean loss: 463.51
 ---- batch: 080 ----
mean loss: 477.63
 ---- batch: 090 ----
mean loss: 456.03
train mean loss: 471.85
epoch train time: 0:00:00.664310
elapsed time: 0:01:09.013137
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 22:44:47.473916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.43
 ---- batch: 020 ----
mean loss: 459.01
 ---- batch: 030 ----
mean loss: 470.01
 ---- batch: 040 ----
mean loss: 455.12
 ---- batch: 050 ----
mean loss: 465.89
 ---- batch: 060 ----
mean loss: 466.51
 ---- batch: 070 ----
mean loss: 468.06
 ---- batch: 080 ----
mean loss: 456.35
 ---- batch: 090 ----
mean loss: 463.97
train mean loss: 464.81
epoch train time: 0:00:00.677728
elapsed time: 0:01:09.691002
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 22:44:48.151809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.05
 ---- batch: 020 ----
mean loss: 454.67
 ---- batch: 030 ----
mean loss: 457.26
 ---- batch: 040 ----
mean loss: 461.40
 ---- batch: 050 ----
mean loss: 462.00
 ---- batch: 060 ----
mean loss: 458.29
 ---- batch: 070 ----
mean loss: 471.26
 ---- batch: 080 ----
mean loss: 440.47
 ---- batch: 090 ----
mean loss: 453.54
train mean loss: 458.09
epoch train time: 0:00:00.675079
elapsed time: 0:01:10.366252
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 22:44:48.827107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.28
 ---- batch: 020 ----
mean loss: 447.45
 ---- batch: 030 ----
mean loss: 464.16
 ---- batch: 040 ----
mean loss: 447.77
 ---- batch: 050 ----
mean loss: 454.87
 ---- batch: 060 ----
mean loss: 448.14
 ---- batch: 070 ----
mean loss: 448.65
 ---- batch: 080 ----
mean loss: 448.18
 ---- batch: 090 ----
mean loss: 442.46
train mean loss: 451.20
epoch train time: 0:00:00.673979
elapsed time: 0:01:11.040438
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 22:44:49.501226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.21
 ---- batch: 020 ----
mean loss: 455.68
 ---- batch: 030 ----
mean loss: 453.38
 ---- batch: 040 ----
mean loss: 435.20
 ---- batch: 050 ----
mean loss: 457.65
 ---- batch: 060 ----
mean loss: 445.57
 ---- batch: 070 ----
mean loss: 438.44
 ---- batch: 080 ----
mean loss: 439.45
 ---- batch: 090 ----
mean loss: 440.64
train mean loss: 444.23
epoch train time: 0:00:00.680738
elapsed time: 0:01:11.721342
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 22:44:50.182165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.89
 ---- batch: 020 ----
mean loss: 434.44
 ---- batch: 030 ----
mean loss: 441.08
 ---- batch: 040 ----
mean loss: 439.22
 ---- batch: 050 ----
mean loss: 435.46
 ---- batch: 060 ----
mean loss: 431.31
 ---- batch: 070 ----
mean loss: 437.46
 ---- batch: 080 ----
mean loss: 438.28
 ---- batch: 090 ----
mean loss: 439.98
train mean loss: 437.61
epoch train time: 0:00:00.676763
elapsed time: 0:01:12.398286
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 22:44:50.859089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.18
 ---- batch: 020 ----
mean loss: 430.14
 ---- batch: 030 ----
mean loss: 429.11
 ---- batch: 040 ----
mean loss: 431.66
 ---- batch: 050 ----
mean loss: 429.60
 ---- batch: 060 ----
mean loss: 435.76
 ---- batch: 070 ----
mean loss: 430.24
 ---- batch: 080 ----
mean loss: 430.33
 ---- batch: 090 ----
mean loss: 431.74
train mean loss: 431.75
epoch train time: 0:00:00.673471
elapsed time: 0:01:13.071923
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 22:44:51.532714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.77
 ---- batch: 020 ----
mean loss: 420.06
 ---- batch: 030 ----
mean loss: 419.95
 ---- batch: 040 ----
mean loss: 433.71
 ---- batch: 050 ----
mean loss: 424.48
 ---- batch: 060 ----
mean loss: 421.76
 ---- batch: 070 ----
mean loss: 428.52
 ---- batch: 080 ----
mean loss: 433.60
 ---- batch: 090 ----
mean loss: 426.52
train mean loss: 425.09
epoch train time: 0:00:00.678400
elapsed time: 0:01:13.750467
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 22:44:52.211253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.67
 ---- batch: 020 ----
mean loss: 412.60
 ---- batch: 030 ----
mean loss: 427.08
 ---- batch: 040 ----
mean loss: 408.34
 ---- batch: 050 ----
mean loss: 413.24
 ---- batch: 060 ----
mean loss: 423.22
 ---- batch: 070 ----
mean loss: 418.41
 ---- batch: 080 ----
mean loss: 416.91
 ---- batch: 090 ----
mean loss: 433.56
train mean loss: 418.79
epoch train time: 0:00:00.673901
elapsed time: 0:01:14.424509
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 22:44:52.885297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.26
 ---- batch: 020 ----
mean loss: 410.42
 ---- batch: 030 ----
mean loss: 413.44
 ---- batch: 040 ----
mean loss: 407.47
 ---- batch: 050 ----
mean loss: 418.37
 ---- batch: 060 ----
mean loss: 413.45
 ---- batch: 070 ----
mean loss: 417.49
 ---- batch: 080 ----
mean loss: 413.63
 ---- batch: 090 ----
mean loss: 408.68
train mean loss: 413.16
epoch train time: 0:00:00.671443
elapsed time: 0:01:15.096115
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 22:44:53.556934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.41
 ---- batch: 020 ----
mean loss: 407.66
 ---- batch: 030 ----
mean loss: 415.09
 ---- batch: 040 ----
mean loss: 407.11
 ---- batch: 050 ----
mean loss: 423.18
 ---- batch: 060 ----
mean loss: 403.61
 ---- batch: 070 ----
mean loss: 400.34
 ---- batch: 080 ----
mean loss: 403.19
 ---- batch: 090 ----
mean loss: 401.03
train mean loss: 407.41
epoch train time: 0:00:00.673614
elapsed time: 0:01:15.769970
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 22:44:54.230784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.00
 ---- batch: 020 ----
mean loss: 406.06
 ---- batch: 030 ----
mean loss: 400.34
 ---- batch: 040 ----
mean loss: 409.11
 ---- batch: 050 ----
mean loss: 404.90
 ---- batch: 060 ----
mean loss: 398.36
 ---- batch: 070 ----
mean loss: 393.58
 ---- batch: 080 ----
mean loss: 402.42
 ---- batch: 090 ----
mean loss: 399.55
train mean loss: 402.21
epoch train time: 0:00:00.677843
elapsed time: 0:01:16.447981
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 22:44:54.908780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.05
 ---- batch: 020 ----
mean loss: 400.07
 ---- batch: 030 ----
mean loss: 401.59
 ---- batch: 040 ----
mean loss: 383.70
 ---- batch: 050 ----
mean loss: 392.47
 ---- batch: 060 ----
mean loss: 404.60
 ---- batch: 070 ----
mean loss: 393.92
 ---- batch: 080 ----
mean loss: 392.87
 ---- batch: 090 ----
mean loss: 400.50
train mean loss: 395.55
epoch train time: 0:00:00.668467
elapsed time: 0:01:17.116599
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 22:44:55.577404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.92
 ---- batch: 020 ----
mean loss: 385.16
 ---- batch: 030 ----
mean loss: 399.55
 ---- batch: 040 ----
mean loss: 389.60
 ---- batch: 050 ----
mean loss: 393.12
 ---- batch: 060 ----
mean loss: 384.13
 ---- batch: 070 ----
mean loss: 387.04
 ---- batch: 080 ----
mean loss: 383.44
 ---- batch: 090 ----
mean loss: 385.41
train mean loss: 388.88
epoch train time: 0:00:00.675417
elapsed time: 0:01:17.792181
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 22:44:56.252972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.12
 ---- batch: 020 ----
mean loss: 380.78
 ---- batch: 030 ----
mean loss: 372.17
 ---- batch: 040 ----
mean loss: 373.10
 ---- batch: 050 ----
mean loss: 386.70
 ---- batch: 060 ----
mean loss: 386.11
 ---- batch: 070 ----
mean loss: 384.06
 ---- batch: 080 ----
mean loss: 375.59
 ---- batch: 090 ----
mean loss: 367.92
train mean loss: 378.32
epoch train time: 0:00:00.671913
elapsed time: 0:01:18.464241
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 22:44:56.925030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.69
 ---- batch: 020 ----
mean loss: 369.54
 ---- batch: 030 ----
mean loss: 366.05
 ---- batch: 040 ----
mean loss: 361.25
 ---- batch: 050 ----
mean loss: 367.91
 ---- batch: 060 ----
mean loss: 361.98
 ---- batch: 070 ----
mean loss: 367.33
 ---- batch: 080 ----
mean loss: 355.23
 ---- batch: 090 ----
mean loss: 354.69
train mean loss: 364.68
epoch train time: 0:00:00.667274
elapsed time: 0:01:19.131667
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 22:44:57.592459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.88
 ---- batch: 020 ----
mean loss: 346.96
 ---- batch: 030 ----
mean loss: 355.63
 ---- batch: 040 ----
mean loss: 353.33
 ---- batch: 050 ----
mean loss: 342.61
 ---- batch: 060 ----
mean loss: 342.06
 ---- batch: 070 ----
mean loss: 334.31
 ---- batch: 080 ----
mean loss: 362.66
 ---- batch: 090 ----
mean loss: 356.87
train mean loss: 350.56
epoch train time: 0:00:00.674661
elapsed time: 0:01:19.806477
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 22:44:58.267268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.51
 ---- batch: 020 ----
mean loss: 341.66
 ---- batch: 030 ----
mean loss: 349.19
 ---- batch: 040 ----
mean loss: 347.83
 ---- batch: 050 ----
mean loss: 339.19
 ---- batch: 060 ----
mean loss: 331.29
 ---- batch: 070 ----
mean loss: 335.86
 ---- batch: 080 ----
mean loss: 332.96
 ---- batch: 090 ----
mean loss: 338.91
train mean loss: 338.68
epoch train time: 0:00:00.673372
elapsed time: 0:01:20.479993
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 22:44:58.940800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.14
 ---- batch: 020 ----
mean loss: 325.28
 ---- batch: 030 ----
mean loss: 336.39
 ---- batch: 040 ----
mean loss: 332.27
 ---- batch: 050 ----
mean loss: 334.64
 ---- batch: 060 ----
mean loss: 334.73
 ---- batch: 070 ----
mean loss: 319.84
 ---- batch: 080 ----
mean loss: 323.87
 ---- batch: 090 ----
mean loss: 331.12
train mean loss: 329.26
epoch train time: 0:00:00.673290
elapsed time: 0:01:21.153511
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 22:44:59.614320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.29
 ---- batch: 020 ----
mean loss: 312.07
 ---- batch: 030 ----
mean loss: 314.55
 ---- batch: 040 ----
mean loss: 327.58
 ---- batch: 050 ----
mean loss: 324.57
 ---- batch: 060 ----
mean loss: 328.66
 ---- batch: 070 ----
mean loss: 321.08
 ---- batch: 080 ----
mean loss: 324.83
 ---- batch: 090 ----
mean loss: 320.53
train mean loss: 321.31
epoch train time: 0:00:00.676796
elapsed time: 0:01:21.830479
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 22:45:00.291304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.20
 ---- batch: 020 ----
mean loss: 314.38
 ---- batch: 030 ----
mean loss: 323.70
 ---- batch: 040 ----
mean loss: 310.40
 ---- batch: 050 ----
mean loss: 312.57
 ---- batch: 060 ----
mean loss: 312.67
 ---- batch: 070 ----
mean loss: 317.02
 ---- batch: 080 ----
mean loss: 320.12
 ---- batch: 090 ----
mean loss: 311.54
train mean loss: 314.48
epoch train time: 0:00:00.675216
elapsed time: 0:01:22.505887
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 22:45:00.966675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.18
 ---- batch: 020 ----
mean loss: 308.40
 ---- batch: 030 ----
mean loss: 318.77
 ---- batch: 040 ----
mean loss: 302.69
 ---- batch: 050 ----
mean loss: 305.69
 ---- batch: 060 ----
mean loss: 312.42
 ---- batch: 070 ----
mean loss: 310.89
 ---- batch: 080 ----
mean loss: 306.76
 ---- batch: 090 ----
mean loss: 301.52
train mean loss: 308.43
epoch train time: 0:00:00.673092
elapsed time: 0:01:23.179137
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 22:45:01.639939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.27
 ---- batch: 020 ----
mean loss: 305.87
 ---- batch: 030 ----
mean loss: 302.63
 ---- batch: 040 ----
mean loss: 306.26
 ---- batch: 050 ----
mean loss: 294.64
 ---- batch: 060 ----
mean loss: 303.74
 ---- batch: 070 ----
mean loss: 308.56
 ---- batch: 080 ----
mean loss: 303.62
 ---- batch: 090 ----
mean loss: 307.37
train mean loss: 302.80
epoch train time: 0:00:00.675785
elapsed time: 0:01:23.855104
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 22:45:02.315902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.78
 ---- batch: 020 ----
mean loss: 300.07
 ---- batch: 030 ----
mean loss: 283.63
 ---- batch: 040 ----
mean loss: 299.18
 ---- batch: 050 ----
mean loss: 302.28
 ---- batch: 060 ----
mean loss: 287.51
 ---- batch: 070 ----
mean loss: 295.56
 ---- batch: 080 ----
mean loss: 302.87
 ---- batch: 090 ----
mean loss: 291.47
train mean loss: 296.63
epoch train time: 0:00:00.671791
elapsed time: 0:01:24.527059
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 22:45:02.987847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.97
 ---- batch: 020 ----
mean loss: 295.01
 ---- batch: 030 ----
mean loss: 286.79
 ---- batch: 040 ----
mean loss: 301.18
 ---- batch: 050 ----
mean loss: 286.36
 ---- batch: 060 ----
mean loss: 288.07
 ---- batch: 070 ----
mean loss: 288.99
 ---- batch: 080 ----
mean loss: 293.70
 ---- batch: 090 ----
mean loss: 289.79
train mean loss: 290.60
epoch train time: 0:00:00.667577
elapsed time: 0:01:25.194800
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 22:45:03.655617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.67
 ---- batch: 020 ----
mean loss: 283.14
 ---- batch: 030 ----
mean loss: 282.19
 ---- batch: 040 ----
mean loss: 298.50
 ---- batch: 050 ----
mean loss: 287.95
 ---- batch: 060 ----
mean loss: 277.68
 ---- batch: 070 ----
mean loss: 296.73
 ---- batch: 080 ----
mean loss: 280.68
 ---- batch: 090 ----
mean loss: 287.20
train mean loss: 285.53
epoch train time: 0:00:00.683786
elapsed time: 0:01:25.878765
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 22:45:04.339554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.09
 ---- batch: 020 ----
mean loss: 283.30
 ---- batch: 030 ----
mean loss: 280.43
 ---- batch: 040 ----
mean loss: 287.06
 ---- batch: 050 ----
mean loss: 275.22
 ---- batch: 060 ----
mean loss: 291.87
 ---- batch: 070 ----
mean loss: 267.96
 ---- batch: 080 ----
mean loss: 277.53
 ---- batch: 090 ----
mean loss: 281.93
train mean loss: 280.44
epoch train time: 0:00:00.683958
elapsed time: 0:01:26.562889
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 22:45:05.023688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.18
 ---- batch: 020 ----
mean loss: 273.32
 ---- batch: 030 ----
mean loss: 277.78
 ---- batch: 040 ----
mean loss: 276.04
 ---- batch: 050 ----
mean loss: 275.86
 ---- batch: 060 ----
mean loss: 275.08
 ---- batch: 070 ----
mean loss: 274.27
 ---- batch: 080 ----
mean loss: 276.89
 ---- batch: 090 ----
mean loss: 275.29
train mean loss: 276.62
epoch train time: 0:00:00.695429
elapsed time: 0:01:27.258494
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 22:45:05.719285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.65
 ---- batch: 020 ----
mean loss: 279.92
 ---- batch: 030 ----
mean loss: 281.09
 ---- batch: 040 ----
mean loss: 269.18
 ---- batch: 050 ----
mean loss: 282.88
 ---- batch: 060 ----
mean loss: 267.76
 ---- batch: 070 ----
mean loss: 270.29
 ---- batch: 080 ----
mean loss: 262.09
 ---- batch: 090 ----
mean loss: 271.12
train mean loss: 272.66
epoch train time: 0:00:00.691316
elapsed time: 0:01:27.949955
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 22:45:06.410742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.11
 ---- batch: 020 ----
mean loss: 263.70
 ---- batch: 030 ----
mean loss: 264.52
 ---- batch: 040 ----
mean loss: 270.70
 ---- batch: 050 ----
mean loss: 269.10
 ---- batch: 060 ----
mean loss: 278.50
 ---- batch: 070 ----
mean loss: 261.88
 ---- batch: 080 ----
mean loss: 278.20
 ---- batch: 090 ----
mean loss: 270.00
train mean loss: 269.46
epoch train time: 0:00:00.681425
elapsed time: 0:01:28.631554
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 22:45:07.092361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.65
 ---- batch: 020 ----
mean loss: 259.36
 ---- batch: 030 ----
mean loss: 270.15
 ---- batch: 040 ----
mean loss: 266.17
 ---- batch: 050 ----
mean loss: 270.08
 ---- batch: 060 ----
mean loss: 270.74
 ---- batch: 070 ----
mean loss: 263.96
 ---- batch: 080 ----
mean loss: 264.50
 ---- batch: 090 ----
mean loss: 265.77
train mean loss: 267.28
epoch train time: 0:00:00.685028
elapsed time: 0:01:29.316749
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 22:45:07.777546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.92
 ---- batch: 020 ----
mean loss: 259.16
 ---- batch: 030 ----
mean loss: 265.87
 ---- batch: 040 ----
mean loss: 254.86
 ---- batch: 050 ----
mean loss: 270.41
 ---- batch: 060 ----
mean loss: 265.24
 ---- batch: 070 ----
mean loss: 262.41
 ---- batch: 080 ----
mean loss: 263.79
 ---- batch: 090 ----
mean loss: 273.86
train mean loss: 264.42
epoch train time: 0:00:00.691776
elapsed time: 0:01:30.008674
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 22:45:08.469463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.76
 ---- batch: 020 ----
mean loss: 270.32
 ---- batch: 030 ----
mean loss: 257.66
 ---- batch: 040 ----
mean loss: 270.37
 ---- batch: 050 ----
mean loss: 263.00
 ---- batch: 060 ----
mean loss: 254.03
 ---- batch: 070 ----
mean loss: 249.09
 ---- batch: 080 ----
mean loss: 256.93
 ---- batch: 090 ----
mean loss: 272.02
train mean loss: 263.11
epoch train time: 0:00:00.683159
elapsed time: 0:01:30.691978
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 22:45:09.152789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.04
 ---- batch: 020 ----
mean loss: 259.70
 ---- batch: 030 ----
mean loss: 262.67
 ---- batch: 040 ----
mean loss: 266.33
 ---- batch: 050 ----
mean loss: 257.78
 ---- batch: 060 ----
mean loss: 260.22
 ---- batch: 070 ----
mean loss: 258.04
 ---- batch: 080 ----
mean loss: 261.64
 ---- batch: 090 ----
mean loss: 254.55
train mean loss: 260.89
epoch train time: 0:00:00.687253
elapsed time: 0:01:31.379406
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 22:45:09.840229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.59
 ---- batch: 020 ----
mean loss: 262.75
 ---- batch: 030 ----
mean loss: 265.43
 ---- batch: 040 ----
mean loss: 264.00
 ---- batch: 050 ----
mean loss: 247.67
 ---- batch: 060 ----
mean loss: 263.87
 ---- batch: 070 ----
mean loss: 257.10
 ---- batch: 080 ----
mean loss: 262.92
 ---- batch: 090 ----
mean loss: 256.85
train mean loss: 259.23
epoch train time: 0:00:00.688292
elapsed time: 0:01:32.067895
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 22:45:10.528721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.28
 ---- batch: 020 ----
mean loss: 262.65
 ---- batch: 030 ----
mean loss: 260.88
 ---- batch: 040 ----
mean loss: 255.75
 ---- batch: 050 ----
mean loss: 253.12
 ---- batch: 060 ----
mean loss: 262.21
 ---- batch: 070 ----
mean loss: 253.06
 ---- batch: 080 ----
mean loss: 257.26
 ---- batch: 090 ----
mean loss: 251.83
train mean loss: 257.70
epoch train time: 0:00:00.686761
elapsed time: 0:01:32.754883
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 22:45:11.215671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.02
 ---- batch: 020 ----
mean loss: 262.85
 ---- batch: 030 ----
mean loss: 260.23
 ---- batch: 040 ----
mean loss: 253.91
 ---- batch: 050 ----
mean loss: 258.89
 ---- batch: 060 ----
mean loss: 252.84
 ---- batch: 070 ----
mean loss: 254.14
 ---- batch: 080 ----
mean loss: 255.80
 ---- batch: 090 ----
mean loss: 257.22
train mean loss: 256.20
epoch train time: 0:00:00.681652
elapsed time: 0:01:33.436693
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 22:45:11.897486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.92
 ---- batch: 020 ----
mean loss: 256.38
 ---- batch: 030 ----
mean loss: 246.10
 ---- batch: 040 ----
mean loss: 254.83
 ---- batch: 050 ----
mean loss: 263.37
 ---- batch: 060 ----
mean loss: 265.17
 ---- batch: 070 ----
mean loss: 257.12
 ---- batch: 080 ----
mean loss: 252.86
 ---- batch: 090 ----
mean loss: 249.69
train mean loss: 255.09
epoch train time: 0:00:00.685519
elapsed time: 0:01:34.122362
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 22:45:12.583167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.82
 ---- batch: 020 ----
mean loss: 259.24
 ---- batch: 030 ----
mean loss: 251.55
 ---- batch: 040 ----
mean loss: 247.55
 ---- batch: 050 ----
mean loss: 258.21
 ---- batch: 060 ----
mean loss: 264.64
 ---- batch: 070 ----
mean loss: 254.26
 ---- batch: 080 ----
mean loss: 253.36
 ---- batch: 090 ----
mean loss: 247.18
train mean loss: 253.64
epoch train time: 0:00:00.687086
elapsed time: 0:01:34.809607
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 22:45:13.270394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.90
 ---- batch: 020 ----
mean loss: 258.51
 ---- batch: 030 ----
mean loss: 247.47
 ---- batch: 040 ----
mean loss: 254.33
 ---- batch: 050 ----
mean loss: 248.95
 ---- batch: 060 ----
mean loss: 255.49
 ---- batch: 070 ----
mean loss: 249.59
 ---- batch: 080 ----
mean loss: 257.05
 ---- batch: 090 ----
mean loss: 255.78
train mean loss: 253.06
epoch train time: 0:00:00.696695
elapsed time: 0:01:35.506450
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 22:45:13.967241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.12
 ---- batch: 020 ----
mean loss: 259.44
 ---- batch: 030 ----
mean loss: 252.91
 ---- batch: 040 ----
mean loss: 254.75
 ---- batch: 050 ----
mean loss: 250.61
 ---- batch: 060 ----
mean loss: 256.80
 ---- batch: 070 ----
mean loss: 256.43
 ---- batch: 080 ----
mean loss: 256.38
 ---- batch: 090 ----
mean loss: 241.41
train mean loss: 252.42
epoch train time: 0:00:00.693108
elapsed time: 0:01:36.199710
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 22:45:14.660519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.29
 ---- batch: 020 ----
mean loss: 246.65
 ---- batch: 030 ----
mean loss: 258.23
 ---- batch: 040 ----
mean loss: 241.36
 ---- batch: 050 ----
mean loss: 250.09
 ---- batch: 060 ----
mean loss: 257.29
 ---- batch: 070 ----
mean loss: 251.70
 ---- batch: 080 ----
mean loss: 246.68
 ---- batch: 090 ----
mean loss: 259.02
train mean loss: 251.16
epoch train time: 0:00:00.699802
elapsed time: 0:01:36.899681
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 22:45:15.360475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.52
 ---- batch: 020 ----
mean loss: 243.30
 ---- batch: 030 ----
mean loss: 249.14
 ---- batch: 040 ----
mean loss: 253.35
 ---- batch: 050 ----
mean loss: 252.88
 ---- batch: 060 ----
mean loss: 251.55
 ---- batch: 070 ----
mean loss: 244.52
 ---- batch: 080 ----
mean loss: 249.77
 ---- batch: 090 ----
mean loss: 250.64
train mean loss: 250.27
epoch train time: 0:00:00.680544
elapsed time: 0:01:37.580393
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 22:45:16.041186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.58
 ---- batch: 020 ----
mean loss: 245.05
 ---- batch: 030 ----
mean loss: 250.68
 ---- batch: 040 ----
mean loss: 249.36
 ---- batch: 050 ----
mean loss: 247.62
 ---- batch: 060 ----
mean loss: 250.14
 ---- batch: 070 ----
mean loss: 252.67
 ---- batch: 080 ----
mean loss: 246.18
 ---- batch: 090 ----
mean loss: 254.85
train mean loss: 249.88
epoch train time: 0:00:00.670317
elapsed time: 0:01:38.250859
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 22:45:16.711647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.02
 ---- batch: 020 ----
mean loss: 244.73
 ---- batch: 030 ----
mean loss: 244.85
 ---- batch: 040 ----
mean loss: 250.21
 ---- batch: 050 ----
mean loss: 248.54
 ---- batch: 060 ----
mean loss: 241.71
 ---- batch: 070 ----
mean loss: 254.11
 ---- batch: 080 ----
mean loss: 252.96
 ---- batch: 090 ----
mean loss: 256.70
train mean loss: 248.66
epoch train time: 0:00:00.673805
elapsed time: 0:01:38.924804
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 22:45:17.385591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.90
 ---- batch: 020 ----
mean loss: 252.06
 ---- batch: 030 ----
mean loss: 243.29
 ---- batch: 040 ----
mean loss: 249.57
 ---- batch: 050 ----
mean loss: 243.16
 ---- batch: 060 ----
mean loss: 250.06
 ---- batch: 070 ----
mean loss: 255.23
 ---- batch: 080 ----
mean loss: 249.13
 ---- batch: 090 ----
mean loss: 243.28
train mean loss: 248.36
epoch train time: 0:00:00.673326
elapsed time: 0:01:39.598280
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 22:45:18.059073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.95
 ---- batch: 020 ----
mean loss: 239.38
 ---- batch: 030 ----
mean loss: 245.63
 ---- batch: 040 ----
mean loss: 244.95
 ---- batch: 050 ----
mean loss: 252.38
 ---- batch: 060 ----
mean loss: 250.62
 ---- batch: 070 ----
mean loss: 246.23
 ---- batch: 080 ----
mean loss: 250.41
 ---- batch: 090 ----
mean loss: 248.34
train mean loss: 247.94
epoch train time: 0:00:00.665031
elapsed time: 0:01:40.263473
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 22:45:18.724262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.20
 ---- batch: 020 ----
mean loss: 241.30
 ---- batch: 030 ----
mean loss: 245.20
 ---- batch: 040 ----
mean loss: 245.38
 ---- batch: 050 ----
mean loss: 257.42
 ---- batch: 060 ----
mean loss: 242.29
 ---- batch: 070 ----
mean loss: 238.51
 ---- batch: 080 ----
mean loss: 245.28
 ---- batch: 090 ----
mean loss: 248.49
train mean loss: 247.23
epoch train time: 0:00:00.674719
elapsed time: 0:01:40.938367
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 22:45:19.399179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.99
 ---- batch: 020 ----
mean loss: 249.54
 ---- batch: 030 ----
mean loss: 242.71
 ---- batch: 040 ----
mean loss: 245.10
 ---- batch: 050 ----
mean loss: 252.21
 ---- batch: 060 ----
mean loss: 250.96
 ---- batch: 070 ----
mean loss: 244.19
 ---- batch: 080 ----
mean loss: 243.10
 ---- batch: 090 ----
mean loss: 245.27
train mean loss: 246.67
epoch train time: 0:00:00.685226
elapsed time: 0:01:41.623773
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 22:45:20.084565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.53
 ---- batch: 020 ----
mean loss: 243.45
 ---- batch: 030 ----
mean loss: 246.92
 ---- batch: 040 ----
mean loss: 246.76
 ---- batch: 050 ----
mean loss: 240.79
 ---- batch: 060 ----
mean loss: 249.15
 ---- batch: 070 ----
mean loss: 244.16
 ---- batch: 080 ----
mean loss: 248.12
 ---- batch: 090 ----
mean loss: 248.90
train mean loss: 246.19
epoch train time: 0:00:00.686000
elapsed time: 0:01:42.309930
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 22:45:20.770731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.60
 ---- batch: 020 ----
mean loss: 250.52
 ---- batch: 030 ----
mean loss: 241.18
 ---- batch: 040 ----
mean loss: 244.50
 ---- batch: 050 ----
mean loss: 239.83
 ---- batch: 060 ----
mean loss: 248.50
 ---- batch: 070 ----
mean loss: 250.75
 ---- batch: 080 ----
mean loss: 240.17
 ---- batch: 090 ----
mean loss: 243.82
train mean loss: 245.75
epoch train time: 0:00:00.679509
elapsed time: 0:01:42.989597
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 22:45:21.450387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.05
 ---- batch: 020 ----
mean loss: 246.80
 ---- batch: 030 ----
mean loss: 240.99
 ---- batch: 040 ----
mean loss: 245.46
 ---- batch: 050 ----
mean loss: 244.29
 ---- batch: 060 ----
mean loss: 250.35
 ---- batch: 070 ----
mean loss: 238.73
 ---- batch: 080 ----
mean loss: 244.96
 ---- batch: 090 ----
mean loss: 240.60
train mean loss: 245.27
epoch train time: 0:00:00.680398
elapsed time: 0:01:43.670161
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 22:45:22.130962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.19
 ---- batch: 020 ----
mean loss: 243.45
 ---- batch: 030 ----
mean loss: 253.26
 ---- batch: 040 ----
mean loss: 243.10
 ---- batch: 050 ----
mean loss: 236.06
 ---- batch: 060 ----
mean loss: 250.20
 ---- batch: 070 ----
mean loss: 241.16
 ---- batch: 080 ----
mean loss: 242.62
 ---- batch: 090 ----
mean loss: 247.78
train mean loss: 244.63
epoch train time: 0:00:00.671695
elapsed time: 0:01:44.342014
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 22:45:22.802802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.42
 ---- batch: 020 ----
mean loss: 248.98
 ---- batch: 030 ----
mean loss: 245.99
 ---- batch: 040 ----
mean loss: 234.02
 ---- batch: 050 ----
mean loss: 249.04
 ---- batch: 060 ----
mean loss: 245.18
 ---- batch: 070 ----
mean loss: 242.72
 ---- batch: 080 ----
mean loss: 241.26
 ---- batch: 090 ----
mean loss: 241.68
train mean loss: 243.95
epoch train time: 0:00:00.666586
elapsed time: 0:01:45.008758
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 22:45:23.469534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.89
 ---- batch: 020 ----
mean loss: 244.43
 ---- batch: 030 ----
mean loss: 242.03
 ---- batch: 040 ----
mean loss: 245.65
 ---- batch: 050 ----
mean loss: 238.45
 ---- batch: 060 ----
mean loss: 245.74
 ---- batch: 070 ----
mean loss: 246.46
 ---- batch: 080 ----
mean loss: 249.36
 ---- batch: 090 ----
mean loss: 243.81
train mean loss: 243.38
epoch train time: 0:00:00.673140
elapsed time: 0:01:45.682074
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 22:45:24.142902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.63
 ---- batch: 020 ----
mean loss: 243.98
 ---- batch: 030 ----
mean loss: 241.71
 ---- batch: 040 ----
mean loss: 240.80
 ---- batch: 050 ----
mean loss: 234.15
 ---- batch: 060 ----
mean loss: 243.80
 ---- batch: 070 ----
mean loss: 246.18
 ---- batch: 080 ----
mean loss: 242.68
 ---- batch: 090 ----
mean loss: 243.03
train mean loss: 243.00
epoch train time: 0:00:00.681948
elapsed time: 0:01:46.364209
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 22:45:24.825018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.08
 ---- batch: 020 ----
mean loss: 240.83
 ---- batch: 030 ----
mean loss: 245.02
 ---- batch: 040 ----
mean loss: 237.90
 ---- batch: 050 ----
mean loss: 242.75
 ---- batch: 060 ----
mean loss: 245.07
 ---- batch: 070 ----
mean loss: 247.31
 ---- batch: 080 ----
mean loss: 239.95
 ---- batch: 090 ----
mean loss: 239.67
train mean loss: 243.77
epoch train time: 0:00:00.672762
elapsed time: 0:01:47.037134
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 22:45:25.497930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.24
 ---- batch: 020 ----
mean loss: 243.93
 ---- batch: 030 ----
mean loss: 245.68
 ---- batch: 040 ----
mean loss: 241.11
 ---- batch: 050 ----
mean loss: 241.10
 ---- batch: 060 ----
mean loss: 247.01
 ---- batch: 070 ----
mean loss: 240.67
 ---- batch: 080 ----
mean loss: 240.84
 ---- batch: 090 ----
mean loss: 240.17
train mean loss: 242.58
epoch train time: 0:00:00.671313
elapsed time: 0:01:47.708616
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 22:45:26.169406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.59
 ---- batch: 020 ----
mean loss: 242.53
 ---- batch: 030 ----
mean loss: 248.08
 ---- batch: 040 ----
mean loss: 249.49
 ---- batch: 050 ----
mean loss: 243.48
 ---- batch: 060 ----
mean loss: 247.88
 ---- batch: 070 ----
mean loss: 245.22
 ---- batch: 080 ----
mean loss: 236.99
 ---- batch: 090 ----
mean loss: 236.01
train mean loss: 242.17
epoch train time: 0:00:00.669064
elapsed time: 0:01:48.377827
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 22:45:26.838615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.79
 ---- batch: 020 ----
mean loss: 241.55
 ---- batch: 030 ----
mean loss: 234.37
 ---- batch: 040 ----
mean loss: 236.24
 ---- batch: 050 ----
mean loss: 236.58
 ---- batch: 060 ----
mean loss: 246.95
 ---- batch: 070 ----
mean loss: 254.22
 ---- batch: 080 ----
mean loss: 245.18
 ---- batch: 090 ----
mean loss: 242.00
train mean loss: 242.38
epoch train time: 0:00:00.671211
elapsed time: 0:01:49.049185
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 22:45:27.509974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.40
 ---- batch: 020 ----
mean loss: 237.43
 ---- batch: 030 ----
mean loss: 241.87
 ---- batch: 040 ----
mean loss: 244.66
 ---- batch: 050 ----
mean loss: 237.57
 ---- batch: 060 ----
mean loss: 246.80
 ---- batch: 070 ----
mean loss: 237.17
 ---- batch: 080 ----
mean loss: 242.36
 ---- batch: 090 ----
mean loss: 242.98
train mean loss: 242.01
epoch train time: 0:00:00.667258
elapsed time: 0:01:49.716588
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 22:45:28.177378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.38
 ---- batch: 020 ----
mean loss: 243.70
 ---- batch: 030 ----
mean loss: 235.92
 ---- batch: 040 ----
mean loss: 241.72
 ---- batch: 050 ----
mean loss: 236.17
 ---- batch: 060 ----
mean loss: 244.35
 ---- batch: 070 ----
mean loss: 242.72
 ---- batch: 080 ----
mean loss: 233.38
 ---- batch: 090 ----
mean loss: 241.60
train mean loss: 241.05
epoch train time: 0:00:00.665902
elapsed time: 0:01:50.382632
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 22:45:28.843418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.08
 ---- batch: 020 ----
mean loss: 242.18
 ---- batch: 030 ----
mean loss: 235.31
 ---- batch: 040 ----
mean loss: 231.09
 ---- batch: 050 ----
mean loss: 253.37
 ---- batch: 060 ----
mean loss: 243.10
 ---- batch: 070 ----
mean loss: 245.50
 ---- batch: 080 ----
mean loss: 237.45
 ---- batch: 090 ----
mean loss: 237.47
train mean loss: 241.03
epoch train time: 0:00:00.672714
elapsed time: 0:01:51.055524
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 22:45:29.516345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.28
 ---- batch: 020 ----
mean loss: 231.83
 ---- batch: 030 ----
mean loss: 243.36
 ---- batch: 040 ----
mean loss: 238.04
 ---- batch: 050 ----
mean loss: 242.99
 ---- batch: 060 ----
mean loss: 244.53
 ---- batch: 070 ----
mean loss: 249.72
 ---- batch: 080 ----
mean loss: 248.34
 ---- batch: 090 ----
mean loss: 246.46
train mean loss: 240.11
epoch train time: 0:00:00.684271
elapsed time: 0:01:51.739973
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 22:45:30.200763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.48
 ---- batch: 020 ----
mean loss: 234.36
 ---- batch: 030 ----
mean loss: 243.22
 ---- batch: 040 ----
mean loss: 240.11
 ---- batch: 050 ----
mean loss: 240.51
 ---- batch: 060 ----
mean loss: 232.00
 ---- batch: 070 ----
mean loss: 243.53
 ---- batch: 080 ----
mean loss: 243.36
 ---- batch: 090 ----
mean loss: 249.26
train mean loss: 240.47
epoch train time: 0:00:00.698620
elapsed time: 0:01:52.438748
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 22:45:30.899543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.61
 ---- batch: 020 ----
mean loss: 240.59
 ---- batch: 030 ----
mean loss: 242.80
 ---- batch: 040 ----
mean loss: 247.25
 ---- batch: 050 ----
mean loss: 243.84
 ---- batch: 060 ----
mean loss: 242.68
 ---- batch: 070 ----
mean loss: 237.34
 ---- batch: 080 ----
mean loss: 243.74
 ---- batch: 090 ----
mean loss: 228.79
train mean loss: 240.35
epoch train time: 0:00:00.684816
elapsed time: 0:01:53.123726
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 22:45:31.584520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.63
 ---- batch: 020 ----
mean loss: 236.69
 ---- batch: 030 ----
mean loss: 237.55
 ---- batch: 040 ----
mean loss: 241.09
 ---- batch: 050 ----
mean loss: 232.15
 ---- batch: 060 ----
mean loss: 244.04
 ---- batch: 070 ----
mean loss: 239.25
 ---- batch: 080 ----
mean loss: 244.49
 ---- batch: 090 ----
mean loss: 238.90
train mean loss: 240.02
epoch train time: 0:00:00.677240
elapsed time: 0:01:53.801122
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 22:45:32.261922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.34
 ---- batch: 020 ----
mean loss: 238.76
 ---- batch: 030 ----
mean loss: 238.51
 ---- batch: 040 ----
mean loss: 246.57
 ---- batch: 050 ----
mean loss: 237.91
 ---- batch: 060 ----
mean loss: 242.26
 ---- batch: 070 ----
mean loss: 235.75
 ---- batch: 080 ----
mean loss: 241.98
 ---- batch: 090 ----
mean loss: 240.54
train mean loss: 239.46
epoch train time: 0:00:00.674648
elapsed time: 0:01:54.475942
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 22:45:32.936765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.65
 ---- batch: 020 ----
mean loss: 238.66
 ---- batch: 030 ----
mean loss: 235.63
 ---- batch: 040 ----
mean loss: 238.32
 ---- batch: 050 ----
mean loss: 240.00
 ---- batch: 060 ----
mean loss: 239.81
 ---- batch: 070 ----
mean loss: 241.95
 ---- batch: 080 ----
mean loss: 246.43
 ---- batch: 090 ----
mean loss: 242.15
train mean loss: 239.00
epoch train time: 0:00:00.678518
elapsed time: 0:01:55.154643
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 22:45:33.615434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.28
 ---- batch: 020 ----
mean loss: 243.34
 ---- batch: 030 ----
mean loss: 233.65
 ---- batch: 040 ----
mean loss: 239.76
 ---- batch: 050 ----
mean loss: 242.18
 ---- batch: 060 ----
mean loss: 238.26
 ---- batch: 070 ----
mean loss: 229.87
 ---- batch: 080 ----
mean loss: 241.07
 ---- batch: 090 ----
mean loss: 236.53
train mean loss: 238.71
epoch train time: 0:00:00.685671
elapsed time: 0:01:55.840464
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 22:45:34.301255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.30
 ---- batch: 020 ----
mean loss: 233.77
 ---- batch: 030 ----
mean loss: 233.47
 ---- batch: 040 ----
mean loss: 240.88
 ---- batch: 050 ----
mean loss: 238.76
 ---- batch: 060 ----
mean loss: 236.00
 ---- batch: 070 ----
mean loss: 240.11
 ---- batch: 080 ----
mean loss: 243.22
 ---- batch: 090 ----
mean loss: 240.63
train mean loss: 238.64
epoch train time: 0:00:00.676425
elapsed time: 0:01:56.517039
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 22:45:34.977848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.16
 ---- batch: 020 ----
mean loss: 233.22
 ---- batch: 030 ----
mean loss: 235.02
 ---- batch: 040 ----
mean loss: 239.79
 ---- batch: 050 ----
mean loss: 236.01
 ---- batch: 060 ----
mean loss: 248.95
 ---- batch: 070 ----
mean loss: 239.69
 ---- batch: 080 ----
mean loss: 238.43
 ---- batch: 090 ----
mean loss: 240.48
train mean loss: 238.03
epoch train time: 0:00:00.669556
elapsed time: 0:01:57.186761
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 22:45:35.647553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.43
 ---- batch: 020 ----
mean loss: 238.17
 ---- batch: 030 ----
mean loss: 228.29
 ---- batch: 040 ----
mean loss: 231.19
 ---- batch: 050 ----
mean loss: 241.85
 ---- batch: 060 ----
mean loss: 239.03
 ---- batch: 070 ----
mean loss: 236.29
 ---- batch: 080 ----
mean loss: 245.68
 ---- batch: 090 ----
mean loss: 245.12
train mean loss: 237.73
epoch train time: 0:00:00.674634
elapsed time: 0:01:57.861551
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 22:45:36.322343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.66
 ---- batch: 020 ----
mean loss: 237.11
 ---- batch: 030 ----
mean loss: 242.61
 ---- batch: 040 ----
mean loss: 235.56
 ---- batch: 050 ----
mean loss: 238.30
 ---- batch: 060 ----
mean loss: 234.42
 ---- batch: 070 ----
mean loss: 235.35
 ---- batch: 080 ----
mean loss: 243.51
 ---- batch: 090 ----
mean loss: 243.52
train mean loss: 237.87
epoch train time: 0:00:00.677078
elapsed time: 0:01:58.538790
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 22:45:36.999564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.81
 ---- batch: 020 ----
mean loss: 230.90
 ---- batch: 030 ----
mean loss: 234.12
 ---- batch: 040 ----
mean loss: 241.75
 ---- batch: 050 ----
mean loss: 241.32
 ---- batch: 060 ----
mean loss: 239.12
 ---- batch: 070 ----
mean loss: 228.86
 ---- batch: 080 ----
mean loss: 236.18
 ---- batch: 090 ----
mean loss: 246.77
train mean loss: 237.54
epoch train time: 0:00:00.672705
elapsed time: 0:01:59.211641
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 22:45:37.672471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.76
 ---- batch: 020 ----
mean loss: 229.73
 ---- batch: 030 ----
mean loss: 233.84
 ---- batch: 040 ----
mean loss: 236.61
 ---- batch: 050 ----
mean loss: 234.63
 ---- batch: 060 ----
mean loss: 233.70
 ---- batch: 070 ----
mean loss: 238.33
 ---- batch: 080 ----
mean loss: 243.21
 ---- batch: 090 ----
mean loss: 243.55
train mean loss: 237.27
epoch train time: 0:00:00.679219
elapsed time: 0:01:59.891051
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 22:45:38.351846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.23
 ---- batch: 020 ----
mean loss: 236.81
 ---- batch: 030 ----
mean loss: 230.10
 ---- batch: 040 ----
mean loss: 234.24
 ---- batch: 050 ----
mean loss: 230.83
 ---- batch: 060 ----
mean loss: 230.62
 ---- batch: 070 ----
mean loss: 238.07
 ---- batch: 080 ----
mean loss: 245.26
 ---- batch: 090 ----
mean loss: 242.27
train mean loss: 236.92
epoch train time: 0:00:00.673946
elapsed time: 0:02:00.565149
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 22:45:39.025946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.98
 ---- batch: 020 ----
mean loss: 235.01
 ---- batch: 030 ----
mean loss: 233.63
 ---- batch: 040 ----
mean loss: 246.73
 ---- batch: 050 ----
mean loss: 233.91
 ---- batch: 060 ----
mean loss: 234.11
 ---- batch: 070 ----
mean loss: 235.81
 ---- batch: 080 ----
mean loss: 236.00
 ---- batch: 090 ----
mean loss: 236.64
train mean loss: 236.50
epoch train time: 0:00:00.674441
elapsed time: 0:02:01.239808
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 22:45:39.700627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.86
 ---- batch: 020 ----
mean loss: 231.61
 ---- batch: 030 ----
mean loss: 234.70
 ---- batch: 040 ----
mean loss: 237.29
 ---- batch: 050 ----
mean loss: 239.27
 ---- batch: 060 ----
mean loss: 231.90
 ---- batch: 070 ----
mean loss: 239.84
 ---- batch: 080 ----
mean loss: 246.28
 ---- batch: 090 ----
mean loss: 234.97
train mean loss: 236.61
epoch train time: 0:00:00.673856
elapsed time: 0:02:01.913839
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 22:45:40.374627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.94
 ---- batch: 020 ----
mean loss: 236.03
 ---- batch: 030 ----
mean loss: 230.22
 ---- batch: 040 ----
mean loss: 230.92
 ---- batch: 050 ----
mean loss: 238.82
 ---- batch: 060 ----
mean loss: 240.42
 ---- batch: 070 ----
mean loss: 232.26
 ---- batch: 080 ----
mean loss: 240.87
 ---- batch: 090 ----
mean loss: 235.40
train mean loss: 236.08
epoch train time: 0:00:00.670146
elapsed time: 0:02:02.584129
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 22:45:41.044943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.68
 ---- batch: 020 ----
mean loss: 225.36
 ---- batch: 030 ----
mean loss: 234.99
 ---- batch: 040 ----
mean loss: 237.00
 ---- batch: 050 ----
mean loss: 237.99
 ---- batch: 060 ----
mean loss: 231.38
 ---- batch: 070 ----
mean loss: 242.97
 ---- batch: 080 ----
mean loss: 237.68
 ---- batch: 090 ----
mean loss: 236.95
train mean loss: 236.19
epoch train time: 0:00:00.668143
elapsed time: 0:02:03.252476
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 22:45:41.713265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.69
 ---- batch: 020 ----
mean loss: 234.70
 ---- batch: 030 ----
mean loss: 238.89
 ---- batch: 040 ----
mean loss: 231.20
 ---- batch: 050 ----
mean loss: 232.68
 ---- batch: 060 ----
mean loss: 244.37
 ---- batch: 070 ----
mean loss: 238.18
 ---- batch: 080 ----
mean loss: 233.79
 ---- batch: 090 ----
mean loss: 232.44
train mean loss: 235.95
epoch train time: 0:00:00.671479
elapsed time: 0:02:03.924100
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 22:45:42.384890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.03
 ---- batch: 020 ----
mean loss: 233.74
 ---- batch: 030 ----
mean loss: 229.78
 ---- batch: 040 ----
mean loss: 237.33
 ---- batch: 050 ----
mean loss: 230.31
 ---- batch: 060 ----
mean loss: 233.25
 ---- batch: 070 ----
mean loss: 237.29
 ---- batch: 080 ----
mean loss: 239.72
 ---- batch: 090 ----
mean loss: 238.04
train mean loss: 235.34
epoch train time: 0:00:00.671794
elapsed time: 0:02:04.596041
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 22:45:43.056828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.82
 ---- batch: 020 ----
mean loss: 235.24
 ---- batch: 030 ----
mean loss: 236.79
 ---- batch: 040 ----
mean loss: 231.06
 ---- batch: 050 ----
mean loss: 232.42
 ---- batch: 060 ----
mean loss: 234.08
 ---- batch: 070 ----
mean loss: 229.72
 ---- batch: 080 ----
mean loss: 236.69
 ---- batch: 090 ----
mean loss: 237.19
train mean loss: 235.05
epoch train time: 0:00:00.666413
elapsed time: 0:02:05.262602
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 22:45:43.723392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.17
 ---- batch: 020 ----
mean loss: 236.27
 ---- batch: 030 ----
mean loss: 234.00
 ---- batch: 040 ----
mean loss: 242.56
 ---- batch: 050 ----
mean loss: 235.71
 ---- batch: 060 ----
mean loss: 241.10
 ---- batch: 070 ----
mean loss: 237.55
 ---- batch: 080 ----
mean loss: 233.08
 ---- batch: 090 ----
mean loss: 227.53
train mean loss: 234.88
epoch train time: 0:00:00.680367
elapsed time: 0:02:05.943120
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 22:45:44.403927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.40
 ---- batch: 020 ----
mean loss: 235.05
 ---- batch: 030 ----
mean loss: 237.88
 ---- batch: 040 ----
mean loss: 231.72
 ---- batch: 050 ----
mean loss: 239.58
 ---- batch: 060 ----
mean loss: 231.58
 ---- batch: 070 ----
mean loss: 231.76
 ---- batch: 080 ----
mean loss: 236.27
 ---- batch: 090 ----
mean loss: 241.91
train mean loss: 234.79
epoch train time: 0:00:00.683561
elapsed time: 0:02:06.626842
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 22:45:45.087650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.75
 ---- batch: 020 ----
mean loss: 234.85
 ---- batch: 030 ----
mean loss: 239.31
 ---- batch: 040 ----
mean loss: 224.68
 ---- batch: 050 ----
mean loss: 231.33
 ---- batch: 060 ----
mean loss: 237.29
 ---- batch: 070 ----
mean loss: 236.01
 ---- batch: 080 ----
mean loss: 244.92
 ---- batch: 090 ----
mean loss: 229.21
train mean loss: 233.91
epoch train time: 0:00:00.669209
elapsed time: 0:02:07.296217
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 22:45:45.757009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.61
 ---- batch: 020 ----
mean loss: 234.42
 ---- batch: 030 ----
mean loss: 234.55
 ---- batch: 040 ----
mean loss: 236.47
 ---- batch: 050 ----
mean loss: 232.67
 ---- batch: 060 ----
mean loss: 237.21
 ---- batch: 070 ----
mean loss: 225.52
 ---- batch: 080 ----
mean loss: 235.56
 ---- batch: 090 ----
mean loss: 239.61
train mean loss: 234.46
epoch train time: 0:00:00.676255
elapsed time: 0:02:07.972624
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 22:45:46.433431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.84
 ---- batch: 020 ----
mean loss: 238.23
 ---- batch: 030 ----
mean loss: 237.63
 ---- batch: 040 ----
mean loss: 225.62
 ---- batch: 050 ----
mean loss: 237.98
 ---- batch: 060 ----
mean loss: 230.98
 ---- batch: 070 ----
mean loss: 234.74
 ---- batch: 080 ----
mean loss: 231.25
 ---- batch: 090 ----
mean loss: 234.56
train mean loss: 234.14
epoch train time: 0:00:00.672440
elapsed time: 0:02:08.645264
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 22:45:47.106053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.92
 ---- batch: 020 ----
mean loss: 232.74
 ---- batch: 030 ----
mean loss: 235.88
 ---- batch: 040 ----
mean loss: 232.85
 ---- batch: 050 ----
mean loss: 236.45
 ---- batch: 060 ----
mean loss: 238.54
 ---- batch: 070 ----
mean loss: 232.64
 ---- batch: 080 ----
mean loss: 230.72
 ---- batch: 090 ----
mean loss: 230.90
train mean loss: 233.63
epoch train time: 0:00:00.667874
elapsed time: 0:02:09.313282
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 22:45:47.774071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.44
 ---- batch: 020 ----
mean loss: 238.61
 ---- batch: 030 ----
mean loss: 225.75
 ---- batch: 040 ----
mean loss: 236.87
 ---- batch: 050 ----
mean loss: 230.97
 ---- batch: 060 ----
mean loss: 227.16
 ---- batch: 070 ----
mean loss: 235.79
 ---- batch: 080 ----
mean loss: 228.51
 ---- batch: 090 ----
mean loss: 227.49
train mean loss: 233.47
epoch train time: 0:00:00.671432
elapsed time: 0:02:09.984861
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 22:45:48.445709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.56
 ---- batch: 020 ----
mean loss: 239.02
 ---- batch: 030 ----
mean loss: 231.78
 ---- batch: 040 ----
mean loss: 234.77
 ---- batch: 050 ----
mean loss: 224.22
 ---- batch: 060 ----
mean loss: 239.26
 ---- batch: 070 ----
mean loss: 234.80
 ---- batch: 080 ----
mean loss: 232.06
 ---- batch: 090 ----
mean loss: 237.64
train mean loss: 233.04
epoch train time: 0:00:00.671216
elapsed time: 0:02:10.656286
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 22:45:49.117093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.80
 ---- batch: 020 ----
mean loss: 234.34
 ---- batch: 030 ----
mean loss: 226.17
 ---- batch: 040 ----
mean loss: 239.43
 ---- batch: 050 ----
mean loss: 234.39
 ---- batch: 060 ----
mean loss: 239.75
 ---- batch: 070 ----
mean loss: 229.61
 ---- batch: 080 ----
mean loss: 237.64
 ---- batch: 090 ----
mean loss: 230.34
train mean loss: 232.66
epoch train time: 0:00:00.664947
elapsed time: 0:02:11.321412
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 22:45:49.782217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.08
 ---- batch: 020 ----
mean loss: 235.78
 ---- batch: 030 ----
mean loss: 234.18
 ---- batch: 040 ----
mean loss: 233.86
 ---- batch: 050 ----
mean loss: 228.99
 ---- batch: 060 ----
mean loss: 231.76
 ---- batch: 070 ----
mean loss: 232.50
 ---- batch: 080 ----
mean loss: 229.54
 ---- batch: 090 ----
mean loss: 228.71
train mean loss: 232.93
epoch train time: 0:00:00.668982
elapsed time: 0:02:11.990555
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 22:45:50.451358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.17
 ---- batch: 020 ----
mean loss: 241.56
 ---- batch: 030 ----
mean loss: 230.51
 ---- batch: 040 ----
mean loss: 232.35
 ---- batch: 050 ----
mean loss: 232.56
 ---- batch: 060 ----
mean loss: 231.03
 ---- batch: 070 ----
mean loss: 222.24
 ---- batch: 080 ----
mean loss: 236.43
 ---- batch: 090 ----
mean loss: 237.35
train mean loss: 232.76
epoch train time: 0:00:00.667121
elapsed time: 0:02:12.657836
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 22:45:51.118628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.01
 ---- batch: 020 ----
mean loss: 232.23
 ---- batch: 030 ----
mean loss: 230.91
 ---- batch: 040 ----
mean loss: 236.38
 ---- batch: 050 ----
mean loss: 228.57
 ---- batch: 060 ----
mean loss: 226.81
 ---- batch: 070 ----
mean loss: 232.55
 ---- batch: 080 ----
mean loss: 235.39
 ---- batch: 090 ----
mean loss: 231.23
train mean loss: 232.27
epoch train time: 0:00:00.670010
elapsed time: 0:02:13.328015
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 22:45:51.788792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.37
 ---- batch: 020 ----
mean loss: 229.71
 ---- batch: 030 ----
mean loss: 230.15
 ---- batch: 040 ----
mean loss: 232.51
 ---- batch: 050 ----
mean loss: 236.76
 ---- batch: 060 ----
mean loss: 239.78
 ---- batch: 070 ----
mean loss: 232.54
 ---- batch: 080 ----
mean loss: 226.13
 ---- batch: 090 ----
mean loss: 239.13
train mean loss: 232.56
epoch train time: 0:00:00.671670
elapsed time: 0:02:13.999821
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 22:45:52.460612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.87
 ---- batch: 020 ----
mean loss: 237.75
 ---- batch: 030 ----
mean loss: 229.25
 ---- batch: 040 ----
mean loss: 237.36
 ---- batch: 050 ----
mean loss: 222.84
 ---- batch: 060 ----
mean loss: 232.34
 ---- batch: 070 ----
mean loss: 227.00
 ---- batch: 080 ----
mean loss: 236.69
 ---- batch: 090 ----
mean loss: 226.61
train mean loss: 231.91
epoch train time: 0:00:00.661250
elapsed time: 0:02:14.661224
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 22:45:53.122030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.62
 ---- batch: 020 ----
mean loss: 233.91
 ---- batch: 030 ----
mean loss: 234.51
 ---- batch: 040 ----
mean loss: 229.47
 ---- batch: 050 ----
mean loss: 226.18
 ---- batch: 060 ----
mean loss: 233.45
 ---- batch: 070 ----
mean loss: 230.70
 ---- batch: 080 ----
mean loss: 233.37
 ---- batch: 090 ----
mean loss: 241.84
train mean loss: 231.79
epoch train time: 0:00:00.662444
elapsed time: 0:02:15.323837
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 22:45:53.784630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.20
 ---- batch: 020 ----
mean loss: 242.30
 ---- batch: 030 ----
mean loss: 233.60
 ---- batch: 040 ----
mean loss: 227.58
 ---- batch: 050 ----
mean loss: 230.80
 ---- batch: 060 ----
mean loss: 226.91
 ---- batch: 070 ----
mean loss: 228.50
 ---- batch: 080 ----
mean loss: 230.79
 ---- batch: 090 ----
mean loss: 233.28
train mean loss: 231.05
epoch train time: 0:00:00.668123
elapsed time: 0:02:15.992107
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 22:45:54.452894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.78
 ---- batch: 020 ----
mean loss: 224.10
 ---- batch: 030 ----
mean loss: 240.44
 ---- batch: 040 ----
mean loss: 240.05
 ---- batch: 050 ----
mean loss: 231.72
 ---- batch: 060 ----
mean loss: 227.52
 ---- batch: 070 ----
mean loss: 226.79
 ---- batch: 080 ----
mean loss: 229.72
 ---- batch: 090 ----
mean loss: 226.64
train mean loss: 231.44
epoch train time: 0:00:00.673089
elapsed time: 0:02:16.665355
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 22:45:55.126163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.26
 ---- batch: 020 ----
mean loss: 228.01
 ---- batch: 030 ----
mean loss: 225.07
 ---- batch: 040 ----
mean loss: 235.20
 ---- batch: 050 ----
mean loss: 231.83
 ---- batch: 060 ----
mean loss: 239.51
 ---- batch: 070 ----
mean loss: 227.65
 ---- batch: 080 ----
mean loss: 233.99
 ---- batch: 090 ----
mean loss: 228.60
train mean loss: 231.01
epoch train time: 0:00:00.672585
elapsed time: 0:02:17.338108
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 22:45:55.798902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.22
 ---- batch: 020 ----
mean loss: 232.95
 ---- batch: 030 ----
mean loss: 231.72
 ---- batch: 040 ----
mean loss: 224.81
 ---- batch: 050 ----
mean loss: 231.45
 ---- batch: 060 ----
mean loss: 236.68
 ---- batch: 070 ----
mean loss: 233.22
 ---- batch: 080 ----
mean loss: 228.24
 ---- batch: 090 ----
mean loss: 233.88
train mean loss: 231.08
epoch train time: 0:00:00.676579
elapsed time: 0:02:18.014839
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 22:45:56.475628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.73
 ---- batch: 020 ----
mean loss: 222.94
 ---- batch: 030 ----
mean loss: 235.01
 ---- batch: 040 ----
mean loss: 235.54
 ---- batch: 050 ----
mean loss: 223.96
 ---- batch: 060 ----
mean loss: 233.66
 ---- batch: 070 ----
mean loss: 237.81
 ---- batch: 080 ----
mean loss: 238.92
 ---- batch: 090 ----
mean loss: 225.95
train mean loss: 230.83
epoch train time: 0:00:00.672128
elapsed time: 0:02:18.687145
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 22:45:57.147944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.00
 ---- batch: 020 ----
mean loss: 236.14
 ---- batch: 030 ----
mean loss: 228.98
 ---- batch: 040 ----
mean loss: 231.23
 ---- batch: 050 ----
mean loss: 235.97
 ---- batch: 060 ----
mean loss: 227.07
 ---- batch: 070 ----
mean loss: 230.09
 ---- batch: 080 ----
mean loss: 228.16
 ---- batch: 090 ----
mean loss: 229.74
train mean loss: 230.38
epoch train time: 0:00:00.663249
elapsed time: 0:02:19.350551
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 22:45:57.811343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.35
 ---- batch: 020 ----
mean loss: 223.14
 ---- batch: 030 ----
mean loss: 230.41
 ---- batch: 040 ----
mean loss: 228.28
 ---- batch: 050 ----
mean loss: 224.04
 ---- batch: 060 ----
mean loss: 226.80
 ---- batch: 070 ----
mean loss: 230.48
 ---- batch: 080 ----
mean loss: 239.40
 ---- batch: 090 ----
mean loss: 226.55
train mean loss: 230.68
epoch train time: 0:00:00.670658
elapsed time: 0:02:20.021365
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 22:45:58.482155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.74
 ---- batch: 020 ----
mean loss: 224.93
 ---- batch: 030 ----
mean loss: 237.46
 ---- batch: 040 ----
mean loss: 229.67
 ---- batch: 050 ----
mean loss: 235.06
 ---- batch: 060 ----
mean loss: 225.63
 ---- batch: 070 ----
mean loss: 229.66
 ---- batch: 080 ----
mean loss: 228.46
 ---- batch: 090 ----
mean loss: 231.96
train mean loss: 230.17
epoch train time: 0:00:00.669496
elapsed time: 0:02:20.691020
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 22:45:59.151814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.79
 ---- batch: 020 ----
mean loss: 227.81
 ---- batch: 030 ----
mean loss: 224.27
 ---- batch: 040 ----
mean loss: 230.52
 ---- batch: 050 ----
mean loss: 228.74
 ---- batch: 060 ----
mean loss: 231.41
 ---- batch: 070 ----
mean loss: 232.32
 ---- batch: 080 ----
mean loss: 234.47
 ---- batch: 090 ----
mean loss: 233.40
train mean loss: 230.22
epoch train time: 0:00:00.675277
elapsed time: 0:02:21.366451
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 22:45:59.827260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.59
 ---- batch: 020 ----
mean loss: 223.14
 ---- batch: 030 ----
mean loss: 237.08
 ---- batch: 040 ----
mean loss: 222.58
 ---- batch: 050 ----
mean loss: 224.14
 ---- batch: 060 ----
mean loss: 237.07
 ---- batch: 070 ----
mean loss: 238.07
 ---- batch: 080 ----
mean loss: 227.46
 ---- batch: 090 ----
mean loss: 228.91
train mean loss: 229.79
epoch train time: 0:00:00.668469
elapsed time: 0:02:22.035107
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 22:46:00.495901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.65
 ---- batch: 020 ----
mean loss: 235.05
 ---- batch: 030 ----
mean loss: 239.12
 ---- batch: 040 ----
mean loss: 229.65
 ---- batch: 050 ----
mean loss: 222.34
 ---- batch: 060 ----
mean loss: 234.45
 ---- batch: 070 ----
mean loss: 230.06
 ---- batch: 080 ----
mean loss: 230.03
 ---- batch: 090 ----
mean loss: 224.83
train mean loss: 229.49
epoch train time: 0:00:00.680068
elapsed time: 0:02:22.715336
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 22:46:01.176131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.57
 ---- batch: 020 ----
mean loss: 219.79
 ---- batch: 030 ----
mean loss: 230.36
 ---- batch: 040 ----
mean loss: 231.15
 ---- batch: 050 ----
mean loss: 231.68
 ---- batch: 060 ----
mean loss: 232.63
 ---- batch: 070 ----
mean loss: 229.64
 ---- batch: 080 ----
mean loss: 227.46
 ---- batch: 090 ----
mean loss: 232.65
train mean loss: 229.64
epoch train time: 0:00:00.675133
elapsed time: 0:02:23.390616
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 22:46:01.851404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.30
 ---- batch: 020 ----
mean loss: 228.17
 ---- batch: 030 ----
mean loss: 227.89
 ---- batch: 040 ----
mean loss: 221.60
 ---- batch: 050 ----
mean loss: 235.78
 ---- batch: 060 ----
mean loss: 234.28
 ---- batch: 070 ----
mean loss: 230.10
 ---- batch: 080 ----
mean loss: 221.87
 ---- batch: 090 ----
mean loss: 234.02
train mean loss: 229.15
epoch train time: 0:00:00.662763
elapsed time: 0:02:24.053526
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 22:46:02.514315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.63
 ---- batch: 020 ----
mean loss: 227.70
 ---- batch: 030 ----
mean loss: 223.13
 ---- batch: 040 ----
mean loss: 238.78
 ---- batch: 050 ----
mean loss: 229.31
 ---- batch: 060 ----
mean loss: 237.93
 ---- batch: 070 ----
mean loss: 220.85
 ---- batch: 080 ----
mean loss: 220.91
 ---- batch: 090 ----
mean loss: 233.79
train mean loss: 229.08
epoch train time: 0:00:00.664614
elapsed time: 0:02:24.718290
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 22:46:03.179081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.17
 ---- batch: 020 ----
mean loss: 228.23
 ---- batch: 030 ----
mean loss: 231.25
 ---- batch: 040 ----
mean loss: 225.36
 ---- batch: 050 ----
mean loss: 229.89
 ---- batch: 060 ----
mean loss: 221.77
 ---- batch: 070 ----
mean loss: 220.94
 ---- batch: 080 ----
mean loss: 230.09
 ---- batch: 090 ----
mean loss: 234.00
train mean loss: 228.93
epoch train time: 0:00:00.674270
elapsed time: 0:02:25.392729
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 22:46:03.853529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.05
 ---- batch: 020 ----
mean loss: 226.35
 ---- batch: 030 ----
mean loss: 232.60
 ---- batch: 040 ----
mean loss: 221.54
 ---- batch: 050 ----
mean loss: 229.63
 ---- batch: 060 ----
mean loss: 223.02
 ---- batch: 070 ----
mean loss: 229.25
 ---- batch: 080 ----
mean loss: 230.06
 ---- batch: 090 ----
mean loss: 229.50
train mean loss: 228.69
epoch train time: 0:00:00.676159
elapsed time: 0:02:26.069059
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 22:46:04.529847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.79
 ---- batch: 020 ----
mean loss: 227.40
 ---- batch: 030 ----
mean loss: 219.93
 ---- batch: 040 ----
mean loss: 226.26
 ---- batch: 050 ----
mean loss: 229.97
 ---- batch: 060 ----
mean loss: 234.27
 ---- batch: 070 ----
mean loss: 225.05
 ---- batch: 080 ----
mean loss: 230.65
 ---- batch: 090 ----
mean loss: 227.84
train mean loss: 228.72
epoch train time: 0:00:00.665677
elapsed time: 0:02:26.734879
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 22:46:05.195685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.42
 ---- batch: 020 ----
mean loss: 224.81
 ---- batch: 030 ----
mean loss: 225.71
 ---- batch: 040 ----
mean loss: 230.77
 ---- batch: 050 ----
mean loss: 223.84
 ---- batch: 060 ----
mean loss: 229.27
 ---- batch: 070 ----
mean loss: 235.51
 ---- batch: 080 ----
mean loss: 231.36
 ---- batch: 090 ----
mean loss: 225.89
train mean loss: 228.28
epoch train time: 0:00:00.671405
elapsed time: 0:02:27.406451
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 22:46:05.867246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.95
 ---- batch: 020 ----
mean loss: 228.72
 ---- batch: 030 ----
mean loss: 226.11
 ---- batch: 040 ----
mean loss: 232.27
 ---- batch: 050 ----
mean loss: 233.43
 ---- batch: 060 ----
mean loss: 226.10
 ---- batch: 070 ----
mean loss: 230.96
 ---- batch: 080 ----
mean loss: 218.05
 ---- batch: 090 ----
mean loss: 228.96
train mean loss: 227.83
epoch train time: 0:00:00.666093
elapsed time: 0:02:28.072702
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 22:46:06.533492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.80
 ---- batch: 020 ----
mean loss: 226.43
 ---- batch: 030 ----
mean loss: 215.62
 ---- batch: 040 ----
mean loss: 227.02
 ---- batch: 050 ----
mean loss: 228.30
 ---- batch: 060 ----
mean loss: 234.39
 ---- batch: 070 ----
mean loss: 235.35
 ---- batch: 080 ----
mean loss: 223.12
 ---- batch: 090 ----
mean loss: 231.08
train mean loss: 228.09
epoch train time: 0:00:00.667377
elapsed time: 0:02:28.740253
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 22:46:07.201057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.43
 ---- batch: 020 ----
mean loss: 218.60
 ---- batch: 030 ----
mean loss: 231.16
 ---- batch: 040 ----
mean loss: 235.39
 ---- batch: 050 ----
mean loss: 225.20
 ---- batch: 060 ----
mean loss: 223.30
 ---- batch: 070 ----
mean loss: 226.56
 ---- batch: 080 ----
mean loss: 228.58
 ---- batch: 090 ----
mean loss: 227.69
train mean loss: 227.65
epoch train time: 0:00:00.674969
elapsed time: 0:02:29.415391
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 22:46:07.876191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.90
 ---- batch: 020 ----
mean loss: 220.90
 ---- batch: 030 ----
mean loss: 231.92
 ---- batch: 040 ----
mean loss: 222.11
 ---- batch: 050 ----
mean loss: 221.62
 ---- batch: 060 ----
mean loss: 235.63
 ---- batch: 070 ----
mean loss: 229.62
 ---- batch: 080 ----
mean loss: 232.15
 ---- batch: 090 ----
mean loss: 229.06
train mean loss: 227.68
epoch train time: 0:00:00.676513
elapsed time: 0:02:30.092086
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 22:46:08.552860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.80
 ---- batch: 020 ----
mean loss: 230.98
 ---- batch: 030 ----
mean loss: 239.38
 ---- batch: 040 ----
mean loss: 218.92
 ---- batch: 050 ----
mean loss: 222.94
 ---- batch: 060 ----
mean loss: 231.86
 ---- batch: 070 ----
mean loss: 218.45
 ---- batch: 080 ----
mean loss: 228.23
 ---- batch: 090 ----
mean loss: 229.17
train mean loss: 227.36
epoch train time: 0:00:00.665336
elapsed time: 0:02:30.757553
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 22:46:09.218347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.65
 ---- batch: 020 ----
mean loss: 217.57
 ---- batch: 030 ----
mean loss: 226.23
 ---- batch: 040 ----
mean loss: 232.91
 ---- batch: 050 ----
mean loss: 226.09
 ---- batch: 060 ----
mean loss: 229.91
 ---- batch: 070 ----
mean loss: 232.14
 ---- batch: 080 ----
mean loss: 222.23
 ---- batch: 090 ----
mean loss: 226.66
train mean loss: 226.98
epoch train time: 0:00:00.668254
elapsed time: 0:02:31.425960
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 22:46:09.886766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.66
 ---- batch: 020 ----
mean loss: 224.17
 ---- batch: 030 ----
mean loss: 222.67
 ---- batch: 040 ----
mean loss: 232.55
 ---- batch: 050 ----
mean loss: 233.88
 ---- batch: 060 ----
mean loss: 227.64
 ---- batch: 070 ----
mean loss: 224.07
 ---- batch: 080 ----
mean loss: 217.58
 ---- batch: 090 ----
mean loss: 230.42
train mean loss: 226.89
epoch train time: 0:00:00.661789
elapsed time: 0:02:32.087915
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 22:46:10.548734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.68
 ---- batch: 020 ----
mean loss: 233.05
 ---- batch: 030 ----
mean loss: 228.17
 ---- batch: 040 ----
mean loss: 222.56
 ---- batch: 050 ----
mean loss: 218.85
 ---- batch: 060 ----
mean loss: 229.19
 ---- batch: 070 ----
mean loss: 229.30
 ---- batch: 080 ----
mean loss: 225.87
 ---- batch: 090 ----
mean loss: 220.43
train mean loss: 226.34
epoch train time: 0:00:00.669252
elapsed time: 0:02:32.757346
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 22:46:11.218154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.01
 ---- batch: 020 ----
mean loss: 231.46
 ---- batch: 030 ----
mean loss: 227.84
 ---- batch: 040 ----
mean loss: 227.83
 ---- batch: 050 ----
mean loss: 220.31
 ---- batch: 060 ----
mean loss: 231.59
 ---- batch: 070 ----
mean loss: 222.89
 ---- batch: 080 ----
mean loss: 220.96
 ---- batch: 090 ----
mean loss: 225.46
train mean loss: 226.47
epoch train time: 0:00:00.671325
elapsed time: 0:02:33.428837
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 22:46:11.889651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.58
 ---- batch: 020 ----
mean loss: 228.30
 ---- batch: 030 ----
mean loss: 229.01
 ---- batch: 040 ----
mean loss: 223.73
 ---- batch: 050 ----
mean loss: 232.79
 ---- batch: 060 ----
mean loss: 226.12
 ---- batch: 070 ----
mean loss: 218.71
 ---- batch: 080 ----
mean loss: 225.98
 ---- batch: 090 ----
mean loss: 224.36
train mean loss: 226.46
epoch train time: 0:00:00.671669
elapsed time: 0:02:34.100678
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 22:46:12.561477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.73
 ---- batch: 020 ----
mean loss: 224.33
 ---- batch: 030 ----
mean loss: 225.75
 ---- batch: 040 ----
mean loss: 234.31
 ---- batch: 050 ----
mean loss: 223.94
 ---- batch: 060 ----
mean loss: 222.85
 ---- batch: 070 ----
mean loss: 223.02
 ---- batch: 080 ----
mean loss: 227.90
 ---- batch: 090 ----
mean loss: 225.27
train mean loss: 226.20
epoch train time: 0:00:00.673846
elapsed time: 0:02:34.774696
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 22:46:13.235499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.18
 ---- batch: 020 ----
mean loss: 231.91
 ---- batch: 030 ----
mean loss: 227.55
 ---- batch: 040 ----
mean loss: 228.79
 ---- batch: 050 ----
mean loss: 219.91
 ---- batch: 060 ----
mean loss: 220.11
 ---- batch: 070 ----
mean loss: 227.86
 ---- batch: 080 ----
mean loss: 226.79
 ---- batch: 090 ----
mean loss: 225.13
train mean loss: 225.97
epoch train time: 0:00:00.703253
elapsed time: 0:02:35.478124
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 22:46:13.938920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.91
 ---- batch: 020 ----
mean loss: 225.17
 ---- batch: 030 ----
mean loss: 233.27
 ---- batch: 040 ----
mean loss: 224.36
 ---- batch: 050 ----
mean loss: 225.00
 ---- batch: 060 ----
mean loss: 228.90
 ---- batch: 070 ----
mean loss: 223.59
 ---- batch: 080 ----
mean loss: 226.74
 ---- batch: 090 ----
mean loss: 215.20
train mean loss: 225.54
epoch train time: 0:00:00.682673
elapsed time: 0:02:36.161018
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 22:46:14.621839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.76
 ---- batch: 020 ----
mean loss: 229.07
 ---- batch: 030 ----
mean loss: 220.89
 ---- batch: 040 ----
mean loss: 230.00
 ---- batch: 050 ----
mean loss: 232.42
 ---- batch: 060 ----
mean loss: 224.31
 ---- batch: 070 ----
mean loss: 224.97
 ---- batch: 080 ----
mean loss: 224.24
 ---- batch: 090 ----
mean loss: 221.87
train mean loss: 225.90
epoch train time: 0:00:00.685445
elapsed time: 0:02:36.846645
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 22:46:15.307458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.97
 ---- batch: 020 ----
mean loss: 223.87
 ---- batch: 030 ----
mean loss: 226.12
 ---- batch: 040 ----
mean loss: 227.33
 ---- batch: 050 ----
mean loss: 225.81
 ---- batch: 060 ----
mean loss: 233.75
 ---- batch: 070 ----
mean loss: 221.43
 ---- batch: 080 ----
mean loss: 220.78
 ---- batch: 090 ----
mean loss: 222.04
train mean loss: 225.60
epoch train time: 0:00:00.680219
elapsed time: 0:02:37.527032
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 22:46:15.987825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.26
 ---- batch: 020 ----
mean loss: 218.23
 ---- batch: 030 ----
mean loss: 223.59
 ---- batch: 040 ----
mean loss: 230.66
 ---- batch: 050 ----
mean loss: 235.59
 ---- batch: 060 ----
mean loss: 226.67
 ---- batch: 070 ----
mean loss: 223.53
 ---- batch: 080 ----
mean loss: 222.40
 ---- batch: 090 ----
mean loss: 228.33
train mean loss: 225.46
epoch train time: 0:00:00.672156
elapsed time: 0:02:38.199334
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 22:46:16.660120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.37
 ---- batch: 020 ----
mean loss: 221.38
 ---- batch: 030 ----
mean loss: 228.45
 ---- batch: 040 ----
mean loss: 228.36
 ---- batch: 050 ----
mean loss: 219.83
 ---- batch: 060 ----
mean loss: 229.29
 ---- batch: 070 ----
mean loss: 227.47
 ---- batch: 080 ----
mean loss: 221.66
 ---- batch: 090 ----
mean loss: 226.31
train mean loss: 225.30
epoch train time: 0:00:00.665276
elapsed time: 0:02:38.864770
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 22:46:17.325551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.50
 ---- batch: 020 ----
mean loss: 227.61
 ---- batch: 030 ----
mean loss: 221.50
 ---- batch: 040 ----
mean loss: 221.43
 ---- batch: 050 ----
mean loss: 228.16
 ---- batch: 060 ----
mean loss: 226.11
 ---- batch: 070 ----
mean loss: 221.26
 ---- batch: 080 ----
mean loss: 224.69
 ---- batch: 090 ----
mean loss: 225.30
train mean loss: 224.70
epoch train time: 0:00:00.677346
elapsed time: 0:02:39.542282
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 22:46:18.003063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.26
 ---- batch: 020 ----
mean loss: 228.89
 ---- batch: 030 ----
mean loss: 218.92
 ---- batch: 040 ----
mean loss: 219.72
 ---- batch: 050 ----
mean loss: 228.81
 ---- batch: 060 ----
mean loss: 218.64
 ---- batch: 070 ----
mean loss: 227.63
 ---- batch: 080 ----
mean loss: 230.44
 ---- batch: 090 ----
mean loss: 228.06
train mean loss: 225.02
epoch train time: 0:00:00.673546
elapsed time: 0:02:40.215973
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 22:46:18.676767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.39
 ---- batch: 020 ----
mean loss: 223.92
 ---- batch: 030 ----
mean loss: 235.11
 ---- batch: 040 ----
mean loss: 216.43
 ---- batch: 050 ----
mean loss: 225.66
 ---- batch: 060 ----
mean loss: 218.36
 ---- batch: 070 ----
mean loss: 214.84
 ---- batch: 080 ----
mean loss: 231.91
 ---- batch: 090 ----
mean loss: 229.82
train mean loss: 224.53
epoch train time: 0:00:00.666604
elapsed time: 0:02:40.882771
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 22:46:19.343585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.20
 ---- batch: 020 ----
mean loss: 223.83
 ---- batch: 030 ----
mean loss: 224.40
 ---- batch: 040 ----
mean loss: 228.85
 ---- batch: 050 ----
mean loss: 228.94
 ---- batch: 060 ----
mean loss: 222.45
 ---- batch: 070 ----
mean loss: 224.86
 ---- batch: 080 ----
mean loss: 230.62
 ---- batch: 090 ----
mean loss: 218.60
train mean loss: 224.57
epoch train time: 0:00:00.674575
elapsed time: 0:02:41.557517
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 22:46:20.018306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.81
 ---- batch: 020 ----
mean loss: 226.10
 ---- batch: 030 ----
mean loss: 227.94
 ---- batch: 040 ----
mean loss: 219.12
 ---- batch: 050 ----
mean loss: 227.22
 ---- batch: 060 ----
mean loss: 222.20
 ---- batch: 070 ----
mean loss: 225.39
 ---- batch: 080 ----
mean loss: 220.12
 ---- batch: 090 ----
mean loss: 223.41
train mean loss: 224.18
epoch train time: 0:00:00.668778
elapsed time: 0:02:42.226470
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 22:46:20.687290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.62
 ---- batch: 020 ----
mean loss: 223.02
 ---- batch: 030 ----
mean loss: 219.72
 ---- batch: 040 ----
mean loss: 228.28
 ---- batch: 050 ----
mean loss: 223.27
 ---- batch: 060 ----
mean loss: 227.54
 ---- batch: 070 ----
mean loss: 219.94
 ---- batch: 080 ----
mean loss: 219.31
 ---- batch: 090 ----
mean loss: 226.69
train mean loss: 223.73
epoch train time: 0:00:00.669209
elapsed time: 0:02:42.895850
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 22:46:21.356638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.96
 ---- batch: 020 ----
mean loss: 217.51
 ---- batch: 030 ----
mean loss: 224.62
 ---- batch: 040 ----
mean loss: 223.45
 ---- batch: 050 ----
mean loss: 223.63
 ---- batch: 060 ----
mean loss: 212.35
 ---- batch: 070 ----
mean loss: 230.55
 ---- batch: 080 ----
mean loss: 228.12
 ---- batch: 090 ----
mean loss: 227.61
train mean loss: 224.00
epoch train time: 0:00:00.670535
elapsed time: 0:02:43.566544
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 22:46:22.027332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.71
 ---- batch: 020 ----
mean loss: 221.64
 ---- batch: 030 ----
mean loss: 223.39
 ---- batch: 040 ----
mean loss: 222.43
 ---- batch: 050 ----
mean loss: 216.83
 ---- batch: 060 ----
mean loss: 228.57
 ---- batch: 070 ----
mean loss: 225.16
 ---- batch: 080 ----
mean loss: 225.48
 ---- batch: 090 ----
mean loss: 220.69
train mean loss: 224.08
epoch train time: 0:00:00.663904
elapsed time: 0:02:44.230595
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 22:46:22.691388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.55
 ---- batch: 020 ----
mean loss: 220.27
 ---- batch: 030 ----
mean loss: 229.16
 ---- batch: 040 ----
mean loss: 223.12
 ---- batch: 050 ----
mean loss: 221.60
 ---- batch: 060 ----
mean loss: 225.76
 ---- batch: 070 ----
mean loss: 227.48
 ---- batch: 080 ----
mean loss: 223.60
 ---- batch: 090 ----
mean loss: 222.35
train mean loss: 224.18
epoch train time: 0:00:00.663323
elapsed time: 0:02:44.894065
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 22:46:23.354853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.50
 ---- batch: 020 ----
mean loss: 223.85
 ---- batch: 030 ----
mean loss: 221.99
 ---- batch: 040 ----
mean loss: 227.31
 ---- batch: 050 ----
mean loss: 222.31
 ---- batch: 060 ----
mean loss: 222.05
 ---- batch: 070 ----
mean loss: 222.16
 ---- batch: 080 ----
mean loss: 221.14
 ---- batch: 090 ----
mean loss: 227.76
train mean loss: 223.31
epoch train time: 0:00:00.669659
elapsed time: 0:02:45.563870
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 22:46:24.024660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.29
 ---- batch: 020 ----
mean loss: 228.91
 ---- batch: 030 ----
mean loss: 223.88
 ---- batch: 040 ----
mean loss: 219.62
 ---- batch: 050 ----
mean loss: 222.76
 ---- batch: 060 ----
mean loss: 225.68
 ---- batch: 070 ----
mean loss: 217.94
 ---- batch: 080 ----
mean loss: 221.43
 ---- batch: 090 ----
mean loss: 230.96
train mean loss: 223.35
epoch train time: 0:00:00.663119
elapsed time: 0:02:46.227149
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 22:46:24.687951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.14
 ---- batch: 020 ----
mean loss: 216.86
 ---- batch: 030 ----
mean loss: 222.29
 ---- batch: 040 ----
mean loss: 227.14
 ---- batch: 050 ----
mean loss: 232.61
 ---- batch: 060 ----
mean loss: 224.67
 ---- batch: 070 ----
mean loss: 228.12
 ---- batch: 080 ----
mean loss: 226.11
 ---- batch: 090 ----
mean loss: 216.51
train mean loss: 223.43
epoch train time: 0:00:00.675235
elapsed time: 0:02:46.902553
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 22:46:25.363367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.75
 ---- batch: 020 ----
mean loss: 213.07
 ---- batch: 030 ----
mean loss: 227.95
 ---- batch: 040 ----
mean loss: 221.56
 ---- batch: 050 ----
mean loss: 224.78
 ---- batch: 060 ----
mean loss: 227.20
 ---- batch: 070 ----
mean loss: 221.46
 ---- batch: 080 ----
mean loss: 229.72
 ---- batch: 090 ----
mean loss: 217.13
train mean loss: 222.72
epoch train time: 0:00:00.661295
elapsed time: 0:02:47.564023
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 22:46:26.024816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.34
 ---- batch: 020 ----
mean loss: 224.11
 ---- batch: 030 ----
mean loss: 217.46
 ---- batch: 040 ----
mean loss: 223.78
 ---- batch: 050 ----
mean loss: 222.95
 ---- batch: 060 ----
mean loss: 217.52
 ---- batch: 070 ----
mean loss: 223.93
 ---- batch: 080 ----
mean loss: 228.67
 ---- batch: 090 ----
mean loss: 226.93
train mean loss: 223.33
epoch train time: 0:00:00.668282
elapsed time: 0:02:48.232452
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 22:46:26.693241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.27
 ---- batch: 020 ----
mean loss: 218.82
 ---- batch: 030 ----
mean loss: 220.21
 ---- batch: 040 ----
mean loss: 229.17
 ---- batch: 050 ----
mean loss: 223.04
 ---- batch: 060 ----
mean loss: 212.22
 ---- batch: 070 ----
mean loss: 221.94
 ---- batch: 080 ----
mean loss: 222.53
 ---- batch: 090 ----
mean loss: 225.46
train mean loss: 222.39
epoch train time: 0:00:00.669804
elapsed time: 0:02:48.902475
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 22:46:27.363251
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.16
 ---- batch: 020 ----
mean loss: 222.49
 ---- batch: 030 ----
mean loss: 223.58
 ---- batch: 040 ----
mean loss: 214.62
 ---- batch: 050 ----
mean loss: 226.65
 ---- batch: 060 ----
mean loss: 216.70
 ---- batch: 070 ----
mean loss: 222.42
 ---- batch: 080 ----
mean loss: 225.02
 ---- batch: 090 ----
mean loss: 218.83
train mean loss: 222.24
epoch train time: 0:00:00.676354
elapsed time: 0:02:49.578973
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 22:46:28.039763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.15
 ---- batch: 020 ----
mean loss: 222.72
 ---- batch: 030 ----
mean loss: 216.94
 ---- batch: 040 ----
mean loss: 221.06
 ---- batch: 050 ----
mean loss: 223.54
 ---- batch: 060 ----
mean loss: 217.59
 ---- batch: 070 ----
mean loss: 231.71
 ---- batch: 080 ----
mean loss: 220.42
 ---- batch: 090 ----
mean loss: 223.27
train mean loss: 222.12
epoch train time: 0:00:00.670938
elapsed time: 0:02:50.250061
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 22:46:28.710851
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.82
 ---- batch: 020 ----
mean loss: 224.72
 ---- batch: 030 ----
mean loss: 223.48
 ---- batch: 040 ----
mean loss: 221.83
 ---- batch: 050 ----
mean loss: 223.22
 ---- batch: 060 ----
mean loss: 223.39
 ---- batch: 070 ----
mean loss: 217.22
 ---- batch: 080 ----
mean loss: 233.58
 ---- batch: 090 ----
mean loss: 216.65
train mean loss: 221.98
epoch train time: 0:00:00.679009
elapsed time: 0:02:50.929221
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 22:46:29.390013
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.55
 ---- batch: 020 ----
mean loss: 226.49
 ---- batch: 030 ----
mean loss: 232.74
 ---- batch: 040 ----
mean loss: 209.88
 ---- batch: 050 ----
mean loss: 225.39
 ---- batch: 060 ----
mean loss: 222.68
 ---- batch: 070 ----
mean loss: 219.60
 ---- batch: 080 ----
mean loss: 233.29
 ---- batch: 090 ----
mean loss: 216.80
train mean loss: 222.01
epoch train time: 0:00:00.679010
elapsed time: 0:02:51.608385
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 22:46:30.069175
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.79
 ---- batch: 020 ----
mean loss: 216.95
 ---- batch: 030 ----
mean loss: 218.09
 ---- batch: 040 ----
mean loss: 231.23
 ---- batch: 050 ----
mean loss: 221.55
 ---- batch: 060 ----
mean loss: 220.27
 ---- batch: 070 ----
mean loss: 226.25
 ---- batch: 080 ----
mean loss: 229.60
 ---- batch: 090 ----
mean loss: 218.75
train mean loss: 222.27
epoch train time: 0:00:00.662365
elapsed time: 0:02:52.270909
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 22:46:30.731747
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.49
 ---- batch: 020 ----
mean loss: 218.19
 ---- batch: 030 ----
mean loss: 224.11
 ---- batch: 040 ----
mean loss: 228.43
 ---- batch: 050 ----
mean loss: 219.76
 ---- batch: 060 ----
mean loss: 213.49
 ---- batch: 070 ----
mean loss: 224.85
 ---- batch: 080 ----
mean loss: 221.75
 ---- batch: 090 ----
mean loss: 223.23
train mean loss: 222.20
epoch train time: 0:00:00.664410
elapsed time: 0:02:52.935514
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 22:46:31.396303
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.70
 ---- batch: 020 ----
mean loss: 224.54
 ---- batch: 030 ----
mean loss: 218.58
 ---- batch: 040 ----
mean loss: 231.34
 ---- batch: 050 ----
mean loss: 224.78
 ---- batch: 060 ----
mean loss: 219.10
 ---- batch: 070 ----
mean loss: 220.41
 ---- batch: 080 ----
mean loss: 223.10
 ---- batch: 090 ----
mean loss: 218.92
train mean loss: 222.12
epoch train time: 0:00:00.676075
elapsed time: 0:02:53.611752
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 22:46:32.072543
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.10
 ---- batch: 020 ----
mean loss: 227.87
 ---- batch: 030 ----
mean loss: 220.86
 ---- batch: 040 ----
mean loss: 219.57
 ---- batch: 050 ----
mean loss: 224.31
 ---- batch: 060 ----
mean loss: 225.09
 ---- batch: 070 ----
mean loss: 222.03
 ---- batch: 080 ----
mean loss: 223.00
 ---- batch: 090 ----
mean loss: 219.42
train mean loss: 222.16
epoch train time: 0:00:00.658955
elapsed time: 0:02:54.270847
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 22:46:32.731636
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.19
 ---- batch: 020 ----
mean loss: 225.52
 ---- batch: 030 ----
mean loss: 223.26
 ---- batch: 040 ----
mean loss: 218.55
 ---- batch: 050 ----
mean loss: 215.90
 ---- batch: 060 ----
mean loss: 232.73
 ---- batch: 070 ----
mean loss: 217.69
 ---- batch: 080 ----
mean loss: 219.54
 ---- batch: 090 ----
mean loss: 217.10
train mean loss: 221.98
epoch train time: 0:00:00.667504
elapsed time: 0:02:54.938497
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 22:46:33.399286
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.33
 ---- batch: 020 ----
mean loss: 220.97
 ---- batch: 030 ----
mean loss: 223.51
 ---- batch: 040 ----
mean loss: 221.46
 ---- batch: 050 ----
mean loss: 222.81
 ---- batch: 060 ----
mean loss: 221.46
 ---- batch: 070 ----
mean loss: 226.55
 ---- batch: 080 ----
mean loss: 221.70
 ---- batch: 090 ----
mean loss: 218.36
train mean loss: 222.32
epoch train time: 0:00:00.676821
elapsed time: 0:02:55.615496
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 22:46:34.076304
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.93
 ---- batch: 020 ----
mean loss: 223.95
 ---- batch: 030 ----
mean loss: 225.05
 ---- batch: 040 ----
mean loss: 224.58
 ---- batch: 050 ----
mean loss: 219.15
 ---- batch: 060 ----
mean loss: 214.30
 ---- batch: 070 ----
mean loss: 224.80
 ---- batch: 080 ----
mean loss: 219.56
 ---- batch: 090 ----
mean loss: 221.88
train mean loss: 222.13
epoch train time: 0:00:00.669743
elapsed time: 0:02:56.285406
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 22:46:34.746198
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.07
 ---- batch: 020 ----
mean loss: 226.60
 ---- batch: 030 ----
mean loss: 220.82
 ---- batch: 040 ----
mean loss: 223.77
 ---- batch: 050 ----
mean loss: 227.65
 ---- batch: 060 ----
mean loss: 224.17
 ---- batch: 070 ----
mean loss: 215.77
 ---- batch: 080 ----
mean loss: 217.85
 ---- batch: 090 ----
mean loss: 219.86
train mean loss: 221.93
epoch train time: 0:00:00.672136
elapsed time: 0:02:56.957707
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 22:46:35.418516
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.74
 ---- batch: 020 ----
mean loss: 209.32
 ---- batch: 030 ----
mean loss: 217.99
 ---- batch: 040 ----
mean loss: 217.56
 ---- batch: 050 ----
mean loss: 223.76
 ---- batch: 060 ----
mean loss: 233.01
 ---- batch: 070 ----
mean loss: 230.06
 ---- batch: 080 ----
mean loss: 228.05
 ---- batch: 090 ----
mean loss: 225.21
train mean loss: 222.41
epoch train time: 0:00:00.672632
elapsed time: 0:02:57.630549
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 22:46:36.091414
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.50
 ---- batch: 020 ----
mean loss: 224.82
 ---- batch: 030 ----
mean loss: 219.90
 ---- batch: 040 ----
mean loss: 218.84
 ---- batch: 050 ----
mean loss: 220.06
 ---- batch: 060 ----
mean loss: 227.58
 ---- batch: 070 ----
mean loss: 218.89
 ---- batch: 080 ----
mean loss: 223.56
 ---- batch: 090 ----
mean loss: 218.05
train mean loss: 221.96
epoch train time: 0:00:00.676330
elapsed time: 0:02:58.307163
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 22:46:36.767972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.41
 ---- batch: 020 ----
mean loss: 222.87
 ---- batch: 030 ----
mean loss: 223.56
 ---- batch: 040 ----
mean loss: 220.10
 ---- batch: 050 ----
mean loss: 219.18
 ---- batch: 060 ----
mean loss: 222.32
 ---- batch: 070 ----
mean loss: 220.00
 ---- batch: 080 ----
mean loss: 225.83
 ---- batch: 090 ----
mean loss: 220.26
train mean loss: 221.77
epoch train time: 0:00:00.673726
elapsed time: 0:02:58.981090
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 22:46:37.441880
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.96
 ---- batch: 020 ----
mean loss: 222.17
 ---- batch: 030 ----
mean loss: 228.89
 ---- batch: 040 ----
mean loss: 216.12
 ---- batch: 050 ----
mean loss: 225.57
 ---- batch: 060 ----
mean loss: 220.67
 ---- batch: 070 ----
mean loss: 216.10
 ---- batch: 080 ----
mean loss: 229.41
 ---- batch: 090 ----
mean loss: 221.37
train mean loss: 221.96
epoch train time: 0:00:00.676733
elapsed time: 0:02:59.657985
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 22:46:38.118776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.41
 ---- batch: 020 ----
mean loss: 228.09
 ---- batch: 030 ----
mean loss: 225.44
 ---- batch: 040 ----
mean loss: 221.34
 ---- batch: 050 ----
mean loss: 217.55
 ---- batch: 060 ----
mean loss: 221.65
 ---- batch: 070 ----
mean loss: 217.76
 ---- batch: 080 ----
mean loss: 224.66
 ---- batch: 090 ----
mean loss: 222.61
train mean loss: 222.14
epoch train time: 0:00:00.676648
elapsed time: 0:03:00.334782
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 22:46:38.795573
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.79
 ---- batch: 020 ----
mean loss: 217.99
 ---- batch: 030 ----
mean loss: 226.91
 ---- batch: 040 ----
mean loss: 225.95
 ---- batch: 050 ----
mean loss: 233.12
 ---- batch: 060 ----
mean loss: 215.20
 ---- batch: 070 ----
mean loss: 217.42
 ---- batch: 080 ----
mean loss: 228.11
 ---- batch: 090 ----
mean loss: 220.27
train mean loss: 221.88
epoch train time: 0:00:00.666969
elapsed time: 0:03:01.001904
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 22:46:39.462692
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.09
 ---- batch: 020 ----
mean loss: 217.02
 ---- batch: 030 ----
mean loss: 222.68
 ---- batch: 040 ----
mean loss: 221.15
 ---- batch: 050 ----
mean loss: 225.85
 ---- batch: 060 ----
mean loss: 219.84
 ---- batch: 070 ----
mean loss: 222.94
 ---- batch: 080 ----
mean loss: 229.73
 ---- batch: 090 ----
mean loss: 217.53
train mean loss: 222.05
epoch train time: 0:00:00.671662
elapsed time: 0:03:01.673714
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 22:46:40.134503
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.79
 ---- batch: 020 ----
mean loss: 218.51
 ---- batch: 030 ----
mean loss: 220.64
 ---- batch: 040 ----
mean loss: 220.76
 ---- batch: 050 ----
mean loss: 223.58
 ---- batch: 060 ----
mean loss: 226.64
 ---- batch: 070 ----
mean loss: 225.21
 ---- batch: 080 ----
mean loss: 232.91
 ---- batch: 090 ----
mean loss: 212.17
train mean loss: 221.85
epoch train time: 0:00:00.663380
elapsed time: 0:03:02.337334
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 22:46:40.798188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.78
 ---- batch: 020 ----
mean loss: 223.74
 ---- batch: 030 ----
mean loss: 220.43
 ---- batch: 040 ----
mean loss: 217.54
 ---- batch: 050 ----
mean loss: 215.80
 ---- batch: 060 ----
mean loss: 223.84
 ---- batch: 070 ----
mean loss: 223.11
 ---- batch: 080 ----
mean loss: 233.76
 ---- batch: 090 ----
mean loss: 222.38
train mean loss: 221.90
epoch train time: 0:00:00.664431
elapsed time: 0:03:03.001990
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 22:46:41.462788
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.56
 ---- batch: 020 ----
mean loss: 219.77
 ---- batch: 030 ----
mean loss: 226.46
 ---- batch: 040 ----
mean loss: 220.59
 ---- batch: 050 ----
mean loss: 220.07
 ---- batch: 060 ----
mean loss: 220.13
 ---- batch: 070 ----
mean loss: 223.52
 ---- batch: 080 ----
mean loss: 214.81
 ---- batch: 090 ----
mean loss: 230.62
train mean loss: 221.87
epoch train time: 0:00:00.671380
elapsed time: 0:03:03.673529
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 22:46:42.134321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.68
 ---- batch: 020 ----
mean loss: 216.60
 ---- batch: 030 ----
mean loss: 219.32
 ---- batch: 040 ----
mean loss: 223.14
 ---- batch: 050 ----
mean loss: 222.55
 ---- batch: 060 ----
mean loss: 218.86
 ---- batch: 070 ----
mean loss: 220.33
 ---- batch: 080 ----
mean loss: 233.35
 ---- batch: 090 ----
mean loss: 223.18
train mean loss: 221.80
epoch train time: 0:00:00.673579
elapsed time: 0:03:04.347272
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 22:46:42.808062
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.54
 ---- batch: 020 ----
mean loss: 227.36
 ---- batch: 030 ----
mean loss: 218.21
 ---- batch: 040 ----
mean loss: 225.12
 ---- batch: 050 ----
mean loss: 219.17
 ---- batch: 060 ----
mean loss: 218.21
 ---- batch: 070 ----
mean loss: 222.35
 ---- batch: 080 ----
mean loss: 216.32
 ---- batch: 090 ----
mean loss: 221.76
train mean loss: 221.82
epoch train time: 0:00:00.668592
elapsed time: 0:03:05.016013
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 22:46:43.476803
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.92
 ---- batch: 020 ----
mean loss: 225.54
 ---- batch: 030 ----
mean loss: 224.49
 ---- batch: 040 ----
mean loss: 216.02
 ---- batch: 050 ----
mean loss: 220.29
 ---- batch: 060 ----
mean loss: 221.10
 ---- batch: 070 ----
mean loss: 214.22
 ---- batch: 080 ----
mean loss: 225.52
 ---- batch: 090 ----
mean loss: 227.93
train mean loss: 221.98
epoch train time: 0:00:00.676795
elapsed time: 0:03:05.692980
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 22:46:44.153769
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.47
 ---- batch: 020 ----
mean loss: 225.65
 ---- batch: 030 ----
mean loss: 221.92
 ---- batch: 040 ----
mean loss: 222.86
 ---- batch: 050 ----
mean loss: 228.44
 ---- batch: 060 ----
mean loss: 217.87
 ---- batch: 070 ----
mean loss: 222.23
 ---- batch: 080 ----
mean loss: 221.96
 ---- batch: 090 ----
mean loss: 220.75
train mean loss: 221.80
epoch train time: 0:00:00.669568
elapsed time: 0:03:06.362691
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 22:46:44.823506
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 235.09
 ---- batch: 020 ----
mean loss: 214.19
 ---- batch: 030 ----
mean loss: 225.72
 ---- batch: 040 ----
mean loss: 221.86
 ---- batch: 050 ----
mean loss: 217.37
 ---- batch: 060 ----
mean loss: 223.35
 ---- batch: 070 ----
mean loss: 224.89
 ---- batch: 080 ----
mean loss: 218.13
 ---- batch: 090 ----
mean loss: 219.96
train mean loss: 221.73
epoch train time: 0:00:00.672625
elapsed time: 0:03:07.035486
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 22:46:45.496303
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.81
 ---- batch: 020 ----
mean loss: 223.36
 ---- batch: 030 ----
mean loss: 219.63
 ---- batch: 040 ----
mean loss: 227.02
 ---- batch: 050 ----
mean loss: 226.94
 ---- batch: 060 ----
mean loss: 217.01
 ---- batch: 070 ----
mean loss: 225.84
 ---- batch: 080 ----
mean loss: 213.19
 ---- batch: 090 ----
mean loss: 229.67
train mean loss: 221.79
epoch train time: 0:00:00.670796
elapsed time: 0:03:07.706458
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 22:46:46.167248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.34
 ---- batch: 020 ----
mean loss: 224.97
 ---- batch: 030 ----
mean loss: 228.59
 ---- batch: 040 ----
mean loss: 216.90
 ---- batch: 050 ----
mean loss: 216.31
 ---- batch: 060 ----
mean loss: 217.77
 ---- batch: 070 ----
mean loss: 222.86
 ---- batch: 080 ----
mean loss: 226.02
 ---- batch: 090 ----
mean loss: 220.17
train mean loss: 221.83
epoch train time: 0:00:00.681341
elapsed time: 0:03:08.387948
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 22:46:46.848759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.35
 ---- batch: 020 ----
mean loss: 219.70
 ---- batch: 030 ----
mean loss: 217.59
 ---- batch: 040 ----
mean loss: 219.35
 ---- batch: 050 ----
mean loss: 219.32
 ---- batch: 060 ----
mean loss: 213.61
 ---- batch: 070 ----
mean loss: 221.97
 ---- batch: 080 ----
mean loss: 222.09
 ---- batch: 090 ----
mean loss: 229.63
train mean loss: 221.65
epoch train time: 0:00:00.665603
elapsed time: 0:03:09.053725
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 22:46:47.514531
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.50
 ---- batch: 020 ----
mean loss: 223.75
 ---- batch: 030 ----
mean loss: 222.91
 ---- batch: 040 ----
mean loss: 221.35
 ---- batch: 050 ----
mean loss: 216.93
 ---- batch: 060 ----
mean loss: 224.23
 ---- batch: 070 ----
mean loss: 219.40
 ---- batch: 080 ----
mean loss: 219.20
 ---- batch: 090 ----
mean loss: 225.89
train mean loss: 221.59
epoch train time: 0:00:00.664730
elapsed time: 0:03:09.718614
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 22:46:48.179418
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.17
 ---- batch: 020 ----
mean loss: 219.62
 ---- batch: 030 ----
mean loss: 215.82
 ---- batch: 040 ----
mean loss: 216.76
 ---- batch: 050 ----
mean loss: 223.37
 ---- batch: 060 ----
mean loss: 224.18
 ---- batch: 070 ----
mean loss: 216.69
 ---- batch: 080 ----
mean loss: 222.12
 ---- batch: 090 ----
mean loss: 225.47
train mean loss: 221.82
epoch train time: 0:00:00.673304
elapsed time: 0:03:10.392132
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 22:46:48.852912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.77
 ---- batch: 020 ----
mean loss: 220.57
 ---- batch: 030 ----
mean loss: 220.33
 ---- batch: 040 ----
mean loss: 225.85
 ---- batch: 050 ----
mean loss: 225.78
 ---- batch: 060 ----
mean loss: 220.48
 ---- batch: 070 ----
mean loss: 228.97
 ---- batch: 080 ----
mean loss: 218.10
 ---- batch: 090 ----
mean loss: 219.16
train mean loss: 221.85
epoch train time: 0:00:00.675755
elapsed time: 0:03:11.068054
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 22:46:49.528843
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.18
 ---- batch: 020 ----
mean loss: 219.68
 ---- batch: 030 ----
mean loss: 220.77
 ---- batch: 040 ----
mean loss: 217.95
 ---- batch: 050 ----
mean loss: 227.58
 ---- batch: 060 ----
mean loss: 227.55
 ---- batch: 070 ----
mean loss: 218.82
 ---- batch: 080 ----
mean loss: 222.39
 ---- batch: 090 ----
mean loss: 220.20
train mean loss: 221.64
epoch train time: 0:00:00.676551
elapsed time: 0:03:11.744749
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 22:46:50.205538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.48
 ---- batch: 020 ----
mean loss: 225.62
 ---- batch: 030 ----
mean loss: 222.06
 ---- batch: 040 ----
mean loss: 225.78
 ---- batch: 050 ----
mean loss: 221.30
 ---- batch: 060 ----
mean loss: 215.49
 ---- batch: 070 ----
mean loss: 225.82
 ---- batch: 080 ----
mean loss: 222.30
 ---- batch: 090 ----
mean loss: 221.20
train mean loss: 221.45
epoch train time: 0:00:00.672298
elapsed time: 0:03:12.417224
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 22:46:50.878047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.20
 ---- batch: 020 ----
mean loss: 215.99
 ---- batch: 030 ----
mean loss: 227.71
 ---- batch: 040 ----
mean loss: 224.41
 ---- batch: 050 ----
mean loss: 219.60
 ---- batch: 060 ----
mean loss: 213.19
 ---- batch: 070 ----
mean loss: 221.27
 ---- batch: 080 ----
mean loss: 224.87
 ---- batch: 090 ----
mean loss: 222.42
train mean loss: 221.93
epoch train time: 0:00:00.662486
elapsed time: 0:03:13.079890
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 22:46:51.540680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.15
 ---- batch: 020 ----
mean loss: 219.34
 ---- batch: 030 ----
mean loss: 223.64
 ---- batch: 040 ----
mean loss: 221.17
 ---- batch: 050 ----
mean loss: 222.96
 ---- batch: 060 ----
mean loss: 213.39
 ---- batch: 070 ----
mean loss: 220.37
 ---- batch: 080 ----
mean loss: 219.18
 ---- batch: 090 ----
mean loss: 226.12
train mean loss: 221.96
epoch train time: 0:00:00.673958
elapsed time: 0:03:13.754002
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 22:46:52.214793
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.47
 ---- batch: 020 ----
mean loss: 222.65
 ---- batch: 030 ----
mean loss: 220.08
 ---- batch: 040 ----
mean loss: 214.60
 ---- batch: 050 ----
mean loss: 221.65
 ---- batch: 060 ----
mean loss: 229.85
 ---- batch: 070 ----
mean loss: 217.66
 ---- batch: 080 ----
mean loss: 221.87
 ---- batch: 090 ----
mean loss: 225.23
train mean loss: 221.80
epoch train time: 0:00:00.667121
elapsed time: 0:03:14.421276
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 22:46:52.882081
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.96
 ---- batch: 020 ----
mean loss: 217.50
 ---- batch: 030 ----
mean loss: 223.32
 ---- batch: 040 ----
mean loss: 218.60
 ---- batch: 050 ----
mean loss: 221.18
 ---- batch: 060 ----
mean loss: 217.31
 ---- batch: 070 ----
mean loss: 223.53
 ---- batch: 080 ----
mean loss: 225.23
 ---- batch: 090 ----
mean loss: 217.69
train mean loss: 221.85
epoch train time: 0:00:00.658360
elapsed time: 0:03:15.079802
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 22:46:53.540591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.67
 ---- batch: 020 ----
mean loss: 218.02
 ---- batch: 030 ----
mean loss: 226.20
 ---- batch: 040 ----
mean loss: 215.32
 ---- batch: 050 ----
mean loss: 221.84
 ---- batch: 060 ----
mean loss: 215.45
 ---- batch: 070 ----
mean loss: 225.88
 ---- batch: 080 ----
mean loss: 216.92
 ---- batch: 090 ----
mean loss: 225.51
train mean loss: 221.92
epoch train time: 0:00:00.671398
elapsed time: 0:03:15.751379
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 22:46:54.212167
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.99
 ---- batch: 020 ----
mean loss: 214.82
 ---- batch: 030 ----
mean loss: 227.48
 ---- batch: 040 ----
mean loss: 218.77
 ---- batch: 050 ----
mean loss: 219.60
 ---- batch: 060 ----
mean loss: 217.08
 ---- batch: 070 ----
mean loss: 219.52
 ---- batch: 080 ----
mean loss: 219.19
 ---- batch: 090 ----
mean loss: 229.67
train mean loss: 221.62
epoch train time: 0:00:00.666131
elapsed time: 0:03:16.417659
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 22:46:54.878449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.78
 ---- batch: 020 ----
mean loss: 223.11
 ---- batch: 030 ----
mean loss: 221.72
 ---- batch: 040 ----
mean loss: 214.35
 ---- batch: 050 ----
mean loss: 218.17
 ---- batch: 060 ----
mean loss: 226.85
 ---- batch: 070 ----
mean loss: 226.68
 ---- batch: 080 ----
mean loss: 214.67
 ---- batch: 090 ----
mean loss: 226.36
train mean loss: 221.52
epoch train time: 0:00:00.669081
elapsed time: 0:03:17.086902
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 22:46:55.547724
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.24
 ---- batch: 020 ----
mean loss: 230.02
 ---- batch: 030 ----
mean loss: 216.77
 ---- batch: 040 ----
mean loss: 220.94
 ---- batch: 050 ----
mean loss: 227.67
 ---- batch: 060 ----
mean loss: 216.58
 ---- batch: 070 ----
mean loss: 221.54
 ---- batch: 080 ----
mean loss: 215.96
 ---- batch: 090 ----
mean loss: 224.74
train mean loss: 221.49
epoch train time: 0:00:00.668932
elapsed time: 0:03:17.756030
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 22:46:56.216821
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.20
 ---- batch: 020 ----
mean loss: 220.24
 ---- batch: 030 ----
mean loss: 218.99
 ---- batch: 040 ----
mean loss: 227.18
 ---- batch: 050 ----
mean loss: 221.81
 ---- batch: 060 ----
mean loss: 221.18
 ---- batch: 070 ----
mean loss: 216.77
 ---- batch: 080 ----
mean loss: 228.01
 ---- batch: 090 ----
mean loss: 225.50
train mean loss: 221.50
epoch train time: 0:00:00.678665
elapsed time: 0:03:18.434888
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 22:46:56.895711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 232.99
 ---- batch: 020 ----
mean loss: 225.70
 ---- batch: 030 ----
mean loss: 215.37
 ---- batch: 040 ----
mean loss: 226.71
 ---- batch: 050 ----
mean loss: 222.87
 ---- batch: 060 ----
mean loss: 213.49
 ---- batch: 070 ----
mean loss: 218.96
 ---- batch: 080 ----
mean loss: 214.83
 ---- batch: 090 ----
mean loss: 223.17
train mean loss: 221.66
epoch train time: 0:00:00.666278
elapsed time: 0:03:19.101393
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 22:46:57.562182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.56
 ---- batch: 020 ----
mean loss: 214.35
 ---- batch: 030 ----
mean loss: 222.83
 ---- batch: 040 ----
mean loss: 228.38
 ---- batch: 050 ----
mean loss: 224.05
 ---- batch: 060 ----
mean loss: 226.38
 ---- batch: 070 ----
mean loss: 213.79
 ---- batch: 080 ----
mean loss: 214.78
 ---- batch: 090 ----
mean loss: 224.26
train mean loss: 221.42
epoch train time: 0:00:00.668045
elapsed time: 0:03:19.769616
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 22:46:58.230405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.65
 ---- batch: 020 ----
mean loss: 224.72
 ---- batch: 030 ----
mean loss: 217.59
 ---- batch: 040 ----
mean loss: 225.45
 ---- batch: 050 ----
mean loss: 217.31
 ---- batch: 060 ----
mean loss: 224.07
 ---- batch: 070 ----
mean loss: 231.51
 ---- batch: 080 ----
mean loss: 214.09
 ---- batch: 090 ----
mean loss: 217.83
train mean loss: 221.56
epoch train time: 0:00:00.675458
elapsed time: 0:03:20.445223
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 22:46:58.906027
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.83
 ---- batch: 020 ----
mean loss: 223.69
 ---- batch: 030 ----
mean loss: 219.38
 ---- batch: 040 ----
mean loss: 215.08
 ---- batch: 050 ----
mean loss: 233.64
 ---- batch: 060 ----
mean loss: 222.54
 ---- batch: 070 ----
mean loss: 215.62
 ---- batch: 080 ----
mean loss: 226.96
 ---- batch: 090 ----
mean loss: 225.34
train mean loss: 221.18
epoch train time: 0:00:00.673754
elapsed time: 0:03:21.121425
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_5/checkpoint.pth.tar
**** end time: 2019-09-25 22:46:59.582178 ****
