Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_8', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 23851
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-25 22:54:33.087122 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 16, 11]             560
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
            Conv2d-4            [-1, 14, 7, 11]             224
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
            Linear-8                    [-1, 1]             462
================================================================
Total params: 1,246
Trainable params: 1,246
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 22:54:33.092629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4247.58
 ---- batch: 020 ----
mean loss: 3989.21
 ---- batch: 030 ----
mean loss: 3861.47
 ---- batch: 040 ----
mean loss: 3611.07
 ---- batch: 050 ----
mean loss: 3324.30
 ---- batch: 060 ----
mean loss: 3173.76
 ---- batch: 070 ----
mean loss: 2875.25
 ---- batch: 080 ----
mean loss: 2678.59
 ---- batch: 090 ----
mean loss: 2439.92
train mean loss: 3286.39
epoch train time: 0:00:33.867862
elapsed time: 0:00:33.874708
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 22:55:06.961870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2042.33
 ---- batch: 020 ----
mean loss: 1899.27
 ---- batch: 030 ----
mean loss: 1710.47
 ---- batch: 040 ----
mean loss: 1555.55
 ---- batch: 050 ----
mean loss: 1388.05
 ---- batch: 060 ----
mean loss: 1298.94
 ---- batch: 070 ----
mean loss: 1203.17
 ---- batch: 080 ----
mean loss: 1125.91
 ---- batch: 090 ----
mean loss: 1091.05
train mean loss: 1450.74
epoch train time: 0:00:00.725063
elapsed time: 0:00:34.599935
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 22:55:07.687119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 998.29
 ---- batch: 020 ----
mean loss: 957.45
 ---- batch: 030 ----
mean loss: 945.57
 ---- batch: 040 ----
mean loss: 942.56
 ---- batch: 050 ----
mean loss: 928.43
 ---- batch: 060 ----
mean loss: 897.46
 ---- batch: 070 ----
mean loss: 905.30
 ---- batch: 080 ----
mean loss: 875.77
 ---- batch: 090 ----
mean loss: 892.10
train mean loss: 924.24
epoch train time: 0:00:00.722473
elapsed time: 0:00:35.322585
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 22:55:08.409776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.28
 ---- batch: 020 ----
mean loss: 873.57
 ---- batch: 030 ----
mean loss: 879.65
 ---- batch: 040 ----
mean loss: 886.96
 ---- batch: 050 ----
mean loss: 868.35
 ---- batch: 060 ----
mean loss: 872.33
 ---- batch: 070 ----
mean loss: 891.37
 ---- batch: 080 ----
mean loss: 885.27
 ---- batch: 090 ----
mean loss: 870.11
train mean loss: 880.34
epoch train time: 0:00:00.701104
elapsed time: 0:00:36.023867
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 22:55:09.111044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.14
 ---- batch: 020 ----
mean loss: 863.96
 ---- batch: 030 ----
mean loss: 880.89
 ---- batch: 040 ----
mean loss: 880.48
 ---- batch: 050 ----
mean loss: 860.75
 ---- batch: 060 ----
mean loss: 874.44
 ---- batch: 070 ----
mean loss: 889.73
 ---- batch: 080 ----
mean loss: 882.71
 ---- batch: 090 ----
mean loss: 865.19
train mean loss: 874.71
epoch train time: 0:00:00.696330
elapsed time: 0:00:36.720347
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 22:55:09.807520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.33
 ---- batch: 020 ----
mean loss: 873.74
 ---- batch: 030 ----
mean loss: 883.75
 ---- batch: 040 ----
mean loss: 873.11
 ---- batch: 050 ----
mean loss: 868.77
 ---- batch: 060 ----
mean loss: 847.98
 ---- batch: 070 ----
mean loss: 884.77
 ---- batch: 080 ----
mean loss: 874.55
 ---- batch: 090 ----
mean loss: 862.56
train mean loss: 872.17
epoch train time: 0:00:00.698627
elapsed time: 0:00:37.419186
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 22:55:10.506361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.75
 ---- batch: 020 ----
mean loss: 872.41
 ---- batch: 030 ----
mean loss: 879.70
 ---- batch: 040 ----
mean loss: 879.67
 ---- batch: 050 ----
mean loss: 877.26
 ---- batch: 060 ----
mean loss: 866.77
 ---- batch: 070 ----
mean loss: 863.99
 ---- batch: 080 ----
mean loss: 863.29
 ---- batch: 090 ----
mean loss: 859.78
train mean loss: 868.50
epoch train time: 0:00:00.693422
elapsed time: 0:00:38.112759
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 22:55:11.199931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.49
 ---- batch: 020 ----
mean loss: 870.16
 ---- batch: 030 ----
mean loss: 902.18
 ---- batch: 040 ----
mean loss: 866.63
 ---- batch: 050 ----
mean loss: 860.43
 ---- batch: 060 ----
mean loss: 856.51
 ---- batch: 070 ----
mean loss: 877.86
 ---- batch: 080 ----
mean loss: 858.95
 ---- batch: 090 ----
mean loss: 840.64
train mean loss: 863.55
epoch train time: 0:00:00.695511
elapsed time: 0:00:38.808431
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 22:55:11.895608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 854.57
 ---- batch: 030 ----
mean loss: 863.68
 ---- batch: 040 ----
mean loss: 886.79
 ---- batch: 050 ----
mean loss: 844.22
 ---- batch: 060 ----
mean loss: 852.72
 ---- batch: 070 ----
mean loss: 851.34
 ---- batch: 080 ----
mean loss: 844.70
 ---- batch: 090 ----
mean loss: 870.02
train mean loss: 858.81
epoch train time: 0:00:00.694434
elapsed time: 0:00:39.503024
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 22:55:12.590200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.17
 ---- batch: 020 ----
mean loss: 859.04
 ---- batch: 030 ----
mean loss: 831.31
 ---- batch: 040 ----
mean loss: 851.14
 ---- batch: 050 ----
mean loss: 866.48
 ---- batch: 060 ----
mean loss: 868.50
 ---- batch: 070 ----
mean loss: 855.79
 ---- batch: 080 ----
mean loss: 841.97
 ---- batch: 090 ----
mean loss: 845.42
train mean loss: 853.25
epoch train time: 0:00:00.701973
elapsed time: 0:00:40.205160
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 22:55:13.292333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.78
 ---- batch: 020 ----
mean loss: 859.05
 ---- batch: 030 ----
mean loss: 849.70
 ---- batch: 040 ----
mean loss: 851.99
 ---- batch: 050 ----
mean loss: 840.33
 ---- batch: 060 ----
mean loss: 850.33
 ---- batch: 070 ----
mean loss: 848.64
 ---- batch: 080 ----
mean loss: 833.58
 ---- batch: 090 ----
mean loss: 847.76
train mean loss: 845.49
epoch train time: 0:00:00.712542
elapsed time: 0:00:40.917855
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 22:55:14.005050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.98
 ---- batch: 020 ----
mean loss: 830.63
 ---- batch: 030 ----
mean loss: 841.48
 ---- batch: 040 ----
mean loss: 860.98
 ---- batch: 050 ----
mean loss: 855.87
 ---- batch: 060 ----
mean loss: 841.88
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 833.13
 ---- batch: 090 ----
mean loss: 825.28
train mean loss: 836.78
epoch train time: 0:00:00.699691
elapsed time: 0:00:41.617784
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 22:55:14.704958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.36
 ---- batch: 020 ----
mean loss: 834.45
 ---- batch: 030 ----
mean loss: 836.15
 ---- batch: 040 ----
mean loss: 820.72
 ---- batch: 050 ----
mean loss: 838.14
 ---- batch: 060 ----
mean loss: 835.46
 ---- batch: 070 ----
mean loss: 810.09
 ---- batch: 080 ----
mean loss: 825.15
 ---- batch: 090 ----
mean loss: 840.60
train mean loss: 829.90
epoch train time: 0:00:00.693321
elapsed time: 0:00:42.311254
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 22:55:15.398429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.86
 ---- batch: 020 ----
mean loss: 828.22
 ---- batch: 030 ----
mean loss: 823.67
 ---- batch: 040 ----
mean loss: 800.41
 ---- batch: 050 ----
mean loss: 828.67
 ---- batch: 060 ----
mean loss: 813.84
 ---- batch: 070 ----
mean loss: 840.72
 ---- batch: 080 ----
mean loss: 818.50
 ---- batch: 090 ----
mean loss: 818.63
train mean loss: 822.35
epoch train time: 0:00:00.698814
elapsed time: 0:00:43.010238
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 22:55:16.097420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 833.07
 ---- batch: 020 ----
mean loss: 817.59
 ---- batch: 030 ----
mean loss: 820.93
 ---- batch: 040 ----
mean loss: 813.20
 ---- batch: 050 ----
mean loss: 808.05
 ---- batch: 060 ----
mean loss: 804.31
 ---- batch: 070 ----
mean loss: 805.89
 ---- batch: 080 ----
mean loss: 824.87
 ---- batch: 090 ----
mean loss: 814.74
train mean loss: 816.00
epoch train time: 0:00:00.701453
elapsed time: 0:00:43.711856
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 22:55:16.799021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 805.14
 ---- batch: 020 ----
mean loss: 815.06
 ---- batch: 030 ----
mean loss: 804.15
 ---- batch: 040 ----
mean loss: 820.66
 ---- batch: 050 ----
mean loss: 817.17
 ---- batch: 060 ----
mean loss: 800.45
 ---- batch: 070 ----
mean loss: 791.23
 ---- batch: 080 ----
mean loss: 802.68
 ---- batch: 090 ----
mean loss: 810.63
train mean loss: 808.04
epoch train time: 0:00:00.702390
elapsed time: 0:00:44.414400
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 22:55:17.501584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 810.71
 ---- batch: 020 ----
mean loss: 785.54
 ---- batch: 030 ----
mean loss: 799.45
 ---- batch: 040 ----
mean loss: 808.23
 ---- batch: 050 ----
mean loss: 786.23
 ---- batch: 060 ----
mean loss: 806.90
 ---- batch: 070 ----
mean loss: 812.35
 ---- batch: 080 ----
mean loss: 816.90
 ---- batch: 090 ----
mean loss: 782.92
train mean loss: 801.46
epoch train time: 0:00:00.703059
elapsed time: 0:00:45.117645
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 22:55:18.204818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 810.30
 ---- batch: 020 ----
mean loss: 790.37
 ---- batch: 030 ----
mean loss: 807.58
 ---- batch: 040 ----
mean loss: 803.16
 ---- batch: 050 ----
mean loss: 779.21
 ---- batch: 060 ----
mean loss: 778.45
 ---- batch: 070 ----
mean loss: 793.95
 ---- batch: 080 ----
mean loss: 789.37
 ---- batch: 090 ----
mean loss: 789.09
train mean loss: 793.60
epoch train time: 0:00:00.697067
elapsed time: 0:00:45.814892
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 22:55:18.902074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 795.54
 ---- batch: 020 ----
mean loss: 797.20
 ---- batch: 030 ----
mean loss: 773.43
 ---- batch: 040 ----
mean loss: 790.08
 ---- batch: 050 ----
mean loss: 787.68
 ---- batch: 060 ----
mean loss: 785.32
 ---- batch: 070 ----
mean loss: 768.95
 ---- batch: 080 ----
mean loss: 792.10
 ---- batch: 090 ----
mean loss: 791.48
train mean loss: 785.98
epoch train time: 0:00:00.694478
elapsed time: 0:00:46.509534
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 22:55:19.596708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 780.61
 ---- batch: 020 ----
mean loss: 790.86
 ---- batch: 030 ----
mean loss: 782.19
 ---- batch: 040 ----
mean loss: 784.96
 ---- batch: 050 ----
mean loss: 767.92
 ---- batch: 060 ----
mean loss: 779.27
 ---- batch: 070 ----
mean loss: 781.20
 ---- batch: 080 ----
mean loss: 767.03
 ---- batch: 090 ----
mean loss: 769.15
train mean loss: 778.37
epoch train time: 0:00:00.700094
elapsed time: 0:00:47.209787
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 22:55:20.296978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 777.40
 ---- batch: 020 ----
mean loss: 769.50
 ---- batch: 030 ----
mean loss: 763.69
 ---- batch: 040 ----
mean loss: 771.80
 ---- batch: 050 ----
mean loss: 780.51
 ---- batch: 060 ----
mean loss: 766.04
 ---- batch: 070 ----
mean loss: 755.70
 ---- batch: 080 ----
mean loss: 776.60
 ---- batch: 090 ----
mean loss: 772.05
train mean loss: 769.20
epoch train time: 0:00:00.704149
elapsed time: 0:00:47.914099
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 22:55:21.001271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 762.50
 ---- batch: 020 ----
mean loss: 771.98
 ---- batch: 030 ----
mean loss: 756.54
 ---- batch: 040 ----
mean loss: 746.81
 ---- batch: 050 ----
mean loss: 754.45
 ---- batch: 060 ----
mean loss: 774.31
 ---- batch: 070 ----
mean loss: 761.34
 ---- batch: 080 ----
mean loss: 758.99
 ---- batch: 090 ----
mean loss: 763.17
train mean loss: 761.41
epoch train time: 0:00:00.706111
elapsed time: 0:00:48.620361
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 22:55:21.707563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 763.42
 ---- batch: 020 ----
mean loss: 749.04
 ---- batch: 030 ----
mean loss: 740.69
 ---- batch: 040 ----
mean loss: 748.47
 ---- batch: 050 ----
mean loss: 752.26
 ---- batch: 060 ----
mean loss: 751.33
 ---- batch: 070 ----
mean loss: 752.59
 ---- batch: 080 ----
mean loss: 755.66
 ---- batch: 090 ----
mean loss: 755.93
train mean loss: 752.10
epoch train time: 0:00:00.704172
elapsed time: 0:00:49.324723
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 22:55:22.411905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 759.86
 ---- batch: 020 ----
mean loss: 738.37
 ---- batch: 030 ----
mean loss: 744.04
 ---- batch: 040 ----
mean loss: 733.30
 ---- batch: 050 ----
mean loss: 740.74
 ---- batch: 060 ----
mean loss: 731.80
 ---- batch: 070 ----
mean loss: 747.40
 ---- batch: 080 ----
mean loss: 749.70
 ---- batch: 090 ----
mean loss: 738.29
train mean loss: 743.72
epoch train time: 0:00:00.702486
elapsed time: 0:00:50.027380
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 22:55:23.114555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 729.72
 ---- batch: 020 ----
mean loss: 748.23
 ---- batch: 030 ----
mean loss: 739.28
 ---- batch: 040 ----
mean loss: 740.17
 ---- batch: 050 ----
mean loss: 739.60
 ---- batch: 060 ----
mean loss: 732.52
 ---- batch: 070 ----
mean loss: 728.48
 ---- batch: 080 ----
mean loss: 725.81
 ---- batch: 090 ----
mean loss: 735.50
train mean loss: 733.68
epoch train time: 0:00:00.701180
elapsed time: 0:00:50.728730
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 22:55:23.815907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.55
 ---- batch: 020 ----
mean loss: 722.88
 ---- batch: 030 ----
mean loss: 719.30
 ---- batch: 040 ----
mean loss: 715.36
 ---- batch: 050 ----
mean loss: 717.84
 ---- batch: 060 ----
mean loss: 742.95
 ---- batch: 070 ----
mean loss: 728.08
 ---- batch: 080 ----
mean loss: 722.99
 ---- batch: 090 ----
mean loss: 717.14
train mean loss: 723.80
epoch train time: 0:00:00.696169
elapsed time: 0:00:51.425066
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 22:55:24.512257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 721.87
 ---- batch: 020 ----
mean loss: 723.51
 ---- batch: 030 ----
mean loss: 710.69
 ---- batch: 040 ----
mean loss: 712.97
 ---- batch: 050 ----
mean loss: 701.17
 ---- batch: 060 ----
mean loss: 712.54
 ---- batch: 070 ----
mean loss: 717.34
 ---- batch: 080 ----
mean loss: 719.52
 ---- batch: 090 ----
mean loss: 717.20
train mean loss: 713.90
epoch train time: 0:00:00.698459
elapsed time: 0:00:52.123707
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 22:55:25.210885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.27
 ---- batch: 020 ----
mean loss: 702.77
 ---- batch: 030 ----
mean loss: 705.30
 ---- batch: 040 ----
mean loss: 713.63
 ---- batch: 050 ----
mean loss: 707.78
 ---- batch: 060 ----
mean loss: 695.46
 ---- batch: 070 ----
mean loss: 690.75
 ---- batch: 080 ----
mean loss: 716.91
 ---- batch: 090 ----
mean loss: 687.14
train mean loss: 703.54
epoch train time: 0:00:00.696082
elapsed time: 0:00:52.819952
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 22:55:25.907126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 699.96
 ---- batch: 020 ----
mean loss: 693.32
 ---- batch: 030 ----
mean loss: 684.51
 ---- batch: 040 ----
mean loss: 695.13
 ---- batch: 050 ----
mean loss: 699.57
 ---- batch: 060 ----
mean loss: 699.77
 ---- batch: 070 ----
mean loss: 708.78
 ---- batch: 080 ----
mean loss: 687.25
 ---- batch: 090 ----
mean loss: 679.41
train mean loss: 694.02
epoch train time: 0:00:00.702174
elapsed time: 0:00:53.522279
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 22:55:26.609454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 698.97
 ---- batch: 020 ----
mean loss: 692.04
 ---- batch: 030 ----
mean loss: 683.64
 ---- batch: 040 ----
mean loss: 682.67
 ---- batch: 050 ----
mean loss: 687.97
 ---- batch: 060 ----
mean loss: 694.19
 ---- batch: 070 ----
mean loss: 679.06
 ---- batch: 080 ----
mean loss: 679.74
 ---- batch: 090 ----
mean loss: 658.10
train mean loss: 683.12
epoch train time: 0:00:00.704817
elapsed time: 0:00:54.227246
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 22:55:27.314420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.09
 ---- batch: 020 ----
mean loss: 674.72
 ---- batch: 030 ----
mean loss: 671.71
 ---- batch: 040 ----
mean loss: 678.42
 ---- batch: 050 ----
mean loss: 690.24
 ---- batch: 060 ----
mean loss: 665.86
 ---- batch: 070 ----
mean loss: 683.20
 ---- batch: 080 ----
mean loss: 669.16
 ---- batch: 090 ----
mean loss: 658.08
train mean loss: 672.91
epoch train time: 0:00:00.707036
elapsed time: 0:00:54.934441
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 22:55:28.021639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 670.63
 ---- batch: 020 ----
mean loss: 672.58
 ---- batch: 030 ----
mean loss: 666.36
 ---- batch: 040 ----
mean loss: 655.23
 ---- batch: 050 ----
mean loss: 655.26
 ---- batch: 060 ----
mean loss: 669.69
 ---- batch: 070 ----
mean loss: 647.03
 ---- batch: 080 ----
mean loss: 675.85
 ---- batch: 090 ----
mean loss: 655.87
train mean loss: 662.72
epoch train time: 0:00:00.710645
elapsed time: 0:00:55.645258
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 22:55:28.732430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 658.12
 ---- batch: 020 ----
mean loss: 660.07
 ---- batch: 030 ----
mean loss: 655.70
 ---- batch: 040 ----
mean loss: 653.11
 ---- batch: 050 ----
mean loss: 645.47
 ---- batch: 060 ----
mean loss: 648.11
 ---- batch: 070 ----
mean loss: 651.16
 ---- batch: 080 ----
mean loss: 650.37
 ---- batch: 090 ----
mean loss: 645.87
train mean loss: 652.99
epoch train time: 0:00:00.726338
elapsed time: 0:00:56.371746
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 22:55:29.458942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 650.40
 ---- batch: 020 ----
mean loss: 650.63
 ---- batch: 030 ----
mean loss: 648.03
 ---- batch: 040 ----
mean loss: 643.35
 ---- batch: 050 ----
mean loss: 649.82
 ---- batch: 060 ----
mean loss: 640.22
 ---- batch: 070 ----
mean loss: 640.48
 ---- batch: 080 ----
mean loss: 628.13
 ---- batch: 090 ----
mean loss: 634.49
train mean loss: 642.62
epoch train time: 0:00:00.718747
elapsed time: 0:00:57.090672
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 22:55:30.177844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 635.38
 ---- batch: 020 ----
mean loss: 635.70
 ---- batch: 030 ----
mean loss: 626.50
 ---- batch: 040 ----
mean loss: 632.41
 ---- batch: 050 ----
mean loss: 629.30
 ---- batch: 060 ----
mean loss: 637.20
 ---- batch: 070 ----
mean loss: 634.09
 ---- batch: 080 ----
mean loss: 625.45
 ---- batch: 090 ----
mean loss: 640.75
train mean loss: 632.75
epoch train time: 0:00:00.714796
elapsed time: 0:00:57.805636
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 22:55:30.892814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.29
 ---- batch: 020 ----
mean loss: 624.98
 ---- batch: 030 ----
mean loss: 632.07
 ---- batch: 040 ----
mean loss: 631.55
 ---- batch: 050 ----
mean loss: 627.24
 ---- batch: 060 ----
mean loss: 607.26
 ---- batch: 070 ----
mean loss: 611.48
 ---- batch: 080 ----
mean loss: 613.66
 ---- batch: 090 ----
mean loss: 636.15
train mean loss: 622.63
epoch train time: 0:00:00.716044
elapsed time: 0:00:58.521839
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 22:55:31.609013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.66
 ---- batch: 020 ----
mean loss: 617.00
 ---- batch: 030 ----
mean loss: 611.78
 ---- batch: 040 ----
mean loss: 623.83
 ---- batch: 050 ----
mean loss: 619.74
 ---- batch: 060 ----
mean loss: 607.48
 ---- batch: 070 ----
mean loss: 614.66
 ---- batch: 080 ----
mean loss: 592.26
 ---- batch: 090 ----
mean loss: 600.50
train mean loss: 613.11
epoch train time: 0:00:00.713382
elapsed time: 0:00:59.235376
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 22:55:32.322549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 598.67
 ---- batch: 020 ----
mean loss: 613.27
 ---- batch: 030 ----
mean loss: 602.35
 ---- batch: 040 ----
mean loss: 615.13
 ---- batch: 050 ----
mean loss: 600.68
 ---- batch: 060 ----
mean loss: 601.52
 ---- batch: 070 ----
mean loss: 604.76
 ---- batch: 080 ----
mean loss: 605.03
 ---- batch: 090 ----
mean loss: 588.26
train mean loss: 603.63
epoch train time: 0:00:00.715137
elapsed time: 0:00:59.950666
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 22:55:33.037840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 600.45
 ---- batch: 020 ----
mean loss: 595.99
 ---- batch: 030 ----
mean loss: 592.96
 ---- batch: 040 ----
mean loss: 598.00
 ---- batch: 050 ----
mean loss: 589.52
 ---- batch: 060 ----
mean loss: 598.06
 ---- batch: 070 ----
mean loss: 600.42
 ---- batch: 080 ----
mean loss: 589.51
 ---- batch: 090 ----
mean loss: 590.56
train mean loss: 594.94
epoch train time: 0:00:00.714275
elapsed time: 0:01:00.665091
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 22:55:33.752264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 592.80
 ---- batch: 020 ----
mean loss: 579.64
 ---- batch: 030 ----
mean loss: 581.53
 ---- batch: 040 ----
mean loss: 594.21
 ---- batch: 050 ----
mean loss: 588.58
 ---- batch: 060 ----
mean loss: 584.83
 ---- batch: 070 ----
mean loss: 580.38
 ---- batch: 080 ----
mean loss: 578.09
 ---- batch: 090 ----
mean loss: 586.50
train mean loss: 585.61
epoch train time: 0:00:00.710027
elapsed time: 0:01:01.375267
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 22:55:34.462437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.17
 ---- batch: 020 ----
mean loss: 582.31
 ---- batch: 030 ----
mean loss: 574.50
 ---- batch: 040 ----
mean loss: 573.25
 ---- batch: 050 ----
mean loss: 577.01
 ---- batch: 060 ----
mean loss: 580.88
 ---- batch: 070 ----
mean loss: 578.93
 ---- batch: 080 ----
mean loss: 572.54
 ---- batch: 090 ----
mean loss: 565.28
train mean loss: 576.96
epoch train time: 0:00:00.716924
elapsed time: 0:01:02.092337
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 22:55:35.179528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 563.98
 ---- batch: 020 ----
mean loss: 572.25
 ---- batch: 030 ----
mean loss: 572.51
 ---- batch: 040 ----
mean loss: 571.98
 ---- batch: 050 ----
mean loss: 560.23
 ---- batch: 060 ----
mean loss: 578.83
 ---- batch: 070 ----
mean loss: 562.18
 ---- batch: 080 ----
mean loss: 560.78
 ---- batch: 090 ----
mean loss: 571.05
train mean loss: 568.62
epoch train time: 0:00:00.715645
elapsed time: 0:01:02.808158
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 22:55:35.895335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.07
 ---- batch: 020 ----
mean loss: 552.62
 ---- batch: 030 ----
mean loss: 563.53
 ---- batch: 040 ----
mean loss: 543.02
 ---- batch: 050 ----
mean loss: 559.07
 ---- batch: 060 ----
mean loss: 562.89
 ---- batch: 070 ----
mean loss: 559.44
 ---- batch: 080 ----
mean loss: 568.36
 ---- batch: 090 ----
mean loss: 564.06
train mean loss: 560.62
epoch train time: 0:00:00.712764
elapsed time: 0:01:03.521074
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 22:55:36.608247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.46
 ---- batch: 020 ----
mean loss: 559.39
 ---- batch: 030 ----
mean loss: 559.25
 ---- batch: 040 ----
mean loss: 555.36
 ---- batch: 050 ----
mean loss: 547.98
 ---- batch: 060 ----
mean loss: 549.25
 ---- batch: 070 ----
mean loss: 548.26
 ---- batch: 080 ----
mean loss: 549.28
 ---- batch: 090 ----
mean loss: 552.22
train mean loss: 552.53
epoch train time: 0:00:00.719547
elapsed time: 0:01:04.240769
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 22:55:37.327943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 546.03
 ---- batch: 020 ----
mean loss: 539.26
 ---- batch: 030 ----
mean loss: 565.13
 ---- batch: 040 ----
mean loss: 540.95
 ---- batch: 050 ----
mean loss: 538.75
 ---- batch: 060 ----
mean loss: 545.68
 ---- batch: 070 ----
mean loss: 548.42
 ---- batch: 080 ----
mean loss: 533.45
 ---- batch: 090 ----
mean loss: 546.87
train mean loss: 544.77
epoch train time: 0:00:00.720498
elapsed time: 0:01:04.961428
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 22:55:38.048618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 540.64
 ---- batch: 020 ----
mean loss: 546.36
 ---- batch: 030 ----
mean loss: 538.45
 ---- batch: 040 ----
mean loss: 540.28
 ---- batch: 050 ----
mean loss: 532.91
 ---- batch: 060 ----
mean loss: 531.54
 ---- batch: 070 ----
mean loss: 543.64
 ---- batch: 080 ----
mean loss: 528.28
 ---- batch: 090 ----
mean loss: 534.64
train mean loss: 536.99
epoch train time: 0:00:00.700811
elapsed time: 0:01:05.662422
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 22:55:38.749597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.68
 ---- batch: 020 ----
mean loss: 531.66
 ---- batch: 030 ----
mean loss: 535.70
 ---- batch: 040 ----
mean loss: 524.76
 ---- batch: 050 ----
mean loss: 536.05
 ---- batch: 060 ----
mean loss: 534.10
 ---- batch: 070 ----
mean loss: 531.81
 ---- batch: 080 ----
mean loss: 514.39
 ---- batch: 090 ----
mean loss: 521.20
train mean loss: 529.27
epoch train time: 0:00:00.707198
elapsed time: 0:01:06.369817
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 22:55:39.457012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.73
 ---- batch: 020 ----
mean loss: 521.10
 ---- batch: 030 ----
mean loss: 523.06
 ---- batch: 040 ----
mean loss: 514.06
 ---- batch: 050 ----
mean loss: 539.71
 ---- batch: 060 ----
mean loss: 513.02
 ---- batch: 070 ----
mean loss: 524.89
 ---- batch: 080 ----
mean loss: 513.63
 ---- batch: 090 ----
mean loss: 524.63
train mean loss: 522.07
epoch train time: 0:00:00.695024
elapsed time: 0:01:07.065010
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 22:55:40.152201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.87
 ---- batch: 020 ----
mean loss: 509.76
 ---- batch: 030 ----
mean loss: 510.32
 ---- batch: 040 ----
mean loss: 505.55
 ---- batch: 050 ----
mean loss: 519.76
 ---- batch: 060 ----
mean loss: 508.48
 ---- batch: 070 ----
mean loss: 522.41
 ---- batch: 080 ----
mean loss: 520.64
 ---- batch: 090 ----
mean loss: 521.26
train mean loss: 514.60
epoch train time: 0:00:00.701457
elapsed time: 0:01:07.766650
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 22:55:40.853825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.53
 ---- batch: 020 ----
mean loss: 502.34
 ---- batch: 030 ----
mean loss: 510.81
 ---- batch: 040 ----
mean loss: 515.76
 ---- batch: 050 ----
mean loss: 496.49
 ---- batch: 060 ----
mean loss: 516.06
 ---- batch: 070 ----
mean loss: 499.31
 ---- batch: 080 ----
mean loss: 509.84
 ---- batch: 090 ----
mean loss: 503.47
train mean loss: 507.27
epoch train time: 0:00:00.698168
elapsed time: 0:01:08.464974
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 22:55:41.552167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.56
 ---- batch: 020 ----
mean loss: 504.24
 ---- batch: 030 ----
mean loss: 506.94
 ---- batch: 040 ----
mean loss: 499.87
 ---- batch: 050 ----
mean loss: 492.37
 ---- batch: 060 ----
mean loss: 498.13
 ---- batch: 070 ----
mean loss: 501.78
 ---- batch: 080 ----
mean loss: 494.95
 ---- batch: 090 ----
mean loss: 499.23
train mean loss: 499.88
epoch train time: 0:00:00.702138
elapsed time: 0:01:09.167278
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 22:55:42.254473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.36
 ---- batch: 020 ----
mean loss: 491.72
 ---- batch: 030 ----
mean loss: 499.01
 ---- batch: 040 ----
mean loss: 493.51
 ---- batch: 050 ----
mean loss: 481.58
 ---- batch: 060 ----
mean loss: 485.72
 ---- batch: 070 ----
mean loss: 493.92
 ---- batch: 080 ----
mean loss: 495.97
 ---- batch: 090 ----
mean loss: 498.55
train mean loss: 492.88
epoch train time: 0:00:00.694647
elapsed time: 0:01:09.862097
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 22:55:42.949271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.87
 ---- batch: 020 ----
mean loss: 485.17
 ---- batch: 030 ----
mean loss: 488.37
 ---- batch: 040 ----
mean loss: 477.33
 ---- batch: 050 ----
mean loss: 492.04
 ---- batch: 060 ----
mean loss: 493.69
 ---- batch: 070 ----
mean loss: 477.06
 ---- batch: 080 ----
mean loss: 486.47
 ---- batch: 090 ----
mean loss: 480.39
train mean loss: 485.26
epoch train time: 0:00:00.695139
elapsed time: 0:01:10.557382
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 22:55:43.644554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.44
 ---- batch: 020 ----
mean loss: 476.72
 ---- batch: 030 ----
mean loss: 494.65
 ---- batch: 040 ----
mean loss: 478.09
 ---- batch: 050 ----
mean loss: 479.34
 ---- batch: 060 ----
mean loss: 477.00
 ---- batch: 070 ----
mean loss: 469.65
 ---- batch: 080 ----
mean loss: 483.66
 ---- batch: 090 ----
mean loss: 462.18
train mean loss: 477.92
epoch train time: 0:00:00.699273
elapsed time: 0:01:11.256817
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 22:55:44.343990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.46
 ---- batch: 020 ----
mean loss: 464.86
 ---- batch: 030 ----
mean loss: 475.96
 ---- batch: 040 ----
mean loss: 460.98
 ---- batch: 050 ----
mean loss: 470.99
 ---- batch: 060 ----
mean loss: 472.65
 ---- batch: 070 ----
mean loss: 473.36
 ---- batch: 080 ----
mean loss: 461.41
 ---- batch: 090 ----
mean loss: 470.08
train mean loss: 470.61
epoch train time: 0:00:00.696057
elapsed time: 0:01:11.953052
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 22:55:45.040225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.30
 ---- batch: 020 ----
mean loss: 459.43
 ---- batch: 030 ----
mean loss: 462.87
 ---- batch: 040 ----
mean loss: 467.05
 ---- batch: 050 ----
mean loss: 466.74
 ---- batch: 060 ----
mean loss: 463.61
 ---- batch: 070 ----
mean loss: 476.72
 ---- batch: 080 ----
mean loss: 445.24
 ---- batch: 090 ----
mean loss: 458.61
train mean loss: 463.37
epoch train time: 0:00:00.704176
elapsed time: 0:01:12.657393
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 22:55:45.744601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.99
 ---- batch: 020 ----
mean loss: 453.04
 ---- batch: 030 ----
mean loss: 469.12
 ---- batch: 040 ----
mean loss: 451.35
 ---- batch: 050 ----
mean loss: 459.31
 ---- batch: 060 ----
mean loss: 451.03
 ---- batch: 070 ----
mean loss: 453.58
 ---- batch: 080 ----
mean loss: 452.55
 ---- batch: 090 ----
mean loss: 447.63
train mean loss: 455.69
epoch train time: 0:00:00.705310
elapsed time: 0:01:13.362893
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 22:55:46.450082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.17
 ---- batch: 020 ----
mean loss: 459.85
 ---- batch: 030 ----
mean loss: 455.94
 ---- batch: 040 ----
mean loss: 437.81
 ---- batch: 050 ----
mean loss: 458.81
 ---- batch: 060 ----
mean loss: 445.96
 ---- batch: 070 ----
mean loss: 438.54
 ---- batch: 080 ----
mean loss: 436.75
 ---- batch: 090 ----
mean loss: 435.05
train mean loss: 444.54
epoch train time: 0:00:00.698748
elapsed time: 0:01:14.061808
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 22:55:47.148978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.09
 ---- batch: 020 ----
mean loss: 422.52
 ---- batch: 030 ----
mean loss: 426.02
 ---- batch: 040 ----
mean loss: 422.25
 ---- batch: 050 ----
mean loss: 414.73
 ---- batch: 060 ----
mean loss: 408.40
 ---- batch: 070 ----
mean loss: 411.23
 ---- batch: 080 ----
mean loss: 409.80
 ---- batch: 090 ----
mean loss: 409.08
train mean loss: 416.64
epoch train time: 0:00:00.696185
elapsed time: 0:01:14.758161
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 22:55:47.845342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.99
 ---- batch: 020 ----
mean loss: 394.91
 ---- batch: 030 ----
mean loss: 389.67
 ---- batch: 040 ----
mean loss: 389.66
 ---- batch: 050 ----
mean loss: 386.12
 ---- batch: 060 ----
mean loss: 391.43
 ---- batch: 070 ----
mean loss: 382.12
 ---- batch: 080 ----
mean loss: 380.52
 ---- batch: 090 ----
mean loss: 381.85
train mean loss: 388.22
epoch train time: 0:00:00.694909
elapsed time: 0:01:15.453256
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 22:55:48.540428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.29
 ---- batch: 020 ----
mean loss: 366.06
 ---- batch: 030 ----
mean loss: 362.92
 ---- batch: 040 ----
mean loss: 374.38
 ---- batch: 050 ----
mean loss: 363.85
 ---- batch: 060 ----
mean loss: 360.14
 ---- batch: 070 ----
mean loss: 371.17
 ---- batch: 080 ----
mean loss: 371.92
 ---- batch: 090 ----
mean loss: 366.05
train mean loss: 366.39
epoch train time: 0:00:00.705439
elapsed time: 0:01:16.158845
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 22:55:49.246018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.03
 ---- batch: 020 ----
mean loss: 348.93
 ---- batch: 030 ----
mean loss: 357.19
 ---- batch: 040 ----
mean loss: 339.78
 ---- batch: 050 ----
mean loss: 346.65
 ---- batch: 060 ----
mean loss: 355.82
 ---- batch: 070 ----
mean loss: 347.75
 ---- batch: 080 ----
mean loss: 345.51
 ---- batch: 090 ----
mean loss: 359.35
train mean loss: 350.38
epoch train time: 0:00:00.704723
elapsed time: 0:01:16.863728
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 22:55:49.950900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.20
 ---- batch: 020 ----
mean loss: 334.84
 ---- batch: 030 ----
mean loss: 342.00
 ---- batch: 040 ----
mean loss: 335.37
 ---- batch: 050 ----
mean loss: 342.10
 ---- batch: 060 ----
mean loss: 340.49
 ---- batch: 070 ----
mean loss: 339.69
 ---- batch: 080 ----
mean loss: 337.38
 ---- batch: 090 ----
mean loss: 332.04
train mean loss: 338.73
epoch train time: 0:00:00.695958
elapsed time: 0:01:17.559841
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 22:55:50.647017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.42
 ---- batch: 020 ----
mean loss: 331.36
 ---- batch: 030 ----
mean loss: 333.63
 ---- batch: 040 ----
mean loss: 329.35
 ---- batch: 050 ----
mean loss: 341.61
 ---- batch: 060 ----
mean loss: 323.68
 ---- batch: 070 ----
mean loss: 323.05
 ---- batch: 080 ----
mean loss: 327.67
 ---- batch: 090 ----
mean loss: 323.20
train mean loss: 329.37
epoch train time: 0:00:00.702893
elapsed time: 0:01:18.262892
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 22:55:51.350085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.02
 ---- batch: 020 ----
mean loss: 327.32
 ---- batch: 030 ----
mean loss: 321.03
 ---- batch: 040 ----
mean loss: 329.21
 ---- batch: 050 ----
mean loss: 328.83
 ---- batch: 060 ----
mean loss: 321.02
 ---- batch: 070 ----
mean loss: 311.55
 ---- batch: 080 ----
mean loss: 320.61
 ---- batch: 090 ----
mean loss: 317.32
train mean loss: 322.33
epoch train time: 0:00:00.698218
elapsed time: 0:01:18.961286
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 22:55:52.048460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.63
 ---- batch: 020 ----
mean loss: 319.26
 ---- batch: 030 ----
mean loss: 323.52
 ---- batch: 040 ----
mean loss: 310.33
 ---- batch: 050 ----
mean loss: 313.60
 ---- batch: 060 ----
mean loss: 322.07
 ---- batch: 070 ----
mean loss: 309.51
 ---- batch: 080 ----
mean loss: 314.89
 ---- batch: 090 ----
mean loss: 318.41
train mean loss: 315.47
epoch train time: 0:00:00.702515
elapsed time: 0:01:19.663950
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 22:55:52.751124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.18
 ---- batch: 020 ----
mean loss: 306.17
 ---- batch: 030 ----
mean loss: 321.73
 ---- batch: 040 ----
mean loss: 318.05
 ---- batch: 050 ----
mean loss: 315.65
 ---- batch: 060 ----
mean loss: 308.40
 ---- batch: 070 ----
mean loss: 308.75
 ---- batch: 080 ----
mean loss: 305.32
 ---- batch: 090 ----
mean loss: 305.05
train mean loss: 310.52
epoch train time: 0:00:00.795857
elapsed time: 0:01:20.459980
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 22:55:53.547182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.15
 ---- batch: 020 ----
mean loss: 305.49
 ---- batch: 030 ----
mean loss: 298.17
 ---- batch: 040 ----
mean loss: 297.91
 ---- batch: 050 ----
mean loss: 311.07
 ---- batch: 060 ----
mean loss: 311.48
 ---- batch: 070 ----
mean loss: 314.79
 ---- batch: 080 ----
mean loss: 308.79
 ---- batch: 090 ----
mean loss: 298.99
train mean loss: 305.53
epoch train time: 0:00:00.712984
elapsed time: 0:01:21.173174
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 22:55:54.260362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.73
 ---- batch: 020 ----
mean loss: 306.17
 ---- batch: 030 ----
mean loss: 298.20
 ---- batch: 040 ----
mean loss: 298.91
 ---- batch: 050 ----
mean loss: 299.67
 ---- batch: 060 ----
mean loss: 300.48
 ---- batch: 070 ----
mean loss: 305.99
 ---- batch: 080 ----
mean loss: 294.90
 ---- batch: 090 ----
mean loss: 302.58
train mean loss: 301.82
epoch train time: 0:00:00.694615
elapsed time: 0:01:21.867956
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 22:55:54.955176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.44
 ---- batch: 020 ----
mean loss: 290.50
 ---- batch: 030 ----
mean loss: 297.40
 ---- batch: 040 ----
mean loss: 298.74
 ---- batch: 050 ----
mean loss: 293.88
 ---- batch: 060 ----
mean loss: 287.79
 ---- batch: 070 ----
mean loss: 286.75
 ---- batch: 080 ----
mean loss: 311.81
 ---- batch: 090 ----
mean loss: 306.51
train mean loss: 298.00
epoch train time: 0:00:00.694715
elapsed time: 0:01:22.562892
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 22:55:55.650070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.48
 ---- batch: 020 ----
mean loss: 292.13
 ---- batch: 030 ----
mean loss: 300.84
 ---- batch: 040 ----
mean loss: 305.94
 ---- batch: 050 ----
mean loss: 299.52
 ---- batch: 060 ----
mean loss: 288.68
 ---- batch: 070 ----
mean loss: 294.03
 ---- batch: 080 ----
mean loss: 286.71
 ---- batch: 090 ----
mean loss: 297.51
train mean loss: 294.47
epoch train time: 0:00:00.706551
elapsed time: 0:01:23.269625
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 22:55:56.356815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.73
 ---- batch: 020 ----
mean loss: 286.08
 ---- batch: 030 ----
mean loss: 297.89
 ---- batch: 040 ----
mean loss: 291.22
 ---- batch: 050 ----
mean loss: 297.87
 ---- batch: 060 ----
mean loss: 299.47
 ---- batch: 070 ----
mean loss: 283.72
 ---- batch: 080 ----
mean loss: 287.48
 ---- batch: 090 ----
mean loss: 291.00
train mean loss: 291.47
epoch train time: 0:00:00.703803
elapsed time: 0:01:23.973590
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 22:55:57.060780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.67
 ---- batch: 020 ----
mean loss: 278.58
 ---- batch: 030 ----
mean loss: 282.96
 ---- batch: 040 ----
mean loss: 292.39
 ---- batch: 050 ----
mean loss: 295.69
 ---- batch: 060 ----
mean loss: 292.04
 ---- batch: 070 ----
mean loss: 288.44
 ---- batch: 080 ----
mean loss: 292.98
 ---- batch: 090 ----
mean loss: 289.78
train mean loss: 288.57
epoch train time: 0:00:00.700401
elapsed time: 0:01:24.674158
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 22:55:57.761331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.16
 ---- batch: 020 ----
mean loss: 287.76
 ---- batch: 030 ----
mean loss: 295.00
 ---- batch: 040 ----
mean loss: 281.70
 ---- batch: 050 ----
mean loss: 283.39
 ---- batch: 060 ----
mean loss: 284.24
 ---- batch: 070 ----
mean loss: 288.20
 ---- batch: 080 ----
mean loss: 291.85
 ---- batch: 090 ----
mean loss: 283.28
train mean loss: 285.83
epoch train time: 0:00:00.694723
elapsed time: 0:01:25.369029
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 22:55:58.456201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.55
 ---- batch: 020 ----
mean loss: 286.45
 ---- batch: 030 ----
mean loss: 292.12
 ---- batch: 040 ----
mean loss: 280.20
 ---- batch: 050 ----
mean loss: 279.31
 ---- batch: 060 ----
mean loss: 287.96
 ---- batch: 070 ----
mean loss: 283.73
 ---- batch: 080 ----
mean loss: 282.74
 ---- batch: 090 ----
mean loss: 281.91
train mean loss: 283.67
epoch train time: 0:00:00.702033
elapsed time: 0:01:26.071227
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 22:55:59.158416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.09
 ---- batch: 020 ----
mean loss: 284.28
 ---- batch: 030 ----
mean loss: 281.04
 ---- batch: 040 ----
mean loss: 284.82
 ---- batch: 050 ----
mean loss: 275.48
 ---- batch: 060 ----
mean loss: 283.07
 ---- batch: 070 ----
mean loss: 288.81
 ---- batch: 080 ----
mean loss: 283.22
 ---- batch: 090 ----
mean loss: 285.54
train mean loss: 282.08
epoch train time: 0:00:00.711998
elapsed time: 0:01:26.783407
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 22:55:59.870590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.12
 ---- batch: 020 ----
mean loss: 281.11
 ---- batch: 030 ----
mean loss: 264.66
 ---- batch: 040 ----
mean loss: 282.47
 ---- batch: 050 ----
mean loss: 285.05
 ---- batch: 060 ----
mean loss: 273.01
 ---- batch: 070 ----
mean loss: 282.49
 ---- batch: 080 ----
mean loss: 287.08
 ---- batch: 090 ----
mean loss: 274.51
train mean loss: 279.89
epoch train time: 0:00:00.723898
elapsed time: 0:01:27.507489
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 22:56:00.594666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.10
 ---- batch: 020 ----
mean loss: 282.50
 ---- batch: 030 ----
mean loss: 272.31
 ---- batch: 040 ----
mean loss: 286.90
 ---- batch: 050 ----
mean loss: 272.58
 ---- batch: 060 ----
mean loss: 274.71
 ---- batch: 070 ----
mean loss: 278.14
 ---- batch: 080 ----
mean loss: 281.14
 ---- batch: 090 ----
mean loss: 279.27
train mean loss: 278.01
epoch train time: 0:00:00.724264
elapsed time: 0:01:28.231907
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 22:56:01.319081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.44
 ---- batch: 020 ----
mean loss: 271.24
 ---- batch: 030 ----
mean loss: 272.03
 ---- batch: 040 ----
mean loss: 289.39
 ---- batch: 050 ----
mean loss: 278.95
 ---- batch: 060 ----
mean loss: 270.43
 ---- batch: 070 ----
mean loss: 287.05
 ---- batch: 080 ----
mean loss: 275.21
 ---- batch: 090 ----
mean loss: 278.15
train mean loss: 276.43
epoch train time: 0:00:00.711475
elapsed time: 0:01:28.943533
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 22:56:02.030724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.45
 ---- batch: 020 ----
mean loss: 273.43
 ---- batch: 030 ----
mean loss: 275.75
 ---- batch: 040 ----
mean loss: 280.45
 ---- batch: 050 ----
mean loss: 269.60
 ---- batch: 060 ----
mean loss: 286.24
 ---- batch: 070 ----
mean loss: 264.01
 ---- batch: 080 ----
mean loss: 271.29
 ---- batch: 090 ----
mean loss: 277.59
train mean loss: 274.46
epoch train time: 0:00:00.702218
elapsed time: 0:01:29.645937
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 22:56:02.733125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.66
 ---- batch: 020 ----
mean loss: 270.27
 ---- batch: 030 ----
mean loss: 271.53
 ---- batch: 040 ----
mean loss: 272.34
 ---- batch: 050 ----
mean loss: 272.52
 ---- batch: 060 ----
mean loss: 268.25
 ---- batch: 070 ----
mean loss: 272.36
 ---- batch: 080 ----
mean loss: 276.90
 ---- batch: 090 ----
mean loss: 274.30
train mean loss: 273.10
epoch train time: 0:00:00.710051
elapsed time: 0:01:30.356154
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 22:56:03.443329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.00
 ---- batch: 020 ----
mean loss: 279.02
 ---- batch: 030 ----
mean loss: 277.60
 ---- batch: 040 ----
mean loss: 269.67
 ---- batch: 050 ----
mean loss: 282.22
 ---- batch: 060 ----
mean loss: 266.03
 ---- batch: 070 ----
mean loss: 268.34
 ---- batch: 080 ----
mean loss: 261.93
 ---- batch: 090 ----
mean loss: 271.49
train mean loss: 271.36
epoch train time: 0:00:00.724900
elapsed time: 0:01:31.081223
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 22:56:04.168402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.65
 ---- batch: 020 ----
mean loss: 266.08
 ---- batch: 030 ----
mean loss: 264.27
 ---- batch: 040 ----
mean loss: 269.39
 ---- batch: 050 ----
mean loss: 270.50
 ---- batch: 060 ----
mean loss: 277.02
 ---- batch: 070 ----
mean loss: 265.25
 ---- batch: 080 ----
mean loss: 278.37
 ---- batch: 090 ----
mean loss: 269.38
train mean loss: 269.81
epoch train time: 0:00:00.726627
elapsed time: 0:01:31.808004
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 22:56:04.895214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.50
 ---- batch: 020 ----
mean loss: 260.51
 ---- batch: 030 ----
mean loss: 270.06
 ---- batch: 040 ----
mean loss: 266.92
 ---- batch: 050 ----
mean loss: 273.89
 ---- batch: 060 ----
mean loss: 271.90
 ---- batch: 070 ----
mean loss: 266.96
 ---- batch: 080 ----
mean loss: 267.01
 ---- batch: 090 ----
mean loss: 265.85
train mean loss: 268.72
epoch train time: 0:00:00.709945
elapsed time: 0:01:32.518144
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 22:56:05.605318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.90
 ---- batch: 020 ----
mean loss: 262.84
 ---- batch: 030 ----
mean loss: 267.11
 ---- batch: 040 ----
mean loss: 256.62
 ---- batch: 050 ----
mean loss: 272.95
 ---- batch: 060 ----
mean loss: 268.55
 ---- batch: 070 ----
mean loss: 266.69
 ---- batch: 080 ----
mean loss: 266.26
 ---- batch: 090 ----
mean loss: 276.79
train mean loss: 267.07
epoch train time: 0:00:00.704272
elapsed time: 0:01:33.222590
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 22:56:06.309763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.85
 ---- batch: 020 ----
mean loss: 272.64
 ---- batch: 030 ----
mean loss: 261.06
 ---- batch: 040 ----
mean loss: 271.51
 ---- batch: 050 ----
mean loss: 265.31
 ---- batch: 060 ----
mean loss: 257.33
 ---- batch: 070 ----
mean loss: 253.87
 ---- batch: 080 ----
mean loss: 262.62
 ---- batch: 090 ----
mean loss: 277.38
train mean loss: 266.46
epoch train time: 0:00:00.703886
elapsed time: 0:01:33.926643
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 22:56:07.013839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.15
 ---- batch: 020 ----
mean loss: 263.48
 ---- batch: 030 ----
mean loss: 266.37
 ---- batch: 040 ----
mean loss: 269.72
 ---- batch: 050 ----
mean loss: 265.25
 ---- batch: 060 ----
mean loss: 266.64
 ---- batch: 070 ----
mean loss: 261.54
 ---- batch: 080 ----
mean loss: 266.26
 ---- batch: 090 ----
mean loss: 256.28
train mean loss: 265.09
epoch train time: 0:00:00.710866
elapsed time: 0:01:34.637700
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 22:56:07.724925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.36
 ---- batch: 020 ----
mean loss: 264.50
 ---- batch: 030 ----
mean loss: 268.96
 ---- batch: 040 ----
mean loss: 271.08
 ---- batch: 050 ----
mean loss: 251.07
 ---- batch: 060 ----
mean loss: 269.26
 ---- batch: 070 ----
mean loss: 262.36
 ---- batch: 080 ----
mean loss: 269.11
 ---- batch: 090 ----
mean loss: 263.25
train mean loss: 263.94
epoch train time: 0:00:00.704458
elapsed time: 0:01:35.342367
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 22:56:08.429540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.40
 ---- batch: 020 ----
mean loss: 264.26
 ---- batch: 030 ----
mean loss: 261.47
 ---- batch: 040 ----
mean loss: 262.44
 ---- batch: 050 ----
mean loss: 261.11
 ---- batch: 060 ----
mean loss: 268.57
 ---- batch: 070 ----
mean loss: 257.11
 ---- batch: 080 ----
mean loss: 259.45
 ---- batch: 090 ----
mean loss: 257.42
train mean loss: 262.99
epoch train time: 0:00:00.694853
elapsed time: 0:01:36.037372
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 22:56:09.124547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.88
 ---- batch: 020 ----
mean loss: 266.55
 ---- batch: 030 ----
mean loss: 266.03
 ---- batch: 040 ----
mean loss: 262.20
 ---- batch: 050 ----
mean loss: 263.38
 ---- batch: 060 ----
mean loss: 256.03
 ---- batch: 070 ----
mean loss: 259.39
 ---- batch: 080 ----
mean loss: 259.89
 ---- batch: 090 ----
mean loss: 265.72
train mean loss: 261.91
epoch train time: 0:00:00.696062
elapsed time: 0:01:36.733588
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 22:56:09.820763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.53
 ---- batch: 020 ----
mean loss: 262.61
 ---- batch: 030 ----
mean loss: 249.34
 ---- batch: 040 ----
mean loss: 262.25
 ---- batch: 050 ----
mean loss: 267.22
 ---- batch: 060 ----
mean loss: 273.87
 ---- batch: 070 ----
mean loss: 261.26
 ---- batch: 080 ----
mean loss: 258.86
 ---- batch: 090 ----
mean loss: 256.44
train mean loss: 260.89
epoch train time: 0:00:00.702146
elapsed time: 0:01:37.435898
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 22:56:10.523088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.66
 ---- batch: 020 ----
mean loss: 262.37
 ---- batch: 030 ----
mean loss: 258.29
 ---- batch: 040 ----
mean loss: 257.66
 ---- batch: 050 ----
mean loss: 267.28
 ---- batch: 060 ----
mean loss: 271.92
 ---- batch: 070 ----
mean loss: 255.59
 ---- batch: 080 ----
mean loss: 257.20
 ---- batch: 090 ----
mean loss: 255.55
train mean loss: 260.01
epoch train time: 0:00:00.701053
elapsed time: 0:01:38.137116
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 22:56:11.224289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.80
 ---- batch: 020 ----
mean loss: 264.29
 ---- batch: 030 ----
mean loss: 256.88
 ---- batch: 040 ----
mean loss: 261.13
 ---- batch: 050 ----
mean loss: 257.28
 ---- batch: 060 ----
mean loss: 260.59
 ---- batch: 070 ----
mean loss: 258.04
 ---- batch: 080 ----
mean loss: 260.36
 ---- batch: 090 ----
mean loss: 260.42
train mean loss: 259.48
epoch train time: 0:00:00.705179
elapsed time: 0:01:38.842474
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 22:56:11.929672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.24
 ---- batch: 020 ----
mean loss: 266.25
 ---- batch: 030 ----
mean loss: 258.88
 ---- batch: 040 ----
mean loss: 261.01
 ---- batch: 050 ----
mean loss: 256.56
 ---- batch: 060 ----
mean loss: 259.40
 ---- batch: 070 ----
mean loss: 262.40
 ---- batch: 080 ----
mean loss: 264.53
 ---- batch: 090 ----
mean loss: 250.93
train mean loss: 259.04
epoch train time: 0:00:00.701891
elapsed time: 0:01:39.544540
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 22:56:12.631747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.41
 ---- batch: 020 ----
mean loss: 255.95
 ---- batch: 030 ----
mean loss: 268.80
 ---- batch: 040 ----
mean loss: 249.47
 ---- batch: 050 ----
mean loss: 253.02
 ---- batch: 060 ----
mean loss: 266.16
 ---- batch: 070 ----
mean loss: 259.29
 ---- batch: 080 ----
mean loss: 250.81
 ---- batch: 090 ----
mean loss: 262.04
train mean loss: 257.68
epoch train time: 0:00:00.716528
elapsed time: 0:01:40.261257
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 22:56:13.348430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.30
 ---- batch: 020 ----
mean loss: 248.29
 ---- batch: 030 ----
mean loss: 258.62
 ---- batch: 040 ----
mean loss: 256.84
 ---- batch: 050 ----
mean loss: 257.37
 ---- batch: 060 ----
mean loss: 256.15
 ---- batch: 070 ----
mean loss: 253.86
 ---- batch: 080 ----
mean loss: 257.47
 ---- batch: 090 ----
mean loss: 259.40
train mean loss: 256.92
epoch train time: 0:00:00.703345
elapsed time: 0:01:40.964752
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 22:56:14.051928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.55
 ---- batch: 020 ----
mean loss: 252.60
 ---- batch: 030 ----
mean loss: 258.28
 ---- batch: 040 ----
mean loss: 257.22
 ---- batch: 050 ----
mean loss: 254.39
 ---- batch: 060 ----
mean loss: 256.00
 ---- batch: 070 ----
mean loss: 259.14
 ---- batch: 080 ----
mean loss: 253.31
 ---- batch: 090 ----
mean loss: 258.05
train mean loss: 256.66
epoch train time: 0:00:00.701867
elapsed time: 0:01:41.666775
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 22:56:14.753949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.81
 ---- batch: 020 ----
mean loss: 252.89
 ---- batch: 030 ----
mean loss: 251.72
 ---- batch: 040 ----
mean loss: 256.83
 ---- batch: 050 ----
mean loss: 254.63
 ---- batch: 060 ----
mean loss: 249.65
 ---- batch: 070 ----
mean loss: 260.08
 ---- batch: 080 ----
mean loss: 258.62
 ---- batch: 090 ----
mean loss: 262.69
train mean loss: 255.58
epoch train time: 0:00:00.724661
elapsed time: 0:01:42.391641
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 22:56:15.479992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.96
 ---- batch: 020 ----
mean loss: 257.27
 ---- batch: 030 ----
mean loss: 251.88
 ---- batch: 040 ----
mean loss: 257.54
 ---- batch: 050 ----
mean loss: 248.83
 ---- batch: 060 ----
mean loss: 255.03
 ---- batch: 070 ----
mean loss: 261.33
 ---- batch: 080 ----
mean loss: 255.15
 ---- batch: 090 ----
mean loss: 252.65
train mean loss: 254.93
epoch train time: 0:00:00.709460
elapsed time: 0:01:43.102426
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 22:56:16.189600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.56
 ---- batch: 020 ----
mean loss: 247.79
 ---- batch: 030 ----
mean loss: 250.57
 ---- batch: 040 ----
mean loss: 252.18
 ---- batch: 050 ----
mean loss: 258.38
 ---- batch: 060 ----
mean loss: 258.08
 ---- batch: 070 ----
mean loss: 253.54
 ---- batch: 080 ----
mean loss: 253.05
 ---- batch: 090 ----
mean loss: 257.74
train mean loss: 254.40
epoch train time: 0:00:00.708834
elapsed time: 0:01:43.811417
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 22:56:16.898608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.49
 ---- batch: 020 ----
mean loss: 250.63
 ---- batch: 030 ----
mean loss: 253.12
 ---- batch: 040 ----
mean loss: 255.37
 ---- batch: 050 ----
mean loss: 264.88
 ---- batch: 060 ----
mean loss: 247.42
 ---- batch: 070 ----
mean loss: 243.00
 ---- batch: 080 ----
mean loss: 250.83
 ---- batch: 090 ----
mean loss: 252.36
train mean loss: 253.67
epoch train time: 0:00:00.708755
elapsed time: 0:01:44.520351
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 22:56:17.607561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.39
 ---- batch: 020 ----
mean loss: 254.04
 ---- batch: 030 ----
mean loss: 251.81
 ---- batch: 040 ----
mean loss: 252.18
 ---- batch: 050 ----
mean loss: 260.19
 ---- batch: 060 ----
mean loss: 257.10
 ---- batch: 070 ----
mean loss: 248.81
 ---- batch: 080 ----
mean loss: 249.17
 ---- batch: 090 ----
mean loss: 251.38
train mean loss: 252.91
epoch train time: 0:00:00.697778
elapsed time: 0:01:45.218311
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 22:56:18.305484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.91
 ---- batch: 020 ----
mean loss: 247.52
 ---- batch: 030 ----
mean loss: 249.22
 ---- batch: 040 ----
mean loss: 254.62
 ---- batch: 050 ----
mean loss: 249.37
 ---- batch: 060 ----
mean loss: 258.15
 ---- batch: 070 ----
mean loss: 250.03
 ---- batch: 080 ----
mean loss: 251.78
 ---- batch: 090 ----
mean loss: 257.23
train mean loss: 252.31
epoch train time: 0:00:00.694835
elapsed time: 0:01:45.913323
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 22:56:19.000494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.29
 ---- batch: 020 ----
mean loss: 257.27
 ---- batch: 030 ----
mean loss: 245.55
 ---- batch: 040 ----
mean loss: 253.88
 ---- batch: 050 ----
mean loss: 245.91
 ---- batch: 060 ----
mean loss: 253.34
 ---- batch: 070 ----
mean loss: 254.90
 ---- batch: 080 ----
mean loss: 247.93
 ---- batch: 090 ----
mean loss: 250.46
train mean loss: 251.93
epoch train time: 0:00:00.702162
elapsed time: 0:01:46.615642
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 22:56:19.702832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.81
 ---- batch: 020 ----
mean loss: 254.09
 ---- batch: 030 ----
mean loss: 246.27
 ---- batch: 040 ----
mean loss: 249.71
 ---- batch: 050 ----
mean loss: 249.85
 ---- batch: 060 ----
mean loss: 259.28
 ---- batch: 070 ----
mean loss: 242.03
 ---- batch: 080 ----
mean loss: 251.14
 ---- batch: 090 ----
mean loss: 247.91
train mean loss: 251.22
epoch train time: 0:00:00.691660
elapsed time: 0:01:47.307482
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 22:56:20.394671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.12
 ---- batch: 020 ----
mean loss: 248.67
 ---- batch: 030 ----
mean loss: 259.37
 ---- batch: 040 ----
mean loss: 248.32
 ---- batch: 050 ----
mean loss: 242.45
 ---- batch: 060 ----
mean loss: 255.93
 ---- batch: 070 ----
mean loss: 248.82
 ---- batch: 080 ----
mean loss: 249.85
 ---- batch: 090 ----
mean loss: 255.11
train mean loss: 250.51
epoch train time: 0:00:00.698607
elapsed time: 0:01:48.006268
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 22:56:21.093461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.28
 ---- batch: 020 ----
mean loss: 252.65
 ---- batch: 030 ----
mean loss: 251.54
 ---- batch: 040 ----
mean loss: 240.58
 ---- batch: 050 ----
mean loss: 252.77
 ---- batch: 060 ----
mean loss: 251.26
 ---- batch: 070 ----
mean loss: 246.82
 ---- batch: 080 ----
mean loss: 249.12
 ---- batch: 090 ----
mean loss: 248.83
train mean loss: 249.83
epoch train time: 0:00:00.703591
elapsed time: 0:01:48.710053
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 22:56:21.797222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.45
 ---- batch: 020 ----
mean loss: 248.85
 ---- batch: 030 ----
mean loss: 247.89
 ---- batch: 040 ----
mean loss: 251.68
 ---- batch: 050 ----
mean loss: 245.52
 ---- batch: 060 ----
mean loss: 252.00
 ---- batch: 070 ----
mean loss: 251.14
 ---- batch: 080 ----
mean loss: 254.66
 ---- batch: 090 ----
mean loss: 247.08
train mean loss: 248.97
epoch train time: 0:00:00.690678
elapsed time: 0:01:49.400876
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 22:56:22.488049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.81
 ---- batch: 020 ----
mean loss: 246.00
 ---- batch: 030 ----
mean loss: 244.22
 ---- batch: 040 ----
mean loss: 247.60
 ---- batch: 050 ----
mean loss: 243.17
 ---- batch: 060 ----
mean loss: 249.82
 ---- batch: 070 ----
mean loss: 247.53
 ---- batch: 080 ----
mean loss: 250.15
 ---- batch: 090 ----
mean loss: 250.92
train mean loss: 248.55
epoch train time: 0:00:00.704288
elapsed time: 0:01:50.105310
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 22:56:23.192502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.98
 ---- batch: 020 ----
mean loss: 247.01
 ---- batch: 030 ----
mean loss: 249.94
 ---- batch: 040 ----
mean loss: 241.56
 ---- batch: 050 ----
mean loss: 250.28
 ---- batch: 060 ----
mean loss: 252.38
 ---- batch: 070 ----
mean loss: 252.09
 ---- batch: 080 ----
mean loss: 245.36
 ---- batch: 090 ----
mean loss: 242.32
train mean loss: 248.81
epoch train time: 0:00:00.697711
elapsed time: 0:01:50.803187
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 22:56:23.890358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.92
 ---- batch: 020 ----
mean loss: 247.69
 ---- batch: 030 ----
mean loss: 253.02
 ---- batch: 040 ----
mean loss: 245.72
 ---- batch: 050 ----
mean loss: 245.48
 ---- batch: 060 ----
mean loss: 252.49
 ---- batch: 070 ----
mean loss: 245.36
 ---- batch: 080 ----
mean loss: 246.72
 ---- batch: 090 ----
mean loss: 245.49
train mean loss: 247.73
epoch train time: 0:00:00.702171
elapsed time: 0:01:51.505516
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 22:56:24.592693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.88
 ---- batch: 020 ----
mean loss: 247.07
 ---- batch: 030 ----
mean loss: 250.70
 ---- batch: 040 ----
mean loss: 251.39
 ---- batch: 050 ----
mean loss: 252.24
 ---- batch: 060 ----
mean loss: 252.98
 ---- batch: 070 ----
mean loss: 250.25
 ---- batch: 080 ----
mean loss: 243.73
 ---- batch: 090 ----
mean loss: 239.34
train mean loss: 247.24
epoch train time: 0:00:00.705533
elapsed time: 0:01:52.211203
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 22:56:25.298379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.25
 ---- batch: 020 ----
mean loss: 246.87
 ---- batch: 030 ----
mean loss: 240.65
 ---- batch: 040 ----
mean loss: 241.35
 ---- batch: 050 ----
mean loss: 240.76
 ---- batch: 060 ----
mean loss: 252.97
 ---- batch: 070 ----
mean loss: 258.83
 ---- batch: 080 ----
mean loss: 248.25
 ---- batch: 090 ----
mean loss: 249.76
train mean loss: 247.30
epoch train time: 0:00:00.703629
elapsed time: 0:01:52.914981
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 22:56:26.002153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.59
 ---- batch: 020 ----
mean loss: 241.47
 ---- batch: 030 ----
mean loss: 247.45
 ---- batch: 040 ----
mean loss: 249.39
 ---- batch: 050 ----
mean loss: 243.25
 ---- batch: 060 ----
mean loss: 250.77
 ---- batch: 070 ----
mean loss: 239.19
 ---- batch: 080 ----
mean loss: 249.76
 ---- batch: 090 ----
mean loss: 247.15
train mean loss: 246.64
epoch train time: 0:00:00.688734
elapsed time: 0:01:53.603871
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 22:56:26.691053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.98
 ---- batch: 020 ----
mean loss: 246.04
 ---- batch: 030 ----
mean loss: 241.19
 ---- batch: 040 ----
mean loss: 246.44
 ---- batch: 050 ----
mean loss: 242.27
 ---- batch: 060 ----
mean loss: 247.73
 ---- batch: 070 ----
mean loss: 246.65
 ---- batch: 080 ----
mean loss: 238.89
 ---- batch: 090 ----
mean loss: 246.18
train mean loss: 245.74
epoch train time: 0:00:00.698640
elapsed time: 0:01:54.302670
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 22:56:27.389843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.91
 ---- batch: 020 ----
mean loss: 247.25
 ---- batch: 030 ----
mean loss: 238.90
 ---- batch: 040 ----
mean loss: 234.29
 ---- batch: 050 ----
mean loss: 255.42
 ---- batch: 060 ----
mean loss: 246.06
 ---- batch: 070 ----
mean loss: 254.90
 ---- batch: 080 ----
mean loss: 240.49
 ---- batch: 090 ----
mean loss: 242.84
train mean loss: 245.56
epoch train time: 0:00:00.696497
elapsed time: 0:01:54.999322
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 22:56:28.086513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.91
 ---- batch: 020 ----
mean loss: 235.35
 ---- batch: 030 ----
mean loss: 247.31
 ---- batch: 040 ----
mean loss: 242.53
 ---- batch: 050 ----
mean loss: 246.33
 ---- batch: 060 ----
mean loss: 248.06
 ---- batch: 070 ----
mean loss: 254.77
 ---- batch: 080 ----
mean loss: 254.46
 ---- batch: 090 ----
mean loss: 251.22
train mean loss: 244.54
epoch train time: 0:00:00.693519
elapsed time: 0:01:55.693034
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 22:56:28.780213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.42
 ---- batch: 020 ----
mean loss: 238.97
 ---- batch: 030 ----
mean loss: 244.32
 ---- batch: 040 ----
mean loss: 243.11
 ---- batch: 050 ----
mean loss: 242.88
 ---- batch: 060 ----
mean loss: 240.08
 ---- batch: 070 ----
mean loss: 247.45
 ---- batch: 080 ----
mean loss: 249.90
 ---- batch: 090 ----
mean loss: 252.72
train mean loss: 244.64
epoch train time: 0:00:00.689474
elapsed time: 0:01:56.382660
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 22:56:29.469830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.65
 ---- batch: 020 ----
mean loss: 245.66
 ---- batch: 030 ----
mean loss: 246.40
 ---- batch: 040 ----
mean loss: 249.85
 ---- batch: 050 ----
mean loss: 247.69
 ---- batch: 060 ----
mean loss: 247.60
 ---- batch: 070 ----
mean loss: 242.24
 ---- batch: 080 ----
mean loss: 247.66
 ---- batch: 090 ----
mean loss: 231.20
train mean loss: 244.39
epoch train time: 0:00:00.700771
elapsed time: 0:01:57.083570
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 22:56:30.170754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.45
 ---- batch: 020 ----
mean loss: 242.50
 ---- batch: 030 ----
mean loss: 239.06
 ---- batch: 040 ----
mean loss: 241.99
 ---- batch: 050 ----
mean loss: 235.38
 ---- batch: 060 ----
mean loss: 248.07
 ---- batch: 070 ----
mean loss: 244.88
 ---- batch: 080 ----
mean loss: 250.70
 ---- batch: 090 ----
mean loss: 244.50
train mean loss: 243.95
epoch train time: 0:00:00.687892
elapsed time: 0:01:57.771624
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 22:56:30.858800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.88
 ---- batch: 020 ----
mean loss: 242.63
 ---- batch: 030 ----
mean loss: 241.84
 ---- batch: 040 ----
mean loss: 251.63
 ---- batch: 050 ----
mean loss: 241.09
 ---- batch: 060 ----
mean loss: 244.69
 ---- batch: 070 ----
mean loss: 240.58
 ---- batch: 080 ----
mean loss: 245.95
 ---- batch: 090 ----
mean loss: 244.09
train mean loss: 243.27
epoch train time: 0:00:00.690216
elapsed time: 0:01:58.461993
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 22:56:31.549166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.94
 ---- batch: 020 ----
mean loss: 246.01
 ---- batch: 030 ----
mean loss: 241.63
 ---- batch: 040 ----
mean loss: 242.06
 ---- batch: 050 ----
mean loss: 241.70
 ---- batch: 060 ----
mean loss: 243.81
 ---- batch: 070 ----
mean loss: 243.79
 ---- batch: 080 ----
mean loss: 251.39
 ---- batch: 090 ----
mean loss: 245.15
train mean loss: 242.84
epoch train time: 0:00:00.692503
elapsed time: 0:01:59.154662
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 22:56:32.241880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.70
 ---- batch: 020 ----
mean loss: 243.33
 ---- batch: 030 ----
mean loss: 236.82
 ---- batch: 040 ----
mean loss: 248.83
 ---- batch: 050 ----
mean loss: 247.45
 ---- batch: 060 ----
mean loss: 241.70
 ---- batch: 070 ----
mean loss: 234.63
 ---- batch: 080 ----
mean loss: 243.94
 ---- batch: 090 ----
mean loss: 238.10
train mean loss: 242.66
epoch train time: 0:00:00.685852
elapsed time: 0:01:59.840701
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 22:56:32.927874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.97
 ---- batch: 020 ----
mean loss: 235.64
 ---- batch: 030 ----
mean loss: 238.31
 ---- batch: 040 ----
mean loss: 245.06
 ---- batch: 050 ----
mean loss: 242.86
 ---- batch: 060 ----
mean loss: 243.10
 ---- batch: 070 ----
mean loss: 242.29
 ---- batch: 080 ----
mean loss: 245.85
 ---- batch: 090 ----
mean loss: 240.95
train mean loss: 242.16
epoch train time: 0:00:00.686784
elapsed time: 0:02:00.527633
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 22:56:33.614842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.81
 ---- batch: 020 ----
mean loss: 235.46
 ---- batch: 030 ----
mean loss: 236.95
 ---- batch: 040 ----
mean loss: 245.14
 ---- batch: 050 ----
mean loss: 242.23
 ---- batch: 060 ----
mean loss: 251.26
 ---- batch: 070 ----
mean loss: 241.37
 ---- batch: 080 ----
mean loss: 242.95
 ---- batch: 090 ----
mean loss: 245.86
train mean loss: 241.67
epoch train time: 0:00:00.704112
elapsed time: 0:02:01.231931
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 22:56:34.319106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.66
 ---- batch: 020 ----
mean loss: 240.56
 ---- batch: 030 ----
mean loss: 229.67
 ---- batch: 040 ----
mean loss: 235.39
 ---- batch: 050 ----
mean loss: 243.93
 ---- batch: 060 ----
mean loss: 242.92
 ---- batch: 070 ----
mean loss: 241.75
 ---- batch: 080 ----
mean loss: 250.47
 ---- batch: 090 ----
mean loss: 245.10
train mean loss: 241.28
epoch train time: 0:00:00.701113
elapsed time: 0:02:01.933240
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 22:56:35.020415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.03
 ---- batch: 020 ----
mean loss: 237.52
 ---- batch: 030 ----
mean loss: 246.20
 ---- batch: 040 ----
mean loss: 240.88
 ---- batch: 050 ----
mean loss: 243.09
 ---- batch: 060 ----
mean loss: 237.96
 ---- batch: 070 ----
mean loss: 238.05
 ---- batch: 080 ----
mean loss: 244.45
 ---- batch: 090 ----
mean loss: 249.65
train mean loss: 241.09
epoch train time: 0:00:00.687715
elapsed time: 0:02:02.621121
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 22:56:35.708282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.45
 ---- batch: 020 ----
mean loss: 234.10
 ---- batch: 030 ----
mean loss: 239.13
 ---- batch: 040 ----
mean loss: 246.85
 ---- batch: 050 ----
mean loss: 241.54
 ---- batch: 060 ----
mean loss: 243.06
 ---- batch: 070 ----
mean loss: 231.72
 ---- batch: 080 ----
mean loss: 240.27
 ---- batch: 090 ----
mean loss: 247.69
train mean loss: 240.83
epoch train time: 0:00:00.692209
elapsed time: 0:02:03.313463
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 22:56:36.400636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.33
 ---- batch: 020 ----
mean loss: 231.67
 ---- batch: 030 ----
mean loss: 237.04
 ---- batch: 040 ----
mean loss: 239.00
 ---- batch: 050 ----
mean loss: 237.96
 ---- batch: 060 ----
mean loss: 239.43
 ---- batch: 070 ----
mean loss: 243.97
 ---- batch: 080 ----
mean loss: 243.09
 ---- batch: 090 ----
mean loss: 247.12
train mean loss: 240.15
epoch train time: 0:00:00.689618
elapsed time: 0:02:04.003229
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 22:56:37.090401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.27
 ---- batch: 020 ----
mean loss: 239.72
 ---- batch: 030 ----
mean loss: 230.81
 ---- batch: 040 ----
mean loss: 237.63
 ---- batch: 050 ----
mean loss: 236.24
 ---- batch: 060 ----
mean loss: 235.11
 ---- batch: 070 ----
mean loss: 243.35
 ---- batch: 080 ----
mean loss: 248.68
 ---- batch: 090 ----
mean loss: 243.74
train mean loss: 240.08
epoch train time: 0:00:00.695594
elapsed time: 0:02:04.698973
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 22:56:37.786146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.90
 ---- batch: 020 ----
mean loss: 240.01
 ---- batch: 030 ----
mean loss: 238.94
 ---- batch: 040 ----
mean loss: 248.12
 ---- batch: 050 ----
mean loss: 236.69
 ---- batch: 060 ----
mean loss: 236.49
 ---- batch: 070 ----
mean loss: 240.86
 ---- batch: 080 ----
mean loss: 240.35
 ---- batch: 090 ----
mean loss: 237.19
train mean loss: 239.61
epoch train time: 0:00:00.690084
elapsed time: 0:02:05.389203
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 22:56:38.476413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.44
 ---- batch: 020 ----
mean loss: 234.93
 ---- batch: 030 ----
mean loss: 236.18
 ---- batch: 040 ----
mean loss: 238.00
 ---- batch: 050 ----
mean loss: 240.14
 ---- batch: 060 ----
mean loss: 235.15
 ---- batch: 070 ----
mean loss: 240.97
 ---- batch: 080 ----
mean loss: 249.02
 ---- batch: 090 ----
mean loss: 238.85
train mean loss: 239.54
epoch train time: 0:00:00.691374
elapsed time: 0:02:06.080756
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 22:56:39.167925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.20
 ---- batch: 020 ----
mean loss: 237.94
 ---- batch: 030 ----
mean loss: 234.98
 ---- batch: 040 ----
mean loss: 232.12
 ---- batch: 050 ----
mean loss: 239.81
 ---- batch: 060 ----
mean loss: 242.06
 ---- batch: 070 ----
mean loss: 238.58
 ---- batch: 080 ----
mean loss: 244.52
 ---- batch: 090 ----
mean loss: 237.82
train mean loss: 238.94
epoch train time: 0:00:00.690438
elapsed time: 0:02:06.771349
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 22:56:39.858540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.46
 ---- batch: 020 ----
mean loss: 228.45
 ---- batch: 030 ----
mean loss: 235.91
 ---- batch: 040 ----
mean loss: 240.79
 ---- batch: 050 ----
mean loss: 240.03
 ---- batch: 060 ----
mean loss: 233.47
 ---- batch: 070 ----
mean loss: 246.24
 ---- batch: 080 ----
mean loss: 240.86
 ---- batch: 090 ----
mean loss: 237.97
train mean loss: 239.09
epoch train time: 0:00:00.700138
elapsed time: 0:02:07.471716
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 22:56:40.558889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.53
 ---- batch: 020 ----
mean loss: 239.48
 ---- batch: 030 ----
mean loss: 239.88
 ---- batch: 040 ----
mean loss: 236.35
 ---- batch: 050 ----
mean loss: 233.41
 ---- batch: 060 ----
mean loss: 246.03
 ---- batch: 070 ----
mean loss: 240.66
 ---- batch: 080 ----
mean loss: 235.52
 ---- batch: 090 ----
mean loss: 237.69
train mean loss: 238.61
epoch train time: 0:00:00.694679
elapsed time: 0:02:08.166571
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 22:56:41.253750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.50
 ---- batch: 020 ----
mean loss: 237.65
 ---- batch: 030 ----
mean loss: 230.92
 ---- batch: 040 ----
mean loss: 238.83
 ---- batch: 050 ----
mean loss: 235.24
 ---- batch: 060 ----
mean loss: 237.55
 ---- batch: 070 ----
mean loss: 235.59
 ---- batch: 080 ----
mean loss: 239.19
 ---- batch: 090 ----
mean loss: 241.21
train mean loss: 238.00
epoch train time: 0:00:00.694573
elapsed time: 0:02:08.861296
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 22:56:41.948467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.87
 ---- batch: 020 ----
mean loss: 237.07
 ---- batch: 030 ----
mean loss: 239.93
 ---- batch: 040 ----
mean loss: 233.40
 ---- batch: 050 ----
mean loss: 233.95
 ---- batch: 060 ----
mean loss: 234.85
 ---- batch: 070 ----
mean loss: 233.22
 ---- batch: 080 ----
mean loss: 239.97
 ---- batch: 090 ----
mean loss: 243.02
train mean loss: 237.55
epoch train time: 0:00:00.694624
elapsed time: 0:02:09.556064
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 22:56:42.643236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.53
 ---- batch: 020 ----
mean loss: 237.61
 ---- batch: 030 ----
mean loss: 237.50
 ---- batch: 040 ----
mean loss: 243.82
 ---- batch: 050 ----
mean loss: 236.99
 ---- batch: 060 ----
mean loss: 240.98
 ---- batch: 070 ----
mean loss: 242.59
 ---- batch: 080 ----
mean loss: 237.22
 ---- batch: 090 ----
mean loss: 229.11
train mean loss: 237.42
epoch train time: 0:00:00.692316
elapsed time: 0:02:10.248546
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 22:56:43.335719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.27
 ---- batch: 020 ----
mean loss: 238.00
 ---- batch: 030 ----
mean loss: 241.92
 ---- batch: 040 ----
mean loss: 235.50
 ---- batch: 050 ----
mean loss: 243.58
 ---- batch: 060 ----
mean loss: 233.33
 ---- batch: 070 ----
mean loss: 233.29
 ---- batch: 080 ----
mean loss: 236.19
 ---- batch: 090 ----
mean loss: 242.45
train mean loss: 237.23
epoch train time: 0:00:00.695944
elapsed time: 0:02:10.944638
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 22:56:44.031835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.73
 ---- batch: 020 ----
mean loss: 233.46
 ---- batch: 030 ----
mean loss: 242.64
 ---- batch: 040 ----
mean loss: 231.70
 ---- batch: 050 ----
mean loss: 232.27
 ---- batch: 060 ----
mean loss: 237.96
 ---- batch: 070 ----
mean loss: 237.39
 ---- batch: 080 ----
mean loss: 248.72
 ---- batch: 090 ----
mean loss: 231.06
train mean loss: 236.31
epoch train time: 0:00:00.690385
elapsed time: 0:02:11.635213
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 22:56:44.722385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.98
 ---- batch: 020 ----
mean loss: 234.16
 ---- batch: 030 ----
mean loss: 236.65
 ---- batch: 040 ----
mean loss: 238.31
 ---- batch: 050 ----
mean loss: 237.74
 ---- batch: 060 ----
mean loss: 238.20
 ---- batch: 070 ----
mean loss: 228.69
 ---- batch: 080 ----
mean loss: 237.16
 ---- batch: 090 ----
mean loss: 245.66
train mean loss: 236.70
epoch train time: 0:00:00.690313
elapsed time: 0:02:12.325672
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 22:56:45.412845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.63
 ---- batch: 020 ----
mean loss: 238.86
 ---- batch: 030 ----
mean loss: 240.26
 ---- batch: 040 ----
mean loss: 231.05
 ---- batch: 050 ----
mean loss: 239.23
 ---- batch: 060 ----
mean loss: 236.00
 ---- batch: 070 ----
mean loss: 236.90
 ---- batch: 080 ----
mean loss: 232.40
 ---- batch: 090 ----
mean loss: 235.49
train mean loss: 236.30
epoch train time: 0:00:00.699439
elapsed time: 0:02:13.025264
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 22:56:46.112441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.16
 ---- batch: 020 ----
mean loss: 234.01
 ---- batch: 030 ----
mean loss: 238.59
 ---- batch: 040 ----
mean loss: 238.15
 ---- batch: 050 ----
mean loss: 240.30
 ---- batch: 060 ----
mean loss: 237.96
 ---- batch: 070 ----
mean loss: 234.63
 ---- batch: 080 ----
mean loss: 233.52
 ---- batch: 090 ----
mean loss: 231.28
train mean loss: 235.87
epoch train time: 0:00:00.705472
elapsed time: 0:02:13.730892
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 22:56:46.818066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.09
 ---- batch: 020 ----
mean loss: 238.27
 ---- batch: 030 ----
mean loss: 225.75
 ---- batch: 040 ----
mean loss: 237.29
 ---- batch: 050 ----
mean loss: 233.83
 ---- batch: 060 ----
mean loss: 230.73
 ---- batch: 070 ----
mean loss: 236.48
 ---- batch: 080 ----
mean loss: 232.80
 ---- batch: 090 ----
mean loss: 231.39
train mean loss: 235.69
epoch train time: 0:00:00.696477
elapsed time: 0:02:14.427548
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 22:56:47.514718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.05
 ---- batch: 020 ----
mean loss: 244.11
 ---- batch: 030 ----
mean loss: 232.72
 ---- batch: 040 ----
mean loss: 233.78
 ---- batch: 050 ----
mean loss: 225.52
 ---- batch: 060 ----
mean loss: 240.99
 ---- batch: 070 ----
mean loss: 240.17
 ---- batch: 080 ----
mean loss: 232.13
 ---- batch: 090 ----
mean loss: 236.51
train mean loss: 235.04
epoch train time: 0:00:00.698699
elapsed time: 0:02:15.126393
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 22:56:48.213565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.10
 ---- batch: 020 ----
mean loss: 237.67
 ---- batch: 030 ----
mean loss: 228.27
 ---- batch: 040 ----
mean loss: 236.85
 ---- batch: 050 ----
mean loss: 237.45
 ---- batch: 060 ----
mean loss: 241.65
 ---- batch: 070 ----
mean loss: 229.64
 ---- batch: 080 ----
mean loss: 241.06
 ---- batch: 090 ----
mean loss: 234.02
train mean loss: 234.73
epoch train time: 0:00:00.696923
elapsed time: 0:02:15.823462
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 22:56:48.910634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.12
 ---- batch: 020 ----
mean loss: 235.87
 ---- batch: 030 ----
mean loss: 237.09
 ---- batch: 040 ----
mean loss: 234.53
 ---- batch: 050 ----
mean loss: 233.08
 ---- batch: 060 ----
mean loss: 232.54
 ---- batch: 070 ----
mean loss: 232.60
 ---- batch: 080 ----
mean loss: 234.09
 ---- batch: 090 ----
mean loss: 232.61
train mean loss: 234.71
epoch train time: 0:00:00.693349
elapsed time: 0:02:16.516957
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 22:56:49.604147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.92
 ---- batch: 020 ----
mean loss: 242.54
 ---- batch: 030 ----
mean loss: 234.44
 ---- batch: 040 ----
mean loss: 234.27
 ---- batch: 050 ----
mean loss: 232.19
 ---- batch: 060 ----
mean loss: 235.74
 ---- batch: 070 ----
mean loss: 227.26
 ---- batch: 080 ----
mean loss: 237.51
 ---- batch: 090 ----
mean loss: 235.99
train mean loss: 234.62
epoch train time: 0:00:00.707701
elapsed time: 0:02:17.224870
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 22:56:50.312055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.74
 ---- batch: 020 ----
mean loss: 231.87
 ---- batch: 030 ----
mean loss: 230.25
 ---- batch: 040 ----
mean loss: 238.89
 ---- batch: 050 ----
mean loss: 229.43
 ---- batch: 060 ----
mean loss: 230.21
 ---- batch: 070 ----
mean loss: 232.67
 ---- batch: 080 ----
mean loss: 237.31
 ---- batch: 090 ----
mean loss: 236.30
train mean loss: 233.95
epoch train time: 0:00:00.704410
elapsed time: 0:02:17.929455
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 22:56:51.016616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.08
 ---- batch: 020 ----
mean loss: 232.52
 ---- batch: 030 ----
mean loss: 232.30
 ---- batch: 040 ----
mean loss: 231.56
 ---- batch: 050 ----
mean loss: 235.55
 ---- batch: 060 ----
mean loss: 242.32
 ---- batch: 070 ----
mean loss: 238.00
 ---- batch: 080 ----
mean loss: 228.61
 ---- batch: 090 ----
mean loss: 238.10
train mean loss: 233.84
epoch train time: 0:00:00.702369
elapsed time: 0:02:18.631991
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 22:56:51.719179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.00
 ---- batch: 020 ----
mean loss: 237.39
 ---- batch: 030 ----
mean loss: 229.52
 ---- batch: 040 ----
mean loss: 238.81
 ---- batch: 050 ----
mean loss: 224.57
 ---- batch: 060 ----
mean loss: 233.83
 ---- batch: 070 ----
mean loss: 229.08
 ---- batch: 080 ----
mean loss: 239.18
 ---- batch: 090 ----
mean loss: 229.91
train mean loss: 233.61
epoch train time: 0:00:00.696670
elapsed time: 0:02:19.328826
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 22:56:52.416001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.38
 ---- batch: 020 ----
mean loss: 237.11
 ---- batch: 030 ----
mean loss: 230.69
 ---- batch: 040 ----
mean loss: 233.40
 ---- batch: 050 ----
mean loss: 231.01
 ---- batch: 060 ----
mean loss: 234.98
 ---- batch: 070 ----
mean loss: 230.72
 ---- batch: 080 ----
mean loss: 235.02
 ---- batch: 090 ----
mean loss: 242.71
train mean loss: 233.31
epoch train time: 0:00:00.699349
elapsed time: 0:02:20.028336
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 22:56:53.115505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.91
 ---- batch: 020 ----
mean loss: 242.62
 ---- batch: 030 ----
mean loss: 235.20
 ---- batch: 040 ----
mean loss: 226.48
 ---- batch: 050 ----
mean loss: 234.28
 ---- batch: 060 ----
mean loss: 231.31
 ---- batch: 070 ----
mean loss: 232.39
 ---- batch: 080 ----
mean loss: 232.24
 ---- batch: 090 ----
mean loss: 233.65
train mean loss: 232.70
epoch train time: 0:00:00.699425
elapsed time: 0:02:20.727907
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 22:56:53.815079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.82
 ---- batch: 020 ----
mean loss: 224.44
 ---- batch: 030 ----
mean loss: 238.29
 ---- batch: 040 ----
mean loss: 244.49
 ---- batch: 050 ----
mean loss: 234.89
 ---- batch: 060 ----
mean loss: 229.06
 ---- batch: 070 ----
mean loss: 227.74
 ---- batch: 080 ----
mean loss: 228.95
 ---- batch: 090 ----
mean loss: 228.80
train mean loss: 233.05
epoch train time: 0:00:00.697229
elapsed time: 0:02:21.425283
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 22:56:54.512475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.19
 ---- batch: 020 ----
mean loss: 230.51
 ---- batch: 030 ----
mean loss: 227.38
 ---- batch: 040 ----
mean loss: 237.44
 ---- batch: 050 ----
mean loss: 232.39
 ---- batch: 060 ----
mean loss: 239.35
 ---- batch: 070 ----
mean loss: 229.25
 ---- batch: 080 ----
mean loss: 236.00
 ---- batch: 090 ----
mean loss: 229.66
train mean loss: 232.51
epoch train time: 0:00:00.707543
elapsed time: 0:02:22.133003
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 22:56:55.220193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.17
 ---- batch: 020 ----
mean loss: 234.28
 ---- batch: 030 ----
mean loss: 235.67
 ---- batch: 040 ----
mean loss: 223.46
 ---- batch: 050 ----
mean loss: 232.39
 ---- batch: 060 ----
mean loss: 241.64
 ---- batch: 070 ----
mean loss: 232.23
 ---- batch: 080 ----
mean loss: 229.98
 ---- batch: 090 ----
mean loss: 233.66
train mean loss: 232.55
epoch train time: 0:00:00.719435
elapsed time: 0:02:22.852607
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 22:56:55.939784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.14
 ---- batch: 020 ----
mean loss: 226.78
 ---- batch: 030 ----
mean loss: 235.27
 ---- batch: 040 ----
mean loss: 239.63
 ---- batch: 050 ----
mean loss: 221.65
 ---- batch: 060 ----
mean loss: 234.68
 ---- batch: 070 ----
mean loss: 238.93
 ---- batch: 080 ----
mean loss: 242.10
 ---- batch: 090 ----
mean loss: 225.78
train mean loss: 232.14
epoch train time: 0:00:00.691173
elapsed time: 0:02:23.543933
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 22:56:56.631113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.10
 ---- batch: 020 ----
mean loss: 236.78
 ---- batch: 030 ----
mean loss: 229.04
 ---- batch: 040 ----
mean loss: 231.74
 ---- batch: 050 ----
mean loss: 238.72
 ---- batch: 060 ----
mean loss: 231.51
 ---- batch: 070 ----
mean loss: 231.22
 ---- batch: 080 ----
mean loss: 228.96
 ---- batch: 090 ----
mean loss: 231.08
train mean loss: 231.89
epoch train time: 0:00:00.699337
elapsed time: 0:02:24.243458
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 22:56:57.330631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.08
 ---- batch: 020 ----
mean loss: 226.44
 ---- batch: 030 ----
mean loss: 232.73
 ---- batch: 040 ----
mean loss: 227.56
 ---- batch: 050 ----
mean loss: 225.64
 ---- batch: 060 ----
mean loss: 228.98
 ---- batch: 070 ----
mean loss: 229.74
 ---- batch: 080 ----
mean loss: 237.72
 ---- batch: 090 ----
mean loss: 230.34
train mean loss: 231.99
epoch train time: 0:00:00.693491
elapsed time: 0:02:24.937097
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 22:56:58.024269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.49
 ---- batch: 020 ----
mean loss: 224.45
 ---- batch: 030 ----
mean loss: 239.15
 ---- batch: 040 ----
mean loss: 235.90
 ---- batch: 050 ----
mean loss: 236.94
 ---- batch: 060 ----
mean loss: 229.48
 ---- batch: 070 ----
mean loss: 230.95
 ---- batch: 080 ----
mean loss: 226.94
 ---- batch: 090 ----
mean loss: 230.88
train mean loss: 231.32
epoch train time: 0:00:00.694082
elapsed time: 0:02:25.631336
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 22:56:58.718512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.03
 ---- batch: 020 ----
mean loss: 229.60
 ---- batch: 030 ----
mean loss: 227.72
 ---- batch: 040 ----
mean loss: 230.67
 ---- batch: 050 ----
mean loss: 227.78
 ---- batch: 060 ----
mean loss: 231.53
 ---- batch: 070 ----
mean loss: 237.16
 ---- batch: 080 ----
mean loss: 235.06
 ---- batch: 090 ----
mean loss: 232.16
train mean loss: 231.43
epoch train time: 0:00:00.693409
elapsed time: 0:02:26.324892
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 22:56:59.412063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.85
 ---- batch: 020 ----
mean loss: 225.99
 ---- batch: 030 ----
mean loss: 236.74
 ---- batch: 040 ----
mean loss: 226.17
 ---- batch: 050 ----
mean loss: 225.75
 ---- batch: 060 ----
mean loss: 236.04
 ---- batch: 070 ----
mean loss: 238.20
 ---- batch: 080 ----
mean loss: 229.01
 ---- batch: 090 ----
mean loss: 231.67
train mean loss: 230.82
epoch train time: 0:00:00.692614
elapsed time: 0:02:27.017650
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 22:57:00.104842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.22
 ---- batch: 020 ----
mean loss: 236.28
 ---- batch: 030 ----
mean loss: 238.32
 ---- batch: 040 ----
mean loss: 231.29
 ---- batch: 050 ----
mean loss: 224.76
 ---- batch: 060 ----
mean loss: 238.38
 ---- batch: 070 ----
mean loss: 229.36
 ---- batch: 080 ----
mean loss: 230.72
 ---- batch: 090 ----
mean loss: 226.14
train mean loss: 230.51
epoch train time: 0:00:00.694666
elapsed time: 0:02:27.712485
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 22:57:00.799659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.09
 ---- batch: 020 ----
mean loss: 222.76
 ---- batch: 030 ----
mean loss: 233.76
 ---- batch: 040 ----
mean loss: 231.52
 ---- batch: 050 ----
mean loss: 231.29
 ---- batch: 060 ----
mean loss: 235.04
 ---- batch: 070 ----
mean loss: 231.98
 ---- batch: 080 ----
mean loss: 226.45
 ---- batch: 090 ----
mean loss: 232.29
train mean loss: 230.52
epoch train time: 0:00:00.689980
elapsed time: 0:02:28.402612
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 22:57:01.489785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.74
 ---- batch: 020 ----
mean loss: 225.46
 ---- batch: 030 ----
mean loss: 230.37
 ---- batch: 040 ----
mean loss: 225.75
 ---- batch: 050 ----
mean loss: 237.09
 ---- batch: 060 ----
mean loss: 235.76
 ---- batch: 070 ----
mean loss: 229.27
 ---- batch: 080 ----
mean loss: 223.36
 ---- batch: 090 ----
mean loss: 236.43
train mean loss: 230.18
epoch train time: 0:00:00.696994
elapsed time: 0:02:29.099754
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 22:57:02.186937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.56
 ---- batch: 020 ----
mean loss: 226.34
 ---- batch: 030 ----
mean loss: 224.99
 ---- batch: 040 ----
mean loss: 239.41
 ---- batch: 050 ----
mean loss: 231.18
 ---- batch: 060 ----
mean loss: 240.03
 ---- batch: 070 ----
mean loss: 223.10
 ---- batch: 080 ----
mean loss: 221.23
 ---- batch: 090 ----
mean loss: 233.73
train mean loss: 230.03
epoch train time: 0:00:00.690127
elapsed time: 0:02:29.790054
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 22:57:02.877243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.47
 ---- batch: 020 ----
mean loss: 228.57
 ---- batch: 030 ----
mean loss: 231.76
 ---- batch: 040 ----
mean loss: 227.27
 ---- batch: 050 ----
mean loss: 231.45
 ---- batch: 060 ----
mean loss: 223.30
 ---- batch: 070 ----
mean loss: 221.52
 ---- batch: 080 ----
mean loss: 230.60
 ---- batch: 090 ----
mean loss: 236.86
train mean loss: 229.87
epoch train time: 0:00:00.691089
elapsed time: 0:02:30.481323
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 22:57:03.568501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.58
 ---- batch: 020 ----
mean loss: 227.35
 ---- batch: 030 ----
mean loss: 236.03
 ---- batch: 040 ----
mean loss: 219.88
 ---- batch: 050 ----
mean loss: 230.83
 ---- batch: 060 ----
mean loss: 226.48
 ---- batch: 070 ----
mean loss: 234.13
 ---- batch: 080 ----
mean loss: 229.06
 ---- batch: 090 ----
mean loss: 227.45
train mean loss: 229.72
epoch train time: 0:00:00.716184
elapsed time: 0:02:31.197711
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 22:57:04.284890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.67
 ---- batch: 020 ----
mean loss: 229.87
 ---- batch: 030 ----
mean loss: 223.70
 ---- batch: 040 ----
mean loss: 226.96
 ---- batch: 050 ----
mean loss: 227.27
 ---- batch: 060 ----
mean loss: 236.90
 ---- batch: 070 ----
mean loss: 226.93
 ---- batch: 080 ----
mean loss: 230.03
 ---- batch: 090 ----
mean loss: 228.44
train mean loss: 229.53
epoch train time: 0:00:00.705373
elapsed time: 0:02:31.903247
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 22:57:04.990439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.26
 ---- batch: 020 ----
mean loss: 225.86
 ---- batch: 030 ----
mean loss: 224.93
 ---- batch: 040 ----
mean loss: 231.70
 ---- batch: 050 ----
mean loss: 224.47
 ---- batch: 060 ----
mean loss: 232.29
 ---- batch: 070 ----
mean loss: 236.08
 ---- batch: 080 ----
mean loss: 233.41
 ---- batch: 090 ----
mean loss: 226.80
train mean loss: 229.12
epoch train time: 0:00:00.688012
elapsed time: 0:02:32.591436
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 22:57:05.678611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.76
 ---- batch: 020 ----
mean loss: 226.71
 ---- batch: 030 ----
mean loss: 227.29
 ---- batch: 040 ----
mean loss: 232.25
 ---- batch: 050 ----
mean loss: 233.42
 ---- batch: 060 ----
mean loss: 232.44
 ---- batch: 070 ----
mean loss: 233.31
 ---- batch: 080 ----
mean loss: 219.23
 ---- batch: 090 ----
mean loss: 226.39
train mean loss: 228.63
epoch train time: 0:00:00.694021
elapsed time: 0:02:33.285608
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 22:57:06.372783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.61
 ---- batch: 020 ----
mean loss: 225.93
 ---- batch: 030 ----
mean loss: 220.25
 ---- batch: 040 ----
mean loss: 228.73
 ---- batch: 050 ----
mean loss: 229.25
 ---- batch: 060 ----
mean loss: 231.98
 ---- batch: 070 ----
mean loss: 238.63
 ---- batch: 080 ----
mean loss: 223.80
 ---- batch: 090 ----
mean loss: 232.99
train mean loss: 228.77
epoch train time: 0:00:00.695350
elapsed time: 0:02:33.981103
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 22:57:07.068274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.45
 ---- batch: 020 ----
mean loss: 218.30
 ---- batch: 030 ----
mean loss: 231.16
 ---- batch: 040 ----
mean loss: 235.10
 ---- batch: 050 ----
mean loss: 227.13
 ---- batch: 060 ----
mean loss: 220.16
 ---- batch: 070 ----
mean loss: 230.98
 ---- batch: 080 ----
mean loss: 232.39
 ---- batch: 090 ----
mean loss: 225.88
train mean loss: 228.53
epoch train time: 0:00:00.690944
elapsed time: 0:02:34.672190
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 22:57:07.759365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.70
 ---- batch: 020 ----
mean loss: 224.00
 ---- batch: 030 ----
mean loss: 235.16
 ---- batch: 040 ----
mean loss: 222.63
 ---- batch: 050 ----
mean loss: 222.63
 ---- batch: 060 ----
mean loss: 234.92
 ---- batch: 070 ----
mean loss: 231.04
 ---- batch: 080 ----
mean loss: 231.13
 ---- batch: 090 ----
mean loss: 227.35
train mean loss: 228.33
epoch train time: 0:00:00.695299
elapsed time: 0:02:35.367658
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 22:57:08.454819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.06
 ---- batch: 020 ----
mean loss: 231.99
 ---- batch: 030 ----
mean loss: 238.99
 ---- batch: 040 ----
mean loss: 220.86
 ---- batch: 050 ----
mean loss: 225.41
 ---- batch: 060 ----
mean loss: 232.08
 ---- batch: 070 ----
mean loss: 221.39
 ---- batch: 080 ----
mean loss: 228.07
 ---- batch: 090 ----
mean loss: 228.16
train mean loss: 227.91
epoch train time: 0:00:00.695498
elapsed time: 0:02:36.063295
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 22:57:09.150468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.52
 ---- batch: 020 ----
mean loss: 221.21
 ---- batch: 030 ----
mean loss: 227.06
 ---- batch: 040 ----
mean loss: 231.73
 ---- batch: 050 ----
mean loss: 227.62
 ---- batch: 060 ----
mean loss: 231.85
 ---- batch: 070 ----
mean loss: 232.78
 ---- batch: 080 ----
mean loss: 222.03
 ---- batch: 090 ----
mean loss: 225.24
train mean loss: 227.80
epoch train time: 0:00:00.689410
elapsed time: 0:02:36.752848
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 22:57:09.840019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.09
 ---- batch: 020 ----
mean loss: 225.85
 ---- batch: 030 ----
mean loss: 226.00
 ---- batch: 040 ----
mean loss: 229.97
 ---- batch: 050 ----
mean loss: 234.44
 ---- batch: 060 ----
mean loss: 225.30
 ---- batch: 070 ----
mean loss: 222.51
 ---- batch: 080 ----
mean loss: 218.71
 ---- batch: 090 ----
mean loss: 236.96
train mean loss: 227.59
epoch train time: 0:00:00.690498
elapsed time: 0:02:37.443493
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 22:57:10.530682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.17
 ---- batch: 020 ----
mean loss: 232.18
 ---- batch: 030 ----
mean loss: 228.45
 ---- batch: 040 ----
mean loss: 221.50
 ---- batch: 050 ----
mean loss: 220.93
 ---- batch: 060 ----
mean loss: 228.35
 ---- batch: 070 ----
mean loss: 231.09
 ---- batch: 080 ----
mean loss: 227.25
 ---- batch: 090 ----
mean loss: 223.50
train mean loss: 227.20
epoch train time: 0:00:00.700226
elapsed time: 0:02:38.143886
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 22:57:11.231061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.81
 ---- batch: 020 ----
mean loss: 231.21
 ---- batch: 030 ----
mean loss: 228.24
 ---- batch: 040 ----
mean loss: 231.05
 ---- batch: 050 ----
mean loss: 220.92
 ---- batch: 060 ----
mean loss: 231.35
 ---- batch: 070 ----
mean loss: 223.03
 ---- batch: 080 ----
mean loss: 220.96
 ---- batch: 090 ----
mean loss: 225.75
train mean loss: 227.36
epoch train time: 0:00:00.706248
elapsed time: 0:02:38.850304
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 22:57:11.937526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.47
 ---- batch: 020 ----
mean loss: 228.07
 ---- batch: 030 ----
mean loss: 229.19
 ---- batch: 040 ----
mean loss: 226.90
 ---- batch: 050 ----
mean loss: 232.34
 ---- batch: 060 ----
mean loss: 225.21
 ---- batch: 070 ----
mean loss: 221.72
 ---- batch: 080 ----
mean loss: 225.58
 ---- batch: 090 ----
mean loss: 227.40
train mean loss: 227.20
epoch train time: 0:00:00.692259
elapsed time: 0:02:39.542810
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 22:57:12.629984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.60
 ---- batch: 020 ----
mean loss: 227.38
 ---- batch: 030 ----
mean loss: 227.69
 ---- batch: 040 ----
mean loss: 234.60
 ---- batch: 050 ----
mean loss: 224.93
 ---- batch: 060 ----
mean loss: 223.79
 ---- batch: 070 ----
mean loss: 223.98
 ---- batch: 080 ----
mean loss: 227.94
 ---- batch: 090 ----
mean loss: 224.67
train mean loss: 227.18
epoch train time: 0:00:00.700333
elapsed time: 0:02:40.243302
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 22:57:13.330476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.46
 ---- batch: 020 ----
mean loss: 230.08
 ---- batch: 030 ----
mean loss: 227.36
 ---- batch: 040 ----
mean loss: 228.22
 ---- batch: 050 ----
mean loss: 221.61
 ---- batch: 060 ----
mean loss: 225.92
 ---- batch: 070 ----
mean loss: 229.58
 ---- batch: 080 ----
mean loss: 225.45
 ---- batch: 090 ----
mean loss: 224.20
train mean loss: 226.74
epoch train time: 0:00:00.695693
elapsed time: 0:02:40.939152
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 22:57:14.026322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.18
 ---- batch: 020 ----
mean loss: 225.89
 ---- batch: 030 ----
mean loss: 233.15
 ---- batch: 040 ----
mean loss: 225.80
 ---- batch: 050 ----
mean loss: 224.05
 ---- batch: 060 ----
mean loss: 231.72
 ---- batch: 070 ----
mean loss: 223.76
 ---- batch: 080 ----
mean loss: 230.68
 ---- batch: 090 ----
mean loss: 214.85
train mean loss: 226.40
epoch train time: 0:00:00.708039
elapsed time: 0:02:41.647343
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 22:57:14.734517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.52
 ---- batch: 020 ----
mean loss: 225.41
 ---- batch: 030 ----
mean loss: 221.39
 ---- batch: 040 ----
mean loss: 232.55
 ---- batch: 050 ----
mean loss: 229.59
 ---- batch: 060 ----
mean loss: 225.89
 ---- batch: 070 ----
mean loss: 227.35
 ---- batch: 080 ----
mean loss: 227.55
 ---- batch: 090 ----
mean loss: 221.89
train mean loss: 226.49
epoch train time: 0:00:00.698984
elapsed time: 0:02:42.346518
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 22:57:15.433748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.33
 ---- batch: 020 ----
mean loss: 222.57
 ---- batch: 030 ----
mean loss: 226.39
 ---- batch: 040 ----
mean loss: 227.48
 ---- batch: 050 ----
mean loss: 230.44
 ---- batch: 060 ----
mean loss: 236.68
 ---- batch: 070 ----
mean loss: 223.87
 ---- batch: 080 ----
mean loss: 222.15
 ---- batch: 090 ----
mean loss: 221.94
train mean loss: 226.66
epoch train time: 0:00:00.702121
elapsed time: 0:02:43.048853
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 22:57:16.136035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.36
 ---- batch: 020 ----
mean loss: 218.91
 ---- batch: 030 ----
mean loss: 225.20
 ---- batch: 040 ----
mean loss: 233.05
 ---- batch: 050 ----
mean loss: 238.42
 ---- batch: 060 ----
mean loss: 224.75
 ---- batch: 070 ----
mean loss: 223.34
 ---- batch: 080 ----
mean loss: 224.33
 ---- batch: 090 ----
mean loss: 227.23
train mean loss: 226.12
epoch train time: 0:00:00.696439
elapsed time: 0:02:43.745474
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 22:57:16.832673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.47
 ---- batch: 020 ----
mean loss: 222.51
 ---- batch: 030 ----
mean loss: 229.83
 ---- batch: 040 ----
mean loss: 227.39
 ---- batch: 050 ----
mean loss: 219.04
 ---- batch: 060 ----
mean loss: 229.32
 ---- batch: 070 ----
mean loss: 231.04
 ---- batch: 080 ----
mean loss: 222.26
 ---- batch: 090 ----
mean loss: 228.50
train mean loss: 226.11
epoch train time: 0:00:00.702199
elapsed time: 0:02:44.447868
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 22:57:17.535043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.04
 ---- batch: 020 ----
mean loss: 227.55
 ---- batch: 030 ----
mean loss: 220.35
 ---- batch: 040 ----
mean loss: 223.59
 ---- batch: 050 ----
mean loss: 228.98
 ---- batch: 060 ----
mean loss: 227.67
 ---- batch: 070 ----
mean loss: 222.78
 ---- batch: 080 ----
mean loss: 227.70
 ---- batch: 090 ----
mean loss: 228.96
train mean loss: 225.57
epoch train time: 0:00:00.701412
elapsed time: 0:02:45.149435
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 22:57:18.236611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.30
 ---- batch: 020 ----
mean loss: 228.09
 ---- batch: 030 ----
mean loss: 221.80
 ---- batch: 040 ----
mean loss: 220.82
 ---- batch: 050 ----
mean loss: 231.00
 ---- batch: 060 ----
mean loss: 221.36
 ---- batch: 070 ----
mean loss: 229.99
 ---- batch: 080 ----
mean loss: 227.73
 ---- batch: 090 ----
mean loss: 226.68
train mean loss: 225.86
epoch train time: 0:00:00.707389
elapsed time: 0:02:45.856977
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 22:57:18.944163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.55
 ---- batch: 020 ----
mean loss: 224.41
 ---- batch: 030 ----
mean loss: 234.38
 ---- batch: 040 ----
mean loss: 221.28
 ---- batch: 050 ----
mean loss: 226.58
 ---- batch: 060 ----
mean loss: 217.53
 ---- batch: 070 ----
mean loss: 216.28
 ---- batch: 080 ----
mean loss: 230.70
 ---- batch: 090 ----
mean loss: 228.77
train mean loss: 225.40
epoch train time: 0:00:00.702586
elapsed time: 0:02:46.559768
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 22:57:19.646964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.71
 ---- batch: 020 ----
mean loss: 222.28
 ---- batch: 030 ----
mean loss: 228.27
 ---- batch: 040 ----
mean loss: 226.69
 ---- batch: 050 ----
mean loss: 229.08
 ---- batch: 060 ----
mean loss: 224.95
 ---- batch: 070 ----
mean loss: 222.81
 ---- batch: 080 ----
mean loss: 234.52
 ---- batch: 090 ----
mean loss: 219.38
train mean loss: 225.49
epoch train time: 0:00:00.706020
elapsed time: 0:02:47.266018
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 22:57:20.353188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.70
 ---- batch: 020 ----
mean loss: 229.54
 ---- batch: 030 ----
mean loss: 226.94
 ---- batch: 040 ----
mean loss: 220.05
 ---- batch: 050 ----
mean loss: 226.78
 ---- batch: 060 ----
mean loss: 225.11
 ---- batch: 070 ----
mean loss: 225.83
 ---- batch: 080 ----
mean loss: 222.02
 ---- batch: 090 ----
mean loss: 222.87
train mean loss: 225.17
epoch train time: 0:00:00.717277
elapsed time: 0:02:47.983437
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 22:57:21.070623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.32
 ---- batch: 020 ----
mean loss: 223.87
 ---- batch: 030 ----
mean loss: 220.87
 ---- batch: 040 ----
mean loss: 226.29
 ---- batch: 050 ----
mean loss: 224.84
 ---- batch: 060 ----
mean loss: 230.83
 ---- batch: 070 ----
mean loss: 219.90
 ---- batch: 080 ----
mean loss: 223.70
 ---- batch: 090 ----
mean loss: 223.36
train mean loss: 224.66
epoch train time: 0:00:00.697843
elapsed time: 0:02:48.681451
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 22:57:21.768618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.00
 ---- batch: 020 ----
mean loss: 219.66
 ---- batch: 030 ----
mean loss: 225.08
 ---- batch: 040 ----
mean loss: 223.98
 ---- batch: 050 ----
mean loss: 224.29
 ---- batch: 060 ----
mean loss: 213.72
 ---- batch: 070 ----
mean loss: 230.68
 ---- batch: 080 ----
mean loss: 229.87
 ---- batch: 090 ----
mean loss: 226.99
train mean loss: 224.88
epoch train time: 0:00:00.695877
elapsed time: 0:02:49.377467
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 22:57:22.464643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.81
 ---- batch: 020 ----
mean loss: 222.36
 ---- batch: 030 ----
mean loss: 223.34
 ---- batch: 040 ----
mean loss: 222.91
 ---- batch: 050 ----
mean loss: 217.94
 ---- batch: 060 ----
mean loss: 228.32
 ---- batch: 070 ----
mean loss: 227.97
 ---- batch: 080 ----
mean loss: 223.76
 ---- batch: 090 ----
mean loss: 226.21
train mean loss: 224.89
epoch train time: 0:00:00.703120
elapsed time: 0:02:50.080762
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 22:57:23.167935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.32
 ---- batch: 020 ----
mean loss: 222.04
 ---- batch: 030 ----
mean loss: 230.54
 ---- batch: 040 ----
mean loss: 222.84
 ---- batch: 050 ----
mean loss: 221.70
 ---- batch: 060 ----
mean loss: 229.41
 ---- batch: 070 ----
mean loss: 227.83
 ---- batch: 080 ----
mean loss: 224.55
 ---- batch: 090 ----
mean loss: 223.43
train mean loss: 225.03
epoch train time: 0:00:00.696147
elapsed time: 0:02:50.777068
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 22:57:23.864234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.69
 ---- batch: 020 ----
mean loss: 227.12
 ---- batch: 030 ----
mean loss: 221.72
 ---- batch: 040 ----
mean loss: 227.51
 ---- batch: 050 ----
mean loss: 221.72
 ---- batch: 060 ----
mean loss: 220.98
 ---- batch: 070 ----
mean loss: 223.07
 ---- batch: 080 ----
mean loss: 219.75
 ---- batch: 090 ----
mean loss: 230.60
train mean loss: 224.28
epoch train time: 0:00:00.694128
elapsed time: 0:02:51.471386
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 22:57:24.558559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.14
 ---- batch: 020 ----
mean loss: 225.89
 ---- batch: 030 ----
mean loss: 226.46
 ---- batch: 040 ----
mean loss: 222.68
 ---- batch: 050 ----
mean loss: 222.01
 ---- batch: 060 ----
mean loss: 230.31
 ---- batch: 070 ----
mean loss: 218.45
 ---- batch: 080 ----
mean loss: 219.44
 ---- batch: 090 ----
mean loss: 233.16
train mean loss: 224.20
epoch train time: 0:00:00.701107
elapsed time: 0:02:52.172643
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 22:57:25.259817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.92
 ---- batch: 020 ----
mean loss: 220.48
 ---- batch: 030 ----
mean loss: 220.06
 ---- batch: 040 ----
mean loss: 232.13
 ---- batch: 050 ----
mean loss: 230.22
 ---- batch: 060 ----
mean loss: 224.04
 ---- batch: 070 ----
mean loss: 225.64
 ---- batch: 080 ----
mean loss: 229.30
 ---- batch: 090 ----
mean loss: 215.50
train mean loss: 224.32
epoch train time: 0:00:00.695661
elapsed time: 0:02:52.868458
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 22:57:25.955645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.53
 ---- batch: 020 ----
mean loss: 214.13
 ---- batch: 030 ----
mean loss: 229.28
 ---- batch: 040 ----
mean loss: 225.73
 ---- batch: 050 ----
mean loss: 226.76
 ---- batch: 060 ----
mean loss: 228.27
 ---- batch: 070 ----
mean loss: 222.44
 ---- batch: 080 ----
mean loss: 229.60
 ---- batch: 090 ----
mean loss: 218.29
train mean loss: 223.84
epoch train time: 0:00:00.691166
elapsed time: 0:02:53.559808
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 22:57:26.646983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.84
 ---- batch: 020 ----
mean loss: 226.60
 ---- batch: 030 ----
mean loss: 218.94
 ---- batch: 040 ----
mean loss: 223.76
 ---- batch: 050 ----
mean loss: 225.36
 ---- batch: 060 ----
mean loss: 215.73
 ---- batch: 070 ----
mean loss: 224.28
 ---- batch: 080 ----
mean loss: 228.08
 ---- batch: 090 ----
mean loss: 229.04
train mean loss: 224.24
epoch train time: 0:00:00.690615
elapsed time: 0:02:54.250611
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 22:57:27.337786
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.03
 ---- batch: 020 ----
mean loss: 220.57
 ---- batch: 030 ----
mean loss: 220.04
 ---- batch: 040 ----
mean loss: 232.36
 ---- batch: 050 ----
mean loss: 223.22
 ---- batch: 060 ----
mean loss: 213.51
 ---- batch: 070 ----
mean loss: 222.71
 ---- batch: 080 ----
mean loss: 226.40
 ---- batch: 090 ----
mean loss: 226.47
train mean loss: 223.58
epoch train time: 0:00:00.700063
elapsed time: 0:02:54.950844
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 22:57:28.038004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.84
 ---- batch: 020 ----
mean loss: 221.67
 ---- batch: 030 ----
mean loss: 225.42
 ---- batch: 040 ----
mean loss: 215.73
 ---- batch: 050 ----
mean loss: 227.73
 ---- batch: 060 ----
mean loss: 216.03
 ---- batch: 070 ----
mean loss: 224.16
 ---- batch: 080 ----
mean loss: 225.78
 ---- batch: 090 ----
mean loss: 221.19
train mean loss: 223.29
epoch train time: 0:00:00.695640
elapsed time: 0:02:55.646631
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 22:57:28.733801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.40
 ---- batch: 020 ----
mean loss: 224.73
 ---- batch: 030 ----
mean loss: 217.52
 ---- batch: 040 ----
mean loss: 224.94
 ---- batch: 050 ----
mean loss: 224.49
 ---- batch: 060 ----
mean loss: 216.14
 ---- batch: 070 ----
mean loss: 234.73
 ---- batch: 080 ----
mean loss: 219.71
 ---- batch: 090 ----
mean loss: 224.63
train mean loss: 223.25
epoch train time: 0:00:00.699187
elapsed time: 0:02:56.345997
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 22:57:29.433170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.62
 ---- batch: 020 ----
mean loss: 227.81
 ---- batch: 030 ----
mean loss: 224.02
 ---- batch: 040 ----
mean loss: 221.98
 ---- batch: 050 ----
mean loss: 220.90
 ---- batch: 060 ----
mean loss: 227.44
 ---- batch: 070 ----
mean loss: 221.33
 ---- batch: 080 ----
mean loss: 230.16
 ---- batch: 090 ----
mean loss: 219.37
train mean loss: 223.28
epoch train time: 0:00:00.705467
elapsed time: 0:02:57.051610
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 22:57:30.138779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.46
 ---- batch: 020 ----
mean loss: 223.71
 ---- batch: 030 ----
mean loss: 232.25
 ---- batch: 040 ----
mean loss: 211.76
 ---- batch: 050 ----
mean loss: 225.78
 ---- batch: 060 ----
mean loss: 223.10
 ---- batch: 070 ----
mean loss: 222.43
 ---- batch: 080 ----
mean loss: 235.46
 ---- batch: 090 ----
mean loss: 219.61
train mean loss: 223.22
epoch train time: 0:00:00.695804
elapsed time: 0:02:57.747628
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 22:57:30.834854
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.56
 ---- batch: 020 ----
mean loss: 219.11
 ---- batch: 030 ----
mean loss: 218.57
 ---- batch: 040 ----
mean loss: 232.09
 ---- batch: 050 ----
mean loss: 224.58
 ---- batch: 060 ----
mean loss: 221.85
 ---- batch: 070 ----
mean loss: 226.06
 ---- batch: 080 ----
mean loss: 228.25
 ---- batch: 090 ----
mean loss: 220.95
train mean loss: 223.39
epoch train time: 0:00:00.702616
elapsed time: 0:02:58.450444
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 22:57:31.537660
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.13
 ---- batch: 020 ----
mean loss: 217.59
 ---- batch: 030 ----
mean loss: 224.84
 ---- batch: 040 ----
mean loss: 231.35
 ---- batch: 050 ----
mean loss: 220.15
 ---- batch: 060 ----
mean loss: 215.43
 ---- batch: 070 ----
mean loss: 226.10
 ---- batch: 080 ----
mean loss: 224.66
 ---- batch: 090 ----
mean loss: 223.53
train mean loss: 223.36
epoch train time: 0:00:00.698582
elapsed time: 0:02:59.149219
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 22:57:32.236394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.84
 ---- batch: 020 ----
mean loss: 226.97
 ---- batch: 030 ----
mean loss: 217.73
 ---- batch: 040 ----
mean loss: 234.25
 ---- batch: 050 ----
mean loss: 227.09
 ---- batch: 060 ----
mean loss: 219.82
 ---- batch: 070 ----
mean loss: 220.74
 ---- batch: 080 ----
mean loss: 223.30
 ---- batch: 090 ----
mean loss: 220.82
train mean loss: 223.16
epoch train time: 0:00:00.706374
elapsed time: 0:02:59.855761
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 22:57:32.942935
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.40
 ---- batch: 020 ----
mean loss: 226.78
 ---- batch: 030 ----
mean loss: 221.74
 ---- batch: 040 ----
mean loss: 218.79
 ---- batch: 050 ----
mean loss: 228.91
 ---- batch: 060 ----
mean loss: 229.11
 ---- batch: 070 ----
mean loss: 224.70
 ---- batch: 080 ----
mean loss: 222.43
 ---- batch: 090 ----
mean loss: 218.12
train mean loss: 223.31
epoch train time: 0:00:00.688863
elapsed time: 0:03:00.544775
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 22:57:33.631947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.37
 ---- batch: 020 ----
mean loss: 226.94
 ---- batch: 030 ----
mean loss: 225.23
 ---- batch: 040 ----
mean loss: 219.50
 ---- batch: 050 ----
mean loss: 216.81
 ---- batch: 060 ----
mean loss: 230.99
 ---- batch: 070 ----
mean loss: 220.17
 ---- batch: 080 ----
mean loss: 222.95
 ---- batch: 090 ----
mean loss: 221.49
train mean loss: 223.10
epoch train time: 0:00:00.692381
elapsed time: 0:03:01.237313
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 22:57:34.324501
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.97
 ---- batch: 020 ----
mean loss: 224.54
 ---- batch: 030 ----
mean loss: 222.60
 ---- batch: 040 ----
mean loss: 224.30
 ---- batch: 050 ----
mean loss: 220.48
 ---- batch: 060 ----
mean loss: 222.22
 ---- batch: 070 ----
mean loss: 225.81
 ---- batch: 080 ----
mean loss: 220.06
 ---- batch: 090 ----
mean loss: 221.12
train mean loss: 223.42
epoch train time: 0:00:00.698959
elapsed time: 0:03:01.936433
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 22:57:35.023607
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.86
 ---- batch: 020 ----
mean loss: 226.96
 ---- batch: 030 ----
mean loss: 220.62
 ---- batch: 040 ----
mean loss: 224.88
 ---- batch: 050 ----
mean loss: 220.82
 ---- batch: 060 ----
mean loss: 215.87
 ---- batch: 070 ----
mean loss: 227.93
 ---- batch: 080 ----
mean loss: 222.98
 ---- batch: 090 ----
mean loss: 222.30
train mean loss: 223.30
epoch train time: 0:00:00.694467
elapsed time: 0:03:02.631051
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 22:57:35.718224
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.84
 ---- batch: 020 ----
mean loss: 224.70
 ---- batch: 030 ----
mean loss: 222.78
 ---- batch: 040 ----
mean loss: 225.43
 ---- batch: 050 ----
mean loss: 229.05
 ---- batch: 060 ----
mean loss: 224.85
 ---- batch: 070 ----
mean loss: 217.92
 ---- batch: 080 ----
mean loss: 222.00
 ---- batch: 090 ----
mean loss: 219.32
train mean loss: 223.03
epoch train time: 0:00:00.691423
elapsed time: 0:03:03.322639
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 22:57:36.409830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.35
 ---- batch: 020 ----
mean loss: 209.84
 ---- batch: 030 ----
mean loss: 218.05
 ---- batch: 040 ----
mean loss: 219.64
 ---- batch: 050 ----
mean loss: 224.14
 ---- batch: 060 ----
mean loss: 232.08
 ---- batch: 070 ----
mean loss: 230.45
 ---- batch: 080 ----
mean loss: 232.42
 ---- batch: 090 ----
mean loss: 226.55
train mean loss: 223.54
epoch train time: 0:00:00.692174
elapsed time: 0:03:04.014981
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 22:57:37.102153
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.59
 ---- batch: 020 ----
mean loss: 225.58
 ---- batch: 030 ----
mean loss: 223.28
 ---- batch: 040 ----
mean loss: 221.50
 ---- batch: 050 ----
mean loss: 222.93
 ---- batch: 060 ----
mean loss: 227.18
 ---- batch: 070 ----
mean loss: 220.84
 ---- batch: 080 ----
mean loss: 220.74
 ---- batch: 090 ----
mean loss: 219.64
train mean loss: 223.32
epoch train time: 0:00:00.690977
elapsed time: 0:03:04.706130
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 22:57:37.793293
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.08
 ---- batch: 020 ----
mean loss: 221.64
 ---- batch: 030 ----
mean loss: 227.13
 ---- batch: 040 ----
mean loss: 219.32
 ---- batch: 050 ----
mean loss: 222.81
 ---- batch: 060 ----
mean loss: 225.57
 ---- batch: 070 ----
mean loss: 215.15
 ---- batch: 080 ----
mean loss: 229.69
 ---- batch: 090 ----
mean loss: 222.61
train mean loss: 222.91
epoch train time: 0:00:00.703012
elapsed time: 0:03:05.409278
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 22:57:38.496451
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.15
 ---- batch: 020 ----
mean loss: 222.31
 ---- batch: 030 ----
mean loss: 228.56
 ---- batch: 040 ----
mean loss: 219.46
 ---- batch: 050 ----
mean loss: 226.39
 ---- batch: 060 ----
mean loss: 221.25
 ---- batch: 070 ----
mean loss: 215.56
 ---- batch: 080 ----
mean loss: 230.17
 ---- batch: 090 ----
mean loss: 222.54
train mean loss: 223.23
epoch train time: 0:00:00.702128
elapsed time: 0:03:06.111569
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 22:57:39.198741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.40
 ---- batch: 020 ----
mean loss: 227.71
 ---- batch: 030 ----
mean loss: 224.87
 ---- batch: 040 ----
mean loss: 224.18
 ---- batch: 050 ----
mean loss: 217.67
 ---- batch: 060 ----
mean loss: 221.81
 ---- batch: 070 ----
mean loss: 220.50
 ---- batch: 080 ----
mean loss: 228.32
 ---- batch: 090 ----
mean loss: 223.37
train mean loss: 223.34
epoch train time: 0:00:00.699572
elapsed time: 0:03:06.811293
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 22:57:39.898469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.52
 ---- batch: 020 ----
mean loss: 222.63
 ---- batch: 030 ----
mean loss: 228.02
 ---- batch: 040 ----
mean loss: 224.35
 ---- batch: 050 ----
mean loss: 231.27
 ---- batch: 060 ----
mean loss: 213.59
 ---- batch: 070 ----
mean loss: 220.74
 ---- batch: 080 ----
mean loss: 227.93
 ---- batch: 090 ----
mean loss: 223.41
train mean loss: 223.10
epoch train time: 0:00:00.704288
elapsed time: 0:03:07.515737
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 22:57:40.602910
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.51
 ---- batch: 020 ----
mean loss: 220.31
 ---- batch: 030 ----
mean loss: 225.08
 ---- batch: 040 ----
mean loss: 220.24
 ---- batch: 050 ----
mean loss: 226.46
 ---- batch: 060 ----
mean loss: 221.90
 ---- batch: 070 ----
mean loss: 219.96
 ---- batch: 080 ----
mean loss: 228.82
 ---- batch: 090 ----
mean loss: 223.03
train mean loss: 223.33
epoch train time: 0:00:00.700464
elapsed time: 0:03:08.216371
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 22:57:41.303544
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.00
 ---- batch: 020 ----
mean loss: 218.68
 ---- batch: 030 ----
mean loss: 222.64
 ---- batch: 040 ----
mean loss: 223.26
 ---- batch: 050 ----
mean loss: 225.44
 ---- batch: 060 ----
mean loss: 229.99
 ---- batch: 070 ----
mean loss: 225.01
 ---- batch: 080 ----
mean loss: 229.28
 ---- batch: 090 ----
mean loss: 213.65
train mean loss: 223.00
epoch train time: 0:00:00.697919
elapsed time: 0:03:08.914440
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 22:57:42.001653
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.61
 ---- batch: 020 ----
mean loss: 224.33
 ---- batch: 030 ----
mean loss: 219.93
 ---- batch: 040 ----
mean loss: 219.39
 ---- batch: 050 ----
mean loss: 220.05
 ---- batch: 060 ----
mean loss: 222.43
 ---- batch: 070 ----
mean loss: 224.51
 ---- batch: 080 ----
mean loss: 232.91
 ---- batch: 090 ----
mean loss: 227.22
train mean loss: 223.03
epoch train time: 0:00:00.691863
elapsed time: 0:03:09.606525
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 22:57:42.693702
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.08
 ---- batch: 020 ----
mean loss: 220.53
 ---- batch: 030 ----
mean loss: 227.84
 ---- batch: 040 ----
mean loss: 221.80
 ---- batch: 050 ----
mean loss: 225.29
 ---- batch: 060 ----
mean loss: 221.27
 ---- batch: 070 ----
mean loss: 224.71
 ---- batch: 080 ----
mean loss: 213.93
 ---- batch: 090 ----
mean loss: 231.57
train mean loss: 223.02
epoch train time: 0:00:00.693981
elapsed time: 0:03:10.300654
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 22:57:43.387825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.15
 ---- batch: 020 ----
mean loss: 214.79
 ---- batch: 030 ----
mean loss: 222.54
 ---- batch: 040 ----
mean loss: 222.77
 ---- batch: 050 ----
mean loss: 221.74
 ---- batch: 060 ----
mean loss: 218.69
 ---- batch: 070 ----
mean loss: 226.03
 ---- batch: 080 ----
mean loss: 235.25
 ---- batch: 090 ----
mean loss: 226.23
train mean loss: 222.97
epoch train time: 0:00:00.696764
elapsed time: 0:03:10.997566
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 22:57:44.084754
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.90
 ---- batch: 020 ----
mean loss: 228.31
 ---- batch: 030 ----
mean loss: 220.97
 ---- batch: 040 ----
mean loss: 222.33
 ---- batch: 050 ----
mean loss: 221.41
 ---- batch: 060 ----
mean loss: 220.21
 ---- batch: 070 ----
mean loss: 223.73
 ---- batch: 080 ----
mean loss: 220.18
 ---- batch: 090 ----
mean loss: 223.71
train mean loss: 223.00
epoch train time: 0:00:00.692763
elapsed time: 0:03:11.690520
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 22:57:44.777707
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.56
 ---- batch: 020 ----
mean loss: 226.94
 ---- batch: 030 ----
mean loss: 223.51
 ---- batch: 040 ----
mean loss: 216.05
 ---- batch: 050 ----
mean loss: 221.13
 ---- batch: 060 ----
mean loss: 222.95
 ---- batch: 070 ----
mean loss: 215.55
 ---- batch: 080 ----
mean loss: 227.05
 ---- batch: 090 ----
mean loss: 228.41
train mean loss: 223.17
epoch train time: 0:00:00.693326
elapsed time: 0:03:12.384003
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 22:57:45.471178
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.88
 ---- batch: 020 ----
mean loss: 226.82
 ---- batch: 030 ----
mean loss: 220.93
 ---- batch: 040 ----
mean loss: 224.20
 ---- batch: 050 ----
mean loss: 228.38
 ---- batch: 060 ----
mean loss: 218.66
 ---- batch: 070 ----
mean loss: 221.69
 ---- batch: 080 ----
mean loss: 225.69
 ---- batch: 090 ----
mean loss: 222.69
train mean loss: 223.00
epoch train time: 0:00:00.712785
elapsed time: 0:03:13.096933
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 22:57:46.184105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 235.66
 ---- batch: 020 ----
mean loss: 216.98
 ---- batch: 030 ----
mean loss: 225.21
 ---- batch: 040 ----
mean loss: 225.92
 ---- batch: 050 ----
mean loss: 217.31
 ---- batch: 060 ----
mean loss: 221.43
 ---- batch: 070 ----
mean loss: 227.16
 ---- batch: 080 ----
mean loss: 220.87
 ---- batch: 090 ----
mean loss: 219.86
train mean loss: 223.01
epoch train time: 0:00:00.702571
elapsed time: 0:03:13.799657
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 22:57:46.886851
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.82
 ---- batch: 020 ----
mean loss: 224.34
 ---- batch: 030 ----
mean loss: 219.74
 ---- batch: 040 ----
mean loss: 227.22
 ---- batch: 050 ----
mean loss: 223.40
 ---- batch: 060 ----
mean loss: 218.40
 ---- batch: 070 ----
mean loss: 226.35
 ---- batch: 080 ----
mean loss: 217.39
 ---- batch: 090 ----
mean loss: 235.72
train mean loss: 222.95
epoch train time: 0:00:00.695487
elapsed time: 0:03:14.495327
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 22:57:47.582552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.27
 ---- batch: 020 ----
mean loss: 225.09
 ---- batch: 030 ----
mean loss: 229.55
 ---- batch: 040 ----
mean loss: 220.91
 ---- batch: 050 ----
mean loss: 215.84
 ---- batch: 060 ----
mean loss: 215.42
 ---- batch: 070 ----
mean loss: 224.72
 ---- batch: 080 ----
mean loss: 224.50
 ---- batch: 090 ----
mean loss: 224.00
train mean loss: 223.11
epoch train time: 0:00:00.701098
elapsed time: 0:03:15.196627
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 22:57:48.283817
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.99
 ---- batch: 020 ----
mean loss: 219.27
 ---- batch: 030 ----
mean loss: 223.18
 ---- batch: 040 ----
mean loss: 223.27
 ---- batch: 050 ----
mean loss: 219.13
 ---- batch: 060 ----
mean loss: 217.10
 ---- batch: 070 ----
mean loss: 218.51
 ---- batch: 080 ----
mean loss: 223.33
 ---- batch: 090 ----
mean loss: 230.12
train mean loss: 222.81
epoch train time: 0:00:00.708262
elapsed time: 0:03:15.905053
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 22:57:48.992226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.26
 ---- batch: 020 ----
mean loss: 225.76
 ---- batch: 030 ----
mean loss: 226.91
 ---- batch: 040 ----
mean loss: 219.81
 ---- batch: 050 ----
mean loss: 216.17
 ---- batch: 060 ----
mean loss: 226.71
 ---- batch: 070 ----
mean loss: 219.91
 ---- batch: 080 ----
mean loss: 218.97
 ---- batch: 090 ----
mean loss: 227.06
train mean loss: 222.78
epoch train time: 0:00:00.694766
elapsed time: 0:03:16.599983
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 22:57:49.687168
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.99
 ---- batch: 020 ----
mean loss: 222.33
 ---- batch: 030 ----
mean loss: 218.97
 ---- batch: 040 ----
mean loss: 218.70
 ---- batch: 050 ----
mean loss: 224.74
 ---- batch: 060 ----
mean loss: 225.62
 ---- batch: 070 ----
mean loss: 215.51
 ---- batch: 080 ----
mean loss: 223.52
 ---- batch: 090 ----
mean loss: 223.67
train mean loss: 222.98
epoch train time: 0:00:00.694158
elapsed time: 0:03:17.294336
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 22:57:50.381498
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.97
 ---- batch: 020 ----
mean loss: 223.50
 ---- batch: 030 ----
mean loss: 219.64
 ---- batch: 040 ----
mean loss: 228.27
 ---- batch: 050 ----
mean loss: 226.04
 ---- batch: 060 ----
mean loss: 217.75
 ---- batch: 070 ----
mean loss: 229.17
 ---- batch: 080 ----
mean loss: 222.85
 ---- batch: 090 ----
mean loss: 222.77
train mean loss: 222.98
epoch train time: 0:00:00.692595
elapsed time: 0:03:17.987083
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 22:57:51.074258
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.42
 ---- batch: 020 ----
mean loss: 221.82
 ---- batch: 030 ----
mean loss: 220.74
 ---- batch: 040 ----
mean loss: 218.26
 ---- batch: 050 ----
mean loss: 229.02
 ---- batch: 060 ----
mean loss: 230.75
 ---- batch: 070 ----
mean loss: 221.36
 ---- batch: 080 ----
mean loss: 220.05
 ---- batch: 090 ----
mean loss: 221.04
train mean loss: 222.88
epoch train time: 0:00:00.697852
elapsed time: 0:03:18.685088
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 22:57:51.772266
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.87
 ---- batch: 020 ----
mean loss: 223.22
 ---- batch: 030 ----
mean loss: 220.87
 ---- batch: 040 ----
mean loss: 227.72
 ---- batch: 050 ----
mean loss: 221.84
 ---- batch: 060 ----
mean loss: 215.29
 ---- batch: 070 ----
mean loss: 227.66
 ---- batch: 080 ----
mean loss: 224.86
 ---- batch: 090 ----
mean loss: 225.12
train mean loss: 222.75
epoch train time: 0:00:00.693160
elapsed time: 0:03:19.378400
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 22:57:52.465586
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.39
 ---- batch: 020 ----
mean loss: 217.30
 ---- batch: 030 ----
mean loss: 227.48
 ---- batch: 040 ----
mean loss: 225.28
 ---- batch: 050 ----
mean loss: 221.37
 ---- batch: 060 ----
mean loss: 213.69
 ---- batch: 070 ----
mean loss: 222.53
 ---- batch: 080 ----
mean loss: 228.03
 ---- batch: 090 ----
mean loss: 223.20
train mean loss: 222.97
epoch train time: 0:00:00.700602
elapsed time: 0:03:20.079241
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 22:57:53.166413
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.15
 ---- batch: 020 ----
mean loss: 223.25
 ---- batch: 030 ----
mean loss: 223.41
 ---- batch: 040 ----
mean loss: 221.37
 ---- batch: 050 ----
mean loss: 222.94
 ---- batch: 060 ----
mean loss: 215.96
 ---- batch: 070 ----
mean loss: 221.71
 ---- batch: 080 ----
mean loss: 221.30
 ---- batch: 090 ----
mean loss: 225.81
train mean loss: 223.09
epoch train time: 0:00:00.701711
elapsed time: 0:03:20.781096
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 22:57:53.868268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.32
 ---- batch: 020 ----
mean loss: 224.94
 ---- batch: 030 ----
mean loss: 219.26
 ---- batch: 040 ----
mean loss: 212.06
 ---- batch: 050 ----
mean loss: 219.77
 ---- batch: 060 ----
mean loss: 232.22
 ---- batch: 070 ----
mean loss: 216.29
 ---- batch: 080 ----
mean loss: 225.12
 ---- batch: 090 ----
mean loss: 229.19
train mean loss: 222.89
epoch train time: 0:00:00.689053
elapsed time: 0:03:21.470297
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 22:57:54.557470
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.41
 ---- batch: 020 ----
mean loss: 218.39
 ---- batch: 030 ----
mean loss: 225.09
 ---- batch: 040 ----
mean loss: 221.22
 ---- batch: 050 ----
mean loss: 222.10
 ---- batch: 060 ----
mean loss: 220.56
 ---- batch: 070 ----
mean loss: 226.70
 ---- batch: 080 ----
mean loss: 223.30
 ---- batch: 090 ----
mean loss: 219.06
train mean loss: 222.94
epoch train time: 0:00:00.695426
elapsed time: 0:03:22.165911
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 22:57:55.253125
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.25
 ---- batch: 020 ----
mean loss: 221.92
 ---- batch: 030 ----
mean loss: 229.08
 ---- batch: 040 ----
mean loss: 214.84
 ---- batch: 050 ----
mean loss: 219.58
 ---- batch: 060 ----
mean loss: 217.07
 ---- batch: 070 ----
mean loss: 226.25
 ---- batch: 080 ----
mean loss: 218.07
 ---- batch: 090 ----
mean loss: 227.57
train mean loss: 223.05
epoch train time: 0:00:00.693370
elapsed time: 0:03:22.859474
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 22:57:55.946655
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.54
 ---- batch: 020 ----
mean loss: 219.00
 ---- batch: 030 ----
mean loss: 227.80
 ---- batch: 040 ----
mean loss: 217.41
 ---- batch: 050 ----
mean loss: 220.86
 ---- batch: 060 ----
mean loss: 220.07
 ---- batch: 070 ----
mean loss: 224.21
 ---- batch: 080 ----
mean loss: 220.72
 ---- batch: 090 ----
mean loss: 228.82
train mean loss: 222.84
epoch train time: 0:00:00.689812
elapsed time: 0:03:23.549447
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 22:57:56.636621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.52
 ---- batch: 020 ----
mean loss: 224.88
 ---- batch: 030 ----
mean loss: 222.75
 ---- batch: 040 ----
mean loss: 213.95
 ---- batch: 050 ----
mean loss: 218.39
 ---- batch: 060 ----
mean loss: 229.05
 ---- batch: 070 ----
mean loss: 228.50
 ---- batch: 080 ----
mean loss: 215.34
 ---- batch: 090 ----
mean loss: 229.04
train mean loss: 222.71
epoch train time: 0:00:00.694260
elapsed time: 0:03:24.243873
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 22:57:57.331050
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.74
 ---- batch: 020 ----
mean loss: 233.31
 ---- batch: 030 ----
mean loss: 217.53
 ---- batch: 040 ----
mean loss: 219.64
 ---- batch: 050 ----
mean loss: 226.19
 ---- batch: 060 ----
mean loss: 217.08
 ---- batch: 070 ----
mean loss: 221.09
 ---- batch: 080 ----
mean loss: 219.39
 ---- batch: 090 ----
mean loss: 228.54
train mean loss: 222.74
epoch train time: 0:00:00.694188
elapsed time: 0:03:24.938213
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 22:57:58.025413
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.88
 ---- batch: 020 ----
mean loss: 222.14
 ---- batch: 030 ----
mean loss: 220.32
 ---- batch: 040 ----
mean loss: 227.79
 ---- batch: 050 ----
mean loss: 223.25
 ---- batch: 060 ----
mean loss: 222.40
 ---- batch: 070 ----
mean loss: 217.38
 ---- batch: 080 ----
mean loss: 228.27
 ---- batch: 090 ----
mean loss: 226.37
train mean loss: 222.71
epoch train time: 0:00:00.696358
elapsed time: 0:03:25.634751
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 22:57:58.721942
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.23
 ---- batch: 020 ----
mean loss: 225.79
 ---- batch: 030 ----
mean loss: 217.13
 ---- batch: 040 ----
mean loss: 226.69
 ---- batch: 050 ----
mean loss: 225.68
 ---- batch: 060 ----
mean loss: 214.31
 ---- batch: 070 ----
mean loss: 221.42
 ---- batch: 080 ----
mean loss: 217.19
 ---- batch: 090 ----
mean loss: 226.13
train mean loss: 222.72
epoch train time: 0:00:00.694522
elapsed time: 0:03:26.329449
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 22:57:59.416634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.22
 ---- batch: 020 ----
mean loss: 219.92
 ---- batch: 030 ----
mean loss: 224.07
 ---- batch: 040 ----
mean loss: 228.18
 ---- batch: 050 ----
mean loss: 224.70
 ---- batch: 060 ----
mean loss: 226.62
 ---- batch: 070 ----
mean loss: 213.04
 ---- batch: 080 ----
mean loss: 216.67
 ---- batch: 090 ----
mean loss: 227.08
train mean loss: 222.59
epoch train time: 0:00:00.704958
elapsed time: 0:03:27.034618
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 22:58:00.121792
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.81
 ---- batch: 020 ----
mean loss: 225.08
 ---- batch: 030 ----
mean loss: 216.81
 ---- batch: 040 ----
mean loss: 226.23
 ---- batch: 050 ----
mean loss: 218.92
 ---- batch: 060 ----
mean loss: 224.23
 ---- batch: 070 ----
mean loss: 232.04
 ---- batch: 080 ----
mean loss: 217.44
 ---- batch: 090 ----
mean loss: 219.71
train mean loss: 222.76
epoch train time: 0:00:00.697119
elapsed time: 0:03:27.731896
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 22:58:00.819067
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.94
 ---- batch: 020 ----
mean loss: 226.09
 ---- batch: 030 ----
mean loss: 218.55
 ---- batch: 040 ----
mean loss: 216.59
 ---- batch: 050 ----
mean loss: 233.45
 ---- batch: 060 ----
mean loss: 223.53
 ---- batch: 070 ----
mean loss: 220.21
 ---- batch: 080 ----
mean loss: 226.19
 ---- batch: 090 ----
mean loss: 225.91
train mean loss: 222.51
epoch train time: 0:00:00.695711
elapsed time: 0:03:28.429803
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_8/checkpoint.pth.tar
**** end time: 2019-09-25 22:58:01.516940 ****
