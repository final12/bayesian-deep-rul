Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 13707
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-26 21:56:39.715965 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 11, 11]           1,120
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
    BayesianConv2d-4            [-1, 14, 4, 11]             448
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
    BayesianLinear-8                    [-1, 1]             616
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,184
Trainable params: 2,184
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 21:56:39.729458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4701.89
 ---- batch: 020 ----
mean loss: 4545.31
 ---- batch: 030 ----
mean loss: 4374.45
 ---- batch: 040 ----
mean loss: 4179.07
 ---- batch: 050 ----
mean loss: 4012.19
 ---- batch: 060 ----
mean loss: 3787.07
 ---- batch: 070 ----
mean loss: 3645.73
 ---- batch: 080 ----
mean loss: 3457.02
 ---- batch: 090 ----
mean loss: 3281.77
 ---- batch: 100 ----
mean loss: 3120.22
 ---- batch: 110 ----
mean loss: 2968.10
train mean loss: 3798.36
epoch train time: 0:00:35.648754
elapsed time: 0:00:35.665416
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 21:57:15.381457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2722.50
 ---- batch: 020 ----
mean loss: 2539.92
 ---- batch: 030 ----
mean loss: 2404.86
 ---- batch: 040 ----
mean loss: 2255.28
 ---- batch: 050 ----
mean loss: 2135.43
 ---- batch: 060 ----
mean loss: 1983.12
 ---- batch: 070 ----
mean loss: 1848.47
 ---- batch: 080 ----
mean loss: 1752.05
 ---- batch: 090 ----
mean loss: 1638.02
 ---- batch: 100 ----
mean loss: 1528.88
 ---- batch: 110 ----
mean loss: 1432.65
train mean loss: 2005.98
epoch train time: 0:00:02.573126
elapsed time: 0:00:38.238780
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 21:57:17.954963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1343.89
 ---- batch: 020 ----
mean loss: 1283.40
 ---- batch: 030 ----
mean loss: 1227.74
 ---- batch: 040 ----
mean loss: 1175.55
 ---- batch: 050 ----
mean loss: 1109.76
 ---- batch: 060 ----
mean loss: 1064.42
 ---- batch: 070 ----
mean loss: 1056.49
 ---- batch: 080 ----
mean loss: 1010.02
 ---- batch: 090 ----
mean loss: 988.37
 ---- batch: 100 ----
mean loss: 961.22
 ---- batch: 110 ----
mean loss: 947.21
train mean loss: 1101.46
epoch train time: 0:00:02.561541
elapsed time: 0:00:40.800692
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 21:57:20.516895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.63
 ---- batch: 020 ----
mean loss: 922.57
 ---- batch: 030 ----
mean loss: 920.08
 ---- batch: 040 ----
mean loss: 892.74
 ---- batch: 050 ----
mean loss: 874.16
 ---- batch: 060 ----
mean loss: 880.27
 ---- batch: 070 ----
mean loss: 879.19
 ---- batch: 080 ----
mean loss: 852.92
 ---- batch: 090 ----
mean loss: 875.18
 ---- batch: 100 ----
mean loss: 874.94
 ---- batch: 110 ----
mean loss: 860.80
train mean loss: 887.65
epoch train time: 0:00:02.548745
elapsed time: 0:00:43.349867
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 21:57:23.066072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.46
 ---- batch: 020 ----
mean loss: 854.06
 ---- batch: 030 ----
mean loss: 865.45
 ---- batch: 040 ----
mean loss: 868.38
 ---- batch: 050 ----
mean loss: 857.64
 ---- batch: 060 ----
mean loss: 847.55
 ---- batch: 070 ----
mean loss: 849.74
 ---- batch: 080 ----
mean loss: 845.42
 ---- batch: 090 ----
mean loss: 853.02
 ---- batch: 100 ----
mean loss: 860.89
 ---- batch: 110 ----
mean loss: 861.76
train mean loss: 856.45
epoch train time: 0:00:02.582141
elapsed time: 0:00:45.932453
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 21:57:25.648640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.36
 ---- batch: 020 ----
mean loss: 855.91
 ---- batch: 030 ----
mean loss: 859.02
 ---- batch: 040 ----
mean loss: 835.51
 ---- batch: 050 ----
mean loss: 848.42
 ---- batch: 060 ----
mean loss: 872.87
 ---- batch: 070 ----
mean loss: 847.30
 ---- batch: 080 ----
mean loss: 867.50
 ---- batch: 090 ----
mean loss: 849.17
 ---- batch: 100 ----
mean loss: 851.01
 ---- batch: 110 ----
mean loss: 855.66
train mean loss: 853.31
epoch train time: 0:00:02.569029
elapsed time: 0:00:48.501888
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 21:57:28.218086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.38
 ---- batch: 020 ----
mean loss: 838.30
 ---- batch: 030 ----
mean loss: 841.15
 ---- batch: 040 ----
mean loss: 843.99
 ---- batch: 050 ----
mean loss: 820.75
 ---- batch: 060 ----
mean loss: 849.62
 ---- batch: 070 ----
mean loss: 838.21
 ---- batch: 080 ----
mean loss: 858.32
 ---- batch: 090 ----
mean loss: 858.25
 ---- batch: 100 ----
mean loss: 864.25
 ---- batch: 110 ----
mean loss: 847.60
train mean loss: 847.48
epoch train time: 0:00:02.547420
elapsed time: 0:00:51.049758
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 21:57:30.766001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.77
 ---- batch: 020 ----
mean loss: 830.54
 ---- batch: 030 ----
mean loss: 864.44
 ---- batch: 040 ----
mean loss: 846.61
 ---- batch: 050 ----
mean loss: 848.74
 ---- batch: 060 ----
mean loss: 854.81
 ---- batch: 070 ----
mean loss: 822.10
 ---- batch: 080 ----
mean loss: 846.09
 ---- batch: 090 ----
mean loss: 841.87
 ---- batch: 100 ----
mean loss: 849.28
 ---- batch: 110 ----
mean loss: 849.47
train mean loss: 844.15
epoch train time: 0:00:02.551370
elapsed time: 0:00:53.601591
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 21:57:33.317821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.09
 ---- batch: 020 ----
mean loss: 847.01
 ---- batch: 030 ----
mean loss: 825.33
 ---- batch: 040 ----
mean loss: 828.51
 ---- batch: 050 ----
mean loss: 843.38
 ---- batch: 060 ----
mean loss: 824.11
 ---- batch: 070 ----
mean loss: 847.60
 ---- batch: 080 ----
mean loss: 833.18
 ---- batch: 090 ----
mean loss: 831.54
 ---- batch: 100 ----
mean loss: 854.25
 ---- batch: 110 ----
mean loss: 844.36
train mean loss: 838.53
epoch train time: 0:00:02.578702
elapsed time: 0:00:56.180767
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 21:57:35.897004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.26
 ---- batch: 020 ----
mean loss: 847.20
 ---- batch: 030 ----
mean loss: 814.68
 ---- batch: 040 ----
mean loss: 848.74
 ---- batch: 050 ----
mean loss: 828.46
 ---- batch: 060 ----
mean loss: 836.68
 ---- batch: 070 ----
mean loss: 828.69
 ---- batch: 080 ----
mean loss: 859.58
 ---- batch: 090 ----
mean loss: 825.50
 ---- batch: 100 ----
mean loss: 817.50
 ---- batch: 110 ----
mean loss: 840.16
train mean loss: 833.71
epoch train time: 0:00:02.548246
elapsed time: 0:00:58.729474
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 21:57:38.445683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.41
 ---- batch: 020 ----
mean loss: 813.14
 ---- batch: 030 ----
mean loss: 827.77
 ---- batch: 040 ----
mean loss: 844.11
 ---- batch: 050 ----
mean loss: 822.26
 ---- batch: 060 ----
mean loss: 827.57
 ---- batch: 070 ----
mean loss: 830.34
 ---- batch: 080 ----
mean loss: 823.24
 ---- batch: 090 ----
mean loss: 839.92
 ---- batch: 100 ----
mean loss: 823.41
 ---- batch: 110 ----
mean loss: 825.74
train mean loss: 828.75
epoch train time: 0:00:02.546431
elapsed time: 0:01:01.276330
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 21:57:40.992442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.76
 ---- batch: 020 ----
mean loss: 804.70
 ---- batch: 030 ----
mean loss: 840.63
 ---- batch: 040 ----
mean loss: 832.52
 ---- batch: 050 ----
mean loss: 823.95
 ---- batch: 060 ----
mean loss: 818.86
 ---- batch: 070 ----
mean loss: 823.42
 ---- batch: 080 ----
mean loss: 811.75
 ---- batch: 090 ----
mean loss: 828.96
 ---- batch: 100 ----
mean loss: 818.23
 ---- batch: 110 ----
mean loss: 794.78
train mean loss: 823.41
epoch train time: 0:00:02.584550
elapsed time: 0:01:03.861195
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 21:57:43.577393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.59
 ---- batch: 020 ----
mean loss: 827.73
 ---- batch: 030 ----
mean loss: 822.55
 ---- batch: 040 ----
mean loss: 814.86
 ---- batch: 050 ----
mean loss: 817.08
 ---- batch: 060 ----
mean loss: 815.46
 ---- batch: 070 ----
mean loss: 833.26
 ---- batch: 080 ----
mean loss: 796.73
 ---- batch: 090 ----
mean loss: 818.41
 ---- batch: 100 ----
mean loss: 830.05
 ---- batch: 110 ----
mean loss: 803.90
train mean loss: 820.48
epoch train time: 0:00:02.591508
elapsed time: 0:01:06.453130
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 21:57:46.169305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.78
 ---- batch: 020 ----
mean loss: 819.02
 ---- batch: 030 ----
mean loss: 805.37
 ---- batch: 040 ----
mean loss: 800.32
 ---- batch: 050 ----
mean loss: 805.26
 ---- batch: 060 ----
mean loss: 818.71
 ---- batch: 070 ----
mean loss: 816.76
 ---- batch: 080 ----
mean loss: 818.75
 ---- batch: 090 ----
mean loss: 809.88
 ---- batch: 100 ----
mean loss: 822.28
 ---- batch: 110 ----
mean loss: 824.85
train mean loss: 813.77
epoch train time: 0:00:02.544581
elapsed time: 0:01:08.998130
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 21:57:48.714335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 811.83
 ---- batch: 020 ----
mean loss: 804.87
 ---- batch: 030 ----
mean loss: 819.53
 ---- batch: 040 ----
mean loss: 823.78
 ---- batch: 050 ----
mean loss: 813.84
 ---- batch: 060 ----
mean loss: 803.48
 ---- batch: 070 ----
mean loss: 807.65
 ---- batch: 080 ----
mean loss: 798.76
 ---- batch: 090 ----
mean loss: 806.82
 ---- batch: 100 ----
mean loss: 805.97
 ---- batch: 110 ----
mean loss: 823.18
train mean loss: 809.73
epoch train time: 0:00:02.587096
elapsed time: 0:01:11.585704
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 21:57:51.301900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 810.27
 ---- batch: 020 ----
mean loss: 813.07
 ---- batch: 030 ----
mean loss: 799.69
 ---- batch: 040 ----
mean loss: 789.12
 ---- batch: 050 ----
mean loss: 804.19
 ---- batch: 060 ----
mean loss: 812.13
 ---- batch: 070 ----
mean loss: 819.15
 ---- batch: 080 ----
mean loss: 794.28
 ---- batch: 090 ----
mean loss: 795.34
 ---- batch: 100 ----
mean loss: 815.85
 ---- batch: 110 ----
mean loss: 793.77
train mean loss: 805.05
epoch train time: 0:00:02.563304
elapsed time: 0:01:14.149533
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 21:57:53.865748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 795.28
 ---- batch: 020 ----
mean loss: 772.71
 ---- batch: 030 ----
mean loss: 798.19
 ---- batch: 040 ----
mean loss: 822.55
 ---- batch: 050 ----
mean loss: 821.20
 ---- batch: 060 ----
mean loss: 819.06
 ---- batch: 070 ----
mean loss: 811.80
 ---- batch: 080 ----
mean loss: 801.22
 ---- batch: 090 ----
mean loss: 787.73
 ---- batch: 100 ----
mean loss: 796.50
 ---- batch: 110 ----
mean loss: 795.19
train mean loss: 801.94
epoch train time: 0:00:02.573990
elapsed time: 0:01:16.723965
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 21:57:56.440169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 778.10
 ---- batch: 020 ----
mean loss: 804.28
 ---- batch: 030 ----
mean loss: 786.39
 ---- batch: 040 ----
mean loss: 802.82
 ---- batch: 050 ----
mean loss: 812.26
 ---- batch: 060 ----
mean loss: 780.38
 ---- batch: 070 ----
mean loss: 809.71
 ---- batch: 080 ----
mean loss: 790.70
 ---- batch: 090 ----
mean loss: 792.40
 ---- batch: 100 ----
mean loss: 810.22
 ---- batch: 110 ----
mean loss: 807.29
train mean loss: 797.41
epoch train time: 0:00:02.569035
elapsed time: 0:01:19.293417
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 21:57:59.009654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 795.37
 ---- batch: 020 ----
mean loss: 804.75
 ---- batch: 030 ----
mean loss: 793.74
 ---- batch: 040 ----
mean loss: 774.81
 ---- batch: 050 ----
mean loss: 781.76
 ---- batch: 060 ----
mean loss: 798.44
 ---- batch: 070 ----
mean loss: 793.51
 ---- batch: 080 ----
mean loss: 791.82
 ---- batch: 090 ----
mean loss: 796.76
 ---- batch: 100 ----
mean loss: 789.30
 ---- batch: 110 ----
mean loss: 809.16
train mean loss: 792.69
epoch train time: 0:00:02.579200
elapsed time: 0:01:21.873047
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 21:58:01.589234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 788.15
 ---- batch: 020 ----
mean loss: 797.30
 ---- batch: 030 ----
mean loss: 799.09
 ---- batch: 040 ----
mean loss: 785.28
 ---- batch: 050 ----
mean loss: 775.23
 ---- batch: 060 ----
mean loss: 796.37
 ---- batch: 070 ----
mean loss: 791.92
 ---- batch: 080 ----
mean loss: 789.24
 ---- batch: 090 ----
mean loss: 783.99
 ---- batch: 100 ----
mean loss: 789.30
 ---- batch: 110 ----
mean loss: 789.55
train mean loss: 788.57
epoch train time: 0:00:02.549255
elapsed time: 0:01:24.422707
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 21:58:04.138896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 752.38
 ---- batch: 020 ----
mean loss: 824.01
 ---- batch: 030 ----
mean loss: 789.25
 ---- batch: 040 ----
mean loss: 810.52
 ---- batch: 050 ----
mean loss: 792.44
 ---- batch: 060 ----
mean loss: 794.75
 ---- batch: 070 ----
mean loss: 780.90
 ---- batch: 080 ----
mean loss: 793.45
 ---- batch: 090 ----
mean loss: 773.44
 ---- batch: 100 ----
mean loss: 770.92
 ---- batch: 110 ----
mean loss: 762.54
train mean loss: 785.69
epoch train time: 0:00:02.573457
elapsed time: 0:01:26.996588
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 21:58:06.712730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 772.08
 ---- batch: 020 ----
mean loss: 797.81
 ---- batch: 030 ----
mean loss: 773.11
 ---- batch: 040 ----
mean loss: 799.41
 ---- batch: 050 ----
mean loss: 784.53
 ---- batch: 060 ----
mean loss: 780.34
 ---- batch: 070 ----
mean loss: 790.45
 ---- batch: 080 ----
mean loss: 777.81
 ---- batch: 090 ----
mean loss: 772.22
 ---- batch: 100 ----
mean loss: 767.84
 ---- batch: 110 ----
mean loss: 785.08
train mean loss: 781.42
epoch train time: 0:00:02.548106
elapsed time: 0:01:29.545040
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 21:58:09.261221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 762.63
 ---- batch: 020 ----
mean loss: 766.86
 ---- batch: 030 ----
mean loss: 757.15
 ---- batch: 040 ----
mean loss: 783.31
 ---- batch: 050 ----
mean loss: 795.45
 ---- batch: 060 ----
mean loss: 773.94
 ---- batch: 070 ----
mean loss: 797.96
 ---- batch: 080 ----
mean loss: 766.08
 ---- batch: 090 ----
mean loss: 769.62
 ---- batch: 100 ----
mean loss: 799.79
 ---- batch: 110 ----
mean loss: 768.75
train mean loss: 777.16
epoch train time: 0:00:02.545793
elapsed time: 0:01:32.091249
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 21:58:11.807443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 754.12
 ---- batch: 020 ----
mean loss: 777.09
 ---- batch: 030 ----
mean loss: 788.14
 ---- batch: 040 ----
mean loss: 768.90
 ---- batch: 050 ----
mean loss: 788.02
 ---- batch: 060 ----
mean loss: 767.10
 ---- batch: 070 ----
mean loss: 766.08
 ---- batch: 080 ----
mean loss: 779.56
 ---- batch: 090 ----
mean loss: 768.21
 ---- batch: 100 ----
mean loss: 779.92
 ---- batch: 110 ----
mean loss: 757.89
train mean loss: 772.57
epoch train time: 0:00:02.564413
elapsed time: 0:01:34.656064
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 21:58:14.372257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 768.93
 ---- batch: 020 ----
mean loss: 766.31
 ---- batch: 030 ----
mean loss: 759.48
 ---- batch: 040 ----
mean loss: 774.15
 ---- batch: 050 ----
mean loss: 770.46
 ---- batch: 060 ----
mean loss: 778.71
 ---- batch: 070 ----
mean loss: 748.20
 ---- batch: 080 ----
mean loss: 772.04
 ---- batch: 090 ----
mean loss: 777.64
 ---- batch: 100 ----
mean loss: 756.15
 ---- batch: 110 ----
mean loss: 778.41
train mean loss: 768.62
epoch train time: 0:00:02.577787
elapsed time: 0:01:37.234350
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 21:58:16.950566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 766.99
 ---- batch: 020 ----
mean loss: 765.11
 ---- batch: 030 ----
mean loss: 757.29
 ---- batch: 040 ----
mean loss: 763.40
 ---- batch: 050 ----
mean loss: 764.08
 ---- batch: 060 ----
mean loss: 772.95
 ---- batch: 070 ----
mean loss: 776.23
 ---- batch: 080 ----
mean loss: 749.78
 ---- batch: 090 ----
mean loss: 773.40
 ---- batch: 100 ----
mean loss: 757.83
 ---- batch: 110 ----
mean loss: 761.93
train mean loss: 763.72
epoch train time: 0:00:02.551285
elapsed time: 0:01:39.786056
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 21:58:19.502243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 764.50
 ---- batch: 020 ----
mean loss: 765.81
 ---- batch: 030 ----
mean loss: 772.18
 ---- batch: 040 ----
mean loss: 764.25
 ---- batch: 050 ----
mean loss: 754.74
 ---- batch: 060 ----
mean loss: 745.76
 ---- batch: 070 ----
mean loss: 742.91
 ---- batch: 080 ----
mean loss: 776.86
 ---- batch: 090 ----
mean loss: 772.68
 ---- batch: 100 ----
mean loss: 745.27
 ---- batch: 110 ----
mean loss: 750.43
train mean loss: 759.32
epoch train time: 0:00:02.553717
elapsed time: 0:01:42.340180
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 21:58:22.056400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 753.69
 ---- batch: 020 ----
mean loss: 746.05
 ---- batch: 030 ----
mean loss: 771.81
 ---- batch: 040 ----
mean loss: 771.72
 ---- batch: 050 ----
mean loss: 745.96
 ---- batch: 060 ----
mean loss: 752.02
 ---- batch: 070 ----
mean loss: 748.13
 ---- batch: 080 ----
mean loss: 749.51
 ---- batch: 090 ----
mean loss: 758.41
 ---- batch: 100 ----
mean loss: 743.73
 ---- batch: 110 ----
mean loss: 752.83
train mean loss: 753.22
epoch train time: 0:00:02.603628
elapsed time: 0:01:44.944268
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 21:58:24.660467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 717.13
 ---- batch: 020 ----
mean loss: 745.69
 ---- batch: 030 ----
mean loss: 759.73
 ---- batch: 040 ----
mean loss: 770.88
 ---- batch: 050 ----
mean loss: 769.76
 ---- batch: 060 ----
mean loss: 742.35
 ---- batch: 070 ----
mean loss: 748.26
 ---- batch: 080 ----
mean loss: 749.36
 ---- batch: 090 ----
mean loss: 732.05
 ---- batch: 100 ----
mean loss: 752.38
 ---- batch: 110 ----
mean loss: 743.77
train mean loss: 748.59
epoch train time: 0:00:02.572528
elapsed time: 0:01:47.517205
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 21:58:27.233401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 751.94
 ---- batch: 020 ----
mean loss: 729.16
 ---- batch: 030 ----
mean loss: 737.02
 ---- batch: 040 ----
mean loss: 737.74
 ---- batch: 050 ----
mean loss: 750.36
 ---- batch: 060 ----
mean loss: 740.41
 ---- batch: 070 ----
mean loss: 740.96
 ---- batch: 080 ----
mean loss: 753.47
 ---- batch: 090 ----
mean loss: 742.18
 ---- batch: 100 ----
mean loss: 749.98
 ---- batch: 110 ----
mean loss: 744.08
train mean loss: 742.64
epoch train time: 0:00:02.619775
elapsed time: 0:01:50.137430
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 21:58:29.853659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 740.35
 ---- batch: 020 ----
mean loss: 732.53
 ---- batch: 030 ----
mean loss: 746.38
 ---- batch: 040 ----
mean loss: 744.41
 ---- batch: 050 ----
mean loss: 742.67
 ---- batch: 060 ----
mean loss: 739.36
 ---- batch: 070 ----
mean loss: 704.90
 ---- batch: 080 ----
mean loss: 745.80
 ---- batch: 090 ----
mean loss: 736.55
 ---- batch: 100 ----
mean loss: 738.57
 ---- batch: 110 ----
mean loss: 743.54
train mean loss: 737.75
epoch train time: 0:00:02.615351
elapsed time: 0:01:52.753232
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 21:58:32.469435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 740.37
 ---- batch: 020 ----
mean loss: 718.41
 ---- batch: 030 ----
mean loss: 750.37
 ---- batch: 040 ----
mean loss: 752.33
 ---- batch: 050 ----
mean loss: 713.95
 ---- batch: 060 ----
mean loss: 734.39
 ---- batch: 070 ----
mean loss: 733.99
 ---- batch: 080 ----
mean loss: 731.13
 ---- batch: 090 ----
mean loss: 725.68
 ---- batch: 100 ----
mean loss: 730.98
 ---- batch: 110 ----
mean loss: 718.43
train mean loss: 731.69
epoch train time: 0:00:02.563837
elapsed time: 0:01:55.317579
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 21:58:35.033820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 715.80
 ---- batch: 020 ----
mean loss: 716.98
 ---- batch: 030 ----
mean loss: 738.51
 ---- batch: 040 ----
mean loss: 739.11
 ---- batch: 050 ----
mean loss: 728.48
 ---- batch: 060 ----
mean loss: 734.80
 ---- batch: 070 ----
mean loss: 700.52
 ---- batch: 080 ----
mean loss: 734.74
 ---- batch: 090 ----
mean loss: 720.95
 ---- batch: 100 ----
mean loss: 731.85
 ---- batch: 110 ----
mean loss: 714.78
train mean loss: 724.86
epoch train time: 0:00:02.561236
elapsed time: 0:01:57.879316
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 21:58:37.595554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 715.77
 ---- batch: 020 ----
mean loss: 724.31
 ---- batch: 030 ----
mean loss: 712.88
 ---- batch: 040 ----
mean loss: 721.13
 ---- batch: 050 ----
mean loss: 703.70
 ---- batch: 060 ----
mean loss: 710.46
 ---- batch: 070 ----
mean loss: 725.84
 ---- batch: 080 ----
mean loss: 727.37
 ---- batch: 090 ----
mean loss: 725.10
 ---- batch: 100 ----
mean loss: 724.71
 ---- batch: 110 ----
mean loss: 717.26
train mean loss: 718.28
epoch train time: 0:00:02.545010
elapsed time: 0:02:00.424781
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 21:58:40.140991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 709.08
 ---- batch: 020 ----
mean loss: 687.06
 ---- batch: 030 ----
mean loss: 711.35
 ---- batch: 040 ----
mean loss: 725.79
 ---- batch: 050 ----
mean loss: 713.73
 ---- batch: 060 ----
mean loss: 721.87
 ---- batch: 070 ----
mean loss: 695.96
 ---- batch: 080 ----
mean loss: 718.91
 ---- batch: 090 ----
mean loss: 719.42
 ---- batch: 100 ----
mean loss: 718.39
 ---- batch: 110 ----
mean loss: 713.28
train mean loss: 711.65
epoch train time: 0:00:02.569458
elapsed time: 0:02:02.994677
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 21:58:42.710722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 735.11
 ---- batch: 020 ----
mean loss: 715.92
 ---- batch: 030 ----
mean loss: 704.03
 ---- batch: 040 ----
mean loss: 705.83
 ---- batch: 050 ----
mean loss: 704.90
 ---- batch: 060 ----
mean loss: 697.32
 ---- batch: 070 ----
mean loss: 701.72
 ---- batch: 080 ----
mean loss: 703.87
 ---- batch: 090 ----
mean loss: 692.58
 ---- batch: 100 ----
mean loss: 701.87
 ---- batch: 110 ----
mean loss: 694.18
train mean loss: 705.05
epoch train time: 0:00:02.564866
elapsed time: 0:02:05.559825
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 21:58:45.276070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.66
 ---- batch: 020 ----
mean loss: 697.10
 ---- batch: 030 ----
mean loss: 708.58
 ---- batch: 040 ----
mean loss: 700.34
 ---- batch: 050 ----
mean loss: 688.99
 ---- batch: 060 ----
mean loss: 704.56
 ---- batch: 070 ----
mean loss: 689.28
 ---- batch: 080 ----
mean loss: 687.16
 ---- batch: 090 ----
mean loss: 708.26
 ---- batch: 100 ----
mean loss: 700.74
 ---- batch: 110 ----
mean loss: 679.07
train mean loss: 697.87
epoch train time: 0:00:02.572132
elapsed time: 0:02:08.132480
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 21:58:47.848691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 694.92
 ---- batch: 020 ----
mean loss: 703.54
 ---- batch: 030 ----
mean loss: 690.39
 ---- batch: 040 ----
mean loss: 692.88
 ---- batch: 050 ----
mean loss: 709.40
 ---- batch: 060 ----
mean loss: 675.73
 ---- batch: 070 ----
mean loss: 682.50
 ---- batch: 080 ----
mean loss: 681.46
 ---- batch: 090 ----
mean loss: 676.29
 ---- batch: 100 ----
mean loss: 686.38
 ---- batch: 110 ----
mean loss: 705.05
train mean loss: 690.47
epoch train time: 0:00:02.567338
elapsed time: 0:02:10.700238
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 21:58:50.416456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.34
 ---- batch: 020 ----
mean loss: 687.48
 ---- batch: 030 ----
mean loss: 690.02
 ---- batch: 040 ----
mean loss: 687.98
 ---- batch: 050 ----
mean loss: 662.26
 ---- batch: 060 ----
mean loss: 677.20
 ---- batch: 070 ----
mean loss: 680.35
 ---- batch: 080 ----
mean loss: 690.25
 ---- batch: 090 ----
mean loss: 676.85
 ---- batch: 100 ----
mean loss: 675.28
 ---- batch: 110 ----
mean loss: 685.31
train mean loss: 682.95
epoch train time: 0:00:02.555116
elapsed time: 0:02:13.255776
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 21:58:52.972014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 677.80
 ---- batch: 020 ----
mean loss: 681.67
 ---- batch: 030 ----
mean loss: 664.93
 ---- batch: 040 ----
mean loss: 676.89
 ---- batch: 050 ----
mean loss: 676.49
 ---- batch: 060 ----
mean loss: 670.33
 ---- batch: 070 ----
mean loss: 672.65
 ---- batch: 080 ----
mean loss: 679.83
 ---- batch: 090 ----
mean loss: 672.20
 ---- batch: 100 ----
mean loss: 679.94
 ---- batch: 110 ----
mean loss: 675.35
train mean loss: 674.43
epoch train time: 0:00:02.549849
elapsed time: 0:02:15.806069
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 21:58:55.522317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.13
 ---- batch: 020 ----
mean loss: 682.07
 ---- batch: 030 ----
mean loss: 658.67
 ---- batch: 040 ----
mean loss: 674.05
 ---- batch: 050 ----
mean loss: 670.46
 ---- batch: 060 ----
mean loss: 664.97
 ---- batch: 070 ----
mean loss: 665.77
 ---- batch: 080 ----
mean loss: 666.22
 ---- batch: 090 ----
mean loss: 650.87
 ---- batch: 100 ----
mean loss: 657.79
 ---- batch: 110 ----
mean loss: 666.58
train mean loss: 664.96
epoch train time: 0:00:02.584986
elapsed time: 0:02:18.391504
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 21:58:58.107714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 670.20
 ---- batch: 020 ----
mean loss: 648.08
 ---- batch: 030 ----
mean loss: 658.62
 ---- batch: 040 ----
mean loss: 658.13
 ---- batch: 050 ----
mean loss: 645.73
 ---- batch: 060 ----
mean loss: 647.00
 ---- batch: 070 ----
mean loss: 668.63
 ---- batch: 080 ----
mean loss: 671.19
 ---- batch: 090 ----
mean loss: 656.84
 ---- batch: 100 ----
mean loss: 651.70
 ---- batch: 110 ----
mean loss: 639.04
train mean loss: 655.22
epoch train time: 0:00:02.553524
elapsed time: 0:02:20.945560
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 21:59:00.661835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 651.37
 ---- batch: 020 ----
mean loss: 644.52
 ---- batch: 030 ----
mean loss: 649.52
 ---- batch: 040 ----
mean loss: 652.34
 ---- batch: 050 ----
mean loss: 650.40
 ---- batch: 060 ----
mean loss: 641.46
 ---- batch: 070 ----
mean loss: 647.90
 ---- batch: 080 ----
mean loss: 642.12
 ---- batch: 090 ----
mean loss: 636.80
 ---- batch: 100 ----
mean loss: 624.05
 ---- batch: 110 ----
mean loss: 639.89
train mean loss: 643.57
epoch train time: 0:00:02.557615
elapsed time: 0:02:23.503656
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 21:59:03.219941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 629.22
 ---- batch: 020 ----
mean loss: 625.61
 ---- batch: 030 ----
mean loss: 643.22
 ---- batch: 040 ----
mean loss: 633.93
 ---- batch: 050 ----
mean loss: 629.85
 ---- batch: 060 ----
mean loss: 623.85
 ---- batch: 070 ----
mean loss: 612.75
 ---- batch: 080 ----
mean loss: 635.88
 ---- batch: 090 ----
mean loss: 618.10
 ---- batch: 100 ----
mean loss: 627.41
 ---- batch: 110 ----
mean loss: 617.50
train mean loss: 627.22
epoch train time: 0:00:02.544272
elapsed time: 0:02:26.048447
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 21:59:05.764642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.76
 ---- batch: 020 ----
mean loss: 614.11
 ---- batch: 030 ----
mean loss: 610.59
 ---- batch: 040 ----
mean loss: 615.65
 ---- batch: 050 ----
mean loss: 609.66
 ---- batch: 060 ----
mean loss: 638.46
 ---- batch: 070 ----
mean loss: 615.93
 ---- batch: 080 ----
mean loss: 600.38
 ---- batch: 090 ----
mean loss: 602.69
 ---- batch: 100 ----
mean loss: 585.83
 ---- batch: 110 ----
mean loss: 603.81
train mean loss: 609.44
epoch train time: 0:00:02.557550
elapsed time: 0:02:28.606428
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 21:59:08.322618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 587.15
 ---- batch: 020 ----
mean loss: 612.95
 ---- batch: 030 ----
mean loss: 607.78
 ---- batch: 040 ----
mean loss: 600.61
 ---- batch: 050 ----
mean loss: 590.06
 ---- batch: 060 ----
mean loss: 583.70
 ---- batch: 070 ----
mean loss: 585.55
 ---- batch: 080 ----
mean loss: 586.56
 ---- batch: 090 ----
mean loss: 578.63
 ---- batch: 100 ----
mean loss: 594.98
 ---- batch: 110 ----
mean loss: 593.97
train mean loss: 592.31
epoch train time: 0:00:02.554499
elapsed time: 0:02:31.161315
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 21:59:10.877506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.74
 ---- batch: 020 ----
mean loss: 583.42
 ---- batch: 030 ----
mean loss: 581.34
 ---- batch: 040 ----
mean loss: 575.33
 ---- batch: 050 ----
mean loss: 574.28
 ---- batch: 060 ----
mean loss: 561.66
 ---- batch: 070 ----
mean loss: 586.96
 ---- batch: 080 ----
mean loss: 570.22
 ---- batch: 090 ----
mean loss: 571.71
 ---- batch: 100 ----
mean loss: 566.43
 ---- batch: 110 ----
mean loss: 574.60
train mean loss: 574.79
epoch train time: 0:00:02.556124
elapsed time: 0:02:33.717842
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 21:59:13.434097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.98
 ---- batch: 020 ----
mean loss: 574.77
 ---- batch: 030 ----
mean loss: 556.80
 ---- batch: 040 ----
mean loss: 548.01
 ---- batch: 050 ----
mean loss: 565.65
 ---- batch: 060 ----
mean loss: 556.87
 ---- batch: 070 ----
mean loss: 574.62
 ---- batch: 080 ----
mean loss: 553.84
 ---- batch: 090 ----
mean loss: 561.22
 ---- batch: 100 ----
mean loss: 537.35
 ---- batch: 110 ----
mean loss: 551.81
train mean loss: 558.26
epoch train time: 0:00:02.574134
elapsed time: 0:02:36.292481
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 21:59:16.008694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 555.37
 ---- batch: 020 ----
mean loss: 545.99
 ---- batch: 030 ----
mean loss: 556.59
 ---- batch: 040 ----
mean loss: 531.91
 ---- batch: 050 ----
mean loss: 540.81
 ---- batch: 060 ----
mean loss: 548.69
 ---- batch: 070 ----
mean loss: 547.65
 ---- batch: 080 ----
mean loss: 546.85
 ---- batch: 090 ----
mean loss: 532.95
 ---- batch: 100 ----
mean loss: 536.75
 ---- batch: 110 ----
mean loss: 535.36
train mean loss: 543.53
epoch train time: 0:00:02.583538
elapsed time: 0:02:38.876427
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 21:59:18.592626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 537.99
 ---- batch: 020 ----
mean loss: 539.88
 ---- batch: 030 ----
mean loss: 530.98
 ---- batch: 040 ----
mean loss: 542.24
 ---- batch: 050 ----
mean loss: 522.18
 ---- batch: 060 ----
mean loss: 527.48
 ---- batch: 070 ----
mean loss: 523.25
 ---- batch: 080 ----
mean loss: 535.56
 ---- batch: 090 ----
mean loss: 533.32
 ---- batch: 100 ----
mean loss: 519.12
 ---- batch: 110 ----
mean loss: 513.04
train mean loss: 530.09
epoch train time: 0:00:02.563956
elapsed time: 0:02:41.440779
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 21:59:21.156980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 526.05
 ---- batch: 020 ----
mean loss: 529.37
 ---- batch: 030 ----
mean loss: 511.52
 ---- batch: 040 ----
mean loss: 522.36
 ---- batch: 050 ----
mean loss: 524.60
 ---- batch: 060 ----
mean loss: 519.16
 ---- batch: 070 ----
mean loss: 514.88
 ---- batch: 080 ----
mean loss: 517.71
 ---- batch: 090 ----
mean loss: 511.89
 ---- batch: 100 ----
mean loss: 507.15
 ---- batch: 110 ----
mean loss: 509.83
train mean loss: 517.59
epoch train time: 0:00:02.605804
elapsed time: 0:02:44.047000
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 21:59:23.763231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.36
 ---- batch: 020 ----
mean loss: 514.95
 ---- batch: 030 ----
mean loss: 524.89
 ---- batch: 040 ----
mean loss: 504.56
 ---- batch: 050 ----
mean loss: 497.53
 ---- batch: 060 ----
mean loss: 509.23
 ---- batch: 070 ----
mean loss: 501.02
 ---- batch: 080 ----
mean loss: 497.46
 ---- batch: 090 ----
mean loss: 493.33
 ---- batch: 100 ----
mean loss: 509.87
 ---- batch: 110 ----
mean loss: 503.36
train mean loss: 505.69
epoch train time: 0:00:02.560513
elapsed time: 0:02:46.607975
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 21:59:26.324191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.28
 ---- batch: 020 ----
mean loss: 506.03
 ---- batch: 030 ----
mean loss: 494.07
 ---- batch: 040 ----
mean loss: 488.73
 ---- batch: 050 ----
mean loss: 482.47
 ---- batch: 060 ----
mean loss: 496.96
 ---- batch: 070 ----
mean loss: 502.20
 ---- batch: 080 ----
mean loss: 503.72
 ---- batch: 090 ----
mean loss: 483.02
 ---- batch: 100 ----
mean loss: 487.13
 ---- batch: 110 ----
mean loss: 504.88
train mean loss: 495.48
epoch train time: 0:00:02.563240
elapsed time: 0:02:49.171666
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 21:59:28.887923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.62
 ---- batch: 020 ----
mean loss: 490.63
 ---- batch: 030 ----
mean loss: 493.05
 ---- batch: 040 ----
mean loss: 482.59
 ---- batch: 050 ----
mean loss: 472.75
 ---- batch: 060 ----
mean loss: 492.83
 ---- batch: 070 ----
mean loss: 488.69
 ---- batch: 080 ----
mean loss: 486.24
 ---- batch: 090 ----
mean loss: 471.51
 ---- batch: 100 ----
mean loss: 483.60
 ---- batch: 110 ----
mean loss: 479.73
train mean loss: 484.62
epoch train time: 0:00:02.553961
elapsed time: 0:02:51.726123
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 21:59:31.442332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.65
 ---- batch: 020 ----
mean loss: 472.63
 ---- batch: 030 ----
mean loss: 486.33
 ---- batch: 040 ----
mean loss: 485.31
 ---- batch: 050 ----
mean loss: 483.10
 ---- batch: 060 ----
mean loss: 459.15
 ---- batch: 070 ----
mean loss: 468.27
 ---- batch: 080 ----
mean loss: 465.86
 ---- batch: 090 ----
mean loss: 469.58
 ---- batch: 100 ----
mean loss: 481.14
 ---- batch: 110 ----
mean loss: 472.95
train mean loss: 474.71
epoch train time: 0:00:02.564388
elapsed time: 0:02:54.290924
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 21:59:34.007115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.58
 ---- batch: 020 ----
mean loss: 473.76
 ---- batch: 030 ----
mean loss: 483.62
 ---- batch: 040 ----
mean loss: 460.85
 ---- batch: 050 ----
mean loss: 465.59
 ---- batch: 060 ----
mean loss: 458.18
 ---- batch: 070 ----
mean loss: 446.72
 ---- batch: 080 ----
mean loss: 460.77
 ---- batch: 090 ----
mean loss: 467.32
 ---- batch: 100 ----
mean loss: 452.83
 ---- batch: 110 ----
mean loss: 464.55
train mean loss: 465.55
epoch train time: 0:00:02.565572
elapsed time: 0:02:56.856896
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 21:59:36.573122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.12
 ---- batch: 020 ----
mean loss: 457.14
 ---- batch: 030 ----
mean loss: 460.21
 ---- batch: 040 ----
mean loss: 456.29
 ---- batch: 050 ----
mean loss: 464.97
 ---- batch: 060 ----
mean loss: 471.45
 ---- batch: 070 ----
mean loss: 439.86
 ---- batch: 080 ----
mean loss: 444.22
 ---- batch: 090 ----
mean loss: 454.28
 ---- batch: 100 ----
mean loss: 454.30
 ---- batch: 110 ----
mean loss: 452.52
train mean loss: 456.27
epoch train time: 0:00:02.557172
elapsed time: 0:02:59.414508
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 21:59:39.130557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.61
 ---- batch: 020 ----
mean loss: 456.01
 ---- batch: 030 ----
mean loss: 440.03
 ---- batch: 040 ----
mean loss: 444.60
 ---- batch: 050 ----
mean loss: 439.97
 ---- batch: 060 ----
mean loss: 443.22
 ---- batch: 070 ----
mean loss: 448.81
 ---- batch: 080 ----
mean loss: 432.55
 ---- batch: 090 ----
mean loss: 454.56
 ---- batch: 100 ----
mean loss: 454.73
 ---- batch: 110 ----
mean loss: 453.38
train mean loss: 447.99
epoch train time: 0:00:02.567990
elapsed time: 0:03:01.982771
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 21:59:41.698970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.56
 ---- batch: 020 ----
mean loss: 447.41
 ---- batch: 030 ----
mean loss: 444.60
 ---- batch: 040 ----
mean loss: 446.76
 ---- batch: 050 ----
mean loss: 442.02
 ---- batch: 060 ----
mean loss: 415.12
 ---- batch: 070 ----
mean loss: 436.01
 ---- batch: 080 ----
mean loss: 443.09
 ---- batch: 090 ----
mean loss: 432.59
 ---- batch: 100 ----
mean loss: 438.53
 ---- batch: 110 ----
mean loss: 439.59
train mean loss: 438.31
epoch train time: 0:00:02.580280
elapsed time: 0:03:04.563453
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 21:59:44.279650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.90
 ---- batch: 020 ----
mean loss: 434.79
 ---- batch: 030 ----
mean loss: 440.67
 ---- batch: 040 ----
mean loss: 428.46
 ---- batch: 050 ----
mean loss: 420.25
 ---- batch: 060 ----
mean loss: 430.40
 ---- batch: 070 ----
mean loss: 413.51
 ---- batch: 080 ----
mean loss: 426.61
 ---- batch: 090 ----
mean loss: 439.38
 ---- batch: 100 ----
mean loss: 424.24
 ---- batch: 110 ----
mean loss: 422.49
train mean loss: 429.09
epoch train time: 0:00:02.551538
elapsed time: 0:03:07.115485
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 21:59:46.831758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.98
 ---- batch: 020 ----
mean loss: 417.17
 ---- batch: 030 ----
mean loss: 428.63
 ---- batch: 040 ----
mean loss: 423.92
 ---- batch: 050 ----
mean loss: 434.11
 ---- batch: 060 ----
mean loss: 406.90
 ---- batch: 070 ----
mean loss: 411.34
 ---- batch: 080 ----
mean loss: 416.31
 ---- batch: 090 ----
mean loss: 410.45
 ---- batch: 100 ----
mean loss: 421.38
 ---- batch: 110 ----
mean loss: 414.53
train mean loss: 419.21
epoch train time: 0:00:02.565721
elapsed time: 0:03:09.681756
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 21:59:49.397960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.39
 ---- batch: 020 ----
mean loss: 413.98
 ---- batch: 030 ----
mean loss: 414.23
 ---- batch: 040 ----
mean loss: 404.06
 ---- batch: 050 ----
mean loss: 407.65
 ---- batch: 060 ----
mean loss: 409.74
 ---- batch: 070 ----
mean loss: 415.05
 ---- batch: 080 ----
mean loss: 400.63
 ---- batch: 090 ----
mean loss: 412.12
 ---- batch: 100 ----
mean loss: 403.98
 ---- batch: 110 ----
mean loss: 413.31
train mean loss: 408.06
epoch train time: 0:00:02.578501
elapsed time: 0:03:12.260665
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 21:59:51.976863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.45
 ---- batch: 020 ----
mean loss: 392.50
 ---- batch: 030 ----
mean loss: 396.38
 ---- batch: 040 ----
mean loss: 408.98
 ---- batch: 050 ----
mean loss: 402.66
 ---- batch: 060 ----
mean loss: 394.77
 ---- batch: 070 ----
mean loss: 380.84
 ---- batch: 080 ----
mean loss: 397.69
 ---- batch: 090 ----
mean loss: 395.29
 ---- batch: 100 ----
mean loss: 393.71
 ---- batch: 110 ----
mean loss: 398.61
train mean loss: 397.30
epoch train time: 0:00:02.586945
elapsed time: 0:03:14.848052
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 21:59:54.564259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.22
 ---- batch: 020 ----
mean loss: 380.84
 ---- batch: 030 ----
mean loss: 382.93
 ---- batch: 040 ----
mean loss: 389.99
 ---- batch: 050 ----
mean loss: 376.20
 ---- batch: 060 ----
mean loss: 384.99
 ---- batch: 070 ----
mean loss: 395.01
 ---- batch: 080 ----
mean loss: 381.54
 ---- batch: 090 ----
mean loss: 380.03
 ---- batch: 100 ----
mean loss: 388.25
 ---- batch: 110 ----
mean loss: 398.95
train mean loss: 385.63
epoch train time: 0:00:02.561252
elapsed time: 0:03:17.409775
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 21:59:57.126000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.71
 ---- batch: 020 ----
mean loss: 381.30
 ---- batch: 030 ----
mean loss: 375.66
 ---- batch: 040 ----
mean loss: 358.12
 ---- batch: 050 ----
mean loss: 371.64
 ---- batch: 060 ----
mean loss: 375.41
 ---- batch: 070 ----
mean loss: 370.86
 ---- batch: 080 ----
mean loss: 375.51
 ---- batch: 090 ----
mean loss: 388.57
 ---- batch: 100 ----
mean loss: 370.57
 ---- batch: 110 ----
mean loss: 372.17
train mean loss: 374.78
epoch train time: 0:00:02.572707
elapsed time: 0:03:19.982975
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 21:59:59.699310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.06
 ---- batch: 020 ----
mean loss: 360.71
 ---- batch: 030 ----
mean loss: 363.92
 ---- batch: 040 ----
mean loss: 370.26
 ---- batch: 050 ----
mean loss: 380.07
 ---- batch: 060 ----
mean loss: 362.73
 ---- batch: 070 ----
mean loss: 360.19
 ---- batch: 080 ----
mean loss: 363.31
 ---- batch: 090 ----
mean loss: 355.22
 ---- batch: 100 ----
mean loss: 350.83
 ---- batch: 110 ----
mean loss: 361.63
train mean loss: 364.20
epoch train time: 0:00:02.584219
elapsed time: 0:03:22.567757
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 22:00:02.283946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.87
 ---- batch: 020 ----
mean loss: 360.19
 ---- batch: 030 ----
mean loss: 358.41
 ---- batch: 040 ----
mean loss: 357.31
 ---- batch: 050 ----
mean loss: 352.28
 ---- batch: 060 ----
mean loss: 364.20
 ---- batch: 070 ----
mean loss: 357.29
 ---- batch: 080 ----
mean loss: 356.85
 ---- batch: 090 ----
mean loss: 349.28
 ---- batch: 100 ----
mean loss: 349.13
 ---- batch: 110 ----
mean loss: 341.18
train mean loss: 354.46
epoch train time: 0:00:02.588537
elapsed time: 0:03:25.156689
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 22:00:04.872901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.78
 ---- batch: 020 ----
mean loss: 352.61
 ---- batch: 030 ----
mean loss: 340.05
 ---- batch: 040 ----
mean loss: 351.01
 ---- batch: 050 ----
mean loss: 341.84
 ---- batch: 060 ----
mean loss: 340.02
 ---- batch: 070 ----
mean loss: 338.45
 ---- batch: 080 ----
mean loss: 337.85
 ---- batch: 090 ----
mean loss: 344.67
 ---- batch: 100 ----
mean loss: 344.50
 ---- batch: 110 ----
mean loss: 350.44
train mean loss: 345.21
epoch train time: 0:00:02.571460
elapsed time: 0:03:27.728606
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 22:00:07.444798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.83
 ---- batch: 020 ----
mean loss: 341.85
 ---- batch: 030 ----
mean loss: 331.45
 ---- batch: 040 ----
mean loss: 337.50
 ---- batch: 050 ----
mean loss: 339.26
 ---- batch: 060 ----
mean loss: 348.63
 ---- batch: 070 ----
mean loss: 341.93
 ---- batch: 080 ----
mean loss: 332.11
 ---- batch: 090 ----
mean loss: 328.99
 ---- batch: 100 ----
mean loss: 325.99
 ---- batch: 110 ----
mean loss: 340.21
train mean loss: 336.10
epoch train time: 0:00:02.558794
elapsed time: 0:03:30.287824
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 22:00:10.004029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.21
 ---- batch: 020 ----
mean loss: 325.96
 ---- batch: 030 ----
mean loss: 334.18
 ---- batch: 040 ----
mean loss: 323.20
 ---- batch: 050 ----
mean loss: 324.51
 ---- batch: 060 ----
mean loss: 337.77
 ---- batch: 070 ----
mean loss: 334.58
 ---- batch: 080 ----
mean loss: 319.33
 ---- batch: 090 ----
mean loss: 334.10
 ---- batch: 100 ----
mean loss: 315.33
 ---- batch: 110 ----
mean loss: 333.01
train mean loss: 328.71
epoch train time: 0:00:02.540719
elapsed time: 0:03:32.828945
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 22:00:12.545139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.99
 ---- batch: 020 ----
mean loss: 326.32
 ---- batch: 030 ----
mean loss: 329.42
 ---- batch: 040 ----
mean loss: 322.95
 ---- batch: 050 ----
mean loss: 320.27
 ---- batch: 060 ----
mean loss: 310.15
 ---- batch: 070 ----
mean loss: 323.25
 ---- batch: 080 ----
mean loss: 321.18
 ---- batch: 090 ----
mean loss: 317.98
 ---- batch: 100 ----
mean loss: 317.45
 ---- batch: 110 ----
mean loss: 318.28
train mean loss: 321.05
epoch train time: 0:00:02.568287
elapsed time: 0:03:35.397640
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 22:00:15.113851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.89
 ---- batch: 020 ----
mean loss: 313.87
 ---- batch: 030 ----
mean loss: 318.61
 ---- batch: 040 ----
mean loss: 312.12
 ---- batch: 050 ----
mean loss: 294.63
 ---- batch: 060 ----
mean loss: 309.92
 ---- batch: 070 ----
mean loss: 318.94
 ---- batch: 080 ----
mean loss: 324.70
 ---- batch: 090 ----
mean loss: 313.10
 ---- batch: 100 ----
mean loss: 319.10
 ---- batch: 110 ----
mean loss: 311.49
train mean loss: 314.23
epoch train time: 0:00:02.561113
elapsed time: 0:03:37.959188
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 22:00:17.675425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.80
 ---- batch: 020 ----
mean loss: 306.67
 ---- batch: 030 ----
mean loss: 306.17
 ---- batch: 040 ----
mean loss: 316.38
 ---- batch: 050 ----
mean loss: 314.12
 ---- batch: 060 ----
mean loss: 306.47
 ---- batch: 070 ----
mean loss: 302.05
 ---- batch: 080 ----
mean loss: 310.43
 ---- batch: 090 ----
mean loss: 309.86
 ---- batch: 100 ----
mean loss: 304.99
 ---- batch: 110 ----
mean loss: 306.00
train mean loss: 307.23
epoch train time: 0:00:02.581979
elapsed time: 0:03:40.541617
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 22:00:20.257853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.02
 ---- batch: 020 ----
mean loss: 305.90
 ---- batch: 030 ----
mean loss: 294.51
 ---- batch: 040 ----
mean loss: 299.89
 ---- batch: 050 ----
mean loss: 294.60
 ---- batch: 060 ----
mean loss: 304.21
 ---- batch: 070 ----
mean loss: 305.54
 ---- batch: 080 ----
mean loss: 303.50
 ---- batch: 090 ----
mean loss: 298.67
 ---- batch: 100 ----
mean loss: 295.69
 ---- batch: 110 ----
mean loss: 299.60
train mean loss: 300.87
epoch train time: 0:00:02.566930
elapsed time: 0:03:43.109011
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 22:00:22.825206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.04
 ---- batch: 020 ----
mean loss: 293.86
 ---- batch: 030 ----
mean loss: 287.10
 ---- batch: 040 ----
mean loss: 291.97
 ---- batch: 050 ----
mean loss: 291.18
 ---- batch: 060 ----
mean loss: 304.04
 ---- batch: 070 ----
mean loss: 298.04
 ---- batch: 080 ----
mean loss: 293.03
 ---- batch: 090 ----
mean loss: 290.21
 ---- batch: 100 ----
mean loss: 297.00
 ---- batch: 110 ----
mean loss: 290.35
train mean loss: 294.82
epoch train time: 0:00:02.556671
elapsed time: 0:03:45.666075
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 22:00:25.382279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.64
 ---- batch: 020 ----
mean loss: 291.09
 ---- batch: 030 ----
mean loss: 301.42
 ---- batch: 040 ----
mean loss: 294.94
 ---- batch: 050 ----
mean loss: 298.82
 ---- batch: 060 ----
mean loss: 283.94
 ---- batch: 070 ----
mean loss: 282.24
 ---- batch: 080 ----
mean loss: 280.48
 ---- batch: 090 ----
mean loss: 286.69
 ---- batch: 100 ----
mean loss: 285.26
 ---- batch: 110 ----
mean loss: 286.97
train mean loss: 289.40
epoch train time: 0:00:02.572450
elapsed time: 0:03:48.238951
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 22:00:27.955156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.96
 ---- batch: 020 ----
mean loss: 279.52
 ---- batch: 030 ----
mean loss: 287.82
 ---- batch: 040 ----
mean loss: 287.79
 ---- batch: 050 ----
mean loss: 284.05
 ---- batch: 060 ----
mean loss: 286.24
 ---- batch: 070 ----
mean loss: 284.29
 ---- batch: 080 ----
mean loss: 285.50
 ---- batch: 090 ----
mean loss: 283.37
 ---- batch: 100 ----
mean loss: 281.37
 ---- batch: 110 ----
mean loss: 283.77
train mean loss: 283.81
epoch train time: 0:00:02.571551
elapsed time: 0:03:50.810908
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 22:00:30.527093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.93
 ---- batch: 020 ----
mean loss: 287.24
 ---- batch: 030 ----
mean loss: 276.03
 ---- batch: 040 ----
mean loss: 278.24
 ---- batch: 050 ----
mean loss: 284.52
 ---- batch: 060 ----
mean loss: 285.17
 ---- batch: 070 ----
mean loss: 275.56
 ---- batch: 080 ----
mean loss: 276.15
 ---- batch: 090 ----
mean loss: 286.37
 ---- batch: 100 ----
mean loss: 275.68
 ---- batch: 110 ----
mean loss: 274.08
train mean loss: 279.93
epoch train time: 0:00:02.557425
elapsed time: 0:03:53.368723
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 22:00:33.084910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.08
 ---- batch: 020 ----
mean loss: 275.89
 ---- batch: 030 ----
mean loss: 271.41
 ---- batch: 040 ----
mean loss: 275.05
 ---- batch: 050 ----
mean loss: 280.90
 ---- batch: 060 ----
mean loss: 282.19
 ---- batch: 070 ----
mean loss: 278.16
 ---- batch: 080 ----
mean loss: 281.77
 ---- batch: 090 ----
mean loss: 266.57
 ---- batch: 100 ----
mean loss: 272.34
 ---- batch: 110 ----
mean loss: 271.07
train mean loss: 275.39
epoch train time: 0:00:02.577370
elapsed time: 0:03:55.946504
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 22:00:35.662703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.94
 ---- batch: 020 ----
mean loss: 272.12
 ---- batch: 030 ----
mean loss: 270.90
 ---- batch: 040 ----
mean loss: 277.72
 ---- batch: 050 ----
mean loss: 275.24
 ---- batch: 060 ----
mean loss: 266.65
 ---- batch: 070 ----
mean loss: 269.58
 ---- batch: 080 ----
mean loss: 270.48
 ---- batch: 090 ----
mean loss: 283.26
 ---- batch: 100 ----
mean loss: 255.16
 ---- batch: 110 ----
mean loss: 277.40
train mean loss: 271.53
epoch train time: 0:00:02.557346
elapsed time: 0:03:58.504249
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 22:00:38.220462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.45
 ---- batch: 020 ----
mean loss: 273.29
 ---- batch: 030 ----
mean loss: 262.82
 ---- batch: 040 ----
mean loss: 265.16
 ---- batch: 050 ----
mean loss: 273.57
 ---- batch: 060 ----
mean loss: 266.82
 ---- batch: 070 ----
mean loss: 273.96
 ---- batch: 080 ----
mean loss: 264.65
 ---- batch: 090 ----
mean loss: 267.56
 ---- batch: 100 ----
mean loss: 274.52
 ---- batch: 110 ----
mean loss: 260.67
train mean loss: 268.07
epoch train time: 0:00:02.567119
elapsed time: 0:04:01.071799
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 22:00:40.787982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.37
 ---- batch: 020 ----
mean loss: 267.89
 ---- batch: 030 ----
mean loss: 256.61
 ---- batch: 040 ----
mean loss: 262.01
 ---- batch: 050 ----
mean loss: 261.28
 ---- batch: 060 ----
mean loss: 264.44
 ---- batch: 070 ----
mean loss: 263.44
 ---- batch: 080 ----
mean loss: 276.21
 ---- batch: 090 ----
mean loss: 269.37
 ---- batch: 100 ----
mean loss: 256.98
 ---- batch: 110 ----
mean loss: 270.16
train mean loss: 264.55
epoch train time: 0:00:02.546209
elapsed time: 0:04:03.618398
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 22:00:43.334582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.51
 ---- batch: 020 ----
mean loss: 268.86
 ---- batch: 030 ----
mean loss: 264.64
 ---- batch: 040 ----
mean loss: 266.70
 ---- batch: 050 ----
mean loss: 258.52
 ---- batch: 060 ----
mean loss: 264.45
 ---- batch: 070 ----
mean loss: 261.57
 ---- batch: 080 ----
mean loss: 257.34
 ---- batch: 090 ----
mean loss: 259.13
 ---- batch: 100 ----
mean loss: 262.02
 ---- batch: 110 ----
mean loss: 265.07
train mean loss: 262.20
epoch train time: 0:00:02.557167
elapsed time: 0:04:06.176004
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 22:00:45.892200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.70
 ---- batch: 020 ----
mean loss: 266.07
 ---- batch: 030 ----
mean loss: 253.84
 ---- batch: 040 ----
mean loss: 258.60
 ---- batch: 050 ----
mean loss: 262.62
 ---- batch: 060 ----
mean loss: 257.50
 ---- batch: 070 ----
mean loss: 246.60
 ---- batch: 080 ----
mean loss: 258.80
 ---- batch: 090 ----
mean loss: 258.48
 ---- batch: 100 ----
mean loss: 262.64
 ---- batch: 110 ----
mean loss: 258.28
train mean loss: 258.50
epoch train time: 0:00:02.555405
elapsed time: 0:04:08.731806
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 22:00:48.448005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.11
 ---- batch: 020 ----
mean loss: 263.21
 ---- batch: 030 ----
mean loss: 255.40
 ---- batch: 040 ----
mean loss: 253.20
 ---- batch: 050 ----
mean loss: 247.16
 ---- batch: 060 ----
mean loss: 253.85
 ---- batch: 070 ----
mean loss: 261.04
 ---- batch: 080 ----
mean loss: 263.03
 ---- batch: 090 ----
mean loss: 256.30
 ---- batch: 100 ----
mean loss: 257.31
 ---- batch: 110 ----
mean loss: 248.90
train mean loss: 255.72
epoch train time: 0:00:02.574535
elapsed time: 0:04:11.306738
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 22:00:51.022975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.70
 ---- batch: 020 ----
mean loss: 262.89
 ---- batch: 030 ----
mean loss: 243.10
 ---- batch: 040 ----
mean loss: 260.49
 ---- batch: 050 ----
mean loss: 260.34
 ---- batch: 060 ----
mean loss: 252.25
 ---- batch: 070 ----
mean loss: 254.80
 ---- batch: 080 ----
mean loss: 254.78
 ---- batch: 090 ----
mean loss: 251.95
 ---- batch: 100 ----
mean loss: 251.08
 ---- batch: 110 ----
mean loss: 248.09
train mean loss: 253.17
epoch train time: 0:00:02.565575
elapsed time: 0:04:13.872755
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 22:00:53.588980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.93
 ---- batch: 020 ----
mean loss: 262.52
 ---- batch: 030 ----
mean loss: 246.35
 ---- batch: 040 ----
mean loss: 250.52
 ---- batch: 050 ----
mean loss: 252.35
 ---- batch: 060 ----
mean loss: 246.17
 ---- batch: 070 ----
mean loss: 243.24
 ---- batch: 080 ----
mean loss: 244.03
 ---- batch: 090 ----
mean loss: 253.99
 ---- batch: 100 ----
mean loss: 251.28
 ---- batch: 110 ----
mean loss: 253.28
train mean loss: 250.56
epoch train time: 0:00:02.584393
elapsed time: 0:04:16.457633
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 22:00:56.173895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.16
 ---- batch: 020 ----
mean loss: 251.69
 ---- batch: 030 ----
mean loss: 248.48
 ---- batch: 040 ----
mean loss: 239.53
 ---- batch: 050 ----
mean loss: 250.79
 ---- batch: 060 ----
mean loss: 248.16
 ---- batch: 070 ----
mean loss: 246.13
 ---- batch: 080 ----
mean loss: 253.43
 ---- batch: 090 ----
mean loss: 247.09
 ---- batch: 100 ----
mean loss: 238.94
 ---- batch: 110 ----
mean loss: 248.75
train mean loss: 248.22
epoch train time: 0:00:02.542888
elapsed time: 0:04:19.001010
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 22:00:58.717221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.63
 ---- batch: 020 ----
mean loss: 244.34
 ---- batch: 030 ----
mean loss: 249.02
 ---- batch: 040 ----
mean loss: 247.43
 ---- batch: 050 ----
mean loss: 252.22
 ---- batch: 060 ----
mean loss: 240.04
 ---- batch: 070 ----
mean loss: 245.72
 ---- batch: 080 ----
mean loss: 248.85
 ---- batch: 090 ----
mean loss: 249.40
 ---- batch: 100 ----
mean loss: 248.90
 ---- batch: 110 ----
mean loss: 246.20
train mean loss: 245.93
epoch train time: 0:00:02.579865
elapsed time: 0:04:21.581290
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 22:01:01.297478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.39
 ---- batch: 020 ----
mean loss: 244.72
 ---- batch: 030 ----
mean loss: 250.36
 ---- batch: 040 ----
mean loss: 252.98
 ---- batch: 050 ----
mean loss: 236.09
 ---- batch: 060 ----
mean loss: 238.16
 ---- batch: 070 ----
mean loss: 244.65
 ---- batch: 080 ----
mean loss: 241.27
 ---- batch: 090 ----
mean loss: 245.04
 ---- batch: 100 ----
mean loss: 243.21
 ---- batch: 110 ----
mean loss: 236.92
train mean loss: 243.78
epoch train time: 0:00:02.568169
elapsed time: 0:04:24.149853
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 22:01:03.866075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.26
 ---- batch: 020 ----
mean loss: 240.77
 ---- batch: 030 ----
mean loss: 233.85
 ---- batch: 040 ----
mean loss: 242.07
 ---- batch: 050 ----
mean loss: 238.95
 ---- batch: 060 ----
mean loss: 245.90
 ---- batch: 070 ----
mean loss: 247.43
 ---- batch: 080 ----
mean loss: 249.39
 ---- batch: 090 ----
mean loss: 240.01
 ---- batch: 100 ----
mean loss: 239.71
 ---- batch: 110 ----
mean loss: 245.00
train mean loss: 241.62
epoch train time: 0:00:02.581828
elapsed time: 0:04:26.732118
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 22:01:06.448350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.54
 ---- batch: 020 ----
mean loss: 250.20
 ---- batch: 030 ----
mean loss: 247.99
 ---- batch: 040 ----
mean loss: 237.22
 ---- batch: 050 ----
mean loss: 239.69
 ---- batch: 060 ----
mean loss: 239.98
 ---- batch: 070 ----
mean loss: 238.84
 ---- batch: 080 ----
mean loss: 234.00
 ---- batch: 090 ----
mean loss: 238.74
 ---- batch: 100 ----
mean loss: 227.08
 ---- batch: 110 ----
mean loss: 244.21
train mean loss: 239.95
epoch train time: 0:00:02.579209
elapsed time: 0:04:29.311770
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 22:01:09.027993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.26
 ---- batch: 020 ----
mean loss: 238.85
 ---- batch: 030 ----
mean loss: 238.52
 ---- batch: 040 ----
mean loss: 243.51
 ---- batch: 050 ----
mean loss: 230.37
 ---- batch: 060 ----
mean loss: 236.29
 ---- batch: 070 ----
mean loss: 242.83
 ---- batch: 080 ----
mean loss: 235.24
 ---- batch: 090 ----
mean loss: 232.93
 ---- batch: 100 ----
mean loss: 232.88
 ---- batch: 110 ----
mean loss: 241.85
train mean loss: 237.92
epoch train time: 0:00:02.570581
elapsed time: 0:04:31.882775
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 22:01:11.599018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.10
 ---- batch: 020 ----
mean loss: 239.77
 ---- batch: 030 ----
mean loss: 237.45
 ---- batch: 040 ----
mean loss: 240.19
 ---- batch: 050 ----
mean loss: 231.81
 ---- batch: 060 ----
mean loss: 227.63
 ---- batch: 070 ----
mean loss: 235.23
 ---- batch: 080 ----
mean loss: 233.45
 ---- batch: 090 ----
mean loss: 243.10
 ---- batch: 100 ----
mean loss: 236.72
 ---- batch: 110 ----
mean loss: 233.36
train mean loss: 235.92
epoch train time: 0:00:02.553317
elapsed time: 0:04:34.436554
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 22:01:14.152764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.21
 ---- batch: 020 ----
mean loss: 244.59
 ---- batch: 030 ----
mean loss: 231.61
 ---- batch: 040 ----
mean loss: 235.91
 ---- batch: 050 ----
mean loss: 230.18
 ---- batch: 060 ----
mean loss: 234.21
 ---- batch: 070 ----
mean loss: 237.89
 ---- batch: 080 ----
mean loss: 241.47
 ---- batch: 090 ----
mean loss: 234.32
 ---- batch: 100 ----
mean loss: 230.68
 ---- batch: 110 ----
mean loss: 229.70
train mean loss: 234.34
epoch train time: 0:00:02.573909
elapsed time: 0:04:37.010963
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 22:01:16.727176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.23
 ---- batch: 020 ----
mean loss: 234.20
 ---- batch: 030 ----
mean loss: 238.58
 ---- batch: 040 ----
mean loss: 228.17
 ---- batch: 050 ----
mean loss: 237.15
 ---- batch: 060 ----
mean loss: 234.13
 ---- batch: 070 ----
mean loss: 236.83
 ---- batch: 080 ----
mean loss: 229.58
 ---- batch: 090 ----
mean loss: 234.42
 ---- batch: 100 ----
mean loss: 230.79
 ---- batch: 110 ----
mean loss: 227.95
train mean loss: 232.59
epoch train time: 0:00:02.558115
elapsed time: 0:04:39.569571
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 22:01:19.285820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.67
 ---- batch: 020 ----
mean loss: 231.09
 ---- batch: 030 ----
mean loss: 231.84
 ---- batch: 040 ----
mean loss: 236.83
 ---- batch: 050 ----
mean loss: 238.34
 ---- batch: 060 ----
mean loss: 227.41
 ---- batch: 070 ----
mean loss: 224.61
 ---- batch: 080 ----
mean loss: 226.94
 ---- batch: 090 ----
mean loss: 230.89
 ---- batch: 100 ----
mean loss: 229.47
 ---- batch: 110 ----
mean loss: 231.47
train mean loss: 231.33
epoch train time: 0:00:02.589820
elapsed time: 0:04:42.159847
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 22:01:21.876051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.73
 ---- batch: 020 ----
mean loss: 223.34
 ---- batch: 030 ----
mean loss: 209.37
 ---- batch: 040 ----
mean loss: 235.02
 ---- batch: 050 ----
mean loss: 242.67
 ---- batch: 060 ----
mean loss: 234.17
 ---- batch: 070 ----
mean loss: 229.86
 ---- batch: 080 ----
mean loss: 231.87
 ---- batch: 090 ----
mean loss: 229.44
 ---- batch: 100 ----
mean loss: 226.80
 ---- batch: 110 ----
mean loss: 236.44
train mean loss: 229.71
epoch train time: 0:00:02.554277
elapsed time: 0:04:44.714576
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 22:01:24.430776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.66
 ---- batch: 020 ----
mean loss: 220.63
 ---- batch: 030 ----
mean loss: 228.47
 ---- batch: 040 ----
mean loss: 227.79
 ---- batch: 050 ----
mean loss: 239.66
 ---- batch: 060 ----
mean loss: 224.07
 ---- batch: 070 ----
mean loss: 229.12
 ---- batch: 080 ----
mean loss: 233.47
 ---- batch: 090 ----
mean loss: 232.89
 ---- batch: 100 ----
mean loss: 231.36
 ---- batch: 110 ----
mean loss: 226.34
train mean loss: 228.33
epoch train time: 0:00:02.581622
elapsed time: 0:04:47.296625
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 22:01:27.012666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.49
 ---- batch: 020 ----
mean loss: 228.00
 ---- batch: 030 ----
mean loss: 225.56
 ---- batch: 040 ----
mean loss: 220.01
 ---- batch: 050 ----
mean loss: 224.82
 ---- batch: 060 ----
mean loss: 231.11
 ---- batch: 070 ----
mean loss: 227.25
 ---- batch: 080 ----
mean loss: 238.91
 ---- batch: 090 ----
mean loss: 228.27
 ---- batch: 100 ----
mean loss: 227.56
 ---- batch: 110 ----
mean loss: 225.40
train mean loss: 227.09
epoch train time: 0:00:02.587466
elapsed time: 0:04:49.884346
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 22:01:29.600593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.00
 ---- batch: 020 ----
mean loss: 234.62
 ---- batch: 030 ----
mean loss: 219.20
 ---- batch: 040 ----
mean loss: 233.31
 ---- batch: 050 ----
mean loss: 217.15
 ---- batch: 060 ----
mean loss: 229.23
 ---- batch: 070 ----
mean loss: 216.36
 ---- batch: 080 ----
mean loss: 216.64
 ---- batch: 090 ----
mean loss: 224.63
 ---- batch: 100 ----
mean loss: 228.14
 ---- batch: 110 ----
mean loss: 233.10
train mean loss: 225.84
epoch train time: 0:00:02.575752
elapsed time: 0:04:52.460552
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 22:01:32.176758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.88
 ---- batch: 020 ----
mean loss: 225.24
 ---- batch: 030 ----
mean loss: 233.25
 ---- batch: 040 ----
mean loss: 222.61
 ---- batch: 050 ----
mean loss: 225.98
 ---- batch: 060 ----
mean loss: 225.38
 ---- batch: 070 ----
mean loss: 225.21
 ---- batch: 080 ----
mean loss: 223.25
 ---- batch: 090 ----
mean loss: 220.13
 ---- batch: 100 ----
mean loss: 230.46
 ---- batch: 110 ----
mean loss: 223.74
train mean loss: 224.63
epoch train time: 0:00:02.570474
elapsed time: 0:04:55.031471
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 22:01:34.747670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.79
 ---- batch: 020 ----
mean loss: 223.47
 ---- batch: 030 ----
mean loss: 228.84
 ---- batch: 040 ----
mean loss: 222.14
 ---- batch: 050 ----
mean loss: 225.53
 ---- batch: 060 ----
mean loss: 218.04
 ---- batch: 070 ----
mean loss: 222.37
 ---- batch: 080 ----
mean loss: 228.56
 ---- batch: 090 ----
mean loss: 224.34
 ---- batch: 100 ----
mean loss: 217.21
 ---- batch: 110 ----
mean loss: 224.24
train mean loss: 223.61
epoch train time: 0:00:02.574010
elapsed time: 0:04:57.605896
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 22:01:37.322103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.72
 ---- batch: 020 ----
mean loss: 218.44
 ---- batch: 030 ----
mean loss: 228.96
 ---- batch: 040 ----
mean loss: 224.33
 ---- batch: 050 ----
mean loss: 221.97
 ---- batch: 060 ----
mean loss: 227.75
 ---- batch: 070 ----
mean loss: 217.67
 ---- batch: 080 ----
mean loss: 225.88
 ---- batch: 090 ----
mean loss: 216.84
 ---- batch: 100 ----
mean loss: 219.44
 ---- batch: 110 ----
mean loss: 216.79
train mean loss: 222.45
epoch train time: 0:00:02.572832
elapsed time: 0:05:00.179156
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 22:01:39.895423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.75
 ---- batch: 020 ----
mean loss: 218.92
 ---- batch: 030 ----
mean loss: 221.73
 ---- batch: 040 ----
mean loss: 217.93
 ---- batch: 050 ----
mean loss: 215.68
 ---- batch: 060 ----
mean loss: 224.56
 ---- batch: 070 ----
mean loss: 218.28
 ---- batch: 080 ----
mean loss: 219.08
 ---- batch: 090 ----
mean loss: 216.79
 ---- batch: 100 ----
mean loss: 223.46
 ---- batch: 110 ----
mean loss: 229.64
train mean loss: 221.63
epoch train time: 0:00:02.589359
elapsed time: 0:05:02.769002
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 22:01:42.485220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.17
 ---- batch: 020 ----
mean loss: 222.76
 ---- batch: 030 ----
mean loss: 219.61
 ---- batch: 040 ----
mean loss: 219.32
 ---- batch: 050 ----
mean loss: 220.93
 ---- batch: 060 ----
mean loss: 211.51
 ---- batch: 070 ----
mean loss: 227.25
 ---- batch: 080 ----
mean loss: 213.50
 ---- batch: 090 ----
mean loss: 219.35
 ---- batch: 100 ----
mean loss: 224.28
 ---- batch: 110 ----
mean loss: 230.25
train mean loss: 220.39
epoch train time: 0:00:02.579456
elapsed time: 0:05:05.348904
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 22:01:45.065113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.84
 ---- batch: 020 ----
mean loss: 219.14
 ---- batch: 030 ----
mean loss: 219.24
 ---- batch: 040 ----
mean loss: 213.83
 ---- batch: 050 ----
mean loss: 212.77
 ---- batch: 060 ----
mean loss: 225.67
 ---- batch: 070 ----
mean loss: 220.76
 ---- batch: 080 ----
mean loss: 216.66
 ---- batch: 090 ----
mean loss: 217.58
 ---- batch: 100 ----
mean loss: 228.12
 ---- batch: 110 ----
mean loss: 215.66
train mean loss: 219.80
epoch train time: 0:00:02.571592
elapsed time: 0:05:07.921019
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 22:01:47.637126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.38
 ---- batch: 020 ----
mean loss: 229.57
 ---- batch: 030 ----
mean loss: 213.43
 ---- batch: 040 ----
mean loss: 219.67
 ---- batch: 050 ----
mean loss: 222.81
 ---- batch: 060 ----
mean loss: 216.69
 ---- batch: 070 ----
mean loss: 212.66
 ---- batch: 080 ----
mean loss: 224.93
 ---- batch: 090 ----
mean loss: 223.45
 ---- batch: 100 ----
mean loss: 210.07
 ---- batch: 110 ----
mean loss: 219.89
train mean loss: 218.85
epoch train time: 0:00:02.581750
elapsed time: 0:05:10.503110
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 22:01:50.219323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.38
 ---- batch: 020 ----
mean loss: 218.68
 ---- batch: 030 ----
mean loss: 221.37
 ---- batch: 040 ----
mean loss: 212.99
 ---- batch: 050 ----
mean loss: 228.62
 ---- batch: 060 ----
mean loss: 211.28
 ---- batch: 070 ----
mean loss: 228.20
 ---- batch: 080 ----
mean loss: 218.40
 ---- batch: 090 ----
mean loss: 222.88
 ---- batch: 100 ----
mean loss: 205.13
 ---- batch: 110 ----
mean loss: 210.52
train mean loss: 217.81
epoch train time: 0:00:02.555999
elapsed time: 0:05:13.059543
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 22:01:52.775744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.33
 ---- batch: 020 ----
mean loss: 221.17
 ---- batch: 030 ----
mean loss: 207.50
 ---- batch: 040 ----
mean loss: 212.73
 ---- batch: 050 ----
mean loss: 220.74
 ---- batch: 060 ----
mean loss: 215.12
 ---- batch: 070 ----
mean loss: 221.55
 ---- batch: 080 ----
mean loss: 216.83
 ---- batch: 090 ----
mean loss: 221.18
 ---- batch: 100 ----
mean loss: 223.40
 ---- batch: 110 ----
mean loss: 216.42
train mean loss: 217.17
epoch train time: 0:00:02.557059
elapsed time: 0:05:15.617028
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 22:01:55.333227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.02
 ---- batch: 020 ----
mean loss: 208.74
 ---- batch: 030 ----
mean loss: 223.07
 ---- batch: 040 ----
mean loss: 210.12
 ---- batch: 050 ----
mean loss: 218.79
 ---- batch: 060 ----
mean loss: 209.82
 ---- batch: 070 ----
mean loss: 223.80
 ---- batch: 080 ----
mean loss: 217.55
 ---- batch: 090 ----
mean loss: 211.36
 ---- batch: 100 ----
mean loss: 219.05
 ---- batch: 110 ----
mean loss: 221.91
train mean loss: 216.39
epoch train time: 0:00:02.579997
elapsed time: 0:05:18.197468
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 22:01:57.913751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.59
 ---- batch: 020 ----
mean loss: 217.37
 ---- batch: 030 ----
mean loss: 211.71
 ---- batch: 040 ----
mean loss: 207.53
 ---- batch: 050 ----
mean loss: 223.69
 ---- batch: 060 ----
mean loss: 212.15
 ---- batch: 070 ----
mean loss: 215.29
 ---- batch: 080 ----
mean loss: 223.31
 ---- batch: 090 ----
mean loss: 213.44
 ---- batch: 100 ----
mean loss: 208.22
 ---- batch: 110 ----
mean loss: 221.98
train mean loss: 215.95
epoch train time: 0:00:02.562302
elapsed time: 0:05:20.760260
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 22:02:00.476485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.72
 ---- batch: 020 ----
mean loss: 212.52
 ---- batch: 030 ----
mean loss: 215.58
 ---- batch: 040 ----
mean loss: 216.02
 ---- batch: 050 ----
mean loss: 204.84
 ---- batch: 060 ----
mean loss: 217.86
 ---- batch: 070 ----
mean loss: 218.45
 ---- batch: 080 ----
mean loss: 219.39
 ---- batch: 090 ----
mean loss: 221.79
 ---- batch: 100 ----
mean loss: 205.80
 ---- batch: 110 ----
mean loss: 217.74
train mean loss: 215.13
epoch train time: 0:00:02.557663
elapsed time: 0:05:23.318349
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 22:02:03.034590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.21
 ---- batch: 020 ----
mean loss: 201.88
 ---- batch: 030 ----
mean loss: 215.42
 ---- batch: 040 ----
mean loss: 203.39
 ---- batch: 050 ----
mean loss: 214.10
 ---- batch: 060 ----
mean loss: 218.68
 ---- batch: 070 ----
mean loss: 215.74
 ---- batch: 080 ----
mean loss: 220.95
 ---- batch: 090 ----
mean loss: 215.40
 ---- batch: 100 ----
mean loss: 215.80
 ---- batch: 110 ----
mean loss: 223.06
train mean loss: 214.50
epoch train time: 0:00:02.586273
elapsed time: 0:05:25.905172
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 22:02:05.621434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.05
 ---- batch: 020 ----
mean loss: 222.24
 ---- batch: 030 ----
mean loss: 210.06
 ---- batch: 040 ----
mean loss: 214.74
 ---- batch: 050 ----
mean loss: 221.64
 ---- batch: 060 ----
mean loss: 228.05
 ---- batch: 070 ----
mean loss: 217.85
 ---- batch: 080 ----
mean loss: 203.22
 ---- batch: 090 ----
mean loss: 212.94
 ---- batch: 100 ----
mean loss: 212.21
 ---- batch: 110 ----
mean loss: 207.56
train mean loss: 214.20
epoch train time: 0:00:02.570925
elapsed time: 0:05:28.476642
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 22:02:08.192877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.43
 ---- batch: 020 ----
mean loss: 221.33
 ---- batch: 030 ----
mean loss: 214.86
 ---- batch: 040 ----
mean loss: 203.75
 ---- batch: 050 ----
mean loss: 213.38
 ---- batch: 060 ----
mean loss: 213.90
 ---- batch: 070 ----
mean loss: 210.04
 ---- batch: 080 ----
mean loss: 220.10
 ---- batch: 090 ----
mean loss: 216.48
 ---- batch: 100 ----
mean loss: 205.06
 ---- batch: 110 ----
mean loss: 207.51
train mean loss: 213.40
epoch train time: 0:00:02.567171
elapsed time: 0:05:31.044274
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 22:02:10.760469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.32
 ---- batch: 020 ----
mean loss: 215.06
 ---- batch: 030 ----
mean loss: 209.96
 ---- batch: 040 ----
mean loss: 210.23
 ---- batch: 050 ----
mean loss: 210.06
 ---- batch: 060 ----
mean loss: 211.10
 ---- batch: 070 ----
mean loss: 217.44
 ---- batch: 080 ----
mean loss: 216.35
 ---- batch: 090 ----
mean loss: 211.95
 ---- batch: 100 ----
mean loss: 217.64
 ---- batch: 110 ----
mean loss: 210.75
train mean loss: 212.52
epoch train time: 0:00:02.560634
elapsed time: 0:05:33.605353
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 22:02:13.321544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.14
 ---- batch: 020 ----
mean loss: 217.19
 ---- batch: 030 ----
mean loss: 214.37
 ---- batch: 040 ----
mean loss: 211.02
 ---- batch: 050 ----
mean loss: 209.14
 ---- batch: 060 ----
mean loss: 210.87
 ---- batch: 070 ----
mean loss: 209.16
 ---- batch: 080 ----
mean loss: 222.94
 ---- batch: 090 ----
mean loss: 210.31
 ---- batch: 100 ----
mean loss: 210.84
 ---- batch: 110 ----
mean loss: 205.12
train mean loss: 212.33
epoch train time: 0:00:02.523551
elapsed time: 0:05:36.129383
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 22:02:15.845589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.03
 ---- batch: 020 ----
mean loss: 209.67
 ---- batch: 030 ----
mean loss: 215.56
 ---- batch: 040 ----
mean loss: 208.88
 ---- batch: 050 ----
mean loss: 211.55
 ---- batch: 060 ----
mean loss: 209.01
 ---- batch: 070 ----
mean loss: 209.17
 ---- batch: 080 ----
mean loss: 215.71
 ---- batch: 090 ----
mean loss: 216.03
 ---- batch: 100 ----
mean loss: 213.69
 ---- batch: 110 ----
mean loss: 209.83
train mean loss: 211.54
epoch train time: 0:00:02.554894
elapsed time: 0:05:38.684752
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 22:02:18.400964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.43
 ---- batch: 020 ----
mean loss: 205.46
 ---- batch: 030 ----
mean loss: 207.66
 ---- batch: 040 ----
mean loss: 215.32
 ---- batch: 050 ----
mean loss: 212.44
 ---- batch: 060 ----
mean loss: 221.60
 ---- batch: 070 ----
mean loss: 211.27
 ---- batch: 080 ----
mean loss: 208.80
 ---- batch: 090 ----
mean loss: 218.35
 ---- batch: 100 ----
mean loss: 207.75
 ---- batch: 110 ----
mean loss: 211.89
train mean loss: 211.39
epoch train time: 0:00:02.562688
elapsed time: 0:05:41.247881
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 22:02:20.964062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.87
 ---- batch: 020 ----
mean loss: 197.84
 ---- batch: 030 ----
mean loss: 216.44
 ---- batch: 040 ----
mean loss: 211.23
 ---- batch: 050 ----
mean loss: 206.16
 ---- batch: 060 ----
mean loss: 210.04
 ---- batch: 070 ----
mean loss: 216.10
 ---- batch: 080 ----
mean loss: 198.01
 ---- batch: 090 ----
mean loss: 220.69
 ---- batch: 100 ----
mean loss: 210.87
 ---- batch: 110 ----
mean loss: 214.32
train mean loss: 210.74
epoch train time: 0:00:02.575159
elapsed time: 0:05:43.823431
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 22:02:23.539626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.99
 ---- batch: 020 ----
mean loss: 208.86
 ---- batch: 030 ----
mean loss: 208.18
 ---- batch: 040 ----
mean loss: 212.80
 ---- batch: 050 ----
mean loss: 204.65
 ---- batch: 060 ----
mean loss: 208.39
 ---- batch: 070 ----
mean loss: 209.23
 ---- batch: 080 ----
mean loss: 211.08
 ---- batch: 090 ----
mean loss: 210.23
 ---- batch: 100 ----
mean loss: 215.59
 ---- batch: 110 ----
mean loss: 211.73
train mean loss: 210.24
epoch train time: 0:00:02.580239
elapsed time: 0:05:46.404065
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 22:02:26.120256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.52
 ---- batch: 020 ----
mean loss: 215.89
 ---- batch: 030 ----
mean loss: 206.81
 ---- batch: 040 ----
mean loss: 214.29
 ---- batch: 050 ----
mean loss: 216.57
 ---- batch: 060 ----
mean loss: 207.84
 ---- batch: 070 ----
mean loss: 214.27
 ---- batch: 080 ----
mean loss: 213.83
 ---- batch: 090 ----
mean loss: 195.86
 ---- batch: 100 ----
mean loss: 215.02
 ---- batch: 110 ----
mean loss: 201.46
train mean loss: 209.95
epoch train time: 0:00:02.587962
elapsed time: 0:05:48.992427
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 22:02:28.708610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.99
 ---- batch: 020 ----
mean loss: 203.10
 ---- batch: 030 ----
mean loss: 207.22
 ---- batch: 040 ----
mean loss: 210.82
 ---- batch: 050 ----
mean loss: 208.10
 ---- batch: 060 ----
mean loss: 211.75
 ---- batch: 070 ----
mean loss: 209.73
 ---- batch: 080 ----
mean loss: 211.85
 ---- batch: 090 ----
mean loss: 215.78
 ---- batch: 100 ----
mean loss: 209.51
 ---- batch: 110 ----
mean loss: 201.12
train mean loss: 209.26
epoch train time: 0:00:02.570739
elapsed time: 0:05:51.563558
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 22:02:31.279739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.74
 ---- batch: 020 ----
mean loss: 205.91
 ---- batch: 030 ----
mean loss: 209.56
 ---- batch: 040 ----
mean loss: 214.92
 ---- batch: 050 ----
mean loss: 217.55
 ---- batch: 060 ----
mean loss: 214.62
 ---- batch: 070 ----
mean loss: 211.25
 ---- batch: 080 ----
mean loss: 204.38
 ---- batch: 090 ----
mean loss: 208.25
 ---- batch: 100 ----
mean loss: 198.58
 ---- batch: 110 ----
mean loss: 204.38
train mean loss: 208.93
epoch train time: 0:00:02.594530
elapsed time: 0:05:54.158551
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 22:02:33.874754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.51
 ---- batch: 020 ----
mean loss: 203.33
 ---- batch: 030 ----
mean loss: 211.24
 ---- batch: 040 ----
mean loss: 217.01
 ---- batch: 050 ----
mean loss: 202.00
 ---- batch: 060 ----
mean loss: 208.64
 ---- batch: 070 ----
mean loss: 217.20
 ---- batch: 080 ----
mean loss: 214.35
 ---- batch: 090 ----
mean loss: 203.93
 ---- batch: 100 ----
mean loss: 201.71
 ---- batch: 110 ----
mean loss: 210.18
train mean loss: 208.58
epoch train time: 0:00:02.562650
elapsed time: 0:05:56.721595
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 22:02:36.437858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.78
 ---- batch: 020 ----
mean loss: 202.87
 ---- batch: 030 ----
mean loss: 203.44
 ---- batch: 040 ----
mean loss: 211.90
 ---- batch: 050 ----
mean loss: 209.14
 ---- batch: 060 ----
mean loss: 205.69
 ---- batch: 070 ----
mean loss: 209.77
 ---- batch: 080 ----
mean loss: 214.01
 ---- batch: 090 ----
mean loss: 209.14
 ---- batch: 100 ----
mean loss: 206.77
 ---- batch: 110 ----
mean loss: 205.70
train mean loss: 208.26
epoch train time: 0:00:02.549228
elapsed time: 0:05:59.271398
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 22:02:38.987489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.42
 ---- batch: 020 ----
mean loss: 207.69
 ---- batch: 030 ----
mean loss: 210.24
 ---- batch: 040 ----
mean loss: 192.58
 ---- batch: 050 ----
mean loss: 219.67
 ---- batch: 060 ----
mean loss: 206.92
 ---- batch: 070 ----
mean loss: 207.79
 ---- batch: 080 ----
mean loss: 206.03
 ---- batch: 090 ----
mean loss: 209.63
 ---- batch: 100 ----
mean loss: 206.67
 ---- batch: 110 ----
mean loss: 207.67
train mean loss: 207.77
epoch train time: 0:00:02.575308
elapsed time: 0:06:01.846997
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 22:02:41.563187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.07
 ---- batch: 020 ----
mean loss: 201.83
 ---- batch: 030 ----
mean loss: 204.47
 ---- batch: 040 ----
mean loss: 205.71
 ---- batch: 050 ----
mean loss: 216.89
 ---- batch: 060 ----
mean loss: 207.68
 ---- batch: 070 ----
mean loss: 208.97
 ---- batch: 080 ----
mean loss: 208.94
 ---- batch: 090 ----
mean loss: 201.93
 ---- batch: 100 ----
mean loss: 213.63
 ---- batch: 110 ----
mean loss: 199.56
train mean loss: 207.59
epoch train time: 0:00:02.568505
elapsed time: 0:06:04.415963
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 22:02:44.132166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.25
 ---- batch: 020 ----
mean loss: 214.58
 ---- batch: 030 ----
mean loss: 205.23
 ---- batch: 040 ----
mean loss: 207.77
 ---- batch: 050 ----
mean loss: 210.77
 ---- batch: 060 ----
mean loss: 210.50
 ---- batch: 070 ----
mean loss: 198.88
 ---- batch: 080 ----
mean loss: 208.46
 ---- batch: 090 ----
mean loss: 204.27
 ---- batch: 100 ----
mean loss: 206.03
 ---- batch: 110 ----
mean loss: 208.97
train mean loss: 207.05
epoch train time: 0:00:02.571639
elapsed time: 0:06:06.988045
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 22:02:46.704217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.47
 ---- batch: 020 ----
mean loss: 203.69
 ---- batch: 030 ----
mean loss: 208.34
 ---- batch: 040 ----
mean loss: 216.60
 ---- batch: 050 ----
mean loss: 198.88
 ---- batch: 060 ----
mean loss: 202.20
 ---- batch: 070 ----
mean loss: 213.55
 ---- batch: 080 ----
mean loss: 209.41
 ---- batch: 090 ----
mean loss: 209.58
 ---- batch: 100 ----
mean loss: 203.34
 ---- batch: 110 ----
mean loss: 202.12
train mean loss: 206.51
epoch train time: 0:00:02.567966
elapsed time: 0:06:09.556386
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 22:02:49.272591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.80
 ---- batch: 020 ----
mean loss: 206.08
 ---- batch: 030 ----
mean loss: 211.49
 ---- batch: 040 ----
mean loss: 207.12
 ---- batch: 050 ----
mean loss: 208.42
 ---- batch: 060 ----
mean loss: 204.97
 ---- batch: 070 ----
mean loss: 208.40
 ---- batch: 080 ----
mean loss: 206.03
 ---- batch: 090 ----
mean loss: 199.86
 ---- batch: 100 ----
mean loss: 209.36
 ---- batch: 110 ----
mean loss: 205.11
train mean loss: 206.70
epoch train time: 0:00:02.553066
elapsed time: 0:06:12.109868
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 22:02:51.826072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.18
 ---- batch: 020 ----
mean loss: 211.51
 ---- batch: 030 ----
mean loss: 209.68
 ---- batch: 040 ----
mean loss: 208.90
 ---- batch: 050 ----
mean loss: 204.23
 ---- batch: 060 ----
mean loss: 207.64
 ---- batch: 070 ----
mean loss: 212.03
 ---- batch: 080 ----
mean loss: 201.53
 ---- batch: 090 ----
mean loss: 196.02
 ---- batch: 100 ----
mean loss: 204.97
 ---- batch: 110 ----
mean loss: 201.48
train mean loss: 205.96
epoch train time: 0:00:02.554838
elapsed time: 0:06:14.665126
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 22:02:54.381320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.44
 ---- batch: 020 ----
mean loss: 203.20
 ---- batch: 030 ----
mean loss: 207.16
 ---- batch: 040 ----
mean loss: 197.31
 ---- batch: 050 ----
mean loss: 204.98
 ---- batch: 060 ----
mean loss: 199.83
 ---- batch: 070 ----
mean loss: 203.07
 ---- batch: 080 ----
mean loss: 202.79
 ---- batch: 090 ----
mean loss: 212.57
 ---- batch: 100 ----
mean loss: 197.61
 ---- batch: 110 ----
mean loss: 211.68
train mean loss: 205.77
epoch train time: 0:00:02.577541
elapsed time: 0:06:17.243081
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 22:02:56.959275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.68
 ---- batch: 020 ----
mean loss: 210.11
 ---- batch: 030 ----
mean loss: 213.21
 ---- batch: 040 ----
mean loss: 207.71
 ---- batch: 050 ----
mean loss: 204.66
 ---- batch: 060 ----
mean loss: 207.91
 ---- batch: 070 ----
mean loss: 205.25
 ---- batch: 080 ----
mean loss: 199.36
 ---- batch: 090 ----
mean loss: 201.56
 ---- batch: 100 ----
mean loss: 205.33
 ---- batch: 110 ----
mean loss: 200.56
train mean loss: 205.47
epoch train time: 0:00:02.576903
elapsed time: 0:06:19.820392
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 22:02:59.536597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.06
 ---- batch: 020 ----
mean loss: 213.34
 ---- batch: 030 ----
mean loss: 210.21
 ---- batch: 040 ----
mean loss: 205.11
 ---- batch: 050 ----
mean loss: 202.22
 ---- batch: 060 ----
mean loss: 203.51
 ---- batch: 070 ----
mean loss: 201.68
 ---- batch: 080 ----
mean loss: 200.42
 ---- batch: 090 ----
mean loss: 208.01
 ---- batch: 100 ----
mean loss: 200.84
 ---- batch: 110 ----
mean loss: 206.26
train mean loss: 205.13
epoch train time: 0:00:02.579502
elapsed time: 0:06:22.400313
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 22:03:02.116609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.91
 ---- batch: 020 ----
mean loss: 208.44
 ---- batch: 030 ----
mean loss: 204.52
 ---- batch: 040 ----
mean loss: 204.96
 ---- batch: 050 ----
mean loss: 195.89
 ---- batch: 060 ----
mean loss: 200.73
 ---- batch: 070 ----
mean loss: 208.68
 ---- batch: 080 ----
mean loss: 203.92
 ---- batch: 090 ----
mean loss: 208.58
 ---- batch: 100 ----
mean loss: 196.64
 ---- batch: 110 ----
mean loss: 202.16
train mean loss: 204.79
epoch train time: 0:00:02.577102
elapsed time: 0:06:24.977923
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 22:03:04.694137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.77
 ---- batch: 020 ----
mean loss: 207.25
 ---- batch: 030 ----
mean loss: 206.96
 ---- batch: 040 ----
mean loss: 207.01
 ---- batch: 050 ----
mean loss: 207.64
 ---- batch: 060 ----
mean loss: 210.77
 ---- batch: 070 ----
mean loss: 196.55
 ---- batch: 080 ----
mean loss: 206.78
 ---- batch: 090 ----
mean loss: 193.57
 ---- batch: 100 ----
mean loss: 209.46
 ---- batch: 110 ----
mean loss: 207.25
train mean loss: 204.50
epoch train time: 0:00:02.572689
elapsed time: 0:06:27.551046
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 22:03:07.267231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.34
 ---- batch: 020 ----
mean loss: 211.13
 ---- batch: 030 ----
mean loss: 199.11
 ---- batch: 040 ----
mean loss: 209.38
 ---- batch: 050 ----
mean loss: 195.30
 ---- batch: 060 ----
mean loss: 209.55
 ---- batch: 070 ----
mean loss: 196.96
 ---- batch: 080 ----
mean loss: 203.87
 ---- batch: 090 ----
mean loss: 196.91
 ---- batch: 100 ----
mean loss: 210.78
 ---- batch: 110 ----
mean loss: 198.08
train mean loss: 204.06
epoch train time: 0:00:02.572362
elapsed time: 0:06:30.123830
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 22:03:09.840024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.09
 ---- batch: 020 ----
mean loss: 213.52
 ---- batch: 030 ----
mean loss: 206.33
 ---- batch: 040 ----
mean loss: 209.85
 ---- batch: 050 ----
mean loss: 207.20
 ---- batch: 060 ----
mean loss: 208.37
 ---- batch: 070 ----
mean loss: 197.32
 ---- batch: 080 ----
mean loss: 197.70
 ---- batch: 090 ----
mean loss: 203.04
 ---- batch: 100 ----
mean loss: 193.71
 ---- batch: 110 ----
mean loss: 205.95
train mean loss: 203.83
epoch train time: 0:00:02.558941
elapsed time: 0:06:32.683168
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 22:03:12.399379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.66
 ---- batch: 020 ----
mean loss: 202.23
 ---- batch: 030 ----
mean loss: 213.32
 ---- batch: 040 ----
mean loss: 207.88
 ---- batch: 050 ----
mean loss: 209.32
 ---- batch: 060 ----
mean loss: 196.92
 ---- batch: 070 ----
mean loss: 207.43
 ---- batch: 080 ----
mean loss: 204.32
 ---- batch: 090 ----
mean loss: 213.14
 ---- batch: 100 ----
mean loss: 199.46
 ---- batch: 110 ----
mean loss: 193.44
train mean loss: 203.64
epoch train time: 0:00:02.557372
elapsed time: 0:06:35.240949
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 22:03:14.957127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.85
 ---- batch: 020 ----
mean loss: 210.08
 ---- batch: 030 ----
mean loss: 203.03
 ---- batch: 040 ----
mean loss: 196.59
 ---- batch: 050 ----
mean loss: 199.27
 ---- batch: 060 ----
mean loss: 208.33
 ---- batch: 070 ----
mean loss: 202.39
 ---- batch: 080 ----
mean loss: 207.24
 ---- batch: 090 ----
mean loss: 202.55
 ---- batch: 100 ----
mean loss: 198.37
 ---- batch: 110 ----
mean loss: 199.50
train mean loss: 203.19
epoch train time: 0:00:02.547615
elapsed time: 0:06:37.788947
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 22:03:17.505145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.14
 ---- batch: 020 ----
mean loss: 212.65
 ---- batch: 030 ----
mean loss: 198.99
 ---- batch: 040 ----
mean loss: 202.14
 ---- batch: 050 ----
mean loss: 200.67
 ---- batch: 060 ----
mean loss: 201.75
 ---- batch: 070 ----
mean loss: 200.56
 ---- batch: 080 ----
mean loss: 200.52
 ---- batch: 090 ----
mean loss: 194.93
 ---- batch: 100 ----
mean loss: 207.98
 ---- batch: 110 ----
mean loss: 210.74
train mean loss: 203.03
epoch train time: 0:00:02.566161
elapsed time: 0:06:40.355514
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 22:03:20.071736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.21
 ---- batch: 020 ----
mean loss: 197.31
 ---- batch: 030 ----
mean loss: 195.50
 ---- batch: 040 ----
mean loss: 202.00
 ---- batch: 050 ----
mean loss: 209.97
 ---- batch: 060 ----
mean loss: 194.92
 ---- batch: 070 ----
mean loss: 204.80
 ---- batch: 080 ----
mean loss: 212.53
 ---- batch: 090 ----
mean loss: 207.77
 ---- batch: 100 ----
mean loss: 206.49
 ---- batch: 110 ----
mean loss: 193.14
train mean loss: 202.69
epoch train time: 0:00:02.552214
elapsed time: 0:06:42.908157
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 22:03:22.624363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.15
 ---- batch: 020 ----
mean loss: 202.98
 ---- batch: 030 ----
mean loss: 203.70
 ---- batch: 040 ----
mean loss: 207.68
 ---- batch: 050 ----
mean loss: 199.50
 ---- batch: 060 ----
mean loss: 203.00
 ---- batch: 070 ----
mean loss: 196.52
 ---- batch: 080 ----
mean loss: 210.77
 ---- batch: 090 ----
mean loss: 199.80
 ---- batch: 100 ----
mean loss: 206.76
 ---- batch: 110 ----
mean loss: 199.12
train mean loss: 202.44
epoch train time: 0:00:02.575969
elapsed time: 0:06:45.484604
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 22:03:25.200800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.93
 ---- batch: 020 ----
mean loss: 205.99
 ---- batch: 030 ----
mean loss: 210.30
 ---- batch: 040 ----
mean loss: 198.51
 ---- batch: 050 ----
mean loss: 207.99
 ---- batch: 060 ----
mean loss: 205.23
 ---- batch: 070 ----
mean loss: 199.38
 ---- batch: 080 ----
mean loss: 202.60
 ---- batch: 090 ----
mean loss: 200.52
 ---- batch: 100 ----
mean loss: 200.24
 ---- batch: 110 ----
mean loss: 203.66
train mean loss: 202.09
epoch train time: 0:00:02.562477
elapsed time: 0:06:48.047495
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 22:03:27.763714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.26
 ---- batch: 020 ----
mean loss: 205.38
 ---- batch: 030 ----
mean loss: 196.02
 ---- batch: 040 ----
mean loss: 203.73
 ---- batch: 050 ----
mean loss: 196.64
 ---- batch: 060 ----
mean loss: 202.08
 ---- batch: 070 ----
mean loss: 205.95
 ---- batch: 080 ----
mean loss: 199.75
 ---- batch: 090 ----
mean loss: 201.95
 ---- batch: 100 ----
mean loss: 197.56
 ---- batch: 110 ----
mean loss: 204.89
train mean loss: 201.90
epoch train time: 0:00:02.543500
elapsed time: 0:06:50.591433
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 22:03:30.307666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.85
 ---- batch: 020 ----
mean loss: 205.02
 ---- batch: 030 ----
mean loss: 201.27
 ---- batch: 040 ----
mean loss: 201.31
 ---- batch: 050 ----
mean loss: 209.63
 ---- batch: 060 ----
mean loss: 197.72
 ---- batch: 070 ----
mean loss: 203.19
 ---- batch: 080 ----
mean loss: 199.11
 ---- batch: 090 ----
mean loss: 201.36
 ---- batch: 100 ----
mean loss: 208.82
 ---- batch: 110 ----
mean loss: 191.34
train mean loss: 201.42
epoch train time: 0:00:02.580341
elapsed time: 0:06:53.172255
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 22:03:32.888471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.13
 ---- batch: 020 ----
mean loss: 203.17
 ---- batch: 030 ----
mean loss: 208.52
 ---- batch: 040 ----
mean loss: 196.34
 ---- batch: 050 ----
mean loss: 202.62
 ---- batch: 060 ----
mean loss: 191.45
 ---- batch: 070 ----
mean loss: 198.11
 ---- batch: 080 ----
mean loss: 210.09
 ---- batch: 090 ----
mean loss: 203.66
 ---- batch: 100 ----
mean loss: 205.80
 ---- batch: 110 ----
mean loss: 195.73
train mean loss: 201.25
epoch train time: 0:00:02.636185
elapsed time: 0:06:55.809000
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 22:03:35.525094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.42
 ---- batch: 020 ----
mean loss: 199.03
 ---- batch: 030 ----
mean loss: 194.70
 ---- batch: 040 ----
mean loss: 190.39
 ---- batch: 050 ----
mean loss: 201.56
 ---- batch: 060 ----
mean loss: 203.67
 ---- batch: 070 ----
mean loss: 205.15
 ---- batch: 080 ----
mean loss: 204.78
 ---- batch: 090 ----
mean loss: 203.00
 ---- batch: 100 ----
mean loss: 203.84
 ---- batch: 110 ----
mean loss: 197.37
train mean loss: 200.99
epoch train time: 0:00:02.600688
elapsed time: 0:06:58.410048
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 22:03:38.126281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.21
 ---- batch: 020 ----
mean loss: 197.67
 ---- batch: 030 ----
mean loss: 205.52
 ---- batch: 040 ----
mean loss: 195.93
 ---- batch: 050 ----
mean loss: 201.48
 ---- batch: 060 ----
mean loss: 207.81
 ---- batch: 070 ----
mean loss: 195.97
 ---- batch: 080 ----
mean loss: 208.00
 ---- batch: 090 ----
mean loss: 200.46
 ---- batch: 100 ----
mean loss: 195.63
 ---- batch: 110 ----
mean loss: 196.85
train mean loss: 200.73
epoch train time: 0:00:02.621091
elapsed time: 0:07:01.031581
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 22:03:40.747773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.44
 ---- batch: 020 ----
mean loss: 197.72
 ---- batch: 030 ----
mean loss: 202.85
 ---- batch: 040 ----
mean loss: 199.72
 ---- batch: 050 ----
mean loss: 197.15
 ---- batch: 060 ----
mean loss: 203.86
 ---- batch: 070 ----
mean loss: 202.09
 ---- batch: 080 ----
mean loss: 200.44
 ---- batch: 090 ----
mean loss: 200.03
 ---- batch: 100 ----
mean loss: 204.95
 ---- batch: 110 ----
mean loss: 193.88
train mean loss: 200.42
epoch train time: 0:00:02.624471
elapsed time: 0:07:03.656452
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 22:03:43.372668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.48
 ---- batch: 020 ----
mean loss: 195.41
 ---- batch: 030 ----
mean loss: 199.86
 ---- batch: 040 ----
mean loss: 190.86
 ---- batch: 050 ----
mean loss: 202.13
 ---- batch: 060 ----
mean loss: 196.08
 ---- batch: 070 ----
mean loss: 193.16
 ---- batch: 080 ----
mean loss: 208.23
 ---- batch: 090 ----
mean loss: 205.07
 ---- batch: 100 ----
mean loss: 199.70
 ---- batch: 110 ----
mean loss: 196.56
train mean loss: 199.86
epoch train time: 0:00:02.547766
elapsed time: 0:07:06.204660
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 22:03:45.920871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.22
 ---- batch: 020 ----
mean loss: 189.74
 ---- batch: 030 ----
mean loss: 201.15
 ---- batch: 040 ----
mean loss: 195.56
 ---- batch: 050 ----
mean loss: 203.83
 ---- batch: 060 ----
mean loss: 202.26
 ---- batch: 070 ----
mean loss: 204.40
 ---- batch: 080 ----
mean loss: 196.81
 ---- batch: 090 ----
mean loss: 196.70
 ---- batch: 100 ----
mean loss: 197.23
 ---- batch: 110 ----
mean loss: 202.90
train mean loss: 199.48
epoch train time: 0:00:02.578395
elapsed time: 0:07:08.783484
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 22:03:48.499678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.10
 ---- batch: 020 ----
mean loss: 196.82
 ---- batch: 030 ----
mean loss: 199.48
 ---- batch: 040 ----
mean loss: 194.47
 ---- batch: 050 ----
mean loss: 198.94
 ---- batch: 060 ----
mean loss: 197.32
 ---- batch: 070 ----
mean loss: 201.54
 ---- batch: 080 ----
mean loss: 198.67
 ---- batch: 090 ----
mean loss: 203.19
 ---- batch: 100 ----
mean loss: 194.83
 ---- batch: 110 ----
mean loss: 204.65
train mean loss: 198.89
epoch train time: 0:00:02.546871
elapsed time: 0:07:11.330745
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 22:03:51.046938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.72
 ---- batch: 020 ----
mean loss: 194.10
 ---- batch: 030 ----
mean loss: 196.90
 ---- batch: 040 ----
mean loss: 201.54
 ---- batch: 050 ----
mean loss: 193.94
 ---- batch: 060 ----
mean loss: 196.71
 ---- batch: 070 ----
mean loss: 205.24
 ---- batch: 080 ----
mean loss: 193.43
 ---- batch: 090 ----
mean loss: 203.76
 ---- batch: 100 ----
mean loss: 200.58
 ---- batch: 110 ----
mean loss: 202.56
train mean loss: 198.86
epoch train time: 0:00:02.569558
elapsed time: 0:07:13.900716
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 22:03:53.616928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.53
 ---- batch: 020 ----
mean loss: 191.95
 ---- batch: 030 ----
mean loss: 207.92
 ---- batch: 040 ----
mean loss: 210.34
 ---- batch: 050 ----
mean loss: 192.71
 ---- batch: 060 ----
mean loss: 200.84
 ---- batch: 070 ----
mean loss: 199.44
 ---- batch: 080 ----
mean loss: 194.89
 ---- batch: 090 ----
mean loss: 192.60
 ---- batch: 100 ----
mean loss: 200.14
 ---- batch: 110 ----
mean loss: 192.63
train mean loss: 198.47
epoch train time: 0:00:02.597624
elapsed time: 0:07:16.498767
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 22:03:56.214978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.56
 ---- batch: 020 ----
mean loss: 197.49
 ---- batch: 030 ----
mean loss: 201.73
 ---- batch: 040 ----
mean loss: 194.98
 ---- batch: 050 ----
mean loss: 189.78
 ---- batch: 060 ----
mean loss: 204.26
 ---- batch: 070 ----
mean loss: 192.64
 ---- batch: 080 ----
mean loss: 195.98
 ---- batch: 090 ----
mean loss: 199.73
 ---- batch: 100 ----
mean loss: 203.05
 ---- batch: 110 ----
mean loss: 205.08
train mean loss: 198.14
epoch train time: 0:00:02.575862
elapsed time: 0:07:19.075053
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 22:03:58.791247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.99
 ---- batch: 020 ----
mean loss: 195.89
 ---- batch: 030 ----
mean loss: 200.71
 ---- batch: 040 ----
mean loss: 193.09
 ---- batch: 050 ----
mean loss: 194.94
 ---- batch: 060 ----
mean loss: 194.86
 ---- batch: 070 ----
mean loss: 197.46
 ---- batch: 080 ----
mean loss: 194.29
 ---- batch: 090 ----
mean loss: 191.28
 ---- batch: 100 ----
mean loss: 201.89
 ---- batch: 110 ----
mean loss: 206.77
train mean loss: 197.73
epoch train time: 0:00:02.576366
elapsed time: 0:07:21.651822
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 22:04:01.368023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.43
 ---- batch: 020 ----
mean loss: 194.79
 ---- batch: 030 ----
mean loss: 192.32
 ---- batch: 040 ----
mean loss: 202.73
 ---- batch: 050 ----
mean loss: 201.51
 ---- batch: 060 ----
mean loss: 194.23
 ---- batch: 070 ----
mean loss: 201.51
 ---- batch: 080 ----
mean loss: 196.99
 ---- batch: 090 ----
mean loss: 189.87
 ---- batch: 100 ----
mean loss: 207.16
 ---- batch: 110 ----
mean loss: 193.32
train mean loss: 197.53
epoch train time: 0:00:02.575352
elapsed time: 0:07:24.227591
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 22:04:03.943793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.76
 ---- batch: 020 ----
mean loss: 195.02
 ---- batch: 030 ----
mean loss: 190.62
 ---- batch: 040 ----
mean loss: 201.69
 ---- batch: 050 ----
mean loss: 212.51
 ---- batch: 060 ----
mean loss: 191.79
 ---- batch: 070 ----
mean loss: 191.05
 ---- batch: 080 ----
mean loss: 205.07
 ---- batch: 090 ----
mean loss: 201.11
 ---- batch: 100 ----
mean loss: 187.49
 ---- batch: 110 ----
mean loss: 193.19
train mean loss: 197.11
epoch train time: 0:00:02.569978
elapsed time: 0:07:26.798057
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 22:04:06.514276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.80
 ---- batch: 020 ----
mean loss: 199.67
 ---- batch: 030 ----
mean loss: 199.56
 ---- batch: 040 ----
mean loss: 199.27
 ---- batch: 050 ----
mean loss: 207.46
 ---- batch: 060 ----
mean loss: 196.09
 ---- batch: 070 ----
mean loss: 193.17
 ---- batch: 080 ----
mean loss: 193.53
 ---- batch: 090 ----
mean loss: 196.09
 ---- batch: 100 ----
mean loss: 203.70
 ---- batch: 110 ----
mean loss: 195.95
train mean loss: 197.02
epoch train time: 0:00:02.561632
elapsed time: 0:07:29.360183
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 22:04:09.076388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.78
 ---- batch: 020 ----
mean loss: 203.44
 ---- batch: 030 ----
mean loss: 199.59
 ---- batch: 040 ----
mean loss: 189.30
 ---- batch: 050 ----
mean loss: 208.80
 ---- batch: 060 ----
mean loss: 198.23
 ---- batch: 070 ----
mean loss: 191.45
 ---- batch: 080 ----
mean loss: 180.76
 ---- batch: 090 ----
mean loss: 194.55
 ---- batch: 100 ----
mean loss: 201.11
 ---- batch: 110 ----
mean loss: 200.78
train mean loss: 197.13
epoch train time: 0:00:02.570411
elapsed time: 0:07:31.931009
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 22:04:11.647212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.84
 ---- batch: 020 ----
mean loss: 200.79
 ---- batch: 030 ----
mean loss: 204.41
 ---- batch: 040 ----
mean loss: 206.12
 ---- batch: 050 ----
mean loss: 196.96
 ---- batch: 060 ----
mean loss: 195.54
 ---- batch: 070 ----
mean loss: 196.28
 ---- batch: 080 ----
mean loss: 195.40
 ---- batch: 090 ----
mean loss: 187.52
 ---- batch: 100 ----
mean loss: 185.25
 ---- batch: 110 ----
mean loss: 195.24
train mean loss: 196.43
epoch train time: 0:00:02.566907
elapsed time: 0:07:34.498361
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 22:04:14.214570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.32
 ---- batch: 020 ----
mean loss: 198.34
 ---- batch: 030 ----
mean loss: 192.17
 ---- batch: 040 ----
mean loss: 195.93
 ---- batch: 050 ----
mean loss: 202.66
 ---- batch: 060 ----
mean loss: 190.96
 ---- batch: 070 ----
mean loss: 208.04
 ---- batch: 080 ----
mean loss: 194.85
 ---- batch: 090 ----
mean loss: 187.81
 ---- batch: 100 ----
mean loss: 197.47
 ---- batch: 110 ----
mean loss: 197.23
train mean loss: 196.51
epoch train time: 0:00:02.563096
elapsed time: 0:07:37.061887
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 22:04:16.778110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.53
 ---- batch: 020 ----
mean loss: 186.66
 ---- batch: 030 ----
mean loss: 200.34
 ---- batch: 040 ----
mean loss: 196.71
 ---- batch: 050 ----
mean loss: 207.30
 ---- batch: 060 ----
mean loss: 195.21
 ---- batch: 070 ----
mean loss: 194.98
 ---- batch: 080 ----
mean loss: 199.53
 ---- batch: 090 ----
mean loss: 199.58
 ---- batch: 100 ----
mean loss: 192.49
 ---- batch: 110 ----
mean loss: 185.61
train mean loss: 196.36
epoch train time: 0:00:02.540129
elapsed time: 0:07:39.602445
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 22:04:19.318635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.15
 ---- batch: 020 ----
mean loss: 196.92
 ---- batch: 030 ----
mean loss: 192.81
 ---- batch: 040 ----
mean loss: 192.06
 ---- batch: 050 ----
mean loss: 187.81
 ---- batch: 060 ----
mean loss: 198.07
 ---- batch: 070 ----
mean loss: 184.79
 ---- batch: 080 ----
mean loss: 206.38
 ---- batch: 090 ----
mean loss: 202.80
 ---- batch: 100 ----
mean loss: 211.07
 ---- batch: 110 ----
mean loss: 189.36
train mean loss: 196.15
epoch train time: 0:00:02.577013
elapsed time: 0:07:42.179853
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 22:04:21.896061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.82
 ---- batch: 020 ----
mean loss: 192.86
 ---- batch: 030 ----
mean loss: 192.95
 ---- batch: 040 ----
mean loss: 189.44
 ---- batch: 050 ----
mean loss: 198.43
 ---- batch: 060 ----
mean loss: 192.48
 ---- batch: 070 ----
mean loss: 194.21
 ---- batch: 080 ----
mean loss: 190.83
 ---- batch: 090 ----
mean loss: 202.42
 ---- batch: 100 ----
mean loss: 202.71
 ---- batch: 110 ----
mean loss: 198.37
train mean loss: 195.77
epoch train time: 0:00:02.551291
elapsed time: 0:07:44.731553
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 22:04:24.447745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.25
 ---- batch: 020 ----
mean loss: 201.51
 ---- batch: 030 ----
mean loss: 196.06
 ---- batch: 040 ----
mean loss: 207.09
 ---- batch: 050 ----
mean loss: 205.56
 ---- batch: 060 ----
mean loss: 191.05
 ---- batch: 070 ----
mean loss: 184.98
 ---- batch: 080 ----
mean loss: 196.18
 ---- batch: 090 ----
mean loss: 191.81
 ---- batch: 100 ----
mean loss: 188.78
 ---- batch: 110 ----
mean loss: 192.40
train mean loss: 195.89
epoch train time: 0:00:02.577090
elapsed time: 0:07:47.309029
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 22:04:27.025239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.46
 ---- batch: 020 ----
mean loss: 195.98
 ---- batch: 030 ----
mean loss: 185.71
 ---- batch: 040 ----
mean loss: 205.96
 ---- batch: 050 ----
mean loss: 203.24
 ---- batch: 060 ----
mean loss: 197.74
 ---- batch: 070 ----
mean loss: 194.93
 ---- batch: 080 ----
mean loss: 201.34
 ---- batch: 090 ----
mean loss: 191.32
 ---- batch: 100 ----
mean loss: 197.28
 ---- batch: 110 ----
mean loss: 187.29
train mean loss: 195.62
epoch train time: 0:00:02.574735
elapsed time: 0:07:49.884193
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 22:04:29.600421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.31
 ---- batch: 020 ----
mean loss: 201.95
 ---- batch: 030 ----
mean loss: 198.99
 ---- batch: 040 ----
mean loss: 192.10
 ---- batch: 050 ----
mean loss: 190.94
 ---- batch: 060 ----
mean loss: 200.85
 ---- batch: 070 ----
mean loss: 191.25
 ---- batch: 080 ----
mean loss: 196.02
 ---- batch: 090 ----
mean loss: 199.08
 ---- batch: 100 ----
mean loss: 189.73
 ---- batch: 110 ----
mean loss: 189.90
train mean loss: 195.26
epoch train time: 0:00:02.575973
elapsed time: 0:07:52.460654
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 22:04:32.176862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.83
 ---- batch: 020 ----
mean loss: 206.89
 ---- batch: 030 ----
mean loss: 200.40
 ---- batch: 040 ----
mean loss: 192.80
 ---- batch: 050 ----
mean loss: 195.62
 ---- batch: 060 ----
mean loss: 187.45
 ---- batch: 070 ----
mean loss: 200.85
 ---- batch: 080 ----
mean loss: 193.22
 ---- batch: 090 ----
mean loss: 195.40
 ---- batch: 100 ----
mean loss: 194.43
 ---- batch: 110 ----
mean loss: 194.68
train mean loss: 195.12
epoch train time: 0:00:02.564070
elapsed time: 0:07:55.025162
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 22:04:34.741391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.03
 ---- batch: 020 ----
mean loss: 202.60
 ---- batch: 030 ----
mean loss: 194.68
 ---- batch: 040 ----
mean loss: 187.34
 ---- batch: 050 ----
mean loss: 193.19
 ---- batch: 060 ----
mean loss: 195.91
 ---- batch: 070 ----
mean loss: 187.52
 ---- batch: 080 ----
mean loss: 198.19
 ---- batch: 090 ----
mean loss: 201.04
 ---- batch: 100 ----
mean loss: 194.00
 ---- batch: 110 ----
mean loss: 199.90
train mean loss: 194.92
epoch train time: 0:00:02.548345
elapsed time: 0:07:57.573934
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 22:04:37.290126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.36
 ---- batch: 020 ----
mean loss: 194.68
 ---- batch: 030 ----
mean loss: 199.61
 ---- batch: 040 ----
mean loss: 197.35
 ---- batch: 050 ----
mean loss: 189.21
 ---- batch: 060 ----
mean loss: 192.44
 ---- batch: 070 ----
mean loss: 195.44
 ---- batch: 080 ----
mean loss: 189.77
 ---- batch: 090 ----
mean loss: 202.70
 ---- batch: 100 ----
mean loss: 185.10
 ---- batch: 110 ----
mean loss: 198.23
train mean loss: 194.87
epoch train time: 0:00:02.575001
elapsed time: 0:08:00.149507
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 22:04:39.865563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.80
 ---- batch: 020 ----
mean loss: 204.24
 ---- batch: 030 ----
mean loss: 192.33
 ---- batch: 040 ----
mean loss: 194.80
 ---- batch: 050 ----
mean loss: 199.46
 ---- batch: 060 ----
mean loss: 194.25
 ---- batch: 070 ----
mean loss: 192.93
 ---- batch: 080 ----
mean loss: 187.45
 ---- batch: 090 ----
mean loss: 191.41
 ---- batch: 100 ----
mean loss: 198.27
 ---- batch: 110 ----
mean loss: 193.29
train mean loss: 194.61
epoch train time: 0:00:02.557037
elapsed time: 0:08:02.706815
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 22:04:42.423040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.33
 ---- batch: 020 ----
mean loss: 195.32
 ---- batch: 030 ----
mean loss: 191.22
 ---- batch: 040 ----
mean loss: 192.47
 ---- batch: 050 ----
mean loss: 190.65
 ---- batch: 060 ----
mean loss: 194.25
 ---- batch: 070 ----
mean loss: 197.83
 ---- batch: 080 ----
mean loss: 190.55
 ---- batch: 090 ----
mean loss: 201.52
 ---- batch: 100 ----
mean loss: 196.06
 ---- batch: 110 ----
mean loss: 189.60
train mean loss: 194.64
epoch train time: 0:00:02.585528
elapsed time: 0:08:05.292764
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 22:04:45.008948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.69
 ---- batch: 020 ----
mean loss: 194.38
 ---- batch: 030 ----
mean loss: 182.90
 ---- batch: 040 ----
mean loss: 205.50
 ---- batch: 050 ----
mean loss: 197.52
 ---- batch: 060 ----
mean loss: 197.50
 ---- batch: 070 ----
mean loss: 195.50
 ---- batch: 080 ----
mean loss: 194.47
 ---- batch: 090 ----
mean loss: 189.76
 ---- batch: 100 ----
mean loss: 194.12
 ---- batch: 110 ----
mean loss: 191.18
train mean loss: 194.43
epoch train time: 0:00:02.576727
elapsed time: 0:08:07.869878
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 22:04:47.586116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.23
 ---- batch: 020 ----
mean loss: 203.60
 ---- batch: 030 ----
mean loss: 199.26
 ---- batch: 040 ----
mean loss: 187.04
 ---- batch: 050 ----
mean loss: 194.26
 ---- batch: 060 ----
mean loss: 192.69
 ---- batch: 070 ----
mean loss: 186.45
 ---- batch: 080 ----
mean loss: 196.23
 ---- batch: 090 ----
mean loss: 195.52
 ---- batch: 100 ----
mean loss: 196.38
 ---- batch: 110 ----
mean loss: 184.77
train mean loss: 194.23
epoch train time: 0:00:02.562908
elapsed time: 0:08:10.433227
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 22:04:50.149421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.12
 ---- batch: 020 ----
mean loss: 201.19
 ---- batch: 030 ----
mean loss: 194.23
 ---- batch: 040 ----
mean loss: 191.74
 ---- batch: 050 ----
mean loss: 188.98
 ---- batch: 060 ----
mean loss: 195.73
 ---- batch: 070 ----
mean loss: 194.48
 ---- batch: 080 ----
mean loss: 188.94
 ---- batch: 090 ----
mean loss: 199.76
 ---- batch: 100 ----
mean loss: 188.21
 ---- batch: 110 ----
mean loss: 199.82
train mean loss: 194.29
epoch train time: 0:00:02.573162
elapsed time: 0:08:13.006786
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 22:04:52.722986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.34
 ---- batch: 020 ----
mean loss: 189.04
 ---- batch: 030 ----
mean loss: 196.81
 ---- batch: 040 ----
mean loss: 198.60
 ---- batch: 050 ----
mean loss: 201.39
 ---- batch: 060 ----
mean loss: 189.85
 ---- batch: 070 ----
mean loss: 197.98
 ---- batch: 080 ----
mean loss: 192.99
 ---- batch: 090 ----
mean loss: 190.98
 ---- batch: 100 ----
mean loss: 189.72
 ---- batch: 110 ----
mean loss: 201.20
train mean loss: 194.11
epoch train time: 0:00:02.599831
elapsed time: 0:08:15.607007
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 22:04:55.323194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.79
 ---- batch: 020 ----
mean loss: 201.47
 ---- batch: 030 ----
mean loss: 190.34
 ---- batch: 040 ----
mean loss: 185.35
 ---- batch: 050 ----
mean loss: 202.17
 ---- batch: 060 ----
mean loss: 198.44
 ---- batch: 070 ----
mean loss: 194.03
 ---- batch: 080 ----
mean loss: 195.37
 ---- batch: 090 ----
mean loss: 185.64
 ---- batch: 100 ----
mean loss: 194.66
 ---- batch: 110 ----
mean loss: 197.60
train mean loss: 193.82
epoch train time: 0:00:02.576188
elapsed time: 0:08:18.183588
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 22:04:57.899779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.30
 ---- batch: 020 ----
mean loss: 194.95
 ---- batch: 030 ----
mean loss: 207.28
 ---- batch: 040 ----
mean loss: 185.04
 ---- batch: 050 ----
mean loss: 192.86
 ---- batch: 060 ----
mean loss: 182.99
 ---- batch: 070 ----
mean loss: 188.24
 ---- batch: 080 ----
mean loss: 195.33
 ---- batch: 090 ----
mean loss: 196.15
 ---- batch: 100 ----
mean loss: 197.48
 ---- batch: 110 ----
mean loss: 195.77
train mean loss: 193.82
epoch train time: 0:00:02.556836
elapsed time: 0:08:20.740836
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 22:05:00.457045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.39
 ---- batch: 020 ----
mean loss: 198.08
 ---- batch: 030 ----
mean loss: 191.91
 ---- batch: 040 ----
mean loss: 198.64
 ---- batch: 050 ----
mean loss: 192.65
 ---- batch: 060 ----
mean loss: 196.99
 ---- batch: 070 ----
mean loss: 192.24
 ---- batch: 080 ----
mean loss: 189.62
 ---- batch: 090 ----
mean loss: 195.30
 ---- batch: 100 ----
mean loss: 191.12
 ---- batch: 110 ----
mean loss: 185.39
train mean loss: 193.67
epoch train time: 0:00:02.597050
elapsed time: 0:08:23.338292
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 22:05:03.054493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.41
 ---- batch: 020 ----
mean loss: 195.33
 ---- batch: 030 ----
mean loss: 185.08
 ---- batch: 040 ----
mean loss: 198.70
 ---- batch: 050 ----
mean loss: 183.68
 ---- batch: 060 ----
mean loss: 192.85
 ---- batch: 070 ----
mean loss: 193.16
 ---- batch: 080 ----
mean loss: 195.22
 ---- batch: 090 ----
mean loss: 205.32
 ---- batch: 100 ----
mean loss: 195.40
 ---- batch: 110 ----
mean loss: 190.03
train mean loss: 193.24
epoch train time: 0:00:02.589220
elapsed time: 0:08:25.927979
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 22:05:05.644171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.34
 ---- batch: 020 ----
mean loss: 195.18
 ---- batch: 030 ----
mean loss: 197.06
 ---- batch: 040 ----
mean loss: 191.48
 ---- batch: 050 ----
mean loss: 197.59
 ---- batch: 060 ----
mean loss: 199.18
 ---- batch: 070 ----
mean loss: 192.96
 ---- batch: 080 ----
mean loss: 193.11
 ---- batch: 090 ----
mean loss: 185.70
 ---- batch: 100 ----
mean loss: 189.65
 ---- batch: 110 ----
mean loss: 198.49
train mean loss: 193.18
epoch train time: 0:00:02.653844
elapsed time: 0:08:28.582224
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 22:05:08.298457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.25
 ---- batch: 020 ----
mean loss: 182.00
 ---- batch: 030 ----
mean loss: 200.99
 ---- batch: 040 ----
mean loss: 192.14
 ---- batch: 050 ----
mean loss: 196.13
 ---- batch: 060 ----
mean loss: 195.82
 ---- batch: 070 ----
mean loss: 193.96
 ---- batch: 080 ----
mean loss: 187.03
 ---- batch: 090 ----
mean loss: 191.51
 ---- batch: 100 ----
mean loss: 198.75
 ---- batch: 110 ----
mean loss: 195.90
train mean loss: 193.29
epoch train time: 0:00:02.601107
elapsed time: 0:08:31.183811
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 22:05:10.900060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.42
 ---- batch: 020 ----
mean loss: 201.31
 ---- batch: 030 ----
mean loss: 198.26
 ---- batch: 040 ----
mean loss: 187.53
 ---- batch: 050 ----
mean loss: 191.45
 ---- batch: 060 ----
mean loss: 193.09
 ---- batch: 070 ----
mean loss: 195.01
 ---- batch: 080 ----
mean loss: 192.95
 ---- batch: 090 ----
mean loss: 186.35
 ---- batch: 100 ----
mean loss: 185.56
 ---- batch: 110 ----
mean loss: 190.77
train mean loss: 192.99
epoch train time: 0:00:02.563595
elapsed time: 0:08:33.747870
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 22:05:13.464065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.16
 ---- batch: 020 ----
mean loss: 189.16
 ---- batch: 030 ----
mean loss: 194.10
 ---- batch: 040 ----
mean loss: 195.68
 ---- batch: 050 ----
mean loss: 197.60
 ---- batch: 060 ----
mean loss: 201.84
 ---- batch: 070 ----
mean loss: 189.79
 ---- batch: 080 ----
mean loss: 185.45
 ---- batch: 090 ----
mean loss: 197.60
 ---- batch: 100 ----
mean loss: 192.10
 ---- batch: 110 ----
mean loss: 194.23
train mean loss: 193.08
epoch train time: 0:00:02.549103
elapsed time: 0:08:36.297395
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 22:05:16.013623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.16
 ---- batch: 020 ----
mean loss: 195.16
 ---- batch: 030 ----
mean loss: 201.75
 ---- batch: 040 ----
mean loss: 184.09
 ---- batch: 050 ----
mean loss: 194.10
 ---- batch: 060 ----
mean loss: 188.93
 ---- batch: 070 ----
mean loss: 193.44
 ---- batch: 080 ----
mean loss: 190.41
 ---- batch: 090 ----
mean loss: 195.43
 ---- batch: 100 ----
mean loss: 194.71
 ---- batch: 110 ----
mean loss: 191.51
train mean loss: 192.88
epoch train time: 0:00:02.579296
elapsed time: 0:08:38.877121
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 22:05:18.593312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.81
 ---- batch: 020 ----
mean loss: 192.66
 ---- batch: 030 ----
mean loss: 202.87
 ---- batch: 040 ----
mean loss: 195.32
 ---- batch: 050 ----
mean loss: 190.58
 ---- batch: 060 ----
mean loss: 191.72
 ---- batch: 070 ----
mean loss: 190.15
 ---- batch: 080 ----
mean loss: 200.89
 ---- batch: 090 ----
mean loss: 194.04
 ---- batch: 100 ----
mean loss: 184.93
 ---- batch: 110 ----
mean loss: 185.28
train mean loss: 192.82
epoch train time: 0:00:02.593574
elapsed time: 0:08:41.471103
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 22:05:21.187299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.55
 ---- batch: 020 ----
mean loss: 192.44
 ---- batch: 030 ----
mean loss: 199.99
 ---- batch: 040 ----
mean loss: 188.60
 ---- batch: 050 ----
mean loss: 184.94
 ---- batch: 060 ----
mean loss: 197.61
 ---- batch: 070 ----
mean loss: 198.85
 ---- batch: 080 ----
mean loss: 191.85
 ---- batch: 090 ----
mean loss: 188.32
 ---- batch: 100 ----
mean loss: 195.42
 ---- batch: 110 ----
mean loss: 197.91
train mean loss: 192.80
epoch train time: 0:00:02.583177
elapsed time: 0:08:44.054684
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 22:05:23.770881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.07
 ---- batch: 020 ----
mean loss: 186.19
 ---- batch: 030 ----
mean loss: 192.85
 ---- batch: 040 ----
mean loss: 193.22
 ---- batch: 050 ----
mean loss: 202.02
 ---- batch: 060 ----
mean loss: 196.44
 ---- batch: 070 ----
mean loss: 187.55
 ---- batch: 080 ----
mean loss: 185.02
 ---- batch: 090 ----
mean loss: 197.47
 ---- batch: 100 ----
mean loss: 196.25
 ---- batch: 110 ----
mean loss: 188.93
train mean loss: 192.38
epoch train time: 0:00:02.569724
elapsed time: 0:08:46.624832
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 22:05:26.341078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.00
 ---- batch: 020 ----
mean loss: 188.90
 ---- batch: 030 ----
mean loss: 188.68
 ---- batch: 040 ----
mean loss: 194.46
 ---- batch: 050 ----
mean loss: 196.10
 ---- batch: 060 ----
mean loss: 202.29
 ---- batch: 070 ----
mean loss: 185.13
 ---- batch: 080 ----
mean loss: 194.45
 ---- batch: 090 ----
mean loss: 188.44
 ---- batch: 100 ----
mean loss: 186.79
 ---- batch: 110 ----
mean loss: 190.01
train mean loss: 192.34
epoch train time: 0:00:02.605212
elapsed time: 0:08:49.230518
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 22:05:28.946712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.26
 ---- batch: 020 ----
mean loss: 194.50
 ---- batch: 030 ----
mean loss: 191.84
 ---- batch: 040 ----
mean loss: 194.42
 ---- batch: 050 ----
mean loss: 198.40
 ---- batch: 060 ----
mean loss: 183.86
 ---- batch: 070 ----
mean loss: 184.91
 ---- batch: 080 ----
mean loss: 188.65
 ---- batch: 090 ----
mean loss: 184.89
 ---- batch: 100 ----
mean loss: 200.20
 ---- batch: 110 ----
mean loss: 198.45
train mean loss: 192.17
epoch train time: 0:00:02.610837
elapsed time: 0:08:51.841779
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 22:05:31.557987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.88
 ---- batch: 020 ----
mean loss: 199.61
 ---- batch: 030 ----
mean loss: 195.99
 ---- batch: 040 ----
mean loss: 192.75
 ---- batch: 050 ----
mean loss: 185.01
 ---- batch: 060 ----
mean loss: 189.67
 ---- batch: 070 ----
mean loss: 198.43
 ---- batch: 080 ----
mean loss: 190.23
 ---- batch: 090 ----
mean loss: 196.01
 ---- batch: 100 ----
mean loss: 197.81
 ---- batch: 110 ----
mean loss: 182.43
train mean loss: 192.07
epoch train time: 0:00:02.612013
elapsed time: 0:08:54.454227
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 22:05:34.170433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.42
 ---- batch: 020 ----
mean loss: 199.86
 ---- batch: 030 ----
mean loss: 200.96
 ---- batch: 040 ----
mean loss: 194.34
 ---- batch: 050 ----
mean loss: 183.81
 ---- batch: 060 ----
mean loss: 187.10
 ---- batch: 070 ----
mean loss: 191.83
 ---- batch: 080 ----
mean loss: 190.59
 ---- batch: 090 ----
mean loss: 196.06
 ---- batch: 100 ----
mean loss: 190.67
 ---- batch: 110 ----
mean loss: 195.14
train mean loss: 192.04
epoch train time: 0:00:02.597845
elapsed time: 0:08:57.052478
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 22:05:36.768682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.29
 ---- batch: 020 ----
mean loss: 194.37
 ---- batch: 030 ----
mean loss: 188.64
 ---- batch: 040 ----
mean loss: 201.80
 ---- batch: 050 ----
mean loss: 187.08
 ---- batch: 060 ----
mean loss: 187.42
 ---- batch: 070 ----
mean loss: 192.35
 ---- batch: 080 ----
mean loss: 191.93
 ---- batch: 090 ----
mean loss: 189.62
 ---- batch: 100 ----
mean loss: 181.26
 ---- batch: 110 ----
mean loss: 198.43
train mean loss: 191.80
epoch train time: 0:00:02.595193
elapsed time: 0:08:59.648091
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 22:05:39.364281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.21
 ---- batch: 020 ----
mean loss: 188.98
 ---- batch: 030 ----
mean loss: 195.21
 ---- batch: 040 ----
mean loss: 184.48
 ---- batch: 050 ----
mean loss: 195.79
 ---- batch: 060 ----
mean loss: 186.64
 ---- batch: 070 ----
mean loss: 205.01
 ---- batch: 080 ----
mean loss: 197.50
 ---- batch: 090 ----
mean loss: 189.86
 ---- batch: 100 ----
mean loss: 189.51
 ---- batch: 110 ----
mean loss: 188.56
train mean loss: 191.75
epoch train time: 0:00:02.585481
elapsed time: 0:09:02.233965
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 22:05:41.950178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.08
 ---- batch: 020 ----
mean loss: 189.50
 ---- batch: 030 ----
mean loss: 192.09
 ---- batch: 040 ----
mean loss: 192.12
 ---- batch: 050 ----
mean loss: 190.05
 ---- batch: 060 ----
mean loss: 197.89
 ---- batch: 070 ----
mean loss: 184.57
 ---- batch: 080 ----
mean loss: 186.83
 ---- batch: 090 ----
mean loss: 190.70
 ---- batch: 100 ----
mean loss: 181.99
 ---- batch: 110 ----
mean loss: 199.00
train mean loss: 191.72
epoch train time: 0:00:02.587221
elapsed time: 0:09:04.821601
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 22:05:44.537802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.28
 ---- batch: 020 ----
mean loss: 184.49
 ---- batch: 030 ----
mean loss: 199.96
 ---- batch: 040 ----
mean loss: 184.50
 ---- batch: 050 ----
mean loss: 193.36
 ---- batch: 060 ----
mean loss: 203.11
 ---- batch: 070 ----
mean loss: 193.20
 ---- batch: 080 ----
mean loss: 190.95
 ---- batch: 090 ----
mean loss: 183.45
 ---- batch: 100 ----
mean loss: 187.89
 ---- batch: 110 ----
mean loss: 192.62
train mean loss: 191.51
epoch train time: 0:00:02.576304
elapsed time: 0:09:07.398334
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 22:05:47.114535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.16
 ---- batch: 020 ----
mean loss: 185.78
 ---- batch: 030 ----
mean loss: 190.69
 ---- batch: 040 ----
mean loss: 187.14
 ---- batch: 050 ----
mean loss: 194.90
 ---- batch: 060 ----
mean loss: 196.36
 ---- batch: 070 ----
mean loss: 191.68
 ---- batch: 080 ----
mean loss: 182.23
 ---- batch: 090 ----
mean loss: 190.76
 ---- batch: 100 ----
mean loss: 194.38
 ---- batch: 110 ----
mean loss: 199.63
train mean loss: 191.59
epoch train time: 0:00:02.564425
elapsed time: 0:09:09.963185
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 22:05:49.679395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.17
 ---- batch: 020 ----
mean loss: 187.05
 ---- batch: 030 ----
mean loss: 199.69
 ---- batch: 040 ----
mean loss: 193.08
 ---- batch: 050 ----
mean loss: 185.67
 ---- batch: 060 ----
mean loss: 194.39
 ---- batch: 070 ----
mean loss: 187.25
 ---- batch: 080 ----
mean loss: 190.45
 ---- batch: 090 ----
mean loss: 193.55
 ---- batch: 100 ----
mean loss: 185.02
 ---- batch: 110 ----
mean loss: 190.67
train mean loss: 191.01
epoch train time: 0:00:02.590936
elapsed time: 0:09:12.554859
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 22:05:52.270871
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.58
 ---- batch: 020 ----
mean loss: 186.62
 ---- batch: 030 ----
mean loss: 182.90
 ---- batch: 040 ----
mean loss: 194.12
 ---- batch: 050 ----
mean loss: 193.13
 ---- batch: 060 ----
mean loss: 193.42
 ---- batch: 070 ----
mean loss: 185.29
 ---- batch: 080 ----
mean loss: 199.22
 ---- batch: 090 ----
mean loss: 195.20
 ---- batch: 100 ----
mean loss: 191.20
 ---- batch: 110 ----
mean loss: 183.37
train mean loss: 191.01
epoch train time: 0:00:02.569993
elapsed time: 0:09:15.125069
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 22:05:54.841297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.90
 ---- batch: 020 ----
mean loss: 193.75
 ---- batch: 030 ----
mean loss: 196.92
 ---- batch: 040 ----
mean loss: 193.08
 ---- batch: 050 ----
mean loss: 193.76
 ---- batch: 060 ----
mean loss: 192.07
 ---- batch: 070 ----
mean loss: 184.03
 ---- batch: 080 ----
mean loss: 197.13
 ---- batch: 090 ----
mean loss: 188.76
 ---- batch: 100 ----
mean loss: 186.22
 ---- batch: 110 ----
mean loss: 186.19
train mean loss: 190.94
epoch train time: 0:00:02.578358
elapsed time: 0:09:17.703878
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 22:05:57.420067
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.24
 ---- batch: 020 ----
mean loss: 191.21
 ---- batch: 030 ----
mean loss: 185.86
 ---- batch: 040 ----
mean loss: 189.54
 ---- batch: 050 ----
mean loss: 193.90
 ---- batch: 060 ----
mean loss: 191.36
 ---- batch: 070 ----
mean loss: 193.83
 ---- batch: 080 ----
mean loss: 195.41
 ---- batch: 090 ----
mean loss: 194.94
 ---- batch: 100 ----
mean loss: 187.05
 ---- batch: 110 ----
mean loss: 186.33
train mean loss: 190.92
epoch train time: 0:00:02.581764
elapsed time: 0:09:20.286039
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 22:06:00.002255
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.00
 ---- batch: 020 ----
mean loss: 188.09
 ---- batch: 030 ----
mean loss: 191.50
 ---- batch: 040 ----
mean loss: 187.56
 ---- batch: 050 ----
mean loss: 192.94
 ---- batch: 060 ----
mean loss: 194.10
 ---- batch: 070 ----
mean loss: 192.44
 ---- batch: 080 ----
mean loss: 195.42
 ---- batch: 090 ----
mean loss: 185.43
 ---- batch: 100 ----
mean loss: 183.99
 ---- batch: 110 ----
mean loss: 198.75
train mean loss: 190.91
epoch train time: 0:00:02.578304
elapsed time: 0:09:22.864796
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 22:06:02.581078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.95
 ---- batch: 020 ----
mean loss: 188.61
 ---- batch: 030 ----
mean loss: 193.18
 ---- batch: 040 ----
mean loss: 192.43
 ---- batch: 050 ----
mean loss: 185.37
 ---- batch: 060 ----
mean loss: 190.90
 ---- batch: 070 ----
mean loss: 192.96
 ---- batch: 080 ----
mean loss: 196.44
 ---- batch: 090 ----
mean loss: 190.13
 ---- batch: 100 ----
mean loss: 180.49
 ---- batch: 110 ----
mean loss: 197.12
train mean loss: 190.96
epoch train time: 0:00:02.585450
elapsed time: 0:09:25.450759
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 22:06:05.166978
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.66
 ---- batch: 020 ----
mean loss: 191.60
 ---- batch: 030 ----
mean loss: 192.36
 ---- batch: 040 ----
mean loss: 200.98
 ---- batch: 050 ----
mean loss: 180.05
 ---- batch: 060 ----
mean loss: 191.97
 ---- batch: 070 ----
mean loss: 181.61
 ---- batch: 080 ----
mean loss: 196.05
 ---- batch: 090 ----
mean loss: 187.83
 ---- batch: 100 ----
mean loss: 199.87
 ---- batch: 110 ----
mean loss: 189.92
train mean loss: 190.99
epoch train time: 0:00:02.576120
elapsed time: 0:09:28.027319
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 22:06:07.743540
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.86
 ---- batch: 020 ----
mean loss: 191.06
 ---- batch: 030 ----
mean loss: 184.43
 ---- batch: 040 ----
mean loss: 186.10
 ---- batch: 050 ----
mean loss: 193.27
 ---- batch: 060 ----
mean loss: 198.02
 ---- batch: 070 ----
mean loss: 196.68
 ---- batch: 080 ----
mean loss: 196.13
 ---- batch: 090 ----
mean loss: 188.76
 ---- batch: 100 ----
mean loss: 187.18
 ---- batch: 110 ----
mean loss: 194.63
train mean loss: 190.95
epoch train time: 0:00:02.568495
elapsed time: 0:09:30.596241
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 22:06:10.312441
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.45
 ---- batch: 020 ----
mean loss: 175.25
 ---- batch: 030 ----
mean loss: 194.67
 ---- batch: 040 ----
mean loss: 186.62
 ---- batch: 050 ----
mean loss: 186.12
 ---- batch: 060 ----
mean loss: 199.51
 ---- batch: 070 ----
mean loss: 188.89
 ---- batch: 080 ----
mean loss: 190.22
 ---- batch: 090 ----
mean loss: 192.15
 ---- batch: 100 ----
mean loss: 192.10
 ---- batch: 110 ----
mean loss: 204.49
train mean loss: 190.85
epoch train time: 0:00:02.583775
elapsed time: 0:09:33.180427
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 22:06:12.896630
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.12
 ---- batch: 020 ----
mean loss: 187.31
 ---- batch: 030 ----
mean loss: 191.87
 ---- batch: 040 ----
mean loss: 196.40
 ---- batch: 050 ----
mean loss: 194.74
 ---- batch: 060 ----
mean loss: 196.85
 ---- batch: 070 ----
mean loss: 185.79
 ---- batch: 080 ----
mean loss: 184.44
 ---- batch: 090 ----
mean loss: 182.06
 ---- batch: 100 ----
mean loss: 185.10
 ---- batch: 110 ----
mean loss: 190.72
train mean loss: 190.94
epoch train time: 0:00:02.570457
elapsed time: 0:09:35.751332
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 22:06:15.467562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.42
 ---- batch: 020 ----
mean loss: 198.92
 ---- batch: 030 ----
mean loss: 192.54
 ---- batch: 040 ----
mean loss: 194.65
 ---- batch: 050 ----
mean loss: 197.36
 ---- batch: 060 ----
mean loss: 191.56
 ---- batch: 070 ----
mean loss: 180.87
 ---- batch: 080 ----
mean loss: 185.73
 ---- batch: 090 ----
mean loss: 197.33
 ---- batch: 100 ----
mean loss: 182.35
 ---- batch: 110 ----
mean loss: 188.11
train mean loss: 190.89
epoch train time: 0:00:02.551080
elapsed time: 0:09:38.302838
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 22:06:18.019060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.45
 ---- batch: 020 ----
mean loss: 188.47
 ---- batch: 030 ----
mean loss: 189.94
 ---- batch: 040 ----
mean loss: 195.84
 ---- batch: 050 ----
mean loss: 195.94
 ---- batch: 060 ----
mean loss: 193.06
 ---- batch: 070 ----
mean loss: 195.48
 ---- batch: 080 ----
mean loss: 185.88
 ---- batch: 090 ----
mean loss: 189.66
 ---- batch: 100 ----
mean loss: 190.06
 ---- batch: 110 ----
mean loss: 189.40
train mean loss: 190.79
epoch train time: 0:00:02.558702
elapsed time: 0:09:40.862004
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 22:06:20.578201
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.21
 ---- batch: 020 ----
mean loss: 188.79
 ---- batch: 030 ----
mean loss: 180.09
 ---- batch: 040 ----
mean loss: 196.61
 ---- batch: 050 ----
mean loss: 193.92
 ---- batch: 060 ----
mean loss: 195.30
 ---- batch: 070 ----
mean loss: 183.56
 ---- batch: 080 ----
mean loss: 190.64
 ---- batch: 090 ----
mean loss: 181.35
 ---- batch: 100 ----
mean loss: 194.19
 ---- batch: 110 ----
mean loss: 198.15
train mean loss: 190.80
epoch train time: 0:00:02.572833
elapsed time: 0:09:43.435236
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 22:06:23.151422
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.42
 ---- batch: 020 ----
mean loss: 189.56
 ---- batch: 030 ----
mean loss: 191.06
 ---- batch: 040 ----
mean loss: 180.32
 ---- batch: 050 ----
mean loss: 195.42
 ---- batch: 060 ----
mean loss: 188.52
 ---- batch: 070 ----
mean loss: 197.30
 ---- batch: 080 ----
mean loss: 194.95
 ---- batch: 090 ----
mean loss: 186.68
 ---- batch: 100 ----
mean loss: 191.96
 ---- batch: 110 ----
mean loss: 195.60
train mean loss: 190.85
epoch train time: 0:00:02.579949
elapsed time: 0:09:46.015582
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 22:06:25.731781
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.29
 ---- batch: 020 ----
mean loss: 197.55
 ---- batch: 030 ----
mean loss: 192.19
 ---- batch: 040 ----
mean loss: 198.33
 ---- batch: 050 ----
mean loss: 187.55
 ---- batch: 060 ----
mean loss: 184.77
 ---- batch: 070 ----
mean loss: 196.94
 ---- batch: 080 ----
mean loss: 180.50
 ---- batch: 090 ----
mean loss: 177.12
 ---- batch: 100 ----
mean loss: 187.24
 ---- batch: 110 ----
mean loss: 193.89
train mean loss: 190.83
epoch train time: 0:00:02.572263
elapsed time: 0:09:48.588254
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 22:06:28.304462
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.62
 ---- batch: 020 ----
mean loss: 192.05
 ---- batch: 030 ----
mean loss: 192.24
 ---- batch: 040 ----
mean loss: 192.40
 ---- batch: 050 ----
mean loss: 189.54
 ---- batch: 060 ----
mean loss: 189.18
 ---- batch: 070 ----
mean loss: 182.63
 ---- batch: 080 ----
mean loss: 192.68
 ---- batch: 090 ----
mean loss: 192.72
 ---- batch: 100 ----
mean loss: 193.75
 ---- batch: 110 ----
mean loss: 189.47
train mean loss: 190.86
epoch train time: 0:00:02.559315
elapsed time: 0:09:51.148029
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 22:06:30.864242
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.03
 ---- batch: 020 ----
mean loss: 188.24
 ---- batch: 030 ----
mean loss: 190.27
 ---- batch: 040 ----
mean loss: 188.33
 ---- batch: 050 ----
mean loss: 190.10
 ---- batch: 060 ----
mean loss: 190.86
 ---- batch: 070 ----
mean loss: 189.06
 ---- batch: 080 ----
mean loss: 193.43
 ---- batch: 090 ----
mean loss: 189.94
 ---- batch: 100 ----
mean loss: 202.20
 ---- batch: 110 ----
mean loss: 192.12
train mean loss: 190.88
epoch train time: 0:00:02.574509
elapsed time: 0:09:53.722950
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 22:06:33.439180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.61
 ---- batch: 020 ----
mean loss: 196.40
 ---- batch: 030 ----
mean loss: 197.12
 ---- batch: 040 ----
mean loss: 193.20
 ---- batch: 050 ----
mean loss: 188.03
 ---- batch: 060 ----
mean loss: 194.29
 ---- batch: 070 ----
mean loss: 190.90
 ---- batch: 080 ----
mean loss: 178.41
 ---- batch: 090 ----
mean loss: 189.44
 ---- batch: 100 ----
mean loss: 192.77
 ---- batch: 110 ----
mean loss: 189.12
train mean loss: 190.81
epoch train time: 0:00:02.595930
elapsed time: 0:09:56.319342
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 22:06:36.035544
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.57
 ---- batch: 020 ----
mean loss: 195.23
 ---- batch: 030 ----
mean loss: 184.48
 ---- batch: 040 ----
mean loss: 187.86
 ---- batch: 050 ----
mean loss: 193.32
 ---- batch: 060 ----
mean loss: 182.98
 ---- batch: 070 ----
mean loss: 190.72
 ---- batch: 080 ----
mean loss: 196.77
 ---- batch: 090 ----
mean loss: 192.40
 ---- batch: 100 ----
mean loss: 192.63
 ---- batch: 110 ----
mean loss: 193.37
train mean loss: 190.82
epoch train time: 0:00:02.602731
elapsed time: 0:09:58.922538
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 22:06:38.638754
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.56
 ---- batch: 020 ----
mean loss: 192.66
 ---- batch: 030 ----
mean loss: 192.76
 ---- batch: 040 ----
mean loss: 190.21
 ---- batch: 050 ----
mean loss: 197.39
 ---- batch: 060 ----
mean loss: 187.07
 ---- batch: 070 ----
mean loss: 193.57
 ---- batch: 080 ----
mean loss: 191.51
 ---- batch: 090 ----
mean loss: 192.27
 ---- batch: 100 ----
mean loss: 194.59
 ---- batch: 110 ----
mean loss: 187.37
train mean loss: 190.80
epoch train time: 0:00:02.559684
elapsed time: 0:10:01.482650
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 22:06:41.198872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.35
 ---- batch: 020 ----
mean loss: 185.58
 ---- batch: 030 ----
mean loss: 194.22
 ---- batch: 040 ----
mean loss: 199.26
 ---- batch: 050 ----
mean loss: 183.04
 ---- batch: 060 ----
mean loss: 191.32
 ---- batch: 070 ----
mean loss: 198.31
 ---- batch: 080 ----
mean loss: 194.67
 ---- batch: 090 ----
mean loss: 186.64
 ---- batch: 100 ----
mean loss: 187.46
 ---- batch: 110 ----
mean loss: 192.25
train mean loss: 190.70
epoch train time: 0:00:02.563974
elapsed time: 0:10:04.047075
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 22:06:43.763273
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.39
 ---- batch: 020 ----
mean loss: 196.17
 ---- batch: 030 ----
mean loss: 201.43
 ---- batch: 040 ----
mean loss: 185.31
 ---- batch: 050 ----
mean loss: 189.11
 ---- batch: 060 ----
mean loss: 189.15
 ---- batch: 070 ----
mean loss: 190.82
 ---- batch: 080 ----
mean loss: 186.03
 ---- batch: 090 ----
mean loss: 186.49
 ---- batch: 100 ----
mean loss: 193.11
 ---- batch: 110 ----
mean loss: 185.48
train mean loss: 190.85
epoch train time: 0:00:02.571447
elapsed time: 0:10:06.618942
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 22:06:46.335196
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.32
 ---- batch: 020 ----
mean loss: 188.81
 ---- batch: 030 ----
mean loss: 185.89
 ---- batch: 040 ----
mean loss: 195.07
 ---- batch: 050 ----
mean loss: 197.88
 ---- batch: 060 ----
mean loss: 188.61
 ---- batch: 070 ----
mean loss: 189.30
 ---- batch: 080 ----
mean loss: 188.39
 ---- batch: 090 ----
mean loss: 194.27
 ---- batch: 100 ----
mean loss: 193.02
 ---- batch: 110 ----
mean loss: 188.64
train mean loss: 190.74
epoch train time: 0:00:02.572678
elapsed time: 0:10:09.192105
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 22:06:48.908319
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.95
 ---- batch: 020 ----
mean loss: 192.65
 ---- batch: 030 ----
mean loss: 188.37
 ---- batch: 040 ----
mean loss: 193.54
 ---- batch: 050 ----
mean loss: 189.09
 ---- batch: 060 ----
mean loss: 200.31
 ---- batch: 070 ----
mean loss: 187.48
 ---- batch: 080 ----
mean loss: 196.87
 ---- batch: 090 ----
mean loss: 192.34
 ---- batch: 100 ----
mean loss: 179.29
 ---- batch: 110 ----
mean loss: 188.25
train mean loss: 190.79
epoch train time: 0:00:02.576963
elapsed time: 0:10:11.769485
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 22:06:51.485753
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.03
 ---- batch: 020 ----
mean loss: 187.44
 ---- batch: 030 ----
mean loss: 194.91
 ---- batch: 040 ----
mean loss: 198.51
 ---- batch: 050 ----
mean loss: 180.83
 ---- batch: 060 ----
mean loss: 192.99
 ---- batch: 070 ----
mean loss: 184.32
 ---- batch: 080 ----
mean loss: 185.44
 ---- batch: 090 ----
mean loss: 194.90
 ---- batch: 100 ----
mean loss: 196.71
 ---- batch: 110 ----
mean loss: 187.40
train mean loss: 190.79
epoch train time: 0:00:02.592972
elapsed time: 0:10:14.362939
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 22:06:54.079115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.69
 ---- batch: 020 ----
mean loss: 197.58
 ---- batch: 030 ----
mean loss: 194.28
 ---- batch: 040 ----
mean loss: 187.77
 ---- batch: 050 ----
mean loss: 192.26
 ---- batch: 060 ----
mean loss: 176.92
 ---- batch: 070 ----
mean loss: 196.38
 ---- batch: 080 ----
mean loss: 186.71
 ---- batch: 090 ----
mean loss: 197.81
 ---- batch: 100 ----
mean loss: 194.38
 ---- batch: 110 ----
mean loss: 189.92
train mean loss: 190.71
epoch train time: 0:00:02.564231
elapsed time: 0:10:16.927581
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 22:06:56.643795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.31
 ---- batch: 020 ----
mean loss: 189.56
 ---- batch: 030 ----
mean loss: 194.59
 ---- batch: 040 ----
mean loss: 183.34
 ---- batch: 050 ----
mean loss: 188.75
 ---- batch: 060 ----
mean loss: 194.32
 ---- batch: 070 ----
mean loss: 199.95
 ---- batch: 080 ----
mean loss: 196.27
 ---- batch: 090 ----
mean loss: 193.83
 ---- batch: 100 ----
mean loss: 192.71
 ---- batch: 110 ----
mean loss: 184.65
train mean loss: 190.69
epoch train time: 0:00:02.592696
elapsed time: 0:10:19.520699
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 22:06:59.236931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.25
 ---- batch: 020 ----
mean loss: 186.19
 ---- batch: 030 ----
mean loss: 188.02
 ---- batch: 040 ----
mean loss: 192.92
 ---- batch: 050 ----
mean loss: 195.95
 ---- batch: 060 ----
mean loss: 191.96
 ---- batch: 070 ----
mean loss: 197.73
 ---- batch: 080 ----
mean loss: 192.04
 ---- batch: 090 ----
mean loss: 186.01
 ---- batch: 100 ----
mean loss: 186.66
 ---- batch: 110 ----
mean loss: 191.24
train mean loss: 190.73
epoch train time: 0:00:02.572298
elapsed time: 0:10:22.093439
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 22:07:01.809646
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.49
 ---- batch: 020 ----
mean loss: 194.77
 ---- batch: 030 ----
mean loss: 199.65
 ---- batch: 040 ----
mean loss: 192.58
 ---- batch: 050 ----
mean loss: 189.59
 ---- batch: 060 ----
mean loss: 190.09
 ---- batch: 070 ----
mean loss: 188.41
 ---- batch: 080 ----
mean loss: 181.27
 ---- batch: 090 ----
mean loss: 194.99
 ---- batch: 100 ----
mean loss: 192.93
 ---- batch: 110 ----
mean loss: 189.01
train mean loss: 190.67
epoch train time: 0:00:02.576847
elapsed time: 0:10:24.670759
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 22:07:04.387049
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.21
 ---- batch: 020 ----
mean loss: 186.65
 ---- batch: 030 ----
mean loss: 184.09
 ---- batch: 040 ----
mean loss: 204.21
 ---- batch: 050 ----
mean loss: 185.35
 ---- batch: 060 ----
mean loss: 189.49
 ---- batch: 070 ----
mean loss: 190.53
 ---- batch: 080 ----
mean loss: 191.31
 ---- batch: 090 ----
mean loss: 188.10
 ---- batch: 100 ----
mean loss: 182.97
 ---- batch: 110 ----
mean loss: 195.75
train mean loss: 190.72
epoch train time: 0:00:02.580411
elapsed time: 0:10:27.251663
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 22:07:06.967868
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.48
 ---- batch: 020 ----
mean loss: 181.64
 ---- batch: 030 ----
mean loss: 182.09
 ---- batch: 040 ----
mean loss: 199.11
 ---- batch: 050 ----
mean loss: 193.48
 ---- batch: 060 ----
mean loss: 183.60
 ---- batch: 070 ----
mean loss: 192.30
 ---- batch: 080 ----
mean loss: 200.98
 ---- batch: 090 ----
mean loss: 187.14
 ---- batch: 100 ----
mean loss: 199.80
 ---- batch: 110 ----
mean loss: 191.90
train mean loss: 190.64
epoch train time: 0:00:02.605539
elapsed time: 0:10:29.857625
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 22:07:09.573858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.71
 ---- batch: 020 ----
mean loss: 185.40
 ---- batch: 030 ----
mean loss: 194.68
 ---- batch: 040 ----
mean loss: 191.51
 ---- batch: 050 ----
mean loss: 180.90
 ---- batch: 060 ----
mean loss: 196.91
 ---- batch: 070 ----
mean loss: 193.26
 ---- batch: 080 ----
mean loss: 194.72
 ---- batch: 090 ----
mean loss: 191.53
 ---- batch: 100 ----
mean loss: 186.80
 ---- batch: 110 ----
mean loss: 191.59
train mean loss: 190.55
epoch train time: 0:00:02.604659
elapsed time: 0:10:32.462744
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 22:07:12.178936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.79
 ---- batch: 020 ----
mean loss: 182.19
 ---- batch: 030 ----
mean loss: 189.33
 ---- batch: 040 ----
mean loss: 192.18
 ---- batch: 050 ----
mean loss: 188.38
 ---- batch: 060 ----
mean loss: 201.00
 ---- batch: 070 ----
mean loss: 193.47
 ---- batch: 080 ----
mean loss: 192.55
 ---- batch: 090 ----
mean loss: 187.51
 ---- batch: 100 ----
mean loss: 193.53
 ---- batch: 110 ----
mean loss: 188.29
train mean loss: 190.60
epoch train time: 0:00:02.607542
elapsed time: 0:10:35.070889
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 22:07:14.786920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.88
 ---- batch: 020 ----
mean loss: 189.41
 ---- batch: 030 ----
mean loss: 181.64
 ---- batch: 040 ----
mean loss: 191.52
 ---- batch: 050 ----
mean loss: 186.76
 ---- batch: 060 ----
mean loss: 190.76
 ---- batch: 070 ----
mean loss: 187.00
 ---- batch: 080 ----
mean loss: 198.69
 ---- batch: 090 ----
mean loss: 196.44
 ---- batch: 100 ----
mean loss: 192.08
 ---- batch: 110 ----
mean loss: 191.25
train mean loss: 190.59
epoch train time: 0:00:02.610043
elapsed time: 0:10:37.681176
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 22:07:17.397372
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.70
 ---- batch: 020 ----
mean loss: 185.15
 ---- batch: 030 ----
mean loss: 184.49
 ---- batch: 040 ----
mean loss: 198.46
 ---- batch: 050 ----
mean loss: 185.89
 ---- batch: 060 ----
mean loss: 195.50
 ---- batch: 070 ----
mean loss: 193.82
 ---- batch: 080 ----
mean loss: 184.51
 ---- batch: 090 ----
mean loss: 190.63
 ---- batch: 100 ----
mean loss: 195.04
 ---- batch: 110 ----
mean loss: 188.47
train mean loss: 190.63
epoch train time: 0:00:02.593906
elapsed time: 0:10:40.275484
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 22:07:19.991705
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.66
 ---- batch: 020 ----
mean loss: 191.63
 ---- batch: 030 ----
mean loss: 184.02
 ---- batch: 040 ----
mean loss: 195.79
 ---- batch: 050 ----
mean loss: 191.03
 ---- batch: 060 ----
mean loss: 202.55
 ---- batch: 070 ----
mean loss: 193.13
 ---- batch: 080 ----
mean loss: 185.75
 ---- batch: 090 ----
mean loss: 189.51
 ---- batch: 100 ----
mean loss: 186.79
 ---- batch: 110 ----
mean loss: 186.99
train mean loss: 190.64
epoch train time: 0:00:02.574924
elapsed time: 0:10:42.850840
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 22:07:22.566887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.47
 ---- batch: 020 ----
mean loss: 200.44
 ---- batch: 030 ----
mean loss: 189.72
 ---- batch: 040 ----
mean loss: 185.29
 ---- batch: 050 ----
mean loss: 192.67
 ---- batch: 060 ----
mean loss: 193.15
 ---- batch: 070 ----
mean loss: 180.81
 ---- batch: 080 ----
mean loss: 185.06
 ---- batch: 090 ----
mean loss: 195.73
 ---- batch: 100 ----
mean loss: 192.70
 ---- batch: 110 ----
mean loss: 185.00
train mean loss: 190.56
epoch train time: 0:00:02.570976
elapsed time: 0:10:45.422069
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 22:07:25.138320
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.83
 ---- batch: 020 ----
mean loss: 193.16
 ---- batch: 030 ----
mean loss: 188.20
 ---- batch: 040 ----
mean loss: 192.65
 ---- batch: 050 ----
mean loss: 189.46
 ---- batch: 060 ----
mean loss: 190.33
 ---- batch: 070 ----
mean loss: 201.10
 ---- batch: 080 ----
mean loss: 186.39
 ---- batch: 090 ----
mean loss: 183.38
 ---- batch: 100 ----
mean loss: 197.32
 ---- batch: 110 ----
mean loss: 187.46
train mean loss: 190.59
epoch train time: 0:00:02.561054
elapsed time: 0:10:47.983573
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 22:07:27.699753
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.66
 ---- batch: 020 ----
mean loss: 192.68
 ---- batch: 030 ----
mean loss: 193.49
 ---- batch: 040 ----
mean loss: 196.16
 ---- batch: 050 ----
mean loss: 185.26
 ---- batch: 060 ----
mean loss: 199.01
 ---- batch: 070 ----
mean loss: 192.45
 ---- batch: 080 ----
mean loss: 200.16
 ---- batch: 090 ----
mean loss: 183.87
 ---- batch: 100 ----
mean loss: 189.93
 ---- batch: 110 ----
mean loss: 181.94
train mean loss: 190.60
epoch train time: 0:00:02.566394
elapsed time: 0:10:50.550360
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 22:07:30.266541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.42
 ---- batch: 020 ----
mean loss: 186.77
 ---- batch: 030 ----
mean loss: 195.21
 ---- batch: 040 ----
mean loss: 180.58
 ---- batch: 050 ----
mean loss: 190.10
 ---- batch: 060 ----
mean loss: 191.79
 ---- batch: 070 ----
mean loss: 193.63
 ---- batch: 080 ----
mean loss: 189.95
 ---- batch: 090 ----
mean loss: 201.17
 ---- batch: 100 ----
mean loss: 188.82
 ---- batch: 110 ----
mean loss: 192.95
train mean loss: 190.62
epoch train time: 0:00:02.587201
elapsed time: 0:10:53.137985
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 22:07:32.854192
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.59
 ---- batch: 020 ----
mean loss: 189.09
 ---- batch: 030 ----
mean loss: 188.71
 ---- batch: 040 ----
mean loss: 189.11
 ---- batch: 050 ----
mean loss: 193.49
 ---- batch: 060 ----
mean loss: 194.37
 ---- batch: 070 ----
mean loss: 193.52
 ---- batch: 080 ----
mean loss: 192.04
 ---- batch: 090 ----
mean loss: 192.72
 ---- batch: 100 ----
mean loss: 199.61
 ---- batch: 110 ----
mean loss: 184.71
train mean loss: 190.51
epoch train time: 0:00:02.570975
elapsed time: 0:10:55.709380
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 22:07:35.425570
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.85
 ---- batch: 020 ----
mean loss: 207.06
 ---- batch: 030 ----
mean loss: 195.25
 ---- batch: 040 ----
mean loss: 183.18
 ---- batch: 050 ----
mean loss: 185.03
 ---- batch: 060 ----
mean loss: 191.26
 ---- batch: 070 ----
mean loss: 194.24
 ---- batch: 080 ----
mean loss: 175.97
 ---- batch: 090 ----
mean loss: 188.89
 ---- batch: 100 ----
mean loss: 196.75
 ---- batch: 110 ----
mean loss: 189.03
train mean loss: 190.55
epoch train time: 0:00:02.558845
elapsed time: 0:10:58.268622
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 22:07:37.984804
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.11
 ---- batch: 020 ----
mean loss: 196.28
 ---- batch: 030 ----
mean loss: 184.71
 ---- batch: 040 ----
mean loss: 201.84
 ---- batch: 050 ----
mean loss: 194.15
 ---- batch: 060 ----
mean loss: 185.56
 ---- batch: 070 ----
mean loss: 196.24
 ---- batch: 080 ----
mean loss: 184.89
 ---- batch: 090 ----
mean loss: 182.53
 ---- batch: 100 ----
mean loss: 196.18
 ---- batch: 110 ----
mean loss: 188.00
train mean loss: 190.54
epoch train time: 0:00:02.550906
elapsed time: 0:11:00.819906
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 22:07:40.536159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.80
 ---- batch: 020 ----
mean loss: 192.34
 ---- batch: 030 ----
mean loss: 190.27
 ---- batch: 040 ----
mean loss: 189.66
 ---- batch: 050 ----
mean loss: 189.60
 ---- batch: 060 ----
mean loss: 185.37
 ---- batch: 070 ----
mean loss: 195.16
 ---- batch: 080 ----
mean loss: 197.22
 ---- batch: 090 ----
mean loss: 189.40
 ---- batch: 100 ----
mean loss: 193.65
 ---- batch: 110 ----
mean loss: 194.67
train mean loss: 190.51
epoch train time: 0:00:02.575065
elapsed time: 0:11:03.395453
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 22:07:43.111650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.31
 ---- batch: 020 ----
mean loss: 188.18
 ---- batch: 030 ----
mean loss: 193.40
 ---- batch: 040 ----
mean loss: 188.98
 ---- batch: 050 ----
mean loss: 187.85
 ---- batch: 060 ----
mean loss: 194.41
 ---- batch: 070 ----
mean loss: 193.23
 ---- batch: 080 ----
mean loss: 186.23
 ---- batch: 090 ----
mean loss: 187.71
 ---- batch: 100 ----
mean loss: 195.81
 ---- batch: 110 ----
mean loss: 195.65
train mean loss: 190.55
epoch train time: 0:00:02.562335
elapsed time: 0:11:05.958213
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 22:07:45.674419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.70
 ---- batch: 020 ----
mean loss: 191.11
 ---- batch: 030 ----
mean loss: 190.14
 ---- batch: 040 ----
mean loss: 189.96
 ---- batch: 050 ----
mean loss: 191.04
 ---- batch: 060 ----
mean loss: 187.43
 ---- batch: 070 ----
mean loss: 193.28
 ---- batch: 080 ----
mean loss: 187.63
 ---- batch: 090 ----
mean loss: 188.14
 ---- batch: 100 ----
mean loss: 189.23
 ---- batch: 110 ----
mean loss: 191.25
train mean loss: 190.52
epoch train time: 0:00:02.577980
elapsed time: 0:11:08.536654
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 22:07:48.252901
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.78
 ---- batch: 020 ----
mean loss: 194.30
 ---- batch: 030 ----
mean loss: 192.49
 ---- batch: 040 ----
mean loss: 192.99
 ---- batch: 050 ----
mean loss: 189.38
 ---- batch: 060 ----
mean loss: 191.77
 ---- batch: 070 ----
mean loss: 193.88
 ---- batch: 080 ----
mean loss: 186.00
 ---- batch: 090 ----
mean loss: 188.90
 ---- batch: 100 ----
mean loss: 196.02
 ---- batch: 110 ----
mean loss: 196.02
train mean loss: 190.48
epoch train time: 0:00:02.586890
elapsed time: 0:11:11.124012
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 22:07:50.840213
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.66
 ---- batch: 020 ----
mean loss: 190.15
 ---- batch: 030 ----
mean loss: 189.03
 ---- batch: 040 ----
mean loss: 190.40
 ---- batch: 050 ----
mean loss: 187.56
 ---- batch: 060 ----
mean loss: 186.22
 ---- batch: 070 ----
mean loss: 184.34
 ---- batch: 080 ----
mean loss: 191.84
 ---- batch: 090 ----
mean loss: 193.52
 ---- batch: 100 ----
mean loss: 199.72
 ---- batch: 110 ----
mean loss: 191.39
train mean loss: 190.42
epoch train time: 0:00:02.583776
elapsed time: 0:11:13.708254
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 22:07:53.424449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.51
 ---- batch: 020 ----
mean loss: 191.63
 ---- batch: 030 ----
mean loss: 187.83
 ---- batch: 040 ----
mean loss: 189.98
 ---- batch: 050 ----
mean loss: 185.61
 ---- batch: 060 ----
mean loss: 197.36
 ---- batch: 070 ----
mean loss: 189.38
 ---- batch: 080 ----
mean loss: 190.18
 ---- batch: 090 ----
mean loss: 190.48
 ---- batch: 100 ----
mean loss: 195.99
 ---- batch: 110 ----
mean loss: 190.23
train mean loss: 190.47
epoch train time: 0:00:02.578303
elapsed time: 0:11:16.290854
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_8/checkpoint.pth.tar
**** end time: 2019-09-26 22:07:56.006836 ****
