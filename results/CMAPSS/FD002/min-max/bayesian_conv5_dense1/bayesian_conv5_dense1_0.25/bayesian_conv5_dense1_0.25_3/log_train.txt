Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_3', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.25, resume=False, step_size=200, visualize_step=50)
pid: 15020
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 13:43:47.069890 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 13:43:47.086709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1630.11
 ---- batch: 020 ----
mean loss: 1226.87
train mean loss: 1393.63
epoch train time: 0:00:11.615931
elapsed time: 0:00:11.641038
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 13:43:58.710969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1125.18
 ---- batch: 020 ----
mean loss: 1095.81
train mean loss: 1099.62
epoch train time: 0:00:04.184429
elapsed time: 0:00:15.825945
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 13:44:02.895982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1054.48
 ---- batch: 020 ----
mean loss: 1044.69
train mean loss: 1043.32
epoch train time: 0:00:04.215695
elapsed time: 0:00:20.042208
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 13:44:07.112209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1011.69
 ---- batch: 020 ----
mean loss: 1019.66
train mean loss: 1015.26
epoch train time: 0:00:04.204831
elapsed time: 0:00:24.247565
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 13:44:11.317625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 982.82
 ---- batch: 020 ----
mean loss: 1013.61
train mean loss: 993.21
epoch train time: 0:00:04.203425
elapsed time: 0:00:28.451570
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 13:44:15.521580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1002.12
 ---- batch: 020 ----
mean loss: 962.67
train mean loss: 978.69
epoch train time: 0:00:04.211325
elapsed time: 0:00:32.663414
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 13:44:19.733422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 969.25
 ---- batch: 020 ----
mean loss: 994.48
train mean loss: 978.61
epoch train time: 0:00:04.219475
elapsed time: 0:00:36.883429
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 13:44:23.953439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.14
 ---- batch: 020 ----
mean loss: 959.62
train mean loss: 963.35
epoch train time: 0:00:04.316740
elapsed time: 0:00:41.200967
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 13:44:28.270981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 961.48
 ---- batch: 020 ----
mean loss: 969.19
train mean loss: 966.08
epoch train time: 0:00:04.226767
elapsed time: 0:00:45.428324
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 13:44:32.498365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 956.98
 ---- batch: 020 ----
mean loss: 944.87
train mean loss: 951.77
epoch train time: 0:00:04.268994
elapsed time: 0:00:49.697935
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 13:44:36.767973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.97
 ---- batch: 020 ----
mean loss: 926.07
train mean loss: 941.22
epoch train time: 0:00:04.223271
elapsed time: 0:00:53.921851
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 13:44:40.991854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.09
 ---- batch: 020 ----
mean loss: 934.42
train mean loss: 943.32
epoch train time: 0:00:04.267645
elapsed time: 0:00:58.190033
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 13:44:45.260057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.07
 ---- batch: 020 ----
mean loss: 945.15
train mean loss: 940.12
epoch train time: 0:00:04.237611
elapsed time: 0:01:02.428204
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 13:44:49.498222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.42
 ---- batch: 020 ----
mean loss: 956.28
train mean loss: 947.19
epoch train time: 0:00:04.251768
elapsed time: 0:01:06.680597
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 13:44:53.750610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.19
 ---- batch: 020 ----
mean loss: 931.29
train mean loss: 940.25
epoch train time: 0:00:04.281310
elapsed time: 0:01:10.962837
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 13:44:58.032864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.90
 ---- batch: 020 ----
mean loss: 941.17
train mean loss: 940.38
epoch train time: 0:00:04.256362
elapsed time: 0:01:15.219809
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 13:45:02.289828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.60
 ---- batch: 020 ----
mean loss: 946.66
train mean loss: 935.41
epoch train time: 0:00:04.260980
elapsed time: 0:01:19.481437
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 13:45:06.551451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.92
 ---- batch: 020 ----
mean loss: 939.22
train mean loss: 939.37
epoch train time: 0:00:04.268819
elapsed time: 0:01:23.750842
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 13:45:10.820859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.05
 ---- batch: 020 ----
mean loss: 939.71
train mean loss: 927.18
epoch train time: 0:00:04.258144
elapsed time: 0:01:28.009540
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 13:45:15.079562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.17
 ---- batch: 020 ----
mean loss: 914.10
train mean loss: 931.66
epoch train time: 0:00:04.283949
elapsed time: 0:01:32.294089
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 13:45:19.364137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.57
 ---- batch: 020 ----
mean loss: 938.05
train mean loss: 925.06
epoch train time: 0:00:04.210449
elapsed time: 0:01:36.505191
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 13:45:23.575236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.50
 ---- batch: 020 ----
mean loss: 920.60
train mean loss: 931.44
epoch train time: 0:00:04.199076
elapsed time: 0:01:40.704891
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 13:45:27.774904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.34
 ---- batch: 020 ----
mean loss: 929.94
train mean loss: 923.50
epoch train time: 0:00:04.209008
elapsed time: 0:01:44.914437
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 13:45:31.984441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.89
 ---- batch: 020 ----
mean loss: 930.12
train mean loss: 931.32
epoch train time: 0:00:04.218709
elapsed time: 0:01:49.133687
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 13:45:36.203703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.15
 ---- batch: 020 ----
mean loss: 927.86
train mean loss: 923.44
epoch train time: 0:00:04.209646
elapsed time: 0:01:53.343974
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 13:45:40.414001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.83
 ---- batch: 020 ----
mean loss: 931.40
train mean loss: 925.39
epoch train time: 0:00:04.206941
elapsed time: 0:01:57.551515
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 13:45:44.621526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.53
 ---- batch: 020 ----
mean loss: 933.33
train mean loss: 927.86
epoch train time: 0:00:04.204568
elapsed time: 0:02:01.756657
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 13:45:48.826684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.95
 ---- batch: 020 ----
mean loss: 927.41
train mean loss: 925.17
epoch train time: 0:00:04.222586
elapsed time: 0:02:05.979920
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 13:45:53.049960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.63
 ---- batch: 020 ----
mean loss: 926.82
train mean loss: 920.15
epoch train time: 0:00:04.219833
elapsed time: 0:02:10.200450
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 13:45:57.270498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.87
 ---- batch: 020 ----
mean loss: 914.93
train mean loss: 921.64
epoch train time: 0:00:04.225172
elapsed time: 0:02:14.426226
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 13:46:01.496239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.77
 ---- batch: 020 ----
mean loss: 907.24
train mean loss: 924.84
epoch train time: 0:00:04.223087
elapsed time: 0:02:18.649867
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 13:46:05.719880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.85
 ---- batch: 020 ----
mean loss: 932.72
train mean loss: 926.46
epoch train time: 0:00:04.228357
elapsed time: 0:02:22.878799
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 13:46:09.948836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.49
 ---- batch: 020 ----
mean loss: 912.83
train mean loss: 916.87
epoch train time: 0:00:04.220495
elapsed time: 0:02:27.099952
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 13:46:14.169961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.26
 ---- batch: 020 ----
mean loss: 919.71
train mean loss: 921.50
epoch train time: 0:00:04.208392
elapsed time: 0:02:31.308934
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 13:46:18.378990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.27
 ---- batch: 020 ----
mean loss: 903.55
train mean loss: 911.81
epoch train time: 0:00:04.226029
elapsed time: 0:02:35.535580
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 13:46:22.605635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.32
 ---- batch: 020 ----
mean loss: 929.83
train mean loss: 915.44
epoch train time: 0:00:04.218414
elapsed time: 0:02:39.754700
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 13:46:26.824729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.40
 ---- batch: 020 ----
mean loss: 915.64
train mean loss: 913.31
epoch train time: 0:00:04.213563
elapsed time: 0:02:43.968860
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 13:46:31.038867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.23
 ---- batch: 020 ----
mean loss: 914.76
train mean loss: 913.38
epoch train time: 0:00:04.215719
elapsed time: 0:02:48.185135
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 13:46:35.255153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.11
 ---- batch: 020 ----
mean loss: 895.26
train mean loss: 916.67
epoch train time: 0:00:04.201385
elapsed time: 0:02:52.387053
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 13:46:39.457092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.27
 ---- batch: 020 ----
mean loss: 922.68
train mean loss: 912.75
epoch train time: 0:00:04.200669
elapsed time: 0:02:56.588384
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 13:46:43.658413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 938.01
 ---- batch: 020 ----
mean loss: 904.17
train mean loss: 920.73
epoch train time: 0:00:04.208220
elapsed time: 0:03:00.797188
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 13:46:47.867227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.43
 ---- batch: 020 ----
mean loss: 919.88
train mean loss: 915.97
epoch train time: 0:00:04.204887
elapsed time: 0:03:05.002653
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 13:46:52.072669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.07
 ---- batch: 020 ----
mean loss: 923.94
train mean loss: 912.98
epoch train time: 0:00:04.207432
elapsed time: 0:03:09.210641
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 13:46:56.280640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.34
 ---- batch: 020 ----
mean loss: 895.22
train mean loss: 908.98
epoch train time: 0:00:04.215275
elapsed time: 0:03:13.426477
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 13:47:00.496504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.46
 ---- batch: 020 ----
mean loss: 899.48
train mean loss: 904.54
epoch train time: 0:00:04.235185
elapsed time: 0:03:17.662246
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 13:47:04.732267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.58
 ---- batch: 020 ----
mean loss: 904.98
train mean loss: 906.43
epoch train time: 0:00:04.287694
elapsed time: 0:03:21.950888
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 13:47:09.020913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.89
 ---- batch: 020 ----
mean loss: 913.69
train mean loss: 913.92
epoch train time: 0:00:04.270243
elapsed time: 0:03:26.221719
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 13:47:13.291720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.31
 ---- batch: 020 ----
mean loss: 896.91
train mean loss: 904.11
epoch train time: 0:00:04.256811
elapsed time: 0:03:30.479072
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 13:47:17.549089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.32
 ---- batch: 020 ----
mean loss: 917.30
train mean loss: 909.09
epoch train time: 0:00:04.200570
elapsed time: 0:03:34.680224
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 13:47:21.750226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.51
 ---- batch: 020 ----
mean loss: 900.75
train mean loss: 907.57
epoch train time: 0:00:04.215359
elapsed time: 0:03:38.896153
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 13:47:25.966156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.71
 ---- batch: 020 ----
mean loss: 904.31
train mean loss: 899.16
epoch train time: 0:00:04.224100
elapsed time: 0:03:43.120811
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 13:47:30.190850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.61
 ---- batch: 020 ----
mean loss: 902.89
train mean loss: 908.72
epoch train time: 0:00:04.223959
elapsed time: 0:03:47.345408
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 13:47:34.415480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.88
 ---- batch: 020 ----
mean loss: 908.61
train mean loss: 909.53
epoch train time: 0:00:04.204292
elapsed time: 0:03:51.550317
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 13:47:38.620340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.23
 ---- batch: 020 ----
mean loss: 901.81
train mean loss: 907.99
epoch train time: 0:00:04.193442
elapsed time: 0:03:55.744344
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 13:47:42.814381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.88
 ---- batch: 020 ----
mean loss: 925.54
train mean loss: 903.01
epoch train time: 0:00:04.199086
elapsed time: 0:03:59.944084
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 13:47:47.014094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.21
 ---- batch: 020 ----
mean loss: 909.90
train mean loss: 907.86
epoch train time: 0:00:04.210759
elapsed time: 0:04:04.155496
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 13:47:51.225511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.24
 ---- batch: 020 ----
mean loss: 894.21
train mean loss: 898.47
epoch train time: 0:00:04.202761
elapsed time: 0:04:08.358821
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 13:47:55.428824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.04
 ---- batch: 020 ----
mean loss: 918.27
train mean loss: 897.13
epoch train time: 0:00:04.209851
elapsed time: 0:04:12.569194
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 13:47:59.639206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.69
 ---- batch: 020 ----
mean loss: 907.18
train mean loss: 897.97
epoch train time: 0:00:04.208155
elapsed time: 0:04:16.777931
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 13:48:03.847966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.01
 ---- batch: 020 ----
mean loss: 897.26
train mean loss: 898.90
epoch train time: 0:00:04.198791
elapsed time: 0:04:20.977314
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 13:48:08.047332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 904.18
 ---- batch: 020 ----
mean loss: 899.54
train mean loss: 902.14
epoch train time: 0:00:04.189347
elapsed time: 0:04:25.167201
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 13:48:12.237234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.29
 ---- batch: 020 ----
mean loss: 891.93
train mean loss: 901.30
epoch train time: 0:00:04.201555
elapsed time: 0:04:29.369360
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 13:48:16.439371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.23
 ---- batch: 020 ----
mean loss: 890.25
train mean loss: 895.90
epoch train time: 0:00:04.203066
elapsed time: 0:04:33.573058
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 13:48:20.643108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.93
 ---- batch: 020 ----
mean loss: 906.58
train mean loss: 900.05
epoch train time: 0:00:04.218204
elapsed time: 0:04:37.791867
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 13:48:24.861883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 904.40
 ---- batch: 020 ----
mean loss: 890.56
train mean loss: 897.21
epoch train time: 0:00:04.201253
elapsed time: 0:04:41.993712
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 13:48:29.063735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.78
 ---- batch: 020 ----
mean loss: 881.13
train mean loss: 892.40
epoch train time: 0:00:04.184080
elapsed time: 0:04:46.178321
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 13:48:33.248370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.58
 ---- batch: 020 ----
mean loss: 901.21
train mean loss: 895.12
epoch train time: 0:00:04.144991
elapsed time: 0:04:50.323917
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 13:48:37.393987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.99
 ---- batch: 020 ----
mean loss: 892.45
train mean loss: 892.13
epoch train time: 0:00:04.139609
elapsed time: 0:04:54.464106
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 13:48:41.534121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.84
 ---- batch: 020 ----
mean loss: 898.65
train mean loss: 888.76
epoch train time: 0:00:04.128039
elapsed time: 0:04:58.592651
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 13:48:45.662666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.83
 ---- batch: 020 ----
mean loss: 900.55
train mean loss: 885.41
epoch train time: 0:00:04.126813
elapsed time: 0:05:02.720040
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 13:48:49.790065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.40
 ---- batch: 020 ----
mean loss: 909.84
train mean loss: 891.41
epoch train time: 0:00:04.127445
elapsed time: 0:05:06.848292
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 13:48:53.918311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.35
 ---- batch: 020 ----
mean loss: 877.82
train mean loss: 883.62
epoch train time: 0:00:04.117457
elapsed time: 0:05:10.966298
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 13:48:58.036321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.24
 ---- batch: 020 ----
mean loss: 876.73
train mean loss: 883.06
epoch train time: 0:00:04.127833
elapsed time: 0:05:15.094646
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 13:49:02.164657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.41
 ---- batch: 020 ----
mean loss: 891.95
train mean loss: 877.37
epoch train time: 0:00:04.147177
elapsed time: 0:05:19.242365
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 13:49:06.312388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.86
 ---- batch: 020 ----
mean loss: 884.54
train mean loss: 875.96
epoch train time: 0:00:04.130457
elapsed time: 0:05:23.373426
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 13:49:10.443443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.93
 ---- batch: 020 ----
mean loss: 851.82
train mean loss: 868.07
epoch train time: 0:00:04.142416
elapsed time: 0:05:27.516374
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 13:49:14.586393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.02
 ---- batch: 020 ----
mean loss: 862.71
train mean loss: 852.34
epoch train time: 0:00:04.131929
elapsed time: 0:05:31.648864
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 13:49:18.718936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.13
 ---- batch: 020 ----
mean loss: 845.10
train mean loss: 832.14
epoch train time: 0:00:04.134034
elapsed time: 0:05:35.783484
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 13:49:22.853522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 814.19
 ---- batch: 020 ----
mean loss: 772.70
train mean loss: 788.97
epoch train time: 0:00:04.108456
elapsed time: 0:05:39.892601
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 13:49:26.962623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 756.87
 ---- batch: 020 ----
mean loss: 718.72
train mean loss: 734.67
epoch train time: 0:00:04.128074
elapsed time: 0:05:44.021212
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 13:49:31.091215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.60
 ---- batch: 020 ----
mean loss: 692.04
train mean loss: 693.15
epoch train time: 0:00:04.118198
elapsed time: 0:05:48.139984
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 13:49:35.210045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 672.76
 ---- batch: 020 ----
mean loss: 666.28
train mean loss: 666.83
epoch train time: 0:00:04.107150
elapsed time: 0:05:52.247729
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 13:49:39.317766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 662.45
 ---- batch: 020 ----
mean loss: 647.41
train mean loss: 651.96
epoch train time: 0:00:04.085151
elapsed time: 0:05:56.333387
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 13:49:43.403438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 626.54
 ---- batch: 020 ----
mean loss: 635.87
train mean loss: 633.49
epoch train time: 0:00:04.108035
elapsed time: 0:06:00.441951
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 13:49:47.511947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 618.05
 ---- batch: 020 ----
mean loss: 611.33
train mean loss: 615.42
epoch train time: 0:00:04.117182
elapsed time: 0:06:04.559599
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 13:49:51.629645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 600.03
 ---- batch: 020 ----
mean loss: 589.66
train mean loss: 595.66
epoch train time: 0:00:04.132664
elapsed time: 0:06:08.692827
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 13:49:55.762843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 591.30
 ---- batch: 020 ----
mean loss: 564.35
train mean loss: 576.55
epoch train time: 0:00:04.143819
elapsed time: 0:06:12.837255
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 13:49:59.907267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 575.90
 ---- batch: 020 ----
mean loss: 558.45
train mean loss: 566.00
epoch train time: 0:00:04.146992
elapsed time: 0:06:16.984780
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 13:50:04.054786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 557.51
 ---- batch: 020 ----
mean loss: 548.90
train mean loss: 550.73
epoch train time: 0:00:04.123563
elapsed time: 0:06:21.108927
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 13:50:08.178946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 547.92
 ---- batch: 020 ----
mean loss: 526.31
train mean loss: 535.42
epoch train time: 0:00:04.117066
elapsed time: 0:06:25.226624
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 13:50:12.296653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.44
 ---- batch: 020 ----
mean loss: 516.40
train mean loss: 514.70
epoch train time: 0:00:04.122254
elapsed time: 0:06:29.349441
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 13:50:16.419470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.30
 ---- batch: 020 ----
mean loss: 515.19
train mean loss: 507.29
epoch train time: 0:00:04.121328
elapsed time: 0:06:33.471307
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 13:50:20.541318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.13
 ---- batch: 020 ----
mean loss: 489.10
train mean loss: 489.36
epoch train time: 0:00:04.135452
elapsed time: 0:06:37.607320
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 13:50:24.677331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.07
 ---- batch: 020 ----
mean loss: 478.28
train mean loss: 481.30
epoch train time: 0:00:04.124216
elapsed time: 0:06:41.732137
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 13:50:28.802159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.49
 ---- batch: 020 ----
mean loss: 472.86
train mean loss: 474.14
epoch train time: 0:00:04.140537
elapsed time: 0:06:45.873245
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 13:50:32.943251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.50
 ---- batch: 020 ----
mean loss: 463.78
train mean loss: 466.30
epoch train time: 0:00:04.121759
elapsed time: 0:06:49.995531
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 13:50:37.065550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.89
 ---- batch: 020 ----
mean loss: 473.86
train mean loss: 465.18
epoch train time: 0:00:04.118421
elapsed time: 0:06:54.114466
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 13:50:41.184487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.43
 ---- batch: 020 ----
mean loss: 461.72
train mean loss: 456.76
epoch train time: 0:00:04.143811
elapsed time: 0:06:58.258826
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 13:50:45.328925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.49
 ---- batch: 020 ----
mean loss: 441.99
train mean loss: 446.28
epoch train time: 0:00:04.108137
elapsed time: 0:07:02.367608
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 13:50:49.437661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.56
 ---- batch: 020 ----
mean loss: 432.11
train mean loss: 430.95
epoch train time: 0:00:04.114615
elapsed time: 0:07:06.482747
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 13:50:53.552744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.71
 ---- batch: 020 ----
mean loss: 436.51
train mean loss: 431.42
epoch train time: 0:00:04.129887
elapsed time: 0:07:10.613078
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 13:50:57.683091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.00
 ---- batch: 020 ----
mean loss: 430.68
train mean loss: 428.06
epoch train time: 0:00:04.118464
elapsed time: 0:07:14.732125
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 13:51:01.802140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.03
 ---- batch: 020 ----
mean loss: 419.93
train mean loss: 427.43
epoch train time: 0:00:04.120381
elapsed time: 0:07:18.853101
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 13:51:05.923122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.80
 ---- batch: 020 ----
mean loss: 416.79
train mean loss: 412.40
epoch train time: 0:00:04.132630
elapsed time: 0:07:22.986373
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 13:51:10.056390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.88
 ---- batch: 020 ----
mean loss: 407.17
train mean loss: 407.55
epoch train time: 0:00:04.150724
elapsed time: 0:07:27.137683
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 13:51:14.207718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.14
 ---- batch: 020 ----
mean loss: 402.36
train mean loss: 403.77
epoch train time: 0:00:04.160646
elapsed time: 0:07:31.298905
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 13:51:18.368925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.85
 ---- batch: 020 ----
mean loss: 385.58
train mean loss: 395.05
epoch train time: 0:00:04.238318
elapsed time: 0:07:35.537911
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 13:51:22.607855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.78
 ---- batch: 020 ----
mean loss: 402.60
train mean loss: 401.49
epoch train time: 0:00:04.219809
elapsed time: 0:07:39.758205
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 13:51:26.828220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.72
 ---- batch: 020 ----
mean loss: 400.45
train mean loss: 404.36
epoch train time: 0:00:04.233328
elapsed time: 0:07:43.992104
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 13:51:31.062146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.89
 ---- batch: 020 ----
mean loss: 394.91
train mean loss: 390.52
epoch train time: 0:00:04.212328
elapsed time: 0:07:48.204991
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 13:51:35.275011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.04
 ---- batch: 020 ----
mean loss: 385.48
train mean loss: 387.22
epoch train time: 0:00:04.232124
elapsed time: 0:07:52.437676
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 13:51:39.507699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.22
 ---- batch: 020 ----
mean loss: 374.33
train mean loss: 377.94
epoch train time: 0:00:04.235550
elapsed time: 0:07:56.673806
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 13:51:43.743816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.70
 ---- batch: 020 ----
mean loss: 381.68
train mean loss: 385.49
epoch train time: 0:00:04.228883
elapsed time: 0:08:00.903229
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 13:51:47.973259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.71
 ---- batch: 020 ----
mean loss: 369.20
train mean loss: 373.01
epoch train time: 0:00:04.234756
elapsed time: 0:08:05.138619
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 13:51:52.208638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.81
 ---- batch: 020 ----
mean loss: 374.03
train mean loss: 380.37
epoch train time: 0:00:04.211664
elapsed time: 0:08:09.350929
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 13:51:56.420968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.69
 ---- batch: 020 ----
mean loss: 374.06
train mean loss: 374.09
epoch train time: 0:00:04.259451
elapsed time: 0:08:13.610968
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 13:52:00.681054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.43
 ---- batch: 020 ----
mean loss: 373.61
train mean loss: 368.45
epoch train time: 0:00:04.224639
elapsed time: 0:08:17.836264
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 13:52:04.906271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.45
 ---- batch: 020 ----
mean loss: 353.83
train mean loss: 356.78
epoch train time: 0:00:04.241705
elapsed time: 0:08:22.078563
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 13:52:09.148619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.40
 ---- batch: 020 ----
mean loss: 351.68
train mean loss: 360.34
epoch train time: 0:00:04.248637
elapsed time: 0:08:26.327794
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 13:52:13.397805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.27
 ---- batch: 020 ----
mean loss: 348.43
train mean loss: 361.01
epoch train time: 0:00:04.243016
elapsed time: 0:08:30.571450
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 13:52:17.641475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.52
 ---- batch: 020 ----
mean loss: 356.45
train mean loss: 355.76
epoch train time: 0:00:04.293019
elapsed time: 0:08:34.865246
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 13:52:21.935314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.38
 ---- batch: 020 ----
mean loss: 345.74
train mean loss: 355.92
epoch train time: 0:00:04.321802
elapsed time: 0:08:39.187662
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 13:52:26.257705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.23
 ---- batch: 020 ----
mean loss: 355.24
train mean loss: 353.60
epoch train time: 0:00:04.248878
elapsed time: 0:08:43.437156
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 13:52:30.507169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.08
 ---- batch: 020 ----
mean loss: 352.47
train mean loss: 352.51
epoch train time: 0:00:04.256928
elapsed time: 0:08:47.694733
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 13:52:34.764777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.52
 ---- batch: 020 ----
mean loss: 356.13
train mean loss: 351.66
epoch train time: 0:00:04.270258
elapsed time: 0:08:51.965621
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 13:52:39.035643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.12
 ---- batch: 020 ----
mean loss: 347.01
train mean loss: 344.94
epoch train time: 0:00:04.277659
elapsed time: 0:08:56.244116
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 13:52:43.314160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.99
 ---- batch: 020 ----
mean loss: 344.31
train mean loss: 342.80
epoch train time: 0:00:04.232013
elapsed time: 0:09:00.476798
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 13:52:47.546735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.35
 ---- batch: 020 ----
mean loss: 344.77
train mean loss: 342.91
epoch train time: 0:00:04.195632
elapsed time: 0:09:04.672905
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 13:52:51.742916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.94
 ---- batch: 020 ----
mean loss: 336.31
train mean loss: 339.47
epoch train time: 0:00:04.194100
elapsed time: 0:09:08.867582
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 13:52:55.937665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.35
 ---- batch: 020 ----
mean loss: 341.51
train mean loss: 341.55
epoch train time: 0:00:04.199205
elapsed time: 0:09:13.067410
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 13:53:00.137439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.72
 ---- batch: 020 ----
mean loss: 331.89
train mean loss: 334.05
epoch train time: 0:00:04.209348
elapsed time: 0:09:17.277334
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 13:53:04.347380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.02
 ---- batch: 020 ----
mean loss: 339.02
train mean loss: 343.56
epoch train time: 0:00:04.207487
elapsed time: 0:09:21.485412
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 13:53:08.555427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.06
 ---- batch: 020 ----
mean loss: 334.73
train mean loss: 332.51
epoch train time: 0:00:04.198569
elapsed time: 0:09:25.684489
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 13:53:12.754502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.76
 ---- batch: 020 ----
mean loss: 322.13
train mean loss: 325.81
epoch train time: 0:00:04.207065
elapsed time: 0:09:29.892207
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 13:53:16.962242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.41
 ---- batch: 020 ----
mean loss: 324.86
train mean loss: 325.88
epoch train time: 0:00:04.189767
elapsed time: 0:09:34.082551
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 13:53:21.152574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.16
 ---- batch: 020 ----
mean loss: 325.17
train mean loss: 321.51
epoch train time: 0:00:04.197156
elapsed time: 0:09:38.280231
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 13:53:25.350245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.51
 ---- batch: 020 ----
mean loss: 313.56
train mean loss: 322.77
epoch train time: 0:00:04.204917
elapsed time: 0:09:42.485736
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 13:53:29.555757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.00
 ---- batch: 020 ----
mean loss: 314.86
train mean loss: 322.20
epoch train time: 0:00:04.200635
elapsed time: 0:09:46.686937
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 13:53:33.756977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.45
 ---- batch: 020 ----
mean loss: 318.14
train mean loss: 321.17
epoch train time: 0:00:04.220988
elapsed time: 0:09:50.908551
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 13:53:37.978572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.62
 ---- batch: 020 ----
mean loss: 328.64
train mean loss: 318.68
epoch train time: 0:00:04.208034
elapsed time: 0:09:55.117141
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 13:53:42.187156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.28
 ---- batch: 020 ----
mean loss: 316.64
train mean loss: 316.59
epoch train time: 0:00:04.219757
elapsed time: 0:09:59.337500
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 13:53:46.407511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.10
 ---- batch: 020 ----
mean loss: 331.66
train mean loss: 323.70
epoch train time: 0:00:04.239244
elapsed time: 0:10:03.577304
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 13:53:50.647318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.59
 ---- batch: 020 ----
mean loss: 315.44
train mean loss: 318.66
epoch train time: 0:00:04.227022
elapsed time: 0:10:07.804999
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 13:53:54.875023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.23
 ---- batch: 020 ----
mean loss: 307.72
train mean loss: 311.64
epoch train time: 0:00:04.228751
elapsed time: 0:10:12.034310
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 13:53:59.104347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.71
 ---- batch: 020 ----
mean loss: 312.14
train mean loss: 309.55
epoch train time: 0:00:04.219645
elapsed time: 0:10:16.254535
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 13:54:03.324542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.52
 ---- batch: 020 ----
mean loss: 316.84
train mean loss: 312.72
epoch train time: 0:00:04.218148
elapsed time: 0:10:20.473223
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 13:54:07.543244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.69
 ---- batch: 020 ----
mean loss: 307.72
train mean loss: 306.03
epoch train time: 0:00:04.218377
elapsed time: 0:10:24.692147
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 13:54:11.762155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.75
 ---- batch: 020 ----
mean loss: 306.32
train mean loss: 310.58
epoch train time: 0:00:04.218863
elapsed time: 0:10:28.911649
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 13:54:15.981662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.89
 ---- batch: 020 ----
mean loss: 302.78
train mean loss: 303.64
epoch train time: 0:00:04.225120
elapsed time: 0:10:33.137410
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 13:54:20.207357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.64
 ---- batch: 020 ----
mean loss: 308.94
train mean loss: 309.48
epoch train time: 0:00:04.220379
elapsed time: 0:10:37.358269
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 13:54:24.428327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.40
 ---- batch: 020 ----
mean loss: 309.27
train mean loss: 301.30
epoch train time: 0:00:04.203783
elapsed time: 0:10:41.562642
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 13:54:28.632655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.41
 ---- batch: 020 ----
mean loss: 300.07
train mean loss: 305.03
epoch train time: 0:00:04.177901
elapsed time: 0:10:45.741098
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 13:54:32.811115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.94
 ---- batch: 020 ----
mean loss: 304.45
train mean loss: 300.95
epoch train time: 0:00:04.146787
elapsed time: 0:10:49.888426
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 13:54:36.958435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.08
 ---- batch: 020 ----
mean loss: 297.20
train mean loss: 300.19
epoch train time: 0:00:04.147827
elapsed time: 0:10:54.036775
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 13:54:41.106786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.92
 ---- batch: 020 ----
mean loss: 301.43
train mean loss: 302.22
epoch train time: 0:00:04.117995
elapsed time: 0:10:58.155282
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 13:54:45.225290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.45
 ---- batch: 020 ----
mean loss: 297.11
train mean loss: 297.57
epoch train time: 0:00:04.137995
elapsed time: 0:11:02.293814
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 13:54:49.363864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.97
 ---- batch: 020 ----
mean loss: 296.64
train mean loss: 293.26
epoch train time: 0:00:04.144225
elapsed time: 0:11:06.438575
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 13:54:53.508583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.66
 ---- batch: 020 ----
mean loss: 296.84
train mean loss: 294.45
epoch train time: 0:00:04.128485
elapsed time: 0:11:10.567612
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 13:54:57.637668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.30
 ---- batch: 020 ----
mean loss: 292.88
train mean loss: 292.72
epoch train time: 0:00:04.134852
elapsed time: 0:11:14.703004
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 13:55:01.773002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.93
 ---- batch: 020 ----
mean loss: 286.10
train mean loss: 291.95
epoch train time: 0:00:04.139816
elapsed time: 0:11:18.843397
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 13:55:05.913420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.02
 ---- batch: 020 ----
mean loss: 286.88
train mean loss: 292.11
epoch train time: 0:00:04.122658
elapsed time: 0:11:22.966602
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 13:55:10.036609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.49
 ---- batch: 020 ----
mean loss: 297.17
train mean loss: 291.19
epoch train time: 0:00:04.155981
elapsed time: 0:11:27.123078
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 13:55:14.193084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.75
 ---- batch: 020 ----
mean loss: 286.22
train mean loss: 290.51
epoch train time: 0:00:04.148561
elapsed time: 0:11:31.272271
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 13:55:18.342280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.13
 ---- batch: 020 ----
mean loss: 284.27
train mean loss: 288.15
epoch train time: 0:00:04.137052
elapsed time: 0:11:35.409831
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 13:55:22.479835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.99
 ---- batch: 020 ----
mean loss: 283.94
train mean loss: 287.49
epoch train time: 0:00:04.134678
elapsed time: 0:11:39.545055
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 13:55:26.615062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.52
 ---- batch: 020 ----
mean loss: 287.29
train mean loss: 286.84
epoch train time: 0:00:04.127992
elapsed time: 0:11:43.673575
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 13:55:30.743609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.41
 ---- batch: 020 ----
mean loss: 293.87
train mean loss: 288.85
epoch train time: 0:00:04.125470
elapsed time: 0:11:47.799597
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 13:55:34.869619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.74
 ---- batch: 020 ----
mean loss: 285.49
train mean loss: 288.53
epoch train time: 0:00:04.117016
elapsed time: 0:11:51.917228
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 13:55:38.987251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.25
 ---- batch: 020 ----
mean loss: 288.57
train mean loss: 286.13
epoch train time: 0:00:04.103139
elapsed time: 0:11:56.020907
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 13:55:43.090910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.42
 ---- batch: 020 ----
mean loss: 288.72
train mean loss: 289.14
epoch train time: 0:00:04.120735
elapsed time: 0:12:00.142132
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 13:55:47.212139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.36
 ---- batch: 020 ----
mean loss: 289.19
train mean loss: 285.97
epoch train time: 0:00:04.120389
elapsed time: 0:12:04.263040
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 13:55:51.333058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.66
 ---- batch: 020 ----
mean loss: 283.34
train mean loss: 282.02
epoch train time: 0:00:04.136597
elapsed time: 0:12:08.400156
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 13:55:55.470167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.05
 ---- batch: 020 ----
mean loss: 280.74
train mean loss: 281.53
epoch train time: 0:00:04.135613
elapsed time: 0:12:12.536402
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 13:55:59.606409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.18
 ---- batch: 020 ----
mean loss: 277.55
train mean loss: 279.73
epoch train time: 0:00:04.136924
elapsed time: 0:12:16.673927
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 13:56:03.743874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.94
 ---- batch: 020 ----
mean loss: 278.58
train mean loss: 279.03
epoch train time: 0:00:04.135712
elapsed time: 0:12:20.810163
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 13:56:07.880205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.98
 ---- batch: 020 ----
mean loss: 274.41
train mean loss: 279.97
epoch train time: 0:00:04.127183
elapsed time: 0:12:24.937979
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 13:56:12.008001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.59
 ---- batch: 020 ----
mean loss: 280.10
train mean loss: 282.25
epoch train time: 0:00:04.127294
elapsed time: 0:12:29.065870
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 13:56:16.135876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.18
 ---- batch: 020 ----
mean loss: 276.18
train mean loss: 279.81
epoch train time: 0:00:04.122139
elapsed time: 0:12:33.188630
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 13:56:20.258643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.88
 ---- batch: 020 ----
mean loss: 273.31
train mean loss: 278.07
epoch train time: 0:00:04.113360
elapsed time: 0:12:37.302522
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 13:56:24.372526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.98
 ---- batch: 020 ----
mean loss: 275.64
train mean loss: 274.15
epoch train time: 0:00:04.137273
elapsed time: 0:12:41.440335
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 13:56:28.510357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.43
 ---- batch: 020 ----
mean loss: 276.27
train mean loss: 277.73
epoch train time: 0:00:04.139586
elapsed time: 0:12:45.580462
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 13:56:32.650480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.15
 ---- batch: 020 ----
mean loss: 269.81
train mean loss: 276.26
epoch train time: 0:00:04.110367
elapsed time: 0:12:49.691397
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 13:56:36.761403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.98
 ---- batch: 020 ----
mean loss: 271.31
train mean loss: 272.53
epoch train time: 0:00:04.110758
elapsed time: 0:12:53.802714
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 13:56:40.872727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.51
 ---- batch: 020 ----
mean loss: 274.52
train mean loss: 273.96
epoch train time: 0:00:04.128178
elapsed time: 0:12:57.931450
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 13:56:45.001481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.81
 ---- batch: 020 ----
mean loss: 272.21
train mean loss: 270.35
epoch train time: 0:00:04.126239
elapsed time: 0:13:02.058282
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 13:56:49.128288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.84
 ---- batch: 020 ----
mean loss: 281.20
train mean loss: 273.84
epoch train time: 0:00:04.128190
elapsed time: 0:13:06.186983
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 13:56:53.257009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.96
 ---- batch: 020 ----
mean loss: 265.07
train mean loss: 269.36
epoch train time: 0:00:04.128567
elapsed time: 0:13:10.316078
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 13:56:57.386085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.45
 ---- batch: 020 ----
mean loss: 279.06
train mean loss: 272.65
epoch train time: 0:00:04.137848
elapsed time: 0:13:14.454472
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 13:57:01.524477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.47
 ---- batch: 020 ----
mean loss: 276.76
train mean loss: 269.40
epoch train time: 0:00:04.123711
elapsed time: 0:13:18.578744
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 13:57:05.648746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.62
 ---- batch: 020 ----
mean loss: 265.05
train mean loss: 270.72
epoch train time: 0:00:04.105373
elapsed time: 0:13:22.684647
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 13:57:09.754652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.58
 ---- batch: 020 ----
mean loss: 275.45
train mean loss: 273.80
epoch train time: 0:00:04.143661
elapsed time: 0:13:26.828851
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 13:57:13.898902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.28
 ---- batch: 020 ----
mean loss: 268.16
train mean loss: 268.87
epoch train time: 0:00:04.163367
elapsed time: 0:13:30.993047
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 13:57:18.063076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.97
 ---- batch: 020 ----
mean loss: 269.04
train mean loss: 267.18
epoch train time: 0:00:04.135385
elapsed time: 0:13:35.128940
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 13:57:22.198944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.61
 ---- batch: 020 ----
mean loss: 269.11
train mean loss: 268.39
epoch train time: 0:00:04.107593
elapsed time: 0:13:39.237028
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 13:57:26.307048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.45
 ---- batch: 020 ----
mean loss: 267.07
train mean loss: 271.21
epoch train time: 0:00:04.151262
elapsed time: 0:13:43.388925
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 13:57:30.458942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.83
 ---- batch: 020 ----
mean loss: 264.70
train mean loss: 266.94
epoch train time: 0:00:04.132432
elapsed time: 0:13:47.521941
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 13:57:34.591959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.54
 ---- batch: 020 ----
mean loss: 262.54
train mean loss: 264.50
epoch train time: 0:00:04.136552
elapsed time: 0:13:51.659048
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 13:57:38.729059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.10
 ---- batch: 020 ----
mean loss: 264.38
train mean loss: 263.52
epoch train time: 0:00:04.106849
elapsed time: 0:13:55.766462
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 13:57:42.836475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.07
 ---- batch: 020 ----
mean loss: 256.94
train mean loss: 262.28
epoch train time: 0:00:04.100009
elapsed time: 0:13:59.867099
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 13:57:46.937117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.41
 ---- batch: 020 ----
mean loss: 266.92
train mean loss: 263.98
epoch train time: 0:00:04.116985
elapsed time: 0:14:03.984621
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 13:57:51.054629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.29
 ---- batch: 020 ----
mean loss: 265.99
train mean loss: 266.14
epoch train time: 0:00:04.113999
elapsed time: 0:14:08.099221
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 13:57:55.169238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.72
 ---- batch: 020 ----
mean loss: 266.85
train mean loss: 261.48
epoch train time: 0:00:04.125588
elapsed time: 0:14:12.225521
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 13:57:59.295473
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.80
 ---- batch: 020 ----
mean loss: 267.79
train mean loss: 260.64
epoch train time: 0:00:04.120844
elapsed time: 0:14:16.346844
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 13:58:03.416853
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.60
 ---- batch: 020 ----
mean loss: 260.97
train mean loss: 261.66
epoch train time: 0:00:04.105962
elapsed time: 0:14:20.453344
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 13:58:07.523351
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.32
 ---- batch: 020 ----
mean loss: 258.62
train mean loss: 259.28
epoch train time: 0:00:04.090838
elapsed time: 0:14:24.544681
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 13:58:11.614695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.06
 ---- batch: 020 ----
mean loss: 266.37
train mean loss: 261.55
epoch train time: 0:00:04.104837
elapsed time: 0:14:28.650037
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 13:58:15.720048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.87
 ---- batch: 020 ----
mean loss: 254.20
train mean loss: 260.06
epoch train time: 0:00:04.100543
elapsed time: 0:14:32.751125
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 13:58:19.821184
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.34
 ---- batch: 020 ----
mean loss: 263.64
train mean loss: 260.16
epoch train time: 0:00:04.126801
elapsed time: 0:14:36.878471
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 13:58:23.948498
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.25
 ---- batch: 020 ----
mean loss: 253.87
train mean loss: 258.10
epoch train time: 0:00:04.124150
elapsed time: 0:14:41.003161
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 13:58:28.073185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.02
 ---- batch: 020 ----
mean loss: 261.11
train mean loss: 260.38
epoch train time: 0:00:04.108275
elapsed time: 0:14:45.111984
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 13:58:32.182003
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.86
 ---- batch: 020 ----
mean loss: 259.47
train mean loss: 257.80
epoch train time: 0:00:04.119309
elapsed time: 0:14:49.231808
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 13:58:36.301816
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.46
 ---- batch: 020 ----
mean loss: 253.69
train mean loss: 258.26
epoch train time: 0:00:04.119838
elapsed time: 0:14:53.352121
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 13:58:40.422125
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 263.26
 ---- batch: 020 ----
mean loss: 255.04
train mean loss: 258.80
epoch train time: 0:00:04.091809
elapsed time: 0:14:57.444477
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 13:58:44.514478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.30
 ---- batch: 020 ----
mean loss: 259.81
train mean loss: 259.49
epoch train time: 0:00:04.099794
elapsed time: 0:15:01.544898
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 13:58:48.614946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.44
 ---- batch: 020 ----
mean loss: 255.33
train mean loss: 258.66
epoch train time: 0:00:04.108485
elapsed time: 0:15:05.653939
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 13:58:52.723964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.03
 ---- batch: 020 ----
mean loss: 258.35
train mean loss: 260.70
epoch train time: 0:00:04.087456
elapsed time: 0:15:09.741997
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 13:58:56.812018
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.72
 ---- batch: 020 ----
mean loss: 266.61
train mean loss: 260.07
epoch train time: 0:00:04.087380
elapsed time: 0:15:13.829911
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 13:59:00.899931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.28
 ---- batch: 020 ----
mean loss: 257.91
train mean loss: 258.92
epoch train time: 0:00:04.086953
elapsed time: 0:15:17.917517
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 13:59:04.987535
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.79
 ---- batch: 020 ----
mean loss: 259.04
train mean loss: 259.94
epoch train time: 0:00:04.093061
elapsed time: 0:15:22.011250
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 13:59:09.081259
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.59
 ---- batch: 020 ----
mean loss: 260.54
train mean loss: 260.44
epoch train time: 0:00:04.082503
elapsed time: 0:15:26.094285
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 13:59:13.164296
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.88
 ---- batch: 020 ----
mean loss: 261.30
train mean loss: 257.89
epoch train time: 0:00:04.100172
elapsed time: 0:15:30.194961
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 13:59:17.264966
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.38
 ---- batch: 020 ----
mean loss: 263.10
train mean loss: 260.28
epoch train time: 0:00:04.088559
elapsed time: 0:15:34.284063
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 13:59:21.354106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.57
 ---- batch: 020 ----
mean loss: 256.77
train mean loss: 259.05
epoch train time: 0:00:04.072867
elapsed time: 0:15:38.357472
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 13:59:25.427481
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.54
 ---- batch: 020 ----
mean loss: 256.75
train mean loss: 258.29
epoch train time: 0:00:04.096705
elapsed time: 0:15:42.454786
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 13:59:29.524804
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.16
 ---- batch: 020 ----
mean loss: 259.06
train mean loss: 257.97
epoch train time: 0:00:04.109176
elapsed time: 0:15:46.564465
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 13:59:33.634463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.85
 ---- batch: 020 ----
mean loss: 255.46
train mean loss: 259.28
epoch train time: 0:00:04.087346
elapsed time: 0:15:50.652333
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 13:59:37.722336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.46
 ---- batch: 020 ----
mean loss: 255.92
train mean loss: 258.66
epoch train time: 0:00:04.096896
elapsed time: 0:15:54.749806
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 13:59:41.819822
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.64
 ---- batch: 020 ----
mean loss: 262.68
train mean loss: 260.08
epoch train time: 0:00:04.092921
elapsed time: 0:15:58.843246
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 13:59:45.913251
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.37
 ---- batch: 020 ----
mean loss: 260.67
train mean loss: 258.33
epoch train time: 0:00:04.095202
elapsed time: 0:16:02.938975
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 13:59:50.008982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.95
 ---- batch: 020 ----
mean loss: 256.66
train mean loss: 259.84
epoch train time: 0:00:04.105925
elapsed time: 0:16:07.045471
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 13:59:54.115498
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.29
 ---- batch: 020 ----
mean loss: 258.15
train mean loss: 257.66
epoch train time: 0:00:04.103676
elapsed time: 0:16:11.149696
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 13:59:58.219706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.48
 ---- batch: 020 ----
mean loss: 265.06
train mean loss: 255.73
epoch train time: 0:00:04.104250
elapsed time: 0:16:15.254485
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 14:00:02.324483
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.24
 ---- batch: 020 ----
mean loss: 258.83
train mean loss: 258.32
epoch train time: 0:00:04.109844
elapsed time: 0:16:19.364983
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 14:00:06.435103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.38
 ---- batch: 020 ----
mean loss: 257.58
train mean loss: 257.27
epoch train time: 0:00:04.101310
elapsed time: 0:16:23.467164
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 14:00:10.537126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.81
 ---- batch: 020 ----
mean loss: 257.08
train mean loss: 257.94
epoch train time: 0:00:04.096079
elapsed time: 0:16:27.563690
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 14:00:14.633720
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.68
 ---- batch: 020 ----
mean loss: 250.88
train mean loss: 258.17
epoch train time: 0:00:04.087947
elapsed time: 0:16:31.652190
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 14:00:18.722203
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.29
 ---- batch: 020 ----
mean loss: 250.79
train mean loss: 259.33
epoch train time: 0:00:04.095934
elapsed time: 0:16:35.748725
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 14:00:22.818755
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.62
 ---- batch: 020 ----
mean loss: 254.95
train mean loss: 256.10
epoch train time: 0:00:04.098177
elapsed time: 0:16:39.847629
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 14:00:26.917739
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 247.26
 ---- batch: 020 ----
mean loss: 260.20
train mean loss: 255.80
epoch train time: 0:00:04.110478
elapsed time: 0:16:43.958735
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 14:00:31.028741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.71
 ---- batch: 020 ----
mean loss: 260.10
train mean loss: 258.71
epoch train time: 0:00:04.107133
elapsed time: 0:16:48.066425
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 14:00:35.136439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.08
 ---- batch: 020 ----
mean loss: 261.99
train mean loss: 258.43
epoch train time: 0:00:04.099795
elapsed time: 0:16:52.166747
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 14:00:39.236756
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.33
 ---- batch: 020 ----
mean loss: 257.52
train mean loss: 258.03
epoch train time: 0:00:04.119128
elapsed time: 0:16:56.286390
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 14:00:43.356386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.74
 ---- batch: 020 ----
mean loss: 259.98
train mean loss: 257.30
epoch train time: 0:00:04.083567
elapsed time: 0:17:00.370562
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 14:00:47.440618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.47
 ---- batch: 020 ----
mean loss: 254.89
train mean loss: 256.83
epoch train time: 0:00:04.095826
elapsed time: 0:17:04.467074
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 14:00:51.537107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.14
 ---- batch: 020 ----
mean loss: 255.14
train mean loss: 257.92
epoch train time: 0:00:04.115052
elapsed time: 0:17:08.582636
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 14:00:55.652641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.02
 ---- batch: 020 ----
mean loss: 262.83
train mean loss: 259.49
epoch train time: 0:00:04.097643
elapsed time: 0:17:12.680823
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 14:00:59.750899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.03
 ---- batch: 020 ----
mean loss: 258.34
train mean loss: 257.67
epoch train time: 0:00:04.126914
elapsed time: 0:17:16.808389
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 14:01:03.878500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.88
 ---- batch: 020 ----
mean loss: 253.82
train mean loss: 255.74
epoch train time: 0:00:04.092080
elapsed time: 0:17:20.901118
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 14:01:07.971120
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.63
 ---- batch: 020 ----
mean loss: 260.89
train mean loss: 257.61
epoch train time: 0:00:04.111216
elapsed time: 0:17:25.012840
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 14:01:12.082852
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.27
 ---- batch: 020 ----
mean loss: 253.38
train mean loss: 256.19
epoch train time: 0:00:04.098528
elapsed time: 0:17:29.120863
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_3/checkpoint.pth.tar
**** end time: 2019-09-25 14:01:16.190787 ****
