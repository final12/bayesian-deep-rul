Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.25, resume=False, step_size=200, visualize_step=50)
pid: 10337
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 18:22:01.320292 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 18:22:01.339167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2790.38
 ---- batch: 020 ----
mean loss: 1409.29
train mean loss: 1823.08
epoch train time: 0:00:11.466131
elapsed time: 0:00:11.493673
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 18:22:12.814021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1170.49
 ---- batch: 020 ----
mean loss: 1110.43
train mean loss: 1132.20
epoch train time: 0:00:03.993134
elapsed time: 0:00:15.488005
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 18:22:16.808575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1083.73
 ---- batch: 020 ----
mean loss: 1051.45
train mean loss: 1060.13
epoch train time: 0:00:03.997707
elapsed time: 0:00:19.487177
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 18:22:20.807714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1015.83
 ---- batch: 020 ----
mean loss: 1013.43
train mean loss: 1013.92
epoch train time: 0:00:03.980262
elapsed time: 0:00:23.468823
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 18:22:24.789346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 985.72
 ---- batch: 020 ----
mean loss: 1021.60
train mean loss: 990.95
epoch train time: 0:00:03.992433
elapsed time: 0:00:27.462599
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 18:22:28.783122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 971.24
 ---- batch: 020 ----
mean loss: 968.38
train mean loss: 969.04
epoch train time: 0:00:04.009465
elapsed time: 0:00:31.473474
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 18:22:32.794046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.27
 ---- batch: 020 ----
mean loss: 975.42
train mean loss: 962.51
epoch train time: 0:00:03.999397
elapsed time: 0:00:35.474379
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 18:22:36.794914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.20
 ---- batch: 020 ----
mean loss: 959.23
train mean loss: 946.08
epoch train time: 0:00:03.989883
elapsed time: 0:00:39.465640
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 18:22:40.786175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 938.18
 ---- batch: 020 ----
mean loss: 922.62
train mean loss: 937.95
epoch train time: 0:00:03.984970
elapsed time: 0:00:43.451892
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 18:22:44.772406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.95
 ---- batch: 020 ----
mean loss: 933.18
train mean loss: 929.79
epoch train time: 0:00:03.978928
elapsed time: 0:00:47.432174
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 18:22:48.752706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.49
 ---- batch: 020 ----
mean loss: 929.62
train mean loss: 921.98
epoch train time: 0:00:03.989522
elapsed time: 0:00:51.423097
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 18:22:52.743630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.94
 ---- batch: 020 ----
mean loss: 936.13
train mean loss: 927.56
epoch train time: 0:00:04.015797
elapsed time: 0:00:55.440295
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 18:22:56.760838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.26
 ---- batch: 020 ----
mean loss: 945.38
train mean loss: 927.40
epoch train time: 0:00:04.020078
elapsed time: 0:00:59.461666
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 18:23:00.782219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.84
 ---- batch: 020 ----
mean loss: 914.36
train mean loss: 916.80
epoch train time: 0:00:04.008992
elapsed time: 0:01:03.471962
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 18:23:04.792509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.85
 ---- batch: 020 ----
mean loss: 904.44
train mean loss: 913.06
epoch train time: 0:00:04.018232
elapsed time: 0:01:07.491596
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 18:23:08.812152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.66
 ---- batch: 020 ----
mean loss: 896.97
train mean loss: 910.71
epoch train time: 0:00:04.013636
elapsed time: 0:01:11.506528
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 18:23:12.827072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.52
 ---- batch: 020 ----
mean loss: 888.13
train mean loss: 899.68
epoch train time: 0:00:04.007560
elapsed time: 0:01:15.515394
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 18:23:16.835945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.94
 ---- batch: 020 ----
mean loss: 887.72
train mean loss: 903.85
epoch train time: 0:00:04.016849
elapsed time: 0:01:19.533680
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 18:23:20.854193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.08
 ---- batch: 020 ----
mean loss: 907.56
train mean loss: 912.10
epoch train time: 0:00:04.016792
elapsed time: 0:01:23.551737
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 18:23:24.872254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.85
 ---- batch: 020 ----
mean loss: 910.75
train mean loss: 904.36
epoch train time: 0:00:04.003085
elapsed time: 0:01:27.556114
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 18:23:28.876640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.83
 ---- batch: 020 ----
mean loss: 894.75
train mean loss: 904.94
epoch train time: 0:00:04.006490
elapsed time: 0:01:31.563863
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 18:23:32.884371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.70
 ---- batch: 020 ----
mean loss: 893.58
train mean loss: 893.96
epoch train time: 0:00:04.003755
elapsed time: 0:01:35.568892
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 18:23:36.889417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 904.81
 ---- batch: 020 ----
mean loss: 897.98
train mean loss: 894.88
epoch train time: 0:00:04.020272
elapsed time: 0:01:39.590484
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 18:23:40.911038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.90
 ---- batch: 020 ----
mean loss: 896.00
train mean loss: 897.51
epoch train time: 0:00:04.000515
elapsed time: 0:01:43.592311
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 18:23:44.912822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.79
 ---- batch: 020 ----
mean loss: 866.46
train mean loss: 895.65
epoch train time: 0:00:04.048576
elapsed time: 0:01:47.642325
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 18:23:48.962880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.08
 ---- batch: 020 ----
mean loss: 897.29
train mean loss: 884.25
epoch train time: 0:00:04.017855
elapsed time: 0:01:51.661499
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 18:23:52.982019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.35
 ---- batch: 020 ----
mean loss: 864.27
train mean loss: 887.78
epoch train time: 0:00:04.009826
elapsed time: 0:01:55.672650
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 18:23:56.993189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.09
 ---- batch: 020 ----
mean loss: 895.39
train mean loss: 892.47
epoch train time: 0:00:04.030934
elapsed time: 0:01:59.704826
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 18:24:01.025376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.47
 ---- batch: 020 ----
mean loss: 896.40
train mean loss: 884.68
epoch train time: 0:00:04.010208
elapsed time: 0:02:03.716273
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 18:24:05.036807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.17
 ---- batch: 020 ----
mean loss: 898.43
train mean loss: 891.21
epoch train time: 0:00:04.020279
elapsed time: 0:02:07.737991
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 18:24:09.058559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 904.03
 ---- batch: 020 ----
mean loss: 878.84
train mean loss: 882.04
epoch train time: 0:00:04.011465
elapsed time: 0:02:11.750872
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 18:24:13.071397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.49
 ---- batch: 020 ----
mean loss: 892.81
train mean loss: 885.73
epoch train time: 0:00:04.012435
elapsed time: 0:02:15.764519
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 18:24:17.085038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.11
 ---- batch: 020 ----
mean loss: 866.00
train mean loss: 881.15
epoch train time: 0:00:04.023166
elapsed time: 0:02:19.788875
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 18:24:21.109390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.61
 ---- batch: 020 ----
mean loss: 874.50
train mean loss: 873.97
epoch train time: 0:00:04.018446
elapsed time: 0:02:23.808627
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 18:24:25.129148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.92
 ---- batch: 020 ----
mean loss: 878.48
train mean loss: 878.82
epoch train time: 0:00:04.007067
elapsed time: 0:02:27.816970
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 18:24:29.137498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.69
 ---- batch: 020 ----
mean loss: 893.14
train mean loss: 869.08
epoch train time: 0:00:04.034074
elapsed time: 0:02:31.852323
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 18:24:33.172856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.87
 ---- batch: 020 ----
mean loss: 877.09
train mean loss: 868.51
epoch train time: 0:00:04.013565
elapsed time: 0:02:35.867217
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 18:24:37.187743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.96
 ---- batch: 020 ----
mean loss: 873.60
train mean loss: 872.38
epoch train time: 0:00:03.999138
elapsed time: 0:02:39.867748
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 18:24:41.188280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.64
 ---- batch: 020 ----
mean loss: 862.83
train mean loss: 863.03
epoch train time: 0:00:04.036279
elapsed time: 0:02:43.905342
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 18:24:45.225923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.63
 ---- batch: 020 ----
mean loss: 870.61
train mean loss: 869.22
epoch train time: 0:00:04.029135
elapsed time: 0:02:47.935846
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 18:24:49.256355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.01
 ---- batch: 020 ----
mean loss: 864.77
train mean loss: 855.30
epoch train time: 0:00:04.008536
elapsed time: 0:02:51.945782
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 18:24:53.266303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.92
 ---- batch: 020 ----
mean loss: 869.02
train mean loss: 860.88
epoch train time: 0:00:04.025303
elapsed time: 0:02:55.972281
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 18:24:57.292797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.05
 ---- batch: 020 ----
mean loss: 856.20
train mean loss: 851.36
epoch train time: 0:00:04.017659
elapsed time: 0:02:59.991217
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 18:25:01.311744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.21
 ---- batch: 020 ----
mean loss: 854.29
train mean loss: 845.41
epoch train time: 0:00:04.022087
elapsed time: 0:03:04.014632
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 18:25:05.335157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.23
 ---- batch: 020 ----
mean loss: 869.42
train mean loss: 847.46
epoch train time: 0:00:04.026873
elapsed time: 0:03:08.042771
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 18:25:09.363282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.21
 ---- batch: 020 ----
mean loss: 827.09
train mean loss: 845.26
epoch train time: 0:00:04.003445
elapsed time: 0:03:12.047469
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 18:25:13.368005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.60
 ---- batch: 020 ----
mean loss: 816.34
train mean loss: 841.12
epoch train time: 0:00:04.038484
elapsed time: 0:03:16.087456
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 18:25:17.408038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.61
 ---- batch: 020 ----
mean loss: 830.23
train mean loss: 832.90
epoch train time: 0:00:04.023511
elapsed time: 0:03:20.112306
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 18:25:21.432850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.92
 ---- batch: 020 ----
mean loss: 846.67
train mean loss: 829.36
epoch train time: 0:00:04.027549
elapsed time: 0:03:24.141101
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 18:25:25.461645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 805.07
 ---- batch: 020 ----
mean loss: 832.28
train mean loss: 816.48
epoch train time: 0:00:04.040638
elapsed time: 0:03:28.183130
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 18:25:29.503649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.06
 ---- batch: 020 ----
mean loss: 810.52
train mean loss: 810.36
epoch train time: 0:00:04.043791
elapsed time: 0:03:32.228382
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 18:25:33.548928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 800.31
 ---- batch: 020 ----
mean loss: 777.17
train mean loss: 791.19
epoch train time: 0:00:04.041904
elapsed time: 0:03:36.271596
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 18:25:37.592106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 793.42
 ---- batch: 020 ----
mean loss: 769.53
train mean loss: 777.54
epoch train time: 0:00:04.025699
elapsed time: 0:03:40.298594
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 18:25:41.619156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.22
 ---- batch: 020 ----
mean loss: 759.88
train mean loss: 761.04
epoch train time: 0:00:04.033910
elapsed time: 0:03:44.333882
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 18:25:45.654411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 747.83
 ---- batch: 020 ----
mean loss: 733.41
train mean loss: 746.34
epoch train time: 0:00:04.036358
elapsed time: 0:03:48.371558
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 18:25:49.692114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 716.83
 ---- batch: 020 ----
mean loss: 742.08
train mean loss: 731.13
epoch train time: 0:00:04.035556
elapsed time: 0:03:52.408525
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 18:25:53.729069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.66
 ---- batch: 020 ----
mean loss: 710.91
train mean loss: 713.92
epoch train time: 0:00:04.036336
elapsed time: 0:03:56.446324
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 18:25:57.766838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 715.35
 ---- batch: 020 ----
mean loss: 707.39
train mean loss: 707.52
epoch train time: 0:00:04.049022
elapsed time: 0:04:00.496583
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 18:26:01.817120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 705.14
 ---- batch: 020 ----
mean loss: 689.87
train mean loss: 696.03
epoch train time: 0:00:04.090140
elapsed time: 0:04:04.588173
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 18:26:05.908791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.48
 ---- batch: 020 ----
mean loss: 676.98
train mean loss: 682.36
epoch train time: 0:00:04.069737
elapsed time: 0:04:08.659355
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 18:26:09.979881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 660.32
 ---- batch: 020 ----
mean loss: 672.38
train mean loss: 666.68
epoch train time: 0:00:04.035531
elapsed time: 0:04:12.696146
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 18:26:14.016675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 668.92
 ---- batch: 020 ----
mean loss: 674.05
train mean loss: 665.95
epoch train time: 0:00:04.011702
elapsed time: 0:04:16.709260
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 18:26:18.029779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.01
 ---- batch: 020 ----
mean loss: 657.84
train mean loss: 644.91
epoch train time: 0:00:04.020498
elapsed time: 0:04:20.731015
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 18:26:22.051564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 632.70
 ---- batch: 020 ----
mean loss: 640.18
train mean loss: 630.94
epoch train time: 0:00:04.022885
elapsed time: 0:04:24.755265
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 18:26:26.075783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 637.30
 ---- batch: 020 ----
mean loss: 625.59
train mean loss: 638.04
epoch train time: 0:00:04.021618
elapsed time: 0:04:28.778213
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 18:26:30.098762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 619.28
 ---- batch: 020 ----
mean loss: 613.87
train mean loss: 617.71
epoch train time: 0:00:04.033620
elapsed time: 0:04:32.813111
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 18:26:34.133697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 631.15
 ---- batch: 020 ----
mean loss: 606.11
train mean loss: 614.19
epoch train time: 0:00:04.011501
elapsed time: 0:04:36.826027
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 18:26:38.146584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.66
 ---- batch: 020 ----
mean loss: 586.74
train mean loss: 586.30
epoch train time: 0:00:04.016868
elapsed time: 0:04:40.844304
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 18:26:42.164861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 575.46
 ---- batch: 020 ----
mean loss: 596.58
train mean loss: 587.92
epoch train time: 0:00:04.025892
elapsed time: 0:04:44.871448
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 18:26:46.191967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 568.95
 ---- batch: 020 ----
mean loss: 573.77
train mean loss: 568.98
epoch train time: 0:00:04.022661
elapsed time: 0:04:48.895427
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 18:26:50.215966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.59
 ---- batch: 020 ----
mean loss: 562.83
train mean loss: 562.61
epoch train time: 0:00:04.049470
elapsed time: 0:04:52.946199
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 18:26:54.266808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 559.32
 ---- batch: 020 ----
mean loss: 548.94
train mean loss: 550.29
epoch train time: 0:00:04.044430
elapsed time: 0:04:56.992004
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 18:26:58.312544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 542.99
 ---- batch: 020 ----
mean loss: 555.41
train mean loss: 547.34
epoch train time: 0:00:04.052734
elapsed time: 0:05:01.046070
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 18:27:02.366611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.99
 ---- batch: 020 ----
mean loss: 554.92
train mean loss: 542.44
epoch train time: 0:00:04.063597
elapsed time: 0:05:05.110984
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 18:27:06.431537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 517.15
 ---- batch: 020 ----
mean loss: 534.99
train mean loss: 531.65
epoch train time: 0:00:04.063447
elapsed time: 0:05:09.175808
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 18:27:10.496337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 534.51
 ---- batch: 020 ----
mean loss: 522.89
train mean loss: 522.39
epoch train time: 0:00:04.061896
elapsed time: 0:05:13.239118
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 18:27:14.559664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.34
 ---- batch: 020 ----
mean loss: 511.04
train mean loss: 515.75
epoch train time: 0:00:04.046275
elapsed time: 0:05:17.286728
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 18:27:18.607251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.79
 ---- batch: 020 ----
mean loss: 518.99
train mean loss: 511.12
epoch train time: 0:00:04.085795
elapsed time: 0:05:21.373844
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 18:27:22.694415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.70
 ---- batch: 020 ----
mean loss: 497.24
train mean loss: 504.10
epoch train time: 0:00:04.025960
elapsed time: 0:05:25.401203
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 18:27:26.721754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.01
 ---- batch: 020 ----
mean loss: 506.33
train mean loss: 496.13
epoch train time: 0:00:04.025746
elapsed time: 0:05:29.428301
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 18:27:30.748835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.55
 ---- batch: 020 ----
mean loss: 484.25
train mean loss: 484.10
epoch train time: 0:00:04.029406
elapsed time: 0:05:33.459002
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 18:27:34.779527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.32
 ---- batch: 020 ----
mean loss: 492.32
train mean loss: 476.61
epoch train time: 0:00:04.025440
elapsed time: 0:05:37.485725
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 18:27:38.806280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.23
 ---- batch: 020 ----
mean loss: 475.55
train mean loss: 470.98
epoch train time: 0:00:04.014438
elapsed time: 0:05:41.501530
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 18:27:42.822066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.04
 ---- batch: 020 ----
mean loss: 463.62
train mean loss: 466.77
epoch train time: 0:00:04.024092
elapsed time: 0:05:45.526935
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 18:27:46.847479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.67
 ---- batch: 020 ----
mean loss: 465.94
train mean loss: 463.12
epoch train time: 0:00:04.027227
elapsed time: 0:05:49.555400
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 18:27:50.875953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.78
 ---- batch: 020 ----
mean loss: 448.89
train mean loss: 453.66
epoch train time: 0:00:04.047748
elapsed time: 0:05:53.604486
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 18:27:54.925015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.79
 ---- batch: 020 ----
mean loss: 448.23
train mean loss: 447.63
epoch train time: 0:00:04.047970
elapsed time: 0:05:57.654052
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 18:27:58.974604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.80
 ---- batch: 020 ----
mean loss: 453.12
train mean loss: 443.23
epoch train time: 0:00:04.053106
elapsed time: 0:06:01.708447
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 18:28:03.028967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.28
 ---- batch: 020 ----
mean loss: 429.87
train mean loss: 440.90
epoch train time: 0:00:04.098175
elapsed time: 0:06:05.807873
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 18:28:07.128391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.87
 ---- batch: 020 ----
mean loss: 434.95
train mean loss: 431.56
epoch train time: 0:00:04.103233
elapsed time: 0:06:09.912430
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 18:28:11.232968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.39
 ---- batch: 020 ----
mean loss: 425.49
train mean loss: 424.43
epoch train time: 0:00:04.095611
elapsed time: 0:06:14.009349
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 18:28:15.329895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.24
 ---- batch: 020 ----
mean loss: 425.62
train mean loss: 427.06
epoch train time: 0:00:04.096114
elapsed time: 0:06:18.106866
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 18:28:19.427372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.47
 ---- batch: 020 ----
mean loss: 427.73
train mean loss: 422.11
epoch train time: 0:00:04.095795
elapsed time: 0:06:22.204078
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 18:28:23.524609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.77
 ---- batch: 020 ----
mean loss: 405.08
train mean loss: 413.05
epoch train time: 0:00:04.091995
elapsed time: 0:06:26.297416
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 18:28:27.617939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.72
 ---- batch: 020 ----
mean loss: 406.30
train mean loss: 411.58
epoch train time: 0:00:04.082841
elapsed time: 0:06:30.381587
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 18:28:31.702127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.05
 ---- batch: 020 ----
mean loss: 397.25
train mean loss: 405.38
epoch train time: 0:00:04.082683
elapsed time: 0:06:34.465583
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 18:28:35.786101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.31
 ---- batch: 020 ----
mean loss: 393.93
train mean loss: 402.40
epoch train time: 0:00:04.079569
elapsed time: 0:06:38.546579
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 18:28:39.867107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.48
 ---- batch: 020 ----
mean loss: 389.20
train mean loss: 393.26
epoch train time: 0:00:04.089870
elapsed time: 0:06:42.637745
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 18:28:43.958279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.21
 ---- batch: 020 ----
mean loss: 402.61
train mean loss: 390.49
epoch train time: 0:00:04.080098
elapsed time: 0:06:46.719159
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 18:28:48.039707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.57
 ---- batch: 020 ----
mean loss: 387.27
train mean loss: 391.41
epoch train time: 0:00:04.099717
elapsed time: 0:06:50.820185
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 18:28:52.140715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.91
 ---- batch: 020 ----
mean loss: 382.25
train mean loss: 386.89
epoch train time: 0:00:04.105486
elapsed time: 0:06:54.927091
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 18:28:56.247650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.72
 ---- batch: 020 ----
mean loss: 381.47
train mean loss: 387.03
epoch train time: 0:00:04.097916
elapsed time: 0:06:59.026306
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 18:29:00.346855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.12
 ---- batch: 020 ----
mean loss: 371.04
train mean loss: 375.89
epoch train time: 0:00:04.094815
elapsed time: 0:07:03.122463
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 18:29:04.442989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.72
 ---- batch: 020 ----
mean loss: 385.16
train mean loss: 374.11
epoch train time: 0:00:04.102666
elapsed time: 0:07:07.226418
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 18:29:08.546931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.29
 ---- batch: 020 ----
mean loss: 365.02
train mean loss: 373.46
epoch train time: 0:00:04.107775
elapsed time: 0:07:11.335557
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 18:29:12.656093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.90
 ---- batch: 020 ----
mean loss: 373.81
train mean loss: 372.46
epoch train time: 0:00:04.086556
elapsed time: 0:07:15.423412
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 18:29:16.743948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.20
 ---- batch: 020 ----
mean loss: 371.35
train mean loss: 372.85
epoch train time: 0:00:04.102080
elapsed time: 0:07:19.527102
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 18:29:20.847478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.15
 ---- batch: 020 ----
mean loss: 367.61
train mean loss: 365.23
epoch train time: 0:00:04.090759
elapsed time: 0:07:23.619039
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 18:29:24.939609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.57
 ---- batch: 020 ----
mean loss: 367.43
train mean loss: 364.44
epoch train time: 0:00:04.066513
elapsed time: 0:07:27.686966
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 18:29:29.007510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.93
 ---- batch: 020 ----
mean loss: 361.85
train mean loss: 361.48
epoch train time: 0:00:04.098831
elapsed time: 0:07:31.787109
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 18:29:33.107690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.98
 ---- batch: 020 ----
mean loss: 362.35
train mean loss: 363.93
epoch train time: 0:00:04.082643
elapsed time: 0:07:35.871143
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 18:29:37.191675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.07
 ---- batch: 020 ----
mean loss: 360.39
train mean loss: 362.21
epoch train time: 0:00:04.082442
elapsed time: 0:07:39.954838
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 18:29:41.275360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.11
 ---- batch: 020 ----
mean loss: 346.77
train mean loss: 349.56
epoch train time: 0:00:04.091246
elapsed time: 0:07:44.047320
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 18:29:45.367842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.05
 ---- batch: 020 ----
mean loss: 357.14
train mean loss: 352.22
epoch train time: 0:00:04.091136
elapsed time: 0:07:48.139761
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 18:29:49.460323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.75
 ---- batch: 020 ----
mean loss: 352.75
train mean loss: 355.24
epoch train time: 0:00:04.099644
elapsed time: 0:07:52.240746
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 18:29:53.561285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.81
 ---- batch: 020 ----
mean loss: 347.05
train mean loss: 347.31
epoch train time: 0:00:04.112824
elapsed time: 0:07:56.354865
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 18:29:57.675400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.48
 ---- batch: 020 ----
mean loss: 350.86
train mean loss: 349.42
epoch train time: 0:00:04.087982
elapsed time: 0:08:00.444120
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 18:30:01.764704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.67
 ---- batch: 020 ----
mean loss: 349.54
train mean loss: 348.23
epoch train time: 0:00:04.117247
elapsed time: 0:08:04.562854
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 18:30:05.883381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.24
 ---- batch: 020 ----
mean loss: 349.33
train mean loss: 344.21
epoch train time: 0:00:04.090188
elapsed time: 0:08:08.654575
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 18:30:09.975108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.01
 ---- batch: 020 ----
mean loss: 348.71
train mean loss: 342.27
epoch train time: 0:00:04.086104
elapsed time: 0:08:12.741970
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 18:30:14.062479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.32
 ---- batch: 020 ----
mean loss: 341.35
train mean loss: 334.44
epoch train time: 0:00:04.078348
elapsed time: 0:08:16.821594
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 18:30:18.142141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.34
 ---- batch: 020 ----
mean loss: 344.51
train mean loss: 337.01
epoch train time: 0:00:04.099644
elapsed time: 0:08:20.922580
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 18:30:22.243115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.29
 ---- batch: 020 ----
mean loss: 336.79
train mean loss: 334.25
epoch train time: 0:00:04.108683
elapsed time: 0:08:25.032800
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 18:30:26.353362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.85
 ---- batch: 020 ----
mean loss: 341.21
train mean loss: 332.13
epoch train time: 0:00:04.098467
elapsed time: 0:08:29.132580
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 18:30:30.453092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.08
 ---- batch: 020 ----
mean loss: 331.85
train mean loss: 329.30
epoch train time: 0:00:04.112233
elapsed time: 0:08:33.246205
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 18:30:34.566747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.77
 ---- batch: 020 ----
mean loss: 332.47
train mean loss: 333.56
epoch train time: 0:00:04.108774
elapsed time: 0:08:37.356437
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 18:30:38.677009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.22
 ---- batch: 020 ----
mean loss: 331.50
train mean loss: 328.64
epoch train time: 0:00:04.097022
elapsed time: 0:08:41.455072
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 18:30:42.775424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.27
 ---- batch: 020 ----
mean loss: 322.82
train mean loss: 326.37
epoch train time: 0:00:04.103594
elapsed time: 0:08:45.559832
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 18:30:46.880378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.75
 ---- batch: 020 ----
mean loss: 322.76
train mean loss: 325.66
epoch train time: 0:00:04.114040
elapsed time: 0:08:49.675220
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 18:30:50.995790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.96
 ---- batch: 020 ----
mean loss: 320.70
train mean loss: 319.78
epoch train time: 0:00:04.094969
elapsed time: 0:08:53.771542
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 18:30:55.092098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.52
 ---- batch: 020 ----
mean loss: 312.14
train mean loss: 322.75
epoch train time: 0:00:04.104717
elapsed time: 0:08:57.877657
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 18:30:59.198179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.68
 ---- batch: 020 ----
mean loss: 328.66
train mean loss: 323.08
epoch train time: 0:00:04.107253
elapsed time: 0:09:01.986321
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 18:31:03.306884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.00
 ---- batch: 020 ----
mean loss: 324.25
train mean loss: 319.95
epoch train time: 0:00:04.122334
elapsed time: 0:09:06.109987
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 18:31:07.430522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.34
 ---- batch: 020 ----
mean loss: 318.29
train mean loss: 320.81
epoch train time: 0:00:04.140990
elapsed time: 0:09:10.252405
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 18:31:11.573028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.52
 ---- batch: 020 ----
mean loss: 310.76
train mean loss: 316.27
epoch train time: 0:00:04.146667
elapsed time: 0:09:14.400476
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 18:31:15.721019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.55
 ---- batch: 020 ----
mean loss: 303.69
train mean loss: 318.63
epoch train time: 0:00:04.156482
elapsed time: 0:09:18.558236
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 18:31:19.878751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.61
 ---- batch: 020 ----
mean loss: 311.61
train mean loss: 316.58
epoch train time: 0:00:04.136811
elapsed time: 0:09:22.696292
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 18:31:24.016870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.82
 ---- batch: 020 ----
mean loss: 309.61
train mean loss: 313.93
epoch train time: 0:00:04.095557
elapsed time: 0:09:26.793117
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 18:31:28.113640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.24
 ---- batch: 020 ----
mean loss: 309.10
train mean loss: 311.90
epoch train time: 0:00:04.108203
elapsed time: 0:09:30.902742
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 18:31:32.223267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.60
 ---- batch: 020 ----
mean loss: 306.61
train mean loss: 311.55
epoch train time: 0:00:04.092399
elapsed time: 0:09:34.996416
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 18:31:36.316935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.76
 ---- batch: 020 ----
mean loss: 318.55
train mean loss: 313.89
epoch train time: 0:00:04.073910
elapsed time: 0:09:39.071565
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 18:31:40.392096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.76
 ---- batch: 020 ----
mean loss: 321.74
train mean loss: 315.80
epoch train time: 0:00:04.070767
elapsed time: 0:09:43.143624
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 18:31:44.464160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.51
 ---- batch: 020 ----
mean loss: 309.25
train mean loss: 311.35
epoch train time: 0:00:04.088007
elapsed time: 0:09:47.233039
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 18:31:48.553649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.81
 ---- batch: 020 ----
mean loss: 308.21
train mean loss: 307.33
epoch train time: 0:00:04.104308
elapsed time: 0:09:51.338701
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 18:31:52.659222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.65
 ---- batch: 020 ----
mean loss: 303.07
train mean loss: 308.31
epoch train time: 0:00:04.112160
elapsed time: 0:09:55.452252
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 18:31:56.772805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.66
 ---- batch: 020 ----
mean loss: 304.26
train mean loss: 303.45
epoch train time: 0:00:04.111215
elapsed time: 0:09:59.564947
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 18:32:00.885486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.42
 ---- batch: 020 ----
mean loss: 305.54
train mean loss: 298.64
epoch train time: 0:00:04.092612
elapsed time: 0:10:03.658872
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 18:32:04.979385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.37
 ---- batch: 020 ----
mean loss: 305.57
train mean loss: 302.04
epoch train time: 0:00:04.109142
elapsed time: 0:10:07.769238
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 18:32:09.089826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.30
 ---- batch: 020 ----
mean loss: 305.29
train mean loss: 298.53
epoch train time: 0:00:04.099562
elapsed time: 0:10:11.870311
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 18:32:13.190654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.64
 ---- batch: 020 ----
mean loss: 291.70
train mean loss: 298.22
epoch train time: 0:00:04.108701
elapsed time: 0:10:15.980275
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 18:32:17.300812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.82
 ---- batch: 020 ----
mean loss: 287.93
train mean loss: 296.75
epoch train time: 0:00:04.070124
elapsed time: 0:10:20.051917
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 18:32:21.372460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.71
 ---- batch: 020 ----
mean loss: 299.08
train mean loss: 295.84
epoch train time: 0:00:04.097976
elapsed time: 0:10:24.151229
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 18:32:25.471763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.40
 ---- batch: 020 ----
mean loss: 295.27
train mean loss: 292.68
epoch train time: 0:00:04.119070
elapsed time: 0:10:28.271566
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 18:32:29.592101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.53
 ---- batch: 020 ----
mean loss: 289.88
train mean loss: 294.59
epoch train time: 0:00:04.094665
elapsed time: 0:10:32.367499
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 18:32:33.688041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.67
 ---- batch: 020 ----
mean loss: 282.69
train mean loss: 289.04
epoch train time: 0:00:04.094567
elapsed time: 0:10:36.463445
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 18:32:37.784012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.38
 ---- batch: 020 ----
mean loss: 292.47
train mean loss: 289.78
epoch train time: 0:00:04.087662
elapsed time: 0:10:40.552387
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 18:32:41.872908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.62
 ---- batch: 020 ----
mean loss: 297.79
train mean loss: 291.68
epoch train time: 0:00:04.097503
elapsed time: 0:10:44.651315
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 18:32:45.971849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.21
 ---- batch: 020 ----
mean loss: 290.98
train mean loss: 294.56
epoch train time: 0:00:04.094549
elapsed time: 0:10:48.747176
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 18:32:50.067722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.01
 ---- batch: 020 ----
mean loss: 298.67
train mean loss: 289.58
epoch train time: 0:00:04.084186
elapsed time: 0:10:52.832783
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 18:32:54.153341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.96
 ---- batch: 020 ----
mean loss: 287.69
train mean loss: 287.96
epoch train time: 0:00:04.106480
elapsed time: 0:10:56.940531
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 18:32:58.261052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.68
 ---- batch: 020 ----
mean loss: 280.08
train mean loss: 287.90
epoch train time: 0:00:04.086204
elapsed time: 0:11:01.028108
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 18:33:02.348636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.33
 ---- batch: 020 ----
mean loss: 284.47
train mean loss: 286.73
epoch train time: 0:00:04.086271
elapsed time: 0:11:05.115730
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 18:33:06.436269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.34
 ---- batch: 020 ----
mean loss: 287.02
train mean loss: 286.05
epoch train time: 0:00:04.109973
elapsed time: 0:11:09.227214
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 18:33:10.547779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.62
 ---- batch: 020 ----
mean loss: 281.61
train mean loss: 286.55
epoch train time: 0:00:04.088951
elapsed time: 0:11:13.317502
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 18:33:14.638061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.33
 ---- batch: 020 ----
mean loss: 272.14
train mean loss: 285.25
epoch train time: 0:00:04.097906
elapsed time: 0:11:17.416867
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 18:33:18.737447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.91
 ---- batch: 020 ----
mean loss: 276.42
train mean loss: 283.37
epoch train time: 0:00:04.117464
elapsed time: 0:11:21.535725
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 18:33:22.856290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.45
 ---- batch: 020 ----
mean loss: 280.02
train mean loss: 281.40
epoch train time: 0:00:04.114997
elapsed time: 0:11:25.652098
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 18:33:26.972628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.49
 ---- batch: 020 ----
mean loss: 286.92
train mean loss: 282.85
epoch train time: 0:00:04.095472
elapsed time: 0:11:29.748856
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 18:33:31.069378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.09
 ---- batch: 020 ----
mean loss: 278.27
train mean loss: 287.10
epoch train time: 0:00:04.097031
elapsed time: 0:11:33.847188
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 18:33:35.167715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.02
 ---- batch: 020 ----
mean loss: 285.21
train mean loss: 288.51
epoch train time: 0:00:04.099746
elapsed time: 0:11:37.948245
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 18:33:39.268780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.72
 ---- batch: 020 ----
mean loss: 272.34
train mean loss: 279.86
epoch train time: 0:00:04.077472
elapsed time: 0:11:42.027066
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 18:33:43.347608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.20
 ---- batch: 020 ----
mean loss: 275.64
train mean loss: 283.06
epoch train time: 0:00:04.100435
elapsed time: 0:11:46.128747
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 18:33:47.449276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.54
 ---- batch: 020 ----
mean loss: 280.84
train mean loss: 278.24
epoch train time: 0:00:04.095583
elapsed time: 0:11:50.225674
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 18:33:51.546199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.39
 ---- batch: 020 ----
mean loss: 280.09
train mean loss: 279.00
epoch train time: 0:00:04.095051
elapsed time: 0:11:54.322230
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 18:33:55.642585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.50
 ---- batch: 020 ----
mean loss: 281.39
train mean loss: 275.55
epoch train time: 0:00:04.099706
elapsed time: 0:11:58.423052
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 18:33:59.743629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.44
 ---- batch: 020 ----
mean loss: 278.22
train mean loss: 272.42
epoch train time: 0:00:04.108349
elapsed time: 0:12:02.532940
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 18:34:03.853530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.92
 ---- batch: 020 ----
mean loss: 283.50
train mean loss: 280.58
epoch train time: 0:00:04.108426
elapsed time: 0:12:06.642701
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 18:34:07.963211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.62
 ---- batch: 020 ----
mean loss: 272.18
train mean loss: 277.96
epoch train time: 0:00:04.094214
elapsed time: 0:12:10.738334
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 18:34:12.058903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.39
 ---- batch: 020 ----
mean loss: 273.34
train mean loss: 271.53
epoch train time: 0:00:04.115230
elapsed time: 0:12:14.855164
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 18:34:16.175729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.98
 ---- batch: 020 ----
mean loss: 264.46
train mean loss: 275.84
epoch train time: 0:00:04.103307
elapsed time: 0:12:18.959749
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 18:34:20.280270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.20
 ---- batch: 020 ----
mean loss: 267.88
train mean loss: 271.62
epoch train time: 0:00:04.095654
elapsed time: 0:12:23.056818
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 18:34:24.377372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.22
 ---- batch: 020 ----
mean loss: 282.58
train mean loss: 273.08
epoch train time: 0:00:04.113681
elapsed time: 0:12:27.171806
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 18:34:28.492321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.25
 ---- batch: 020 ----
mean loss: 274.74
train mean loss: 271.09
epoch train time: 0:00:04.106843
elapsed time: 0:12:31.279912
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 18:34:32.600447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.56
 ---- batch: 020 ----
mean loss: 270.91
train mean loss: 272.03
epoch train time: 0:00:04.103227
elapsed time: 0:12:35.384439
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 18:34:36.704973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.01
 ---- batch: 020 ----
mean loss: 278.51
train mean loss: 270.81
epoch train time: 0:00:04.112657
elapsed time: 0:12:39.498401
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 18:34:40.818936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.72
 ---- batch: 020 ----
mean loss: 272.29
train mean loss: 268.24
epoch train time: 0:00:04.096977
elapsed time: 0:12:43.596719
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 18:34:44.917264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.60
 ---- batch: 020 ----
mean loss: 261.36
train mean loss: 263.93
epoch train time: 0:00:04.115836
elapsed time: 0:12:47.713933
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 18:34:49.034463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.12
 ---- batch: 020 ----
mean loss: 265.52
train mean loss: 266.70
epoch train time: 0:00:04.113055
elapsed time: 0:12:51.828277
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 18:34:53.148822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.58
 ---- batch: 020 ----
mean loss: 271.09
train mean loss: 264.88
epoch train time: 0:00:04.102586
elapsed time: 0:12:55.932266
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 18:34:57.252782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.72
 ---- batch: 020 ----
mean loss: 263.99
train mean loss: 266.09
epoch train time: 0:00:04.089205
elapsed time: 0:13:00.022820
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 18:35:01.343344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.68
 ---- batch: 020 ----
mean loss: 265.58
train mean loss: 261.84
epoch train time: 0:00:04.107017
elapsed time: 0:13:04.131099
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 18:35:05.451677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.69
 ---- batch: 020 ----
mean loss: 258.04
train mean loss: 262.22
epoch train time: 0:00:04.067664
elapsed time: 0:13:08.200043
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 18:35:09.520936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.32
 ---- batch: 020 ----
mean loss: 258.39
train mean loss: 259.52
epoch train time: 0:00:04.028236
elapsed time: 0:13:12.229888
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 18:35:13.550478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.57
 ---- batch: 020 ----
mean loss: 257.58
train mean loss: 263.02
epoch train time: 0:00:04.030792
elapsed time: 0:13:16.261988
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 18:35:17.582560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.16
 ---- batch: 020 ----
mean loss: 263.19
train mean loss: 261.12
epoch train time: 0:00:04.034272
elapsed time: 0:13:20.297681
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 18:35:21.618198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.71
 ---- batch: 020 ----
mean loss: 258.68
train mean loss: 262.24
epoch train time: 0:00:04.039755
elapsed time: 0:13:24.338758
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 18:35:25.659286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.09
 ---- batch: 020 ----
mean loss: 275.83
train mean loss: 269.12
epoch train time: 0:00:04.023440
elapsed time: 0:13:28.363450
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 18:35:29.683976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.45
 ---- batch: 020 ----
mean loss: 263.20
train mean loss: 262.46
epoch train time: 0:00:04.039864
elapsed time: 0:13:32.404693
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 18:35:33.725293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.81
 ---- batch: 020 ----
mean loss: 257.30
train mean loss: 259.01
epoch train time: 0:00:04.011573
elapsed time: 0:13:36.417672
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 18:35:37.738191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.39
 ---- batch: 020 ----
mean loss: 258.89
train mean loss: 261.27
epoch train time: 0:00:04.013737
elapsed time: 0:13:40.432691
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 18:35:41.753216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.36
 ---- batch: 020 ----
mean loss: 252.57
train mean loss: 260.23
epoch train time: 0:00:04.020718
elapsed time: 0:13:44.454755
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 18:35:45.775287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.32
 ---- batch: 020 ----
mean loss: 260.59
train mean loss: 255.47
epoch train time: 0:00:04.018312
elapsed time: 0:13:48.474615
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 18:35:49.794959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.42
 ---- batch: 020 ----
mean loss: 254.52
train mean loss: 254.56
epoch train time: 0:00:04.017200
elapsed time: 0:13:52.493017
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 18:35:53.813622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.76
 ---- batch: 020 ----
mean loss: 266.54
train mean loss: 255.14
epoch train time: 0:00:04.026166
elapsed time: 0:13:56.520577
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 18:35:57.841213
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.31
 ---- batch: 020 ----
mean loss: 248.28
train mean loss: 256.02
epoch train time: 0:00:04.012294
elapsed time: 0:14:00.534317
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 18:36:01.854850
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.37
 ---- batch: 020 ----
mean loss: 251.33
train mean loss: 254.46
epoch train time: 0:00:04.001060
elapsed time: 0:14:04.536598
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 18:36:05.857142
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.05
 ---- batch: 020 ----
mean loss: 257.85
train mean loss: 254.12
epoch train time: 0:00:03.997386
elapsed time: 0:14:08.535386
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 18:36:09.855948
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.04
 ---- batch: 020 ----
mean loss: 254.70
train mean loss: 257.96
epoch train time: 0:00:04.046838
elapsed time: 0:14:12.583629
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 18:36:13.904170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.16
 ---- batch: 020 ----
mean loss: 261.58
train mean loss: 256.10
epoch train time: 0:00:04.046925
elapsed time: 0:14:16.631980
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 18:36:17.952856
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.08
 ---- batch: 020 ----
mean loss: 250.70
train mean loss: 258.37
epoch train time: 0:00:04.008690
elapsed time: 0:14:20.642293
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 18:36:21.962874
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.07
 ---- batch: 020 ----
mean loss: 258.84
train mean loss: 254.01
epoch train time: 0:00:04.046133
elapsed time: 0:14:24.689927
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 18:36:26.010472
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.16
 ---- batch: 020 ----
mean loss: 252.94
train mean loss: 253.22
epoch train time: 0:00:04.082989
elapsed time: 0:14:28.774465
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 18:36:30.095008
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.24
 ---- batch: 020 ----
mean loss: 257.65
train mean loss: 258.08
epoch train time: 0:00:04.065814
elapsed time: 0:14:32.841589
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 18:36:34.162124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.42
 ---- batch: 020 ----
mean loss: 252.14
train mean loss: 254.18
epoch train time: 0:00:04.014148
elapsed time: 0:14:36.856970
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 18:36:38.177502
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.42
 ---- batch: 020 ----
mean loss: 253.93
train mean loss: 251.95
epoch train time: 0:00:03.999477
elapsed time: 0:14:40.857779
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 18:36:42.178319
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.63
 ---- batch: 020 ----
mean loss: 253.38
train mean loss: 257.78
epoch train time: 0:00:04.013441
elapsed time: 0:14:44.872578
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 18:36:46.193106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.46
 ---- batch: 020 ----
mean loss: 252.15
train mean loss: 252.90
epoch train time: 0:00:04.036465
elapsed time: 0:14:48.910314
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 18:36:50.230865
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.24
 ---- batch: 020 ----
mean loss: 256.16
train mean loss: 254.54
epoch train time: 0:00:04.007541
elapsed time: 0:14:52.919134
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 18:36:54.239651
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.92
 ---- batch: 020 ----
mean loss: 247.76
train mean loss: 253.11
epoch train time: 0:00:04.024906
elapsed time: 0:14:56.945352
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 18:36:58.265884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.92
 ---- batch: 020 ----
mean loss: 257.78
train mean loss: 255.55
epoch train time: 0:00:03.999383
elapsed time: 0:15:00.945989
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 18:37:02.266532
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.03
 ---- batch: 020 ----
mean loss: 254.08
train mean loss: 253.44
epoch train time: 0:00:03.990335
elapsed time: 0:15:04.937614
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 18:37:06.258140
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.63
 ---- batch: 020 ----
mean loss: 261.16
train mean loss: 253.70
epoch train time: 0:00:03.996579
elapsed time: 0:15:08.935483
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 18:37:10.256006
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.71
 ---- batch: 020 ----
mean loss: 246.07
train mean loss: 252.84
epoch train time: 0:00:03.994130
elapsed time: 0:15:12.930990
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 18:37:14.251534
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.31
 ---- batch: 020 ----
mean loss: 252.29
train mean loss: 254.95
epoch train time: 0:00:04.006222
elapsed time: 0:15:16.938440
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 18:37:18.259297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 248.07
 ---- batch: 020 ----
mean loss: 255.71
train mean loss: 254.38
epoch train time: 0:00:04.008620
elapsed time: 0:15:20.948653
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 18:37:22.269190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.58
 ---- batch: 020 ----
mean loss: 256.81
train mean loss: 254.56
epoch train time: 0:00:04.030637
elapsed time: 0:15:24.980642
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 18:37:26.301177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.70
 ---- batch: 020 ----
mean loss: 252.16
train mean loss: 252.86
epoch train time: 0:00:04.028315
elapsed time: 0:15:29.010282
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 18:37:30.330814
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.18
 ---- batch: 020 ----
mean loss: 255.40
train mean loss: 252.80
epoch train time: 0:00:04.022754
elapsed time: 0:15:33.034417
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 18:37:34.354959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.88
 ---- batch: 020 ----
mean loss: 255.08
train mean loss: 252.34
epoch train time: 0:00:04.011735
elapsed time: 0:15:37.047405
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 18:37:38.367966
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.95
 ---- batch: 020 ----
mean loss: 256.71
train mean loss: 254.38
epoch train time: 0:00:03.998914
elapsed time: 0:15:41.047613
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 18:37:42.368132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.45
 ---- batch: 020 ----
mean loss: 254.05
train mean loss: 253.95
epoch train time: 0:00:04.000262
elapsed time: 0:15:45.049169
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 18:37:46.369717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.36
 ---- batch: 020 ----
mean loss: 246.74
train mean loss: 253.35
epoch train time: 0:00:04.008061
elapsed time: 0:15:49.058486
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 18:37:50.379012
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 247.15
 ---- batch: 020 ----
mean loss: 251.80
train mean loss: 254.29
epoch train time: 0:00:04.001690
elapsed time: 0:15:53.061445
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 18:37:54.381987
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 241.74
 ---- batch: 020 ----
mean loss: 256.10
train mean loss: 251.60
epoch train time: 0:00:04.025920
elapsed time: 0:15:57.088854
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 18:37:58.409196
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.02
 ---- batch: 020 ----
mean loss: 254.72
train mean loss: 253.08
epoch train time: 0:00:04.032854
elapsed time: 0:16:01.122930
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 18:38:02.443483
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.18
 ---- batch: 020 ----
mean loss: 258.05
train mean loss: 250.36
epoch train time: 0:00:04.043713
elapsed time: 0:16:05.167960
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 18:38:06.488489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.11
 ---- batch: 020 ----
mean loss: 252.62
train mean loss: 254.42
epoch train time: 0:00:04.022203
elapsed time: 0:16:09.191410
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 18:38:10.511936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 248.97
 ---- batch: 020 ----
mean loss: 260.49
train mean loss: 255.42
epoch train time: 0:00:04.016344
elapsed time: 0:16:13.209164
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 18:38:14.529739
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.99
 ---- batch: 020 ----
mean loss: 252.23
train mean loss: 254.51
epoch train time: 0:00:04.018484
elapsed time: 0:16:17.228983
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 18:38:18.549534
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 242.79
 ---- batch: 020 ----
mean loss: 255.21
train mean loss: 252.47
epoch train time: 0:00:03.999873
elapsed time: 0:16:21.230356
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 18:38:22.550892
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.37
 ---- batch: 020 ----
mean loss: 247.58
train mean loss: 251.44
epoch train time: 0:00:04.001280
elapsed time: 0:16:25.232855
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 18:38:26.553401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 247.64
 ---- batch: 020 ----
mean loss: 258.88
train mean loss: 254.45
epoch train time: 0:00:04.011470
elapsed time: 0:16:29.245635
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 18:38:30.566173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.00
 ---- batch: 020 ----
mean loss: 245.62
train mean loss: 254.29
epoch train time: 0:00:04.001383
elapsed time: 0:16:33.248316
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 18:38:34.568867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.04
 ---- batch: 020 ----
mean loss: 257.75
train mean loss: 253.38
epoch train time: 0:00:04.011197
elapsed time: 0:16:37.260826
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 18:38:38.581345
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.20
 ---- batch: 020 ----
mean loss: 249.24
train mean loss: 254.31
epoch train time: 0:00:03.996899
elapsed time: 0:16:41.259018
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 18:38:42.579538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.18
 ---- batch: 020 ----
mean loss: 254.05
train mean loss: 252.99
epoch train time: 0:00:04.005151
elapsed time: 0:16:45.265462
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 18:38:46.585997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.69
 ---- batch: 020 ----
mean loss: 247.99
train mean loss: 253.62
epoch train time: 0:00:04.007202
elapsed time: 0:16:49.273982
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 18:38:50.594510
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.34
 ---- batch: 020 ----
mean loss: 246.84
train mean loss: 252.99
epoch train time: 0:00:04.001054
elapsed time: 0:16:53.276521
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 18:38:54.597043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.36
 ---- batch: 020 ----
mean loss: 250.20
train mean loss: 250.22
epoch train time: 0:00:04.020087
elapsed time: 0:16:57.297832
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 18:38:58.618354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.08
 ---- batch: 020 ----
mean loss: 260.39
train mean loss: 255.53
epoch train time: 0:00:03.998940
elapsed time: 0:17:01.306708
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_8/checkpoint.pth.tar
**** end time: 2019-09-26 18:39:02.627040 ****
