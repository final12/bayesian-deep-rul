Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_9', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8914
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistDense3...
Done.
**** start time: 2019-09-20 23:49:51.529469 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
            Linear-2                  [-1, 100]          42,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 62,100
Trainable params: 62,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:49:51.532410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4718.43
 ---- batch: 020 ----
mean loss: 4615.02
 ---- batch: 030 ----
mean loss: 4569.06
 ---- batch: 040 ----
mean loss: 4416.95
train mean loss: 4562.50
epoch train time: 0:00:14.831043
elapsed time: 0:00:14.836190
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:50:06.365715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4356.17
 ---- batch: 020 ----
mean loss: 4224.89
 ---- batch: 030 ----
mean loss: 4137.18
 ---- batch: 040 ----
mean loss: 4164.41
train mean loss: 4208.19
epoch train time: 0:00:00.209590
elapsed time: 0:00:15.045930
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:50:06.575483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3982.25
 ---- batch: 020 ----
mean loss: 3951.67
 ---- batch: 030 ----
mean loss: 3950.47
 ---- batch: 040 ----
mean loss: 3828.21
train mean loss: 3913.08
epoch train time: 0:00:00.204079
elapsed time: 0:00:15.250183
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:50:06.779694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3716.34
 ---- batch: 020 ----
mean loss: 3767.28
 ---- batch: 030 ----
mean loss: 3512.35
 ---- batch: 040 ----
mean loss: 3584.81
train mean loss: 3645.25
epoch train time: 0:00:00.201385
elapsed time: 0:00:15.451706
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:50:06.981219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3505.22
 ---- batch: 020 ----
mean loss: 3405.54
 ---- batch: 030 ----
mean loss: 3384.01
 ---- batch: 040 ----
mean loss: 3299.47
train mean loss: 3389.91
epoch train time: 0:00:00.199084
elapsed time: 0:00:15.650940
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:50:07.180453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3219.88
 ---- batch: 020 ----
mean loss: 3235.00
 ---- batch: 030 ----
mean loss: 3136.35
 ---- batch: 040 ----
mean loss: 3071.25
train mean loss: 3160.92
epoch train time: 0:00:00.201892
elapsed time: 0:00:15.852951
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:50:07.382463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3003.89
 ---- batch: 020 ----
mean loss: 3024.49
 ---- batch: 030 ----
mean loss: 2949.44
 ---- batch: 040 ----
mean loss: 2825.85
train mean loss: 2945.62
epoch train time: 0:00:00.201369
elapsed time: 0:00:16.054440
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:50:07.583952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2834.93
 ---- batch: 020 ----
mean loss: 2727.70
 ---- batch: 030 ----
mean loss: 2760.40
 ---- batch: 040 ----
mean loss: 2680.10
train mean loss: 2746.96
epoch train time: 0:00:00.200844
elapsed time: 0:00:16.255418
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:50:07.784935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2692.26
 ---- batch: 020 ----
mean loss: 2569.23
 ---- batch: 030 ----
mean loss: 2536.98
 ---- batch: 040 ----
mean loss: 2488.52
train mean loss: 2566.27
epoch train time: 0:00:00.214735
elapsed time: 0:00:16.470288
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:50:07.999802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2453.28
 ---- batch: 020 ----
mean loss: 2409.09
 ---- batch: 030 ----
mean loss: 2386.58
 ---- batch: 040 ----
mean loss: 2383.89
train mean loss: 2401.15
epoch train time: 0:00:00.202902
elapsed time: 0:00:16.673309
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:50:08.202843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2295.50
 ---- batch: 020 ----
mean loss: 2275.92
 ---- batch: 030 ----
mean loss: 2250.12
 ---- batch: 040 ----
mean loss: 2163.86
train mean loss: 2247.82
epoch train time: 0:00:00.200578
elapsed time: 0:00:16.874081
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:50:08.403598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2191.97
 ---- batch: 020 ----
mean loss: 2098.07
 ---- batch: 030 ----
mean loss: 2068.77
 ---- batch: 040 ----
mean loss: 2056.85
train mean loss: 2101.02
epoch train time: 0:00:00.196441
elapsed time: 0:00:17.070641
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:50:08.600150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2005.56
 ---- batch: 020 ----
mean loss: 1992.01
 ---- batch: 030 ----
mean loss: 1943.72
 ---- batch: 040 ----
mean loss: 1945.97
train mean loss: 1967.82
epoch train time: 0:00:00.200586
elapsed time: 0:00:17.271344
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:50:08.800852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1886.19
 ---- batch: 020 ----
mean loss: 1861.91
 ---- batch: 030 ----
mean loss: 1827.22
 ---- batch: 040 ----
mean loss: 1816.47
train mean loss: 1846.54
epoch train time: 0:00:00.212175
elapsed time: 0:00:17.483634
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:50:09.013158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1780.31
 ---- batch: 020 ----
mean loss: 1732.41
 ---- batch: 030 ----
mean loss: 1701.10
 ---- batch: 040 ----
mean loss: 1698.96
train mean loss: 1721.87
epoch train time: 0:00:00.202407
elapsed time: 0:00:17.686178
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:50:09.215693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1629.24
 ---- batch: 020 ----
mean loss: 1638.56
 ---- batch: 030 ----
mean loss: 1605.34
 ---- batch: 040 ----
mean loss: 1566.36
train mean loss: 1605.70
epoch train time: 0:00:00.201377
elapsed time: 0:00:17.887677
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:50:09.417190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1520.06
 ---- batch: 020 ----
mean loss: 1509.91
 ---- batch: 030 ----
mean loss: 1497.93
 ---- batch: 040 ----
mean loss: 1482.82
train mean loss: 1502.44
epoch train time: 0:00:00.202938
elapsed time: 0:00:18.090728
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:50:09.620235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1432.66
 ---- batch: 020 ----
mean loss: 1420.60
 ---- batch: 030 ----
mean loss: 1399.34
 ---- batch: 040 ----
mean loss: 1380.71
train mean loss: 1406.11
epoch train time: 0:00:00.200259
elapsed time: 0:00:18.291101
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:50:09.820613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1365.29
 ---- batch: 020 ----
mean loss: 1304.90
 ---- batch: 030 ----
mean loss: 1327.62
 ---- batch: 040 ----
mean loss: 1291.01
train mean loss: 1318.33
epoch train time: 0:00:00.200328
elapsed time: 0:00:18.491561
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:50:10.021072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1255.29
 ---- batch: 020 ----
mean loss: 1257.64
 ---- batch: 030 ----
mean loss: 1251.79
 ---- batch: 040 ----
mean loss: 1201.16
train mean loss: 1241.22
epoch train time: 0:00:00.205340
elapsed time: 0:00:18.697040
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:50:10.226556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1190.29
 ---- batch: 020 ----
mean loss: 1184.50
 ---- batch: 030 ----
mean loss: 1163.58
 ---- batch: 040 ----
mean loss: 1149.17
train mean loss: 1169.98
epoch train time: 0:00:00.209628
elapsed time: 0:00:18.906792
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:50:10.436305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1130.07
 ---- batch: 020 ----
mean loss: 1056.72
 ---- batch: 030 ----
mean loss: 1024.08
 ---- batch: 040 ----
mean loss: 1010.77
train mean loss: 1050.21
epoch train time: 0:00:00.206496
elapsed time: 0:00:19.113407
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:50:10.642921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.69
 ---- batch: 020 ----
mean loss: 957.80
 ---- batch: 030 ----
mean loss: 940.01
 ---- batch: 040 ----
mean loss: 912.68
train mean loss: 945.21
epoch train time: 0:00:00.205776
elapsed time: 0:00:19.319341
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:50:10.848866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.54
 ---- batch: 020 ----
mean loss: 898.02
 ---- batch: 030 ----
mean loss: 868.81
 ---- batch: 040 ----
mean loss: 847.34
train mean loss: 875.56
epoch train time: 0:00:00.206295
elapsed time: 0:00:19.525784
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:50:11.055309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.11
 ---- batch: 020 ----
mean loss: 814.83
 ---- batch: 030 ----
mean loss: 807.68
 ---- batch: 040 ----
mean loss: 789.27
train mean loss: 812.04
epoch train time: 0:00:00.201801
elapsed time: 0:00:19.727732
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:50:11.257271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 778.06
 ---- batch: 020 ----
mean loss: 776.82
 ---- batch: 030 ----
mean loss: 748.75
 ---- batch: 040 ----
mean loss: 729.13
train mean loss: 754.20
epoch train time: 0:00:00.202381
elapsed time: 0:00:19.930257
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:50:11.459794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 729.96
 ---- batch: 020 ----
mean loss: 708.39
 ---- batch: 030 ----
mean loss: 686.58
 ---- batch: 040 ----
mean loss: 684.82
train mean loss: 700.62
epoch train time: 0:00:00.200684
elapsed time: 0:00:20.131086
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:50:11.660607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 672.01
 ---- batch: 020 ----
mean loss: 654.49
 ---- batch: 030 ----
mean loss: 649.84
 ---- batch: 040 ----
mean loss: 630.37
train mean loss: 648.84
epoch train time: 0:00:00.209308
elapsed time: 0:00:20.340519
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:50:11.870048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 625.72
 ---- batch: 020 ----
mean loss: 602.76
 ---- batch: 030 ----
mean loss: 589.66
 ---- batch: 040 ----
mean loss: 580.53
train mean loss: 597.55
epoch train time: 0:00:00.206451
elapsed time: 0:00:20.547110
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:50:12.076624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 568.35
 ---- batch: 020 ----
mean loss: 546.51
 ---- batch: 030 ----
mean loss: 550.97
 ---- batch: 040 ----
mean loss: 535.10
train mean loss: 548.21
epoch train time: 0:00:00.203838
elapsed time: 0:00:20.751068
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:50:12.280579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.12
 ---- batch: 020 ----
mean loss: 513.48
 ---- batch: 030 ----
mean loss: 502.00
 ---- batch: 040 ----
mean loss: 489.14
train mean loss: 500.64
epoch train time: 0:00:00.202025
elapsed time: 0:00:20.953257
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:50:12.482766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.74
 ---- batch: 020 ----
mean loss: 468.27
 ---- batch: 030 ----
mean loss: 451.98
 ---- batch: 040 ----
mean loss: 440.95
train mean loss: 456.54
epoch train time: 0:00:00.197520
elapsed time: 0:00:21.150897
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:50:12.680458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.89
 ---- batch: 020 ----
mean loss: 427.66
 ---- batch: 030 ----
mean loss: 407.27
 ---- batch: 040 ----
mean loss: 416.54
train mean loss: 417.32
epoch train time: 0:00:00.201431
elapsed time: 0:00:21.352521
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:50:12.882038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.78
 ---- batch: 020 ----
mean loss: 382.82
 ---- batch: 030 ----
mean loss: 377.65
 ---- batch: 040 ----
mean loss: 373.72
train mean loss: 381.97
epoch train time: 0:00:00.200879
elapsed time: 0:00:21.553583
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:50:13.083141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.27
 ---- batch: 020 ----
mean loss: 355.32
 ---- batch: 030 ----
mean loss: 347.95
 ---- batch: 040 ----
mean loss: 335.54
train mean loss: 350.67
epoch train time: 0:00:00.198869
elapsed time: 0:00:21.752618
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:50:13.282134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.39
 ---- batch: 020 ----
mean loss: 326.06
 ---- batch: 030 ----
mean loss: 320.60
 ---- batch: 040 ----
mean loss: 319.66
train mean loss: 322.46
epoch train time: 0:00:00.205388
elapsed time: 0:00:21.958136
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:50:13.487658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.28
 ---- batch: 020 ----
mean loss: 302.94
 ---- batch: 030 ----
mean loss: 295.34
 ---- batch: 040 ----
mean loss: 289.22
train mean loss: 297.09
epoch train time: 0:00:00.200565
elapsed time: 0:00:22.158823
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:50:13.688330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.83
 ---- batch: 020 ----
mean loss: 276.24
 ---- batch: 030 ----
mean loss: 270.59
 ---- batch: 040 ----
mean loss: 266.50
train mean loss: 273.04
epoch train time: 0:00:00.198235
elapsed time: 0:00:22.357176
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:50:13.886718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.20
 ---- batch: 020 ----
mean loss: 255.57
 ---- batch: 030 ----
mean loss: 247.98
 ---- batch: 040 ----
mean loss: 247.21
train mean loss: 252.40
epoch train time: 0:00:00.204079
elapsed time: 0:00:22.561401
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:50:14.090910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.54
 ---- batch: 020 ----
mean loss: 235.31
 ---- batch: 030 ----
mean loss: 231.28
 ---- batch: 040 ----
mean loss: 226.13
train mean loss: 233.29
epoch train time: 0:00:00.204079
elapsed time: 0:00:22.765600
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:50:14.295114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.18
 ---- batch: 020 ----
mean loss: 217.71
 ---- batch: 030 ----
mean loss: 217.30
 ---- batch: 040 ----
mean loss: 211.07
train mean loss: 216.61
epoch train time: 0:00:00.205997
elapsed time: 0:00:22.971725
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:50:14.501255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.74
 ---- batch: 020 ----
mean loss: 196.07
 ---- batch: 030 ----
mean loss: 203.52
 ---- batch: 040 ----
mean loss: 196.84
train mean loss: 200.50
epoch train time: 0:00:00.205573
elapsed time: 0:00:23.177438
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:50:14.706981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.91
 ---- batch: 020 ----
mean loss: 189.75
 ---- batch: 030 ----
mean loss: 186.14
 ---- batch: 040 ----
mean loss: 180.43
train mean loss: 186.59
epoch train time: 0:00:00.208356
elapsed time: 0:00:23.385952
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:50:14.915498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.03
 ---- batch: 020 ----
mean loss: 181.19
 ---- batch: 030 ----
mean loss: 169.76
 ---- batch: 040 ----
mean loss: 166.22
train mean loss: 173.73
epoch train time: 0:00:00.206336
elapsed time: 0:00:23.592442
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:50:15.121996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.72
 ---- batch: 020 ----
mean loss: 165.86
 ---- batch: 030 ----
mean loss: 162.36
 ---- batch: 040 ----
mean loss: 160.24
train mean loss: 163.41
epoch train time: 0:00:00.206228
elapsed time: 0:00:23.798835
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:50:15.328350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.03
 ---- batch: 020 ----
mean loss: 154.35
 ---- batch: 030 ----
mean loss: 152.10
 ---- batch: 040 ----
mean loss: 149.49
train mean loss: 152.70
epoch train time: 0:00:00.205758
elapsed time: 0:00:24.004716
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:50:15.534241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.77
 ---- batch: 020 ----
mean loss: 142.03
 ---- batch: 030 ----
mean loss: 145.42
 ---- batch: 040 ----
mean loss: 140.60
train mean loss: 142.79
epoch train time: 0:00:00.205099
elapsed time: 0:00:24.209954
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:50:15.739468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.14
 ---- batch: 020 ----
mean loss: 135.03
 ---- batch: 030 ----
mean loss: 132.15
 ---- batch: 040 ----
mean loss: 134.03
train mean loss: 134.37
epoch train time: 0:00:00.206774
elapsed time: 0:00:24.416848
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:50:15.946360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.08
 ---- batch: 020 ----
mean loss: 126.18
 ---- batch: 030 ----
mean loss: 124.93
 ---- batch: 040 ----
mean loss: 128.34
train mean loss: 126.48
epoch train time: 0:00:00.201527
elapsed time: 0:00:24.618491
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:50:16.148030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.11
 ---- batch: 020 ----
mean loss: 120.84
 ---- batch: 030 ----
mean loss: 116.11
 ---- batch: 040 ----
mean loss: 117.96
train mean loss: 119.94
epoch train time: 0:00:00.201172
elapsed time: 0:00:24.819816
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:50:16.349353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.64
 ---- batch: 020 ----
mean loss: 115.50
 ---- batch: 030 ----
mean loss: 112.15
 ---- batch: 040 ----
mean loss: 112.97
train mean loss: 113.84
epoch train time: 0:00:00.195961
elapsed time: 0:00:25.015919
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:50:16.545429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.68
 ---- batch: 020 ----
mean loss: 107.50
 ---- batch: 030 ----
mean loss: 107.33
 ---- batch: 040 ----
mean loss: 106.14
train mean loss: 108.05
epoch train time: 0:00:00.195514
elapsed time: 0:00:25.211550
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:50:16.741060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.45
 ---- batch: 020 ----
mean loss: 103.02
 ---- batch: 030 ----
mean loss: 104.64
 ---- batch: 040 ----
mean loss: 101.25
train mean loss: 103.67
epoch train time: 0:00:00.197756
elapsed time: 0:00:25.409416
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:50:16.938923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.64
 ---- batch: 020 ----
mean loss: 98.01
 ---- batch: 030 ----
mean loss: 97.74
 ---- batch: 040 ----
mean loss: 98.41
train mean loss: 98.50
epoch train time: 0:00:00.192959
elapsed time: 0:00:25.602489
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:50:17.131999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.68
 ---- batch: 020 ----
mean loss: 92.14
 ---- batch: 030 ----
mean loss: 96.11
 ---- batch: 040 ----
mean loss: 92.64
train mean loss: 94.58
epoch train time: 0:00:00.194451
elapsed time: 0:00:25.797056
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:50:17.326582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.28
 ---- batch: 020 ----
mean loss: 92.80
 ---- batch: 030 ----
mean loss: 93.69
 ---- batch: 040 ----
mean loss: 88.27
train mean loss: 91.88
epoch train time: 0:00:00.192888
elapsed time: 0:00:25.990067
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:50:17.519574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.08
 ---- batch: 020 ----
mean loss: 87.44
 ---- batch: 030 ----
mean loss: 85.52
 ---- batch: 040 ----
mean loss: 87.32
train mean loss: 87.73
epoch train time: 0:00:00.196514
elapsed time: 0:00:26.186692
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:50:17.716235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.52
 ---- batch: 020 ----
mean loss: 84.29
 ---- batch: 030 ----
mean loss: 85.86
 ---- batch: 040 ----
mean loss: 83.72
train mean loss: 84.92
epoch train time: 0:00:00.195745
elapsed time: 0:00:26.382577
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:50:17.912085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.55
 ---- batch: 020 ----
mean loss: 80.77
 ---- batch: 030 ----
mean loss: 79.89
 ---- batch: 040 ----
mean loss: 83.15
train mean loss: 81.73
epoch train time: 0:00:00.193628
elapsed time: 0:00:26.576326
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:50:18.105850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.50
 ---- batch: 020 ----
mean loss: 80.58
 ---- batch: 030 ----
mean loss: 78.56
 ---- batch: 040 ----
mean loss: 78.73
train mean loss: 79.90
epoch train time: 0:00:00.195970
elapsed time: 0:00:26.772433
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:50:18.301943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.80
 ---- batch: 020 ----
mean loss: 77.35
 ---- batch: 030 ----
mean loss: 79.50
 ---- batch: 040 ----
mean loss: 76.56
train mean loss: 77.62
epoch train time: 0:00:00.204073
elapsed time: 0:00:26.976625
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:50:18.506139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.18
 ---- batch: 020 ----
mean loss: 76.05
 ---- batch: 030 ----
mean loss: 73.65
 ---- batch: 040 ----
mean loss: 75.69
train mean loss: 75.69
epoch train time: 0:00:00.205686
elapsed time: 0:00:27.182433
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:50:18.711960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.90
 ---- batch: 020 ----
mean loss: 76.07
 ---- batch: 030 ----
mean loss: 75.47
 ---- batch: 040 ----
mean loss: 75.63
train mean loss: 74.41
epoch train time: 0:00:00.212122
elapsed time: 0:00:27.394701
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:50:18.924242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.52
 ---- batch: 020 ----
mean loss: 72.68
 ---- batch: 030 ----
mean loss: 75.46
 ---- batch: 040 ----
mean loss: 70.69
train mean loss: 72.95
epoch train time: 0:00:00.202130
elapsed time: 0:00:27.596978
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:50:19.126491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.40
 ---- batch: 020 ----
mean loss: 68.57
 ---- batch: 030 ----
mean loss: 70.56
 ---- batch: 040 ----
mean loss: 72.45
train mean loss: 70.24
epoch train time: 0:00:00.203949
elapsed time: 0:00:27.801048
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:50:19.330560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.78
 ---- batch: 020 ----
mean loss: 70.26
 ---- batch: 030 ----
mean loss: 67.32
 ---- batch: 040 ----
mean loss: 68.97
train mean loss: 69.20
epoch train time: 0:00:00.210707
elapsed time: 0:00:28.011877
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:50:19.541389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.09
 ---- batch: 020 ----
mean loss: 67.16
 ---- batch: 030 ----
mean loss: 68.55
 ---- batch: 040 ----
mean loss: 66.55
train mean loss: 67.68
epoch train time: 0:00:00.213014
elapsed time: 0:00:28.225018
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:50:19.754540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.06
 ---- batch: 020 ----
mean loss: 68.71
 ---- batch: 030 ----
mean loss: 69.04
 ---- batch: 040 ----
mean loss: 64.18
train mean loss: 67.22
epoch train time: 0:00:00.219611
elapsed time: 0:00:28.444759
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:50:19.974272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.00
 ---- batch: 020 ----
mean loss: 64.85
 ---- batch: 030 ----
mean loss: 66.83
 ---- batch: 040 ----
mean loss: 67.80
train mean loss: 66.82
epoch train time: 0:00:00.207044
elapsed time: 0:00:28.651924
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:50:20.181434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.38
 ---- batch: 020 ----
mean loss: 63.26
 ---- batch: 030 ----
mean loss: 62.34
 ---- batch: 040 ----
mean loss: 66.62
train mean loss: 64.99
epoch train time: 0:00:00.208150
elapsed time: 0:00:28.860203
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:50:20.389716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.61
 ---- batch: 020 ----
mean loss: 61.97
 ---- batch: 030 ----
mean loss: 63.41
 ---- batch: 040 ----
mean loss: 65.01
train mean loss: 63.92
epoch train time: 0:00:00.209970
elapsed time: 0:00:29.070290
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:50:20.599801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.57
 ---- batch: 020 ----
mean loss: 65.10
 ---- batch: 030 ----
mean loss: 63.78
 ---- batch: 040 ----
mean loss: 61.49
train mean loss: 63.06
epoch train time: 0:00:00.206173
elapsed time: 0:00:29.276584
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:50:20.806104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.15
 ---- batch: 020 ----
mean loss: 66.11
 ---- batch: 030 ----
mean loss: 62.61
 ---- batch: 040 ----
mean loss: 62.21
train mean loss: 62.50
epoch train time: 0:00:00.204411
elapsed time: 0:00:29.481121
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:50:21.010647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.31
 ---- batch: 020 ----
mean loss: 59.40
 ---- batch: 030 ----
mean loss: 62.46
 ---- batch: 040 ----
mean loss: 63.89
train mean loss: 61.92
epoch train time: 0:00:00.200601
elapsed time: 0:00:29.681852
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:50:21.211378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.58
 ---- batch: 020 ----
mean loss: 62.24
 ---- batch: 030 ----
mean loss: 61.26
 ---- batch: 040 ----
mean loss: 59.89
train mean loss: 61.33
epoch train time: 0:00:00.203294
elapsed time: 0:00:29.885275
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:50:21.414786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.04
 ---- batch: 020 ----
mean loss: 62.52
 ---- batch: 030 ----
mean loss: 61.31
 ---- batch: 040 ----
mean loss: 57.63
train mean loss: 61.03
epoch train time: 0:00:00.199228
elapsed time: 0:00:30.084664
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:50:21.614173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.30
 ---- batch: 020 ----
mean loss: 56.02
 ---- batch: 030 ----
mean loss: 61.39
 ---- batch: 040 ----
mean loss: 61.94
train mean loss: 60.10
epoch train time: 0:00:00.203581
elapsed time: 0:00:30.288391
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:50:21.817903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.56
 ---- batch: 020 ----
mean loss: 60.72
 ---- batch: 030 ----
mean loss: 59.73
 ---- batch: 040 ----
mean loss: 58.63
train mean loss: 59.33
epoch train time: 0:00:00.205524
elapsed time: 0:00:30.494063
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:50:22.023578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.70
 ---- batch: 020 ----
mean loss: 55.54
 ---- batch: 030 ----
mean loss: 61.31
 ---- batch: 040 ----
mean loss: 59.62
train mean loss: 58.97
epoch train time: 0:00:00.205765
elapsed time: 0:00:30.699946
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:50:22.229456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.95
 ---- batch: 020 ----
mean loss: 57.10
 ---- batch: 030 ----
mean loss: 58.93
 ---- batch: 040 ----
mean loss: 59.29
train mean loss: 58.64
epoch train time: 0:00:00.205306
elapsed time: 0:00:30.905365
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:50:22.434905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.20
 ---- batch: 020 ----
mean loss: 55.50
 ---- batch: 030 ----
mean loss: 57.69
 ---- batch: 040 ----
mean loss: 57.91
train mean loss: 57.77
epoch train time: 0:00:00.213358
elapsed time: 0:00:31.118873
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:50:22.648410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.37
 ---- batch: 020 ----
mean loss: 57.28
 ---- batch: 030 ----
mean loss: 58.20
 ---- batch: 040 ----
mean loss: 60.35
train mean loss: 57.74
epoch train time: 0:00:00.219278
elapsed time: 0:00:31.338305
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:50:22.867834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.99
 ---- batch: 020 ----
mean loss: 56.92
 ---- batch: 030 ----
mean loss: 57.23
 ---- batch: 040 ----
mean loss: 59.07
train mean loss: 58.39
epoch train time: 0:00:00.219384
elapsed time: 0:00:31.557853
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:50:23.087380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.74
 ---- batch: 020 ----
mean loss: 59.17
 ---- batch: 030 ----
mean loss: 57.93
 ---- batch: 040 ----
mean loss: 59.59
train mean loss: 58.19
epoch train time: 0:00:00.214865
elapsed time: 0:00:31.772874
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:50:23.302388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.72
 ---- batch: 020 ----
mean loss: 54.21
 ---- batch: 030 ----
mean loss: 57.21
 ---- batch: 040 ----
mean loss: 54.85
train mean loss: 56.56
epoch train time: 0:00:00.212839
elapsed time: 0:00:31.985864
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:50:23.515375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.80
 ---- batch: 020 ----
mean loss: 55.78
 ---- batch: 030 ----
mean loss: 54.57
 ---- batch: 040 ----
mean loss: 57.62
train mean loss: 56.08
epoch train time: 0:00:00.212154
elapsed time: 0:00:32.198159
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:50:23.727714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.83
 ---- batch: 020 ----
mean loss: 55.27
 ---- batch: 030 ----
mean loss: 54.96
 ---- batch: 040 ----
mean loss: 58.88
train mean loss: 55.85
epoch train time: 0:00:00.220069
elapsed time: 0:00:32.418391
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:50:23.947903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.90
 ---- batch: 020 ----
mean loss: 55.44
 ---- batch: 030 ----
mean loss: 55.50
 ---- batch: 040 ----
mean loss: 53.89
train mean loss: 55.40
epoch train time: 0:00:00.207284
elapsed time: 0:00:32.625795
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:50:24.155311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.93
 ---- batch: 020 ----
mean loss: 53.99
 ---- batch: 030 ----
mean loss: 52.76
 ---- batch: 040 ----
mean loss: 54.21
train mean loss: 54.56
epoch train time: 0:00:00.208598
elapsed time: 0:00:32.834537
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:50:24.364050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.94
 ---- batch: 020 ----
mean loss: 52.53
 ---- batch: 030 ----
mean loss: 54.42
 ---- batch: 040 ----
mean loss: 57.12
train mean loss: 54.42
epoch train time: 0:00:00.208079
elapsed time: 0:00:33.042731
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:50:24.572241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.42
 ---- batch: 020 ----
mean loss: 57.25
 ---- batch: 030 ----
mean loss: 52.41
 ---- batch: 040 ----
mean loss: 55.27
train mean loss: 54.85
epoch train time: 0:00:00.208331
elapsed time: 0:00:33.251187
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:50:24.780704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.80
 ---- batch: 020 ----
mean loss: 54.65
 ---- batch: 030 ----
mean loss: 53.07
 ---- batch: 040 ----
mean loss: 52.60
train mean loss: 53.67
epoch train time: 0:00:00.203252
elapsed time: 0:00:33.454564
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:50:24.984076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.47
 ---- batch: 020 ----
mean loss: 53.65
 ---- batch: 030 ----
mean loss: 55.45
 ---- batch: 040 ----
mean loss: 54.73
train mean loss: 53.92
epoch train time: 0:00:00.196726
elapsed time: 0:00:33.651410
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:50:25.180922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.80
 ---- batch: 020 ----
mean loss: 55.13
 ---- batch: 030 ----
mean loss: 51.60
 ---- batch: 040 ----
mean loss: 53.11
train mean loss: 53.25
epoch train time: 0:00:00.199535
elapsed time: 0:00:33.851059
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:50:25.380569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.18
 ---- batch: 020 ----
mean loss: 55.17
 ---- batch: 030 ----
mean loss: 52.04
 ---- batch: 040 ----
mean loss: 54.81
train mean loss: 53.40
epoch train time: 0:00:00.197424
elapsed time: 0:00:34.048642
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:50:25.578170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.37
 ---- batch: 020 ----
mean loss: 55.34
 ---- batch: 030 ----
mean loss: 54.69
 ---- batch: 040 ----
mean loss: 52.58
train mean loss: 54.37
epoch train time: 0:00:00.197207
elapsed time: 0:00:34.245982
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:50:25.775492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.79
 ---- batch: 020 ----
mean loss: 54.69
 ---- batch: 030 ----
mean loss: 49.04
 ---- batch: 040 ----
mean loss: 52.43
train mean loss: 52.47
epoch train time: 0:00:00.201367
elapsed time: 0:00:34.447492
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:50:25.977012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.75
 ---- batch: 020 ----
mean loss: 52.08
 ---- batch: 030 ----
mean loss: 48.97
 ---- batch: 040 ----
mean loss: 52.78
train mean loss: 52.20
epoch train time: 0:00:00.200848
elapsed time: 0:00:34.648464
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:50:26.178007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.84
 ---- batch: 020 ----
mean loss: 52.21
 ---- batch: 030 ----
mean loss: 52.16
 ---- batch: 040 ----
mean loss: 53.15
train mean loss: 52.36
epoch train time: 0:00:00.209235
elapsed time: 0:00:34.857856
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:50:26.387369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.75
 ---- batch: 020 ----
mean loss: 55.19
 ---- batch: 030 ----
mean loss: 55.29
 ---- batch: 040 ----
mean loss: 53.08
train mean loss: 54.24
epoch train time: 0:00:00.212903
elapsed time: 0:00:35.070878
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:50:26.600409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.62
 ---- batch: 020 ----
mean loss: 51.13
 ---- batch: 030 ----
mean loss: 53.47
 ---- batch: 040 ----
mean loss: 52.18
train mean loss: 51.78
epoch train time: 0:00:00.214002
elapsed time: 0:00:35.285023
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:50:26.814537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.04
 ---- batch: 020 ----
mean loss: 50.13
 ---- batch: 030 ----
mean loss: 51.33
 ---- batch: 040 ----
mean loss: 53.19
train mean loss: 51.72
epoch train time: 0:00:00.213913
elapsed time: 0:00:35.499080
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:50:27.028614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.05
 ---- batch: 020 ----
mean loss: 48.67
 ---- batch: 030 ----
mean loss: 52.59
 ---- batch: 040 ----
mean loss: 53.32
train mean loss: 51.19
epoch train time: 0:00:00.211366
elapsed time: 0:00:35.710587
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:50:27.240099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.43
 ---- batch: 020 ----
mean loss: 51.47
 ---- batch: 030 ----
mean loss: 51.36
 ---- batch: 040 ----
mean loss: 48.70
train mean loss: 50.64
epoch train time: 0:00:00.208728
elapsed time: 0:00:35.919434
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:50:27.448946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.96
 ---- batch: 020 ----
mean loss: 51.28
 ---- batch: 030 ----
mean loss: 47.71
 ---- batch: 040 ----
mean loss: 51.88
train mean loss: 50.50
epoch train time: 0:00:00.208920
elapsed time: 0:00:36.128469
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:50:27.657981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.64
 ---- batch: 020 ----
mean loss: 51.41
 ---- batch: 030 ----
mean loss: 50.56
 ---- batch: 040 ----
mean loss: 55.36
train mean loss: 51.35
epoch train time: 0:00:00.222391
elapsed time: 0:00:36.351003
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:50:27.880517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.61
 ---- batch: 020 ----
mean loss: 49.07
 ---- batch: 030 ----
mean loss: 50.70
 ---- batch: 040 ----
mean loss: 51.12
train mean loss: 50.46
epoch train time: 0:00:00.207319
elapsed time: 0:00:36.558471
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:50:28.087975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.40
 ---- batch: 020 ----
mean loss: 50.64
 ---- batch: 030 ----
mean loss: 48.99
 ---- batch: 040 ----
mean loss: 50.19
train mean loss: 50.32
epoch train time: 0:00:00.210150
elapsed time: 0:00:36.768731
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:50:28.298242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.12
 ---- batch: 020 ----
mean loss: 48.30
 ---- batch: 030 ----
mean loss: 48.76
 ---- batch: 040 ----
mean loss: 53.83
train mean loss: 50.13
epoch train time: 0:00:00.209902
elapsed time: 0:00:36.978752
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:50:28.508265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.04
 ---- batch: 020 ----
mean loss: 47.32
 ---- batch: 030 ----
mean loss: 51.39
 ---- batch: 040 ----
mean loss: 46.94
train mean loss: 49.45
epoch train time: 0:00:00.213203
elapsed time: 0:00:37.192090
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:50:28.721616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.92
 ---- batch: 020 ----
mean loss: 48.41
 ---- batch: 030 ----
mean loss: 51.24
 ---- batch: 040 ----
mean loss: 48.87
train mean loss: 49.25
epoch train time: 0:00:00.204521
elapsed time: 0:00:37.396759
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:50:28.926271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.40
 ---- batch: 020 ----
mean loss: 50.57
 ---- batch: 030 ----
mean loss: 48.65
 ---- batch: 040 ----
mean loss: 48.93
train mean loss: 49.93
epoch train time: 0:00:00.200672
elapsed time: 0:00:37.597544
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:50:29.127051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.63
 ---- batch: 020 ----
mean loss: 50.83
 ---- batch: 030 ----
mean loss: 48.62
 ---- batch: 040 ----
mean loss: 47.73
train mean loss: 49.43
epoch train time: 0:00:00.201725
elapsed time: 0:00:37.799380
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:50:29.328892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.27
 ---- batch: 020 ----
mean loss: 50.48
 ---- batch: 030 ----
mean loss: 48.34
 ---- batch: 040 ----
mean loss: 46.48
train mean loss: 48.67
epoch train time: 0:00:00.202843
elapsed time: 0:00:38.002336
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:50:29.531845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.59
 ---- batch: 020 ----
mean loss: 46.91
 ---- batch: 030 ----
mean loss: 50.93
 ---- batch: 040 ----
mean loss: 49.11
train mean loss: 48.74
epoch train time: 0:00:00.200278
elapsed time: 0:00:38.202758
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:50:29.732284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.89
 ---- batch: 020 ----
mean loss: 50.93
 ---- batch: 030 ----
mean loss: 50.06
 ---- batch: 040 ----
mean loss: 49.64
train mean loss: 50.04
epoch train time: 0:00:00.210874
elapsed time: 0:00:38.413765
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:50:29.943277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.41
 ---- batch: 020 ----
mean loss: 48.36
 ---- batch: 030 ----
mean loss: 47.92
 ---- batch: 040 ----
mean loss: 53.98
train mean loss: 49.31
epoch train time: 0:00:00.206176
elapsed time: 0:00:38.620059
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:50:30.149571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.91
 ---- batch: 020 ----
mean loss: 52.71
 ---- batch: 030 ----
mean loss: 48.75
 ---- batch: 040 ----
mean loss: 46.88
train mean loss: 50.82
epoch train time: 0:00:00.206810
elapsed time: 0:00:38.827043
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:50:30.356558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.78
 ---- batch: 020 ----
mean loss: 46.86
 ---- batch: 030 ----
mean loss: 48.45
 ---- batch: 040 ----
mean loss: 49.81
train mean loss: 48.23
epoch train time: 0:00:00.208166
elapsed time: 0:00:39.036015
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:50:30.565595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.06
 ---- batch: 020 ----
mean loss: 49.09
 ---- batch: 030 ----
mean loss: 50.53
 ---- batch: 040 ----
mean loss: 46.40
train mean loss: 48.82
epoch train time: 0:00:00.209652
elapsed time: 0:00:39.245902
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:50:30.775420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.78
 ---- batch: 020 ----
mean loss: 49.06
 ---- batch: 030 ----
mean loss: 49.11
 ---- batch: 040 ----
mean loss: 48.44
train mean loss: 47.98
epoch train time: 0:00:00.222802
elapsed time: 0:00:39.468840
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:50:30.998356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.49
 ---- batch: 020 ----
mean loss: 49.39
 ---- batch: 030 ----
mean loss: 44.96
 ---- batch: 040 ----
mean loss: 50.68
train mean loss: 47.66
epoch train time: 0:00:00.212847
elapsed time: 0:00:39.681828
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:50:31.211355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.38
 ---- batch: 020 ----
mean loss: 47.71
 ---- batch: 030 ----
mean loss: 47.22
 ---- batch: 040 ----
mean loss: 47.45
train mean loss: 47.44
epoch train time: 0:00:00.213823
elapsed time: 0:00:39.895802
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:50:31.425315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.00
 ---- batch: 020 ----
mean loss: 47.71
 ---- batch: 030 ----
mean loss: 47.59
 ---- batch: 040 ----
mean loss: 45.78
train mean loss: 47.14
epoch train time: 0:00:00.211416
elapsed time: 0:00:40.107337
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:50:31.636849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.92
 ---- batch: 020 ----
mean loss: 45.10
 ---- batch: 030 ----
mean loss: 45.34
 ---- batch: 040 ----
mean loss: 47.02
train mean loss: 46.58
epoch train time: 0:00:00.213449
elapsed time: 0:00:40.320906
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:50:31.850419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.31
 ---- batch: 020 ----
mean loss: 46.41
 ---- batch: 030 ----
mean loss: 49.75
 ---- batch: 040 ----
mean loss: 46.13
train mean loss: 46.77
epoch train time: 0:00:00.210709
elapsed time: 0:00:40.531737
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:50:32.061277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.22
 ---- batch: 020 ----
mean loss: 48.01
 ---- batch: 030 ----
mean loss: 46.85
 ---- batch: 040 ----
mean loss: 46.01
train mean loss: 47.64
epoch train time: 0:00:00.211621
elapsed time: 0:00:40.743520
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:50:32.273025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.01
 ---- batch: 020 ----
mean loss: 45.20
 ---- batch: 030 ----
mean loss: 45.47
 ---- batch: 040 ----
mean loss: 47.10
train mean loss: 46.62
epoch train time: 0:00:00.210213
elapsed time: 0:00:40.953852
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:50:32.483364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.27
 ---- batch: 020 ----
mean loss: 46.62
 ---- batch: 030 ----
mean loss: 45.90
 ---- batch: 040 ----
mean loss: 47.24
train mean loss: 46.34
epoch train time: 0:00:00.212571
elapsed time: 0:00:41.166570
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:50:32.696093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.72
 ---- batch: 020 ----
mean loss: 45.74
 ---- batch: 030 ----
mean loss: 47.39
 ---- batch: 040 ----
mean loss: 45.46
train mean loss: 46.63
epoch train time: 0:00:00.220496
elapsed time: 0:00:41.387201
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:50:32.916753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.11
 ---- batch: 020 ----
mean loss: 47.05
 ---- batch: 030 ----
mean loss: 43.90
 ---- batch: 040 ----
mean loss: 47.61
train mean loss: 46.21
epoch train time: 0:00:00.202775
elapsed time: 0:00:41.590184
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:50:33.119710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.23
 ---- batch: 020 ----
mean loss: 48.40
 ---- batch: 030 ----
mean loss: 45.09
 ---- batch: 040 ----
mean loss: 45.66
train mean loss: 47.24
epoch train time: 0:00:00.204360
elapsed time: 0:00:41.794673
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:50:33.324183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.35
 ---- batch: 020 ----
mean loss: 47.03
 ---- batch: 030 ----
mean loss: 45.19
 ---- batch: 040 ----
mean loss: 43.22
train mean loss: 45.85
epoch train time: 0:00:00.203685
elapsed time: 0:00:41.998471
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:50:33.527983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.63
 ---- batch: 020 ----
mean loss: 45.32
 ---- batch: 030 ----
mean loss: 46.70
 ---- batch: 040 ----
mean loss: 45.19
train mean loss: 45.12
epoch train time: 0:00:00.204707
elapsed time: 0:00:42.203307
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:50:33.732833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.73
 ---- batch: 020 ----
mean loss: 46.37
 ---- batch: 030 ----
mean loss: 45.93
 ---- batch: 040 ----
mean loss: 45.18
train mean loss: 45.83
epoch train time: 0:00:00.204156
elapsed time: 0:00:42.407611
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:50:33.937123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.20
 ---- batch: 020 ----
mean loss: 44.97
 ---- batch: 030 ----
mean loss: 44.87
 ---- batch: 040 ----
mean loss: 46.45
train mean loss: 45.04
epoch train time: 0:00:00.202503
elapsed time: 0:00:42.610228
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:50:34.139737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.07
 ---- batch: 020 ----
mean loss: 46.01
 ---- batch: 030 ----
mean loss: 44.64
 ---- batch: 040 ----
mean loss: 44.40
train mean loss: 44.76
epoch train time: 0:00:00.201759
elapsed time: 0:00:42.812102
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:50:34.341614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.62
 ---- batch: 020 ----
mean loss: 45.13
 ---- batch: 030 ----
mean loss: 46.28
 ---- batch: 040 ----
mean loss: 45.34
train mean loss: 45.33
epoch train time: 0:00:00.200234
elapsed time: 0:00:43.012449
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:50:34.542006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.36
 ---- batch: 020 ----
mean loss: 50.09
 ---- batch: 030 ----
mean loss: 45.94
 ---- batch: 040 ----
mean loss: 44.72
train mean loss: 46.22
epoch train time: 0:00:00.210359
elapsed time: 0:00:43.222997
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:50:34.752516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.81
 ---- batch: 020 ----
mean loss: 43.39
 ---- batch: 030 ----
mean loss: 44.35
 ---- batch: 040 ----
mean loss: 44.31
train mean loss: 44.41
epoch train time: 0:00:00.214715
elapsed time: 0:00:43.437853
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:50:34.967379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.89
 ---- batch: 020 ----
mean loss: 41.99
 ---- batch: 030 ----
mean loss: 44.52
 ---- batch: 040 ----
mean loss: 44.92
train mean loss: 44.07
epoch train time: 0:00:00.213806
elapsed time: 0:00:43.651824
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:50:35.181342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.07
 ---- batch: 020 ----
mean loss: 45.14
 ---- batch: 030 ----
mean loss: 45.67
 ---- batch: 040 ----
mean loss: 44.74
train mean loss: 44.30
epoch train time: 0:00:00.212227
elapsed time: 0:00:43.864176
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:50:35.393689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.90
 ---- batch: 020 ----
mean loss: 46.67
 ---- batch: 030 ----
mean loss: 45.32
 ---- batch: 040 ----
mean loss: 43.60
train mean loss: 44.34
epoch train time: 0:00:00.212362
elapsed time: 0:00:44.076657
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:50:35.606168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.46
 ---- batch: 020 ----
mean loss: 46.32
 ---- batch: 030 ----
mean loss: 44.77
 ---- batch: 040 ----
mean loss: 44.28
train mean loss: 44.85
epoch train time: 0:00:00.212940
elapsed time: 0:00:44.289716
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:50:35.819227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.74
 ---- batch: 020 ----
mean loss: 46.17
 ---- batch: 030 ----
mean loss: 43.63
 ---- batch: 040 ----
mean loss: 42.63
train mean loss: 43.89
epoch train time: 0:00:00.208572
elapsed time: 0:00:44.498436
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:50:36.027947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.89
 ---- batch: 020 ----
mean loss: 41.13
 ---- batch: 030 ----
mean loss: 43.76
 ---- batch: 040 ----
mean loss: 44.58
train mean loss: 43.58
epoch train time: 0:00:00.208851
elapsed time: 0:00:44.707424
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:50:36.236952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.35
 ---- batch: 020 ----
mean loss: 45.10
 ---- batch: 030 ----
mean loss: 42.77
 ---- batch: 040 ----
mean loss: 44.94
train mean loss: 44.03
epoch train time: 0:00:00.208095
elapsed time: 0:00:44.915654
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:50:36.445166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.08
 ---- batch: 020 ----
mean loss: 43.19
 ---- batch: 030 ----
mean loss: 44.86
 ---- batch: 040 ----
mean loss: 42.18
train mean loss: 44.00
epoch train time: 0:00:00.203093
elapsed time: 0:00:45.118867
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:50:36.648381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.90
 ---- batch: 020 ----
mean loss: 45.86
 ---- batch: 030 ----
mean loss: 42.57
 ---- batch: 040 ----
mean loss: 41.48
train mean loss: 43.02
epoch train time: 0:00:00.205631
elapsed time: 0:00:45.324635
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:50:36.854141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.70
 ---- batch: 020 ----
mean loss: 42.71
 ---- batch: 030 ----
mean loss: 43.63
 ---- batch: 040 ----
mean loss: 43.15
train mean loss: 43.12
epoch train time: 0:00:00.203508
elapsed time: 0:00:45.528254
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:50:37.057765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.84
 ---- batch: 020 ----
mean loss: 43.20
 ---- batch: 030 ----
mean loss: 43.50
 ---- batch: 040 ----
mean loss: 45.41
train mean loss: 43.55
epoch train time: 0:00:00.197581
elapsed time: 0:00:45.725954
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:50:37.255465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.01
 ---- batch: 020 ----
mean loss: 43.89
 ---- batch: 030 ----
mean loss: 41.79
 ---- batch: 040 ----
mean loss: 42.48
train mean loss: 42.47
epoch train time: 0:00:00.195855
elapsed time: 0:00:45.921939
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:50:37.451462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.41
 ---- batch: 020 ----
mean loss: 42.33
 ---- batch: 030 ----
mean loss: 43.29
 ---- batch: 040 ----
mean loss: 42.41
train mean loss: 42.51
epoch train time: 0:00:00.197633
elapsed time: 0:00:46.119699
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:50:37.649209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.28
 ---- batch: 020 ----
mean loss: 43.00
 ---- batch: 030 ----
mean loss: 39.54
 ---- batch: 040 ----
mean loss: 41.84
train mean loss: 42.18
epoch train time: 0:00:00.201207
elapsed time: 0:00:46.321019
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:50:37.850529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.03
 ---- batch: 020 ----
mean loss: 42.34
 ---- batch: 030 ----
mean loss: 41.79
 ---- batch: 040 ----
mean loss: 42.46
train mean loss: 42.34
epoch train time: 0:00:00.203348
elapsed time: 0:00:46.524482
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:50:38.053994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.60
 ---- batch: 020 ----
mean loss: 42.07
 ---- batch: 030 ----
mean loss: 43.31
 ---- batch: 040 ----
mean loss: 40.92
train mean loss: 41.96
epoch train time: 0:00:00.197864
elapsed time: 0:00:46.722461
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:50:38.251971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.97
 ---- batch: 020 ----
mean loss: 42.28
 ---- batch: 030 ----
mean loss: 42.74
 ---- batch: 040 ----
mean loss: 41.78
train mean loss: 41.86
epoch train time: 0:00:00.200126
elapsed time: 0:00:46.922698
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:50:38.452207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.96
 ---- batch: 020 ----
mean loss: 42.90
 ---- batch: 030 ----
mean loss: 42.27
 ---- batch: 040 ----
mean loss: 42.85
train mean loss: 41.95
epoch train time: 0:00:00.199146
elapsed time: 0:00:47.121974
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:50:38.651484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.63
 ---- batch: 020 ----
mean loss: 41.67
 ---- batch: 030 ----
mean loss: 41.63
 ---- batch: 040 ----
mean loss: 40.88
train mean loss: 41.38
epoch train time: 0:00:00.201762
elapsed time: 0:00:47.323853
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:50:38.853379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.94
 ---- batch: 020 ----
mean loss: 40.04
 ---- batch: 030 ----
mean loss: 41.57
 ---- batch: 040 ----
mean loss: 44.87
train mean loss: 41.65
epoch train time: 0:00:00.201688
elapsed time: 0:00:47.525676
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:50:39.055190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.68
 ---- batch: 020 ----
mean loss: 40.60
 ---- batch: 030 ----
mean loss: 40.89
 ---- batch: 040 ----
mean loss: 42.03
train mean loss: 41.19
epoch train time: 0:00:00.202220
elapsed time: 0:00:47.728018
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:50:39.257531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.79
 ---- batch: 020 ----
mean loss: 38.86
 ---- batch: 030 ----
mean loss: 41.57
 ---- batch: 040 ----
mean loss: 42.15
train mean loss: 40.89
epoch train time: 0:00:00.205909
elapsed time: 0:00:47.934049
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:50:39.463563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.11
 ---- batch: 020 ----
mean loss: 41.55
 ---- batch: 030 ----
mean loss: 38.93
 ---- batch: 040 ----
mean loss: 42.62
train mean loss: 40.59
epoch train time: 0:00:00.205572
elapsed time: 0:00:48.139749
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:50:39.669263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.26
 ---- batch: 020 ----
mean loss: 39.46
 ---- batch: 030 ----
mean loss: 41.05
 ---- batch: 040 ----
mean loss: 41.58
train mean loss: 40.27
epoch train time: 0:00:00.205347
elapsed time: 0:00:48.345231
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:50:39.874742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.50
 ---- batch: 020 ----
mean loss: 41.18
 ---- batch: 030 ----
mean loss: 41.80
 ---- batch: 040 ----
mean loss: 39.88
train mean loss: 40.56
epoch train time: 0:00:00.201946
elapsed time: 0:00:48.547295
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:50:40.076806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.58
 ---- batch: 020 ----
mean loss: 40.71
 ---- batch: 030 ----
mean loss: 39.99
 ---- batch: 040 ----
mean loss: 41.85
train mean loss: 40.94
epoch train time: 0:00:00.201729
elapsed time: 0:00:48.749141
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:50:40.278669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.78
 ---- batch: 020 ----
mean loss: 40.07
 ---- batch: 030 ----
mean loss: 41.03
 ---- batch: 040 ----
mean loss: 37.90
train mean loss: 40.22
epoch train time: 0:00:00.203554
elapsed time: 0:00:48.952841
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:50:40.482354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.97
 ---- batch: 020 ----
mean loss: 41.00
 ---- batch: 030 ----
mean loss: 39.26
 ---- batch: 040 ----
mean loss: 38.67
train mean loss: 39.76
epoch train time: 0:00:00.202574
elapsed time: 0:00:49.155538
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:50:40.685053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.67
 ---- batch: 020 ----
mean loss: 40.67
 ---- batch: 030 ----
mean loss: 44.38
 ---- batch: 040 ----
mean loss: 38.80
train mean loss: 40.93
epoch train time: 0:00:00.203375
elapsed time: 0:00:49.359037
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:50:40.888567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.43
 ---- batch: 020 ----
mean loss: 39.04
 ---- batch: 030 ----
mean loss: 42.80
 ---- batch: 040 ----
mean loss: 41.42
train mean loss: 40.40
epoch train time: 0:00:00.200664
elapsed time: 0:00:49.559878
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:50:41.089389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.69
 ---- batch: 020 ----
mean loss: 38.94
 ---- batch: 030 ----
mean loss: 40.09
 ---- batch: 040 ----
mean loss: 40.31
train mean loss: 39.92
epoch train time: 0:00:00.198536
elapsed time: 0:00:49.758528
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:50:41.288053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.83
 ---- batch: 020 ----
mean loss: 38.40
 ---- batch: 030 ----
mean loss: 41.12
 ---- batch: 040 ----
mean loss: 40.40
train mean loss: 39.58
epoch train time: 0:00:00.194876
elapsed time: 0:00:49.953536
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:50:41.483044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.75
 ---- batch: 020 ----
mean loss: 40.96
 ---- batch: 030 ----
mean loss: 39.25
 ---- batch: 040 ----
mean loss: 42.61
train mean loss: 40.57
epoch train time: 0:00:00.194697
elapsed time: 0:00:50.148357
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:50:41.677876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.28
 ---- batch: 020 ----
mean loss: 42.57
 ---- batch: 030 ----
mean loss: 40.78
 ---- batch: 040 ----
mean loss: 39.70
train mean loss: 40.20
epoch train time: 0:00:00.202534
elapsed time: 0:00:50.351040
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:50:41.880580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.07
 ---- batch: 020 ----
mean loss: 39.29
 ---- batch: 030 ----
mean loss: 39.29
 ---- batch: 040 ----
mean loss: 38.38
train mean loss: 39.13
epoch train time: 0:00:00.199174
elapsed time: 0:00:50.550371
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:50:42.079881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.64
 ---- batch: 020 ----
mean loss: 40.51
 ---- batch: 030 ----
mean loss: 39.95
 ---- batch: 040 ----
mean loss: 38.93
train mean loss: 39.82
epoch train time: 0:00:00.195958
elapsed time: 0:00:50.746441
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:50:42.275950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.14
 ---- batch: 020 ----
mean loss: 39.48
 ---- batch: 030 ----
mean loss: 39.70
 ---- batch: 040 ----
mean loss: 41.96
train mean loss: 39.64
epoch train time: 0:00:00.198614
elapsed time: 0:00:50.945172
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:50:42.474685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.73
 ---- batch: 020 ----
mean loss: 37.36
 ---- batch: 030 ----
mean loss: 37.00
 ---- batch: 040 ----
mean loss: 40.11
train mean loss: 38.78
epoch train time: 0:00:00.196836
elapsed time: 0:00:51.142140
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:50:42.671650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.10
 ---- batch: 020 ----
mean loss: 39.98
 ---- batch: 030 ----
mean loss: 38.00
 ---- batch: 040 ----
mean loss: 39.24
train mean loss: 39.48
epoch train time: 0:00:00.204723
elapsed time: 0:00:51.346995
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:50:42.876506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.33
 ---- batch: 020 ----
mean loss: 43.87
 ---- batch: 030 ----
mean loss: 38.48
 ---- batch: 040 ----
mean loss: 37.51
train mean loss: 40.28
epoch train time: 0:00:00.198802
elapsed time: 0:00:51.545914
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:50:43.075425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.40
 ---- batch: 020 ----
mean loss: 40.29
 ---- batch: 030 ----
mean loss: 37.71
 ---- batch: 040 ----
mean loss: 37.01
train mean loss: 38.36
epoch train time: 0:00:00.207023
elapsed time: 0:00:51.753055
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:50:43.282572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.15
 ---- batch: 020 ----
mean loss: 37.08
 ---- batch: 030 ----
mean loss: 37.78
 ---- batch: 040 ----
mean loss: 38.74
train mean loss: 38.03
epoch train time: 0:00:00.205807
elapsed time: 0:00:51.959008
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:50:43.488519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.94
 ---- batch: 020 ----
mean loss: 37.41
 ---- batch: 030 ----
mean loss: 35.54
 ---- batch: 040 ----
mean loss: 39.64
train mean loss: 37.67
epoch train time: 0:00:00.203416
elapsed time: 0:00:52.162539
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:50:43.692065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.82
 ---- batch: 020 ----
mean loss: 37.87
 ---- batch: 030 ----
mean loss: 42.35
 ---- batch: 040 ----
mean loss: 39.45
train mean loss: 39.22
epoch train time: 0:00:00.205750
elapsed time: 0:00:52.368435
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:50:43.897946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.08
 ---- batch: 020 ----
mean loss: 40.61
 ---- batch: 030 ----
mean loss: 36.52
 ---- batch: 040 ----
mean loss: 39.95
train mean loss: 38.49
epoch train time: 0:00:00.204289
elapsed time: 0:00:52.572860
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:50:44.102373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.30
 ---- batch: 020 ----
mean loss: 35.68
 ---- batch: 030 ----
mean loss: 41.08
 ---- batch: 040 ----
mean loss: 37.89
train mean loss: 38.31
epoch train time: 0:00:00.205883
elapsed time: 0:00:52.778866
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:50:44.308380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.03
 ---- batch: 020 ----
mean loss: 39.05
 ---- batch: 030 ----
mean loss: 38.13
 ---- batch: 040 ----
mean loss: 32.91
train mean loss: 37.44
epoch train time: 0:00:00.210196
elapsed time: 0:00:52.989190
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:50:44.518705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.96
 ---- batch: 020 ----
mean loss: 40.48
 ---- batch: 030 ----
mean loss: 38.56
 ---- batch: 040 ----
mean loss: 39.87
train mean loss: 38.64
epoch train time: 0:00:00.206917
elapsed time: 0:00:53.196246
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:50:44.725761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.23
 ---- batch: 020 ----
mean loss: 36.89
 ---- batch: 030 ----
mean loss: 37.23
 ---- batch: 040 ----
mean loss: 38.90
train mean loss: 37.39
epoch train time: 0:00:00.206959
elapsed time: 0:00:53.403330
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:50:44.932843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.39
 ---- batch: 020 ----
mean loss: 36.89
 ---- batch: 030 ----
mean loss: 38.83
 ---- batch: 040 ----
mean loss: 41.39
train mean loss: 38.07
epoch train time: 0:00:00.201130
elapsed time: 0:00:53.604606
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:50:45.134116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.29
 ---- batch: 020 ----
mean loss: 38.83
 ---- batch: 030 ----
mean loss: 35.39
 ---- batch: 040 ----
mean loss: 37.73
train mean loss: 37.27
epoch train time: 0:00:00.199338
elapsed time: 0:00:53.804053
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:50:45.333576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.08
 ---- batch: 020 ----
mean loss: 35.24
 ---- batch: 030 ----
mean loss: 38.02
 ---- batch: 040 ----
mean loss: 36.38
train mean loss: 36.55
epoch train time: 0:00:00.194667
elapsed time: 0:00:53.998847
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:50:45.528359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.00
 ---- batch: 020 ----
mean loss: 36.68
 ---- batch: 030 ----
mean loss: 38.72
 ---- batch: 040 ----
mean loss: 35.44
train mean loss: 37.40
epoch train time: 0:00:00.196534
elapsed time: 0:00:54.195501
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:50:45.725013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.72
 ---- batch: 020 ----
mean loss: 35.97
 ---- batch: 030 ----
mean loss: 38.49
 ---- batch: 040 ----
mean loss: 36.77
train mean loss: 37.26
epoch train time: 0:00:00.199001
elapsed time: 0:00:54.394617
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:50:45.924141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.51
 ---- batch: 020 ----
mean loss: 39.64
 ---- batch: 030 ----
mean loss: 35.53
 ---- batch: 040 ----
mean loss: 35.91
train mean loss: 37.32
epoch train time: 0:00:00.198764
elapsed time: 0:00:54.593523
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:50:46.123048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.42
 ---- batch: 020 ----
mean loss: 36.25
 ---- batch: 030 ----
mean loss: 38.70
 ---- batch: 040 ----
mean loss: 38.49
train mean loss: 37.30
epoch train time: 0:00:00.197862
elapsed time: 0:00:54.791514
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:50:46.321024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.74
 ---- batch: 020 ----
mean loss: 38.61
 ---- batch: 030 ----
mean loss: 37.04
 ---- batch: 040 ----
mean loss: 38.36
train mean loss: 37.59
epoch train time: 0:00:00.198977
elapsed time: 0:00:54.990608
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:50:46.520119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.33
 ---- batch: 020 ----
mean loss: 36.82
 ---- batch: 030 ----
mean loss: 36.77
 ---- batch: 040 ----
mean loss: 34.80
train mean loss: 36.68
epoch train time: 0:00:00.199980
elapsed time: 0:00:55.190703
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:50:46.720213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.35
 ---- batch: 020 ----
mean loss: 35.60
 ---- batch: 030 ----
mean loss: 36.81
 ---- batch: 040 ----
mean loss: 36.54
train mean loss: 35.90
epoch train time: 0:00:00.202659
elapsed time: 0:00:55.393476
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:50:46.922984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.92
 ---- batch: 020 ----
mean loss: 35.85
 ---- batch: 030 ----
mean loss: 36.28
 ---- batch: 040 ----
mean loss: 36.61
train mean loss: 36.08
epoch train time: 0:00:00.197367
elapsed time: 0:00:55.590977
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:50:47.120496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.11
 ---- batch: 020 ----
mean loss: 35.58
 ---- batch: 030 ----
mean loss: 35.99
 ---- batch: 040 ----
mean loss: 36.55
train mean loss: 36.11
epoch train time: 0:00:00.211495
elapsed time: 0:00:55.802598
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:50:47.332111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.93
 ---- batch: 020 ----
mean loss: 33.96
 ---- batch: 030 ----
mean loss: 34.72
 ---- batch: 040 ----
mean loss: 36.18
train mean loss: 34.60
epoch train time: 0:00:00.205335
elapsed time: 0:00:56.008073
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:50:47.537577
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.33
 ---- batch: 020 ----
mean loss: 35.34
 ---- batch: 030 ----
mean loss: 35.08
 ---- batch: 040 ----
mean loss: 33.96
train mean loss: 34.45
epoch train time: 0:00:00.209048
elapsed time: 0:00:56.217239
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:50:47.746763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.82
 ---- batch: 020 ----
mean loss: 34.85
 ---- batch: 030 ----
mean loss: 33.72
 ---- batch: 040 ----
mean loss: 33.41
train mean loss: 34.40
epoch train time: 0:00:00.215667
elapsed time: 0:00:56.433040
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:50:47.962553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.79
 ---- batch: 020 ----
mean loss: 34.84
 ---- batch: 030 ----
mean loss: 34.43
 ---- batch: 040 ----
mean loss: 35.05
train mean loss: 34.37
epoch train time: 0:00:00.205213
elapsed time: 0:00:56.638389
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:50:48.167901
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.52
 ---- batch: 020 ----
mean loss: 33.85
 ---- batch: 030 ----
mean loss: 35.30
 ---- batch: 040 ----
mean loss: 34.43
train mean loss: 34.41
epoch train time: 0:00:00.208301
elapsed time: 0:00:56.846822
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:50:48.376336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.84
 ---- batch: 020 ----
mean loss: 33.98
 ---- batch: 030 ----
mean loss: 35.02
 ---- batch: 040 ----
mean loss: 33.64
train mean loss: 34.41
epoch train time: 0:00:00.205212
elapsed time: 0:00:57.052159
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:50:48.581687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.87
 ---- batch: 020 ----
mean loss: 33.35
 ---- batch: 030 ----
mean loss: 34.14
 ---- batch: 040 ----
mean loss: 34.09
train mean loss: 34.50
epoch train time: 0:00:00.207161
elapsed time: 0:00:57.259461
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:50:48.788975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.29
 ---- batch: 020 ----
mean loss: 34.83
 ---- batch: 030 ----
mean loss: 32.62
 ---- batch: 040 ----
mean loss: 33.31
train mean loss: 34.31
epoch train time: 0:00:00.205239
elapsed time: 0:00:57.464843
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:50:48.994356
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.62
 ---- batch: 020 ----
mean loss: 34.42
 ---- batch: 030 ----
mean loss: 32.98
 ---- batch: 040 ----
mean loss: 36.83
train mean loss: 34.37
epoch train time: 0:00:00.203526
elapsed time: 0:00:57.668487
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:50:49.197997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.71
 ---- batch: 020 ----
mean loss: 35.06
 ---- batch: 030 ----
mean loss: 34.96
 ---- batch: 040 ----
mean loss: 32.84
train mean loss: 34.34
epoch train time: 0:00:00.200819
elapsed time: 0:00:57.869420
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:50:49.398928
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.11
 ---- batch: 020 ----
mean loss: 33.64
 ---- batch: 030 ----
mean loss: 33.87
 ---- batch: 040 ----
mean loss: 33.75
train mean loss: 34.32
epoch train time: 0:00:00.196244
elapsed time: 0:00:58.065774
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:50:49.595292
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.00
 ---- batch: 020 ----
mean loss: 33.56
 ---- batch: 030 ----
mean loss: 33.40
 ---- batch: 040 ----
mean loss: 35.39
train mean loss: 34.30
epoch train time: 0:00:00.196627
elapsed time: 0:00:58.262528
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:50:49.792041
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.18
 ---- batch: 020 ----
mean loss: 32.40
 ---- batch: 030 ----
mean loss: 33.40
 ---- batch: 040 ----
mean loss: 35.34
train mean loss: 34.29
epoch train time: 0:00:00.200746
elapsed time: 0:00:58.463393
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:50:49.992903
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.55
 ---- batch: 020 ----
mean loss: 34.05
 ---- batch: 030 ----
mean loss: 34.45
 ---- batch: 040 ----
mean loss: 35.03
train mean loss: 34.25
epoch train time: 0:00:00.198373
elapsed time: 0:00:58.661879
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:50:50.191388
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.97
 ---- batch: 020 ----
mean loss: 33.66
 ---- batch: 030 ----
mean loss: 34.80
 ---- batch: 040 ----
mean loss: 34.94
train mean loss: 34.21
epoch train time: 0:00:00.200261
elapsed time: 0:00:58.862263
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:50:50.391777
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.73
 ---- batch: 020 ----
mean loss: 34.34
 ---- batch: 030 ----
mean loss: 34.08
 ---- batch: 040 ----
mean loss: 31.78
train mean loss: 34.26
epoch train time: 0:00:00.199210
elapsed time: 0:00:59.061592
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:50:50.591103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.15
 ---- batch: 020 ----
mean loss: 34.17
 ---- batch: 030 ----
mean loss: 34.93
 ---- batch: 040 ----
mean loss: 33.41
train mean loss: 34.26
epoch train time: 0:00:00.199561
elapsed time: 0:00:59.261270
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:50:50.790784
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.80
 ---- batch: 020 ----
mean loss: 36.06
 ---- batch: 030 ----
mean loss: 33.46
 ---- batch: 040 ----
mean loss: 34.95
train mean loss: 34.19
epoch train time: 0:00:00.202171
elapsed time: 0:00:59.463558
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:50:50.993080
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.63
 ---- batch: 020 ----
mean loss: 34.92
 ---- batch: 030 ----
mean loss: 33.89
 ---- batch: 040 ----
mean loss: 33.06
train mean loss: 34.28
epoch train time: 0:00:00.201652
elapsed time: 0:00:59.665339
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:50:51.194878
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.07
 ---- batch: 020 ----
mean loss: 34.51
 ---- batch: 030 ----
mean loss: 35.64
 ---- batch: 040 ----
mean loss: 33.90
train mean loss: 34.24
epoch train time: 0:00:00.202762
elapsed time: 0:00:59.868287
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:50:51.397810
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.11
 ---- batch: 020 ----
mean loss: 33.73
 ---- batch: 030 ----
mean loss: 33.90
 ---- batch: 040 ----
mean loss: 35.75
train mean loss: 34.24
epoch train time: 0:00:00.205088
elapsed time: 0:01:00.073506
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:50:51.603017
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.55
 ---- batch: 020 ----
mean loss: 35.27
 ---- batch: 030 ----
mean loss: 33.25
 ---- batch: 040 ----
mean loss: 34.51
train mean loss: 34.25
epoch train time: 0:00:00.205820
elapsed time: 0:01:00.279448
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:50:51.808961
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.43
 ---- batch: 020 ----
mean loss: 33.50
 ---- batch: 030 ----
mean loss: 33.66
 ---- batch: 040 ----
mean loss: 34.67
train mean loss: 34.07
epoch train time: 0:00:00.205815
elapsed time: 0:01:00.485398
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:50:52.014914
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.18
 ---- batch: 020 ----
mean loss: 34.89
 ---- batch: 030 ----
mean loss: 33.24
 ---- batch: 040 ----
mean loss: 34.46
train mean loss: 34.15
epoch train time: 0:00:00.205248
elapsed time: 0:01:00.690774
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:50:52.220300
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.45
 ---- batch: 020 ----
mean loss: 34.62
 ---- batch: 030 ----
mean loss: 34.22
 ---- batch: 040 ----
mean loss: 33.93
train mean loss: 34.15
epoch train time: 0:00:00.209594
elapsed time: 0:01:00.900502
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:50:52.430015
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.25
 ---- batch: 020 ----
mean loss: 34.27
 ---- batch: 030 ----
mean loss: 34.53
 ---- batch: 040 ----
mean loss: 34.16
train mean loss: 34.06
epoch train time: 0:00:00.205538
elapsed time: 0:01:01.106162
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:50:52.635675
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.33
 ---- batch: 020 ----
mean loss: 34.47
 ---- batch: 030 ----
mean loss: 34.70
 ---- batch: 040 ----
mean loss: 33.03
train mean loss: 33.99
epoch train time: 0:00:00.203697
elapsed time: 0:01:01.309978
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:50:52.839491
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.52
 ---- batch: 020 ----
mean loss: 32.04
 ---- batch: 030 ----
mean loss: 35.71
 ---- batch: 040 ----
mean loss: 34.37
train mean loss: 34.07
epoch train time: 0:00:00.206708
elapsed time: 0:01:01.516811
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:50:53.046354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.06
 ---- batch: 020 ----
mean loss: 34.81
 ---- batch: 030 ----
mean loss: 34.20
 ---- batch: 040 ----
mean loss: 35.32
train mean loss: 33.99
epoch train time: 0:00:00.209214
elapsed time: 0:01:01.726195
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:50:53.255709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.54
 ---- batch: 020 ----
mean loss: 33.93
 ---- batch: 030 ----
mean loss: 34.46
 ---- batch: 040 ----
mean loss: 32.92
train mean loss: 34.05
epoch train time: 0:00:00.205647
elapsed time: 0:01:01.931975
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:50:53.461502
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.25
 ---- batch: 020 ----
mean loss: 32.32
 ---- batch: 030 ----
mean loss: 34.08
 ---- batch: 040 ----
mean loss: 35.24
train mean loss: 34.15
epoch train time: 0:00:00.200802
elapsed time: 0:01:02.132911
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:50:53.662422
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.32
 ---- batch: 020 ----
mean loss: 34.16
 ---- batch: 030 ----
mean loss: 34.35
 ---- batch: 040 ----
mean loss: 32.48
train mean loss: 33.98
epoch train time: 0:00:00.198700
elapsed time: 0:01:02.331725
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:50:53.861234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.91
 ---- batch: 020 ----
mean loss: 35.93
 ---- batch: 030 ----
mean loss: 32.70
 ---- batch: 040 ----
mean loss: 33.07
train mean loss: 34.22
epoch train time: 0:00:00.198596
elapsed time: 0:01:02.530447
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:50:54.059951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.83
 ---- batch: 020 ----
mean loss: 33.82
 ---- batch: 030 ----
mean loss: 34.42
 ---- batch: 040 ----
mean loss: 34.31
train mean loss: 34.03
epoch train time: 0:00:00.209255
elapsed time: 0:01:02.739810
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:50:54.269320
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.91
 ---- batch: 020 ----
mean loss: 33.08
 ---- batch: 030 ----
mean loss: 32.12
 ---- batch: 040 ----
mean loss: 34.69
train mean loss: 33.96
epoch train time: 0:00:00.192708
elapsed time: 0:01:02.932636
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:50:54.462147
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.74
 ---- batch: 020 ----
mean loss: 33.78
 ---- batch: 030 ----
mean loss: 34.12
 ---- batch: 040 ----
mean loss: 33.86
train mean loss: 33.87
epoch train time: 0:00:00.193865
elapsed time: 0:01:03.126638
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:50:54.656169
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.86
 ---- batch: 020 ----
mean loss: 32.10
 ---- batch: 030 ----
mean loss: 34.46
 ---- batch: 040 ----
mean loss: 33.25
train mean loss: 33.94
epoch train time: 0:00:00.204980
elapsed time: 0:01:03.331758
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:50:54.861271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.73
 ---- batch: 020 ----
mean loss: 34.66
 ---- batch: 030 ----
mean loss: 33.13
 ---- batch: 040 ----
mean loss: 33.93
train mean loss: 33.96
epoch train time: 0:00:00.198075
elapsed time: 0:01:03.529949
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:50:55.059476
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.55
 ---- batch: 020 ----
mean loss: 33.85
 ---- batch: 030 ----
mean loss: 35.26
 ---- batch: 040 ----
mean loss: 34.62
train mean loss: 33.87
epoch train time: 0:00:00.202943
elapsed time: 0:01:03.733043
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:50:55.262555
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.34
 ---- batch: 020 ----
mean loss: 35.11
 ---- batch: 030 ----
mean loss: 31.86
 ---- batch: 040 ----
mean loss: 34.61
train mean loss: 33.86
epoch train time: 0:00:00.199694
elapsed time: 0:01:03.932863
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:50:55.462380
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.37
 ---- batch: 020 ----
mean loss: 33.81
 ---- batch: 030 ----
mean loss: 33.91
 ---- batch: 040 ----
mean loss: 34.82
train mean loss: 33.83
epoch train time: 0:00:00.206629
elapsed time: 0:01:04.139620
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:50:55.669135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.89
 ---- batch: 020 ----
mean loss: 33.79
 ---- batch: 030 ----
mean loss: 34.03
 ---- batch: 040 ----
mean loss: 33.41
train mean loss: 33.77
epoch train time: 0:00:00.208955
elapsed time: 0:01:04.348702
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:50:55.878234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.18
 ---- batch: 020 ----
mean loss: 32.64
 ---- batch: 030 ----
mean loss: 34.27
 ---- batch: 040 ----
mean loss: 34.12
train mean loss: 33.80
epoch train time: 0:00:00.205554
elapsed time: 0:01:04.554398
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:50:56.083913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.14
 ---- batch: 020 ----
mean loss: 33.92
 ---- batch: 030 ----
mean loss: 34.51
 ---- batch: 040 ----
mean loss: 33.81
train mean loss: 33.72
epoch train time: 0:00:00.201469
elapsed time: 0:01:04.755988
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:50:56.285500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.41
 ---- batch: 020 ----
mean loss: 34.56
 ---- batch: 030 ----
mean loss: 34.15
 ---- batch: 040 ----
mean loss: 34.05
train mean loss: 33.77
epoch train time: 0:00:00.201008
elapsed time: 0:01:04.957112
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:50:56.486624
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.07
 ---- batch: 020 ----
mean loss: 32.15
 ---- batch: 030 ----
mean loss: 35.12
 ---- batch: 040 ----
mean loss: 35.78
train mean loss: 33.77
epoch train time: 0:00:00.201494
elapsed time: 0:01:05.158729
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:50:56.688244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.54
 ---- batch: 020 ----
mean loss: 34.16
 ---- batch: 030 ----
mean loss: 33.01
 ---- batch: 040 ----
mean loss: 34.15
train mean loss: 33.78
epoch train time: 0:00:00.207303
elapsed time: 0:01:05.366156
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:50:56.895670
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.05
 ---- batch: 020 ----
mean loss: 33.31
 ---- batch: 030 ----
mean loss: 34.37
 ---- batch: 040 ----
mean loss: 32.47
train mean loss: 33.82
epoch train time: 0:00:00.204921
elapsed time: 0:01:05.571282
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:50:57.100798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.67
 ---- batch: 020 ----
mean loss: 34.25
 ---- batch: 030 ----
mean loss: 33.42
 ---- batch: 040 ----
mean loss: 32.91
train mean loss: 33.70
epoch train time: 0:00:00.207132
elapsed time: 0:01:05.781864
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_9/checkpoint.pth.tar
**** end time: 2019-09-20 23:50:57.311351 ****
