Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_3', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 15891
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-27 00:16:56.404621 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 16, 24]             100
              Tanh-2           [-1, 10, 16, 24]               0
            Conv2d-3           [-1, 10, 15, 24]           1,000
              Tanh-4           [-1, 10, 15, 24]               0
            Conv2d-5           [-1, 10, 16, 24]           1,000
              Tanh-6           [-1, 10, 16, 24]               0
            Conv2d-7           [-1, 10, 15, 24]           1,000
              Tanh-8           [-1, 10, 15, 24]               0
            Conv2d-9            [-1, 1, 15, 24]              30
             Tanh-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
          Dropout-12                  [-1, 360]               0
           Linear-13                  [-1, 100]          36,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 39,230
Trainable params: 39,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 00:16:56.413907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4837.45
 ---- batch: 020 ----
mean loss: 3530.98
 ---- batch: 030 ----
mean loss: 1708.88
 ---- batch: 040 ----
mean loss: 1333.22
 ---- batch: 050 ----
mean loss: 1238.26
 ---- batch: 060 ----
mean loss: 1120.73
 ---- batch: 070 ----
mean loss: 1079.51
 ---- batch: 080 ----
mean loss: 1048.35
 ---- batch: 090 ----
mean loss: 985.27
 ---- batch: 100 ----
mean loss: 973.78
 ---- batch: 110 ----
mean loss: 939.88
train mean loss: 1688.34
epoch train time: 0:00:33.888897
elapsed time: 0:00:33.900976
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 00:17:30.305714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.85
 ---- batch: 020 ----
mean loss: 885.54
 ---- batch: 030 ----
mean loss: 856.02
 ---- batch: 040 ----
mean loss: 852.49
 ---- batch: 050 ----
mean loss: 807.08
 ---- batch: 060 ----
mean loss: 810.23
 ---- batch: 070 ----
mean loss: 812.76
 ---- batch: 080 ----
mean loss: 782.35
 ---- batch: 090 ----
mean loss: 791.07
 ---- batch: 100 ----
mean loss: 782.99
 ---- batch: 110 ----
mean loss: 791.58
train mean loss: 823.10
epoch train time: 0:00:02.254821
elapsed time: 0:00:36.156021
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 00:17:32.560701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 738.67
 ---- batch: 020 ----
mean loss: 754.37
 ---- batch: 030 ----
mean loss: 731.33
 ---- batch: 040 ----
mean loss: 732.89
 ---- batch: 050 ----
mean loss: 728.45
 ---- batch: 060 ----
mean loss: 712.23
 ---- batch: 070 ----
mean loss: 725.74
 ---- batch: 080 ----
mean loss: 707.74
 ---- batch: 090 ----
mean loss: 701.50
 ---- batch: 100 ----
mean loss: 687.80
 ---- batch: 110 ----
mean loss: 689.59
train mean loss: 717.29
epoch train time: 0:00:02.187345
elapsed time: 0:00:38.343532
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 00:17:34.748221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.37
 ---- batch: 020 ----
mean loss: 690.43
 ---- batch: 030 ----
mean loss: 667.61
 ---- batch: 040 ----
mean loss: 660.23
 ---- batch: 050 ----
mean loss: 661.72
 ---- batch: 060 ----
mean loss: 659.17
 ---- batch: 070 ----
mean loss: 651.55
 ---- batch: 080 ----
mean loss: 641.35
 ---- batch: 090 ----
mean loss: 643.10
 ---- batch: 100 ----
mean loss: 644.53
 ---- batch: 110 ----
mean loss: 648.24
train mean loss: 660.07
epoch train time: 0:00:02.177186
elapsed time: 0:00:40.520900
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 00:17:36.925668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 625.08
 ---- batch: 020 ----
mean loss: 632.06
 ---- batch: 030 ----
mean loss: 636.04
 ---- batch: 040 ----
mean loss: 630.41
 ---- batch: 050 ----
mean loss: 626.16
 ---- batch: 060 ----
mean loss: 632.77
 ---- batch: 070 ----
mean loss: 615.92
 ---- batch: 080 ----
mean loss: 625.01
 ---- batch: 090 ----
mean loss: 619.86
 ---- batch: 100 ----
mean loss: 613.97
 ---- batch: 110 ----
mean loss: 630.82
train mean loss: 625.87
epoch train time: 0:00:02.180510
elapsed time: 0:00:42.701676
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 00:17:39.106350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 625.58
 ---- batch: 020 ----
mean loss: 616.51
 ---- batch: 030 ----
mean loss: 606.49
 ---- batch: 040 ----
mean loss: 591.70
 ---- batch: 050 ----
mean loss: 608.54
 ---- batch: 060 ----
mean loss: 594.78
 ---- batch: 070 ----
mean loss: 597.52
 ---- batch: 080 ----
mean loss: 611.18
 ---- batch: 090 ----
mean loss: 606.29
 ---- batch: 100 ----
mean loss: 603.97
 ---- batch: 110 ----
mean loss: 601.41
train mean loss: 605.25
epoch train time: 0:00:02.173809
elapsed time: 0:00:44.875651
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 00:17:41.280327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 592.04
 ---- batch: 020 ----
mean loss: 591.67
 ---- batch: 030 ----
mean loss: 587.25
 ---- batch: 040 ----
mean loss: 586.08
 ---- batch: 050 ----
mean loss: 583.03
 ---- batch: 060 ----
mean loss: 574.15
 ---- batch: 070 ----
mean loss: 568.58
 ---- batch: 080 ----
mean loss: 580.80
 ---- batch: 090 ----
mean loss: 592.60
 ---- batch: 100 ----
mean loss: 603.39
 ---- batch: 110 ----
mean loss: 581.99
train mean loss: 585.81
epoch train time: 0:00:02.168646
elapsed time: 0:00:47.044481
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 00:17:43.449158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.42
 ---- batch: 020 ----
mean loss: 567.34
 ---- batch: 030 ----
mean loss: 588.56
 ---- batch: 040 ----
mean loss: 576.28
 ---- batch: 050 ----
mean loss: 583.65
 ---- batch: 060 ----
mean loss: 573.82
 ---- batch: 070 ----
mean loss: 577.64
 ---- batch: 080 ----
mean loss: 580.15
 ---- batch: 090 ----
mean loss: 551.39
 ---- batch: 100 ----
mean loss: 587.74
 ---- batch: 110 ----
mean loss: 577.02
train mean loss: 577.97
epoch train time: 0:00:02.166309
elapsed time: 0:00:49.210966
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 00:17:45.615640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 567.80
 ---- batch: 020 ----
mean loss: 571.28
 ---- batch: 030 ----
mean loss: 574.85
 ---- batch: 040 ----
mean loss: 575.18
 ---- batch: 050 ----
mean loss: 557.85
 ---- batch: 060 ----
mean loss: 560.87
 ---- batch: 070 ----
mean loss: 574.05
 ---- batch: 080 ----
mean loss: 562.20
 ---- batch: 090 ----
mean loss: 594.20
 ---- batch: 100 ----
mean loss: 604.66
 ---- batch: 110 ----
mean loss: 585.67
train mean loss: 575.13
epoch train time: 0:00:02.171034
elapsed time: 0:00:51.382192
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 00:17:47.786870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.68
 ---- batch: 020 ----
mean loss: 567.10
 ---- batch: 030 ----
mean loss: 553.97
 ---- batch: 040 ----
mean loss: 563.37
 ---- batch: 050 ----
mean loss: 562.73
 ---- batch: 060 ----
mean loss: 581.02
 ---- batch: 070 ----
mean loss: 550.18
 ---- batch: 080 ----
mean loss: 562.33
 ---- batch: 090 ----
mean loss: 551.74
 ---- batch: 100 ----
mean loss: 553.02
 ---- batch: 110 ----
mean loss: 569.51
train mean loss: 560.82
epoch train time: 0:00:02.169327
elapsed time: 0:00:53.551693
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 00:17:49.956385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.34
 ---- batch: 020 ----
mean loss: 551.21
 ---- batch: 030 ----
mean loss: 571.15
 ---- batch: 040 ----
mean loss: 578.45
 ---- batch: 050 ----
mean loss: 554.60
 ---- batch: 060 ----
mean loss: 572.26
 ---- batch: 070 ----
mean loss: 564.09
 ---- batch: 080 ----
mean loss: 565.94
 ---- batch: 090 ----
mean loss: 553.85
 ---- batch: 100 ----
mean loss: 552.94
 ---- batch: 110 ----
mean loss: 548.01
train mean loss: 561.64
epoch train time: 0:00:02.166136
elapsed time: 0:00:55.718004
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 00:17:52.122697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 571.32
 ---- batch: 020 ----
mean loss: 557.80
 ---- batch: 030 ----
mean loss: 565.81
 ---- batch: 040 ----
mean loss: 563.38
 ---- batch: 050 ----
mean loss: 553.31
 ---- batch: 060 ----
mean loss: 546.18
 ---- batch: 070 ----
mean loss: 545.92
 ---- batch: 080 ----
mean loss: 551.22
 ---- batch: 090 ----
mean loss: 555.27
 ---- batch: 100 ----
mean loss: 541.00
 ---- batch: 110 ----
mean loss: 516.65
train mean loss: 551.77
epoch train time: 0:00:02.170018
elapsed time: 0:00:57.888219
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 00:17:54.292927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 568.16
 ---- batch: 020 ----
mean loss: 559.92
 ---- batch: 030 ----
mean loss: 535.15
 ---- batch: 040 ----
mean loss: 557.25
 ---- batch: 050 ----
mean loss: 533.93
 ---- batch: 060 ----
mean loss: 560.30
 ---- batch: 070 ----
mean loss: 541.14
 ---- batch: 080 ----
mean loss: 548.01
 ---- batch: 090 ----
mean loss: 540.65
 ---- batch: 100 ----
mean loss: 551.53
 ---- batch: 110 ----
mean loss: 534.96
train mean loss: 548.08
epoch train time: 0:00:02.163612
elapsed time: 0:01:00.052031
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 00:17:56.456719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 554.79
 ---- batch: 020 ----
mean loss: 545.28
 ---- batch: 030 ----
mean loss: 542.67
 ---- batch: 040 ----
mean loss: 526.86
 ---- batch: 050 ----
mean loss: 537.04
 ---- batch: 060 ----
mean loss: 544.17
 ---- batch: 070 ----
mean loss: 546.17
 ---- batch: 080 ----
mean loss: 540.23
 ---- batch: 090 ----
mean loss: 532.34
 ---- batch: 100 ----
mean loss: 556.90
 ---- batch: 110 ----
mean loss: 547.50
train mean loss: 543.86
epoch train time: 0:00:02.167628
elapsed time: 0:01:02.219822
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 00:17:58.624497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.29
 ---- batch: 020 ----
mean loss: 541.81
 ---- batch: 030 ----
mean loss: 570.46
 ---- batch: 040 ----
mean loss: 563.21
 ---- batch: 050 ----
mean loss: 539.70
 ---- batch: 060 ----
mean loss: 544.44
 ---- batch: 070 ----
mean loss: 545.28
 ---- batch: 080 ----
mean loss: 548.81
 ---- batch: 090 ----
mean loss: 533.12
 ---- batch: 100 ----
mean loss: 534.58
 ---- batch: 110 ----
mean loss: 532.09
train mean loss: 545.01
epoch train time: 0:00:02.158530
elapsed time: 0:01:04.378516
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 00:18:00.783191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.17
 ---- batch: 020 ----
mean loss: 545.30
 ---- batch: 030 ----
mean loss: 542.42
 ---- batch: 040 ----
mean loss: 538.62
 ---- batch: 050 ----
mean loss: 523.66
 ---- batch: 060 ----
mean loss: 543.80
 ---- batch: 070 ----
mean loss: 527.35
 ---- batch: 080 ----
mean loss: 537.20
 ---- batch: 090 ----
mean loss: 545.12
 ---- batch: 100 ----
mean loss: 523.14
 ---- batch: 110 ----
mean loss: 521.20
train mean loss: 534.89
epoch train time: 0:00:02.164498
elapsed time: 0:01:06.543174
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 00:18:02.947855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 551.60
 ---- batch: 020 ----
mean loss: 522.21
 ---- batch: 030 ----
mean loss: 552.18
 ---- batch: 040 ----
mean loss: 532.94
 ---- batch: 050 ----
mean loss: 550.39
 ---- batch: 060 ----
mean loss: 538.63
 ---- batch: 070 ----
mean loss: 535.06
 ---- batch: 080 ----
mean loss: 542.98
 ---- batch: 090 ----
mean loss: 519.84
 ---- batch: 100 ----
mean loss: 513.29
 ---- batch: 110 ----
mean loss: 535.54
train mean loss: 535.93
epoch train time: 0:00:02.163987
elapsed time: 0:01:08.707353
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 00:18:05.112061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.28
 ---- batch: 020 ----
mean loss: 543.45
 ---- batch: 030 ----
mean loss: 522.94
 ---- batch: 040 ----
mean loss: 528.07
 ---- batch: 050 ----
mean loss: 537.34
 ---- batch: 060 ----
mean loss: 521.69
 ---- batch: 070 ----
mean loss: 520.81
 ---- batch: 080 ----
mean loss: 516.16
 ---- batch: 090 ----
mean loss: 522.27
 ---- batch: 100 ----
mean loss: 533.66
 ---- batch: 110 ----
mean loss: 525.44
train mean loss: 525.68
epoch train time: 0:00:02.175453
elapsed time: 0:01:10.883027
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 00:18:07.287701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 522.84
 ---- batch: 020 ----
mean loss: 540.35
 ---- batch: 030 ----
mean loss: 522.39
 ---- batch: 040 ----
mean loss: 522.92
 ---- batch: 050 ----
mean loss: 544.34
 ---- batch: 060 ----
mean loss: 549.72
 ---- batch: 070 ----
mean loss: 532.31
 ---- batch: 080 ----
mean loss: 528.40
 ---- batch: 090 ----
mean loss: 503.88
 ---- batch: 100 ----
mean loss: 529.85
 ---- batch: 110 ----
mean loss: 523.79
train mean loss: 528.53
epoch train time: 0:00:02.184720
elapsed time: 0:01:13.067987
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 00:18:09.472688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.51
 ---- batch: 020 ----
mean loss: 509.51
 ---- batch: 030 ----
mean loss: 533.03
 ---- batch: 040 ----
mean loss: 505.30
 ---- batch: 050 ----
mean loss: 509.60
 ---- batch: 060 ----
mean loss: 518.98
 ---- batch: 070 ----
mean loss: 519.23
 ---- batch: 080 ----
mean loss: 529.82
 ---- batch: 090 ----
mean loss: 516.10
 ---- batch: 100 ----
mean loss: 509.93
 ---- batch: 110 ----
mean loss: 517.47
train mean loss: 518.43
epoch train time: 0:00:02.178929
elapsed time: 0:01:15.247095
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 00:18:11.651767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.51
 ---- batch: 020 ----
mean loss: 512.03
 ---- batch: 030 ----
mean loss: 516.59
 ---- batch: 040 ----
mean loss: 532.20
 ---- batch: 050 ----
mean loss: 530.16
 ---- batch: 060 ----
mean loss: 513.72
 ---- batch: 070 ----
mean loss: 524.37
 ---- batch: 080 ----
mean loss: 516.97
 ---- batch: 090 ----
mean loss: 508.17
 ---- batch: 100 ----
mean loss: 500.54
 ---- batch: 110 ----
mean loss: 507.75
train mean loss: 514.71
epoch train time: 0:00:02.194489
elapsed time: 0:01:17.441748
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 00:18:13.846425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.27
 ---- batch: 020 ----
mean loss: 535.91
 ---- batch: 030 ----
mean loss: 516.18
 ---- batch: 040 ----
mean loss: 519.02
 ---- batch: 050 ----
mean loss: 520.61
 ---- batch: 060 ----
mean loss: 511.29
 ---- batch: 070 ----
mean loss: 523.46
 ---- batch: 080 ----
mean loss: 506.84
 ---- batch: 090 ----
mean loss: 510.32
 ---- batch: 100 ----
mean loss: 513.91
 ---- batch: 110 ----
mean loss: 547.31
train mean loss: 518.90
epoch train time: 0:00:02.198237
elapsed time: 0:01:19.640150
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 00:18:16.044824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.86
 ---- batch: 020 ----
mean loss: 504.13
 ---- batch: 030 ----
mean loss: 519.83
 ---- batch: 040 ----
mean loss: 514.61
 ---- batch: 050 ----
mean loss: 509.32
 ---- batch: 060 ----
mean loss: 526.38
 ---- batch: 070 ----
mean loss: 526.19
 ---- batch: 080 ----
mean loss: 510.32
 ---- batch: 090 ----
mean loss: 499.13
 ---- batch: 100 ----
mean loss: 510.83
 ---- batch: 110 ----
mean loss: 513.56
train mean loss: 513.50
epoch train time: 0:00:02.200642
elapsed time: 0:01:21.840956
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 00:18:18.245651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.59
 ---- batch: 020 ----
mean loss: 514.37
 ---- batch: 030 ----
mean loss: 503.98
 ---- batch: 040 ----
mean loss: 514.00
 ---- batch: 050 ----
mean loss: 502.93
 ---- batch: 060 ----
mean loss: 508.33
 ---- batch: 070 ----
mean loss: 522.50
 ---- batch: 080 ----
mean loss: 514.71
 ---- batch: 090 ----
mean loss: 502.99
 ---- batch: 100 ----
mean loss: 513.40
 ---- batch: 110 ----
mean loss: 500.24
train mean loss: 508.90
epoch train time: 0:00:02.197445
elapsed time: 0:01:24.038598
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 00:18:20.443292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.03
 ---- batch: 020 ----
mean loss: 502.43
 ---- batch: 030 ----
mean loss: 521.33
 ---- batch: 040 ----
mean loss: 523.16
 ---- batch: 050 ----
mean loss: 512.13
 ---- batch: 060 ----
mean loss: 517.54
 ---- batch: 070 ----
mean loss: 507.73
 ---- batch: 080 ----
mean loss: 509.65
 ---- batch: 090 ----
mean loss: 496.75
 ---- batch: 100 ----
mean loss: 502.20
 ---- batch: 110 ----
mean loss: 495.93
train mean loss: 507.91
epoch train time: 0:00:02.190180
elapsed time: 0:01:26.229023
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 00:18:22.633702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.36
 ---- batch: 020 ----
mean loss: 525.85
 ---- batch: 030 ----
mean loss: 501.41
 ---- batch: 040 ----
mean loss: 506.91
 ---- batch: 050 ----
mean loss: 511.07
 ---- batch: 060 ----
mean loss: 512.75
 ---- batch: 070 ----
mean loss: 509.41
 ---- batch: 080 ----
mean loss: 508.55
 ---- batch: 090 ----
mean loss: 491.26
 ---- batch: 100 ----
mean loss: 499.92
 ---- batch: 110 ----
mean loss: 504.58
train mean loss: 506.03
epoch train time: 0:00:02.185513
elapsed time: 0:01:28.414740
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 00:18:24.819420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.66
 ---- batch: 020 ----
mean loss: 519.87
 ---- batch: 030 ----
mean loss: 503.67
 ---- batch: 040 ----
mean loss: 503.05
 ---- batch: 050 ----
mean loss: 507.68
 ---- batch: 060 ----
mean loss: 499.08
 ---- batch: 070 ----
mean loss: 494.61
 ---- batch: 080 ----
mean loss: 498.29
 ---- batch: 090 ----
mean loss: 492.53
 ---- batch: 100 ----
mean loss: 509.51
 ---- batch: 110 ----
mean loss: 506.54
train mean loss: 504.44
epoch train time: 0:00:02.182100
elapsed time: 0:01:30.597035
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 00:18:27.001713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.54
 ---- batch: 020 ----
mean loss: 509.93
 ---- batch: 030 ----
mean loss: 519.43
 ---- batch: 040 ----
mean loss: 515.49
 ---- batch: 050 ----
mean loss: 515.17
 ---- batch: 060 ----
mean loss: 491.00
 ---- batch: 070 ----
mean loss: 504.45
 ---- batch: 080 ----
mean loss: 504.55
 ---- batch: 090 ----
mean loss: 521.67
 ---- batch: 100 ----
mean loss: 501.64
 ---- batch: 110 ----
mean loss: 509.00
train mean loss: 509.11
epoch train time: 0:00:02.184521
elapsed time: 0:01:32.781747
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 00:18:29.186426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.95
 ---- batch: 020 ----
mean loss: 512.61
 ---- batch: 030 ----
mean loss: 506.43
 ---- batch: 040 ----
mean loss: 511.79
 ---- batch: 050 ----
mean loss: 509.76
 ---- batch: 060 ----
mean loss: 491.38
 ---- batch: 070 ----
mean loss: 512.86
 ---- batch: 080 ----
mean loss: 508.23
 ---- batch: 090 ----
mean loss: 483.81
 ---- batch: 100 ----
mean loss: 496.43
 ---- batch: 110 ----
mean loss: 497.66
train mean loss: 501.46
epoch train time: 0:00:02.188832
elapsed time: 0:01:34.970793
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 00:18:31.375470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.30
 ---- batch: 020 ----
mean loss: 490.46
 ---- batch: 030 ----
mean loss: 487.82
 ---- batch: 040 ----
mean loss: 490.74
 ---- batch: 050 ----
mean loss: 507.12
 ---- batch: 060 ----
mean loss: 519.82
 ---- batch: 070 ----
mean loss: 496.77
 ---- batch: 080 ----
mean loss: 502.74
 ---- batch: 090 ----
mean loss: 493.17
 ---- batch: 100 ----
mean loss: 518.84
 ---- batch: 110 ----
mean loss: 522.62
train mean loss: 502.46
epoch train time: 0:00:02.184424
elapsed time: 0:01:37.155391
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 00:18:33.560068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.83
 ---- batch: 020 ----
mean loss: 499.41
 ---- batch: 030 ----
mean loss: 502.21
 ---- batch: 040 ----
mean loss: 497.71
 ---- batch: 050 ----
mean loss: 486.90
 ---- batch: 060 ----
mean loss: 504.12
 ---- batch: 070 ----
mean loss: 492.55
 ---- batch: 080 ----
mean loss: 502.82
 ---- batch: 090 ----
mean loss: 505.23
 ---- batch: 100 ----
mean loss: 500.91
 ---- batch: 110 ----
mean loss: 497.24
train mean loss: 499.13
epoch train time: 0:00:02.191927
elapsed time: 0:01:39.347491
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 00:18:35.752188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.63
 ---- batch: 020 ----
mean loss: 505.39
 ---- batch: 030 ----
mean loss: 494.64
 ---- batch: 040 ----
mean loss: 511.98
 ---- batch: 050 ----
mean loss: 489.98
 ---- batch: 060 ----
mean loss: 498.35
 ---- batch: 070 ----
mean loss: 483.11
 ---- batch: 080 ----
mean loss: 487.54
 ---- batch: 090 ----
mean loss: 499.80
 ---- batch: 100 ----
mean loss: 495.80
 ---- batch: 110 ----
mean loss: 503.04
train mean loss: 496.58
epoch train time: 0:00:02.189644
elapsed time: 0:01:41.537330
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 00:18:37.942006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.68
 ---- batch: 020 ----
mean loss: 484.10
 ---- batch: 030 ----
mean loss: 499.56
 ---- batch: 040 ----
mean loss: 483.67
 ---- batch: 050 ----
mean loss: 494.03
 ---- batch: 060 ----
mean loss: 498.77
 ---- batch: 070 ----
mean loss: 487.42
 ---- batch: 080 ----
mean loss: 510.48
 ---- batch: 090 ----
mean loss: 492.07
 ---- batch: 100 ----
mean loss: 490.76
 ---- batch: 110 ----
mean loss: 490.68
train mean loss: 493.40
epoch train time: 0:00:02.190102
elapsed time: 0:01:43.727614
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 00:18:40.132293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.75
 ---- batch: 020 ----
mean loss: 492.68
 ---- batch: 030 ----
mean loss: 498.28
 ---- batch: 040 ----
mean loss: 483.23
 ---- batch: 050 ----
mean loss: 501.26
 ---- batch: 060 ----
mean loss: 486.73
 ---- batch: 070 ----
mean loss: 511.06
 ---- batch: 080 ----
mean loss: 500.03
 ---- batch: 090 ----
mean loss: 509.43
 ---- batch: 100 ----
mean loss: 489.55
 ---- batch: 110 ----
mean loss: 492.75
train mean loss: 497.03
epoch train time: 0:00:02.190368
elapsed time: 0:01:45.918176
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 00:18:42.322860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.95
 ---- batch: 020 ----
mean loss: 489.64
 ---- batch: 030 ----
mean loss: 480.81
 ---- batch: 040 ----
mean loss: 504.55
 ---- batch: 050 ----
mean loss: 493.41
 ---- batch: 060 ----
mean loss: 493.36
 ---- batch: 070 ----
mean loss: 491.50
 ---- batch: 080 ----
mean loss: 519.77
 ---- batch: 090 ----
mean loss: 514.37
 ---- batch: 100 ----
mean loss: 504.73
 ---- batch: 110 ----
mean loss: 513.91
train mean loss: 500.66
epoch train time: 0:00:02.188868
elapsed time: 0:01:48.107227
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 00:18:44.511920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 528.47
 ---- batch: 020 ----
mean loss: 500.23
 ---- batch: 030 ----
mean loss: 480.23
 ---- batch: 040 ----
mean loss: 483.71
 ---- batch: 050 ----
mean loss: 498.20
 ---- batch: 060 ----
mean loss: 497.06
 ---- batch: 070 ----
mean loss: 500.38
 ---- batch: 080 ----
mean loss: 512.44
 ---- batch: 090 ----
mean loss: 488.43
 ---- batch: 100 ----
mean loss: 514.47
 ---- batch: 110 ----
mean loss: 504.84
train mean loss: 500.29
epoch train time: 0:00:02.180770
elapsed time: 0:01:50.288193
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 00:18:46.692869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 521.35
 ---- batch: 020 ----
mean loss: 487.20
 ---- batch: 030 ----
mean loss: 496.09
 ---- batch: 040 ----
mean loss: 497.39
 ---- batch: 050 ----
mean loss: 502.04
 ---- batch: 060 ----
mean loss: 505.64
 ---- batch: 070 ----
mean loss: 497.12
 ---- batch: 080 ----
mean loss: 497.83
 ---- batch: 090 ----
mean loss: 497.89
 ---- batch: 100 ----
mean loss: 492.88
 ---- batch: 110 ----
mean loss: 496.01
train mean loss: 497.93
epoch train time: 0:00:02.191135
elapsed time: 0:01:52.479502
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 00:18:48.884201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.16
 ---- batch: 020 ----
mean loss: 526.52
 ---- batch: 030 ----
mean loss: 507.23
 ---- batch: 040 ----
mean loss: 486.86
 ---- batch: 050 ----
mean loss: 510.74
 ---- batch: 060 ----
mean loss: 488.58
 ---- batch: 070 ----
mean loss: 500.38
 ---- batch: 080 ----
mean loss: 487.48
 ---- batch: 090 ----
mean loss: 480.76
 ---- batch: 100 ----
mean loss: 504.38
 ---- batch: 110 ----
mean loss: 516.83
train mean loss: 501.84
epoch train time: 0:00:02.186141
elapsed time: 0:01:54.665827
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 00:18:51.070500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.93
 ---- batch: 020 ----
mean loss: 496.24
 ---- batch: 030 ----
mean loss: 501.88
 ---- batch: 040 ----
mean loss: 503.23
 ---- batch: 050 ----
mean loss: 480.05
 ---- batch: 060 ----
mean loss: 495.54
 ---- batch: 070 ----
mean loss: 493.99
 ---- batch: 080 ----
mean loss: 496.79
 ---- batch: 090 ----
mean loss: 497.34
 ---- batch: 100 ----
mean loss: 487.15
 ---- batch: 110 ----
mean loss: 478.11
train mean loss: 494.12
epoch train time: 0:00:02.187970
elapsed time: 0:01:56.853968
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 00:18:53.258643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.12
 ---- batch: 020 ----
mean loss: 509.71
 ---- batch: 030 ----
mean loss: 499.91
 ---- batch: 040 ----
mean loss: 489.42
 ---- batch: 050 ----
mean loss: 497.93
 ---- batch: 060 ----
mean loss: 505.84
 ---- batch: 070 ----
mean loss: 495.79
 ---- batch: 080 ----
mean loss: 499.20
 ---- batch: 090 ----
mean loss: 503.38
 ---- batch: 100 ----
mean loss: 491.14
 ---- batch: 110 ----
mean loss: 492.67
train mean loss: 497.65
epoch train time: 0:00:02.185703
elapsed time: 0:01:59.039867
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 00:18:55.444550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.50
 ---- batch: 020 ----
mean loss: 477.54
 ---- batch: 030 ----
mean loss: 505.17
 ---- batch: 040 ----
mean loss: 511.37
 ---- batch: 050 ----
mean loss: 505.98
 ---- batch: 060 ----
mean loss: 502.78
 ---- batch: 070 ----
mean loss: 490.76
 ---- batch: 080 ----
mean loss: 485.35
 ---- batch: 090 ----
mean loss: 496.41
 ---- batch: 100 ----
mean loss: 493.49
 ---- batch: 110 ----
mean loss: 484.47
train mean loss: 493.95
epoch train time: 0:00:02.193083
elapsed time: 0:02:01.233137
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 00:18:57.637812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.55
 ---- batch: 020 ----
mean loss: 489.18
 ---- batch: 030 ----
mean loss: 497.60
 ---- batch: 040 ----
mean loss: 480.71
 ---- batch: 050 ----
mean loss: 506.27
 ---- batch: 060 ----
mean loss: 500.76
 ---- batch: 070 ----
mean loss: 508.32
 ---- batch: 080 ----
mean loss: 509.10
 ---- batch: 090 ----
mean loss: 497.84
 ---- batch: 100 ----
mean loss: 503.29
 ---- batch: 110 ----
mean loss: 477.02
train mean loss: 496.80
epoch train time: 0:00:02.183213
elapsed time: 0:02:03.416540
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 00:18:59.821222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.31
 ---- batch: 020 ----
mean loss: 490.55
 ---- batch: 030 ----
mean loss: 506.09
 ---- batch: 040 ----
mean loss: 501.61
 ---- batch: 050 ----
mean loss: 496.81
 ---- batch: 060 ----
mean loss: 481.79
 ---- batch: 070 ----
mean loss: 496.18
 ---- batch: 080 ----
mean loss: 503.66
 ---- batch: 090 ----
mean loss: 493.96
 ---- batch: 100 ----
mean loss: 491.01
 ---- batch: 110 ----
mean loss: 498.15
train mean loss: 496.74
epoch train time: 0:00:02.181355
elapsed time: 0:02:05.598078
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 00:19:02.002771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.93
 ---- batch: 020 ----
mean loss: 482.16
 ---- batch: 030 ----
mean loss: 492.11
 ---- batch: 040 ----
mean loss: 495.52
 ---- batch: 050 ----
mean loss: 498.23
 ---- batch: 060 ----
mean loss: 484.07
 ---- batch: 070 ----
mean loss: 482.03
 ---- batch: 080 ----
mean loss: 510.87
 ---- batch: 090 ----
mean loss: 479.21
 ---- batch: 100 ----
mean loss: 489.42
 ---- batch: 110 ----
mean loss: 502.51
train mean loss: 493.53
epoch train time: 0:00:02.181092
elapsed time: 0:02:07.779375
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 00:19:04.184067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.70
 ---- batch: 020 ----
mean loss: 484.80
 ---- batch: 030 ----
mean loss: 489.38
 ---- batch: 040 ----
mean loss: 499.49
 ---- batch: 050 ----
mean loss: 500.38
 ---- batch: 060 ----
mean loss: 504.27
 ---- batch: 070 ----
mean loss: 489.92
 ---- batch: 080 ----
mean loss: 489.89
 ---- batch: 090 ----
mean loss: 497.32
 ---- batch: 100 ----
mean loss: 478.13
 ---- batch: 110 ----
mean loss: 475.50
train mean loss: 490.06
epoch train time: 0:00:02.191035
elapsed time: 0:02:09.970613
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 00:19:06.375292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.03
 ---- batch: 020 ----
mean loss: 495.55
 ---- batch: 030 ----
mean loss: 500.05
 ---- batch: 040 ----
mean loss: 497.87
 ---- batch: 050 ----
mean loss: 474.65
 ---- batch: 060 ----
mean loss: 486.21
 ---- batch: 070 ----
mean loss: 487.32
 ---- batch: 080 ----
mean loss: 488.90
 ---- batch: 090 ----
mean loss: 486.16
 ---- batch: 100 ----
mean loss: 492.06
 ---- batch: 110 ----
mean loss: 480.49
train mean loss: 488.00
epoch train time: 0:00:02.187447
elapsed time: 0:02:12.158245
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 00:19:08.562923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.80
 ---- batch: 020 ----
mean loss: 495.35
 ---- batch: 030 ----
mean loss: 476.48
 ---- batch: 040 ----
mean loss: 492.12
 ---- batch: 050 ----
mean loss: 482.05
 ---- batch: 060 ----
mean loss: 499.50
 ---- batch: 070 ----
mean loss: 497.95
 ---- batch: 080 ----
mean loss: 476.27
 ---- batch: 090 ----
mean loss: 485.90
 ---- batch: 100 ----
mean loss: 484.62
 ---- batch: 110 ----
mean loss: 493.32
train mean loss: 488.41
epoch train time: 0:00:02.186466
elapsed time: 0:02:14.344940
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 00:19:10.749651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.84
 ---- batch: 020 ----
mean loss: 498.93
 ---- batch: 030 ----
mean loss: 496.20
 ---- batch: 040 ----
mean loss: 490.88
 ---- batch: 050 ----
mean loss: 489.53
 ---- batch: 060 ----
mean loss: 490.52
 ---- batch: 070 ----
mean loss: 504.84
 ---- batch: 080 ----
mean loss: 500.02
 ---- batch: 090 ----
mean loss: 487.22
 ---- batch: 100 ----
mean loss: 473.14
 ---- batch: 110 ----
mean loss: 488.40
train mean loss: 490.51
epoch train time: 0:00:02.191984
elapsed time: 0:02:16.537150
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 00:19:12.941829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.25
 ---- batch: 020 ----
mean loss: 490.04
 ---- batch: 030 ----
mean loss: 489.82
 ---- batch: 040 ----
mean loss: 492.90
 ---- batch: 050 ----
mean loss: 479.20
 ---- batch: 060 ----
mean loss: 487.11
 ---- batch: 070 ----
mean loss: 496.18
 ---- batch: 080 ----
mean loss: 496.88
 ---- batch: 090 ----
mean loss: 492.99
 ---- batch: 100 ----
mean loss: 491.19
 ---- batch: 110 ----
mean loss: 489.11
train mean loss: 489.65
epoch train time: 0:00:02.189336
elapsed time: 0:02:18.726653
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 00:19:15.131332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.73
 ---- batch: 020 ----
mean loss: 504.07
 ---- batch: 030 ----
mean loss: 481.35
 ---- batch: 040 ----
mean loss: 492.86
 ---- batch: 050 ----
mean loss: 490.99
 ---- batch: 060 ----
mean loss: 492.34
 ---- batch: 070 ----
mean loss: 469.25
 ---- batch: 080 ----
mean loss: 500.98
 ---- batch: 090 ----
mean loss: 496.36
 ---- batch: 100 ----
mean loss: 484.94
 ---- batch: 110 ----
mean loss: 483.54
train mean loss: 490.95
epoch train time: 0:00:02.192268
elapsed time: 0:02:20.919117
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 00:19:17.323816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.97
 ---- batch: 020 ----
mean loss: 475.66
 ---- batch: 030 ----
mean loss: 485.14
 ---- batch: 040 ----
mean loss: 483.81
 ---- batch: 050 ----
mean loss: 496.94
 ---- batch: 060 ----
mean loss: 479.99
 ---- batch: 070 ----
mean loss: 512.97
 ---- batch: 080 ----
mean loss: 497.90
 ---- batch: 090 ----
mean loss: 483.23
 ---- batch: 100 ----
mean loss: 470.86
 ---- batch: 110 ----
mean loss: 477.33
train mean loss: 487.19
epoch train time: 0:00:02.186647
elapsed time: 0:02:23.105983
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 00:19:19.510663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.30
 ---- batch: 020 ----
mean loss: 502.46
 ---- batch: 030 ----
mean loss: 498.70
 ---- batch: 040 ----
mean loss: 492.84
 ---- batch: 050 ----
mean loss: 489.98
 ---- batch: 060 ----
mean loss: 479.38
 ---- batch: 070 ----
mean loss: 486.63
 ---- batch: 080 ----
mean loss: 477.92
 ---- batch: 090 ----
mean loss: 484.94
 ---- batch: 100 ----
mean loss: 486.67
 ---- batch: 110 ----
mean loss: 483.65
train mean loss: 487.00
epoch train time: 0:00:02.186811
elapsed time: 0:02:25.292978
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 00:19:21.697676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.81
 ---- batch: 020 ----
mean loss: 490.69
 ---- batch: 030 ----
mean loss: 494.70
 ---- batch: 040 ----
mean loss: 478.73
 ---- batch: 050 ----
mean loss: 481.54
 ---- batch: 060 ----
mean loss: 475.43
 ---- batch: 070 ----
mean loss: 492.40
 ---- batch: 080 ----
mean loss: 477.29
 ---- batch: 090 ----
mean loss: 492.50
 ---- batch: 100 ----
mean loss: 490.39
 ---- batch: 110 ----
mean loss: 490.92
train mean loss: 487.75
epoch train time: 0:00:02.183736
elapsed time: 0:02:27.476953
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 00:19:23.881651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.43
 ---- batch: 020 ----
mean loss: 486.94
 ---- batch: 030 ----
mean loss: 497.37
 ---- batch: 040 ----
mean loss: 488.69
 ---- batch: 050 ----
mean loss: 482.66
 ---- batch: 060 ----
mean loss: 486.38
 ---- batch: 070 ----
mean loss: 475.80
 ---- batch: 080 ----
mean loss: 474.39
 ---- batch: 090 ----
mean loss: 474.85
 ---- batch: 100 ----
mean loss: 478.89
 ---- batch: 110 ----
mean loss: 468.40
train mean loss: 480.74
epoch train time: 0:00:02.179401
elapsed time: 0:02:29.656561
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 00:19:26.061239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.78
 ---- batch: 020 ----
mean loss: 482.29
 ---- batch: 030 ----
mean loss: 489.99
 ---- batch: 040 ----
mean loss: 484.52
 ---- batch: 050 ----
mean loss: 479.97
 ---- batch: 060 ----
mean loss: 482.75
 ---- batch: 070 ----
mean loss: 482.86
 ---- batch: 080 ----
mean loss: 471.13
 ---- batch: 090 ----
mean loss: 484.51
 ---- batch: 100 ----
mean loss: 479.48
 ---- batch: 110 ----
mean loss: 489.68
train mean loss: 483.75
epoch train time: 0:00:02.189367
elapsed time: 0:02:31.846109
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 00:19:28.250785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.51
 ---- batch: 020 ----
mean loss: 483.56
 ---- batch: 030 ----
mean loss: 488.19
 ---- batch: 040 ----
mean loss: 484.95
 ---- batch: 050 ----
mean loss: 473.01
 ---- batch: 060 ----
mean loss: 479.82
 ---- batch: 070 ----
mean loss: 483.00
 ---- batch: 080 ----
mean loss: 470.04
 ---- batch: 090 ----
mean loss: 490.69
 ---- batch: 100 ----
mean loss: 491.36
 ---- batch: 110 ----
mean loss: 483.93
train mean loss: 483.81
epoch train time: 0:00:02.189155
elapsed time: 0:02:34.035448
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 00:19:30.440141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.09
 ---- batch: 020 ----
mean loss: 477.63
 ---- batch: 030 ----
mean loss: 496.63
 ---- batch: 040 ----
mean loss: 469.41
 ---- batch: 050 ----
mean loss: 493.70
 ---- batch: 060 ----
mean loss: 475.35
 ---- batch: 070 ----
mean loss: 476.54
 ---- batch: 080 ----
mean loss: 471.32
 ---- batch: 090 ----
mean loss: 490.04
 ---- batch: 100 ----
mean loss: 479.87
 ---- batch: 110 ----
mean loss: 491.84
train mean loss: 482.78
epoch train time: 0:00:02.184837
elapsed time: 0:02:36.220482
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 00:19:32.625172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.14
 ---- batch: 020 ----
mean loss: 486.30
 ---- batch: 030 ----
mean loss: 470.14
 ---- batch: 040 ----
mean loss: 475.58
 ---- batch: 050 ----
mean loss: 484.16
 ---- batch: 060 ----
mean loss: 490.34
 ---- batch: 070 ----
mean loss: 478.98
 ---- batch: 080 ----
mean loss: 461.82
 ---- batch: 090 ----
mean loss: 484.30
 ---- batch: 100 ----
mean loss: 487.67
 ---- batch: 110 ----
mean loss: 475.15
train mean loss: 480.62
epoch train time: 0:00:02.190607
elapsed time: 0:02:38.411299
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 00:19:34.815978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.80
 ---- batch: 020 ----
mean loss: 487.12
 ---- batch: 030 ----
mean loss: 486.38
 ---- batch: 040 ----
mean loss: 485.18
 ---- batch: 050 ----
mean loss: 480.37
 ---- batch: 060 ----
mean loss: 479.05
 ---- batch: 070 ----
mean loss: 490.91
 ---- batch: 080 ----
mean loss: 494.19
 ---- batch: 090 ----
mean loss: 478.29
 ---- batch: 100 ----
mean loss: 489.09
 ---- batch: 110 ----
mean loss: 486.85
train mean loss: 484.93
epoch train time: 0:00:02.184913
elapsed time: 0:02:40.596396
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 00:19:37.001073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.41
 ---- batch: 020 ----
mean loss: 491.03
 ---- batch: 030 ----
mean loss: 501.90
 ---- batch: 040 ----
mean loss: 477.85
 ---- batch: 050 ----
mean loss: 475.95
 ---- batch: 060 ----
mean loss: 485.09
 ---- batch: 070 ----
mean loss: 478.36
 ---- batch: 080 ----
mean loss: 492.11
 ---- batch: 090 ----
mean loss: 471.44
 ---- batch: 100 ----
mean loss: 488.68
 ---- batch: 110 ----
mean loss: 502.85
train mean loss: 485.86
epoch train time: 0:00:02.168699
elapsed time: 0:02:42.765289
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 00:19:39.169964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.31
 ---- batch: 020 ----
mean loss: 477.87
 ---- batch: 030 ----
mean loss: 486.89
 ---- batch: 040 ----
mean loss: 472.76
 ---- batch: 050 ----
mean loss: 478.53
 ---- batch: 060 ----
mean loss: 474.93
 ---- batch: 070 ----
mean loss: 480.32
 ---- batch: 080 ----
mean loss: 488.80
 ---- batch: 090 ----
mean loss: 476.51
 ---- batch: 100 ----
mean loss: 485.97
 ---- batch: 110 ----
mean loss: 471.96
train mean loss: 481.65
epoch train time: 0:00:02.154899
elapsed time: 0:02:44.920337
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 00:19:41.325011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.89
 ---- batch: 020 ----
mean loss: 464.40
 ---- batch: 030 ----
mean loss: 473.34
 ---- batch: 040 ----
mean loss: 466.04
 ---- batch: 050 ----
mean loss: 474.59
 ---- batch: 060 ----
mean loss: 479.53
 ---- batch: 070 ----
mean loss: 488.56
 ---- batch: 080 ----
mean loss: 473.76
 ---- batch: 090 ----
mean loss: 488.89
 ---- batch: 100 ----
mean loss: 471.85
 ---- batch: 110 ----
mean loss: 482.45
train mean loss: 475.96
epoch train time: 0:00:02.142969
elapsed time: 0:02:47.063462
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 00:19:43.468163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.78
 ---- batch: 020 ----
mean loss: 476.81
 ---- batch: 030 ----
mean loss: 475.33
 ---- batch: 040 ----
mean loss: 467.77
 ---- batch: 050 ----
mean loss: 478.07
 ---- batch: 060 ----
mean loss: 488.87
 ---- batch: 070 ----
mean loss: 480.02
 ---- batch: 080 ----
mean loss: 492.96
 ---- batch: 090 ----
mean loss: 491.25
 ---- batch: 100 ----
mean loss: 484.56
 ---- batch: 110 ----
mean loss: 489.56
train mean loss: 483.27
epoch train time: 0:00:02.151317
elapsed time: 0:02:49.214958
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 00:19:45.619653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.94
 ---- batch: 020 ----
mean loss: 477.40
 ---- batch: 030 ----
mean loss: 477.89
 ---- batch: 040 ----
mean loss: 472.49
 ---- batch: 050 ----
mean loss: 474.32
 ---- batch: 060 ----
mean loss: 485.22
 ---- batch: 070 ----
mean loss: 493.88
 ---- batch: 080 ----
mean loss: 481.86
 ---- batch: 090 ----
mean loss: 467.01
 ---- batch: 100 ----
mean loss: 484.66
 ---- batch: 110 ----
mean loss: 492.06
train mean loss: 481.32
epoch train time: 0:00:02.142554
elapsed time: 0:02:51.357696
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 00:19:47.762382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.43
 ---- batch: 020 ----
mean loss: 486.45
 ---- batch: 030 ----
mean loss: 488.83
 ---- batch: 040 ----
mean loss: 478.73
 ---- batch: 050 ----
mean loss: 463.91
 ---- batch: 060 ----
mean loss: 481.51
 ---- batch: 070 ----
mean loss: 486.99
 ---- batch: 080 ----
mean loss: 475.35
 ---- batch: 090 ----
mean loss: 475.29
 ---- batch: 100 ----
mean loss: 485.78
 ---- batch: 110 ----
mean loss: 468.34
train mean loss: 478.98
epoch train time: 0:00:02.154271
elapsed time: 0:02:53.512137
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 00:19:49.916813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.73
 ---- batch: 020 ----
mean loss: 481.80
 ---- batch: 030 ----
mean loss: 486.39
 ---- batch: 040 ----
mean loss: 489.64
 ---- batch: 050 ----
mean loss: 493.98
 ---- batch: 060 ----
mean loss: 481.13
 ---- batch: 070 ----
mean loss: 490.79
 ---- batch: 080 ----
mean loss: 486.53
 ---- batch: 090 ----
mean loss: 485.01
 ---- batch: 100 ----
mean loss: 460.10
 ---- batch: 110 ----
mean loss: 480.88
train mean loss: 484.24
epoch train time: 0:00:02.144201
elapsed time: 0:02:55.656485
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 00:19:52.061190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.83
 ---- batch: 020 ----
mean loss: 485.51
 ---- batch: 030 ----
mean loss: 501.62
 ---- batch: 040 ----
mean loss: 499.38
 ---- batch: 050 ----
mean loss: 466.93
 ---- batch: 060 ----
mean loss: 473.94
 ---- batch: 070 ----
mean loss: 476.56
 ---- batch: 080 ----
mean loss: 486.75
 ---- batch: 090 ----
mean loss: 471.95
 ---- batch: 100 ----
mean loss: 465.75
 ---- batch: 110 ----
mean loss: 471.22
train mean loss: 479.87
epoch train time: 0:00:02.143547
elapsed time: 0:02:57.800212
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 00:19:54.204884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.75
 ---- batch: 020 ----
mean loss: 469.68
 ---- batch: 030 ----
mean loss: 469.54
 ---- batch: 040 ----
mean loss: 485.06
 ---- batch: 050 ----
mean loss: 498.27
 ---- batch: 060 ----
mean loss: 501.74
 ---- batch: 070 ----
mean loss: 497.63
 ---- batch: 080 ----
mean loss: 468.37
 ---- batch: 090 ----
mean loss: 484.01
 ---- batch: 100 ----
mean loss: 475.03
 ---- batch: 110 ----
mean loss: 480.15
train mean loss: 483.21
epoch train time: 0:00:02.146845
elapsed time: 0:02:59.947233
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 00:19:56.351910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.10
 ---- batch: 020 ----
mean loss: 483.40
 ---- batch: 030 ----
mean loss: 490.62
 ---- batch: 040 ----
mean loss: 476.98
 ---- batch: 050 ----
mean loss: 476.66
 ---- batch: 060 ----
mean loss: 480.91
 ---- batch: 070 ----
mean loss: 498.98
 ---- batch: 080 ----
mean loss: 487.64
 ---- batch: 090 ----
mean loss: 481.49
 ---- batch: 100 ----
mean loss: 473.00
 ---- batch: 110 ----
mean loss: 478.17
train mean loss: 482.34
epoch train time: 0:00:02.140085
elapsed time: 0:03:02.087472
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 00:19:58.492146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.49
 ---- batch: 020 ----
mean loss: 458.92
 ---- batch: 030 ----
mean loss: 495.79
 ---- batch: 040 ----
mean loss: 464.62
 ---- batch: 050 ----
mean loss: 474.58
 ---- batch: 060 ----
mean loss: 492.19
 ---- batch: 070 ----
mean loss: 483.12
 ---- batch: 080 ----
mean loss: 490.40
 ---- batch: 090 ----
mean loss: 480.41
 ---- batch: 100 ----
mean loss: 483.81
 ---- batch: 110 ----
mean loss: 472.93
train mean loss: 480.10
epoch train time: 0:00:02.148980
elapsed time: 0:03:04.236594
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 00:20:00.641283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.49
 ---- batch: 020 ----
mean loss: 463.06
 ---- batch: 030 ----
mean loss: 476.29
 ---- batch: 040 ----
mean loss: 480.05
 ---- batch: 050 ----
mean loss: 464.90
 ---- batch: 060 ----
mean loss: 479.62
 ---- batch: 070 ----
mean loss: 467.21
 ---- batch: 080 ----
mean loss: 480.45
 ---- batch: 090 ----
mean loss: 471.41
 ---- batch: 100 ----
mean loss: 486.52
 ---- batch: 110 ----
mean loss: 477.85
train mean loss: 474.97
epoch train time: 0:00:02.134210
elapsed time: 0:03:06.370968
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 00:20:02.775653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.72
 ---- batch: 020 ----
mean loss: 486.61
 ---- batch: 030 ----
mean loss: 493.33
 ---- batch: 040 ----
mean loss: 465.85
 ---- batch: 050 ----
mean loss: 463.79
 ---- batch: 060 ----
mean loss: 469.71
 ---- batch: 070 ----
mean loss: 481.90
 ---- batch: 080 ----
mean loss: 507.39
 ---- batch: 090 ----
mean loss: 479.38
 ---- batch: 100 ----
mean loss: 482.53
 ---- batch: 110 ----
mean loss: 477.19
train mean loss: 481.81
epoch train time: 0:00:02.137091
elapsed time: 0:03:08.508220
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 00:20:04.912894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.93
 ---- batch: 020 ----
mean loss: 466.92
 ---- batch: 030 ----
mean loss: 479.13
 ---- batch: 040 ----
mean loss: 476.41
 ---- batch: 050 ----
mean loss: 476.12
 ---- batch: 060 ----
mean loss: 488.27
 ---- batch: 070 ----
mean loss: 477.27
 ---- batch: 080 ----
mean loss: 480.50
 ---- batch: 090 ----
mean loss: 477.22
 ---- batch: 100 ----
mean loss: 485.51
 ---- batch: 110 ----
mean loss: 489.59
train mean loss: 477.36
epoch train time: 0:00:02.138271
elapsed time: 0:03:10.646703
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 00:20:07.051379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.15
 ---- batch: 020 ----
mean loss: 488.43
 ---- batch: 030 ----
mean loss: 470.00
 ---- batch: 040 ----
mean loss: 453.81
 ---- batch: 050 ----
mean loss: 473.48
 ---- batch: 060 ----
mean loss: 486.35
 ---- batch: 070 ----
mean loss: 473.25
 ---- batch: 080 ----
mean loss: 488.57
 ---- batch: 090 ----
mean loss: 482.85
 ---- batch: 100 ----
mean loss: 473.24
 ---- batch: 110 ----
mean loss: 487.29
train mean loss: 479.44
epoch train time: 0:00:02.137809
elapsed time: 0:03:12.784662
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 00:20:09.189336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.13
 ---- batch: 020 ----
mean loss: 492.94
 ---- batch: 030 ----
mean loss: 473.29
 ---- batch: 040 ----
mean loss: 478.08
 ---- batch: 050 ----
mean loss: 468.69
 ---- batch: 060 ----
mean loss: 480.66
 ---- batch: 070 ----
mean loss: 470.73
 ---- batch: 080 ----
mean loss: 475.10
 ---- batch: 090 ----
mean loss: 485.01
 ---- batch: 100 ----
mean loss: 476.49
 ---- batch: 110 ----
mean loss: 491.56
train mean loss: 479.68
epoch train time: 0:00:02.144120
elapsed time: 0:03:14.928946
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 00:20:11.333640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.98
 ---- batch: 020 ----
mean loss: 477.20
 ---- batch: 030 ----
mean loss: 481.85
 ---- batch: 040 ----
mean loss: 470.53
 ---- batch: 050 ----
mean loss: 471.01
 ---- batch: 060 ----
mean loss: 482.09
 ---- batch: 070 ----
mean loss: 481.08
 ---- batch: 080 ----
mean loss: 473.95
 ---- batch: 090 ----
mean loss: 483.09
 ---- batch: 100 ----
mean loss: 479.04
 ---- batch: 110 ----
mean loss: 472.78
train mean loss: 476.99
epoch train time: 0:00:02.162319
elapsed time: 0:03:17.091442
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 00:20:13.496115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.65
 ---- batch: 020 ----
mean loss: 461.10
 ---- batch: 030 ----
mean loss: 486.11
 ---- batch: 040 ----
mean loss: 483.32
 ---- batch: 050 ----
mean loss: 485.22
 ---- batch: 060 ----
mean loss: 487.60
 ---- batch: 070 ----
mean loss: 481.26
 ---- batch: 080 ----
mean loss: 486.36
 ---- batch: 090 ----
mean loss: 472.76
 ---- batch: 100 ----
mean loss: 479.56
 ---- batch: 110 ----
mean loss: 484.84
train mean loss: 479.69
epoch train time: 0:00:02.176157
elapsed time: 0:03:19.267753
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 00:20:15.672481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.53
 ---- batch: 020 ----
mean loss: 491.71
 ---- batch: 030 ----
mean loss: 484.21
 ---- batch: 040 ----
mean loss: 476.12
 ---- batch: 050 ----
mean loss: 497.95
 ---- batch: 060 ----
mean loss: 482.37
 ---- batch: 070 ----
mean loss: 470.79
 ---- batch: 080 ----
mean loss: 480.82
 ---- batch: 090 ----
mean loss: 485.18
 ---- batch: 100 ----
mean loss: 465.88
 ---- batch: 110 ----
mean loss: 467.12
train mean loss: 481.82
epoch train time: 0:00:02.175501
elapsed time: 0:03:21.443477
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 00:20:17.848165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.79
 ---- batch: 020 ----
mean loss: 469.55
 ---- batch: 030 ----
mean loss: 467.14
 ---- batch: 040 ----
mean loss: 480.71
 ---- batch: 050 ----
mean loss: 473.21
 ---- batch: 060 ----
mean loss: 483.25
 ---- batch: 070 ----
mean loss: 481.98
 ---- batch: 080 ----
mean loss: 474.25
 ---- batch: 090 ----
mean loss: 465.26
 ---- batch: 100 ----
mean loss: 474.11
 ---- batch: 110 ----
mean loss: 473.28
train mean loss: 475.05
epoch train time: 0:00:02.157687
elapsed time: 0:03:23.601353
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 00:20:20.006029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.15
 ---- batch: 020 ----
mean loss: 465.28
 ---- batch: 030 ----
mean loss: 480.45
 ---- batch: 040 ----
mean loss: 484.44
 ---- batch: 050 ----
mean loss: 482.00
 ---- batch: 060 ----
mean loss: 471.21
 ---- batch: 070 ----
mean loss: 463.99
 ---- batch: 080 ----
mean loss: 464.80
 ---- batch: 090 ----
mean loss: 490.02
 ---- batch: 100 ----
mean loss: 461.90
 ---- batch: 110 ----
mean loss: 494.01
train mean loss: 475.18
epoch train time: 0:00:02.177707
elapsed time: 0:03:25.779241
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 00:20:22.183929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.38
 ---- batch: 020 ----
mean loss: 485.52
 ---- batch: 030 ----
mean loss: 471.23
 ---- batch: 040 ----
mean loss: 468.68
 ---- batch: 050 ----
mean loss: 480.13
 ---- batch: 060 ----
mean loss: 484.59
 ---- batch: 070 ----
mean loss: 487.18
 ---- batch: 080 ----
mean loss: 483.30
 ---- batch: 090 ----
mean loss: 499.16
 ---- batch: 100 ----
mean loss: 503.07
 ---- batch: 110 ----
mean loss: 465.08
train mean loss: 481.44
epoch train time: 0:00:02.186750
elapsed time: 0:03:27.966171
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 00:20:24.370846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.91
 ---- batch: 020 ----
mean loss: 481.43
 ---- batch: 030 ----
mean loss: 476.74
 ---- batch: 040 ----
mean loss: 471.21
 ---- batch: 050 ----
mean loss: 486.91
 ---- batch: 060 ----
mean loss: 472.15
 ---- batch: 070 ----
mean loss: 481.02
 ---- batch: 080 ----
mean loss: 488.11
 ---- batch: 090 ----
mean loss: 488.59
 ---- batch: 100 ----
mean loss: 468.16
 ---- batch: 110 ----
mean loss: 473.92
train mean loss: 477.94
epoch train time: 0:00:02.181435
elapsed time: 0:03:30.147783
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 00:20:26.552460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.54
 ---- batch: 020 ----
mean loss: 481.23
 ---- batch: 030 ----
mean loss: 493.95
 ---- batch: 040 ----
mean loss: 475.05
 ---- batch: 050 ----
mean loss: 480.36
 ---- batch: 060 ----
mean loss: 481.41
 ---- batch: 070 ----
mean loss: 491.89
 ---- batch: 080 ----
mean loss: 470.50
 ---- batch: 090 ----
mean loss: 459.06
 ---- batch: 100 ----
mean loss: 486.93
 ---- batch: 110 ----
mean loss: 471.57
train mean loss: 477.29
epoch train time: 0:00:02.180172
elapsed time: 0:03:32.328128
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 00:20:28.732823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.18
 ---- batch: 020 ----
mean loss: 492.36
 ---- batch: 030 ----
mean loss: 480.45
 ---- batch: 040 ----
mean loss: 471.16
 ---- batch: 050 ----
mean loss: 484.15
 ---- batch: 060 ----
mean loss: 467.76
 ---- batch: 070 ----
mean loss: 476.84
 ---- batch: 080 ----
mean loss: 479.18
 ---- batch: 090 ----
mean loss: 479.95
 ---- batch: 100 ----
mean loss: 471.45
 ---- batch: 110 ----
mean loss: 470.32
train mean loss: 478.33
epoch train time: 0:00:02.180895
elapsed time: 0:03:34.509203
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 00:20:30.913878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.33
 ---- batch: 020 ----
mean loss: 479.69
 ---- batch: 030 ----
mean loss: 481.16
 ---- batch: 040 ----
mean loss: 468.83
 ---- batch: 050 ----
mean loss: 480.48
 ---- batch: 060 ----
mean loss: 478.67
 ---- batch: 070 ----
mean loss: 470.84
 ---- batch: 080 ----
mean loss: 481.71
 ---- batch: 090 ----
mean loss: 477.83
 ---- batch: 100 ----
mean loss: 495.69
 ---- batch: 110 ----
mean loss: 478.05
train mean loss: 478.19
epoch train time: 0:00:02.179159
elapsed time: 0:03:36.688528
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 00:20:33.093201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.70
 ---- batch: 020 ----
mean loss: 507.16
 ---- batch: 030 ----
mean loss: 485.03
 ---- batch: 040 ----
mean loss: 496.83
 ---- batch: 050 ----
mean loss: 480.18
 ---- batch: 060 ----
mean loss: 477.20
 ---- batch: 070 ----
mean loss: 469.98
 ---- batch: 080 ----
mean loss: 473.98
 ---- batch: 090 ----
mean loss: 490.63
 ---- batch: 100 ----
mean loss: 461.65
 ---- batch: 110 ----
mean loss: 483.15
train mean loss: 479.86
epoch train time: 0:00:02.180609
elapsed time: 0:03:38.869304
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 00:20:35.273987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.18
 ---- batch: 020 ----
mean loss: 486.44
 ---- batch: 030 ----
mean loss: 480.77
 ---- batch: 040 ----
mean loss: 461.97
 ---- batch: 050 ----
mean loss: 483.75
 ---- batch: 060 ----
mean loss: 472.90
 ---- batch: 070 ----
mean loss: 472.78
 ---- batch: 080 ----
mean loss: 461.17
 ---- batch: 090 ----
mean loss: 484.04
 ---- batch: 100 ----
mean loss: 479.89
 ---- batch: 110 ----
mean loss: 490.79
train mean loss: 476.61
epoch train time: 0:00:02.181426
elapsed time: 0:03:41.050923
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 00:20:37.455602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.75
 ---- batch: 020 ----
mean loss: 476.62
 ---- batch: 030 ----
mean loss: 481.86
 ---- batch: 040 ----
mean loss: 465.45
 ---- batch: 050 ----
mean loss: 485.21
 ---- batch: 060 ----
mean loss: 480.04
 ---- batch: 070 ----
mean loss: 469.18
 ---- batch: 080 ----
mean loss: 486.31
 ---- batch: 090 ----
mean loss: 474.44
 ---- batch: 100 ----
mean loss: 466.13
 ---- batch: 110 ----
mean loss: 480.97
train mean loss: 477.40
epoch train time: 0:00:02.179319
elapsed time: 0:03:43.230460
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 00:20:39.635164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.01
 ---- batch: 020 ----
mean loss: 467.53
 ---- batch: 030 ----
mean loss: 470.20
 ---- batch: 040 ----
mean loss: 471.37
 ---- batch: 050 ----
mean loss: 468.75
 ---- batch: 060 ----
mean loss: 458.31
 ---- batch: 070 ----
mean loss: 461.46
 ---- batch: 080 ----
mean loss: 475.42
 ---- batch: 090 ----
mean loss: 482.50
 ---- batch: 100 ----
mean loss: 493.88
 ---- batch: 110 ----
mean loss: 490.76
train mean loss: 474.31
epoch train time: 0:00:02.176904
elapsed time: 0:03:45.407587
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 00:20:41.812288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.99
 ---- batch: 020 ----
mean loss: 473.56
 ---- batch: 030 ----
mean loss: 458.42
 ---- batch: 040 ----
mean loss: 480.72
 ---- batch: 050 ----
mean loss: 474.89
 ---- batch: 060 ----
mean loss: 473.47
 ---- batch: 070 ----
mean loss: 484.19
 ---- batch: 080 ----
mean loss: 478.23
 ---- batch: 090 ----
mean loss: 471.12
 ---- batch: 100 ----
mean loss: 475.02
 ---- batch: 110 ----
mean loss: 473.51
train mean loss: 474.34
epoch train time: 0:00:02.178424
elapsed time: 0:03:47.586193
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 00:20:43.990884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.55
 ---- batch: 020 ----
mean loss: 477.66
 ---- batch: 030 ----
mean loss: 463.63
 ---- batch: 040 ----
mean loss: 480.68
 ---- batch: 050 ----
mean loss: 469.02
 ---- batch: 060 ----
mean loss: 485.23
 ---- batch: 070 ----
mean loss: 465.25
 ---- batch: 080 ----
mean loss: 475.30
 ---- batch: 090 ----
mean loss: 484.57
 ---- batch: 100 ----
mean loss: 494.97
 ---- batch: 110 ----
mean loss: 482.45
train mean loss: 476.03
epoch train time: 0:00:02.182589
elapsed time: 0:03:49.768963
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 00:20:46.173662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.25
 ---- batch: 020 ----
mean loss: 476.73
 ---- batch: 030 ----
mean loss: 475.89
 ---- batch: 040 ----
mean loss: 468.18
 ---- batch: 050 ----
mean loss: 462.85
 ---- batch: 060 ----
mean loss: 477.91
 ---- batch: 070 ----
mean loss: 468.73
 ---- batch: 080 ----
mean loss: 482.31
 ---- batch: 090 ----
mean loss: 470.67
 ---- batch: 100 ----
mean loss: 460.36
 ---- batch: 110 ----
mean loss: 474.18
train mean loss: 471.27
epoch train time: 0:00:02.183904
elapsed time: 0:03:51.953066
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 00:20:48.357742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.76
 ---- batch: 020 ----
mean loss: 467.59
 ---- batch: 030 ----
mean loss: 497.42
 ---- batch: 040 ----
mean loss: 479.42
 ---- batch: 050 ----
mean loss: 480.04
 ---- batch: 060 ----
mean loss: 476.85
 ---- batch: 070 ----
mean loss: 478.19
 ---- batch: 080 ----
mean loss: 459.36
 ---- batch: 090 ----
mean loss: 470.44
 ---- batch: 100 ----
mean loss: 462.39
 ---- batch: 110 ----
mean loss: 479.33
train mean loss: 477.89
epoch train time: 0:00:02.184642
elapsed time: 0:03:54.137890
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 00:20:50.542564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.14
 ---- batch: 020 ----
mean loss: 500.44
 ---- batch: 030 ----
mean loss: 491.46
 ---- batch: 040 ----
mean loss: 474.27
 ---- batch: 050 ----
mean loss: 472.40
 ---- batch: 060 ----
mean loss: 452.43
 ---- batch: 070 ----
mean loss: 473.02
 ---- batch: 080 ----
mean loss: 474.97
 ---- batch: 090 ----
mean loss: 464.70
 ---- batch: 100 ----
mean loss: 492.74
 ---- batch: 110 ----
mean loss: 480.55
train mean loss: 476.53
epoch train time: 0:00:02.179819
elapsed time: 0:03:56.317874
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 00:20:52.722552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.82
 ---- batch: 020 ----
mean loss: 469.84
 ---- batch: 030 ----
mean loss: 478.53
 ---- batch: 040 ----
mean loss: 464.55
 ---- batch: 050 ----
mean loss: 480.92
 ---- batch: 060 ----
mean loss: 488.28
 ---- batch: 070 ----
mean loss: 479.53
 ---- batch: 080 ----
mean loss: 477.12
 ---- batch: 090 ----
mean loss: 460.96
 ---- batch: 100 ----
mean loss: 467.07
 ---- batch: 110 ----
mean loss: 469.50
train mean loss: 475.94
epoch train time: 0:00:02.183226
elapsed time: 0:03:58.501270
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 00:20:54.905945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.83
 ---- batch: 020 ----
mean loss: 474.26
 ---- batch: 030 ----
mean loss: 467.44
 ---- batch: 040 ----
mean loss: 484.89
 ---- batch: 050 ----
mean loss: 468.64
 ---- batch: 060 ----
mean loss: 470.75
 ---- batch: 070 ----
mean loss: 488.49
 ---- batch: 080 ----
mean loss: 473.31
 ---- batch: 090 ----
mean loss: 485.07
 ---- batch: 100 ----
mean loss: 464.13
 ---- batch: 110 ----
mean loss: 465.79
train mean loss: 472.99
epoch train time: 0:00:02.184662
elapsed time: 0:04:00.686114
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 00:20:57.090826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.45
 ---- batch: 020 ----
mean loss: 493.11
 ---- batch: 030 ----
mean loss: 481.23
 ---- batch: 040 ----
mean loss: 477.16
 ---- batch: 050 ----
mean loss: 487.70
 ---- batch: 060 ----
mean loss: 464.08
 ---- batch: 070 ----
mean loss: 463.30
 ---- batch: 080 ----
mean loss: 486.06
 ---- batch: 090 ----
mean loss: 485.57
 ---- batch: 100 ----
mean loss: 466.01
 ---- batch: 110 ----
mean loss: 470.92
train mean loss: 476.33
epoch train time: 0:00:02.181603
elapsed time: 0:04:02.867931
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 00:20:59.272607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.42
 ---- batch: 020 ----
mean loss: 464.24
 ---- batch: 030 ----
mean loss: 452.03
 ---- batch: 040 ----
mean loss: 481.97
 ---- batch: 050 ----
mean loss: 484.52
 ---- batch: 060 ----
mean loss: 471.93
 ---- batch: 070 ----
mean loss: 490.76
 ---- batch: 080 ----
mean loss: 464.65
 ---- batch: 090 ----
mean loss: 467.90
 ---- batch: 100 ----
mean loss: 471.20
 ---- batch: 110 ----
mean loss: 475.65
train mean loss: 473.84
epoch train time: 0:00:02.183987
elapsed time: 0:04:05.052078
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 00:21:01.456754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.59
 ---- batch: 020 ----
mean loss: 459.51
 ---- batch: 030 ----
mean loss: 468.24
 ---- batch: 040 ----
mean loss: 470.33
 ---- batch: 050 ----
mean loss: 479.90
 ---- batch: 060 ----
mean loss: 469.78
 ---- batch: 070 ----
mean loss: 468.51
 ---- batch: 080 ----
mean loss: 469.88
 ---- batch: 090 ----
mean loss: 472.70
 ---- batch: 100 ----
mean loss: 484.71
 ---- batch: 110 ----
mean loss: 480.97
train mean loss: 470.33
epoch train time: 0:00:02.179172
elapsed time: 0:04:07.231430
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 00:21:03.636105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.72
 ---- batch: 020 ----
mean loss: 491.67
 ---- batch: 030 ----
mean loss: 476.60
 ---- batch: 040 ----
mean loss: 472.54
 ---- batch: 050 ----
mean loss: 469.52
 ---- batch: 060 ----
mean loss: 475.05
 ---- batch: 070 ----
mean loss: 463.48
 ---- batch: 080 ----
mean loss: 479.85
 ---- batch: 090 ----
mean loss: 476.24
 ---- batch: 100 ----
mean loss: 468.12
 ---- batch: 110 ----
mean loss: 483.88
train mean loss: 474.31
epoch train time: 0:00:02.184790
elapsed time: 0:04:09.416384
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 00:21:05.821065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.32
 ---- batch: 020 ----
mean loss: 468.64
 ---- batch: 030 ----
mean loss: 466.21
 ---- batch: 040 ----
mean loss: 484.11
 ---- batch: 050 ----
mean loss: 450.50
 ---- batch: 060 ----
mean loss: 459.31
 ---- batch: 070 ----
mean loss: 454.53
 ---- batch: 080 ----
mean loss: 459.74
 ---- batch: 090 ----
mean loss: 473.59
 ---- batch: 100 ----
mean loss: 482.68
 ---- batch: 110 ----
mean loss: 478.53
train mean loss: 468.30
epoch train time: 0:00:02.177174
elapsed time: 0:04:11.593777
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 00:21:07.998457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.32
 ---- batch: 020 ----
mean loss: 473.54
 ---- batch: 030 ----
mean loss: 473.57
 ---- batch: 040 ----
mean loss: 471.61
 ---- batch: 050 ----
mean loss: 471.42
 ---- batch: 060 ----
mean loss: 467.91
 ---- batch: 070 ----
mean loss: 478.19
 ---- batch: 080 ----
mean loss: 472.85
 ---- batch: 090 ----
mean loss: 481.15
 ---- batch: 100 ----
mean loss: 480.00
 ---- batch: 110 ----
mean loss: 467.82
train mean loss: 474.77
epoch train time: 0:00:02.182407
elapsed time: 0:04:13.776387
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 00:21:10.181060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.32
 ---- batch: 020 ----
mean loss: 468.44
 ---- batch: 030 ----
mean loss: 495.73
 ---- batch: 040 ----
mean loss: 474.66
 ---- batch: 050 ----
mean loss: 471.34
 ---- batch: 060 ----
mean loss: 456.89
 ---- batch: 070 ----
mean loss: 470.76
 ---- batch: 080 ----
mean loss: 482.58
 ---- batch: 090 ----
mean loss: 471.34
 ---- batch: 100 ----
mean loss: 476.11
 ---- batch: 110 ----
mean loss: 459.29
train mean loss: 472.87
epoch train time: 0:00:02.178262
elapsed time: 0:04:15.954852
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 00:21:12.359546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.67
 ---- batch: 020 ----
mean loss: 451.79
 ---- batch: 030 ----
mean loss: 483.04
 ---- batch: 040 ----
mean loss: 471.46
 ---- batch: 050 ----
mean loss: 478.97
 ---- batch: 060 ----
mean loss: 465.25
 ---- batch: 070 ----
mean loss: 471.61
 ---- batch: 080 ----
mean loss: 481.46
 ---- batch: 090 ----
mean loss: 479.01
 ---- batch: 100 ----
mean loss: 468.56
 ---- batch: 110 ----
mean loss: 465.90
train mean loss: 472.41
epoch train time: 0:00:02.183740
elapsed time: 0:04:18.138783
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 00:21:14.543459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.28
 ---- batch: 020 ----
mean loss: 484.59
 ---- batch: 030 ----
mean loss: 469.30
 ---- batch: 040 ----
mean loss: 473.59
 ---- batch: 050 ----
mean loss: 467.07
 ---- batch: 060 ----
mean loss: 470.33
 ---- batch: 070 ----
mean loss: 467.00
 ---- batch: 080 ----
mean loss: 494.99
 ---- batch: 090 ----
mean loss: 473.91
 ---- batch: 100 ----
mean loss: 483.98
 ---- batch: 110 ----
mean loss: 483.14
train mean loss: 476.95
epoch train time: 0:00:02.180065
elapsed time: 0:04:20.319089
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 00:21:16.723776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.31
 ---- batch: 020 ----
mean loss: 488.82
 ---- batch: 030 ----
mean loss: 468.89
 ---- batch: 040 ----
mean loss: 468.13
 ---- batch: 050 ----
mean loss: 479.09
 ---- batch: 060 ----
mean loss: 466.70
 ---- batch: 070 ----
mean loss: 485.81
 ---- batch: 080 ----
mean loss: 472.19
 ---- batch: 090 ----
mean loss: 478.41
 ---- batch: 100 ----
mean loss: 475.77
 ---- batch: 110 ----
mean loss: 468.33
train mean loss: 473.59
epoch train time: 0:00:02.180870
elapsed time: 0:04:22.500166
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 00:21:18.904845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.56
 ---- batch: 020 ----
mean loss: 478.31
 ---- batch: 030 ----
mean loss: 480.49
 ---- batch: 040 ----
mean loss: 470.33
 ---- batch: 050 ----
mean loss: 459.38
 ---- batch: 060 ----
mean loss: 475.85
 ---- batch: 070 ----
mean loss: 461.84
 ---- batch: 080 ----
mean loss: 474.10
 ---- batch: 090 ----
mean loss: 473.88
 ---- batch: 100 ----
mean loss: 488.20
 ---- batch: 110 ----
mean loss: 465.92
train mean loss: 471.03
epoch train time: 0:00:02.177901
elapsed time: 0:04:24.678277
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 00:21:21.082940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.65
 ---- batch: 020 ----
mean loss: 475.07
 ---- batch: 030 ----
mean loss: 460.18
 ---- batch: 040 ----
mean loss: 464.61
 ---- batch: 050 ----
mean loss: 482.27
 ---- batch: 060 ----
mean loss: 475.13
 ---- batch: 070 ----
mean loss: 465.41
 ---- batch: 080 ----
mean loss: 474.76
 ---- batch: 090 ----
mean loss: 472.23
 ---- batch: 100 ----
mean loss: 451.11
 ---- batch: 110 ----
mean loss: 460.69
train mean loss: 468.26
epoch train time: 0:00:02.184031
elapsed time: 0:04:26.862501
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 00:21:23.267183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.43
 ---- batch: 020 ----
mean loss: 464.35
 ---- batch: 030 ----
mean loss: 477.66
 ---- batch: 040 ----
mean loss: 474.79
 ---- batch: 050 ----
mean loss: 486.63
 ---- batch: 060 ----
mean loss: 475.43
 ---- batch: 070 ----
mean loss: 473.51
 ---- batch: 080 ----
mean loss: 476.25
 ---- batch: 090 ----
mean loss: 487.15
 ---- batch: 100 ----
mean loss: 451.37
 ---- batch: 110 ----
mean loss: 457.51
train mean loss: 471.90
epoch train time: 0:00:02.183077
elapsed time: 0:04:29.045759
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 00:21:25.450454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.33
 ---- batch: 020 ----
mean loss: 479.38
 ---- batch: 030 ----
mean loss: 480.93
 ---- batch: 040 ----
mean loss: 465.15
 ---- batch: 050 ----
mean loss: 475.53
 ---- batch: 060 ----
mean loss: 458.00
 ---- batch: 070 ----
mean loss: 477.32
 ---- batch: 080 ----
mean loss: 474.75
 ---- batch: 090 ----
mean loss: 470.97
 ---- batch: 100 ----
mean loss: 467.50
 ---- batch: 110 ----
mean loss: 467.66
train mean loss: 471.67
epoch train time: 0:00:02.184692
elapsed time: 0:04:31.230647
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 00:21:27.635326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.36
 ---- batch: 020 ----
mean loss: 454.70
 ---- batch: 030 ----
mean loss: 477.93
 ---- batch: 040 ----
mean loss: 476.50
 ---- batch: 050 ----
mean loss: 455.28
 ---- batch: 060 ----
mean loss: 453.85
 ---- batch: 070 ----
mean loss: 484.52
 ---- batch: 080 ----
mean loss: 473.49
 ---- batch: 090 ----
mean loss: 457.79
 ---- batch: 100 ----
mean loss: 473.58
 ---- batch: 110 ----
mean loss: 476.34
train mean loss: 468.34
epoch train time: 0:00:02.177682
elapsed time: 0:04:33.408492
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 00:21:29.813166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.28
 ---- batch: 020 ----
mean loss: 469.74
 ---- batch: 030 ----
mean loss: 474.16
 ---- batch: 040 ----
mean loss: 461.17
 ---- batch: 050 ----
mean loss: 471.06
 ---- batch: 060 ----
mean loss: 470.16
 ---- batch: 070 ----
mean loss: 469.03
 ---- batch: 080 ----
mean loss: 481.69
 ---- batch: 090 ----
mean loss: 470.62
 ---- batch: 100 ----
mean loss: 471.07
 ---- batch: 110 ----
mean loss: 480.40
train mean loss: 472.26
epoch train time: 0:00:02.185618
elapsed time: 0:04:35.594351
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 00:21:31.999044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.82
 ---- batch: 020 ----
mean loss: 462.85
 ---- batch: 030 ----
mean loss: 466.43
 ---- batch: 040 ----
mean loss: 474.11
 ---- batch: 050 ----
mean loss: 447.60
 ---- batch: 060 ----
mean loss: 467.41
 ---- batch: 070 ----
mean loss: 452.21
 ---- batch: 080 ----
mean loss: 471.33
 ---- batch: 090 ----
mean loss: 473.06
 ---- batch: 100 ----
mean loss: 474.76
 ---- batch: 110 ----
mean loss: 460.29
train mean loss: 464.88
epoch train time: 0:00:02.176941
elapsed time: 0:04:37.771475
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 00:21:34.176151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.47
 ---- batch: 020 ----
mean loss: 455.19
 ---- batch: 030 ----
mean loss: 469.24
 ---- batch: 040 ----
mean loss: 470.24
 ---- batch: 050 ----
mean loss: 479.23
 ---- batch: 060 ----
mean loss: 477.31
 ---- batch: 070 ----
mean loss: 472.67
 ---- batch: 080 ----
mean loss: 470.74
 ---- batch: 090 ----
mean loss: 464.19
 ---- batch: 100 ----
mean loss: 446.67
 ---- batch: 110 ----
mean loss: 474.93
train mean loss: 468.01
epoch train time: 0:00:02.178031
elapsed time: 0:04:39.949678
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 00:21:36.354353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.96
 ---- batch: 020 ----
mean loss: 481.86
 ---- batch: 030 ----
mean loss: 454.48
 ---- batch: 040 ----
mean loss: 462.04
 ---- batch: 050 ----
mean loss: 479.41
 ---- batch: 060 ----
mean loss: 463.27
 ---- batch: 070 ----
mean loss: 464.60
 ---- batch: 080 ----
mean loss: 454.59
 ---- batch: 090 ----
mean loss: 458.36
 ---- batch: 100 ----
mean loss: 467.08
 ---- batch: 110 ----
mean loss: 461.21
train mean loss: 465.54
epoch train time: 0:00:02.181006
elapsed time: 0:04:42.130852
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 00:21:38.535531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.79
 ---- batch: 020 ----
mean loss: 489.89
 ---- batch: 030 ----
mean loss: 469.10
 ---- batch: 040 ----
mean loss: 452.89
 ---- batch: 050 ----
mean loss: 460.64
 ---- batch: 060 ----
mean loss: 456.95
 ---- batch: 070 ----
mean loss: 454.83
 ---- batch: 080 ----
mean loss: 475.11
 ---- batch: 090 ----
mean loss: 458.58
 ---- batch: 100 ----
mean loss: 479.70
 ---- batch: 110 ----
mean loss: 463.64
train mean loss: 467.28
epoch train time: 0:00:02.182261
elapsed time: 0:04:44.313283
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 00:21:40.717988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.20
 ---- batch: 020 ----
mean loss: 465.53
 ---- batch: 030 ----
mean loss: 455.02
 ---- batch: 040 ----
mean loss: 473.70
 ---- batch: 050 ----
mean loss: 462.63
 ---- batch: 060 ----
mean loss: 466.36
 ---- batch: 070 ----
mean loss: 474.12
 ---- batch: 080 ----
mean loss: 470.64
 ---- batch: 090 ----
mean loss: 458.24
 ---- batch: 100 ----
mean loss: 480.95
 ---- batch: 110 ----
mean loss: 457.33
train mean loss: 465.90
epoch train time: 0:00:02.172285
elapsed time: 0:04:46.485783
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 00:21:42.890464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.09
 ---- batch: 020 ----
mean loss: 466.13
 ---- batch: 030 ----
mean loss: 477.58
 ---- batch: 040 ----
mean loss: 463.60
 ---- batch: 050 ----
mean loss: 451.71
 ---- batch: 060 ----
mean loss: 479.31
 ---- batch: 070 ----
mean loss: 464.16
 ---- batch: 080 ----
mean loss: 486.20
 ---- batch: 090 ----
mean loss: 453.02
 ---- batch: 100 ----
mean loss: 479.97
 ---- batch: 110 ----
mean loss: 456.55
train mean loss: 468.36
epoch train time: 0:00:02.181422
elapsed time: 0:04:48.667389
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 00:21:45.072073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.32
 ---- batch: 020 ----
mean loss: 460.57
 ---- batch: 030 ----
mean loss: 458.18
 ---- batch: 040 ----
mean loss: 459.50
 ---- batch: 050 ----
mean loss: 483.55
 ---- batch: 060 ----
mean loss: 478.52
 ---- batch: 070 ----
mean loss: 469.99
 ---- batch: 080 ----
mean loss: 489.98
 ---- batch: 090 ----
mean loss: 495.55
 ---- batch: 100 ----
mean loss: 465.99
 ---- batch: 110 ----
mean loss: 462.18
train mean loss: 470.14
epoch train time: 0:00:02.184317
elapsed time: 0:04:50.851928
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 00:21:47.256630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.06
 ---- batch: 020 ----
mean loss: 449.55
 ---- batch: 030 ----
mean loss: 463.03
 ---- batch: 040 ----
mean loss: 452.17
 ---- batch: 050 ----
mean loss: 476.24
 ---- batch: 060 ----
mean loss: 474.32
 ---- batch: 070 ----
mean loss: 478.71
 ---- batch: 080 ----
mean loss: 479.72
 ---- batch: 090 ----
mean loss: 474.63
 ---- batch: 100 ----
mean loss: 462.45
 ---- batch: 110 ----
mean loss: 463.12
train mean loss: 464.44
epoch train time: 0:00:02.184745
elapsed time: 0:04:53.036885
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 00:21:49.441562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.11
 ---- batch: 020 ----
mean loss: 457.16
 ---- batch: 030 ----
mean loss: 462.93
 ---- batch: 040 ----
mean loss: 472.32
 ---- batch: 050 ----
mean loss: 468.79
 ---- batch: 060 ----
mean loss: 453.90
 ---- batch: 070 ----
mean loss: 447.61
 ---- batch: 080 ----
mean loss: 456.52
 ---- batch: 090 ----
mean loss: 457.45
 ---- batch: 100 ----
mean loss: 474.50
 ---- batch: 110 ----
mean loss: 462.61
train mean loss: 461.09
epoch train time: 0:00:02.186169
elapsed time: 0:04:55.223241
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 00:21:51.627918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.49
 ---- batch: 020 ----
mean loss: 467.04
 ---- batch: 030 ----
mean loss: 464.85
 ---- batch: 040 ----
mean loss: 473.05
 ---- batch: 050 ----
mean loss: 460.14
 ---- batch: 060 ----
mean loss: 475.40
 ---- batch: 070 ----
mean loss: 469.35
 ---- batch: 080 ----
mean loss: 473.60
 ---- batch: 090 ----
mean loss: 466.07
 ---- batch: 100 ----
mean loss: 466.72
 ---- batch: 110 ----
mean loss: 452.88
train mean loss: 466.25
epoch train time: 0:00:02.187034
elapsed time: 0:04:57.410473
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 00:21:53.815167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.47
 ---- batch: 020 ----
mean loss: 462.81
 ---- batch: 030 ----
mean loss: 464.55
 ---- batch: 040 ----
mean loss: 472.35
 ---- batch: 050 ----
mean loss: 465.61
 ---- batch: 060 ----
mean loss: 465.49
 ---- batch: 070 ----
mean loss: 464.77
 ---- batch: 080 ----
mean loss: 459.83
 ---- batch: 090 ----
mean loss: 451.94
 ---- batch: 100 ----
mean loss: 462.81
 ---- batch: 110 ----
mean loss: 467.41
train mean loss: 463.78
epoch train time: 0:00:02.185671
elapsed time: 0:04:59.596318
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 00:21:56.001010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.00
 ---- batch: 020 ----
mean loss: 460.94
 ---- batch: 030 ----
mean loss: 451.17
 ---- batch: 040 ----
mean loss: 468.01
 ---- batch: 050 ----
mean loss: 467.25
 ---- batch: 060 ----
mean loss: 458.88
 ---- batch: 070 ----
mean loss: 454.38
 ---- batch: 080 ----
mean loss: 451.16
 ---- batch: 090 ----
mean loss: 467.55
 ---- batch: 100 ----
mean loss: 457.65
 ---- batch: 110 ----
mean loss: 465.94
train mean loss: 460.65
epoch train time: 0:00:02.183837
elapsed time: 0:05:01.780339
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 00:21:58.185015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.95
 ---- batch: 020 ----
mean loss: 458.36
 ---- batch: 030 ----
mean loss: 455.45
 ---- batch: 040 ----
mean loss: 473.87
 ---- batch: 050 ----
mean loss: 465.57
 ---- batch: 060 ----
mean loss: 453.24
 ---- batch: 070 ----
mean loss: 452.33
 ---- batch: 080 ----
mean loss: 456.71
 ---- batch: 090 ----
mean loss: 463.38
 ---- batch: 100 ----
mean loss: 451.40
 ---- batch: 110 ----
mean loss: 470.56
train mean loss: 460.31
epoch train time: 0:00:02.180447
elapsed time: 0:05:03.960961
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 00:22:00.365669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.58
 ---- batch: 020 ----
mean loss: 448.40
 ---- batch: 030 ----
mean loss: 467.63
 ---- batch: 040 ----
mean loss: 449.02
 ---- batch: 050 ----
mean loss: 445.52
 ---- batch: 060 ----
mean loss: 457.00
 ---- batch: 070 ----
mean loss: 463.74
 ---- batch: 080 ----
mean loss: 462.21
 ---- batch: 090 ----
mean loss: 451.42
 ---- batch: 100 ----
mean loss: 465.07
 ---- batch: 110 ----
mean loss: 461.92
train mean loss: 456.69
epoch train time: 0:00:02.180704
elapsed time: 0:05:06.141965
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 00:22:02.547845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.86
 ---- batch: 020 ----
mean loss: 464.63
 ---- batch: 030 ----
mean loss: 446.93
 ---- batch: 040 ----
mean loss: 453.47
 ---- batch: 050 ----
mean loss: 463.93
 ---- batch: 060 ----
mean loss: 456.36
 ---- batch: 070 ----
mean loss: 454.47
 ---- batch: 080 ----
mean loss: 475.05
 ---- batch: 090 ----
mean loss: 471.89
 ---- batch: 100 ----
mean loss: 473.29
 ---- batch: 110 ----
mean loss: 469.31
train mean loss: 464.40
epoch train time: 0:00:02.181421
elapsed time: 0:05:08.324797
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 00:22:04.729464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.72
 ---- batch: 020 ----
mean loss: 460.33
 ---- batch: 030 ----
mean loss: 454.87
 ---- batch: 040 ----
mean loss: 453.30
 ---- batch: 050 ----
mean loss: 464.43
 ---- batch: 060 ----
mean loss: 451.83
 ---- batch: 070 ----
mean loss: 454.75
 ---- batch: 080 ----
mean loss: 448.72
 ---- batch: 090 ----
mean loss: 475.00
 ---- batch: 100 ----
mean loss: 456.51
 ---- batch: 110 ----
mean loss: 450.49
train mean loss: 457.18
epoch train time: 0:00:02.182347
elapsed time: 0:05:10.507335
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 00:22:06.912017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.68
 ---- batch: 020 ----
mean loss: 451.03
 ---- batch: 030 ----
mean loss: 464.04
 ---- batch: 040 ----
mean loss: 467.86
 ---- batch: 050 ----
mean loss: 464.73
 ---- batch: 060 ----
mean loss: 448.54
 ---- batch: 070 ----
mean loss: 460.51
 ---- batch: 080 ----
mean loss: 448.24
 ---- batch: 090 ----
mean loss: 451.25
 ---- batch: 100 ----
mean loss: 463.62
 ---- batch: 110 ----
mean loss: 449.21
train mean loss: 457.85
epoch train time: 0:00:02.184603
elapsed time: 0:05:12.692107
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 00:22:09.096807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.45
 ---- batch: 020 ----
mean loss: 463.99
 ---- batch: 030 ----
mean loss: 450.20
 ---- batch: 040 ----
mean loss: 454.92
 ---- batch: 050 ----
mean loss: 449.89
 ---- batch: 060 ----
mean loss: 451.91
 ---- batch: 070 ----
mean loss: 434.60
 ---- batch: 080 ----
mean loss: 438.30
 ---- batch: 090 ----
mean loss: 455.61
 ---- batch: 100 ----
mean loss: 444.57
 ---- batch: 110 ----
mean loss: 458.79
train mean loss: 451.96
epoch train time: 0:00:02.188555
elapsed time: 0:05:14.880851
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 00:22:11.285526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.48
 ---- batch: 020 ----
mean loss: 449.31
 ---- batch: 030 ----
mean loss: 446.55
 ---- batch: 040 ----
mean loss: 463.63
 ---- batch: 050 ----
mean loss: 446.85
 ---- batch: 060 ----
mean loss: 455.47
 ---- batch: 070 ----
mean loss: 451.68
 ---- batch: 080 ----
mean loss: 454.46
 ---- batch: 090 ----
mean loss: 442.40
 ---- batch: 100 ----
mean loss: 457.95
 ---- batch: 110 ----
mean loss: 459.91
train mean loss: 451.96
epoch train time: 0:00:02.184362
elapsed time: 0:05:17.065372
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 00:22:13.470046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.96
 ---- batch: 020 ----
mean loss: 461.08
 ---- batch: 030 ----
mean loss: 479.89
 ---- batch: 040 ----
mean loss: 451.95
 ---- batch: 050 ----
mean loss: 455.12
 ---- batch: 060 ----
mean loss: 460.93
 ---- batch: 070 ----
mean loss: 459.95
 ---- batch: 080 ----
mean loss: 454.39
 ---- batch: 090 ----
mean loss: 465.07
 ---- batch: 100 ----
mean loss: 455.89
 ---- batch: 110 ----
mean loss: 456.35
train mean loss: 460.35
epoch train time: 0:00:02.185755
elapsed time: 0:05:19.251309
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 00:22:15.655988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.61
 ---- batch: 020 ----
mean loss: 456.09
 ---- batch: 030 ----
mean loss: 454.05
 ---- batch: 040 ----
mean loss: 441.52
 ---- batch: 050 ----
mean loss: 441.43
 ---- batch: 060 ----
mean loss: 455.67
 ---- batch: 070 ----
mean loss: 464.07
 ---- batch: 080 ----
mean loss: 453.40
 ---- batch: 090 ----
mean loss: 445.72
 ---- batch: 100 ----
mean loss: 456.55
 ---- batch: 110 ----
mean loss: 451.84
train mean loss: 451.30
epoch train time: 0:00:02.181879
elapsed time: 0:05:21.433412
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 00:22:17.838106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.65
 ---- batch: 020 ----
mean loss: 464.31
 ---- batch: 030 ----
mean loss: 458.66
 ---- batch: 040 ----
mean loss: 453.20
 ---- batch: 050 ----
mean loss: 444.10
 ---- batch: 060 ----
mean loss: 464.46
 ---- batch: 070 ----
mean loss: 458.92
 ---- batch: 080 ----
mean loss: 461.79
 ---- batch: 090 ----
mean loss: 452.87
 ---- batch: 100 ----
mean loss: 441.51
 ---- batch: 110 ----
mean loss: 454.42
train mean loss: 455.76
epoch train time: 0:00:02.180710
elapsed time: 0:05:23.614325
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 00:22:20.019007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.89
 ---- batch: 020 ----
mean loss: 461.57
 ---- batch: 030 ----
mean loss: 448.94
 ---- batch: 040 ----
mean loss: 451.39
 ---- batch: 050 ----
mean loss: 451.60
 ---- batch: 060 ----
mean loss: 465.27
 ---- batch: 070 ----
mean loss: 437.97
 ---- batch: 080 ----
mean loss: 449.71
 ---- batch: 090 ----
mean loss: 445.41
 ---- batch: 100 ----
mean loss: 442.88
 ---- batch: 110 ----
mean loss: 434.26
train mean loss: 448.60
epoch train time: 0:00:02.186313
elapsed time: 0:05:25.800803
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 00:22:22.205498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.12
 ---- batch: 020 ----
mean loss: 455.13
 ---- batch: 030 ----
mean loss: 451.88
 ---- batch: 040 ----
mean loss: 457.71
 ---- batch: 050 ----
mean loss: 453.12
 ---- batch: 060 ----
mean loss: 454.42
 ---- batch: 070 ----
mean loss: 445.53
 ---- batch: 080 ----
mean loss: 444.04
 ---- batch: 090 ----
mean loss: 460.00
 ---- batch: 100 ----
mean loss: 439.38
 ---- batch: 110 ----
mean loss: 438.25
train mean loss: 448.70
epoch train time: 0:00:02.184919
elapsed time: 0:05:27.985900
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 00:22:24.390595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.91
 ---- batch: 020 ----
mean loss: 447.00
 ---- batch: 030 ----
mean loss: 449.58
 ---- batch: 040 ----
mean loss: 443.63
 ---- batch: 050 ----
mean loss: 441.08
 ---- batch: 060 ----
mean loss: 447.20
 ---- batch: 070 ----
mean loss: 461.86
 ---- batch: 080 ----
mean loss: 439.01
 ---- batch: 090 ----
mean loss: 439.37
 ---- batch: 100 ----
mean loss: 427.18
 ---- batch: 110 ----
mean loss: 447.82
train mean loss: 444.84
epoch train time: 0:00:02.178601
elapsed time: 0:05:30.164700
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 00:22:26.569377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.08
 ---- batch: 020 ----
mean loss: 448.05
 ---- batch: 030 ----
mean loss: 437.14
 ---- batch: 040 ----
mean loss: 438.22
 ---- batch: 050 ----
mean loss: 440.87
 ---- batch: 060 ----
mean loss: 432.97
 ---- batch: 070 ----
mean loss: 443.18
 ---- batch: 080 ----
mean loss: 459.68
 ---- batch: 090 ----
mean loss: 434.54
 ---- batch: 100 ----
mean loss: 452.44
 ---- batch: 110 ----
mean loss: 463.26
train mean loss: 444.39
epoch train time: 0:00:02.188895
elapsed time: 0:05:32.353772
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 00:22:28.758450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.99
 ---- batch: 020 ----
mean loss: 433.21
 ---- batch: 030 ----
mean loss: 430.63
 ---- batch: 040 ----
mean loss: 433.43
 ---- batch: 050 ----
mean loss: 446.38
 ---- batch: 060 ----
mean loss: 438.06
 ---- batch: 070 ----
mean loss: 428.21
 ---- batch: 080 ----
mean loss: 433.98
 ---- batch: 090 ----
mean loss: 445.59
 ---- batch: 100 ----
mean loss: 432.34
 ---- batch: 110 ----
mean loss: 443.03
train mean loss: 437.99
epoch train time: 0:00:02.181960
elapsed time: 0:05:34.535909
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 00:22:30.940585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.94
 ---- batch: 020 ----
mean loss: 437.05
 ---- batch: 030 ----
mean loss: 432.52
 ---- batch: 040 ----
mean loss: 435.62
 ---- batch: 050 ----
mean loss: 455.42
 ---- batch: 060 ----
mean loss: 442.63
 ---- batch: 070 ----
mean loss: 424.70
 ---- batch: 080 ----
mean loss: 434.43
 ---- batch: 090 ----
mean loss: 446.57
 ---- batch: 100 ----
mean loss: 426.80
 ---- batch: 110 ----
mean loss: 439.07
train mean loss: 437.61
epoch train time: 0:00:02.181738
elapsed time: 0:05:36.717841
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 00:22:33.122517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.60
 ---- batch: 020 ----
mean loss: 441.98
 ---- batch: 030 ----
mean loss: 434.23
 ---- batch: 040 ----
mean loss: 437.28
 ---- batch: 050 ----
mean loss: 444.66
 ---- batch: 060 ----
mean loss: 439.80
 ---- batch: 070 ----
mean loss: 432.85
 ---- batch: 080 ----
mean loss: 435.00
 ---- batch: 090 ----
mean loss: 449.13
 ---- batch: 100 ----
mean loss: 445.56
 ---- batch: 110 ----
mean loss: 448.02
train mean loss: 440.34
epoch train time: 0:00:02.179639
elapsed time: 0:05:38.897652
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 00:22:35.302334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.14
 ---- batch: 020 ----
mean loss: 432.08
 ---- batch: 030 ----
mean loss: 437.02
 ---- batch: 040 ----
mean loss: 435.90
 ---- batch: 050 ----
mean loss: 420.70
 ---- batch: 060 ----
mean loss: 432.05
 ---- batch: 070 ----
mean loss: 450.34
 ---- batch: 080 ----
mean loss: 436.34
 ---- batch: 090 ----
mean loss: 437.68
 ---- batch: 100 ----
mean loss: 443.29
 ---- batch: 110 ----
mean loss: 416.39
train mean loss: 434.82
epoch train time: 0:00:02.183748
elapsed time: 0:05:41.081604
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 00:22:37.486304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.94
 ---- batch: 020 ----
mean loss: 448.10
 ---- batch: 030 ----
mean loss: 417.82
 ---- batch: 040 ----
mean loss: 426.08
 ---- batch: 050 ----
mean loss: 417.82
 ---- batch: 060 ----
mean loss: 433.19
 ---- batch: 070 ----
mean loss: 423.94
 ---- batch: 080 ----
mean loss: 434.26
 ---- batch: 090 ----
mean loss: 420.70
 ---- batch: 100 ----
mean loss: 438.56
 ---- batch: 110 ----
mean loss: 430.37
train mean loss: 425.90
epoch train time: 0:00:02.199023
elapsed time: 0:05:43.280828
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 00:22:39.685506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.04
 ---- batch: 020 ----
mean loss: 419.82
 ---- batch: 030 ----
mean loss: 437.10
 ---- batch: 040 ----
mean loss: 450.49
 ---- batch: 050 ----
mean loss: 436.48
 ---- batch: 060 ----
mean loss: 423.20
 ---- batch: 070 ----
mean loss: 435.24
 ---- batch: 080 ----
mean loss: 440.93
 ---- batch: 090 ----
mean loss: 415.39
 ---- batch: 100 ----
mean loss: 436.00
 ---- batch: 110 ----
mean loss: 429.34
train mean loss: 431.71
epoch train time: 0:00:02.194816
elapsed time: 0:05:45.475820
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 00:22:41.880496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.33
 ---- batch: 020 ----
mean loss: 427.13
 ---- batch: 030 ----
mean loss: 423.12
 ---- batch: 040 ----
mean loss: 428.89
 ---- batch: 050 ----
mean loss: 424.55
 ---- batch: 060 ----
mean loss: 430.81
 ---- batch: 070 ----
mean loss: 433.70
 ---- batch: 080 ----
mean loss: 429.42
 ---- batch: 090 ----
mean loss: 420.36
 ---- batch: 100 ----
mean loss: 441.85
 ---- batch: 110 ----
mean loss: 423.18
train mean loss: 428.90
epoch train time: 0:00:02.188336
elapsed time: 0:05:47.664338
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 00:22:44.069017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.43
 ---- batch: 020 ----
mean loss: 414.43
 ---- batch: 030 ----
mean loss: 434.23
 ---- batch: 040 ----
mean loss: 421.72
 ---- batch: 050 ----
mean loss: 436.60
 ---- batch: 060 ----
mean loss: 440.44
 ---- batch: 070 ----
mean loss: 431.50
 ---- batch: 080 ----
mean loss: 432.88
 ---- batch: 090 ----
mean loss: 430.89
 ---- batch: 100 ----
mean loss: 422.75
 ---- batch: 110 ----
mean loss: 415.49
train mean loss: 426.56
epoch train time: 0:00:02.180028
elapsed time: 0:05:49.844532
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 00:22:46.249207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.07
 ---- batch: 020 ----
mean loss: 423.80
 ---- batch: 030 ----
mean loss: 419.52
 ---- batch: 040 ----
mean loss: 407.95
 ---- batch: 050 ----
mean loss: 422.17
 ---- batch: 060 ----
mean loss: 414.19
 ---- batch: 070 ----
mean loss: 418.96
 ---- batch: 080 ----
mean loss: 414.85
 ---- batch: 090 ----
mean loss: 428.56
 ---- batch: 100 ----
mean loss: 423.47
 ---- batch: 110 ----
mean loss: 413.97
train mean loss: 421.28
epoch train time: 0:00:02.179101
elapsed time: 0:05:52.023788
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 00:22:48.428463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.32
 ---- batch: 020 ----
mean loss: 434.29
 ---- batch: 030 ----
mean loss: 419.09
 ---- batch: 040 ----
mean loss: 413.20
 ---- batch: 050 ----
mean loss: 432.48
 ---- batch: 060 ----
mean loss: 411.60
 ---- batch: 070 ----
mean loss: 418.82
 ---- batch: 080 ----
mean loss: 424.48
 ---- batch: 090 ----
mean loss: 414.20
 ---- batch: 100 ----
mean loss: 418.13
 ---- batch: 110 ----
mean loss: 395.89
train mean loss: 418.00
epoch train time: 0:00:02.182861
elapsed time: 0:05:54.206826
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 00:22:50.611516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.96
 ---- batch: 020 ----
mean loss: 413.88
 ---- batch: 030 ----
mean loss: 431.48
 ---- batch: 040 ----
mean loss: 415.24
 ---- batch: 050 ----
mean loss: 428.18
 ---- batch: 060 ----
mean loss: 413.49
 ---- batch: 070 ----
mean loss: 427.71
 ---- batch: 080 ----
mean loss: 443.17
 ---- batch: 090 ----
mean loss: 413.94
 ---- batch: 100 ----
mean loss: 417.09
 ---- batch: 110 ----
mean loss: 408.64
train mean loss: 419.65
epoch train time: 0:00:02.180772
elapsed time: 0:05:56.387790
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 00:22:52.792475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.64
 ---- batch: 020 ----
mean loss: 421.34
 ---- batch: 030 ----
mean loss: 403.92
 ---- batch: 040 ----
mean loss: 401.70
 ---- batch: 050 ----
mean loss: 422.44
 ---- batch: 060 ----
mean loss: 422.33
 ---- batch: 070 ----
mean loss: 425.67
 ---- batch: 080 ----
mean loss: 423.51
 ---- batch: 090 ----
mean loss: 416.33
 ---- batch: 100 ----
mean loss: 424.01
 ---- batch: 110 ----
mean loss: 412.34
train mean loss: 417.23
epoch train time: 0:00:02.183249
elapsed time: 0:05:58.571217
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 00:22:54.975895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.59
 ---- batch: 020 ----
mean loss: 416.18
 ---- batch: 030 ----
mean loss: 412.98
 ---- batch: 040 ----
mean loss: 415.68
 ---- batch: 050 ----
mean loss: 409.07
 ---- batch: 060 ----
mean loss: 434.00
 ---- batch: 070 ----
mean loss: 427.26
 ---- batch: 080 ----
mean loss: 431.94
 ---- batch: 090 ----
mean loss: 412.27
 ---- batch: 100 ----
mean loss: 402.73
 ---- batch: 110 ----
mean loss: 409.68
train mean loss: 416.10
epoch train time: 0:00:02.181293
elapsed time: 0:06:00.752684
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 00:22:57.157356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.84
 ---- batch: 020 ----
mean loss: 403.71
 ---- batch: 030 ----
mean loss: 413.34
 ---- batch: 040 ----
mean loss: 423.06
 ---- batch: 050 ----
mean loss: 412.37
 ---- batch: 060 ----
mean loss: 424.82
 ---- batch: 070 ----
mean loss: 414.87
 ---- batch: 080 ----
mean loss: 412.70
 ---- batch: 090 ----
mean loss: 418.66
 ---- batch: 100 ----
mean loss: 418.25
 ---- batch: 110 ----
mean loss: 401.57
train mean loss: 413.60
epoch train time: 0:00:02.180525
elapsed time: 0:06:02.933377
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 00:22:59.338061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.05
 ---- batch: 020 ----
mean loss: 409.53
 ---- batch: 030 ----
mean loss: 405.11
 ---- batch: 040 ----
mean loss: 405.77
 ---- batch: 050 ----
mean loss: 404.66
 ---- batch: 060 ----
mean loss: 401.69
 ---- batch: 070 ----
mean loss: 417.96
 ---- batch: 080 ----
mean loss: 404.26
 ---- batch: 090 ----
mean loss: 414.06
 ---- batch: 100 ----
mean loss: 411.95
 ---- batch: 110 ----
mean loss: 414.42
train mean loss: 410.21
epoch train time: 0:00:02.172074
elapsed time: 0:06:05.105640
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 00:23:01.510315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.56
 ---- batch: 020 ----
mean loss: 396.59
 ---- batch: 030 ----
mean loss: 394.53
 ---- batch: 040 ----
mean loss: 407.58
 ---- batch: 050 ----
mean loss: 405.56
 ---- batch: 060 ----
mean loss: 406.60
 ---- batch: 070 ----
mean loss: 398.53
 ---- batch: 080 ----
mean loss: 409.45
 ---- batch: 090 ----
mean loss: 399.69
 ---- batch: 100 ----
mean loss: 406.45
 ---- batch: 110 ----
mean loss: 404.12
train mean loss: 403.58
epoch train time: 0:00:02.173641
elapsed time: 0:06:07.279461
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 00:23:03.684135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.35
 ---- batch: 020 ----
mean loss: 396.83
 ---- batch: 030 ----
mean loss: 403.29
 ---- batch: 040 ----
mean loss: 399.42
 ---- batch: 050 ----
mean loss: 403.46
 ---- batch: 060 ----
mean loss: 408.33
 ---- batch: 070 ----
mean loss: 407.59
 ---- batch: 080 ----
mean loss: 404.47
 ---- batch: 090 ----
mean loss: 407.21
 ---- batch: 100 ----
mean loss: 401.67
 ---- batch: 110 ----
mean loss: 402.60
train mean loss: 403.27
epoch train time: 0:00:02.174620
elapsed time: 0:06:09.454252
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 00:23:05.859148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.62
 ---- batch: 020 ----
mean loss: 408.70
 ---- batch: 030 ----
mean loss: 385.55
 ---- batch: 040 ----
mean loss: 411.66
 ---- batch: 050 ----
mean loss: 400.87
 ---- batch: 060 ----
mean loss: 397.45
 ---- batch: 070 ----
mean loss: 405.44
 ---- batch: 080 ----
mean loss: 410.16
 ---- batch: 090 ----
mean loss: 404.35
 ---- batch: 100 ----
mean loss: 410.65
 ---- batch: 110 ----
mean loss: 408.61
train mean loss: 403.88
epoch train time: 0:00:02.171841
elapsed time: 0:06:11.626494
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 00:23:08.031184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.53
 ---- batch: 020 ----
mean loss: 394.96
 ---- batch: 030 ----
mean loss: 409.38
 ---- batch: 040 ----
mean loss: 403.44
 ---- batch: 050 ----
mean loss: 404.85
 ---- batch: 060 ----
mean loss: 394.81
 ---- batch: 070 ----
mean loss: 405.60
 ---- batch: 080 ----
mean loss: 392.37
 ---- batch: 090 ----
mean loss: 388.54
 ---- batch: 100 ----
mean loss: 397.64
 ---- batch: 110 ----
mean loss: 388.57
train mean loss: 399.43
epoch train time: 0:00:02.174469
elapsed time: 0:06:13.801160
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 00:23:10.205853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.99
 ---- batch: 020 ----
mean loss: 394.50
 ---- batch: 030 ----
mean loss: 393.33
 ---- batch: 040 ----
mean loss: 396.49
 ---- batch: 050 ----
mean loss: 393.36
 ---- batch: 060 ----
mean loss: 403.91
 ---- batch: 070 ----
mean loss: 393.51
 ---- batch: 080 ----
mean loss: 403.15
 ---- batch: 090 ----
mean loss: 410.86
 ---- batch: 100 ----
mean loss: 395.25
 ---- batch: 110 ----
mean loss: 401.67
train mean loss: 398.18
epoch train time: 0:00:02.167833
elapsed time: 0:06:15.969157
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 00:23:12.373827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.76
 ---- batch: 020 ----
mean loss: 400.96
 ---- batch: 030 ----
mean loss: 399.34
 ---- batch: 040 ----
mean loss: 395.07
 ---- batch: 050 ----
mean loss: 412.46
 ---- batch: 060 ----
mean loss: 409.81
 ---- batch: 070 ----
mean loss: 395.78
 ---- batch: 080 ----
mean loss: 386.65
 ---- batch: 090 ----
mean loss: 397.53
 ---- batch: 100 ----
mean loss: 407.67
 ---- batch: 110 ----
mean loss: 398.10
train mean loss: 401.24
epoch train time: 0:00:02.147404
elapsed time: 0:06:18.116704
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 00:23:14.521377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.62
 ---- batch: 020 ----
mean loss: 398.45
 ---- batch: 030 ----
mean loss: 388.20
 ---- batch: 040 ----
mean loss: 391.27
 ---- batch: 050 ----
mean loss: 389.13
 ---- batch: 060 ----
mean loss: 394.69
 ---- batch: 070 ----
mean loss: 401.31
 ---- batch: 080 ----
mean loss: 399.25
 ---- batch: 090 ----
mean loss: 385.07
 ---- batch: 100 ----
mean loss: 408.97
 ---- batch: 110 ----
mean loss: 404.83
train mean loss: 396.61
epoch train time: 0:00:02.150763
elapsed time: 0:06:20.267617
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 00:23:16.672290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.72
 ---- batch: 020 ----
mean loss: 381.26
 ---- batch: 030 ----
mean loss: 393.10
 ---- batch: 040 ----
mean loss: 396.06
 ---- batch: 050 ----
mean loss: 409.68
 ---- batch: 060 ----
mean loss: 396.49
 ---- batch: 070 ----
mean loss: 390.44
 ---- batch: 080 ----
mean loss: 408.63
 ---- batch: 090 ----
mean loss: 411.04
 ---- batch: 100 ----
mean loss: 385.21
 ---- batch: 110 ----
mean loss: 387.48
train mean loss: 397.07
epoch train time: 0:00:02.145506
elapsed time: 0:06:22.413274
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 00:23:18.817944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.52
 ---- batch: 020 ----
mean loss: 397.01
 ---- batch: 030 ----
mean loss: 402.41
 ---- batch: 040 ----
mean loss: 397.84
 ---- batch: 050 ----
mean loss: 406.57
 ---- batch: 060 ----
mean loss: 400.12
 ---- batch: 070 ----
mean loss: 385.60
 ---- batch: 080 ----
mean loss: 387.29
 ---- batch: 090 ----
mean loss: 401.20
 ---- batch: 100 ----
mean loss: 408.14
 ---- batch: 110 ----
mean loss: 387.53
train mean loss: 396.12
epoch train time: 0:00:02.150964
elapsed time: 0:06:24.564380
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 00:23:20.969073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.07
 ---- batch: 020 ----
mean loss: 385.33
 ---- batch: 030 ----
mean loss: 406.80
 ---- batch: 040 ----
mean loss: 381.18
 ---- batch: 050 ----
mean loss: 404.19
 ---- batch: 060 ----
mean loss: 391.67
 ---- batch: 070 ----
mean loss: 399.32
 ---- batch: 080 ----
mean loss: 378.73
 ---- batch: 090 ----
mean loss: 395.71
 ---- batch: 100 ----
mean loss: 414.45
 ---- batch: 110 ----
mean loss: 396.10
train mean loss: 395.27
epoch train time: 0:00:02.148888
elapsed time: 0:06:26.713462
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 00:23:23.118139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.46
 ---- batch: 020 ----
mean loss: 392.12
 ---- batch: 030 ----
mean loss: 399.83
 ---- batch: 040 ----
mean loss: 389.75
 ---- batch: 050 ----
mean loss: 380.72
 ---- batch: 060 ----
mean loss: 390.72
 ---- batch: 070 ----
mean loss: 388.72
 ---- batch: 080 ----
mean loss: 405.83
 ---- batch: 090 ----
mean loss: 389.28
 ---- batch: 100 ----
mean loss: 378.84
 ---- batch: 110 ----
mean loss: 395.31
train mean loss: 391.30
epoch train time: 0:00:02.150893
elapsed time: 0:06:28.864522
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 00:23:25.269193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.88
 ---- batch: 020 ----
mean loss: 392.10
 ---- batch: 030 ----
mean loss: 385.74
 ---- batch: 040 ----
mean loss: 399.02
 ---- batch: 050 ----
mean loss: 409.56
 ---- batch: 060 ----
mean loss: 394.98
 ---- batch: 070 ----
mean loss: 409.18
 ---- batch: 080 ----
mean loss: 390.19
 ---- batch: 090 ----
mean loss: 389.59
 ---- batch: 100 ----
mean loss: 392.32
 ---- batch: 110 ----
mean loss: 389.61
train mean loss: 394.26
epoch train time: 0:00:02.155969
elapsed time: 0:06:31.020653
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 00:23:27.425329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.91
 ---- batch: 020 ----
mean loss: 384.38
 ---- batch: 030 ----
mean loss: 388.26
 ---- batch: 040 ----
mean loss: 399.40
 ---- batch: 050 ----
mean loss: 407.18
 ---- batch: 060 ----
mean loss: 386.77
 ---- batch: 070 ----
mean loss: 392.39
 ---- batch: 080 ----
mean loss: 396.24
 ---- batch: 090 ----
mean loss: 400.51
 ---- batch: 100 ----
mean loss: 395.08
 ---- batch: 110 ----
mean loss: 395.09
train mean loss: 394.87
epoch train time: 0:00:02.150364
elapsed time: 0:06:33.171186
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 00:23:29.575875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.94
 ---- batch: 020 ----
mean loss: 377.37
 ---- batch: 030 ----
mean loss: 389.97
 ---- batch: 040 ----
mean loss: 394.29
 ---- batch: 050 ----
mean loss: 369.55
 ---- batch: 060 ----
mean loss: 381.90
 ---- batch: 070 ----
mean loss: 377.01
 ---- batch: 080 ----
mean loss: 399.70
 ---- batch: 090 ----
mean loss: 400.97
 ---- batch: 100 ----
mean loss: 394.65
 ---- batch: 110 ----
mean loss: 381.28
train mean loss: 388.10
epoch train time: 0:00:02.155874
elapsed time: 0:06:35.327232
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 00:23:31.731949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.49
 ---- batch: 020 ----
mean loss: 382.21
 ---- batch: 030 ----
mean loss: 377.45
 ---- batch: 040 ----
mean loss: 384.64
 ---- batch: 050 ----
mean loss: 391.65
 ---- batch: 060 ----
mean loss: 395.00
 ---- batch: 070 ----
mean loss: 404.48
 ---- batch: 080 ----
mean loss: 393.57
 ---- batch: 090 ----
mean loss: 404.88
 ---- batch: 100 ----
mean loss: 392.26
 ---- batch: 110 ----
mean loss: 404.54
train mean loss: 392.48
epoch train time: 0:00:02.149553
elapsed time: 0:06:37.477006
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 00:23:33.881686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.95
 ---- batch: 020 ----
mean loss: 390.55
 ---- batch: 030 ----
mean loss: 378.51
 ---- batch: 040 ----
mean loss: 397.72
 ---- batch: 050 ----
mean loss: 397.16
 ---- batch: 060 ----
mean loss: 380.46
 ---- batch: 070 ----
mean loss: 371.63
 ---- batch: 080 ----
mean loss: 388.22
 ---- batch: 090 ----
mean loss: 377.92
 ---- batch: 100 ----
mean loss: 391.91
 ---- batch: 110 ----
mean loss: 371.83
train mean loss: 384.11
epoch train time: 0:00:02.158102
elapsed time: 0:06:39.635288
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 00:23:36.039997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.63
 ---- batch: 020 ----
mean loss: 381.32
 ---- batch: 030 ----
mean loss: 360.95
 ---- batch: 040 ----
mean loss: 393.35
 ---- batch: 050 ----
mean loss: 379.83
 ---- batch: 060 ----
mean loss: 396.87
 ---- batch: 070 ----
mean loss: 389.17
 ---- batch: 080 ----
mean loss: 389.88
 ---- batch: 090 ----
mean loss: 385.79
 ---- batch: 100 ----
mean loss: 387.89
 ---- batch: 110 ----
mean loss: 375.25
train mean loss: 383.75
epoch train time: 0:00:02.163585
elapsed time: 0:06:41.799070
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 00:23:38.203746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.37
 ---- batch: 020 ----
mean loss: 399.39
 ---- batch: 030 ----
mean loss: 389.82
 ---- batch: 040 ----
mean loss: 378.65
 ---- batch: 050 ----
mean loss: 379.76
 ---- batch: 060 ----
mean loss: 402.85
 ---- batch: 070 ----
mean loss: 378.10
 ---- batch: 080 ----
mean loss: 388.55
 ---- batch: 090 ----
mean loss: 375.81
 ---- batch: 100 ----
mean loss: 392.88
 ---- batch: 110 ----
mean loss: 371.05
train mean loss: 385.71
epoch train time: 0:00:02.154697
elapsed time: 0:06:43.953938
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 00:23:40.358618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.99
 ---- batch: 020 ----
mean loss: 402.20
 ---- batch: 030 ----
mean loss: 375.11
 ---- batch: 040 ----
mean loss: 372.69
 ---- batch: 050 ----
mean loss: 379.20
 ---- batch: 060 ----
mean loss: 382.46
 ---- batch: 070 ----
mean loss: 390.69
 ---- batch: 080 ----
mean loss: 380.25
 ---- batch: 090 ----
mean loss: 382.78
 ---- batch: 100 ----
mean loss: 375.91
 ---- batch: 110 ----
mean loss: 382.27
train mean loss: 382.52
epoch train time: 0:00:02.171484
elapsed time: 0:06:46.125622
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 00:23:42.530298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.68
 ---- batch: 020 ----
mean loss: 383.76
 ---- batch: 030 ----
mean loss: 386.70
 ---- batch: 040 ----
mean loss: 373.34
 ---- batch: 050 ----
mean loss: 373.66
 ---- batch: 060 ----
mean loss: 395.51
 ---- batch: 070 ----
mean loss: 376.55
 ---- batch: 080 ----
mean loss: 387.81
 ---- batch: 090 ----
mean loss: 381.77
 ---- batch: 100 ----
mean loss: 370.48
 ---- batch: 110 ----
mean loss: 385.30
train mean loss: 381.96
epoch train time: 0:00:02.167251
elapsed time: 0:06:48.293048
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 00:23:44.697741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.49
 ---- batch: 020 ----
mean loss: 374.27
 ---- batch: 030 ----
mean loss: 390.93
 ---- batch: 040 ----
mean loss: 375.54
 ---- batch: 050 ----
mean loss: 372.63
 ---- batch: 060 ----
mean loss: 371.04
 ---- batch: 070 ----
mean loss: 370.97
 ---- batch: 080 ----
mean loss: 377.47
 ---- batch: 090 ----
mean loss: 384.36
 ---- batch: 100 ----
mean loss: 369.87
 ---- batch: 110 ----
mean loss: 372.56
train mean loss: 375.76
epoch train time: 0:00:02.165848
elapsed time: 0:06:50.459102
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 00:23:46.863783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.42
 ---- batch: 020 ----
mean loss: 384.41
 ---- batch: 030 ----
mean loss: 383.65
 ---- batch: 040 ----
mean loss: 370.91
 ---- batch: 050 ----
mean loss: 376.89
 ---- batch: 060 ----
mean loss: 359.78
 ---- batch: 070 ----
mean loss: 375.35
 ---- batch: 080 ----
mean loss: 364.57
 ---- batch: 090 ----
mean loss: 388.08
 ---- batch: 100 ----
mean loss: 377.73
 ---- batch: 110 ----
mean loss: 376.87
train mean loss: 376.57
epoch train time: 0:00:02.181347
elapsed time: 0:06:52.640644
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 00:23:49.045352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.35
 ---- batch: 020 ----
mean loss: 367.87
 ---- batch: 030 ----
mean loss: 375.47
 ---- batch: 040 ----
mean loss: 372.68
 ---- batch: 050 ----
mean loss: 371.48
 ---- batch: 060 ----
mean loss: 365.48
 ---- batch: 070 ----
mean loss: 380.01
 ---- batch: 080 ----
mean loss: 372.64
 ---- batch: 090 ----
mean loss: 384.80
 ---- batch: 100 ----
mean loss: 382.76
 ---- batch: 110 ----
mean loss: 363.30
train mean loss: 373.76
epoch train time: 0:00:02.179131
elapsed time: 0:06:54.819983
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 00:23:51.224663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.40
 ---- batch: 020 ----
mean loss: 367.94
 ---- batch: 030 ----
mean loss: 370.24
 ---- batch: 040 ----
mean loss: 384.19
 ---- batch: 050 ----
mean loss: 369.88
 ---- batch: 060 ----
mean loss: 378.76
 ---- batch: 070 ----
mean loss: 379.59
 ---- batch: 080 ----
mean loss: 363.22
 ---- batch: 090 ----
mean loss: 370.99
 ---- batch: 100 ----
mean loss: 363.73
 ---- batch: 110 ----
mean loss: 371.77
train mean loss: 371.39
epoch train time: 0:00:02.181005
elapsed time: 0:06:57.001177
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 00:23:53.405853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.41
 ---- batch: 020 ----
mean loss: 377.78
 ---- batch: 030 ----
mean loss: 362.84
 ---- batch: 040 ----
mean loss: 365.77
 ---- batch: 050 ----
mean loss: 363.54
 ---- batch: 060 ----
mean loss: 379.22
 ---- batch: 070 ----
mean loss: 357.28
 ---- batch: 080 ----
mean loss: 372.60
 ---- batch: 090 ----
mean loss: 377.76
 ---- batch: 100 ----
mean loss: 368.34
 ---- batch: 110 ----
mean loss: 365.28
train mean loss: 368.56
epoch train time: 0:00:02.183603
elapsed time: 0:06:59.185009
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 00:23:55.589695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.42
 ---- batch: 020 ----
mean loss: 367.23
 ---- batch: 030 ----
mean loss: 372.37
 ---- batch: 040 ----
mean loss: 351.66
 ---- batch: 050 ----
mean loss: 369.60
 ---- batch: 060 ----
mean loss: 366.42
 ---- batch: 070 ----
mean loss: 364.98
 ---- batch: 080 ----
mean loss: 367.54
 ---- batch: 090 ----
mean loss: 367.71
 ---- batch: 100 ----
mean loss: 367.50
 ---- batch: 110 ----
mean loss: 363.73
train mean loss: 366.48
epoch train time: 0:00:02.172651
elapsed time: 0:07:01.357863
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 00:23:57.762541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.15
 ---- batch: 020 ----
mean loss: 362.06
 ---- batch: 030 ----
mean loss: 364.09
 ---- batch: 040 ----
mean loss: 368.78
 ---- batch: 050 ----
mean loss: 367.45
 ---- batch: 060 ----
mean loss: 358.12
 ---- batch: 070 ----
mean loss: 370.80
 ---- batch: 080 ----
mean loss: 366.96
 ---- batch: 090 ----
mean loss: 367.83
 ---- batch: 100 ----
mean loss: 358.24
 ---- batch: 110 ----
mean loss: 365.72
train mean loss: 364.10
epoch train time: 0:00:02.188580
elapsed time: 0:07:03.546630
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 00:23:59.951307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.96
 ---- batch: 020 ----
mean loss: 374.37
 ---- batch: 030 ----
mean loss: 364.42
 ---- batch: 040 ----
mean loss: 359.67
 ---- batch: 050 ----
mean loss: 367.29
 ---- batch: 060 ----
mean loss: 373.03
 ---- batch: 070 ----
mean loss: 374.16
 ---- batch: 080 ----
mean loss: 369.31
 ---- batch: 090 ----
mean loss: 354.87
 ---- batch: 100 ----
mean loss: 361.54
 ---- batch: 110 ----
mean loss: 361.36
train mean loss: 365.01
epoch train time: 0:00:02.179409
elapsed time: 0:07:05.726201
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 00:24:02.130897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.80
 ---- batch: 020 ----
mean loss: 363.07
 ---- batch: 030 ----
mean loss: 363.80
 ---- batch: 040 ----
mean loss: 355.79
 ---- batch: 050 ----
mean loss: 352.39
 ---- batch: 060 ----
mean loss: 352.79
 ---- batch: 070 ----
mean loss: 359.44
 ---- batch: 080 ----
mean loss: 355.65
 ---- batch: 090 ----
mean loss: 363.49
 ---- batch: 100 ----
mean loss: 351.17
 ---- batch: 110 ----
mean loss: 362.39
train mean loss: 358.17
epoch train time: 0:00:02.180943
elapsed time: 0:07:07.907331
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 00:24:04.312028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.61
 ---- batch: 020 ----
mean loss: 368.51
 ---- batch: 030 ----
mean loss: 359.64
 ---- batch: 040 ----
mean loss: 354.83
 ---- batch: 050 ----
mean loss: 347.44
 ---- batch: 060 ----
mean loss: 370.46
 ---- batch: 070 ----
mean loss: 368.57
 ---- batch: 080 ----
mean loss: 345.28
 ---- batch: 090 ----
mean loss: 352.87
 ---- batch: 100 ----
mean loss: 347.25
 ---- batch: 110 ----
mean loss: 345.95
train mean loss: 357.47
epoch train time: 0:00:02.186407
elapsed time: 0:07:10.093970
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 00:24:06.498653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.49
 ---- batch: 020 ----
mean loss: 358.14
 ---- batch: 030 ----
mean loss: 350.25
 ---- batch: 040 ----
mean loss: 363.48
 ---- batch: 050 ----
mean loss: 340.30
 ---- batch: 060 ----
mean loss: 365.99
 ---- batch: 070 ----
mean loss: 346.55
 ---- batch: 080 ----
mean loss: 348.96
 ---- batch: 090 ----
mean loss: 363.41
 ---- batch: 100 ----
mean loss: 363.75
 ---- batch: 110 ----
mean loss: 356.56
train mean loss: 355.36
epoch train time: 0:00:02.182374
elapsed time: 0:07:12.276517
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 00:24:08.681190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.12
 ---- batch: 020 ----
mean loss: 359.48
 ---- batch: 030 ----
mean loss: 356.94
 ---- batch: 040 ----
mean loss: 355.93
 ---- batch: 050 ----
mean loss: 362.65
 ---- batch: 060 ----
mean loss: 355.18
 ---- batch: 070 ----
mean loss: 356.04
 ---- batch: 080 ----
mean loss: 343.82
 ---- batch: 090 ----
mean loss: 350.14
 ---- batch: 100 ----
mean loss: 350.66
 ---- batch: 110 ----
mean loss: 358.55
train mean loss: 353.86
epoch train time: 0:00:02.178515
elapsed time: 0:07:14.455205
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 00:24:10.859883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.13
 ---- batch: 020 ----
mean loss: 339.34
 ---- batch: 030 ----
mean loss: 356.40
 ---- batch: 040 ----
mean loss: 350.68
 ---- batch: 050 ----
mean loss: 352.83
 ---- batch: 060 ----
mean loss: 352.28
 ---- batch: 070 ----
mean loss: 361.69
 ---- batch: 080 ----
mean loss: 347.79
 ---- batch: 090 ----
mean loss: 347.62
 ---- batch: 100 ----
mean loss: 337.41
 ---- batch: 110 ----
mean loss: 357.91
train mean loss: 350.21
epoch train time: 0:00:02.179969
elapsed time: 0:07:16.635356
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 00:24:13.040039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.08
 ---- batch: 020 ----
mean loss: 345.39
 ---- batch: 030 ----
mean loss: 360.08
 ---- batch: 040 ----
mean loss: 350.38
 ---- batch: 050 ----
mean loss: 353.12
 ---- batch: 060 ----
mean loss: 348.06
 ---- batch: 070 ----
mean loss: 354.53
 ---- batch: 080 ----
mean loss: 330.75
 ---- batch: 090 ----
mean loss: 338.36
 ---- batch: 100 ----
mean loss: 347.88
 ---- batch: 110 ----
mean loss: 340.83
train mean loss: 348.30
epoch train time: 0:00:02.177841
elapsed time: 0:07:18.813365
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 00:24:15.218041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.16
 ---- batch: 020 ----
mean loss: 350.74
 ---- batch: 030 ----
mean loss: 360.91
 ---- batch: 040 ----
mean loss: 336.76
 ---- batch: 050 ----
mean loss: 346.78
 ---- batch: 060 ----
mean loss: 357.46
 ---- batch: 070 ----
mean loss: 334.41
 ---- batch: 080 ----
mean loss: 339.38
 ---- batch: 090 ----
mean loss: 350.10
 ---- batch: 100 ----
mean loss: 343.28
 ---- batch: 110 ----
mean loss: 344.07
train mean loss: 348.02
epoch train time: 0:00:02.179532
elapsed time: 0:07:20.993092
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 00:24:17.397791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.86
 ---- batch: 020 ----
mean loss: 358.01
 ---- batch: 030 ----
mean loss: 353.83
 ---- batch: 040 ----
mean loss: 339.83
 ---- batch: 050 ----
mean loss: 348.59
 ---- batch: 060 ----
mean loss: 352.17
 ---- batch: 070 ----
mean loss: 351.75
 ---- batch: 080 ----
mean loss: 344.59
 ---- batch: 090 ----
mean loss: 346.27
 ---- batch: 100 ----
mean loss: 338.55
 ---- batch: 110 ----
mean loss: 336.68
train mean loss: 347.73
epoch train time: 0:00:02.186234
elapsed time: 0:07:23.179511
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 00:24:19.584184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.00
 ---- batch: 020 ----
mean loss: 341.94
 ---- batch: 030 ----
mean loss: 364.48
 ---- batch: 040 ----
mean loss: 371.55
 ---- batch: 050 ----
mean loss: 358.84
 ---- batch: 060 ----
mean loss: 346.38
 ---- batch: 070 ----
mean loss: 331.88
 ---- batch: 080 ----
mean loss: 344.44
 ---- batch: 090 ----
mean loss: 344.21
 ---- batch: 100 ----
mean loss: 344.97
 ---- batch: 110 ----
mean loss: 332.51
train mean loss: 347.17
epoch train time: 0:00:02.184096
elapsed time: 0:07:25.363763
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 00:24:21.768439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.02
 ---- batch: 020 ----
mean loss: 331.89
 ---- batch: 030 ----
mean loss: 352.92
 ---- batch: 040 ----
mean loss: 326.55
 ---- batch: 050 ----
mean loss: 341.00
 ---- batch: 060 ----
mean loss: 343.80
 ---- batch: 070 ----
mean loss: 344.12
 ---- batch: 080 ----
mean loss: 342.34
 ---- batch: 090 ----
mean loss: 341.95
 ---- batch: 100 ----
mean loss: 349.29
 ---- batch: 110 ----
mean loss: 355.57
train mean loss: 341.70
epoch train time: 0:00:02.178101
elapsed time: 0:07:27.542037
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 00:24:23.946713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.24
 ---- batch: 020 ----
mean loss: 335.04
 ---- batch: 030 ----
mean loss: 327.84
 ---- batch: 040 ----
mean loss: 355.91
 ---- batch: 050 ----
mean loss: 346.83
 ---- batch: 060 ----
mean loss: 347.04
 ---- batch: 070 ----
mean loss: 346.46
 ---- batch: 080 ----
mean loss: 341.18
 ---- batch: 090 ----
mean loss: 339.67
 ---- batch: 100 ----
mean loss: 343.98
 ---- batch: 110 ----
mean loss: 340.82
train mean loss: 342.39
epoch train time: 0:00:02.179590
elapsed time: 0:07:29.721789
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 00:24:26.126476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.87
 ---- batch: 020 ----
mean loss: 340.65
 ---- batch: 030 ----
mean loss: 340.55
 ---- batch: 040 ----
mean loss: 344.74
 ---- batch: 050 ----
mean loss: 340.43
 ---- batch: 060 ----
mean loss: 350.13
 ---- batch: 070 ----
mean loss: 327.57
 ---- batch: 080 ----
mean loss: 343.95
 ---- batch: 090 ----
mean loss: 336.13
 ---- batch: 100 ----
mean loss: 333.68
 ---- batch: 110 ----
mean loss: 347.19
train mean loss: 341.26
epoch train time: 0:00:02.185243
elapsed time: 0:07:31.907216
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 00:24:28.311895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.51
 ---- batch: 020 ----
mean loss: 350.71
 ---- batch: 030 ----
mean loss: 339.92
 ---- batch: 040 ----
mean loss: 336.02
 ---- batch: 050 ----
mean loss: 348.36
 ---- batch: 060 ----
mean loss: 329.32
 ---- batch: 070 ----
mean loss: 337.57
 ---- batch: 080 ----
mean loss: 344.88
 ---- batch: 090 ----
mean loss: 328.88
 ---- batch: 100 ----
mean loss: 340.59
 ---- batch: 110 ----
mean loss: 347.07
train mean loss: 340.63
epoch train time: 0:00:02.180789
elapsed time: 0:07:34.088188
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 00:24:30.492863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.75
 ---- batch: 020 ----
mean loss: 349.70
 ---- batch: 030 ----
mean loss: 338.48
 ---- batch: 040 ----
mean loss: 334.82
 ---- batch: 050 ----
mean loss: 324.80
 ---- batch: 060 ----
mean loss: 345.31
 ---- batch: 070 ----
mean loss: 342.99
 ---- batch: 080 ----
mean loss: 333.09
 ---- batch: 090 ----
mean loss: 332.15
 ---- batch: 100 ----
mean loss: 340.95
 ---- batch: 110 ----
mean loss: 333.78
train mean loss: 337.20
epoch train time: 0:00:02.177717
elapsed time: 0:07:36.266082
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 00:24:32.670778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.68
 ---- batch: 020 ----
mean loss: 341.45
 ---- batch: 030 ----
mean loss: 347.89
 ---- batch: 040 ----
mean loss: 340.00
 ---- batch: 050 ----
mean loss: 326.09
 ---- batch: 060 ----
mean loss: 341.09
 ---- batch: 070 ----
mean loss: 340.53
 ---- batch: 080 ----
mean loss: 342.42
 ---- batch: 090 ----
mean loss: 342.59
 ---- batch: 100 ----
mean loss: 322.44
 ---- batch: 110 ----
mean loss: 348.41
train mean loss: 339.26
epoch train time: 0:00:02.183453
elapsed time: 0:07:38.449736
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 00:24:34.854429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.93
 ---- batch: 020 ----
mean loss: 336.46
 ---- batch: 030 ----
mean loss: 340.49
 ---- batch: 040 ----
mean loss: 351.16
 ---- batch: 050 ----
mean loss: 333.46
 ---- batch: 060 ----
mean loss: 335.25
 ---- batch: 070 ----
mean loss: 339.46
 ---- batch: 080 ----
mean loss: 341.97
 ---- batch: 090 ----
mean loss: 332.96
 ---- batch: 100 ----
mean loss: 332.37
 ---- batch: 110 ----
mean loss: 340.90
train mean loss: 338.56
epoch train time: 0:00:02.184132
elapsed time: 0:07:40.634074
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 00:24:37.038752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.11
 ---- batch: 020 ----
mean loss: 344.25
 ---- batch: 030 ----
mean loss: 343.38
 ---- batch: 040 ----
mean loss: 328.27
 ---- batch: 050 ----
mean loss: 352.45
 ---- batch: 060 ----
mean loss: 320.73
 ---- batch: 070 ----
mean loss: 342.72
 ---- batch: 080 ----
mean loss: 335.27
 ---- batch: 090 ----
mean loss: 328.93
 ---- batch: 100 ----
mean loss: 337.56
 ---- batch: 110 ----
mean loss: 315.05
train mean loss: 335.62
epoch train time: 0:00:02.179699
elapsed time: 0:07:42.813945
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 00:24:39.218622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.56
 ---- batch: 020 ----
mean loss: 331.19
 ---- batch: 030 ----
mean loss: 334.69
 ---- batch: 040 ----
mean loss: 333.78
 ---- batch: 050 ----
mean loss: 331.64
 ---- batch: 060 ----
mean loss: 338.11
 ---- batch: 070 ----
mean loss: 328.78
 ---- batch: 080 ----
mean loss: 327.58
 ---- batch: 090 ----
mean loss: 339.01
 ---- batch: 100 ----
mean loss: 337.44
 ---- batch: 110 ----
mean loss: 338.74
train mean loss: 334.88
epoch train time: 0:00:02.180737
elapsed time: 0:07:44.994905
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 00:24:41.399586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.44
 ---- batch: 020 ----
mean loss: 313.94
 ---- batch: 030 ----
mean loss: 347.49
 ---- batch: 040 ----
mean loss: 333.41
 ---- batch: 050 ----
mean loss: 324.53
 ---- batch: 060 ----
mean loss: 344.68
 ---- batch: 070 ----
mean loss: 336.86
 ---- batch: 080 ----
mean loss: 332.89
 ---- batch: 090 ----
mean loss: 327.28
 ---- batch: 100 ----
mean loss: 324.74
 ---- batch: 110 ----
mean loss: 333.57
train mean loss: 332.57
epoch train time: 0:00:02.178870
elapsed time: 0:07:47.174003
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 00:24:43.578678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.59
 ---- batch: 020 ----
mean loss: 328.18
 ---- batch: 030 ----
mean loss: 322.27
 ---- batch: 040 ----
mean loss: 328.19
 ---- batch: 050 ----
mean loss: 328.61
 ---- batch: 060 ----
mean loss: 334.43
 ---- batch: 070 ----
mean loss: 334.74
 ---- batch: 080 ----
mean loss: 321.70
 ---- batch: 090 ----
mean loss: 330.88
 ---- batch: 100 ----
mean loss: 334.34
 ---- batch: 110 ----
mean loss: 358.22
train mean loss: 332.55
epoch train time: 0:00:02.181155
elapsed time: 0:07:49.355332
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 00:24:45.760010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 328.12
 ---- batch: 020 ----
mean loss: 322.75
 ---- batch: 030 ----
mean loss: 330.22
 ---- batch: 040 ----
mean loss: 330.44
 ---- batch: 050 ----
mean loss: 327.73
 ---- batch: 060 ----
mean loss: 330.01
 ---- batch: 070 ----
mean loss: 320.47
 ---- batch: 080 ----
mean loss: 328.68
 ---- batch: 090 ----
mean loss: 328.29
 ---- batch: 100 ----
mean loss: 328.21
 ---- batch: 110 ----
mean loss: 314.17
train mean loss: 325.50
epoch train time: 0:00:02.185717
elapsed time: 0:07:51.541242
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 00:24:47.945929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 330.23
 ---- batch: 020 ----
mean loss: 314.36
 ---- batch: 030 ----
mean loss: 309.33
 ---- batch: 040 ----
mean loss: 332.16
 ---- batch: 050 ----
mean loss: 320.71
 ---- batch: 060 ----
mean loss: 319.22
 ---- batch: 070 ----
mean loss: 307.67
 ---- batch: 080 ----
mean loss: 330.70
 ---- batch: 090 ----
mean loss: 344.59
 ---- batch: 100 ----
mean loss: 322.70
 ---- batch: 110 ----
mean loss: 317.40
train mean loss: 322.67
epoch train time: 0:00:02.184176
elapsed time: 0:07:53.725589
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 00:24:50.130263
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.95
 ---- batch: 020 ----
mean loss: 333.96
 ---- batch: 030 ----
mean loss: 327.69
 ---- batch: 040 ----
mean loss: 323.12
 ---- batch: 050 ----
mean loss: 332.08
 ---- batch: 060 ----
mean loss: 324.22
 ---- batch: 070 ----
mean loss: 324.85
 ---- batch: 080 ----
mean loss: 326.86
 ---- batch: 090 ----
mean loss: 319.15
 ---- batch: 100 ----
mean loss: 321.12
 ---- batch: 110 ----
mean loss: 321.00
train mean loss: 324.45
epoch train time: 0:00:02.178792
elapsed time: 0:07:55.904558
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 00:24:52.309236
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 332.24
 ---- batch: 020 ----
mean loss: 328.00
 ---- batch: 030 ----
mean loss: 320.74
 ---- batch: 040 ----
mean loss: 315.90
 ---- batch: 050 ----
mean loss: 326.02
 ---- batch: 060 ----
mean loss: 313.25
 ---- batch: 070 ----
mean loss: 323.11
 ---- batch: 080 ----
mean loss: 328.11
 ---- batch: 090 ----
mean loss: 319.82
 ---- batch: 100 ----
mean loss: 324.97
 ---- batch: 110 ----
mean loss: 318.52
train mean loss: 321.79
epoch train time: 0:00:02.183011
elapsed time: 0:07:58.087790
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 00:24:54.492467
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.23
 ---- batch: 020 ----
mean loss: 317.12
 ---- batch: 030 ----
mean loss: 320.70
 ---- batch: 040 ----
mean loss: 325.47
 ---- batch: 050 ----
mean loss: 325.05
 ---- batch: 060 ----
mean loss: 321.36
 ---- batch: 070 ----
mean loss: 320.31
 ---- batch: 080 ----
mean loss: 322.10
 ---- batch: 090 ----
mean loss: 323.43
 ---- batch: 100 ----
mean loss: 310.27
 ---- batch: 110 ----
mean loss: 323.96
train mean loss: 321.14
epoch train time: 0:00:02.179630
elapsed time: 0:08:00.267608
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 00:24:56.672295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 322.10
 ---- batch: 020 ----
mean loss: 322.87
 ---- batch: 030 ----
mean loss: 320.55
 ---- batch: 040 ----
mean loss: 328.18
 ---- batch: 050 ----
mean loss: 315.20
 ---- batch: 060 ----
mean loss: 326.20
 ---- batch: 070 ----
mean loss: 328.49
 ---- batch: 080 ----
mean loss: 325.46
 ---- batch: 090 ----
mean loss: 319.42
 ---- batch: 100 ----
mean loss: 311.31
 ---- batch: 110 ----
mean loss: 328.73
train mean loss: 322.68
epoch train time: 0:00:02.179213
elapsed time: 0:08:02.446996
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 00:24:58.851671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.88
 ---- batch: 020 ----
mean loss: 323.55
 ---- batch: 030 ----
mean loss: 316.39
 ---- batch: 040 ----
mean loss: 329.77
 ---- batch: 050 ----
mean loss: 318.28
 ---- batch: 060 ----
mean loss: 321.86
 ---- batch: 070 ----
mean loss: 313.76
 ---- batch: 080 ----
mean loss: 329.97
 ---- batch: 090 ----
mean loss: 320.10
 ---- batch: 100 ----
mean loss: 336.71
 ---- batch: 110 ----
mean loss: 316.96
train mean loss: 322.17
epoch train time: 0:00:02.170407
elapsed time: 0:08:04.617569
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 00:25:01.022279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.94
 ---- batch: 020 ----
mean loss: 318.74
 ---- batch: 030 ----
mean loss: 311.03
 ---- batch: 040 ----
mean loss: 321.22
 ---- batch: 050 ----
mean loss: 326.42
 ---- batch: 060 ----
mean loss: 332.79
 ---- batch: 070 ----
mean loss: 325.25
 ---- batch: 080 ----
mean loss: 324.26
 ---- batch: 090 ----
mean loss: 315.49
 ---- batch: 100 ----
mean loss: 318.55
 ---- batch: 110 ----
mean loss: 327.15
train mean loss: 321.54
epoch train time: 0:00:02.161609
elapsed time: 0:08:06.779378
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 00:25:03.184064
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.46
 ---- batch: 020 ----
mean loss: 312.16
 ---- batch: 030 ----
mean loss: 339.16
 ---- batch: 040 ----
mean loss: 317.57
 ---- batch: 050 ----
mean loss: 319.51
 ---- batch: 060 ----
mean loss: 321.14
 ---- batch: 070 ----
mean loss: 319.10
 ---- batch: 080 ----
mean loss: 324.05
 ---- batch: 090 ----
mean loss: 326.13
 ---- batch: 100 ----
mean loss: 326.82
 ---- batch: 110 ----
mean loss: 325.09
train mean loss: 322.59
epoch train time: 0:00:02.167740
elapsed time: 0:08:08.947294
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 00:25:05.351968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 318.45
 ---- batch: 020 ----
mean loss: 318.55
 ---- batch: 030 ----
mean loss: 315.80
 ---- batch: 040 ----
mean loss: 323.98
 ---- batch: 050 ----
mean loss: 326.00
 ---- batch: 060 ----
mean loss: 331.57
 ---- batch: 070 ----
mean loss: 326.56
 ---- batch: 080 ----
mean loss: 320.91
 ---- batch: 090 ----
mean loss: 321.77
 ---- batch: 100 ----
mean loss: 327.86
 ---- batch: 110 ----
mean loss: 320.07
train mean loss: 323.79
epoch train time: 0:00:02.163736
elapsed time: 0:08:11.111195
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 00:25:07.515873
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 328.42
 ---- batch: 020 ----
mean loss: 324.38
 ---- batch: 030 ----
mean loss: 327.63
 ---- batch: 040 ----
mean loss: 323.00
 ---- batch: 050 ----
mean loss: 332.13
 ---- batch: 060 ----
mean loss: 322.57
 ---- batch: 070 ----
mean loss: 314.46
 ---- batch: 080 ----
mean loss: 321.15
 ---- batch: 090 ----
mean loss: 331.70
 ---- batch: 100 ----
mean loss: 315.51
 ---- batch: 110 ----
mean loss: 326.80
train mean loss: 324.07
epoch train time: 0:00:02.163019
elapsed time: 0:08:13.274410
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 00:25:09.679100
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.08
 ---- batch: 020 ----
mean loss: 327.91
 ---- batch: 030 ----
mean loss: 314.25
 ---- batch: 040 ----
mean loss: 328.49
 ---- batch: 050 ----
mean loss: 330.68
 ---- batch: 060 ----
mean loss: 319.78
 ---- batch: 070 ----
mean loss: 325.82
 ---- batch: 080 ----
mean loss: 322.73
 ---- batch: 090 ----
mean loss: 318.95
 ---- batch: 100 ----
mean loss: 326.97
 ---- batch: 110 ----
mean loss: 322.17
train mean loss: 322.76
epoch train time: 0:00:02.163635
elapsed time: 0:08:15.438243
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 00:25:11.842916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 318.84
 ---- batch: 020 ----
mean loss: 324.91
 ---- batch: 030 ----
mean loss: 316.72
 ---- batch: 040 ----
mean loss: 322.95
 ---- batch: 050 ----
mean loss: 322.85
 ---- batch: 060 ----
mean loss: 327.78
 ---- batch: 070 ----
mean loss: 322.99
 ---- batch: 080 ----
mean loss: 324.65
 ---- batch: 090 ----
mean loss: 309.73
 ---- batch: 100 ----
mean loss: 330.90
 ---- batch: 110 ----
mean loss: 329.46
train mean loss: 322.84
epoch train time: 0:00:02.163315
elapsed time: 0:08:17.601732
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 00:25:14.006416
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.66
 ---- batch: 020 ----
mean loss: 334.75
 ---- batch: 030 ----
mean loss: 320.48
 ---- batch: 040 ----
mean loss: 309.62
 ---- batch: 050 ----
mean loss: 323.00
 ---- batch: 060 ----
mean loss: 324.02
 ---- batch: 070 ----
mean loss: 332.33
 ---- batch: 080 ----
mean loss: 322.91
 ---- batch: 090 ----
mean loss: 317.94
 ---- batch: 100 ----
mean loss: 321.96
 ---- batch: 110 ----
mean loss: 325.69
train mean loss: 322.69
epoch train time: 0:00:02.167398
elapsed time: 0:08:19.769293
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 00:25:16.173995
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.45
 ---- batch: 020 ----
mean loss: 322.18
 ---- batch: 030 ----
mean loss: 331.37
 ---- batch: 040 ----
mean loss: 323.86
 ---- batch: 050 ----
mean loss: 318.06
 ---- batch: 060 ----
mean loss: 314.12
 ---- batch: 070 ----
mean loss: 321.62
 ---- batch: 080 ----
mean loss: 320.21
 ---- batch: 090 ----
mean loss: 310.13
 ---- batch: 100 ----
mean loss: 315.58
 ---- batch: 110 ----
mean loss: 319.13
train mean loss: 319.79
epoch train time: 0:00:02.162180
elapsed time: 0:08:21.931662
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 00:25:18.336338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 322.94
 ---- batch: 020 ----
mean loss: 322.02
 ---- batch: 030 ----
mean loss: 322.99
 ---- batch: 040 ----
mean loss: 321.10
 ---- batch: 050 ----
mean loss: 321.53
 ---- batch: 060 ----
mean loss: 314.34
 ---- batch: 070 ----
mean loss: 315.71
 ---- batch: 080 ----
mean loss: 325.22
 ---- batch: 090 ----
mean loss: 321.69
 ---- batch: 100 ----
mean loss: 324.52
 ---- batch: 110 ----
mean loss: 326.58
train mean loss: 321.47
epoch train time: 0:00:02.163789
elapsed time: 0:08:24.095617
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 00:25:20.500291
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 310.74
 ---- batch: 020 ----
mean loss: 318.30
 ---- batch: 030 ----
mean loss: 333.00
 ---- batch: 040 ----
mean loss: 321.39
 ---- batch: 050 ----
mean loss: 320.60
 ---- batch: 060 ----
mean loss: 323.77
 ---- batch: 070 ----
mean loss: 325.61
 ---- batch: 080 ----
mean loss: 315.14
 ---- batch: 090 ----
mean loss: 317.30
 ---- batch: 100 ----
mean loss: 315.09
 ---- batch: 110 ----
mean loss: 328.66
train mean loss: 320.73
epoch train time: 0:00:02.170435
elapsed time: 0:08:26.266219
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 00:25:22.670895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.47
 ---- batch: 020 ----
mean loss: 330.32
 ---- batch: 030 ----
mean loss: 322.64
 ---- batch: 040 ----
mean loss: 324.94
 ---- batch: 050 ----
mean loss: 319.91
 ---- batch: 060 ----
mean loss: 335.58
 ---- batch: 070 ----
mean loss: 318.46
 ---- batch: 080 ----
mean loss: 312.28
 ---- batch: 090 ----
mean loss: 311.18
 ---- batch: 100 ----
mean loss: 316.02
 ---- batch: 110 ----
mean loss: 319.13
train mean loss: 320.27
epoch train time: 0:00:02.166744
elapsed time: 0:08:28.433133
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 00:25:24.837810
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 320.68
 ---- batch: 020 ----
mean loss: 313.43
 ---- batch: 030 ----
mean loss: 311.86
 ---- batch: 040 ----
mean loss: 324.10
 ---- batch: 050 ----
mean loss: 325.59
 ---- batch: 060 ----
mean loss: 319.12
 ---- batch: 070 ----
mean loss: 311.36
 ---- batch: 080 ----
mean loss: 341.58
 ---- batch: 090 ----
mean loss: 320.77
 ---- batch: 100 ----
mean loss: 310.75
 ---- batch: 110 ----
mean loss: 308.61
train mean loss: 318.88
epoch train time: 0:00:02.159442
elapsed time: 0:08:30.592735
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 00:25:26.997419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 319.88
 ---- batch: 020 ----
mean loss: 321.05
 ---- batch: 030 ----
mean loss: 324.32
 ---- batch: 040 ----
mean loss: 330.88
 ---- batch: 050 ----
mean loss: 333.88
 ---- batch: 060 ----
mean loss: 311.91
 ---- batch: 070 ----
mean loss: 318.64
 ---- batch: 080 ----
mean loss: 320.87
 ---- batch: 090 ----
mean loss: 330.21
 ---- batch: 100 ----
mean loss: 319.94
 ---- batch: 110 ----
mean loss: 320.88
train mean loss: 322.50
epoch train time: 0:00:02.162118
elapsed time: 0:08:32.755014
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 00:25:29.159708
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 312.08
 ---- batch: 020 ----
mean loss: 317.09
 ---- batch: 030 ----
mean loss: 331.31
 ---- batch: 040 ----
mean loss: 325.49
 ---- batch: 050 ----
mean loss: 317.08
 ---- batch: 060 ----
mean loss: 311.42
 ---- batch: 070 ----
mean loss: 320.31
 ---- batch: 080 ----
mean loss: 323.00
 ---- batch: 090 ----
mean loss: 312.51
 ---- batch: 100 ----
mean loss: 323.65
 ---- batch: 110 ----
mean loss: 319.42
train mean loss: 319.33
epoch train time: 0:00:02.166280
elapsed time: 0:08:34.921466
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 00:25:31.326138
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 322.14
 ---- batch: 020 ----
mean loss: 321.59
 ---- batch: 030 ----
mean loss: 327.69
 ---- batch: 040 ----
mean loss: 316.78
 ---- batch: 050 ----
mean loss: 331.75
 ---- batch: 060 ----
mean loss: 323.26
 ---- batch: 070 ----
mean loss: 313.26
 ---- batch: 080 ----
mean loss: 315.31
 ---- batch: 090 ----
mean loss: 316.07
 ---- batch: 100 ----
mean loss: 325.42
 ---- batch: 110 ----
mean loss: 313.14
train mean loss: 320.40
epoch train time: 0:00:02.159600
elapsed time: 0:08:37.081237
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 00:25:33.485932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.42
 ---- batch: 020 ----
mean loss: 316.66
 ---- batch: 030 ----
mean loss: 314.36
 ---- batch: 040 ----
mean loss: 310.59
 ---- batch: 050 ----
mean loss: 327.83
 ---- batch: 060 ----
mean loss: 320.62
 ---- batch: 070 ----
mean loss: 324.95
 ---- batch: 080 ----
mean loss: 316.85
 ---- batch: 090 ----
mean loss: 316.59
 ---- batch: 100 ----
mean loss: 333.38
 ---- batch: 110 ----
mean loss: 310.39
train mean loss: 319.54
epoch train time: 0:00:02.171397
elapsed time: 0:08:39.252810
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 00:25:35.657486
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 319.59
 ---- batch: 020 ----
mean loss: 317.87
 ---- batch: 030 ----
mean loss: 320.28
 ---- batch: 040 ----
mean loss: 321.46
 ---- batch: 050 ----
mean loss: 325.19
 ---- batch: 060 ----
mean loss: 326.55
 ---- batch: 070 ----
mean loss: 317.19
 ---- batch: 080 ----
mean loss: 319.28
 ---- batch: 090 ----
mean loss: 323.58
 ---- batch: 100 ----
mean loss: 316.52
 ---- batch: 110 ----
mean loss: 319.09
train mean loss: 320.62
epoch train time: 0:00:02.179267
elapsed time: 0:08:41.432254
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 00:25:37.836930
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.24
 ---- batch: 020 ----
mean loss: 312.43
 ---- batch: 030 ----
mean loss: 330.23
 ---- batch: 040 ----
mean loss: 327.01
 ---- batch: 050 ----
mean loss: 317.46
 ---- batch: 060 ----
mean loss: 326.29
 ---- batch: 070 ----
mean loss: 323.79
 ---- batch: 080 ----
mean loss: 308.37
 ---- batch: 090 ----
mean loss: 326.75
 ---- batch: 100 ----
mean loss: 328.56
 ---- batch: 110 ----
mean loss: 313.64
train mean loss: 322.24
epoch train time: 0:00:02.180950
elapsed time: 0:08:43.613398
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 00:25:40.018074
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.97
 ---- batch: 020 ----
mean loss: 328.87
 ---- batch: 030 ----
mean loss: 318.75
 ---- batch: 040 ----
mean loss: 312.19
 ---- batch: 050 ----
mean loss: 318.32
 ---- batch: 060 ----
mean loss: 306.13
 ---- batch: 070 ----
mean loss: 323.96
 ---- batch: 080 ----
mean loss: 322.61
 ---- batch: 090 ----
mean loss: 324.45
 ---- batch: 100 ----
mean loss: 318.19
 ---- batch: 110 ----
mean loss: 323.27
train mean loss: 318.97
epoch train time: 0:00:02.176629
elapsed time: 0:08:45.790199
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 00:25:42.194896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.36
 ---- batch: 020 ----
mean loss: 318.13
 ---- batch: 030 ----
mean loss: 320.75
 ---- batch: 040 ----
mean loss: 306.59
 ---- batch: 050 ----
mean loss: 315.35
 ---- batch: 060 ----
mean loss: 313.22
 ---- batch: 070 ----
mean loss: 329.26
 ---- batch: 080 ----
mean loss: 336.61
 ---- batch: 090 ----
mean loss: 331.68
 ---- batch: 100 ----
mean loss: 316.91
 ---- batch: 110 ----
mean loss: 320.14
train mean loss: 319.38
epoch train time: 0:00:02.179080
elapsed time: 0:08:47.969473
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 00:25:44.374174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 322.83
 ---- batch: 020 ----
mean loss: 312.31
 ---- batch: 030 ----
mean loss: 310.86
 ---- batch: 040 ----
mean loss: 323.24
 ---- batch: 050 ----
mean loss: 316.38
 ---- batch: 060 ----
mean loss: 323.49
 ---- batch: 070 ----
mean loss: 329.80
 ---- batch: 080 ----
mean loss: 321.74
 ---- batch: 090 ----
mean loss: 314.17
 ---- batch: 100 ----
mean loss: 318.02
 ---- batch: 110 ----
mean loss: 321.35
train mean loss: 319.57
epoch train time: 0:00:02.184396
elapsed time: 0:08:50.154053
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 00:25:46.558727
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.66
 ---- batch: 020 ----
mean loss: 325.95
 ---- batch: 030 ----
mean loss: 325.62
 ---- batch: 040 ----
mean loss: 312.03
 ---- batch: 050 ----
mean loss: 322.30
 ---- batch: 060 ----
mean loss: 321.27
 ---- batch: 070 ----
mean loss: 317.52
 ---- batch: 080 ----
mean loss: 313.16
 ---- batch: 090 ----
mean loss: 318.88
 ---- batch: 100 ----
mean loss: 320.23
 ---- batch: 110 ----
mean loss: 316.50
train mean loss: 319.74
epoch train time: 0:00:02.184211
elapsed time: 0:08:52.338448
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 00:25:48.743126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 324.52
 ---- batch: 020 ----
mean loss: 316.90
 ---- batch: 030 ----
mean loss: 324.29
 ---- batch: 040 ----
mean loss: 335.02
 ---- batch: 050 ----
mean loss: 316.15
 ---- batch: 060 ----
mean loss: 320.73
 ---- batch: 070 ----
mean loss: 315.24
 ---- batch: 080 ----
mean loss: 314.14
 ---- batch: 090 ----
mean loss: 310.28
 ---- batch: 100 ----
mean loss: 312.06
 ---- batch: 110 ----
mean loss: 321.76
train mean loss: 320.12
epoch train time: 0:00:02.178192
elapsed time: 0:08:54.516800
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 00:25:50.921474
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.58
 ---- batch: 020 ----
mean loss: 311.98
 ---- batch: 030 ----
mean loss: 310.59
 ---- batch: 040 ----
mean loss: 317.29
 ---- batch: 050 ----
mean loss: 327.20
 ---- batch: 060 ----
mean loss: 314.43
 ---- batch: 070 ----
mean loss: 321.23
 ---- batch: 080 ----
mean loss: 324.79
 ---- batch: 090 ----
mean loss: 319.86
 ---- batch: 100 ----
mean loss: 325.77
 ---- batch: 110 ----
mean loss: 318.50
train mean loss: 319.05
epoch train time: 0:00:02.178366
elapsed time: 0:08:56.695335
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 00:25:53.100011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 322.38
 ---- batch: 020 ----
mean loss: 308.82
 ---- batch: 030 ----
mean loss: 317.99
 ---- batch: 040 ----
mean loss: 332.67
 ---- batch: 050 ----
mean loss: 306.19
 ---- batch: 060 ----
mean loss: 319.22
 ---- batch: 070 ----
mean loss: 318.48
 ---- batch: 080 ----
mean loss: 319.22
 ---- batch: 090 ----
mean loss: 318.19
 ---- batch: 100 ----
mean loss: 312.92
 ---- batch: 110 ----
mean loss: 316.22
train mean loss: 317.09
epoch train time: 0:00:02.183154
elapsed time: 0:08:58.878668
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 00:25:55.283347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 320.73
 ---- batch: 020 ----
mean loss: 313.05
 ---- batch: 030 ----
mean loss: 312.63
 ---- batch: 040 ----
mean loss: 318.24
 ---- batch: 050 ----
mean loss: 316.11
 ---- batch: 060 ----
mean loss: 326.45
 ---- batch: 070 ----
mean loss: 331.25
 ---- batch: 080 ----
mean loss: 321.23
 ---- batch: 090 ----
mean loss: 311.00
 ---- batch: 100 ----
mean loss: 329.49
 ---- batch: 110 ----
mean loss: 320.60
train mean loss: 319.75
epoch train time: 0:00:02.183499
elapsed time: 0:09:01.062362
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 00:25:57.467046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.93
 ---- batch: 020 ----
mean loss: 310.64
 ---- batch: 030 ----
mean loss: 307.26
 ---- batch: 040 ----
mean loss: 319.15
 ---- batch: 050 ----
mean loss: 321.67
 ---- batch: 060 ----
mean loss: 318.52
 ---- batch: 070 ----
mean loss: 320.45
 ---- batch: 080 ----
mean loss: 321.68
 ---- batch: 090 ----
mean loss: 330.35
 ---- batch: 100 ----
mean loss: 322.18
 ---- batch: 110 ----
mean loss: 318.36
train mean loss: 319.56
epoch train time: 0:00:02.178296
elapsed time: 0:09:03.240848
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 00:25:59.645527
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.63
 ---- batch: 020 ----
mean loss: 309.87
 ---- batch: 030 ----
mean loss: 323.33
 ---- batch: 040 ----
mean loss: 326.54
 ---- batch: 050 ----
mean loss: 314.42
 ---- batch: 060 ----
mean loss: 323.16
 ---- batch: 070 ----
mean loss: 329.11
 ---- batch: 080 ----
mean loss: 303.05
 ---- batch: 090 ----
mean loss: 318.17
 ---- batch: 100 ----
mean loss: 323.24
 ---- batch: 110 ----
mean loss: 313.82
train mean loss: 318.71
epoch train time: 0:00:02.181497
elapsed time: 0:09:05.422519
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 00:26:01.827193
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 318.09
 ---- batch: 020 ----
mean loss: 323.80
 ---- batch: 030 ----
mean loss: 313.92
 ---- batch: 040 ----
mean loss: 322.79
 ---- batch: 050 ----
mean loss: 318.50
 ---- batch: 060 ----
mean loss: 325.34
 ---- batch: 070 ----
mean loss: 325.29
 ---- batch: 080 ----
mean loss: 308.02
 ---- batch: 090 ----
mean loss: 314.09
 ---- batch: 100 ----
mean loss: 313.20
 ---- batch: 110 ----
mean loss: 315.82
train mean loss: 317.94
epoch train time: 0:00:02.183024
elapsed time: 0:09:07.605727
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 00:26:04.010404
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.12
 ---- batch: 020 ----
mean loss: 330.96
 ---- batch: 030 ----
mean loss: 318.68
 ---- batch: 040 ----
mean loss: 312.65
 ---- batch: 050 ----
mean loss: 322.44
 ---- batch: 060 ----
mean loss: 323.45
 ---- batch: 070 ----
mean loss: 306.11
 ---- batch: 080 ----
mean loss: 317.05
 ---- batch: 090 ----
mean loss: 321.85
 ---- batch: 100 ----
mean loss: 319.57
 ---- batch: 110 ----
mean loss: 311.97
train mean loss: 318.93
epoch train time: 0:00:02.185514
elapsed time: 0:09:09.791422
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 00:26:06.196097
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.20
 ---- batch: 020 ----
mean loss: 316.38
 ---- batch: 030 ----
mean loss: 319.03
 ---- batch: 040 ----
mean loss: 315.12
 ---- batch: 050 ----
mean loss: 311.25
 ---- batch: 060 ----
mean loss: 311.48
 ---- batch: 070 ----
mean loss: 340.84
 ---- batch: 080 ----
mean loss: 315.42
 ---- batch: 090 ----
mean loss: 310.74
 ---- batch: 100 ----
mean loss: 320.52
 ---- batch: 110 ----
mean loss: 314.94
train mean loss: 317.25
epoch train time: 0:00:02.180314
elapsed time: 0:09:11.971907
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 00:26:08.376608
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.25
 ---- batch: 020 ----
mean loss: 313.53
 ---- batch: 030 ----
mean loss: 322.36
 ---- batch: 040 ----
mean loss: 315.66
 ---- batch: 050 ----
mean loss: 312.10
 ---- batch: 060 ----
mean loss: 327.88
 ---- batch: 070 ----
mean loss: 315.97
 ---- batch: 080 ----
mean loss: 318.40
 ---- batch: 090 ----
mean loss: 313.87
 ---- batch: 100 ----
mean loss: 321.25
 ---- batch: 110 ----
mean loss: 312.25
train mean loss: 317.37
epoch train time: 0:00:02.179292
elapsed time: 0:09:14.151403
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 00:26:10.556124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 319.87
 ---- batch: 020 ----
mean loss: 314.13
 ---- batch: 030 ----
mean loss: 312.41
 ---- batch: 040 ----
mean loss: 315.75
 ---- batch: 050 ----
mean loss: 314.11
 ---- batch: 060 ----
mean loss: 319.18
 ---- batch: 070 ----
mean loss: 323.45
 ---- batch: 080 ----
mean loss: 322.10
 ---- batch: 090 ----
mean loss: 329.97
 ---- batch: 100 ----
mean loss: 308.59
 ---- batch: 110 ----
mean loss: 320.38
train mean loss: 318.38
epoch train time: 0:00:02.187060
elapsed time: 0:09:16.338687
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 00:26:12.743381
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 314.50
 ---- batch: 020 ----
mean loss: 312.20
 ---- batch: 030 ----
mean loss: 315.93
 ---- batch: 040 ----
mean loss: 302.97
 ---- batch: 050 ----
mean loss: 324.33
 ---- batch: 060 ----
mean loss: 322.09
 ---- batch: 070 ----
mean loss: 316.92
 ---- batch: 080 ----
mean loss: 319.04
 ---- batch: 090 ----
mean loss: 318.06
 ---- batch: 100 ----
mean loss: 330.35
 ---- batch: 110 ----
mean loss: 314.16
train mean loss: 317.16
epoch train time: 0:00:02.179697
elapsed time: 0:09:18.518578
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 00:26:14.923253
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.48
 ---- batch: 020 ----
mean loss: 322.84
 ---- batch: 030 ----
mean loss: 338.85
 ---- batch: 040 ----
mean loss: 306.84
 ---- batch: 050 ----
mean loss: 307.49
 ---- batch: 060 ----
mean loss: 314.43
 ---- batch: 070 ----
mean loss: 314.32
 ---- batch: 080 ----
mean loss: 297.66
 ---- batch: 090 ----
mean loss: 316.84
 ---- batch: 100 ----
mean loss: 312.97
 ---- batch: 110 ----
mean loss: 317.10
train mean loss: 314.98
epoch train time: 0:00:02.181512
elapsed time: 0:09:20.700256
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 00:26:17.104931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.24
 ---- batch: 020 ----
mean loss: 327.95
 ---- batch: 030 ----
mean loss: 314.68
 ---- batch: 040 ----
mean loss: 326.69
 ---- batch: 050 ----
mean loss: 316.23
 ---- batch: 060 ----
mean loss: 323.18
 ---- batch: 070 ----
mean loss: 318.56
 ---- batch: 080 ----
mean loss: 313.41
 ---- batch: 090 ----
mean loss: 306.77
 ---- batch: 100 ----
mean loss: 312.48
 ---- batch: 110 ----
mean loss: 314.00
train mean loss: 317.73
epoch train time: 0:00:02.179411
elapsed time: 0:09:22.879851
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 00:26:19.284528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 312.52
 ---- batch: 020 ----
mean loss: 319.43
 ---- batch: 030 ----
mean loss: 320.55
 ---- batch: 040 ----
mean loss: 316.77
 ---- batch: 050 ----
mean loss: 327.72
 ---- batch: 060 ----
mean loss: 312.95
 ---- batch: 070 ----
mean loss: 324.64
 ---- batch: 080 ----
mean loss: 324.59
 ---- batch: 090 ----
mean loss: 311.72
 ---- batch: 100 ----
mean loss: 315.08
 ---- batch: 110 ----
mean loss: 322.71
train mean loss: 318.98
epoch train time: 0:00:02.180983
elapsed time: 0:09:25.061046
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 00:26:21.465741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 322.07
 ---- batch: 020 ----
mean loss: 318.59
 ---- batch: 030 ----
mean loss: 318.32
 ---- batch: 040 ----
mean loss: 318.67
 ---- batch: 050 ----
mean loss: 309.10
 ---- batch: 060 ----
mean loss: 315.88
 ---- batch: 070 ----
mean loss: 321.88
 ---- batch: 080 ----
mean loss: 320.02
 ---- batch: 090 ----
mean loss: 318.22
 ---- batch: 100 ----
mean loss: 317.18
 ---- batch: 110 ----
mean loss: 324.80
train mean loss: 318.49
epoch train time: 0:00:02.168604
elapsed time: 0:09:27.229864
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 00:26:23.634539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 324.31
 ---- batch: 020 ----
mean loss: 316.49
 ---- batch: 030 ----
mean loss: 320.21
 ---- batch: 040 ----
mean loss: 321.02
 ---- batch: 050 ----
mean loss: 312.24
 ---- batch: 060 ----
mean loss: 315.12
 ---- batch: 070 ----
mean loss: 314.85
 ---- batch: 080 ----
mean loss: 315.22
 ---- batch: 090 ----
mean loss: 323.86
 ---- batch: 100 ----
mean loss: 316.01
 ---- batch: 110 ----
mean loss: 321.68
train mean loss: 318.52
epoch train time: 0:00:02.175382
elapsed time: 0:09:29.405416
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 00:26:25.810129
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 310.61
 ---- batch: 020 ----
mean loss: 317.85
 ---- batch: 030 ----
mean loss: 326.66
 ---- batch: 040 ----
mean loss: 317.44
 ---- batch: 050 ----
mean loss: 315.76
 ---- batch: 060 ----
mean loss: 315.71
 ---- batch: 070 ----
mean loss: 328.43
 ---- batch: 080 ----
mean loss: 309.69
 ---- batch: 090 ----
mean loss: 312.32
 ---- batch: 100 ----
mean loss: 320.03
 ---- batch: 110 ----
mean loss: 314.60
train mean loss: 316.59
epoch train time: 0:00:02.168795
elapsed time: 0:09:31.574414
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 00:26:27.979088
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 312.50
 ---- batch: 020 ----
mean loss: 313.45
 ---- batch: 030 ----
mean loss: 309.72
 ---- batch: 040 ----
mean loss: 317.64
 ---- batch: 050 ----
mean loss: 317.73
 ---- batch: 060 ----
mean loss: 309.29
 ---- batch: 070 ----
mean loss: 313.92
 ---- batch: 080 ----
mean loss: 319.65
 ---- batch: 090 ----
mean loss: 321.82
 ---- batch: 100 ----
mean loss: 320.81
 ---- batch: 110 ----
mean loss: 316.66
train mean loss: 315.61
epoch train time: 0:00:02.174297
elapsed time: 0:09:33.748885
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 00:26:30.153563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.09
 ---- batch: 020 ----
mean loss: 312.29
 ---- batch: 030 ----
mean loss: 306.89
 ---- batch: 040 ----
mean loss: 316.73
 ---- batch: 050 ----
mean loss: 311.59
 ---- batch: 060 ----
mean loss: 322.32
 ---- batch: 070 ----
mean loss: 308.48
 ---- batch: 080 ----
mean loss: 324.52
 ---- batch: 090 ----
mean loss: 318.11
 ---- batch: 100 ----
mean loss: 328.50
 ---- batch: 110 ----
mean loss: 320.37
train mean loss: 316.03
epoch train time: 0:00:02.167743
elapsed time: 0:09:35.920237
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_3/checkpoint.pth.tar
**** end time: 2019-09-27 00:26:32.324873 ****
