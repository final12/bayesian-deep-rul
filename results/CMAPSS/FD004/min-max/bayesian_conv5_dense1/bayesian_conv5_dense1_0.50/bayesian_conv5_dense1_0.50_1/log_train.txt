Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 4439
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 11:06:10.600395 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 11:06:10.617788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1594.94
 ---- batch: 020 ----
mean loss: 1334.61
 ---- batch: 030 ----
mean loss: 1158.70
 ---- batch: 040 ----
mean loss: 1101.76
 ---- batch: 050 ----
mean loss: 1069.84
train mean loss: 1223.26
epoch train time: 0:00:22.929425
elapsed time: 0:00:22.955297
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 11:06:33.555731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1058.36
 ---- batch: 020 ----
mean loss: 1013.63
 ---- batch: 030 ----
mean loss: 964.08
 ---- batch: 040 ----
mean loss: 992.12
 ---- batch: 050 ----
mean loss: 973.19
train mean loss: 997.60
epoch train time: 0:00:07.922449
elapsed time: 0:00:30.878537
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 11:06:41.479293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.70
 ---- batch: 020 ----
mean loss: 959.00
 ---- batch: 030 ----
mean loss: 952.49
 ---- batch: 040 ----
mean loss: 950.51
 ---- batch: 050 ----
mean loss: 916.51
train mean loss: 944.64
epoch train time: 0:00:07.914897
elapsed time: 0:00:38.794612
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 11:06:49.395364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.97
 ---- batch: 020 ----
mean loss: 924.15
 ---- batch: 030 ----
mean loss: 926.85
 ---- batch: 040 ----
mean loss: 932.85
 ---- batch: 050 ----
mean loss: 928.77
train mean loss: 930.89
epoch train time: 0:00:07.928525
elapsed time: 0:00:46.724273
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 11:06:57.325020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.61
 ---- batch: 020 ----
mean loss: 948.26
 ---- batch: 030 ----
mean loss: 885.98
 ---- batch: 040 ----
mean loss: 908.45
 ---- batch: 050 ----
mean loss: 913.39
train mean loss: 921.28
epoch train time: 0:00:07.930171
elapsed time: 0:00:54.655553
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 11:07:05.256315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.91
 ---- batch: 020 ----
mean loss: 912.15
 ---- batch: 030 ----
mean loss: 901.02
 ---- batch: 040 ----
mean loss: 912.98
 ---- batch: 050 ----
mean loss: 899.60
train mean loss: 913.05
epoch train time: 0:00:07.918175
elapsed time: 0:01:02.574957
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 11:07:13.175744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.40
 ---- batch: 020 ----
mean loss: 911.45
 ---- batch: 030 ----
mean loss: 894.64
 ---- batch: 040 ----
mean loss: 935.30
 ---- batch: 050 ----
mean loss: 882.29
train mean loss: 910.83
epoch train time: 0:00:07.917705
elapsed time: 0:01:10.493923
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 11:07:21.094685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.96
 ---- batch: 020 ----
mean loss: 917.72
 ---- batch: 030 ----
mean loss: 919.57
 ---- batch: 040 ----
mean loss: 890.81
 ---- batch: 050 ----
mean loss: 908.81
train mean loss: 905.18
epoch train time: 0:00:07.913529
elapsed time: 0:01:18.408674
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 11:07:29.009529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.54
 ---- batch: 020 ----
mean loss: 870.58
 ---- batch: 030 ----
mean loss: 900.41
 ---- batch: 040 ----
mean loss: 909.43
 ---- batch: 050 ----
mean loss: 903.40
train mean loss: 900.92
epoch train time: 0:00:07.921657
elapsed time: 0:01:26.331624
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 11:07:36.932410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.45
 ---- batch: 020 ----
mean loss: 878.11
 ---- batch: 030 ----
mean loss: 899.86
 ---- batch: 040 ----
mean loss: 910.49
 ---- batch: 050 ----
mean loss: 908.32
train mean loss: 901.57
epoch train time: 0:00:07.896641
elapsed time: 0:01:34.229532
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 11:07:44.830343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.83
 ---- batch: 020 ----
mean loss: 900.10
 ---- batch: 030 ----
mean loss: 907.68
 ---- batch: 040 ----
mean loss: 878.51
 ---- batch: 050 ----
mean loss: 899.61
train mean loss: 898.14
epoch train time: 0:00:07.972532
elapsed time: 0:01:42.203380
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 11:07:52.804149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.10
 ---- batch: 020 ----
mean loss: 891.63
 ---- batch: 030 ----
mean loss: 876.23
 ---- batch: 040 ----
mean loss: 891.71
 ---- batch: 050 ----
mean loss: 909.26
train mean loss: 894.36
epoch train time: 0:00:07.913610
elapsed time: 0:01:50.118163
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 11:08:00.718825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.41
 ---- batch: 020 ----
mean loss: 890.18
 ---- batch: 030 ----
mean loss: 917.43
 ---- batch: 040 ----
mean loss: 894.71
 ---- batch: 050 ----
mean loss: 876.51
train mean loss: 892.03
epoch train time: 0:00:07.906061
elapsed time: 0:01:58.025347
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 11:08:08.626150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.40
 ---- batch: 020 ----
mean loss: 872.62
 ---- batch: 030 ----
mean loss: 887.08
 ---- batch: 040 ----
mean loss: 901.12
 ---- batch: 050 ----
mean loss: 874.80
train mean loss: 888.75
epoch train time: 0:00:07.915289
elapsed time: 0:02:05.941840
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 11:08:16.542628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.85
 ---- batch: 020 ----
mean loss: 880.92
 ---- batch: 030 ----
mean loss: 902.78
 ---- batch: 040 ----
mean loss: 872.70
 ---- batch: 050 ----
mean loss: 891.22
train mean loss: 889.21
epoch train time: 0:00:07.905900
elapsed time: 0:02:13.849033
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 11:08:24.449863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.54
 ---- batch: 020 ----
mean loss: 870.68
 ---- batch: 030 ----
mean loss: 890.79
 ---- batch: 040 ----
mean loss: 898.91
 ---- batch: 050 ----
mean loss: 891.80
train mean loss: 889.82
epoch train time: 0:00:07.916807
elapsed time: 0:02:21.767057
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 11:08:32.367834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 920.78
 ---- batch: 020 ----
mean loss: 871.86
 ---- batch: 030 ----
mean loss: 885.39
 ---- batch: 040 ----
mean loss: 889.58
 ---- batch: 050 ----
mean loss: 873.07
train mean loss: 888.65
epoch train time: 0:00:07.910831
elapsed time: 0:02:29.679118
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 11:08:40.279899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.66
 ---- batch: 020 ----
mean loss: 878.53
 ---- batch: 030 ----
mean loss: 875.47
 ---- batch: 040 ----
mean loss: 866.24
 ---- batch: 050 ----
mean loss: 877.54
train mean loss: 881.81
epoch train time: 0:00:07.895468
elapsed time: 0:02:37.575792
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 11:08:48.176563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.39
 ---- batch: 020 ----
mean loss: 878.00
 ---- batch: 030 ----
mean loss: 884.71
 ---- batch: 040 ----
mean loss: 898.59
 ---- batch: 050 ----
mean loss: 865.85
train mean loss: 881.34
epoch train time: 0:00:07.881448
elapsed time: 0:02:45.458625
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 11:08:56.059442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.07
 ---- batch: 020 ----
mean loss: 896.82
 ---- batch: 030 ----
mean loss: 882.12
 ---- batch: 040 ----
mean loss: 858.75
 ---- batch: 050 ----
mean loss: 888.10
train mean loss: 883.14
epoch train time: 0:00:07.975642
elapsed time: 0:02:53.435500
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 11:09:04.036264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.32
 ---- batch: 020 ----
mean loss: 906.20
 ---- batch: 030 ----
mean loss: 865.31
 ---- batch: 040 ----
mean loss: 904.55
 ---- batch: 050 ----
mean loss: 870.05
train mean loss: 882.58
epoch train time: 0:00:07.896467
elapsed time: 0:03:01.333248
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 11:09:11.934008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.86
 ---- batch: 020 ----
mean loss: 882.81
 ---- batch: 030 ----
mean loss: 877.09
 ---- batch: 040 ----
mean loss: 866.51
 ---- batch: 050 ----
mean loss: 883.28
train mean loss: 876.30
epoch train time: 0:00:07.912494
elapsed time: 0:03:09.246934
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 11:09:19.847549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.31
 ---- batch: 020 ----
mean loss: 885.69
 ---- batch: 030 ----
mean loss: 863.69
 ---- batch: 040 ----
mean loss: 886.21
 ---- batch: 050 ----
mean loss: 871.39
train mean loss: 878.19
epoch train time: 0:00:07.908434
elapsed time: 0:03:17.156394
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 11:09:27.757184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.33
 ---- batch: 020 ----
mean loss: 864.93
 ---- batch: 030 ----
mean loss: 895.93
 ---- batch: 040 ----
mean loss: 857.36
 ---- batch: 050 ----
mean loss: 875.98
train mean loss: 874.97
epoch train time: 0:00:07.939868
elapsed time: 0:03:25.097542
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 11:09:35.698305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.75
 ---- batch: 020 ----
mean loss: 872.98
 ---- batch: 030 ----
mean loss: 870.63
 ---- batch: 040 ----
mean loss: 879.87
 ---- batch: 050 ----
mean loss: 870.29
train mean loss: 872.60
epoch train time: 0:00:07.934878
elapsed time: 0:03:33.033641
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 11:09:43.634454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.17
 ---- batch: 020 ----
mean loss: 857.58
 ---- batch: 030 ----
mean loss: 876.34
 ---- batch: 040 ----
mean loss: 878.28
 ---- batch: 050 ----
mean loss: 876.69
train mean loss: 870.02
epoch train time: 0:00:07.955391
elapsed time: 0:03:40.990214
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 11:09:51.590965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.90
 ---- batch: 020 ----
mean loss: 871.90
 ---- batch: 030 ----
mean loss: 852.31
 ---- batch: 040 ----
mean loss: 865.04
 ---- batch: 050 ----
mean loss: 887.25
train mean loss: 867.51
epoch train time: 0:00:07.930612
elapsed time: 0:03:48.922017
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 11:09:59.522804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.74
 ---- batch: 020 ----
mean loss: 859.10
 ---- batch: 030 ----
mean loss: 852.07
 ---- batch: 040 ----
mean loss: 857.40
 ---- batch: 050 ----
mean loss: 890.70
train mean loss: 866.48
epoch train time: 0:00:07.925307
elapsed time: 0:03:56.848565
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 11:10:07.449372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.19
 ---- batch: 020 ----
mean loss: 876.18
 ---- batch: 030 ----
mean loss: 860.06
 ---- batch: 040 ----
mean loss: 866.17
 ---- batch: 050 ----
mean loss: 843.28
train mean loss: 859.07
epoch train time: 0:00:07.907093
elapsed time: 0:04:04.756905
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 11:10:15.357683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.95
 ---- batch: 020 ----
mean loss: 871.05
 ---- batch: 030 ----
mean loss: 861.10
 ---- batch: 040 ----
mean loss: 835.61
 ---- batch: 050 ----
mean loss: 840.89
train mean loss: 855.96
epoch train time: 0:00:07.906946
elapsed time: 0:04:12.665000
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 11:10:23.265814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.05
 ---- batch: 020 ----
mean loss: 858.15
 ---- batch: 030 ----
mean loss: 859.44
 ---- batch: 040 ----
mean loss: 838.39
 ---- batch: 050 ----
mean loss: 844.89
train mean loss: 853.99
epoch train time: 0:00:07.918680
elapsed time: 0:04:20.584946
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 11:10:31.185736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.79
 ---- batch: 020 ----
mean loss: 849.17
 ---- batch: 030 ----
mean loss: 861.22
 ---- batch: 040 ----
mean loss: 852.64
 ---- batch: 050 ----
mean loss: 813.48
train mean loss: 847.41
epoch train time: 0:00:07.938147
elapsed time: 0:04:28.524326
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 11:10:39.125111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.38
 ---- batch: 020 ----
mean loss: 843.92
 ---- batch: 030 ----
mean loss: 844.48
 ---- batch: 040 ----
mean loss: 834.83
 ---- batch: 050 ----
mean loss: 865.63
train mean loss: 841.03
epoch train time: 0:00:07.934382
elapsed time: 0:04:36.460047
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 11:10:47.060822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.12
 ---- batch: 020 ----
mean loss: 847.42
 ---- batch: 030 ----
mean loss: 834.87
 ---- batch: 040 ----
mean loss: 809.34
 ---- batch: 050 ----
mean loss: 810.25
train mean loss: 823.59
epoch train time: 0:00:07.929987
elapsed time: 0:04:44.391230
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 11:10:54.991986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 819.10
 ---- batch: 020 ----
mean loss: 827.15
 ---- batch: 030 ----
mean loss: 811.78
 ---- batch: 040 ----
mean loss: 794.40
 ---- batch: 050 ----
mean loss: 767.41
train mean loss: 801.47
epoch train time: 0:00:07.982442
elapsed time: 0:04:52.374872
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 11:11:02.975637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 792.16
 ---- batch: 020 ----
mean loss: 745.48
 ---- batch: 030 ----
mean loss: 756.46
 ---- batch: 040 ----
mean loss: 728.03
 ---- batch: 050 ----
mean loss: 743.92
train mean loss: 750.89
epoch train time: 0:00:07.942923
elapsed time: 0:05:00.319024
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 11:11:10.919830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 722.21
 ---- batch: 020 ----
mean loss: 723.39
 ---- batch: 030 ----
mean loss: 723.13
 ---- batch: 040 ----
mean loss: 714.95
 ---- batch: 050 ----
mean loss: 705.37
train mean loss: 715.57
epoch train time: 0:00:07.933987
elapsed time: 0:05:08.254186
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 11:11:18.854876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.20
 ---- batch: 020 ----
mean loss: 688.98
 ---- batch: 030 ----
mean loss: 692.68
 ---- batch: 040 ----
mean loss: 664.30
 ---- batch: 050 ----
mean loss: 676.98
train mean loss: 680.84
epoch train time: 0:00:07.941043
elapsed time: 0:05:16.196437
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 11:11:26.797223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 680.86
 ---- batch: 020 ----
mean loss: 656.42
 ---- batch: 030 ----
mean loss: 646.05
 ---- batch: 040 ----
mean loss: 635.39
 ---- batch: 050 ----
mean loss: 637.41
train mean loss: 649.25
epoch train time: 0:00:07.944487
elapsed time: 0:05:24.142196
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 11:11:34.742982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 637.38
 ---- batch: 020 ----
mean loss: 626.31
 ---- batch: 030 ----
mean loss: 611.39
 ---- batch: 040 ----
mean loss: 630.98
 ---- batch: 050 ----
mean loss: 605.73
train mean loss: 619.62
epoch train time: 0:00:07.926145
elapsed time: 0:05:32.069618
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 11:11:42.670398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 647.40
 ---- batch: 020 ----
mean loss: 607.18
 ---- batch: 030 ----
mean loss: 611.37
 ---- batch: 040 ----
mean loss: 584.09
 ---- batch: 050 ----
mean loss: 579.16
train mean loss: 605.53
epoch train time: 0:00:07.953047
elapsed time: 0:05:40.023953
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 11:11:50.624796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 580.85
 ---- batch: 020 ----
mean loss: 599.95
 ---- batch: 030 ----
mean loss: 583.55
 ---- batch: 040 ----
mean loss: 585.48
 ---- batch: 050 ----
mean loss: 581.30
train mean loss: 581.69
epoch train time: 0:00:07.939360
elapsed time: 0:05:47.964566
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 11:11:58.565326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 582.42
 ---- batch: 020 ----
mean loss: 572.46
 ---- batch: 030 ----
mean loss: 546.81
 ---- batch: 040 ----
mean loss: 572.11
 ---- batch: 050 ----
mean loss: 564.51
train mean loss: 563.22
epoch train time: 0:00:07.939590
elapsed time: 0:05:55.905455
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 11:12:06.506174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 571.53
 ---- batch: 020 ----
mean loss: 545.07
 ---- batch: 030 ----
mean loss: 541.87
 ---- batch: 040 ----
mean loss: 537.78
 ---- batch: 050 ----
mean loss: 547.50
train mean loss: 548.50
epoch train time: 0:00:07.931837
elapsed time: 0:06:03.838540
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 11:12:14.439169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.56
 ---- batch: 020 ----
mean loss: 527.23
 ---- batch: 030 ----
mean loss: 542.65
 ---- batch: 040 ----
mean loss: 535.39
 ---- batch: 050 ----
mean loss: 535.10
train mean loss: 532.88
epoch train time: 0:00:07.952108
elapsed time: 0:06:11.791626
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 11:12:22.392385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.62
 ---- batch: 020 ----
mean loss: 520.46
 ---- batch: 030 ----
mean loss: 525.45
 ---- batch: 040 ----
mean loss: 522.60
 ---- batch: 050 ----
mean loss: 505.93
train mean loss: 519.31
epoch train time: 0:00:07.940830
elapsed time: 0:06:19.733603
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 11:12:30.334362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.45
 ---- batch: 020 ----
mean loss: 506.06
 ---- batch: 030 ----
mean loss: 519.39
 ---- batch: 040 ----
mean loss: 512.82
 ---- batch: 050 ----
mean loss: 487.02
train mean loss: 508.14
epoch train time: 0:00:07.961490
elapsed time: 0:06:27.696327
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 11:12:38.297076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.41
 ---- batch: 020 ----
mean loss: 489.36
 ---- batch: 030 ----
mean loss: 501.80
 ---- batch: 040 ----
mean loss: 495.16
 ---- batch: 050 ----
mean loss: 486.41
train mean loss: 493.08
epoch train time: 0:00:07.946133
elapsed time: 0:06:35.643675
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 11:12:46.244477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.31
 ---- batch: 020 ----
mean loss: 479.99
 ---- batch: 030 ----
mean loss: 474.44
 ---- batch: 040 ----
mean loss: 488.25
 ---- batch: 050 ----
mean loss: 468.63
train mean loss: 481.33
epoch train time: 0:00:07.939001
elapsed time: 0:06:43.583838
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 11:12:54.184623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.50
 ---- batch: 020 ----
mean loss: 480.07
 ---- batch: 030 ----
mean loss: 474.49
 ---- batch: 040 ----
mean loss: 463.19
 ---- batch: 050 ----
mean loss: 475.63
train mean loss: 472.00
epoch train time: 0:00:07.969300
elapsed time: 0:06:51.554388
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 11:13:02.155163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.25
 ---- batch: 020 ----
mean loss: 466.83
 ---- batch: 030 ----
mean loss: 460.65
 ---- batch: 040 ----
mean loss: 459.48
 ---- batch: 050 ----
mean loss: 452.35
train mean loss: 460.18
epoch train time: 0:00:07.929626
elapsed time: 0:06:59.485200
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 11:13:10.085995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.98
 ---- batch: 020 ----
mean loss: 453.96
 ---- batch: 030 ----
mean loss: 453.19
 ---- batch: 040 ----
mean loss: 439.80
 ---- batch: 050 ----
mean loss: 452.99
train mean loss: 451.75
epoch train time: 0:00:07.952345
elapsed time: 0:07:07.438734
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 11:13:18.039483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.61
 ---- batch: 020 ----
mean loss: 448.40
 ---- batch: 030 ----
mean loss: 430.63
 ---- batch: 040 ----
mean loss: 444.84
 ---- batch: 050 ----
mean loss: 447.37
train mean loss: 440.64
epoch train time: 0:00:07.973343
elapsed time: 0:07:15.413234
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 11:13:26.014113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.59
 ---- batch: 020 ----
mean loss: 438.28
 ---- batch: 030 ----
mean loss: 436.99
 ---- batch: 040 ----
mean loss: 430.56
 ---- batch: 050 ----
mean loss: 428.86
train mean loss: 431.59
epoch train time: 0:00:07.953278
elapsed time: 0:07:23.367770
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 11:13:33.968539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.62
 ---- batch: 020 ----
mean loss: 451.42
 ---- batch: 030 ----
mean loss: 435.21
 ---- batch: 040 ----
mean loss: 425.71
 ---- batch: 050 ----
mean loss: 412.65
train mean loss: 429.61
epoch train time: 0:00:07.942274
elapsed time: 0:07:31.311255
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 11:13:41.912014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.28
 ---- batch: 020 ----
mean loss: 426.20
 ---- batch: 030 ----
mean loss: 423.45
 ---- batch: 040 ----
mean loss: 410.18
 ---- batch: 050 ----
mean loss: 417.57
train mean loss: 417.11
epoch train time: 0:00:07.956038
elapsed time: 0:07:39.268448
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 11:13:49.869265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.96
 ---- batch: 020 ----
mean loss: 410.36
 ---- batch: 030 ----
mean loss: 409.25
 ---- batch: 040 ----
mean loss: 406.16
 ---- batch: 050 ----
mean loss: 414.71
train mean loss: 410.38
epoch train time: 0:00:07.953135
elapsed time: 0:07:47.222873
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 11:13:57.823639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.84
 ---- batch: 020 ----
mean loss: 404.50
 ---- batch: 030 ----
mean loss: 402.59
 ---- batch: 040 ----
mean loss: 402.59
 ---- batch: 050 ----
mean loss: 414.01
train mean loss: 404.00
epoch train time: 0:00:07.934660
elapsed time: 0:07:55.158722
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 11:14:05.759542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.41
 ---- batch: 020 ----
mean loss: 392.85
 ---- batch: 030 ----
mean loss: 398.72
 ---- batch: 040 ----
mean loss: 394.82
 ---- batch: 050 ----
mean loss: 393.13
train mean loss: 393.89
epoch train time: 0:00:07.924516
elapsed time: 0:08:03.084455
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 11:14:13.685230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.32
 ---- batch: 020 ----
mean loss: 388.73
 ---- batch: 030 ----
mean loss: 399.03
 ---- batch: 040 ----
mean loss: 391.77
 ---- batch: 050 ----
mean loss: 394.85
train mean loss: 393.27
epoch train time: 0:00:07.945412
elapsed time: 0:08:11.031051
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 11:14:21.631792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.53
 ---- batch: 020 ----
mean loss: 388.29
 ---- batch: 030 ----
mean loss: 380.87
 ---- batch: 040 ----
mean loss: 378.01
 ---- batch: 050 ----
mean loss: 380.19
train mean loss: 383.41
epoch train time: 0:00:07.920264
elapsed time: 0:08:18.952435
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 11:14:29.553187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.11
 ---- batch: 020 ----
mean loss: 376.42
 ---- batch: 030 ----
mean loss: 388.07
 ---- batch: 040 ----
mean loss: 384.06
 ---- batch: 050 ----
mean loss: 386.31
train mean loss: 382.26
epoch train time: 0:00:07.926711
elapsed time: 0:08:26.880307
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 11:14:37.481076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.40
 ---- batch: 020 ----
mean loss: 364.45
 ---- batch: 030 ----
mean loss: 367.48
 ---- batch: 040 ----
mean loss: 366.52
 ---- batch: 050 ----
mean loss: 388.43
train mean loss: 372.87
epoch train time: 0:00:07.946318
elapsed time: 0:08:34.827793
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 11:14:45.428569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.57
 ---- batch: 020 ----
mean loss: 370.80
 ---- batch: 030 ----
mean loss: 371.61
 ---- batch: 040 ----
mean loss: 369.24
 ---- batch: 050 ----
mean loss: 359.48
train mean loss: 370.02
epoch train time: 0:00:07.923566
elapsed time: 0:08:42.752530
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 11:14:53.353298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.21
 ---- batch: 020 ----
mean loss: 360.82
 ---- batch: 030 ----
mean loss: 370.15
 ---- batch: 040 ----
mean loss: 363.38
 ---- batch: 050 ----
mean loss: 371.15
train mean loss: 363.99
epoch train time: 0:00:07.922285
elapsed time: 0:08:50.675943
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 11:15:01.276706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.03
 ---- batch: 020 ----
mean loss: 373.35
 ---- batch: 030 ----
mean loss: 353.09
 ---- batch: 040 ----
mean loss: 359.86
 ---- batch: 050 ----
mean loss: 362.65
train mean loss: 359.63
epoch train time: 0:00:07.923621
elapsed time: 0:08:58.600862
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 11:15:09.201815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.73
 ---- batch: 020 ----
mean loss: 358.66
 ---- batch: 030 ----
mean loss: 352.85
 ---- batch: 040 ----
mean loss: 366.18
 ---- batch: 050 ----
mean loss: 361.20
train mean loss: 356.24
epoch train time: 0:00:07.925912
elapsed time: 0:09:06.528387
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 11:15:17.129240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.53
 ---- batch: 020 ----
mean loss: 349.62
 ---- batch: 030 ----
mean loss: 349.92
 ---- batch: 040 ----
mean loss: 352.60
 ---- batch: 050 ----
mean loss: 346.90
train mean loss: 349.10
epoch train time: 0:00:07.900736
elapsed time: 0:09:14.430377
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 11:15:25.031169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.64
 ---- batch: 020 ----
mean loss: 345.61
 ---- batch: 030 ----
mean loss: 349.84
 ---- batch: 040 ----
mean loss: 355.13
 ---- batch: 050 ----
mean loss: 344.25
train mean loss: 345.22
epoch train time: 0:00:07.922837
elapsed time: 0:09:22.354382
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 11:15:32.955159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.75
 ---- batch: 020 ----
mean loss: 345.58
 ---- batch: 030 ----
mean loss: 344.16
 ---- batch: 040 ----
mean loss: 349.31
 ---- batch: 050 ----
mean loss: 344.03
train mean loss: 342.80
epoch train time: 0:00:07.910290
elapsed time: 0:09:30.265917
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 11:15:40.866712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.73
 ---- batch: 020 ----
mean loss: 338.47
 ---- batch: 030 ----
mean loss: 328.91
 ---- batch: 040 ----
mean loss: 334.28
 ---- batch: 050 ----
mean loss: 334.72
train mean loss: 337.05
epoch train time: 0:00:07.900170
elapsed time: 0:09:38.167284
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 11:15:48.768025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.02
 ---- batch: 020 ----
mean loss: 345.64
 ---- batch: 030 ----
mean loss: 340.16
 ---- batch: 040 ----
mean loss: 339.60
 ---- batch: 050 ----
mean loss: 337.48
train mean loss: 337.56
epoch train time: 0:00:07.949491
elapsed time: 0:09:46.118144
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 11:15:56.718905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.82
 ---- batch: 020 ----
mean loss: 323.80
 ---- batch: 030 ----
mean loss: 330.14
 ---- batch: 040 ----
mean loss: 337.09
 ---- batch: 050 ----
mean loss: 340.36
train mean loss: 333.37
epoch train time: 0:00:07.936856
elapsed time: 0:09:54.056200
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 11:16:04.657019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.60
 ---- batch: 020 ----
mean loss: 319.53
 ---- batch: 030 ----
mean loss: 327.14
 ---- batch: 040 ----
mean loss: 326.46
 ---- batch: 050 ----
mean loss: 321.17
train mean loss: 325.27
epoch train time: 0:00:07.974645
elapsed time: 0:10:02.032041
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 11:16:12.632875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.56
 ---- batch: 020 ----
mean loss: 323.01
 ---- batch: 030 ----
mean loss: 318.31
 ---- batch: 040 ----
mean loss: 332.50
 ---- batch: 050 ----
mean loss: 319.41
train mean loss: 325.25
epoch train time: 0:00:07.958766
elapsed time: 0:10:09.992077
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 11:16:20.592844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.13
 ---- batch: 020 ----
mean loss: 325.13
 ---- batch: 030 ----
mean loss: 318.50
 ---- batch: 040 ----
mean loss: 331.83
 ---- batch: 050 ----
mean loss: 311.20
train mean loss: 322.30
epoch train time: 0:00:07.916161
elapsed time: 0:10:17.909425
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 11:16:28.510288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.16
 ---- batch: 020 ----
mean loss: 325.88
 ---- batch: 030 ----
mean loss: 326.15
 ---- batch: 040 ----
mean loss: 319.09
 ---- batch: 050 ----
mean loss: 319.72
train mean loss: 317.76
epoch train time: 0:00:07.926096
elapsed time: 0:10:25.836854
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 11:16:36.437649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.89
 ---- batch: 020 ----
mean loss: 316.34
 ---- batch: 030 ----
mean loss: 326.56
 ---- batch: 040 ----
mean loss: 321.44
 ---- batch: 050 ----
mean loss: 320.86
train mean loss: 318.46
epoch train time: 0:00:07.921540
elapsed time: 0:10:33.759563
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 11:16:44.360328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.07
 ---- batch: 020 ----
mean loss: 318.50
 ---- batch: 030 ----
mean loss: 307.66
 ---- batch: 040 ----
mean loss: 312.41
 ---- batch: 050 ----
mean loss: 312.87
train mean loss: 311.73
epoch train time: 0:00:07.910887
elapsed time: 0:10:41.671612
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 11:16:52.272358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.45
 ---- batch: 020 ----
mean loss: 307.86
 ---- batch: 030 ----
mean loss: 323.20
 ---- batch: 040 ----
mean loss: 324.84
 ---- batch: 050 ----
mean loss: 307.37
train mean loss: 314.10
epoch train time: 0:00:07.942808
elapsed time: 0:10:49.615534
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 11:17:00.216303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.34
 ---- batch: 020 ----
mean loss: 303.06
 ---- batch: 030 ----
mean loss: 307.61
 ---- batch: 040 ----
mean loss: 324.31
 ---- batch: 050 ----
mean loss: 321.88
train mean loss: 312.31
epoch train time: 0:00:07.956182
elapsed time: 0:10:57.572890
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 11:17:08.173710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.68
 ---- batch: 020 ----
mean loss: 302.88
 ---- batch: 030 ----
mean loss: 300.69
 ---- batch: 040 ----
mean loss: 320.19
 ---- batch: 050 ----
mean loss: 314.95
train mean loss: 309.63
epoch train time: 0:00:07.925534
elapsed time: 0:11:05.499612
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 11:17:16.100376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.15
 ---- batch: 020 ----
mean loss: 302.88
 ---- batch: 030 ----
mean loss: 303.47
 ---- batch: 040 ----
mean loss: 300.94
 ---- batch: 050 ----
mean loss: 303.65
train mean loss: 304.35
epoch train time: 0:00:07.942773
elapsed time: 0:11:13.443527
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 11:17:24.044279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.19
 ---- batch: 020 ----
mean loss: 296.44
 ---- batch: 030 ----
mean loss: 296.07
 ---- batch: 040 ----
mean loss: 311.85
 ---- batch: 050 ----
mean loss: 302.13
train mean loss: 303.36
epoch train time: 0:00:07.963304
elapsed time: 0:11:21.407969
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 11:17:32.008739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.64
 ---- batch: 020 ----
mean loss: 297.36
 ---- batch: 030 ----
mean loss: 304.31
 ---- batch: 040 ----
mean loss: 300.75
 ---- batch: 050 ----
mean loss: 308.11
train mean loss: 303.04
epoch train time: 0:00:07.929505
elapsed time: 0:11:29.338655
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 11:17:39.939420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.34
 ---- batch: 020 ----
mean loss: 312.99
 ---- batch: 030 ----
mean loss: 286.05
 ---- batch: 040 ----
mean loss: 304.88
 ---- batch: 050 ----
mean loss: 300.15
train mean loss: 299.81
epoch train time: 0:00:07.929520
elapsed time: 0:11:37.269340
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 11:17:47.869933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.99
 ---- batch: 020 ----
mean loss: 296.24
 ---- batch: 030 ----
mean loss: 297.03
 ---- batch: 040 ----
mean loss: 295.29
 ---- batch: 050 ----
mean loss: 299.13
train mean loss: 297.51
epoch train time: 0:00:07.925193
elapsed time: 0:11:45.195602
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 11:17:55.796332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.32
 ---- batch: 020 ----
mean loss: 296.19
 ---- batch: 030 ----
mean loss: 296.97
 ---- batch: 040 ----
mean loss: 288.15
 ---- batch: 050 ----
mean loss: 297.52
train mean loss: 295.35
epoch train time: 0:00:07.967650
elapsed time: 0:11:53.164359
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 11:18:03.765165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.78
 ---- batch: 020 ----
mean loss: 288.43
 ---- batch: 030 ----
mean loss: 302.72
 ---- batch: 040 ----
mean loss: 301.66
 ---- batch: 050 ----
mean loss: 297.62
train mean loss: 296.11
epoch train time: 0:00:07.925506
elapsed time: 0:12:01.091132
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 11:18:11.691886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.48
 ---- batch: 020 ----
mean loss: 302.15
 ---- batch: 030 ----
mean loss: 298.35
 ---- batch: 040 ----
mean loss: 287.04
 ---- batch: 050 ----
mean loss: 292.11
train mean loss: 293.57
epoch train time: 0:00:07.915982
elapsed time: 0:12:09.008241
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 11:18:19.608991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.56
 ---- batch: 020 ----
mean loss: 289.78
 ---- batch: 030 ----
mean loss: 286.87
 ---- batch: 040 ----
mean loss: 281.72
 ---- batch: 050 ----
mean loss: 298.65
train mean loss: 289.66
epoch train time: 0:00:07.891437
elapsed time: 0:12:16.900861
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 11:18:27.501672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.09
 ---- batch: 020 ----
mean loss: 283.60
 ---- batch: 030 ----
mean loss: 294.83
 ---- batch: 040 ----
mean loss: 288.43
 ---- batch: 050 ----
mean loss: 290.12
train mean loss: 287.84
epoch train time: 0:00:07.911306
elapsed time: 0:12:24.813453
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 11:18:35.414227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.18
 ---- batch: 020 ----
mean loss: 288.76
 ---- batch: 030 ----
mean loss: 288.68
 ---- batch: 040 ----
mean loss: 282.19
 ---- batch: 050 ----
mean loss: 288.95
train mean loss: 287.57
epoch train time: 0:00:07.908005
elapsed time: 0:12:32.722645
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 11:18:43.323451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.09
 ---- batch: 020 ----
mean loss: 286.31
 ---- batch: 030 ----
mean loss: 285.79
 ---- batch: 040 ----
mean loss: 288.47
 ---- batch: 050 ----
mean loss: 284.57
train mean loss: 285.52
epoch train time: 0:00:07.876233
elapsed time: 0:12:40.600129
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 11:18:51.200922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.69
 ---- batch: 020 ----
mean loss: 283.79
 ---- batch: 030 ----
mean loss: 285.42
 ---- batch: 040 ----
mean loss: 286.49
 ---- batch: 050 ----
mean loss: 276.41
train mean loss: 282.88
epoch train time: 0:00:07.872311
elapsed time: 0:12:48.473753
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 11:18:59.074564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.41
 ---- batch: 020 ----
mean loss: 283.40
 ---- batch: 030 ----
mean loss: 283.42
 ---- batch: 040 ----
mean loss: 277.61
 ---- batch: 050 ----
mean loss: 288.84
train mean loss: 279.46
epoch train time: 0:00:07.878347
elapsed time: 0:12:56.353330
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 11:19:06.954136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.74
 ---- batch: 020 ----
mean loss: 272.53
 ---- batch: 030 ----
mean loss: 283.01
 ---- batch: 040 ----
mean loss: 279.69
 ---- batch: 050 ----
mean loss: 279.52
train mean loss: 277.24
epoch train time: 0:00:07.874000
elapsed time: 0:13:04.228541
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 11:19:14.829305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.22
 ---- batch: 020 ----
mean loss: 287.07
 ---- batch: 030 ----
mean loss: 272.92
 ---- batch: 040 ----
mean loss: 273.37
 ---- batch: 050 ----
mean loss: 289.07
train mean loss: 280.06
epoch train time: 0:00:07.892270
elapsed time: 0:13:12.122026
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 11:19:22.722787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.49
 ---- batch: 020 ----
mean loss: 283.43
 ---- batch: 030 ----
mean loss: 270.33
 ---- batch: 040 ----
mean loss: 274.64
 ---- batch: 050 ----
mean loss: 274.40
train mean loss: 275.38
epoch train time: 0:00:07.889750
elapsed time: 0:13:20.012902
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 11:19:30.613702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.70
 ---- batch: 020 ----
mean loss: 279.23
 ---- batch: 030 ----
mean loss: 273.57
 ---- batch: 040 ----
mean loss: 284.09
 ---- batch: 050 ----
mean loss: 274.89
train mean loss: 278.07
epoch train time: 0:00:07.894771
elapsed time: 0:13:27.908853
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 11:19:38.509632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.38
 ---- batch: 020 ----
mean loss: 289.28
 ---- batch: 030 ----
mean loss: 278.89
 ---- batch: 040 ----
mean loss: 266.79
 ---- batch: 050 ----
mean loss: 271.83
train mean loss: 276.74
epoch train time: 0:00:07.908624
elapsed time: 0:13:35.818710
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 11:19:46.419470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.86
 ---- batch: 020 ----
mean loss: 275.68
 ---- batch: 030 ----
mean loss: 271.26
 ---- batch: 040 ----
mean loss: 270.19
 ---- batch: 050 ----
mean loss: 268.91
train mean loss: 272.12
epoch train time: 0:00:07.890660
elapsed time: 0:13:43.710529
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 11:19:54.311300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.64
 ---- batch: 020 ----
mean loss: 272.70
 ---- batch: 030 ----
mean loss: 271.48
 ---- batch: 040 ----
mean loss: 263.83
 ---- batch: 050 ----
mean loss: 270.00
train mean loss: 273.14
epoch train time: 0:00:07.922971
elapsed time: 0:13:51.634723
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 11:20:02.235583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.08
 ---- batch: 020 ----
mean loss: 261.01
 ---- batch: 030 ----
mean loss: 271.00
 ---- batch: 040 ----
mean loss: 270.39
 ---- batch: 050 ----
mean loss: 274.98
train mean loss: 270.42
epoch train time: 0:00:07.918708
elapsed time: 0:13:59.554661
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 11:20:10.155437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.32
 ---- batch: 020 ----
mean loss: 269.36
 ---- batch: 030 ----
mean loss: 273.90
 ---- batch: 040 ----
mean loss: 267.56
 ---- batch: 050 ----
mean loss: 276.26
train mean loss: 271.44
epoch train time: 0:00:07.924871
elapsed time: 0:14:07.480700
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 11:20:18.081463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.80
 ---- batch: 020 ----
mean loss: 264.45
 ---- batch: 030 ----
mean loss: 264.87
 ---- batch: 040 ----
mean loss: 271.73
 ---- batch: 050 ----
mean loss: 284.13
train mean loss: 267.46
epoch train time: 0:00:07.937525
elapsed time: 0:14:15.419378
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 11:20:26.020143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.69
 ---- batch: 020 ----
mean loss: 263.25
 ---- batch: 030 ----
mean loss: 264.87
 ---- batch: 040 ----
mean loss: 266.17
 ---- batch: 050 ----
mean loss: 261.04
train mean loss: 264.96
epoch train time: 0:00:07.959389
elapsed time: 0:14:23.380251
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 11:20:33.980785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.31
 ---- batch: 020 ----
mean loss: 255.89
 ---- batch: 030 ----
mean loss: 264.73
 ---- batch: 040 ----
mean loss: 261.69
 ---- batch: 050 ----
mean loss: 269.83
train mean loss: 262.67
epoch train time: 0:00:07.931731
elapsed time: 0:14:31.313032
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 11:20:41.913832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.90
 ---- batch: 020 ----
mean loss: 264.13
 ---- batch: 030 ----
mean loss: 253.27
 ---- batch: 040 ----
mean loss: 266.38
 ---- batch: 050 ----
mean loss: 259.04
train mean loss: 262.82
epoch train time: 0:00:07.913391
elapsed time: 0:14:39.227618
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 11:20:49.828417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.76
 ---- batch: 020 ----
mean loss: 259.24
 ---- batch: 030 ----
mean loss: 250.47
 ---- batch: 040 ----
mean loss: 274.29
 ---- batch: 050 ----
mean loss: 271.35
train mean loss: 263.44
epoch train time: 0:00:07.922091
elapsed time: 0:14:47.150980
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 11:20:57.751843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.86
 ---- batch: 020 ----
mean loss: 261.19
 ---- batch: 030 ----
mean loss: 266.97
 ---- batch: 040 ----
mean loss: 274.50
 ---- batch: 050 ----
mean loss: 262.74
train mean loss: 263.69
epoch train time: 0:00:07.894178
elapsed time: 0:14:55.046406
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 11:21:05.647155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.71
 ---- batch: 020 ----
mean loss: 255.51
 ---- batch: 030 ----
mean loss: 273.76
 ---- batch: 040 ----
mean loss: 262.55
 ---- batch: 050 ----
mean loss: 260.41
train mean loss: 261.64
epoch train time: 0:00:07.919219
elapsed time: 0:15:02.966866
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 11:21:13.567668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.19
 ---- batch: 020 ----
mean loss: 251.90
 ---- batch: 030 ----
mean loss: 260.51
 ---- batch: 040 ----
mean loss: 253.16
 ---- batch: 050 ----
mean loss: 261.27
train mean loss: 259.71
epoch train time: 0:00:07.963145
elapsed time: 0:15:10.931182
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 11:21:21.532011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.97
 ---- batch: 020 ----
mean loss: 261.49
 ---- batch: 030 ----
mean loss: 257.48
 ---- batch: 040 ----
mean loss: 259.38
 ---- batch: 050 ----
mean loss: 247.88
train mean loss: 256.19
epoch train time: 0:00:07.989322
elapsed time: 0:15:18.921806
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 11:21:29.522573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.16
 ---- batch: 020 ----
mean loss: 267.21
 ---- batch: 030 ----
mean loss: 257.74
 ---- batch: 040 ----
mean loss: 258.71
 ---- batch: 050 ----
mean loss: 251.07
train mean loss: 257.83
epoch train time: 0:00:07.955606
elapsed time: 0:15:26.878625
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 11:21:37.479380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.66
 ---- batch: 020 ----
mean loss: 251.74
 ---- batch: 030 ----
mean loss: 261.10
 ---- batch: 040 ----
mean loss: 266.40
 ---- batch: 050 ----
mean loss: 251.31
train mean loss: 255.96
epoch train time: 0:00:07.947901
elapsed time: 0:15:34.827692
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 11:21:45.428455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.75
 ---- batch: 020 ----
mean loss: 252.83
 ---- batch: 030 ----
mean loss: 251.75
 ---- batch: 040 ----
mean loss: 259.98
 ---- batch: 050 ----
mean loss: 255.13
train mean loss: 254.35
epoch train time: 0:00:07.936398
elapsed time: 0:15:42.765279
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 11:21:53.366057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.91
 ---- batch: 020 ----
mean loss: 257.33
 ---- batch: 030 ----
mean loss: 250.90
 ---- batch: 040 ----
mean loss: 245.17
 ---- batch: 050 ----
mean loss: 254.32
train mean loss: 253.83
epoch train time: 0:00:07.964932
elapsed time: 0:15:50.731385
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 11:22:01.332148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.69
 ---- batch: 020 ----
mean loss: 244.23
 ---- batch: 030 ----
mean loss: 250.23
 ---- batch: 040 ----
mean loss: 253.03
 ---- batch: 050 ----
mean loss: 257.24
train mean loss: 252.70
epoch train time: 0:00:07.926378
elapsed time: 0:15:58.659018
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 11:22:09.259808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.29
 ---- batch: 020 ----
mean loss: 255.06
 ---- batch: 030 ----
mean loss: 243.73
 ---- batch: 040 ----
mean loss: 254.51
 ---- batch: 050 ----
mean loss: 247.27
train mean loss: 251.91
epoch train time: 0:00:07.922675
elapsed time: 0:16:06.582886
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 11:22:17.183664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.84
 ---- batch: 020 ----
mean loss: 250.03
 ---- batch: 030 ----
mean loss: 254.66
 ---- batch: 040 ----
mean loss: 245.32
 ---- batch: 050 ----
mean loss: 261.02
train mean loss: 252.06
epoch train time: 0:00:07.916410
elapsed time: 0:16:14.500491
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 11:22:25.101252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.35
 ---- batch: 020 ----
mean loss: 256.71
 ---- batch: 030 ----
mean loss: 251.55
 ---- batch: 040 ----
mean loss: 243.45
 ---- batch: 050 ----
mean loss: 240.45
train mean loss: 248.99
epoch train time: 0:00:07.934806
elapsed time: 0:16:22.436444
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 11:22:33.037219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.36
 ---- batch: 020 ----
mean loss: 246.76
 ---- batch: 030 ----
mean loss: 247.20
 ---- batch: 040 ----
mean loss: 253.42
 ---- batch: 050 ----
mean loss: 241.94
train mean loss: 247.14
epoch train time: 0:00:07.915127
elapsed time: 0:16:30.353062
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 11:22:40.954031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.52
 ---- batch: 020 ----
mean loss: 246.26
 ---- batch: 030 ----
mean loss: 240.84
 ---- batch: 040 ----
mean loss: 249.33
 ---- batch: 050 ----
mean loss: 242.75
train mean loss: 245.14
epoch train time: 0:00:07.931810
elapsed time: 0:16:38.286296
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 11:22:48.887043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.88
 ---- batch: 020 ----
mean loss: 251.45
 ---- batch: 030 ----
mean loss: 249.37
 ---- batch: 040 ----
mean loss: 247.83
 ---- batch: 050 ----
mean loss: 251.28
train mean loss: 247.70
epoch train time: 0:00:07.913251
elapsed time: 0:16:46.200763
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 11:22:56.801516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.81
 ---- batch: 020 ----
mean loss: 247.47
 ---- batch: 030 ----
mean loss: 237.80
 ---- batch: 040 ----
mean loss: 244.70
 ---- batch: 050 ----
mean loss: 245.57
train mean loss: 245.24
epoch train time: 0:00:07.960668
elapsed time: 0:16:54.162719
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 11:23:04.763562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.99
 ---- batch: 020 ----
mean loss: 243.20
 ---- batch: 030 ----
mean loss: 246.41
 ---- batch: 040 ----
mean loss: 246.41
 ---- batch: 050 ----
mean loss: 249.16
train mean loss: 244.29
epoch train time: 0:00:07.979668
elapsed time: 0:17:02.144108
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 11:23:12.744694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.70
 ---- batch: 020 ----
mean loss: 242.18
 ---- batch: 030 ----
mean loss: 244.80
 ---- batch: 040 ----
mean loss: 234.22
 ---- batch: 050 ----
mean loss: 244.13
train mean loss: 241.92
epoch train time: 0:00:07.950494
elapsed time: 0:17:10.095665
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 11:23:20.696420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.78
 ---- batch: 020 ----
mean loss: 234.55
 ---- batch: 030 ----
mean loss: 240.68
 ---- batch: 040 ----
mean loss: 246.12
 ---- batch: 050 ----
mean loss: 236.72
train mean loss: 240.26
epoch train time: 0:00:07.934478
elapsed time: 0:17:18.031468
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 11:23:28.632237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.25
 ---- batch: 020 ----
mean loss: 241.58
 ---- batch: 030 ----
mean loss: 239.69
 ---- batch: 040 ----
mean loss: 238.34
 ---- batch: 050 ----
mean loss: 235.75
train mean loss: 240.59
epoch train time: 0:00:07.963060
elapsed time: 0:17:25.995706
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 11:23:36.596464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.02
 ---- batch: 020 ----
mean loss: 242.63
 ---- batch: 030 ----
mean loss: 241.91
 ---- batch: 040 ----
mean loss: 234.88
 ---- batch: 050 ----
mean loss: 240.34
train mean loss: 237.36
epoch train time: 0:00:07.942514
elapsed time: 0:17:33.939382
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 11:23:44.540147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.91
 ---- batch: 020 ----
mean loss: 237.36
 ---- batch: 030 ----
mean loss: 235.77
 ---- batch: 040 ----
mean loss: 240.93
 ---- batch: 050 ----
mean loss: 228.78
train mean loss: 237.19
epoch train time: 0:00:07.943074
elapsed time: 0:17:41.883578
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 11:23:52.484338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.41
 ---- batch: 020 ----
mean loss: 236.74
 ---- batch: 030 ----
mean loss: 238.94
 ---- batch: 040 ----
mean loss: 241.89
 ---- batch: 050 ----
mean loss: 234.80
train mean loss: 237.78
epoch train time: 0:00:07.937228
elapsed time: 0:17:49.821990
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 11:24:00.422805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.50
 ---- batch: 020 ----
mean loss: 237.36
 ---- batch: 030 ----
mean loss: 244.00
 ---- batch: 040 ----
mean loss: 239.32
 ---- batch: 050 ----
mean loss: 233.71
train mean loss: 236.45
epoch train time: 0:00:07.918558
elapsed time: 0:17:57.741901
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 11:24:08.342756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.90
 ---- batch: 020 ----
mean loss: 241.88
 ---- batch: 030 ----
mean loss: 234.33
 ---- batch: 040 ----
mean loss: 231.17
 ---- batch: 050 ----
mean loss: 228.98
train mean loss: 235.37
epoch train time: 0:00:07.927213
elapsed time: 0:18:05.670404
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 11:24:16.271238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.14
 ---- batch: 020 ----
mean loss: 235.67
 ---- batch: 030 ----
mean loss: 231.95
 ---- batch: 040 ----
mean loss: 234.50
 ---- batch: 050 ----
mean loss: 235.93
train mean loss: 234.21
epoch train time: 0:00:07.900160
elapsed time: 0:18:13.571810
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 11:24:24.172581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.14
 ---- batch: 020 ----
mean loss: 232.21
 ---- batch: 030 ----
mean loss: 236.71
 ---- batch: 040 ----
mean loss: 239.09
 ---- batch: 050 ----
mean loss: 236.74
train mean loss: 234.35
epoch train time: 0:00:07.936561
elapsed time: 0:18:21.509844
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 11:24:32.110636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.13
 ---- batch: 020 ----
mean loss: 244.43
 ---- batch: 030 ----
mean loss: 228.86
 ---- batch: 040 ----
mean loss: 237.74
 ---- batch: 050 ----
mean loss: 232.91
train mean loss: 232.53
epoch train time: 0:00:07.954510
elapsed time: 0:18:29.465628
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 11:24:40.066399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.36
 ---- batch: 020 ----
mean loss: 241.32
 ---- batch: 030 ----
mean loss: 226.84
 ---- batch: 040 ----
mean loss: 228.63
 ---- batch: 050 ----
mean loss: 233.73
train mean loss: 232.57
epoch train time: 0:00:07.942704
elapsed time: 0:18:37.409478
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 11:24:48.010254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.38
 ---- batch: 020 ----
mean loss: 232.69
 ---- batch: 030 ----
mean loss: 230.20
 ---- batch: 040 ----
mean loss: 235.26
 ---- batch: 050 ----
mean loss: 229.78
train mean loss: 232.19
epoch train time: 0:00:07.933251
elapsed time: 0:18:45.343946
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 11:24:55.944689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.83
 ---- batch: 020 ----
mean loss: 229.03
 ---- batch: 030 ----
mean loss: 240.01
 ---- batch: 040 ----
mean loss: 224.87
 ---- batch: 050 ----
mean loss: 237.18
train mean loss: 231.33
epoch train time: 0:00:07.970703
elapsed time: 0:18:53.315945
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 11:25:03.917116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.41
 ---- batch: 020 ----
mean loss: 242.17
 ---- batch: 030 ----
mean loss: 226.75
 ---- batch: 040 ----
mean loss: 232.95
 ---- batch: 050 ----
mean loss: 224.78
train mean loss: 231.01
epoch train time: 0:00:07.938664
elapsed time: 0:19:01.256253
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 11:25:11.857024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.14
 ---- batch: 020 ----
mean loss: 223.05
 ---- batch: 030 ----
mean loss: 228.22
 ---- batch: 040 ----
mean loss: 228.42
 ---- batch: 050 ----
mean loss: 228.76
train mean loss: 228.34
epoch train time: 0:00:07.949291
elapsed time: 0:19:09.206761
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 11:25:19.807558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.76
 ---- batch: 020 ----
mean loss: 226.24
 ---- batch: 030 ----
mean loss: 224.02
 ---- batch: 040 ----
mean loss: 221.33
 ---- batch: 050 ----
mean loss: 224.48
train mean loss: 225.22
epoch train time: 0:00:07.949749
elapsed time: 0:19:17.157739
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 11:25:27.758514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.66
 ---- batch: 020 ----
mean loss: 224.41
 ---- batch: 030 ----
mean loss: 222.74
 ---- batch: 040 ----
mean loss: 218.64
 ---- batch: 050 ----
mean loss: 234.44
train mean loss: 224.49
epoch train time: 0:00:07.944753
elapsed time: 0:19:25.103734
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 11:25:35.704629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.35
 ---- batch: 020 ----
mean loss: 228.71
 ---- batch: 030 ----
mean loss: 230.74
 ---- batch: 040 ----
mean loss: 237.74
 ---- batch: 050 ----
mean loss: 216.19
train mean loss: 228.07
epoch train time: 0:00:07.976008
elapsed time: 0:19:33.081033
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 11:25:43.681811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.43
 ---- batch: 020 ----
mean loss: 221.08
 ---- batch: 030 ----
mean loss: 216.21
 ---- batch: 040 ----
mean loss: 230.67
 ---- batch: 050 ----
mean loss: 226.21
train mean loss: 224.04
epoch train time: 0:00:07.961834
elapsed time: 0:19:41.044072
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 11:25:51.644835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.82
 ---- batch: 020 ----
mean loss: 214.05
 ---- batch: 030 ----
mean loss: 227.13
 ---- batch: 040 ----
mean loss: 232.84
 ---- batch: 050 ----
mean loss: 221.61
train mean loss: 223.16
epoch train time: 0:00:07.951422
elapsed time: 0:19:48.996717
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 11:25:59.597483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.96
 ---- batch: 020 ----
mean loss: 214.29
 ---- batch: 030 ----
mean loss: 223.64
 ---- batch: 040 ----
mean loss: 225.91
 ---- batch: 050 ----
mean loss: 223.03
train mean loss: 223.31
epoch train time: 0:00:07.945598
elapsed time: 0:19:56.943761
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 11:26:07.544291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.24
 ---- batch: 020 ----
mean loss: 220.66
 ---- batch: 030 ----
mean loss: 223.26
 ---- batch: 040 ----
mean loss: 213.80
 ---- batch: 050 ----
mean loss: 229.29
train mean loss: 220.76
epoch train time: 0:00:07.918566
elapsed time: 0:20:04.863268
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 11:26:15.464088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.47
 ---- batch: 020 ----
mean loss: 222.99
 ---- batch: 030 ----
mean loss: 226.02
 ---- batch: 040 ----
mean loss: 220.64
 ---- batch: 050 ----
mean loss: 222.54
train mean loss: 222.58
epoch train time: 0:00:07.915304
elapsed time: 0:20:12.779876
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 11:26:23.380682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.12
 ---- batch: 020 ----
mean loss: 220.39
 ---- batch: 030 ----
mean loss: 220.98
 ---- batch: 040 ----
mean loss: 214.18
 ---- batch: 050 ----
mean loss: 217.55
train mean loss: 220.05
epoch train time: 0:00:07.941335
elapsed time: 0:20:20.722553
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 11:26:31.323325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.66
 ---- batch: 020 ----
mean loss: 222.97
 ---- batch: 030 ----
mean loss: 217.06
 ---- batch: 040 ----
mean loss: 223.23
 ---- batch: 050 ----
mean loss: 220.81
train mean loss: 218.58
epoch train time: 0:00:08.014165
elapsed time: 0:20:28.737867
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 11:26:39.338673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.64
 ---- batch: 020 ----
mean loss: 223.84
 ---- batch: 030 ----
mean loss: 226.19
 ---- batch: 040 ----
mean loss: 208.12
 ---- batch: 050 ----
mean loss: 215.17
train mean loss: 218.26
epoch train time: 0:00:07.928656
elapsed time: 0:20:36.667697
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 11:26:47.268462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.77
 ---- batch: 020 ----
mean loss: 226.28
 ---- batch: 030 ----
mean loss: 209.59
 ---- batch: 040 ----
mean loss: 224.25
 ---- batch: 050 ----
mean loss: 217.14
train mean loss: 218.67
epoch train time: 0:00:07.921962
elapsed time: 0:20:44.590800
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 11:26:55.191591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.44
 ---- batch: 020 ----
mean loss: 209.41
 ---- batch: 030 ----
mean loss: 227.06
 ---- batch: 040 ----
mean loss: 213.09
 ---- batch: 050 ----
mean loss: 219.02
train mean loss: 215.13
epoch train time: 0:00:07.931238
elapsed time: 0:20:52.523228
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 11:27:03.124024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.23
 ---- batch: 020 ----
mean loss: 216.16
 ---- batch: 030 ----
mean loss: 208.49
 ---- batch: 040 ----
mean loss: 220.18
 ---- batch: 050 ----
mean loss: 219.52
train mean loss: 217.87
epoch train time: 0:00:07.934819
elapsed time: 0:21:00.459280
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 11:27:11.060048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.78
 ---- batch: 020 ----
mean loss: 220.37
 ---- batch: 030 ----
mean loss: 215.54
 ---- batch: 040 ----
mean loss: 209.18
 ---- batch: 050 ----
mean loss: 217.00
train mean loss: 215.52
epoch train time: 0:00:07.957138
elapsed time: 0:21:08.417697
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 11:27:19.018498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.22
 ---- batch: 020 ----
mean loss: 211.15
 ---- batch: 030 ----
mean loss: 220.30
 ---- batch: 040 ----
mean loss: 213.29
 ---- batch: 050 ----
mean loss: 208.21
train mean loss: 213.70
epoch train time: 0:00:07.936142
elapsed time: 0:21:16.355071
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 11:27:26.955971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.17
 ---- batch: 020 ----
mean loss: 214.84
 ---- batch: 030 ----
mean loss: 204.70
 ---- batch: 040 ----
mean loss: 210.45
 ---- batch: 050 ----
mean loss: 215.81
train mean loss: 211.27
epoch train time: 0:00:07.924614
elapsed time: 0:21:24.280965
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 11:27:34.881758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.07
 ---- batch: 020 ----
mean loss: 206.43
 ---- batch: 030 ----
mean loss: 208.97
 ---- batch: 040 ----
mean loss: 219.92
 ---- batch: 050 ----
mean loss: 207.63
train mean loss: 214.02
epoch train time: 0:00:08.012789
elapsed time: 0:21:32.294989
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 11:27:42.895770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.00
 ---- batch: 020 ----
mean loss: 224.88
 ---- batch: 030 ----
mean loss: 212.25
 ---- batch: 040 ----
mean loss: 206.08
 ---- batch: 050 ----
mean loss: 215.77
train mean loss: 214.55
epoch train time: 0:00:07.977748
elapsed time: 0:21:40.273901
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 11:27:50.874666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.04
 ---- batch: 020 ----
mean loss: 208.80
 ---- batch: 030 ----
mean loss: 213.25
 ---- batch: 040 ----
mean loss: 208.50
 ---- batch: 050 ----
mean loss: 208.36
train mean loss: 210.63
epoch train time: 0:00:07.955353
elapsed time: 0:21:48.230477
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 11:27:58.831234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.00
 ---- batch: 020 ----
mean loss: 209.79
 ---- batch: 030 ----
mean loss: 207.92
 ---- batch: 040 ----
mean loss: 209.56
 ---- batch: 050 ----
mean loss: 211.89
train mean loss: 210.15
epoch train time: 0:00:07.941509
elapsed time: 0:21:56.173129
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 11:28:06.773918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.55
 ---- batch: 020 ----
mean loss: 210.34
 ---- batch: 030 ----
mean loss: 221.18
 ---- batch: 040 ----
mean loss: 212.46
 ---- batch: 050 ----
mean loss: 205.60
train mean loss: 211.57
epoch train time: 0:00:07.989989
elapsed time: 0:22:04.164281
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 11:28:14.765048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.02
 ---- batch: 020 ----
mean loss: 213.35
 ---- batch: 030 ----
mean loss: 219.61
 ---- batch: 040 ----
mean loss: 201.15
 ---- batch: 050 ----
mean loss: 207.56
train mean loss: 208.93
epoch train time: 0:00:07.961101
elapsed time: 0:22:12.126621
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 11:28:22.727372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.77
 ---- batch: 020 ----
mean loss: 203.02
 ---- batch: 030 ----
mean loss: 213.48
 ---- batch: 040 ----
mean loss: 209.13
 ---- batch: 050 ----
mean loss: 208.10
train mean loss: 209.46
epoch train time: 0:00:07.957641
elapsed time: 0:22:20.085527
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 11:28:30.686313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.36
 ---- batch: 020 ----
mean loss: 210.41
 ---- batch: 030 ----
mean loss: 214.17
 ---- batch: 040 ----
mean loss: 203.92
 ---- batch: 050 ----
mean loss: 202.19
train mean loss: 207.54
epoch train time: 0:00:07.944823
elapsed time: 0:22:28.031762
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 11:28:38.632401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.39
 ---- batch: 020 ----
mean loss: 212.79
 ---- batch: 030 ----
mean loss: 206.76
 ---- batch: 040 ----
mean loss: 205.70
 ---- batch: 050 ----
mean loss: 200.60
train mean loss: 205.93
epoch train time: 0:00:07.948278
elapsed time: 0:22:35.981294
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 11:28:46.582085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.20
 ---- batch: 020 ----
mean loss: 213.27
 ---- batch: 030 ----
mean loss: 205.32
 ---- batch: 040 ----
mean loss: 212.05
 ---- batch: 050 ----
mean loss: 202.85
train mean loss: 207.85
epoch train time: 0:00:07.958597
elapsed time: 0:22:43.941078
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 11:28:54.541925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.87
 ---- batch: 020 ----
mean loss: 209.58
 ---- batch: 030 ----
mean loss: 213.67
 ---- batch: 040 ----
mean loss: 210.01
 ---- batch: 050 ----
mean loss: 200.32
train mean loss: 208.64
epoch train time: 0:00:07.950049
elapsed time: 0:22:51.892440
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 11:29:02.493236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.84
 ---- batch: 020 ----
mean loss: 206.77
 ---- batch: 030 ----
mean loss: 204.97
 ---- batch: 040 ----
mean loss: 216.59
 ---- batch: 050 ----
mean loss: 200.55
train mean loss: 205.92
epoch train time: 0:00:07.941125
elapsed time: 0:22:59.834764
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 11:29:10.435526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.53
 ---- batch: 020 ----
mean loss: 210.85
 ---- batch: 030 ----
mean loss: 205.11
 ---- batch: 040 ----
mean loss: 205.31
 ---- batch: 050 ----
mean loss: 200.70
train mean loss: 204.84
epoch train time: 0:00:07.959949
elapsed time: 0:23:07.795963
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 11:29:18.396826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.06
 ---- batch: 020 ----
mean loss: 204.49
 ---- batch: 030 ----
mean loss: 211.50
 ---- batch: 040 ----
mean loss: 204.57
 ---- batch: 050 ----
mean loss: 207.63
train mean loss: 204.38
epoch train time: 0:00:07.941037
elapsed time: 0:23:15.738541
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 11:29:26.339050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.36
 ---- batch: 020 ----
mean loss: 201.22
 ---- batch: 030 ----
mean loss: 205.60
 ---- batch: 040 ----
mean loss: 202.31
 ---- batch: 050 ----
mean loss: 210.07
train mean loss: 204.02
epoch train time: 0:00:07.938220
elapsed time: 0:23:23.677842
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 11:29:34.278675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.51
 ---- batch: 020 ----
mean loss: 204.31
 ---- batch: 030 ----
mean loss: 204.92
 ---- batch: 040 ----
mean loss: 200.51
 ---- batch: 050 ----
mean loss: 205.24
train mean loss: 206.26
epoch train time: 0:00:07.949552
elapsed time: 0:23:31.628659
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 11:29:42.229410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.91
 ---- batch: 020 ----
mean loss: 203.04
 ---- batch: 030 ----
mean loss: 209.19
 ---- batch: 040 ----
mean loss: 211.70
 ---- batch: 050 ----
mean loss: 205.28
train mean loss: 203.39
epoch train time: 0:00:07.961550
elapsed time: 0:23:39.591425
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 11:29:50.192283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.94
 ---- batch: 020 ----
mean loss: 207.26
 ---- batch: 030 ----
mean loss: 202.90
 ---- batch: 040 ----
mean loss: 200.53
 ---- batch: 050 ----
mean loss: 200.34
train mean loss: 202.46
epoch train time: 0:00:07.965023
elapsed time: 0:23:47.557747
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 11:29:58.158569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.92
 ---- batch: 020 ----
mean loss: 194.53
 ---- batch: 030 ----
mean loss: 203.53
 ---- batch: 040 ----
mean loss: 206.86
 ---- batch: 050 ----
mean loss: 201.88
train mean loss: 199.78
epoch train time: 0:00:07.951495
elapsed time: 0:23:55.510424
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 11:30:06.111181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.35
 ---- batch: 020 ----
mean loss: 198.87
 ---- batch: 030 ----
mean loss: 197.99
 ---- batch: 040 ----
mean loss: 203.08
 ---- batch: 050 ----
mean loss: 199.23
train mean loss: 199.35
epoch train time: 0:00:07.948375
elapsed time: 0:24:03.459976
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 11:30:14.060755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.57
 ---- batch: 020 ----
mean loss: 208.11
 ---- batch: 030 ----
mean loss: 204.38
 ---- batch: 040 ----
mean loss: 198.26
 ---- batch: 050 ----
mean loss: 194.00
train mean loss: 199.88
epoch train time: 0:00:07.955301
elapsed time: 0:24:11.416452
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 11:30:22.017195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.46
 ---- batch: 020 ----
mean loss: 201.81
 ---- batch: 030 ----
mean loss: 194.30
 ---- batch: 040 ----
mean loss: 206.46
 ---- batch: 050 ----
mean loss: 212.24
train mean loss: 202.11
epoch train time: 0:00:07.951017
elapsed time: 0:24:19.368624
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 11:30:29.969395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.40
 ---- batch: 020 ----
mean loss: 202.14
 ---- batch: 030 ----
mean loss: 197.53
 ---- batch: 040 ----
mean loss: 208.36
 ---- batch: 050 ----
mean loss: 207.88
train mean loss: 201.85
epoch train time: 0:00:07.949861
elapsed time: 0:24:27.319700
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 11:30:37.920448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.49
 ---- batch: 020 ----
mean loss: 197.38
 ---- batch: 030 ----
mean loss: 205.75
 ---- batch: 040 ----
mean loss: 197.43
 ---- batch: 050 ----
mean loss: 200.63
train mean loss: 200.30
epoch train time: 0:00:07.952431
elapsed time: 0:24:35.273365
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 11:30:45.874153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.79
 ---- batch: 020 ----
mean loss: 200.86
 ---- batch: 030 ----
mean loss: 193.62
 ---- batch: 040 ----
mean loss: 200.90
 ---- batch: 050 ----
mean loss: 192.33
train mean loss: 197.28
epoch train time: 0:00:07.959179
elapsed time: 0:24:43.233806
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 11:30:53.834633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.26
 ---- batch: 020 ----
mean loss: 207.32
 ---- batch: 030 ----
mean loss: 197.97
 ---- batch: 040 ----
mean loss: 192.94
 ---- batch: 050 ----
mean loss: 200.81
train mean loss: 198.76
epoch train time: 0:00:07.966934
elapsed time: 0:24:51.202029
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 11:31:01.802804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.61
 ---- batch: 020 ----
mean loss: 190.96
 ---- batch: 030 ----
mean loss: 189.00
 ---- batch: 040 ----
mean loss: 201.17
 ---- batch: 050 ----
mean loss: 204.92
train mean loss: 196.58
epoch train time: 0:00:07.921342
elapsed time: 0:24:59.124648
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 11:31:09.725399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.08
 ---- batch: 020 ----
mean loss: 196.04
 ---- batch: 030 ----
mean loss: 195.99
 ---- batch: 040 ----
mean loss: 197.82
 ---- batch: 050 ----
mean loss: 201.78
train mean loss: 198.99
epoch train time: 0:00:07.921462
elapsed time: 0:25:07.047328
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 11:31:17.648122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.44
 ---- batch: 020 ----
mean loss: 195.95
 ---- batch: 030 ----
mean loss: 194.51
 ---- batch: 040 ----
mean loss: 192.88
 ---- batch: 050 ----
mean loss: 199.00
train mean loss: 196.21
epoch train time: 0:00:07.956986
elapsed time: 0:25:15.005611
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 11:31:25.606365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.69
 ---- batch: 020 ----
mean loss: 196.41
 ---- batch: 030 ----
mean loss: 204.18
 ---- batch: 040 ----
mean loss: 194.58
 ---- batch: 050 ----
mean loss: 190.51
train mean loss: 196.12
epoch train time: 0:00:07.960335
elapsed time: 0:25:22.967143
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 11:31:33.567940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.16
 ---- batch: 020 ----
mean loss: 201.12
 ---- batch: 030 ----
mean loss: 188.05
 ---- batch: 040 ----
mean loss: 196.97
 ---- batch: 050 ----
mean loss: 192.16
train mean loss: 197.05
epoch train time: 0:00:07.984044
elapsed time: 0:25:30.952391
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 11:31:41.553180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.13
 ---- batch: 020 ----
mean loss: 198.38
 ---- batch: 030 ----
mean loss: 192.83
 ---- batch: 040 ----
mean loss: 199.27
 ---- batch: 050 ----
mean loss: 198.86
train mean loss: 195.65
epoch train time: 0:00:08.043229
elapsed time: 0:25:38.996846
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 11:31:49.597658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.57
 ---- batch: 020 ----
mean loss: 193.35
 ---- batch: 030 ----
mean loss: 199.34
 ---- batch: 040 ----
mean loss: 199.24
 ---- batch: 050 ----
mean loss: 185.10
train mean loss: 194.58
epoch train time: 0:00:07.924003
elapsed time: 0:25:46.922067
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 11:31:57.522830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.06
 ---- batch: 020 ----
mean loss: 193.82
 ---- batch: 030 ----
mean loss: 192.42
 ---- batch: 040 ----
mean loss: 205.58
 ---- batch: 050 ----
mean loss: 198.87
train mean loss: 194.49
epoch train time: 0:00:07.934549
elapsed time: 0:25:54.857817
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 11:32:05.458600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.01
 ---- batch: 020 ----
mean loss: 196.34
 ---- batch: 030 ----
mean loss: 191.21
 ---- batch: 040 ----
mean loss: 193.19
 ---- batch: 050 ----
mean loss: 196.04
train mean loss: 195.60
epoch train time: 0:00:07.930413
elapsed time: 0:26:02.789467
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 11:32:13.390235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.69
 ---- batch: 020 ----
mean loss: 196.53
 ---- batch: 030 ----
mean loss: 192.53
 ---- batch: 040 ----
mean loss: 193.29
 ---- batch: 050 ----
mean loss: 197.68
train mean loss: 195.04
epoch train time: 0:00:07.917467
elapsed time: 0:26:10.708182
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 11:32:21.308944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.01
 ---- batch: 020 ----
mean loss: 195.28
 ---- batch: 030 ----
mean loss: 190.02
 ---- batch: 040 ----
mean loss: 193.01
 ---- batch: 050 ----
mean loss: 194.47
train mean loss: 194.02
epoch train time: 0:00:07.945369
elapsed time: 0:26:18.654749
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 11:32:29.255531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.77
 ---- batch: 020 ----
mean loss: 186.34
 ---- batch: 030 ----
mean loss: 200.50
 ---- batch: 040 ----
mean loss: 193.59
 ---- batch: 050 ----
mean loss: 194.23
train mean loss: 193.54
epoch train time: 0:00:07.949503
elapsed time: 0:26:26.605428
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 11:32:37.206214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.48
 ---- batch: 020 ----
mean loss: 194.98
 ---- batch: 030 ----
mean loss: 202.52
 ---- batch: 040 ----
mean loss: 190.96
 ---- batch: 050 ----
mean loss: 192.66
train mean loss: 194.07
epoch train time: 0:00:07.942690
elapsed time: 0:26:34.549389
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 11:32:45.150132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.83
 ---- batch: 020 ----
mean loss: 193.30
 ---- batch: 030 ----
mean loss: 197.21
 ---- batch: 040 ----
mean loss: 200.96
 ---- batch: 050 ----
mean loss: 190.72
train mean loss: 193.79
epoch train time: 0:00:07.932561
elapsed time: 0:26:42.483141
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 11:32:53.083922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.56
 ---- batch: 020 ----
mean loss: 196.69
 ---- batch: 030 ----
mean loss: 191.57
 ---- batch: 040 ----
mean loss: 191.92
 ---- batch: 050 ----
mean loss: 195.35
train mean loss: 193.23
epoch train time: 0:00:07.939038
elapsed time: 0:26:50.423514
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 11:33:01.024435
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.77
 ---- batch: 020 ----
mean loss: 187.67
 ---- batch: 030 ----
mean loss: 187.82
 ---- batch: 040 ----
mean loss: 188.85
 ---- batch: 050 ----
mean loss: 193.40
train mean loss: 190.06
epoch train time: 0:00:07.931755
elapsed time: 0:26:58.356951
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 11:33:08.957407
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.10
 ---- batch: 020 ----
mean loss: 189.31
 ---- batch: 030 ----
mean loss: 186.40
 ---- batch: 040 ----
mean loss: 181.41
 ---- batch: 050 ----
mean loss: 190.89
train mean loss: 189.62
epoch train time: 0:00:07.930652
elapsed time: 0:27:06.288591
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 11:33:16.889370
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.32
 ---- batch: 020 ----
mean loss: 190.12
 ---- batch: 030 ----
mean loss: 184.89
 ---- batch: 040 ----
mean loss: 191.37
 ---- batch: 050 ----
mean loss: 194.24
train mean loss: 190.27
epoch train time: 0:00:07.977475
elapsed time: 0:27:14.267252
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 11:33:24.868019
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.89
 ---- batch: 020 ----
mean loss: 187.56
 ---- batch: 030 ----
mean loss: 187.06
 ---- batch: 040 ----
mean loss: 189.78
 ---- batch: 050 ----
mean loss: 188.99
train mean loss: 189.84
epoch train time: 0:00:07.942879
elapsed time: 0:27:22.211271
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 11:33:32.812042
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.50
 ---- batch: 020 ----
mean loss: 195.78
 ---- batch: 030 ----
mean loss: 187.95
 ---- batch: 040 ----
mean loss: 189.60
 ---- batch: 050 ----
mean loss: 196.54
train mean loss: 189.65
epoch train time: 0:00:07.948263
elapsed time: 0:27:30.160682
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 11:33:40.761455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.28
 ---- batch: 020 ----
mean loss: 190.96
 ---- batch: 030 ----
mean loss: 194.55
 ---- batch: 040 ----
mean loss: 194.25
 ---- batch: 050 ----
mean loss: 191.45
train mean loss: 189.35
epoch train time: 0:00:07.935233
elapsed time: 0:27:38.097109
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 11:33:48.697888
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.49
 ---- batch: 020 ----
mean loss: 187.62
 ---- batch: 030 ----
mean loss: 188.71
 ---- batch: 040 ----
mean loss: 196.11
 ---- batch: 050 ----
mean loss: 185.37
train mean loss: 189.93
epoch train time: 0:00:07.942796
elapsed time: 0:27:46.041078
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 11:33:56.641871
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.05
 ---- batch: 020 ----
mean loss: 193.22
 ---- batch: 030 ----
mean loss: 193.85
 ---- batch: 040 ----
mean loss: 179.99
 ---- batch: 050 ----
mean loss: 192.14
train mean loss: 189.74
epoch train time: 0:00:07.938898
elapsed time: 0:27:53.981131
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 11:34:04.581954
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.62
 ---- batch: 020 ----
mean loss: 188.98
 ---- batch: 030 ----
mean loss: 183.84
 ---- batch: 040 ----
mean loss: 196.03
 ---- batch: 050 ----
mean loss: 189.04
train mean loss: 189.76
epoch train time: 0:00:07.930854
elapsed time: 0:28:01.913389
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 11:34:12.514165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.66
 ---- batch: 020 ----
mean loss: 196.76
 ---- batch: 030 ----
mean loss: 188.42
 ---- batch: 040 ----
mean loss: 189.12
 ---- batch: 050 ----
mean loss: 189.34
train mean loss: 189.64
epoch train time: 0:00:07.938063
elapsed time: 0:28:09.852727
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 11:34:20.453535
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.46
 ---- batch: 020 ----
mean loss: 196.34
 ---- batch: 030 ----
mean loss: 190.46
 ---- batch: 040 ----
mean loss: 181.12
 ---- batch: 050 ----
mean loss: 193.24
train mean loss: 189.88
epoch train time: 0:00:07.952006
elapsed time: 0:28:17.806050
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 11:34:28.406812
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.88
 ---- batch: 020 ----
mean loss: 188.89
 ---- batch: 030 ----
mean loss: 191.93
 ---- batch: 040 ----
mean loss: 185.24
 ---- batch: 050 ----
mean loss: 189.00
train mean loss: 189.17
epoch train time: 0:00:07.925473
elapsed time: 0:28:25.732672
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 11:34:36.333442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.83
 ---- batch: 020 ----
mean loss: 186.78
 ---- batch: 030 ----
mean loss: 191.98
 ---- batch: 040 ----
mean loss: 181.22
 ---- batch: 050 ----
mean loss: 190.89
train mean loss: 189.63
epoch train time: 0:00:07.926146
elapsed time: 0:28:33.659982
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 11:34:44.260784
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.76
 ---- batch: 020 ----
mean loss: 191.01
 ---- batch: 030 ----
mean loss: 186.60
 ---- batch: 040 ----
mean loss: 183.94
 ---- batch: 050 ----
mean loss: 188.02
train mean loss: 189.46
epoch train time: 0:00:07.939237
elapsed time: 0:28:41.600374
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 11:34:52.201015
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.40
 ---- batch: 020 ----
mean loss: 185.87
 ---- batch: 030 ----
mean loss: 182.55
 ---- batch: 040 ----
mean loss: 202.65
 ---- batch: 050 ----
mean loss: 191.11
train mean loss: 189.74
epoch train time: 0:00:07.940737
elapsed time: 0:28:49.542202
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 11:35:00.142973
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.52
 ---- batch: 020 ----
mean loss: 193.04
 ---- batch: 030 ----
mean loss: 193.31
 ---- batch: 040 ----
mean loss: 191.75
 ---- batch: 050 ----
mean loss: 183.81
train mean loss: 189.49
epoch train time: 0:00:07.933960
elapsed time: 0:28:57.477576
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 11:35:08.078359
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.68
 ---- batch: 020 ----
mean loss: 190.10
 ---- batch: 030 ----
mean loss: 194.19
 ---- batch: 040 ----
mean loss: 190.76
 ---- batch: 050 ----
mean loss: 182.24
train mean loss: 190.28
epoch train time: 0:00:07.957964
elapsed time: 0:29:05.436701
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 11:35:16.037455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.20
 ---- batch: 020 ----
mean loss: 181.29
 ---- batch: 030 ----
mean loss: 191.30
 ---- batch: 040 ----
mean loss: 183.35
 ---- batch: 050 ----
mean loss: 188.31
train mean loss: 189.72
epoch train time: 0:00:07.960367
elapsed time: 0:29:13.398241
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 11:35:23.999002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.34
 ---- batch: 020 ----
mean loss: 184.37
 ---- batch: 030 ----
mean loss: 187.61
 ---- batch: 040 ----
mean loss: 190.09
 ---- batch: 050 ----
mean loss: 190.64
train mean loss: 189.80
epoch train time: 0:00:07.959310
elapsed time: 0:29:21.358827
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 11:35:31.959528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.62
 ---- batch: 020 ----
mean loss: 195.61
 ---- batch: 030 ----
mean loss: 179.66
 ---- batch: 040 ----
mean loss: 196.40
 ---- batch: 050 ----
mean loss: 185.14
train mean loss: 188.20
epoch train time: 0:00:07.904790
elapsed time: 0:29:29.264740
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 11:35:39.865510
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.11
 ---- batch: 020 ----
mean loss: 194.98
 ---- batch: 030 ----
mean loss: 193.08
 ---- batch: 040 ----
mean loss: 189.46
 ---- batch: 050 ----
mean loss: 199.14
train mean loss: 189.05
epoch train time: 0:00:07.912870
elapsed time: 0:29:37.178816
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 11:35:47.779591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.09
 ---- batch: 020 ----
mean loss: 195.08
 ---- batch: 030 ----
mean loss: 184.01
 ---- batch: 040 ----
mean loss: 187.65
 ---- batch: 050 ----
mean loss: 194.04
train mean loss: 188.94
epoch train time: 0:00:07.954313
elapsed time: 0:29:45.134294
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 11:35:55.735106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.68
 ---- batch: 020 ----
mean loss: 185.60
 ---- batch: 030 ----
mean loss: 189.41
 ---- batch: 040 ----
mean loss: 192.24
 ---- batch: 050 ----
mean loss: 189.76
train mean loss: 189.57
epoch train time: 0:00:07.954888
elapsed time: 0:29:53.090378
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 11:36:03.691142
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.10
 ---- batch: 020 ----
mean loss: 195.59
 ---- batch: 030 ----
mean loss: 192.94
 ---- batch: 040 ----
mean loss: 184.94
 ---- batch: 050 ----
mean loss: 189.94
train mean loss: 189.01
epoch train time: 0:00:07.917226
elapsed time: 0:30:01.008756
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 11:36:11.609502
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.92
 ---- batch: 020 ----
mean loss: 189.43
 ---- batch: 030 ----
mean loss: 186.63
 ---- batch: 040 ----
mean loss: 190.81
 ---- batch: 050 ----
mean loss: 181.71
train mean loss: 187.88
epoch train time: 0:00:07.906461
elapsed time: 0:30:08.916427
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 11:36:19.517212
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.30
 ---- batch: 020 ----
mean loss: 186.24
 ---- batch: 030 ----
mean loss: 190.31
 ---- batch: 040 ----
mean loss: 185.21
 ---- batch: 050 ----
mean loss: 191.19
train mean loss: 188.30
epoch train time: 0:00:07.921682
elapsed time: 0:30:16.839392
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 11:36:27.440180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.57
 ---- batch: 020 ----
mean loss: 181.96
 ---- batch: 030 ----
mean loss: 191.58
 ---- batch: 040 ----
mean loss: 192.98
 ---- batch: 050 ----
mean loss: 184.18
train mean loss: 189.28
epoch train time: 0:00:07.932049
elapsed time: 0:30:24.772637
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 11:36:35.373411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.90
 ---- batch: 020 ----
mean loss: 191.03
 ---- batch: 030 ----
mean loss: 188.10
 ---- batch: 040 ----
mean loss: 189.80
 ---- batch: 050 ----
mean loss: 189.06
train mean loss: 189.47
epoch train time: 0:00:07.922983
elapsed time: 0:30:32.696905
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 11:36:43.297657
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.35
 ---- batch: 020 ----
mean loss: 191.71
 ---- batch: 030 ----
mean loss: 191.59
 ---- batch: 040 ----
mean loss: 177.05
 ---- batch: 050 ----
mean loss: 192.96
train mean loss: 189.23
epoch train time: 0:00:07.913231
elapsed time: 0:30:40.611268
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 11:36:51.212060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.75
 ---- batch: 020 ----
mean loss: 186.76
 ---- batch: 030 ----
mean loss: 185.70
 ---- batch: 040 ----
mean loss: 193.42
 ---- batch: 050 ----
mean loss: 194.78
train mean loss: 189.00
epoch train time: 0:00:07.983742
elapsed time: 0:30:48.596195
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 11:36:59.196952
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.50
 ---- batch: 020 ----
mean loss: 189.47
 ---- batch: 030 ----
mean loss: 191.32
 ---- batch: 040 ----
mean loss: 182.84
 ---- batch: 050 ----
mean loss: 193.83
train mean loss: 189.59
epoch train time: 0:00:07.936932
elapsed time: 0:30:56.534326
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 11:37:07.135078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.37
 ---- batch: 020 ----
mean loss: 187.33
 ---- batch: 030 ----
mean loss: 197.19
 ---- batch: 040 ----
mean loss: 186.38
 ---- batch: 050 ----
mean loss: 181.52
train mean loss: 188.48
epoch train time: 0:00:07.920501
elapsed time: 0:31:04.455953
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 11:37:15.056717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.84
 ---- batch: 020 ----
mean loss: 185.66
 ---- batch: 030 ----
mean loss: 183.23
 ---- batch: 040 ----
mean loss: 188.88
 ---- batch: 050 ----
mean loss: 188.60
train mean loss: 188.40
epoch train time: 0:00:07.895309
elapsed time: 0:31:12.352760
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 11:37:22.953209
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.43
 ---- batch: 020 ----
mean loss: 185.16
 ---- batch: 030 ----
mean loss: 190.25
 ---- batch: 040 ----
mean loss: 186.83
 ---- batch: 050 ----
mean loss: 193.51
train mean loss: 188.37
epoch train time: 0:00:07.935269
elapsed time: 0:31:20.288865
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 11:37:30.889657
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.51
 ---- batch: 020 ----
mean loss: 189.81
 ---- batch: 030 ----
mean loss: 188.66
 ---- batch: 040 ----
mean loss: 185.97
 ---- batch: 050 ----
mean loss: 182.34
train mean loss: 188.28
epoch train time: 0:00:07.938991
elapsed time: 0:31:28.229393
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 11:37:38.830248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.09
 ---- batch: 020 ----
mean loss: 185.64
 ---- batch: 030 ----
mean loss: 181.56
 ---- batch: 040 ----
mean loss: 194.11
 ---- batch: 050 ----
mean loss: 195.28
train mean loss: 188.95
epoch train time: 0:00:07.927268
elapsed time: 0:31:36.157969
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 11:37:46.758765
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.73
 ---- batch: 020 ----
mean loss: 192.98
 ---- batch: 030 ----
mean loss: 189.79
 ---- batch: 040 ----
mean loss: 193.49
 ---- batch: 050 ----
mean loss: 178.79
train mean loss: 188.81
epoch train time: 0:00:07.943820
elapsed time: 0:31:44.103061
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 11:37:54.703845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.35
 ---- batch: 020 ----
mean loss: 185.70
 ---- batch: 030 ----
mean loss: 195.65
 ---- batch: 040 ----
mean loss: 188.75
 ---- batch: 050 ----
mean loss: 189.04
train mean loss: 188.43
epoch train time: 0:00:07.963211
elapsed time: 0:31:52.067538
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 11:38:02.668291
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.11
 ---- batch: 020 ----
mean loss: 190.47
 ---- batch: 030 ----
mean loss: 191.13
 ---- batch: 040 ----
mean loss: 183.69
 ---- batch: 050 ----
mean loss: 195.20
train mean loss: 188.81
epoch train time: 0:00:07.937022
elapsed time: 0:32:00.005756
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 11:38:10.606688
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.06
 ---- batch: 020 ----
mean loss: 180.95
 ---- batch: 030 ----
mean loss: 183.39
 ---- batch: 040 ----
mean loss: 192.36
 ---- batch: 050 ----
mean loss: 188.03
train mean loss: 188.69
epoch train time: 0:00:07.966148
elapsed time: 0:32:07.973328
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 11:38:18.574092
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.42
 ---- batch: 020 ----
mean loss: 191.90
 ---- batch: 030 ----
mean loss: 191.95
 ---- batch: 040 ----
mean loss: 198.23
 ---- batch: 050 ----
mean loss: 180.85
train mean loss: 188.27
epoch train time: 0:00:07.983597
elapsed time: 0:32:15.958197
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 11:38:26.558963
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.50
 ---- batch: 020 ----
mean loss: 189.70
 ---- batch: 030 ----
mean loss: 182.26
 ---- batch: 040 ----
mean loss: 188.38
 ---- batch: 050 ----
mean loss: 186.42
train mean loss: 188.13
epoch train time: 0:00:07.934898
elapsed time: 0:32:23.894209
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 11:38:34.494955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.14
 ---- batch: 020 ----
mean loss: 191.84
 ---- batch: 030 ----
mean loss: 183.19
 ---- batch: 040 ----
mean loss: 187.00
 ---- batch: 050 ----
mean loss: 187.26
train mean loss: 188.89
epoch train time: 0:00:07.955123
elapsed time: 0:32:31.850494
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 11:38:42.451391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.59
 ---- batch: 020 ----
mean loss: 184.01
 ---- batch: 030 ----
mean loss: 184.69
 ---- batch: 040 ----
mean loss: 190.68
 ---- batch: 050 ----
mean loss: 196.48
train mean loss: 189.21
epoch train time: 0:00:07.939558
elapsed time: 0:32:39.791346
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 11:38:50.392125
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.45
 ---- batch: 020 ----
mean loss: 189.26
 ---- batch: 030 ----
mean loss: 185.66
 ---- batch: 040 ----
mean loss: 191.15
 ---- batch: 050 ----
mean loss: 192.90
train mean loss: 188.16
epoch train time: 0:00:07.916132
elapsed time: 0:32:47.708715
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 11:38:58.309470
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.87
 ---- batch: 020 ----
mean loss: 190.36
 ---- batch: 030 ----
mean loss: 191.44
 ---- batch: 040 ----
mean loss: 191.11
 ---- batch: 050 ----
mean loss: 186.82
train mean loss: 188.24
epoch train time: 0:00:07.941454
elapsed time: 0:32:55.651500
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 11:39:06.252280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.99
 ---- batch: 020 ----
mean loss: 182.97
 ---- batch: 030 ----
mean loss: 188.62
 ---- batch: 040 ----
mean loss: 189.71
 ---- batch: 050 ----
mean loss: 197.31
train mean loss: 188.80
epoch train time: 0:00:07.945781
elapsed time: 0:33:03.598590
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 11:39:14.199395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.03
 ---- batch: 020 ----
mean loss: 188.81
 ---- batch: 030 ----
mean loss: 186.59
 ---- batch: 040 ----
mean loss: 192.05
 ---- batch: 050 ----
mean loss: 182.96
train mean loss: 187.94
epoch train time: 0:00:07.972712
elapsed time: 0:33:11.572538
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 11:39:22.173303
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.33
 ---- batch: 020 ----
mean loss: 185.97
 ---- batch: 030 ----
mean loss: 195.09
 ---- batch: 040 ----
mean loss: 184.30
 ---- batch: 050 ----
mean loss: 188.49
train mean loss: 188.32
epoch train time: 0:00:07.935898
elapsed time: 0:33:19.518478
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_1/checkpoint.pth.tar
**** end time: 2019-09-26 11:39:30.118890 ****
