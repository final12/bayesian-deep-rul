Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_2', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 15743
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-27 00:07:06.399731 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 16, 24]             100
              Tanh-2           [-1, 10, 16, 24]               0
            Conv2d-3           [-1, 10, 15, 24]           1,000
              Tanh-4           [-1, 10, 15, 24]               0
            Conv2d-5           [-1, 10, 16, 24]           1,000
              Tanh-6           [-1, 10, 16, 24]               0
            Conv2d-7           [-1, 10, 15, 24]           1,000
              Tanh-8           [-1, 10, 15, 24]               0
            Conv2d-9            [-1, 1, 15, 24]              30
             Tanh-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
          Dropout-12                  [-1, 360]               0
           Linear-13                  [-1, 100]          36,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 39,230
Trainable params: 39,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 00:07:06.408876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4685.63
 ---- batch: 020 ----
mean loss: 2850.89
 ---- batch: 030 ----
mean loss: 1478.06
 ---- batch: 040 ----
mean loss: 1342.17
 ---- batch: 050 ----
mean loss: 1196.65
 ---- batch: 060 ----
mean loss: 1119.82
 ---- batch: 070 ----
mean loss: 1086.54
 ---- batch: 080 ----
mean loss: 1050.70
 ---- batch: 090 ----
mean loss: 999.79
 ---- batch: 100 ----
mean loss: 977.64
 ---- batch: 110 ----
mean loss: 944.12
train mean loss: 1593.99
epoch train time: 0:00:34.631850
elapsed time: 0:00:34.643350
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 00:07:41.043121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.37
 ---- batch: 020 ----
mean loss: 902.01
 ---- batch: 030 ----
mean loss: 847.59
 ---- batch: 040 ----
mean loss: 853.58
 ---- batch: 050 ----
mean loss: 820.70
 ---- batch: 060 ----
mean loss: 801.63
 ---- batch: 070 ----
mean loss: 815.14
 ---- batch: 080 ----
mean loss: 790.19
 ---- batch: 090 ----
mean loss: 802.22
 ---- batch: 100 ----
mean loss: 790.31
 ---- batch: 110 ----
mean loss: 796.14
train mean loss: 828.10
epoch train time: 0:00:02.203400
elapsed time: 0:00:36.846889
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 00:07:43.246681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 750.43
 ---- batch: 020 ----
mean loss: 754.72
 ---- batch: 030 ----
mean loss: 717.52
 ---- batch: 040 ----
mean loss: 727.61
 ---- batch: 050 ----
mean loss: 722.37
 ---- batch: 060 ----
mean loss: 708.30
 ---- batch: 070 ----
mean loss: 734.20
 ---- batch: 080 ----
mean loss: 713.75
 ---- batch: 090 ----
mean loss: 697.03
 ---- batch: 100 ----
mean loss: 689.87
 ---- batch: 110 ----
mean loss: 699.02
train mean loss: 717.33
epoch train time: 0:00:02.135953
elapsed time: 0:00:38.983005
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 00:07:45.382792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.28
 ---- batch: 020 ----
mean loss: 681.87
 ---- batch: 030 ----
mean loss: 682.48
 ---- batch: 040 ----
mean loss: 666.92
 ---- batch: 050 ----
mean loss: 659.87
 ---- batch: 060 ----
mean loss: 658.12
 ---- batch: 070 ----
mean loss: 668.27
 ---- batch: 080 ----
mean loss: 648.85
 ---- batch: 090 ----
mean loss: 652.26
 ---- batch: 100 ----
mean loss: 652.96
 ---- batch: 110 ----
mean loss: 627.39
train mean loss: 661.24
epoch train time: 0:00:02.144928
elapsed time: 0:00:41.128085
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 00:07:47.527868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.75
 ---- batch: 020 ----
mean loss: 641.31
 ---- batch: 030 ----
mean loss: 636.86
 ---- batch: 040 ----
mean loss: 657.18
 ---- batch: 050 ----
mean loss: 624.55
 ---- batch: 060 ----
mean loss: 621.11
 ---- batch: 070 ----
mean loss: 635.84
 ---- batch: 080 ----
mean loss: 633.97
 ---- batch: 090 ----
mean loss: 620.72
 ---- batch: 100 ----
mean loss: 618.82
 ---- batch: 110 ----
mean loss: 632.86
train mean loss: 631.49
epoch train time: 0:00:02.148063
elapsed time: 0:00:43.276316
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 00:07:49.676100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 615.78
 ---- batch: 020 ----
mean loss: 599.81
 ---- batch: 030 ----
mean loss: 604.04
 ---- batch: 040 ----
mean loss: 597.60
 ---- batch: 050 ----
mean loss: 600.88
 ---- batch: 060 ----
mean loss: 597.25
 ---- batch: 070 ----
mean loss: 601.36
 ---- batch: 080 ----
mean loss: 603.74
 ---- batch: 090 ----
mean loss: 590.94
 ---- batch: 100 ----
mean loss: 599.50
 ---- batch: 110 ----
mean loss: 601.10
train mean loss: 600.23
epoch train time: 0:00:02.191375
elapsed time: 0:00:45.467869
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 00:07:51.867664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 584.42
 ---- batch: 020 ----
mean loss: 590.54
 ---- batch: 030 ----
mean loss: 573.32
 ---- batch: 040 ----
mean loss: 571.88
 ---- batch: 050 ----
mean loss: 560.96
 ---- batch: 060 ----
mean loss: 573.90
 ---- batch: 070 ----
mean loss: 582.67
 ---- batch: 080 ----
mean loss: 562.72
 ---- batch: 090 ----
mean loss: 582.50
 ---- batch: 100 ----
mean loss: 585.27
 ---- batch: 110 ----
mean loss: 575.85
train mean loss: 576.03
epoch train time: 0:00:02.190588
elapsed time: 0:00:47.658638
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 00:07:54.058448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 560.01
 ---- batch: 020 ----
mean loss: 557.35
 ---- batch: 030 ----
mean loss: 571.28
 ---- batch: 040 ----
mean loss: 555.84
 ---- batch: 050 ----
mean loss: 565.96
 ---- batch: 060 ----
mean loss: 563.93
 ---- batch: 070 ----
mean loss: 537.49
 ---- batch: 080 ----
mean loss: 561.42
 ---- batch: 090 ----
mean loss: 543.23
 ---- batch: 100 ----
mean loss: 560.55
 ---- batch: 110 ----
mean loss: 561.28
train mean loss: 557.33
epoch train time: 0:00:02.162636
elapsed time: 0:00:49.821453
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 00:07:56.221238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 555.95
 ---- batch: 020 ----
mean loss: 546.64
 ---- batch: 030 ----
mean loss: 542.89
 ---- batch: 040 ----
mean loss: 544.81
 ---- batch: 050 ----
mean loss: 552.75
 ---- batch: 060 ----
mean loss: 536.33
 ---- batch: 070 ----
mean loss: 530.57
 ---- batch: 080 ----
mean loss: 525.77
 ---- batch: 090 ----
mean loss: 537.91
 ---- batch: 100 ----
mean loss: 552.81
 ---- batch: 110 ----
mean loss: 547.64
train mean loss: 542.79
epoch train time: 0:00:02.145173
elapsed time: 0:00:51.966782
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 00:07:58.366566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.33
 ---- batch: 020 ----
mean loss: 546.84
 ---- batch: 030 ----
mean loss: 531.07
 ---- batch: 040 ----
mean loss: 531.02
 ---- batch: 050 ----
mean loss: 534.19
 ---- batch: 060 ----
mean loss: 532.70
 ---- batch: 070 ----
mean loss: 518.98
 ---- batch: 080 ----
mean loss: 548.65
 ---- batch: 090 ----
mean loss: 523.43
 ---- batch: 100 ----
mean loss: 522.71
 ---- batch: 110 ----
mean loss: 547.12
train mean loss: 533.64
epoch train time: 0:00:02.148036
elapsed time: 0:00:54.114970
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 00:08:00.514773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.73
 ---- batch: 020 ----
mean loss: 532.47
 ---- batch: 030 ----
mean loss: 524.97
 ---- batch: 040 ----
mean loss: 536.40
 ---- batch: 050 ----
mean loss: 518.64
 ---- batch: 060 ----
mean loss: 544.83
 ---- batch: 070 ----
mean loss: 538.87
 ---- batch: 080 ----
mean loss: 533.35
 ---- batch: 090 ----
mean loss: 538.35
 ---- batch: 100 ----
mean loss: 537.60
 ---- batch: 110 ----
mean loss: 535.00
train mean loss: 534.74
epoch train time: 0:00:02.136176
elapsed time: 0:00:56.251317
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 00:08:02.651103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 531.22
 ---- batch: 020 ----
mean loss: 515.16
 ---- batch: 030 ----
mean loss: 548.96
 ---- batch: 040 ----
mean loss: 536.04
 ---- batch: 050 ----
mean loss: 526.96
 ---- batch: 060 ----
mean loss: 519.05
 ---- batch: 070 ----
mean loss: 521.90
 ---- batch: 080 ----
mean loss: 518.76
 ---- batch: 090 ----
mean loss: 530.82
 ---- batch: 100 ----
mean loss: 517.11
 ---- batch: 110 ----
mean loss: 513.14
train mean loss: 525.24
epoch train time: 0:00:02.140999
elapsed time: 0:00:58.392464
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 00:08:04.792245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.94
 ---- batch: 020 ----
mean loss: 521.81
 ---- batch: 030 ----
mean loss: 517.41
 ---- batch: 040 ----
mean loss: 528.94
 ---- batch: 050 ----
mean loss: 520.11
 ---- batch: 060 ----
mean loss: 524.96
 ---- batch: 070 ----
mean loss: 526.98
 ---- batch: 080 ----
mean loss: 517.80
 ---- batch: 090 ----
mean loss: 515.29
 ---- batch: 100 ----
mean loss: 522.58
 ---- batch: 110 ----
mean loss: 519.90
train mean loss: 522.35
epoch train time: 0:00:02.138721
elapsed time: 0:01:00.531330
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 00:08:06.931134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 510.81
 ---- batch: 020 ----
mean loss: 526.30
 ---- batch: 030 ----
mean loss: 522.43
 ---- batch: 040 ----
mean loss: 518.25
 ---- batch: 050 ----
mean loss: 512.28
 ---- batch: 060 ----
mean loss: 517.60
 ---- batch: 070 ----
mean loss: 523.69
 ---- batch: 080 ----
mean loss: 514.20
 ---- batch: 090 ----
mean loss: 524.11
 ---- batch: 100 ----
mean loss: 509.51
 ---- batch: 110 ----
mean loss: 516.84
train mean loss: 518.26
epoch train time: 0:00:02.143954
elapsed time: 0:01:02.675466
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 00:08:09.075251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.67
 ---- batch: 020 ----
mean loss: 517.98
 ---- batch: 030 ----
mean loss: 523.52
 ---- batch: 040 ----
mean loss: 514.56
 ---- batch: 050 ----
mean loss: 531.14
 ---- batch: 060 ----
mean loss: 497.67
 ---- batch: 070 ----
mean loss: 508.23
 ---- batch: 080 ----
mean loss: 519.95
 ---- batch: 090 ----
mean loss: 520.46
 ---- batch: 100 ----
mean loss: 515.05
 ---- batch: 110 ----
mean loss: 539.98
train mean loss: 518.56
epoch train time: 0:00:02.133986
elapsed time: 0:01:04.809601
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 00:08:11.209387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.28
 ---- batch: 020 ----
mean loss: 530.01
 ---- batch: 030 ----
mean loss: 510.14
 ---- batch: 040 ----
mean loss: 503.35
 ---- batch: 050 ----
mean loss: 521.84
 ---- batch: 060 ----
mean loss: 538.59
 ---- batch: 070 ----
mean loss: 529.12
 ---- batch: 080 ----
mean loss: 505.92
 ---- batch: 090 ----
mean loss: 517.47
 ---- batch: 100 ----
mean loss: 514.19
 ---- batch: 110 ----
mean loss: 513.68
train mean loss: 518.67
epoch train time: 0:00:02.134616
elapsed time: 0:01:06.944383
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 00:08:13.344167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.47
 ---- batch: 020 ----
mean loss: 505.96
 ---- batch: 030 ----
mean loss: 517.36
 ---- batch: 040 ----
mean loss: 521.44
 ---- batch: 050 ----
mean loss: 524.58
 ---- batch: 060 ----
mean loss: 520.18
 ---- batch: 070 ----
mean loss: 531.29
 ---- batch: 080 ----
mean loss: 527.32
 ---- batch: 090 ----
mean loss: 511.83
 ---- batch: 100 ----
mean loss: 512.54
 ---- batch: 110 ----
mean loss: 521.92
train mean loss: 517.37
epoch train time: 0:00:02.137370
elapsed time: 0:01:09.081929
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 00:08:15.481716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.62
 ---- batch: 020 ----
mean loss: 525.35
 ---- batch: 030 ----
mean loss: 498.39
 ---- batch: 040 ----
mean loss: 518.32
 ---- batch: 050 ----
mean loss: 518.22
 ---- batch: 060 ----
mean loss: 505.28
 ---- batch: 070 ----
mean loss: 513.25
 ---- batch: 080 ----
mean loss: 512.72
 ---- batch: 090 ----
mean loss: 512.49
 ---- batch: 100 ----
mean loss: 524.43
 ---- batch: 110 ----
mean loss: 521.86
train mean loss: 512.94
epoch train time: 0:00:02.143299
elapsed time: 0:01:11.225379
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 00:08:17.625193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.67
 ---- batch: 020 ----
mean loss: 525.86
 ---- batch: 030 ----
mean loss: 510.63
 ---- batch: 040 ----
mean loss: 496.71
 ---- batch: 050 ----
mean loss: 501.49
 ---- batch: 060 ----
mean loss: 515.21
 ---- batch: 070 ----
mean loss: 512.82
 ---- batch: 080 ----
mean loss: 517.02
 ---- batch: 090 ----
mean loss: 504.27
 ---- batch: 100 ----
mean loss: 526.35
 ---- batch: 110 ----
mean loss: 513.41
train mean loss: 512.03
epoch train time: 0:00:02.138147
elapsed time: 0:01:13.363703
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 00:08:19.763484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 523.90
 ---- batch: 020 ----
mean loss: 518.69
 ---- batch: 030 ----
mean loss: 524.26
 ---- batch: 040 ----
mean loss: 517.14
 ---- batch: 050 ----
mean loss: 508.43
 ---- batch: 060 ----
mean loss: 512.40
 ---- batch: 070 ----
mean loss: 506.40
 ---- batch: 080 ----
mean loss: 514.64
 ---- batch: 090 ----
mean loss: 520.25
 ---- batch: 100 ----
mean loss: 504.51
 ---- batch: 110 ----
mean loss: 497.66
train mean loss: 513.70
epoch train time: 0:00:02.136273
elapsed time: 0:01:15.500118
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 00:08:21.899982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.32
 ---- batch: 020 ----
mean loss: 529.09
 ---- batch: 030 ----
mean loss: 522.85
 ---- batch: 040 ----
mean loss: 512.80
 ---- batch: 050 ----
mean loss: 508.44
 ---- batch: 060 ----
mean loss: 504.83
 ---- batch: 070 ----
mean loss: 515.31
 ---- batch: 080 ----
mean loss: 507.58
 ---- batch: 090 ----
mean loss: 497.59
 ---- batch: 100 ----
mean loss: 510.83
 ---- batch: 110 ----
mean loss: 507.61
train mean loss: 511.55
epoch train time: 0:00:02.137708
elapsed time: 0:01:17.638058
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 00:08:24.037841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 522.77
 ---- batch: 020 ----
mean loss: 508.69
 ---- batch: 030 ----
mean loss: 507.09
 ---- batch: 040 ----
mean loss: 511.92
 ---- batch: 050 ----
mean loss: 498.91
 ---- batch: 060 ----
mean loss: 510.51
 ---- batch: 070 ----
mean loss: 508.72
 ---- batch: 080 ----
mean loss: 494.11
 ---- batch: 090 ----
mean loss: 507.57
 ---- batch: 100 ----
mean loss: 492.76
 ---- batch: 110 ----
mean loss: 503.11
train mean loss: 506.21
epoch train time: 0:00:02.139318
elapsed time: 0:01:19.777527
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 00:08:26.177311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.32
 ---- batch: 020 ----
mean loss: 490.75
 ---- batch: 030 ----
mean loss: 492.85
 ---- batch: 040 ----
mean loss: 503.03
 ---- batch: 050 ----
mean loss: 508.43
 ---- batch: 060 ----
mean loss: 503.47
 ---- batch: 070 ----
mean loss: 521.12
 ---- batch: 080 ----
mean loss: 496.43
 ---- batch: 090 ----
mean loss: 512.54
 ---- batch: 100 ----
mean loss: 511.91
 ---- batch: 110 ----
mean loss: 500.59
train mean loss: 503.99
epoch train time: 0:00:02.139439
elapsed time: 0:01:21.917120
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 00:08:28.316902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 510.13
 ---- batch: 020 ----
mean loss: 513.57
 ---- batch: 030 ----
mean loss: 506.41
 ---- batch: 040 ----
mean loss: 504.94
 ---- batch: 050 ----
mean loss: 509.76
 ---- batch: 060 ----
mean loss: 517.20
 ---- batch: 070 ----
mean loss: 508.47
 ---- batch: 080 ----
mean loss: 509.38
 ---- batch: 090 ----
mean loss: 503.25
 ---- batch: 100 ----
mean loss: 501.42
 ---- batch: 110 ----
mean loss: 508.32
train mean loss: 508.54
epoch train time: 0:00:02.139809
elapsed time: 0:01:24.057076
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 00:08:30.456858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.43
 ---- batch: 020 ----
mean loss: 500.39
 ---- batch: 030 ----
mean loss: 498.66
 ---- batch: 040 ----
mean loss: 516.04
 ---- batch: 050 ----
mean loss: 503.00
 ---- batch: 060 ----
mean loss: 523.85
 ---- batch: 070 ----
mean loss: 488.59
 ---- batch: 080 ----
mean loss: 514.88
 ---- batch: 090 ----
mean loss: 490.46
 ---- batch: 100 ----
mean loss: 506.39
 ---- batch: 110 ----
mean loss: 506.97
train mean loss: 503.37
epoch train time: 0:00:02.136446
elapsed time: 0:01:26.193672
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 00:08:32.593457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.65
 ---- batch: 020 ----
mean loss: 504.00
 ---- batch: 030 ----
mean loss: 506.84
 ---- batch: 040 ----
mean loss: 493.90
 ---- batch: 050 ----
mean loss: 502.55
 ---- batch: 060 ----
mean loss: 490.04
 ---- batch: 070 ----
mean loss: 505.66
 ---- batch: 080 ----
mean loss: 497.13
 ---- batch: 090 ----
mean loss: 475.33
 ---- batch: 100 ----
mean loss: 503.87
 ---- batch: 110 ----
mean loss: 506.42
train mean loss: 499.25
epoch train time: 0:00:02.137223
elapsed time: 0:01:28.331058
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 00:08:34.730871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.03
 ---- batch: 020 ----
mean loss: 513.17
 ---- batch: 030 ----
mean loss: 495.48
 ---- batch: 040 ----
mean loss: 497.45
 ---- batch: 050 ----
mean loss: 498.16
 ---- batch: 060 ----
mean loss: 492.63
 ---- batch: 070 ----
mean loss: 478.00
 ---- batch: 080 ----
mean loss: 501.67
 ---- batch: 090 ----
mean loss: 501.60
 ---- batch: 100 ----
mean loss: 501.99
 ---- batch: 110 ----
mean loss: 506.72
train mean loss: 498.25
epoch train time: 0:00:02.141588
elapsed time: 0:01:30.472830
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 00:08:36.872625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.13
 ---- batch: 020 ----
mean loss: 506.21
 ---- batch: 030 ----
mean loss: 510.31
 ---- batch: 040 ----
mean loss: 505.45
 ---- batch: 050 ----
mean loss: 506.59
 ---- batch: 060 ----
mean loss: 503.89
 ---- batch: 070 ----
mean loss: 483.06
 ---- batch: 080 ----
mean loss: 488.38
 ---- batch: 090 ----
mean loss: 512.21
 ---- batch: 100 ----
mean loss: 501.07
 ---- batch: 110 ----
mean loss: 501.00
train mean loss: 501.66
epoch train time: 0:00:02.138046
elapsed time: 0:01:32.611044
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 00:08:39.010827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.71
 ---- batch: 020 ----
mean loss: 497.77
 ---- batch: 030 ----
mean loss: 491.86
 ---- batch: 040 ----
mean loss: 504.63
 ---- batch: 050 ----
mean loss: 517.73
 ---- batch: 060 ----
mean loss: 494.12
 ---- batch: 070 ----
mean loss: 504.48
 ---- batch: 080 ----
mean loss: 511.16
 ---- batch: 090 ----
mean loss: 510.85
 ---- batch: 100 ----
mean loss: 504.00
 ---- batch: 110 ----
mean loss: 510.30
train mean loss: 502.65
epoch train time: 0:00:02.142791
elapsed time: 0:01:34.754057
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 00:08:41.153856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.16
 ---- batch: 020 ----
mean loss: 504.98
 ---- batch: 030 ----
mean loss: 468.45
 ---- batch: 040 ----
mean loss: 485.04
 ---- batch: 050 ----
mean loss: 491.66
 ---- batch: 060 ----
mean loss: 487.59
 ---- batch: 070 ----
mean loss: 508.98
 ---- batch: 080 ----
mean loss: 485.23
 ---- batch: 090 ----
mean loss: 484.09
 ---- batch: 100 ----
mean loss: 507.48
 ---- batch: 110 ----
mean loss: 506.06
train mean loss: 493.91
epoch train time: 0:00:02.146724
elapsed time: 0:01:36.900948
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 00:08:43.300730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.08
 ---- batch: 020 ----
mean loss: 501.76
 ---- batch: 030 ----
mean loss: 495.40
 ---- batch: 040 ----
mean loss: 490.47
 ---- batch: 050 ----
mean loss: 490.23
 ---- batch: 060 ----
mean loss: 493.92
 ---- batch: 070 ----
mean loss: 490.71
 ---- batch: 080 ----
mean loss: 512.00
 ---- batch: 090 ----
mean loss: 505.70
 ---- batch: 100 ----
mean loss: 503.35
 ---- batch: 110 ----
mean loss: 506.84
train mean loss: 498.84
epoch train time: 0:00:02.144288
elapsed time: 0:01:39.045383
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 00:08:45.445167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.71
 ---- batch: 020 ----
mean loss: 515.92
 ---- batch: 030 ----
mean loss: 477.10
 ---- batch: 040 ----
mean loss: 495.35
 ---- batch: 050 ----
mean loss: 490.96
 ---- batch: 060 ----
mean loss: 481.87
 ---- batch: 070 ----
mean loss: 487.29
 ---- batch: 080 ----
mean loss: 492.52
 ---- batch: 090 ----
mean loss: 502.18
 ---- batch: 100 ----
mean loss: 491.70
 ---- batch: 110 ----
mean loss: 488.47
train mean loss: 493.24
epoch train time: 0:00:02.146485
elapsed time: 0:01:41.192021
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 00:08:47.591807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.05
 ---- batch: 020 ----
mean loss: 482.15
 ---- batch: 030 ----
mean loss: 504.09
 ---- batch: 040 ----
mean loss: 489.16
 ---- batch: 050 ----
mean loss: 493.43
 ---- batch: 060 ----
mean loss: 517.96
 ---- batch: 070 ----
mean loss: 494.96
 ---- batch: 080 ----
mean loss: 502.81
 ---- batch: 090 ----
mean loss: 503.81
 ---- batch: 100 ----
mean loss: 506.22
 ---- batch: 110 ----
mean loss: 491.73
train mean loss: 497.30
epoch train time: 0:00:02.139998
elapsed time: 0:01:43.332189
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 00:08:49.731992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.56
 ---- batch: 020 ----
mean loss: 507.62
 ---- batch: 030 ----
mean loss: 498.26
 ---- batch: 040 ----
mean loss: 488.82
 ---- batch: 050 ----
mean loss: 491.08
 ---- batch: 060 ----
mean loss: 477.64
 ---- batch: 070 ----
mean loss: 516.57
 ---- batch: 080 ----
mean loss: 502.46
 ---- batch: 090 ----
mean loss: 493.13
 ---- batch: 100 ----
mean loss: 495.00
 ---- batch: 110 ----
mean loss: 494.29
train mean loss: 496.39
epoch train time: 0:00:02.138924
elapsed time: 0:01:45.471297
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 00:08:51.871098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.89
 ---- batch: 020 ----
mean loss: 495.02
 ---- batch: 030 ----
mean loss: 485.83
 ---- batch: 040 ----
mean loss: 497.98
 ---- batch: 050 ----
mean loss: 486.90
 ---- batch: 060 ----
mean loss: 486.14
 ---- batch: 070 ----
mean loss: 470.41
 ---- batch: 080 ----
mean loss: 506.97
 ---- batch: 090 ----
mean loss: 497.78
 ---- batch: 100 ----
mean loss: 495.62
 ---- batch: 110 ----
mean loss: 500.06
train mean loss: 492.78
epoch train time: 0:00:02.137319
elapsed time: 0:01:47.608810
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 00:08:54.008595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.68
 ---- batch: 020 ----
mean loss: 500.21
 ---- batch: 030 ----
mean loss: 488.79
 ---- batch: 040 ----
mean loss: 508.24
 ---- batch: 050 ----
mean loss: 503.91
 ---- batch: 060 ----
mean loss: 492.86
 ---- batch: 070 ----
mean loss: 492.48
 ---- batch: 080 ----
mean loss: 500.69
 ---- batch: 090 ----
mean loss: 490.06
 ---- batch: 100 ----
mean loss: 511.83
 ---- batch: 110 ----
mean loss: 471.51
train mean loss: 497.90
epoch train time: 0:00:02.137605
elapsed time: 0:01:49.746568
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 00:08:56.146351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.18
 ---- batch: 020 ----
mean loss: 494.94
 ---- batch: 030 ----
mean loss: 498.75
 ---- batch: 040 ----
mean loss: 497.35
 ---- batch: 050 ----
mean loss: 490.62
 ---- batch: 060 ----
mean loss: 504.09
 ---- batch: 070 ----
mean loss: 489.49
 ---- batch: 080 ----
mean loss: 486.41
 ---- batch: 090 ----
mean loss: 500.16
 ---- batch: 100 ----
mean loss: 497.37
 ---- batch: 110 ----
mean loss: 493.15
train mean loss: 495.83
epoch train time: 0:00:02.138097
elapsed time: 0:01:51.884822
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 00:08:58.284604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.57
 ---- batch: 020 ----
mean loss: 493.53
 ---- batch: 030 ----
mean loss: 499.09
 ---- batch: 040 ----
mean loss: 494.90
 ---- batch: 050 ----
mean loss: 518.42
 ---- batch: 060 ----
mean loss: 491.54
 ---- batch: 070 ----
mean loss: 497.80
 ---- batch: 080 ----
mean loss: 484.78
 ---- batch: 090 ----
mean loss: 472.89
 ---- batch: 100 ----
mean loss: 492.97
 ---- batch: 110 ----
mean loss: 502.98
train mean loss: 493.50
epoch train time: 0:00:02.140195
elapsed time: 0:01:54.025178
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 00:09:00.424968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.76
 ---- batch: 020 ----
mean loss: 499.40
 ---- batch: 030 ----
mean loss: 497.46
 ---- batch: 040 ----
mean loss: 491.25
 ---- batch: 050 ----
mean loss: 493.15
 ---- batch: 060 ----
mean loss: 487.48
 ---- batch: 070 ----
mean loss: 488.74
 ---- batch: 080 ----
mean loss: 502.79
 ---- batch: 090 ----
mean loss: 492.24
 ---- batch: 100 ----
mean loss: 487.34
 ---- batch: 110 ----
mean loss: 484.18
train mean loss: 491.93
epoch train time: 0:00:02.138281
elapsed time: 0:01:56.163613
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 00:09:02.563411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.21
 ---- batch: 020 ----
mean loss: 485.48
 ---- batch: 030 ----
mean loss: 485.09
 ---- batch: 040 ----
mean loss: 484.63
 ---- batch: 050 ----
mean loss: 479.53
 ---- batch: 060 ----
mean loss: 482.80
 ---- batch: 070 ----
mean loss: 504.13
 ---- batch: 080 ----
mean loss: 513.02
 ---- batch: 090 ----
mean loss: 499.19
 ---- batch: 100 ----
mean loss: 501.91
 ---- batch: 110 ----
mean loss: 491.65
train mean loss: 492.01
epoch train time: 0:00:02.136909
elapsed time: 0:01:58.300696
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 00:09:04.700487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.93
 ---- batch: 020 ----
mean loss: 495.66
 ---- batch: 030 ----
mean loss: 484.73
 ---- batch: 040 ----
mean loss: 483.83
 ---- batch: 050 ----
mean loss: 480.77
 ---- batch: 060 ----
mean loss: 491.27
 ---- batch: 070 ----
mean loss: 489.78
 ---- batch: 080 ----
mean loss: 478.33
 ---- batch: 090 ----
mean loss: 495.88
 ---- batch: 100 ----
mean loss: 487.54
 ---- batch: 110 ----
mean loss: 501.31
train mean loss: 488.00
epoch train time: 0:00:02.140013
elapsed time: 0:02:00.440875
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 00:09:06.840660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.25
 ---- batch: 020 ----
mean loss: 482.80
 ---- batch: 030 ----
mean loss: 491.95
 ---- batch: 040 ----
mean loss: 482.63
 ---- batch: 050 ----
mean loss: 477.74
 ---- batch: 060 ----
mean loss: 495.94
 ---- batch: 070 ----
mean loss: 494.64
 ---- batch: 080 ----
mean loss: 499.28
 ---- batch: 090 ----
mean loss: 476.30
 ---- batch: 100 ----
mean loss: 500.82
 ---- batch: 110 ----
mean loss: 480.85
train mean loss: 488.12
epoch train time: 0:00:02.139504
elapsed time: 0:02:02.580544
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 00:09:08.980339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.17
 ---- batch: 020 ----
mean loss: 485.49
 ---- batch: 030 ----
mean loss: 483.55
 ---- batch: 040 ----
mean loss: 508.79
 ---- batch: 050 ----
mean loss: 497.78
 ---- batch: 060 ----
mean loss: 476.40
 ---- batch: 070 ----
mean loss: 499.93
 ---- batch: 080 ----
mean loss: 485.76
 ---- batch: 090 ----
mean loss: 492.65
 ---- batch: 100 ----
mean loss: 493.43
 ---- batch: 110 ----
mean loss: 501.76
train mean loss: 492.11
epoch train time: 0:00:02.141527
elapsed time: 0:02:04.722245
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 00:09:11.122048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.31
 ---- batch: 020 ----
mean loss: 484.02
 ---- batch: 030 ----
mean loss: 477.29
 ---- batch: 040 ----
mean loss: 498.99
 ---- batch: 050 ----
mean loss: 493.18
 ---- batch: 060 ----
mean loss: 475.17
 ---- batch: 070 ----
mean loss: 493.46
 ---- batch: 080 ----
mean loss: 490.08
 ---- batch: 090 ----
mean loss: 485.52
 ---- batch: 100 ----
mean loss: 494.07
 ---- batch: 110 ----
mean loss: 491.99
train mean loss: 488.66
epoch train time: 0:00:02.140046
elapsed time: 0:02:06.862464
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 00:09:13.262250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.77
 ---- batch: 020 ----
mean loss: 484.26
 ---- batch: 030 ----
mean loss: 474.23
 ---- batch: 040 ----
mean loss: 491.83
 ---- batch: 050 ----
mean loss: 491.65
 ---- batch: 060 ----
mean loss: 497.68
 ---- batch: 070 ----
mean loss: 492.35
 ---- batch: 080 ----
mean loss: 491.34
 ---- batch: 090 ----
mean loss: 481.96
 ---- batch: 100 ----
mean loss: 499.18
 ---- batch: 110 ----
mean loss: 480.98
train mean loss: 487.75
epoch train time: 0:00:02.137031
elapsed time: 0:02:08.999650
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 00:09:15.399434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.31
 ---- batch: 020 ----
mean loss: 492.05
 ---- batch: 030 ----
mean loss: 484.56
 ---- batch: 040 ----
mean loss: 498.09
 ---- batch: 050 ----
mean loss: 477.79
 ---- batch: 060 ----
mean loss: 479.01
 ---- batch: 070 ----
mean loss: 496.13
 ---- batch: 080 ----
mean loss: 489.36
 ---- batch: 090 ----
mean loss: 487.87
 ---- batch: 100 ----
mean loss: 487.70
 ---- batch: 110 ----
mean loss: 487.94
train mean loss: 487.20
epoch train time: 0:00:02.140415
elapsed time: 0:02:11.140212
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 00:09:17.540018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.08
 ---- batch: 020 ----
mean loss: 496.40
 ---- batch: 030 ----
mean loss: 496.49
 ---- batch: 040 ----
mean loss: 493.35
 ---- batch: 050 ----
mean loss: 491.34
 ---- batch: 060 ----
mean loss: 477.20
 ---- batch: 070 ----
mean loss: 492.78
 ---- batch: 080 ----
mean loss: 475.89
 ---- batch: 090 ----
mean loss: 483.17
 ---- batch: 100 ----
mean loss: 486.42
 ---- batch: 110 ----
mean loss: 484.59
train mean loss: 487.75
epoch train time: 0:00:02.140637
elapsed time: 0:02:13.281055
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 00:09:19.680842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.28
 ---- batch: 020 ----
mean loss: 498.01
 ---- batch: 030 ----
mean loss: 466.86
 ---- batch: 040 ----
mean loss: 499.91
 ---- batch: 050 ----
mean loss: 491.67
 ---- batch: 060 ----
mean loss: 484.05
 ---- batch: 070 ----
mean loss: 475.16
 ---- batch: 080 ----
mean loss: 476.41
 ---- batch: 090 ----
mean loss: 492.97
 ---- batch: 100 ----
mean loss: 467.03
 ---- batch: 110 ----
mean loss: 478.70
train mean loss: 482.71
epoch train time: 0:00:02.139344
elapsed time: 0:02:15.420552
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 00:09:21.820340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.87
 ---- batch: 020 ----
mean loss: 483.80
 ---- batch: 030 ----
mean loss: 493.63
 ---- batch: 040 ----
mean loss: 498.29
 ---- batch: 050 ----
mean loss: 491.76
 ---- batch: 060 ----
mean loss: 487.95
 ---- batch: 070 ----
mean loss: 504.18
 ---- batch: 080 ----
mean loss: 496.08
 ---- batch: 090 ----
mean loss: 480.13
 ---- batch: 100 ----
mean loss: 480.95
 ---- batch: 110 ----
mean loss: 479.73
train mean loss: 490.34
epoch train time: 0:00:02.139176
elapsed time: 0:02:17.559915
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 00:09:23.959729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.92
 ---- batch: 020 ----
mean loss: 493.16
 ---- batch: 030 ----
mean loss: 479.31
 ---- batch: 040 ----
mean loss: 503.07
 ---- batch: 050 ----
mean loss: 499.53
 ---- batch: 060 ----
mean loss: 469.70
 ---- batch: 070 ----
mean loss: 467.31
 ---- batch: 080 ----
mean loss: 483.88
 ---- batch: 090 ----
mean loss: 489.51
 ---- batch: 100 ----
mean loss: 481.95
 ---- batch: 110 ----
mean loss: 481.22
train mean loss: 485.62
epoch train time: 0:00:02.138639
elapsed time: 0:02:19.698744
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 00:09:26.098530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.55
 ---- batch: 020 ----
mean loss: 482.45
 ---- batch: 030 ----
mean loss: 489.29
 ---- batch: 040 ----
mean loss: 472.65
 ---- batch: 050 ----
mean loss: 474.30
 ---- batch: 060 ----
mean loss: 466.11
 ---- batch: 070 ----
mean loss: 492.46
 ---- batch: 080 ----
mean loss: 493.27
 ---- batch: 090 ----
mean loss: 498.96
 ---- batch: 100 ----
mean loss: 480.17
 ---- batch: 110 ----
mean loss: 498.33
train mean loss: 485.01
epoch train time: 0:00:02.136413
elapsed time: 0:02:21.835310
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 00:09:28.235095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.44
 ---- batch: 020 ----
mean loss: 489.12
 ---- batch: 030 ----
mean loss: 483.78
 ---- batch: 040 ----
mean loss: 474.87
 ---- batch: 050 ----
mean loss: 477.34
 ---- batch: 060 ----
mean loss: 481.17
 ---- batch: 070 ----
mean loss: 487.08
 ---- batch: 080 ----
mean loss: 473.57
 ---- batch: 090 ----
mean loss: 475.87
 ---- batch: 100 ----
mean loss: 495.59
 ---- batch: 110 ----
mean loss: 483.39
train mean loss: 481.25
epoch train time: 0:00:02.143404
elapsed time: 0:02:23.978865
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 00:09:30.378646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.75
 ---- batch: 020 ----
mean loss: 490.84
 ---- batch: 030 ----
mean loss: 474.21
 ---- batch: 040 ----
mean loss: 474.03
 ---- batch: 050 ----
mean loss: 461.35
 ---- batch: 060 ----
mean loss: 478.19
 ---- batch: 070 ----
mean loss: 480.43
 ---- batch: 080 ----
mean loss: 481.31
 ---- batch: 090 ----
mean loss: 468.53
 ---- batch: 100 ----
mean loss: 477.53
 ---- batch: 110 ----
mean loss: 492.97
train mean loss: 480.06
epoch train time: 0:00:02.138301
elapsed time: 0:02:26.117326
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 00:09:32.517133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.78
 ---- batch: 020 ----
mean loss: 471.72
 ---- batch: 030 ----
mean loss: 490.76
 ---- batch: 040 ----
mean loss: 484.55
 ---- batch: 050 ----
mean loss: 485.20
 ---- batch: 060 ----
mean loss: 493.69
 ---- batch: 070 ----
mean loss: 471.78
 ---- batch: 080 ----
mean loss: 482.76
 ---- batch: 090 ----
mean loss: 473.34
 ---- batch: 100 ----
mean loss: 478.49
 ---- batch: 110 ----
mean loss: 471.37
train mean loss: 481.92
epoch train time: 0:00:02.143158
elapsed time: 0:02:28.260686
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 00:09:34.660515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.13
 ---- batch: 020 ----
mean loss: 488.93
 ---- batch: 030 ----
mean loss: 484.20
 ---- batch: 040 ----
mean loss: 486.17
 ---- batch: 050 ----
mean loss: 488.27
 ---- batch: 060 ----
mean loss: 451.75
 ---- batch: 070 ----
mean loss: 465.19
 ---- batch: 080 ----
mean loss: 474.27
 ---- batch: 090 ----
mean loss: 484.43
 ---- batch: 100 ----
mean loss: 483.60
 ---- batch: 110 ----
mean loss: 500.25
train mean loss: 480.88
epoch train time: 0:00:02.141190
elapsed time: 0:02:30.402071
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 00:09:36.801854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.64
 ---- batch: 020 ----
mean loss: 485.55
 ---- batch: 030 ----
mean loss: 467.96
 ---- batch: 040 ----
mean loss: 480.04
 ---- batch: 050 ----
mean loss: 492.53
 ---- batch: 060 ----
mean loss: 476.70
 ---- batch: 070 ----
mean loss: 476.56
 ---- batch: 080 ----
mean loss: 462.11
 ---- batch: 090 ----
mean loss: 492.08
 ---- batch: 100 ----
mean loss: 483.62
 ---- batch: 110 ----
mean loss: 492.20
train mean loss: 480.78
epoch train time: 0:00:02.148055
elapsed time: 0:02:32.550281
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 00:09:38.950063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.77
 ---- batch: 020 ----
mean loss: 475.90
 ---- batch: 030 ----
mean loss: 487.83
 ---- batch: 040 ----
mean loss: 482.61
 ---- batch: 050 ----
mean loss: 490.25
 ---- batch: 060 ----
mean loss: 487.24
 ---- batch: 070 ----
mean loss: 491.94
 ---- batch: 080 ----
mean loss: 458.31
 ---- batch: 090 ----
mean loss: 480.65
 ---- batch: 100 ----
mean loss: 475.44
 ---- batch: 110 ----
mean loss: 479.48
train mean loss: 481.03
epoch train time: 0:00:02.138907
elapsed time: 0:02:34.689344
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 00:09:41.089126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.30
 ---- batch: 020 ----
mean loss: 463.54
 ---- batch: 030 ----
mean loss: 466.69
 ---- batch: 040 ----
mean loss: 485.56
 ---- batch: 050 ----
mean loss: 471.41
 ---- batch: 060 ----
mean loss: 475.83
 ---- batch: 070 ----
mean loss: 478.88
 ---- batch: 080 ----
mean loss: 479.23
 ---- batch: 090 ----
mean loss: 487.78
 ---- batch: 100 ----
mean loss: 488.92
 ---- batch: 110 ----
mean loss: 481.09
train mean loss: 478.42
epoch train time: 0:00:02.144665
elapsed time: 0:02:36.834166
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 00:09:43.233952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.79
 ---- batch: 020 ----
mean loss: 490.77
 ---- batch: 030 ----
mean loss: 483.29
 ---- batch: 040 ----
mean loss: 467.69
 ---- batch: 050 ----
mean loss: 473.19
 ---- batch: 060 ----
mean loss: 461.97
 ---- batch: 070 ----
mean loss: 476.00
 ---- batch: 080 ----
mean loss: 491.80
 ---- batch: 090 ----
mean loss: 478.98
 ---- batch: 100 ----
mean loss: 474.27
 ---- batch: 110 ----
mean loss: 477.04
train mean loss: 477.07
epoch train time: 0:00:02.143905
elapsed time: 0:02:38.978235
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 00:09:45.378037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.12
 ---- batch: 020 ----
mean loss: 478.56
 ---- batch: 030 ----
mean loss: 482.39
 ---- batch: 040 ----
mean loss: 464.33
 ---- batch: 050 ----
mean loss: 470.69
 ---- batch: 060 ----
mean loss: 481.84
 ---- batch: 070 ----
mean loss: 470.36
 ---- batch: 080 ----
mean loss: 480.66
 ---- batch: 090 ----
mean loss: 479.57
 ---- batch: 100 ----
mean loss: 492.89
 ---- batch: 110 ----
mean loss: 484.22
train mean loss: 478.29
epoch train time: 0:00:02.142358
elapsed time: 0:02:41.120762
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 00:09:47.520555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.30
 ---- batch: 020 ----
mean loss: 472.12
 ---- batch: 030 ----
mean loss: 485.07
 ---- batch: 040 ----
mean loss: 474.29
 ---- batch: 050 ----
mean loss: 475.70
 ---- batch: 060 ----
mean loss: 476.29
 ---- batch: 070 ----
mean loss: 459.92
 ---- batch: 080 ----
mean loss: 487.03
 ---- batch: 090 ----
mean loss: 470.68
 ---- batch: 100 ----
mean loss: 468.38
 ---- batch: 110 ----
mean loss: 480.03
train mean loss: 474.95
epoch train time: 0:00:02.143984
elapsed time: 0:02:43.264920
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 00:09:49.664704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.08
 ---- batch: 020 ----
mean loss: 469.46
 ---- batch: 030 ----
mean loss: 470.64
 ---- batch: 040 ----
mean loss: 478.98
 ---- batch: 050 ----
mean loss: 467.05
 ---- batch: 060 ----
mean loss: 477.97
 ---- batch: 070 ----
mean loss: 483.46
 ---- batch: 080 ----
mean loss: 484.91
 ---- batch: 090 ----
mean loss: 479.43
 ---- batch: 100 ----
mean loss: 467.40
 ---- batch: 110 ----
mean loss: 486.02
train mean loss: 476.32
epoch train time: 0:00:02.141133
elapsed time: 0:02:45.406206
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 00:09:51.805989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.66
 ---- batch: 020 ----
mean loss: 473.20
 ---- batch: 030 ----
mean loss: 461.01
 ---- batch: 040 ----
mean loss: 482.37
 ---- batch: 050 ----
mean loss: 484.56
 ---- batch: 060 ----
mean loss: 473.93
 ---- batch: 070 ----
mean loss: 466.24
 ---- batch: 080 ----
mean loss: 474.70
 ---- batch: 090 ----
mean loss: 478.89
 ---- batch: 100 ----
mean loss: 476.43
 ---- batch: 110 ----
mean loss: 482.82
train mean loss: 477.10
epoch train time: 0:00:02.147565
elapsed time: 0:02:47.553941
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 00:09:53.953728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.33
 ---- batch: 020 ----
mean loss: 460.01
 ---- batch: 030 ----
mean loss: 471.07
 ---- batch: 040 ----
mean loss: 474.94
 ---- batch: 050 ----
mean loss: 467.27
 ---- batch: 060 ----
mean loss: 476.78
 ---- batch: 070 ----
mean loss: 488.58
 ---- batch: 080 ----
mean loss: 475.40
 ---- batch: 090 ----
mean loss: 459.58
 ---- batch: 100 ----
mean loss: 484.21
 ---- batch: 110 ----
mean loss: 475.17
train mean loss: 474.60
epoch train time: 0:00:02.143904
elapsed time: 0:02:49.698001
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 00:09:56.097789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.69
 ---- batch: 020 ----
mean loss: 475.54
 ---- batch: 030 ----
mean loss: 483.89
 ---- batch: 040 ----
mean loss: 469.55
 ---- batch: 050 ----
mean loss: 468.88
 ---- batch: 060 ----
mean loss: 484.91
 ---- batch: 070 ----
mean loss: 468.04
 ---- batch: 080 ----
mean loss: 477.11
 ---- batch: 090 ----
mean loss: 470.68
 ---- batch: 100 ----
mean loss: 480.39
 ---- batch: 110 ----
mean loss: 472.90
train mean loss: 475.55
epoch train time: 0:00:02.145335
elapsed time: 0:02:51.843495
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 00:09:58.243282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.62
 ---- batch: 020 ----
mean loss: 469.74
 ---- batch: 030 ----
mean loss: 468.18
 ---- batch: 040 ----
mean loss: 481.16
 ---- batch: 050 ----
mean loss: 473.74
 ---- batch: 060 ----
mean loss: 470.23
 ---- batch: 070 ----
mean loss: 480.82
 ---- batch: 080 ----
mean loss: 476.82
 ---- batch: 090 ----
mean loss: 462.79
 ---- batch: 100 ----
mean loss: 460.47
 ---- batch: 110 ----
mean loss: 467.39
train mean loss: 472.37
epoch train time: 0:00:02.144524
elapsed time: 0:02:53.988178
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 00:10:00.387986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.25
 ---- batch: 020 ----
mean loss: 484.63
 ---- batch: 030 ----
mean loss: 486.82
 ---- batch: 040 ----
mean loss: 477.76
 ---- batch: 050 ----
mean loss: 471.54
 ---- batch: 060 ----
mean loss: 476.83
 ---- batch: 070 ----
mean loss: 469.43
 ---- batch: 080 ----
mean loss: 474.96
 ---- batch: 090 ----
mean loss: 473.58
 ---- batch: 100 ----
mean loss: 453.16
 ---- batch: 110 ----
mean loss: 462.27
train mean loss: 473.34
epoch train time: 0:00:02.145686
elapsed time: 0:02:56.134038
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 00:10:02.533823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.46
 ---- batch: 020 ----
mean loss: 469.97
 ---- batch: 030 ----
mean loss: 462.77
 ---- batch: 040 ----
mean loss: 476.95
 ---- batch: 050 ----
mean loss: 505.39
 ---- batch: 060 ----
mean loss: 461.29
 ---- batch: 070 ----
mean loss: 478.66
 ---- batch: 080 ----
mean loss: 476.99
 ---- batch: 090 ----
mean loss: 469.23
 ---- batch: 100 ----
mean loss: 458.12
 ---- batch: 110 ----
mean loss: 478.26
train mean loss: 474.79
epoch train time: 0:00:02.158218
elapsed time: 0:02:58.292426
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 00:10:04.692218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.23
 ---- batch: 020 ----
mean loss: 474.80
 ---- batch: 030 ----
mean loss: 460.29
 ---- batch: 040 ----
mean loss: 471.42
 ---- batch: 050 ----
mean loss: 474.12
 ---- batch: 060 ----
mean loss: 483.37
 ---- batch: 070 ----
mean loss: 483.90
 ---- batch: 080 ----
mean loss: 458.80
 ---- batch: 090 ----
mean loss: 470.10
 ---- batch: 100 ----
mean loss: 470.03
 ---- batch: 110 ----
mean loss: 475.54
train mean loss: 471.28
epoch train time: 0:00:02.143059
elapsed time: 0:03:00.435648
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 00:10:06.835429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.92
 ---- batch: 020 ----
mean loss: 462.35
 ---- batch: 030 ----
mean loss: 470.01
 ---- batch: 040 ----
mean loss: 456.29
 ---- batch: 050 ----
mean loss: 462.61
 ---- batch: 060 ----
mean loss: 474.66
 ---- batch: 070 ----
mean loss: 475.62
 ---- batch: 080 ----
mean loss: 479.69
 ---- batch: 090 ----
mean loss: 471.92
 ---- batch: 100 ----
mean loss: 466.86
 ---- batch: 110 ----
mean loss: 467.49
train mean loss: 470.01
epoch train time: 0:00:02.141764
elapsed time: 0:03:02.577555
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 00:10:08.977338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.25
 ---- batch: 020 ----
mean loss: 456.29
 ---- batch: 030 ----
mean loss: 474.88
 ---- batch: 040 ----
mean loss: 460.97
 ---- batch: 050 ----
mean loss: 468.96
 ---- batch: 060 ----
mean loss: 468.36
 ---- batch: 070 ----
mean loss: 467.28
 ---- batch: 080 ----
mean loss: 465.48
 ---- batch: 090 ----
mean loss: 480.21
 ---- batch: 100 ----
mean loss: 471.95
 ---- batch: 110 ----
mean loss: 476.82
train mean loss: 468.93
epoch train time: 0:00:02.151272
elapsed time: 0:03:04.728976
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 00:10:11.128756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.89
 ---- batch: 020 ----
mean loss: 463.72
 ---- batch: 030 ----
mean loss: 479.72
 ---- batch: 040 ----
mean loss: 466.02
 ---- batch: 050 ----
mean loss: 459.10
 ---- batch: 060 ----
mean loss: 466.40
 ---- batch: 070 ----
mean loss: 457.12
 ---- batch: 080 ----
mean loss: 491.90
 ---- batch: 090 ----
mean loss: 469.33
 ---- batch: 100 ----
mean loss: 483.25
 ---- batch: 110 ----
mean loss: 479.22
train mean loss: 470.48
epoch train time: 0:00:02.145029
elapsed time: 0:03:06.874148
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 00:10:13.273951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.37
 ---- batch: 020 ----
mean loss: 468.25
 ---- batch: 030 ----
mean loss: 467.38
 ---- batch: 040 ----
mean loss: 455.49
 ---- batch: 050 ----
mean loss: 469.57
 ---- batch: 060 ----
mean loss: 466.28
 ---- batch: 070 ----
mean loss: 461.29
 ---- batch: 080 ----
mean loss: 492.79
 ---- batch: 090 ----
mean loss: 469.99
 ---- batch: 100 ----
mean loss: 480.65
 ---- batch: 110 ----
mean loss: 459.46
train mean loss: 466.64
epoch train time: 0:00:02.146750
elapsed time: 0:03:09.021083
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 00:10:15.420867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.22
 ---- batch: 020 ----
mean loss: 460.07
 ---- batch: 030 ----
mean loss: 451.42
 ---- batch: 040 ----
mean loss: 449.42
 ---- batch: 050 ----
mean loss: 462.83
 ---- batch: 060 ----
mean loss: 466.93
 ---- batch: 070 ----
mean loss: 468.11
 ---- batch: 080 ----
mean loss: 469.25
 ---- batch: 090 ----
mean loss: 464.67
 ---- batch: 100 ----
mean loss: 460.67
 ---- batch: 110 ----
mean loss: 475.21
train mean loss: 464.17
epoch train time: 0:00:02.152999
elapsed time: 0:03:11.174243
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 00:10:17.574033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.17
 ---- batch: 020 ----
mean loss: 469.05
 ---- batch: 030 ----
mean loss: 455.95
 ---- batch: 040 ----
mean loss: 456.29
 ---- batch: 050 ----
mean loss: 470.38
 ---- batch: 060 ----
mean loss: 473.04
 ---- batch: 070 ----
mean loss: 459.27
 ---- batch: 080 ----
mean loss: 465.53
 ---- batch: 090 ----
mean loss: 483.76
 ---- batch: 100 ----
mean loss: 460.31
 ---- batch: 110 ----
mean loss: 455.47
train mean loss: 465.73
epoch train time: 0:00:02.138377
elapsed time: 0:03:13.312778
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 00:10:19.712569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.14
 ---- batch: 020 ----
mean loss: 468.93
 ---- batch: 030 ----
mean loss: 464.06
 ---- batch: 040 ----
mean loss: 470.18
 ---- batch: 050 ----
mean loss: 464.02
 ---- batch: 060 ----
mean loss: 453.55
 ---- batch: 070 ----
mean loss: 456.68
 ---- batch: 080 ----
mean loss: 459.77
 ---- batch: 090 ----
mean loss: 448.35
 ---- batch: 100 ----
mean loss: 455.67
 ---- batch: 110 ----
mean loss: 455.09
train mean loss: 459.93
epoch train time: 0:00:02.141303
elapsed time: 0:03:15.454265
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 00:10:21.854070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.70
 ---- batch: 020 ----
mean loss: 452.04
 ---- batch: 030 ----
mean loss: 462.96
 ---- batch: 040 ----
mean loss: 463.59
 ---- batch: 050 ----
mean loss: 454.86
 ---- batch: 060 ----
mean loss: 443.94
 ---- batch: 070 ----
mean loss: 453.14
 ---- batch: 080 ----
mean loss: 464.48
 ---- batch: 090 ----
mean loss: 457.13
 ---- batch: 100 ----
mean loss: 451.94
 ---- batch: 110 ----
mean loss: 463.99
train mean loss: 456.31
epoch train time: 0:00:02.147388
elapsed time: 0:03:17.601833
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 00:10:24.001639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.10
 ---- batch: 020 ----
mean loss: 461.03
 ---- batch: 030 ----
mean loss: 453.06
 ---- batch: 040 ----
mean loss: 454.03
 ---- batch: 050 ----
mean loss: 473.30
 ---- batch: 060 ----
mean loss: 464.45
 ---- batch: 070 ----
mean loss: 446.09
 ---- batch: 080 ----
mean loss: 444.11
 ---- batch: 090 ----
mean loss: 467.29
 ---- batch: 100 ----
mean loss: 459.16
 ---- batch: 110 ----
mean loss: 443.56
train mean loss: 456.36
epoch train time: 0:00:02.149008
elapsed time: 0:03:19.751020
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 00:10:26.150834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.83
 ---- batch: 020 ----
mean loss: 443.44
 ---- batch: 030 ----
mean loss: 450.95
 ---- batch: 040 ----
mean loss: 460.69
 ---- batch: 050 ----
mean loss: 449.21
 ---- batch: 060 ----
mean loss: 462.46
 ---- batch: 070 ----
mean loss: 461.05
 ---- batch: 080 ----
mean loss: 439.93
 ---- batch: 090 ----
mean loss: 442.30
 ---- batch: 100 ----
mean loss: 457.11
 ---- batch: 110 ----
mean loss: 442.83
train mean loss: 450.36
epoch train time: 0:00:02.149706
elapsed time: 0:03:21.900919
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 00:10:28.300725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.02
 ---- batch: 020 ----
mean loss: 459.48
 ---- batch: 030 ----
mean loss: 434.88
 ---- batch: 040 ----
mean loss: 443.81
 ---- batch: 050 ----
mean loss: 458.04
 ---- batch: 060 ----
mean loss: 450.35
 ---- batch: 070 ----
mean loss: 436.79
 ---- batch: 080 ----
mean loss: 437.72
 ---- batch: 090 ----
mean loss: 457.27
 ---- batch: 100 ----
mean loss: 435.45
 ---- batch: 110 ----
mean loss: 441.04
train mean loss: 445.20
epoch train time: 0:00:02.147059
elapsed time: 0:03:24.048156
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 00:10:30.447946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.06
 ---- batch: 020 ----
mean loss: 453.88
 ---- batch: 030 ----
mean loss: 422.15
 ---- batch: 040 ----
mean loss: 433.70
 ---- batch: 050 ----
mean loss: 451.46
 ---- batch: 060 ----
mean loss: 442.61
 ---- batch: 070 ----
mean loss: 441.95
 ---- batch: 080 ----
mean loss: 431.56
 ---- batch: 090 ----
mean loss: 437.85
 ---- batch: 100 ----
mean loss: 438.46
 ---- batch: 110 ----
mean loss: 432.93
train mean loss: 439.27
epoch train time: 0:00:02.144250
elapsed time: 0:03:26.192562
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 00:10:32.592345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.47
 ---- batch: 020 ----
mean loss: 435.87
 ---- batch: 030 ----
mean loss: 434.33
 ---- batch: 040 ----
mean loss: 435.30
 ---- batch: 050 ----
mean loss: 423.37
 ---- batch: 060 ----
mean loss: 427.56
 ---- batch: 070 ----
mean loss: 422.63
 ---- batch: 080 ----
mean loss: 429.70
 ---- batch: 090 ----
mean loss: 441.26
 ---- batch: 100 ----
mean loss: 420.90
 ---- batch: 110 ----
mean loss: 427.49
train mean loss: 428.73
epoch train time: 0:00:02.141617
elapsed time: 0:03:28.334327
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 00:10:34.734108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.80
 ---- batch: 020 ----
mean loss: 429.48
 ---- batch: 030 ----
mean loss: 439.96
 ---- batch: 040 ----
mean loss: 423.04
 ---- batch: 050 ----
mean loss: 421.44
 ---- batch: 060 ----
mean loss: 415.58
 ---- batch: 070 ----
mean loss: 425.83
 ---- batch: 080 ----
mean loss: 420.90
 ---- batch: 090 ----
mean loss: 409.64
 ---- batch: 100 ----
mean loss: 415.89
 ---- batch: 110 ----
mean loss: 420.85
train mean loss: 423.41
epoch train time: 0:00:02.142203
elapsed time: 0:03:30.476679
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 00:10:36.876461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.41
 ---- batch: 020 ----
mean loss: 426.97
 ---- batch: 030 ----
mean loss: 420.05
 ---- batch: 040 ----
mean loss: 415.10
 ---- batch: 050 ----
mean loss: 425.30
 ---- batch: 060 ----
mean loss: 421.26
 ---- batch: 070 ----
mean loss: 403.16
 ---- batch: 080 ----
mean loss: 406.70
 ---- batch: 090 ----
mean loss: 413.30
 ---- batch: 100 ----
mean loss: 421.85
 ---- batch: 110 ----
mean loss: 417.90
train mean loss: 418.13
epoch train time: 0:00:02.139836
elapsed time: 0:03:32.616661
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 00:10:39.016462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.63
 ---- batch: 020 ----
mean loss: 408.13
 ---- batch: 030 ----
mean loss: 415.36
 ---- batch: 040 ----
mean loss: 408.28
 ---- batch: 050 ----
mean loss: 412.10
 ---- batch: 060 ----
mean loss: 398.52
 ---- batch: 070 ----
mean loss: 407.07
 ---- batch: 080 ----
mean loss: 418.47
 ---- batch: 090 ----
mean loss: 413.71
 ---- batch: 100 ----
mean loss: 420.75
 ---- batch: 110 ----
mean loss: 403.62
train mean loss: 410.03
epoch train time: 0:00:02.137434
elapsed time: 0:03:34.754257
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 00:10:41.154053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.82
 ---- batch: 020 ----
mean loss: 417.04
 ---- batch: 030 ----
mean loss: 398.67
 ---- batch: 040 ----
mean loss: 414.18
 ---- batch: 050 ----
mean loss: 410.53
 ---- batch: 060 ----
mean loss: 411.94
 ---- batch: 070 ----
mean loss: 409.10
 ---- batch: 080 ----
mean loss: 414.83
 ---- batch: 090 ----
mean loss: 409.32
 ---- batch: 100 ----
mean loss: 409.71
 ---- batch: 110 ----
mean loss: 400.63
train mean loss: 408.54
epoch train time: 0:00:02.138013
elapsed time: 0:03:36.892458
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 00:10:43.292238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.88
 ---- batch: 020 ----
mean loss: 432.65
 ---- batch: 030 ----
mean loss: 398.21
 ---- batch: 040 ----
mean loss: 403.71
 ---- batch: 050 ----
mean loss: 415.43
 ---- batch: 060 ----
mean loss: 412.94
 ---- batch: 070 ----
mean loss: 391.39
 ---- batch: 080 ----
mean loss: 393.21
 ---- batch: 090 ----
mean loss: 415.74
 ---- batch: 100 ----
mean loss: 404.13
 ---- batch: 110 ----
mean loss: 405.14
train mean loss: 406.54
epoch train time: 0:00:02.142178
elapsed time: 0:03:39.034775
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 00:10:45.434551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.97
 ---- batch: 020 ----
mean loss: 403.81
 ---- batch: 030 ----
mean loss: 411.19
 ---- batch: 040 ----
mean loss: 382.86
 ---- batch: 050 ----
mean loss: 391.81
 ---- batch: 060 ----
mean loss: 407.45
 ---- batch: 070 ----
mean loss: 400.85
 ---- batch: 080 ----
mean loss: 406.85
 ---- batch: 090 ----
mean loss: 395.61
 ---- batch: 100 ----
mean loss: 387.53
 ---- batch: 110 ----
mean loss: 398.49
train mean loss: 399.29
epoch train time: 0:00:02.149528
elapsed time: 0:03:41.184483
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 00:10:47.584262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.40
 ---- batch: 020 ----
mean loss: 393.09
 ---- batch: 030 ----
mean loss: 407.17
 ---- batch: 040 ----
mean loss: 401.37
 ---- batch: 050 ----
mean loss: 395.39
 ---- batch: 060 ----
mean loss: 393.63
 ---- batch: 070 ----
mean loss: 390.39
 ---- batch: 080 ----
mean loss: 401.51
 ---- batch: 090 ----
mean loss: 398.34
 ---- batch: 100 ----
mean loss: 394.81
 ---- batch: 110 ----
mean loss: 405.25
train mean loss: 398.09
epoch train time: 0:00:02.141123
elapsed time: 0:03:43.325771
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 00:10:49.725588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.08
 ---- batch: 020 ----
mean loss: 404.99
 ---- batch: 030 ----
mean loss: 389.92
 ---- batch: 040 ----
mean loss: 408.31
 ---- batch: 050 ----
mean loss: 384.09
 ---- batch: 060 ----
mean loss: 389.31
 ---- batch: 070 ----
mean loss: 402.13
 ---- batch: 080 ----
mean loss: 396.58
 ---- batch: 090 ----
mean loss: 388.53
 ---- batch: 100 ----
mean loss: 387.13
 ---- batch: 110 ----
mean loss: 386.04
train mean loss: 395.10
epoch train time: 0:00:02.138441
elapsed time: 0:03:45.464394
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 00:10:51.864175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.60
 ---- batch: 020 ----
mean loss: 379.80
 ---- batch: 030 ----
mean loss: 383.26
 ---- batch: 040 ----
mean loss: 396.40
 ---- batch: 050 ----
mean loss: 399.61
 ---- batch: 060 ----
mean loss: 391.29
 ---- batch: 070 ----
mean loss: 394.13
 ---- batch: 080 ----
mean loss: 397.28
 ---- batch: 090 ----
mean loss: 392.83
 ---- batch: 100 ----
mean loss: 396.97
 ---- batch: 110 ----
mean loss: 402.02
train mean loss: 393.52
epoch train time: 0:00:02.141105
elapsed time: 0:03:47.605644
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 00:10:54.005433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.15
 ---- batch: 020 ----
mean loss: 388.31
 ---- batch: 030 ----
mean loss: 384.60
 ---- batch: 040 ----
mean loss: 378.46
 ---- batch: 050 ----
mean loss: 385.05
 ---- batch: 060 ----
mean loss: 406.65
 ---- batch: 070 ----
mean loss: 388.08
 ---- batch: 080 ----
mean loss: 391.17
 ---- batch: 090 ----
mean loss: 404.94
 ---- batch: 100 ----
mean loss: 383.85
 ---- batch: 110 ----
mean loss: 394.71
train mean loss: 389.86
epoch train time: 0:00:02.153391
elapsed time: 0:03:49.759195
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 00:10:56.159003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.47
 ---- batch: 020 ----
mean loss: 391.43
 ---- batch: 030 ----
mean loss: 373.93
 ---- batch: 040 ----
mean loss: 398.14
 ---- batch: 050 ----
mean loss: 374.36
 ---- batch: 060 ----
mean loss: 376.18
 ---- batch: 070 ----
mean loss: 394.82
 ---- batch: 080 ----
mean loss: 377.52
 ---- batch: 090 ----
mean loss: 386.38
 ---- batch: 100 ----
mean loss: 381.43
 ---- batch: 110 ----
mean loss: 386.19
train mean loss: 384.45
epoch train time: 0:00:02.146655
elapsed time: 0:03:51.906025
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 00:10:58.305810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.93
 ---- batch: 020 ----
mean loss: 397.59
 ---- batch: 030 ----
mean loss: 385.99
 ---- batch: 040 ----
mean loss: 383.85
 ---- batch: 050 ----
mean loss: 390.91
 ---- batch: 060 ----
mean loss: 383.28
 ---- batch: 070 ----
mean loss: 376.26
 ---- batch: 080 ----
mean loss: 372.30
 ---- batch: 090 ----
mean loss: 386.27
 ---- batch: 100 ----
mean loss: 384.64
 ---- batch: 110 ----
mean loss: 390.90
train mean loss: 385.38
epoch train time: 0:00:02.140131
elapsed time: 0:03:54.046302
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 00:11:00.446084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.32
 ---- batch: 020 ----
mean loss: 388.96
 ---- batch: 030 ----
mean loss: 382.81
 ---- batch: 040 ----
mean loss: 375.08
 ---- batch: 050 ----
mean loss: 384.05
 ---- batch: 060 ----
mean loss: 386.65
 ---- batch: 070 ----
mean loss: 394.50
 ---- batch: 080 ----
mean loss: 379.95
 ---- batch: 090 ----
mean loss: 381.46
 ---- batch: 100 ----
mean loss: 371.52
 ---- batch: 110 ----
mean loss: 372.94
train mean loss: 381.58
epoch train time: 0:00:02.144162
elapsed time: 0:03:56.190611
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 00:11:02.590390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.64
 ---- batch: 020 ----
mean loss: 371.19
 ---- batch: 030 ----
mean loss: 383.27
 ---- batch: 040 ----
mean loss: 380.92
 ---- batch: 050 ----
mean loss: 374.21
 ---- batch: 060 ----
mean loss: 373.34
 ---- batch: 070 ----
mean loss: 388.35
 ---- batch: 080 ----
mean loss: 376.09
 ---- batch: 090 ----
mean loss: 385.97
 ---- batch: 100 ----
mean loss: 369.80
 ---- batch: 110 ----
mean loss: 368.42
train mean loss: 375.79
epoch train time: 0:00:02.139839
elapsed time: 0:03:58.330629
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 00:11:04.730418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.97
 ---- batch: 020 ----
mean loss: 387.02
 ---- batch: 030 ----
mean loss: 377.65
 ---- batch: 040 ----
mean loss: 373.32
 ---- batch: 050 ----
mean loss: 386.34
 ---- batch: 060 ----
mean loss: 372.52
 ---- batch: 070 ----
mean loss: 368.68
 ---- batch: 080 ----
mean loss: 370.27
 ---- batch: 090 ----
mean loss: 385.23
 ---- batch: 100 ----
mean loss: 374.43
 ---- batch: 110 ----
mean loss: 365.42
train mean loss: 377.44
epoch train time: 0:00:02.178341
elapsed time: 0:04:00.509142
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 00:11:06.908927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.01
 ---- batch: 020 ----
mean loss: 375.72
 ---- batch: 030 ----
mean loss: 358.28
 ---- batch: 040 ----
mean loss: 380.67
 ---- batch: 050 ----
mean loss: 369.32
 ---- batch: 060 ----
mean loss: 371.76
 ---- batch: 070 ----
mean loss: 380.17
 ---- batch: 080 ----
mean loss: 370.56
 ---- batch: 090 ----
mean loss: 362.83
 ---- batch: 100 ----
mean loss: 372.38
 ---- batch: 110 ----
mean loss: 377.73
train mean loss: 372.28
epoch train time: 0:00:02.146987
elapsed time: 0:04:02.656278
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 00:11:09.056061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.94
 ---- batch: 020 ----
mean loss: 365.96
 ---- batch: 030 ----
mean loss: 368.90
 ---- batch: 040 ----
mean loss: 360.57
 ---- batch: 050 ----
mean loss: 378.39
 ---- batch: 060 ----
mean loss: 359.09
 ---- batch: 070 ----
mean loss: 369.08
 ---- batch: 080 ----
mean loss: 381.38
 ---- batch: 090 ----
mean loss: 375.74
 ---- batch: 100 ----
mean loss: 377.41
 ---- batch: 110 ----
mean loss: 367.14
train mean loss: 369.43
epoch train time: 0:00:02.132989
elapsed time: 0:04:04.789422
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 00:11:11.189226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.94
 ---- batch: 020 ----
mean loss: 371.86
 ---- batch: 030 ----
mean loss: 372.82
 ---- batch: 040 ----
mean loss: 359.17
 ---- batch: 050 ----
mean loss: 368.27
 ---- batch: 060 ----
mean loss: 370.28
 ---- batch: 070 ----
mean loss: 358.44
 ---- batch: 080 ----
mean loss: 370.71
 ---- batch: 090 ----
mean loss: 376.36
 ---- batch: 100 ----
mean loss: 364.05
 ---- batch: 110 ----
mean loss: 361.06
train mean loss: 366.43
epoch train time: 0:00:02.139266
elapsed time: 0:04:06.928859
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 00:11:13.328644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.25
 ---- batch: 020 ----
mean loss: 366.54
 ---- batch: 030 ----
mean loss: 364.49
 ---- batch: 040 ----
mean loss: 361.50
 ---- batch: 050 ----
mean loss: 357.24
 ---- batch: 060 ----
mean loss: 365.56
 ---- batch: 070 ----
mean loss: 364.72
 ---- batch: 080 ----
mean loss: 353.85
 ---- batch: 090 ----
mean loss: 364.13
 ---- batch: 100 ----
mean loss: 369.30
 ---- batch: 110 ----
mean loss: 372.41
train mean loss: 364.99
epoch train time: 0:00:02.136328
elapsed time: 0:04:09.065351
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 00:11:15.465131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.49
 ---- batch: 020 ----
mean loss: 372.20
 ---- batch: 030 ----
mean loss: 373.65
 ---- batch: 040 ----
mean loss: 356.91
 ---- batch: 050 ----
mean loss: 365.95
 ---- batch: 060 ----
mean loss: 365.55
 ---- batch: 070 ----
mean loss: 364.02
 ---- batch: 080 ----
mean loss: 364.71
 ---- batch: 090 ----
mean loss: 351.96
 ---- batch: 100 ----
mean loss: 360.46
 ---- batch: 110 ----
mean loss: 352.93
train mean loss: 362.25
epoch train time: 0:00:02.143196
elapsed time: 0:04:11.208699
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 00:11:17.608482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.81
 ---- batch: 020 ----
mean loss: 361.66
 ---- batch: 030 ----
mean loss: 366.62
 ---- batch: 040 ----
mean loss: 361.69
 ---- batch: 050 ----
mean loss: 366.12
 ---- batch: 060 ----
mean loss: 343.91
 ---- batch: 070 ----
mean loss: 356.21
 ---- batch: 080 ----
mean loss: 356.96
 ---- batch: 090 ----
mean loss: 353.08
 ---- batch: 100 ----
mean loss: 360.63
 ---- batch: 110 ----
mean loss: 356.68
train mean loss: 359.43
epoch train time: 0:00:02.143732
elapsed time: 0:04:13.352619
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 00:11:19.752477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.42
 ---- batch: 020 ----
mean loss: 357.96
 ---- batch: 030 ----
mean loss: 371.41
 ---- batch: 040 ----
mean loss: 357.51
 ---- batch: 050 ----
mean loss: 355.74
 ---- batch: 060 ----
mean loss: 364.37
 ---- batch: 070 ----
mean loss: 351.08
 ---- batch: 080 ----
mean loss: 366.39
 ---- batch: 090 ----
mean loss: 349.40
 ---- batch: 100 ----
mean loss: 369.94
 ---- batch: 110 ----
mean loss: 351.15
train mean loss: 359.24
epoch train time: 0:00:02.136070
elapsed time: 0:04:15.488925
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 00:11:21.888708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.28
 ---- batch: 020 ----
mean loss: 360.81
 ---- batch: 030 ----
mean loss: 357.90
 ---- batch: 040 ----
mean loss: 355.84
 ---- batch: 050 ----
mean loss: 345.00
 ---- batch: 060 ----
mean loss: 363.70
 ---- batch: 070 ----
mean loss: 343.61
 ---- batch: 080 ----
mean loss: 353.13
 ---- batch: 090 ----
mean loss: 348.29
 ---- batch: 100 ----
mean loss: 352.51
 ---- batch: 110 ----
mean loss: 356.29
train mean loss: 353.98
epoch train time: 0:00:02.169198
elapsed time: 0:04:17.658312
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 00:11:24.058127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.84
 ---- batch: 020 ----
mean loss: 359.38
 ---- batch: 030 ----
mean loss: 352.71
 ---- batch: 040 ----
mean loss: 359.74
 ---- batch: 050 ----
mean loss: 351.59
 ---- batch: 060 ----
mean loss: 352.76
 ---- batch: 070 ----
mean loss: 350.19
 ---- batch: 080 ----
mean loss: 349.10
 ---- batch: 090 ----
mean loss: 353.07
 ---- batch: 100 ----
mean loss: 365.13
 ---- batch: 110 ----
mean loss: 363.15
train mean loss: 354.75
epoch train time: 0:00:02.149514
elapsed time: 0:04:19.808012
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 00:11:26.207798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.58
 ---- batch: 020 ----
mean loss: 346.49
 ---- batch: 030 ----
mean loss: 357.19
 ---- batch: 040 ----
mean loss: 341.29
 ---- batch: 050 ----
mean loss: 351.44
 ---- batch: 060 ----
mean loss: 364.63
 ---- batch: 070 ----
mean loss: 352.97
 ---- batch: 080 ----
mean loss: 353.53
 ---- batch: 090 ----
mean loss: 346.61
 ---- batch: 100 ----
mean loss: 363.48
 ---- batch: 110 ----
mean loss: 351.43
train mean loss: 352.94
epoch train time: 0:00:02.149232
elapsed time: 0:04:21.957420
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 00:11:28.357197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.51
 ---- batch: 020 ----
mean loss: 362.28
 ---- batch: 030 ----
mean loss: 349.38
 ---- batch: 040 ----
mean loss: 347.46
 ---- batch: 050 ----
mean loss: 361.83
 ---- batch: 060 ----
mean loss: 341.36
 ---- batch: 070 ----
mean loss: 354.20
 ---- batch: 080 ----
mean loss: 353.14
 ---- batch: 090 ----
mean loss: 343.95
 ---- batch: 100 ----
mean loss: 337.36
 ---- batch: 110 ----
mean loss: 355.45
train mean loss: 350.79
epoch train time: 0:00:02.160163
elapsed time: 0:04:24.117728
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 00:11:30.517512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.86
 ---- batch: 020 ----
mean loss: 356.25
 ---- batch: 030 ----
mean loss: 348.96
 ---- batch: 040 ----
mean loss: 357.16
 ---- batch: 050 ----
mean loss: 345.34
 ---- batch: 060 ----
mean loss: 348.02
 ---- batch: 070 ----
mean loss: 344.28
 ---- batch: 080 ----
mean loss: 350.94
 ---- batch: 090 ----
mean loss: 355.00
 ---- batch: 100 ----
mean loss: 332.46
 ---- batch: 110 ----
mean loss: 341.11
train mean loss: 348.12
epoch train time: 0:00:02.149284
elapsed time: 0:04:26.267173
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 00:11:32.666958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.48
 ---- batch: 020 ----
mean loss: 357.03
 ---- batch: 030 ----
mean loss: 340.31
 ---- batch: 040 ----
mean loss: 349.24
 ---- batch: 050 ----
mean loss: 343.54
 ---- batch: 060 ----
mean loss: 346.10
 ---- batch: 070 ----
mean loss: 343.69
 ---- batch: 080 ----
mean loss: 353.27
 ---- batch: 090 ----
mean loss: 341.94
 ---- batch: 100 ----
mean loss: 359.27
 ---- batch: 110 ----
mean loss: 355.94
train mean loss: 348.19
epoch train time: 0:00:02.145762
elapsed time: 0:04:28.413128
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 00:11:34.812919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.25
 ---- batch: 020 ----
mean loss: 349.21
 ---- batch: 030 ----
mean loss: 349.32
 ---- batch: 040 ----
mean loss: 336.36
 ---- batch: 050 ----
mean loss: 340.89
 ---- batch: 060 ----
mean loss: 343.43
 ---- batch: 070 ----
mean loss: 357.78
 ---- batch: 080 ----
mean loss: 338.50
 ---- batch: 090 ----
mean loss: 334.37
 ---- batch: 100 ----
mean loss: 338.00
 ---- batch: 110 ----
mean loss: 352.72
train mean loss: 344.69
epoch train time: 0:00:02.141519
elapsed time: 0:04:30.554805
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 00:11:36.954590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.54
 ---- batch: 020 ----
mean loss: 351.34
 ---- batch: 030 ----
mean loss: 342.57
 ---- batch: 040 ----
mean loss: 337.37
 ---- batch: 050 ----
mean loss: 352.90
 ---- batch: 060 ----
mean loss: 348.91
 ---- batch: 070 ----
mean loss: 338.73
 ---- batch: 080 ----
mean loss: 350.53
 ---- batch: 090 ----
mean loss: 350.20
 ---- batch: 100 ----
mean loss: 332.83
 ---- batch: 110 ----
mean loss: 341.64
train mean loss: 345.92
epoch train time: 0:00:02.146134
elapsed time: 0:04:32.701092
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 00:11:39.100899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.70
 ---- batch: 020 ----
mean loss: 337.30
 ---- batch: 030 ----
mean loss: 346.36
 ---- batch: 040 ----
mean loss: 343.60
 ---- batch: 050 ----
mean loss: 338.35
 ---- batch: 060 ----
mean loss: 366.06
 ---- batch: 070 ----
mean loss: 364.45
 ---- batch: 080 ----
mean loss: 354.66
 ---- batch: 090 ----
mean loss: 348.83
 ---- batch: 100 ----
mean loss: 342.60
 ---- batch: 110 ----
mean loss: 341.17
train mean loss: 348.33
epoch train time: 0:00:02.137233
elapsed time: 0:04:34.838498
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 00:11:41.238281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.67
 ---- batch: 020 ----
mean loss: 345.12
 ---- batch: 030 ----
mean loss: 343.61
 ---- batch: 040 ----
mean loss: 335.86
 ---- batch: 050 ----
mean loss: 349.40
 ---- batch: 060 ----
mean loss: 330.85
 ---- batch: 070 ----
mean loss: 341.53
 ---- batch: 080 ----
mean loss: 342.70
 ---- batch: 090 ----
mean loss: 340.24
 ---- batch: 100 ----
mean loss: 339.95
 ---- batch: 110 ----
mean loss: 341.41
train mean loss: 341.74
epoch train time: 0:00:02.139899
elapsed time: 0:04:36.978548
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 00:11:43.378332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.17
 ---- batch: 020 ----
mean loss: 347.16
 ---- batch: 030 ----
mean loss: 331.21
 ---- batch: 040 ----
mean loss: 334.53
 ---- batch: 050 ----
mean loss: 339.14
 ---- batch: 060 ----
mean loss: 341.89
 ---- batch: 070 ----
mean loss: 337.95
 ---- batch: 080 ----
mean loss: 335.68
 ---- batch: 090 ----
mean loss: 341.42
 ---- batch: 100 ----
mean loss: 340.02
 ---- batch: 110 ----
mean loss: 344.70
train mean loss: 339.66
epoch train time: 0:00:02.139389
elapsed time: 0:04:39.118094
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 00:11:45.517881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.63
 ---- batch: 020 ----
mean loss: 346.44
 ---- batch: 030 ----
mean loss: 339.86
 ---- batch: 040 ----
mean loss: 330.37
 ---- batch: 050 ----
mean loss: 340.99
 ---- batch: 060 ----
mean loss: 332.54
 ---- batch: 070 ----
mean loss: 336.36
 ---- batch: 080 ----
mean loss: 340.21
 ---- batch: 090 ----
mean loss: 337.07
 ---- batch: 100 ----
mean loss: 325.40
 ---- batch: 110 ----
mean loss: 333.97
train mean loss: 336.29
epoch train time: 0:00:02.139046
elapsed time: 0:04:41.257294
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 00:11:47.657079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.02
 ---- batch: 020 ----
mean loss: 333.59
 ---- batch: 030 ----
mean loss: 338.93
 ---- batch: 040 ----
mean loss: 343.32
 ---- batch: 050 ----
mean loss: 340.65
 ---- batch: 060 ----
mean loss: 340.09
 ---- batch: 070 ----
mean loss: 338.15
 ---- batch: 080 ----
mean loss: 340.11
 ---- batch: 090 ----
mean loss: 334.79
 ---- batch: 100 ----
mean loss: 349.97
 ---- batch: 110 ----
mean loss: 330.25
train mean loss: 338.13
epoch train time: 0:00:02.144195
elapsed time: 0:04:43.401636
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 00:11:49.801435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.11
 ---- batch: 020 ----
mean loss: 332.11
 ---- batch: 030 ----
mean loss: 338.71
 ---- batch: 040 ----
mean loss: 333.76
 ---- batch: 050 ----
mean loss: 335.43
 ---- batch: 060 ----
mean loss: 324.53
 ---- batch: 070 ----
mean loss: 332.65
 ---- batch: 080 ----
mean loss: 342.38
 ---- batch: 090 ----
mean loss: 329.28
 ---- batch: 100 ----
mean loss: 335.82
 ---- batch: 110 ----
mean loss: 328.51
train mean loss: 334.42
epoch train time: 0:00:02.140874
elapsed time: 0:04:45.542673
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 00:11:51.942477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.41
 ---- batch: 020 ----
mean loss: 338.34
 ---- batch: 030 ----
mean loss: 338.53
 ---- batch: 040 ----
mean loss: 325.19
 ---- batch: 050 ----
mean loss: 336.71
 ---- batch: 060 ----
mean loss: 348.27
 ---- batch: 070 ----
mean loss: 331.10
 ---- batch: 080 ----
mean loss: 347.58
 ---- batch: 090 ----
mean loss: 334.45
 ---- batch: 100 ----
mean loss: 334.07
 ---- batch: 110 ----
mean loss: 333.05
train mean loss: 335.77
epoch train time: 0:00:02.141553
elapsed time: 0:04:47.684399
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 00:11:54.084184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.07
 ---- batch: 020 ----
mean loss: 338.35
 ---- batch: 030 ----
mean loss: 332.53
 ---- batch: 040 ----
mean loss: 334.72
 ---- batch: 050 ----
mean loss: 335.13
 ---- batch: 060 ----
mean loss: 339.29
 ---- batch: 070 ----
mean loss: 325.85
 ---- batch: 080 ----
mean loss: 330.53
 ---- batch: 090 ----
mean loss: 338.26
 ---- batch: 100 ----
mean loss: 330.46
 ---- batch: 110 ----
mean loss: 335.74
train mean loss: 333.26
epoch train time: 0:00:02.145695
elapsed time: 0:04:49.830246
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 00:11:56.230036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.39
 ---- batch: 020 ----
mean loss: 315.28
 ---- batch: 030 ----
mean loss: 330.84
 ---- batch: 040 ----
mean loss: 334.85
 ---- batch: 050 ----
mean loss: 330.61
 ---- batch: 060 ----
mean loss: 341.52
 ---- batch: 070 ----
mean loss: 333.28
 ---- batch: 080 ----
mean loss: 332.29
 ---- batch: 090 ----
mean loss: 341.94
 ---- batch: 100 ----
mean loss: 321.33
 ---- batch: 110 ----
mean loss: 327.53
train mean loss: 331.25
epoch train time: 0:00:02.146975
elapsed time: 0:04:51.977377
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 00:11:58.377160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.18
 ---- batch: 020 ----
mean loss: 334.32
 ---- batch: 030 ----
mean loss: 332.49
 ---- batch: 040 ----
mean loss: 332.15
 ---- batch: 050 ----
mean loss: 316.94
 ---- batch: 060 ----
mean loss: 334.10
 ---- batch: 070 ----
mean loss: 331.55
 ---- batch: 080 ----
mean loss: 325.57
 ---- batch: 090 ----
mean loss: 335.49
 ---- batch: 100 ----
mean loss: 342.32
 ---- batch: 110 ----
mean loss: 332.00
train mean loss: 331.72
epoch train time: 0:00:02.147144
elapsed time: 0:04:54.124683
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 00:12:00.524491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.90
 ---- batch: 020 ----
mean loss: 331.05
 ---- batch: 030 ----
mean loss: 322.40
 ---- batch: 040 ----
mean loss: 336.78
 ---- batch: 050 ----
mean loss: 326.70
 ---- batch: 060 ----
mean loss: 328.24
 ---- batch: 070 ----
mean loss: 333.85
 ---- batch: 080 ----
mean loss: 339.33
 ---- batch: 090 ----
mean loss: 327.03
 ---- batch: 100 ----
mean loss: 337.97
 ---- batch: 110 ----
mean loss: 319.23
train mean loss: 330.07
epoch train time: 0:00:02.152346
elapsed time: 0:04:56.277217
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 00:12:02.677001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.02
 ---- batch: 020 ----
mean loss: 331.54
 ---- batch: 030 ----
mean loss: 320.90
 ---- batch: 040 ----
mean loss: 325.37
 ---- batch: 050 ----
mean loss: 323.55
 ---- batch: 060 ----
mean loss: 335.66
 ---- batch: 070 ----
mean loss: 327.41
 ---- batch: 080 ----
mean loss: 335.90
 ---- batch: 090 ----
mean loss: 347.92
 ---- batch: 100 ----
mean loss: 325.30
 ---- batch: 110 ----
mean loss: 321.96
train mean loss: 328.90
epoch train time: 0:00:02.148159
elapsed time: 0:04:58.425529
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 00:12:04.825316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.51
 ---- batch: 020 ----
mean loss: 315.62
 ---- batch: 030 ----
mean loss: 332.10
 ---- batch: 040 ----
mean loss: 334.32
 ---- batch: 050 ----
mean loss: 328.96
 ---- batch: 060 ----
mean loss: 321.25
 ---- batch: 070 ----
mean loss: 332.83
 ---- batch: 080 ----
mean loss: 326.19
 ---- batch: 090 ----
mean loss: 330.46
 ---- batch: 100 ----
mean loss: 332.62
 ---- batch: 110 ----
mean loss: 319.29
train mean loss: 327.09
epoch train time: 0:00:02.165413
elapsed time: 0:05:00.591108
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 00:12:06.990912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.55
 ---- batch: 020 ----
mean loss: 315.60
 ---- batch: 030 ----
mean loss: 331.76
 ---- batch: 040 ----
mean loss: 334.64
 ---- batch: 050 ----
mean loss: 328.47
 ---- batch: 060 ----
mean loss: 327.82
 ---- batch: 070 ----
mean loss: 323.68
 ---- batch: 080 ----
mean loss: 340.76
 ---- batch: 090 ----
mean loss: 319.20
 ---- batch: 100 ----
mean loss: 324.52
 ---- batch: 110 ----
mean loss: 327.90
train mean loss: 326.70
epoch train time: 0:00:02.161904
elapsed time: 0:05:02.753176
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 00:12:09.152968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.71
 ---- batch: 020 ----
mean loss: 327.26
 ---- batch: 030 ----
mean loss: 325.13
 ---- batch: 040 ----
mean loss: 331.90
 ---- batch: 050 ----
mean loss: 334.53
 ---- batch: 060 ----
mean loss: 326.08
 ---- batch: 070 ----
mean loss: 314.10
 ---- batch: 080 ----
mean loss: 332.78
 ---- batch: 090 ----
mean loss: 324.39
 ---- batch: 100 ----
mean loss: 317.67
 ---- batch: 110 ----
mean loss: 328.84
train mean loss: 326.54
epoch train time: 0:00:02.158437
elapsed time: 0:05:04.911839
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 00:12:11.311640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.76
 ---- batch: 020 ----
mean loss: 323.65
 ---- batch: 030 ----
mean loss: 321.52
 ---- batch: 040 ----
mean loss: 314.92
 ---- batch: 050 ----
mean loss: 337.21
 ---- batch: 060 ----
mean loss: 326.40
 ---- batch: 070 ----
mean loss: 314.19
 ---- batch: 080 ----
mean loss: 321.52
 ---- batch: 090 ----
mean loss: 323.69
 ---- batch: 100 ----
mean loss: 316.09
 ---- batch: 110 ----
mean loss: 325.07
train mean loss: 323.53
epoch train time: 0:00:02.165292
elapsed time: 0:05:07.077314
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 00:12:13.477096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.17
 ---- batch: 020 ----
mean loss: 319.35
 ---- batch: 030 ----
mean loss: 334.17
 ---- batch: 040 ----
mean loss: 329.58
 ---- batch: 050 ----
mean loss: 329.55
 ---- batch: 060 ----
mean loss: 336.59
 ---- batch: 070 ----
mean loss: 326.41
 ---- batch: 080 ----
mean loss: 333.94
 ---- batch: 090 ----
mean loss: 313.38
 ---- batch: 100 ----
mean loss: 326.57
 ---- batch: 110 ----
mean loss: 318.26
train mean loss: 326.49
epoch train time: 0:00:02.171616
elapsed time: 0:05:09.249096
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 00:12:15.648882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.93
 ---- batch: 020 ----
mean loss: 334.87
 ---- batch: 030 ----
mean loss: 331.09
 ---- batch: 040 ----
mean loss: 327.03
 ---- batch: 050 ----
mean loss: 318.00
 ---- batch: 060 ----
mean loss: 323.13
 ---- batch: 070 ----
mean loss: 321.47
 ---- batch: 080 ----
mean loss: 315.96
 ---- batch: 090 ----
mean loss: 323.29
 ---- batch: 100 ----
mean loss: 315.70
 ---- batch: 110 ----
mean loss: 323.89
train mean loss: 323.53
epoch train time: 0:00:02.176656
elapsed time: 0:05:11.425966
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 00:12:17.825756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.54
 ---- batch: 020 ----
mean loss: 324.72
 ---- batch: 030 ----
mean loss: 325.82
 ---- batch: 040 ----
mean loss: 331.67
 ---- batch: 050 ----
mean loss: 321.38
 ---- batch: 060 ----
mean loss: 322.85
 ---- batch: 070 ----
mean loss: 331.43
 ---- batch: 080 ----
mean loss: 326.67
 ---- batch: 090 ----
mean loss: 331.47
 ---- batch: 100 ----
mean loss: 325.01
 ---- batch: 110 ----
mean loss: 321.78
train mean loss: 324.92
epoch train time: 0:00:02.174565
elapsed time: 0:05:13.600728
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 00:12:20.000564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.87
 ---- batch: 020 ----
mean loss: 326.50
 ---- batch: 030 ----
mean loss: 320.70
 ---- batch: 040 ----
mean loss: 320.75
 ---- batch: 050 ----
mean loss: 323.20
 ---- batch: 060 ----
mean loss: 319.17
 ---- batch: 070 ----
mean loss: 327.85
 ---- batch: 080 ----
mean loss: 332.13
 ---- batch: 090 ----
mean loss: 313.24
 ---- batch: 100 ----
mean loss: 314.87
 ---- batch: 110 ----
mean loss: 316.25
train mean loss: 321.68
epoch train time: 0:00:02.175392
elapsed time: 0:05:15.776330
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 00:12:22.176114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.37
 ---- batch: 020 ----
mean loss: 328.73
 ---- batch: 030 ----
mean loss: 327.71
 ---- batch: 040 ----
mean loss: 328.86
 ---- batch: 050 ----
mean loss: 320.98
 ---- batch: 060 ----
mean loss: 324.59
 ---- batch: 070 ----
mean loss: 320.78
 ---- batch: 080 ----
mean loss: 320.47
 ---- batch: 090 ----
mean loss: 318.82
 ---- batch: 100 ----
mean loss: 315.43
 ---- batch: 110 ----
mean loss: 314.35
train mean loss: 322.09
epoch train time: 0:00:02.174552
elapsed time: 0:05:17.951046
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 00:12:24.350832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.91
 ---- batch: 020 ----
mean loss: 319.36
 ---- batch: 030 ----
mean loss: 320.06
 ---- batch: 040 ----
mean loss: 324.43
 ---- batch: 050 ----
mean loss: 318.65
 ---- batch: 060 ----
mean loss: 317.73
 ---- batch: 070 ----
mean loss: 326.22
 ---- batch: 080 ----
mean loss: 317.40
 ---- batch: 090 ----
mean loss: 323.95
 ---- batch: 100 ----
mean loss: 301.14
 ---- batch: 110 ----
mean loss: 318.82
train mean loss: 319.94
epoch train time: 0:00:02.168337
elapsed time: 0:05:20.119578
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 00:12:26.519364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.79
 ---- batch: 020 ----
mean loss: 326.99
 ---- batch: 030 ----
mean loss: 328.72
 ---- batch: 040 ----
mean loss: 319.47
 ---- batch: 050 ----
mean loss: 315.67
 ---- batch: 060 ----
mean loss: 324.00
 ---- batch: 070 ----
mean loss: 323.72
 ---- batch: 080 ----
mean loss: 322.87
 ---- batch: 090 ----
mean loss: 324.54
 ---- batch: 100 ----
mean loss: 310.01
 ---- batch: 110 ----
mean loss: 316.01
train mean loss: 321.59
epoch train time: 0:00:02.188595
elapsed time: 0:05:22.308341
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 00:12:28.708151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.92
 ---- batch: 020 ----
mean loss: 323.31
 ---- batch: 030 ----
mean loss: 330.51
 ---- batch: 040 ----
mean loss: 329.39
 ---- batch: 050 ----
mean loss: 313.61
 ---- batch: 060 ----
mean loss: 321.47
 ---- batch: 070 ----
mean loss: 315.95
 ---- batch: 080 ----
mean loss: 317.69
 ---- batch: 090 ----
mean loss: 316.76
 ---- batch: 100 ----
mean loss: 316.48
 ---- batch: 110 ----
mean loss: 322.57
train mean loss: 320.08
epoch train time: 0:00:02.187146
elapsed time: 0:05:24.495673
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 00:12:30.895460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.52
 ---- batch: 020 ----
mean loss: 325.83
 ---- batch: 030 ----
mean loss: 326.29
 ---- batch: 040 ----
mean loss: 321.99
 ---- batch: 050 ----
mean loss: 312.35
 ---- batch: 060 ----
mean loss: 319.13
 ---- batch: 070 ----
mean loss: 326.51
 ---- batch: 080 ----
mean loss: 318.89
 ---- batch: 090 ----
mean loss: 317.06
 ---- batch: 100 ----
mean loss: 309.33
 ---- batch: 110 ----
mean loss: 312.27
train mean loss: 320.29
epoch train time: 0:00:02.173669
elapsed time: 0:05:26.669511
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 00:12:33.069325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.51
 ---- batch: 020 ----
mean loss: 316.48
 ---- batch: 030 ----
mean loss: 312.65
 ---- batch: 040 ----
mean loss: 326.91
 ---- batch: 050 ----
mean loss: 318.60
 ---- batch: 060 ----
mean loss: 325.10
 ---- batch: 070 ----
mean loss: 316.11
 ---- batch: 080 ----
mean loss: 322.90
 ---- batch: 090 ----
mean loss: 313.45
 ---- batch: 100 ----
mean loss: 317.70
 ---- batch: 110 ----
mean loss: 327.27
train mean loss: 318.71
epoch train time: 0:00:02.181920
elapsed time: 0:05:28.851634
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 00:12:35.251439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.31
 ---- batch: 020 ----
mean loss: 329.22
 ---- batch: 030 ----
mean loss: 311.99
 ---- batch: 040 ----
mean loss: 323.09
 ---- batch: 050 ----
mean loss: 313.51
 ---- batch: 060 ----
mean loss: 332.99
 ---- batch: 070 ----
mean loss: 305.82
 ---- batch: 080 ----
mean loss: 314.91
 ---- batch: 090 ----
mean loss: 318.53
 ---- batch: 100 ----
mean loss: 322.35
 ---- batch: 110 ----
mean loss: 315.56
train mean loss: 319.98
epoch train time: 0:00:02.177493
elapsed time: 0:05:31.029303
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 00:12:37.429085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.52
 ---- batch: 020 ----
mean loss: 314.75
 ---- batch: 030 ----
mean loss: 330.53
 ---- batch: 040 ----
mean loss: 331.02
 ---- batch: 050 ----
mean loss: 317.64
 ---- batch: 060 ----
mean loss: 313.17
 ---- batch: 070 ----
mean loss: 305.17
 ---- batch: 080 ----
mean loss: 309.99
 ---- batch: 090 ----
mean loss: 327.00
 ---- batch: 100 ----
mean loss: 310.32
 ---- batch: 110 ----
mean loss: 323.12
train mean loss: 317.98
epoch train time: 0:00:02.178110
elapsed time: 0:05:33.207579
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 00:12:39.607360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.44
 ---- batch: 020 ----
mean loss: 325.21
 ---- batch: 030 ----
mean loss: 322.73
 ---- batch: 040 ----
mean loss: 318.65
 ---- batch: 050 ----
mean loss: 321.06
 ---- batch: 060 ----
mean loss: 313.90
 ---- batch: 070 ----
mean loss: 314.62
 ---- batch: 080 ----
mean loss: 324.79
 ---- batch: 090 ----
mean loss: 318.31
 ---- batch: 100 ----
mean loss: 317.05
 ---- batch: 110 ----
mean loss: 308.56
train mean loss: 317.00
epoch train time: 0:00:02.182181
elapsed time: 0:05:35.389946
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 00:12:41.789735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.96
 ---- batch: 020 ----
mean loss: 316.85
 ---- batch: 030 ----
mean loss: 316.87
 ---- batch: 040 ----
mean loss: 316.95
 ---- batch: 050 ----
mean loss: 309.03
 ---- batch: 060 ----
mean loss: 307.02
 ---- batch: 070 ----
mean loss: 322.27
 ---- batch: 080 ----
mean loss: 313.17
 ---- batch: 090 ----
mean loss: 319.97
 ---- batch: 100 ----
mean loss: 312.46
 ---- batch: 110 ----
mean loss: 302.76
train mean loss: 313.92
epoch train time: 0:00:02.176205
elapsed time: 0:05:37.566343
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 00:12:43.966129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.56
 ---- batch: 020 ----
mean loss: 330.36
 ---- batch: 030 ----
mean loss: 312.98
 ---- batch: 040 ----
mean loss: 305.35
 ---- batch: 050 ----
mean loss: 307.11
 ---- batch: 060 ----
mean loss: 314.62
 ---- batch: 070 ----
mean loss: 307.82
 ---- batch: 080 ----
mean loss: 318.41
 ---- batch: 090 ----
mean loss: 312.98
 ---- batch: 100 ----
mean loss: 323.25
 ---- batch: 110 ----
mean loss: 318.66
train mean loss: 315.49
epoch train time: 0:00:02.180811
elapsed time: 0:05:39.747325
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 00:12:46.147112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.61
 ---- batch: 020 ----
mean loss: 313.54
 ---- batch: 030 ----
mean loss: 306.99
 ---- batch: 040 ----
mean loss: 319.10
 ---- batch: 050 ----
mean loss: 314.60
 ---- batch: 060 ----
mean loss: 313.16
 ---- batch: 070 ----
mean loss: 316.25
 ---- batch: 080 ----
mean loss: 325.95
 ---- batch: 090 ----
mean loss: 314.41
 ---- batch: 100 ----
mean loss: 313.46
 ---- batch: 110 ----
mean loss: 312.60
train mean loss: 314.44
epoch train time: 0:00:02.176712
elapsed time: 0:05:41.924194
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 00:12:48.323985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.09
 ---- batch: 020 ----
mean loss: 303.14
 ---- batch: 030 ----
mean loss: 322.58
 ---- batch: 040 ----
mean loss: 314.23
 ---- batch: 050 ----
mean loss: 315.68
 ---- batch: 060 ----
mean loss: 317.34
 ---- batch: 070 ----
mean loss: 319.01
 ---- batch: 080 ----
mean loss: 328.27
 ---- batch: 090 ----
mean loss: 317.58
 ---- batch: 100 ----
mean loss: 322.92
 ---- batch: 110 ----
mean loss: 314.85
train mean loss: 317.76
epoch train time: 0:00:02.176557
elapsed time: 0:05:44.100925
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 00:12:50.500732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.82
 ---- batch: 020 ----
mean loss: 313.91
 ---- batch: 030 ----
mean loss: 322.82
 ---- batch: 040 ----
mean loss: 304.64
 ---- batch: 050 ----
mean loss: 317.36
 ---- batch: 060 ----
mean loss: 311.72
 ---- batch: 070 ----
mean loss: 309.38
 ---- batch: 080 ----
mean loss: 305.95
 ---- batch: 090 ----
mean loss: 309.66
 ---- batch: 100 ----
mean loss: 303.50
 ---- batch: 110 ----
mean loss: 314.77
train mean loss: 310.96
epoch train time: 0:00:02.181052
elapsed time: 0:05:46.282177
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 00:12:52.681972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.78
 ---- batch: 020 ----
mean loss: 314.36
 ---- batch: 030 ----
mean loss: 310.21
 ---- batch: 040 ----
mean loss: 314.47
 ---- batch: 050 ----
mean loss: 308.12
 ---- batch: 060 ----
mean loss: 300.45
 ---- batch: 070 ----
mean loss: 323.79
 ---- batch: 080 ----
mean loss: 310.38
 ---- batch: 090 ----
mean loss: 314.98
 ---- batch: 100 ----
mean loss: 314.93
 ---- batch: 110 ----
mean loss: 309.78
train mean loss: 313.90
epoch train time: 0:00:02.172538
elapsed time: 0:05:48.454936
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 00:12:54.854724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.84
 ---- batch: 020 ----
mean loss: 303.97
 ---- batch: 030 ----
mean loss: 311.29
 ---- batch: 040 ----
mean loss: 309.57
 ---- batch: 050 ----
mean loss: 326.77
 ---- batch: 060 ----
mean loss: 308.08
 ---- batch: 070 ----
mean loss: 311.17
 ---- batch: 080 ----
mean loss: 319.71
 ---- batch: 090 ----
mean loss: 315.21
 ---- batch: 100 ----
mean loss: 319.13
 ---- batch: 110 ----
mean loss: 297.05
train mean loss: 312.24
epoch train time: 0:00:02.176533
elapsed time: 0:05:50.631673
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 00:12:57.031467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.96
 ---- batch: 020 ----
mean loss: 317.55
 ---- batch: 030 ----
mean loss: 316.74
 ---- batch: 040 ----
mean loss: 308.03
 ---- batch: 050 ----
mean loss: 309.75
 ---- batch: 060 ----
mean loss: 311.35
 ---- batch: 070 ----
mean loss: 325.41
 ---- batch: 080 ----
mean loss: 314.82
 ---- batch: 090 ----
mean loss: 313.69
 ---- batch: 100 ----
mean loss: 311.05
 ---- batch: 110 ----
mean loss: 312.09
train mean loss: 313.36
epoch train time: 0:00:02.175058
elapsed time: 0:05:52.806939
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 00:12:59.206712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.39
 ---- batch: 020 ----
mean loss: 307.88
 ---- batch: 030 ----
mean loss: 302.77
 ---- batch: 040 ----
mean loss: 309.28
 ---- batch: 050 ----
mean loss: 300.97
 ---- batch: 060 ----
mean loss: 310.38
 ---- batch: 070 ----
mean loss: 316.11
 ---- batch: 080 ----
mean loss: 316.24
 ---- batch: 090 ----
mean loss: 316.38
 ---- batch: 100 ----
mean loss: 314.91
 ---- batch: 110 ----
mean loss: 317.16
train mean loss: 312.31
epoch train time: 0:00:02.190347
elapsed time: 0:05:54.997435
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 00:13:01.397219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.98
 ---- batch: 020 ----
mean loss: 308.46
 ---- batch: 030 ----
mean loss: 313.19
 ---- batch: 040 ----
mean loss: 308.51
 ---- batch: 050 ----
mean loss: 310.83
 ---- batch: 060 ----
mean loss: 323.89
 ---- batch: 070 ----
mean loss: 305.91
 ---- batch: 080 ----
mean loss: 308.07
 ---- batch: 090 ----
mean loss: 305.35
 ---- batch: 100 ----
mean loss: 302.49
 ---- batch: 110 ----
mean loss: 312.63
train mean loss: 310.34
epoch train time: 0:00:02.182588
elapsed time: 0:05:57.180184
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 00:13:03.579986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.86
 ---- batch: 020 ----
mean loss: 309.66
 ---- batch: 030 ----
mean loss: 311.15
 ---- batch: 040 ----
mean loss: 313.58
 ---- batch: 050 ----
mean loss: 316.62
 ---- batch: 060 ----
mean loss: 320.03
 ---- batch: 070 ----
mean loss: 316.59
 ---- batch: 080 ----
mean loss: 320.52
 ---- batch: 090 ----
mean loss: 318.39
 ---- batch: 100 ----
mean loss: 314.05
 ---- batch: 110 ----
mean loss: 299.68
train mean loss: 312.60
epoch train time: 0:00:02.189374
elapsed time: 0:05:59.369769
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 00:13:05.769563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.66
 ---- batch: 020 ----
mean loss: 306.71
 ---- batch: 030 ----
mean loss: 310.74
 ---- batch: 040 ----
mean loss: 309.90
 ---- batch: 050 ----
mean loss: 321.34
 ---- batch: 060 ----
mean loss: 302.20
 ---- batch: 070 ----
mean loss: 305.58
 ---- batch: 080 ----
mean loss: 317.74
 ---- batch: 090 ----
mean loss: 311.42
 ---- batch: 100 ----
mean loss: 306.70
 ---- batch: 110 ----
mean loss: 305.74
train mean loss: 309.82
epoch train time: 0:00:02.189372
elapsed time: 0:06:01.559349
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 00:13:07.959141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.16
 ---- batch: 020 ----
mean loss: 310.88
 ---- batch: 030 ----
mean loss: 317.64
 ---- batch: 040 ----
mean loss: 307.32
 ---- batch: 050 ----
mean loss: 331.60
 ---- batch: 060 ----
mean loss: 320.17
 ---- batch: 070 ----
mean loss: 315.80
 ---- batch: 080 ----
mean loss: 317.34
 ---- batch: 090 ----
mean loss: 304.50
 ---- batch: 100 ----
mean loss: 306.57
 ---- batch: 110 ----
mean loss: 312.32
train mean loss: 313.19
epoch train time: 0:00:02.192436
elapsed time: 0:06:03.751957
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 00:13:10.151740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.13
 ---- batch: 020 ----
mean loss: 310.52
 ---- batch: 030 ----
mean loss: 314.25
 ---- batch: 040 ----
mean loss: 290.49
 ---- batch: 050 ----
mean loss: 303.46
 ---- batch: 060 ----
mean loss: 313.05
 ---- batch: 070 ----
mean loss: 310.12
 ---- batch: 080 ----
mean loss: 304.76
 ---- batch: 090 ----
mean loss: 309.02
 ---- batch: 100 ----
mean loss: 300.66
 ---- batch: 110 ----
mean loss: 302.18
train mean loss: 306.04
epoch train time: 0:00:02.179593
elapsed time: 0:06:05.931711
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 00:13:12.331498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.12
 ---- batch: 020 ----
mean loss: 301.43
 ---- batch: 030 ----
mean loss: 306.32
 ---- batch: 040 ----
mean loss: 295.82
 ---- batch: 050 ----
mean loss: 304.96
 ---- batch: 060 ----
mean loss: 303.39
 ---- batch: 070 ----
mean loss: 315.03
 ---- batch: 080 ----
mean loss: 303.11
 ---- batch: 090 ----
mean loss: 309.52
 ---- batch: 100 ----
mean loss: 309.69
 ---- batch: 110 ----
mean loss: 312.81
train mean loss: 306.69
epoch train time: 0:00:02.175658
elapsed time: 0:06:08.107596
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 00:13:14.507384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.08
 ---- batch: 020 ----
mean loss: 304.58
 ---- batch: 030 ----
mean loss: 315.28
 ---- batch: 040 ----
mean loss: 324.38
 ---- batch: 050 ----
mean loss: 315.27
 ---- batch: 060 ----
mean loss: 305.29
 ---- batch: 070 ----
mean loss: 304.77
 ---- batch: 080 ----
mean loss: 301.03
 ---- batch: 090 ----
mean loss: 295.30
 ---- batch: 100 ----
mean loss: 305.07
 ---- batch: 110 ----
mean loss: 312.14
train mean loss: 308.52
epoch train time: 0:00:02.172566
elapsed time: 0:06:10.280326
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 00:13:16.680113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.51
 ---- batch: 020 ----
mean loss: 311.06
 ---- batch: 030 ----
mean loss: 302.80
 ---- batch: 040 ----
mean loss: 313.10
 ---- batch: 050 ----
mean loss: 295.07
 ---- batch: 060 ----
mean loss: 305.59
 ---- batch: 070 ----
mean loss: 294.84
 ---- batch: 080 ----
mean loss: 301.08
 ---- batch: 090 ----
mean loss: 308.18
 ---- batch: 100 ----
mean loss: 309.19
 ---- batch: 110 ----
mean loss: 311.82
train mean loss: 305.16
epoch train time: 0:00:02.177917
elapsed time: 0:06:12.458423
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 00:13:18.858234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.77
 ---- batch: 020 ----
mean loss: 300.99
 ---- batch: 030 ----
mean loss: 311.56
 ---- batch: 040 ----
mean loss: 295.33
 ---- batch: 050 ----
mean loss: 311.34
 ---- batch: 060 ----
mean loss: 307.26
 ---- batch: 070 ----
mean loss: 316.29
 ---- batch: 080 ----
mean loss: 298.97
 ---- batch: 090 ----
mean loss: 307.01
 ---- batch: 100 ----
mean loss: 310.81
 ---- batch: 110 ----
mean loss: 310.05
train mean loss: 307.00
epoch train time: 0:00:02.177435
elapsed time: 0:06:14.636062
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 00:13:21.035850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.52
 ---- batch: 020 ----
mean loss: 309.17
 ---- batch: 030 ----
mean loss: 301.04
 ---- batch: 040 ----
mean loss: 307.70
 ---- batch: 050 ----
mean loss: 307.90
 ---- batch: 060 ----
mean loss: 299.92
 ---- batch: 070 ----
mean loss: 315.84
 ---- batch: 080 ----
mean loss: 302.41
 ---- batch: 090 ----
mean loss: 296.68
 ---- batch: 100 ----
mean loss: 315.66
 ---- batch: 110 ----
mean loss: 310.55
train mean loss: 307.53
epoch train time: 0:00:02.177139
elapsed time: 0:06:16.813364
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 00:13:23.213146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.52
 ---- batch: 020 ----
mean loss: 303.80
 ---- batch: 030 ----
mean loss: 307.11
 ---- batch: 040 ----
mean loss: 308.46
 ---- batch: 050 ----
mean loss: 330.06
 ---- batch: 060 ----
mean loss: 302.03
 ---- batch: 070 ----
mean loss: 303.42
 ---- batch: 080 ----
mean loss: 317.57
 ---- batch: 090 ----
mean loss: 309.73
 ---- batch: 100 ----
mean loss: 294.81
 ---- batch: 110 ----
mean loss: 291.02
train mean loss: 306.48
epoch train time: 0:00:02.174385
elapsed time: 0:06:18.987909
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 00:13:25.387700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.67
 ---- batch: 020 ----
mean loss: 311.04
 ---- batch: 030 ----
mean loss: 304.87
 ---- batch: 040 ----
mean loss: 305.40
 ---- batch: 050 ----
mean loss: 316.92
 ---- batch: 060 ----
mean loss: 303.51
 ---- batch: 070 ----
mean loss: 294.21
 ---- batch: 080 ----
mean loss: 303.90
 ---- batch: 090 ----
mean loss: 298.22
 ---- batch: 100 ----
mean loss: 310.34
 ---- batch: 110 ----
mean loss: 303.67
train mean loss: 304.17
epoch train time: 0:00:02.171216
elapsed time: 0:06:21.159296
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 00:13:27.559083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.16
 ---- batch: 020 ----
mean loss: 308.08
 ---- batch: 030 ----
mean loss: 309.80
 ---- batch: 040 ----
mean loss: 300.04
 ---- batch: 050 ----
mean loss: 313.34
 ---- batch: 060 ----
mean loss: 309.69
 ---- batch: 070 ----
mean loss: 302.59
 ---- batch: 080 ----
mean loss: 284.56
 ---- batch: 090 ----
mean loss: 305.12
 ---- batch: 100 ----
mean loss: 309.79
 ---- batch: 110 ----
mean loss: 312.92
train mean loss: 306.32
epoch train time: 0:00:02.177106
elapsed time: 0:06:23.336582
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 00:13:29.736371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.63
 ---- batch: 020 ----
mean loss: 313.76
 ---- batch: 030 ----
mean loss: 313.82
 ---- batch: 040 ----
mean loss: 314.58
 ---- batch: 050 ----
mean loss: 306.19
 ---- batch: 060 ----
mean loss: 303.82
 ---- batch: 070 ----
mean loss: 290.54
 ---- batch: 080 ----
mean loss: 302.68
 ---- batch: 090 ----
mean loss: 303.60
 ---- batch: 100 ----
mean loss: 297.61
 ---- batch: 110 ----
mean loss: 312.47
train mean loss: 305.75
epoch train time: 0:00:02.173995
elapsed time: 0:06:25.510748
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 00:13:31.910556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.43
 ---- batch: 020 ----
mean loss: 308.78
 ---- batch: 030 ----
mean loss: 298.69
 ---- batch: 040 ----
mean loss: 314.60
 ---- batch: 050 ----
mean loss: 306.91
 ---- batch: 060 ----
mean loss: 301.17
 ---- batch: 070 ----
mean loss: 317.47
 ---- batch: 080 ----
mean loss: 294.51
 ---- batch: 090 ----
mean loss: 294.87
 ---- batch: 100 ----
mean loss: 302.15
 ---- batch: 110 ----
mean loss: 298.89
train mean loss: 303.48
epoch train time: 0:00:02.178224
elapsed time: 0:06:27.689181
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 00:13:34.088977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.43
 ---- batch: 020 ----
mean loss: 292.48
 ---- batch: 030 ----
mean loss: 304.65
 ---- batch: 040 ----
mean loss: 303.81
 ---- batch: 050 ----
mean loss: 312.24
 ---- batch: 060 ----
mean loss: 308.22
 ---- batch: 070 ----
mean loss: 305.43
 ---- batch: 080 ----
mean loss: 310.24
 ---- batch: 090 ----
mean loss: 306.72
 ---- batch: 100 ----
mean loss: 298.50
 ---- batch: 110 ----
mean loss: 290.45
train mean loss: 303.55
epoch train time: 0:00:02.176414
elapsed time: 0:06:29.865770
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 00:13:36.265567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.77
 ---- batch: 020 ----
mean loss: 304.68
 ---- batch: 030 ----
mean loss: 294.48
 ---- batch: 040 ----
mean loss: 294.20
 ---- batch: 050 ----
mean loss: 291.59
 ---- batch: 060 ----
mean loss: 298.72
 ---- batch: 070 ----
mean loss: 288.26
 ---- batch: 080 ----
mean loss: 306.00
 ---- batch: 090 ----
mean loss: 297.25
 ---- batch: 100 ----
mean loss: 315.15
 ---- batch: 110 ----
mean loss: 293.71
train mean loss: 298.84
epoch train time: 0:00:02.181662
elapsed time: 0:06:32.047619
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 00:13:38.447409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.89
 ---- batch: 020 ----
mean loss: 305.49
 ---- batch: 030 ----
mean loss: 291.90
 ---- batch: 040 ----
mean loss: 293.36
 ---- batch: 050 ----
mean loss: 301.58
 ---- batch: 060 ----
mean loss: 293.98
 ---- batch: 070 ----
mean loss: 303.96
 ---- batch: 080 ----
mean loss: 295.83
 ---- batch: 090 ----
mean loss: 309.06
 ---- batch: 100 ----
mean loss: 301.74
 ---- batch: 110 ----
mean loss: 311.25
train mean loss: 300.66
epoch train time: 0:00:02.179511
elapsed time: 0:06:34.227303
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 00:13:40.627091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.01
 ---- batch: 020 ----
mean loss: 302.91
 ---- batch: 030 ----
mean loss: 298.89
 ---- batch: 040 ----
mean loss: 303.37
 ---- batch: 050 ----
mean loss: 302.48
 ---- batch: 060 ----
mean loss: 302.72
 ---- batch: 070 ----
mean loss: 295.90
 ---- batch: 080 ----
mean loss: 298.98
 ---- batch: 090 ----
mean loss: 306.01
 ---- batch: 100 ----
mean loss: 308.48
 ---- batch: 110 ----
mean loss: 306.04
train mean loss: 302.09
epoch train time: 0:00:02.173787
elapsed time: 0:06:36.401282
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 00:13:42.801080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.62
 ---- batch: 020 ----
mean loss: 300.56
 ---- batch: 030 ----
mean loss: 296.29
 ---- batch: 040 ----
mean loss: 302.25
 ---- batch: 050 ----
mean loss: 302.88
 ---- batch: 060 ----
mean loss: 299.83
 ---- batch: 070 ----
mean loss: 299.49
 ---- batch: 080 ----
mean loss: 295.12
 ---- batch: 090 ----
mean loss: 301.21
 ---- batch: 100 ----
mean loss: 299.99
 ---- batch: 110 ----
mean loss: 290.76
train mean loss: 298.81
epoch train time: 0:00:02.174187
elapsed time: 0:06:38.575635
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 00:13:44.975420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.06
 ---- batch: 020 ----
mean loss: 306.76
 ---- batch: 030 ----
mean loss: 299.47
 ---- batch: 040 ----
mean loss: 295.04
 ---- batch: 050 ----
mean loss: 281.70
 ---- batch: 060 ----
mean loss: 300.12
 ---- batch: 070 ----
mean loss: 302.29
 ---- batch: 080 ----
mean loss: 297.37
 ---- batch: 090 ----
mean loss: 302.57
 ---- batch: 100 ----
mean loss: 297.80
 ---- batch: 110 ----
mean loss: 294.43
train mean loss: 297.62
epoch train time: 0:00:02.173518
elapsed time: 0:06:40.749311
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 00:13:47.149123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.99
 ---- batch: 020 ----
mean loss: 301.85
 ---- batch: 030 ----
mean loss: 297.97
 ---- batch: 040 ----
mean loss: 293.17
 ---- batch: 050 ----
mean loss: 300.16
 ---- batch: 060 ----
mean loss: 287.26
 ---- batch: 070 ----
mean loss: 310.04
 ---- batch: 080 ----
mean loss: 301.43
 ---- batch: 090 ----
mean loss: 316.48
 ---- batch: 100 ----
mean loss: 302.24
 ---- batch: 110 ----
mean loss: 294.88
train mean loss: 299.12
epoch train time: 0:00:02.177914
elapsed time: 0:06:42.927438
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 00:13:49.327225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.95
 ---- batch: 020 ----
mean loss: 299.99
 ---- batch: 030 ----
mean loss: 302.94
 ---- batch: 040 ----
mean loss: 295.19
 ---- batch: 050 ----
mean loss: 299.69
 ---- batch: 060 ----
mean loss: 298.67
 ---- batch: 070 ----
mean loss: 292.80
 ---- batch: 080 ----
mean loss: 303.57
 ---- batch: 090 ----
mean loss: 312.53
 ---- batch: 100 ----
mean loss: 294.91
 ---- batch: 110 ----
mean loss: 302.42
train mean loss: 299.82
epoch train time: 0:00:02.178582
elapsed time: 0:06:45.106218
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 00:13:51.506008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.31
 ---- batch: 020 ----
mean loss: 299.17
 ---- batch: 030 ----
mean loss: 302.18
 ---- batch: 040 ----
mean loss: 302.84
 ---- batch: 050 ----
mean loss: 286.47
 ---- batch: 060 ----
mean loss: 298.88
 ---- batch: 070 ----
mean loss: 294.28
 ---- batch: 080 ----
mean loss: 298.58
 ---- batch: 090 ----
mean loss: 309.88
 ---- batch: 100 ----
mean loss: 292.74
 ---- batch: 110 ----
mean loss: 299.62
train mean loss: 298.25
epoch train time: 0:00:02.177173
elapsed time: 0:06:47.283580
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 00:13:53.683357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.24
 ---- batch: 020 ----
mean loss: 295.77
 ---- batch: 030 ----
mean loss: 305.35
 ---- batch: 040 ----
mean loss: 295.20
 ---- batch: 050 ----
mean loss: 298.00
 ---- batch: 060 ----
mean loss: 297.59
 ---- batch: 070 ----
mean loss: 291.69
 ---- batch: 080 ----
mean loss: 291.56
 ---- batch: 090 ----
mean loss: 294.50
 ---- batch: 100 ----
mean loss: 294.50
 ---- batch: 110 ----
mean loss: 294.47
train mean loss: 295.68
epoch train time: 0:00:02.175402
elapsed time: 0:06:49.459163
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 00:13:55.858972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.40
 ---- batch: 020 ----
mean loss: 295.29
 ---- batch: 030 ----
mean loss: 288.60
 ---- batch: 040 ----
mean loss: 288.92
 ---- batch: 050 ----
mean loss: 303.11
 ---- batch: 060 ----
mean loss: 289.77
 ---- batch: 070 ----
mean loss: 301.26
 ---- batch: 080 ----
mean loss: 297.39
 ---- batch: 090 ----
mean loss: 303.61
 ---- batch: 100 ----
mean loss: 305.00
 ---- batch: 110 ----
mean loss: 300.07
train mean loss: 298.73
epoch train time: 0:00:02.173046
elapsed time: 0:06:51.632405
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 00:13:58.032193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.94
 ---- batch: 020 ----
mean loss: 299.61
 ---- batch: 030 ----
mean loss: 285.65
 ---- batch: 040 ----
mean loss: 304.13
 ---- batch: 050 ----
mean loss: 298.45
 ---- batch: 060 ----
mean loss: 297.47
 ---- batch: 070 ----
mean loss: 297.62
 ---- batch: 080 ----
mean loss: 301.06
 ---- batch: 090 ----
mean loss: 290.12
 ---- batch: 100 ----
mean loss: 300.08
 ---- batch: 110 ----
mean loss: 295.79
train mean loss: 297.20
epoch train time: 0:00:02.170734
elapsed time: 0:06:53.803313
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 00:14:00.203120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.43
 ---- batch: 020 ----
mean loss: 301.45
 ---- batch: 030 ----
mean loss: 294.63
 ---- batch: 040 ----
mean loss: 281.92
 ---- batch: 050 ----
mean loss: 301.05
 ---- batch: 060 ----
mean loss: 300.33
 ---- batch: 070 ----
mean loss: 292.97
 ---- batch: 080 ----
mean loss: 300.66
 ---- batch: 090 ----
mean loss: 299.82
 ---- batch: 100 ----
mean loss: 292.41
 ---- batch: 110 ----
mean loss: 282.71
train mean loss: 295.68
epoch train time: 0:00:02.167897
elapsed time: 0:06:55.971392
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 00:14:02.371184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.42
 ---- batch: 020 ----
mean loss: 302.68
 ---- batch: 030 ----
mean loss: 300.04
 ---- batch: 040 ----
mean loss: 285.59
 ---- batch: 050 ----
mean loss: 286.49
 ---- batch: 060 ----
mean loss: 294.16
 ---- batch: 070 ----
mean loss: 285.75
 ---- batch: 080 ----
mean loss: 283.16
 ---- batch: 090 ----
mean loss: 307.87
 ---- batch: 100 ----
mean loss: 302.60
 ---- batch: 110 ----
mean loss: 301.94
train mean loss: 294.77
epoch train time: 0:00:02.181096
elapsed time: 0:06:58.152662
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 00:14:04.552448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.35
 ---- batch: 020 ----
mean loss: 289.45
 ---- batch: 030 ----
mean loss: 303.41
 ---- batch: 040 ----
mean loss: 296.46
 ---- batch: 050 ----
mean loss: 292.05
 ---- batch: 060 ----
mean loss: 293.40
 ---- batch: 070 ----
mean loss: 302.20
 ---- batch: 080 ----
mean loss: 298.56
 ---- batch: 090 ----
mean loss: 295.94
 ---- batch: 100 ----
mean loss: 298.58
 ---- batch: 110 ----
mean loss: 297.94
train mean loss: 295.60
epoch train time: 0:00:02.169405
elapsed time: 0:07:00.322233
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 00:14:06.722017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.97
 ---- batch: 020 ----
mean loss: 304.18
 ---- batch: 030 ----
mean loss: 302.38
 ---- batch: 040 ----
mean loss: 294.06
 ---- batch: 050 ----
mean loss: 306.57
 ---- batch: 060 ----
mean loss: 299.43
 ---- batch: 070 ----
mean loss: 290.94
 ---- batch: 080 ----
mean loss: 290.54
 ---- batch: 090 ----
mean loss: 287.90
 ---- batch: 100 ----
mean loss: 295.66
 ---- batch: 110 ----
mean loss: 295.78
train mean loss: 296.47
epoch train time: 0:00:02.171822
elapsed time: 0:07:02.494223
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 00:14:08.894077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.36
 ---- batch: 020 ----
mean loss: 300.90
 ---- batch: 030 ----
mean loss: 300.60
 ---- batch: 040 ----
mean loss: 287.30
 ---- batch: 050 ----
mean loss: 293.09
 ---- batch: 060 ----
mean loss: 288.23
 ---- batch: 070 ----
mean loss: 287.88
 ---- batch: 080 ----
mean loss: 293.53
 ---- batch: 090 ----
mean loss: 287.37
 ---- batch: 100 ----
mean loss: 303.03
 ---- batch: 110 ----
mean loss: 294.97
train mean loss: 292.89
epoch train time: 0:00:02.169063
elapsed time: 0:07:04.663537
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 00:14:11.063322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.10
 ---- batch: 020 ----
mean loss: 298.34
 ---- batch: 030 ----
mean loss: 294.62
 ---- batch: 040 ----
mean loss: 294.38
 ---- batch: 050 ----
mean loss: 289.28
 ---- batch: 060 ----
mean loss: 295.79
 ---- batch: 070 ----
mean loss: 293.57
 ---- batch: 080 ----
mean loss: 297.86
 ---- batch: 090 ----
mean loss: 297.97
 ---- batch: 100 ----
mean loss: 289.25
 ---- batch: 110 ----
mean loss: 284.89
train mean loss: 294.00
epoch train time: 0:00:02.179540
elapsed time: 0:07:06.843267
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 00:14:13.243066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.78
 ---- batch: 020 ----
mean loss: 286.82
 ---- batch: 030 ----
mean loss: 293.34
 ---- batch: 040 ----
mean loss: 303.11
 ---- batch: 050 ----
mean loss: 295.43
 ---- batch: 060 ----
mean loss: 290.99
 ---- batch: 070 ----
mean loss: 292.51
 ---- batch: 080 ----
mean loss: 292.06
 ---- batch: 090 ----
mean loss: 298.44
 ---- batch: 100 ----
mean loss: 295.26
 ---- batch: 110 ----
mean loss: 294.97
train mean loss: 292.94
epoch train time: 0:00:02.170323
elapsed time: 0:07:09.013784
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 00:14:15.413600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.24
 ---- batch: 020 ----
mean loss: 290.61
 ---- batch: 030 ----
mean loss: 297.99
 ---- batch: 040 ----
mean loss: 292.66
 ---- batch: 050 ----
mean loss: 299.50
 ---- batch: 060 ----
mean loss: 298.73
 ---- batch: 070 ----
mean loss: 288.90
 ---- batch: 080 ----
mean loss: 291.60
 ---- batch: 090 ----
mean loss: 284.97
 ---- batch: 100 ----
mean loss: 288.53
 ---- batch: 110 ----
mean loss: 292.62
train mean loss: 292.03
epoch train time: 0:00:02.172489
elapsed time: 0:07:11.186476
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 00:14:17.586264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.92
 ---- batch: 020 ----
mean loss: 283.17
 ---- batch: 030 ----
mean loss: 295.52
 ---- batch: 040 ----
mean loss: 301.97
 ---- batch: 050 ----
mean loss: 293.44
 ---- batch: 060 ----
mean loss: 288.08
 ---- batch: 070 ----
mean loss: 302.71
 ---- batch: 080 ----
mean loss: 298.12
 ---- batch: 090 ----
mean loss: 289.79
 ---- batch: 100 ----
mean loss: 289.93
 ---- batch: 110 ----
mean loss: 287.19
train mean loss: 292.61
epoch train time: 0:00:02.171577
elapsed time: 0:07:13.358256
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 00:14:19.758040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.60
 ---- batch: 020 ----
mean loss: 295.99
 ---- batch: 030 ----
mean loss: 295.89
 ---- batch: 040 ----
mean loss: 283.05
 ---- batch: 050 ----
mean loss: 297.90
 ---- batch: 060 ----
mean loss: 288.61
 ---- batch: 070 ----
mean loss: 289.15
 ---- batch: 080 ----
mean loss: 287.93
 ---- batch: 090 ----
mean loss: 285.74
 ---- batch: 100 ----
mean loss: 289.40
 ---- batch: 110 ----
mean loss: 292.47
train mean loss: 291.91
epoch train time: 0:00:02.168960
elapsed time: 0:07:15.527374
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 00:14:21.927170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.98
 ---- batch: 020 ----
mean loss: 283.80
 ---- batch: 030 ----
mean loss: 289.56
 ---- batch: 040 ----
mean loss: 294.88
 ---- batch: 050 ----
mean loss: 299.64
 ---- batch: 060 ----
mean loss: 296.07
 ---- batch: 070 ----
mean loss: 279.16
 ---- batch: 080 ----
mean loss: 292.64
 ---- batch: 090 ----
mean loss: 298.21
 ---- batch: 100 ----
mean loss: 285.77
 ---- batch: 110 ----
mean loss: 286.60
train mean loss: 290.22
epoch train time: 0:00:02.171046
elapsed time: 0:07:17.698613
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 00:14:24.098402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.14
 ---- batch: 020 ----
mean loss: 299.12
 ---- batch: 030 ----
mean loss: 298.99
 ---- batch: 040 ----
mean loss: 280.30
 ---- batch: 050 ----
mean loss: 290.35
 ---- batch: 060 ----
mean loss: 291.68
 ---- batch: 070 ----
mean loss: 295.17
 ---- batch: 080 ----
mean loss: 287.72
 ---- batch: 090 ----
mean loss: 294.90
 ---- batch: 100 ----
mean loss: 293.35
 ---- batch: 110 ----
mean loss: 286.00
train mean loss: 292.05
epoch train time: 0:00:02.172979
elapsed time: 0:07:19.871779
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 00:14:26.271563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.29
 ---- batch: 020 ----
mean loss: 290.32
 ---- batch: 030 ----
mean loss: 304.50
 ---- batch: 040 ----
mean loss: 299.16
 ---- batch: 050 ----
mean loss: 290.02
 ---- batch: 060 ----
mean loss: 288.60
 ---- batch: 070 ----
mean loss: 277.01
 ---- batch: 080 ----
mean loss: 290.36
 ---- batch: 090 ----
mean loss: 292.16
 ---- batch: 100 ----
mean loss: 289.95
 ---- batch: 110 ----
mean loss: 289.86
train mean loss: 291.24
epoch train time: 0:00:02.183475
elapsed time: 0:07:22.055447
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 00:14:28.455233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.24
 ---- batch: 020 ----
mean loss: 282.63
 ---- batch: 030 ----
mean loss: 303.19
 ---- batch: 040 ----
mean loss: 286.27
 ---- batch: 050 ----
mean loss: 281.23
 ---- batch: 060 ----
mean loss: 298.88
 ---- batch: 070 ----
mean loss: 300.02
 ---- batch: 080 ----
mean loss: 287.72
 ---- batch: 090 ----
mean loss: 288.57
 ---- batch: 100 ----
mean loss: 289.51
 ---- batch: 110 ----
mean loss: 294.24
train mean loss: 289.55
epoch train time: 0:00:02.184036
elapsed time: 0:07:24.239649
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 00:14:30.639459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.55
 ---- batch: 020 ----
mean loss: 281.32
 ---- batch: 030 ----
mean loss: 284.14
 ---- batch: 040 ----
mean loss: 292.30
 ---- batch: 050 ----
mean loss: 297.96
 ---- batch: 060 ----
mean loss: 296.04
 ---- batch: 070 ----
mean loss: 294.20
 ---- batch: 080 ----
mean loss: 289.29
 ---- batch: 090 ----
mean loss: 299.69
 ---- batch: 100 ----
mean loss: 289.42
 ---- batch: 110 ----
mean loss: 291.82
train mean loss: 291.21
epoch train time: 0:00:02.182783
elapsed time: 0:07:26.422650
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 00:14:32.822450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.99
 ---- batch: 020 ----
mean loss: 284.52
 ---- batch: 030 ----
mean loss: 298.13
 ---- batch: 040 ----
mean loss: 290.94
 ---- batch: 050 ----
mean loss: 302.44
 ---- batch: 060 ----
mean loss: 310.17
 ---- batch: 070 ----
mean loss: 281.86
 ---- batch: 080 ----
mean loss: 300.14
 ---- batch: 090 ----
mean loss: 284.04
 ---- batch: 100 ----
mean loss: 280.67
 ---- batch: 110 ----
mean loss: 289.62
train mean loss: 292.89
epoch train time: 0:00:02.187400
elapsed time: 0:07:28.610250
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 00:14:35.010035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.92
 ---- batch: 020 ----
mean loss: 287.00
 ---- batch: 030 ----
mean loss: 291.81
 ---- batch: 040 ----
mean loss: 288.86
 ---- batch: 050 ----
mean loss: 298.56
 ---- batch: 060 ----
mean loss: 274.78
 ---- batch: 070 ----
mean loss: 285.51
 ---- batch: 080 ----
mean loss: 300.15
 ---- batch: 090 ----
mean loss: 277.54
 ---- batch: 100 ----
mean loss: 291.07
 ---- batch: 110 ----
mean loss: 295.68
train mean loss: 289.25
epoch train time: 0:00:02.189512
elapsed time: 0:07:30.799920
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 00:14:37.199703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.38
 ---- batch: 020 ----
mean loss: 301.78
 ---- batch: 030 ----
mean loss: 296.05
 ---- batch: 040 ----
mean loss: 289.84
 ---- batch: 050 ----
mean loss: 275.47
 ---- batch: 060 ----
mean loss: 281.77
 ---- batch: 070 ----
mean loss: 296.20
 ---- batch: 080 ----
mean loss: 279.91
 ---- batch: 090 ----
mean loss: 292.76
 ---- batch: 100 ----
mean loss: 295.56
 ---- batch: 110 ----
mean loss: 282.28
train mean loss: 288.91
epoch train time: 0:00:02.197045
elapsed time: 0:07:32.997161
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 00:14:39.396967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.07
 ---- batch: 020 ----
mean loss: 281.07
 ---- batch: 030 ----
mean loss: 302.35
 ---- batch: 040 ----
mean loss: 291.84
 ---- batch: 050 ----
mean loss: 278.34
 ---- batch: 060 ----
mean loss: 285.94
 ---- batch: 070 ----
mean loss: 288.17
 ---- batch: 080 ----
mean loss: 294.44
 ---- batch: 090 ----
mean loss: 277.54
 ---- batch: 100 ----
mean loss: 280.00
 ---- batch: 110 ----
mean loss: 295.06
train mean loss: 287.00
epoch train time: 0:00:02.184968
elapsed time: 0:07:35.182366
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 00:14:41.582151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.76
 ---- batch: 020 ----
mean loss: 286.82
 ---- batch: 030 ----
mean loss: 288.58
 ---- batch: 040 ----
mean loss: 298.01
 ---- batch: 050 ----
mean loss: 284.70
 ---- batch: 060 ----
mean loss: 284.10
 ---- batch: 070 ----
mean loss: 291.03
 ---- batch: 080 ----
mean loss: 292.50
 ---- batch: 090 ----
mean loss: 289.87
 ---- batch: 100 ----
mean loss: 280.46
 ---- batch: 110 ----
mean loss: 282.06
train mean loss: 287.92
epoch train time: 0:00:02.181020
elapsed time: 0:07:37.363568
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 00:14:43.763380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.08
 ---- batch: 020 ----
mean loss: 295.27
 ---- batch: 030 ----
mean loss: 292.93
 ---- batch: 040 ----
mean loss: 283.28
 ---- batch: 050 ----
mean loss: 292.08
 ---- batch: 060 ----
mean loss: 281.77
 ---- batch: 070 ----
mean loss: 297.10
 ---- batch: 080 ----
mean loss: 289.55
 ---- batch: 090 ----
mean loss: 283.79
 ---- batch: 100 ----
mean loss: 282.35
 ---- batch: 110 ----
mean loss: 280.03
train mean loss: 287.47
epoch train time: 0:00:02.181030
elapsed time: 0:07:39.544826
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 00:14:45.944611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.34
 ---- batch: 020 ----
mean loss: 284.40
 ---- batch: 030 ----
mean loss: 293.62
 ---- batch: 040 ----
mean loss: 290.17
 ---- batch: 050 ----
mean loss: 283.69
 ---- batch: 060 ----
mean loss: 291.24
 ---- batch: 070 ----
mean loss: 277.87
 ---- batch: 080 ----
mean loss: 287.30
 ---- batch: 090 ----
mean loss: 285.77
 ---- batch: 100 ----
mean loss: 285.19
 ---- batch: 110 ----
mean loss: 285.86
train mean loss: 287.17
epoch train time: 0:00:02.189731
elapsed time: 0:07:41.734747
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 00:14:48.134535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.86
 ---- batch: 020 ----
mean loss: 286.47
 ---- batch: 030 ----
mean loss: 292.47
 ---- batch: 040 ----
mean loss: 275.21
 ---- batch: 050 ----
mean loss: 279.21
 ---- batch: 060 ----
mean loss: 297.84
 ---- batch: 070 ----
mean loss: 288.44
 ---- batch: 080 ----
mean loss: 282.41
 ---- batch: 090 ----
mean loss: 284.46
 ---- batch: 100 ----
mean loss: 282.25
 ---- batch: 110 ----
mean loss: 287.60
train mean loss: 285.47
epoch train time: 0:00:02.182615
elapsed time: 0:07:43.917530
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 00:14:50.317315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.68
 ---- batch: 020 ----
mean loss: 283.28
 ---- batch: 030 ----
mean loss: 296.45
 ---- batch: 040 ----
mean loss: 286.22
 ---- batch: 050 ----
mean loss: 298.71
 ---- batch: 060 ----
mean loss: 292.13
 ---- batch: 070 ----
mean loss: 284.89
 ---- batch: 080 ----
mean loss: 282.42
 ---- batch: 090 ----
mean loss: 289.24
 ---- batch: 100 ----
mean loss: 282.50
 ---- batch: 110 ----
mean loss: 295.40
train mean loss: 289.32
epoch train time: 0:00:02.181692
elapsed time: 0:07:46.099412
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 00:14:52.499200
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 273.11
 ---- batch: 020 ----
mean loss: 280.14
 ---- batch: 030 ----
mean loss: 279.36
 ---- batch: 040 ----
mean loss: 284.36
 ---- batch: 050 ----
mean loss: 276.43
 ---- batch: 060 ----
mean loss: 279.62
 ---- batch: 070 ----
mean loss: 272.01
 ---- batch: 080 ----
mean loss: 286.14
 ---- batch: 090 ----
mean loss: 287.20
 ---- batch: 100 ----
mean loss: 279.18
 ---- batch: 110 ----
mean loss: 278.24
train mean loss: 279.06
epoch train time: 0:00:02.190951
elapsed time: 0:07:48.290550
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 00:14:54.690325
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.24
 ---- batch: 020 ----
mean loss: 274.84
 ---- batch: 030 ----
mean loss: 263.66
 ---- batch: 040 ----
mean loss: 282.51
 ---- batch: 050 ----
mean loss: 275.27
 ---- batch: 060 ----
mean loss: 289.15
 ---- batch: 070 ----
mean loss: 275.74
 ---- batch: 080 ----
mean loss: 282.25
 ---- batch: 090 ----
mean loss: 285.69
 ---- batch: 100 ----
mean loss: 273.31
 ---- batch: 110 ----
mean loss: 271.71
train mean loss: 277.73
epoch train time: 0:00:02.186227
elapsed time: 0:07:50.476946
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 00:14:56.876730
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.12
 ---- batch: 020 ----
mean loss: 273.61
 ---- batch: 030 ----
mean loss: 283.97
 ---- batch: 040 ----
mean loss: 274.97
 ---- batch: 050 ----
mean loss: 280.59
 ---- batch: 060 ----
mean loss: 278.86
 ---- batch: 070 ----
mean loss: 269.19
 ---- batch: 080 ----
mean loss: 284.95
 ---- batch: 090 ----
mean loss: 273.37
 ---- batch: 100 ----
mean loss: 275.39
 ---- batch: 110 ----
mean loss: 276.92
train mean loss: 276.94
epoch train time: 0:00:02.180344
elapsed time: 0:07:52.657452
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 00:14:59.057255
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.96
 ---- batch: 020 ----
mean loss: 280.07
 ---- batch: 030 ----
mean loss: 276.41
 ---- batch: 040 ----
mean loss: 272.05
 ---- batch: 050 ----
mean loss: 277.72
 ---- batch: 060 ----
mean loss: 277.82
 ---- batch: 070 ----
mean loss: 280.48
 ---- batch: 080 ----
mean loss: 283.73
 ---- batch: 090 ----
mean loss: 279.88
 ---- batch: 100 ----
mean loss: 274.05
 ---- batch: 110 ----
mean loss: 275.65
train mean loss: 277.95
epoch train time: 0:00:02.185136
elapsed time: 0:07:54.842792
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 00:15:01.242578
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.77
 ---- batch: 020 ----
mean loss: 277.05
 ---- batch: 030 ----
mean loss: 278.61
 ---- batch: 040 ----
mean loss: 274.22
 ---- batch: 050 ----
mean loss: 280.70
 ---- batch: 060 ----
mean loss: 282.76
 ---- batch: 070 ----
mean loss: 288.18
 ---- batch: 080 ----
mean loss: 278.78
 ---- batch: 090 ----
mean loss: 277.58
 ---- batch: 100 ----
mean loss: 272.46
 ---- batch: 110 ----
mean loss: 282.37
train mean loss: 279.72
epoch train time: 0:00:02.179532
elapsed time: 0:07:57.022503
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 00:15:03.422290
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.31
 ---- batch: 020 ----
mean loss: 274.96
 ---- batch: 030 ----
mean loss: 270.19
 ---- batch: 040 ----
mean loss: 281.33
 ---- batch: 050 ----
mean loss: 273.07
 ---- batch: 060 ----
mean loss: 277.88
 ---- batch: 070 ----
mean loss: 282.55
 ---- batch: 080 ----
mean loss: 286.66
 ---- batch: 090 ----
mean loss: 277.70
 ---- batch: 100 ----
mean loss: 269.48
 ---- batch: 110 ----
mean loss: 287.72
train mean loss: 278.74
epoch train time: 0:00:02.185761
elapsed time: 0:07:59.208435
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 00:15:05.608221
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.85
 ---- batch: 020 ----
mean loss: 277.98
 ---- batch: 030 ----
mean loss: 275.96
 ---- batch: 040 ----
mean loss: 283.52
 ---- batch: 050 ----
mean loss: 266.49
 ---- batch: 060 ----
mean loss: 272.59
 ---- batch: 070 ----
mean loss: 266.60
 ---- batch: 080 ----
mean loss: 280.22
 ---- batch: 090 ----
mean loss: 281.04
 ---- batch: 100 ----
mean loss: 291.27
 ---- batch: 110 ----
mean loss: 274.24
train mean loss: 276.87
epoch train time: 0:00:02.185353
elapsed time: 0:08:01.394046
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 00:15:07.793843
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 272.84
 ---- batch: 020 ----
mean loss: 278.33
 ---- batch: 030 ----
mean loss: 279.89
 ---- batch: 040 ----
mean loss: 280.45
 ---- batch: 050 ----
mean loss: 272.79
 ---- batch: 060 ----
mean loss: 299.01
 ---- batch: 070 ----
mean loss: 282.31
 ---- batch: 080 ----
mean loss: 278.29
 ---- batch: 090 ----
mean loss: 270.16
 ---- batch: 100 ----
mean loss: 276.39
 ---- batch: 110 ----
mean loss: 276.40
train mean loss: 278.27
epoch train time: 0:00:02.184777
elapsed time: 0:08:03.578999
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 00:15:09.978785
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.89
 ---- batch: 020 ----
mean loss: 268.88
 ---- batch: 030 ----
mean loss: 288.66
 ---- batch: 040 ----
mean loss: 276.94
 ---- batch: 050 ----
mean loss: 272.82
 ---- batch: 060 ----
mean loss: 278.77
 ---- batch: 070 ----
mean loss: 274.41
 ---- batch: 080 ----
mean loss: 273.16
 ---- batch: 090 ----
mean loss: 284.81
 ---- batch: 100 ----
mean loss: 275.46
 ---- batch: 110 ----
mean loss: 293.21
train mean loss: 278.20
epoch train time: 0:00:02.179076
elapsed time: 0:08:05.758242
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 00:15:12.158048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.43
 ---- batch: 020 ----
mean loss: 277.47
 ---- batch: 030 ----
mean loss: 277.34
 ---- batch: 040 ----
mean loss: 269.10
 ---- batch: 050 ----
mean loss: 288.34
 ---- batch: 060 ----
mean loss: 278.30
 ---- batch: 070 ----
mean loss: 274.89
 ---- batch: 080 ----
mean loss: 272.93
 ---- batch: 090 ----
mean loss: 269.77
 ---- batch: 100 ----
mean loss: 269.83
 ---- batch: 110 ----
mean loss: 285.82
train mean loss: 277.78
epoch train time: 0:00:02.168570
elapsed time: 0:08:07.926994
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 00:15:14.326780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.32
 ---- batch: 020 ----
mean loss: 281.08
 ---- batch: 030 ----
mean loss: 286.64
 ---- batch: 040 ----
mean loss: 281.49
 ---- batch: 050 ----
mean loss: 285.12
 ---- batch: 060 ----
mean loss: 279.22
 ---- batch: 070 ----
mean loss: 270.02
 ---- batch: 080 ----
mean loss: 265.82
 ---- batch: 090 ----
mean loss: 275.62
 ---- batch: 100 ----
mean loss: 273.50
 ---- batch: 110 ----
mean loss: 280.15
train mean loss: 278.16
epoch train time: 0:00:02.171050
elapsed time: 0:08:10.098214
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 00:15:16.498007
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.36
 ---- batch: 020 ----
mean loss: 274.32
 ---- batch: 030 ----
mean loss: 270.05
 ---- batch: 040 ----
mean loss: 279.45
 ---- batch: 050 ----
mean loss: 280.00
 ---- batch: 060 ----
mean loss: 281.46
 ---- batch: 070 ----
mean loss: 285.13
 ---- batch: 080 ----
mean loss: 283.09
 ---- batch: 090 ----
mean loss: 276.75
 ---- batch: 100 ----
mean loss: 274.82
 ---- batch: 110 ----
mean loss: 279.57
train mean loss: 278.47
epoch train time: 0:00:02.177164
elapsed time: 0:08:12.275553
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 00:15:18.675344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.90
 ---- batch: 020 ----
mean loss: 280.45
 ---- batch: 030 ----
mean loss: 274.37
 ---- batch: 040 ----
mean loss: 288.23
 ---- batch: 050 ----
mean loss: 268.39
 ---- batch: 060 ----
mean loss: 281.45
 ---- batch: 070 ----
mean loss: 274.03
 ---- batch: 080 ----
mean loss: 274.60
 ---- batch: 090 ----
mean loss: 269.98
 ---- batch: 100 ----
mean loss: 281.97
 ---- batch: 110 ----
mean loss: 287.35
train mean loss: 278.13
epoch train time: 0:00:02.168263
elapsed time: 0:08:14.443979
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 00:15:20.843763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 270.25
 ---- batch: 020 ----
mean loss: 283.43
 ---- batch: 030 ----
mean loss: 279.60
 ---- batch: 040 ----
mean loss: 263.84
 ---- batch: 050 ----
mean loss: 281.91
 ---- batch: 060 ----
mean loss: 277.13
 ---- batch: 070 ----
mean loss: 289.78
 ---- batch: 080 ----
mean loss: 280.32
 ---- batch: 090 ----
mean loss: 278.20
 ---- batch: 100 ----
mean loss: 278.34
 ---- batch: 110 ----
mean loss: 278.26
train mean loss: 278.20
epoch train time: 0:00:02.162580
elapsed time: 0:08:16.606723
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 00:15:23.006517
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.89
 ---- batch: 020 ----
mean loss: 280.81
 ---- batch: 030 ----
mean loss: 292.54
 ---- batch: 040 ----
mean loss: 281.69
 ---- batch: 050 ----
mean loss: 275.11
 ---- batch: 060 ----
mean loss: 271.58
 ---- batch: 070 ----
mean loss: 286.42
 ---- batch: 080 ----
mean loss: 272.89
 ---- batch: 090 ----
mean loss: 264.00
 ---- batch: 100 ----
mean loss: 272.66
 ---- batch: 110 ----
mean loss: 270.16
train mean loss: 277.44
epoch train time: 0:00:02.168566
elapsed time: 0:08:18.775484
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 00:15:25.175271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.17
 ---- batch: 020 ----
mean loss: 274.54
 ---- batch: 030 ----
mean loss: 279.76
 ---- batch: 040 ----
mean loss: 279.93
 ---- batch: 050 ----
mean loss: 278.91
 ---- batch: 060 ----
mean loss: 273.14
 ---- batch: 070 ----
mean loss: 270.71
 ---- batch: 080 ----
mean loss: 289.07
 ---- batch: 090 ----
mean loss: 275.77
 ---- batch: 100 ----
mean loss: 281.85
 ---- batch: 110 ----
mean loss: 278.06
train mean loss: 277.71
epoch train time: 0:00:02.164582
elapsed time: 0:08:20.940219
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 00:15:27.340020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 267.06
 ---- batch: 020 ----
mean loss: 267.30
 ---- batch: 030 ----
mean loss: 283.39
 ---- batch: 040 ----
mean loss: 278.60
 ---- batch: 050 ----
mean loss: 281.55
 ---- batch: 060 ----
mean loss: 287.49
 ---- batch: 070 ----
mean loss: 279.83
 ---- batch: 080 ----
mean loss: 277.08
 ---- batch: 090 ----
mean loss: 276.72
 ---- batch: 100 ----
mean loss: 285.98
 ---- batch: 110 ----
mean loss: 277.49
train mean loss: 278.05
epoch train time: 0:00:02.171559
elapsed time: 0:08:23.111967
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 00:15:29.511753
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 279.13
 ---- batch: 020 ----
mean loss: 286.56
 ---- batch: 030 ----
mean loss: 278.12
 ---- batch: 040 ----
mean loss: 282.87
 ---- batch: 050 ----
mean loss: 277.45
 ---- batch: 060 ----
mean loss: 281.92
 ---- batch: 070 ----
mean loss: 277.54
 ---- batch: 080 ----
mean loss: 269.53
 ---- batch: 090 ----
mean loss: 274.51
 ---- batch: 100 ----
mean loss: 280.02
 ---- batch: 110 ----
mean loss: 272.66
train mean loss: 277.87
epoch train time: 0:00:02.166348
elapsed time: 0:08:25.278480
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 00:15:31.678279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 273.58
 ---- batch: 020 ----
mean loss: 274.75
 ---- batch: 030 ----
mean loss: 277.77
 ---- batch: 040 ----
mean loss: 276.77
 ---- batch: 050 ----
mean loss: 281.93
 ---- batch: 060 ----
mean loss: 270.30
 ---- batch: 070 ----
mean loss: 275.68
 ---- batch: 080 ----
mean loss: 286.34
 ---- batch: 090 ----
mean loss: 274.12
 ---- batch: 100 ----
mean loss: 280.75
 ---- batch: 110 ----
mean loss: 273.47
train mean loss: 276.84
epoch train time: 0:00:02.169024
elapsed time: 0:08:27.447692
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 00:15:33.847475
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.59
 ---- batch: 020 ----
mean loss: 276.73
 ---- batch: 030 ----
mean loss: 282.52
 ---- batch: 040 ----
mean loss: 279.27
 ---- batch: 050 ----
mean loss: 286.12
 ---- batch: 060 ----
mean loss: 270.69
 ---- batch: 070 ----
mean loss: 277.67
 ---- batch: 080 ----
mean loss: 282.16
 ---- batch: 090 ----
mean loss: 281.02
 ---- batch: 100 ----
mean loss: 275.42
 ---- batch: 110 ----
mean loss: 279.68
train mean loss: 278.33
epoch train time: 0:00:02.170406
elapsed time: 0:08:29.618255
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 00:15:36.018052
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.85
 ---- batch: 020 ----
mean loss: 280.80
 ---- batch: 030 ----
mean loss: 283.02
 ---- batch: 040 ----
mean loss: 278.06
 ---- batch: 050 ----
mean loss: 269.16
 ---- batch: 060 ----
mean loss: 270.57
 ---- batch: 070 ----
mean loss: 283.59
 ---- batch: 080 ----
mean loss: 280.46
 ---- batch: 090 ----
mean loss: 273.88
 ---- batch: 100 ----
mean loss: 276.57
 ---- batch: 110 ----
mean loss: 275.77
train mean loss: 276.41
epoch train time: 0:00:02.164347
elapsed time: 0:08:31.782783
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 00:15:38.182569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.32
 ---- batch: 020 ----
mean loss: 276.66
 ---- batch: 030 ----
mean loss: 283.84
 ---- batch: 040 ----
mean loss: 272.43
 ---- batch: 050 ----
mean loss: 282.35
 ---- batch: 060 ----
mean loss: 277.18
 ---- batch: 070 ----
mean loss: 273.70
 ---- batch: 080 ----
mean loss: 275.29
 ---- batch: 090 ----
mean loss: 279.03
 ---- batch: 100 ----
mean loss: 277.18
 ---- batch: 110 ----
mean loss: 276.68
train mean loss: 278.21
epoch train time: 0:00:02.159628
elapsed time: 0:08:33.942604
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 00:15:40.342419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.72
 ---- batch: 020 ----
mean loss: 274.45
 ---- batch: 030 ----
mean loss: 272.25
 ---- batch: 040 ----
mean loss: 276.79
 ---- batch: 050 ----
mean loss: 280.05
 ---- batch: 060 ----
mean loss: 271.23
 ---- batch: 070 ----
mean loss: 283.99
 ---- batch: 080 ----
mean loss: 271.23
 ---- batch: 090 ----
mean loss: 278.35
 ---- batch: 100 ----
mean loss: 287.35
 ---- batch: 110 ----
mean loss: 276.59
train mean loss: 277.25
epoch train time: 0:00:02.163315
elapsed time: 0:08:36.106125
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 00:15:42.505911
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.32
 ---- batch: 020 ----
mean loss: 283.45
 ---- batch: 030 ----
mean loss: 269.55
 ---- batch: 040 ----
mean loss: 272.36
 ---- batch: 050 ----
mean loss: 281.62
 ---- batch: 060 ----
mean loss: 287.62
 ---- batch: 070 ----
mean loss: 275.09
 ---- batch: 080 ----
mean loss: 280.98
 ---- batch: 090 ----
mean loss: 277.85
 ---- batch: 100 ----
mean loss: 263.91
 ---- batch: 110 ----
mean loss: 273.26
train mean loss: 277.03
epoch train time: 0:00:02.156367
elapsed time: 0:08:38.262675
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 00:15:44.662469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.56
 ---- batch: 020 ----
mean loss: 273.95
 ---- batch: 030 ----
mean loss: 283.60
 ---- batch: 040 ----
mean loss: 283.12
 ---- batch: 050 ----
mean loss: 269.31
 ---- batch: 060 ----
mean loss: 277.77
 ---- batch: 070 ----
mean loss: 269.90
 ---- batch: 080 ----
mean loss: 270.04
 ---- batch: 090 ----
mean loss: 278.48
 ---- batch: 100 ----
mean loss: 279.85
 ---- batch: 110 ----
mean loss: 269.14
train mean loss: 276.27
epoch train time: 0:00:02.173882
elapsed time: 0:08:40.436740
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 00:15:46.836526
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 267.79
 ---- batch: 020 ----
mean loss: 285.50
 ---- batch: 030 ----
mean loss: 279.23
 ---- batch: 040 ----
mean loss: 271.63
 ---- batch: 050 ----
mean loss: 275.58
 ---- batch: 060 ----
mean loss: 262.54
 ---- batch: 070 ----
mean loss: 287.50
 ---- batch: 080 ----
mean loss: 276.62
 ---- batch: 090 ----
mean loss: 284.03
 ---- batch: 100 ----
mean loss: 286.49
 ---- batch: 110 ----
mean loss: 277.32
train mean loss: 277.89
epoch train time: 0:00:02.175350
elapsed time: 0:08:42.612293
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 00:15:49.012105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.95
 ---- batch: 020 ----
mean loss: 271.72
 ---- batch: 030 ----
mean loss: 282.16
 ---- batch: 040 ----
mean loss: 273.72
 ---- batch: 050 ----
mean loss: 268.67
 ---- batch: 060 ----
mean loss: 279.77
 ---- batch: 070 ----
mean loss: 295.90
 ---- batch: 080 ----
mean loss: 281.30
 ---- batch: 090 ----
mean loss: 277.48
 ---- batch: 100 ----
mean loss: 279.53
 ---- batch: 110 ----
mean loss: 269.58
train mean loss: 276.90
epoch train time: 0:00:02.182577
elapsed time: 0:08:44.795082
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 00:15:51.194872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 269.49
 ---- batch: 020 ----
mean loss: 277.49
 ---- batch: 030 ----
mean loss: 267.53
 ---- batch: 040 ----
mean loss: 272.54
 ---- batch: 050 ----
mean loss: 278.59
 ---- batch: 060 ----
mean loss: 278.28
 ---- batch: 070 ----
mean loss: 276.46
 ---- batch: 080 ----
mean loss: 280.25
 ---- batch: 090 ----
mean loss: 274.73
 ---- batch: 100 ----
mean loss: 269.40
 ---- batch: 110 ----
mean loss: 277.13
train mean loss: 274.81
epoch train time: 0:00:02.180066
elapsed time: 0:08:46.975335
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 00:15:53.375122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.98
 ---- batch: 020 ----
mean loss: 282.19
 ---- batch: 030 ----
mean loss: 290.73
 ---- batch: 040 ----
mean loss: 277.62
 ---- batch: 050 ----
mean loss: 279.05
 ---- batch: 060 ----
mean loss: 277.55
 ---- batch: 070 ----
mean loss: 278.74
 ---- batch: 080 ----
mean loss: 269.31
 ---- batch: 090 ----
mean loss: 273.54
 ---- batch: 100 ----
mean loss: 279.69
 ---- batch: 110 ----
mean loss: 274.11
train mean loss: 277.77
epoch train time: 0:00:02.183120
elapsed time: 0:08:49.158624
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 00:15:55.558428
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 273.11
 ---- batch: 020 ----
mean loss: 275.11
 ---- batch: 030 ----
mean loss: 276.33
 ---- batch: 040 ----
mean loss: 285.67
 ---- batch: 050 ----
mean loss: 276.39
 ---- batch: 060 ----
mean loss: 273.48
 ---- batch: 070 ----
mean loss: 275.81
 ---- batch: 080 ----
mean loss: 277.92
 ---- batch: 090 ----
mean loss: 275.45
 ---- batch: 100 ----
mean loss: 265.94
 ---- batch: 110 ----
mean loss: 277.70
train mean loss: 276.02
epoch train time: 0:00:02.180565
elapsed time: 0:08:51.339401
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 00:15:57.739198
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 267.28
 ---- batch: 020 ----
mean loss: 269.79
 ---- batch: 030 ----
mean loss: 271.09
 ---- batch: 040 ----
mean loss: 276.80
 ---- batch: 050 ----
mean loss: 296.23
 ---- batch: 060 ----
mean loss: 275.20
 ---- batch: 070 ----
mean loss: 274.48
 ---- batch: 080 ----
mean loss: 275.33
 ---- batch: 090 ----
mean loss: 276.29
 ---- batch: 100 ----
mean loss: 279.24
 ---- batch: 110 ----
mean loss: 279.60
train mean loss: 276.30
epoch train time: 0:00:02.185048
elapsed time: 0:08:53.524638
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 00:15:59.924428
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 273.52
 ---- batch: 020 ----
mean loss: 268.82
 ---- batch: 030 ----
mean loss: 279.92
 ---- batch: 040 ----
mean loss: 282.62
 ---- batch: 050 ----
mean loss: 268.05
 ---- batch: 060 ----
mean loss: 283.51
 ---- batch: 070 ----
mean loss: 275.22
 ---- batch: 080 ----
mean loss: 280.49
 ---- batch: 090 ----
mean loss: 279.39
 ---- batch: 100 ----
mean loss: 278.70
 ---- batch: 110 ----
mean loss: 269.84
train mean loss: 275.78
epoch train time: 0:00:02.186066
elapsed time: 0:08:55.710889
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 00:16:02.110679
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 277.04
 ---- batch: 020 ----
mean loss: 270.93
 ---- batch: 030 ----
mean loss: 282.66
 ---- batch: 040 ----
mean loss: 278.62
 ---- batch: 050 ----
mean loss: 274.44
 ---- batch: 060 ----
mean loss: 283.61
 ---- batch: 070 ----
mean loss: 287.76
 ---- batch: 080 ----
mean loss: 274.94
 ---- batch: 090 ----
mean loss: 276.77
 ---- batch: 100 ----
mean loss: 282.34
 ---- batch: 110 ----
mean loss: 272.99
train mean loss: 277.76
epoch train time: 0:00:02.183171
elapsed time: 0:08:57.894271
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 00:16:04.294043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.06
 ---- batch: 020 ----
mean loss: 271.19
 ---- batch: 030 ----
mean loss: 263.65
 ---- batch: 040 ----
mean loss: 268.78
 ---- batch: 050 ----
mean loss: 280.79
 ---- batch: 060 ----
mean loss: 279.61
 ---- batch: 070 ----
mean loss: 285.55
 ---- batch: 080 ----
mean loss: 278.82
 ---- batch: 090 ----
mean loss: 280.27
 ---- batch: 100 ----
mean loss: 283.10
 ---- batch: 110 ----
mean loss: 276.70
train mean loss: 277.14
epoch train time: 0:00:02.198273
elapsed time: 0:09:00.092723
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 00:16:06.492514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.67
 ---- batch: 020 ----
mean loss: 274.64
 ---- batch: 030 ----
mean loss: 283.85
 ---- batch: 040 ----
mean loss: 290.72
 ---- batch: 050 ----
mean loss: 279.40
 ---- batch: 060 ----
mean loss: 282.05
 ---- batch: 070 ----
mean loss: 287.89
 ---- batch: 080 ----
mean loss: 269.85
 ---- batch: 090 ----
mean loss: 277.72
 ---- batch: 100 ----
mean loss: 274.70
 ---- batch: 110 ----
mean loss: 279.14
train mean loss: 279.66
epoch train time: 0:00:02.184540
elapsed time: 0:09:02.277428
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 00:16:08.677211
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 274.50
 ---- batch: 020 ----
mean loss: 275.86
 ---- batch: 030 ----
mean loss: 281.26
 ---- batch: 040 ----
mean loss: 277.79
 ---- batch: 050 ----
mean loss: 273.85
 ---- batch: 060 ----
mean loss: 285.35
 ---- batch: 070 ----
mean loss: 281.44
 ---- batch: 080 ----
mean loss: 272.25
 ---- batch: 090 ----
mean loss: 262.05
 ---- batch: 100 ----
mean loss: 271.29
 ---- batch: 110 ----
mean loss: 275.26
train mean loss: 275.53
epoch train time: 0:00:02.185884
elapsed time: 0:09:04.463487
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 00:16:10.863305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.61
 ---- batch: 020 ----
mean loss: 278.90
 ---- batch: 030 ----
mean loss: 270.89
 ---- batch: 040 ----
mean loss: 270.26
 ---- batch: 050 ----
mean loss: 275.65
 ---- batch: 060 ----
mean loss: 285.03
 ---- batch: 070 ----
mean loss: 270.22
 ---- batch: 080 ----
mean loss: 267.92
 ---- batch: 090 ----
mean loss: 282.22
 ---- batch: 100 ----
mean loss: 276.85
 ---- batch: 110 ----
mean loss: 276.03
train mean loss: 275.34
epoch train time: 0:00:02.183053
elapsed time: 0:09:06.646782
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 00:16:13.046572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.12
 ---- batch: 020 ----
mean loss: 280.38
 ---- batch: 030 ----
mean loss: 275.33
 ---- batch: 040 ----
mean loss: 281.93
 ---- batch: 050 ----
mean loss: 274.08
 ---- batch: 060 ----
mean loss: 280.19
 ---- batch: 070 ----
mean loss: 291.81
 ---- batch: 080 ----
mean loss: 270.22
 ---- batch: 090 ----
mean loss: 265.76
 ---- batch: 100 ----
mean loss: 277.70
 ---- batch: 110 ----
mean loss: 274.65
train mean loss: 276.93
epoch train time: 0:00:02.183829
elapsed time: 0:09:08.830831
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 00:16:15.230619
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.86
 ---- batch: 020 ----
mean loss: 274.20
 ---- batch: 030 ----
mean loss: 282.75
 ---- batch: 040 ----
mean loss: 268.18
 ---- batch: 050 ----
mean loss: 272.38
 ---- batch: 060 ----
mean loss: 284.04
 ---- batch: 070 ----
mean loss: 280.03
 ---- batch: 080 ----
mean loss: 282.46
 ---- batch: 090 ----
mean loss: 272.24
 ---- batch: 100 ----
mean loss: 274.01
 ---- batch: 110 ----
mean loss: 282.90
train mean loss: 277.29
epoch train time: 0:00:02.191216
elapsed time: 0:09:11.022239
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 00:16:17.422033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 277.74
 ---- batch: 020 ----
mean loss: 282.19
 ---- batch: 030 ----
mean loss: 275.10
 ---- batch: 040 ----
mean loss: 274.15
 ---- batch: 050 ----
mean loss: 276.46
 ---- batch: 060 ----
mean loss: 271.45
 ---- batch: 070 ----
mean loss: 278.70
 ---- batch: 080 ----
mean loss: 275.85
 ---- batch: 090 ----
mean loss: 290.03
 ---- batch: 100 ----
mean loss: 269.93
 ---- batch: 110 ----
mean loss: 285.78
train mean loss: 278.31
epoch train time: 0:00:02.184391
elapsed time: 0:09:13.206851
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 00:16:19.606638
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 273.22
 ---- batch: 020 ----
mean loss: 274.50
 ---- batch: 030 ----
mean loss: 272.79
 ---- batch: 040 ----
mean loss: 276.07
 ---- batch: 050 ----
mean loss: 276.61
 ---- batch: 060 ----
mean loss: 281.93
 ---- batch: 070 ----
mean loss: 280.13
 ---- batch: 080 ----
mean loss: 281.32
 ---- batch: 090 ----
mean loss: 277.92
 ---- batch: 100 ----
mean loss: 284.43
 ---- batch: 110 ----
mean loss: 266.80
train mean loss: 276.45
epoch train time: 0:00:02.186564
elapsed time: 0:09:15.393594
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 00:16:21.793381
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 281.31
 ---- batch: 020 ----
mean loss: 284.82
 ---- batch: 030 ----
mean loss: 289.35
 ---- batch: 040 ----
mean loss: 269.79
 ---- batch: 050 ----
mean loss: 268.27
 ---- batch: 060 ----
mean loss: 275.75
 ---- batch: 070 ----
mean loss: 280.82
 ---- batch: 080 ----
mean loss: 262.87
 ---- batch: 090 ----
mean loss: 273.68
 ---- batch: 100 ----
mean loss: 276.11
 ---- batch: 110 ----
mean loss: 275.94
train mean loss: 276.20
epoch train time: 0:00:02.188528
elapsed time: 0:09:17.582316
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 00:16:23.982127
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 270.59
 ---- batch: 020 ----
mean loss: 289.52
 ---- batch: 030 ----
mean loss: 267.67
 ---- batch: 040 ----
mean loss: 281.22
 ---- batch: 050 ----
mean loss: 273.81
 ---- batch: 060 ----
mean loss: 274.81
 ---- batch: 070 ----
mean loss: 284.93
 ---- batch: 080 ----
mean loss: 273.44
 ---- batch: 090 ----
mean loss: 267.69
 ---- batch: 100 ----
mean loss: 275.91
 ---- batch: 110 ----
mean loss: 271.37
train mean loss: 275.36
epoch train time: 0:00:02.190239
elapsed time: 0:09:19.772755
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 00:16:26.172543
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.83
 ---- batch: 020 ----
mean loss: 275.49
 ---- batch: 030 ----
mean loss: 277.42
 ---- batch: 040 ----
mean loss: 277.11
 ---- batch: 050 ----
mean loss: 288.67
 ---- batch: 060 ----
mean loss: 271.89
 ---- batch: 070 ----
mean loss: 286.18
 ---- batch: 080 ----
mean loss: 280.62
 ---- batch: 090 ----
mean loss: 272.12
 ---- batch: 100 ----
mean loss: 277.38
 ---- batch: 110 ----
mean loss: 277.24
train mean loss: 277.53
epoch train time: 0:00:02.182394
elapsed time: 0:09:21.955337
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 00:16:28.355123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 277.54
 ---- batch: 020 ----
mean loss: 272.56
 ---- batch: 030 ----
mean loss: 274.10
 ---- batch: 040 ----
mean loss: 270.43
 ---- batch: 050 ----
mean loss: 267.92
 ---- batch: 060 ----
mean loss: 278.38
 ---- batch: 070 ----
mean loss: 281.18
 ---- batch: 080 ----
mean loss: 273.56
 ---- batch: 090 ----
mean loss: 282.59
 ---- batch: 100 ----
mean loss: 276.59
 ---- batch: 110 ----
mean loss: 282.37
train mean loss: 275.92
epoch train time: 0:00:02.190574
elapsed time: 0:09:24.146081
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 00:16:30.545867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.33
 ---- batch: 020 ----
mean loss: 268.54
 ---- batch: 030 ----
mean loss: 275.26
 ---- batch: 040 ----
mean loss: 280.83
 ---- batch: 050 ----
mean loss: 276.62
 ---- batch: 060 ----
mean loss: 271.76
 ---- batch: 070 ----
mean loss: 277.27
 ---- batch: 080 ----
mean loss: 276.71
 ---- batch: 090 ----
mean loss: 269.22
 ---- batch: 100 ----
mean loss: 273.46
 ---- batch: 110 ----
mean loss: 273.35
train mean loss: 275.24
epoch train time: 0:00:02.184085
elapsed time: 0:09:26.330334
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 00:16:32.730120
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 269.89
 ---- batch: 020 ----
mean loss: 281.44
 ---- batch: 030 ----
mean loss: 280.43
 ---- batch: 040 ----
mean loss: 277.81
 ---- batch: 050 ----
mean loss: 269.08
 ---- batch: 060 ----
mean loss: 280.48
 ---- batch: 070 ----
mean loss: 280.64
 ---- batch: 080 ----
mean loss: 270.16
 ---- batch: 090 ----
mean loss: 265.63
 ---- batch: 100 ----
mean loss: 286.46
 ---- batch: 110 ----
mean loss: 285.30
train mean loss: 276.43
epoch train time: 0:00:02.186685
elapsed time: 0:09:28.517202
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 00:16:34.916987
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 270.17
 ---- batch: 020 ----
mean loss: 269.61
 ---- batch: 030 ----
mean loss: 272.75
 ---- batch: 040 ----
mean loss: 271.25
 ---- batch: 050 ----
mean loss: 275.43
 ---- batch: 060 ----
mean loss: 272.86
 ---- batch: 070 ----
mean loss: 277.50
 ---- batch: 080 ----
mean loss: 274.97
 ---- batch: 090 ----
mean loss: 280.26
 ---- batch: 100 ----
mean loss: 281.88
 ---- batch: 110 ----
mean loss: 275.97
train mean loss: 274.62
epoch train time: 0:00:02.182976
elapsed time: 0:09:30.700373
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 00:16:37.100172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 270.77
 ---- batch: 020 ----
mean loss: 270.57
 ---- batch: 030 ----
mean loss: 271.42
 ---- batch: 040 ----
mean loss: 274.20
 ---- batch: 050 ----
mean loss: 267.43
 ---- batch: 060 ----
mean loss: 294.47
 ---- batch: 070 ----
mean loss: 276.45
 ---- batch: 080 ----
mean loss: 283.33
 ---- batch: 090 ----
mean loss: 280.54
 ---- batch: 100 ----
mean loss: 280.44
 ---- batch: 110 ----
mean loss: 274.06
train mean loss: 276.68
epoch train time: 0:00:02.191022
elapsed time: 0:09:32.894944
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_2/checkpoint.pth.tar
**** end time: 2019-09-27 00:16:39.294694 ****
