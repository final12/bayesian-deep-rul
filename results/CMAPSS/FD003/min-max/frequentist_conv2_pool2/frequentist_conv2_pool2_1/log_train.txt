Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_1', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 7897
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-20 23:25:19.765668 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:25:19.770657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4739.92
 ---- batch: 020 ----
mean loss: 4705.42
 ---- batch: 030 ----
mean loss: 4731.79
 ---- batch: 040 ----
mean loss: 4625.30
train mean loss: 4690.46
epoch train time: 0:00:14.944112
elapsed time: 0:00:14.950300
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:25:34.716005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4591.88
 ---- batch: 020 ----
mean loss: 4465.70
 ---- batch: 030 ----
mean loss: 4376.71
 ---- batch: 040 ----
mean loss: 4401.80
train mean loss: 4445.61
epoch train time: 0:00:00.216867
elapsed time: 0:00:15.167280
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:25:34.932991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4195.72
 ---- batch: 020 ----
mean loss: 4146.35
 ---- batch: 030 ----
mean loss: 4122.70
 ---- batch: 040 ----
mean loss: 3970.52
train mean loss: 4089.71
epoch train time: 0:00:00.203451
elapsed time: 0:00:15.370854
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:25:35.136565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3821.98
 ---- batch: 020 ----
mean loss: 3848.38
 ---- batch: 030 ----
mean loss: 3564.78
 ---- batch: 040 ----
mean loss: 3615.58
train mean loss: 3709.30
epoch train time: 0:00:00.205246
elapsed time: 0:00:15.576220
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:25:35.341932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3511.17
 ---- batch: 020 ----
mean loss: 3394.43
 ---- batch: 030 ----
mean loss: 3356.65
 ---- batch: 040 ----
mean loss: 3257.18
train mean loss: 3368.96
epoch train time: 0:00:00.203045
elapsed time: 0:00:15.779389
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:25:35.545101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3160.21
 ---- batch: 020 ----
mean loss: 3161.90
 ---- batch: 030 ----
mean loss: 3052.55
 ---- batch: 040 ----
mean loss: 2977.02
train mean loss: 3081.48
epoch train time: 0:00:00.203141
elapsed time: 0:00:15.982650
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:25:35.748360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2898.42
 ---- batch: 020 ----
mean loss: 2910.72
 ---- batch: 030 ----
mean loss: 2829.48
 ---- batch: 040 ----
mean loss: 2702.12
train mean loss: 2828.83
epoch train time: 0:00:00.200535
elapsed time: 0:00:16.183302
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:25:35.949010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2701.01
 ---- batch: 020 ----
mean loss: 2590.43
 ---- batch: 030 ----
mean loss: 2615.45
 ---- batch: 040 ----
mean loss: 2531.12
train mean loss: 2604.75
epoch train time: 0:00:00.201311
elapsed time: 0:00:16.384729
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:25:36.150450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2534.21
 ---- batch: 020 ----
mean loss: 2410.43
 ---- batch: 030 ----
mean loss: 2372.11
 ---- batch: 040 ----
mean loss: 2321.78
train mean loss: 2403.58
epoch train time: 0:00:00.202054
elapsed time: 0:00:16.586923
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:25:36.352630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2281.08
 ---- batch: 020 ----
mean loss: 2232.55
 ---- batch: 030 ----
mean loss: 2206.85
 ---- batch: 040 ----
mean loss: 2199.15
train mean loss: 2222.41
epoch train time: 0:00:00.199686
elapsed time: 0:00:16.786737
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:25:36.552449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2108.72
 ---- batch: 020 ----
mean loss: 2087.93
 ---- batch: 030 ----
mean loss: 2059.93
 ---- batch: 040 ----
mean loss: 1977.62
train mean loss: 2059.31
epoch train time: 0:00:00.200798
elapsed time: 0:00:16.987653
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:25:36.753364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1998.25
 ---- batch: 020 ----
mean loss: 1908.93
 ---- batch: 030 ----
mean loss: 1880.79
 ---- batch: 040 ----
mean loss: 1867.06
train mean loss: 1910.48
epoch train time: 0:00:00.202407
elapsed time: 0:00:17.190179
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:25:36.955946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1814.21
 ---- batch: 020 ----
mean loss: 1797.09
 ---- batch: 030 ----
mean loss: 1752.73
 ---- batch: 040 ----
mean loss: 1752.96
train mean loss: 1775.22
epoch train time: 0:00:00.203002
elapsed time: 0:00:17.393355
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:25:37.159065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1692.07
 ---- batch: 020 ----
mean loss: 1669.76
 ---- batch: 030 ----
mean loss: 1638.32
 ---- batch: 040 ----
mean loss: 1624.51
train mean loss: 1654.56
epoch train time: 0:00:00.201706
elapsed time: 0:00:17.595181
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:25:37.360890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1593.96
 ---- batch: 020 ----
mean loss: 1551.14
 ---- batch: 030 ----
mean loss: 1524.71
 ---- batch: 040 ----
mean loss: 1522.20
train mean loss: 1542.66
epoch train time: 0:00:00.200986
elapsed time: 0:00:17.796280
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:25:37.562022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1460.67
 ---- batch: 020 ----
mean loss: 1471.81
 ---- batch: 030 ----
mean loss: 1442.46
 ---- batch: 040 ----
mean loss: 1409.70
train mean loss: 1442.79
epoch train time: 0:00:00.201102
elapsed time: 0:00:17.997533
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:25:37.763242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1369.91
 ---- batch: 020 ----
mean loss: 1359.50
 ---- batch: 030 ----
mean loss: 1350.25
 ---- batch: 040 ----
mean loss: 1334.64
train mean loss: 1353.41
epoch train time: 0:00:00.206735
elapsed time: 0:00:18.204402
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:25:37.970128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1292.86
 ---- batch: 020 ----
mean loss: 1283.14
 ---- batch: 030 ----
mean loss: 1264.51
 ---- batch: 040 ----
mean loss: 1250.37
train mean loss: 1271.35
epoch train time: 0:00:00.210776
elapsed time: 0:00:18.415323
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:25:38.181035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1237.77
 ---- batch: 020 ----
mean loss: 1185.47
 ---- batch: 030 ----
mean loss: 1205.12
 ---- batch: 040 ----
mean loss: 1175.75
train mean loss: 1197.76
epoch train time: 0:00:00.210611
elapsed time: 0:00:18.626072
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:25:38.391785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1143.76
 ---- batch: 020 ----
mean loss: 1146.89
 ---- batch: 030 ----
mean loss: 1142.94
 ---- batch: 040 ----
mean loss: 1100.71
train mean loss: 1133.59
epoch train time: 0:00:00.210165
elapsed time: 0:00:18.836394
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:25:38.602106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1092.36
 ---- batch: 020 ----
mean loss: 1084.71
 ---- batch: 030 ----
mean loss: 1069.59
 ---- batch: 040 ----
mean loss: 1056.37
train mean loss: 1074.18
epoch train time: 0:00:00.212314
elapsed time: 0:00:19.048834
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:25:38.814561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1045.09
 ---- batch: 020 ----
mean loss: 1018.23
 ---- batch: 030 ----
mean loss: 1015.87
 ---- batch: 040 ----
mean loss: 1014.34
train mean loss: 1021.24
epoch train time: 0:00:00.203897
elapsed time: 0:00:19.252895
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:25:39.018604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 994.93
 ---- batch: 020 ----
mean loss: 987.33
 ---- batch: 030 ----
mean loss: 969.50
 ---- batch: 040 ----
mean loss: 951.35
train mean loss: 974.91
epoch train time: 0:00:00.203601
elapsed time: 0:00:19.456630
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:25:39.222370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.33
 ---- batch: 020 ----
mean loss: 946.57
 ---- batch: 030 ----
mean loss: 927.51
 ---- batch: 040 ----
mean loss: 911.97
train mean loss: 933.54
epoch train time: 0:00:00.202270
elapsed time: 0:00:19.659059
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:25:39.424778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.23
 ---- batch: 020 ----
mean loss: 903.08
 ---- batch: 030 ----
mean loss: 898.62
 ---- batch: 040 ----
mean loss: 871.14
train mean loss: 897.29
epoch train time: 0:00:00.203537
elapsed time: 0:00:19.862751
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:25:39.628465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.77
 ---- batch: 020 ----
mean loss: 882.49
 ---- batch: 030 ----
mean loss: 861.93
 ---- batch: 040 ----
mean loss: 851.03
train mean loss: 864.52
epoch train time: 0:00:00.211762
elapsed time: 0:00:20.074642
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:25:39.840374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.36
 ---- batch: 020 ----
mean loss: 827.36
 ---- batch: 030 ----
mean loss: 834.85
 ---- batch: 040 ----
mean loss: 832.28
train mean loss: 836.21
epoch train time: 0:00:00.200185
elapsed time: 0:00:20.274967
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:25:40.040690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 819.08
 ---- batch: 020 ----
mean loss: 817.27
 ---- batch: 030 ----
mean loss: 807.80
 ---- batch: 040 ----
mean loss: 808.62
train mean loss: 811.77
epoch train time: 0:00:00.195897
elapsed time: 0:00:20.470991
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:25:40.236698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 810.57
 ---- batch: 020 ----
mean loss: 781.07
 ---- batch: 030 ----
mean loss: 787.18
 ---- batch: 040 ----
mean loss: 784.23
train mean loss: 790.25
epoch train time: 0:00:00.198763
elapsed time: 0:00:20.669913
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:25:40.435623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 778.39
 ---- batch: 020 ----
mean loss: 760.63
 ---- batch: 030 ----
mean loss: 781.94
 ---- batch: 040 ----
mean loss: 763.26
train mean loss: 771.90
epoch train time: 0:00:00.198704
elapsed time: 0:00:20.868739
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:25:40.634448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.51
 ---- batch: 020 ----
mean loss: 750.89
 ---- batch: 030 ----
mean loss: 759.88
 ---- batch: 040 ----
mean loss: 752.71
train mean loss: 755.08
epoch train time: 0:00:00.212793
elapsed time: 0:00:21.081665
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:25:40.847390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 758.43
 ---- batch: 020 ----
mean loss: 732.94
 ---- batch: 030 ----
mean loss: 749.08
 ---- batch: 040 ----
mean loss: 729.77
train mean loss: 741.05
epoch train time: 0:00:00.201159
elapsed time: 0:00:21.282959
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:25:41.048668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.63
 ---- batch: 020 ----
mean loss: 713.82
 ---- batch: 030 ----
mean loss: 716.17
 ---- batch: 040 ----
mean loss: 738.89
train mean loss: 729.04
epoch train time: 0:00:00.202561
elapsed time: 0:00:21.485634
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:25:41.251342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.36
 ---- batch: 020 ----
mean loss: 720.57
 ---- batch: 030 ----
mean loss: 724.48
 ---- batch: 040 ----
mean loss: 713.84
train mean loss: 718.66
epoch train time: 0:00:00.198387
elapsed time: 0:00:21.684135
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:25:41.449843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 709.77
 ---- batch: 020 ----
mean loss: 712.40
 ---- batch: 030 ----
mean loss: 698.15
 ---- batch: 040 ----
mean loss: 713.25
train mean loss: 710.37
epoch train time: 0:00:00.209695
elapsed time: 0:00:21.893944
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:25:41.659653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 707.29
 ---- batch: 020 ----
mean loss: 693.66
 ---- batch: 030 ----
mean loss: 696.73
 ---- batch: 040 ----
mean loss: 718.52
train mean loss: 702.13
epoch train time: 0:00:00.207355
elapsed time: 0:00:22.101417
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:25:41.867137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 696.64
 ---- batch: 020 ----
mean loss: 703.36
 ---- batch: 030 ----
mean loss: 697.69
 ---- batch: 040 ----
mean loss: 698.99
train mean loss: 696.34
epoch train time: 0:00:00.205508
elapsed time: 0:00:22.307067
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:25:42.072795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.74
 ---- batch: 020 ----
mean loss: 682.98
 ---- batch: 030 ----
mean loss: 682.49
 ---- batch: 040 ----
mean loss: 709.08
train mean loss: 691.14
epoch train time: 0:00:00.207771
elapsed time: 0:00:22.515006
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:25:42.280726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.68
 ---- batch: 020 ----
mean loss: 673.87
 ---- batch: 030 ----
mean loss: 706.07
 ---- batch: 040 ----
mean loss: 683.46
train mean loss: 686.32
epoch train time: 0:00:00.208537
elapsed time: 0:00:22.723675
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:25:42.489425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.87
 ---- batch: 020 ----
mean loss: 684.23
 ---- batch: 030 ----
mean loss: 682.01
 ---- batch: 040 ----
mean loss: 674.87
train mean loss: 682.19
epoch train time: 0:00:00.208893
elapsed time: 0:00:22.932728
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:25:42.698440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 668.85
 ---- batch: 020 ----
mean loss: 696.55
 ---- batch: 030 ----
mean loss: 681.92
 ---- batch: 040 ----
mean loss: 672.67
train mean loss: 679.41
epoch train time: 0:00:00.205356
elapsed time: 0:00:23.138219
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:25:42.903927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 693.90
 ---- batch: 020 ----
mean loss: 678.43
 ---- batch: 030 ----
mean loss: 670.55
 ---- batch: 040 ----
mean loss: 663.51
train mean loss: 676.88
epoch train time: 0:00:00.204099
elapsed time: 0:00:23.342451
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:25:43.108161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 674.90
 ---- batch: 020 ----
mean loss: 675.00
 ---- batch: 030 ----
mean loss: 678.02
 ---- batch: 040 ----
mean loss: 676.72
train mean loss: 674.60
epoch train time: 0:00:00.204048
elapsed time: 0:00:23.546677
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:25:43.312407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.19
 ---- batch: 020 ----
mean loss: 661.58
 ---- batch: 030 ----
mean loss: 673.17
 ---- batch: 040 ----
mean loss: 666.52
train mean loss: 673.24
epoch train time: 0:00:00.204097
elapsed time: 0:00:23.750916
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:25:43.516628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.44
 ---- batch: 020 ----
mean loss: 645.59
 ---- batch: 030 ----
mean loss: 675.69
 ---- batch: 040 ----
mean loss: 685.79
train mean loss: 670.91
epoch train time: 0:00:00.202086
elapsed time: 0:00:23.953137
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:25:43.718846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.94
 ---- batch: 020 ----
mean loss: 652.36
 ---- batch: 030 ----
mean loss: 669.49
 ---- batch: 040 ----
mean loss: 675.13
train mean loss: 669.68
epoch train time: 0:00:00.202605
elapsed time: 0:00:24.155875
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:25:43.921587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 661.80
 ---- batch: 020 ----
mean loss: 677.32
 ---- batch: 030 ----
mean loss: 657.80
 ---- batch: 040 ----
mean loss: 683.35
train mean loss: 667.93
epoch train time: 0:00:00.200819
elapsed time: 0:00:24.356827
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:25:44.122550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.10
 ---- batch: 020 ----
mean loss: 677.11
 ---- batch: 030 ----
mean loss: 662.90
 ---- batch: 040 ----
mean loss: 670.18
train mean loss: 665.92
epoch train time: 0:00:00.195775
elapsed time: 0:00:24.552739
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:25:44.318462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 666.14
 ---- batch: 020 ----
mean loss: 673.42
 ---- batch: 030 ----
mean loss: 648.92
 ---- batch: 040 ----
mean loss: 659.23
train mean loss: 658.89
epoch train time: 0:00:00.193269
elapsed time: 0:00:24.746152
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:25:44.511860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 618.92
 ---- batch: 020 ----
mean loss: 583.56
 ---- batch: 030 ----
mean loss: 547.40
 ---- batch: 040 ----
mean loss: 482.98
train mean loss: 550.12
epoch train time: 0:00:00.196698
elapsed time: 0:00:24.942996
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:25:44.708704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.21
 ---- batch: 020 ----
mean loss: 439.41
 ---- batch: 030 ----
mean loss: 420.71
 ---- batch: 040 ----
mean loss: 430.57
train mean loss: 434.54
epoch train time: 0:00:00.198322
elapsed time: 0:00:25.141429
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:25:44.907136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.48
 ---- batch: 020 ----
mean loss: 415.16
 ---- batch: 030 ----
mean loss: 417.13
 ---- batch: 040 ----
mean loss: 414.58
train mean loss: 416.65
epoch train time: 0:00:00.203224
elapsed time: 0:00:25.344795
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:25:45.110511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.54
 ---- batch: 020 ----
mean loss: 409.79
 ---- batch: 030 ----
mean loss: 402.62
 ---- batch: 040 ----
mean loss: 395.42
train mean loss: 406.25
epoch train time: 0:00:00.201814
elapsed time: 0:00:25.546769
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:25:45.312515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.17
 ---- batch: 020 ----
mean loss: 399.97
 ---- batch: 030 ----
mean loss: 396.67
 ---- batch: 040 ----
mean loss: 392.58
train mean loss: 396.24
epoch train time: 0:00:00.197480
elapsed time: 0:00:25.744397
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:25:45.510103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.11
 ---- batch: 020 ----
mean loss: 391.60
 ---- batch: 030 ----
mean loss: 390.01
 ---- batch: 040 ----
mean loss: 383.81
train mean loss: 388.41
epoch train time: 0:00:00.198401
elapsed time: 0:00:25.942924
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:25:45.708644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.57
 ---- batch: 020 ----
mean loss: 375.20
 ---- batch: 030 ----
mean loss: 381.84
 ---- batch: 040 ----
mean loss: 377.57
train mean loss: 381.41
epoch train time: 0:00:00.201895
elapsed time: 0:00:26.144946
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:25:45.910655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.53
 ---- batch: 020 ----
mean loss: 369.22
 ---- batch: 030 ----
mean loss: 375.78
 ---- batch: 040 ----
mean loss: 378.33
train mean loss: 375.57
epoch train time: 0:00:00.204146
elapsed time: 0:00:26.349214
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:25:46.114925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.54
 ---- batch: 020 ----
mean loss: 368.94
 ---- batch: 030 ----
mean loss: 369.46
 ---- batch: 040 ----
mean loss: 366.30
train mean loss: 370.13
epoch train time: 0:00:00.209347
elapsed time: 0:00:26.558698
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:25:46.324437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.07
 ---- batch: 020 ----
mean loss: 371.01
 ---- batch: 030 ----
mean loss: 365.31
 ---- batch: 040 ----
mean loss: 364.52
train mean loss: 365.43
epoch train time: 0:00:00.207835
elapsed time: 0:00:26.766686
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:25:46.532429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.83
 ---- batch: 020 ----
mean loss: 365.57
 ---- batch: 030 ----
mean loss: 362.46
 ---- batch: 040 ----
mean loss: 360.87
train mean loss: 360.78
epoch train time: 0:00:00.210282
elapsed time: 0:00:26.977128
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:25:46.742853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.26
 ---- batch: 020 ----
mean loss: 355.24
 ---- batch: 030 ----
mean loss: 362.77
 ---- batch: 040 ----
mean loss: 351.38
train mean loss: 357.15
epoch train time: 0:00:00.210137
elapsed time: 0:00:27.187400
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:25:46.953110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.47
 ---- batch: 020 ----
mean loss: 358.17
 ---- batch: 030 ----
mean loss: 349.31
 ---- batch: 040 ----
mean loss: 357.80
train mean loss: 353.76
epoch train time: 0:00:00.207157
elapsed time: 0:00:27.394693
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:25:47.160456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.46
 ---- batch: 020 ----
mean loss: 352.75
 ---- batch: 030 ----
mean loss: 348.45
 ---- batch: 040 ----
mean loss: 348.78
train mean loss: 350.50
epoch train time: 0:00:00.203425
elapsed time: 0:00:27.598289
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:25:47.364001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.74
 ---- batch: 020 ----
mean loss: 351.59
 ---- batch: 030 ----
mean loss: 351.61
 ---- batch: 040 ----
mean loss: 346.21
train mean loss: 347.70
epoch train time: 0:00:00.203733
elapsed time: 0:00:27.802140
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:25:47.567857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.83
 ---- batch: 020 ----
mean loss: 347.37
 ---- batch: 030 ----
mean loss: 349.07
 ---- batch: 040 ----
mean loss: 332.97
train mean loss: 345.15
epoch train time: 0:00:00.204574
elapsed time: 0:00:28.006845
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:25:47.772558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.50
 ---- batch: 020 ----
mean loss: 344.43
 ---- batch: 030 ----
mean loss: 344.62
 ---- batch: 040 ----
mean loss: 340.21
train mean loss: 343.43
epoch train time: 0:00:00.205194
elapsed time: 0:00:28.212161
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:25:47.977872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.91
 ---- batch: 020 ----
mean loss: 325.68
 ---- batch: 030 ----
mean loss: 345.33
 ---- batch: 040 ----
mean loss: 344.08
train mean loss: 341.25
epoch train time: 0:00:00.202556
elapsed time: 0:00:28.414838
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:25:48.180560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.74
 ---- batch: 020 ----
mean loss: 340.58
 ---- batch: 030 ----
mean loss: 335.45
 ---- batch: 040 ----
mean loss: 340.34
train mean loss: 339.41
epoch train time: 0:00:00.196751
elapsed time: 0:00:28.611731
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:25:48.377439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.35
 ---- batch: 020 ----
mean loss: 326.25
 ---- batch: 030 ----
mean loss: 340.85
 ---- batch: 040 ----
mean loss: 343.97
train mean loss: 337.87
epoch train time: 0:00:00.195363
elapsed time: 0:00:28.807209
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:25:48.572918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.11
 ---- batch: 020 ----
mean loss: 334.90
 ---- batch: 030 ----
mean loss: 327.53
 ---- batch: 040 ----
mean loss: 332.85
train mean loss: 336.62
epoch train time: 0:00:00.194764
elapsed time: 0:00:29.002118
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:25:48.767825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.23
 ---- batch: 020 ----
mean loss: 338.48
 ---- batch: 030 ----
mean loss: 339.82
 ---- batch: 040 ----
mean loss: 328.60
train mean loss: 335.16
epoch train time: 0:00:00.198460
elapsed time: 0:00:29.200690
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:25:48.966418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.57
 ---- batch: 020 ----
mean loss: 337.99
 ---- batch: 030 ----
mean loss: 336.15
 ---- batch: 040 ----
mean loss: 331.19
train mean loss: 334.21
epoch train time: 0:00:00.197475
elapsed time: 0:00:29.398296
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:25:49.164003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.83
 ---- batch: 020 ----
mean loss: 336.20
 ---- batch: 030 ----
mean loss: 331.41
 ---- batch: 040 ----
mean loss: 335.48
train mean loss: 332.80
epoch train time: 0:00:00.192641
elapsed time: 0:00:29.591056
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:25:49.356797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.90
 ---- batch: 020 ----
mean loss: 323.41
 ---- batch: 030 ----
mean loss: 331.88
 ---- batch: 040 ----
mean loss: 332.66
train mean loss: 332.01
epoch train time: 0:00:00.192779
elapsed time: 0:00:29.783976
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:25:49.549682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.81
 ---- batch: 020 ----
mean loss: 330.77
 ---- batch: 030 ----
mean loss: 320.13
 ---- batch: 040 ----
mean loss: 337.77
train mean loss: 331.79
epoch train time: 0:00:00.197388
elapsed time: 0:00:29.981473
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:25:49.747178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.61
 ---- batch: 020 ----
mean loss: 331.88
 ---- batch: 030 ----
mean loss: 325.65
 ---- batch: 040 ----
mean loss: 339.34
train mean loss: 330.15
epoch train time: 0:00:00.202757
elapsed time: 0:00:30.184344
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:25:49.950049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.60
 ---- batch: 020 ----
mean loss: 329.65
 ---- batch: 030 ----
mean loss: 329.77
 ---- batch: 040 ----
mean loss: 331.45
train mean loss: 329.79
epoch train time: 0:00:00.200605
elapsed time: 0:00:30.385085
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:25:50.150797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.82
 ---- batch: 020 ----
mean loss: 334.01
 ---- batch: 030 ----
mean loss: 323.25
 ---- batch: 040 ----
mean loss: 330.06
train mean loss: 329.03
epoch train time: 0:00:00.206344
elapsed time: 0:00:30.591548
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:25:50.357268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.22
 ---- batch: 020 ----
mean loss: 328.81
 ---- batch: 030 ----
mean loss: 324.77
 ---- batch: 040 ----
mean loss: 330.51
train mean loss: 328.31
epoch train time: 0:00:00.208048
elapsed time: 0:00:30.799742
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:25:50.565454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.68
 ---- batch: 020 ----
mean loss: 325.52
 ---- batch: 030 ----
mean loss: 335.97
 ---- batch: 040 ----
mean loss: 318.93
train mean loss: 327.82
epoch train time: 0:00:00.206248
elapsed time: 0:00:31.006129
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:25:50.771840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.94
 ---- batch: 020 ----
mean loss: 329.03
 ---- batch: 030 ----
mean loss: 335.37
 ---- batch: 040 ----
mean loss: 316.08
train mean loss: 327.22
epoch train time: 0:00:00.204739
elapsed time: 0:00:31.210990
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:25:50.976701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.03
 ---- batch: 020 ----
mean loss: 322.64
 ---- batch: 030 ----
mean loss: 323.13
 ---- batch: 040 ----
mean loss: 329.06
train mean loss: 326.31
epoch train time: 0:00:00.202961
elapsed time: 0:00:31.414084
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:25:51.179842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.62
 ---- batch: 020 ----
mean loss: 334.45
 ---- batch: 030 ----
mean loss: 321.89
 ---- batch: 040 ----
mean loss: 330.56
train mean loss: 326.39
epoch train time: 0:00:00.200949
elapsed time: 0:00:31.615218
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:25:51.380956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.34
 ---- batch: 020 ----
mean loss: 329.37
 ---- batch: 030 ----
mean loss: 316.76
 ---- batch: 040 ----
mean loss: 332.54
train mean loss: 325.47
epoch train time: 0:00:00.204700
elapsed time: 0:00:31.820060
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:25:51.585771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.10
 ---- batch: 020 ----
mean loss: 326.75
 ---- batch: 030 ----
mean loss: 322.35
 ---- batch: 040 ----
mean loss: 327.58
train mean loss: 325.21
epoch train time: 0:00:00.204037
elapsed time: 0:00:32.024232
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:25:51.789960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.57
 ---- batch: 020 ----
mean loss: 324.75
 ---- batch: 030 ----
mean loss: 328.62
 ---- batch: 040 ----
mean loss: 331.11
train mean loss: 324.97
epoch train time: 0:00:00.203043
elapsed time: 0:00:32.227413
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:25:51.993123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.21
 ---- batch: 020 ----
mean loss: 328.38
 ---- batch: 030 ----
mean loss: 316.38
 ---- batch: 040 ----
mean loss: 326.73
train mean loss: 324.82
epoch train time: 0:00:00.201128
elapsed time: 0:00:32.428657
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:25:52.194366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.25
 ---- batch: 020 ----
mean loss: 315.68
 ---- batch: 030 ----
mean loss: 333.45
 ---- batch: 040 ----
mean loss: 324.47
train mean loss: 324.33
epoch train time: 0:00:00.199517
elapsed time: 0:00:32.628298
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:25:52.394025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.50
 ---- batch: 020 ----
mean loss: 326.87
 ---- batch: 030 ----
mean loss: 327.34
 ---- batch: 040 ----
mean loss: 319.90
train mean loss: 323.52
epoch train time: 0:00:00.199570
elapsed time: 0:00:32.828000
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:25:52.593707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.59
 ---- batch: 020 ----
mean loss: 318.69
 ---- batch: 030 ----
mean loss: 324.30
 ---- batch: 040 ----
mean loss: 324.75
train mean loss: 323.33
epoch train time: 0:00:00.204587
elapsed time: 0:00:33.032711
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:25:52.798425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.71
 ---- batch: 020 ----
mean loss: 321.18
 ---- batch: 030 ----
mean loss: 322.75
 ---- batch: 040 ----
mean loss: 325.95
train mean loss: 322.80
epoch train time: 0:00:00.211100
elapsed time: 0:00:33.243981
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:25:53.009692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.18
 ---- batch: 020 ----
mean loss: 327.38
 ---- batch: 030 ----
mean loss: 322.73
 ---- batch: 040 ----
mean loss: 323.63
train mean loss: 322.54
epoch train time: 0:00:00.203600
elapsed time: 0:00:33.447695
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:25:53.213420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.80
 ---- batch: 020 ----
mean loss: 328.15
 ---- batch: 030 ----
mean loss: 321.51
 ---- batch: 040 ----
mean loss: 320.85
train mean loss: 322.64
epoch train time: 0:00:00.199111
elapsed time: 0:00:33.646942
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:25:53.412654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.67
 ---- batch: 020 ----
mean loss: 321.47
 ---- batch: 030 ----
mean loss: 311.47
 ---- batch: 040 ----
mean loss: 328.89
train mean loss: 321.93
epoch train time: 0:00:00.199916
elapsed time: 0:00:33.846996
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:25:53.612707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.21
 ---- batch: 020 ----
mean loss: 327.49
 ---- batch: 030 ----
mean loss: 326.66
 ---- batch: 040 ----
mean loss: 316.58
train mean loss: 321.62
epoch train time: 0:00:00.200877
elapsed time: 0:00:34.048019
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:25:53.813751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.68
 ---- batch: 020 ----
mean loss: 323.73
 ---- batch: 030 ----
mean loss: 310.86
 ---- batch: 040 ----
mean loss: 322.24
train mean loss: 321.43
epoch train time: 0:00:00.195768
elapsed time: 0:00:34.243942
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:25:54.009651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.13
 ---- batch: 020 ----
mean loss: 320.42
 ---- batch: 030 ----
mean loss: 317.09
 ---- batch: 040 ----
mean loss: 328.93
train mean loss: 320.88
epoch train time: 0:00:00.198086
elapsed time: 0:00:34.442162
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:25:54.207880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.55
 ---- batch: 020 ----
mean loss: 321.00
 ---- batch: 030 ----
mean loss: 322.99
 ---- batch: 040 ----
mean loss: 321.05
train mean loss: 320.48
epoch train time: 0:00:00.199934
elapsed time: 0:00:34.642238
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:25:54.407947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.43
 ---- batch: 020 ----
mean loss: 320.83
 ---- batch: 030 ----
mean loss: 326.54
 ---- batch: 040 ----
mean loss: 315.87
train mean loss: 320.29
epoch train time: 0:00:00.207864
elapsed time: 0:00:34.850234
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:25:54.615949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.24
 ---- batch: 020 ----
mean loss: 322.09
 ---- batch: 030 ----
mean loss: 315.19
 ---- batch: 040 ----
mean loss: 327.02
train mean loss: 319.95
epoch train time: 0:00:00.210749
elapsed time: 0:00:35.061140
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:25:54.826869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.90
 ---- batch: 020 ----
mean loss: 322.42
 ---- batch: 030 ----
mean loss: 326.08
 ---- batch: 040 ----
mean loss: 316.50
train mean loss: 320.14
epoch train time: 0:00:00.209568
elapsed time: 0:00:35.270853
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:25:55.036565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.72
 ---- batch: 020 ----
mean loss: 321.40
 ---- batch: 030 ----
mean loss: 321.78
 ---- batch: 040 ----
mean loss: 313.27
train mean loss: 319.87
epoch train time: 0:00:00.206327
elapsed time: 0:00:35.477300
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:25:55.243012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.82
 ---- batch: 020 ----
mean loss: 327.94
 ---- batch: 030 ----
mean loss: 324.98
 ---- batch: 040 ----
mean loss: 307.76
train mean loss: 319.15
epoch train time: 0:00:00.206846
elapsed time: 0:00:35.684265
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:25:55.449976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.38
 ---- batch: 020 ----
mean loss: 325.81
 ---- batch: 030 ----
mean loss: 316.64
 ---- batch: 040 ----
mean loss: 316.14
train mean loss: 319.34
epoch train time: 0:00:00.205415
elapsed time: 0:00:35.889795
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:25:55.655504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.19
 ---- batch: 020 ----
mean loss: 320.49
 ---- batch: 030 ----
mean loss: 308.86
 ---- batch: 040 ----
mean loss: 317.64
train mean loss: 319.13
epoch train time: 0:00:00.207389
elapsed time: 0:00:36.097331
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:25:55.863040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.13
 ---- batch: 020 ----
mean loss: 323.99
 ---- batch: 030 ----
mean loss: 318.02
 ---- batch: 040 ----
mean loss: 315.65
train mean loss: 318.79
epoch train time: 0:00:00.204169
elapsed time: 0:00:36.301619
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:25:56.067330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.19
 ---- batch: 020 ----
mean loss: 322.72
 ---- batch: 030 ----
mean loss: 311.66
 ---- batch: 040 ----
mean loss: 316.48
train mean loss: 318.26
epoch train time: 0:00:00.203621
elapsed time: 0:00:36.505387
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:25:56.271126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.65
 ---- batch: 020 ----
mean loss: 331.60
 ---- batch: 030 ----
mean loss: 315.42
 ---- batch: 040 ----
mean loss: 310.05
train mean loss: 317.90
epoch train time: 0:00:00.199436
elapsed time: 0:00:36.704965
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:25:56.470672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.58
 ---- batch: 020 ----
mean loss: 324.69
 ---- batch: 030 ----
mean loss: 310.85
 ---- batch: 040 ----
mean loss: 311.94
train mean loss: 318.04
epoch train time: 0:00:00.195117
elapsed time: 0:00:36.900222
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:25:56.665935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.37
 ---- batch: 020 ----
mean loss: 324.50
 ---- batch: 030 ----
mean loss: 322.00
 ---- batch: 040 ----
mean loss: 312.14
train mean loss: 318.11
epoch train time: 0:00:00.203639
elapsed time: 0:00:37.103984
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:25:56.869694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.32
 ---- batch: 020 ----
mean loss: 303.62
 ---- batch: 030 ----
mean loss: 325.73
 ---- batch: 040 ----
mean loss: 325.95
train mean loss: 317.07
epoch train time: 0:00:00.202154
elapsed time: 0:00:37.306257
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:25:57.071967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.57
 ---- batch: 020 ----
mean loss: 310.25
 ---- batch: 030 ----
mean loss: 318.47
 ---- batch: 040 ----
mean loss: 318.64
train mean loss: 317.10
epoch train time: 0:00:00.201052
elapsed time: 0:00:37.507424
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:25:57.273147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.64
 ---- batch: 020 ----
mean loss: 312.41
 ---- batch: 030 ----
mean loss: 317.32
 ---- batch: 040 ----
mean loss: 321.51
train mean loss: 317.09
epoch train time: 0:00:00.205678
elapsed time: 0:00:37.713232
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:25:57.478942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.19
 ---- batch: 020 ----
mean loss: 325.12
 ---- batch: 030 ----
mean loss: 318.07
 ---- batch: 040 ----
mean loss: 312.83
train mean loss: 316.60
epoch train time: 0:00:00.211662
elapsed time: 0:00:37.925034
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:25:57.690748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.17
 ---- batch: 020 ----
mean loss: 317.05
 ---- batch: 030 ----
mean loss: 312.70
 ---- batch: 040 ----
mean loss: 312.40
train mean loss: 316.91
epoch train time: 0:00:00.215306
elapsed time: 0:00:38.140473
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:25:57.906205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.02
 ---- batch: 020 ----
mean loss: 313.78
 ---- batch: 030 ----
mean loss: 319.15
 ---- batch: 040 ----
mean loss: 315.21
train mean loss: 316.39
epoch train time: 0:00:00.210409
elapsed time: 0:00:38.351025
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:25:58.116736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.11
 ---- batch: 020 ----
mean loss: 316.80
 ---- batch: 030 ----
mean loss: 311.20
 ---- batch: 040 ----
mean loss: 312.48
train mean loss: 316.32
epoch train time: 0:00:00.207573
elapsed time: 0:00:38.558712
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:25:58.324444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.57
 ---- batch: 020 ----
mean loss: 324.81
 ---- batch: 030 ----
mean loss: 315.97
 ---- batch: 040 ----
mean loss: 312.32
train mean loss: 315.83
epoch train time: 0:00:00.206694
elapsed time: 0:00:38.765600
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:25:58.531313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.01
 ---- batch: 020 ----
mean loss: 308.93
 ---- batch: 030 ----
mean loss: 314.96
 ---- batch: 040 ----
mean loss: 315.49
train mean loss: 316.09
epoch train time: 0:00:00.207595
elapsed time: 0:00:38.973316
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:25:58.739040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.68
 ---- batch: 020 ----
mean loss: 315.40
 ---- batch: 030 ----
mean loss: 312.79
 ---- batch: 040 ----
mean loss: 319.57
train mean loss: 315.42
epoch train time: 0:00:00.210202
elapsed time: 0:00:39.183652
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:25:58.949362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.68
 ---- batch: 020 ----
mean loss: 316.15
 ---- batch: 030 ----
mean loss: 314.73
 ---- batch: 040 ----
mean loss: 313.29
train mean loss: 315.39
epoch train time: 0:00:00.210607
elapsed time: 0:00:39.394379
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:25:59.160089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.42
 ---- batch: 020 ----
mean loss: 323.21
 ---- batch: 030 ----
mean loss: 314.59
 ---- batch: 040 ----
mean loss: 311.43
train mean loss: 315.04
epoch train time: 0:00:00.206283
elapsed time: 0:00:39.600788
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:25:59.366501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.08
 ---- batch: 020 ----
mean loss: 311.46
 ---- batch: 030 ----
mean loss: 316.77
 ---- batch: 040 ----
mean loss: 324.03
train mean loss: 314.85
epoch train time: 0:00:00.211264
elapsed time: 0:00:39.812220
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:25:59.577934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.42
 ---- batch: 020 ----
mean loss: 310.34
 ---- batch: 030 ----
mean loss: 313.01
 ---- batch: 040 ----
mean loss: 315.79
train mean loss: 314.80
epoch train time: 0:00:00.207994
elapsed time: 0:00:40.020346
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:25:59.786061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.52
 ---- batch: 020 ----
mean loss: 312.13
 ---- batch: 030 ----
mean loss: 314.24
 ---- batch: 040 ----
mean loss: 312.40
train mean loss: 314.64
epoch train time: 0:00:00.207548
elapsed time: 0:00:40.228022
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:25:59.993736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.98
 ---- batch: 020 ----
mean loss: 319.56
 ---- batch: 030 ----
mean loss: 312.19
 ---- batch: 040 ----
mean loss: 329.57
train mean loss: 314.47
epoch train time: 0:00:00.211078
elapsed time: 0:00:40.439229
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:26:00.204944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.66
 ---- batch: 020 ----
mean loss: 308.89
 ---- batch: 030 ----
mean loss: 319.94
 ---- batch: 040 ----
mean loss: 308.22
train mean loss: 314.23
epoch train time: 0:00:00.209020
elapsed time: 0:00:40.648382
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:26:00.414087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.37
 ---- batch: 020 ----
mean loss: 307.24
 ---- batch: 030 ----
mean loss: 318.19
 ---- batch: 040 ----
mean loss: 315.81
train mean loss: 314.64
epoch train time: 0:00:00.206189
elapsed time: 0:00:40.854685
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:26:00.620416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.36
 ---- batch: 020 ----
mean loss: 310.10
 ---- batch: 030 ----
mean loss: 306.63
 ---- batch: 040 ----
mean loss: 316.07
train mean loss: 313.84
epoch train time: 0:00:00.203578
elapsed time: 0:00:41.058399
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:26:00.824109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.00
 ---- batch: 020 ----
mean loss: 314.98
 ---- batch: 030 ----
mean loss: 321.61
 ---- batch: 040 ----
mean loss: 316.18
train mean loss: 313.66
epoch train time: 0:00:00.198939
elapsed time: 0:00:41.257503
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:26:01.023226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.15
 ---- batch: 020 ----
mean loss: 308.04
 ---- batch: 030 ----
mean loss: 316.37
 ---- batch: 040 ----
mean loss: 309.21
train mean loss: 313.66
epoch train time: 0:00:00.203660
elapsed time: 0:00:41.461295
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:26:01.227024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.02
 ---- batch: 020 ----
mean loss: 313.51
 ---- batch: 030 ----
mean loss: 308.46
 ---- batch: 040 ----
mean loss: 316.06
train mean loss: 313.27
epoch train time: 0:00:00.201762
elapsed time: 0:00:41.663207
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:26:01.428916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.32
 ---- batch: 020 ----
mean loss: 312.74
 ---- batch: 030 ----
mean loss: 319.64
 ---- batch: 040 ----
mean loss: 317.85
train mean loss: 313.39
epoch train time: 0:00:00.196013
elapsed time: 0:00:41.859350
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:26:01.625070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.68
 ---- batch: 020 ----
mean loss: 311.97
 ---- batch: 030 ----
mean loss: 319.30
 ---- batch: 040 ----
mean loss: 305.37
train mean loss: 313.06
epoch train time: 0:00:00.199664
elapsed time: 0:00:42.059146
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:26:01.824855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.31
 ---- batch: 020 ----
mean loss: 316.57
 ---- batch: 030 ----
mean loss: 308.95
 ---- batch: 040 ----
mean loss: 314.28
train mean loss: 312.97
epoch train time: 0:00:00.198120
elapsed time: 0:00:42.257409
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:26:02.023116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.96
 ---- batch: 020 ----
mean loss: 313.60
 ---- batch: 030 ----
mean loss: 309.76
 ---- batch: 040 ----
mean loss: 317.03
train mean loss: 312.90
epoch train time: 0:00:00.205495
elapsed time: 0:00:42.463022
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:26:02.228733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.23
 ---- batch: 020 ----
mean loss: 309.32
 ---- batch: 030 ----
mean loss: 312.53
 ---- batch: 040 ----
mean loss: 314.35
train mean loss: 312.52
epoch train time: 0:00:00.204004
elapsed time: 0:00:42.667151
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:26:02.432863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.22
 ---- batch: 020 ----
mean loss: 318.50
 ---- batch: 030 ----
mean loss: 313.49
 ---- batch: 040 ----
mean loss: 308.08
train mean loss: 312.61
epoch train time: 0:00:00.212754
elapsed time: 0:00:42.880028
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:26:02.645740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.14
 ---- batch: 020 ----
mean loss: 309.11
 ---- batch: 030 ----
mean loss: 316.64
 ---- batch: 040 ----
mean loss: 306.89
train mean loss: 312.02
epoch train time: 0:00:00.211265
elapsed time: 0:00:43.091435
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:26:02.857147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.22
 ---- batch: 020 ----
mean loss: 307.69
 ---- batch: 030 ----
mean loss: 314.16
 ---- batch: 040 ----
mean loss: 309.86
train mean loss: 311.97
epoch train time: 0:00:00.209773
elapsed time: 0:00:43.301330
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:26:03.067040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.00
 ---- batch: 020 ----
mean loss: 312.60
 ---- batch: 030 ----
mean loss: 309.75
 ---- batch: 040 ----
mean loss: 318.59
train mean loss: 311.66
epoch train time: 0:00:00.212918
elapsed time: 0:00:43.514369
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:26:03.280080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.51
 ---- batch: 020 ----
mean loss: 310.60
 ---- batch: 030 ----
mean loss: 313.20
 ---- batch: 040 ----
mean loss: 315.29
train mean loss: 311.96
epoch train time: 0:00:00.210913
elapsed time: 0:00:43.725436
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:26:03.491147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.29
 ---- batch: 020 ----
mean loss: 314.64
 ---- batch: 030 ----
mean loss: 306.11
 ---- batch: 040 ----
mean loss: 306.29
train mean loss: 311.65
epoch train time: 0:00:00.215613
elapsed time: 0:00:43.941187
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:26:03.706930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.36
 ---- batch: 020 ----
mean loss: 307.41
 ---- batch: 030 ----
mean loss: 312.07
 ---- batch: 040 ----
mean loss: 316.29
train mean loss: 311.62
epoch train time: 0:00:00.207219
elapsed time: 0:00:44.148576
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:26:03.914312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.07
 ---- batch: 020 ----
mean loss: 311.75
 ---- batch: 030 ----
mean loss: 311.06
 ---- batch: 040 ----
mean loss: 309.60
train mean loss: 311.13
epoch train time: 0:00:00.206903
elapsed time: 0:00:44.355628
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:26:04.121339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.05
 ---- batch: 020 ----
mean loss: 319.13
 ---- batch: 030 ----
mean loss: 309.86
 ---- batch: 040 ----
mean loss: 306.03
train mean loss: 311.11
epoch train time: 0:00:00.208847
elapsed time: 0:00:44.564613
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:26:04.330336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.99
 ---- batch: 020 ----
mean loss: 316.91
 ---- batch: 030 ----
mean loss: 308.03
 ---- batch: 040 ----
mean loss: 314.08
train mean loss: 310.87
epoch train time: 0:00:00.203738
elapsed time: 0:00:44.768481
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:26:04.534190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.30
 ---- batch: 020 ----
mean loss: 311.54
 ---- batch: 030 ----
mean loss: 310.72
 ---- batch: 040 ----
mean loss: 308.59
train mean loss: 311.35
epoch train time: 0:00:00.210810
elapsed time: 0:00:44.979412
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:26:04.745122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.94
 ---- batch: 020 ----
mean loss: 297.49
 ---- batch: 030 ----
mean loss: 317.43
 ---- batch: 040 ----
mean loss: 311.85
train mean loss: 310.73
epoch train time: 0:00:00.205248
elapsed time: 0:00:45.184789
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:26:04.950492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.11
 ---- batch: 020 ----
mean loss: 308.28
 ---- batch: 030 ----
mean loss: 312.88
 ---- batch: 040 ----
mean loss: 309.83
train mean loss: 310.71
epoch train time: 0:00:00.204593
elapsed time: 0:00:45.389493
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:26:05.155201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.50
 ---- batch: 020 ----
mean loss: 306.89
 ---- batch: 030 ----
mean loss: 318.96
 ---- batch: 040 ----
mean loss: 307.83
train mean loss: 310.46
epoch train time: 0:00:00.199918
elapsed time: 0:00:45.589529
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:26:05.355250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.79
 ---- batch: 020 ----
mean loss: 301.65
 ---- batch: 030 ----
mean loss: 318.59
 ---- batch: 040 ----
mean loss: 310.33
train mean loss: 310.12
epoch train time: 0:00:00.198111
elapsed time: 0:00:45.787770
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:26:05.553484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.40
 ---- batch: 020 ----
mean loss: 307.21
 ---- batch: 030 ----
mean loss: 306.74
 ---- batch: 040 ----
mean loss: 309.35
train mean loss: 310.31
epoch train time: 0:00:00.198053
elapsed time: 0:00:45.985937
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:26:05.751643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.86
 ---- batch: 020 ----
mean loss: 310.42
 ---- batch: 030 ----
mean loss: 302.85
 ---- batch: 040 ----
mean loss: 314.41
train mean loss: 310.16
epoch train time: 0:00:00.198620
elapsed time: 0:00:46.184671
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:26:05.950380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.31
 ---- batch: 020 ----
mean loss: 313.23
 ---- batch: 030 ----
mean loss: 304.14
 ---- batch: 040 ----
mean loss: 317.92
train mean loss: 309.75
epoch train time: 0:00:00.206348
elapsed time: 0:00:46.391137
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:26:06.156861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.83
 ---- batch: 020 ----
mean loss: 305.94
 ---- batch: 030 ----
mean loss: 316.81
 ---- batch: 040 ----
mean loss: 309.18
train mean loss: 309.54
epoch train time: 0:00:00.207176
elapsed time: 0:00:46.598443
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:26:06.364153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.42
 ---- batch: 020 ----
mean loss: 319.20
 ---- batch: 030 ----
mean loss: 307.44
 ---- batch: 040 ----
mean loss: 308.60
train mean loss: 308.96
epoch train time: 0:00:00.204969
elapsed time: 0:00:46.803536
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:26:06.569258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.43
 ---- batch: 020 ----
mean loss: 312.93
 ---- batch: 030 ----
mean loss: 312.23
 ---- batch: 040 ----
mean loss: 297.43
train mean loss: 309.52
epoch train time: 0:00:00.210342
elapsed time: 0:00:47.014012
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:26:06.779733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.57
 ---- batch: 020 ----
mean loss: 312.94
 ---- batch: 030 ----
mean loss: 306.38
 ---- batch: 040 ----
mean loss: 307.49
train mean loss: 309.01
epoch train time: 0:00:00.209699
elapsed time: 0:00:47.223874
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:26:06.989603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.06
 ---- batch: 020 ----
mean loss: 298.22
 ---- batch: 030 ----
mean loss: 306.29
 ---- batch: 040 ----
mean loss: 324.21
train mean loss: 308.94
epoch train time: 0:00:00.210126
elapsed time: 0:00:47.434139
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:26:07.199849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.41
 ---- batch: 020 ----
mean loss: 311.39
 ---- batch: 030 ----
mean loss: 305.68
 ---- batch: 040 ----
mean loss: 306.61
train mean loss: 308.46
epoch train time: 0:00:00.210109
elapsed time: 0:00:47.644374
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:26:07.410088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.59
 ---- batch: 020 ----
mean loss: 307.72
 ---- batch: 030 ----
mean loss: 303.60
 ---- batch: 040 ----
mean loss: 306.86
train mean loss: 308.37
epoch train time: 0:00:00.211840
elapsed time: 0:00:47.856364
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:26:07.622079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.59
 ---- batch: 020 ----
mean loss: 300.29
 ---- batch: 030 ----
mean loss: 310.10
 ---- batch: 040 ----
mean loss: 309.25
train mean loss: 308.57
epoch train time: 0:00:00.217461
elapsed time: 0:00:48.073964
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:26:07.839705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.98
 ---- batch: 020 ----
mean loss: 308.70
 ---- batch: 030 ----
mean loss: 308.19
 ---- batch: 040 ----
mean loss: 310.91
train mean loss: 307.95
epoch train time: 0:00:00.206426
elapsed time: 0:00:48.280561
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:26:08.046272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.72
 ---- batch: 020 ----
mean loss: 310.28
 ---- batch: 030 ----
mean loss: 315.17
 ---- batch: 040 ----
mean loss: 308.02
train mean loss: 307.96
epoch train time: 0:00:00.205730
elapsed time: 0:00:48.486408
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:26:08.252115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.45
 ---- batch: 020 ----
mean loss: 307.93
 ---- batch: 030 ----
mean loss: 304.67
 ---- batch: 040 ----
mean loss: 313.00
train mean loss: 308.22
epoch train time: 0:00:00.205651
elapsed time: 0:00:48.692174
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:26:08.457884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.88
 ---- batch: 020 ----
mean loss: 301.15
 ---- batch: 030 ----
mean loss: 316.17
 ---- batch: 040 ----
mean loss: 309.16
train mean loss: 307.89
epoch train time: 0:00:00.205547
elapsed time: 0:00:48.897840
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:26:08.663580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.14
 ---- batch: 020 ----
mean loss: 312.89
 ---- batch: 030 ----
mean loss: 312.55
 ---- batch: 040 ----
mean loss: 304.63
train mean loss: 306.93
epoch train time: 0:00:00.202387
elapsed time: 0:00:49.100378
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:26:08.866090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.78
 ---- batch: 020 ----
mean loss: 307.02
 ---- batch: 030 ----
mean loss: 307.02
 ---- batch: 040 ----
mean loss: 311.94
train mean loss: 307.51
epoch train time: 0:00:00.199407
elapsed time: 0:00:49.299903
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:26:09.065611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.86
 ---- batch: 020 ----
mean loss: 310.56
 ---- batch: 030 ----
mean loss: 312.41
 ---- batch: 040 ----
mean loss: 297.69
train mean loss: 307.68
epoch train time: 0:00:00.200711
elapsed time: 0:00:49.500731
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:26:09.266441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.71
 ---- batch: 020 ----
mean loss: 304.98
 ---- batch: 030 ----
mean loss: 307.65
 ---- batch: 040 ----
mean loss: 302.59
train mean loss: 307.63
epoch train time: 0:00:00.198635
elapsed time: 0:00:49.699479
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:26:09.465186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.64
 ---- batch: 020 ----
mean loss: 303.00
 ---- batch: 030 ----
mean loss: 309.12
 ---- batch: 040 ----
mean loss: 309.03
train mean loss: 307.26
epoch train time: 0:00:00.199081
elapsed time: 0:00:49.898670
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:26:09.664444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.22
 ---- batch: 020 ----
mean loss: 306.65
 ---- batch: 030 ----
mean loss: 308.17
 ---- batch: 040 ----
mean loss: 313.20
train mean loss: 306.54
epoch train time: 0:00:00.201282
elapsed time: 0:00:50.100138
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:26:09.865874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.08
 ---- batch: 020 ----
mean loss: 308.10
 ---- batch: 030 ----
mean loss: 304.34
 ---- batch: 040 ----
mean loss: 305.93
train mean loss: 306.86
epoch train time: 0:00:00.199683
elapsed time: 0:00:50.299976
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:26:10.065679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.10
 ---- batch: 020 ----
mean loss: 309.37
 ---- batch: 030 ----
mean loss: 310.72
 ---- batch: 040 ----
mean loss: 295.56
train mean loss: 306.76
epoch train time: 0:00:00.200245
elapsed time: 0:00:50.500337
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:26:10.266049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.59
 ---- batch: 020 ----
mean loss: 296.43
 ---- batch: 030 ----
mean loss: 311.83
 ---- batch: 040 ----
mean loss: 317.11
train mean loss: 306.57
epoch train time: 0:00:00.200954
elapsed time: 0:00:50.701425
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:26:10.467150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.55
 ---- batch: 020 ----
mean loss: 311.92
 ---- batch: 030 ----
mean loss: 305.87
 ---- batch: 040 ----
mean loss: 300.45
train mean loss: 306.59
epoch train time: 0:00:00.214437
elapsed time: 0:00:50.916009
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:26:10.681719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.24
 ---- batch: 020 ----
mean loss: 296.75
 ---- batch: 030 ----
mean loss: 306.41
 ---- batch: 040 ----
mean loss: 311.53
train mean loss: 306.12
epoch train time: 0:00:00.211347
elapsed time: 0:00:51.127490
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:26:10.893218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.10
 ---- batch: 020 ----
mean loss: 309.39
 ---- batch: 030 ----
mean loss: 301.85
 ---- batch: 040 ----
mean loss: 303.77
train mean loss: 306.13
epoch train time: 0:00:00.209374
elapsed time: 0:00:51.337005
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:26:11.102731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.63
 ---- batch: 020 ----
mean loss: 298.52
 ---- batch: 030 ----
mean loss: 303.23
 ---- batch: 040 ----
mean loss: 312.63
train mean loss: 305.25
epoch train time: 0:00:00.201647
elapsed time: 0:00:51.538802
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:26:11.304510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.27
 ---- batch: 020 ----
mean loss: 302.64
 ---- batch: 030 ----
mean loss: 307.09
 ---- batch: 040 ----
mean loss: 310.18
train mean loss: 305.18
epoch train time: 0:00:00.200082
elapsed time: 0:00:51.739002
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:26:11.504712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.91
 ---- batch: 020 ----
mean loss: 307.07
 ---- batch: 030 ----
mean loss: 300.07
 ---- batch: 040 ----
mean loss: 306.96
train mean loss: 305.36
epoch train time: 0:00:00.210957
elapsed time: 0:00:51.950111
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:26:11.715864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.19
 ---- batch: 020 ----
mean loss: 311.48
 ---- batch: 030 ----
mean loss: 300.30
 ---- batch: 040 ----
mean loss: 303.22
train mean loss: 305.29
epoch train time: 0:00:00.212465
elapsed time: 0:00:52.162761
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:26:11.928476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.92
 ---- batch: 020 ----
mean loss: 310.96
 ---- batch: 030 ----
mean loss: 298.80
 ---- batch: 040 ----
mean loss: 304.45
train mean loss: 305.06
epoch train time: 0:00:00.208744
elapsed time: 0:00:52.371641
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:26:12.137353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.24
 ---- batch: 020 ----
mean loss: 299.76
 ---- batch: 030 ----
mean loss: 307.27
 ---- batch: 040 ----
mean loss: 310.29
train mean loss: 304.89
epoch train time: 0:00:00.207165
elapsed time: 0:00:52.578943
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:26:12.344652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.90
 ---- batch: 020 ----
mean loss: 302.91
 ---- batch: 030 ----
mean loss: 297.05
 ---- batch: 040 ----
mean loss: 315.53
train mean loss: 304.74
epoch train time: 0:00:00.206311
elapsed time: 0:00:52.785368
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:26:12.551076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.22
 ---- batch: 020 ----
mean loss: 309.55
 ---- batch: 030 ----
mean loss: 307.34
 ---- batch: 040 ----
mean loss: 300.77
train mean loss: 304.83
epoch train time: 0:00:00.203911
elapsed time: 0:00:52.989394
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:26:12.755101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.72
 ---- batch: 020 ----
mean loss: 298.31
 ---- batch: 030 ----
mean loss: 309.63
 ---- batch: 040 ----
mean loss: 308.20
train mean loss: 304.77
epoch train time: 0:00:00.201397
elapsed time: 0:00:53.190940
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:26:12.956651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.84
 ---- batch: 020 ----
mean loss: 304.38
 ---- batch: 030 ----
mean loss: 305.94
 ---- batch: 040 ----
mean loss: 299.24
train mean loss: 304.24
epoch train time: 0:00:00.200088
elapsed time: 0:00:53.391160
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:26:13.156869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.88
 ---- batch: 020 ----
mean loss: 314.68
 ---- batch: 030 ----
mean loss: 296.76
 ---- batch: 040 ----
mean loss: 309.12
train mean loss: 304.58
epoch train time: 0:00:00.199140
elapsed time: 0:00:53.590419
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:26:13.356130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.68
 ---- batch: 020 ----
mean loss: 308.39
 ---- batch: 030 ----
mean loss: 308.19
 ---- batch: 040 ----
mean loss: 293.26
train mean loss: 303.82
epoch train time: 0:00:00.195917
elapsed time: 0:00:53.786452
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:26:13.552175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.80
 ---- batch: 020 ----
mean loss: 302.54
 ---- batch: 030 ----
mean loss: 303.00
 ---- batch: 040 ----
mean loss: 308.07
train mean loss: 303.69
epoch train time: 0:00:00.204095
elapsed time: 0:00:53.990763
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:26:13.756475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.14
 ---- batch: 020 ----
mean loss: 301.31
 ---- batch: 030 ----
mean loss: 298.59
 ---- batch: 040 ----
mean loss: 312.46
train mean loss: 303.95
epoch train time: 0:00:00.201898
elapsed time: 0:00:54.192784
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:26:13.958491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.72
 ---- batch: 020 ----
mean loss: 308.86
 ---- batch: 030 ----
mean loss: 304.96
 ---- batch: 040 ----
mean loss: 309.02
train mean loss: 303.84
epoch train time: 0:00:00.197812
elapsed time: 0:00:54.390707
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:26:14.156439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.42
 ---- batch: 020 ----
mean loss: 315.78
 ---- batch: 030 ----
mean loss: 301.68
 ---- batch: 040 ----
mean loss: 298.99
train mean loss: 303.09
epoch train time: 0:00:00.196362
elapsed time: 0:00:54.587220
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:26:14.352933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.63
 ---- batch: 020 ----
mean loss: 303.51
 ---- batch: 030 ----
mean loss: 302.63
 ---- batch: 040 ----
mean loss: 304.88
train mean loss: 303.00
epoch train time: 0:00:00.192658
elapsed time: 0:00:54.780009
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:26:14.545714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.00
 ---- batch: 020 ----
mean loss: 302.35
 ---- batch: 030 ----
mean loss: 298.95
 ---- batch: 040 ----
mean loss: 306.03
train mean loss: 302.95
epoch train time: 0:00:00.200798
elapsed time: 0:00:54.980964
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:26:14.746672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.59
 ---- batch: 020 ----
mean loss: 304.35
 ---- batch: 030 ----
mean loss: 308.00
 ---- batch: 040 ----
mean loss: 301.89
train mean loss: 302.88
epoch train time: 0:00:00.215923
elapsed time: 0:00:55.197001
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:26:14.962724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.27
 ---- batch: 020 ----
mean loss: 300.04
 ---- batch: 030 ----
mean loss: 298.86
 ---- batch: 040 ----
mean loss: 300.43
train mean loss: 302.53
epoch train time: 0:00:00.202528
elapsed time: 0:00:55.399664
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:26:15.165374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.54
 ---- batch: 020 ----
mean loss: 313.16
 ---- batch: 030 ----
mean loss: 297.18
 ---- batch: 040 ----
mean loss: 307.07
train mean loss: 302.25
epoch train time: 0:00:00.202491
elapsed time: 0:00:55.602287
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:26:15.367996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.24
 ---- batch: 020 ----
mean loss: 299.03
 ---- batch: 030 ----
mean loss: 299.88
 ---- batch: 040 ----
mean loss: 305.66
train mean loss: 302.44
epoch train time: 0:00:00.203186
elapsed time: 0:00:55.805588
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:26:15.571302
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.40
 ---- batch: 020 ----
mean loss: 305.00
 ---- batch: 030 ----
mean loss: 300.32
 ---- batch: 040 ----
mean loss: 303.17
train mean loss: 302.36
epoch train time: 0:00:00.205149
elapsed time: 0:00:56.010892
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:26:15.776596
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 294.83
 ---- batch: 020 ----
mean loss: 307.58
 ---- batch: 030 ----
mean loss: 300.94
 ---- batch: 040 ----
mean loss: 303.84
train mean loss: 301.95
epoch train time: 0:00:00.203200
elapsed time: 0:00:56.214204
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:26:15.979929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.48
 ---- batch: 020 ----
mean loss: 310.83
 ---- batch: 030 ----
mean loss: 305.91
 ---- batch: 040 ----
mean loss: 299.48
train mean loss: 302.15
epoch train time: 0:00:00.206182
elapsed time: 0:00:56.420528
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:26:16.186242
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.18
 ---- batch: 020 ----
mean loss: 303.53
 ---- batch: 030 ----
mean loss: 295.23
 ---- batch: 040 ----
mean loss: 310.53
train mean loss: 302.36
epoch train time: 0:00:00.206321
elapsed time: 0:00:56.626979
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:26:16.392692
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.84
 ---- batch: 020 ----
mean loss: 305.09
 ---- batch: 030 ----
mean loss: 303.99
 ---- batch: 040 ----
mean loss: 294.63
train mean loss: 302.03
epoch train time: 0:00:00.206189
elapsed time: 0:00:56.833291
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:26:16.599018
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.37
 ---- batch: 020 ----
mean loss: 300.04
 ---- batch: 030 ----
mean loss: 308.74
 ---- batch: 040 ----
mean loss: 300.53
train mean loss: 301.92
epoch train time: 0:00:00.208449
elapsed time: 0:00:57.041884
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:26:16.807597
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.39
 ---- batch: 020 ----
mean loss: 298.97
 ---- batch: 030 ----
mean loss: 307.34
 ---- batch: 040 ----
mean loss: 297.14
train mean loss: 302.39
epoch train time: 0:00:00.201251
elapsed time: 0:00:57.243277
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:26:17.008988
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.52
 ---- batch: 020 ----
mean loss: 309.82
 ---- batch: 030 ----
mean loss: 297.84
 ---- batch: 040 ----
mean loss: 303.22
train mean loss: 301.94
epoch train time: 0:00:00.202696
elapsed time: 0:00:57.446088
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:26:17.211795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.26
 ---- batch: 020 ----
mean loss: 307.29
 ---- batch: 030 ----
mean loss: 299.07
 ---- batch: 040 ----
mean loss: 305.70
train mean loss: 302.25
epoch train time: 0:00:00.197249
elapsed time: 0:00:57.643447
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:26:17.409154
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.32
 ---- batch: 020 ----
mean loss: 300.04
 ---- batch: 030 ----
mean loss: 297.05
 ---- batch: 040 ----
mean loss: 310.62
train mean loss: 302.07
epoch train time: 0:00:00.202780
elapsed time: 0:00:57.846339
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:26:17.612047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.07
 ---- batch: 020 ----
mean loss: 301.58
 ---- batch: 030 ----
mean loss: 302.80
 ---- batch: 040 ----
mean loss: 306.78
train mean loss: 301.93
epoch train time: 0:00:00.214154
elapsed time: 0:00:58.060623
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:26:17.826338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.15
 ---- batch: 020 ----
mean loss: 299.70
 ---- batch: 030 ----
mean loss: 299.56
 ---- batch: 040 ----
mean loss: 306.73
train mean loss: 302.39
epoch train time: 0:00:00.203811
elapsed time: 0:00:58.264555
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:26:18.030288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.47
 ---- batch: 020 ----
mean loss: 303.98
 ---- batch: 030 ----
mean loss: 301.06
 ---- batch: 040 ----
mean loss: 301.74
train mean loss: 301.90
epoch train time: 0:00:00.201228
elapsed time: 0:00:58.465926
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:26:18.231636
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.83
 ---- batch: 020 ----
mean loss: 295.97
 ---- batch: 030 ----
mean loss: 302.25
 ---- batch: 040 ----
mean loss: 307.92
train mean loss: 302.08
epoch train time: 0:00:00.202114
elapsed time: 0:00:58.668153
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:26:18.433888
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.88
 ---- batch: 020 ----
mean loss: 309.37
 ---- batch: 030 ----
mean loss: 295.43
 ---- batch: 040 ----
mean loss: 299.65
train mean loss: 302.10
epoch train time: 0:00:00.201712
elapsed time: 0:00:58.870004
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:26:18.635712
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.30
 ---- batch: 020 ----
mean loss: 297.28
 ---- batch: 030 ----
mean loss: 303.56
 ---- batch: 040 ----
mean loss: 299.40
train mean loss: 301.95
epoch train time: 0:00:00.210806
elapsed time: 0:00:59.080946
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:26:18.846671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.86
 ---- batch: 020 ----
mean loss: 302.49
 ---- batch: 030 ----
mean loss: 295.51
 ---- batch: 040 ----
mean loss: 305.91
train mean loss: 302.24
epoch train time: 0:00:00.211404
elapsed time: 0:00:59.292487
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:26:19.058198
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.56
 ---- batch: 020 ----
mean loss: 308.56
 ---- batch: 030 ----
mean loss: 300.76
 ---- batch: 040 ----
mean loss: 295.58
train mean loss: 301.83
epoch train time: 0:00:00.213025
elapsed time: 0:00:59.505668
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:26:19.271397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.21
 ---- batch: 020 ----
mean loss: 297.54
 ---- batch: 030 ----
mean loss: 296.14
 ---- batch: 040 ----
mean loss: 302.24
train mean loss: 302.03
epoch train time: 0:00:00.209765
elapsed time: 0:00:59.715575
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:26:19.481303
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.92
 ---- batch: 020 ----
mean loss: 302.74
 ---- batch: 030 ----
mean loss: 301.51
 ---- batch: 040 ----
mean loss: 304.61
train mean loss: 301.88
epoch train time: 0:00:00.209671
elapsed time: 0:00:59.925385
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:26:19.691097
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.29
 ---- batch: 020 ----
mean loss: 308.11
 ---- batch: 030 ----
mean loss: 296.80
 ---- batch: 040 ----
mean loss: 288.50
train mean loss: 302.47
epoch train time: 0:00:00.207916
elapsed time: 0:01:00.133422
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:26:19.899133
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.54
 ---- batch: 020 ----
mean loss: 301.06
 ---- batch: 030 ----
mean loss: 306.98
 ---- batch: 040 ----
mean loss: 300.97
train mean loss: 302.17
epoch train time: 0:00:00.207558
elapsed time: 0:01:00.341103
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:26:20.106826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.04
 ---- batch: 020 ----
mean loss: 305.38
 ---- batch: 030 ----
mean loss: 294.72
 ---- batch: 040 ----
mean loss: 302.05
train mean loss: 301.87
epoch train time: 0:00:00.207162
elapsed time: 0:01:00.548397
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:26:20.314108
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.31
 ---- batch: 020 ----
mean loss: 303.57
 ---- batch: 030 ----
mean loss: 302.66
 ---- batch: 040 ----
mean loss: 299.20
train mean loss: 302.02
epoch train time: 0:00:00.208275
elapsed time: 0:01:00.756812
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:26:20.522541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.40
 ---- batch: 020 ----
mean loss: 290.36
 ---- batch: 030 ----
mean loss: 302.75
 ---- batch: 040 ----
mean loss: 306.67
train mean loss: 301.92
epoch train time: 0:00:00.208886
elapsed time: 0:01:00.965837
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:26:20.731548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.26
 ---- batch: 020 ----
mean loss: 297.38
 ---- batch: 030 ----
mean loss: 306.34
 ---- batch: 040 ----
mean loss: 299.73
train mean loss: 301.63
epoch train time: 0:00:00.202405
elapsed time: 0:01:01.168360
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:26:20.934083
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.75
 ---- batch: 020 ----
mean loss: 302.65
 ---- batch: 030 ----
mean loss: 298.95
 ---- batch: 040 ----
mean loss: 298.66
train mean loss: 301.67
epoch train time: 0:00:00.198649
elapsed time: 0:01:01.367144
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:26:21.132883
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.90
 ---- batch: 020 ----
mean loss: 296.97
 ---- batch: 030 ----
mean loss: 304.40
 ---- batch: 040 ----
mean loss: 303.48
train mean loss: 301.69
epoch train time: 0:00:00.196944
elapsed time: 0:01:01.564229
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:26:21.329936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.60
 ---- batch: 020 ----
mean loss: 308.25
 ---- batch: 030 ----
mean loss: 294.41
 ---- batch: 040 ----
mean loss: 302.41
train mean loss: 301.65
epoch train time: 0:00:00.194809
elapsed time: 0:01:01.759171
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:26:21.524896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.93
 ---- batch: 020 ----
mean loss: 304.92
 ---- batch: 030 ----
mean loss: 305.16
 ---- batch: 040 ----
mean loss: 307.10
train mean loss: 301.24
epoch train time: 0:00:00.197873
elapsed time: 0:01:01.957173
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:26:21.722879
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.23
 ---- batch: 020 ----
mean loss: 297.49
 ---- batch: 030 ----
mean loss: 307.34
 ---- batch: 040 ----
mean loss: 303.08
train mean loss: 301.64
epoch train time: 0:00:00.203616
elapsed time: 0:01:02.160901
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:26:21.926608
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.37
 ---- batch: 020 ----
mean loss: 302.56
 ---- batch: 030 ----
mean loss: 306.10
 ---- batch: 040 ----
mean loss: 304.77
train mean loss: 301.78
epoch train time: 0:00:00.200523
elapsed time: 0:01:02.361537
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:26:22.127248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.38
 ---- batch: 020 ----
mean loss: 307.92
 ---- batch: 030 ----
mean loss: 300.92
 ---- batch: 040 ----
mean loss: 299.89
train mean loss: 301.72
epoch train time: 0:00:00.201811
elapsed time: 0:01:02.563490
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:26:22.329206
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.80
 ---- batch: 020 ----
mean loss: 307.06
 ---- batch: 030 ----
mean loss: 305.45
 ---- batch: 040 ----
mean loss: 294.23
train mean loss: 301.63
epoch train time: 0:00:00.201650
elapsed time: 0:01:02.765260
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:26:22.530970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.98
 ---- batch: 020 ----
mean loss: 303.69
 ---- batch: 030 ----
mean loss: 300.98
 ---- batch: 040 ----
mean loss: 300.97
train mean loss: 301.33
epoch train time: 0:00:00.203665
elapsed time: 0:01:02.969071
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:26:22.734780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 307.37
 ---- batch: 020 ----
mean loss: 293.98
 ---- batch: 030 ----
mean loss: 304.81
 ---- batch: 040 ----
mean loss: 295.26
train mean loss: 301.72
epoch train time: 0:00:00.206205
elapsed time: 0:01:03.175394
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:26:22.941120
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 310.62
 ---- batch: 020 ----
mean loss: 297.09
 ---- batch: 030 ----
mean loss: 302.90
 ---- batch: 040 ----
mean loss: 299.41
train mean loss: 301.36
epoch train time: 0:00:00.207156
elapsed time: 0:01:03.382689
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:26:23.148424
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.16
 ---- batch: 020 ----
mean loss: 298.08
 ---- batch: 030 ----
mean loss: 299.00
 ---- batch: 040 ----
mean loss: 296.61
train mean loss: 301.64
epoch train time: 0:00:00.212786
elapsed time: 0:01:03.595621
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:26:23.361350
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.96
 ---- batch: 020 ----
mean loss: 302.19
 ---- batch: 030 ----
mean loss: 298.50
 ---- batch: 040 ----
mean loss: 308.17
train mean loss: 301.34
epoch train time: 0:00:00.209228
elapsed time: 0:01:03.805005
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:26:23.570732
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.59
 ---- batch: 020 ----
mean loss: 305.56
 ---- batch: 030 ----
mean loss: 303.00
 ---- batch: 040 ----
mean loss: 303.56
train mean loss: 301.31
epoch train time: 0:00:00.208191
elapsed time: 0:01:04.013336
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:26:23.779047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.09
 ---- batch: 020 ----
mean loss: 304.51
 ---- batch: 030 ----
mean loss: 305.35
 ---- batch: 040 ----
mean loss: 299.65
train mean loss: 301.74
epoch train time: 0:00:00.203491
elapsed time: 0:01:04.216958
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:26:23.982666
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.05
 ---- batch: 020 ----
mean loss: 304.66
 ---- batch: 030 ----
mean loss: 305.08
 ---- batch: 040 ----
mean loss: 296.24
train mean loss: 301.57
epoch train time: 0:00:00.203856
elapsed time: 0:01:04.420944
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:26:24.186654
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.99
 ---- batch: 020 ----
mean loss: 300.71
 ---- batch: 030 ----
mean loss: 299.33
 ---- batch: 040 ----
mean loss: 304.92
train mean loss: 301.36
epoch train time: 0:00:00.203108
elapsed time: 0:01:04.624168
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:26:24.389877
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.92
 ---- batch: 020 ----
mean loss: 303.35
 ---- batch: 030 ----
mean loss: 305.33
 ---- batch: 040 ----
mean loss: 303.56
train mean loss: 301.32
epoch train time: 0:00:00.202718
elapsed time: 0:01:04.827009
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:26:24.592719
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.32
 ---- batch: 020 ----
mean loss: 301.36
 ---- batch: 030 ----
mean loss: 300.51
 ---- batch: 040 ----
mean loss: 302.51
train mean loss: 301.26
epoch train time: 0:00:00.210478
elapsed time: 0:01:05.037633
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:26:24.803363
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.80
 ---- batch: 020 ----
mean loss: 301.60
 ---- batch: 030 ----
mean loss: 307.07
 ---- batch: 040 ----
mean loss: 295.05
train mean loss: 301.75
epoch train time: 0:00:00.214564
elapsed time: 0:01:05.252337
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:26:25.018047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.67
 ---- batch: 020 ----
mean loss: 306.92
 ---- batch: 030 ----
mean loss: 300.04
 ---- batch: 040 ----
mean loss: 296.98
train mean loss: 301.97
epoch train time: 0:00:00.202826
elapsed time: 0:01:05.455283
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:26:25.220993
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.67
 ---- batch: 020 ----
mean loss: 295.30
 ---- batch: 030 ----
mean loss: 310.14
 ---- batch: 040 ----
mean loss: 294.97
train mean loss: 301.32
epoch train time: 0:00:00.198983
elapsed time: 0:01:05.654382
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:26:25.420090
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.37
 ---- batch: 020 ----
mean loss: 307.92
 ---- batch: 030 ----
mean loss: 299.72
 ---- batch: 040 ----
mean loss: 296.04
train mean loss: 301.30
epoch train time: 0:00:00.199479
elapsed time: 0:01:05.855762
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_1/checkpoint.pth.tar
**** end time: 2019-09-20 23:26:25.621443 ****
