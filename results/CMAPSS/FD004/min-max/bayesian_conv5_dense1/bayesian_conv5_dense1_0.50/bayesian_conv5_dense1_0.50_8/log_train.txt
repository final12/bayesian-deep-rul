Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 7535
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 15:00:16.724682 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 15:00:16.742455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2342.56
 ---- batch: 020 ----
mean loss: 1450.67
 ---- batch: 030 ----
mean loss: 1214.99
 ---- batch: 040 ----
mean loss: 1178.68
 ---- batch: 050 ----
mean loss: 1132.12
train mean loss: 1413.69
epoch train time: 0:00:22.886767
elapsed time: 0:00:22.912717
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 15:00:39.637449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1098.90
 ---- batch: 020 ----
mean loss: 1088.03
 ---- batch: 030 ----
mean loss: 1009.24
 ---- batch: 040 ----
mean loss: 1027.43
 ---- batch: 050 ----
mean loss: 1014.89
train mean loss: 1044.69
epoch train time: 0:00:07.830812
elapsed time: 0:00:30.744278
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 15:00:47.469335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 971.13
 ---- batch: 020 ----
mean loss: 980.94
 ---- batch: 030 ----
mean loss: 997.69
 ---- batch: 040 ----
mean loss: 973.04
 ---- batch: 050 ----
mean loss: 946.37
train mean loss: 974.09
epoch train time: 0:00:07.826206
elapsed time: 0:00:38.571581
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 15:00:55.296636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.63
 ---- batch: 020 ----
mean loss: 921.35
 ---- batch: 030 ----
mean loss: 953.77
 ---- batch: 040 ----
mean loss: 946.47
 ---- batch: 050 ----
mean loss: 963.08
train mean loss: 944.69
epoch train time: 0:00:07.839909
elapsed time: 0:00:46.412580
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 15:01:03.137656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 970.82
 ---- batch: 020 ----
mean loss: 961.99
 ---- batch: 030 ----
mean loss: 918.08
 ---- batch: 040 ----
mean loss: 919.61
 ---- batch: 050 ----
mean loss: 903.54
train mean loss: 929.39
epoch train time: 0:00:07.824639
elapsed time: 0:00:54.238369
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 15:01:10.963422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.11
 ---- batch: 020 ----
mean loss: 930.45
 ---- batch: 030 ----
mean loss: 914.03
 ---- batch: 040 ----
mean loss: 927.58
 ---- batch: 050 ----
mean loss: 911.25
train mean loss: 922.71
epoch train time: 0:00:07.846808
elapsed time: 0:01:02.086297
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 15:01:18.811340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.96
 ---- batch: 020 ----
mean loss: 910.31
 ---- batch: 030 ----
mean loss: 895.39
 ---- batch: 040 ----
mean loss: 930.91
 ---- batch: 050 ----
mean loss: 888.17
train mean loss: 914.26
epoch train time: 0:00:07.848008
elapsed time: 0:01:09.935450
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 15:01:26.660518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.19
 ---- batch: 020 ----
mean loss: 917.76
 ---- batch: 030 ----
mean loss: 924.77
 ---- batch: 040 ----
mean loss: 903.71
 ---- batch: 050 ----
mean loss: 906.30
train mean loss: 908.69
epoch train time: 0:00:07.844039
elapsed time: 0:01:17.780606
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 15:01:34.505781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.56
 ---- batch: 020 ----
mean loss: 880.19
 ---- batch: 030 ----
mean loss: 903.06
 ---- batch: 040 ----
mean loss: 904.71
 ---- batch: 050 ----
mean loss: 907.92
train mean loss: 904.86
epoch train time: 0:00:07.841677
elapsed time: 0:01:25.623710
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 15:01:42.348758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.77
 ---- batch: 020 ----
mean loss: 889.24
 ---- batch: 030 ----
mean loss: 903.04
 ---- batch: 040 ----
mean loss: 921.86
 ---- batch: 050 ----
mean loss: 899.65
train mean loss: 902.05
epoch train time: 0:00:07.836687
elapsed time: 0:01:33.461500
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 15:01:50.186563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.02
 ---- batch: 020 ----
mean loss: 902.47
 ---- batch: 030 ----
mean loss: 904.22
 ---- batch: 040 ----
mean loss: 873.91
 ---- batch: 050 ----
mean loss: 899.36
train mean loss: 895.47
epoch train time: 0:00:07.838628
elapsed time: 0:01:41.301241
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 15:01:58.026275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.83
 ---- batch: 020 ----
mean loss: 884.98
 ---- batch: 030 ----
mean loss: 871.53
 ---- batch: 040 ----
mean loss: 890.06
 ---- batch: 050 ----
mean loss: 892.00
train mean loss: 885.42
epoch train time: 0:00:07.807300
elapsed time: 0:01:49.109786
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 15:02:05.834779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.89
 ---- batch: 020 ----
mean loss: 890.39
 ---- batch: 030 ----
mean loss: 905.61
 ---- batch: 040 ----
mean loss: 893.44
 ---- batch: 050 ----
mean loss: 871.84
train mean loss: 888.14
epoch train time: 0:00:07.871767
elapsed time: 0:01:56.982667
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 15:02:13.707723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.89
 ---- batch: 020 ----
mean loss: 877.89
 ---- batch: 030 ----
mean loss: 876.88
 ---- batch: 040 ----
mean loss: 889.52
 ---- batch: 050 ----
mean loss: 862.69
train mean loss: 879.68
epoch train time: 0:00:07.854478
elapsed time: 0:02:04.838339
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 15:02:21.563411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.78
 ---- batch: 020 ----
mean loss: 873.70
 ---- batch: 030 ----
mean loss: 898.65
 ---- batch: 040 ----
mean loss: 849.54
 ---- batch: 050 ----
mean loss: 877.12
train mean loss: 876.66
epoch train time: 0:00:07.834227
elapsed time: 0:02:12.673740
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 15:02:29.398785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.09
 ---- batch: 020 ----
mean loss: 870.90
 ---- batch: 030 ----
mean loss: 871.15
 ---- batch: 040 ----
mean loss: 884.67
 ---- batch: 050 ----
mean loss: 867.91
train mean loss: 872.87
epoch train time: 0:00:07.838852
elapsed time: 0:02:20.513751
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 15:02:37.238781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.59
 ---- batch: 020 ----
mean loss: 863.11
 ---- batch: 030 ----
mean loss: 860.61
 ---- batch: 040 ----
mean loss: 866.25
 ---- batch: 050 ----
mean loss: 862.49
train mean loss: 867.73
epoch train time: 0:00:07.830307
elapsed time: 0:02:28.345230
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 15:02:45.070295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.80
 ---- batch: 020 ----
mean loss: 861.11
 ---- batch: 030 ----
mean loss: 866.90
 ---- batch: 040 ----
mean loss: 849.22
 ---- batch: 050 ----
mean loss: 857.24
train mean loss: 864.91
epoch train time: 0:00:07.817120
elapsed time: 0:02:36.163436
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 15:02:52.888495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.95
 ---- batch: 020 ----
mean loss: 850.32
 ---- batch: 030 ----
mean loss: 878.61
 ---- batch: 040 ----
mean loss: 882.58
 ---- batch: 050 ----
mean loss: 836.15
train mean loss: 861.62
epoch train time: 0:00:07.863793
elapsed time: 0:02:44.028414
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 15:03:00.753461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.38
 ---- batch: 020 ----
mean loss: 869.43
 ---- batch: 030 ----
mean loss: 856.28
 ---- batch: 040 ----
mean loss: 833.90
 ---- batch: 050 ----
mean loss: 847.74
train mean loss: 851.81
epoch train time: 0:00:07.870197
elapsed time: 0:02:51.899834
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 15:03:08.624901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.53
 ---- batch: 020 ----
mean loss: 867.38
 ---- batch: 030 ----
mean loss: 834.30
 ---- batch: 040 ----
mean loss: 865.72
 ---- batch: 050 ----
mean loss: 825.34
train mean loss: 846.02
epoch train time: 0:00:07.899844
elapsed time: 0:02:59.800889
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 15:03:16.525978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.22
 ---- batch: 020 ----
mean loss: 837.32
 ---- batch: 030 ----
mean loss: 848.41
 ---- batch: 040 ----
mean loss: 835.93
 ---- batch: 050 ----
mean loss: 833.14
train mean loss: 840.16
epoch train time: 0:00:07.872513
elapsed time: 0:03:07.674696
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 15:03:24.399604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.14
 ---- batch: 020 ----
mean loss: 835.07
 ---- batch: 030 ----
mean loss: 816.87
 ---- batch: 040 ----
mean loss: 840.03
 ---- batch: 050 ----
mean loss: 838.58
train mean loss: 833.06
epoch train time: 0:00:07.852919
elapsed time: 0:03:15.528535
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 15:03:32.253621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.89
 ---- batch: 020 ----
mean loss: 825.08
 ---- batch: 030 ----
mean loss: 828.32
 ---- batch: 040 ----
mean loss: 806.33
 ---- batch: 050 ----
mean loss: 826.02
train mean loss: 822.39
epoch train time: 0:00:07.835863
elapsed time: 0:03:23.365584
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 15:03:40.090637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.30
 ---- batch: 020 ----
mean loss: 799.51
 ---- batch: 030 ----
mean loss: 812.98
 ---- batch: 040 ----
mean loss: 812.49
 ---- batch: 050 ----
mean loss: 817.96
train mean loss: 810.22
epoch train time: 0:00:07.848114
elapsed time: 0:03:31.214929
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 15:03:47.940003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 795.86
 ---- batch: 020 ----
mean loss: 789.16
 ---- batch: 030 ----
mean loss: 804.98
 ---- batch: 040 ----
mean loss: 800.91
 ---- batch: 050 ----
mean loss: 786.68
train mean loss: 793.69
epoch train time: 0:00:07.855533
elapsed time: 0:03:39.071585
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 15:03:55.796672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 762.41
 ---- batch: 020 ----
mean loss: 780.03
 ---- batch: 030 ----
mean loss: 764.91
 ---- batch: 040 ----
mean loss: 788.78
 ---- batch: 050 ----
mean loss: 785.15
train mean loss: 778.20
epoch train time: 0:00:07.916225
elapsed time: 0:03:46.989053
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 15:04:03.714110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 759.08
 ---- batch: 020 ----
mean loss: 758.07
 ---- batch: 030 ----
mean loss: 761.76
 ---- batch: 040 ----
mean loss: 756.52
 ---- batch: 050 ----
mean loss: 767.62
train mean loss: 762.27
epoch train time: 0:00:07.912903
elapsed time: 0:03:54.903067
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 15:04:11.628214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 754.56
 ---- batch: 020 ----
mean loss: 740.79
 ---- batch: 030 ----
mean loss: 735.33
 ---- batch: 040 ----
mean loss: 718.22
 ---- batch: 050 ----
mean loss: 721.07
train mean loss: 728.99
epoch train time: 0:00:07.840393
elapsed time: 0:04:02.744639
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 15:04:19.469699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.02
 ---- batch: 020 ----
mean loss: 707.83
 ---- batch: 030 ----
mean loss: 689.71
 ---- batch: 040 ----
mean loss: 681.61
 ---- batch: 050 ----
mean loss: 673.83
train mean loss: 691.82
epoch train time: 0:00:07.889487
elapsed time: 0:04:10.635306
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 15:04:27.360394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.31
 ---- batch: 020 ----
mean loss: 670.56
 ---- batch: 030 ----
mean loss: 664.86
 ---- batch: 040 ----
mean loss: 640.57
 ---- batch: 050 ----
mean loss: 663.64
train mean loss: 662.28
epoch train time: 0:00:07.860713
elapsed time: 0:04:18.497200
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 15:04:35.222249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 635.66
 ---- batch: 020 ----
mean loss: 645.87
 ---- batch: 030 ----
mean loss: 639.12
 ---- batch: 040 ----
mean loss: 627.32
 ---- batch: 050 ----
mean loss: 620.75
train mean loss: 635.76
epoch train time: 0:00:07.858729
elapsed time: 0:04:26.357138
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 15:04:43.082212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.65
 ---- batch: 020 ----
mean loss: 611.76
 ---- batch: 030 ----
mean loss: 618.77
 ---- batch: 040 ----
mean loss: 611.28
 ---- batch: 050 ----
mean loss: 622.01
train mean loss: 612.10
epoch train time: 0:00:07.870324
elapsed time: 0:04:34.228608
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 15:04:50.953677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 594.74
 ---- batch: 020 ----
mean loss: 617.62
 ---- batch: 030 ----
mean loss: 592.65
 ---- batch: 040 ----
mean loss: 567.28
 ---- batch: 050 ----
mean loss: 588.50
train mean loss: 589.08
epoch train time: 0:00:07.858021
elapsed time: 0:04:42.087948
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 15:04:58.813046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 571.42
 ---- batch: 020 ----
mean loss: 581.64
 ---- batch: 030 ----
mean loss: 569.77
 ---- batch: 040 ----
mean loss: 567.27
 ---- batch: 050 ----
mean loss: 551.40
train mean loss: 569.74
epoch train time: 0:00:07.851771
elapsed time: 0:04:49.940999
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 15:05:06.666080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 575.83
 ---- batch: 020 ----
mean loss: 550.05
 ---- batch: 030 ----
mean loss: 550.33
 ---- batch: 040 ----
mean loss: 537.01
 ---- batch: 050 ----
mean loss: 543.02
train mean loss: 550.46
epoch train time: 0:00:07.860119
elapsed time: 0:04:57.802272
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 15:05:14.527337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.52
 ---- batch: 020 ----
mean loss: 516.96
 ---- batch: 030 ----
mean loss: 536.41
 ---- batch: 040 ----
mean loss: 541.01
 ---- batch: 050 ----
mean loss: 538.36
train mean loss: 531.73
epoch train time: 0:00:07.864405
elapsed time: 0:05:05.667814
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 15:05:22.392861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.39
 ---- batch: 020 ----
mean loss: 536.43
 ---- batch: 030 ----
mean loss: 534.78
 ---- batch: 040 ----
mean loss: 500.29
 ---- batch: 050 ----
mean loss: 502.41
train mean loss: 521.59
epoch train time: 0:00:07.855688
elapsed time: 0:05:13.524719
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 15:05:30.249842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.51
 ---- batch: 020 ----
mean loss: 498.63
 ---- batch: 030 ----
mean loss: 504.37
 ---- batch: 040 ----
mean loss: 488.54
 ---- batch: 050 ----
mean loss: 486.85
train mean loss: 500.58
epoch train time: 0:00:07.842517
elapsed time: 0:05:21.368623
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 15:05:38.093701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.16
 ---- batch: 020 ----
mean loss: 499.14
 ---- batch: 030 ----
mean loss: 482.58
 ---- batch: 040 ----
mean loss: 501.15
 ---- batch: 050 ----
mean loss: 492.66
train mean loss: 494.45
epoch train time: 0:00:07.891956
elapsed time: 0:05:29.261715
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 15:05:45.986742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.24
 ---- batch: 020 ----
mean loss: 480.34
 ---- batch: 030 ----
mean loss: 478.49
 ---- batch: 040 ----
mean loss: 478.16
 ---- batch: 050 ----
mean loss: 467.11
train mean loss: 483.65
epoch train time: 0:00:07.842852
elapsed time: 0:05:37.105743
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 15:05:53.830814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.00
 ---- batch: 020 ----
mean loss: 488.06
 ---- batch: 030 ----
mean loss: 476.16
 ---- batch: 040 ----
mean loss: 463.44
 ---- batch: 050 ----
mean loss: 470.34
train mean loss: 471.09
epoch train time: 0:00:07.845341
elapsed time: 0:05:44.952252
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 15:06:01.677321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.14
 ---- batch: 020 ----
mean loss: 474.49
 ---- batch: 030 ----
mean loss: 452.01
 ---- batch: 040 ----
mean loss: 461.31
 ---- batch: 050 ----
mean loss: 459.31
train mean loss: 461.98
epoch train time: 0:00:07.830253
elapsed time: 0:05:52.783667
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 15:06:09.508722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.70
 ---- batch: 020 ----
mean loss: 450.65
 ---- batch: 030 ----
mean loss: 432.14
 ---- batch: 040 ----
mean loss: 446.61
 ---- batch: 050 ----
mean loss: 453.65
train mean loss: 449.36
epoch train time: 0:00:07.856552
elapsed time: 0:06:00.641401
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 15:06:17.366317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.53
 ---- batch: 020 ----
mean loss: 439.80
 ---- batch: 030 ----
mean loss: 443.38
 ---- batch: 040 ----
mean loss: 454.49
 ---- batch: 050 ----
mean loss: 447.39
train mean loss: 441.50
epoch train time: 0:00:07.875744
elapsed time: 0:06:08.518082
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 15:06:25.243138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.61
 ---- batch: 020 ----
mean loss: 438.27
 ---- batch: 030 ----
mean loss: 435.81
 ---- batch: 040 ----
mean loss: 438.94
 ---- batch: 050 ----
mean loss: 437.45
train mean loss: 437.11
epoch train time: 0:00:07.837882
elapsed time: 0:06:16.357073
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 15:06:33.082125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.16
 ---- batch: 020 ----
mean loss: 424.25
 ---- batch: 030 ----
mean loss: 433.57
 ---- batch: 040 ----
mean loss: 422.02
 ---- batch: 050 ----
mean loss: 417.56
train mean loss: 425.83
epoch train time: 0:00:07.816310
elapsed time: 0:06:24.174482
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 15:06:40.899563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.52
 ---- batch: 020 ----
mean loss: 410.03
 ---- batch: 030 ----
mean loss: 418.70
 ---- batch: 040 ----
mean loss: 411.78
 ---- batch: 050 ----
mean loss: 413.68
train mean loss: 412.31
epoch train time: 0:00:07.840929
elapsed time: 0:06:32.016545
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 15:06:48.741632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.41
 ---- batch: 020 ----
mean loss: 403.39
 ---- batch: 030 ----
mean loss: 411.42
 ---- batch: 040 ----
mean loss: 406.97
 ---- batch: 050 ----
mean loss: 402.60
train mean loss: 410.05
epoch train time: 0:00:07.846249
elapsed time: 0:06:39.863942
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 15:06:56.589070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.88
 ---- batch: 020 ----
mean loss: 409.35
 ---- batch: 030 ----
mean loss: 410.30
 ---- batch: 040 ----
mean loss: 402.30
 ---- batch: 050 ----
mean loss: 401.33
train mean loss: 403.23
epoch train time: 0:00:07.861606
elapsed time: 0:06:47.726727
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 15:07:04.451823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.95
 ---- batch: 020 ----
mean loss: 389.66
 ---- batch: 030 ----
mean loss: 396.32
 ---- batch: 040 ----
mean loss: 389.04
 ---- batch: 050 ----
mean loss: 388.71
train mean loss: 391.35
epoch train time: 0:00:07.821058
elapsed time: 0:06:55.548979
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 15:07:12.274017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.71
 ---- batch: 020 ----
mean loss: 388.93
 ---- batch: 030 ----
mean loss: 384.32
 ---- batch: 040 ----
mean loss: 379.01
 ---- batch: 050 ----
mean loss: 387.35
train mean loss: 387.28
epoch train time: 0:00:07.879527
elapsed time: 0:07:03.429593
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 15:07:20.154678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.92
 ---- batch: 020 ----
mean loss: 390.49
 ---- batch: 030 ----
mean loss: 376.38
 ---- batch: 040 ----
mean loss: 387.87
 ---- batch: 050 ----
mean loss: 386.98
train mean loss: 383.62
epoch train time: 0:00:07.840219
elapsed time: 0:07:11.270959
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 15:07:27.996067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.36
 ---- batch: 020 ----
mean loss: 378.42
 ---- batch: 030 ----
mean loss: 381.70
 ---- batch: 040 ----
mean loss: 374.53
 ---- batch: 050 ----
mean loss: 377.26
train mean loss: 377.67
epoch train time: 0:00:07.830256
elapsed time: 0:07:19.102392
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 15:07:35.827470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.35
 ---- batch: 020 ----
mean loss: 381.14
 ---- batch: 030 ----
mean loss: 369.26
 ---- batch: 040 ----
mean loss: 370.74
 ---- batch: 050 ----
mean loss: 363.36
train mean loss: 371.34
epoch train time: 0:00:07.826433
elapsed time: 0:07:26.929941
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 15:07:43.654993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.29
 ---- batch: 020 ----
mean loss: 363.71
 ---- batch: 030 ----
mean loss: 378.34
 ---- batch: 040 ----
mean loss: 354.85
 ---- batch: 050 ----
mean loss: 363.78
train mean loss: 362.49
epoch train time: 0:00:07.839109
elapsed time: 0:07:34.770169
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 15:07:51.495272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.31
 ---- batch: 020 ----
mean loss: 357.66
 ---- batch: 030 ----
mean loss: 358.43
 ---- batch: 040 ----
mean loss: 364.44
 ---- batch: 050 ----
mean loss: 365.62
train mean loss: 361.28
epoch train time: 0:00:07.849840
elapsed time: 0:07:42.621218
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 15:07:59.346279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.79
 ---- batch: 020 ----
mean loss: 357.00
 ---- batch: 030 ----
mean loss: 349.26
 ---- batch: 040 ----
mean loss: 355.11
 ---- batch: 050 ----
mean loss: 363.15
train mean loss: 356.38
epoch train time: 0:00:07.829026
elapsed time: 0:07:50.451353
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 15:08:07.176403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.66
 ---- batch: 020 ----
mean loss: 347.14
 ---- batch: 030 ----
mean loss: 358.18
 ---- batch: 040 ----
mean loss: 350.04
 ---- batch: 050 ----
mean loss: 347.10
train mean loss: 349.81
epoch train time: 0:00:07.855600
elapsed time: 0:07:58.308066
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 15:08:15.033101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.94
 ---- batch: 020 ----
mean loss: 333.08
 ---- batch: 030 ----
mean loss: 355.35
 ---- batch: 040 ----
mean loss: 349.14
 ---- batch: 050 ----
mean loss: 353.41
train mean loss: 348.98
epoch train time: 0:00:07.825429
elapsed time: 0:08:06.134560
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 15:08:22.859623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.82
 ---- batch: 020 ----
mean loss: 346.73
 ---- batch: 030 ----
mean loss: 348.58
 ---- batch: 040 ----
mean loss: 333.10
 ---- batch: 050 ----
mean loss: 339.92
train mean loss: 341.07
epoch train time: 0:00:07.827716
elapsed time: 0:08:13.963466
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 15:08:30.688601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.44
 ---- batch: 020 ----
mean loss: 333.27
 ---- batch: 030 ----
mean loss: 345.39
 ---- batch: 040 ----
mean loss: 341.57
 ---- batch: 050 ----
mean loss: 341.48
train mean loss: 340.47
epoch train time: 0:00:07.824166
elapsed time: 0:08:21.788783
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 15:08:38.513822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.54
 ---- batch: 020 ----
mean loss: 325.09
 ---- batch: 030 ----
mean loss: 329.13
 ---- batch: 040 ----
mean loss: 341.68
 ---- batch: 050 ----
mean loss: 355.16
train mean loss: 336.96
epoch train time: 0:00:07.855577
elapsed time: 0:08:29.645876
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 15:08:46.370926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.01
 ---- batch: 020 ----
mean loss: 335.56
 ---- batch: 030 ----
mean loss: 329.70
 ---- batch: 040 ----
mean loss: 329.46
 ---- batch: 050 ----
mean loss: 326.28
train mean loss: 331.59
epoch train time: 0:00:07.838923
elapsed time: 0:08:37.485963
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 15:08:54.211008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.47
 ---- batch: 020 ----
mean loss: 326.50
 ---- batch: 030 ----
mean loss: 339.74
 ---- batch: 040 ----
mean loss: 335.33
 ---- batch: 050 ----
mean loss: 338.81
train mean loss: 332.58
epoch train time: 0:00:07.858245
elapsed time: 0:08:45.345283
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 15:09:02.070373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.92
 ---- batch: 020 ----
mean loss: 339.37
 ---- batch: 030 ----
mean loss: 324.88
 ---- batch: 040 ----
mean loss: 330.31
 ---- batch: 050 ----
mean loss: 334.50
train mean loss: 330.35
epoch train time: 0:00:07.829898
elapsed time: 0:08:53.176324
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 15:09:09.901365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.16
 ---- batch: 020 ----
mean loss: 326.37
 ---- batch: 030 ----
mean loss: 331.29
 ---- batch: 040 ----
mean loss: 331.46
 ---- batch: 050 ----
mean loss: 337.65
train mean loss: 329.36
epoch train time: 0:00:07.883462
elapsed time: 0:09:01.061030
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 15:09:17.786065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.10
 ---- batch: 020 ----
mean loss: 317.79
 ---- batch: 030 ----
mean loss: 322.11
 ---- batch: 040 ----
mean loss: 322.44
 ---- batch: 050 ----
mean loss: 322.69
train mean loss: 321.36
epoch train time: 0:00:07.868723
elapsed time: 0:09:08.930911
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 15:09:25.655999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.03
 ---- batch: 020 ----
mean loss: 320.75
 ---- batch: 030 ----
mean loss: 323.56
 ---- batch: 040 ----
mean loss: 320.98
 ---- batch: 050 ----
mean loss: 316.60
train mean loss: 319.61
epoch train time: 0:00:07.822017
elapsed time: 0:09:16.754111
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 15:09:33.479185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.46
 ---- batch: 020 ----
mean loss: 326.79
 ---- batch: 030 ----
mean loss: 312.61
 ---- batch: 040 ----
mean loss: 320.81
 ---- batch: 050 ----
mean loss: 320.01
train mean loss: 318.45
epoch train time: 0:00:07.813323
elapsed time: 0:09:24.568599
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 15:09:41.293629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.05
 ---- batch: 020 ----
mean loss: 316.60
 ---- batch: 030 ----
mean loss: 308.14
 ---- batch: 040 ----
mean loss: 314.85
 ---- batch: 050 ----
mean loss: 316.14
train mean loss: 316.11
epoch train time: 0:00:07.819132
elapsed time: 0:09:32.388836
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 15:09:49.113931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.74
 ---- batch: 020 ----
mean loss: 318.84
 ---- batch: 030 ----
mean loss: 311.56
 ---- batch: 040 ----
mean loss: 318.82
 ---- batch: 050 ----
mean loss: 308.07
train mean loss: 310.76
epoch train time: 0:00:07.855934
elapsed time: 0:09:40.245941
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 15:09:56.970965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.54
 ---- batch: 020 ----
mean loss: 313.90
 ---- batch: 030 ----
mean loss: 301.20
 ---- batch: 040 ----
mean loss: 309.20
 ---- batch: 050 ----
mean loss: 314.94
train mean loss: 311.28
epoch train time: 0:00:07.854016
elapsed time: 0:09:48.101085
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 15:10:04.826215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.81
 ---- batch: 020 ----
mean loss: 304.47
 ---- batch: 030 ----
mean loss: 307.19
 ---- batch: 040 ----
mean loss: 304.39
 ---- batch: 050 ----
mean loss: 306.07
train mean loss: 307.98
epoch train time: 0:00:07.814580
elapsed time: 0:09:55.916871
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 15:10:12.641964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.97
 ---- batch: 020 ----
mean loss: 307.10
 ---- batch: 030 ----
mean loss: 303.52
 ---- batch: 040 ----
mean loss: 311.19
 ---- batch: 050 ----
mean loss: 293.80
train mean loss: 305.60
epoch train time: 0:00:07.814003
elapsed time: 0:10:03.731973
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 15:10:20.457042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.74
 ---- batch: 020 ----
mean loss: 305.46
 ---- batch: 030 ----
mean loss: 299.06
 ---- batch: 040 ----
mean loss: 305.26
 ---- batch: 050 ----
mean loss: 299.87
train mean loss: 303.62
epoch train time: 0:00:07.827948
elapsed time: 0:10:11.561082
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 15:10:28.286119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.52
 ---- batch: 020 ----
mean loss: 305.46
 ---- batch: 030 ----
mean loss: 307.67
 ---- batch: 040 ----
mean loss: 296.75
 ---- batch: 050 ----
mean loss: 304.93
train mean loss: 301.25
epoch train time: 0:00:07.808807
elapsed time: 0:10:19.371110
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 15:10:36.096281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.70
 ---- batch: 020 ----
mean loss: 302.44
 ---- batch: 030 ----
mean loss: 306.76
 ---- batch: 040 ----
mean loss: 299.21
 ---- batch: 050 ----
mean loss: 295.53
train mean loss: 299.33
epoch train time: 0:00:07.837942
elapsed time: 0:10:27.210257
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 15:10:43.935370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.12
 ---- batch: 020 ----
mean loss: 296.94
 ---- batch: 030 ----
mean loss: 292.85
 ---- batch: 040 ----
mean loss: 292.77
 ---- batch: 050 ----
mean loss: 296.77
train mean loss: 295.15
epoch train time: 0:00:07.815502
elapsed time: 0:10:35.027027
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 15:10:51.752147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.23
 ---- batch: 020 ----
mean loss: 288.76
 ---- batch: 030 ----
mean loss: 301.95
 ---- batch: 040 ----
mean loss: 309.47
 ---- batch: 050 ----
mean loss: 289.61
train mean loss: 296.35
epoch train time: 0:00:07.861940
elapsed time: 0:10:42.890297
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 15:10:59.615354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.50
 ---- batch: 020 ----
mean loss: 296.24
 ---- batch: 030 ----
mean loss: 294.20
 ---- batch: 040 ----
mean loss: 294.74
 ---- batch: 050 ----
mean loss: 305.21
train mean loss: 297.22
epoch train time: 0:00:07.824382
elapsed time: 0:10:50.715776
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 15:11:07.440823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.85
 ---- batch: 020 ----
mean loss: 288.00
 ---- batch: 030 ----
mean loss: 285.57
 ---- batch: 040 ----
mean loss: 299.44
 ---- batch: 050 ----
mean loss: 301.59
train mean loss: 292.59
epoch train time: 0:00:07.827300
elapsed time: 0:10:58.544180
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 15:11:15.269248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.60
 ---- batch: 020 ----
mean loss: 287.35
 ---- batch: 030 ----
mean loss: 284.39
 ---- batch: 040 ----
mean loss: 285.84
 ---- batch: 050 ----
mean loss: 289.16
train mean loss: 287.68
epoch train time: 0:00:07.803454
elapsed time: 0:11:06.348808
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 15:11:23.073984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.85
 ---- batch: 020 ----
mean loss: 287.12
 ---- batch: 030 ----
mean loss: 280.28
 ---- batch: 040 ----
mean loss: 293.50
 ---- batch: 050 ----
mean loss: 286.05
train mean loss: 288.40
epoch train time: 0:00:07.827447
elapsed time: 0:11:14.177638
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 15:11:30.902706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.56
 ---- batch: 020 ----
mean loss: 281.88
 ---- batch: 030 ----
mean loss: 289.92
 ---- batch: 040 ----
mean loss: 297.11
 ---- batch: 050 ----
mean loss: 293.31
train mean loss: 290.42
epoch train time: 0:00:07.825555
elapsed time: 0:11:22.004530
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 15:11:38.729734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.67
 ---- batch: 020 ----
mean loss: 306.08
 ---- batch: 030 ----
mean loss: 272.47
 ---- batch: 040 ----
mean loss: 289.36
 ---- batch: 050 ----
mean loss: 283.53
train mean loss: 286.79
epoch train time: 0:00:07.832913
elapsed time: 0:11:29.838723
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 15:11:46.563600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.36
 ---- batch: 020 ----
mean loss: 287.49
 ---- batch: 030 ----
mean loss: 293.48
 ---- batch: 040 ----
mean loss: 286.99
 ---- batch: 050 ----
mean loss: 292.12
train mean loss: 290.17
epoch train time: 0:00:07.849764
elapsed time: 0:11:37.689443
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 15:11:54.414513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.98
 ---- batch: 020 ----
mean loss: 278.67
 ---- batch: 030 ----
mean loss: 280.88
 ---- batch: 040 ----
mean loss: 274.48
 ---- batch: 050 ----
mean loss: 283.61
train mean loss: 279.72
epoch train time: 0:00:07.849106
elapsed time: 0:11:45.539702
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 15:12:02.264757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.32
 ---- batch: 020 ----
mean loss: 267.57
 ---- batch: 030 ----
mean loss: 289.38
 ---- batch: 040 ----
mean loss: 281.89
 ---- batch: 050 ----
mean loss: 284.49
train mean loss: 279.61
epoch train time: 0:00:07.836212
elapsed time: 0:11:53.377027
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 15:12:10.102143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.35
 ---- batch: 020 ----
mean loss: 288.02
 ---- batch: 030 ----
mean loss: 280.89
 ---- batch: 040 ----
mean loss: 280.58
 ---- batch: 050 ----
mean loss: 275.64
train mean loss: 279.42
epoch train time: 0:00:07.826242
elapsed time: 0:12:01.204572
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 15:12:17.929689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.14
 ---- batch: 020 ----
mean loss: 283.11
 ---- batch: 030 ----
mean loss: 276.26
 ---- batch: 040 ----
mean loss: 270.13
 ---- batch: 050 ----
mean loss: 285.21
train mean loss: 277.90
epoch train time: 0:00:07.875611
elapsed time: 0:12:09.081472
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 15:12:25.806545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.16
 ---- batch: 020 ----
mean loss: 269.69
 ---- batch: 030 ----
mean loss: 279.06
 ---- batch: 040 ----
mean loss: 276.68
 ---- batch: 050 ----
mean loss: 281.18
train mean loss: 275.53
epoch train time: 0:00:07.827032
elapsed time: 0:12:16.909677
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 15:12:33.634739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.57
 ---- batch: 020 ----
mean loss: 280.90
 ---- batch: 030 ----
mean loss: 275.51
 ---- batch: 040 ----
mean loss: 271.07
 ---- batch: 050 ----
mean loss: 275.78
train mean loss: 276.72
epoch train time: 0:00:07.842083
elapsed time: 0:12:24.752907
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 15:12:41.478047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.44
 ---- batch: 020 ----
mean loss: 273.10
 ---- batch: 030 ----
mean loss: 274.90
 ---- batch: 040 ----
mean loss: 274.47
 ---- batch: 050 ----
mean loss: 279.08
train mean loss: 274.55
epoch train time: 0:00:07.839734
elapsed time: 0:12:32.594011
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 15:12:49.319067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.22
 ---- batch: 020 ----
mean loss: 274.09
 ---- batch: 030 ----
mean loss: 272.60
 ---- batch: 040 ----
mean loss: 267.94
 ---- batch: 050 ----
mean loss: 270.03
train mean loss: 270.86
epoch train time: 0:00:07.842349
elapsed time: 0:12:40.437486
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 15:12:57.162512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.06
 ---- batch: 020 ----
mean loss: 271.00
 ---- batch: 030 ----
mean loss: 272.05
 ---- batch: 040 ----
mean loss: 276.26
 ---- batch: 050 ----
mean loss: 280.87
train mean loss: 272.48
epoch train time: 0:00:07.856262
elapsed time: 0:12:48.295012
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 15:13:05.020193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.19
 ---- batch: 020 ----
mean loss: 263.92
 ---- batch: 030 ----
mean loss: 275.80
 ---- batch: 040 ----
mean loss: 271.79
 ---- batch: 050 ----
mean loss: 273.91
train mean loss: 268.93
epoch train time: 0:00:07.870231
elapsed time: 0:12:56.166575
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 15:13:12.891720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.63
 ---- batch: 020 ----
mean loss: 274.08
 ---- batch: 030 ----
mean loss: 261.28
 ---- batch: 040 ----
mean loss: 265.22
 ---- batch: 050 ----
mean loss: 273.03
train mean loss: 268.42
epoch train time: 0:00:07.863453
elapsed time: 0:13:04.031224
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 15:13:20.756266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.58
 ---- batch: 020 ----
mean loss: 274.39
 ---- batch: 030 ----
mean loss: 260.58
 ---- batch: 040 ----
mean loss: 266.51
 ---- batch: 050 ----
mean loss: 261.09
train mean loss: 266.06
epoch train time: 0:00:07.852318
elapsed time: 0:13:11.884642
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 15:13:28.609746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.42
 ---- batch: 020 ----
mean loss: 267.23
 ---- batch: 030 ----
mean loss: 263.83
 ---- batch: 040 ----
mean loss: 273.80
 ---- batch: 050 ----
mean loss: 257.71
train mean loss: 265.40
epoch train time: 0:00:07.832778
elapsed time: 0:13:19.718569
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 15:13:36.443685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.55
 ---- batch: 020 ----
mean loss: 273.85
 ---- batch: 030 ----
mean loss: 271.37
 ---- batch: 040 ----
mean loss: 257.19
 ---- batch: 050 ----
mean loss: 261.63
train mean loss: 264.94
epoch train time: 0:00:07.821307
elapsed time: 0:13:27.541060
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 15:13:44.266146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.24
 ---- batch: 020 ----
mean loss: 266.82
 ---- batch: 030 ----
mean loss: 265.90
 ---- batch: 040 ----
mean loss: 263.68
 ---- batch: 050 ----
mean loss: 257.83
train mean loss: 264.49
epoch train time: 0:00:07.833154
elapsed time: 0:13:35.375326
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 15:13:52.100378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.65
 ---- batch: 020 ----
mean loss: 263.93
 ---- batch: 030 ----
mean loss: 265.34
 ---- batch: 040 ----
mean loss: 253.80
 ---- batch: 050 ----
mean loss: 263.74
train mean loss: 265.53
epoch train time: 0:00:07.853583
elapsed time: 0:13:43.230008
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 15:13:59.955073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.64
 ---- batch: 020 ----
mean loss: 254.23
 ---- batch: 030 ----
mean loss: 264.72
 ---- batch: 040 ----
mean loss: 264.05
 ---- batch: 050 ----
mean loss: 264.38
train mean loss: 262.39
epoch train time: 0:00:07.847486
elapsed time: 0:13:51.078729
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 15:14:07.803761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.34
 ---- batch: 020 ----
mean loss: 261.69
 ---- batch: 030 ----
mean loss: 268.30
 ---- batch: 040 ----
mean loss: 259.96
 ---- batch: 050 ----
mean loss: 266.96
train mean loss: 262.83
epoch train time: 0:00:07.832593
elapsed time: 0:13:58.912334
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 15:14:15.637359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.62
 ---- batch: 020 ----
mean loss: 255.01
 ---- batch: 030 ----
mean loss: 255.89
 ---- batch: 040 ----
mean loss: 261.54
 ---- batch: 050 ----
mean loss: 276.25
train mean loss: 258.68
epoch train time: 0:00:07.825326
elapsed time: 0:14:06.738818
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 15:14:23.463882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.07
 ---- batch: 020 ----
mean loss: 258.63
 ---- batch: 030 ----
mean loss: 255.58
 ---- batch: 040 ----
mean loss: 259.46
 ---- batch: 050 ----
mean loss: 259.58
train mean loss: 259.60
epoch train time: 0:00:07.925859
elapsed time: 0:14:14.666057
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 15:14:31.390886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.03
 ---- batch: 020 ----
mean loss: 251.99
 ---- batch: 030 ----
mean loss: 263.99
 ---- batch: 040 ----
mean loss: 255.03
 ---- batch: 050 ----
mean loss: 262.45
train mean loss: 256.60
epoch train time: 0:00:07.859558
elapsed time: 0:14:22.526485
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 15:14:39.251573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.29
 ---- batch: 020 ----
mean loss: 258.17
 ---- batch: 030 ----
mean loss: 247.42
 ---- batch: 040 ----
mean loss: 259.29
 ---- batch: 050 ----
mean loss: 252.52
train mean loss: 254.90
epoch train time: 0:00:07.859901
elapsed time: 0:14:30.387561
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 15:14:47.112602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.12
 ---- batch: 020 ----
mean loss: 251.11
 ---- batch: 030 ----
mean loss: 243.61
 ---- batch: 040 ----
mean loss: 263.38
 ---- batch: 050 ----
mean loss: 261.31
train mean loss: 254.92
epoch train time: 0:00:07.855255
elapsed time: 0:14:38.244002
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 15:14:54.969076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.94
 ---- batch: 020 ----
mean loss: 259.28
 ---- batch: 030 ----
mean loss: 257.61
 ---- batch: 040 ----
mean loss: 269.77
 ---- batch: 050 ----
mean loss: 257.90
train mean loss: 258.03
epoch train time: 0:00:07.856505
elapsed time: 0:14:46.101704
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 15:15:02.826770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.21
 ---- batch: 020 ----
mean loss: 250.13
 ---- batch: 030 ----
mean loss: 262.27
 ---- batch: 040 ----
mean loss: 258.84
 ---- batch: 050 ----
mean loss: 253.71
train mean loss: 255.75
epoch train time: 0:00:07.810925
elapsed time: 0:14:53.913760
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 15:15:10.638866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.60
 ---- batch: 020 ----
mean loss: 239.72
 ---- batch: 030 ----
mean loss: 255.30
 ---- batch: 040 ----
mean loss: 248.03
 ---- batch: 050 ----
mean loss: 254.29
train mean loss: 251.87
epoch train time: 0:00:07.860653
elapsed time: 0:15:01.775558
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 15:15:18.500611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.94
 ---- batch: 020 ----
mean loss: 252.49
 ---- batch: 030 ----
mean loss: 259.58
 ---- batch: 040 ----
mean loss: 253.39
 ---- batch: 050 ----
mean loss: 246.95
train mean loss: 251.49
epoch train time: 0:00:07.867768
elapsed time: 0:15:09.644631
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 15:15:26.369699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.13
 ---- batch: 020 ----
mean loss: 261.06
 ---- batch: 030 ----
mean loss: 260.00
 ---- batch: 040 ----
mean loss: 252.97
 ---- batch: 050 ----
mean loss: 246.75
train mean loss: 253.25
epoch train time: 0:00:07.873183
elapsed time: 0:15:17.519056
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 15:15:34.244120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.15
 ---- batch: 020 ----
mean loss: 249.63
 ---- batch: 030 ----
mean loss: 257.34
 ---- batch: 040 ----
mean loss: 262.60
 ---- batch: 050 ----
mean loss: 243.72
train mean loss: 250.37
epoch train time: 0:00:07.845363
elapsed time: 0:15:25.365575
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 15:15:42.090715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.16
 ---- batch: 020 ----
mean loss: 245.45
 ---- batch: 030 ----
mean loss: 250.05
 ---- batch: 040 ----
mean loss: 253.85
 ---- batch: 050 ----
mean loss: 247.85
train mean loss: 247.95
epoch train time: 0:00:07.794215
elapsed time: 0:15:33.161059
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 15:15:49.886144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.36
 ---- batch: 020 ----
mean loss: 248.25
 ---- batch: 030 ----
mean loss: 244.38
 ---- batch: 040 ----
mean loss: 242.01
 ---- batch: 050 ----
mean loss: 247.50
train mean loss: 247.82
epoch train time: 0:00:07.828787
elapsed time: 0:15:40.991001
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 15:15:57.716083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.39
 ---- batch: 020 ----
mean loss: 239.89
 ---- batch: 030 ----
mean loss: 246.65
 ---- batch: 040 ----
mean loss: 249.47
 ---- batch: 050 ----
mean loss: 251.89
train mean loss: 248.50
epoch train time: 0:00:07.846240
elapsed time: 0:15:48.838354
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 15:16:05.563397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.67
 ---- batch: 020 ----
mean loss: 248.30
 ---- batch: 030 ----
mean loss: 242.57
 ---- batch: 040 ----
mean loss: 253.36
 ---- batch: 050 ----
mean loss: 245.95
train mean loss: 248.20
epoch train time: 0:00:07.841847
elapsed time: 0:15:56.681365
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 15:16:13.406463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.75
 ---- batch: 020 ----
mean loss: 248.23
 ---- batch: 030 ----
mean loss: 249.21
 ---- batch: 040 ----
mean loss: 241.55
 ---- batch: 050 ----
mean loss: 257.08
train mean loss: 248.09
epoch train time: 0:00:07.836651
elapsed time: 0:16:04.519244
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 15:16:21.244320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.63
 ---- batch: 020 ----
mean loss: 251.92
 ---- batch: 030 ----
mean loss: 249.25
 ---- batch: 040 ----
mean loss: 241.15
 ---- batch: 050 ----
mean loss: 234.02
train mean loss: 245.68
epoch train time: 0:00:07.819900
elapsed time: 0:16:12.340403
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 15:16:29.065471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.53
 ---- batch: 020 ----
mean loss: 244.06
 ---- batch: 030 ----
mean loss: 245.47
 ---- batch: 040 ----
mean loss: 249.16
 ---- batch: 050 ----
mean loss: 239.24
train mean loss: 244.15
epoch train time: 0:00:07.799452
elapsed time: 0:16:20.140997
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 15:16:36.866031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.65
 ---- batch: 020 ----
mean loss: 245.24
 ---- batch: 030 ----
mean loss: 237.12
 ---- batch: 040 ----
mean loss: 243.13
 ---- batch: 050 ----
mean loss: 241.18
train mean loss: 241.66
epoch train time: 0:00:07.795017
elapsed time: 0:16:27.937222
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 15:16:44.662304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.40
 ---- batch: 020 ----
mean loss: 245.66
 ---- batch: 030 ----
mean loss: 247.98
 ---- batch: 040 ----
mean loss: 245.45
 ---- batch: 050 ----
mean loss: 244.64
train mean loss: 243.95
epoch train time: 0:00:07.807338
elapsed time: 0:16:35.745740
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 15:16:52.470800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.10
 ---- batch: 020 ----
mean loss: 240.96
 ---- batch: 030 ----
mean loss: 235.86
 ---- batch: 040 ----
mean loss: 245.62
 ---- batch: 050 ----
mean loss: 237.22
train mean loss: 241.21
epoch train time: 0:00:07.806248
elapsed time: 0:16:43.553128
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 15:17:00.278204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.05
 ---- batch: 020 ----
mean loss: 238.83
 ---- batch: 030 ----
mean loss: 245.33
 ---- batch: 040 ----
mean loss: 241.35
 ---- batch: 050 ----
mean loss: 242.83
train mean loss: 240.52
epoch train time: 0:00:07.804185
elapsed time: 0:16:51.358648
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 15:17:08.083461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.95
 ---- batch: 020 ----
mean loss: 236.77
 ---- batch: 030 ----
mean loss: 243.67
 ---- batch: 040 ----
mean loss: 229.54
 ---- batch: 050 ----
mean loss: 241.80
train mean loss: 239.18
epoch train time: 0:00:07.815500
elapsed time: 0:16:59.175111
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 15:17:15.900179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.82
 ---- batch: 020 ----
mean loss: 231.34
 ---- batch: 030 ----
mean loss: 237.03
 ---- batch: 040 ----
mean loss: 240.09
 ---- batch: 050 ----
mean loss: 236.23
train mean loss: 237.22
epoch train time: 0:00:07.822713
elapsed time: 0:17:06.999005
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 15:17:23.724090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.80
 ---- batch: 020 ----
mean loss: 239.57
 ---- batch: 030 ----
mean loss: 237.33
 ---- batch: 040 ----
mean loss: 237.60
 ---- batch: 050 ----
mean loss: 234.21
train mean loss: 238.16
epoch train time: 0:00:07.877302
elapsed time: 0:17:14.877526
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 15:17:31.602574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.19
 ---- batch: 020 ----
mean loss: 241.84
 ---- batch: 030 ----
mean loss: 240.25
 ---- batch: 040 ----
mean loss: 233.98
 ---- batch: 050 ----
mean loss: 238.05
train mean loss: 236.60
epoch train time: 0:00:07.829248
elapsed time: 0:17:22.707953
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 15:17:39.432994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.93
 ---- batch: 020 ----
mean loss: 232.65
 ---- batch: 030 ----
mean loss: 236.49
 ---- batch: 040 ----
mean loss: 237.31
 ---- batch: 050 ----
mean loss: 235.16
train mean loss: 236.44
epoch train time: 0:00:07.803355
elapsed time: 0:17:30.512413
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 15:17:47.237483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.71
 ---- batch: 020 ----
mean loss: 228.95
 ---- batch: 030 ----
mean loss: 236.82
 ---- batch: 040 ----
mean loss: 242.95
 ---- batch: 050 ----
mean loss: 238.24
train mean loss: 236.57
epoch train time: 0:00:07.828245
elapsed time: 0:17:38.341878
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 15:17:55.066936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.26
 ---- batch: 020 ----
mean loss: 235.78
 ---- batch: 030 ----
mean loss: 242.36
 ---- batch: 040 ----
mean loss: 243.43
 ---- batch: 050 ----
mean loss: 233.07
train mean loss: 236.34
epoch train time: 0:00:07.848936
elapsed time: 0:17:46.191997
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 15:18:02.917040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.46
 ---- batch: 020 ----
mean loss: 241.25
 ---- batch: 030 ----
mean loss: 229.47
 ---- batch: 040 ----
mean loss: 227.70
 ---- batch: 050 ----
mean loss: 226.70
train mean loss: 233.96
epoch train time: 0:00:07.848381
elapsed time: 0:17:54.041540
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 15:18:10.766653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.56
 ---- batch: 020 ----
mean loss: 231.60
 ---- batch: 030 ----
mean loss: 233.95
 ---- batch: 040 ----
mean loss: 235.95
 ---- batch: 050 ----
mean loss: 232.32
train mean loss: 234.07
epoch train time: 0:00:07.837202
elapsed time: 0:18:01.879983
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 15:18:18.605073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.23
 ---- batch: 020 ----
mean loss: 234.74
 ---- batch: 030 ----
mean loss: 239.95
 ---- batch: 040 ----
mean loss: 238.98
 ---- batch: 050 ----
mean loss: 239.35
train mean loss: 235.47
epoch train time: 0:00:07.819573
elapsed time: 0:18:09.700722
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 15:18:26.425837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.81
 ---- batch: 020 ----
mean loss: 241.39
 ---- batch: 030 ----
mean loss: 229.45
 ---- batch: 040 ----
mean loss: 241.31
 ---- batch: 050 ----
mean loss: 232.58
train mean loss: 232.98
epoch train time: 0:00:07.820420
elapsed time: 0:18:17.522417
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 15:18:34.247529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.94
 ---- batch: 020 ----
mean loss: 240.41
 ---- batch: 030 ----
mean loss: 223.76
 ---- batch: 040 ----
mean loss: 229.91
 ---- batch: 050 ----
mean loss: 235.15
train mean loss: 232.23
epoch train time: 0:00:07.828904
elapsed time: 0:18:25.352544
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 15:18:42.077588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.13
 ---- batch: 020 ----
mean loss: 230.77
 ---- batch: 030 ----
mean loss: 231.13
 ---- batch: 040 ----
mean loss: 236.12
 ---- batch: 050 ----
mean loss: 225.72
train mean loss: 231.50
epoch train time: 0:00:07.828937
elapsed time: 0:18:33.182697
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 15:18:49.907762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.44
 ---- batch: 020 ----
mean loss: 235.33
 ---- batch: 030 ----
mean loss: 236.68
 ---- batch: 040 ----
mean loss: 224.19
 ---- batch: 050 ----
mean loss: 237.80
train mean loss: 232.43
epoch train time: 0:00:07.847979
elapsed time: 0:18:41.031883
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 15:18:57.756958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.55
 ---- batch: 020 ----
mean loss: 242.58
 ---- batch: 030 ----
mean loss: 228.57
 ---- batch: 040 ----
mean loss: 232.29
 ---- batch: 050 ----
mean loss: 222.39
train mean loss: 231.36
epoch train time: 0:00:07.828011
elapsed time: 0:18:48.860999
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 15:19:05.586052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.24
 ---- batch: 020 ----
mean loss: 225.35
 ---- batch: 030 ----
mean loss: 229.72
 ---- batch: 040 ----
mean loss: 225.49
 ---- batch: 050 ----
mean loss: 230.40
train mean loss: 229.19
epoch train time: 0:00:07.809706
elapsed time: 0:18:56.671885
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 15:19:13.396948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.82
 ---- batch: 020 ----
mean loss: 227.68
 ---- batch: 030 ----
mean loss: 227.90
 ---- batch: 040 ----
mean loss: 223.15
 ---- batch: 050 ----
mean loss: 225.95
train mean loss: 227.24
epoch train time: 0:00:07.839395
elapsed time: 0:19:04.512499
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 15:19:21.237574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.11
 ---- batch: 020 ----
mean loss: 228.73
 ---- batch: 030 ----
mean loss: 223.96
 ---- batch: 040 ----
mean loss: 217.53
 ---- batch: 050 ----
mean loss: 238.24
train mean loss: 226.97
epoch train time: 0:00:07.836799
elapsed time: 0:19:12.350499
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 15:19:29.075570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.25
 ---- batch: 020 ----
mean loss: 232.86
 ---- batch: 030 ----
mean loss: 227.64
 ---- batch: 040 ----
mean loss: 236.33
 ---- batch: 050 ----
mean loss: 219.56
train mean loss: 229.45
epoch train time: 0:00:07.870850
elapsed time: 0:19:20.222495
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 15:19:36.947586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.46
 ---- batch: 020 ----
mean loss: 222.68
 ---- batch: 030 ----
mean loss: 221.48
 ---- batch: 040 ----
mean loss: 231.97
 ---- batch: 050 ----
mean loss: 230.53
train mean loss: 226.74
epoch train time: 0:00:07.888633
elapsed time: 0:19:28.112358
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 15:19:44.837413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.32
 ---- batch: 020 ----
mean loss: 214.93
 ---- batch: 030 ----
mean loss: 227.81
 ---- batch: 040 ----
mean loss: 235.26
 ---- batch: 050 ----
mean loss: 222.66
train mean loss: 225.19
epoch train time: 0:00:07.826074
elapsed time: 0:19:35.939621
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 15:19:52.664661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.80
 ---- batch: 020 ----
mean loss: 216.27
 ---- batch: 030 ----
mean loss: 227.41
 ---- batch: 040 ----
mean loss: 228.95
 ---- batch: 050 ----
mean loss: 220.02
train mean loss: 224.54
epoch train time: 0:00:07.858350
elapsed time: 0:19:43.799410
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 15:20:00.524238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.18
 ---- batch: 020 ----
mean loss: 223.26
 ---- batch: 030 ----
mean loss: 227.77
 ---- batch: 040 ----
mean loss: 217.57
 ---- batch: 050 ----
mean loss: 235.22
train mean loss: 225.38
epoch train time: 0:00:07.823099
elapsed time: 0:19:51.623364
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 15:20:08.348421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.51
 ---- batch: 020 ----
mean loss: 228.02
 ---- batch: 030 ----
mean loss: 230.35
 ---- batch: 040 ----
mean loss: 220.88
 ---- batch: 050 ----
mean loss: 224.64
train mean loss: 225.86
epoch train time: 0:00:07.823793
elapsed time: 0:19:59.448270
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 15:20:16.173293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.54
 ---- batch: 020 ----
mean loss: 224.64
 ---- batch: 030 ----
mean loss: 225.28
 ---- batch: 040 ----
mean loss: 217.53
 ---- batch: 050 ----
mean loss: 220.48
train mean loss: 224.52
epoch train time: 0:00:07.828175
elapsed time: 0:20:07.277667
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 15:20:24.002706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.02
 ---- batch: 020 ----
mean loss: 225.95
 ---- batch: 030 ----
mean loss: 218.43
 ---- batch: 040 ----
mean loss: 224.45
 ---- batch: 050 ----
mean loss: 229.14
train mean loss: 222.84
epoch train time: 0:00:07.828098
elapsed time: 0:20:15.106924
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 15:20:31.832020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.69
 ---- batch: 020 ----
mean loss: 226.42
 ---- batch: 030 ----
mean loss: 228.99
 ---- batch: 040 ----
mean loss: 212.46
 ---- batch: 050 ----
mean loss: 218.33
train mean loss: 222.15
epoch train time: 0:00:07.824414
elapsed time: 0:20:22.932787
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 15:20:39.657918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.43
 ---- batch: 020 ----
mean loss: 230.29
 ---- batch: 030 ----
mean loss: 215.82
 ---- batch: 040 ----
mean loss: 226.07
 ---- batch: 050 ----
mean loss: 222.61
train mean loss: 223.35
epoch train time: 0:00:07.824505
elapsed time: 0:20:30.758490
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 15:20:47.483547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.46
 ---- batch: 020 ----
mean loss: 211.93
 ---- batch: 030 ----
mean loss: 230.28
 ---- batch: 040 ----
mean loss: 216.88
 ---- batch: 050 ----
mean loss: 224.15
train mean loss: 220.22
epoch train time: 0:00:07.815567
elapsed time: 0:20:38.575120
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 15:20:55.300156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.37
 ---- batch: 020 ----
mean loss: 220.69
 ---- batch: 030 ----
mean loss: 214.60
 ---- batch: 040 ----
mean loss: 224.34
 ---- batch: 050 ----
mean loss: 224.56
train mean loss: 222.75
epoch train time: 0:00:07.843690
elapsed time: 0:20:46.419884
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 15:21:03.144990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.15
 ---- batch: 020 ----
mean loss: 223.78
 ---- batch: 030 ----
mean loss: 218.09
 ---- batch: 040 ----
mean loss: 213.59
 ---- batch: 050 ----
mean loss: 220.69
train mean loss: 218.92
epoch train time: 0:00:07.863643
elapsed time: 0:20:54.284734
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 15:21:11.009811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.79
 ---- batch: 020 ----
mean loss: 216.78
 ---- batch: 030 ----
mean loss: 224.60
 ---- batch: 040 ----
mean loss: 218.49
 ---- batch: 050 ----
mean loss: 213.16
train mean loss: 219.30
epoch train time: 0:00:07.844012
elapsed time: 0:21:02.129861
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 15:21:18.854895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.77
 ---- batch: 020 ----
mean loss: 222.09
 ---- batch: 030 ----
mean loss: 211.37
 ---- batch: 040 ----
mean loss: 217.61
 ---- batch: 050 ----
mean loss: 220.11
train mean loss: 218.72
epoch train time: 0:00:07.819310
elapsed time: 0:21:09.950292
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 15:21:26.675368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.06
 ---- batch: 020 ----
mean loss: 217.27
 ---- batch: 030 ----
mean loss: 217.00
 ---- batch: 040 ----
mean loss: 224.40
 ---- batch: 050 ----
mean loss: 215.00
train mean loss: 221.21
epoch train time: 0:00:07.836241
elapsed time: 0:21:17.787656
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 15:21:34.512712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.34
 ---- batch: 020 ----
mean loss: 226.02
 ---- batch: 030 ----
mean loss: 218.63
 ---- batch: 040 ----
mean loss: 211.73
 ---- batch: 050 ----
mean loss: 221.26
train mean loss: 218.43
epoch train time: 0:00:07.828190
elapsed time: 0:21:25.616947
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 15:21:42.341998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.06
 ---- batch: 020 ----
mean loss: 213.36
 ---- batch: 030 ----
mean loss: 219.66
 ---- batch: 040 ----
mean loss: 214.10
 ---- batch: 050 ----
mean loss: 218.65
train mean loss: 216.05
epoch train time: 0:00:07.817459
elapsed time: 0:21:33.435463
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 15:21:50.160483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.89
 ---- batch: 020 ----
mean loss: 215.37
 ---- batch: 030 ----
mean loss: 212.83
 ---- batch: 040 ----
mean loss: 217.45
 ---- batch: 050 ----
mean loss: 219.78
train mean loss: 217.03
epoch train time: 0:00:07.850836
elapsed time: 0:21:41.287371
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 15:21:58.012420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.51
 ---- batch: 020 ----
mean loss: 217.59
 ---- batch: 030 ----
mean loss: 226.75
 ---- batch: 040 ----
mean loss: 218.16
 ---- batch: 050 ----
mean loss: 209.51
train mean loss: 217.38
epoch train time: 0:00:07.832876
elapsed time: 0:21:49.121422
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 15:22:05.846481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.37
 ---- batch: 020 ----
mean loss: 219.09
 ---- batch: 030 ----
mean loss: 225.07
 ---- batch: 040 ----
mean loss: 203.50
 ---- batch: 050 ----
mean loss: 214.93
train mean loss: 215.41
epoch train time: 0:00:07.799072
elapsed time: 0:21:56.921553
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 15:22:13.646731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.36
 ---- batch: 020 ----
mean loss: 208.10
 ---- batch: 030 ----
mean loss: 219.58
 ---- batch: 040 ----
mean loss: 216.64
 ---- batch: 050 ----
mean loss: 214.42
train mean loss: 215.36
epoch train time: 0:00:07.813943
elapsed time: 0:22:04.736673
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 15:22:21.461747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.20
 ---- batch: 020 ----
mean loss: 218.42
 ---- batch: 030 ----
mean loss: 218.91
 ---- batch: 040 ----
mean loss: 209.85
 ---- batch: 050 ----
mean loss: 206.33
train mean loss: 213.94
epoch train time: 0:00:07.843236
elapsed time: 0:22:12.581095
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 15:22:29.306136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.57
 ---- batch: 020 ----
mean loss: 220.90
 ---- batch: 030 ----
mean loss: 213.19
 ---- batch: 040 ----
mean loss: 211.92
 ---- batch: 050 ----
mean loss: 209.21
train mean loss: 212.65
epoch train time: 0:00:07.859855
elapsed time: 0:22:20.442053
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 15:22:37.167116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.27
 ---- batch: 020 ----
mean loss: 218.12
 ---- batch: 030 ----
mean loss: 210.99
 ---- batch: 040 ----
mean loss: 216.72
 ---- batch: 050 ----
mean loss: 210.69
train mean loss: 213.83
epoch train time: 0:00:07.842657
elapsed time: 0:22:28.285797
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 15:22:45.010884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.17
 ---- batch: 020 ----
mean loss: 216.22
 ---- batch: 030 ----
mean loss: 219.81
 ---- batch: 040 ----
mean loss: 215.12
 ---- batch: 050 ----
mean loss: 205.49
train mean loss: 215.80
epoch train time: 0:00:07.826165
elapsed time: 0:22:36.113184
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 15:22:52.838237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.80
 ---- batch: 020 ----
mean loss: 213.71
 ---- batch: 030 ----
mean loss: 211.27
 ---- batch: 040 ----
mean loss: 226.32
 ---- batch: 050 ----
mean loss: 206.61
train mean loss: 213.25
epoch train time: 0:00:07.862054
elapsed time: 0:22:43.976486
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 15:23:00.701547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.75
 ---- batch: 020 ----
mean loss: 216.28
 ---- batch: 030 ----
mean loss: 210.98
 ---- batch: 040 ----
mean loss: 213.18
 ---- batch: 050 ----
mean loss: 208.09
train mean loss: 212.01
epoch train time: 0:00:07.851352
elapsed time: 0:22:51.828988
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 15:23:08.554051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.21
 ---- batch: 020 ----
mean loss: 211.70
 ---- batch: 030 ----
mean loss: 218.24
 ---- batch: 040 ----
mean loss: 211.67
 ---- batch: 050 ----
mean loss: 212.11
train mean loss: 211.60
epoch train time: 0:00:07.857689
elapsed time: 0:22:59.688203
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 15:23:16.413005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.00
 ---- batch: 020 ----
mean loss: 209.29
 ---- batch: 030 ----
mean loss: 211.00
 ---- batch: 040 ----
mean loss: 210.09
 ---- batch: 050 ----
mean loss: 218.77
train mean loss: 212.08
epoch train time: 0:00:07.849710
elapsed time: 0:23:07.538828
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 15:23:24.263907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.28
 ---- batch: 020 ----
mean loss: 213.54
 ---- batch: 030 ----
mean loss: 210.46
 ---- batch: 040 ----
mean loss: 208.28
 ---- batch: 050 ----
mean loss: 212.88
train mean loss: 213.40
epoch train time: 0:00:07.840435
elapsed time: 0:23:15.380508
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 15:23:32.105537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.65
 ---- batch: 020 ----
mean loss: 207.63
 ---- batch: 030 ----
mean loss: 218.47
 ---- batch: 040 ----
mean loss: 216.79
 ---- batch: 050 ----
mean loss: 212.37
train mean loss: 209.52
epoch train time: 0:00:07.854967
elapsed time: 0:23:23.236538
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 15:23:39.961584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.84
 ---- batch: 020 ----
mean loss: 216.42
 ---- batch: 030 ----
mean loss: 211.04
 ---- batch: 040 ----
mean loss: 206.13
 ---- batch: 050 ----
mean loss: 206.22
train mean loss: 209.69
epoch train time: 0:00:07.845965
elapsed time: 0:23:31.083579
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 15:23:47.808643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.20
 ---- batch: 020 ----
mean loss: 201.33
 ---- batch: 030 ----
mean loss: 209.02
 ---- batch: 040 ----
mean loss: 213.48
 ---- batch: 050 ----
mean loss: 211.37
train mean loss: 207.66
epoch train time: 0:00:07.854929
elapsed time: 0:23:38.939695
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 15:23:55.664778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.97
 ---- batch: 020 ----
mean loss: 208.34
 ---- batch: 030 ----
mean loss: 205.53
 ---- batch: 040 ----
mean loss: 209.13
 ---- batch: 050 ----
mean loss: 205.43
train mean loss: 207.26
epoch train time: 0:00:07.865417
elapsed time: 0:23:46.806385
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 15:24:03.531302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.55
 ---- batch: 020 ----
mean loss: 214.34
 ---- batch: 030 ----
mean loss: 212.15
 ---- batch: 040 ----
mean loss: 208.20
 ---- batch: 050 ----
mean loss: 203.58
train mean loss: 208.42
epoch train time: 0:00:07.821527
elapsed time: 0:23:54.628938
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 15:24:11.353993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.94
 ---- batch: 020 ----
mean loss: 210.49
 ---- batch: 030 ----
mean loss: 204.76
 ---- batch: 040 ----
mean loss: 210.23
 ---- batch: 050 ----
mean loss: 219.23
train mean loss: 209.62
epoch train time: 0:00:07.818684
elapsed time: 0:24:02.448685
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 15:24:19.173798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.73
 ---- batch: 020 ----
mean loss: 209.70
 ---- batch: 030 ----
mean loss: 207.36
 ---- batch: 040 ----
mean loss: 215.08
 ---- batch: 050 ----
mean loss: 216.65
train mean loss: 210.08
epoch train time: 0:00:07.827917
elapsed time: 0:24:10.277731
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 15:24:27.002780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.65
 ---- batch: 020 ----
mean loss: 206.07
 ---- batch: 030 ----
mean loss: 211.99
 ---- batch: 040 ----
mean loss: 205.92
 ---- batch: 050 ----
mean loss: 209.16
train mean loss: 208.00
epoch train time: 0:00:07.805941
elapsed time: 0:24:18.084811
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 15:24:34.809929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.36
 ---- batch: 020 ----
mean loss: 206.75
 ---- batch: 030 ----
mean loss: 202.93
 ---- batch: 040 ----
mean loss: 214.82
 ---- batch: 050 ----
mean loss: 198.95
train mean loss: 205.53
epoch train time: 0:00:07.823720
elapsed time: 0:24:25.909687
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 15:24:42.634729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.22
 ---- batch: 020 ----
mean loss: 213.61
 ---- batch: 030 ----
mean loss: 204.09
 ---- batch: 040 ----
mean loss: 200.81
 ---- batch: 050 ----
mean loss: 211.01
train mean loss: 206.72
epoch train time: 0:00:07.881014
elapsed time: 0:24:33.791795
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 15:24:50.516873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.34
 ---- batch: 020 ----
mean loss: 196.48
 ---- batch: 030 ----
mean loss: 196.87
 ---- batch: 040 ----
mean loss: 212.49
 ---- batch: 050 ----
mean loss: 211.82
train mean loss: 204.76
epoch train time: 0:00:07.838398
elapsed time: 0:24:41.631360
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 15:24:58.356535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.45
 ---- batch: 020 ----
mean loss: 203.27
 ---- batch: 030 ----
mean loss: 200.67
 ---- batch: 040 ----
mean loss: 206.41
 ---- batch: 050 ----
mean loss: 208.95
train mean loss: 206.95
epoch train time: 0:00:07.839497
elapsed time: 0:24:49.472116
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 15:25:06.197172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.42
 ---- batch: 020 ----
mean loss: 201.75
 ---- batch: 030 ----
mean loss: 202.15
 ---- batch: 040 ----
mean loss: 201.03
 ---- batch: 050 ----
mean loss: 208.48
train mean loss: 204.48
epoch train time: 0:00:07.854981
elapsed time: 0:24:57.328202
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 15:25:14.053273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.60
 ---- batch: 020 ----
mean loss: 203.86
 ---- batch: 030 ----
mean loss: 212.56
 ---- batch: 040 ----
mean loss: 202.34
 ---- batch: 050 ----
mean loss: 199.19
train mean loss: 204.69
epoch train time: 0:00:07.832690
elapsed time: 0:25:05.162078
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 15:25:21.887148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.86
 ---- batch: 020 ----
mean loss: 206.98
 ---- batch: 030 ----
mean loss: 195.79
 ---- batch: 040 ----
mean loss: 204.80
 ---- batch: 050 ----
mean loss: 198.74
train mean loss: 204.53
epoch train time: 0:00:07.825094
elapsed time: 0:25:12.988261
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 15:25:29.713285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.61
 ---- batch: 020 ----
mean loss: 207.97
 ---- batch: 030 ----
mean loss: 200.69
 ---- batch: 040 ----
mean loss: 207.95
 ---- batch: 050 ----
mean loss: 208.98
train mean loss: 204.28
epoch train time: 0:00:07.820360
elapsed time: 0:25:20.809764
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 15:25:37.534915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.62
 ---- batch: 020 ----
mean loss: 200.37
 ---- batch: 030 ----
mean loss: 206.10
 ---- batch: 040 ----
mean loss: 206.53
 ---- batch: 050 ----
mean loss: 194.14
train mean loss: 202.43
epoch train time: 0:00:07.851872
elapsed time: 0:25:28.662930
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 15:25:45.387970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.71
 ---- batch: 020 ----
mean loss: 201.22
 ---- batch: 030 ----
mean loss: 201.25
 ---- batch: 040 ----
mean loss: 218.41
 ---- batch: 050 ----
mean loss: 208.73
train mean loss: 204.33
epoch train time: 0:00:07.834695
elapsed time: 0:25:36.499151
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 15:25:53.224373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.09
 ---- batch: 020 ----
mean loss: 202.84
 ---- batch: 030 ----
mean loss: 201.32
 ---- batch: 040 ----
mean loss: 203.73
 ---- batch: 050 ----
mean loss: 204.88
train mean loss: 203.90
epoch train time: 0:00:07.835269
elapsed time: 0:25:44.335769
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 15:26:01.060843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.36
 ---- batch: 020 ----
mean loss: 205.33
 ---- batch: 030 ----
mean loss: 200.96
 ---- batch: 040 ----
mean loss: 201.57
 ---- batch: 050 ----
mean loss: 204.73
train mean loss: 203.11
epoch train time: 0:00:07.841424
elapsed time: 0:25:52.178324
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 15:26:08.903464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.63
 ---- batch: 020 ----
mean loss: 202.62
 ---- batch: 030 ----
mean loss: 200.40
 ---- batch: 040 ----
mean loss: 201.60
 ---- batch: 050 ----
mean loss: 201.42
train mean loss: 202.07
epoch train time: 0:00:07.836712
elapsed time: 0:26:00.016554
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 15:26:16.741675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.28
 ---- batch: 020 ----
mean loss: 196.09
 ---- batch: 030 ----
mean loss: 209.94
 ---- batch: 040 ----
mean loss: 201.76
 ---- batch: 050 ----
mean loss: 202.76
train mean loss: 202.34
epoch train time: 0:00:07.839218
elapsed time: 0:26:07.856973
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 15:26:24.581971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.83
 ---- batch: 020 ----
mean loss: 202.77
 ---- batch: 030 ----
mean loss: 210.98
 ---- batch: 040 ----
mean loss: 199.23
 ---- batch: 050 ----
mean loss: 201.36
train mean loss: 202.39
epoch train time: 0:00:07.814597
elapsed time: 0:26:15.672627
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 15:26:32.397725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.98
 ---- batch: 020 ----
mean loss: 203.28
 ---- batch: 030 ----
mean loss: 206.46
 ---- batch: 040 ----
mean loss: 207.65
 ---- batch: 050 ----
mean loss: 196.05
train mean loss: 201.95
epoch train time: 0:00:07.807721
elapsed time: 0:26:23.481647
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 15:26:40.206709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.83
 ---- batch: 020 ----
mean loss: 203.89
 ---- batch: 030 ----
mean loss: 196.85
 ---- batch: 040 ----
mean loss: 202.46
 ---- batch: 050 ----
mean loss: 202.51
train mean loss: 201.17
epoch train time: 0:00:07.826553
elapsed time: 0:26:31.309371
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 15:26:48.034464
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.09
 ---- batch: 020 ----
mean loss: 196.19
 ---- batch: 030 ----
mean loss: 196.38
 ---- batch: 040 ----
mean loss: 197.26
 ---- batch: 050 ----
mean loss: 202.79
train mean loss: 198.84
epoch train time: 0:00:07.847059
elapsed time: 0:26:39.158015
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 15:26:55.882767
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.51
 ---- batch: 020 ----
mean loss: 198.46
 ---- batch: 030 ----
mean loss: 195.67
 ---- batch: 040 ----
mean loss: 190.66
 ---- batch: 050 ----
mean loss: 201.52
train mean loss: 199.12
epoch train time: 0:00:07.842819
elapsed time: 0:26:47.001720
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 15:27:03.726779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.34
 ---- batch: 020 ----
mean loss: 197.65
 ---- batch: 030 ----
mean loss: 189.67
 ---- batch: 040 ----
mean loss: 197.64
 ---- batch: 050 ----
mean loss: 202.77
train mean loss: 197.93
epoch train time: 0:00:07.839282
elapsed time: 0:26:54.842156
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 15:27:11.567185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.72
 ---- batch: 020 ----
mean loss: 197.30
 ---- batch: 030 ----
mean loss: 195.06
 ---- batch: 040 ----
mean loss: 198.42
 ---- batch: 050 ----
mean loss: 197.51
train mean loss: 197.99
epoch train time: 0:00:07.848905
elapsed time: 0:27:02.692438
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 15:27:19.417639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.11
 ---- batch: 020 ----
mean loss: 203.00
 ---- batch: 030 ----
mean loss: 198.07
 ---- batch: 040 ----
mean loss: 198.43
 ---- batch: 050 ----
mean loss: 204.98
train mean loss: 198.54
epoch train time: 0:00:07.808552
elapsed time: 0:27:10.502297
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 15:27:27.227376
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.72
 ---- batch: 020 ----
mean loss: 201.86
 ---- batch: 030 ----
mean loss: 202.75
 ---- batch: 040 ----
mean loss: 203.95
 ---- batch: 050 ----
mean loss: 199.11
train mean loss: 197.84
epoch train time: 0:00:07.843235
elapsed time: 0:27:18.346673
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 15:27:35.071794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.69
 ---- batch: 020 ----
mean loss: 196.81
 ---- batch: 030 ----
mean loss: 196.79
 ---- batch: 040 ----
mean loss: 205.91
 ---- batch: 050 ----
mean loss: 194.27
train mean loss: 198.42
epoch train time: 0:00:07.842998
elapsed time: 0:27:26.190961
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 15:27:42.916009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.92
 ---- batch: 020 ----
mean loss: 203.66
 ---- batch: 030 ----
mean loss: 200.88
 ---- batch: 040 ----
mean loss: 187.05
 ---- batch: 050 ----
mean loss: 199.96
train mean loss: 197.55
epoch train time: 0:00:07.840949
elapsed time: 0:27:34.033124
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 15:27:50.758177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.80
 ---- batch: 020 ----
mean loss: 200.70
 ---- batch: 030 ----
mean loss: 190.60
 ---- batch: 040 ----
mean loss: 203.97
 ---- batch: 050 ----
mean loss: 196.93
train mean loss: 197.59
epoch train time: 0:00:07.858419
elapsed time: 0:27:41.892639
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 15:27:58.617722
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.96
 ---- batch: 020 ----
mean loss: 202.49
 ---- batch: 030 ----
mean loss: 197.77
 ---- batch: 040 ----
mean loss: 198.22
 ---- batch: 050 ----
mean loss: 197.07
train mean loss: 197.33
epoch train time: 0:00:07.852257
elapsed time: 0:27:49.746094
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 15:28:06.471190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.64
 ---- batch: 020 ----
mean loss: 205.91
 ---- batch: 030 ----
mean loss: 198.88
 ---- batch: 040 ----
mean loss: 187.46
 ---- batch: 050 ----
mean loss: 201.92
train mean loss: 198.20
epoch train time: 0:00:07.850812
elapsed time: 0:27:57.598123
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 15:28:14.323172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.33
 ---- batch: 020 ----
mean loss: 197.59
 ---- batch: 030 ----
mean loss: 203.77
 ---- batch: 040 ----
mean loss: 193.04
 ---- batch: 050 ----
mean loss: 198.23
train mean loss: 198.33
epoch train time: 0:00:07.829520
elapsed time: 0:28:05.428795
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 15:28:22.153882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.24
 ---- batch: 020 ----
mean loss: 195.35
 ---- batch: 030 ----
mean loss: 201.45
 ---- batch: 040 ----
mean loss: 189.57
 ---- batch: 050 ----
mean loss: 201.45
train mean loss: 198.50
epoch train time: 0:00:07.822177
elapsed time: 0:28:13.252043
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 15:28:29.977047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.61
 ---- batch: 020 ----
mean loss: 198.62
 ---- batch: 030 ----
mean loss: 193.47
 ---- batch: 040 ----
mean loss: 192.24
 ---- batch: 050 ----
mean loss: 195.96
train mean loss: 197.40
epoch train time: 0:00:07.838535
elapsed time: 0:28:21.091790
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 15:28:37.816689
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.89
 ---- batch: 020 ----
mean loss: 194.06
 ---- batch: 030 ----
mean loss: 188.72
 ---- batch: 040 ----
mean loss: 211.77
 ---- batch: 050 ----
mean loss: 200.88
train mean loss: 197.85
epoch train time: 0:00:07.853352
elapsed time: 0:28:28.946152
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 15:28:45.671249
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.35
 ---- batch: 020 ----
mean loss: 201.91
 ---- batch: 030 ----
mean loss: 201.56
 ---- batch: 040 ----
mean loss: 199.49
 ---- batch: 050 ----
mean loss: 194.88
train mean loss: 198.34
epoch train time: 0:00:07.839523
elapsed time: 0:28:36.786857
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 15:28:53.511900
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.78
 ---- batch: 020 ----
mean loss: 196.59
 ---- batch: 030 ----
mean loss: 201.58
 ---- batch: 040 ----
mean loss: 201.06
 ---- batch: 050 ----
mean loss: 190.49
train mean loss: 198.52
epoch train time: 0:00:07.846699
elapsed time: 0:28:44.634674
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 15:29:01.359793
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.34
 ---- batch: 020 ----
mean loss: 190.21
 ---- batch: 030 ----
mean loss: 197.29
 ---- batch: 040 ----
mean loss: 190.23
 ---- batch: 050 ----
mean loss: 198.95
train mean loss: 197.75
epoch train time: 0:00:07.846978
elapsed time: 0:28:52.482870
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 15:29:09.207937
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.85
 ---- batch: 020 ----
mean loss: 192.14
 ---- batch: 030 ----
mean loss: 194.96
 ---- batch: 040 ----
mean loss: 196.31
 ---- batch: 050 ----
mean loss: 197.49
train mean loss: 197.80
epoch train time: 0:00:07.882727
elapsed time: 0:29:00.366708
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 15:29:17.091830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.83
 ---- batch: 020 ----
mean loss: 203.70
 ---- batch: 030 ----
mean loss: 190.04
 ---- batch: 040 ----
mean loss: 206.19
 ---- batch: 050 ----
mean loss: 194.00
train mean loss: 197.30
epoch train time: 0:00:07.854420
elapsed time: 0:29:08.222512
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 15:29:24.947562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.91
 ---- batch: 020 ----
mean loss: 202.64
 ---- batch: 030 ----
mean loss: 200.25
 ---- batch: 040 ----
mean loss: 198.65
 ---- batch: 050 ----
mean loss: 208.74
train mean loss: 197.85
epoch train time: 0:00:07.862826
elapsed time: 0:29:16.086599
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 15:29:32.811696
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.59
 ---- batch: 020 ----
mean loss: 202.38
 ---- batch: 030 ----
mean loss: 193.53
 ---- batch: 040 ----
mean loss: 196.91
 ---- batch: 050 ----
mean loss: 203.55
train mean loss: 197.50
epoch train time: 0:00:07.846490
elapsed time: 0:29:23.934273
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 15:29:40.659319
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.89
 ---- batch: 020 ----
mean loss: 192.70
 ---- batch: 030 ----
mean loss: 197.61
 ---- batch: 040 ----
mean loss: 200.60
 ---- batch: 050 ----
mean loss: 197.75
train mean loss: 197.34
epoch train time: 0:00:07.840352
elapsed time: 0:29:31.775707
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 15:29:48.500745
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.59
 ---- batch: 020 ----
mean loss: 206.36
 ---- batch: 030 ----
mean loss: 202.79
 ---- batch: 040 ----
mean loss: 193.21
 ---- batch: 050 ----
mean loss: 195.88
train mean loss: 197.25
epoch train time: 0:00:07.887868
elapsed time: 0:29:39.664716
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 15:29:56.389831
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.40
 ---- batch: 020 ----
mean loss: 199.56
 ---- batch: 030 ----
mean loss: 197.22
 ---- batch: 040 ----
mean loss: 198.20
 ---- batch: 050 ----
mean loss: 189.96
train mean loss: 196.73
epoch train time: 0:00:07.897333
elapsed time: 0:29:47.563254
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 15:30:04.288316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.07
 ---- batch: 020 ----
mean loss: 197.35
 ---- batch: 030 ----
mean loss: 198.72
 ---- batch: 040 ----
mean loss: 194.05
 ---- batch: 050 ----
mean loss: 197.37
train mean loss: 196.72
epoch train time: 0:00:07.845593
elapsed time: 0:29:55.409940
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 15:30:12.135011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.26
 ---- batch: 020 ----
mean loss: 190.49
 ---- batch: 030 ----
mean loss: 201.34
 ---- batch: 040 ----
mean loss: 201.53
 ---- batch: 050 ----
mean loss: 191.96
train mean loss: 197.58
epoch train time: 0:00:07.868823
elapsed time: 0:30:03.280036
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 15:30:20.005179
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.73
 ---- batch: 020 ----
mean loss: 198.49
 ---- batch: 030 ----
mean loss: 196.47
 ---- batch: 040 ----
mean loss: 197.21
 ---- batch: 050 ----
mean loss: 197.81
train mean loss: 197.35
epoch train time: 0:00:07.874734
elapsed time: 0:30:11.155995
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 15:30:27.881027
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.12
 ---- batch: 020 ----
mean loss: 200.64
 ---- batch: 030 ----
mean loss: 197.92
 ---- batch: 040 ----
mean loss: 186.34
 ---- batch: 050 ----
mean loss: 201.76
train mean loss: 197.57
epoch train time: 0:00:07.874235
elapsed time: 0:30:19.031350
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 15:30:35.756439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.70
 ---- batch: 020 ----
mean loss: 195.08
 ---- batch: 030 ----
mean loss: 192.67
 ---- batch: 040 ----
mean loss: 200.08
 ---- batch: 050 ----
mean loss: 206.36
train mean loss: 197.93
epoch train time: 0:00:07.878508
elapsed time: 0:30:26.911044
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 15:30:43.636113
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.42
 ---- batch: 020 ----
mean loss: 196.31
 ---- batch: 030 ----
mean loss: 199.54
 ---- batch: 040 ----
mean loss: 192.08
 ---- batch: 050 ----
mean loss: 201.90
train mean loss: 197.09
epoch train time: 0:00:07.847591
elapsed time: 0:30:34.759729
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 15:30:51.484786
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.48
 ---- batch: 020 ----
mean loss: 197.61
 ---- batch: 030 ----
mean loss: 205.40
 ---- batch: 040 ----
mean loss: 196.07
 ---- batch: 050 ----
mean loss: 190.66
train mean loss: 196.90
epoch train time: 0:00:07.841922
elapsed time: 0:30:42.603022
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 15:30:59.328216
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.58
 ---- batch: 020 ----
mean loss: 193.02
 ---- batch: 030 ----
mean loss: 191.71
 ---- batch: 040 ----
mean loss: 200.05
 ---- batch: 050 ----
mean loss: 195.92
train mean loss: 196.95
epoch train time: 0:00:07.845585
elapsed time: 0:30:50.450182
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 15:31:07.174953
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.79
 ---- batch: 020 ----
mean loss: 193.71
 ---- batch: 030 ----
mean loss: 200.11
 ---- batch: 040 ----
mean loss: 196.80
 ---- batch: 050 ----
mean loss: 203.14
train mean loss: 197.59
epoch train time: 0:00:07.847687
elapsed time: 0:30:58.298702
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 15:31:15.023735
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.34
 ---- batch: 020 ----
mean loss: 200.44
 ---- batch: 030 ----
mean loss: 196.98
 ---- batch: 040 ----
mean loss: 194.43
 ---- batch: 050 ----
mean loss: 190.69
train mean loss: 196.85
epoch train time: 0:00:07.851721
elapsed time: 0:31:06.151609
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 15:31:22.876648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.34
 ---- batch: 020 ----
mean loss: 193.34
 ---- batch: 030 ----
mean loss: 192.63
 ---- batch: 040 ----
mean loss: 202.26
 ---- batch: 050 ----
mean loss: 203.99
train mean loss: 197.45
epoch train time: 0:00:07.844090
elapsed time: 0:31:13.996813
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 15:31:30.721889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.37
 ---- batch: 020 ----
mean loss: 201.10
 ---- batch: 030 ----
mean loss: 197.10
 ---- batch: 040 ----
mean loss: 200.81
 ---- batch: 050 ----
mean loss: 186.73
train mean loss: 196.79
epoch train time: 0:00:07.838012
elapsed time: 0:31:21.835952
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 15:31:38.560997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.99
 ---- batch: 020 ----
mean loss: 195.24
 ---- batch: 030 ----
mean loss: 202.05
 ---- batch: 040 ----
mean loss: 196.27
 ---- batch: 050 ----
mean loss: 197.01
train mean loss: 196.84
epoch train time: 0:00:07.839040
elapsed time: 0:31:29.676080
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 15:31:46.401189
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.07
 ---- batch: 020 ----
mean loss: 199.44
 ---- batch: 030 ----
mean loss: 198.19
 ---- batch: 040 ----
mean loss: 192.11
 ---- batch: 050 ----
mean loss: 202.14
train mean loss: 197.35
epoch train time: 0:00:07.840202
elapsed time: 0:31:37.517457
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 15:31:54.242543
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.73
 ---- batch: 020 ----
mean loss: 190.66
 ---- batch: 030 ----
mean loss: 192.42
 ---- batch: 040 ----
mean loss: 202.45
 ---- batch: 050 ----
mean loss: 198.65
train mean loss: 198.07
epoch train time: 0:00:07.854573
elapsed time: 0:31:45.373238
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 15:32:02.098276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.43
 ---- batch: 020 ----
mean loss: 198.75
 ---- batch: 030 ----
mean loss: 199.69
 ---- batch: 040 ----
mean loss: 208.08
 ---- batch: 050 ----
mean loss: 190.43
train mean loss: 196.93
epoch train time: 0:00:07.835078
elapsed time: 0:31:53.209509
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 15:32:09.934867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.22
 ---- batch: 020 ----
mean loss: 196.96
 ---- batch: 030 ----
mean loss: 191.65
 ---- batch: 040 ----
mean loss: 199.07
 ---- batch: 050 ----
mean loss: 196.36
train mean loss: 196.96
epoch train time: 0:00:07.847176
elapsed time: 0:32:01.058106
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 15:32:17.783171
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.98
 ---- batch: 020 ----
mean loss: 200.78
 ---- batch: 030 ----
mean loss: 191.21
 ---- batch: 040 ----
mean loss: 194.77
 ---- batch: 050 ----
mean loss: 196.97
train mean loss: 197.08
epoch train time: 0:00:07.835621
elapsed time: 0:32:08.894927
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 15:32:25.619963
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.39
 ---- batch: 020 ----
mean loss: 193.61
 ---- batch: 030 ----
mean loss: 190.86
 ---- batch: 040 ----
mean loss: 197.02
 ---- batch: 050 ----
mean loss: 206.91
train mean loss: 197.48
epoch train time: 0:00:07.831745
elapsed time: 0:32:16.727882
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 15:32:33.452967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.86
 ---- batch: 020 ----
mean loss: 198.17
 ---- batch: 030 ----
mean loss: 194.75
 ---- batch: 040 ----
mean loss: 196.17
 ---- batch: 050 ----
mean loss: 200.39
train mean loss: 195.89
epoch train time: 0:00:07.810636
elapsed time: 0:32:24.539747
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 15:32:41.264825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.44
 ---- batch: 020 ----
mean loss: 198.20
 ---- batch: 030 ----
mean loss: 200.78
 ---- batch: 040 ----
mean loss: 197.87
 ---- batch: 050 ----
mean loss: 193.44
train mean loss: 196.78
epoch train time: 0:00:07.891158
elapsed time: 0:32:32.432078
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 15:32:49.157169
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.96
 ---- batch: 020 ----
mean loss: 188.41
 ---- batch: 030 ----
mean loss: 195.99
 ---- batch: 040 ----
mean loss: 198.46
 ---- batch: 050 ----
mean loss: 207.51
train mean loss: 196.37
epoch train time: 0:00:07.840118
elapsed time: 0:32:40.273475
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 15:32:56.998564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.13
 ---- batch: 020 ----
mean loss: 197.31
 ---- batch: 030 ----
mean loss: 196.36
 ---- batch: 040 ----
mean loss: 198.07
 ---- batch: 050 ----
mean loss: 190.51
train mean loss: 196.19
epoch train time: 0:00:07.848517
elapsed time: 0:32:48.123191
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 15:33:04.848243
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.03
 ---- batch: 020 ----
mean loss: 192.19
 ---- batch: 030 ----
mean loss: 202.76
 ---- batch: 040 ----
mean loss: 192.22
 ---- batch: 050 ----
mean loss: 198.21
train mean loss: 196.24
epoch train time: 0:00:07.863397
elapsed time: 0:32:55.997754
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_8/checkpoint.pth.tar
**** end time: 2019-09-26 15:33:12.722456 ****
