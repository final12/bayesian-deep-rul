Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 519
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 07:15:34.196878 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 07:15:34.214077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2008.07
 ---- batch: 020 ----
mean loss: 1402.14
 ---- batch: 030 ----
mean loss: 1195.28
 ---- batch: 040 ----
mean loss: 1139.14
 ---- batch: 050 ----
mean loss: 1131.11
 ---- batch: 060 ----
mean loss: 1089.91
 ---- batch: 070 ----
mean loss: 1053.22
 ---- batch: 080 ----
mean loss: 1047.77
 ---- batch: 090 ----
mean loss: 1017.54
 ---- batch: 100 ----
mean loss: 1003.06
 ---- batch: 110 ----
mean loss: 1014.63
train mean loss: 1185.78
epoch train time: 0:00:45.427924
elapsed time: 0:00:45.453349
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 07:16:19.650270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 989.44
 ---- batch: 020 ----
mean loss: 983.33
 ---- batch: 030 ----
mean loss: 951.80
 ---- batch: 040 ----
mean loss: 966.73
 ---- batch: 050 ----
mean loss: 941.85
 ---- batch: 060 ----
mean loss: 950.85
 ---- batch: 070 ----
mean loss: 975.18
 ---- batch: 080 ----
mean loss: 945.83
 ---- batch: 090 ----
mean loss: 971.61
 ---- batch: 100 ----
mean loss: 961.33
 ---- batch: 110 ----
mean loss: 938.01
train mean loss: 961.09
epoch train time: 0:00:15.380122
elapsed time: 0:01:00.833956
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 07:16:35.031296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.65
 ---- batch: 020 ----
mean loss: 953.04
 ---- batch: 030 ----
mean loss: 925.29
 ---- batch: 040 ----
mean loss: 922.11
 ---- batch: 050 ----
mean loss: 916.48
 ---- batch: 060 ----
mean loss: 914.51
 ---- batch: 070 ----
mean loss: 930.63
 ---- batch: 080 ----
mean loss: 926.01
 ---- batch: 090 ----
mean loss: 928.62
 ---- batch: 100 ----
mean loss: 908.91
 ---- batch: 110 ----
mean loss: 937.78
train mean loss: 924.30
epoch train time: 0:00:15.415841
elapsed time: 0:01:16.250693
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 07:16:50.448104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.48
 ---- batch: 020 ----
mean loss: 931.00
 ---- batch: 030 ----
mean loss: 911.23
 ---- batch: 040 ----
mean loss: 894.37
 ---- batch: 050 ----
mean loss: 895.06
 ---- batch: 060 ----
mean loss: 902.76
 ---- batch: 070 ----
mean loss: 910.03
 ---- batch: 080 ----
mean loss: 885.58
 ---- batch: 090 ----
mean loss: 910.50
 ---- batch: 100 ----
mean loss: 915.80
 ---- batch: 110 ----
mean loss: 896.92
train mean loss: 905.39
epoch train time: 0:00:15.341459
elapsed time: 0:01:31.593136
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 07:17:05.790496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.86
 ---- batch: 020 ----
mean loss: 887.63
 ---- batch: 030 ----
mean loss: 897.56
 ---- batch: 040 ----
mean loss: 908.35
 ---- batch: 050 ----
mean loss: 898.62
 ---- batch: 060 ----
mean loss: 891.26
 ---- batch: 070 ----
mean loss: 894.74
 ---- batch: 080 ----
mean loss: 891.90
 ---- batch: 090 ----
mean loss: 889.92
 ---- batch: 100 ----
mean loss: 907.57
 ---- batch: 110 ----
mean loss: 895.46
train mean loss: 895.91
epoch train time: 0:00:15.400225
elapsed time: 0:01:46.994336
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 07:17:21.191815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.39
 ---- batch: 020 ----
mean loss: 893.65
 ---- batch: 030 ----
mean loss: 891.41
 ---- batch: 040 ----
mean loss: 861.51
 ---- batch: 050 ----
mean loss: 885.13
 ---- batch: 060 ----
mean loss: 901.15
 ---- batch: 070 ----
mean loss: 883.39
 ---- batch: 080 ----
mean loss: 906.94
 ---- batch: 090 ----
mean loss: 880.87
 ---- batch: 100 ----
mean loss: 891.54
 ---- batch: 110 ----
mean loss: 895.22
train mean loss: 888.06
epoch train time: 0:00:15.428262
elapsed time: 0:02:02.423599
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 07:17:36.620969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.34
 ---- batch: 020 ----
mean loss: 863.82
 ---- batch: 030 ----
mean loss: 877.01
 ---- batch: 040 ----
mean loss: 887.13
 ---- batch: 050 ----
mean loss: 854.83
 ---- batch: 060 ----
mean loss: 889.76
 ---- batch: 070 ----
mean loss: 878.25
 ---- batch: 080 ----
mean loss: 889.61
 ---- batch: 090 ----
mean loss: 888.43
 ---- batch: 100 ----
mean loss: 901.18
 ---- batch: 110 ----
mean loss: 876.64
train mean loss: 881.86
epoch train time: 0:00:15.357570
elapsed time: 0:02:17.782118
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 07:17:51.979488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.15
 ---- batch: 020 ----
mean loss: 862.40
 ---- batch: 030 ----
mean loss: 899.86
 ---- batch: 040 ----
mean loss: 881.00
 ---- batch: 050 ----
mean loss: 875.32
 ---- batch: 060 ----
mean loss: 872.32
 ---- batch: 070 ----
mean loss: 845.24
 ---- batch: 080 ----
mean loss: 879.46
 ---- batch: 090 ----
mean loss: 877.94
 ---- batch: 100 ----
mean loss: 884.99
 ---- batch: 110 ----
mean loss: 881.11
train mean loss: 874.71
epoch train time: 0:00:15.472672
elapsed time: 0:02:33.255672
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 07:18:07.453028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.49
 ---- batch: 020 ----
mean loss: 868.24
 ---- batch: 030 ----
mean loss: 858.01
 ---- batch: 040 ----
mean loss: 860.35
 ---- batch: 050 ----
mean loss: 866.08
 ---- batch: 060 ----
mean loss: 856.64
 ---- batch: 070 ----
mean loss: 876.08
 ---- batch: 080 ----
mean loss: 857.30
 ---- batch: 090 ----
mean loss: 864.47
 ---- batch: 100 ----
mean loss: 881.76
 ---- batch: 110 ----
mean loss: 877.26
train mean loss: 868.90
epoch train time: 0:00:15.352565
elapsed time: 0:02:48.609085
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 07:18:22.806468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.46
 ---- batch: 020 ----
mean loss: 872.88
 ---- batch: 030 ----
mean loss: 838.64
 ---- batch: 040 ----
mean loss: 891.06
 ---- batch: 050 ----
mean loss: 863.31
 ---- batch: 060 ----
mean loss: 861.22
 ---- batch: 070 ----
mean loss: 854.87
 ---- batch: 080 ----
mean loss: 894.17
 ---- batch: 090 ----
mean loss: 852.49
 ---- batch: 100 ----
mean loss: 849.46
 ---- batch: 110 ----
mean loss: 866.35
train mean loss: 863.53
epoch train time: 0:00:15.393049
elapsed time: 0:03:04.003073
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 07:18:38.200489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.54
 ---- batch: 020 ----
mean loss: 843.94
 ---- batch: 030 ----
mean loss: 852.98
 ---- batch: 040 ----
mean loss: 865.97
 ---- batch: 050 ----
mean loss: 854.92
 ---- batch: 060 ----
mean loss: 858.39
 ---- batch: 070 ----
mean loss: 845.09
 ---- batch: 080 ----
mean loss: 857.52
 ---- batch: 090 ----
mean loss: 860.57
 ---- batch: 100 ----
mean loss: 851.84
 ---- batch: 110 ----
mean loss: 842.09
train mean loss: 853.54
epoch train time: 0:00:15.316091
elapsed time: 0:03:19.320103
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 07:18:53.517502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.40
 ---- batch: 020 ----
mean loss: 835.27
 ---- batch: 030 ----
mean loss: 860.50
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 849.34
 ---- batch: 060 ----
mean loss: 849.13
 ---- batch: 070 ----
mean loss: 838.76
 ---- batch: 080 ----
mean loss: 836.34
 ---- batch: 090 ----
mean loss: 858.52
 ---- batch: 100 ----
mean loss: 844.78
 ---- batch: 110 ----
mean loss: 804.00
train mean loss: 846.80
epoch train time: 0:00:15.403979
elapsed time: 0:03:34.724976
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 07:19:08.922461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.36
 ---- batch: 020 ----
mean loss: 854.14
 ---- batch: 030 ----
mean loss: 851.86
 ---- batch: 040 ----
mean loss: 830.27
 ---- batch: 050 ----
mean loss: 825.27
 ---- batch: 060 ----
mean loss: 837.30
 ---- batch: 070 ----
mean loss: 845.93
 ---- batch: 080 ----
mean loss: 806.30
 ---- batch: 090 ----
mean loss: 833.47
 ---- batch: 100 ----
mean loss: 846.12
 ---- batch: 110 ----
mean loss: 812.60
train mean loss: 836.42
epoch train time: 0:00:15.342672
elapsed time: 0:03:50.068690
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 07:19:24.266083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.36
 ---- batch: 020 ----
mean loss: 825.79
 ---- batch: 030 ----
mean loss: 826.65
 ---- batch: 040 ----
mean loss: 816.45
 ---- batch: 050 ----
mean loss: 809.99
 ---- batch: 060 ----
mean loss: 837.71
 ---- batch: 070 ----
mean loss: 825.85
 ---- batch: 080 ----
mean loss: 819.41
 ---- batch: 090 ----
mean loss: 821.33
 ---- batch: 100 ----
mean loss: 832.75
 ---- batch: 110 ----
mean loss: 838.73
train mean loss: 825.11
epoch train time: 0:00:15.304807
elapsed time: 0:04:05.374432
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 07:19:39.571836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.36
 ---- batch: 020 ----
mean loss: 799.42
 ---- batch: 030 ----
mean loss: 809.01
 ---- batch: 040 ----
mean loss: 818.94
 ---- batch: 050 ----
mean loss: 809.30
 ---- batch: 060 ----
mean loss: 790.76
 ---- batch: 070 ----
mean loss: 799.56
 ---- batch: 080 ----
mean loss: 786.71
 ---- batch: 090 ----
mean loss: 785.77
 ---- batch: 100 ----
mean loss: 783.49
 ---- batch: 110 ----
mean loss: 803.61
train mean loss: 799.33
epoch train time: 0:00:15.354808
elapsed time: 0:04:20.730214
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 07:19:54.927670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.42
 ---- batch: 020 ----
mean loss: 769.10
 ---- batch: 030 ----
mean loss: 761.35
 ---- batch: 040 ----
mean loss: 742.54
 ---- batch: 050 ----
mean loss: 753.23
 ---- batch: 060 ----
mean loss: 765.86
 ---- batch: 070 ----
mean loss: 757.15
 ---- batch: 080 ----
mean loss: 731.04
 ---- batch: 090 ----
mean loss: 719.36
 ---- batch: 100 ----
mean loss: 737.91
 ---- batch: 110 ----
mean loss: 713.28
train mean loss: 747.69
epoch train time: 0:00:15.322649
elapsed time: 0:04:36.053922
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 07:20:10.251341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 703.81
 ---- batch: 020 ----
mean loss: 695.07
 ---- batch: 030 ----
mean loss: 709.80
 ---- batch: 040 ----
mean loss: 716.05
 ---- batch: 050 ----
mean loss: 709.04
 ---- batch: 060 ----
mean loss: 711.17
 ---- batch: 070 ----
mean loss: 689.72
 ---- batch: 080 ----
mean loss: 682.43
 ---- batch: 090 ----
mean loss: 652.55
 ---- batch: 100 ----
mean loss: 668.67
 ---- batch: 110 ----
mean loss: 661.50
train mean loss: 690.66
epoch train time: 0:00:15.311708
elapsed time: 0:04:51.366548
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 07:20:25.563947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 658.74
 ---- batch: 020 ----
mean loss: 678.39
 ---- batch: 030 ----
mean loss: 646.14
 ---- batch: 040 ----
mean loss: 657.85
 ---- batch: 050 ----
mean loss: 657.40
 ---- batch: 060 ----
mean loss: 634.92
 ---- batch: 070 ----
mean loss: 648.88
 ---- batch: 080 ----
mean loss: 641.22
 ---- batch: 090 ----
mean loss: 637.33
 ---- batch: 100 ----
mean loss: 635.05
 ---- batch: 110 ----
mean loss: 640.88
train mean loss: 647.28
epoch train time: 0:00:15.404248
elapsed time: 0:05:06.771766
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 07:20:40.969150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 619.71
 ---- batch: 020 ----
mean loss: 630.12
 ---- batch: 030 ----
mean loss: 630.69
 ---- batch: 040 ----
mean loss: 597.29
 ---- batch: 050 ----
mean loss: 610.59
 ---- batch: 060 ----
mean loss: 600.51
 ---- batch: 070 ----
mean loss: 621.56
 ---- batch: 080 ----
mean loss: 617.18
 ---- batch: 090 ----
mean loss: 589.80
 ---- batch: 100 ----
mean loss: 598.10
 ---- batch: 110 ----
mean loss: 608.97
train mean loss: 610.63
epoch train time: 0:00:15.231508
elapsed time: 0:05:22.004185
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 07:20:56.201568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 593.61
 ---- batch: 020 ----
mean loss: 583.15
 ---- batch: 030 ----
mean loss: 600.49
 ---- batch: 040 ----
mean loss: 577.16
 ---- batch: 050 ----
mean loss: 575.61
 ---- batch: 060 ----
mean loss: 594.29
 ---- batch: 070 ----
mean loss: 583.88
 ---- batch: 080 ----
mean loss: 590.97
 ---- batch: 090 ----
mean loss: 568.38
 ---- batch: 100 ----
mean loss: 559.24
 ---- batch: 110 ----
mean loss: 570.61
train mean loss: 580.85
epoch train time: 0:00:15.281896
elapsed time: 0:05:37.287085
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 07:21:11.484475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.91
 ---- batch: 020 ----
mean loss: 568.31
 ---- batch: 030 ----
mean loss: 575.88
 ---- batch: 040 ----
mean loss: 562.46
 ---- batch: 050 ----
mean loss: 561.19
 ---- batch: 060 ----
mean loss: 533.40
 ---- batch: 070 ----
mean loss: 542.74
 ---- batch: 080 ----
mean loss: 534.67
 ---- batch: 090 ----
mean loss: 536.16
 ---- batch: 100 ----
mean loss: 531.40
 ---- batch: 110 ----
mean loss: 535.89
train mean loss: 547.97
epoch train time: 0:00:15.357670
elapsed time: 0:05:52.645915
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 07:21:26.843456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 522.30
 ---- batch: 020 ----
mean loss: 527.28
 ---- batch: 030 ----
mean loss: 531.79
 ---- batch: 040 ----
mean loss: 524.34
 ---- batch: 050 ----
mean loss: 528.59
 ---- batch: 060 ----
mean loss: 526.31
 ---- batch: 070 ----
mean loss: 525.22
 ---- batch: 080 ----
mean loss: 512.66
 ---- batch: 090 ----
mean loss: 506.52
 ---- batch: 100 ----
mean loss: 505.77
 ---- batch: 110 ----
mean loss: 490.28
train mean loss: 518.00
epoch train time: 0:00:15.252216
elapsed time: 0:06:07.899109
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 07:21:42.096477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.61
 ---- batch: 020 ----
mean loss: 489.87
 ---- batch: 030 ----
mean loss: 487.94
 ---- batch: 040 ----
mean loss: 494.07
 ---- batch: 050 ----
mean loss: 491.36
 ---- batch: 060 ----
mean loss: 494.70
 ---- batch: 070 ----
mean loss: 498.82
 ---- batch: 080 ----
mean loss: 482.91
 ---- batch: 090 ----
mean loss: 485.72
 ---- batch: 100 ----
mean loss: 481.79
 ---- batch: 110 ----
mean loss: 477.50
train mean loss: 488.85
epoch train time: 0:00:15.327581
elapsed time: 0:06:23.227596
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 07:21:57.424954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.31
 ---- batch: 020 ----
mean loss: 472.31
 ---- batch: 030 ----
mean loss: 481.64
 ---- batch: 040 ----
mean loss: 473.28
 ---- batch: 050 ----
mean loss: 478.56
 ---- batch: 060 ----
mean loss: 477.67
 ---- batch: 070 ----
mean loss: 476.27
 ---- batch: 080 ----
mean loss: 472.32
 ---- batch: 090 ----
mean loss: 458.66
 ---- batch: 100 ----
mean loss: 465.69
 ---- batch: 110 ----
mean loss: 461.72
train mean loss: 472.02
epoch train time: 0:00:15.307135
elapsed time: 0:06:38.535604
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 07:22:12.732998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.42
 ---- batch: 020 ----
mean loss: 465.89
 ---- batch: 030 ----
mean loss: 445.47
 ---- batch: 040 ----
mean loss: 451.49
 ---- batch: 050 ----
mean loss: 445.44
 ---- batch: 060 ----
mean loss: 451.00
 ---- batch: 070 ----
mean loss: 450.30
 ---- batch: 080 ----
mean loss: 450.33
 ---- batch: 090 ----
mean loss: 450.39
 ---- batch: 100 ----
mean loss: 445.08
 ---- batch: 110 ----
mean loss: 447.22
train mean loss: 450.29
epoch train time: 0:00:15.290391
elapsed time: 0:06:53.826982
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 07:22:28.024391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.55
 ---- batch: 020 ----
mean loss: 449.50
 ---- batch: 030 ----
mean loss: 438.07
 ---- batch: 040 ----
mean loss: 437.18
 ---- batch: 050 ----
mean loss: 438.56
 ---- batch: 060 ----
mean loss: 438.13
 ---- batch: 070 ----
mean loss: 436.75
 ---- batch: 080 ----
mean loss: 439.86
 ---- batch: 090 ----
mean loss: 426.54
 ---- batch: 100 ----
mean loss: 419.90
 ---- batch: 110 ----
mean loss: 423.13
train mean loss: 434.95
epoch train time: 0:00:15.318030
elapsed time: 0:07:09.145949
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 07:22:43.343362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.97
 ---- batch: 020 ----
mean loss: 425.56
 ---- batch: 030 ----
mean loss: 426.73
 ---- batch: 040 ----
mean loss: 420.54
 ---- batch: 050 ----
mean loss: 408.40
 ---- batch: 060 ----
mean loss: 410.23
 ---- batch: 070 ----
mean loss: 403.78
 ---- batch: 080 ----
mean loss: 423.75
 ---- batch: 090 ----
mean loss: 424.96
 ---- batch: 100 ----
mean loss: 402.75
 ---- batch: 110 ----
mean loss: 412.61
train mean loss: 417.55
epoch train time: 0:00:15.375794
elapsed time: 0:07:24.522719
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 07:22:58.720084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.77
 ---- batch: 020 ----
mean loss: 403.29
 ---- batch: 030 ----
mean loss: 418.62
 ---- batch: 040 ----
mean loss: 408.77
 ---- batch: 050 ----
mean loss: 414.33
 ---- batch: 060 ----
mean loss: 399.24
 ---- batch: 070 ----
mean loss: 407.68
 ---- batch: 080 ----
mean loss: 403.67
 ---- batch: 090 ----
mean loss: 399.99
 ---- batch: 100 ----
mean loss: 402.88
 ---- batch: 110 ----
mean loss: 399.03
train mean loss: 405.87
epoch train time: 0:00:15.408505
elapsed time: 0:07:39.932185
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 07:23:14.129575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.47
 ---- batch: 020 ----
mean loss: 390.70
 ---- batch: 030 ----
mean loss: 385.64
 ---- batch: 040 ----
mean loss: 393.16
 ---- batch: 050 ----
mean loss: 400.29
 ---- batch: 060 ----
mean loss: 392.56
 ---- batch: 070 ----
mean loss: 389.57
 ---- batch: 080 ----
mean loss: 396.77
 ---- batch: 090 ----
mean loss: 388.83
 ---- batch: 100 ----
mean loss: 385.48
 ---- batch: 110 ----
mean loss: 391.60
train mean loss: 391.01
epoch train time: 0:00:15.328186
elapsed time: 0:07:55.261297
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 07:23:29.458763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.97
 ---- batch: 020 ----
mean loss: 388.15
 ---- batch: 030 ----
mean loss: 381.98
 ---- batch: 040 ----
mean loss: 385.00
 ---- batch: 050 ----
mean loss: 388.00
 ---- batch: 060 ----
mean loss: 373.57
 ---- batch: 070 ----
mean loss: 377.87
 ---- batch: 080 ----
mean loss: 376.42
 ---- batch: 090 ----
mean loss: 373.32
 ---- batch: 100 ----
mean loss: 378.95
 ---- batch: 110 ----
mean loss: 389.79
train mean loss: 381.34
epoch train time: 0:00:15.424675
elapsed time: 0:08:10.686977
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 07:23:44.884400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.10
 ---- batch: 020 ----
mean loss: 370.10
 ---- batch: 030 ----
mean loss: 370.30
 ---- batch: 040 ----
mean loss: 366.60
 ---- batch: 050 ----
mean loss: 365.55
 ---- batch: 060 ----
mean loss: 380.00
 ---- batch: 070 ----
mean loss: 366.49
 ---- batch: 080 ----
mean loss: 370.63
 ---- batch: 090 ----
mean loss: 371.01
 ---- batch: 100 ----
mean loss: 371.80
 ---- batch: 110 ----
mean loss: 369.13
train mean loss: 370.93
epoch train time: 0:00:15.324266
elapsed time: 0:08:26.012232
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 07:24:00.209737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.30
 ---- batch: 020 ----
mean loss: 379.03
 ---- batch: 030 ----
mean loss: 368.16
 ---- batch: 040 ----
mean loss: 364.05
 ---- batch: 050 ----
mean loss: 357.98
 ---- batch: 060 ----
mean loss: 366.46
 ---- batch: 070 ----
mean loss: 350.08
 ---- batch: 080 ----
mean loss: 362.18
 ---- batch: 090 ----
mean loss: 361.35
 ---- batch: 100 ----
mean loss: 356.98
 ---- batch: 110 ----
mean loss: 357.11
train mean loss: 362.76
epoch train time: 0:00:15.330926
elapsed time: 0:08:41.344041
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 07:24:15.541477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.61
 ---- batch: 020 ----
mean loss: 343.78
 ---- batch: 030 ----
mean loss: 348.34
 ---- batch: 040 ----
mean loss: 346.96
 ---- batch: 050 ----
mean loss: 353.02
 ---- batch: 060 ----
mean loss: 351.59
 ---- batch: 070 ----
mean loss: 349.98
 ---- batch: 080 ----
mean loss: 365.52
 ---- batch: 090 ----
mean loss: 355.64
 ---- batch: 100 ----
mean loss: 348.64
 ---- batch: 110 ----
mean loss: 348.27
train mean loss: 351.96
epoch train time: 0:00:15.351222
elapsed time: 0:08:56.696255
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 07:24:30.893653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.37
 ---- batch: 020 ----
mean loss: 339.65
 ---- batch: 030 ----
mean loss: 343.62
 ---- batch: 040 ----
mean loss: 335.65
 ---- batch: 050 ----
mean loss: 343.90
 ---- batch: 060 ----
mean loss: 336.68
 ---- batch: 070 ----
mean loss: 351.14
 ---- batch: 080 ----
mean loss: 337.68
 ---- batch: 090 ----
mean loss: 349.31
 ---- batch: 100 ----
mean loss: 334.88
 ---- batch: 110 ----
mean loss: 340.72
train mean loss: 341.43
epoch train time: 0:00:15.306463
elapsed time: 0:09:12.003686
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 07:24:46.201055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.64
 ---- batch: 020 ----
mean loss: 335.45
 ---- batch: 030 ----
mean loss: 338.49
 ---- batch: 040 ----
mean loss: 350.92
 ---- batch: 050 ----
mean loss: 330.45
 ---- batch: 060 ----
mean loss: 337.88
 ---- batch: 070 ----
mean loss: 323.97
 ---- batch: 080 ----
mean loss: 341.84
 ---- batch: 090 ----
mean loss: 334.93
 ---- batch: 100 ----
mean loss: 343.64
 ---- batch: 110 ----
mean loss: 351.85
train mean loss: 338.81
epoch train time: 0:00:15.319064
elapsed time: 0:09:27.323541
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 07:25:01.520923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.12
 ---- batch: 020 ----
mean loss: 327.71
 ---- batch: 030 ----
mean loss: 333.26
 ---- batch: 040 ----
mean loss: 322.63
 ---- batch: 050 ----
mean loss: 330.10
 ---- batch: 060 ----
mean loss: 346.41
 ---- batch: 070 ----
mean loss: 334.17
 ---- batch: 080 ----
mean loss: 333.89
 ---- batch: 090 ----
mean loss: 331.05
 ---- batch: 100 ----
mean loss: 339.11
 ---- batch: 110 ----
mean loss: 323.77
train mean loss: 332.93
epoch train time: 0:00:15.311972
elapsed time: 0:09:42.636482
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 07:25:16.833874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.26
 ---- batch: 020 ----
mean loss: 328.02
 ---- batch: 030 ----
mean loss: 331.56
 ---- batch: 040 ----
mean loss: 321.07
 ---- batch: 050 ----
mean loss: 324.03
 ---- batch: 060 ----
mean loss: 330.13
 ---- batch: 070 ----
mean loss: 327.91
 ---- batch: 080 ----
mean loss: 323.25
 ---- batch: 090 ----
mean loss: 324.97
 ---- batch: 100 ----
mean loss: 321.04
 ---- batch: 110 ----
mean loss: 324.66
train mean loss: 325.51
epoch train time: 0:00:15.302492
elapsed time: 0:09:57.939934
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 07:25:32.137339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.93
 ---- batch: 020 ----
mean loss: 314.81
 ---- batch: 030 ----
mean loss: 331.49
 ---- batch: 040 ----
mean loss: 322.66
 ---- batch: 050 ----
mean loss: 325.77
 ---- batch: 060 ----
mean loss: 324.84
 ---- batch: 070 ----
mean loss: 307.30
 ---- batch: 080 ----
mean loss: 318.09
 ---- batch: 090 ----
mean loss: 311.85
 ---- batch: 100 ----
mean loss: 324.25
 ---- batch: 110 ----
mean loss: 327.74
train mean loss: 319.83
epoch train time: 0:00:15.352502
elapsed time: 0:10:13.293379
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 07:25:47.490729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.58
 ---- batch: 020 ----
mean loss: 324.19
 ---- batch: 030 ----
mean loss: 316.40
 ---- batch: 040 ----
mean loss: 318.20
 ---- batch: 050 ----
mean loss: 309.98
 ---- batch: 060 ----
mean loss: 313.04
 ---- batch: 070 ----
mean loss: 307.34
 ---- batch: 080 ----
mean loss: 320.97
 ---- batch: 090 ----
mean loss: 306.27
 ---- batch: 100 ----
mean loss: 315.35
 ---- batch: 110 ----
mean loss: 307.25
train mean loss: 314.82
epoch train time: 0:00:15.420910
elapsed time: 0:10:28.715170
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 07:26:02.912576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.17
 ---- batch: 020 ----
mean loss: 308.98
 ---- batch: 030 ----
mean loss: 301.38
 ---- batch: 040 ----
mean loss: 307.11
 ---- batch: 050 ----
mean loss: 306.32
 ---- batch: 060 ----
mean loss: 306.98
 ---- batch: 070 ----
mean loss: 315.27
 ---- batch: 080 ----
mean loss: 310.32
 ---- batch: 090 ----
mean loss: 310.61
 ---- batch: 100 ----
mean loss: 315.32
 ---- batch: 110 ----
mean loss: 308.69
train mean loss: 309.21
epoch train time: 0:00:15.381065
elapsed time: 0:10:44.097238
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 07:26:18.294616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.33
 ---- batch: 020 ----
mean loss: 303.25
 ---- batch: 030 ----
mean loss: 310.25
 ---- batch: 040 ----
mean loss: 309.03
 ---- batch: 050 ----
mean loss: 310.33
 ---- batch: 060 ----
mean loss: 312.56
 ---- batch: 070 ----
mean loss: 301.87
 ---- batch: 080 ----
mean loss: 301.13
 ---- batch: 090 ----
mean loss: 302.47
 ---- batch: 100 ----
mean loss: 297.67
 ---- batch: 110 ----
mean loss: 303.71
train mean loss: 304.53
epoch train time: 0:00:15.395554
elapsed time: 0:10:59.493732
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 07:26:33.691071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.82
 ---- batch: 020 ----
mean loss: 302.07
 ---- batch: 030 ----
mean loss: 309.52
 ---- batch: 040 ----
mean loss: 305.50
 ---- batch: 050 ----
mean loss: 300.71
 ---- batch: 060 ----
mean loss: 292.47
 ---- batch: 070 ----
mean loss: 309.78
 ---- batch: 080 ----
mean loss: 300.78
 ---- batch: 090 ----
mean loss: 299.65
 ---- batch: 100 ----
mean loss: 293.90
 ---- batch: 110 ----
mean loss: 301.67
train mean loss: 301.07
epoch train time: 0:00:15.363285
elapsed time: 0:11:14.857896
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 07:26:49.055340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.82
 ---- batch: 020 ----
mean loss: 294.45
 ---- batch: 030 ----
mean loss: 302.64
 ---- batch: 040 ----
mean loss: 300.09
 ---- batch: 050 ----
mean loss: 293.85
 ---- batch: 060 ----
mean loss: 301.12
 ---- batch: 070 ----
mean loss: 301.74
 ---- batch: 080 ----
mean loss: 296.93
 ---- batch: 090 ----
mean loss: 291.39
 ---- batch: 100 ----
mean loss: 296.75
 ---- batch: 110 ----
mean loss: 303.76
train mean loss: 297.65
epoch train time: 0:00:15.391107
elapsed time: 0:11:30.249988
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 07:27:04.447391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.76
 ---- batch: 020 ----
mean loss: 291.54
 ---- batch: 030 ----
mean loss: 298.39
 ---- batch: 040 ----
mean loss: 300.58
 ---- batch: 050 ----
mean loss: 290.85
 ---- batch: 060 ----
mean loss: 298.51
 ---- batch: 070 ----
mean loss: 294.27
 ---- batch: 080 ----
mean loss: 297.28
 ---- batch: 090 ----
mean loss: 286.83
 ---- batch: 100 ----
mean loss: 292.36
 ---- batch: 110 ----
mean loss: 293.40
train mean loss: 294.53
epoch train time: 0:00:15.386535
elapsed time: 0:11:45.637394
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 07:27:19.834818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.33
 ---- batch: 020 ----
mean loss: 296.51
 ---- batch: 030 ----
mean loss: 279.56
 ---- batch: 040 ----
mean loss: 291.99
 ---- batch: 050 ----
mean loss: 289.97
 ---- batch: 060 ----
mean loss: 307.33
 ---- batch: 070 ----
mean loss: 287.67
 ---- batch: 080 ----
mean loss: 285.08
 ---- batch: 090 ----
mean loss: 305.39
 ---- batch: 100 ----
mean loss: 290.33
 ---- batch: 110 ----
mean loss: 275.40
train mean loss: 290.53
epoch train time: 0:00:15.311351
elapsed time: 0:12:00.949752
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 07:27:35.147245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.37
 ---- batch: 020 ----
mean loss: 297.29
 ---- batch: 030 ----
mean loss: 296.19
 ---- batch: 040 ----
mean loss: 289.94
 ---- batch: 050 ----
mean loss: 285.26
 ---- batch: 060 ----
mean loss: 280.14
 ---- batch: 070 ----
mean loss: 295.58
 ---- batch: 080 ----
mean loss: 293.82
 ---- batch: 090 ----
mean loss: 287.16
 ---- batch: 100 ----
mean loss: 285.00
 ---- batch: 110 ----
mean loss: 279.56
train mean loss: 289.81
epoch train time: 0:00:15.372187
elapsed time: 0:12:16.322852
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 07:27:50.520270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.55
 ---- batch: 020 ----
mean loss: 284.40
 ---- batch: 030 ----
mean loss: 283.00
 ---- batch: 040 ----
mean loss: 280.59
 ---- batch: 050 ----
mean loss: 289.42
 ---- batch: 060 ----
mean loss: 288.16
 ---- batch: 070 ----
mean loss: 287.83
 ---- batch: 080 ----
mean loss: 278.14
 ---- batch: 090 ----
mean loss: 283.12
 ---- batch: 100 ----
mean loss: 279.86
 ---- batch: 110 ----
mean loss: 284.77
train mean loss: 284.16
epoch train time: 0:00:15.347537
elapsed time: 0:12:31.671356
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 07:28:05.868755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.91
 ---- batch: 020 ----
mean loss: 292.29
 ---- batch: 030 ----
mean loss: 277.58
 ---- batch: 040 ----
mean loss: 284.21
 ---- batch: 050 ----
mean loss: 281.30
 ---- batch: 060 ----
mean loss: 284.73
 ---- batch: 070 ----
mean loss: 287.03
 ---- batch: 080 ----
mean loss: 289.09
 ---- batch: 090 ----
mean loss: 280.46
 ---- batch: 100 ----
mean loss: 262.23
 ---- batch: 110 ----
mean loss: 273.96
train mean loss: 281.13
epoch train time: 0:00:15.354724
elapsed time: 0:12:47.027021
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 07:28:21.224451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.37
 ---- batch: 020 ----
mean loss: 272.95
 ---- batch: 030 ----
mean loss: 275.49
 ---- batch: 040 ----
mean loss: 279.86
 ---- batch: 050 ----
mean loss: 274.06
 ---- batch: 060 ----
mean loss: 280.84
 ---- batch: 070 ----
mean loss: 279.55
 ---- batch: 080 ----
mean loss: 288.81
 ---- batch: 090 ----
mean loss: 279.59
 ---- batch: 100 ----
mean loss: 270.77
 ---- batch: 110 ----
mean loss: 277.86
train mean loss: 278.59
epoch train time: 0:00:15.334510
elapsed time: 0:13:02.362450
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 07:28:36.559856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.22
 ---- batch: 020 ----
mean loss: 277.13
 ---- batch: 030 ----
mean loss: 271.26
 ---- batch: 040 ----
mean loss: 290.98
 ---- batch: 050 ----
mean loss: 274.93
 ---- batch: 060 ----
mean loss: 277.79
 ---- batch: 070 ----
mean loss: 271.58
 ---- batch: 080 ----
mean loss: 274.55
 ---- batch: 090 ----
mean loss: 271.27
 ---- batch: 100 ----
mean loss: 275.53
 ---- batch: 110 ----
mean loss: 265.11
train mean loss: 276.18
epoch train time: 0:00:15.304742
elapsed time: 0:13:17.668192
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 07:28:51.865592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.51
 ---- batch: 020 ----
mean loss: 267.34
 ---- batch: 030 ----
mean loss: 266.74
 ---- batch: 040 ----
mean loss: 271.85
 ---- batch: 050 ----
mean loss: 274.36
 ---- batch: 060 ----
mean loss: 272.06
 ---- batch: 070 ----
mean loss: 269.76
 ---- batch: 080 ----
mean loss: 264.67
 ---- batch: 090 ----
mean loss: 282.81
 ---- batch: 100 ----
mean loss: 276.74
 ---- batch: 110 ----
mean loss: 279.54
train mean loss: 272.18
epoch train time: 0:00:15.389824
elapsed time: 0:13:33.058938
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 07:29:07.256312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.66
 ---- batch: 020 ----
mean loss: 275.14
 ---- batch: 030 ----
mean loss: 281.00
 ---- batch: 040 ----
mean loss: 266.27
 ---- batch: 050 ----
mean loss: 263.39
 ---- batch: 060 ----
mean loss: 265.64
 ---- batch: 070 ----
mean loss: 275.81
 ---- batch: 080 ----
mean loss: 272.26
 ---- batch: 090 ----
mean loss: 268.22
 ---- batch: 100 ----
mean loss: 273.53
 ---- batch: 110 ----
mean loss: 267.88
train mean loss: 270.43
epoch train time: 0:00:15.314834
elapsed time: 0:13:48.374683
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 07:29:22.572197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.24
 ---- batch: 020 ----
mean loss: 271.05
 ---- batch: 030 ----
mean loss: 270.54
 ---- batch: 040 ----
mean loss: 271.51
 ---- batch: 050 ----
mean loss: 262.38
 ---- batch: 060 ----
mean loss: 261.65
 ---- batch: 070 ----
mean loss: 266.50
 ---- batch: 080 ----
mean loss: 263.98
 ---- batch: 090 ----
mean loss: 262.08
 ---- batch: 100 ----
mean loss: 261.39
 ---- batch: 110 ----
mean loss: 271.09
train mean loss: 267.29
epoch train time: 0:00:15.321283
elapsed time: 0:14:03.697148
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 07:29:37.894556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.80
 ---- batch: 020 ----
mean loss: 266.96
 ---- batch: 030 ----
mean loss: 264.78
 ---- batch: 040 ----
mean loss: 262.33
 ---- batch: 050 ----
mean loss: 271.58
 ---- batch: 060 ----
mean loss: 271.47
 ---- batch: 070 ----
mean loss: 267.70
 ---- batch: 080 ----
mean loss: 262.86
 ---- batch: 090 ----
mean loss: 260.74
 ---- batch: 100 ----
mean loss: 263.27
 ---- batch: 110 ----
mean loss: 268.79
train mean loss: 266.67
epoch train time: 0:00:15.369967
elapsed time: 0:14:19.068056
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 07:29:53.265431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.42
 ---- batch: 020 ----
mean loss: 265.93
 ---- batch: 030 ----
mean loss: 273.07
 ---- batch: 040 ----
mean loss: 264.83
 ---- batch: 050 ----
mean loss: 265.24
 ---- batch: 060 ----
mean loss: 253.75
 ---- batch: 070 ----
mean loss: 252.81
 ---- batch: 080 ----
mean loss: 259.12
 ---- batch: 090 ----
mean loss: 258.04
 ---- batch: 100 ----
mean loss: 272.64
 ---- batch: 110 ----
mean loss: 272.04
train mean loss: 264.45
epoch train time: 0:00:15.372698
elapsed time: 0:14:34.441691
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 07:30:08.639128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.11
 ---- batch: 020 ----
mean loss: 256.70
 ---- batch: 030 ----
mean loss: 273.57
 ---- batch: 040 ----
mean loss: 260.34
 ---- batch: 050 ----
mean loss: 260.72
 ---- batch: 060 ----
mean loss: 258.23
 ---- batch: 070 ----
mean loss: 261.00
 ---- batch: 080 ----
mean loss: 257.73
 ---- batch: 090 ----
mean loss: 266.52
 ---- batch: 100 ----
mean loss: 258.97
 ---- batch: 110 ----
mean loss: 262.41
train mean loss: 261.90
epoch train time: 0:00:15.364262
elapsed time: 0:14:49.806949
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 07:30:24.004316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.10
 ---- batch: 020 ----
mean loss: 262.32
 ---- batch: 030 ----
mean loss: 271.79
 ---- batch: 040 ----
mean loss: 257.72
 ---- batch: 050 ----
mean loss: 267.22
 ---- batch: 060 ----
mean loss: 262.26
 ---- batch: 070 ----
mean loss: 251.43
 ---- batch: 080 ----
mean loss: 250.99
 ---- batch: 090 ----
mean loss: 256.91
 ---- batch: 100 ----
mean loss: 250.25
 ---- batch: 110 ----
mean loss: 265.76
train mean loss: 259.92
epoch train time: 0:00:15.396579
elapsed time: 0:15:05.204425
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 07:30:39.401838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.69
 ---- batch: 020 ----
mean loss: 257.14
 ---- batch: 030 ----
mean loss: 258.48
 ---- batch: 040 ----
mean loss: 246.66
 ---- batch: 050 ----
mean loss: 258.88
 ---- batch: 060 ----
mean loss: 254.91
 ---- batch: 070 ----
mean loss: 256.18
 ---- batch: 080 ----
mean loss: 256.07
 ---- batch: 090 ----
mean loss: 259.21
 ---- batch: 100 ----
mean loss: 256.68
 ---- batch: 110 ----
mean loss: 263.86
train mean loss: 257.41
epoch train time: 0:00:15.357284
elapsed time: 0:15:20.562668
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 07:30:54.760052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.56
 ---- batch: 020 ----
mean loss: 261.35
 ---- batch: 030 ----
mean loss: 257.63
 ---- batch: 040 ----
mean loss: 256.06
 ---- batch: 050 ----
mean loss: 250.14
 ---- batch: 060 ----
mean loss: 248.11
 ---- batch: 070 ----
mean loss: 257.00
 ---- batch: 080 ----
mean loss: 259.52
 ---- batch: 090 ----
mean loss: 251.90
 ---- batch: 100 ----
mean loss: 259.16
 ---- batch: 110 ----
mean loss: 253.10
train mean loss: 255.23
epoch train time: 0:00:15.292147
elapsed time: 0:15:35.855718
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 07:31:10.053138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.53
 ---- batch: 020 ----
mean loss: 255.77
 ---- batch: 030 ----
mean loss: 257.68
 ---- batch: 040 ----
mean loss: 251.08
 ---- batch: 050 ----
mean loss: 239.61
 ---- batch: 060 ----
mean loss: 251.92
 ---- batch: 070 ----
mean loss: 249.45
 ---- batch: 080 ----
mean loss: 258.22
 ---- batch: 090 ----
mean loss: 261.53
 ---- batch: 100 ----
mean loss: 258.43
 ---- batch: 110 ----
mean loss: 266.93
train mean loss: 254.93
epoch train time: 0:00:15.286239
elapsed time: 0:15:51.142954
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 07:31:25.340472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.94
 ---- batch: 020 ----
mean loss: 243.82
 ---- batch: 030 ----
mean loss: 251.69
 ---- batch: 040 ----
mean loss: 253.50
 ---- batch: 050 ----
mean loss: 249.33
 ---- batch: 060 ----
mean loss: 251.72
 ---- batch: 070 ----
mean loss: 248.73
 ---- batch: 080 ----
mean loss: 258.09
 ---- batch: 090 ----
mean loss: 254.11
 ---- batch: 100 ----
mean loss: 250.49
 ---- batch: 110 ----
mean loss: 250.11
train mean loss: 251.30
epoch train time: 0:00:15.310790
elapsed time: 0:16:06.454977
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 07:31:40.652407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.28
 ---- batch: 020 ----
mean loss: 256.59
 ---- batch: 030 ----
mean loss: 247.88
 ---- batch: 040 ----
mean loss: 239.03
 ---- batch: 050 ----
mean loss: 247.18
 ---- batch: 060 ----
mean loss: 248.58
 ---- batch: 070 ----
mean loss: 259.19
 ---- batch: 080 ----
mean loss: 248.05
 ---- batch: 090 ----
mean loss: 254.28
 ---- batch: 100 ----
mean loss: 243.50
 ---- batch: 110 ----
mean loss: 256.86
train mean loss: 249.31
epoch train time: 0:00:15.290575
elapsed time: 0:16:21.746552
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 07:31:55.944006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.17
 ---- batch: 020 ----
mean loss: 237.77
 ---- batch: 030 ----
mean loss: 245.66
 ---- batch: 040 ----
mean loss: 252.36
 ---- batch: 050 ----
mean loss: 243.83
 ---- batch: 060 ----
mean loss: 253.75
 ---- batch: 070 ----
mean loss: 232.83
 ---- batch: 080 ----
mean loss: 257.33
 ---- batch: 090 ----
mean loss: 245.70
 ---- batch: 100 ----
mean loss: 250.10
 ---- batch: 110 ----
mean loss: 247.18
train mean loss: 247.38
epoch train time: 0:00:15.312553
elapsed time: 0:16:37.060089
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 07:32:11.257473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.38
 ---- batch: 020 ----
mean loss: 239.20
 ---- batch: 030 ----
mean loss: 246.84
 ---- batch: 040 ----
mean loss: 246.95
 ---- batch: 050 ----
mean loss: 241.35
 ---- batch: 060 ----
mean loss: 244.63
 ---- batch: 070 ----
mean loss: 254.66
 ---- batch: 080 ----
mean loss: 240.46
 ---- batch: 090 ----
mean loss: 240.29
 ---- batch: 100 ----
mean loss: 242.19
 ---- batch: 110 ----
mean loss: 258.76
train mean loss: 245.07
epoch train time: 0:00:15.345923
elapsed time: 0:16:52.406941
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 07:32:26.604355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.37
 ---- batch: 020 ----
mean loss: 246.28
 ---- batch: 030 ----
mean loss: 241.42
 ---- batch: 040 ----
mean loss: 229.63
 ---- batch: 050 ----
mean loss: 242.83
 ---- batch: 060 ----
mean loss: 245.73
 ---- batch: 070 ----
mean loss: 249.21
 ---- batch: 080 ----
mean loss: 244.09
 ---- batch: 090 ----
mean loss: 249.98
 ---- batch: 100 ----
mean loss: 245.77
 ---- batch: 110 ----
mean loss: 238.43
train mean loss: 243.64
epoch train time: 0:00:15.319320
elapsed time: 0:17:07.727249
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 07:32:41.924726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.96
 ---- batch: 020 ----
mean loss: 233.03
 ---- batch: 030 ----
mean loss: 248.25
 ---- batch: 040 ----
mean loss: 245.25
 ---- batch: 050 ----
mean loss: 246.76
 ---- batch: 060 ----
mean loss: 238.89
 ---- batch: 070 ----
mean loss: 249.11
 ---- batch: 080 ----
mean loss: 240.26
 ---- batch: 090 ----
mean loss: 233.40
 ---- batch: 100 ----
mean loss: 237.58
 ---- batch: 110 ----
mean loss: 240.49
train mean loss: 241.98
epoch train time: 0:00:15.361220
elapsed time: 0:17:23.089473
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 07:32:57.286877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.35
 ---- batch: 020 ----
mean loss: 240.33
 ---- batch: 030 ----
mean loss: 240.26
 ---- batch: 040 ----
mean loss: 238.13
 ---- batch: 050 ----
mean loss: 233.90
 ---- batch: 060 ----
mean loss: 248.15
 ---- batch: 070 ----
mean loss: 235.86
 ---- batch: 080 ----
mean loss: 246.75
 ---- batch: 090 ----
mean loss: 238.22
 ---- batch: 100 ----
mean loss: 240.55
 ---- batch: 110 ----
mean loss: 237.10
train mean loss: 240.43
epoch train time: 0:00:15.356339
elapsed time: 0:17:38.446784
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 07:33:12.644213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.88
 ---- batch: 020 ----
mean loss: 236.87
 ---- batch: 030 ----
mean loss: 232.93
 ---- batch: 040 ----
mean loss: 239.08
 ---- batch: 050 ----
mean loss: 245.83
 ---- batch: 060 ----
mean loss: 227.25
 ---- batch: 070 ----
mean loss: 240.29
 ---- batch: 080 ----
mean loss: 232.79
 ---- batch: 090 ----
mean loss: 242.65
 ---- batch: 100 ----
mean loss: 234.80
 ---- batch: 110 ----
mean loss: 241.50
train mean loss: 237.37
epoch train time: 0:00:15.427962
elapsed time: 0:17:53.875770
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 07:33:28.073246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.86
 ---- batch: 020 ----
mean loss: 240.48
 ---- batch: 030 ----
mean loss: 229.56
 ---- batch: 040 ----
mean loss: 241.15
 ---- batch: 050 ----
mean loss: 239.23
 ---- batch: 060 ----
mean loss: 244.90
 ---- batch: 070 ----
mean loss: 233.90
 ---- batch: 080 ----
mean loss: 227.63
 ---- batch: 090 ----
mean loss: 239.50
 ---- batch: 100 ----
mean loss: 225.60
 ---- batch: 110 ----
mean loss: 238.97
train mean loss: 235.82
epoch train time: 0:00:15.362916
elapsed time: 0:18:09.239676
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 07:33:43.437092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.48
 ---- batch: 020 ----
mean loss: 234.49
 ---- batch: 030 ----
mean loss: 242.83
 ---- batch: 040 ----
mean loss: 224.37
 ---- batch: 050 ----
mean loss: 235.30
 ---- batch: 060 ----
mean loss: 246.04
 ---- batch: 070 ----
mean loss: 233.43
 ---- batch: 080 ----
mean loss: 230.27
 ---- batch: 090 ----
mean loss: 234.01
 ---- batch: 100 ----
mean loss: 228.20
 ---- batch: 110 ----
mean loss: 235.95
train mean loss: 235.41
epoch train time: 0:00:15.346453
elapsed time: 0:18:24.587094
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 07:33:58.784486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.09
 ---- batch: 020 ----
mean loss: 235.08
 ---- batch: 030 ----
mean loss: 241.36
 ---- batch: 040 ----
mean loss: 235.93
 ---- batch: 050 ----
mean loss: 227.37
 ---- batch: 060 ----
mean loss: 229.44
 ---- batch: 070 ----
mean loss: 234.61
 ---- batch: 080 ----
mean loss: 231.96
 ---- batch: 090 ----
mean loss: 237.74
 ---- batch: 100 ----
mean loss: 233.88
 ---- batch: 110 ----
mean loss: 232.95
train mean loss: 233.70
epoch train time: 0:00:15.287045
elapsed time: 0:18:39.875043
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 07:34:14.072418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.30
 ---- batch: 020 ----
mean loss: 227.72
 ---- batch: 030 ----
mean loss: 239.08
 ---- batch: 040 ----
mean loss: 230.30
 ---- batch: 050 ----
mean loss: 215.11
 ---- batch: 060 ----
mean loss: 232.04
 ---- batch: 070 ----
mean loss: 232.24
 ---- batch: 080 ----
mean loss: 239.84
 ---- batch: 090 ----
mean loss: 227.59
 ---- batch: 100 ----
mean loss: 230.59
 ---- batch: 110 ----
mean loss: 238.51
train mean loss: 231.70
epoch train time: 0:00:15.278461
elapsed time: 0:18:55.154296
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 07:34:29.351731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.11
 ---- batch: 020 ----
mean loss: 232.27
 ---- batch: 030 ----
mean loss: 231.06
 ---- batch: 040 ----
mean loss: 233.47
 ---- batch: 050 ----
mean loss: 235.51
 ---- batch: 060 ----
mean loss: 227.58
 ---- batch: 070 ----
mean loss: 221.70
 ---- batch: 080 ----
mean loss: 224.76
 ---- batch: 090 ----
mean loss: 226.69
 ---- batch: 100 ----
mean loss: 237.09
 ---- batch: 110 ----
mean loss: 224.69
train mean loss: 229.09
epoch train time: 0:00:15.294274
elapsed time: 0:19:10.449586
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 07:34:44.647011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.73
 ---- batch: 020 ----
mean loss: 236.70
 ---- batch: 030 ----
mean loss: 227.01
 ---- batch: 040 ----
mean loss: 225.09
 ---- batch: 050 ----
mean loss: 223.20
 ---- batch: 060 ----
mean loss: 233.61
 ---- batch: 070 ----
mean loss: 223.78
 ---- batch: 080 ----
mean loss: 235.36
 ---- batch: 090 ----
mean loss: 228.51
 ---- batch: 100 ----
mean loss: 228.02
 ---- batch: 110 ----
mean loss: 221.40
train mean loss: 228.61
epoch train time: 0:00:15.228194
elapsed time: 0:19:25.678757
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 07:34:59.876147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.65
 ---- batch: 020 ----
mean loss: 224.98
 ---- batch: 030 ----
mean loss: 225.83
 ---- batch: 040 ----
mean loss: 226.46
 ---- batch: 050 ----
mean loss: 221.92
 ---- batch: 060 ----
mean loss: 230.01
 ---- batch: 070 ----
mean loss: 230.63
 ---- batch: 080 ----
mean loss: 232.31
 ---- batch: 090 ----
mean loss: 226.78
 ---- batch: 100 ----
mean loss: 225.54
 ---- batch: 110 ----
mean loss: 224.86
train mean loss: 227.64
epoch train time: 0:00:15.319616
elapsed time: 0:19:40.999350
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 07:35:15.196772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.18
 ---- batch: 020 ----
mean loss: 218.78
 ---- batch: 030 ----
mean loss: 232.65
 ---- batch: 040 ----
mean loss: 226.05
 ---- batch: 050 ----
mean loss: 233.70
 ---- batch: 060 ----
mean loss: 219.82
 ---- batch: 070 ----
mean loss: 221.62
 ---- batch: 080 ----
mean loss: 225.64
 ---- batch: 090 ----
mean loss: 225.83
 ---- batch: 100 ----
mean loss: 223.13
 ---- batch: 110 ----
mean loss: 226.50
train mean loss: 225.36
epoch train time: 0:00:15.333963
elapsed time: 0:19:56.334312
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 07:35:30.531689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.81
 ---- batch: 020 ----
mean loss: 221.56
 ---- batch: 030 ----
mean loss: 228.30
 ---- batch: 040 ----
mean loss: 227.09
 ---- batch: 050 ----
mean loss: 225.13
 ---- batch: 060 ----
mean loss: 226.22
 ---- batch: 070 ----
mean loss: 227.96
 ---- batch: 080 ----
mean loss: 219.97
 ---- batch: 090 ----
mean loss: 216.90
 ---- batch: 100 ----
mean loss: 213.60
 ---- batch: 110 ----
mean loss: 227.40
train mean loss: 223.44
epoch train time: 0:00:15.307538
elapsed time: 0:20:11.642662
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 07:35:45.840014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.39
 ---- batch: 020 ----
mean loss: 229.05
 ---- batch: 030 ----
mean loss: 218.94
 ---- batch: 040 ----
mean loss: 217.46
 ---- batch: 050 ----
mean loss: 227.31
 ---- batch: 060 ----
mean loss: 224.46
 ---- batch: 070 ----
mean loss: 218.70
 ---- batch: 080 ----
mean loss: 223.42
 ---- batch: 090 ----
mean loss: 231.76
 ---- batch: 100 ----
mean loss: 230.51
 ---- batch: 110 ----
mean loss: 216.91
train mean loss: 223.43
epoch train time: 0:00:15.234425
elapsed time: 0:20:26.877948
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 07:36:01.075408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.78
 ---- batch: 020 ----
mean loss: 217.27
 ---- batch: 030 ----
mean loss: 211.52
 ---- batch: 040 ----
mean loss: 223.31
 ---- batch: 050 ----
mean loss: 230.06
 ---- batch: 060 ----
mean loss: 231.97
 ---- batch: 070 ----
mean loss: 230.04
 ---- batch: 080 ----
mean loss: 227.51
 ---- batch: 090 ----
mean loss: 219.27
 ---- batch: 100 ----
mean loss: 216.52
 ---- batch: 110 ----
mean loss: 221.09
train mean loss: 222.03
epoch train time: 0:00:15.225220
elapsed time: 0:20:42.104116
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 07:36:16.301481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.00
 ---- batch: 020 ----
mean loss: 219.40
 ---- batch: 030 ----
mean loss: 212.83
 ---- batch: 040 ----
mean loss: 222.71
 ---- batch: 050 ----
mean loss: 220.83
 ---- batch: 060 ----
mean loss: 223.29
 ---- batch: 070 ----
mean loss: 227.41
 ---- batch: 080 ----
mean loss: 215.08
 ---- batch: 090 ----
mean loss: 228.61
 ---- batch: 100 ----
mean loss: 212.44
 ---- batch: 110 ----
mean loss: 231.04
train mean loss: 220.60
epoch train time: 0:00:15.327016
elapsed time: 0:20:57.432022
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 07:36:31.629395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.15
 ---- batch: 020 ----
mean loss: 223.66
 ---- batch: 030 ----
mean loss: 213.15
 ---- batch: 040 ----
mean loss: 216.19
 ---- batch: 050 ----
mean loss: 222.71
 ---- batch: 060 ----
mean loss: 220.63
 ---- batch: 070 ----
mean loss: 227.73
 ---- batch: 080 ----
mean loss: 210.30
 ---- batch: 090 ----
mean loss: 219.29
 ---- batch: 100 ----
mean loss: 218.49
 ---- batch: 110 ----
mean loss: 217.48
train mean loss: 218.37
epoch train time: 0:00:15.402859
elapsed time: 0:21:12.835936
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 07:36:47.033390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.64
 ---- batch: 020 ----
mean loss: 218.28
 ---- batch: 030 ----
mean loss: 211.16
 ---- batch: 040 ----
mean loss: 219.07
 ---- batch: 050 ----
mean loss: 218.92
 ---- batch: 060 ----
mean loss: 213.69
 ---- batch: 070 ----
mean loss: 213.44
 ---- batch: 080 ----
mean loss: 236.13
 ---- batch: 090 ----
mean loss: 220.05
 ---- batch: 100 ----
mean loss: 207.65
 ---- batch: 110 ----
mean loss: 222.24
train mean loss: 217.53
epoch train time: 0:00:15.461355
elapsed time: 0:21:28.298248
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 07:37:02.495839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.75
 ---- batch: 020 ----
mean loss: 218.37
 ---- batch: 030 ----
mean loss: 224.27
 ---- batch: 040 ----
mean loss: 219.31
 ---- batch: 050 ----
mean loss: 213.82
 ---- batch: 060 ----
mean loss: 217.93
 ---- batch: 070 ----
mean loss: 217.96
 ---- batch: 080 ----
mean loss: 212.72
 ---- batch: 090 ----
mean loss: 213.12
 ---- batch: 100 ----
mean loss: 211.66
 ---- batch: 110 ----
mean loss: 212.94
train mean loss: 215.76
epoch train time: 0:00:15.498453
elapsed time: 0:21:43.797913
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 07:37:17.995379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.39
 ---- batch: 020 ----
mean loss: 214.67
 ---- batch: 030 ----
mean loss: 214.49
 ---- batch: 040 ----
mean loss: 208.55
 ---- batch: 050 ----
mean loss: 216.57
 ---- batch: 060 ----
mean loss: 215.39
 ---- batch: 070 ----
mean loss: 211.60
 ---- batch: 080 ----
mean loss: 214.21
 ---- batch: 090 ----
mean loss: 219.13
 ---- batch: 100 ----
mean loss: 219.49
 ---- batch: 110 ----
mean loss: 212.31
train mean loss: 214.53
epoch train time: 0:00:15.453999
elapsed time: 0:21:59.252988
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 07:37:33.450380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.14
 ---- batch: 020 ----
mean loss: 216.87
 ---- batch: 030 ----
mean loss: 214.01
 ---- batch: 040 ----
mean loss: 212.02
 ---- batch: 050 ----
mean loss: 205.64
 ---- batch: 060 ----
mean loss: 214.75
 ---- batch: 070 ----
mean loss: 221.61
 ---- batch: 080 ----
mean loss: 220.93
 ---- batch: 090 ----
mean loss: 213.71
 ---- batch: 100 ----
mean loss: 214.69
 ---- batch: 110 ----
mean loss: 208.52
train mean loss: 214.24
epoch train time: 0:00:15.397742
elapsed time: 0:22:14.651656
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 07:37:48.849139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.14
 ---- batch: 020 ----
mean loss: 213.46
 ---- batch: 030 ----
mean loss: 199.46
 ---- batch: 040 ----
mean loss: 218.94
 ---- batch: 050 ----
mean loss: 209.68
 ---- batch: 060 ----
mean loss: 208.57
 ---- batch: 070 ----
mean loss: 213.55
 ---- batch: 080 ----
mean loss: 214.68
 ---- batch: 090 ----
mean loss: 213.82
 ---- batch: 100 ----
mean loss: 208.86
 ---- batch: 110 ----
mean loss: 212.24
train mean loss: 210.44
epoch train time: 0:00:15.386754
elapsed time: 0:22:30.039459
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 07:38:04.236880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.91
 ---- batch: 020 ----
mean loss: 223.55
 ---- batch: 030 ----
mean loss: 205.13
 ---- batch: 040 ----
mean loss: 205.88
 ---- batch: 050 ----
mean loss: 216.41
 ---- batch: 060 ----
mean loss: 212.53
 ---- batch: 070 ----
mean loss: 202.49
 ---- batch: 080 ----
mean loss: 207.11
 ---- batch: 090 ----
mean loss: 212.97
 ---- batch: 100 ----
mean loss: 211.93
 ---- batch: 110 ----
mean loss: 213.52
train mean loss: 210.52
epoch train time: 0:00:15.387734
elapsed time: 0:22:45.428233
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 07:38:19.625662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.88
 ---- batch: 020 ----
mean loss: 211.01
 ---- batch: 030 ----
mean loss: 209.11
 ---- batch: 040 ----
mean loss: 199.92
 ---- batch: 050 ----
mean loss: 207.17
 ---- batch: 060 ----
mean loss: 213.50
 ---- batch: 070 ----
mean loss: 213.44
 ---- batch: 080 ----
mean loss: 223.40
 ---- batch: 090 ----
mean loss: 207.16
 ---- batch: 100 ----
mean loss: 197.62
 ---- batch: 110 ----
mean loss: 213.00
train mean loss: 209.70
epoch train time: 0:00:15.526670
elapsed time: 0:23:00.955986
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 07:38:35.153399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.20
 ---- batch: 020 ----
mean loss: 203.07
 ---- batch: 030 ----
mean loss: 215.55
 ---- batch: 040 ----
mean loss: 211.35
 ---- batch: 050 ----
mean loss: 212.38
 ---- batch: 060 ----
mean loss: 208.04
 ---- batch: 070 ----
mean loss: 208.78
 ---- batch: 080 ----
mean loss: 210.96
 ---- batch: 090 ----
mean loss: 216.43
 ---- batch: 100 ----
mean loss: 212.54
 ---- batch: 110 ----
mean loss: 208.22
train mean loss: 209.71
epoch train time: 0:00:15.660186
elapsed time: 0:23:16.617240
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 07:38:50.814815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.94
 ---- batch: 020 ----
mean loss: 211.36
 ---- batch: 030 ----
mean loss: 207.29
 ---- batch: 040 ----
mean loss: 214.29
 ---- batch: 050 ----
mean loss: 200.81
 ---- batch: 060 ----
mean loss: 202.78
 ---- batch: 070 ----
mean loss: 208.42
 ---- batch: 080 ----
mean loss: 207.06
 ---- batch: 090 ----
mean loss: 204.99
 ---- batch: 100 ----
mean loss: 200.94
 ---- batch: 110 ----
mean loss: 205.94
train mean loss: 206.85
epoch train time: 0:00:15.352083
elapsed time: 0:23:31.970450
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 07:39:06.167820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.88
 ---- batch: 020 ----
mean loss: 205.42
 ---- batch: 030 ----
mean loss: 198.99
 ---- batch: 040 ----
mean loss: 206.14
 ---- batch: 050 ----
mean loss: 200.23
 ---- batch: 060 ----
mean loss: 212.52
 ---- batch: 070 ----
mean loss: 218.55
 ---- batch: 080 ----
mean loss: 210.09
 ---- batch: 090 ----
mean loss: 202.98
 ---- batch: 100 ----
mean loss: 208.29
 ---- batch: 110 ----
mean loss: 211.37
train mean loss: 207.11
epoch train time: 0:00:15.266417
elapsed time: 0:23:47.237765
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 07:39:21.435129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.54
 ---- batch: 020 ----
mean loss: 209.02
 ---- batch: 030 ----
mean loss: 209.00
 ---- batch: 040 ----
mean loss: 202.96
 ---- batch: 050 ----
mean loss: 201.13
 ---- batch: 060 ----
mean loss: 208.10
 ---- batch: 070 ----
mean loss: 205.81
 ---- batch: 080 ----
mean loss: 201.54
 ---- batch: 090 ----
mean loss: 205.30
 ---- batch: 100 ----
mean loss: 202.38
 ---- batch: 110 ----
mean loss: 208.28
train mean loss: 205.50
epoch train time: 0:00:15.315843
elapsed time: 0:24:02.554602
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 07:39:36.752063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.60
 ---- batch: 020 ----
mean loss: 209.06
 ---- batch: 030 ----
mean loss: 200.42
 ---- batch: 040 ----
mean loss: 212.56
 ---- batch: 050 ----
mean loss: 196.07
 ---- batch: 060 ----
mean loss: 198.95
 ---- batch: 070 ----
mean loss: 207.94
 ---- batch: 080 ----
mean loss: 207.86
 ---- batch: 090 ----
mean loss: 200.74
 ---- batch: 100 ----
mean loss: 199.93
 ---- batch: 110 ----
mean loss: 205.51
train mean loss: 203.88
epoch train time: 0:00:15.299330
elapsed time: 0:24:17.854988
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 07:39:52.052378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.43
 ---- batch: 020 ----
mean loss: 212.68
 ---- batch: 030 ----
mean loss: 206.96
 ---- batch: 040 ----
mean loss: 210.81
 ---- batch: 050 ----
mean loss: 206.03
 ---- batch: 060 ----
mean loss: 194.48
 ---- batch: 070 ----
mean loss: 202.23
 ---- batch: 080 ----
mean loss: 192.45
 ---- batch: 090 ----
mean loss: 209.28
 ---- batch: 100 ----
mean loss: 202.62
 ---- batch: 110 ----
mean loss: 204.16
train mean loss: 204.09
epoch train time: 0:00:15.269181
elapsed time: 0:24:33.125102
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 07:40:07.322508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.48
 ---- batch: 020 ----
mean loss: 210.94
 ---- batch: 030 ----
mean loss: 206.35
 ---- batch: 040 ----
mean loss: 197.97
 ---- batch: 050 ----
mean loss: 195.06
 ---- batch: 060 ----
mean loss: 208.93
 ---- batch: 070 ----
mean loss: 211.86
 ---- batch: 080 ----
mean loss: 205.41
 ---- batch: 090 ----
mean loss: 196.44
 ---- batch: 100 ----
mean loss: 202.75
 ---- batch: 110 ----
mean loss: 195.08
train mean loss: 202.85
epoch train time: 0:00:15.323132
elapsed time: 0:24:48.449213
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 07:40:22.646614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.35
 ---- batch: 020 ----
mean loss: 204.57
 ---- batch: 030 ----
mean loss: 207.96
 ---- batch: 040 ----
mean loss: 195.02
 ---- batch: 050 ----
mean loss: 204.33
 ---- batch: 060 ----
mean loss: 198.85
 ---- batch: 070 ----
mean loss: 204.90
 ---- batch: 080 ----
mean loss: 204.93
 ---- batch: 090 ----
mean loss: 200.14
 ---- batch: 100 ----
mean loss: 200.73
 ---- batch: 110 ----
mean loss: 196.20
train mean loss: 201.51
epoch train time: 0:00:15.307294
elapsed time: 0:25:03.757442
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 07:40:37.954786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.22
 ---- batch: 020 ----
mean loss: 200.90
 ---- batch: 030 ----
mean loss: 202.41
 ---- batch: 040 ----
mean loss: 210.43
 ---- batch: 050 ----
mean loss: 207.68
 ---- batch: 060 ----
mean loss: 194.35
 ---- batch: 070 ----
mean loss: 192.59
 ---- batch: 080 ----
mean loss: 195.39
 ---- batch: 090 ----
mean loss: 203.23
 ---- batch: 100 ----
mean loss: 201.06
 ---- batch: 110 ----
mean loss: 201.38
train mean loss: 201.54
epoch train time: 0:00:15.267398
elapsed time: 0:25:19.025680
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 07:40:53.223067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.17
 ---- batch: 020 ----
mean loss: 197.15
 ---- batch: 030 ----
mean loss: 185.24
 ---- batch: 040 ----
mean loss: 205.05
 ---- batch: 050 ----
mean loss: 208.56
 ---- batch: 060 ----
mean loss: 209.02
 ---- batch: 070 ----
mean loss: 203.97
 ---- batch: 080 ----
mean loss: 200.10
 ---- batch: 090 ----
mean loss: 197.10
 ---- batch: 100 ----
mean loss: 199.34
 ---- batch: 110 ----
mean loss: 203.76
train mean loss: 200.67
epoch train time: 0:00:15.334569
elapsed time: 0:25:34.361187
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 07:41:08.558580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.85
 ---- batch: 020 ----
mean loss: 198.27
 ---- batch: 030 ----
mean loss: 199.09
 ---- batch: 040 ----
mean loss: 204.07
 ---- batch: 050 ----
mean loss: 208.64
 ---- batch: 060 ----
mean loss: 193.31
 ---- batch: 070 ----
mean loss: 196.43
 ---- batch: 080 ----
mean loss: 207.32
 ---- batch: 090 ----
mean loss: 205.19
 ---- batch: 100 ----
mean loss: 205.52
 ---- batch: 110 ----
mean loss: 199.60
train mean loss: 200.90
epoch train time: 0:00:15.336309
elapsed time: 0:25:49.698553
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 07:41:23.895937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.82
 ---- batch: 020 ----
mean loss: 201.02
 ---- batch: 030 ----
mean loss: 199.82
 ---- batch: 040 ----
mean loss: 193.66
 ---- batch: 050 ----
mean loss: 202.47
 ---- batch: 060 ----
mean loss: 203.75
 ---- batch: 070 ----
mean loss: 198.07
 ---- batch: 080 ----
mean loss: 206.09
 ---- batch: 090 ----
mean loss: 205.34
 ---- batch: 100 ----
mean loss: 202.49
 ---- batch: 110 ----
mean loss: 195.47
train mean loss: 200.31
epoch train time: 0:00:15.347897
elapsed time: 0:26:05.047417
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 07:41:39.244776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.94
 ---- batch: 020 ----
mean loss: 201.59
 ---- batch: 030 ----
mean loss: 187.41
 ---- batch: 040 ----
mean loss: 200.94
 ---- batch: 050 ----
mean loss: 195.32
 ---- batch: 060 ----
mean loss: 202.17
 ---- batch: 070 ----
mean loss: 192.97
 ---- batch: 080 ----
mean loss: 186.82
 ---- batch: 090 ----
mean loss: 198.81
 ---- batch: 100 ----
mean loss: 198.64
 ---- batch: 110 ----
mean loss: 203.81
train mean loss: 197.46
epoch train time: 0:00:15.351848
elapsed time: 0:26:20.400157
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 07:41:54.597520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.12
 ---- batch: 020 ----
mean loss: 196.01
 ---- batch: 030 ----
mean loss: 206.30
 ---- batch: 040 ----
mean loss: 193.39
 ---- batch: 050 ----
mean loss: 196.02
 ---- batch: 060 ----
mean loss: 200.03
 ---- batch: 070 ----
mean loss: 195.46
 ---- batch: 080 ----
mean loss: 196.95
 ---- batch: 090 ----
mean loss: 193.22
 ---- batch: 100 ----
mean loss: 201.94
 ---- batch: 110 ----
mean loss: 198.64
train mean loss: 196.87
epoch train time: 0:00:15.322929
elapsed time: 0:26:35.724086
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 07:42:09.921494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.39
 ---- batch: 020 ----
mean loss: 200.61
 ---- batch: 030 ----
mean loss: 199.54
 ---- batch: 040 ----
mean loss: 188.89
 ---- batch: 050 ----
mean loss: 199.87
 ---- batch: 060 ----
mean loss: 191.96
 ---- batch: 070 ----
mean loss: 196.70
 ---- batch: 080 ----
mean loss: 198.45
 ---- batch: 090 ----
mean loss: 194.37
 ---- batch: 100 ----
mean loss: 190.63
 ---- batch: 110 ----
mean loss: 198.70
train mean loss: 196.78
epoch train time: 0:00:15.376277
elapsed time: 0:26:51.101310
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 07:42:25.298733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.90
 ---- batch: 020 ----
mean loss: 200.59
 ---- batch: 030 ----
mean loss: 197.53
 ---- batch: 040 ----
mean loss: 196.69
 ---- batch: 050 ----
mean loss: 197.06
 ---- batch: 060 ----
mean loss: 202.56
 ---- batch: 070 ----
mean loss: 189.74
 ---- batch: 080 ----
mean loss: 202.10
 ---- batch: 090 ----
mean loss: 192.30
 ---- batch: 100 ----
mean loss: 193.66
 ---- batch: 110 ----
mean loss: 190.77
train mean loss: 196.74
epoch train time: 0:00:15.447217
elapsed time: 0:27:06.549474
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 07:42:40.746881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.18
 ---- batch: 020 ----
mean loss: 193.33
 ---- batch: 030 ----
mean loss: 200.70
 ---- batch: 040 ----
mean loss: 194.15
 ---- batch: 050 ----
mean loss: 187.97
 ---- batch: 060 ----
mean loss: 201.19
 ---- batch: 070 ----
mean loss: 193.28
 ---- batch: 080 ----
mean loss: 191.98
 ---- batch: 090 ----
mean loss: 189.36
 ---- batch: 100 ----
mean loss: 196.99
 ---- batch: 110 ----
mean loss: 202.45
train mean loss: 195.52
epoch train time: 0:00:15.307401
elapsed time: 0:27:21.857690
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 07:42:56.055101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.39
 ---- batch: 020 ----
mean loss: 196.25
 ---- batch: 030 ----
mean loss: 196.65
 ---- batch: 040 ----
mean loss: 191.85
 ---- batch: 050 ----
mean loss: 195.83
 ---- batch: 060 ----
mean loss: 192.08
 ---- batch: 070 ----
mean loss: 195.22
 ---- batch: 080 ----
mean loss: 191.39
 ---- batch: 090 ----
mean loss: 193.78
 ---- batch: 100 ----
mean loss: 202.30
 ---- batch: 110 ----
mean loss: 207.01
train mean loss: 195.12
epoch train time: 0:00:15.352374
elapsed time: 0:27:37.210976
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 07:43:11.408490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.91
 ---- batch: 020 ----
mean loss: 192.18
 ---- batch: 030 ----
mean loss: 199.92
 ---- batch: 040 ----
mean loss: 193.54
 ---- batch: 050 ----
mean loss: 188.17
 ---- batch: 060 ----
mean loss: 199.85
 ---- batch: 070 ----
mean loss: 197.07
 ---- batch: 080 ----
mean loss: 190.99
 ---- batch: 090 ----
mean loss: 193.51
 ---- batch: 100 ----
mean loss: 198.12
 ---- batch: 110 ----
mean loss: 192.08
train mean loss: 195.07
epoch train time: 0:00:15.335874
elapsed time: 0:27:52.548170
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 07:43:26.745505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.61
 ---- batch: 020 ----
mean loss: 205.98
 ---- batch: 030 ----
mean loss: 189.75
 ---- batch: 040 ----
mean loss: 198.54
 ---- batch: 050 ----
mean loss: 198.91
 ---- batch: 060 ----
mean loss: 192.09
 ---- batch: 070 ----
mean loss: 187.42
 ---- batch: 080 ----
mean loss: 202.66
 ---- batch: 090 ----
mean loss: 205.35
 ---- batch: 100 ----
mean loss: 187.88
 ---- batch: 110 ----
mean loss: 197.63
train mean loss: 195.82
epoch train time: 0:00:15.331245
elapsed time: 0:28:07.880339
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 07:43:42.077879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.25
 ---- batch: 020 ----
mean loss: 198.74
 ---- batch: 030 ----
mean loss: 195.20
 ---- batch: 040 ----
mean loss: 191.17
 ---- batch: 050 ----
mean loss: 201.79
 ---- batch: 060 ----
mean loss: 183.82
 ---- batch: 070 ----
mean loss: 198.33
 ---- batch: 080 ----
mean loss: 196.46
 ---- batch: 090 ----
mean loss: 193.87
 ---- batch: 100 ----
mean loss: 178.77
 ---- batch: 110 ----
mean loss: 191.54
train mean loss: 193.60
epoch train time: 0:00:15.501397
elapsed time: 0:28:23.382885
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 07:43:57.580281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.63
 ---- batch: 020 ----
mean loss: 199.46
 ---- batch: 030 ----
mean loss: 182.71
 ---- batch: 040 ----
mean loss: 189.93
 ---- batch: 050 ----
mean loss: 195.28
 ---- batch: 060 ----
mean loss: 192.08
 ---- batch: 070 ----
mean loss: 192.97
 ---- batch: 080 ----
mean loss: 194.63
 ---- batch: 090 ----
mean loss: 198.50
 ---- batch: 100 ----
mean loss: 196.85
 ---- batch: 110 ----
mean loss: 194.28
train mean loss: 193.24
epoch train time: 0:00:15.517707
elapsed time: 0:28:38.901515
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 07:44:13.098916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.96
 ---- batch: 020 ----
mean loss: 185.22
 ---- batch: 030 ----
mean loss: 194.70
 ---- batch: 040 ----
mean loss: 190.31
 ---- batch: 050 ----
mean loss: 189.45
 ---- batch: 060 ----
mean loss: 192.21
 ---- batch: 070 ----
mean loss: 201.48
 ---- batch: 080 ----
mean loss: 195.99
 ---- batch: 090 ----
mean loss: 184.18
 ---- batch: 100 ----
mean loss: 191.42
 ---- batch: 110 ----
mean loss: 195.56
train mean loss: 191.90
epoch train time: 0:00:15.488579
elapsed time: 0:28:54.391196
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 07:44:28.588665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.02
 ---- batch: 020 ----
mean loss: 192.97
 ---- batch: 030 ----
mean loss: 190.19
 ---- batch: 040 ----
mean loss: 181.56
 ---- batch: 050 ----
mean loss: 197.95
 ---- batch: 060 ----
mean loss: 189.35
 ---- batch: 070 ----
mean loss: 191.24
 ---- batch: 080 ----
mean loss: 199.07
 ---- batch: 090 ----
mean loss: 187.41
 ---- batch: 100 ----
mean loss: 187.82
 ---- batch: 110 ----
mean loss: 198.49
train mean loss: 192.11
epoch train time: 0:00:15.529850
elapsed time: 0:29:09.922157
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 07:44:44.119570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.69
 ---- batch: 020 ----
mean loss: 185.51
 ---- batch: 030 ----
mean loss: 190.57
 ---- batch: 040 ----
mean loss: 193.48
 ---- batch: 050 ----
mean loss: 188.16
 ---- batch: 060 ----
mean loss: 196.78
 ---- batch: 070 ----
mean loss: 195.49
 ---- batch: 080 ----
mean loss: 195.16
 ---- batch: 090 ----
mean loss: 197.69
 ---- batch: 100 ----
mean loss: 183.27
 ---- batch: 110 ----
mean loss: 191.98
train mean loss: 191.84
epoch train time: 0:00:15.493324
elapsed time: 0:29:25.416469
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 07:44:59.613869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.71
 ---- batch: 020 ----
mean loss: 182.57
 ---- batch: 030 ----
mean loss: 192.67
 ---- batch: 040 ----
mean loss: 182.90
 ---- batch: 050 ----
mean loss: 191.26
 ---- batch: 060 ----
mean loss: 194.61
 ---- batch: 070 ----
mean loss: 185.45
 ---- batch: 080 ----
mean loss: 195.62
 ---- batch: 090 ----
mean loss: 193.01
 ---- batch: 100 ----
mean loss: 190.80
 ---- batch: 110 ----
mean loss: 199.75
train mean loss: 191.12
epoch train time: 0:00:15.408865
elapsed time: 0:29:40.826250
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 07:45:15.023870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.96
 ---- batch: 020 ----
mean loss: 201.94
 ---- batch: 030 ----
mean loss: 185.45
 ---- batch: 040 ----
mean loss: 191.06
 ---- batch: 050 ----
mean loss: 196.55
 ---- batch: 060 ----
mean loss: 204.41
 ---- batch: 070 ----
mean loss: 191.75
 ---- batch: 080 ----
mean loss: 182.19
 ---- batch: 090 ----
mean loss: 189.60
 ---- batch: 100 ----
mean loss: 185.67
 ---- batch: 110 ----
mean loss: 189.82
train mean loss: 191.59
epoch train time: 0:00:15.343592
elapsed time: 0:29:56.170989
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 07:45:30.368427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.68
 ---- batch: 020 ----
mean loss: 197.29
 ---- batch: 030 ----
mean loss: 186.79
 ---- batch: 040 ----
mean loss: 186.44
 ---- batch: 050 ----
mean loss: 192.15
 ---- batch: 060 ----
mean loss: 186.22
 ---- batch: 070 ----
mean loss: 187.90
 ---- batch: 080 ----
mean loss: 201.05
 ---- batch: 090 ----
mean loss: 193.90
 ---- batch: 100 ----
mean loss: 178.44
 ---- batch: 110 ----
mean loss: 184.69
train mean loss: 190.33
epoch train time: 0:00:15.313798
elapsed time: 0:30:11.485799
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 07:45:45.683176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.00
 ---- batch: 020 ----
mean loss: 188.12
 ---- batch: 030 ----
mean loss: 186.72
 ---- batch: 040 ----
mean loss: 187.98
 ---- batch: 050 ----
mean loss: 187.08
 ---- batch: 060 ----
mean loss: 191.68
 ---- batch: 070 ----
mean loss: 205.51
 ---- batch: 080 ----
mean loss: 198.53
 ---- batch: 090 ----
mean loss: 186.39
 ---- batch: 100 ----
mean loss: 197.00
 ---- batch: 110 ----
mean loss: 184.02
train mean loss: 190.75
epoch train time: 0:00:15.310019
elapsed time: 0:30:26.796854
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 07:46:00.994261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.24
 ---- batch: 020 ----
mean loss: 190.57
 ---- batch: 030 ----
mean loss: 187.89
 ---- batch: 040 ----
mean loss: 187.02
 ---- batch: 050 ----
mean loss: 187.44
 ---- batch: 060 ----
mean loss: 195.89
 ---- batch: 070 ----
mean loss: 193.86
 ---- batch: 080 ----
mean loss: 198.77
 ---- batch: 090 ----
mean loss: 184.84
 ---- batch: 100 ----
mean loss: 188.92
 ---- batch: 110 ----
mean loss: 189.46
train mean loss: 190.74
epoch train time: 0:00:15.329807
elapsed time: 0:30:42.127625
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 07:46:16.325012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.20
 ---- batch: 020 ----
mean loss: 186.64
 ---- batch: 030 ----
mean loss: 187.32
 ---- batch: 040 ----
mean loss: 184.99
 ---- batch: 050 ----
mean loss: 188.40
 ---- batch: 060 ----
mean loss: 188.53
 ---- batch: 070 ----
mean loss: 186.13
 ---- batch: 080 ----
mean loss: 195.69
 ---- batch: 090 ----
mean loss: 197.99
 ---- batch: 100 ----
mean loss: 193.94
 ---- batch: 110 ----
mean loss: 187.85
train mean loss: 189.44
epoch train time: 0:00:15.266253
elapsed time: 0:30:57.394771
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 07:46:31.592149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.60
 ---- batch: 020 ----
mean loss: 192.37
 ---- batch: 030 ----
mean loss: 186.36
 ---- batch: 040 ----
mean loss: 188.33
 ---- batch: 050 ----
mean loss: 191.76
 ---- batch: 060 ----
mean loss: 196.32
 ---- batch: 070 ----
mean loss: 184.97
 ---- batch: 080 ----
mean loss: 186.62
 ---- batch: 090 ----
mean loss: 192.66
 ---- batch: 100 ----
mean loss: 186.98
 ---- batch: 110 ----
mean loss: 189.39
train mean loss: 188.81
epoch train time: 0:00:15.265374
elapsed time: 0:31:12.660974
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 07:46:46.858404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.06
 ---- batch: 020 ----
mean loss: 177.06
 ---- batch: 030 ----
mean loss: 192.81
 ---- batch: 040 ----
mean loss: 189.21
 ---- batch: 050 ----
mean loss: 187.31
 ---- batch: 060 ----
mean loss: 180.12
 ---- batch: 070 ----
mean loss: 194.43
 ---- batch: 080 ----
mean loss: 180.87
 ---- batch: 090 ----
mean loss: 193.78
 ---- batch: 100 ----
mean loss: 192.73
 ---- batch: 110 ----
mean loss: 193.73
train mean loss: 188.10
epoch train time: 0:00:15.329353
elapsed time: 0:31:27.991309
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 07:47:02.188676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.45
 ---- batch: 020 ----
mean loss: 187.72
 ---- batch: 030 ----
mean loss: 186.35
 ---- batch: 040 ----
mean loss: 186.65
 ---- batch: 050 ----
mean loss: 180.47
 ---- batch: 060 ----
mean loss: 184.40
 ---- batch: 070 ----
mean loss: 191.19
 ---- batch: 080 ----
mean loss: 190.72
 ---- batch: 090 ----
mean loss: 185.81
 ---- batch: 100 ----
mean loss: 190.97
 ---- batch: 110 ----
mean loss: 188.56
train mean loss: 187.10
epoch train time: 0:00:15.284899
elapsed time: 0:31:43.277360
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 07:47:17.474817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.40
 ---- batch: 020 ----
mean loss: 194.71
 ---- batch: 030 ----
mean loss: 181.83
 ---- batch: 040 ----
mean loss: 191.76
 ---- batch: 050 ----
mean loss: 191.66
 ---- batch: 060 ----
mean loss: 185.58
 ---- batch: 070 ----
mean loss: 190.01
 ---- batch: 080 ----
mean loss: 190.99
 ---- batch: 090 ----
mean loss: 180.03
 ---- batch: 100 ----
mean loss: 189.99
 ---- batch: 110 ----
mean loss: 182.05
train mean loss: 188.32
epoch train time: 0:00:15.298174
elapsed time: 0:31:58.576507
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 07:47:32.773954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.78
 ---- batch: 020 ----
mean loss: 185.24
 ---- batch: 030 ----
mean loss: 184.51
 ---- batch: 040 ----
mean loss: 191.04
 ---- batch: 050 ----
mean loss: 186.35
 ---- batch: 060 ----
mean loss: 186.39
 ---- batch: 070 ----
mean loss: 185.44
 ---- batch: 080 ----
mean loss: 190.76
 ---- batch: 090 ----
mean loss: 195.11
 ---- batch: 100 ----
mean loss: 191.83
 ---- batch: 110 ----
mean loss: 180.24
train mean loss: 188.10
epoch train time: 0:00:15.324321
elapsed time: 0:32:13.901713
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 07:47:48.099086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.87
 ---- batch: 020 ----
mean loss: 176.99
 ---- batch: 030 ----
mean loss: 187.44
 ---- batch: 040 ----
mean loss: 191.16
 ---- batch: 050 ----
mean loss: 194.95
 ---- batch: 060 ----
mean loss: 187.67
 ---- batch: 070 ----
mean loss: 194.02
 ---- batch: 080 ----
mean loss: 181.42
 ---- batch: 090 ----
mean loss: 188.54
 ---- batch: 100 ----
mean loss: 181.49
 ---- batch: 110 ----
mean loss: 187.71
train mean loss: 187.18
epoch train time: 0:00:15.334635
elapsed time: 0:32:29.237235
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 07:48:03.434607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.07
 ---- batch: 020 ----
mean loss: 178.65
 ---- batch: 030 ----
mean loss: 186.00
 ---- batch: 040 ----
mean loss: 195.77
 ---- batch: 050 ----
mean loss: 181.68
 ---- batch: 060 ----
mean loss: 192.12
 ---- batch: 070 ----
mean loss: 195.14
 ---- batch: 080 ----
mean loss: 192.02
 ---- batch: 090 ----
mean loss: 179.73
 ---- batch: 100 ----
mean loss: 182.79
 ---- batch: 110 ----
mean loss: 188.08
train mean loss: 187.20
epoch train time: 0:00:15.269536
elapsed time: 0:32:44.507713
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 07:48:18.705339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.82
 ---- batch: 020 ----
mean loss: 182.38
 ---- batch: 030 ----
mean loss: 186.95
 ---- batch: 040 ----
mean loss: 191.10
 ---- batch: 050 ----
mean loss: 186.51
 ---- batch: 060 ----
mean loss: 189.35
 ---- batch: 070 ----
mean loss: 185.52
 ---- batch: 080 ----
mean loss: 195.00
 ---- batch: 090 ----
mean loss: 188.27
 ---- batch: 100 ----
mean loss: 180.11
 ---- batch: 110 ----
mean loss: 184.03
train mean loss: 186.97
epoch train time: 0:00:15.279816
elapsed time: 0:32:59.788920
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 07:48:33.986381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.92
 ---- batch: 020 ----
mean loss: 189.73
 ---- batch: 030 ----
mean loss: 190.12
 ---- batch: 040 ----
mean loss: 171.19
 ---- batch: 050 ----
mean loss: 198.30
 ---- batch: 060 ----
mean loss: 184.51
 ---- batch: 070 ----
mean loss: 188.77
 ---- batch: 080 ----
mean loss: 186.32
 ---- batch: 090 ----
mean loss: 191.07
 ---- batch: 100 ----
mean loss: 180.62
 ---- batch: 110 ----
mean loss: 186.62
train mean loss: 187.08
epoch train time: 0:00:15.340691
elapsed time: 0:33:15.130673
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 07:48:49.328162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.53
 ---- batch: 020 ----
mean loss: 179.44
 ---- batch: 030 ----
mean loss: 185.78
 ---- batch: 040 ----
mean loss: 185.54
 ---- batch: 050 ----
mean loss: 191.93
 ---- batch: 060 ----
mean loss: 188.28
 ---- batch: 070 ----
mean loss: 187.21
 ---- batch: 080 ----
mean loss: 188.71
 ---- batch: 090 ----
mean loss: 176.63
 ---- batch: 100 ----
mean loss: 192.83
 ---- batch: 110 ----
mean loss: 178.59
train mean loss: 186.16
epoch train time: 0:00:15.553258
elapsed time: 0:33:30.685036
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 07:49:04.882470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.56
 ---- batch: 020 ----
mean loss: 196.43
 ---- batch: 030 ----
mean loss: 189.10
 ---- batch: 040 ----
mean loss: 188.93
 ---- batch: 050 ----
mean loss: 190.23
 ---- batch: 060 ----
mean loss: 189.00
 ---- batch: 070 ----
mean loss: 179.87
 ---- batch: 080 ----
mean loss: 183.15
 ---- batch: 090 ----
mean loss: 180.68
 ---- batch: 100 ----
mean loss: 181.03
 ---- batch: 110 ----
mean loss: 188.12
train mean loss: 186.53
epoch train time: 0:00:15.622635
elapsed time: 0:33:46.308647
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 07:49:20.506032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.55
 ---- batch: 020 ----
mean loss: 185.37
 ---- batch: 030 ----
mean loss: 184.49
 ---- batch: 040 ----
mean loss: 191.28
 ---- batch: 050 ----
mean loss: 179.79
 ---- batch: 060 ----
mean loss: 182.16
 ---- batch: 070 ----
mean loss: 190.50
 ---- batch: 080 ----
mean loss: 188.08
 ---- batch: 090 ----
mean loss: 187.23
 ---- batch: 100 ----
mean loss: 185.53
 ---- batch: 110 ----
mean loss: 180.76
train mean loss: 184.95
epoch train time: 0:00:15.410441
elapsed time: 0:34:01.720085
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 07:49:35.917461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.98
 ---- batch: 020 ----
mean loss: 186.65
 ---- batch: 030 ----
mean loss: 186.13
 ---- batch: 040 ----
mean loss: 188.74
 ---- batch: 050 ----
mean loss: 184.64
 ---- batch: 060 ----
mean loss: 181.37
 ---- batch: 070 ----
mean loss: 185.29
 ---- batch: 080 ----
mean loss: 180.41
 ---- batch: 090 ----
mean loss: 179.88
 ---- batch: 100 ----
mean loss: 192.30
 ---- batch: 110 ----
mean loss: 186.84
train mean loss: 185.10
epoch train time: 0:00:15.300048
elapsed time: 0:34:17.021233
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 07:49:51.218655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.62
 ---- batch: 020 ----
mean loss: 183.83
 ---- batch: 030 ----
mean loss: 186.15
 ---- batch: 040 ----
mean loss: 187.65
 ---- batch: 050 ----
mean loss: 183.13
 ---- batch: 060 ----
mean loss: 187.61
 ---- batch: 070 ----
mean loss: 193.68
 ---- batch: 080 ----
mean loss: 177.14
 ---- batch: 090 ----
mean loss: 177.93
 ---- batch: 100 ----
mean loss: 181.20
 ---- batch: 110 ----
mean loss: 179.36
train mean loss: 184.08
epoch train time: 0:00:15.304462
elapsed time: 0:34:32.326712
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 07:50:06.524129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.97
 ---- batch: 020 ----
mean loss: 189.42
 ---- batch: 030 ----
mean loss: 189.85
 ---- batch: 040 ----
mean loss: 186.43
 ---- batch: 050 ----
mean loss: 192.49
 ---- batch: 060 ----
mean loss: 179.20
 ---- batch: 070 ----
mean loss: 179.18
 ---- batch: 080 ----
mean loss: 179.79
 ---- batch: 090 ----
mean loss: 193.08
 ---- batch: 100 ----
mean loss: 177.49
 ---- batch: 110 ----
mean loss: 189.54
train mean loss: 187.11
epoch train time: 0:00:15.329864
elapsed time: 0:34:47.657552
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 07:50:21.854945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.54
 ---- batch: 020 ----
mean loss: 194.33
 ---- batch: 030 ----
mean loss: 193.85
 ---- batch: 040 ----
mean loss: 185.13
 ---- batch: 050 ----
mean loss: 184.50
 ---- batch: 060 ----
mean loss: 185.30
 ---- batch: 070 ----
mean loss: 183.39
 ---- batch: 080 ----
mean loss: 178.71
 ---- batch: 090 ----
mean loss: 182.29
 ---- batch: 100 ----
mean loss: 188.16
 ---- batch: 110 ----
mean loss: 180.85
train mean loss: 185.04
epoch train time: 0:00:15.265333
elapsed time: 0:35:02.923823
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 07:50:37.121273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.36
 ---- batch: 020 ----
mean loss: 193.54
 ---- batch: 030 ----
mean loss: 184.17
 ---- batch: 040 ----
mean loss: 184.90
 ---- batch: 050 ----
mean loss: 176.56
 ---- batch: 060 ----
mean loss: 188.88
 ---- batch: 070 ----
mean loss: 182.17
 ---- batch: 080 ----
mean loss: 180.27
 ---- batch: 090 ----
mean loss: 185.90
 ---- batch: 100 ----
mean loss: 176.45
 ---- batch: 110 ----
mean loss: 186.15
train mean loss: 183.70
epoch train time: 0:00:15.313334
elapsed time: 0:35:18.238135
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 07:50:52.435533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.29
 ---- batch: 020 ----
mean loss: 185.13
 ---- batch: 030 ----
mean loss: 185.83
 ---- batch: 040 ----
mean loss: 180.37
 ---- batch: 050 ----
mean loss: 176.28
 ---- batch: 060 ----
mean loss: 181.71
 ---- batch: 070 ----
mean loss: 189.08
 ---- batch: 080 ----
mean loss: 188.51
 ---- batch: 090 ----
mean loss: 189.57
 ---- batch: 100 ----
mean loss: 179.62
 ---- batch: 110 ----
mean loss: 187.91
train mean loss: 185.39
epoch train time: 0:00:15.353658
elapsed time: 0:35:33.592750
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 07:51:07.790116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.39
 ---- batch: 020 ----
mean loss: 188.30
 ---- batch: 030 ----
mean loss: 183.45
 ---- batch: 040 ----
mean loss: 184.04
 ---- batch: 050 ----
mean loss: 182.32
 ---- batch: 060 ----
mean loss: 188.35
 ---- batch: 070 ----
mean loss: 180.70
 ---- batch: 080 ----
mean loss: 182.68
 ---- batch: 090 ----
mean loss: 170.92
 ---- batch: 100 ----
mean loss: 191.11
 ---- batch: 110 ----
mean loss: 191.28
train mean loss: 183.28
epoch train time: 0:00:15.324338
elapsed time: 0:35:48.917988
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 07:51:23.115415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.96
 ---- batch: 020 ----
mean loss: 194.29
 ---- batch: 030 ----
mean loss: 176.06
 ---- batch: 040 ----
mean loss: 195.06
 ---- batch: 050 ----
mean loss: 176.01
 ---- batch: 060 ----
mean loss: 188.01
 ---- batch: 070 ----
mean loss: 176.91
 ---- batch: 080 ----
mean loss: 184.51
 ---- batch: 090 ----
mean loss: 173.26
 ---- batch: 100 ----
mean loss: 187.13
 ---- batch: 110 ----
mean loss: 180.14
train mean loss: 184.04
epoch train time: 0:00:15.303908
elapsed time: 0:36:04.222862
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 07:51:38.420250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.18
 ---- batch: 020 ----
mean loss: 185.29
 ---- batch: 030 ----
mean loss: 184.42
 ---- batch: 040 ----
mean loss: 189.58
 ---- batch: 050 ----
mean loss: 189.65
 ---- batch: 060 ----
mean loss: 190.40
 ---- batch: 070 ----
mean loss: 175.19
 ---- batch: 080 ----
mean loss: 173.97
 ---- batch: 090 ----
mean loss: 183.07
 ---- batch: 100 ----
mean loss: 172.87
 ---- batch: 110 ----
mean loss: 186.24
train mean loss: 182.69
epoch train time: 0:00:15.343172
elapsed time: 0:36:19.566966
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 07:51:53.764355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.88
 ---- batch: 020 ----
mean loss: 182.00
 ---- batch: 030 ----
mean loss: 187.69
 ---- batch: 040 ----
mean loss: 191.91
 ---- batch: 050 ----
mean loss: 186.60
 ---- batch: 060 ----
mean loss: 175.44
 ---- batch: 070 ----
mean loss: 184.04
 ---- batch: 080 ----
mean loss: 182.80
 ---- batch: 090 ----
mean loss: 188.09
 ---- batch: 100 ----
mean loss: 185.96
 ---- batch: 110 ----
mean loss: 180.12
train mean loss: 183.82
epoch train time: 0:00:15.311786
elapsed time: 0:36:34.879678
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 07:52:09.077111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.00
 ---- batch: 020 ----
mean loss: 186.12
 ---- batch: 030 ----
mean loss: 178.80
 ---- batch: 040 ----
mean loss: 182.83
 ---- batch: 050 ----
mean loss: 181.76
 ---- batch: 060 ----
mean loss: 182.38
 ---- batch: 070 ----
mean loss: 182.37
 ---- batch: 080 ----
mean loss: 182.88
 ---- batch: 090 ----
mean loss: 185.05
 ---- batch: 100 ----
mean loss: 179.32
 ---- batch: 110 ----
mean loss: 178.21
train mean loss: 182.79
epoch train time: 0:00:15.269897
elapsed time: 0:36:50.150525
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 07:52:24.348015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.97
 ---- batch: 020 ----
mean loss: 191.46
 ---- batch: 030 ----
mean loss: 179.57
 ---- batch: 040 ----
mean loss: 184.30
 ---- batch: 050 ----
mean loss: 184.99
 ---- batch: 060 ----
mean loss: 188.73
 ---- batch: 070 ----
mean loss: 180.98
 ---- batch: 080 ----
mean loss: 179.19
 ---- batch: 090 ----
mean loss: 171.38
 ---- batch: 100 ----
mean loss: 188.56
 ---- batch: 110 ----
mean loss: 195.83
train mean loss: 184.06
epoch train time: 0:00:15.314163
elapsed time: 0:37:05.465746
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 07:52:39.663234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.58
 ---- batch: 020 ----
mean loss: 176.50
 ---- batch: 030 ----
mean loss: 177.80
 ---- batch: 040 ----
mean loss: 181.42
 ---- batch: 050 ----
mean loss: 185.82
 ---- batch: 060 ----
mean loss: 176.66
 ---- batch: 070 ----
mean loss: 184.43
 ---- batch: 080 ----
mean loss: 188.66
 ---- batch: 090 ----
mean loss: 190.61
 ---- batch: 100 ----
mean loss: 182.99
 ---- batch: 110 ----
mean loss: 173.17
train mean loss: 182.46
epoch train time: 0:00:15.310971
elapsed time: 0:37:20.777777
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 07:52:54.975192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.88
 ---- batch: 020 ----
mean loss: 179.24
 ---- batch: 030 ----
mean loss: 182.31
 ---- batch: 040 ----
mean loss: 184.70
 ---- batch: 050 ----
mean loss: 181.17
 ---- batch: 060 ----
mean loss: 179.60
 ---- batch: 070 ----
mean loss: 179.31
 ---- batch: 080 ----
mean loss: 186.64
 ---- batch: 090 ----
mean loss: 181.02
 ---- batch: 100 ----
mean loss: 184.24
 ---- batch: 110 ----
mean loss: 180.50
train mean loss: 181.88
epoch train time: 0:00:15.274945
elapsed time: 0:37:36.053723
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 07:53:10.251154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.32
 ---- batch: 020 ----
mean loss: 186.47
 ---- batch: 030 ----
mean loss: 187.44
 ---- batch: 040 ----
mean loss: 178.96
 ---- batch: 050 ----
mean loss: 187.07
 ---- batch: 060 ----
mean loss: 182.91
 ---- batch: 070 ----
mean loss: 181.97
 ---- batch: 080 ----
mean loss: 180.99
 ---- batch: 090 ----
mean loss: 177.77
 ---- batch: 100 ----
mean loss: 178.37
 ---- batch: 110 ----
mean loss: 182.27
train mean loss: 181.61
epoch train time: 0:00:15.375387
elapsed time: 0:37:51.430071
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 07:53:25.627491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.40
 ---- batch: 020 ----
mean loss: 186.68
 ---- batch: 030 ----
mean loss: 177.66
 ---- batch: 040 ----
mean loss: 182.88
 ---- batch: 050 ----
mean loss: 176.86
 ---- batch: 060 ----
mean loss: 180.11
 ---- batch: 070 ----
mean loss: 185.21
 ---- batch: 080 ----
mean loss: 180.34
 ---- batch: 090 ----
mean loss: 183.14
 ---- batch: 100 ----
mean loss: 178.55
 ---- batch: 110 ----
mean loss: 184.54
train mean loss: 182.67
epoch train time: 0:00:15.400163
elapsed time: 0:38:06.831258
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 07:53:41.028679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.97
 ---- batch: 020 ----
mean loss: 185.14
 ---- batch: 030 ----
mean loss: 177.88
 ---- batch: 040 ----
mean loss: 176.87
 ---- batch: 050 ----
mean loss: 186.64
 ---- batch: 060 ----
mean loss: 178.84
 ---- batch: 070 ----
mean loss: 180.45
 ---- batch: 080 ----
mean loss: 185.40
 ---- batch: 090 ----
mean loss: 187.80
 ---- batch: 100 ----
mean loss: 190.95
 ---- batch: 110 ----
mean loss: 174.18
train mean loss: 181.96
epoch train time: 0:00:15.362464
elapsed time: 0:38:22.194627
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 07:53:56.392265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.31
 ---- batch: 020 ----
mean loss: 184.23
 ---- batch: 030 ----
mean loss: 184.19
 ---- batch: 040 ----
mean loss: 180.73
 ---- batch: 050 ----
mean loss: 179.93
 ---- batch: 060 ----
mean loss: 179.49
 ---- batch: 070 ----
mean loss: 180.99
 ---- batch: 080 ----
mean loss: 185.96
 ---- batch: 090 ----
mean loss: 183.98
 ---- batch: 100 ----
mean loss: 185.54
 ---- batch: 110 ----
mean loss: 177.18
train mean loss: 181.62
epoch train time: 0:00:15.412738
elapsed time: 0:38:37.609313
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 07:54:11.806436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.98
 ---- batch: 020 ----
mean loss: 176.85
 ---- batch: 030 ----
mean loss: 175.82
 ---- batch: 040 ----
mean loss: 172.98
 ---- batch: 050 ----
mean loss: 176.55
 ---- batch: 060 ----
mean loss: 183.68
 ---- batch: 070 ----
mean loss: 187.27
 ---- batch: 080 ----
mean loss: 185.85
 ---- batch: 090 ----
mean loss: 183.66
 ---- batch: 100 ----
mean loss: 187.10
 ---- batch: 110 ----
mean loss: 177.04
train mean loss: 181.74
epoch train time: 0:00:15.441826
elapsed time: 0:38:53.051781
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 07:54:27.249241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.64
 ---- batch: 020 ----
mean loss: 181.71
 ---- batch: 030 ----
mean loss: 186.75
 ---- batch: 040 ----
mean loss: 179.57
 ---- batch: 050 ----
mean loss: 180.95
 ---- batch: 060 ----
mean loss: 188.61
 ---- batch: 070 ----
mean loss: 179.01
 ---- batch: 080 ----
mean loss: 188.74
 ---- batch: 090 ----
mean loss: 181.44
 ---- batch: 100 ----
mean loss: 173.54
 ---- batch: 110 ----
mean loss: 179.62
train mean loss: 181.89
epoch train time: 0:00:15.312027
elapsed time: 0:39:08.364789
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 07:54:42.562144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.68
 ---- batch: 020 ----
mean loss: 177.69
 ---- batch: 030 ----
mean loss: 190.47
 ---- batch: 040 ----
mean loss: 186.99
 ---- batch: 050 ----
mean loss: 175.51
 ---- batch: 060 ----
mean loss: 180.85
 ---- batch: 070 ----
mean loss: 182.67
 ---- batch: 080 ----
mean loss: 181.07
 ---- batch: 090 ----
mean loss: 178.57
 ---- batch: 100 ----
mean loss: 182.21
 ---- batch: 110 ----
mean loss: 173.91
train mean loss: 181.49
epoch train time: 0:00:15.339010
elapsed time: 0:39:23.704700
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 07:54:57.902111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.94
 ---- batch: 020 ----
mean loss: 179.64
 ---- batch: 030 ----
mean loss: 178.95
 ---- batch: 040 ----
mean loss: 178.46
 ---- batch: 050 ----
mean loss: 187.08
 ---- batch: 060 ----
mean loss: 182.41
 ---- batch: 070 ----
mean loss: 175.89
 ---- batch: 080 ----
mean loss: 185.37
 ---- batch: 090 ----
mean loss: 183.67
 ---- batch: 100 ----
mean loss: 178.40
 ---- batch: 110 ----
mean loss: 175.27
train mean loss: 181.49
epoch train time: 0:00:15.342890
elapsed time: 0:39:39.048547
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 07:55:13.245987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.88
 ---- batch: 020 ----
mean loss: 179.30
 ---- batch: 030 ----
mean loss: 181.60
 ---- batch: 040 ----
mean loss: 180.51
 ---- batch: 050 ----
mean loss: 182.65
 ---- batch: 060 ----
mean loss: 185.76
 ---- batch: 070 ----
mean loss: 185.36
 ---- batch: 080 ----
mean loss: 176.30
 ---- batch: 090 ----
mean loss: 173.36
 ---- batch: 100 ----
mean loss: 180.30
 ---- batch: 110 ----
mean loss: 186.51
train mean loss: 181.92
epoch train time: 0:00:15.377516
elapsed time: 0:39:54.427003
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 07:55:28.624446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.79
 ---- batch: 020 ----
mean loss: 176.19
 ---- batch: 030 ----
mean loss: 181.29
 ---- batch: 040 ----
mean loss: 178.97
 ---- batch: 050 ----
mean loss: 185.23
 ---- batch: 060 ----
mean loss: 179.49
 ---- batch: 070 ----
mean loss: 181.15
 ---- batch: 080 ----
mean loss: 173.20
 ---- batch: 090 ----
mean loss: 180.15
 ---- batch: 100 ----
mean loss: 177.83
 ---- batch: 110 ----
mean loss: 184.81
train mean loss: 179.65
epoch train time: 0:00:15.367528
elapsed time: 0:40:09.795512
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 07:55:43.992915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.11
 ---- batch: 020 ----
mean loss: 174.89
 ---- batch: 030 ----
mean loss: 174.65
 ---- batch: 040 ----
mean loss: 176.36
 ---- batch: 050 ----
mean loss: 175.42
 ---- batch: 060 ----
mean loss: 177.97
 ---- batch: 070 ----
mean loss: 189.48
 ---- batch: 080 ----
mean loss: 176.92
 ---- batch: 090 ----
mean loss: 184.00
 ---- batch: 100 ----
mean loss: 177.71
 ---- batch: 110 ----
mean loss: 190.26
train mean loss: 180.19
epoch train time: 0:00:15.320388
elapsed time: 0:40:25.116943
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 07:55:59.314390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.73
 ---- batch: 020 ----
mean loss: 175.47
 ---- batch: 030 ----
mean loss: 192.02
 ---- batch: 040 ----
mean loss: 189.02
 ---- batch: 050 ----
mean loss: 176.38
 ---- batch: 060 ----
mean loss: 176.65
 ---- batch: 070 ----
mean loss: 176.05
 ---- batch: 080 ----
mean loss: 176.88
 ---- batch: 090 ----
mean loss: 175.99
 ---- batch: 100 ----
mean loss: 181.56
 ---- batch: 110 ----
mean loss: 178.71
train mean loss: 180.30
epoch train time: 0:00:15.300974
elapsed time: 0:40:40.418926
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 07:56:14.616339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.46
 ---- batch: 020 ----
mean loss: 177.37
 ---- batch: 030 ----
mean loss: 181.43
 ---- batch: 040 ----
mean loss: 181.94
 ---- batch: 050 ----
mean loss: 171.19
 ---- batch: 060 ----
mean loss: 187.91
 ---- batch: 070 ----
mean loss: 170.12
 ---- batch: 080 ----
mean loss: 175.94
 ---- batch: 090 ----
mean loss: 188.07
 ---- batch: 100 ----
mean loss: 185.57
 ---- batch: 110 ----
mean loss: 184.34
train mean loss: 180.02
epoch train time: 0:00:15.300855
elapsed time: 0:40:55.720804
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 07:56:29.918183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.66
 ---- batch: 020 ----
mean loss: 175.78
 ---- batch: 030 ----
mean loss: 181.74
 ---- batch: 040 ----
mean loss: 173.85
 ---- batch: 050 ----
mean loss: 179.50
 ---- batch: 060 ----
mean loss: 177.53
 ---- batch: 070 ----
mean loss: 189.30
 ---- batch: 080 ----
mean loss: 180.61
 ---- batch: 090 ----
mean loss: 172.44
 ---- batch: 100 ----
mean loss: 184.58
 ---- batch: 110 ----
mean loss: 185.83
train mean loss: 180.64
epoch train time: 0:00:15.270518
elapsed time: 0:41:10.992120
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 07:56:45.189516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.39
 ---- batch: 020 ----
mean loss: 177.45
 ---- batch: 030 ----
mean loss: 176.88
 ---- batch: 040 ----
mean loss: 189.08
 ---- batch: 050 ----
mean loss: 179.67
 ---- batch: 060 ----
mean loss: 175.65
 ---- batch: 070 ----
mean loss: 183.07
 ---- batch: 080 ----
mean loss: 178.12
 ---- batch: 090 ----
mean loss: 173.65
 ---- batch: 100 ----
mean loss: 186.17
 ---- batch: 110 ----
mean loss: 177.12
train mean loss: 180.24
epoch train time: 0:00:15.378101
elapsed time: 0:41:26.371103
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 07:57:00.568533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.42
 ---- batch: 020 ----
mean loss: 180.01
 ---- batch: 030 ----
mean loss: 173.16
 ---- batch: 040 ----
mean loss: 185.83
 ---- batch: 050 ----
mean loss: 196.40
 ---- batch: 060 ----
mean loss: 172.48
 ---- batch: 070 ----
mean loss: 172.56
 ---- batch: 080 ----
mean loss: 184.67
 ---- batch: 090 ----
mean loss: 183.59
 ---- batch: 100 ----
mean loss: 167.14
 ---- batch: 110 ----
mean loss: 172.15
train mean loss: 180.03
epoch train time: 0:00:15.299763
elapsed time: 0:41:41.671718
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 07:57:15.869097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.76
 ---- batch: 020 ----
mean loss: 183.82
 ---- batch: 030 ----
mean loss: 179.06
 ---- batch: 040 ----
mean loss: 183.29
 ---- batch: 050 ----
mean loss: 189.08
 ---- batch: 060 ----
mean loss: 178.40
 ---- batch: 070 ----
mean loss: 180.00
 ---- batch: 080 ----
mean loss: 174.22
 ---- batch: 090 ----
mean loss: 181.34
 ---- batch: 100 ----
mean loss: 191.69
 ---- batch: 110 ----
mean loss: 176.30
train mean loss: 180.75
epoch train time: 0:00:15.308096
elapsed time: 0:41:56.980616
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 07:57:31.178030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.80
 ---- batch: 020 ----
mean loss: 184.02
 ---- batch: 030 ----
mean loss: 182.54
 ---- batch: 040 ----
mean loss: 171.02
 ---- batch: 050 ----
mean loss: 190.47
 ---- batch: 060 ----
mean loss: 181.22
 ---- batch: 070 ----
mean loss: 178.10
 ---- batch: 080 ----
mean loss: 163.53
 ---- batch: 090 ----
mean loss: 176.47
 ---- batch: 100 ----
mean loss: 187.18
 ---- batch: 110 ----
mean loss: 181.76
train mean loss: 180.10
epoch train time: 0:00:15.268773
elapsed time: 0:42:12.250313
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 07:57:46.447732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.40
 ---- batch: 020 ----
mean loss: 184.00
 ---- batch: 030 ----
mean loss: 186.45
 ---- batch: 040 ----
mean loss: 183.60
 ---- batch: 050 ----
mean loss: 181.19
 ---- batch: 060 ----
mean loss: 177.06
 ---- batch: 070 ----
mean loss: 173.88
 ---- batch: 080 ----
mean loss: 180.37
 ---- batch: 090 ----
mean loss: 172.40
 ---- batch: 100 ----
mean loss: 171.46
 ---- batch: 110 ----
mean loss: 181.02
train mean loss: 179.84
epoch train time: 0:00:15.291489
elapsed time: 0:42:27.542793
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 07:58:01.740166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.58
 ---- batch: 020 ----
mean loss: 186.01
 ---- batch: 030 ----
mean loss: 178.27
 ---- batch: 040 ----
mean loss: 182.99
 ---- batch: 050 ----
mean loss: 186.36
 ---- batch: 060 ----
mean loss: 175.39
 ---- batch: 070 ----
mean loss: 193.60
 ---- batch: 080 ----
mean loss: 173.63
 ---- batch: 090 ----
mean loss: 168.77
 ---- batch: 100 ----
mean loss: 182.59
 ---- batch: 110 ----
mean loss: 179.60
train mean loss: 180.06
epoch train time: 0:00:15.317803
elapsed time: 0:42:42.861501
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 07:58:17.058871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.77
 ---- batch: 020 ----
mean loss: 169.11
 ---- batch: 030 ----
mean loss: 181.37
 ---- batch: 040 ----
mean loss: 179.90
 ---- batch: 050 ----
mean loss: 189.72
 ---- batch: 060 ----
mean loss: 174.66
 ---- batch: 070 ----
mean loss: 175.05
 ---- batch: 080 ----
mean loss: 178.56
 ---- batch: 090 ----
mean loss: 181.97
 ---- batch: 100 ----
mean loss: 173.30
 ---- batch: 110 ----
mean loss: 166.34
train mean loss: 178.44
epoch train time: 0:00:15.295632
elapsed time: 0:42:58.158048
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 07:58:32.355430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.69
 ---- batch: 020 ----
mean loss: 179.55
 ---- batch: 030 ----
mean loss: 179.50
 ---- batch: 040 ----
mean loss: 176.59
 ---- batch: 050 ----
mean loss: 171.15
 ---- batch: 060 ----
mean loss: 176.62
 ---- batch: 070 ----
mean loss: 165.25
 ---- batch: 080 ----
mean loss: 187.63
 ---- batch: 090 ----
mean loss: 184.93
 ---- batch: 100 ----
mean loss: 194.99
 ---- batch: 110 ----
mean loss: 174.35
train mean loss: 179.13
epoch train time: 0:00:15.345496
elapsed time: 0:43:13.504383
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 07:58:47.701811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.55
 ---- batch: 020 ----
mean loss: 179.32
 ---- batch: 030 ----
mean loss: 171.81
 ---- batch: 040 ----
mean loss: 172.21
 ---- batch: 050 ----
mean loss: 184.37
 ---- batch: 060 ----
mean loss: 177.67
 ---- batch: 070 ----
mean loss: 177.86
 ---- batch: 080 ----
mean loss: 168.46
 ---- batch: 090 ----
mean loss: 188.32
 ---- batch: 100 ----
mean loss: 180.91
 ---- batch: 110 ----
mean loss: 190.18
train mean loss: 179.24
epoch train time: 0:00:15.316199
elapsed time: 0:43:28.821556
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 07:59:03.018970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.41
 ---- batch: 020 ----
mean loss: 183.41
 ---- batch: 030 ----
mean loss: 176.39
 ---- batch: 040 ----
mean loss: 185.97
 ---- batch: 050 ----
mean loss: 185.62
 ---- batch: 060 ----
mean loss: 176.84
 ---- batch: 070 ----
mean loss: 169.24
 ---- batch: 080 ----
mean loss: 179.63
 ---- batch: 090 ----
mean loss: 173.77
 ---- batch: 100 ----
mean loss: 174.54
 ---- batch: 110 ----
mean loss: 175.99
train mean loss: 178.69
epoch train time: 0:00:15.417344
elapsed time: 0:43:44.239746
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 07:59:18.437313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.52
 ---- batch: 020 ----
mean loss: 178.78
 ---- batch: 030 ----
mean loss: 168.03
 ---- batch: 040 ----
mean loss: 186.85
 ---- batch: 050 ----
mean loss: 185.15
 ---- batch: 060 ----
mean loss: 183.02
 ---- batch: 070 ----
mean loss: 177.07
 ---- batch: 080 ----
mean loss: 180.78
 ---- batch: 090 ----
mean loss: 177.62
 ---- batch: 100 ----
mean loss: 177.44
 ---- batch: 110 ----
mean loss: 168.46
train mean loss: 178.23
epoch train time: 0:00:15.437197
elapsed time: 0:43:59.677945
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 07:59:33.875377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.43
 ---- batch: 020 ----
mean loss: 185.93
 ---- batch: 030 ----
mean loss: 184.25
 ---- batch: 040 ----
mean loss: 178.35
 ---- batch: 050 ----
mean loss: 179.23
 ---- batch: 060 ----
mean loss: 185.74
 ---- batch: 070 ----
mean loss: 171.92
 ---- batch: 080 ----
mean loss: 180.70
 ---- batch: 090 ----
mean loss: 178.10
 ---- batch: 100 ----
mean loss: 170.36
 ---- batch: 110 ----
mean loss: 168.70
train mean loss: 178.61
epoch train time: 0:00:15.285149
elapsed time: 0:44:14.964025
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 07:59:49.161443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.65
 ---- batch: 020 ----
mean loss: 189.42
 ---- batch: 030 ----
mean loss: 185.91
 ---- batch: 040 ----
mean loss: 179.09
 ---- batch: 050 ----
mean loss: 174.26
 ---- batch: 060 ----
mean loss: 170.26
 ---- batch: 070 ----
mean loss: 185.57
 ---- batch: 080 ----
mean loss: 176.64
 ---- batch: 090 ----
mean loss: 176.44
 ---- batch: 100 ----
mean loss: 180.62
 ---- batch: 110 ----
mean loss: 178.69
train mean loss: 178.58
epoch train time: 0:00:15.341062
elapsed time: 0:44:30.306062
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 08:00:04.503455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.63
 ---- batch: 020 ----
mean loss: 181.63
 ---- batch: 030 ----
mean loss: 182.76
 ---- batch: 040 ----
mean loss: 172.17
 ---- batch: 050 ----
mean loss: 173.59
 ---- batch: 060 ----
mean loss: 181.74
 ---- batch: 070 ----
mean loss: 173.99
 ---- batch: 080 ----
mean loss: 179.97
 ---- batch: 090 ----
mean loss: 182.96
 ---- batch: 100 ----
mean loss: 182.34
 ---- batch: 110 ----
mean loss: 177.91
train mean loss: 178.76
epoch train time: 0:00:15.322303
elapsed time: 0:44:45.629322
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 08:00:19.826932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.58
 ---- batch: 020 ----
mean loss: 173.92
 ---- batch: 030 ----
mean loss: 183.29
 ---- batch: 040 ----
mean loss: 179.09
 ---- batch: 050 ----
mean loss: 167.34
 ---- batch: 060 ----
mean loss: 177.82
 ---- batch: 070 ----
mean loss: 185.66
 ---- batch: 080 ----
mean loss: 181.22
 ---- batch: 090 ----
mean loss: 191.38
 ---- batch: 100 ----
mean loss: 173.67
 ---- batch: 110 ----
mean loss: 185.80
train mean loss: 179.50
epoch train time: 0:00:15.316300
elapsed time: 0:45:00.947372
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 08:00:35.144488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.26
 ---- batch: 020 ----
mean loss: 185.69
 ---- batch: 030 ----
mean loss: 180.29
 ---- batch: 040 ----
mean loss: 178.04
 ---- batch: 050 ----
mean loss: 184.90
 ---- batch: 060 ----
mean loss: 177.55
 ---- batch: 070 ----
mean loss: 177.60
 ---- batch: 080 ----
mean loss: 173.01
 ---- batch: 090 ----
mean loss: 178.65
 ---- batch: 100 ----
mean loss: 181.35
 ---- batch: 110 ----
mean loss: 182.88
train mean loss: 179.43
epoch train time: 0:00:15.312602
elapsed time: 0:45:16.260680
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 08:00:50.458150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.07
 ---- batch: 020 ----
mean loss: 176.90
 ---- batch: 030 ----
mean loss: 177.01
 ---- batch: 040 ----
mean loss: 178.08
 ---- batch: 050 ----
mean loss: 175.71
 ---- batch: 060 ----
mean loss: 176.20
 ---- batch: 070 ----
mean loss: 182.01
 ---- batch: 080 ----
mean loss: 180.18
 ---- batch: 090 ----
mean loss: 181.39
 ---- batch: 100 ----
mean loss: 180.42
 ---- batch: 110 ----
mean loss: 169.54
train mean loss: 178.51
epoch train time: 0:00:15.356845
elapsed time: 0:45:31.618575
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 08:01:05.815971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.27
 ---- batch: 020 ----
mean loss: 178.96
 ---- batch: 030 ----
mean loss: 169.22
 ---- batch: 040 ----
mean loss: 190.63
 ---- batch: 050 ----
mean loss: 179.96
 ---- batch: 060 ----
mean loss: 178.65
 ---- batch: 070 ----
mean loss: 177.44
 ---- batch: 080 ----
mean loss: 180.90
 ---- batch: 090 ----
mean loss: 170.66
 ---- batch: 100 ----
mean loss: 176.70
 ---- batch: 110 ----
mean loss: 185.24
train mean loss: 179.03
epoch train time: 0:00:15.335465
elapsed time: 0:45:46.954949
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 08:01:21.152397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.67
 ---- batch: 020 ----
mean loss: 184.77
 ---- batch: 030 ----
mean loss: 180.32
 ---- batch: 040 ----
mean loss: 172.02
 ---- batch: 050 ----
mean loss: 177.06
 ---- batch: 060 ----
mean loss: 172.68
 ---- batch: 070 ----
mean loss: 172.14
 ---- batch: 080 ----
mean loss: 177.54
 ---- batch: 090 ----
mean loss: 183.08
 ---- batch: 100 ----
mean loss: 184.31
 ---- batch: 110 ----
mean loss: 167.58
train mean loss: 177.67
epoch train time: 0:00:15.290295
elapsed time: 0:46:02.246196
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 08:01:36.443627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.04
 ---- batch: 020 ----
mean loss: 191.03
 ---- batch: 030 ----
mean loss: 185.24
 ---- batch: 040 ----
mean loss: 173.47
 ---- batch: 050 ----
mean loss: 167.98
 ---- batch: 060 ----
mean loss: 180.19
 ---- batch: 070 ----
mean loss: 172.56
 ---- batch: 080 ----
mean loss: 171.97
 ---- batch: 090 ----
mean loss: 181.14
 ---- batch: 100 ----
mean loss: 172.12
 ---- batch: 110 ----
mean loss: 179.62
train mean loss: 177.91
epoch train time: 0:00:15.299542
elapsed time: 0:46:17.546867
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 08:01:51.744296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.47
 ---- batch: 020 ----
mean loss: 170.45
 ---- batch: 030 ----
mean loss: 176.37
 ---- batch: 040 ----
mean loss: 179.93
 ---- batch: 050 ----
mean loss: 180.34
 ---- batch: 060 ----
mean loss: 171.46
 ---- batch: 070 ----
mean loss: 185.01
 ---- batch: 080 ----
mean loss: 177.69
 ---- batch: 090 ----
mean loss: 175.60
 ---- batch: 100 ----
mean loss: 175.41
 ---- batch: 110 ----
mean loss: 184.46
train mean loss: 176.95
epoch train time: 0:00:15.337020
elapsed time: 0:46:32.884884
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 08:02:07.082242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.49
 ---- batch: 020 ----
mean loss: 182.21
 ---- batch: 030 ----
mean loss: 173.79
 ---- batch: 040 ----
mean loss: 170.15
 ---- batch: 050 ----
mean loss: 188.23
 ---- batch: 060 ----
mean loss: 181.23
 ---- batch: 070 ----
mean loss: 181.25
 ---- batch: 080 ----
mean loss: 182.41
 ---- batch: 090 ----
mean loss: 175.75
 ---- batch: 100 ----
mean loss: 182.85
 ---- batch: 110 ----
mean loss: 178.98
train mean loss: 179.16
epoch train time: 0:00:15.303347
elapsed time: 0:46:48.189126
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 08:02:22.386539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.18
 ---- batch: 020 ----
mean loss: 178.53
 ---- batch: 030 ----
mean loss: 186.93
 ---- batch: 040 ----
mean loss: 171.75
 ---- batch: 050 ----
mean loss: 174.47
 ---- batch: 060 ----
mean loss: 167.38
 ---- batch: 070 ----
mean loss: 175.40
 ---- batch: 080 ----
mean loss: 176.54
 ---- batch: 090 ----
mean loss: 176.89
 ---- batch: 100 ----
mean loss: 181.32
 ---- batch: 110 ----
mean loss: 183.65
train mean loss: 176.90
epoch train time: 0:00:15.296324
elapsed time: 0:47:03.486473
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 08:02:37.683849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.09
 ---- batch: 020 ----
mean loss: 181.46
 ---- batch: 030 ----
mean loss: 173.22
 ---- batch: 040 ----
mean loss: 179.70
 ---- batch: 050 ----
mean loss: 177.95
 ---- batch: 060 ----
mean loss: 183.61
 ---- batch: 070 ----
mean loss: 170.91
 ---- batch: 080 ----
mean loss: 173.89
 ---- batch: 090 ----
mean loss: 173.56
 ---- batch: 100 ----
mean loss: 178.23
 ---- batch: 110 ----
mean loss: 167.70
train mean loss: 176.48
epoch train time: 0:00:15.349302
elapsed time: 0:47:18.836707
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 08:02:53.034107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.84
 ---- batch: 020 ----
mean loss: 177.47
 ---- batch: 030 ----
mean loss: 170.61
 ---- batch: 040 ----
mean loss: 175.57
 ---- batch: 050 ----
mean loss: 171.65
 ---- batch: 060 ----
mean loss: 173.69
 ---- batch: 070 ----
mean loss: 182.56
 ---- batch: 080 ----
mean loss: 179.49
 ---- batch: 090 ----
mean loss: 191.25
 ---- batch: 100 ----
mean loss: 175.66
 ---- batch: 110 ----
mean loss: 173.68
train mean loss: 176.92
epoch train time: 0:00:15.337691
elapsed time: 0:47:34.175209
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 08:03:08.372567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.07
 ---- batch: 020 ----
mean loss: 181.30
 ---- batch: 030 ----
mean loss: 178.69
 ---- batch: 040 ----
mean loss: 174.10
 ---- batch: 050 ----
mean loss: 183.61
 ---- batch: 060 ----
mean loss: 187.08
 ---- batch: 070 ----
mean loss: 180.43
 ---- batch: 080 ----
mean loss: 177.25
 ---- batch: 090 ----
mean loss: 163.36
 ---- batch: 100 ----
mean loss: 171.05
 ---- batch: 110 ----
mean loss: 183.79
train mean loss: 177.75
epoch train time: 0:00:15.344296
elapsed time: 0:47:49.520428
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 08:03:23.717860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.02
 ---- batch: 020 ----
mean loss: 169.48
 ---- batch: 030 ----
mean loss: 183.41
 ---- batch: 040 ----
mean loss: 178.32
 ---- batch: 050 ----
mean loss: 178.73
 ---- batch: 060 ----
mean loss: 176.80
 ---- batch: 070 ----
mean loss: 179.37
 ---- batch: 080 ----
mean loss: 168.32
 ---- batch: 090 ----
mean loss: 173.35
 ---- batch: 100 ----
mean loss: 175.00
 ---- batch: 110 ----
mean loss: 178.83
train mean loss: 176.20
epoch train time: 0:00:15.344475
elapsed time: 0:48:04.865912
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 08:03:39.063300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.50
 ---- batch: 020 ----
mean loss: 183.16
 ---- batch: 030 ----
mean loss: 183.43
 ---- batch: 040 ----
mean loss: 172.99
 ---- batch: 050 ----
mean loss: 173.71
 ---- batch: 060 ----
mean loss: 176.93
 ---- batch: 070 ----
mean loss: 175.67
 ---- batch: 080 ----
mean loss: 173.12
 ---- batch: 090 ----
mean loss: 171.59
 ---- batch: 100 ----
mean loss: 165.92
 ---- batch: 110 ----
mean loss: 173.72
train mean loss: 175.89
epoch train time: 0:00:15.309387
elapsed time: 0:48:20.176232
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 08:03:54.373638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.07
 ---- batch: 020 ----
mean loss: 182.27
 ---- batch: 030 ----
mean loss: 181.09
 ---- batch: 040 ----
mean loss: 181.98
 ---- batch: 050 ----
mean loss: 182.03
 ---- batch: 060 ----
mean loss: 179.53
 ---- batch: 070 ----
mean loss: 167.08
 ---- batch: 080 ----
mean loss: 171.26
 ---- batch: 090 ----
mean loss: 180.62
 ---- batch: 100 ----
mean loss: 179.97
 ---- batch: 110 ----
mean loss: 178.32
train mean loss: 177.63
epoch train time: 0:00:15.278610
elapsed time: 0:48:35.455829
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 08:04:09.653208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.83
 ---- batch: 020 ----
mean loss: 178.19
 ---- batch: 030 ----
mean loss: 183.54
 ---- batch: 040 ----
mean loss: 164.89
 ---- batch: 050 ----
mean loss: 175.75
 ---- batch: 060 ----
mean loss: 176.19
 ---- batch: 070 ----
mean loss: 175.39
 ---- batch: 080 ----
mean loss: 172.55
 ---- batch: 090 ----
mean loss: 182.18
 ---- batch: 100 ----
mean loss: 176.52
 ---- batch: 110 ----
mean loss: 170.69
train mean loss: 176.02
epoch train time: 0:00:15.359505
elapsed time: 0:48:50.816283
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 08:04:25.013742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.55
 ---- batch: 020 ----
mean loss: 177.38
 ---- batch: 030 ----
mean loss: 190.61
 ---- batch: 040 ----
mean loss: 186.96
 ---- batch: 050 ----
mean loss: 176.65
 ---- batch: 060 ----
mean loss: 173.39
 ---- batch: 070 ----
mean loss: 168.77
 ---- batch: 080 ----
mean loss: 185.52
 ---- batch: 090 ----
mean loss: 174.34
 ---- batch: 100 ----
mean loss: 165.61
 ---- batch: 110 ----
mean loss: 172.06
train mean loss: 177.48
epoch train time: 0:00:15.395105
elapsed time: 0:49:06.212409
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 08:04:40.409808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.48
 ---- batch: 020 ----
mean loss: 172.67
 ---- batch: 030 ----
mean loss: 180.55
 ---- batch: 040 ----
mean loss: 173.16
 ---- batch: 050 ----
mean loss: 174.93
 ---- batch: 060 ----
mean loss: 181.63
 ---- batch: 070 ----
mean loss: 183.80
 ---- batch: 080 ----
mean loss: 173.89
 ---- batch: 090 ----
mean loss: 171.97
 ---- batch: 100 ----
mean loss: 174.39
 ---- batch: 110 ----
mean loss: 182.87
train mean loss: 176.09
epoch train time: 0:00:15.248004
elapsed time: 0:49:21.461340
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 08:04:55.658721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.11
 ---- batch: 020 ----
mean loss: 168.84
 ---- batch: 030 ----
mean loss: 176.01
 ---- batch: 040 ----
mean loss: 178.72
 ---- batch: 050 ----
mean loss: 185.23
 ---- batch: 060 ----
mean loss: 177.90
 ---- batch: 070 ----
mean loss: 179.40
 ---- batch: 080 ----
mean loss: 175.32
 ---- batch: 090 ----
mean loss: 181.90
 ---- batch: 100 ----
mean loss: 180.99
 ---- batch: 110 ----
mean loss: 178.10
train mean loss: 178.32
epoch train time: 0:00:15.279365
elapsed time: 0:49:36.741595
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 08:05:10.938967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.54
 ---- batch: 020 ----
mean loss: 175.17
 ---- batch: 030 ----
mean loss: 177.73
 ---- batch: 040 ----
mean loss: 175.84
 ---- batch: 050 ----
mean loss: 179.93
 ---- batch: 060 ----
mean loss: 182.79
 ---- batch: 070 ----
mean loss: 168.28
 ---- batch: 080 ----
mean loss: 176.62
 ---- batch: 090 ----
mean loss: 168.90
 ---- batch: 100 ----
mean loss: 172.70
 ---- batch: 110 ----
mean loss: 178.07
train mean loss: 176.46
epoch train time: 0:00:15.335238
elapsed time: 0:49:52.077731
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 08:05:26.275150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.59
 ---- batch: 020 ----
mean loss: 179.74
 ---- batch: 030 ----
mean loss: 176.78
 ---- batch: 040 ----
mean loss: 179.82
 ---- batch: 050 ----
mean loss: 177.68
 ---- batch: 060 ----
mean loss: 165.98
 ---- batch: 070 ----
mean loss: 168.72
 ---- batch: 080 ----
mean loss: 173.45
 ---- batch: 090 ----
mean loss: 166.64
 ---- batch: 100 ----
mean loss: 183.47
 ---- batch: 110 ----
mean loss: 178.47
train mean loss: 175.30
epoch train time: 0:00:15.272827
elapsed time: 0:50:07.351631
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 08:05:41.549135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.99
 ---- batch: 020 ----
mean loss: 190.05
 ---- batch: 030 ----
mean loss: 180.75
 ---- batch: 040 ----
mean loss: 176.16
 ---- batch: 050 ----
mean loss: 170.35
 ---- batch: 060 ----
mean loss: 171.90
 ---- batch: 070 ----
mean loss: 183.77
 ---- batch: 080 ----
mean loss: 174.03
 ---- batch: 090 ----
mean loss: 179.05
 ---- batch: 100 ----
mean loss: 176.76
 ---- batch: 110 ----
mean loss: 165.67
train mean loss: 176.10
epoch train time: 0:00:15.317629
elapsed time: 0:50:22.670199
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 08:05:56.867582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.77
 ---- batch: 020 ----
mean loss: 180.93
 ---- batch: 030 ----
mean loss: 184.30
 ---- batch: 040 ----
mean loss: 183.15
 ---- batch: 050 ----
mean loss: 169.29
 ---- batch: 060 ----
mean loss: 170.93
 ---- batch: 070 ----
mean loss: 174.72
 ---- batch: 080 ----
mean loss: 174.23
 ---- batch: 090 ----
mean loss: 180.94
 ---- batch: 100 ----
mean loss: 170.35
 ---- batch: 110 ----
mean loss: 181.97
train mean loss: 176.44
epoch train time: 0:00:15.360528
elapsed time: 0:50:38.031645
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 08:06:12.229127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.78
 ---- batch: 020 ----
mean loss: 173.71
 ---- batch: 030 ----
mean loss: 174.67
 ---- batch: 040 ----
mean loss: 191.28
 ---- batch: 050 ----
mean loss: 172.59
 ---- batch: 060 ----
mean loss: 168.11
 ---- batch: 070 ----
mean loss: 175.63
 ---- batch: 080 ----
mean loss: 178.61
 ---- batch: 090 ----
mean loss: 172.64
 ---- batch: 100 ----
mean loss: 166.87
 ---- batch: 110 ----
mean loss: 180.21
train mean loss: 175.65
epoch train time: 0:00:15.305827
elapsed time: 0:50:53.338502
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 08:06:27.535890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.87
 ---- batch: 020 ----
mean loss: 180.02
 ---- batch: 030 ----
mean loss: 180.41
 ---- batch: 040 ----
mean loss: 164.52
 ---- batch: 050 ----
mean loss: 180.10
 ---- batch: 060 ----
mean loss: 168.14
 ---- batch: 070 ----
mean loss: 187.96
 ---- batch: 080 ----
mean loss: 181.68
 ---- batch: 090 ----
mean loss: 172.36
 ---- batch: 100 ----
mean loss: 174.25
 ---- batch: 110 ----
mean loss: 173.10
train mean loss: 176.04
epoch train time: 0:00:15.251371
elapsed time: 0:51:08.590832
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 08:06:42.788219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.45
 ---- batch: 020 ----
mean loss: 171.19
 ---- batch: 030 ----
mean loss: 171.62
 ---- batch: 040 ----
mean loss: 176.58
 ---- batch: 050 ----
mean loss: 174.46
 ---- batch: 060 ----
mean loss: 182.85
 ---- batch: 070 ----
mean loss: 171.78
 ---- batch: 080 ----
mean loss: 177.70
 ---- batch: 090 ----
mean loss: 170.97
 ---- batch: 100 ----
mean loss: 167.02
 ---- batch: 110 ----
mean loss: 177.15
train mean loss: 174.84
epoch train time: 0:00:15.309850
elapsed time: 0:51:23.901604
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 08:06:58.098961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.31
 ---- batch: 020 ----
mean loss: 165.16
 ---- batch: 030 ----
mean loss: 186.96
 ---- batch: 040 ----
mean loss: 166.33
 ---- batch: 050 ----
mean loss: 176.91
 ---- batch: 060 ----
mean loss: 187.60
 ---- batch: 070 ----
mean loss: 180.69
 ---- batch: 080 ----
mean loss: 173.23
 ---- batch: 090 ----
mean loss: 166.59
 ---- batch: 100 ----
mean loss: 174.81
 ---- batch: 110 ----
mean loss: 176.84
train mean loss: 175.49
epoch train time: 0:00:15.283053
elapsed time: 0:51:39.185527
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 08:07:13.382920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.61
 ---- batch: 020 ----
mean loss: 173.23
 ---- batch: 030 ----
mean loss: 174.55
 ---- batch: 040 ----
mean loss: 174.74
 ---- batch: 050 ----
mean loss: 173.74
 ---- batch: 060 ----
mean loss: 181.36
 ---- batch: 070 ----
mean loss: 173.53
 ---- batch: 080 ----
mean loss: 165.27
 ---- batch: 090 ----
mean loss: 174.51
 ---- batch: 100 ----
mean loss: 178.90
 ---- batch: 110 ----
mean loss: 184.89
train mean loss: 175.27
epoch train time: 0:00:15.311999
elapsed time: 0:51:54.498452
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 08:07:28.696041
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.10
 ---- batch: 020 ----
mean loss: 172.15
 ---- batch: 030 ----
mean loss: 181.68
 ---- batch: 040 ----
mean loss: 170.48
 ---- batch: 050 ----
mean loss: 165.42
 ---- batch: 060 ----
mean loss: 174.05
 ---- batch: 070 ----
mean loss: 169.54
 ---- batch: 080 ----
mean loss: 174.14
 ---- batch: 090 ----
mean loss: 173.57
 ---- batch: 100 ----
mean loss: 173.45
 ---- batch: 110 ----
mean loss: 168.34
train mean loss: 172.25
epoch train time: 0:00:15.319402
elapsed time: 0:52:09.819704
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 08:07:44.016685
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.11
 ---- batch: 020 ----
mean loss: 165.47
 ---- batch: 030 ----
mean loss: 165.86
 ---- batch: 040 ----
mean loss: 174.98
 ---- batch: 050 ----
mean loss: 175.21
 ---- batch: 060 ----
mean loss: 172.71
 ---- batch: 070 ----
mean loss: 165.97
 ---- batch: 080 ----
mean loss: 181.33
 ---- batch: 090 ----
mean loss: 176.40
 ---- batch: 100 ----
mean loss: 173.50
 ---- batch: 110 ----
mean loss: 166.93
train mean loss: 172.30
epoch train time: 0:00:15.291473
elapsed time: 0:52:25.111696
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 08:07:59.309078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.75
 ---- batch: 020 ----
mean loss: 173.54
 ---- batch: 030 ----
mean loss: 178.68
 ---- batch: 040 ----
mean loss: 173.12
 ---- batch: 050 ----
mean loss: 174.89
 ---- batch: 060 ----
mean loss: 177.77
 ---- batch: 070 ----
mean loss: 166.24
 ---- batch: 080 ----
mean loss: 178.39
 ---- batch: 090 ----
mean loss: 168.41
 ---- batch: 100 ----
mean loss: 171.68
 ---- batch: 110 ----
mean loss: 168.00
train mean loss: 172.41
epoch train time: 0:00:15.287837
elapsed time: 0:52:40.400433
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 08:08:14.597876
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.96
 ---- batch: 020 ----
mean loss: 168.48
 ---- batch: 030 ----
mean loss: 166.76
 ---- batch: 040 ----
mean loss: 169.86
 ---- batch: 050 ----
mean loss: 174.55
 ---- batch: 060 ----
mean loss: 174.69
 ---- batch: 070 ----
mean loss: 177.38
 ---- batch: 080 ----
mean loss: 176.19
 ---- batch: 090 ----
mean loss: 175.82
 ---- batch: 100 ----
mean loss: 164.80
 ---- batch: 110 ----
mean loss: 170.97
train mean loss: 172.14
epoch train time: 0:00:15.290717
elapsed time: 0:52:55.692129
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 08:08:29.889563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.26
 ---- batch: 020 ----
mean loss: 169.33
 ---- batch: 030 ----
mean loss: 174.52
 ---- batch: 040 ----
mean loss: 170.50
 ---- batch: 050 ----
mean loss: 173.25
 ---- batch: 060 ----
mean loss: 175.13
 ---- batch: 070 ----
mean loss: 173.15
 ---- batch: 080 ----
mean loss: 178.53
 ---- batch: 090 ----
mean loss: 163.87
 ---- batch: 100 ----
mean loss: 165.85
 ---- batch: 110 ----
mean loss: 175.93
train mean loss: 172.34
epoch train time: 0:00:15.225714
elapsed time: 0:53:10.918896
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 08:08:45.116298
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.54
 ---- batch: 020 ----
mean loss: 169.09
 ---- batch: 030 ----
mean loss: 173.29
 ---- batch: 040 ----
mean loss: 174.50
 ---- batch: 050 ----
mean loss: 166.72
 ---- batch: 060 ----
mean loss: 176.72
 ---- batch: 070 ----
mean loss: 175.01
 ---- batch: 080 ----
mean loss: 176.64
 ---- batch: 090 ----
mean loss: 168.34
 ---- batch: 100 ----
mean loss: 164.59
 ---- batch: 110 ----
mean loss: 177.64
train mean loss: 172.24
epoch train time: 0:00:15.266261
elapsed time: 0:53:26.186051
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 08:09:00.383426
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.34
 ---- batch: 020 ----
mean loss: 176.47
 ---- batch: 030 ----
mean loss: 173.74
 ---- batch: 040 ----
mean loss: 185.07
 ---- batch: 050 ----
mean loss: 162.44
 ---- batch: 060 ----
mean loss: 172.47
 ---- batch: 070 ----
mean loss: 164.12
 ---- batch: 080 ----
mean loss: 179.95
 ---- batch: 090 ----
mean loss: 167.34
 ---- batch: 100 ----
mean loss: 178.54
 ---- batch: 110 ----
mean loss: 169.47
train mean loss: 172.12
epoch train time: 0:00:15.312317
elapsed time: 0:53:41.499305
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 08:09:15.696748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.43
 ---- batch: 020 ----
mean loss: 171.85
 ---- batch: 030 ----
mean loss: 169.75
 ---- batch: 040 ----
mean loss: 167.12
 ---- batch: 050 ----
mean loss: 173.17
 ---- batch: 060 ----
mean loss: 178.78
 ---- batch: 070 ----
mean loss: 179.57
 ---- batch: 080 ----
mean loss: 173.27
 ---- batch: 090 ----
mean loss: 171.12
 ---- batch: 100 ----
mean loss: 166.42
 ---- batch: 110 ----
mean loss: 175.89
train mean loss: 172.21
epoch train time: 0:00:15.364383
elapsed time: 0:53:56.864695
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 08:09:31.062084
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.41
 ---- batch: 020 ----
mean loss: 158.35
 ---- batch: 030 ----
mean loss: 177.95
 ---- batch: 040 ----
mean loss: 167.91
 ---- batch: 050 ----
mean loss: 168.62
 ---- batch: 060 ----
mean loss: 176.30
 ---- batch: 070 ----
mean loss: 174.50
 ---- batch: 080 ----
mean loss: 170.49
 ---- batch: 090 ----
mean loss: 175.65
 ---- batch: 100 ----
mean loss: 170.85
 ---- batch: 110 ----
mean loss: 181.70
train mean loss: 172.24
epoch train time: 0:00:15.355013
elapsed time: 0:54:12.220630
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 08:09:46.418149
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.16
 ---- batch: 020 ----
mean loss: 171.67
 ---- batch: 030 ----
mean loss: 167.00
 ---- batch: 040 ----
mean loss: 174.56
 ---- batch: 050 ----
mean loss: 176.44
 ---- batch: 060 ----
mean loss: 173.76
 ---- batch: 070 ----
mean loss: 166.82
 ---- batch: 080 ----
mean loss: 167.75
 ---- batch: 090 ----
mean loss: 166.99
 ---- batch: 100 ----
mean loss: 170.90
 ---- batch: 110 ----
mean loss: 168.06
train mean loss: 172.05
epoch train time: 0:00:15.345926
elapsed time: 0:54:27.567592
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 08:10:01.765014
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.91
 ---- batch: 020 ----
mean loss: 173.96
 ---- batch: 030 ----
mean loss: 172.92
 ---- batch: 040 ----
mean loss: 176.16
 ---- batch: 050 ----
mean loss: 173.45
 ---- batch: 060 ----
mean loss: 174.46
 ---- batch: 070 ----
mean loss: 165.29
 ---- batch: 080 ----
mean loss: 170.58
 ---- batch: 090 ----
mean loss: 175.42
 ---- batch: 100 ----
mean loss: 165.70
 ---- batch: 110 ----
mean loss: 172.71
train mean loss: 172.38
epoch train time: 0:00:15.297012
elapsed time: 0:54:42.865534
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 08:10:17.062902
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.60
 ---- batch: 020 ----
mean loss: 170.24
 ---- batch: 030 ----
mean loss: 170.72
 ---- batch: 040 ----
mean loss: 178.98
 ---- batch: 050 ----
mean loss: 174.76
 ---- batch: 060 ----
mean loss: 173.32
 ---- batch: 070 ----
mean loss: 175.23
 ---- batch: 080 ----
mean loss: 170.75
 ---- batch: 090 ----
mean loss: 170.42
 ---- batch: 100 ----
mean loss: 173.84
 ---- batch: 110 ----
mean loss: 167.73
train mean loss: 172.04
epoch train time: 0:00:15.372965
elapsed time: 0:54:58.239393
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 08:10:32.436761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.50
 ---- batch: 020 ----
mean loss: 166.34
 ---- batch: 030 ----
mean loss: 161.76
 ---- batch: 040 ----
mean loss: 182.01
 ---- batch: 050 ----
mean loss: 172.67
 ---- batch: 060 ----
mean loss: 177.93
 ---- batch: 070 ----
mean loss: 165.74
 ---- batch: 080 ----
mean loss: 168.62
 ---- batch: 090 ----
mean loss: 164.43
 ---- batch: 100 ----
mean loss: 180.02
 ---- batch: 110 ----
mean loss: 177.35
train mean loss: 171.94
epoch train time: 0:00:15.369008
elapsed time: 0:55:13.609334
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 08:10:47.806867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.97
 ---- batch: 020 ----
mean loss: 177.04
 ---- batch: 030 ----
mean loss: 167.18
 ---- batch: 040 ----
mean loss: 163.21
 ---- batch: 050 ----
mean loss: 176.50
 ---- batch: 060 ----
mean loss: 169.87
 ---- batch: 070 ----
mean loss: 180.71
 ---- batch: 080 ----
mean loss: 173.36
 ---- batch: 090 ----
mean loss: 169.67
 ---- batch: 100 ----
mean loss: 174.69
 ---- batch: 110 ----
mean loss: 175.57
train mean loss: 172.21
epoch train time: 0:00:15.389552
elapsed time: 0:55:28.999908
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 08:11:03.197294
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.98
 ---- batch: 020 ----
mean loss: 176.55
 ---- batch: 030 ----
mean loss: 176.73
 ---- batch: 040 ----
mean loss: 175.52
 ---- batch: 050 ----
mean loss: 166.66
 ---- batch: 060 ----
mean loss: 170.41
 ---- batch: 070 ----
mean loss: 176.63
 ---- batch: 080 ----
mean loss: 163.04
 ---- batch: 090 ----
mean loss: 162.31
 ---- batch: 100 ----
mean loss: 168.99
 ---- batch: 110 ----
mean loss: 173.68
train mean loss: 172.16
epoch train time: 0:00:15.365716
elapsed time: 0:55:44.366415
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 08:11:18.563904
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.84
 ---- batch: 020 ----
mean loss: 173.83
 ---- batch: 030 ----
mean loss: 171.18
 ---- batch: 040 ----
mean loss: 171.12
 ---- batch: 050 ----
mean loss: 168.48
 ---- batch: 060 ----
mean loss: 169.37
 ---- batch: 070 ----
mean loss: 165.32
 ---- batch: 080 ----
mean loss: 173.71
 ---- batch: 090 ----
mean loss: 176.20
 ---- batch: 100 ----
mean loss: 174.98
 ---- batch: 110 ----
mean loss: 170.72
train mean loss: 172.03
epoch train time: 0:00:15.373665
elapsed time: 0:55:59.741216
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 08:11:33.938595
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.14
 ---- batch: 020 ----
mean loss: 167.20
 ---- batch: 030 ----
mean loss: 176.28
 ---- batch: 040 ----
mean loss: 170.15
 ---- batch: 050 ----
mean loss: 172.65
 ---- batch: 060 ----
mean loss: 174.07
 ---- batch: 070 ----
mean loss: 173.65
 ---- batch: 080 ----
mean loss: 174.97
 ---- batch: 090 ----
mean loss: 173.28
 ---- batch: 100 ----
mean loss: 179.64
 ---- batch: 110 ----
mean loss: 166.57
train mean loss: 172.07
epoch train time: 0:00:15.309142
elapsed time: 0:56:15.051164
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 08:11:49.248524
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.68
 ---- batch: 020 ----
mean loss: 176.45
 ---- batch: 030 ----
mean loss: 179.60
 ---- batch: 040 ----
mean loss: 172.56
 ---- batch: 050 ----
mean loss: 169.94
 ---- batch: 060 ----
mean loss: 174.04
 ---- batch: 070 ----
mean loss: 167.30
 ---- batch: 080 ----
mean loss: 163.77
 ---- batch: 090 ----
mean loss: 166.32
 ---- batch: 100 ----
mean loss: 177.43
 ---- batch: 110 ----
mean loss: 168.88
train mean loss: 171.95
epoch train time: 0:00:15.387915
elapsed time: 0:56:30.440001
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 08:12:04.637392
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.93
 ---- batch: 020 ----
mean loss: 171.79
 ---- batch: 030 ----
mean loss: 167.72
 ---- batch: 040 ----
mean loss: 169.55
 ---- batch: 050 ----
mean loss: 177.74
 ---- batch: 060 ----
mean loss: 166.60
 ---- batch: 070 ----
mean loss: 169.12
 ---- batch: 080 ----
mean loss: 182.36
 ---- batch: 090 ----
mean loss: 172.26
 ---- batch: 100 ----
mean loss: 170.98
 ---- batch: 110 ----
mean loss: 173.65
train mean loss: 172.09
epoch train time: 0:00:15.470291
elapsed time: 0:56:45.911196
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 08:12:20.108600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.08
 ---- batch: 020 ----
mean loss: 173.16
 ---- batch: 030 ----
mean loss: 176.63
 ---- batch: 040 ----
mean loss: 174.57
 ---- batch: 050 ----
mean loss: 179.50
 ---- batch: 060 ----
mean loss: 169.57
 ---- batch: 070 ----
mean loss: 169.25
 ---- batch: 080 ----
mean loss: 168.40
 ---- batch: 090 ----
mean loss: 170.62
 ---- batch: 100 ----
mean loss: 175.71
 ---- batch: 110 ----
mean loss: 172.69
train mean loss: 172.07
epoch train time: 0:00:15.616615
elapsed time: 0:57:01.528787
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 08:12:35.726175
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.11
 ---- batch: 020 ----
mean loss: 169.59
 ---- batch: 030 ----
mean loss: 177.26
 ---- batch: 040 ----
mean loss: 177.96
 ---- batch: 050 ----
mean loss: 166.18
 ---- batch: 060 ----
mean loss: 170.14
 ---- batch: 070 ----
mean loss: 177.59
 ---- batch: 080 ----
mean loss: 178.94
 ---- batch: 090 ----
mean loss: 166.44
 ---- batch: 100 ----
mean loss: 168.86
 ---- batch: 110 ----
mean loss: 172.78
train mean loss: 172.01
epoch train time: 0:00:15.478380
elapsed time: 0:57:17.008019
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 08:12:51.205393
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.29
 ---- batch: 020 ----
mean loss: 173.10
 ---- batch: 030 ----
mean loss: 181.70
 ---- batch: 040 ----
mean loss: 169.23
 ---- batch: 050 ----
mean loss: 173.96
 ---- batch: 060 ----
mean loss: 172.23
 ---- batch: 070 ----
mean loss: 168.88
 ---- batch: 080 ----
mean loss: 168.32
 ---- batch: 090 ----
mean loss: 168.15
 ---- batch: 100 ----
mean loss: 175.55
 ---- batch: 110 ----
mean loss: 168.19
train mean loss: 171.96
epoch train time: 0:00:15.595892
elapsed time: 0:57:32.604905
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 08:13:06.802307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.81
 ---- batch: 020 ----
mean loss: 169.56
 ---- batch: 030 ----
mean loss: 166.19
 ---- batch: 040 ----
mean loss: 169.31
 ---- batch: 050 ----
mean loss: 181.38
 ---- batch: 060 ----
mean loss: 171.65
 ---- batch: 070 ----
mean loss: 173.13
 ---- batch: 080 ----
mean loss: 171.39
 ---- batch: 090 ----
mean loss: 177.86
 ---- batch: 100 ----
mean loss: 173.63
 ---- batch: 110 ----
mean loss: 166.05
train mean loss: 171.97
epoch train time: 0:00:15.521764
elapsed time: 0:57:48.127593
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 08:13:22.324964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.70
 ---- batch: 020 ----
mean loss: 176.33
 ---- batch: 030 ----
mean loss: 173.03
 ---- batch: 040 ----
mean loss: 168.85
 ---- batch: 050 ----
mean loss: 174.31
 ---- batch: 060 ----
mean loss: 178.32
 ---- batch: 070 ----
mean loss: 170.97
 ---- batch: 080 ----
mean loss: 177.93
 ---- batch: 090 ----
mean loss: 174.45
 ---- batch: 100 ----
mean loss: 162.12
 ---- batch: 110 ----
mean loss: 170.11
train mean loss: 172.17
epoch train time: 0:00:15.470992
elapsed time: 0:58:03.599573
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 08:13:37.796984
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.09
 ---- batch: 020 ----
mean loss: 166.16
 ---- batch: 030 ----
mean loss: 179.55
 ---- batch: 040 ----
mean loss: 177.89
 ---- batch: 050 ----
mean loss: 164.89
 ---- batch: 060 ----
mean loss: 178.46
 ---- batch: 070 ----
mean loss: 164.57
 ---- batch: 080 ----
mean loss: 165.73
 ---- batch: 090 ----
mean loss: 177.21
 ---- batch: 100 ----
mean loss: 176.49
 ---- batch: 110 ----
mean loss: 172.43
train mean loss: 172.12
epoch train time: 0:00:15.479648
elapsed time: 0:58:19.080207
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 08:13:53.277629
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.19
 ---- batch: 020 ----
mean loss: 180.28
 ---- batch: 030 ----
mean loss: 174.30
 ---- batch: 040 ----
mean loss: 166.12
 ---- batch: 050 ----
mean loss: 173.56
 ---- batch: 060 ----
mean loss: 157.66
 ---- batch: 070 ----
mean loss: 181.89
 ---- batch: 080 ----
mean loss: 166.77
 ---- batch: 090 ----
mean loss: 176.76
 ---- batch: 100 ----
mean loss: 173.65
 ---- batch: 110 ----
mean loss: 172.47
train mean loss: 171.74
epoch train time: 0:00:15.475229
elapsed time: 0:58:34.556348
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 08:14:08.753781
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.07
 ---- batch: 020 ----
mean loss: 169.69
 ---- batch: 030 ----
mean loss: 175.22
 ---- batch: 040 ----
mean loss: 168.53
 ---- batch: 050 ----
mean loss: 172.11
 ---- batch: 060 ----
mean loss: 172.71
 ---- batch: 070 ----
mean loss: 181.54
 ---- batch: 080 ----
mean loss: 178.34
 ---- batch: 090 ----
mean loss: 170.85
 ---- batch: 100 ----
mean loss: 171.40
 ---- batch: 110 ----
mean loss: 165.37
train mean loss: 171.71
epoch train time: 0:00:15.409487
elapsed time: 0:58:49.966704
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 08:14:24.164081
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.25
 ---- batch: 020 ----
mean loss: 168.63
 ---- batch: 030 ----
mean loss: 167.41
 ---- batch: 040 ----
mean loss: 176.23
 ---- batch: 050 ----
mean loss: 173.07
 ---- batch: 060 ----
mean loss: 173.62
 ---- batch: 070 ----
mean loss: 175.23
 ---- batch: 080 ----
mean loss: 175.44
 ---- batch: 090 ----
mean loss: 170.40
 ---- batch: 100 ----
mean loss: 167.92
 ---- batch: 110 ----
mean loss: 172.21
train mean loss: 172.06
epoch train time: 0:00:15.424037
elapsed time: 0:59:05.391635
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 08:14:39.589036
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.40
 ---- batch: 020 ----
mean loss: 178.09
 ---- batch: 030 ----
mean loss: 180.99
 ---- batch: 040 ----
mean loss: 175.85
 ---- batch: 050 ----
mean loss: 171.79
 ---- batch: 060 ----
mean loss: 174.38
 ---- batch: 070 ----
mean loss: 170.24
 ---- batch: 080 ----
mean loss: 162.45
 ---- batch: 090 ----
mean loss: 175.20
 ---- batch: 100 ----
mean loss: 171.25
 ---- batch: 110 ----
mean loss: 169.13
train mean loss: 171.99
epoch train time: 0:00:15.484905
elapsed time: 0:59:20.877374
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 08:14:55.074752
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.84
 ---- batch: 020 ----
mean loss: 163.86
 ---- batch: 030 ----
mean loss: 166.71
 ---- batch: 040 ----
mean loss: 182.41
 ---- batch: 050 ----
mean loss: 169.72
 ---- batch: 060 ----
mean loss: 172.34
 ---- batch: 070 ----
mean loss: 175.05
 ---- batch: 080 ----
mean loss: 169.66
 ---- batch: 090 ----
mean loss: 173.50
 ---- batch: 100 ----
mean loss: 166.07
 ---- batch: 110 ----
mean loss: 175.54
train mean loss: 171.88
epoch train time: 0:00:15.456534
elapsed time: 0:59:36.334809
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 08:15:10.532253
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.85
 ---- batch: 020 ----
mean loss: 162.67
 ---- batch: 030 ----
mean loss: 165.48
 ---- batch: 040 ----
mean loss: 180.71
 ---- batch: 050 ----
mean loss: 178.64
 ---- batch: 060 ----
mean loss: 163.75
 ---- batch: 070 ----
mean loss: 170.34
 ---- batch: 080 ----
mean loss: 178.63
 ---- batch: 090 ----
mean loss: 168.33
 ---- batch: 100 ----
mean loss: 178.10
 ---- batch: 110 ----
mean loss: 177.08
train mean loss: 171.84
epoch train time: 0:00:15.499252
elapsed time: 0:59:51.835052
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 08:15:26.032423
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.92
 ---- batch: 020 ----
mean loss: 171.54
 ---- batch: 030 ----
mean loss: 172.68
 ---- batch: 040 ----
mean loss: 175.28
 ---- batch: 050 ----
mean loss: 163.46
 ---- batch: 060 ----
mean loss: 178.98
 ---- batch: 070 ----
mean loss: 172.49
 ---- batch: 080 ----
mean loss: 175.19
 ---- batch: 090 ----
mean loss: 172.59
 ---- batch: 100 ----
mean loss: 167.36
 ---- batch: 110 ----
mean loss: 169.33
train mean loss: 171.91
epoch train time: 0:00:15.588690
elapsed time: 1:00:07.424691
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 08:15:41.622343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.21
 ---- batch: 020 ----
mean loss: 167.09
 ---- batch: 030 ----
mean loss: 173.83
 ---- batch: 040 ----
mean loss: 174.94
 ---- batch: 050 ----
mean loss: 167.08
 ---- batch: 060 ----
mean loss: 175.47
 ---- batch: 070 ----
mean loss: 176.15
 ---- batch: 080 ----
mean loss: 173.47
 ---- batch: 090 ----
mean loss: 169.15
 ---- batch: 100 ----
mean loss: 178.96
 ---- batch: 110 ----
mean loss: 169.41
train mean loss: 172.29
epoch train time: 0:00:15.599158
elapsed time: 1:00:23.025716
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 08:15:57.222789
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.58
 ---- batch: 020 ----
mean loss: 169.64
 ---- batch: 030 ----
mean loss: 162.39
 ---- batch: 040 ----
mean loss: 172.80
 ---- batch: 050 ----
mean loss: 168.73
 ---- batch: 060 ----
mean loss: 176.34
 ---- batch: 070 ----
mean loss: 168.89
 ---- batch: 080 ----
mean loss: 177.65
 ---- batch: 090 ----
mean loss: 177.29
 ---- batch: 100 ----
mean loss: 172.70
 ---- batch: 110 ----
mean loss: 173.82
train mean loss: 171.90
epoch train time: 0:00:15.596481
elapsed time: 1:00:38.622860
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 08:16:12.820343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.27
 ---- batch: 020 ----
mean loss: 163.84
 ---- batch: 030 ----
mean loss: 166.97
 ---- batch: 040 ----
mean loss: 182.30
 ---- batch: 050 ----
mean loss: 168.52
 ---- batch: 060 ----
mean loss: 177.63
 ---- batch: 070 ----
mean loss: 173.66
 ---- batch: 080 ----
mean loss: 166.81
 ---- batch: 090 ----
mean loss: 169.95
 ---- batch: 100 ----
mean loss: 168.80
 ---- batch: 110 ----
mean loss: 170.57
train mean loss: 171.81
epoch train time: 0:00:15.631035
elapsed time: 1:00:54.254860
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 08:16:28.452274
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.07
 ---- batch: 020 ----
mean loss: 175.22
 ---- batch: 030 ----
mean loss: 168.38
 ---- batch: 040 ----
mean loss: 175.83
 ---- batch: 050 ----
mean loss: 172.11
 ---- batch: 060 ----
mean loss: 182.48
 ---- batch: 070 ----
mean loss: 171.98
 ---- batch: 080 ----
mean loss: 169.36
 ---- batch: 090 ----
mean loss: 165.50
 ---- batch: 100 ----
mean loss: 169.68
 ---- batch: 110 ----
mean loss: 167.13
train mean loss: 171.67
epoch train time: 0:00:15.555893
elapsed time: 1:01:09.811692
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 08:16:44.009050
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.21
 ---- batch: 020 ----
mean loss: 183.54
 ---- batch: 030 ----
mean loss: 168.92
 ---- batch: 040 ----
mean loss: 169.09
 ---- batch: 050 ----
mean loss: 172.57
 ---- batch: 060 ----
mean loss: 176.38
 ---- batch: 070 ----
mean loss: 165.66
 ---- batch: 080 ----
mean loss: 167.68
 ---- batch: 090 ----
mean loss: 174.60
 ---- batch: 100 ----
mean loss: 168.63
 ---- batch: 110 ----
mean loss: 166.38
train mean loss: 171.58
epoch train time: 0:00:15.516510
elapsed time: 1:01:25.329175
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 08:16:59.526541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.17
 ---- batch: 020 ----
mean loss: 174.06
 ---- batch: 030 ----
mean loss: 172.60
 ---- batch: 040 ----
mean loss: 176.99
 ---- batch: 050 ----
mean loss: 169.96
 ---- batch: 060 ----
mean loss: 174.19
 ---- batch: 070 ----
mean loss: 184.46
 ---- batch: 080 ----
mean loss: 162.64
 ---- batch: 090 ----
mean loss: 162.02
 ---- batch: 100 ----
mean loss: 178.56
 ---- batch: 110 ----
mean loss: 166.30
train mean loss: 171.70
epoch train time: 0:00:15.578918
elapsed time: 1:01:40.909063
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 08:17:15.106504
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.68
 ---- batch: 020 ----
mean loss: 175.92
 ---- batch: 030 ----
mean loss: 174.79
 ---- batch: 040 ----
mean loss: 170.82
 ---- batch: 050 ----
mean loss: 169.88
 ---- batch: 060 ----
mean loss: 182.79
 ---- batch: 070 ----
mean loss: 175.78
 ---- batch: 080 ----
mean loss: 179.04
 ---- batch: 090 ----
mean loss: 164.74
 ---- batch: 100 ----
mean loss: 168.12
 ---- batch: 110 ----
mean loss: 161.10
train mean loss: 171.71
epoch train time: 0:00:15.577354
elapsed time: 1:01:56.487470
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 08:17:30.684963
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.93
 ---- batch: 020 ----
mean loss: 165.96
 ---- batch: 030 ----
mean loss: 174.60
 ---- batch: 040 ----
mean loss: 164.50
 ---- batch: 050 ----
mean loss: 170.18
 ---- batch: 060 ----
mean loss: 172.01
 ---- batch: 070 ----
mean loss: 179.03
 ---- batch: 080 ----
mean loss: 169.81
 ---- batch: 090 ----
mean loss: 184.86
 ---- batch: 100 ----
mean loss: 168.53
 ---- batch: 110 ----
mean loss: 173.65
train mean loss: 171.91
epoch train time: 0:00:15.570539
elapsed time: 1:02:12.059029
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 08:17:46.256400
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.78
 ---- batch: 020 ----
mean loss: 167.98
 ---- batch: 030 ----
mean loss: 172.15
 ---- batch: 040 ----
mean loss: 169.46
 ---- batch: 050 ----
mean loss: 176.74
 ---- batch: 060 ----
mean loss: 179.32
 ---- batch: 070 ----
mean loss: 174.66
 ---- batch: 080 ----
mean loss: 168.50
 ---- batch: 090 ----
mean loss: 169.59
 ---- batch: 100 ----
mean loss: 180.38
 ---- batch: 110 ----
mean loss: 165.18
train mean loss: 171.51
epoch train time: 0:00:15.561437
elapsed time: 1:02:27.621469
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 08:18:01.818849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.87
 ---- batch: 020 ----
mean loss: 181.86
 ---- batch: 030 ----
mean loss: 181.31
 ---- batch: 040 ----
mean loss: 165.56
 ---- batch: 050 ----
mean loss: 165.49
 ---- batch: 060 ----
mean loss: 170.68
 ---- batch: 070 ----
mean loss: 173.64
 ---- batch: 080 ----
mean loss: 156.94
 ---- batch: 090 ----
mean loss: 168.92
 ---- batch: 100 ----
mean loss: 176.59
 ---- batch: 110 ----
mean loss: 173.60
train mean loss: 171.66
epoch train time: 0:00:15.524467
elapsed time: 1:02:43.146934
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 08:18:17.344350
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.24
 ---- batch: 020 ----
mean loss: 181.43
 ---- batch: 030 ----
mean loss: 165.18
 ---- batch: 040 ----
mean loss: 183.46
 ---- batch: 050 ----
mean loss: 177.09
 ---- batch: 060 ----
mean loss: 168.05
 ---- batch: 070 ----
mean loss: 177.57
 ---- batch: 080 ----
mean loss: 163.92
 ---- batch: 090 ----
mean loss: 163.54
 ---- batch: 100 ----
mean loss: 176.30
 ---- batch: 110 ----
mean loss: 167.77
train mean loss: 171.73
epoch train time: 0:00:15.534135
elapsed time: 1:02:58.682144
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 08:18:32.879539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.93
 ---- batch: 020 ----
mean loss: 169.90
 ---- batch: 030 ----
mean loss: 169.01
 ---- batch: 040 ----
mean loss: 169.87
 ---- batch: 050 ----
mean loss: 171.13
 ---- batch: 060 ----
mean loss: 169.42
 ---- batch: 070 ----
mean loss: 178.09
 ---- batch: 080 ----
mean loss: 176.83
 ---- batch: 090 ----
mean loss: 167.32
 ---- batch: 100 ----
mean loss: 175.38
 ---- batch: 110 ----
mean loss: 176.41
train mean loss: 171.45
epoch train time: 0:00:15.515755
elapsed time: 1:03:14.198828
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 08:18:48.396245
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.15
 ---- batch: 020 ----
mean loss: 169.06
 ---- batch: 030 ----
mean loss: 176.52
 ---- batch: 040 ----
mean loss: 172.65
 ---- batch: 050 ----
mean loss: 165.49
 ---- batch: 060 ----
mean loss: 176.50
 ---- batch: 070 ----
mean loss: 171.21
 ---- batch: 080 ----
mean loss: 164.16
 ---- batch: 090 ----
mean loss: 169.11
 ---- batch: 100 ----
mean loss: 176.13
 ---- batch: 110 ----
mean loss: 176.88
train mean loss: 171.72
epoch train time: 0:00:15.521647
elapsed time: 1:03:29.721498
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 08:19:03.918922
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.51
 ---- batch: 020 ----
mean loss: 174.69
 ---- batch: 030 ----
mean loss: 171.34
 ---- batch: 040 ----
mean loss: 169.37
 ---- batch: 050 ----
mean loss: 172.11
 ---- batch: 060 ----
mean loss: 169.66
 ---- batch: 070 ----
mean loss: 171.90
 ---- batch: 080 ----
mean loss: 169.39
 ---- batch: 090 ----
mean loss: 170.24
 ---- batch: 100 ----
mean loss: 170.79
 ---- batch: 110 ----
mean loss: 171.18
train mean loss: 171.87
epoch train time: 0:00:15.541947
elapsed time: 1:03:45.264445
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 08:19:19.461932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.28
 ---- batch: 020 ----
mean loss: 174.70
 ---- batch: 030 ----
mean loss: 173.11
 ---- batch: 040 ----
mean loss: 168.89
 ---- batch: 050 ----
mean loss: 168.12
 ---- batch: 060 ----
mean loss: 173.19
 ---- batch: 070 ----
mean loss: 173.12
 ---- batch: 080 ----
mean loss: 165.84
 ---- batch: 090 ----
mean loss: 171.02
 ---- batch: 100 ----
mean loss: 176.62
 ---- batch: 110 ----
mean loss: 181.64
train mean loss: 171.46
epoch train time: 0:00:15.531876
elapsed time: 1:04:00.797255
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 08:19:34.994650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.33
 ---- batch: 020 ----
mean loss: 169.56
 ---- batch: 030 ----
mean loss: 167.72
 ---- batch: 040 ----
mean loss: 171.40
 ---- batch: 050 ----
mean loss: 172.78
 ---- batch: 060 ----
mean loss: 167.47
 ---- batch: 070 ----
mean loss: 167.38
 ---- batch: 080 ----
mean loss: 172.29
 ---- batch: 090 ----
mean loss: 177.27
 ---- batch: 100 ----
mean loss: 180.92
 ---- batch: 110 ----
mean loss: 172.34
train mean loss: 171.67
epoch train time: 0:00:15.579382
elapsed time: 1:04:16.377555
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 08:19:50.574931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.28
 ---- batch: 020 ----
mean loss: 171.80
 ---- batch: 030 ----
mean loss: 164.59
 ---- batch: 040 ----
mean loss: 172.85
 ---- batch: 050 ----
mean loss: 165.72
 ---- batch: 060 ----
mean loss: 175.17
 ---- batch: 070 ----
mean loss: 170.38
 ---- batch: 080 ----
mean loss: 175.35
 ---- batch: 090 ----
mean loss: 174.20
 ---- batch: 100 ----
mean loss: 175.68
 ---- batch: 110 ----
mean loss: 173.57
train mean loss: 171.46
epoch train time: 0:00:15.562201
elapsed time: 1:04:31.950217
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_7/checkpoint.pth.tar
**** end time: 2019-09-26 08:20:06.147112 ****
