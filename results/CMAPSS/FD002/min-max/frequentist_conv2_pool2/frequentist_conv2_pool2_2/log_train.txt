Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_2', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 23259
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-25 22:32:48.778138 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 16, 11]             560
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
            Conv2d-4            [-1, 14, 7, 11]             224
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
            Linear-8                    [-1, 1]             462
================================================================
Total params: 1,246
Trainable params: 1,246
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 22:32:48.783905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4279.75
 ---- batch: 020 ----
mean loss: 4023.07
 ---- batch: 030 ----
mean loss: 3897.56
 ---- batch: 040 ----
mean loss: 3647.65
 ---- batch: 050 ----
mean loss: 3359.94
 ---- batch: 060 ----
mean loss: 3209.44
 ---- batch: 070 ----
mean loss: 2908.92
 ---- batch: 080 ----
mean loss: 2711.38
 ---- batch: 090 ----
mean loss: 2471.14
train mean loss: 3320.32
epoch train time: 0:00:32.802375
elapsed time: 0:00:32.809382
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 22:33:21.587578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2069.57
 ---- batch: 020 ----
mean loss: 1924.79
 ---- batch: 030 ----
mean loss: 1732.94
 ---- batch: 040 ----
mean loss: 1575.08
 ---- batch: 050 ----
mean loss: 1404.43
 ---- batch: 060 ----
mean loss: 1313.06
 ---- batch: 070 ----
mean loss: 1214.76
 ---- batch: 080 ----
mean loss: 1135.14
 ---- batch: 090 ----
mean loss: 1098.47
train mean loss: 1467.14
epoch train time: 0:00:00.688217
elapsed time: 0:00:33.497768
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 22:33:22.275949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1003.43
 ---- batch: 020 ----
mean loss: 961.47
 ---- batch: 030 ----
mean loss: 948.81
 ---- batch: 040 ----
mean loss: 945.00
 ---- batch: 050 ----
mean loss: 930.33
 ---- batch: 060 ----
mean loss: 898.89
 ---- batch: 070 ----
mean loss: 906.48
 ---- batch: 080 ----
mean loss: 876.59
 ---- batch: 090 ----
mean loss: 892.50
train mean loss: 926.42
epoch train time: 0:00:00.677030
elapsed time: 0:00:34.174950
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 22:33:22.953152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.56
 ---- batch: 020 ----
mean loss: 873.71
 ---- batch: 030 ----
mean loss: 879.66
 ---- batch: 040 ----
mean loss: 886.95
 ---- batch: 050 ----
mean loss: 868.35
 ---- batch: 060 ----
mean loss: 872.37
 ---- batch: 070 ----
mean loss: 891.24
 ---- batch: 080 ----
mean loss: 885.30
 ---- batch: 090 ----
mean loss: 870.20
train mean loss: 880.40
epoch train time: 0:00:00.678910
elapsed time: 0:00:34.854060
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 22:33:23.632243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.19
 ---- batch: 020 ----
mean loss: 863.95
 ---- batch: 030 ----
mean loss: 880.90
 ---- batch: 040 ----
mean loss: 880.48
 ---- batch: 050 ----
mean loss: 860.78
 ---- batch: 060 ----
mean loss: 874.49
 ---- batch: 070 ----
mean loss: 889.79
 ---- batch: 080 ----
mean loss: 882.72
 ---- batch: 090 ----
mean loss: 865.30
train mean loss: 874.74
epoch train time: 0:00:00.676768
elapsed time: 0:00:35.530979
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 22:33:24.309165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.38
 ---- batch: 020 ----
mean loss: 873.82
 ---- batch: 030 ----
mean loss: 883.84
 ---- batch: 040 ----
mean loss: 873.22
 ---- batch: 050 ----
mean loss: 868.82
 ---- batch: 060 ----
mean loss: 848.03
 ---- batch: 070 ----
mean loss: 884.81
 ---- batch: 080 ----
mean loss: 874.57
 ---- batch: 090 ----
mean loss: 862.65
train mean loss: 872.24
epoch train time: 0:00:00.671484
elapsed time: 0:00:36.202621
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 22:33:24.980802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.82
 ---- batch: 020 ----
mean loss: 872.50
 ---- batch: 030 ----
mean loss: 879.91
 ---- batch: 040 ----
mean loss: 879.70
 ---- batch: 050 ----
mean loss: 877.41
 ---- batch: 060 ----
mean loss: 866.92
 ---- batch: 070 ----
mean loss: 864.13
 ---- batch: 080 ----
mean loss: 863.36
 ---- batch: 090 ----
mean loss: 859.83
train mean loss: 868.60
epoch train time: 0:00:00.661520
elapsed time: 0:00:36.864286
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 22:33:25.642475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.71
 ---- batch: 020 ----
mean loss: 870.25
 ---- batch: 030 ----
mean loss: 902.39
 ---- batch: 040 ----
mean loss: 866.93
 ---- batch: 050 ----
mean loss: 860.68
 ---- batch: 060 ----
mean loss: 856.68
 ---- batch: 070 ----
mean loss: 878.17
 ---- batch: 080 ----
mean loss: 859.30
 ---- batch: 090 ----
mean loss: 840.82
train mean loss: 863.78
epoch train time: 0:00:00.672478
elapsed time: 0:00:37.536912
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 22:33:26.315099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.33
 ---- batch: 020 ----
mean loss: 855.12
 ---- batch: 030 ----
mean loss: 864.18
 ---- batch: 040 ----
mean loss: 887.24
 ---- batch: 050 ----
mean loss: 844.62
 ---- batch: 060 ----
mean loss: 853.27
 ---- batch: 070 ----
mean loss: 851.99
 ---- batch: 080 ----
mean loss: 845.32
 ---- batch: 090 ----
mean loss: 870.44
train mean loss: 859.31
epoch train time: 0:00:00.683121
elapsed time: 0:00:38.220183
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 22:33:26.998371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.85
 ---- batch: 020 ----
mean loss: 859.77
 ---- batch: 030 ----
mean loss: 832.19
 ---- batch: 040 ----
mean loss: 852.11
 ---- batch: 050 ----
mean loss: 867.39
 ---- batch: 060 ----
mean loss: 869.21
 ---- batch: 070 ----
mean loss: 856.97
 ---- batch: 080 ----
mean loss: 842.90
 ---- batch: 090 ----
mean loss: 846.57
train mean loss: 854.19
epoch train time: 0:00:00.662962
elapsed time: 0:00:38.883306
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 22:33:27.661525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.21
 ---- batch: 020 ----
mean loss: 860.40
 ---- batch: 030 ----
mean loss: 851.21
 ---- batch: 040 ----
mean loss: 853.76
 ---- batch: 050 ----
mean loss: 842.19
 ---- batch: 060 ----
mean loss: 852.38
 ---- batch: 070 ----
mean loss: 850.50
 ---- batch: 080 ----
mean loss: 835.32
 ---- batch: 090 ----
mean loss: 850.03
train mean loss: 847.27
epoch train time: 0:00:00.675451
elapsed time: 0:00:39.558938
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 22:33:28.337125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 806.48
 ---- batch: 020 ----
mean loss: 833.05
 ---- batch: 030 ----
mean loss: 843.96
 ---- batch: 040 ----
mean loss: 863.60
 ---- batch: 050 ----
mean loss: 858.62
 ---- batch: 060 ----
mean loss: 844.89
 ---- batch: 070 ----
mean loss: 854.43
 ---- batch: 080 ----
mean loss: 836.28
 ---- batch: 090 ----
mean loss: 828.61
train mean loss: 839.55
epoch train time: 0:00:00.674017
elapsed time: 0:00:40.233102
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 22:33:29.011291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.56
 ---- batch: 020 ----
mean loss: 837.94
 ---- batch: 030 ----
mean loss: 839.27
 ---- batch: 040 ----
mean loss: 823.43
 ---- batch: 050 ----
mean loss: 841.10
 ---- batch: 060 ----
mean loss: 839.08
 ---- batch: 070 ----
mean loss: 813.29
 ---- batch: 080 ----
mean loss: 828.27
 ---- batch: 090 ----
mean loss: 843.76
train mean loss: 833.09
epoch train time: 0:00:00.671307
elapsed time: 0:00:40.904554
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 22:33:29.682739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.01
 ---- batch: 020 ----
mean loss: 831.36
 ---- batch: 030 ----
mean loss: 826.63
 ---- batch: 040 ----
mean loss: 803.53
 ---- batch: 050 ----
mean loss: 831.92
 ---- batch: 060 ----
mean loss: 817.22
 ---- batch: 070 ----
mean loss: 844.01
 ---- batch: 080 ----
mean loss: 821.74
 ---- batch: 090 ----
mean loss: 821.86
train mean loss: 825.54
epoch train time: 0:00:00.672874
elapsed time: 0:00:41.577605
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 22:33:30.355790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.05
 ---- batch: 020 ----
mean loss: 820.54
 ---- batch: 030 ----
mean loss: 823.75
 ---- batch: 040 ----
mean loss: 816.19
 ---- batch: 050 ----
mean loss: 810.74
 ---- batch: 060 ----
mean loss: 806.98
 ---- batch: 070 ----
mean loss: 808.68
 ---- batch: 080 ----
mean loss: 827.69
 ---- batch: 090 ----
mean loss: 817.61
train mean loss: 818.84
epoch train time: 0:00:00.670064
elapsed time: 0:00:42.247828
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 22:33:31.026017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.45
 ---- batch: 020 ----
mean loss: 817.43
 ---- batch: 030 ----
mean loss: 806.64
 ---- batch: 040 ----
mean loss: 822.89
 ---- batch: 050 ----
mean loss: 819.25
 ---- batch: 060 ----
mean loss: 802.51
 ---- batch: 070 ----
mean loss: 793.01
 ---- batch: 080 ----
mean loss: 804.94
 ---- batch: 090 ----
mean loss: 812.65
train mean loss: 810.19
epoch train time: 0:00:00.662357
elapsed time: 0:00:42.910339
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 22:33:31.688527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 812.61
 ---- batch: 020 ----
mean loss: 787.03
 ---- batch: 030 ----
mean loss: 801.31
 ---- batch: 040 ----
mean loss: 810.02
 ---- batch: 050 ----
mean loss: 787.77
 ---- batch: 060 ----
mean loss: 808.44
 ---- batch: 070 ----
mean loss: 813.99
 ---- batch: 080 ----
mean loss: 818.45
 ---- batch: 090 ----
mean loss: 784.55
train mean loss: 803.11
epoch train time: 0:00:00.670662
elapsed time: 0:00:43.581172
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 22:33:32.359359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 811.67
 ---- batch: 020 ----
mean loss: 791.92
 ---- batch: 030 ----
mean loss: 809.01
 ---- batch: 040 ----
mean loss: 804.51
 ---- batch: 050 ----
mean loss: 780.55
 ---- batch: 060 ----
mean loss: 779.77
 ---- batch: 070 ----
mean loss: 795.27
 ---- batch: 080 ----
mean loss: 790.60
 ---- batch: 090 ----
mean loss: 790.27
train mean loss: 794.94
epoch train time: 0:00:00.671388
elapsed time: 0:00:44.252705
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 22:33:33.030917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 796.68
 ---- batch: 020 ----
mean loss: 798.38
 ---- batch: 030 ----
mean loss: 774.63
 ---- batch: 040 ----
mean loss: 791.39
 ---- batch: 050 ----
mean loss: 788.80
 ---- batch: 060 ----
mean loss: 786.29
 ---- batch: 070 ----
mean loss: 769.83
 ---- batch: 080 ----
mean loss: 793.32
 ---- batch: 090 ----
mean loss: 792.57
train mean loss: 787.09
epoch train time: 0:00:00.667073
elapsed time: 0:00:44.919946
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 22:33:33.698135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 781.69
 ---- batch: 020 ----
mean loss: 791.85
 ---- batch: 030 ----
mean loss: 783.16
 ---- batch: 040 ----
mean loss: 785.95
 ---- batch: 050 ----
mean loss: 768.84
 ---- batch: 060 ----
mean loss: 780.10
 ---- batch: 070 ----
mean loss: 782.18
 ---- batch: 080 ----
mean loss: 767.81
 ---- batch: 090 ----
mean loss: 769.96
train mean loss: 779.28
epoch train time: 0:00:00.666101
elapsed time: 0:00:45.586200
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 22:33:34.364399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 777.98
 ---- batch: 020 ----
mean loss: 770.15
 ---- batch: 030 ----
mean loss: 764.52
 ---- batch: 040 ----
mean loss: 772.61
 ---- batch: 050 ----
mean loss: 781.44
 ---- batch: 060 ----
mean loss: 766.86
 ---- batch: 070 ----
mean loss: 756.34
 ---- batch: 080 ----
mean loss: 777.49
 ---- batch: 090 ----
mean loss: 772.81
train mean loss: 769.96
epoch train time: 0:00:00.683786
elapsed time: 0:00:46.270201
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 22:33:35.048462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 763.14
 ---- batch: 020 ----
mean loss: 772.79
 ---- batch: 030 ----
mean loss: 757.14
 ---- batch: 040 ----
mean loss: 747.35
 ---- batch: 050 ----
mean loss: 755.00
 ---- batch: 060 ----
mean loss: 775.06
 ---- batch: 070 ----
mean loss: 762.01
 ---- batch: 080 ----
mean loss: 759.53
 ---- batch: 090 ----
mean loss: 763.89
train mean loss: 762.05
epoch train time: 0:00:00.670087
elapsed time: 0:00:46.940510
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 22:33:35.718698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 763.96
 ---- batch: 020 ----
mean loss: 749.72
 ---- batch: 030 ----
mean loss: 741.25
 ---- batch: 040 ----
mean loss: 748.92
 ---- batch: 050 ----
mean loss: 752.90
 ---- batch: 060 ----
mean loss: 751.93
 ---- batch: 070 ----
mean loss: 753.18
 ---- batch: 080 ----
mean loss: 756.19
 ---- batch: 090 ----
mean loss: 756.46
train mean loss: 752.66
epoch train time: 0:00:00.665399
elapsed time: 0:00:47.606056
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 22:33:36.384274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.30
 ---- batch: 020 ----
mean loss: 738.91
 ---- batch: 030 ----
mean loss: 744.61
 ---- batch: 040 ----
mean loss: 733.79
 ---- batch: 050 ----
mean loss: 741.20
 ---- batch: 060 ----
mean loss: 732.35
 ---- batch: 070 ----
mean loss: 747.83
 ---- batch: 080 ----
mean loss: 750.20
 ---- batch: 090 ----
mean loss: 738.90
train mean loss: 744.24
epoch train time: 0:00:00.672463
elapsed time: 0:00:48.278697
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 22:33:37.056911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 730.24
 ---- batch: 020 ----
mean loss: 748.76
 ---- batch: 030 ----
mean loss: 739.81
 ---- batch: 040 ----
mean loss: 740.69
 ---- batch: 050 ----
mean loss: 740.04
 ---- batch: 060 ----
mean loss: 733.09
 ---- batch: 070 ----
mean loss: 728.95
 ---- batch: 080 ----
mean loss: 726.22
 ---- batch: 090 ----
mean loss: 735.97
train mean loss: 734.17
epoch train time: 0:00:00.665671
elapsed time: 0:00:48.944592
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 22:33:37.722799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 721.07
 ---- batch: 020 ----
mean loss: 723.36
 ---- batch: 030 ----
mean loss: 719.81
 ---- batch: 040 ----
mean loss: 715.66
 ---- batch: 050 ----
mean loss: 718.44
 ---- batch: 060 ----
mean loss: 743.41
 ---- batch: 070 ----
mean loss: 728.55
 ---- batch: 080 ----
mean loss: 723.44
 ---- batch: 090 ----
mean loss: 717.54
train mean loss: 724.27
epoch train time: 0:00:00.676635
elapsed time: 0:00:49.621391
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 22:33:38.399579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 722.31
 ---- batch: 020 ----
mean loss: 724.00
 ---- batch: 030 ----
mean loss: 710.99
 ---- batch: 040 ----
mean loss: 713.37
 ---- batch: 050 ----
mean loss: 701.64
 ---- batch: 060 ----
mean loss: 713.05
 ---- batch: 070 ----
mean loss: 717.81
 ---- batch: 080 ----
mean loss: 720.02
 ---- batch: 090 ----
mean loss: 717.63
train mean loss: 714.33
epoch train time: 0:00:00.691793
elapsed time: 0:00:50.313349
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 22:33:39.091537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.68
 ---- batch: 020 ----
mean loss: 703.24
 ---- batch: 030 ----
mean loss: 705.60
 ---- batch: 040 ----
mean loss: 714.07
 ---- batch: 050 ----
mean loss: 708.22
 ---- batch: 060 ----
mean loss: 695.91
 ---- batch: 070 ----
mean loss: 691.07
 ---- batch: 080 ----
mean loss: 717.32
 ---- batch: 090 ----
mean loss: 687.64
train mean loss: 703.94
epoch train time: 0:00:00.687676
elapsed time: 0:00:51.001205
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 22:33:39.779387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.33
 ---- batch: 020 ----
mean loss: 693.59
 ---- batch: 030 ----
mean loss: 684.87
 ---- batch: 040 ----
mean loss: 695.53
 ---- batch: 050 ----
mean loss: 699.95
 ---- batch: 060 ----
mean loss: 700.37
 ---- batch: 070 ----
mean loss: 709.11
 ---- batch: 080 ----
mean loss: 687.54
 ---- batch: 090 ----
mean loss: 679.68
train mean loss: 694.39
epoch train time: 0:00:00.685615
elapsed time: 0:00:51.686969
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 22:33:40.465151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 699.39
 ---- batch: 020 ----
mean loss: 692.45
 ---- batch: 030 ----
mean loss: 683.93
 ---- batch: 040 ----
mean loss: 683.05
 ---- batch: 050 ----
mean loss: 688.22
 ---- batch: 060 ----
mean loss: 694.42
 ---- batch: 070 ----
mean loss: 679.36
 ---- batch: 080 ----
mean loss: 680.06
 ---- batch: 090 ----
mean loss: 658.31
train mean loss: 683.44
epoch train time: 0:00:00.690133
elapsed time: 0:00:52.377248
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 22:33:41.155439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.32
 ---- batch: 020 ----
mean loss: 675.10
 ---- batch: 030 ----
mean loss: 672.01
 ---- batch: 040 ----
mean loss: 678.66
 ---- batch: 050 ----
mean loss: 690.45
 ---- batch: 060 ----
mean loss: 666.15
 ---- batch: 070 ----
mean loss: 683.55
 ---- batch: 080 ----
mean loss: 669.40
 ---- batch: 090 ----
mean loss: 658.35
train mean loss: 673.17
epoch train time: 0:00:00.683127
elapsed time: 0:00:53.060537
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 22:33:41.838742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 670.91
 ---- batch: 020 ----
mean loss: 672.79
 ---- batch: 030 ----
mean loss: 666.45
 ---- batch: 040 ----
mean loss: 655.39
 ---- batch: 050 ----
mean loss: 655.57
 ---- batch: 060 ----
mean loss: 669.88
 ---- batch: 070 ----
mean loss: 647.15
 ---- batch: 080 ----
mean loss: 676.07
 ---- batch: 090 ----
mean loss: 656.01
train mean loss: 662.92
epoch train time: 0:00:00.682676
elapsed time: 0:00:53.743419
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 22:33:42.521645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 658.28
 ---- batch: 020 ----
mean loss: 660.26
 ---- batch: 030 ----
mean loss: 655.85
 ---- batch: 040 ----
mean loss: 653.30
 ---- batch: 050 ----
mean loss: 645.60
 ---- batch: 060 ----
mean loss: 648.27
 ---- batch: 070 ----
mean loss: 651.25
 ---- batch: 080 ----
mean loss: 650.54
 ---- batch: 090 ----
mean loss: 645.79
train mean loss: 653.13
epoch train time: 0:00:00.694046
elapsed time: 0:00:54.437648
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 22:33:43.215855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 650.46
 ---- batch: 020 ----
mean loss: 650.82
 ---- batch: 030 ----
mean loss: 648.08
 ---- batch: 040 ----
mean loss: 643.52
 ---- batch: 050 ----
mean loss: 649.92
 ---- batch: 060 ----
mean loss: 640.36
 ---- batch: 070 ----
mean loss: 640.67
 ---- batch: 080 ----
mean loss: 628.01
 ---- batch: 090 ----
mean loss: 634.45
train mean loss: 642.69
epoch train time: 0:00:00.681223
elapsed time: 0:00:55.119052
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 22:33:43.897244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 635.36
 ---- batch: 020 ----
mean loss: 635.78
 ---- batch: 030 ----
mean loss: 626.39
 ---- batch: 040 ----
mean loss: 632.47
 ---- batch: 050 ----
mean loss: 629.35
 ---- batch: 060 ----
mean loss: 637.40
 ---- batch: 070 ----
mean loss: 634.19
 ---- batch: 080 ----
mean loss: 625.59
 ---- batch: 090 ----
mean loss: 640.73
train mean loss: 632.78
epoch train time: 0:00:00.675018
elapsed time: 0:00:55.794280
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 22:33:44.572471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.40
 ---- batch: 020 ----
mean loss: 624.98
 ---- batch: 030 ----
mean loss: 631.97
 ---- batch: 040 ----
mean loss: 631.47
 ---- batch: 050 ----
mean loss: 627.46
 ---- batch: 060 ----
mean loss: 607.20
 ---- batch: 070 ----
mean loss: 611.27
 ---- batch: 080 ----
mean loss: 613.65
 ---- batch: 090 ----
mean loss: 636.30
train mean loss: 622.65
epoch train time: 0:00:00.667670
elapsed time: 0:00:56.462101
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 22:33:45.240289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.74
 ---- batch: 020 ----
mean loss: 617.17
 ---- batch: 030 ----
mean loss: 611.88
 ---- batch: 040 ----
mean loss: 623.86
 ---- batch: 050 ----
mean loss: 619.74
 ---- batch: 060 ----
mean loss: 607.35
 ---- batch: 070 ----
mean loss: 614.43
 ---- batch: 080 ----
mean loss: 592.40
 ---- batch: 090 ----
mean loss: 600.63
train mean loss: 613.14
epoch train time: 0:00:00.669487
elapsed time: 0:00:57.131735
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 22:33:45.909932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 598.70
 ---- batch: 020 ----
mean loss: 613.29
 ---- batch: 030 ----
mean loss: 602.46
 ---- batch: 040 ----
mean loss: 615.04
 ---- batch: 050 ----
mean loss: 600.61
 ---- batch: 060 ----
mean loss: 601.79
 ---- batch: 070 ----
mean loss: 604.87
 ---- batch: 080 ----
mean loss: 605.13
 ---- batch: 090 ----
mean loss: 588.28
train mean loss: 603.69
epoch train time: 0:00:00.670658
elapsed time: 0:00:57.802563
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 22:33:46.580767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 600.52
 ---- batch: 020 ----
mean loss: 596.06
 ---- batch: 030 ----
mean loss: 592.95
 ---- batch: 040 ----
mean loss: 598.19
 ---- batch: 050 ----
mean loss: 589.38
 ---- batch: 060 ----
mean loss: 598.23
 ---- batch: 070 ----
mean loss: 600.61
 ---- batch: 080 ----
mean loss: 589.64
 ---- batch: 090 ----
mean loss: 590.90
train mean loss: 595.05
epoch train time: 0:00:00.676242
elapsed time: 0:00:58.478976
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 22:33:47.257169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 592.90
 ---- batch: 020 ----
mean loss: 579.64
 ---- batch: 030 ----
mean loss: 581.72
 ---- batch: 040 ----
mean loss: 594.32
 ---- batch: 050 ----
mean loss: 588.31
 ---- batch: 060 ----
mean loss: 585.15
 ---- batch: 070 ----
mean loss: 580.62
 ---- batch: 080 ----
mean loss: 578.20
 ---- batch: 090 ----
mean loss: 586.75
train mean loss: 585.74
epoch train time: 0:00:00.670317
elapsed time: 0:00:59.149448
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 22:33:47.927654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.34
 ---- batch: 020 ----
mean loss: 582.34
 ---- batch: 030 ----
mean loss: 574.67
 ---- batch: 040 ----
mean loss: 573.45
 ---- batch: 050 ----
mean loss: 576.97
 ---- batch: 060 ----
mean loss: 581.15
 ---- batch: 070 ----
mean loss: 579.14
 ---- batch: 080 ----
mean loss: 572.66
 ---- batch: 090 ----
mean loss: 565.23
train mean loss: 577.07
epoch train time: 0:00:00.675837
elapsed time: 0:00:59.825446
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 22:33:48.603650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.04
 ---- batch: 020 ----
mean loss: 572.38
 ---- batch: 030 ----
mean loss: 572.37
 ---- batch: 040 ----
mean loss: 571.74
 ---- batch: 050 ----
mean loss: 560.23
 ---- batch: 060 ----
mean loss: 579.00
 ---- batch: 070 ----
mean loss: 562.52
 ---- batch: 080 ----
mean loss: 560.80
 ---- batch: 090 ----
mean loss: 571.20
train mean loss: 568.68
epoch train time: 0:00:00.677890
elapsed time: 0:01:00.503601
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 22:33:49.281831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.29
 ---- batch: 020 ----
mean loss: 552.46
 ---- batch: 030 ----
mean loss: 563.75
 ---- batch: 040 ----
mean loss: 542.77
 ---- batch: 050 ----
mean loss: 558.72
 ---- batch: 060 ----
mean loss: 562.78
 ---- batch: 070 ----
mean loss: 559.23
 ---- batch: 080 ----
mean loss: 568.33
 ---- batch: 090 ----
mean loss: 564.05
train mean loss: 560.55
epoch train time: 0:00:00.674545
elapsed time: 0:01:01.178367
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 22:33:49.956557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.29
 ---- batch: 020 ----
mean loss: 559.04
 ---- batch: 030 ----
mean loss: 559.21
 ---- batch: 040 ----
mean loss: 554.88
 ---- batch: 050 ----
mean loss: 547.83
 ---- batch: 060 ----
mean loss: 548.92
 ---- batch: 070 ----
mean loss: 547.95
 ---- batch: 080 ----
mean loss: 549.20
 ---- batch: 090 ----
mean loss: 551.77
train mean loss: 552.28
epoch train time: 0:00:00.669305
elapsed time: 0:01:01.847818
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 22:33:50.626007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.69
 ---- batch: 020 ----
mean loss: 538.86
 ---- batch: 030 ----
mean loss: 564.45
 ---- batch: 040 ----
mean loss: 540.50
 ---- batch: 050 ----
mean loss: 538.17
 ---- batch: 060 ----
mean loss: 545.18
 ---- batch: 070 ----
mean loss: 547.84
 ---- batch: 080 ----
mean loss: 533.06
 ---- batch: 090 ----
mean loss: 546.09
train mean loss: 544.24
epoch train time: 0:00:00.667499
elapsed time: 0:01:02.515529
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 22:33:51.293718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 539.77
 ---- batch: 020 ----
mean loss: 545.89
 ---- batch: 030 ----
mean loss: 537.16
 ---- batch: 040 ----
mean loss: 539.62
 ---- batch: 050 ----
mean loss: 532.16
 ---- batch: 060 ----
mean loss: 530.73
 ---- batch: 070 ----
mean loss: 542.71
 ---- batch: 080 ----
mean loss: 527.58
 ---- batch: 090 ----
mean loss: 532.96
train mean loss: 536.07
epoch train time: 0:00:00.661314
elapsed time: 0:01:03.177046
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 22:33:51.955241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 532.01
 ---- batch: 020 ----
mean loss: 530.29
 ---- batch: 030 ----
mean loss: 534.78
 ---- batch: 040 ----
mean loss: 523.41
 ---- batch: 050 ----
mean loss: 534.69
 ---- batch: 060 ----
mean loss: 532.60
 ---- batch: 070 ----
mean loss: 530.45
 ---- batch: 080 ----
mean loss: 512.72
 ---- batch: 090 ----
mean loss: 519.74
train mean loss: 527.89
epoch train time: 0:00:00.674659
elapsed time: 0:01:03.851866
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 22:33:52.630065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 522.66
 ---- batch: 020 ----
mean loss: 519.43
 ---- batch: 030 ----
mean loss: 521.30
 ---- batch: 040 ----
mean loss: 512.41
 ---- batch: 050 ----
mean loss: 537.29
 ---- batch: 060 ----
mean loss: 510.88
 ---- batch: 070 ----
mean loss: 522.11
 ---- batch: 080 ----
mean loss: 510.46
 ---- batch: 090 ----
mean loss: 521.16
train mean loss: 519.69
epoch train time: 0:00:00.680253
elapsed time: 0:01:04.532275
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 22:33:53.310483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.99
 ---- batch: 020 ----
mean loss: 504.74
 ---- batch: 030 ----
mean loss: 503.60
 ---- batch: 040 ----
mean loss: 497.65
 ---- batch: 050 ----
mean loss: 509.94
 ---- batch: 060 ----
mean loss: 496.95
 ---- batch: 070 ----
mean loss: 507.68
 ---- batch: 080 ----
mean loss: 503.87
 ---- batch: 090 ----
mean loss: 501.18
train mean loss: 503.14
epoch train time: 0:00:00.666905
elapsed time: 0:01:05.199354
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 22:33:53.977543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.94
 ---- batch: 020 ----
mean loss: 476.25
 ---- batch: 030 ----
mean loss: 483.08
 ---- batch: 040 ----
mean loss: 485.02
 ---- batch: 050 ----
mean loss: 464.57
 ---- batch: 060 ----
mean loss: 481.81
 ---- batch: 070 ----
mean loss: 463.03
 ---- batch: 080 ----
mean loss: 470.48
 ---- batch: 090 ----
mean loss: 462.61
train mean loss: 474.25
epoch train time: 0:00:00.665965
elapsed time: 0:01:05.865478
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 22:33:54.643696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.28
 ---- batch: 020 ----
mean loss: 455.06
 ---- batch: 030 ----
mean loss: 458.41
 ---- batch: 040 ----
mean loss: 448.91
 ---- batch: 050 ----
mean loss: 440.60
 ---- batch: 060 ----
mean loss: 444.47
 ---- batch: 070 ----
mean loss: 444.32
 ---- batch: 080 ----
mean loss: 436.88
 ---- batch: 090 ----
mean loss: 437.92
train mean loss: 446.53
epoch train time: 0:00:00.671869
elapsed time: 0:01:06.537531
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 22:33:55.315722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.25
 ---- batch: 020 ----
mean loss: 426.01
 ---- batch: 030 ----
mean loss: 431.95
 ---- batch: 040 ----
mean loss: 426.06
 ---- batch: 050 ----
mean loss: 411.53
 ---- batch: 060 ----
mean loss: 414.61
 ---- batch: 070 ----
mean loss: 416.36
 ---- batch: 080 ----
mean loss: 421.32
 ---- batch: 090 ----
mean loss: 421.22
train mean loss: 422.15
epoch train time: 0:00:00.673607
elapsed time: 0:01:07.211288
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 22:33:55.989478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.81
 ---- batch: 020 ----
mean loss: 407.05
 ---- batch: 030 ----
mean loss: 405.94
 ---- batch: 040 ----
mean loss: 394.05
 ---- batch: 050 ----
mean loss: 408.57
 ---- batch: 060 ----
mean loss: 407.41
 ---- batch: 070 ----
mean loss: 387.95
 ---- batch: 080 ----
mean loss: 396.19
 ---- batch: 090 ----
mean loss: 393.47
train mean loss: 400.32
epoch train time: 0:00:00.671980
elapsed time: 0:01:07.883416
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 22:33:56.661624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.69
 ---- batch: 020 ----
mean loss: 386.15
 ---- batch: 030 ----
mean loss: 399.67
 ---- batch: 040 ----
mean loss: 382.12
 ---- batch: 050 ----
mean loss: 381.87
 ---- batch: 060 ----
mean loss: 381.00
 ---- batch: 070 ----
mean loss: 369.98
 ---- batch: 080 ----
mean loss: 386.62
 ---- batch: 090 ----
mean loss: 362.63
train mean loss: 382.00
epoch train time: 0:00:00.671721
elapsed time: 0:01:08.555305
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 22:33:57.333495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.53
 ---- batch: 020 ----
mean loss: 366.57
 ---- batch: 030 ----
mean loss: 375.01
 ---- batch: 040 ----
mean loss: 358.67
 ---- batch: 050 ----
mean loss: 368.68
 ---- batch: 060 ----
mean loss: 370.56
 ---- batch: 070 ----
mean loss: 369.10
 ---- batch: 080 ----
mean loss: 352.83
 ---- batch: 090 ----
mean loss: 363.31
train mean loss: 366.25
epoch train time: 0:00:00.667870
elapsed time: 0:01:09.223329
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 22:33:58.001517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.35
 ---- batch: 020 ----
mean loss: 353.54
 ---- batch: 030 ----
mean loss: 356.08
 ---- batch: 040 ----
mean loss: 358.18
 ---- batch: 050 ----
mean loss: 357.01
 ---- batch: 060 ----
mean loss: 352.01
 ---- batch: 070 ----
mean loss: 362.31
 ---- batch: 080 ----
mean loss: 334.89
 ---- batch: 090 ----
mean loss: 345.26
train mean loss: 353.21
epoch train time: 0:00:00.664225
elapsed time: 0:01:09.887703
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 22:33:58.665914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.47
 ---- batch: 020 ----
mean loss: 338.82
 ---- batch: 030 ----
mean loss: 355.93
 ---- batch: 040 ----
mean loss: 343.06
 ---- batch: 050 ----
mean loss: 347.25
 ---- batch: 060 ----
mean loss: 341.08
 ---- batch: 070 ----
mean loss: 339.16
 ---- batch: 080 ----
mean loss: 337.54
 ---- batch: 090 ----
mean loss: 334.73
train mean loss: 342.48
epoch train time: 0:00:00.662450
elapsed time: 0:01:10.550321
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 22:33:59.328509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.15
 ---- batch: 020 ----
mean loss: 345.06
 ---- batch: 030 ----
mean loss: 341.70
 ---- batch: 040 ----
mean loss: 329.56
 ---- batch: 050 ----
mean loss: 344.28
 ---- batch: 060 ----
mean loss: 334.48
 ---- batch: 070 ----
mean loss: 322.28
 ---- batch: 080 ----
mean loss: 327.40
 ---- batch: 090 ----
mean loss: 330.87
train mean loss: 333.55
epoch train time: 0:00:00.666544
elapsed time: 0:01:11.217033
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 22:33:59.995228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.17
 ---- batch: 020 ----
mean loss: 320.79
 ---- batch: 030 ----
mean loss: 328.88
 ---- batch: 040 ----
mean loss: 328.35
 ---- batch: 050 ----
mean loss: 321.60
 ---- batch: 060 ----
mean loss: 323.17
 ---- batch: 070 ----
mean loss: 324.85
 ---- batch: 080 ----
mean loss: 329.27
 ---- batch: 090 ----
mean loss: 328.27
train mean loss: 325.81
epoch train time: 0:00:00.679074
elapsed time: 0:01:11.896271
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 22:34:00.674464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.61
 ---- batch: 020 ----
mean loss: 323.34
 ---- batch: 030 ----
mean loss: 316.74
 ---- batch: 040 ----
mean loss: 319.79
 ---- batch: 050 ----
mean loss: 318.44
 ---- batch: 060 ----
mean loss: 326.38
 ---- batch: 070 ----
mean loss: 314.19
 ---- batch: 080 ----
mean loss: 317.03
 ---- batch: 090 ----
mean loss: 319.45
train mean loss: 319.99
epoch train time: 0:00:00.679383
elapsed time: 0:01:12.575813
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 22:34:01.354044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.77
 ---- batch: 020 ----
mean loss: 308.78
 ---- batch: 030 ----
mean loss: 309.36
 ---- batch: 040 ----
mean loss: 319.65
 ---- batch: 050 ----
mean loss: 309.25
 ---- batch: 060 ----
mean loss: 308.07
 ---- batch: 070 ----
mean loss: 324.20
 ---- batch: 080 ----
mean loss: 320.90
 ---- batch: 090 ----
mean loss: 320.79
train mean loss: 313.98
epoch train time: 0:00:00.669686
elapsed time: 0:01:13.245700
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 22:34:02.023880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.29
 ---- batch: 020 ----
mean loss: 306.59
 ---- batch: 030 ----
mean loss: 310.69
 ---- batch: 040 ----
mean loss: 297.23
 ---- batch: 050 ----
mean loss: 306.34
 ---- batch: 060 ----
mean loss: 316.99
 ---- batch: 070 ----
mean loss: 307.71
 ---- batch: 080 ----
mean loss: 306.31
 ---- batch: 090 ----
mean loss: 321.15
train mean loss: 309.35
epoch train time: 0:00:00.666141
elapsed time: 0:01:13.911980
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 22:34:02.690168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.98
 ---- batch: 020 ----
mean loss: 298.04
 ---- batch: 030 ----
mean loss: 308.17
 ---- batch: 040 ----
mean loss: 300.54
 ---- batch: 050 ----
mean loss: 307.72
 ---- batch: 060 ----
mean loss: 307.40
 ---- batch: 070 ----
mean loss: 307.19
 ---- batch: 080 ----
mean loss: 307.92
 ---- batch: 090 ----
mean loss: 300.91
train mean loss: 305.20
epoch train time: 0:00:00.672965
elapsed time: 0:01:14.585104
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 22:34:03.363319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.58
 ---- batch: 020 ----
mean loss: 301.16
 ---- batch: 030 ----
mean loss: 302.76
 ---- batch: 040 ----
mean loss: 301.70
 ---- batch: 050 ----
mean loss: 312.96
 ---- batch: 060 ----
mean loss: 297.07
 ---- batch: 070 ----
mean loss: 295.55
 ---- batch: 080 ----
mean loss: 300.13
 ---- batch: 090 ----
mean loss: 296.98
train mean loss: 301.31
epoch train time: 0:00:00.668654
elapsed time: 0:01:15.253947
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 22:34:04.032137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.28
 ---- batch: 020 ----
mean loss: 300.21
 ---- batch: 030 ----
mean loss: 297.81
 ---- batch: 040 ----
mean loss: 304.62
 ---- batch: 050 ----
mean loss: 305.93
 ---- batch: 060 ----
mean loss: 297.06
 ---- batch: 070 ----
mean loss: 289.33
 ---- batch: 080 ----
mean loss: 297.73
 ---- batch: 090 ----
mean loss: 294.32
train mean loss: 298.41
epoch train time: 0:00:00.676758
elapsed time: 0:01:15.930859
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 22:34:04.709074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.43
 ---- batch: 020 ----
mean loss: 296.16
 ---- batch: 030 ----
mean loss: 301.83
 ---- batch: 040 ----
mean loss: 289.25
 ---- batch: 050 ----
mean loss: 293.63
 ---- batch: 060 ----
mean loss: 301.45
 ---- batch: 070 ----
mean loss: 289.54
 ---- batch: 080 ----
mean loss: 294.67
 ---- batch: 090 ----
mean loss: 297.18
train mean loss: 294.46
epoch train time: 0:00:00.687044
elapsed time: 0:01:16.618084
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 22:34:05.396268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.37
 ---- batch: 020 ----
mean loss: 286.58
 ---- batch: 030 ----
mean loss: 304.92
 ---- batch: 040 ----
mean loss: 300.82
 ---- batch: 050 ----
mean loss: 296.40
 ---- batch: 060 ----
mean loss: 290.90
 ---- batch: 070 ----
mean loss: 288.69
 ---- batch: 080 ----
mean loss: 287.43
 ---- batch: 090 ----
mean loss: 286.86
train mean loss: 292.13
epoch train time: 0:00:00.687448
elapsed time: 0:01:17.305720
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 22:34:06.083991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.56
 ---- batch: 020 ----
mean loss: 289.15
 ---- batch: 030 ----
mean loss: 281.88
 ---- batch: 040 ----
mean loss: 281.06
 ---- batch: 050 ----
mean loss: 292.65
 ---- batch: 060 ----
mean loss: 296.96
 ---- batch: 070 ----
mean loss: 299.70
 ---- batch: 080 ----
mean loss: 293.78
 ---- batch: 090 ----
mean loss: 281.65
train mean loss: 289.11
epoch train time: 0:00:00.677205
elapsed time: 0:01:17.983157
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 22:34:06.761347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.65
 ---- batch: 020 ----
mean loss: 290.10
 ---- batch: 030 ----
mean loss: 282.40
 ---- batch: 040 ----
mean loss: 284.61
 ---- batch: 050 ----
mean loss: 284.28
 ---- batch: 060 ----
mean loss: 285.62
 ---- batch: 070 ----
mean loss: 289.58
 ---- batch: 080 ----
mean loss: 281.57
 ---- batch: 090 ----
mean loss: 290.53
train mean loss: 287.00
epoch train time: 0:00:00.673889
elapsed time: 0:01:18.657194
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 22:34:07.435399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.50
 ---- batch: 020 ----
mean loss: 278.54
 ---- batch: 030 ----
mean loss: 284.30
 ---- batch: 040 ----
mean loss: 284.21
 ---- batch: 050 ----
mean loss: 281.40
 ---- batch: 060 ----
mean loss: 275.83
 ---- batch: 070 ----
mean loss: 272.99
 ---- batch: 080 ----
mean loss: 296.19
 ---- batch: 090 ----
mean loss: 294.21
train mean loss: 284.65
epoch train time: 0:00:00.673648
elapsed time: 0:01:19.331005
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 22:34:08.109194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.20
 ---- batch: 020 ----
mean loss: 279.77
 ---- batch: 030 ----
mean loss: 288.69
 ---- batch: 040 ----
mean loss: 293.28
 ---- batch: 050 ----
mean loss: 287.43
 ---- batch: 060 ----
mean loss: 276.31
 ---- batch: 070 ----
mean loss: 283.50
 ---- batch: 080 ----
mean loss: 276.79
 ---- batch: 090 ----
mean loss: 283.25
train mean loss: 282.34
epoch train time: 0:00:00.671374
elapsed time: 0:01:20.002545
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 22:34:08.780766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.68
 ---- batch: 020 ----
mean loss: 274.06
 ---- batch: 030 ----
mean loss: 286.32
 ---- batch: 040 ----
mean loss: 281.55
 ---- batch: 050 ----
mean loss: 286.92
 ---- batch: 060 ----
mean loss: 288.46
 ---- batch: 070 ----
mean loss: 271.48
 ---- batch: 080 ----
mean loss: 278.06
 ---- batch: 090 ----
mean loss: 278.76
train mean loss: 280.55
epoch train time: 0:00:00.671644
elapsed time: 0:01:20.674386
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 22:34:09.452578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.81
 ---- batch: 020 ----
mean loss: 269.23
 ---- batch: 030 ----
mean loss: 271.63
 ---- batch: 040 ----
mean loss: 283.10
 ---- batch: 050 ----
mean loss: 284.98
 ---- batch: 060 ----
mean loss: 280.56
 ---- batch: 070 ----
mean loss: 278.89
 ---- batch: 080 ----
mean loss: 283.57
 ---- batch: 090 ----
mean loss: 280.02
train mean loss: 278.56
epoch train time: 0:00:00.677006
elapsed time: 0:01:21.351568
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 22:34:10.129762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.63
 ---- batch: 020 ----
mean loss: 276.70
 ---- batch: 030 ----
mean loss: 287.15
 ---- batch: 040 ----
mean loss: 273.45
 ---- batch: 050 ----
mean loss: 272.39
 ---- batch: 060 ----
mean loss: 275.12
 ---- batch: 070 ----
mean loss: 276.44
 ---- batch: 080 ----
mean loss: 283.51
 ---- batch: 090 ----
mean loss: 275.80
train mean loss: 276.58
epoch train time: 0:00:00.673747
elapsed time: 0:01:22.025470
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 22:34:10.803668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.76
 ---- batch: 020 ----
mean loss: 278.05
 ---- batch: 030 ----
mean loss: 283.80
 ---- batch: 040 ----
mean loss: 271.94
 ---- batch: 050 ----
mean loss: 269.92
 ---- batch: 060 ----
mean loss: 278.78
 ---- batch: 070 ----
mean loss: 274.06
 ---- batch: 080 ----
mean loss: 272.15
 ---- batch: 090 ----
mean loss: 273.45
train mean loss: 275.13
epoch train time: 0:00:00.666455
elapsed time: 0:01:22.692085
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 22:34:11.470275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.06
 ---- batch: 020 ----
mean loss: 276.72
 ---- batch: 030 ----
mean loss: 273.91
 ---- batch: 040 ----
mean loss: 277.11
 ---- batch: 050 ----
mean loss: 264.84
 ---- batch: 060 ----
mean loss: 274.33
 ---- batch: 070 ----
mean loss: 280.45
 ---- batch: 080 ----
mean loss: 274.81
 ---- batch: 090 ----
mean loss: 278.39
train mean loss: 274.06
epoch train time: 0:00:00.670065
elapsed time: 0:01:23.362295
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 22:34:12.140484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.28
 ---- batch: 020 ----
mean loss: 272.59
 ---- batch: 030 ----
mean loss: 256.24
 ---- batch: 040 ----
mean loss: 274.23
 ---- batch: 050 ----
mean loss: 278.91
 ---- batch: 060 ----
mean loss: 267.18
 ---- batch: 070 ----
mean loss: 275.85
 ---- batch: 080 ----
mean loss: 278.55
 ---- batch: 090 ----
mean loss: 267.77
train mean loss: 272.47
epoch train time: 0:00:00.671929
elapsed time: 0:01:24.034409
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 22:34:12.812627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.41
 ---- batch: 020 ----
mean loss: 275.67
 ---- batch: 030 ----
mean loss: 262.80
 ---- batch: 040 ----
mean loss: 279.98
 ---- batch: 050 ----
mean loss: 267.12
 ---- batch: 060 ----
mean loss: 268.12
 ---- batch: 070 ----
mean loss: 272.11
 ---- batch: 080 ----
mean loss: 272.92
 ---- batch: 090 ----
mean loss: 271.71
train mean loss: 271.10
epoch train time: 0:00:00.674898
elapsed time: 0:01:24.709483
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 22:34:13.487688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.52
 ---- batch: 020 ----
mean loss: 264.82
 ---- batch: 030 ----
mean loss: 265.23
 ---- batch: 040 ----
mean loss: 283.61
 ---- batch: 050 ----
mean loss: 273.02
 ---- batch: 060 ----
mean loss: 263.83
 ---- batch: 070 ----
mean loss: 283.64
 ---- batch: 080 ----
mean loss: 267.02
 ---- batch: 090 ----
mean loss: 271.06
train mean loss: 269.96
epoch train time: 0:00:00.683606
elapsed time: 0:01:25.393252
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 22:34:14.171441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.03
 ---- batch: 020 ----
mean loss: 268.72
 ---- batch: 030 ----
mean loss: 268.95
 ---- batch: 040 ----
mean loss: 277.05
 ---- batch: 050 ----
mean loss: 262.64
 ---- batch: 060 ----
mean loss: 278.31
 ---- batch: 070 ----
mean loss: 258.06
 ---- batch: 080 ----
mean loss: 267.35
 ---- batch: 090 ----
mean loss: 270.35
train mean loss: 268.40
epoch train time: 0:00:00.683451
elapsed time: 0:01:26.076861
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 22:34:14.855057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.24
 ---- batch: 020 ----
mean loss: 264.66
 ---- batch: 030 ----
mean loss: 267.72
 ---- batch: 040 ----
mean loss: 265.34
 ---- batch: 050 ----
mean loss: 266.28
 ---- batch: 060 ----
mean loss: 261.80
 ---- batch: 070 ----
mean loss: 266.88
 ---- batch: 080 ----
mean loss: 269.55
 ---- batch: 090 ----
mean loss: 269.70
train mean loss: 267.41
epoch train time: 0:00:00.670232
elapsed time: 0:01:26.747267
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 22:34:15.525489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.94
 ---- batch: 020 ----
mean loss: 273.09
 ---- batch: 030 ----
mean loss: 273.26
 ---- batch: 040 ----
mean loss: 264.37
 ---- batch: 050 ----
mean loss: 277.19
 ---- batch: 060 ----
mean loss: 259.32
 ---- batch: 070 ----
mean loss: 263.01
 ---- batch: 080 ----
mean loss: 254.83
 ---- batch: 090 ----
mean loss: 266.31
train mean loss: 266.01
epoch train time: 0:00:00.665651
elapsed time: 0:01:27.413102
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 22:34:16.191291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.80
 ---- batch: 020 ----
mean loss: 260.64
 ---- batch: 030 ----
mean loss: 259.33
 ---- batch: 040 ----
mean loss: 262.80
 ---- batch: 050 ----
mean loss: 265.40
 ---- batch: 060 ----
mean loss: 270.96
 ---- batch: 070 ----
mean loss: 260.45
 ---- batch: 080 ----
mean loss: 274.68
 ---- batch: 090 ----
mean loss: 264.76
train mean loss: 264.71
epoch train time: 0:00:00.678369
elapsed time: 0:01:28.091627
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 22:34:16.869836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.37
 ---- batch: 020 ----
mean loss: 255.58
 ---- batch: 030 ----
mean loss: 266.35
 ---- batch: 040 ----
mean loss: 261.25
 ---- batch: 050 ----
mean loss: 268.78
 ---- batch: 060 ----
mean loss: 267.52
 ---- batch: 070 ----
mean loss: 261.46
 ---- batch: 080 ----
mean loss: 262.10
 ---- batch: 090 ----
mean loss: 262.49
train mean loss: 263.93
epoch train time: 0:00:00.674371
elapsed time: 0:01:28.766163
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 22:34:17.544366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.99
 ---- batch: 020 ----
mean loss: 258.05
 ---- batch: 030 ----
mean loss: 264.21
 ---- batch: 040 ----
mean loss: 251.06
 ---- batch: 050 ----
mean loss: 268.49
 ---- batch: 060 ----
mean loss: 265.20
 ---- batch: 070 ----
mean loss: 261.04
 ---- batch: 080 ----
mean loss: 262.96
 ---- batch: 090 ----
mean loss: 271.07
train mean loss: 262.42
epoch train time: 0:00:00.668967
elapsed time: 0:01:29.435304
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 22:34:18.213493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.04
 ---- batch: 020 ----
mean loss: 270.70
 ---- batch: 030 ----
mean loss: 257.88
 ---- batch: 040 ----
mean loss: 265.89
 ---- batch: 050 ----
mean loss: 260.46
 ---- batch: 060 ----
mean loss: 253.61
 ---- batch: 070 ----
mean loss: 247.64
 ---- batch: 080 ----
mean loss: 258.26
 ---- batch: 090 ----
mean loss: 273.05
train mean loss: 262.05
epoch train time: 0:00:00.676080
elapsed time: 0:01:30.111610
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 22:34:18.889831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.46
 ---- batch: 020 ----
mean loss: 259.72
 ---- batch: 030 ----
mean loss: 262.65
 ---- batch: 040 ----
mean loss: 266.23
 ---- batch: 050 ----
mean loss: 259.28
 ---- batch: 060 ----
mean loss: 260.33
 ---- batch: 070 ----
mean loss: 257.97
 ---- batch: 080 ----
mean loss: 262.32
 ---- batch: 090 ----
mean loss: 253.02
train mean loss: 260.76
epoch train time: 0:00:00.668033
elapsed time: 0:01:30.779831
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 22:34:19.558020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.10
 ---- batch: 020 ----
mean loss: 262.62
 ---- batch: 030 ----
mean loss: 262.71
 ---- batch: 040 ----
mean loss: 267.11
 ---- batch: 050 ----
mean loss: 246.89
 ---- batch: 060 ----
mean loss: 264.95
 ---- batch: 070 ----
mean loss: 258.12
 ---- batch: 080 ----
mean loss: 264.40
 ---- batch: 090 ----
mean loss: 258.42
train mean loss: 259.80
epoch train time: 0:00:00.668676
elapsed time: 0:01:31.448665
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 22:34:20.226855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.69
 ---- batch: 020 ----
mean loss: 262.71
 ---- batch: 030 ----
mean loss: 259.22
 ---- batch: 040 ----
mean loss: 257.27
 ---- batch: 050 ----
mean loss: 256.19
 ---- batch: 060 ----
mean loss: 262.52
 ---- batch: 070 ----
mean loss: 250.51
 ---- batch: 080 ----
mean loss: 257.38
 ---- batch: 090 ----
mean loss: 256.80
train mean loss: 258.93
epoch train time: 0:00:00.671221
elapsed time: 0:01:32.120098
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 22:34:20.898319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.20
 ---- batch: 020 ----
mean loss: 262.98
 ---- batch: 030 ----
mean loss: 261.49
 ---- batch: 040 ----
mean loss: 256.30
 ---- batch: 050 ----
mean loss: 258.79
 ---- batch: 060 ----
mean loss: 252.49
 ---- batch: 070 ----
mean loss: 255.22
 ---- batch: 080 ----
mean loss: 258.00
 ---- batch: 090 ----
mean loss: 259.86
train mean loss: 257.98
epoch train time: 0:00:00.668919
elapsed time: 0:01:32.789212
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 22:34:21.567408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.84
 ---- batch: 020 ----
mean loss: 258.18
 ---- batch: 030 ----
mean loss: 245.85
 ---- batch: 040 ----
mean loss: 259.77
 ---- batch: 050 ----
mean loss: 262.10
 ---- batch: 060 ----
mean loss: 271.13
 ---- batch: 070 ----
mean loss: 257.71
 ---- batch: 080 ----
mean loss: 255.22
 ---- batch: 090 ----
mean loss: 252.32
train mean loss: 257.14
epoch train time: 0:00:00.673294
elapsed time: 0:01:33.462669
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 22:34:22.240868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.30
 ---- batch: 020 ----
mean loss: 259.54
 ---- batch: 030 ----
mean loss: 254.40
 ---- batch: 040 ----
mean loss: 254.44
 ---- batch: 050 ----
mean loss: 262.52
 ---- batch: 060 ----
mean loss: 267.62
 ---- batch: 070 ----
mean loss: 252.25
 ---- batch: 080 ----
mean loss: 254.23
 ---- batch: 090 ----
mean loss: 250.68
train mean loss: 256.19
epoch train time: 0:00:00.678272
elapsed time: 0:01:34.141171
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 22:34:22.919378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.11
 ---- batch: 020 ----
mean loss: 259.74
 ---- batch: 030 ----
mean loss: 252.35
 ---- batch: 040 ----
mean loss: 257.20
 ---- batch: 050 ----
mean loss: 254.74
 ---- batch: 060 ----
mean loss: 257.20
 ---- batch: 070 ----
mean loss: 254.57
 ---- batch: 080 ----
mean loss: 257.45
 ---- batch: 090 ----
mean loss: 257.12
train mean loss: 255.86
epoch train time: 0:00:00.669344
elapsed time: 0:01:34.810682
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 22:34:23.588899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.18
 ---- batch: 020 ----
mean loss: 263.00
 ---- batch: 030 ----
mean loss: 255.67
 ---- batch: 040 ----
mean loss: 255.68
 ---- batch: 050 ----
mean loss: 252.79
 ---- batch: 060 ----
mean loss: 259.70
 ---- batch: 070 ----
mean loss: 258.78
 ---- batch: 080 ----
mean loss: 261.32
 ---- batch: 090 ----
mean loss: 244.70
train mean loss: 255.34
epoch train time: 0:00:00.670783
elapsed time: 0:01:35.481640
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 22:34:24.259829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.21
 ---- batch: 020 ----
mean loss: 253.07
 ---- batch: 030 ----
mean loss: 264.28
 ---- batch: 040 ----
mean loss: 245.23
 ---- batch: 050 ----
mean loss: 249.94
 ---- batch: 060 ----
mean loss: 261.36
 ---- batch: 070 ----
mean loss: 254.33
 ---- batch: 080 ----
mean loss: 248.33
 ---- batch: 090 ----
mean loss: 260.05
train mean loss: 254.11
epoch train time: 0:00:00.678426
elapsed time: 0:01:36.160231
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 22:34:24.938429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.41
 ---- batch: 020 ----
mean loss: 246.98
 ---- batch: 030 ----
mean loss: 254.43
 ---- batch: 040 ----
mean loss: 255.64
 ---- batch: 050 ----
mean loss: 251.18
 ---- batch: 060 ----
mean loss: 253.63
 ---- batch: 070 ----
mean loss: 250.41
 ---- batch: 080 ----
mean loss: 253.08
 ---- batch: 090 ----
mean loss: 254.66
train mean loss: 253.34
epoch train time: 0:00:00.676031
elapsed time: 0:01:36.836435
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 22:34:25.614640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.21
 ---- batch: 020 ----
mean loss: 249.64
 ---- batch: 030 ----
mean loss: 252.28
 ---- batch: 040 ----
mean loss: 252.98
 ---- batch: 050 ----
mean loss: 249.22
 ---- batch: 060 ----
mean loss: 253.61
 ---- batch: 070 ----
mean loss: 255.81
 ---- batch: 080 ----
mean loss: 249.64
 ---- batch: 090 ----
mean loss: 256.96
train mean loss: 253.02
epoch train time: 0:00:00.680635
elapsed time: 0:01:37.517239
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 22:34:26.295431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.50
 ---- batch: 020 ----
mean loss: 248.24
 ---- batch: 030 ----
mean loss: 249.09
 ---- batch: 040 ----
mean loss: 252.92
 ---- batch: 050 ----
mean loss: 252.32
 ---- batch: 060 ----
mean loss: 246.49
 ---- batch: 070 ----
mean loss: 254.84
 ---- batch: 080 ----
mean loss: 256.52
 ---- batch: 090 ----
mean loss: 258.61
train mean loss: 252.03
epoch train time: 0:00:00.676711
elapsed time: 0:01:38.194104
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 22:34:26.972295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.55
 ---- batch: 020 ----
mean loss: 255.48
 ---- batch: 030 ----
mean loss: 248.62
 ---- batch: 040 ----
mean loss: 251.74
 ---- batch: 050 ----
mean loss: 245.26
 ---- batch: 060 ----
mean loss: 253.03
 ---- batch: 070 ----
mean loss: 257.64
 ---- batch: 080 ----
mean loss: 251.52
 ---- batch: 090 ----
mean loss: 250.03
train mean loss: 251.44
epoch train time: 0:00:00.668605
elapsed time: 0:01:38.862863
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 22:34:27.641052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.51
 ---- batch: 020 ----
mean loss: 242.87
 ---- batch: 030 ----
mean loss: 248.21
 ---- batch: 040 ----
mean loss: 248.53
 ---- batch: 050 ----
mean loss: 254.86
 ---- batch: 060 ----
mean loss: 253.65
 ---- batch: 070 ----
mean loss: 248.81
 ---- batch: 080 ----
mean loss: 250.13
 ---- batch: 090 ----
mean loss: 253.59
train mean loss: 251.00
epoch train time: 0:00:00.674288
elapsed time: 0:01:39.537350
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 22:34:28.315541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.43
 ---- batch: 020 ----
mean loss: 247.09
 ---- batch: 030 ----
mean loss: 252.11
 ---- batch: 040 ----
mean loss: 251.29
 ---- batch: 050 ----
mean loss: 260.56
 ---- batch: 060 ----
mean loss: 242.24
 ---- batch: 070 ----
mean loss: 239.96
 ---- batch: 080 ----
mean loss: 248.66
 ---- batch: 090 ----
mean loss: 250.61
train mean loss: 250.28
epoch train time: 0:00:00.668596
elapsed time: 0:01:40.206091
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 22:34:28.984295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.82
 ---- batch: 020 ----
mean loss: 250.78
 ---- batch: 030 ----
mean loss: 249.21
 ---- batch: 040 ----
mean loss: 248.24
 ---- batch: 050 ----
mean loss: 255.76
 ---- batch: 060 ----
mean loss: 254.17
 ---- batch: 070 ----
mean loss: 248.16
 ---- batch: 080 ----
mean loss: 244.96
 ---- batch: 090 ----
mean loss: 245.97
train mean loss: 249.53
epoch train time: 0:00:00.663743
elapsed time: 0:01:40.869994
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 22:34:29.648182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.47
 ---- batch: 020 ----
mean loss: 243.73
 ---- batch: 030 ----
mean loss: 245.77
 ---- batch: 040 ----
mean loss: 250.30
 ---- batch: 050 ----
mean loss: 246.49
 ---- batch: 060 ----
mean loss: 253.17
 ---- batch: 070 ----
mean loss: 247.20
 ---- batch: 080 ----
mean loss: 250.75
 ---- batch: 090 ----
mean loss: 253.59
train mean loss: 249.03
epoch train time: 0:00:00.670698
elapsed time: 0:01:41.540885
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 22:34:30.319075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.44
 ---- batch: 020 ----
mean loss: 254.26
 ---- batch: 030 ----
mean loss: 244.28
 ---- batch: 040 ----
mean loss: 247.61
 ---- batch: 050 ----
mean loss: 243.66
 ---- batch: 060 ----
mean loss: 249.33
 ---- batch: 070 ----
mean loss: 253.23
 ---- batch: 080 ----
mean loss: 244.47
 ---- batch: 090 ----
mean loss: 244.44
train mean loss: 248.57
epoch train time: 0:00:00.669777
elapsed time: 0:01:42.210820
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 22:34:30.989011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.31
 ---- batch: 020 ----
mean loss: 249.85
 ---- batch: 030 ----
mean loss: 244.85
 ---- batch: 040 ----
mean loss: 246.62
 ---- batch: 050 ----
mean loss: 247.04
 ---- batch: 060 ----
mean loss: 255.48
 ---- batch: 070 ----
mean loss: 239.53
 ---- batch: 080 ----
mean loss: 247.95
 ---- batch: 090 ----
mean loss: 245.15
train mean loss: 247.96
epoch train time: 0:00:00.664930
elapsed time: 0:01:42.875912
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 22:34:31.654101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.13
 ---- batch: 020 ----
mean loss: 245.52
 ---- batch: 030 ----
mean loss: 256.38
 ---- batch: 040 ----
mean loss: 244.07
 ---- batch: 050 ----
mean loss: 239.04
 ---- batch: 060 ----
mean loss: 252.28
 ---- batch: 070 ----
mean loss: 246.13
 ---- batch: 080 ----
mean loss: 246.09
 ---- batch: 090 ----
mean loss: 251.82
train mean loss: 247.26
epoch train time: 0:00:00.669781
elapsed time: 0:01:43.545843
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 22:34:32.324031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.31
 ---- batch: 020 ----
mean loss: 250.12
 ---- batch: 030 ----
mean loss: 248.21
 ---- batch: 040 ----
mean loss: 237.31
 ---- batch: 050 ----
mean loss: 250.42
 ---- batch: 060 ----
mean loss: 247.95
 ---- batch: 070 ----
mean loss: 244.26
 ---- batch: 080 ----
mean loss: 246.35
 ---- batch: 090 ----
mean loss: 246.94
train mean loss: 246.63
epoch train time: 0:00:00.672854
elapsed time: 0:01:44.218896
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 22:34:32.997078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.90
 ---- batch: 020 ----
mean loss: 247.70
 ---- batch: 030 ----
mean loss: 244.89
 ---- batch: 040 ----
mean loss: 248.43
 ---- batch: 050 ----
mean loss: 242.86
 ---- batch: 060 ----
mean loss: 248.60
 ---- batch: 070 ----
mean loss: 246.62
 ---- batch: 080 ----
mean loss: 250.80
 ---- batch: 090 ----
mean loss: 245.54
train mean loss: 246.03
epoch train time: 0:00:00.669528
elapsed time: 0:01:44.888567
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 22:34:33.666773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.14
 ---- batch: 020 ----
mean loss: 245.83
 ---- batch: 030 ----
mean loss: 239.94
 ---- batch: 040 ----
mean loss: 243.56
 ---- batch: 050 ----
mean loss: 238.66
 ---- batch: 060 ----
mean loss: 246.57
 ---- batch: 070 ----
mean loss: 247.10
 ---- batch: 080 ----
mean loss: 246.49
 ---- batch: 090 ----
mean loss: 245.37
train mean loss: 245.57
epoch train time: 0:00:00.675090
elapsed time: 0:01:45.563866
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 22:34:34.342084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.68
 ---- batch: 020 ----
mean loss: 244.08
 ---- batch: 030 ----
mean loss: 246.26
 ---- batch: 040 ----
mean loss: 239.77
 ---- batch: 050 ----
mean loss: 247.31
 ---- batch: 060 ----
mean loss: 251.72
 ---- batch: 070 ----
mean loss: 249.33
 ---- batch: 080 ----
mean loss: 241.58
 ---- batch: 090 ----
mean loss: 240.37
train mean loss: 246.03
epoch train time: 0:00:00.674656
elapsed time: 0:01:46.238702
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 22:34:35.016891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.64
 ---- batch: 020 ----
mean loss: 243.79
 ---- batch: 030 ----
mean loss: 251.97
 ---- batch: 040 ----
mean loss: 243.80
 ---- batch: 050 ----
mean loss: 242.10
 ---- batch: 060 ----
mean loss: 249.31
 ---- batch: 070 ----
mean loss: 241.53
 ---- batch: 080 ----
mean loss: 244.53
 ---- batch: 090 ----
mean loss: 242.86
train mean loss: 245.06
epoch train time: 0:00:00.669174
elapsed time: 0:01:46.908032
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 22:34:35.686238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.95
 ---- batch: 020 ----
mean loss: 243.12
 ---- batch: 030 ----
mean loss: 248.23
 ---- batch: 040 ----
mean loss: 249.95
 ---- batch: 050 ----
mean loss: 248.34
 ---- batch: 060 ----
mean loss: 250.62
 ---- batch: 070 ----
mean loss: 249.12
 ---- batch: 080 ----
mean loss: 241.67
 ---- batch: 090 ----
mean loss: 236.11
train mean loss: 244.57
epoch train time: 0:00:00.668397
elapsed time: 0:01:47.576595
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 22:34:36.354783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.27
 ---- batch: 020 ----
mean loss: 245.85
 ---- batch: 030 ----
mean loss: 237.77
 ---- batch: 040 ----
mean loss: 238.47
 ---- batch: 050 ----
mean loss: 239.45
 ---- batch: 060 ----
mean loss: 249.55
 ---- batch: 070 ----
mean loss: 256.88
 ---- batch: 080 ----
mean loss: 244.48
 ---- batch: 090 ----
mean loss: 245.22
train mean loss: 244.75
epoch train time: 0:00:00.666146
elapsed time: 0:01:48.242890
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 22:34:37.021080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.59
 ---- batch: 020 ----
mean loss: 236.94
 ---- batch: 030 ----
mean loss: 243.96
 ---- batch: 040 ----
mean loss: 246.71
 ---- batch: 050 ----
mean loss: 240.65
 ---- batch: 060 ----
mean loss: 248.39
 ---- batch: 070 ----
mean loss: 237.65
 ---- batch: 080 ----
mean loss: 247.17
 ---- batch: 090 ----
mean loss: 245.82
train mean loss: 244.17
epoch train time: 0:00:00.678801
elapsed time: 0:01:48.921861
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 22:34:37.700053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.03
 ---- batch: 020 ----
mean loss: 244.52
 ---- batch: 030 ----
mean loss: 237.55
 ---- batch: 040 ----
mean loss: 243.93
 ---- batch: 050 ----
mean loss: 240.01
 ---- batch: 060 ----
mean loss: 245.23
 ---- batch: 070 ----
mean loss: 243.98
 ---- batch: 080 ----
mean loss: 236.95
 ---- batch: 090 ----
mean loss: 243.56
train mean loss: 243.31
epoch train time: 0:00:00.676277
elapsed time: 0:01:49.598345
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 22:34:38.376536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.58
 ---- batch: 020 ----
mean loss: 244.69
 ---- batch: 030 ----
mean loss: 237.12
 ---- batch: 040 ----
mean loss: 232.98
 ---- batch: 050 ----
mean loss: 253.54
 ---- batch: 060 ----
mean loss: 244.85
 ---- batch: 070 ----
mean loss: 251.34
 ---- batch: 080 ----
mean loss: 238.13
 ---- batch: 090 ----
mean loss: 238.66
train mean loss: 243.28
epoch train time: 0:00:00.676437
elapsed time: 0:01:50.274936
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 22:34:39.053145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.58
 ---- batch: 020 ----
mean loss: 233.40
 ---- batch: 030 ----
mean loss: 245.74
 ---- batch: 040 ----
mean loss: 241.67
 ---- batch: 050 ----
mean loss: 243.99
 ---- batch: 060 ----
mean loss: 247.26
 ---- batch: 070 ----
mean loss: 255.07
 ---- batch: 080 ----
mean loss: 250.88
 ---- batch: 090 ----
mean loss: 247.62
train mean loss: 242.42
epoch train time: 0:00:00.673346
elapsed time: 0:01:50.948450
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 22:34:39.726646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.57
 ---- batch: 020 ----
mean loss: 234.26
 ---- batch: 030 ----
mean loss: 241.25
 ---- batch: 040 ----
mean loss: 242.65
 ---- batch: 050 ----
mean loss: 242.75
 ---- batch: 060 ----
mean loss: 235.70
 ---- batch: 070 ----
mean loss: 247.31
 ---- batch: 080 ----
mean loss: 246.19
 ---- batch: 090 ----
mean loss: 251.45
train mean loss: 242.51
epoch train time: 0:00:00.669554
elapsed time: 0:01:51.618220
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 22:34:40.396446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.89
 ---- batch: 020 ----
mean loss: 245.54
 ---- batch: 030 ----
mean loss: 244.25
 ---- batch: 040 ----
mean loss: 248.99
 ---- batch: 050 ----
mean loss: 245.84
 ---- batch: 060 ----
mean loss: 244.65
 ---- batch: 070 ----
mean loss: 239.43
 ---- batch: 080 ----
mean loss: 246.69
 ---- batch: 090 ----
mean loss: 227.76
train mean loss: 242.39
epoch train time: 0:00:00.668917
elapsed time: 0:01:52.287331
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 22:34:41.065521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.16
 ---- batch: 020 ----
mean loss: 240.90
 ---- batch: 030 ----
mean loss: 237.55
 ---- batch: 040 ----
mean loss: 241.77
 ---- batch: 050 ----
mean loss: 233.28
 ---- batch: 060 ----
mean loss: 245.34
 ---- batch: 070 ----
mean loss: 242.63
 ---- batch: 080 ----
mean loss: 249.47
 ---- batch: 090 ----
mean loss: 241.40
train mean loss: 242.02
epoch train time: 0:00:00.673742
elapsed time: 0:01:52.961237
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 22:34:41.739427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.98
 ---- batch: 020 ----
mean loss: 239.91
 ---- batch: 030 ----
mean loss: 240.91
 ---- batch: 040 ----
mean loss: 249.51
 ---- batch: 050 ----
mean loss: 242.15
 ---- batch: 060 ----
mean loss: 244.03
 ---- batch: 070 ----
mean loss: 236.33
 ---- batch: 080 ----
mean loss: 243.91
 ---- batch: 090 ----
mean loss: 242.76
train mean loss: 241.49
epoch train time: 0:00:00.667626
elapsed time: 0:01:53.629009
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 22:34:42.407197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.57
 ---- batch: 020 ----
mean loss: 243.09
 ---- batch: 030 ----
mean loss: 238.38
 ---- batch: 040 ----
mean loss: 240.60
 ---- batch: 050 ----
mean loss: 241.00
 ---- batch: 060 ----
mean loss: 241.76
 ---- batch: 070 ----
mean loss: 241.79
 ---- batch: 080 ----
mean loss: 253.32
 ---- batch: 090 ----
mean loss: 242.83
train mean loss: 241.15
epoch train time: 0:00:00.682045
elapsed time: 0:01:54.311206
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 22:34:43.089399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.29
 ---- batch: 020 ----
mean loss: 243.67
 ---- batch: 030 ----
mean loss: 236.18
 ---- batch: 040 ----
mean loss: 246.87
 ---- batch: 050 ----
mean loss: 244.05
 ---- batch: 060 ----
mean loss: 238.93
 ---- batch: 070 ----
mean loss: 232.73
 ---- batch: 080 ----
mean loss: 241.11
 ---- batch: 090 ----
mean loss: 237.72
train mean loss: 240.95
epoch train time: 0:00:00.687423
elapsed time: 0:01:54.998783
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 22:34:43.776975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.88
 ---- batch: 020 ----
mean loss: 234.85
 ---- batch: 030 ----
mean loss: 235.89
 ---- batch: 040 ----
mean loss: 241.53
 ---- batch: 050 ----
mean loss: 241.26
 ---- batch: 060 ----
mean loss: 241.05
 ---- batch: 070 ----
mean loss: 241.54
 ---- batch: 080 ----
mean loss: 244.97
 ---- batch: 090 ----
mean loss: 240.60
train mean loss: 240.68
epoch train time: 0:00:00.688220
elapsed time: 0:01:55.687168
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 22:34:44.465374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.91
 ---- batch: 020 ----
mean loss: 235.45
 ---- batch: 030 ----
mean loss: 235.77
 ---- batch: 040 ----
mean loss: 243.02
 ---- batch: 050 ----
mean loss: 240.21
 ---- batch: 060 ----
mean loss: 247.13
 ---- batch: 070 ----
mean loss: 241.26
 ---- batch: 080 ----
mean loss: 243.67
 ---- batch: 090 ----
mean loss: 244.56
train mean loss: 240.22
epoch train time: 0:00:00.689864
elapsed time: 0:01:56.377206
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 22:34:45.155399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.59
 ---- batch: 020 ----
mean loss: 237.79
 ---- batch: 030 ----
mean loss: 228.50
 ---- batch: 040 ----
mean loss: 234.74
 ---- batch: 050 ----
mean loss: 244.01
 ---- batch: 060 ----
mean loss: 241.73
 ---- batch: 070 ----
mean loss: 240.86
 ---- batch: 080 ----
mean loss: 249.69
 ---- batch: 090 ----
mean loss: 244.95
train mean loss: 239.98
epoch train time: 0:00:00.697187
elapsed time: 0:01:57.074603
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 22:34:45.852800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.26
 ---- batch: 020 ----
mean loss: 235.58
 ---- batch: 030 ----
mean loss: 244.84
 ---- batch: 040 ----
mean loss: 242.42
 ---- batch: 050 ----
mean loss: 242.25
 ---- batch: 060 ----
mean loss: 236.99
 ---- batch: 070 ----
mean loss: 237.23
 ---- batch: 080 ----
mean loss: 243.05
 ---- batch: 090 ----
mean loss: 247.60
train mean loss: 239.91
epoch train time: 0:00:00.677865
elapsed time: 0:01:57.752639
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 22:34:46.530813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.30
 ---- batch: 020 ----
mean loss: 232.15
 ---- batch: 030 ----
mean loss: 238.67
 ---- batch: 040 ----
mean loss: 246.41
 ---- batch: 050 ----
mean loss: 240.34
 ---- batch: 060 ----
mean loss: 243.09
 ---- batch: 070 ----
mean loss: 229.71
 ---- batch: 080 ----
mean loss: 237.83
 ---- batch: 090 ----
mean loss: 246.76
train mean loss: 239.78
epoch train time: 0:00:00.685971
elapsed time: 0:01:58.438745
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 22:34:47.216945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.53
 ---- batch: 020 ----
mean loss: 230.03
 ---- batch: 030 ----
mean loss: 236.00
 ---- batch: 040 ----
mean loss: 237.83
 ---- batch: 050 ----
mean loss: 235.66
 ---- batch: 060 ----
mean loss: 236.81
 ---- batch: 070 ----
mean loss: 242.92
 ---- batch: 080 ----
mean loss: 244.84
 ---- batch: 090 ----
mean loss: 245.66
train mean loss: 239.19
epoch train time: 0:00:00.685887
elapsed time: 0:01:59.124822
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 22:34:47.903015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.30
 ---- batch: 020 ----
mean loss: 237.86
 ---- batch: 030 ----
mean loss: 230.61
 ---- batch: 040 ----
mean loss: 237.10
 ---- batch: 050 ----
mean loss: 234.17
 ---- batch: 060 ----
mean loss: 233.50
 ---- batch: 070 ----
mean loss: 241.30
 ---- batch: 080 ----
mean loss: 248.09
 ---- batch: 090 ----
mean loss: 245.04
train mean loss: 239.13
epoch train time: 0:00:00.687839
elapsed time: 0:01:59.812823
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 22:34:48.591015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.03
 ---- batch: 020 ----
mean loss: 239.53
 ---- batch: 030 ----
mean loss: 238.88
 ---- batch: 040 ----
mean loss: 246.98
 ---- batch: 050 ----
mean loss: 235.95
 ---- batch: 060 ----
mean loss: 236.95
 ---- batch: 070 ----
mean loss: 237.86
 ---- batch: 080 ----
mean loss: 238.84
 ---- batch: 090 ----
mean loss: 237.58
train mean loss: 238.81
epoch train time: 0:00:00.690834
elapsed time: 0:02:00.503814
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 22:34:49.282019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.62
 ---- batch: 020 ----
mean loss: 235.28
 ---- batch: 030 ----
mean loss: 236.10
 ---- batch: 040 ----
mean loss: 237.67
 ---- batch: 050 ----
mean loss: 238.51
 ---- batch: 060 ----
mean loss: 234.62
 ---- batch: 070 ----
mean loss: 241.95
 ---- batch: 080 ----
mean loss: 246.07
 ---- batch: 090 ----
mean loss: 236.59
train mean loss: 238.93
epoch train time: 0:00:00.687635
elapsed time: 0:02:01.191611
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 22:34:49.969816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.50
 ---- batch: 020 ----
mean loss: 237.20
 ---- batch: 030 ----
mean loss: 233.91
 ---- batch: 040 ----
mean loss: 231.48
 ---- batch: 050 ----
mean loss: 238.95
 ---- batch: 060 ----
mean loss: 241.26
 ---- batch: 070 ----
mean loss: 236.00
 ---- batch: 080 ----
mean loss: 244.01
 ---- batch: 090 ----
mean loss: 237.51
train mean loss: 238.38
epoch train time: 0:00:00.684912
elapsed time: 0:02:01.876701
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 22:34:50.654923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.42
 ---- batch: 020 ----
mean loss: 227.27
 ---- batch: 030 ----
mean loss: 235.04
 ---- batch: 040 ----
mean loss: 238.92
 ---- batch: 050 ----
mean loss: 242.82
 ---- batch: 060 ----
mean loss: 232.50
 ---- batch: 070 ----
mean loss: 245.10
 ---- batch: 080 ----
mean loss: 242.14
 ---- batch: 090 ----
mean loss: 238.09
train mean loss: 238.59
epoch train time: 0:00:00.700408
elapsed time: 0:02:02.577309
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 22:34:51.355506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.52
 ---- batch: 020 ----
mean loss: 238.04
 ---- batch: 030 ----
mean loss: 237.79
 ---- batch: 040 ----
mean loss: 236.73
 ---- batch: 050 ----
mean loss: 231.79
 ---- batch: 060 ----
mean loss: 246.76
 ---- batch: 070 ----
mean loss: 241.59
 ---- batch: 080 ----
mean loss: 236.58
 ---- batch: 090 ----
mean loss: 236.77
train mean loss: 238.08
epoch train time: 0:00:00.691135
elapsed time: 0:02:03.268607
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 22:34:52.046798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.55
 ---- batch: 020 ----
mean loss: 237.88
 ---- batch: 030 ----
mean loss: 230.90
 ---- batch: 040 ----
mean loss: 239.96
 ---- batch: 050 ----
mean loss: 235.98
 ---- batch: 060 ----
mean loss: 237.14
 ---- batch: 070 ----
mean loss: 236.92
 ---- batch: 080 ----
mean loss: 237.09
 ---- batch: 090 ----
mean loss: 240.09
train mean loss: 237.66
epoch train time: 0:00:00.689862
elapsed time: 0:02:03.958631
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 22:34:52.736820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.57
 ---- batch: 020 ----
mean loss: 237.41
 ---- batch: 030 ----
mean loss: 239.73
 ---- batch: 040 ----
mean loss: 234.98
 ---- batch: 050 ----
mean loss: 234.38
 ---- batch: 060 ----
mean loss: 234.66
 ---- batch: 070 ----
mean loss: 231.53
 ---- batch: 080 ----
mean loss: 239.14
 ---- batch: 090 ----
mean loss: 242.07
train mean loss: 237.27
epoch train time: 0:00:00.685399
elapsed time: 0:02:04.644205
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 22:34:53.422404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.37
 ---- batch: 020 ----
mean loss: 237.95
 ---- batch: 030 ----
mean loss: 237.13
 ---- batch: 040 ----
mean loss: 243.48
 ---- batch: 050 ----
mean loss: 237.37
 ---- batch: 060 ----
mean loss: 240.73
 ---- batch: 070 ----
mean loss: 242.08
 ---- batch: 080 ----
mean loss: 235.95
 ---- batch: 090 ----
mean loss: 229.35
train mean loss: 237.31
epoch train time: 0:00:00.693351
elapsed time: 0:02:05.337724
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 22:34:54.115917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.21
 ---- batch: 020 ----
mean loss: 237.96
 ---- batch: 030 ----
mean loss: 241.84
 ---- batch: 040 ----
mean loss: 233.75
 ---- batch: 050 ----
mean loss: 243.73
 ---- batch: 060 ----
mean loss: 232.69
 ---- batch: 070 ----
mean loss: 233.57
 ---- batch: 080 ----
mean loss: 236.36
 ---- batch: 090 ----
mean loss: 244.06
train mean loss: 237.17
epoch train time: 0:00:00.674711
elapsed time: 0:02:06.012587
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 22:34:54.790797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.19
 ---- batch: 020 ----
mean loss: 235.00
 ---- batch: 030 ----
mean loss: 240.36
 ---- batch: 040 ----
mean loss: 230.80
 ---- batch: 050 ----
mean loss: 233.68
 ---- batch: 060 ----
mean loss: 238.47
 ---- batch: 070 ----
mean loss: 239.42
 ---- batch: 080 ----
mean loss: 248.31
 ---- batch: 090 ----
mean loss: 230.57
train mean loss: 236.32
epoch train time: 0:00:00.677208
elapsed time: 0:02:06.689979
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 22:34:55.468171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.94
 ---- batch: 020 ----
mean loss: 234.53
 ---- batch: 030 ----
mean loss: 236.63
 ---- batch: 040 ----
mean loss: 239.39
 ---- batch: 050 ----
mean loss: 237.78
 ---- batch: 060 ----
mean loss: 238.95
 ---- batch: 070 ----
mean loss: 227.11
 ---- batch: 080 ----
mean loss: 235.56
 ---- batch: 090 ----
mean loss: 246.92
train mean loss: 236.75
epoch train time: 0:00:00.677921
elapsed time: 0:02:07.368051
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 22:34:56.146239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.36
 ---- batch: 020 ----
mean loss: 237.12
 ---- batch: 030 ----
mean loss: 240.17
 ---- batch: 040 ----
mean loss: 230.55
 ---- batch: 050 ----
mean loss: 238.84
 ---- batch: 060 ----
mean loss: 236.17
 ---- batch: 070 ----
mean loss: 235.43
 ---- batch: 080 ----
mean loss: 233.90
 ---- batch: 090 ----
mean loss: 235.40
train mean loss: 236.43
epoch train time: 0:00:00.670348
elapsed time: 0:02:08.038552
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 22:34:56.816741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.05
 ---- batch: 020 ----
mean loss: 232.54
 ---- batch: 030 ----
mean loss: 239.18
 ---- batch: 040 ----
mean loss: 233.77
 ---- batch: 050 ----
mean loss: 241.29
 ---- batch: 060 ----
mean loss: 238.26
 ---- batch: 070 ----
mean loss: 236.51
 ---- batch: 080 ----
mean loss: 234.55
 ---- batch: 090 ----
mean loss: 232.07
train mean loss: 236.12
epoch train time: 0:00:00.677595
elapsed time: 0:02:08.716300
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 22:34:57.494488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.41
 ---- batch: 020 ----
mean loss: 239.18
 ---- batch: 030 ----
mean loss: 226.26
 ---- batch: 040 ----
mean loss: 239.82
 ---- batch: 050 ----
mean loss: 234.21
 ---- batch: 060 ----
mean loss: 229.49
 ---- batch: 070 ----
mean loss: 234.84
 ---- batch: 080 ----
mean loss: 232.81
 ---- batch: 090 ----
mean loss: 231.63
train mean loss: 236.01
epoch train time: 0:00:00.674798
elapsed time: 0:02:09.391244
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 22:34:58.169441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.45
 ---- batch: 020 ----
mean loss: 244.95
 ---- batch: 030 ----
mean loss: 232.72
 ---- batch: 040 ----
mean loss: 236.50
 ---- batch: 050 ----
mean loss: 226.36
 ---- batch: 060 ----
mean loss: 241.22
 ---- batch: 070 ----
mean loss: 238.44
 ---- batch: 080 ----
mean loss: 233.45
 ---- batch: 090 ----
mean loss: 237.68
train mean loss: 235.55
epoch train time: 0:00:00.669169
elapsed time: 0:02:10.060568
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 22:34:58.838755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.08
 ---- batch: 020 ----
mean loss: 237.27
 ---- batch: 030 ----
mean loss: 228.48
 ---- batch: 040 ----
mean loss: 239.58
 ---- batch: 050 ----
mean loss: 238.36
 ---- batch: 060 ----
mean loss: 240.73
 ---- batch: 070 ----
mean loss: 228.26
 ---- batch: 080 ----
mean loss: 241.04
 ---- batch: 090 ----
mean loss: 235.85
train mean loss: 235.22
epoch train time: 0:00:00.658356
elapsed time: 0:02:10.719065
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 22:34:59.497280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.37
 ---- batch: 020 ----
mean loss: 237.06
 ---- batch: 030 ----
mean loss: 238.41
 ---- batch: 040 ----
mean loss: 234.00
 ---- batch: 050 ----
mean loss: 233.61
 ---- batch: 060 ----
mean loss: 232.61
 ---- batch: 070 ----
mean loss: 233.72
 ---- batch: 080 ----
mean loss: 233.07
 ---- batch: 090 ----
mean loss: 233.70
train mean loss: 235.41
epoch train time: 0:00:00.678450
elapsed time: 0:02:11.397708
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 22:35:00.175897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.05
 ---- batch: 020 ----
mean loss: 243.58
 ---- batch: 030 ----
mean loss: 235.03
 ---- batch: 040 ----
mean loss: 233.27
 ---- batch: 050 ----
mean loss: 234.62
 ---- batch: 060 ----
mean loss: 235.19
 ---- batch: 070 ----
mean loss: 226.61
 ---- batch: 080 ----
mean loss: 237.72
 ---- batch: 090 ----
mean loss: 238.40
train mean loss: 235.40
epoch train time: 0:00:00.674305
elapsed time: 0:02:12.072178
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 22:35:00.850371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.03
 ---- batch: 020 ----
mean loss: 233.82
 ---- batch: 030 ----
mean loss: 232.91
 ---- batch: 040 ----
mean loss: 239.91
 ---- batch: 050 ----
mean loss: 230.72
 ---- batch: 060 ----
mean loss: 229.49
 ---- batch: 070 ----
mean loss: 233.74
 ---- batch: 080 ----
mean loss: 238.62
 ---- batch: 090 ----
mean loss: 235.09
train mean loss: 234.75
epoch train time: 0:00:00.663736
elapsed time: 0:02:12.736078
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 22:35:01.514268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.92
 ---- batch: 020 ----
mean loss: 233.38
 ---- batch: 030 ----
mean loss: 232.36
 ---- batch: 040 ----
mean loss: 233.72
 ---- batch: 050 ----
mean loss: 235.58
 ---- batch: 060 ----
mean loss: 244.23
 ---- batch: 070 ----
mean loss: 239.02
 ---- batch: 080 ----
mean loss: 230.69
 ---- batch: 090 ----
mean loss: 238.51
train mean loss: 234.84
epoch train time: 0:00:00.668574
elapsed time: 0:02:13.404810
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 22:35:02.182999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.66
 ---- batch: 020 ----
mean loss: 239.61
 ---- batch: 030 ----
mean loss: 231.20
 ---- batch: 040 ----
mean loss: 238.40
 ---- batch: 050 ----
mean loss: 226.18
 ---- batch: 060 ----
mean loss: 234.86
 ---- batch: 070 ----
mean loss: 230.15
 ---- batch: 080 ----
mean loss: 240.25
 ---- batch: 090 ----
mean loss: 231.42
train mean loss: 234.58
epoch train time: 0:00:00.669763
elapsed time: 0:02:14.074723
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 22:35:02.852921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.19
 ---- batch: 020 ----
mean loss: 235.58
 ---- batch: 030 ----
mean loss: 234.09
 ---- batch: 040 ----
mean loss: 235.34
 ---- batch: 050 ----
mean loss: 230.25
 ---- batch: 060 ----
mean loss: 236.60
 ---- batch: 070 ----
mean loss: 232.95
 ---- batch: 080 ----
mean loss: 235.17
 ---- batch: 090 ----
mean loss: 245.33
train mean loss: 234.49
epoch train time: 0:00:00.671909
elapsed time: 0:02:14.746785
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 22:35:03.524969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.35
 ---- batch: 020 ----
mean loss: 243.86
 ---- batch: 030 ----
mean loss: 238.10
 ---- batch: 040 ----
mean loss: 229.73
 ---- batch: 050 ----
mean loss: 236.69
 ---- batch: 060 ----
mean loss: 229.72
 ---- batch: 070 ----
mean loss: 233.99
 ---- batch: 080 ----
mean loss: 230.48
 ---- batch: 090 ----
mean loss: 235.42
train mean loss: 233.81
epoch train time: 0:00:00.661758
elapsed time: 0:02:15.408693
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 22:35:04.186884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.71
 ---- batch: 020 ----
mean loss: 224.33
 ---- batch: 030 ----
mean loss: 239.90
 ---- batch: 040 ----
mean loss: 245.60
 ---- batch: 050 ----
mean loss: 233.52
 ---- batch: 060 ----
mean loss: 231.45
 ---- batch: 070 ----
mean loss: 230.40
 ---- batch: 080 ----
mean loss: 232.75
 ---- batch: 090 ----
mean loss: 230.24
train mean loss: 234.26
epoch train time: 0:00:00.670635
elapsed time: 0:02:16.079503
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 22:35:04.857715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.68
 ---- batch: 020 ----
mean loss: 230.12
 ---- batch: 030 ----
mean loss: 229.41
 ---- batch: 040 ----
mean loss: 235.62
 ---- batch: 050 ----
mean loss: 232.94
 ---- batch: 060 ----
mean loss: 240.58
 ---- batch: 070 ----
mean loss: 231.74
 ---- batch: 080 ----
mean loss: 239.05
 ---- batch: 090 ----
mean loss: 233.21
train mean loss: 233.82
epoch train time: 0:00:00.680689
elapsed time: 0:02:16.760456
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 22:35:05.538665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.46
 ---- batch: 020 ----
mean loss: 236.46
 ---- batch: 030 ----
mean loss: 236.15
 ---- batch: 040 ----
mean loss: 225.26
 ---- batch: 050 ----
mean loss: 230.97
 ---- batch: 060 ----
mean loss: 242.93
 ---- batch: 070 ----
mean loss: 235.99
 ---- batch: 080 ----
mean loss: 231.56
 ---- batch: 090 ----
mean loss: 236.45
train mean loss: 233.89
epoch train time: 0:00:00.680282
elapsed time: 0:02:17.440909
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 22:35:06.219098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.31
 ---- batch: 020 ----
mean loss: 225.87
 ---- batch: 030 ----
mean loss: 238.78
 ---- batch: 040 ----
mean loss: 239.63
 ---- batch: 050 ----
mean loss: 223.92
 ---- batch: 060 ----
mean loss: 236.15
 ---- batch: 070 ----
mean loss: 239.91
 ---- batch: 080 ----
mean loss: 245.20
 ---- batch: 090 ----
mean loss: 226.18
train mean loss: 233.59
epoch train time: 0:00:00.671489
elapsed time: 0:02:18.112552
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 22:35:06.890744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.28
 ---- batch: 020 ----
mean loss: 238.40
 ---- batch: 030 ----
mean loss: 231.56
 ---- batch: 040 ----
mean loss: 232.79
 ---- batch: 050 ----
mean loss: 239.67
 ---- batch: 060 ----
mean loss: 231.35
 ---- batch: 070 ----
mean loss: 233.16
 ---- batch: 080 ----
mean loss: 231.22
 ---- batch: 090 ----
mean loss: 230.69
train mean loss: 233.40
epoch train time: 0:00:00.670410
elapsed time: 0:02:18.783118
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 22:35:07.561310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.57
 ---- batch: 020 ----
mean loss: 226.89
 ---- batch: 030 ----
mean loss: 232.81
 ---- batch: 040 ----
mean loss: 230.57
 ---- batch: 050 ----
mean loss: 227.22
 ---- batch: 060 ----
mean loss: 232.45
 ---- batch: 070 ----
mean loss: 230.71
 ---- batch: 080 ----
mean loss: 238.99
 ---- batch: 090 ----
mean loss: 229.92
train mean loss: 233.49
epoch train time: 0:00:00.668852
elapsed time: 0:02:19.452117
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 22:35:08.230317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.59
 ---- batch: 020 ----
mean loss: 226.58
 ---- batch: 030 ----
mean loss: 240.33
 ---- batch: 040 ----
mean loss: 236.83
 ---- batch: 050 ----
mean loss: 237.85
 ---- batch: 060 ----
mean loss: 231.14
 ---- batch: 070 ----
mean loss: 232.92
 ---- batch: 080 ----
mean loss: 229.72
 ---- batch: 090 ----
mean loss: 232.04
train mean loss: 232.93
epoch train time: 0:00:00.669946
elapsed time: 0:02:20.122226
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 22:35:08.900415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.30
 ---- batch: 020 ----
mean loss: 232.32
 ---- batch: 030 ----
mean loss: 229.04
 ---- batch: 040 ----
mean loss: 233.75
 ---- batch: 050 ----
mean loss: 227.38
 ---- batch: 060 ----
mean loss: 232.34
 ---- batch: 070 ----
mean loss: 239.47
 ---- batch: 080 ----
mean loss: 236.45
 ---- batch: 090 ----
mean loss: 234.21
train mean loss: 233.10
epoch train time: 0:00:00.670349
elapsed time: 0:02:20.792735
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 22:35:09.570960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.24
 ---- batch: 020 ----
mean loss: 225.99
 ---- batch: 030 ----
mean loss: 241.77
 ---- batch: 040 ----
mean loss: 227.38
 ---- batch: 050 ----
mean loss: 229.31
 ---- batch: 060 ----
mean loss: 237.30
 ---- batch: 070 ----
mean loss: 238.97
 ---- batch: 080 ----
mean loss: 229.52
 ---- batch: 090 ----
mean loss: 231.60
train mean loss: 232.56
epoch train time: 0:00:00.662144
elapsed time: 0:02:21.455059
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 22:35:10.233261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.06
 ---- batch: 020 ----
mean loss: 236.47
 ---- batch: 030 ----
mean loss: 243.10
 ---- batch: 040 ----
mean loss: 234.25
 ---- batch: 050 ----
mean loss: 223.80
 ---- batch: 060 ----
mean loss: 239.91
 ---- batch: 070 ----
mean loss: 231.06
 ---- batch: 080 ----
mean loss: 234.04
 ---- batch: 090 ----
mean loss: 225.83
train mean loss: 232.25
epoch train time: 0:00:00.680950
elapsed time: 0:02:22.136184
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 22:35:10.914386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.89
 ---- batch: 020 ----
mean loss: 224.49
 ---- batch: 030 ----
mean loss: 233.94
 ---- batch: 040 ----
mean loss: 232.71
 ---- batch: 050 ----
mean loss: 235.27
 ---- batch: 060 ----
mean loss: 235.94
 ---- batch: 070 ----
mean loss: 232.24
 ---- batch: 080 ----
mean loss: 228.70
 ---- batch: 090 ----
mean loss: 234.69
train mean loss: 232.38
epoch train time: 0:00:00.684177
elapsed time: 0:02:22.820555
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 22:35:11.598747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.13
 ---- batch: 020 ----
mean loss: 229.19
 ---- batch: 030 ----
mean loss: 229.53
 ---- batch: 040 ----
mean loss: 228.08
 ---- batch: 050 ----
mean loss: 240.16
 ---- batch: 060 ----
mean loss: 236.51
 ---- batch: 070 ----
mean loss: 230.74
 ---- batch: 080 ----
mean loss: 223.62
 ---- batch: 090 ----
mean loss: 240.47
train mean loss: 232.03
epoch train time: 0:00:00.672088
elapsed time: 0:02:23.492798
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 22:35:12.270988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.03
 ---- batch: 020 ----
mean loss: 227.09
 ---- batch: 030 ----
mean loss: 226.71
 ---- batch: 040 ----
mean loss: 240.96
 ---- batch: 050 ----
mean loss: 234.65
 ---- batch: 060 ----
mean loss: 241.83
 ---- batch: 070 ----
mean loss: 225.04
 ---- batch: 080 ----
mean loss: 223.85
 ---- batch: 090 ----
mean loss: 234.64
train mean loss: 231.95
epoch train time: 0:00:00.679614
elapsed time: 0:02:24.172559
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 22:35:12.950744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.22
 ---- batch: 020 ----
mean loss: 231.28
 ---- batch: 030 ----
mean loss: 234.89
 ---- batch: 040 ----
mean loss: 228.49
 ---- batch: 050 ----
mean loss: 230.00
 ---- batch: 060 ----
mean loss: 225.24
 ---- batch: 070 ----
mean loss: 224.77
 ---- batch: 080 ----
mean loss: 232.95
 ---- batch: 090 ----
mean loss: 239.02
train mean loss: 231.75
epoch train time: 0:00:00.680210
elapsed time: 0:02:24.852914
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 22:35:13.631102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.46
 ---- batch: 020 ----
mean loss: 228.39
 ---- batch: 030 ----
mean loss: 237.02
 ---- batch: 040 ----
mean loss: 222.94
 ---- batch: 050 ----
mean loss: 232.31
 ---- batch: 060 ----
mean loss: 226.02
 ---- batch: 070 ----
mean loss: 234.19
 ---- batch: 080 ----
mean loss: 234.06
 ---- batch: 090 ----
mean loss: 232.17
train mean loss: 231.70
epoch train time: 0:00:00.672696
elapsed time: 0:02:25.525756
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 22:35:14.303945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.41
 ---- batch: 020 ----
mean loss: 231.64
 ---- batch: 030 ----
mean loss: 223.97
 ---- batch: 040 ----
mean loss: 229.77
 ---- batch: 050 ----
mean loss: 232.39
 ---- batch: 060 ----
mean loss: 237.68
 ---- batch: 070 ----
mean loss: 227.51
 ---- batch: 080 ----
mean loss: 232.99
 ---- batch: 090 ----
mean loss: 230.07
train mean loss: 231.58
epoch train time: 0:00:00.674208
elapsed time: 0:02:26.200134
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 22:35:14.978341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.77
 ---- batch: 020 ----
mean loss: 227.13
 ---- batch: 030 ----
mean loss: 226.78
 ---- batch: 040 ----
mean loss: 235.14
 ---- batch: 050 ----
mean loss: 225.88
 ---- batch: 060 ----
mean loss: 233.30
 ---- batch: 070 ----
mean loss: 237.53
 ---- batch: 080 ----
mean loss: 233.95
 ---- batch: 090 ----
mean loss: 228.85
train mean loss: 231.20
epoch train time: 0:00:00.665932
elapsed time: 0:02:26.866246
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 22:35:15.644437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.30
 ---- batch: 020 ----
mean loss: 231.04
 ---- batch: 030 ----
mean loss: 228.69
 ---- batch: 040 ----
mean loss: 234.50
 ---- batch: 050 ----
mean loss: 235.44
 ---- batch: 060 ----
mean loss: 232.63
 ---- batch: 070 ----
mean loss: 235.19
 ---- batch: 080 ----
mean loss: 219.11
 ---- batch: 090 ----
mean loss: 228.23
train mean loss: 230.67
epoch train time: 0:00:00.668186
elapsed time: 0:02:27.534646
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 22:35:16.312850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.11
 ---- batch: 020 ----
mean loss: 227.24
 ---- batch: 030 ----
mean loss: 220.03
 ---- batch: 040 ----
mean loss: 230.42
 ---- batch: 050 ----
mean loss: 231.42
 ---- batch: 060 ----
mean loss: 236.59
 ---- batch: 070 ----
mean loss: 239.94
 ---- batch: 080 ----
mean loss: 227.69
 ---- batch: 090 ----
mean loss: 233.64
train mean loss: 230.84
epoch train time: 0:00:00.681931
elapsed time: 0:02:28.216737
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 22:35:16.994922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.74
 ---- batch: 020 ----
mean loss: 218.33
 ---- batch: 030 ----
mean loss: 234.19
 ---- batch: 040 ----
mean loss: 237.42
 ---- batch: 050 ----
mean loss: 229.25
 ---- batch: 060 ----
mean loss: 221.46
 ---- batch: 070 ----
mean loss: 232.49
 ---- batch: 080 ----
mean loss: 233.93
 ---- batch: 090 ----
mean loss: 228.78
train mean loss: 230.62
epoch train time: 0:00:00.660617
elapsed time: 0:02:28.877512
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 22:35:17.655749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.40
 ---- batch: 020 ----
mean loss: 223.62
 ---- batch: 030 ----
mean loss: 236.47
 ---- batch: 040 ----
mean loss: 223.69
 ---- batch: 050 ----
mean loss: 224.48
 ---- batch: 060 ----
mean loss: 235.70
 ---- batch: 070 ----
mean loss: 233.08
 ---- batch: 080 ----
mean loss: 233.63
 ---- batch: 090 ----
mean loss: 230.37
train mean loss: 230.46
epoch train time: 0:00:00.675199
elapsed time: 0:02:29.552920
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 22:35:18.331105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.83
 ---- batch: 020 ----
mean loss: 233.54
 ---- batch: 030 ----
mean loss: 243.36
 ---- batch: 040 ----
mean loss: 222.38
 ---- batch: 050 ----
mean loss: 225.55
 ---- batch: 060 ----
mean loss: 233.62
 ---- batch: 070 ----
mean loss: 223.82
 ---- batch: 080 ----
mean loss: 231.68
 ---- batch: 090 ----
mean loss: 230.42
train mean loss: 230.09
epoch train time: 0:00:00.682931
elapsed time: 0:02:30.235997
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 22:35:19.014188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.28
 ---- batch: 020 ----
mean loss: 223.14
 ---- batch: 030 ----
mean loss: 230.94
 ---- batch: 040 ----
mean loss: 232.62
 ---- batch: 050 ----
mean loss: 229.17
 ---- batch: 060 ----
mean loss: 233.71
 ---- batch: 070 ----
mean loss: 235.21
 ---- batch: 080 ----
mean loss: 223.84
 ---- batch: 090 ----
mean loss: 227.26
train mean loss: 229.87
epoch train time: 0:00:00.677413
elapsed time: 0:02:30.913558
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 22:35:19.691766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.29
 ---- batch: 020 ----
mean loss: 228.15
 ---- batch: 030 ----
mean loss: 227.52
 ---- batch: 040 ----
mean loss: 232.17
 ---- batch: 050 ----
mean loss: 236.86
 ---- batch: 060 ----
mean loss: 228.15
 ---- batch: 070 ----
mean loss: 225.56
 ---- batch: 080 ----
mean loss: 221.36
 ---- batch: 090 ----
mean loss: 237.27
train mean loss: 229.74
epoch train time: 0:00:00.670762
elapsed time: 0:02:31.584485
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 22:35:20.362674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.40
 ---- batch: 020 ----
mean loss: 236.63
 ---- batch: 030 ----
mean loss: 227.74
 ---- batch: 040 ----
mean loss: 224.53
 ---- batch: 050 ----
mean loss: 221.87
 ---- batch: 060 ----
mean loss: 230.96
 ---- batch: 070 ----
mean loss: 232.73
 ---- batch: 080 ----
mean loss: 229.30
 ---- batch: 090 ----
mean loss: 225.06
train mean loss: 229.33
epoch train time: 0:00:00.671846
elapsed time: 0:02:32.256486
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 22:35:21.034678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.84
 ---- batch: 020 ----
mean loss: 233.63
 ---- batch: 030 ----
mean loss: 230.80
 ---- batch: 040 ----
mean loss: 232.72
 ---- batch: 050 ----
mean loss: 220.86
 ---- batch: 060 ----
mean loss: 233.93
 ---- batch: 070 ----
mean loss: 226.14
 ---- batch: 080 ----
mean loss: 222.04
 ---- batch: 090 ----
mean loss: 227.38
train mean loss: 229.41
epoch train time: 0:00:00.673593
elapsed time: 0:02:32.930232
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 22:35:21.708420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.74
 ---- batch: 020 ----
mean loss: 230.52
 ---- batch: 030 ----
mean loss: 231.58
 ---- batch: 040 ----
mean loss: 228.28
 ---- batch: 050 ----
mean loss: 235.53
 ---- batch: 060 ----
mean loss: 228.78
 ---- batch: 070 ----
mean loss: 222.75
 ---- batch: 080 ----
mean loss: 228.47
 ---- batch: 090 ----
mean loss: 228.47
train mean loss: 229.27
epoch train time: 0:00:00.666246
elapsed time: 0:02:33.596626
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 22:35:22.374814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.12
 ---- batch: 020 ----
mean loss: 229.25
 ---- batch: 030 ----
mean loss: 231.04
 ---- batch: 040 ----
mean loss: 237.62
 ---- batch: 050 ----
mean loss: 226.76
 ---- batch: 060 ----
mean loss: 224.17
 ---- batch: 070 ----
mean loss: 226.84
 ---- batch: 080 ----
mean loss: 228.75
 ---- batch: 090 ----
mean loss: 225.63
train mean loss: 229.31
epoch train time: 0:00:00.675647
elapsed time: 0:02:34.272420
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 22:35:23.050608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.28
 ---- batch: 020 ----
mean loss: 233.78
 ---- batch: 030 ----
mean loss: 229.05
 ---- batch: 040 ----
mean loss: 231.85
 ---- batch: 050 ----
mean loss: 222.38
 ---- batch: 060 ----
mean loss: 225.73
 ---- batch: 070 ----
mean loss: 231.24
 ---- batch: 080 ----
mean loss: 229.11
 ---- batch: 090 ----
mean loss: 227.37
train mean loss: 228.86
epoch train time: 0:00:00.667523
elapsed time: 0:02:34.940100
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 22:35:23.718291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.70
 ---- batch: 020 ----
mean loss: 228.11
 ---- batch: 030 ----
mean loss: 235.81
 ---- batch: 040 ----
mean loss: 230.28
 ---- batch: 050 ----
mean loss: 225.56
 ---- batch: 060 ----
mean loss: 234.10
 ---- batch: 070 ----
mean loss: 226.65
 ---- batch: 080 ----
mean loss: 231.81
 ---- batch: 090 ----
mean loss: 217.31
train mean loss: 228.53
epoch train time: 0:00:00.673065
elapsed time: 0:02:35.613331
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 22:35:24.391535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.58
 ---- batch: 020 ----
mean loss: 229.16
 ---- batch: 030 ----
mean loss: 225.53
 ---- batch: 040 ----
mean loss: 234.15
 ---- batch: 050 ----
mean loss: 229.83
 ---- batch: 060 ----
mean loss: 227.92
 ---- batch: 070 ----
mean loss: 230.11
 ---- batch: 080 ----
mean loss: 228.47
 ---- batch: 090 ----
mean loss: 223.93
train mean loss: 228.70
epoch train time: 0:00:00.680752
elapsed time: 0:02:36.294263
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 22:35:25.072472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.22
 ---- batch: 020 ----
mean loss: 225.29
 ---- batch: 030 ----
mean loss: 227.63
 ---- batch: 040 ----
mean loss: 229.45
 ---- batch: 050 ----
mean loss: 231.07
 ---- batch: 060 ----
mean loss: 238.25
 ---- batch: 070 ----
mean loss: 225.00
 ---- batch: 080 ----
mean loss: 224.48
 ---- batch: 090 ----
mean loss: 225.49
train mean loss: 228.72
epoch train time: 0:00:00.662844
elapsed time: 0:02:36.957292
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 22:35:25.735480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.88
 ---- batch: 020 ----
mean loss: 218.80
 ---- batch: 030 ----
mean loss: 225.77
 ---- batch: 040 ----
mean loss: 236.80
 ---- batch: 050 ----
mean loss: 238.61
 ---- batch: 060 ----
mean loss: 229.27
 ---- batch: 070 ----
mean loss: 225.89
 ---- batch: 080 ----
mean loss: 225.72
 ---- batch: 090 ----
mean loss: 231.93
train mean loss: 228.30
epoch train time: 0:00:00.662155
elapsed time: 0:02:37.619597
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 22:35:26.397786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.86
 ---- batch: 020 ----
mean loss: 223.61
 ---- batch: 030 ----
mean loss: 231.94
 ---- batch: 040 ----
mean loss: 230.17
 ---- batch: 050 ----
mean loss: 220.23
 ---- batch: 060 ----
mean loss: 231.74
 ---- batch: 070 ----
mean loss: 233.06
 ---- batch: 080 ----
mean loss: 225.43
 ---- batch: 090 ----
mean loss: 230.37
train mean loss: 228.28
epoch train time: 0:00:00.670821
elapsed time: 0:02:38.290581
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 22:35:27.068781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.59
 ---- batch: 020 ----
mean loss: 230.67
 ---- batch: 030 ----
mean loss: 223.87
 ---- batch: 040 ----
mean loss: 224.77
 ---- batch: 050 ----
mean loss: 230.40
 ---- batch: 060 ----
mean loss: 229.47
 ---- batch: 070 ----
mean loss: 225.51
 ---- batch: 080 ----
mean loss: 229.56
 ---- batch: 090 ----
mean loss: 229.31
train mean loss: 227.79
epoch train time: 0:00:00.657077
elapsed time: 0:02:38.947862
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 22:35:27.726057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.33
 ---- batch: 020 ----
mean loss: 229.75
 ---- batch: 030 ----
mean loss: 224.19
 ---- batch: 040 ----
mean loss: 221.97
 ---- batch: 050 ----
mean loss: 232.78
 ---- batch: 060 ----
mean loss: 225.04
 ---- batch: 070 ----
mean loss: 230.93
 ---- batch: 080 ----
mean loss: 229.06
 ---- batch: 090 ----
mean loss: 229.80
train mean loss: 228.02
epoch train time: 0:00:00.669359
elapsed time: 0:02:39.617374
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 22:35:28.395562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.75
 ---- batch: 020 ----
mean loss: 226.84
 ---- batch: 030 ----
mean loss: 237.60
 ---- batch: 040 ----
mean loss: 221.50
 ---- batch: 050 ----
mean loss: 228.87
 ---- batch: 060 ----
mean loss: 219.72
 ---- batch: 070 ----
mean loss: 218.13
 ---- batch: 080 ----
mean loss: 232.91
 ---- batch: 090 ----
mean loss: 231.20
train mean loss: 227.61
epoch train time: 0:00:00.678253
elapsed time: 0:02:40.295832
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 22:35:29.074021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.67
 ---- batch: 020 ----
mean loss: 226.40
 ---- batch: 030 ----
mean loss: 230.31
 ---- batch: 040 ----
mean loss: 230.72
 ---- batch: 050 ----
mean loss: 231.46
 ---- batch: 060 ----
mean loss: 225.32
 ---- batch: 070 ----
mean loss: 225.35
 ---- batch: 080 ----
mean loss: 236.31
 ---- batch: 090 ----
mean loss: 220.03
train mean loss: 227.60
epoch train time: 0:00:00.671102
elapsed time: 0:02:40.967079
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 22:35:29.745282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.40
 ---- batch: 020 ----
mean loss: 229.86
 ---- batch: 030 ----
mean loss: 229.68
 ---- batch: 040 ----
mean loss: 221.85
 ---- batch: 050 ----
mean loss: 229.04
 ---- batch: 060 ----
mean loss: 226.88
 ---- batch: 070 ----
mean loss: 228.16
 ---- batch: 080 ----
mean loss: 224.56
 ---- batch: 090 ----
mean loss: 226.04
train mean loss: 227.17
epoch train time: 0:00:00.666940
elapsed time: 0:02:41.634196
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 22:35:30.412382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.31
 ---- batch: 020 ----
mean loss: 227.20
 ---- batch: 030 ----
mean loss: 221.29
 ---- batch: 040 ----
mean loss: 230.73
 ---- batch: 050 ----
mean loss: 225.79
 ---- batch: 060 ----
mean loss: 230.29
 ---- batch: 070 ----
mean loss: 221.42
 ---- batch: 080 ----
mean loss: 226.41
 ---- batch: 090 ----
mean loss: 226.23
train mean loss: 226.83
epoch train time: 0:00:00.676650
elapsed time: 0:02:42.310988
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 22:35:31.089176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.89
 ---- batch: 020 ----
mean loss: 220.44
 ---- batch: 030 ----
mean loss: 226.73
 ---- batch: 040 ----
mean loss: 226.65
 ---- batch: 050 ----
mean loss: 226.62
 ---- batch: 060 ----
mean loss: 216.57
 ---- batch: 070 ----
mean loss: 233.25
 ---- batch: 080 ----
mean loss: 232.87
 ---- batch: 090 ----
mean loss: 229.25
train mean loss: 227.12
epoch train time: 0:00:00.673136
elapsed time: 0:02:42.984299
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 22:35:31.762489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.27
 ---- batch: 020 ----
mean loss: 224.62
 ---- batch: 030 ----
mean loss: 226.18
 ---- batch: 040 ----
mean loss: 225.92
 ---- batch: 050 ----
mean loss: 221.38
 ---- batch: 060 ----
mean loss: 230.06
 ---- batch: 070 ----
mean loss: 230.86
 ---- batch: 080 ----
mean loss: 223.79
 ---- batch: 090 ----
mean loss: 228.12
train mean loss: 227.07
epoch train time: 0:00:00.670688
elapsed time: 0:02:43.655131
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 22:35:32.433317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.14
 ---- batch: 020 ----
mean loss: 223.39
 ---- batch: 030 ----
mean loss: 233.58
 ---- batch: 040 ----
mean loss: 224.80
 ---- batch: 050 ----
mean loss: 223.85
 ---- batch: 060 ----
mean loss: 231.68
 ---- batch: 070 ----
mean loss: 231.84
 ---- batch: 080 ----
mean loss: 226.16
 ---- batch: 090 ----
mean loss: 223.97
train mean loss: 227.16
epoch train time: 0:00:00.663027
elapsed time: 0:02:44.318366
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 22:35:33.096552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.77
 ---- batch: 020 ----
mean loss: 225.59
 ---- batch: 030 ----
mean loss: 222.30
 ---- batch: 040 ----
mean loss: 227.24
 ---- batch: 050 ----
mean loss: 223.74
 ---- batch: 060 ----
mean loss: 221.97
 ---- batch: 070 ----
mean loss: 226.79
 ---- batch: 080 ----
mean loss: 223.46
 ---- batch: 090 ----
mean loss: 233.89
train mean loss: 226.37
epoch train time: 0:00:00.655671
elapsed time: 0:02:44.974182
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 22:35:33.752386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.22
 ---- batch: 020 ----
mean loss: 227.67
 ---- batch: 030 ----
mean loss: 228.20
 ---- batch: 040 ----
mean loss: 223.25
 ---- batch: 050 ----
mean loss: 226.84
 ---- batch: 060 ----
mean loss: 231.30
 ---- batch: 070 ----
mean loss: 221.56
 ---- batch: 080 ----
mean loss: 221.16
 ---- batch: 090 ----
mean loss: 236.08
train mean loss: 226.26
epoch train time: 0:00:00.658753
elapsed time: 0:02:45.633093
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 22:35:34.411310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.79
 ---- batch: 020 ----
mean loss: 221.79
 ---- batch: 030 ----
mean loss: 222.16
 ---- batch: 040 ----
mean loss: 233.43
 ---- batch: 050 ----
mean loss: 233.85
 ---- batch: 060 ----
mean loss: 226.86
 ---- batch: 070 ----
mean loss: 229.27
 ---- batch: 080 ----
mean loss: 231.44
 ---- batch: 090 ----
mean loss: 218.06
train mean loss: 226.35
epoch train time: 0:00:00.664615
elapsed time: 0:02:46.297931
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 22:35:35.076135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.79
 ---- batch: 020 ----
mean loss: 216.08
 ---- batch: 030 ----
mean loss: 229.26
 ---- batch: 040 ----
mean loss: 226.42
 ---- batch: 050 ----
mean loss: 228.87
 ---- batch: 060 ----
mean loss: 229.57
 ---- batch: 070 ----
mean loss: 224.59
 ---- batch: 080 ----
mean loss: 233.16
 ---- batch: 090 ----
mean loss: 218.85
train mean loss: 225.80
epoch train time: 0:00:00.660311
elapsed time: 0:02:46.958413
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 22:35:35.736601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.21
 ---- batch: 020 ----
mean loss: 229.11
 ---- batch: 030 ----
mean loss: 222.24
 ---- batch: 040 ----
mean loss: 227.00
 ---- batch: 050 ----
mean loss: 226.53
 ---- batch: 060 ----
mean loss: 218.24
 ---- batch: 070 ----
mean loss: 227.42
 ---- batch: 080 ----
mean loss: 229.06
 ---- batch: 090 ----
mean loss: 229.03
train mean loss: 226.25
epoch train time: 0:00:00.664843
elapsed time: 0:02:47.623401
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 22:35:36.401593
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.44
 ---- batch: 020 ----
mean loss: 221.27
 ---- batch: 030 ----
mean loss: 221.71
 ---- batch: 040 ----
mean loss: 235.55
 ---- batch: 050 ----
mean loss: 223.75
 ---- batch: 060 ----
mean loss: 215.69
 ---- batch: 070 ----
mean loss: 224.64
 ---- batch: 080 ----
mean loss: 229.79
 ---- batch: 090 ----
mean loss: 228.42
train mean loss: 225.59
epoch train time: 0:00:00.675891
elapsed time: 0:02:48.299504
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 22:35:37.077699
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.56
 ---- batch: 020 ----
mean loss: 224.09
 ---- batch: 030 ----
mean loss: 227.36
 ---- batch: 040 ----
mean loss: 217.31
 ---- batch: 050 ----
mean loss: 229.86
 ---- batch: 060 ----
mean loss: 217.81
 ---- batch: 070 ----
mean loss: 224.63
 ---- batch: 080 ----
mean loss: 228.55
 ---- batch: 090 ----
mean loss: 224.00
train mean loss: 225.39
epoch train time: 0:00:00.654796
elapsed time: 0:02:48.954451
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 22:35:37.732636
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 233.19
 ---- batch: 020 ----
mean loss: 226.02
 ---- batch: 030 ----
mean loss: 217.94
 ---- batch: 040 ----
mean loss: 224.96
 ---- batch: 050 ----
mean loss: 227.02
 ---- batch: 060 ----
mean loss: 217.99
 ---- batch: 070 ----
mean loss: 235.80
 ---- batch: 080 ----
mean loss: 223.14
 ---- batch: 090 ----
mean loss: 226.35
train mean loss: 225.31
epoch train time: 0:00:00.676752
elapsed time: 0:02:49.631350
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 22:35:38.409539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.22
 ---- batch: 020 ----
mean loss: 229.77
 ---- batch: 030 ----
mean loss: 228.23
 ---- batch: 040 ----
mean loss: 224.10
 ---- batch: 050 ----
mean loss: 224.04
 ---- batch: 060 ----
mean loss: 227.69
 ---- batch: 070 ----
mean loss: 221.69
 ---- batch: 080 ----
mean loss: 233.51
 ---- batch: 090 ----
mean loss: 220.86
train mean loss: 225.19
epoch train time: 0:00:00.677905
elapsed time: 0:02:50.309400
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 22:35:39.087587
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.84
 ---- batch: 020 ----
mean loss: 227.59
 ---- batch: 030 ----
mean loss: 234.53
 ---- batch: 040 ----
mean loss: 212.70
 ---- batch: 050 ----
mean loss: 224.67
 ---- batch: 060 ----
mean loss: 226.64
 ---- batch: 070 ----
mean loss: 223.65
 ---- batch: 080 ----
mean loss: 236.98
 ---- batch: 090 ----
mean loss: 222.21
train mean loss: 225.15
epoch train time: 0:00:00.656773
elapsed time: 0:02:50.966316
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 22:35:39.744518
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.60
 ---- batch: 020 ----
mean loss: 221.39
 ---- batch: 030 ----
mean loss: 221.95
 ---- batch: 040 ----
mean loss: 236.29
 ---- batch: 050 ----
mean loss: 224.42
 ---- batch: 060 ----
mean loss: 221.25
 ---- batch: 070 ----
mean loss: 228.56
 ---- batch: 080 ----
mean loss: 232.02
 ---- batch: 090 ----
mean loss: 222.55
train mean loss: 225.46
epoch train time: 0:00:00.656707
elapsed time: 0:02:51.623198
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 22:35:40.401404
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.44
 ---- batch: 020 ----
mean loss: 219.67
 ---- batch: 030 ----
mean loss: 224.75
 ---- batch: 040 ----
mean loss: 232.50
 ---- batch: 050 ----
mean loss: 225.15
 ---- batch: 060 ----
mean loss: 217.47
 ---- batch: 070 ----
mean loss: 227.49
 ---- batch: 080 ----
mean loss: 224.03
 ---- batch: 090 ----
mean loss: 226.59
train mean loss: 225.40
epoch train time: 0:00:00.675810
elapsed time: 0:02:52.299197
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 22:35:41.077389
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.03
 ---- batch: 020 ----
mean loss: 228.56
 ---- batch: 030 ----
mean loss: 223.12
 ---- batch: 040 ----
mean loss: 237.08
 ---- batch: 050 ----
mean loss: 227.81
 ---- batch: 060 ----
mean loss: 221.71
 ---- batch: 070 ----
mean loss: 221.36
 ---- batch: 080 ----
mean loss: 223.73
 ---- batch: 090 ----
mean loss: 222.64
train mean loss: 225.21
epoch train time: 0:00:00.663594
elapsed time: 0:02:52.962939
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 22:35:41.741126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.52
 ---- batch: 020 ----
mean loss: 230.07
 ---- batch: 030 ----
mean loss: 223.92
 ---- batch: 040 ----
mean loss: 220.98
 ---- batch: 050 ----
mean loss: 229.58
 ---- batch: 060 ----
mean loss: 230.72
 ---- batch: 070 ----
mean loss: 226.16
 ---- batch: 080 ----
mean loss: 226.73
 ---- batch: 090 ----
mean loss: 220.00
train mean loss: 225.34
epoch train time: 0:00:00.668593
elapsed time: 0:02:53.631680
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 22:35:42.409869
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.60
 ---- batch: 020 ----
mean loss: 227.99
 ---- batch: 030 ----
mean loss: 229.51
 ---- batch: 040 ----
mean loss: 221.16
 ---- batch: 050 ----
mean loss: 217.33
 ---- batch: 060 ----
mean loss: 234.43
 ---- batch: 070 ----
mean loss: 221.67
 ---- batch: 080 ----
mean loss: 225.58
 ---- batch: 090 ----
mean loss: 222.60
train mean loss: 225.07
epoch train time: 0:00:00.663096
elapsed time: 0:02:54.294923
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 22:35:43.073111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.59
 ---- batch: 020 ----
mean loss: 226.17
 ---- batch: 030 ----
mean loss: 225.57
 ---- batch: 040 ----
mean loss: 224.09
 ---- batch: 050 ----
mean loss: 224.04
 ---- batch: 060 ----
mean loss: 225.48
 ---- batch: 070 ----
mean loss: 227.73
 ---- batch: 080 ----
mean loss: 223.67
 ---- batch: 090 ----
mean loss: 222.37
train mean loss: 225.50
epoch train time: 0:00:00.663724
elapsed time: 0:02:54.958788
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 22:35:43.736974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.88
 ---- batch: 020 ----
mean loss: 228.44
 ---- batch: 030 ----
mean loss: 223.08
 ---- batch: 040 ----
mean loss: 227.41
 ---- batch: 050 ----
mean loss: 222.03
 ---- batch: 060 ----
mean loss: 218.98
 ---- batch: 070 ----
mean loss: 230.87
 ---- batch: 080 ----
mean loss: 224.17
 ---- batch: 090 ----
mean loss: 225.72
train mean loss: 225.27
epoch train time: 0:00:00.666475
elapsed time: 0:02:55.625454
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 22:35:44.403659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.09
 ---- batch: 020 ----
mean loss: 226.97
 ---- batch: 030 ----
mean loss: 225.69
 ---- batch: 040 ----
mean loss: 225.65
 ---- batch: 050 ----
mean loss: 231.61
 ---- batch: 060 ----
mean loss: 224.82
 ---- batch: 070 ----
mean loss: 222.10
 ---- batch: 080 ----
mean loss: 223.67
 ---- batch: 090 ----
mean loss: 224.72
train mean loss: 225.04
epoch train time: 0:00:00.668151
elapsed time: 0:02:56.293766
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 22:35:45.071989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.90
 ---- batch: 020 ----
mean loss: 212.45
 ---- batch: 030 ----
mean loss: 220.45
 ---- batch: 040 ----
mean loss: 221.51
 ---- batch: 050 ----
mean loss: 225.21
 ---- batch: 060 ----
mean loss: 235.11
 ---- batch: 070 ----
mean loss: 232.93
 ---- batch: 080 ----
mean loss: 233.60
 ---- batch: 090 ----
mean loss: 228.62
train mean loss: 225.55
epoch train time: 0:00:00.663525
elapsed time: 0:02:56.957478
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 22:35:45.735668
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.41
 ---- batch: 020 ----
mean loss: 227.15
 ---- batch: 030 ----
mean loss: 223.20
 ---- batch: 040 ----
mean loss: 223.57
 ---- batch: 050 ----
mean loss: 224.40
 ---- batch: 060 ----
mean loss: 231.64
 ---- batch: 070 ----
mean loss: 221.87
 ---- batch: 080 ----
mean loss: 221.88
 ---- batch: 090 ----
mean loss: 222.62
train mean loss: 225.22
epoch train time: 0:00:00.665047
elapsed time: 0:02:57.622714
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 22:35:46.400902
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.09
 ---- batch: 020 ----
mean loss: 224.38
 ---- batch: 030 ----
mean loss: 227.54
 ---- batch: 040 ----
mean loss: 221.91
 ---- batch: 050 ----
mean loss: 223.27
 ---- batch: 060 ----
mean loss: 226.26
 ---- batch: 070 ----
mean loss: 220.52
 ---- batch: 080 ----
mean loss: 232.96
 ---- batch: 090 ----
mean loss: 223.27
train mean loss: 224.97
epoch train time: 0:00:00.661562
elapsed time: 0:02:58.284429
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 22:35:47.062620
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.46
 ---- batch: 020 ----
mean loss: 226.69
 ---- batch: 030 ----
mean loss: 230.28
 ---- batch: 040 ----
mean loss: 221.98
 ---- batch: 050 ----
mean loss: 226.45
 ---- batch: 060 ----
mean loss: 222.48
 ---- batch: 070 ----
mean loss: 217.61
 ---- batch: 080 ----
mean loss: 235.22
 ---- batch: 090 ----
mean loss: 222.43
train mean loss: 225.20
epoch train time: 0:00:00.662835
elapsed time: 0:02:58.947416
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 22:35:47.725625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.63
 ---- batch: 020 ----
mean loss: 228.75
 ---- batch: 030 ----
mean loss: 228.22
 ---- batch: 040 ----
mean loss: 226.58
 ---- batch: 050 ----
mean loss: 219.95
 ---- batch: 060 ----
mean loss: 224.63
 ---- batch: 070 ----
mean loss: 221.38
 ---- batch: 080 ----
mean loss: 228.45
 ---- batch: 090 ----
mean loss: 226.64
train mean loss: 225.30
epoch train time: 0:00:00.660515
elapsed time: 0:02:59.608105
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 22:35:48.386291
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.15
 ---- batch: 020 ----
mean loss: 223.06
 ---- batch: 030 ----
mean loss: 230.04
 ---- batch: 040 ----
mean loss: 226.74
 ---- batch: 050 ----
mean loss: 234.14
 ---- batch: 060 ----
mean loss: 215.75
 ---- batch: 070 ----
mean loss: 222.72
 ---- batch: 080 ----
mean loss: 232.16
 ---- batch: 090 ----
mean loss: 224.98
train mean loss: 225.00
epoch train time: 0:00:00.667539
elapsed time: 0:03:00.275797
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 22:35:49.053988
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.16
 ---- batch: 020 ----
mean loss: 220.25
 ---- batch: 030 ----
mean loss: 226.66
 ---- batch: 040 ----
mean loss: 221.37
 ---- batch: 050 ----
mean loss: 229.84
 ---- batch: 060 ----
mean loss: 224.24
 ---- batch: 070 ----
mean loss: 225.62
 ---- batch: 080 ----
mean loss: 228.63
 ---- batch: 090 ----
mean loss: 223.84
train mean loss: 225.36
epoch train time: 0:00:00.662447
elapsed time: 0:03:00.938387
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 22:35:49.716586
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.45
 ---- batch: 020 ----
mean loss: 221.37
 ---- batch: 030 ----
mean loss: 226.32
 ---- batch: 040 ----
mean loss: 225.36
 ---- batch: 050 ----
mean loss: 226.00
 ---- batch: 060 ----
mean loss: 231.96
 ---- batch: 070 ----
mean loss: 227.09
 ---- batch: 080 ----
mean loss: 232.35
 ---- batch: 090 ----
mean loss: 217.13
train mean loss: 224.98
epoch train time: 0:00:00.659102
elapsed time: 0:03:01.597696
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 22:35:50.375914
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.17
 ---- batch: 020 ----
mean loss: 224.38
 ---- batch: 030 ----
mean loss: 221.74
 ---- batch: 040 ----
mean loss: 221.83
 ---- batch: 050 ----
mean loss: 220.34
 ---- batch: 060 ----
mean loss: 224.39
 ---- batch: 070 ----
mean loss: 226.29
 ---- batch: 080 ----
mean loss: 237.29
 ---- batch: 090 ----
mean loss: 228.89
train mean loss: 224.94
epoch train time: 0:00:00.667885
elapsed time: 0:03:02.265762
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 22:35:51.043955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.13
 ---- batch: 020 ----
mean loss: 222.16
 ---- batch: 030 ----
mean loss: 227.63
 ---- batch: 040 ----
mean loss: 223.36
 ---- batch: 050 ----
mean loss: 228.34
 ---- batch: 060 ----
mean loss: 222.21
 ---- batch: 070 ----
mean loss: 226.42
 ---- batch: 080 ----
mean loss: 216.70
 ---- batch: 090 ----
mean loss: 235.39
train mean loss: 225.03
epoch train time: 0:00:00.673718
elapsed time: 0:03:02.939631
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 22:35:51.717818
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.20
 ---- batch: 020 ----
mean loss: 216.12
 ---- batch: 030 ----
mean loss: 224.59
 ---- batch: 040 ----
mean loss: 224.10
 ---- batch: 050 ----
mean loss: 226.28
 ---- batch: 060 ----
mean loss: 221.24
 ---- batch: 070 ----
mean loss: 227.33
 ---- batch: 080 ----
mean loss: 236.03
 ---- batch: 090 ----
mean loss: 227.83
train mean loss: 224.94
epoch train time: 0:00:00.668068
elapsed time: 0:03:03.607845
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 22:35:52.386038
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.36
 ---- batch: 020 ----
mean loss: 230.85
 ---- batch: 030 ----
mean loss: 223.82
 ---- batch: 040 ----
mean loss: 224.95
 ---- batch: 050 ----
mean loss: 224.04
 ---- batch: 060 ----
mean loss: 223.69
 ---- batch: 070 ----
mean loss: 225.13
 ---- batch: 080 ----
mean loss: 221.25
 ---- batch: 090 ----
mean loss: 225.16
train mean loss: 225.02
epoch train time: 0:00:00.666879
elapsed time: 0:03:04.274874
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 22:35:53.053064
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.84
 ---- batch: 020 ----
mean loss: 228.79
 ---- batch: 030 ----
mean loss: 225.63
 ---- batch: 040 ----
mean loss: 219.12
 ---- batch: 050 ----
mean loss: 222.48
 ---- batch: 060 ----
mean loss: 225.26
 ---- batch: 070 ----
mean loss: 219.42
 ---- batch: 080 ----
mean loss: 227.73
 ---- batch: 090 ----
mean loss: 230.55
train mean loss: 225.07
epoch train time: 0:00:00.656432
elapsed time: 0:03:04.931454
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 22:35:53.709663
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.96
 ---- batch: 020 ----
mean loss: 229.09
 ---- batch: 030 ----
mean loss: 222.99
 ---- batch: 040 ----
mean loss: 226.99
 ---- batch: 050 ----
mean loss: 229.80
 ---- batch: 060 ----
mean loss: 220.70
 ---- batch: 070 ----
mean loss: 225.19
 ---- batch: 080 ----
mean loss: 227.29
 ---- batch: 090 ----
mean loss: 222.03
train mean loss: 224.91
epoch train time: 0:00:00.664228
elapsed time: 0:03:05.595851
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 22:35:54.374047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 239.75
 ---- batch: 020 ----
mean loss: 218.31
 ---- batch: 030 ----
mean loss: 229.70
 ---- batch: 040 ----
mean loss: 227.41
 ---- batch: 050 ----
mean loss: 217.24
 ---- batch: 060 ----
mean loss: 221.83
 ---- batch: 070 ----
mean loss: 228.02
 ---- batch: 080 ----
mean loss: 223.29
 ---- batch: 090 ----
mean loss: 222.04
train mean loss: 224.93
epoch train time: 0:00:00.660986
elapsed time: 0:03:06.257002
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 22:35:55.035227
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.31
 ---- batch: 020 ----
mean loss: 225.31
 ---- batch: 030 ----
mean loss: 220.82
 ---- batch: 040 ----
mean loss: 229.92
 ---- batch: 050 ----
mean loss: 227.01
 ---- batch: 060 ----
mean loss: 219.47
 ---- batch: 070 ----
mean loss: 228.86
 ---- batch: 080 ----
mean loss: 219.49
 ---- batch: 090 ----
mean loss: 235.18
train mean loss: 224.96
epoch train time: 0:00:00.665031
elapsed time: 0:03:06.922233
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 22:35:55.700424
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.14
 ---- batch: 020 ----
mean loss: 226.72
 ---- batch: 030 ----
mean loss: 233.28
 ---- batch: 040 ----
mean loss: 220.77
 ---- batch: 050 ----
mean loss: 216.78
 ---- batch: 060 ----
mean loss: 218.57
 ---- batch: 070 ----
mean loss: 226.34
 ---- batch: 080 ----
mean loss: 227.18
 ---- batch: 090 ----
mean loss: 226.23
train mean loss: 225.05
epoch train time: 0:00:00.667240
elapsed time: 0:03:07.589622
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 22:35:56.367832
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 232.88
 ---- batch: 020 ----
mean loss: 222.55
 ---- batch: 030 ----
mean loss: 224.95
 ---- batch: 040 ----
mean loss: 224.06
 ---- batch: 050 ----
mean loss: 221.88
 ---- batch: 060 ----
mean loss: 217.48
 ---- batch: 070 ----
mean loss: 220.50
 ---- batch: 080 ----
mean loss: 223.79
 ---- batch: 090 ----
mean loss: 232.11
train mean loss: 224.73
epoch train time: 0:00:00.670169
elapsed time: 0:03:08.259963
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 22:35:57.038153
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.85
 ---- batch: 020 ----
mean loss: 227.85
 ---- batch: 030 ----
mean loss: 229.11
 ---- batch: 040 ----
mean loss: 222.93
 ---- batch: 050 ----
mean loss: 217.39
 ---- batch: 060 ----
mean loss: 229.04
 ---- batch: 070 ----
mean loss: 220.13
 ---- batch: 080 ----
mean loss: 221.95
 ---- batch: 090 ----
mean loss: 228.49
train mean loss: 224.72
epoch train time: 0:00:00.662298
elapsed time: 0:03:08.922408
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 22:35:57.700597
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.23
 ---- batch: 020 ----
mean loss: 224.81
 ---- batch: 030 ----
mean loss: 220.29
 ---- batch: 040 ----
mean loss: 220.21
 ---- batch: 050 ----
mean loss: 227.01
 ---- batch: 060 ----
mean loss: 226.06
 ---- batch: 070 ----
mean loss: 219.71
 ---- batch: 080 ----
mean loss: 224.81
 ---- batch: 090 ----
mean loss: 227.05
train mean loss: 224.92
epoch train time: 0:00:00.669500
elapsed time: 0:03:09.592087
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 22:35:58.370277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.21
 ---- batch: 020 ----
mean loss: 224.20
 ---- batch: 030 ----
mean loss: 222.76
 ---- batch: 040 ----
mean loss: 230.49
 ---- batch: 050 ----
mean loss: 229.06
 ---- batch: 060 ----
mean loss: 223.41
 ---- batch: 070 ----
mean loss: 231.05
 ---- batch: 080 ----
mean loss: 221.90
 ---- batch: 090 ----
mean loss: 221.55
train mean loss: 224.91
epoch train time: 0:00:00.667252
elapsed time: 0:03:10.259511
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 22:35:59.037705
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.46
 ---- batch: 020 ----
mean loss: 223.26
 ---- batch: 030 ----
mean loss: 222.87
 ---- batch: 040 ----
mean loss: 219.76
 ---- batch: 050 ----
mean loss: 230.18
 ---- batch: 060 ----
mean loss: 233.26
 ---- batch: 070 ----
mean loss: 221.53
 ---- batch: 080 ----
mean loss: 224.13
 ---- batch: 090 ----
mean loss: 222.89
train mean loss: 224.78
epoch train time: 0:00:00.664463
elapsed time: 0:03:10.924143
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 22:35:59.702359
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.49
 ---- batch: 020 ----
mean loss: 224.03
 ---- batch: 030 ----
mean loss: 226.27
 ---- batch: 040 ----
mean loss: 231.24
 ---- batch: 050 ----
mean loss: 223.24
 ---- batch: 060 ----
mean loss: 216.87
 ---- batch: 070 ----
mean loss: 230.02
 ---- batch: 080 ----
mean loss: 225.70
 ---- batch: 090 ----
mean loss: 226.61
train mean loss: 224.81
epoch train time: 0:00:00.661043
elapsed time: 0:03:11.585375
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 22:36:00.363577
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.29
 ---- batch: 020 ----
mean loss: 217.33
 ---- batch: 030 ----
mean loss: 228.78
 ---- batch: 040 ----
mean loss: 228.50
 ---- batch: 050 ----
mean loss: 224.27
 ---- batch: 060 ----
mean loss: 217.99
 ---- batch: 070 ----
mean loss: 223.52
 ---- batch: 080 ----
mean loss: 229.15
 ---- batch: 090 ----
mean loss: 223.85
train mean loss: 224.95
epoch train time: 0:00:00.665987
elapsed time: 0:03:12.251566
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 22:36:01.029761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.83
 ---- batch: 020 ----
mean loss: 224.99
 ---- batch: 030 ----
mean loss: 224.55
 ---- batch: 040 ----
mean loss: 225.06
 ---- batch: 050 ----
mean loss: 229.11
 ---- batch: 060 ----
mean loss: 215.32
 ---- batch: 070 ----
mean loss: 222.67
 ---- batch: 080 ----
mean loss: 222.28
 ---- batch: 090 ----
mean loss: 228.81
train mean loss: 225.14
epoch train time: 0:00:00.665259
elapsed time: 0:03:12.916980
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 22:36:01.695208
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.86
 ---- batch: 020 ----
mean loss: 225.92
 ---- batch: 030 ----
mean loss: 222.09
 ---- batch: 040 ----
mean loss: 217.04
 ---- batch: 050 ----
mean loss: 222.49
 ---- batch: 060 ----
mean loss: 232.35
 ---- batch: 070 ----
mean loss: 219.04
 ---- batch: 080 ----
mean loss: 226.94
 ---- batch: 090 ----
mean loss: 229.29
train mean loss: 224.88
epoch train time: 0:00:00.666831
elapsed time: 0:03:13.584005
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 22:36:02.362197
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.90
 ---- batch: 020 ----
mean loss: 221.29
 ---- batch: 030 ----
mean loss: 226.96
 ---- batch: 040 ----
mean loss: 221.92
 ---- batch: 050 ----
mean loss: 226.03
 ---- batch: 060 ----
mean loss: 221.67
 ---- batch: 070 ----
mean loss: 226.11
 ---- batch: 080 ----
mean loss: 227.56
 ---- batch: 090 ----
mean loss: 219.21
train mean loss: 225.01
epoch train time: 0:00:00.667098
elapsed time: 0:03:14.251266
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 22:36:03.029452
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.98
 ---- batch: 020 ----
mean loss: 224.21
 ---- batch: 030 ----
mean loss: 230.86
 ---- batch: 040 ----
mean loss: 217.36
 ---- batch: 050 ----
mean loss: 224.63
 ---- batch: 060 ----
mean loss: 217.85
 ---- batch: 070 ----
mean loss: 227.08
 ---- batch: 080 ----
mean loss: 219.41
 ---- batch: 090 ----
mean loss: 230.15
train mean loss: 225.03
epoch train time: 0:00:00.658531
elapsed time: 0:03:14.909936
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 22:36:03.688122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.90
 ---- batch: 020 ----
mean loss: 220.89
 ---- batch: 030 ----
mean loss: 228.30
 ---- batch: 040 ----
mean loss: 218.33
 ---- batch: 050 ----
mean loss: 223.87
 ---- batch: 060 ----
mean loss: 221.10
 ---- batch: 070 ----
mean loss: 226.14
 ---- batch: 080 ----
mean loss: 222.76
 ---- batch: 090 ----
mean loss: 232.58
train mean loss: 224.84
epoch train time: 0:00:00.661638
elapsed time: 0:03:15.571715
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 22:36:04.349900
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.94
 ---- batch: 020 ----
mean loss: 226.70
 ---- batch: 030 ----
mean loss: 225.35
 ---- batch: 040 ----
mean loss: 215.69
 ---- batch: 050 ----
mean loss: 221.82
 ---- batch: 060 ----
mean loss: 231.01
 ---- batch: 070 ----
mean loss: 230.09
 ---- batch: 080 ----
mean loss: 216.30
 ---- batch: 090 ----
mean loss: 231.69
train mean loss: 224.70
epoch train time: 0:00:00.665136
elapsed time: 0:03:16.237027
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 22:36:05.015241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.92
 ---- batch: 020 ----
mean loss: 234.20
 ---- batch: 030 ----
mean loss: 218.81
 ---- batch: 040 ----
mean loss: 221.88
 ---- batch: 050 ----
mean loss: 230.73
 ---- batch: 060 ----
mean loss: 218.60
 ---- batch: 070 ----
mean loss: 222.38
 ---- batch: 080 ----
mean loss: 221.70
 ---- batch: 090 ----
mean loss: 230.64
train mean loss: 224.65
epoch train time: 0:00:00.675889
elapsed time: 0:03:16.913126
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 22:36:05.691317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.60
 ---- batch: 020 ----
mean loss: 222.97
 ---- batch: 030 ----
mean loss: 221.64
 ---- batch: 040 ----
mean loss: 228.82
 ---- batch: 050 ----
mean loss: 224.23
 ---- batch: 060 ----
mean loss: 225.31
 ---- batch: 070 ----
mean loss: 219.84
 ---- batch: 080 ----
mean loss: 229.62
 ---- batch: 090 ----
mean loss: 230.09
train mean loss: 224.62
epoch train time: 0:00:00.672440
elapsed time: 0:03:17.585774
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 22:36:06.364044
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 233.41
 ---- batch: 020 ----
mean loss: 226.83
 ---- batch: 030 ----
mean loss: 218.66
 ---- batch: 040 ----
mean loss: 229.79
 ---- batch: 050 ----
mean loss: 228.76
 ---- batch: 060 ----
mean loss: 214.01
 ---- batch: 070 ----
mean loss: 221.49
 ---- batch: 080 ----
mean loss: 218.44
 ---- batch: 090 ----
mean loss: 229.75
train mean loss: 224.73
epoch train time: 0:00:00.675361
elapsed time: 0:03:18.261367
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 22:36:07.039555
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.69
 ---- batch: 020 ----
mean loss: 222.29
 ---- batch: 030 ----
mean loss: 223.41
 ---- batch: 040 ----
mean loss: 231.54
 ---- batch: 050 ----
mean loss: 226.78
 ---- batch: 060 ----
mean loss: 231.27
 ---- batch: 070 ----
mean loss: 215.01
 ---- batch: 080 ----
mean loss: 215.87
 ---- batch: 090 ----
mean loss: 229.49
train mean loss: 224.60
epoch train time: 0:00:00.673799
elapsed time: 0:03:18.935313
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 22:36:07.713501
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.12
 ---- batch: 020 ----
mean loss: 227.70
 ---- batch: 030 ----
mean loss: 219.74
 ---- batch: 040 ----
mean loss: 229.12
 ---- batch: 050 ----
mean loss: 220.30
 ---- batch: 060 ----
mean loss: 225.45
 ---- batch: 070 ----
mean loss: 234.34
 ---- batch: 080 ----
mean loss: 219.22
 ---- batch: 090 ----
mean loss: 220.87
train mean loss: 224.64
epoch train time: 0:00:00.658781
elapsed time: 0:03:19.594254
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 22:36:08.372439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.52
 ---- batch: 020 ----
mean loss: 226.97
 ---- batch: 030 ----
mean loss: 221.00
 ---- batch: 040 ----
mean loss: 218.46
 ---- batch: 050 ----
mean loss: 236.24
 ---- batch: 060 ----
mean loss: 226.28
 ---- batch: 070 ----
mean loss: 221.69
 ---- batch: 080 ----
mean loss: 228.26
 ---- batch: 090 ----
mean loss: 228.55
train mean loss: 224.42
epoch train time: 0:00:00.668497
elapsed time: 0:03:20.265166
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_2/checkpoint.pth.tar
**** end time: 2019-09-25 22:36:09.043335 ****
