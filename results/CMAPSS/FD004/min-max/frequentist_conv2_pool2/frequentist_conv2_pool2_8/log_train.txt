Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_8', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17615
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 01:56:16.386759 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 11, 11]             560
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
            Conv2d-4            [-1, 14, 4, 11]             224
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
            Linear-8                    [-1, 1]             308
================================================================
Total params: 1,092
Trainable params: 1,092
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 01:56:16.392227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4977.83
 ---- batch: 020 ----
mean loss: 4828.98
 ---- batch: 030 ----
mean loss: 4684.13
 ---- batch: 040 ----
mean loss: 4508.10
 ---- batch: 050 ----
mean loss: 4340.55
 ---- batch: 060 ----
mean loss: 4116.10
 ---- batch: 070 ----
mean loss: 3948.57
 ---- batch: 080 ----
mean loss: 3733.43
 ---- batch: 090 ----
mean loss: 3515.19
 ---- batch: 100 ----
mean loss: 3324.95
 ---- batch: 110 ----
mean loss: 3123.57
train mean loss: 4069.86
epoch train time: 0:00:32.862406
elapsed time: 0:00:32.869247
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 01:56:49.256052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2823.85
 ---- batch: 020 ----
mean loss: 2604.16
 ---- batch: 030 ----
mean loss: 2436.12
 ---- batch: 040 ----
mean loss: 2260.13
 ---- batch: 050 ----
mean loss: 2118.09
 ---- batch: 060 ----
mean loss: 1957.40
 ---- batch: 070 ----
mean loss: 1817.69
 ---- batch: 080 ----
mean loss: 1712.19
 ---- batch: 090 ----
mean loss: 1598.48
 ---- batch: 100 ----
mean loss: 1491.06
 ---- batch: 110 ----
mean loss: 1401.19
train mean loss: 2003.08
epoch train time: 0:00:00.721978
elapsed time: 0:00:33.591411
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 01:56:49.978229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1312.46
 ---- batch: 020 ----
mean loss: 1259.42
 ---- batch: 030 ----
mean loss: 1204.30
 ---- batch: 040 ----
mean loss: 1156.00
 ---- batch: 050 ----
mean loss: 1098.46
 ---- batch: 060 ----
mean loss: 1053.33
 ---- batch: 070 ----
mean loss: 1049.24
 ---- batch: 080 ----
mean loss: 1007.11
 ---- batch: 090 ----
mean loss: 983.13
 ---- batch: 100 ----
mean loss: 960.88
 ---- batch: 110 ----
mean loss: 944.42
train mean loss: 1088.99
epoch train time: 0:00:00.717262
elapsed time: 0:00:34.308822
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 01:56:50.695631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 933.82
 ---- batch: 020 ----
mean loss: 915.22
 ---- batch: 030 ----
mean loss: 912.67
 ---- batch: 040 ----
mean loss: 888.10
 ---- batch: 050 ----
mean loss: 870.67
 ---- batch: 060 ----
mean loss: 872.55
 ---- batch: 070 ----
mean loss: 873.61
 ---- batch: 080 ----
mean loss: 847.12
 ---- batch: 090 ----
mean loss: 870.81
 ---- batch: 100 ----
mean loss: 873.03
 ---- batch: 110 ----
mean loss: 852.57
train mean loss: 881.88
epoch train time: 0:00:00.717833
elapsed time: 0:00:35.026792
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 01:56:51.413637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.55
 ---- batch: 020 ----
mean loss: 848.20
 ---- batch: 030 ----
mean loss: 854.45
 ---- batch: 040 ----
mean loss: 860.55
 ---- batch: 050 ----
mean loss: 853.83
 ---- batch: 060 ----
mean loss: 839.47
 ---- batch: 070 ----
mean loss: 842.79
 ---- batch: 080 ----
mean loss: 839.68
 ---- batch: 090 ----
mean loss: 840.70
 ---- batch: 100 ----
mean loss: 855.76
 ---- batch: 110 ----
mean loss: 854.14
train mean loss: 848.79
epoch train time: 0:00:00.725929
elapsed time: 0:00:35.752895
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 01:56:52.139715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.36
 ---- batch: 020 ----
mean loss: 842.97
 ---- batch: 030 ----
mean loss: 847.93
 ---- batch: 040 ----
mean loss: 822.58
 ---- batch: 050 ----
mean loss: 836.78
 ---- batch: 060 ----
mean loss: 857.95
 ---- batch: 070 ----
mean loss: 836.03
 ---- batch: 080 ----
mean loss: 857.14
 ---- batch: 090 ----
mean loss: 836.91
 ---- batch: 100 ----
mean loss: 844.42
 ---- batch: 110 ----
mean loss: 842.46
train mean loss: 842.11
epoch train time: 0:00:00.720902
elapsed time: 0:00:36.473947
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 01:56:52.860766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.58
 ---- batch: 020 ----
mean loss: 825.15
 ---- batch: 030 ----
mean loss: 828.30
 ---- batch: 040 ----
mean loss: 838.11
 ---- batch: 050 ----
mean loss: 810.94
 ---- batch: 060 ----
mean loss: 839.35
 ---- batch: 070 ----
mean loss: 829.94
 ---- batch: 080 ----
mean loss: 853.34
 ---- batch: 090 ----
mean loss: 848.76
 ---- batch: 100 ----
mean loss: 853.02
 ---- batch: 110 ----
mean loss: 840.17
train mean loss: 837.95
epoch train time: 0:00:00.719245
elapsed time: 0:00:37.193340
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 01:56:53.580182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.20
 ---- batch: 020 ----
mean loss: 819.90
 ---- batch: 030 ----
mean loss: 854.99
 ---- batch: 040 ----
mean loss: 835.77
 ---- batch: 050 ----
mean loss: 836.34
 ---- batch: 060 ----
mean loss: 841.75
 ---- batch: 070 ----
mean loss: 812.30
 ---- batch: 080 ----
mean loss: 831.87
 ---- batch: 090 ----
mean loss: 835.20
 ---- batch: 100 ----
mean loss: 842.53
 ---- batch: 110 ----
mean loss: 837.43
train mean loss: 833.75
epoch train time: 0:00:00.724569
elapsed time: 0:00:37.918081
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 01:56:54.304908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.77
 ---- batch: 020 ----
mean loss: 836.07
 ---- batch: 030 ----
mean loss: 817.20
 ---- batch: 040 ----
mean loss: 816.68
 ---- batch: 050 ----
mean loss: 833.66
 ---- batch: 060 ----
mean loss: 816.59
 ---- batch: 070 ----
mean loss: 839.17
 ---- batch: 080 ----
mean loss: 823.89
 ---- batch: 090 ----
mean loss: 822.84
 ---- batch: 100 ----
mean loss: 847.44
 ---- batch: 110 ----
mean loss: 835.87
train mean loss: 829.58
epoch train time: 0:00:00.744899
elapsed time: 0:00:38.663158
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 01:56:55.049973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.80
 ---- batch: 020 ----
mean loss: 834.99
 ---- batch: 030 ----
mean loss: 806.66
 ---- batch: 040 ----
mean loss: 839.35
 ---- batch: 050 ----
mean loss: 819.86
 ---- batch: 060 ----
mean loss: 826.74
 ---- batch: 070 ----
mean loss: 820.58
 ---- batch: 080 ----
mean loss: 855.05
 ---- batch: 090 ----
mean loss: 816.09
 ---- batch: 100 ----
mean loss: 809.85
 ---- batch: 110 ----
mean loss: 831.56
train mean loss: 825.07
epoch train time: 0:00:00.718113
elapsed time: 0:00:39.381414
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 01:56:55.768220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.45
 ---- batch: 020 ----
mean loss: 806.39
 ---- batch: 030 ----
mean loss: 822.17
 ---- batch: 040 ----
mean loss: 838.90
 ---- batch: 050 ----
mean loss: 813.36
 ---- batch: 060 ----
mean loss: 817.73
 ---- batch: 070 ----
mean loss: 822.82
 ---- batch: 080 ----
mean loss: 815.16
 ---- batch: 090 ----
mean loss: 830.76
 ---- batch: 100 ----
mean loss: 814.81
 ---- batch: 110 ----
mean loss: 817.18
train mean loss: 820.60
epoch train time: 0:00:00.726632
elapsed time: 0:00:40.108191
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 01:56:56.494998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.52
 ---- batch: 020 ----
mean loss: 798.00
 ---- batch: 030 ----
mean loss: 833.02
 ---- batch: 040 ----
mean loss: 826.17
 ---- batch: 050 ----
mean loss: 816.84
 ---- batch: 060 ----
mean loss: 809.74
 ---- batch: 070 ----
mean loss: 813.85
 ---- batch: 080 ----
mean loss: 806.78
 ---- batch: 090 ----
mean loss: 822.15
 ---- batch: 100 ----
mean loss: 811.54
 ---- batch: 110 ----
mean loss: 784.54
train mean loss: 815.80
epoch train time: 0:00:00.722461
elapsed time: 0:00:40.830805
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 01:56:57.217662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.61
 ---- batch: 020 ----
mean loss: 819.64
 ---- batch: 030 ----
mean loss: 817.06
 ---- batch: 040 ----
mean loss: 806.26
 ---- batch: 050 ----
mean loss: 809.19
 ---- batch: 060 ----
mean loss: 804.78
 ---- batch: 070 ----
mean loss: 820.38
 ---- batch: 080 ----
mean loss: 786.24
 ---- batch: 090 ----
mean loss: 807.14
 ---- batch: 100 ----
mean loss: 819.75
 ---- batch: 110 ----
mean loss: 792.47
train mean loss: 810.69
epoch train time: 0:00:00.726154
elapsed time: 0:00:41.557182
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 01:56:57.943992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 794.60
 ---- batch: 020 ----
mean loss: 809.03
 ---- batch: 030 ----
mean loss: 800.90
 ---- batch: 040 ----
mean loss: 789.86
 ---- batch: 050 ----
mean loss: 795.59
 ---- batch: 060 ----
mean loss: 811.05
 ---- batch: 070 ----
mean loss: 808.35
 ---- batch: 080 ----
mean loss: 810.40
 ---- batch: 090 ----
mean loss: 796.15
 ---- batch: 100 ----
mean loss: 812.39
 ---- batch: 110 ----
mean loss: 814.89
train mean loss: 804.65
epoch train time: 0:00:00.724868
elapsed time: 0:00:42.282194
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 01:56:58.669033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.51
 ---- batch: 020 ----
mean loss: 793.26
 ---- batch: 030 ----
mean loss: 810.12
 ---- batch: 040 ----
mean loss: 810.75
 ---- batch: 050 ----
mean loss: 802.18
 ---- batch: 060 ----
mean loss: 790.57
 ---- batch: 070 ----
mean loss: 795.49
 ---- batch: 080 ----
mean loss: 785.99
 ---- batch: 090 ----
mean loss: 794.51
 ---- batch: 100 ----
mean loss: 794.32
 ---- batch: 110 ----
mean loss: 813.59
train mean loss: 798.45
epoch train time: 0:00:00.731962
elapsed time: 0:00:43.014329
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 01:56:59.401134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 798.53
 ---- batch: 020 ----
mean loss: 801.17
 ---- batch: 030 ----
mean loss: 790.30
 ---- batch: 040 ----
mean loss: 778.36
 ---- batch: 050 ----
mean loss: 790.33
 ---- batch: 060 ----
mean loss: 801.08
 ---- batch: 070 ----
mean loss: 803.07
 ---- batch: 080 ----
mean loss: 780.01
 ---- batch: 090 ----
mean loss: 782.48
 ---- batch: 100 ----
mean loss: 803.41
 ---- batch: 110 ----
mean loss: 779.38
train mean loss: 792.55
epoch train time: 0:00:00.721972
elapsed time: 0:00:43.736472
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 01:57:00.123293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 783.61
 ---- batch: 020 ----
mean loss: 758.31
 ---- batch: 030 ----
mean loss: 785.10
 ---- batch: 040 ----
mean loss: 807.83
 ---- batch: 050 ----
mean loss: 805.39
 ---- batch: 060 ----
mean loss: 801.87
 ---- batch: 070 ----
mean loss: 798.31
 ---- batch: 080 ----
mean loss: 785.07
 ---- batch: 090 ----
mean loss: 771.77
 ---- batch: 100 ----
mean loss: 779.53
 ---- batch: 110 ----
mean loss: 777.42
train mean loss: 786.77
epoch train time: 0:00:00.714892
elapsed time: 0:00:44.451519
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 01:57:00.838326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 763.67
 ---- batch: 020 ----
mean loss: 788.63
 ---- batch: 030 ----
mean loss: 769.18
 ---- batch: 040 ----
mean loss: 787.41
 ---- batch: 050 ----
mean loss: 797.10
 ---- batch: 060 ----
mean loss: 764.74
 ---- batch: 070 ----
mean loss: 791.76
 ---- batch: 080 ----
mean loss: 775.53
 ---- batch: 090 ----
mean loss: 775.88
 ---- batch: 100 ----
mean loss: 792.74
 ---- batch: 110 ----
mean loss: 788.09
train mean loss: 780.86
epoch train time: 0:00:00.717028
elapsed time: 0:00:45.168700
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 01:57:01.555508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 776.85
 ---- batch: 020 ----
mean loss: 788.48
 ---- batch: 030 ----
mean loss: 776.07
 ---- batch: 040 ----
mean loss: 757.43
 ---- batch: 050 ----
mean loss: 765.87
 ---- batch: 060 ----
mean loss: 781.09
 ---- batch: 070 ----
mean loss: 774.11
 ---- batch: 080 ----
mean loss: 774.42
 ---- batch: 090 ----
mean loss: 779.43
 ---- batch: 100 ----
mean loss: 769.34
 ---- batch: 110 ----
mean loss: 791.76
train mean loss: 774.99
epoch train time: 0:00:00.725712
elapsed time: 0:00:45.894548
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 01:57:02.281354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 770.01
 ---- batch: 020 ----
mean loss: 777.52
 ---- batch: 030 ----
mean loss: 781.44
 ---- batch: 040 ----
mean loss: 764.40
 ---- batch: 050 ----
mean loss: 756.37
 ---- batch: 060 ----
mean loss: 776.96
 ---- batch: 070 ----
mean loss: 770.59
 ---- batch: 080 ----
mean loss: 772.51
 ---- batch: 090 ----
mean loss: 762.64
 ---- batch: 100 ----
mean loss: 768.50
 ---- batch: 110 ----
mean loss: 769.97
train mean loss: 769.13
epoch train time: 0:00:00.740309
elapsed time: 0:00:46.634994
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 01:57:03.021818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 732.68
 ---- batch: 020 ----
mean loss: 800.70
 ---- batch: 030 ----
mean loss: 766.46
 ---- batch: 040 ----
mean loss: 786.18
 ---- batch: 050 ----
mean loss: 769.62
 ---- batch: 060 ----
mean loss: 770.20
 ---- batch: 070 ----
mean loss: 759.59
 ---- batch: 080 ----
mean loss: 769.34
 ---- batch: 090 ----
mean loss: 751.42
 ---- batch: 100 ----
mean loss: 747.85
 ---- batch: 110 ----
mean loss: 742.36
train mean loss: 763.13
epoch train time: 0:00:00.722564
elapsed time: 0:00:47.357718
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 01:57:03.744527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 751.01
 ---- batch: 020 ----
mean loss: 773.42
 ---- batch: 030 ----
mean loss: 748.93
 ---- batch: 040 ----
mean loss: 774.29
 ---- batch: 050 ----
mean loss: 760.64
 ---- batch: 060 ----
mean loss: 753.16
 ---- batch: 070 ----
mean loss: 765.49
 ---- batch: 080 ----
mean loss: 752.11
 ---- batch: 090 ----
mean loss: 747.68
 ---- batch: 100 ----
mean loss: 745.77
 ---- batch: 110 ----
mean loss: 759.61
train mean loss: 756.93
epoch train time: 0:00:00.718728
elapsed time: 0:00:48.076584
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 01:57:04.463391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.61
 ---- batch: 020 ----
mean loss: 739.01
 ---- batch: 030 ----
mean loss: 731.81
 ---- batch: 040 ----
mean loss: 756.33
 ---- batch: 050 ----
mean loss: 769.17
 ---- batch: 060 ----
mean loss: 747.36
 ---- batch: 070 ----
mean loss: 770.18
 ---- batch: 080 ----
mean loss: 739.96
 ---- batch: 090 ----
mean loss: 743.23
 ---- batch: 100 ----
mean loss: 772.42
 ---- batch: 110 ----
mean loss: 741.43
train mean loss: 750.38
epoch train time: 0:00:00.718305
elapsed time: 0:00:48.795026
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 01:57:05.181832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 727.78
 ---- batch: 020 ----
mean loss: 746.18
 ---- batch: 030 ----
mean loss: 759.34
 ---- batch: 040 ----
mean loss: 739.78
 ---- batch: 050 ----
mean loss: 757.68
 ---- batch: 060 ----
mean loss: 737.74
 ---- batch: 070 ----
mean loss: 740.52
 ---- batch: 080 ----
mean loss: 752.08
 ---- batch: 090 ----
mean loss: 739.02
 ---- batch: 100 ----
mean loss: 747.65
 ---- batch: 110 ----
mean loss: 727.97
train mean loss: 743.37
epoch train time: 0:00:00.720312
elapsed time: 0:00:49.515478
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 01:57:05.902316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.47
 ---- batch: 020 ----
mean loss: 737.86
 ---- batch: 030 ----
mean loss: 725.01
 ---- batch: 040 ----
mean loss: 743.93
 ---- batch: 050 ----
mean loss: 738.03
 ---- batch: 060 ----
mean loss: 744.87
 ---- batch: 070 ----
mean loss: 719.57
 ---- batch: 080 ----
mean loss: 737.30
 ---- batch: 090 ----
mean loss: 743.67
 ---- batch: 100 ----
mean loss: 722.81
 ---- batch: 110 ----
mean loss: 742.65
train mean loss: 735.92
epoch train time: 0:00:00.727928
elapsed time: 0:00:50.243576
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 01:57:06.630400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.20
 ---- batch: 020 ----
mean loss: 730.35
 ---- batch: 030 ----
mean loss: 721.41
 ---- batch: 040 ----
mean loss: 729.64
 ---- batch: 050 ----
mean loss: 729.67
 ---- batch: 060 ----
mean loss: 735.23
 ---- batch: 070 ----
mean loss: 740.01
 ---- batch: 080 ----
mean loss: 712.96
 ---- batch: 090 ----
mean loss: 735.44
 ---- batch: 100 ----
mean loss: 721.19
 ---- batch: 110 ----
mean loss: 723.25
train mean loss: 727.67
epoch train time: 0:00:00.724844
elapsed time: 0:00:50.968592
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 01:57:07.355431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 726.73
 ---- batch: 020 ----
mean loss: 726.23
 ---- batch: 030 ----
mean loss: 731.17
 ---- batch: 040 ----
mean loss: 725.77
 ---- batch: 050 ----
mean loss: 714.44
 ---- batch: 060 ----
mean loss: 705.44
 ---- batch: 070 ----
mean loss: 701.16
 ---- batch: 080 ----
mean loss: 734.81
 ---- batch: 090 ----
mean loss: 727.40
 ---- batch: 100 ----
mean loss: 701.84
 ---- batch: 110 ----
mean loss: 706.58
train mean loss: 717.82
epoch train time: 0:00:00.724088
elapsed time: 0:00:51.692846
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 01:57:08.079670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.64
 ---- batch: 020 ----
mean loss: 697.93
 ---- batch: 030 ----
mean loss: 724.99
 ---- batch: 040 ----
mean loss: 721.63
 ---- batch: 050 ----
mean loss: 701.59
 ---- batch: 060 ----
mean loss: 700.92
 ---- batch: 070 ----
mean loss: 698.76
 ---- batch: 080 ----
mean loss: 696.71
 ---- batch: 090 ----
mean loss: 705.46
 ---- batch: 100 ----
mean loss: 689.81
 ---- batch: 110 ----
mean loss: 696.80
train mean loss: 702.96
epoch train time: 0:00:00.721636
elapsed time: 0:00:52.414638
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 01:57:08.801448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 660.04
 ---- batch: 020 ----
mean loss: 690.15
 ---- batch: 030 ----
mean loss: 698.85
 ---- batch: 040 ----
mean loss: 708.09
 ---- batch: 050 ----
mean loss: 707.69
 ---- batch: 060 ----
mean loss: 675.92
 ---- batch: 070 ----
mean loss: 681.90
 ---- batch: 080 ----
mean loss: 683.38
 ---- batch: 090 ----
mean loss: 670.45
 ---- batch: 100 ----
mean loss: 682.25
 ---- batch: 110 ----
mean loss: 675.03
train mean loss: 684.86
epoch train time: 0:00:00.721733
elapsed time: 0:00:53.136511
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 01:57:09.523334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.55
 ---- batch: 020 ----
mean loss: 656.18
 ---- batch: 030 ----
mean loss: 666.95
 ---- batch: 040 ----
mean loss: 660.06
 ---- batch: 050 ----
mean loss: 675.82
 ---- batch: 060 ----
mean loss: 660.80
 ---- batch: 070 ----
mean loss: 660.05
 ---- batch: 080 ----
mean loss: 667.51
 ---- batch: 090 ----
mean loss: 660.46
 ---- batch: 100 ----
mean loss: 667.41
 ---- batch: 110 ----
mean loss: 659.28
train mean loss: 663.78
epoch train time: 0:00:00.717445
elapsed time: 0:00:53.854122
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 01:57:10.240928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.42
 ---- batch: 020 ----
mean loss: 643.69
 ---- batch: 030 ----
mean loss: 652.86
 ---- batch: 040 ----
mean loss: 646.36
 ---- batch: 050 ----
mean loss: 649.10
 ---- batch: 060 ----
mean loss: 651.05
 ---- batch: 070 ----
mean loss: 612.78
 ---- batch: 080 ----
mean loss: 652.81
 ---- batch: 090 ----
mean loss: 641.91
 ---- batch: 100 ----
mean loss: 637.76
 ---- batch: 110 ----
mean loss: 642.66
train mean loss: 644.07
epoch train time: 0:00:00.720870
elapsed time: 0:00:54.575129
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 01:57:10.961935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 635.43
 ---- batch: 020 ----
mean loss: 616.58
 ---- batch: 030 ----
mean loss: 640.28
 ---- batch: 040 ----
mean loss: 643.73
 ---- batch: 050 ----
mean loss: 618.00
 ---- batch: 060 ----
mean loss: 638.55
 ---- batch: 070 ----
mean loss: 619.62
 ---- batch: 080 ----
mean loss: 623.68
 ---- batch: 090 ----
mean loss: 616.41
 ---- batch: 100 ----
mean loss: 621.07
 ---- batch: 110 ----
mean loss: 612.01
train mean loss: 625.83
epoch train time: 0:00:00.715037
elapsed time: 0:00:55.290298
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 01:57:11.677104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 608.48
 ---- batch: 020 ----
mean loss: 607.84
 ---- batch: 030 ----
mean loss: 620.97
 ---- batch: 040 ----
mean loss: 619.28
 ---- batch: 050 ----
mean loss: 610.87
 ---- batch: 060 ----
mean loss: 617.56
 ---- batch: 070 ----
mean loss: 581.18
 ---- batch: 080 ----
mean loss: 617.05
 ---- batch: 090 ----
mean loss: 605.61
 ---- batch: 100 ----
mean loss: 614.71
 ---- batch: 110 ----
mean loss: 599.15
train mean loss: 609.23
epoch train time: 0:00:00.723392
elapsed time: 0:00:56.013823
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 01:57:12.400645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 595.57
 ---- batch: 020 ----
mean loss: 602.24
 ---- batch: 030 ----
mean loss: 588.95
 ---- batch: 040 ----
mean loss: 598.30
 ---- batch: 050 ----
mean loss: 584.18
 ---- batch: 060 ----
mean loss: 590.67
 ---- batch: 070 ----
mean loss: 608.01
 ---- batch: 080 ----
mean loss: 596.58
 ---- batch: 090 ----
mean loss: 598.39
 ---- batch: 100 ----
mean loss: 596.58
 ---- batch: 110 ----
mean loss: 585.01
train mean loss: 594.44
epoch train time: 0:00:00.731822
elapsed time: 0:00:56.745826
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 01:57:13.132631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 580.90
 ---- batch: 020 ----
mean loss: 558.59
 ---- batch: 030 ----
mean loss: 576.43
 ---- batch: 040 ----
mean loss: 600.01
 ---- batch: 050 ----
mean loss: 579.27
 ---- batch: 060 ----
mean loss: 592.45
 ---- batch: 070 ----
mean loss: 562.94
 ---- batch: 080 ----
mean loss: 588.26
 ---- batch: 090 ----
mean loss: 583.92
 ---- batch: 100 ----
mean loss: 589.98
 ---- batch: 110 ----
mean loss: 589.55
train mean loss: 581.27
epoch train time: 0:00:00.733231
elapsed time: 0:00:57.479192
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 01:57:13.866001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 598.03
 ---- batch: 020 ----
mean loss: 577.57
 ---- batch: 030 ----
mean loss: 570.63
 ---- batch: 040 ----
mean loss: 564.45
 ---- batch: 050 ----
mean loss: 575.01
 ---- batch: 060 ----
mean loss: 564.67
 ---- batch: 070 ----
mean loss: 566.34
 ---- batch: 080 ----
mean loss: 563.20
 ---- batch: 090 ----
mean loss: 555.11
 ---- batch: 100 ----
mean loss: 573.11
 ---- batch: 110 ----
mean loss: 562.92
train mean loss: 569.53
epoch train time: 0:00:00.712285
elapsed time: 0:00:58.191629
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 01:57:14.578446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.97
 ---- batch: 020 ----
mean loss: 560.89
 ---- batch: 030 ----
mean loss: 566.83
 ---- batch: 040 ----
mean loss: 559.93
 ---- batch: 050 ----
mean loss: 558.11
 ---- batch: 060 ----
mean loss: 564.42
 ---- batch: 070 ----
mean loss: 553.17
 ---- batch: 080 ----
mean loss: 552.95
 ---- batch: 090 ----
mean loss: 561.12
 ---- batch: 100 ----
mean loss: 558.93
 ---- batch: 110 ----
mean loss: 540.17
train mean loss: 558.61
epoch train time: 0:00:00.728011
elapsed time: 0:00:58.919798
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 01:57:15.306641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.24
 ---- batch: 020 ----
mean loss: 556.48
 ---- batch: 030 ----
mean loss: 562.90
 ---- batch: 040 ----
mean loss: 551.83
 ---- batch: 050 ----
mean loss: 561.36
 ---- batch: 060 ----
mean loss: 531.01
 ---- batch: 070 ----
mean loss: 540.44
 ---- batch: 080 ----
mean loss: 549.06
 ---- batch: 090 ----
mean loss: 539.47
 ---- batch: 100 ----
mean loss: 538.74
 ---- batch: 110 ----
mean loss: 555.64
train mean loss: 548.24
epoch train time: 0:00:00.720740
elapsed time: 0:00:59.640715
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 01:57:16.027520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 548.93
 ---- batch: 020 ----
mean loss: 548.80
 ---- batch: 030 ----
mean loss: 544.07
 ---- batch: 040 ----
mean loss: 532.49
 ---- batch: 050 ----
mean loss: 528.49
 ---- batch: 060 ----
mean loss: 535.37
 ---- batch: 070 ----
mean loss: 528.61
 ---- batch: 080 ----
mean loss: 546.75
 ---- batch: 090 ----
mean loss: 538.56
 ---- batch: 100 ----
mean loss: 525.47
 ---- batch: 110 ----
mean loss: 536.85
train mean loss: 537.94
epoch train time: 0:00:00.722141
elapsed time: 0:01:00.362990
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 01:57:16.749812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 526.98
 ---- batch: 020 ----
mean loss: 539.30
 ---- batch: 030 ----
mean loss: 515.02
 ---- batch: 040 ----
mean loss: 525.77
 ---- batch: 050 ----
mean loss: 537.71
 ---- batch: 060 ----
mean loss: 524.22
 ---- batch: 070 ----
mean loss: 527.92
 ---- batch: 080 ----
mean loss: 529.63
 ---- batch: 090 ----
mean loss: 521.93
 ---- batch: 100 ----
mean loss: 530.31
 ---- batch: 110 ----
mean loss: 528.99
train mean loss: 526.93
epoch train time: 0:00:00.719896
elapsed time: 0:01:01.083042
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 01:57:17.469865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 510.80
 ---- batch: 020 ----
mean loss: 524.71
 ---- batch: 030 ----
mean loss: 511.89
 ---- batch: 040 ----
mean loss: 518.42
 ---- batch: 050 ----
mean loss: 516.95
 ---- batch: 060 ----
mean loss: 520.91
 ---- batch: 070 ----
mean loss: 523.87
 ---- batch: 080 ----
mean loss: 520.32
 ---- batch: 090 ----
mean loss: 500.66
 ---- batch: 100 ----
mean loss: 511.61
 ---- batch: 110 ----
mean loss: 515.05
train mean loss: 515.64
epoch train time: 0:00:00.723108
elapsed time: 0:01:01.806300
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 01:57:18.193106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.55
 ---- batch: 020 ----
mean loss: 503.74
 ---- batch: 030 ----
mean loss: 510.93
 ---- batch: 040 ----
mean loss: 509.19
 ---- batch: 050 ----
mean loss: 498.42
 ---- batch: 060 ----
mean loss: 492.84
 ---- batch: 070 ----
mean loss: 521.20
 ---- batch: 080 ----
mean loss: 514.30
 ---- batch: 090 ----
mean loss: 505.41
 ---- batch: 100 ----
mean loss: 498.87
 ---- batch: 110 ----
mean loss: 492.73
train mean loss: 504.47
epoch train time: 0:00:00.728430
elapsed time: 0:01:02.534890
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 01:57:18.921717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.32
 ---- batch: 020 ----
mean loss: 491.35
 ---- batch: 030 ----
mean loss: 496.86
 ---- batch: 040 ----
mean loss: 506.05
 ---- batch: 050 ----
mean loss: 495.83
 ---- batch: 060 ----
mean loss: 497.74
 ---- batch: 070 ----
mean loss: 500.91
 ---- batch: 080 ----
mean loss: 483.79
 ---- batch: 090 ----
mean loss: 488.41
 ---- batch: 100 ----
mean loss: 473.54
 ---- batch: 110 ----
mean loss: 496.04
train mean loss: 493.46
epoch train time: 0:00:00.717172
elapsed time: 0:01:03.252215
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 01:57:19.639025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.72
 ---- batch: 020 ----
mean loss: 476.88
 ---- batch: 030 ----
mean loss: 492.50
 ---- batch: 040 ----
mean loss: 493.69
 ---- batch: 050 ----
mean loss: 483.62
 ---- batch: 060 ----
mean loss: 484.59
 ---- batch: 070 ----
mean loss: 473.86
 ---- batch: 080 ----
mean loss: 485.93
 ---- batch: 090 ----
mean loss: 475.57
 ---- batch: 100 ----
mean loss: 483.81
 ---- batch: 110 ----
mean loss: 469.95
train mean loss: 482.22
epoch train time: 0:00:00.724752
elapsed time: 0:01:03.977103
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 01:57:20.363916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.51
 ---- batch: 020 ----
mean loss: 476.28
 ---- batch: 030 ----
mean loss: 464.66
 ---- batch: 040 ----
mean loss: 474.74
 ---- batch: 050 ----
mean loss: 471.69
 ---- batch: 060 ----
mean loss: 499.71
 ---- batch: 070 ----
mean loss: 478.44
 ---- batch: 080 ----
mean loss: 464.33
 ---- batch: 090 ----
mean loss: 470.55
 ---- batch: 100 ----
mean loss: 455.70
 ---- batch: 110 ----
mean loss: 464.75
train mean loss: 470.95
epoch train time: 0:00:00.717991
elapsed time: 0:01:04.695242
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 01:57:21.082048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.02
 ---- batch: 020 ----
mean loss: 476.54
 ---- batch: 030 ----
mean loss: 470.66
 ---- batch: 040 ----
mean loss: 464.81
 ---- batch: 050 ----
mean loss: 459.58
 ---- batch: 060 ----
mean loss: 453.42
 ---- batch: 070 ----
mean loss: 456.52
 ---- batch: 080 ----
mean loss: 457.42
 ---- batch: 090 ----
mean loss: 447.79
 ---- batch: 100 ----
mean loss: 461.35
 ---- batch: 110 ----
mean loss: 459.98
train mean loss: 460.00
epoch train time: 0:00:00.717405
elapsed time: 0:01:05.412783
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 01:57:21.799609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.82
 ---- batch: 020 ----
mean loss: 461.70
 ---- batch: 030 ----
mean loss: 454.40
 ---- batch: 040 ----
mean loss: 446.52
 ---- batch: 050 ----
mean loss: 451.86
 ---- batch: 060 ----
mean loss: 436.51
 ---- batch: 070 ----
mean loss: 461.10
 ---- batch: 080 ----
mean loss: 447.51
 ---- batch: 090 ----
mean loss: 439.20
 ---- batch: 100 ----
mean loss: 441.23
 ---- batch: 110 ----
mean loss: 450.68
train mean loss: 449.35
epoch train time: 0:00:00.727499
elapsed time: 0:01:06.140452
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 01:57:22.527270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.01
 ---- batch: 020 ----
mean loss: 453.80
 ---- batch: 030 ----
mean loss: 440.65
 ---- batch: 040 ----
mean loss: 432.89
 ---- batch: 050 ----
mean loss: 446.59
 ---- batch: 060 ----
mean loss: 434.80
 ---- batch: 070 ----
mean loss: 451.38
 ---- batch: 080 ----
mean loss: 433.40
 ---- batch: 090 ----
mean loss: 443.65
 ---- batch: 100 ----
mean loss: 419.81
 ---- batch: 110 ----
mean loss: 434.32
train mean loss: 439.09
epoch train time: 0:00:00.712328
elapsed time: 0:01:06.852926
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 01:57:23.239732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.64
 ---- batch: 020 ----
mean loss: 429.13
 ---- batch: 030 ----
mean loss: 434.73
 ---- batch: 040 ----
mean loss: 416.85
 ---- batch: 050 ----
mean loss: 425.81
 ---- batch: 060 ----
mean loss: 436.70
 ---- batch: 070 ----
mean loss: 431.36
 ---- batch: 080 ----
mean loss: 436.73
 ---- batch: 090 ----
mean loss: 425.36
 ---- batch: 100 ----
mean loss: 426.63
 ---- batch: 110 ----
mean loss: 422.47
train mean loss: 429.55
epoch train time: 0:00:00.718513
elapsed time: 0:01:07.571578
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 01:57:23.958392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.87
 ---- batch: 020 ----
mean loss: 428.94
 ---- batch: 030 ----
mean loss: 417.36
 ---- batch: 040 ----
mean loss: 428.61
 ---- batch: 050 ----
mean loss: 415.21
 ---- batch: 060 ----
mean loss: 416.04
 ---- batch: 070 ----
mean loss: 417.98
 ---- batch: 080 ----
mean loss: 425.48
 ---- batch: 090 ----
mean loss: 423.08
 ---- batch: 100 ----
mean loss: 412.90
 ---- batch: 110 ----
mean loss: 402.11
train mean loss: 420.25
epoch train time: 0:00:00.717525
elapsed time: 0:01:08.289247
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 01:57:24.676055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.86
 ---- batch: 020 ----
mean loss: 413.00
 ---- batch: 030 ----
mean loss: 408.98
 ---- batch: 040 ----
mean loss: 415.70
 ---- batch: 050 ----
mean loss: 416.04
 ---- batch: 060 ----
mean loss: 410.18
 ---- batch: 070 ----
mean loss: 414.26
 ---- batch: 080 ----
mean loss: 406.18
 ---- batch: 090 ----
mean loss: 409.38
 ---- batch: 100 ----
mean loss: 410.52
 ---- batch: 110 ----
mean loss: 410.65
train mean loss: 411.90
epoch train time: 0:00:00.728353
elapsed time: 0:01:09.017785
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 01:57:25.404617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.97
 ---- batch: 020 ----
mean loss: 409.37
 ---- batch: 030 ----
mean loss: 421.20
 ---- batch: 040 ----
mean loss: 400.05
 ---- batch: 050 ----
mean loss: 395.65
 ---- batch: 060 ----
mean loss: 403.26
 ---- batch: 070 ----
mean loss: 401.75
 ---- batch: 080 ----
mean loss: 403.66
 ---- batch: 090 ----
mean loss: 396.61
 ---- batch: 100 ----
mean loss: 409.75
 ---- batch: 110 ----
mean loss: 404.43
train mean loss: 403.75
epoch train time: 0:00:00.726325
elapsed time: 0:01:09.744275
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 01:57:26.131085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.12
 ---- batch: 020 ----
mean loss: 411.40
 ---- batch: 030 ----
mean loss: 399.72
 ---- batch: 040 ----
mean loss: 389.64
 ---- batch: 050 ----
mean loss: 384.32
 ---- batch: 060 ----
mean loss: 395.81
 ---- batch: 070 ----
mean loss: 397.89
 ---- batch: 080 ----
mean loss: 398.79
 ---- batch: 090 ----
mean loss: 386.59
 ---- batch: 100 ----
mean loss: 389.22
 ---- batch: 110 ----
mean loss: 398.93
train mean loss: 395.95
epoch train time: 0:00:00.732123
elapsed time: 0:01:10.476540
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 01:57:26.863366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.35
 ---- batch: 020 ----
mean loss: 392.71
 ---- batch: 030 ----
mean loss: 397.81
 ---- batch: 040 ----
mean loss: 385.15
 ---- batch: 050 ----
mean loss: 383.63
 ---- batch: 060 ----
mean loss: 390.84
 ---- batch: 070 ----
mean loss: 388.98
 ---- batch: 080 ----
mean loss: 388.12
 ---- batch: 090 ----
mean loss: 375.71
 ---- batch: 100 ----
mean loss: 387.15
 ---- batch: 110 ----
mean loss: 383.06
train mean loss: 387.70
epoch train time: 0:00:00.716120
elapsed time: 0:01:11.192815
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 01:57:27.579621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.02
 ---- batch: 020 ----
mean loss: 378.16
 ---- batch: 030 ----
mean loss: 391.49
 ---- batch: 040 ----
mean loss: 389.24
 ---- batch: 050 ----
mean loss: 384.93
 ---- batch: 060 ----
mean loss: 363.18
 ---- batch: 070 ----
mean loss: 367.87
 ---- batch: 080 ----
mean loss: 370.75
 ---- batch: 090 ----
mean loss: 374.90
 ---- batch: 100 ----
mean loss: 390.11
 ---- batch: 110 ----
mean loss: 373.43
train mean loss: 378.40
epoch train time: 0:00:00.720797
elapsed time: 0:01:11.913749
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 01:57:28.300557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.26
 ---- batch: 020 ----
mean loss: 373.90
 ---- batch: 030 ----
mean loss: 381.44
 ---- batch: 040 ----
mean loss: 369.55
 ---- batch: 050 ----
mean loss: 369.53
 ---- batch: 060 ----
mean loss: 360.96
 ---- batch: 070 ----
mean loss: 355.47
 ---- batch: 080 ----
mean loss: 369.87
 ---- batch: 090 ----
mean loss: 372.54
 ---- batch: 100 ----
mean loss: 356.03
 ---- batch: 110 ----
mean loss: 361.47
train mean loss: 368.80
epoch train time: 0:00:00.721175
elapsed time: 0:01:12.635063
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 01:57:29.021874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.48
 ---- batch: 020 ----
mean loss: 363.37
 ---- batch: 030 ----
mean loss: 370.08
 ---- batch: 040 ----
mean loss: 359.22
 ---- batch: 050 ----
mean loss: 368.70
 ---- batch: 060 ----
mean loss: 369.53
 ---- batch: 070 ----
mean loss: 347.51
 ---- batch: 080 ----
mean loss: 345.80
 ---- batch: 090 ----
mean loss: 356.52
 ---- batch: 100 ----
mean loss: 351.56
 ---- batch: 110 ----
mean loss: 357.49
train mean loss: 359.56
epoch train time: 0:00:00.720343
elapsed time: 0:01:13.355547
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 01:57:29.742353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.86
 ---- batch: 020 ----
mean loss: 355.75
 ---- batch: 030 ----
mean loss: 342.64
 ---- batch: 040 ----
mean loss: 347.78
 ---- batch: 050 ----
mean loss: 346.74
 ---- batch: 060 ----
mean loss: 351.67
 ---- batch: 070 ----
mean loss: 352.54
 ---- batch: 080 ----
mean loss: 339.96
 ---- batch: 090 ----
mean loss: 356.90
 ---- batch: 100 ----
mean loss: 349.41
 ---- batch: 110 ----
mean loss: 354.75
train mean loss: 351.29
epoch train time: 0:00:00.723199
elapsed time: 0:01:14.078902
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 01:57:30.465714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.31
 ---- batch: 020 ----
mean loss: 349.81
 ---- batch: 030 ----
mean loss: 350.40
 ---- batch: 040 ----
mean loss: 347.31
 ---- batch: 050 ----
mean loss: 339.51
 ---- batch: 060 ----
mean loss: 325.21
 ---- batch: 070 ----
mean loss: 336.68
 ---- batch: 080 ----
mean loss: 352.70
 ---- batch: 090 ----
mean loss: 340.67
 ---- batch: 100 ----
mean loss: 349.55
 ---- batch: 110 ----
mean loss: 344.38
train mean loss: 343.75
epoch train time: 0:00:00.723195
elapsed time: 0:01:14.802241
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 01:57:31.189048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.84
 ---- batch: 020 ----
mean loss: 338.07
 ---- batch: 030 ----
mean loss: 349.68
 ---- batch: 040 ----
mean loss: 337.77
 ---- batch: 050 ----
mean loss: 329.54
 ---- batch: 060 ----
mean loss: 333.34
 ---- batch: 070 ----
mean loss: 320.39
 ---- batch: 080 ----
mean loss: 337.28
 ---- batch: 090 ----
mean loss: 350.20
 ---- batch: 100 ----
mean loss: 340.66
 ---- batch: 110 ----
mean loss: 333.83
train mean loss: 337.52
epoch train time: 0:00:00.731228
elapsed time: 0:01:15.533619
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 01:57:31.920443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.85
 ---- batch: 020 ----
mean loss: 323.00
 ---- batch: 030 ----
mean loss: 342.29
 ---- batch: 040 ----
mean loss: 331.90
 ---- batch: 050 ----
mean loss: 341.69
 ---- batch: 060 ----
mean loss: 325.26
 ---- batch: 070 ----
mean loss: 323.33
 ---- batch: 080 ----
mean loss: 329.68
 ---- batch: 090 ----
mean loss: 327.61
 ---- batch: 100 ----
mean loss: 333.44
 ---- batch: 110 ----
mean loss: 330.16
train mean loss: 331.76
epoch train time: 0:00:00.730189
elapsed time: 0:01:16.263991
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 01:57:32.650799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.61
 ---- batch: 020 ----
mean loss: 335.77
 ---- batch: 030 ----
mean loss: 327.49
 ---- batch: 040 ----
mean loss: 317.37
 ---- batch: 050 ----
mean loss: 328.37
 ---- batch: 060 ----
mean loss: 327.98
 ---- batch: 070 ----
mean loss: 333.33
 ---- batch: 080 ----
mean loss: 322.33
 ---- batch: 090 ----
mean loss: 331.71
 ---- batch: 100 ----
mean loss: 321.31
 ---- batch: 110 ----
mean loss: 333.52
train mean loss: 326.78
epoch train time: 0:00:00.727759
elapsed time: 0:01:16.991888
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 01:57:33.378693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.16
 ---- batch: 020 ----
mean loss: 311.75
 ---- batch: 030 ----
mean loss: 317.44
 ---- batch: 040 ----
mean loss: 333.52
 ---- batch: 050 ----
mean loss: 321.66
 ---- batch: 060 ----
mean loss: 321.19
 ---- batch: 070 ----
mean loss: 308.34
 ---- batch: 080 ----
mean loss: 326.50
 ---- batch: 090 ----
mean loss: 324.31
 ---- batch: 100 ----
mean loss: 323.06
 ---- batch: 110 ----
mean loss: 322.94
train mean loss: 322.42
epoch train time: 0:00:00.721543
elapsed time: 0:01:17.713563
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 01:57:34.100371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.28
 ---- batch: 020 ----
mean loss: 312.81
 ---- batch: 030 ----
mean loss: 315.44
 ---- batch: 040 ----
mean loss: 323.27
 ---- batch: 050 ----
mean loss: 308.91
 ---- batch: 060 ----
mean loss: 313.21
 ---- batch: 070 ----
mean loss: 327.65
 ---- batch: 080 ----
mean loss: 315.68
 ---- batch: 090 ----
mean loss: 316.04
 ---- batch: 100 ----
mean loss: 318.03
 ---- batch: 110 ----
mean loss: 336.91
train mean loss: 318.47
epoch train time: 0:00:00.720790
elapsed time: 0:01:18.434500
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 01:57:34.821315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.12
 ---- batch: 020 ----
mean loss: 319.36
 ---- batch: 030 ----
mean loss: 313.66
 ---- batch: 040 ----
mean loss: 300.09
 ---- batch: 050 ----
mean loss: 314.15
 ---- batch: 060 ----
mean loss: 315.80
 ---- batch: 070 ----
mean loss: 315.51
 ---- batch: 080 ----
mean loss: 316.89
 ---- batch: 090 ----
mean loss: 329.03
 ---- batch: 100 ----
mean loss: 311.48
 ---- batch: 110 ----
mean loss: 310.07
train mean loss: 315.10
epoch train time: 0:00:00.726641
elapsed time: 0:01:19.161288
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 01:57:35.548095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.60
 ---- batch: 020 ----
mean loss: 309.16
 ---- batch: 030 ----
mean loss: 311.22
 ---- batch: 040 ----
mean loss: 316.50
 ---- batch: 050 ----
mean loss: 323.79
 ---- batch: 060 ----
mean loss: 306.83
 ---- batch: 070 ----
mean loss: 313.15
 ---- batch: 080 ----
mean loss: 310.13
 ---- batch: 090 ----
mean loss: 302.46
 ---- batch: 100 ----
mean loss: 303.98
 ---- batch: 110 ----
mean loss: 314.20
train mean loss: 311.97
epoch train time: 0:00:00.726465
elapsed time: 0:01:19.887932
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 01:57:36.274755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.75
 ---- batch: 020 ----
mean loss: 315.32
 ---- batch: 030 ----
mean loss: 310.05
 ---- batch: 040 ----
mean loss: 307.53
 ---- batch: 050 ----
mean loss: 302.13
 ---- batch: 060 ----
mean loss: 319.27
 ---- batch: 070 ----
mean loss: 312.53
 ---- batch: 080 ----
mean loss: 310.77
 ---- batch: 090 ----
mean loss: 308.78
 ---- batch: 100 ----
mean loss: 308.38
 ---- batch: 110 ----
mean loss: 299.41
train mean loss: 309.35
epoch train time: 0:00:00.734564
elapsed time: 0:01:20.622671
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 01:57:37.009479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.94
 ---- batch: 020 ----
mean loss: 308.90
 ---- batch: 030 ----
mean loss: 301.13
 ---- batch: 040 ----
mean loss: 311.09
 ---- batch: 050 ----
mean loss: 306.04
 ---- batch: 060 ----
mean loss: 302.13
 ---- batch: 070 ----
mean loss: 303.67
 ---- batch: 080 ----
mean loss: 301.84
 ---- batch: 090 ----
mean loss: 303.74
 ---- batch: 100 ----
mean loss: 308.24
 ---- batch: 110 ----
mean loss: 312.63
train mean loss: 306.83
epoch train time: 0:00:00.720477
elapsed time: 0:01:21.343285
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 01:57:37.730091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.51
 ---- batch: 020 ----
mean loss: 309.12
 ---- batch: 030 ----
mean loss: 299.48
 ---- batch: 040 ----
mean loss: 309.88
 ---- batch: 050 ----
mean loss: 306.48
 ---- batch: 060 ----
mean loss: 315.40
 ---- batch: 070 ----
mean loss: 306.46
 ---- batch: 080 ----
mean loss: 297.03
 ---- batch: 090 ----
mean loss: 299.61
 ---- batch: 100 ----
mean loss: 293.67
 ---- batch: 110 ----
mean loss: 310.80
train mean loss: 304.27
epoch train time: 0:00:00.715439
elapsed time: 0:01:22.058894
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 01:57:38.445724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.33
 ---- batch: 020 ----
mean loss: 301.19
 ---- batch: 030 ----
mean loss: 303.83
 ---- batch: 040 ----
mean loss: 294.52
 ---- batch: 050 ----
mean loss: 295.46
 ---- batch: 060 ----
mean loss: 311.75
 ---- batch: 070 ----
mean loss: 305.56
 ---- batch: 080 ----
mean loss: 296.78
 ---- batch: 090 ----
mean loss: 304.91
 ---- batch: 100 ----
mean loss: 292.74
 ---- batch: 110 ----
mean loss: 309.01
train mean loss: 302.14
epoch train time: 0:00:00.727869
elapsed time: 0:01:22.786944
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 01:57:39.173767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.68
 ---- batch: 020 ----
mean loss: 302.28
 ---- batch: 030 ----
mean loss: 306.84
 ---- batch: 040 ----
mean loss: 303.50
 ---- batch: 050 ----
mean loss: 295.90
 ---- batch: 060 ----
mean loss: 289.94
 ---- batch: 070 ----
mean loss: 302.13
 ---- batch: 080 ----
mean loss: 298.73
 ---- batch: 090 ----
mean loss: 301.51
 ---- batch: 100 ----
mean loss: 301.32
 ---- batch: 110 ----
mean loss: 299.37
train mean loss: 299.96
epoch train time: 0:00:00.715671
elapsed time: 0:01:23.502784
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 01:57:39.889592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.41
 ---- batch: 020 ----
mean loss: 296.33
 ---- batch: 030 ----
mean loss: 299.29
 ---- batch: 040 ----
mean loss: 296.64
 ---- batch: 050 ----
mean loss: 277.88
 ---- batch: 060 ----
mean loss: 295.18
 ---- batch: 070 ----
mean loss: 305.58
 ---- batch: 080 ----
mean loss: 306.98
 ---- batch: 090 ----
mean loss: 295.53
 ---- batch: 100 ----
mean loss: 306.21
 ---- batch: 110 ----
mean loss: 294.11
train mean loss: 298.01
epoch train time: 0:00:00.713014
elapsed time: 0:01:24.215951
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 01:57:40.602759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.01
 ---- batch: 020 ----
mean loss: 291.17
 ---- batch: 030 ----
mean loss: 299.12
 ---- batch: 040 ----
mean loss: 304.48
 ---- batch: 050 ----
mean loss: 301.48
 ---- batch: 060 ----
mean loss: 294.35
 ---- batch: 070 ----
mean loss: 295.16
 ---- batch: 080 ----
mean loss: 296.69
 ---- batch: 090 ----
mean loss: 299.43
 ---- batch: 100 ----
mean loss: 295.76
 ---- batch: 110 ----
mean loss: 296.16
train mean loss: 296.06
epoch train time: 0:00:00.716050
elapsed time: 0:01:24.932141
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 01:57:41.318964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.01
 ---- batch: 020 ----
mean loss: 299.38
 ---- batch: 030 ----
mean loss: 289.67
 ---- batch: 040 ----
mean loss: 291.85
 ---- batch: 050 ----
mean loss: 286.11
 ---- batch: 060 ----
mean loss: 295.89
 ---- batch: 070 ----
mean loss: 296.12
 ---- batch: 080 ----
mean loss: 301.96
 ---- batch: 090 ----
mean loss: 289.11
 ---- batch: 100 ----
mean loss: 292.65
 ---- batch: 110 ----
mean loss: 296.02
train mean loss: 294.43
epoch train time: 0:00:00.724655
elapsed time: 0:01:25.656969
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 01:57:42.043776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.97
 ---- batch: 020 ----
mean loss: 289.86
 ---- batch: 030 ----
mean loss: 287.00
 ---- batch: 040 ----
mean loss: 293.46
 ---- batch: 050 ----
mean loss: 283.91
 ---- batch: 060 ----
mean loss: 305.41
 ---- batch: 070 ----
mean loss: 297.37
 ---- batch: 080 ----
mean loss: 292.40
 ---- batch: 090 ----
mean loss: 286.56
 ---- batch: 100 ----
mean loss: 298.28
 ---- batch: 110 ----
mean loss: 287.67
train mean loss: 292.72
epoch train time: 0:00:00.719886
elapsed time: 0:01:26.377007
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 01:57:42.763812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.36
 ---- batch: 020 ----
mean loss: 293.08
 ---- batch: 030 ----
mean loss: 303.39
 ---- batch: 040 ----
mean loss: 296.54
 ---- batch: 050 ----
mean loss: 300.05
 ---- batch: 060 ----
mean loss: 287.39
 ---- batch: 070 ----
mean loss: 284.19
 ---- batch: 080 ----
mean loss: 281.88
 ---- batch: 090 ----
mean loss: 290.38
 ---- batch: 100 ----
mean loss: 287.33
 ---- batch: 110 ----
mean loss: 287.12
train mean loss: 290.90
epoch train time: 0:00:00.715158
elapsed time: 0:01:27.092299
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 01:57:43.479104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.14
 ---- batch: 020 ----
mean loss: 288.03
 ---- batch: 030 ----
mean loss: 289.28
 ---- batch: 040 ----
mean loss: 290.61
 ---- batch: 050 ----
mean loss: 289.26
 ---- batch: 060 ----
mean loss: 293.93
 ---- batch: 070 ----
mean loss: 290.57
 ---- batch: 080 ----
mean loss: 292.23
 ---- batch: 090 ----
mean loss: 289.67
 ---- batch: 100 ----
mean loss: 288.52
 ---- batch: 110 ----
mean loss: 288.69
train mean loss: 289.15
epoch train time: 0:00:00.718860
elapsed time: 0:01:27.811304
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 01:57:44.198115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.17
 ---- batch: 020 ----
mean loss: 292.71
 ---- batch: 030 ----
mean loss: 281.74
 ---- batch: 040 ----
mean loss: 286.08
 ---- batch: 050 ----
mean loss: 295.99
 ---- batch: 060 ----
mean loss: 292.53
 ---- batch: 070 ----
mean loss: 282.42
 ---- batch: 080 ----
mean loss: 286.59
 ---- batch: 090 ----
mean loss: 294.83
 ---- batch: 100 ----
mean loss: 284.11
 ---- batch: 110 ----
mean loss: 280.88
train mean loss: 287.70
epoch train time: 0:00:00.717487
elapsed time: 0:01:28.528935
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 01:57:44.915742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.93
 ---- batch: 020 ----
mean loss: 286.70
 ---- batch: 030 ----
mean loss: 282.31
 ---- batch: 040 ----
mean loss: 286.82
 ---- batch: 050 ----
mean loss: 288.74
 ---- batch: 060 ----
mean loss: 293.97
 ---- batch: 070 ----
mean loss: 288.39
 ---- batch: 080 ----
mean loss: 293.77
 ---- batch: 090 ----
mean loss: 276.62
 ---- batch: 100 ----
mean loss: 282.72
 ---- batch: 110 ----
mean loss: 282.38
train mean loss: 286.16
epoch train time: 0:00:00.723802
elapsed time: 0:01:29.252875
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 01:57:45.639701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.35
 ---- batch: 020 ----
mean loss: 284.91
 ---- batch: 030 ----
mean loss: 282.64
 ---- batch: 040 ----
mean loss: 291.68
 ---- batch: 050 ----
mean loss: 288.19
 ---- batch: 060 ----
mean loss: 284.71
 ---- batch: 070 ----
mean loss: 280.68
 ---- batch: 080 ----
mean loss: 283.28
 ---- batch: 090 ----
mean loss: 294.77
 ---- batch: 100 ----
mean loss: 268.81
 ---- batch: 110 ----
mean loss: 290.59
train mean loss: 284.76
epoch train time: 0:00:00.726909
elapsed time: 0:01:29.979948
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 01:57:46.366762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.32
 ---- batch: 020 ----
mean loss: 284.48
 ---- batch: 030 ----
mean loss: 280.09
 ---- batch: 040 ----
mean loss: 278.77
 ---- batch: 050 ----
mean loss: 288.67
 ---- batch: 060 ----
mean loss: 282.57
 ---- batch: 070 ----
mean loss: 289.46
 ---- batch: 080 ----
mean loss: 278.54
 ---- batch: 090 ----
mean loss: 283.66
 ---- batch: 100 ----
mean loss: 289.80
 ---- batch: 110 ----
mean loss: 278.67
train mean loss: 283.38
epoch train time: 0:00:00.722292
elapsed time: 0:01:30.702384
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 01:57:47.089190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.63
 ---- batch: 020 ----
mean loss: 284.71
 ---- batch: 030 ----
mean loss: 275.26
 ---- batch: 040 ----
mean loss: 276.33
 ---- batch: 050 ----
mean loss: 277.93
 ---- batch: 060 ----
mean loss: 281.65
 ---- batch: 070 ----
mean loss: 280.22
 ---- batch: 080 ----
mean loss: 293.45
 ---- batch: 090 ----
mean loss: 289.16
 ---- batch: 100 ----
mean loss: 273.30
 ---- batch: 110 ----
mean loss: 289.91
train mean loss: 281.92
epoch train time: 0:00:00.718977
elapsed time: 0:01:31.421498
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 01:57:47.808304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.66
 ---- batch: 020 ----
mean loss: 286.50
 ---- batch: 030 ----
mean loss: 281.17
 ---- batch: 040 ----
mean loss: 285.28
 ---- batch: 050 ----
mean loss: 281.10
 ---- batch: 060 ----
mean loss: 282.45
 ---- batch: 070 ----
mean loss: 280.46
 ---- batch: 080 ----
mean loss: 276.50
 ---- batch: 090 ----
mean loss: 274.34
 ---- batch: 100 ----
mean loss: 282.26
 ---- batch: 110 ----
mean loss: 286.33
train mean loss: 280.82
epoch train time: 0:00:00.716642
elapsed time: 0:01:32.138273
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 01:57:48.525097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.61
 ---- batch: 020 ----
mean loss: 286.28
 ---- batch: 030 ----
mean loss: 276.53
 ---- batch: 040 ----
mean loss: 277.43
 ---- batch: 050 ----
mean loss: 281.98
 ---- batch: 060 ----
mean loss: 278.22
 ---- batch: 070 ----
mean loss: 266.12
 ---- batch: 080 ----
mean loss: 278.08
 ---- batch: 090 ----
mean loss: 280.32
 ---- batch: 100 ----
mean loss: 283.82
 ---- batch: 110 ----
mean loss: 279.00
train mean loss: 279.16
epoch train time: 0:00:00.723478
elapsed time: 0:01:32.861903
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 01:57:49.248712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.98
 ---- batch: 020 ----
mean loss: 281.52
 ---- batch: 030 ----
mean loss: 278.64
 ---- batch: 040 ----
mean loss: 272.95
 ---- batch: 050 ----
mean loss: 268.70
 ---- batch: 060 ----
mean loss: 275.87
 ---- batch: 070 ----
mean loss: 283.93
 ---- batch: 080 ----
mean loss: 287.30
 ---- batch: 090 ----
mean loss: 278.30
 ---- batch: 100 ----
mean loss: 283.34
 ---- batch: 110 ----
mean loss: 271.98
train mean loss: 277.97
epoch train time: 0:00:00.723804
elapsed time: 0:01:33.585840
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 01:57:49.972646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.39
 ---- batch: 020 ----
mean loss: 286.87
 ---- batch: 030 ----
mean loss: 267.82
 ---- batch: 040 ----
mean loss: 283.39
 ---- batch: 050 ----
mean loss: 284.12
 ---- batch: 060 ----
mean loss: 270.31
 ---- batch: 070 ----
mean loss: 278.94
 ---- batch: 080 ----
mean loss: 279.13
 ---- batch: 090 ----
mean loss: 276.21
 ---- batch: 100 ----
mean loss: 273.99
 ---- batch: 110 ----
mean loss: 269.82
train mean loss: 276.63
epoch train time: 0:00:00.719621
elapsed time: 0:01:34.305596
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 01:57:50.692418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.09
 ---- batch: 020 ----
mean loss: 284.87
 ---- batch: 030 ----
mean loss: 273.86
 ---- batch: 040 ----
mean loss: 276.73
 ---- batch: 050 ----
mean loss: 278.04
 ---- batch: 060 ----
mean loss: 270.32
 ---- batch: 070 ----
mean loss: 265.00
 ---- batch: 080 ----
mean loss: 267.70
 ---- batch: 090 ----
mean loss: 277.79
 ---- batch: 100 ----
mean loss: 277.71
 ---- batch: 110 ----
mean loss: 276.86
train mean loss: 275.45
epoch train time: 0:00:00.723008
elapsed time: 0:01:35.028761
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 01:57:51.415569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.56
 ---- batch: 020 ----
mean loss: 279.08
 ---- batch: 030 ----
mean loss: 270.65
 ---- batch: 040 ----
mean loss: 265.86
 ---- batch: 050 ----
mean loss: 277.99
 ---- batch: 060 ----
mean loss: 276.05
 ---- batch: 070 ----
mean loss: 270.38
 ---- batch: 080 ----
mean loss: 280.48
 ---- batch: 090 ----
mean loss: 273.22
 ---- batch: 100 ----
mean loss: 265.00
 ---- batch: 110 ----
mean loss: 273.19
train mean loss: 274.16
epoch train time: 0:00:00.728626
elapsed time: 0:01:35.757525
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 01:57:52.144333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.58
 ---- batch: 020 ----
mean loss: 271.22
 ---- batch: 030 ----
mean loss: 277.33
 ---- batch: 040 ----
mean loss: 274.92
 ---- batch: 050 ----
mean loss: 280.04
 ---- batch: 060 ----
mean loss: 264.77
 ---- batch: 070 ----
mean loss: 272.29
 ---- batch: 080 ----
mean loss: 275.89
 ---- batch: 090 ----
mean loss: 275.27
 ---- batch: 100 ----
mean loss: 278.68
 ---- batch: 110 ----
mean loss: 271.14
train mean loss: 272.89
epoch train time: 0:00:00.726714
elapsed time: 0:01:36.484383
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 01:57:52.871189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.19
 ---- batch: 020 ----
mean loss: 273.07
 ---- batch: 030 ----
mean loss: 277.93
 ---- batch: 040 ----
mean loss: 280.00
 ---- batch: 050 ----
mean loss: 266.63
 ---- batch: 060 ----
mean loss: 266.63
 ---- batch: 070 ----
mean loss: 272.53
 ---- batch: 080 ----
mean loss: 268.05
 ---- batch: 090 ----
mean loss: 269.17
 ---- batch: 100 ----
mean loss: 270.15
 ---- batch: 110 ----
mean loss: 267.21
train mean loss: 271.73
epoch train time: 0:00:00.716896
elapsed time: 0:01:37.201440
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 01:57:53.588244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.64
 ---- batch: 020 ----
mean loss: 266.74
 ---- batch: 030 ----
mean loss: 259.07
 ---- batch: 040 ----
mean loss: 273.83
 ---- batch: 050 ----
mean loss: 268.17
 ---- batch: 060 ----
mean loss: 274.59
 ---- batch: 070 ----
mean loss: 273.35
 ---- batch: 080 ----
mean loss: 278.32
 ---- batch: 090 ----
mean loss: 273.21
 ---- batch: 100 ----
mean loss: 272.11
 ---- batch: 110 ----
mean loss: 272.93
train mean loss: 270.53
epoch train time: 0:00:00.726842
elapsed time: 0:01:37.928436
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 01:57:54.315251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.97
 ---- batch: 020 ----
mean loss: 280.20
 ---- batch: 030 ----
mean loss: 276.10
 ---- batch: 040 ----
mean loss: 264.23
 ---- batch: 050 ----
mean loss: 271.55
 ---- batch: 060 ----
mean loss: 273.34
 ---- batch: 070 ----
mean loss: 266.61
 ---- batch: 080 ----
mean loss: 264.61
 ---- batch: 090 ----
mean loss: 268.62
 ---- batch: 100 ----
mean loss: 257.46
 ---- batch: 110 ----
mean loss: 271.48
train mean loss: 269.37
epoch train time: 0:00:00.723149
elapsed time: 0:01:38.651769
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 01:57:55.038601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.26
 ---- batch: 020 ----
mean loss: 271.13
 ---- batch: 030 ----
mean loss: 271.42
 ---- batch: 040 ----
mean loss: 273.10
 ---- batch: 050 ----
mean loss: 261.99
 ---- batch: 060 ----
mean loss: 262.79
 ---- batch: 070 ----
mean loss: 273.51
 ---- batch: 080 ----
mean loss: 263.89
 ---- batch: 090 ----
mean loss: 262.07
 ---- batch: 100 ----
mean loss: 262.25
 ---- batch: 110 ----
mean loss: 275.34
train mean loss: 268.33
epoch train time: 0:00:00.719023
elapsed time: 0:01:39.370971
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 01:57:55.757780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.50
 ---- batch: 020 ----
mean loss: 273.69
 ---- batch: 030 ----
mean loss: 270.60
 ---- batch: 040 ----
mean loss: 274.43
 ---- batch: 050 ----
mean loss: 258.83
 ---- batch: 060 ----
mean loss: 257.95
 ---- batch: 070 ----
mean loss: 269.45
 ---- batch: 080 ----
mean loss: 265.06
 ---- batch: 090 ----
mean loss: 273.41
 ---- batch: 100 ----
mean loss: 264.15
 ---- batch: 110 ----
mean loss: 263.77
train mean loss: 267.11
epoch train time: 0:00:00.721697
elapsed time: 0:01:40.092820
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 01:57:56.479656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.09
 ---- batch: 020 ----
mean loss: 277.77
 ---- batch: 030 ----
mean loss: 260.84
 ---- batch: 040 ----
mean loss: 263.73
 ---- batch: 050 ----
mean loss: 260.57
 ---- batch: 060 ----
mean loss: 267.28
 ---- batch: 070 ----
mean loss: 269.89
 ---- batch: 080 ----
mean loss: 272.33
 ---- batch: 090 ----
mean loss: 266.99
 ---- batch: 100 ----
mean loss: 263.50
 ---- batch: 110 ----
mean loss: 263.02
train mean loss: 266.19
epoch train time: 0:00:00.730282
elapsed time: 0:01:40.823289
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 01:57:57.210105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.83
 ---- batch: 020 ----
mean loss: 264.88
 ---- batch: 030 ----
mean loss: 266.79
 ---- batch: 040 ----
mean loss: 264.92
 ---- batch: 050 ----
mean loss: 267.18
 ---- batch: 060 ----
mean loss: 266.43
 ---- batch: 070 ----
mean loss: 267.59
 ---- batch: 080 ----
mean loss: 264.20
 ---- batch: 090 ----
mean loss: 268.97
 ---- batch: 100 ----
mean loss: 264.26
 ---- batch: 110 ----
mean loss: 257.40
train mean loss: 264.84
epoch train time: 0:00:00.735080
elapsed time: 0:01:41.558515
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 01:57:57.945322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.80
 ---- batch: 020 ----
mean loss: 271.41
 ---- batch: 030 ----
mean loss: 267.71
 ---- batch: 040 ----
mean loss: 265.30
 ---- batch: 050 ----
mean loss: 270.17
 ---- batch: 060 ----
mean loss: 257.04
 ---- batch: 070 ----
mean loss: 256.10
 ---- batch: 080 ----
mean loss: 259.82
 ---- batch: 090 ----
mean loss: 262.57
 ---- batch: 100 ----
mean loss: 261.82
 ---- batch: 110 ----
mean loss: 265.63
train mean loss: 263.77
epoch train time: 0:00:00.717906
elapsed time: 0:01:42.276555
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 01:57:58.663361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.41
 ---- batch: 020 ----
mean loss: 254.46
 ---- batch: 030 ----
mean loss: 239.59
 ---- batch: 040 ----
mean loss: 266.26
 ---- batch: 050 ----
mean loss: 278.80
 ---- batch: 060 ----
mean loss: 270.06
 ---- batch: 070 ----
mean loss: 260.09
 ---- batch: 080 ----
mean loss: 260.38
 ---- batch: 090 ----
mean loss: 261.71
 ---- batch: 100 ----
mean loss: 260.55
 ---- batch: 110 ----
mean loss: 274.72
train mean loss: 262.56
epoch train time: 0:00:00.723821
elapsed time: 0:01:43.000514
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 01:57:59.387323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.54
 ---- batch: 020 ----
mean loss: 252.47
 ---- batch: 030 ----
mean loss: 263.70
 ---- batch: 040 ----
mean loss: 259.07
 ---- batch: 050 ----
mean loss: 274.78
 ---- batch: 060 ----
mean loss: 259.92
 ---- batch: 070 ----
mean loss: 261.49
 ---- batch: 080 ----
mean loss: 266.05
 ---- batch: 090 ----
mean loss: 264.83
 ---- batch: 100 ----
mean loss: 264.34
 ---- batch: 110 ----
mean loss: 259.70
train mean loss: 261.62
epoch train time: 0:00:00.723814
elapsed time: 0:01:43.724465
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 01:58:00.111287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.04
 ---- batch: 020 ----
mean loss: 259.46
 ---- batch: 030 ----
mean loss: 261.34
 ---- batch: 040 ----
mean loss: 250.98
 ---- batch: 050 ----
mean loss: 258.29
 ---- batch: 060 ----
mean loss: 267.34
 ---- batch: 070 ----
mean loss: 258.79
 ---- batch: 080 ----
mean loss: 273.90
 ---- batch: 090 ----
mean loss: 262.68
 ---- batch: 100 ----
mean loss: 259.43
 ---- batch: 110 ----
mean loss: 258.14
train mean loss: 260.45
epoch train time: 0:00:00.727900
elapsed time: 0:01:44.452518
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 01:58:00.839323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.88
 ---- batch: 020 ----
mean loss: 267.51
 ---- batch: 030 ----
mean loss: 254.61
 ---- batch: 040 ----
mean loss: 264.41
 ---- batch: 050 ----
mean loss: 249.97
 ---- batch: 060 ----
mean loss: 262.75
 ---- batch: 070 ----
mean loss: 251.34
 ---- batch: 080 ----
mean loss: 248.59
 ---- batch: 090 ----
mean loss: 259.97
 ---- batch: 100 ----
mean loss: 262.99
 ---- batch: 110 ----
mean loss: 265.70
train mean loss: 259.29
epoch train time: 0:00:00.726392
elapsed time: 0:01:45.179047
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 01:58:01.565870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.90
 ---- batch: 020 ----
mean loss: 261.48
 ---- batch: 030 ----
mean loss: 266.82
 ---- batch: 040 ----
mean loss: 252.86
 ---- batch: 050 ----
mean loss: 262.33
 ---- batch: 060 ----
mean loss: 258.45
 ---- batch: 070 ----
mean loss: 259.70
 ---- batch: 080 ----
mean loss: 257.76
 ---- batch: 090 ----
mean loss: 251.22
 ---- batch: 100 ----
mean loss: 262.82
 ---- batch: 110 ----
mean loss: 257.10
train mean loss: 258.35
epoch train time: 0:00:00.716621
elapsed time: 0:01:45.895868
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 01:58:02.282672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.00
 ---- batch: 020 ----
mean loss: 258.33
 ---- batch: 030 ----
mean loss: 260.11
 ---- batch: 040 ----
mean loss: 255.57
 ---- batch: 050 ----
mean loss: 260.03
 ---- batch: 060 ----
mean loss: 251.99
 ---- batch: 070 ----
mean loss: 255.90
 ---- batch: 080 ----
mean loss: 261.64
 ---- batch: 090 ----
mean loss: 257.07
 ---- batch: 100 ----
mean loss: 254.03
 ---- batch: 110 ----
mean loss: 256.17
train mean loss: 257.11
epoch train time: 0:00:00.725967
elapsed time: 0:01:46.621967
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 01:58:03.008773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.05
 ---- batch: 020 ----
mean loss: 253.09
 ---- batch: 030 ----
mean loss: 266.29
 ---- batch: 040 ----
mean loss: 258.23
 ---- batch: 050 ----
mean loss: 254.31
 ---- batch: 060 ----
mean loss: 259.44
 ---- batch: 070 ----
mean loss: 251.53
 ---- batch: 080 ----
mean loss: 259.32
 ---- batch: 090 ----
mean loss: 253.88
 ---- batch: 100 ----
mean loss: 252.80
 ---- batch: 110 ----
mean loss: 248.97
train mean loss: 256.04
epoch train time: 0:00:00.724344
elapsed time: 0:01:47.346493
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 01:58:03.733342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.49
 ---- batch: 020 ----
mean loss: 250.78
 ---- batch: 030 ----
mean loss: 253.00
 ---- batch: 040 ----
mean loss: 252.30
 ---- batch: 050 ----
mean loss: 249.71
 ---- batch: 060 ----
mean loss: 256.37
 ---- batch: 070 ----
mean loss: 252.66
 ---- batch: 080 ----
mean loss: 252.80
 ---- batch: 090 ----
mean loss: 249.83
 ---- batch: 100 ----
mean loss: 256.45
 ---- batch: 110 ----
mean loss: 264.99
train mean loss: 254.95
epoch train time: 0:00:00.722210
elapsed time: 0:01:48.068884
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 01:58:04.455691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.31
 ---- batch: 020 ----
mean loss: 257.40
 ---- batch: 030 ----
mean loss: 250.40
 ---- batch: 040 ----
mean loss: 254.03
 ---- batch: 050 ----
mean loss: 254.62
 ---- batch: 060 ----
mean loss: 245.79
 ---- batch: 070 ----
mean loss: 262.53
 ---- batch: 080 ----
mean loss: 246.94
 ---- batch: 090 ----
mean loss: 252.58
 ---- batch: 100 ----
mean loss: 257.83
 ---- batch: 110 ----
mean loss: 260.81
train mean loss: 253.97
epoch train time: 0:00:00.721334
elapsed time: 0:01:48.790365
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 01:58:05.177196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.99
 ---- batch: 020 ----
mean loss: 250.67
 ---- batch: 030 ----
mean loss: 249.28
 ---- batch: 040 ----
mean loss: 245.16
 ---- batch: 050 ----
mean loss: 247.57
 ---- batch: 060 ----
mean loss: 261.66
 ---- batch: 070 ----
mean loss: 250.13
 ---- batch: 080 ----
mean loss: 249.12
 ---- batch: 090 ----
mean loss: 256.29
 ---- batch: 100 ----
mean loss: 265.91
 ---- batch: 110 ----
mean loss: 245.68
train mean loss: 252.87
epoch train time: 0:00:00.733214
elapsed time: 0:01:49.523753
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 01:58:05.910566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.87
 ---- batch: 020 ----
mean loss: 261.93
 ---- batch: 030 ----
mean loss: 243.35
 ---- batch: 040 ----
mean loss: 252.05
 ---- batch: 050 ----
mean loss: 256.65
 ---- batch: 060 ----
mean loss: 248.37
 ---- batch: 070 ----
mean loss: 246.46
 ---- batch: 080 ----
mean loss: 257.91
 ---- batch: 090 ----
mean loss: 256.22
 ---- batch: 100 ----
mean loss: 241.18
 ---- batch: 110 ----
mean loss: 255.66
train mean loss: 251.94
epoch train time: 0:00:00.734969
elapsed time: 0:01:50.258893
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 01:58:06.645720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.58
 ---- batch: 020 ----
mean loss: 250.88
 ---- batch: 030 ----
mean loss: 255.88
 ---- batch: 040 ----
mean loss: 248.68
 ---- batch: 050 ----
mean loss: 261.16
 ---- batch: 060 ----
mean loss: 245.43
 ---- batch: 070 ----
mean loss: 257.42
 ---- batch: 080 ----
mean loss: 252.09
 ---- batch: 090 ----
mean loss: 257.57
 ---- batch: 100 ----
mean loss: 235.67
 ---- batch: 110 ----
mean loss: 240.81
train mean loss: 250.87
epoch train time: 0:00:00.725000
elapsed time: 0:01:50.984052
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 01:58:07.370859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.84
 ---- batch: 020 ----
mean loss: 254.29
 ---- batch: 030 ----
mean loss: 240.36
 ---- batch: 040 ----
mean loss: 244.44
 ---- batch: 050 ----
mean loss: 252.34
 ---- batch: 060 ----
mean loss: 247.24
 ---- batch: 070 ----
mean loss: 257.16
 ---- batch: 080 ----
mean loss: 249.45
 ---- batch: 090 ----
mean loss: 254.57
 ---- batch: 100 ----
mean loss: 253.64
 ---- batch: 110 ----
mean loss: 250.73
train mean loss: 250.12
epoch train time: 0:00:00.724168
elapsed time: 0:01:51.708419
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 01:58:08.095244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.33
 ---- batch: 020 ----
mean loss: 238.72
 ---- batch: 030 ----
mean loss: 256.11
 ---- batch: 040 ----
mean loss: 242.28
 ---- batch: 050 ----
mean loss: 252.41
 ---- batch: 060 ----
mean loss: 240.45
 ---- batch: 070 ----
mean loss: 255.88
 ---- batch: 080 ----
mean loss: 248.64
 ---- batch: 090 ----
mean loss: 246.13
 ---- batch: 100 ----
mean loss: 253.88
 ---- batch: 110 ----
mean loss: 255.78
train mean loss: 249.10
epoch train time: 0:00:00.725572
elapsed time: 0:01:52.434142
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 01:58:08.820946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.69
 ---- batch: 020 ----
mean loss: 247.73
 ---- batch: 030 ----
mean loss: 246.64
 ---- batch: 040 ----
mean loss: 239.24
 ---- batch: 050 ----
mean loss: 258.12
 ---- batch: 060 ----
mean loss: 246.35
 ---- batch: 070 ----
mean loss: 246.61
 ---- batch: 080 ----
mean loss: 257.47
 ---- batch: 090 ----
mean loss: 241.49
 ---- batch: 100 ----
mean loss: 239.11
 ---- batch: 110 ----
mean loss: 258.82
train mean loss: 248.30
epoch train time: 0:00:00.733049
elapsed time: 0:01:53.167324
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 01:58:09.554147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.83
 ---- batch: 020 ----
mean loss: 247.12
 ---- batch: 030 ----
mean loss: 251.54
 ---- batch: 040 ----
mean loss: 248.16
 ---- batch: 050 ----
mean loss: 239.21
 ---- batch: 060 ----
mean loss: 252.63
 ---- batch: 070 ----
mean loss: 240.45
 ---- batch: 080 ----
mean loss: 253.81
 ---- batch: 090 ----
mean loss: 256.15
 ---- batch: 100 ----
mean loss: 235.81
 ---- batch: 110 ----
mean loss: 245.52
train mean loss: 247.37
epoch train time: 0:00:00.717049
elapsed time: 0:01:53.884525
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 01:58:10.271331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.91
 ---- batch: 020 ----
mean loss: 238.06
 ---- batch: 030 ----
mean loss: 249.98
 ---- batch: 040 ----
mean loss: 233.52
 ---- batch: 050 ----
mean loss: 245.72
 ---- batch: 060 ----
mean loss: 248.88
 ---- batch: 070 ----
mean loss: 249.75
 ---- batch: 080 ----
mean loss: 253.62
 ---- batch: 090 ----
mean loss: 248.86
 ---- batch: 100 ----
mean loss: 248.37
 ---- batch: 110 ----
mean loss: 249.71
train mean loss: 246.44
epoch train time: 0:00:00.727115
elapsed time: 0:01:54.611783
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 01:58:10.998589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.82
 ---- batch: 020 ----
mean loss: 255.55
 ---- batch: 030 ----
mean loss: 239.55
 ---- batch: 040 ----
mean loss: 245.67
 ---- batch: 050 ----
mean loss: 255.34
 ---- batch: 060 ----
mean loss: 262.67
 ---- batch: 070 ----
mean loss: 249.35
 ---- batch: 080 ----
mean loss: 234.45
 ---- batch: 090 ----
mean loss: 241.99
 ---- batch: 100 ----
mean loss: 242.95
 ---- batch: 110 ----
mean loss: 238.52
train mean loss: 246.09
epoch train time: 0:00:00.725413
elapsed time: 0:01:55.337375
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 01:58:11.724181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.89
 ---- batch: 020 ----
mean loss: 253.22
 ---- batch: 030 ----
mean loss: 248.03
 ---- batch: 040 ----
mean loss: 235.32
 ---- batch: 050 ----
mean loss: 244.61
 ---- batch: 060 ----
mean loss: 242.39
 ---- batch: 070 ----
mean loss: 238.40
 ---- batch: 080 ----
mean loss: 251.76
 ---- batch: 090 ----
mean loss: 248.48
 ---- batch: 100 ----
mean loss: 236.77
 ---- batch: 110 ----
mean loss: 238.94
train mean loss: 244.97
epoch train time: 0:00:00.723907
elapsed time: 0:01:56.061416
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 01:58:12.448219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.20
 ---- batch: 020 ----
mean loss: 243.08
 ---- batch: 030 ----
mean loss: 240.98
 ---- batch: 040 ----
mean loss: 241.85
 ---- batch: 050 ----
mean loss: 241.96
 ---- batch: 060 ----
mean loss: 246.81
 ---- batch: 070 ----
mean loss: 249.59
 ---- batch: 080 ----
mean loss: 249.29
 ---- batch: 090 ----
mean loss: 244.42
 ---- batch: 100 ----
mean loss: 250.17
 ---- batch: 110 ----
mean loss: 241.17
train mean loss: 244.26
epoch train time: 0:00:00.726187
elapsed time: 0:01:56.787741
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 01:58:13.174548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.22
 ---- batch: 020 ----
mean loss: 246.62
 ---- batch: 030 ----
mean loss: 246.54
 ---- batch: 040 ----
mean loss: 241.88
 ---- batch: 050 ----
mean loss: 237.10
 ---- batch: 060 ----
mean loss: 242.96
 ---- batch: 070 ----
mean loss: 242.75
 ---- batch: 080 ----
mean loss: 255.22
 ---- batch: 090 ----
mean loss: 238.84
 ---- batch: 100 ----
mean loss: 243.92
 ---- batch: 110 ----
mean loss: 235.73
train mean loss: 243.49
epoch train time: 0:00:00.726368
elapsed time: 0:01:57.514303
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 01:58:13.901129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.86
 ---- batch: 020 ----
mean loss: 237.76
 ---- batch: 030 ----
mean loss: 243.09
 ---- batch: 040 ----
mean loss: 240.11
 ---- batch: 050 ----
mean loss: 245.47
 ---- batch: 060 ----
mean loss: 242.29
 ---- batch: 070 ----
mean loss: 240.91
 ---- batch: 080 ----
mean loss: 251.05
 ---- batch: 090 ----
mean loss: 249.88
 ---- batch: 100 ----
mean loss: 243.39
 ---- batch: 110 ----
mean loss: 235.59
train mean loss: 242.71
epoch train time: 0:00:00.726421
elapsed time: 0:01:58.240932
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 01:58:14.627774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.09
 ---- batch: 020 ----
mean loss: 235.30
 ---- batch: 030 ----
mean loss: 241.66
 ---- batch: 040 ----
mean loss: 243.47
 ---- batch: 050 ----
mean loss: 242.36
 ---- batch: 060 ----
mean loss: 254.31
 ---- batch: 070 ----
mean loss: 238.71
 ---- batch: 080 ----
mean loss: 244.63
 ---- batch: 090 ----
mean loss: 249.78
 ---- batch: 100 ----
mean loss: 232.78
 ---- batch: 110 ----
mean loss: 244.32
train mean loss: 242.28
epoch train time: 0:00:00.722297
elapsed time: 0:01:58.963398
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 01:58:15.350203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.52
 ---- batch: 020 ----
mean loss: 226.92
 ---- batch: 030 ----
mean loss: 242.61
 ---- batch: 040 ----
mean loss: 246.51
 ---- batch: 050 ----
mean loss: 238.84
 ---- batch: 060 ----
mean loss: 240.29
 ---- batch: 070 ----
mean loss: 246.45
 ---- batch: 080 ----
mean loss: 232.02
 ---- batch: 090 ----
mean loss: 247.67
 ---- batch: 100 ----
mean loss: 239.60
 ---- batch: 110 ----
mean loss: 245.73
train mean loss: 241.46
epoch train time: 0:00:00.727090
elapsed time: 0:01:59.690619
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 01:58:16.077425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.96
 ---- batch: 020 ----
mean loss: 242.54
 ---- batch: 030 ----
mean loss: 237.83
 ---- batch: 040 ----
mean loss: 246.66
 ---- batch: 050 ----
mean loss: 234.56
 ---- batch: 060 ----
mean loss: 241.16
 ---- batch: 070 ----
mean loss: 239.94
 ---- batch: 080 ----
mean loss: 239.56
 ---- batch: 090 ----
mean loss: 243.32
 ---- batch: 100 ----
mean loss: 244.24
 ---- batch: 110 ----
mean loss: 238.69
train mean loss: 240.75
epoch train time: 0:00:00.723923
elapsed time: 0:02:00.414686
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 01:58:16.801494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.30
 ---- batch: 020 ----
mean loss: 244.60
 ---- batch: 030 ----
mean loss: 234.30
 ---- batch: 040 ----
mean loss: 241.54
 ---- batch: 050 ----
mean loss: 250.84
 ---- batch: 060 ----
mean loss: 240.63
 ---- batch: 070 ----
mean loss: 243.22
 ---- batch: 080 ----
mean loss: 244.53
 ---- batch: 090 ----
mean loss: 223.59
 ---- batch: 100 ----
mean loss: 246.72
 ---- batch: 110 ----
mean loss: 231.62
train mean loss: 240.11
epoch train time: 0:00:00.720058
elapsed time: 0:02:01.134897
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 01:58:17.521705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.87
 ---- batch: 020 ----
mean loss: 227.98
 ---- batch: 030 ----
mean loss: 239.10
 ---- batch: 040 ----
mean loss: 242.80
 ---- batch: 050 ----
mean loss: 246.14
 ---- batch: 060 ----
mean loss: 242.03
 ---- batch: 070 ----
mean loss: 237.15
 ---- batch: 080 ----
mean loss: 240.02
 ---- batch: 090 ----
mean loss: 248.69
 ---- batch: 100 ----
mean loss: 233.16
 ---- batch: 110 ----
mean loss: 234.84
train mean loss: 239.40
epoch train time: 0:00:00.726452
elapsed time: 0:02:01.861547
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 01:58:18.248353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.10
 ---- batch: 020 ----
mean loss: 238.43
 ---- batch: 030 ----
mean loss: 239.38
 ---- batch: 040 ----
mean loss: 248.36
 ---- batch: 050 ----
mean loss: 247.62
 ---- batch: 060 ----
mean loss: 239.73
 ---- batch: 070 ----
mean loss: 241.17
 ---- batch: 080 ----
mean loss: 235.04
 ---- batch: 090 ----
mean loss: 236.42
 ---- batch: 100 ----
mean loss: 228.30
 ---- batch: 110 ----
mean loss: 233.40
train mean loss: 238.84
epoch train time: 0:00:00.720709
elapsed time: 0:02:02.582393
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 01:58:18.969221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.95
 ---- batch: 020 ----
mean loss: 231.98
 ---- batch: 030 ----
mean loss: 244.14
 ---- batch: 040 ----
mean loss: 248.89
 ---- batch: 050 ----
mean loss: 232.33
 ---- batch: 060 ----
mean loss: 236.89
 ---- batch: 070 ----
mean loss: 244.04
 ---- batch: 080 ----
mean loss: 244.03
 ---- batch: 090 ----
mean loss: 230.57
 ---- batch: 100 ----
mean loss: 232.70
 ---- batch: 110 ----
mean loss: 238.33
train mean loss: 238.28
epoch train time: 0:00:00.723648
elapsed time: 0:02:03.306221
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 01:58:19.693029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.79
 ---- batch: 020 ----
mean loss: 232.99
 ---- batch: 030 ----
mean loss: 231.31
 ---- batch: 040 ----
mean loss: 238.98
 ---- batch: 050 ----
mean loss: 240.81
 ---- batch: 060 ----
mean loss: 234.05
 ---- batch: 070 ----
mean loss: 241.38
 ---- batch: 080 ----
mean loss: 243.92
 ---- batch: 090 ----
mean loss: 237.08
 ---- batch: 100 ----
mean loss: 240.50
 ---- batch: 110 ----
mean loss: 237.43
train mean loss: 237.89
epoch train time: 0:00:00.728619
elapsed time: 0:02:04.034993
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 01:58:20.421792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.56
 ---- batch: 020 ----
mean loss: 233.45
 ---- batch: 030 ----
mean loss: 236.87
 ---- batch: 040 ----
mean loss: 221.72
 ---- batch: 050 ----
mean loss: 249.24
 ---- batch: 060 ----
mean loss: 236.00
 ---- batch: 070 ----
mean loss: 235.79
 ---- batch: 080 ----
mean loss: 241.92
 ---- batch: 090 ----
mean loss: 236.87
 ---- batch: 100 ----
mean loss: 237.98
 ---- batch: 110 ----
mean loss: 237.38
train mean loss: 237.12
epoch train time: 0:00:00.727232
elapsed time: 0:02:04.762357
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 01:58:21.149191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.75
 ---- batch: 020 ----
mean loss: 232.20
 ---- batch: 030 ----
mean loss: 234.23
 ---- batch: 040 ----
mean loss: 238.79
 ---- batch: 050 ----
mean loss: 243.62
 ---- batch: 060 ----
mean loss: 236.57
 ---- batch: 070 ----
mean loss: 237.73
 ---- batch: 080 ----
mean loss: 237.55
 ---- batch: 090 ----
mean loss: 231.00
 ---- batch: 100 ----
mean loss: 245.47
 ---- batch: 110 ----
mean loss: 225.11
train mean loss: 236.53
epoch train time: 0:00:00.747819
elapsed time: 0:02:05.510360
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 01:58:21.897200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.59
 ---- batch: 020 ----
mean loss: 245.83
 ---- batch: 030 ----
mean loss: 232.89
 ---- batch: 040 ----
mean loss: 236.02
 ---- batch: 050 ----
mean loss: 238.55
 ---- batch: 060 ----
mean loss: 243.89
 ---- batch: 070 ----
mean loss: 223.84
 ---- batch: 080 ----
mean loss: 235.63
 ---- batch: 090 ----
mean loss: 235.80
 ---- batch: 100 ----
mean loss: 236.83
 ---- batch: 110 ----
mean loss: 234.11
train mean loss: 236.04
epoch train time: 0:00:00.725267
elapsed time: 0:02:06.235815
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 01:58:22.622636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.92
 ---- batch: 020 ----
mean loss: 233.45
 ---- batch: 030 ----
mean loss: 235.66
 ---- batch: 040 ----
mean loss: 246.06
 ---- batch: 050 ----
mean loss: 225.97
 ---- batch: 060 ----
mean loss: 232.42
 ---- batch: 070 ----
mean loss: 245.16
 ---- batch: 080 ----
mean loss: 240.41
 ---- batch: 090 ----
mean loss: 237.61
 ---- batch: 100 ----
mean loss: 232.39
 ---- batch: 110 ----
mean loss: 230.58
train mean loss: 235.46
epoch train time: 0:00:00.736309
elapsed time: 0:02:06.972274
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 01:58:23.359078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.36
 ---- batch: 020 ----
mean loss: 232.10
 ---- batch: 030 ----
mean loss: 243.26
 ---- batch: 040 ----
mean loss: 236.25
 ---- batch: 050 ----
mean loss: 236.19
 ---- batch: 060 ----
mean loss: 229.00
 ---- batch: 070 ----
mean loss: 238.34
 ---- batch: 080 ----
mean loss: 234.11
 ---- batch: 090 ----
mean loss: 228.77
 ---- batch: 100 ----
mean loss: 237.49
 ---- batch: 110 ----
mean loss: 232.82
train mean loss: 235.12
epoch train time: 0:00:00.732083
elapsed time: 0:02:07.704491
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 01:58:24.091316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.92
 ---- batch: 020 ----
mean loss: 244.13
 ---- batch: 030 ----
mean loss: 235.39
 ---- batch: 040 ----
mean loss: 235.52
 ---- batch: 050 ----
mean loss: 236.03
 ---- batch: 060 ----
mean loss: 235.34
 ---- batch: 070 ----
mean loss: 239.95
 ---- batch: 080 ----
mean loss: 231.19
 ---- batch: 090 ----
mean loss: 220.39
 ---- batch: 100 ----
mean loss: 234.01
 ---- batch: 110 ----
mean loss: 229.16
train mean loss: 234.33
epoch train time: 0:00:00.716570
elapsed time: 0:02:08.421217
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 01:58:24.808028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.23
 ---- batch: 020 ----
mean loss: 234.20
 ---- batch: 030 ----
mean loss: 235.35
 ---- batch: 040 ----
mean loss: 226.88
 ---- batch: 050 ----
mean loss: 230.94
 ---- batch: 060 ----
mean loss: 224.76
 ---- batch: 070 ----
mean loss: 233.53
 ---- batch: 080 ----
mean loss: 234.23
 ---- batch: 090 ----
mean loss: 243.93
 ---- batch: 100 ----
mean loss: 222.02
 ---- batch: 110 ----
mean loss: 239.79
train mean loss: 233.76
epoch train time: 0:00:00.722097
elapsed time: 0:02:09.143452
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 01:58:25.530256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.31
 ---- batch: 020 ----
mean loss: 236.03
 ---- batch: 030 ----
mean loss: 240.95
 ---- batch: 040 ----
mean loss: 234.76
 ---- batch: 050 ----
mean loss: 233.18
 ---- batch: 060 ----
mean loss: 237.96
 ---- batch: 070 ----
mean loss: 230.16
 ---- batch: 080 ----
mean loss: 227.67
 ---- batch: 090 ----
mean loss: 227.55
 ---- batch: 100 ----
mean loss: 231.40
 ---- batch: 110 ----
mean loss: 233.34
train mean loss: 233.38
epoch train time: 0:00:00.726486
elapsed time: 0:02:09.870105
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 01:58:26.256930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.86
 ---- batch: 020 ----
mean loss: 239.54
 ---- batch: 030 ----
mean loss: 240.88
 ---- batch: 040 ----
mean loss: 233.26
 ---- batch: 050 ----
mean loss: 229.11
 ---- batch: 060 ----
mean loss: 229.29
 ---- batch: 070 ----
mean loss: 229.91
 ---- batch: 080 ----
mean loss: 231.89
 ---- batch: 090 ----
mean loss: 237.14
 ---- batch: 100 ----
mean loss: 230.67
 ---- batch: 110 ----
mean loss: 233.79
train mean loss: 232.93
epoch train time: 0:00:00.729233
elapsed time: 0:02:10.599495
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 01:58:26.986302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.10
 ---- batch: 020 ----
mean loss: 241.31
 ---- batch: 030 ----
mean loss: 235.18
 ---- batch: 040 ----
mean loss: 237.69
 ---- batch: 050 ----
mean loss: 224.30
 ---- batch: 060 ----
mean loss: 227.86
 ---- batch: 070 ----
mean loss: 235.67
 ---- batch: 080 ----
mean loss: 227.67
 ---- batch: 090 ----
mean loss: 232.76
 ---- batch: 100 ----
mean loss: 222.33
 ---- batch: 110 ----
mean loss: 226.04
train mean loss: 232.42
epoch train time: 0:00:00.715050
elapsed time: 0:02:11.314694
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 01:58:27.701515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.78
 ---- batch: 020 ----
mean loss: 233.42
 ---- batch: 030 ----
mean loss: 233.40
 ---- batch: 040 ----
mean loss: 233.04
 ---- batch: 050 ----
mean loss: 235.36
 ---- batch: 060 ----
mean loss: 236.45
 ---- batch: 070 ----
mean loss: 224.97
 ---- batch: 080 ----
mean loss: 232.12
 ---- batch: 090 ----
mean loss: 224.53
 ---- batch: 100 ----
mean loss: 239.03
 ---- batch: 110 ----
mean loss: 235.96
train mean loss: 231.67
epoch train time: 0:00:00.725573
elapsed time: 0:02:12.040427
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 01:58:28.427259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.93
 ---- batch: 020 ----
mean loss: 234.89
 ---- batch: 030 ----
mean loss: 227.41
 ---- batch: 040 ----
mean loss: 233.11
 ---- batch: 050 ----
mean loss: 223.60
 ---- batch: 060 ----
mean loss: 239.17
 ---- batch: 070 ----
mean loss: 221.56
 ---- batch: 080 ----
mean loss: 230.35
 ---- batch: 090 ----
mean loss: 230.19
 ---- batch: 100 ----
mean loss: 237.93
 ---- batch: 110 ----
mean loss: 224.47
train mean loss: 231.18
epoch train time: 0:00:00.728738
elapsed time: 0:02:12.769332
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 01:58:29.156144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.06
 ---- batch: 020 ----
mean loss: 241.43
 ---- batch: 030 ----
mean loss: 231.80
 ---- batch: 040 ----
mean loss: 234.63
 ---- batch: 050 ----
mean loss: 232.86
 ---- batch: 060 ----
mean loss: 230.16
 ---- batch: 070 ----
mean loss: 226.62
 ---- batch: 080 ----
mean loss: 224.93
 ---- batch: 090 ----
mean loss: 233.74
 ---- batch: 100 ----
mean loss: 219.53
 ---- batch: 110 ----
mean loss: 233.55
train mean loss: 230.77
epoch train time: 0:00:00.727435
elapsed time: 0:02:13.496903
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 01:58:29.883705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.06
 ---- batch: 020 ----
mean loss: 235.85
 ---- batch: 030 ----
mean loss: 240.46
 ---- batch: 040 ----
mean loss: 236.26
 ---- batch: 050 ----
mean loss: 234.54
 ---- batch: 060 ----
mean loss: 222.50
 ---- batch: 070 ----
mean loss: 233.48
 ---- batch: 080 ----
mean loss: 232.77
 ---- batch: 090 ----
mean loss: 238.29
 ---- batch: 100 ----
mean loss: 224.28
 ---- batch: 110 ----
mean loss: 220.44
train mean loss: 230.39
epoch train time: 0:00:00.713132
elapsed time: 0:02:14.210211
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 01:58:30.597016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.77
 ---- batch: 020 ----
mean loss: 239.73
 ---- batch: 030 ----
mean loss: 230.70
 ---- batch: 040 ----
mean loss: 226.82
 ---- batch: 050 ----
mean loss: 220.54
 ---- batch: 060 ----
mean loss: 234.07
 ---- batch: 070 ----
mean loss: 231.11
 ---- batch: 080 ----
mean loss: 234.95
 ---- batch: 090 ----
mean loss: 231.66
 ---- batch: 100 ----
mean loss: 224.62
 ---- batch: 110 ----
mean loss: 223.31
train mean loss: 229.79
epoch train time: 0:00:00.725943
elapsed time: 0:02:14.936293
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 01:58:31.323099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.30
 ---- batch: 020 ----
mean loss: 240.91
 ---- batch: 030 ----
mean loss: 227.74
 ---- batch: 040 ----
mean loss: 228.81
 ---- batch: 050 ----
mean loss: 221.90
 ---- batch: 060 ----
mean loss: 228.47
 ---- batch: 070 ----
mean loss: 226.80
 ---- batch: 080 ----
mean loss: 227.24
 ---- batch: 090 ----
mean loss: 221.64
 ---- batch: 100 ----
mean loss: 232.95
 ---- batch: 110 ----
mean loss: 237.92
train mean loss: 229.36
epoch train time: 0:00:00.728533
elapsed time: 0:02:15.664958
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 01:58:32.051775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.68
 ---- batch: 020 ----
mean loss: 222.79
 ---- batch: 030 ----
mean loss: 221.36
 ---- batch: 040 ----
mean loss: 224.42
 ---- batch: 050 ----
mean loss: 236.70
 ---- batch: 060 ----
mean loss: 221.13
 ---- batch: 070 ----
mean loss: 232.08
 ---- batch: 080 ----
mean loss: 242.04
 ---- batch: 090 ----
mean loss: 230.91
 ---- batch: 100 ----
mean loss: 232.95
 ---- batch: 110 ----
mean loss: 219.94
train mean loss: 228.81
epoch train time: 0:00:00.717540
elapsed time: 0:02:16.382654
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 01:58:32.769461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.94
 ---- batch: 020 ----
mean loss: 227.05
 ---- batch: 030 ----
mean loss: 234.47
 ---- batch: 040 ----
mean loss: 236.00
 ---- batch: 050 ----
mean loss: 226.39
 ---- batch: 060 ----
mean loss: 231.49
 ---- batch: 070 ----
mean loss: 220.84
 ---- batch: 080 ----
mean loss: 236.79
 ---- batch: 090 ----
mean loss: 222.83
 ---- batch: 100 ----
mean loss: 225.86
 ---- batch: 110 ----
mean loss: 225.74
train mean loss: 228.44
epoch train time: 0:00:00.722824
elapsed time: 0:02:17.105612
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 01:58:33.492433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.77
 ---- batch: 020 ----
mean loss: 233.72
 ---- batch: 030 ----
mean loss: 236.24
 ---- batch: 040 ----
mean loss: 222.22
 ---- batch: 050 ----
mean loss: 237.23
 ---- batch: 060 ----
mean loss: 234.32
 ---- batch: 070 ----
mean loss: 224.42
 ---- batch: 080 ----
mean loss: 227.16
 ---- batch: 090 ----
mean loss: 224.95
 ---- batch: 100 ----
mean loss: 226.19
 ---- batch: 110 ----
mean loss: 225.09
train mean loss: 227.85
epoch train time: 0:00:00.724207
elapsed time: 0:02:17.830002
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 01:58:34.216809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.44
 ---- batch: 020 ----
mean loss: 232.15
 ---- batch: 030 ----
mean loss: 223.56
 ---- batch: 040 ----
mean loss: 232.89
 ---- batch: 050 ----
mean loss: 221.24
 ---- batch: 060 ----
mean loss: 222.41
 ---- batch: 070 ----
mean loss: 233.59
 ---- batch: 080 ----
mean loss: 223.73
 ---- batch: 090 ----
mean loss: 232.85
 ---- batch: 100 ----
mean loss: 221.44
 ---- batch: 110 ----
mean loss: 225.96
train mean loss: 227.58
epoch train time: 0:00:00.722454
elapsed time: 0:02:18.552599
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 01:58:34.939405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.77
 ---- batch: 020 ----
mean loss: 229.66
 ---- batch: 030 ----
mean loss: 226.12
 ---- batch: 040 ----
mean loss: 223.05
 ---- batch: 050 ----
mean loss: 237.70
 ---- batch: 060 ----
mean loss: 223.12
 ---- batch: 070 ----
mean loss: 227.23
 ---- batch: 080 ----
mean loss: 227.02
 ---- batch: 090 ----
mean loss: 229.39
 ---- batch: 100 ----
mean loss: 235.04
 ---- batch: 110 ----
mean loss: 215.12
train mean loss: 226.94
epoch train time: 0:00:00.710793
elapsed time: 0:02:19.263531
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 01:58:35.650344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.14
 ---- batch: 020 ----
mean loss: 231.45
 ---- batch: 030 ----
mean loss: 234.42
 ---- batch: 040 ----
mean loss: 220.49
 ---- batch: 050 ----
mean loss: 226.33
 ---- batch: 060 ----
mean loss: 212.87
 ---- batch: 070 ----
mean loss: 225.33
 ---- batch: 080 ----
mean loss: 237.80
 ---- batch: 090 ----
mean loss: 231.55
 ---- batch: 100 ----
mean loss: 231.98
 ---- batch: 110 ----
mean loss: 218.40
train mean loss: 226.50
epoch train time: 0:00:00.725388
elapsed time: 0:02:19.989093
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 01:58:36.375891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.69
 ---- batch: 020 ----
mean loss: 223.67
 ---- batch: 030 ----
mean loss: 218.49
 ---- batch: 040 ----
mean loss: 211.72
 ---- batch: 050 ----
mean loss: 223.81
 ---- batch: 060 ----
mean loss: 229.15
 ---- batch: 070 ----
mean loss: 233.04
 ---- batch: 080 ----
mean loss: 229.50
 ---- batch: 090 ----
mean loss: 228.08
 ---- batch: 100 ----
mean loss: 231.20
 ---- batch: 110 ----
mean loss: 221.39
train mean loss: 226.10
epoch train time: 0:00:00.718827
elapsed time: 0:02:20.708048
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 01:58:37.094852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.03
 ---- batch: 020 ----
mean loss: 225.70
 ---- batch: 030 ----
mean loss: 227.76
 ---- batch: 040 ----
mean loss: 220.85
 ---- batch: 050 ----
mean loss: 224.43
 ---- batch: 060 ----
mean loss: 230.40
 ---- batch: 070 ----
mean loss: 221.33
 ---- batch: 080 ----
mean loss: 233.86
 ---- batch: 090 ----
mean loss: 231.48
 ---- batch: 100 ----
mean loss: 219.25
 ---- batch: 110 ----
mean loss: 221.01
train mean loss: 225.74
epoch train time: 0:00:00.722606
elapsed time: 0:02:21.430818
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 01:58:37.817643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.90
 ---- batch: 020 ----
mean loss: 222.61
 ---- batch: 030 ----
mean loss: 227.75
 ---- batch: 040 ----
mean loss: 227.33
 ---- batch: 050 ----
mean loss: 223.19
 ---- batch: 060 ----
mean loss: 228.66
 ---- batch: 070 ----
mean loss: 224.01
 ---- batch: 080 ----
mean loss: 220.69
 ---- batch: 090 ----
mean loss: 226.86
 ---- batch: 100 ----
mean loss: 233.40
 ---- batch: 110 ----
mean loss: 217.54
train mean loss: 225.46
epoch train time: 0:00:00.717955
elapsed time: 0:02:22.148928
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 01:58:38.535757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.14
 ---- batch: 020 ----
mean loss: 221.95
 ---- batch: 030 ----
mean loss: 223.72
 ---- batch: 040 ----
mean loss: 216.95
 ---- batch: 050 ----
mean loss: 226.97
 ---- batch: 060 ----
mean loss: 220.31
 ---- batch: 070 ----
mean loss: 220.13
 ---- batch: 080 ----
mean loss: 233.19
 ---- batch: 090 ----
mean loss: 230.69
 ---- batch: 100 ----
mean loss: 225.57
 ---- batch: 110 ----
mean loss: 218.02
train mean loss: 224.83
epoch train time: 0:00:00.723849
elapsed time: 0:02:22.872945
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 01:58:39.259780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.85
 ---- batch: 020 ----
mean loss: 214.15
 ---- batch: 030 ----
mean loss: 224.87
 ---- batch: 040 ----
mean loss: 224.87
 ---- batch: 050 ----
mean loss: 232.74
 ---- batch: 060 ----
mean loss: 227.31
 ---- batch: 070 ----
mean loss: 228.67
 ---- batch: 080 ----
mean loss: 222.13
 ---- batch: 090 ----
mean loss: 221.75
 ---- batch: 100 ----
mean loss: 220.63
 ---- batch: 110 ----
mean loss: 226.62
train mean loss: 224.37
epoch train time: 0:00:00.718247
elapsed time: 0:02:23.591373
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 01:58:39.978178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.00
 ---- batch: 020 ----
mean loss: 221.47
 ---- batch: 030 ----
mean loss: 224.98
 ---- batch: 040 ----
mean loss: 220.73
 ---- batch: 050 ----
mean loss: 223.50
 ---- batch: 060 ----
mean loss: 219.20
 ---- batch: 070 ----
mean loss: 224.89
 ---- batch: 080 ----
mean loss: 226.29
 ---- batch: 090 ----
mean loss: 230.22
 ---- batch: 100 ----
mean loss: 218.71
 ---- batch: 110 ----
mean loss: 229.56
train mean loss: 223.94
epoch train time: 0:00:00.719568
elapsed time: 0:02:24.311075
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 01:58:40.697881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.47
 ---- batch: 020 ----
mean loss: 218.94
 ---- batch: 030 ----
mean loss: 220.08
 ---- batch: 040 ----
mean loss: 226.07
 ---- batch: 050 ----
mean loss: 222.18
 ---- batch: 060 ----
mean loss: 218.21
 ---- batch: 070 ----
mean loss: 229.31
 ---- batch: 080 ----
mean loss: 221.44
 ---- batch: 090 ----
mean loss: 226.57
 ---- batch: 100 ----
mean loss: 222.81
 ---- batch: 110 ----
mean loss: 228.91
train mean loss: 223.71
epoch train time: 0:00:00.726880
elapsed time: 0:02:25.038110
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 01:58:41.424917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.63
 ---- batch: 020 ----
mean loss: 217.67
 ---- batch: 030 ----
mean loss: 227.79
 ---- batch: 040 ----
mean loss: 232.42
 ---- batch: 050 ----
mean loss: 222.18
 ---- batch: 060 ----
mean loss: 222.48
 ---- batch: 070 ----
mean loss: 223.10
 ---- batch: 080 ----
mean loss: 218.57
 ---- batch: 090 ----
mean loss: 215.80
 ---- batch: 100 ----
mean loss: 228.25
 ---- batch: 110 ----
mean loss: 220.75
train mean loss: 223.49
epoch train time: 0:00:00.723900
elapsed time: 0:02:25.762142
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 01:58:42.148947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.87
 ---- batch: 020 ----
mean loss: 223.56
 ---- batch: 030 ----
mean loss: 225.53
 ---- batch: 040 ----
mean loss: 220.00
 ---- batch: 050 ----
mean loss: 214.05
 ---- batch: 060 ----
mean loss: 232.05
 ---- batch: 070 ----
mean loss: 215.00
 ---- batch: 080 ----
mean loss: 223.15
 ---- batch: 090 ----
mean loss: 224.22
 ---- batch: 100 ----
mean loss: 226.58
 ---- batch: 110 ----
mean loss: 227.95
train mean loss: 222.93
epoch train time: 0:00:00.713598
elapsed time: 0:02:26.475874
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 01:58:42.862697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.89
 ---- batch: 020 ----
mean loss: 217.87
 ---- batch: 030 ----
mean loss: 227.00
 ---- batch: 040 ----
mean loss: 215.71
 ---- batch: 050 ----
mean loss: 221.48
 ---- batch: 060 ----
mean loss: 220.49
 ---- batch: 070 ----
mean loss: 222.72
 ---- batch: 080 ----
mean loss: 217.82
 ---- batch: 090 ----
mean loss: 214.78
 ---- batch: 100 ----
mean loss: 226.78
 ---- batch: 110 ----
mean loss: 233.07
train mean loss: 222.40
epoch train time: 0:00:00.714728
elapsed time: 0:02:27.190785
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 01:58:43.577590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.70
 ---- batch: 020 ----
mean loss: 221.98
 ---- batch: 030 ----
mean loss: 216.76
 ---- batch: 040 ----
mean loss: 223.47
 ---- batch: 050 ----
mean loss: 223.65
 ---- batch: 060 ----
mean loss: 215.50
 ---- batch: 070 ----
mean loss: 226.50
 ---- batch: 080 ----
mean loss: 224.08
 ---- batch: 090 ----
mean loss: 214.95
 ---- batch: 100 ----
mean loss: 236.12
 ---- batch: 110 ----
mean loss: 219.06
train mean loss: 222.34
epoch train time: 0:00:00.719108
elapsed time: 0:02:27.910031
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 01:58:44.296840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.01
 ---- batch: 020 ----
mean loss: 217.02
 ---- batch: 030 ----
mean loss: 218.45
 ---- batch: 040 ----
mean loss: 221.95
 ---- batch: 050 ----
mean loss: 240.02
 ---- batch: 060 ----
mean loss: 216.67
 ---- batch: 070 ----
mean loss: 218.23
 ---- batch: 080 ----
mean loss: 231.44
 ---- batch: 090 ----
mean loss: 226.44
 ---- batch: 100 ----
mean loss: 209.64
 ---- batch: 110 ----
mean loss: 214.32
train mean loss: 221.66
epoch train time: 0:00:00.716967
elapsed time: 0:02:28.627142
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 01:58:45.013966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.54
 ---- batch: 020 ----
mean loss: 223.17
 ---- batch: 030 ----
mean loss: 226.17
 ---- batch: 040 ----
mean loss: 223.15
 ---- batch: 050 ----
mean loss: 231.82
 ---- batch: 060 ----
mean loss: 218.62
 ---- batch: 070 ----
mean loss: 221.07
 ---- batch: 080 ----
mean loss: 218.49
 ---- batch: 090 ----
mean loss: 221.26
 ---- batch: 100 ----
mean loss: 229.68
 ---- batch: 110 ----
mean loss: 218.72
train mean loss: 221.42
epoch train time: 0:00:00.719173
elapsed time: 0:02:29.346464
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 01:58:45.733282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.72
 ---- batch: 020 ----
mean loss: 227.81
 ---- batch: 030 ----
mean loss: 225.49
 ---- batch: 040 ----
mean loss: 213.75
 ---- batch: 050 ----
mean loss: 229.70
 ---- batch: 060 ----
mean loss: 223.43
 ---- batch: 070 ----
mean loss: 210.89
 ---- batch: 080 ----
mean loss: 202.43
 ---- batch: 090 ----
mean loss: 219.81
 ---- batch: 100 ----
mean loss: 223.89
 ---- batch: 110 ----
mean loss: 228.52
train mean loss: 221.14
epoch train time: 0:00:00.727264
elapsed time: 0:02:30.073876
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 01:58:46.460684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.54
 ---- batch: 020 ----
mean loss: 226.54
 ---- batch: 030 ----
mean loss: 229.73
 ---- batch: 040 ----
mean loss: 229.70
 ---- batch: 050 ----
mean loss: 221.98
 ---- batch: 060 ----
mean loss: 219.00
 ---- batch: 070 ----
mean loss: 221.38
 ---- batch: 080 ----
mean loss: 218.31
 ---- batch: 090 ----
mean loss: 210.02
 ---- batch: 100 ----
mean loss: 207.13
 ---- batch: 110 ----
mean loss: 220.13
train mean loss: 220.71
epoch train time: 0:00:00.719777
elapsed time: 0:02:30.793793
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 01:58:47.180600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.49
 ---- batch: 020 ----
mean loss: 225.38
 ---- batch: 030 ----
mean loss: 217.13
 ---- batch: 040 ----
mean loss: 219.62
 ---- batch: 050 ----
mean loss: 225.71
 ---- batch: 060 ----
mean loss: 216.12
 ---- batch: 070 ----
mean loss: 232.94
 ---- batch: 080 ----
mean loss: 216.13
 ---- batch: 090 ----
mean loss: 210.27
 ---- batch: 100 ----
mean loss: 218.29
 ---- batch: 110 ----
mean loss: 220.49
train mean loss: 220.35
epoch train time: 0:00:00.720715
elapsed time: 0:02:31.514643
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 01:58:47.901467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.80
 ---- batch: 020 ----
mean loss: 216.70
 ---- batch: 030 ----
mean loss: 224.75
 ---- batch: 040 ----
mean loss: 220.59
 ---- batch: 050 ----
mean loss: 234.13
 ---- batch: 060 ----
mean loss: 218.83
 ---- batch: 070 ----
mean loss: 215.16
 ---- batch: 080 ----
mean loss: 220.80
 ---- batch: 090 ----
mean loss: 221.15
 ---- batch: 100 ----
mean loss: 213.08
 ---- batch: 110 ----
mean loss: 210.77
train mean loss: 220.05
epoch train time: 0:00:00.717646
elapsed time: 0:02:32.232471
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 01:58:48.619276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.20
 ---- batch: 020 ----
mean loss: 218.34
 ---- batch: 030 ----
mean loss: 219.08
 ---- batch: 040 ----
mean loss: 211.88
 ---- batch: 050 ----
mean loss: 215.03
 ---- batch: 060 ----
mean loss: 221.93
 ---- batch: 070 ----
mean loss: 204.80
 ---- batch: 080 ----
mean loss: 232.06
 ---- batch: 090 ----
mean loss: 226.26
 ---- batch: 100 ----
mean loss: 234.06
 ---- batch: 110 ----
mean loss: 215.29
train mean loss: 219.77
epoch train time: 0:00:00.729155
elapsed time: 0:02:32.961764
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 01:58:49.348577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.85
 ---- batch: 020 ----
mean loss: 217.48
 ---- batch: 030 ----
mean loss: 215.22
 ---- batch: 040 ----
mean loss: 211.65
 ---- batch: 050 ----
mean loss: 221.24
 ---- batch: 060 ----
mean loss: 217.74
 ---- batch: 070 ----
mean loss: 219.26
 ---- batch: 080 ----
mean loss: 211.65
 ---- batch: 090 ----
mean loss: 229.05
 ---- batch: 100 ----
mean loss: 223.66
 ---- batch: 110 ----
mean loss: 225.78
train mean loss: 219.24
epoch train time: 0:00:00.725912
elapsed time: 0:02:33.687819
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 01:58:50.074624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.10
 ---- batch: 020 ----
mean loss: 226.69
 ---- batch: 030 ----
mean loss: 219.20
 ---- batch: 040 ----
mean loss: 230.65
 ---- batch: 050 ----
mean loss: 228.52
 ---- batch: 060 ----
mean loss: 214.11
 ---- batch: 070 ----
mean loss: 208.91
 ---- batch: 080 ----
mean loss: 218.64
 ---- batch: 090 ----
mean loss: 209.90
 ---- batch: 100 ----
mean loss: 212.54
 ---- batch: 110 ----
mean loss: 214.17
train mean loss: 219.06
epoch train time: 0:00:00.728169
elapsed time: 0:02:34.416126
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 01:58:50.802932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.81
 ---- batch: 020 ----
mean loss: 220.97
 ---- batch: 030 ----
mean loss: 208.25
 ---- batch: 040 ----
mean loss: 227.27
 ---- batch: 050 ----
mean loss: 223.87
 ---- batch: 060 ----
mean loss: 221.91
 ---- batch: 070 ----
mean loss: 217.61
 ---- batch: 080 ----
mean loss: 223.87
 ---- batch: 090 ----
mean loss: 212.54
 ---- batch: 100 ----
mean loss: 221.55
 ---- batch: 110 ----
mean loss: 213.60
train mean loss: 218.77
epoch train time: 0:00:00.721746
elapsed time: 0:02:35.138008
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 01:58:51.524815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.75
 ---- batch: 020 ----
mean loss: 222.52
 ---- batch: 030 ----
mean loss: 225.51
 ---- batch: 040 ----
mean loss: 212.99
 ---- batch: 050 ----
mean loss: 211.71
 ---- batch: 060 ----
mean loss: 223.70
 ---- batch: 070 ----
mean loss: 216.13
 ---- batch: 080 ----
mean loss: 220.79
 ---- batch: 090 ----
mean loss: 222.07
 ---- batch: 100 ----
mean loss: 211.14
 ---- batch: 110 ----
mean loss: 213.25
train mean loss: 218.35
epoch train time: 0:00:00.716385
elapsed time: 0:02:35.854527
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 01:58:52.241350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.68
 ---- batch: 020 ----
mean loss: 228.74
 ---- batch: 030 ----
mean loss: 223.60
 ---- batch: 040 ----
mean loss: 216.78
 ---- batch: 050 ----
mean loss: 218.69
 ---- batch: 060 ----
mean loss: 209.13
 ---- batch: 070 ----
mean loss: 226.76
 ---- batch: 080 ----
mean loss: 219.52
 ---- batch: 090 ----
mean loss: 218.09
 ---- batch: 100 ----
mean loss: 215.35
 ---- batch: 110 ----
mean loss: 215.94
train mean loss: 217.99
epoch train time: 0:00:00.715122
elapsed time: 0:02:36.569818
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 01:58:52.956625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.72
 ---- batch: 020 ----
mean loss: 224.86
 ---- batch: 030 ----
mean loss: 216.74
 ---- batch: 040 ----
mean loss: 212.35
 ---- batch: 050 ----
mean loss: 216.39
 ---- batch: 060 ----
mean loss: 216.18
 ---- batch: 070 ----
mean loss: 210.88
 ---- batch: 080 ----
mean loss: 220.36
 ---- batch: 090 ----
mean loss: 224.11
 ---- batch: 100 ----
mean loss: 211.42
 ---- batch: 110 ----
mean loss: 221.89
train mean loss: 217.58
epoch train time: 0:00:00.726996
elapsed time: 0:02:37.296950
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 01:58:53.683757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.55
 ---- batch: 020 ----
mean loss: 216.30
 ---- batch: 030 ----
mean loss: 224.74
 ---- batch: 040 ----
mean loss: 218.82
 ---- batch: 050 ----
mean loss: 212.50
 ---- batch: 060 ----
mean loss: 217.12
 ---- batch: 070 ----
mean loss: 217.06
 ---- batch: 080 ----
mean loss: 214.98
 ---- batch: 090 ----
mean loss: 225.49
 ---- batch: 100 ----
mean loss: 206.22
 ---- batch: 110 ----
mean loss: 220.44
train mean loss: 217.38
epoch train time: 0:00:00.725078
elapsed time: 0:02:38.022176
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 01:58:54.408973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.45
 ---- batch: 020 ----
mean loss: 222.27
 ---- batch: 030 ----
mean loss: 220.29
 ---- batch: 040 ----
mean loss: 215.91
 ---- batch: 050 ----
mean loss: 221.10
 ---- batch: 060 ----
mean loss: 214.38
 ---- batch: 070 ----
mean loss: 216.38
 ---- batch: 080 ----
mean loss: 210.06
 ---- batch: 090 ----
mean loss: 212.59
 ---- batch: 100 ----
mean loss: 219.27
 ---- batch: 110 ----
mean loss: 218.43
train mean loss: 216.92
epoch train time: 0:00:00.727267
elapsed time: 0:02:38.749571
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 01:58:55.136378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.17
 ---- batch: 020 ----
mean loss: 217.58
 ---- batch: 030 ----
mean loss: 212.79
 ---- batch: 040 ----
mean loss: 216.33
 ---- batch: 050 ----
mean loss: 211.50
 ---- batch: 060 ----
mean loss: 219.80
 ---- batch: 070 ----
mean loss: 220.32
 ---- batch: 080 ----
mean loss: 211.00
 ---- batch: 090 ----
mean loss: 223.78
 ---- batch: 100 ----
mean loss: 216.20
 ---- batch: 110 ----
mean loss: 208.72
train mean loss: 216.72
epoch train time: 0:00:00.734106
elapsed time: 0:02:39.483823
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 01:58:55.870635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.70
 ---- batch: 020 ----
mean loss: 215.61
 ---- batch: 030 ----
mean loss: 204.70
 ---- batch: 040 ----
mean loss: 230.87
 ---- batch: 050 ----
mean loss: 222.15
 ---- batch: 060 ----
mean loss: 217.05
 ---- batch: 070 ----
mean loss: 219.71
 ---- batch: 080 ----
mean loss: 216.99
 ---- batch: 090 ----
mean loss: 211.02
 ---- batch: 100 ----
mean loss: 215.24
 ---- batch: 110 ----
mean loss: 209.23
train mean loss: 216.37
epoch train time: 0:00:00.731347
elapsed time: 0:02:40.215319
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 01:58:56.602128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.47
 ---- batch: 020 ----
mean loss: 231.71
 ---- batch: 030 ----
mean loss: 218.03
 ---- batch: 040 ----
mean loss: 208.62
 ---- batch: 050 ----
mean loss: 214.74
 ---- batch: 060 ----
mean loss: 215.99
 ---- batch: 070 ----
mean loss: 206.88
 ---- batch: 080 ----
mean loss: 217.56
 ---- batch: 090 ----
mean loss: 219.25
 ---- batch: 100 ----
mean loss: 213.94
 ---- batch: 110 ----
mean loss: 207.02
train mean loss: 216.08
epoch train time: 0:00:00.727334
elapsed time: 0:02:40.942789
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 01:58:57.329635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.39
 ---- batch: 020 ----
mean loss: 220.98
 ---- batch: 030 ----
mean loss: 215.69
 ---- batch: 040 ----
mean loss: 211.14
 ---- batch: 050 ----
mean loss: 211.91
 ---- batch: 060 ----
mean loss: 220.96
 ---- batch: 070 ----
mean loss: 213.43
 ---- batch: 080 ----
mean loss: 212.86
 ---- batch: 090 ----
mean loss: 216.61
 ---- batch: 100 ----
mean loss: 210.30
 ---- batch: 110 ----
mean loss: 223.71
train mean loss: 215.84
epoch train time: 0:00:00.716968
elapsed time: 0:02:41.659951
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 01:58:58.046774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.49
 ---- batch: 020 ----
mean loss: 207.09
 ---- batch: 030 ----
mean loss: 218.67
 ---- batch: 040 ----
mean loss: 222.93
 ---- batch: 050 ----
mean loss: 222.37
 ---- batch: 060 ----
mean loss: 209.56
 ---- batch: 070 ----
mean loss: 219.57
 ---- batch: 080 ----
mean loss: 214.80
 ---- batch: 090 ----
mean loss: 215.29
 ---- batch: 100 ----
mean loss: 211.59
 ---- batch: 110 ----
mean loss: 222.63
train mean loss: 215.63
epoch train time: 0:00:00.728306
elapsed time: 0:02:42.388432
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 01:58:58.775270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.33
 ---- batch: 020 ----
mean loss: 223.60
 ---- batch: 030 ----
mean loss: 213.18
 ---- batch: 040 ----
mean loss: 207.03
 ---- batch: 050 ----
mean loss: 222.69
 ---- batch: 060 ----
mean loss: 218.25
 ---- batch: 070 ----
mean loss: 216.76
 ---- batch: 080 ----
mean loss: 218.69
 ---- batch: 090 ----
mean loss: 209.64
 ---- batch: 100 ----
mean loss: 215.42
 ---- batch: 110 ----
mean loss: 214.52
train mean loss: 215.18
epoch train time: 0:00:00.724524
elapsed time: 0:02:43.113125
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 01:58:59.499951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.28
 ---- batch: 020 ----
mean loss: 214.47
 ---- batch: 030 ----
mean loss: 224.86
 ---- batch: 040 ----
mean loss: 206.26
 ---- batch: 050 ----
mean loss: 210.31
 ---- batch: 060 ----
mean loss: 206.65
 ---- batch: 070 ----
mean loss: 212.45
 ---- batch: 080 ----
mean loss: 216.66
 ---- batch: 090 ----
mean loss: 218.06
 ---- batch: 100 ----
mean loss: 218.99
 ---- batch: 110 ----
mean loss: 216.03
train mean loss: 214.96
epoch train time: 0:00:00.718703
elapsed time: 0:02:43.831983
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 01:59:00.218786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.94
 ---- batch: 020 ----
mean loss: 220.04
 ---- batch: 030 ----
mean loss: 212.56
 ---- batch: 040 ----
mean loss: 219.38
 ---- batch: 050 ----
mean loss: 211.03
 ---- batch: 060 ----
mean loss: 220.16
 ---- batch: 070 ----
mean loss: 216.41
 ---- batch: 080 ----
mean loss: 210.28
 ---- batch: 090 ----
mean loss: 217.17
 ---- batch: 100 ----
mean loss: 210.26
 ---- batch: 110 ----
mean loss: 205.48
train mean loss: 214.72
epoch train time: 0:00:00.716139
elapsed time: 0:02:44.548251
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 01:59:00.935055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.36
 ---- batch: 020 ----
mean loss: 218.49
 ---- batch: 030 ----
mean loss: 206.03
 ---- batch: 040 ----
mean loss: 219.74
 ---- batch: 050 ----
mean loss: 204.76
 ---- batch: 060 ----
mean loss: 209.89
 ---- batch: 070 ----
mean loss: 211.79
 ---- batch: 080 ----
mean loss: 214.18
 ---- batch: 090 ----
mean loss: 225.94
 ---- batch: 100 ----
mean loss: 219.26
 ---- batch: 110 ----
mean loss: 211.08
train mean loss: 214.26
epoch train time: 0:00:00.713463
elapsed time: 0:02:45.261876
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 01:59:01.648682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.95
 ---- batch: 020 ----
mean loss: 217.21
 ---- batch: 030 ----
mean loss: 218.61
 ---- batch: 040 ----
mean loss: 210.56
 ---- batch: 050 ----
mean loss: 217.68
 ---- batch: 060 ----
mean loss: 221.83
 ---- batch: 070 ----
mean loss: 213.56
 ---- batch: 080 ----
mean loss: 213.91
 ---- batch: 090 ----
mean loss: 205.87
 ---- batch: 100 ----
mean loss: 212.01
 ---- batch: 110 ----
mean loss: 218.73
train mean loss: 213.96
epoch train time: 0:00:00.722135
elapsed time: 0:02:45.984167
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 01:59:02.370990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.35
 ---- batch: 020 ----
mean loss: 201.39
 ---- batch: 030 ----
mean loss: 220.02
 ---- batch: 040 ----
mean loss: 213.94
 ---- batch: 050 ----
mean loss: 218.05
 ---- batch: 060 ----
mean loss: 213.47
 ---- batch: 070 ----
mean loss: 216.08
 ---- batch: 080 ----
mean loss: 209.58
 ---- batch: 090 ----
mean loss: 212.97
 ---- batch: 100 ----
mean loss: 219.92
 ---- batch: 110 ----
mean loss: 219.44
train mean loss: 213.85
epoch train time: 0:00:00.712561
elapsed time: 0:02:46.696880
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 01:59:03.083684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.81
 ---- batch: 020 ----
mean loss: 221.55
 ---- batch: 030 ----
mean loss: 217.31
 ---- batch: 040 ----
mean loss: 210.67
 ---- batch: 050 ----
mean loss: 214.52
 ---- batch: 060 ----
mean loss: 212.09
 ---- batch: 070 ----
mean loss: 214.50
 ---- batch: 080 ----
mean loss: 211.31
 ---- batch: 090 ----
mean loss: 206.08
 ---- batch: 100 ----
mean loss: 206.67
 ---- batch: 110 ----
mean loss: 214.75
train mean loss: 213.56
epoch train time: 0:00:00.714569
elapsed time: 0:02:47.411583
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 01:59:03.798389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.21
 ---- batch: 020 ----
mean loss: 207.58
 ---- batch: 030 ----
mean loss: 211.77
 ---- batch: 040 ----
mean loss: 216.66
 ---- batch: 050 ----
mean loss: 217.92
 ---- batch: 060 ----
mean loss: 224.53
 ---- batch: 070 ----
mean loss: 209.62
 ---- batch: 080 ----
mean loss: 209.41
 ---- batch: 090 ----
mean loss: 221.95
 ---- batch: 100 ----
mean loss: 211.60
 ---- batch: 110 ----
mean loss: 212.09
train mean loss: 213.42
epoch train time: 0:00:00.718526
elapsed time: 0:02:48.130246
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 01:59:04.517057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.16
 ---- batch: 020 ----
mean loss: 215.51
 ---- batch: 030 ----
mean loss: 225.21
 ---- batch: 040 ----
mean loss: 204.57
 ---- batch: 050 ----
mean loss: 214.74
 ---- batch: 060 ----
mean loss: 208.97
 ---- batch: 070 ----
mean loss: 217.31
 ---- batch: 080 ----
mean loss: 206.17
 ---- batch: 090 ----
mean loss: 213.16
 ---- batch: 100 ----
mean loss: 213.83
 ---- batch: 110 ----
mean loss: 209.07
train mean loss: 212.97
epoch train time: 0:00:00.723358
elapsed time: 0:02:48.853776
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 01:59:05.240613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.51
 ---- batch: 020 ----
mean loss: 212.72
 ---- batch: 030 ----
mean loss: 223.45
 ---- batch: 040 ----
mean loss: 216.74
 ---- batch: 050 ----
mean loss: 213.72
 ---- batch: 060 ----
mean loss: 211.71
 ---- batch: 070 ----
mean loss: 210.32
 ---- batch: 080 ----
mean loss: 220.60
 ---- batch: 090 ----
mean loss: 211.40
 ---- batch: 100 ----
mean loss: 203.27
 ---- batch: 110 ----
mean loss: 205.85
train mean loss: 212.93
epoch train time: 0:00:00.721669
elapsed time: 0:02:49.575612
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 01:59:05.962424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.32
 ---- batch: 020 ----
mean loss: 207.67
 ---- batch: 030 ----
mean loss: 221.68
 ---- batch: 040 ----
mean loss: 208.96
 ---- batch: 050 ----
mean loss: 204.10
 ---- batch: 060 ----
mean loss: 217.65
 ---- batch: 070 ----
mean loss: 225.09
 ---- batch: 080 ----
mean loss: 212.86
 ---- batch: 090 ----
mean loss: 208.35
 ---- batch: 100 ----
mean loss: 211.37
 ---- batch: 110 ----
mean loss: 221.42
train mean loss: 212.69
epoch train time: 0:00:00.721172
elapsed time: 0:02:50.296927
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 01:59:06.683769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.19
 ---- batch: 020 ----
mean loss: 204.05
 ---- batch: 030 ----
mean loss: 210.46
 ---- batch: 040 ----
mean loss: 216.28
 ---- batch: 050 ----
mean loss: 223.38
 ---- batch: 060 ----
mean loss: 215.56
 ---- batch: 070 ----
mean loss: 210.12
 ---- batch: 080 ----
mean loss: 205.79
 ---- batch: 090 ----
mean loss: 217.73
 ---- batch: 100 ----
mean loss: 212.10
 ---- batch: 110 ----
mean loss: 208.77
train mean loss: 212.38
epoch train time: 0:00:00.716780
elapsed time: 0:02:51.013900
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 01:59:07.400704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.45
 ---- batch: 020 ----
mean loss: 209.72
 ---- batch: 030 ----
mean loss: 208.72
 ---- batch: 040 ----
mean loss: 211.21
 ---- batch: 050 ----
mean loss: 217.27
 ---- batch: 060 ----
mean loss: 222.34
 ---- batch: 070 ----
mean loss: 207.37
 ---- batch: 080 ----
mean loss: 211.73
 ---- batch: 090 ----
mean loss: 207.82
 ---- batch: 100 ----
mean loss: 205.03
 ---- batch: 110 ----
mean loss: 212.27
train mean loss: 212.09
epoch train time: 0:00:00.718954
elapsed time: 0:02:51.732984
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 01:59:08.119789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.20
 ---- batch: 020 ----
mean loss: 217.16
 ---- batch: 030 ----
mean loss: 211.47
 ---- batch: 040 ----
mean loss: 215.29
 ---- batch: 050 ----
mean loss: 219.15
 ---- batch: 060 ----
mean loss: 204.30
 ---- batch: 070 ----
mean loss: 205.97
 ---- batch: 080 ----
mean loss: 210.45
 ---- batch: 090 ----
mean loss: 204.65
 ---- batch: 100 ----
mean loss: 214.94
 ---- batch: 110 ----
mean loss: 218.35
train mean loss: 211.76
epoch train time: 0:00:00.723134
elapsed time: 0:02:52.456262
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 01:59:08.843076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.00
 ---- batch: 020 ----
mean loss: 220.89
 ---- batch: 030 ----
mean loss: 215.98
 ---- batch: 040 ----
mean loss: 209.47
 ---- batch: 050 ----
mean loss: 202.81
 ---- batch: 060 ----
mean loss: 210.90
 ---- batch: 070 ----
mean loss: 220.13
 ---- batch: 080 ----
mean loss: 208.87
 ---- batch: 090 ----
mean loss: 209.69
 ---- batch: 100 ----
mean loss: 216.93
 ---- batch: 110 ----
mean loss: 203.98
train mean loss: 211.48
epoch train time: 0:00:00.721475
elapsed time: 0:02:53.177876
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 01:59:09.564697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.58
 ---- batch: 020 ----
mean loss: 218.32
 ---- batch: 030 ----
mean loss: 220.30
 ---- batch: 040 ----
mean loss: 217.24
 ---- batch: 050 ----
mean loss: 199.58
 ---- batch: 060 ----
mean loss: 207.24
 ---- batch: 070 ----
mean loss: 211.96
 ---- batch: 080 ----
mean loss: 206.46
 ---- batch: 090 ----
mean loss: 216.63
 ---- batch: 100 ----
mean loss: 209.05
 ---- batch: 110 ----
mean loss: 217.50
train mean loss: 211.35
epoch train time: 0:00:00.724319
elapsed time: 0:02:53.902344
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 01:59:10.289150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.29
 ---- batch: 020 ----
mean loss: 213.46
 ---- batch: 030 ----
mean loss: 206.44
 ---- batch: 040 ----
mean loss: 219.90
 ---- batch: 050 ----
mean loss: 210.21
 ---- batch: 060 ----
mean loss: 209.81
 ---- batch: 070 ----
mean loss: 213.29
 ---- batch: 080 ----
mean loss: 210.59
 ---- batch: 090 ----
mean loss: 208.61
 ---- batch: 100 ----
mean loss: 200.62
 ---- batch: 110 ----
mean loss: 215.36
train mean loss: 210.96
epoch train time: 0:00:00.731271
elapsed time: 0:02:54.633749
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 01:59:11.020555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.78
 ---- batch: 020 ----
mean loss: 211.92
 ---- batch: 030 ----
mean loss: 216.15
 ---- batch: 040 ----
mean loss: 206.26
 ---- batch: 050 ----
mean loss: 216.55
 ---- batch: 060 ----
mean loss: 205.09
 ---- batch: 070 ----
mean loss: 225.42
 ---- batch: 080 ----
mean loss: 213.77
 ---- batch: 090 ----
mean loss: 204.54
 ---- batch: 100 ----
mean loss: 207.34
 ---- batch: 110 ----
mean loss: 204.51
train mean loss: 210.83
epoch train time: 0:00:00.718162
elapsed time: 0:02:55.352046
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 01:59:11.738867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.07
 ---- batch: 020 ----
mean loss: 209.99
 ---- batch: 030 ----
mean loss: 210.26
 ---- batch: 040 ----
mean loss: 209.94
 ---- batch: 050 ----
mean loss: 209.41
 ---- batch: 060 ----
mean loss: 216.90
 ---- batch: 070 ----
mean loss: 207.06
 ---- batch: 080 ----
mean loss: 207.87
 ---- batch: 090 ----
mean loss: 209.09
 ---- batch: 100 ----
mean loss: 197.61
 ---- batch: 110 ----
mean loss: 216.93
train mean loss: 210.68
epoch train time: 0:00:00.719224
elapsed time: 0:02:56.071423
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 01:59:12.458229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.99
 ---- batch: 020 ----
mean loss: 205.08
 ---- batch: 030 ----
mean loss: 221.39
 ---- batch: 040 ----
mean loss: 203.29
 ---- batch: 050 ----
mean loss: 212.23
 ---- batch: 060 ----
mean loss: 222.58
 ---- batch: 070 ----
mean loss: 209.99
 ---- batch: 080 ----
mean loss: 212.52
 ---- batch: 090 ----
mean loss: 202.04
 ---- batch: 100 ----
mean loss: 204.30
 ---- batch: 110 ----
mean loss: 211.58
train mean loss: 210.31
epoch train time: 0:00:00.714628
elapsed time: 0:02:56.786205
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 01:59:13.173024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.25
 ---- batch: 020 ----
mean loss: 205.68
 ---- batch: 030 ----
mean loss: 211.68
 ---- batch: 040 ----
mean loss: 206.62
 ---- batch: 050 ----
mean loss: 214.61
 ---- batch: 060 ----
mean loss: 213.07
 ---- batch: 070 ----
mean loss: 210.04
 ---- batch: 080 ----
mean loss: 201.05
 ---- batch: 090 ----
mean loss: 208.88
 ---- batch: 100 ----
mean loss: 209.81
 ---- batch: 110 ----
mean loss: 219.21
train mean loss: 210.25
epoch train time: 0:00:00.718415
elapsed time: 0:02:57.504769
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 01:59:13.891605
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.90
 ---- batch: 020 ----
mean loss: 208.87
 ---- batch: 030 ----
mean loss: 219.29
 ---- batch: 040 ----
mean loss: 210.36
 ---- batch: 050 ----
mean loss: 207.79
 ---- batch: 060 ----
mean loss: 210.27
 ---- batch: 070 ----
mean loss: 203.90
 ---- batch: 080 ----
mean loss: 211.74
 ---- batch: 090 ----
mean loss: 210.48
 ---- batch: 100 ----
mean loss: 200.17
 ---- batch: 110 ----
mean loss: 208.78
train mean loss: 209.64
epoch train time: 0:00:00.721825
elapsed time: 0:02:58.226768
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 01:59:14.613566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.43
 ---- batch: 020 ----
mean loss: 204.07
 ---- batch: 030 ----
mean loss: 199.21
 ---- batch: 040 ----
mean loss: 212.21
 ---- batch: 050 ----
mean loss: 210.76
 ---- batch: 060 ----
mean loss: 211.44
 ---- batch: 070 ----
mean loss: 205.15
 ---- batch: 080 ----
mean loss: 221.99
 ---- batch: 090 ----
mean loss: 214.99
 ---- batch: 100 ----
mean loss: 209.33
 ---- batch: 110 ----
mean loss: 202.13
train mean loss: 209.62
epoch train time: 0:00:00.715552
elapsed time: 0:02:58.942444
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 01:59:15.329247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.69
 ---- batch: 020 ----
mean loss: 213.40
 ---- batch: 030 ----
mean loss: 211.98
 ---- batch: 040 ----
mean loss: 211.23
 ---- batch: 050 ----
mean loss: 214.27
 ---- batch: 060 ----
mean loss: 207.34
 ---- batch: 070 ----
mean loss: 203.26
 ---- batch: 080 ----
mean loss: 217.29
 ---- batch: 090 ----
mean loss: 211.78
 ---- batch: 100 ----
mean loss: 205.62
 ---- batch: 110 ----
mean loss: 201.97
train mean loss: 209.51
epoch train time: 0:00:00.723643
elapsed time: 0:02:59.666224
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 01:59:16.053048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.30
 ---- batch: 020 ----
mean loss: 210.98
 ---- batch: 030 ----
mean loss: 204.51
 ---- batch: 040 ----
mean loss: 208.25
 ---- batch: 050 ----
mean loss: 214.58
 ---- batch: 060 ----
mean loss: 207.05
 ---- batch: 070 ----
mean loss: 214.90
 ---- batch: 080 ----
mean loss: 216.00
 ---- batch: 090 ----
mean loss: 210.83
 ---- batch: 100 ----
mean loss: 204.57
 ---- batch: 110 ----
mean loss: 205.95
train mean loss: 209.50
epoch train time: 0:00:00.718191
elapsed time: 0:03:00.384571
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 01:59:16.771377
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.60
 ---- batch: 020 ----
mean loss: 203.17
 ---- batch: 030 ----
mean loss: 207.63
 ---- batch: 040 ----
mean loss: 207.70
 ---- batch: 050 ----
mean loss: 214.70
 ---- batch: 060 ----
mean loss: 211.73
 ---- batch: 070 ----
mean loss: 210.71
 ---- batch: 080 ----
mean loss: 215.52
 ---- batch: 090 ----
mean loss: 206.28
 ---- batch: 100 ----
mean loss: 200.67
 ---- batch: 110 ----
mean loss: 219.73
train mean loss: 209.51
epoch train time: 0:00:00.717160
elapsed time: 0:03:01.101863
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 01:59:17.488668
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.06
 ---- batch: 020 ----
mean loss: 208.03
 ---- batch: 030 ----
mean loss: 208.25
 ---- batch: 040 ----
mean loss: 212.43
 ---- batch: 050 ----
mean loss: 202.69
 ---- batch: 060 ----
mean loss: 209.73
 ---- batch: 070 ----
mean loss: 209.82
 ---- batch: 080 ----
mean loss: 214.86
 ---- batch: 090 ----
mean loss: 205.94
 ---- batch: 100 ----
mean loss: 202.12
 ---- batch: 110 ----
mean loss: 217.69
train mean loss: 209.54
epoch train time: 0:00:00.720495
elapsed time: 0:03:01.822491
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 01:59:18.209296
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.51
 ---- batch: 020 ----
mean loss: 210.00
 ---- batch: 030 ----
mean loss: 213.98
 ---- batch: 040 ----
mean loss: 220.28
 ---- batch: 050 ----
mean loss: 197.63
 ---- batch: 060 ----
mean loss: 209.31
 ---- batch: 070 ----
mean loss: 202.98
 ---- batch: 080 ----
mean loss: 211.36
 ---- batch: 090 ----
mean loss: 203.57
 ---- batch: 100 ----
mean loss: 219.92
 ---- batch: 110 ----
mean loss: 206.80
train mean loss: 209.53
epoch train time: 0:00:00.721666
elapsed time: 0:03:02.544329
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 01:59:18.931152
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.55
 ---- batch: 020 ----
mean loss: 208.10
 ---- batch: 030 ----
mean loss: 202.05
 ---- batch: 040 ----
mean loss: 205.37
 ---- batch: 050 ----
mean loss: 213.53
 ---- batch: 060 ----
mean loss: 219.30
 ---- batch: 070 ----
mean loss: 215.67
 ---- batch: 080 ----
mean loss: 217.23
 ---- batch: 090 ----
mean loss: 207.65
 ---- batch: 100 ----
mean loss: 205.27
 ---- batch: 110 ----
mean loss: 212.30
train mean loss: 209.53
epoch train time: 0:00:00.726964
elapsed time: 0:03:03.271446
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 01:59:19.658251
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.34
 ---- batch: 020 ----
mean loss: 189.81
 ---- batch: 030 ----
mean loss: 214.83
 ---- batch: 040 ----
mean loss: 203.38
 ---- batch: 050 ----
mean loss: 209.47
 ---- batch: 060 ----
mean loss: 216.24
 ---- batch: 070 ----
mean loss: 209.78
 ---- batch: 080 ----
mean loss: 207.73
 ---- batch: 090 ----
mean loss: 210.88
 ---- batch: 100 ----
mean loss: 209.16
 ---- batch: 110 ----
mean loss: 225.70
train mean loss: 209.41
epoch train time: 0:00:00.716718
elapsed time: 0:03:03.988294
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 01:59:20.375096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.27
 ---- batch: 020 ----
mean loss: 206.31
 ---- batch: 030 ----
mean loss: 212.73
 ---- batch: 040 ----
mean loss: 215.69
 ---- batch: 050 ----
mean loss: 212.10
 ---- batch: 060 ----
mean loss: 213.99
 ---- batch: 070 ----
mean loss: 204.46
 ---- batch: 080 ----
mean loss: 201.27
 ---- batch: 090 ----
mean loss: 199.19
 ---- batch: 100 ----
mean loss: 203.65
 ---- batch: 110 ----
mean loss: 210.02
train mean loss: 209.40
epoch train time: 0:00:00.713112
elapsed time: 0:03:04.701535
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 01:59:21.088357
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.41
 ---- batch: 020 ----
mean loss: 214.57
 ---- batch: 030 ----
mean loss: 206.81
 ---- batch: 040 ----
mean loss: 212.90
 ---- batch: 050 ----
mean loss: 218.35
 ---- batch: 060 ----
mean loss: 209.15
 ---- batch: 070 ----
mean loss: 202.40
 ---- batch: 080 ----
mean loss: 202.36
 ---- batch: 090 ----
mean loss: 216.73
 ---- batch: 100 ----
mean loss: 199.54
 ---- batch: 110 ----
mean loss: 206.45
train mean loss: 209.38
epoch train time: 0:00:00.727292
elapsed time: 0:03:05.428996
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 01:59:21.815817
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.48
 ---- batch: 020 ----
mean loss: 207.28
 ---- batch: 030 ----
mean loss: 209.29
 ---- batch: 040 ----
mean loss: 211.45
 ---- batch: 050 ----
mean loss: 214.96
 ---- batch: 060 ----
mean loss: 213.46
 ---- batch: 070 ----
mean loss: 214.51
 ---- batch: 080 ----
mean loss: 203.72
 ---- batch: 090 ----
mean loss: 209.31
 ---- batch: 100 ----
mean loss: 208.68
 ---- batch: 110 ----
mean loss: 207.85
train mean loss: 209.31
epoch train time: 0:00:00.721819
elapsed time: 0:03:06.150981
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 01:59:22.537796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.35
 ---- batch: 020 ----
mean loss: 210.74
 ---- batch: 030 ----
mean loss: 199.07
 ---- batch: 040 ----
mean loss: 216.17
 ---- batch: 050 ----
mean loss: 213.16
 ---- batch: 060 ----
mean loss: 213.10
 ---- batch: 070 ----
mean loss: 201.55
 ---- batch: 080 ----
mean loss: 205.25
 ---- batch: 090 ----
mean loss: 200.86
 ---- batch: 100 ----
mean loss: 214.80
 ---- batch: 110 ----
mean loss: 216.20
train mean loss: 209.26
epoch train time: 0:00:00.726006
elapsed time: 0:03:06.877129
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 01:59:23.263934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.30
 ---- batch: 020 ----
mean loss: 217.03
 ---- batch: 030 ----
mean loss: 209.83
 ---- batch: 040 ----
mean loss: 197.43
 ---- batch: 050 ----
mean loss: 213.26
 ---- batch: 060 ----
mean loss: 204.37
 ---- batch: 070 ----
mean loss: 218.46
 ---- batch: 080 ----
mean loss: 212.16
 ---- batch: 090 ----
mean loss: 204.46
 ---- batch: 100 ----
mean loss: 207.56
 ---- batch: 110 ----
mean loss: 211.27
train mean loss: 209.35
epoch train time: 0:00:00.721628
elapsed time: 0:03:07.598930
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 01:59:23.985740
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.64
 ---- batch: 020 ----
mean loss: 215.77
 ---- batch: 030 ----
mean loss: 209.28
 ---- batch: 040 ----
mean loss: 218.13
 ---- batch: 050 ----
mean loss: 204.17
 ---- batch: 060 ----
mean loss: 202.26
 ---- batch: 070 ----
mean loss: 219.14
 ---- batch: 080 ----
mean loss: 200.70
 ---- batch: 090 ----
mean loss: 197.11
 ---- batch: 100 ----
mean loss: 205.43
 ---- batch: 110 ----
mean loss: 208.87
train mean loss: 209.30
epoch train time: 0:00:00.721489
elapsed time: 0:03:08.320558
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 01:59:24.707364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.57
 ---- batch: 020 ----
mean loss: 205.78
 ---- batch: 030 ----
mean loss: 210.71
 ---- batch: 040 ----
mean loss: 209.63
 ---- batch: 050 ----
mean loss: 205.52
 ---- batch: 060 ----
mean loss: 206.82
 ---- batch: 070 ----
mean loss: 202.52
 ---- batch: 080 ----
mean loss: 214.84
 ---- batch: 090 ----
mean loss: 210.44
 ---- batch: 100 ----
mean loss: 216.17
 ---- batch: 110 ----
mean loss: 208.75
train mean loss: 209.29
epoch train time: 0:00:00.715031
elapsed time: 0:03:09.035754
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 01:59:25.422606
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.93
 ---- batch: 020 ----
mean loss: 204.80
 ---- batch: 030 ----
mean loss: 206.99
 ---- batch: 040 ----
mean loss: 206.11
 ---- batch: 050 ----
mean loss: 205.53
 ---- batch: 060 ----
mean loss: 211.84
 ---- batch: 070 ----
mean loss: 208.42
 ---- batch: 080 ----
mean loss: 215.86
 ---- batch: 090 ----
mean loss: 210.27
 ---- batch: 100 ----
mean loss: 215.61
 ---- batch: 110 ----
mean loss: 214.77
train mean loss: 209.28
epoch train time: 0:00:00.733170
elapsed time: 0:03:09.769112
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 01:59:26.155918
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.04
 ---- batch: 020 ----
mean loss: 216.44
 ---- batch: 030 ----
mean loss: 215.32
 ---- batch: 040 ----
mean loss: 214.86
 ---- batch: 050 ----
mean loss: 206.53
 ---- batch: 060 ----
mean loss: 209.98
 ---- batch: 070 ----
mean loss: 208.86
 ---- batch: 080 ----
mean loss: 198.70
 ---- batch: 090 ----
mean loss: 206.55
 ---- batch: 100 ----
mean loss: 206.19
 ---- batch: 110 ----
mean loss: 209.25
train mean loss: 209.18
epoch train time: 0:00:00.745500
elapsed time: 0:03:10.514772
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 01:59:26.901579
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.61
 ---- batch: 020 ----
mean loss: 213.79
 ---- batch: 030 ----
mean loss: 205.62
 ---- batch: 040 ----
mean loss: 206.74
 ---- batch: 050 ----
mean loss: 211.99
 ---- batch: 060 ----
mean loss: 203.92
 ---- batch: 070 ----
mean loss: 206.00
 ---- batch: 080 ----
mean loss: 216.15
 ---- batch: 090 ----
mean loss: 211.15
 ---- batch: 100 ----
mean loss: 207.78
 ---- batch: 110 ----
mean loss: 208.32
train mean loss: 209.21
epoch train time: 0:00:00.730933
elapsed time: 0:03:11.245870
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 01:59:27.632674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.21
 ---- batch: 020 ----
mean loss: 212.20
 ---- batch: 030 ----
mean loss: 208.75
 ---- batch: 040 ----
mean loss: 207.21
 ---- batch: 050 ----
mean loss: 216.16
 ---- batch: 060 ----
mean loss: 201.51
 ---- batch: 070 ----
mean loss: 211.67
 ---- batch: 080 ----
mean loss: 211.60
 ---- batch: 090 ----
mean loss: 209.45
 ---- batch: 100 ----
mean loss: 219.37
 ---- batch: 110 ----
mean loss: 202.50
train mean loss: 209.15
epoch train time: 0:00:00.739683
elapsed time: 0:03:11.985722
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 01:59:28.372559
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.23
 ---- batch: 020 ----
mean loss: 202.59
 ---- batch: 030 ----
mean loss: 212.29
 ---- batch: 040 ----
mean loss: 218.95
 ---- batch: 050 ----
mean loss: 203.28
 ---- batch: 060 ----
mean loss: 205.29
 ---- batch: 070 ----
mean loss: 216.04
 ---- batch: 080 ----
mean loss: 215.14
 ---- batch: 090 ----
mean loss: 204.62
 ---- batch: 100 ----
mean loss: 208.08
 ---- batch: 110 ----
mean loss: 212.00
train mean loss: 209.10
epoch train time: 0:00:00.750028
elapsed time: 0:03:12.735951
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 01:59:29.122757
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.77
 ---- batch: 020 ----
mean loss: 216.53
 ---- batch: 030 ----
mean loss: 219.51
 ---- batch: 040 ----
mean loss: 202.43
 ---- batch: 050 ----
mean loss: 207.36
 ---- batch: 060 ----
mean loss: 207.99
 ---- batch: 070 ----
mean loss: 209.63
 ---- batch: 080 ----
mean loss: 205.13
 ---- batch: 090 ----
mean loss: 202.34
 ---- batch: 100 ----
mean loss: 209.21
 ---- batch: 110 ----
mean loss: 205.88
train mean loss: 209.18
epoch train time: 0:00:00.742023
elapsed time: 0:03:13.478107
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 01:59:29.864915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.21
 ---- batch: 020 ----
mean loss: 208.54
 ---- batch: 030 ----
mean loss: 201.36
 ---- batch: 040 ----
mean loss: 215.19
 ---- batch: 050 ----
mean loss: 215.17
 ---- batch: 060 ----
mean loss: 207.51
 ---- batch: 070 ----
mean loss: 208.86
 ---- batch: 080 ----
mean loss: 203.90
 ---- batch: 090 ----
mean loss: 210.62
 ---- batch: 100 ----
mean loss: 213.80
 ---- batch: 110 ----
mean loss: 207.46
train mean loss: 209.10
epoch train time: 0:00:00.732442
elapsed time: 0:03:14.210721
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 01:59:30.597548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.22
 ---- batch: 020 ----
mean loss: 209.75
 ---- batch: 030 ----
mean loss: 210.12
 ---- batch: 040 ----
mean loss: 211.07
 ---- batch: 050 ----
mean loss: 204.77
 ---- batch: 060 ----
mean loss: 219.67
 ---- batch: 070 ----
mean loss: 207.36
 ---- batch: 080 ----
mean loss: 213.52
 ---- batch: 090 ----
mean loss: 211.15
 ---- batch: 100 ----
mean loss: 199.60
 ---- batch: 110 ----
mean loss: 206.64
train mean loss: 209.08
epoch train time: 0:00:00.742611
elapsed time: 0:03:14.953503
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 01:59:31.340312
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.99
 ---- batch: 020 ----
mean loss: 205.36
 ---- batch: 030 ----
mean loss: 213.20
 ---- batch: 040 ----
mean loss: 217.29
 ---- batch: 050 ----
mean loss: 200.19
 ---- batch: 060 ----
mean loss: 207.87
 ---- batch: 070 ----
mean loss: 205.15
 ---- batch: 080 ----
mean loss: 201.44
 ---- batch: 090 ----
mean loss: 214.46
 ---- batch: 100 ----
mean loss: 215.26
 ---- batch: 110 ----
mean loss: 204.74
train mean loss: 209.11
epoch train time: 0:00:00.725867
elapsed time: 0:03:15.679515
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 01:59:32.066321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.57
 ---- batch: 020 ----
mean loss: 216.91
 ---- batch: 030 ----
mean loss: 211.57
 ---- batch: 040 ----
mean loss: 204.70
 ---- batch: 050 ----
mean loss: 209.04
 ---- batch: 060 ----
mean loss: 195.67
 ---- batch: 070 ----
mean loss: 213.76
 ---- batch: 080 ----
mean loss: 204.76
 ---- batch: 090 ----
mean loss: 217.60
 ---- batch: 100 ----
mean loss: 213.96
 ---- batch: 110 ----
mean loss: 208.13
train mean loss: 209.00
epoch train time: 0:00:00.723135
elapsed time: 0:03:16.402794
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 01:59:32.789626
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.45
 ---- batch: 020 ----
mean loss: 211.05
 ---- batch: 030 ----
mean loss: 209.59
 ---- batch: 040 ----
mean loss: 203.23
 ---- batch: 050 ----
mean loss: 205.26
 ---- batch: 060 ----
mean loss: 210.95
 ---- batch: 070 ----
mean loss: 218.61
 ---- batch: 080 ----
mean loss: 216.88
 ---- batch: 090 ----
mean loss: 211.84
 ---- batch: 100 ----
mean loss: 210.00
 ---- batch: 110 ----
mean loss: 202.62
train mean loss: 208.99
epoch train time: 0:00:00.714823
elapsed time: 0:03:17.117776
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 01:59:33.504580
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.94
 ---- batch: 020 ----
mean loss: 205.94
 ---- batch: 030 ----
mean loss: 203.84
 ---- batch: 040 ----
mean loss: 209.80
 ---- batch: 050 ----
mean loss: 211.99
 ---- batch: 060 ----
mean loss: 212.43
 ---- batch: 070 ----
mean loss: 218.32
 ---- batch: 080 ----
mean loss: 209.22
 ---- batch: 090 ----
mean loss: 203.64
 ---- batch: 100 ----
mean loss: 207.11
 ---- batch: 110 ----
mean loss: 206.68
train mean loss: 209.02
epoch train time: 0:00:00.714748
elapsed time: 0:03:17.832671
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 01:59:34.219478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.77
 ---- batch: 020 ----
mean loss: 212.45
 ---- batch: 030 ----
mean loss: 219.28
 ---- batch: 040 ----
mean loss: 210.85
 ---- batch: 050 ----
mean loss: 209.46
 ---- batch: 060 ----
mean loss: 204.85
 ---- batch: 070 ----
mean loss: 206.17
 ---- batch: 080 ----
mean loss: 196.99
 ---- batch: 090 ----
mean loss: 214.40
 ---- batch: 100 ----
mean loss: 211.45
 ---- batch: 110 ----
mean loss: 208.66
train mean loss: 208.92
epoch train time: 0:00:00.727040
elapsed time: 0:03:18.559851
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 01:59:34.946678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.81
 ---- batch: 020 ----
mean loss: 204.40
 ---- batch: 030 ----
mean loss: 199.22
 ---- batch: 040 ----
mean loss: 224.10
 ---- batch: 050 ----
mean loss: 199.67
 ---- batch: 060 ----
mean loss: 206.53
 ---- batch: 070 ----
mean loss: 216.36
 ---- batch: 080 ----
mean loss: 212.18
 ---- batch: 090 ----
mean loss: 207.81
 ---- batch: 100 ----
mean loss: 200.26
 ---- batch: 110 ----
mean loss: 211.94
train mean loss: 209.00
epoch train time: 0:00:00.720095
elapsed time: 0:03:19.280111
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 01:59:35.666927
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.35
 ---- batch: 020 ----
mean loss: 198.25
 ---- batch: 030 ----
mean loss: 200.18
 ---- batch: 040 ----
mean loss: 213.84
 ---- batch: 050 ----
mean loss: 211.72
 ---- batch: 060 ----
mean loss: 200.80
 ---- batch: 070 ----
mean loss: 212.43
 ---- batch: 080 ----
mean loss: 220.01
 ---- batch: 090 ----
mean loss: 205.53
 ---- batch: 100 ----
mean loss: 218.88
 ---- batch: 110 ----
mean loss: 208.75
train mean loss: 208.91
epoch train time: 0:00:00.723079
elapsed time: 0:03:20.003336
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 01:59:36.390140
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.46
 ---- batch: 020 ----
mean loss: 202.36
 ---- batch: 030 ----
mean loss: 215.62
 ---- batch: 040 ----
mean loss: 212.70
 ---- batch: 050 ----
mean loss: 196.82
 ---- batch: 060 ----
mean loss: 213.55
 ---- batch: 070 ----
mean loss: 210.31
 ---- batch: 080 ----
mean loss: 216.74
 ---- batch: 090 ----
mean loss: 210.75
 ---- batch: 100 ----
mean loss: 204.87
 ---- batch: 110 ----
mean loss: 208.71
train mean loss: 208.82
epoch train time: 0:00:00.722476
elapsed time: 0:03:20.725942
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 01:59:37.112749
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.65
 ---- batch: 020 ----
mean loss: 203.59
 ---- batch: 030 ----
mean loss: 203.95
 ---- batch: 040 ----
mean loss: 208.54
 ---- batch: 050 ----
mean loss: 208.89
 ---- batch: 060 ----
mean loss: 218.87
 ---- batch: 070 ----
mean loss: 211.42
 ---- batch: 080 ----
mean loss: 210.99
 ---- batch: 090 ----
mean loss: 204.66
 ---- batch: 100 ----
mean loss: 212.71
 ---- batch: 110 ----
mean loss: 208.17
train mean loss: 208.84
epoch train time: 0:00:00.718152
elapsed time: 0:03:21.444253
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 01:59:37.831063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.28
 ---- batch: 020 ----
mean loss: 207.25
 ---- batch: 030 ----
mean loss: 198.16
 ---- batch: 040 ----
mean loss: 211.81
 ---- batch: 050 ----
mean loss: 205.71
 ---- batch: 060 ----
mean loss: 211.39
 ---- batch: 070 ----
mean loss: 207.61
 ---- batch: 080 ----
mean loss: 217.73
 ---- batch: 090 ----
mean loss: 214.99
 ---- batch: 100 ----
mean loss: 208.03
 ---- batch: 110 ----
mean loss: 205.21
train mean loss: 208.80
epoch train time: 0:00:00.720328
elapsed time: 0:03:22.164720
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 01:59:38.551543
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.12
 ---- batch: 020 ----
mean loss: 200.80
 ---- batch: 030 ----
mean loss: 205.96
 ---- batch: 040 ----
mean loss: 219.35
 ---- batch: 050 ----
mean loss: 205.18
 ---- batch: 060 ----
mean loss: 214.80
 ---- batch: 070 ----
mean loss: 211.32
 ---- batch: 080 ----
mean loss: 202.74
 ---- batch: 090 ----
mean loss: 204.79
 ---- batch: 100 ----
mean loss: 211.90
 ---- batch: 110 ----
mean loss: 206.86
train mean loss: 208.82
epoch train time: 0:00:00.722314
elapsed time: 0:03:22.887192
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 01:59:39.274001
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.18
 ---- batch: 020 ----
mean loss: 211.64
 ---- batch: 030 ----
mean loss: 203.85
 ---- batch: 040 ----
mean loss: 216.25
 ---- batch: 050 ----
mean loss: 208.22
 ---- batch: 060 ----
mean loss: 222.70
 ---- batch: 070 ----
mean loss: 209.56
 ---- batch: 080 ----
mean loss: 202.85
 ---- batch: 090 ----
mean loss: 207.70
 ---- batch: 100 ----
mean loss: 205.52
 ---- batch: 110 ----
mean loss: 203.18
train mean loss: 208.80
epoch train time: 0:00:00.722855
elapsed time: 0:03:23.610196
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 01:59:39.997020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.73
 ---- batch: 020 ----
mean loss: 219.15
 ---- batch: 030 ----
mean loss: 208.93
 ---- batch: 040 ----
mean loss: 201.43
 ---- batch: 050 ----
mean loss: 208.82
 ---- batch: 060 ----
mean loss: 213.00
 ---- batch: 070 ----
mean loss: 200.87
 ---- batch: 080 ----
mean loss: 203.27
 ---- batch: 090 ----
mean loss: 213.56
 ---- batch: 100 ----
mean loss: 212.46
 ---- batch: 110 ----
mean loss: 203.11
train mean loss: 208.76
epoch train time: 0:00:00.715364
elapsed time: 0:03:24.325714
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 01:59:40.712535
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.23
 ---- batch: 020 ----
mean loss: 212.96
 ---- batch: 030 ----
mean loss: 207.81
 ---- batch: 040 ----
mean loss: 208.89
 ---- batch: 050 ----
mean loss: 203.76
 ---- batch: 060 ----
mean loss: 210.23
 ---- batch: 070 ----
mean loss: 220.51
 ---- batch: 080 ----
mean loss: 202.97
 ---- batch: 090 ----
mean loss: 202.84
 ---- batch: 100 ----
mean loss: 217.01
 ---- batch: 110 ----
mean loss: 206.64
train mean loss: 208.71
epoch train time: 0:00:00.716729
elapsed time: 0:03:25.042595
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 01:59:41.429401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.89
 ---- batch: 020 ----
mean loss: 208.35
 ---- batch: 030 ----
mean loss: 214.41
 ---- batch: 040 ----
mean loss: 214.66
 ---- batch: 050 ----
mean loss: 205.70
 ---- batch: 060 ----
mean loss: 215.11
 ---- batch: 070 ----
mean loss: 208.80
 ---- batch: 080 ----
mean loss: 218.05
 ---- batch: 090 ----
mean loss: 202.82
 ---- batch: 100 ----
mean loss: 206.30
 ---- batch: 110 ----
mean loss: 200.12
train mean loss: 208.74
epoch train time: 0:00:00.717329
elapsed time: 0:03:25.760060
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 01:59:42.146866
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.49
 ---- batch: 020 ----
mean loss: 205.21
 ---- batch: 030 ----
mean loss: 211.67
 ---- batch: 040 ----
mean loss: 200.12
 ---- batch: 050 ----
mean loss: 209.48
 ---- batch: 060 ----
mean loss: 210.77
 ---- batch: 070 ----
mean loss: 210.30
 ---- batch: 080 ----
mean loss: 208.00
 ---- batch: 090 ----
mean loss: 221.28
 ---- batch: 100 ----
mean loss: 207.25
 ---- batch: 110 ----
mean loss: 209.75
train mean loss: 208.73
epoch train time: 0:00:00.717389
elapsed time: 0:03:26.477606
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 01:59:42.864415
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.26
 ---- batch: 020 ----
mean loss: 207.33
 ---- batch: 030 ----
mean loss: 206.97
 ---- batch: 040 ----
mean loss: 209.16
 ---- batch: 050 ----
mean loss: 213.57
 ---- batch: 060 ----
mean loss: 214.92
 ---- batch: 070 ----
mean loss: 210.14
 ---- batch: 080 ----
mean loss: 211.24
 ---- batch: 090 ----
mean loss: 209.65
 ---- batch: 100 ----
mean loss: 215.44
 ---- batch: 110 ----
mean loss: 200.02
train mean loss: 208.67
epoch train time: 0:00:00.717867
elapsed time: 0:03:27.195613
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 01:59:43.582450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.42
 ---- batch: 020 ----
mean loss: 228.07
 ---- batch: 030 ----
mean loss: 218.64
 ---- batch: 040 ----
mean loss: 199.93
 ---- batch: 050 ----
mean loss: 204.02
 ---- batch: 060 ----
mean loss: 207.09
 ---- batch: 070 ----
mean loss: 211.07
 ---- batch: 080 ----
mean loss: 190.67
 ---- batch: 090 ----
mean loss: 208.94
 ---- batch: 100 ----
mean loss: 214.13
 ---- batch: 110 ----
mean loss: 203.95
train mean loss: 208.62
epoch train time: 0:00:00.725103
elapsed time: 0:03:27.920888
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 01:59:44.307695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.34
 ---- batch: 020 ----
mean loss: 214.78
 ---- batch: 030 ----
mean loss: 204.27
 ---- batch: 040 ----
mean loss: 214.98
 ---- batch: 050 ----
mean loss: 213.10
 ---- batch: 060 ----
mean loss: 207.12
 ---- batch: 070 ----
mean loss: 216.56
 ---- batch: 080 ----
mean loss: 200.73
 ---- batch: 090 ----
mean loss: 200.07
 ---- batch: 100 ----
mean loss: 217.92
 ---- batch: 110 ----
mean loss: 203.03
train mean loss: 208.62
epoch train time: 0:00:00.720710
elapsed time: 0:03:28.641739
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 01:59:45.028567
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.02
 ---- batch: 020 ----
mean loss: 208.69
 ---- batch: 030 ----
mean loss: 209.53
 ---- batch: 040 ----
mean loss: 208.33
 ---- batch: 050 ----
mean loss: 209.25
 ---- batch: 060 ----
mean loss: 201.36
 ---- batch: 070 ----
mean loss: 212.22
 ---- batch: 080 ----
mean loss: 215.70
 ---- batch: 090 ----
mean loss: 207.41
 ---- batch: 100 ----
mean loss: 207.32
 ---- batch: 110 ----
mean loss: 213.36
train mean loss: 208.57
epoch train time: 0:00:00.715288
elapsed time: 0:03:29.357214
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 01:59:45.744021
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.78
 ---- batch: 020 ----
mean loss: 204.97
 ---- batch: 030 ----
mean loss: 211.31
 ---- batch: 040 ----
mean loss: 207.68
 ---- batch: 050 ----
mean loss: 205.45
 ---- batch: 060 ----
mean loss: 210.82
 ---- batch: 070 ----
mean loss: 212.47
 ---- batch: 080 ----
mean loss: 204.44
 ---- batch: 090 ----
mean loss: 207.75
 ---- batch: 100 ----
mean loss: 212.88
 ---- batch: 110 ----
mean loss: 214.43
train mean loss: 208.61
epoch train time: 0:00:00.718346
elapsed time: 0:03:30.075707
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 01:59:46.462514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.37
 ---- batch: 020 ----
mean loss: 204.40
 ---- batch: 030 ----
mean loss: 206.50
 ---- batch: 040 ----
mean loss: 207.16
 ---- batch: 050 ----
mean loss: 212.35
 ---- batch: 060 ----
mean loss: 209.38
 ---- batch: 070 ----
mean loss: 210.82
 ---- batch: 080 ----
mean loss: 206.62
 ---- batch: 090 ----
mean loss: 201.09
 ---- batch: 100 ----
mean loss: 210.78
 ---- batch: 110 ----
mean loss: 209.35
train mean loss: 208.60
epoch train time: 0:00:00.721056
elapsed time: 0:03:30.796897
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 01:59:47.183702
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.08
 ---- batch: 020 ----
mean loss: 217.46
 ---- batch: 030 ----
mean loss: 208.90
 ---- batch: 040 ----
mean loss: 211.15
 ---- batch: 050 ----
mean loss: 203.78
 ---- batch: 060 ----
mean loss: 209.80
 ---- batch: 070 ----
mean loss: 210.87
 ---- batch: 080 ----
mean loss: 202.02
 ---- batch: 090 ----
mean loss: 210.34
 ---- batch: 100 ----
mean loss: 213.61
 ---- batch: 110 ----
mean loss: 217.17
train mean loss: 208.52
epoch train time: 0:00:00.715805
elapsed time: 0:03:31.512852
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 01:59:47.899657
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.31
 ---- batch: 020 ----
mean loss: 209.78
 ---- batch: 030 ----
mean loss: 206.95
 ---- batch: 040 ----
mean loss: 206.91
 ---- batch: 050 ----
mean loss: 204.76
 ---- batch: 060 ----
mean loss: 205.47
 ---- batch: 070 ----
mean loss: 202.40
 ---- batch: 080 ----
mean loss: 210.76
 ---- batch: 090 ----
mean loss: 210.61
 ---- batch: 100 ----
mean loss: 216.92
 ---- batch: 110 ----
mean loss: 210.68
train mean loss: 208.43
epoch train time: 0:00:00.719662
elapsed time: 0:03:32.232649
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 01:59:48.619463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.89
 ---- batch: 020 ----
mean loss: 210.50
 ---- batch: 030 ----
mean loss: 205.76
 ---- batch: 040 ----
mean loss: 206.25
 ---- batch: 050 ----
mean loss: 205.58
 ---- batch: 060 ----
mean loss: 215.70
 ---- batch: 070 ----
mean loss: 210.69
 ---- batch: 080 ----
mean loss: 209.02
 ---- batch: 090 ----
mean loss: 208.99
 ---- batch: 100 ----
mean loss: 213.25
 ---- batch: 110 ----
mean loss: 204.84
train mean loss: 208.46
epoch train time: 0:00:00.727231
elapsed time: 0:03:32.962082
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_8/checkpoint.pth.tar
**** end time: 2019-09-27 01:59:49.348856 ****
