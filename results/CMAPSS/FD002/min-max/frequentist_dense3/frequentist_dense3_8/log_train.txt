Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_8', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24734
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistDense3...
Done.
**** start time: 2019-09-25 23:25:26.286517 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
            Linear-2                  [-1, 100]          48,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 68,100
Trainable params: 68,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 23:25:26.289802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4281.92
 ---- batch: 020 ----
mean loss: 4110.75
 ---- batch: 030 ----
mean loss: 4078.46
 ---- batch: 040 ----
mean loss: 3948.38
 ---- batch: 050 ----
mean loss: 3805.62
 ---- batch: 060 ----
mean loss: 3836.35
 ---- batch: 070 ----
mean loss: 3703.69
 ---- batch: 080 ----
mean loss: 3696.73
 ---- batch: 090 ----
mean loss: 3637.98
train mean loss: 3879.79
epoch train time: 0:00:33.927105
elapsed time: 0:00:33.932679
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 23:26:00.219235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3474.17
 ---- batch: 020 ----
mean loss: 3507.42
 ---- batch: 030 ----
mean loss: 3435.67
 ---- batch: 040 ----
mean loss: 3385.83
 ---- batch: 050 ----
mean loss: 3263.01
 ---- batch: 060 ----
mean loss: 3263.64
 ---- batch: 070 ----
mean loss: 3202.89
 ---- batch: 080 ----
mean loss: 3119.79
 ---- batch: 090 ----
mean loss: 3090.07
train mean loss: 3288.63
epoch train time: 0:00:00.503732
elapsed time: 0:00:34.436545
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 23:26:00.723115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2967.10
 ---- batch: 020 ----
mean loss: 2907.16
 ---- batch: 030 ----
mean loss: 2893.89
 ---- batch: 040 ----
mean loss: 2848.56
 ---- batch: 050 ----
mean loss: 2815.62
 ---- batch: 060 ----
mean loss: 2763.83
 ---- batch: 070 ----
mean loss: 2747.67
 ---- batch: 080 ----
mean loss: 2673.37
 ---- batch: 090 ----
mean loss: 2599.03
train mean loss: 2791.26
epoch train time: 0:00:00.507874
elapsed time: 0:00:34.944582
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 23:26:01.231150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2556.09
 ---- batch: 020 ----
mean loss: 2480.95
 ---- batch: 030 ----
mean loss: 2431.97
 ---- batch: 040 ----
mean loss: 2447.75
 ---- batch: 050 ----
mean loss: 2382.36
 ---- batch: 060 ----
mean loss: 2360.87
 ---- batch: 070 ----
mean loss: 2274.05
 ---- batch: 080 ----
mean loss: 2251.98
 ---- batch: 090 ----
mean loss: 2242.35
train mean loss: 2369.03
epoch train time: 0:00:00.504985
elapsed time: 0:00:35.449729
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 23:26:01.736290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2144.03
 ---- batch: 020 ----
mean loss: 2130.50
 ---- batch: 030 ----
mean loss: 2074.92
 ---- batch: 040 ----
mean loss: 2040.81
 ---- batch: 050 ----
mean loss: 2053.19
 ---- batch: 060 ----
mean loss: 1969.41
 ---- batch: 070 ----
mean loss: 1961.13
 ---- batch: 080 ----
mean loss: 1960.99
 ---- batch: 090 ----
mean loss: 1904.41
train mean loss: 2016.50
epoch train time: 0:00:00.498822
elapsed time: 0:00:35.948746
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 23:26:02.235315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1830.56
 ---- batch: 020 ----
mean loss: 1799.30
 ---- batch: 030 ----
mean loss: 1807.01
 ---- batch: 040 ----
mean loss: 1759.43
 ---- batch: 050 ----
mean loss: 1789.80
 ---- batch: 060 ----
mean loss: 1684.72
 ---- batch: 070 ----
mean loss: 1705.05
 ---- batch: 080 ----
mean loss: 1706.16
 ---- batch: 090 ----
mean loss: 1628.00
train mean loss: 1736.79
epoch train time: 0:00:00.499988
elapsed time: 0:00:36.448878
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 23:26:02.735500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1604.62
 ---- batch: 020 ----
mean loss: 1596.47
 ---- batch: 030 ----
mean loss: 1517.15
 ---- batch: 040 ----
mean loss: 1544.81
 ---- batch: 050 ----
mean loss: 1514.15
 ---- batch: 060 ----
mean loss: 1506.00
 ---- batch: 070 ----
mean loss: 1471.11
 ---- batch: 080 ----
mean loss: 1458.41
 ---- batch: 090 ----
mean loss: 1441.80
train mean loss: 1509.20
epoch train time: 0:00:00.509440
elapsed time: 0:00:36.958516
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 23:26:03.245083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1385.08
 ---- batch: 020 ----
mean loss: 1391.33
 ---- batch: 030 ----
mean loss: 1392.12
 ---- batch: 040 ----
mean loss: 1318.43
 ---- batch: 050 ----
mean loss: 1349.87
 ---- batch: 060 ----
mean loss: 1327.52
 ---- batch: 070 ----
mean loss: 1284.75
 ---- batch: 080 ----
mean loss: 1299.59
 ---- batch: 090 ----
mean loss: 1259.02
train mean loss: 1328.19
epoch train time: 0:00:00.504949
elapsed time: 0:00:37.463611
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 23:26:03.750195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1251.31
 ---- batch: 020 ----
mean loss: 1212.54
 ---- batch: 030 ----
mean loss: 1211.57
 ---- batch: 040 ----
mean loss: 1199.80
 ---- batch: 050 ----
mean loss: 1215.06
 ---- batch: 060 ----
mean loss: 1167.58
 ---- batch: 070 ----
mean loss: 1181.05
 ---- batch: 080 ----
mean loss: 1140.75
 ---- batch: 090 ----
mean loss: 1167.12
train mean loss: 1191.50
epoch train time: 0:00:00.505343
elapsed time: 0:00:37.969121
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 23:26:04.255680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1138.00
 ---- batch: 020 ----
mean loss: 1112.92
 ---- batch: 030 ----
mean loss: 1113.12
 ---- batch: 040 ----
mean loss: 1075.15
 ---- batch: 050 ----
mean loss: 1100.93
 ---- batch: 060 ----
mean loss: 1084.01
 ---- batch: 070 ----
mean loss: 1091.30
 ---- batch: 080 ----
mean loss: 1051.00
 ---- batch: 090 ----
mean loss: 1063.55
train mean loss: 1088.73
epoch train time: 0:00:00.499214
elapsed time: 0:00:38.468497
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 23:26:04.755067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1026.85
 ---- batch: 020 ----
mean loss: 1044.58
 ---- batch: 030 ----
mean loss: 1038.76
 ---- batch: 040 ----
mean loss: 1013.32
 ---- batch: 050 ----
mean loss: 1010.58
 ---- batch: 060 ----
mean loss: 1022.56
 ---- batch: 070 ----
mean loss: 1008.36
 ---- batch: 080 ----
mean loss: 980.85
 ---- batch: 090 ----
mean loss: 1003.28
train mean loss: 1014.64
epoch train time: 0:00:00.501916
elapsed time: 0:00:38.970575
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 23:26:05.257144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.99
 ---- batch: 020 ----
mean loss: 977.96
 ---- batch: 030 ----
mean loss: 983.37
 ---- batch: 040 ----
mean loss: 977.78
 ---- batch: 050 ----
mean loss: 972.52
 ---- batch: 060 ----
mean loss: 965.73
 ---- batch: 070 ----
mean loss: 968.25
 ---- batch: 080 ----
mean loss: 942.33
 ---- batch: 090 ----
mean loss: 933.48
train mean loss: 962.28
epoch train time: 0:00:00.511661
elapsed time: 0:00:39.482400
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 23:26:05.768968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 933.97
 ---- batch: 020 ----
mean loss: 930.81
 ---- batch: 030 ----
mean loss: 942.18
 ---- batch: 040 ----
mean loss: 921.16
 ---- batch: 050 ----
mean loss: 947.97
 ---- batch: 060 ----
mean loss: 929.55
 ---- batch: 070 ----
mean loss: 910.76
 ---- batch: 080 ----
mean loss: 922.40
 ---- batch: 090 ----
mean loss: 925.67
train mean loss: 929.12
epoch train time: 0:00:00.506790
elapsed time: 0:00:39.989334
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 23:26:06.275901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.82
 ---- batch: 020 ----
mean loss: 912.55
 ---- batch: 030 ----
mean loss: 908.13
 ---- batch: 040 ----
mean loss: 888.40
 ---- batch: 050 ----
mean loss: 909.71
 ---- batch: 060 ----
mean loss: 903.10
 ---- batch: 070 ----
mean loss: 920.59
 ---- batch: 080 ----
mean loss: 904.42
 ---- batch: 090 ----
mean loss: 904.10
train mean loss: 907.07
epoch train time: 0:00:00.497611
elapsed time: 0:00:40.487126
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 23:26:06.773716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 910.73
 ---- batch: 020 ----
mean loss: 900.60
 ---- batch: 030 ----
mean loss: 896.20
 ---- batch: 040 ----
mean loss: 884.07
 ---- batch: 050 ----
mean loss: 888.20
 ---- batch: 060 ----
mean loss: 882.93
 ---- batch: 070 ----
mean loss: 886.75
 ---- batch: 080 ----
mean loss: 902.74
 ---- batch: 090 ----
mean loss: 894.48
train mean loss: 894.26
epoch train time: 0:00:00.499978
elapsed time: 0:00:40.987300
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 23:26:07.273916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.55
 ---- batch: 020 ----
mean loss: 888.72
 ---- batch: 030 ----
mean loss: 885.22
 ---- batch: 040 ----
mean loss: 896.81
 ---- batch: 050 ----
mean loss: 890.36
 ---- batch: 060 ----
mean loss: 878.60
 ---- batch: 070 ----
mean loss: 863.98
 ---- batch: 080 ----
mean loss: 884.16
 ---- batch: 090 ----
mean loss: 887.36
train mean loss: 885.55
epoch train time: 0:00:00.498936
elapsed time: 0:00:41.486427
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 23:26:07.772994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.58
 ---- batch: 020 ----
mean loss: 861.51
 ---- batch: 030 ----
mean loss: 878.62
 ---- batch: 040 ----
mean loss: 886.72
 ---- batch: 050 ----
mean loss: 862.67
 ---- batch: 060 ----
mean loss: 888.92
 ---- batch: 070 ----
mean loss: 895.19
 ---- batch: 080 ----
mean loss: 897.20
 ---- batch: 090 ----
mean loss: 867.07
train mean loss: 881.23
epoch train time: 0:00:00.506940
elapsed time: 0:00:41.993511
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 23:26:08.280094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.41
 ---- batch: 020 ----
mean loss: 873.71
 ---- batch: 030 ----
mean loss: 895.35
 ---- batch: 040 ----
mean loss: 886.21
 ---- batch: 050 ----
mean loss: 868.10
 ---- batch: 060 ----
mean loss: 862.15
 ---- batch: 070 ----
mean loss: 877.17
 ---- batch: 080 ----
mean loss: 873.84
 ---- batch: 090 ----
mean loss: 871.58
train mean loss: 878.04
epoch train time: 0:00:00.500794
elapsed time: 0:00:42.494465
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 23:26:08.781033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.39
 ---- batch: 020 ----
mean loss: 885.75
 ---- batch: 030 ----
mean loss: 862.53
 ---- batch: 040 ----
mean loss: 882.27
 ---- batch: 050 ----
mean loss: 879.53
 ---- batch: 060 ----
mean loss: 873.61
 ---- batch: 070 ----
mean loss: 858.69
 ---- batch: 080 ----
mean loss: 890.00
 ---- batch: 090 ----
mean loss: 884.47
train mean loss: 876.76
epoch train time: 0:00:00.505172
elapsed time: 0:00:42.999783
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 23:26:09.286350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.73
 ---- batch: 020 ----
mean loss: 888.38
 ---- batch: 030 ----
mean loss: 875.85
 ---- batch: 040 ----
mean loss: 883.53
 ---- batch: 050 ----
mean loss: 866.24
 ---- batch: 060 ----
mean loss: 877.99
 ---- batch: 070 ----
mean loss: 885.57
 ---- batch: 080 ----
mean loss: 863.26
 ---- batch: 090 ----
mean loss: 871.58
train mean loss: 876.20
epoch train time: 0:00:00.507130
elapsed time: 0:00:43.507057
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 23:26:09.793648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.40
 ---- batch: 020 ----
mean loss: 868.40
 ---- batch: 030 ----
mean loss: 867.53
 ---- batch: 040 ----
mean loss: 875.43
 ---- batch: 050 ----
mean loss: 894.07
 ---- batch: 060 ----
mean loss: 873.37
 ---- batch: 070 ----
mean loss: 857.29
 ---- batch: 080 ----
mean loss: 889.51
 ---- batch: 090 ----
mean loss: 879.05
train mean loss: 874.93
epoch train time: 0:00:00.503534
elapsed time: 0:00:44.010756
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 23:26:10.297323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.36
 ---- batch: 020 ----
mean loss: 887.92
 ---- batch: 030 ----
mean loss: 866.73
 ---- batch: 040 ----
mean loss: 856.46
 ---- batch: 050 ----
mean loss: 867.97
 ---- batch: 060 ----
mean loss: 890.99
 ---- batch: 070 ----
mean loss: 882.09
 ---- batch: 080 ----
mean loss: 871.08
 ---- batch: 090 ----
mean loss: 876.48
train mean loss: 875.79
epoch train time: 0:00:00.499357
elapsed time: 0:00:44.510269
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 23:26:10.796844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.97
 ---- batch: 020 ----
mean loss: 870.40
 ---- batch: 030 ----
mean loss: 860.87
 ---- batch: 040 ----
mean loss: 868.39
 ---- batch: 050 ----
mean loss: 878.78
 ---- batch: 060 ----
mean loss: 879.29
 ---- batch: 070 ----
mean loss: 875.74
 ---- batch: 080 ----
mean loss: 878.33
 ---- batch: 090 ----
mean loss: 885.04
train mean loss: 875.04
epoch train time: 0:00:00.507855
elapsed time: 0:00:45.018278
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 23:26:11.304883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.59
 ---- batch: 020 ----
mean loss: 871.05
 ---- batch: 030 ----
mean loss: 877.11
 ---- batch: 040 ----
mean loss: 857.83
 ---- batch: 050 ----
mean loss: 872.97
 ---- batch: 060 ----
mean loss: 865.46
 ---- batch: 070 ----
mean loss: 881.41
 ---- batch: 080 ----
mean loss: 881.57
 ---- batch: 090 ----
mean loss: 873.44
train mean loss: 876.22
epoch train time: 0:00:00.506951
elapsed time: 0:00:45.525423
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 23:26:11.811991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.72
 ---- batch: 020 ----
mean loss: 888.96
 ---- batch: 030 ----
mean loss: 883.42
 ---- batch: 040 ----
mean loss: 876.11
 ---- batch: 050 ----
mean loss: 881.02
 ---- batch: 060 ----
mean loss: 877.86
 ---- batch: 070 ----
mean loss: 873.89
 ---- batch: 080 ----
mean loss: 865.52
 ---- batch: 090 ----
mean loss: 883.92
train mean loss: 875.19
epoch train time: 0:00:00.505501
elapsed time: 0:00:46.031070
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 23:26:12.317663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.51
 ---- batch: 020 ----
mean loss: 869.74
 ---- batch: 030 ----
mean loss: 863.24
 ---- batch: 040 ----
mean loss: 866.18
 ---- batch: 050 ----
mean loss: 864.47
 ---- batch: 060 ----
mean loss: 902.43
 ---- batch: 070 ----
mean loss: 883.60
 ---- batch: 080 ----
mean loss: 878.41
 ---- batch: 090 ----
mean loss: 869.08
train mean loss: 874.90
epoch train time: 0:00:00.511147
elapsed time: 0:00:46.542388
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 23:26:12.828963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.27
 ---- batch: 020 ----
mean loss: 873.48
 ---- batch: 030 ----
mean loss: 871.78
 ---- batch: 040 ----
mean loss: 872.86
 ---- batch: 050 ----
mean loss: 864.73
 ---- batch: 060 ----
mean loss: 869.84
 ---- batch: 070 ----
mean loss: 879.61
 ---- batch: 080 ----
mean loss: 887.27
 ---- batch: 090 ----
mean loss: 888.06
train mean loss: 875.11
epoch train time: 0:00:00.508100
elapsed time: 0:00:47.050648
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 23:26:13.337212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.94
 ---- batch: 020 ----
mean loss: 869.18
 ---- batch: 030 ----
mean loss: 878.10
 ---- batch: 040 ----
mean loss: 885.39
 ---- batch: 050 ----
mean loss: 873.31
 ---- batch: 060 ----
mean loss: 871.07
 ---- batch: 070 ----
mean loss: 862.60
 ---- batch: 080 ----
mean loss: 892.13
 ---- batch: 090 ----
mean loss: 862.39
train mean loss: 874.89
epoch train time: 0:00:00.496442
elapsed time: 0:00:47.547239
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 23:26:13.833798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.86
 ---- batch: 020 ----
mean loss: 874.97
 ---- batch: 030 ----
mean loss: 861.88
 ---- batch: 040 ----
mean loss: 874.73
 ---- batch: 050 ----
mean loss: 883.70
 ---- batch: 060 ----
mean loss: 883.33
 ---- batch: 070 ----
mean loss: 889.99
 ---- batch: 080 ----
mean loss: 863.96
 ---- batch: 090 ----
mean loss: 859.30
train mean loss: 875.83
epoch train time: 0:00:00.503275
elapsed time: 0:00:48.050648
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 23:26:14.337217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.98
 ---- batch: 020 ----
mean loss: 886.47
 ---- batch: 030 ----
mean loss: 870.55
 ---- batch: 040 ----
mean loss: 876.61
 ---- batch: 050 ----
mean loss: 875.43
 ---- batch: 060 ----
mean loss: 878.11
 ---- batch: 070 ----
mean loss: 872.73
 ---- batch: 080 ----
mean loss: 870.15
 ---- batch: 090 ----
mean loss: 857.33
train mean loss: 874.97
epoch train time: 0:00:00.514382
elapsed time: 0:00:48.565177
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 23:26:14.851764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.88
 ---- batch: 020 ----
mean loss: 871.10
 ---- batch: 030 ----
mean loss: 868.37
 ---- batch: 040 ----
mean loss: 878.46
 ---- batch: 050 ----
mean loss: 892.61
 ---- batch: 060 ----
mean loss: 866.61
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 868.92
 ---- batch: 090 ----
mean loss: 861.10
train mean loss: 875.07
epoch train time: 0:00:00.505207
elapsed time: 0:00:49.070545
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 23:26:15.357113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.11
 ---- batch: 020 ----
mean loss: 879.14
 ---- batch: 030 ----
mean loss: 874.04
 ---- batch: 040 ----
mean loss: 868.00
 ---- batch: 050 ----
mean loss: 876.37
 ---- batch: 060 ----
mean loss: 886.70
 ---- batch: 070 ----
mean loss: 861.68
 ---- batch: 080 ----
mean loss: 882.78
 ---- batch: 090 ----
mean loss: 870.74
train mean loss: 874.91
epoch train time: 0:00:00.507975
elapsed time: 0:00:49.578677
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 23:26:15.865237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.46
 ---- batch: 020 ----
mean loss: 876.58
 ---- batch: 030 ----
mean loss: 877.96
 ---- batch: 040 ----
mean loss: 870.24
 ---- batch: 050 ----
mean loss: 869.36
 ---- batch: 060 ----
mean loss: 868.26
 ---- batch: 070 ----
mean loss: 868.20
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 877.78
train mean loss: 876.12
epoch train time: 0:00:00.513056
elapsed time: 0:00:50.091873
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 23:26:16.378439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.88
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 866.29
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 889.38
 ---- batch: 060 ----
mean loss: 879.60
 ---- batch: 070 ----
mean loss: 884.78
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 873.48
train mean loss: 875.11
epoch train time: 0:00:00.507588
elapsed time: 0:00:50.599617
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 23:26:16.886177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.81
 ---- batch: 020 ----
mean loss: 872.41
 ---- batch: 030 ----
mean loss: 858.52
 ---- batch: 040 ----
mean loss: 878.56
 ---- batch: 050 ----
mean loss: 870.22
 ---- batch: 060 ----
mean loss: 889.60
 ---- batch: 070 ----
mean loss: 878.83
 ---- batch: 080 ----
mean loss: 875.94
 ---- batch: 090 ----
mean loss: 887.80
train mean loss: 875.70
epoch train time: 0:00:00.506706
elapsed time: 0:00:51.106477
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 23:26:17.393068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 870.46
 ---- batch: 030 ----
mean loss: 875.57
 ---- batch: 040 ----
mean loss: 886.46
 ---- batch: 050 ----
mean loss: 888.80
 ---- batch: 060 ----
mean loss: 856.43
 ---- batch: 070 ----
mean loss: 861.54
 ---- batch: 080 ----
mean loss: 865.08
 ---- batch: 090 ----
mean loss: 908.22
train mean loss: 875.33
epoch train time: 0:00:00.508382
elapsed time: 0:00:51.615035
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 23:26:17.901634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.66
 ---- batch: 020 ----
mean loss: 867.37
 ---- batch: 030 ----
mean loss: 881.86
 ---- batch: 040 ----
mean loss: 896.14
 ---- batch: 050 ----
mean loss: 895.66
 ---- batch: 060 ----
mean loss: 872.61
 ---- batch: 070 ----
mean loss: 871.27
 ---- batch: 080 ----
mean loss: 850.89
 ---- batch: 090 ----
mean loss: 868.18
train mean loss: 874.80
epoch train time: 0:00:00.504995
elapsed time: 0:00:52.120224
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 23:26:18.406791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 879.71
 ---- batch: 040 ----
mean loss: 880.95
 ---- batch: 050 ----
mean loss: 865.29
 ---- batch: 060 ----
mean loss: 882.77
 ---- batch: 070 ----
mean loss: 869.99
 ---- batch: 080 ----
mean loss: 882.00
 ---- batch: 090 ----
mean loss: 857.67
train mean loss: 875.43
epoch train time: 0:00:00.517152
elapsed time: 0:00:52.637528
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 23:26:18.924124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 874.52
 ---- batch: 030 ----
mean loss: 869.89
 ---- batch: 040 ----
mean loss: 872.46
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 884.13
 ---- batch: 070 ----
mean loss: 882.33
 ---- batch: 080 ----
mean loss: 868.00
 ---- batch: 090 ----
mean loss: 884.71
train mean loss: 875.94
epoch train time: 0:00:00.500432
elapsed time: 0:00:53.138130
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 23:26:19.424695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.36
 ---- batch: 020 ----
mean loss: 863.17
 ---- batch: 030 ----
mean loss: 868.58
 ---- batch: 040 ----
mean loss: 872.91
 ---- batch: 050 ----
mean loss: 875.23
 ---- batch: 060 ----
mean loss: 874.13
 ---- batch: 070 ----
mean loss: 875.79
 ---- batch: 080 ----
mean loss: 872.88
 ---- batch: 090 ----
mean loss: 880.58
train mean loss: 875.20
epoch train time: 0:00:00.503192
elapsed time: 0:00:53.641471
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 23:26:19.928038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
 ---- batch: 020 ----
mean loss: 888.19
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 864.90
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 877.49
 ---- batch: 070 ----
mean loss: 873.63
 ---- batch: 080 ----
mean loss: 861.95
 ---- batch: 090 ----
mean loss: 867.30
train mean loss: 874.55
epoch train time: 0:00:00.512261
elapsed time: 0:00:54.153878
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 23:26:20.440458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.19
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 878.84
 ---- batch: 040 ----
mean loss: 868.87
 ---- batch: 050 ----
mean loss: 860.44
 ---- batch: 060 ----
mean loss: 885.18
 ---- batch: 070 ----
mean loss: 876.98
 ---- batch: 080 ----
mean loss: 875.13
 ---- batch: 090 ----
mean loss: 883.68
train mean loss: 875.45
epoch train time: 0:00:00.512099
elapsed time: 0:00:54.666133
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 23:26:20.952697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.89
 ---- batch: 020 ----
mean loss: 862.14
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 852.48
 ---- batch: 060 ----
mean loss: 873.50
 ---- batch: 070 ----
mean loss: 889.67
 ---- batch: 080 ----
mean loss: 893.95
 ---- batch: 090 ----
mean loss: 900.18
train mean loss: 875.66
epoch train time: 0:00:00.496452
elapsed time: 0:00:55.162740
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 23:26:21.449320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.25
 ---- batch: 020 ----
mean loss: 859.42
 ---- batch: 030 ----
mean loss: 880.51
 ---- batch: 040 ----
mean loss: 889.92
 ---- batch: 050 ----
mean loss: 869.13
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 886.11
 ---- batch: 090 ----
mean loss: 885.30
train mean loss: 875.24
epoch train time: 0:00:00.501859
elapsed time: 0:00:55.664756
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 23:26:21.951322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.92
 ---- batch: 020 ----
mean loss: 896.00
 ---- batch: 030 ----
mean loss: 895.89
 ---- batch: 040 ----
mean loss: 875.86
 ---- batch: 050 ----
mean loss: 854.48
 ---- batch: 060 ----
mean loss: 885.08
 ---- batch: 070 ----
mean loss: 872.57
 ---- batch: 080 ----
mean loss: 874.54
 ---- batch: 090 ----
mean loss: 870.23
train mean loss: 875.66
epoch train time: 0:00:00.498981
elapsed time: 0:00:56.163880
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 23:26:22.450461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.48
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 875.70
 ---- batch: 040 ----
mean loss: 877.24
 ---- batch: 050 ----
mean loss: 878.31
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.63
 ---- batch: 080 ----
mean loss: 876.96
 ---- batch: 090 ----
mean loss: 864.58
train mean loss: 874.88
epoch train time: 0:00:00.501617
elapsed time: 0:00:56.665653
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 23:26:22.952220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 874.57
 ---- batch: 030 ----
mean loss: 885.12
 ---- batch: 040 ----
mean loss: 870.30
 ---- batch: 050 ----
mean loss: 885.77
 ---- batch: 060 ----
mean loss: 877.45
 ---- batch: 070 ----
mean loss: 893.99
 ---- batch: 080 ----
mean loss: 865.69
 ---- batch: 090 ----
mean loss: 871.94
train mean loss: 875.10
epoch train time: 0:00:00.494303
elapsed time: 0:00:57.160100
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 23:26:23.446681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.71
 ---- batch: 020 ----
mean loss: 867.95
 ---- batch: 030 ----
mean loss: 868.71
 ---- batch: 040 ----
mean loss: 880.39
 ---- batch: 050 ----
mean loss: 874.64
 ---- batch: 060 ----
mean loss: 876.82
 ---- batch: 070 ----
mean loss: 871.95
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 896.57
train mean loss: 874.86
epoch train time: 0:00:00.501221
elapsed time: 0:00:57.661481
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 23:26:23.948047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.54
 ---- batch: 020 ----
mean loss: 874.88
 ---- batch: 030 ----
mean loss: 842.41
 ---- batch: 040 ----
mean loss: 870.90
 ---- batch: 050 ----
mean loss: 879.94
 ---- batch: 060 ----
mean loss: 870.33
 ---- batch: 070 ----
mean loss: 893.30
 ---- batch: 080 ----
mean loss: 874.30
 ---- batch: 090 ----
mean loss: 901.59
train mean loss: 874.94
epoch train time: 0:00:00.504897
elapsed time: 0:00:58.166517
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 23:26:24.453082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.79
 ---- batch: 020 ----
mean loss: 885.95
 ---- batch: 030 ----
mean loss: 869.68
 ---- batch: 040 ----
mean loss: 881.87
 ---- batch: 050 ----
mean loss: 866.18
 ---- batch: 060 ----
mean loss: 869.62
 ---- batch: 070 ----
mean loss: 865.49
 ---- batch: 080 ----
mean loss: 885.13
 ---- batch: 090 ----
mean loss: 878.95
train mean loss: 874.97
epoch train time: 0:00:00.521913
elapsed time: 0:00:58.688578
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 23:26:24.975144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.61
 ---- batch: 020 ----
mean loss: 893.23
 ---- batch: 030 ----
mean loss: 872.99
 ---- batch: 040 ----
mean loss: 888.75
 ---- batch: 050 ----
mean loss: 867.76
 ---- batch: 060 ----
mean loss: 869.12
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 858.03
 ---- batch: 090 ----
mean loss: 875.46
train mean loss: 875.16
epoch train time: 0:00:00.504585
elapsed time: 0:00:59.193305
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 23:26:25.479871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.39
 ---- batch: 020 ----
mean loss: 873.37
 ---- batch: 030 ----
mean loss: 870.53
 ---- batch: 040 ----
mean loss: 862.73
 ---- batch: 050 ----
mean loss: 871.28
 ---- batch: 060 ----
mean loss: 875.54
 ---- batch: 070 ----
mean loss: 898.47
 ---- batch: 080 ----
mean loss: 874.15
 ---- batch: 090 ----
mean loss: 886.52
train mean loss: 875.37
epoch train time: 0:00:00.504578
elapsed time: 0:00:59.698027
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 23:26:25.984594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.00
 ---- batch: 020 ----
mean loss: 853.34
 ---- batch: 030 ----
mean loss: 821.26
 ---- batch: 040 ----
mean loss: 794.84
 ---- batch: 050 ----
mean loss: 759.92
 ---- batch: 060 ----
mean loss: 739.41
 ---- batch: 070 ----
mean loss: 705.33
 ---- batch: 080 ----
mean loss: 668.48
 ---- batch: 090 ----
mean loss: 600.17
train mean loss: 744.43
epoch train time: 0:00:00.502235
elapsed time: 0:01:00.200432
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 23:26:26.487014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.99
 ---- batch: 020 ----
mean loss: 448.48
 ---- batch: 030 ----
mean loss: 438.39
 ---- batch: 040 ----
mean loss: 413.70
 ---- batch: 050 ----
mean loss: 404.58
 ---- batch: 060 ----
mean loss: 375.74
 ---- batch: 070 ----
mean loss: 368.91
 ---- batch: 080 ----
mean loss: 369.36
 ---- batch: 090 ----
mean loss: 348.11
train mean loss: 404.29
epoch train time: 0:00:00.509356
elapsed time: 0:01:00.709966
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 23:26:26.996535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.09
 ---- batch: 020 ----
mean loss: 342.77
 ---- batch: 030 ----
mean loss: 331.29
 ---- batch: 040 ----
mean loss: 319.95
 ---- batch: 050 ----
mean loss: 309.28
 ---- batch: 060 ----
mean loss: 315.52
 ---- batch: 070 ----
mean loss: 319.53
 ---- batch: 080 ----
mean loss: 297.88
 ---- batch: 090 ----
mean loss: 307.40
train mean loss: 321.28
epoch train time: 0:00:00.514535
elapsed time: 0:01:01.224662
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 23:26:27.511228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.36
 ---- batch: 020 ----
mean loss: 281.12
 ---- batch: 030 ----
mean loss: 283.34
 ---- batch: 040 ----
mean loss: 281.88
 ---- batch: 050 ----
mean loss: 281.34
 ---- batch: 060 ----
mean loss: 280.44
 ---- batch: 070 ----
mean loss: 285.48
 ---- batch: 080 ----
mean loss: 261.87
 ---- batch: 090 ----
mean loss: 259.51
train mean loss: 278.87
epoch train time: 0:00:00.510442
elapsed time: 0:01:01.735250
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 23:26:28.021832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.08
 ---- batch: 020 ----
mean loss: 260.10
 ---- batch: 030 ----
mean loss: 261.92
 ---- batch: 040 ----
mean loss: 262.68
 ---- batch: 050 ----
mean loss: 257.41
 ---- batch: 060 ----
mean loss: 258.73
 ---- batch: 070 ----
mean loss: 257.28
 ---- batch: 080 ----
mean loss: 255.52
 ---- batch: 090 ----
mean loss: 255.77
train mean loss: 258.60
epoch train time: 0:00:00.500683
elapsed time: 0:01:02.236093
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 23:26:28.522661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.54
 ---- batch: 020 ----
mean loss: 251.14
 ---- batch: 030 ----
mean loss: 247.04
 ---- batch: 040 ----
mean loss: 244.63
 ---- batch: 050 ----
mean loss: 250.92
 ---- batch: 060 ----
mean loss: 249.33
 ---- batch: 070 ----
mean loss: 239.26
 ---- batch: 080 ----
mean loss: 241.94
 ---- batch: 090 ----
mean loss: 246.14
train mean loss: 246.17
epoch train time: 0:00:00.521505
elapsed time: 0:01:02.757745
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 23:26:29.044312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.87
 ---- batch: 020 ----
mean loss: 235.58
 ---- batch: 030 ----
mean loss: 238.87
 ---- batch: 040 ----
mean loss: 241.22
 ---- batch: 050 ----
mean loss: 236.96
 ---- batch: 060 ----
mean loss: 238.36
 ---- batch: 070 ----
mean loss: 233.69
 ---- batch: 080 ----
mean loss: 240.13
 ---- batch: 090 ----
mean loss: 243.20
train mean loss: 238.76
epoch train time: 0:00:00.507309
elapsed time: 0:01:03.265201
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 23:26:29.551768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.17
 ---- batch: 020 ----
mean loss: 242.47
 ---- batch: 030 ----
mean loss: 231.19
 ---- batch: 040 ----
mean loss: 229.95
 ---- batch: 050 ----
mean loss: 233.99
 ---- batch: 060 ----
mean loss: 236.04
 ---- batch: 070 ----
mean loss: 232.28
 ---- batch: 080 ----
mean loss: 229.23
 ---- batch: 090 ----
mean loss: 231.85
train mean loss: 233.57
epoch train time: 0:00:00.510607
elapsed time: 0:01:03.775957
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 23:26:30.062544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.50
 ---- batch: 020 ----
mean loss: 233.22
 ---- batch: 030 ----
mean loss: 226.75
 ---- batch: 040 ----
mean loss: 227.38
 ---- batch: 050 ----
mean loss: 220.28
 ---- batch: 060 ----
mean loss: 223.38
 ---- batch: 070 ----
mean loss: 231.63
 ---- batch: 080 ----
mean loss: 231.12
 ---- batch: 090 ----
mean loss: 226.63
train mean loss: 227.68
epoch train time: 0:00:00.513039
elapsed time: 0:01:04.289161
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 23:26:30.575731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.43
 ---- batch: 020 ----
mean loss: 221.40
 ---- batch: 030 ----
mean loss: 215.81
 ---- batch: 040 ----
mean loss: 220.07
 ---- batch: 050 ----
mean loss: 226.63
 ---- batch: 060 ----
mean loss: 231.32
 ---- batch: 070 ----
mean loss: 221.10
 ---- batch: 080 ----
mean loss: 216.40
 ---- batch: 090 ----
mean loss: 233.49
train mean loss: 223.86
epoch train time: 0:00:00.507037
elapsed time: 0:01:04.796374
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 23:26:31.082941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.05
 ---- batch: 020 ----
mean loss: 211.49
 ---- batch: 030 ----
mean loss: 223.31
 ---- batch: 040 ----
mean loss: 210.93
 ---- batch: 050 ----
mean loss: 220.97
 ---- batch: 060 ----
mean loss: 225.72
 ---- batch: 070 ----
mean loss: 228.67
 ---- batch: 080 ----
mean loss: 226.68
 ---- batch: 090 ----
mean loss: 218.68
train mean loss: 220.75
epoch train time: 0:00:00.500389
elapsed time: 0:01:05.296906
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 23:26:31.583473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.72
 ---- batch: 020 ----
mean loss: 220.24
 ---- batch: 030 ----
mean loss: 214.17
 ---- batch: 040 ----
mean loss: 219.66
 ---- batch: 050 ----
mean loss: 226.70
 ---- batch: 060 ----
mean loss: 214.76
 ---- batch: 070 ----
mean loss: 213.22
 ---- batch: 080 ----
mean loss: 214.18
 ---- batch: 090 ----
mean loss: 211.88
train mean loss: 216.69
epoch train time: 0:00:00.504647
elapsed time: 0:01:05.801697
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 23:26:32.088272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.97
 ---- batch: 020 ----
mean loss: 212.16
 ---- batch: 030 ----
mean loss: 219.79
 ---- batch: 040 ----
mean loss: 219.84
 ---- batch: 050 ----
mean loss: 220.01
 ---- batch: 060 ----
mean loss: 217.19
 ---- batch: 070 ----
mean loss: 209.44
 ---- batch: 080 ----
mean loss: 211.97
 ---- batch: 090 ----
mean loss: 211.04
train mean loss: 215.07
epoch train time: 0:00:00.506621
elapsed time: 0:01:06.308469
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 23:26:32.595064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.81
 ---- batch: 020 ----
mean loss: 210.68
 ---- batch: 030 ----
mean loss: 213.04
 ---- batch: 040 ----
mean loss: 210.20
 ---- batch: 050 ----
mean loss: 215.30
 ---- batch: 060 ----
mean loss: 211.60
 ---- batch: 070 ----
mean loss: 208.36
 ---- batch: 080 ----
mean loss: 216.29
 ---- batch: 090 ----
mean loss: 207.88
train mean loss: 211.14
epoch train time: 0:00:00.518131
elapsed time: 0:01:06.826784
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 23:26:33.113357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.32
 ---- batch: 020 ----
mean loss: 203.07
 ---- batch: 030 ----
mean loss: 218.92
 ---- batch: 040 ----
mean loss: 216.99
 ---- batch: 050 ----
mean loss: 213.66
 ---- batch: 060 ----
mean loss: 217.54
 ---- batch: 070 ----
mean loss: 208.17
 ---- batch: 080 ----
mean loss: 207.16
 ---- batch: 090 ----
mean loss: 201.42
train mean loss: 209.63
epoch train time: 0:00:00.518289
elapsed time: 0:01:07.345223
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 23:26:33.631789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.96
 ---- batch: 020 ----
mean loss: 207.29
 ---- batch: 030 ----
mean loss: 200.98
 ---- batch: 040 ----
mean loss: 198.88
 ---- batch: 050 ----
mean loss: 200.74
 ---- batch: 060 ----
mean loss: 201.26
 ---- batch: 070 ----
mean loss: 206.25
 ---- batch: 080 ----
mean loss: 210.85
 ---- batch: 090 ----
mean loss: 202.68
train mean loss: 204.23
epoch train time: 0:00:00.524961
elapsed time: 0:01:07.870332
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 23:26:34.156922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.95
 ---- batch: 020 ----
mean loss: 207.75
 ---- batch: 030 ----
mean loss: 193.89
 ---- batch: 040 ----
mean loss: 200.39
 ---- batch: 050 ----
mean loss: 195.46
 ---- batch: 060 ----
mean loss: 200.78
 ---- batch: 070 ----
mean loss: 212.02
 ---- batch: 080 ----
mean loss: 202.41
 ---- batch: 090 ----
mean loss: 205.91
train mean loss: 202.66
epoch train time: 0:00:00.521171
elapsed time: 0:01:08.391684
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 23:26:34.678252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.60
 ---- batch: 020 ----
mean loss: 196.69
 ---- batch: 030 ----
mean loss: 194.77
 ---- batch: 040 ----
mean loss: 199.97
 ---- batch: 050 ----
mean loss: 199.27
 ---- batch: 060 ----
mean loss: 197.40
 ---- batch: 070 ----
mean loss: 190.85
 ---- batch: 080 ----
mean loss: 206.61
 ---- batch: 090 ----
mean loss: 205.02
train mean loss: 200.62
epoch train time: 0:00:00.527337
elapsed time: 0:01:08.919192
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 23:26:35.205781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.11
 ---- batch: 020 ----
mean loss: 189.43
 ---- batch: 030 ----
mean loss: 196.62
 ---- batch: 040 ----
mean loss: 206.69
 ---- batch: 050 ----
mean loss: 197.93
 ---- batch: 060 ----
mean loss: 197.78
 ---- batch: 070 ----
mean loss: 199.64
 ---- batch: 080 ----
mean loss: 196.09
 ---- batch: 090 ----
mean loss: 197.59
train mean loss: 197.14
epoch train time: 0:00:00.526224
elapsed time: 0:01:09.445590
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 23:26:35.732163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.66
 ---- batch: 020 ----
mean loss: 193.52
 ---- batch: 030 ----
mean loss: 195.88
 ---- batch: 040 ----
mean loss: 199.73
 ---- batch: 050 ----
mean loss: 198.32
 ---- batch: 060 ----
mean loss: 201.22
 ---- batch: 070 ----
mean loss: 193.68
 ---- batch: 080 ----
mean loss: 198.63
 ---- batch: 090 ----
mean loss: 200.80
train mean loss: 197.27
epoch train time: 0:00:00.523386
elapsed time: 0:01:09.969128
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 23:26:36.255699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.41
 ---- batch: 020 ----
mean loss: 181.46
 ---- batch: 030 ----
mean loss: 196.32
 ---- batch: 040 ----
mean loss: 194.34
 ---- batch: 050 ----
mean loss: 203.53
 ---- batch: 060 ----
mean loss: 197.90
 ---- batch: 070 ----
mean loss: 196.45
 ---- batch: 080 ----
mean loss: 203.01
 ---- batch: 090 ----
mean loss: 197.61
train mean loss: 194.77
epoch train time: 0:00:00.523866
elapsed time: 0:01:10.493145
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 23:26:36.779715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.36
 ---- batch: 020 ----
mean loss: 191.83
 ---- batch: 030 ----
mean loss: 197.69
 ---- batch: 040 ----
mean loss: 189.62
 ---- batch: 050 ----
mean loss: 193.34
 ---- batch: 060 ----
mean loss: 195.81
 ---- batch: 070 ----
mean loss: 188.68
 ---- batch: 080 ----
mean loss: 197.77
 ---- batch: 090 ----
mean loss: 195.24
train mean loss: 193.17
epoch train time: 0:00:00.534849
elapsed time: 0:01:11.028146
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 23:26:37.314713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.92
 ---- batch: 020 ----
mean loss: 193.66
 ---- batch: 030 ----
mean loss: 189.08
 ---- batch: 040 ----
mean loss: 193.47
 ---- batch: 050 ----
mean loss: 192.36
 ---- batch: 060 ----
mean loss: 194.61
 ---- batch: 070 ----
mean loss: 190.36
 ---- batch: 080 ----
mean loss: 188.89
 ---- batch: 090 ----
mean loss: 198.91
train mean loss: 192.89
epoch train time: 0:00:00.524533
elapsed time: 0:01:11.552824
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 23:26:37.839391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.99
 ---- batch: 020 ----
mean loss: 187.84
 ---- batch: 030 ----
mean loss: 191.40
 ---- batch: 040 ----
mean loss: 197.28
 ---- batch: 050 ----
mean loss: 189.38
 ---- batch: 060 ----
mean loss: 186.30
 ---- batch: 070 ----
mean loss: 196.86
 ---- batch: 080 ----
mean loss: 190.82
 ---- batch: 090 ----
mean loss: 189.71
train mean loss: 190.37
epoch train time: 0:00:00.513219
elapsed time: 0:01:12.066222
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 23:26:38.352813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.39
 ---- batch: 020 ----
mean loss: 188.40
 ---- batch: 030 ----
mean loss: 178.01
 ---- batch: 040 ----
mean loss: 184.58
 ---- batch: 050 ----
mean loss: 195.81
 ---- batch: 060 ----
mean loss: 188.05
 ---- batch: 070 ----
mean loss: 194.87
 ---- batch: 080 ----
mean loss: 192.15
 ---- batch: 090 ----
mean loss: 189.95
train mean loss: 188.85
epoch train time: 0:00:00.513257
elapsed time: 0:01:12.579677
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 23:26:38.866259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.03
 ---- batch: 020 ----
mean loss: 187.26
 ---- batch: 030 ----
mean loss: 187.10
 ---- batch: 040 ----
mean loss: 193.08
 ---- batch: 050 ----
mean loss: 177.87
 ---- batch: 060 ----
mean loss: 183.34
 ---- batch: 070 ----
mean loss: 189.21
 ---- batch: 080 ----
mean loss: 189.11
 ---- batch: 090 ----
mean loss: 186.04
train mean loss: 186.08
epoch train time: 0:00:00.514295
elapsed time: 0:01:13.094152
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 23:26:39.380720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.11
 ---- batch: 020 ----
mean loss: 181.55
 ---- batch: 030 ----
mean loss: 181.49
 ---- batch: 040 ----
mean loss: 194.97
 ---- batch: 050 ----
mean loss: 185.27
 ---- batch: 060 ----
mean loss: 180.13
 ---- batch: 070 ----
mean loss: 198.81
 ---- batch: 080 ----
mean loss: 187.22
 ---- batch: 090 ----
mean loss: 188.66
train mean loss: 185.71
epoch train time: 0:00:00.519186
elapsed time: 0:01:13.613482
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 23:26:39.900047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.01
 ---- batch: 020 ----
mean loss: 184.30
 ---- batch: 030 ----
mean loss: 179.12
 ---- batch: 040 ----
mean loss: 192.89
 ---- batch: 050 ----
mean loss: 186.48
 ---- batch: 060 ----
mean loss: 190.10
 ---- batch: 070 ----
mean loss: 180.25
 ---- batch: 080 ----
mean loss: 185.86
 ---- batch: 090 ----
mean loss: 185.37
train mean loss: 184.51
epoch train time: 0:00:00.511929
elapsed time: 0:01:14.125564
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 23:26:40.412147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.22
 ---- batch: 020 ----
mean loss: 177.26
 ---- batch: 030 ----
mean loss: 178.67
 ---- batch: 040 ----
mean loss: 181.48
 ---- batch: 050 ----
mean loss: 186.96
 ---- batch: 060 ----
mean loss: 182.55
 ---- batch: 070 ----
mean loss: 184.78
 ---- batch: 080 ----
mean loss: 183.53
 ---- batch: 090 ----
mean loss: 186.02
train mean loss: 182.97
epoch train time: 0:00:00.528731
elapsed time: 0:01:14.654457
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 23:26:40.941038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.00
 ---- batch: 020 ----
mean loss: 179.95
 ---- batch: 030 ----
mean loss: 189.89
 ---- batch: 040 ----
mean loss: 182.78
 ---- batch: 050 ----
mean loss: 183.13
 ---- batch: 060 ----
mean loss: 177.18
 ---- batch: 070 ----
mean loss: 180.43
 ---- batch: 080 ----
mean loss: 179.86
 ---- batch: 090 ----
mean loss: 182.61
train mean loss: 181.42
epoch train time: 0:00:00.525154
elapsed time: 0:01:15.179772
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 23:26:41.466340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.55
 ---- batch: 020 ----
mean loss: 177.99
 ---- batch: 030 ----
mean loss: 173.32
 ---- batch: 040 ----
mean loss: 179.72
 ---- batch: 050 ----
mean loss: 188.94
 ---- batch: 060 ----
mean loss: 184.53
 ---- batch: 070 ----
mean loss: 176.27
 ---- batch: 080 ----
mean loss: 188.24
 ---- batch: 090 ----
mean loss: 179.36
train mean loss: 181.09
epoch train time: 0:00:00.538510
elapsed time: 0:01:15.718427
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 23:26:42.005056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.13
 ---- batch: 020 ----
mean loss: 172.24
 ---- batch: 030 ----
mean loss: 182.11
 ---- batch: 040 ----
mean loss: 175.99
 ---- batch: 050 ----
mean loss: 184.96
 ---- batch: 060 ----
mean loss: 187.18
 ---- batch: 070 ----
mean loss: 176.64
 ---- batch: 080 ----
mean loss: 182.23
 ---- batch: 090 ----
mean loss: 185.39
train mean loss: 180.60
epoch train time: 0:00:00.514034
elapsed time: 0:01:16.232665
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 23:26:42.519232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.10
 ---- batch: 020 ----
mean loss: 175.57
 ---- batch: 030 ----
mean loss: 176.43
 ---- batch: 040 ----
mean loss: 173.72
 ---- batch: 050 ----
mean loss: 180.97
 ---- batch: 060 ----
mean loss: 189.26
 ---- batch: 070 ----
mean loss: 171.00
 ---- batch: 080 ----
mean loss: 179.11
 ---- batch: 090 ----
mean loss: 190.04
train mean loss: 179.57
epoch train time: 0:00:00.542403
elapsed time: 0:01:16.775214
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 23:26:43.061782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.76
 ---- batch: 020 ----
mean loss: 183.96
 ---- batch: 030 ----
mean loss: 172.38
 ---- batch: 040 ----
mean loss: 182.19
 ---- batch: 050 ----
mean loss: 179.03
 ---- batch: 060 ----
mean loss: 175.10
 ---- batch: 070 ----
mean loss: 172.40
 ---- batch: 080 ----
mean loss: 178.87
 ---- batch: 090 ----
mean loss: 182.73
train mean loss: 178.74
epoch train time: 0:00:00.503632
elapsed time: 0:01:17.278988
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 23:26:43.565554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.36
 ---- batch: 020 ----
mean loss: 176.21
 ---- batch: 030 ----
mean loss: 179.01
 ---- batch: 040 ----
mean loss: 181.65
 ---- batch: 050 ----
mean loss: 174.21
 ---- batch: 060 ----
mean loss: 178.03
 ---- batch: 070 ----
mean loss: 175.59
 ---- batch: 080 ----
mean loss: 179.53
 ---- batch: 090 ----
mean loss: 166.94
train mean loss: 175.95
epoch train time: 0:00:00.510908
elapsed time: 0:01:17.790041
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 23:26:44.076608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.71
 ---- batch: 020 ----
mean loss: 172.78
 ---- batch: 030 ----
mean loss: 172.44
 ---- batch: 040 ----
mean loss: 176.70
 ---- batch: 050 ----
mean loss: 165.69
 ---- batch: 060 ----
mean loss: 176.62
 ---- batch: 070 ----
mean loss: 177.31
 ---- batch: 080 ----
mean loss: 177.83
 ---- batch: 090 ----
mean loss: 184.57
train mean loss: 175.67
epoch train time: 0:00:00.513692
elapsed time: 0:01:18.303894
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 23:26:44.590479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.06
 ---- batch: 020 ----
mean loss: 175.99
 ---- batch: 030 ----
mean loss: 173.47
 ---- batch: 040 ----
mean loss: 174.94
 ---- batch: 050 ----
mean loss: 169.54
 ---- batch: 060 ----
mean loss: 181.20
 ---- batch: 070 ----
mean loss: 174.08
 ---- batch: 080 ----
mean loss: 179.70
 ---- batch: 090 ----
mean loss: 174.90
train mean loss: 175.84
epoch train time: 0:00:00.516077
elapsed time: 0:01:18.820148
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 23:26:45.106725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.98
 ---- batch: 020 ----
mean loss: 174.71
 ---- batch: 030 ----
mean loss: 176.79
 ---- batch: 040 ----
mean loss: 180.67
 ---- batch: 050 ----
mean loss: 178.69
 ---- batch: 060 ----
mean loss: 175.64
 ---- batch: 070 ----
mean loss: 176.36
 ---- batch: 080 ----
mean loss: 177.36
 ---- batch: 090 ----
mean loss: 176.32
train mean loss: 175.83
epoch train time: 0:00:00.506447
elapsed time: 0:01:19.326749
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 23:26:45.613320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.70
 ---- batch: 020 ----
mean loss: 169.48
 ---- batch: 030 ----
mean loss: 165.70
 ---- batch: 040 ----
mean loss: 172.04
 ---- batch: 050 ----
mean loss: 175.70
 ---- batch: 060 ----
mean loss: 180.20
 ---- batch: 070 ----
mean loss: 169.39
 ---- batch: 080 ----
mean loss: 180.12
 ---- batch: 090 ----
mean loss: 172.92
train mean loss: 173.25
epoch train time: 0:00:00.509088
elapsed time: 0:01:19.835990
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 23:26:46.122574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.18
 ---- batch: 020 ----
mean loss: 177.49
 ---- batch: 030 ----
mean loss: 167.02
 ---- batch: 040 ----
mean loss: 167.40
 ---- batch: 050 ----
mean loss: 174.77
 ---- batch: 060 ----
mean loss: 179.65
 ---- batch: 070 ----
mean loss: 173.55
 ---- batch: 080 ----
mean loss: 176.33
 ---- batch: 090 ----
mean loss: 172.40
train mean loss: 173.15
epoch train time: 0:00:00.509286
elapsed time: 0:01:20.345437
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 23:26:46.632016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.40
 ---- batch: 020 ----
mean loss: 174.99
 ---- batch: 030 ----
mean loss: 170.45
 ---- batch: 040 ----
mean loss: 168.08
 ---- batch: 050 ----
mean loss: 164.04
 ---- batch: 060 ----
mean loss: 175.86
 ---- batch: 070 ----
mean loss: 169.38
 ---- batch: 080 ----
mean loss: 181.68
 ---- batch: 090 ----
mean loss: 173.15
train mean loss: 171.89
epoch train time: 0:00:00.529504
elapsed time: 0:01:20.875119
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 23:26:47.161690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.40
 ---- batch: 020 ----
mean loss: 173.39
 ---- batch: 030 ----
mean loss: 170.96
 ---- batch: 040 ----
mean loss: 172.57
 ---- batch: 050 ----
mean loss: 172.26
 ---- batch: 060 ----
mean loss: 180.17
 ---- batch: 070 ----
mean loss: 176.94
 ---- batch: 080 ----
mean loss: 170.52
 ---- batch: 090 ----
mean loss: 168.47
train mean loss: 171.42
epoch train time: 0:00:00.500798
elapsed time: 0:01:21.376096
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 23:26:47.662662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.60
 ---- batch: 020 ----
mean loss: 163.84
 ---- batch: 030 ----
mean loss: 178.19
 ---- batch: 040 ----
mean loss: 164.05
 ---- batch: 050 ----
mean loss: 169.33
 ---- batch: 060 ----
mean loss: 181.18
 ---- batch: 070 ----
mean loss: 172.31
 ---- batch: 080 ----
mean loss: 173.60
 ---- batch: 090 ----
mean loss: 178.06
train mean loss: 171.43
epoch train time: 0:00:00.513111
elapsed time: 0:01:21.889363
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 23:26:48.175929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.00
 ---- batch: 020 ----
mean loss: 165.16
 ---- batch: 030 ----
mean loss: 166.30
 ---- batch: 040 ----
mean loss: 167.06
 ---- batch: 050 ----
mean loss: 178.26
 ---- batch: 060 ----
mean loss: 178.83
 ---- batch: 070 ----
mean loss: 168.83
 ---- batch: 080 ----
mean loss: 172.99
 ---- batch: 090 ----
mean loss: 175.27
train mean loss: 171.72
epoch train time: 0:00:00.504233
elapsed time: 0:01:22.393740
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 23:26:48.680308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.54
 ---- batch: 020 ----
mean loss: 161.83
 ---- batch: 030 ----
mean loss: 165.72
 ---- batch: 040 ----
mean loss: 170.22
 ---- batch: 050 ----
mean loss: 167.66
 ---- batch: 060 ----
mean loss: 168.52
 ---- batch: 070 ----
mean loss: 182.05
 ---- batch: 080 ----
mean loss: 173.09
 ---- batch: 090 ----
mean loss: 169.81
train mean loss: 169.54
epoch train time: 0:00:00.506969
elapsed time: 0:01:22.900853
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 23:26:49.187434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.25
 ---- batch: 020 ----
mean loss: 164.40
 ---- batch: 030 ----
mean loss: 167.61
 ---- batch: 040 ----
mean loss: 171.18
 ---- batch: 050 ----
mean loss: 168.98
 ---- batch: 060 ----
mean loss: 166.34
 ---- batch: 070 ----
mean loss: 171.62
 ---- batch: 080 ----
mean loss: 169.58
 ---- batch: 090 ----
mean loss: 175.07
train mean loss: 168.79
epoch train time: 0:00:00.500028
elapsed time: 0:01:23.401050
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 23:26:49.687637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.96
 ---- batch: 020 ----
mean loss: 169.36
 ---- batch: 030 ----
mean loss: 165.32
 ---- batch: 040 ----
mean loss: 165.53
 ---- batch: 050 ----
mean loss: 167.55
 ---- batch: 060 ----
mean loss: 172.14
 ---- batch: 070 ----
mean loss: 173.13
 ---- batch: 080 ----
mean loss: 170.78
 ---- batch: 090 ----
mean loss: 165.64
train mean loss: 168.23
epoch train time: 0:00:00.501093
elapsed time: 0:01:23.902308
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 23:26:50.188895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.08
 ---- batch: 020 ----
mean loss: 159.14
 ---- batch: 030 ----
mean loss: 164.88
 ---- batch: 040 ----
mean loss: 161.40
 ---- batch: 050 ----
mean loss: 175.57
 ---- batch: 060 ----
mean loss: 166.83
 ---- batch: 070 ----
mean loss: 165.07
 ---- batch: 080 ----
mean loss: 173.75
 ---- batch: 090 ----
mean loss: 169.95
train mean loss: 167.84
epoch train time: 0:00:00.502974
elapsed time: 0:01:24.405470
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 23:26:50.692047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.62
 ---- batch: 020 ----
mean loss: 160.50
 ---- batch: 030 ----
mean loss: 160.78
 ---- batch: 040 ----
mean loss: 168.71
 ---- batch: 050 ----
mean loss: 171.60
 ---- batch: 060 ----
mean loss: 159.25
 ---- batch: 070 ----
mean loss: 165.18
 ---- batch: 080 ----
mean loss: 168.40
 ---- batch: 090 ----
mean loss: 172.51
train mean loss: 166.40
epoch train time: 0:00:00.513124
elapsed time: 0:01:24.918775
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 23:26:51.205341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.82
 ---- batch: 020 ----
mean loss: 164.02
 ---- batch: 030 ----
mean loss: 166.09
 ---- batch: 040 ----
mean loss: 168.29
 ---- batch: 050 ----
mean loss: 173.82
 ---- batch: 060 ----
mean loss: 170.69
 ---- batch: 070 ----
mean loss: 163.67
 ---- batch: 080 ----
mean loss: 162.25
 ---- batch: 090 ----
mean loss: 162.50
train mean loss: 166.85
epoch train time: 0:00:00.505295
elapsed time: 0:01:25.424224
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 23:26:51.710802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.26
 ---- batch: 020 ----
mean loss: 160.79
 ---- batch: 030 ----
mean loss: 165.95
 ---- batch: 040 ----
mean loss: 164.33
 ---- batch: 050 ----
mean loss: 168.74
 ---- batch: 060 ----
mean loss: 167.51
 ---- batch: 070 ----
mean loss: 166.86
 ---- batch: 080 ----
mean loss: 167.23
 ---- batch: 090 ----
mean loss: 167.42
train mean loss: 166.13
epoch train time: 0:00:00.511406
elapsed time: 0:01:25.935787
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 23:26:52.222353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.37
 ---- batch: 020 ----
mean loss: 167.42
 ---- batch: 030 ----
mean loss: 159.16
 ---- batch: 040 ----
mean loss: 162.35
 ---- batch: 050 ----
mean loss: 162.89
 ---- batch: 060 ----
mean loss: 163.41
 ---- batch: 070 ----
mean loss: 173.47
 ---- batch: 080 ----
mean loss: 157.07
 ---- batch: 090 ----
mean loss: 168.41
train mean loss: 164.62
epoch train time: 0:00:00.523451
elapsed time: 0:01:26.459407
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 23:26:52.745976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.66
 ---- batch: 020 ----
mean loss: 163.78
 ---- batch: 030 ----
mean loss: 156.91
 ---- batch: 040 ----
mean loss: 167.83
 ---- batch: 050 ----
mean loss: 164.17
 ---- batch: 060 ----
mean loss: 169.02
 ---- batch: 070 ----
mean loss: 159.00
 ---- batch: 080 ----
mean loss: 162.34
 ---- batch: 090 ----
mean loss: 159.73
train mean loss: 164.14
epoch train time: 0:00:00.505619
elapsed time: 0:01:26.965170
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 23:26:53.251751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.90
 ---- batch: 020 ----
mean loss: 156.79
 ---- batch: 030 ----
mean loss: 166.86
 ---- batch: 040 ----
mean loss: 157.54
 ---- batch: 050 ----
mean loss: 159.70
 ---- batch: 060 ----
mean loss: 165.38
 ---- batch: 070 ----
mean loss: 168.46
 ---- batch: 080 ----
mean loss: 166.76
 ---- batch: 090 ----
mean loss: 167.77
train mean loss: 163.51
epoch train time: 0:00:00.508046
elapsed time: 0:01:27.473386
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 23:26:53.759999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.78
 ---- batch: 020 ----
mean loss: 165.46
 ---- batch: 030 ----
mean loss: 158.42
 ---- batch: 040 ----
mean loss: 159.94
 ---- batch: 050 ----
mean loss: 170.46
 ---- batch: 060 ----
mean loss: 163.16
 ---- batch: 070 ----
mean loss: 164.85
 ---- batch: 080 ----
mean loss: 160.67
 ---- batch: 090 ----
mean loss: 165.89
train mean loss: 163.84
epoch train time: 0:00:00.512198
elapsed time: 0:01:27.985804
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 23:26:54.272373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.81
 ---- batch: 020 ----
mean loss: 164.24
 ---- batch: 030 ----
mean loss: 161.08
 ---- batch: 040 ----
mean loss: 166.29
 ---- batch: 050 ----
mean loss: 159.53
 ---- batch: 060 ----
mean loss: 160.00
 ---- batch: 070 ----
mean loss: 165.08
 ---- batch: 080 ----
mean loss: 166.77
 ---- batch: 090 ----
mean loss: 164.29
train mean loss: 162.36
epoch train time: 0:00:00.511803
elapsed time: 0:01:28.497762
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 23:26:54.784337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.12
 ---- batch: 020 ----
mean loss: 166.82
 ---- batch: 030 ----
mean loss: 155.20
 ---- batch: 040 ----
mean loss: 162.14
 ---- batch: 050 ----
mean loss: 157.37
 ---- batch: 060 ----
mean loss: 164.62
 ---- batch: 070 ----
mean loss: 165.09
 ---- batch: 080 ----
mean loss: 161.73
 ---- batch: 090 ----
mean loss: 158.14
train mean loss: 162.20
epoch train time: 0:00:00.504088
elapsed time: 0:01:29.001999
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 23:26:55.288573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.22
 ---- batch: 020 ----
mean loss: 159.59
 ---- batch: 030 ----
mean loss: 166.47
 ---- batch: 040 ----
mean loss: 153.10
 ---- batch: 050 ----
mean loss: 165.08
 ---- batch: 060 ----
mean loss: 167.43
 ---- batch: 070 ----
mean loss: 166.20
 ---- batch: 080 ----
mean loss: 162.72
 ---- batch: 090 ----
mean loss: 156.68
train mean loss: 162.14
epoch train time: 0:00:00.508069
elapsed time: 0:01:29.510216
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 23:26:55.796787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.47
 ---- batch: 020 ----
mean loss: 159.89
 ---- batch: 030 ----
mean loss: 161.41
 ---- batch: 040 ----
mean loss: 163.72
 ---- batch: 050 ----
mean loss: 158.23
 ---- batch: 060 ----
mean loss: 169.94
 ---- batch: 070 ----
mean loss: 165.82
 ---- batch: 080 ----
mean loss: 164.04
 ---- batch: 090 ----
mean loss: 163.41
train mean loss: 163.17
epoch train time: 0:00:00.508037
elapsed time: 0:01:30.018399
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 23:26:56.304983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.64
 ---- batch: 020 ----
mean loss: 158.39
 ---- batch: 030 ----
mean loss: 162.98
 ---- batch: 040 ----
mean loss: 164.82
 ---- batch: 050 ----
mean loss: 163.25
 ---- batch: 060 ----
mean loss: 161.56
 ---- batch: 070 ----
mean loss: 166.41
 ---- batch: 080 ----
mean loss: 157.29
 ---- batch: 090 ----
mean loss: 160.18
train mean loss: 160.50
epoch train time: 0:00:00.504813
elapsed time: 0:01:30.523411
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 23:26:56.809994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.76
 ---- batch: 020 ----
mean loss: 163.08
 ---- batch: 030 ----
mean loss: 162.20
 ---- batch: 040 ----
mean loss: 153.93
 ---- batch: 050 ----
mean loss: 157.56
 ---- batch: 060 ----
mean loss: 164.24
 ---- batch: 070 ----
mean loss: 165.18
 ---- batch: 080 ----
mean loss: 159.39
 ---- batch: 090 ----
mean loss: 161.41
train mean loss: 161.05
epoch train time: 0:00:00.502556
elapsed time: 0:01:31.026125
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 23:26:57.312708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.82
 ---- batch: 020 ----
mean loss: 159.60
 ---- batch: 030 ----
mean loss: 159.11
 ---- batch: 040 ----
mean loss: 161.89
 ---- batch: 050 ----
mean loss: 163.80
 ---- batch: 060 ----
mean loss: 163.35
 ---- batch: 070 ----
mean loss: 157.80
 ---- batch: 080 ----
mean loss: 159.06
 ---- batch: 090 ----
mean loss: 158.59
train mean loss: 160.50
epoch train time: 0:00:00.502325
elapsed time: 0:01:31.528612
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 23:26:57.815205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.35
 ---- batch: 020 ----
mean loss: 157.53
 ---- batch: 030 ----
mean loss: 155.24
 ---- batch: 040 ----
mean loss: 157.36
 ---- batch: 050 ----
mean loss: 158.07
 ---- batch: 060 ----
mean loss: 166.01
 ---- batch: 070 ----
mean loss: 164.65
 ---- batch: 080 ----
mean loss: 154.10
 ---- batch: 090 ----
mean loss: 168.98
train mean loss: 160.30
epoch train time: 0:00:00.506277
elapsed time: 0:01:32.035061
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 23:26:58.321659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.40
 ---- batch: 020 ----
mean loss: 156.61
 ---- batch: 030 ----
mean loss: 159.55
 ---- batch: 040 ----
mean loss: 153.17
 ---- batch: 050 ----
mean loss: 164.65
 ---- batch: 060 ----
mean loss: 158.66
 ---- batch: 070 ----
mean loss: 161.49
 ---- batch: 080 ----
mean loss: 155.53
 ---- batch: 090 ----
mean loss: 163.25
train mean loss: 159.06
epoch train time: 0:00:00.515429
elapsed time: 0:01:32.550705
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 23:26:58.837285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.61
 ---- batch: 020 ----
mean loss: 149.47
 ---- batch: 030 ----
mean loss: 156.73
 ---- batch: 040 ----
mean loss: 157.20
 ---- batch: 050 ----
mean loss: 163.46
 ---- batch: 060 ----
mean loss: 163.10
 ---- batch: 070 ----
mean loss: 160.19
 ---- batch: 080 ----
mean loss: 164.36
 ---- batch: 090 ----
mean loss: 168.62
train mean loss: 158.60
epoch train time: 0:00:00.504985
elapsed time: 0:01:33.055848
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 23:26:59.342415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.14
 ---- batch: 020 ----
mean loss: 149.61
 ---- batch: 030 ----
mean loss: 155.56
 ---- batch: 040 ----
mean loss: 154.15
 ---- batch: 050 ----
mean loss: 157.85
 ---- batch: 060 ----
mean loss: 156.18
 ---- batch: 070 ----
mean loss: 159.24
 ---- batch: 080 ----
mean loss: 161.47
 ---- batch: 090 ----
mean loss: 170.34
train mean loss: 158.16
epoch train time: 0:00:00.505049
elapsed time: 0:01:33.561037
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 23:26:59.847604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.58
 ---- batch: 020 ----
mean loss: 156.77
 ---- batch: 030 ----
mean loss: 163.00
 ---- batch: 040 ----
mean loss: 160.11
 ---- batch: 050 ----
mean loss: 162.28
 ---- batch: 060 ----
mean loss: 158.04
 ---- batch: 070 ----
mean loss: 157.01
 ---- batch: 080 ----
mean loss: 160.09
 ---- batch: 090 ----
mean loss: 155.11
train mean loss: 157.86
epoch train time: 0:00:00.505395
elapsed time: 0:01:34.066587
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 23:27:00.353162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.69
 ---- batch: 020 ----
mean loss: 153.75
 ---- batch: 030 ----
mean loss: 153.00
 ---- batch: 040 ----
mean loss: 161.08
 ---- batch: 050 ----
mean loss: 156.95
 ---- batch: 060 ----
mean loss: 161.75
 ---- batch: 070 ----
mean loss: 161.75
 ---- batch: 080 ----
mean loss: 159.44
 ---- batch: 090 ----
mean loss: 156.98
train mean loss: 158.08
epoch train time: 0:00:00.510418
elapsed time: 0:01:34.577157
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 23:27:00.863759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.15
 ---- batch: 020 ----
mean loss: 156.31
 ---- batch: 030 ----
mean loss: 157.33
 ---- batch: 040 ----
mean loss: 158.75
 ---- batch: 050 ----
mean loss: 163.14
 ---- batch: 060 ----
mean loss: 155.35
 ---- batch: 070 ----
mean loss: 149.45
 ---- batch: 080 ----
mean loss: 161.26
 ---- batch: 090 ----
mean loss: 153.35
train mean loss: 156.45
epoch train time: 0:00:00.506745
elapsed time: 0:01:35.084091
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 23:27:01.370678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.03
 ---- batch: 020 ----
mean loss: 152.09
 ---- batch: 030 ----
mean loss: 155.09
 ---- batch: 040 ----
mean loss: 157.68
 ---- batch: 050 ----
mean loss: 154.70
 ---- batch: 060 ----
mean loss: 157.34
 ---- batch: 070 ----
mean loss: 159.28
 ---- batch: 080 ----
mean loss: 165.02
 ---- batch: 090 ----
mean loss: 156.24
train mean loss: 156.61
epoch train time: 0:00:00.505909
elapsed time: 0:01:35.590163
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 23:27:01.876731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.99
 ---- batch: 020 ----
mean loss: 156.96
 ---- batch: 030 ----
mean loss: 155.38
 ---- batch: 040 ----
mean loss: 156.96
 ---- batch: 050 ----
mean loss: 154.48
 ---- batch: 060 ----
mean loss: 156.80
 ---- batch: 070 ----
mean loss: 156.60
 ---- batch: 080 ----
mean loss: 155.21
 ---- batch: 090 ----
mean loss: 155.30
train mean loss: 155.93
epoch train time: 0:00:00.498476
elapsed time: 0:01:36.088785
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 23:27:02.375354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.19
 ---- batch: 020 ----
mean loss: 147.62
 ---- batch: 030 ----
mean loss: 150.21
 ---- batch: 040 ----
mean loss: 153.34
 ---- batch: 050 ----
mean loss: 159.66
 ---- batch: 060 ----
mean loss: 156.92
 ---- batch: 070 ----
mean loss: 156.77
 ---- batch: 080 ----
mean loss: 160.18
 ---- batch: 090 ----
mean loss: 158.60
train mean loss: 155.02
epoch train time: 0:00:00.508039
elapsed time: 0:01:36.596981
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 23:27:02.883551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.63
 ---- batch: 020 ----
mean loss: 154.68
 ---- batch: 030 ----
mean loss: 153.17
 ---- batch: 040 ----
mean loss: 157.71
 ---- batch: 050 ----
mean loss: 151.27
 ---- batch: 060 ----
mean loss: 164.73
 ---- batch: 070 ----
mean loss: 151.66
 ---- batch: 080 ----
mean loss: 157.93
 ---- batch: 090 ----
mean loss: 153.42
train mean loss: 155.87
epoch train time: 0:00:00.507344
elapsed time: 0:01:37.104488
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 23:27:03.391056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.37
 ---- batch: 020 ----
mean loss: 157.62
 ---- batch: 030 ----
mean loss: 151.34
 ---- batch: 040 ----
mean loss: 150.55
 ---- batch: 050 ----
mean loss: 152.80
 ---- batch: 060 ----
mean loss: 156.70
 ---- batch: 070 ----
mean loss: 154.37
 ---- batch: 080 ----
mean loss: 156.56
 ---- batch: 090 ----
mean loss: 163.18
train mean loss: 154.76
epoch train time: 0:00:00.507792
elapsed time: 0:01:37.612477
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 23:27:03.899050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.22
 ---- batch: 020 ----
mean loss: 150.53
 ---- batch: 030 ----
mean loss: 147.99
 ---- batch: 040 ----
mean loss: 155.05
 ---- batch: 050 ----
mean loss: 154.39
 ---- batch: 060 ----
mean loss: 159.48
 ---- batch: 070 ----
mean loss: 153.94
 ---- batch: 080 ----
mean loss: 160.23
 ---- batch: 090 ----
mean loss: 163.28
train mean loss: 154.69
epoch train time: 0:00:00.511653
elapsed time: 0:01:38.124304
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 23:27:04.410862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.65
 ---- batch: 020 ----
mean loss: 154.21
 ---- batch: 030 ----
mean loss: 152.23
 ---- batch: 040 ----
mean loss: 158.24
 ---- batch: 050 ----
mean loss: 159.86
 ---- batch: 060 ----
mean loss: 152.62
 ---- batch: 070 ----
mean loss: 147.84
 ---- batch: 080 ----
mean loss: 152.37
 ---- batch: 090 ----
mean loss: 163.72
train mean loss: 154.41
epoch train time: 0:00:00.515757
elapsed time: 0:01:38.640204
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 23:27:04.926818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.59
 ---- batch: 020 ----
mean loss: 147.43
 ---- batch: 030 ----
mean loss: 146.20
 ---- batch: 040 ----
mean loss: 152.68
 ---- batch: 050 ----
mean loss: 152.07
 ---- batch: 060 ----
mean loss: 150.41
 ---- batch: 070 ----
mean loss: 155.73
 ---- batch: 080 ----
mean loss: 158.82
 ---- batch: 090 ----
mean loss: 159.99
train mean loss: 152.74
epoch train time: 0:00:00.519278
elapsed time: 0:01:39.159681
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 23:27:05.446257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.67
 ---- batch: 020 ----
mean loss: 147.82
 ---- batch: 030 ----
mean loss: 151.38
 ---- batch: 040 ----
mean loss: 154.48
 ---- batch: 050 ----
mean loss: 151.78
 ---- batch: 060 ----
mean loss: 149.49
 ---- batch: 070 ----
mean loss: 152.09
 ---- batch: 080 ----
mean loss: 159.73
 ---- batch: 090 ----
mean loss: 160.12
train mean loss: 153.40
epoch train time: 0:00:00.511942
elapsed time: 0:01:39.671777
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 23:27:05.958348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.17
 ---- batch: 020 ----
mean loss: 152.59
 ---- batch: 030 ----
mean loss: 152.72
 ---- batch: 040 ----
mean loss: 154.27
 ---- batch: 050 ----
mean loss: 152.21
 ---- batch: 060 ----
mean loss: 148.10
 ---- batch: 070 ----
mean loss: 152.88
 ---- batch: 080 ----
mean loss: 151.57
 ---- batch: 090 ----
mean loss: 161.12
train mean loss: 152.81
epoch train time: 0:00:00.502062
elapsed time: 0:01:40.173988
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 23:27:06.460556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.47
 ---- batch: 020 ----
mean loss: 146.63
 ---- batch: 030 ----
mean loss: 157.13
 ---- batch: 040 ----
mean loss: 145.75
 ---- batch: 050 ----
mean loss: 154.35
 ---- batch: 060 ----
mean loss: 154.32
 ---- batch: 070 ----
mean loss: 153.27
 ---- batch: 080 ----
mean loss: 163.07
 ---- batch: 090 ----
mean loss: 153.91
train mean loss: 153.52
epoch train time: 0:00:00.502898
elapsed time: 0:01:40.677035
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 23:27:06.963605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.55
 ---- batch: 020 ----
mean loss: 147.07
 ---- batch: 030 ----
mean loss: 153.24
 ---- batch: 040 ----
mean loss: 151.00
 ---- batch: 050 ----
mean loss: 146.25
 ---- batch: 060 ----
mean loss: 154.28
 ---- batch: 070 ----
mean loss: 153.79
 ---- batch: 080 ----
mean loss: 157.09
 ---- batch: 090 ----
mean loss: 152.91
train mean loss: 152.46
epoch train time: 0:00:00.495427
elapsed time: 0:01:41.172604
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 23:27:07.459169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.27
 ---- batch: 020 ----
mean loss: 143.87
 ---- batch: 030 ----
mean loss: 148.79
 ---- batch: 040 ----
mean loss: 152.36
 ---- batch: 050 ----
mean loss: 151.66
 ---- batch: 060 ----
mean loss: 147.50
 ---- batch: 070 ----
mean loss: 151.97
 ---- batch: 080 ----
mean loss: 158.20
 ---- batch: 090 ----
mean loss: 159.97
train mean loss: 152.09
epoch train time: 0:00:00.511748
elapsed time: 0:01:41.684494
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 23:27:07.971078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.84
 ---- batch: 020 ----
mean loss: 151.60
 ---- batch: 030 ----
mean loss: 147.27
 ---- batch: 040 ----
mean loss: 152.00
 ---- batch: 050 ----
mean loss: 146.07
 ---- batch: 060 ----
mean loss: 157.73
 ---- batch: 070 ----
mean loss: 156.69
 ---- batch: 080 ----
mean loss: 150.25
 ---- batch: 090 ----
mean loss: 147.90
train mean loss: 150.72
epoch train time: 0:00:00.508113
elapsed time: 0:01:42.192766
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 23:27:08.479334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.65
 ---- batch: 020 ----
mean loss: 147.39
 ---- batch: 030 ----
mean loss: 146.00
 ---- batch: 040 ----
mean loss: 149.83
 ---- batch: 050 ----
mean loss: 148.67
 ---- batch: 060 ----
mean loss: 148.98
 ---- batch: 070 ----
mean loss: 150.16
 ---- batch: 080 ----
mean loss: 155.75
 ---- batch: 090 ----
mean loss: 158.30
train mean loss: 151.32
epoch train time: 0:00:00.526191
elapsed time: 0:01:42.719124
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 23:27:09.005717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.11
 ---- batch: 020 ----
mean loss: 146.91
 ---- batch: 030 ----
mean loss: 147.22
 ---- batch: 040 ----
mean loss: 149.83
 ---- batch: 050 ----
mean loss: 152.79
 ---- batch: 060 ----
mean loss: 151.71
 ---- batch: 070 ----
mean loss: 153.66
 ---- batch: 080 ----
mean loss: 151.50
 ---- batch: 090 ----
mean loss: 153.76
train mean loss: 150.82
epoch train time: 0:00:00.500845
elapsed time: 0:01:43.220141
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 23:27:09.506709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.46
 ---- batch: 020 ----
mean loss: 150.36
 ---- batch: 030 ----
mean loss: 150.03
 ---- batch: 040 ----
mean loss: 153.51
 ---- batch: 050 ----
mean loss: 148.52
 ---- batch: 060 ----
mean loss: 157.52
 ---- batch: 070 ----
mean loss: 151.37
 ---- batch: 080 ----
mean loss: 149.75
 ---- batch: 090 ----
mean loss: 151.51
train mean loss: 150.72
epoch train time: 0:00:00.512329
elapsed time: 0:01:43.732631
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 23:27:10.019197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.82
 ---- batch: 020 ----
mean loss: 150.00
 ---- batch: 030 ----
mean loss: 151.05
 ---- batch: 040 ----
mean loss: 147.32
 ---- batch: 050 ----
mean loss: 155.17
 ---- batch: 060 ----
mean loss: 146.15
 ---- batch: 070 ----
mean loss: 144.96
 ---- batch: 080 ----
mean loss: 153.26
 ---- batch: 090 ----
mean loss: 164.46
train mean loss: 150.49
epoch train time: 0:00:00.506810
elapsed time: 0:01:44.239585
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 23:27:10.526153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.01
 ---- batch: 020 ----
mean loss: 145.77
 ---- batch: 030 ----
mean loss: 150.35
 ---- batch: 040 ----
mean loss: 147.07
 ---- batch: 050 ----
mean loss: 146.33
 ---- batch: 060 ----
mean loss: 149.60
 ---- batch: 070 ----
mean loss: 156.21
 ---- batch: 080 ----
mean loss: 158.54
 ---- batch: 090 ----
mean loss: 151.41
train mean loss: 149.91
epoch train time: 0:00:00.515392
elapsed time: 0:01:44.755144
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 23:27:11.041718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.14
 ---- batch: 020 ----
mean loss: 152.10
 ---- batch: 030 ----
mean loss: 146.54
 ---- batch: 040 ----
mean loss: 151.86
 ---- batch: 050 ----
mean loss: 151.12
 ---- batch: 060 ----
mean loss: 149.42
 ---- batch: 070 ----
mean loss: 148.52
 ---- batch: 080 ----
mean loss: 151.15
 ---- batch: 090 ----
mean loss: 153.79
train mean loss: 149.81
epoch train time: 0:00:00.502279
elapsed time: 0:01:45.257578
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 23:27:11.544146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.93
 ---- batch: 020 ----
mean loss: 148.33
 ---- batch: 030 ----
mean loss: 153.45
 ---- batch: 040 ----
mean loss: 141.21
 ---- batch: 050 ----
mean loss: 151.83
 ---- batch: 060 ----
mean loss: 140.87
 ---- batch: 070 ----
mean loss: 151.27
 ---- batch: 080 ----
mean loss: 148.07
 ---- batch: 090 ----
mean loss: 157.51
train mean loss: 148.77
epoch train time: 0:00:00.505181
elapsed time: 0:01:45.762911
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 23:27:12.049480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.87
 ---- batch: 020 ----
mean loss: 146.52
 ---- batch: 030 ----
mean loss: 144.34
 ---- batch: 040 ----
mean loss: 152.02
 ---- batch: 050 ----
mean loss: 154.16
 ---- batch: 060 ----
mean loss: 153.26
 ---- batch: 070 ----
mean loss: 148.42
 ---- batch: 080 ----
mean loss: 151.04
 ---- batch: 090 ----
mean loss: 147.64
train mean loss: 149.23
epoch train time: 0:00:00.502145
elapsed time: 0:01:46.265201
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 23:27:12.551808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.57
 ---- batch: 020 ----
mean loss: 150.89
 ---- batch: 030 ----
mean loss: 143.17
 ---- batch: 040 ----
mean loss: 157.18
 ---- batch: 050 ----
mean loss: 143.94
 ---- batch: 060 ----
mean loss: 144.37
 ---- batch: 070 ----
mean loss: 147.49
 ---- batch: 080 ----
mean loss: 145.48
 ---- batch: 090 ----
mean loss: 146.08
train mean loss: 148.26
epoch train time: 0:00:00.517673
elapsed time: 0:01:46.783062
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 23:27:13.069657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.62
 ---- batch: 020 ----
mean loss: 153.67
 ---- batch: 030 ----
mean loss: 148.09
 ---- batch: 040 ----
mean loss: 153.78
 ---- batch: 050 ----
mean loss: 144.84
 ---- batch: 060 ----
mean loss: 150.01
 ---- batch: 070 ----
mean loss: 150.98
 ---- batch: 080 ----
mean loss: 146.58
 ---- batch: 090 ----
mean loss: 145.91
train mean loss: 147.68
epoch train time: 0:00:00.509861
elapsed time: 0:01:47.293093
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 23:27:13.579659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.73
 ---- batch: 020 ----
mean loss: 146.84
 ---- batch: 030 ----
mean loss: 141.41
 ---- batch: 040 ----
mean loss: 150.46
 ---- batch: 050 ----
mean loss: 153.19
 ---- batch: 060 ----
mean loss: 153.07
 ---- batch: 070 ----
mean loss: 146.26
 ---- batch: 080 ----
mean loss: 154.98
 ---- batch: 090 ----
mean loss: 151.67
train mean loss: 148.85
epoch train time: 0:00:00.509862
elapsed time: 0:01:47.803132
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 23:27:14.089703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.92
 ---- batch: 020 ----
mean loss: 144.63
 ---- batch: 030 ----
mean loss: 149.17
 ---- batch: 040 ----
mean loss: 150.46
 ---- batch: 050 ----
mean loss: 151.54
 ---- batch: 060 ----
mean loss: 151.54
 ---- batch: 070 ----
mean loss: 140.71
 ---- batch: 080 ----
mean loss: 148.12
 ---- batch: 090 ----
mean loss: 146.56
train mean loss: 147.73
epoch train time: 0:00:00.504030
elapsed time: 0:01:48.307310
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 23:27:14.593876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.21
 ---- batch: 020 ----
mean loss: 148.94
 ---- batch: 030 ----
mean loss: 143.32
 ---- batch: 040 ----
mean loss: 142.26
 ---- batch: 050 ----
mean loss: 146.60
 ---- batch: 060 ----
mean loss: 150.03
 ---- batch: 070 ----
mean loss: 147.12
 ---- batch: 080 ----
mean loss: 155.75
 ---- batch: 090 ----
mean loss: 151.68
train mean loss: 147.52
epoch train time: 0:00:00.507572
elapsed time: 0:01:48.815055
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 23:27:15.101647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.44
 ---- batch: 020 ----
mean loss: 149.58
 ---- batch: 030 ----
mean loss: 143.29
 ---- batch: 040 ----
mean loss: 148.45
 ---- batch: 050 ----
mean loss: 139.63
 ---- batch: 060 ----
mean loss: 138.25
 ---- batch: 070 ----
mean loss: 148.16
 ---- batch: 080 ----
mean loss: 146.99
 ---- batch: 090 ----
mean loss: 149.87
train mean loss: 146.64
epoch train time: 0:00:00.505978
elapsed time: 0:01:49.321208
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 23:27:15.607760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.90
 ---- batch: 020 ----
mean loss: 140.89
 ---- batch: 030 ----
mean loss: 143.09
 ---- batch: 040 ----
mean loss: 150.05
 ---- batch: 050 ----
mean loss: 147.77
 ---- batch: 060 ----
mean loss: 149.99
 ---- batch: 070 ----
mean loss: 154.57
 ---- batch: 080 ----
mean loss: 144.24
 ---- batch: 090 ----
mean loss: 150.28
train mean loss: 147.11
epoch train time: 0:00:00.512643
elapsed time: 0:01:49.833987
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 23:27:16.120556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.32
 ---- batch: 020 ----
mean loss: 149.55
 ---- batch: 030 ----
mean loss: 141.19
 ---- batch: 040 ----
mean loss: 153.53
 ---- batch: 050 ----
mean loss: 141.72
 ---- batch: 060 ----
mean loss: 148.63
 ---- batch: 070 ----
mean loss: 140.08
 ---- batch: 080 ----
mean loss: 153.24
 ---- batch: 090 ----
mean loss: 150.13
train mean loss: 147.12
epoch train time: 0:00:00.502989
elapsed time: 0:01:50.337122
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 23:27:16.623706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.80
 ---- batch: 020 ----
mean loss: 145.39
 ---- batch: 030 ----
mean loss: 138.68
 ---- batch: 040 ----
mean loss: 148.31
 ---- batch: 050 ----
mean loss: 148.68
 ---- batch: 060 ----
mean loss: 148.68
 ---- batch: 070 ----
mean loss: 142.46
 ---- batch: 080 ----
mean loss: 146.25
 ---- batch: 090 ----
mean loss: 152.25
train mean loss: 146.12
epoch train time: 0:00:00.511349
elapsed time: 0:01:50.848662
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 23:27:17.135232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.20
 ---- batch: 020 ----
mean loss: 144.82
 ---- batch: 030 ----
mean loss: 149.60
 ---- batch: 040 ----
mean loss: 145.08
 ---- batch: 050 ----
mean loss: 142.68
 ---- batch: 060 ----
mean loss: 143.38
 ---- batch: 070 ----
mean loss: 151.14
 ---- batch: 080 ----
mean loss: 143.91
 ---- batch: 090 ----
mean loss: 149.41
train mean loss: 145.21
epoch train time: 0:00:00.510927
elapsed time: 0:01:51.359738
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 23:27:17.646305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.19
 ---- batch: 020 ----
mean loss: 140.50
 ---- batch: 030 ----
mean loss: 147.36
 ---- batch: 040 ----
mean loss: 146.78
 ---- batch: 050 ----
mean loss: 140.90
 ---- batch: 060 ----
mean loss: 140.63
 ---- batch: 070 ----
mean loss: 146.88
 ---- batch: 080 ----
mean loss: 144.91
 ---- batch: 090 ----
mean loss: 148.74
train mean loss: 144.86
epoch train time: 0:00:00.511566
elapsed time: 0:01:51.871472
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 23:27:18.158040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.47
 ---- batch: 020 ----
mean loss: 145.22
 ---- batch: 030 ----
mean loss: 139.23
 ---- batch: 040 ----
mean loss: 145.66
 ---- batch: 050 ----
mean loss: 151.28
 ---- batch: 060 ----
mean loss: 150.81
 ---- batch: 070 ----
mean loss: 151.78
 ---- batch: 080 ----
mean loss: 150.88
 ---- batch: 090 ----
mean loss: 150.67
train mean loss: 147.31
epoch train time: 0:00:00.506685
elapsed time: 0:01:52.378311
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 23:27:18.664887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.42
 ---- batch: 020 ----
mean loss: 149.15
 ---- batch: 030 ----
mean loss: 143.21
 ---- batch: 040 ----
mean loss: 139.04
 ---- batch: 050 ----
mean loss: 144.03
 ---- batch: 060 ----
mean loss: 151.72
 ---- batch: 070 ----
mean loss: 144.93
 ---- batch: 080 ----
mean loss: 146.09
 ---- batch: 090 ----
mean loss: 144.23
train mean loss: 145.32
epoch train time: 0:00:00.507030
elapsed time: 0:01:52.885513
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 23:27:19.172081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.45
 ---- batch: 020 ----
mean loss: 139.32
 ---- batch: 030 ----
mean loss: 149.34
 ---- batch: 040 ----
mean loss: 145.73
 ---- batch: 050 ----
mean loss: 141.74
 ---- batch: 060 ----
mean loss: 144.85
 ---- batch: 070 ----
mean loss: 145.61
 ---- batch: 080 ----
mean loss: 152.54
 ---- batch: 090 ----
mean loss: 145.17
train mean loss: 144.58
epoch train time: 0:00:00.505182
elapsed time: 0:01:53.390839
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 23:27:19.677406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.78
 ---- batch: 020 ----
mean loss: 148.55
 ---- batch: 030 ----
mean loss: 145.31
 ---- batch: 040 ----
mean loss: 140.46
 ---- batch: 050 ----
mean loss: 150.46
 ---- batch: 060 ----
mean loss: 144.49
 ---- batch: 070 ----
mean loss: 137.15
 ---- batch: 080 ----
mean loss: 144.45
 ---- batch: 090 ----
mean loss: 145.68
train mean loss: 144.63
epoch train time: 0:00:00.508484
elapsed time: 0:01:53.899469
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 23:27:20.186051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.91
 ---- batch: 020 ----
mean loss: 140.72
 ---- batch: 030 ----
mean loss: 143.32
 ---- batch: 040 ----
mean loss: 144.42
 ---- batch: 050 ----
mean loss: 138.89
 ---- batch: 060 ----
mean loss: 135.87
 ---- batch: 070 ----
mean loss: 145.50
 ---- batch: 080 ----
mean loss: 145.80
 ---- batch: 090 ----
mean loss: 146.35
train mean loss: 143.51
epoch train time: 0:00:00.507602
elapsed time: 0:01:54.407234
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 23:27:20.693804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.89
 ---- batch: 020 ----
mean loss: 139.41
 ---- batch: 030 ----
mean loss: 142.66
 ---- batch: 040 ----
mean loss: 144.83
 ---- batch: 050 ----
mean loss: 149.02
 ---- batch: 060 ----
mean loss: 140.25
 ---- batch: 070 ----
mean loss: 149.29
 ---- batch: 080 ----
mean loss: 145.75
 ---- batch: 090 ----
mean loss: 147.85
train mean loss: 143.65
epoch train time: 0:00:00.516768
elapsed time: 0:01:54.924153
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 23:27:21.210720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.67
 ---- batch: 020 ----
mean loss: 143.59
 ---- batch: 030 ----
mean loss: 138.80
 ---- batch: 040 ----
mean loss: 140.80
 ---- batch: 050 ----
mean loss: 143.12
 ---- batch: 060 ----
mean loss: 144.71
 ---- batch: 070 ----
mean loss: 152.36
 ---- batch: 080 ----
mean loss: 151.45
 ---- batch: 090 ----
mean loss: 142.61
train mean loss: 143.45
epoch train time: 0:00:00.511045
elapsed time: 0:01:55.435355
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 23:27:21.721920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.33
 ---- batch: 020 ----
mean loss: 136.13
 ---- batch: 030 ----
mean loss: 137.91
 ---- batch: 040 ----
mean loss: 141.72
 ---- batch: 050 ----
mean loss: 139.13
 ---- batch: 060 ----
mean loss: 141.48
 ---- batch: 070 ----
mean loss: 151.27
 ---- batch: 080 ----
mean loss: 143.50
 ---- batch: 090 ----
mean loss: 146.83
train mean loss: 142.62
epoch train time: 0:00:00.510580
elapsed time: 0:01:55.946079
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 23:27:22.232647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.11
 ---- batch: 020 ----
mean loss: 149.53
 ---- batch: 030 ----
mean loss: 146.91
 ---- batch: 040 ----
mean loss: 146.62
 ---- batch: 050 ----
mean loss: 138.20
 ---- batch: 060 ----
mean loss: 144.16
 ---- batch: 070 ----
mean loss: 149.61
 ---- batch: 080 ----
mean loss: 144.02
 ---- batch: 090 ----
mean loss: 138.36
train mean loss: 143.15
epoch train time: 0:00:00.514846
elapsed time: 0:01:56.461071
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 23:27:22.747655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.36
 ---- batch: 020 ----
mean loss: 139.81
 ---- batch: 030 ----
mean loss: 142.17
 ---- batch: 040 ----
mean loss: 144.49
 ---- batch: 050 ----
mean loss: 142.97
 ---- batch: 060 ----
mean loss: 145.29
 ---- batch: 070 ----
mean loss: 143.95
 ---- batch: 080 ----
mean loss: 139.17
 ---- batch: 090 ----
mean loss: 145.66
train mean loss: 143.30
epoch train time: 0:00:00.511443
elapsed time: 0:01:56.972686
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 23:27:23.259255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.35
 ---- batch: 020 ----
mean loss: 139.86
 ---- batch: 030 ----
mean loss: 136.01
 ---- batch: 040 ----
mean loss: 135.88
 ---- batch: 050 ----
mean loss: 147.24
 ---- batch: 060 ----
mean loss: 143.71
 ---- batch: 070 ----
mean loss: 147.78
 ---- batch: 080 ----
mean loss: 142.21
 ---- batch: 090 ----
mean loss: 143.64
train mean loss: 142.59
epoch train time: 0:00:00.504535
elapsed time: 0:01:57.477400
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 23:27:23.763970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.35
 ---- batch: 020 ----
mean loss: 141.92
 ---- batch: 030 ----
mean loss: 138.67
 ---- batch: 040 ----
mean loss: 146.80
 ---- batch: 050 ----
mean loss: 143.48
 ---- batch: 060 ----
mean loss: 143.38
 ---- batch: 070 ----
mean loss: 144.56
 ---- batch: 080 ----
mean loss: 138.42
 ---- batch: 090 ----
mean loss: 139.45
train mean loss: 141.88
epoch train time: 0:00:00.511433
elapsed time: 0:01:57.988997
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 23:27:24.275586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.69
 ---- batch: 020 ----
mean loss: 139.79
 ---- batch: 030 ----
mean loss: 144.67
 ---- batch: 040 ----
mean loss: 141.13
 ---- batch: 050 ----
mean loss: 142.57
 ---- batch: 060 ----
mean loss: 139.30
 ---- batch: 070 ----
mean loss: 135.57
 ---- batch: 080 ----
mean loss: 143.32
 ---- batch: 090 ----
mean loss: 143.71
train mean loss: 141.62
epoch train time: 0:00:00.508821
elapsed time: 0:01:58.497982
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 23:27:24.784558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.60
 ---- batch: 020 ----
mean loss: 137.13
 ---- batch: 030 ----
mean loss: 147.00
 ---- batch: 040 ----
mean loss: 136.74
 ---- batch: 050 ----
mean loss: 143.29
 ---- batch: 060 ----
mean loss: 136.05
 ---- batch: 070 ----
mean loss: 145.16
 ---- batch: 080 ----
mean loss: 140.71
 ---- batch: 090 ----
mean loss: 139.71
train mean loss: 141.16
epoch train time: 0:00:00.527709
elapsed time: 0:01:59.025864
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 23:27:25.312454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.39
 ---- batch: 020 ----
mean loss: 143.36
 ---- batch: 030 ----
mean loss: 133.59
 ---- batch: 040 ----
mean loss: 140.65
 ---- batch: 050 ----
mean loss: 142.88
 ---- batch: 060 ----
mean loss: 149.74
 ---- batch: 070 ----
mean loss: 143.32
 ---- batch: 080 ----
mean loss: 145.19
 ---- batch: 090 ----
mean loss: 142.33
train mean loss: 141.52
epoch train time: 0:00:00.535221
elapsed time: 0:01:59.561275
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 23:27:25.847847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.06
 ---- batch: 020 ----
mean loss: 135.11
 ---- batch: 030 ----
mean loss: 135.75
 ---- batch: 040 ----
mean loss: 145.79
 ---- batch: 050 ----
mean loss: 138.92
 ---- batch: 060 ----
mean loss: 142.64
 ---- batch: 070 ----
mean loss: 140.49
 ---- batch: 080 ----
mean loss: 143.92
 ---- batch: 090 ----
mean loss: 144.21
train mean loss: 141.34
epoch train time: 0:00:00.512234
elapsed time: 0:02:00.073660
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 23:27:26.360228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.22
 ---- batch: 020 ----
mean loss: 140.67
 ---- batch: 030 ----
mean loss: 137.72
 ---- batch: 040 ----
mean loss: 139.96
 ---- batch: 050 ----
mean loss: 145.05
 ---- batch: 060 ----
mean loss: 137.97
 ---- batch: 070 ----
mean loss: 145.87
 ---- batch: 080 ----
mean loss: 136.37
 ---- batch: 090 ----
mean loss: 139.37
train mean loss: 140.17
epoch train time: 0:00:00.524090
elapsed time: 0:02:00.597927
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 23:27:26.884498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.06
 ---- batch: 020 ----
mean loss: 138.18
 ---- batch: 030 ----
mean loss: 136.56
 ---- batch: 040 ----
mean loss: 137.65
 ---- batch: 050 ----
mean loss: 140.96
 ---- batch: 060 ----
mean loss: 139.55
 ---- batch: 070 ----
mean loss: 142.58
 ---- batch: 080 ----
mean loss: 140.15
 ---- batch: 090 ----
mean loss: 146.52
train mean loss: 140.22
epoch train time: 0:00:00.511037
elapsed time: 0:02:01.109113
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 23:27:27.395681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.74
 ---- batch: 020 ----
mean loss: 137.25
 ---- batch: 030 ----
mean loss: 138.13
 ---- batch: 040 ----
mean loss: 142.73
 ---- batch: 050 ----
mean loss: 141.23
 ---- batch: 060 ----
mean loss: 136.75
 ---- batch: 070 ----
mean loss: 143.35
 ---- batch: 080 ----
mean loss: 143.05
 ---- batch: 090 ----
mean loss: 141.87
train mean loss: 140.71
epoch train time: 0:00:00.518091
elapsed time: 0:02:01.627373
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 23:27:27.913943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.71
 ---- batch: 020 ----
mean loss: 133.81
 ---- batch: 030 ----
mean loss: 144.65
 ---- batch: 040 ----
mean loss: 134.11
 ---- batch: 050 ----
mean loss: 136.42
 ---- batch: 060 ----
mean loss: 144.84
 ---- batch: 070 ----
mean loss: 140.65
 ---- batch: 080 ----
mean loss: 141.06
 ---- batch: 090 ----
mean loss: 143.00
train mean loss: 139.70
epoch train time: 0:00:00.515757
elapsed time: 0:02:02.143312
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 23:27:28.429897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.08
 ---- batch: 020 ----
mean loss: 141.12
 ---- batch: 030 ----
mean loss: 141.39
 ---- batch: 040 ----
mean loss: 135.53
 ---- batch: 050 ----
mean loss: 136.30
 ---- batch: 060 ----
mean loss: 144.28
 ---- batch: 070 ----
mean loss: 138.00
 ---- batch: 080 ----
mean loss: 139.36
 ---- batch: 090 ----
mean loss: 140.39
train mean loss: 139.61
epoch train time: 0:00:00.515274
elapsed time: 0:02:02.658754
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 23:27:28.945321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.70
 ---- batch: 020 ----
mean loss: 135.59
 ---- batch: 030 ----
mean loss: 137.72
 ---- batch: 040 ----
mean loss: 144.19
 ---- batch: 050 ----
mean loss: 135.94
 ---- batch: 060 ----
mean loss: 141.97
 ---- batch: 070 ----
mean loss: 141.63
 ---- batch: 080 ----
mean loss: 138.87
 ---- batch: 090 ----
mean loss: 137.33
train mean loss: 138.90
epoch train time: 0:00:00.510874
elapsed time: 0:02:03.169784
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 23:27:29.456347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.99
 ---- batch: 020 ----
mean loss: 137.53
 ---- batch: 030 ----
mean loss: 133.66
 ---- batch: 040 ----
mean loss: 139.14
 ---- batch: 050 ----
mean loss: 143.70
 ---- batch: 060 ----
mean loss: 142.80
 ---- batch: 070 ----
mean loss: 139.05
 ---- batch: 080 ----
mean loss: 137.32
 ---- batch: 090 ----
mean loss: 143.27
train mean loss: 138.96
epoch train time: 0:00:00.525105
elapsed time: 0:02:03.695136
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 23:27:29.981715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.76
 ---- batch: 020 ----
mean loss: 138.62
 ---- batch: 030 ----
mean loss: 142.09
 ---- batch: 040 ----
mean loss: 133.98
 ---- batch: 050 ----
mean loss: 132.91
 ---- batch: 060 ----
mean loss: 140.80
 ---- batch: 070 ----
mean loss: 144.25
 ---- batch: 080 ----
mean loss: 137.84
 ---- batch: 090 ----
mean loss: 137.70
train mean loss: 138.60
epoch train time: 0:00:00.516210
elapsed time: 0:02:04.211515
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 23:27:30.498085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.58
 ---- batch: 020 ----
mean loss: 137.36
 ---- batch: 030 ----
mean loss: 135.22
 ---- batch: 040 ----
mean loss: 137.96
 ---- batch: 050 ----
mean loss: 142.06
 ---- batch: 060 ----
mean loss: 139.41
 ---- batch: 070 ----
mean loss: 136.08
 ---- batch: 080 ----
mean loss: 135.04
 ---- batch: 090 ----
mean loss: 141.27
train mean loss: 138.21
epoch train time: 0:00:00.513353
elapsed time: 0:02:04.725014
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 23:27:31.011583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.83
 ---- batch: 020 ----
mean loss: 135.63
 ---- batch: 030 ----
mean loss: 137.77
 ---- batch: 040 ----
mean loss: 135.10
 ---- batch: 050 ----
mean loss: 141.72
 ---- batch: 060 ----
mean loss: 139.77
 ---- batch: 070 ----
mean loss: 136.27
 ---- batch: 080 ----
mean loss: 138.29
 ---- batch: 090 ----
mean loss: 144.22
train mean loss: 138.64
epoch train time: 0:00:00.498101
elapsed time: 0:02:05.223260
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 23:27:31.509825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.08
 ---- batch: 020 ----
mean loss: 138.30
 ---- batch: 030 ----
mean loss: 136.72
 ---- batch: 040 ----
mean loss: 139.16
 ---- batch: 050 ----
mean loss: 142.15
 ---- batch: 060 ----
mean loss: 134.96
 ---- batch: 070 ----
mean loss: 138.64
 ---- batch: 080 ----
mean loss: 137.98
 ---- batch: 090 ----
mean loss: 136.66
train mean loss: 138.32
epoch train time: 0:00:00.515846
elapsed time: 0:02:05.739265
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 23:27:32.025866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.65
 ---- batch: 020 ----
mean loss: 132.66
 ---- batch: 030 ----
mean loss: 138.72
 ---- batch: 040 ----
mean loss: 133.88
 ---- batch: 050 ----
mean loss: 134.05
 ---- batch: 060 ----
mean loss: 146.34
 ---- batch: 070 ----
mean loss: 138.96
 ---- batch: 080 ----
mean loss: 140.08
 ---- batch: 090 ----
mean loss: 135.94
train mean loss: 137.39
epoch train time: 0:00:00.514753
elapsed time: 0:02:06.254198
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 23:27:32.540767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.45
 ---- batch: 020 ----
mean loss: 139.12
 ---- batch: 030 ----
mean loss: 139.35
 ---- batch: 040 ----
mean loss: 138.23
 ---- batch: 050 ----
mean loss: 134.31
 ---- batch: 060 ----
mean loss: 137.08
 ---- batch: 070 ----
mean loss: 133.32
 ---- batch: 080 ----
mean loss: 141.38
 ---- batch: 090 ----
mean loss: 134.33
train mean loss: 137.63
epoch train time: 0:00:00.517845
elapsed time: 0:02:06.772192
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 23:27:33.058762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.51
 ---- batch: 020 ----
mean loss: 136.56
 ---- batch: 030 ----
mean loss: 132.64
 ---- batch: 040 ----
mean loss: 134.46
 ---- batch: 050 ----
mean loss: 137.73
 ---- batch: 060 ----
mean loss: 143.55
 ---- batch: 070 ----
mean loss: 138.37
 ---- batch: 080 ----
mean loss: 132.77
 ---- batch: 090 ----
mean loss: 134.87
train mean loss: 137.39
epoch train time: 0:00:00.503351
elapsed time: 0:02:07.275690
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 23:27:33.562258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.01
 ---- batch: 020 ----
mean loss: 134.02
 ---- batch: 030 ----
mean loss: 132.58
 ---- batch: 040 ----
mean loss: 140.63
 ---- batch: 050 ----
mean loss: 140.06
 ---- batch: 060 ----
mean loss: 140.81
 ---- batch: 070 ----
mean loss: 136.21
 ---- batch: 080 ----
mean loss: 134.91
 ---- batch: 090 ----
mean loss: 137.84
train mean loss: 137.19
epoch train time: 0:00:00.509397
elapsed time: 0:02:07.785235
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 23:27:34.071804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.81
 ---- batch: 020 ----
mean loss: 131.35
 ---- batch: 030 ----
mean loss: 131.96
 ---- batch: 040 ----
mean loss: 139.95
 ---- batch: 050 ----
mean loss: 144.86
 ---- batch: 060 ----
mean loss: 141.07
 ---- batch: 070 ----
mean loss: 138.40
 ---- batch: 080 ----
mean loss: 136.09
 ---- batch: 090 ----
mean loss: 138.49
train mean loss: 137.75
epoch train time: 0:00:00.509537
elapsed time: 0:02:08.294916
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 23:27:34.581483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.48
 ---- batch: 020 ----
mean loss: 129.29
 ---- batch: 030 ----
mean loss: 141.54
 ---- batch: 040 ----
mean loss: 137.98
 ---- batch: 050 ----
mean loss: 137.23
 ---- batch: 060 ----
mean loss: 140.02
 ---- batch: 070 ----
mean loss: 144.18
 ---- batch: 080 ----
mean loss: 131.72
 ---- batch: 090 ----
mean loss: 141.92
train mean loss: 137.94
epoch train time: 0:00:00.520196
elapsed time: 0:02:08.815258
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 23:27:35.101826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.97
 ---- batch: 020 ----
mean loss: 136.18
 ---- batch: 030 ----
mean loss: 133.09
 ---- batch: 040 ----
mean loss: 134.91
 ---- batch: 050 ----
mean loss: 145.60
 ---- batch: 060 ----
mean loss: 142.38
 ---- batch: 070 ----
mean loss: 136.99
 ---- batch: 080 ----
mean loss: 137.02
 ---- batch: 090 ----
mean loss: 139.73
train mean loss: 137.22
epoch train time: 0:00:00.522591
elapsed time: 0:02:09.338001
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 23:27:35.624571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.54
 ---- batch: 020 ----
mean loss: 134.03
 ---- batch: 030 ----
mean loss: 129.67
 ---- batch: 040 ----
mean loss: 134.18
 ---- batch: 050 ----
mean loss: 131.55
 ---- batch: 060 ----
mean loss: 137.67
 ---- batch: 070 ----
mean loss: 138.58
 ---- batch: 080 ----
mean loss: 146.68
 ---- batch: 090 ----
mean loss: 141.13
train mean loss: 136.13
epoch train time: 0:00:00.517685
elapsed time: 0:02:09.855835
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 23:27:36.142427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.69
 ---- batch: 020 ----
mean loss: 130.19
 ---- batch: 030 ----
mean loss: 137.80
 ---- batch: 040 ----
mean loss: 131.68
 ---- batch: 050 ----
mean loss: 132.35
 ---- batch: 060 ----
mean loss: 135.01
 ---- batch: 070 ----
mean loss: 136.80
 ---- batch: 080 ----
mean loss: 144.60
 ---- batch: 090 ----
mean loss: 145.12
train mean loss: 136.46
epoch train time: 0:00:00.499803
elapsed time: 0:02:10.355810
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 23:27:36.642381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.22
 ---- batch: 020 ----
mean loss: 133.58
 ---- batch: 030 ----
mean loss: 135.91
 ---- batch: 040 ----
mean loss: 138.56
 ---- batch: 050 ----
mean loss: 136.41
 ---- batch: 060 ----
mean loss: 134.68
 ---- batch: 070 ----
mean loss: 138.33
 ---- batch: 080 ----
mean loss: 138.31
 ---- batch: 090 ----
mean loss: 131.91
train mean loss: 135.51
epoch train time: 0:00:00.513534
elapsed time: 0:02:10.869507
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 23:27:37.156080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.06
 ---- batch: 020 ----
mean loss: 129.68
 ---- batch: 030 ----
mean loss: 132.96
 ---- batch: 040 ----
mean loss: 133.78
 ---- batch: 050 ----
mean loss: 135.36
 ---- batch: 060 ----
mean loss: 140.49
 ---- batch: 070 ----
mean loss: 139.51
 ---- batch: 080 ----
mean loss: 140.56
 ---- batch: 090 ----
mean loss: 140.04
train mean loss: 136.00
epoch train time: 0:00:00.502844
elapsed time: 0:02:11.372495
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 23:27:37.659069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.34
 ---- batch: 020 ----
mean loss: 137.28
 ---- batch: 030 ----
mean loss: 139.01
 ---- batch: 040 ----
mean loss: 135.67
 ---- batch: 050 ----
mean loss: 139.56
 ---- batch: 060 ----
mean loss: 136.72
 ---- batch: 070 ----
mean loss: 135.79
 ---- batch: 080 ----
mean loss: 129.52
 ---- batch: 090 ----
mean loss: 140.77
train mean loss: 136.58
epoch train time: 0:00:00.507008
elapsed time: 0:02:11.879655
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 23:27:38.166223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.03
 ---- batch: 020 ----
mean loss: 130.23
 ---- batch: 030 ----
mean loss: 131.67
 ---- batch: 040 ----
mean loss: 134.25
 ---- batch: 050 ----
mean loss: 134.41
 ---- batch: 060 ----
mean loss: 131.43
 ---- batch: 070 ----
mean loss: 133.98
 ---- batch: 080 ----
mean loss: 134.23
 ---- batch: 090 ----
mean loss: 145.53
train mean loss: 134.41
epoch train time: 0:00:00.496593
elapsed time: 0:02:12.376395
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 23:27:38.662963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.11
 ---- batch: 020 ----
mean loss: 132.10
 ---- batch: 030 ----
mean loss: 133.05
 ---- batch: 040 ----
mean loss: 137.80
 ---- batch: 050 ----
mean loss: 137.57
 ---- batch: 060 ----
mean loss: 137.91
 ---- batch: 070 ----
mean loss: 133.02
 ---- batch: 080 ----
mean loss: 136.29
 ---- batch: 090 ----
mean loss: 136.80
train mean loss: 135.58
epoch train time: 0:00:00.517408
elapsed time: 0:02:12.893947
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 23:27:39.180514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.86
 ---- batch: 020 ----
mean loss: 136.41
 ---- batch: 030 ----
mean loss: 136.99
 ---- batch: 040 ----
mean loss: 131.95
 ---- batch: 050 ----
mean loss: 131.97
 ---- batch: 060 ----
mean loss: 137.49
 ---- batch: 070 ----
mean loss: 139.59
 ---- batch: 080 ----
mean loss: 136.73
 ---- batch: 090 ----
mean loss: 137.91
train mean loss: 135.37
epoch train time: 0:00:00.497070
elapsed time: 0:02:13.391187
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 23:27:39.677788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.13
 ---- batch: 020 ----
mean loss: 135.36
 ---- batch: 030 ----
mean loss: 133.91
 ---- batch: 040 ----
mean loss: 133.89
 ---- batch: 050 ----
mean loss: 130.34
 ---- batch: 060 ----
mean loss: 141.51
 ---- batch: 070 ----
mean loss: 136.29
 ---- batch: 080 ----
mean loss: 127.92
 ---- batch: 090 ----
mean loss: 144.92
train mean loss: 135.57
epoch train time: 0:00:00.507089
elapsed time: 0:02:13.898474
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 23:27:40.185041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.94
 ---- batch: 020 ----
mean loss: 131.83
 ---- batch: 030 ----
mean loss: 133.23
 ---- batch: 040 ----
mean loss: 131.28
 ---- batch: 050 ----
mean loss: 130.95
 ---- batch: 060 ----
mean loss: 139.81
 ---- batch: 070 ----
mean loss: 128.10
 ---- batch: 080 ----
mean loss: 136.25
 ---- batch: 090 ----
mean loss: 145.98
train mean loss: 134.78
epoch train time: 0:00:00.511898
elapsed time: 0:02:14.410527
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 23:27:40.697104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.23
 ---- batch: 020 ----
mean loss: 127.51
 ---- batch: 030 ----
mean loss: 131.45
 ---- batch: 040 ----
mean loss: 136.00
 ---- batch: 050 ----
mean loss: 135.26
 ---- batch: 060 ----
mean loss: 140.90
 ---- batch: 070 ----
mean loss: 135.42
 ---- batch: 080 ----
mean loss: 137.82
 ---- batch: 090 ----
mean loss: 134.68
train mean loss: 133.91
epoch train time: 0:00:00.524964
elapsed time: 0:02:14.935656
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 23:27:41.222235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.01
 ---- batch: 020 ----
mean loss: 131.99
 ---- batch: 030 ----
mean loss: 131.10
 ---- batch: 040 ----
mean loss: 134.82
 ---- batch: 050 ----
mean loss: 133.63
 ---- batch: 060 ----
mean loss: 141.87
 ---- batch: 070 ----
mean loss: 135.47
 ---- batch: 080 ----
mean loss: 138.16
 ---- batch: 090 ----
mean loss: 134.19
train mean loss: 134.61
epoch train time: 0:00:00.518925
elapsed time: 0:02:15.454745
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 23:27:41.741323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.74
 ---- batch: 020 ----
mean loss: 137.08
 ---- batch: 030 ----
mean loss: 127.32
 ---- batch: 040 ----
mean loss: 132.97
 ---- batch: 050 ----
mean loss: 129.67
 ---- batch: 060 ----
mean loss: 138.42
 ---- batch: 070 ----
mean loss: 138.51
 ---- batch: 080 ----
mean loss: 135.97
 ---- batch: 090 ----
mean loss: 139.57
train mean loss: 134.51
epoch train time: 0:00:00.517663
elapsed time: 0:02:15.972578
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 23:27:42.259152
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.10
 ---- batch: 020 ----
mean loss: 130.77
 ---- batch: 030 ----
mean loss: 123.17
 ---- batch: 040 ----
mean loss: 132.88
 ---- batch: 050 ----
mean loss: 125.17
 ---- batch: 060 ----
mean loss: 123.94
 ---- batch: 070 ----
mean loss: 128.78
 ---- batch: 080 ----
mean loss: 127.02
 ---- batch: 090 ----
mean loss: 126.30
train mean loss: 128.32
epoch train time: 0:00:00.518776
elapsed time: 0:02:16.491523
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 23:27:42.778096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.98
 ---- batch: 020 ----
mean loss: 122.77
 ---- batch: 030 ----
mean loss: 125.26
 ---- batch: 040 ----
mean loss: 120.47
 ---- batch: 050 ----
mean loss: 126.60
 ---- batch: 060 ----
mean loss: 132.22
 ---- batch: 070 ----
mean loss: 124.67
 ---- batch: 080 ----
mean loss: 129.42
 ---- batch: 090 ----
mean loss: 124.24
train mean loss: 126.58
epoch train time: 0:00:00.506030
elapsed time: 0:02:16.997701
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 23:27:43.284267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.16
 ---- batch: 020 ----
mean loss: 125.79
 ---- batch: 030 ----
mean loss: 119.26
 ---- batch: 040 ----
mean loss: 128.79
 ---- batch: 050 ----
mean loss: 129.04
 ---- batch: 060 ----
mean loss: 121.03
 ---- batch: 070 ----
mean loss: 129.06
 ---- batch: 080 ----
mean loss: 129.58
 ---- batch: 090 ----
mean loss: 127.21
train mean loss: 126.00
epoch train time: 0:00:00.502896
elapsed time: 0:02:17.500793
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 23:27:43.787401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.00
 ---- batch: 020 ----
mean loss: 118.82
 ---- batch: 030 ----
mean loss: 130.40
 ---- batch: 040 ----
mean loss: 123.74
 ---- batch: 050 ----
mean loss: 125.35
 ---- batch: 060 ----
mean loss: 127.72
 ---- batch: 070 ----
mean loss: 120.98
 ---- batch: 080 ----
mean loss: 135.47
 ---- batch: 090 ----
mean loss: 123.50
train mean loss: 125.79
epoch train time: 0:00:00.509680
elapsed time: 0:02:18.010675
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 23:27:44.297243
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.75
 ---- batch: 020 ----
mean loss: 125.79
 ---- batch: 030 ----
mean loss: 130.67
 ---- batch: 040 ----
mean loss: 117.62
 ---- batch: 050 ----
mean loss: 125.40
 ---- batch: 060 ----
mean loss: 128.85
 ---- batch: 070 ----
mean loss: 123.06
 ---- batch: 080 ----
mean loss: 132.08
 ---- batch: 090 ----
mean loss: 124.44
train mean loss: 126.07
epoch train time: 0:00:00.511001
elapsed time: 0:02:18.521831
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 23:27:44.808424
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.40
 ---- batch: 020 ----
mean loss: 120.52
 ---- batch: 030 ----
mean loss: 126.89
 ---- batch: 040 ----
mean loss: 128.56
 ---- batch: 050 ----
mean loss: 128.09
 ---- batch: 060 ----
mean loss: 126.26
 ---- batch: 070 ----
mean loss: 125.23
 ---- batch: 080 ----
mean loss: 127.64
 ---- batch: 090 ----
mean loss: 126.85
train mean loss: 126.05
epoch train time: 0:00:00.507274
elapsed time: 0:02:19.029274
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 23:27:45.315841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.36
 ---- batch: 020 ----
mean loss: 123.29
 ---- batch: 030 ----
mean loss: 126.10
 ---- batch: 040 ----
mean loss: 130.51
 ---- batch: 050 ----
mean loss: 127.71
 ---- batch: 060 ----
mean loss: 120.00
 ---- batch: 070 ----
mean loss: 123.82
 ---- batch: 080 ----
mean loss: 127.31
 ---- batch: 090 ----
mean loss: 126.86
train mean loss: 125.93
epoch train time: 0:00:00.506074
elapsed time: 0:02:19.535502
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 23:27:45.822115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.28
 ---- batch: 020 ----
mean loss: 126.84
 ---- batch: 030 ----
mean loss: 128.82
 ---- batch: 040 ----
mean loss: 128.21
 ---- batch: 050 ----
mean loss: 128.21
 ---- batch: 060 ----
mean loss: 122.72
 ---- batch: 070 ----
mean loss: 127.16
 ---- batch: 080 ----
mean loss: 122.70
 ---- batch: 090 ----
mean loss: 119.95
train mean loss: 125.57
epoch train time: 0:00:00.518787
elapsed time: 0:02:20.054485
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 23:27:46.341052
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.85
 ---- batch: 020 ----
mean loss: 130.17
 ---- batch: 030 ----
mean loss: 123.98
 ---- batch: 040 ----
mean loss: 121.60
 ---- batch: 050 ----
mean loss: 132.40
 ---- batch: 060 ----
mean loss: 125.39
 ---- batch: 070 ----
mean loss: 123.42
 ---- batch: 080 ----
mean loss: 125.03
 ---- batch: 090 ----
mean loss: 126.64
train mean loss: 125.73
epoch train time: 0:00:00.510883
elapsed time: 0:02:20.565516
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 23:27:46.852086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.88
 ---- batch: 020 ----
mean loss: 122.73
 ---- batch: 030 ----
mean loss: 131.38
 ---- batch: 040 ----
mean loss: 127.46
 ---- batch: 050 ----
mean loss: 120.10
 ---- batch: 060 ----
mean loss: 127.18
 ---- batch: 070 ----
mean loss: 122.73
 ---- batch: 080 ----
mean loss: 130.02
 ---- batch: 090 ----
mean loss: 123.35
train mean loss: 125.67
epoch train time: 0:00:00.518720
elapsed time: 0:02:21.084401
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 23:27:47.370986
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.36
 ---- batch: 020 ----
mean loss: 119.59
 ---- batch: 030 ----
mean loss: 126.83
 ---- batch: 040 ----
mean loss: 125.81
 ---- batch: 050 ----
mean loss: 124.27
 ---- batch: 060 ----
mean loss: 128.93
 ---- batch: 070 ----
mean loss: 128.80
 ---- batch: 080 ----
mean loss: 124.82
 ---- batch: 090 ----
mean loss: 126.56
train mean loss: 125.52
epoch train time: 0:00:00.508278
elapsed time: 0:02:21.592844
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 23:27:47.879417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.24
 ---- batch: 020 ----
mean loss: 120.23
 ---- batch: 030 ----
mean loss: 127.42
 ---- batch: 040 ----
mean loss: 130.47
 ---- batch: 050 ----
mean loss: 123.69
 ---- batch: 060 ----
mean loss: 124.28
 ---- batch: 070 ----
mean loss: 127.21
 ---- batch: 080 ----
mean loss: 125.56
 ---- batch: 090 ----
mean loss: 123.79
train mean loss: 125.65
epoch train time: 0:00:00.495205
elapsed time: 0:02:22.088200
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 23:27:48.374768
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.96
 ---- batch: 020 ----
mean loss: 122.83
 ---- batch: 030 ----
mean loss: 119.09
 ---- batch: 040 ----
mean loss: 130.85
 ---- batch: 050 ----
mean loss: 130.76
 ---- batch: 060 ----
mean loss: 129.32
 ---- batch: 070 ----
mean loss: 126.04
 ---- batch: 080 ----
mean loss: 127.68
 ---- batch: 090 ----
mean loss: 126.05
train mean loss: 125.58
epoch train time: 0:00:00.503291
elapsed time: 0:02:22.591815
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 23:27:48.878391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.37
 ---- batch: 020 ----
mean loss: 118.24
 ---- batch: 030 ----
mean loss: 123.82
 ---- batch: 040 ----
mean loss: 120.83
 ---- batch: 050 ----
mean loss: 123.92
 ---- batch: 060 ----
mean loss: 127.52
 ---- batch: 070 ----
mean loss: 128.97
 ---- batch: 080 ----
mean loss: 133.33
 ---- batch: 090 ----
mean loss: 128.54
train mean loss: 125.72
epoch train time: 0:00:00.525585
elapsed time: 0:02:23.117555
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 23:27:49.404126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.08
 ---- batch: 020 ----
mean loss: 122.21
 ---- batch: 030 ----
mean loss: 123.09
 ---- batch: 040 ----
mean loss: 122.87
 ---- batch: 050 ----
mean loss: 123.89
 ---- batch: 060 ----
mean loss: 126.90
 ---- batch: 070 ----
mean loss: 126.37
 ---- batch: 080 ----
mean loss: 130.00
 ---- batch: 090 ----
mean loss: 125.23
train mean loss: 125.66
epoch train time: 0:00:00.510069
elapsed time: 0:02:23.627779
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 23:27:49.914349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.84
 ---- batch: 020 ----
mean loss: 123.37
 ---- batch: 030 ----
mean loss: 128.87
 ---- batch: 040 ----
mean loss: 123.88
 ---- batch: 050 ----
mean loss: 125.04
 ---- batch: 060 ----
mean loss: 124.83
 ---- batch: 070 ----
mean loss: 128.74
 ---- batch: 080 ----
mean loss: 122.84
 ---- batch: 090 ----
mean loss: 127.84
train mean loss: 125.34
epoch train time: 0:00:00.504263
elapsed time: 0:02:24.132194
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 23:27:50.418762
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.44
 ---- batch: 020 ----
mean loss: 126.69
 ---- batch: 030 ----
mean loss: 127.76
 ---- batch: 040 ----
mean loss: 119.62
 ---- batch: 050 ----
mean loss: 127.89
 ---- batch: 060 ----
mean loss: 125.75
 ---- batch: 070 ----
mean loss: 125.62
 ---- batch: 080 ----
mean loss: 128.85
 ---- batch: 090 ----
mean loss: 125.67
train mean loss: 125.35
epoch train time: 0:00:00.508870
elapsed time: 0:02:24.641219
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 23:27:50.927795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.13
 ---- batch: 020 ----
mean loss: 126.15
 ---- batch: 030 ----
mean loss: 127.54
 ---- batch: 040 ----
mean loss: 125.40
 ---- batch: 050 ----
mean loss: 124.77
 ---- batch: 060 ----
mean loss: 126.79
 ---- batch: 070 ----
mean loss: 122.00
 ---- batch: 080 ----
mean loss: 130.36
 ---- batch: 090 ----
mean loss: 131.52
train mean loss: 125.29
epoch train time: 0:00:00.520779
elapsed time: 0:02:25.162151
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 23:27:51.448746
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.45
 ---- batch: 020 ----
mean loss: 126.25
 ---- batch: 030 ----
mean loss: 124.53
 ---- batch: 040 ----
mean loss: 132.65
 ---- batch: 050 ----
mean loss: 125.62
 ---- batch: 060 ----
mean loss: 119.11
 ---- batch: 070 ----
mean loss: 118.89
 ---- batch: 080 ----
mean loss: 129.86
 ---- batch: 090 ----
mean loss: 125.15
train mean loss: 125.39
epoch train time: 0:00:00.516375
elapsed time: 0:02:25.678698
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 23:27:51.965267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.57
 ---- batch: 020 ----
mean loss: 122.68
 ---- batch: 030 ----
mean loss: 122.06
 ---- batch: 040 ----
mean loss: 125.27
 ---- batch: 050 ----
mean loss: 125.78
 ---- batch: 060 ----
mean loss: 124.90
 ---- batch: 070 ----
mean loss: 123.31
 ---- batch: 080 ----
mean loss: 131.25
 ---- batch: 090 ----
mean loss: 127.67
train mean loss: 125.31
epoch train time: 0:00:00.501771
elapsed time: 0:02:26.180637
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 23:27:52.467196
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.70
 ---- batch: 020 ----
mean loss: 120.89
 ---- batch: 030 ----
mean loss: 123.18
 ---- batch: 040 ----
mean loss: 124.13
 ---- batch: 050 ----
mean loss: 128.03
 ---- batch: 060 ----
mean loss: 130.98
 ---- batch: 070 ----
mean loss: 124.56
 ---- batch: 080 ----
mean loss: 128.98
 ---- batch: 090 ----
mean loss: 122.19
train mean loss: 125.08
epoch train time: 0:00:00.513907
elapsed time: 0:02:26.694700
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 23:27:52.981268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.19
 ---- batch: 020 ----
mean loss: 125.41
 ---- batch: 030 ----
mean loss: 127.98
 ---- batch: 040 ----
mean loss: 123.74
 ---- batch: 050 ----
mean loss: 116.72
 ---- batch: 060 ----
mean loss: 125.76
 ---- batch: 070 ----
mean loss: 126.26
 ---- batch: 080 ----
mean loss: 129.67
 ---- batch: 090 ----
mean loss: 126.66
train mean loss: 125.35
epoch train time: 0:00:00.503604
elapsed time: 0:02:27.198460
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 23:27:53.485029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.52
 ---- batch: 020 ----
mean loss: 126.79
 ---- batch: 030 ----
mean loss: 124.68
 ---- batch: 040 ----
mean loss: 123.85
 ---- batch: 050 ----
mean loss: 124.03
 ---- batch: 060 ----
mean loss: 121.43
 ---- batch: 070 ----
mean loss: 126.45
 ---- batch: 080 ----
mean loss: 126.21
 ---- batch: 090 ----
mean loss: 131.11
train mean loss: 125.35
epoch train time: 0:00:00.510634
elapsed time: 0:02:27.709249
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 23:27:53.995835
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.93
 ---- batch: 020 ----
mean loss: 120.16
 ---- batch: 030 ----
mean loss: 125.94
 ---- batch: 040 ----
mean loss: 120.23
 ---- batch: 050 ----
mean loss: 125.85
 ---- batch: 060 ----
mean loss: 122.50
 ---- batch: 070 ----
mean loss: 129.67
 ---- batch: 080 ----
mean loss: 130.75
 ---- batch: 090 ----
mean loss: 129.79
train mean loss: 125.16
epoch train time: 0:00:00.505666
elapsed time: 0:02:28.215080
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 23:27:54.501673
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.72
 ---- batch: 020 ----
mean loss: 130.84
 ---- batch: 030 ----
mean loss: 126.99
 ---- batch: 040 ----
mean loss: 125.02
 ---- batch: 050 ----
mean loss: 123.92
 ---- batch: 060 ----
mean loss: 121.50
 ---- batch: 070 ----
mean loss: 121.41
 ---- batch: 080 ----
mean loss: 122.88
 ---- batch: 090 ----
mean loss: 128.96
train mean loss: 125.10
epoch train time: 0:00:00.513761
elapsed time: 0:02:28.729021
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 23:27:55.015616
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.81
 ---- batch: 020 ----
mean loss: 130.80
 ---- batch: 030 ----
mean loss: 123.12
 ---- batch: 040 ----
mean loss: 122.25
 ---- batch: 050 ----
mean loss: 122.47
 ---- batch: 060 ----
mean loss: 123.16
 ---- batch: 070 ----
mean loss: 122.53
 ---- batch: 080 ----
mean loss: 124.61
 ---- batch: 090 ----
mean loss: 131.82
train mean loss: 125.20
epoch train time: 0:00:00.509547
elapsed time: 0:02:29.238787
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 23:27:55.525369
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.01
 ---- batch: 020 ----
mean loss: 130.59
 ---- batch: 030 ----
mean loss: 124.66
 ---- batch: 040 ----
mean loss: 132.15
 ---- batch: 050 ----
mean loss: 132.14
 ---- batch: 060 ----
mean loss: 122.74
 ---- batch: 070 ----
mean loss: 122.36
 ---- batch: 080 ----
mean loss: 124.13
 ---- batch: 090 ----
mean loss: 116.75
train mean loss: 125.09
epoch train time: 0:00:00.511833
elapsed time: 0:02:29.750778
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 23:27:56.037348
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.41
 ---- batch: 020 ----
mean loss: 122.37
 ---- batch: 030 ----
mean loss: 132.25
 ---- batch: 040 ----
mean loss: 125.86
 ---- batch: 050 ----
mean loss: 124.56
 ---- batch: 060 ----
mean loss: 126.13
 ---- batch: 070 ----
mean loss: 126.47
 ---- batch: 080 ----
mean loss: 122.77
 ---- batch: 090 ----
mean loss: 123.28
train mean loss: 124.85
epoch train time: 0:00:00.501911
elapsed time: 0:02:30.252836
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 23:27:56.539402
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.96
 ---- batch: 020 ----
mean loss: 125.65
 ---- batch: 030 ----
mean loss: 121.79
 ---- batch: 040 ----
mean loss: 127.66
 ---- batch: 050 ----
mean loss: 125.76
 ---- batch: 060 ----
mean loss: 123.61
 ---- batch: 070 ----
mean loss: 123.57
 ---- batch: 080 ----
mean loss: 119.50
 ---- batch: 090 ----
mean loss: 132.64
train mean loss: 124.90
epoch train time: 0:00:00.509346
elapsed time: 0:02:30.762324
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 23:27:57.048890
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.99
 ---- batch: 020 ----
mean loss: 126.15
 ---- batch: 030 ----
mean loss: 127.43
 ---- batch: 040 ----
mean loss: 121.48
 ---- batch: 050 ----
mean loss: 122.32
 ---- batch: 060 ----
mean loss: 122.77
 ---- batch: 070 ----
mean loss: 123.93
 ---- batch: 080 ----
mean loss: 130.47
 ---- batch: 090 ----
mean loss: 121.35
train mean loss: 124.97
epoch train time: 0:00:00.494473
elapsed time: 0:02:31.256945
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 23:27:57.543513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.43
 ---- batch: 020 ----
mean loss: 122.16
 ---- batch: 030 ----
mean loss: 125.64
 ---- batch: 040 ----
mean loss: 126.61
 ---- batch: 050 ----
mean loss: 121.88
 ---- batch: 060 ----
mean loss: 118.56
 ---- batch: 070 ----
mean loss: 124.21
 ---- batch: 080 ----
mean loss: 131.79
 ---- batch: 090 ----
mean loss: 125.54
train mean loss: 124.87
epoch train time: 0:00:00.496755
elapsed time: 0:02:31.753844
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 23:27:58.040410
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.71
 ---- batch: 020 ----
mean loss: 123.97
 ---- batch: 030 ----
mean loss: 122.30
 ---- batch: 040 ----
mean loss: 121.45
 ---- batch: 050 ----
mean loss: 123.47
 ---- batch: 060 ----
mean loss: 126.39
 ---- batch: 070 ----
mean loss: 128.96
 ---- batch: 080 ----
mean loss: 125.97
 ---- batch: 090 ----
mean loss: 124.86
train mean loss: 124.94
epoch train time: 0:00:00.497365
elapsed time: 0:02:32.251352
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 23:27:58.537920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.09
 ---- batch: 020 ----
mean loss: 125.00
 ---- batch: 030 ----
mean loss: 123.73
 ---- batch: 040 ----
mean loss: 123.10
 ---- batch: 050 ----
mean loss: 125.04
 ---- batch: 060 ----
mean loss: 123.54
 ---- batch: 070 ----
mean loss: 119.94
 ---- batch: 080 ----
mean loss: 129.92
 ---- batch: 090 ----
mean loss: 124.06
train mean loss: 125.01
epoch train time: 0:00:00.501336
elapsed time: 0:02:32.752850
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 23:27:59.039422
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.67
 ---- batch: 020 ----
mean loss: 132.28
 ---- batch: 030 ----
mean loss: 120.08
 ---- batch: 040 ----
mean loss: 124.04
 ---- batch: 050 ----
mean loss: 125.17
 ---- batch: 060 ----
mean loss: 123.52
 ---- batch: 070 ----
mean loss: 127.50
 ---- batch: 080 ----
mean loss: 120.93
 ---- batch: 090 ----
mean loss: 131.17
train mean loss: 124.99
epoch train time: 0:00:00.498348
elapsed time: 0:02:33.251358
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 23:27:59.537921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.84
 ---- batch: 020 ----
mean loss: 126.95
 ---- batch: 030 ----
mean loss: 120.92
 ---- batch: 040 ----
mean loss: 122.90
 ---- batch: 050 ----
mean loss: 127.75
 ---- batch: 060 ----
mean loss: 127.93
 ---- batch: 070 ----
mean loss: 124.59
 ---- batch: 080 ----
mean loss: 124.16
 ---- batch: 090 ----
mean loss: 125.56
train mean loss: 124.87
epoch train time: 0:00:00.505520
elapsed time: 0:02:33.757023
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 23:28:00.043593
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.48
 ---- batch: 020 ----
mean loss: 124.19
 ---- batch: 030 ----
mean loss: 127.09
 ---- batch: 040 ----
mean loss: 126.97
 ---- batch: 050 ----
mean loss: 125.49
 ---- batch: 060 ----
mean loss: 118.21
 ---- batch: 070 ----
mean loss: 121.30
 ---- batch: 080 ----
mean loss: 127.43
 ---- batch: 090 ----
mean loss: 125.85
train mean loss: 124.75
epoch train time: 0:00:00.511427
elapsed time: 0:02:34.268597
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 23:28:00.555219
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.97
 ---- batch: 020 ----
mean loss: 117.98
 ---- batch: 030 ----
mean loss: 130.86
 ---- batch: 040 ----
mean loss: 123.17
 ---- batch: 050 ----
mean loss: 124.79
 ---- batch: 060 ----
mean loss: 123.16
 ---- batch: 070 ----
mean loss: 125.62
 ---- batch: 080 ----
mean loss: 129.71
 ---- batch: 090 ----
mean loss: 128.07
train mean loss: 124.80
epoch train time: 0:00:00.514487
elapsed time: 0:02:34.783285
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 23:28:01.069855
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.61
 ---- batch: 020 ----
mean loss: 127.78
 ---- batch: 030 ----
mean loss: 122.69
 ---- batch: 040 ----
mean loss: 123.46
 ---- batch: 050 ----
mean loss: 128.11
 ---- batch: 060 ----
mean loss: 123.65
 ---- batch: 070 ----
mean loss: 125.51
 ---- batch: 080 ----
mean loss: 123.20
 ---- batch: 090 ----
mean loss: 123.92
train mean loss: 124.71
epoch train time: 0:00:00.508657
elapsed time: 0:02:35.292099
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 23:28:01.578667
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.25
 ---- batch: 020 ----
mean loss: 125.82
 ---- batch: 030 ----
mean loss: 124.89
 ---- batch: 040 ----
mean loss: 117.37
 ---- batch: 050 ----
mean loss: 132.44
 ---- batch: 060 ----
mean loss: 122.82
 ---- batch: 070 ----
mean loss: 119.57
 ---- batch: 080 ----
mean loss: 128.81
 ---- batch: 090 ----
mean loss: 127.14
train mean loss: 124.91
epoch train time: 0:00:00.507421
elapsed time: 0:02:35.799685
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 23:28:02.086275
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.86
 ---- batch: 020 ----
mean loss: 129.18
 ---- batch: 030 ----
mean loss: 125.45
 ---- batch: 040 ----
mean loss: 118.21
 ---- batch: 050 ----
mean loss: 126.82
 ---- batch: 060 ----
mean loss: 121.46
 ---- batch: 070 ----
mean loss: 126.22
 ---- batch: 080 ----
mean loss: 124.81
 ---- batch: 090 ----
mean loss: 117.09
train mean loss: 124.69
epoch train time: 0:00:00.494823
elapsed time: 0:02:36.294705
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 23:28:02.581289
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.03
 ---- batch: 020 ----
mean loss: 121.57
 ---- batch: 030 ----
mean loss: 129.48
 ---- batch: 040 ----
mean loss: 121.74
 ---- batch: 050 ----
mean loss: 122.39
 ---- batch: 060 ----
mean loss: 122.34
 ---- batch: 070 ----
mean loss: 127.41
 ---- batch: 080 ----
mean loss: 122.66
 ---- batch: 090 ----
mean loss: 123.39
train mean loss: 124.75
epoch train time: 0:00:00.521111
elapsed time: 0:02:36.815989
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 23:28:03.102560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.18
 ---- batch: 020 ----
mean loss: 120.17
 ---- batch: 030 ----
mean loss: 122.42
 ---- batch: 040 ----
mean loss: 124.64
 ---- batch: 050 ----
mean loss: 121.76
 ---- batch: 060 ----
mean loss: 122.43
 ---- batch: 070 ----
mean loss: 126.66
 ---- batch: 080 ----
mean loss: 124.86
 ---- batch: 090 ----
mean loss: 128.51
train mean loss: 124.64
epoch train time: 0:00:00.506463
elapsed time: 0:02:37.322598
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 23:28:03.609165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.16
 ---- batch: 020 ----
mean loss: 126.11
 ---- batch: 030 ----
mean loss: 122.12
 ---- batch: 040 ----
mean loss: 120.16
 ---- batch: 050 ----
mean loss: 122.41
 ---- batch: 060 ----
mean loss: 131.69
 ---- batch: 070 ----
mean loss: 125.75
 ---- batch: 080 ----
mean loss: 119.24
 ---- batch: 090 ----
mean loss: 130.76
train mean loss: 124.83
epoch train time: 0:00:00.510492
elapsed time: 0:02:37.833257
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 23:28:04.119816
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.30
 ---- batch: 020 ----
mean loss: 131.00
 ---- batch: 030 ----
mean loss: 119.39
 ---- batch: 040 ----
mean loss: 119.44
 ---- batch: 050 ----
mean loss: 124.19
 ---- batch: 060 ----
mean loss: 124.93
 ---- batch: 070 ----
mean loss: 129.03
 ---- batch: 080 ----
mean loss: 122.22
 ---- batch: 090 ----
mean loss: 129.13
train mean loss: 124.63
epoch train time: 0:00:00.499869
elapsed time: 0:02:38.333258
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 23:28:04.619823
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.74
 ---- batch: 020 ----
mean loss: 126.32
 ---- batch: 030 ----
mean loss: 117.99
 ---- batch: 040 ----
mean loss: 122.38
 ---- batch: 050 ----
mean loss: 129.40
 ---- batch: 060 ----
mean loss: 126.75
 ---- batch: 070 ----
mean loss: 126.47
 ---- batch: 080 ----
mean loss: 126.94
 ---- batch: 090 ----
mean loss: 126.96
train mean loss: 124.45
epoch train time: 0:00:00.517581
elapsed time: 0:02:38.851028
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 23:28:05.137629
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.76
 ---- batch: 020 ----
mean loss: 122.99
 ---- batch: 030 ----
mean loss: 123.83
 ---- batch: 040 ----
mean loss: 127.96
 ---- batch: 050 ----
mean loss: 122.46
 ---- batch: 060 ----
mean loss: 125.37
 ---- batch: 070 ----
mean loss: 127.35
 ---- batch: 080 ----
mean loss: 118.13
 ---- batch: 090 ----
mean loss: 127.51
train mean loss: 124.48
epoch train time: 0:00:00.514922
elapsed time: 0:02:39.366130
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 23:28:05.652698
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.35
 ---- batch: 020 ----
mean loss: 122.17
 ---- batch: 030 ----
mean loss: 123.49
 ---- batch: 040 ----
mean loss: 127.67
 ---- batch: 050 ----
mean loss: 128.33
 ---- batch: 060 ----
mean loss: 129.71
 ---- batch: 070 ----
mean loss: 120.74
 ---- batch: 080 ----
mean loss: 126.62
 ---- batch: 090 ----
mean loss: 123.27
train mean loss: 124.27
epoch train time: 0:00:00.511729
elapsed time: 0:02:39.878017
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 23:28:06.164587
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.52
 ---- batch: 020 ----
mean loss: 115.63
 ---- batch: 030 ----
mean loss: 121.19
 ---- batch: 040 ----
mean loss: 126.03
 ---- batch: 050 ----
mean loss: 132.30
 ---- batch: 060 ----
mean loss: 127.04
 ---- batch: 070 ----
mean loss: 133.34
 ---- batch: 080 ----
mean loss: 122.23
 ---- batch: 090 ----
mean loss: 120.65
train mean loss: 124.42
epoch train time: 0:00:00.503593
elapsed time: 0:02:40.381755
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 23:28:06.668338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.02
 ---- batch: 020 ----
mean loss: 127.30
 ---- batch: 030 ----
mean loss: 122.21
 ---- batch: 040 ----
mean loss: 116.88
 ---- batch: 050 ----
mean loss: 129.19
 ---- batch: 060 ----
mean loss: 126.59
 ---- batch: 070 ----
mean loss: 124.83
 ---- batch: 080 ----
mean loss: 128.97
 ---- batch: 090 ----
mean loss: 126.70
train mean loss: 124.33
epoch train time: 0:00:00.506801
elapsed time: 0:02:40.891965
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_8/checkpoint.pth.tar
**** end time: 2019-09-25 23:28:07.178499 ****
