Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_6', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 13275
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-26 21:34:11.970769 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 11, 11]           1,120
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
    BayesianConv2d-4            [-1, 14, 4, 11]             448
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
    BayesianLinear-8                    [-1, 1]             616
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,184
Trainable params: 2,184
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 21:34:11.982112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4884.97
 ---- batch: 020 ----
mean loss: 4772.64
 ---- batch: 030 ----
mean loss: 4615.03
 ---- batch: 040 ----
mean loss: 4410.54
 ---- batch: 050 ----
mean loss: 4204.51
 ---- batch: 060 ----
mean loss: 3953.79
 ---- batch: 070 ----
mean loss: 3783.49
 ---- batch: 080 ----
mean loss: 3574.62
 ---- batch: 090 ----
mean loss: 3361.82
 ---- batch: 100 ----
mean loss: 3196.74
 ---- batch: 110 ----
mean loss: 3030.75
train mean loss: 3951.86
epoch train time: 0:00:34.989261
elapsed time: 0:00:35.003854
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 21:34:46.974666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2771.03
 ---- batch: 020 ----
mean loss: 2594.18
 ---- batch: 030 ----
mean loss: 2443.18
 ---- batch: 040 ----
mean loss: 2288.14
 ---- batch: 050 ----
mean loss: 2172.02
 ---- batch: 060 ----
mean loss: 2025.47
 ---- batch: 070 ----
mean loss: 1894.64
 ---- batch: 080 ----
mean loss: 1802.79
 ---- batch: 090 ----
mean loss: 1681.27
 ---- batch: 100 ----
mean loss: 1575.86
 ---- batch: 110 ----
mean loss: 1503.45
train mean loss: 2052.60
epoch train time: 0:00:02.476082
elapsed time: 0:00:37.480149
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 21:34:49.451161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1407.48
 ---- batch: 020 ----
mean loss: 1350.27
 ---- batch: 030 ----
mean loss: 1290.22
 ---- batch: 040 ----
mean loss: 1235.28
 ---- batch: 050 ----
mean loss: 1177.03
 ---- batch: 060 ----
mean loss: 1126.64
 ---- batch: 070 ----
mean loss: 1112.54
 ---- batch: 080 ----
mean loss: 1068.43
 ---- batch: 090 ----
mean loss: 1044.19
 ---- batch: 100 ----
mean loss: 1008.54
 ---- batch: 110 ----
mean loss: 992.33
train mean loss: 1159.48
epoch train time: 0:00:02.460613
elapsed time: 0:00:39.941168
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 21:34:51.912176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 972.29
 ---- batch: 020 ----
mean loss: 948.10
 ---- batch: 030 ----
mean loss: 948.06
 ---- batch: 040 ----
mean loss: 922.23
 ---- batch: 050 ----
mean loss: 899.74
 ---- batch: 060 ----
mean loss: 902.80
 ---- batch: 070 ----
mean loss: 896.11
 ---- batch: 080 ----
mean loss: 875.62
 ---- batch: 090 ----
mean loss: 895.38
 ---- batch: 100 ----
mean loss: 891.11
 ---- batch: 110 ----
mean loss: 873.96
train mean loss: 910.32
epoch train time: 0:00:02.470908
elapsed time: 0:00:42.412495
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 21:34:54.383510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.50
 ---- batch: 020 ----
mean loss: 864.87
 ---- batch: 030 ----
mean loss: 876.54
 ---- batch: 040 ----
mean loss: 878.10
 ---- batch: 050 ----
mean loss: 873.04
 ---- batch: 060 ----
mean loss: 854.84
 ---- batch: 070 ----
mean loss: 864.97
 ---- batch: 080 ----
mean loss: 856.60
 ---- batch: 090 ----
mean loss: 860.91
 ---- batch: 100 ----
mean loss: 876.41
 ---- batch: 110 ----
mean loss: 869.03
train mean loss: 867.32
epoch train time: 0:00:02.462137
elapsed time: 0:00:44.875064
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 21:34:56.846061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.44
 ---- batch: 020 ----
mean loss: 862.81
 ---- batch: 030 ----
mean loss: 865.57
 ---- batch: 040 ----
mean loss: 842.08
 ---- batch: 050 ----
mean loss: 856.44
 ---- batch: 060 ----
mean loss: 875.02
 ---- batch: 070 ----
mean loss: 848.53
 ---- batch: 080 ----
mean loss: 879.26
 ---- batch: 090 ----
mean loss: 854.75
 ---- batch: 100 ----
mean loss: 866.60
 ---- batch: 110 ----
mean loss: 863.82
train mean loss: 860.46
epoch train time: 0:00:02.470839
elapsed time: 0:00:47.346362
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 21:34:59.317400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.90
 ---- batch: 020 ----
mean loss: 842.40
 ---- batch: 030 ----
mean loss: 839.99
 ---- batch: 040 ----
mean loss: 859.56
 ---- batch: 050 ----
mean loss: 828.87
 ---- batch: 060 ----
mean loss: 865.48
 ---- batch: 070 ----
mean loss: 845.97
 ---- batch: 080 ----
mean loss: 871.46
 ---- batch: 090 ----
mean loss: 863.89
 ---- batch: 100 ----
mean loss: 870.17
 ---- batch: 110 ----
mean loss: 856.90
train mean loss: 856.11
epoch train time: 0:00:02.465581
elapsed time: 0:00:49.812411
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 21:35:01.783404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.59
 ---- batch: 020 ----
mean loss: 836.69
 ---- batch: 030 ----
mean loss: 871.55
 ---- batch: 040 ----
mean loss: 849.28
 ---- batch: 050 ----
mean loss: 858.17
 ---- batch: 060 ----
mean loss: 859.34
 ---- batch: 070 ----
mean loss: 831.54
 ---- batch: 080 ----
mean loss: 855.14
 ---- batch: 090 ----
mean loss: 851.22
 ---- batch: 100 ----
mean loss: 854.90
 ---- batch: 110 ----
mean loss: 857.83
train mean loss: 851.45
epoch train time: 0:00:02.503857
elapsed time: 0:00:52.316738
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 21:35:04.287729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.32
 ---- batch: 020 ----
mean loss: 855.22
 ---- batch: 030 ----
mean loss: 833.42
 ---- batch: 040 ----
mean loss: 838.07
 ---- batch: 050 ----
mean loss: 853.48
 ---- batch: 060 ----
mean loss: 834.02
 ---- batch: 070 ----
mean loss: 860.34
 ---- batch: 080 ----
mean loss: 845.67
 ---- batch: 090 ----
mean loss: 837.27
 ---- batch: 100 ----
mean loss: 867.67
 ---- batch: 110 ----
mean loss: 861.53
train mean loss: 849.33
epoch train time: 0:00:02.463582
elapsed time: 0:00:54.780729
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 21:35:06.751725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.76
 ---- batch: 020 ----
mean loss: 855.85
 ---- batch: 030 ----
mean loss: 826.40
 ---- batch: 040 ----
mean loss: 859.59
 ---- batch: 050 ----
mean loss: 836.10
 ---- batch: 060 ----
mean loss: 848.82
 ---- batch: 070 ----
mean loss: 843.53
 ---- batch: 080 ----
mean loss: 878.57
 ---- batch: 090 ----
mean loss: 834.75
 ---- batch: 100 ----
mean loss: 829.55
 ---- batch: 110 ----
mean loss: 849.52
train mean loss: 844.94
epoch train time: 0:00:02.469624
elapsed time: 0:00:57.250803
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 21:35:09.221892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.47
 ---- batch: 020 ----
mean loss: 826.58
 ---- batch: 030 ----
mean loss: 841.39
 ---- batch: 040 ----
mean loss: 860.35
 ---- batch: 050 ----
mean loss: 834.81
 ---- batch: 060 ----
mean loss: 838.46
 ---- batch: 070 ----
mean loss: 841.10
 ---- batch: 080 ----
mean loss: 834.13
 ---- batch: 090 ----
mean loss: 850.74
 ---- batch: 100 ----
mean loss: 841.04
 ---- batch: 110 ----
mean loss: 836.19
train mean loss: 841.28
epoch train time: 0:00:02.470955
elapsed time: 0:00:59.722253
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 21:35:11.693172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.60
 ---- batch: 020 ----
mean loss: 818.56
 ---- batch: 030 ----
mean loss: 855.70
 ---- batch: 040 ----
mean loss: 845.36
 ---- batch: 050 ----
mean loss: 837.03
 ---- batch: 060 ----
mean loss: 830.30
 ---- batch: 070 ----
mean loss: 837.19
 ---- batch: 080 ----
mean loss: 829.35
 ---- batch: 090 ----
mean loss: 846.62
 ---- batch: 100 ----
mean loss: 834.00
 ---- batch: 110 ----
mean loss: 809.49
train mean loss: 838.36
epoch train time: 0:00:02.468624
elapsed time: 0:01:02.191262
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 21:35:14.162270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.12
 ---- batch: 020 ----
mean loss: 838.47
 ---- batch: 030 ----
mean loss: 844.38
 ---- batch: 040 ----
mean loss: 827.79
 ---- batch: 050 ----
mean loss: 834.77
 ---- batch: 060 ----
mean loss: 826.24
 ---- batch: 070 ----
mean loss: 844.44
 ---- batch: 080 ----
mean loss: 810.35
 ---- batch: 090 ----
mean loss: 831.80
 ---- batch: 100 ----
mean loss: 843.51
 ---- batch: 110 ----
mean loss: 814.21
train mean loss: 833.66
epoch train time: 0:00:02.480395
elapsed time: 0:01:04.672069
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 21:35:16.643071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.64
 ---- batch: 020 ----
mean loss: 834.13
 ---- batch: 030 ----
mean loss: 819.22
 ---- batch: 040 ----
mean loss: 814.56
 ---- batch: 050 ----
mean loss: 822.67
 ---- batch: 060 ----
mean loss: 835.20
 ---- batch: 070 ----
mean loss: 836.33
 ---- batch: 080 ----
mean loss: 834.69
 ---- batch: 090 ----
mean loss: 821.90
 ---- batch: 100 ----
mean loss: 839.84
 ---- batch: 110 ----
mean loss: 838.17
train mean loss: 829.73
epoch train time: 0:00:02.475839
elapsed time: 0:01:07.148379
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 21:35:19.119382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.07
 ---- batch: 020 ----
mean loss: 816.85
 ---- batch: 030 ----
mean loss: 837.66
 ---- batch: 040 ----
mean loss: 836.90
 ---- batch: 050 ----
mean loss: 831.85
 ---- batch: 060 ----
mean loss: 816.17
 ---- batch: 070 ----
mean loss: 820.38
 ---- batch: 080 ----
mean loss: 812.33
 ---- batch: 090 ----
mean loss: 820.40
 ---- batch: 100 ----
mean loss: 817.60
 ---- batch: 110 ----
mean loss: 843.89
train mean loss: 825.27
epoch train time: 0:00:02.467886
elapsed time: 0:01:09.616678
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 21:35:21.587746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.53
 ---- batch: 020 ----
mean loss: 825.75
 ---- batch: 030 ----
mean loss: 818.61
 ---- batch: 040 ----
mean loss: 810.35
 ---- batch: 050 ----
mean loss: 818.97
 ---- batch: 060 ----
mean loss: 829.19
 ---- batch: 070 ----
mean loss: 832.51
 ---- batch: 080 ----
mean loss: 808.52
 ---- batch: 090 ----
mean loss: 810.02
 ---- batch: 100 ----
mean loss: 831.54
 ---- batch: 110 ----
mean loss: 807.12
train mean loss: 821.15
epoch train time: 0:00:02.476873
elapsed time: 0:01:12.094056
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 21:35:24.065061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.26
 ---- batch: 020 ----
mean loss: 786.57
 ---- batch: 030 ----
mean loss: 819.13
 ---- batch: 040 ----
mean loss: 843.31
 ---- batch: 050 ----
mean loss: 840.46
 ---- batch: 060 ----
mean loss: 832.20
 ---- batch: 070 ----
mean loss: 829.81
 ---- batch: 080 ----
mean loss: 815.90
 ---- batch: 090 ----
mean loss: 805.38
 ---- batch: 100 ----
mean loss: 814.67
 ---- batch: 110 ----
mean loss: 808.47
train mean loss: 819.19
epoch train time: 0:00:02.466358
elapsed time: 0:01:14.560813
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 21:35:26.531825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 796.68
 ---- batch: 020 ----
mean loss: 819.58
 ---- batch: 030 ----
mean loss: 800.23
 ---- batch: 040 ----
mean loss: 814.84
 ---- batch: 050 ----
mean loss: 830.02
 ---- batch: 060 ----
mean loss: 795.28
 ---- batch: 070 ----
mean loss: 826.71
 ---- batch: 080 ----
mean loss: 805.17
 ---- batch: 090 ----
mean loss: 811.39
 ---- batch: 100 ----
mean loss: 826.87
 ---- batch: 110 ----
mean loss: 820.41
train mean loss: 812.99
epoch train time: 0:00:02.446334
elapsed time: 0:01:17.007594
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 21:35:28.978612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.46
 ---- batch: 020 ----
mean loss: 821.12
 ---- batch: 030 ----
mean loss: 808.41
 ---- batch: 040 ----
mean loss: 792.79
 ---- batch: 050 ----
mean loss: 798.25
 ---- batch: 060 ----
mean loss: 816.58
 ---- batch: 070 ----
mean loss: 805.01
 ---- batch: 080 ----
mean loss: 808.05
 ---- batch: 090 ----
mean loss: 814.74
 ---- batch: 100 ----
mean loss: 799.06
 ---- batch: 110 ----
mean loss: 825.56
train mean loss: 807.97
epoch train time: 0:00:02.469707
elapsed time: 0:01:19.477761
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 21:35:31.448746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 806.65
 ---- batch: 020 ----
mean loss: 813.93
 ---- batch: 030 ----
mean loss: 812.48
 ---- batch: 040 ----
mean loss: 794.72
 ---- batch: 050 ----
mean loss: 789.43
 ---- batch: 060 ----
mean loss: 808.12
 ---- batch: 070 ----
mean loss: 803.51
 ---- batch: 080 ----
mean loss: 808.27
 ---- batch: 090 ----
mean loss: 801.11
 ---- batch: 100 ----
mean loss: 801.22
 ---- batch: 110 ----
mean loss: 805.13
train mean loss: 803.01
epoch train time: 0:00:02.494536
elapsed time: 0:01:21.972663
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 21:35:33.943635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 766.96
 ---- batch: 020 ----
mean loss: 840.46
 ---- batch: 030 ----
mean loss: 799.15
 ---- batch: 040 ----
mean loss: 821.78
 ---- batch: 050 ----
mean loss: 802.52
 ---- batch: 060 ----
mean loss: 804.46
 ---- batch: 070 ----
mean loss: 795.10
 ---- batch: 080 ----
mean loss: 803.87
 ---- batch: 090 ----
mean loss: 784.34
 ---- batch: 100 ----
mean loss: 782.54
 ---- batch: 110 ----
mean loss: 779.19
train mean loss: 797.97
epoch train time: 0:00:02.474860
elapsed time: 0:01:24.447923
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 21:35:36.418836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 784.00
 ---- batch: 020 ----
mean loss: 807.78
 ---- batch: 030 ----
mean loss: 787.11
 ---- batch: 040 ----
mean loss: 810.93
 ---- batch: 050 ----
mean loss: 795.45
 ---- batch: 060 ----
mean loss: 785.33
 ---- batch: 070 ----
mean loss: 801.38
 ---- batch: 080 ----
mean loss: 787.82
 ---- batch: 090 ----
mean loss: 787.99
 ---- batch: 100 ----
mean loss: 781.45
 ---- batch: 110 ----
mean loss: 792.24
train mean loss: 792.34
epoch train time: 0:00:02.471182
elapsed time: 0:01:26.919419
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 21:35:38.890421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 772.28
 ---- batch: 020 ----
mean loss: 775.22
 ---- batch: 030 ----
mean loss: 765.22
 ---- batch: 040 ----
mean loss: 792.66
 ---- batch: 050 ----
mean loss: 808.18
 ---- batch: 060 ----
mean loss: 783.55
 ---- batch: 070 ----
mean loss: 807.35
 ---- batch: 080 ----
mean loss: 772.38
 ---- batch: 090 ----
mean loss: 783.17
 ---- batch: 100 ----
mean loss: 807.18
 ---- batch: 110 ----
mean loss: 780.99
train mean loss: 786.89
epoch train time: 0:00:02.452827
elapsed time: 0:01:29.372657
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 21:35:41.343666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 765.80
 ---- batch: 020 ----
mean loss: 783.66
 ---- batch: 030 ----
mean loss: 798.17
 ---- batch: 040 ----
mean loss: 776.83
 ---- batch: 050 ----
mean loss: 798.74
 ---- batch: 060 ----
mean loss: 774.55
 ---- batch: 070 ----
mean loss: 778.94
 ---- batch: 080 ----
mean loss: 789.68
 ---- batch: 090 ----
mean loss: 779.83
 ---- batch: 100 ----
mean loss: 786.93
 ---- batch: 110 ----
mean loss: 765.22
train mean loss: 781.77
epoch train time: 0:00:02.480099
elapsed time: 0:01:31.853191
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 21:35:43.824204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 777.86
 ---- batch: 020 ----
mean loss: 778.96
 ---- batch: 030 ----
mean loss: 765.78
 ---- batch: 040 ----
mean loss: 784.29
 ---- batch: 050 ----
mean loss: 777.85
 ---- batch: 060 ----
mean loss: 785.85
 ---- batch: 070 ----
mean loss: 755.13
 ---- batch: 080 ----
mean loss: 777.05
 ---- batch: 090 ----
mean loss: 785.29
 ---- batch: 100 ----
mean loss: 762.36
 ---- batch: 110 ----
mean loss: 781.00
train mean loss: 775.90
epoch train time: 0:00:02.477011
elapsed time: 0:01:34.330620
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 21:35:46.301664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.16
 ---- batch: 020 ----
mean loss: 770.84
 ---- batch: 030 ----
mean loss: 763.93
 ---- batch: 040 ----
mean loss: 768.63
 ---- batch: 050 ----
mean loss: 766.54
 ---- batch: 060 ----
mean loss: 776.54
 ---- batch: 070 ----
mean loss: 783.71
 ---- batch: 080 ----
mean loss: 753.61
 ---- batch: 090 ----
mean loss: 776.83
 ---- batch: 100 ----
mean loss: 760.57
 ---- batch: 110 ----
mean loss: 764.96
train mean loss: 768.39
epoch train time: 0:00:02.492573
elapsed time: 0:01:36.823684
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 21:35:48.794682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 771.48
 ---- batch: 020 ----
mean loss: 771.22
 ---- batch: 030 ----
mean loss: 774.37
 ---- batch: 040 ----
mean loss: 768.24
 ---- batch: 050 ----
mean loss: 757.70
 ---- batch: 060 ----
mean loss: 749.20
 ---- batch: 070 ----
mean loss: 748.71
 ---- batch: 080 ----
mean loss: 782.06
 ---- batch: 090 ----
mean loss: 772.32
 ---- batch: 100 ----
mean loss: 747.10
 ---- batch: 110 ----
mean loss: 754.26
train mean loss: 762.68
epoch train time: 0:00:02.454909
elapsed time: 0:01:39.279003
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 21:35:51.250019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 759.12
 ---- batch: 020 ----
mean loss: 747.40
 ---- batch: 030 ----
mean loss: 775.48
 ---- batch: 040 ----
mean loss: 776.80
 ---- batch: 050 ----
mean loss: 751.97
 ---- batch: 060 ----
mean loss: 753.55
 ---- batch: 070 ----
mean loss: 751.93
 ---- batch: 080 ----
mean loss: 750.36
 ---- batch: 090 ----
mean loss: 763.75
 ---- batch: 100 ----
mean loss: 749.16
 ---- batch: 110 ----
mean loss: 754.29
train mean loss: 756.80
epoch train time: 0:00:02.442909
elapsed time: 0:01:41.722353
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 21:35:53.693333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 718.32
 ---- batch: 020 ----
mean loss: 748.99
 ---- batch: 030 ----
mean loss: 766.15
 ---- batch: 040 ----
mean loss: 771.67
 ---- batch: 050 ----
mean loss: 768.57
 ---- batch: 060 ----
mean loss: 740.20
 ---- batch: 070 ----
mean loss: 751.90
 ---- batch: 080 ----
mean loss: 750.59
 ---- batch: 090 ----
mean loss: 734.22
 ---- batch: 100 ----
mean loss: 754.72
 ---- batch: 110 ----
mean loss: 745.68
train mean loss: 750.38
epoch train time: 0:00:02.461677
elapsed time: 0:01:44.184455
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 21:35:56.155454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 752.70
 ---- batch: 020 ----
mean loss: 729.80
 ---- batch: 030 ----
mean loss: 739.82
 ---- batch: 040 ----
mean loss: 740.62
 ---- batch: 050 ----
mean loss: 750.49
 ---- batch: 060 ----
mean loss: 741.02
 ---- batch: 070 ----
mean loss: 739.73
 ---- batch: 080 ----
mean loss: 755.24
 ---- batch: 090 ----
mean loss: 744.53
 ---- batch: 100 ----
mean loss: 749.13
 ---- batch: 110 ----
mean loss: 739.50
train mean loss: 743.24
epoch train time: 0:00:02.471046
elapsed time: 0:01:46.655921
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 21:35:58.626947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 740.41
 ---- batch: 020 ----
mean loss: 734.29
 ---- batch: 030 ----
mean loss: 741.25
 ---- batch: 040 ----
mean loss: 744.08
 ---- batch: 050 ----
mean loss: 738.74
 ---- batch: 060 ----
mean loss: 737.49
 ---- batch: 070 ----
mean loss: 704.60
 ---- batch: 080 ----
mean loss: 746.47
 ---- batch: 090 ----
mean loss: 737.42
 ---- batch: 100 ----
mean loss: 737.71
 ---- batch: 110 ----
mean loss: 739.68
train mean loss: 736.47
epoch train time: 0:00:02.484087
elapsed time: 0:01:49.140435
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 21:36:01.111441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.73
 ---- batch: 020 ----
mean loss: 717.33
 ---- batch: 030 ----
mean loss: 743.50
 ---- batch: 040 ----
mean loss: 749.72
 ---- batch: 050 ----
mean loss: 712.50
 ---- batch: 060 ----
mean loss: 729.64
 ---- batch: 070 ----
mean loss: 732.46
 ---- batch: 080 ----
mean loss: 730.16
 ---- batch: 090 ----
mean loss: 724.11
 ---- batch: 100 ----
mean loss: 728.25
 ---- batch: 110 ----
mean loss: 714.61
train mean loss: 728.93
epoch train time: 0:00:02.455411
elapsed time: 0:01:51.596257
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 21:36:03.567255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 715.20
 ---- batch: 020 ----
mean loss: 714.52
 ---- batch: 030 ----
mean loss: 735.70
 ---- batch: 040 ----
mean loss: 737.13
 ---- batch: 050 ----
mean loss: 727.20
 ---- batch: 060 ----
mean loss: 731.19
 ---- batch: 070 ----
mean loss: 697.47
 ---- batch: 080 ----
mean loss: 729.23
 ---- batch: 090 ----
mean loss: 716.41
 ---- batch: 100 ----
mean loss: 726.54
 ---- batch: 110 ----
mean loss: 713.30
train mean loss: 721.94
epoch train time: 0:00:02.453684
elapsed time: 0:01:54.050352
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 21:36:06.021347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 709.30
 ---- batch: 020 ----
mean loss: 718.50
 ---- batch: 030 ----
mean loss: 707.63
 ---- batch: 040 ----
mean loss: 713.25
 ---- batch: 050 ----
mean loss: 702.62
 ---- batch: 060 ----
mean loss: 709.17
 ---- batch: 070 ----
mean loss: 720.79
 ---- batch: 080 ----
mean loss: 723.40
 ---- batch: 090 ----
mean loss: 720.79
 ---- batch: 100 ----
mean loss: 719.91
 ---- batch: 110 ----
mean loss: 714.57
train mean loss: 713.89
epoch train time: 0:00:02.463703
elapsed time: 0:01:56.514525
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 21:36:08.485561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 706.88
 ---- batch: 020 ----
mean loss: 683.29
 ---- batch: 030 ----
mean loss: 703.06
 ---- batch: 040 ----
mean loss: 720.23
 ---- batch: 050 ----
mean loss: 710.03
 ---- batch: 060 ----
mean loss: 716.45
 ---- batch: 070 ----
mean loss: 691.54
 ---- batch: 080 ----
mean loss: 712.21
 ---- batch: 090 ----
mean loss: 712.77
 ---- batch: 100 ----
mean loss: 712.18
 ---- batch: 110 ----
mean loss: 712.87
train mean loss: 706.85
epoch train time: 0:00:02.454770
elapsed time: 0:01:58.969768
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 21:36:10.940614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.15
 ---- batch: 020 ----
mean loss: 708.95
 ---- batch: 030 ----
mean loss: 696.58
 ---- batch: 040 ----
mean loss: 699.59
 ---- batch: 050 ----
mean loss: 696.98
 ---- batch: 060 ----
mean loss: 692.42
 ---- batch: 070 ----
mean loss: 695.85
 ---- batch: 080 ----
mean loss: 698.99
 ---- batch: 090 ----
mean loss: 682.91
 ---- batch: 100 ----
mean loss: 696.24
 ---- batch: 110 ----
mean loss: 688.71
train mean loss: 699.12
epoch train time: 0:00:02.463373
elapsed time: 0:02:01.433438
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 21:36:13.404466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.50
 ---- batch: 020 ----
mean loss: 689.43
 ---- batch: 030 ----
mean loss: 701.59
 ---- batch: 040 ----
mean loss: 695.31
 ---- batch: 050 ----
mean loss: 682.71
 ---- batch: 060 ----
mean loss: 699.23
 ---- batch: 070 ----
mean loss: 684.48
 ---- batch: 080 ----
mean loss: 680.36
 ---- batch: 090 ----
mean loss: 701.30
 ---- batch: 100 ----
mean loss: 695.35
 ---- batch: 110 ----
mean loss: 674.04
train mean loss: 692.12
epoch train time: 0:00:02.498871
elapsed time: 0:02:03.932739
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 21:36:15.903744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.42
 ---- batch: 020 ----
mean loss: 694.43
 ---- batch: 030 ----
mean loss: 684.05
 ---- batch: 040 ----
mean loss: 685.67
 ---- batch: 050 ----
mean loss: 704.53
 ---- batch: 060 ----
mean loss: 669.54
 ---- batch: 070 ----
mean loss: 677.86
 ---- batch: 080 ----
mean loss: 676.24
 ---- batch: 090 ----
mean loss: 670.60
 ---- batch: 100 ----
mean loss: 680.47
 ---- batch: 110 ----
mean loss: 694.98
train mean loss: 684.08
epoch train time: 0:00:02.476631
elapsed time: 0:02:06.409800
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 21:36:18.380792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.70
 ---- batch: 020 ----
mean loss: 683.67
 ---- batch: 030 ----
mean loss: 684.04
 ---- batch: 040 ----
mean loss: 680.12
 ---- batch: 050 ----
mean loss: 656.39
 ---- batch: 060 ----
mean loss: 672.19
 ---- batch: 070 ----
mean loss: 674.01
 ---- batch: 080 ----
mean loss: 684.08
 ---- batch: 090 ----
mean loss: 667.73
 ---- batch: 100 ----
mean loss: 669.72
 ---- batch: 110 ----
mean loss: 676.78
train mean loss: 676.65
epoch train time: 0:00:02.480283
elapsed time: 0:02:08.890486
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 21:36:20.861508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.04
 ---- batch: 020 ----
mean loss: 672.82
 ---- batch: 030 ----
mean loss: 659.65
 ---- batch: 040 ----
mean loss: 668.96
 ---- batch: 050 ----
mean loss: 670.21
 ---- batch: 060 ----
mean loss: 666.60
 ---- batch: 070 ----
mean loss: 665.29
 ---- batch: 080 ----
mean loss: 675.86
 ---- batch: 090 ----
mean loss: 665.28
 ---- batch: 100 ----
mean loss: 675.43
 ---- batch: 110 ----
mean loss: 671.12
train mean loss: 668.73
epoch train time: 0:00:02.454355
elapsed time: 0:02:11.345286
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 21:36:23.316340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 660.94
 ---- batch: 020 ----
mean loss: 675.15
 ---- batch: 030 ----
mean loss: 652.41
 ---- batch: 040 ----
mean loss: 670.74
 ---- batch: 050 ----
mean loss: 669.09
 ---- batch: 060 ----
mean loss: 662.24
 ---- batch: 070 ----
mean loss: 665.29
 ---- batch: 080 ----
mean loss: 667.70
 ---- batch: 090 ----
mean loss: 648.36
 ---- batch: 100 ----
mean loss: 654.83
 ---- batch: 110 ----
mean loss: 657.70
train mean loss: 661.67
epoch train time: 0:00:02.457752
elapsed time: 0:02:13.803490
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 21:36:25.774477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.98
 ---- batch: 020 ----
mean loss: 646.45
 ---- batch: 030 ----
mean loss: 656.90
 ---- batch: 040 ----
mean loss: 659.33
 ---- batch: 050 ----
mean loss: 647.51
 ---- batch: 060 ----
mean loss: 644.66
 ---- batch: 070 ----
mean loss: 667.88
 ---- batch: 080 ----
mean loss: 670.18
 ---- batch: 090 ----
mean loss: 655.63
 ---- batch: 100 ----
mean loss: 652.49
 ---- batch: 110 ----
mean loss: 639.49
train mean loss: 654.29
epoch train time: 0:00:02.483228
elapsed time: 0:02:16.287186
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 21:36:28.258206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 650.72
 ---- batch: 020 ----
mean loss: 644.22
 ---- batch: 030 ----
mean loss: 652.01
 ---- batch: 040 ----
mean loss: 653.87
 ---- batch: 050 ----
mean loss: 652.76
 ---- batch: 060 ----
mean loss: 644.52
 ---- batch: 070 ----
mean loss: 652.87
 ---- batch: 080 ----
mean loss: 648.20
 ---- batch: 090 ----
mean loss: 644.52
 ---- batch: 100 ----
mean loss: 633.40
 ---- batch: 110 ----
mean loss: 645.30
train mean loss: 647.39
epoch train time: 0:00:02.456669
elapsed time: 0:02:18.744298
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 21:36:30.715334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 637.09
 ---- batch: 020 ----
mean loss: 633.72
 ---- batch: 030 ----
mean loss: 650.74
 ---- batch: 040 ----
mean loss: 639.16
 ---- batch: 050 ----
mean loss: 637.94
 ---- batch: 060 ----
mean loss: 636.93
 ---- batch: 070 ----
mean loss: 623.00
 ---- batch: 080 ----
mean loss: 648.15
 ---- batch: 090 ----
mean loss: 635.19
 ---- batch: 100 ----
mean loss: 640.72
 ---- batch: 110 ----
mean loss: 635.28
train mean loss: 638.45
epoch train time: 0:00:02.479933
elapsed time: 0:02:21.224677
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 21:36:33.195673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 630.70
 ---- batch: 020 ----
mean loss: 632.96
 ---- batch: 030 ----
mean loss: 628.46
 ---- batch: 040 ----
mean loss: 635.58
 ---- batch: 050 ----
mean loss: 628.60
 ---- batch: 060 ----
mean loss: 656.97
 ---- batch: 070 ----
mean loss: 638.19
 ---- batch: 080 ----
mean loss: 623.21
 ---- batch: 090 ----
mean loss: 621.81
 ---- batch: 100 ----
mean loss: 609.62
 ---- batch: 110 ----
mean loss: 627.46
train mean loss: 630.20
epoch train time: 0:00:02.428194
elapsed time: 0:02:23.653261
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 21:36:35.624262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 605.02
 ---- batch: 020 ----
mean loss: 638.93
 ---- batch: 030 ----
mean loss: 631.13
 ---- batch: 040 ----
mean loss: 625.83
 ---- batch: 050 ----
mean loss: 608.29
 ---- batch: 060 ----
mean loss: 611.15
 ---- batch: 070 ----
mean loss: 611.93
 ---- batch: 080 ----
mean loss: 614.14
 ---- batch: 090 ----
mean loss: 605.61
 ---- batch: 100 ----
mean loss: 619.36
 ---- batch: 110 ----
mean loss: 619.23
train mean loss: 616.82
epoch train time: 0:00:02.459463
elapsed time: 0:02:26.113159
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 21:36:38.084177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 604.36
 ---- batch: 020 ----
mean loss: 607.30
 ---- batch: 030 ----
mean loss: 611.24
 ---- batch: 040 ----
mean loss: 606.16
 ---- batch: 050 ----
mean loss: 603.05
 ---- batch: 060 ----
mean loss: 589.92
 ---- batch: 070 ----
mean loss: 619.29
 ---- batch: 080 ----
mean loss: 595.21
 ---- batch: 090 ----
mean loss: 606.93
 ---- batch: 100 ----
mean loss: 593.49
 ---- batch: 110 ----
mean loss: 608.50
train mean loss: 603.96
epoch train time: 0:00:02.458497
elapsed time: 0:02:28.572072
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 21:36:40.543102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.73
 ---- batch: 020 ----
mean loss: 602.43
 ---- batch: 030 ----
mean loss: 590.45
 ---- batch: 040 ----
mean loss: 580.46
 ---- batch: 050 ----
mean loss: 598.19
 ---- batch: 060 ----
mean loss: 586.37
 ---- batch: 070 ----
mean loss: 607.28
 ---- batch: 080 ----
mean loss: 590.57
 ---- batch: 090 ----
mean loss: 593.42
 ---- batch: 100 ----
mean loss: 571.32
 ---- batch: 110 ----
mean loss: 581.46
train mean loss: 590.51
epoch train time: 0:00:02.477496
elapsed time: 0:02:31.050035
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 21:36:43.021024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.97
 ---- batch: 020 ----
mean loss: 577.33
 ---- batch: 030 ----
mean loss: 587.45
 ---- batch: 040 ----
mean loss: 569.35
 ---- batch: 050 ----
mean loss: 568.40
 ---- batch: 060 ----
mean loss: 582.80
 ---- batch: 070 ----
mean loss: 586.57
 ---- batch: 080 ----
mean loss: 577.66
 ---- batch: 090 ----
mean loss: 565.61
 ---- batch: 100 ----
mean loss: 572.03
 ---- batch: 110 ----
mean loss: 564.65
train mean loss: 576.26
epoch train time: 0:00:02.451721
elapsed time: 0:02:33.502160
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 21:36:45.473182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 572.22
 ---- batch: 020 ----
mean loss: 569.27
 ---- batch: 030 ----
mean loss: 565.06
 ---- batch: 040 ----
mean loss: 567.04
 ---- batch: 050 ----
mean loss: 550.27
 ---- batch: 060 ----
mean loss: 557.41
 ---- batch: 070 ----
mean loss: 550.87
 ---- batch: 080 ----
mean loss: 556.94
 ---- batch: 090 ----
mean loss: 557.68
 ---- batch: 100 ----
mean loss: 542.67
 ---- batch: 110 ----
mean loss: 537.78
train mean loss: 557.46
epoch train time: 0:00:02.454953
elapsed time: 0:02:35.957528
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 21:36:47.928517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 546.89
 ---- batch: 020 ----
mean loss: 552.74
 ---- batch: 030 ----
mean loss: 536.06
 ---- batch: 040 ----
mean loss: 545.30
 ---- batch: 050 ----
mean loss: 547.78
 ---- batch: 060 ----
mean loss: 541.73
 ---- batch: 070 ----
mean loss: 539.04
 ---- batch: 080 ----
mean loss: 535.45
 ---- batch: 090 ----
mean loss: 535.77
 ---- batch: 100 ----
mean loss: 526.41
 ---- batch: 110 ----
mean loss: 531.32
train mean loss: 539.76
epoch train time: 0:00:02.462146
elapsed time: 0:02:38.420075
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 21:36:50.391067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 528.78
 ---- batch: 020 ----
mean loss: 532.61
 ---- batch: 030 ----
mean loss: 538.72
 ---- batch: 040 ----
mean loss: 523.54
 ---- batch: 050 ----
mean loss: 516.01
 ---- batch: 060 ----
mean loss: 530.65
 ---- batch: 070 ----
mean loss: 519.44
 ---- batch: 080 ----
mean loss: 515.29
 ---- batch: 090 ----
mean loss: 509.28
 ---- batch: 100 ----
mean loss: 526.94
 ---- batch: 110 ----
mean loss: 523.85
train mean loss: 523.91
epoch train time: 0:00:02.473911
elapsed time: 0:02:40.894431
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 21:36:52.865454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.13
 ---- batch: 020 ----
mean loss: 520.18
 ---- batch: 030 ----
mean loss: 509.80
 ---- batch: 040 ----
mean loss: 503.45
 ---- batch: 050 ----
mean loss: 500.79
 ---- batch: 060 ----
mean loss: 509.73
 ---- batch: 070 ----
mean loss: 516.43
 ---- batch: 080 ----
mean loss: 516.24
 ---- batch: 090 ----
mean loss: 497.03
 ---- batch: 100 ----
mean loss: 497.93
 ---- batch: 110 ----
mean loss: 518.85
train mean loss: 509.80
epoch train time: 0:00:02.475391
elapsed time: 0:02:43.370240
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 21:36:55.341237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.06
 ---- batch: 020 ----
mean loss: 502.71
 ---- batch: 030 ----
mean loss: 504.77
 ---- batch: 040 ----
mean loss: 488.32
 ---- batch: 050 ----
mean loss: 484.76
 ---- batch: 060 ----
mean loss: 502.02
 ---- batch: 070 ----
mean loss: 503.34
 ---- batch: 080 ----
mean loss: 498.30
 ---- batch: 090 ----
mean loss: 481.71
 ---- batch: 100 ----
mean loss: 497.98
 ---- batch: 110 ----
mean loss: 490.04
train mean loss: 496.10
epoch train time: 0:00:02.471579
elapsed time: 0:02:45.842257
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 21:36:57.813249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.30
 ---- batch: 020 ----
mean loss: 488.83
 ---- batch: 030 ----
mean loss: 493.32
 ---- batch: 040 ----
mean loss: 496.63
 ---- batch: 050 ----
mean loss: 488.86
 ---- batch: 060 ----
mean loss: 466.07
 ---- batch: 070 ----
mean loss: 473.33
 ---- batch: 080 ----
mean loss: 469.42
 ---- batch: 090 ----
mean loss: 471.32
 ---- batch: 100 ----
mean loss: 487.80
 ---- batch: 110 ----
mean loss: 474.23
train mean loss: 481.74
epoch train time: 0:00:02.472300
elapsed time: 0:02:48.315014
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 21:37:00.286000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.51
 ---- batch: 020 ----
mean loss: 474.17
 ---- batch: 030 ----
mean loss: 486.38
 ---- batch: 040 ----
mean loss: 468.13
 ---- batch: 050 ----
mean loss: 470.72
 ---- batch: 060 ----
mean loss: 459.60
 ---- batch: 070 ----
mean loss: 452.80
 ---- batch: 080 ----
mean loss: 464.16
 ---- batch: 090 ----
mean loss: 470.51
 ---- batch: 100 ----
mean loss: 454.64
 ---- batch: 110 ----
mean loss: 461.59
train mean loss: 468.14
epoch train time: 0:00:02.462785
elapsed time: 0:02:50.778199
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 21:37:02.749255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.91
 ---- batch: 020 ----
mean loss: 458.87
 ---- batch: 030 ----
mean loss: 465.09
 ---- batch: 040 ----
mean loss: 455.27
 ---- batch: 050 ----
mean loss: 463.83
 ---- batch: 060 ----
mean loss: 466.23
 ---- batch: 070 ----
mean loss: 438.04
 ---- batch: 080 ----
mean loss: 440.69
 ---- batch: 090 ----
mean loss: 448.98
 ---- batch: 100 ----
mean loss: 448.37
 ---- batch: 110 ----
mean loss: 445.81
train mean loss: 454.24
epoch train time: 0:00:02.484553
elapsed time: 0:02:53.263258
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 21:37:05.234121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.47
 ---- batch: 020 ----
mean loss: 451.76
 ---- batch: 030 ----
mean loss: 430.64
 ---- batch: 040 ----
mean loss: 437.42
 ---- batch: 050 ----
mean loss: 438.06
 ---- batch: 060 ----
mean loss: 434.10
 ---- batch: 070 ----
mean loss: 438.45
 ---- batch: 080 ----
mean loss: 423.29
 ---- batch: 090 ----
mean loss: 446.58
 ---- batch: 100 ----
mean loss: 442.88
 ---- batch: 110 ----
mean loss: 446.12
train mean loss: 440.31
epoch train time: 0:00:02.465578
elapsed time: 0:02:55.729124
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 21:37:07.700152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.37
 ---- batch: 020 ----
mean loss: 433.41
 ---- batch: 030 ----
mean loss: 429.95
 ---- batch: 040 ----
mean loss: 434.18
 ---- batch: 050 ----
mean loss: 427.01
 ---- batch: 060 ----
mean loss: 406.16
 ---- batch: 070 ----
mean loss: 423.40
 ---- batch: 080 ----
mean loss: 434.08
 ---- batch: 090 ----
mean loss: 423.35
 ---- batch: 100 ----
mean loss: 427.22
 ---- batch: 110 ----
mean loss: 427.46
train mean loss: 427.10
epoch train time: 0:00:02.479999
elapsed time: 0:02:58.209554
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 21:37:10.180559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.03
 ---- batch: 020 ----
mean loss: 420.26
 ---- batch: 030 ----
mean loss: 429.58
 ---- batch: 040 ----
mean loss: 412.91
 ---- batch: 050 ----
mean loss: 407.82
 ---- batch: 060 ----
mean loss: 412.55
 ---- batch: 070 ----
mean loss: 401.34
 ---- batch: 080 ----
mean loss: 411.95
 ---- batch: 090 ----
mean loss: 422.77
 ---- batch: 100 ----
mean loss: 417.12
 ---- batch: 110 ----
mean loss: 408.03
train mean loss: 415.62
epoch train time: 0:00:02.473360
elapsed time: 0:03:00.683359
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 21:37:12.654452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.59
 ---- batch: 020 ----
mean loss: 402.04
 ---- batch: 030 ----
mean loss: 415.07
 ---- batch: 040 ----
mean loss: 406.06
 ---- batch: 050 ----
mean loss: 412.02
 ---- batch: 060 ----
mean loss: 393.33
 ---- batch: 070 ----
mean loss: 395.71
 ---- batch: 080 ----
mean loss: 402.97
 ---- batch: 090 ----
mean loss: 397.38
 ---- batch: 100 ----
mean loss: 401.57
 ---- batch: 110 ----
mean loss: 401.61
train mean loss: 403.60
epoch train time: 0:00:02.476522
elapsed time: 0:03:03.160384
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 21:37:15.131386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.41
 ---- batch: 020 ----
mean loss: 400.19
 ---- batch: 030 ----
mean loss: 400.46
 ---- batch: 040 ----
mean loss: 387.51
 ---- batch: 050 ----
mean loss: 395.80
 ---- batch: 060 ----
mean loss: 396.66
 ---- batch: 070 ----
mean loss: 396.48
 ---- batch: 080 ----
mean loss: 387.91
 ---- batch: 090 ----
mean loss: 393.76
 ---- batch: 100 ----
mean loss: 379.68
 ---- batch: 110 ----
mean loss: 394.07
train mean loss: 392.00
epoch train time: 0:00:02.470399
elapsed time: 0:03:05.631178
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 21:37:17.602187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.90
 ---- batch: 020 ----
mean loss: 377.87
 ---- batch: 030 ----
mean loss: 377.70
 ---- batch: 040 ----
mean loss: 391.33
 ---- batch: 050 ----
mean loss: 388.80
 ---- batch: 060 ----
mean loss: 381.40
 ---- batch: 070 ----
mean loss: 364.76
 ---- batch: 080 ----
mean loss: 377.10
 ---- batch: 090 ----
mean loss: 381.89
 ---- batch: 100 ----
mean loss: 379.94
 ---- batch: 110 ----
mean loss: 382.88
train mean loss: 381.30
epoch train time: 0:00:02.457662
elapsed time: 0:03:08.089264
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 21:37:20.060354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.62
 ---- batch: 020 ----
mean loss: 362.10
 ---- batch: 030 ----
mean loss: 366.95
 ---- batch: 040 ----
mean loss: 375.49
 ---- batch: 050 ----
mean loss: 360.76
 ---- batch: 060 ----
mean loss: 368.14
 ---- batch: 070 ----
mean loss: 377.87
 ---- batch: 080 ----
mean loss: 363.35
 ---- batch: 090 ----
mean loss: 364.10
 ---- batch: 100 ----
mean loss: 370.05
 ---- batch: 110 ----
mean loss: 384.86
train mean loss: 369.23
epoch train time: 0:00:02.471733
elapsed time: 0:03:10.561549
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 21:37:22.532615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.37
 ---- batch: 020 ----
mean loss: 366.46
 ---- batch: 030 ----
mean loss: 358.92
 ---- batch: 040 ----
mean loss: 345.64
 ---- batch: 050 ----
mean loss: 356.30
 ---- batch: 060 ----
mean loss: 357.37
 ---- batch: 070 ----
mean loss: 357.29
 ---- batch: 080 ----
mean loss: 357.59
 ---- batch: 090 ----
mean loss: 370.87
 ---- batch: 100 ----
mean loss: 348.32
 ---- batch: 110 ----
mean loss: 350.83
train mean loss: 357.41
epoch train time: 0:00:02.483557
elapsed time: 0:03:13.045580
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 21:37:25.016577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.51
 ---- batch: 020 ----
mean loss: 343.64
 ---- batch: 030 ----
mean loss: 345.51
 ---- batch: 040 ----
mean loss: 350.14
 ---- batch: 050 ----
mean loss: 357.83
 ---- batch: 060 ----
mean loss: 344.30
 ---- batch: 070 ----
mean loss: 342.42
 ---- batch: 080 ----
mean loss: 341.83
 ---- batch: 090 ----
mean loss: 336.53
 ---- batch: 100 ----
mean loss: 332.17
 ---- batch: 110 ----
mean loss: 344.09
train mean loss: 345.14
epoch train time: 0:00:02.449574
elapsed time: 0:03:15.495583
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 21:37:27.466988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.99
 ---- batch: 020 ----
mean loss: 340.46
 ---- batch: 030 ----
mean loss: 340.13
 ---- batch: 040 ----
mean loss: 337.36
 ---- batch: 050 ----
mean loss: 331.09
 ---- batch: 060 ----
mean loss: 341.76
 ---- batch: 070 ----
mean loss: 333.83
 ---- batch: 080 ----
mean loss: 333.18
 ---- batch: 090 ----
mean loss: 329.74
 ---- batch: 100 ----
mean loss: 327.08
 ---- batch: 110 ----
mean loss: 319.71
train mean loss: 333.66
epoch train time: 0:00:02.483043
elapsed time: 0:03:17.979450
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 21:37:29.950435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.71
 ---- batch: 020 ----
mean loss: 327.79
 ---- batch: 030 ----
mean loss: 316.22
 ---- batch: 040 ----
mean loss: 328.89
 ---- batch: 050 ----
mean loss: 323.35
 ---- batch: 060 ----
mean loss: 315.85
 ---- batch: 070 ----
mean loss: 317.16
 ---- batch: 080 ----
mean loss: 313.38
 ---- batch: 090 ----
mean loss: 319.20
 ---- batch: 100 ----
mean loss: 319.61
 ---- batch: 110 ----
mean loss: 325.76
train mean loss: 321.84
epoch train time: 0:00:02.439992
elapsed time: 0:03:20.419839
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 21:37:32.390845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.55
 ---- batch: 020 ----
mean loss: 318.54
 ---- batch: 030 ----
mean loss: 310.28
 ---- batch: 040 ----
mean loss: 317.13
 ---- batch: 050 ----
mean loss: 313.01
 ---- batch: 060 ----
mean loss: 320.62
 ---- batch: 070 ----
mean loss: 316.13
 ---- batch: 080 ----
mean loss: 303.04
 ---- batch: 090 ----
mean loss: 305.93
 ---- batch: 100 ----
mean loss: 297.98
 ---- batch: 110 ----
mean loss: 314.43
train mean loss: 311.53
epoch train time: 0:00:02.463097
elapsed time: 0:03:22.883358
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 21:37:34.854387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.49
 ---- batch: 020 ----
mean loss: 301.46
 ---- batch: 030 ----
mean loss: 302.66
 ---- batch: 040 ----
mean loss: 297.92
 ---- batch: 050 ----
mean loss: 298.85
 ---- batch: 060 ----
mean loss: 312.26
 ---- batch: 070 ----
mean loss: 305.81
 ---- batch: 080 ----
mean loss: 296.86
 ---- batch: 090 ----
mean loss: 303.72
 ---- batch: 100 ----
mean loss: 288.67
 ---- batch: 110 ----
mean loss: 302.90
train mean loss: 302.04
epoch train time: 0:00:02.461096
elapsed time: 0:03:25.344911
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 21:37:37.315923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.44
 ---- batch: 020 ----
mean loss: 297.50
 ---- batch: 030 ----
mean loss: 300.27
 ---- batch: 040 ----
mean loss: 300.32
 ---- batch: 050 ----
mean loss: 290.46
 ---- batch: 060 ----
mean loss: 283.45
 ---- batch: 070 ----
mean loss: 300.83
 ---- batch: 080 ----
mean loss: 294.25
 ---- batch: 090 ----
mean loss: 294.94
 ---- batch: 100 ----
mean loss: 292.80
 ---- batch: 110 ----
mean loss: 293.05
train mean loss: 294.40
epoch train time: 0:00:02.458751
elapsed time: 0:03:27.804122
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 21:37:39.775174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.49
 ---- batch: 020 ----
mean loss: 284.50
 ---- batch: 030 ----
mean loss: 291.55
 ---- batch: 040 ----
mean loss: 284.91
 ---- batch: 050 ----
mean loss: 271.39
 ---- batch: 060 ----
mean loss: 285.62
 ---- batch: 070 ----
mean loss: 291.15
 ---- batch: 080 ----
mean loss: 296.20
 ---- batch: 090 ----
mean loss: 288.55
 ---- batch: 100 ----
mean loss: 290.52
 ---- batch: 110 ----
mean loss: 285.09
train mean loss: 287.27
epoch train time: 0:00:02.527066
elapsed time: 0:03:30.331655
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 21:37:42.302651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.14
 ---- batch: 020 ----
mean loss: 281.93
 ---- batch: 030 ----
mean loss: 279.56
 ---- batch: 040 ----
mean loss: 287.99
 ---- batch: 050 ----
mean loss: 288.03
 ---- batch: 060 ----
mean loss: 277.97
 ---- batch: 070 ----
mean loss: 277.58
 ---- batch: 080 ----
mean loss: 285.19
 ---- batch: 090 ----
mean loss: 282.55
 ---- batch: 100 ----
mean loss: 279.31
 ---- batch: 110 ----
mean loss: 276.78
train mean loss: 280.62
epoch train time: 0:00:02.500034
elapsed time: 0:03:32.832109
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 21:37:44.803118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.37
 ---- batch: 020 ----
mean loss: 284.53
 ---- batch: 030 ----
mean loss: 270.91
 ---- batch: 040 ----
mean loss: 272.76
 ---- batch: 050 ----
mean loss: 268.66
 ---- batch: 060 ----
mean loss: 282.63
 ---- batch: 070 ----
mean loss: 273.26
 ---- batch: 080 ----
mean loss: 281.41
 ---- batch: 090 ----
mean loss: 275.87
 ---- batch: 100 ----
mean loss: 272.03
 ---- batch: 110 ----
mean loss: 272.27
train mean loss: 275.73
epoch train time: 0:00:02.523452
elapsed time: 0:03:35.355996
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 21:37:47.326988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.10
 ---- batch: 020 ----
mean loss: 269.28
 ---- batch: 030 ----
mean loss: 266.25
 ---- batch: 040 ----
mean loss: 269.40
 ---- batch: 050 ----
mean loss: 264.26
 ---- batch: 060 ----
mean loss: 279.80
 ---- batch: 070 ----
mean loss: 276.30
 ---- batch: 080 ----
mean loss: 273.65
 ---- batch: 090 ----
mean loss: 267.28
 ---- batch: 100 ----
mean loss: 274.45
 ---- batch: 110 ----
mean loss: 263.51
train mean loss: 271.42
epoch train time: 0:00:02.532846
elapsed time: 0:03:37.889271
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 21:37:49.860264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.19
 ---- batch: 020 ----
mean loss: 270.78
 ---- batch: 030 ----
mean loss: 279.43
 ---- batch: 040 ----
mean loss: 270.69
 ---- batch: 050 ----
mean loss: 274.34
 ---- batch: 060 ----
mean loss: 262.31
 ---- batch: 070 ----
mean loss: 261.01
 ---- batch: 080 ----
mean loss: 263.61
 ---- batch: 090 ----
mean loss: 266.78
 ---- batch: 100 ----
mean loss: 263.71
 ---- batch: 110 ----
mean loss: 265.29
train mean loss: 267.80
epoch train time: 0:00:02.471994
elapsed time: 0:03:40.361664
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 21:37:52.332684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.69
 ---- batch: 020 ----
mean loss: 257.96
 ---- batch: 030 ----
mean loss: 271.36
 ---- batch: 040 ----
mean loss: 271.05
 ---- batch: 050 ----
mean loss: 263.69
 ---- batch: 060 ----
mean loss: 266.57
 ---- batch: 070 ----
mean loss: 262.33
 ---- batch: 080 ----
mean loss: 262.70
 ---- batch: 090 ----
mean loss: 264.87
 ---- batch: 100 ----
mean loss: 263.17
 ---- batch: 110 ----
mean loss: 262.97
train mean loss: 264.25
epoch train time: 0:00:02.489867
elapsed time: 0:03:42.852007
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 21:37:54.823022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.72
 ---- batch: 020 ----
mean loss: 271.84
 ---- batch: 030 ----
mean loss: 256.75
 ---- batch: 040 ----
mean loss: 261.95
 ---- batch: 050 ----
mean loss: 269.23
 ---- batch: 060 ----
mean loss: 265.17
 ---- batch: 070 ----
mean loss: 255.05
 ---- batch: 080 ----
mean loss: 258.52
 ---- batch: 090 ----
mean loss: 266.70
 ---- batch: 100 ----
mean loss: 261.20
 ---- batch: 110 ----
mean loss: 255.44
train mean loss: 262.10
epoch train time: 0:00:02.484292
elapsed time: 0:03:45.336734
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 21:37:57.307744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.98
 ---- batch: 020 ----
mean loss: 260.63
 ---- batch: 030 ----
mean loss: 253.91
 ---- batch: 040 ----
mean loss: 259.96
 ---- batch: 050 ----
mean loss: 265.30
 ---- batch: 060 ----
mean loss: 268.68
 ---- batch: 070 ----
mean loss: 259.91
 ---- batch: 080 ----
mean loss: 265.31
 ---- batch: 090 ----
mean loss: 255.09
 ---- batch: 100 ----
mean loss: 255.08
 ---- batch: 110 ----
mean loss: 256.15
train mean loss: 259.76
epoch train time: 0:00:02.492515
elapsed time: 0:03:47.829694
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 21:37:59.800737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.17
 ---- batch: 020 ----
mean loss: 259.96
 ---- batch: 030 ----
mean loss: 254.98
 ---- batch: 040 ----
mean loss: 263.96
 ---- batch: 050 ----
mean loss: 258.07
 ---- batch: 060 ----
mean loss: 256.17
 ---- batch: 070 ----
mean loss: 256.49
 ---- batch: 080 ----
mean loss: 251.84
 ---- batch: 090 ----
mean loss: 266.67
 ---- batch: 100 ----
mean loss: 244.61
 ---- batch: 110 ----
mean loss: 264.17
train mean loss: 257.31
epoch train time: 0:00:02.487740
elapsed time: 0:03:50.317902
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 21:38:02.288907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.55
 ---- batch: 020 ----
mean loss: 261.61
 ---- batch: 030 ----
mean loss: 247.22
 ---- batch: 040 ----
mean loss: 249.48
 ---- batch: 050 ----
mean loss: 261.41
 ---- batch: 060 ----
mean loss: 257.85
 ---- batch: 070 ----
mean loss: 263.40
 ---- batch: 080 ----
mean loss: 252.39
 ---- batch: 090 ----
mean loss: 255.13
 ---- batch: 100 ----
mean loss: 260.04
 ---- batch: 110 ----
mean loss: 245.70
train mean loss: 255.37
epoch train time: 0:00:02.479463
elapsed time: 0:03:52.797778
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 21:38:04.768780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.70
 ---- batch: 020 ----
mean loss: 255.67
 ---- batch: 030 ----
mean loss: 251.91
 ---- batch: 040 ----
mean loss: 252.33
 ---- batch: 050 ----
mean loss: 253.86
 ---- batch: 060 ----
mean loss: 251.57
 ---- batch: 070 ----
mean loss: 248.34
 ---- batch: 080 ----
mean loss: 265.96
 ---- batch: 090 ----
mean loss: 258.81
 ---- batch: 100 ----
mean loss: 244.92
 ---- batch: 110 ----
mean loss: 259.03
train mean loss: 254.01
epoch train time: 0:00:02.463288
elapsed time: 0:03:55.261477
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 21:38:07.232489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.31
 ---- batch: 020 ----
mean loss: 255.33
 ---- batch: 030 ----
mean loss: 256.72
 ---- batch: 040 ----
mean loss: 256.71
 ---- batch: 050 ----
mean loss: 248.01
 ---- batch: 060 ----
mean loss: 254.14
 ---- batch: 070 ----
mean loss: 255.70
 ---- batch: 080 ----
mean loss: 248.69
 ---- batch: 090 ----
mean loss: 249.98
 ---- batch: 100 ----
mean loss: 255.90
 ---- batch: 110 ----
mean loss: 250.84
train mean loss: 252.60
epoch train time: 0:00:02.482585
elapsed time: 0:03:57.744544
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 21:38:09.715568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.35
 ---- batch: 020 ----
mean loss: 256.60
 ---- batch: 030 ----
mean loss: 249.19
 ---- batch: 040 ----
mean loss: 245.48
 ---- batch: 050 ----
mean loss: 254.50
 ---- batch: 060 ----
mean loss: 249.18
 ---- batch: 070 ----
mean loss: 242.89
 ---- batch: 080 ----
mean loss: 249.04
 ---- batch: 090 ----
mean loss: 252.55
 ---- batch: 100 ----
mean loss: 254.57
 ---- batch: 110 ----
mean loss: 252.23
train mean loss: 251.16
epoch train time: 0:00:02.472095
elapsed time: 0:04:00.217058
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 21:38:12.188070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.71
 ---- batch: 020 ----
mean loss: 255.17
 ---- batch: 030 ----
mean loss: 249.12
 ---- batch: 040 ----
mean loss: 245.77
 ---- batch: 050 ----
mean loss: 244.46
 ---- batch: 060 ----
mean loss: 250.18
 ---- batch: 070 ----
mean loss: 255.25
 ---- batch: 080 ----
mean loss: 258.56
 ---- batch: 090 ----
mean loss: 250.43
 ---- batch: 100 ----
mean loss: 252.85
 ---- batch: 110 ----
mean loss: 244.46
train mean loss: 249.81
epoch train time: 0:00:02.468490
elapsed time: 0:04:02.685967
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 21:38:14.656972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.62
 ---- batch: 020 ----
mean loss: 255.03
 ---- batch: 030 ----
mean loss: 240.30
 ---- batch: 040 ----
mean loss: 254.51
 ---- batch: 050 ----
mean loss: 256.89
 ---- batch: 060 ----
mean loss: 246.29
 ---- batch: 070 ----
mean loss: 250.57
 ---- batch: 080 ----
mean loss: 253.74
 ---- batch: 090 ----
mean loss: 248.72
 ---- batch: 100 ----
mean loss: 249.54
 ---- batch: 110 ----
mean loss: 241.03
train mean loss: 248.44
epoch train time: 0:00:02.474293
elapsed time: 0:04:05.160669
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 21:38:17.131658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.75
 ---- batch: 020 ----
mean loss: 258.25
 ---- batch: 030 ----
mean loss: 244.11
 ---- batch: 040 ----
mean loss: 243.47
 ---- batch: 050 ----
mean loss: 247.99
 ---- batch: 060 ----
mean loss: 243.48
 ---- batch: 070 ----
mean loss: 238.49
 ---- batch: 080 ----
mean loss: 243.57
 ---- batch: 090 ----
mean loss: 250.48
 ---- batch: 100 ----
mean loss: 246.99
 ---- batch: 110 ----
mean loss: 254.06
train mean loss: 247.35
epoch train time: 0:00:02.455069
elapsed time: 0:04:07.616135
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 21:38:19.587136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.55
 ---- batch: 020 ----
mean loss: 244.92
 ---- batch: 030 ----
mean loss: 246.91
 ---- batch: 040 ----
mean loss: 237.21
 ---- batch: 050 ----
mean loss: 247.55
 ---- batch: 060 ----
mean loss: 246.84
 ---- batch: 070 ----
mean loss: 247.25
 ---- batch: 080 ----
mean loss: 257.12
 ---- batch: 090 ----
mean loss: 246.27
 ---- batch: 100 ----
mean loss: 238.47
 ---- batch: 110 ----
mean loss: 244.47
train mean loss: 246.44
epoch train time: 0:00:02.445231
elapsed time: 0:04:10.061791
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 21:38:22.032801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.27
 ---- batch: 020 ----
mean loss: 243.08
 ---- batch: 030 ----
mean loss: 249.47
 ---- batch: 040 ----
mean loss: 249.54
 ---- batch: 050 ----
mean loss: 249.93
 ---- batch: 060 ----
mean loss: 239.42
 ---- batch: 070 ----
mean loss: 242.54
 ---- batch: 080 ----
mean loss: 252.22
 ---- batch: 090 ----
mean loss: 247.23
 ---- batch: 100 ----
mean loss: 246.20
 ---- batch: 110 ----
mean loss: 243.96
train mean loss: 244.98
epoch train time: 0:00:02.461674
elapsed time: 0:04:12.523881
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 21:38:24.494894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.65
 ---- batch: 020 ----
mean loss: 246.88
 ---- batch: 030 ----
mean loss: 252.43
 ---- batch: 040 ----
mean loss: 248.03
 ---- batch: 050 ----
mean loss: 237.70
 ---- batch: 060 ----
mean loss: 239.24
 ---- batch: 070 ----
mean loss: 245.38
 ---- batch: 080 ----
mean loss: 243.66
 ---- batch: 090 ----
mean loss: 244.08
 ---- batch: 100 ----
mean loss: 244.29
 ---- batch: 110 ----
mean loss: 236.76
train mean loss: 244.47
epoch train time: 0:00:02.474005
elapsed time: 0:04:14.998308
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 21:38:26.969290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.60
 ---- batch: 020 ----
mean loss: 240.35
 ---- batch: 030 ----
mean loss: 238.40
 ---- batch: 040 ----
mean loss: 243.70
 ---- batch: 050 ----
mean loss: 238.64
 ---- batch: 060 ----
mean loss: 248.97
 ---- batch: 070 ----
mean loss: 247.89
 ---- batch: 080 ----
mean loss: 250.20
 ---- batch: 090 ----
mean loss: 243.34
 ---- batch: 100 ----
mean loss: 239.33
 ---- batch: 110 ----
mean loss: 243.80
train mean loss: 243.21
epoch train time: 0:00:02.452866
elapsed time: 0:04:17.451551
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 21:38:29.422543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.79
 ---- batch: 020 ----
mean loss: 251.59
 ---- batch: 030 ----
mean loss: 250.40
 ---- batch: 040 ----
mean loss: 237.49
 ---- batch: 050 ----
mean loss: 241.44
 ---- batch: 060 ----
mean loss: 242.88
 ---- batch: 070 ----
mean loss: 242.25
 ---- batch: 080 ----
mean loss: 238.64
 ---- batch: 090 ----
mean loss: 240.74
 ---- batch: 100 ----
mean loss: 230.74
 ---- batch: 110 ----
mean loss: 246.92
train mean loss: 242.52
epoch train time: 0:00:02.455230
elapsed time: 0:04:19.907180
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 21:38:31.878166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.38
 ---- batch: 020 ----
mean loss: 243.70
 ---- batch: 030 ----
mean loss: 243.05
 ---- batch: 040 ----
mean loss: 251.93
 ---- batch: 050 ----
mean loss: 232.12
 ---- batch: 060 ----
mean loss: 239.76
 ---- batch: 070 ----
mean loss: 245.80
 ---- batch: 080 ----
mean loss: 238.65
 ---- batch: 090 ----
mean loss: 236.20
 ---- batch: 100 ----
mean loss: 237.83
 ---- batch: 110 ----
mean loss: 244.44
train mean loss: 241.87
epoch train time: 0:00:02.460779
elapsed time: 0:04:22.368340
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 21:38:34.339370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.32
 ---- batch: 020 ----
mean loss: 243.59
 ---- batch: 030 ----
mean loss: 243.18
 ---- batch: 040 ----
mean loss: 243.13
 ---- batch: 050 ----
mean loss: 236.22
 ---- batch: 060 ----
mean loss: 232.53
 ---- batch: 070 ----
mean loss: 243.33
 ---- batch: 080 ----
mean loss: 237.36
 ---- batch: 090 ----
mean loss: 249.13
 ---- batch: 100 ----
mean loss: 240.53
 ---- batch: 110 ----
mean loss: 235.70
train mean loss: 240.80
epoch train time: 0:00:02.466815
elapsed time: 0:04:24.835633
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 21:38:36.806632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.86
 ---- batch: 020 ----
mean loss: 250.06
 ---- batch: 030 ----
mean loss: 236.00
 ---- batch: 040 ----
mean loss: 238.92
 ---- batch: 050 ----
mean loss: 238.31
 ---- batch: 060 ----
mean loss: 238.72
 ---- batch: 070 ----
mean loss: 244.61
 ---- batch: 080 ----
mean loss: 245.62
 ---- batch: 090 ----
mean loss: 236.69
 ---- batch: 100 ----
mean loss: 238.56
 ---- batch: 110 ----
mean loss: 237.41
train mean loss: 240.17
epoch train time: 0:00:02.454840
elapsed time: 0:04:27.290895
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 21:38:39.261901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.36
 ---- batch: 020 ----
mean loss: 242.07
 ---- batch: 030 ----
mean loss: 244.91
 ---- batch: 040 ----
mean loss: 235.34
 ---- batch: 050 ----
mean loss: 241.16
 ---- batch: 060 ----
mean loss: 240.39
 ---- batch: 070 ----
mean loss: 242.26
 ---- batch: 080 ----
mean loss: 237.45
 ---- batch: 090 ----
mean loss: 241.36
 ---- batch: 100 ----
mean loss: 237.00
 ---- batch: 110 ----
mean loss: 232.65
train mean loss: 239.13
epoch train time: 0:00:02.476848
elapsed time: 0:04:29.768160
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 21:38:41.739163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.00
 ---- batch: 020 ----
mean loss: 240.15
 ---- batch: 030 ----
mean loss: 240.70
 ---- batch: 040 ----
mean loss: 243.36
 ---- batch: 050 ----
mean loss: 245.80
 ---- batch: 060 ----
mean loss: 237.00
 ---- batch: 070 ----
mean loss: 228.73
 ---- batch: 080 ----
mean loss: 232.60
 ---- batch: 090 ----
mean loss: 237.87
 ---- batch: 100 ----
mean loss: 236.67
 ---- batch: 110 ----
mean loss: 238.54
train mean loss: 238.65
epoch train time: 0:00:02.460477
elapsed time: 0:04:32.229038
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 21:38:44.200042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.94
 ---- batch: 020 ----
mean loss: 231.34
 ---- batch: 030 ----
mean loss: 222.33
 ---- batch: 040 ----
mean loss: 244.43
 ---- batch: 050 ----
mean loss: 251.72
 ---- batch: 060 ----
mean loss: 240.27
 ---- batch: 070 ----
mean loss: 236.32
 ---- batch: 080 ----
mean loss: 234.79
 ---- batch: 090 ----
mean loss: 238.61
 ---- batch: 100 ----
mean loss: 236.37
 ---- batch: 110 ----
mean loss: 242.44
train mean loss: 237.89
epoch train time: 0:00:02.461222
elapsed time: 0:04:34.690666
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 21:38:46.661681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.90
 ---- batch: 020 ----
mean loss: 230.03
 ---- batch: 030 ----
mean loss: 235.36
 ---- batch: 040 ----
mean loss: 236.26
 ---- batch: 050 ----
mean loss: 248.23
 ---- batch: 060 ----
mean loss: 230.76
 ---- batch: 070 ----
mean loss: 236.40
 ---- batch: 080 ----
mean loss: 243.37
 ---- batch: 090 ----
mean loss: 239.93
 ---- batch: 100 ----
mean loss: 240.95
 ---- batch: 110 ----
mean loss: 239.82
train mean loss: 237.21
epoch train time: 0:00:02.476804
elapsed time: 0:04:37.167916
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 21:38:49.138749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.76
 ---- batch: 020 ----
mean loss: 236.54
 ---- batch: 030 ----
mean loss: 234.18
 ---- batch: 040 ----
mean loss: 227.65
 ---- batch: 050 ----
mean loss: 234.23
 ---- batch: 060 ----
mean loss: 239.99
 ---- batch: 070 ----
mean loss: 235.78
 ---- batch: 080 ----
mean loss: 247.78
 ---- batch: 090 ----
mean loss: 238.42
 ---- batch: 100 ----
mean loss: 234.25
 ---- batch: 110 ----
mean loss: 235.12
train mean loss: 236.09
epoch train time: 0:00:02.458494
elapsed time: 0:04:39.626634
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 21:38:51.597645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.93
 ---- batch: 020 ----
mean loss: 240.16
 ---- batch: 030 ----
mean loss: 226.31
 ---- batch: 040 ----
mean loss: 242.56
 ---- batch: 050 ----
mean loss: 229.29
 ---- batch: 060 ----
mean loss: 239.52
 ---- batch: 070 ----
mean loss: 228.24
 ---- batch: 080 ----
mean loss: 226.66
 ---- batch: 090 ----
mean loss: 234.00
 ---- batch: 100 ----
mean loss: 236.81
 ---- batch: 110 ----
mean loss: 243.87
train mean loss: 235.52
epoch train time: 0:00:02.468352
elapsed time: 0:04:42.095398
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 21:38:54.066398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.32
 ---- batch: 020 ----
mean loss: 235.58
 ---- batch: 030 ----
mean loss: 242.99
 ---- batch: 040 ----
mean loss: 231.13
 ---- batch: 050 ----
mean loss: 237.78
 ---- batch: 060 ----
mean loss: 237.11
 ---- batch: 070 ----
mean loss: 236.04
 ---- batch: 080 ----
mean loss: 234.61
 ---- batch: 090 ----
mean loss: 228.61
 ---- batch: 100 ----
mean loss: 240.86
 ---- batch: 110 ----
mean loss: 232.12
train mean loss: 235.04
epoch train time: 0:00:02.461156
elapsed time: 0:04:44.556948
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 21:38:56.527986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.10
 ---- batch: 020 ----
mean loss: 231.79
 ---- batch: 030 ----
mean loss: 237.79
 ---- batch: 040 ----
mean loss: 232.15
 ---- batch: 050 ----
mean loss: 236.79
 ---- batch: 060 ----
mean loss: 229.16
 ---- batch: 070 ----
mean loss: 231.23
 ---- batch: 080 ----
mean loss: 237.74
 ---- batch: 090 ----
mean loss: 237.53
 ---- batch: 100 ----
mean loss: 228.66
 ---- batch: 110 ----
mean loss: 236.17
train mean loss: 234.29
epoch train time: 0:00:02.468490
elapsed time: 0:04:47.025884
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 21:38:58.996883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.67
 ---- batch: 020 ----
mean loss: 231.66
 ---- batch: 030 ----
mean loss: 238.93
 ---- batch: 040 ----
mean loss: 235.85
 ---- batch: 050 ----
mean loss: 232.55
 ---- batch: 060 ----
mean loss: 240.42
 ---- batch: 070 ----
mean loss: 230.62
 ---- batch: 080 ----
mean loss: 238.97
 ---- batch: 090 ----
mean loss: 223.79
 ---- batch: 100 ----
mean loss: 231.14
 ---- batch: 110 ----
mean loss: 229.45
train mean loss: 233.84
epoch train time: 0:00:02.457149
elapsed time: 0:04:49.483431
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 21:39:01.454421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.91
 ---- batch: 020 ----
mean loss: 227.09
 ---- batch: 030 ----
mean loss: 234.84
 ---- batch: 040 ----
mean loss: 232.32
 ---- batch: 050 ----
mean loss: 226.82
 ---- batch: 060 ----
mean loss: 236.96
 ---- batch: 070 ----
mean loss: 231.67
 ---- batch: 080 ----
mean loss: 230.60
 ---- batch: 090 ----
mean loss: 228.84
 ---- batch: 100 ----
mean loss: 234.38
 ---- batch: 110 ----
mean loss: 241.14
train mean loss: 233.27
epoch train time: 0:00:02.486642
elapsed time: 0:04:51.970491
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 21:39:03.941495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.25
 ---- batch: 020 ----
mean loss: 231.80
 ---- batch: 030 ----
mean loss: 230.44
 ---- batch: 040 ----
mean loss: 229.11
 ---- batch: 050 ----
mean loss: 232.35
 ---- batch: 060 ----
mean loss: 228.08
 ---- batch: 070 ----
mean loss: 237.94
 ---- batch: 080 ----
mean loss: 225.71
 ---- batch: 090 ----
mean loss: 234.14
 ---- batch: 100 ----
mean loss: 239.59
 ---- batch: 110 ----
mean loss: 243.10
train mean loss: 232.58
epoch train time: 0:00:02.474245
elapsed time: 0:04:54.445163
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 21:39:06.416167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.95
 ---- batch: 020 ----
mean loss: 228.62
 ---- batch: 030 ----
mean loss: 232.89
 ---- batch: 040 ----
mean loss: 223.93
 ---- batch: 050 ----
mean loss: 225.70
 ---- batch: 060 ----
mean loss: 239.94
 ---- batch: 070 ----
mean loss: 232.98
 ---- batch: 080 ----
mean loss: 229.54
 ---- batch: 090 ----
mean loss: 231.42
 ---- batch: 100 ----
mean loss: 240.75
 ---- batch: 110 ----
mean loss: 231.36
train mean loss: 231.96
epoch train time: 0:00:02.473294
elapsed time: 0:04:56.919095
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 21:39:08.890087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.33
 ---- batch: 020 ----
mean loss: 241.66
 ---- batch: 030 ----
mean loss: 226.30
 ---- batch: 040 ----
mean loss: 234.55
 ---- batch: 050 ----
mean loss: 235.18
 ---- batch: 060 ----
mean loss: 229.70
 ---- batch: 070 ----
mean loss: 227.60
 ---- batch: 080 ----
mean loss: 236.88
 ---- batch: 090 ----
mean loss: 238.36
 ---- batch: 100 ----
mean loss: 222.05
 ---- batch: 110 ----
mean loss: 231.30
train mean loss: 231.72
epoch train time: 0:00:02.477409
elapsed time: 0:04:59.396917
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 21:39:11.367917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.24
 ---- batch: 020 ----
mean loss: 231.03
 ---- batch: 030 ----
mean loss: 238.93
 ---- batch: 040 ----
mean loss: 226.35
 ---- batch: 050 ----
mean loss: 239.41
 ---- batch: 060 ----
mean loss: 222.05
 ---- batch: 070 ----
mean loss: 239.14
 ---- batch: 080 ----
mean loss: 229.44
 ---- batch: 090 ----
mean loss: 236.19
 ---- batch: 100 ----
mean loss: 221.27
 ---- batch: 110 ----
mean loss: 225.68
train mean loss: 230.78
epoch train time: 0:00:02.501832
elapsed time: 0:05:01.899188
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 21:39:13.870219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.99
 ---- batch: 020 ----
mean loss: 236.50
 ---- batch: 030 ----
mean loss: 222.08
 ---- batch: 040 ----
mean loss: 223.95
 ---- batch: 050 ----
mean loss: 234.46
 ---- batch: 060 ----
mean loss: 228.06
 ---- batch: 070 ----
mean loss: 232.62
 ---- batch: 080 ----
mean loss: 231.38
 ---- batch: 090 ----
mean loss: 236.06
 ---- batch: 100 ----
mean loss: 234.27
 ---- batch: 110 ----
mean loss: 229.47
train mean loss: 230.33
epoch train time: 0:00:02.479541
elapsed time: 0:05:04.379220
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 21:39:16.350235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.71
 ---- batch: 020 ----
mean loss: 221.21
 ---- batch: 030 ----
mean loss: 235.02
 ---- batch: 040 ----
mean loss: 223.57
 ---- batch: 050 ----
mean loss: 233.39
 ---- batch: 060 ----
mean loss: 223.91
 ---- batch: 070 ----
mean loss: 238.91
 ---- batch: 080 ----
mean loss: 230.24
 ---- batch: 090 ----
mean loss: 226.21
 ---- batch: 100 ----
mean loss: 232.47
 ---- batch: 110 ----
mean loss: 236.11
train mean loss: 229.85
epoch train time: 0:00:02.464911
elapsed time: 0:05:06.844583
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 21:39:18.815594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.24
 ---- batch: 020 ----
mean loss: 228.48
 ---- batch: 030 ----
mean loss: 224.52
 ---- batch: 040 ----
mean loss: 218.47
 ---- batch: 050 ----
mean loss: 238.85
 ---- batch: 060 ----
mean loss: 229.55
 ---- batch: 070 ----
mean loss: 229.47
 ---- batch: 080 ----
mean loss: 240.98
 ---- batch: 090 ----
mean loss: 223.97
 ---- batch: 100 ----
mean loss: 222.97
 ---- batch: 110 ----
mean loss: 234.99
train mean loss: 229.55
epoch train time: 0:00:02.463070
elapsed time: 0:05:09.308067
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 21:39:21.279087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.60
 ---- batch: 020 ----
mean loss: 230.34
 ---- batch: 030 ----
mean loss: 229.42
 ---- batch: 040 ----
mean loss: 231.29
 ---- batch: 050 ----
mean loss: 217.46
 ---- batch: 060 ----
mean loss: 232.47
 ---- batch: 070 ----
mean loss: 231.27
 ---- batch: 080 ----
mean loss: 232.14
 ---- batch: 090 ----
mean loss: 235.32
 ---- batch: 100 ----
mean loss: 222.32
 ---- batch: 110 ----
mean loss: 230.38
train mean loss: 229.08
epoch train time: 0:00:02.480546
elapsed time: 0:05:11.789058
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 21:39:23.760081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.88
 ---- batch: 020 ----
mean loss: 218.38
 ---- batch: 030 ----
mean loss: 229.09
 ---- batch: 040 ----
mean loss: 219.71
 ---- batch: 050 ----
mean loss: 230.12
 ---- batch: 060 ----
mean loss: 231.48
 ---- batch: 070 ----
mean loss: 231.02
 ---- batch: 080 ----
mean loss: 232.37
 ---- batch: 090 ----
mean loss: 229.82
 ---- batch: 100 ----
mean loss: 227.87
 ---- batch: 110 ----
mean loss: 235.04
train mean loss: 228.51
epoch train time: 0:00:02.455389
elapsed time: 0:05:14.244866
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 21:39:26.215860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.39
 ---- batch: 020 ----
mean loss: 238.96
 ---- batch: 030 ----
mean loss: 224.44
 ---- batch: 040 ----
mean loss: 229.39
 ---- batch: 050 ----
mean loss: 234.82
 ---- batch: 060 ----
mean loss: 244.12
 ---- batch: 070 ----
mean loss: 230.59
 ---- batch: 080 ----
mean loss: 221.38
 ---- batch: 090 ----
mean loss: 223.89
 ---- batch: 100 ----
mean loss: 225.73
 ---- batch: 110 ----
mean loss: 220.82
train mean loss: 228.62
epoch train time: 0:00:02.450956
elapsed time: 0:05:16.696247
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 21:39:28.667296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.53
 ---- batch: 020 ----
mean loss: 237.02
 ---- batch: 030 ----
mean loss: 229.64
 ---- batch: 040 ----
mean loss: 215.27
 ---- batch: 050 ----
mean loss: 230.34
 ---- batch: 060 ----
mean loss: 226.99
 ---- batch: 070 ----
mean loss: 222.69
 ---- batch: 080 ----
mean loss: 232.49
 ---- batch: 090 ----
mean loss: 231.62
 ---- batch: 100 ----
mean loss: 219.01
 ---- batch: 110 ----
mean loss: 221.94
train mean loss: 227.44
epoch train time: 0:00:02.499225
elapsed time: 0:05:19.195929
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 21:39:31.166925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.43
 ---- batch: 020 ----
mean loss: 224.94
 ---- batch: 030 ----
mean loss: 222.71
 ---- batch: 040 ----
mean loss: 223.74
 ---- batch: 050 ----
mean loss: 223.46
 ---- batch: 060 ----
mean loss: 228.34
 ---- batch: 070 ----
mean loss: 234.07
 ---- batch: 080 ----
mean loss: 230.92
 ---- batch: 090 ----
mean loss: 228.65
 ---- batch: 100 ----
mean loss: 232.28
 ---- batch: 110 ----
mean loss: 226.56
train mean loss: 227.22
epoch train time: 0:00:02.451468
elapsed time: 0:05:21.647815
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 21:39:33.618838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.47
 ---- batch: 020 ----
mean loss: 232.54
 ---- batch: 030 ----
mean loss: 228.56
 ---- batch: 040 ----
mean loss: 224.54
 ---- batch: 050 ----
mean loss: 220.72
 ---- batch: 060 ----
mean loss: 225.05
 ---- batch: 070 ----
mean loss: 223.77
 ---- batch: 080 ----
mean loss: 239.33
 ---- batch: 090 ----
mean loss: 224.20
 ---- batch: 100 ----
mean loss: 226.05
 ---- batch: 110 ----
mean loss: 216.81
train mean loss: 226.54
epoch train time: 0:00:02.457716
elapsed time: 0:05:24.105962
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 21:39:36.076990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.03
 ---- batch: 020 ----
mean loss: 225.77
 ---- batch: 030 ----
mean loss: 230.36
 ---- batch: 040 ----
mean loss: 221.20
 ---- batch: 050 ----
mean loss: 224.80
 ---- batch: 060 ----
mean loss: 223.41
 ---- batch: 070 ----
mean loss: 225.17
 ---- batch: 080 ----
mean loss: 231.88
 ---- batch: 090 ----
mean loss: 231.59
 ---- batch: 100 ----
mean loss: 226.68
 ---- batch: 110 ----
mean loss: 224.07
train mean loss: 225.97
epoch train time: 0:00:02.490644
elapsed time: 0:05:26.597050
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 21:39:38.568047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.87
 ---- batch: 020 ----
mean loss: 218.31
 ---- batch: 030 ----
mean loss: 224.61
 ---- batch: 040 ----
mean loss: 229.13
 ---- batch: 050 ----
mean loss: 231.78
 ---- batch: 060 ----
mean loss: 236.40
 ---- batch: 070 ----
mean loss: 223.60
 ---- batch: 080 ----
mean loss: 223.52
 ---- batch: 090 ----
mean loss: 231.75
 ---- batch: 100 ----
mean loss: 219.61
 ---- batch: 110 ----
mean loss: 227.53
train mean loss: 225.86
epoch train time: 0:00:02.504351
elapsed time: 0:05:29.101798
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 21:39:41.072821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.81
 ---- batch: 020 ----
mean loss: 211.43
 ---- batch: 030 ----
mean loss: 229.17
 ---- batch: 040 ----
mean loss: 224.18
 ---- batch: 050 ----
mean loss: 221.81
 ---- batch: 060 ----
mean loss: 224.76
 ---- batch: 070 ----
mean loss: 231.73
 ---- batch: 080 ----
mean loss: 216.85
 ---- batch: 090 ----
mean loss: 234.47
 ---- batch: 100 ----
mean loss: 225.38
 ---- batch: 110 ----
mean loss: 227.18
train mean loss: 225.32
epoch train time: 0:00:02.482869
elapsed time: 0:05:31.585132
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 21:39:43.556184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.59
 ---- batch: 020 ----
mean loss: 225.40
 ---- batch: 030 ----
mean loss: 223.89
 ---- batch: 040 ----
mean loss: 227.09
 ---- batch: 050 ----
mean loss: 218.64
 ---- batch: 060 ----
mean loss: 223.98
 ---- batch: 070 ----
mean loss: 223.14
 ---- batch: 080 ----
mean loss: 225.33
 ---- batch: 090 ----
mean loss: 222.65
 ---- batch: 100 ----
mean loss: 230.24
 ---- batch: 110 ----
mean loss: 225.25
train mean loss: 224.67
epoch train time: 0:00:02.470625
elapsed time: 0:05:34.056226
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 21:39:46.027286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.10
 ---- batch: 020 ----
mean loss: 230.20
 ---- batch: 030 ----
mean loss: 221.06
 ---- batch: 040 ----
mean loss: 228.58
 ---- batch: 050 ----
mean loss: 229.95
 ---- batch: 060 ----
mean loss: 223.10
 ---- batch: 070 ----
mean loss: 226.93
 ---- batch: 080 ----
mean loss: 228.17
 ---- batch: 090 ----
mean loss: 207.16
 ---- batch: 100 ----
mean loss: 229.64
 ---- batch: 110 ----
mean loss: 216.81
train mean loss: 224.24
epoch train time: 0:00:02.467932
elapsed time: 0:05:36.524612
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 21:39:48.495603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.77
 ---- batch: 020 ----
mean loss: 215.76
 ---- batch: 030 ----
mean loss: 221.32
 ---- batch: 040 ----
mean loss: 226.39
 ---- batch: 050 ----
mean loss: 224.75
 ---- batch: 060 ----
mean loss: 230.25
 ---- batch: 070 ----
mean loss: 220.55
 ---- batch: 080 ----
mean loss: 227.68
 ---- batch: 090 ----
mean loss: 228.10
 ---- batch: 100 ----
mean loss: 224.38
 ---- batch: 110 ----
mean loss: 216.75
train mean loss: 223.86
epoch train time: 0:00:02.470202
elapsed time: 0:05:38.995223
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 21:39:50.966215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.49
 ---- batch: 020 ----
mean loss: 215.59
 ---- batch: 030 ----
mean loss: 225.29
 ---- batch: 040 ----
mean loss: 229.92
 ---- batch: 050 ----
mean loss: 232.65
 ---- batch: 060 ----
mean loss: 225.61
 ---- batch: 070 ----
mean loss: 229.91
 ---- batch: 080 ----
mean loss: 221.41
 ---- batch: 090 ----
mean loss: 222.59
 ---- batch: 100 ----
mean loss: 211.68
 ---- batch: 110 ----
mean loss: 220.07
train mean loss: 223.59
epoch train time: 0:00:02.461933
elapsed time: 0:05:41.457580
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 21:39:53.428612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.80
 ---- batch: 020 ----
mean loss: 220.22
 ---- batch: 030 ----
mean loss: 226.01
 ---- batch: 040 ----
mean loss: 228.95
 ---- batch: 050 ----
mean loss: 215.85
 ---- batch: 060 ----
mean loss: 225.72
 ---- batch: 070 ----
mean loss: 229.94
 ---- batch: 080 ----
mean loss: 229.42
 ---- batch: 090 ----
mean loss: 216.46
 ---- batch: 100 ----
mean loss: 215.98
 ---- batch: 110 ----
mean loss: 224.72
train mean loss: 223.10
epoch train time: 0:00:02.465062
elapsed time: 0:05:43.923077
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 21:39:55.894092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.51
 ---- batch: 020 ----
mean loss: 219.16
 ---- batch: 030 ----
mean loss: 218.21
 ---- batch: 040 ----
mean loss: 227.02
 ---- batch: 050 ----
mean loss: 221.83
 ---- batch: 060 ----
mean loss: 217.59
 ---- batch: 070 ----
mean loss: 224.94
 ---- batch: 080 ----
mean loss: 225.31
 ---- batch: 090 ----
mean loss: 223.17
 ---- batch: 100 ----
mean loss: 221.08
 ---- batch: 110 ----
mean loss: 222.33
train mean loss: 222.65
epoch train time: 0:00:02.474932
elapsed time: 0:05:46.398514
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 21:39:58.369401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.83
 ---- batch: 020 ----
mean loss: 221.35
 ---- batch: 030 ----
mean loss: 219.92
 ---- batch: 040 ----
mean loss: 208.87
 ---- batch: 050 ----
mean loss: 232.47
 ---- batch: 060 ----
mean loss: 224.16
 ---- batch: 070 ----
mean loss: 218.29
 ---- batch: 080 ----
mean loss: 224.25
 ---- batch: 090 ----
mean loss: 224.49
 ---- batch: 100 ----
mean loss: 221.17
 ---- batch: 110 ----
mean loss: 222.38
train mean loss: 222.16
epoch train time: 0:00:02.475428
elapsed time: 0:05:48.874247
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 21:40:00.845288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.03
 ---- batch: 020 ----
mean loss: 218.28
 ---- batch: 030 ----
mean loss: 220.02
 ---- batch: 040 ----
mean loss: 220.64
 ---- batch: 050 ----
mean loss: 231.50
 ---- batch: 060 ----
mean loss: 221.22
 ---- batch: 070 ----
mean loss: 224.79
 ---- batch: 080 ----
mean loss: 223.84
 ---- batch: 090 ----
mean loss: 209.47
 ---- batch: 100 ----
mean loss: 230.06
 ---- batch: 110 ----
mean loss: 213.27
train mean loss: 221.87
epoch train time: 0:00:02.471004
elapsed time: 0:05:51.345730
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 21:40:03.316764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.47
 ---- batch: 020 ----
mean loss: 232.83
 ---- batch: 030 ----
mean loss: 218.75
 ---- batch: 040 ----
mean loss: 221.59
 ---- batch: 050 ----
mean loss: 222.43
 ---- batch: 060 ----
mean loss: 224.80
 ---- batch: 070 ----
mean loss: 213.44
 ---- batch: 080 ----
mean loss: 221.23
 ---- batch: 090 ----
mean loss: 219.95
 ---- batch: 100 ----
mean loss: 218.80
 ---- batch: 110 ----
mean loss: 223.11
train mean loss: 221.30
epoch train time: 0:00:02.450832
elapsed time: 0:05:53.796985
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 21:40:05.767972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.67
 ---- batch: 020 ----
mean loss: 220.57
 ---- batch: 030 ----
mean loss: 218.02
 ---- batch: 040 ----
mean loss: 231.40
 ---- batch: 050 ----
mean loss: 212.07
 ---- batch: 060 ----
mean loss: 220.10
 ---- batch: 070 ----
mean loss: 229.31
 ---- batch: 080 ----
mean loss: 223.11
 ---- batch: 090 ----
mean loss: 224.32
 ---- batch: 100 ----
mean loss: 218.69
 ---- batch: 110 ----
mean loss: 215.78
train mean loss: 220.71
epoch train time: 0:00:02.458493
elapsed time: 0:05:56.255854
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 21:40:08.226837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.57
 ---- batch: 020 ----
mean loss: 220.91
 ---- batch: 030 ----
mean loss: 224.77
 ---- batch: 040 ----
mean loss: 219.96
 ---- batch: 050 ----
mean loss: 223.51
 ---- batch: 060 ----
mean loss: 220.61
 ---- batch: 070 ----
mean loss: 222.31
 ---- batch: 080 ----
mean loss: 221.04
 ---- batch: 090 ----
mean loss: 214.10
 ---- batch: 100 ----
mean loss: 224.03
 ---- batch: 110 ----
mean loss: 217.71
train mean loss: 220.67
epoch train time: 0:00:02.438052
elapsed time: 0:05:58.694317
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 21:40:10.665435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.14
 ---- batch: 020 ----
mean loss: 227.60
 ---- batch: 030 ----
mean loss: 224.36
 ---- batch: 040 ----
mean loss: 220.33
 ---- batch: 050 ----
mean loss: 217.95
 ---- batch: 060 ----
mean loss: 220.37
 ---- batch: 070 ----
mean loss: 225.49
 ---- batch: 080 ----
mean loss: 216.04
 ---- batch: 090 ----
mean loss: 209.41
 ---- batch: 100 ----
mean loss: 219.05
 ---- batch: 110 ----
mean loss: 217.15
train mean loss: 219.94
epoch train time: 0:00:02.471994
elapsed time: 0:06:01.166868
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 21:40:13.137901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.79
 ---- batch: 020 ----
mean loss: 217.98
 ---- batch: 030 ----
mean loss: 221.62
 ---- batch: 040 ----
mean loss: 216.09
 ---- batch: 050 ----
mean loss: 219.00
 ---- batch: 060 ----
mean loss: 212.88
 ---- batch: 070 ----
mean loss: 219.53
 ---- batch: 080 ----
mean loss: 215.94
 ---- batch: 090 ----
mean loss: 227.56
 ---- batch: 100 ----
mean loss: 206.94
 ---- batch: 110 ----
mean loss: 226.12
train mean loss: 219.65
epoch train time: 0:00:02.463436
elapsed time: 0:06:03.630721
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 21:40:15.601727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.15
 ---- batch: 020 ----
mean loss: 220.15
 ---- batch: 030 ----
mean loss: 228.00
 ---- batch: 040 ----
mean loss: 222.37
 ---- batch: 050 ----
mean loss: 216.22
 ---- batch: 060 ----
mean loss: 223.67
 ---- batch: 070 ----
mean loss: 218.19
 ---- batch: 080 ----
mean loss: 212.91
 ---- batch: 090 ----
mean loss: 217.06
 ---- batch: 100 ----
mean loss: 220.83
 ---- batch: 110 ----
mean loss: 214.19
train mean loss: 219.42
epoch train time: 0:00:02.467998
elapsed time: 0:06:06.099143
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 21:40:18.070170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.36
 ---- batch: 020 ----
mean loss: 226.93
 ---- batch: 030 ----
mean loss: 226.54
 ---- batch: 040 ----
mean loss: 223.39
 ---- batch: 050 ----
mean loss: 215.32
 ---- batch: 060 ----
mean loss: 216.72
 ---- batch: 070 ----
mean loss: 213.31
 ---- batch: 080 ----
mean loss: 214.87
 ---- batch: 090 ----
mean loss: 225.01
 ---- batch: 100 ----
mean loss: 214.81
 ---- batch: 110 ----
mean loss: 216.69
train mean loss: 218.97
epoch train time: 0:00:02.440402
elapsed time: 0:06:08.539958
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 21:40:20.510932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.00
 ---- batch: 020 ----
mean loss: 221.88
 ---- batch: 030 ----
mean loss: 219.65
 ---- batch: 040 ----
mean loss: 219.05
 ---- batch: 050 ----
mean loss: 211.69
 ---- batch: 060 ----
mean loss: 212.07
 ---- batch: 070 ----
mean loss: 223.81
 ---- batch: 080 ----
mean loss: 215.42
 ---- batch: 090 ----
mean loss: 222.06
 ---- batch: 100 ----
mean loss: 212.83
 ---- batch: 110 ----
mean loss: 211.84
train mean loss: 218.53
epoch train time: 0:00:02.484870
elapsed time: 0:06:11.025199
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 21:40:22.996206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.54
 ---- batch: 020 ----
mean loss: 219.59
 ---- batch: 030 ----
mean loss: 220.11
 ---- batch: 040 ----
mean loss: 221.86
 ---- batch: 050 ----
mean loss: 221.88
 ---- batch: 060 ----
mean loss: 222.61
 ---- batch: 070 ----
mean loss: 211.13
 ---- batch: 080 ----
mean loss: 219.12
 ---- batch: 090 ----
mean loss: 208.47
 ---- batch: 100 ----
mean loss: 223.17
 ---- batch: 110 ----
mean loss: 223.10
train mean loss: 218.08
epoch train time: 0:00:02.483880
elapsed time: 0:06:13.509500
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 21:40:25.480500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.54
 ---- batch: 020 ----
mean loss: 225.29
 ---- batch: 030 ----
mean loss: 214.50
 ---- batch: 040 ----
mean loss: 225.81
 ---- batch: 050 ----
mean loss: 208.59
 ---- batch: 060 ----
mean loss: 221.68
 ---- batch: 070 ----
mean loss: 208.23
 ---- batch: 080 ----
mean loss: 219.30
 ---- batch: 090 ----
mean loss: 212.75
 ---- batch: 100 ----
mean loss: 221.79
 ---- batch: 110 ----
mean loss: 212.42
train mean loss: 217.60
epoch train time: 0:00:02.464766
elapsed time: 0:06:15.974670
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 21:40:27.945715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.64
 ---- batch: 020 ----
mean loss: 226.78
 ---- batch: 030 ----
mean loss: 220.77
 ---- batch: 040 ----
mean loss: 223.91
 ---- batch: 050 ----
mean loss: 221.28
 ---- batch: 060 ----
mean loss: 218.54
 ---- batch: 070 ----
mean loss: 211.90
 ---- batch: 080 ----
mean loss: 213.86
 ---- batch: 090 ----
mean loss: 218.27
 ---- batch: 100 ----
mean loss: 204.51
 ---- batch: 110 ----
mean loss: 218.77
train mean loss: 217.51
epoch train time: 0:00:02.470131
elapsed time: 0:06:18.445242
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 21:40:30.416233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.50
 ---- batch: 020 ----
mean loss: 218.43
 ---- batch: 030 ----
mean loss: 227.86
 ---- batch: 040 ----
mean loss: 221.79
 ---- batch: 050 ----
mean loss: 223.07
 ---- batch: 060 ----
mean loss: 210.11
 ---- batch: 070 ----
mean loss: 223.03
 ---- batch: 080 ----
mean loss: 215.23
 ---- batch: 090 ----
mean loss: 224.26
 ---- batch: 100 ----
mean loss: 214.26
 ---- batch: 110 ----
mean loss: 206.81
train mean loss: 217.25
epoch train time: 0:00:02.485751
elapsed time: 0:06:20.931392
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 21:40:32.902400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.38
 ---- batch: 020 ----
mean loss: 228.97
 ---- batch: 030 ----
mean loss: 215.04
 ---- batch: 040 ----
mean loss: 212.15
 ---- batch: 050 ----
mean loss: 209.98
 ---- batch: 060 ----
mean loss: 220.37
 ---- batch: 070 ----
mean loss: 217.74
 ---- batch: 080 ----
mean loss: 220.46
 ---- batch: 090 ----
mean loss: 212.98
 ---- batch: 100 ----
mean loss: 213.80
 ---- batch: 110 ----
mean loss: 210.44
train mean loss: 216.61
epoch train time: 0:00:02.470453
elapsed time: 0:06:23.402285
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 21:40:35.373310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.84
 ---- batch: 020 ----
mean loss: 227.35
 ---- batch: 030 ----
mean loss: 216.11
 ---- batch: 040 ----
mean loss: 212.14
 ---- batch: 050 ----
mean loss: 212.12
 ---- batch: 060 ----
mean loss: 213.28
 ---- batch: 070 ----
mean loss: 214.07
 ---- batch: 080 ----
mean loss: 212.88
 ---- batch: 090 ----
mean loss: 208.89
 ---- batch: 100 ----
mean loss: 221.03
 ---- batch: 110 ----
mean loss: 227.53
train mean loss: 216.42
epoch train time: 0:00:02.462290
elapsed time: 0:06:25.865018
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 21:40:37.836037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.84
 ---- batch: 020 ----
mean loss: 211.17
 ---- batch: 030 ----
mean loss: 208.07
 ---- batch: 040 ----
mean loss: 214.23
 ---- batch: 050 ----
mean loss: 223.44
 ---- batch: 060 ----
mean loss: 210.22
 ---- batch: 070 ----
mean loss: 216.74
 ---- batch: 080 ----
mean loss: 228.04
 ---- batch: 090 ----
mean loss: 217.13
 ---- batch: 100 ----
mean loss: 222.35
 ---- batch: 110 ----
mean loss: 208.02
train mean loss: 216.20
epoch train time: 0:00:02.469029
elapsed time: 0:06:28.334500
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 21:40:40.305506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.43
 ---- batch: 020 ----
mean loss: 214.45
 ---- batch: 030 ----
mean loss: 216.56
 ---- batch: 040 ----
mean loss: 221.98
 ---- batch: 050 ----
mean loss: 213.01
 ---- batch: 060 ----
mean loss: 217.79
 ---- batch: 070 ----
mean loss: 211.51
 ---- batch: 080 ----
mean loss: 226.18
 ---- batch: 090 ----
mean loss: 211.34
 ---- batch: 100 ----
mean loss: 220.53
 ---- batch: 110 ----
mean loss: 211.47
train mean loss: 215.91
epoch train time: 0:00:02.471049
elapsed time: 0:06:30.805964
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 21:40:42.776972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.02
 ---- batch: 020 ----
mean loss: 219.78
 ---- batch: 030 ----
mean loss: 225.62
 ---- batch: 040 ----
mean loss: 212.01
 ---- batch: 050 ----
mean loss: 222.13
 ---- batch: 060 ----
mean loss: 221.12
 ---- batch: 070 ----
mean loss: 212.23
 ---- batch: 080 ----
mean loss: 216.47
 ---- batch: 090 ----
mean loss: 213.30
 ---- batch: 100 ----
mean loss: 212.79
 ---- batch: 110 ----
mean loss: 214.88
train mean loss: 215.50
epoch train time: 0:00:02.476995
elapsed time: 0:06:33.283378
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 21:40:45.254373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.64
 ---- batch: 020 ----
mean loss: 218.39
 ---- batch: 030 ----
mean loss: 212.78
 ---- batch: 040 ----
mean loss: 217.77
 ---- batch: 050 ----
mean loss: 210.00
 ---- batch: 060 ----
mean loss: 213.93
 ---- batch: 070 ----
mean loss: 221.19
 ---- batch: 080 ----
mean loss: 210.84
 ---- batch: 090 ----
mean loss: 218.04
 ---- batch: 100 ----
mean loss: 207.33
 ---- batch: 110 ----
mean loss: 217.20
train mean loss: 215.39
epoch train time: 0:00:02.474494
elapsed time: 0:06:35.758321
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 21:40:47.729325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.61
 ---- batch: 020 ----
mean loss: 220.14
 ---- batch: 030 ----
mean loss: 215.04
 ---- batch: 040 ----
mean loss: 212.13
 ---- batch: 050 ----
mean loss: 219.97
 ---- batch: 060 ----
mean loss: 210.95
 ---- batch: 070 ----
mean loss: 216.61
 ---- batch: 080 ----
mean loss: 213.70
 ---- batch: 090 ----
mean loss: 212.99
 ---- batch: 100 ----
mean loss: 221.98
 ---- batch: 110 ----
mean loss: 207.14
train mean loss: 214.89
epoch train time: 0:00:02.498547
elapsed time: 0:06:38.257280
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 21:40:50.228297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.82
 ---- batch: 020 ----
mean loss: 219.25
 ---- batch: 030 ----
mean loss: 221.55
 ---- batch: 040 ----
mean loss: 208.89
 ---- batch: 050 ----
mean loss: 217.18
 ---- batch: 060 ----
mean loss: 205.53
 ---- batch: 070 ----
mean loss: 213.20
 ---- batch: 080 ----
mean loss: 223.32
 ---- batch: 090 ----
mean loss: 217.45
 ---- batch: 100 ----
mean loss: 218.62
 ---- batch: 110 ----
mean loss: 207.13
train mean loss: 214.76
epoch train time: 0:00:02.449790
elapsed time: 0:06:40.707622
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 21:40:52.678506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.35
 ---- batch: 020 ----
mean loss: 210.48
 ---- batch: 030 ----
mean loss: 206.53
 ---- batch: 040 ----
mean loss: 201.81
 ---- batch: 050 ----
mean loss: 212.39
 ---- batch: 060 ----
mean loss: 217.90
 ---- batch: 070 ----
mean loss: 218.65
 ---- batch: 080 ----
mean loss: 218.10
 ---- batch: 090 ----
mean loss: 217.58
 ---- batch: 100 ----
mean loss: 218.58
 ---- batch: 110 ----
mean loss: 214.47
train mean loss: 214.37
epoch train time: 0:00:02.493106
elapsed time: 0:06:43.201051
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 21:40:55.172044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.01
 ---- batch: 020 ----
mean loss: 210.37
 ---- batch: 030 ----
mean loss: 220.45
 ---- batch: 040 ----
mean loss: 207.91
 ---- batch: 050 ----
mean loss: 214.41
 ---- batch: 060 ----
mean loss: 222.69
 ---- batch: 070 ----
mean loss: 209.54
 ---- batch: 080 ----
mean loss: 223.14
 ---- batch: 090 ----
mean loss: 212.17
 ---- batch: 100 ----
mean loss: 209.81
 ---- batch: 110 ----
mean loss: 212.08
train mean loss: 214.43
epoch train time: 0:00:02.458288
elapsed time: 0:06:45.659806
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 21:40:57.630840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.90
 ---- batch: 020 ----
mean loss: 206.25
 ---- batch: 030 ----
mean loss: 216.99
 ---- batch: 040 ----
mean loss: 215.73
 ---- batch: 050 ----
mean loss: 214.95
 ---- batch: 060 ----
mean loss: 217.28
 ---- batch: 070 ----
mean loss: 214.84
 ---- batch: 080 ----
mean loss: 215.27
 ---- batch: 090 ----
mean loss: 216.08
 ---- batch: 100 ----
mean loss: 217.74
 ---- batch: 110 ----
mean loss: 206.17
train mean loss: 214.00
epoch train time: 0:00:02.492105
elapsed time: 0:06:48.152369
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 21:41:00.123368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.42
 ---- batch: 020 ----
mean loss: 212.64
 ---- batch: 030 ----
mean loss: 213.25
 ---- batch: 040 ----
mean loss: 203.30
 ---- batch: 050 ----
mean loss: 214.53
 ---- batch: 060 ----
mean loss: 209.40
 ---- batch: 070 ----
mean loss: 209.23
 ---- batch: 080 ----
mean loss: 223.62
 ---- batch: 090 ----
mean loss: 218.39
 ---- batch: 100 ----
mean loss: 212.84
 ---- batch: 110 ----
mean loss: 206.74
train mean loss: 213.56
epoch train time: 0:00:02.484006
elapsed time: 0:06:50.636773
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 21:41:02.607800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.38
 ---- batch: 020 ----
mean loss: 201.55
 ---- batch: 030 ----
mean loss: 215.42
 ---- batch: 040 ----
mean loss: 210.80
 ---- batch: 050 ----
mean loss: 220.50
 ---- batch: 060 ----
mean loss: 216.57
 ---- batch: 070 ----
mean loss: 214.99
 ---- batch: 080 ----
mean loss: 211.63
 ---- batch: 090 ----
mean loss: 210.57
 ---- batch: 100 ----
mean loss: 213.51
 ---- batch: 110 ----
mean loss: 219.01
train mean loss: 213.58
epoch train time: 0:00:02.468686
elapsed time: 0:06:53.105896
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 21:41:05.076954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.23
 ---- batch: 020 ----
mean loss: 208.83
 ---- batch: 030 ----
mean loss: 216.63
 ---- batch: 040 ----
mean loss: 209.03
 ---- batch: 050 ----
mean loss: 214.10
 ---- batch: 060 ----
mean loss: 210.34
 ---- batch: 070 ----
mean loss: 212.77
 ---- batch: 080 ----
mean loss: 213.40
 ---- batch: 090 ----
mean loss: 220.02
 ---- batch: 100 ----
mean loss: 206.47
 ---- batch: 110 ----
mean loss: 220.09
train mean loss: 213.03
epoch train time: 0:00:02.448346
elapsed time: 0:06:55.554726
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 21:41:07.525765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.81
 ---- batch: 020 ----
mean loss: 210.03
 ---- batch: 030 ----
mean loss: 212.69
 ---- batch: 040 ----
mean loss: 216.54
 ---- batch: 050 ----
mean loss: 206.58
 ---- batch: 060 ----
mean loss: 207.80
 ---- batch: 070 ----
mean loss: 215.57
 ---- batch: 080 ----
mean loss: 213.32
 ---- batch: 090 ----
mean loss: 216.91
 ---- batch: 100 ----
mean loss: 214.54
 ---- batch: 110 ----
mean loss: 215.23
train mean loss: 213.06
epoch train time: 0:00:02.463062
elapsed time: 0:06:58.018218
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 21:41:09.989201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.03
 ---- batch: 020 ----
mean loss: 209.80
 ---- batch: 030 ----
mean loss: 219.12
 ---- batch: 040 ----
mean loss: 225.40
 ---- batch: 050 ----
mean loss: 206.92
 ---- batch: 060 ----
mean loss: 213.08
 ---- batch: 070 ----
mean loss: 214.55
 ---- batch: 080 ----
mean loss: 209.60
 ---- batch: 090 ----
mean loss: 204.86
 ---- batch: 100 ----
mean loss: 214.16
 ---- batch: 110 ----
mean loss: 209.42
train mean loss: 212.97
epoch train time: 0:00:02.450176
elapsed time: 0:07:00.468837
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 21:41:12.439836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.59
 ---- batch: 020 ----
mean loss: 211.42
 ---- batch: 030 ----
mean loss: 215.59
 ---- batch: 040 ----
mean loss: 210.65
 ---- batch: 050 ----
mean loss: 204.16
 ---- batch: 060 ----
mean loss: 220.35
 ---- batch: 070 ----
mean loss: 205.05
 ---- batch: 080 ----
mean loss: 214.69
 ---- batch: 090 ----
mean loss: 214.90
 ---- batch: 100 ----
mean loss: 213.81
 ---- batch: 110 ----
mean loss: 216.61
train mean loss: 212.71
epoch train time: 0:00:02.475317
elapsed time: 0:07:02.944692
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 21:41:14.915780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.13
 ---- batch: 020 ----
mean loss: 211.04
 ---- batch: 030 ----
mean loss: 215.21
 ---- batch: 040 ----
mean loss: 205.25
 ---- batch: 050 ----
mean loss: 208.24
 ---- batch: 060 ----
mean loss: 209.53
 ---- batch: 070 ----
mean loss: 212.56
 ---- batch: 080 ----
mean loss: 206.13
 ---- batch: 090 ----
mean loss: 205.40
 ---- batch: 100 ----
mean loss: 216.33
 ---- batch: 110 ----
mean loss: 223.46
train mean loss: 212.28
epoch train time: 0:00:02.453842
elapsed time: 0:07:05.399056
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 21:41:17.370072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.07
 ---- batch: 020 ----
mean loss: 210.01
 ---- batch: 030 ----
mean loss: 207.67
 ---- batch: 040 ----
mean loss: 212.63
 ---- batch: 050 ----
mean loss: 215.12
 ---- batch: 060 ----
mean loss: 207.71
 ---- batch: 070 ----
mean loss: 217.09
 ---- batch: 080 ----
mean loss: 211.59
 ---- batch: 090 ----
mean loss: 205.81
 ---- batch: 100 ----
mean loss: 223.27
 ---- batch: 110 ----
mean loss: 208.68
train mean loss: 212.20
epoch train time: 0:00:02.474773
elapsed time: 0:07:07.874289
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 21:41:19.845281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.56
 ---- batch: 020 ----
mean loss: 212.74
 ---- batch: 030 ----
mean loss: 207.84
 ---- batch: 040 ----
mean loss: 212.85
 ---- batch: 050 ----
mean loss: 229.65
 ---- batch: 060 ----
mean loss: 204.63
 ---- batch: 070 ----
mean loss: 205.27
 ---- batch: 080 ----
mean loss: 220.37
 ---- batch: 090 ----
mean loss: 216.49
 ---- batch: 100 ----
mean loss: 202.52
 ---- batch: 110 ----
mean loss: 207.10
train mean loss: 211.74
epoch train time: 0:00:02.463482
elapsed time: 0:07:10.338179
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 21:41:22.309193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.20
 ---- batch: 020 ----
mean loss: 214.96
 ---- batch: 030 ----
mean loss: 213.85
 ---- batch: 040 ----
mean loss: 214.29
 ---- batch: 050 ----
mean loss: 221.40
 ---- batch: 060 ----
mean loss: 210.11
 ---- batch: 070 ----
mean loss: 208.50
 ---- batch: 080 ----
mean loss: 208.84
 ---- batch: 090 ----
mean loss: 209.78
 ---- batch: 100 ----
mean loss: 218.68
 ---- batch: 110 ----
mean loss: 208.54
train mean loss: 211.59
epoch train time: 0:00:02.489482
elapsed time: 0:07:12.828090
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 21:41:24.799094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.19
 ---- batch: 020 ----
mean loss: 216.38
 ---- batch: 030 ----
mean loss: 214.23
 ---- batch: 040 ----
mean loss: 202.87
 ---- batch: 050 ----
mean loss: 223.55
 ---- batch: 060 ----
mean loss: 213.66
 ---- batch: 070 ----
mean loss: 203.85
 ---- batch: 080 ----
mean loss: 198.98
 ---- batch: 090 ----
mean loss: 207.39
 ---- batch: 100 ----
mean loss: 212.84
 ---- batch: 110 ----
mean loss: 217.75
train mean loss: 211.76
epoch train time: 0:00:02.461514
elapsed time: 0:07:15.290041
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 21:41:27.261087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.54
 ---- batch: 020 ----
mean loss: 214.10
 ---- batch: 030 ----
mean loss: 217.45
 ---- batch: 040 ----
mean loss: 218.79
 ---- batch: 050 ----
mean loss: 212.71
 ---- batch: 060 ----
mean loss: 211.10
 ---- batch: 070 ----
mean loss: 211.66
 ---- batch: 080 ----
mean loss: 211.65
 ---- batch: 090 ----
mean loss: 201.29
 ---- batch: 100 ----
mean loss: 198.79
 ---- batch: 110 ----
mean loss: 212.25
train mean loss: 211.23
epoch train time: 0:00:02.488254
elapsed time: 0:07:17.778747
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 21:41:29.749771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.80
 ---- batch: 020 ----
mean loss: 215.19
 ---- batch: 030 ----
mean loss: 209.57
 ---- batch: 040 ----
mean loss: 212.33
 ---- batch: 050 ----
mean loss: 214.81
 ---- batch: 060 ----
mean loss: 205.36
 ---- batch: 070 ----
mean loss: 225.29
 ---- batch: 080 ----
mean loss: 206.02
 ---- batch: 090 ----
mean loss: 202.32
 ---- batch: 100 ----
mean loss: 211.15
 ---- batch: 110 ----
mean loss: 209.27
train mean loss: 211.20
epoch train time: 0:00:02.451086
elapsed time: 0:07:20.230289
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 21:41:32.201281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.30
 ---- batch: 020 ----
mean loss: 205.96
 ---- batch: 030 ----
mean loss: 213.70
 ---- batch: 040 ----
mean loss: 211.45
 ---- batch: 050 ----
mean loss: 223.14
 ---- batch: 060 ----
mean loss: 209.81
 ---- batch: 070 ----
mean loss: 207.89
 ---- batch: 080 ----
mean loss: 213.80
 ---- batch: 090 ----
mean loss: 213.46
 ---- batch: 100 ----
mean loss: 205.27
 ---- batch: 110 ----
mean loss: 200.68
train mean loss: 210.98
epoch train time: 0:00:02.427552
elapsed time: 0:07:22.658230
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 21:41:34.629217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.61
 ---- batch: 020 ----
mean loss: 211.55
 ---- batch: 030 ----
mean loss: 206.45
 ---- batch: 040 ----
mean loss: 205.49
 ---- batch: 050 ----
mean loss: 201.29
 ---- batch: 060 ----
mean loss: 212.40
 ---- batch: 070 ----
mean loss: 198.51
 ---- batch: 080 ----
mean loss: 224.26
 ---- batch: 090 ----
mean loss: 214.38
 ---- batch: 100 ----
mean loss: 224.47
 ---- batch: 110 ----
mean loss: 206.03
train mean loss: 210.78
epoch train time: 0:00:02.456788
elapsed time: 0:07:25.115408
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 21:41:37.086423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.38
 ---- batch: 020 ----
mean loss: 213.46
 ---- batch: 030 ----
mean loss: 206.49
 ---- batch: 040 ----
mean loss: 205.04
 ---- batch: 050 ----
mean loss: 212.53
 ---- batch: 060 ----
mean loss: 204.25
 ---- batch: 070 ----
mean loss: 208.17
 ---- batch: 080 ----
mean loss: 206.81
 ---- batch: 090 ----
mean loss: 215.45
 ---- batch: 100 ----
mean loss: 215.64
 ---- batch: 110 ----
mean loss: 216.43
train mean loss: 210.38
epoch train time: 0:00:02.468884
elapsed time: 0:07:27.584719
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 21:41:39.555762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.17
 ---- batch: 020 ----
mean loss: 217.66
 ---- batch: 030 ----
mean loss: 209.53
 ---- batch: 040 ----
mean loss: 219.68
 ---- batch: 050 ----
mean loss: 218.92
 ---- batch: 060 ----
mean loss: 208.19
 ---- batch: 070 ----
mean loss: 199.95
 ---- batch: 080 ----
mean loss: 209.92
 ---- batch: 090 ----
mean loss: 203.74
 ---- batch: 100 ----
mean loss: 206.82
 ---- batch: 110 ----
mean loss: 204.99
train mean loss: 210.37
epoch train time: 0:00:02.468116
elapsed time: 0:07:30.053379
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 21:41:42.024382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.70
 ---- batch: 020 ----
mean loss: 211.53
 ---- batch: 030 ----
mean loss: 196.78
 ---- batch: 040 ----
mean loss: 220.94
 ---- batch: 050 ----
mean loss: 215.75
 ---- batch: 060 ----
mean loss: 212.02
 ---- batch: 070 ----
mean loss: 210.07
 ---- batch: 080 ----
mean loss: 215.59
 ---- batch: 090 ----
mean loss: 206.57
 ---- batch: 100 ----
mean loss: 211.91
 ---- batch: 110 ----
mean loss: 204.10
train mean loss: 210.27
epoch train time: 0:00:02.463078
elapsed time: 0:07:32.516937
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 21:41:44.487969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.42
 ---- batch: 020 ----
mean loss: 215.74
 ---- batch: 030 ----
mean loss: 213.58
 ---- batch: 040 ----
mean loss: 204.95
 ---- batch: 050 ----
mean loss: 205.04
 ---- batch: 060 ----
mean loss: 217.06
 ---- batch: 070 ----
mean loss: 205.14
 ---- batch: 080 ----
mean loss: 212.40
 ---- batch: 090 ----
mean loss: 215.83
 ---- batch: 100 ----
mean loss: 204.18
 ---- batch: 110 ----
mean loss: 205.68
train mean loss: 209.93
epoch train time: 0:00:02.484181
elapsed time: 0:07:35.001591
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 21:41:46.972631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.58
 ---- batch: 020 ----
mean loss: 219.05
 ---- batch: 030 ----
mean loss: 215.05
 ---- batch: 040 ----
mean loss: 206.41
 ---- batch: 050 ----
mean loss: 211.54
 ---- batch: 060 ----
mean loss: 202.64
 ---- batch: 070 ----
mean loss: 217.20
 ---- batch: 080 ----
mean loss: 206.65
 ---- batch: 090 ----
mean loss: 211.23
 ---- batch: 100 ----
mean loss: 207.51
 ---- batch: 110 ----
mean loss: 209.30
train mean loss: 209.63
epoch train time: 0:00:02.452581
elapsed time: 0:07:37.454641
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 21:41:49.425681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.93
 ---- batch: 020 ----
mean loss: 218.83
 ---- batch: 030 ----
mean loss: 209.65
 ---- batch: 040 ----
mean loss: 203.28
 ---- batch: 050 ----
mean loss: 209.95
 ---- batch: 060 ----
mean loss: 208.57
 ---- batch: 070 ----
mean loss: 197.33
 ---- batch: 080 ----
mean loss: 215.14
 ---- batch: 090 ----
mean loss: 214.14
 ---- batch: 100 ----
mean loss: 208.01
 ---- batch: 110 ----
mean loss: 212.13
train mean loss: 209.56
epoch train time: 0:00:02.480402
elapsed time: 0:07:39.935494
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 21:41:51.906488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.04
 ---- batch: 020 ----
mean loss: 207.55
 ---- batch: 030 ----
mean loss: 217.27
 ---- batch: 040 ----
mean loss: 208.22
 ---- batch: 050 ----
mean loss: 205.38
 ---- batch: 060 ----
mean loss: 207.06
 ---- batch: 070 ----
mean loss: 211.19
 ---- batch: 080 ----
mean loss: 207.94
 ---- batch: 090 ----
mean loss: 217.19
 ---- batch: 100 ----
mean loss: 202.82
 ---- batch: 110 ----
mean loss: 208.09
train mean loss: 209.43
epoch train time: 0:00:02.476860
elapsed time: 0:07:42.412879
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 21:41:54.383789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.88
 ---- batch: 020 ----
mean loss: 216.99
 ---- batch: 030 ----
mean loss: 208.97
 ---- batch: 040 ----
mean loss: 209.34
 ---- batch: 050 ----
mean loss: 210.91
 ---- batch: 060 ----
mean loss: 207.92
 ---- batch: 070 ----
mean loss: 208.79
 ---- batch: 080 ----
mean loss: 200.65
 ---- batch: 090 ----
mean loss: 208.74
 ---- batch: 100 ----
mean loss: 213.73
 ---- batch: 110 ----
mean loss: 206.61
train mean loss: 209.07
epoch train time: 0:00:02.481186
elapsed time: 0:07:44.894377
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 21:41:56.865388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.71
 ---- batch: 020 ----
mean loss: 206.97
 ---- batch: 030 ----
mean loss: 207.19
 ---- batch: 040 ----
mean loss: 207.52
 ---- batch: 050 ----
mean loss: 206.79
 ---- batch: 060 ----
mean loss: 211.64
 ---- batch: 070 ----
mean loss: 212.33
 ---- batch: 080 ----
mean loss: 201.47
 ---- batch: 090 ----
mean loss: 215.01
 ---- batch: 100 ----
mean loss: 210.28
 ---- batch: 110 ----
mean loss: 203.36
train mean loss: 209.07
epoch train time: 0:00:02.487353
elapsed time: 0:07:47.382149
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 21:41:59.353144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.58
 ---- batch: 020 ----
mean loss: 209.94
 ---- batch: 030 ----
mean loss: 196.99
 ---- batch: 040 ----
mean loss: 220.32
 ---- batch: 050 ----
mean loss: 215.73
 ---- batch: 060 ----
mean loss: 208.19
 ---- batch: 070 ----
mean loss: 211.08
 ---- batch: 080 ----
mean loss: 208.32
 ---- batch: 090 ----
mean loss: 206.23
 ---- batch: 100 ----
mean loss: 205.07
 ---- batch: 110 ----
mean loss: 204.67
train mean loss: 208.81
epoch train time: 0:00:02.475673
elapsed time: 0:07:49.858263
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 21:42:01.829311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.70
 ---- batch: 020 ----
mean loss: 219.30
 ---- batch: 030 ----
mean loss: 215.96
 ---- batch: 040 ----
mean loss: 200.03
 ---- batch: 050 ----
mean loss: 207.83
 ---- batch: 060 ----
mean loss: 207.89
 ---- batch: 070 ----
mean loss: 200.97
 ---- batch: 080 ----
mean loss: 210.03
 ---- batch: 090 ----
mean loss: 209.11
 ---- batch: 100 ----
mean loss: 209.27
 ---- batch: 110 ----
mean loss: 197.92
train mean loss: 208.67
epoch train time: 0:00:02.475788
elapsed time: 0:07:52.334529
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 21:42:04.305596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.69
 ---- batch: 020 ----
mean loss: 216.64
 ---- batch: 030 ----
mean loss: 209.84
 ---- batch: 040 ----
mean loss: 204.03
 ---- batch: 050 ----
mean loss: 201.78
 ---- batch: 060 ----
mean loss: 210.89
 ---- batch: 070 ----
mean loss: 204.67
 ---- batch: 080 ----
mean loss: 203.20
 ---- batch: 090 ----
mean loss: 214.77
 ---- batch: 100 ----
mean loss: 206.59
 ---- batch: 110 ----
mean loss: 213.92
train mean loss: 208.57
epoch train time: 0:00:02.496618
elapsed time: 0:07:54.831630
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 21:42:06.802631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.21
 ---- batch: 020 ----
mean loss: 201.84
 ---- batch: 030 ----
mean loss: 210.73
 ---- batch: 040 ----
mean loss: 213.45
 ---- batch: 050 ----
mean loss: 213.09
 ---- batch: 060 ----
mean loss: 201.92
 ---- batch: 070 ----
mean loss: 213.73
 ---- batch: 080 ----
mean loss: 208.90
 ---- batch: 090 ----
mean loss: 206.13
 ---- batch: 100 ----
mean loss: 201.04
 ---- batch: 110 ----
mean loss: 220.48
train mean loss: 208.44
epoch train time: 0:00:02.461685
elapsed time: 0:07:57.293703
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 21:42:09.264698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.69
 ---- batch: 020 ----
mean loss: 214.30
 ---- batch: 030 ----
mean loss: 203.76
 ---- batch: 040 ----
mean loss: 199.87
 ---- batch: 050 ----
mean loss: 216.15
 ---- batch: 060 ----
mean loss: 213.18
 ---- batch: 070 ----
mean loss: 211.32
 ---- batch: 080 ----
mean loss: 208.95
 ---- batch: 090 ----
mean loss: 200.62
 ---- batch: 100 ----
mean loss: 208.40
 ---- batch: 110 ----
mean loss: 208.55
train mean loss: 208.12
epoch train time: 0:00:02.466380
elapsed time: 0:07:59.760490
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 21:42:11.731509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.21
 ---- batch: 020 ----
mean loss: 210.71
 ---- batch: 030 ----
mean loss: 220.21
 ---- batch: 040 ----
mean loss: 195.79
 ---- batch: 050 ----
mean loss: 204.43
 ---- batch: 060 ----
mean loss: 201.66
 ---- batch: 070 ----
mean loss: 202.68
 ---- batch: 080 ----
mean loss: 213.00
 ---- batch: 090 ----
mean loss: 211.80
 ---- batch: 100 ----
mean loss: 209.58
 ---- batch: 110 ----
mean loss: 208.43
train mean loss: 208.08
epoch train time: 0:00:02.501183
elapsed time: 0:08:02.262117
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 21:42:14.233125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.72
 ---- batch: 020 ----
mean loss: 213.60
 ---- batch: 030 ----
mean loss: 208.26
 ---- batch: 040 ----
mean loss: 211.84
 ---- batch: 050 ----
mean loss: 203.30
 ---- batch: 060 ----
mean loss: 214.23
 ---- batch: 070 ----
mean loss: 209.61
 ---- batch: 080 ----
mean loss: 200.19
 ---- batch: 090 ----
mean loss: 210.64
 ---- batch: 100 ----
mean loss: 203.05
 ---- batch: 110 ----
mean loss: 198.25
train mean loss: 207.95
epoch train time: 0:00:02.463081
elapsed time: 0:08:04.725641
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 21:42:16.696659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.13
 ---- batch: 020 ----
mean loss: 211.15
 ---- batch: 030 ----
mean loss: 198.33
 ---- batch: 040 ----
mean loss: 214.93
 ---- batch: 050 ----
mean loss: 196.62
 ---- batch: 060 ----
mean loss: 208.23
 ---- batch: 070 ----
mean loss: 205.29
 ---- batch: 080 ----
mean loss: 206.10
 ---- batch: 090 ----
mean loss: 221.58
 ---- batch: 100 ----
mean loss: 211.20
 ---- batch: 110 ----
mean loss: 203.90
train mean loss: 207.65
epoch train time: 0:00:02.492077
elapsed time: 0:08:07.218184
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 21:42:19.189178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.94
 ---- batch: 020 ----
mean loss: 209.06
 ---- batch: 030 ----
mean loss: 211.65
 ---- batch: 040 ----
mean loss: 206.06
 ---- batch: 050 ----
mean loss: 211.38
 ---- batch: 060 ----
mean loss: 209.90
 ---- batch: 070 ----
mean loss: 207.97
 ---- batch: 080 ----
mean loss: 208.03
 ---- batch: 090 ----
mean loss: 200.33
 ---- batch: 100 ----
mean loss: 203.03
 ---- batch: 110 ----
mean loss: 213.14
train mean loss: 207.42
epoch train time: 0:00:02.477403
elapsed time: 0:08:09.695987
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 21:42:21.666994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.95
 ---- batch: 020 ----
mean loss: 197.91
 ---- batch: 030 ----
mean loss: 213.54
 ---- batch: 040 ----
mean loss: 205.14
 ---- batch: 050 ----
mean loss: 209.71
 ---- batch: 060 ----
mean loss: 206.37
 ---- batch: 070 ----
mean loss: 208.32
 ---- batch: 080 ----
mean loss: 205.20
 ---- batch: 090 ----
mean loss: 204.55
 ---- batch: 100 ----
mean loss: 212.01
 ---- batch: 110 ----
mean loss: 212.70
train mean loss: 207.50
epoch train time: 0:00:02.480301
elapsed time: 0:08:12.176723
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 21:42:24.147760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.42
 ---- batch: 020 ----
mean loss: 218.72
 ---- batch: 030 ----
mean loss: 212.93
 ---- batch: 040 ----
mean loss: 203.08
 ---- batch: 050 ----
mean loss: 208.41
 ---- batch: 060 ----
mean loss: 203.14
 ---- batch: 070 ----
mean loss: 208.80
 ---- batch: 080 ----
mean loss: 206.57
 ---- batch: 090 ----
mean loss: 197.40
 ---- batch: 100 ----
mean loss: 199.32
 ---- batch: 110 ----
mean loss: 208.07
train mean loss: 207.18
epoch train time: 0:00:02.489301
elapsed time: 0:08:14.666491
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 21:42:26.637537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.91
 ---- batch: 020 ----
mean loss: 201.89
 ---- batch: 030 ----
mean loss: 205.44
 ---- batch: 040 ----
mean loss: 211.07
 ---- batch: 050 ----
mean loss: 212.64
 ---- batch: 060 ----
mean loss: 216.79
 ---- batch: 070 ----
mean loss: 203.96
 ---- batch: 080 ----
mean loss: 198.93
 ---- batch: 090 ----
mean loss: 215.38
 ---- batch: 100 ----
mean loss: 207.11
 ---- batch: 110 ----
mean loss: 207.80
train mean loss: 207.08
epoch train time: 0:00:02.475883
elapsed time: 0:08:17.142814
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 21:42:29.113932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.04
 ---- batch: 020 ----
mean loss: 208.55
 ---- batch: 030 ----
mean loss: 216.52
 ---- batch: 040 ----
mean loss: 201.84
 ---- batch: 050 ----
mean loss: 210.36
 ---- batch: 060 ----
mean loss: 203.39
 ---- batch: 070 ----
mean loss: 210.53
 ---- batch: 080 ----
mean loss: 203.16
 ---- batch: 090 ----
mean loss: 204.77
 ---- batch: 100 ----
mean loss: 205.82
 ---- batch: 110 ----
mean loss: 203.13
train mean loss: 206.90
epoch train time: 0:00:02.486773
elapsed time: 0:08:19.630122
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 21:42:31.601108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.12
 ---- batch: 020 ----
mean loss: 206.97
 ---- batch: 030 ----
mean loss: 219.89
 ---- batch: 040 ----
mean loss: 205.90
 ---- batch: 050 ----
mean loss: 210.34
 ---- batch: 060 ----
mean loss: 204.61
 ---- batch: 070 ----
mean loss: 205.21
 ---- batch: 080 ----
mean loss: 212.29
 ---- batch: 090 ----
mean loss: 206.97
 ---- batch: 100 ----
mean loss: 195.30
 ---- batch: 110 ----
mean loss: 201.23
train mean loss: 206.75
epoch train time: 0:00:02.477470
elapsed time: 0:08:22.107967
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 21:42:34.078955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.02
 ---- batch: 020 ----
mean loss: 202.42
 ---- batch: 030 ----
mean loss: 215.38
 ---- batch: 040 ----
mean loss: 204.94
 ---- batch: 050 ----
mean loss: 202.30
 ---- batch: 060 ----
mean loss: 210.80
 ---- batch: 070 ----
mean loss: 215.34
 ---- batch: 080 ----
mean loss: 204.71
 ---- batch: 090 ----
mean loss: 201.26
 ---- batch: 100 ----
mean loss: 208.77
 ---- batch: 110 ----
mean loss: 212.97
train mean loss: 206.85
epoch train time: 0:00:02.460609
elapsed time: 0:08:24.568971
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 21:42:36.539978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.87
 ---- batch: 020 ----
mean loss: 201.01
 ---- batch: 030 ----
mean loss: 206.62
 ---- batch: 040 ----
mean loss: 206.69
 ---- batch: 050 ----
mean loss: 211.02
 ---- batch: 060 ----
mean loss: 210.84
 ---- batch: 070 ----
mean loss: 205.35
 ---- batch: 080 ----
mean loss: 200.22
 ---- batch: 090 ----
mean loss: 208.98
 ---- batch: 100 ----
mean loss: 210.56
 ---- batch: 110 ----
mean loss: 200.96
train mean loss: 206.38
epoch train time: 0:00:02.467332
elapsed time: 0:08:27.036718
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 21:42:39.007712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.95
 ---- batch: 020 ----
mean loss: 202.69
 ---- batch: 030 ----
mean loss: 202.35
 ---- batch: 040 ----
mean loss: 203.74
 ---- batch: 050 ----
mean loss: 213.82
 ---- batch: 060 ----
mean loss: 217.16
 ---- batch: 070 ----
mean loss: 199.17
 ---- batch: 080 ----
mean loss: 206.35
 ---- batch: 090 ----
mean loss: 203.29
 ---- batch: 100 ----
mean loss: 201.48
 ---- batch: 110 ----
mean loss: 207.16
train mean loss: 206.25
epoch train time: 0:00:02.472502
elapsed time: 0:08:29.509657
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 21:42:41.480697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.42
 ---- batch: 020 ----
mean loss: 209.63
 ---- batch: 030 ----
mean loss: 206.18
 ---- batch: 040 ----
mean loss: 208.49
 ---- batch: 050 ----
mean loss: 214.54
 ---- batch: 060 ----
mean loss: 198.30
 ---- batch: 070 ----
mean loss: 196.25
 ---- batch: 080 ----
mean loss: 203.71
 ---- batch: 090 ----
mean loss: 196.81
 ---- batch: 100 ----
mean loss: 214.16
 ---- batch: 110 ----
mean loss: 216.00
train mean loss: 206.26
epoch train time: 0:00:02.481917
elapsed time: 0:08:31.992010
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 21:42:43.963056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.27
 ---- batch: 020 ----
mean loss: 217.45
 ---- batch: 030 ----
mean loss: 210.46
 ---- batch: 040 ----
mean loss: 203.50
 ---- batch: 050 ----
mean loss: 199.59
 ---- batch: 060 ----
mean loss: 202.80
 ---- batch: 070 ----
mean loss: 218.28
 ---- batch: 080 ----
mean loss: 201.68
 ---- batch: 090 ----
mean loss: 207.19
 ---- batch: 100 ----
mean loss: 208.73
 ---- batch: 110 ----
mean loss: 195.66
train mean loss: 206.04
epoch train time: 0:00:02.475618
elapsed time: 0:08:34.468117
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 21:42:46.439064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.08
 ---- batch: 020 ----
mean loss: 212.72
 ---- batch: 030 ----
mean loss: 216.46
 ---- batch: 040 ----
mean loss: 209.34
 ---- batch: 050 ----
mean loss: 194.31
 ---- batch: 060 ----
mean loss: 201.43
 ---- batch: 070 ----
mean loss: 204.09
 ---- batch: 080 ----
mean loss: 201.25
 ---- batch: 090 ----
mean loss: 209.83
 ---- batch: 100 ----
mean loss: 203.35
 ---- batch: 110 ----
mean loss: 212.36
train mean loss: 205.83
epoch train time: 0:00:02.508402
elapsed time: 0:08:36.976862
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 21:42:48.947854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.07
 ---- batch: 020 ----
mean loss: 208.29
 ---- batch: 030 ----
mean loss: 200.49
 ---- batch: 040 ----
mean loss: 215.50
 ---- batch: 050 ----
mean loss: 203.00
 ---- batch: 060 ----
mean loss: 204.98
 ---- batch: 070 ----
mean loss: 204.79
 ---- batch: 080 ----
mean loss: 204.89
 ---- batch: 090 ----
mean loss: 200.77
 ---- batch: 100 ----
mean loss: 192.45
 ---- batch: 110 ----
mean loss: 214.35
train mean loss: 205.68
epoch train time: 0:00:02.510950
elapsed time: 0:08:39.488233
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 21:42:51.459251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.85
 ---- batch: 020 ----
mean loss: 202.48
 ---- batch: 030 ----
mean loss: 210.57
 ---- batch: 040 ----
mean loss: 200.55
 ---- batch: 050 ----
mean loss: 212.80
 ---- batch: 060 ----
mean loss: 198.57
 ---- batch: 070 ----
mean loss: 221.16
 ---- batch: 080 ----
mean loss: 210.42
 ---- batch: 090 ----
mean loss: 202.89
 ---- batch: 100 ----
mean loss: 200.46
 ---- batch: 110 ----
mean loss: 199.80
train mean loss: 205.61
epoch train time: 0:00:02.531914
elapsed time: 0:08:42.020588
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 21:42:53.991656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.44
 ---- batch: 020 ----
mean loss: 202.42
 ---- batch: 030 ----
mean loss: 203.68
 ---- batch: 040 ----
mean loss: 205.63
 ---- batch: 050 ----
mean loss: 202.86
 ---- batch: 060 ----
mean loss: 215.45
 ---- batch: 070 ----
mean loss: 199.27
 ---- batch: 080 ----
mean loss: 200.48
 ---- batch: 090 ----
mean loss: 204.93
 ---- batch: 100 ----
mean loss: 196.59
 ---- batch: 110 ----
mean loss: 210.02
train mean loss: 205.38
epoch train time: 0:00:02.536847
elapsed time: 0:08:44.557936
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 21:42:56.528983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.07
 ---- batch: 020 ----
mean loss: 198.81
 ---- batch: 030 ----
mean loss: 217.32
 ---- batch: 040 ----
mean loss: 195.52
 ---- batch: 050 ----
mean loss: 206.81
 ---- batch: 060 ----
mean loss: 218.84
 ---- batch: 070 ----
mean loss: 205.40
 ---- batch: 080 ----
mean loss: 207.56
 ---- batch: 090 ----
mean loss: 195.60
 ---- batch: 100 ----
mean loss: 202.58
 ---- batch: 110 ----
mean loss: 207.55
train mean loss: 205.29
epoch train time: 0:00:02.537780
elapsed time: 0:08:47.096179
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 21:42:59.067226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.25
 ---- batch: 020 ----
mean loss: 199.15
 ---- batch: 030 ----
mean loss: 204.03
 ---- batch: 040 ----
mean loss: 199.21
 ---- batch: 050 ----
mean loss: 209.82
 ---- batch: 060 ----
mean loss: 208.47
 ---- batch: 070 ----
mean loss: 205.84
 ---- batch: 080 ----
mean loss: 196.41
 ---- batch: 090 ----
mean loss: 206.73
 ---- batch: 100 ----
mean loss: 207.64
 ---- batch: 110 ----
mean loss: 215.45
train mean loss: 205.33
epoch train time: 0:00:02.505026
elapsed time: 0:08:49.601647
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 21:43:01.572678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.42
 ---- batch: 020 ----
mean loss: 202.17
 ---- batch: 030 ----
mean loss: 214.38
 ---- batch: 040 ----
mean loss: 205.51
 ---- batch: 050 ----
mean loss: 198.55
 ---- batch: 060 ----
mean loss: 204.99
 ---- batch: 070 ----
mean loss: 199.12
 ---- batch: 080 ----
mean loss: 205.42
 ---- batch: 090 ----
mean loss: 205.06
 ---- batch: 100 ----
mean loss: 201.51
 ---- batch: 110 ----
mean loss: 205.28
train mean loss: 204.69
epoch train time: 0:00:02.491055
elapsed time: 0:08:52.093333
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 21:43:04.064148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.04
 ---- batch: 020 ----
mean loss: 200.33
 ---- batch: 030 ----
mean loss: 195.57
 ---- batch: 040 ----
mean loss: 209.00
 ---- batch: 050 ----
mean loss: 205.88
 ---- batch: 060 ----
mean loss: 205.13
 ---- batch: 070 ----
mean loss: 202.29
 ---- batch: 080 ----
mean loss: 214.24
 ---- batch: 090 ----
mean loss: 209.45
 ---- batch: 100 ----
mean loss: 200.85
 ---- batch: 110 ----
mean loss: 196.83
train mean loss: 204.65
epoch train time: 0:00:02.475107
elapsed time: 0:08:54.568668
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 21:43:06.539669
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.86
 ---- batch: 020 ----
mean loss: 205.82
 ---- batch: 030 ----
mean loss: 205.96
 ---- batch: 040 ----
mean loss: 205.09
 ---- batch: 050 ----
mean loss: 208.67
 ---- batch: 060 ----
mean loss: 203.09
 ---- batch: 070 ----
mean loss: 202.52
 ---- batch: 080 ----
mean loss: 212.96
 ---- batch: 090 ----
mean loss: 202.02
 ---- batch: 100 ----
mean loss: 204.23
 ---- batch: 110 ----
mean loss: 198.17
train mean loss: 204.58
epoch train time: 0:00:02.492204
elapsed time: 0:08:57.061270
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 21:43:09.032269
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.94
 ---- batch: 020 ----
mean loss: 205.74
 ---- batch: 030 ----
mean loss: 198.98
 ---- batch: 040 ----
mean loss: 201.23
 ---- batch: 050 ----
mean loss: 208.50
 ---- batch: 060 ----
mean loss: 203.54
 ---- batch: 070 ----
mean loss: 208.82
 ---- batch: 080 ----
mean loss: 210.13
 ---- batch: 090 ----
mean loss: 206.54
 ---- batch: 100 ----
mean loss: 201.70
 ---- batch: 110 ----
mean loss: 200.00
train mean loss: 204.51
epoch train time: 0:00:02.480422
elapsed time: 0:08:59.542123
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 21:43:11.513136
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.74
 ---- batch: 020 ----
mean loss: 200.53
 ---- batch: 030 ----
mean loss: 205.06
 ---- batch: 040 ----
mean loss: 200.37
 ---- batch: 050 ----
mean loss: 206.15
 ---- batch: 060 ----
mean loss: 207.36
 ---- batch: 070 ----
mean loss: 206.57
 ---- batch: 080 ----
mean loss: 209.94
 ---- batch: 090 ----
mean loss: 201.30
 ---- batch: 100 ----
mean loss: 197.17
 ---- batch: 110 ----
mean loss: 211.49
train mean loss: 204.56
epoch train time: 0:00:02.463360
elapsed time: 0:09:02.005892
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 21:43:13.976903
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.02
 ---- batch: 020 ----
mean loss: 203.14
 ---- batch: 030 ----
mean loss: 205.30
 ---- batch: 040 ----
mean loss: 205.76
 ---- batch: 050 ----
mean loss: 199.16
 ---- batch: 060 ----
mean loss: 209.06
 ---- batch: 070 ----
mean loss: 208.75
 ---- batch: 080 ----
mean loss: 206.93
 ---- batch: 090 ----
mean loss: 201.22
 ---- batch: 100 ----
mean loss: 196.59
 ---- batch: 110 ----
mean loss: 209.57
train mean loss: 204.61
epoch train time: 0:00:02.461376
elapsed time: 0:09:04.467699
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 21:43:16.438764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.28
 ---- batch: 020 ----
mean loss: 206.95
 ---- batch: 030 ----
mean loss: 204.18
 ---- batch: 040 ----
mean loss: 215.62
 ---- batch: 050 ----
mean loss: 193.32
 ---- batch: 060 ----
mean loss: 205.69
 ---- batch: 070 ----
mean loss: 196.67
 ---- batch: 080 ----
mean loss: 208.25
 ---- batch: 090 ----
mean loss: 199.96
 ---- batch: 100 ----
mean loss: 215.71
 ---- batch: 110 ----
mean loss: 201.74
train mean loss: 204.61
epoch train time: 0:00:02.874827
elapsed time: 0:09:07.343016
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 21:43:19.314039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.82
 ---- batch: 020 ----
mean loss: 203.82
 ---- batch: 030 ----
mean loss: 196.54
 ---- batch: 040 ----
mean loss: 202.13
 ---- batch: 050 ----
mean loss: 205.49
 ---- batch: 060 ----
mean loss: 215.67
 ---- batch: 070 ----
mean loss: 211.29
 ---- batch: 080 ----
mean loss: 211.27
 ---- batch: 090 ----
mean loss: 201.33
 ---- batch: 100 ----
mean loss: 198.40
 ---- batch: 110 ----
mean loss: 208.79
train mean loss: 204.54
epoch train time: 0:00:02.487289
elapsed time: 0:09:09.830787
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 21:43:21.801863
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.47
 ---- batch: 020 ----
mean loss: 187.16
 ---- batch: 030 ----
mean loss: 209.40
 ---- batch: 040 ----
mean loss: 199.38
 ---- batch: 050 ----
mean loss: 202.54
 ---- batch: 060 ----
mean loss: 211.11
 ---- batch: 070 ----
mean loss: 204.44
 ---- batch: 080 ----
mean loss: 200.95
 ---- batch: 090 ----
mean loss: 206.35
 ---- batch: 100 ----
mean loss: 204.63
 ---- batch: 110 ----
mean loss: 218.12
train mean loss: 204.51
epoch train time: 0:00:02.507735
elapsed time: 0:09:12.339004
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 21:43:24.310011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.63
 ---- batch: 020 ----
mean loss: 204.99
 ---- batch: 030 ----
mean loss: 203.15
 ---- batch: 040 ----
mean loss: 208.70
 ---- batch: 050 ----
mean loss: 208.19
 ---- batch: 060 ----
mean loss: 210.47
 ---- batch: 070 ----
mean loss: 197.67
 ---- batch: 080 ----
mean loss: 197.64
 ---- batch: 090 ----
mean loss: 195.09
 ---- batch: 100 ----
mean loss: 198.97
 ---- batch: 110 ----
mean loss: 206.16
train mean loss: 204.49
epoch train time: 0:00:02.459036
elapsed time: 0:09:14.798459
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 21:43:26.769487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.89
 ---- batch: 020 ----
mean loss: 210.09
 ---- batch: 030 ----
mean loss: 204.56
 ---- batch: 040 ----
mean loss: 206.25
 ---- batch: 050 ----
mean loss: 214.16
 ---- batch: 060 ----
mean loss: 204.13
 ---- batch: 070 ----
mean loss: 198.04
 ---- batch: 080 ----
mean loss: 198.10
 ---- batch: 090 ----
mean loss: 208.39
 ---- batch: 100 ----
mean loss: 194.98
 ---- batch: 110 ----
mean loss: 201.45
train mean loss: 204.46
epoch train time: 0:00:02.492608
elapsed time: 0:09:17.291540
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 21:43:29.262546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.21
 ---- batch: 020 ----
mean loss: 204.94
 ---- batch: 030 ----
mean loss: 204.08
 ---- batch: 040 ----
mean loss: 207.03
 ---- batch: 050 ----
mean loss: 210.80
 ---- batch: 060 ----
mean loss: 207.44
 ---- batch: 070 ----
mean loss: 206.64
 ---- batch: 080 ----
mean loss: 199.45
 ---- batch: 090 ----
mean loss: 206.88
 ---- batch: 100 ----
mean loss: 203.21
 ---- batch: 110 ----
mean loss: 201.63
train mean loss: 204.37
epoch train time: 0:00:02.458561
elapsed time: 0:09:19.750535
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 21:43:31.721566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.10
 ---- batch: 020 ----
mean loss: 199.44
 ---- batch: 030 ----
mean loss: 195.30
 ---- batch: 040 ----
mean loss: 207.84
 ---- batch: 050 ----
mean loss: 206.86
 ---- batch: 060 ----
mean loss: 208.91
 ---- batch: 070 ----
mean loss: 197.46
 ---- batch: 080 ----
mean loss: 202.96
 ---- batch: 090 ----
mean loss: 199.43
 ---- batch: 100 ----
mean loss: 210.95
 ---- batch: 110 ----
mean loss: 209.15
train mean loss: 204.43
epoch train time: 0:00:02.463472
elapsed time: 0:09:22.214457
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 21:43:34.185576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.16
 ---- batch: 020 ----
mean loss: 205.10
 ---- batch: 030 ----
mean loss: 207.28
 ---- batch: 040 ----
mean loss: 195.66
 ---- batch: 050 ----
mean loss: 206.98
 ---- batch: 060 ----
mean loss: 199.90
 ---- batch: 070 ----
mean loss: 213.79
 ---- batch: 080 ----
mean loss: 204.91
 ---- batch: 090 ----
mean loss: 200.48
 ---- batch: 100 ----
mean loss: 204.69
 ---- batch: 110 ----
mean loss: 209.41
train mean loss: 204.49
epoch train time: 0:00:02.472095
elapsed time: 0:09:24.687110
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 21:43:36.658105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.57
 ---- batch: 020 ----
mean loss: 211.36
 ---- batch: 030 ----
mean loss: 208.09
 ---- batch: 040 ----
mean loss: 212.16
 ---- batch: 050 ----
mean loss: 199.70
 ---- batch: 060 ----
mean loss: 196.66
 ---- batch: 070 ----
mean loss: 210.16
 ---- batch: 080 ----
mean loss: 198.52
 ---- batch: 090 ----
mean loss: 189.73
 ---- batch: 100 ----
mean loss: 200.37
 ---- batch: 110 ----
mean loss: 206.64
train mean loss: 204.46
epoch train time: 0:00:02.509383
elapsed time: 0:09:27.196902
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 21:43:39.167903
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.15
 ---- batch: 020 ----
mean loss: 203.99
 ---- batch: 030 ----
mean loss: 202.81
 ---- batch: 040 ----
mean loss: 208.25
 ---- batch: 050 ----
mean loss: 203.47
 ---- batch: 060 ----
mean loss: 203.89
 ---- batch: 070 ----
mean loss: 202.78
 ---- batch: 080 ----
mean loss: 205.18
 ---- batch: 090 ----
mean loss: 205.13
 ---- batch: 100 ----
mean loss: 205.58
 ---- batch: 110 ----
mean loss: 203.32
train mean loss: 204.44
epoch train time: 0:00:02.479161
elapsed time: 0:09:29.676497
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 21:43:41.647499
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.80
 ---- batch: 020 ----
mean loss: 201.57
 ---- batch: 030 ----
mean loss: 206.18
 ---- batch: 040 ----
mean loss: 204.67
 ---- batch: 050 ----
mean loss: 200.64
 ---- batch: 060 ----
mean loss: 206.64
 ---- batch: 070 ----
mean loss: 202.86
 ---- batch: 080 ----
mean loss: 207.84
 ---- batch: 090 ----
mean loss: 202.02
 ---- batch: 100 ----
mean loss: 211.25
 ---- batch: 110 ----
mean loss: 204.17
train mean loss: 204.39
epoch train time: 0:00:02.478395
elapsed time: 0:09:32.155322
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 21:43:44.126316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.20
 ---- batch: 020 ----
mean loss: 210.38
 ---- batch: 030 ----
mean loss: 209.47
 ---- batch: 040 ----
mean loss: 208.42
 ---- batch: 050 ----
mean loss: 204.90
 ---- batch: 060 ----
mean loss: 207.15
 ---- batch: 070 ----
mean loss: 206.21
 ---- batch: 080 ----
mean loss: 193.93
 ---- batch: 090 ----
mean loss: 202.31
 ---- batch: 100 ----
mean loss: 202.46
 ---- batch: 110 ----
mean loss: 200.70
train mean loss: 204.30
epoch train time: 0:00:02.474005
elapsed time: 0:09:34.629743
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 21:43:46.600775
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.52
 ---- batch: 020 ----
mean loss: 211.61
 ---- batch: 030 ----
mean loss: 200.69
 ---- batch: 040 ----
mean loss: 204.91
 ---- batch: 050 ----
mean loss: 203.75
 ---- batch: 060 ----
mean loss: 196.31
 ---- batch: 070 ----
mean loss: 202.89
 ---- batch: 080 ----
mean loss: 209.50
 ---- batch: 090 ----
mean loss: 205.57
 ---- batch: 100 ----
mean loss: 204.60
 ---- batch: 110 ----
mean loss: 205.83
train mean loss: 204.36
epoch train time: 0:00:02.472401
elapsed time: 0:09:37.102591
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 21:43:49.073643
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.57
 ---- batch: 020 ----
mean loss: 209.08
 ---- batch: 030 ----
mean loss: 205.38
 ---- batch: 040 ----
mean loss: 201.77
 ---- batch: 050 ----
mean loss: 212.00
 ---- batch: 060 ----
mean loss: 200.99
 ---- batch: 070 ----
mean loss: 204.17
 ---- batch: 080 ----
mean loss: 207.45
 ---- batch: 090 ----
mean loss: 204.45
 ---- batch: 100 ----
mean loss: 209.22
 ---- batch: 110 ----
mean loss: 200.47
train mean loss: 204.34
epoch train time: 0:00:02.475835
elapsed time: 0:09:39.578895
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 21:43:51.549931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.91
 ---- batch: 020 ----
mean loss: 199.32
 ---- batch: 030 ----
mean loss: 210.95
 ---- batch: 040 ----
mean loss: 214.53
 ---- batch: 050 ----
mean loss: 197.32
 ---- batch: 060 ----
mean loss: 199.06
 ---- batch: 070 ----
mean loss: 208.69
 ---- batch: 080 ----
mean loss: 207.81
 ---- batch: 090 ----
mean loss: 201.28
 ---- batch: 100 ----
mean loss: 202.93
 ---- batch: 110 ----
mean loss: 206.27
train mean loss: 204.30
epoch train time: 0:00:02.502526
elapsed time: 0:09:42.081872
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 21:43:54.052875
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.30
 ---- batch: 020 ----
mean loss: 209.14
 ---- batch: 030 ----
mean loss: 217.01
 ---- batch: 040 ----
mean loss: 195.13
 ---- batch: 050 ----
mean loss: 205.14
 ---- batch: 060 ----
mean loss: 203.06
 ---- batch: 070 ----
mean loss: 204.88
 ---- batch: 080 ----
mean loss: 201.40
 ---- batch: 090 ----
mean loss: 197.62
 ---- batch: 100 ----
mean loss: 207.62
 ---- batch: 110 ----
mean loss: 198.62
train mean loss: 204.35
epoch train time: 0:00:02.480829
elapsed time: 0:09:44.563111
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 21:43:56.534113
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.45
 ---- batch: 020 ----
mean loss: 202.46
 ---- batch: 030 ----
mean loss: 200.19
 ---- batch: 040 ----
mean loss: 210.57
 ---- batch: 050 ----
mean loss: 212.20
 ---- batch: 060 ----
mean loss: 202.69
 ---- batch: 070 ----
mean loss: 202.72
 ---- batch: 080 ----
mean loss: 200.63
 ---- batch: 090 ----
mean loss: 206.04
 ---- batch: 100 ----
mean loss: 204.75
 ---- batch: 110 ----
mean loss: 202.04
train mean loss: 204.30
epoch train time: 0:00:02.484897
elapsed time: 0:09:47.048435
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 21:43:59.019434
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.74
 ---- batch: 020 ----
mean loss: 205.23
 ---- batch: 030 ----
mean loss: 203.41
 ---- batch: 040 ----
mean loss: 202.25
 ---- batch: 050 ----
mean loss: 203.10
 ---- batch: 060 ----
mean loss: 217.65
 ---- batch: 070 ----
mean loss: 203.47
 ---- batch: 080 ----
mean loss: 208.47
 ---- batch: 090 ----
mean loss: 207.48
 ---- batch: 100 ----
mean loss: 190.74
 ---- batch: 110 ----
mean loss: 200.84
train mean loss: 204.32
epoch train time: 0:00:02.484077
elapsed time: 0:09:49.532933
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 21:44:01.503978
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.20
 ---- batch: 020 ----
mean loss: 200.30
 ---- batch: 030 ----
mean loss: 207.47
 ---- batch: 040 ----
mean loss: 211.50
 ---- batch: 050 ----
mean loss: 194.34
 ---- batch: 060 ----
mean loss: 204.13
 ---- batch: 070 ----
mean loss: 202.36
 ---- batch: 080 ----
mean loss: 197.29
 ---- batch: 090 ----
mean loss: 208.35
 ---- batch: 100 ----
mean loss: 209.02
 ---- batch: 110 ----
mean loss: 203.13
train mean loss: 204.28
epoch train time: 0:00:02.489231
elapsed time: 0:09:52.022600
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 21:44:03.993589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.67
 ---- batch: 020 ----
mean loss: 210.42
 ---- batch: 030 ----
mean loss: 205.69
 ---- batch: 040 ----
mean loss: 199.69
 ---- batch: 050 ----
mean loss: 205.93
 ---- batch: 060 ----
mean loss: 193.66
 ---- batch: 070 ----
mean loss: 209.56
 ---- batch: 080 ----
mean loss: 197.97
 ---- batch: 090 ----
mean loss: 213.29
 ---- batch: 100 ----
mean loss: 206.69
 ---- batch: 110 ----
mean loss: 209.28
train mean loss: 204.20
epoch train time: 0:00:02.502787
elapsed time: 0:09:54.525785
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 21:44:06.496801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.13
 ---- batch: 020 ----
mean loss: 207.04
 ---- batch: 030 ----
mean loss: 206.04
 ---- batch: 040 ----
mean loss: 200.88
 ---- batch: 050 ----
mean loss: 198.59
 ---- batch: 060 ----
mean loss: 204.97
 ---- batch: 070 ----
mean loss: 211.77
 ---- batch: 080 ----
mean loss: 210.77
 ---- batch: 090 ----
mean loss: 207.39
 ---- batch: 100 ----
mean loss: 203.60
 ---- batch: 110 ----
mean loss: 200.44
train mean loss: 204.26
epoch train time: 0:00:02.502342
elapsed time: 0:09:57.028535
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 21:44:08.999542
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.20
 ---- batch: 020 ----
mean loss: 198.50
 ---- batch: 030 ----
mean loss: 199.11
 ---- batch: 040 ----
mean loss: 206.83
 ---- batch: 050 ----
mean loss: 206.01
 ---- batch: 060 ----
mean loss: 206.89
 ---- batch: 070 ----
mean loss: 211.44
 ---- batch: 080 ----
mean loss: 204.06
 ---- batch: 090 ----
mean loss: 198.39
 ---- batch: 100 ----
mean loss: 204.88
 ---- batch: 110 ----
mean loss: 202.35
train mean loss: 204.25
epoch train time: 0:00:02.468138
elapsed time: 0:09:59.497103
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 21:44:11.468103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.97
 ---- batch: 020 ----
mean loss: 207.40
 ---- batch: 030 ----
mean loss: 216.51
 ---- batch: 040 ----
mean loss: 206.03
 ---- batch: 050 ----
mean loss: 203.66
 ---- batch: 060 ----
mean loss: 204.85
 ---- batch: 070 ----
mean loss: 200.94
 ---- batch: 080 ----
mean loss: 192.04
 ---- batch: 090 ----
mean loss: 206.09
 ---- batch: 100 ----
mean loss: 204.36
 ---- batch: 110 ----
mean loss: 207.21
train mean loss: 204.16
epoch train time: 0:00:02.486848
elapsed time: 0:10:01.984349
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 21:44:13.955358
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.62
 ---- batch: 020 ----
mean loss: 198.06
 ---- batch: 030 ----
mean loss: 200.32
 ---- batch: 040 ----
mean loss: 217.15
 ---- batch: 050 ----
mean loss: 196.66
 ---- batch: 060 ----
mean loss: 201.90
 ---- batch: 070 ----
mean loss: 207.35
 ---- batch: 080 ----
mean loss: 206.82
 ---- batch: 090 ----
mean loss: 203.79
 ---- batch: 100 ----
mean loss: 195.31
 ---- batch: 110 ----
mean loss: 207.98
train mean loss: 204.28
epoch train time: 0:00:02.477838
elapsed time: 0:10:04.462615
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 21:44:16.433648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.99
 ---- batch: 020 ----
mean loss: 194.23
 ---- batch: 030 ----
mean loss: 196.51
 ---- batch: 040 ----
mean loss: 212.93
 ---- batch: 050 ----
mean loss: 209.25
 ---- batch: 060 ----
mean loss: 196.20
 ---- batch: 070 ----
mean loss: 204.63
 ---- batch: 080 ----
mean loss: 213.05
 ---- batch: 090 ----
mean loss: 202.15
 ---- batch: 100 ----
mean loss: 214.51
 ---- batch: 110 ----
mean loss: 203.93
train mean loss: 204.14
epoch train time: 0:00:02.469599
elapsed time: 0:10:06.932661
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 21:44:18.903678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.94
 ---- batch: 020 ----
mean loss: 197.78
 ---- batch: 030 ----
mean loss: 210.90
 ---- batch: 040 ----
mean loss: 208.03
 ---- batch: 050 ----
mean loss: 198.02
 ---- batch: 060 ----
mean loss: 210.05
 ---- batch: 070 ----
mean loss: 203.72
 ---- batch: 080 ----
mean loss: 210.22
 ---- batch: 090 ----
mean loss: 199.39
 ---- batch: 100 ----
mean loss: 200.01
 ---- batch: 110 ----
mean loss: 203.91
train mean loss: 204.08
epoch train time: 0:00:02.483796
elapsed time: 0:10:09.416886
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 21:44:21.387872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.42
 ---- batch: 020 ----
mean loss: 196.13
 ---- batch: 030 ----
mean loss: 198.40
 ---- batch: 040 ----
mean loss: 206.99
 ---- batch: 050 ----
mean loss: 204.72
 ---- batch: 060 ----
mean loss: 215.11
 ---- batch: 070 ----
mean loss: 207.79
 ---- batch: 080 ----
mean loss: 205.80
 ---- batch: 090 ----
mean loss: 198.58
 ---- batch: 100 ----
mean loss: 207.84
 ---- batch: 110 ----
mean loss: 203.09
train mean loss: 204.14
epoch train time: 0:00:02.473453
elapsed time: 0:10:11.890974
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 21:44:23.861815
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.44
 ---- batch: 020 ----
mean loss: 202.40
 ---- batch: 030 ----
mean loss: 192.96
 ---- batch: 040 ----
mean loss: 207.83
 ---- batch: 050 ----
mean loss: 199.71
 ---- batch: 060 ----
mean loss: 206.10
 ---- batch: 070 ----
mean loss: 201.54
 ---- batch: 080 ----
mean loss: 211.62
 ---- batch: 090 ----
mean loss: 211.43
 ---- batch: 100 ----
mean loss: 203.60
 ---- batch: 110 ----
mean loss: 200.45
train mean loss: 204.13
epoch train time: 0:00:02.478849
elapsed time: 0:10:14.370052
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 21:44:26.341096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.33
 ---- batch: 020 ----
mean loss: 197.85
 ---- batch: 030 ----
mean loss: 199.88
 ---- batch: 040 ----
mean loss: 211.70
 ---- batch: 050 ----
mean loss: 196.91
 ---- batch: 060 ----
mean loss: 212.66
 ---- batch: 070 ----
mean loss: 207.54
 ---- batch: 080 ----
mean loss: 196.76
 ---- batch: 090 ----
mean loss: 201.67
 ---- batch: 100 ----
mean loss: 209.74
 ---- batch: 110 ----
mean loss: 203.43
train mean loss: 204.12
epoch train time: 0:00:02.465194
elapsed time: 0:10:16.835713
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 21:44:28.806707
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.59
 ---- batch: 020 ----
mean loss: 206.64
 ---- batch: 030 ----
mean loss: 198.71
 ---- batch: 040 ----
mean loss: 209.10
 ---- batch: 050 ----
mean loss: 202.30
 ---- batch: 060 ----
mean loss: 215.40
 ---- batch: 070 ----
mean loss: 209.41
 ---- batch: 080 ----
mean loss: 200.26
 ---- batch: 090 ----
mean loss: 199.45
 ---- batch: 100 ----
mean loss: 199.43
 ---- batch: 110 ----
mean loss: 201.10
train mean loss: 204.09
epoch train time: 0:00:02.478019
elapsed time: 0:10:19.314171
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 21:44:31.285002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.38
 ---- batch: 020 ----
mean loss: 215.98
 ---- batch: 030 ----
mean loss: 204.31
 ---- batch: 040 ----
mean loss: 198.20
 ---- batch: 050 ----
mean loss: 201.31
 ---- batch: 060 ----
mean loss: 205.87
 ---- batch: 070 ----
mean loss: 193.75
 ---- batch: 080 ----
mean loss: 199.68
 ---- batch: 090 ----
mean loss: 209.35
 ---- batch: 100 ----
mean loss: 210.39
 ---- batch: 110 ----
mean loss: 199.09
train mean loss: 204.12
epoch train time: 0:00:02.477789
elapsed time: 0:10:21.792206
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 21:44:33.763203
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.18
 ---- batch: 020 ----
mean loss: 208.22
 ---- batch: 030 ----
mean loss: 202.81
 ---- batch: 040 ----
mean loss: 203.71
 ---- batch: 050 ----
mean loss: 203.98
 ---- batch: 060 ----
mean loss: 204.14
 ---- batch: 070 ----
mean loss: 214.71
 ---- batch: 080 ----
mean loss: 200.48
 ---- batch: 090 ----
mean loss: 195.10
 ---- batch: 100 ----
mean loss: 209.26
 ---- batch: 110 ----
mean loss: 199.83
train mean loss: 204.07
epoch train time: 0:00:02.508700
elapsed time: 0:10:24.301300
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 21:44:36.272292
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.85
 ---- batch: 020 ----
mean loss: 206.92
 ---- batch: 030 ----
mean loss: 205.30
 ---- batch: 040 ----
mean loss: 209.33
 ---- batch: 050 ----
mean loss: 197.86
 ---- batch: 060 ----
mean loss: 213.97
 ---- batch: 070 ----
mean loss: 205.82
 ---- batch: 080 ----
mean loss: 211.82
 ---- batch: 090 ----
mean loss: 196.64
 ---- batch: 100 ----
mean loss: 203.89
 ---- batch: 110 ----
mean loss: 196.87
train mean loss: 204.05
epoch train time: 0:00:02.469334
elapsed time: 0:10:26.771038
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 21:44:38.742102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.56
 ---- batch: 020 ----
mean loss: 198.78
 ---- batch: 030 ----
mean loss: 207.29
 ---- batch: 040 ----
mean loss: 196.46
 ---- batch: 050 ----
mean loss: 202.00
 ---- batch: 060 ----
mean loss: 206.77
 ---- batch: 070 ----
mean loss: 206.22
 ---- batch: 080 ----
mean loss: 200.87
 ---- batch: 090 ----
mean loss: 217.39
 ---- batch: 100 ----
mean loss: 202.01
 ---- batch: 110 ----
mean loss: 209.36
train mean loss: 204.13
epoch train time: 0:00:02.437872
elapsed time: 0:10:29.209370
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 21:44:41.180364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.54
 ---- batch: 020 ----
mean loss: 201.25
 ---- batch: 030 ----
mean loss: 200.61
 ---- batch: 040 ----
mean loss: 200.58
 ---- batch: 050 ----
mean loss: 207.73
 ---- batch: 060 ----
mean loss: 210.64
 ---- batch: 070 ----
mean loss: 204.46
 ---- batch: 080 ----
mean loss: 205.40
 ---- batch: 090 ----
mean loss: 209.84
 ---- batch: 100 ----
mean loss: 209.93
 ---- batch: 110 ----
mean loss: 199.12
train mean loss: 204.08
epoch train time: 0:00:02.511030
elapsed time: 0:10:31.720873
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 21:44:43.691939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.87
 ---- batch: 020 ----
mean loss: 219.17
 ---- batch: 030 ----
mean loss: 211.98
 ---- batch: 040 ----
mean loss: 195.34
 ---- batch: 050 ----
mean loss: 199.84
 ---- batch: 060 ----
mean loss: 206.18
 ---- batch: 070 ----
mean loss: 209.42
 ---- batch: 080 ----
mean loss: 189.26
 ---- batch: 090 ----
mean loss: 202.07
 ---- batch: 100 ----
mean loss: 207.24
 ---- batch: 110 ----
mean loss: 200.74
train mean loss: 204.02
epoch train time: 0:00:02.522398
elapsed time: 0:10:34.243757
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 21:44:46.214773
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.03
 ---- batch: 020 ----
mean loss: 210.97
 ---- batch: 030 ----
mean loss: 200.64
 ---- batch: 040 ----
mean loss: 212.95
 ---- batch: 050 ----
mean loss: 208.27
 ---- batch: 060 ----
mean loss: 202.37
 ---- batch: 070 ----
mean loss: 208.78
 ---- batch: 080 ----
mean loss: 196.71
 ---- batch: 090 ----
mean loss: 196.32
 ---- batch: 100 ----
mean loss: 206.37
 ---- batch: 110 ----
mean loss: 201.54
train mean loss: 204.03
epoch train time: 0:00:02.504856
elapsed time: 0:10:36.749038
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 21:44:48.720095
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.61
 ---- batch: 020 ----
mean loss: 204.13
 ---- batch: 030 ----
mean loss: 205.98
 ---- batch: 040 ----
mean loss: 206.94
 ---- batch: 050 ----
mean loss: 205.12
 ---- batch: 060 ----
mean loss: 195.64
 ---- batch: 070 ----
mean loss: 206.55
 ---- batch: 080 ----
mean loss: 210.63
 ---- batch: 090 ----
mean loss: 202.20
 ---- batch: 100 ----
mean loss: 205.07
 ---- batch: 110 ----
mean loss: 205.93
train mean loss: 203.96
epoch train time: 0:00:02.448240
elapsed time: 0:10:39.197760
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 21:44:51.168757
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.79
 ---- batch: 020 ----
mean loss: 200.30
 ---- batch: 030 ----
mean loss: 204.24
 ---- batch: 040 ----
mean loss: 207.82
 ---- batch: 050 ----
mean loss: 198.42
 ---- batch: 060 ----
mean loss: 206.37
 ---- batch: 070 ----
mean loss: 208.50
 ---- batch: 080 ----
mean loss: 199.67
 ---- batch: 090 ----
mean loss: 204.05
 ---- batch: 100 ----
mean loss: 206.98
 ---- batch: 110 ----
mean loss: 209.36
train mean loss: 203.97
epoch train time: 0:00:02.472368
elapsed time: 0:10:41.670521
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 21:44:53.641513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.60
 ---- batch: 020 ----
mean loss: 201.08
 ---- batch: 030 ----
mean loss: 205.59
 ---- batch: 040 ----
mean loss: 203.70
 ---- batch: 050 ----
mean loss: 205.00
 ---- batch: 060 ----
mean loss: 201.62
 ---- batch: 070 ----
mean loss: 203.02
 ---- batch: 080 ----
mean loss: 203.69
 ---- batch: 090 ----
mean loss: 201.25
 ---- batch: 100 ----
mean loss: 206.95
 ---- batch: 110 ----
mean loss: 204.29
train mean loss: 203.99
epoch train time: 0:00:02.480036
elapsed time: 0:10:44.151039
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 21:44:56.122086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.93
 ---- batch: 020 ----
mean loss: 211.74
 ---- batch: 030 ----
mean loss: 206.70
 ---- batch: 040 ----
mean loss: 207.64
 ---- batch: 050 ----
mean loss: 199.60
 ---- batch: 060 ----
mean loss: 204.42
 ---- batch: 070 ----
mean loss: 210.44
 ---- batch: 080 ----
mean loss: 200.11
 ---- batch: 090 ----
mean loss: 200.60
 ---- batch: 100 ----
mean loss: 206.03
 ---- batch: 110 ----
mean loss: 211.05
train mean loss: 203.94
epoch train time: 0:00:02.475452
elapsed time: 0:10:46.626958
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 21:44:58.597956
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.41
 ---- batch: 020 ----
mean loss: 202.92
 ---- batch: 030 ----
mean loss: 203.81
 ---- batch: 040 ----
mean loss: 201.09
 ---- batch: 050 ----
mean loss: 200.26
 ---- batch: 060 ----
mean loss: 201.58
 ---- batch: 070 ----
mean loss: 200.41
 ---- batch: 080 ----
mean loss: 205.32
 ---- batch: 090 ----
mean loss: 206.41
 ---- batch: 100 ----
mean loss: 210.96
 ---- batch: 110 ----
mean loss: 208.05
train mean loss: 203.83
epoch train time: 0:00:02.470667
elapsed time: 0:10:49.098052
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 21:45:01.069084
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.23
 ---- batch: 020 ----
mean loss: 208.07
 ---- batch: 030 ----
mean loss: 200.98
 ---- batch: 040 ----
mean loss: 202.83
 ---- batch: 050 ----
mean loss: 199.93
 ---- batch: 060 ----
mean loss: 212.47
 ---- batch: 070 ----
mean loss: 202.64
 ---- batch: 080 ----
mean loss: 203.04
 ---- batch: 090 ----
mean loss: 206.87
 ---- batch: 100 ----
mean loss: 205.76
 ---- batch: 110 ----
mean loss: 202.85
train mean loss: 203.88
epoch train time: 0:00:02.477730
elapsed time: 0:10:51.580129
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_6/checkpoint.pth.tar
**** end time: 2019-09-26 21:45:03.550915 ****
