Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_0', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 23082
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-25 22:25:35.131463 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 16, 11]             560
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
            Conv2d-4            [-1, 14, 7, 11]             224
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
            Linear-8                    [-1, 1]             462
================================================================
Total params: 1,246
Trainable params: 1,246
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 22:25:35.136792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4302.28
 ---- batch: 020 ----
mean loss: 4030.76
 ---- batch: 030 ----
mean loss: 3895.31
 ---- batch: 040 ----
mean loss: 3637.22
 ---- batch: 050 ----
mean loss: 3342.43
 ---- batch: 060 ----
mean loss: 3184.77
 ---- batch: 070 ----
mean loss: 2879.21
 ---- batch: 080 ----
mean loss: 2677.89
 ---- batch: 090 ----
mean loss: 2437.10
train mean loss: 3305.63
epoch train time: 0:00:32.873138
elapsed time: 0:00:32.879810
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 22:26:08.011314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2040.81
 ---- batch: 020 ----
mean loss: 1900.73
 ---- batch: 030 ----
mean loss: 1715.23
 ---- batch: 040 ----
mean loss: 1563.59
 ---- batch: 050 ----
mean loss: 1398.45
 ---- batch: 060 ----
mean loss: 1311.25
 ---- batch: 070 ----
mean loss: 1216.28
 ---- batch: 080 ----
mean loss: 1138.72
 ---- batch: 090 ----
mean loss: 1103.20
train mean loss: 1459.15
epoch train time: 0:00:00.703940
elapsed time: 0:00:33.583905
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 22:26:08.715469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1008.64
 ---- batch: 020 ----
mean loss: 966.35
 ---- batch: 030 ----
mean loss: 953.37
 ---- batch: 040 ----
mean loss: 949.02
 ---- batch: 050 ----
mean loss: 933.88
 ---- batch: 060 ----
mean loss: 901.92
 ---- batch: 070 ----
mean loss: 909.07
 ---- batch: 080 ----
mean loss: 878.72
 ---- batch: 090 ----
mean loss: 893.87
train mean loss: 929.79
epoch train time: 0:00:00.683407
elapsed time: 0:00:34.267504
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 22:26:09.399015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 897.74
 ---- batch: 020 ----
mean loss: 874.54
 ---- batch: 030 ----
mean loss: 880.05
 ---- batch: 040 ----
mean loss: 887.46
 ---- batch: 050 ----
mean loss: 868.77
 ---- batch: 060 ----
mean loss: 872.77
 ---- batch: 070 ----
mean loss: 891.50
 ---- batch: 080 ----
mean loss: 885.68
 ---- batch: 090 ----
mean loss: 870.72
train mean loss: 880.94
epoch train time: 0:00:00.678249
elapsed time: 0:00:34.945898
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 22:26:10.077414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.46
 ---- batch: 020 ----
mean loss: 864.26
 ---- batch: 030 ----
mean loss: 881.28
 ---- batch: 040 ----
mean loss: 880.93
 ---- batch: 050 ----
mean loss: 861.20
 ---- batch: 060 ----
mean loss: 874.87
 ---- batch: 070 ----
mean loss: 890.30
 ---- batch: 080 ----
mean loss: 883.06
 ---- batch: 090 ----
mean loss: 865.77
train mean loss: 875.14
epoch train time: 0:00:00.675721
elapsed time: 0:00:35.621764
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 22:26:10.753275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.88
 ---- batch: 020 ----
mean loss: 874.43
 ---- batch: 030 ----
mean loss: 884.27
 ---- batch: 040 ----
mean loss: 873.82
 ---- batch: 050 ----
mean loss: 869.26
 ---- batch: 060 ----
mean loss: 848.47
 ---- batch: 070 ----
mean loss: 885.31
 ---- batch: 080 ----
mean loss: 875.08
 ---- batch: 090 ----
mean loss: 863.13
train mean loss: 872.74
epoch train time: 0:00:00.675860
elapsed time: 0:00:36.297766
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 22:26:11.429280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.25
 ---- batch: 020 ----
mean loss: 873.02
 ---- batch: 030 ----
mean loss: 880.31
 ---- batch: 040 ----
mean loss: 880.28
 ---- batch: 050 ----
mean loss: 877.83
 ---- batch: 060 ----
mean loss: 867.44
 ---- batch: 070 ----
mean loss: 864.56
 ---- batch: 080 ----
mean loss: 863.89
 ---- batch: 090 ----
mean loss: 860.34
train mean loss: 869.08
epoch train time: 0:00:00.672296
elapsed time: 0:00:36.970249
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 22:26:12.101761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.91
 ---- batch: 020 ----
mean loss: 870.78
 ---- batch: 030 ----
mean loss: 902.67
 ---- batch: 040 ----
mean loss: 867.10
 ---- batch: 050 ----
mean loss: 860.94
 ---- batch: 060 ----
mean loss: 856.92
 ---- batch: 070 ----
mean loss: 878.24
 ---- batch: 080 ----
mean loss: 859.37
 ---- batch: 090 ----
mean loss: 840.88
train mean loss: 863.98
epoch train time: 0:00:00.676158
elapsed time: 0:00:37.646560
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 22:26:12.778103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.37
 ---- batch: 020 ----
mean loss: 855.06
 ---- batch: 030 ----
mean loss: 864.06
 ---- batch: 040 ----
mean loss: 886.94
 ---- batch: 050 ----
mean loss: 844.53
 ---- batch: 060 ----
mean loss: 852.93
 ---- batch: 070 ----
mean loss: 851.64
 ---- batch: 080 ----
mean loss: 845.06
 ---- batch: 090 ----
mean loss: 870.31
train mean loss: 859.14
epoch train time: 0:00:00.681360
elapsed time: 0:00:38.328138
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 22:26:13.459695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.68
 ---- batch: 020 ----
mean loss: 859.44
 ---- batch: 030 ----
mean loss: 831.90
 ---- batch: 040 ----
mean loss: 851.74
 ---- batch: 050 ----
mean loss: 867.13
 ---- batch: 060 ----
mean loss: 869.17
 ---- batch: 070 ----
mean loss: 856.64
 ---- batch: 080 ----
mean loss: 842.78
 ---- batch: 090 ----
mean loss: 846.38
train mean loss: 853.96
epoch train time: 0:00:00.673401
elapsed time: 0:00:39.001730
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 22:26:14.133245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.17
 ---- batch: 020 ----
mean loss: 860.35
 ---- batch: 030 ----
mean loss: 851.12
 ---- batch: 040 ----
mean loss: 853.80
 ---- batch: 050 ----
mean loss: 842.23
 ---- batch: 060 ----
mean loss: 852.28
 ---- batch: 070 ----
mean loss: 850.69
 ---- batch: 080 ----
mean loss: 835.40
 ---- batch: 090 ----
mean loss: 849.96
train mean loss: 847.26
epoch train time: 0:00:00.674104
elapsed time: 0:00:39.676025
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 22:26:14.807550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 806.38
 ---- batch: 020 ----
mean loss: 832.98
 ---- batch: 030 ----
mean loss: 843.85
 ---- batch: 040 ----
mean loss: 863.44
 ---- batch: 050 ----
mean loss: 858.39
 ---- batch: 060 ----
mean loss: 844.75
 ---- batch: 070 ----
mean loss: 854.22
 ---- batch: 080 ----
mean loss: 835.98
 ---- batch: 090 ----
mean loss: 828.31
train mean loss: 839.36
epoch train time: 0:00:00.683740
elapsed time: 0:00:40.359929
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 22:26:15.491446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.08
 ---- batch: 020 ----
mean loss: 837.36
 ---- batch: 030 ----
mean loss: 838.70
 ---- batch: 040 ----
mean loss: 822.78
 ---- batch: 050 ----
mean loss: 840.50
 ---- batch: 060 ----
mean loss: 838.38
 ---- batch: 070 ----
mean loss: 812.41
 ---- batch: 080 ----
mean loss: 827.44
 ---- batch: 090 ----
mean loss: 842.68
train mean loss: 832.35
epoch train time: 0:00:00.667461
elapsed time: 0:00:41.027535
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 22:26:16.159047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.68
 ---- batch: 020 ----
mean loss: 829.90
 ---- batch: 030 ----
mean loss: 825.06
 ---- batch: 040 ----
mean loss: 801.85
 ---- batch: 050 ----
mean loss: 830.02
 ---- batch: 060 ----
mean loss: 815.27
 ---- batch: 070 ----
mean loss: 841.80
 ---- batch: 080 ----
mean loss: 819.52
 ---- batch: 090 ----
mean loss: 819.54
train mean loss: 823.66
epoch train time: 0:00:00.662468
elapsed time: 0:00:41.690207
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 22:26:16.821725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 833.64
 ---- batch: 020 ----
mean loss: 817.78
 ---- batch: 030 ----
mean loss: 821.16
 ---- batch: 040 ----
mean loss: 813.44
 ---- batch: 050 ----
mean loss: 808.09
 ---- batch: 060 ----
mean loss: 804.35
 ---- batch: 070 ----
mean loss: 805.81
 ---- batch: 080 ----
mean loss: 824.95
 ---- batch: 090 ----
mean loss: 814.75
train mean loss: 816.14
epoch train time: 0:00:00.668786
elapsed time: 0:00:42.359148
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 22:26:17.490663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 804.69
 ---- batch: 020 ----
mean loss: 814.69
 ---- batch: 030 ----
mean loss: 803.84
 ---- batch: 040 ----
mean loss: 820.08
 ---- batch: 050 ----
mean loss: 816.68
 ---- batch: 060 ----
mean loss: 799.81
 ---- batch: 070 ----
mean loss: 790.63
 ---- batch: 080 ----
mean loss: 802.15
 ---- batch: 090 ----
mean loss: 809.80
train mean loss: 807.47
epoch train time: 0:00:00.669797
elapsed time: 0:00:43.029104
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 22:26:18.160616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 809.97
 ---- batch: 020 ----
mean loss: 784.72
 ---- batch: 030 ----
mean loss: 798.48
 ---- batch: 040 ----
mean loss: 807.42
 ---- batch: 050 ----
mean loss: 785.21
 ---- batch: 060 ----
mean loss: 805.83
 ---- batch: 070 ----
mean loss: 811.03
 ---- batch: 080 ----
mean loss: 815.88
 ---- batch: 090 ----
mean loss: 781.65
train mean loss: 800.45
epoch train time: 0:00:00.668149
elapsed time: 0:00:43.697426
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 22:26:18.828940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.93
 ---- batch: 020 ----
mean loss: 789.31
 ---- batch: 030 ----
mean loss: 806.31
 ---- batch: 040 ----
mean loss: 801.67
 ---- batch: 050 ----
mean loss: 777.94
 ---- batch: 060 ----
mean loss: 776.97
 ---- batch: 070 ----
mean loss: 792.73
 ---- batch: 080 ----
mean loss: 788.32
 ---- batch: 090 ----
mean loss: 787.66
train mean loss: 792.30
epoch train time: 0:00:00.670146
elapsed time: 0:00:44.367725
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 22:26:19.499229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 794.13
 ---- batch: 020 ----
mean loss: 795.71
 ---- batch: 030 ----
mean loss: 772.18
 ---- batch: 040 ----
mean loss: 788.62
 ---- batch: 050 ----
mean loss: 786.12
 ---- batch: 060 ----
mean loss: 783.60
 ---- batch: 070 ----
mean loss: 767.60
 ---- batch: 080 ----
mean loss: 790.64
 ---- batch: 090 ----
mean loss: 789.89
train mean loss: 784.49
epoch train time: 0:00:00.677617
elapsed time: 0:00:45.045482
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 22:26:20.176994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 779.06
 ---- batch: 020 ----
mean loss: 789.13
 ---- batch: 030 ----
mean loss: 780.69
 ---- batch: 040 ----
mean loss: 783.50
 ---- batch: 050 ----
mean loss: 766.35
 ---- batch: 060 ----
mean loss: 777.56
 ---- batch: 070 ----
mean loss: 779.70
 ---- batch: 080 ----
mean loss: 765.40
 ---- batch: 090 ----
mean loss: 767.54
train mean loss: 776.76
epoch train time: 0:00:00.669370
elapsed time: 0:00:45.715006
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 22:26:20.846521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 775.71
 ---- batch: 020 ----
mean loss: 767.69
 ---- batch: 030 ----
mean loss: 762.04
 ---- batch: 040 ----
mean loss: 770.13
 ---- batch: 050 ----
mean loss: 778.93
 ---- batch: 060 ----
mean loss: 764.33
 ---- batch: 070 ----
mean loss: 754.26
 ---- batch: 080 ----
mean loss: 774.96
 ---- batch: 090 ----
mean loss: 770.53
train mean loss: 767.58
epoch train time: 0:00:00.670268
elapsed time: 0:00:46.385427
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 22:26:21.516942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.97
 ---- batch: 020 ----
mean loss: 770.30
 ---- batch: 030 ----
mean loss: 754.99
 ---- batch: 040 ----
mean loss: 745.21
 ---- batch: 050 ----
mean loss: 752.82
 ---- batch: 060 ----
mean loss: 772.77
 ---- batch: 070 ----
mean loss: 759.71
 ---- batch: 080 ----
mean loss: 757.35
 ---- batch: 090 ----
mean loss: 761.80
train mean loss: 759.84
epoch train time: 0:00:00.663315
elapsed time: 0:00:47.048884
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 22:26:22.180395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 761.91
 ---- batch: 020 ----
mean loss: 747.63
 ---- batch: 030 ----
mean loss: 739.06
 ---- batch: 040 ----
mean loss: 746.97
 ---- batch: 050 ----
mean loss: 750.86
 ---- batch: 060 ----
mean loss: 749.88
 ---- batch: 070 ----
mean loss: 751.21
 ---- batch: 080 ----
mean loss: 754.26
 ---- batch: 090 ----
mean loss: 754.46
train mean loss: 750.63
epoch train time: 0:00:00.665882
elapsed time: 0:00:47.714928
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 22:26:22.846472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 758.59
 ---- batch: 020 ----
mean loss: 737.04
 ---- batch: 030 ----
mean loss: 742.72
 ---- batch: 040 ----
mean loss: 731.84
 ---- batch: 050 ----
mean loss: 739.42
 ---- batch: 060 ----
mean loss: 730.45
 ---- batch: 070 ----
mean loss: 746.29
 ---- batch: 080 ----
mean loss: 748.34
 ---- batch: 090 ----
mean loss: 737.10
train mean loss: 742.41
epoch train time: 0:00:00.671742
elapsed time: 0:00:48.386892
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 22:26:23.518443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 728.63
 ---- batch: 020 ----
mean loss: 747.03
 ---- batch: 030 ----
mean loss: 737.90
 ---- batch: 040 ----
mean loss: 738.94
 ---- batch: 050 ----
mean loss: 738.49
 ---- batch: 060 ----
mean loss: 731.55
 ---- batch: 070 ----
mean loss: 727.49
 ---- batch: 080 ----
mean loss: 724.76
 ---- batch: 090 ----
mean loss: 734.46
train mean loss: 732.56
epoch train time: 0:00:00.670945
elapsed time: 0:00:49.058017
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 22:26:24.189528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 719.41
 ---- batch: 020 ----
mean loss: 722.02
 ---- batch: 030 ----
mean loss: 718.23
 ---- batch: 040 ----
mean loss: 714.34
 ---- batch: 050 ----
mean loss: 717.04
 ---- batch: 060 ----
mean loss: 742.33
 ---- batch: 070 ----
mean loss: 727.18
 ---- batch: 080 ----
mean loss: 722.39
 ---- batch: 090 ----
mean loss: 716.02
train mean loss: 722.90
epoch train time: 0:00:00.671538
elapsed time: 0:00:49.729767
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 22:26:24.861298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 721.15
 ---- batch: 020 ----
mean loss: 722.90
 ---- batch: 030 ----
mean loss: 709.91
 ---- batch: 040 ----
mean loss: 712.13
 ---- batch: 050 ----
mean loss: 700.45
 ---- batch: 060 ----
mean loss: 711.76
 ---- batch: 070 ----
mean loss: 716.69
 ---- batch: 080 ----
mean loss: 719.01
 ---- batch: 090 ----
mean loss: 716.55
train mean loss: 713.20
epoch train time: 0:00:00.665300
elapsed time: 0:00:50.395236
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 22:26:25.526753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 719.83
 ---- batch: 020 ----
mean loss: 702.19
 ---- batch: 030 ----
mean loss: 704.78
 ---- batch: 040 ----
mean loss: 713.16
 ---- batch: 050 ----
mean loss: 707.45
 ---- batch: 060 ----
mean loss: 695.10
 ---- batch: 070 ----
mean loss: 690.17
 ---- batch: 080 ----
mean loss: 716.32
 ---- batch: 090 ----
mean loss: 686.73
train mean loss: 703.06
epoch train time: 0:00:00.678116
elapsed time: 0:00:51.073512
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 22:26:26.205023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 699.54
 ---- batch: 020 ----
mean loss: 692.57
 ---- batch: 030 ----
mean loss: 684.31
 ---- batch: 040 ----
mean loss: 694.81
 ---- batch: 050 ----
mean loss: 699.23
 ---- batch: 060 ----
mean loss: 699.90
 ---- batch: 070 ----
mean loss: 708.73
 ---- batch: 080 ----
mean loss: 687.12
 ---- batch: 090 ----
mean loss: 678.99
train mean loss: 693.74
epoch train time: 0:00:00.665723
elapsed time: 0:00:51.739377
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 22:26:26.870890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 698.79
 ---- batch: 020 ----
mean loss: 692.11
 ---- batch: 030 ----
mean loss: 683.36
 ---- batch: 040 ----
mean loss: 682.62
 ---- batch: 050 ----
mean loss: 687.95
 ---- batch: 060 ----
mean loss: 693.97
 ---- batch: 070 ----
mean loss: 678.96
 ---- batch: 080 ----
mean loss: 679.85
 ---- batch: 090 ----
mean loss: 657.85
train mean loss: 683.03
epoch train time: 0:00:00.664915
elapsed time: 0:00:52.404436
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 22:26:27.535967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.09
 ---- batch: 020 ----
mean loss: 674.71
 ---- batch: 030 ----
mean loss: 671.74
 ---- batch: 040 ----
mean loss: 678.63
 ---- batch: 050 ----
mean loss: 690.16
 ---- batch: 060 ----
mean loss: 665.82
 ---- batch: 070 ----
mean loss: 683.43
 ---- batch: 080 ----
mean loss: 669.43
 ---- batch: 090 ----
mean loss: 658.36
train mean loss: 672.98
epoch train time: 0:00:00.667459
elapsed time: 0:00:53.072086
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 22:26:28.203599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 670.90
 ---- batch: 020 ----
mean loss: 672.51
 ---- batch: 030 ----
mean loss: 666.52
 ---- batch: 040 ----
mean loss: 655.38
 ---- batch: 050 ----
mean loss: 655.57
 ---- batch: 060 ----
mean loss: 669.86
 ---- batch: 070 ----
mean loss: 647.40
 ---- batch: 080 ----
mean loss: 676.09
 ---- batch: 090 ----
mean loss: 656.12
train mean loss: 662.93
epoch train time: 0:00:00.666535
elapsed time: 0:00:53.738771
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 22:26:28.870284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 658.41
 ---- batch: 020 ----
mean loss: 660.32
 ---- batch: 030 ----
mean loss: 655.83
 ---- batch: 040 ----
mean loss: 653.55
 ---- batch: 050 ----
mean loss: 645.68
 ---- batch: 060 ----
mean loss: 648.73
 ---- batch: 070 ----
mean loss: 651.59
 ---- batch: 080 ----
mean loss: 650.88
 ---- batch: 090 ----
mean loss: 645.86
train mean loss: 653.33
epoch train time: 0:00:00.659841
elapsed time: 0:00:54.398779
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 22:26:29.530294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 650.72
 ---- batch: 020 ----
mean loss: 651.22
 ---- batch: 030 ----
mean loss: 648.41
 ---- batch: 040 ----
mean loss: 643.99
 ---- batch: 050 ----
mean loss: 650.25
 ---- batch: 060 ----
mean loss: 640.90
 ---- batch: 070 ----
mean loss: 640.93
 ---- batch: 080 ----
mean loss: 628.20
 ---- batch: 090 ----
mean loss: 634.90
train mean loss: 643.04
epoch train time: 0:00:00.670537
elapsed time: 0:00:55.069466
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 22:26:30.200978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 636.00
 ---- batch: 020 ----
mean loss: 636.09
 ---- batch: 030 ----
mean loss: 626.76
 ---- batch: 040 ----
mean loss: 632.81
 ---- batch: 050 ----
mean loss: 629.95
 ---- batch: 060 ----
mean loss: 637.89
 ---- batch: 070 ----
mean loss: 634.62
 ---- batch: 080 ----
mean loss: 626.21
 ---- batch: 090 ----
mean loss: 641.15
train mean loss: 633.25
epoch train time: 0:00:00.662775
elapsed time: 0:00:55.732502
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 22:26:30.864028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.70
 ---- batch: 020 ----
mean loss: 625.49
 ---- batch: 030 ----
mean loss: 632.51
 ---- batch: 040 ----
mean loss: 631.73
 ---- batch: 050 ----
mean loss: 628.83
 ---- batch: 060 ----
mean loss: 607.80
 ---- batch: 070 ----
mean loss: 611.84
 ---- batch: 080 ----
mean loss: 614.15
 ---- batch: 090 ----
mean loss: 636.68
train mean loss: 623.20
epoch train time: 0:00:00.663886
elapsed time: 0:00:56.396550
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 22:26:31.528063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 628.60
 ---- batch: 020 ----
mean loss: 618.06
 ---- batch: 030 ----
mean loss: 612.69
 ---- batch: 040 ----
mean loss: 624.53
 ---- batch: 050 ----
mean loss: 619.98
 ---- batch: 060 ----
mean loss: 607.65
 ---- batch: 070 ----
mean loss: 614.84
 ---- batch: 080 ----
mean loss: 593.04
 ---- batch: 090 ----
mean loss: 601.12
train mean loss: 613.74
epoch train time: 0:00:00.671900
elapsed time: 0:00:57.068595
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 22:26:32.200127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 599.31
 ---- batch: 020 ----
mean loss: 613.97
 ---- batch: 030 ----
mean loss: 603.02
 ---- batch: 040 ----
mean loss: 615.50
 ---- batch: 050 ----
mean loss: 601.13
 ---- batch: 060 ----
mean loss: 602.24
 ---- batch: 070 ----
mean loss: 605.58
 ---- batch: 080 ----
mean loss: 605.87
 ---- batch: 090 ----
mean loss: 588.91
train mean loss: 604.31
epoch train time: 0:00:00.667092
elapsed time: 0:00:57.735851
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 22:26:32.867381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 600.74
 ---- batch: 020 ----
mean loss: 596.53
 ---- batch: 030 ----
mean loss: 593.18
 ---- batch: 040 ----
mean loss: 598.87
 ---- batch: 050 ----
mean loss: 590.13
 ---- batch: 060 ----
mean loss: 598.98
 ---- batch: 070 ----
mean loss: 601.12
 ---- batch: 080 ----
mean loss: 590.53
 ---- batch: 090 ----
mean loss: 591.77
train mean loss: 595.62
epoch train time: 0:00:00.664907
elapsed time: 0:00:58.400933
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 22:26:33.532450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 593.65
 ---- batch: 020 ----
mean loss: 580.44
 ---- batch: 030 ----
mean loss: 581.85
 ---- batch: 040 ----
mean loss: 595.12
 ---- batch: 050 ----
mean loss: 588.48
 ---- batch: 060 ----
mean loss: 585.85
 ---- batch: 070 ----
mean loss: 581.28
 ---- batch: 080 ----
mean loss: 578.38
 ---- batch: 090 ----
mean loss: 587.53
train mean loss: 586.27
epoch train time: 0:00:00.673886
elapsed time: 0:00:59.074972
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 22:26:34.206485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.57
 ---- batch: 020 ----
mean loss: 582.69
 ---- batch: 030 ----
mean loss: 574.56
 ---- batch: 040 ----
mean loss: 574.06
 ---- batch: 050 ----
mean loss: 577.47
 ---- batch: 060 ----
mean loss: 581.54
 ---- batch: 070 ----
mean loss: 580.15
 ---- batch: 080 ----
mean loss: 573.31
 ---- batch: 090 ----
mean loss: 565.75
train mean loss: 577.51
epoch train time: 0:00:00.681952
elapsed time: 0:00:59.757066
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 22:26:34.888577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.35
 ---- batch: 020 ----
mean loss: 572.62
 ---- batch: 030 ----
mean loss: 572.49
 ---- batch: 040 ----
mean loss: 572.11
 ---- batch: 050 ----
mean loss: 560.47
 ---- batch: 060 ----
mean loss: 579.15
 ---- batch: 070 ----
mean loss: 563.10
 ---- batch: 080 ----
mean loss: 561.23
 ---- batch: 090 ----
mean loss: 571.77
train mean loss: 569.02
epoch train time: 0:00:00.663994
elapsed time: 0:01:00.421237
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 22:26:35.552749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.45
 ---- batch: 020 ----
mean loss: 553.13
 ---- batch: 030 ----
mean loss: 564.51
 ---- batch: 040 ----
mean loss: 543.14
 ---- batch: 050 ----
mean loss: 558.89
 ---- batch: 060 ----
mean loss: 562.83
 ---- batch: 070 ----
mean loss: 559.19
 ---- batch: 080 ----
mean loss: 568.36
 ---- batch: 090 ----
mean loss: 563.95
train mean loss: 560.82
epoch train time: 0:00:00.671202
elapsed time: 0:01:01.092583
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 22:26:36.224098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.43
 ---- batch: 020 ----
mean loss: 559.36
 ---- batch: 030 ----
mean loss: 559.16
 ---- batch: 040 ----
mean loss: 554.90
 ---- batch: 050 ----
mean loss: 547.97
 ---- batch: 060 ----
mean loss: 549.27
 ---- batch: 070 ----
mean loss: 548.02
 ---- batch: 080 ----
mean loss: 549.70
 ---- batch: 090 ----
mean loss: 551.52
train mean loss: 552.48
epoch train time: 0:00:00.675843
elapsed time: 0:01:01.768579
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 22:26:36.900092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 546.06
 ---- batch: 020 ----
mean loss: 539.27
 ---- batch: 030 ----
mean loss: 564.04
 ---- batch: 040 ----
mean loss: 540.43
 ---- batch: 050 ----
mean loss: 539.12
 ---- batch: 060 ----
mean loss: 545.34
 ---- batch: 070 ----
mean loss: 547.56
 ---- batch: 080 ----
mean loss: 533.78
 ---- batch: 090 ----
mean loss: 546.29
train mean loss: 544.45
epoch train time: 0:00:00.657190
elapsed time: 0:01:02.425918
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 22:26:37.557449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 539.68
 ---- batch: 020 ----
mean loss: 546.38
 ---- batch: 030 ----
mean loss: 537.69
 ---- batch: 040 ----
mean loss: 540.03
 ---- batch: 050 ----
mean loss: 532.73
 ---- batch: 060 ----
mean loss: 531.05
 ---- batch: 070 ----
mean loss: 542.67
 ---- batch: 080 ----
mean loss: 527.49
 ---- batch: 090 ----
mean loss: 532.98
train mean loss: 536.35
epoch train time: 0:00:00.665486
elapsed time: 0:01:03.091578
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 22:26:38.223093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 532.20
 ---- batch: 020 ----
mean loss: 529.98
 ---- batch: 030 ----
mean loss: 534.90
 ---- batch: 040 ----
mean loss: 524.01
 ---- batch: 050 ----
mean loss: 535.55
 ---- batch: 060 ----
mean loss: 533.64
 ---- batch: 070 ----
mean loss: 531.07
 ---- batch: 080 ----
mean loss: 513.36
 ---- batch: 090 ----
mean loss: 520.53
train mean loss: 528.39
epoch train time: 0:00:00.665281
elapsed time: 0:01:03.757046
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 22:26:38.888602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 523.25
 ---- batch: 020 ----
mean loss: 519.40
 ---- batch: 030 ----
mean loss: 522.42
 ---- batch: 040 ----
mean loss: 513.99
 ---- batch: 050 ----
mean loss: 538.60
 ---- batch: 060 ----
mean loss: 512.08
 ---- batch: 070 ----
mean loss: 523.81
 ---- batch: 080 ----
mean loss: 512.29
 ---- batch: 090 ----
mean loss: 522.63
train mean loss: 520.96
epoch train time: 0:00:00.667976
elapsed time: 0:01:04.425256
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 22:26:39.556787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.76
 ---- batch: 020 ----
mean loss: 508.80
 ---- batch: 030 ----
mean loss: 509.51
 ---- batch: 040 ----
mean loss: 504.22
 ---- batch: 050 ----
mean loss: 518.73
 ---- batch: 060 ----
mean loss: 506.99
 ---- batch: 070 ----
mean loss: 520.49
 ---- batch: 080 ----
mean loss: 519.07
 ---- batch: 090 ----
mean loss: 519.91
train mean loss: 513.33
epoch train time: 0:00:00.667590
elapsed time: 0:01:05.093047
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 22:26:40.224575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.18
 ---- batch: 020 ----
mean loss: 501.19
 ---- batch: 030 ----
mean loss: 509.28
 ---- batch: 040 ----
mean loss: 513.87
 ---- batch: 050 ----
mean loss: 495.44
 ---- batch: 060 ----
mean loss: 514.91
 ---- batch: 070 ----
mean loss: 498.29
 ---- batch: 080 ----
mean loss: 508.12
 ---- batch: 090 ----
mean loss: 501.97
train mean loss: 505.89
epoch train time: 0:00:00.662385
elapsed time: 0:01:05.755592
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 22:26:40.887123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.47
 ---- batch: 020 ----
mean loss: 502.70
 ---- batch: 030 ----
mean loss: 505.54
 ---- batch: 040 ----
mean loss: 498.25
 ---- batch: 050 ----
mean loss: 491.30
 ---- batch: 060 ----
mean loss: 496.57
 ---- batch: 070 ----
mean loss: 500.86
 ---- batch: 080 ----
mean loss: 492.88
 ---- batch: 090 ----
mean loss: 498.10
train mean loss: 498.49
epoch train time: 0:00:00.673905
elapsed time: 0:01:06.429658
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 22:26:41.561171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.36
 ---- batch: 020 ----
mean loss: 489.97
 ---- batch: 030 ----
mean loss: 497.64
 ---- batch: 040 ----
mean loss: 491.76
 ---- batch: 050 ----
mean loss: 480.80
 ---- batch: 060 ----
mean loss: 483.94
 ---- batch: 070 ----
mean loss: 492.37
 ---- batch: 080 ----
mean loss: 494.97
 ---- batch: 090 ----
mean loss: 497.08
train mean loss: 491.58
epoch train time: 0:00:00.668566
elapsed time: 0:01:07.098376
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 22:26:42.229890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.35
 ---- batch: 020 ----
mean loss: 483.59
 ---- batch: 030 ----
mean loss: 487.73
 ---- batch: 040 ----
mean loss: 476.38
 ---- batch: 050 ----
mean loss: 490.87
 ---- batch: 060 ----
mean loss: 491.66
 ---- batch: 070 ----
mean loss: 476.76
 ---- batch: 080 ----
mean loss: 485.32
 ---- batch: 090 ----
mean loss: 478.74
train mean loss: 484.11
epoch train time: 0:00:00.671605
elapsed time: 0:01:07.770159
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 22:26:42.901702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.76
 ---- batch: 020 ----
mean loss: 475.65
 ---- batch: 030 ----
mean loss: 493.56
 ---- batch: 040 ----
mean loss: 477.91
 ---- batch: 050 ----
mean loss: 478.53
 ---- batch: 060 ----
mean loss: 475.80
 ---- batch: 070 ----
mean loss: 469.41
 ---- batch: 080 ----
mean loss: 482.45
 ---- batch: 090 ----
mean loss: 461.41
train mean loss: 477.05
epoch train time: 0:00:00.687036
elapsed time: 0:01:08.457368
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 22:26:43.588882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.13
 ---- batch: 020 ----
mean loss: 464.27
 ---- batch: 030 ----
mean loss: 475.20
 ---- batch: 040 ----
mean loss: 460.22
 ---- batch: 050 ----
mean loss: 470.49
 ---- batch: 060 ----
mean loss: 472.10
 ---- batch: 070 ----
mean loss: 472.42
 ---- batch: 080 ----
mean loss: 461.76
 ---- batch: 090 ----
mean loss: 469.53
train mean loss: 470.05
epoch train time: 0:00:00.672971
elapsed time: 0:01:09.130488
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 22:26:44.262003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.48
 ---- batch: 020 ----
mean loss: 459.78
 ---- batch: 030 ----
mean loss: 462.47
 ---- batch: 040 ----
mean loss: 466.48
 ---- batch: 050 ----
mean loss: 467.15
 ---- batch: 060 ----
mean loss: 463.44
 ---- batch: 070 ----
mean loss: 476.21
 ---- batch: 080 ----
mean loss: 445.64
 ---- batch: 090 ----
mean loss: 459.24
train mean loss: 463.33
epoch train time: 0:00:00.667478
elapsed time: 0:01:09.798114
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 22:26:44.929665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.43
 ---- batch: 020 ----
mean loss: 453.42
 ---- batch: 030 ----
mean loss: 469.17
 ---- batch: 040 ----
mean loss: 452.09
 ---- batch: 050 ----
mean loss: 459.85
 ---- batch: 060 ----
mean loss: 453.12
 ---- batch: 070 ----
mean loss: 453.37
 ---- batch: 080 ----
mean loss: 453.83
 ---- batch: 090 ----
mean loss: 447.88
train mean loss: 456.35
epoch train time: 0:00:00.662679
elapsed time: 0:01:10.460989
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 22:26:45.592506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.58
 ---- batch: 020 ----
mean loss: 460.69
 ---- batch: 030 ----
mean loss: 458.39
 ---- batch: 040 ----
mean loss: 439.86
 ---- batch: 050 ----
mean loss: 463.05
 ---- batch: 060 ----
mean loss: 450.86
 ---- batch: 070 ----
mean loss: 443.83
 ---- batch: 080 ----
mean loss: 443.86
 ---- batch: 090 ----
mean loss: 444.95
train mean loss: 449.27
epoch train time: 0:00:00.665280
elapsed time: 0:01:11.126425
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 22:26:46.257942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.63
 ---- batch: 020 ----
mean loss: 439.76
 ---- batch: 030 ----
mean loss: 446.43
 ---- batch: 040 ----
mean loss: 443.72
 ---- batch: 050 ----
mean loss: 440.08
 ---- batch: 060 ----
mean loss: 435.60
 ---- batch: 070 ----
mean loss: 441.69
 ---- batch: 080 ----
mean loss: 442.69
 ---- batch: 090 ----
mean loss: 445.29
train mean loss: 442.45
epoch train time: 0:00:00.669987
elapsed time: 0:01:11.796572
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 22:26:46.928089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.32
 ---- batch: 020 ----
mean loss: 434.91
 ---- batch: 030 ----
mean loss: 434.91
 ---- batch: 040 ----
mean loss: 435.54
 ---- batch: 050 ----
mean loss: 433.65
 ---- batch: 060 ----
mean loss: 440.68
 ---- batch: 070 ----
mean loss: 435.58
 ---- batch: 080 ----
mean loss: 434.39
 ---- batch: 090 ----
mean loss: 436.77
train mean loss: 436.40
epoch train time: 0:00:00.667099
elapsed time: 0:01:12.463853
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 22:26:47.595365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.99
 ---- batch: 020 ----
mean loss: 425.20
 ---- batch: 030 ----
mean loss: 424.05
 ---- batch: 040 ----
mean loss: 438.09
 ---- batch: 050 ----
mean loss: 428.87
 ---- batch: 060 ----
mean loss: 425.82
 ---- batch: 070 ----
mean loss: 432.30
 ---- batch: 080 ----
mean loss: 437.94
 ---- batch: 090 ----
mean loss: 430.70
train mean loss: 429.47
epoch train time: 0:00:00.657886
elapsed time: 0:01:13.121951
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 22:26:48.253491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.64
 ---- batch: 020 ----
mean loss: 416.38
 ---- batch: 030 ----
mean loss: 430.77
 ---- batch: 040 ----
mean loss: 412.70
 ---- batch: 050 ----
mean loss: 417.67
 ---- batch: 060 ----
mean loss: 426.64
 ---- batch: 070 ----
mean loss: 421.89
 ---- batch: 080 ----
mean loss: 421.64
 ---- batch: 090 ----
mean loss: 437.98
train mean loss: 422.89
epoch train time: 0:00:00.660823
elapsed time: 0:01:13.782944
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 22:26:48.914456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.79
 ---- batch: 020 ----
mean loss: 414.25
 ---- batch: 030 ----
mean loss: 416.85
 ---- batch: 040 ----
mean loss: 411.37
 ---- batch: 050 ----
mean loss: 422.04
 ---- batch: 060 ----
mean loss: 417.60
 ---- batch: 070 ----
mean loss: 421.88
 ---- batch: 080 ----
mean loss: 416.68
 ---- batch: 090 ----
mean loss: 411.90
train mean loss: 416.84
epoch train time: 0:00:00.664652
elapsed time: 0:01:14.447739
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 22:26:49.579257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.97
 ---- batch: 020 ----
mean loss: 410.71
 ---- batch: 030 ----
mean loss: 417.37
 ---- batch: 040 ----
mean loss: 410.71
 ---- batch: 050 ----
mean loss: 426.09
 ---- batch: 060 ----
mean loss: 406.94
 ---- batch: 070 ----
mean loss: 404.60
 ---- batch: 080 ----
mean loss: 407.18
 ---- batch: 090 ----
mean loss: 402.47
train mean loss: 410.51
epoch train time: 0:00:00.667261
elapsed time: 0:01:15.115154
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 22:26:50.246668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.02
 ---- batch: 020 ----
mean loss: 409.94
 ---- batch: 030 ----
mean loss: 401.00
 ---- batch: 040 ----
mean loss: 411.28
 ---- batch: 050 ----
mean loss: 407.23
 ---- batch: 060 ----
mean loss: 401.33
 ---- batch: 070 ----
mean loss: 394.63
 ---- batch: 080 ----
mean loss: 403.20
 ---- batch: 090 ----
mean loss: 399.42
train mean loss: 403.81
epoch train time: 0:00:00.663463
elapsed time: 0:01:15.778767
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 22:26:50.910279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.12
 ---- batch: 020 ----
mean loss: 397.87
 ---- batch: 030 ----
mean loss: 398.64
 ---- batch: 040 ----
mean loss: 380.87
 ---- batch: 050 ----
mean loss: 389.48
 ---- batch: 060 ----
mean loss: 398.57
 ---- batch: 070 ----
mean loss: 387.43
 ---- batch: 080 ----
mean loss: 383.51
 ---- batch: 090 ----
mean loss: 388.29
train mean loss: 390.00
epoch train time: 0:00:00.668161
elapsed time: 0:01:16.447092
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 22:26:51.578607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.50
 ---- batch: 020 ----
mean loss: 369.90
 ---- batch: 030 ----
mean loss: 382.34
 ---- batch: 040 ----
mean loss: 371.98
 ---- batch: 050 ----
mean loss: 371.39
 ---- batch: 060 ----
mean loss: 360.98
 ---- batch: 070 ----
mean loss: 360.98
 ---- batch: 080 ----
mean loss: 356.19
 ---- batch: 090 ----
mean loss: 358.35
train mean loss: 367.19
epoch train time: 0:00:00.801000
elapsed time: 0:01:17.248273
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 22:26:52.379836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.10
 ---- batch: 020 ----
mean loss: 350.17
 ---- batch: 030 ----
mean loss: 342.21
 ---- batch: 040 ----
mean loss: 341.22
 ---- batch: 050 ----
mean loss: 354.76
 ---- batch: 060 ----
mean loss: 351.20
 ---- batch: 070 ----
mean loss: 351.97
 ---- batch: 080 ----
mean loss: 343.65
 ---- batch: 090 ----
mean loss: 333.73
train mean loss: 346.16
epoch train time: 0:00:00.676937
elapsed time: 0:01:17.925408
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 22:26:53.056949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.19
 ---- batch: 020 ----
mean loss: 337.82
 ---- batch: 030 ----
mean loss: 332.80
 ---- batch: 040 ----
mean loss: 327.60
 ---- batch: 050 ----
mean loss: 331.81
 ---- batch: 060 ----
mean loss: 330.04
 ---- batch: 070 ----
mean loss: 335.38
 ---- batch: 080 ----
mean loss: 322.14
 ---- batch: 090 ----
mean loss: 328.71
train mean loss: 332.23
epoch train time: 0:00:00.687216
elapsed time: 0:01:18.612828
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 22:26:53.744357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.85
 ---- batch: 020 ----
mean loss: 315.32
 ---- batch: 030 ----
mean loss: 324.63
 ---- batch: 040 ----
mean loss: 321.77
 ---- batch: 050 ----
mean loss: 316.74
 ---- batch: 060 ----
mean loss: 313.78
 ---- batch: 070 ----
mean loss: 305.97
 ---- batch: 080 ----
mean loss: 335.39
 ---- batch: 090 ----
mean loss: 330.54
train mean loss: 321.96
epoch train time: 0:00:00.677598
elapsed time: 0:01:19.290590
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 22:26:54.422102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.04
 ---- batch: 020 ----
mean loss: 312.85
 ---- batch: 030 ----
mean loss: 323.87
 ---- batch: 040 ----
mean loss: 323.16
 ---- batch: 050 ----
mean loss: 316.32
 ---- batch: 060 ----
mean loss: 307.65
 ---- batch: 070 ----
mean loss: 313.97
 ---- batch: 080 ----
mean loss: 307.63
 ---- batch: 090 ----
mean loss: 313.99
train mean loss: 314.05
epoch train time: 0:00:00.658359
elapsed time: 0:01:19.949124
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 22:26:55.080639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.01
 ---- batch: 020 ----
mean loss: 304.35
 ---- batch: 030 ----
mean loss: 315.57
 ---- batch: 040 ----
mean loss: 307.83
 ---- batch: 050 ----
mean loss: 315.41
 ---- batch: 060 ----
mean loss: 315.57
 ---- batch: 070 ----
mean loss: 297.05
 ---- batch: 080 ----
mean loss: 304.64
 ---- batch: 090 ----
mean loss: 307.84
train mean loss: 307.99
epoch train time: 0:00:00.668301
elapsed time: 0:01:20.617575
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 22:26:55.749088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.40
 ---- batch: 020 ----
mean loss: 294.40
 ---- batch: 030 ----
mean loss: 298.15
 ---- batch: 040 ----
mean loss: 308.16
 ---- batch: 050 ----
mean loss: 307.03
 ---- batch: 060 ----
mean loss: 305.63
 ---- batch: 070 ----
mean loss: 304.13
 ---- batch: 080 ----
mean loss: 307.58
 ---- batch: 090 ----
mean loss: 302.95
train mean loss: 303.13
epoch train time: 0:00:00.672744
elapsed time: 0:01:21.290475
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 22:26:56.421990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.23
 ---- batch: 020 ----
mean loss: 298.95
 ---- batch: 030 ----
mean loss: 307.85
 ---- batch: 040 ----
mean loss: 294.37
 ---- batch: 050 ----
mean loss: 296.13
 ---- batch: 060 ----
mean loss: 298.20
 ---- batch: 070 ----
mean loss: 301.67
 ---- batch: 080 ----
mean loss: 304.88
 ---- batch: 090 ----
mean loss: 297.11
train mean loss: 299.02
epoch train time: 0:00:00.668398
elapsed time: 0:01:21.959022
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 22:26:57.090535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.28
 ---- batch: 020 ----
mean loss: 297.67
 ---- batch: 030 ----
mean loss: 303.36
 ---- batch: 040 ----
mean loss: 291.54
 ---- batch: 050 ----
mean loss: 291.83
 ---- batch: 060 ----
mean loss: 299.26
 ---- batch: 070 ----
mean loss: 297.97
 ---- batch: 080 ----
mean loss: 294.20
 ---- batch: 090 ----
mean loss: 292.74
train mean loss: 295.84
epoch train time: 0:00:00.674809
elapsed time: 0:01:22.633971
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 22:26:57.765483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.23
 ---- batch: 020 ----
mean loss: 295.97
 ---- batch: 030 ----
mean loss: 291.92
 ---- batch: 040 ----
mean loss: 296.85
 ---- batch: 050 ----
mean loss: 284.24
 ---- batch: 060 ----
mean loss: 294.87
 ---- batch: 070 ----
mean loss: 300.10
 ---- batch: 080 ----
mean loss: 295.83
 ---- batch: 090 ----
mean loss: 298.44
train mean loss: 293.51
epoch train time: 0:00:00.680946
elapsed time: 0:01:23.315063
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 22:26:58.446608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.67
 ---- batch: 020 ----
mean loss: 292.27
 ---- batch: 030 ----
mean loss: 274.59
 ---- batch: 040 ----
mean loss: 293.63
 ---- batch: 050 ----
mean loss: 297.06
 ---- batch: 060 ----
mean loss: 284.28
 ---- batch: 070 ----
mean loss: 291.26
 ---- batch: 080 ----
mean loss: 297.95
 ---- batch: 090 ----
mean loss: 286.09
train mean loss: 290.82
epoch train time: 0:00:00.669686
elapsed time: 0:01:23.984929
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 22:26:59.116444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.41
 ---- batch: 020 ----
mean loss: 293.73
 ---- batch: 030 ----
mean loss: 282.04
 ---- batch: 040 ----
mean loss: 300.30
 ---- batch: 050 ----
mean loss: 283.73
 ---- batch: 060 ----
mean loss: 286.70
 ---- batch: 070 ----
mean loss: 286.78
 ---- batch: 080 ----
mean loss: 291.96
 ---- batch: 090 ----
mean loss: 287.08
train mean loss: 288.56
epoch train time: 0:00:00.670331
elapsed time: 0:01:24.655407
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 22:26:59.786932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.94
 ---- batch: 020 ----
mean loss: 282.04
 ---- batch: 030 ----
mean loss: 280.03
 ---- batch: 040 ----
mean loss: 299.12
 ---- batch: 050 ----
mean loss: 290.25
 ---- batch: 060 ----
mean loss: 280.66
 ---- batch: 070 ----
mean loss: 299.63
 ---- batch: 080 ----
mean loss: 283.48
 ---- batch: 090 ----
mean loss: 291.06
train mean loss: 286.73
epoch train time: 0:00:00.669284
elapsed time: 0:01:25.324849
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 22:27:00.456362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.77
 ---- batch: 020 ----
mean loss: 286.21
 ---- batch: 030 ----
mean loss: 286.09
 ---- batch: 040 ----
mean loss: 292.03
 ---- batch: 050 ----
mean loss: 277.93
 ---- batch: 060 ----
mean loss: 296.28
 ---- batch: 070 ----
mean loss: 271.18
 ---- batch: 080 ----
mean loss: 281.62
 ---- batch: 090 ----
mean loss: 287.48
train mean loss: 284.41
epoch train time: 0:00:00.668427
elapsed time: 0:01:25.993427
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 22:27:01.124943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.00
 ---- batch: 020 ----
mean loss: 279.05
 ---- batch: 030 ----
mean loss: 283.66
 ---- batch: 040 ----
mean loss: 282.93
 ---- batch: 050 ----
mean loss: 282.48
 ---- batch: 060 ----
mean loss: 280.06
 ---- batch: 070 ----
mean loss: 282.15
 ---- batch: 080 ----
mean loss: 283.39
 ---- batch: 090 ----
mean loss: 283.10
train mean loss: 282.96
epoch train time: 0:00:00.673132
elapsed time: 0:01:26.666707
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 22:27:01.798220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.26
 ---- batch: 020 ----
mean loss: 287.44
 ---- batch: 030 ----
mean loss: 288.63
 ---- batch: 040 ----
mean loss: 280.15
 ---- batch: 050 ----
mean loss: 292.78
 ---- batch: 060 ----
mean loss: 276.02
 ---- batch: 070 ----
mean loss: 277.63
 ---- batch: 080 ----
mean loss: 270.60
 ---- batch: 090 ----
mean loss: 280.41
train mean loss: 280.82
epoch train time: 0:00:00.666637
elapsed time: 0:01:27.333498
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 22:27:02.465004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.62
 ---- batch: 020 ----
mean loss: 273.25
 ---- batch: 030 ----
mean loss: 273.46
 ---- batch: 040 ----
mean loss: 279.39
 ---- batch: 050 ----
mean loss: 279.01
 ---- batch: 060 ----
mean loss: 289.02
 ---- batch: 070 ----
mean loss: 273.87
 ---- batch: 080 ----
mean loss: 288.63
 ---- batch: 090 ----
mean loss: 279.85
train mean loss: 279.18
epoch train time: 0:00:00.671048
elapsed time: 0:01:28.004703
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 22:27:03.136230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.49
 ---- batch: 020 ----
mean loss: 268.22
 ---- batch: 030 ----
mean loss: 278.38
 ---- batch: 040 ----
mean loss: 277.37
 ---- batch: 050 ----
mean loss: 283.67
 ---- batch: 060 ----
mean loss: 281.76
 ---- batch: 070 ----
mean loss: 276.57
 ---- batch: 080 ----
mean loss: 275.86
 ---- batch: 090 ----
mean loss: 275.75
train mean loss: 278.16
epoch train time: 0:00:00.673594
elapsed time: 0:01:28.678512
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 22:27:03.810025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.04
 ---- batch: 020 ----
mean loss: 270.30
 ---- batch: 030 ----
mean loss: 276.55
 ---- batch: 040 ----
mean loss: 264.95
 ---- batch: 050 ----
mean loss: 284.21
 ---- batch: 060 ----
mean loss: 276.02
 ---- batch: 070 ----
mean loss: 275.90
 ---- batch: 080 ----
mean loss: 276.13
 ---- batch: 090 ----
mean loss: 286.00
train mean loss: 276.31
epoch train time: 0:00:00.670648
elapsed time: 0:01:29.349323
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 22:27:04.480835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.18
 ---- batch: 020 ----
mean loss: 281.99
 ---- batch: 030 ----
mean loss: 268.98
 ---- batch: 040 ----
mean loss: 281.14
 ---- batch: 050 ----
mean loss: 276.62
 ---- batch: 060 ----
mean loss: 266.43
 ---- batch: 070 ----
mean loss: 263.63
 ---- batch: 080 ----
mean loss: 269.82
 ---- batch: 090 ----
mean loss: 286.09
train mean loss: 275.65
epoch train time: 0:00:00.670730
elapsed time: 0:01:30.020212
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 22:27:05.151726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.14
 ---- batch: 020 ----
mean loss: 271.87
 ---- batch: 030 ----
mean loss: 274.14
 ---- batch: 040 ----
mean loss: 279.75
 ---- batch: 050 ----
mean loss: 273.60
 ---- batch: 060 ----
mean loss: 273.68
 ---- batch: 070 ----
mean loss: 272.58
 ---- batch: 080 ----
mean loss: 276.70
 ---- batch: 090 ----
mean loss: 267.53
train mean loss: 274.27
epoch train time: 0:00:00.665996
elapsed time: 0:01:30.686356
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 22:27:05.817869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.01
 ---- batch: 020 ----
mean loss: 276.42
 ---- batch: 030 ----
mean loss: 279.71
 ---- batch: 040 ----
mean loss: 279.08
 ---- batch: 050 ----
mean loss: 261.78
 ---- batch: 060 ----
mean loss: 277.21
 ---- batch: 070 ----
mean loss: 269.59
 ---- batch: 080 ----
mean loss: 278.49
 ---- batch: 090 ----
mean loss: 271.34
train mean loss: 273.11
epoch train time: 0:00:00.666011
elapsed time: 0:01:31.352527
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 22:27:06.484039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.03
 ---- batch: 020 ----
mean loss: 275.79
 ---- batch: 030 ----
mean loss: 272.37
 ---- batch: 040 ----
mean loss: 271.24
 ---- batch: 050 ----
mean loss: 270.43
 ---- batch: 060 ----
mean loss: 277.21
 ---- batch: 070 ----
mean loss: 265.46
 ---- batch: 080 ----
mean loss: 268.78
 ---- batch: 090 ----
mean loss: 267.44
train mean loss: 272.13
epoch train time: 0:00:00.676508
elapsed time: 0:01:32.029179
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 22:27:07.160697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.78
 ---- batch: 020 ----
mean loss: 276.68
 ---- batch: 030 ----
mean loss: 275.90
 ---- batch: 040 ----
mean loss: 268.12
 ---- batch: 050 ----
mean loss: 271.80
 ---- batch: 060 ----
mean loss: 266.55
 ---- batch: 070 ----
mean loss: 268.21
 ---- batch: 080 ----
mean loss: 271.16
 ---- batch: 090 ----
mean loss: 271.92
train mean loss: 270.99
epoch train time: 0:00:00.674700
elapsed time: 0:01:32.704030
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 22:27:07.835558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.04
 ---- batch: 020 ----
mean loss: 267.98
 ---- batch: 030 ----
mean loss: 259.74
 ---- batch: 040 ----
mean loss: 270.99
 ---- batch: 050 ----
mean loss: 277.83
 ---- batch: 060 ----
mean loss: 283.42
 ---- batch: 070 ----
mean loss: 272.20
 ---- batch: 080 ----
mean loss: 267.17
 ---- batch: 090 ----
mean loss: 264.99
train mean loss: 270.24
epoch train time: 0:00:00.669322
elapsed time: 0:01:33.373514
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 22:27:08.505043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.25
 ---- batch: 020 ----
mean loss: 272.45
 ---- batch: 030 ----
mean loss: 269.70
 ---- batch: 040 ----
mean loss: 265.77
 ---- batch: 050 ----
mean loss: 275.84
 ---- batch: 060 ----
mean loss: 280.80
 ---- batch: 070 ----
mean loss: 267.76
 ---- batch: 080 ----
mean loss: 267.71
 ---- batch: 090 ----
mean loss: 264.40
train mean loss: 269.23
epoch train time: 0:00:00.679142
elapsed time: 0:01:34.052818
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 22:27:09.184331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.18
 ---- batch: 020 ----
mean loss: 274.90
 ---- batch: 030 ----
mean loss: 265.24
 ---- batch: 040 ----
mean loss: 269.52
 ---- batch: 050 ----
mean loss: 265.22
 ---- batch: 060 ----
mean loss: 271.21
 ---- batch: 070 ----
mean loss: 265.64
 ---- batch: 080 ----
mean loss: 271.26
 ---- batch: 090 ----
mean loss: 271.69
train mean loss: 268.84
epoch train time: 0:00:00.665457
elapsed time: 0:01:34.718432
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 22:27:09.849949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.91
 ---- batch: 020 ----
mean loss: 276.54
 ---- batch: 030 ----
mean loss: 268.58
 ---- batch: 040 ----
mean loss: 270.19
 ---- batch: 050 ----
mean loss: 265.56
 ---- batch: 060 ----
mean loss: 271.91
 ---- batch: 070 ----
mean loss: 271.85
 ---- batch: 080 ----
mean loss: 272.78
 ---- batch: 090 ----
mean loss: 259.33
train mean loss: 268.45
epoch train time: 0:00:00.679613
elapsed time: 0:01:35.398262
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 22:27:10.529791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.57
 ---- batch: 020 ----
mean loss: 264.03
 ---- batch: 030 ----
mean loss: 277.21
 ---- batch: 040 ----
mean loss: 257.26
 ---- batch: 050 ----
mean loss: 263.67
 ---- batch: 060 ----
mean loss: 273.74
 ---- batch: 070 ----
mean loss: 269.48
 ---- batch: 080 ----
mean loss: 261.54
 ---- batch: 090 ----
mean loss: 273.17
train mean loss: 267.20
epoch train time: 0:00:00.673079
elapsed time: 0:01:36.071507
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 22:27:11.203021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.76
 ---- batch: 020 ----
mean loss: 258.27
 ---- batch: 030 ----
mean loss: 266.71
 ---- batch: 040 ----
mean loss: 270.72
 ---- batch: 050 ----
mean loss: 269.12
 ---- batch: 060 ----
mean loss: 265.15
 ---- batch: 070 ----
mean loss: 263.34
 ---- batch: 080 ----
mean loss: 265.11
 ---- batch: 090 ----
mean loss: 267.79
train mean loss: 266.51
epoch train time: 0:00:00.678341
elapsed time: 0:01:36.750034
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 22:27:11.881586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.11
 ---- batch: 020 ----
mean loss: 263.17
 ---- batch: 030 ----
mean loss: 269.90
 ---- batch: 040 ----
mean loss: 263.81
 ---- batch: 050 ----
mean loss: 263.68
 ---- batch: 060 ----
mean loss: 267.04
 ---- batch: 070 ----
mean loss: 268.41
 ---- batch: 080 ----
mean loss: 263.72
 ---- batch: 090 ----
mean loss: 269.61
train mean loss: 266.28
epoch train time: 0:00:00.665933
elapsed time: 0:01:37.416184
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 22:27:12.547700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.61
 ---- batch: 020 ----
mean loss: 261.48
 ---- batch: 030 ----
mean loss: 260.50
 ---- batch: 040 ----
mean loss: 264.06
 ---- batch: 050 ----
mean loss: 266.03
 ---- batch: 060 ----
mean loss: 260.41
 ---- batch: 070 ----
mean loss: 268.40
 ---- batch: 080 ----
mean loss: 270.47
 ---- batch: 090 ----
mean loss: 273.83
train mean loss: 265.12
epoch train time: 0:00:00.666478
elapsed time: 0:01:38.082813
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 22:27:13.214345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.90
 ---- batch: 020 ----
mean loss: 269.02
 ---- batch: 030 ----
mean loss: 260.59
 ---- batch: 040 ----
mean loss: 267.27
 ---- batch: 050 ----
mean loss: 258.70
 ---- batch: 060 ----
mean loss: 264.20
 ---- batch: 070 ----
mean loss: 272.26
 ---- batch: 080 ----
mean loss: 264.49
 ---- batch: 090 ----
mean loss: 260.39
train mean loss: 264.82
epoch train time: 0:00:00.666532
elapsed time: 0:01:38.749507
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 22:27:13.881023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.65
 ---- batch: 020 ----
mean loss: 256.33
 ---- batch: 030 ----
mean loss: 259.46
 ---- batch: 040 ----
mean loss: 263.17
 ---- batch: 050 ----
mean loss: 268.22
 ---- batch: 060 ----
mean loss: 267.36
 ---- batch: 070 ----
mean loss: 262.83
 ---- batch: 080 ----
mean loss: 264.66
 ---- batch: 090 ----
mean loss: 265.09
train mean loss: 264.35
epoch train time: 0:00:00.674222
elapsed time: 0:01:39.423914
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 22:27:14.555449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.08
 ---- batch: 020 ----
mean loss: 261.57
 ---- batch: 030 ----
mean loss: 262.73
 ---- batch: 040 ----
mean loss: 259.55
 ---- batch: 050 ----
mean loss: 273.18
 ---- batch: 060 ----
mean loss: 258.65
 ---- batch: 070 ----
mean loss: 254.41
 ---- batch: 080 ----
mean loss: 260.40
 ---- batch: 090 ----
mean loss: 265.33
train mean loss: 263.62
epoch train time: 0:00:00.674594
elapsed time: 0:01:40.098680
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 22:27:15.230195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.18
 ---- batch: 020 ----
mean loss: 266.46
 ---- batch: 030 ----
mean loss: 262.57
 ---- batch: 040 ----
mean loss: 261.10
 ---- batch: 050 ----
mean loss: 269.20
 ---- batch: 060 ----
mean loss: 266.42
 ---- batch: 070 ----
mean loss: 258.93
 ---- batch: 080 ----
mean loss: 259.70
 ---- batch: 090 ----
mean loss: 260.44
train mean loss: 262.92
epoch train time: 0:00:00.668928
elapsed time: 0:01:40.767760
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 22:27:15.899289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.06
 ---- batch: 020 ----
mean loss: 257.39
 ---- batch: 030 ----
mean loss: 258.38
 ---- batch: 040 ----
mean loss: 265.02
 ---- batch: 050 ----
mean loss: 260.81
 ---- batch: 060 ----
mean loss: 267.07
 ---- batch: 070 ----
mean loss: 260.87
 ---- batch: 080 ----
mean loss: 262.43
 ---- batch: 090 ----
mean loss: 265.60
train mean loss: 262.37
epoch train time: 0:00:00.666433
elapsed time: 0:01:41.434351
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 22:27:16.565862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.76
 ---- batch: 020 ----
mean loss: 267.93
 ---- batch: 030 ----
mean loss: 255.71
 ---- batch: 040 ----
mean loss: 262.31
 ---- batch: 050 ----
mean loss: 257.09
 ---- batch: 060 ----
mean loss: 263.09
 ---- batch: 070 ----
mean loss: 266.21
 ---- batch: 080 ----
mean loss: 258.26
 ---- batch: 090 ----
mean loss: 259.73
train mean loss: 262.11
epoch train time: 0:00:00.668617
elapsed time: 0:01:42.103125
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 22:27:17.234656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.40
 ---- batch: 020 ----
mean loss: 261.57
 ---- batch: 030 ----
mean loss: 256.33
 ---- batch: 040 ----
mean loss: 259.10
 ---- batch: 050 ----
mean loss: 259.65
 ---- batch: 060 ----
mean loss: 267.93
 ---- batch: 070 ----
mean loss: 254.26
 ---- batch: 080 ----
mean loss: 264.27
 ---- batch: 090 ----
mean loss: 258.52
train mean loss: 261.48
epoch train time: 0:00:00.667720
elapsed time: 0:01:42.771021
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 22:27:17.902569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.74
 ---- batch: 020 ----
mean loss: 258.71
 ---- batch: 030 ----
mean loss: 268.92
 ---- batch: 040 ----
mean loss: 258.85
 ---- batch: 050 ----
mean loss: 252.81
 ---- batch: 060 ----
mean loss: 265.32
 ---- batch: 070 ----
mean loss: 257.87
 ---- batch: 080 ----
mean loss: 259.28
 ---- batch: 090 ----
mean loss: 267.26
train mean loss: 260.80
epoch train time: 0:00:00.669048
elapsed time: 0:01:43.440263
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 22:27:18.571811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.54
 ---- batch: 020 ----
mean loss: 265.39
 ---- batch: 030 ----
mean loss: 261.28
 ---- batch: 040 ----
mean loss: 251.71
 ---- batch: 050 ----
mean loss: 264.36
 ---- batch: 060 ----
mean loss: 260.76
 ---- batch: 070 ----
mean loss: 258.18
 ---- batch: 080 ----
mean loss: 256.71
 ---- batch: 090 ----
mean loss: 258.97
train mean loss: 260.12
epoch train time: 0:00:00.670002
elapsed time: 0:01:44.110510
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 22:27:19.242025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.16
 ---- batch: 020 ----
mean loss: 261.02
 ---- batch: 030 ----
mean loss: 260.10
 ---- batch: 040 ----
mean loss: 262.77
 ---- batch: 050 ----
mean loss: 254.88
 ---- batch: 060 ----
mean loss: 261.94
 ---- batch: 070 ----
mean loss: 262.89
 ---- batch: 080 ----
mean loss: 264.68
 ---- batch: 090 ----
mean loss: 256.06
train mean loss: 259.43
epoch train time: 0:00:00.665711
elapsed time: 0:01:44.776388
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 22:27:19.907903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.39
 ---- batch: 020 ----
mean loss: 257.48
 ---- batch: 030 ----
mean loss: 257.53
 ---- batch: 040 ----
mean loss: 257.56
 ---- batch: 050 ----
mean loss: 248.64
 ---- batch: 060 ----
mean loss: 260.64
 ---- batch: 070 ----
mean loss: 260.03
 ---- batch: 080 ----
mean loss: 260.07
 ---- batch: 090 ----
mean loss: 260.42
train mean loss: 258.91
epoch train time: 0:00:00.667324
elapsed time: 0:01:45.443864
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 22:27:20.575380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.88
 ---- batch: 020 ----
mean loss: 258.18
 ---- batch: 030 ----
mean loss: 259.23
 ---- batch: 040 ----
mean loss: 253.19
 ---- batch: 050 ----
mean loss: 259.99
 ---- batch: 060 ----
mean loss: 263.69
 ---- batch: 070 ----
mean loss: 263.04
 ---- batch: 080 ----
mean loss: 254.71
 ---- batch: 090 ----
mean loss: 255.23
train mean loss: 259.51
epoch train time: 0:00:00.675782
elapsed time: 0:01:46.119790
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 22:27:21.251301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.75
 ---- batch: 020 ----
mean loss: 258.31
 ---- batch: 030 ----
mean loss: 266.14
 ---- batch: 040 ----
mean loss: 255.77
 ---- batch: 050 ----
mean loss: 255.85
 ---- batch: 060 ----
mean loss: 263.71
 ---- batch: 070 ----
mean loss: 254.99
 ---- batch: 080 ----
mean loss: 256.81
 ---- batch: 090 ----
mean loss: 256.17
train mean loss: 258.48
epoch train time: 0:00:00.676840
elapsed time: 0:01:46.796783
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 22:27:21.928298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.14
 ---- batch: 020 ----
mean loss: 259.00
 ---- batch: 030 ----
mean loss: 262.17
 ---- batch: 040 ----
mean loss: 263.59
 ---- batch: 050 ----
mean loss: 260.74
 ---- batch: 060 ----
mean loss: 264.26
 ---- batch: 070 ----
mean loss: 263.42
 ---- batch: 080 ----
mean loss: 253.45
 ---- batch: 090 ----
mean loss: 247.44
train mean loss: 257.94
epoch train time: 0:00:00.660533
elapsed time: 0:01:47.457460
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 22:27:22.588970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.07
 ---- batch: 020 ----
mean loss: 260.05
 ---- batch: 030 ----
mean loss: 249.03
 ---- batch: 040 ----
mean loss: 251.39
 ---- batch: 050 ----
mean loss: 252.20
 ---- batch: 060 ----
mean loss: 263.00
 ---- batch: 070 ----
mean loss: 270.95
 ---- batch: 080 ----
mean loss: 259.37
 ---- batch: 090 ----
mean loss: 256.51
train mean loss: 258.16
epoch train time: 0:00:00.659067
elapsed time: 0:01:48.116664
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 22:27:23.248190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.42
 ---- batch: 020 ----
mean loss: 249.53
 ---- batch: 030 ----
mean loss: 257.81
 ---- batch: 040 ----
mean loss: 260.52
 ---- batch: 050 ----
mean loss: 253.45
 ---- batch: 060 ----
mean loss: 262.54
 ---- batch: 070 ----
mean loss: 251.21
 ---- batch: 080 ----
mean loss: 261.08
 ---- batch: 090 ----
mean loss: 261.78
train mean loss: 257.40
epoch train time: 0:00:00.667013
elapsed time: 0:01:48.783843
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 22:27:23.915361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.68
 ---- batch: 020 ----
mean loss: 258.48
 ---- batch: 030 ----
mean loss: 247.96
 ---- batch: 040 ----
mean loss: 256.70
 ---- batch: 050 ----
mean loss: 252.69
 ---- batch: 060 ----
mean loss: 261.15
 ---- batch: 070 ----
mean loss: 258.19
 ---- batch: 080 ----
mean loss: 252.08
 ---- batch: 090 ----
mean loss: 254.78
train mean loss: 256.51
epoch train time: 0:00:00.667621
elapsed time: 0:01:49.451615
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 22:27:24.583129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.23
 ---- batch: 020 ----
mean loss: 259.11
 ---- batch: 030 ----
mean loss: 249.26
 ---- batch: 040 ----
mean loss: 246.27
 ---- batch: 050 ----
mean loss: 266.54
 ---- batch: 060 ----
mean loss: 256.43
 ---- batch: 070 ----
mean loss: 264.77
 ---- batch: 080 ----
mean loss: 252.96
 ---- batch: 090 ----
mean loss: 251.19
train mean loss: 256.57
epoch train time: 0:00:00.669099
elapsed time: 0:01:50.120863
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 22:27:25.252377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.65
 ---- batch: 020 ----
mean loss: 245.72
 ---- batch: 030 ----
mean loss: 259.58
 ---- batch: 040 ----
mean loss: 254.54
 ---- batch: 050 ----
mean loss: 258.13
 ---- batch: 060 ----
mean loss: 260.43
 ---- batch: 070 ----
mean loss: 264.87
 ---- batch: 080 ----
mean loss: 264.61
 ---- batch: 090 ----
mean loss: 261.20
train mean loss: 255.65
epoch train time: 0:00:00.670159
elapsed time: 0:01:50.791197
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 22:27:25.922718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.83
 ---- batch: 020 ----
mean loss: 250.66
 ---- batch: 030 ----
mean loss: 256.22
 ---- batch: 040 ----
mean loss: 254.16
 ---- batch: 050 ----
mean loss: 255.54
 ---- batch: 060 ----
mean loss: 248.93
 ---- batch: 070 ----
mean loss: 261.11
 ---- batch: 080 ----
mean loss: 257.05
 ---- batch: 090 ----
mean loss: 262.76
train mean loss: 255.72
epoch train time: 0:00:00.674607
elapsed time: 0:01:51.465963
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 22:27:26.597476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.44
 ---- batch: 020 ----
mean loss: 258.34
 ---- batch: 030 ----
mean loss: 257.25
 ---- batch: 040 ----
mean loss: 262.27
 ---- batch: 050 ----
mean loss: 258.31
 ---- batch: 060 ----
mean loss: 256.48
 ---- batch: 070 ----
mean loss: 254.69
 ---- batch: 080 ----
mean loss: 257.81
 ---- batch: 090 ----
mean loss: 240.90
train mean loss: 255.47
epoch train time: 0:00:00.665160
elapsed time: 0:01:52.131265
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 22:27:27.262776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.60
 ---- batch: 020 ----
mean loss: 253.61
 ---- batch: 030 ----
mean loss: 251.06
 ---- batch: 040 ----
mean loss: 252.65
 ---- batch: 050 ----
mean loss: 243.98
 ---- batch: 060 ----
mean loss: 258.58
 ---- batch: 070 ----
mean loss: 255.14
 ---- batch: 080 ----
mean loss: 264.37
 ---- batch: 090 ----
mean loss: 255.23
train mean loss: 255.09
epoch train time: 0:00:00.669034
elapsed time: 0:01:52.800442
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 22:27:27.931962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.75
 ---- batch: 020 ----
mean loss: 254.53
 ---- batch: 030 ----
mean loss: 255.08
 ---- batch: 040 ----
mean loss: 260.97
 ---- batch: 050 ----
mean loss: 250.57
 ---- batch: 060 ----
mean loss: 257.16
 ---- batch: 070 ----
mean loss: 249.76
 ---- batch: 080 ----
mean loss: 257.50
 ---- batch: 090 ----
mean loss: 256.32
train mean loss: 254.38
epoch train time: 0:00:00.675490
elapsed time: 0:01:53.476114
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 22:27:28.607675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.35
 ---- batch: 020 ----
mean loss: 256.61
 ---- batch: 030 ----
mean loss: 251.01
 ---- batch: 040 ----
mean loss: 254.96
 ---- batch: 050 ----
mean loss: 252.59
 ---- batch: 060 ----
mean loss: 254.26
 ---- batch: 070 ----
mean loss: 252.53
 ---- batch: 080 ----
mean loss: 264.61
 ---- batch: 090 ----
mean loss: 258.11
train mean loss: 253.99
epoch train time: 0:00:00.663989
elapsed time: 0:01:54.140293
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 22:27:29.271805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.43
 ---- batch: 020 ----
mean loss: 256.16
 ---- batch: 030 ----
mean loss: 247.63
 ---- batch: 040 ----
mean loss: 257.81
 ---- batch: 050 ----
mean loss: 256.26
 ---- batch: 060 ----
mean loss: 255.18
 ---- batch: 070 ----
mean loss: 245.58
 ---- batch: 080 ----
mean loss: 254.43
 ---- batch: 090 ----
mean loss: 251.79
train mean loss: 253.84
epoch train time: 0:00:00.675711
elapsed time: 0:01:54.816145
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 22:27:29.947657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.60
 ---- batch: 020 ----
mean loss: 249.32
 ---- batch: 030 ----
mean loss: 248.08
 ---- batch: 040 ----
mean loss: 253.73
 ---- batch: 050 ----
mean loss: 252.16
 ---- batch: 060 ----
mean loss: 254.95
 ---- batch: 070 ----
mean loss: 253.41
 ---- batch: 080 ----
mean loss: 258.29
 ---- batch: 090 ----
mean loss: 254.58
train mean loss: 253.43
epoch train time: 0:00:00.663483
elapsed time: 0:01:55.479774
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 22:27:30.611288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.97
 ---- batch: 020 ----
mean loss: 248.28
 ---- batch: 030 ----
mean loss: 247.45
 ---- batch: 040 ----
mean loss: 256.25
 ---- batch: 050 ----
mean loss: 253.28
 ---- batch: 060 ----
mean loss: 261.40
 ---- batch: 070 ----
mean loss: 253.80
 ---- batch: 080 ----
mean loss: 253.45
 ---- batch: 090 ----
mean loss: 256.84
train mean loss: 252.85
epoch train time: 0:00:00.673361
elapsed time: 0:01:56.153283
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 22:27:31.284797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.54
 ---- batch: 020 ----
mean loss: 250.17
 ---- batch: 030 ----
mean loss: 240.19
 ---- batch: 040 ----
mean loss: 245.16
 ---- batch: 050 ----
mean loss: 257.92
 ---- batch: 060 ----
mean loss: 252.99
 ---- batch: 070 ----
mean loss: 252.04
 ---- batch: 080 ----
mean loss: 263.36
 ---- batch: 090 ----
mean loss: 259.35
train mean loss: 252.56
epoch train time: 0:00:00.671421
elapsed time: 0:01:56.824851
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 22:27:31.956380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.33
 ---- batch: 020 ----
mean loss: 248.75
 ---- batch: 030 ----
mean loss: 256.05
 ---- batch: 040 ----
mean loss: 252.42
 ---- batch: 050 ----
mean loss: 255.01
 ---- batch: 060 ----
mean loss: 246.37
 ---- batch: 070 ----
mean loss: 249.42
 ---- batch: 080 ----
mean loss: 259.20
 ---- batch: 090 ----
mean loss: 260.37
train mean loss: 252.45
epoch train time: 0:00:00.660585
elapsed time: 0:01:57.485610
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 22:27:32.617111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.70
 ---- batch: 020 ----
mean loss: 244.63
 ---- batch: 030 ----
mean loss: 248.70
 ---- batch: 040 ----
mean loss: 256.27
 ---- batch: 050 ----
mean loss: 255.12
 ---- batch: 060 ----
mean loss: 258.44
 ---- batch: 070 ----
mean loss: 240.82
 ---- batch: 080 ----
mean loss: 251.16
 ---- batch: 090 ----
mean loss: 259.00
train mean loss: 252.20
epoch train time: 0:00:00.671767
elapsed time: 0:01:58.157523
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 22:27:33.289054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.16
 ---- batch: 020 ----
mean loss: 242.82
 ---- batch: 030 ----
mean loss: 248.46
 ---- batch: 040 ----
mean loss: 251.74
 ---- batch: 050 ----
mean loss: 247.33
 ---- batch: 060 ----
mean loss: 248.25
 ---- batch: 070 ----
mean loss: 254.94
 ---- batch: 080 ----
mean loss: 253.12
 ---- batch: 090 ----
mean loss: 260.19
train mean loss: 251.42
epoch train time: 0:00:00.666865
elapsed time: 0:01:58.824549
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 22:27:33.956064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.65
 ---- batch: 020 ----
mean loss: 252.17
 ---- batch: 030 ----
mean loss: 245.37
 ---- batch: 040 ----
mean loss: 249.29
 ---- batch: 050 ----
mean loss: 245.55
 ---- batch: 060 ----
mean loss: 241.88
 ---- batch: 070 ----
mean loss: 256.66
 ---- batch: 080 ----
mean loss: 261.02
 ---- batch: 090 ----
mean loss: 253.73
train mean loss: 251.45
epoch train time: 0:00:00.670076
elapsed time: 0:01:59.494773
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 22:27:34.626284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.05
 ---- batch: 020 ----
mean loss: 250.84
 ---- batch: 030 ----
mean loss: 250.95
 ---- batch: 040 ----
mean loss: 262.90
 ---- batch: 050 ----
mean loss: 250.11
 ---- batch: 060 ----
mean loss: 246.99
 ---- batch: 070 ----
mean loss: 250.60
 ---- batch: 080 ----
mean loss: 250.38
 ---- batch: 090 ----
mean loss: 247.78
train mean loss: 251.27
epoch train time: 0:00:00.666627
elapsed time: 0:02:00.161559
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 22:27:35.293089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.37
 ---- batch: 020 ----
mean loss: 247.19
 ---- batch: 030 ----
mean loss: 247.72
 ---- batch: 040 ----
mean loss: 252.64
 ---- batch: 050 ----
mean loss: 251.73
 ---- batch: 060 ----
mean loss: 246.18
 ---- batch: 070 ----
mean loss: 252.52
 ---- batch: 080 ----
mean loss: 258.23
 ---- batch: 090 ----
mean loss: 250.53
train mean loss: 251.04
epoch train time: 0:00:00.666823
elapsed time: 0:02:00.828543
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 22:27:35.960056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.81
 ---- batch: 020 ----
mean loss: 248.21
 ---- batch: 030 ----
mean loss: 243.36
 ---- batch: 040 ----
mean loss: 244.37
 ---- batch: 050 ----
mean loss: 253.35
 ---- batch: 060 ----
mean loss: 255.97
 ---- batch: 070 ----
mean loss: 246.62
 ---- batch: 080 ----
mean loss: 255.57
 ---- batch: 090 ----
mean loss: 249.91
train mean loss: 250.51
epoch train time: 0:00:00.668498
elapsed time: 0:02:01.497196
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 22:27:36.628726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.67
 ---- batch: 020 ----
mean loss: 236.89
 ---- batch: 030 ----
mean loss: 248.06
 ---- batch: 040 ----
mean loss: 250.48
 ---- batch: 050 ----
mean loss: 254.12
 ---- batch: 060 ----
mean loss: 244.68
 ---- batch: 070 ----
mean loss: 258.30
 ---- batch: 080 ----
mean loss: 253.11
 ---- batch: 090 ----
mean loss: 252.31
train mean loss: 250.53
epoch train time: 0:00:00.671486
elapsed time: 0:02:02.168846
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 22:27:37.300360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.11
 ---- batch: 020 ----
mean loss: 248.73
 ---- batch: 030 ----
mean loss: 253.00
 ---- batch: 040 ----
mean loss: 247.11
 ---- batch: 050 ----
mean loss: 244.85
 ---- batch: 060 ----
mean loss: 259.14
 ---- batch: 070 ----
mean loss: 254.42
 ---- batch: 080 ----
mean loss: 247.62
 ---- batch: 090 ----
mean loss: 247.60
train mean loss: 250.11
epoch train time: 0:00:00.670040
elapsed time: 0:02:02.839031
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 22:27:37.970557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.94
 ---- batch: 020 ----
mean loss: 250.18
 ---- batch: 030 ----
mean loss: 240.06
 ---- batch: 040 ----
mean loss: 251.66
 ---- batch: 050 ----
mean loss: 246.48
 ---- batch: 060 ----
mean loss: 248.55
 ---- batch: 070 ----
mean loss: 248.96
 ---- batch: 080 ----
mean loss: 253.35
 ---- batch: 090 ----
mean loss: 252.00
train mean loss: 249.50
epoch train time: 0:00:00.666752
elapsed time: 0:02:03.505955
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 22:27:38.637488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.92
 ---- batch: 020 ----
mean loss: 248.89
 ---- batch: 030 ----
mean loss: 252.40
 ---- batch: 040 ----
mean loss: 244.37
 ---- batch: 050 ----
mean loss: 243.77
 ---- batch: 060 ----
mean loss: 246.29
 ---- batch: 070 ----
mean loss: 244.81
 ---- batch: 080 ----
mean loss: 251.18
 ---- batch: 090 ----
mean loss: 254.69
train mean loss: 249.15
epoch train time: 0:00:00.666420
elapsed time: 0:02:04.172539
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 22:27:39.304050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.74
 ---- batch: 020 ----
mean loss: 250.02
 ---- batch: 030 ----
mean loss: 249.87
 ---- batch: 040 ----
mean loss: 257.34
 ---- batch: 050 ----
mean loss: 248.72
 ---- batch: 060 ----
mean loss: 253.34
 ---- batch: 070 ----
mean loss: 251.84
 ---- batch: 080 ----
mean loss: 247.47
 ---- batch: 090 ----
mean loss: 240.06
train mean loss: 248.94
epoch train time: 0:00:00.668011
elapsed time: 0:02:04.840693
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 22:27:39.972207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.73
 ---- batch: 020 ----
mean loss: 249.14
 ---- batch: 030 ----
mean loss: 253.06
 ---- batch: 040 ----
mean loss: 248.13
 ---- batch: 050 ----
mean loss: 258.70
 ---- batch: 060 ----
mean loss: 242.84
 ---- batch: 070 ----
mean loss: 244.26
 ---- batch: 080 ----
mean loss: 247.83
 ---- batch: 090 ----
mean loss: 254.25
train mean loss: 248.72
epoch train time: 0:00:00.666265
elapsed time: 0:02:05.507121
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 22:27:40.638632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.63
 ---- batch: 020 ----
mean loss: 246.41
 ---- batch: 030 ----
mean loss: 253.12
 ---- batch: 040 ----
mean loss: 241.32
 ---- batch: 050 ----
mean loss: 245.80
 ---- batch: 060 ----
mean loss: 249.55
 ---- batch: 070 ----
mean loss: 250.03
 ---- batch: 080 ----
mean loss: 260.46
 ---- batch: 090 ----
mean loss: 242.80
train mean loss: 247.85
epoch train time: 0:00:00.675735
elapsed time: 0:02:06.183047
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 22:27:41.314558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.14
 ---- batch: 020 ----
mean loss: 247.47
 ---- batch: 030 ----
mean loss: 247.59
 ---- batch: 040 ----
mean loss: 250.73
 ---- batch: 050 ----
mean loss: 248.04
 ---- batch: 060 ----
mean loss: 251.95
 ---- batch: 070 ----
mean loss: 238.33
 ---- batch: 080 ----
mean loss: 248.02
 ---- batch: 090 ----
mean loss: 256.72
train mean loss: 248.38
epoch train time: 0:00:00.680885
elapsed time: 0:02:06.864109
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 22:27:41.995625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.86
 ---- batch: 020 ----
mean loss: 250.60
 ---- batch: 030 ----
mean loss: 249.52
 ---- batch: 040 ----
mean loss: 242.74
 ---- batch: 050 ----
mean loss: 251.20
 ---- batch: 060 ----
mean loss: 248.14
 ---- batch: 070 ----
mean loss: 246.64
 ---- batch: 080 ----
mean loss: 247.07
 ---- batch: 090 ----
mean loss: 246.38
train mean loss: 248.00
epoch train time: 0:00:00.670213
elapsed time: 0:02:07.534480
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 22:27:42.665994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.97
 ---- batch: 020 ----
mean loss: 245.36
 ---- batch: 030 ----
mean loss: 249.86
 ---- batch: 040 ----
mean loss: 246.33
 ---- batch: 050 ----
mean loss: 251.14
 ---- batch: 060 ----
mean loss: 249.59
 ---- batch: 070 ----
mean loss: 247.83
 ---- batch: 080 ----
mean loss: 245.84
 ---- batch: 090 ----
mean loss: 243.23
train mean loss: 247.59
epoch train time: 0:00:00.678119
elapsed time: 0:02:08.212748
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 22:27:43.344263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.66
 ---- batch: 020 ----
mean loss: 252.38
 ---- batch: 030 ----
mean loss: 239.06
 ---- batch: 040 ----
mean loss: 249.57
 ---- batch: 050 ----
mean loss: 246.50
 ---- batch: 060 ----
mean loss: 241.25
 ---- batch: 070 ----
mean loss: 247.82
 ---- batch: 080 ----
mean loss: 243.06
 ---- batch: 090 ----
mean loss: 240.90
train mean loss: 247.35
epoch train time: 0:00:00.687877
elapsed time: 0:02:08.900775
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 22:27:44.032330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.07
 ---- batch: 020 ----
mean loss: 256.50
 ---- batch: 030 ----
mean loss: 242.45
 ---- batch: 040 ----
mean loss: 247.41
 ---- batch: 050 ----
mean loss: 237.50
 ---- batch: 060 ----
mean loss: 252.47
 ---- batch: 070 ----
mean loss: 251.13
 ---- batch: 080 ----
mean loss: 244.16
 ---- batch: 090 ----
mean loss: 251.56
train mean loss: 246.88
epoch train time: 0:00:00.666200
elapsed time: 0:02:09.567218
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 22:27:44.698745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.37
 ---- batch: 020 ----
mean loss: 250.15
 ---- batch: 030 ----
mean loss: 240.45
 ---- batch: 040 ----
mean loss: 250.22
 ---- batch: 050 ----
mean loss: 246.00
 ---- batch: 060 ----
mean loss: 254.22
 ---- batch: 070 ----
mean loss: 241.77
 ---- batch: 080 ----
mean loss: 253.25
 ---- batch: 090 ----
mean loss: 245.89
train mean loss: 246.45
epoch train time: 0:00:00.671427
elapsed time: 0:02:10.238807
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 22:27:45.370321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.73
 ---- batch: 020 ----
mean loss: 246.34
 ---- batch: 030 ----
mean loss: 250.41
 ---- batch: 040 ----
mean loss: 245.72
 ---- batch: 050 ----
mean loss: 242.84
 ---- batch: 060 ----
mean loss: 245.81
 ---- batch: 070 ----
mean loss: 243.37
 ---- batch: 080 ----
mean loss: 246.01
 ---- batch: 090 ----
mean loss: 245.03
train mean loss: 246.64
epoch train time: 0:00:00.675722
elapsed time: 0:02:10.914680
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 22:27:46.046195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.88
 ---- batch: 020 ----
mean loss: 256.62
 ---- batch: 030 ----
mean loss: 245.78
 ---- batch: 040 ----
mean loss: 247.64
 ---- batch: 050 ----
mean loss: 246.29
 ---- batch: 060 ----
mean loss: 246.51
 ---- batch: 070 ----
mean loss: 233.16
 ---- batch: 080 ----
mean loss: 251.19
 ---- batch: 090 ----
mean loss: 247.96
train mean loss: 246.47
epoch train time: 0:00:00.673198
elapsed time: 0:02:11.588059
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 22:27:46.719620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.16
 ---- batch: 020 ----
mean loss: 244.56
 ---- batch: 030 ----
mean loss: 243.79
 ---- batch: 040 ----
mean loss: 253.03
 ---- batch: 050 ----
mean loss: 240.87
 ---- batch: 060 ----
mean loss: 240.95
 ---- batch: 070 ----
mean loss: 244.65
 ---- batch: 080 ----
mean loss: 249.82
 ---- batch: 090 ----
mean loss: 245.15
train mean loss: 245.81
epoch train time: 0:00:00.673017
elapsed time: 0:02:12.261280
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 22:27:47.392794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.54
 ---- batch: 020 ----
mean loss: 246.18
 ---- batch: 030 ----
mean loss: 244.06
 ---- batch: 040 ----
mean loss: 242.60
 ---- batch: 050 ----
mean loss: 245.80
 ---- batch: 060 ----
mean loss: 255.69
 ---- batch: 070 ----
mean loss: 249.35
 ---- batch: 080 ----
mean loss: 238.63
 ---- batch: 090 ----
mean loss: 253.05
train mean loss: 245.84
epoch train time: 0:00:00.670817
elapsed time: 0:02:12.932260
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 22:27:48.063787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.47
 ---- batch: 020 ----
mean loss: 253.20
 ---- batch: 030 ----
mean loss: 241.62
 ---- batch: 040 ----
mean loss: 251.33
 ---- batch: 050 ----
mean loss: 235.40
 ---- batch: 060 ----
mean loss: 244.86
 ---- batch: 070 ----
mean loss: 242.34
 ---- batch: 080 ----
mean loss: 248.91
 ---- batch: 090 ----
mean loss: 241.86
train mean loss: 245.68
epoch train time: 0:00:00.677381
elapsed time: 0:02:13.609807
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 22:27:48.741338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.48
 ---- batch: 020 ----
mean loss: 248.07
 ---- batch: 030 ----
mean loss: 245.01
 ---- batch: 040 ----
mean loss: 243.50
 ---- batch: 050 ----
mean loss: 240.95
 ---- batch: 060 ----
mean loss: 247.17
 ---- batch: 070 ----
mean loss: 246.02
 ---- batch: 080 ----
mean loss: 246.46
 ---- batch: 090 ----
mean loss: 257.13
train mean loss: 245.45
epoch train time: 0:00:00.666563
elapsed time: 0:02:14.276542
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 22:27:49.408058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.85
 ---- batch: 020 ----
mean loss: 256.32
 ---- batch: 030 ----
mean loss: 248.75
 ---- batch: 040 ----
mean loss: 239.60
 ---- batch: 050 ----
mean loss: 246.39
 ---- batch: 060 ----
mean loss: 240.36
 ---- batch: 070 ----
mean loss: 244.48
 ---- batch: 080 ----
mean loss: 242.47
 ---- batch: 090 ----
mean loss: 244.44
train mean loss: 244.80
epoch train time: 0:00:00.673371
elapsed time: 0:02:14.950057
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 22:27:50.081568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.21
 ---- batch: 020 ----
mean loss: 236.31
 ---- batch: 030 ----
mean loss: 252.54
 ---- batch: 040 ----
mean loss: 254.56
 ---- batch: 050 ----
mean loss: 247.72
 ---- batch: 060 ----
mean loss: 241.08
 ---- batch: 070 ----
mean loss: 241.00
 ---- batch: 080 ----
mean loss: 241.58
 ---- batch: 090 ----
mean loss: 240.55
train mean loss: 245.21
epoch train time: 0:00:00.668724
elapsed time: 0:02:15.618929
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 22:27:50.750473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.77
 ---- batch: 020 ----
mean loss: 242.42
 ---- batch: 030 ----
mean loss: 237.84
 ---- batch: 040 ----
mean loss: 248.51
 ---- batch: 050 ----
mean loss: 242.60
 ---- batch: 060 ----
mean loss: 250.98
 ---- batch: 070 ----
mean loss: 241.22
 ---- batch: 080 ----
mean loss: 249.06
 ---- batch: 090 ----
mean loss: 245.35
train mean loss: 244.70
epoch train time: 0:00:00.670371
elapsed time: 0:02:16.289476
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 22:27:51.420989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.33
 ---- batch: 020 ----
mean loss: 246.94
 ---- batch: 030 ----
mean loss: 244.55
 ---- batch: 040 ----
mean loss: 236.51
 ---- batch: 050 ----
mean loss: 247.23
 ---- batch: 060 ----
mean loss: 253.70
 ---- batch: 070 ----
mean loss: 245.64
 ---- batch: 080 ----
mean loss: 241.48
 ---- batch: 090 ----
mean loss: 248.34
train mean loss: 244.76
epoch train time: 0:00:00.669750
elapsed time: 0:02:16.959372
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 22:27:52.090892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.55
 ---- batch: 020 ----
mean loss: 238.90
 ---- batch: 030 ----
mean loss: 249.11
 ---- batch: 040 ----
mean loss: 250.22
 ---- batch: 050 ----
mean loss: 233.73
 ---- batch: 060 ----
mean loss: 248.39
 ---- batch: 070 ----
mean loss: 249.26
 ---- batch: 080 ----
mean loss: 255.54
 ---- batch: 090 ----
mean loss: 236.96
train mean loss: 244.46
epoch train time: 0:00:00.666943
elapsed time: 0:02:17.626465
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 22:27:52.757974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.35
 ---- batch: 020 ----
mean loss: 250.51
 ---- batch: 030 ----
mean loss: 242.31
 ---- batch: 040 ----
mean loss: 243.75
 ---- batch: 050 ----
mean loss: 249.58
 ---- batch: 060 ----
mean loss: 242.54
 ---- batch: 070 ----
mean loss: 244.50
 ---- batch: 080 ----
mean loss: 243.95
 ---- batch: 090 ----
mean loss: 243.04
train mean loss: 244.11
epoch train time: 0:00:00.663445
elapsed time: 0:02:18.290055
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 22:27:53.421570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.31
 ---- batch: 020 ----
mean loss: 236.02
 ---- batch: 030 ----
mean loss: 244.84
 ---- batch: 040 ----
mean loss: 243.53
 ---- batch: 050 ----
mean loss: 239.25
 ---- batch: 060 ----
mean loss: 242.68
 ---- batch: 070 ----
mean loss: 241.81
 ---- batch: 080 ----
mean loss: 251.21
 ---- batch: 090 ----
mean loss: 241.44
train mean loss: 244.33
epoch train time: 0:00:00.682037
elapsed time: 0:02:18.972271
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 22:27:54.103802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.01
 ---- batch: 020 ----
mean loss: 238.39
 ---- batch: 030 ----
mean loss: 251.37
 ---- batch: 040 ----
mean loss: 246.25
 ---- batch: 050 ----
mean loss: 249.11
 ---- batch: 060 ----
mean loss: 240.19
 ---- batch: 070 ----
mean loss: 243.75
 ---- batch: 080 ----
mean loss: 238.43
 ---- batch: 090 ----
mean loss: 242.57
train mean loss: 243.79
epoch train time: 0:00:00.671961
elapsed time: 0:02:19.644425
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 22:27:54.775942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.46
 ---- batch: 020 ----
mean loss: 241.95
 ---- batch: 030 ----
mean loss: 238.46
 ---- batch: 040 ----
mean loss: 244.40
 ---- batch: 050 ----
mean loss: 240.47
 ---- batch: 060 ----
mean loss: 244.17
 ---- batch: 070 ----
mean loss: 246.31
 ---- batch: 080 ----
mean loss: 248.14
 ---- batch: 090 ----
mean loss: 247.10
train mean loss: 243.84
epoch train time: 0:00:00.671192
elapsed time: 0:02:20.315795
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 22:27:55.447308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.38
 ---- batch: 020 ----
mean loss: 236.04
 ---- batch: 030 ----
mean loss: 253.44
 ---- batch: 040 ----
mean loss: 237.40
 ---- batch: 050 ----
mean loss: 238.26
 ---- batch: 060 ----
mean loss: 248.94
 ---- batch: 070 ----
mean loss: 251.34
 ---- batch: 080 ----
mean loss: 239.80
 ---- batch: 090 ----
mean loss: 243.15
train mean loss: 243.37
epoch train time: 0:00:00.670170
elapsed time: 0:02:20.986111
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 22:27:56.117648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.37
 ---- batch: 020 ----
mean loss: 247.33
 ---- batch: 030 ----
mean loss: 254.15
 ---- batch: 040 ----
mean loss: 243.07
 ---- batch: 050 ----
mean loss: 236.75
 ---- batch: 060 ----
mean loss: 253.04
 ---- batch: 070 ----
mean loss: 241.48
 ---- batch: 080 ----
mean loss: 243.76
 ---- batch: 090 ----
mean loss: 237.34
train mean loss: 242.92
epoch train time: 0:00:00.670109
elapsed time: 0:02:21.656456
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 22:27:56.788003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.73
 ---- batch: 020 ----
mean loss: 233.71
 ---- batch: 030 ----
mean loss: 244.15
 ---- batch: 040 ----
mean loss: 243.53
 ---- batch: 050 ----
mean loss: 243.98
 ---- batch: 060 ----
mean loss: 247.08
 ---- batch: 070 ----
mean loss: 242.91
 ---- batch: 080 ----
mean loss: 239.45
 ---- batch: 090 ----
mean loss: 247.45
train mean loss: 243.14
epoch train time: 0:00:00.668658
elapsed time: 0:02:22.325294
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 22:27:57.456807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.62
 ---- batch: 020 ----
mean loss: 238.82
 ---- batch: 030 ----
mean loss: 240.22
 ---- batch: 040 ----
mean loss: 236.76
 ---- batch: 050 ----
mean loss: 252.21
 ---- batch: 060 ----
mean loss: 248.43
 ---- batch: 070 ----
mean loss: 239.74
 ---- batch: 080 ----
mean loss: 236.62
 ---- batch: 090 ----
mean loss: 251.06
train mean loss: 242.68
epoch train time: 0:00:00.669944
elapsed time: 0:02:22.995385
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 22:27:58.126908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.20
 ---- batch: 020 ----
mean loss: 236.68
 ---- batch: 030 ----
mean loss: 235.30
 ---- batch: 040 ----
mean loss: 251.96
 ---- batch: 050 ----
mean loss: 245.37
 ---- batch: 060 ----
mean loss: 253.80
 ---- batch: 070 ----
mean loss: 234.33
 ---- batch: 080 ----
mean loss: 233.16
 ---- batch: 090 ----
mean loss: 247.24
train mean loss: 242.56
epoch train time: 0:00:00.665356
elapsed time: 0:02:23.660909
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 22:27:58.792441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.14
 ---- batch: 020 ----
mean loss: 241.40
 ---- batch: 030 ----
mean loss: 244.27
 ---- batch: 040 ----
mean loss: 238.87
 ---- batch: 050 ----
mean loss: 241.25
 ---- batch: 060 ----
mean loss: 236.59
 ---- batch: 070 ----
mean loss: 235.63
 ---- batch: 080 ----
mean loss: 244.30
 ---- batch: 090 ----
mean loss: 248.34
train mean loss: 242.28
epoch train time: 0:00:00.665444
elapsed time: 0:02:24.326525
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 22:27:59.458040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.28
 ---- batch: 020 ----
mean loss: 239.23
 ---- batch: 030 ----
mean loss: 250.13
 ---- batch: 040 ----
mean loss: 231.50
 ---- batch: 050 ----
mean loss: 243.89
 ---- batch: 060 ----
mean loss: 238.19
 ---- batch: 070 ----
mean loss: 244.15
 ---- batch: 080 ----
mean loss: 243.02
 ---- batch: 090 ----
mean loss: 242.78
train mean loss: 242.35
epoch train time: 0:00:00.671727
elapsed time: 0:02:24.998396
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 22:28:00.129922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.13
 ---- batch: 020 ----
mean loss: 240.57
 ---- batch: 030 ----
mean loss: 234.84
 ---- batch: 040 ----
mean loss: 240.83
 ---- batch: 050 ----
mean loss: 242.12
 ---- batch: 060 ----
mean loss: 247.79
 ---- batch: 070 ----
mean loss: 239.33
 ---- batch: 080 ----
mean loss: 244.33
 ---- batch: 090 ----
mean loss: 241.72
train mean loss: 242.24
epoch train time: 0:00:00.670452
elapsed time: 0:02:25.669012
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 22:28:00.800526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.66
 ---- batch: 020 ----
mean loss: 238.26
 ---- batch: 030 ----
mean loss: 238.76
 ---- batch: 040 ----
mean loss: 243.39
 ---- batch: 050 ----
mean loss: 236.01
 ---- batch: 060 ----
mean loss: 243.95
 ---- batch: 070 ----
mean loss: 249.47
 ---- batch: 080 ----
mean loss: 246.96
 ---- batch: 090 ----
mean loss: 239.71
train mean loss: 241.84
epoch train time: 0:00:00.664708
elapsed time: 0:02:26.333860
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 22:28:01.465380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.84
 ---- batch: 020 ----
mean loss: 243.16
 ---- batch: 030 ----
mean loss: 238.70
 ---- batch: 040 ----
mean loss: 243.42
 ---- batch: 050 ----
mean loss: 248.53
 ---- batch: 060 ----
mean loss: 242.76
 ---- batch: 070 ----
mean loss: 245.77
 ---- batch: 080 ----
mean loss: 230.71
 ---- batch: 090 ----
mean loss: 238.37
train mean loss: 241.36
epoch train time: 0:00:00.669520
elapsed time: 0:02:27.003544
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 22:28:02.135056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.44
 ---- batch: 020 ----
mean loss: 238.78
 ---- batch: 030 ----
mean loss: 229.32
 ---- batch: 040 ----
mean loss: 240.59
 ---- batch: 050 ----
mean loss: 241.91
 ---- batch: 060 ----
mean loss: 247.29
 ---- batch: 070 ----
mean loss: 250.56
 ---- batch: 080 ----
mean loss: 236.92
 ---- batch: 090 ----
mean loss: 246.08
train mean loss: 241.62
epoch train time: 0:00:00.658377
elapsed time: 0:02:27.662063
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 22:28:02.793573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.74
 ---- batch: 020 ----
mean loss: 228.95
 ---- batch: 030 ----
mean loss: 246.31
 ---- batch: 040 ----
mean loss: 248.93
 ---- batch: 050 ----
mean loss: 236.71
 ---- batch: 060 ----
mean loss: 234.28
 ---- batch: 070 ----
mean loss: 239.87
 ---- batch: 080 ----
mean loss: 242.24
 ---- batch: 090 ----
mean loss: 240.08
train mean loss: 241.36
epoch train time: 0:00:00.665439
elapsed time: 0:02:28.327650
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 22:28:03.459165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.91
 ---- batch: 020 ----
mean loss: 234.05
 ---- batch: 030 ----
mean loss: 246.93
 ---- batch: 040 ----
mean loss: 236.01
 ---- batch: 050 ----
mean loss: 233.34
 ---- batch: 060 ----
mean loss: 245.18
 ---- batch: 070 ----
mean loss: 242.85
 ---- batch: 080 ----
mean loss: 244.52
 ---- batch: 090 ----
mean loss: 242.59
train mean loss: 241.04
epoch train time: 0:00:00.660390
elapsed time: 0:02:28.988272
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 22:28:04.119790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.11
 ---- batch: 020 ----
mean loss: 242.73
 ---- batch: 030 ----
mean loss: 253.22
 ---- batch: 040 ----
mean loss: 231.29
 ---- batch: 050 ----
mean loss: 234.69
 ---- batch: 060 ----
mean loss: 245.05
 ---- batch: 070 ----
mean loss: 231.68
 ---- batch: 080 ----
mean loss: 246.06
 ---- batch: 090 ----
mean loss: 242.80
train mean loss: 240.82
epoch train time: 0:00:00.660074
elapsed time: 0:02:29.648494
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 22:28:04.780019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.57
 ---- batch: 020 ----
mean loss: 233.87
 ---- batch: 030 ----
mean loss: 240.66
 ---- batch: 040 ----
mean loss: 245.38
 ---- batch: 050 ----
mean loss: 238.58
 ---- batch: 060 ----
mean loss: 243.41
 ---- batch: 070 ----
mean loss: 244.42
 ---- batch: 080 ----
mean loss: 236.15
 ---- batch: 090 ----
mean loss: 240.34
train mean loss: 240.54
epoch train time: 0:00:00.662240
elapsed time: 0:02:30.310900
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 22:28:05.442416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.39
 ---- batch: 020 ----
mean loss: 237.66
 ---- batch: 030 ----
mean loss: 239.33
 ---- batch: 040 ----
mean loss: 242.50
 ---- batch: 050 ----
mean loss: 249.35
 ---- batch: 060 ----
mean loss: 241.57
 ---- batch: 070 ----
mean loss: 235.38
 ---- batch: 080 ----
mean loss: 231.97
 ---- batch: 090 ----
mean loss: 246.24
train mean loss: 240.36
epoch train time: 0:00:00.671481
elapsed time: 0:02:30.982549
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 22:28:06.114153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.84
 ---- batch: 020 ----
mean loss: 245.34
 ---- batch: 030 ----
mean loss: 238.71
 ---- batch: 040 ----
mean loss: 234.90
 ---- batch: 050 ----
mean loss: 231.44
 ---- batch: 060 ----
mean loss: 244.31
 ---- batch: 070 ----
mean loss: 243.41
 ---- batch: 080 ----
mean loss: 240.47
 ---- batch: 090 ----
mean loss: 236.65
train mean loss: 239.91
epoch train time: 0:00:00.669571
elapsed time: 0:02:31.652357
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 22:28:06.783873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.29
 ---- batch: 020 ----
mean loss: 246.40
 ---- batch: 030 ----
mean loss: 243.07
 ---- batch: 040 ----
mean loss: 243.81
 ---- batch: 050 ----
mean loss: 230.20
 ---- batch: 060 ----
mean loss: 242.88
 ---- batch: 070 ----
mean loss: 235.73
 ---- batch: 080 ----
mean loss: 232.29
 ---- batch: 090 ----
mean loss: 240.36
train mean loss: 240.13
epoch train time: 0:00:00.658108
elapsed time: 0:02:32.310628
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 22:28:07.442172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.03
 ---- batch: 020 ----
mean loss: 240.82
 ---- batch: 030 ----
mean loss: 243.16
 ---- batch: 040 ----
mean loss: 238.83
 ---- batch: 050 ----
mean loss: 244.74
 ---- batch: 060 ----
mean loss: 238.89
 ---- batch: 070 ----
mean loss: 233.88
 ---- batch: 080 ----
mean loss: 239.48
 ---- batch: 090 ----
mean loss: 237.93
train mean loss: 239.96
epoch train time: 0:00:00.669599
elapsed time: 0:02:32.980435
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 22:28:08.111962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.04
 ---- batch: 020 ----
mean loss: 235.64
 ---- batch: 030 ----
mean loss: 238.90
 ---- batch: 040 ----
mean loss: 247.28
 ---- batch: 050 ----
mean loss: 237.85
 ---- batch: 060 ----
mean loss: 237.18
 ---- batch: 070 ----
mean loss: 239.11
 ---- batch: 080 ----
mean loss: 240.13
 ---- batch: 090 ----
mean loss: 238.53
train mean loss: 239.98
epoch train time: 0:00:00.666427
elapsed time: 0:02:33.647021
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 22:28:08.778553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.37
 ---- batch: 020 ----
mean loss: 246.04
 ---- batch: 030 ----
mean loss: 238.46
 ---- batch: 040 ----
mean loss: 244.17
 ---- batch: 050 ----
mean loss: 231.98
 ---- batch: 060 ----
mean loss: 234.08
 ---- batch: 070 ----
mean loss: 241.54
 ---- batch: 080 ----
mean loss: 240.00
 ---- batch: 090 ----
mean loss: 239.39
train mean loss: 239.64
epoch train time: 0:00:00.670745
elapsed time: 0:02:34.317929
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 22:28:09.449467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.63
 ---- batch: 020 ----
mean loss: 239.51
 ---- batch: 030 ----
mean loss: 249.87
 ---- batch: 040 ----
mean loss: 237.98
 ---- batch: 050 ----
mean loss: 238.73
 ---- batch: 060 ----
mean loss: 242.05
 ---- batch: 070 ----
mean loss: 237.51
 ---- batch: 080 ----
mean loss: 241.48
 ---- batch: 090 ----
mean loss: 228.78
train mean loss: 239.25
epoch train time: 0:00:00.671635
elapsed time: 0:02:34.989736
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 22:28:10.121252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.78
 ---- batch: 020 ----
mean loss: 237.88
 ---- batch: 030 ----
mean loss: 236.20
 ---- batch: 040 ----
mean loss: 246.55
 ---- batch: 050 ----
mean loss: 243.77
 ---- batch: 060 ----
mean loss: 239.17
 ---- batch: 070 ----
mean loss: 239.25
 ---- batch: 080 ----
mean loss: 237.48
 ---- batch: 090 ----
mean loss: 235.93
train mean loss: 239.45
epoch train time: 0:00:00.668785
elapsed time: 0:02:35.658667
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 22:28:10.790179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.49
 ---- batch: 020 ----
mean loss: 234.64
 ---- batch: 030 ----
mean loss: 239.16
 ---- batch: 040 ----
mean loss: 242.17
 ---- batch: 050 ----
mean loss: 242.25
 ---- batch: 060 ----
mean loss: 248.21
 ---- batch: 070 ----
mean loss: 235.54
 ---- batch: 080 ----
mean loss: 236.39
 ---- batch: 090 ----
mean loss: 234.51
train mean loss: 239.41
epoch train time: 0:00:00.665008
elapsed time: 0:02:36.323837
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 22:28:11.455405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.44
 ---- batch: 020 ----
mean loss: 231.71
 ---- batch: 030 ----
mean loss: 239.14
 ---- batch: 040 ----
mean loss: 246.36
 ---- batch: 050 ----
mean loss: 248.92
 ---- batch: 060 ----
mean loss: 240.42
 ---- batch: 070 ----
mean loss: 236.23
 ---- batch: 080 ----
mean loss: 234.39
 ---- batch: 090 ----
mean loss: 239.47
train mean loss: 238.95
epoch train time: 0:00:00.667697
elapsed time: 0:02:36.991764
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 22:28:12.123280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.80
 ---- batch: 020 ----
mean loss: 235.02
 ---- batch: 030 ----
mean loss: 243.59
 ---- batch: 040 ----
mean loss: 241.69
 ---- batch: 050 ----
mean loss: 232.17
 ---- batch: 060 ----
mean loss: 240.35
 ---- batch: 070 ----
mean loss: 244.34
 ---- batch: 080 ----
mean loss: 234.89
 ---- batch: 090 ----
mean loss: 241.89
train mean loss: 239.02
epoch train time: 0:00:00.669018
elapsed time: 0:02:37.660942
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 22:28:12.792459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.62
 ---- batch: 020 ----
mean loss: 240.07
 ---- batch: 030 ----
mean loss: 237.65
 ---- batch: 040 ----
mean loss: 236.41
 ---- batch: 050 ----
mean loss: 239.56
 ---- batch: 060 ----
mean loss: 240.22
 ---- batch: 070 ----
mean loss: 236.47
 ---- batch: 080 ----
mean loss: 240.93
 ---- batch: 090 ----
mean loss: 240.68
train mean loss: 238.57
epoch train time: 0:00:00.663242
elapsed time: 0:02:38.324336
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 22:28:13.455852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.20
 ---- batch: 020 ----
mean loss: 240.84
 ---- batch: 030 ----
mean loss: 235.75
 ---- batch: 040 ----
mean loss: 232.80
 ---- batch: 050 ----
mean loss: 244.90
 ---- batch: 060 ----
mean loss: 231.54
 ---- batch: 070 ----
mean loss: 241.46
 ---- batch: 080 ----
mean loss: 242.12
 ---- batch: 090 ----
mean loss: 240.05
train mean loss: 238.85
epoch train time: 0:00:00.671200
elapsed time: 0:02:38.995682
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 22:28:14.127216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.38
 ---- batch: 020 ----
mean loss: 237.27
 ---- batch: 030 ----
mean loss: 247.75
 ---- batch: 040 ----
mean loss: 232.81
 ---- batch: 050 ----
mean loss: 239.86
 ---- batch: 060 ----
mean loss: 232.54
 ---- batch: 070 ----
mean loss: 229.29
 ---- batch: 080 ----
mean loss: 243.75
 ---- batch: 090 ----
mean loss: 242.06
train mean loss: 238.48
epoch train time: 0:00:00.670334
elapsed time: 0:02:39.666210
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 22:28:14.797732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.54
 ---- batch: 020 ----
mean loss: 236.67
 ---- batch: 030 ----
mean loss: 240.40
 ---- batch: 040 ----
mean loss: 243.19
 ---- batch: 050 ----
mean loss: 242.06
 ---- batch: 060 ----
mean loss: 235.70
 ---- batch: 070 ----
mean loss: 236.85
 ---- batch: 080 ----
mean loss: 248.86
 ---- batch: 090 ----
mean loss: 229.38
train mean loss: 238.32
epoch train time: 0:00:00.664772
elapsed time: 0:02:40.331171
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 22:28:15.462697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.90
 ---- batch: 020 ----
mean loss: 240.59
 ---- batch: 030 ----
mean loss: 239.97
 ---- batch: 040 ----
mean loss: 232.99
 ---- batch: 050 ----
mean loss: 241.05
 ---- batch: 060 ----
mean loss: 234.43
 ---- batch: 070 ----
mean loss: 240.05
 ---- batch: 080 ----
mean loss: 234.32
 ---- batch: 090 ----
mean loss: 237.23
train mean loss: 238.06
epoch train time: 0:00:00.667211
elapsed time: 0:02:40.998545
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 22:28:16.130060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.93
 ---- batch: 020 ----
mean loss: 237.47
 ---- batch: 030 ----
mean loss: 230.65
 ---- batch: 040 ----
mean loss: 244.20
 ---- batch: 050 ----
mean loss: 235.06
 ---- batch: 060 ----
mean loss: 241.04
 ---- batch: 070 ----
mean loss: 232.55
 ---- batch: 080 ----
mean loss: 237.90
 ---- batch: 090 ----
mean loss: 237.36
train mean loss: 237.58
epoch train time: 0:00:00.670355
elapsed time: 0:02:41.669045
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 22:28:16.800565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.20
 ---- batch: 020 ----
mean loss: 230.31
 ---- batch: 030 ----
mean loss: 237.47
 ---- batch: 040 ----
mean loss: 238.47
 ---- batch: 050 ----
mean loss: 237.80
 ---- batch: 060 ----
mean loss: 225.39
 ---- batch: 070 ----
mean loss: 242.95
 ---- batch: 080 ----
mean loss: 245.38
 ---- batch: 090 ----
mean loss: 238.24
train mean loss: 237.80
epoch train time: 0:00:00.666181
elapsed time: 0:02:42.335382
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 22:28:17.466895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.25
 ---- batch: 020 ----
mean loss: 236.71
 ---- batch: 030 ----
mean loss: 235.45
 ---- batch: 040 ----
mean loss: 235.79
 ---- batch: 050 ----
mean loss: 232.79
 ---- batch: 060 ----
mean loss: 241.78
 ---- batch: 070 ----
mean loss: 241.62
 ---- batch: 080 ----
mean loss: 234.98
 ---- batch: 090 ----
mean loss: 239.27
train mean loss: 237.94
epoch train time: 0:00:00.663079
elapsed time: 0:02:42.998607
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 22:28:18.130120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.21
 ---- batch: 020 ----
mean loss: 232.93
 ---- batch: 030 ----
mean loss: 242.47
 ---- batch: 040 ----
mean loss: 237.60
 ---- batch: 050 ----
mean loss: 235.50
 ---- batch: 060 ----
mean loss: 241.79
 ---- batch: 070 ----
mean loss: 239.63
 ---- batch: 080 ----
mean loss: 239.05
 ---- batch: 090 ----
mean loss: 235.92
train mean loss: 237.99
epoch train time: 0:00:00.660413
elapsed time: 0:02:43.659167
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 22:28:18.790695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.53
 ---- batch: 020 ----
mean loss: 236.76
 ---- batch: 030 ----
mean loss: 238.22
 ---- batch: 040 ----
mean loss: 239.46
 ---- batch: 050 ----
mean loss: 234.18
 ---- batch: 060 ----
mean loss: 230.71
 ---- batch: 070 ----
mean loss: 237.44
 ---- batch: 080 ----
mean loss: 233.57
 ---- batch: 090 ----
mean loss: 244.88
train mean loss: 237.29
epoch train time: 0:00:00.666521
elapsed time: 0:02:44.325846
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 22:28:19.457360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.47
 ---- batch: 020 ----
mean loss: 241.39
 ---- batch: 030 ----
mean loss: 238.54
 ---- batch: 040 ----
mean loss: 232.78
 ---- batch: 050 ----
mean loss: 235.97
 ---- batch: 060 ----
mean loss: 243.43
 ---- batch: 070 ----
mean loss: 232.25
 ---- batch: 080 ----
mean loss: 231.65
 ---- batch: 090 ----
mean loss: 246.42
train mean loss: 237.21
epoch train time: 0:00:00.666405
elapsed time: 0:02:44.992414
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 22:28:20.123937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.45
 ---- batch: 020 ----
mean loss: 232.79
 ---- batch: 030 ----
mean loss: 232.48
 ---- batch: 040 ----
mean loss: 244.61
 ---- batch: 050 ----
mean loss: 246.44
 ---- batch: 060 ----
mean loss: 237.30
 ---- batch: 070 ----
mean loss: 242.16
 ---- batch: 080 ----
mean loss: 241.54
 ---- batch: 090 ----
mean loss: 226.54
train mean loss: 237.18
epoch train time: 0:00:00.665610
elapsed time: 0:02:45.658226
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 22:28:20.789742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.22
 ---- batch: 020 ----
mean loss: 223.20
 ---- batch: 030 ----
mean loss: 240.77
 ---- batch: 040 ----
mean loss: 235.33
 ---- batch: 050 ----
mean loss: 242.58
 ---- batch: 060 ----
mean loss: 236.72
 ---- batch: 070 ----
mean loss: 236.27
 ---- batch: 080 ----
mean loss: 244.51
 ---- batch: 090 ----
mean loss: 233.32
train mean loss: 236.73
epoch train time: 0:00:00.665739
elapsed time: 0:02:46.324123
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 22:28:21.455638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.95
 ---- batch: 020 ----
mean loss: 236.75
 ---- batch: 030 ----
mean loss: 229.90
 ---- batch: 040 ----
mean loss: 237.15
 ---- batch: 050 ----
mean loss: 238.53
 ---- batch: 060 ----
mean loss: 230.54
 ---- batch: 070 ----
mean loss: 236.50
 ---- batch: 080 ----
mean loss: 242.58
 ---- batch: 090 ----
mean loss: 240.75
train mean loss: 237.11
epoch train time: 0:00:00.672028
elapsed time: 0:02:46.996298
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 22:28:22.127812
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 234.16
 ---- batch: 020 ----
mean loss: 232.71
 ---- batch: 030 ----
mean loss: 232.65
 ---- batch: 040 ----
mean loss: 247.61
 ---- batch: 050 ----
mean loss: 237.26
 ---- batch: 060 ----
mean loss: 224.21
 ---- batch: 070 ----
mean loss: 234.63
 ---- batch: 080 ----
mean loss: 238.70
 ---- batch: 090 ----
mean loss: 240.54
train mean loss: 236.48
epoch train time: 0:00:00.668517
elapsed time: 0:02:47.665014
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 22:28:22.796523
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 243.74
 ---- batch: 020 ----
mean loss: 233.75
 ---- batch: 030 ----
mean loss: 238.51
 ---- batch: 040 ----
mean loss: 228.45
 ---- batch: 050 ----
mean loss: 241.44
 ---- batch: 060 ----
mean loss: 227.00
 ---- batch: 070 ----
mean loss: 235.53
 ---- batch: 080 ----
mean loss: 238.36
 ---- batch: 090 ----
mean loss: 234.40
train mean loss: 236.32
epoch train time: 0:00:00.660527
elapsed time: 0:02:48.325683
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 22:28:23.457197
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 241.41
 ---- batch: 020 ----
mean loss: 236.21
 ---- batch: 030 ----
mean loss: 229.28
 ---- batch: 040 ----
mean loss: 235.04
 ---- batch: 050 ----
mean loss: 237.99
 ---- batch: 060 ----
mean loss: 231.34
 ---- batch: 070 ----
mean loss: 248.92
 ---- batch: 080 ----
mean loss: 236.69
 ---- batch: 090 ----
mean loss: 235.97
train mean loss: 236.16
epoch train time: 0:00:00.668426
elapsed time: 0:02:48.994250
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 22:28:24.125779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 235.92
 ---- batch: 020 ----
mean loss: 242.25
 ---- batch: 030 ----
mean loss: 240.09
 ---- batch: 040 ----
mean loss: 233.92
 ---- batch: 050 ----
mean loss: 235.05
 ---- batch: 060 ----
mean loss: 237.14
 ---- batch: 070 ----
mean loss: 230.85
 ---- batch: 080 ----
mean loss: 245.46
 ---- batch: 090 ----
mean loss: 229.89
train mean loss: 236.03
epoch train time: 0:00:00.657798
elapsed time: 0:02:49.652204
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 22:28:24.783714
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.91
 ---- batch: 020 ----
mean loss: 238.60
 ---- batch: 030 ----
mean loss: 247.54
 ---- batch: 040 ----
mean loss: 225.96
 ---- batch: 050 ----
mean loss: 237.02
 ---- batch: 060 ----
mean loss: 234.89
 ---- batch: 070 ----
mean loss: 237.04
 ---- batch: 080 ----
mean loss: 247.24
 ---- batch: 090 ----
mean loss: 231.33
train mean loss: 236.00
epoch train time: 0:00:00.659018
elapsed time: 0:02:50.311368
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 22:28:25.442882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.95
 ---- batch: 020 ----
mean loss: 232.42
 ---- batch: 030 ----
mean loss: 231.56
 ---- batch: 040 ----
mean loss: 250.02
 ---- batch: 050 ----
mean loss: 235.23
 ---- batch: 060 ----
mean loss: 233.82
 ---- batch: 070 ----
mean loss: 238.97
 ---- batch: 080 ----
mean loss: 240.85
 ---- batch: 090 ----
mean loss: 234.04
train mean loss: 236.32
epoch train time: 0:00:00.669381
elapsed time: 0:02:50.980898
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 22:28:26.112413
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 239.69
 ---- batch: 020 ----
mean loss: 231.15
 ---- batch: 030 ----
mean loss: 235.26
 ---- batch: 040 ----
mean loss: 245.12
 ---- batch: 050 ----
mean loss: 236.86
 ---- batch: 060 ----
mean loss: 225.84
 ---- batch: 070 ----
mean loss: 239.24
 ---- batch: 080 ----
mean loss: 233.74
 ---- batch: 090 ----
mean loss: 238.25
train mean loss: 236.24
epoch train time: 0:00:00.677365
elapsed time: 0:02:51.658411
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 22:28:26.789923
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 232.10
 ---- batch: 020 ----
mean loss: 239.74
 ---- batch: 030 ----
mean loss: 232.32
 ---- batch: 040 ----
mean loss: 248.12
 ---- batch: 050 ----
mean loss: 239.45
 ---- batch: 060 ----
mean loss: 230.73
 ---- batch: 070 ----
mean loss: 230.77
 ---- batch: 080 ----
mean loss: 235.24
 ---- batch: 090 ----
mean loss: 236.51
train mean loss: 236.12
epoch train time: 0:00:00.665429
elapsed time: 0:02:52.324004
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 22:28:27.455517
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.45
 ---- batch: 020 ----
mean loss: 240.78
 ---- batch: 030 ----
mean loss: 235.12
 ---- batch: 040 ----
mean loss: 233.75
 ---- batch: 050 ----
mean loss: 239.06
 ---- batch: 060 ----
mean loss: 240.10
 ---- batch: 070 ----
mean loss: 237.64
 ---- batch: 080 ----
mean loss: 237.10
 ---- batch: 090 ----
mean loss: 230.62
train mean loss: 236.28
epoch train time: 0:00:00.671019
elapsed time: 0:02:52.995169
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 22:28:28.126696
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 239.88
 ---- batch: 020 ----
mean loss: 239.58
 ---- batch: 030 ----
mean loss: 239.67
 ---- batch: 040 ----
mean loss: 231.41
 ---- batch: 050 ----
mean loss: 229.86
 ---- batch: 060 ----
mean loss: 245.31
 ---- batch: 070 ----
mean loss: 234.69
 ---- batch: 080 ----
mean loss: 233.72
 ---- batch: 090 ----
mean loss: 231.81
train mean loss: 235.98
epoch train time: 0:00:00.662839
elapsed time: 0:02:53.658196
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 22:28:28.789730
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 239.15
 ---- batch: 020 ----
mean loss: 236.40
 ---- batch: 030 ----
mean loss: 235.21
 ---- batch: 040 ----
mean loss: 236.24
 ---- batch: 050 ----
mean loss: 233.42
 ---- batch: 060 ----
mean loss: 237.08
 ---- batch: 070 ----
mean loss: 239.02
 ---- batch: 080 ----
mean loss: 237.60
 ---- batch: 090 ----
mean loss: 233.11
train mean loss: 236.40
epoch train time: 0:00:00.670675
elapsed time: 0:02:54.329057
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 22:28:29.460571
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 235.71
 ---- batch: 020 ----
mean loss: 239.32
 ---- batch: 030 ----
mean loss: 237.19
 ---- batch: 040 ----
mean loss: 238.92
 ---- batch: 050 ----
mean loss: 230.59
 ---- batch: 060 ----
mean loss: 228.05
 ---- batch: 070 ----
mean loss: 240.63
 ---- batch: 080 ----
mean loss: 236.74
 ---- batch: 090 ----
mean loss: 236.62
train mean loss: 236.14
epoch train time: 0:00:00.672770
elapsed time: 0:02:55.001978
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 22:28:30.133493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 238.94
 ---- batch: 020 ----
mean loss: 236.25
 ---- batch: 030 ----
mean loss: 236.06
 ---- batch: 040 ----
mean loss: 235.60
 ---- batch: 050 ----
mean loss: 240.26
 ---- batch: 060 ----
mean loss: 240.40
 ---- batch: 070 ----
mean loss: 230.63
 ---- batch: 080 ----
mean loss: 234.37
 ---- batch: 090 ----
mean loss: 234.51
train mean loss: 235.99
epoch train time: 0:00:00.668605
elapsed time: 0:02:55.670734
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 22:28:30.802264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.53
 ---- batch: 020 ----
mean loss: 221.86
 ---- batch: 030 ----
mean loss: 233.24
 ---- batch: 040 ----
mean loss: 231.61
 ---- batch: 050 ----
mean loss: 238.95
 ---- batch: 060 ----
mean loss: 246.04
 ---- batch: 070 ----
mean loss: 243.32
 ---- batch: 080 ----
mean loss: 244.97
 ---- batch: 090 ----
mean loss: 237.84
train mean loss: 236.51
epoch train time: 0:00:00.660439
elapsed time: 0:02:56.331338
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 22:28:31.462851
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 239.54
 ---- batch: 020 ----
mean loss: 239.14
 ---- batch: 030 ----
mean loss: 235.34
 ---- batch: 040 ----
mean loss: 232.70
 ---- batch: 050 ----
mean loss: 233.43
 ---- batch: 060 ----
mean loss: 242.47
 ---- batch: 070 ----
mean loss: 233.00
 ---- batch: 080 ----
mean loss: 234.13
 ---- batch: 090 ----
mean loss: 232.22
train mean loss: 236.15
epoch train time: 0:00:00.662985
elapsed time: 0:02:56.994480
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 22:28:32.125990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 237.48
 ---- batch: 020 ----
mean loss: 234.49
 ---- batch: 030 ----
mean loss: 238.90
 ---- batch: 040 ----
mean loss: 234.50
 ---- batch: 050 ----
mean loss: 235.90
 ---- batch: 060 ----
mean loss: 238.00
 ---- batch: 070 ----
mean loss: 231.13
 ---- batch: 080 ----
mean loss: 241.25
 ---- batch: 090 ----
mean loss: 234.27
train mean loss: 235.93
epoch train time: 0:00:00.662061
elapsed time: 0:02:57.656680
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 22:28:32.788190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 240.28
 ---- batch: 020 ----
mean loss: 238.77
 ---- batch: 030 ----
mean loss: 240.34
 ---- batch: 040 ----
mean loss: 231.20
 ---- batch: 050 ----
mean loss: 237.61
 ---- batch: 060 ----
mean loss: 234.37
 ---- batch: 070 ----
mean loss: 228.88
 ---- batch: 080 ----
mean loss: 243.94
 ---- batch: 090 ----
mean loss: 234.11
train mean loss: 236.13
epoch train time: 0:00:00.680376
elapsed time: 0:02:58.337276
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 22:28:33.468787
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 235.40
 ---- batch: 020 ----
mean loss: 241.97
 ---- batch: 030 ----
mean loss: 239.93
 ---- batch: 040 ----
mean loss: 237.24
 ---- batch: 050 ----
mean loss: 230.98
 ---- batch: 060 ----
mean loss: 232.44
 ---- batch: 070 ----
mean loss: 231.67
 ---- batch: 080 ----
mean loss: 240.01
 ---- batch: 090 ----
mean loss: 235.56
train mean loss: 236.30
epoch train time: 0:00:00.681517
elapsed time: 0:02:59.018956
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 22:28:34.150487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.34
 ---- batch: 020 ----
mean loss: 232.92
 ---- batch: 030 ----
mean loss: 241.70
 ---- batch: 040 ----
mean loss: 237.67
 ---- batch: 050 ----
mean loss: 244.78
 ---- batch: 060 ----
mean loss: 227.49
 ---- batch: 070 ----
mean loss: 233.97
 ---- batch: 080 ----
mean loss: 243.74
 ---- batch: 090 ----
mean loss: 234.97
train mean loss: 235.95
epoch train time: 0:00:00.690232
elapsed time: 0:02:59.709359
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 22:28:34.840863
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 235.69
 ---- batch: 020 ----
mean loss: 230.67
 ---- batch: 030 ----
mean loss: 239.77
 ---- batch: 040 ----
mean loss: 229.45
 ---- batch: 050 ----
mean loss: 238.80
 ---- batch: 060 ----
mean loss: 234.35
 ---- batch: 070 ----
mean loss: 238.30
 ---- batch: 080 ----
mean loss: 241.41
 ---- batch: 090 ----
mean loss: 235.80
train mean loss: 236.26
epoch train time: 0:00:00.679548
elapsed time: 0:03:00.389042
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 22:28:35.520554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.93
 ---- batch: 020 ----
mean loss: 234.48
 ---- batch: 030 ----
mean loss: 237.14
 ---- batch: 040 ----
mean loss: 235.70
 ---- batch: 050 ----
mean loss: 236.44
 ---- batch: 060 ----
mean loss: 239.86
 ---- batch: 070 ----
mean loss: 237.64
 ---- batch: 080 ----
mean loss: 247.55
 ---- batch: 090 ----
mean loss: 225.33
train mean loss: 235.90
epoch train time: 0:00:00.686294
elapsed time: 0:03:01.075478
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 22:28:36.206988
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 238.37
 ---- batch: 020 ----
mean loss: 233.80
 ---- batch: 030 ----
mean loss: 231.96
 ---- batch: 040 ----
mean loss: 231.04
 ---- batch: 050 ----
mean loss: 230.99
 ---- batch: 060 ----
mean loss: 236.93
 ---- batch: 070 ----
mean loss: 237.70
 ---- batch: 080 ----
mean loss: 249.73
 ---- batch: 090 ----
mean loss: 238.19
train mean loss: 235.89
epoch train time: 0:00:00.681294
elapsed time: 0:03:01.756914
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 22:28:36.888436
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 239.36
 ---- batch: 020 ----
mean loss: 230.40
 ---- batch: 030 ----
mean loss: 242.93
 ---- batch: 040 ----
mean loss: 234.74
 ---- batch: 050 ----
mean loss: 238.07
 ---- batch: 060 ----
mean loss: 232.17
 ---- batch: 070 ----
mean loss: 235.66
 ---- batch: 080 ----
mean loss: 227.69
 ---- batch: 090 ----
mean loss: 246.84
train mean loss: 235.88
epoch train time: 0:00:00.678937
elapsed time: 0:03:02.436009
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 22:28:37.567535
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 232.92
 ---- batch: 020 ----
mean loss: 228.82
 ---- batch: 030 ----
mean loss: 233.60
 ---- batch: 040 ----
mean loss: 236.21
 ---- batch: 050 ----
mean loss: 238.47
 ---- batch: 060 ----
mean loss: 233.97
 ---- batch: 070 ----
mean loss: 236.71
 ---- batch: 080 ----
mean loss: 246.24
 ---- batch: 090 ----
mean loss: 237.18
train mean loss: 235.94
epoch train time: 0:00:00.682533
elapsed time: 0:03:03.118760
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 22:28:38.250315
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 232.99
 ---- batch: 020 ----
mean loss: 242.09
 ---- batch: 030 ----
mean loss: 234.62
 ---- batch: 040 ----
mean loss: 234.51
 ---- batch: 050 ----
mean loss: 235.26
 ---- batch: 060 ----
mean loss: 231.60
 ---- batch: 070 ----
mean loss: 238.05
 ---- batch: 080 ----
mean loss: 232.25
 ---- batch: 090 ----
mean loss: 238.69
train mean loss: 235.88
epoch train time: 0:00:00.682924
elapsed time: 0:03:03.801874
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 22:28:38.933405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 237.07
 ---- batch: 020 ----
mean loss: 240.07
 ---- batch: 030 ----
mean loss: 239.65
 ---- batch: 040 ----
mean loss: 228.84
 ---- batch: 050 ----
mean loss: 234.84
 ---- batch: 060 ----
mean loss: 236.08
 ---- batch: 070 ----
mean loss: 229.42
 ---- batch: 080 ----
mean loss: 237.63
 ---- batch: 090 ----
mean loss: 240.92
train mean loss: 236.08
epoch train time: 0:00:00.672712
elapsed time: 0:03:04.474754
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 22:28:39.606270
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 237.57
 ---- batch: 020 ----
mean loss: 240.27
 ---- batch: 030 ----
mean loss: 233.95
 ---- batch: 040 ----
mean loss: 235.68
 ---- batch: 050 ----
mean loss: 242.37
 ---- batch: 060 ----
mean loss: 233.71
 ---- batch: 070 ----
mean loss: 235.06
 ---- batch: 080 ----
mean loss: 237.52
 ---- batch: 090 ----
mean loss: 233.35
train mean loss: 235.86
epoch train time: 0:00:00.670214
elapsed time: 0:03:05.145114
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 22:28:40.276642
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.40
 ---- batch: 020 ----
mean loss: 230.57
 ---- batch: 030 ----
mean loss: 237.80
 ---- batch: 040 ----
mean loss: 236.61
 ---- batch: 050 ----
mean loss: 230.38
 ---- batch: 060 ----
mean loss: 234.53
 ---- batch: 070 ----
mean loss: 239.93
 ---- batch: 080 ----
mean loss: 232.05
 ---- batch: 090 ----
mean loss: 233.43
train mean loss: 235.81
epoch train time: 0:00:00.662429
elapsed time: 0:03:05.807700
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 22:28:40.939212
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.45
 ---- batch: 020 ----
mean loss: 239.96
 ---- batch: 030 ----
mean loss: 230.94
 ---- batch: 040 ----
mean loss: 243.33
 ---- batch: 050 ----
mean loss: 239.06
 ---- batch: 060 ----
mean loss: 229.23
 ---- batch: 070 ----
mean loss: 240.28
 ---- batch: 080 ----
mean loss: 229.12
 ---- batch: 090 ----
mean loss: 244.98
train mean loss: 235.84
epoch train time: 0:00:00.665951
elapsed time: 0:03:06.473792
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 22:28:41.605317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 242.10
 ---- batch: 020 ----
mean loss: 237.25
 ---- batch: 030 ----
mean loss: 243.71
 ---- batch: 040 ----
mean loss: 232.89
 ---- batch: 050 ----
mean loss: 227.17
 ---- batch: 060 ----
mean loss: 231.74
 ---- batch: 070 ----
mean loss: 236.39
 ---- batch: 080 ----
mean loss: 239.05
 ---- batch: 090 ----
mean loss: 235.32
train mean loss: 236.02
epoch train time: 0:00:00.666979
elapsed time: 0:03:07.140939
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 22:28:42.272474
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 245.81
 ---- batch: 020 ----
mean loss: 231.32
 ---- batch: 030 ----
mean loss: 237.26
 ---- batch: 040 ----
mean loss: 232.07
 ---- batch: 050 ----
mean loss: 233.82
 ---- batch: 060 ----
mean loss: 228.74
 ---- batch: 070 ----
mean loss: 232.22
 ---- batch: 080 ----
mean loss: 236.29
 ---- batch: 090 ----
mean loss: 242.03
train mean loss: 235.65
epoch train time: 0:00:00.669718
elapsed time: 0:03:07.810853
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 22:28:42.942367
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 244.33
 ---- batch: 020 ----
mean loss: 237.62
 ---- batch: 030 ----
mean loss: 240.02
 ---- batch: 040 ----
mean loss: 232.82
 ---- batch: 050 ----
mean loss: 227.18
 ---- batch: 060 ----
mean loss: 240.06
 ---- batch: 070 ----
mean loss: 232.95
 ---- batch: 080 ----
mean loss: 231.47
 ---- batch: 090 ----
mean loss: 241.01
train mean loss: 235.65
epoch train time: 0:00:00.655326
elapsed time: 0:03:08.466325
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 22:28:43.597837
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 237.32
 ---- batch: 020 ----
mean loss: 233.32
 ---- batch: 030 ----
mean loss: 230.76
 ---- batch: 040 ----
mean loss: 231.96
 ---- batch: 050 ----
mean loss: 237.87
 ---- batch: 060 ----
mean loss: 233.22
 ---- batch: 070 ----
mean loss: 232.02
 ---- batch: 080 ----
mean loss: 239.14
 ---- batch: 090 ----
mean loss: 239.51
train mean loss: 235.81
epoch train time: 0:00:00.672595
elapsed time: 0:03:09.139081
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 22:28:44.270600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 233.88
 ---- batch: 020 ----
mean loss: 234.43
 ---- batch: 030 ----
mean loss: 237.86
 ---- batch: 040 ----
mean loss: 240.73
 ---- batch: 050 ----
mean loss: 241.39
 ---- batch: 060 ----
mean loss: 232.17
 ---- batch: 070 ----
mean loss: 242.07
 ---- batch: 080 ----
mean loss: 231.81
 ---- batch: 090 ----
mean loss: 231.13
train mean loss: 235.93
epoch train time: 0:00:00.665978
elapsed time: 0:03:09.805230
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 22:28:44.936747
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 233.87
 ---- batch: 020 ----
mean loss: 233.08
 ---- batch: 030 ----
mean loss: 234.76
 ---- batch: 040 ----
mean loss: 230.13
 ---- batch: 050 ----
mean loss: 240.75
 ---- batch: 060 ----
mean loss: 245.24
 ---- batch: 070 ----
mean loss: 233.21
 ---- batch: 080 ----
mean loss: 235.66
 ---- batch: 090 ----
mean loss: 234.74
train mean loss: 235.76
epoch train time: 0:00:00.665726
elapsed time: 0:03:10.471123
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 22:28:45.602639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 232.90
 ---- batch: 020 ----
mean loss: 237.85
 ---- batch: 030 ----
mean loss: 232.74
 ---- batch: 040 ----
mean loss: 242.37
 ---- batch: 050 ----
mean loss: 237.04
 ---- batch: 060 ----
mean loss: 228.18
 ---- batch: 070 ----
mean loss: 244.91
 ---- batch: 080 ----
mean loss: 235.69
 ---- batch: 090 ----
mean loss: 235.55
train mean loss: 235.59
epoch train time: 0:00:00.665894
elapsed time: 0:03:11.137167
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 22:28:46.268681
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 236.20
 ---- batch: 020 ----
mean loss: 228.39
 ---- batch: 030 ----
mean loss: 239.41
 ---- batch: 040 ----
mean loss: 238.99
 ---- batch: 050 ----
mean loss: 235.87
 ---- batch: 060 ----
mean loss: 229.37
 ---- batch: 070 ----
mean loss: 237.20
 ---- batch: 080 ----
mean loss: 237.23
 ---- batch: 090 ----
mean loss: 237.00
train mean loss: 235.96
epoch train time: 0:00:00.664183
elapsed time: 0:03:11.801501
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 22:28:46.933012
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 239.57
 ---- batch: 020 ----
mean loss: 233.45
 ---- batch: 030 ----
mean loss: 234.84
 ---- batch: 040 ----
mean loss: 235.04
 ---- batch: 050 ----
mean loss: 240.00
 ---- batch: 060 ----
mean loss: 225.96
 ---- batch: 070 ----
mean loss: 232.84
 ---- batch: 080 ----
mean loss: 235.76
 ---- batch: 090 ----
mean loss: 241.52
train mean loss: 236.08
epoch train time: 0:00:00.655553
elapsed time: 0:03:12.457195
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 22:28:47.588708
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 235.92
 ---- batch: 020 ----
mean loss: 235.94
 ---- batch: 030 ----
mean loss: 231.22
 ---- batch: 040 ----
mean loss: 228.36
 ---- batch: 050 ----
mean loss: 233.79
 ---- batch: 060 ----
mean loss: 246.75
 ---- batch: 070 ----
mean loss: 230.67
 ---- batch: 080 ----
mean loss: 239.14
 ---- batch: 090 ----
mean loss: 238.99
train mean loss: 235.85
epoch train time: 0:00:00.658773
elapsed time: 0:03:13.116112
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 22:28:48.247624
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 238.31
 ---- batch: 020 ----
mean loss: 231.34
 ---- batch: 030 ----
mean loss: 236.87
 ---- batch: 040 ----
mean loss: 235.51
 ---- batch: 050 ----
mean loss: 237.59
 ---- batch: 060 ----
mean loss: 231.80
 ---- batch: 070 ----
mean loss: 237.29
 ---- batch: 080 ----
mean loss: 238.86
 ---- batch: 090 ----
mean loss: 230.80
train mean loss: 235.99
epoch train time: 0:00:00.661292
elapsed time: 0:03:13.777548
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 22:28:48.909060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 237.52
 ---- batch: 020 ----
mean loss: 234.63
 ---- batch: 030 ----
mean loss: 241.41
 ---- batch: 040 ----
mean loss: 229.68
 ---- batch: 050 ----
mean loss: 234.58
 ---- batch: 060 ----
mean loss: 228.10
 ---- batch: 070 ----
mean loss: 240.37
 ---- batch: 080 ----
mean loss: 230.69
 ---- batch: 090 ----
mean loss: 239.55
train mean loss: 235.97
epoch train time: 0:00:00.679704
elapsed time: 0:03:14.457406
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 22:28:49.588977
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 240.45
 ---- batch: 020 ----
mean loss: 231.84
 ---- batch: 030 ----
mean loss: 239.67
 ---- batch: 040 ----
mean loss: 228.45
 ---- batch: 050 ----
mean loss: 231.35
 ---- batch: 060 ----
mean loss: 231.04
 ---- batch: 070 ----
mean loss: 238.35
 ---- batch: 080 ----
mean loss: 236.12
 ---- batch: 090 ----
mean loss: 245.54
train mean loss: 235.77
epoch train time: 0:00:00.671681
elapsed time: 0:03:15.129286
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 22:28:50.260796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 235.84
 ---- batch: 020 ----
mean loss: 238.51
 ---- batch: 030 ----
mean loss: 236.40
 ---- batch: 040 ----
mean loss: 225.49
 ---- batch: 050 ----
mean loss: 234.20
 ---- batch: 060 ----
mean loss: 241.29
 ---- batch: 070 ----
mean loss: 239.33
 ---- batch: 080 ----
mean loss: 228.74
 ---- batch: 090 ----
mean loss: 242.36
train mean loss: 235.58
epoch train time: 0:00:00.663785
elapsed time: 0:03:15.793222
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 22:28:50.924737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 234.11
 ---- batch: 020 ----
mean loss: 243.59
 ---- batch: 030 ----
mean loss: 229.40
 ---- batch: 040 ----
mean loss: 234.54
 ---- batch: 050 ----
mean loss: 242.15
 ---- batch: 060 ----
mean loss: 228.30
 ---- batch: 070 ----
mean loss: 233.80
 ---- batch: 080 ----
mean loss: 235.39
 ---- batch: 090 ----
mean loss: 239.48
train mean loss: 235.58
epoch train time: 0:00:00.667471
elapsed time: 0:03:16.460859
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 22:28:51.592372
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 234.20
 ---- batch: 020 ----
mean loss: 235.50
 ---- batch: 030 ----
mean loss: 234.93
 ---- batch: 040 ----
mean loss: 238.01
 ---- batch: 050 ----
mean loss: 235.20
 ---- batch: 060 ----
mean loss: 233.40
 ---- batch: 070 ----
mean loss: 230.49
 ---- batch: 080 ----
mean loss: 243.06
 ---- batch: 090 ----
mean loss: 237.71
train mean loss: 235.61
epoch train time: 0:00:00.665193
elapsed time: 0:03:17.126220
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 22:28:52.257755
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 243.10
 ---- batch: 020 ----
mean loss: 237.63
 ---- batch: 030 ----
mean loss: 231.45
 ---- batch: 040 ----
mean loss: 242.38
 ---- batch: 050 ----
mean loss: 238.57
 ---- batch: 060 ----
mean loss: 224.94
 ---- batch: 070 ----
mean loss: 231.63
 ---- batch: 080 ----
mean loss: 227.26
 ---- batch: 090 ----
mean loss: 243.01
train mean loss: 235.74
epoch train time: 0:00:00.684753
elapsed time: 0:03:17.811166
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 22:28:52.942687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 233.88
 ---- batch: 020 ----
mean loss: 232.83
 ---- batch: 030 ----
mean loss: 233.80
 ---- batch: 040 ----
mean loss: 241.89
 ---- batch: 050 ----
mean loss: 237.28
 ---- batch: 060 ----
mean loss: 244.76
 ---- batch: 070 ----
mean loss: 227.56
 ---- batch: 080 ----
mean loss: 227.06
 ---- batch: 090 ----
mean loss: 239.52
train mean loss: 235.47
epoch train time: 0:00:00.665231
elapsed time: 0:03:18.476551
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 22:28:53.608063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.97
 ---- batch: 020 ----
mean loss: 237.99
 ---- batch: 030 ----
mean loss: 230.84
 ---- batch: 040 ----
mean loss: 239.06
 ---- batch: 050 ----
mean loss: 231.10
 ---- batch: 060 ----
mean loss: 235.67
 ---- batch: 070 ----
mean loss: 245.21
 ---- batch: 080 ----
mean loss: 231.27
 ---- batch: 090 ----
mean loss: 232.04
train mean loss: 235.69
epoch train time: 0:00:00.668073
elapsed time: 0:03:19.144766
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 22:28:54.276302
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.07
 ---- batch: 020 ----
mean loss: 238.80
 ---- batch: 030 ----
mean loss: 232.54
 ---- batch: 040 ----
mean loss: 229.86
 ---- batch: 050 ----
mean loss: 248.21
 ---- batch: 060 ----
mean loss: 234.43
 ---- batch: 070 ----
mean loss: 233.44
 ---- batch: 080 ----
mean loss: 238.32
 ---- batch: 090 ----
mean loss: 239.43
train mean loss: 235.32
epoch train time: 0:00:00.670438
elapsed time: 0:03:19.817686
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_0/checkpoint.pth.tar
**** end time: 2019-09-25 22:28:54.949165 ****
