Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_7', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18312
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistDense3...
Done.
**** start time: 2019-09-27 02:26:00.023699 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
            Linear-2                  [-1, 100]          36,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 56,100
Trainable params: 56,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:26:00.027130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4960.71
 ---- batch: 020 ----
mean loss: 4852.67
 ---- batch: 030 ----
mean loss: 4757.13
 ---- batch: 040 ----
mean loss: 4655.29
 ---- batch: 050 ----
mean loss: 4586.26
 ---- batch: 060 ----
mean loss: 4477.36
 ---- batch: 070 ----
mean loss: 4440.18
 ---- batch: 080 ----
mean loss: 4365.10
 ---- batch: 090 ----
mean loss: 4292.57
 ---- batch: 100 ----
mean loss: 4257.24
 ---- batch: 110 ----
mean loss: 4216.18
train mean loss: 4522.40
epoch train time: 0:00:32.853657
elapsed time: 0:00:32.859369
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:26:32.883104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4090.89
 ---- batch: 020 ----
mean loss: 3995.69
 ---- batch: 030 ----
mean loss: 3963.53
 ---- batch: 040 ----
mean loss: 3891.50
 ---- batch: 050 ----
mean loss: 3863.52
 ---- batch: 060 ----
mean loss: 3767.02
 ---- batch: 070 ----
mean loss: 3668.10
 ---- batch: 080 ----
mean loss: 3647.75
 ---- batch: 090 ----
mean loss: 3553.68
 ---- batch: 100 ----
mean loss: 3459.64
 ---- batch: 110 ----
mean loss: 3372.43
train mean loss: 3743.65
epoch train time: 0:00:00.563569
elapsed time: 0:00:33.423110
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:26:33.446884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3342.89
 ---- batch: 020 ----
mean loss: 3281.80
 ---- batch: 030 ----
mean loss: 3250.07
 ---- batch: 040 ----
mean loss: 3185.48
 ---- batch: 050 ----
mean loss: 3116.42
 ---- batch: 060 ----
mean loss: 3047.32
 ---- batch: 070 ----
mean loss: 3014.62
 ---- batch: 080 ----
mean loss: 2924.02
 ---- batch: 090 ----
mean loss: 2858.22
 ---- batch: 100 ----
mean loss: 2847.81
 ---- batch: 110 ----
mean loss: 2719.99
train mean loss: 3046.56
epoch train time: 0:00:00.561264
elapsed time: 0:00:33.984538
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:26:34.008308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2699.00
 ---- batch: 020 ----
mean loss: 2612.12
 ---- batch: 030 ----
mean loss: 2636.75
 ---- batch: 040 ----
mean loss: 2595.54
 ---- batch: 050 ----
mean loss: 2512.81
 ---- batch: 060 ----
mean loss: 2500.84
 ---- batch: 070 ----
mean loss: 2452.73
 ---- batch: 080 ----
mean loss: 2447.03
 ---- batch: 090 ----
mean loss: 2353.66
 ---- batch: 100 ----
mean loss: 2334.94
 ---- batch: 110 ----
mean loss: 2293.18
train mean loss: 2488.33
epoch train time: 0:00:00.543871
elapsed time: 0:00:34.528567
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:26:34.552318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2277.81
 ---- batch: 020 ----
mean loss: 2214.28
 ---- batch: 030 ----
mean loss: 2162.65
 ---- batch: 040 ----
mean loss: 2120.96
 ---- batch: 050 ----
mean loss: 2096.38
 ---- batch: 060 ----
mean loss: 2046.96
 ---- batch: 070 ----
mean loss: 1994.01
 ---- batch: 080 ----
mean loss: 1956.71
 ---- batch: 090 ----
mean loss: 1946.90
 ---- batch: 100 ----
mean loss: 1929.58
 ---- batch: 110 ----
mean loss: 1907.03
train mean loss: 2053.82
epoch train time: 0:00:00.554629
elapsed time: 0:00:35.083340
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:26:35.107088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1862.10
 ---- batch: 020 ----
mean loss: 1828.39
 ---- batch: 030 ----
mean loss: 1793.91
 ---- batch: 040 ----
mean loss: 1794.55
 ---- batch: 050 ----
mean loss: 1721.35
 ---- batch: 060 ----
mean loss: 1697.47
 ---- batch: 070 ----
mean loss: 1664.76
 ---- batch: 080 ----
mean loss: 1638.39
 ---- batch: 090 ----
mean loss: 1627.97
 ---- batch: 100 ----
mean loss: 1622.53
 ---- batch: 110 ----
mean loss: 1585.63
train mean loss: 1709.25
epoch train time: 0:00:00.562347
elapsed time: 0:00:35.645820
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:26:35.669568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1564.64
 ---- batch: 020 ----
mean loss: 1534.13
 ---- batch: 030 ----
mean loss: 1505.21
 ---- batch: 040 ----
mean loss: 1496.78
 ---- batch: 050 ----
mean loss: 1507.85
 ---- batch: 060 ----
mean loss: 1430.67
 ---- batch: 070 ----
mean loss: 1424.74
 ---- batch: 080 ----
mean loss: 1406.30
 ---- batch: 090 ----
mean loss: 1402.53
 ---- batch: 100 ----
mean loss: 1369.86
 ---- batch: 110 ----
mean loss: 1385.93
train mean loss: 1454.43
epoch train time: 0:00:00.553381
elapsed time: 0:00:36.199341
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:26:36.223090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1326.86
 ---- batch: 020 ----
mean loss: 1335.52
 ---- batch: 030 ----
mean loss: 1299.63
 ---- batch: 040 ----
mean loss: 1281.47
 ---- batch: 050 ----
mean loss: 1280.93
 ---- batch: 060 ----
mean loss: 1267.40
 ---- batch: 070 ----
mean loss: 1249.28
 ---- batch: 080 ----
mean loss: 1240.00
 ---- batch: 090 ----
mean loss: 1244.70
 ---- batch: 100 ----
mean loss: 1232.22
 ---- batch: 110 ----
mean loss: 1181.74
train mean loss: 1265.67
epoch train time: 0:00:00.561062
elapsed time: 0:00:36.760537
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:26:36.784282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1173.39
 ---- batch: 020 ----
mean loss: 1180.48
 ---- batch: 030 ----
mean loss: 1150.57
 ---- batch: 040 ----
mean loss: 1133.50
 ---- batch: 050 ----
mean loss: 1138.69
 ---- batch: 060 ----
mean loss: 1135.15
 ---- batch: 070 ----
mean loss: 1125.13
 ---- batch: 080 ----
mean loss: 1107.71
 ---- batch: 090 ----
mean loss: 1093.91
 ---- batch: 100 ----
mean loss: 1096.84
 ---- batch: 110 ----
mean loss: 1089.10
train mean loss: 1127.27
epoch train time: 0:00:00.549480
elapsed time: 0:00:37.310163
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:26:37.333908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1065.03
 ---- batch: 020 ----
mean loss: 1064.82
 ---- batch: 030 ----
mean loss: 1036.76
 ---- batch: 040 ----
mean loss: 1033.07
 ---- batch: 050 ----
mean loss: 1026.21
 ---- batch: 060 ----
mean loss: 1027.37
 ---- batch: 070 ----
mean loss: 1022.01
 ---- batch: 080 ----
mean loss: 1016.85
 ---- batch: 090 ----
mean loss: 1004.82
 ---- batch: 100 ----
mean loss: 983.41
 ---- batch: 110 ----
mean loss: 1000.37
train mean loss: 1024.55
epoch train time: 0:00:00.550820
elapsed time: 0:00:37.861115
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:26:37.884912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.52
 ---- batch: 020 ----
mean loss: 969.97
 ---- batch: 030 ----
mean loss: 972.63
 ---- batch: 040 ----
mean loss: 971.58
 ---- batch: 050 ----
mean loss: 960.11
 ---- batch: 060 ----
mean loss: 955.84
 ---- batch: 070 ----
mean loss: 942.72
 ---- batch: 080 ----
mean loss: 943.61
 ---- batch: 090 ----
mean loss: 949.71
 ---- batch: 100 ----
mean loss: 943.67
 ---- batch: 110 ----
mean loss: 929.84
train mean loss: 955.76
epoch train time: 0:00:00.541593
elapsed time: 0:00:38.402893
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:26:38.426639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.16
 ---- batch: 020 ----
mean loss: 918.97
 ---- batch: 030 ----
mean loss: 924.10
 ---- batch: 040 ----
mean loss: 914.11
 ---- batch: 050 ----
mean loss: 903.66
 ---- batch: 060 ----
mean loss: 908.35
 ---- batch: 070 ----
mean loss: 911.94
 ---- batch: 080 ----
mean loss: 902.52
 ---- batch: 090 ----
mean loss: 907.88
 ---- batch: 100 ----
mean loss: 901.72
 ---- batch: 110 ----
mean loss: 884.71
train mean loss: 910.99
epoch train time: 0:00:00.560089
elapsed time: 0:00:38.963133
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:26:38.986895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.12
 ---- batch: 020 ----
mean loss: 899.90
 ---- batch: 030 ----
mean loss: 894.60
 ---- batch: 040 ----
mean loss: 884.43
 ---- batch: 050 ----
mean loss: 880.05
 ---- batch: 060 ----
mean loss: 867.23
 ---- batch: 070 ----
mean loss: 891.40
 ---- batch: 080 ----
mean loss: 864.27
 ---- batch: 090 ----
mean loss: 874.42
 ---- batch: 100 ----
mean loss: 886.49
 ---- batch: 110 ----
mean loss: 861.44
train mean loss: 882.68
epoch train time: 0:00:00.545029
elapsed time: 0:00:39.508310
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:26:39.532055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.05
 ---- batch: 020 ----
mean loss: 867.17
 ---- batch: 030 ----
mean loss: 865.80
 ---- batch: 040 ----
mean loss: 857.70
 ---- batch: 050 ----
mean loss: 860.99
 ---- batch: 060 ----
mean loss: 874.48
 ---- batch: 070 ----
mean loss: 866.89
 ---- batch: 080 ----
mean loss: 869.62
 ---- batch: 090 ----
mean loss: 857.48
 ---- batch: 100 ----
mean loss: 867.50
 ---- batch: 110 ----
mean loss: 867.23
train mean loss: 865.57
epoch train time: 0:00:00.548204
elapsed time: 0:00:40.056737
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:26:40.080485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.95
 ---- batch: 020 ----
mean loss: 855.03
 ---- batch: 030 ----
mean loss: 871.69
 ---- batch: 040 ----
mean loss: 869.14
 ---- batch: 050 ----
mean loss: 856.51
 ---- batch: 060 ----
mean loss: 847.91
 ---- batch: 070 ----
mean loss: 848.32
 ---- batch: 080 ----
mean loss: 843.89
 ---- batch: 090 ----
mean loss: 852.89
 ---- batch: 100 ----
mean loss: 846.06
 ---- batch: 110 ----
mean loss: 866.92
train mean loss: 855.47
epoch train time: 0:00:00.550238
elapsed time: 0:00:40.607119
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:26:40.630860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.77
 ---- batch: 020 ----
mean loss: 857.37
 ---- batch: 030 ----
mean loss: 847.91
 ---- batch: 040 ----
mean loss: 836.00
 ---- batch: 050 ----
mean loss: 846.73
 ---- batch: 060 ----
mean loss: 853.86
 ---- batch: 070 ----
mean loss: 863.51
 ---- batch: 080 ----
mean loss: 838.48
 ---- batch: 090 ----
mean loss: 842.82
 ---- batch: 100 ----
mean loss: 857.64
 ---- batch: 110 ----
mean loss: 837.02
train mean loss: 849.95
epoch train time: 0:00:00.548711
elapsed time: 0:00:41.155951
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:26:41.179710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.38
 ---- batch: 020 ----
mean loss: 819.85
 ---- batch: 030 ----
mean loss: 845.83
 ---- batch: 040 ----
mean loss: 865.46
 ---- batch: 050 ----
mean loss: 866.31
 ---- batch: 060 ----
mean loss: 861.45
 ---- batch: 070 ----
mean loss: 856.41
 ---- batch: 080 ----
mean loss: 847.29
 ---- batch: 090 ----
mean loss: 833.10
 ---- batch: 100 ----
mean loss: 838.74
 ---- batch: 110 ----
mean loss: 839.09
train mean loss: 846.95
epoch train time: 0:00:00.546175
elapsed time: 0:00:41.702314
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:26:41.726076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 825.28
 ---- batch: 020 ----
mean loss: 849.56
 ---- batch: 030 ----
mean loss: 833.28
 ---- batch: 040 ----
mean loss: 846.42
 ---- batch: 050 ----
mean loss: 860.95
 ---- batch: 060 ----
mean loss: 825.02
 ---- batch: 070 ----
mean loss: 859.35
 ---- batch: 080 ----
mean loss: 841.51
 ---- batch: 090 ----
mean loss: 841.94
 ---- batch: 100 ----
mean loss: 860.60
 ---- batch: 110 ----
mean loss: 856.54
train mean loss: 845.25
epoch train time: 0:00:00.551060
elapsed time: 0:00:42.253537
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:26:42.277281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.72
 ---- batch: 020 ----
mean loss: 855.95
 ---- batch: 030 ----
mean loss: 843.26
 ---- batch: 040 ----
mean loss: 822.66
 ---- batch: 050 ----
mean loss: 835.50
 ---- batch: 060 ----
mean loss: 852.06
 ---- batch: 070 ----
mean loss: 841.48
 ---- batch: 080 ----
mean loss: 841.91
 ---- batch: 090 ----
mean loss: 852.54
 ---- batch: 100 ----
mean loss: 840.29
 ---- batch: 110 ----
mean loss: 863.04
train mean loss: 844.42
epoch train time: 0:00:00.550020
elapsed time: 0:00:42.803711
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:26:42.827487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.96
 ---- batch: 020 ----
mean loss: 852.82
 ---- batch: 030 ----
mean loss: 854.72
 ---- batch: 040 ----
mean loss: 838.65
 ---- batch: 050 ----
mean loss: 828.36
 ---- batch: 060 ----
mean loss: 851.62
 ---- batch: 070 ----
mean loss: 842.99
 ---- batch: 080 ----
mean loss: 848.95
 ---- batch: 090 ----
mean loss: 837.98
 ---- batch: 100 ----
mean loss: 846.05
 ---- batch: 110 ----
mean loss: 851.20
train mean loss: 844.14
epoch train time: 0:00:00.543371
elapsed time: 0:00:43.347248
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:26:43.370992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.78
 ---- batch: 020 ----
mean loss: 880.96
 ---- batch: 030 ----
mean loss: 843.26
 ---- batch: 040 ----
mean loss: 869.80
 ---- batch: 050 ----
mean loss: 847.83
 ---- batch: 060 ----
mean loss: 859.64
 ---- batch: 070 ----
mean loss: 840.66
 ---- batch: 080 ----
mean loss: 857.17
 ---- batch: 090 ----
mean loss: 830.06
 ---- batch: 100 ----
mean loss: 826.07
 ---- batch: 110 ----
mean loss: 827.67
train mean loss: 844.05
epoch train time: 0:00:00.554035
elapsed time: 0:00:43.901427
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:26:43.925172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.64
 ---- batch: 020 ----
mean loss: 856.32
 ---- batch: 030 ----
mean loss: 828.47
 ---- batch: 040 ----
mean loss: 859.03
 ---- batch: 050 ----
mean loss: 850.49
 ---- batch: 060 ----
mean loss: 841.68
 ---- batch: 070 ----
mean loss: 853.76
 ---- batch: 080 ----
mean loss: 837.55
 ---- batch: 090 ----
mean loss: 835.49
 ---- batch: 100 ----
mean loss: 831.82
 ---- batch: 110 ----
mean loss: 853.07
train mean loss: 844.00
epoch train time: 0:00:00.554046
elapsed time: 0:00:44.455613
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:26:44.479365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.45
 ---- batch: 020 ----
mean loss: 834.56
 ---- batch: 030 ----
mean loss: 818.94
 ---- batch: 040 ----
mean loss: 844.21
 ---- batch: 050 ----
mean loss: 871.48
 ---- batch: 060 ----
mean loss: 835.60
 ---- batch: 070 ----
mean loss: 864.78
 ---- batch: 080 ----
mean loss: 832.26
 ---- batch: 090 ----
mean loss: 846.15
 ---- batch: 100 ----
mean loss: 862.06
 ---- batch: 110 ----
mean loss: 844.17
train mean loss: 844.02
epoch train time: 0:00:00.573095
elapsed time: 0:00:45.028845
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:26:45.052609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.38
 ---- batch: 020 ----
mean loss: 841.60
 ---- batch: 030 ----
mean loss: 856.91
 ---- batch: 040 ----
mean loss: 844.27
 ---- batch: 050 ----
mean loss: 860.70
 ---- batch: 060 ----
mean loss: 834.28
 ---- batch: 070 ----
mean loss: 833.49
 ---- batch: 080 ----
mean loss: 858.25
 ---- batch: 090 ----
mean loss: 841.82
 ---- batch: 100 ----
mean loss: 851.80
 ---- batch: 110 ----
mean loss: 829.22
train mean loss: 843.99
epoch train time: 0:00:00.561630
elapsed time: 0:00:45.590658
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:26:45.614411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.90
 ---- batch: 020 ----
mean loss: 842.31
 ---- batch: 030 ----
mean loss: 827.30
 ---- batch: 040 ----
mean loss: 849.85
 ---- batch: 050 ----
mean loss: 846.90
 ---- batch: 060 ----
mean loss: 858.69
 ---- batch: 070 ----
mean loss: 818.88
 ---- batch: 080 ----
mean loss: 846.67
 ---- batch: 090 ----
mean loss: 854.56
 ---- batch: 100 ----
mean loss: 835.48
 ---- batch: 110 ----
mean loss: 854.71
train mean loss: 844.00
epoch train time: 0:00:00.551792
elapsed time: 0:00:46.142589
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:26:46.166334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.76
 ---- batch: 020 ----
mean loss: 845.49
 ---- batch: 030 ----
mean loss: 843.53
 ---- batch: 040 ----
mean loss: 840.50
 ---- batch: 050 ----
mean loss: 834.65
 ---- batch: 060 ----
mean loss: 852.35
 ---- batch: 070 ----
mean loss: 864.89
 ---- batch: 080 ----
mean loss: 830.97
 ---- batch: 090 ----
mean loss: 848.37
 ---- batch: 100 ----
mean loss: 838.74
 ---- batch: 110 ----
mean loss: 838.40
train mean loss: 843.96
epoch train time: 0:00:00.544882
elapsed time: 0:00:46.687600
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:26:46.711344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.07
 ---- batch: 020 ----
mean loss: 852.37
 ---- batch: 030 ----
mean loss: 858.97
 ---- batch: 040 ----
mean loss: 845.35
 ---- batch: 050 ----
mean loss: 839.65
 ---- batch: 060 ----
mean loss: 819.39
 ---- batch: 070 ----
mean loss: 828.47
 ---- batch: 080 ----
mean loss: 860.89
 ---- batch: 090 ----
mean loss: 863.14
 ---- batch: 100 ----
mean loss: 836.78
 ---- batch: 110 ----
mean loss: 837.83
train mean loss: 843.92
epoch train time: 0:00:00.551608
elapsed time: 0:00:47.239340
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:26:47.263085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.02
 ---- batch: 020 ----
mean loss: 834.65
 ---- batch: 030 ----
mean loss: 854.13
 ---- batch: 040 ----
mean loss: 865.57
 ---- batch: 050 ----
mean loss: 838.80
 ---- batch: 060 ----
mean loss: 839.52
 ---- batch: 070 ----
mean loss: 829.95
 ---- batch: 080 ----
mean loss: 849.25
 ---- batch: 090 ----
mean loss: 852.49
 ---- batch: 100 ----
mean loss: 835.78
 ---- batch: 110 ----
mean loss: 847.17
train mean loss: 843.85
epoch train time: 0:00:00.557833
elapsed time: 0:00:47.797305
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:26:47.821051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.60
 ---- batch: 020 ----
mean loss: 839.61
 ---- batch: 030 ----
mean loss: 857.25
 ---- batch: 040 ----
mean loss: 861.74
 ---- batch: 050 ----
mean loss: 858.49
 ---- batch: 060 ----
mean loss: 838.93
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 844.46
 ---- batch: 090 ----
mean loss: 822.49
 ---- batch: 100 ----
mean loss: 855.87
 ---- batch: 110 ----
mean loss: 837.78
train mean loss: 843.94
epoch train time: 0:00:00.560522
elapsed time: 0:00:48.357982
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:26:48.381731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 823.18
 ---- batch: 030 ----
mean loss: 834.62
 ---- batch: 040 ----
mean loss: 843.74
 ---- batch: 050 ----
mean loss: 847.57
 ---- batch: 060 ----
mean loss: 842.47
 ---- batch: 070 ----
mean loss: 847.41
 ---- batch: 080 ----
mean loss: 859.33
 ---- batch: 090 ----
mean loss: 843.56
 ---- batch: 100 ----
mean loss: 848.51
 ---- batch: 110 ----
mean loss: 841.92
train mean loss: 843.91
epoch train time: 0:00:00.558264
elapsed time: 0:00:48.916386
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:26:48.940148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.91
 ---- batch: 020 ----
mean loss: 839.22
 ---- batch: 030 ----
mean loss: 847.48
 ---- batch: 040 ----
mean loss: 860.20
 ---- batch: 050 ----
mean loss: 844.45
 ---- batch: 060 ----
mean loss: 840.44
 ---- batch: 070 ----
mean loss: 807.10
 ---- batch: 080 ----
mean loss: 851.62
 ---- batch: 090 ----
mean loss: 844.55
 ---- batch: 100 ----
mean loss: 855.06
 ---- batch: 110 ----
mean loss: 856.51
train mean loss: 844.02
epoch train time: 0:00:00.571764
elapsed time: 0:00:49.488304
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:26:49.512072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.92
 ---- batch: 020 ----
mean loss: 833.89
 ---- batch: 030 ----
mean loss: 864.36
 ---- batch: 040 ----
mean loss: 867.12
 ---- batch: 050 ----
mean loss: 818.04
 ---- batch: 060 ----
mean loss: 833.13
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 846.87
 ---- batch: 090 ----
mean loss: 842.99
 ---- batch: 100 ----
mean loss: 845.87
 ---- batch: 110 ----
mean loss: 831.11
train mean loss: 844.04
epoch train time: 0:00:00.557238
elapsed time: 0:00:50.045693
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:26:50.069444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.77
 ---- batch: 020 ----
mean loss: 832.53
 ---- batch: 030 ----
mean loss: 855.60
 ---- batch: 040 ----
mean loss: 864.32
 ---- batch: 050 ----
mean loss: 850.39
 ---- batch: 060 ----
mean loss: 854.95
 ---- batch: 070 ----
mean loss: 825.56
 ---- batch: 080 ----
mean loss: 851.27
 ---- batch: 090 ----
mean loss: 841.01
 ---- batch: 100 ----
mean loss: 851.35
 ---- batch: 110 ----
mean loss: 831.01
train mean loss: 843.87
epoch train time: 0:00:00.571446
elapsed time: 0:00:50.617288
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:26:50.641032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.60
 ---- batch: 020 ----
mean loss: 848.83
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 843.44
 ---- batch: 050 ----
mean loss: 824.87
 ---- batch: 060 ----
mean loss: 836.67
 ---- batch: 070 ----
mean loss: 843.43
 ---- batch: 080 ----
mean loss: 858.18
 ---- batch: 090 ----
mean loss: 855.87
 ---- batch: 100 ----
mean loss: 850.60
 ---- batch: 110 ----
mean loss: 855.83
train mean loss: 843.97
epoch train time: 0:00:00.557249
elapsed time: 0:00:51.174674
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:26:51.198440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.51
 ---- batch: 020 ----
mean loss: 814.72
 ---- batch: 030 ----
mean loss: 838.89
 ---- batch: 040 ----
mean loss: 851.10
 ---- batch: 050 ----
mean loss: 848.75
 ---- batch: 060 ----
mean loss: 852.61
 ---- batch: 070 ----
mean loss: 834.73
 ---- batch: 080 ----
mean loss: 850.42
 ---- batch: 090 ----
mean loss: 854.91
 ---- batch: 100 ----
mean loss: 853.50
 ---- batch: 110 ----
mean loss: 840.83
train mean loss: 844.05
epoch train time: 0:00:00.557272
elapsed time: 0:00:51.732099
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:26:51.755847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.12
 ---- batch: 020 ----
mean loss: 861.63
 ---- batch: 030 ----
mean loss: 838.12
 ---- batch: 040 ----
mean loss: 845.02
 ---- batch: 050 ----
mean loss: 836.21
 ---- batch: 060 ----
mean loss: 835.64
 ---- batch: 070 ----
mean loss: 842.72
 ---- batch: 080 ----
mean loss: 851.19
 ---- batch: 090 ----
mean loss: 833.50
 ---- batch: 100 ----
mean loss: 834.01
 ---- batch: 110 ----
mean loss: 834.96
train mean loss: 844.04
epoch train time: 0:00:00.555908
elapsed time: 0:00:52.288160
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:26:52.311935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.26
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 855.51
 ---- batch: 040 ----
mean loss: 842.87
 ---- batch: 050 ----
mean loss: 826.74
 ---- batch: 060 ----
mean loss: 850.21
 ---- batch: 070 ----
mean loss: 835.43
 ---- batch: 080 ----
mean loss: 827.13
 ---- batch: 090 ----
mean loss: 864.26
 ---- batch: 100 ----
mean loss: 852.66
 ---- batch: 110 ----
mean loss: 827.14
train mean loss: 843.95
epoch train time: 0:00:00.557741
elapsed time: 0:00:52.846063
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:26:52.869808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.02
 ---- batch: 020 ----
mean loss: 855.72
 ---- batch: 030 ----
mean loss: 830.99
 ---- batch: 040 ----
mean loss: 838.39
 ---- batch: 050 ----
mean loss: 868.38
 ---- batch: 060 ----
mean loss: 833.25
 ---- batch: 070 ----
mean loss: 839.23
 ---- batch: 080 ----
mean loss: 829.76
 ---- batch: 090 ----
mean loss: 826.33
 ---- batch: 100 ----
mean loss: 855.08
 ---- batch: 110 ----
mean loss: 855.25
train mean loss: 843.94
epoch train time: 0:00:00.556577
elapsed time: 0:00:53.402796
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:26:53.426543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.59
 ---- batch: 020 ----
mean loss: 836.39
 ---- batch: 030 ----
mean loss: 854.91
 ---- batch: 040 ----
mean loss: 859.66
 ---- batch: 050 ----
mean loss: 817.84
 ---- batch: 060 ----
mean loss: 832.36
 ---- batch: 070 ----
mean loss: 858.03
 ---- batch: 080 ----
mean loss: 854.70
 ---- batch: 090 ----
mean loss: 824.22
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 844.56
train mean loss: 844.09
epoch train time: 0:00:00.551187
elapsed time: 0:00:53.954151
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:26:53.977893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.50
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 842.46
 ---- batch: 040 ----
mean loss: 845.67
 ---- batch: 050 ----
mean loss: 833.50
 ---- batch: 060 ----
mean loss: 835.28
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 852.60
 ---- batch: 090 ----
mean loss: 845.35
 ---- batch: 100 ----
mean loss: 850.67
 ---- batch: 110 ----
mean loss: 849.95
train mean loss: 844.01
epoch train time: 0:00:00.551765
elapsed time: 0:00:54.506067
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:26:54.547480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.99
 ---- batch: 020 ----
mean loss: 855.23
 ---- batch: 030 ----
mean loss: 833.33
 ---- batch: 040 ----
mean loss: 858.46
 ---- batch: 050 ----
mean loss: 855.47
 ---- batch: 060 ----
mean loss: 845.19
 ---- batch: 070 ----
mean loss: 832.66
 ---- batch: 080 ----
mean loss: 846.02
 ---- batch: 090 ----
mean loss: 835.69
 ---- batch: 100 ----
mean loss: 840.27
 ---- batch: 110 ----
mean loss: 845.14
train mean loss: 843.93
epoch train time: 0:00:00.559823
elapsed time: 0:00:55.083725
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:26:55.107471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.50
 ---- batch: 020 ----
mean loss: 825.58
 ---- batch: 030 ----
mean loss: 839.15
 ---- batch: 040 ----
mean loss: 848.79
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 850.23
 ---- batch: 070 ----
mean loss: 845.20
 ---- batch: 080 ----
mean loss: 866.49
 ---- batch: 090 ----
mean loss: 853.99
 ---- batch: 100 ----
mean loss: 844.03
 ---- batch: 110 ----
mean loss: 822.73
train mean loss: 843.97
epoch train time: 0:00:00.552037
elapsed time: 0:00:55.635893
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:26:55.659639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.68
 ---- batch: 020 ----
mean loss: 835.69
 ---- batch: 030 ----
mean loss: 846.89
 ---- batch: 040 ----
mean loss: 835.59
 ---- batch: 050 ----
mean loss: 868.30
 ---- batch: 060 ----
mean loss: 830.60
 ---- batch: 070 ----
mean loss: 843.45
 ---- batch: 080 ----
mean loss: 856.95
 ---- batch: 090 ----
mean loss: 841.46
 ---- batch: 100 ----
mean loss: 842.00
 ---- batch: 110 ----
mean loss: 840.71
train mean loss: 844.08
epoch train time: 0:00:00.559553
elapsed time: 0:00:56.195578
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:26:56.219341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.32
 ---- batch: 020 ----
mean loss: 848.50
 ---- batch: 030 ----
mean loss: 852.80
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 845.95
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 809.16
 ---- batch: 080 ----
mean loss: 856.45
 ---- batch: 090 ----
mean loss: 845.24
 ---- batch: 100 ----
mean loss: 849.24
 ---- batch: 110 ----
mean loss: 851.07
train mean loss: 844.06
epoch train time: 0:00:00.546356
elapsed time: 0:00:56.742099
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:26:56.765858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.24
 ---- batch: 020 ----
mean loss: 835.81
 ---- batch: 030 ----
mean loss: 850.60
 ---- batch: 040 ----
mean loss: 858.11
 ---- batch: 050 ----
mean loss: 833.74
 ---- batch: 060 ----
mean loss: 865.07
 ---- batch: 070 ----
mean loss: 852.63
 ---- batch: 080 ----
mean loss: 833.13
 ---- batch: 090 ----
mean loss: 822.31
 ---- batch: 100 ----
mean loss: 830.73
 ---- batch: 110 ----
mean loss: 843.90
train mean loss: 844.04
epoch train time: 0:00:00.557827
elapsed time: 0:00:57.300070
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:26:57.323815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.98
 ---- batch: 020 ----
mean loss: 869.92
 ---- batch: 030 ----
mean loss: 857.71
 ---- batch: 040 ----
mean loss: 850.94
 ---- batch: 050 ----
mean loss: 831.70
 ---- batch: 060 ----
mean loss: 842.32
 ---- batch: 070 ----
mean loss: 837.32
 ---- batch: 080 ----
mean loss: 844.26
 ---- batch: 090 ----
mean loss: 840.97
 ---- batch: 100 ----
mean loss: 843.34
 ---- batch: 110 ----
mean loss: 853.45
train mean loss: 843.94
epoch train time: 0:00:00.555131
elapsed time: 0:00:57.855351
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:26:57.879100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.24
 ---- batch: 020 ----
mean loss: 826.17
 ---- batch: 030 ----
mean loss: 857.23
 ---- batch: 040 ----
mean loss: 855.75
 ---- batch: 050 ----
mean loss: 829.66
 ---- batch: 060 ----
mean loss: 825.05
 ---- batch: 070 ----
mean loss: 862.09
 ---- batch: 080 ----
mean loss: 821.70
 ---- batch: 090 ----
mean loss: 868.28
 ---- batch: 100 ----
mean loss: 840.49
 ---- batch: 110 ----
mean loss: 854.33
train mean loss: 844.02
epoch train time: 0:00:00.557882
elapsed time: 0:00:58.413366
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:26:58.437109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.59
 ---- batch: 020 ----
mean loss: 839.91
 ---- batch: 030 ----
mean loss: 832.06
 ---- batch: 040 ----
mean loss: 833.44
 ---- batch: 050 ----
mean loss: 843.63
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 865.28
 ---- batch: 080 ----
mean loss: 850.58
 ---- batch: 090 ----
mean loss: 864.03
 ---- batch: 100 ----
mean loss: 829.88
 ---- batch: 110 ----
mean loss: 840.02
train mean loss: 844.02
epoch train time: 0:00:00.563651
elapsed time: 0:00:58.977147
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:26:59.000946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.87
 ---- batch: 020 ----
mean loss: 843.80
 ---- batch: 030 ----
mean loss: 851.05
 ---- batch: 040 ----
mean loss: 856.67
 ---- batch: 050 ----
mean loss: 832.84
 ---- batch: 060 ----
mean loss: 851.55
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 833.22
 ---- batch: 090 ----
mean loss: 833.16
 ---- batch: 100 ----
mean loss: 851.11
 ---- batch: 110 ----
mean loss: 824.81
train mean loss: 844.03
epoch train time: 0:00:00.557821
elapsed time: 0:00:59.535160
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:26:59.558928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.72
 ---- batch: 020 ----
mean loss: 849.35
 ---- batch: 030 ----
mean loss: 854.59
 ---- batch: 040 ----
mean loss: 833.74
 ---- batch: 050 ----
mean loss: 834.98
 ---- batch: 060 ----
mean loss: 847.40
 ---- batch: 070 ----
mean loss: 828.85
 ---- batch: 080 ----
mean loss: 845.63
 ---- batch: 090 ----
mean loss: 847.92
 ---- batch: 100 ----
mean loss: 839.45
 ---- batch: 110 ----
mean loss: 846.27
train mean loss: 844.00
epoch train time: 0:00:00.560472
elapsed time: 0:01:00.095785
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:27:00.119561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.21
 ---- batch: 020 ----
mean loss: 846.51
 ---- batch: 030 ----
mean loss: 862.86
 ---- batch: 040 ----
mean loss: 852.97
 ---- batch: 050 ----
mean loss: 837.18
 ---- batch: 060 ----
mean loss: 840.37
 ---- batch: 070 ----
mean loss: 860.78
 ---- batch: 080 ----
mean loss: 855.69
 ---- batch: 090 ----
mean loss: 833.01
 ---- batch: 100 ----
mean loss: 817.31
 ---- batch: 110 ----
mean loss: 839.15
train mean loss: 843.95
epoch train time: 0:00:00.566099
elapsed time: 0:01:00.662063
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:27:00.685808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.07
 ---- batch: 020 ----
mean loss: 842.82
 ---- batch: 030 ----
mean loss: 842.87
 ---- batch: 040 ----
mean loss: 838.15
 ---- batch: 050 ----
mean loss: 844.73
 ---- batch: 060 ----
mean loss: 863.77
 ---- batch: 070 ----
mean loss: 842.70
 ---- batch: 080 ----
mean loss: 808.25
 ---- batch: 090 ----
mean loss: 833.56
 ---- batch: 100 ----
mean loss: 859.02
 ---- batch: 110 ----
mean loss: 849.06
train mean loss: 844.05
epoch train time: 0:00:00.567647
elapsed time: 0:01:01.229843
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:27:01.253627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.94
 ---- batch: 020 ----
mean loss: 835.17
 ---- batch: 030 ----
mean loss: 837.94
 ---- batch: 040 ----
mean loss: 824.17
 ---- batch: 050 ----
mean loss: 839.35
 ---- batch: 060 ----
mean loss: 857.76
 ---- batch: 070 ----
mean loss: 822.81
 ---- batch: 080 ----
mean loss: 857.45
 ---- batch: 090 ----
mean loss: 835.61
 ---- batch: 100 ----
mean loss: 851.21
 ---- batch: 110 ----
mean loss: 863.64
train mean loss: 844.00
epoch train time: 0:00:00.560458
elapsed time: 0:01:01.790470
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:27:01.814216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.58
 ---- batch: 020 ----
mean loss: 858.18
 ---- batch: 030 ----
mean loss: 852.87
 ---- batch: 040 ----
mean loss: 839.77
 ---- batch: 050 ----
mean loss: 830.20
 ---- batch: 060 ----
mean loss: 841.44
 ---- batch: 070 ----
mean loss: 855.69
 ---- batch: 080 ----
mean loss: 849.65
 ---- batch: 090 ----
mean loss: 832.65
 ---- batch: 100 ----
mean loss: 836.73
 ---- batch: 110 ----
mean loss: 842.08
train mean loss: 843.93
epoch train time: 0:00:00.543361
elapsed time: 0:01:02.333979
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:27:02.357724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.39
 ---- batch: 020 ----
mean loss: 853.11
 ---- batch: 030 ----
mean loss: 847.67
 ---- batch: 040 ----
mean loss: 843.46
 ---- batch: 050 ----
mean loss: 859.02
 ---- batch: 060 ----
mean loss: 816.28
 ---- batch: 070 ----
mean loss: 860.17
 ---- batch: 080 ----
mean loss: 813.15
 ---- batch: 090 ----
mean loss: 834.15
 ---- batch: 100 ----
mean loss: 842.65
 ---- batch: 110 ----
mean loss: 856.96
train mean loss: 843.85
epoch train time: 0:00:00.543019
elapsed time: 0:01:02.877126
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:27:02.900869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.44
 ---- batch: 020 ----
mean loss: 830.55
 ---- batch: 030 ----
mean loss: 816.17
 ---- batch: 040 ----
mean loss: 798.61
 ---- batch: 050 ----
mean loss: 763.17
 ---- batch: 060 ----
mean loss: 723.72
 ---- batch: 070 ----
mean loss: 697.13
 ---- batch: 080 ----
mean loss: 646.58
 ---- batch: 090 ----
mean loss: 612.22
 ---- batch: 100 ----
mean loss: 543.98
 ---- batch: 110 ----
mean loss: 492.71
train mean loss: 700.55
epoch train time: 0:00:00.556452
elapsed time: 0:01:03.433715
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:27:03.457475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.63
 ---- batch: 020 ----
mean loss: 417.04
 ---- batch: 030 ----
mean loss: 414.67
 ---- batch: 040 ----
mean loss: 386.65
 ---- batch: 050 ----
mean loss: 386.14
 ---- batch: 060 ----
mean loss: 369.24
 ---- batch: 070 ----
mean loss: 350.00
 ---- batch: 080 ----
mean loss: 334.23
 ---- batch: 090 ----
mean loss: 339.39
 ---- batch: 100 ----
mean loss: 323.28
 ---- batch: 110 ----
mean loss: 324.30
train mean loss: 369.83
epoch train time: 0:00:00.564706
elapsed time: 0:01:03.998624
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:27:04.022411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.69
 ---- batch: 020 ----
mean loss: 300.95
 ---- batch: 030 ----
mean loss: 294.28
 ---- batch: 040 ----
mean loss: 287.39
 ---- batch: 050 ----
mean loss: 291.73
 ---- batch: 060 ----
mean loss: 284.59
 ---- batch: 070 ----
mean loss: 286.09
 ---- batch: 080 ----
mean loss: 278.30
 ---- batch: 090 ----
mean loss: 277.58
 ---- batch: 100 ----
mean loss: 270.11
 ---- batch: 110 ----
mean loss: 280.14
train mean loss: 287.74
epoch train time: 0:00:00.559981
elapsed time: 0:01:04.558782
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:27:04.582539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.40
 ---- batch: 020 ----
mean loss: 269.33
 ---- batch: 030 ----
mean loss: 266.12
 ---- batch: 040 ----
mean loss: 256.75
 ---- batch: 050 ----
mean loss: 250.86
 ---- batch: 060 ----
mean loss: 246.15
 ---- batch: 070 ----
mean loss: 249.58
 ---- batch: 080 ----
mean loss: 260.09
 ---- batch: 090 ----
mean loss: 248.60
 ---- batch: 100 ----
mean loss: 260.85
 ---- batch: 110 ----
mean loss: 247.36
train mean loss: 255.39
epoch train time: 0:00:00.568027
elapsed time: 0:01:05.126961
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:27:05.150713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.87
 ---- batch: 020 ----
mean loss: 237.32
 ---- batch: 030 ----
mean loss: 244.25
 ---- batch: 040 ----
mean loss: 242.76
 ---- batch: 050 ----
mean loss: 233.13
 ---- batch: 060 ----
mean loss: 232.54
 ---- batch: 070 ----
mean loss: 229.10
 ---- batch: 080 ----
mean loss: 239.56
 ---- batch: 090 ----
mean loss: 248.44
 ---- batch: 100 ----
mean loss: 243.04
 ---- batch: 110 ----
mean loss: 243.70
train mean loss: 239.47
epoch train time: 0:00:00.558208
elapsed time: 0:01:05.685304
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:27:05.709047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.18
 ---- batch: 020 ----
mean loss: 227.05
 ---- batch: 030 ----
mean loss: 239.19
 ---- batch: 040 ----
mean loss: 233.08
 ---- batch: 050 ----
mean loss: 232.66
 ---- batch: 060 ----
mean loss: 231.67
 ---- batch: 070 ----
mean loss: 224.36
 ---- batch: 080 ----
mean loss: 232.85
 ---- batch: 090 ----
mean loss: 231.97
 ---- batch: 100 ----
mean loss: 224.98
 ---- batch: 110 ----
mean loss: 223.65
train mean loss: 229.88
epoch train time: 0:00:00.559990
elapsed time: 0:01:06.245421
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:27:06.269166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.81
 ---- batch: 020 ----
mean loss: 231.82
 ---- batch: 030 ----
mean loss: 224.56
 ---- batch: 040 ----
mean loss: 211.43
 ---- batch: 050 ----
mean loss: 225.27
 ---- batch: 060 ----
mean loss: 218.85
 ---- batch: 070 ----
mean loss: 223.49
 ---- batch: 080 ----
mean loss: 216.58
 ---- batch: 090 ----
mean loss: 225.30
 ---- batch: 100 ----
mean loss: 210.03
 ---- batch: 110 ----
mean loss: 225.93
train mean loss: 220.53
epoch train time: 0:00:00.555697
elapsed time: 0:01:06.801256
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:27:06.825023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.45
 ---- batch: 020 ----
mean loss: 206.80
 ---- batch: 030 ----
mean loss: 209.93
 ---- batch: 040 ----
mean loss: 220.58
 ---- batch: 050 ----
mean loss: 202.86
 ---- batch: 060 ----
mean loss: 212.60
 ---- batch: 070 ----
mean loss: 200.24
 ---- batch: 080 ----
mean loss: 222.43
 ---- batch: 090 ----
mean loss: 212.87
 ---- batch: 100 ----
mean loss: 213.34
 ---- batch: 110 ----
mean loss: 211.85
train mean loss: 212.56
epoch train time: 0:00:00.557878
elapsed time: 0:01:07.359288
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:27:07.383031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.21
 ---- batch: 020 ----
mean loss: 202.44
 ---- batch: 030 ----
mean loss: 207.29
 ---- batch: 040 ----
mean loss: 210.76
 ---- batch: 050 ----
mean loss: 201.65
 ---- batch: 060 ----
mean loss: 204.47
 ---- batch: 070 ----
mean loss: 210.78
 ---- batch: 080 ----
mean loss: 205.65
 ---- batch: 090 ----
mean loss: 199.31
 ---- batch: 100 ----
mean loss: 205.53
 ---- batch: 110 ----
mean loss: 217.75
train mean loss: 206.49
epoch train time: 0:00:00.551987
elapsed time: 0:01:07.911403
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:27:07.935161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.80
 ---- batch: 020 ----
mean loss: 203.23
 ---- batch: 030 ----
mean loss: 196.00
 ---- batch: 040 ----
mean loss: 187.54
 ---- batch: 050 ----
mean loss: 203.25
 ---- batch: 060 ----
mean loss: 207.54
 ---- batch: 070 ----
mean loss: 205.46
 ---- batch: 080 ----
mean loss: 207.75
 ---- batch: 090 ----
mean loss: 212.96
 ---- batch: 100 ----
mean loss: 200.77
 ---- batch: 110 ----
mean loss: 194.41
train mean loss: 201.99
epoch train time: 0:00:00.551105
elapsed time: 0:01:08.462664
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:27:08.486429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.44
 ---- batch: 020 ----
mean loss: 189.34
 ---- batch: 030 ----
mean loss: 204.46
 ---- batch: 040 ----
mean loss: 201.47
 ---- batch: 050 ----
mean loss: 205.04
 ---- batch: 060 ----
mean loss: 200.41
 ---- batch: 070 ----
mean loss: 205.70
 ---- batch: 080 ----
mean loss: 199.53
 ---- batch: 090 ----
mean loss: 192.99
 ---- batch: 100 ----
mean loss: 194.71
 ---- batch: 110 ----
mean loss: 201.48
train mean loss: 199.60
epoch train time: 0:00:00.556510
elapsed time: 0:01:09.019340
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:27:09.043097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.96
 ---- batch: 020 ----
mean loss: 192.05
 ---- batch: 030 ----
mean loss: 192.19
 ---- batch: 040 ----
mean loss: 189.92
 ---- batch: 050 ----
mean loss: 185.98
 ---- batch: 060 ----
mean loss: 195.80
 ---- batch: 070 ----
mean loss: 195.26
 ---- batch: 080 ----
mean loss: 202.00
 ---- batch: 090 ----
mean loss: 193.39
 ---- batch: 100 ----
mean loss: 194.83
 ---- batch: 110 ----
mean loss: 194.60
train mean loss: 193.56
epoch train time: 0:00:00.561600
elapsed time: 0:01:09.581088
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:27:09.604852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.95
 ---- batch: 020 ----
mean loss: 187.44
 ---- batch: 030 ----
mean loss: 179.47
 ---- batch: 040 ----
mean loss: 189.90
 ---- batch: 050 ----
mean loss: 191.83
 ---- batch: 060 ----
mean loss: 185.80
 ---- batch: 070 ----
mean loss: 193.81
 ---- batch: 080 ----
mean loss: 192.92
 ---- batch: 090 ----
mean loss: 200.28
 ---- batch: 100 ----
mean loss: 196.36
 ---- batch: 110 ----
mean loss: 198.03
train mean loss: 191.63
epoch train time: 0:00:00.557554
elapsed time: 0:01:10.138809
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:27:10.162557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.09
 ---- batch: 020 ----
mean loss: 189.27
 ---- batch: 030 ----
mean loss: 177.62
 ---- batch: 040 ----
mean loss: 188.60
 ---- batch: 050 ----
mean loss: 191.27
 ---- batch: 060 ----
mean loss: 196.38
 ---- batch: 070 ----
mean loss: 191.23
 ---- batch: 080 ----
mean loss: 186.77
 ---- batch: 090 ----
mean loss: 198.84
 ---- batch: 100 ----
mean loss: 178.29
 ---- batch: 110 ----
mean loss: 194.98
train mean loss: 189.16
epoch train time: 0:00:00.562539
elapsed time: 0:01:10.701499
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:27:10.725255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.74
 ---- batch: 020 ----
mean loss: 190.64
 ---- batch: 030 ----
mean loss: 190.11
 ---- batch: 040 ----
mean loss: 177.99
 ---- batch: 050 ----
mean loss: 191.02
 ---- batch: 060 ----
mean loss: 191.99
 ---- batch: 070 ----
mean loss: 180.42
 ---- batch: 080 ----
mean loss: 181.12
 ---- batch: 090 ----
mean loss: 183.40
 ---- batch: 100 ----
mean loss: 181.47
 ---- batch: 110 ----
mean loss: 192.70
train mean loss: 186.66
epoch train time: 0:00:00.551571
elapsed time: 0:01:11.253236
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:27:11.276976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.08
 ---- batch: 020 ----
mean loss: 187.63
 ---- batch: 030 ----
mean loss: 194.76
 ---- batch: 040 ----
mean loss: 185.77
 ---- batch: 050 ----
mean loss: 185.27
 ---- batch: 060 ----
mean loss: 181.33
 ---- batch: 070 ----
mean loss: 188.30
 ---- batch: 080 ----
mean loss: 183.99
 ---- batch: 090 ----
mean loss: 189.40
 ---- batch: 100 ----
mean loss: 184.93
 ---- batch: 110 ----
mean loss: 186.57
train mean loss: 185.82
epoch train time: 0:00:00.561045
elapsed time: 0:01:11.814413
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:27:11.838159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.78
 ---- batch: 020 ----
mean loss: 176.22
 ---- batch: 030 ----
mean loss: 186.60
 ---- batch: 040 ----
mean loss: 182.90
 ---- batch: 050 ----
mean loss: 172.01
 ---- batch: 060 ----
mean loss: 188.26
 ---- batch: 070 ----
mean loss: 187.31
 ---- batch: 080 ----
mean loss: 193.98
 ---- batch: 090 ----
mean loss: 182.68
 ---- batch: 100 ----
mean loss: 181.20
 ---- batch: 110 ----
mean loss: 184.89
train mean loss: 183.26
epoch train time: 0:00:00.557863
elapsed time: 0:01:12.372406
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:27:12.396152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.17
 ---- batch: 020 ----
mean loss: 181.18
 ---- batch: 030 ----
mean loss: 182.62
 ---- batch: 040 ----
mean loss: 183.18
 ---- batch: 050 ----
mean loss: 186.75
 ---- batch: 060 ----
mean loss: 178.14
 ---- batch: 070 ----
mean loss: 182.19
 ---- batch: 080 ----
mean loss: 178.82
 ---- batch: 090 ----
mean loss: 179.88
 ---- batch: 100 ----
mean loss: 185.51
 ---- batch: 110 ----
mean loss: 178.44
train mean loss: 181.38
epoch train time: 0:00:00.562009
elapsed time: 0:01:12.934556
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:27:12.958304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.40
 ---- batch: 020 ----
mean loss: 182.58
 ---- batch: 030 ----
mean loss: 179.55
 ---- batch: 040 ----
mean loss: 174.04
 ---- batch: 050 ----
mean loss: 177.36
 ---- batch: 060 ----
mean loss: 186.72
 ---- batch: 070 ----
mean loss: 177.42
 ---- batch: 080 ----
mean loss: 186.24
 ---- batch: 090 ----
mean loss: 185.39
 ---- batch: 100 ----
mean loss: 185.30
 ---- batch: 110 ----
mean loss: 176.90
train mean loss: 180.89
epoch train time: 0:00:00.553014
elapsed time: 0:01:13.487705
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:27:13.511452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.14
 ---- batch: 020 ----
mean loss: 176.23
 ---- batch: 030 ----
mean loss: 177.60
 ---- batch: 040 ----
mean loss: 172.65
 ---- batch: 050 ----
mean loss: 180.20
 ---- batch: 060 ----
mean loss: 181.88
 ---- batch: 070 ----
mean loss: 182.56
 ---- batch: 080 ----
mean loss: 183.43
 ---- batch: 090 ----
mean loss: 179.34
 ---- batch: 100 ----
mean loss: 176.89
 ---- batch: 110 ----
mean loss: 178.76
train mean loss: 179.56
epoch train time: 0:00:00.558186
elapsed time: 0:01:14.046023
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:27:14.069768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.66
 ---- batch: 020 ----
mean loss: 176.26
 ---- batch: 030 ----
mean loss: 184.89
 ---- batch: 040 ----
mean loss: 179.62
 ---- batch: 050 ----
mean loss: 183.15
 ---- batch: 060 ----
mean loss: 170.71
 ---- batch: 070 ----
mean loss: 175.75
 ---- batch: 080 ----
mean loss: 179.53
 ---- batch: 090 ----
mean loss: 182.05
 ---- batch: 100 ----
mean loss: 173.87
 ---- batch: 110 ----
mean loss: 183.59
train mean loss: 178.66
epoch train time: 0:00:00.549617
elapsed time: 0:01:14.595785
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:27:14.619560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.42
 ---- batch: 020 ----
mean loss: 167.58
 ---- batch: 030 ----
mean loss: 180.02
 ---- batch: 040 ----
mean loss: 181.34
 ---- batch: 050 ----
mean loss: 180.76
 ---- batch: 060 ----
mean loss: 182.88
 ---- batch: 070 ----
mean loss: 183.73
 ---- batch: 080 ----
mean loss: 173.73
 ---- batch: 090 ----
mean loss: 177.10
 ---- batch: 100 ----
mean loss: 168.21
 ---- batch: 110 ----
mean loss: 182.53
train mean loss: 177.83
epoch train time: 0:00:00.562468
elapsed time: 0:01:15.158428
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:27:15.182192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.37
 ---- batch: 020 ----
mean loss: 179.33
 ---- batch: 030 ----
mean loss: 169.70
 ---- batch: 040 ----
mean loss: 175.23
 ---- batch: 050 ----
mean loss: 177.87
 ---- batch: 060 ----
mean loss: 174.90
 ---- batch: 070 ----
mean loss: 175.94
 ---- batch: 080 ----
mean loss: 175.94
 ---- batch: 090 ----
mean loss: 181.15
 ---- batch: 100 ----
mean loss: 180.48
 ---- batch: 110 ----
mean loss: 171.59
train mean loss: 176.51
epoch train time: 0:00:00.570642
elapsed time: 0:01:15.729222
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:27:15.752969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.83
 ---- batch: 020 ----
mean loss: 171.48
 ---- batch: 030 ----
mean loss: 166.99
 ---- batch: 040 ----
mean loss: 176.61
 ---- batch: 050 ----
mean loss: 176.75
 ---- batch: 060 ----
mean loss: 185.67
 ---- batch: 070 ----
mean loss: 174.08
 ---- batch: 080 ----
mean loss: 179.55
 ---- batch: 090 ----
mean loss: 178.54
 ---- batch: 100 ----
mean loss: 171.85
 ---- batch: 110 ----
mean loss: 176.59
train mean loss: 174.84
epoch train time: 0:00:00.552272
elapsed time: 0:01:16.281630
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:27:16.305379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.74
 ---- batch: 020 ----
mean loss: 172.79
 ---- batch: 030 ----
mean loss: 168.78
 ---- batch: 040 ----
mean loss: 177.26
 ---- batch: 050 ----
mean loss: 172.78
 ---- batch: 060 ----
mean loss: 176.01
 ---- batch: 070 ----
mean loss: 175.95
 ---- batch: 080 ----
mean loss: 171.06
 ---- batch: 090 ----
mean loss: 178.73
 ---- batch: 100 ----
mean loss: 168.88
 ---- batch: 110 ----
mean loss: 190.78
train mean loss: 174.28
epoch train time: 0:00:00.562799
elapsed time: 0:01:16.844565
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:27:16.868328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.08
 ---- batch: 020 ----
mean loss: 177.04
 ---- batch: 030 ----
mean loss: 169.49
 ---- batch: 040 ----
mean loss: 171.95
 ---- batch: 050 ----
mean loss: 172.25
 ---- batch: 060 ----
mean loss: 177.63
 ---- batch: 070 ----
mean loss: 179.41
 ---- batch: 080 ----
mean loss: 171.94
 ---- batch: 090 ----
mean loss: 176.18
 ---- batch: 100 ----
mean loss: 178.18
 ---- batch: 110 ----
mean loss: 172.76
train mean loss: 174.14
epoch train time: 0:00:00.556314
elapsed time: 0:01:17.401032
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:27:17.424779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.83
 ---- batch: 020 ----
mean loss: 168.49
 ---- batch: 030 ----
mean loss: 173.17
 ---- batch: 040 ----
mean loss: 169.75
 ---- batch: 050 ----
mean loss: 172.69
 ---- batch: 060 ----
mean loss: 166.82
 ---- batch: 070 ----
mean loss: 167.60
 ---- batch: 080 ----
mean loss: 186.31
 ---- batch: 090 ----
mean loss: 175.22
 ---- batch: 100 ----
mean loss: 160.31
 ---- batch: 110 ----
mean loss: 181.83
train mean loss: 172.10
epoch train time: 0:00:00.567885
elapsed time: 0:01:17.969049
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:27:17.992814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.02
 ---- batch: 020 ----
mean loss: 177.70
 ---- batch: 030 ----
mean loss: 177.67
 ---- batch: 040 ----
mean loss: 173.92
 ---- batch: 050 ----
mean loss: 165.23
 ---- batch: 060 ----
mean loss: 176.15
 ---- batch: 070 ----
mean loss: 181.15
 ---- batch: 080 ----
mean loss: 173.37
 ---- batch: 090 ----
mean loss: 170.42
 ---- batch: 100 ----
mean loss: 169.12
 ---- batch: 110 ----
mean loss: 171.45
train mean loss: 173.17
epoch train time: 0:00:00.562117
elapsed time: 0:01:18.531362
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:27:18.555111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.36
 ---- batch: 020 ----
mean loss: 170.10
 ---- batch: 030 ----
mean loss: 175.00
 ---- batch: 040 ----
mean loss: 167.85
 ---- batch: 050 ----
mean loss: 172.73
 ---- batch: 060 ----
mean loss: 168.93
 ---- batch: 070 ----
mean loss: 168.43
 ---- batch: 080 ----
mean loss: 170.35
 ---- batch: 090 ----
mean loss: 173.26
 ---- batch: 100 ----
mean loss: 175.50
 ---- batch: 110 ----
mean loss: 171.89
train mean loss: 171.45
epoch train time: 0:00:00.561495
elapsed time: 0:01:19.092990
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:27:19.116749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.90
 ---- batch: 020 ----
mean loss: 173.22
 ---- batch: 030 ----
mean loss: 169.88
 ---- batch: 040 ----
mean loss: 164.02
 ---- batch: 050 ----
mean loss: 159.57
 ---- batch: 060 ----
mean loss: 172.88
 ---- batch: 070 ----
mean loss: 179.35
 ---- batch: 080 ----
mean loss: 177.12
 ---- batch: 090 ----
mean loss: 170.18
 ---- batch: 100 ----
mean loss: 175.12
 ---- batch: 110 ----
mean loss: 171.20
train mean loss: 171.27
epoch train time: 0:00:00.547967
elapsed time: 0:01:19.641101
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:27:19.664868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.04
 ---- batch: 020 ----
mean loss: 174.66
 ---- batch: 030 ----
mean loss: 156.88
 ---- batch: 040 ----
mean loss: 171.95
 ---- batch: 050 ----
mean loss: 172.80
 ---- batch: 060 ----
mean loss: 169.44
 ---- batch: 070 ----
mean loss: 169.13
 ---- batch: 080 ----
mean loss: 175.50
 ---- batch: 090 ----
mean loss: 173.60
 ---- batch: 100 ----
mean loss: 176.14
 ---- batch: 110 ----
mean loss: 175.00
train mean loss: 170.36
epoch train time: 0:00:00.557236
elapsed time: 0:01:20.198489
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:27:20.222238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.59
 ---- batch: 020 ----
mean loss: 178.92
 ---- batch: 030 ----
mean loss: 160.88
 ---- batch: 040 ----
mean loss: 169.12
 ---- batch: 050 ----
mean loss: 173.94
 ---- batch: 060 ----
mean loss: 167.03
 ---- batch: 070 ----
mean loss: 166.27
 ---- batch: 080 ----
mean loss: 169.79
 ---- batch: 090 ----
mean loss: 171.86
 ---- batch: 100 ----
mean loss: 173.65
 ---- batch: 110 ----
mean loss: 177.13
train mean loss: 169.67
epoch train time: 0:00:00.558862
elapsed time: 0:01:20.757488
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:27:20.781238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.26
 ---- batch: 020 ----
mean loss: 171.45
 ---- batch: 030 ----
mean loss: 169.45
 ---- batch: 040 ----
mean loss: 157.10
 ---- batch: 050 ----
mean loss: 167.41
 ---- batch: 060 ----
mean loss: 168.65
 ---- batch: 070 ----
mean loss: 172.03
 ---- batch: 080 ----
mean loss: 177.99
 ---- batch: 090 ----
mean loss: 168.26
 ---- batch: 100 ----
mean loss: 162.87
 ---- batch: 110 ----
mean loss: 176.58
train mean loss: 168.88
epoch train time: 0:00:00.556074
elapsed time: 0:01:21.313696
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:27:21.337442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.67
 ---- batch: 020 ----
mean loss: 162.40
 ---- batch: 030 ----
mean loss: 172.83
 ---- batch: 040 ----
mean loss: 167.21
 ---- batch: 050 ----
mean loss: 167.43
 ---- batch: 060 ----
mean loss: 161.20
 ---- batch: 070 ----
mean loss: 173.58
 ---- batch: 080 ----
mean loss: 168.62
 ---- batch: 090 ----
mean loss: 170.11
 ---- batch: 100 ----
mean loss: 177.68
 ---- batch: 110 ----
mean loss: 165.52
train mean loss: 167.63
epoch train time: 0:00:00.560004
elapsed time: 0:01:21.873838
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:27:21.897630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.27
 ---- batch: 020 ----
mean loss: 164.37
 ---- batch: 030 ----
mean loss: 165.30
 ---- batch: 040 ----
mean loss: 169.21
 ---- batch: 050 ----
mean loss: 159.23
 ---- batch: 060 ----
mean loss: 166.93
 ---- batch: 070 ----
mean loss: 168.67
 ---- batch: 080 ----
mean loss: 173.28
 ---- batch: 090 ----
mean loss: 167.17
 ---- batch: 100 ----
mean loss: 167.90
 ---- batch: 110 ----
mean loss: 172.14
train mean loss: 166.93
epoch train time: 0:00:00.550625
elapsed time: 0:01:22.424642
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:27:22.448387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.16
 ---- batch: 020 ----
mean loss: 160.90
 ---- batch: 030 ----
mean loss: 161.14
 ---- batch: 040 ----
mean loss: 164.06
 ---- batch: 050 ----
mean loss: 159.84
 ---- batch: 060 ----
mean loss: 173.45
 ---- batch: 070 ----
mean loss: 178.73
 ---- batch: 080 ----
mean loss: 170.57
 ---- batch: 090 ----
mean loss: 162.03
 ---- batch: 100 ----
mean loss: 169.97
 ---- batch: 110 ----
mean loss: 169.56
train mean loss: 167.35
epoch train time: 0:00:00.560761
elapsed time: 0:01:22.985533
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:27:23.009278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.47
 ---- batch: 020 ----
mean loss: 164.85
 ---- batch: 030 ----
mean loss: 162.58
 ---- batch: 040 ----
mean loss: 158.84
 ---- batch: 050 ----
mean loss: 165.93
 ---- batch: 060 ----
mean loss: 171.83
 ---- batch: 070 ----
mean loss: 166.63
 ---- batch: 080 ----
mean loss: 163.46
 ---- batch: 090 ----
mean loss: 166.83
 ---- batch: 100 ----
mean loss: 172.06
 ---- batch: 110 ----
mean loss: 170.17
train mean loss: 166.17
epoch train time: 0:00:00.554834
elapsed time: 0:01:23.540497
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:27:23.564243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.64
 ---- batch: 020 ----
mean loss: 167.80
 ---- batch: 030 ----
mean loss: 156.58
 ---- batch: 040 ----
mean loss: 169.76
 ---- batch: 050 ----
mean loss: 159.93
 ---- batch: 060 ----
mean loss: 165.87
 ---- batch: 070 ----
mean loss: 170.40
 ---- batch: 080 ----
mean loss: 174.83
 ---- batch: 090 ----
mean loss: 161.82
 ---- batch: 100 ----
mean loss: 166.13
 ---- batch: 110 ----
mean loss: 170.31
train mean loss: 166.07
epoch train time: 0:00:00.560719
elapsed time: 0:01:24.101348
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:27:24.125092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.79
 ---- batch: 020 ----
mean loss: 167.88
 ---- batch: 030 ----
mean loss: 162.87
 ---- batch: 040 ----
mean loss: 167.85
 ---- batch: 050 ----
mean loss: 163.07
 ---- batch: 060 ----
mean loss: 153.99
 ---- batch: 070 ----
mean loss: 168.69
 ---- batch: 080 ----
mean loss: 156.67
 ---- batch: 090 ----
mean loss: 170.25
 ---- batch: 100 ----
mean loss: 167.59
 ---- batch: 110 ----
mean loss: 167.01
train mean loss: 164.65
epoch train time: 0:00:00.556223
elapsed time: 0:01:24.657708
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:27:24.681457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.58
 ---- batch: 020 ----
mean loss: 172.30
 ---- batch: 030 ----
mean loss: 163.98
 ---- batch: 040 ----
mean loss: 159.06
 ---- batch: 050 ----
mean loss: 160.93
 ---- batch: 060 ----
mean loss: 167.98
 ---- batch: 070 ----
mean loss: 168.16
 ---- batch: 080 ----
mean loss: 169.69
 ---- batch: 090 ----
mean loss: 160.02
 ---- batch: 100 ----
mean loss: 166.45
 ---- batch: 110 ----
mean loss: 163.11
train mean loss: 164.44
epoch train time: 0:00:00.562046
elapsed time: 0:01:25.219891
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:27:25.243663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.94
 ---- batch: 020 ----
mean loss: 164.50
 ---- batch: 030 ----
mean loss: 168.37
 ---- batch: 040 ----
mean loss: 160.25
 ---- batch: 050 ----
mean loss: 167.27
 ---- batch: 060 ----
mean loss: 162.71
 ---- batch: 070 ----
mean loss: 161.83
 ---- batch: 080 ----
mean loss: 164.05
 ---- batch: 090 ----
mean loss: 163.02
 ---- batch: 100 ----
mean loss: 165.21
 ---- batch: 110 ----
mean loss: 163.54
train mean loss: 164.15
epoch train time: 0:00:00.563873
elapsed time: 0:01:25.783922
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:27:25.807666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.09
 ---- batch: 020 ----
mean loss: 166.61
 ---- batch: 030 ----
mean loss: 162.72
 ---- batch: 040 ----
mean loss: 172.12
 ---- batch: 050 ----
mean loss: 170.22
 ---- batch: 060 ----
mean loss: 160.54
 ---- batch: 070 ----
mean loss: 160.72
 ---- batch: 080 ----
mean loss: 157.31
 ---- batch: 090 ----
mean loss: 165.68
 ---- batch: 100 ----
mean loss: 162.69
 ---- batch: 110 ----
mean loss: 165.56
train mean loss: 164.46
epoch train time: 0:00:00.548510
elapsed time: 0:01:26.332566
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:27:26.356314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.31
 ---- batch: 020 ----
mean loss: 155.03
 ---- batch: 030 ----
mean loss: 147.60
 ---- batch: 040 ----
mean loss: 166.50
 ---- batch: 050 ----
mean loss: 172.38
 ---- batch: 060 ----
mean loss: 169.30
 ---- batch: 070 ----
mean loss: 170.23
 ---- batch: 080 ----
mean loss: 161.20
 ---- batch: 090 ----
mean loss: 157.62
 ---- batch: 100 ----
mean loss: 167.32
 ---- batch: 110 ----
mean loss: 169.59
train mean loss: 163.45
epoch train time: 0:00:00.569376
elapsed time: 0:01:26.902079
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:27:26.925826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.31
 ---- batch: 020 ----
mean loss: 158.92
 ---- batch: 030 ----
mean loss: 158.92
 ---- batch: 040 ----
mean loss: 165.57
 ---- batch: 050 ----
mean loss: 174.45
 ---- batch: 060 ----
mean loss: 158.58
 ---- batch: 070 ----
mean loss: 155.84
 ---- batch: 080 ----
mean loss: 169.66
 ---- batch: 090 ----
mean loss: 172.23
 ---- batch: 100 ----
mean loss: 164.26
 ---- batch: 110 ----
mean loss: 161.94
train mean loss: 163.17
epoch train time: 0:00:00.555258
elapsed time: 0:01:27.457469
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:27:27.481216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.96
 ---- batch: 020 ----
mean loss: 160.56
 ---- batch: 030 ----
mean loss: 159.78
 ---- batch: 040 ----
mean loss: 156.56
 ---- batch: 050 ----
mean loss: 161.83
 ---- batch: 060 ----
mean loss: 167.34
 ---- batch: 070 ----
mean loss: 166.58
 ---- batch: 080 ----
mean loss: 174.91
 ---- batch: 090 ----
mean loss: 162.75
 ---- batch: 100 ----
mean loss: 163.96
 ---- batch: 110 ----
mean loss: 157.07
train mean loss: 162.15
epoch train time: 0:00:00.581359
elapsed time: 0:01:28.038981
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:27:28.062772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.55
 ---- batch: 020 ----
mean loss: 165.42
 ---- batch: 030 ----
mean loss: 152.65
 ---- batch: 040 ----
mean loss: 161.25
 ---- batch: 050 ----
mean loss: 160.03
 ---- batch: 060 ----
mean loss: 160.35
 ---- batch: 070 ----
mean loss: 155.98
 ---- batch: 080 ----
mean loss: 153.63
 ---- batch: 090 ----
mean loss: 163.35
 ---- batch: 100 ----
mean loss: 166.24
 ---- batch: 110 ----
mean loss: 171.91
train mean loss: 161.60
epoch train time: 0:00:00.558721
elapsed time: 0:01:28.597896
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:27:28.621666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.80
 ---- batch: 020 ----
mean loss: 157.52
 ---- batch: 030 ----
mean loss: 163.52
 ---- batch: 040 ----
mean loss: 155.49
 ---- batch: 050 ----
mean loss: 160.72
 ---- batch: 060 ----
mean loss: 168.88
 ---- batch: 070 ----
mean loss: 160.99
 ---- batch: 080 ----
mean loss: 160.90
 ---- batch: 090 ----
mean loss: 157.32
 ---- batch: 100 ----
mean loss: 165.62
 ---- batch: 110 ----
mean loss: 164.86
train mean loss: 160.79
epoch train time: 0:00:00.560758
elapsed time: 0:01:29.158835
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:27:29.182595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.19
 ---- batch: 020 ----
mean loss: 162.94
 ---- batch: 030 ----
mean loss: 160.89
 ---- batch: 040 ----
mean loss: 155.06
 ---- batch: 050 ----
mean loss: 165.81
 ---- batch: 060 ----
mean loss: 155.06
 ---- batch: 070 ----
mean loss: 160.45
 ---- batch: 080 ----
mean loss: 163.66
 ---- batch: 090 ----
mean loss: 162.22
 ---- batch: 100 ----
mean loss: 152.85
 ---- batch: 110 ----
mean loss: 162.98
train mean loss: 160.65
epoch train time: 0:00:00.573465
elapsed time: 0:01:29.732451
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:27:29.756199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.29
 ---- batch: 020 ----
mean loss: 160.55
 ---- batch: 030 ----
mean loss: 161.31
 ---- batch: 040 ----
mean loss: 160.23
 ---- batch: 050 ----
mean loss: 158.21
 ---- batch: 060 ----
mean loss: 163.46
 ---- batch: 070 ----
mean loss: 157.95
 ---- batch: 080 ----
mean loss: 172.13
 ---- batch: 090 ----
mean loss: 157.05
 ---- batch: 100 ----
mean loss: 165.45
 ---- batch: 110 ----
mean loss: 158.34
train mean loss: 161.52
epoch train time: 0:00:00.551146
elapsed time: 0:01:30.283739
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:27:30.307501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.43
 ---- batch: 020 ----
mean loss: 152.80
 ---- batch: 030 ----
mean loss: 159.32
 ---- batch: 040 ----
mean loss: 159.32
 ---- batch: 050 ----
mean loss: 153.92
 ---- batch: 060 ----
mean loss: 167.71
 ---- batch: 070 ----
mean loss: 158.39
 ---- batch: 080 ----
mean loss: 161.09
 ---- batch: 090 ----
mean loss: 155.06
 ---- batch: 100 ----
mean loss: 163.36
 ---- batch: 110 ----
mean loss: 168.04
train mean loss: 160.06
epoch train time: 0:00:00.546745
elapsed time: 0:01:30.830648
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:27:30.854391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.32
 ---- batch: 020 ----
mean loss: 154.06
 ---- batch: 030 ----
mean loss: 158.66
 ---- batch: 040 ----
mean loss: 158.10
 ---- batch: 050 ----
mean loss: 158.32
 ---- batch: 060 ----
mean loss: 159.41
 ---- batch: 070 ----
mean loss: 156.89
 ---- batch: 080 ----
mean loss: 153.48
 ---- batch: 090 ----
mean loss: 160.75
 ---- batch: 100 ----
mean loss: 166.90
 ---- batch: 110 ----
mean loss: 171.85
train mean loss: 158.77
epoch train time: 0:00:00.546829
elapsed time: 0:01:31.377601
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:27:31.401346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.29
 ---- batch: 020 ----
mean loss: 155.61
 ---- batch: 030 ----
mean loss: 157.87
 ---- batch: 040 ----
mean loss: 154.66
 ---- batch: 050 ----
mean loss: 152.67
 ---- batch: 060 ----
mean loss: 165.48
 ---- batch: 070 ----
mean loss: 159.90
 ---- batch: 080 ----
mean loss: 156.50
 ---- batch: 090 ----
mean loss: 162.71
 ---- batch: 100 ----
mean loss: 169.33
 ---- batch: 110 ----
mean loss: 157.46
train mean loss: 159.71
epoch train time: 0:00:00.547966
elapsed time: 0:01:31.925710
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:27:31.949448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.89
 ---- batch: 020 ----
mean loss: 163.53
 ---- batch: 030 ----
mean loss: 155.99
 ---- batch: 040 ----
mean loss: 154.87
 ---- batch: 050 ----
mean loss: 160.38
 ---- batch: 060 ----
mean loss: 157.74
 ---- batch: 070 ----
mean loss: 155.13
 ---- batch: 080 ----
mean loss: 162.41
 ---- batch: 090 ----
mean loss: 162.17
 ---- batch: 100 ----
mean loss: 156.86
 ---- batch: 110 ----
mean loss: 162.11
train mean loss: 158.47
epoch train time: 0:00:00.560930
elapsed time: 0:01:32.486767
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:27:32.510534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.83
 ---- batch: 020 ----
mean loss: 164.24
 ---- batch: 030 ----
mean loss: 160.19
 ---- batch: 040 ----
mean loss: 156.78
 ---- batch: 050 ----
mean loss: 161.27
 ---- batch: 060 ----
mean loss: 153.26
 ---- batch: 070 ----
mean loss: 165.36
 ---- batch: 080 ----
mean loss: 161.07
 ---- batch: 090 ----
mean loss: 159.27
 ---- batch: 100 ----
mean loss: 150.62
 ---- batch: 110 ----
mean loss: 161.05
train mean loss: 158.80
epoch train time: 0:00:00.552270
elapsed time: 0:01:33.039206
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:27:33.062951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.54
 ---- batch: 020 ----
mean loss: 160.13
 ---- batch: 030 ----
mean loss: 146.23
 ---- batch: 040 ----
mean loss: 155.38
 ---- batch: 050 ----
mean loss: 160.59
 ---- batch: 060 ----
mean loss: 154.30
 ---- batch: 070 ----
mean loss: 164.73
 ---- batch: 080 ----
mean loss: 160.06
 ---- batch: 090 ----
mean loss: 163.05
 ---- batch: 100 ----
mean loss: 165.57
 ---- batch: 110 ----
mean loss: 159.67
train mean loss: 158.58
epoch train time: 0:00:00.551594
elapsed time: 0:01:33.590936
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:27:33.614682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.91
 ---- batch: 020 ----
mean loss: 150.15
 ---- batch: 030 ----
mean loss: 159.92
 ---- batch: 040 ----
mean loss: 152.68
 ---- batch: 050 ----
mean loss: 160.34
 ---- batch: 060 ----
mean loss: 156.22
 ---- batch: 070 ----
mean loss: 164.18
 ---- batch: 080 ----
mean loss: 161.28
 ---- batch: 090 ----
mean loss: 154.65
 ---- batch: 100 ----
mean loss: 157.96
 ---- batch: 110 ----
mean loss: 159.60
train mean loss: 157.19
epoch train time: 0:00:00.564434
elapsed time: 0:01:34.155520
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:27:34.179265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.43
 ---- batch: 020 ----
mean loss: 152.12
 ---- batch: 030 ----
mean loss: 153.28
 ---- batch: 040 ----
mean loss: 148.47
 ---- batch: 050 ----
mean loss: 163.81
 ---- batch: 060 ----
mean loss: 158.15
 ---- batch: 070 ----
mean loss: 157.88
 ---- batch: 080 ----
mean loss: 161.03
 ---- batch: 090 ----
mean loss: 151.17
 ---- batch: 100 ----
mean loss: 153.85
 ---- batch: 110 ----
mean loss: 163.09
train mean loss: 156.77
epoch train time: 0:00:00.553344
elapsed time: 0:01:34.708991
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:27:34.732736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.03
 ---- batch: 020 ----
mean loss: 154.67
 ---- batch: 030 ----
mean loss: 153.38
 ---- batch: 040 ----
mean loss: 159.84
 ---- batch: 050 ----
mean loss: 157.01
 ---- batch: 060 ----
mean loss: 157.31
 ---- batch: 070 ----
mean loss: 161.42
 ---- batch: 080 ----
mean loss: 162.40
 ---- batch: 090 ----
mean loss: 166.18
 ---- batch: 100 ----
mean loss: 147.28
 ---- batch: 110 ----
mean loss: 160.57
train mean loss: 158.08
epoch train time: 0:00:00.547028
elapsed time: 0:01:35.256149
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:27:35.279892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.04
 ---- batch: 020 ----
mean loss: 152.39
 ---- batch: 030 ----
mean loss: 154.49
 ---- batch: 040 ----
mean loss: 145.81
 ---- batch: 050 ----
mean loss: 153.13
 ---- batch: 060 ----
mean loss: 157.17
 ---- batch: 070 ----
mean loss: 150.74
 ---- batch: 080 ----
mean loss: 163.20
 ---- batch: 090 ----
mean loss: 158.35
 ---- batch: 100 ----
mean loss: 159.99
 ---- batch: 110 ----
mean loss: 164.94
train mean loss: 156.56
epoch train time: 0:00:00.556451
elapsed time: 0:01:35.812733
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:27:35.836480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.36
 ---- batch: 020 ----
mean loss: 162.80
 ---- batch: 030 ----
mean loss: 151.63
 ---- batch: 040 ----
mean loss: 156.55
 ---- batch: 050 ----
mean loss: 159.66
 ---- batch: 060 ----
mean loss: 170.16
 ---- batch: 070 ----
mean loss: 150.74
 ---- batch: 080 ----
mean loss: 149.31
 ---- batch: 090 ----
mean loss: 155.83
 ---- batch: 100 ----
mean loss: 158.57
 ---- batch: 110 ----
mean loss: 155.85
train mean loss: 156.69
epoch train time: 0:00:00.554125
elapsed time: 0:01:36.366993
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:27:36.390755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.69
 ---- batch: 020 ----
mean loss: 162.65
 ---- batch: 030 ----
mean loss: 153.64
 ---- batch: 040 ----
mean loss: 148.20
 ---- batch: 050 ----
mean loss: 156.29
 ---- batch: 060 ----
mean loss: 152.61
 ---- batch: 070 ----
mean loss: 155.06
 ---- batch: 080 ----
mean loss: 162.50
 ---- batch: 090 ----
mean loss: 156.19
 ---- batch: 100 ----
mean loss: 144.43
 ---- batch: 110 ----
mean loss: 149.20
train mean loss: 155.00
epoch train time: 0:00:00.565001
elapsed time: 0:01:36.932161
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:27:36.955917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.08
 ---- batch: 020 ----
mean loss: 152.23
 ---- batch: 030 ----
mean loss: 158.49
 ---- batch: 040 ----
mean loss: 159.31
 ---- batch: 050 ----
mean loss: 156.63
 ---- batch: 060 ----
mean loss: 156.98
 ---- batch: 070 ----
mean loss: 162.97
 ---- batch: 080 ----
mean loss: 163.51
 ---- batch: 090 ----
mean loss: 154.49
 ---- batch: 100 ----
mean loss: 161.13
 ---- batch: 110 ----
mean loss: 154.03
train mean loss: 157.03
epoch train time: 0:00:00.553519
elapsed time: 0:01:37.485840
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:27:37.509579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.41
 ---- batch: 020 ----
mean loss: 157.85
 ---- batch: 030 ----
mean loss: 151.11
 ---- batch: 040 ----
mean loss: 152.88
 ---- batch: 050 ----
mean loss: 151.80
 ---- batch: 060 ----
mean loss: 155.71
 ---- batch: 070 ----
mean loss: 156.75
 ---- batch: 080 ----
mean loss: 158.55
 ---- batch: 090 ----
mean loss: 155.70
 ---- batch: 100 ----
mean loss: 158.25
 ---- batch: 110 ----
mean loss: 152.35
train mean loss: 155.39
epoch train time: 0:00:00.554944
elapsed time: 0:01:38.040910
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:27:38.064656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.07
 ---- batch: 020 ----
mean loss: 149.83
 ---- batch: 030 ----
mean loss: 152.96
 ---- batch: 040 ----
mean loss: 153.75
 ---- batch: 050 ----
mean loss: 154.52
 ---- batch: 060 ----
mean loss: 150.52
 ---- batch: 070 ----
mean loss: 158.31
 ---- batch: 080 ----
mean loss: 163.57
 ---- batch: 090 ----
mean loss: 155.48
 ---- batch: 100 ----
mean loss: 155.24
 ---- batch: 110 ----
mean loss: 156.67
train mean loss: 154.63
epoch train time: 0:00:00.549831
elapsed time: 0:01:38.590900
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:27:38.614646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.70
 ---- batch: 020 ----
mean loss: 151.28
 ---- batch: 030 ----
mean loss: 152.27
 ---- batch: 040 ----
mean loss: 151.49
 ---- batch: 050 ----
mean loss: 159.82
 ---- batch: 060 ----
mean loss: 156.72
 ---- batch: 070 ----
mean loss: 154.43
 ---- batch: 080 ----
mean loss: 155.13
 ---- batch: 090 ----
mean loss: 158.81
 ---- batch: 100 ----
mean loss: 152.69
 ---- batch: 110 ----
mean loss: 160.69
train mean loss: 155.01
epoch train time: 0:00:00.557991
elapsed time: 0:01:39.149023
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:27:39.172770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.43
 ---- batch: 020 ----
mean loss: 141.27
 ---- batch: 030 ----
mean loss: 164.08
 ---- batch: 040 ----
mean loss: 156.75
 ---- batch: 050 ----
mean loss: 155.38
 ---- batch: 060 ----
mean loss: 150.85
 ---- batch: 070 ----
mean loss: 161.77
 ---- batch: 080 ----
mean loss: 146.96
 ---- batch: 090 ----
mean loss: 162.28
 ---- batch: 100 ----
mean loss: 154.41
 ---- batch: 110 ----
mean loss: 154.09
train mean loss: 154.81
epoch train time: 0:00:00.576586
elapsed time: 0:01:39.725754
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:27:39.749504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.69
 ---- batch: 020 ----
mean loss: 146.91
 ---- batch: 030 ----
mean loss: 151.56
 ---- batch: 040 ----
mean loss: 154.24
 ---- batch: 050 ----
mean loss: 140.93
 ---- batch: 060 ----
mean loss: 151.98
 ---- batch: 070 ----
mean loss: 153.43
 ---- batch: 080 ----
mean loss: 159.83
 ---- batch: 090 ----
mean loss: 156.32
 ---- batch: 100 ----
mean loss: 158.17
 ---- batch: 110 ----
mean loss: 158.07
train mean loss: 153.22
epoch train time: 0:00:00.581403
elapsed time: 0:01:40.307295
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:27:40.331063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.52
 ---- batch: 020 ----
mean loss: 158.02
 ---- batch: 030 ----
mean loss: 149.52
 ---- batch: 040 ----
mean loss: 155.12
 ---- batch: 050 ----
mean loss: 156.70
 ---- batch: 060 ----
mean loss: 155.94
 ---- batch: 070 ----
mean loss: 156.77
 ---- batch: 080 ----
mean loss: 153.31
 ---- batch: 090 ----
mean loss: 151.49
 ---- batch: 100 ----
mean loss: 160.22
 ---- batch: 110 ----
mean loss: 146.04
train mean loss: 154.27
epoch train time: 0:00:00.577675
elapsed time: 0:01:40.885146
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:27:40.908901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.79
 ---- batch: 020 ----
mean loss: 144.87
 ---- batch: 030 ----
mean loss: 152.66
 ---- batch: 040 ----
mean loss: 156.00
 ---- batch: 050 ----
mean loss: 151.26
 ---- batch: 060 ----
mean loss: 151.70
 ---- batch: 070 ----
mean loss: 153.05
 ---- batch: 080 ----
mean loss: 155.86
 ---- batch: 090 ----
mean loss: 158.27
 ---- batch: 100 ----
mean loss: 156.49
 ---- batch: 110 ----
mean loss: 149.92
train mean loss: 152.77
epoch train time: 0:00:00.575614
elapsed time: 0:01:41.460905
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:27:41.484652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.68
 ---- batch: 020 ----
mean loss: 144.16
 ---- batch: 030 ----
mean loss: 154.05
 ---- batch: 040 ----
mean loss: 152.43
 ---- batch: 050 ----
mean loss: 158.35
 ---- batch: 060 ----
mean loss: 160.21
 ---- batch: 070 ----
mean loss: 163.74
 ---- batch: 080 ----
mean loss: 150.63
 ---- batch: 090 ----
mean loss: 155.12
 ---- batch: 100 ----
mean loss: 150.69
 ---- batch: 110 ----
mean loss: 148.35
train mean loss: 153.44
epoch train time: 0:00:00.572884
elapsed time: 0:01:42.033945
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:27:42.057695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.32
 ---- batch: 020 ----
mean loss: 146.02
 ---- batch: 030 ----
mean loss: 155.51
 ---- batch: 040 ----
mean loss: 154.42
 ---- batch: 050 ----
mean loss: 148.29
 ---- batch: 060 ----
mean loss: 153.27
 ---- batch: 070 ----
mean loss: 159.94
 ---- batch: 080 ----
mean loss: 160.20
 ---- batch: 090 ----
mean loss: 156.07
 ---- batch: 100 ----
mean loss: 150.64
 ---- batch: 110 ----
mean loss: 154.53
train mean loss: 153.80
epoch train time: 0:00:00.566201
elapsed time: 0:01:42.600282
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:27:42.624030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.84
 ---- batch: 020 ----
mean loss: 146.74
 ---- batch: 030 ----
mean loss: 153.18
 ---- batch: 040 ----
mean loss: 152.82
 ---- batch: 050 ----
mean loss: 152.80
 ---- batch: 060 ----
mean loss: 149.59
 ---- batch: 070 ----
mean loss: 152.40
 ---- batch: 080 ----
mean loss: 154.95
 ---- batch: 090 ----
mean loss: 153.91
 ---- batch: 100 ----
mean loss: 153.86
 ---- batch: 110 ----
mean loss: 151.80
train mean loss: 152.07
epoch train time: 0:00:00.591145
elapsed time: 0:01:43.191582
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:27:43.215321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.53
 ---- batch: 020 ----
mean loss: 153.57
 ---- batch: 030 ----
mean loss: 153.39
 ---- batch: 040 ----
mean loss: 137.35
 ---- batch: 050 ----
mean loss: 164.30
 ---- batch: 060 ----
mean loss: 156.86
 ---- batch: 070 ----
mean loss: 151.50
 ---- batch: 080 ----
mean loss: 152.70
 ---- batch: 090 ----
mean loss: 152.83
 ---- batch: 100 ----
mean loss: 151.83
 ---- batch: 110 ----
mean loss: 152.65
train mean loss: 152.88
epoch train time: 0:00:00.573249
elapsed time: 0:01:43.764975
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:27:43.788789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.34
 ---- batch: 020 ----
mean loss: 141.46
 ---- batch: 030 ----
mean loss: 152.38
 ---- batch: 040 ----
mean loss: 150.03
 ---- batch: 050 ----
mean loss: 153.58
 ---- batch: 060 ----
mean loss: 151.12
 ---- batch: 070 ----
mean loss: 157.14
 ---- batch: 080 ----
mean loss: 157.90
 ---- batch: 090 ----
mean loss: 146.98
 ---- batch: 100 ----
mean loss: 160.97
 ---- batch: 110 ----
mean loss: 145.09
train mean loss: 152.10
epoch train time: 0:00:00.566117
elapsed time: 0:01:44.331314
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:27:44.355063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.07
 ---- batch: 020 ----
mean loss: 158.49
 ---- batch: 030 ----
mean loss: 147.08
 ---- batch: 040 ----
mean loss: 148.59
 ---- batch: 050 ----
mean loss: 153.75
 ---- batch: 060 ----
mean loss: 155.47
 ---- batch: 070 ----
mean loss: 141.86
 ---- batch: 080 ----
mean loss: 154.22
 ---- batch: 090 ----
mean loss: 152.62
 ---- batch: 100 ----
mean loss: 152.11
 ---- batch: 110 ----
mean loss: 156.81
train mean loss: 151.76
epoch train time: 0:00:00.568678
elapsed time: 0:01:44.900158
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:27:44.923924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.57
 ---- batch: 020 ----
mean loss: 147.39
 ---- batch: 030 ----
mean loss: 151.97
 ---- batch: 040 ----
mean loss: 156.12
 ---- batch: 050 ----
mean loss: 145.68
 ---- batch: 060 ----
mean loss: 145.94
 ---- batch: 070 ----
mean loss: 161.93
 ---- batch: 080 ----
mean loss: 155.49
 ---- batch: 090 ----
mean loss: 156.73
 ---- batch: 100 ----
mean loss: 147.43
 ---- batch: 110 ----
mean loss: 152.75
train mean loss: 151.60
epoch train time: 0:00:00.568440
elapsed time: 0:01:45.468756
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:27:45.492499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.55
 ---- batch: 020 ----
mean loss: 151.87
 ---- batch: 030 ----
mean loss: 151.35
 ---- batch: 040 ----
mean loss: 150.25
 ---- batch: 050 ----
mean loss: 151.85
 ---- batch: 060 ----
mean loss: 150.46
 ---- batch: 070 ----
mean loss: 156.64
 ---- batch: 080 ----
mean loss: 148.42
 ---- batch: 090 ----
mean loss: 145.83
 ---- batch: 100 ----
mean loss: 157.18
 ---- batch: 110 ----
mean loss: 157.69
train mean loss: 151.79
epoch train time: 0:00:00.581929
elapsed time: 0:01:46.050857
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:27:46.074606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.40
 ---- batch: 020 ----
mean loss: 151.93
 ---- batch: 030 ----
mean loss: 148.71
 ---- batch: 040 ----
mean loss: 152.41
 ---- batch: 050 ----
mean loss: 149.27
 ---- batch: 060 ----
mean loss: 154.11
 ---- batch: 070 ----
mean loss: 157.92
 ---- batch: 080 ----
mean loss: 143.44
 ---- batch: 090 ----
mean loss: 142.98
 ---- batch: 100 ----
mean loss: 151.38
 ---- batch: 110 ----
mean loss: 148.27
train mean loss: 150.19
epoch train time: 0:00:00.569156
elapsed time: 0:01:46.620149
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:27:46.643895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.40
 ---- batch: 020 ----
mean loss: 145.34
 ---- batch: 030 ----
mean loss: 146.65
 ---- batch: 040 ----
mean loss: 143.59
 ---- batch: 050 ----
mean loss: 153.37
 ---- batch: 060 ----
mean loss: 141.60
 ---- batch: 070 ----
mean loss: 151.70
 ---- batch: 080 ----
mean loss: 150.32
 ---- batch: 090 ----
mean loss: 160.42
 ---- batch: 100 ----
mean loss: 145.16
 ---- batch: 110 ----
mean loss: 155.94
train mean loss: 150.54
epoch train time: 0:00:00.567124
elapsed time: 0:01:47.187409
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:27:47.211155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.30
 ---- batch: 020 ----
mean loss: 150.47
 ---- batch: 030 ----
mean loss: 155.48
 ---- batch: 040 ----
mean loss: 153.51
 ---- batch: 050 ----
mean loss: 151.66
 ---- batch: 060 ----
mean loss: 149.70
 ---- batch: 070 ----
mean loss: 151.83
 ---- batch: 080 ----
mean loss: 146.48
 ---- batch: 090 ----
mean loss: 150.23
 ---- batch: 100 ----
mean loss: 153.87
 ---- batch: 110 ----
mean loss: 150.86
train mean loss: 151.01
epoch train time: 0:00:00.574108
elapsed time: 0:01:47.761651
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:27:47.785416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.45
 ---- batch: 020 ----
mean loss: 153.11
 ---- batch: 030 ----
mean loss: 155.11
 ---- batch: 040 ----
mean loss: 159.48
 ---- batch: 050 ----
mean loss: 147.41
 ---- batch: 060 ----
mean loss: 151.34
 ---- batch: 070 ----
mean loss: 150.19
 ---- batch: 080 ----
mean loss: 151.03
 ---- batch: 090 ----
mean loss: 153.93
 ---- batch: 100 ----
mean loss: 145.88
 ---- batch: 110 ----
mean loss: 156.68
train mean loss: 151.87
epoch train time: 0:00:00.574700
elapsed time: 0:01:48.336533
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:27:48.360278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.88
 ---- batch: 020 ----
mean loss: 152.50
 ---- batch: 030 ----
mean loss: 152.68
 ---- batch: 040 ----
mean loss: 145.96
 ---- batch: 050 ----
mean loss: 142.84
 ---- batch: 060 ----
mean loss: 150.84
 ---- batch: 070 ----
mean loss: 152.73
 ---- batch: 080 ----
mean loss: 147.06
 ---- batch: 090 ----
mean loss: 155.07
 ---- batch: 100 ----
mean loss: 143.75
 ---- batch: 110 ----
mean loss: 146.47
train mean loss: 149.90
epoch train time: 0:00:00.575378
elapsed time: 0:01:48.912042
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:27:48.935786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.73
 ---- batch: 020 ----
mean loss: 148.13
 ---- batch: 030 ----
mean loss: 147.27
 ---- batch: 040 ----
mean loss: 151.32
 ---- batch: 050 ----
mean loss: 147.82
 ---- batch: 060 ----
mean loss: 149.79
 ---- batch: 070 ----
mean loss: 143.40
 ---- batch: 080 ----
mean loss: 151.02
 ---- batch: 090 ----
mean loss: 139.64
 ---- batch: 100 ----
mean loss: 159.36
 ---- batch: 110 ----
mean loss: 158.60
train mean loss: 148.42
epoch train time: 0:00:00.571632
elapsed time: 0:01:49.483802
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:27:49.507549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.35
 ---- batch: 020 ----
mean loss: 156.63
 ---- batch: 030 ----
mean loss: 139.18
 ---- batch: 040 ----
mean loss: 155.70
 ---- batch: 050 ----
mean loss: 143.31
 ---- batch: 060 ----
mean loss: 150.69
 ---- batch: 070 ----
mean loss: 146.65
 ---- batch: 080 ----
mean loss: 150.15
 ---- batch: 090 ----
mean loss: 144.32
 ---- batch: 100 ----
mean loss: 155.39
 ---- batch: 110 ----
mean loss: 147.65
train mean loss: 149.29
epoch train time: 0:00:00.576755
elapsed time: 0:01:50.060697
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:27:50.084444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.80
 ---- batch: 020 ----
mean loss: 149.85
 ---- batch: 030 ----
mean loss: 150.87
 ---- batch: 040 ----
mean loss: 159.23
 ---- batch: 050 ----
mean loss: 153.02
 ---- batch: 060 ----
mean loss: 150.42
 ---- batch: 070 ----
mean loss: 144.94
 ---- batch: 080 ----
mean loss: 143.93
 ---- batch: 090 ----
mean loss: 147.75
 ---- batch: 100 ----
mean loss: 140.14
 ---- batch: 110 ----
mean loss: 154.78
train mean loss: 148.80
epoch train time: 0:00:00.560878
elapsed time: 0:01:50.621754
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:27:50.645498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.77
 ---- batch: 020 ----
mean loss: 144.66
 ---- batch: 030 ----
mean loss: 149.80
 ---- batch: 040 ----
mean loss: 153.20
 ---- batch: 050 ----
mean loss: 148.50
 ---- batch: 060 ----
mean loss: 145.32
 ---- batch: 070 ----
mean loss: 150.13
 ---- batch: 080 ----
mean loss: 157.10
 ---- batch: 090 ----
mean loss: 157.90
 ---- batch: 100 ----
mean loss: 148.53
 ---- batch: 110 ----
mean loss: 144.99
train mean loss: 149.14
epoch train time: 0:00:00.549857
elapsed time: 0:01:51.171743
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:27:51.195518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.95
 ---- batch: 020 ----
mean loss: 146.98
 ---- batch: 030 ----
mean loss: 146.09
 ---- batch: 040 ----
mean loss: 140.45
 ---- batch: 050 ----
mean loss: 152.03
 ---- batch: 060 ----
mean loss: 148.52
 ---- batch: 070 ----
mean loss: 151.20
 ---- batch: 080 ----
mean loss: 149.75
 ---- batch: 090 ----
mean loss: 153.06
 ---- batch: 100 ----
mean loss: 146.10
 ---- batch: 110 ----
mean loss: 143.72
train mean loss: 148.38
epoch train time: 0:00:00.549979
elapsed time: 0:01:51.721887
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:27:51.745659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.91
 ---- batch: 020 ----
mean loss: 154.07
 ---- batch: 030 ----
mean loss: 144.59
 ---- batch: 040 ----
mean loss: 144.85
 ---- batch: 050 ----
mean loss: 141.43
 ---- batch: 060 ----
mean loss: 152.41
 ---- batch: 070 ----
mean loss: 148.14
 ---- batch: 080 ----
mean loss: 148.77
 ---- batch: 090 ----
mean loss: 142.81
 ---- batch: 100 ----
mean loss: 158.32
 ---- batch: 110 ----
mean loss: 157.20
train mean loss: 148.51
epoch train time: 0:00:00.559495
elapsed time: 0:01:52.281540
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:27:52.305296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.59
 ---- batch: 020 ----
mean loss: 143.59
 ---- batch: 030 ----
mean loss: 141.19
 ---- batch: 040 ----
mean loss: 146.63
 ---- batch: 050 ----
mean loss: 152.09
 ---- batch: 060 ----
mean loss: 140.59
 ---- batch: 070 ----
mean loss: 154.37
 ---- batch: 080 ----
mean loss: 155.34
 ---- batch: 090 ----
mean loss: 159.08
 ---- batch: 100 ----
mean loss: 151.81
 ---- batch: 110 ----
mean loss: 139.81
train mean loss: 147.73
epoch train time: 0:00:00.560107
elapsed time: 0:01:52.841791
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:27:52.865536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.96
 ---- batch: 020 ----
mean loss: 142.97
 ---- batch: 030 ----
mean loss: 152.13
 ---- batch: 040 ----
mean loss: 150.91
 ---- batch: 050 ----
mean loss: 144.01
 ---- batch: 060 ----
mean loss: 147.91
 ---- batch: 070 ----
mean loss: 141.63
 ---- batch: 080 ----
mean loss: 148.71
 ---- batch: 090 ----
mean loss: 145.55
 ---- batch: 100 ----
mean loss: 153.31
 ---- batch: 110 ----
mean loss: 151.27
train mean loss: 147.71
epoch train time: 0:00:00.550368
elapsed time: 0:01:53.392306
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:27:53.416050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.09
 ---- batch: 020 ----
mean loss: 154.01
 ---- batch: 030 ----
mean loss: 150.31
 ---- batch: 040 ----
mean loss: 148.09
 ---- batch: 050 ----
mean loss: 152.36
 ---- batch: 060 ----
mean loss: 147.40
 ---- batch: 070 ----
mean loss: 146.42
 ---- batch: 080 ----
mean loss: 148.34
 ---- batch: 090 ----
mean loss: 151.43
 ---- batch: 100 ----
mean loss: 144.54
 ---- batch: 110 ----
mean loss: 143.34
train mean loss: 147.50
epoch train time: 0:00:00.558756
elapsed time: 0:01:53.951205
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:27:53.974954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.70
 ---- batch: 020 ----
mean loss: 146.61
 ---- batch: 030 ----
mean loss: 141.17
 ---- batch: 040 ----
mean loss: 146.06
 ---- batch: 050 ----
mean loss: 144.98
 ---- batch: 060 ----
mean loss: 144.30
 ---- batch: 070 ----
mean loss: 152.16
 ---- batch: 080 ----
mean loss: 145.56
 ---- batch: 090 ----
mean loss: 147.60
 ---- batch: 100 ----
mean loss: 150.49
 ---- batch: 110 ----
mean loss: 148.21
train mean loss: 147.04
epoch train time: 0:00:00.556096
elapsed time: 0:01:54.507436
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:27:54.531183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.85
 ---- batch: 020 ----
mean loss: 146.81
 ---- batch: 030 ----
mean loss: 143.53
 ---- batch: 040 ----
mean loss: 141.89
 ---- batch: 050 ----
mean loss: 146.78
 ---- batch: 060 ----
mean loss: 144.87
 ---- batch: 070 ----
mean loss: 150.47
 ---- batch: 080 ----
mean loss: 146.15
 ---- batch: 090 ----
mean loss: 154.39
 ---- batch: 100 ----
mean loss: 152.87
 ---- batch: 110 ----
mean loss: 139.39
train mean loss: 146.42
epoch train time: 0:00:00.552155
elapsed time: 0:01:55.059731
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:27:55.083496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.03
 ---- batch: 020 ----
mean loss: 150.54
 ---- batch: 030 ----
mean loss: 148.25
 ---- batch: 040 ----
mean loss: 137.45
 ---- batch: 050 ----
mean loss: 145.66
 ---- batch: 060 ----
mean loss: 138.82
 ---- batch: 070 ----
mean loss: 152.48
 ---- batch: 080 ----
mean loss: 151.54
 ---- batch: 090 ----
mean loss: 146.91
 ---- batch: 100 ----
mean loss: 150.69
 ---- batch: 110 ----
mean loss: 147.09
train mean loss: 146.16
epoch train time: 0:00:00.551115
elapsed time: 0:01:55.611016
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:27:55.634754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.33
 ---- batch: 020 ----
mean loss: 143.42
 ---- batch: 030 ----
mean loss: 138.40
 ---- batch: 040 ----
mean loss: 134.71
 ---- batch: 050 ----
mean loss: 139.32
 ---- batch: 060 ----
mean loss: 151.21
 ---- batch: 070 ----
mean loss: 146.46
 ---- batch: 080 ----
mean loss: 150.41
 ---- batch: 090 ----
mean loss: 149.85
 ---- batch: 100 ----
mean loss: 155.93
 ---- batch: 110 ----
mean loss: 142.60
train mean loss: 145.80
epoch train time: 0:00:00.546647
elapsed time: 0:01:56.157804
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:27:56.181564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.44
 ---- batch: 020 ----
mean loss: 137.67
 ---- batch: 030 ----
mean loss: 149.27
 ---- batch: 040 ----
mean loss: 138.49
 ---- batch: 050 ----
mean loss: 147.81
 ---- batch: 060 ----
mean loss: 152.51
 ---- batch: 070 ----
mean loss: 144.05
 ---- batch: 080 ----
mean loss: 155.47
 ---- batch: 090 ----
mean loss: 153.14
 ---- batch: 100 ----
mean loss: 140.01
 ---- batch: 110 ----
mean loss: 145.83
train mean loss: 145.99
epoch train time: 0:00:00.543761
elapsed time: 0:01:56.701718
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:27:56.725461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.28
 ---- batch: 020 ----
mean loss: 144.43
 ---- batch: 030 ----
mean loss: 148.69
 ---- batch: 040 ----
mean loss: 145.92
 ---- batch: 050 ----
mean loss: 139.21
 ---- batch: 060 ----
mean loss: 149.42
 ---- batch: 070 ----
mean loss: 148.09
 ---- batch: 080 ----
mean loss: 145.28
 ---- batch: 090 ----
mean loss: 154.73
 ---- batch: 100 ----
mean loss: 149.48
 ---- batch: 110 ----
mean loss: 140.99
train mean loss: 146.69
epoch train time: 0:00:00.551703
elapsed time: 0:01:57.253551
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:27:57.277297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.44
 ---- batch: 020 ----
mean loss: 138.42
 ---- batch: 030 ----
mean loss: 142.06
 ---- batch: 040 ----
mean loss: 139.96
 ---- batch: 050 ----
mean loss: 146.07
 ---- batch: 060 ----
mean loss: 146.71
 ---- batch: 070 ----
mean loss: 144.48
 ---- batch: 080 ----
mean loss: 154.09
 ---- batch: 090 ----
mean loss: 150.25
 ---- batch: 100 ----
mean loss: 146.09
 ---- batch: 110 ----
mean loss: 142.84
train mean loss: 145.92
epoch train time: 0:00:00.549551
elapsed time: 0:01:57.803237
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:27:57.826981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.69
 ---- batch: 020 ----
mean loss: 134.24
 ---- batch: 030 ----
mean loss: 142.80
 ---- batch: 040 ----
mean loss: 144.86
 ---- batch: 050 ----
mean loss: 152.44
 ---- batch: 060 ----
mean loss: 148.44
 ---- batch: 070 ----
mean loss: 158.13
 ---- batch: 080 ----
mean loss: 148.68
 ---- batch: 090 ----
mean loss: 140.46
 ---- batch: 100 ----
mean loss: 146.24
 ---- batch: 110 ----
mean loss: 148.01
train mean loss: 146.43
epoch train time: 0:00:00.558114
elapsed time: 0:01:58.361485
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:27:58.385263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.23
 ---- batch: 020 ----
mean loss: 140.62
 ---- batch: 030 ----
mean loss: 142.43
 ---- batch: 040 ----
mean loss: 144.37
 ---- batch: 050 ----
mean loss: 144.79
 ---- batch: 060 ----
mean loss: 147.06
 ---- batch: 070 ----
mean loss: 138.57
 ---- batch: 080 ----
mean loss: 145.87
 ---- batch: 090 ----
mean loss: 151.22
 ---- batch: 100 ----
mean loss: 145.83
 ---- batch: 110 ----
mean loss: 152.27
train mean loss: 145.04
epoch train time: 0:00:00.564573
elapsed time: 0:01:58.926272
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:27:58.950091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.13
 ---- batch: 020 ----
mean loss: 143.20
 ---- batch: 030 ----
mean loss: 146.09
 ---- batch: 040 ----
mean loss: 143.40
 ---- batch: 050 ----
mean loss: 141.73
 ---- batch: 060 ----
mean loss: 139.44
 ---- batch: 070 ----
mean loss: 156.32
 ---- batch: 080 ----
mean loss: 144.17
 ---- batch: 090 ----
mean loss: 146.41
 ---- batch: 100 ----
mean loss: 144.16
 ---- batch: 110 ----
mean loss: 152.97
train mean loss: 145.75
epoch train time: 0:00:00.560060
elapsed time: 0:01:59.486554
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:27:59.510314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.73
 ---- batch: 020 ----
mean loss: 140.28
 ---- batch: 030 ----
mean loss: 149.98
 ---- batch: 040 ----
mean loss: 155.17
 ---- batch: 050 ----
mean loss: 149.67
 ---- batch: 060 ----
mean loss: 149.87
 ---- batch: 070 ----
mean loss: 148.40
 ---- batch: 080 ----
mean loss: 144.07
 ---- batch: 090 ----
mean loss: 140.31
 ---- batch: 100 ----
mean loss: 149.82
 ---- batch: 110 ----
mean loss: 142.48
train mean loss: 146.67
epoch train time: 0:00:00.560608
elapsed time: 0:02:00.047325
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:28:00.071068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.94
 ---- batch: 020 ----
mean loss: 138.66
 ---- batch: 030 ----
mean loss: 149.62
 ---- batch: 040 ----
mean loss: 143.76
 ---- batch: 050 ----
mean loss: 132.56
 ---- batch: 060 ----
mean loss: 148.21
 ---- batch: 070 ----
mean loss: 135.49
 ---- batch: 080 ----
mean loss: 145.68
 ---- batch: 090 ----
mean loss: 152.68
 ---- batch: 100 ----
mean loss: 142.94
 ---- batch: 110 ----
mean loss: 152.93
train mean loss: 143.95
epoch train time: 0:00:00.546739
elapsed time: 0:02:00.594191
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:28:00.617936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.88
 ---- batch: 020 ----
mean loss: 140.09
 ---- batch: 030 ----
mean loss: 146.02
 ---- batch: 040 ----
mean loss: 142.00
 ---- batch: 050 ----
mean loss: 143.18
 ---- batch: 060 ----
mean loss: 140.21
 ---- batch: 070 ----
mean loss: 146.20
 ---- batch: 080 ----
mean loss: 138.78
 ---- batch: 090 ----
mean loss: 137.01
 ---- batch: 100 ----
mean loss: 146.96
 ---- batch: 110 ----
mean loss: 153.05
train mean loss: 143.72
epoch train time: 0:00:00.550753
elapsed time: 0:02:01.145078
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:28:01.168825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.19
 ---- batch: 020 ----
mean loss: 142.85
 ---- batch: 030 ----
mean loss: 138.52
 ---- batch: 040 ----
mean loss: 151.07
 ---- batch: 050 ----
mean loss: 144.16
 ---- batch: 060 ----
mean loss: 138.13
 ---- batch: 070 ----
mean loss: 143.83
 ---- batch: 080 ----
mean loss: 144.53
 ---- batch: 090 ----
mean loss: 141.86
 ---- batch: 100 ----
mean loss: 151.03
 ---- batch: 110 ----
mean loss: 143.30
train mean loss: 144.44
epoch train time: 0:00:00.575807
elapsed time: 0:02:01.721022
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:28:01.744769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.66
 ---- batch: 020 ----
mean loss: 144.07
 ---- batch: 030 ----
mean loss: 135.45
 ---- batch: 040 ----
mean loss: 145.94
 ---- batch: 050 ----
mean loss: 151.97
 ---- batch: 060 ----
mean loss: 141.92
 ---- batch: 070 ----
mean loss: 137.67
 ---- batch: 080 ----
mean loss: 157.47
 ---- batch: 090 ----
mean loss: 151.03
 ---- batch: 100 ----
mean loss: 135.99
 ---- batch: 110 ----
mean loss: 138.06
train mean loss: 143.48
epoch train time: 0:00:00.563459
elapsed time: 0:02:02.284615
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:28:02.308377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.35
 ---- batch: 020 ----
mean loss: 145.13
 ---- batch: 030 ----
mean loss: 136.83
 ---- batch: 040 ----
mean loss: 147.35
 ---- batch: 050 ----
mean loss: 150.47
 ---- batch: 060 ----
mean loss: 144.17
 ---- batch: 070 ----
mean loss: 140.87
 ---- batch: 080 ----
mean loss: 145.02
 ---- batch: 090 ----
mean loss: 140.25
 ---- batch: 100 ----
mean loss: 147.53
 ---- batch: 110 ----
mean loss: 146.65
train mean loss: 143.77
epoch train time: 0:00:00.554863
elapsed time: 0:02:02.839622
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:28:02.863364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.17
 ---- batch: 020 ----
mean loss: 147.61
 ---- batch: 030 ----
mean loss: 144.62
 ---- batch: 040 ----
mean loss: 134.06
 ---- batch: 050 ----
mean loss: 153.55
 ---- batch: 060 ----
mean loss: 145.39
 ---- batch: 070 ----
mean loss: 142.35
 ---- batch: 080 ----
mean loss: 131.47
 ---- batch: 090 ----
mean loss: 139.91
 ---- batch: 100 ----
mean loss: 149.35
 ---- batch: 110 ----
mean loss: 147.61
train mean loss: 143.55
epoch train time: 0:00:00.549718
elapsed time: 0:02:03.389468
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:28:03.413212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.14
 ---- batch: 020 ----
mean loss: 142.89
 ---- batch: 030 ----
mean loss: 146.63
 ---- batch: 040 ----
mean loss: 144.95
 ---- batch: 050 ----
mean loss: 147.39
 ---- batch: 060 ----
mean loss: 143.07
 ---- batch: 070 ----
mean loss: 144.53
 ---- batch: 080 ----
mean loss: 141.72
 ---- batch: 090 ----
mean loss: 141.36
 ---- batch: 100 ----
mean loss: 137.82
 ---- batch: 110 ----
mean loss: 144.46
train mean loss: 143.27
epoch train time: 0:00:00.549188
elapsed time: 0:02:03.938782
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:28:03.962525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.15
 ---- batch: 020 ----
mean loss: 148.32
 ---- batch: 030 ----
mean loss: 146.72
 ---- batch: 040 ----
mean loss: 140.89
 ---- batch: 050 ----
mean loss: 143.54
 ---- batch: 060 ----
mean loss: 140.76
 ---- batch: 070 ----
mean loss: 152.38
 ---- batch: 080 ----
mean loss: 140.80
 ---- batch: 090 ----
mean loss: 135.72
 ---- batch: 100 ----
mean loss: 143.56
 ---- batch: 110 ----
mean loss: 138.94
train mean loss: 142.70
epoch train time: 0:00:00.551777
elapsed time: 0:02:04.490694
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:28:04.514442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.68
 ---- batch: 020 ----
mean loss: 136.01
 ---- batch: 030 ----
mean loss: 145.83
 ---- batch: 040 ----
mean loss: 140.43
 ---- batch: 050 ----
mean loss: 152.56
 ---- batch: 060 ----
mean loss: 140.28
 ---- batch: 070 ----
mean loss: 143.28
 ---- batch: 080 ----
mean loss: 144.45
 ---- batch: 090 ----
mean loss: 147.13
 ---- batch: 100 ----
mean loss: 143.33
 ---- batch: 110 ----
mean loss: 132.51
train mean loss: 142.77
epoch train time: 0:00:00.551845
elapsed time: 0:02:05.042680
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:28:05.066427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.73
 ---- batch: 020 ----
mean loss: 141.33
 ---- batch: 030 ----
mean loss: 138.00
 ---- batch: 040 ----
mean loss: 139.11
 ---- batch: 050 ----
mean loss: 137.98
 ---- batch: 060 ----
mean loss: 142.82
 ---- batch: 070 ----
mean loss: 131.64
 ---- batch: 080 ----
mean loss: 154.22
 ---- batch: 090 ----
mean loss: 146.60
 ---- batch: 100 ----
mean loss: 155.91
 ---- batch: 110 ----
mean loss: 139.90
train mean loss: 142.12
epoch train time: 0:00:00.557969
elapsed time: 0:02:05.600794
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:28:05.624543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.56
 ---- batch: 020 ----
mean loss: 139.72
 ---- batch: 030 ----
mean loss: 134.80
 ---- batch: 040 ----
mean loss: 136.09
 ---- batch: 050 ----
mean loss: 143.42
 ---- batch: 060 ----
mean loss: 137.49
 ---- batch: 070 ----
mean loss: 140.87
 ---- batch: 080 ----
mean loss: 136.17
 ---- batch: 090 ----
mean loss: 147.58
 ---- batch: 100 ----
mean loss: 149.82
 ---- batch: 110 ----
mean loss: 148.27
train mean loss: 141.60
epoch train time: 0:00:00.548233
elapsed time: 0:02:06.149177
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:28:06.172953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.21
 ---- batch: 020 ----
mean loss: 144.67
 ---- batch: 030 ----
mean loss: 136.43
 ---- batch: 040 ----
mean loss: 152.47
 ---- batch: 050 ----
mean loss: 151.83
 ---- batch: 060 ----
mean loss: 136.34
 ---- batch: 070 ----
mean loss: 133.84
 ---- batch: 080 ----
mean loss: 147.67
 ---- batch: 090 ----
mean loss: 142.17
 ---- batch: 100 ----
mean loss: 139.18
 ---- batch: 110 ----
mean loss: 141.74
train mean loss: 142.10
epoch train time: 0:00:00.548173
elapsed time: 0:02:06.697510
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:28:06.721253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.19
 ---- batch: 020 ----
mean loss: 140.37
 ---- batch: 030 ----
mean loss: 129.01
 ---- batch: 040 ----
mean loss: 150.27
 ---- batch: 050 ----
mean loss: 145.24
 ---- batch: 060 ----
mean loss: 143.85
 ---- batch: 070 ----
mean loss: 140.64
 ---- batch: 080 ----
mean loss: 146.86
 ---- batch: 090 ----
mean loss: 143.63
 ---- batch: 100 ----
mean loss: 147.41
 ---- batch: 110 ----
mean loss: 138.56
train mean loss: 141.62
epoch train time: 0:00:00.549622
elapsed time: 0:02:07.247276
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:28:07.271021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.99
 ---- batch: 020 ----
mean loss: 137.79
 ---- batch: 030 ----
mean loss: 140.51
 ---- batch: 040 ----
mean loss: 145.22
 ---- batch: 050 ----
mean loss: 139.99
 ---- batch: 060 ----
mean loss: 145.88
 ---- batch: 070 ----
mean loss: 139.89
 ---- batch: 080 ----
mean loss: 145.49
 ---- batch: 090 ----
mean loss: 147.28
 ---- batch: 100 ----
mean loss: 138.65
 ---- batch: 110 ----
mean loss: 141.77
train mean loss: 142.50
epoch train time: 0:00:00.563400
elapsed time: 0:02:07.810842
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:28:07.834592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.34
 ---- batch: 020 ----
mean loss: 145.97
 ---- batch: 030 ----
mean loss: 151.40
 ---- batch: 040 ----
mean loss: 136.86
 ---- batch: 050 ----
mean loss: 136.78
 ---- batch: 060 ----
mean loss: 137.43
 ---- batch: 070 ----
mean loss: 149.12
 ---- batch: 080 ----
mean loss: 139.58
 ---- batch: 090 ----
mean loss: 140.84
 ---- batch: 100 ----
mean loss: 140.96
 ---- batch: 110 ----
mean loss: 142.63
train mean loss: 141.77
epoch train time: 0:00:00.554846
elapsed time: 0:02:08.365824
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:28:08.389571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.69
 ---- batch: 020 ----
mean loss: 144.63
 ---- batch: 030 ----
mean loss: 136.63
 ---- batch: 040 ----
mean loss: 133.58
 ---- batch: 050 ----
mean loss: 142.59
 ---- batch: 060 ----
mean loss: 140.59
 ---- batch: 070 ----
mean loss: 140.33
 ---- batch: 080 ----
mean loss: 142.21
 ---- batch: 090 ----
mean loss: 146.49
 ---- batch: 100 ----
mean loss: 144.66
 ---- batch: 110 ----
mean loss: 148.25
train mean loss: 141.74
epoch train time: 0:00:00.554708
elapsed time: 0:02:08.920659
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:28:08.944405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.52
 ---- batch: 020 ----
mean loss: 137.36
 ---- batch: 030 ----
mean loss: 138.94
 ---- batch: 040 ----
mean loss: 141.15
 ---- batch: 050 ----
mean loss: 134.82
 ---- batch: 060 ----
mean loss: 139.91
 ---- batch: 070 ----
mean loss: 145.24
 ---- batch: 080 ----
mean loss: 135.40
 ---- batch: 090 ----
mean loss: 148.72
 ---- batch: 100 ----
mean loss: 133.54
 ---- batch: 110 ----
mean loss: 146.82
train mean loss: 140.62
epoch train time: 0:00:00.567206
elapsed time: 0:02:09.488031
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:28:09.511805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.18
 ---- batch: 020 ----
mean loss: 140.86
 ---- batch: 030 ----
mean loss: 134.25
 ---- batch: 040 ----
mean loss: 144.08
 ---- batch: 050 ----
mean loss: 148.80
 ---- batch: 060 ----
mean loss: 140.67
 ---- batch: 070 ----
mean loss: 140.76
 ---- batch: 080 ----
mean loss: 138.15
 ---- batch: 090 ----
mean loss: 146.55
 ---- batch: 100 ----
mean loss: 144.16
 ---- batch: 110 ----
mean loss: 143.76
train mean loss: 141.13
epoch train time: 0:00:00.562483
elapsed time: 0:02:10.050690
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:28:10.074434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.03
 ---- batch: 020 ----
mean loss: 136.26
 ---- batch: 030 ----
mean loss: 137.39
 ---- batch: 040 ----
mean loss: 140.52
 ---- batch: 050 ----
mean loss: 138.92
 ---- batch: 060 ----
mean loss: 142.51
 ---- batch: 070 ----
mean loss: 149.08
 ---- batch: 080 ----
mean loss: 140.59
 ---- batch: 090 ----
mean loss: 140.17
 ---- batch: 100 ----
mean loss: 138.96
 ---- batch: 110 ----
mean loss: 139.19
train mean loss: 140.90
epoch train time: 0:00:00.553891
elapsed time: 0:02:10.604750
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:28:10.628495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.45
 ---- batch: 020 ----
mean loss: 140.03
 ---- batch: 030 ----
mean loss: 132.81
 ---- batch: 040 ----
mean loss: 148.85
 ---- batch: 050 ----
mean loss: 144.00
 ---- batch: 060 ----
mean loss: 138.53
 ---- batch: 070 ----
mean loss: 138.08
 ---- batch: 080 ----
mean loss: 146.14
 ---- batch: 090 ----
mean loss: 135.33
 ---- batch: 100 ----
mean loss: 143.45
 ---- batch: 110 ----
mean loss: 136.58
train mean loss: 140.16
epoch train time: 0:00:00.553092
elapsed time: 0:02:11.157994
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:28:11.181741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.39
 ---- batch: 020 ----
mean loss: 140.50
 ---- batch: 030 ----
mean loss: 143.34
 ---- batch: 040 ----
mean loss: 136.60
 ---- batch: 050 ----
mean loss: 141.18
 ---- batch: 060 ----
mean loss: 137.08
 ---- batch: 070 ----
mean loss: 136.11
 ---- batch: 080 ----
mean loss: 142.98
 ---- batch: 090 ----
mean loss: 139.60
 ---- batch: 100 ----
mean loss: 146.40
 ---- batch: 110 ----
mean loss: 142.84
train mean loss: 140.46
epoch train time: 0:00:00.555364
elapsed time: 0:02:11.713523
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:28:11.737272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.64
 ---- batch: 020 ----
mean loss: 143.82
 ---- batch: 030 ----
mean loss: 141.58
 ---- batch: 040 ----
mean loss: 133.11
 ---- batch: 050 ----
mean loss: 137.17
 ---- batch: 060 ----
mean loss: 144.00
 ---- batch: 070 ----
mean loss: 134.55
 ---- batch: 080 ----
mean loss: 136.45
 ---- batch: 090 ----
mean loss: 143.45
 ---- batch: 100 ----
mean loss: 136.31
 ---- batch: 110 ----
mean loss: 147.87
train mean loss: 140.04
epoch train time: 0:00:00.557680
elapsed time: 0:02:12.271342
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:28:12.295089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.14
 ---- batch: 020 ----
mean loss: 135.93
 ---- batch: 030 ----
mean loss: 135.92
 ---- batch: 040 ----
mean loss: 143.94
 ---- batch: 050 ----
mean loss: 140.23
 ---- batch: 060 ----
mean loss: 136.33
 ---- batch: 070 ----
mean loss: 144.70
 ---- batch: 080 ----
mean loss: 142.03
 ---- batch: 090 ----
mean loss: 138.58
 ---- batch: 100 ----
mean loss: 139.08
 ---- batch: 110 ----
mean loss: 147.02
train mean loss: 139.94
epoch train time: 0:00:00.557155
elapsed time: 0:02:12.828643
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:28:12.852415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.98
 ---- batch: 020 ----
mean loss: 144.60
 ---- batch: 030 ----
mean loss: 134.42
 ---- batch: 040 ----
mean loss: 131.09
 ---- batch: 050 ----
mean loss: 147.36
 ---- batch: 060 ----
mean loss: 136.08
 ---- batch: 070 ----
mean loss: 146.48
 ---- batch: 080 ----
mean loss: 139.19
 ---- batch: 090 ----
mean loss: 133.68
 ---- batch: 100 ----
mean loss: 145.87
 ---- batch: 110 ----
mean loss: 139.40
train mean loss: 139.02
epoch train time: 0:00:00.558228
elapsed time: 0:02:13.387028
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:28:13.410791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.67
 ---- batch: 020 ----
mean loss: 139.21
 ---- batch: 030 ----
mean loss: 150.39
 ---- batch: 040 ----
mean loss: 129.18
 ---- batch: 050 ----
mean loss: 139.69
 ---- batch: 060 ----
mean loss: 134.06
 ---- batch: 070 ----
mean loss: 137.05
 ---- batch: 080 ----
mean loss: 142.84
 ---- batch: 090 ----
mean loss: 139.54
 ---- batch: 100 ----
mean loss: 147.10
 ---- batch: 110 ----
mean loss: 143.04
train mean loss: 139.54
epoch train time: 0:00:00.554758
elapsed time: 0:02:13.941972
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:28:13.965751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.80
 ---- batch: 020 ----
mean loss: 144.74
 ---- batch: 030 ----
mean loss: 141.32
 ---- batch: 040 ----
mean loss: 145.27
 ---- batch: 050 ----
mean loss: 143.34
 ---- batch: 060 ----
mean loss: 144.42
 ---- batch: 070 ----
mean loss: 138.56
 ---- batch: 080 ----
mean loss: 135.28
 ---- batch: 090 ----
mean loss: 140.82
 ---- batch: 100 ----
mean loss: 143.08
 ---- batch: 110 ----
mean loss: 138.46
train mean loss: 141.41
epoch train time: 0:00:00.556331
elapsed time: 0:02:14.498468
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:28:14.522215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.52
 ---- batch: 020 ----
mean loss: 139.40
 ---- batch: 030 ----
mean loss: 128.04
 ---- batch: 040 ----
mean loss: 141.26
 ---- batch: 050 ----
mean loss: 130.16
 ---- batch: 060 ----
mean loss: 132.34
 ---- batch: 070 ----
mean loss: 143.82
 ---- batch: 080 ----
mean loss: 142.98
 ---- batch: 090 ----
mean loss: 153.74
 ---- batch: 100 ----
mean loss: 140.65
 ---- batch: 110 ----
mean loss: 139.11
train mean loss: 139.49
epoch train time: 0:00:00.547898
elapsed time: 0:02:15.046498
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:28:15.070243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.52
 ---- batch: 020 ----
mean loss: 143.43
 ---- batch: 030 ----
mean loss: 140.53
 ---- batch: 040 ----
mean loss: 137.78
 ---- batch: 050 ----
mean loss: 141.31
 ---- batch: 060 ----
mean loss: 138.10
 ---- batch: 070 ----
mean loss: 138.44
 ---- batch: 080 ----
mean loss: 142.32
 ---- batch: 090 ----
mean loss: 132.72
 ---- batch: 100 ----
mean loss: 143.84
 ---- batch: 110 ----
mean loss: 148.51
train mean loss: 139.84
epoch train time: 0:00:00.545737
elapsed time: 0:02:15.592377
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:28:15.616125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.00
 ---- batch: 020 ----
mean loss: 134.15
 ---- batch: 030 ----
mean loss: 144.88
 ---- batch: 040 ----
mean loss: 134.37
 ---- batch: 050 ----
mean loss: 138.34
 ---- batch: 060 ----
mean loss: 143.01
 ---- batch: 070 ----
mean loss: 142.14
 ---- batch: 080 ----
mean loss: 133.51
 ---- batch: 090 ----
mean loss: 135.36
 ---- batch: 100 ----
mean loss: 142.08
 ---- batch: 110 ----
mean loss: 145.95
train mean loss: 138.90
epoch train time: 0:00:00.560854
elapsed time: 0:02:16.153364
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:28:16.177108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.87
 ---- batch: 020 ----
mean loss: 146.62
 ---- batch: 030 ----
mean loss: 144.68
 ---- batch: 040 ----
mean loss: 141.33
 ---- batch: 050 ----
mean loss: 138.45
 ---- batch: 060 ----
mean loss: 141.12
 ---- batch: 070 ----
mean loss: 139.50
 ---- batch: 080 ----
mean loss: 137.34
 ---- batch: 090 ----
mean loss: 137.06
 ---- batch: 100 ----
mean loss: 133.98
 ---- batch: 110 ----
mean loss: 138.50
train mean loss: 139.53
epoch train time: 0:00:00.564077
elapsed time: 0:02:16.717570
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:28:16.741333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.97
 ---- batch: 020 ----
mean loss: 133.46
 ---- batch: 030 ----
mean loss: 133.33
 ---- batch: 040 ----
mean loss: 145.48
 ---- batch: 050 ----
mean loss: 144.18
 ---- batch: 060 ----
mean loss: 150.67
 ---- batch: 070 ----
mean loss: 129.58
 ---- batch: 080 ----
mean loss: 130.25
 ---- batch: 090 ----
mean loss: 139.75
 ---- batch: 100 ----
mean loss: 140.86
 ---- batch: 110 ----
mean loss: 141.76
train mean loss: 138.55
epoch train time: 0:00:00.553768
elapsed time: 0:02:17.271500
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:28:17.295246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.05
 ---- batch: 020 ----
mean loss: 136.64
 ---- batch: 030 ----
mean loss: 141.63
 ---- batch: 040 ----
mean loss: 129.49
 ---- batch: 050 ----
mean loss: 141.90
 ---- batch: 060 ----
mean loss: 136.98
 ---- batch: 070 ----
mean loss: 135.13
 ---- batch: 080 ----
mean loss: 139.89
 ---- batch: 090 ----
mean loss: 143.98
 ---- batch: 100 ----
mean loss: 138.57
 ---- batch: 110 ----
mean loss: 134.21
train mean loss: 137.58
epoch train time: 0:00:00.567324
elapsed time: 0:02:17.838986
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:28:17.862737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.45
 ---- batch: 020 ----
mean loss: 140.02
 ---- batch: 030 ----
mean loss: 149.66
 ---- batch: 040 ----
mean loss: 140.96
 ---- batch: 050 ----
mean loss: 133.20
 ---- batch: 060 ----
mean loss: 136.70
 ---- batch: 070 ----
mean loss: 131.31
 ---- batch: 080 ----
mean loss: 141.49
 ---- batch: 090 ----
mean loss: 136.26
 ---- batch: 100 ----
mean loss: 132.51
 ---- batch: 110 ----
mean loss: 138.41
train mean loss: 138.16
epoch train time: 0:00:00.566277
elapsed time: 0:02:18.405402
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:28:18.429148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.54
 ---- batch: 020 ----
mean loss: 130.19
 ---- batch: 030 ----
mean loss: 138.35
 ---- batch: 040 ----
mean loss: 136.21
 ---- batch: 050 ----
mean loss: 133.97
 ---- batch: 060 ----
mean loss: 144.22
 ---- batch: 070 ----
mean loss: 148.64
 ---- batch: 080 ----
mean loss: 136.33
 ---- batch: 090 ----
mean loss: 138.47
 ---- batch: 100 ----
mean loss: 140.97
 ---- batch: 110 ----
mean loss: 148.44
train mean loss: 138.71
epoch train time: 0:00:00.555334
elapsed time: 0:02:18.960886
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:28:18.984633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.17
 ---- batch: 020 ----
mean loss: 129.41
 ---- batch: 030 ----
mean loss: 130.63
 ---- batch: 040 ----
mean loss: 138.54
 ---- batch: 050 ----
mean loss: 138.91
 ---- batch: 060 ----
mean loss: 143.94
 ---- batch: 070 ----
mean loss: 135.95
 ---- batch: 080 ----
mean loss: 136.56
 ---- batch: 090 ----
mean loss: 135.25
 ---- batch: 100 ----
mean loss: 144.05
 ---- batch: 110 ----
mean loss: 137.92
train mean loss: 136.94
epoch train time: 0:00:00.556964
elapsed time: 0:02:19.518010
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:28:19.541762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.21
 ---- batch: 020 ----
mean loss: 142.36
 ---- batch: 030 ----
mean loss: 144.47
 ---- batch: 040 ----
mean loss: 138.56
 ---- batch: 050 ----
mean loss: 140.80
 ---- batch: 060 ----
mean loss: 140.55
 ---- batch: 070 ----
mean loss: 130.68
 ---- batch: 080 ----
mean loss: 135.61
 ---- batch: 090 ----
mean loss: 129.13
 ---- batch: 100 ----
mean loss: 135.11
 ---- batch: 110 ----
mean loss: 139.81
train mean loss: 138.55
epoch train time: 0:00:00.571143
elapsed time: 0:02:20.089304
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:28:20.113050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.64
 ---- batch: 020 ----
mean loss: 134.14
 ---- batch: 030 ----
mean loss: 132.90
 ---- batch: 040 ----
mean loss: 140.35
 ---- batch: 050 ----
mean loss: 136.50
 ---- batch: 060 ----
mean loss: 134.93
 ---- batch: 070 ----
mean loss: 128.43
 ---- batch: 080 ----
mean loss: 133.97
 ---- batch: 090 ----
mean loss: 134.35
 ---- batch: 100 ----
mean loss: 150.96
 ---- batch: 110 ----
mean loss: 149.18
train mean loss: 137.39
epoch train time: 0:00:00.558934
elapsed time: 0:02:20.648371
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:28:20.672136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.66
 ---- batch: 020 ----
mean loss: 151.80
 ---- batch: 030 ----
mean loss: 139.02
 ---- batch: 040 ----
mean loss: 138.78
 ---- batch: 050 ----
mean loss: 133.50
 ---- batch: 060 ----
mean loss: 129.62
 ---- batch: 070 ----
mean loss: 143.28
 ---- batch: 080 ----
mean loss: 136.28
 ---- batch: 090 ----
mean loss: 137.20
 ---- batch: 100 ----
mean loss: 142.71
 ---- batch: 110 ----
mean loss: 136.53
train mean loss: 137.81
epoch train time: 0:00:00.551355
elapsed time: 0:02:21.199875
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:28:21.223618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.58
 ---- batch: 020 ----
mean loss: 140.54
 ---- batch: 030 ----
mean loss: 146.07
 ---- batch: 040 ----
mean loss: 138.15
 ---- batch: 050 ----
mean loss: 128.37
 ---- batch: 060 ----
mean loss: 132.62
 ---- batch: 070 ----
mean loss: 139.70
 ---- batch: 080 ----
mean loss: 130.53
 ---- batch: 090 ----
mean loss: 141.73
 ---- batch: 100 ----
mean loss: 136.47
 ---- batch: 110 ----
mean loss: 139.89
train mean loss: 137.03
epoch train time: 0:00:00.553518
elapsed time: 0:02:21.753529
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:28:21.777274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.36
 ---- batch: 020 ----
mean loss: 131.39
 ---- batch: 030 ----
mean loss: 131.09
 ---- batch: 040 ----
mean loss: 145.67
 ---- batch: 050 ----
mean loss: 131.19
 ---- batch: 060 ----
mean loss: 133.57
 ---- batch: 070 ----
mean loss: 140.39
 ---- batch: 080 ----
mean loss: 135.50
 ---- batch: 090 ----
mean loss: 138.38
 ---- batch: 100 ----
mean loss: 132.45
 ---- batch: 110 ----
mean loss: 144.74
train mean loss: 136.87
epoch train time: 0:00:00.547888
elapsed time: 0:02:22.301548
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:28:22.325295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.49
 ---- batch: 020 ----
mean loss: 135.99
 ---- batch: 030 ----
mean loss: 137.53
 ---- batch: 040 ----
mean loss: 132.24
 ---- batch: 050 ----
mean loss: 138.34
 ---- batch: 060 ----
mean loss: 130.75
 ---- batch: 070 ----
mean loss: 149.47
 ---- batch: 080 ----
mean loss: 142.48
 ---- batch: 090 ----
mean loss: 139.13
 ---- batch: 100 ----
mean loss: 139.19
 ---- batch: 110 ----
mean loss: 132.33
train mean loss: 137.35
epoch train time: 0:00:00.563902
elapsed time: 0:02:22.865583
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:28:22.889330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.91
 ---- batch: 020 ----
mean loss: 130.34
 ---- batch: 030 ----
mean loss: 137.64
 ---- batch: 040 ----
mean loss: 134.90
 ---- batch: 050 ----
mean loss: 136.97
 ---- batch: 060 ----
mean loss: 146.16
 ---- batch: 070 ----
mean loss: 130.07
 ---- batch: 080 ----
mean loss: 140.33
 ---- batch: 090 ----
mean loss: 135.20
 ---- batch: 100 ----
mean loss: 134.74
 ---- batch: 110 ----
mean loss: 143.16
train mean loss: 137.23
epoch train time: 0:00:00.545648
elapsed time: 0:02:23.411389
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:28:23.435153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.32
 ---- batch: 020 ----
mean loss: 131.71
 ---- batch: 030 ----
mean loss: 143.05
 ---- batch: 040 ----
mean loss: 126.66
 ---- batch: 050 ----
mean loss: 135.03
 ---- batch: 060 ----
mean loss: 140.14
 ---- batch: 070 ----
mean loss: 140.01
 ---- batch: 080 ----
mean loss: 138.48
 ---- batch: 090 ----
mean loss: 133.44
 ---- batch: 100 ----
mean loss: 133.44
 ---- batch: 110 ----
mean loss: 143.30
train mean loss: 136.73
epoch train time: 0:00:00.565537
elapsed time: 0:02:23.977080
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:28:24.000847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.86
 ---- batch: 020 ----
mean loss: 130.09
 ---- batch: 030 ----
mean loss: 131.43
 ---- batch: 040 ----
mean loss: 133.94
 ---- batch: 050 ----
mean loss: 137.98
 ---- batch: 060 ----
mean loss: 140.48
 ---- batch: 070 ----
mean loss: 136.78
 ---- batch: 080 ----
mean loss: 127.89
 ---- batch: 090 ----
mean loss: 142.77
 ---- batch: 100 ----
mean loss: 135.78
 ---- batch: 110 ----
mean loss: 147.43
train mean loss: 136.16
epoch train time: 0:00:00.551210
elapsed time: 0:02:24.528451
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:28:24.552216
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.41
 ---- batch: 020 ----
mean loss: 129.73
 ---- batch: 030 ----
mean loss: 141.56
 ---- batch: 040 ----
mean loss: 127.88
 ---- batch: 050 ----
mean loss: 124.45
 ---- batch: 060 ----
mean loss: 127.30
 ---- batch: 070 ----
mean loss: 124.77
 ---- batch: 080 ----
mean loss: 130.69
 ---- batch: 090 ----
mean loss: 131.89
 ---- batch: 100 ----
mean loss: 132.10
 ---- batch: 110 ----
mean loss: 131.27
train mean loss: 130.12
epoch train time: 0:00:00.547605
elapsed time: 0:02:25.076237
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:28:25.099973
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.48
 ---- batch: 020 ----
mean loss: 129.05
 ---- batch: 030 ----
mean loss: 120.86
 ---- batch: 040 ----
mean loss: 132.68
 ---- batch: 050 ----
mean loss: 127.90
 ---- batch: 060 ----
mean loss: 130.15
 ---- batch: 070 ----
mean loss: 125.71
 ---- batch: 080 ----
mean loss: 134.28
 ---- batch: 090 ----
mean loss: 131.29
 ---- batch: 100 ----
mean loss: 128.71
 ---- batch: 110 ----
mean loss: 123.81
train mean loss: 129.31
epoch train time: 0:00:00.544547
elapsed time: 0:02:25.620908
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:28:25.644654
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.33
 ---- batch: 020 ----
mean loss: 131.87
 ---- batch: 030 ----
mean loss: 137.29
 ---- batch: 040 ----
mean loss: 123.09
 ---- batch: 050 ----
mean loss: 131.34
 ---- batch: 060 ----
mean loss: 129.39
 ---- batch: 070 ----
mean loss: 125.21
 ---- batch: 080 ----
mean loss: 133.68
 ---- batch: 090 ----
mean loss: 130.10
 ---- batch: 100 ----
mean loss: 125.69
 ---- batch: 110 ----
mean loss: 126.94
train mean loss: 128.97
epoch train time: 0:00:00.547104
elapsed time: 0:02:26.168168
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:28:26.191933
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.36
 ---- batch: 020 ----
mean loss: 124.39
 ---- batch: 030 ----
mean loss: 126.38
 ---- batch: 040 ----
mean loss: 123.51
 ---- batch: 050 ----
mean loss: 133.48
 ---- batch: 060 ----
mean loss: 130.34
 ---- batch: 070 ----
mean loss: 137.52
 ---- batch: 080 ----
mean loss: 128.08
 ---- batch: 090 ----
mean loss: 135.58
 ---- batch: 100 ----
mean loss: 122.00
 ---- batch: 110 ----
mean loss: 126.53
train mean loss: 128.84
epoch train time: 0:00:00.544205
elapsed time: 0:02:26.712538
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:28:26.736281
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.76
 ---- batch: 020 ----
mean loss: 130.57
 ---- batch: 030 ----
mean loss: 126.58
 ---- batch: 040 ----
mean loss: 126.50
 ---- batch: 050 ----
mean loss: 126.77
 ---- batch: 060 ----
mean loss: 128.94
 ---- batch: 070 ----
mean loss: 129.36
 ---- batch: 080 ----
mean loss: 135.21
 ---- batch: 090 ----
mean loss: 121.80
 ---- batch: 100 ----
mean loss: 126.86
 ---- batch: 110 ----
mean loss: 130.03
train mean loss: 128.88
epoch train time: 0:00:00.548291
elapsed time: 0:02:27.260959
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:28:27.284706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.52
 ---- batch: 020 ----
mean loss: 129.23
 ---- batch: 030 ----
mean loss: 126.04
 ---- batch: 040 ----
mean loss: 134.48
 ---- batch: 050 ----
mean loss: 121.64
 ---- batch: 060 ----
mean loss: 134.47
 ---- batch: 070 ----
mean loss: 127.67
 ---- batch: 080 ----
mean loss: 130.18
 ---- batch: 090 ----
mean loss: 130.95
 ---- batch: 100 ----
mean loss: 121.03
 ---- batch: 110 ----
mean loss: 131.22
train mean loss: 128.76
epoch train time: 0:00:00.556338
elapsed time: 0:02:27.817431
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:28:27.841226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.76
 ---- batch: 020 ----
mean loss: 126.45
 ---- batch: 030 ----
mean loss: 130.83
 ---- batch: 040 ----
mean loss: 137.63
 ---- batch: 050 ----
mean loss: 126.59
 ---- batch: 060 ----
mean loss: 130.79
 ---- batch: 070 ----
mean loss: 123.06
 ---- batch: 080 ----
mean loss: 133.01
 ---- batch: 090 ----
mean loss: 126.84
 ---- batch: 100 ----
mean loss: 132.87
 ---- batch: 110 ----
mean loss: 129.38
train mean loss: 128.66
epoch train time: 0:00:00.546845
elapsed time: 0:02:28.364453
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:28:28.388196
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.73
 ---- batch: 020 ----
mean loss: 125.65
 ---- batch: 030 ----
mean loss: 124.71
 ---- batch: 040 ----
mean loss: 126.06
 ---- batch: 050 ----
mean loss: 131.00
 ---- batch: 060 ----
mean loss: 137.44
 ---- batch: 070 ----
mean loss: 133.53
 ---- batch: 080 ----
mean loss: 128.65
 ---- batch: 090 ----
mean loss: 131.61
 ---- batch: 100 ----
mean loss: 125.42
 ---- batch: 110 ----
mean loss: 129.62
train mean loss: 128.66
epoch train time: 0:00:00.551784
elapsed time: 0:02:28.916376
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:28:28.940139
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.01
 ---- batch: 020 ----
mean loss: 121.53
 ---- batch: 030 ----
mean loss: 133.81
 ---- batch: 040 ----
mean loss: 127.05
 ---- batch: 050 ----
mean loss: 125.44
 ---- batch: 060 ----
mean loss: 126.26
 ---- batch: 070 ----
mean loss: 129.76
 ---- batch: 080 ----
mean loss: 122.01
 ---- batch: 090 ----
mean loss: 132.61
 ---- batch: 100 ----
mean loss: 129.08
 ---- batch: 110 ----
mean loss: 139.59
train mean loss: 128.57
epoch train time: 0:00:00.545547
elapsed time: 0:02:29.462070
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:28:29.485829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.47
 ---- batch: 020 ----
mean loss: 125.26
 ---- batch: 030 ----
mean loss: 125.72
 ---- batch: 040 ----
mean loss: 133.54
 ---- batch: 050 ----
mean loss: 132.81
 ---- batch: 060 ----
mean loss: 130.99
 ---- batch: 070 ----
mean loss: 122.67
 ---- batch: 080 ----
mean loss: 124.74
 ---- batch: 090 ----
mean loss: 125.73
 ---- batch: 100 ----
mean loss: 126.75
 ---- batch: 110 ----
mean loss: 127.96
train mean loss: 128.56
epoch train time: 0:00:00.559169
elapsed time: 0:02:30.021385
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:28:30.045145
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.24
 ---- batch: 020 ----
mean loss: 133.02
 ---- batch: 030 ----
mean loss: 133.21
 ---- batch: 040 ----
mean loss: 126.89
 ---- batch: 050 ----
mean loss: 131.18
 ---- batch: 060 ----
mean loss: 128.74
 ---- batch: 070 ----
mean loss: 123.73
 ---- batch: 080 ----
mean loss: 125.21
 ---- batch: 090 ----
mean loss: 133.31
 ---- batch: 100 ----
mean loss: 121.08
 ---- batch: 110 ----
mean loss: 125.17
train mean loss: 128.68
epoch train time: 0:00:00.545871
elapsed time: 0:02:30.567400
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:28:30.591166
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.26
 ---- batch: 020 ----
mean loss: 122.80
 ---- batch: 030 ----
mean loss: 133.95
 ---- batch: 040 ----
mean loss: 132.33
 ---- batch: 050 ----
mean loss: 128.79
 ---- batch: 060 ----
mean loss: 130.04
 ---- batch: 070 ----
mean loss: 126.42
 ---- batch: 080 ----
mean loss: 129.99
 ---- batch: 090 ----
mean loss: 128.95
 ---- batch: 100 ----
mean loss: 129.93
 ---- batch: 110 ----
mean loss: 126.60
train mean loss: 128.52
epoch train time: 0:00:00.554762
elapsed time: 0:02:31.122318
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:28:31.146062
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.67
 ---- batch: 020 ----
mean loss: 128.34
 ---- batch: 030 ----
mean loss: 115.80
 ---- batch: 040 ----
mean loss: 138.00
 ---- batch: 050 ----
mean loss: 130.42
 ---- batch: 060 ----
mean loss: 134.48
 ---- batch: 070 ----
mean loss: 121.88
 ---- batch: 080 ----
mean loss: 130.09
 ---- batch: 090 ----
mean loss: 119.77
 ---- batch: 100 ----
mean loss: 132.41
 ---- batch: 110 ----
mean loss: 133.52
train mean loss: 128.49
epoch train time: 0:00:00.554040
elapsed time: 0:02:31.676520
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:28:31.700283
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.29
 ---- batch: 020 ----
mean loss: 127.26
 ---- batch: 030 ----
mean loss: 128.59
 ---- batch: 040 ----
mean loss: 122.69
 ---- batch: 050 ----
mean loss: 128.82
 ---- batch: 060 ----
mean loss: 125.62
 ---- batch: 070 ----
mean loss: 134.55
 ---- batch: 080 ----
mean loss: 126.93
 ---- batch: 090 ----
mean loss: 127.75
 ---- batch: 100 ----
mean loss: 131.40
 ---- batch: 110 ----
mean loss: 135.67
train mean loss: 128.57
epoch train time: 0:00:00.551856
elapsed time: 0:02:32.228533
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:28:32.252295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.50
 ---- batch: 020 ----
mean loss: 125.63
 ---- batch: 030 ----
mean loss: 128.62
 ---- batch: 040 ----
mean loss: 132.54
 ---- batch: 050 ----
mean loss: 126.90
 ---- batch: 060 ----
mean loss: 127.72
 ---- batch: 070 ----
mean loss: 132.71
 ---- batch: 080 ----
mean loss: 122.95
 ---- batch: 090 ----
mean loss: 122.14
 ---- batch: 100 ----
mean loss: 127.06
 ---- batch: 110 ----
mean loss: 130.35
train mean loss: 128.59
epoch train time: 0:00:00.557914
elapsed time: 0:02:32.786607
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:28:32.810355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.47
 ---- batch: 020 ----
mean loss: 129.05
 ---- batch: 030 ----
mean loss: 127.62
 ---- batch: 040 ----
mean loss: 127.89
 ---- batch: 050 ----
mean loss: 125.05
 ---- batch: 060 ----
mean loss: 127.47
 ---- batch: 070 ----
mean loss: 124.89
 ---- batch: 080 ----
mean loss: 131.97
 ---- batch: 090 ----
mean loss: 129.50
 ---- batch: 100 ----
mean loss: 126.63
 ---- batch: 110 ----
mean loss: 128.25
train mean loss: 128.42
epoch train time: 0:00:00.560141
elapsed time: 0:02:33.346881
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:28:33.370625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.84
 ---- batch: 020 ----
mean loss: 126.80
 ---- batch: 030 ----
mean loss: 130.30
 ---- batch: 040 ----
mean loss: 128.61
 ---- batch: 050 ----
mean loss: 125.52
 ---- batch: 060 ----
mean loss: 130.33
 ---- batch: 070 ----
mean loss: 125.20
 ---- batch: 080 ----
mean loss: 131.65
 ---- batch: 090 ----
mean loss: 131.67
 ---- batch: 100 ----
mean loss: 131.24
 ---- batch: 110 ----
mean loss: 126.58
train mean loss: 128.42
epoch train time: 0:00:00.556838
elapsed time: 0:02:33.903863
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:28:33.927608
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.03
 ---- batch: 020 ----
mean loss: 130.10
 ---- batch: 030 ----
mean loss: 130.20
 ---- batch: 040 ----
mean loss: 129.94
 ---- batch: 050 ----
mean loss: 127.91
 ---- batch: 060 ----
mean loss: 126.76
 ---- batch: 070 ----
mean loss: 128.82
 ---- batch: 080 ----
mean loss: 125.44
 ---- batch: 090 ----
mean loss: 128.93
 ---- batch: 100 ----
mean loss: 133.40
 ---- batch: 110 ----
mean loss: 124.41
train mean loss: 128.38
epoch train time: 0:00:00.544778
elapsed time: 0:02:34.448804
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:28:34.472599
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.61
 ---- batch: 020 ----
mean loss: 124.10
 ---- batch: 030 ----
mean loss: 125.16
 ---- batch: 040 ----
mean loss: 125.91
 ---- batch: 050 ----
mean loss: 133.34
 ---- batch: 060 ----
mean loss: 121.17
 ---- batch: 070 ----
mean loss: 127.84
 ---- batch: 080 ----
mean loss: 138.09
 ---- batch: 090 ----
mean loss: 130.69
 ---- batch: 100 ----
mean loss: 126.79
 ---- batch: 110 ----
mean loss: 131.71
train mean loss: 128.36
epoch train time: 0:00:00.545735
elapsed time: 0:02:34.994724
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:28:35.018495
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.83
 ---- batch: 020 ----
mean loss: 131.09
 ---- batch: 030 ----
mean loss: 133.18
 ---- batch: 040 ----
mean loss: 126.01
 ---- batch: 050 ----
mean loss: 134.07
 ---- batch: 060 ----
mean loss: 123.51
 ---- batch: 070 ----
mean loss: 125.99
 ---- batch: 080 ----
mean loss: 126.00
 ---- batch: 090 ----
mean loss: 129.93
 ---- batch: 100 ----
mean loss: 132.49
 ---- batch: 110 ----
mean loss: 131.38
train mean loss: 128.30
epoch train time: 0:00:00.540987
elapsed time: 0:02:35.535883
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:28:35.559628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.87
 ---- batch: 020 ----
mean loss: 123.17
 ---- batch: 030 ----
mean loss: 129.85
 ---- batch: 040 ----
mean loss: 129.30
 ---- batch: 050 ----
mean loss: 126.29
 ---- batch: 060 ----
mean loss: 126.94
 ---- batch: 070 ----
mean loss: 133.93
 ---- batch: 080 ----
mean loss: 137.08
 ---- batch: 090 ----
mean loss: 127.44
 ---- batch: 100 ----
mean loss: 126.64
 ---- batch: 110 ----
mean loss: 129.71
train mean loss: 128.35
epoch train time: 0:00:00.552992
elapsed time: 0:02:36.089004
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:28:36.112747
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.37
 ---- batch: 020 ----
mean loss: 129.93
 ---- batch: 030 ----
mean loss: 135.74
 ---- batch: 040 ----
mean loss: 123.62
 ---- batch: 050 ----
mean loss: 127.70
 ---- batch: 060 ----
mean loss: 132.29
 ---- batch: 070 ----
mean loss: 126.44
 ---- batch: 080 ----
mean loss: 123.52
 ---- batch: 090 ----
mean loss: 125.30
 ---- batch: 100 ----
mean loss: 128.91
 ---- batch: 110 ----
mean loss: 123.69
train mean loss: 128.39
epoch train time: 0:00:00.541637
elapsed time: 0:02:36.630781
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:28:36.654546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.34
 ---- batch: 020 ----
mean loss: 131.30
 ---- batch: 030 ----
mean loss: 122.75
 ---- batch: 040 ----
mean loss: 127.17
 ---- batch: 050 ----
mean loss: 135.23
 ---- batch: 060 ----
mean loss: 130.18
 ---- batch: 070 ----
mean loss: 128.56
 ---- batch: 080 ----
mean loss: 126.82
 ---- batch: 090 ----
mean loss: 129.74
 ---- batch: 100 ----
mean loss: 128.87
 ---- batch: 110 ----
mean loss: 124.42
train mean loss: 128.22
epoch train time: 0:00:00.564467
elapsed time: 0:02:37.195399
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:28:37.219143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.31
 ---- batch: 020 ----
mean loss: 129.61
 ---- batch: 030 ----
mean loss: 126.62
 ---- batch: 040 ----
mean loss: 126.46
 ---- batch: 050 ----
mean loss: 130.80
 ---- batch: 060 ----
mean loss: 129.02
 ---- batch: 070 ----
mean loss: 129.02
 ---- batch: 080 ----
mean loss: 133.92
 ---- batch: 090 ----
mean loss: 133.12
 ---- batch: 100 ----
mean loss: 118.25
 ---- batch: 110 ----
mean loss: 130.89
train mean loss: 128.36
epoch train time: 0:00:00.555139
elapsed time: 0:02:37.750666
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:28:37.774416
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.26
 ---- batch: 020 ----
mean loss: 124.85
 ---- batch: 030 ----
mean loss: 133.52
 ---- batch: 040 ----
mean loss: 130.68
 ---- batch: 050 ----
mean loss: 122.17
 ---- batch: 060 ----
mean loss: 131.79
 ---- batch: 070 ----
mean loss: 122.62
 ---- batch: 080 ----
mean loss: 122.70
 ---- batch: 090 ----
mean loss: 128.94
 ---- batch: 100 ----
mean loss: 133.02
 ---- batch: 110 ----
mean loss: 127.46
train mean loss: 128.34
epoch train time: 0:00:00.554583
elapsed time: 0:02:38.305382
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:28:38.329127
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.90
 ---- batch: 020 ----
mean loss: 136.87
 ---- batch: 030 ----
mean loss: 129.12
 ---- batch: 040 ----
mean loss: 124.66
 ---- batch: 050 ----
mean loss: 124.50
 ---- batch: 060 ----
mean loss: 120.15
 ---- batch: 070 ----
mean loss: 137.64
 ---- batch: 080 ----
mean loss: 122.82
 ---- batch: 090 ----
mean loss: 134.73
 ---- batch: 100 ----
mean loss: 129.16
 ---- batch: 110 ----
mean loss: 129.28
train mean loss: 128.05
epoch train time: 0:00:00.551466
elapsed time: 0:02:38.857026
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:28:38.880788
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.90
 ---- batch: 020 ----
mean loss: 126.67
 ---- batch: 030 ----
mean loss: 126.59
 ---- batch: 040 ----
mean loss: 123.89
 ---- batch: 050 ----
mean loss: 123.05
 ---- batch: 060 ----
mean loss: 133.41
 ---- batch: 070 ----
mean loss: 133.99
 ---- batch: 080 ----
mean loss: 133.70
 ---- batch: 090 ----
mean loss: 129.42
 ---- batch: 100 ----
mean loss: 128.67
 ---- batch: 110 ----
mean loss: 130.41
train mean loss: 128.06
epoch train time: 0:00:00.560891
elapsed time: 0:02:39.418065
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:28:39.441811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.98
 ---- batch: 020 ----
mean loss: 123.87
 ---- batch: 030 ----
mean loss: 127.72
 ---- batch: 040 ----
mean loss: 133.71
 ---- batch: 050 ----
mean loss: 125.88
 ---- batch: 060 ----
mean loss: 126.74
 ---- batch: 070 ----
mean loss: 132.31
 ---- batch: 080 ----
mean loss: 128.08
 ---- batch: 090 ----
mean loss: 125.06
 ---- batch: 100 ----
mean loss: 124.74
 ---- batch: 110 ----
mean loss: 130.04
train mean loss: 128.01
epoch train time: 0:00:00.556269
elapsed time: 0:02:39.974471
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:28:39.998220
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.79
 ---- batch: 020 ----
mean loss: 131.16
 ---- batch: 030 ----
mean loss: 135.46
 ---- batch: 040 ----
mean loss: 130.04
 ---- batch: 050 ----
mean loss: 126.09
 ---- batch: 060 ----
mean loss: 131.29
 ---- batch: 070 ----
mean loss: 126.91
 ---- batch: 080 ----
mean loss: 118.56
 ---- batch: 090 ----
mean loss: 130.04
 ---- batch: 100 ----
mean loss: 128.11
 ---- batch: 110 ----
mean loss: 129.72
train mean loss: 128.01
epoch train time: 0:00:00.558126
elapsed time: 0:02:40.532733
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:28:40.556478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.14
 ---- batch: 020 ----
mean loss: 126.65
 ---- batch: 030 ----
mean loss: 123.43
 ---- batch: 040 ----
mean loss: 140.35
 ---- batch: 050 ----
mean loss: 126.59
 ---- batch: 060 ----
mean loss: 126.07
 ---- batch: 070 ----
mean loss: 124.00
 ---- batch: 080 ----
mean loss: 130.39
 ---- batch: 090 ----
mean loss: 124.02
 ---- batch: 100 ----
mean loss: 125.01
 ---- batch: 110 ----
mean loss: 131.95
train mean loss: 128.22
epoch train time: 0:00:00.553855
elapsed time: 0:02:41.086721
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:28:41.110469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.41
 ---- batch: 020 ----
mean loss: 120.09
 ---- batch: 030 ----
mean loss: 124.64
 ---- batch: 040 ----
mean loss: 136.44
 ---- batch: 050 ----
mean loss: 130.55
 ---- batch: 060 ----
mean loss: 120.66
 ---- batch: 070 ----
mean loss: 127.71
 ---- batch: 080 ----
mean loss: 129.55
 ---- batch: 090 ----
mean loss: 126.71
 ---- batch: 100 ----
mean loss: 139.88
 ---- batch: 110 ----
mean loss: 134.21
train mean loss: 128.01
epoch train time: 0:00:00.557291
elapsed time: 0:02:41.644149
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:28:41.667919
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.17
 ---- batch: 020 ----
mean loss: 123.43
 ---- batch: 030 ----
mean loss: 130.00
 ---- batch: 040 ----
mean loss: 131.10
 ---- batch: 050 ----
mean loss: 119.35
 ---- batch: 060 ----
mean loss: 136.27
 ---- batch: 070 ----
mean loss: 128.87
 ---- batch: 080 ----
mean loss: 127.57
 ---- batch: 090 ----
mean loss: 128.22
 ---- batch: 100 ----
mean loss: 123.67
 ---- batch: 110 ----
mean loss: 128.67
train mean loss: 128.02
epoch train time: 0:00:00.554755
elapsed time: 0:02:42.199056
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:28:42.222817
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.27
 ---- batch: 020 ----
mean loss: 122.23
 ---- batch: 030 ----
mean loss: 125.98
 ---- batch: 040 ----
mean loss: 134.32
 ---- batch: 050 ----
mean loss: 127.81
 ---- batch: 060 ----
mean loss: 131.63
 ---- batch: 070 ----
mean loss: 130.15
 ---- batch: 080 ----
mean loss: 126.67
 ---- batch: 090 ----
mean loss: 127.40
 ---- batch: 100 ----
mean loss: 128.59
 ---- batch: 110 ----
mean loss: 129.59
train mean loss: 127.96
epoch train time: 0:00:00.549789
elapsed time: 0:02:42.749009
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:28:42.772762
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.66
 ---- batch: 020 ----
mean loss: 125.07
 ---- batch: 030 ----
mean loss: 124.20
 ---- batch: 040 ----
mean loss: 128.33
 ---- batch: 050 ----
mean loss: 124.95
 ---- batch: 060 ----
mean loss: 130.45
 ---- batch: 070 ----
mean loss: 128.40
 ---- batch: 080 ----
mean loss: 129.57
 ---- batch: 090 ----
mean loss: 133.25
 ---- batch: 100 ----
mean loss: 129.91
 ---- batch: 110 ----
mean loss: 127.90
train mean loss: 127.90
epoch train time: 0:00:00.547389
elapsed time: 0:02:43.296536
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:28:43.320295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.06
 ---- batch: 020 ----
mean loss: 121.42
 ---- batch: 030 ----
mean loss: 124.30
 ---- batch: 040 ----
mean loss: 135.44
 ---- batch: 050 ----
mean loss: 126.67
 ---- batch: 060 ----
mean loss: 129.94
 ---- batch: 070 ----
mean loss: 129.83
 ---- batch: 080 ----
mean loss: 123.11
 ---- batch: 090 ----
mean loss: 131.31
 ---- batch: 100 ----
mean loss: 126.87
 ---- batch: 110 ----
mean loss: 127.12
train mean loss: 127.95
epoch train time: 0:00:00.558150
elapsed time: 0:02:43.854843
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:28:43.878596
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.26
 ---- batch: 020 ----
mean loss: 129.18
 ---- batch: 030 ----
mean loss: 127.67
 ---- batch: 040 ----
mean loss: 132.99
 ---- batch: 050 ----
mean loss: 127.28
 ---- batch: 060 ----
mean loss: 138.56
 ---- batch: 070 ----
mean loss: 129.00
 ---- batch: 080 ----
mean loss: 128.25
 ---- batch: 090 ----
mean loss: 120.03
 ---- batch: 100 ----
mean loss: 128.44
 ---- batch: 110 ----
mean loss: 119.67
train mean loss: 127.98
epoch train time: 0:00:00.545119
elapsed time: 0:02:44.400103
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:28:44.423851
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.54
 ---- batch: 020 ----
mean loss: 138.40
 ---- batch: 030 ----
mean loss: 127.15
 ---- batch: 040 ----
mean loss: 127.50
 ---- batch: 050 ----
mean loss: 126.12
 ---- batch: 060 ----
mean loss: 131.41
 ---- batch: 070 ----
mean loss: 119.63
 ---- batch: 080 ----
mean loss: 122.40
 ---- batch: 090 ----
mean loss: 128.37
 ---- batch: 100 ----
mean loss: 126.73
 ---- batch: 110 ----
mean loss: 124.73
train mean loss: 127.85
epoch train time: 0:00:00.557257
elapsed time: 0:02:44.957490
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:28:44.981235
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.45
 ---- batch: 020 ----
mean loss: 129.48
 ---- batch: 030 ----
mean loss: 124.50
 ---- batch: 040 ----
mean loss: 133.70
 ---- batch: 050 ----
mean loss: 129.04
 ---- batch: 060 ----
mean loss: 130.18
 ---- batch: 070 ----
mean loss: 135.32
 ---- batch: 080 ----
mean loss: 121.81
 ---- batch: 090 ----
mean loss: 120.44
 ---- batch: 100 ----
mean loss: 131.01
 ---- batch: 110 ----
mean loss: 124.66
train mean loss: 127.91
epoch train time: 0:00:00.559583
elapsed time: 0:02:45.517222
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:28:45.540977
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.89
 ---- batch: 020 ----
mean loss: 136.14
 ---- batch: 030 ----
mean loss: 126.97
 ---- batch: 040 ----
mean loss: 131.89
 ---- batch: 050 ----
mean loss: 123.16
 ---- batch: 060 ----
mean loss: 142.39
 ---- batch: 070 ----
mean loss: 134.18
 ---- batch: 080 ----
mean loss: 130.61
 ---- batch: 090 ----
mean loss: 118.53
 ---- batch: 100 ----
mean loss: 123.98
 ---- batch: 110 ----
mean loss: 119.74
train mean loss: 127.85
epoch train time: 0:00:00.577310
elapsed time: 0:02:46.094672
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:28:46.118435
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.24
 ---- batch: 020 ----
mean loss: 124.90
 ---- batch: 030 ----
mean loss: 127.07
 ---- batch: 040 ----
mean loss: 124.13
 ---- batch: 050 ----
mean loss: 125.74
 ---- batch: 060 ----
mean loss: 124.78
 ---- batch: 070 ----
mean loss: 125.68
 ---- batch: 080 ----
mean loss: 129.59
 ---- batch: 090 ----
mean loss: 143.18
 ---- batch: 100 ----
mean loss: 124.78
 ---- batch: 110 ----
mean loss: 132.21
train mean loss: 127.88
epoch train time: 0:00:00.541705
elapsed time: 0:02:46.636535
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:28:46.660276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.28
 ---- batch: 020 ----
mean loss: 125.13
 ---- batch: 030 ----
mean loss: 124.52
 ---- batch: 040 ----
mean loss: 124.45
 ---- batch: 050 ----
mean loss: 135.35
 ---- batch: 060 ----
mean loss: 129.40
 ---- batch: 070 ----
mean loss: 130.65
 ---- batch: 080 ----
mean loss: 125.09
 ---- batch: 090 ----
mean loss: 129.61
 ---- batch: 100 ----
mean loss: 134.83
 ---- batch: 110 ----
mean loss: 125.41
train mean loss: 127.73
epoch train time: 0:00:00.547676
elapsed time: 0:02:47.184345
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:28:47.208091
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.06
 ---- batch: 020 ----
mean loss: 134.54
 ---- batch: 030 ----
mean loss: 133.87
 ---- batch: 040 ----
mean loss: 118.39
 ---- batch: 050 ----
mean loss: 124.08
 ---- batch: 060 ----
mean loss: 133.88
 ---- batch: 070 ----
mean loss: 129.14
 ---- batch: 080 ----
mean loss: 116.67
 ---- batch: 090 ----
mean loss: 124.51
 ---- batch: 100 ----
mean loss: 132.09
 ---- batch: 110 ----
mean loss: 135.00
train mean loss: 127.90
epoch train time: 0:00:00.550218
elapsed time: 0:02:47.734696
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:28:47.758445
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.19
 ---- batch: 020 ----
mean loss: 135.90
 ---- batch: 030 ----
mean loss: 123.11
 ---- batch: 040 ----
mean loss: 135.02
 ---- batch: 050 ----
mean loss: 132.25
 ---- batch: 060 ----
mean loss: 122.26
 ---- batch: 070 ----
mean loss: 133.58
 ---- batch: 080 ----
mean loss: 116.16
 ---- batch: 090 ----
mean loss: 124.23
 ---- batch: 100 ----
mean loss: 131.62
 ---- batch: 110 ----
mean loss: 126.94
train mean loss: 127.71
epoch train time: 0:00:00.562103
elapsed time: 0:02:48.296935
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:28:48.320680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.01
 ---- batch: 020 ----
mean loss: 122.11
 ---- batch: 030 ----
mean loss: 125.41
 ---- batch: 040 ----
mean loss: 128.49
 ---- batch: 050 ----
mean loss: 126.66
 ---- batch: 060 ----
mean loss: 127.49
 ---- batch: 070 ----
mean loss: 134.02
 ---- batch: 080 ----
mean loss: 131.60
 ---- batch: 090 ----
mean loss: 123.39
 ---- batch: 100 ----
mean loss: 134.04
 ---- batch: 110 ----
mean loss: 128.90
train mean loss: 127.76
epoch train time: 0:00:00.551908
elapsed time: 0:02:48.848974
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:28:48.872719
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.95
 ---- batch: 020 ----
mean loss: 127.36
 ---- batch: 030 ----
mean loss: 129.96
 ---- batch: 040 ----
mean loss: 126.02
 ---- batch: 050 ----
mean loss: 124.29
 ---- batch: 060 ----
mean loss: 127.30
 ---- batch: 070 ----
mean loss: 129.53
 ---- batch: 080 ----
mean loss: 123.66
 ---- batch: 090 ----
mean loss: 125.84
 ---- batch: 100 ----
mean loss: 134.41
 ---- batch: 110 ----
mean loss: 133.20
train mean loss: 127.69
epoch train time: 0:00:00.545810
elapsed time: 0:02:49.394913
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:28:49.418674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.84
 ---- batch: 020 ----
mean loss: 124.66
 ---- batch: 030 ----
mean loss: 124.59
 ---- batch: 040 ----
mean loss: 125.56
 ---- batch: 050 ----
mean loss: 129.38
 ---- batch: 060 ----
mean loss: 126.82
 ---- batch: 070 ----
mean loss: 130.83
 ---- batch: 080 ----
mean loss: 127.14
 ---- batch: 090 ----
mean loss: 127.61
 ---- batch: 100 ----
mean loss: 129.80
 ---- batch: 110 ----
mean loss: 124.99
train mean loss: 127.72
epoch train time: 0:00:00.554758
elapsed time: 0:02:49.949821
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:28:49.973575
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.67
 ---- batch: 020 ----
mean loss: 125.75
 ---- batch: 030 ----
mean loss: 129.25
 ---- batch: 040 ----
mean loss: 122.86
 ---- batch: 050 ----
mean loss: 128.04
 ---- batch: 060 ----
mean loss: 128.18
 ---- batch: 070 ----
mean loss: 131.61
 ---- batch: 080 ----
mean loss: 124.54
 ---- batch: 090 ----
mean loss: 124.84
 ---- batch: 100 ----
mean loss: 131.12
 ---- batch: 110 ----
mean loss: 136.85
train mean loss: 127.47
epoch train time: 0:00:00.566078
elapsed time: 0:02:50.516040
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:28:50.539802
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.50
 ---- batch: 020 ----
mean loss: 124.90
 ---- batch: 030 ----
mean loss: 122.17
 ---- batch: 040 ----
mean loss: 126.51
 ---- batch: 050 ----
mean loss: 124.65
 ---- batch: 060 ----
mean loss: 128.47
 ---- batch: 070 ----
mean loss: 126.70
 ---- batch: 080 ----
mean loss: 126.85
 ---- batch: 090 ----
mean loss: 132.25
 ---- batch: 100 ----
mean loss: 131.77
 ---- batch: 110 ----
mean loss: 131.57
train mean loss: 127.67
epoch train time: 0:00:00.560168
elapsed time: 0:02:51.076354
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:28:51.100096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.63
 ---- batch: 020 ----
mean loss: 127.93
 ---- batch: 030 ----
mean loss: 122.20
 ---- batch: 040 ----
mean loss: 128.93
 ---- batch: 050 ----
mean loss: 122.79
 ---- batch: 060 ----
mean loss: 132.06
 ---- batch: 070 ----
mean loss: 123.70
 ---- batch: 080 ----
mean loss: 130.81
 ---- batch: 090 ----
mean loss: 130.62
 ---- batch: 100 ----
mean loss: 130.42
 ---- batch: 110 ----
mean loss: 127.66
train mean loss: 127.61
epoch train time: 0:00:00.562015
elapsed time: 0:02:51.641595
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_7/checkpoint.pth.tar
**** end time: 2019-09-27 02:28:51.665311 ****
