Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_0', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 24724
use_cuda: True
Dataset: CMAPSS/FD001
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-27 10:34:33.632936 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 31, 14]             200
           Sigmoid-2           [-1, 10, 31, 14]               0
    BayesianConv2d-3           [-1, 10, 30, 14]           2,000
           Sigmoid-4           [-1, 10, 30, 14]               0
    BayesianConv2d-5           [-1, 10, 31, 14]           2,000
           Sigmoid-6           [-1, 10, 31, 14]               0
    BayesianConv2d-7           [-1, 10, 30, 14]           2,000
           Sigmoid-8           [-1, 10, 30, 14]               0
    BayesianConv2d-9            [-1, 1, 30, 14]              60
         Softplus-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
   BayesianLinear-12                  [-1, 100]          84,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 90,460
Trainable params: 90,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 10:34:33.651719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2383.48
train mean loss: 2127.60
epoch train time: 0:00:08.758924
elapsed time: 0:00:08.786285
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 10:34:42.419260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1475.40
train mean loss: 1403.18
epoch train time: 0:00:03.535395
elapsed time: 0:00:12.322815
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 10:34:45.955993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1222.78
train mean loss: 1214.10
epoch train time: 0:00:03.554823
elapsed time: 0:00:15.879429
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 10:34:49.512636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1144.81
train mean loss: 1134.70
epoch train time: 0:00:03.580070
elapsed time: 0:00:19.460835
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 10:34:53.094063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1076.51
train mean loss: 1083.33
epoch train time: 0:00:03.415153
elapsed time: 0:00:22.877307
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 10:34:56.510467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1059.98
train mean loss: 1061.73
epoch train time: 0:00:03.402453
elapsed time: 0:00:26.281072
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 10:34:59.914267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1049.82
train mean loss: 1035.45
epoch train time: 0:00:03.430448
elapsed time: 0:00:29.712920
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 10:35:03.346097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1017.69
train mean loss: 1015.74
epoch train time: 0:00:03.436407
elapsed time: 0:00:33.150597
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 10:35:06.783738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1030.12
train mean loss: 1025.55
epoch train time: 0:00:03.416910
elapsed time: 0:00:36.568754
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 10:35:10.202016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1030.38
train mean loss: 1011.04
epoch train time: 0:00:03.435704
elapsed time: 0:00:40.005940
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 10:35:13.639125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1007.48
train mean loss: 1010.42
epoch train time: 0:00:03.449922
elapsed time: 0:00:43.457257
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 10:35:17.090410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1011.27
train mean loss: 1000.03
epoch train time: 0:00:03.471863
elapsed time: 0:00:46.930899
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 10:35:20.564057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 981.85
train mean loss: 987.55
epoch train time: 0:00:03.262938
elapsed time: 0:00:50.195163
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 10:35:23.828348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1013.32
train mean loss: 1006.46
epoch train time: 0:00:03.366385
elapsed time: 0:00:53.562802
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 10:35:27.195979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 972.51
train mean loss: 972.52
epoch train time: 0:00:03.464579
elapsed time: 0:00:57.028646
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 10:35:30.661815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.26
train mean loss: 979.83
epoch train time: 0:00:03.473360
elapsed time: 0:01:00.503259
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 10:35:34.136423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 952.67
train mean loss: 932.31
epoch train time: 0:00:03.249461
elapsed time: 0:01:03.754001
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 10:35:37.387163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.59
train mean loss: 863.03
epoch train time: 0:00:03.304197
elapsed time: 0:01:07.059451
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 10:35:40.692613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 735.42
train mean loss: 699.88
epoch train time: 0:00:03.341055
elapsed time: 0:01:10.402096
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 10:35:44.035372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 583.47
train mean loss: 562.74
epoch train time: 0:00:03.281435
elapsed time: 0:01:13.684870
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 10:35:47.318001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.39
train mean loss: 471.88
epoch train time: 0:00:03.260269
elapsed time: 0:01:16.946278
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 10:35:50.579423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.34
train mean loss: 441.76
epoch train time: 0:00:03.303860
elapsed time: 0:01:20.251419
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 10:35:53.884640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.83
train mean loss: 434.51
epoch train time: 0:00:03.320116
elapsed time: 0:01:23.572923
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 10:35:57.206088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.27
train mean loss: 414.24
epoch train time: 0:00:03.340294
elapsed time: 0:01:26.914510
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 10:36:00.547649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.84
train mean loss: 410.91
epoch train time: 0:00:03.350532
elapsed time: 0:01:30.266263
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 10:36:03.899403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.19
train mean loss: 397.45
epoch train time: 0:00:03.226653
elapsed time: 0:01:33.494152
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 10:36:07.127298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.20
train mean loss: 398.42
epoch train time: 0:00:03.287688
elapsed time: 0:01:36.783250
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 10:36:10.416464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.16
train mean loss: 391.92
epoch train time: 0:00:03.327652
elapsed time: 0:01:40.112289
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 10:36:13.745438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.89
train mean loss: 385.04
epoch train time: 0:00:03.448075
elapsed time: 0:01:43.561612
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 10:36:17.194771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.96
train mean loss: 375.12
epoch train time: 0:00:03.287279
elapsed time: 0:01:46.850066
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 10:36:20.483209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.85
train mean loss: 368.01
epoch train time: 0:00:03.291081
elapsed time: 0:01:50.142376
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 10:36:23.775534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.81
train mean loss: 359.59
epoch train time: 0:00:03.319843
elapsed time: 0:01:53.463417
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 10:36:27.096568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.70
train mean loss: 349.28
epoch train time: 0:00:03.321476
elapsed time: 0:01:56.786267
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 10:36:30.419414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.74
train mean loss: 349.59
epoch train time: 0:00:03.302027
elapsed time: 0:02:00.089539
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 10:36:33.722683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.06
train mean loss: 338.96
epoch train time: 0:00:03.306882
elapsed time: 0:02:03.397625
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 10:36:37.030804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.92
train mean loss: 335.57
epoch train time: 0:00:03.239409
elapsed time: 0:02:06.638325
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 10:36:40.271507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.29
train mean loss: 332.02
epoch train time: 0:00:03.293329
elapsed time: 0:02:09.932907
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 10:36:43.566062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.39
train mean loss: 324.74
epoch train time: 0:00:03.320258
elapsed time: 0:02:13.254415
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 10:36:46.887558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.89
train mean loss: 306.65
epoch train time: 0:00:03.332900
elapsed time: 0:02:16.588554
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 10:36:50.221722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.19
train mean loss: 300.43
epoch train time: 0:00:03.246339
elapsed time: 0:02:19.836081
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 10:36:53.469255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.17
train mean loss: 297.71
epoch train time: 0:00:03.277036
elapsed time: 0:02:23.114781
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 10:36:56.748037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.52
train mean loss: 280.90
epoch train time: 0:00:03.286838
elapsed time: 0:02:26.402964
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 10:37:00.036136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.86
train mean loss: 280.54
epoch train time: 0:00:03.326224
elapsed time: 0:02:29.730530
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 10:37:03.363661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.88
train mean loss: 270.32
epoch train time: 0:00:03.439039
elapsed time: 0:02:33.170888
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 10:37:06.804064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.63
train mean loss: 256.62
epoch train time: 0:00:03.272112
elapsed time: 0:02:36.444196
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 10:37:10.077324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.57
train mean loss: 255.13
epoch train time: 0:00:03.233769
elapsed time: 0:02:39.679074
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 10:37:13.312251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.08
train mean loss: 248.19
epoch train time: 0:00:03.217050
elapsed time: 0:02:42.897419
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 10:37:16.530567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.30
train mean loss: 245.30
epoch train time: 0:00:03.211779
elapsed time: 0:02:46.110347
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 10:37:19.743492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.25
train mean loss: 245.93
epoch train time: 0:00:03.216838
elapsed time: 0:02:49.328400
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 10:37:22.961750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.08
train mean loss: 234.68
epoch train time: 0:00:03.358560
elapsed time: 0:02:52.688445
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 10:37:26.321596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.63
train mean loss: 231.83
epoch train time: 0:00:03.351673
elapsed time: 0:02:56.041431
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 10:37:29.674606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.98
train mean loss: 227.75
epoch train time: 0:00:03.350749
elapsed time: 0:02:59.393655
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 10:37:33.026822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.58
train mean loss: 222.68
epoch train time: 0:00:03.235183
elapsed time: 0:03:02.630011
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 10:37:36.263147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.63
train mean loss: 223.76
epoch train time: 0:00:03.252981
elapsed time: 0:03:05.884475
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 10:37:39.517668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.80
train mean loss: 217.65
epoch train time: 0:00:03.295424
elapsed time: 0:03:09.181364
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 10:37:42.814512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.08
train mean loss: 218.27
epoch train time: 0:00:03.331931
elapsed time: 0:03:12.514540
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 10:37:46.147679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.17
train mean loss: 213.62
epoch train time: 0:00:03.250377
elapsed time: 0:03:15.766199
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 10:37:49.399341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.31
train mean loss: 213.05
epoch train time: 0:00:03.288849
elapsed time: 0:03:19.056226
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 10:37:52.689360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.60
train mean loss: 211.96
epoch train time: 0:00:03.392512
elapsed time: 0:03:22.449988
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 10:37:56.083143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.73
train mean loss: 208.66
epoch train time: 0:00:03.421980
elapsed time: 0:03:25.873424
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 10:37:59.506590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.22
train mean loss: 205.12
epoch train time: 0:00:03.434304
elapsed time: 0:03:29.309213
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 10:38:02.942368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.31
train mean loss: 200.08
epoch train time: 0:00:03.331045
elapsed time: 0:03:32.641713
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 10:38:06.274924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.95
train mean loss: 206.52
epoch train time: 0:00:03.205551
elapsed time: 0:03:35.848605
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 10:38:09.481777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.62
train mean loss: 196.45
epoch train time: 0:00:03.259730
elapsed time: 0:03:39.109673
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 10:38:12.742823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.96
train mean loss: 193.35
epoch train time: 0:00:03.275523
elapsed time: 0:03:42.386540
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 10:38:16.019704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.61
train mean loss: 196.44
epoch train time: 0:00:03.292804
elapsed time: 0:03:45.680660
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 10:38:19.313832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.34
train mean loss: 191.01
epoch train time: 0:00:03.268776
elapsed time: 0:03:48.950696
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 10:38:22.583830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.62
train mean loss: 190.98
epoch train time: 0:00:03.308706
elapsed time: 0:03:52.260526
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 10:38:25.893726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.77
train mean loss: 186.83
epoch train time: 0:00:03.207346
elapsed time: 0:03:55.469265
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 10:38:29.102440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.82
train mean loss: 186.67
epoch train time: 0:00:03.260475
elapsed time: 0:03:58.731040
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 10:38:32.364205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.90
train mean loss: 185.65
epoch train time: 0:00:03.281521
elapsed time: 0:04:02.014089
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 10:38:35.647243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.03
train mean loss: 180.30
epoch train time: 0:00:03.298590
elapsed time: 0:04:05.314045
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 10:38:38.947200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.38
train mean loss: 188.06
epoch train time: 0:00:03.316166
elapsed time: 0:04:08.631525
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 10:38:42.264657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.10
train mean loss: 178.99
epoch train time: 0:00:03.402915
elapsed time: 0:04:12.035902
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 10:38:45.669153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.23
train mean loss: 177.86
epoch train time: 0:00:03.269554
elapsed time: 0:04:15.306769
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 10:38:48.939924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.07
train mean loss: 177.56
epoch train time: 0:00:03.296628
elapsed time: 0:04:18.604750
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 10:38:52.237938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.39
train mean loss: 175.25
epoch train time: 0:00:03.327589
elapsed time: 0:04:21.933715
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 10:38:55.566888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.13
train mean loss: 186.12
epoch train time: 0:00:03.344311
elapsed time: 0:04:25.279267
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 10:38:58.912420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.22
train mean loss: 178.29
epoch train time: 0:00:03.276621
elapsed time: 0:04:28.557153
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 10:39:02.190303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.51
train mean loss: 177.01
epoch train time: 0:00:03.254373
elapsed time: 0:04:31.812821
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 10:39:05.446033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.27
train mean loss: 174.82
epoch train time: 0:00:03.252501
elapsed time: 0:04:35.066607
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 10:39:08.699756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.32
train mean loss: 172.02
epoch train time: 0:00:03.292311
elapsed time: 0:04:38.360107
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 10:39:11.993262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.10
train mean loss: 165.13
epoch train time: 0:00:03.293299
elapsed time: 0:04:41.654741
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 10:39:15.287906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.04
train mean loss: 169.23
epoch train time: 0:00:03.274199
elapsed time: 0:04:44.930162
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 10:39:18.563304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.84
train mean loss: 169.95
epoch train time: 0:00:03.295582
elapsed time: 0:04:48.227020
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 10:39:21.860163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.90
train mean loss: 168.47
epoch train time: 0:00:03.210526
elapsed time: 0:04:51.438721
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 10:39:25.071930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.26
train mean loss: 167.30
epoch train time: 0:00:03.228283
elapsed time: 0:04:54.668382
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 10:39:28.301593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.27
train mean loss: 165.82
epoch train time: 0:00:03.319167
elapsed time: 0:04:57.988874
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 10:39:31.622038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.21
train mean loss: 165.93
epoch train time: 0:00:03.431590
elapsed time: 0:05:01.421706
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 10:39:35.054861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.72
train mean loss: 165.69
epoch train time: 0:00:03.461327
elapsed time: 0:05:04.884848
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 10:39:38.518005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.88
train mean loss: 160.71
epoch train time: 0:00:03.441917
elapsed time: 0:05:08.327927
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 10:39:41.961115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.90
train mean loss: 162.82
epoch train time: 0:00:03.285745
elapsed time: 0:05:11.614956
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 10:39:45.248128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.55
train mean loss: 161.52
epoch train time: 0:00:03.349937
elapsed time: 0:05:14.966196
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 10:39:48.599364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.68
train mean loss: 165.95
epoch train time: 0:00:03.357321
elapsed time: 0:05:18.324822
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 10:39:51.957978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.43
train mean loss: 159.00
epoch train time: 0:00:03.335855
elapsed time: 0:05:21.661974
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 10:39:55.295168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.79
train mean loss: 157.94
epoch train time: 0:00:03.336484
elapsed time: 0:05:24.999712
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 10:39:58.632853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.87
train mean loss: 158.95
epoch train time: 0:00:03.351400
elapsed time: 0:05:28.352534
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 10:40:01.985753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.87
train mean loss: 156.71
epoch train time: 0:00:03.316338
elapsed time: 0:05:31.670172
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 10:40:05.303336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.52
train mean loss: 156.11
epoch train time: 0:00:03.279166
elapsed time: 0:05:34.950597
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 10:40:08.583736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.60
train mean loss: 154.85
epoch train time: 0:00:03.301712
elapsed time: 0:05:38.253618
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 10:40:11.886772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.82
train mean loss: 154.99
epoch train time: 0:00:03.327518
elapsed time: 0:05:41.582430
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 10:40:15.215628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.40
train mean loss: 155.95
epoch train time: 0:00:03.334997
elapsed time: 0:05:44.918677
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 10:40:18.551809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.20
train mean loss: 157.20
epoch train time: 0:00:03.423988
elapsed time: 0:05:48.343946
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 10:40:21.977130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.12
train mean loss: 153.01
epoch train time: 0:00:03.389926
elapsed time: 0:05:51.735283
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 10:40:25.368448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.05
train mean loss: 153.42
epoch train time: 0:00:03.268769
elapsed time: 0:05:55.005396
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 10:40:28.638586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.66
train mean loss: 150.67
epoch train time: 0:00:03.317525
elapsed time: 0:05:58.324294
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 10:40:31.957443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.13
train mean loss: 157.10
epoch train time: 0:00:03.322789
elapsed time: 0:06:01.648473
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 10:40:35.281483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.31
train mean loss: 149.84
epoch train time: 0:00:03.317671
elapsed time: 0:06:04.967275
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 10:40:38.600428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.99
train mean loss: 152.14
epoch train time: 0:00:03.252254
elapsed time: 0:06:08.220664
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 10:40:41.853848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.10
train mean loss: 149.40
epoch train time: 0:00:03.212339
elapsed time: 0:06:11.434265
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 10:40:45.067422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.12
train mean loss: 149.78
epoch train time: 0:00:03.215998
elapsed time: 0:06:14.651441
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 10:40:48.284633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.50
train mean loss: 148.47
epoch train time: 0:00:03.208448
elapsed time: 0:06:17.861147
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 10:40:51.494291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.67
train mean loss: 146.54
epoch train time: 0:00:03.239498
elapsed time: 0:06:21.102015
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 10:40:54.735161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.25
train mean loss: 146.28
epoch train time: 0:00:03.350877
elapsed time: 0:06:24.454168
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 10:40:58.087385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.50
train mean loss: 146.11
epoch train time: 0:00:03.343704
elapsed time: 0:06:27.799187
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 10:41:01.432345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.92
train mean loss: 145.44
epoch train time: 0:00:03.275880
elapsed time: 0:06:31.076260
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 10:41:04.709403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.10
train mean loss: 148.61
epoch train time: 0:00:03.333409
elapsed time: 0:06:34.411092
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 10:41:08.044255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.95
train mean loss: 147.06
epoch train time: 0:00:03.447488
elapsed time: 0:06:37.859850
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 10:41:11.493067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.91
train mean loss: 144.62
epoch train time: 0:00:03.467932
elapsed time: 0:06:41.329121
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 10:41:14.962299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.72
train mean loss: 141.01
epoch train time: 0:00:03.402282
elapsed time: 0:06:44.732735
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 10:41:18.365884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.35
train mean loss: 142.78
epoch train time: 0:00:03.399450
elapsed time: 0:06:48.133467
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 10:41:21.766647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.20
train mean loss: 141.56
epoch train time: 0:00:03.401296
elapsed time: 0:06:51.536106
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 10:41:25.169268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.33
train mean loss: 142.84
epoch train time: 0:00:03.398213
elapsed time: 0:06:54.935643
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 10:41:28.568824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.25
train mean loss: 144.12
epoch train time: 0:00:03.426072
elapsed time: 0:06:58.362991
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 10:41:31.996150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.99
train mean loss: 142.70
epoch train time: 0:00:03.348138
elapsed time: 0:07:01.712367
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 10:41:35.345501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.38
train mean loss: 143.46
epoch train time: 0:00:03.375280
elapsed time: 0:07:05.088962
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 10:41:38.722126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.57
train mean loss: 144.63
epoch train time: 0:00:03.411563
elapsed time: 0:07:08.501978
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 10:41:42.134970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.65
train mean loss: 137.95
epoch train time: 0:00:03.396356
elapsed time: 0:07:11.899435
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 10:41:45.532583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.21
train mean loss: 142.00
epoch train time: 0:00:03.347082
elapsed time: 0:07:15.247717
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 10:41:48.880871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.86
train mean loss: 137.85
epoch train time: 0:00:03.351412
elapsed time: 0:07:18.600439
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 10:41:52.233587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.52
train mean loss: 137.66
epoch train time: 0:00:03.377599
elapsed time: 0:07:21.979402
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 10:41:55.612655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.35
train mean loss: 137.92
epoch train time: 0:00:03.381487
elapsed time: 0:07:25.362317
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 10:41:58.995461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.39
train mean loss: 134.78
epoch train time: 0:00:03.474995
elapsed time: 0:07:28.838611
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 10:42:02.471764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.28
train mean loss: 134.59
epoch train time: 0:00:03.383547
elapsed time: 0:07:32.223391
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 10:42:05.856530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.56
train mean loss: 134.07
epoch train time: 0:00:03.421554
elapsed time: 0:07:35.646213
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 10:42:09.279376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.91
train mean loss: 140.17
epoch train time: 0:00:03.368948
elapsed time: 0:07:39.016482
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 10:42:12.649658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.43
train mean loss: 131.07
epoch train time: 0:00:03.386988
elapsed time: 0:07:42.404754
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 10:42:16.037900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.19
train mean loss: 135.16
epoch train time: 0:00:03.348803
elapsed time: 0:07:45.754713
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 10:42:19.387858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.61
train mean loss: 135.88
epoch train time: 0:00:03.328899
elapsed time: 0:07:49.084930
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 10:42:22.718080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.08
train mean loss: 132.53
epoch train time: 0:00:03.379833
elapsed time: 0:07:52.466065
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 10:42:26.099231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.45
train mean loss: 134.92
epoch train time: 0:00:03.373612
elapsed time: 0:07:55.841674
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 10:42:29.474894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.70
train mean loss: 131.64
epoch train time: 0:00:03.384201
elapsed time: 0:07:59.227312
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 10:42:32.860471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.20
train mean loss: 132.03
epoch train time: 0:00:03.405862
elapsed time: 0:08:02.634531
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 10:42:36.267680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.06
train mean loss: 131.41
epoch train time: 0:00:03.313314
elapsed time: 0:08:05.949782
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 10:42:39.582953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.11
train mean loss: 130.26
epoch train time: 0:00:03.340600
elapsed time: 0:08:09.291644
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 10:42:42.924787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.02
train mean loss: 132.22
epoch train time: 0:00:03.369973
elapsed time: 0:08:12.662966
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 10:42:46.296123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.76
train mean loss: 128.04
epoch train time: 0:00:03.357195
elapsed time: 0:08:16.021515
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 10:42:49.654646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.29
train mean loss: 131.28
epoch train time: 0:00:03.326838
elapsed time: 0:08:19.349580
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 10:42:52.982745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.77
train mean loss: 129.53
epoch train time: 0:00:03.322671
elapsed time: 0:08:22.673708
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 10:42:56.306708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.49
train mean loss: 131.53
epoch train time: 0:00:03.298380
elapsed time: 0:08:25.973164
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 10:42:59.606321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.16
train mean loss: 127.52
epoch train time: 0:00:03.250288
elapsed time: 0:08:29.224687
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 10:43:02.857829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.85
train mean loss: 127.83
epoch train time: 0:00:03.282645
elapsed time: 0:08:32.508619
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 10:43:06.141820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.79
train mean loss: 129.61
epoch train time: 0:00:03.308551
elapsed time: 0:08:35.818508
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 10:43:09.451715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.73
train mean loss: 128.93
epoch train time: 0:00:03.303438
elapsed time: 0:08:39.123202
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 10:43:12.756349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.42
train mean loss: 128.75
epoch train time: 0:00:03.325387
elapsed time: 0:08:42.449909
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 10:43:16.083048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.87
train mean loss: 130.60
epoch train time: 0:00:03.215798
elapsed time: 0:08:45.666946
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 10:43:19.300106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.33
train mean loss: 124.90
epoch train time: 0:00:03.264702
elapsed time: 0:08:48.932949
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 10:43:22.566114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.01
train mean loss: 127.11
epoch train time: 0:00:03.286773
elapsed time: 0:08:52.221081
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 10:43:25.854268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.43
train mean loss: 127.39
epoch train time: 0:00:03.310055
elapsed time: 0:08:55.532422
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 10:43:29.165560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.33
train mean loss: 125.08
epoch train time: 0:00:03.340315
elapsed time: 0:08:58.874033
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 10:43:32.507270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.24
train mean loss: 126.87
epoch train time: 0:00:03.436418
elapsed time: 0:09:02.312132
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 10:43:35.945293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.47
train mean loss: 126.80
epoch train time: 0:00:03.492078
elapsed time: 0:09:05.805568
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 10:43:39.438736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.85
train mean loss: 126.41
epoch train time: 0:00:03.311881
elapsed time: 0:09:09.118633
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 10:43:42.751774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.01
train mean loss: 127.19
epoch train time: 0:00:03.346134
elapsed time: 0:09:12.466267
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 10:43:46.099407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.99
train mean loss: 125.52
epoch train time: 0:00:03.390630
elapsed time: 0:09:15.858078
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 10:43:49.491247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.65
train mean loss: 122.45
epoch train time: 0:00:03.393872
elapsed time: 0:09:19.253434
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 10:43:52.886588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.81
train mean loss: 124.88
epoch train time: 0:00:03.399950
elapsed time: 0:09:22.654604
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 10:43:56.287752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.24
train mean loss: 122.87
epoch train time: 0:00:03.395064
elapsed time: 0:09:26.050832
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 10:43:59.683992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.36
train mean loss: 121.60
epoch train time: 0:00:03.296492
elapsed time: 0:09:29.348510
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 10:44:02.981685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.64
train mean loss: 124.02
epoch train time: 0:00:03.311668
elapsed time: 0:09:32.661679
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 10:44:06.294826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.65
train mean loss: 124.94
epoch train time: 0:00:03.332713
elapsed time: 0:09:35.995869
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 10:44:09.629105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.24
train mean loss: 123.73
epoch train time: 0:00:03.358094
elapsed time: 0:09:39.355395
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 10:44:12.988542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.93
train mean loss: 122.73
epoch train time: 0:00:03.351941
elapsed time: 0:09:42.708533
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 10:44:16.341716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.51
train mean loss: 122.48
epoch train time: 0:00:03.409335
elapsed time: 0:09:46.119312
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 10:44:19.752297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.19
train mean loss: 122.97
epoch train time: 0:00:03.401387
elapsed time: 0:09:49.521685
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 10:44:23.154833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.03
train mean loss: 121.82
epoch train time: 0:00:03.242092
elapsed time: 0:09:52.765033
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 10:44:26.398181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.99
train mean loss: 119.56
epoch train time: 0:00:03.244544
elapsed time: 0:09:56.010764
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 10:44:29.643929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.72
train mean loss: 118.61
epoch train time: 0:00:03.237717
elapsed time: 0:09:59.250052
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 10:44:32.883217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.89
train mean loss: 118.15
epoch train time: 0:00:03.195976
elapsed time: 0:10:02.447213
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 10:44:36.080361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.82
train mean loss: 119.62
epoch train time: 0:00:03.308868
elapsed time: 0:10:05.757363
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 10:44:39.390556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.21
train mean loss: 117.49
epoch train time: 0:00:03.338489
elapsed time: 0:10:09.097351
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 10:44:42.730641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.76
train mean loss: 119.05
epoch train time: 0:00:03.317302
elapsed time: 0:10:12.416102
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 10:44:46.049319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.88
train mean loss: 118.47
epoch train time: 0:00:03.228843
elapsed time: 0:10:15.646244
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 10:44:49.279410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.47
train mean loss: 119.99
epoch train time: 0:00:03.249719
elapsed time: 0:10:18.897281
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 10:44:52.530422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.00
train mean loss: 115.57
epoch train time: 0:00:03.297954
elapsed time: 0:10:22.196549
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 10:44:55.829737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.82
train mean loss: 116.50
epoch train time: 0:00:03.331835
elapsed time: 0:10:25.529685
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 10:44:59.162895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.73
train mean loss: 119.72
epoch train time: 0:00:03.338096
elapsed time: 0:10:28.869217
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 10:45:02.502428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.27
train mean loss: 120.56
epoch train time: 0:00:03.304660
elapsed time: 0:10:32.175104
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 10:45:05.808249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.91
train mean loss: 115.07
epoch train time: 0:00:03.304605
elapsed time: 0:10:35.480935
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 10:45:09.114092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.04
train mean loss: 115.27
epoch train time: 0:00:03.382686
elapsed time: 0:10:38.864931
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 10:45:12.498091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.67
train mean loss: 119.34
epoch train time: 0:00:03.425844
elapsed time: 0:10:42.292023
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 10:45:15.925186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.30
train mean loss: 120.37
epoch train time: 0:00:03.387991
elapsed time: 0:10:45.681399
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 10:45:19.314596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.88
train mean loss: 120.09
epoch train time: 0:00:03.381456
elapsed time: 0:10:49.064046
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 10:45:22.697216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.17
train mean loss: 113.81
epoch train time: 0:00:03.299842
elapsed time: 0:10:52.365261
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 10:45:25.998469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.66
train mean loss: 117.86
epoch train time: 0:00:03.284988
elapsed time: 0:10:55.651544
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 10:45:29.284715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.35
train mean loss: 115.99
epoch train time: 0:00:03.300493
elapsed time: 0:10:58.953313
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 10:45:32.586515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.66
train mean loss: 116.76
epoch train time: 0:00:03.321724
elapsed time: 0:11:02.276399
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 10:45:35.909569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.60
train mean loss: 115.15
epoch train time: 0:00:03.312865
elapsed time: 0:11:05.590532
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 10:45:39.223712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.27
train mean loss: 113.42
epoch train time: 0:00:03.313034
elapsed time: 0:11:08.905012
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 10:45:42.538190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.81
train mean loss: 115.31
epoch train time: 0:00:03.262634
elapsed time: 0:11:12.168837
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 10:45:45.802000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.19
train mean loss: 114.35
epoch train time: 0:00:03.208671
elapsed time: 0:11:15.378704
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 10:45:49.011853
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.71
train mean loss: 114.32
epoch train time: 0:00:03.271555
elapsed time: 0:11:18.651637
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 10:45:52.284652
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.72
train mean loss: 112.62
epoch train time: 0:00:03.273477
elapsed time: 0:11:21.926287
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 10:45:55.559446
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.51
train mean loss: 114.10
epoch train time: 0:00:03.328170
elapsed time: 0:11:25.255816
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 10:45:58.889047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.04
train mean loss: 113.90
epoch train time: 0:00:03.433640
elapsed time: 0:11:28.690792
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 10:46:02.323953
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.96
train mean loss: 111.81
epoch train time: 0:00:03.496013
elapsed time: 0:11:32.188825
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 10:46:05.821992
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.30
train mean loss: 114.15
epoch train time: 0:00:03.389130
elapsed time: 0:11:35.579165
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 10:46:09.212325
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.60
train mean loss: 113.63
epoch train time: 0:00:03.347598
elapsed time: 0:11:38.927988
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 10:46:12.561127
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.77
train mean loss: 113.98
epoch train time: 0:00:03.363865
elapsed time: 0:11:42.293226
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 10:46:15.926475
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.67
train mean loss: 113.03
epoch train time: 0:00:03.341001
elapsed time: 0:11:45.635562
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 10:46:19.268716
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.01
train mean loss: 113.40
epoch train time: 0:00:03.296766
elapsed time: 0:11:48.933591
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 10:46:22.566759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.08
train mean loss: 116.91
epoch train time: 0:00:03.278267
elapsed time: 0:11:52.213123
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 10:46:25.846311
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.16
train mean loss: 112.07
epoch train time: 0:00:03.320563
elapsed time: 0:11:55.535172
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 10:46:29.168309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.27
train mean loss: 114.46
epoch train time: 0:00:03.307157
elapsed time: 0:11:58.843597
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 10:46:32.476745
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.65
train mean loss: 113.79
epoch train time: 0:00:03.279605
elapsed time: 0:12:02.124356
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 10:46:35.757522
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.23
train mean loss: 114.79
epoch train time: 0:00:03.220504
elapsed time: 0:12:05.346163
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 10:46:38.979365
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.68
train mean loss: 112.47
epoch train time: 0:00:03.258230
elapsed time: 0:12:08.605713
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 10:46:42.238869
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.01
train mean loss: 113.73
epoch train time: 0:00:03.298765
elapsed time: 0:12:11.905845
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 10:46:45.539028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.18
train mean loss: 113.80
epoch train time: 0:00:03.323658
elapsed time: 0:12:15.230890
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 10:46:48.864060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.97
train mean loss: 111.65
epoch train time: 0:00:03.444495
elapsed time: 0:12:18.676664
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 10:46:52.309966
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.69
train mean loss: 111.02
epoch train time: 0:00:03.389707
elapsed time: 0:12:22.067697
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 10:46:55.700844
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.05
train mean loss: 112.12
epoch train time: 0:00:03.342064
elapsed time: 0:12:25.411008
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 10:46:59.044185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.14
train mean loss: 112.18
epoch train time: 0:00:03.393860
elapsed time: 0:12:28.806287
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 10:47:02.439454
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.52
train mean loss: 113.87
epoch train time: 0:00:03.411485
elapsed time: 0:12:32.219063
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 10:47:05.852212
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.43
train mean loss: 112.10
epoch train time: 0:00:03.388392
elapsed time: 0:12:35.608756
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 10:47:09.241910
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.63
train mean loss: 109.60
epoch train time: 0:00:03.317172
elapsed time: 0:12:38.927095
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 10:47:12.560237
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.45
train mean loss: 114.42
epoch train time: 0:00:03.251338
elapsed time: 0:12:42.179726
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 10:47:15.812906
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.84
train mean loss: 113.46
epoch train time: 0:00:03.281435
elapsed time: 0:12:45.462487
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 10:47:19.095649
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.95
train mean loss: 110.66
epoch train time: 0:00:03.282608
elapsed time: 0:12:48.746437
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 10:47:22.379643
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.64
train mean loss: 113.61
epoch train time: 0:00:03.181407
elapsed time: 0:12:51.929158
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 10:47:25.562308
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.36
train mean loss: 112.96
epoch train time: 0:00:03.200456
elapsed time: 0:12:55.130842
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 10:47:28.763991
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.92
train mean loss: 113.02
epoch train time: 0:00:03.265241
elapsed time: 0:12:58.397373
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 10:47:32.030565
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.46
train mean loss: 113.09
epoch train time: 0:00:03.274698
elapsed time: 0:13:01.673385
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 10:47:35.306622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.44
train mean loss: 114.01
epoch train time: 0:00:03.391404
elapsed time: 0:13:05.066361
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 10:47:38.699367
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.82
train mean loss: 113.36
epoch train time: 0:00:03.389284
elapsed time: 0:13:08.456762
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 10:47:42.089937
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.29
train mean loss: 114.40
epoch train time: 0:00:03.396688
elapsed time: 0:13:11.855079
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 10:47:45.488159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.37
train mean loss: 113.85
epoch train time: 0:00:03.386546
elapsed time: 0:13:15.243043
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 10:47:48.876114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.96
train mean loss: 110.64
epoch train time: 0:00:03.338899
elapsed time: 0:13:18.583133
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 10:47:52.216281
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.62
train mean loss: 112.79
epoch train time: 0:00:03.298035
elapsed time: 0:13:21.882369
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 10:47:55.515523
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.66
train mean loss: 113.31
epoch train time: 0:00:03.300422
elapsed time: 0:13:25.184274
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 10:47:58.817512
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.66
train mean loss: 113.48
epoch train time: 0:00:03.245399
elapsed time: 0:13:28.430979
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 10:48:02.064117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.68
train mean loss: 112.12
epoch train time: 0:00:03.228673
elapsed time: 0:13:31.660827
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 10:48:05.293978
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.83
train mean loss: 115.66
epoch train time: 0:00:03.233853
elapsed time: 0:13:34.895903
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 10:48:08.529087
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.48
train mean loss: 112.15
epoch train time: 0:00:03.269950
elapsed time: 0:13:38.167181
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 10:48:11.800343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.19
train mean loss: 112.36
epoch train time: 0:00:03.343148
elapsed time: 0:13:41.511588
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 10:48:15.144747
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.70
train mean loss: 111.23
epoch train time: 0:00:03.342925
elapsed time: 0:13:44.855813
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 10:48:18.489025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.06
train mean loss: 112.00
epoch train time: 0:00:03.259278
elapsed time: 0:13:48.116398
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 10:48:21.749556
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.34
train mean loss: 110.02
epoch train time: 0:00:03.262219
elapsed time: 0:13:51.379816
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 10:48:25.012968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.79
train mean loss: 110.09
epoch train time: 0:00:03.414153
elapsed time: 0:13:54.795316
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 10:48:28.428455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.94
train mean loss: 112.16
epoch train time: 0:00:03.462471
elapsed time: 0:13:58.268570
checkpoint saved in file: log/CMAPSS/FD001/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_0/checkpoint.pth.tar
**** end time: 2019-09-27 10:48:31.901530 ****
