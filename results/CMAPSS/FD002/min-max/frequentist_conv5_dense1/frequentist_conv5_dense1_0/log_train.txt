Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_0', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 21261
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-25 20:40:07.052707 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 21, 24]             100
              Tanh-2           [-1, 10, 21, 24]               0
            Conv2d-3           [-1, 10, 20, 24]           1,000
              Tanh-4           [-1, 10, 20, 24]               0
            Conv2d-5           [-1, 10, 21, 24]           1,000
              Tanh-6           [-1, 10, 21, 24]               0
            Conv2d-7           [-1, 10, 20, 24]           1,000
              Tanh-8           [-1, 10, 20, 24]               0
            Conv2d-9            [-1, 1, 20, 24]              30
             Tanh-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
          Dropout-12                  [-1, 480]               0
           Linear-13                  [-1, 100]          48,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 51,230
Trainable params: 51,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 20:40:07.139810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3797.76
 ---- batch: 020 ----
mean loss: 1635.82
 ---- batch: 030 ----
mean loss: 1239.56
 ---- batch: 040 ----
mean loss: 1062.49
 ---- batch: 050 ----
mean loss: 1011.79
 ---- batch: 060 ----
mean loss: 976.19
 ---- batch: 070 ----
mean loss: 937.18
 ---- batch: 080 ----
mean loss: 912.40
 ---- batch: 090 ----
mean loss: 906.04
train mean loss: 1354.98
epoch train time: 0:00:35.391535
elapsed time: 0:00:35.481222
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 20:40:42.533974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.53
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 833.70
 ---- batch: 040 ----
mean loss: 812.82
 ---- batch: 050 ----
mean loss: 802.40
 ---- batch: 060 ----
mean loss: 789.96
 ---- batch: 070 ----
mean loss: 779.67
 ---- batch: 080 ----
mean loss: 779.70
 ---- batch: 090 ----
mean loss: 767.41
train mean loss: 803.91
epoch train time: 0:00:02.438781
elapsed time: 0:00:37.920183
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 20:40:44.972980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 721.99
 ---- batch: 020 ----
mean loss: 709.18
 ---- batch: 030 ----
mean loss: 712.29
 ---- batch: 040 ----
mean loss: 716.86
 ---- batch: 050 ----
mean loss: 703.71
 ---- batch: 060 ----
mean loss: 673.37
 ---- batch: 070 ----
mean loss: 673.67
 ---- batch: 080 ----
mean loss: 668.98
 ---- batch: 090 ----
mean loss: 666.57
train mean loss: 691.97
epoch train time: 0:00:02.379116
elapsed time: 0:00:40.299527
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 20:40:47.352297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 657.22
 ---- batch: 020 ----
mean loss: 639.88
 ---- batch: 030 ----
mean loss: 644.58
 ---- batch: 040 ----
mean loss: 633.56
 ---- batch: 050 ----
mean loss: 608.94
 ---- batch: 060 ----
mean loss: 603.63
 ---- batch: 070 ----
mean loss: 612.18
 ---- batch: 080 ----
mean loss: 601.52
 ---- batch: 090 ----
mean loss: 592.06
train mean loss: 620.30
epoch train time: 0:00:02.362664
elapsed time: 0:00:42.662394
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 20:40:49.715203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 592.95
 ---- batch: 020 ----
mean loss: 589.75
 ---- batch: 030 ----
mean loss: 583.13
 ---- batch: 040 ----
mean loss: 560.55
 ---- batch: 050 ----
mean loss: 575.44
 ---- batch: 060 ----
mean loss: 578.01
 ---- batch: 070 ----
mean loss: 561.14
 ---- batch: 080 ----
mean loss: 572.71
 ---- batch: 090 ----
mean loss: 560.97
train mean loss: 573.96
epoch train time: 0:00:02.366166
elapsed time: 0:00:45.028814
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 20:40:52.081590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 561.16
 ---- batch: 020 ----
mean loss: 551.39
 ---- batch: 030 ----
mean loss: 556.29
 ---- batch: 040 ----
mean loss: 555.96
 ---- batch: 050 ----
mean loss: 561.79
 ---- batch: 060 ----
mean loss: 536.82
 ---- batch: 070 ----
mean loss: 538.84
 ---- batch: 080 ----
mean loss: 527.57
 ---- batch: 090 ----
mean loss: 514.93
train mean loss: 544.71
epoch train time: 0:00:02.371228
elapsed time: 0:00:47.400239
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 20:40:54.453011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.34
 ---- batch: 020 ----
mean loss: 513.14
 ---- batch: 030 ----
mean loss: 496.19
 ---- batch: 040 ----
mean loss: 510.91
 ---- batch: 050 ----
mean loss: 497.19
 ---- batch: 060 ----
mean loss: 519.91
 ---- batch: 070 ----
mean loss: 478.99
 ---- batch: 080 ----
mean loss: 481.31
 ---- batch: 090 ----
mean loss: 492.34
train mean loss: 498.29
epoch train time: 0:00:02.369795
elapsed time: 0:00:49.770234
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 20:40:56.823003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.92
 ---- batch: 020 ----
mean loss: 478.82
 ---- batch: 030 ----
mean loss: 469.99
 ---- batch: 040 ----
mean loss: 460.27
 ---- batch: 050 ----
mean loss: 483.43
 ---- batch: 060 ----
mean loss: 440.61
 ---- batch: 070 ----
mean loss: 457.96
 ---- batch: 080 ----
mean loss: 449.02
 ---- batch: 090 ----
mean loss: 439.04
train mean loss: 459.45
epoch train time: 0:00:02.355326
elapsed time: 0:00:52.125787
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 20:40:59.178572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.70
 ---- batch: 020 ----
mean loss: 431.79
 ---- batch: 030 ----
mean loss: 441.92
 ---- batch: 040 ----
mean loss: 446.14
 ---- batch: 050 ----
mean loss: 433.66
 ---- batch: 060 ----
mean loss: 434.15
 ---- batch: 070 ----
mean loss: 462.51
 ---- batch: 080 ----
mean loss: 458.78
 ---- batch: 090 ----
mean loss: 448.86
train mean loss: 445.46
epoch train time: 0:00:02.350849
elapsed time: 0:00:54.476879
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 20:41:01.529665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.13
 ---- batch: 020 ----
mean loss: 439.76
 ---- batch: 030 ----
mean loss: 439.07
 ---- batch: 040 ----
mean loss: 428.59
 ---- batch: 050 ----
mean loss: 458.20
 ---- batch: 060 ----
mean loss: 447.82
 ---- batch: 070 ----
mean loss: 450.60
 ---- batch: 080 ----
mean loss: 453.23
 ---- batch: 090 ----
mean loss: 443.93
train mean loss: 444.05
epoch train time: 0:00:02.355631
elapsed time: 0:00:56.832728
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 20:41:03.885499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.15
 ---- batch: 020 ----
mean loss: 426.54
 ---- batch: 030 ----
mean loss: 433.32
 ---- batch: 040 ----
mean loss: 435.85
 ---- batch: 050 ----
mean loss: 440.48
 ---- batch: 060 ----
mean loss: 425.89
 ---- batch: 070 ----
mean loss: 433.29
 ---- batch: 080 ----
mean loss: 442.04
 ---- batch: 090 ----
mean loss: 428.13
train mean loss: 434.33
epoch train time: 0:00:02.357549
elapsed time: 0:00:59.190457
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 20:41:06.243223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.59
 ---- batch: 020 ----
mean loss: 417.64
 ---- batch: 030 ----
mean loss: 412.28
 ---- batch: 040 ----
mean loss: 450.37
 ---- batch: 050 ----
mean loss: 425.44
 ---- batch: 060 ----
mean loss: 423.81
 ---- batch: 070 ----
mean loss: 429.74
 ---- batch: 080 ----
mean loss: 431.38
 ---- batch: 090 ----
mean loss: 422.59
train mean loss: 426.34
epoch train time: 0:00:02.358596
elapsed time: 0:01:01.549230
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 20:41:08.601998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.38
 ---- batch: 020 ----
mean loss: 421.00
 ---- batch: 030 ----
mean loss: 417.88
 ---- batch: 040 ----
mean loss: 425.67
 ---- batch: 050 ----
mean loss: 432.91
 ---- batch: 060 ----
mean loss: 437.91
 ---- batch: 070 ----
mean loss: 419.93
 ---- batch: 080 ----
mean loss: 428.23
 ---- batch: 090 ----
mean loss: 435.78
train mean loss: 425.68
epoch train time: 0:00:02.352754
elapsed time: 0:01:03.902197
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 20:41:10.954989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.78
 ---- batch: 020 ----
mean loss: 450.07
 ---- batch: 030 ----
mean loss: 419.65
 ---- batch: 040 ----
mean loss: 424.77
 ---- batch: 050 ----
mean loss: 432.78
 ---- batch: 060 ----
mean loss: 409.78
 ---- batch: 070 ----
mean loss: 404.97
 ---- batch: 080 ----
mean loss: 421.10
 ---- batch: 090 ----
mean loss: 412.19
train mean loss: 424.61
epoch train time: 0:00:02.353263
elapsed time: 0:01:06.255665
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 20:41:13.308460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.19
 ---- batch: 020 ----
mean loss: 422.20
 ---- batch: 030 ----
mean loss: 420.25
 ---- batch: 040 ----
mean loss: 427.37
 ---- batch: 050 ----
mean loss: 415.93
 ---- batch: 060 ----
mean loss: 417.50
 ---- batch: 070 ----
mean loss: 421.11
 ---- batch: 080 ----
mean loss: 420.55
 ---- batch: 090 ----
mean loss: 410.41
train mean loss: 418.04
epoch train time: 0:00:02.345558
elapsed time: 0:01:08.601439
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 20:41:15.654228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.16
 ---- batch: 020 ----
mean loss: 430.53
 ---- batch: 030 ----
mean loss: 408.11
 ---- batch: 040 ----
mean loss: 409.44
 ---- batch: 050 ----
mean loss: 414.21
 ---- batch: 060 ----
mean loss: 401.82
 ---- batch: 070 ----
mean loss: 415.78
 ---- batch: 080 ----
mean loss: 421.53
 ---- batch: 090 ----
mean loss: 410.01
train mean loss: 415.76
epoch train time: 0:00:02.342509
elapsed time: 0:01:10.944155
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 20:41:17.996935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.53
 ---- batch: 020 ----
mean loss: 417.86
 ---- batch: 030 ----
mean loss: 406.96
 ---- batch: 040 ----
mean loss: 400.06
 ---- batch: 050 ----
mean loss: 396.91
 ---- batch: 060 ----
mean loss: 411.02
 ---- batch: 070 ----
mean loss: 409.21
 ---- batch: 080 ----
mean loss: 416.34
 ---- batch: 090 ----
mean loss: 429.39
train mean loss: 414.12
epoch train time: 0:00:02.362007
elapsed time: 0:01:13.306349
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 20:41:20.359117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.80
 ---- batch: 020 ----
mean loss: 402.90
 ---- batch: 030 ----
mean loss: 406.05
 ---- batch: 040 ----
mean loss: 409.60
 ---- batch: 050 ----
mean loss: 401.44
 ---- batch: 060 ----
mean loss: 407.52
 ---- batch: 070 ----
mean loss: 417.22
 ---- batch: 080 ----
mean loss: 419.40
 ---- batch: 090 ----
mean loss: 413.30
train mean loss: 409.18
epoch train time: 0:00:02.337874
elapsed time: 0:01:15.644448
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 20:41:22.697239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.13
 ---- batch: 020 ----
mean loss: 396.03
 ---- batch: 030 ----
mean loss: 410.68
 ---- batch: 040 ----
mean loss: 418.76
 ---- batch: 050 ----
mean loss: 406.40
 ---- batch: 060 ----
mean loss: 403.04
 ---- batch: 070 ----
mean loss: 417.37
 ---- batch: 080 ----
mean loss: 428.93
 ---- batch: 090 ----
mean loss: 404.18
train mean loss: 409.50
epoch train time: 0:00:02.345778
elapsed time: 0:01:17.990426
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 20:41:25.043191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.61
 ---- batch: 020 ----
mean loss: 402.82
 ---- batch: 030 ----
mean loss: 415.77
 ---- batch: 040 ----
mean loss: 413.56
 ---- batch: 050 ----
mean loss: 413.77
 ---- batch: 060 ----
mean loss: 404.23
 ---- batch: 070 ----
mean loss: 404.93
 ---- batch: 080 ----
mean loss: 427.23
 ---- batch: 090 ----
mean loss: 416.36
train mean loss: 411.39
epoch train time: 0:00:02.345859
elapsed time: 0:01:20.336470
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 20:41:27.389236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.55
 ---- batch: 020 ----
mean loss: 409.43
 ---- batch: 030 ----
mean loss: 411.80
 ---- batch: 040 ----
mean loss: 417.87
 ---- batch: 050 ----
mean loss: 408.84
 ---- batch: 060 ----
mean loss: 438.05
 ---- batch: 070 ----
mean loss: 418.53
 ---- batch: 080 ----
mean loss: 411.11
 ---- batch: 090 ----
mean loss: 407.35
train mean loss: 413.11
epoch train time: 0:00:02.338019
elapsed time: 0:01:22.674669
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 20:41:29.727437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.59
 ---- batch: 020 ----
mean loss: 408.19
 ---- batch: 030 ----
mean loss: 407.00
 ---- batch: 040 ----
mean loss: 411.27
 ---- batch: 050 ----
mean loss: 404.17
 ---- batch: 060 ----
mean loss: 399.17
 ---- batch: 070 ----
mean loss: 402.51
 ---- batch: 080 ----
mean loss: 412.29
 ---- batch: 090 ----
mean loss: 410.98
train mean loss: 406.03
epoch train time: 0:00:02.347521
elapsed time: 0:01:25.022373
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 20:41:32.075186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.90
 ---- batch: 020 ----
mean loss: 405.27
 ---- batch: 030 ----
mean loss: 390.64
 ---- batch: 040 ----
mean loss: 415.86
 ---- batch: 050 ----
mean loss: 415.60
 ---- batch: 060 ----
mean loss: 422.54
 ---- batch: 070 ----
mean loss: 405.27
 ---- batch: 080 ----
mean loss: 417.12
 ---- batch: 090 ----
mean loss: 391.62
train mean loss: 407.13
epoch train time: 0:00:02.337807
elapsed time: 0:01:27.360422
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 20:41:34.413224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.28
 ---- batch: 020 ----
mean loss: 410.99
 ---- batch: 030 ----
mean loss: 415.97
 ---- batch: 040 ----
mean loss: 408.99
 ---- batch: 050 ----
mean loss: 428.22
 ---- batch: 060 ----
mean loss: 422.99
 ---- batch: 070 ----
mean loss: 409.31
 ---- batch: 080 ----
mean loss: 410.39
 ---- batch: 090 ----
mean loss: 398.18
train mean loss: 411.97
epoch train time: 0:00:02.338364
elapsed time: 0:01:29.699040
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 20:41:36.751823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.59
 ---- batch: 020 ----
mean loss: 416.77
 ---- batch: 030 ----
mean loss: 404.10
 ---- batch: 040 ----
mean loss: 405.09
 ---- batch: 050 ----
mean loss: 403.10
 ---- batch: 060 ----
mean loss: 395.78
 ---- batch: 070 ----
mean loss: 396.72
 ---- batch: 080 ----
mean loss: 393.71
 ---- batch: 090 ----
mean loss: 398.24
train mean loss: 403.38
epoch train time: 0:00:02.327167
elapsed time: 0:01:32.026410
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 20:41:39.079183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.13
 ---- batch: 020 ----
mean loss: 394.68
 ---- batch: 030 ----
mean loss: 387.43
 ---- batch: 040 ----
mean loss: 402.53
 ---- batch: 050 ----
mean loss: 389.22
 ---- batch: 060 ----
mean loss: 397.14
 ---- batch: 070 ----
mean loss: 392.85
 ---- batch: 080 ----
mean loss: 409.87
 ---- batch: 090 ----
mean loss: 401.72
train mean loss: 398.26
epoch train time: 0:00:02.336569
elapsed time: 0:01:34.363197
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 20:41:41.415981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.59
 ---- batch: 020 ----
mean loss: 449.23
 ---- batch: 030 ----
mean loss: 429.77
 ---- batch: 040 ----
mean loss: 411.65
 ---- batch: 050 ----
mean loss: 399.45
 ---- batch: 060 ----
mean loss: 396.47
 ---- batch: 070 ----
mean loss: 390.96
 ---- batch: 080 ----
mean loss: 403.72
 ---- batch: 090 ----
mean loss: 398.77
train mean loss: 410.08
epoch train time: 0:00:02.333488
elapsed time: 0:01:36.696918
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 20:41:43.749688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.78
 ---- batch: 020 ----
mean loss: 419.37
 ---- batch: 030 ----
mean loss: 412.25
 ---- batch: 040 ----
mean loss: 398.04
 ---- batch: 050 ----
mean loss: 412.31
 ---- batch: 060 ----
mean loss: 408.61
 ---- batch: 070 ----
mean loss: 396.18
 ---- batch: 080 ----
mean loss: 402.67
 ---- batch: 090 ----
mean loss: 396.38
train mean loss: 409.40
epoch train time: 0:00:02.342430
elapsed time: 0:01:39.039550
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 20:41:46.092319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.73
 ---- batch: 020 ----
mean loss: 398.21
 ---- batch: 030 ----
mean loss: 393.39
 ---- batch: 040 ----
mean loss: 404.50
 ---- batch: 050 ----
mean loss: 402.95
 ---- batch: 060 ----
mean loss: 392.73
 ---- batch: 070 ----
mean loss: 412.27
 ---- batch: 080 ----
mean loss: 403.63
 ---- batch: 090 ----
mean loss: 388.53
train mean loss: 399.41
epoch train time: 0:00:02.338765
elapsed time: 0:01:41.378493
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 20:41:48.431260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.58
 ---- batch: 020 ----
mean loss: 412.69
 ---- batch: 030 ----
mean loss: 390.77
 ---- batch: 040 ----
mean loss: 395.63
 ---- batch: 050 ----
mean loss: 430.17
 ---- batch: 060 ----
mean loss: 424.73
 ---- batch: 070 ----
mean loss: 419.44
 ---- batch: 080 ----
mean loss: 411.00
 ---- batch: 090 ----
mean loss: 411.49
train mean loss: 414.63
epoch train time: 0:00:02.338331
elapsed time: 0:01:43.717015
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 20:41:50.769801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.18
 ---- batch: 020 ----
mean loss: 398.05
 ---- batch: 030 ----
mean loss: 411.27
 ---- batch: 040 ----
mean loss: 411.37
 ---- batch: 050 ----
mean loss: 410.44
 ---- batch: 060 ----
mean loss: 405.34
 ---- batch: 070 ----
mean loss: 409.83
 ---- batch: 080 ----
mean loss: 408.40
 ---- batch: 090 ----
mean loss: 416.00
train mean loss: 409.55
epoch train time: 0:00:02.337536
elapsed time: 0:01:46.054744
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 20:41:53.107513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.06
 ---- batch: 020 ----
mean loss: 409.59
 ---- batch: 030 ----
mean loss: 408.66
 ---- batch: 040 ----
mean loss: 410.20
 ---- batch: 050 ----
mean loss: 396.79
 ---- batch: 060 ----
mean loss: 406.18
 ---- batch: 070 ----
mean loss: 403.50
 ---- batch: 080 ----
mean loss: 422.64
 ---- batch: 090 ----
mean loss: 413.26
train mean loss: 409.18
epoch train time: 0:00:02.333432
elapsed time: 0:01:48.388362
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 20:41:55.441130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.77
 ---- batch: 020 ----
mean loss: 407.32
 ---- batch: 030 ----
mean loss: 397.09
 ---- batch: 040 ----
mean loss: 403.68
 ---- batch: 050 ----
mean loss: 396.83
 ---- batch: 060 ----
mean loss: 400.31
 ---- batch: 070 ----
mean loss: 415.94
 ---- batch: 080 ----
mean loss: 391.80
 ---- batch: 090 ----
mean loss: 384.56
train mean loss: 401.84
epoch train time: 0:00:02.323102
elapsed time: 0:01:50.711651
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 20:41:57.764419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.82
 ---- batch: 020 ----
mean loss: 408.38
 ---- batch: 030 ----
mean loss: 410.22
 ---- batch: 040 ----
mean loss: 407.54
 ---- batch: 050 ----
mean loss: 396.21
 ---- batch: 060 ----
mean loss: 393.27
 ---- batch: 070 ----
mean loss: 399.69
 ---- batch: 080 ----
mean loss: 401.80
 ---- batch: 090 ----
mean loss: 414.66
train mean loss: 404.77
epoch train time: 0:00:02.348464
elapsed time: 0:01:53.060333
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 20:42:00.113168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.97
 ---- batch: 020 ----
mean loss: 401.62
 ---- batch: 030 ----
mean loss: 398.13
 ---- batch: 040 ----
mean loss: 389.74
 ---- batch: 050 ----
mean loss: 403.89
 ---- batch: 060 ----
mean loss: 401.92
 ---- batch: 070 ----
mean loss: 394.04
 ---- batch: 080 ----
mean loss: 397.01
 ---- batch: 090 ----
mean loss: 406.29
train mean loss: 398.44
epoch train time: 0:00:02.330264
elapsed time: 0:01:55.390864
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 20:42:02.443629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.23
 ---- batch: 020 ----
mean loss: 397.88
 ---- batch: 030 ----
mean loss: 394.72
 ---- batch: 040 ----
mean loss: 388.38
 ---- batch: 050 ----
mean loss: 396.88
 ---- batch: 060 ----
mean loss: 384.03
 ---- batch: 070 ----
mean loss: 389.06
 ---- batch: 080 ----
mean loss: 391.37
 ---- batch: 090 ----
mean loss: 401.13
train mean loss: 395.58
epoch train time: 0:00:02.330072
elapsed time: 0:01:57.721157
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 20:42:04.773958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.17
 ---- batch: 020 ----
mean loss: 402.22
 ---- batch: 030 ----
mean loss: 405.07
 ---- batch: 040 ----
mean loss: 398.06
 ---- batch: 050 ----
mean loss: 397.19
 ---- batch: 060 ----
mean loss: 391.52
 ---- batch: 070 ----
mean loss: 394.84
 ---- batch: 080 ----
mean loss: 384.43
 ---- batch: 090 ----
mean loss: 397.64
train mean loss: 397.08
epoch train time: 0:00:02.340558
elapsed time: 0:02:00.061940
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 20:42:07.114711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.82
 ---- batch: 020 ----
mean loss: 395.95
 ---- batch: 030 ----
mean loss: 386.97
 ---- batch: 040 ----
mean loss: 403.43
 ---- batch: 050 ----
mean loss: 397.98
 ---- batch: 060 ----
mean loss: 401.85
 ---- batch: 070 ----
mean loss: 391.31
 ---- batch: 080 ----
mean loss: 403.39
 ---- batch: 090 ----
mean loss: 409.47
train mean loss: 397.81
epoch train time: 0:00:02.336394
elapsed time: 0:02:02.398514
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 20:42:09.451301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.62
 ---- batch: 020 ----
mean loss: 399.67
 ---- batch: 030 ----
mean loss: 395.22
 ---- batch: 040 ----
mean loss: 395.94
 ---- batch: 050 ----
mean loss: 402.52
 ---- batch: 060 ----
mean loss: 401.44
 ---- batch: 070 ----
mean loss: 409.69
 ---- batch: 080 ----
mean loss: 396.29
 ---- batch: 090 ----
mean loss: 402.99
train mean loss: 399.22
epoch train time: 0:00:02.335639
elapsed time: 0:02:04.734356
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 20:42:11.787125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.41
 ---- batch: 020 ----
mean loss: 391.54
 ---- batch: 030 ----
mean loss: 404.94
 ---- batch: 040 ----
mean loss: 405.09
 ---- batch: 050 ----
mean loss: 405.09
 ---- batch: 060 ----
mean loss: 389.28
 ---- batch: 070 ----
mean loss: 392.69
 ---- batch: 080 ----
mean loss: 393.18
 ---- batch: 090 ----
mean loss: 390.76
train mean loss: 397.17
epoch train time: 0:00:02.340927
elapsed time: 0:02:07.075467
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 20:42:14.128236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.43
 ---- batch: 020 ----
mean loss: 386.64
 ---- batch: 030 ----
mean loss: 399.09
 ---- batch: 040 ----
mean loss: 411.36
 ---- batch: 050 ----
mean loss: 400.29
 ---- batch: 060 ----
mean loss: 390.03
 ---- batch: 070 ----
mean loss: 396.05
 ---- batch: 080 ----
mean loss: 415.51
 ---- batch: 090 ----
mean loss: 423.12
train mean loss: 404.19
epoch train time: 0:00:02.337670
elapsed time: 0:02:09.413323
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 20:42:16.466091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.48
 ---- batch: 020 ----
mean loss: 394.83
 ---- batch: 030 ----
mean loss: 417.74
 ---- batch: 040 ----
mean loss: 405.57
 ---- batch: 050 ----
mean loss: 417.74
 ---- batch: 060 ----
mean loss: 402.76
 ---- batch: 070 ----
mean loss: 398.87
 ---- batch: 080 ----
mean loss: 410.80
 ---- batch: 090 ----
mean loss: 383.22
train mean loss: 403.69
epoch train time: 0:00:02.323321
elapsed time: 0:02:11.736835
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 20:42:18.789619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.09
 ---- batch: 020 ----
mean loss: 403.85
 ---- batch: 030 ----
mean loss: 420.37
 ---- batch: 040 ----
mean loss: 425.13
 ---- batch: 050 ----
mean loss: 399.45
 ---- batch: 060 ----
mean loss: 386.28
 ---- batch: 070 ----
mean loss: 393.60
 ---- batch: 080 ----
mean loss: 411.23
 ---- batch: 090 ----
mean loss: 389.78
train mean loss: 403.81
epoch train time: 0:00:02.327192
elapsed time: 0:02:14.064222
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 20:42:21.116991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.51
 ---- batch: 020 ----
mean loss: 420.31
 ---- batch: 030 ----
mean loss: 401.89
 ---- batch: 040 ----
mean loss: 401.78
 ---- batch: 050 ----
mean loss: 404.48
 ---- batch: 060 ----
mean loss: 394.66
 ---- batch: 070 ----
mean loss: 393.91
 ---- batch: 080 ----
mean loss: 389.15
 ---- batch: 090 ----
mean loss: 396.21
train mean loss: 400.49
epoch train time: 0:00:02.335169
elapsed time: 0:02:16.399569
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 20:42:23.452337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.76
 ---- batch: 020 ----
mean loss: 409.42
 ---- batch: 030 ----
mean loss: 408.66
 ---- batch: 040 ----
mean loss: 392.30
 ---- batch: 050 ----
mean loss: 412.39
 ---- batch: 060 ----
mean loss: 402.66
 ---- batch: 070 ----
mean loss: 399.61
 ---- batch: 080 ----
mean loss: 407.31
 ---- batch: 090 ----
mean loss: 404.20
train mean loss: 407.16
epoch train time: 0:00:02.334426
elapsed time: 0:02:18.734185
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 20:42:25.786986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.08
 ---- batch: 020 ----
mean loss: 407.54
 ---- batch: 030 ----
mean loss: 389.26
 ---- batch: 040 ----
mean loss: 395.74
 ---- batch: 050 ----
mean loss: 401.85
 ---- batch: 060 ----
mean loss: 394.87
 ---- batch: 070 ----
mean loss: 416.87
 ---- batch: 080 ----
mean loss: 415.79
 ---- batch: 090 ----
mean loss: 410.40
train mean loss: 405.14
epoch train time: 0:00:02.330478
elapsed time: 0:02:21.064877
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 20:42:28.117668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.14
 ---- batch: 020 ----
mean loss: 389.99
 ---- batch: 030 ----
mean loss: 393.50
 ---- batch: 040 ----
mean loss: 387.77
 ---- batch: 050 ----
mean loss: 396.15
 ---- batch: 060 ----
mean loss: 400.04
 ---- batch: 070 ----
mean loss: 401.57
 ---- batch: 080 ----
mean loss: 406.96
 ---- batch: 090 ----
mean loss: 410.85
train mean loss: 399.52
epoch train time: 0:00:02.336600
elapsed time: 0:02:23.401743
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 20:42:30.454525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.31
 ---- batch: 020 ----
mean loss: 409.09
 ---- batch: 030 ----
mean loss: 409.65
 ---- batch: 040 ----
mean loss: 398.17
 ---- batch: 050 ----
mean loss: 402.37
 ---- batch: 060 ----
mean loss: 407.87
 ---- batch: 070 ----
mean loss: 413.75
 ---- batch: 080 ----
mean loss: 404.69
 ---- batch: 090 ----
mean loss: 402.87
train mean loss: 406.68
epoch train time: 0:00:02.324546
elapsed time: 0:02:25.726493
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 20:42:32.779266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.31
 ---- batch: 020 ----
mean loss: 402.28
 ---- batch: 030 ----
mean loss: 388.85
 ---- batch: 040 ----
mean loss: 384.51
 ---- batch: 050 ----
mean loss: 397.04
 ---- batch: 060 ----
mean loss: 388.89
 ---- batch: 070 ----
mean loss: 411.00
 ---- batch: 080 ----
mean loss: 409.95
 ---- batch: 090 ----
mean loss: 400.91
train mean loss: 399.51
epoch train time: 0:00:02.332353
elapsed time: 0:02:28.059066
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 20:42:35.111834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.22
 ---- batch: 020 ----
mean loss: 387.58
 ---- batch: 030 ----
mean loss: 424.34
 ---- batch: 040 ----
mean loss: 398.67
 ---- batch: 050 ----
mean loss: 397.09
 ---- batch: 060 ----
mean loss: 403.75
 ---- batch: 070 ----
mean loss: 397.20
 ---- batch: 080 ----
mean loss: 397.81
 ---- batch: 090 ----
mean loss: 392.16
train mean loss: 399.09
epoch train time: 0:00:02.338781
elapsed time: 0:02:30.398025
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 20:42:37.450793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.15
 ---- batch: 020 ----
mean loss: 388.08
 ---- batch: 030 ----
mean loss: 407.10
 ---- batch: 040 ----
mean loss: 413.03
 ---- batch: 050 ----
mean loss: 390.24
 ---- batch: 060 ----
mean loss: 404.93
 ---- batch: 070 ----
mean loss: 396.50
 ---- batch: 080 ----
mean loss: 395.06
 ---- batch: 090 ----
mean loss: 392.78
train mean loss: 397.87
epoch train time: 0:00:02.330393
elapsed time: 0:02:32.728606
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 20:42:39.781375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.45
 ---- batch: 020 ----
mean loss: 394.04
 ---- batch: 030 ----
mean loss: 403.11
 ---- batch: 040 ----
mean loss: 396.84
 ---- batch: 050 ----
mean loss: 383.86
 ---- batch: 060 ----
mean loss: 405.97
 ---- batch: 070 ----
mean loss: 396.88
 ---- batch: 080 ----
mean loss: 400.44
 ---- batch: 090 ----
mean loss: 415.80
train mean loss: 399.94
epoch train time: 0:00:02.336004
elapsed time: 0:02:35.064789
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 20:42:42.117558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.12
 ---- batch: 020 ----
mean loss: 412.74
 ---- batch: 030 ----
mean loss: 405.04
 ---- batch: 040 ----
mean loss: 397.95
 ---- batch: 050 ----
mean loss: 393.55
 ---- batch: 060 ----
mean loss: 395.56
 ---- batch: 070 ----
mean loss: 385.05
 ---- batch: 080 ----
mean loss: 397.25
 ---- batch: 090 ----
mean loss: 389.46
train mean loss: 401.88
epoch train time: 0:00:02.333484
elapsed time: 0:02:37.398463
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 20:42:44.451251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.13
 ---- batch: 020 ----
mean loss: 389.86
 ---- batch: 030 ----
mean loss: 410.04
 ---- batch: 040 ----
mean loss: 402.34
 ---- batch: 050 ----
mean loss: 395.56
 ---- batch: 060 ----
mean loss: 377.80
 ---- batch: 070 ----
mean loss: 404.98
 ---- batch: 080 ----
mean loss: 405.18
 ---- batch: 090 ----
mean loss: 383.20
train mean loss: 395.14
epoch train time: 0:00:02.325285
elapsed time: 0:02:39.723989
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 20:42:46.776785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.55
 ---- batch: 020 ----
mean loss: 386.87
 ---- batch: 030 ----
mean loss: 386.03
 ---- batch: 040 ----
mean loss: 389.31
 ---- batch: 050 ----
mean loss: 389.90
 ---- batch: 060 ----
mean loss: 395.45
 ---- batch: 070 ----
mean loss: 395.44
 ---- batch: 080 ----
mean loss: 390.62
 ---- batch: 090 ----
mean loss: 395.76
train mean loss: 391.62
epoch train time: 0:00:02.338553
elapsed time: 0:02:42.062759
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 20:42:49.115553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.78
 ---- batch: 020 ----
mean loss: 411.54
 ---- batch: 030 ----
mean loss: 412.17
 ---- batch: 040 ----
mean loss: 392.65
 ---- batch: 050 ----
mean loss: 385.98
 ---- batch: 060 ----
mean loss: 395.14
 ---- batch: 070 ----
mean loss: 398.81
 ---- batch: 080 ----
mean loss: 404.43
 ---- batch: 090 ----
mean loss: 394.77
train mean loss: 400.14
epoch train time: 0:00:02.328125
elapsed time: 0:02:44.391107
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 20:42:51.443877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.27
 ---- batch: 020 ----
mean loss: 396.40
 ---- batch: 030 ----
mean loss: 408.17
 ---- batch: 040 ----
mean loss: 411.70
 ---- batch: 050 ----
mean loss: 400.07
 ---- batch: 060 ----
mean loss: 397.95
 ---- batch: 070 ----
mean loss: 398.34
 ---- batch: 080 ----
mean loss: 413.16
 ---- batch: 090 ----
mean loss: 393.20
train mean loss: 400.97
epoch train time: 0:00:02.337224
elapsed time: 0:02:46.728515
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 20:42:53.781282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.50
 ---- batch: 020 ----
mean loss: 388.54
 ---- batch: 030 ----
mean loss: 394.49
 ---- batch: 040 ----
mean loss: 383.90
 ---- batch: 050 ----
mean loss: 399.57
 ---- batch: 060 ----
mean loss: 410.83
 ---- batch: 070 ----
mean loss: 388.77
 ---- batch: 080 ----
mean loss: 391.61
 ---- batch: 090 ----
mean loss: 403.21
train mean loss: 393.91
epoch train time: 0:00:02.336928
elapsed time: 0:02:49.065651
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 20:42:56.118417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.60
 ---- batch: 020 ----
mean loss: 415.09
 ---- batch: 030 ----
mean loss: 400.76
 ---- batch: 040 ----
mean loss: 392.67
 ---- batch: 050 ----
mean loss: 388.70
 ---- batch: 060 ----
mean loss: 387.98
 ---- batch: 070 ----
mean loss: 401.89
 ---- batch: 080 ----
mean loss: 405.06
 ---- batch: 090 ----
mean loss: 392.27
train mean loss: 398.25
epoch train time: 0:00:02.334108
elapsed time: 0:02:51.399936
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 20:42:58.452714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.98
 ---- batch: 020 ----
mean loss: 400.75
 ---- batch: 030 ----
mean loss: 387.87
 ---- batch: 040 ----
mean loss: 401.13
 ---- batch: 050 ----
mean loss: 403.57
 ---- batch: 060 ----
mean loss: 392.49
 ---- batch: 070 ----
mean loss: 385.14
 ---- batch: 080 ----
mean loss: 397.50
 ---- batch: 090 ----
mean loss: 395.66
train mean loss: 395.22
epoch train time: 0:00:02.326547
elapsed time: 0:02:53.726680
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 20:43:00.779469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.45
 ---- batch: 020 ----
mean loss: 375.07
 ---- batch: 030 ----
mean loss: 394.86
 ---- batch: 040 ----
mean loss: 397.01
 ---- batch: 050 ----
mean loss: 395.80
 ---- batch: 060 ----
mean loss: 392.75
 ---- batch: 070 ----
mean loss: 393.79
 ---- batch: 080 ----
mean loss: 406.55
 ---- batch: 090 ----
mean loss: 400.39
train mean loss: 393.07
epoch train time: 0:00:02.334080
elapsed time: 0:02:56.061002
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 20:43:03.113774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.57
 ---- batch: 020 ----
mean loss: 384.64
 ---- batch: 030 ----
mean loss: 395.55
 ---- batch: 040 ----
mean loss: 390.32
 ---- batch: 050 ----
mean loss: 391.48
 ---- batch: 060 ----
mean loss: 400.24
 ---- batch: 070 ----
mean loss: 388.34
 ---- batch: 080 ----
mean loss: 386.88
 ---- batch: 090 ----
mean loss: 406.38
train mean loss: 393.56
epoch train time: 0:00:02.334034
elapsed time: 0:02:58.395222
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 20:43:05.447991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.98
 ---- batch: 020 ----
mean loss: 394.06
 ---- batch: 030 ----
mean loss: 394.59
 ---- batch: 040 ----
mean loss: 379.70
 ---- batch: 050 ----
mean loss: 384.94
 ---- batch: 060 ----
mean loss: 393.82
 ---- batch: 070 ----
mean loss: 419.29
 ---- batch: 080 ----
mean loss: 427.42
 ---- batch: 090 ----
mean loss: 407.70
train mean loss: 400.35
epoch train time: 0:00:02.326874
elapsed time: 0:03:00.722284
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 20:43:07.775053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.46
 ---- batch: 020 ----
mean loss: 393.56
 ---- batch: 030 ----
mean loss: 391.30
 ---- batch: 040 ----
mean loss: 396.63
 ---- batch: 050 ----
mean loss: 414.04
 ---- batch: 060 ----
mean loss: 408.37
 ---- batch: 070 ----
mean loss: 394.78
 ---- batch: 080 ----
mean loss: 394.00
 ---- batch: 090 ----
mean loss: 387.01
train mean loss: 397.84
epoch train time: 0:00:02.329877
elapsed time: 0:03:03.052349
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 20:43:10.105115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.62
 ---- batch: 020 ----
mean loss: 407.94
 ---- batch: 030 ----
mean loss: 398.87
 ---- batch: 040 ----
mean loss: 400.66
 ---- batch: 050 ----
mean loss: 403.17
 ---- batch: 060 ----
mean loss: 398.67
 ---- batch: 070 ----
mean loss: 378.29
 ---- batch: 080 ----
mean loss: 402.47
 ---- batch: 090 ----
mean loss: 396.05
train mean loss: 399.50
epoch train time: 0:00:02.333555
elapsed time: 0:03:05.386088
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 20:43:12.438856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.35
 ---- batch: 020 ----
mean loss: 395.67
 ---- batch: 030 ----
mean loss: 395.42
 ---- batch: 040 ----
mean loss: 397.43
 ---- batch: 050 ----
mean loss: 400.46
 ---- batch: 060 ----
mean loss: 386.21
 ---- batch: 070 ----
mean loss: 384.39
 ---- batch: 080 ----
mean loss: 397.05
 ---- batch: 090 ----
mean loss: 387.07
train mean loss: 392.12
epoch train time: 0:00:02.333232
elapsed time: 0:03:07.719503
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 20:43:14.772271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.71
 ---- batch: 020 ----
mean loss: 394.45
 ---- batch: 030 ----
mean loss: 402.39
 ---- batch: 040 ----
mean loss: 401.69
 ---- batch: 050 ----
mean loss: 401.20
 ---- batch: 060 ----
mean loss: 384.52
 ---- batch: 070 ----
mean loss: 381.46
 ---- batch: 080 ----
mean loss: 386.71
 ---- batch: 090 ----
mean loss: 375.40
train mean loss: 391.61
epoch train time: 0:00:02.336895
elapsed time: 0:03:10.056579
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 20:43:17.109347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.35
 ---- batch: 020 ----
mean loss: 393.67
 ---- batch: 030 ----
mean loss: 398.77
 ---- batch: 040 ----
mean loss: 382.20
 ---- batch: 050 ----
mean loss: 397.59
 ---- batch: 060 ----
mean loss: 396.28
 ---- batch: 070 ----
mean loss: 392.82
 ---- batch: 080 ----
mean loss: 405.93
 ---- batch: 090 ----
mean loss: 388.59
train mean loss: 394.11
epoch train time: 0:00:02.332130
elapsed time: 0:03:12.388890
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 20:43:19.441693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.09
 ---- batch: 020 ----
mean loss: 387.80
 ---- batch: 030 ----
mean loss: 397.59
 ---- batch: 040 ----
mean loss: 386.00
 ---- batch: 050 ----
mean loss: 402.08
 ---- batch: 060 ----
mean loss: 389.43
 ---- batch: 070 ----
mean loss: 396.36
 ---- batch: 080 ----
mean loss: 379.86
 ---- batch: 090 ----
mean loss: 394.60
train mean loss: 393.12
epoch train time: 0:00:02.333157
elapsed time: 0:03:14.722280
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 20:43:21.775063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.94
 ---- batch: 020 ----
mean loss: 395.10
 ---- batch: 030 ----
mean loss: 402.60
 ---- batch: 040 ----
mean loss: 396.99
 ---- batch: 050 ----
mean loss: 399.42
 ---- batch: 060 ----
mean loss: 379.94
 ---- batch: 070 ----
mean loss: 380.54
 ---- batch: 080 ----
mean loss: 406.47
 ---- batch: 090 ----
mean loss: 402.74
train mean loss: 399.54
epoch train time: 0:00:02.332228
elapsed time: 0:03:17.054720
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 20:43:24.107490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.60
 ---- batch: 020 ----
mean loss: 382.41
 ---- batch: 030 ----
mean loss: 394.26
 ---- batch: 040 ----
mean loss: 418.78
 ---- batch: 050 ----
mean loss: 395.57
 ---- batch: 060 ----
mean loss: 377.13
 ---- batch: 070 ----
mean loss: 413.44
 ---- batch: 080 ----
mean loss: 394.68
 ---- batch: 090 ----
mean loss: 398.56
train mean loss: 396.97
epoch train time: 0:00:02.336105
elapsed time: 0:03:19.391019
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 20:43:26.443785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.10
 ---- batch: 020 ----
mean loss: 379.40
 ---- batch: 030 ----
mean loss: 402.19
 ---- batch: 040 ----
mean loss: 387.31
 ---- batch: 050 ----
mean loss: 398.55
 ---- batch: 060 ----
mean loss: 397.47
 ---- batch: 070 ----
mean loss: 389.91
 ---- batch: 080 ----
mean loss: 387.43
 ---- batch: 090 ----
mean loss: 396.42
train mean loss: 391.47
epoch train time: 0:00:02.331820
elapsed time: 0:03:21.723098
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 20:43:28.775880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.46
 ---- batch: 020 ----
mean loss: 387.53
 ---- batch: 030 ----
mean loss: 397.24
 ---- batch: 040 ----
mean loss: 390.57
 ---- batch: 050 ----
mean loss: 392.42
 ---- batch: 060 ----
mean loss: 390.45
 ---- batch: 070 ----
mean loss: 393.22
 ---- batch: 080 ----
mean loss: 394.43
 ---- batch: 090 ----
mean loss: 388.67
train mean loss: 389.61
epoch train time: 0:00:02.331107
elapsed time: 0:03:24.054404
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 20:43:31.107173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.30
 ---- batch: 020 ----
mean loss: 395.67
 ---- batch: 030 ----
mean loss: 396.76
 ---- batch: 040 ----
mean loss: 383.69
 ---- batch: 050 ----
mean loss: 377.57
 ---- batch: 060 ----
mean loss: 387.83
 ---- batch: 070 ----
mean loss: 403.77
 ---- batch: 080 ----
mean loss: 389.15
 ---- batch: 090 ----
mean loss: 392.90
train mean loss: 391.65
epoch train time: 0:00:02.334227
elapsed time: 0:03:26.388807
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 20:43:33.441639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.74
 ---- batch: 020 ----
mean loss: 383.40
 ---- batch: 030 ----
mean loss: 392.51
 ---- batch: 040 ----
mean loss: 404.08
 ---- batch: 050 ----
mean loss: 385.62
 ---- batch: 060 ----
mean loss: 389.92
 ---- batch: 070 ----
mean loss: 384.62
 ---- batch: 080 ----
mean loss: 408.02
 ---- batch: 090 ----
mean loss: 400.18
train mean loss: 392.49
epoch train time: 0:00:02.324646
elapsed time: 0:03:28.713699
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 20:43:35.766483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.71
 ---- batch: 020 ----
mean loss: 398.31
 ---- batch: 030 ----
mean loss: 404.97
 ---- batch: 040 ----
mean loss: 393.37
 ---- batch: 050 ----
mean loss: 375.18
 ---- batch: 060 ----
mean loss: 386.79
 ---- batch: 070 ----
mean loss: 393.68
 ---- batch: 080 ----
mean loss: 393.30
 ---- batch: 090 ----
mean loss: 406.54
train mean loss: 394.29
epoch train time: 0:00:02.324998
elapsed time: 0:03:31.038899
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 20:43:38.091668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.98
 ---- batch: 020 ----
mean loss: 405.85
 ---- batch: 030 ----
mean loss: 390.46
 ---- batch: 040 ----
mean loss: 390.00
 ---- batch: 050 ----
mean loss: 395.44
 ---- batch: 060 ----
mean loss: 384.81
 ---- batch: 070 ----
mean loss: 393.29
 ---- batch: 080 ----
mean loss: 402.93
 ---- batch: 090 ----
mean loss: 402.00
train mean loss: 397.32
epoch train time: 0:00:02.346811
elapsed time: 0:03:33.385893
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 20:43:40.438660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.51
 ---- batch: 020 ----
mean loss: 400.86
 ---- batch: 030 ----
mean loss: 404.21
 ---- batch: 040 ----
mean loss: 404.67
 ---- batch: 050 ----
mean loss: 386.25
 ---- batch: 060 ----
mean loss: 382.93
 ---- batch: 070 ----
mean loss: 390.31
 ---- batch: 080 ----
mean loss: 391.76
 ---- batch: 090 ----
mean loss: 392.16
train mean loss: 393.52
epoch train time: 0:00:02.329472
elapsed time: 0:03:35.715545
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 20:43:42.768314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.99
 ---- batch: 020 ----
mean loss: 387.21
 ---- batch: 030 ----
mean loss: 412.41
 ---- batch: 040 ----
mean loss: 394.02
 ---- batch: 050 ----
mean loss: 400.51
 ---- batch: 060 ----
mean loss: 391.58
 ---- batch: 070 ----
mean loss: 404.01
 ---- batch: 080 ----
mean loss: 396.25
 ---- batch: 090 ----
mean loss: 389.54
train mean loss: 395.76
epoch train time: 0:00:02.320804
elapsed time: 0:03:38.036530
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 20:43:45.089297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.30
 ---- batch: 020 ----
mean loss: 382.16
 ---- batch: 030 ----
mean loss: 387.48
 ---- batch: 040 ----
mean loss: 388.16
 ---- batch: 050 ----
mean loss: 396.32
 ---- batch: 060 ----
mean loss: 403.20
 ---- batch: 070 ----
mean loss: 396.85
 ---- batch: 080 ----
mean loss: 415.00
 ---- batch: 090 ----
mean loss: 419.08
train mean loss: 398.95
epoch train time: 0:00:02.322531
elapsed time: 0:03:40.359256
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 20:43:47.412038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.15
 ---- batch: 020 ----
mean loss: 391.84
 ---- batch: 030 ----
mean loss: 406.64
 ---- batch: 040 ----
mean loss: 394.08
 ---- batch: 050 ----
mean loss: 388.97
 ---- batch: 060 ----
mean loss: 390.33
 ---- batch: 070 ----
mean loss: 395.54
 ---- batch: 080 ----
mean loss: 396.02
 ---- batch: 090 ----
mean loss: 392.97
train mean loss: 395.32
epoch train time: 0:00:02.326805
elapsed time: 0:03:42.686256
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 20:43:49.739023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.02
 ---- batch: 020 ----
mean loss: 386.90
 ---- batch: 030 ----
mean loss: 402.36
 ---- batch: 040 ----
mean loss: 391.06
 ---- batch: 050 ----
mean loss: 394.21
 ---- batch: 060 ----
mean loss: 396.25
 ---- batch: 070 ----
mean loss: 390.58
 ---- batch: 080 ----
mean loss: 399.36
 ---- batch: 090 ----
mean loss: 404.59
train mean loss: 396.11
epoch train time: 0:00:02.327889
elapsed time: 0:03:45.014324
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 20:43:52.067094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.91
 ---- batch: 020 ----
mean loss: 392.17
 ---- batch: 030 ----
mean loss: 381.85
 ---- batch: 040 ----
mean loss: 399.33
 ---- batch: 050 ----
mean loss: 396.91
 ---- batch: 060 ----
mean loss: 392.62
 ---- batch: 070 ----
mean loss: 372.30
 ---- batch: 080 ----
mean loss: 400.95
 ---- batch: 090 ----
mean loss: 389.29
train mean loss: 394.32
epoch train time: 0:00:02.323562
elapsed time: 0:03:47.338077
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 20:43:54.390880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.17
 ---- batch: 020 ----
mean loss: 383.55
 ---- batch: 030 ----
mean loss: 392.05
 ---- batch: 040 ----
mean loss: 396.55
 ---- batch: 050 ----
mean loss: 389.41
 ---- batch: 060 ----
mean loss: 383.46
 ---- batch: 070 ----
mean loss: 387.30
 ---- batch: 080 ----
mean loss: 389.89
 ---- batch: 090 ----
mean loss: 394.73
train mean loss: 390.80
epoch train time: 0:00:02.331032
elapsed time: 0:03:49.669338
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 20:43:56.722113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.46
 ---- batch: 020 ----
mean loss: 398.94
 ---- batch: 030 ----
mean loss: 392.08
 ---- batch: 040 ----
mean loss: 379.94
 ---- batch: 050 ----
mean loss: 394.99
 ---- batch: 060 ----
mean loss: 387.89
 ---- batch: 070 ----
mean loss: 400.75
 ---- batch: 080 ----
mean loss: 405.49
 ---- batch: 090 ----
mean loss: 395.80
train mean loss: 393.64
epoch train time: 0:00:02.324826
elapsed time: 0:03:51.994358
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 20:43:59.047129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.62
 ---- batch: 020 ----
mean loss: 391.37
 ---- batch: 030 ----
mean loss: 395.19
 ---- batch: 040 ----
mean loss: 400.81
 ---- batch: 050 ----
mean loss: 391.22
 ---- batch: 060 ----
mean loss: 387.34
 ---- batch: 070 ----
mean loss: 374.00
 ---- batch: 080 ----
mean loss: 385.02
 ---- batch: 090 ----
mean loss: 396.50
train mean loss: 390.78
epoch train time: 0:00:02.325031
elapsed time: 0:03:54.319576
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 20:44:01.372343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.98
 ---- batch: 020 ----
mean loss: 376.75
 ---- batch: 030 ----
mean loss: 386.34
 ---- batch: 040 ----
mean loss: 395.16
 ---- batch: 050 ----
mean loss: 399.04
 ---- batch: 060 ----
mean loss: 394.85
 ---- batch: 070 ----
mean loss: 383.22
 ---- batch: 080 ----
mean loss: 395.67
 ---- batch: 090 ----
mean loss: 378.73
train mean loss: 390.25
epoch train time: 0:00:02.335338
elapsed time: 0:03:56.655089
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 20:44:03.707877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.96
 ---- batch: 020 ----
mean loss: 391.26
 ---- batch: 030 ----
mean loss: 402.17
 ---- batch: 040 ----
mean loss: 384.91
 ---- batch: 050 ----
mean loss: 385.00
 ---- batch: 060 ----
mean loss: 390.10
 ---- batch: 070 ----
mean loss: 394.75
 ---- batch: 080 ----
mean loss: 387.07
 ---- batch: 090 ----
mean loss: 373.94
train mean loss: 387.85
epoch train time: 0:00:02.329456
elapsed time: 0:03:58.984746
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 20:44:06.037516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.28
 ---- batch: 020 ----
mean loss: 390.95
 ---- batch: 030 ----
mean loss: 392.85
 ---- batch: 040 ----
mean loss: 389.35
 ---- batch: 050 ----
mean loss: 383.10
 ---- batch: 060 ----
mean loss: 391.53
 ---- batch: 070 ----
mean loss: 389.21
 ---- batch: 080 ----
mean loss: 379.61
 ---- batch: 090 ----
mean loss: 392.41
train mean loss: 390.06
epoch train time: 0:00:02.327351
elapsed time: 0:04:01.312284
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 20:44:08.365055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.30
 ---- batch: 020 ----
mean loss: 400.98
 ---- batch: 030 ----
mean loss: 387.27
 ---- batch: 040 ----
mean loss: 393.86
 ---- batch: 050 ----
mean loss: 391.82
 ---- batch: 060 ----
mean loss: 387.90
 ---- batch: 070 ----
mean loss: 385.74
 ---- batch: 080 ----
mean loss: 388.52
 ---- batch: 090 ----
mean loss: 390.54
train mean loss: 390.86
epoch train time: 0:00:02.331512
elapsed time: 0:04:03.643975
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 20:44:10.696741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.56
 ---- batch: 020 ----
mean loss: 390.26
 ---- batch: 030 ----
mean loss: 385.15
 ---- batch: 040 ----
mean loss: 404.92
 ---- batch: 050 ----
mean loss: 414.66
 ---- batch: 060 ----
mean loss: 418.22
 ---- batch: 070 ----
mean loss: 384.15
 ---- batch: 080 ----
mean loss: 398.43
 ---- batch: 090 ----
mean loss: 387.56
train mean loss: 397.79
epoch train time: 0:00:02.328113
elapsed time: 0:04:05.972262
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 20:44:13.025049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.92
 ---- batch: 020 ----
mean loss: 385.74
 ---- batch: 030 ----
mean loss: 394.07
 ---- batch: 040 ----
mean loss: 390.81
 ---- batch: 050 ----
mean loss: 395.59
 ---- batch: 060 ----
mean loss: 413.08
 ---- batch: 070 ----
mean loss: 387.76
 ---- batch: 080 ----
mean loss: 386.38
 ---- batch: 090 ----
mean loss: 380.58
train mean loss: 390.39
epoch train time: 0:00:02.325373
elapsed time: 0:04:08.297839
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 20:44:15.350606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.48
 ---- batch: 020 ----
mean loss: 392.28
 ---- batch: 030 ----
mean loss: 396.31
 ---- batch: 040 ----
mean loss: 403.42
 ---- batch: 050 ----
mean loss: 386.45
 ---- batch: 060 ----
mean loss: 393.21
 ---- batch: 070 ----
mean loss: 374.29
 ---- batch: 080 ----
mean loss: 397.90
 ---- batch: 090 ----
mean loss: 384.00
train mean loss: 389.36
epoch train time: 0:00:02.329848
elapsed time: 0:04:10.627907
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 20:44:17.680674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.20
 ---- batch: 020 ----
mean loss: 400.78
 ---- batch: 030 ----
mean loss: 406.94
 ---- batch: 040 ----
mean loss: 392.83
 ---- batch: 050 ----
mean loss: 386.63
 ---- batch: 060 ----
mean loss: 400.05
 ---- batch: 070 ----
mean loss: 398.27
 ---- batch: 080 ----
mean loss: 393.42
 ---- batch: 090 ----
mean loss: 382.59
train mean loss: 393.08
epoch train time: 0:00:02.326436
elapsed time: 0:04:12.954568
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 20:44:20.007342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.14
 ---- batch: 020 ----
mean loss: 389.61
 ---- batch: 030 ----
mean loss: 399.22
 ---- batch: 040 ----
mean loss: 413.97
 ---- batch: 050 ----
mean loss: 395.68
 ---- batch: 060 ----
mean loss: 415.04
 ---- batch: 070 ----
mean loss: 402.35
 ---- batch: 080 ----
mean loss: 398.13
 ---- batch: 090 ----
mean loss: 395.42
train mean loss: 397.86
epoch train time: 0:00:02.321212
elapsed time: 0:04:15.275965
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 20:44:22.328728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.86
 ---- batch: 020 ----
mean loss: 384.68
 ---- batch: 030 ----
mean loss: 396.45
 ---- batch: 040 ----
mean loss: 394.81
 ---- batch: 050 ----
mean loss: 395.66
 ---- batch: 060 ----
mean loss: 396.51
 ---- batch: 070 ----
mean loss: 393.01
 ---- batch: 080 ----
mean loss: 395.38
 ---- batch: 090 ----
mean loss: 390.37
train mean loss: 393.59
epoch train time: 0:00:02.324635
elapsed time: 0:04:17.600792
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 20:44:24.653562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.74
 ---- batch: 020 ----
mean loss: 388.61
 ---- batch: 030 ----
mean loss: 381.45
 ---- batch: 040 ----
mean loss: 387.80
 ---- batch: 050 ----
mean loss: 377.86
 ---- batch: 060 ----
mean loss: 377.79
 ---- batch: 070 ----
mean loss: 396.01
 ---- batch: 080 ----
mean loss: 394.13
 ---- batch: 090 ----
mean loss: 389.57
train mean loss: 388.89
epoch train time: 0:00:02.331538
elapsed time: 0:04:19.932517
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 20:44:26.985284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.93
 ---- batch: 020 ----
mean loss: 392.98
 ---- batch: 030 ----
mean loss: 381.18
 ---- batch: 040 ----
mean loss: 399.87
 ---- batch: 050 ----
mean loss: 386.16
 ---- batch: 060 ----
mean loss: 383.22
 ---- batch: 070 ----
mean loss: 390.26
 ---- batch: 080 ----
mean loss: 388.18
 ---- batch: 090 ----
mean loss: 387.61
train mean loss: 392.96
epoch train time: 0:00:02.321997
elapsed time: 0:04:22.254706
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 20:44:29.307494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.38
 ---- batch: 020 ----
mean loss: 383.39
 ---- batch: 030 ----
mean loss: 381.87
 ---- batch: 040 ----
mean loss: 380.71
 ---- batch: 050 ----
mean loss: 393.13
 ---- batch: 060 ----
mean loss: 387.55
 ---- batch: 070 ----
mean loss: 395.00
 ---- batch: 080 ----
mean loss: 393.17
 ---- batch: 090 ----
mean loss: 401.82
train mean loss: 390.12
epoch train time: 0:00:02.326410
elapsed time: 0:04:24.581366
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 20:44:31.634161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.18
 ---- batch: 020 ----
mean loss: 398.35
 ---- batch: 030 ----
mean loss: 388.56
 ---- batch: 040 ----
mean loss: 393.18
 ---- batch: 050 ----
mean loss: 394.06
 ---- batch: 060 ----
mean loss: 392.61
 ---- batch: 070 ----
mean loss: 413.79
 ---- batch: 080 ----
mean loss: 387.08
 ---- batch: 090 ----
mean loss: 397.56
train mean loss: 393.43
epoch train time: 0:00:02.332164
elapsed time: 0:04:26.913748
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 20:44:33.966519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.21
 ---- batch: 020 ----
mean loss: 375.15
 ---- batch: 030 ----
mean loss: 396.77
 ---- batch: 040 ----
mean loss: 396.20
 ---- batch: 050 ----
mean loss: 400.65
 ---- batch: 060 ----
mean loss: 372.93
 ---- batch: 070 ----
mean loss: 401.58
 ---- batch: 080 ----
mean loss: 392.59
 ---- batch: 090 ----
mean loss: 388.30
train mean loss: 390.61
epoch train time: 0:00:02.328968
elapsed time: 0:04:29.242903
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 20:44:36.295671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.86
 ---- batch: 020 ----
mean loss: 377.77
 ---- batch: 030 ----
mean loss: 390.32
 ---- batch: 040 ----
mean loss: 394.18
 ---- batch: 050 ----
mean loss: 390.28
 ---- batch: 060 ----
mean loss: 390.85
 ---- batch: 070 ----
mean loss: 383.37
 ---- batch: 080 ----
mean loss: 389.62
 ---- batch: 090 ----
mean loss: 371.07
train mean loss: 386.99
epoch train time: 0:00:02.328334
elapsed time: 0:04:31.571439
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 20:44:38.624206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.79
 ---- batch: 020 ----
mean loss: 365.74
 ---- batch: 030 ----
mean loss: 375.49
 ---- batch: 040 ----
mean loss: 387.90
 ---- batch: 050 ----
mean loss: 393.34
 ---- batch: 060 ----
mean loss: 395.61
 ---- batch: 070 ----
mean loss: 388.08
 ---- batch: 080 ----
mean loss: 384.27
 ---- batch: 090 ----
mean loss: 397.83
train mean loss: 388.24
epoch train time: 0:00:02.330545
elapsed time: 0:04:33.902171
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 20:44:40.954994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.19
 ---- batch: 020 ----
mean loss: 393.19
 ---- batch: 030 ----
mean loss: 384.52
 ---- batch: 040 ----
mean loss: 398.14
 ---- batch: 050 ----
mean loss: 389.18
 ---- batch: 060 ----
mean loss: 381.91
 ---- batch: 070 ----
mean loss: 390.88
 ---- batch: 080 ----
mean loss: 372.41
 ---- batch: 090 ----
mean loss: 384.49
train mean loss: 388.68
epoch train time: 0:00:02.326019
elapsed time: 0:04:36.228425
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 20:44:43.281192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.98
 ---- batch: 020 ----
mean loss: 391.60
 ---- batch: 030 ----
mean loss: 382.93
 ---- batch: 040 ----
mean loss: 383.19
 ---- batch: 050 ----
mean loss: 382.33
 ---- batch: 060 ----
mean loss: 395.49
 ---- batch: 070 ----
mean loss: 378.62
 ---- batch: 080 ----
mean loss: 372.07
 ---- batch: 090 ----
mean loss: 390.18
train mean loss: 386.17
epoch train time: 0:00:02.321110
elapsed time: 0:04:38.549750
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 20:44:45.602528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.69
 ---- batch: 020 ----
mean loss: 384.71
 ---- batch: 030 ----
mean loss: 393.29
 ---- batch: 040 ----
mean loss: 379.28
 ---- batch: 050 ----
mean loss: 373.85
 ---- batch: 060 ----
mean loss: 388.17
 ---- batch: 070 ----
mean loss: 384.08
 ---- batch: 080 ----
mean loss: 385.83
 ---- batch: 090 ----
mean loss: 423.47
train mean loss: 389.40
epoch train time: 0:00:02.332901
elapsed time: 0:04:40.882842
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 20:44:47.935635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.52
 ---- batch: 020 ----
mean loss: 398.65
 ---- batch: 030 ----
mean loss: 398.87
 ---- batch: 040 ----
mean loss: 393.01
 ---- batch: 050 ----
mean loss: 399.04
 ---- batch: 060 ----
mean loss: 390.44
 ---- batch: 070 ----
mean loss: 367.10
 ---- batch: 080 ----
mean loss: 392.80
 ---- batch: 090 ----
mean loss: 392.42
train mean loss: 393.76
epoch train time: 0:00:02.333769
elapsed time: 0:04:43.216838
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 20:44:50.269657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.90
 ---- batch: 020 ----
mean loss: 382.46
 ---- batch: 030 ----
mean loss: 393.47
 ---- batch: 040 ----
mean loss: 390.02
 ---- batch: 050 ----
mean loss: 375.22
 ---- batch: 060 ----
mean loss: 386.55
 ---- batch: 070 ----
mean loss: 389.82
 ---- batch: 080 ----
mean loss: 391.63
 ---- batch: 090 ----
mean loss: 398.05
train mean loss: 385.54
epoch train time: 0:00:02.326095
elapsed time: 0:04:45.543163
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 20:44:52.595930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.99
 ---- batch: 020 ----
mean loss: 388.43
 ---- batch: 030 ----
mean loss: 384.55
 ---- batch: 040 ----
mean loss: 393.65
 ---- batch: 050 ----
mean loss: 383.74
 ---- batch: 060 ----
mean loss: 392.31
 ---- batch: 070 ----
mean loss: 397.01
 ---- batch: 080 ----
mean loss: 387.03
 ---- batch: 090 ----
mean loss: 388.76
train mean loss: 390.13
epoch train time: 0:00:02.330181
elapsed time: 0:04:47.873542
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 20:44:54.926314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.00
 ---- batch: 020 ----
mean loss: 377.96
 ---- batch: 030 ----
mean loss: 397.55
 ---- batch: 040 ----
mean loss: 394.13
 ---- batch: 050 ----
mean loss: 391.34
 ---- batch: 060 ----
mean loss: 385.28
 ---- batch: 070 ----
mean loss: 391.16
 ---- batch: 080 ----
mean loss: 383.56
 ---- batch: 090 ----
mean loss: 392.95
train mean loss: 389.62
epoch train time: 0:00:02.331975
elapsed time: 0:04:50.205703
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 20:44:57.258470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.26
 ---- batch: 020 ----
mean loss: 394.25
 ---- batch: 030 ----
mean loss: 390.75
 ---- batch: 040 ----
mean loss: 367.47
 ---- batch: 050 ----
mean loss: 382.80
 ---- batch: 060 ----
mean loss: 386.74
 ---- batch: 070 ----
mean loss: 380.52
 ---- batch: 080 ----
mean loss: 386.37
 ---- batch: 090 ----
mean loss: 382.63
train mean loss: 385.69
epoch train time: 0:00:02.323743
elapsed time: 0:04:52.529624
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 20:44:59.582392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.80
 ---- batch: 020 ----
mean loss: 383.54
 ---- batch: 030 ----
mean loss: 391.73
 ---- batch: 040 ----
mean loss: 417.19
 ---- batch: 050 ----
mean loss: 414.22
 ---- batch: 060 ----
mean loss: 408.67
 ---- batch: 070 ----
mean loss: 414.24
 ---- batch: 080 ----
mean loss: 391.48
 ---- batch: 090 ----
mean loss: 406.89
train mean loss: 398.51
epoch train time: 0:00:02.332062
elapsed time: 0:04:54.861875
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 20:45:01.914644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.20
 ---- batch: 020 ----
mean loss: 382.67
 ---- batch: 030 ----
mean loss: 373.73
 ---- batch: 040 ----
mean loss: 378.08
 ---- batch: 050 ----
mean loss: 381.02
 ---- batch: 060 ----
mean loss: 396.82
 ---- batch: 070 ----
mean loss: 395.68
 ---- batch: 080 ----
mean loss: 390.66
 ---- batch: 090 ----
mean loss: 385.96
train mean loss: 386.76
epoch train time: 0:00:02.328618
elapsed time: 0:04:57.190745
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 20:45:04.243554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.65
 ---- batch: 020 ----
mean loss: 381.06
 ---- batch: 030 ----
mean loss: 389.74
 ---- batch: 040 ----
mean loss: 381.65
 ---- batch: 050 ----
mean loss: 395.92
 ---- batch: 060 ----
mean loss: 406.29
 ---- batch: 070 ----
mean loss: 379.41
 ---- batch: 080 ----
mean loss: 389.61
 ---- batch: 090 ----
mean loss: 406.27
train mean loss: 390.10
epoch train time: 0:00:02.328099
elapsed time: 0:04:59.519063
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 20:45:06.571833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.17
 ---- batch: 020 ----
mean loss: 377.88
 ---- batch: 030 ----
mean loss: 384.25
 ---- batch: 040 ----
mean loss: 398.07
 ---- batch: 050 ----
mean loss: 383.38
 ---- batch: 060 ----
mean loss: 387.11
 ---- batch: 070 ----
mean loss: 396.34
 ---- batch: 080 ----
mean loss: 376.81
 ---- batch: 090 ----
mean loss: 390.14
train mean loss: 387.61
epoch train time: 0:00:02.336447
elapsed time: 0:05:01.855697
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 20:45:08.908480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.98
 ---- batch: 020 ----
mean loss: 388.78
 ---- batch: 030 ----
mean loss: 376.13
 ---- batch: 040 ----
mean loss: 381.58
 ---- batch: 050 ----
mean loss: 399.57
 ---- batch: 060 ----
mean loss: 376.39
 ---- batch: 070 ----
mean loss: 393.47
 ---- batch: 080 ----
mean loss: 377.43
 ---- batch: 090 ----
mean loss: 402.08
train mean loss: 389.14
epoch train time: 0:00:02.334427
elapsed time: 0:05:04.190320
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 20:45:11.243130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.97
 ---- batch: 020 ----
mean loss: 377.52
 ---- batch: 030 ----
mean loss: 387.49
 ---- batch: 040 ----
mean loss: 390.87
 ---- batch: 050 ----
mean loss: 409.10
 ---- batch: 060 ----
mean loss: 402.55
 ---- batch: 070 ----
mean loss: 391.85
 ---- batch: 080 ----
mean loss: 388.51
 ---- batch: 090 ----
mean loss: 404.08
train mean loss: 391.75
epoch train time: 0:00:02.329213
elapsed time: 0:05:06.519749
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 20:45:13.572518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.97
 ---- batch: 020 ----
mean loss: 401.77
 ---- batch: 030 ----
mean loss: 389.95
 ---- batch: 040 ----
mean loss: 381.24
 ---- batch: 050 ----
mean loss: 386.98
 ---- batch: 060 ----
mean loss: 386.21
 ---- batch: 070 ----
mean loss: 410.43
 ---- batch: 080 ----
mean loss: 413.83
 ---- batch: 090 ----
mean loss: 393.86
train mean loss: 396.46
epoch train time: 0:00:02.333413
elapsed time: 0:05:08.853350
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 20:45:15.906119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.93
 ---- batch: 020 ----
mean loss: 408.12
 ---- batch: 030 ----
mean loss: 407.32
 ---- batch: 040 ----
mean loss: 392.06
 ---- batch: 050 ----
mean loss: 388.53
 ---- batch: 060 ----
mean loss: 378.52
 ---- batch: 070 ----
mean loss: 390.44
 ---- batch: 080 ----
mean loss: 391.95
 ---- batch: 090 ----
mean loss: 380.51
train mean loss: 394.43
epoch train time: 0:00:02.326840
elapsed time: 0:05:11.180369
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 20:45:18.233136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.20
 ---- batch: 020 ----
mean loss: 392.07
 ---- batch: 030 ----
mean loss: 388.22
 ---- batch: 040 ----
mean loss: 387.55
 ---- batch: 050 ----
mean loss: 381.44
 ---- batch: 060 ----
mean loss: 393.69
 ---- batch: 070 ----
mean loss: 392.08
 ---- batch: 080 ----
mean loss: 397.78
 ---- batch: 090 ----
mean loss: 383.25
train mean loss: 390.97
epoch train time: 0:00:02.321889
elapsed time: 0:05:13.502450
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 20:45:20.555223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.86
 ---- batch: 020 ----
mean loss: 395.49
 ---- batch: 030 ----
mean loss: 398.51
 ---- batch: 040 ----
mean loss: 396.87
 ---- batch: 050 ----
mean loss: 384.79
 ---- batch: 060 ----
mean loss: 381.73
 ---- batch: 070 ----
mean loss: 384.66
 ---- batch: 080 ----
mean loss: 393.78
 ---- batch: 090 ----
mean loss: 390.33
train mean loss: 390.25
epoch train time: 0:00:02.333321
elapsed time: 0:05:15.835962
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 20:45:22.888761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.76
 ---- batch: 020 ----
mean loss: 384.03
 ---- batch: 030 ----
mean loss: 380.55
 ---- batch: 040 ----
mean loss: 390.87
 ---- batch: 050 ----
mean loss: 412.50
 ---- batch: 060 ----
mean loss: 390.70
 ---- batch: 070 ----
mean loss: 400.66
 ---- batch: 080 ----
mean loss: 411.96
 ---- batch: 090 ----
mean loss: 399.37
train mean loss: 396.42
epoch train time: 0:00:02.332143
elapsed time: 0:05:18.168316
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 20:45:25.221084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.18
 ---- batch: 020 ----
mean loss: 383.35
 ---- batch: 030 ----
mean loss: 376.32
 ---- batch: 040 ----
mean loss: 389.13
 ---- batch: 050 ----
mean loss: 387.66
 ---- batch: 060 ----
mean loss: 385.08
 ---- batch: 070 ----
mean loss: 382.53
 ---- batch: 080 ----
mean loss: 390.80
 ---- batch: 090 ----
mean loss: 372.74
train mean loss: 385.84
epoch train time: 0:00:02.331737
elapsed time: 0:05:20.500261
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 20:45:27.553036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.39
 ---- batch: 020 ----
mean loss: 374.14
 ---- batch: 030 ----
mean loss: 395.29
 ---- batch: 040 ----
mean loss: 387.06
 ---- batch: 050 ----
mean loss: 386.75
 ---- batch: 060 ----
mean loss: 388.33
 ---- batch: 070 ----
mean loss: 386.92
 ---- batch: 080 ----
mean loss: 387.33
 ---- batch: 090 ----
mean loss: 370.39
train mean loss: 386.37
epoch train time: 0:00:02.327617
elapsed time: 0:05:22.828084
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 20:45:29.880865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.17
 ---- batch: 020 ----
mean loss: 393.99
 ---- batch: 030 ----
mean loss: 381.94
 ---- batch: 040 ----
mean loss: 388.51
 ---- batch: 050 ----
mean loss: 392.67
 ---- batch: 060 ----
mean loss: 399.78
 ---- batch: 070 ----
mean loss: 398.10
 ---- batch: 080 ----
mean loss: 404.99
 ---- batch: 090 ----
mean loss: 403.91
train mean loss: 393.27
epoch train time: 0:00:02.335403
elapsed time: 0:05:25.163689
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 20:45:32.216452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.31
 ---- batch: 020 ----
mean loss: 388.33
 ---- batch: 030 ----
mean loss: 379.54
 ---- batch: 040 ----
mean loss: 380.17
 ---- batch: 050 ----
mean loss: 382.77
 ---- batch: 060 ----
mean loss: 394.59
 ---- batch: 070 ----
mean loss: 389.26
 ---- batch: 080 ----
mean loss: 401.50
 ---- batch: 090 ----
mean loss: 382.02
train mean loss: 387.75
epoch train time: 0:00:02.329522
elapsed time: 0:05:27.493397
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 20:45:34.546159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.60
 ---- batch: 020 ----
mean loss: 377.76
 ---- batch: 030 ----
mean loss: 383.66
 ---- batch: 040 ----
mean loss: 382.69
 ---- batch: 050 ----
mean loss: 390.22
 ---- batch: 060 ----
mean loss: 389.26
 ---- batch: 070 ----
mean loss: 385.45
 ---- batch: 080 ----
mean loss: 397.79
 ---- batch: 090 ----
mean loss: 392.55
train mean loss: 384.93
epoch train time: 0:00:02.332998
elapsed time: 0:05:29.826604
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 20:45:36.879363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.81
 ---- batch: 020 ----
mean loss: 387.85
 ---- batch: 030 ----
mean loss: 397.33
 ---- batch: 040 ----
mean loss: 382.75
 ---- batch: 050 ----
mean loss: 402.42
 ---- batch: 060 ----
mean loss: 388.11
 ---- batch: 070 ----
mean loss: 375.34
 ---- batch: 080 ----
mean loss: 379.38
 ---- batch: 090 ----
mean loss: 390.06
train mean loss: 388.29
epoch train time: 0:00:02.330985
elapsed time: 0:05:32.157759
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 20:45:39.210558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.67
 ---- batch: 020 ----
mean loss: 383.12
 ---- batch: 030 ----
mean loss: 384.86
 ---- batch: 040 ----
mean loss: 392.96
 ---- batch: 050 ----
mean loss: 379.66
 ---- batch: 060 ----
mean loss: 383.45
 ---- batch: 070 ----
mean loss: 382.44
 ---- batch: 080 ----
mean loss: 393.61
 ---- batch: 090 ----
mean loss: 387.96
train mean loss: 388.07
epoch train time: 0:00:02.332034
elapsed time: 0:05:34.490005
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 20:45:41.542773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.10
 ---- batch: 020 ----
mean loss: 388.12
 ---- batch: 030 ----
mean loss: 379.51
 ---- batch: 040 ----
mean loss: 377.26
 ---- batch: 050 ----
mean loss: 375.40
 ---- batch: 060 ----
mean loss: 383.98
 ---- batch: 070 ----
mean loss: 386.55
 ---- batch: 080 ----
mean loss: 404.49
 ---- batch: 090 ----
mean loss: 389.28
train mean loss: 384.61
epoch train time: 0:00:02.326055
elapsed time: 0:05:36.816321
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 20:45:43.869105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.64
 ---- batch: 020 ----
mean loss: 384.15
 ---- batch: 030 ----
mean loss: 371.21
 ---- batch: 040 ----
mean loss: 393.54
 ---- batch: 050 ----
mean loss: 385.77
 ---- batch: 060 ----
mean loss: 387.96
 ---- batch: 070 ----
mean loss: 384.16
 ---- batch: 080 ----
mean loss: 385.42
 ---- batch: 090 ----
mean loss: 391.03
train mean loss: 385.67
epoch train time: 0:00:02.334763
elapsed time: 0:05:39.151275
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 20:45:46.204041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.86
 ---- batch: 020 ----
mean loss: 386.43
 ---- batch: 030 ----
mean loss: 379.00
 ---- batch: 040 ----
mean loss: 380.08
 ---- batch: 050 ----
mean loss: 393.08
 ---- batch: 060 ----
mean loss: 404.16
 ---- batch: 070 ----
mean loss: 390.74
 ---- batch: 080 ----
mean loss: 405.39
 ---- batch: 090 ----
mean loss: 403.59
train mean loss: 393.21
epoch train time: 0:00:02.333137
elapsed time: 0:05:41.484623
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 20:45:48.537424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.92
 ---- batch: 020 ----
mean loss: 390.36
 ---- batch: 030 ----
mean loss: 381.80
 ---- batch: 040 ----
mean loss: 376.18
 ---- batch: 050 ----
mean loss: 395.22
 ---- batch: 060 ----
mean loss: 384.16
 ---- batch: 070 ----
mean loss: 388.30
 ---- batch: 080 ----
mean loss: 402.88
 ---- batch: 090 ----
mean loss: 385.69
train mean loss: 387.04
epoch train time: 0:00:02.323119
elapsed time: 0:05:43.807953
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 20:45:50.860736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.26
 ---- batch: 020 ----
mean loss: 373.90
 ---- batch: 030 ----
mean loss: 374.43
 ---- batch: 040 ----
mean loss: 404.85
 ---- batch: 050 ----
mean loss: 388.36
 ---- batch: 060 ----
mean loss: 404.01
 ---- batch: 070 ----
mean loss: 390.51
 ---- batch: 080 ----
mean loss: 388.95
 ---- batch: 090 ----
mean loss: 388.32
train mean loss: 390.04
epoch train time: 0:00:02.331210
elapsed time: 0:05:46.139361
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 20:45:53.192128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.99
 ---- batch: 020 ----
mean loss: 388.37
 ---- batch: 030 ----
mean loss: 384.58
 ---- batch: 040 ----
mean loss: 379.86
 ---- batch: 050 ----
mean loss: 382.86
 ---- batch: 060 ----
mean loss: 396.44
 ---- batch: 070 ----
mean loss: 387.45
 ---- batch: 080 ----
mean loss: 382.14
 ---- batch: 090 ----
mean loss: 390.99
train mean loss: 387.22
epoch train time: 0:00:02.332543
elapsed time: 0:05:48.472087
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 20:45:55.524854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.53
 ---- batch: 020 ----
mean loss: 383.93
 ---- batch: 030 ----
mean loss: 359.98
 ---- batch: 040 ----
mean loss: 391.44
 ---- batch: 050 ----
mean loss: 395.97
 ---- batch: 060 ----
mean loss: 385.48
 ---- batch: 070 ----
mean loss: 376.24
 ---- batch: 080 ----
mean loss: 382.24
 ---- batch: 090 ----
mean loss: 404.33
train mean loss: 386.67
epoch train time: 0:00:02.327292
elapsed time: 0:05:50.799608
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 20:45:57.852410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.22
 ---- batch: 020 ----
mean loss: 372.64
 ---- batch: 030 ----
mean loss: 382.36
 ---- batch: 040 ----
mean loss: 381.84
 ---- batch: 050 ----
mean loss: 395.69
 ---- batch: 060 ----
mean loss: 385.71
 ---- batch: 070 ----
mean loss: 374.38
 ---- batch: 080 ----
mean loss: 372.81
 ---- batch: 090 ----
mean loss: 400.32
train mean loss: 383.04
epoch train time: 0:00:02.333944
elapsed time: 0:05:53.133788
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 20:46:00.186557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.95
 ---- batch: 020 ----
mean loss: 386.62
 ---- batch: 030 ----
mean loss: 392.64
 ---- batch: 040 ----
mean loss: 382.57
 ---- batch: 050 ----
mean loss: 384.25
 ---- batch: 060 ----
mean loss: 396.56
 ---- batch: 070 ----
mean loss: 381.41
 ---- batch: 080 ----
mean loss: 398.01
 ---- batch: 090 ----
mean loss: 374.46
train mean loss: 387.86
epoch train time: 0:00:02.336478
elapsed time: 0:05:55.470454
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 20:46:02.523224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.42
 ---- batch: 020 ----
mean loss: 379.43
 ---- batch: 030 ----
mean loss: 389.76
 ---- batch: 040 ----
mean loss: 376.80
 ---- batch: 050 ----
mean loss: 380.57
 ---- batch: 060 ----
mean loss: 381.00
 ---- batch: 070 ----
mean loss: 399.84
 ---- batch: 080 ----
mean loss: 378.48
 ---- batch: 090 ----
mean loss: 385.89
train mean loss: 384.71
epoch train time: 0:00:02.331556
elapsed time: 0:05:57.802202
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 20:46:04.854972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.89
 ---- batch: 020 ----
mean loss: 398.51
 ---- batch: 030 ----
mean loss: 392.59
 ---- batch: 040 ----
mean loss: 390.71
 ---- batch: 050 ----
mean loss: 387.78
 ---- batch: 060 ----
mean loss: 396.84
 ---- batch: 070 ----
mean loss: 402.20
 ---- batch: 080 ----
mean loss: 421.24
 ---- batch: 090 ----
mean loss: 390.90
train mean loss: 396.23
epoch train time: 0:00:02.327783
elapsed time: 0:06:00.130180
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 20:46:07.182968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.35
 ---- batch: 020 ----
mean loss: 387.50
 ---- batch: 030 ----
mean loss: 389.50
 ---- batch: 040 ----
mean loss: 381.70
 ---- batch: 050 ----
mean loss: 384.49
 ---- batch: 060 ----
mean loss: 379.91
 ---- batch: 070 ----
mean loss: 389.97
 ---- batch: 080 ----
mean loss: 376.32
 ---- batch: 090 ----
mean loss: 398.55
train mean loss: 385.93
epoch train time: 0:00:02.335959
elapsed time: 0:06:02.466340
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 20:46:09.519108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.64
 ---- batch: 020 ----
mean loss: 401.64
 ---- batch: 030 ----
mean loss: 390.90
 ---- batch: 040 ----
mean loss: 366.56
 ---- batch: 050 ----
mean loss: 383.13
 ---- batch: 060 ----
mean loss: 381.00
 ---- batch: 070 ----
mean loss: 405.85
 ---- batch: 080 ----
mean loss: 401.30
 ---- batch: 090 ----
mean loss: 387.58
train mean loss: 390.35
epoch train time: 0:00:02.336107
elapsed time: 0:06:04.802648
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 20:46:11.855435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.28
 ---- batch: 020 ----
mean loss: 380.84
 ---- batch: 030 ----
mean loss: 371.70
 ---- batch: 040 ----
mean loss: 383.18
 ---- batch: 050 ----
mean loss: 388.41
 ---- batch: 060 ----
mean loss: 387.59
 ---- batch: 070 ----
mean loss: 386.08
 ---- batch: 080 ----
mean loss: 410.65
 ---- batch: 090 ----
mean loss: 390.17
train mean loss: 388.64
epoch train time: 0:00:02.324279
elapsed time: 0:06:07.127125
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 20:46:14.179925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.72
 ---- batch: 020 ----
mean loss: 391.81
 ---- batch: 030 ----
mean loss: 374.08
 ---- batch: 040 ----
mean loss: 392.60
 ---- batch: 050 ----
mean loss: 373.92
 ---- batch: 060 ----
mean loss: 377.04
 ---- batch: 070 ----
mean loss: 400.09
 ---- batch: 080 ----
mean loss: 386.28
 ---- batch: 090 ----
mean loss: 383.09
train mean loss: 383.83
epoch train time: 0:00:02.338131
elapsed time: 0:06:09.465466
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 20:46:16.518234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.51
 ---- batch: 020 ----
mean loss: 392.01
 ---- batch: 030 ----
mean loss: 390.14
 ---- batch: 040 ----
mean loss: 393.81
 ---- batch: 050 ----
mean loss: 376.93
 ---- batch: 060 ----
mean loss: 393.81
 ---- batch: 070 ----
mean loss: 388.20
 ---- batch: 080 ----
mean loss: 392.36
 ---- batch: 090 ----
mean loss: 392.25
train mean loss: 389.28
epoch train time: 0:00:02.331130
elapsed time: 0:06:11.796794
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 20:46:18.849562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.74
 ---- batch: 020 ----
mean loss: 391.62
 ---- batch: 030 ----
mean loss: 379.12
 ---- batch: 040 ----
mean loss: 396.83
 ---- batch: 050 ----
mean loss: 386.24
 ---- batch: 060 ----
mean loss: 381.97
 ---- batch: 070 ----
mean loss: 386.14
 ---- batch: 080 ----
mean loss: 404.07
 ---- batch: 090 ----
mean loss: 381.14
train mean loss: 387.03
epoch train time: 0:00:02.327078
elapsed time: 0:06:14.124054
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 20:46:21.176823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.46
 ---- batch: 020 ----
mean loss: 385.50
 ---- batch: 030 ----
mean loss: 385.21
 ---- batch: 040 ----
mean loss: 382.93
 ---- batch: 050 ----
mean loss: 378.58
 ---- batch: 060 ----
mean loss: 376.67
 ---- batch: 070 ----
mean loss: 373.62
 ---- batch: 080 ----
mean loss: 401.57
 ---- batch: 090 ----
mean loss: 391.41
train mean loss: 385.22
epoch train time: 0:00:02.334044
elapsed time: 0:06:16.458383
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 20:46:23.511158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.37
 ---- batch: 020 ----
mean loss: 399.14
 ---- batch: 030 ----
mean loss: 377.93
 ---- batch: 040 ----
mean loss: 377.79
 ---- batch: 050 ----
mean loss: 393.08
 ---- batch: 060 ----
mean loss: 399.79
 ---- batch: 070 ----
mean loss: 367.82
 ---- batch: 080 ----
mean loss: 392.97
 ---- batch: 090 ----
mean loss: 405.19
train mean loss: 388.92
epoch train time: 0:00:02.342691
elapsed time: 0:06:18.801278
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 20:46:25.854056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.21
 ---- batch: 020 ----
mean loss: 382.77
 ---- batch: 030 ----
mean loss: 382.76
 ---- batch: 040 ----
mean loss: 383.49
 ---- batch: 050 ----
mean loss: 376.28
 ---- batch: 060 ----
mean loss: 388.71
 ---- batch: 070 ----
mean loss: 387.51
 ---- batch: 080 ----
mean loss: 395.39
 ---- batch: 090 ----
mean loss: 397.65
train mean loss: 385.89
epoch train time: 0:00:02.328658
elapsed time: 0:06:21.130149
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 20:46:28.182926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.87
 ---- batch: 020 ----
mean loss: 415.82
 ---- batch: 030 ----
mean loss: 403.36
 ---- batch: 040 ----
mean loss: 405.70
 ---- batch: 050 ----
mean loss: 385.59
 ---- batch: 060 ----
mean loss: 398.78
 ---- batch: 070 ----
mean loss: 392.67
 ---- batch: 080 ----
mean loss: 372.23
 ---- batch: 090 ----
mean loss: 381.58
train mean loss: 394.98
epoch train time: 0:00:02.329500
elapsed time: 0:06:23.459839
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 20:46:30.512604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.57
 ---- batch: 020 ----
mean loss: 378.47
 ---- batch: 030 ----
mean loss: 407.18
 ---- batch: 040 ----
mean loss: 402.85
 ---- batch: 050 ----
mean loss: 388.67
 ---- batch: 060 ----
mean loss: 386.33
 ---- batch: 070 ----
mean loss: 388.98
 ---- batch: 080 ----
mean loss: 382.26
 ---- batch: 090 ----
mean loss: 378.67
train mean loss: 389.62
epoch train time: 0:00:02.330063
elapsed time: 0:06:25.790075
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 20:46:32.842868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.60
 ---- batch: 020 ----
mean loss: 384.31
 ---- batch: 030 ----
mean loss: 397.80
 ---- batch: 040 ----
mean loss: 386.94
 ---- batch: 050 ----
mean loss: 378.06
 ---- batch: 060 ----
mean loss: 387.62
 ---- batch: 070 ----
mean loss: 377.67
 ---- batch: 080 ----
mean loss: 386.73
 ---- batch: 090 ----
mean loss: 391.64
train mean loss: 386.24
epoch train time: 0:00:02.323050
elapsed time: 0:06:28.113327
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 20:46:35.166092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.85
 ---- batch: 020 ----
mean loss: 390.87
 ---- batch: 030 ----
mean loss: 386.61
 ---- batch: 040 ----
mean loss: 385.98
 ---- batch: 050 ----
mean loss: 388.22
 ---- batch: 060 ----
mean loss: 383.55
 ---- batch: 070 ----
mean loss: 384.67
 ---- batch: 080 ----
mean loss: 384.43
 ---- batch: 090 ----
mean loss: 387.46
train mean loss: 383.77
epoch train time: 0:00:02.328378
elapsed time: 0:06:30.441912
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 20:46:37.494696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.10
 ---- batch: 020 ----
mean loss: 380.63
 ---- batch: 030 ----
mean loss: 390.96
 ---- batch: 040 ----
mean loss: 388.62
 ---- batch: 050 ----
mean loss: 391.18
 ---- batch: 060 ----
mean loss: 393.74
 ---- batch: 070 ----
mean loss: 388.12
 ---- batch: 080 ----
mean loss: 380.66
 ---- batch: 090 ----
mean loss: 378.86
train mean loss: 388.47
epoch train time: 0:00:02.332690
elapsed time: 0:06:32.774810
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 20:46:39.827578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.13
 ---- batch: 020 ----
mean loss: 378.19
 ---- batch: 030 ----
mean loss: 386.74
 ---- batch: 040 ----
mean loss: 389.13
 ---- batch: 050 ----
mean loss: 379.90
 ---- batch: 060 ----
mean loss: 397.44
 ---- batch: 070 ----
mean loss: 374.57
 ---- batch: 080 ----
mean loss: 390.65
 ---- batch: 090 ----
mean loss: 373.79
train mean loss: 382.64
epoch train time: 0:00:02.331246
elapsed time: 0:06:35.106262
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 20:46:42.159030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.26
 ---- batch: 020 ----
mean loss: 379.06
 ---- batch: 030 ----
mean loss: 382.78
 ---- batch: 040 ----
mean loss: 371.77
 ---- batch: 050 ----
mean loss: 375.34
 ---- batch: 060 ----
mean loss: 382.19
 ---- batch: 070 ----
mean loss: 395.16
 ---- batch: 080 ----
mean loss: 369.85
 ---- batch: 090 ----
mean loss: 404.81
train mean loss: 385.01
epoch train time: 0:00:02.334235
elapsed time: 0:06:37.440677
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 20:46:44.493445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.54
 ---- batch: 020 ----
mean loss: 379.66
 ---- batch: 030 ----
mean loss: 382.92
 ---- batch: 040 ----
mean loss: 381.73
 ---- batch: 050 ----
mean loss: 376.27
 ---- batch: 060 ----
mean loss: 392.17
 ---- batch: 070 ----
mean loss: 382.62
 ---- batch: 080 ----
mean loss: 392.78
 ---- batch: 090 ----
mean loss: 389.74
train mean loss: 383.52
epoch train time: 0:00:02.330548
elapsed time: 0:06:39.771426
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 20:46:46.824202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.08
 ---- batch: 020 ----
mean loss: 393.80
 ---- batch: 030 ----
mean loss: 381.90
 ---- batch: 040 ----
mean loss: 377.41
 ---- batch: 050 ----
mean loss: 382.95
 ---- batch: 060 ----
mean loss: 382.16
 ---- batch: 070 ----
mean loss: 379.02
 ---- batch: 080 ----
mean loss: 384.64
 ---- batch: 090 ----
mean loss: 391.03
train mean loss: 383.23
epoch train time: 0:00:02.330924
elapsed time: 0:06:42.102552
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 20:46:49.155340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.29
 ---- batch: 020 ----
mean loss: 381.76
 ---- batch: 030 ----
mean loss: 391.04
 ---- batch: 040 ----
mean loss: 394.67
 ---- batch: 050 ----
mean loss: 379.46
 ---- batch: 060 ----
mean loss: 385.20
 ---- batch: 070 ----
mean loss: 383.81
 ---- batch: 080 ----
mean loss: 396.21
 ---- batch: 090 ----
mean loss: 386.20
train mean loss: 388.46
epoch train time: 0:00:02.323674
elapsed time: 0:06:44.426455
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 20:46:51.479219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.09
 ---- batch: 020 ----
mean loss: 403.52
 ---- batch: 030 ----
mean loss: 377.12
 ---- batch: 040 ----
mean loss: 392.33
 ---- batch: 050 ----
mean loss: 394.08
 ---- batch: 060 ----
mean loss: 391.52
 ---- batch: 070 ----
mean loss: 385.45
 ---- batch: 080 ----
mean loss: 387.73
 ---- batch: 090 ----
mean loss: 377.14
train mean loss: 388.79
epoch train time: 0:00:02.328146
elapsed time: 0:06:46.754790
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 20:46:53.807561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.91
 ---- batch: 020 ----
mean loss: 380.22
 ---- batch: 030 ----
mean loss: 386.49
 ---- batch: 040 ----
mean loss: 397.41
 ---- batch: 050 ----
mean loss: 379.45
 ---- batch: 060 ----
mean loss: 382.63
 ---- batch: 070 ----
mean loss: 384.20
 ---- batch: 080 ----
mean loss: 379.22
 ---- batch: 090 ----
mean loss: 384.64
train mean loss: 384.63
epoch train time: 0:00:02.324543
elapsed time: 0:06:49.079516
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 20:46:56.132289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.48
 ---- batch: 020 ----
mean loss: 386.53
 ---- batch: 030 ----
mean loss: 385.42
 ---- batch: 040 ----
mean loss: 386.20
 ---- batch: 050 ----
mean loss: 376.54
 ---- batch: 060 ----
mean loss: 381.57
 ---- batch: 070 ----
mean loss: 380.84
 ---- batch: 080 ----
mean loss: 379.90
 ---- batch: 090 ----
mean loss: 371.93
train mean loss: 383.32
epoch train time: 0:00:02.331412
elapsed time: 0:06:51.411113
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 20:46:58.463882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.42
 ---- batch: 020 ----
mean loss: 408.97
 ---- batch: 030 ----
mean loss: 412.58
 ---- batch: 040 ----
mean loss: 393.10
 ---- batch: 050 ----
mean loss: 378.01
 ---- batch: 060 ----
mean loss: 387.49
 ---- batch: 070 ----
mean loss: 382.47
 ---- batch: 080 ----
mean loss: 383.72
 ---- batch: 090 ----
mean loss: 366.53
train mean loss: 388.48
epoch train time: 0:00:02.336548
elapsed time: 0:06:53.747863
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 20:47:00.800630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.00
 ---- batch: 020 ----
mean loss: 393.83
 ---- batch: 030 ----
mean loss: 390.67
 ---- batch: 040 ----
mean loss: 390.05
 ---- batch: 050 ----
mean loss: 395.17
 ---- batch: 060 ----
mean loss: 403.65
 ---- batch: 070 ----
mean loss: 384.19
 ---- batch: 080 ----
mean loss: 374.59
 ---- batch: 090 ----
mean loss: 380.42
train mean loss: 388.73
epoch train time: 0:00:02.338163
elapsed time: 0:06:56.086212
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 20:47:03.138983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.14
 ---- batch: 020 ----
mean loss: 383.48
 ---- batch: 030 ----
mean loss: 386.95
 ---- batch: 040 ----
mean loss: 373.36
 ---- batch: 050 ----
mean loss: 379.96
 ---- batch: 060 ----
mean loss: 380.09
 ---- batch: 070 ----
mean loss: 384.41
 ---- batch: 080 ----
mean loss: 387.40
 ---- batch: 090 ----
mean loss: 409.96
train mean loss: 386.47
epoch train time: 0:00:02.331294
elapsed time: 0:06:58.417701
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 20:47:05.470466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.14
 ---- batch: 020 ----
mean loss: 385.89
 ---- batch: 030 ----
mean loss: 384.18
 ---- batch: 040 ----
mean loss: 399.51
 ---- batch: 050 ----
mean loss: 383.96
 ---- batch: 060 ----
mean loss: 387.10
 ---- batch: 070 ----
mean loss: 379.92
 ---- batch: 080 ----
mean loss: 378.63
 ---- batch: 090 ----
mean loss: 404.10
train mean loss: 385.84
epoch train time: 0:00:02.345888
elapsed time: 0:07:00.763778
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 20:47:07.816573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.34
 ---- batch: 020 ----
mean loss: 381.30
 ---- batch: 030 ----
mean loss: 387.28
 ---- batch: 040 ----
mean loss: 375.83
 ---- batch: 050 ----
mean loss: 393.57
 ---- batch: 060 ----
mean loss: 384.42
 ---- batch: 070 ----
mean loss: 370.45
 ---- batch: 080 ----
mean loss: 405.09
 ---- batch: 090 ----
mean loss: 396.61
train mean loss: 387.18
epoch train time: 0:00:02.330586
elapsed time: 0:07:03.094569
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 20:47:10.147338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.20
 ---- batch: 020 ----
mean loss: 379.65
 ---- batch: 030 ----
mean loss: 382.81
 ---- batch: 040 ----
mean loss: 375.40
 ---- batch: 050 ----
mean loss: 382.30
 ---- batch: 060 ----
mean loss: 385.62
 ---- batch: 070 ----
mean loss: 382.15
 ---- batch: 080 ----
mean loss: 386.11
 ---- batch: 090 ----
mean loss: 386.98
train mean loss: 384.27
epoch train time: 0:00:02.325903
elapsed time: 0:07:05.420659
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 20:47:12.473426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.33
 ---- batch: 020 ----
mean loss: 377.68
 ---- batch: 030 ----
mean loss: 377.13
 ---- batch: 040 ----
mean loss: 373.84
 ---- batch: 050 ----
mean loss: 379.66
 ---- batch: 060 ----
mean loss: 395.56
 ---- batch: 070 ----
mean loss: 403.67
 ---- batch: 080 ----
mean loss: 398.23
 ---- batch: 090 ----
mean loss: 391.64
train mean loss: 387.03
epoch train time: 0:00:02.325642
elapsed time: 0:07:07.746495
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 20:47:14.799277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.21
 ---- batch: 020 ----
mean loss: 366.63
 ---- batch: 030 ----
mean loss: 381.49
 ---- batch: 040 ----
mean loss: 383.87
 ---- batch: 050 ----
mean loss: 387.73
 ---- batch: 060 ----
mean loss: 392.11
 ---- batch: 070 ----
mean loss: 402.12
 ---- batch: 080 ----
mean loss: 384.33
 ---- batch: 090 ----
mean loss: 370.24
train mean loss: 384.57
epoch train time: 0:00:02.326728
elapsed time: 0:07:10.073428
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 20:47:17.126222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.84
 ---- batch: 020 ----
mean loss: 391.91
 ---- batch: 030 ----
mean loss: 391.37
 ---- batch: 040 ----
mean loss: 373.22
 ---- batch: 050 ----
mean loss: 396.48
 ---- batch: 060 ----
mean loss: 382.18
 ---- batch: 070 ----
mean loss: 391.16
 ---- batch: 080 ----
mean loss: 367.37
 ---- batch: 090 ----
mean loss: 392.29
train mean loss: 389.37
epoch train time: 0:00:02.326468
elapsed time: 0:07:12.400113
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 20:47:19.452881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.06
 ---- batch: 020 ----
mean loss: 412.23
 ---- batch: 030 ----
mean loss: 401.88
 ---- batch: 040 ----
mean loss: 379.69
 ---- batch: 050 ----
mean loss: 394.99
 ---- batch: 060 ----
mean loss: 383.68
 ---- batch: 070 ----
mean loss: 403.27
 ---- batch: 080 ----
mean loss: 384.04
 ---- batch: 090 ----
mean loss: 393.27
train mean loss: 393.58
epoch train time: 0:00:02.329211
elapsed time: 0:07:14.729521
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 20:47:21.782292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.34
 ---- batch: 020 ----
mean loss: 387.03
 ---- batch: 030 ----
mean loss: 378.34
 ---- batch: 040 ----
mean loss: 386.75
 ---- batch: 050 ----
mean loss: 376.32
 ---- batch: 060 ----
mean loss: 367.44
 ---- batch: 070 ----
mean loss: 398.91
 ---- batch: 080 ----
mean loss: 390.22
 ---- batch: 090 ----
mean loss: 381.49
train mean loss: 382.63
epoch train time: 0:00:02.328608
elapsed time: 0:07:17.058309
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 20:47:24.111083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.41
 ---- batch: 020 ----
mean loss: 387.70
 ---- batch: 030 ----
mean loss: 386.90
 ---- batch: 040 ----
mean loss: 375.65
 ---- batch: 050 ----
mean loss: 376.44
 ---- batch: 060 ----
mean loss: 396.52
 ---- batch: 070 ----
mean loss: 382.87
 ---- batch: 080 ----
mean loss: 382.61
 ---- batch: 090 ----
mean loss: 380.77
train mean loss: 383.70
epoch train time: 0:00:02.323438
elapsed time: 0:07:19.381953
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 20:47:26.434726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.24
 ---- batch: 020 ----
mean loss: 393.04
 ---- batch: 030 ----
mean loss: 387.91
 ---- batch: 040 ----
mean loss: 377.30
 ---- batch: 050 ----
mean loss: 377.38
 ---- batch: 060 ----
mean loss: 391.96
 ---- batch: 070 ----
mean loss: 378.75
 ---- batch: 080 ----
mean loss: 369.92
 ---- batch: 090 ----
mean loss: 380.88
train mean loss: 383.81
epoch train time: 0:00:02.327526
elapsed time: 0:07:21.709677
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 20:47:28.762445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.39
 ---- batch: 020 ----
mean loss: 373.27
 ---- batch: 030 ----
mean loss: 375.85
 ---- batch: 040 ----
mean loss: 388.85
 ---- batch: 050 ----
mean loss: 381.67
 ---- batch: 060 ----
mean loss: 387.85
 ---- batch: 070 ----
mean loss: 379.51
 ---- batch: 080 ----
mean loss: 387.12
 ---- batch: 090 ----
mean loss: 383.08
train mean loss: 381.84
epoch train time: 0:00:02.334303
elapsed time: 0:07:24.044161
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 20:47:31.096929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.35
 ---- batch: 020 ----
mean loss: 384.18
 ---- batch: 030 ----
mean loss: 388.43
 ---- batch: 040 ----
mean loss: 393.91
 ---- batch: 050 ----
mean loss: 379.09
 ---- batch: 060 ----
mean loss: 376.07
 ---- batch: 070 ----
mean loss: 386.07
 ---- batch: 080 ----
mean loss: 380.55
 ---- batch: 090 ----
mean loss: 386.91
train mean loss: 383.75
epoch train time: 0:00:02.328865
elapsed time: 0:07:26.373213
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 20:47:33.425983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.58
 ---- batch: 020 ----
mean loss: 391.56
 ---- batch: 030 ----
mean loss: 382.53
 ---- batch: 040 ----
mean loss: 389.06
 ---- batch: 050 ----
mean loss: 379.20
 ---- batch: 060 ----
mean loss: 379.13
 ---- batch: 070 ----
mean loss: 377.39
 ---- batch: 080 ----
mean loss: 379.01
 ---- batch: 090 ----
mean loss: 391.16
train mean loss: 383.65
epoch train time: 0:00:02.332838
elapsed time: 0:07:28.706299
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 20:47:35.759069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.03
 ---- batch: 020 ----
mean loss: 381.97
 ---- batch: 030 ----
mean loss: 380.35
 ---- batch: 040 ----
mean loss: 382.81
 ---- batch: 050 ----
mean loss: 367.33
 ---- batch: 060 ----
mean loss: 389.54
 ---- batch: 070 ----
mean loss: 407.78
 ---- batch: 080 ----
mean loss: 397.35
 ---- batch: 090 ----
mean loss: 399.30
train mean loss: 388.66
epoch train time: 0:00:02.334552
elapsed time: 0:07:31.041051
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 20:47:38.093804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.31
 ---- batch: 020 ----
mean loss: 395.28
 ---- batch: 030 ----
mean loss: 387.88
 ---- batch: 040 ----
mean loss: 389.30
 ---- batch: 050 ----
mean loss: 390.17
 ---- batch: 060 ----
mean loss: 390.16
 ---- batch: 070 ----
mean loss: 375.98
 ---- batch: 080 ----
mean loss: 376.83
 ---- batch: 090 ----
mean loss: 376.27
train mean loss: 385.50
epoch train time: 0:00:02.320643
elapsed time: 0:07:33.361872
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 20:47:40.414637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.71
 ---- batch: 020 ----
mean loss: 370.28
 ---- batch: 030 ----
mean loss: 384.52
 ---- batch: 040 ----
mean loss: 392.30
 ---- batch: 050 ----
mean loss: 371.21
 ---- batch: 060 ----
mean loss: 377.45
 ---- batch: 070 ----
mean loss: 379.17
 ---- batch: 080 ----
mean loss: 384.29
 ---- batch: 090 ----
mean loss: 374.45
train mean loss: 379.92
epoch train time: 0:00:02.324470
elapsed time: 0:07:35.686528
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 20:47:42.739322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.64
 ---- batch: 020 ----
mean loss: 396.64
 ---- batch: 030 ----
mean loss: 376.93
 ---- batch: 040 ----
mean loss: 398.44
 ---- batch: 050 ----
mean loss: 379.65
 ---- batch: 060 ----
mean loss: 372.86
 ---- batch: 070 ----
mean loss: 383.40
 ---- batch: 080 ----
mean loss: 397.46
 ---- batch: 090 ----
mean loss: 377.83
train mean loss: 387.69
epoch train time: 0:00:02.335889
elapsed time: 0:07:38.022621
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 20:47:45.075390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.75
 ---- batch: 020 ----
mean loss: 386.49
 ---- batch: 030 ----
mean loss: 402.21
 ---- batch: 040 ----
mean loss: 382.10
 ---- batch: 050 ----
mean loss: 370.86
 ---- batch: 060 ----
mean loss: 380.86
 ---- batch: 070 ----
mean loss: 394.01
 ---- batch: 080 ----
mean loss: 379.08
 ---- batch: 090 ----
mean loss: 372.05
train mean loss: 382.73
epoch train time: 0:00:02.327127
elapsed time: 0:07:40.349930
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 20:47:47.402697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.48
 ---- batch: 020 ----
mean loss: 384.82
 ---- batch: 030 ----
mean loss: 376.98
 ---- batch: 040 ----
mean loss: 390.49
 ---- batch: 050 ----
mean loss: 403.64
 ---- batch: 060 ----
mean loss: 382.46
 ---- batch: 070 ----
mean loss: 378.75
 ---- batch: 080 ----
mean loss: 380.55
 ---- batch: 090 ----
mean loss: 386.69
train mean loss: 384.21
epoch train time: 0:00:02.334024
elapsed time: 0:07:42.684148
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 20:47:49.736916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.80
 ---- batch: 020 ----
mean loss: 397.27
 ---- batch: 030 ----
mean loss: 378.01
 ---- batch: 040 ----
mean loss: 388.54
 ---- batch: 050 ----
mean loss: 393.47
 ---- batch: 060 ----
mean loss: 384.45
 ---- batch: 070 ----
mean loss: 369.74
 ---- batch: 080 ----
mean loss: 374.04
 ---- batch: 090 ----
mean loss: 370.84
train mean loss: 380.99
epoch train time: 0:00:02.332919
elapsed time: 0:07:45.017277
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 20:47:52.070043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.66
 ---- batch: 020 ----
mean loss: 390.82
 ---- batch: 030 ----
mean loss: 368.37
 ---- batch: 040 ----
mean loss: 405.88
 ---- batch: 050 ----
mean loss: 394.41
 ---- batch: 060 ----
mean loss: 384.43
 ---- batch: 070 ----
mean loss: 377.45
 ---- batch: 080 ----
mean loss: 379.85
 ---- batch: 090 ----
mean loss: 387.17
train mean loss: 382.95
epoch train time: 0:00:02.333123
elapsed time: 0:07:47.350583
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 20:47:54.403353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.77
 ---- batch: 020 ----
mean loss: 384.32
 ---- batch: 030 ----
mean loss: 373.79
 ---- batch: 040 ----
mean loss: 372.54
 ---- batch: 050 ----
mean loss: 362.69
 ---- batch: 060 ----
mean loss: 386.43
 ---- batch: 070 ----
mean loss: 372.60
 ---- batch: 080 ----
mean loss: 380.81
 ---- batch: 090 ----
mean loss: 395.70
train mean loss: 379.36
epoch train time: 0:00:02.331367
elapsed time: 0:07:49.682179
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 20:47:56.734956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.29
 ---- batch: 020 ----
mean loss: 378.90
 ---- batch: 030 ----
mean loss: 364.34
 ---- batch: 040 ----
mean loss: 380.95
 ---- batch: 050 ----
mean loss: 382.04
 ---- batch: 060 ----
mean loss: 376.12
 ---- batch: 070 ----
mean loss: 381.32
 ---- batch: 080 ----
mean loss: 381.89
 ---- batch: 090 ----
mean loss: 380.96
train mean loss: 380.08
epoch train time: 0:00:02.334384
elapsed time: 0:07:52.016766
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 20:47:59.069536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.85
 ---- batch: 020 ----
mean loss: 402.14
 ---- batch: 030 ----
mean loss: 387.85
 ---- batch: 040 ----
mean loss: 387.35
 ---- batch: 050 ----
mean loss: 392.06
 ---- batch: 060 ----
mean loss: 385.23
 ---- batch: 070 ----
mean loss: 380.80
 ---- batch: 080 ----
mean loss: 377.26
 ---- batch: 090 ----
mean loss: 370.94
train mean loss: 385.82
epoch train time: 0:00:02.324595
elapsed time: 0:07:54.341562
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 20:48:01.394355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.68
 ---- batch: 020 ----
mean loss: 377.43
 ---- batch: 030 ----
mean loss: 387.22
 ---- batch: 040 ----
mean loss: 381.92
 ---- batch: 050 ----
mean loss: 383.82
 ---- batch: 060 ----
mean loss: 371.02
 ---- batch: 070 ----
mean loss: 382.09
 ---- batch: 080 ----
mean loss: 389.83
 ---- batch: 090 ----
mean loss: 375.14
train mean loss: 380.48
epoch train time: 0:00:02.328077
elapsed time: 0:07:56.669847
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 20:48:03.722616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.86
 ---- batch: 020 ----
mean loss: 374.49
 ---- batch: 030 ----
mean loss: 381.82
 ---- batch: 040 ----
mean loss: 376.24
 ---- batch: 050 ----
mean loss: 385.45
 ---- batch: 060 ----
mean loss: 389.71
 ---- batch: 070 ----
mean loss: 370.17
 ---- batch: 080 ----
mean loss: 389.58
 ---- batch: 090 ----
mean loss: 386.35
train mean loss: 381.21
epoch train time: 0:00:02.330499
elapsed time: 0:07:59.000525
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 20:48:06.053292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.48
 ---- batch: 020 ----
mean loss: 396.53
 ---- batch: 030 ----
mean loss: 394.43
 ---- batch: 040 ----
mean loss: 387.02
 ---- batch: 050 ----
mean loss: 383.51
 ---- batch: 060 ----
mean loss: 378.50
 ---- batch: 070 ----
mean loss: 386.06
 ---- batch: 080 ----
mean loss: 379.41
 ---- batch: 090 ----
mean loss: 377.85
train mean loss: 385.49
epoch train time: 0:00:02.330628
elapsed time: 0:08:01.331325
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 20:48:08.384106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.91
 ---- batch: 020 ----
mean loss: 381.51
 ---- batch: 030 ----
mean loss: 376.30
 ---- batch: 040 ----
mean loss: 382.32
 ---- batch: 050 ----
mean loss: 378.48
 ---- batch: 060 ----
mean loss: 374.92
 ---- batch: 070 ----
mean loss: 376.29
 ---- batch: 080 ----
mean loss: 378.05
 ---- batch: 090 ----
mean loss: 388.31
train mean loss: 380.35
epoch train time: 0:00:02.323154
elapsed time: 0:08:03.654682
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 20:48:10.707468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.44
 ---- batch: 020 ----
mean loss: 386.65
 ---- batch: 030 ----
mean loss: 389.12
 ---- batch: 040 ----
mean loss: 389.78
 ---- batch: 050 ----
mean loss: 385.04
 ---- batch: 060 ----
mean loss: 375.72
 ---- batch: 070 ----
mean loss: 383.20
 ---- batch: 080 ----
mean loss: 397.60
 ---- batch: 090 ----
mean loss: 371.28
train mean loss: 385.45
epoch train time: 0:00:02.331548
elapsed time: 0:08:05.986431
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 20:48:13.039198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.07
 ---- batch: 020 ----
mean loss: 377.98
 ---- batch: 030 ----
mean loss: 386.89
 ---- batch: 040 ----
mean loss: 396.11
 ---- batch: 050 ----
mean loss: 372.71
 ---- batch: 060 ----
mean loss: 376.16
 ---- batch: 070 ----
mean loss: 391.66
 ---- batch: 080 ----
mean loss: 383.03
 ---- batch: 090 ----
mean loss: 358.44
train mean loss: 381.17
epoch train time: 0:00:02.335186
elapsed time: 0:08:08.321793
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 20:48:15.374561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.62
 ---- batch: 020 ----
mean loss: 369.55
 ---- batch: 030 ----
mean loss: 385.48
 ---- batch: 040 ----
mean loss: 381.62
 ---- batch: 050 ----
mean loss: 388.28
 ---- batch: 060 ----
mean loss: 385.89
 ---- batch: 070 ----
mean loss: 378.13
 ---- batch: 080 ----
mean loss: 388.01
 ---- batch: 090 ----
mean loss: 387.11
train mean loss: 383.61
epoch train time: 0:00:02.330903
elapsed time: 0:08:10.652881
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 20:48:17.705689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.71
 ---- batch: 020 ----
mean loss: 384.90
 ---- batch: 030 ----
mean loss: 376.98
 ---- batch: 040 ----
mean loss: 391.00
 ---- batch: 050 ----
mean loss: 367.84
 ---- batch: 060 ----
mean loss: 386.27
 ---- batch: 070 ----
mean loss: 393.37
 ---- batch: 080 ----
mean loss: 374.08
 ---- batch: 090 ----
mean loss: 379.11
train mean loss: 382.59
epoch train time: 0:00:02.329827
elapsed time: 0:08:12.982929
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 20:48:20.035713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.22
 ---- batch: 020 ----
mean loss: 378.20
 ---- batch: 030 ----
mean loss: 386.99
 ---- batch: 040 ----
mean loss: 378.27
 ---- batch: 050 ----
mean loss: 382.18
 ---- batch: 060 ----
mean loss: 384.84
 ---- batch: 070 ----
mean loss: 383.97
 ---- batch: 080 ----
mean loss: 375.51
 ---- batch: 090 ----
mean loss: 370.00
train mean loss: 379.65
epoch train time: 0:00:02.339450
elapsed time: 0:08:15.322577
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 20:48:22.375345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.57
 ---- batch: 020 ----
mean loss: 370.83
 ---- batch: 030 ----
mean loss: 394.78
 ---- batch: 040 ----
mean loss: 406.00
 ---- batch: 050 ----
mean loss: 391.32
 ---- batch: 060 ----
mean loss: 378.51
 ---- batch: 070 ----
mean loss: 389.86
 ---- batch: 080 ----
mean loss: 402.30
 ---- batch: 090 ----
mean loss: 376.47
train mean loss: 388.86
epoch train time: 0:00:02.331907
elapsed time: 0:08:17.654688
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 20:48:24.707459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.40
 ---- batch: 020 ----
mean loss: 379.46
 ---- batch: 030 ----
mean loss: 384.25
 ---- batch: 040 ----
mean loss: 373.37
 ---- batch: 050 ----
mean loss: 375.62
 ---- batch: 060 ----
mean loss: 387.39
 ---- batch: 070 ----
mean loss: 379.62
 ---- batch: 080 ----
mean loss: 387.23
 ---- batch: 090 ----
mean loss: 380.65
train mean loss: 381.58
epoch train time: 0:00:02.333742
elapsed time: 0:08:19.988632
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 20:48:27.041399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.97
 ---- batch: 020 ----
mean loss: 389.23
 ---- batch: 030 ----
mean loss: 382.69
 ---- batch: 040 ----
mean loss: 377.60
 ---- batch: 050 ----
mean loss: 374.93
 ---- batch: 060 ----
mean loss: 371.42
 ---- batch: 070 ----
mean loss: 380.02
 ---- batch: 080 ----
mean loss: 377.69
 ---- batch: 090 ----
mean loss: 390.17
train mean loss: 381.20
epoch train time: 0:00:02.327991
elapsed time: 0:08:22.316802
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 20:48:29.369574
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.67
 ---- batch: 020 ----
mean loss: 361.66
 ---- batch: 030 ----
mean loss: 361.27
 ---- batch: 040 ----
mean loss: 389.03
 ---- batch: 050 ----
mean loss: 367.36
 ---- batch: 060 ----
mean loss: 370.29
 ---- batch: 070 ----
mean loss: 367.90
 ---- batch: 080 ----
mean loss: 374.86
 ---- batch: 090 ----
mean loss: 371.80
train mean loss: 370.38
epoch train time: 0:00:02.331029
elapsed time: 0:08:24.648051
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 20:48:31.700803
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.49
 ---- batch: 020 ----
mean loss: 367.39
 ---- batch: 030 ----
mean loss: 381.33
 ---- batch: 040 ----
mean loss: 368.09
 ---- batch: 050 ----
mean loss: 388.00
 ---- batch: 060 ----
mean loss: 379.74
 ---- batch: 070 ----
mean loss: 378.27
 ---- batch: 080 ----
mean loss: 367.51
 ---- batch: 090 ----
mean loss: 364.28
train mean loss: 374.38
epoch train time: 0:00:02.336189
elapsed time: 0:08:26.984411
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 20:48:34.037178
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 378.96
 ---- batch: 020 ----
mean loss: 374.09
 ---- batch: 030 ----
mean loss: 358.36
 ---- batch: 040 ----
mean loss: 362.53
 ---- batch: 050 ----
mean loss: 365.75
 ---- batch: 060 ----
mean loss: 370.45
 ---- batch: 070 ----
mean loss: 375.78
 ---- batch: 080 ----
mean loss: 389.13
 ---- batch: 090 ----
mean loss: 373.45
train mean loss: 371.01
epoch train time: 0:00:02.328521
elapsed time: 0:08:29.313123
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 20:48:36.365930
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.14
 ---- batch: 020 ----
mean loss: 381.61
 ---- batch: 030 ----
mean loss: 379.00
 ---- batch: 040 ----
mean loss: 372.68
 ---- batch: 050 ----
mean loss: 363.15
 ---- batch: 060 ----
mean loss: 369.70
 ---- batch: 070 ----
mean loss: 364.17
 ---- batch: 080 ----
mean loss: 385.37
 ---- batch: 090 ----
mean loss: 373.18
train mean loss: 371.90
epoch train time: 0:00:02.337999
elapsed time: 0:08:31.651346
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 20:48:38.704176
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.28
 ---- batch: 020 ----
mean loss: 370.87
 ---- batch: 030 ----
mean loss: 372.86
 ---- batch: 040 ----
mean loss: 365.12
 ---- batch: 050 ----
mean loss: 378.70
 ---- batch: 060 ----
mean loss: 380.88
 ---- batch: 070 ----
mean loss: 364.85
 ---- batch: 080 ----
mean loss: 382.01
 ---- batch: 090 ----
mean loss: 374.06
train mean loss: 371.81
epoch train time: 0:00:02.331512
elapsed time: 0:08:33.983130
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 20:48:41.035898
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.50
 ---- batch: 020 ----
mean loss: 365.62
 ---- batch: 030 ----
mean loss: 366.30
 ---- batch: 040 ----
mean loss: 386.75
 ---- batch: 050 ----
mean loss: 382.43
 ---- batch: 060 ----
mean loss: 372.70
 ---- batch: 070 ----
mean loss: 379.14
 ---- batch: 080 ----
mean loss: 373.96
 ---- batch: 090 ----
mean loss: 367.18
train mean loss: 372.73
epoch train time: 0:00:02.342447
elapsed time: 0:08:36.325792
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 20:48:43.378557
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 374.44
 ---- batch: 020 ----
mean loss: 355.81
 ---- batch: 030 ----
mean loss: 381.62
 ---- batch: 040 ----
mean loss: 377.45
 ---- batch: 050 ----
mean loss: 372.32
 ---- batch: 060 ----
mean loss: 371.65
 ---- batch: 070 ----
mean loss: 375.68
 ---- batch: 080 ----
mean loss: 382.27
 ---- batch: 090 ----
mean loss: 373.20
train mean loss: 375.05
epoch train time: 0:00:02.324881
elapsed time: 0:08:38.650856
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 20:48:45.703625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.53
 ---- batch: 020 ----
mean loss: 370.10
 ---- batch: 030 ----
mean loss: 370.87
 ---- batch: 040 ----
mean loss: 380.63
 ---- batch: 050 ----
mean loss: 370.59
 ---- batch: 060 ----
mean loss: 377.96
 ---- batch: 070 ----
mean loss: 369.03
 ---- batch: 080 ----
mean loss: 383.62
 ---- batch: 090 ----
mean loss: 373.69
train mean loss: 373.01
epoch train time: 0:00:02.332044
elapsed time: 0:08:40.983083
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 20:48:48.035849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.58
 ---- batch: 020 ----
mean loss: 371.28
 ---- batch: 030 ----
mean loss: 365.46
 ---- batch: 040 ----
mean loss: 360.48
 ---- batch: 050 ----
mean loss: 372.21
 ---- batch: 060 ----
mean loss: 373.27
 ---- batch: 070 ----
mean loss: 372.51
 ---- batch: 080 ----
mean loss: 369.62
 ---- batch: 090 ----
mean loss: 370.60
train mean loss: 369.86
epoch train time: 0:00:02.327048
elapsed time: 0:08:43.310309
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 20:48:50.363076
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 377.41
 ---- batch: 020 ----
mean loss: 378.30
 ---- batch: 030 ----
mean loss: 368.85
 ---- batch: 040 ----
mean loss: 359.53
 ---- batch: 050 ----
mean loss: 357.64
 ---- batch: 060 ----
mean loss: 385.54
 ---- batch: 070 ----
mean loss: 376.50
 ---- batch: 080 ----
mean loss: 374.50
 ---- batch: 090 ----
mean loss: 363.20
train mean loss: 370.62
epoch train time: 0:00:02.328386
elapsed time: 0:08:45.638878
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 20:48:52.691680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.95
 ---- batch: 020 ----
mean loss: 364.10
 ---- batch: 030 ----
mean loss: 374.68
 ---- batch: 040 ----
mean loss: 376.21
 ---- batch: 050 ----
mean loss: 369.01
 ---- batch: 060 ----
mean loss: 374.73
 ---- batch: 070 ----
mean loss: 376.77
 ---- batch: 080 ----
mean loss: 370.87
 ---- batch: 090 ----
mean loss: 365.12
train mean loss: 370.85
epoch train time: 0:00:02.323862
elapsed time: 0:08:47.962955
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 20:48:55.015721
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 374.45
 ---- batch: 020 ----
mean loss: 380.54
 ---- batch: 030 ----
mean loss: 372.09
 ---- batch: 040 ----
mean loss: 379.51
 ---- batch: 050 ----
mean loss: 368.97
 ---- batch: 060 ----
mean loss: 364.77
 ---- batch: 070 ----
mean loss: 373.39
 ---- batch: 080 ----
mean loss: 366.54
 ---- batch: 090 ----
mean loss: 381.08
train mean loss: 373.80
epoch train time: 0:00:02.351597
elapsed time: 0:08:50.314740
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 20:48:57.367507
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.12
 ---- batch: 020 ----
mean loss: 366.94
 ---- batch: 030 ----
mean loss: 377.75
 ---- batch: 040 ----
mean loss: 374.42
 ---- batch: 050 ----
mean loss: 377.05
 ---- batch: 060 ----
mean loss: 364.72
 ---- batch: 070 ----
mean loss: 370.46
 ---- batch: 080 ----
mean loss: 375.81
 ---- batch: 090 ----
mean loss: 370.61
train mean loss: 370.91
epoch train time: 0:00:02.328336
elapsed time: 0:08:52.643260
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 20:48:59.696028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 370.38
 ---- batch: 020 ----
mean loss: 370.63
 ---- batch: 030 ----
mean loss: 375.39
 ---- batch: 040 ----
mean loss: 372.56
 ---- batch: 050 ----
mean loss: 364.78
 ---- batch: 060 ----
mean loss: 374.62
 ---- batch: 070 ----
mean loss: 383.28
 ---- batch: 080 ----
mean loss: 369.50
 ---- batch: 090 ----
mean loss: 369.18
train mean loss: 372.59
epoch train time: 0:00:02.352038
elapsed time: 0:08:54.995495
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 20:49:02.048292
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.50
 ---- batch: 020 ----
mean loss: 373.22
 ---- batch: 030 ----
mean loss: 365.29
 ---- batch: 040 ----
mean loss: 371.20
 ---- batch: 050 ----
mean loss: 375.22
 ---- batch: 060 ----
mean loss: 381.04
 ---- batch: 070 ----
mean loss: 366.01
 ---- batch: 080 ----
mean loss: 368.28
 ---- batch: 090 ----
mean loss: 371.15
train mean loss: 371.73
epoch train time: 0:00:02.332954
elapsed time: 0:08:57.328671
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 20:49:04.381456
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.10
 ---- batch: 020 ----
mean loss: 364.67
 ---- batch: 030 ----
mean loss: 358.07
 ---- batch: 040 ----
mean loss: 369.08
 ---- batch: 050 ----
mean loss: 371.37
 ---- batch: 060 ----
mean loss: 371.23
 ---- batch: 070 ----
mean loss: 360.46
 ---- batch: 080 ----
mean loss: 379.76
 ---- batch: 090 ----
mean loss: 360.51
train mean loss: 365.94
epoch train time: 0:00:02.339875
elapsed time: 0:08:59.668759
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 20:49:06.721553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.12
 ---- batch: 020 ----
mean loss: 371.07
 ---- batch: 030 ----
mean loss: 370.36
 ---- batch: 040 ----
mean loss: 363.00
 ---- batch: 050 ----
mean loss: 382.72
 ---- batch: 060 ----
mean loss: 365.60
 ---- batch: 070 ----
mean loss: 361.55
 ---- batch: 080 ----
mean loss: 371.35
 ---- batch: 090 ----
mean loss: 382.28
train mean loss: 370.23
epoch train time: 0:00:02.339166
elapsed time: 0:09:02.008125
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 20:49:09.060889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 374.73
 ---- batch: 020 ----
mean loss: 386.28
 ---- batch: 030 ----
mean loss: 375.01
 ---- batch: 040 ----
mean loss: 378.32
 ---- batch: 050 ----
mean loss: 365.87
 ---- batch: 060 ----
mean loss: 369.24
 ---- batch: 070 ----
mean loss: 361.54
 ---- batch: 080 ----
mean loss: 365.19
 ---- batch: 090 ----
mean loss: 370.17
train mean loss: 372.44
epoch train time: 0:00:02.334372
elapsed time: 0:09:04.342670
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 20:49:11.395470
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.36
 ---- batch: 020 ----
mean loss: 368.93
 ---- batch: 030 ----
mean loss: 371.24
 ---- batch: 040 ----
mean loss: 376.84
 ---- batch: 050 ----
mean loss: 373.74
 ---- batch: 060 ----
mean loss: 370.34
 ---- batch: 070 ----
mean loss: 368.82
 ---- batch: 080 ----
mean loss: 390.22
 ---- batch: 090 ----
mean loss: 363.65
train mean loss: 370.86
epoch train time: 0:00:02.325414
elapsed time: 0:09:06.668303
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 20:49:13.721071
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.58
 ---- batch: 020 ----
mean loss: 368.52
 ---- batch: 030 ----
mean loss: 372.60
 ---- batch: 040 ----
mean loss: 364.32
 ---- batch: 050 ----
mean loss: 381.32
 ---- batch: 060 ----
mean loss: 367.10
 ---- batch: 070 ----
mean loss: 367.35
 ---- batch: 080 ----
mean loss: 376.59
 ---- batch: 090 ----
mean loss: 367.49
train mean loss: 370.75
epoch train time: 0:00:02.334029
elapsed time: 0:09:09.002526
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 20:49:16.055297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.72
 ---- batch: 020 ----
mean loss: 366.75
 ---- batch: 030 ----
mean loss: 370.46
 ---- batch: 040 ----
mean loss: 368.75
 ---- batch: 050 ----
mean loss: 372.05
 ---- batch: 060 ----
mean loss: 379.16
 ---- batch: 070 ----
mean loss: 372.87
 ---- batch: 080 ----
mean loss: 374.12
 ---- batch: 090 ----
mean loss: 363.05
train mean loss: 370.10
epoch train time: 0:00:02.341750
elapsed time: 0:09:11.344470
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 20:49:18.397234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.54
 ---- batch: 020 ----
mean loss: 372.07
 ---- batch: 030 ----
mean loss: 363.84
 ---- batch: 040 ----
mean loss: 369.43
 ---- batch: 050 ----
mean loss: 370.37
 ---- batch: 060 ----
mean loss: 377.14
 ---- batch: 070 ----
mean loss: 378.54
 ---- batch: 080 ----
mean loss: 390.37
 ---- batch: 090 ----
mean loss: 383.68
train mean loss: 374.55
epoch train time: 0:00:02.329529
elapsed time: 0:09:13.674175
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 20:49:20.726950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.17
 ---- batch: 020 ----
mean loss: 375.00
 ---- batch: 030 ----
mean loss: 367.71
 ---- batch: 040 ----
mean loss: 368.67
 ---- batch: 050 ----
mean loss: 372.13
 ---- batch: 060 ----
mean loss: 366.03
 ---- batch: 070 ----
mean loss: 371.28
 ---- batch: 080 ----
mean loss: 368.31
 ---- batch: 090 ----
mean loss: 377.67
train mean loss: 370.15
epoch train time: 0:00:02.328961
elapsed time: 0:09:16.003351
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 20:49:23.056121
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.00
 ---- batch: 020 ----
mean loss: 378.89
 ---- batch: 030 ----
mean loss: 382.58
 ---- batch: 040 ----
mean loss: 374.31
 ---- batch: 050 ----
mean loss: 369.76
 ---- batch: 060 ----
mean loss: 375.14
 ---- batch: 070 ----
mean loss: 363.53
 ---- batch: 080 ----
mean loss: 377.44
 ---- batch: 090 ----
mean loss: 366.58
train mean loss: 373.04
epoch train time: 0:00:02.337655
elapsed time: 0:09:18.341249
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 20:49:25.394016
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.77
 ---- batch: 020 ----
mean loss: 373.07
 ---- batch: 030 ----
mean loss: 373.12
 ---- batch: 040 ----
mean loss: 359.86
 ---- batch: 050 ----
mean loss: 385.46
 ---- batch: 060 ----
mean loss: 372.60
 ---- batch: 070 ----
mean loss: 364.70
 ---- batch: 080 ----
mean loss: 363.18
 ---- batch: 090 ----
mean loss: 374.74
train mean loss: 370.49
epoch train time: 0:00:02.326474
elapsed time: 0:09:20.667929
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 20:49:27.720718
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.66
 ---- batch: 020 ----
mean loss: 365.03
 ---- batch: 030 ----
mean loss: 370.93
 ---- batch: 040 ----
mean loss: 361.61
 ---- batch: 050 ----
mean loss: 363.87
 ---- batch: 060 ----
mean loss: 375.40
 ---- batch: 070 ----
mean loss: 364.43
 ---- batch: 080 ----
mean loss: 377.38
 ---- batch: 090 ----
mean loss: 372.05
train mean loss: 368.43
epoch train time: 0:00:02.326058
elapsed time: 0:09:22.994189
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 20:49:30.046955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.80
 ---- batch: 020 ----
mean loss: 369.99
 ---- batch: 030 ----
mean loss: 365.82
 ---- batch: 040 ----
mean loss: 377.68
 ---- batch: 050 ----
mean loss: 376.69
 ---- batch: 060 ----
mean loss: 364.78
 ---- batch: 070 ----
mean loss: 366.14
 ---- batch: 080 ----
mean loss: 359.52
 ---- batch: 090 ----
mean loss: 373.00
train mean loss: 369.49
epoch train time: 0:00:02.339272
elapsed time: 0:09:25.333641
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 20:49:32.386411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 383.59
 ---- batch: 020 ----
mean loss: 372.74
 ---- batch: 030 ----
mean loss: 373.78
 ---- batch: 040 ----
mean loss: 376.65
 ---- batch: 050 ----
mean loss: 361.87
 ---- batch: 060 ----
mean loss: 373.57
 ---- batch: 070 ----
mean loss: 376.23
 ---- batch: 080 ----
mean loss: 354.95
 ---- batch: 090 ----
mean loss: 363.88
train mean loss: 370.53
epoch train time: 0:00:02.337704
elapsed time: 0:09:27.671534
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 20:49:34.724310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.62
 ---- batch: 020 ----
mean loss: 370.36
 ---- batch: 030 ----
mean loss: 364.32
 ---- batch: 040 ----
mean loss: 371.42
 ---- batch: 050 ----
mean loss: 376.20
 ---- batch: 060 ----
mean loss: 359.42
 ---- batch: 070 ----
mean loss: 375.90
 ---- batch: 080 ----
mean loss: 363.54
 ---- batch: 090 ----
mean loss: 367.70
train mean loss: 367.37
epoch train time: 0:00:02.330632
elapsed time: 0:09:30.002424
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 20:49:37.055207
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 372.67
 ---- batch: 020 ----
mean loss: 365.59
 ---- batch: 030 ----
mean loss: 372.12
 ---- batch: 040 ----
mean loss: 383.10
 ---- batch: 050 ----
mean loss: 365.62
 ---- batch: 060 ----
mean loss: 373.76
 ---- batch: 070 ----
mean loss: 360.33
 ---- batch: 080 ----
mean loss: 378.70
 ---- batch: 090 ----
mean loss: 364.25
train mean loss: 370.14
epoch train time: 0:00:02.336083
elapsed time: 0:09:32.338713
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 20:49:39.391485
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.60
 ---- batch: 020 ----
mean loss: 357.63
 ---- batch: 030 ----
mean loss: 382.91
 ---- batch: 040 ----
mean loss: 362.38
 ---- batch: 050 ----
mean loss: 381.03
 ---- batch: 060 ----
mean loss: 369.41
 ---- batch: 070 ----
mean loss: 365.38
 ---- batch: 080 ----
mean loss: 381.87
 ---- batch: 090 ----
mean loss: 375.84
train mean loss: 372.82
epoch train time: 0:00:02.323206
elapsed time: 0:09:34.662112
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 20:49:41.714877
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 384.21
 ---- batch: 020 ----
mean loss: 369.48
 ---- batch: 030 ----
mean loss: 372.14
 ---- batch: 040 ----
mean loss: 357.26
 ---- batch: 050 ----
mean loss: 360.73
 ---- batch: 060 ----
mean loss: 376.24
 ---- batch: 070 ----
mean loss: 380.81
 ---- batch: 080 ----
mean loss: 373.12
 ---- batch: 090 ----
mean loss: 373.82
train mean loss: 370.75
epoch train time: 0:00:02.334380
elapsed time: 0:09:36.996679
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 20:49:44.049445
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.38
 ---- batch: 020 ----
mean loss: 369.46
 ---- batch: 030 ----
mean loss: 364.09
 ---- batch: 040 ----
mean loss: 371.26
 ---- batch: 050 ----
mean loss: 369.51
 ---- batch: 060 ----
mean loss: 366.06
 ---- batch: 070 ----
mean loss: 366.35
 ---- batch: 080 ----
mean loss: 370.78
 ---- batch: 090 ----
mean loss: 361.00
train mean loss: 368.72
epoch train time: 0:00:02.339209
elapsed time: 0:09:39.336085
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 20:49:46.388862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 370.11
 ---- batch: 020 ----
mean loss: 363.45
 ---- batch: 030 ----
mean loss: 362.57
 ---- batch: 040 ----
mean loss: 372.29
 ---- batch: 050 ----
mean loss: 380.24
 ---- batch: 060 ----
mean loss: 375.93
 ---- batch: 070 ----
mean loss: 376.96
 ---- batch: 080 ----
mean loss: 371.58
 ---- batch: 090 ----
mean loss: 360.05
train mean loss: 369.61
epoch train time: 0:00:02.327050
elapsed time: 0:09:41.663337
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 20:49:48.716114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.96
 ---- batch: 020 ----
mean loss: 363.42
 ---- batch: 030 ----
mean loss: 375.75
 ---- batch: 040 ----
mean loss: 373.53
 ---- batch: 050 ----
mean loss: 374.14
 ---- batch: 060 ----
mean loss: 365.75
 ---- batch: 070 ----
mean loss: 371.98
 ---- batch: 080 ----
mean loss: 374.57
 ---- batch: 090 ----
mean loss: 371.67
train mean loss: 371.23
epoch train time: 0:00:02.331064
elapsed time: 0:09:43.994591
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 20:49:51.047356
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.63
 ---- batch: 020 ----
mean loss: 357.78
 ---- batch: 030 ----
mean loss: 362.47
 ---- batch: 040 ----
mean loss: 381.46
 ---- batch: 050 ----
mean loss: 365.94
 ---- batch: 060 ----
mean loss: 358.09
 ---- batch: 070 ----
mean loss: 373.84
 ---- batch: 080 ----
mean loss: 374.52
 ---- batch: 090 ----
mean loss: 371.38
train mean loss: 367.61
epoch train time: 0:00:02.332315
elapsed time: 0:09:46.327102
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 20:49:53.379869
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 372.88
 ---- batch: 020 ----
mean loss: 366.45
 ---- batch: 030 ----
mean loss: 381.71
 ---- batch: 040 ----
mean loss: 375.09
 ---- batch: 050 ----
mean loss: 362.87
 ---- batch: 060 ----
mean loss: 362.08
 ---- batch: 070 ----
mean loss: 372.22
 ---- batch: 080 ----
mean loss: 377.31
 ---- batch: 090 ----
mean loss: 374.79
train mean loss: 371.56
epoch train time: 0:00:02.332417
elapsed time: 0:09:48.659706
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 20:49:55.712473
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.08
 ---- batch: 020 ----
mean loss: 375.98
 ---- batch: 030 ----
mean loss: 368.72
 ---- batch: 040 ----
mean loss: 366.42
 ---- batch: 050 ----
mean loss: 360.30
 ---- batch: 060 ----
mean loss: 371.67
 ---- batch: 070 ----
mean loss: 368.86
 ---- batch: 080 ----
mean loss: 358.34
 ---- batch: 090 ----
mean loss: 373.56
train mean loss: 368.84
epoch train time: 0:00:02.330453
elapsed time: 0:09:50.990352
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 20:49:58.043121
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.80
 ---- batch: 020 ----
mean loss: 371.38
 ---- batch: 030 ----
mean loss: 365.91
 ---- batch: 040 ----
mean loss: 363.09
 ---- batch: 050 ----
mean loss: 364.95
 ---- batch: 060 ----
mean loss: 372.01
 ---- batch: 070 ----
mean loss: 364.88
 ---- batch: 080 ----
mean loss: 373.40
 ---- batch: 090 ----
mean loss: 374.28
train mean loss: 368.33
epoch train time: 0:00:02.341310
elapsed time: 0:09:53.331839
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 20:50:00.384603
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 370.66
 ---- batch: 020 ----
mean loss: 368.91
 ---- batch: 030 ----
mean loss: 374.18
 ---- batch: 040 ----
mean loss: 368.25
 ---- batch: 050 ----
mean loss: 366.23
 ---- batch: 060 ----
mean loss: 368.70
 ---- batch: 070 ----
mean loss: 381.20
 ---- batch: 080 ----
mean loss: 374.26
 ---- batch: 090 ----
mean loss: 373.87
train mean loss: 371.04
epoch train time: 0:00:02.329744
elapsed time: 0:09:55.661768
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 20:50:02.714554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 376.61
 ---- batch: 020 ----
mean loss: 357.82
 ---- batch: 030 ----
mean loss: 383.07
 ---- batch: 040 ----
mean loss: 356.39
 ---- batch: 050 ----
mean loss: 366.60
 ---- batch: 060 ----
mean loss: 361.28
 ---- batch: 070 ----
mean loss: 367.65
 ---- batch: 080 ----
mean loss: 365.54
 ---- batch: 090 ----
mean loss: 377.08
train mean loss: 368.47
epoch train time: 0:00:02.335371
elapsed time: 0:09:57.997388
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 20:50:05.050170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 387.03
 ---- batch: 020 ----
mean loss: 368.34
 ---- batch: 030 ----
mean loss: 384.53
 ---- batch: 040 ----
mean loss: 371.93
 ---- batch: 050 ----
mean loss: 374.82
 ---- batch: 060 ----
mean loss: 369.35
 ---- batch: 070 ----
mean loss: 368.14
 ---- batch: 080 ----
mean loss: 362.44
 ---- batch: 090 ----
mean loss: 375.49
train mean loss: 374.28
epoch train time: 0:00:02.332593
elapsed time: 0:10:00.330166
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 20:50:07.382939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.33
 ---- batch: 020 ----
mean loss: 371.60
 ---- batch: 030 ----
mean loss: 371.55
 ---- batch: 040 ----
mean loss: 366.09
 ---- batch: 050 ----
mean loss: 357.00
 ---- batch: 060 ----
mean loss: 375.35
 ---- batch: 070 ----
mean loss: 366.36
 ---- batch: 080 ----
mean loss: 356.67
 ---- batch: 090 ----
mean loss: 375.83
train mean loss: 368.08
epoch train time: 0:00:02.327874
elapsed time: 0:10:02.658240
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 20:50:09.711006
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 374.91
 ---- batch: 020 ----
mean loss: 379.78
 ---- batch: 030 ----
mean loss: 363.84
 ---- batch: 040 ----
mean loss: 360.62
 ---- batch: 050 ----
mean loss: 374.18
 ---- batch: 060 ----
mean loss: 363.88
 ---- batch: 070 ----
mean loss: 368.92
 ---- batch: 080 ----
mean loss: 356.82
 ---- batch: 090 ----
mean loss: 379.79
train mean loss: 370.31
epoch train time: 0:00:02.332469
elapsed time: 0:10:04.990890
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 20:50:12.043660
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.14
 ---- batch: 020 ----
mean loss: 364.33
 ---- batch: 030 ----
mean loss: 368.16
 ---- batch: 040 ----
mean loss: 382.18
 ---- batch: 050 ----
mean loss: 367.88
 ---- batch: 060 ----
mean loss: 368.57
 ---- batch: 070 ----
mean loss: 368.13
 ---- batch: 080 ----
mean loss: 375.12
 ---- batch: 090 ----
mean loss: 377.46
train mean loss: 369.12
epoch train time: 0:00:02.333223
elapsed time: 0:10:07.324299
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 20:50:14.377068
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 380.08
 ---- batch: 020 ----
mean loss: 372.39
 ---- batch: 030 ----
mean loss: 360.65
 ---- batch: 040 ----
mean loss: 372.90
 ---- batch: 050 ----
mean loss: 371.82
 ---- batch: 060 ----
mean loss: 356.06
 ---- batch: 070 ----
mean loss: 370.36
 ---- batch: 080 ----
mean loss: 365.60
 ---- batch: 090 ----
mean loss: 371.42
train mean loss: 369.60
epoch train time: 0:00:02.328164
elapsed time: 0:10:09.652644
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 20:50:16.705440
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 361.32
 ---- batch: 020 ----
mean loss: 369.60
 ---- batch: 030 ----
mean loss: 362.15
 ---- batch: 040 ----
mean loss: 378.02
 ---- batch: 050 ----
mean loss: 374.74
 ---- batch: 060 ----
mean loss: 376.70
 ---- batch: 070 ----
mean loss: 360.33
 ---- batch: 080 ----
mean loss: 368.30
 ---- batch: 090 ----
mean loss: 376.01
train mean loss: 369.65
epoch train time: 0:00:02.329398
elapsed time: 0:10:11.982255
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 20:50:19.035037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 370.81
 ---- batch: 020 ----
mean loss: 363.14
 ---- batch: 030 ----
mean loss: 365.33
 ---- batch: 040 ----
mean loss: 377.82
 ---- batch: 050 ----
mean loss: 362.64
 ---- batch: 060 ----
mean loss: 372.93
 ---- batch: 070 ----
mean loss: 385.66
 ---- batch: 080 ----
mean loss: 364.36
 ---- batch: 090 ----
mean loss: 364.56
train mean loss: 370.27
epoch train time: 0:00:02.328944
elapsed time: 0:10:14.311388
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 20:50:21.364174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.49
 ---- batch: 020 ----
mean loss: 374.75
 ---- batch: 030 ----
mean loss: 366.81
 ---- batch: 040 ----
mean loss: 370.12
 ---- batch: 050 ----
mean loss: 378.58
 ---- batch: 060 ----
mean loss: 369.80
 ---- batch: 070 ----
mean loss: 366.49
 ---- batch: 080 ----
mean loss: 373.23
 ---- batch: 090 ----
mean loss: 373.95
train mean loss: 370.19
epoch train time: 0:00:02.333875
elapsed time: 0:10:16.649019
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_0/checkpoint.pth.tar
**** end time: 2019-09-25 20:50:23.701742 ****
