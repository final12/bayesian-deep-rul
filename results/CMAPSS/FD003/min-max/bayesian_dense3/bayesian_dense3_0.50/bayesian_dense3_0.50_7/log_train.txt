Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 23030
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-10-01 14:34:45.570563 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-10-01 14:34:45.579929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4464.91
 ---- batch: 020 ----
mean loss: 4128.03
train mean loss: 4275.71
epoch train time: 0:00:07.732221
elapsed time: 0:00:07.748219
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-10-01 14:34:53.318829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3833.02
 ---- batch: 020 ----
mean loss: 3592.90
train mean loss: 3692.02
epoch train time: 0:00:00.347585
elapsed time: 0:00:08.095967
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-10-01 14:34:53.666594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3322.47
 ---- batch: 020 ----
mean loss: 3268.85
train mean loss: 3289.54
epoch train time: 0:00:00.356630
elapsed time: 0:00:08.452789
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-10-01 14:34:54.023434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3067.62
 ---- batch: 020 ----
mean loss: 2927.34
train mean loss: 2975.99
epoch train time: 0:00:00.356830
elapsed time: 0:00:08.809813
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-10-01 14:34:54.380434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2805.21
 ---- batch: 020 ----
mean loss: 2698.89
train mean loss: 2754.90
epoch train time: 0:00:00.354385
elapsed time: 0:00:09.164373
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-10-01 14:34:54.734996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2626.10
 ---- batch: 020 ----
mean loss: 2530.57
train mean loss: 2567.69
epoch train time: 0:00:00.360217
elapsed time: 0:00:09.524778
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-10-01 14:34:55.095401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2475.94
 ---- batch: 020 ----
mean loss: 2402.28
train mean loss: 2432.96
epoch train time: 0:00:00.360778
elapsed time: 0:00:09.885746
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-10-01 14:34:55.456373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2367.40
 ---- batch: 020 ----
mean loss: 2287.56
train mean loss: 2323.84
epoch train time: 0:00:00.361967
elapsed time: 0:00:10.247901
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-10-01 14:34:55.818529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2268.53
 ---- batch: 020 ----
mean loss: 2203.58
train mean loss: 2225.20
epoch train time: 0:00:00.359219
elapsed time: 0:00:10.607320
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-10-01 14:34:56.177946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2178.40
 ---- batch: 020 ----
mean loss: 2132.34
train mean loss: 2145.02
epoch train time: 0:00:00.355161
elapsed time: 0:00:10.962675
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-10-01 14:34:56.533317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2068.30
 ---- batch: 020 ----
mean loss: 2078.03
train mean loss: 2076.32
epoch train time: 0:00:00.358152
elapsed time: 0:00:11.321017
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-10-01 14:34:56.891641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2028.57
 ---- batch: 020 ----
mean loss: 1968.44
train mean loss: 2004.05
epoch train time: 0:00:00.355301
elapsed time: 0:00:11.676517
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-10-01 14:34:57.247145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1939.43
 ---- batch: 020 ----
mean loss: 1946.58
train mean loss: 1938.98
epoch train time: 0:00:00.354689
elapsed time: 0:00:12.031385
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-10-01 14:34:57.602005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1888.50
 ---- batch: 020 ----
mean loss: 1858.10
train mean loss: 1871.65
epoch train time: 0:00:00.380566
elapsed time: 0:00:12.412203
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-10-01 14:34:57.982870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1834.98
 ---- batch: 020 ----
mean loss: 1786.55
train mean loss: 1817.13
epoch train time: 0:00:00.351612
elapsed time: 0:00:12.764036
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-10-01 14:34:58.334658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1791.91
 ---- batch: 020 ----
mean loss: 1730.27
train mean loss: 1758.14
epoch train time: 0:00:00.351154
elapsed time: 0:00:13.115383
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-10-01 14:34:58.686004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1724.49
 ---- batch: 020 ----
mean loss: 1692.24
train mean loss: 1702.18
epoch train time: 0:00:00.356215
elapsed time: 0:00:13.471779
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-10-01 14:34:59.042401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1671.33
 ---- batch: 020 ----
mean loss: 1627.29
train mean loss: 1652.79
epoch train time: 0:00:00.354712
elapsed time: 0:00:13.826674
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-10-01 14:34:59.397311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1600.45
 ---- batch: 020 ----
mean loss: 1599.76
train mean loss: 1598.90
epoch train time: 0:00:00.360612
elapsed time: 0:00:14.187489
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-10-01 14:34:59.758119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1544.61
 ---- batch: 020 ----
mean loss: 1562.43
train mean loss: 1545.11
epoch train time: 0:00:00.367630
elapsed time: 0:00:14.555324
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-10-01 14:35:00.125951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1524.16
 ---- batch: 020 ----
mean loss: 1494.56
train mean loss: 1503.70
epoch train time: 0:00:00.353753
elapsed time: 0:00:14.909260
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-10-01 14:35:00.479915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1477.43
 ---- batch: 020 ----
mean loss: 1450.33
train mean loss: 1456.65
epoch train time: 0:00:00.363476
elapsed time: 0:00:15.272960
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-10-01 14:35:00.843584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1436.50
 ---- batch: 020 ----
mean loss: 1422.29
train mean loss: 1418.98
epoch train time: 0:00:00.358189
elapsed time: 0:00:15.631369
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-10-01 14:35:01.201994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1390.61
 ---- batch: 020 ----
mean loss: 1386.33
train mean loss: 1387.11
epoch train time: 0:00:00.351480
elapsed time: 0:00:15.983039
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-10-01 14:35:01.553679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1347.97
 ---- batch: 020 ----
mean loss: 1326.83
train mean loss: 1344.63
epoch train time: 0:00:00.359386
elapsed time: 0:00:16.342631
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-10-01 14:35:01.913267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1320.88
 ---- batch: 020 ----
mean loss: 1300.83
train mean loss: 1303.72
epoch train time: 0:00:00.354296
elapsed time: 0:00:16.697151
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-10-01 14:35:02.267775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1280.93
 ---- batch: 020 ----
mean loss: 1266.59
train mean loss: 1265.48
epoch train time: 0:00:00.354046
elapsed time: 0:00:17.051393
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-10-01 14:35:02.622014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1259.27
 ---- batch: 020 ----
mean loss: 1219.71
train mean loss: 1240.11
epoch train time: 0:00:00.361688
elapsed time: 0:00:17.413296
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-10-01 14:35:02.983937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1221.00
 ---- batch: 020 ----
mean loss: 1191.11
train mean loss: 1200.80
epoch train time: 0:00:00.352745
elapsed time: 0:00:17.766248
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-10-01 14:35:03.336876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1174.29
 ---- batch: 020 ----
mean loss: 1146.51
train mean loss: 1161.05
epoch train time: 0:00:00.357119
elapsed time: 0:00:18.123566
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-10-01 14:35:03.694213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1104.71
 ---- batch: 020 ----
mean loss: 1088.35
train mean loss: 1097.81
epoch train time: 0:00:00.355469
elapsed time: 0:00:18.479283
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-10-01 14:35:04.049931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1048.08
 ---- batch: 020 ----
mean loss: 1036.76
train mean loss: 1040.06
epoch train time: 0:00:00.348470
elapsed time: 0:00:18.827964
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-10-01 14:35:04.398588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1008.34
 ---- batch: 020 ----
mean loss: 985.51
train mean loss: 994.72
epoch train time: 0:00:00.351214
elapsed time: 0:00:19.179354
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-10-01 14:35:04.749978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 952.92
 ---- batch: 020 ----
mean loss: 957.73
train mean loss: 954.98
epoch train time: 0:00:00.350205
elapsed time: 0:00:19.529733
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-10-01 14:35:05.100394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.97
 ---- batch: 020 ----
mean loss: 915.11
train mean loss: 920.73
epoch train time: 0:00:00.343015
elapsed time: 0:00:19.872956
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-10-01 14:35:05.443581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.08
 ---- batch: 020 ----
mean loss: 885.28
train mean loss: 888.95
epoch train time: 0:00:00.360776
elapsed time: 0:00:20.233925
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-10-01 14:35:05.804567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.32
 ---- batch: 020 ----
mean loss: 851.34
train mean loss: 859.87
epoch train time: 0:00:00.353547
elapsed time: 0:00:20.587679
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-10-01 14:35:06.158312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.03
 ---- batch: 020 ----
mean loss: 819.69
train mean loss: 827.05
epoch train time: 0:00:00.346762
elapsed time: 0:00:20.934621
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-10-01 14:35:06.505245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 812.84
 ---- batch: 020 ----
mean loss: 783.85
train mean loss: 800.27
epoch train time: 0:00:00.349105
elapsed time: 0:00:21.283914
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-10-01 14:35:06.854536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 765.15
 ---- batch: 020 ----
mean loss: 772.34
train mean loss: 771.44
epoch train time: 0:00:00.350970
elapsed time: 0:00:21.635058
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-10-01 14:35:07.205682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.43
 ---- batch: 020 ----
mean loss: 732.89
train mean loss: 743.85
epoch train time: 0:00:00.351310
elapsed time: 0:00:21.986544
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-10-01 14:35:07.557166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 719.69
 ---- batch: 020 ----
mean loss: 714.66
train mean loss: 716.14
epoch train time: 0:00:00.357552
elapsed time: 0:00:22.344273
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-10-01 14:35:07.914900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.38
 ---- batch: 020 ----
mean loss: 680.70
train mean loss: 691.07
epoch train time: 0:00:00.353449
elapsed time: 0:00:22.697901
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-10-01 14:35:08.268525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 680.15
 ---- batch: 020 ----
mean loss: 656.61
train mean loss: 666.10
epoch train time: 0:00:00.352402
elapsed time: 0:00:23.050493
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-10-01 14:35:08.621119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 643.98
 ---- batch: 020 ----
mean loss: 638.41
train mean loss: 642.93
epoch train time: 0:00:00.355649
elapsed time: 0:00:23.406319
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-10-01 14:35:08.976952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 638.29
 ---- batch: 020 ----
mean loss: 612.49
train mean loss: 621.28
epoch train time: 0:00:00.345168
elapsed time: 0:00:23.751710
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-10-01 14:35:09.322338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.42
 ---- batch: 020 ----
mean loss: 593.51
train mean loss: 597.81
epoch train time: 0:00:00.349327
elapsed time: 0:00:24.101219
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-10-01 14:35:09.671854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 582.23
 ---- batch: 020 ----
mean loss: 569.09
train mean loss: 574.44
epoch train time: 0:00:00.363710
elapsed time: 0:00:24.465118
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-10-01 14:35:10.035745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 553.33
 ---- batch: 020 ----
mean loss: 562.19
train mean loss: 558.06
epoch train time: 0:00:00.353625
elapsed time: 0:00:24.818934
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-10-01 14:35:10.389604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.57
 ---- batch: 020 ----
mean loss: 537.49
train mean loss: 538.25
epoch train time: 0:00:00.358720
elapsed time: 0:00:25.177888
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-10-01 14:35:10.748510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 523.77
 ---- batch: 020 ----
mean loss: 517.27
train mean loss: 520.46
epoch train time: 0:00:00.353757
elapsed time: 0:00:25.531823
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-10-01 14:35:11.102445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.17
 ---- batch: 020 ----
mean loss: 490.74
train mean loss: 503.57
epoch train time: 0:00:00.356533
elapsed time: 0:00:25.888546
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-10-01 14:35:11.459171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.48
 ---- batch: 020 ----
mean loss: 477.72
train mean loss: 481.06
epoch train time: 0:00:00.352195
elapsed time: 0:00:26.240920
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-10-01 14:35:11.811542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.29
 ---- batch: 020 ----
mean loss: 465.82
train mean loss: 466.62
epoch train time: 0:00:00.370739
elapsed time: 0:00:26.611842
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-10-01 14:35:12.182481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.41
 ---- batch: 020 ----
mean loss: 453.39
train mean loss: 455.23
epoch train time: 0:00:00.352708
elapsed time: 0:00:26.964748
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-10-01 14:35:12.535368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.20
 ---- batch: 020 ----
mean loss: 428.69
train mean loss: 437.26
epoch train time: 0:00:00.351188
elapsed time: 0:00:27.316105
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-10-01 14:35:12.886738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.68
 ---- batch: 020 ----
mean loss: 417.90
train mean loss: 420.82
epoch train time: 0:00:00.349193
elapsed time: 0:00:27.665492
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-10-01 14:35:13.236127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.03
 ---- batch: 020 ----
mean loss: 405.29
train mean loss: 407.88
epoch train time: 0:00:00.346322
elapsed time: 0:00:28.012017
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-10-01 14:35:13.582637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.36
 ---- batch: 020 ----
mean loss: 388.26
train mean loss: 391.48
epoch train time: 0:00:00.378227
elapsed time: 0:00:28.390536
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-10-01 14:35:13.961210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.44
 ---- batch: 020 ----
mean loss: 380.74
train mean loss: 380.86
epoch train time: 0:00:00.356450
elapsed time: 0:00:28.747418
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-10-01 14:35:14.318051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.18
 ---- batch: 020 ----
mean loss: 365.74
train mean loss: 366.62
epoch train time: 0:00:00.357970
elapsed time: 0:00:29.105576
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-10-01 14:35:14.676212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.71
 ---- batch: 020 ----
mean loss: 357.42
train mean loss: 356.68
epoch train time: 0:00:00.351705
elapsed time: 0:00:29.457472
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-10-01 14:35:15.028098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.62
 ---- batch: 020 ----
mean loss: 336.94
train mean loss: 344.36
epoch train time: 0:00:00.344590
elapsed time: 0:00:29.802237
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-10-01 14:35:15.372860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.84
 ---- batch: 020 ----
mean loss: 331.26
train mean loss: 333.00
epoch train time: 0:00:00.359703
elapsed time: 0:00:30.162121
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-10-01 14:35:15.732748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.44
 ---- batch: 020 ----
mean loss: 320.23
train mean loss: 320.54
epoch train time: 0:00:00.367744
elapsed time: 0:00:30.530049
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-10-01 14:35:16.100685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.62
 ---- batch: 020 ----
mean loss: 307.83
train mean loss: 310.87
epoch train time: 0:00:00.347725
elapsed time: 0:00:30.877971
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-10-01 14:35:16.448592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.86
 ---- batch: 020 ----
mean loss: 299.34
train mean loss: 300.79
epoch train time: 0:00:00.355532
elapsed time: 0:00:31.233709
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-10-01 14:35:16.804384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.71
 ---- batch: 020 ----
mean loss: 288.85
train mean loss: 293.11
epoch train time: 0:00:00.359270
elapsed time: 0:00:31.593222
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-10-01 14:35:17.163841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.88
 ---- batch: 020 ----
mean loss: 283.29
train mean loss: 281.85
epoch train time: 0:00:00.361878
elapsed time: 0:00:31.955300
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-10-01 14:35:17.525926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.05
 ---- batch: 020 ----
mean loss: 272.24
train mean loss: 274.68
epoch train time: 0:00:00.368779
elapsed time: 0:00:32.324253
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-10-01 14:35:17.894873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.00
 ---- batch: 020 ----
mean loss: 262.62
train mean loss: 264.41
epoch train time: 0:00:00.353840
elapsed time: 0:00:32.678266
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-10-01 14:35:18.248890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.56
 ---- batch: 020 ----
mean loss: 255.56
train mean loss: 255.79
epoch train time: 0:00:00.355760
elapsed time: 0:00:33.034204
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-10-01 14:35:18.604831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.06
 ---- batch: 020 ----
mean loss: 246.88
train mean loss: 249.32
epoch train time: 0:00:00.359090
elapsed time: 0:00:33.393480
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-10-01 14:35:18.964118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.72
 ---- batch: 020 ----
mean loss: 244.83
train mean loss: 242.11
epoch train time: 0:00:00.355574
elapsed time: 0:00:33.749278
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-10-01 14:35:19.319903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.35
 ---- batch: 020 ----
mean loss: 232.41
train mean loss: 233.38
epoch train time: 0:00:00.356475
elapsed time: 0:00:34.105939
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-10-01 14:35:19.676561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.17
 ---- batch: 020 ----
mean loss: 222.06
train mean loss: 226.39
epoch train time: 0:00:00.360547
elapsed time: 0:00:34.466656
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-10-01 14:35:20.037279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.71
 ---- batch: 020 ----
mean loss: 220.90
train mean loss: 221.31
epoch train time: 0:00:00.348926
elapsed time: 0:00:34.815758
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-10-01 14:35:20.386384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.91
 ---- batch: 020 ----
mean loss: 218.42
train mean loss: 215.58
epoch train time: 0:00:00.347128
elapsed time: 0:00:35.163089
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-10-01 14:35:20.733783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.62
 ---- batch: 020 ----
mean loss: 209.05
train mean loss: 209.19
epoch train time: 0:00:00.360231
elapsed time: 0:00:35.523576
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-10-01 14:35:21.094203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.73
 ---- batch: 020 ----
mean loss: 202.43
train mean loss: 202.61
epoch train time: 0:00:00.360530
elapsed time: 0:00:35.884289
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-10-01 14:35:21.454937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.25
 ---- batch: 020 ----
mean loss: 198.13
train mean loss: 198.40
epoch train time: 0:00:00.362127
elapsed time: 0:00:36.246621
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-10-01 14:35:21.817260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.53
 ---- batch: 020 ----
mean loss: 191.21
train mean loss: 193.24
epoch train time: 0:00:00.358751
elapsed time: 0:00:36.605581
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-10-01 14:35:22.176203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.83
 ---- batch: 020 ----
mean loss: 184.99
train mean loss: 186.74
epoch train time: 0:00:00.354921
elapsed time: 0:00:36.960672
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-10-01 14:35:22.531295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.04
 ---- batch: 020 ----
mean loss: 178.00
train mean loss: 181.78
epoch train time: 0:00:00.349049
elapsed time: 0:00:37.309904
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-10-01 14:35:22.880524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.47
 ---- batch: 020 ----
mean loss: 174.80
train mean loss: 178.38
epoch train time: 0:00:00.345924
elapsed time: 0:00:37.656028
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-10-01 14:35:23.226651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.05
 ---- batch: 020 ----
mean loss: 173.21
train mean loss: 172.78
epoch train time: 0:00:00.345769
elapsed time: 0:00:38.001987
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-10-01 14:35:23.572611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.02
 ---- batch: 020 ----
mean loss: 168.93
train mean loss: 168.67
epoch train time: 0:00:00.360860
elapsed time: 0:00:38.363053
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-10-01 14:35:23.933694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.44
 ---- batch: 020 ----
mean loss: 162.69
train mean loss: 165.07
epoch train time: 0:00:00.352557
elapsed time: 0:00:38.715805
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-10-01 14:35:24.286425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.16
 ---- batch: 020 ----
mean loss: 158.43
train mean loss: 160.31
epoch train time: 0:00:00.354981
elapsed time: 0:00:39.070972
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-10-01 14:35:24.641596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.40
 ---- batch: 020 ----
mean loss: 154.66
train mean loss: 155.23
epoch train time: 0:00:00.354757
elapsed time: 0:00:39.425951
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-10-01 14:35:24.996576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.71
 ---- batch: 020 ----
mean loss: 154.74
train mean loss: 153.68
epoch train time: 0:00:00.352793
elapsed time: 0:00:39.778932
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-10-01 14:35:25.349559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.46
 ---- batch: 020 ----
mean loss: 149.33
train mean loss: 150.23
epoch train time: 0:00:00.354311
elapsed time: 0:00:40.133484
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-10-01 14:35:25.704116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.23
 ---- batch: 020 ----
mean loss: 146.44
train mean loss: 146.58
epoch train time: 0:00:00.369708
elapsed time: 0:00:40.503383
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-10-01 14:35:26.074010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.71
 ---- batch: 020 ----
mean loss: 141.79
train mean loss: 143.79
epoch train time: 0:00:00.355647
elapsed time: 0:00:40.859256
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-10-01 14:35:26.429882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.11
 ---- batch: 020 ----
mean loss: 138.28
train mean loss: 140.86
epoch train time: 0:00:00.352269
elapsed time: 0:00:41.211718
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-10-01 14:35:26.782348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.90
 ---- batch: 020 ----
mean loss: 136.62
train mean loss: 137.23
epoch train time: 0:00:00.359688
elapsed time: 0:00:41.571616
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-10-01 14:35:27.142266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.79
 ---- batch: 020 ----
mean loss: 133.31
train mean loss: 134.77
epoch train time: 0:00:00.360734
elapsed time: 0:00:41.932554
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-10-01 14:35:27.503219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.90
 ---- batch: 020 ----
mean loss: 134.04
train mean loss: 132.42
epoch train time: 0:00:00.371824
elapsed time: 0:00:42.304592
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-10-01 14:35:27.875230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.56
 ---- batch: 020 ----
mean loss: 129.40
train mean loss: 130.92
epoch train time: 0:00:00.346980
elapsed time: 0:00:42.651778
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-10-01 14:35:28.222405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.23
 ---- batch: 020 ----
mean loss: 125.75
train mean loss: 127.47
epoch train time: 0:00:00.347677
elapsed time: 0:00:42.999644
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-10-01 14:35:28.570265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.92
 ---- batch: 020 ----
mean loss: 125.15
train mean loss: 126.09
epoch train time: 0:00:00.353461
elapsed time: 0:00:43.353279
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-10-01 14:35:28.923928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.02
 ---- batch: 020 ----
mean loss: 119.99
train mean loss: 122.69
epoch train time: 0:00:00.359108
elapsed time: 0:00:43.712592
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-10-01 14:35:29.283229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.13
 ---- batch: 020 ----
mean loss: 119.75
train mean loss: 119.51
epoch train time: 0:00:00.357993
elapsed time: 0:00:44.070788
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-10-01 14:35:29.641409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.42
 ---- batch: 020 ----
mean loss: 119.54
train mean loss: 119.82
epoch train time: 0:00:00.364540
elapsed time: 0:00:44.435506
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-10-01 14:35:30.006127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.18
 ---- batch: 020 ----
mean loss: 120.29
train mean loss: 117.65
epoch train time: 0:00:00.353151
elapsed time: 0:00:44.788833
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-10-01 14:35:30.359454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.02
 ---- batch: 020 ----
mean loss: 114.70
train mean loss: 113.44
epoch train time: 0:00:00.351441
elapsed time: 0:00:45.140456
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-10-01 14:35:30.711093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.47
 ---- batch: 020 ----
mean loss: 112.84
train mean loss: 114.02
epoch train time: 0:00:00.354897
elapsed time: 0:00:45.495575
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-10-01 14:35:31.066179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.18
 ---- batch: 020 ----
mean loss: 112.74
train mean loss: 109.76
epoch train time: 0:00:00.356995
elapsed time: 0:00:45.852739
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-10-01 14:35:31.423364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.08
 ---- batch: 020 ----
mean loss: 106.70
train mean loss: 109.41
epoch train time: 0:00:00.363008
elapsed time: 0:00:46.215927
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-10-01 14:35:31.786551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.75
 ---- batch: 020 ----
mean loss: 110.09
train mean loss: 110.28
epoch train time: 0:00:00.356556
elapsed time: 0:00:46.572698
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-10-01 14:35:32.143358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.88
 ---- batch: 020 ----
mean loss: 105.87
train mean loss: 105.70
epoch train time: 0:00:00.356724
elapsed time: 0:00:46.929650
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-10-01 14:35:32.500320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.97
 ---- batch: 020 ----
mean loss: 101.33
train mean loss: 105.27
epoch train time: 0:00:00.357101
elapsed time: 0:00:47.286989
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-10-01 14:35:32.857610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.09
 ---- batch: 020 ----
mean loss: 103.88
train mean loss: 104.38
epoch train time: 0:00:00.354625
elapsed time: 0:00:47.641793
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-10-01 14:35:33.212415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.65
 ---- batch: 020 ----
mean loss: 103.52
train mean loss: 102.01
epoch train time: 0:00:00.355990
elapsed time: 0:00:47.997957
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-10-01 14:35:33.568579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.68
 ---- batch: 020 ----
mean loss: 99.36
train mean loss: 100.58
epoch train time: 0:00:00.350464
elapsed time: 0:00:48.348590
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-10-01 14:35:33.919208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.01
 ---- batch: 020 ----
mean loss: 101.99
train mean loss: 99.48
epoch train time: 0:00:00.346232
elapsed time: 0:00:48.695005
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-10-01 14:35:34.265625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.42
 ---- batch: 020 ----
mean loss: 96.44
train mean loss: 99.98
epoch train time: 0:00:00.346448
elapsed time: 0:00:49.041662
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-10-01 14:35:34.612285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.14
 ---- batch: 020 ----
mean loss: 98.55
train mean loss: 96.42
epoch train time: 0:00:00.357898
elapsed time: 0:00:49.399741
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-10-01 14:35:34.970383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.66
 ---- batch: 020 ----
mean loss: 97.28
train mean loss: 96.89
epoch train time: 0:00:00.359922
elapsed time: 0:00:49.759863
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-10-01 14:35:35.330487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.70
 ---- batch: 020 ----
mean loss: 96.30
train mean loss: 95.15
epoch train time: 0:00:00.360662
elapsed time: 0:00:50.120719
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-10-01 14:35:35.691342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.35
 ---- batch: 020 ----
mean loss: 96.37
train mean loss: 93.60
epoch train time: 0:00:00.360889
elapsed time: 0:00:50.481786
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-10-01 14:35:36.052408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.33
 ---- batch: 020 ----
mean loss: 93.22
train mean loss: 92.74
epoch train time: 0:00:00.353192
elapsed time: 0:00:50.835161
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-10-01 14:35:36.405816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.42
 ---- batch: 020 ----
mean loss: 92.38
train mean loss: 91.86
epoch train time: 0:00:00.367911
elapsed time: 0:00:51.203328
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-10-01 14:35:36.773956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.25
 ---- batch: 020 ----
mean loss: 89.96
train mean loss: 91.53
epoch train time: 0:00:00.356618
elapsed time: 0:00:51.560166
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-10-01 14:35:37.130790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.97
 ---- batch: 020 ----
mean loss: 89.97
train mean loss: 88.89
epoch train time: 0:00:00.358678
elapsed time: 0:00:51.919026
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-10-01 14:35:37.489653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.57
 ---- batch: 020 ----
mean loss: 88.64
train mean loss: 87.64
epoch train time: 0:00:00.389997
elapsed time: 0:00:52.309233
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-10-01 14:35:37.879858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.11
 ---- batch: 020 ----
mean loss: 86.87
train mean loss: 88.23
epoch train time: 0:00:00.359251
elapsed time: 0:00:52.668695
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-10-01 14:35:38.239321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.63
 ---- batch: 020 ----
mean loss: 84.33
train mean loss: 86.13
epoch train time: 0:00:00.369977
elapsed time: 0:00:53.038846
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-10-01 14:35:38.609464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.25
 ---- batch: 020 ----
mean loss: 87.36
train mean loss: 87.38
epoch train time: 0:00:00.349494
elapsed time: 0:00:53.388511
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-10-01 14:35:38.959145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.75
 ---- batch: 020 ----
mean loss: 89.18
train mean loss: 85.47
epoch train time: 0:00:00.345756
elapsed time: 0:00:53.734465
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-10-01 14:35:39.305115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.85
 ---- batch: 020 ----
mean loss: 85.36
train mean loss: 84.17
epoch train time: 0:00:00.346861
elapsed time: 0:00:54.081588
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-10-01 14:35:39.652212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.19
 ---- batch: 020 ----
mean loss: 82.55
train mean loss: 83.88
epoch train time: 0:00:00.349041
elapsed time: 0:00:54.430808
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-10-01 14:35:40.001430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.28
 ---- batch: 020 ----
mean loss: 84.38
train mean loss: 82.25
epoch train time: 0:00:00.348703
elapsed time: 0:00:54.779687
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-10-01 14:35:40.350310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.45
 ---- batch: 020 ----
mean loss: 83.03
train mean loss: 82.32
epoch train time: 0:00:00.349427
elapsed time: 0:00:55.129295
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-10-01 14:35:40.699932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.15
 ---- batch: 020 ----
mean loss: 81.01
train mean loss: 81.22
epoch train time: 0:00:00.357379
elapsed time: 0:00:55.486979
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-10-01 14:35:41.057606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.88
 ---- batch: 020 ----
mean loss: 84.27
train mean loss: 80.94
epoch train time: 0:00:00.360263
elapsed time: 0:00:55.847428
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-10-01 14:35:41.418056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.55
 ---- batch: 020 ----
mean loss: 79.87
train mean loss: 79.97
epoch train time: 0:00:00.374513
elapsed time: 0:00:56.222122
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-10-01 14:35:41.792746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.80
 ---- batch: 020 ----
mean loss: 81.79
train mean loss: 80.63
epoch train time: 0:00:00.356766
elapsed time: 0:00:56.579063
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-10-01 14:35:42.149686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.78
 ---- batch: 020 ----
mean loss: 76.29
train mean loss: 77.97
epoch train time: 0:00:00.354463
elapsed time: 0:00:56.933709
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-10-01 14:35:42.504352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.34
 ---- batch: 020 ----
mean loss: 79.06
train mean loss: 78.02
epoch train time: 0:00:00.354952
elapsed time: 0:00:57.288862
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-10-01 14:35:42.859485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.93
 ---- batch: 020 ----
mean loss: 81.94
train mean loss: 78.96
epoch train time: 0:00:00.352178
elapsed time: 0:00:57.641215
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-10-01 14:35:43.211838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.41
 ---- batch: 020 ----
mean loss: 73.03
train mean loss: 77.04
epoch train time: 0:00:00.356614
elapsed time: 0:00:57.998004
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-10-01 14:35:43.568631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.07
 ---- batch: 020 ----
mean loss: 75.21
train mean loss: 76.96
epoch train time: 0:00:00.365785
elapsed time: 0:00:58.363977
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-10-01 14:35:43.934635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.33
 ---- batch: 020 ----
mean loss: 74.75
train mean loss: 75.88
epoch train time: 0:00:00.354544
elapsed time: 0:00:58.718750
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-10-01 14:35:44.289378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.31
 ---- batch: 020 ----
mean loss: 78.91
train mean loss: 77.01
epoch train time: 0:00:00.361817
elapsed time: 0:00:59.080760
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-10-01 14:35:44.651404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.44
 ---- batch: 020 ----
mean loss: 75.70
train mean loss: 75.05
epoch train time: 0:00:00.359353
elapsed time: 0:00:59.440333
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-10-01 14:35:45.010962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.32
 ---- batch: 020 ----
mean loss: 75.02
train mean loss: 74.66
epoch train time: 0:00:00.351671
elapsed time: 0:00:59.792186
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-10-01 14:35:45.362831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.22
 ---- batch: 020 ----
mean loss: 71.23
train mean loss: 73.88
epoch train time: 0:00:00.349040
elapsed time: 0:01:00.141432
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-10-01 14:35:45.712047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.93
 ---- batch: 020 ----
mean loss: 72.99
train mean loss: 74.44
epoch train time: 0:00:00.355855
elapsed time: 0:01:00.497484
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-10-01 14:35:46.068088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.16
 ---- batch: 020 ----
mean loss: 71.67
train mean loss: 72.54
epoch train time: 0:00:00.353392
elapsed time: 0:01:00.851032
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-10-01 14:35:46.421657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.55
 ---- batch: 020 ----
mean loss: 72.55
train mean loss: 72.44
epoch train time: 0:00:00.353961
elapsed time: 0:01:01.205181
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-10-01 14:35:46.775808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.24
 ---- batch: 020 ----
mean loss: 71.72
train mean loss: 71.59
epoch train time: 0:00:00.349626
elapsed time: 0:01:01.554987
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-10-01 14:35:47.125611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.31
 ---- batch: 020 ----
mean loss: 73.25
train mean loss: 71.98
epoch train time: 0:00:00.343337
elapsed time: 0:01:01.898496
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-10-01 14:35:47.469121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.20
 ---- batch: 020 ----
mean loss: 70.59
train mean loss: 71.05
epoch train time: 0:00:00.346941
elapsed time: 0:01:02.245639
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-10-01 14:35:47.816282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.80
 ---- batch: 020 ----
mean loss: 69.70
train mean loss: 70.87
epoch train time: 0:00:00.360047
elapsed time: 0:01:02.605890
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-10-01 14:35:48.176496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.69
 ---- batch: 020 ----
mean loss: 72.17
train mean loss: 71.49
epoch train time: 0:00:00.356786
elapsed time: 0:01:02.962889
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-10-01 14:35:48.533511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.71
 ---- batch: 020 ----
mean loss: 68.08
train mean loss: 68.48
epoch train time: 0:00:00.357594
elapsed time: 0:01:03.320661
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-10-01 14:35:48.891283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.61
 ---- batch: 020 ----
mean loss: 68.62
train mean loss: 69.64
epoch train time: 0:00:00.348615
elapsed time: 0:01:03.669520
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-10-01 14:35:49.240212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.01
 ---- batch: 020 ----
mean loss: 69.09
train mean loss: 69.89
epoch train time: 0:00:00.349522
elapsed time: 0:01:04.019323
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-10-01 14:35:49.589957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.17
 ---- batch: 020 ----
mean loss: 69.35
train mean loss: 69.64
epoch train time: 0:00:00.372933
elapsed time: 0:01:04.392441
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-10-01 14:35:49.963063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.18
 ---- batch: 020 ----
mean loss: 70.17
train mean loss: 69.50
epoch train time: 0:00:00.353326
elapsed time: 0:01:04.745939
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-10-01 14:35:50.316558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.15
 ---- batch: 020 ----
mean loss: 69.05
train mean loss: 67.98
epoch train time: 0:00:00.356981
elapsed time: 0:01:05.103095
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-10-01 14:35:50.673725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.90
 ---- batch: 020 ----
mean loss: 71.75
train mean loss: 69.34
epoch train time: 0:00:00.354412
elapsed time: 0:01:05.457712
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-10-01 14:35:51.028341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.61
 ---- batch: 020 ----
mean loss: 67.13
train mean loss: 68.43
epoch train time: 0:00:00.355844
elapsed time: 0:01:05.813761
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-10-01 14:35:51.384402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.43
 ---- batch: 020 ----
mean loss: 66.57
train mean loss: 67.61
epoch train time: 0:00:00.366211
elapsed time: 0:01:06.180202
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-10-01 14:35:51.750854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.07
 ---- batch: 020 ----
mean loss: 67.17
train mean loss: 66.85
epoch train time: 0:00:00.364044
elapsed time: 0:01:06.544449
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-10-01 14:35:52.115072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.52
 ---- batch: 020 ----
mean loss: 63.49
train mean loss: 66.90
epoch train time: 0:00:00.352354
elapsed time: 0:01:06.896974
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-10-01 14:35:52.467628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.69
 ---- batch: 020 ----
mean loss: 66.52
train mean loss: 67.47
epoch train time: 0:00:00.354539
elapsed time: 0:01:07.251727
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-10-01 14:35:52.822354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.56
 ---- batch: 020 ----
mean loss: 66.28
train mean loss: 65.97
epoch train time: 0:00:00.352099
elapsed time: 0:01:07.604012
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-10-01 14:35:53.174633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.72
 ---- batch: 020 ----
mean loss: 65.83
train mean loss: 66.70
epoch train time: 0:00:00.353483
elapsed time: 0:01:07.957687
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-10-01 14:35:53.528308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.12
 ---- batch: 020 ----
mean loss: 67.31
train mean loss: 65.92
epoch train time: 0:00:00.376515
elapsed time: 0:01:08.334379
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-10-01 14:35:53.905007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.90
 ---- batch: 020 ----
mean loss: 64.30
train mean loss: 64.53
epoch train time: 0:00:00.363125
elapsed time: 0:01:08.697686
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-10-01 14:35:54.268308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.02
 ---- batch: 020 ----
mean loss: 65.50
train mean loss: 65.14
epoch train time: 0:00:00.357974
elapsed time: 0:01:09.055859
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-10-01 14:35:54.626500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.39
 ---- batch: 020 ----
mean loss: 65.27
train mean loss: 64.94
epoch train time: 0:00:00.353812
elapsed time: 0:01:09.409903
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-10-01 14:35:54.980523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.12
 ---- batch: 020 ----
mean loss: 64.87
train mean loss: 64.34
epoch train time: 0:00:00.347993
elapsed time: 0:01:09.758068
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-10-01 14:35:55.328752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.45
 ---- batch: 020 ----
mean loss: 65.85
train mean loss: 64.70
epoch train time: 0:00:00.346797
elapsed time: 0:01:10.105099
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-10-01 14:35:55.675721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.00
 ---- batch: 020 ----
mean loss: 62.24
train mean loss: 63.64
epoch train time: 0:00:00.347959
elapsed time: 0:01:10.453247
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-10-01 14:35:56.023871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.62
 ---- batch: 020 ----
mean loss: 63.68
train mean loss: 62.43
epoch train time: 0:00:00.346037
elapsed time: 0:01:10.799461
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-10-01 14:35:56.370080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.92
 ---- batch: 020 ----
mean loss: 66.77
train mean loss: 64.67
epoch train time: 0:00:00.352595
elapsed time: 0:01:11.152264
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-10-01 14:35:56.722920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.58
 ---- batch: 020 ----
mean loss: 64.41
train mean loss: 63.77
epoch train time: 0:00:00.362873
elapsed time: 0:01:11.515359
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-10-01 14:35:57.085988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.70
 ---- batch: 020 ----
mean loss: 63.71
train mean loss: 62.66
epoch train time: 0:00:00.361388
elapsed time: 0:01:11.876944
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-10-01 14:35:57.447567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.98
 ---- batch: 020 ----
mean loss: 63.28
train mean loss: 62.37
epoch train time: 0:00:00.359174
elapsed time: 0:01:12.236309
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-10-01 14:35:57.806931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.73
 ---- batch: 020 ----
mean loss: 64.30
train mean loss: 64.74
epoch train time: 0:00:00.365023
elapsed time: 0:01:12.601508
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-10-01 14:35:58.172162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.80
 ---- batch: 020 ----
mean loss: 61.08
train mean loss: 62.34
epoch train time: 0:00:00.356257
elapsed time: 0:01:12.958037
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-10-01 14:35:58.528661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.98
 ---- batch: 020 ----
mean loss: 59.69
train mean loss: 61.21
epoch train time: 0:00:00.367961
elapsed time: 0:01:13.326175
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-10-01 14:35:58.896796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.17
 ---- batch: 020 ----
mean loss: 61.24
train mean loss: 61.61
epoch train time: 0:00:00.360670
elapsed time: 0:01:13.687025
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-10-01 14:35:59.257679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.60
 ---- batch: 020 ----
mean loss: 61.89
train mean loss: 60.64
epoch train time: 0:00:00.358809
elapsed time: 0:01:14.046060
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-10-01 14:35:59.616700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.78
 ---- batch: 020 ----
mean loss: 62.28
train mean loss: 61.39
epoch train time: 0:00:00.371058
elapsed time: 0:01:14.417320
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-10-01 14:35:59.987947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.02
 ---- batch: 020 ----
mean loss: 62.34
train mean loss: 61.21
epoch train time: 0:00:00.352294
elapsed time: 0:01:14.769858
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-10-01 14:36:00.340484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.74
 ---- batch: 020 ----
mean loss: 59.91
train mean loss: 60.23
epoch train time: 0:00:00.354349
elapsed time: 0:01:15.124388
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-10-01 14:36:00.695028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.51
 ---- batch: 020 ----
mean loss: 61.16
train mean loss: 60.63
epoch train time: 0:00:00.358061
elapsed time: 0:01:15.482672
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-10-01 14:36:01.053315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.67
 ---- batch: 020 ----
mean loss: 58.78
train mean loss: 60.43
epoch train time: 0:00:00.358417
elapsed time: 0:01:15.841286
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-10-01 14:36:01.411910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.31
 ---- batch: 020 ----
mean loss: 57.42
train mean loss: 60.32
epoch train time: 0:00:00.381495
elapsed time: 0:01:16.222969
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-10-01 14:36:01.793594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.28
 ---- batch: 020 ----
mean loss: 61.02
train mean loss: 59.67
epoch train time: 0:00:00.366109
elapsed time: 0:01:16.589260
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-10-01 14:36:02.159887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.19
 ---- batch: 020 ----
mean loss: 60.71
train mean loss: 58.83
epoch train time: 0:00:00.352813
elapsed time: 0:01:16.942269
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-10-01 14:36:02.512896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.39
 ---- batch: 020 ----
mean loss: 58.67
train mean loss: 59.10
epoch train time: 0:00:00.358234
elapsed time: 0:01:17.300684
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-10-01 14:36:02.871312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.88
 ---- batch: 020 ----
mean loss: 59.89
train mean loss: 58.72
epoch train time: 0:00:00.355049
elapsed time: 0:01:17.655915
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-10-01 14:36:03.226573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.80
 ---- batch: 020 ----
mean loss: 59.81
train mean loss: 58.66
epoch train time: 0:00:00.354697
elapsed time: 0:01:18.010879
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-10-01 14:36:03.581501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.26
 ---- batch: 020 ----
mean loss: 59.61
train mean loss: 59.08
epoch train time: 0:00:00.384323
elapsed time: 0:01:18.395381
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-10-01 14:36:03.966009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.61
 ---- batch: 020 ----
mean loss: 59.30
train mean loss: 58.63
epoch train time: 0:00:00.358688
elapsed time: 0:01:18.754295
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-10-01 14:36:04.324922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.72
 ---- batch: 020 ----
mean loss: 57.84
train mean loss: 57.25
epoch train time: 0:00:00.357301
elapsed time: 0:01:19.111781
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-10-01 14:36:04.682414
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.25
 ---- batch: 020 ----
mean loss: 56.48
train mean loss: 57.19
epoch train time: 0:00:00.356519
elapsed time: 0:01:19.468516
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-10-01 14:36:05.039126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.86
 ---- batch: 020 ----
mean loss: 61.00
train mean loss: 57.76
epoch train time: 0:00:00.360107
elapsed time: 0:01:19.828821
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-10-01 14:36:05.399448
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.19
 ---- batch: 020 ----
mean loss: 57.88
train mean loss: 57.57
epoch train time: 0:00:00.369535
elapsed time: 0:01:20.198577
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-10-01 14:36:05.769248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.34
 ---- batch: 020 ----
mean loss: 59.37
train mean loss: 57.58
epoch train time: 0:00:00.361258
elapsed time: 0:01:20.560094
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-10-01 14:36:06.130722
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.97
 ---- batch: 020 ----
mean loss: 57.28
train mean loss: 56.84
epoch train time: 0:00:00.354852
elapsed time: 0:01:20.915122
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-10-01 14:36:06.485807
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.17
 ---- batch: 020 ----
mean loss: 58.17
train mean loss: 56.93
epoch train time: 0:00:00.350177
elapsed time: 0:01:21.265554
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-10-01 14:36:06.836180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.99
 ---- batch: 020 ----
mean loss: 57.48
train mean loss: 56.64
epoch train time: 0:00:00.352526
elapsed time: 0:01:21.618284
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-10-01 14:36:07.188906
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.54
 ---- batch: 020 ----
mean loss: 55.12
train mean loss: 56.53
epoch train time: 0:00:00.354990
elapsed time: 0:01:21.973461
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-10-01 14:36:07.544082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.97
 ---- batch: 020 ----
mean loss: 56.24
train mean loss: 57.31
epoch train time: 0:00:00.377205
elapsed time: 0:01:22.350849
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-10-01 14:36:07.921477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.31
 ---- batch: 020 ----
mean loss: 57.52
train mean loss: 56.39
epoch train time: 0:00:00.363374
elapsed time: 0:01:22.714409
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-10-01 14:36:08.285033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.14
 ---- batch: 020 ----
mean loss: 56.64
train mean loss: 56.63
epoch train time: 0:00:00.360884
elapsed time: 0:01:23.075536
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-10-01 14:36:08.646158
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.05
 ---- batch: 020 ----
mean loss: 54.58
train mean loss: 56.77
epoch train time: 0:00:00.359663
elapsed time: 0:01:23.435378
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-10-01 14:36:09.006001
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.51
 ---- batch: 020 ----
mean loss: 58.43
train mean loss: 56.28
epoch train time: 0:00:00.354181
elapsed time: 0:01:23.789734
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-10-01 14:36:09.360371
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.31
 ---- batch: 020 ----
mean loss: 56.39
train mean loss: 56.10
epoch train time: 0:00:00.356265
elapsed time: 0:01:24.146217
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-10-01 14:36:09.716848
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.58
 ---- batch: 020 ----
mean loss: 55.68
train mean loss: 56.71
epoch train time: 0:00:00.375648
elapsed time: 0:01:24.522053
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-10-01 14:36:10.092679
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.90
 ---- batch: 020 ----
mean loss: 58.10
train mean loss: 56.33
epoch train time: 0:00:00.356463
elapsed time: 0:01:24.878689
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-10-01 14:36:10.449322
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.92
 ---- batch: 020 ----
mean loss: 56.92
train mean loss: 56.94
epoch train time: 0:00:00.355426
elapsed time: 0:01:25.234317
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-10-01 14:36:10.804940
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.05
 ---- batch: 020 ----
mean loss: 54.96
train mean loss: 55.87
epoch train time: 0:00:00.367604
elapsed time: 0:01:25.602100
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-10-01 14:36:11.172727
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.78
 ---- batch: 020 ----
mean loss: 58.41
train mean loss: 56.08
epoch train time: 0:00:00.377637
elapsed time: 0:01:25.979924
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-10-01 14:36:11.550546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.20
 ---- batch: 020 ----
mean loss: 54.27
train mean loss: 55.67
epoch train time: 0:00:00.374391
elapsed time: 0:01:26.354510
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-10-01 14:36:11.925138
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.85
 ---- batch: 020 ----
mean loss: 58.45
train mean loss: 58.17
epoch train time: 0:00:00.366084
elapsed time: 0:01:26.720789
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-10-01 14:36:12.291427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.64
 ---- batch: 020 ----
mean loss: 57.61
train mean loss: 57.25
epoch train time: 0:00:00.367433
elapsed time: 0:01:27.088421
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-10-01 14:36:12.659049
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.38
 ---- batch: 020 ----
mean loss: 55.79
train mean loss: 56.58
epoch train time: 0:00:00.357499
elapsed time: 0:01:27.446120
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-10-01 14:36:13.016772
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.87
 ---- batch: 020 ----
mean loss: 56.88
train mean loss: 57.02
epoch train time: 0:00:00.353061
elapsed time: 0:01:27.799387
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-10-01 14:36:13.370008
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.15
 ---- batch: 020 ----
mean loss: 58.56
train mean loss: 56.41
epoch train time: 0:00:00.362128
elapsed time: 0:01:28.161702
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-10-01 14:36:13.732326
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.43
 ---- batch: 020 ----
mean loss: 55.31
train mean loss: 56.31
epoch train time: 0:00:00.348487
elapsed time: 0:01:28.510367
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-10-01 14:36:14.081020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.54
 ---- batch: 020 ----
mean loss: 56.87
train mean loss: 56.77
epoch train time: 0:00:00.344840
elapsed time: 0:01:28.855413
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-10-01 14:36:14.426036
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.56
 ---- batch: 020 ----
mean loss: 59.00
train mean loss: 56.69
epoch train time: 0:00:00.349195
elapsed time: 0:01:29.204813
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-10-01 14:36:14.775450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.38
 ---- batch: 020 ----
mean loss: 57.93
train mean loss: 56.29
epoch train time: 0:00:00.365294
elapsed time: 0:01:29.570300
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-10-01 14:36:15.140921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.87
 ---- batch: 020 ----
mean loss: 57.64
train mean loss: 56.66
epoch train time: 0:00:00.359691
elapsed time: 0:01:29.930213
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-10-01 14:36:15.500851
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.87
 ---- batch: 020 ----
mean loss: 58.71
train mean loss: 56.22
epoch train time: 0:00:00.380044
elapsed time: 0:01:30.310480
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-10-01 14:36:15.881109
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.35
 ---- batch: 020 ----
mean loss: 55.49
train mean loss: 56.38
epoch train time: 0:00:00.357820
elapsed time: 0:01:30.668501
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-10-01 14:36:16.239130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.13
 ---- batch: 020 ----
mean loss: 53.63
train mean loss: 56.16
epoch train time: 0:00:00.359419
elapsed time: 0:01:31.028130
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-10-01 14:36:16.598742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.51
 ---- batch: 020 ----
mean loss: 55.98
train mean loss: 55.11
epoch train time: 0:00:00.357808
elapsed time: 0:01:31.386120
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-10-01 14:36:16.956791
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.24
 ---- batch: 020 ----
mean loss: 56.87
train mean loss: 56.21
epoch train time: 0:00:00.358617
elapsed time: 0:01:31.745064
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-10-01 14:36:17.315690
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.39
 ---- batch: 020 ----
mean loss: 55.29
train mean loss: 56.06
epoch train time: 0:00:00.365465
elapsed time: 0:01:32.110726
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-10-01 14:36:17.681368
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.16
 ---- batch: 020 ----
mean loss: 57.83
train mean loss: 56.29
epoch train time: 0:00:00.366464
elapsed time: 0:01:32.477396
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-10-01 14:36:18.048024
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.48
 ---- batch: 020 ----
mean loss: 53.64
train mean loss: 56.71
epoch train time: 0:00:00.356667
elapsed time: 0:01:32.834250
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-10-01 14:36:18.404879
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.35
 ---- batch: 020 ----
mean loss: 56.80
train mean loss: 56.03
epoch train time: 0:00:00.356108
elapsed time: 0:01:33.190589
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-10-01 14:36:18.761228
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.73
 ---- batch: 020 ----
mean loss: 55.91
train mean loss: 55.85
epoch train time: 0:00:00.356249
elapsed time: 0:01:33.547047
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-10-01 14:36:19.117671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.27
 ---- batch: 020 ----
mean loss: 55.46
train mean loss: 54.75
epoch train time: 0:00:00.355628
elapsed time: 0:01:33.902857
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-10-01 14:36:19.473478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.35
 ---- batch: 020 ----
mean loss: 57.39
train mean loss: 56.57
epoch train time: 0:00:00.377432
elapsed time: 0:01:34.280485
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-10-01 14:36:19.851126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.42
 ---- batch: 020 ----
mean loss: 57.68
train mean loss: 57.18
epoch train time: 0:00:00.349920
elapsed time: 0:01:34.630647
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-10-01 14:36:20.201328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.23
 ---- batch: 020 ----
mean loss: 57.20
train mean loss: 55.18
epoch train time: 0:00:00.351535
elapsed time: 0:01:34.982416
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-10-01 14:36:20.553037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.38
 ---- batch: 020 ----
mean loss: 57.50
train mean loss: 56.47
epoch train time: 0:00:00.360219
elapsed time: 0:01:35.342826
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-10-01 14:36:20.913463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.84
 ---- batch: 020 ----
mean loss: 54.11
train mean loss: 55.81
epoch train time: 0:00:00.357350
elapsed time: 0:01:35.700366
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-10-01 14:36:21.270990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.49
 ---- batch: 020 ----
mean loss: 55.44
train mean loss: 56.75
epoch train time: 0:00:00.357393
elapsed time: 0:01:36.057939
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-10-01 14:36:21.628567
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.67
 ---- batch: 020 ----
mean loss: 56.43
train mean loss: 56.00
epoch train time: 0:00:00.359347
elapsed time: 0:01:36.417480
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-10-01 14:36:21.988107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.17
 ---- batch: 020 ----
mean loss: 56.78
train mean loss: 55.81
epoch train time: 0:00:00.354179
elapsed time: 0:01:36.779801
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_7/checkpoint.pth.tar
**** end time: 2019-10-01 14:36:22.350380 ****
