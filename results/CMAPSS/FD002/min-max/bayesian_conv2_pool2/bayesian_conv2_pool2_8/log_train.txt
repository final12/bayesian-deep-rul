Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 19604
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-25 18:58:34.465582 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 16, 11]           1,120
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
    BayesianConv2d-4            [-1, 14, 7, 11]             448
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
    BayesianLinear-8                    [-1, 1]             924
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,492
Trainable params: 2,492
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 18:58:34.477771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4242.56
 ---- batch: 020 ----
mean loss: 4044.03
 ---- batch: 030 ----
mean loss: 3929.23
 ---- batch: 040 ----
mean loss: 3657.55
 ---- batch: 050 ----
mean loss: 3343.58
 ---- batch: 060 ----
mean loss: 3185.38
 ---- batch: 070 ----
mean loss: 2892.17
 ---- batch: 080 ----
mean loss: 2685.30
 ---- batch: 090 ----
mean loss: 2467.00
train mean loss: 3314.63
epoch train time: 0:00:36.918323
elapsed time: 0:00:36.934030
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 18:59:11.399662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2080.44
 ---- batch: 020 ----
mean loss: 1938.39
 ---- batch: 030 ----
mean loss: 1748.53
 ---- batch: 040 ----
mean loss: 1594.73
 ---- batch: 050 ----
mean loss: 1424.26
 ---- batch: 060 ----
mean loss: 1335.97
 ---- batch: 070 ----
mean loss: 1229.94
 ---- batch: 080 ----
mean loss: 1142.54
 ---- batch: 090 ----
mean loss: 1111.97
train mean loss: 1482.37
epoch train time: 0:00:02.673380
elapsed time: 0:00:39.607875
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 18:59:14.073682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1016.16
 ---- batch: 020 ----
mean loss: 976.61
 ---- batch: 030 ----
mean loss: 958.44
 ---- batch: 040 ----
mean loss: 957.95
 ---- batch: 050 ----
mean loss: 934.62
 ---- batch: 060 ----
mean loss: 918.08
 ---- batch: 070 ----
mean loss: 917.63
 ---- batch: 080 ----
mean loss: 889.83
 ---- batch: 090 ----
mean loss: 908.70
train mean loss: 939.17
epoch train time: 0:00:02.694520
elapsed time: 0:00:42.302948
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 18:59:16.768745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.81
 ---- batch: 020 ----
mean loss: 890.82
 ---- batch: 030 ----
mean loss: 899.95
 ---- batch: 040 ----
mean loss: 899.41
 ---- batch: 050 ----
mean loss: 884.81
 ---- batch: 060 ----
mean loss: 889.57
 ---- batch: 070 ----
mean loss: 901.54
 ---- batch: 080 ----
mean loss: 901.52
 ---- batch: 090 ----
mean loss: 893.42
train mean loss: 897.20
epoch train time: 0:00:02.686592
elapsed time: 0:00:44.990058
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 18:59:19.455814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.45
 ---- batch: 020 ----
mean loss: 882.76
 ---- batch: 030 ----
mean loss: 894.21
 ---- batch: 040 ----
mean loss: 898.00
 ---- batch: 050 ----
mean loss: 874.71
 ---- batch: 060 ----
mean loss: 892.67
 ---- batch: 070 ----
mean loss: 903.26
 ---- batch: 080 ----
mean loss: 898.86
 ---- batch: 090 ----
mean loss: 877.97
train mean loss: 888.99
epoch train time: 0:00:02.705437
elapsed time: 0:00:47.695964
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 18:59:22.161752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.56
 ---- batch: 020 ----
mean loss: 887.90
 ---- batch: 030 ----
mean loss: 897.17
 ---- batch: 040 ----
mean loss: 892.75
 ---- batch: 050 ----
mean loss: 892.44
 ---- batch: 060 ----
mean loss: 861.72
 ---- batch: 070 ----
mean loss: 906.33
 ---- batch: 080 ----
mean loss: 898.14
 ---- batch: 090 ----
mean loss: 884.40
train mean loss: 890.79
epoch train time: 0:00:02.645746
elapsed time: 0:00:50.342209
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 18:59:24.807972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.53
 ---- batch: 020 ----
mean loss: 884.08
 ---- batch: 030 ----
mean loss: 893.04
 ---- batch: 040 ----
mean loss: 892.76
 ---- batch: 050 ----
mean loss: 886.75
 ---- batch: 060 ----
mean loss: 881.78
 ---- batch: 070 ----
mean loss: 880.85
 ---- batch: 080 ----
mean loss: 877.78
 ---- batch: 090 ----
mean loss: 873.59
train mean loss: 881.41
epoch train time: 0:00:02.671034
elapsed time: 0:00:53.013810
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 18:59:27.479622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.63
 ---- batch: 020 ----
mean loss: 878.43
 ---- batch: 030 ----
mean loss: 918.62
 ---- batch: 040 ----
mean loss: 878.28
 ---- batch: 050 ----
mean loss: 872.70
 ---- batch: 060 ----
mean loss: 868.21
 ---- batch: 070 ----
mean loss: 890.68
 ---- batch: 080 ----
mean loss: 873.34
 ---- batch: 090 ----
mean loss: 856.19
train mean loss: 876.68
epoch train time: 0:00:02.712833
elapsed time: 0:00:55.727212
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 18:59:30.192974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.35
 ---- batch: 020 ----
mean loss: 868.48
 ---- batch: 030 ----
mean loss: 875.66
 ---- batch: 040 ----
mean loss: 897.23
 ---- batch: 050 ----
mean loss: 860.25
 ---- batch: 060 ----
mean loss: 868.61
 ---- batch: 070 ----
mean loss: 870.97
 ---- batch: 080 ----
mean loss: 855.48
 ---- batch: 090 ----
mean loss: 884.47
train mean loss: 873.01
epoch train time: 0:00:02.675257
elapsed time: 0:00:58.402931
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 18:59:32.868706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.05
 ---- batch: 020 ----
mean loss: 877.07
 ---- batch: 030 ----
mean loss: 846.87
 ---- batch: 040 ----
mean loss: 870.62
 ---- batch: 050 ----
mean loss: 884.74
 ---- batch: 060 ----
mean loss: 879.42
 ---- batch: 070 ----
mean loss: 871.58
 ---- batch: 080 ----
mean loss: 860.27
 ---- batch: 090 ----
mean loss: 865.12
train mean loss: 870.57
epoch train time: 0:00:02.665279
elapsed time: 0:01:01.068731
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 18:59:35.534487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.96
 ---- batch: 020 ----
mean loss: 876.70
 ---- batch: 030 ----
mean loss: 869.01
 ---- batch: 040 ----
mean loss: 871.57
 ---- batch: 050 ----
mean loss: 855.68
 ---- batch: 060 ----
mean loss: 870.29
 ---- batch: 070 ----
mean loss: 870.48
 ---- batch: 080 ----
mean loss: 852.81
 ---- batch: 090 ----
mean loss: 868.88
train mean loss: 865.14
epoch train time: 0:00:02.654070
elapsed time: 0:01:03.723272
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 18:59:38.189029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 823.92
 ---- batch: 020 ----
mean loss: 854.49
 ---- batch: 030 ----
mean loss: 861.61
 ---- batch: 040 ----
mean loss: 881.20
 ---- batch: 050 ----
mean loss: 877.52
 ---- batch: 060 ----
mean loss: 861.17
 ---- batch: 070 ----
mean loss: 880.85
 ---- batch: 080 ----
mean loss: 854.37
 ---- batch: 090 ----
mean loss: 851.07
train mean loss: 859.29
epoch train time: 0:00:02.673086
elapsed time: 0:01:06.396854
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 18:59:40.862607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.90
 ---- batch: 020 ----
mean loss: 858.17
 ---- batch: 030 ----
mean loss: 860.81
 ---- batch: 040 ----
mean loss: 848.75
 ---- batch: 050 ----
mean loss: 862.20
 ---- batch: 060 ----
mean loss: 861.13
 ---- batch: 070 ----
mean loss: 833.15
 ---- batch: 080 ----
mean loss: 847.10
 ---- batch: 090 ----
mean loss: 867.45
train mean loss: 854.73
epoch train time: 0:00:02.631307
elapsed time: 0:01:09.028625
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 18:59:43.494364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.03
 ---- batch: 020 ----
mean loss: 854.03
 ---- batch: 030 ----
mean loss: 848.98
 ---- batch: 040 ----
mean loss: 826.96
 ---- batch: 050 ----
mean loss: 855.85
 ---- batch: 060 ----
mean loss: 844.79
 ---- batch: 070 ----
mean loss: 870.53
 ---- batch: 080 ----
mean loss: 850.40
 ---- batch: 090 ----
mean loss: 846.39
train mean loss: 850.49
epoch train time: 0:00:02.619143
elapsed time: 0:01:11.648478
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 18:59:46.114309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.90
 ---- batch: 020 ----
mean loss: 843.56
 ---- batch: 030 ----
mean loss: 850.24
 ---- batch: 040 ----
mean loss: 848.25
 ---- batch: 050 ----
mean loss: 837.71
 ---- batch: 060 ----
mean loss: 831.78
 ---- batch: 070 ----
mean loss: 833.95
 ---- batch: 080 ----
mean loss: 853.84
 ---- batch: 090 ----
mean loss: 846.14
train mean loss: 844.97
epoch train time: 0:00:02.661467
elapsed time: 0:01:14.310487
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 18:59:48.776261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.65
 ---- batch: 020 ----
mean loss: 843.25
 ---- batch: 030 ----
mean loss: 839.00
 ---- batch: 040 ----
mean loss: 851.71
 ---- batch: 050 ----
mean loss: 845.95
 ---- batch: 060 ----
mean loss: 830.55
 ---- batch: 070 ----
mean loss: 819.25
 ---- batch: 080 ----
mean loss: 839.76
 ---- batch: 090 ----
mean loss: 843.13
train mean loss: 839.13
epoch train time: 0:00:02.676315
elapsed time: 0:01:16.987285
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 18:59:51.453061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.19
 ---- batch: 020 ----
mean loss: 818.40
 ---- batch: 030 ----
mean loss: 836.21
 ---- batch: 040 ----
mean loss: 847.22
 ---- batch: 050 ----
mean loss: 823.47
 ---- batch: 060 ----
mean loss: 842.75
 ---- batch: 070 ----
mean loss: 851.55
 ---- batch: 080 ----
mean loss: 853.50
 ---- batch: 090 ----
mean loss: 819.49
train mean loss: 838.11
epoch train time: 0:00:02.686396
elapsed time: 0:01:19.674146
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 18:59:54.139897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.73
 ---- batch: 020 ----
mean loss: 831.39
 ---- batch: 030 ----
mean loss: 843.57
 ---- batch: 040 ----
mean loss: 843.23
 ---- batch: 050 ----
mean loss: 820.00
 ---- batch: 060 ----
mean loss: 816.23
 ---- batch: 070 ----
mean loss: 833.29
 ---- batch: 080 ----
mean loss: 829.29
 ---- batch: 090 ----
mean loss: 826.16
train mean loss: 832.65
epoch train time: 0:00:02.649818
elapsed time: 0:01:22.324456
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 18:59:56.790219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.66
 ---- batch: 020 ----
mean loss: 836.92
 ---- batch: 030 ----
mean loss: 811.08
 ---- batch: 040 ----
mean loss: 831.58
 ---- batch: 050 ----
mean loss: 829.25
 ---- batch: 060 ----
mean loss: 828.46
 ---- batch: 070 ----
mean loss: 806.50
 ---- batch: 080 ----
mean loss: 834.15
 ---- batch: 090 ----
mean loss: 834.06
train mean loss: 826.53
epoch train time: 0:00:02.629792
elapsed time: 0:01:24.954795
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 18:59:59.420577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 825.35
 ---- batch: 020 ----
mean loss: 829.86
 ---- batch: 030 ----
mean loss: 828.08
 ---- batch: 040 ----
mean loss: 824.32
 ---- batch: 050 ----
mean loss: 812.72
 ---- batch: 060 ----
mean loss: 828.08
 ---- batch: 070 ----
mean loss: 827.31
 ---- batch: 080 ----
mean loss: 808.51
 ---- batch: 090 ----
mean loss: 814.99
train mean loss: 822.16
epoch train time: 0:00:02.673151
elapsed time: 0:01:27.628444
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 19:00:02.094267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 818.12
 ---- batch: 020 ----
mean loss: 815.15
 ---- batch: 030 ----
mean loss: 808.73
 ---- batch: 040 ----
mean loss: 817.29
 ---- batch: 050 ----
mean loss: 827.91
 ---- batch: 060 ----
mean loss: 812.74
 ---- batch: 070 ----
mean loss: 800.90
 ---- batch: 080 ----
mean loss: 827.06
 ---- batch: 090 ----
mean loss: 813.81
train mean loss: 814.87
epoch train time: 0:00:02.669684
elapsed time: 0:01:30.298678
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 19:00:04.764378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 809.65
 ---- batch: 020 ----
mean loss: 817.42
 ---- batch: 030 ----
mean loss: 802.09
 ---- batch: 040 ----
mean loss: 797.66
 ---- batch: 050 ----
mean loss: 807.27
 ---- batch: 060 ----
mean loss: 823.46
 ---- batch: 070 ----
mean loss: 812.40
 ---- batch: 080 ----
mean loss: 806.08
 ---- batch: 090 ----
mean loss: 809.30
train mean loss: 810.12
epoch train time: 0:00:02.660478
elapsed time: 0:01:32.959603
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 19:00:07.425366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 811.27
 ---- batch: 020 ----
mean loss: 800.05
 ---- batch: 030 ----
mean loss: 789.76
 ---- batch: 040 ----
mean loss: 797.09
 ---- batch: 050 ----
mean loss: 801.63
 ---- batch: 060 ----
mean loss: 805.09
 ---- batch: 070 ----
mean loss: 804.55
 ---- batch: 080 ----
mean loss: 801.68
 ---- batch: 090 ----
mean loss: 811.37
train mean loss: 802.33
epoch train time: 0:00:02.686954
elapsed time: 0:01:35.647024
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 19:00:10.112774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 814.01
 ---- batch: 020 ----
mean loss: 791.87
 ---- batch: 030 ----
mean loss: 797.25
 ---- batch: 040 ----
mean loss: 787.23
 ---- batch: 050 ----
mean loss: 790.84
 ---- batch: 060 ----
mean loss: 783.29
 ---- batch: 070 ----
mean loss: 802.64
 ---- batch: 080 ----
mean loss: 802.07
 ---- batch: 090 ----
mean loss: 791.97
train mean loss: 796.90
epoch train time: 0:00:02.663996
elapsed time: 0:01:38.311501
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 19:00:12.777284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 787.27
 ---- batch: 020 ----
mean loss: 805.35
 ---- batch: 030 ----
mean loss: 792.94
 ---- batch: 040 ----
mean loss: 795.77
 ---- batch: 050 ----
mean loss: 798.08
 ---- batch: 060 ----
mean loss: 789.96
 ---- batch: 070 ----
mean loss: 783.95
 ---- batch: 080 ----
mean loss: 780.16
 ---- batch: 090 ----
mean loss: 791.36
train mean loss: 789.73
epoch train time: 0:00:02.673001
elapsed time: 0:01:40.985010
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 19:00:15.450759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 781.38
 ---- batch: 020 ----
mean loss: 782.09
 ---- batch: 030 ----
mean loss: 776.50
 ---- batch: 040 ----
mean loss: 771.01
 ---- batch: 050 ----
mean loss: 774.59
 ---- batch: 060 ----
mean loss: 799.94
 ---- batch: 070 ----
mean loss: 784.53
 ---- batch: 080 ----
mean loss: 778.68
 ---- batch: 090 ----
mean loss: 775.38
train mean loss: 781.32
epoch train time: 0:00:02.687580
elapsed time: 0:01:43.673032
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 19:00:18.138847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 780.07
 ---- batch: 020 ----
mean loss: 783.49
 ---- batch: 030 ----
mean loss: 771.50
 ---- batch: 040 ----
mean loss: 774.44
 ---- batch: 050 ----
mean loss: 764.66
 ---- batch: 060 ----
mean loss: 775.04
 ---- batch: 070 ----
mean loss: 780.45
 ---- batch: 080 ----
mean loss: 781.18
 ---- batch: 090 ----
mean loss: 782.13
train mean loss: 775.50
epoch train time: 0:00:02.643582
elapsed time: 0:01:46.317114
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 19:00:20.782873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 782.53
 ---- batch: 020 ----
mean loss: 760.34
 ---- batch: 030 ----
mean loss: 766.09
 ---- batch: 040 ----
mean loss: 776.05
 ---- batch: 050 ----
mean loss: 770.33
 ---- batch: 060 ----
mean loss: 755.81
 ---- batch: 070 ----
mean loss: 747.90
 ---- batch: 080 ----
mean loss: 781.75
 ---- batch: 090 ----
mean loss: 750.80
train mean loss: 764.84
epoch train time: 0:00:02.687823
elapsed time: 0:01:49.005372
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 19:00:23.471126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 761.12
 ---- batch: 020 ----
mean loss: 756.61
 ---- batch: 030 ----
mean loss: 746.68
 ---- batch: 040 ----
mean loss: 754.55
 ---- batch: 050 ----
mean loss: 762.82
 ---- batch: 060 ----
mean loss: 763.90
 ---- batch: 070 ----
mean loss: 769.28
 ---- batch: 080 ----
mean loss: 749.33
 ---- batch: 090 ----
mean loss: 741.12
train mean loss: 756.20
epoch train time: 0:00:02.670868
elapsed time: 0:01:51.676826
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 19:00:26.142579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.44
 ---- batch: 020 ----
mean loss: 753.69
 ---- batch: 030 ----
mean loss: 745.23
 ---- batch: 040 ----
mean loss: 749.68
 ---- batch: 050 ----
mean loss: 751.89
 ---- batch: 060 ----
mean loss: 756.91
 ---- batch: 070 ----
mean loss: 744.91
 ---- batch: 080 ----
mean loss: 743.62
 ---- batch: 090 ----
mean loss: 721.44
train mean loss: 746.65
epoch train time: 0:00:02.669616
elapsed time: 0:01:54.346887
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 19:00:28.812646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.03
 ---- batch: 020 ----
mean loss: 736.15
 ---- batch: 030 ----
mean loss: 731.13
 ---- batch: 040 ----
mean loss: 742.54
 ---- batch: 050 ----
mean loss: 756.75
 ---- batch: 060 ----
mean loss: 726.49
 ---- batch: 070 ----
mean loss: 748.50
 ---- batch: 080 ----
mean loss: 730.27
 ---- batch: 090 ----
mean loss: 723.81
train mean loss: 736.16
epoch train time: 0:00:02.668136
elapsed time: 0:01:57.015469
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 19:00:31.481259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 735.45
 ---- batch: 020 ----
mean loss: 737.76
 ---- batch: 030 ----
mean loss: 728.80
 ---- batch: 040 ----
mean loss: 716.79
 ---- batch: 050 ----
mean loss: 722.98
 ---- batch: 060 ----
mean loss: 734.52
 ---- batch: 070 ----
mean loss: 709.37
 ---- batch: 080 ----
mean loss: 738.81
 ---- batch: 090 ----
mean loss: 719.14
train mean loss: 726.70
epoch train time: 0:00:02.697012
elapsed time: 0:01:59.712956
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 19:00:34.178737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 721.14
 ---- batch: 020 ----
mean loss: 725.01
 ---- batch: 030 ----
mean loss: 720.79
 ---- batch: 040 ----
mean loss: 715.18
 ---- batch: 050 ----
mean loss: 703.03
 ---- batch: 060 ----
mean loss: 712.76
 ---- batch: 070 ----
mean loss: 712.46
 ---- batch: 080 ----
mean loss: 716.22
 ---- batch: 090 ----
mean loss: 710.85
train mean loss: 716.42
epoch train time: 0:00:02.666390
elapsed time: 0:02:02.379854
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 19:00:36.845620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.15
 ---- batch: 020 ----
mean loss: 713.36
 ---- batch: 030 ----
mean loss: 708.41
 ---- batch: 040 ----
mean loss: 706.38
 ---- batch: 050 ----
mean loss: 710.01
 ---- batch: 060 ----
mean loss: 702.69
 ---- batch: 070 ----
mean loss: 706.82
 ---- batch: 080 ----
mean loss: 688.01
 ---- batch: 090 ----
mean loss: 698.87
train mean loss: 705.13
epoch train time: 0:00:02.667332
elapsed time: 0:02:05.047717
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 19:00:39.513465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 697.93
 ---- batch: 020 ----
mean loss: 697.86
 ---- batch: 030 ----
mean loss: 686.40
 ---- batch: 040 ----
mean loss: 691.95
 ---- batch: 050 ----
mean loss: 689.87
 ---- batch: 060 ----
mean loss: 700.95
 ---- batch: 070 ----
mean loss: 696.57
 ---- batch: 080 ----
mean loss: 690.75
 ---- batch: 090 ----
mean loss: 700.42
train mean loss: 694.99
epoch train time: 0:00:02.672938
elapsed time: 0:02:07.721163
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 19:00:42.186932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 686.99
 ---- batch: 020 ----
mean loss: 684.83
 ---- batch: 030 ----
mean loss: 696.45
 ---- batch: 040 ----
mean loss: 697.10
 ---- batch: 050 ----
mean loss: 689.72
 ---- batch: 060 ----
mean loss: 661.64
 ---- batch: 070 ----
mean loss: 667.56
 ---- batch: 080 ----
mean loss: 673.87
 ---- batch: 090 ----
mean loss: 700.81
train mean loss: 683.30
epoch train time: 0:00:02.644390
elapsed time: 0:02:10.366059
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 19:00:44.831819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 685.67
 ---- batch: 020 ----
mean loss: 673.22
 ---- batch: 030 ----
mean loss: 672.11
 ---- batch: 040 ----
mean loss: 686.08
 ---- batch: 050 ----
mean loss: 681.02
 ---- batch: 060 ----
mean loss: 665.43
 ---- batch: 070 ----
mean loss: 671.96
 ---- batch: 080 ----
mean loss: 651.05
 ---- batch: 090 ----
mean loss: 660.21
train mean loss: 672.03
epoch train time: 0:00:02.652637
elapsed time: 0:02:13.019142
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 19:00:47.484899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 653.72
 ---- batch: 020 ----
mean loss: 672.35
 ---- batch: 030 ----
mean loss: 660.75
 ---- batch: 040 ----
mean loss: 672.27
 ---- batch: 050 ----
mean loss: 656.66
 ---- batch: 060 ----
mean loss: 658.74
 ---- batch: 070 ----
mean loss: 659.65
 ---- batch: 080 ----
mean loss: 660.35
 ---- batch: 090 ----
mean loss: 641.91
train mean loss: 659.60
epoch train time: 0:00:02.648920
elapsed time: 0:02:15.668543
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 19:00:50.134302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 655.39
 ---- batch: 020 ----
mean loss: 651.34
 ---- batch: 030 ----
mean loss: 640.57
 ---- batch: 040 ----
mean loss: 652.31
 ---- batch: 050 ----
mean loss: 636.47
 ---- batch: 060 ----
mean loss: 649.62
 ---- batch: 070 ----
mean loss: 646.42
 ---- batch: 080 ----
mean loss: 635.99
 ---- batch: 090 ----
mean loss: 635.57
train mean loss: 644.64
epoch train time: 0:00:02.667221
elapsed time: 0:02:18.336268
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 19:00:52.802086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 636.47
 ---- batch: 020 ----
mean loss: 622.90
 ---- batch: 030 ----
mean loss: 621.74
 ---- batch: 040 ----
mean loss: 629.98
 ---- batch: 050 ----
mean loss: 621.39
 ---- batch: 060 ----
mean loss: 617.18
 ---- batch: 070 ----
mean loss: 610.75
 ---- batch: 080 ----
mean loss: 605.84
 ---- batch: 090 ----
mean loss: 612.88
train mean loss: 619.94
epoch train time: 0:00:02.685218
elapsed time: 0:02:21.022048
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 19:00:55.487816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 608.56
 ---- batch: 020 ----
mean loss: 600.78
 ---- batch: 030 ----
mean loss: 591.22
 ---- batch: 040 ----
mean loss: 587.66
 ---- batch: 050 ----
mean loss: 592.29
 ---- batch: 060 ----
mean loss: 586.84
 ---- batch: 070 ----
mean loss: 584.80
 ---- batch: 080 ----
mean loss: 576.09
 ---- batch: 090 ----
mean loss: 567.77
train mean loss: 587.46
epoch train time: 0:00:02.647189
elapsed time: 0:02:23.669708
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 19:00:58.135462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 563.18
 ---- batch: 020 ----
mean loss: 569.08
 ---- batch: 030 ----
mean loss: 568.26
 ---- batch: 040 ----
mean loss: 566.72
 ---- batch: 050 ----
mean loss: 547.80
 ---- batch: 060 ----
mean loss: 570.70
 ---- batch: 070 ----
mean loss: 552.51
 ---- batch: 080 ----
mean loss: 545.14
 ---- batch: 090 ----
mean loss: 553.61
train mean loss: 559.29
epoch train time: 0:00:02.635481
elapsed time: 0:02:26.305779
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 19:01:00.771548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.16
 ---- batch: 020 ----
mean loss: 531.70
 ---- batch: 030 ----
mean loss: 541.58
 ---- batch: 040 ----
mean loss: 515.65
 ---- batch: 050 ----
mean loss: 530.77
 ---- batch: 060 ----
mean loss: 534.84
 ---- batch: 070 ----
mean loss: 524.79
 ---- batch: 080 ----
mean loss: 531.67
 ---- batch: 090 ----
mean loss: 523.57
train mean loss: 531.29
epoch train time: 0:00:02.663782
elapsed time: 0:02:28.970017
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 19:01:03.435777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.82
 ---- batch: 020 ----
mean loss: 517.26
 ---- batch: 030 ----
mean loss: 515.14
 ---- batch: 040 ----
mean loss: 506.75
 ---- batch: 050 ----
mean loss: 496.93
 ---- batch: 060 ----
mean loss: 495.44
 ---- batch: 070 ----
mean loss: 496.34
 ---- batch: 080 ----
mean loss: 494.90
 ---- batch: 090 ----
mean loss: 493.42
train mean loss: 502.66
epoch train time: 0:00:02.678906
elapsed time: 0:02:31.649359
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 19:01:06.115118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.21
 ---- batch: 020 ----
mean loss: 471.76
 ---- batch: 030 ----
mean loss: 497.07
 ---- batch: 040 ----
mean loss: 474.37
 ---- batch: 050 ----
mean loss: 472.30
 ---- batch: 060 ----
mean loss: 474.10
 ---- batch: 070 ----
mean loss: 475.96
 ---- batch: 080 ----
mean loss: 459.35
 ---- batch: 090 ----
mean loss: 473.39
train mean loss: 475.62
epoch train time: 0:00:02.645867
elapsed time: 0:02:34.295676
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 19:01:08.761460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.90
 ---- batch: 020 ----
mean loss: 468.20
 ---- batch: 030 ----
mean loss: 454.85
 ---- batch: 040 ----
mean loss: 455.73
 ---- batch: 050 ----
mean loss: 443.40
 ---- batch: 060 ----
mean loss: 446.79
 ---- batch: 070 ----
mean loss: 454.53
 ---- batch: 080 ----
mean loss: 438.68
 ---- batch: 090 ----
mean loss: 440.73
train mean loss: 450.26
epoch train time: 0:00:02.684321
elapsed time: 0:02:36.980510
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 19:01:11.446282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.78
 ---- batch: 020 ----
mean loss: 436.88
 ---- batch: 030 ----
mean loss: 433.38
 ---- batch: 040 ----
mean loss: 422.89
 ---- batch: 050 ----
mean loss: 426.49
 ---- batch: 060 ----
mean loss: 431.11
 ---- batch: 070 ----
mean loss: 422.26
 ---- batch: 080 ----
mean loss: 405.32
 ---- batch: 090 ----
mean loss: 412.71
train mean loss: 425.75
epoch train time: 0:00:02.660301
elapsed time: 0:02:39.641285
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 19:01:14.107039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.76
 ---- batch: 020 ----
mean loss: 415.20
 ---- batch: 030 ----
mean loss: 409.64
 ---- batch: 040 ----
mean loss: 400.48
 ---- batch: 050 ----
mean loss: 419.26
 ---- batch: 060 ----
mean loss: 395.02
 ---- batch: 070 ----
mean loss: 404.69
 ---- batch: 080 ----
mean loss: 392.20
 ---- batch: 090 ----
mean loss: 394.73
train mean loss: 404.80
epoch train time: 0:00:02.628920
elapsed time: 0:02:42.270654
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 19:01:16.736406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.17
 ---- batch: 020 ----
mean loss: 388.68
 ---- batch: 030 ----
mean loss: 392.41
 ---- batch: 040 ----
mean loss: 378.12
 ---- batch: 050 ----
mean loss: 387.33
 ---- batch: 060 ----
mean loss: 383.65
 ---- batch: 070 ----
mean loss: 391.00
 ---- batch: 080 ----
mean loss: 389.46
 ---- batch: 090 ----
mean loss: 385.09
train mean loss: 387.23
epoch train time: 0:00:02.639308
elapsed time: 0:02:44.910451
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 19:01:19.376225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.57
 ---- batch: 020 ----
mean loss: 367.41
 ---- batch: 030 ----
mean loss: 376.25
 ---- batch: 040 ----
mean loss: 376.33
 ---- batch: 050 ----
mean loss: 367.46
 ---- batch: 060 ----
mean loss: 381.64
 ---- batch: 070 ----
mean loss: 365.71
 ---- batch: 080 ----
mean loss: 368.99
 ---- batch: 090 ----
mean loss: 367.13
train mean loss: 371.17
epoch train time: 0:00:02.660298
elapsed time: 0:02:47.571223
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 19:01:22.036974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.74
 ---- batch: 020 ----
mean loss: 360.49
 ---- batch: 030 ----
mean loss: 365.70
 ---- batch: 040 ----
mean loss: 359.40
 ---- batch: 050 ----
mean loss: 356.37
 ---- batch: 060 ----
mean loss: 363.66
 ---- batch: 070 ----
mean loss: 357.29
 ---- batch: 080 ----
mean loss: 351.55
 ---- batch: 090 ----
mean loss: 354.08
train mean loss: 359.10
epoch train time: 0:00:02.659701
elapsed time: 0:02:50.231451
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 19:01:24.697223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.63
 ---- batch: 020 ----
mean loss: 348.47
 ---- batch: 030 ----
mean loss: 351.52
 ---- batch: 040 ----
mean loss: 355.31
 ---- batch: 050 ----
mean loss: 343.50
 ---- batch: 060 ----
mean loss: 345.30
 ---- batch: 070 ----
mean loss: 340.73
 ---- batch: 080 ----
mean loss: 351.01
 ---- batch: 090 ----
mean loss: 355.91
train mean loss: 348.93
epoch train time: 0:00:02.641477
elapsed time: 0:02:52.873412
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 19:01:27.339187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.21
 ---- batch: 020 ----
mean loss: 342.85
 ---- batch: 030 ----
mean loss: 342.50
 ---- batch: 040 ----
mean loss: 334.63
 ---- batch: 050 ----
mean loss: 350.24
 ---- batch: 060 ----
mean loss: 344.92
 ---- batch: 070 ----
mean loss: 326.26
 ---- batch: 080 ----
mean loss: 337.67
 ---- batch: 090 ----
mean loss: 341.26
train mean loss: 339.83
epoch train time: 0:00:02.652972
elapsed time: 0:02:55.526845
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 19:01:29.992675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.13
 ---- batch: 020 ----
mean loss: 333.75
 ---- batch: 030 ----
mean loss: 344.79
 ---- batch: 040 ----
mean loss: 330.15
 ---- batch: 050 ----
mean loss: 329.97
 ---- batch: 060 ----
mean loss: 335.34
 ---- batch: 070 ----
mean loss: 321.91
 ---- batch: 080 ----
mean loss: 341.49
 ---- batch: 090 ----
mean loss: 319.24
train mean loss: 332.46
epoch train time: 0:00:02.639667
elapsed time: 0:02:58.167019
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 19:01:32.632728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.77
 ---- batch: 020 ----
mean loss: 322.99
 ---- batch: 030 ----
mean loss: 334.78
 ---- batch: 040 ----
mean loss: 320.85
 ---- batch: 050 ----
mean loss: 330.59
 ---- batch: 060 ----
mean loss: 332.14
 ---- batch: 070 ----
mean loss: 332.13
 ---- batch: 080 ----
mean loss: 312.88
 ---- batch: 090 ----
mean loss: 327.27
train mean loss: 326.47
epoch train time: 0:00:02.677270
elapsed time: 0:03:00.844751
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 19:01:35.310510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.46
 ---- batch: 020 ----
mean loss: 319.70
 ---- batch: 030 ----
mean loss: 319.63
 ---- batch: 040 ----
mean loss: 326.85
 ---- batch: 050 ----
mean loss: 321.96
 ---- batch: 060 ----
mean loss: 322.25
 ---- batch: 070 ----
mean loss: 329.83
 ---- batch: 080 ----
mean loss: 304.34
 ---- batch: 090 ----
mean loss: 317.88
train mean loss: 321.20
epoch train time: 0:00:02.652206
elapsed time: 0:03:03.497520
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 19:01:37.963297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.84
 ---- batch: 020 ----
mean loss: 313.79
 ---- batch: 030 ----
mean loss: 331.73
 ---- batch: 040 ----
mean loss: 315.48
 ---- batch: 050 ----
mean loss: 322.77
 ---- batch: 060 ----
mean loss: 317.33
 ---- batch: 070 ----
mean loss: 315.83
 ---- batch: 080 ----
mean loss: 313.04
 ---- batch: 090 ----
mean loss: 313.50
train mean loss: 317.08
epoch train time: 0:00:02.672221
elapsed time: 0:03:06.170235
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 19:01:40.635997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.56
 ---- batch: 020 ----
mean loss: 321.41
 ---- batch: 030 ----
mean loss: 319.56
 ---- batch: 040 ----
mean loss: 313.65
 ---- batch: 050 ----
mean loss: 322.43
 ---- batch: 060 ----
mean loss: 314.21
 ---- batch: 070 ----
mean loss: 303.74
 ---- batch: 080 ----
mean loss: 309.76
 ---- batch: 090 ----
mean loss: 310.31
train mean loss: 314.10
epoch train time: 0:00:02.662448
elapsed time: 0:03:08.833192
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 19:01:43.298947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.00
 ---- batch: 020 ----
mean loss: 302.02
 ---- batch: 030 ----
mean loss: 311.97
 ---- batch: 040 ----
mean loss: 312.31
 ---- batch: 050 ----
mean loss: 302.65
 ---- batch: 060 ----
mean loss: 309.11
 ---- batch: 070 ----
mean loss: 311.50
 ---- batch: 080 ----
mean loss: 317.06
 ---- batch: 090 ----
mean loss: 314.71
train mean loss: 309.97
epoch train time: 0:00:02.654645
elapsed time: 0:03:11.488405
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 19:01:45.954209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.16
 ---- batch: 020 ----
mean loss: 308.09
 ---- batch: 030 ----
mean loss: 305.36
 ---- batch: 040 ----
mean loss: 305.63
 ---- batch: 050 ----
mean loss: 307.46
 ---- batch: 060 ----
mean loss: 313.68
 ---- batch: 070 ----
mean loss: 301.79
 ---- batch: 080 ----
mean loss: 303.65
 ---- batch: 090 ----
mean loss: 306.74
train mean loss: 306.72
epoch train time: 0:00:02.655183
elapsed time: 0:03:14.144145
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 19:01:48.609908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.88
 ---- batch: 020 ----
mean loss: 298.59
 ---- batch: 030 ----
mean loss: 295.22
 ---- batch: 040 ----
mean loss: 310.28
 ---- batch: 050 ----
mean loss: 300.09
 ---- batch: 060 ----
mean loss: 297.16
 ---- batch: 070 ----
mean loss: 313.69
 ---- batch: 080 ----
mean loss: 309.11
 ---- batch: 090 ----
mean loss: 311.59
train mean loss: 303.10
epoch train time: 0:00:02.668752
elapsed time: 0:03:16.813326
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 19:01:51.279113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.88
 ---- batch: 020 ----
mean loss: 297.01
 ---- batch: 030 ----
mean loss: 298.56
 ---- batch: 040 ----
mean loss: 290.29
 ---- batch: 050 ----
mean loss: 299.96
 ---- batch: 060 ----
mean loss: 308.92
 ---- batch: 070 ----
mean loss: 299.29
 ---- batch: 080 ----
mean loss: 297.27
 ---- batch: 090 ----
mean loss: 310.74
train mean loss: 300.61
epoch train time: 0:00:02.665167
elapsed time: 0:03:19.479068
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 19:01:53.944835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.04
 ---- batch: 020 ----
mean loss: 288.17
 ---- batch: 030 ----
mean loss: 302.90
 ---- batch: 040 ----
mean loss: 292.66
 ---- batch: 050 ----
mean loss: 303.21
 ---- batch: 060 ----
mean loss: 298.37
 ---- batch: 070 ----
mean loss: 301.31
 ---- batch: 080 ----
mean loss: 302.49
 ---- batch: 090 ----
mean loss: 292.55
train mean loss: 298.36
epoch train time: 0:00:02.669349
elapsed time: 0:03:22.148886
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 19:01:56.614643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.37
 ---- batch: 020 ----
mean loss: 294.70
 ---- batch: 030 ----
mean loss: 296.58
 ---- batch: 040 ----
mean loss: 297.13
 ---- batch: 050 ----
mean loss: 305.56
 ---- batch: 060 ----
mean loss: 290.23
 ---- batch: 070 ----
mean loss: 290.24
 ---- batch: 080 ----
mean loss: 294.04
 ---- batch: 090 ----
mean loss: 293.00
train mean loss: 295.72
epoch train time: 0:00:02.651581
elapsed time: 0:03:24.800901
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 19:01:59.266650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.91
 ---- batch: 020 ----
mean loss: 295.27
 ---- batch: 030 ----
mean loss: 293.36
 ---- batch: 040 ----
mean loss: 300.74
 ---- batch: 050 ----
mean loss: 300.64
 ---- batch: 060 ----
mean loss: 289.98
 ---- batch: 070 ----
mean loss: 282.48
 ---- batch: 080 ----
mean loss: 291.69
 ---- batch: 090 ----
mean loss: 293.21
train mean loss: 294.01
epoch train time: 0:00:02.675380
elapsed time: 0:03:27.476742
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 19:02:01.942557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.05
 ---- batch: 020 ----
mean loss: 293.29
 ---- batch: 030 ----
mean loss: 297.23
 ---- batch: 040 ----
mean loss: 286.42
 ---- batch: 050 ----
mean loss: 287.62
 ---- batch: 060 ----
mean loss: 297.06
 ---- batch: 070 ----
mean loss: 282.57
 ---- batch: 080 ----
mean loss: 296.56
 ---- batch: 090 ----
mean loss: 291.30
train mean loss: 290.43
epoch train time: 0:00:02.643280
elapsed time: 0:03:30.120522
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 19:02:04.586277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.59
 ---- batch: 020 ----
mean loss: 282.32
 ---- batch: 030 ----
mean loss: 300.61
 ---- batch: 040 ----
mean loss: 298.61
 ---- batch: 050 ----
mean loss: 293.58
 ---- batch: 060 ----
mean loss: 288.01
 ---- batch: 070 ----
mean loss: 286.09
 ---- batch: 080 ----
mean loss: 281.92
 ---- batch: 090 ----
mean loss: 283.47
train mean loss: 288.57
epoch train time: 0:00:02.692162
elapsed time: 0:03:32.813109
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 19:02:07.278855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.71
 ---- batch: 020 ----
mean loss: 286.36
 ---- batch: 030 ----
mean loss: 279.73
 ---- batch: 040 ----
mean loss: 277.75
 ---- batch: 050 ----
mean loss: 285.82
 ---- batch: 060 ----
mean loss: 291.08
 ---- batch: 070 ----
mean loss: 296.86
 ---- batch: 080 ----
mean loss: 292.84
 ---- batch: 090 ----
mean loss: 280.11
train mean loss: 286.16
epoch train time: 0:00:02.674302
elapsed time: 0:03:35.487945
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 19:02:09.953738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.82
 ---- batch: 020 ----
mean loss: 288.52
 ---- batch: 030 ----
mean loss: 279.50
 ---- batch: 040 ----
mean loss: 279.95
 ---- batch: 050 ----
mean loss: 279.14
 ---- batch: 060 ----
mean loss: 281.88
 ---- batch: 070 ----
mean loss: 288.04
 ---- batch: 080 ----
mean loss: 280.98
 ---- batch: 090 ----
mean loss: 286.35
train mean loss: 283.94
epoch train time: 0:00:02.664477
elapsed time: 0:03:38.152883
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 19:02:12.618687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.35
 ---- batch: 020 ----
mean loss: 274.18
 ---- batch: 030 ----
mean loss: 280.76
 ---- batch: 040 ----
mean loss: 282.63
 ---- batch: 050 ----
mean loss: 282.24
 ---- batch: 060 ----
mean loss: 270.83
 ---- batch: 070 ----
mean loss: 273.63
 ---- batch: 080 ----
mean loss: 294.62
 ---- batch: 090 ----
mean loss: 288.41
train mean loss: 282.33
epoch train time: 0:00:02.680619
elapsed time: 0:03:40.833978
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 19:02:15.299733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.43
 ---- batch: 020 ----
mean loss: 280.79
 ---- batch: 030 ----
mean loss: 286.33
 ---- batch: 040 ----
mean loss: 290.00
 ---- batch: 050 ----
mean loss: 284.11
 ---- batch: 060 ----
mean loss: 276.68
 ---- batch: 070 ----
mean loss: 279.21
 ---- batch: 080 ----
mean loss: 275.06
 ---- batch: 090 ----
mean loss: 283.23
train mean loss: 280.03
epoch train time: 0:00:02.665217
elapsed time: 0:03:43.499643
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 19:02:17.965413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.71
 ---- batch: 020 ----
mean loss: 272.35
 ---- batch: 030 ----
mean loss: 285.81
 ---- batch: 040 ----
mean loss: 277.24
 ---- batch: 050 ----
mean loss: 283.96
 ---- batch: 060 ----
mean loss: 286.03
 ---- batch: 070 ----
mean loss: 267.93
 ---- batch: 080 ----
mean loss: 278.46
 ---- batch: 090 ----
mean loss: 277.80
train mean loss: 278.23
epoch train time: 0:00:02.674032
elapsed time: 0:03:46.174111
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 19:02:20.639879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.77
 ---- batch: 020 ----
mean loss: 263.23
 ---- batch: 030 ----
mean loss: 271.67
 ---- batch: 040 ----
mean loss: 281.11
 ---- batch: 050 ----
mean loss: 284.73
 ---- batch: 060 ----
mean loss: 279.72
 ---- batch: 070 ----
mean loss: 275.59
 ---- batch: 080 ----
mean loss: 279.95
 ---- batch: 090 ----
mean loss: 281.04
train mean loss: 276.92
epoch train time: 0:00:02.659915
elapsed time: 0:03:48.834515
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 19:02:23.300271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.23
 ---- batch: 020 ----
mean loss: 276.33
 ---- batch: 030 ----
mean loss: 287.61
 ---- batch: 040 ----
mean loss: 271.12
 ---- batch: 050 ----
mean loss: 270.95
 ---- batch: 060 ----
mean loss: 276.44
 ---- batch: 070 ----
mean loss: 275.38
 ---- batch: 080 ----
mean loss: 280.28
 ---- batch: 090 ----
mean loss: 271.38
train mean loss: 274.80
epoch train time: 0:00:02.673410
elapsed time: 0:03:51.508356
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 19:02:25.974106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.33
 ---- batch: 020 ----
mean loss: 277.72
 ---- batch: 030 ----
mean loss: 280.34
 ---- batch: 040 ----
mean loss: 272.23
 ---- batch: 050 ----
mean loss: 272.06
 ---- batch: 060 ----
mean loss: 276.01
 ---- batch: 070 ----
mean loss: 270.68
 ---- batch: 080 ----
mean loss: 273.51
 ---- batch: 090 ----
mean loss: 272.59
train mean loss: 273.87
epoch train time: 0:00:02.685864
elapsed time: 0:03:54.194665
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 19:02:28.660437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.53
 ---- batch: 020 ----
mean loss: 272.76
 ---- batch: 030 ----
mean loss: 273.72
 ---- batch: 040 ----
mean loss: 274.23
 ---- batch: 050 ----
mean loss: 264.11
 ---- batch: 060 ----
mean loss: 273.68
 ---- batch: 070 ----
mean loss: 277.58
 ---- batch: 080 ----
mean loss: 275.84
 ---- batch: 090 ----
mean loss: 274.36
train mean loss: 272.06
epoch train time: 0:00:02.685281
elapsed time: 0:03:56.880420
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 19:02:31.346197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.62
 ---- batch: 020 ----
mean loss: 269.70
 ---- batch: 030 ----
mean loss: 253.74
 ---- batch: 040 ----
mean loss: 273.59
 ---- batch: 050 ----
mean loss: 276.51
 ---- batch: 060 ----
mean loss: 264.50
 ---- batch: 070 ----
mean loss: 276.62
 ---- batch: 080 ----
mean loss: 279.84
 ---- batch: 090 ----
mean loss: 269.11
train mean loss: 271.40
epoch train time: 0:00:02.677125
elapsed time: 0:03:59.558020
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 19:02:34.023781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.94
 ---- batch: 020 ----
mean loss: 276.01
 ---- batch: 030 ----
mean loss: 263.12
 ---- batch: 040 ----
mean loss: 279.90
 ---- batch: 050 ----
mean loss: 265.64
 ---- batch: 060 ----
mean loss: 267.48
 ---- batch: 070 ----
mean loss: 270.57
 ---- batch: 080 ----
mean loss: 270.71
 ---- batch: 090 ----
mean loss: 271.84
train mean loss: 270.23
epoch train time: 0:00:02.679914
elapsed time: 0:04:02.238432
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 19:02:36.704189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.73
 ---- batch: 020 ----
mean loss: 265.07
 ---- batch: 030 ----
mean loss: 265.21
 ---- batch: 040 ----
mean loss: 278.41
 ---- batch: 050 ----
mean loss: 273.53
 ---- batch: 060 ----
mean loss: 264.51
 ---- batch: 070 ----
mean loss: 284.59
 ---- batch: 080 ----
mean loss: 263.23
 ---- batch: 090 ----
mean loss: 270.23
train mean loss: 268.88
epoch train time: 0:00:02.652459
elapsed time: 0:04:04.891326
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 19:02:39.357128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.73
 ---- batch: 020 ----
mean loss: 266.03
 ---- batch: 030 ----
mean loss: 267.00
 ---- batch: 040 ----
mean loss: 274.19
 ---- batch: 050 ----
mean loss: 263.46
 ---- batch: 060 ----
mean loss: 279.60
 ---- batch: 070 ----
mean loss: 257.39
 ---- batch: 080 ----
mean loss: 269.43
 ---- batch: 090 ----
mean loss: 269.38
train mean loss: 267.69
epoch train time: 0:00:02.651987
elapsed time: 0:04:07.543798
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 19:02:42.009547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.61
 ---- batch: 020 ----
mean loss: 265.04
 ---- batch: 030 ----
mean loss: 267.45
 ---- batch: 040 ----
mean loss: 265.07
 ---- batch: 050 ----
mean loss: 264.95
 ---- batch: 060 ----
mean loss: 265.92
 ---- batch: 070 ----
mean loss: 265.42
 ---- batch: 080 ----
mean loss: 267.45
 ---- batch: 090 ----
mean loss: 269.74
train mean loss: 266.70
epoch train time: 0:00:02.669256
elapsed time: 0:04:10.213528
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 19:02:44.679284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.90
 ---- batch: 020 ----
mean loss: 273.61
 ---- batch: 030 ----
mean loss: 275.15
 ---- batch: 040 ----
mean loss: 263.51
 ---- batch: 050 ----
mean loss: 276.77
 ---- batch: 060 ----
mean loss: 263.02
 ---- batch: 070 ----
mean loss: 262.27
 ---- batch: 080 ----
mean loss: 253.90
 ---- batch: 090 ----
mean loss: 264.86
train mean loss: 265.72
epoch train time: 0:00:02.666683
elapsed time: 0:04:12.880692
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 19:02:47.346443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.60
 ---- batch: 020 ----
mean loss: 258.81
 ---- batch: 030 ----
mean loss: 257.67
 ---- batch: 040 ----
mean loss: 260.69
 ---- batch: 050 ----
mean loss: 266.75
 ---- batch: 060 ----
mean loss: 273.50
 ---- batch: 070 ----
mean loss: 260.07
 ---- batch: 080 ----
mean loss: 274.71
 ---- batch: 090 ----
mean loss: 262.81
train mean loss: 264.59
epoch train time: 0:00:02.660786
elapsed time: 0:04:15.541986
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 19:02:50.007747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.45
 ---- batch: 020 ----
mean loss: 254.21
 ---- batch: 030 ----
mean loss: 263.00
 ---- batch: 040 ----
mean loss: 261.33
 ---- batch: 050 ----
mean loss: 268.24
 ---- batch: 060 ----
mean loss: 266.37
 ---- batch: 070 ----
mean loss: 264.05
 ---- batch: 080 ----
mean loss: 262.34
 ---- batch: 090 ----
mean loss: 264.12
train mean loss: 264.11
epoch train time: 0:00:02.644661
elapsed time: 0:04:18.187087
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 19:02:52.652857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.57
 ---- batch: 020 ----
mean loss: 260.02
 ---- batch: 030 ----
mean loss: 261.67
 ---- batch: 040 ----
mean loss: 253.13
 ---- batch: 050 ----
mean loss: 267.17
 ---- batch: 060 ----
mean loss: 266.04
 ---- batch: 070 ----
mean loss: 258.93
 ---- batch: 080 ----
mean loss: 262.81
 ---- batch: 090 ----
mean loss: 269.36
train mean loss: 262.44
epoch train time: 0:00:02.681141
elapsed time: 0:04:20.868687
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 19:02:55.334443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.37
 ---- batch: 020 ----
mean loss: 272.33
 ---- batch: 030 ----
mean loss: 259.57
 ---- batch: 040 ----
mean loss: 266.45
 ---- batch: 050 ----
mean loss: 262.04
 ---- batch: 060 ----
mean loss: 251.63
 ---- batch: 070 ----
mean loss: 250.50
 ---- batch: 080 ----
mean loss: 254.54
 ---- batch: 090 ----
mean loss: 271.16
train mean loss: 262.18
epoch train time: 0:00:02.683744
elapsed time: 0:04:23.552906
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 19:02:58.018666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.58
 ---- batch: 020 ----
mean loss: 260.74
 ---- batch: 030 ----
mean loss: 264.56
 ---- batch: 040 ----
mean loss: 264.16
 ---- batch: 050 ----
mean loss: 261.89
 ---- batch: 060 ----
mean loss: 260.17
 ---- batch: 070 ----
mean loss: 259.49
 ---- batch: 080 ----
mean loss: 261.39
 ---- batch: 090 ----
mean loss: 253.78
train mean loss: 261.15
epoch train time: 0:00:02.736201
elapsed time: 0:04:26.289622
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 19:03:00.755391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.32
 ---- batch: 020 ----
mean loss: 262.78
 ---- batch: 030 ----
mean loss: 264.71
 ---- batch: 040 ----
mean loss: 266.28
 ---- batch: 050 ----
mean loss: 250.06
 ---- batch: 060 ----
mean loss: 266.25
 ---- batch: 070 ----
mean loss: 255.37
 ---- batch: 080 ----
mean loss: 267.49
 ---- batch: 090 ----
mean loss: 258.58
train mean loss: 260.74
epoch train time: 0:00:02.691459
elapsed time: 0:04:28.981556
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 19:03:03.447309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.46
 ---- batch: 020 ----
mean loss: 261.74
 ---- batch: 030 ----
mean loss: 259.02
 ---- batch: 040 ----
mean loss: 256.87
 ---- batch: 050 ----
mean loss: 258.37
 ---- batch: 060 ----
mean loss: 265.37
 ---- batch: 070 ----
mean loss: 254.28
 ---- batch: 080 ----
mean loss: 258.47
 ---- batch: 090 ----
mean loss: 252.75
train mean loss: 259.86
epoch train time: 0:00:02.713849
elapsed time: 0:04:31.695898
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 19:03:06.161683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.10
 ---- batch: 020 ----
mean loss: 264.90
 ---- batch: 030 ----
mean loss: 261.61
 ---- batch: 040 ----
mean loss: 258.39
 ---- batch: 050 ----
mean loss: 260.26
 ---- batch: 060 ----
mean loss: 257.50
 ---- batch: 070 ----
mean loss: 256.32
 ---- batch: 080 ----
mean loss: 256.25
 ---- batch: 090 ----
mean loss: 260.10
train mean loss: 259.06
epoch train time: 0:00:02.705601
elapsed time: 0:04:34.401999
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 19:03:08.867752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.68
 ---- batch: 020 ----
mean loss: 255.76
 ---- batch: 030 ----
mean loss: 246.56
 ---- batch: 040 ----
mean loss: 259.34
 ---- batch: 050 ----
mean loss: 262.30
 ---- batch: 060 ----
mean loss: 271.36
 ---- batch: 070 ----
mean loss: 258.23
 ---- batch: 080 ----
mean loss: 258.64
 ---- batch: 090 ----
mean loss: 255.32
train mean loss: 258.29
epoch train time: 0:00:02.716099
elapsed time: 0:04:37.118569
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 19:03:11.584351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.88
 ---- batch: 020 ----
mean loss: 263.58
 ---- batch: 030 ----
mean loss: 257.47
 ---- batch: 040 ----
mean loss: 256.06
 ---- batch: 050 ----
mean loss: 262.82
 ---- batch: 060 ----
mean loss: 265.69
 ---- batch: 070 ----
mean loss: 255.68
 ---- batch: 080 ----
mean loss: 259.99
 ---- batch: 090 ----
mean loss: 250.63
train mean loss: 257.76
epoch train time: 0:00:02.720014
elapsed time: 0:04:39.839039
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 19:03:14.304857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.22
 ---- batch: 020 ----
mean loss: 262.70
 ---- batch: 030 ----
mean loss: 255.21
 ---- batch: 040 ----
mean loss: 259.20
 ---- batch: 050 ----
mean loss: 253.94
 ---- batch: 060 ----
mean loss: 258.55
 ---- batch: 070 ----
mean loss: 257.23
 ---- batch: 080 ----
mean loss: 259.31
 ---- batch: 090 ----
mean loss: 260.31
train mean loss: 257.64
epoch train time: 0:00:02.677613
elapsed time: 0:04:42.517222
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 19:03:16.982979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.58
 ---- batch: 020 ----
mean loss: 265.48
 ---- batch: 030 ----
mean loss: 257.17
 ---- batch: 040 ----
mean loss: 259.43
 ---- batch: 050 ----
mean loss: 255.17
 ---- batch: 060 ----
mean loss: 261.25
 ---- batch: 070 ----
mean loss: 261.45
 ---- batch: 080 ----
mean loss: 262.96
 ---- batch: 090 ----
mean loss: 246.53
train mean loss: 257.11
epoch train time: 0:00:02.660865
elapsed time: 0:04:45.178556
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 19:03:19.644319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.48
 ---- batch: 020 ----
mean loss: 253.35
 ---- batch: 030 ----
mean loss: 266.46
 ---- batch: 040 ----
mean loss: 245.60
 ---- batch: 050 ----
mean loss: 250.91
 ---- batch: 060 ----
mean loss: 263.25
 ---- batch: 070 ----
mean loss: 257.35
 ---- batch: 080 ----
mean loss: 252.19
 ---- batch: 090 ----
mean loss: 263.32
train mean loss: 256.02
epoch train time: 0:00:02.674923
elapsed time: 0:04:47.853948
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 19:03:22.319708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.47
 ---- batch: 020 ----
mean loss: 249.47
 ---- batch: 030 ----
mean loss: 256.36
 ---- batch: 040 ----
mean loss: 258.71
 ---- batch: 050 ----
mean loss: 254.55
 ---- batch: 060 ----
mean loss: 256.30
 ---- batch: 070 ----
mean loss: 249.85
 ---- batch: 080 ----
mean loss: 254.20
 ---- batch: 090 ----
mean loss: 257.36
train mean loss: 255.31
epoch train time: 0:00:02.687957
elapsed time: 0:04:50.542386
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 19:03:25.008083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.04
 ---- batch: 020 ----
mean loss: 250.29
 ---- batch: 030 ----
mean loss: 255.88
 ---- batch: 040 ----
mean loss: 254.39
 ---- batch: 050 ----
mean loss: 253.81
 ---- batch: 060 ----
mean loss: 254.40
 ---- batch: 070 ----
mean loss: 262.54
 ---- batch: 080 ----
mean loss: 253.20
 ---- batch: 090 ----
mean loss: 257.67
train mean loss: 255.48
epoch train time: 0:00:02.667235
elapsed time: 0:04:53.210028
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 19:03:27.675782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.52
 ---- batch: 020 ----
mean loss: 249.59
 ---- batch: 030 ----
mean loss: 250.83
 ---- batch: 040 ----
mean loss: 255.45
 ---- batch: 050 ----
mean loss: 251.90
 ---- batch: 060 ----
mean loss: 248.49
 ---- batch: 070 ----
mean loss: 257.41
 ---- batch: 080 ----
mean loss: 258.58
 ---- batch: 090 ----
mean loss: 262.67
train mean loss: 254.25
epoch train time: 0:00:02.695925
elapsed time: 0:04:55.906412
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 19:03:30.372201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.58
 ---- batch: 020 ----
mean loss: 259.32
 ---- batch: 030 ----
mean loss: 251.47
 ---- batch: 040 ----
mean loss: 256.88
 ---- batch: 050 ----
mean loss: 246.92
 ---- batch: 060 ----
mean loss: 257.75
 ---- batch: 070 ----
mean loss: 258.22
 ---- batch: 080 ----
mean loss: 253.62
 ---- batch: 090 ----
mean loss: 251.57
train mean loss: 254.03
epoch train time: 0:00:02.687528
elapsed time: 0:04:58.594406
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 19:03:33.060225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.59
 ---- batch: 020 ----
mean loss: 245.07
 ---- batch: 030 ----
mean loss: 250.15
 ---- batch: 040 ----
mean loss: 254.23
 ---- batch: 050 ----
mean loss: 257.51
 ---- batch: 060 ----
mean loss: 254.84
 ---- batch: 070 ----
mean loss: 254.28
 ---- batch: 080 ----
mean loss: 251.78
 ---- batch: 090 ----
mean loss: 254.37
train mean loss: 253.41
epoch train time: 0:00:02.659849
elapsed time: 0:05:01.254799
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 19:03:35.720561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.33
 ---- batch: 020 ----
mean loss: 248.91
 ---- batch: 030 ----
mean loss: 249.97
 ---- batch: 040 ----
mean loss: 251.17
 ---- batch: 050 ----
mean loss: 263.93
 ---- batch: 060 ----
mean loss: 246.48
 ---- batch: 070 ----
mean loss: 246.35
 ---- batch: 080 ----
mean loss: 253.08
 ---- batch: 090 ----
mean loss: 251.05
train mean loss: 252.93
epoch train time: 0:00:02.681555
elapsed time: 0:05:03.936790
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 19:03:38.402560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.03
 ---- batch: 020 ----
mean loss: 255.89
 ---- batch: 030 ----
mean loss: 250.77
 ---- batch: 040 ----
mean loss: 251.85
 ---- batch: 050 ----
mean loss: 259.00
 ---- batch: 060 ----
mean loss: 259.47
 ---- batch: 070 ----
mean loss: 250.31
 ---- batch: 080 ----
mean loss: 246.58
 ---- batch: 090 ----
mean loss: 248.03
train mean loss: 252.32
epoch train time: 0:00:02.678603
elapsed time: 0:05:06.615846
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 19:03:41.081638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.40
 ---- batch: 020 ----
mean loss: 249.74
 ---- batch: 030 ----
mean loss: 248.05
 ---- batch: 040 ----
mean loss: 250.36
 ---- batch: 050 ----
mean loss: 249.30
 ---- batch: 060 ----
mean loss: 257.30
 ---- batch: 070 ----
mean loss: 250.96
 ---- batch: 080 ----
mean loss: 253.18
 ---- batch: 090 ----
mean loss: 259.24
train mean loss: 252.29
epoch train time: 0:00:02.686660
elapsed time: 0:05:09.303003
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 19:03:43.768759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.19
 ---- batch: 020 ----
mean loss: 255.43
 ---- batch: 030 ----
mean loss: 249.12
 ---- batch: 040 ----
mean loss: 250.66
 ---- batch: 050 ----
mean loss: 245.42
 ---- batch: 060 ----
mean loss: 252.00
 ---- batch: 070 ----
mean loss: 259.05
 ---- batch: 080 ----
mean loss: 244.55
 ---- batch: 090 ----
mean loss: 250.43
train mean loss: 251.65
epoch train time: 0:00:02.694616
elapsed time: 0:05:11.998127
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 19:03:46.463883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.21
 ---- batch: 020 ----
mean loss: 252.22
 ---- batch: 030 ----
mean loss: 249.39
 ---- batch: 040 ----
mean loss: 249.31
 ---- batch: 050 ----
mean loss: 250.38
 ---- batch: 060 ----
mean loss: 257.95
 ---- batch: 070 ----
mean loss: 242.88
 ---- batch: 080 ----
mean loss: 249.54
 ---- batch: 090 ----
mean loss: 247.36
train mean loss: 251.03
epoch train time: 0:00:02.702910
elapsed time: 0:05:14.701493
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 19:03:49.167258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.12
 ---- batch: 020 ----
mean loss: 249.50
 ---- batch: 030 ----
mean loss: 259.99
 ---- batch: 040 ----
mean loss: 248.53
 ---- batch: 050 ----
mean loss: 241.32
 ---- batch: 060 ----
mean loss: 254.79
 ---- batch: 070 ----
mean loss: 247.28
 ---- batch: 080 ----
mean loss: 250.02
 ---- batch: 090 ----
mean loss: 254.21
train mean loss: 250.58
epoch train time: 0:00:02.654104
elapsed time: 0:05:17.356128
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 19:03:51.821893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.63
 ---- batch: 020 ----
mean loss: 257.14
 ---- batch: 030 ----
mean loss: 249.82
 ---- batch: 040 ----
mean loss: 239.95
 ---- batch: 050 ----
mean loss: 254.85
 ---- batch: 060 ----
mean loss: 252.06
 ---- batch: 070 ----
mean loss: 245.89
 ---- batch: 080 ----
mean loss: 247.58
 ---- batch: 090 ----
mean loss: 247.76
train mean loss: 249.73
epoch train time: 0:00:02.655604
elapsed time: 0:05:20.012246
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 19:03:54.477973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.79
 ---- batch: 020 ----
mean loss: 251.03
 ---- batch: 030 ----
mean loss: 249.71
 ---- batch: 040 ----
mean loss: 250.72
 ---- batch: 050 ----
mean loss: 246.38
 ---- batch: 060 ----
mean loss: 254.65
 ---- batch: 070 ----
mean loss: 252.37
 ---- batch: 080 ----
mean loss: 254.72
 ---- batch: 090 ----
mean loss: 246.79
train mean loss: 249.23
epoch train time: 0:00:02.684180
elapsed time: 0:05:22.696843
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 19:03:57.162628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.84
 ---- batch: 020 ----
mean loss: 247.72
 ---- batch: 030 ----
mean loss: 246.19
 ---- batch: 040 ----
mean loss: 248.63
 ---- batch: 050 ----
mean loss: 241.12
 ---- batch: 060 ----
mean loss: 248.71
 ---- batch: 070 ----
mean loss: 248.79
 ---- batch: 080 ----
mean loss: 249.10
 ---- batch: 090 ----
mean loss: 249.08
train mean loss: 248.76
epoch train time: 0:00:02.680176
elapsed time: 0:05:25.377614
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 19:03:59.843375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.11
 ---- batch: 020 ----
mean loss: 245.50
 ---- batch: 030 ----
mean loss: 250.19
 ---- batch: 040 ----
mean loss: 244.70
 ---- batch: 050 ----
mean loss: 248.60
 ---- batch: 060 ----
mean loss: 253.49
 ---- batch: 070 ----
mean loss: 252.33
 ---- batch: 080 ----
mean loss: 244.13
 ---- batch: 090 ----
mean loss: 243.64
train mean loss: 249.05
epoch train time: 0:00:02.715663
elapsed time: 0:05:28.093789
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 19:04:02.559594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.07
 ---- batch: 020 ----
mean loss: 245.96
 ---- batch: 030 ----
mean loss: 254.00
 ---- batch: 040 ----
mean loss: 244.43
 ---- batch: 050 ----
mean loss: 245.97
 ---- batch: 060 ----
mean loss: 256.76
 ---- batch: 070 ----
mean loss: 245.79
 ---- batch: 080 ----
mean loss: 245.47
 ---- batch: 090 ----
mean loss: 246.66
train mean loss: 248.46
epoch train time: 0:00:02.693493
elapsed time: 0:05:30.787828
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 19:04:05.253639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.15
 ---- batch: 020 ----
mean loss: 247.12
 ---- batch: 030 ----
mean loss: 248.74
 ---- batch: 040 ----
mean loss: 256.47
 ---- batch: 050 ----
mean loss: 251.49
 ---- batch: 060 ----
mean loss: 254.63
 ---- batch: 070 ----
mean loss: 254.30
 ---- batch: 080 ----
mean loss: 244.00
 ---- batch: 090 ----
mean loss: 240.26
train mean loss: 247.68
epoch train time: 0:00:02.653008
elapsed time: 0:05:33.441399
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 19:04:07.907160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.49
 ---- batch: 020 ----
mean loss: 248.62
 ---- batch: 030 ----
mean loss: 239.74
 ---- batch: 040 ----
mean loss: 239.80
 ---- batch: 050 ----
mean loss: 242.91
 ---- batch: 060 ----
mean loss: 256.12
 ---- batch: 070 ----
mean loss: 260.42
 ---- batch: 080 ----
mean loss: 247.61
 ---- batch: 090 ----
mean loss: 248.54
train mean loss: 247.81
epoch train time: 0:00:02.679638
elapsed time: 0:05:36.121490
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 19:04:10.587256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.40
 ---- batch: 020 ----
mean loss: 239.98
 ---- batch: 030 ----
mean loss: 245.39
 ---- batch: 040 ----
mean loss: 249.08
 ---- batch: 050 ----
mean loss: 244.74
 ---- batch: 060 ----
mean loss: 255.85
 ---- batch: 070 ----
mean loss: 237.63
 ---- batch: 080 ----
mean loss: 249.09
 ---- batch: 090 ----
mean loss: 253.46
train mean loss: 247.33
epoch train time: 0:00:02.677932
elapsed time: 0:05:38.799880
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 19:04:13.265687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.07
 ---- batch: 020 ----
mean loss: 246.16
 ---- batch: 030 ----
mean loss: 240.94
 ---- batch: 040 ----
mean loss: 248.72
 ---- batch: 050 ----
mean loss: 243.19
 ---- batch: 060 ----
mean loss: 247.95
 ---- batch: 070 ----
mean loss: 247.64
 ---- batch: 080 ----
mean loss: 239.15
 ---- batch: 090 ----
mean loss: 246.55
train mean loss: 246.47
epoch train time: 0:00:02.686340
elapsed time: 0:05:41.486730
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 19:04:15.952525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.16
 ---- batch: 020 ----
mean loss: 246.60
 ---- batch: 030 ----
mean loss: 239.57
 ---- batch: 040 ----
mean loss: 235.82
 ---- batch: 050 ----
mean loss: 258.79
 ---- batch: 060 ----
mean loss: 247.58
 ---- batch: 070 ----
mean loss: 256.35
 ---- batch: 080 ----
mean loss: 243.10
 ---- batch: 090 ----
mean loss: 240.93
train mean loss: 246.37
epoch train time: 0:00:02.659397
elapsed time: 0:05:44.146645
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 19:04:18.612471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.05
 ---- batch: 020 ----
mean loss: 235.62
 ---- batch: 030 ----
mean loss: 245.90
 ---- batch: 040 ----
mean loss: 243.18
 ---- batch: 050 ----
mean loss: 246.98
 ---- batch: 060 ----
mean loss: 252.57
 ---- batch: 070 ----
mean loss: 255.99
 ---- batch: 080 ----
mean loss: 254.18
 ---- batch: 090 ----
mean loss: 252.92
train mean loss: 245.39
epoch train time: 0:00:02.654966
elapsed time: 0:05:46.802151
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 19:04:21.267905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.27
 ---- batch: 020 ----
mean loss: 238.00
 ---- batch: 030 ----
mean loss: 244.87
 ---- batch: 040 ----
mean loss: 245.93
 ---- batch: 050 ----
mean loss: 244.16
 ---- batch: 060 ----
mean loss: 240.74
 ---- batch: 070 ----
mean loss: 249.11
 ---- batch: 080 ----
mean loss: 250.18
 ---- batch: 090 ----
mean loss: 252.46
train mean loss: 245.66
epoch train time: 0:00:02.684083
elapsed time: 0:05:49.486694
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 19:04:23.952448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.05
 ---- batch: 020 ----
mean loss: 249.01
 ---- batch: 030 ----
mean loss: 248.16
 ---- batch: 040 ----
mean loss: 250.49
 ---- batch: 050 ----
mean loss: 249.81
 ---- batch: 060 ----
mean loss: 245.48
 ---- batch: 070 ----
mean loss: 243.48
 ---- batch: 080 ----
mean loss: 246.09
 ---- batch: 090 ----
mean loss: 232.10
train mean loss: 245.46
epoch train time: 0:00:02.678717
elapsed time: 0:05:52.165863
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 19:04:26.631634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.62
 ---- batch: 020 ----
mean loss: 243.86
 ---- batch: 030 ----
mean loss: 241.52
 ---- batch: 040 ----
mean loss: 243.21
 ---- batch: 050 ----
mean loss: 234.84
 ---- batch: 060 ----
mean loss: 250.46
 ---- batch: 070 ----
mean loss: 243.13
 ---- batch: 080 ----
mean loss: 254.57
 ---- batch: 090 ----
mean loss: 244.09
train mean loss: 244.99
epoch train time: 0:00:02.690942
elapsed time: 0:05:54.857444
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 19:04:29.323254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.55
 ---- batch: 020 ----
mean loss: 241.37
 ---- batch: 030 ----
mean loss: 243.55
 ---- batch: 040 ----
mean loss: 252.38
 ---- batch: 050 ----
mean loss: 242.42
 ---- batch: 060 ----
mean loss: 247.00
 ---- batch: 070 ----
mean loss: 240.99
 ---- batch: 080 ----
mean loss: 247.24
 ---- batch: 090 ----
mean loss: 245.13
train mean loss: 244.21
epoch train time: 0:00:02.710839
elapsed time: 0:05:57.568819
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 19:04:32.034571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.28
 ---- batch: 020 ----
mean loss: 245.95
 ---- batch: 030 ----
mean loss: 240.67
 ---- batch: 040 ----
mean loss: 242.55
 ---- batch: 050 ----
mean loss: 245.40
 ---- batch: 060 ----
mean loss: 243.36
 ---- batch: 070 ----
mean loss: 244.60
 ---- batch: 080 ----
mean loss: 253.32
 ---- batch: 090 ----
mean loss: 246.06
train mean loss: 243.84
epoch train time: 0:00:02.663140
elapsed time: 0:06:00.232414
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 19:04:34.698222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.18
 ---- batch: 020 ----
mean loss: 245.97
 ---- batch: 030 ----
mean loss: 239.48
 ---- batch: 040 ----
mean loss: 249.18
 ---- batch: 050 ----
mean loss: 244.97
 ---- batch: 060 ----
mean loss: 242.91
 ---- batch: 070 ----
mean loss: 234.86
 ---- batch: 080 ----
mean loss: 241.27
 ---- batch: 090 ----
mean loss: 242.58
train mean loss: 243.43
epoch train time: 0:00:02.679094
elapsed time: 0:06:02.912075
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 19:04:37.377877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.68
 ---- batch: 020 ----
mean loss: 234.99
 ---- batch: 030 ----
mean loss: 241.12
 ---- batch: 040 ----
mean loss: 242.52
 ---- batch: 050 ----
mean loss: 243.42
 ---- batch: 060 ----
mean loss: 243.07
 ---- batch: 070 ----
mean loss: 246.57
 ---- batch: 080 ----
mean loss: 247.25
 ---- batch: 090 ----
mean loss: 243.13
train mean loss: 243.26
epoch train time: 0:00:02.676688
elapsed time: 0:06:05.589246
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 19:04:40.055000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.01
 ---- batch: 020 ----
mean loss: 236.73
 ---- batch: 030 ----
mean loss: 238.76
 ---- batch: 040 ----
mean loss: 245.20
 ---- batch: 050 ----
mean loss: 242.03
 ---- batch: 060 ----
mean loss: 250.47
 ---- batch: 070 ----
mean loss: 244.11
 ---- batch: 080 ----
mean loss: 244.89
 ---- batch: 090 ----
mean loss: 247.18
train mean loss: 242.72
epoch train time: 0:00:02.652932
elapsed time: 0:06:08.242648
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 19:04:42.708423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.64
 ---- batch: 020 ----
mean loss: 240.33
 ---- batch: 030 ----
mean loss: 230.15
 ---- batch: 040 ----
mean loss: 235.62
 ---- batch: 050 ----
mean loss: 246.43
 ---- batch: 060 ----
mean loss: 245.92
 ---- batch: 070 ----
mean loss: 243.44
 ---- batch: 080 ----
mean loss: 251.97
 ---- batch: 090 ----
mean loss: 250.70
train mean loss: 242.35
epoch train time: 0:00:02.662215
elapsed time: 0:06:10.905327
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 19:04:45.371090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.85
 ---- batch: 020 ----
mean loss: 235.67
 ---- batch: 030 ----
mean loss: 247.20
 ---- batch: 040 ----
mean loss: 245.14
 ---- batch: 050 ----
mean loss: 244.68
 ---- batch: 060 ----
mean loss: 236.72
 ---- batch: 070 ----
mean loss: 240.77
 ---- batch: 080 ----
mean loss: 248.63
 ---- batch: 090 ----
mean loss: 246.96
train mean loss: 241.84
epoch train time: 0:00:02.671475
elapsed time: 0:06:13.577309
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 19:04:48.043012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.75
 ---- batch: 020 ----
mean loss: 233.78
 ---- batch: 030 ----
mean loss: 239.17
 ---- batch: 040 ----
mean loss: 248.99
 ---- batch: 050 ----
mean loss: 243.54
 ---- batch: 060 ----
mean loss: 245.25
 ---- batch: 070 ----
mean loss: 230.65
 ---- batch: 080 ----
mean loss: 241.71
 ---- batch: 090 ----
mean loss: 251.08
train mean loss: 242.05
epoch train time: 0:00:02.700823
elapsed time: 0:06:16.278583
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 19:04:50.744421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.48
 ---- batch: 020 ----
mean loss: 231.23
 ---- batch: 030 ----
mean loss: 238.53
 ---- batch: 040 ----
mean loss: 240.12
 ---- batch: 050 ----
mean loss: 237.55
 ---- batch: 060 ----
mean loss: 238.33
 ---- batch: 070 ----
mean loss: 240.45
 ---- batch: 080 ----
mean loss: 247.33
 ---- batch: 090 ----
mean loss: 248.46
train mean loss: 240.88
epoch train time: 0:00:02.684137
elapsed time: 0:06:18.963300
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 19:04:53.429076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.92
 ---- batch: 020 ----
mean loss: 241.20
 ---- batch: 030 ----
mean loss: 233.24
 ---- batch: 040 ----
mean loss: 237.59
 ---- batch: 050 ----
mean loss: 236.06
 ---- batch: 060 ----
mean loss: 232.99
 ---- batch: 070 ----
mean loss: 246.24
 ---- batch: 080 ----
mean loss: 251.20
 ---- batch: 090 ----
mean loss: 242.55
train mean loss: 241.04
epoch train time: 0:00:02.664720
elapsed time: 0:06:21.628467
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 19:04:56.094216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.12
 ---- batch: 020 ----
mean loss: 244.09
 ---- batch: 030 ----
mean loss: 237.94
 ---- batch: 040 ----
mean loss: 247.46
 ---- batch: 050 ----
mean loss: 240.16
 ---- batch: 060 ----
mean loss: 239.39
 ---- batch: 070 ----
mean loss: 240.64
 ---- batch: 080 ----
mean loss: 238.76
 ---- batch: 090 ----
mean loss: 238.41
train mean loss: 240.70
epoch train time: 0:00:02.680394
elapsed time: 0:06:24.309313
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 19:04:58.775066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.06
 ---- batch: 020 ----
mean loss: 236.37
 ---- batch: 030 ----
mean loss: 238.01
 ---- batch: 040 ----
mean loss: 241.01
 ---- batch: 050 ----
mean loss: 240.62
 ---- batch: 060 ----
mean loss: 234.54
 ---- batch: 070 ----
mean loss: 241.10
 ---- batch: 080 ----
mean loss: 249.70
 ---- batch: 090 ----
mean loss: 238.35
train mean loss: 240.50
epoch train time: 0:00:02.671849
elapsed time: 0:06:26.981690
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 19:05:01.447448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.42
 ---- batch: 020 ----
mean loss: 241.06
 ---- batch: 030 ----
mean loss: 233.88
 ---- batch: 040 ----
mean loss: 230.86
 ---- batch: 050 ----
mean loss: 239.38
 ---- batch: 060 ----
mean loss: 245.32
 ---- batch: 070 ----
mean loss: 238.39
 ---- batch: 080 ----
mean loss: 246.30
 ---- batch: 090 ----
mean loss: 238.11
train mean loss: 240.00
epoch train time: 0:00:02.680831
elapsed time: 0:06:29.663043
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 19:05:04.128808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.56
 ---- batch: 020 ----
mean loss: 224.82
 ---- batch: 030 ----
mean loss: 234.08
 ---- batch: 040 ----
mean loss: 241.78
 ---- batch: 050 ----
mean loss: 246.06
 ---- batch: 060 ----
mean loss: 234.40
 ---- batch: 070 ----
mean loss: 248.90
 ---- batch: 080 ----
mean loss: 244.94
 ---- batch: 090 ----
mean loss: 239.54
train mean loss: 240.04
epoch train time: 0:00:02.694634
elapsed time: 0:06:32.358181
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 19:05:06.823937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.24
 ---- batch: 020 ----
mean loss: 239.20
 ---- batch: 030 ----
mean loss: 240.59
 ---- batch: 040 ----
mean loss: 237.43
 ---- batch: 050 ----
mean loss: 233.45
 ---- batch: 060 ----
mean loss: 248.79
 ---- batch: 070 ----
mean loss: 243.12
 ---- batch: 080 ----
mean loss: 235.99
 ---- batch: 090 ----
mean loss: 237.22
train mean loss: 239.50
epoch train time: 0:00:02.673103
elapsed time: 0:06:35.031753
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 19:05:09.497510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.54
 ---- batch: 020 ----
mean loss: 238.64
 ---- batch: 030 ----
mean loss: 232.29
 ---- batch: 040 ----
mean loss: 243.25
 ---- batch: 050 ----
mean loss: 234.48
 ---- batch: 060 ----
mean loss: 239.22
 ---- batch: 070 ----
mean loss: 238.55
 ---- batch: 080 ----
mean loss: 239.01
 ---- batch: 090 ----
mean loss: 240.63
train mean loss: 238.97
epoch train time: 0:00:02.666596
elapsed time: 0:06:37.698804
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 19:05:12.164561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.53
 ---- batch: 020 ----
mean loss: 238.03
 ---- batch: 030 ----
mean loss: 240.92
 ---- batch: 040 ----
mean loss: 234.14
 ---- batch: 050 ----
mean loss: 235.41
 ---- batch: 060 ----
mean loss: 234.95
 ---- batch: 070 ----
mean loss: 234.16
 ---- batch: 080 ----
mean loss: 241.10
 ---- batch: 090 ----
mean loss: 243.93
train mean loss: 238.61
epoch train time: 0:00:02.695755
elapsed time: 0:06:40.395046
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 19:05:14.860898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.53
 ---- batch: 020 ----
mean loss: 239.17
 ---- batch: 030 ----
mean loss: 239.51
 ---- batch: 040 ----
mean loss: 242.95
 ---- batch: 050 ----
mean loss: 237.52
 ---- batch: 060 ----
mean loss: 243.97
 ---- batch: 070 ----
mean loss: 242.42
 ---- batch: 080 ----
mean loss: 237.80
 ---- batch: 090 ----
mean loss: 231.12
train mean loss: 238.45
epoch train time: 0:00:02.665578
elapsed time: 0:06:43.061175
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 19:05:17.526928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.38
 ---- batch: 020 ----
mean loss: 238.24
 ---- batch: 030 ----
mean loss: 241.10
 ---- batch: 040 ----
mean loss: 235.38
 ---- batch: 050 ----
mean loss: 244.44
 ---- batch: 060 ----
mean loss: 232.33
 ---- batch: 070 ----
mean loss: 234.97
 ---- batch: 080 ----
mean loss: 240.14
 ---- batch: 090 ----
mean loss: 243.30
train mean loss: 238.12
epoch train time: 0:00:02.684221
elapsed time: 0:06:45.745914
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 19:05:20.211695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.73
 ---- batch: 020 ----
mean loss: 234.90
 ---- batch: 030 ----
mean loss: 241.62
 ---- batch: 040 ----
mean loss: 231.03
 ---- batch: 050 ----
mean loss: 234.87
 ---- batch: 060 ----
mean loss: 238.36
 ---- batch: 070 ----
mean loss: 240.15
 ---- batch: 080 ----
mean loss: 248.22
 ---- batch: 090 ----
mean loss: 233.98
train mean loss: 237.17
epoch train time: 0:00:02.701126
elapsed time: 0:06:48.447561
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 19:05:22.913318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.23
 ---- batch: 020 ----
mean loss: 237.59
 ---- batch: 030 ----
mean loss: 236.32
 ---- batch: 040 ----
mean loss: 239.16
 ---- batch: 050 ----
mean loss: 238.44
 ---- batch: 060 ----
mean loss: 237.49
 ---- batch: 070 ----
mean loss: 227.71
 ---- batch: 080 ----
mean loss: 236.89
 ---- batch: 090 ----
mean loss: 245.98
train mean loss: 237.55
epoch train time: 0:00:02.680169
elapsed time: 0:06:51.128212
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 19:05:25.594016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.58
 ---- batch: 020 ----
mean loss: 237.58
 ---- batch: 030 ----
mean loss: 240.12
 ---- batch: 040 ----
mean loss: 229.62
 ---- batch: 050 ----
mean loss: 241.59
 ---- batch: 060 ----
mean loss: 236.47
 ---- batch: 070 ----
mean loss: 237.46
 ---- batch: 080 ----
mean loss: 232.21
 ---- batch: 090 ----
mean loss: 236.21
train mean loss: 237.04
epoch train time: 0:00:02.674640
elapsed time: 0:06:53.803386
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 19:05:28.269193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.34
 ---- batch: 020 ----
mean loss: 235.84
 ---- batch: 030 ----
mean loss: 239.23
 ---- batch: 040 ----
mean loss: 234.97
 ---- batch: 050 ----
mean loss: 240.70
 ---- batch: 060 ----
mean loss: 238.57
 ---- batch: 070 ----
mean loss: 234.12
 ---- batch: 080 ----
mean loss: 234.24
 ---- batch: 090 ----
mean loss: 235.02
train mean loss: 236.52
epoch train time: 0:00:02.651148
elapsed time: 0:06:56.455080
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 19:05:30.920831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.16
 ---- batch: 020 ----
mean loss: 239.66
 ---- batch: 030 ----
mean loss: 228.33
 ---- batch: 040 ----
mean loss: 241.09
 ---- batch: 050 ----
mean loss: 232.51
 ---- batch: 060 ----
mean loss: 229.15
 ---- batch: 070 ----
mean loss: 237.48
 ---- batch: 080 ----
mean loss: 234.86
 ---- batch: 090 ----
mean loss: 229.57
train mean loss: 236.20
epoch train time: 0:00:02.669993
elapsed time: 0:06:59.125528
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 19:05:33.591303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.74
 ---- batch: 020 ----
mean loss: 244.91
 ---- batch: 030 ----
mean loss: 234.36
 ---- batch: 040 ----
mean loss: 235.16
 ---- batch: 050 ----
mean loss: 225.50
 ---- batch: 060 ----
mean loss: 243.89
 ---- batch: 070 ----
mean loss: 239.34
 ---- batch: 080 ----
mean loss: 235.18
 ---- batch: 090 ----
mean loss: 236.90
train mean loss: 235.87
epoch train time: 0:00:02.685805
elapsed time: 0:07:01.811801
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 19:05:36.277557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.14
 ---- batch: 020 ----
mean loss: 238.86
 ---- batch: 030 ----
mean loss: 230.03
 ---- batch: 040 ----
mean loss: 236.47
 ---- batch: 050 ----
mean loss: 236.73
 ---- batch: 060 ----
mean loss: 242.26
 ---- batch: 070 ----
mean loss: 228.62
 ---- batch: 080 ----
mean loss: 241.48
 ---- batch: 090 ----
mean loss: 236.53
train mean loss: 235.41
epoch train time: 0:00:02.685604
elapsed time: 0:07:04.497882
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 19:05:38.963656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.43
 ---- batch: 020 ----
mean loss: 238.23
 ---- batch: 030 ----
mean loss: 236.35
 ---- batch: 040 ----
mean loss: 234.99
 ---- batch: 050 ----
mean loss: 231.82
 ---- batch: 060 ----
mean loss: 235.75
 ---- batch: 070 ----
mean loss: 233.48
 ---- batch: 080 ----
mean loss: 232.96
 ---- batch: 090 ----
mean loss: 235.01
train mean loss: 235.80
epoch train time: 0:00:02.689758
elapsed time: 0:07:07.188185
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 19:05:41.653994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.11
 ---- batch: 020 ----
mean loss: 242.15
 ---- batch: 030 ----
mean loss: 235.83
 ---- batch: 040 ----
mean loss: 235.02
 ---- batch: 050 ----
mean loss: 233.91
 ---- batch: 060 ----
mean loss: 235.90
 ---- batch: 070 ----
mean loss: 224.96
 ---- batch: 080 ----
mean loss: 240.57
 ---- batch: 090 ----
mean loss: 238.08
train mean loss: 235.30
epoch train time: 0:00:02.682740
elapsed time: 0:07:09.871473
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 19:05:44.337252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.17
 ---- batch: 020 ----
mean loss: 232.06
 ---- batch: 030 ----
mean loss: 234.51
 ---- batch: 040 ----
mean loss: 239.58
 ---- batch: 050 ----
mean loss: 229.46
 ---- batch: 060 ----
mean loss: 231.30
 ---- batch: 070 ----
mean loss: 234.46
 ---- batch: 080 ----
mean loss: 238.10
 ---- batch: 090 ----
mean loss: 235.16
train mean loss: 234.71
epoch train time: 0:00:02.665233
elapsed time: 0:07:12.537334
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 19:05:47.002974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.86
 ---- batch: 020 ----
mean loss: 232.31
 ---- batch: 030 ----
mean loss: 230.05
 ---- batch: 040 ----
mean loss: 233.11
 ---- batch: 050 ----
mean loss: 237.82
 ---- batch: 060 ----
mean loss: 242.10
 ---- batch: 070 ----
mean loss: 236.89
 ---- batch: 080 ----
mean loss: 228.94
 ---- batch: 090 ----
mean loss: 241.00
train mean loss: 234.52
epoch train time: 0:00:02.674473
elapsed time: 0:07:15.212193
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 19:05:49.677956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.92
 ---- batch: 020 ----
mean loss: 241.08
 ---- batch: 030 ----
mean loss: 231.05
 ---- batch: 040 ----
mean loss: 238.24
 ---- batch: 050 ----
mean loss: 226.24
 ---- batch: 060 ----
mean loss: 234.19
 ---- batch: 070 ----
mean loss: 226.44
 ---- batch: 080 ----
mean loss: 239.39
 ---- batch: 090 ----
mean loss: 229.20
train mean loss: 234.24
epoch train time: 0:00:02.661267
elapsed time: 0:07:17.873925
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 19:05:52.339676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.30
 ---- batch: 020 ----
mean loss: 237.16
 ---- batch: 030 ----
mean loss: 234.44
 ---- batch: 040 ----
mean loss: 231.84
 ---- batch: 050 ----
mean loss: 230.35
 ---- batch: 060 ----
mean loss: 235.67
 ---- batch: 070 ----
mean loss: 230.01
 ---- batch: 080 ----
mean loss: 234.49
 ---- batch: 090 ----
mean loss: 245.41
train mean loss: 234.04
epoch train time: 0:00:02.685831
elapsed time: 0:07:20.560231
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 19:05:55.026015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.61
 ---- batch: 020 ----
mean loss: 240.41
 ---- batch: 030 ----
mean loss: 235.98
 ---- batch: 040 ----
mean loss: 227.98
 ---- batch: 050 ----
mean loss: 236.88
 ---- batch: 060 ----
mean loss: 228.70
 ---- batch: 070 ----
mean loss: 234.79
 ---- batch: 080 ----
mean loss: 231.69
 ---- batch: 090 ----
mean loss: 236.71
train mean loss: 233.52
epoch train time: 0:00:02.659333
elapsed time: 0:07:23.220112
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 19:05:57.685897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.17
 ---- batch: 020 ----
mean loss: 223.34
 ---- batch: 030 ----
mean loss: 240.27
 ---- batch: 040 ----
mean loss: 243.63
 ---- batch: 050 ----
mean loss: 235.72
 ---- batch: 060 ----
mean loss: 229.99
 ---- batch: 070 ----
mean loss: 230.33
 ---- batch: 080 ----
mean loss: 228.43
 ---- batch: 090 ----
mean loss: 229.92
train mean loss: 233.65
epoch train time: 0:00:02.686643
elapsed time: 0:07:25.907224
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 19:06:00.372980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.67
 ---- batch: 020 ----
mean loss: 230.60
 ---- batch: 030 ----
mean loss: 226.50
 ---- batch: 040 ----
mean loss: 238.29
 ---- batch: 050 ----
mean loss: 232.65
 ---- batch: 060 ----
mean loss: 238.27
 ---- batch: 070 ----
mean loss: 229.45
 ---- batch: 080 ----
mean loss: 236.27
 ---- batch: 090 ----
mean loss: 231.09
train mean loss: 233.01
epoch train time: 0:00:02.701208
elapsed time: 0:07:28.608954
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 19:06:03.074714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.64
 ---- batch: 020 ----
mean loss: 237.28
 ---- batch: 030 ----
mean loss: 233.76
 ---- batch: 040 ----
mean loss: 225.50
 ---- batch: 050 ----
mean loss: 233.52
 ---- batch: 060 ----
mean loss: 241.97
 ---- batch: 070 ----
mean loss: 230.80
 ---- batch: 080 ----
mean loss: 230.22
 ---- batch: 090 ----
mean loss: 236.87
train mean loss: 233.17
epoch train time: 0:00:02.691738
elapsed time: 0:07:31.301218
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 19:06:05.767010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.93
 ---- batch: 020 ----
mean loss: 226.08
 ---- batch: 030 ----
mean loss: 235.58
 ---- batch: 040 ----
mean loss: 238.08
 ---- batch: 050 ----
mean loss: 226.00
 ---- batch: 060 ----
mean loss: 234.07
 ---- batch: 070 ----
mean loss: 239.43
 ---- batch: 080 ----
mean loss: 245.89
 ---- batch: 090 ----
mean loss: 225.16
train mean loss: 232.69
epoch train time: 0:00:02.661858
elapsed time: 0:07:33.963585
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 19:06:08.429340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.28
 ---- batch: 020 ----
mean loss: 240.52
 ---- batch: 030 ----
mean loss: 230.52
 ---- batch: 040 ----
mean loss: 232.57
 ---- batch: 050 ----
mean loss: 237.12
 ---- batch: 060 ----
mean loss: 230.67
 ---- batch: 070 ----
mean loss: 230.21
 ---- batch: 080 ----
mean loss: 228.87
 ---- batch: 090 ----
mean loss: 232.38
train mean loss: 232.51
epoch train time: 0:00:02.675718
elapsed time: 0:07:36.639766
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 19:06:11.105537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.18
 ---- batch: 020 ----
mean loss: 224.62
 ---- batch: 030 ----
mean loss: 234.36
 ---- batch: 040 ----
mean loss: 231.50
 ---- batch: 050 ----
mean loss: 227.58
 ---- batch: 060 ----
mean loss: 230.78
 ---- batch: 070 ----
mean loss: 231.32
 ---- batch: 080 ----
mean loss: 238.66
 ---- batch: 090 ----
mean loss: 226.40
train mean loss: 232.56
epoch train time: 0:00:02.690930
elapsed time: 0:07:39.331221
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 19:06:13.797027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.63
 ---- batch: 020 ----
mean loss: 225.87
 ---- batch: 030 ----
mean loss: 238.19
 ---- batch: 040 ----
mean loss: 236.12
 ---- batch: 050 ----
mean loss: 236.52
 ---- batch: 060 ----
mean loss: 228.72
 ---- batch: 070 ----
mean loss: 230.25
 ---- batch: 080 ----
mean loss: 226.88
 ---- batch: 090 ----
mean loss: 231.80
train mean loss: 231.87
epoch train time: 0:00:02.712478
elapsed time: 0:07:42.044249
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 19:06:16.510035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.28
 ---- batch: 020 ----
mean loss: 229.78
 ---- batch: 030 ----
mean loss: 225.57
 ---- batch: 040 ----
mean loss: 234.08
 ---- batch: 050 ----
mean loss: 229.17
 ---- batch: 060 ----
mean loss: 230.11
 ---- batch: 070 ----
mean loss: 235.22
 ---- batch: 080 ----
mean loss: 235.54
 ---- batch: 090 ----
mean loss: 233.78
train mean loss: 231.79
epoch train time: 0:00:02.670850
elapsed time: 0:07:44.715581
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 19:06:19.181341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.27
 ---- batch: 020 ----
mean loss: 226.35
 ---- batch: 030 ----
mean loss: 239.77
 ---- batch: 040 ----
mean loss: 226.40
 ---- batch: 050 ----
mean loss: 223.50
 ---- batch: 060 ----
mean loss: 236.85
 ---- batch: 070 ----
mean loss: 239.24
 ---- batch: 080 ----
mean loss: 226.88
 ---- batch: 090 ----
mean loss: 229.99
train mean loss: 231.33
epoch train time: 0:00:02.691582
elapsed time: 0:07:47.407805
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 19:06:21.873637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.62
 ---- batch: 020 ----
mean loss: 234.95
 ---- batch: 030 ----
mean loss: 243.50
 ---- batch: 040 ----
mean loss: 231.16
 ---- batch: 050 ----
mean loss: 224.32
 ---- batch: 060 ----
mean loss: 237.53
 ---- batch: 070 ----
mean loss: 229.36
 ---- batch: 080 ----
mean loss: 232.56
 ---- batch: 090 ----
mean loss: 227.46
train mean loss: 231.02
epoch train time: 0:00:02.706565
elapsed time: 0:07:50.114886
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 19:06:24.580648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.46
 ---- batch: 020 ----
mean loss: 224.01
 ---- batch: 030 ----
mean loss: 232.97
 ---- batch: 040 ----
mean loss: 232.46
 ---- batch: 050 ----
mean loss: 231.55
 ---- batch: 060 ----
mean loss: 232.11
 ---- batch: 070 ----
mean loss: 234.62
 ---- batch: 080 ----
mean loss: 227.04
 ---- batch: 090 ----
mean loss: 232.27
train mean loss: 230.97
epoch train time: 0:00:02.673534
elapsed time: 0:07:52.788873
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 19:06:27.254630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.24
 ---- batch: 020 ----
mean loss: 228.40
 ---- batch: 030 ----
mean loss: 230.79
 ---- batch: 040 ----
mean loss: 224.21
 ---- batch: 050 ----
mean loss: 238.81
 ---- batch: 060 ----
mean loss: 234.99
 ---- batch: 070 ----
mean loss: 230.11
 ---- batch: 080 ----
mean loss: 223.41
 ---- batch: 090 ----
mean loss: 237.20
train mean loss: 230.68
epoch train time: 0:00:02.670490
elapsed time: 0:07:55.459841
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 19:06:29.925600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.08
 ---- batch: 020 ----
mean loss: 229.53
 ---- batch: 030 ----
mean loss: 219.92
 ---- batch: 040 ----
mean loss: 238.42
 ---- batch: 050 ----
mean loss: 233.83
 ---- batch: 060 ----
mean loss: 241.39
 ---- batch: 070 ----
mean loss: 222.48
 ---- batch: 080 ----
mean loss: 220.06
 ---- batch: 090 ----
mean loss: 233.97
train mean loss: 230.38
epoch train time: 0:00:02.675732
elapsed time: 0:07:58.136105
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 19:06:32.601876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.57
 ---- batch: 020 ----
mean loss: 230.23
 ---- batch: 030 ----
mean loss: 233.99
 ---- batch: 040 ----
mean loss: 226.58
 ---- batch: 050 ----
mean loss: 230.17
 ---- batch: 060 ----
mean loss: 223.30
 ---- batch: 070 ----
mean loss: 222.72
 ---- batch: 080 ----
mean loss: 231.72
 ---- batch: 090 ----
mean loss: 237.15
train mean loss: 230.28
epoch train time: 0:00:02.686810
elapsed time: 0:08:00.823355
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 19:06:35.289146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.10
 ---- batch: 020 ----
mean loss: 228.85
 ---- batch: 030 ----
mean loss: 235.71
 ---- batch: 040 ----
mean loss: 222.28
 ---- batch: 050 ----
mean loss: 231.33
 ---- batch: 060 ----
mean loss: 225.90
 ---- batch: 070 ----
mean loss: 232.65
 ---- batch: 080 ----
mean loss: 227.99
 ---- batch: 090 ----
mean loss: 229.95
train mean loss: 229.99
epoch train time: 0:00:02.668836
elapsed time: 0:08:03.492689
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 19:06:37.958455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.93
 ---- batch: 020 ----
mean loss: 229.48
 ---- batch: 030 ----
mean loss: 220.06
 ---- batch: 040 ----
mean loss: 225.97
 ---- batch: 050 ----
mean loss: 229.39
 ---- batch: 060 ----
mean loss: 236.71
 ---- batch: 070 ----
mean loss: 225.76
 ---- batch: 080 ----
mean loss: 235.78
 ---- batch: 090 ----
mean loss: 230.18
train mean loss: 229.93
epoch train time: 0:00:02.699091
elapsed time: 0:08:06.192238
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 19:06:40.658038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.21
 ---- batch: 020 ----
mean loss: 225.21
 ---- batch: 030 ----
mean loss: 225.50
 ---- batch: 040 ----
mean loss: 234.14
 ---- batch: 050 ----
mean loss: 225.05
 ---- batch: 060 ----
mean loss: 233.17
 ---- batch: 070 ----
mean loss: 236.77
 ---- batch: 080 ----
mean loss: 233.49
 ---- batch: 090 ----
mean loss: 224.41
train mean loss: 229.50
epoch train time: 0:00:02.683415
elapsed time: 0:08:08.876192
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 19:06:43.341971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.76
 ---- batch: 020 ----
mean loss: 228.37
 ---- batch: 030 ----
mean loss: 227.84
 ---- batch: 040 ----
mean loss: 230.69
 ---- batch: 050 ----
mean loss: 235.67
 ---- batch: 060 ----
mean loss: 228.92
 ---- batch: 070 ----
mean loss: 232.99
 ---- batch: 080 ----
mean loss: 218.63
 ---- batch: 090 ----
mean loss: 225.61
train mean loss: 228.91
epoch train time: 0:00:02.670822
elapsed time: 0:08:11.547474
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 19:06:46.013297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.14
 ---- batch: 020 ----
mean loss: 223.85
 ---- batch: 030 ----
mean loss: 219.08
 ---- batch: 040 ----
mean loss: 230.71
 ---- batch: 050 ----
mean loss: 229.19
 ---- batch: 060 ----
mean loss: 231.35
 ---- batch: 070 ----
mean loss: 237.44
 ---- batch: 080 ----
mean loss: 225.88
 ---- batch: 090 ----
mean loss: 231.91
train mean loss: 229.14
epoch train time: 0:00:02.670897
elapsed time: 0:08:14.218921
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 19:06:48.684672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.48
 ---- batch: 020 ----
mean loss: 217.35
 ---- batch: 030 ----
mean loss: 232.52
 ---- batch: 040 ----
mean loss: 235.59
 ---- batch: 050 ----
mean loss: 226.09
 ---- batch: 060 ----
mean loss: 219.72
 ---- batch: 070 ----
mean loss: 227.32
 ---- batch: 080 ----
mean loss: 233.51
 ---- batch: 090 ----
mean loss: 226.72
train mean loss: 228.72
epoch train time: 0:00:02.676950
elapsed time: 0:08:16.896356
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 19:06:51.362107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.89
 ---- batch: 020 ----
mean loss: 222.48
 ---- batch: 030 ----
mean loss: 236.15
 ---- batch: 040 ----
mean loss: 222.28
 ---- batch: 050 ----
mean loss: 223.83
 ---- batch: 060 ----
mean loss: 234.34
 ---- batch: 070 ----
mean loss: 228.42
 ---- batch: 080 ----
mean loss: 231.53
 ---- batch: 090 ----
mean loss: 227.01
train mean loss: 228.50
epoch train time: 0:00:02.665379
elapsed time: 0:08:19.562270
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 19:06:54.027980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.18
 ---- batch: 020 ----
mean loss: 230.48
 ---- batch: 030 ----
mean loss: 239.77
 ---- batch: 040 ----
mean loss: 221.02
 ---- batch: 050 ----
mean loss: 224.05
 ---- batch: 060 ----
mean loss: 231.48
 ---- batch: 070 ----
mean loss: 219.76
 ---- batch: 080 ----
mean loss: 230.74
 ---- batch: 090 ----
mean loss: 230.00
train mean loss: 228.05
epoch train time: 0:00:02.659372
elapsed time: 0:08:22.222066
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 19:06:56.687816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.53
 ---- batch: 020 ----
mean loss: 220.31
 ---- batch: 030 ----
mean loss: 228.37
 ---- batch: 040 ----
mean loss: 231.37
 ---- batch: 050 ----
mean loss: 229.50
 ---- batch: 060 ----
mean loss: 229.02
 ---- batch: 070 ----
mean loss: 231.76
 ---- batch: 080 ----
mean loss: 223.48
 ---- batch: 090 ----
mean loss: 225.98
train mean loss: 227.86
epoch train time: 0:00:02.651823
elapsed time: 0:08:24.874314
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 19:06:59.340116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.60
 ---- batch: 020 ----
mean loss: 227.42
 ---- batch: 030 ----
mean loss: 227.63
 ---- batch: 040 ----
mean loss: 228.06
 ---- batch: 050 ----
mean loss: 234.19
 ---- batch: 060 ----
mean loss: 226.52
 ---- batch: 070 ----
mean loss: 223.51
 ---- batch: 080 ----
mean loss: 219.67
 ---- batch: 090 ----
mean loss: 235.00
train mean loss: 227.69
epoch train time: 0:00:02.668738
elapsed time: 0:08:27.543599
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 19:07:02.009391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.83
 ---- batch: 020 ----
mean loss: 233.90
 ---- batch: 030 ----
mean loss: 225.59
 ---- batch: 040 ----
mean loss: 223.40
 ---- batch: 050 ----
mean loss: 218.75
 ---- batch: 060 ----
mean loss: 227.65
 ---- batch: 070 ----
mean loss: 231.30
 ---- batch: 080 ----
mean loss: 227.34
 ---- batch: 090 ----
mean loss: 223.66
train mean loss: 227.12
epoch train time: 0:00:02.696168
elapsed time: 0:08:30.240306
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 19:07:04.706060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.18
 ---- batch: 020 ----
mean loss: 229.90
 ---- batch: 030 ----
mean loss: 229.43
 ---- batch: 040 ----
mean loss: 231.85
 ---- batch: 050 ----
mean loss: 217.69
 ---- batch: 060 ----
mean loss: 232.12
 ---- batch: 070 ----
mean loss: 221.31
 ---- batch: 080 ----
mean loss: 219.74
 ---- batch: 090 ----
mean loss: 227.43
train mean loss: 227.22
epoch train time: 0:00:02.677018
elapsed time: 0:08:32.917778
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 19:07:07.383542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.44
 ---- batch: 020 ----
mean loss: 228.76
 ---- batch: 030 ----
mean loss: 230.32
 ---- batch: 040 ----
mean loss: 227.62
 ---- batch: 050 ----
mean loss: 231.06
 ---- batch: 060 ----
mean loss: 227.88
 ---- batch: 070 ----
mean loss: 219.55
 ---- batch: 080 ----
mean loss: 225.38
 ---- batch: 090 ----
mean loss: 224.07
train mean loss: 227.17
epoch train time: 0:00:02.670510
elapsed time: 0:08:35.588816
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 19:07:10.054672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.06
 ---- batch: 020 ----
mean loss: 224.95
 ---- batch: 030 ----
mean loss: 225.93
 ---- batch: 040 ----
mean loss: 236.41
 ---- batch: 050 ----
mean loss: 222.75
 ---- batch: 060 ----
mean loss: 223.37
 ---- batch: 070 ----
mean loss: 222.74
 ---- batch: 080 ----
mean loss: 228.88
 ---- batch: 090 ----
mean loss: 224.10
train mean loss: 227.10
epoch train time: 0:00:02.673701
elapsed time: 0:08:38.263074
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 19:07:12.728827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.31
 ---- batch: 020 ----
mean loss: 231.60
 ---- batch: 030 ----
mean loss: 226.04
 ---- batch: 040 ----
mean loss: 229.11
 ---- batch: 050 ----
mean loss: 219.18
 ---- batch: 060 ----
mean loss: 224.38
 ---- batch: 070 ----
mean loss: 228.92
 ---- batch: 080 ----
mean loss: 227.57
 ---- batch: 090 ----
mean loss: 224.90
train mean loss: 226.64
epoch train time: 0:00:02.685419
elapsed time: 0:08:40.948938
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 19:07:15.414712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.72
 ---- batch: 020 ----
mean loss: 224.96
 ---- batch: 030 ----
mean loss: 234.74
 ---- batch: 040 ----
mean loss: 226.82
 ---- batch: 050 ----
mean loss: 223.31
 ---- batch: 060 ----
mean loss: 229.51
 ---- batch: 070 ----
mean loss: 225.62
 ---- batch: 080 ----
mean loss: 228.20
 ---- batch: 090 ----
mean loss: 214.22
train mean loss: 226.12
epoch train time: 0:00:02.698364
elapsed time: 0:08:43.647810
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 19:07:18.113637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.54
 ---- batch: 020 ----
mean loss: 226.83
 ---- batch: 030 ----
mean loss: 223.17
 ---- batch: 040 ----
mean loss: 232.53
 ---- batch: 050 ----
mean loss: 227.89
 ---- batch: 060 ----
mean loss: 225.08
 ---- batch: 070 ----
mean loss: 225.44
 ---- batch: 080 ----
mean loss: 227.67
 ---- batch: 090 ----
mean loss: 222.52
train mean loss: 226.19
epoch train time: 0:00:02.707944
elapsed time: 0:08:46.356345
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 19:07:20.822109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.46
 ---- batch: 020 ----
mean loss: 225.45
 ---- batch: 030 ----
mean loss: 225.52
 ---- batch: 040 ----
mean loss: 228.14
 ---- batch: 050 ----
mean loss: 230.88
 ---- batch: 060 ----
mean loss: 233.00
 ---- batch: 070 ----
mean loss: 222.67
 ---- batch: 080 ----
mean loss: 218.30
 ---- batch: 090 ----
mean loss: 222.54
train mean loss: 226.18
epoch train time: 0:00:02.675910
elapsed time: 0:08:49.032762
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 19:07:23.498507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.63
 ---- batch: 020 ----
mean loss: 218.71
 ---- batch: 030 ----
mean loss: 223.18
 ---- batch: 040 ----
mean loss: 232.97
 ---- batch: 050 ----
mean loss: 238.41
 ---- batch: 060 ----
mean loss: 225.72
 ---- batch: 070 ----
mean loss: 223.70
 ---- batch: 080 ----
mean loss: 224.68
 ---- batch: 090 ----
mean loss: 226.83
train mean loss: 225.71
epoch train time: 0:00:02.683928
elapsed time: 0:08:51.717101
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 19:07:26.182858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.35
 ---- batch: 020 ----
mean loss: 222.15
 ---- batch: 030 ----
mean loss: 228.47
 ---- batch: 040 ----
mean loss: 227.42
 ---- batch: 050 ----
mean loss: 219.78
 ---- batch: 060 ----
mean loss: 226.11
 ---- batch: 070 ----
mean loss: 229.81
 ---- batch: 080 ----
mean loss: 222.96
 ---- batch: 090 ----
mean loss: 229.52
train mean loss: 225.53
epoch train time: 0:00:02.655727
elapsed time: 0:08:54.373342
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 19:07:28.839105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.28
 ---- batch: 020 ----
mean loss: 227.94
 ---- batch: 030 ----
mean loss: 223.17
 ---- batch: 040 ----
mean loss: 222.61
 ---- batch: 050 ----
mean loss: 229.94
 ---- batch: 060 ----
mean loss: 225.72
 ---- batch: 070 ----
mean loss: 222.66
 ---- batch: 080 ----
mean loss: 225.15
 ---- batch: 090 ----
mean loss: 228.47
train mean loss: 225.07
epoch train time: 0:00:02.705117
elapsed time: 0:08:57.078928
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 19:07:31.544711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.17
 ---- batch: 020 ----
mean loss: 228.27
 ---- batch: 030 ----
mean loss: 219.48
 ---- batch: 040 ----
mean loss: 221.04
 ---- batch: 050 ----
mean loss: 229.60
 ---- batch: 060 ----
mean loss: 222.66
 ---- batch: 070 ----
mean loss: 228.28
 ---- batch: 080 ----
mean loss: 228.13
 ---- batch: 090 ----
mean loss: 224.93
train mean loss: 225.37
epoch train time: 0:00:02.673041
elapsed time: 0:08:59.752515
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 19:07:34.218272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.71
 ---- batch: 020 ----
mean loss: 222.70
 ---- batch: 030 ----
mean loss: 234.46
 ---- batch: 040 ----
mean loss: 218.24
 ---- batch: 050 ----
mean loss: 223.78
 ---- batch: 060 ----
mean loss: 215.68
 ---- batch: 070 ----
mean loss: 218.04
 ---- batch: 080 ----
mean loss: 230.09
 ---- batch: 090 ----
mean loss: 231.81
train mean loss: 224.73
epoch train time: 0:00:02.689033
elapsed time: 0:09:02.442047
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 19:07:36.907830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.23
 ---- batch: 020 ----
mean loss: 223.81
 ---- batch: 030 ----
mean loss: 227.28
 ---- batch: 040 ----
mean loss: 229.04
 ---- batch: 050 ----
mean loss: 230.19
 ---- batch: 060 ----
mean loss: 221.79
 ---- batch: 070 ----
mean loss: 221.92
 ---- batch: 080 ----
mean loss: 232.34
 ---- batch: 090 ----
mean loss: 215.51
train mean loss: 224.73
epoch train time: 0:00:02.678345
elapsed time: 0:09:05.120893
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 19:07:39.586633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.24
 ---- batch: 020 ----
mean loss: 227.23
 ---- batch: 030 ----
mean loss: 225.89
 ---- batch: 040 ----
mean loss: 218.85
 ---- batch: 050 ----
mean loss: 228.74
 ---- batch: 060 ----
mean loss: 223.73
 ---- batch: 070 ----
mean loss: 225.34
 ---- batch: 080 ----
mean loss: 221.25
 ---- batch: 090 ----
mean loss: 221.69
train mean loss: 224.37
epoch train time: 0:00:02.660128
elapsed time: 0:09:07.781480
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 19:07:42.247243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.21
 ---- batch: 020 ----
mean loss: 226.34
 ---- batch: 030 ----
mean loss: 218.79
 ---- batch: 040 ----
mean loss: 226.57
 ---- batch: 050 ----
mean loss: 222.93
 ---- batch: 060 ----
mean loss: 226.35
 ---- batch: 070 ----
mean loss: 217.37
 ---- batch: 080 ----
mean loss: 224.15
 ---- batch: 090 ----
mean loss: 223.55
train mean loss: 223.81
epoch train time: 0:00:02.667295
elapsed time: 0:09:10.449301
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 19:07:44.915060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.79
 ---- batch: 020 ----
mean loss: 217.22
 ---- batch: 030 ----
mean loss: 224.34
 ---- batch: 040 ----
mean loss: 224.59
 ---- batch: 050 ----
mean loss: 222.45
 ---- batch: 060 ----
mean loss: 211.40
 ---- batch: 070 ----
mean loss: 229.19
 ---- batch: 080 ----
mean loss: 230.87
 ---- batch: 090 ----
mean loss: 228.12
train mean loss: 224.02
epoch train time: 0:00:02.649403
elapsed time: 0:09:13.099171
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 19:07:47.564928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.51
 ---- batch: 020 ----
mean loss: 220.66
 ---- batch: 030 ----
mean loss: 222.87
 ---- batch: 040 ----
mean loss: 224.56
 ---- batch: 050 ----
mean loss: 220.99
 ---- batch: 060 ----
mean loss: 226.29
 ---- batch: 070 ----
mean loss: 228.88
 ---- batch: 080 ----
mean loss: 222.38
 ---- batch: 090 ----
mean loss: 223.12
train mean loss: 224.05
epoch train time: 0:00:02.675250
elapsed time: 0:09:15.774924
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 19:07:50.240712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.21
 ---- batch: 020 ----
mean loss: 221.11
 ---- batch: 030 ----
mean loss: 230.82
 ---- batch: 040 ----
mean loss: 222.46
 ---- batch: 050 ----
mean loss: 221.80
 ---- batch: 060 ----
mean loss: 229.00
 ---- batch: 070 ----
mean loss: 225.82
 ---- batch: 080 ----
mean loss: 222.73
 ---- batch: 090 ----
mean loss: 219.01
train mean loss: 223.83
epoch train time: 0:00:02.683192
elapsed time: 0:09:18.458752
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 19:07:52.924525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.95
 ---- batch: 020 ----
mean loss: 223.58
 ---- batch: 030 ----
mean loss: 219.85
 ---- batch: 040 ----
mean loss: 226.95
 ---- batch: 050 ----
mean loss: 220.81
 ---- batch: 060 ----
mean loss: 219.80
 ---- batch: 070 ----
mean loss: 223.29
 ---- batch: 080 ----
mean loss: 218.04
 ---- batch: 090 ----
mean loss: 231.06
train mean loss: 223.29
epoch train time: 0:00:02.667303
elapsed time: 0:09:21.126557
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 19:07:55.592331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.02
 ---- batch: 020 ----
mean loss: 227.88
 ---- batch: 030 ----
mean loss: 223.84
 ---- batch: 040 ----
mean loss: 218.96
 ---- batch: 050 ----
mean loss: 223.37
 ---- batch: 060 ----
mean loss: 227.65
 ---- batch: 070 ----
mean loss: 218.36
 ---- batch: 080 ----
mean loss: 216.99
 ---- batch: 090 ----
mean loss: 232.33
train mean loss: 222.98
epoch train time: 0:00:02.657897
elapsed time: 0:09:23.784908
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 19:07:58.250665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.84
 ---- batch: 020 ----
mean loss: 218.31
 ---- batch: 030 ----
mean loss: 221.37
 ---- batch: 040 ----
mean loss: 228.42
 ---- batch: 050 ----
mean loss: 231.09
 ---- batch: 060 ----
mean loss: 224.11
 ---- batch: 070 ----
mean loss: 225.07
 ---- batch: 080 ----
mean loss: 227.64
 ---- batch: 090 ----
mean loss: 214.84
train mean loss: 223.10
epoch train time: 0:00:02.661369
elapsed time: 0:09:26.446786
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 19:08:00.912570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.97
 ---- batch: 020 ----
mean loss: 213.00
 ---- batch: 030 ----
mean loss: 226.54
 ---- batch: 040 ----
mean loss: 222.47
 ---- batch: 050 ----
mean loss: 226.59
 ---- batch: 060 ----
mean loss: 225.18
 ---- batch: 070 ----
mean loss: 218.44
 ---- batch: 080 ----
mean loss: 229.19
 ---- batch: 090 ----
mean loss: 217.27
train mean loss: 222.44
epoch train time: 0:00:02.643233
elapsed time: 0:09:29.090523
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 19:08:03.556283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.07
 ---- batch: 020 ----
mean loss: 224.30
 ---- batch: 030 ----
mean loss: 215.32
 ---- batch: 040 ----
mean loss: 224.31
 ---- batch: 050 ----
mean loss: 220.94
 ---- batch: 060 ----
mean loss: 217.42
 ---- batch: 070 ----
mean loss: 224.46
 ---- batch: 080 ----
mean loss: 227.79
 ---- batch: 090 ----
mean loss: 224.44
train mean loss: 222.67
epoch train time: 0:00:02.735956
elapsed time: 0:09:31.826930
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 19:08:06.292686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.81
 ---- batch: 020 ----
mean loss: 218.86
 ---- batch: 030 ----
mean loss: 218.40
 ---- batch: 040 ----
mean loss: 232.47
 ---- batch: 050 ----
mean loss: 222.71
 ---- batch: 060 ----
mean loss: 212.54
 ---- batch: 070 ----
mean loss: 222.73
 ---- batch: 080 ----
mean loss: 223.97
 ---- batch: 090 ----
mean loss: 225.12
train mean loss: 222.18
epoch train time: 0:00:02.704062
elapsed time: 0:09:34.531550
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 19:08:08.997201
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.07
 ---- batch: 020 ----
mean loss: 219.17
 ---- batch: 030 ----
mean loss: 226.02
 ---- batch: 040 ----
mean loss: 216.96
 ---- batch: 050 ----
mean loss: 226.97
 ---- batch: 060 ----
mean loss: 217.35
 ---- batch: 070 ----
mean loss: 219.45
 ---- batch: 080 ----
mean loss: 222.99
 ---- batch: 090 ----
mean loss: 218.60
train mean loss: 222.04
epoch train time: 0:00:02.672033
elapsed time: 0:09:37.203958
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 19:08:11.669751
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.04
 ---- batch: 020 ----
mean loss: 223.89
 ---- batch: 030 ----
mean loss: 214.05
 ---- batch: 040 ----
mean loss: 221.69
 ---- batch: 050 ----
mean loss: 222.35
 ---- batch: 060 ----
mean loss: 216.82
 ---- batch: 070 ----
mean loss: 232.75
 ---- batch: 080 ----
mean loss: 222.48
 ---- batch: 090 ----
mean loss: 221.65
train mean loss: 221.90
epoch train time: 0:00:02.688710
elapsed time: 0:09:39.893165
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 19:08:14.358925
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.69
 ---- batch: 020 ----
mean loss: 223.80
 ---- batch: 030 ----
mean loss: 224.92
 ---- batch: 040 ----
mean loss: 221.82
 ---- batch: 050 ----
mean loss: 219.65
 ---- batch: 060 ----
mean loss: 225.05
 ---- batch: 070 ----
mean loss: 218.85
 ---- batch: 080 ----
mean loss: 231.33
 ---- batch: 090 ----
mean loss: 217.31
train mean loss: 221.76
epoch train time: 0:00:02.703558
elapsed time: 0:09:42.597214
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 19:08:17.062986
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.17
 ---- batch: 020 ----
mean loss: 222.63
 ---- batch: 030 ----
mean loss: 231.98
 ---- batch: 040 ----
mean loss: 210.49
 ---- batch: 050 ----
mean loss: 223.07
 ---- batch: 060 ----
mean loss: 220.99
 ---- batch: 070 ----
mean loss: 221.09
 ---- batch: 080 ----
mean loss: 233.11
 ---- batch: 090 ----
mean loss: 220.62
train mean loss: 221.80
epoch train time: 0:00:02.709410
elapsed time: 0:09:45.307185
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 19:08:19.773023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.42
 ---- batch: 020 ----
mean loss: 217.47
 ---- batch: 030 ----
mean loss: 218.69
 ---- batch: 040 ----
mean loss: 231.80
 ---- batch: 050 ----
mean loss: 218.84
 ---- batch: 060 ----
mean loss: 221.54
 ---- batch: 070 ----
mean loss: 225.43
 ---- batch: 080 ----
mean loss: 227.36
 ---- batch: 090 ----
mean loss: 219.63
train mean loss: 222.00
epoch train time: 0:00:02.725647
elapsed time: 0:09:48.033414
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 19:08:22.499174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.91
 ---- batch: 020 ----
mean loss: 217.56
 ---- batch: 030 ----
mean loss: 219.91
 ---- batch: 040 ----
mean loss: 229.71
 ---- batch: 050 ----
mean loss: 221.47
 ---- batch: 060 ----
mean loss: 212.80
 ---- batch: 070 ----
mean loss: 224.88
 ---- batch: 080 ----
mean loss: 221.40
 ---- batch: 090 ----
mean loss: 224.81
train mean loss: 221.98
epoch train time: 0:00:02.684279
elapsed time: 0:09:50.718122
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 19:08:25.183951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.40
 ---- batch: 020 ----
mean loss: 224.83
 ---- batch: 030 ----
mean loss: 218.36
 ---- batch: 040 ----
mean loss: 234.58
 ---- batch: 050 ----
mean loss: 225.74
 ---- batch: 060 ----
mean loss: 217.28
 ---- batch: 070 ----
mean loss: 219.28
 ---- batch: 080 ----
mean loss: 222.22
 ---- batch: 090 ----
mean loss: 217.23
train mean loss: 221.73
epoch train time: 0:00:02.670739
elapsed time: 0:09:53.389451
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 19:08:27.855231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.63
 ---- batch: 020 ----
mean loss: 225.21
 ---- batch: 030 ----
mean loss: 221.47
 ---- batch: 040 ----
mean loss: 217.91
 ---- batch: 050 ----
mean loss: 226.16
 ---- batch: 060 ----
mean loss: 227.41
 ---- batch: 070 ----
mean loss: 221.84
 ---- batch: 080 ----
mean loss: 220.34
 ---- batch: 090 ----
mean loss: 218.29
train mean loss: 221.90
epoch train time: 0:00:02.662353
elapsed time: 0:09:56.052322
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 19:08:30.518101
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.60
 ---- batch: 020 ----
mean loss: 224.47
 ---- batch: 030 ----
mean loss: 225.03
 ---- batch: 040 ----
mean loss: 218.32
 ---- batch: 050 ----
mean loss: 214.91
 ---- batch: 060 ----
mean loss: 232.06
 ---- batch: 070 ----
mean loss: 218.99
 ---- batch: 080 ----
mean loss: 222.35
 ---- batch: 090 ----
mean loss: 217.72
train mean loss: 221.59
epoch train time: 0:00:02.656206
elapsed time: 0:09:58.709005
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 19:08:33.174766
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.25
 ---- batch: 020 ----
mean loss: 221.60
 ---- batch: 030 ----
mean loss: 222.40
 ---- batch: 040 ----
mean loss: 221.64
 ---- batch: 050 ----
mean loss: 218.86
 ---- batch: 060 ----
mean loss: 221.29
 ---- batch: 070 ----
mean loss: 226.39
 ---- batch: 080 ----
mean loss: 220.64
 ---- batch: 090 ----
mean loss: 221.39
train mean loss: 222.09
epoch train time: 0:00:02.677082
elapsed time: 0:10:01.386723
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 19:08:35.852509
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.78
 ---- batch: 020 ----
mean loss: 226.44
 ---- batch: 030 ----
mean loss: 218.80
 ---- batch: 040 ----
mean loss: 226.36
 ---- batch: 050 ----
mean loss: 217.33
 ---- batch: 060 ----
mean loss: 213.09
 ---- batch: 070 ----
mean loss: 225.60
 ---- batch: 080 ----
mean loss: 222.72
 ---- batch: 090 ----
mean loss: 222.86
train mean loss: 221.90
epoch train time: 0:00:02.672144
elapsed time: 0:10:04.059422
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 19:08:38.525182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.52
 ---- batch: 020 ----
mean loss: 223.58
 ---- batch: 030 ----
mean loss: 220.52
 ---- batch: 040 ----
mean loss: 223.97
 ---- batch: 050 ----
mean loss: 228.21
 ---- batch: 060 ----
mean loss: 223.77
 ---- batch: 070 ----
mean loss: 215.59
 ---- batch: 080 ----
mean loss: 219.92
 ---- batch: 090 ----
mean loss: 219.12
train mean loss: 221.55
epoch train time: 0:00:02.666671
elapsed time: 0:10:06.726605
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 19:08:41.192373
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.99
 ---- batch: 020 ----
mean loss: 207.05
 ---- batch: 030 ----
mean loss: 217.04
 ---- batch: 040 ----
mean loss: 219.15
 ---- batch: 050 ----
mean loss: 221.59
 ---- batch: 060 ----
mean loss: 230.29
 ---- batch: 070 ----
mean loss: 228.05
 ---- batch: 080 ----
mean loss: 231.15
 ---- batch: 090 ----
mean loss: 224.49
train mean loss: 222.13
epoch train time: 0:00:02.672556
elapsed time: 0:10:09.399676
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 19:08:43.865454
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.33
 ---- batch: 020 ----
mean loss: 222.28
 ---- batch: 030 ----
mean loss: 218.59
 ---- batch: 040 ----
mean loss: 220.35
 ---- batch: 050 ----
mean loss: 220.17
 ---- batch: 060 ----
mean loss: 229.79
 ---- batch: 070 ----
mean loss: 217.44
 ---- batch: 080 ----
mean loss: 222.12
 ---- batch: 090 ----
mean loss: 217.56
train mean loss: 221.74
epoch train time: 0:00:02.673380
elapsed time: 0:10:12.073570
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 19:08:46.539342
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.68
 ---- batch: 020 ----
mean loss: 219.68
 ---- batch: 030 ----
mean loss: 223.29
 ---- batch: 040 ----
mean loss: 220.69
 ---- batch: 050 ----
mean loss: 221.85
 ---- batch: 060 ----
mean loss: 222.53
 ---- batch: 070 ----
mean loss: 216.53
 ---- batch: 080 ----
mean loss: 226.42
 ---- batch: 090 ----
mean loss: 219.57
train mean loss: 221.44
epoch train time: 0:00:02.688579
elapsed time: 0:10:14.762616
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 19:08:49.228397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.31
 ---- batch: 020 ----
mean loss: 222.86
 ---- batch: 030 ----
mean loss: 225.53
 ---- batch: 040 ----
mean loss: 217.37
 ---- batch: 050 ----
mean loss: 223.72
 ---- batch: 060 ----
mean loss: 217.52
 ---- batch: 070 ----
mean loss: 215.82
 ---- batch: 080 ----
mean loss: 232.08
 ---- batch: 090 ----
mean loss: 220.62
train mean loss: 221.77
epoch train time: 0:00:02.657842
elapsed time: 0:10:17.421012
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 19:08:51.886788
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.75
 ---- batch: 020 ----
mean loss: 225.51
 ---- batch: 030 ----
mean loss: 223.33
 ---- batch: 040 ----
mean loss: 222.23
 ---- batch: 050 ----
mean loss: 217.83
 ---- batch: 060 ----
mean loss: 221.86
 ---- batch: 070 ----
mean loss: 218.71
 ---- batch: 080 ----
mean loss: 225.95
 ---- batch: 090 ----
mean loss: 221.50
train mean loss: 221.80
epoch train time: 0:00:02.669037
elapsed time: 0:10:20.090597
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 19:08:54.556352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.59
 ---- batch: 020 ----
mean loss: 219.24
 ---- batch: 030 ----
mean loss: 225.76
 ---- batch: 040 ----
mean loss: 225.76
 ---- batch: 050 ----
mean loss: 230.55
 ---- batch: 060 ----
mean loss: 213.27
 ---- batch: 070 ----
mean loss: 216.80
 ---- batch: 080 ----
mean loss: 229.05
 ---- batch: 090 ----
mean loss: 223.45
train mean loss: 221.56
epoch train time: 0:00:02.668365
elapsed time: 0:10:22.759422
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 19:08:57.225199
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.67
 ---- batch: 020 ----
mean loss: 217.48
 ---- batch: 030 ----
mean loss: 223.49
 ---- batch: 040 ----
mean loss: 219.36
 ---- batch: 050 ----
mean loss: 224.90
 ---- batch: 060 ----
mean loss: 221.11
 ---- batch: 070 ----
mean loss: 222.21
 ---- batch: 080 ----
mean loss: 228.68
 ---- batch: 090 ----
mean loss: 218.70
train mean loss: 221.81
epoch train time: 0:00:02.670555
elapsed time: 0:10:25.430516
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 19:08:59.896355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.41
 ---- batch: 020 ----
mean loss: 217.65
 ---- batch: 030 ----
mean loss: 219.72
 ---- batch: 040 ----
mean loss: 223.23
 ---- batch: 050 ----
mean loss: 223.61
 ---- batch: 060 ----
mean loss: 229.08
 ---- batch: 070 ----
mean loss: 222.31
 ---- batch: 080 ----
mean loss: 229.93
 ---- batch: 090 ----
mean loss: 212.07
train mean loss: 221.49
epoch train time: 0:00:02.675263
elapsed time: 0:10:28.106305
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 19:09:02.572158
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.29
 ---- batch: 020 ----
mean loss: 220.51
 ---- batch: 030 ----
mean loss: 220.70
 ---- batch: 040 ----
mean loss: 218.90
 ---- batch: 050 ----
mean loss: 216.08
 ---- batch: 060 ----
mean loss: 221.75
 ---- batch: 070 ----
mean loss: 222.47
 ---- batch: 080 ----
mean loss: 232.50
 ---- batch: 090 ----
mean loss: 224.92
train mean loss: 221.50
epoch train time: 0:00:02.661269
elapsed time: 0:10:30.768132
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 19:09:05.233887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.73
 ---- batch: 020 ----
mean loss: 218.99
 ---- batch: 030 ----
mean loss: 226.09
 ---- batch: 040 ----
mean loss: 219.53
 ---- batch: 050 ----
mean loss: 221.99
 ---- batch: 060 ----
mean loss: 219.23
 ---- batch: 070 ----
mean loss: 223.34
 ---- batch: 080 ----
mean loss: 213.76
 ---- batch: 090 ----
mean loss: 230.12
train mean loss: 221.50
epoch train time: 0:00:02.674438
elapsed time: 0:10:33.443076
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 19:09:07.908826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.47
 ---- batch: 020 ----
mean loss: 215.24
 ---- batch: 030 ----
mean loss: 219.85
 ---- batch: 040 ----
mean loss: 221.65
 ---- batch: 050 ----
mean loss: 221.48
 ---- batch: 060 ----
mean loss: 219.23
 ---- batch: 070 ----
mean loss: 222.22
 ---- batch: 080 ----
mean loss: 232.69
 ---- batch: 090 ----
mean loss: 223.41
train mean loss: 221.40
epoch train time: 0:00:02.654248
elapsed time: 0:10:36.097778
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 19:09:10.563522
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.99
 ---- batch: 020 ----
mean loss: 226.58
 ---- batch: 030 ----
mean loss: 218.39
 ---- batch: 040 ----
mean loss: 220.77
 ---- batch: 050 ----
mean loss: 222.49
 ---- batch: 060 ----
mean loss: 219.51
 ---- batch: 070 ----
mean loss: 222.33
 ---- batch: 080 ----
mean loss: 217.53
 ---- batch: 090 ----
mean loss: 223.44
train mean loss: 221.51
epoch train time: 0:00:02.683405
elapsed time: 0:10:38.781602
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 19:09:13.247353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.95
 ---- batch: 020 ----
mean loss: 226.91
 ---- batch: 030 ----
mean loss: 223.22
 ---- batch: 040 ----
mean loss: 215.50
 ---- batch: 050 ----
mean loss: 215.63
 ---- batch: 060 ----
mean loss: 223.10
 ---- batch: 070 ----
mean loss: 215.48
 ---- batch: 080 ----
mean loss: 223.80
 ---- batch: 090 ----
mean loss: 228.49
train mean loss: 221.54
epoch train time: 0:00:02.664633
elapsed time: 0:10:41.446763
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 19:09:15.912566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.43
 ---- batch: 020 ----
mean loss: 226.12
 ---- batch: 030 ----
mean loss: 219.63
 ---- batch: 040 ----
mean loss: 225.52
 ---- batch: 050 ----
mean loss: 228.38
 ---- batch: 060 ----
mean loss: 216.10
 ---- batch: 070 ----
mean loss: 221.14
 ---- batch: 080 ----
mean loss: 224.30
 ---- batch: 090 ----
mean loss: 217.05
train mean loss: 221.52
epoch train time: 0:00:02.669913
elapsed time: 0:10:44.117197
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 19:09:18.582949
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 234.96
 ---- batch: 020 ----
mean loss: 212.93
 ---- batch: 030 ----
mean loss: 226.58
 ---- batch: 040 ----
mean loss: 220.58
 ---- batch: 050 ----
mean loss: 216.79
 ---- batch: 060 ----
mean loss: 221.28
 ---- batch: 070 ----
mean loss: 224.88
 ---- batch: 080 ----
mean loss: 216.82
 ---- batch: 090 ----
mean loss: 222.54
train mean loss: 221.38
epoch train time: 0:00:02.672517
elapsed time: 0:10:46.790173
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 19:09:21.255923
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.95
 ---- batch: 020 ----
mean loss: 222.24
 ---- batch: 030 ----
mean loss: 218.53
 ---- batch: 040 ----
mean loss: 229.63
 ---- batch: 050 ----
mean loss: 225.34
 ---- batch: 060 ----
mean loss: 216.92
 ---- batch: 070 ----
mean loss: 224.67
 ---- batch: 080 ----
mean loss: 214.85
 ---- batch: 090 ----
mean loss: 230.67
train mean loss: 221.25
epoch train time: 0:00:02.697375
elapsed time: 0:10:49.488013
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 19:09:23.953786
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.20
 ---- batch: 020 ----
mean loss: 224.71
 ---- batch: 030 ----
mean loss: 228.86
 ---- batch: 040 ----
mean loss: 218.42
 ---- batch: 050 ----
mean loss: 213.58
 ---- batch: 060 ----
mean loss: 214.64
 ---- batch: 070 ----
mean loss: 224.09
 ---- batch: 080 ----
mean loss: 226.03
 ---- batch: 090 ----
mean loss: 219.22
train mean loss: 221.40
epoch train time: 0:00:02.669455
elapsed time: 0:10:52.157932
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 19:09:26.623686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.66
 ---- batch: 020 ----
mean loss: 218.00
 ---- batch: 030 ----
mean loss: 220.20
 ---- batch: 040 ----
mean loss: 217.16
 ---- batch: 050 ----
mean loss: 219.32
 ---- batch: 060 ----
mean loss: 214.72
 ---- batch: 070 ----
mean loss: 218.04
 ---- batch: 080 ----
mean loss: 222.82
 ---- batch: 090 ----
mean loss: 228.65
train mean loss: 221.24
epoch train time: 0:00:02.688229
elapsed time: 0:10:54.846585
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 19:09:29.312329
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.79
 ---- batch: 020 ----
mean loss: 223.89
 ---- batch: 030 ----
mean loss: 226.20
 ---- batch: 040 ----
mean loss: 217.27
 ---- batch: 050 ----
mean loss: 214.51
 ---- batch: 060 ----
mean loss: 223.97
 ---- batch: 070 ----
mean loss: 219.64
 ---- batch: 080 ----
mean loss: 218.88
 ---- batch: 090 ----
mean loss: 225.00
train mean loss: 221.17
epoch train time: 0:00:02.668336
elapsed time: 0:10:57.515374
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 19:09:31.981133
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.00
 ---- batch: 020 ----
mean loss: 218.06
 ---- batch: 030 ----
mean loss: 216.25
 ---- batch: 040 ----
mean loss: 217.98
 ---- batch: 050 ----
mean loss: 223.40
 ---- batch: 060 ----
mean loss: 221.10
 ---- batch: 070 ----
mean loss: 215.44
 ---- batch: 080 ----
mean loss: 225.69
 ---- batch: 090 ----
mean loss: 221.12
train mean loss: 221.49
epoch train time: 0:00:02.666404
elapsed time: 0:11:00.182266
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 19:09:34.647896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.80
 ---- batch: 020 ----
mean loss: 220.58
 ---- batch: 030 ----
mean loss: 220.22
 ---- batch: 040 ----
mean loss: 227.60
 ---- batch: 050 ----
mean loss: 223.72
 ---- batch: 060 ----
mean loss: 221.62
 ---- batch: 070 ----
mean loss: 227.79
 ---- batch: 080 ----
mean loss: 217.47
 ---- batch: 090 ----
mean loss: 216.46
train mean loss: 221.29
epoch train time: 0:00:02.682069
elapsed time: 0:11:02.864636
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 19:09:37.330419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.76
 ---- batch: 020 ----
mean loss: 217.44
 ---- batch: 030 ----
mean loss: 218.18
 ---- batch: 040 ----
mean loss: 216.70
 ---- batch: 050 ----
mean loss: 227.35
 ---- batch: 060 ----
mean loss: 230.31
 ---- batch: 070 ----
mean loss: 221.79
 ---- batch: 080 ----
mean loss: 220.46
 ---- batch: 090 ----
mean loss: 219.05
train mean loss: 221.29
epoch train time: 0:00:02.658938
elapsed time: 0:11:05.524087
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 19:09:39.989848
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.67
 ---- batch: 020 ----
mean loss: 224.97
 ---- batch: 030 ----
mean loss: 222.72
 ---- batch: 040 ----
mean loss: 226.14
 ---- batch: 050 ----
mean loss: 222.01
 ---- batch: 060 ----
mean loss: 212.71
 ---- batch: 070 ----
mean loss: 229.20
 ---- batch: 080 ----
mean loss: 219.61
 ---- batch: 090 ----
mean loss: 220.32
train mean loss: 221.08
epoch train time: 0:00:02.680527
elapsed time: 0:11:08.205071
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 19:09:42.670822
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.31
 ---- batch: 020 ----
mean loss: 216.19
 ---- batch: 030 ----
mean loss: 227.25
 ---- batch: 040 ----
mean loss: 222.25
 ---- batch: 050 ----
mean loss: 219.45
 ---- batch: 060 ----
mean loss: 214.83
 ---- batch: 070 ----
mean loss: 221.40
 ---- batch: 080 ----
mean loss: 224.32
 ---- batch: 090 ----
mean loss: 220.61
train mean loss: 221.38
epoch train time: 0:00:02.667526
elapsed time: 0:11:10.873029
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 19:09:45.338783
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.82
 ---- batch: 020 ----
mean loss: 222.21
 ---- batch: 030 ----
mean loss: 224.29
 ---- batch: 040 ----
mean loss: 218.10
 ---- batch: 050 ----
mean loss: 222.31
 ---- batch: 060 ----
mean loss: 214.59
 ---- batch: 070 ----
mean loss: 219.93
 ---- batch: 080 ----
mean loss: 218.70
 ---- batch: 090 ----
mean loss: 224.91
train mean loss: 221.56
epoch train time: 0:00:02.675277
elapsed time: 0:11:13.548827
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 19:09:48.014640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.37
 ---- batch: 020 ----
mean loss: 223.65
 ---- batch: 030 ----
mean loss: 217.66
 ---- batch: 040 ----
mean loss: 214.00
 ---- batch: 050 ----
mean loss: 220.38
 ---- batch: 060 ----
mean loss: 228.29
 ---- batch: 070 ----
mean loss: 215.19
 ---- batch: 080 ----
mean loss: 224.45
 ---- batch: 090 ----
mean loss: 224.80
train mean loss: 221.25
epoch train time: 0:00:02.698917
elapsed time: 0:11:16.248352
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 19:09:50.714108
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.15
 ---- batch: 020 ----
mean loss: 218.52
 ---- batch: 030 ----
mean loss: 221.77
 ---- batch: 040 ----
mean loss: 219.09
 ---- batch: 050 ----
mean loss: 222.50
 ---- batch: 060 ----
mean loss: 215.94
 ---- batch: 070 ----
mean loss: 222.24
 ---- batch: 080 ----
mean loss: 224.78
 ---- batch: 090 ----
mean loss: 216.95
train mean loss: 221.42
epoch train time: 0:00:02.669186
elapsed time: 0:11:18.918061
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 19:09:53.383820
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.19
 ---- batch: 020 ----
mean loss: 218.92
 ---- batch: 030 ----
mean loss: 223.80
 ---- batch: 040 ----
mean loss: 214.81
 ---- batch: 050 ----
mean loss: 221.19
 ---- batch: 060 ----
mean loss: 213.76
 ---- batch: 070 ----
mean loss: 225.75
 ---- batch: 080 ----
mean loss: 217.45
 ---- batch: 090 ----
mean loss: 227.63
train mean loss: 221.33
epoch train time: 0:00:02.670817
elapsed time: 0:11:21.589312
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 19:09:56.055130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.33
 ---- batch: 020 ----
mean loss: 217.73
 ---- batch: 030 ----
mean loss: 226.70
 ---- batch: 040 ----
mean loss: 215.73
 ---- batch: 050 ----
mean loss: 219.24
 ---- batch: 060 ----
mean loss: 217.99
 ---- batch: 070 ----
mean loss: 220.72
 ---- batch: 080 ----
mean loss: 217.95
 ---- batch: 090 ----
mean loss: 230.19
train mean loss: 221.21
epoch train time: 0:00:02.660250
elapsed time: 0:11:24.250113
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 19:09:58.715889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.27
 ---- batch: 020 ----
mean loss: 222.71
 ---- batch: 030 ----
mean loss: 220.02
 ---- batch: 040 ----
mean loss: 215.56
 ---- batch: 050 ----
mean loss: 217.45
 ---- batch: 060 ----
mean loss: 227.70
 ---- batch: 070 ----
mean loss: 225.57
 ---- batch: 080 ----
mean loss: 212.53
 ---- batch: 090 ----
mean loss: 228.90
train mean loss: 221.18
epoch train time: 0:00:02.664443
elapsed time: 0:11:26.915031
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 19:10:01.380785
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.01
 ---- batch: 020 ----
mean loss: 229.58
 ---- batch: 030 ----
mean loss: 215.28
 ---- batch: 040 ----
mean loss: 218.69
 ---- batch: 050 ----
mean loss: 225.82
 ---- batch: 060 ----
mean loss: 214.32
 ---- batch: 070 ----
mean loss: 220.92
 ---- batch: 080 ----
mean loss: 218.34
 ---- batch: 090 ----
mean loss: 225.03
train mean loss: 220.96
epoch train time: 0:00:02.697084
elapsed time: 0:11:29.612598
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 19:10:04.078378
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.01
 ---- batch: 020 ----
mean loss: 220.19
 ---- batch: 030 ----
mean loss: 218.45
 ---- batch: 040 ----
mean loss: 223.88
 ---- batch: 050 ----
mean loss: 223.78
 ---- batch: 060 ----
mean loss: 221.19
 ---- batch: 070 ----
mean loss: 214.11
 ---- batch: 080 ----
mean loss: 227.83
 ---- batch: 090 ----
mean loss: 225.90
train mean loss: 220.91
epoch train time: 0:00:02.703398
elapsed time: 0:11:32.316511
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 19:10:06.782261
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.29
 ---- batch: 020 ----
mean loss: 221.92
 ---- batch: 030 ----
mean loss: 215.67
 ---- batch: 040 ----
mean loss: 226.17
 ---- batch: 050 ----
mean loss: 224.94
 ---- batch: 060 ----
mean loss: 211.17
 ---- batch: 070 ----
mean loss: 218.17
 ---- batch: 080 ----
mean loss: 217.59
 ---- batch: 090 ----
mean loss: 225.57
train mean loss: 221.14
epoch train time: 0:00:02.655005
elapsed time: 0:11:34.971983
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 19:10:09.437772
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.42
 ---- batch: 020 ----
mean loss: 218.77
 ---- batch: 030 ----
mean loss: 222.18
 ---- batch: 040 ----
mean loss: 227.43
 ---- batch: 050 ----
mean loss: 223.65
 ---- batch: 060 ----
mean loss: 226.03
 ---- batch: 070 ----
mean loss: 209.93
 ---- batch: 080 ----
mean loss: 214.47
 ---- batch: 090 ----
mean loss: 226.04
train mean loss: 220.90
epoch train time: 0:00:02.660014
elapsed time: 0:11:37.632490
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 19:10:12.098247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.71
 ---- batch: 020 ----
mean loss: 223.45
 ---- batch: 030 ----
mean loss: 218.80
 ---- batch: 040 ----
mean loss: 225.36
 ---- batch: 050 ----
mean loss: 215.31
 ---- batch: 060 ----
mean loss: 221.21
 ---- batch: 070 ----
mean loss: 229.38
 ---- batch: 080 ----
mean loss: 216.41
 ---- batch: 090 ----
mean loss: 217.17
train mean loss: 220.92
epoch train time: 0:00:02.673701
elapsed time: 0:11:40.306725
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 19:10:14.772479
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.59
 ---- batch: 020 ----
mean loss: 224.79
 ---- batch: 030 ----
mean loss: 217.67
 ---- batch: 040 ----
mean loss: 214.80
 ---- batch: 050 ----
mean loss: 230.66
 ---- batch: 060 ----
mean loss: 221.23
 ---- batch: 070 ----
mean loss: 216.50
 ---- batch: 080 ----
mean loss: 225.01
 ---- batch: 090 ----
mean loss: 227.06
train mean loss: 220.78
epoch train time: 0:00:02.675139
elapsed time: 0:11:42.986314
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_8/checkpoint.pth.tar
**** end time: 2019-09-25 19:10:17.451913 ****
