Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8127
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 04:27:24.023791 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 04:27:24.041626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2048.08
 ---- batch: 020 ----
mean loss: 1454.08
 ---- batch: 030 ----
mean loss: 1291.22
 ---- batch: 040 ----
mean loss: 1180.24
 ---- batch: 050 ----
mean loss: 1122.15
 ---- batch: 060 ----
mean loss: 1098.18
 ---- batch: 070 ----
mean loss: 1101.92
 ---- batch: 080 ----
mean loss: 1062.29
 ---- batch: 090 ----
mean loss: 1051.62
train mean loss: 1253.43
epoch train time: 0:00:46.876800
elapsed time: 0:00:46.903926
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 04:28:10.927759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1026.89
 ---- batch: 020 ----
mean loss: 1032.64
 ---- batch: 030 ----
mean loss: 1017.36
 ---- batch: 040 ----
mean loss: 1026.14
 ---- batch: 050 ----
mean loss: 1000.67
 ---- batch: 060 ----
mean loss: 987.28
 ---- batch: 070 ----
mean loss: 952.31
 ---- batch: 080 ----
mean loss: 988.79
 ---- batch: 090 ----
mean loss: 1005.03
train mean loss: 1001.36
epoch train time: 0:00:16.651655
elapsed time: 0:01:03.556349
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 04:28:27.580736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 979.23
 ---- batch: 020 ----
mean loss: 956.64
 ---- batch: 030 ----
mean loss: 944.74
 ---- batch: 040 ----
mean loss: 975.02
 ---- batch: 050 ----
mean loss: 969.83
 ---- batch: 060 ----
mean loss: 941.58
 ---- batch: 070 ----
mean loss: 963.55
 ---- batch: 080 ----
mean loss: 928.73
 ---- batch: 090 ----
mean loss: 959.76
train mean loss: 957.30
epoch train time: 0:00:16.655294
elapsed time: 0:01:20.212867
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 04:28:44.237245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.08
 ---- batch: 020 ----
mean loss: 933.80
 ---- batch: 030 ----
mean loss: 946.05
 ---- batch: 040 ----
mean loss: 954.48
 ---- batch: 050 ----
mean loss: 931.12
 ---- batch: 060 ----
mean loss: 933.04
 ---- batch: 070 ----
mean loss: 948.96
 ---- batch: 080 ----
mean loss: 945.90
 ---- batch: 090 ----
mean loss: 936.77
train mean loss: 944.44
epoch train time: 0:00:16.617677
elapsed time: 0:01:36.831760
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 04:29:00.856092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 957.42
 ---- batch: 020 ----
mean loss: 922.32
 ---- batch: 030 ----
mean loss: 939.94
 ---- batch: 040 ----
mean loss: 949.18
 ---- batch: 050 ----
mean loss: 916.99
 ---- batch: 060 ----
mean loss: 944.43
 ---- batch: 070 ----
mean loss: 955.82
 ---- batch: 080 ----
mean loss: 952.63
 ---- batch: 090 ----
mean loss: 927.13
train mean loss: 938.47
epoch train time: 0:00:16.632974
elapsed time: 0:01:53.465999
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 04:29:17.490360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.52
 ---- batch: 020 ----
mean loss: 936.77
 ---- batch: 030 ----
mean loss: 938.88
 ---- batch: 040 ----
mean loss: 931.38
 ---- batch: 050 ----
mean loss: 920.61
 ---- batch: 060 ----
mean loss: 909.81
 ---- batch: 070 ----
mean loss: 940.98
 ---- batch: 080 ----
mean loss: 931.30
 ---- batch: 090 ----
mean loss: 926.07
train mean loss: 931.30
epoch train time: 0:00:16.604070
elapsed time: 0:02:10.071326
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 04:29:34.095821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.41
 ---- batch: 020 ----
mean loss: 934.93
 ---- batch: 030 ----
mean loss: 937.05
 ---- batch: 040 ----
mean loss: 944.32
 ---- batch: 050 ----
mean loss: 936.85
 ---- batch: 060 ----
mean loss: 922.48
 ---- batch: 070 ----
mean loss: 919.38
 ---- batch: 080 ----
mean loss: 927.76
 ---- batch: 090 ----
mean loss: 909.97
train mean loss: 927.84
epoch train time: 0:00:16.638165
elapsed time: 0:02:26.711157
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 04:29:50.735533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.48
 ---- batch: 020 ----
mean loss: 925.80
 ---- batch: 030 ----
mean loss: 956.47
 ---- batch: 040 ----
mean loss: 925.51
 ---- batch: 050 ----
mean loss: 919.64
 ---- batch: 060 ----
mean loss: 923.50
 ---- batch: 070 ----
mean loss: 932.30
 ---- batch: 080 ----
mean loss: 920.26
 ---- batch: 090 ----
mean loss: 908.13
train mean loss: 923.80
epoch train time: 0:00:16.666662
elapsed time: 0:02:43.379006
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 04:30:07.403344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.83
 ---- batch: 020 ----
mean loss: 910.36
 ---- batch: 030 ----
mean loss: 920.94
 ---- batch: 040 ----
mean loss: 946.17
 ---- batch: 050 ----
mean loss: 907.64
 ---- batch: 060 ----
mean loss: 912.95
 ---- batch: 070 ----
mean loss: 922.35
 ---- batch: 080 ----
mean loss: 905.87
 ---- batch: 090 ----
mean loss: 932.54
train mean loss: 919.77
epoch train time: 0:00:16.640396
elapsed time: 0:03:00.020532
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 04:30:24.044913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.46
 ---- batch: 020 ----
mean loss: 934.35
 ---- batch: 030 ----
mean loss: 899.14
 ---- batch: 040 ----
mean loss: 908.11
 ---- batch: 050 ----
mean loss: 921.87
 ---- batch: 060 ----
mean loss: 933.29
 ---- batch: 070 ----
mean loss: 927.75
 ---- batch: 080 ----
mean loss: 908.32
 ---- batch: 090 ----
mean loss: 906.17
train mean loss: 918.88
epoch train time: 0:00:16.649419
elapsed time: 0:03:16.671228
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 04:30:40.695638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.70
 ---- batch: 020 ----
mean loss: 933.62
 ---- batch: 030 ----
mean loss: 915.00
 ---- batch: 040 ----
mean loss: 917.48
 ---- batch: 050 ----
mean loss: 907.87
 ---- batch: 060 ----
mean loss: 924.86
 ---- batch: 070 ----
mean loss: 919.94
 ---- batch: 080 ----
mean loss: 901.45
 ---- batch: 090 ----
mean loss: 919.58
train mean loss: 914.80
epoch train time: 0:00:16.637812
elapsed time: 0:03:33.310324
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 04:30:57.334652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.02
 ---- batch: 020 ----
mean loss: 909.84
 ---- batch: 030 ----
mean loss: 921.29
 ---- batch: 040 ----
mean loss: 923.54
 ---- batch: 050 ----
mean loss: 938.13
 ---- batch: 060 ----
mean loss: 913.50
 ---- batch: 070 ----
mean loss: 918.13
 ---- batch: 080 ----
mean loss: 907.93
 ---- batch: 090 ----
mean loss: 904.61
train mean loss: 912.39
epoch train time: 0:00:16.612214
elapsed time: 0:03:49.923667
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 04:31:13.948009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.10
 ---- batch: 020 ----
mean loss: 913.52
 ---- batch: 030 ----
mean loss: 908.57
 ---- batch: 040 ----
mean loss: 888.48
 ---- batch: 050 ----
mean loss: 917.68
 ---- batch: 060 ----
mean loss: 918.70
 ---- batch: 070 ----
mean loss: 892.93
 ---- batch: 080 ----
mean loss: 904.53
 ---- batch: 090 ----
mean loss: 919.85
train mean loss: 907.69
epoch train time: 0:00:16.604009
elapsed time: 0:04:06.528881
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 04:31:30.553250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.32
 ---- batch: 020 ----
mean loss: 904.16
 ---- batch: 030 ----
mean loss: 897.14
 ---- batch: 040 ----
mean loss: 889.32
 ---- batch: 050 ----
mean loss: 910.00
 ---- batch: 060 ----
mean loss: 891.30
 ---- batch: 070 ----
mean loss: 923.43
 ---- batch: 080 ----
mean loss: 910.76
 ---- batch: 090 ----
mean loss: 905.66
train mean loss: 903.06
epoch train time: 0:00:16.680308
elapsed time: 0:04:23.210421
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 04:31:47.234826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.05
 ---- batch: 020 ----
mean loss: 901.46
 ---- batch: 030 ----
mean loss: 897.86
 ---- batch: 040 ----
mean loss: 895.50
 ---- batch: 050 ----
mean loss: 890.37
 ---- batch: 060 ----
mean loss: 886.92
 ---- batch: 070 ----
mean loss: 884.96
 ---- batch: 080 ----
mean loss: 908.75
 ---- batch: 090 ----
mean loss: 905.97
train mean loss: 900.05
epoch train time: 0:00:16.592185
elapsed time: 0:04:39.803878
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 04:32:03.828286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.94
 ---- batch: 020 ----
mean loss: 900.25
 ---- batch: 030 ----
mean loss: 894.80
 ---- batch: 040 ----
mean loss: 904.50
 ---- batch: 050 ----
mean loss: 899.07
 ---- batch: 060 ----
mean loss: 889.63
 ---- batch: 070 ----
mean loss: 865.51
 ---- batch: 080 ----
mean loss: 890.94
 ---- batch: 090 ----
mean loss: 889.81
train mean loss: 892.00
epoch train time: 0:00:16.599172
elapsed time: 0:04:56.404252
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 04:32:20.428601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.65
 ---- batch: 020 ----
mean loss: 860.88
 ---- batch: 030 ----
mean loss: 880.81
 ---- batch: 040 ----
mean loss: 894.42
 ---- batch: 050 ----
mean loss: 870.82
 ---- batch: 060 ----
mean loss: 887.05
 ---- batch: 070 ----
mean loss: 897.03
 ---- batch: 080 ----
mean loss: 887.14
 ---- batch: 090 ----
mean loss: 858.79
train mean loss: 881.94
epoch train time: 0:00:16.565607
elapsed time: 0:05:12.971102
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 04:32:36.995445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.99
 ---- batch: 020 ----
mean loss: 852.88
 ---- batch: 030 ----
mean loss: 870.79
 ---- batch: 040 ----
mean loss: 869.25
 ---- batch: 050 ----
mean loss: 824.01
 ---- batch: 060 ----
mean loss: 816.01
 ---- batch: 070 ----
mean loss: 816.67
 ---- batch: 080 ----
mean loss: 801.47
 ---- batch: 090 ----
mean loss: 764.10
train mean loss: 826.91
epoch train time: 0:00:16.536979
elapsed time: 0:05:29.509240
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 04:32:53.533749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 757.69
 ---- batch: 020 ----
mean loss: 745.49
 ---- batch: 030 ----
mean loss: 703.21
 ---- batch: 040 ----
mean loss: 713.81
 ---- batch: 050 ----
mean loss: 704.75
 ---- batch: 060 ----
mean loss: 687.54
 ---- batch: 070 ----
mean loss: 674.35
 ---- batch: 080 ----
mean loss: 677.09
 ---- batch: 090 ----
mean loss: 688.10
train mean loss: 702.15
epoch train time: 0:00:16.613545
elapsed time: 0:05:46.124143
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 04:33:10.148519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 660.04
 ---- batch: 020 ----
mean loss: 670.35
 ---- batch: 030 ----
mean loss: 646.35
 ---- batch: 040 ----
mean loss: 656.92
 ---- batch: 050 ----
mean loss: 634.92
 ---- batch: 060 ----
mean loss: 622.34
 ---- batch: 070 ----
mean loss: 628.99
 ---- batch: 080 ----
mean loss: 622.19
 ---- batch: 090 ----
mean loss: 620.21
train mean loss: 638.80
epoch train time: 0:00:16.561631
elapsed time: 0:06:02.687062
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 04:33:26.711459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 601.27
 ---- batch: 020 ----
mean loss: 600.60
 ---- batch: 030 ----
mean loss: 592.10
 ---- batch: 040 ----
mean loss: 585.81
 ---- batch: 050 ----
mean loss: 567.47
 ---- batch: 060 ----
mean loss: 576.90
 ---- batch: 070 ----
mean loss: 556.04
 ---- batch: 080 ----
mean loss: 573.12
 ---- batch: 090 ----
mean loss: 546.23
train mean loss: 575.66
epoch train time: 0:00:16.602293
elapsed time: 0:06:19.290545
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 04:33:43.314881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.84
 ---- batch: 020 ----
mean loss: 537.55
 ---- batch: 030 ----
mean loss: 544.99
 ---- batch: 040 ----
mean loss: 505.07
 ---- batch: 050 ----
mean loss: 506.74
 ---- batch: 060 ----
mean loss: 509.96
 ---- batch: 070 ----
mean loss: 504.27
 ---- batch: 080 ----
mean loss: 499.25
 ---- batch: 090 ----
mean loss: 504.57
train mean loss: 513.46
epoch train time: 0:00:16.595544
elapsed time: 0:06:35.887282
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 04:33:59.911468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.44
 ---- batch: 020 ----
mean loss: 476.98
 ---- batch: 030 ----
mean loss: 477.36
 ---- batch: 040 ----
mean loss: 478.14
 ---- batch: 050 ----
mean loss: 465.17
 ---- batch: 060 ----
mean loss: 464.74
 ---- batch: 070 ----
mean loss: 473.95
 ---- batch: 080 ----
mean loss: 460.06
 ---- batch: 090 ----
mean loss: 452.15
train mean loss: 469.21
epoch train time: 0:00:16.502334
elapsed time: 0:06:52.390721
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 04:34:16.415124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.19
 ---- batch: 020 ----
mean loss: 443.73
 ---- batch: 030 ----
mean loss: 446.15
 ---- batch: 040 ----
mean loss: 437.49
 ---- batch: 050 ----
mean loss: 454.78
 ---- batch: 060 ----
mean loss: 450.60
 ---- batch: 070 ----
mean loss: 443.43
 ---- batch: 080 ----
mean loss: 444.50
 ---- batch: 090 ----
mean loss: 430.02
train mean loss: 445.20
epoch train time: 0:00:16.563816
elapsed time: 0:07:08.955717
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 04:34:32.980162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.63
 ---- batch: 020 ----
mean loss: 441.49
 ---- batch: 030 ----
mean loss: 431.13
 ---- batch: 040 ----
mean loss: 421.28
 ---- batch: 050 ----
mean loss: 415.40
 ---- batch: 060 ----
mean loss: 402.49
 ---- batch: 070 ----
mean loss: 406.14
 ---- batch: 080 ----
mean loss: 417.80
 ---- batch: 090 ----
mean loss: 412.37
train mean loss: 420.77
epoch train time: 0:00:16.577310
elapsed time: 0:07:25.534304
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 04:34:49.558714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.07
 ---- batch: 020 ----
mean loss: 411.90
 ---- batch: 030 ----
mean loss: 406.63
 ---- batch: 040 ----
mean loss: 404.12
 ---- batch: 050 ----
mean loss: 406.08
 ---- batch: 060 ----
mean loss: 408.40
 ---- batch: 070 ----
mean loss: 404.37
 ---- batch: 080 ----
mean loss: 404.86
 ---- batch: 090 ----
mean loss: 405.78
train mean loss: 407.07
epoch train time: 0:00:16.598692
elapsed time: 0:07:42.134265
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 04:35:06.158632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.97
 ---- batch: 020 ----
mean loss: 400.12
 ---- batch: 030 ----
mean loss: 395.98
 ---- batch: 040 ----
mean loss: 411.23
 ---- batch: 050 ----
mean loss: 393.83
 ---- batch: 060 ----
mean loss: 393.40
 ---- batch: 070 ----
mean loss: 395.15
 ---- batch: 080 ----
mean loss: 394.74
 ---- batch: 090 ----
mean loss: 380.62
train mean loss: 395.36
epoch train time: 0:00:16.626309
elapsed time: 0:07:58.761805
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 04:35:22.786246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.87
 ---- batch: 020 ----
mean loss: 390.74
 ---- batch: 030 ----
mean loss: 389.83
 ---- batch: 040 ----
mean loss: 385.23
 ---- batch: 050 ----
mean loss: 384.28
 ---- batch: 060 ----
mean loss: 374.71
 ---- batch: 070 ----
mean loss: 378.29
 ---- batch: 080 ----
mean loss: 367.96
 ---- batch: 090 ----
mean loss: 373.94
train mean loss: 381.42
epoch train time: 0:00:16.620806
elapsed time: 0:08:15.383861
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 04:35:39.408220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.15
 ---- batch: 020 ----
mean loss: 365.89
 ---- batch: 030 ----
mean loss: 370.98
 ---- batch: 040 ----
mean loss: 379.83
 ---- batch: 050 ----
mean loss: 366.84
 ---- batch: 060 ----
mean loss: 368.92
 ---- batch: 070 ----
mean loss: 379.32
 ---- batch: 080 ----
mean loss: 368.52
 ---- batch: 090 ----
mean loss: 356.46
train mean loss: 369.09
epoch train time: 0:00:16.618439
elapsed time: 0:08:32.003465
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 04:35:56.027783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.00
 ---- batch: 020 ----
mean loss: 363.14
 ---- batch: 030 ----
mean loss: 352.53
 ---- batch: 040 ----
mean loss: 345.57
 ---- batch: 050 ----
mean loss: 369.62
 ---- batch: 060 ----
mean loss: 369.12
 ---- batch: 070 ----
mean loss: 349.43
 ---- batch: 080 ----
mean loss: 364.29
 ---- batch: 090 ----
mean loss: 339.03
train mean loss: 358.08
epoch train time: 0:00:16.632496
elapsed time: 0:08:48.637110
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 04:36:12.661457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.79
 ---- batch: 020 ----
mean loss: 361.01
 ---- batch: 030 ----
mean loss: 360.24
 ---- batch: 040 ----
mean loss: 356.84
 ---- batch: 050 ----
mean loss: 354.31
 ---- batch: 060 ----
mean loss: 350.15
 ---- batch: 070 ----
mean loss: 346.11
 ---- batch: 080 ----
mean loss: 344.67
 ---- batch: 090 ----
mean loss: 339.56
train mean loss: 350.85
epoch train time: 0:00:16.590558
elapsed time: 0:09:05.228797
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 04:36:29.253156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.89
 ---- batch: 020 ----
mean loss: 348.50
 ---- batch: 030 ----
mean loss: 342.35
 ---- batch: 040 ----
mean loss: 335.13
 ---- batch: 050 ----
mean loss: 333.92
 ---- batch: 060 ----
mean loss: 349.28
 ---- batch: 070 ----
mean loss: 331.77
 ---- batch: 080 ----
mean loss: 344.25
 ---- batch: 090 ----
mean loss: 341.67
train mean loss: 341.89
epoch train time: 0:00:16.644467
elapsed time: 0:09:21.874445
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 04:36:45.898955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.19
 ---- batch: 020 ----
mean loss: 353.57
 ---- batch: 030 ----
mean loss: 337.15
 ---- batch: 040 ----
mean loss: 344.78
 ---- batch: 050 ----
mean loss: 337.95
 ---- batch: 060 ----
mean loss: 337.14
 ---- batch: 070 ----
mean loss: 347.23
 ---- batch: 080 ----
mean loss: 330.89
 ---- batch: 090 ----
mean loss: 316.54
train mean loss: 338.64
epoch train time: 0:00:16.643892
elapsed time: 0:09:38.519697
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 04:37:02.544011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.22
 ---- batch: 020 ----
mean loss: 335.96
 ---- batch: 030 ----
mean loss: 333.77
 ---- batch: 040 ----
mean loss: 319.75
 ---- batch: 050 ----
mean loss: 324.79
 ---- batch: 060 ----
mean loss: 323.01
 ---- batch: 070 ----
mean loss: 322.95
 ---- batch: 080 ----
mean loss: 329.19
 ---- batch: 090 ----
mean loss: 323.49
train mean loss: 326.67
epoch train time: 0:00:16.686690
elapsed time: 0:09:55.207564
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 04:37:19.232075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.88
 ---- batch: 020 ----
mean loss: 331.01
 ---- batch: 030 ----
mean loss: 324.91
 ---- batch: 040 ----
mean loss: 326.95
 ---- batch: 050 ----
mean loss: 327.49
 ---- batch: 060 ----
mean loss: 322.47
 ---- batch: 070 ----
mean loss: 331.78
 ---- batch: 080 ----
mean loss: 308.76
 ---- batch: 090 ----
mean loss: 315.46
train mean loss: 323.65
epoch train time: 0:00:16.648435
elapsed time: 0:10:11.857371
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 04:37:35.881747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.08
 ---- batch: 020 ----
mean loss: 321.48
 ---- batch: 030 ----
mean loss: 314.54
 ---- batch: 040 ----
mean loss: 317.58
 ---- batch: 050 ----
mean loss: 315.54
 ---- batch: 060 ----
mean loss: 313.22
 ---- batch: 070 ----
mean loss: 309.27
 ---- batch: 080 ----
mean loss: 315.80
 ---- batch: 090 ----
mean loss: 307.01
train mean loss: 316.48
epoch train time: 0:00:16.653556
elapsed time: 0:10:28.512122
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 04:37:52.536438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.25
 ---- batch: 020 ----
mean loss: 327.31
 ---- batch: 030 ----
mean loss: 304.30
 ---- batch: 040 ----
mean loss: 307.90
 ---- batch: 050 ----
mean loss: 312.06
 ---- batch: 060 ----
mean loss: 306.82
 ---- batch: 070 ----
mean loss: 309.66
 ---- batch: 080 ----
mean loss: 308.89
 ---- batch: 090 ----
mean loss: 303.68
train mean loss: 312.22
epoch train time: 0:00:16.689359
elapsed time: 0:10:45.202733
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 04:38:09.226957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.04
 ---- batch: 020 ----
mean loss: 307.14
 ---- batch: 030 ----
mean loss: 302.35
 ---- batch: 040 ----
mean loss: 324.09
 ---- batch: 050 ----
mean loss: 312.84
 ---- batch: 060 ----
mean loss: 310.53
 ---- batch: 070 ----
mean loss: 309.98
 ---- batch: 080 ----
mean loss: 304.10
 ---- batch: 090 ----
mean loss: 304.02
train mean loss: 309.33
epoch train time: 0:00:16.641272
elapsed time: 0:11:01.845302
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 04:38:25.869692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.96
 ---- batch: 020 ----
mean loss: 305.84
 ---- batch: 030 ----
mean loss: 301.32
 ---- batch: 040 ----
mean loss: 303.94
 ---- batch: 050 ----
mean loss: 305.84
 ---- batch: 060 ----
mean loss: 305.57
 ---- batch: 070 ----
mean loss: 305.45
 ---- batch: 080 ----
mean loss: 299.85
 ---- batch: 090 ----
mean loss: 296.31
train mean loss: 301.61
epoch train time: 0:00:16.625785
elapsed time: 0:11:18.472273
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 04:38:42.496559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.89
 ---- batch: 020 ----
mean loss: 290.71
 ---- batch: 030 ----
mean loss: 297.22
 ---- batch: 040 ----
mean loss: 302.42
 ---- batch: 050 ----
mean loss: 306.23
 ---- batch: 060 ----
mean loss: 295.16
 ---- batch: 070 ----
mean loss: 290.78
 ---- batch: 080 ----
mean loss: 294.12
 ---- batch: 090 ----
mean loss: 294.90
train mean loss: 295.83
epoch train time: 0:00:16.692555
elapsed time: 0:11:35.165960
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 04:38:59.190316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.11
 ---- batch: 020 ----
mean loss: 301.82
 ---- batch: 030 ----
mean loss: 290.30
 ---- batch: 040 ----
mean loss: 294.43
 ---- batch: 050 ----
mean loss: 288.88
 ---- batch: 060 ----
mean loss: 293.77
 ---- batch: 070 ----
mean loss: 285.76
 ---- batch: 080 ----
mean loss: 300.06
 ---- batch: 090 ----
mean loss: 286.47
train mean loss: 292.90
epoch train time: 0:00:16.650098
elapsed time: 0:11:51.817190
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 04:39:15.841520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.05
 ---- batch: 020 ----
mean loss: 281.77
 ---- batch: 030 ----
mean loss: 284.75
 ---- batch: 040 ----
mean loss: 293.99
 ---- batch: 050 ----
mean loss: 287.20
 ---- batch: 060 ----
mean loss: 291.96
 ---- batch: 070 ----
mean loss: 284.29
 ---- batch: 080 ----
mean loss: 290.88
 ---- batch: 090 ----
mean loss: 290.26
train mean loss: 287.24
epoch train time: 0:00:16.659066
elapsed time: 0:12:08.477395
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 04:39:32.501790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.70
 ---- batch: 020 ----
mean loss: 279.84
 ---- batch: 030 ----
mean loss: 290.11
 ---- batch: 040 ----
mean loss: 282.17
 ---- batch: 050 ----
mean loss: 290.00
 ---- batch: 060 ----
mean loss: 289.97
 ---- batch: 070 ----
mean loss: 280.93
 ---- batch: 080 ----
mean loss: 290.04
 ---- batch: 090 ----
mean loss: 282.07
train mean loss: 286.35
epoch train time: 0:00:16.654297
elapsed time: 0:12:25.132869
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 04:39:49.157213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.31
 ---- batch: 020 ----
mean loss: 295.11
 ---- batch: 030 ----
mean loss: 280.49
 ---- batch: 040 ----
mean loss: 292.19
 ---- batch: 050 ----
mean loss: 280.57
 ---- batch: 060 ----
mean loss: 284.14
 ---- batch: 070 ----
mean loss: 277.93
 ---- batch: 080 ----
mean loss: 283.57
 ---- batch: 090 ----
mean loss: 285.06
train mean loss: 284.39
epoch train time: 0:00:16.654738
elapsed time: 0:12:41.788781
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 04:40:05.812974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.63
 ---- batch: 020 ----
mean loss: 262.63
 ---- batch: 030 ----
mean loss: 293.89
 ---- batch: 040 ----
mean loss: 275.93
 ---- batch: 050 ----
mean loss: 277.16
 ---- batch: 060 ----
mean loss: 279.32
 ---- batch: 070 ----
mean loss: 287.55
 ---- batch: 080 ----
mean loss: 277.06
 ---- batch: 090 ----
mean loss: 276.82
train mean loss: 280.01
epoch train time: 0:00:16.651573
elapsed time: 0:12:58.441335
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 04:40:22.465688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.04
 ---- batch: 020 ----
mean loss: 284.06
 ---- batch: 030 ----
mean loss: 274.84
 ---- batch: 040 ----
mean loss: 276.22
 ---- batch: 050 ----
mean loss: 273.50
 ---- batch: 060 ----
mean loss: 273.70
 ---- batch: 070 ----
mean loss: 279.99
 ---- batch: 080 ----
mean loss: 267.09
 ---- batch: 090 ----
mean loss: 279.40
train mean loss: 275.67
epoch train time: 0:00:16.700953
elapsed time: 0:13:15.143430
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 04:40:39.167769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.27
 ---- batch: 020 ----
mean loss: 274.86
 ---- batch: 030 ----
mean loss: 284.07
 ---- batch: 040 ----
mean loss: 273.87
 ---- batch: 050 ----
mean loss: 270.73
 ---- batch: 060 ----
mean loss: 279.43
 ---- batch: 070 ----
mean loss: 271.18
 ---- batch: 080 ----
mean loss: 270.27
 ---- batch: 090 ----
mean loss: 269.47
train mean loss: 275.37
epoch train time: 0:00:16.694503
elapsed time: 0:13:31.839091
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 04:40:55.863430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.80
 ---- batch: 020 ----
mean loss: 274.46
 ---- batch: 030 ----
mean loss: 272.96
 ---- batch: 040 ----
mean loss: 264.01
 ---- batch: 050 ----
mean loss: 269.77
 ---- batch: 060 ----
mean loss: 273.47
 ---- batch: 070 ----
mean loss: 274.92
 ---- batch: 080 ----
mean loss: 264.88
 ---- batch: 090 ----
mean loss: 269.16
train mean loss: 270.71
epoch train time: 0:00:16.719802
elapsed time: 0:13:48.560074
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 04:41:12.584418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.69
 ---- batch: 020 ----
mean loss: 263.59
 ---- batch: 030 ----
mean loss: 272.00
 ---- batch: 040 ----
mean loss: 266.03
 ---- batch: 050 ----
mean loss: 268.60
 ---- batch: 060 ----
mean loss: 268.56
 ---- batch: 070 ----
mean loss: 272.95
 ---- batch: 080 ----
mean loss: 277.91
 ---- batch: 090 ----
mean loss: 264.89
train mean loss: 269.55
epoch train time: 0:00:16.673855
elapsed time: 0:14:05.235162
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 04:41:29.259546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.28
 ---- batch: 020 ----
mean loss: 257.40
 ---- batch: 030 ----
mean loss: 275.66
 ---- batch: 040 ----
mean loss: 261.84
 ---- batch: 050 ----
mean loss: 265.51
 ---- batch: 060 ----
mean loss: 273.32
 ---- batch: 070 ----
mean loss: 260.51
 ---- batch: 080 ----
mean loss: 265.46
 ---- batch: 090 ----
mean loss: 258.91
train mean loss: 264.39
epoch train time: 0:00:16.681851
elapsed time: 0:14:21.918320
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 04:41:45.942687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.54
 ---- batch: 020 ----
mean loss: 263.15
 ---- batch: 030 ----
mean loss: 267.90
 ---- batch: 040 ----
mean loss: 259.93
 ---- batch: 050 ----
mean loss: 261.26
 ---- batch: 060 ----
mean loss: 267.80
 ---- batch: 070 ----
mean loss: 260.97
 ---- batch: 080 ----
mean loss: 262.41
 ---- batch: 090 ----
mean loss: 258.86
train mean loss: 263.06
epoch train time: 0:00:16.702442
elapsed time: 0:14:38.622167
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 04:42:02.646593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.06
 ---- batch: 020 ----
mean loss: 259.86
 ---- batch: 030 ----
mean loss: 265.57
 ---- batch: 040 ----
mean loss: 267.78
 ---- batch: 050 ----
mean loss: 255.27
 ---- batch: 060 ----
mean loss: 263.42
 ---- batch: 070 ----
mean loss: 251.62
 ---- batch: 080 ----
mean loss: 255.83
 ---- batch: 090 ----
mean loss: 262.57
train mean loss: 260.93
epoch train time: 0:00:16.609662
elapsed time: 0:14:55.233051
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 04:42:19.257588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.76
 ---- batch: 020 ----
mean loss: 263.61
 ---- batch: 030 ----
mean loss: 258.90
 ---- batch: 040 ----
mean loss: 259.33
 ---- batch: 050 ----
mean loss: 262.41
 ---- batch: 060 ----
mean loss: 262.41
 ---- batch: 070 ----
mean loss: 250.54
 ---- batch: 080 ----
mean loss: 250.88
 ---- batch: 090 ----
mean loss: 259.68
train mean loss: 259.34
epoch train time: 0:00:16.576388
elapsed time: 0:15:11.810967
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 04:42:35.835407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.09
 ---- batch: 020 ----
mean loss: 254.30
 ---- batch: 030 ----
mean loss: 262.78
 ---- batch: 040 ----
mean loss: 254.18
 ---- batch: 050 ----
mean loss: 250.77
 ---- batch: 060 ----
mean loss: 249.13
 ---- batch: 070 ----
mean loss: 255.83
 ---- batch: 080 ----
mean loss: 267.32
 ---- batch: 090 ----
mean loss: 250.23
train mean loss: 255.34
epoch train time: 0:00:16.577785
elapsed time: 0:15:28.390126
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 04:42:52.414633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.20
 ---- batch: 020 ----
mean loss: 255.09
 ---- batch: 030 ----
mean loss: 253.24
 ---- batch: 040 ----
mean loss: 254.29
 ---- batch: 050 ----
mean loss: 256.44
 ---- batch: 060 ----
mean loss: 253.68
 ---- batch: 070 ----
mean loss: 252.76
 ---- batch: 080 ----
mean loss: 245.30
 ---- batch: 090 ----
mean loss: 256.28
train mean loss: 254.21
epoch train time: 0:00:16.536258
elapsed time: 0:15:44.927705
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 04:43:08.952013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.83
 ---- batch: 020 ----
mean loss: 253.49
 ---- batch: 030 ----
mean loss: 251.39
 ---- batch: 040 ----
mean loss: 256.09
 ---- batch: 050 ----
mean loss: 250.13
 ---- batch: 060 ----
mean loss: 254.16
 ---- batch: 070 ----
mean loss: 260.34
 ---- batch: 080 ----
mean loss: 246.23
 ---- batch: 090 ----
mean loss: 245.69
train mean loss: 253.33
epoch train time: 0:00:16.652210
elapsed time: 0:16:01.581069
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 04:43:25.605631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.56
 ---- batch: 020 ----
mean loss: 245.99
 ---- batch: 030 ----
mean loss: 257.76
 ---- batch: 040 ----
mean loss: 253.02
 ---- batch: 050 ----
mean loss: 245.73
 ---- batch: 060 ----
mean loss: 248.16
 ---- batch: 070 ----
mean loss: 249.13
 ---- batch: 080 ----
mean loss: 250.03
 ---- batch: 090 ----
mean loss: 253.37
train mean loss: 249.82
epoch train time: 0:00:16.567388
elapsed time: 0:16:18.149836
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 04:43:42.174191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.66
 ---- batch: 020 ----
mean loss: 250.64
 ---- batch: 030 ----
mean loss: 252.21
 ---- batch: 040 ----
mean loss: 247.34
 ---- batch: 050 ----
mean loss: 253.64
 ---- batch: 060 ----
mean loss: 254.24
 ---- batch: 070 ----
mean loss: 239.76
 ---- batch: 080 ----
mean loss: 251.92
 ---- batch: 090 ----
mean loss: 250.22
train mean loss: 249.24
epoch train time: 0:00:16.562257
elapsed time: 0:16:34.713314
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 04:43:58.737719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.58
 ---- batch: 020 ----
mean loss: 251.22
 ---- batch: 030 ----
mean loss: 249.26
 ---- batch: 040 ----
mean loss: 251.05
 ---- batch: 050 ----
mean loss: 247.17
 ---- batch: 060 ----
mean loss: 246.12
 ---- batch: 070 ----
mean loss: 244.61
 ---- batch: 080 ----
mean loss: 248.57
 ---- batch: 090 ----
mean loss: 244.43
train mean loss: 247.80
epoch train time: 0:00:16.573439
elapsed time: 0:16:51.287968
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 04:44:15.312319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.85
 ---- batch: 020 ----
mean loss: 248.63
 ---- batch: 030 ----
mean loss: 243.15
 ---- batch: 040 ----
mean loss: 249.85
 ---- batch: 050 ----
mean loss: 247.23
 ---- batch: 060 ----
mean loss: 251.18
 ---- batch: 070 ----
mean loss: 243.92
 ---- batch: 080 ----
mean loss: 240.78
 ---- batch: 090 ----
mean loss: 243.11
train mean loss: 245.75
epoch train time: 0:00:16.605565
elapsed time: 0:17:07.894810
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 04:44:31.919146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.11
 ---- batch: 020 ----
mean loss: 247.50
 ---- batch: 030 ----
mean loss: 241.14
 ---- batch: 040 ----
mean loss: 244.93
 ---- batch: 050 ----
mean loss: 244.00
 ---- batch: 060 ----
mean loss: 239.83
 ---- batch: 070 ----
mean loss: 248.11
 ---- batch: 080 ----
mean loss: 246.86
 ---- batch: 090 ----
mean loss: 243.67
train mean loss: 243.79
epoch train time: 0:00:16.596653
elapsed time: 0:17:24.492577
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 04:44:48.517001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.63
 ---- batch: 020 ----
mean loss: 240.21
 ---- batch: 030 ----
mean loss: 237.05
 ---- batch: 040 ----
mean loss: 233.29
 ---- batch: 050 ----
mean loss: 238.87
 ---- batch: 060 ----
mean loss: 251.56
 ---- batch: 070 ----
mean loss: 236.96
 ---- batch: 080 ----
mean loss: 238.49
 ---- batch: 090 ----
mean loss: 252.21
train mean loss: 241.39
epoch train time: 0:00:16.572542
elapsed time: 0:17:41.066380
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 04:45:05.090764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.20
 ---- batch: 020 ----
mean loss: 241.75
 ---- batch: 030 ----
mean loss: 244.97
 ---- batch: 040 ----
mean loss: 235.28
 ---- batch: 050 ----
mean loss: 250.81
 ---- batch: 060 ----
mean loss: 238.03
 ---- batch: 070 ----
mean loss: 243.16
 ---- batch: 080 ----
mean loss: 241.49
 ---- batch: 090 ----
mean loss: 241.73
train mean loss: 241.98
epoch train time: 0:00:16.592326
elapsed time: 0:17:57.659899
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 04:45:21.684245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.60
 ---- batch: 020 ----
mean loss: 243.80
 ---- batch: 030 ----
mean loss: 234.39
 ---- batch: 040 ----
mean loss: 240.66
 ---- batch: 050 ----
mean loss: 249.95
 ---- batch: 060 ----
mean loss: 236.10
 ---- batch: 070 ----
mean loss: 238.87
 ---- batch: 080 ----
mean loss: 231.42
 ---- batch: 090 ----
mean loss: 233.62
train mean loss: 239.08
epoch train time: 0:00:16.631969
elapsed time: 0:18:14.292991
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 04:45:38.317329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.34
 ---- batch: 020 ----
mean loss: 234.61
 ---- batch: 030 ----
mean loss: 242.09
 ---- batch: 040 ----
mean loss: 244.19
 ---- batch: 050 ----
mean loss: 244.61
 ---- batch: 060 ----
mean loss: 240.32
 ---- batch: 070 ----
mean loss: 231.75
 ---- batch: 080 ----
mean loss: 228.61
 ---- batch: 090 ----
mean loss: 236.02
train mean loss: 238.63
epoch train time: 0:00:16.623489
elapsed time: 0:18:30.917709
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 04:45:54.942109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.05
 ---- batch: 020 ----
mean loss: 240.83
 ---- batch: 030 ----
mean loss: 239.14
 ---- batch: 040 ----
mean loss: 237.98
 ---- batch: 050 ----
mean loss: 238.00
 ---- batch: 060 ----
mean loss: 238.68
 ---- batch: 070 ----
mean loss: 233.22
 ---- batch: 080 ----
mean loss: 241.58
 ---- batch: 090 ----
mean loss: 233.56
train mean loss: 237.38
epoch train time: 0:00:16.572218
elapsed time: 0:18:47.491236
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 04:46:11.515644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.24
 ---- batch: 020 ----
mean loss: 234.19
 ---- batch: 030 ----
mean loss: 239.54
 ---- batch: 040 ----
mean loss: 247.20
 ---- batch: 050 ----
mean loss: 238.05
 ---- batch: 060 ----
mean loss: 238.08
 ---- batch: 070 ----
mean loss: 226.67
 ---- batch: 080 ----
mean loss: 229.18
 ---- batch: 090 ----
mean loss: 226.43
train mean loss: 234.82
epoch train time: 0:00:16.622750
elapsed time: 0:19:04.115342
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 04:46:28.139654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.39
 ---- batch: 020 ----
mean loss: 231.78
 ---- batch: 030 ----
mean loss: 228.00
 ---- batch: 040 ----
mean loss: 232.59
 ---- batch: 050 ----
mean loss: 232.92
 ---- batch: 060 ----
mean loss: 233.53
 ---- batch: 070 ----
mean loss: 234.28
 ---- batch: 080 ----
mean loss: 234.99
 ---- batch: 090 ----
mean loss: 230.83
train mean loss: 232.87
epoch train time: 0:00:16.629241
elapsed time: 0:19:20.745707
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 04:46:44.770087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.79
 ---- batch: 020 ----
mean loss: 234.77
 ---- batch: 030 ----
mean loss: 224.83
 ---- batch: 040 ----
mean loss: 229.07
 ---- batch: 050 ----
mean loss: 228.54
 ---- batch: 060 ----
mean loss: 227.73
 ---- batch: 070 ----
mean loss: 241.29
 ---- batch: 080 ----
mean loss: 228.02
 ---- batch: 090 ----
mean loss: 233.31
train mean loss: 231.29
epoch train time: 0:00:16.643449
elapsed time: 0:19:37.390410
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 04:47:01.414816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.65
 ---- batch: 020 ----
mean loss: 228.08
 ---- batch: 030 ----
mean loss: 226.95
 ---- batch: 040 ----
mean loss: 228.98
 ---- batch: 050 ----
mean loss: 228.81
 ---- batch: 060 ----
mean loss: 222.50
 ---- batch: 070 ----
mean loss: 218.18
 ---- batch: 080 ----
mean loss: 235.63
 ---- batch: 090 ----
mean loss: 236.49
train mean loss: 229.92
epoch train time: 0:00:16.601323
elapsed time: 0:19:53.992941
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 04:47:18.017263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.60
 ---- batch: 020 ----
mean loss: 222.97
 ---- batch: 030 ----
mean loss: 230.77
 ---- batch: 040 ----
mean loss: 234.67
 ---- batch: 050 ----
mean loss: 236.57
 ---- batch: 060 ----
mean loss: 224.59
 ---- batch: 070 ----
mean loss: 233.25
 ---- batch: 080 ----
mean loss: 235.32
 ---- batch: 090 ----
mean loss: 229.19
train mean loss: 229.95
epoch train time: 0:00:16.591226
elapsed time: 0:20:10.585276
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 04:47:34.609625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.11
 ---- batch: 020 ----
mean loss: 224.60
 ---- batch: 030 ----
mean loss: 234.45
 ---- batch: 040 ----
mean loss: 228.30
 ---- batch: 050 ----
mean loss: 226.99
 ---- batch: 060 ----
mean loss: 230.40
 ---- batch: 070 ----
mean loss: 223.32
 ---- batch: 080 ----
mean loss: 227.54
 ---- batch: 090 ----
mean loss: 230.10
train mean loss: 228.15
epoch train time: 0:00:16.638044
elapsed time: 0:20:27.224492
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 04:47:51.248849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.25
 ---- batch: 020 ----
mean loss: 211.90
 ---- batch: 030 ----
mean loss: 226.51
 ---- batch: 040 ----
mean loss: 227.89
 ---- batch: 050 ----
mean loss: 231.39
 ---- batch: 060 ----
mean loss: 226.43
 ---- batch: 070 ----
mean loss: 227.57
 ---- batch: 080 ----
mean loss: 235.08
 ---- batch: 090 ----
mean loss: 229.27
train mean loss: 226.31
epoch train time: 0:00:16.570869
elapsed time: 0:20:43.796541
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 04:48:07.820952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.92
 ---- batch: 020 ----
mean loss: 233.56
 ---- batch: 030 ----
mean loss: 237.24
 ---- batch: 040 ----
mean loss: 225.59
 ---- batch: 050 ----
mean loss: 217.62
 ---- batch: 060 ----
mean loss: 226.57
 ---- batch: 070 ----
mean loss: 225.33
 ---- batch: 080 ----
mean loss: 227.05
 ---- batch: 090 ----
mean loss: 225.59
train mean loss: 227.00
epoch train time: 0:00:16.636812
elapsed time: 0:21:00.434771
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 04:48:24.459085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.01
 ---- batch: 020 ----
mean loss: 228.02
 ---- batch: 030 ----
mean loss: 228.38
 ---- batch: 040 ----
mean loss: 229.92
 ---- batch: 050 ----
mean loss: 223.87
 ---- batch: 060 ----
mean loss: 222.41
 ---- batch: 070 ----
mean loss: 221.96
 ---- batch: 080 ----
mean loss: 221.50
 ---- batch: 090 ----
mean loss: 223.50
train mean loss: 225.29
epoch train time: 0:00:16.720965
elapsed time: 0:21:17.157074
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 04:48:41.181422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.90
 ---- batch: 020 ----
mean loss: 221.21
 ---- batch: 030 ----
mean loss: 225.78
 ---- batch: 040 ----
mean loss: 228.55
 ---- batch: 050 ----
mean loss: 220.46
 ---- batch: 060 ----
mean loss: 218.91
 ---- batch: 070 ----
mean loss: 225.64
 ---- batch: 080 ----
mean loss: 228.06
 ---- batch: 090 ----
mean loss: 225.79
train mean loss: 224.12
epoch train time: 0:00:16.586040
elapsed time: 0:21:33.744306
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 04:48:57.768664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.53
 ---- batch: 020 ----
mean loss: 225.90
 ---- batch: 030 ----
mean loss: 217.50
 ---- batch: 040 ----
mean loss: 224.60
 ---- batch: 050 ----
mean loss: 234.96
 ---- batch: 060 ----
mean loss: 219.75
 ---- batch: 070 ----
mean loss: 224.30
 ---- batch: 080 ----
mean loss: 227.49
 ---- batch: 090 ----
mean loss: 228.72
train mean loss: 225.65
epoch train time: 0:00:16.570491
elapsed time: 0:21:50.316023
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 04:49:14.340351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.84
 ---- batch: 020 ----
mean loss: 223.83
 ---- batch: 030 ----
mean loss: 222.67
 ---- batch: 040 ----
mean loss: 234.23
 ---- batch: 050 ----
mean loss: 217.01
 ---- batch: 060 ----
mean loss: 218.82
 ---- batch: 070 ----
mean loss: 221.56
 ---- batch: 080 ----
mean loss: 222.55
 ---- batch: 090 ----
mean loss: 220.38
train mean loss: 222.53
epoch train time: 0:00:16.592013
elapsed time: 0:22:06.909243
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 04:49:30.933599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.87
 ---- batch: 020 ----
mean loss: 218.19
 ---- batch: 030 ----
mean loss: 218.99
 ---- batch: 040 ----
mean loss: 234.40
 ---- batch: 050 ----
mean loss: 218.99
 ---- batch: 060 ----
mean loss: 214.14
 ---- batch: 070 ----
mean loss: 229.59
 ---- batch: 080 ----
mean loss: 216.04
 ---- batch: 090 ----
mean loss: 222.45
train mean loss: 220.75
epoch train time: 0:00:16.568832
elapsed time: 0:22:23.479263
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 04:49:47.503503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.58
 ---- batch: 020 ----
mean loss: 216.00
 ---- batch: 030 ----
mean loss: 218.47
 ---- batch: 040 ----
mean loss: 228.75
 ---- batch: 050 ----
mean loss: 220.13
 ---- batch: 060 ----
mean loss: 225.30
 ---- batch: 070 ----
mean loss: 214.53
 ---- batch: 080 ----
mean loss: 222.31
 ---- batch: 090 ----
mean loss: 222.19
train mean loss: 220.31
epoch train time: 0:00:16.555833
elapsed time: 0:22:40.036170
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 04:50:04.060561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.22
 ---- batch: 020 ----
mean loss: 210.97
 ---- batch: 030 ----
mean loss: 217.21
 ---- batch: 040 ----
mean loss: 216.00
 ---- batch: 050 ----
mean loss: 216.62
 ---- batch: 060 ----
mean loss: 216.41
 ---- batch: 070 ----
mean loss: 221.47
 ---- batch: 080 ----
mean loss: 221.37
 ---- batch: 090 ----
mean loss: 226.13
train mean loss: 218.36
epoch train time: 0:00:16.560815
elapsed time: 0:22:56.598220
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 04:50:20.622616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.01
 ---- batch: 020 ----
mean loss: 221.47
 ---- batch: 030 ----
mean loss: 223.24
 ---- batch: 040 ----
mean loss: 218.89
 ---- batch: 050 ----
mean loss: 226.25
 ---- batch: 060 ----
mean loss: 219.77
 ---- batch: 070 ----
mean loss: 223.43
 ---- batch: 080 ----
mean loss: 212.72
 ---- batch: 090 ----
mean loss: 217.56
train mean loss: 219.84
epoch train time: 0:00:16.630773
elapsed time: 0:23:13.230239
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 04:50:37.254632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.99
 ---- batch: 020 ----
mean loss: 218.10
 ---- batch: 030 ----
mean loss: 216.53
 ---- batch: 040 ----
mean loss: 216.90
 ---- batch: 050 ----
mean loss: 222.92
 ---- batch: 060 ----
mean loss: 229.28
 ---- batch: 070 ----
mean loss: 212.64
 ---- batch: 080 ----
mean loss: 226.50
 ---- batch: 090 ----
mean loss: 213.25
train mean loss: 218.96
epoch train time: 0:00:16.597116
elapsed time: 0:23:29.828622
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 04:50:53.852985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.78
 ---- batch: 020 ----
mean loss: 213.45
 ---- batch: 030 ----
mean loss: 218.49
 ---- batch: 040 ----
mean loss: 213.15
 ---- batch: 050 ----
mean loss: 218.94
 ---- batch: 060 ----
mean loss: 218.46
 ---- batch: 070 ----
mean loss: 215.93
 ---- batch: 080 ----
mean loss: 213.39
 ---- batch: 090 ----
mean loss: 215.52
train mean loss: 216.47
epoch train time: 0:00:16.603670
elapsed time: 0:23:46.433413
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 04:51:10.457756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.83
 ---- batch: 020 ----
mean loss: 215.72
 ---- batch: 030 ----
mean loss: 215.79
 ---- batch: 040 ----
mean loss: 213.27
 ---- batch: 050 ----
mean loss: 220.08
 ---- batch: 060 ----
mean loss: 214.46
 ---- batch: 070 ----
mean loss: 210.05
 ---- batch: 080 ----
mean loss: 217.26
 ---- batch: 090 ----
mean loss: 225.52
train mean loss: 216.12
epoch train time: 0:00:16.628212
elapsed time: 0:24:03.062777
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 04:51:27.087116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.79
 ---- batch: 020 ----
mean loss: 229.77
 ---- batch: 030 ----
mean loss: 220.64
 ---- batch: 040 ----
mean loss: 227.65
 ---- batch: 050 ----
mean loss: 213.18
 ---- batch: 060 ----
mean loss: 207.25
 ---- batch: 070 ----
mean loss: 201.44
 ---- batch: 080 ----
mean loss: 210.46
 ---- batch: 090 ----
mean loss: 222.78
train mean loss: 217.72
epoch train time: 0:00:16.574323
elapsed time: 0:24:19.638326
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 04:51:43.662685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.77
 ---- batch: 020 ----
mean loss: 213.26
 ---- batch: 030 ----
mean loss: 223.03
 ---- batch: 040 ----
mean loss: 219.30
 ---- batch: 050 ----
mean loss: 212.11
 ---- batch: 060 ----
mean loss: 221.55
 ---- batch: 070 ----
mean loss: 212.42
 ---- batch: 080 ----
mean loss: 215.46
 ---- batch: 090 ----
mean loss: 205.33
train mean loss: 215.19
epoch train time: 0:00:16.583708
elapsed time: 0:24:36.223276
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 04:52:00.247645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.48
 ---- batch: 020 ----
mean loss: 214.30
 ---- batch: 030 ----
mean loss: 217.44
 ---- batch: 040 ----
mean loss: 216.28
 ---- batch: 050 ----
mean loss: 202.20
 ---- batch: 060 ----
mean loss: 217.34
 ---- batch: 070 ----
mean loss: 215.71
 ---- batch: 080 ----
mean loss: 212.06
 ---- batch: 090 ----
mean loss: 215.74
train mean loss: 214.14
epoch train time: 0:00:16.600181
elapsed time: 0:24:52.824601
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 04:52:16.848943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.96
 ---- batch: 020 ----
mean loss: 216.92
 ---- batch: 030 ----
mean loss: 213.19
 ---- batch: 040 ----
mean loss: 215.46
 ---- batch: 050 ----
mean loss: 210.28
 ---- batch: 060 ----
mean loss: 224.48
 ---- batch: 070 ----
mean loss: 212.18
 ---- batch: 080 ----
mean loss: 214.59
 ---- batch: 090 ----
mean loss: 210.93
train mean loss: 214.88
epoch train time: 0:00:16.575346
elapsed time: 0:25:09.401156
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 04:52:33.425535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.03
 ---- batch: 020 ----
mean loss: 215.57
 ---- batch: 030 ----
mean loss: 216.79
 ---- batch: 040 ----
mean loss: 215.29
 ---- batch: 050 ----
mean loss: 217.15
 ---- batch: 060 ----
mean loss: 216.02
 ---- batch: 070 ----
mean loss: 213.35
 ---- batch: 080 ----
mean loss: 211.90
 ---- batch: 090 ----
mean loss: 213.40
train mean loss: 214.60
epoch train time: 0:00:16.562851
elapsed time: 0:25:25.965232
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 04:52:49.989537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.94
 ---- batch: 020 ----
mean loss: 212.32
 ---- batch: 030 ----
mean loss: 205.30
 ---- batch: 040 ----
mean loss: 212.60
 ---- batch: 050 ----
mean loss: 218.43
 ---- batch: 060 ----
mean loss: 222.48
 ---- batch: 070 ----
mean loss: 211.52
 ---- batch: 080 ----
mean loss: 222.72
 ---- batch: 090 ----
mean loss: 215.42
train mean loss: 214.63
epoch train time: 0:00:16.561946
elapsed time: 0:25:42.528253
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 04:53:06.552621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.58
 ---- batch: 020 ----
mean loss: 218.19
 ---- batch: 030 ----
mean loss: 207.16
 ---- batch: 040 ----
mean loss: 214.36
 ---- batch: 050 ----
mean loss: 222.15
 ---- batch: 060 ----
mean loss: 218.61
 ---- batch: 070 ----
mean loss: 215.61
 ---- batch: 080 ----
mean loss: 213.44
 ---- batch: 090 ----
mean loss: 206.78
train mean loss: 214.01
epoch train time: 0:00:16.591082
elapsed time: 0:25:59.120499
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 04:53:23.144807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.90
 ---- batch: 020 ----
mean loss: 221.28
 ---- batch: 030 ----
mean loss: 212.55
 ---- batch: 040 ----
mean loss: 213.67
 ---- batch: 050 ----
mean loss: 206.89
 ---- batch: 060 ----
mean loss: 212.15
 ---- batch: 070 ----
mean loss: 209.84
 ---- batch: 080 ----
mean loss: 219.20
 ---- batch: 090 ----
mean loss: 205.37
train mean loss: 212.07
epoch train time: 0:00:16.632861
elapsed time: 0:26:15.754517
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 04:53:39.778853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.44
 ---- batch: 020 ----
mean loss: 215.14
 ---- batch: 030 ----
mean loss: 210.87
 ---- batch: 040 ----
mean loss: 213.89
 ---- batch: 050 ----
mean loss: 212.22
 ---- batch: 060 ----
mean loss: 217.17
 ---- batch: 070 ----
mean loss: 219.10
 ---- batch: 080 ----
mean loss: 209.76
 ---- batch: 090 ----
mean loss: 204.83
train mean loss: 212.21
epoch train time: 0:00:16.621930
elapsed time: 0:26:32.377567
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 04:53:56.401965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.98
 ---- batch: 020 ----
mean loss: 204.35
 ---- batch: 030 ----
mean loss: 219.88
 ---- batch: 040 ----
mean loss: 201.99
 ---- batch: 050 ----
mean loss: 208.07
 ---- batch: 060 ----
mean loss: 220.77
 ---- batch: 070 ----
mean loss: 213.77
 ---- batch: 080 ----
mean loss: 212.11
 ---- batch: 090 ----
mean loss: 213.98
train mean loss: 211.48
epoch train time: 0:00:16.622833
elapsed time: 0:26:49.001620
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 04:54:13.026023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.23
 ---- batch: 020 ----
mean loss: 210.44
 ---- batch: 030 ----
mean loss: 211.07
 ---- batch: 040 ----
mean loss: 204.65
 ---- batch: 050 ----
mean loss: 212.75
 ---- batch: 060 ----
mean loss: 210.69
 ---- batch: 070 ----
mean loss: 206.76
 ---- batch: 080 ----
mean loss: 211.38
 ---- batch: 090 ----
mean loss: 211.68
train mean loss: 210.19
epoch train time: 0:00:16.624516
elapsed time: 0:27:05.627412
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 04:54:29.651759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.50
 ---- batch: 020 ----
mean loss: 205.31
 ---- batch: 030 ----
mean loss: 207.23
 ---- batch: 040 ----
mean loss: 218.67
 ---- batch: 050 ----
mean loss: 211.93
 ---- batch: 060 ----
mean loss: 204.65
 ---- batch: 070 ----
mean loss: 217.16
 ---- batch: 080 ----
mean loss: 206.77
 ---- batch: 090 ----
mean loss: 211.33
train mean loss: 210.93
epoch train time: 0:00:16.582230
elapsed time: 0:27:22.210784
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 04:54:46.235118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.94
 ---- batch: 020 ----
mean loss: 207.50
 ---- batch: 030 ----
mean loss: 209.03
 ---- batch: 040 ----
mean loss: 209.03
 ---- batch: 050 ----
mean loss: 207.89
 ---- batch: 060 ----
mean loss: 207.58
 ---- batch: 070 ----
mean loss: 215.30
 ---- batch: 080 ----
mean loss: 213.29
 ---- batch: 090 ----
mean loss: 217.79
train mean loss: 210.49
epoch train time: 0:00:16.604393
elapsed time: 0:27:38.816341
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 04:55:02.840715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.94
 ---- batch: 020 ----
mean loss: 214.60
 ---- batch: 030 ----
mean loss: 205.30
 ---- batch: 040 ----
mean loss: 207.28
 ---- batch: 050 ----
mean loss: 202.31
 ---- batch: 060 ----
mean loss: 212.23
 ---- batch: 070 ----
mean loss: 213.58
 ---- batch: 080 ----
mean loss: 215.56
 ---- batch: 090 ----
mean loss: 207.84
train mean loss: 209.89
epoch train time: 0:00:16.636523
elapsed time: 0:27:55.454133
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 04:55:19.478471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.09
 ---- batch: 020 ----
mean loss: 201.25
 ---- batch: 030 ----
mean loss: 207.35
 ---- batch: 040 ----
mean loss: 205.03
 ---- batch: 050 ----
mean loss: 212.76
 ---- batch: 060 ----
mean loss: 212.25
 ---- batch: 070 ----
mean loss: 204.21
 ---- batch: 080 ----
mean loss: 209.25
 ---- batch: 090 ----
mean loss: 208.14
train mean loss: 208.61
epoch train time: 0:00:16.653227
elapsed time: 0:28:12.108488
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 04:55:36.132856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.14
 ---- batch: 020 ----
mean loss: 205.13
 ---- batch: 030 ----
mean loss: 207.32
 ---- batch: 040 ----
mean loss: 208.38
 ---- batch: 050 ----
mean loss: 216.49
 ---- batch: 060 ----
mean loss: 197.78
 ---- batch: 070 ----
mean loss: 199.79
 ---- batch: 080 ----
mean loss: 208.05
 ---- batch: 090 ----
mean loss: 209.98
train mean loss: 207.72
epoch train time: 0:00:16.671633
elapsed time: 0:28:28.781432
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 04:55:52.805940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.09
 ---- batch: 020 ----
mean loss: 207.38
 ---- batch: 030 ----
mean loss: 208.75
 ---- batch: 040 ----
mean loss: 202.55
 ---- batch: 050 ----
mean loss: 210.01
 ---- batch: 060 ----
mean loss: 208.91
 ---- batch: 070 ----
mean loss: 203.32
 ---- batch: 080 ----
mean loss: 203.41
 ---- batch: 090 ----
mean loss: 209.05
train mean loss: 207.00
epoch train time: 0:00:16.713735
elapsed time: 0:28:45.496559
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 04:56:09.520920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.60
 ---- batch: 020 ----
mean loss: 202.34
 ---- batch: 030 ----
mean loss: 205.06
 ---- batch: 040 ----
mean loss: 207.85
 ---- batch: 050 ----
mean loss: 203.27
 ---- batch: 060 ----
mean loss: 209.95
 ---- batch: 070 ----
mean loss: 208.80
 ---- batch: 080 ----
mean loss: 202.32
 ---- batch: 090 ----
mean loss: 211.99
train mean loss: 207.45
epoch train time: 0:00:16.692987
elapsed time: 0:29:02.190706
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 04:56:26.215078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.03
 ---- batch: 020 ----
mean loss: 207.70
 ---- batch: 030 ----
mean loss: 204.96
 ---- batch: 040 ----
mean loss: 207.37
 ---- batch: 050 ----
mean loss: 206.86
 ---- batch: 060 ----
mean loss: 207.30
 ---- batch: 070 ----
mean loss: 213.14
 ---- batch: 080 ----
mean loss: 199.60
 ---- batch: 090 ----
mean loss: 207.98
train mean loss: 207.54
epoch train time: 0:00:16.690914
elapsed time: 0:29:18.882892
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 04:56:42.907275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.38
 ---- batch: 020 ----
mean loss: 214.30
 ---- batch: 030 ----
mean loss: 204.87
 ---- batch: 040 ----
mean loss: 211.17
 ---- batch: 050 ----
mean loss: 209.71
 ---- batch: 060 ----
mean loss: 208.59
 ---- batch: 070 ----
mean loss: 196.63
 ---- batch: 080 ----
mean loss: 200.41
 ---- batch: 090 ----
mean loss: 201.83
train mean loss: 206.34
epoch train time: 0:00:16.658561
elapsed time: 0:29:35.542755
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 04:56:59.567115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.82
 ---- batch: 020 ----
mean loss: 201.19
 ---- batch: 030 ----
mean loss: 212.99
 ---- batch: 040 ----
mean loss: 206.45
 ---- batch: 050 ----
mean loss: 199.30
 ---- batch: 060 ----
mean loss: 207.42
 ---- batch: 070 ----
mean loss: 205.05
 ---- batch: 080 ----
mean loss: 209.11
 ---- batch: 090 ----
mean loss: 212.11
train mean loss: 205.98
epoch train time: 0:00:16.710385
elapsed time: 0:29:52.254361
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 04:57:16.278790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.75
 ---- batch: 020 ----
mean loss: 207.21
 ---- batch: 030 ----
mean loss: 204.13
 ---- batch: 040 ----
mean loss: 201.11
 ---- batch: 050 ----
mean loss: 209.52
 ---- batch: 060 ----
mean loss: 209.86
 ---- batch: 070 ----
mean loss: 204.28
 ---- batch: 080 ----
mean loss: 204.40
 ---- batch: 090 ----
mean loss: 204.80
train mean loss: 205.45
epoch train time: 0:00:16.654685
elapsed time: 0:30:08.910704
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 04:57:32.935011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.12
 ---- batch: 020 ----
mean loss: 208.41
 ---- batch: 030 ----
mean loss: 201.08
 ---- batch: 040 ----
mean loss: 204.75
 ---- batch: 050 ----
mean loss: 199.29
 ---- batch: 060 ----
mean loss: 206.58
 ---- batch: 070 ----
mean loss: 204.46
 ---- batch: 080 ----
mean loss: 213.63
 ---- batch: 090 ----
mean loss: 209.96
train mean loss: 205.08
epoch train time: 0:00:16.653969
elapsed time: 0:30:25.565790
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 04:57:49.590153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.33
 ---- batch: 020 ----
mean loss: 208.46
 ---- batch: 030 ----
mean loss: 199.91
 ---- batch: 040 ----
mean loss: 203.25
 ---- batch: 050 ----
mean loss: 198.76
 ---- batch: 060 ----
mean loss: 207.66
 ---- batch: 070 ----
mean loss: 208.59
 ---- batch: 080 ----
mean loss: 207.44
 ---- batch: 090 ----
mean loss: 203.28
train mean loss: 206.02
epoch train time: 0:00:16.642162
elapsed time: 0:30:42.209118
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 04:58:06.233421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.87
 ---- batch: 020 ----
mean loss: 205.29
 ---- batch: 030 ----
mean loss: 207.08
 ---- batch: 040 ----
mean loss: 200.75
 ---- batch: 050 ----
mean loss: 202.17
 ---- batch: 060 ----
mean loss: 206.85
 ---- batch: 070 ----
mean loss: 208.20
 ---- batch: 080 ----
mean loss: 207.04
 ---- batch: 090 ----
mean loss: 205.21
train mean loss: 206.16
epoch train time: 0:00:16.630896
elapsed time: 0:30:58.841140
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 04:58:22.865495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.37
 ---- batch: 020 ----
mean loss: 200.69
 ---- batch: 030 ----
mean loss: 208.72
 ---- batch: 040 ----
mean loss: 203.23
 ---- batch: 050 ----
mean loss: 202.96
 ---- batch: 060 ----
mean loss: 209.25
 ---- batch: 070 ----
mean loss: 200.50
 ---- batch: 080 ----
mean loss: 201.02
 ---- batch: 090 ----
mean loss: 202.60
train mean loss: 204.56
epoch train time: 0:00:16.635243
elapsed time: 0:31:15.477575
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 04:58:39.501956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.84
 ---- batch: 020 ----
mean loss: 200.95
 ---- batch: 030 ----
mean loss: 212.70
 ---- batch: 040 ----
mean loss: 209.40
 ---- batch: 050 ----
mean loss: 206.26
 ---- batch: 060 ----
mean loss: 204.60
 ---- batch: 070 ----
mean loss: 208.72
 ---- batch: 080 ----
mean loss: 199.16
 ---- batch: 090 ----
mean loss: 200.74
train mean loss: 203.70
epoch train time: 0:00:16.659021
elapsed time: 0:31:32.137837
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 04:58:56.162177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.61
 ---- batch: 020 ----
mean loss: 206.55
 ---- batch: 030 ----
mean loss: 199.70
 ---- batch: 040 ----
mean loss: 199.57
 ---- batch: 050 ----
mean loss: 200.53
 ---- batch: 060 ----
mean loss: 207.51
 ---- batch: 070 ----
mean loss: 214.23
 ---- batch: 080 ----
mean loss: 206.07
 ---- batch: 090 ----
mean loss: 208.02
train mean loss: 204.38
epoch train time: 0:00:16.625035
elapsed time: 0:31:48.764016
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 04:59:12.788351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.65
 ---- batch: 020 ----
mean loss: 205.87
 ---- batch: 030 ----
mean loss: 202.42
 ---- batch: 040 ----
mean loss: 211.93
 ---- batch: 050 ----
mean loss: 206.33
 ---- batch: 060 ----
mean loss: 211.29
 ---- batch: 070 ----
mean loss: 194.58
 ---- batch: 080 ----
mean loss: 203.17
 ---- batch: 090 ----
mean loss: 199.70
train mean loss: 205.05
epoch train time: 0:00:16.590926
elapsed time: 0:32:05.356148
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 04:59:29.380473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.84
 ---- batch: 020 ----
mean loss: 206.27
 ---- batch: 030 ----
mean loss: 200.48
 ---- batch: 040 ----
mean loss: 204.52
 ---- batch: 050 ----
mean loss: 205.47
 ---- batch: 060 ----
mean loss: 206.16
 ---- batch: 070 ----
mean loss: 207.13
 ---- batch: 080 ----
mean loss: 191.68
 ---- batch: 090 ----
mean loss: 203.61
train mean loss: 203.41
epoch train time: 0:00:16.636717
elapsed time: 0:32:21.994065
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 04:59:46.018420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.56
 ---- batch: 020 ----
mean loss: 216.10
 ---- batch: 030 ----
mean loss: 214.04
 ---- batch: 040 ----
mean loss: 202.38
 ---- batch: 050 ----
mean loss: 211.77
 ---- batch: 060 ----
mean loss: 203.25
 ---- batch: 070 ----
mean loss: 205.65
 ---- batch: 080 ----
mean loss: 198.69
 ---- batch: 090 ----
mean loss: 202.28
train mean loss: 206.33
epoch train time: 0:00:16.613132
elapsed time: 0:32:38.608375
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 05:00:02.632780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.61
 ---- batch: 020 ----
mean loss: 197.67
 ---- batch: 030 ----
mean loss: 202.72
 ---- batch: 040 ----
mean loss: 198.89
 ---- batch: 050 ----
mean loss: 201.98
 ---- batch: 060 ----
mean loss: 207.92
 ---- batch: 070 ----
mean loss: 210.59
 ---- batch: 080 ----
mean loss: 209.43
 ---- batch: 090 ----
mean loss: 207.35
train mean loss: 202.88
epoch train time: 0:00:16.610654
elapsed time: 0:32:55.220241
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 05:00:19.244617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.33
 ---- batch: 020 ----
mean loss: 196.25
 ---- batch: 030 ----
mean loss: 203.00
 ---- batch: 040 ----
mean loss: 202.05
 ---- batch: 050 ----
mean loss: 199.62
 ---- batch: 060 ----
mean loss: 198.87
 ---- batch: 070 ----
mean loss: 219.38
 ---- batch: 080 ----
mean loss: 211.43
 ---- batch: 090 ----
mean loss: 217.15
train mean loss: 205.34
epoch train time: 0:00:16.617226
elapsed time: 0:33:11.838688
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 05:00:35.863106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.12
 ---- batch: 020 ----
mean loss: 206.89
 ---- batch: 030 ----
mean loss: 204.10
 ---- batch: 040 ----
mean loss: 210.64
 ---- batch: 050 ----
mean loss: 206.23
 ---- batch: 060 ----
mean loss: 201.76
 ---- batch: 070 ----
mean loss: 201.04
 ---- batch: 080 ----
mean loss: 205.75
 ---- batch: 090 ----
mean loss: 196.01
train mean loss: 203.12
epoch train time: 0:00:16.665958
elapsed time: 0:33:28.505946
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 05:00:52.530260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.46
 ---- batch: 020 ----
mean loss: 203.76
 ---- batch: 030 ----
mean loss: 199.19
 ---- batch: 040 ----
mean loss: 205.03
 ---- batch: 050 ----
mean loss: 198.35
 ---- batch: 060 ----
mean loss: 207.71
 ---- batch: 070 ----
mean loss: 201.80
 ---- batch: 080 ----
mean loss: 205.36
 ---- batch: 090 ----
mean loss: 200.96
train mean loss: 203.09
epoch train time: 0:00:16.623800
elapsed time: 0:33:45.130892
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 05:01:09.155272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.24
 ---- batch: 020 ----
mean loss: 201.79
 ---- batch: 030 ----
mean loss: 197.57
 ---- batch: 040 ----
mean loss: 206.35
 ---- batch: 050 ----
mean loss: 205.01
 ---- batch: 060 ----
mean loss: 200.41
 ---- batch: 070 ----
mean loss: 196.10
 ---- batch: 080 ----
mean loss: 205.87
 ---- batch: 090 ----
mean loss: 195.98
train mean loss: 201.49
epoch train time: 0:00:16.622367
elapsed time: 0:34:01.754453
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 05:01:25.778674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.33
 ---- batch: 020 ----
mean loss: 201.05
 ---- batch: 030 ----
mean loss: 207.52
 ---- batch: 040 ----
mean loss: 202.59
 ---- batch: 050 ----
mean loss: 210.55
 ---- batch: 060 ----
mean loss: 202.99
 ---- batch: 070 ----
mean loss: 208.06
 ---- batch: 080 ----
mean loss: 206.61
 ---- batch: 090 ----
mean loss: 196.91
train mean loss: 202.71
epoch train time: 0:00:16.662783
elapsed time: 0:34:18.418341
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 05:01:42.442712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.45
 ---- batch: 020 ----
mean loss: 205.57
 ---- batch: 030 ----
mean loss: 198.64
 ---- batch: 040 ----
mean loss: 204.97
 ---- batch: 050 ----
mean loss: 201.55
 ---- batch: 060 ----
mean loss: 199.81
 ---- batch: 070 ----
mean loss: 193.23
 ---- batch: 080 ----
mean loss: 197.73
 ---- batch: 090 ----
mean loss: 194.55
train mean loss: 200.16
epoch train time: 0:00:16.676981
elapsed time: 0:34:35.096507
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 05:01:59.120830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.70
 ---- batch: 020 ----
mean loss: 190.95
 ---- batch: 030 ----
mean loss: 193.19
 ---- batch: 040 ----
mean loss: 203.67
 ---- batch: 050 ----
mean loss: 204.42
 ---- batch: 060 ----
mean loss: 201.19
 ---- batch: 070 ----
mean loss: 201.48
 ---- batch: 080 ----
mean loss: 201.42
 ---- batch: 090 ----
mean loss: 199.14
train mean loss: 199.78
epoch train time: 0:00:16.648847
elapsed time: 0:34:51.746482
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 05:02:15.770824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.57
 ---- batch: 020 ----
mean loss: 198.86
 ---- batch: 030 ----
mean loss: 193.03
 ---- batch: 040 ----
mean loss: 201.76
 ---- batch: 050 ----
mean loss: 199.13
 ---- batch: 060 ----
mean loss: 203.04
 ---- batch: 070 ----
mean loss: 202.10
 ---- batch: 080 ----
mean loss: 201.23
 ---- batch: 090 ----
mean loss: 203.83
train mean loss: 199.62
epoch train time: 0:00:16.664752
elapsed time: 0:35:08.412386
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 05:02:32.436768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.53
 ---- batch: 020 ----
mean loss: 198.36
 ---- batch: 030 ----
mean loss: 196.23
 ---- batch: 040 ----
mean loss: 197.66
 ---- batch: 050 ----
mean loss: 199.41
 ---- batch: 060 ----
mean loss: 196.81
 ---- batch: 070 ----
mean loss: 200.31
 ---- batch: 080 ----
mean loss: 204.86
 ---- batch: 090 ----
mean loss: 207.26
train mean loss: 199.31
epoch train time: 0:00:16.630030
elapsed time: 0:35:25.043647
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 05:02:49.068061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.61
 ---- batch: 020 ----
mean loss: 200.22
 ---- batch: 030 ----
mean loss: 203.38
 ---- batch: 040 ----
mean loss: 203.97
 ---- batch: 050 ----
mean loss: 199.67
 ---- batch: 060 ----
mean loss: 197.07
 ---- batch: 070 ----
mean loss: 196.65
 ---- batch: 080 ----
mean loss: 200.93
 ---- batch: 090 ----
mean loss: 201.24
train mean loss: 199.50
epoch train time: 0:00:16.679450
elapsed time: 0:35:41.724697
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 05:03:05.748951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.76
 ---- batch: 020 ----
mean loss: 201.46
 ---- batch: 030 ----
mean loss: 198.26
 ---- batch: 040 ----
mean loss: 203.36
 ---- batch: 050 ----
mean loss: 198.57
 ---- batch: 060 ----
mean loss: 197.50
 ---- batch: 070 ----
mean loss: 193.12
 ---- batch: 080 ----
mean loss: 196.04
 ---- batch: 090 ----
mean loss: 205.68
train mean loss: 198.94
epoch train time: 0:00:16.648744
elapsed time: 0:35:58.374533
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 05:03:22.398954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.70
 ---- batch: 020 ----
mean loss: 198.02
 ---- batch: 030 ----
mean loss: 203.22
 ---- batch: 040 ----
mean loss: 205.09
 ---- batch: 050 ----
mean loss: 200.11
 ---- batch: 060 ----
mean loss: 195.00
 ---- batch: 070 ----
mean loss: 200.66
 ---- batch: 080 ----
mean loss: 203.71
 ---- batch: 090 ----
mean loss: 206.66
train mean loss: 201.88
epoch train time: 0:00:16.692200
elapsed time: 0:36:15.067958
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 05:03:39.092288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.99
 ---- batch: 020 ----
mean loss: 197.18
 ---- batch: 030 ----
mean loss: 194.84
 ---- batch: 040 ----
mean loss: 202.81
 ---- batch: 050 ----
mean loss: 199.21
 ---- batch: 060 ----
mean loss: 194.59
 ---- batch: 070 ----
mean loss: 201.50
 ---- batch: 080 ----
mean loss: 204.01
 ---- batch: 090 ----
mean loss: 202.78
train mean loss: 199.72
epoch train time: 0:00:16.694154
elapsed time: 0:36:31.763254
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 05:03:55.787601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.72
 ---- batch: 020 ----
mean loss: 195.68
 ---- batch: 030 ----
mean loss: 197.75
 ---- batch: 040 ----
mean loss: 202.31
 ---- batch: 050 ----
mean loss: 197.70
 ---- batch: 060 ----
mean loss: 195.43
 ---- batch: 070 ----
mean loss: 197.13
 ---- batch: 080 ----
mean loss: 196.64
 ---- batch: 090 ----
mean loss: 205.41
train mean loss: 198.15
epoch train time: 0:00:16.762559
elapsed time: 0:36:48.526953
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 05:04:12.551273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.24
 ---- batch: 020 ----
mean loss: 196.35
 ---- batch: 030 ----
mean loss: 207.67
 ---- batch: 040 ----
mean loss: 210.28
 ---- batch: 050 ----
mean loss: 201.30
 ---- batch: 060 ----
mean loss: 189.28
 ---- batch: 070 ----
mean loss: 198.29
 ---- batch: 080 ----
mean loss: 205.61
 ---- batch: 090 ----
mean loss: 198.69
train mean loss: 201.40
epoch train time: 0:00:16.705202
elapsed time: 0:37:05.233311
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 05:04:29.257701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.22
 ---- batch: 020 ----
mean loss: 199.34
 ---- batch: 030 ----
mean loss: 194.99
 ---- batch: 040 ----
mean loss: 192.36
 ---- batch: 050 ----
mean loss: 198.88
 ---- batch: 060 ----
mean loss: 201.00
 ---- batch: 070 ----
mean loss: 196.72
 ---- batch: 080 ----
mean loss: 203.32
 ---- batch: 090 ----
mean loss: 193.87
train mean loss: 197.73
epoch train time: 0:00:16.698690
elapsed time: 0:37:21.933262
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 05:04:45.957623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.49
 ---- batch: 020 ----
mean loss: 189.75
 ---- batch: 030 ----
mean loss: 193.51
 ---- batch: 040 ----
mean loss: 198.20
 ---- batch: 050 ----
mean loss: 201.07
 ---- batch: 060 ----
mean loss: 194.00
 ---- batch: 070 ----
mean loss: 196.71
 ---- batch: 080 ----
mean loss: 199.97
 ---- batch: 090 ----
mean loss: 202.94
train mean loss: 198.29
epoch train time: 0:00:16.709655
elapsed time: 0:37:38.644188
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 05:05:02.668547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.77
 ---- batch: 020 ----
mean loss: 197.07
 ---- batch: 030 ----
mean loss: 195.32
 ---- batch: 040 ----
mean loss: 195.62
 ---- batch: 050 ----
mean loss: 198.23
 ---- batch: 060 ----
mean loss: 207.72
 ---- batch: 070 ----
mean loss: 197.65
 ---- batch: 080 ----
mean loss: 194.49
 ---- batch: 090 ----
mean loss: 192.97
train mean loss: 197.31
epoch train time: 0:00:16.699609
elapsed time: 0:37:55.344971
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 05:05:19.369308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.95
 ---- batch: 020 ----
mean loss: 201.29
 ---- batch: 030 ----
mean loss: 198.17
 ---- batch: 040 ----
mean loss: 201.75
 ---- batch: 050 ----
mean loss: 192.54
 ---- batch: 060 ----
mean loss: 193.71
 ---- batch: 070 ----
mean loss: 194.02
 ---- batch: 080 ----
mean loss: 196.83
 ---- batch: 090 ----
mean loss: 203.74
train mean loss: 199.14
epoch train time: 0:00:16.702525
elapsed time: 0:38:12.048724
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 05:05:36.073116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.65
 ---- batch: 020 ----
mean loss: 195.06
 ---- batch: 030 ----
mean loss: 200.80
 ---- batch: 040 ----
mean loss: 196.73
 ---- batch: 050 ----
mean loss: 194.18
 ---- batch: 060 ----
mean loss: 199.28
 ---- batch: 070 ----
mean loss: 196.86
 ---- batch: 080 ----
mean loss: 194.81
 ---- batch: 090 ----
mean loss: 197.92
train mean loss: 197.30
epoch train time: 0:00:16.703931
elapsed time: 0:38:28.753901
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 05:05:52.778230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.10
 ---- batch: 020 ----
mean loss: 196.84
 ---- batch: 030 ----
mean loss: 197.25
 ---- batch: 040 ----
mean loss: 198.95
 ---- batch: 050 ----
mean loss: 192.34
 ---- batch: 060 ----
mean loss: 197.78
 ---- batch: 070 ----
mean loss: 203.10
 ---- batch: 080 ----
mean loss: 201.63
 ---- batch: 090 ----
mean loss: 192.06
train mean loss: 197.55
epoch train time: 0:00:16.743736
elapsed time: 0:38:45.498756
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 05:06:09.523098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.23
 ---- batch: 020 ----
mean loss: 205.47
 ---- batch: 030 ----
mean loss: 205.97
 ---- batch: 040 ----
mean loss: 195.51
 ---- batch: 050 ----
mean loss: 203.05
 ---- batch: 060 ----
mean loss: 194.45
 ---- batch: 070 ----
mean loss: 195.83
 ---- batch: 080 ----
mean loss: 196.81
 ---- batch: 090 ----
mean loss: 202.98
train mean loss: 200.28
epoch train time: 0:00:16.716936
elapsed time: 0:39:02.216903
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 05:06:26.241261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.14
 ---- batch: 020 ----
mean loss: 192.90
 ---- batch: 030 ----
mean loss: 199.05
 ---- batch: 040 ----
mean loss: 194.62
 ---- batch: 050 ----
mean loss: 191.48
 ---- batch: 060 ----
mean loss: 198.59
 ---- batch: 070 ----
mean loss: 198.43
 ---- batch: 080 ----
mean loss: 204.11
 ---- batch: 090 ----
mean loss: 191.76
train mean loss: 195.57
epoch train time: 0:00:16.705793
elapsed time: 0:39:18.923915
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 05:06:42.948233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.70
 ---- batch: 020 ----
mean loss: 194.18
 ---- batch: 030 ----
mean loss: 193.57
 ---- batch: 040 ----
mean loss: 197.05
 ---- batch: 050 ----
mean loss: 198.40
 ---- batch: 060 ----
mean loss: 194.94
 ---- batch: 070 ----
mean loss: 196.10
 ---- batch: 080 ----
mean loss: 196.33
 ---- batch: 090 ----
mean loss: 200.42
train mean loss: 195.58
epoch train time: 0:00:16.682772
elapsed time: 0:39:35.607818
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 05:06:59.632185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.97
 ---- batch: 020 ----
mean loss: 197.07
 ---- batch: 030 ----
mean loss: 205.40
 ---- batch: 040 ----
mean loss: 182.52
 ---- batch: 050 ----
mean loss: 201.29
 ---- batch: 060 ----
mean loss: 194.29
 ---- batch: 070 ----
mean loss: 196.31
 ---- batch: 080 ----
mean loss: 191.85
 ---- batch: 090 ----
mean loss: 200.04
train mean loss: 195.51
epoch train time: 0:00:16.706541
elapsed time: 0:39:52.315583
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 05:07:16.339905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.06
 ---- batch: 020 ----
mean loss: 192.02
 ---- batch: 030 ----
mean loss: 199.99
 ---- batch: 040 ----
mean loss: 201.52
 ---- batch: 050 ----
mean loss: 205.06
 ---- batch: 060 ----
mean loss: 201.45
 ---- batch: 070 ----
mean loss: 201.24
 ---- batch: 080 ----
mean loss: 208.63
 ---- batch: 090 ----
mean loss: 198.13
train mean loss: 200.64
epoch train time: 0:00:16.763797
elapsed time: 0:40:09.080494
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 05:07:33.104841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.01
 ---- batch: 020 ----
mean loss: 201.77
 ---- batch: 030 ----
mean loss: 190.26
 ---- batch: 040 ----
mean loss: 199.31
 ---- batch: 050 ----
mean loss: 191.48
 ---- batch: 060 ----
mean loss: 189.38
 ---- batch: 070 ----
mean loss: 197.44
 ---- batch: 080 ----
mean loss: 190.76
 ---- batch: 090 ----
mean loss: 189.54
train mean loss: 195.74
epoch train time: 0:00:16.690736
elapsed time: 0:40:25.772448
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 05:07:49.796850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.77
 ---- batch: 020 ----
mean loss: 202.40
 ---- batch: 030 ----
mean loss: 194.71
 ---- batch: 040 ----
mean loss: 194.71
 ---- batch: 050 ----
mean loss: 186.49
 ---- batch: 060 ----
mean loss: 200.58
 ---- batch: 070 ----
mean loss: 196.76
 ---- batch: 080 ----
mean loss: 190.56
 ---- batch: 090 ----
mean loss: 197.72
train mean loss: 195.11
epoch train time: 0:00:16.716341
elapsed time: 0:40:42.490060
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 05:08:06.514485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.46
 ---- batch: 020 ----
mean loss: 199.13
 ---- batch: 030 ----
mean loss: 190.91
 ---- batch: 040 ----
mean loss: 199.08
 ---- batch: 050 ----
mean loss: 199.78
 ---- batch: 060 ----
mean loss: 198.00
 ---- batch: 070 ----
mean loss: 190.38
 ---- batch: 080 ----
mean loss: 199.56
 ---- batch: 090 ----
mean loss: 192.48
train mean loss: 194.87
epoch train time: 0:00:16.708802
elapsed time: 0:40:59.200067
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 05:08:23.224456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.31
 ---- batch: 020 ----
mean loss: 197.30
 ---- batch: 030 ----
mean loss: 199.34
 ---- batch: 040 ----
mean loss: 198.56
 ---- batch: 050 ----
mean loss: 194.56
 ---- batch: 060 ----
mean loss: 194.18
 ---- batch: 070 ----
mean loss: 194.71
 ---- batch: 080 ----
mean loss: 188.79
 ---- batch: 090 ----
mean loss: 191.77
train mean loss: 194.63
epoch train time: 0:00:16.696609
elapsed time: 0:41:15.897897
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 05:08:39.922275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.45
 ---- batch: 020 ----
mean loss: 200.07
 ---- batch: 030 ----
mean loss: 195.43
 ---- batch: 040 ----
mean loss: 192.84
 ---- batch: 050 ----
mean loss: 194.57
 ---- batch: 060 ----
mean loss: 199.19
 ---- batch: 070 ----
mean loss: 190.08
 ---- batch: 080 ----
mean loss: 198.43
 ---- batch: 090 ----
mean loss: 198.34
train mean loss: 195.25
epoch train time: 0:00:16.680347
elapsed time: 0:41:32.579400
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 05:08:56.603919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.41
 ---- batch: 020 ----
mean loss: 194.61
 ---- batch: 030 ----
mean loss: 194.28
 ---- batch: 040 ----
mean loss: 195.71
 ---- batch: 050 ----
mean loss: 186.14
 ---- batch: 060 ----
mean loss: 191.18
 ---- batch: 070 ----
mean loss: 195.48
 ---- batch: 080 ----
mean loss: 197.80
 ---- batch: 090 ----
mean loss: 196.09
train mean loss: 194.53
epoch train time: 0:00:16.735133
elapsed time: 0:41:49.316566
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 05:09:13.340564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.03
 ---- batch: 020 ----
mean loss: 198.01
 ---- batch: 030 ----
mean loss: 193.03
 ---- batch: 040 ----
mean loss: 194.90
 ---- batch: 050 ----
mean loss: 193.85
 ---- batch: 060 ----
mean loss: 200.43
 ---- batch: 070 ----
mean loss: 196.04
 ---- batch: 080 ----
mean loss: 193.74
 ---- batch: 090 ----
mean loss: 194.05
train mean loss: 194.84
epoch train time: 0:00:16.723984
elapsed time: 0:42:06.041330
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 05:09:30.065696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.43
 ---- batch: 020 ----
mean loss: 200.61
 ---- batch: 030 ----
mean loss: 191.96
 ---- batch: 040 ----
mean loss: 202.39
 ---- batch: 050 ----
mean loss: 197.31
 ---- batch: 060 ----
mean loss: 199.00
 ---- batch: 070 ----
mean loss: 186.38
 ---- batch: 080 ----
mean loss: 199.97
 ---- batch: 090 ----
mean loss: 193.86
train mean loss: 196.63
epoch train time: 0:00:16.655768
elapsed time: 0:42:22.698321
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 05:09:46.722725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.37
 ---- batch: 020 ----
mean loss: 191.63
 ---- batch: 030 ----
mean loss: 191.92
 ---- batch: 040 ----
mean loss: 193.85
 ---- batch: 050 ----
mean loss: 194.14
 ---- batch: 060 ----
mean loss: 211.41
 ---- batch: 070 ----
mean loss: 200.57
 ---- batch: 080 ----
mean loss: 196.77
 ---- batch: 090 ----
mean loss: 200.89
train mean loss: 196.66
epoch train time: 0:00:16.647865
elapsed time: 0:42:39.347355
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 05:10:03.371703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.74
 ---- batch: 020 ----
mean loss: 197.52
 ---- batch: 030 ----
mean loss: 198.99
 ---- batch: 040 ----
mean loss: 189.91
 ---- batch: 050 ----
mean loss: 192.95
 ---- batch: 060 ----
mean loss: 192.73
 ---- batch: 070 ----
mean loss: 196.67
 ---- batch: 080 ----
mean loss: 188.95
 ---- batch: 090 ----
mean loss: 193.02
train mean loss: 193.86
epoch train time: 0:00:16.643314
elapsed time: 0:42:55.991923
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 05:10:20.016278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.68
 ---- batch: 020 ----
mean loss: 189.83
 ---- batch: 030 ----
mean loss: 198.19
 ---- batch: 040 ----
mean loss: 199.14
 ---- batch: 050 ----
mean loss: 192.24
 ---- batch: 060 ----
mean loss: 188.90
 ---- batch: 070 ----
mean loss: 189.82
 ---- batch: 080 ----
mean loss: 188.80
 ---- batch: 090 ----
mean loss: 192.00
train mean loss: 192.85
epoch train time: 0:00:16.596628
elapsed time: 0:43:12.589741
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 05:10:36.614127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.24
 ---- batch: 020 ----
mean loss: 195.43
 ---- batch: 030 ----
mean loss: 188.53
 ---- batch: 040 ----
mean loss: 193.50
 ---- batch: 050 ----
mean loss: 194.24
 ---- batch: 060 ----
mean loss: 203.69
 ---- batch: 070 ----
mean loss: 196.83
 ---- batch: 080 ----
mean loss: 196.02
 ---- batch: 090 ----
mean loss: 196.95
train mean loss: 195.18
epoch train time: 0:00:16.645524
elapsed time: 0:43:29.236442
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 05:10:53.260779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.78
 ---- batch: 020 ----
mean loss: 196.34
 ---- batch: 030 ----
mean loss: 195.43
 ---- batch: 040 ----
mean loss: 190.73
 ---- batch: 050 ----
mean loss: 187.59
 ---- batch: 060 ----
mean loss: 198.31
 ---- batch: 070 ----
mean loss: 191.65
 ---- batch: 080 ----
mean loss: 187.89
 ---- batch: 090 ----
mean loss: 189.05
train mean loss: 192.63
epoch train time: 0:00:16.678058
elapsed time: 0:43:45.915718
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 05:11:09.940092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.03
 ---- batch: 020 ----
mean loss: 187.98
 ---- batch: 030 ----
mean loss: 203.24
 ---- batch: 040 ----
mean loss: 200.75
 ---- batch: 050 ----
mean loss: 184.00
 ---- batch: 060 ----
mean loss: 193.91
 ---- batch: 070 ----
mean loss: 201.14
 ---- batch: 080 ----
mean loss: 212.24
 ---- batch: 090 ----
mean loss: 195.31
train mean loss: 196.39
epoch train time: 0:00:16.715812
elapsed time: 0:44:02.632710
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 05:11:26.657138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.12
 ---- batch: 020 ----
mean loss: 201.12
 ---- batch: 030 ----
mean loss: 195.28
 ---- batch: 040 ----
mean loss: 196.17
 ---- batch: 050 ----
mean loss: 195.92
 ---- batch: 060 ----
mean loss: 198.43
 ---- batch: 070 ----
mean loss: 190.20
 ---- batch: 080 ----
mean loss: 187.76
 ---- batch: 090 ----
mean loss: 188.56
train mean loss: 194.05
epoch train time: 0:00:16.690775
elapsed time: 0:44:19.324786
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 05:11:43.349112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.23
 ---- batch: 020 ----
mean loss: 191.39
 ---- batch: 030 ----
mean loss: 190.88
 ---- batch: 040 ----
mean loss: 187.01
 ---- batch: 050 ----
mean loss: 187.08
 ---- batch: 060 ----
mean loss: 188.41
 ---- batch: 070 ----
mean loss: 194.57
 ---- batch: 080 ----
mean loss: 195.86
 ---- batch: 090 ----
mean loss: 190.96
train mean loss: 192.15
epoch train time: 0:00:16.666418
elapsed time: 0:44:35.992346
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 05:12:00.016774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.32
 ---- batch: 020 ----
mean loss: 184.12
 ---- batch: 030 ----
mean loss: 195.35
 ---- batch: 040 ----
mean loss: 195.37
 ---- batch: 050 ----
mean loss: 197.86
 ---- batch: 060 ----
mean loss: 191.50
 ---- batch: 070 ----
mean loss: 195.95
 ---- batch: 080 ----
mean loss: 194.81
 ---- batch: 090 ----
mean loss: 192.58
train mean loss: 192.61
epoch train time: 0:00:16.630985
elapsed time: 0:44:52.624550
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 05:12:16.648911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.58
 ---- batch: 020 ----
mean loss: 194.02
 ---- batch: 030 ----
mean loss: 187.61
 ---- batch: 040 ----
mean loss: 194.62
 ---- batch: 050 ----
mean loss: 193.15
 ---- batch: 060 ----
mean loss: 191.54
 ---- batch: 070 ----
mean loss: 201.57
 ---- batch: 080 ----
mean loss: 191.18
 ---- batch: 090 ----
mean loss: 190.54
train mean loss: 192.36
epoch train time: 0:00:16.676708
elapsed time: 0:45:09.302458
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 05:12:33.326779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.31
 ---- batch: 020 ----
mean loss: 187.18
 ---- batch: 030 ----
mean loss: 194.76
 ---- batch: 040 ----
mean loss: 191.83
 ---- batch: 050 ----
mean loss: 192.90
 ---- batch: 060 ----
mean loss: 200.97
 ---- batch: 070 ----
mean loss: 198.61
 ---- batch: 080 ----
mean loss: 193.65
 ---- batch: 090 ----
mean loss: 192.80
train mean loss: 193.64
epoch train time: 0:00:16.631896
elapsed time: 0:45:25.935563
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 05:12:49.959921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.77
 ---- batch: 020 ----
mean loss: 200.85
 ---- batch: 030 ----
mean loss: 199.63
 ---- batch: 040 ----
mean loss: 203.06
 ---- batch: 050 ----
mean loss: 189.26
 ---- batch: 060 ----
mean loss: 192.96
 ---- batch: 070 ----
mean loss: 192.12
 ---- batch: 080 ----
mean loss: 195.23
 ---- batch: 090 ----
mean loss: 187.46
train mean loss: 194.18
epoch train time: 0:00:16.550661
elapsed time: 0:45:42.487436
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 05:13:06.511880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.41
 ---- batch: 020 ----
mean loss: 184.92
 ---- batch: 030 ----
mean loss: 189.88
 ---- batch: 040 ----
mean loss: 192.82
 ---- batch: 050 ----
mean loss: 193.29
 ---- batch: 060 ----
mean loss: 194.05
 ---- batch: 070 ----
mean loss: 192.50
 ---- batch: 080 ----
mean loss: 186.11
 ---- batch: 090 ----
mean loss: 191.79
train mean loss: 191.55
epoch train time: 0:00:16.582757
elapsed time: 0:45:59.071476
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 05:13:23.095815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.92
 ---- batch: 020 ----
mean loss: 187.66
 ---- batch: 030 ----
mean loss: 188.80
 ---- batch: 040 ----
mean loss: 186.03
 ---- batch: 050 ----
mean loss: 196.73
 ---- batch: 060 ----
mean loss: 193.02
 ---- batch: 070 ----
mean loss: 195.67
 ---- batch: 080 ----
mean loss: 192.07
 ---- batch: 090 ----
mean loss: 198.58
train mean loss: 192.04
epoch train time: 0:00:16.571727
elapsed time: 0:46:15.644422
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 05:13:39.668795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.01
 ---- batch: 020 ----
mean loss: 190.27
 ---- batch: 030 ----
mean loss: 186.73
 ---- batch: 040 ----
mean loss: 203.06
 ---- batch: 050 ----
mean loss: 190.94
 ---- batch: 060 ----
mean loss: 200.28
 ---- batch: 070 ----
mean loss: 188.80
 ---- batch: 080 ----
mean loss: 191.96
 ---- batch: 090 ----
mean loss: 194.10
train mean loss: 192.68
epoch train time: 0:00:16.581971
elapsed time: 0:46:32.227647
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 05:13:56.252003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.81
 ---- batch: 020 ----
mean loss: 190.85
 ---- batch: 030 ----
mean loss: 194.50
 ---- batch: 040 ----
mean loss: 190.43
 ---- batch: 050 ----
mean loss: 189.30
 ---- batch: 060 ----
mean loss: 187.69
 ---- batch: 070 ----
mean loss: 180.98
 ---- batch: 080 ----
mean loss: 195.31
 ---- batch: 090 ----
mean loss: 200.28
train mean loss: 191.78
epoch train time: 0:00:16.600795
elapsed time: 0:46:48.829606
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 05:14:12.853971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.81
 ---- batch: 020 ----
mean loss: 189.26
 ---- batch: 030 ----
mean loss: 195.34
 ---- batch: 040 ----
mean loss: 187.98
 ---- batch: 050 ----
mean loss: 191.49
 ---- batch: 060 ----
mean loss: 188.06
 ---- batch: 070 ----
mean loss: 193.19
 ---- batch: 080 ----
mean loss: 195.58
 ---- batch: 090 ----
mean loss: 189.97
train mean loss: 191.71
epoch train time: 0:00:16.633917
elapsed time: 0:47:05.464701
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 05:14:29.489129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.84
 ---- batch: 020 ----
mean loss: 194.06
 ---- batch: 030 ----
mean loss: 186.25
 ---- batch: 040 ----
mean loss: 194.65
 ---- batch: 050 ----
mean loss: 196.19
 ---- batch: 060 ----
mean loss: 204.88
 ---- batch: 070 ----
mean loss: 185.09
 ---- batch: 080 ----
mean loss: 194.09
 ---- batch: 090 ----
mean loss: 191.68
train mean loss: 193.56
epoch train time: 0:00:16.606325
elapsed time: 0:47:22.072240
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 05:14:46.096605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.14
 ---- batch: 020 ----
mean loss: 187.00
 ---- batch: 030 ----
mean loss: 186.10
 ---- batch: 040 ----
mean loss: 189.35
 ---- batch: 050 ----
mean loss: 189.95
 ---- batch: 060 ----
mean loss: 196.34
 ---- batch: 070 ----
mean loss: 193.27
 ---- batch: 080 ----
mean loss: 194.03
 ---- batch: 090 ----
mean loss: 186.70
train mean loss: 190.55
epoch train time: 0:00:16.602551
elapsed time: 0:47:38.676066
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 05:15:02.700408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.96
 ---- batch: 020 ----
mean loss: 191.99
 ---- batch: 030 ----
mean loss: 190.07
 ---- batch: 040 ----
mean loss: 191.04
 ---- batch: 050 ----
mean loss: 201.97
 ---- batch: 060 ----
mean loss: 187.93
 ---- batch: 070 ----
mean loss: 191.18
 ---- batch: 080 ----
mean loss: 181.39
 ---- batch: 090 ----
mean loss: 193.95
train mean loss: 192.34
epoch train time: 0:00:16.625143
elapsed time: 0:47:55.302494
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 05:15:19.327108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.81
 ---- batch: 020 ----
mean loss: 188.77
 ---- batch: 030 ----
mean loss: 187.07
 ---- batch: 040 ----
mean loss: 191.56
 ---- batch: 050 ----
mean loss: 189.01
 ---- batch: 060 ----
mean loss: 192.41
 ---- batch: 070 ----
mean loss: 197.15
 ---- batch: 080 ----
mean loss: 188.14
 ---- batch: 090 ----
mean loss: 196.48
train mean loss: 190.82
epoch train time: 0:00:16.575154
elapsed time: 0:48:11.879216
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 05:15:35.903621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.38
 ---- batch: 020 ----
mean loss: 188.01
 ---- batch: 030 ----
mean loss: 190.71
 ---- batch: 040 ----
mean loss: 193.14
 ---- batch: 050 ----
mean loss: 187.30
 ---- batch: 060 ----
mean loss: 189.77
 ---- batch: 070 ----
mean loss: 193.13
 ---- batch: 080 ----
mean loss: 190.15
 ---- batch: 090 ----
mean loss: 186.15
train mean loss: 190.70
epoch train time: 0:00:16.548260
elapsed time: 0:48:28.428702
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 05:15:52.453138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.59
 ---- batch: 020 ----
mean loss: 185.64
 ---- batch: 030 ----
mean loss: 192.61
 ---- batch: 040 ----
mean loss: 183.81
 ---- batch: 050 ----
mean loss: 186.30
 ---- batch: 060 ----
mean loss: 194.54
 ---- batch: 070 ----
mean loss: 192.91
 ---- batch: 080 ----
mean loss: 200.13
 ---- batch: 090 ----
mean loss: 185.58
train mean loss: 189.56
epoch train time: 0:00:16.562967
elapsed time: 0:48:44.993168
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 05:16:09.017149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.32
 ---- batch: 020 ----
mean loss: 197.37
 ---- batch: 030 ----
mean loss: 197.84
 ---- batch: 040 ----
mean loss: 182.48
 ---- batch: 050 ----
mean loss: 188.80
 ---- batch: 060 ----
mean loss: 198.35
 ---- batch: 070 ----
mean loss: 187.99
 ---- batch: 080 ----
mean loss: 191.24
 ---- batch: 090 ----
mean loss: 191.58
train mean loss: 191.52
epoch train time: 0:00:16.596120
elapsed time: 0:49:01.590211
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 05:16:25.614576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.08
 ---- batch: 020 ----
mean loss: 184.87
 ---- batch: 030 ----
mean loss: 187.73
 ---- batch: 040 ----
mean loss: 191.63
 ---- batch: 050 ----
mean loss: 191.73
 ---- batch: 060 ----
mean loss: 192.23
 ---- batch: 070 ----
mean loss: 192.72
 ---- batch: 080 ----
mean loss: 184.24
 ---- batch: 090 ----
mean loss: 189.73
train mean loss: 189.66
epoch train time: 0:00:16.616248
elapsed time: 0:49:18.207645
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 05:16:42.232007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.50
 ---- batch: 020 ----
mean loss: 190.55
 ---- batch: 030 ----
mean loss: 187.90
 ---- batch: 040 ----
mean loss: 191.11
 ---- batch: 050 ----
mean loss: 194.65
 ---- batch: 060 ----
mean loss: 188.11
 ---- batch: 070 ----
mean loss: 182.27
 ---- batch: 080 ----
mean loss: 181.98
 ---- batch: 090 ----
mean loss: 193.33
train mean loss: 188.35
epoch train time: 0:00:16.569587
elapsed time: 0:49:34.778441
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 05:16:58.802777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.91
 ---- batch: 020 ----
mean loss: 190.74
 ---- batch: 030 ----
mean loss: 190.67
 ---- batch: 040 ----
mean loss: 185.08
 ---- batch: 050 ----
mean loss: 182.77
 ---- batch: 060 ----
mean loss: 190.82
 ---- batch: 070 ----
mean loss: 194.39
 ---- batch: 080 ----
mean loss: 184.52
 ---- batch: 090 ----
mean loss: 184.38
train mean loss: 188.36
epoch train time: 0:00:16.552691
elapsed time: 0:49:51.332281
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 05:17:15.356624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.18
 ---- batch: 020 ----
mean loss: 191.04
 ---- batch: 030 ----
mean loss: 191.65
 ---- batch: 040 ----
mean loss: 189.55
 ---- batch: 050 ----
mean loss: 187.06
 ---- batch: 060 ----
mean loss: 191.24
 ---- batch: 070 ----
mean loss: 188.67
 ---- batch: 080 ----
mean loss: 177.47
 ---- batch: 090 ----
mean loss: 189.38
train mean loss: 188.51
epoch train time: 0:00:16.575459
elapsed time: 0:50:07.908966
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 05:17:31.933365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.71
 ---- batch: 020 ----
mean loss: 188.60
 ---- batch: 030 ----
mean loss: 191.00
 ---- batch: 040 ----
mean loss: 184.18
 ---- batch: 050 ----
mean loss: 191.92
 ---- batch: 060 ----
mean loss: 188.60
 ---- batch: 070 ----
mean loss: 185.06
 ---- batch: 080 ----
mean loss: 188.86
 ---- batch: 090 ----
mean loss: 190.87
train mean loss: 188.59
epoch train time: 0:00:16.566490
elapsed time: 0:50:24.476675
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 05:17:48.501022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.29
 ---- batch: 020 ----
mean loss: 187.94
 ---- batch: 030 ----
mean loss: 189.00
 ---- batch: 040 ----
mean loss: 191.78
 ---- batch: 050 ----
mean loss: 188.77
 ---- batch: 060 ----
mean loss: 181.42
 ---- batch: 070 ----
mean loss: 184.09
 ---- batch: 080 ----
mean loss: 187.28
 ---- batch: 090 ----
mean loss: 186.10
train mean loss: 188.21
epoch train time: 0:00:16.590527
elapsed time: 0:50:41.068583
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 05:18:05.092941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.91
 ---- batch: 020 ----
mean loss: 191.06
 ---- batch: 030 ----
mean loss: 188.31
 ---- batch: 040 ----
mean loss: 188.86
 ---- batch: 050 ----
mean loss: 185.17
 ---- batch: 060 ----
mean loss: 192.32
 ---- batch: 070 ----
mean loss: 187.26
 ---- batch: 080 ----
mean loss: 188.77
 ---- batch: 090 ----
mean loss: 185.09
train mean loss: 188.40
epoch train time: 0:00:16.585603
elapsed time: 0:50:57.655379
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 05:18:21.679720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.67
 ---- batch: 020 ----
mean loss: 186.91
 ---- batch: 030 ----
mean loss: 195.72
 ---- batch: 040 ----
mean loss: 192.92
 ---- batch: 050 ----
mean loss: 184.02
 ---- batch: 060 ----
mean loss: 194.37
 ---- batch: 070 ----
mean loss: 186.39
 ---- batch: 080 ----
mean loss: 189.67
 ---- batch: 090 ----
mean loss: 181.77
train mean loss: 189.14
epoch train time: 0:00:16.515775
elapsed time: 0:51:14.172268
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 05:18:38.196605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.79
 ---- batch: 020 ----
mean loss: 190.57
 ---- batch: 030 ----
mean loss: 184.39
 ---- batch: 040 ----
mean loss: 192.00
 ---- batch: 050 ----
mean loss: 190.58
 ---- batch: 060 ----
mean loss: 188.32
 ---- batch: 070 ----
mean loss: 186.74
 ---- batch: 080 ----
mean loss: 192.77
 ---- batch: 090 ----
mean loss: 186.36
train mean loss: 188.92
epoch train time: 0:00:16.517965
elapsed time: 0:51:30.691572
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 05:18:54.715945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.03
 ---- batch: 020 ----
mean loss: 190.37
 ---- batch: 030 ----
mean loss: 183.95
 ---- batch: 040 ----
mean loss: 189.69
 ---- batch: 050 ----
mean loss: 188.25
 ---- batch: 060 ----
mean loss: 192.07
 ---- batch: 070 ----
mean loss: 185.36
 ---- batch: 080 ----
mean loss: 183.50
 ---- batch: 090 ----
mean loss: 188.52
train mean loss: 188.03
epoch train time: 0:00:16.531611
elapsed time: 0:51:47.224455
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 05:19:11.248801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.74
 ---- batch: 020 ----
mean loss: 183.55
 ---- batch: 030 ----
mean loss: 181.96
 ---- batch: 040 ----
mean loss: 194.09
 ---- batch: 050 ----
mean loss: 199.37
 ---- batch: 060 ----
mean loss: 191.39
 ---- batch: 070 ----
mean loss: 189.79
 ---- batch: 080 ----
mean loss: 184.51
 ---- batch: 090 ----
mean loss: 189.13
train mean loss: 189.13
epoch train time: 0:00:16.551940
elapsed time: 0:52:03.777647
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 05:19:27.802097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.54
 ---- batch: 020 ----
mean loss: 182.57
 ---- batch: 030 ----
mean loss: 190.83
 ---- batch: 040 ----
mean loss: 185.22
 ---- batch: 050 ----
mean loss: 185.10
 ---- batch: 060 ----
mean loss: 191.33
 ---- batch: 070 ----
mean loss: 193.08
 ---- batch: 080 ----
mean loss: 184.17
 ---- batch: 090 ----
mean loss: 195.53
train mean loss: 188.18
epoch train time: 0:00:16.575791
elapsed time: 0:52:20.354716
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 05:19:44.379058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.43
 ---- batch: 020 ----
mean loss: 195.64
 ---- batch: 030 ----
mean loss: 186.96
 ---- batch: 040 ----
mean loss: 189.10
 ---- batch: 050 ----
mean loss: 189.13
 ---- batch: 060 ----
mean loss: 185.28
 ---- batch: 070 ----
mean loss: 189.44
 ---- batch: 080 ----
mean loss: 191.76
 ---- batch: 090 ----
mean loss: 187.91
train mean loss: 189.13
epoch train time: 0:00:16.507987
elapsed time: 0:52:36.864052
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 05:20:00.888289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.35
 ---- batch: 020 ----
mean loss: 193.02
 ---- batch: 030 ----
mean loss: 183.51
 ---- batch: 040 ----
mean loss: 181.73
 ---- batch: 050 ----
mean loss: 186.30
 ---- batch: 060 ----
mean loss: 187.68
 ---- batch: 070 ----
mean loss: 191.72
 ---- batch: 080 ----
mean loss: 190.55
 ---- batch: 090 ----
mean loss: 188.05
train mean loss: 187.70
epoch train time: 0:00:16.471308
elapsed time: 0:52:53.336382
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 05:20:17.360703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.41
 ---- batch: 020 ----
mean loss: 187.77
 ---- batch: 030 ----
mean loss: 206.66
 ---- batch: 040 ----
mean loss: 190.96
 ---- batch: 050 ----
mean loss: 190.13
 ---- batch: 060 ----
mean loss: 179.89
 ---- batch: 070 ----
mean loss: 185.70
 ---- batch: 080 ----
mean loss: 194.19
 ---- batch: 090 ----
mean loss: 193.24
train mean loss: 190.15
epoch train time: 0:00:16.542459
elapsed time: 0:53:09.879960
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 05:20:33.904438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.81
 ---- batch: 020 ----
mean loss: 186.04
 ---- batch: 030 ----
mean loss: 191.66
 ---- batch: 040 ----
mean loss: 200.48
 ---- batch: 050 ----
mean loss: 196.43
 ---- batch: 060 ----
mean loss: 185.82
 ---- batch: 070 ----
mean loss: 186.88
 ---- batch: 080 ----
mean loss: 189.46
 ---- batch: 090 ----
mean loss: 179.18
train mean loss: 189.17
epoch train time: 0:00:16.534907
elapsed time: 0:53:26.416155
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 05:20:50.440470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.86
 ---- batch: 020 ----
mean loss: 189.60
 ---- batch: 030 ----
mean loss: 191.62
 ---- batch: 040 ----
mean loss: 182.28
 ---- batch: 050 ----
mean loss: 185.31
 ---- batch: 060 ----
mean loss: 190.42
 ---- batch: 070 ----
mean loss: 186.85
 ---- batch: 080 ----
mean loss: 183.02
 ---- batch: 090 ----
mean loss: 185.77
train mean loss: 186.79
epoch train time: 0:00:16.537258
elapsed time: 0:53:42.954577
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 05:21:06.978988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.97
 ---- batch: 020 ----
mean loss: 191.63
 ---- batch: 030 ----
mean loss: 185.92
 ---- batch: 040 ----
mean loss: 185.43
 ---- batch: 050 ----
mean loss: 189.73
 ---- batch: 060 ----
mean loss: 190.63
 ---- batch: 070 ----
mean loss: 181.58
 ---- batch: 080 ----
mean loss: 181.12
 ---- batch: 090 ----
mean loss: 186.10
train mean loss: 186.56
epoch train time: 0:00:16.572995
elapsed time: 0:53:59.528897
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 05:21:23.553266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.57
 ---- batch: 020 ----
mean loss: 180.03
 ---- batch: 030 ----
mean loss: 186.68
 ---- batch: 040 ----
mean loss: 186.93
 ---- batch: 050 ----
mean loss: 182.90
 ---- batch: 060 ----
mean loss: 178.90
 ---- batch: 070 ----
mean loss: 193.85
 ---- batch: 080 ----
mean loss: 191.68
 ---- batch: 090 ----
mean loss: 194.55
train mean loss: 187.12
epoch train time: 0:00:16.592333
elapsed time: 0:54:16.122553
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 05:21:40.146868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.67
 ---- batch: 020 ----
mean loss: 186.09
 ---- batch: 030 ----
mean loss: 189.16
 ---- batch: 040 ----
mean loss: 188.49
 ---- batch: 050 ----
mean loss: 179.37
 ---- batch: 060 ----
mean loss: 187.44
 ---- batch: 070 ----
mean loss: 187.85
 ---- batch: 080 ----
mean loss: 188.42
 ---- batch: 090 ----
mean loss: 183.68
train mean loss: 186.54
epoch train time: 0:00:16.582888
elapsed time: 0:54:32.706580
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 05:21:56.731007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.73
 ---- batch: 020 ----
mean loss: 189.18
 ---- batch: 030 ----
mean loss: 192.11
 ---- batch: 040 ----
mean loss: 183.58
 ---- batch: 050 ----
mean loss: 184.85
 ---- batch: 060 ----
mean loss: 189.31
 ---- batch: 070 ----
mean loss: 188.46
 ---- batch: 080 ----
mean loss: 184.16
 ---- batch: 090 ----
mean loss: 184.43
train mean loss: 186.20
epoch train time: 0:00:16.568798
elapsed time: 0:54:49.276572
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 05:22:13.300890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.81
 ---- batch: 020 ----
mean loss: 192.37
 ---- batch: 030 ----
mean loss: 185.81
 ---- batch: 040 ----
mean loss: 185.51
 ---- batch: 050 ----
mean loss: 186.04
 ---- batch: 060 ----
mean loss: 184.60
 ---- batch: 070 ----
mean loss: 184.10
 ---- batch: 080 ----
mean loss: 179.18
 ---- batch: 090 ----
mean loss: 188.04
train mean loss: 186.22
epoch train time: 0:00:16.576240
elapsed time: 0:55:05.853948
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 05:22:29.878277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.68
 ---- batch: 020 ----
mean loss: 186.53
 ---- batch: 030 ----
mean loss: 187.91
 ---- batch: 040 ----
mean loss: 185.04
 ---- batch: 050 ----
mean loss: 185.23
 ---- batch: 060 ----
mean loss: 194.03
 ---- batch: 070 ----
mean loss: 181.46
 ---- batch: 080 ----
mean loss: 183.22
 ---- batch: 090 ----
mean loss: 192.74
train mean loss: 186.37
epoch train time: 0:00:16.572682
elapsed time: 0:55:22.427864
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 05:22:46.452246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.91
 ---- batch: 020 ----
mean loss: 182.64
 ---- batch: 030 ----
mean loss: 187.26
 ---- batch: 040 ----
mean loss: 192.40
 ---- batch: 050 ----
mean loss: 193.26
 ---- batch: 060 ----
mean loss: 186.69
 ---- batch: 070 ----
mean loss: 186.85
 ---- batch: 080 ----
mean loss: 190.82
 ---- batch: 090 ----
mean loss: 180.28
train mean loss: 186.27
epoch train time: 0:00:16.545946
elapsed time: 0:55:38.975058
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 05:23:02.999380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.82
 ---- batch: 020 ----
mean loss: 178.55
 ---- batch: 030 ----
mean loss: 188.65
 ---- batch: 040 ----
mean loss: 181.95
 ---- batch: 050 ----
mean loss: 184.06
 ---- batch: 060 ----
mean loss: 187.43
 ---- batch: 070 ----
mean loss: 185.21
 ---- batch: 080 ----
mean loss: 189.94
 ---- batch: 090 ----
mean loss: 185.75
train mean loss: 184.87
epoch train time: 0:00:16.605536
elapsed time: 0:55:55.581720
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 05:23:19.606071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.44
 ---- batch: 020 ----
mean loss: 185.90
 ---- batch: 030 ----
mean loss: 177.54
 ---- batch: 040 ----
mean loss: 185.35
 ---- batch: 050 ----
mean loss: 186.64
 ---- batch: 060 ----
mean loss: 184.63
 ---- batch: 070 ----
mean loss: 191.51
 ---- batch: 080 ----
mean loss: 189.10
 ---- batch: 090 ----
mean loss: 190.81
train mean loss: 186.36
epoch train time: 0:00:16.645352
elapsed time: 0:56:12.228370
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 05:23:36.252767
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.39
 ---- batch: 020 ----
mean loss: 181.31
 ---- batch: 030 ----
mean loss: 181.25
 ---- batch: 040 ----
mean loss: 188.83
 ---- batch: 050 ----
mean loss: 180.76
 ---- batch: 060 ----
mean loss: 177.86
 ---- batch: 070 ----
mean loss: 181.25
 ---- batch: 080 ----
mean loss: 185.37
 ---- batch: 090 ----
mean loss: 185.36
train mean loss: 183.74
epoch train time: 0:00:16.604814
elapsed time: 0:56:28.835004
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 05:23:52.858965
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.66
 ---- batch: 020 ----
mean loss: 178.15
 ---- batch: 030 ----
mean loss: 183.17
 ---- batch: 040 ----
mean loss: 175.69
 ---- batch: 050 ----
mean loss: 184.96
 ---- batch: 060 ----
mean loss: 182.82
 ---- batch: 070 ----
mean loss: 178.01
 ---- batch: 080 ----
mean loss: 184.42
 ---- batch: 090 ----
mean loss: 180.71
train mean loss: 181.81
epoch train time: 0:00:16.564622
elapsed time: 0:56:45.400368
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 05:24:09.424718
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.45
 ---- batch: 020 ----
mean loss: 185.53
 ---- batch: 030 ----
mean loss: 178.24
 ---- batch: 040 ----
mean loss: 184.08
 ---- batch: 050 ----
mean loss: 185.24
 ---- batch: 060 ----
mean loss: 174.31
 ---- batch: 070 ----
mean loss: 185.39
 ---- batch: 080 ----
mean loss: 181.15
 ---- batch: 090 ----
mean loss: 180.51
train mean loss: 181.35
epoch train time: 0:00:16.610195
elapsed time: 0:57:02.011741
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 05:24:26.036061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.87
 ---- batch: 020 ----
mean loss: 181.35
 ---- batch: 030 ----
mean loss: 187.43
 ---- batch: 040 ----
mean loss: 180.24
 ---- batch: 050 ----
mean loss: 178.06
 ---- batch: 060 ----
mean loss: 180.78
 ---- batch: 070 ----
mean loss: 180.73
 ---- batch: 080 ----
mean loss: 192.83
 ---- batch: 090 ----
mean loss: 177.64
train mean loss: 181.46
epoch train time: 0:00:16.588960
elapsed time: 0:57:18.601880
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 05:24:42.626280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.66
 ---- batch: 020 ----
mean loss: 184.58
 ---- batch: 030 ----
mean loss: 188.87
 ---- batch: 040 ----
mean loss: 172.39
 ---- batch: 050 ----
mean loss: 182.64
 ---- batch: 060 ----
mean loss: 183.43
 ---- batch: 070 ----
mean loss: 178.17
 ---- batch: 080 ----
mean loss: 189.67
 ---- batch: 090 ----
mean loss: 177.14
train mean loss: 181.60
epoch train time: 0:00:16.671530
elapsed time: 0:57:35.274608
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 05:24:59.298966
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.40
 ---- batch: 020 ----
mean loss: 176.82
 ---- batch: 030 ----
mean loss: 178.67
 ---- batch: 040 ----
mean loss: 187.96
 ---- batch: 050 ----
mean loss: 180.08
 ---- batch: 060 ----
mean loss: 180.31
 ---- batch: 070 ----
mean loss: 188.84
 ---- batch: 080 ----
mean loss: 181.46
 ---- batch: 090 ----
mean loss: 182.40
train mean loss: 181.66
epoch train time: 0:00:16.601041
elapsed time: 0:57:51.877048
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 05:25:15.901521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.06
 ---- batch: 020 ----
mean loss: 175.67
 ---- batch: 030 ----
mean loss: 183.68
 ---- batch: 040 ----
mean loss: 186.12
 ---- batch: 050 ----
mean loss: 182.49
 ---- batch: 060 ----
mean loss: 178.56
 ---- batch: 070 ----
mean loss: 180.91
 ---- batch: 080 ----
mean loss: 177.32
 ---- batch: 090 ----
mean loss: 182.30
train mean loss: 181.70
epoch train time: 0:00:16.652592
elapsed time: 0:58:08.530915
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 05:25:32.555275
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.41
 ---- batch: 020 ----
mean loss: 182.42
 ---- batch: 030 ----
mean loss: 181.38
 ---- batch: 040 ----
mean loss: 189.86
 ---- batch: 050 ----
mean loss: 183.31
 ---- batch: 060 ----
mean loss: 178.25
 ---- batch: 070 ----
mean loss: 179.81
 ---- batch: 080 ----
mean loss: 183.32
 ---- batch: 090 ----
mean loss: 176.51
train mean loss: 181.59
epoch train time: 0:00:16.607408
elapsed time: 0:58:25.139538
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 05:25:49.163907
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.72
 ---- batch: 020 ----
mean loss: 185.43
 ---- batch: 030 ----
mean loss: 183.32
 ---- batch: 040 ----
mean loss: 173.21
 ---- batch: 050 ----
mean loss: 186.86
 ---- batch: 060 ----
mean loss: 189.02
 ---- batch: 070 ----
mean loss: 178.93
 ---- batch: 080 ----
mean loss: 178.85
 ---- batch: 090 ----
mean loss: 178.94
train mean loss: 181.79
epoch train time: 0:00:16.725286
elapsed time: 0:58:41.866102
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 05:26:05.890537
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.30
 ---- batch: 020 ----
mean loss: 183.88
 ---- batch: 030 ----
mean loss: 183.47
 ---- batch: 040 ----
mean loss: 179.68
 ---- batch: 050 ----
mean loss: 177.74
 ---- batch: 060 ----
mean loss: 187.88
 ---- batch: 070 ----
mean loss: 179.01
 ---- batch: 080 ----
mean loss: 180.19
 ---- batch: 090 ----
mean loss: 179.49
train mean loss: 181.27
epoch train time: 0:00:16.585456
elapsed time: 0:58:58.452749
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 05:26:22.477132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.69
 ---- batch: 020 ----
mean loss: 175.27
 ---- batch: 030 ----
mean loss: 183.15
 ---- batch: 040 ----
mean loss: 180.66
 ---- batch: 050 ----
mean loss: 179.35
 ---- batch: 060 ----
mean loss: 179.95
 ---- batch: 070 ----
mean loss: 187.07
 ---- batch: 080 ----
mean loss: 182.56
 ---- batch: 090 ----
mean loss: 183.40
train mean loss: 181.28
epoch train time: 0:00:16.543491
elapsed time: 0:59:14.997430
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 05:26:39.021849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.82
 ---- batch: 020 ----
mean loss: 181.40
 ---- batch: 030 ----
mean loss: 179.11
 ---- batch: 040 ----
mean loss: 186.45
 ---- batch: 050 ----
mean loss: 179.12
 ---- batch: 060 ----
mean loss: 174.51
 ---- batch: 070 ----
mean loss: 184.38
 ---- batch: 080 ----
mean loss: 183.83
 ---- batch: 090 ----
mean loss: 181.11
train mean loss: 181.59
epoch train time: 0:00:16.527247
elapsed time: 0:59:31.525878
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 05:26:55.550181
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.86
 ---- batch: 020 ----
mean loss: 184.70
 ---- batch: 030 ----
mean loss: 178.19
 ---- batch: 040 ----
mean loss: 185.26
 ---- batch: 050 ----
mean loss: 182.19
 ---- batch: 060 ----
mean loss: 183.45
 ---- batch: 070 ----
mean loss: 178.46
 ---- batch: 080 ----
mean loss: 180.76
 ---- batch: 090 ----
mean loss: 182.26
train mean loss: 181.25
epoch train time: 0:00:16.506296
elapsed time: 0:59:48.033244
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 05:27:12.057562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.48
 ---- batch: 020 ----
mean loss: 170.19
 ---- batch: 030 ----
mean loss: 178.29
 ---- batch: 040 ----
mean loss: 180.57
 ---- batch: 050 ----
mean loss: 181.25
 ---- batch: 060 ----
mean loss: 184.68
 ---- batch: 070 ----
mean loss: 189.49
 ---- batch: 080 ----
mean loss: 188.28
 ---- batch: 090 ----
mean loss: 182.60
train mean loss: 181.73
epoch train time: 0:00:16.510029
elapsed time: 1:00:04.544365
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 05:27:28.568830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.25
 ---- batch: 020 ----
mean loss: 183.22
 ---- batch: 030 ----
mean loss: 176.97
 ---- batch: 040 ----
mean loss: 180.74
 ---- batch: 050 ----
mean loss: 181.81
 ---- batch: 060 ----
mean loss: 182.63
 ---- batch: 070 ----
mean loss: 181.11
 ---- batch: 080 ----
mean loss: 182.12
 ---- batch: 090 ----
mean loss: 178.10
train mean loss: 181.56
epoch train time: 0:00:16.575331
elapsed time: 1:00:21.120948
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 05:27:45.145290
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.13
 ---- batch: 020 ----
mean loss: 179.20
 ---- batch: 030 ----
mean loss: 187.17
 ---- batch: 040 ----
mean loss: 181.73
 ---- batch: 050 ----
mean loss: 181.70
 ---- batch: 060 ----
mean loss: 182.43
 ---- batch: 070 ----
mean loss: 179.56
 ---- batch: 080 ----
mean loss: 184.46
 ---- batch: 090 ----
mean loss: 177.62
train mean loss: 181.67
epoch train time: 0:00:16.513594
elapsed time: 1:00:37.635703
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 05:28:01.660055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.08
 ---- batch: 020 ----
mean loss: 179.24
 ---- batch: 030 ----
mean loss: 183.16
 ---- batch: 040 ----
mean loss: 181.67
 ---- batch: 050 ----
mean loss: 185.70
 ---- batch: 060 ----
mean loss: 178.40
 ---- batch: 070 ----
mean loss: 177.82
 ---- batch: 080 ----
mean loss: 185.97
 ---- batch: 090 ----
mean loss: 182.01
train mean loss: 181.42
epoch train time: 0:00:16.580865
elapsed time: 1:00:54.217750
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 05:28:18.242128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.97
 ---- batch: 020 ----
mean loss: 182.85
 ---- batch: 030 ----
mean loss: 184.96
 ---- batch: 040 ----
mean loss: 181.86
 ---- batch: 050 ----
mean loss: 177.63
 ---- batch: 060 ----
mean loss: 183.91
 ---- batch: 070 ----
mean loss: 176.22
 ---- batch: 080 ----
mean loss: 184.65
 ---- batch: 090 ----
mean loss: 184.13
train mean loss: 181.27
epoch train time: 0:00:16.798605
elapsed time: 1:01:11.017582
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 05:28:35.041971
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.47
 ---- batch: 020 ----
mean loss: 180.75
 ---- batch: 030 ----
mean loss: 184.36
 ---- batch: 040 ----
mean loss: 182.79
 ---- batch: 050 ----
mean loss: 186.34
 ---- batch: 060 ----
mean loss: 176.61
 ---- batch: 070 ----
mean loss: 175.03
 ---- batch: 080 ----
mean loss: 187.08
 ---- batch: 090 ----
mean loss: 177.42
train mean loss: 180.93
epoch train time: 0:00:16.763712
elapsed time: 1:01:27.782640
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 05:28:51.806913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.60
 ---- batch: 020 ----
mean loss: 176.55
 ---- batch: 030 ----
mean loss: 180.52
 ---- batch: 040 ----
mean loss: 182.68
 ---- batch: 050 ----
mean loss: 184.94
 ---- batch: 060 ----
mean loss: 179.16
 ---- batch: 070 ----
mean loss: 180.53
 ---- batch: 080 ----
mean loss: 183.42
 ---- batch: 090 ----
mean loss: 180.53
train mean loss: 181.01
epoch train time: 0:00:16.871442
elapsed time: 1:01:44.655195
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 05:29:08.679547
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.01
 ---- batch: 020 ----
mean loss: 180.25
 ---- batch: 030 ----
mean loss: 177.81
 ---- batch: 040 ----
mean loss: 182.18
 ---- batch: 050 ----
mean loss: 178.90
 ---- batch: 060 ----
mean loss: 189.25
 ---- batch: 070 ----
mean loss: 180.70
 ---- batch: 080 ----
mean loss: 185.96
 ---- batch: 090 ----
mean loss: 176.90
train mean loss: 181.23
epoch train time: 0:00:16.899287
elapsed time: 1:02:01.555588
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 05:29:25.579882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.66
 ---- batch: 020 ----
mean loss: 186.36
 ---- batch: 030 ----
mean loss: 179.40
 ---- batch: 040 ----
mean loss: 177.38
 ---- batch: 050 ----
mean loss: 176.23
 ---- batch: 060 ----
mean loss: 182.57
 ---- batch: 070 ----
mean loss: 180.58
 ---- batch: 080 ----
mean loss: 185.96
 ---- batch: 090 ----
mean loss: 183.23
train mean loss: 181.03
epoch train time: 0:00:16.658218
elapsed time: 1:02:18.214887
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 05:29:42.239248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.95
 ---- batch: 020 ----
mean loss: 181.41
 ---- batch: 030 ----
mean loss: 179.96
 ---- batch: 040 ----
mean loss: 180.23
 ---- batch: 050 ----
mean loss: 182.19
 ---- batch: 060 ----
mean loss: 184.29
 ---- batch: 070 ----
mean loss: 182.89
 ---- batch: 080 ----
mean loss: 173.67
 ---- batch: 090 ----
mean loss: 186.08
train mean loss: 181.12
epoch train time: 0:00:16.727948
elapsed time: 1:02:34.944054
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 05:29:58.968386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.42
 ---- batch: 020 ----
mean loss: 176.86
 ---- batch: 030 ----
mean loss: 179.82
 ---- batch: 040 ----
mean loss: 178.64
 ---- batch: 050 ----
mean loss: 180.10
 ---- batch: 060 ----
mean loss: 177.78
 ---- batch: 070 ----
mean loss: 181.70
 ---- batch: 080 ----
mean loss: 194.50
 ---- batch: 090 ----
mean loss: 184.24
train mean loss: 181.59
epoch train time: 0:00:16.691528
elapsed time: 1:02:51.636725
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 05:30:15.661063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.79
 ---- batch: 020 ----
mean loss: 185.23
 ---- batch: 030 ----
mean loss: 177.86
 ---- batch: 040 ----
mean loss: 183.22
 ---- batch: 050 ----
mean loss: 180.36
 ---- batch: 060 ----
mean loss: 180.35
 ---- batch: 070 ----
mean loss: 176.66
 ---- batch: 080 ----
mean loss: 179.39
 ---- batch: 090 ----
mean loss: 184.17
train mean loss: 181.24
epoch train time: 0:00:16.641015
elapsed time: 1:03:08.278862
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 05:30:32.303225
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.87
 ---- batch: 020 ----
mean loss: 186.94
 ---- batch: 030 ----
mean loss: 181.72
 ---- batch: 040 ----
mean loss: 174.38
 ---- batch: 050 ----
mean loss: 177.37
 ---- batch: 060 ----
mean loss: 180.48
 ---- batch: 070 ----
mean loss: 176.40
 ---- batch: 080 ----
mean loss: 183.43
 ---- batch: 090 ----
mean loss: 188.89
train mean loss: 180.98
epoch train time: 0:00:16.668495
elapsed time: 1:03:24.948599
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 05:30:48.972842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.89
 ---- batch: 020 ----
mean loss: 183.53
 ---- batch: 030 ----
mean loss: 179.51
 ---- batch: 040 ----
mean loss: 185.51
 ---- batch: 050 ----
mean loss: 186.88
 ---- batch: 060 ----
mean loss: 178.13
 ---- batch: 070 ----
mean loss: 180.49
 ---- batch: 080 ----
mean loss: 180.62
 ---- batch: 090 ----
mean loss: 178.84
train mean loss: 180.96
epoch train time: 0:00:16.657750
elapsed time: 1:03:41.607414
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 05:31:05.631778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.57
 ---- batch: 020 ----
mean loss: 175.01
 ---- batch: 030 ----
mean loss: 182.69
 ---- batch: 040 ----
mean loss: 183.56
 ---- batch: 050 ----
mean loss: 176.85
 ---- batch: 060 ----
mean loss: 181.12
 ---- batch: 070 ----
mean loss: 184.24
 ---- batch: 080 ----
mean loss: 177.13
 ---- batch: 090 ----
mean loss: 184.52
train mean loss: 180.76
epoch train time: 0:00:16.661037
elapsed time: 1:03:58.269716
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 05:31:22.294045
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.32
 ---- batch: 020 ----
mean loss: 179.34
 ---- batch: 030 ----
mean loss: 181.27
 ---- batch: 040 ----
mean loss: 185.70
 ---- batch: 050 ----
mean loss: 185.99
 ---- batch: 060 ----
mean loss: 176.90
 ---- batch: 070 ----
mean loss: 182.27
 ---- batch: 080 ----
mean loss: 175.00
 ---- batch: 090 ----
mean loss: 185.65
train mean loss: 181.18
epoch train time: 0:00:16.693076
elapsed time: 1:04:14.963983
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 05:31:38.988343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.78
 ---- batch: 020 ----
mean loss: 183.00
 ---- batch: 030 ----
mean loss: 182.84
 ---- batch: 040 ----
mean loss: 178.88
 ---- batch: 050 ----
mean loss: 179.02
 ---- batch: 060 ----
mean loss: 178.20
 ---- batch: 070 ----
mean loss: 183.06
 ---- batch: 080 ----
mean loss: 181.91
 ---- batch: 090 ----
mean loss: 180.18
train mean loss: 181.15
epoch train time: 0:00:16.832240
elapsed time: 1:04:31.797474
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 05:31:55.821855
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.72
 ---- batch: 020 ----
mean loss: 182.91
 ---- batch: 030 ----
mean loss: 180.63
 ---- batch: 040 ----
mean loss: 182.41
 ---- batch: 050 ----
mean loss: 177.77
 ---- batch: 060 ----
mean loss: 174.09
 ---- batch: 070 ----
mean loss: 179.26
 ---- batch: 080 ----
mean loss: 183.86
 ---- batch: 090 ----
mean loss: 183.05
train mean loss: 180.91
epoch train time: 0:00:16.796850
elapsed time: 1:04:48.595560
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 05:32:12.619913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.24
 ---- batch: 020 ----
mean loss: 179.05
 ---- batch: 030 ----
mean loss: 182.41
 ---- batch: 040 ----
mean loss: 177.21
 ---- batch: 050 ----
mean loss: 176.58
 ---- batch: 060 ----
mean loss: 183.62
 ---- batch: 070 ----
mean loss: 183.03
 ---- batch: 080 ----
mean loss: 176.33
 ---- batch: 090 ----
mean loss: 183.97
train mean loss: 180.63
epoch train time: 0:00:16.843240
elapsed time: 1:05:05.440000
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 05:32:29.464477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.13
 ---- batch: 020 ----
mean loss: 180.28
 ---- batch: 030 ----
mean loss: 177.27
 ---- batch: 040 ----
mean loss: 177.76
 ---- batch: 050 ----
mean loss: 181.88
 ---- batch: 060 ----
mean loss: 182.07
 ---- batch: 070 ----
mean loss: 174.90
 ---- batch: 080 ----
mean loss: 182.00
 ---- batch: 090 ----
mean loss: 181.41
train mean loss: 181.04
epoch train time: 0:00:16.734208
elapsed time: 1:05:22.176255
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 05:32:46.200104
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.56
 ---- batch: 020 ----
mean loss: 183.58
 ---- batch: 030 ----
mean loss: 179.00
 ---- batch: 040 ----
mean loss: 184.59
 ---- batch: 050 ----
mean loss: 181.78
 ---- batch: 060 ----
mean loss: 179.69
 ---- batch: 070 ----
mean loss: 182.50
 ---- batch: 080 ----
mean loss: 181.47
 ---- batch: 090 ----
mean loss: 178.21
train mean loss: 180.64
epoch train time: 0:00:16.789640
elapsed time: 1:05:38.966576
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 05:33:02.990935
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.89
 ---- batch: 020 ----
mean loss: 181.72
 ---- batch: 030 ----
mean loss: 176.82
 ---- batch: 040 ----
mean loss: 177.40
 ---- batch: 050 ----
mean loss: 183.46
 ---- batch: 060 ----
mean loss: 182.75
 ---- batch: 070 ----
mean loss: 180.31
 ---- batch: 080 ----
mean loss: 182.43
 ---- batch: 090 ----
mean loss: 180.29
train mean loss: 180.66
epoch train time: 0:00:16.747260
elapsed time: 1:05:55.715235
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 05:33:19.739664
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.18
 ---- batch: 020 ----
mean loss: 182.39
 ---- batch: 030 ----
mean loss: 181.61
 ---- batch: 040 ----
mean loss: 183.20
 ---- batch: 050 ----
mean loss: 175.29
 ---- batch: 060 ----
mean loss: 174.86
 ---- batch: 070 ----
mean loss: 184.51
 ---- batch: 080 ----
mean loss: 183.33
 ---- batch: 090 ----
mean loss: 182.39
train mean loss: 180.60
epoch train time: 0:00:16.760757
elapsed time: 1:06:12.477197
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 05:33:36.501583
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.26
 ---- batch: 020 ----
mean loss: 175.10
 ---- batch: 030 ----
mean loss: 191.02
 ---- batch: 040 ----
mean loss: 180.05
 ---- batch: 050 ----
mean loss: 180.24
 ---- batch: 060 ----
mean loss: 173.88
 ---- batch: 070 ----
mean loss: 181.66
 ---- batch: 080 ----
mean loss: 186.18
 ---- batch: 090 ----
mean loss: 179.99
train mean loss: 180.95
epoch train time: 0:00:16.856335
elapsed time: 1:06:29.334732
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 05:33:53.359079
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.06
 ---- batch: 020 ----
mean loss: 182.10
 ---- batch: 030 ----
mean loss: 180.50
 ---- batch: 040 ----
mean loss: 180.66
 ---- batch: 050 ----
mean loss: 183.25
 ---- batch: 060 ----
mean loss: 176.98
 ---- batch: 070 ----
mean loss: 181.24
 ---- batch: 080 ----
mean loss: 177.17
 ---- batch: 090 ----
mean loss: 179.46
train mean loss: 180.71
epoch train time: 0:00:16.974822
elapsed time: 1:06:46.310783
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 05:34:10.335172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.05
 ---- batch: 020 ----
mean loss: 184.23
 ---- batch: 030 ----
mean loss: 179.37
 ---- batch: 040 ----
mean loss: 174.41
 ---- batch: 050 ----
mean loss: 183.74
 ---- batch: 060 ----
mean loss: 180.94
 ---- batch: 070 ----
mean loss: 176.82
 ---- batch: 080 ----
mean loss: 179.02
 ---- batch: 090 ----
mean loss: 183.14
train mean loss: 180.65
epoch train time: 0:00:16.946060
elapsed time: 1:07:03.258260
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 05:34:27.282649
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.43
 ---- batch: 020 ----
mean loss: 183.94
 ---- batch: 030 ----
mean loss: 182.26
 ---- batch: 040 ----
mean loss: 173.05
 ---- batch: 050 ----
mean loss: 182.61
 ---- batch: 060 ----
mean loss: 177.61
 ---- batch: 070 ----
mean loss: 184.46
 ---- batch: 080 ----
mean loss: 181.47
 ---- batch: 090 ----
mean loss: 175.36
train mean loss: 180.79
epoch train time: 0:00:16.931701
elapsed time: 1:07:20.191330
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 05:34:44.215777
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.95
 ---- batch: 020 ----
mean loss: 175.67
 ---- batch: 030 ----
mean loss: 182.21
 ---- batch: 040 ----
mean loss: 177.76
 ---- batch: 050 ----
mean loss: 180.65
 ---- batch: 060 ----
mean loss: 173.25
 ---- batch: 070 ----
mean loss: 182.49
 ---- batch: 080 ----
mean loss: 175.91
 ---- batch: 090 ----
mean loss: 186.62
train mean loss: 180.35
epoch train time: 0:00:17.062858
elapsed time: 1:07:37.255511
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 05:35:01.279852
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.62
 ---- batch: 020 ----
mean loss: 176.55
 ---- batch: 030 ----
mean loss: 185.67
 ---- batch: 040 ----
mean loss: 183.24
 ---- batch: 050 ----
mean loss: 175.97
 ---- batch: 060 ----
mean loss: 178.33
 ---- batch: 070 ----
mean loss: 176.85
 ---- batch: 080 ----
mean loss: 178.69
 ---- batch: 090 ----
mean loss: 185.00
train mean loss: 180.36
epoch train time: 0:00:17.066901
elapsed time: 1:07:54.323819
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 05:35:18.348204
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.34
 ---- batch: 020 ----
mean loss: 182.18
 ---- batch: 030 ----
mean loss: 182.68
 ---- batch: 040 ----
mean loss: 176.57
 ---- batch: 050 ----
mean loss: 177.19
 ---- batch: 060 ----
mean loss: 182.90
 ---- batch: 070 ----
mean loss: 186.55
 ---- batch: 080 ----
mean loss: 173.07
 ---- batch: 090 ----
mean loss: 184.47
train mean loss: 180.86
epoch train time: 0:00:16.976001
elapsed time: 1:08:11.301470
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 05:35:35.325920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.78
 ---- batch: 020 ----
mean loss: 189.93
 ---- batch: 030 ----
mean loss: 180.11
 ---- batch: 040 ----
mean loss: 173.33
 ---- batch: 050 ----
mean loss: 181.50
 ---- batch: 060 ----
mean loss: 178.50
 ---- batch: 070 ----
mean loss: 181.88
 ---- batch: 080 ----
mean loss: 175.58
 ---- batch: 090 ----
mean loss: 182.23
train mean loss: 180.54
epoch train time: 0:00:16.990284
elapsed time: 1:08:28.293192
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 05:35:52.317552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.02
 ---- batch: 020 ----
mean loss: 181.21
 ---- batch: 030 ----
mean loss: 176.80
 ---- batch: 040 ----
mean loss: 180.75
 ---- batch: 050 ----
mean loss: 185.83
 ---- batch: 060 ----
mean loss: 182.74
 ---- batch: 070 ----
mean loss: 178.03
 ---- batch: 080 ----
mean loss: 181.03
 ---- batch: 090 ----
mean loss: 184.61
train mean loss: 180.14
epoch train time: 0:00:16.977096
elapsed time: 1:08:45.271541
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 05:36:09.295978
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.97
 ---- batch: 020 ----
mean loss: 180.29
 ---- batch: 030 ----
mean loss: 176.04
 ---- batch: 040 ----
mean loss: 185.34
 ---- batch: 050 ----
mean loss: 179.94
 ---- batch: 060 ----
mean loss: 175.98
 ---- batch: 070 ----
mean loss: 179.67
 ---- batch: 080 ----
mean loss: 176.83
 ---- batch: 090 ----
mean loss: 179.90
train mean loss: 180.49
epoch train time: 0:00:17.145490
elapsed time: 1:09:02.418328
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 05:36:26.442705
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.35
 ---- batch: 020 ----
mean loss: 178.73
 ---- batch: 030 ----
mean loss: 179.60
 ---- batch: 040 ----
mean loss: 185.26
 ---- batch: 050 ----
mean loss: 183.38
 ---- batch: 060 ----
mean loss: 183.60
 ---- batch: 070 ----
mean loss: 175.38
 ---- batch: 080 ----
mean loss: 172.44
 ---- batch: 090 ----
mean loss: 184.79
train mean loss: 180.03
epoch train time: 0:00:16.896874
elapsed time: 1:09:19.316457
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 05:36:43.340801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.60
 ---- batch: 020 ----
mean loss: 180.16
 ---- batch: 030 ----
mean loss: 179.00
 ---- batch: 040 ----
mean loss: 182.30
 ---- batch: 050 ----
mean loss: 176.80
 ---- batch: 060 ----
mean loss: 183.24
 ---- batch: 070 ----
mean loss: 192.85
 ---- batch: 080 ----
mean loss: 174.71
 ---- batch: 090 ----
mean loss: 177.39
train mean loss: 180.34
epoch train time: 0:00:17.010716
elapsed time: 1:09:36.328428
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 05:37:00.352811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.29
 ---- batch: 020 ----
mean loss: 179.58
 ---- batch: 030 ----
mean loss: 180.07
 ---- batch: 040 ----
mean loss: 171.21
 ---- batch: 050 ----
mean loss: 186.52
 ---- batch: 060 ----
mean loss: 182.58
 ---- batch: 070 ----
mean loss: 178.02
 ---- batch: 080 ----
mean loss: 182.35
 ---- batch: 090 ----
mean loss: 183.33
train mean loss: 179.88
epoch train time: 0:00:16.975924
elapsed time: 1:09:53.315456
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_8/checkpoint.pth.tar
**** end time: 2019-09-25 05:37:17.339264 ****
