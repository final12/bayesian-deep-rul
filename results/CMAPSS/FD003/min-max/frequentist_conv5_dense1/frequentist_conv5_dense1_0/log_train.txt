Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_0', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 6416
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-20 22:07:05.938335 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 31, 14]             100
              Tanh-2           [-1, 10, 31, 14]               0
            Conv2d-3           [-1, 10, 30, 14]           1,000
              Tanh-4           [-1, 10, 30, 14]               0
            Conv2d-5           [-1, 10, 31, 14]           1,000
              Tanh-6           [-1, 10, 31, 14]               0
            Conv2d-7           [-1, 10, 30, 14]           1,000
              Tanh-8           [-1, 10, 30, 14]               0
            Conv2d-9            [-1, 1, 30, 14]              30
             Tanh-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
          Dropout-12                  [-1, 420]               0
           Linear-13                  [-1, 100]          42,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 45,230
Trainable params: 45,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 22:07:05.945839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3849.17
 ---- batch: 020 ----
mean loss: 1474.41
 ---- batch: 030 ----
mean loss: 429.34
 ---- batch: 040 ----
mean loss: 442.95
train mean loss: 1469.02
epoch train time: 0:00:15.047230
elapsed time: 0:00:15.057075
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 22:07:20.995452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.53
 ---- batch: 020 ----
mean loss: 343.49
 ---- batch: 030 ----
mean loss: 312.39
 ---- batch: 040 ----
mean loss: 306.04
train mean loss: 326.23
epoch train time: 0:00:01.823237
elapsed time: 0:00:16.880457
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 22:07:22.818863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.06
 ---- batch: 020 ----
mean loss: 288.07
 ---- batch: 030 ----
mean loss: 276.99
 ---- batch: 040 ----
mean loss: 271.03
train mean loss: 279.96
epoch train time: 0:00:01.685587
elapsed time: 0:00:18.566203
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 22:07:24.504584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.72
 ---- batch: 020 ----
mean loss: 238.40
 ---- batch: 030 ----
mean loss: 238.27
 ---- batch: 040 ----
mean loss: 229.86
train mean loss: 237.25
epoch train time: 0:00:01.763644
elapsed time: 0:00:20.329976
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 22:07:26.268348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.42
 ---- batch: 020 ----
mean loss: 229.45
 ---- batch: 030 ----
mean loss: 221.17
 ---- batch: 040 ----
mean loss: 210.73
train mean loss: 221.45
epoch train time: 0:00:01.672401
elapsed time: 0:00:22.002521
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 22:07:27.940901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.92
 ---- batch: 020 ----
mean loss: 203.20
 ---- batch: 030 ----
mean loss: 218.55
 ---- batch: 040 ----
mean loss: 216.16
train mean loss: 213.62
epoch train time: 0:00:01.710886
elapsed time: 0:00:23.713534
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 22:07:29.651913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.17
 ---- batch: 020 ----
mean loss: 209.73
 ---- batch: 030 ----
mean loss: 212.19
 ---- batch: 040 ----
mean loss: 205.94
train mean loss: 208.07
epoch train time: 0:00:01.718328
elapsed time: 0:00:25.432010
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 22:07:31.370395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.50
 ---- batch: 020 ----
mean loss: 209.16
 ---- batch: 030 ----
mean loss: 198.54
 ---- batch: 040 ----
mean loss: 199.03
train mean loss: 204.97
epoch train time: 0:00:01.665230
elapsed time: 0:00:27.097398
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 22:07:33.035780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.13
 ---- batch: 020 ----
mean loss: 204.01
 ---- batch: 030 ----
mean loss: 194.29
 ---- batch: 040 ----
mean loss: 214.36
train mean loss: 205.13
epoch train time: 0:00:01.781852
elapsed time: 0:00:28.879378
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 22:07:34.817756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.23
 ---- batch: 020 ----
mean loss: 196.85
 ---- batch: 030 ----
mean loss: 196.94
 ---- batch: 040 ----
mean loss: 205.19
train mean loss: 201.17
epoch train time: 0:00:01.674265
elapsed time: 0:00:30.553773
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 22:07:36.492155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.67
 ---- batch: 020 ----
mean loss: 202.48
 ---- batch: 030 ----
mean loss: 196.17
 ---- batch: 040 ----
mean loss: 200.73
train mean loss: 201.14
epoch train time: 0:00:01.675296
elapsed time: 0:00:32.229214
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 22:07:38.167605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.94
 ---- batch: 020 ----
mean loss: 201.14
 ---- batch: 030 ----
mean loss: 196.43
 ---- batch: 040 ----
mean loss: 194.87
train mean loss: 197.08
epoch train time: 0:00:01.695459
elapsed time: 0:00:33.924864
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 22:07:39.863273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.40
 ---- batch: 020 ----
mean loss: 199.15
 ---- batch: 030 ----
mean loss: 197.38
 ---- batch: 040 ----
mean loss: 197.38
train mean loss: 199.02
epoch train time: 0:00:01.661864
elapsed time: 0:00:35.586890
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 22:07:41.525272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.58
 ---- batch: 020 ----
mean loss: 189.89
 ---- batch: 030 ----
mean loss: 191.04
 ---- batch: 040 ----
mean loss: 195.04
train mean loss: 193.61
epoch train time: 0:00:01.773276
elapsed time: 0:00:37.360295
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 22:07:43.298689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.17
 ---- batch: 020 ----
mean loss: 198.60
 ---- batch: 030 ----
mean loss: 192.58
 ---- batch: 040 ----
mean loss: 197.48
train mean loss: 196.01
epoch train time: 0:00:01.668786
elapsed time: 0:00:39.029268
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 22:07:44.967651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.12
 ---- batch: 020 ----
mean loss: 194.41
 ---- batch: 030 ----
mean loss: 195.73
 ---- batch: 040 ----
mean loss: 192.51
train mean loss: 195.26
epoch train time: 0:00:01.774072
elapsed time: 0:00:40.803467
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 22:07:46.741848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.09
 ---- batch: 020 ----
mean loss: 192.50
 ---- batch: 030 ----
mean loss: 194.07
 ---- batch: 040 ----
mean loss: 188.12
train mean loss: 192.05
epoch train time: 0:00:01.678282
elapsed time: 0:00:42.481904
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 22:07:48.420286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.38
 ---- batch: 020 ----
mean loss: 198.22
 ---- batch: 030 ----
mean loss: 206.71
 ---- batch: 040 ----
mean loss: 200.47
train mean loss: 198.00
epoch train time: 0:00:01.775245
elapsed time: 0:00:44.257307
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 22:07:50.195702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.22
 ---- batch: 020 ----
mean loss: 194.09
 ---- batch: 030 ----
mean loss: 201.54
 ---- batch: 040 ----
mean loss: 186.65
train mean loss: 192.53
epoch train time: 0:00:01.685227
elapsed time: 0:00:45.942701
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 22:07:51.881084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.55
 ---- batch: 020 ----
mean loss: 193.75
 ---- batch: 030 ----
mean loss: 188.51
 ---- batch: 040 ----
mean loss: 197.52
train mean loss: 191.87
epoch train time: 0:00:01.675417
elapsed time: 0:00:47.618257
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 22:07:53.556638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.53
 ---- batch: 020 ----
mean loss: 192.06
 ---- batch: 030 ----
mean loss: 196.13
 ---- batch: 040 ----
mean loss: 188.78
train mean loss: 191.33
epoch train time: 0:00:01.786574
elapsed time: 0:00:49.404977
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 22:07:55.343369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.54
 ---- batch: 020 ----
mean loss: 202.99
 ---- batch: 030 ----
mean loss: 185.92
 ---- batch: 040 ----
mean loss: 187.85
train mean loss: 191.20
epoch train time: 0:00:01.683112
elapsed time: 0:00:51.088275
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 22:07:57.026677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.82
 ---- batch: 020 ----
mean loss: 192.00
 ---- batch: 030 ----
mean loss: 186.78
 ---- batch: 040 ----
mean loss: 191.78
train mean loss: 191.44
epoch train time: 0:00:01.771447
elapsed time: 0:00:52.859877
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 22:07:58.798257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.35
 ---- batch: 020 ----
mean loss: 181.17
 ---- batch: 030 ----
mean loss: 190.58
 ---- batch: 040 ----
mean loss: 189.00
train mean loss: 187.24
epoch train time: 0:00:01.674871
elapsed time: 0:00:54.534881
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 22:08:00.473279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.04
 ---- batch: 020 ----
mean loss: 194.55
 ---- batch: 030 ----
mean loss: 196.34
 ---- batch: 040 ----
mean loss: 199.68
train mean loss: 193.32
epoch train time: 0:00:01.667979
elapsed time: 0:00:56.203007
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 22:08:02.141404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.94
 ---- batch: 020 ----
mean loss: 192.43
 ---- batch: 030 ----
mean loss: 187.99
 ---- batch: 040 ----
mean loss: 199.26
train mean loss: 191.98
epoch train time: 0:00:01.788742
elapsed time: 0:00:57.991907
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 22:08:03.930304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.23
 ---- batch: 020 ----
mean loss: 186.84
 ---- batch: 030 ----
mean loss: 190.86
 ---- batch: 040 ----
mean loss: 183.47
train mean loss: 186.62
epoch train time: 0:00:01.677862
elapsed time: 0:00:59.669917
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 22:08:05.608309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.26
 ---- batch: 020 ----
mean loss: 195.32
 ---- batch: 030 ----
mean loss: 192.50
 ---- batch: 040 ----
mean loss: 183.46
train mean loss: 188.81
epoch train time: 0:00:01.782810
elapsed time: 0:01:01.452882
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 22:08:07.391271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.65
 ---- batch: 020 ----
mean loss: 182.87
 ---- batch: 030 ----
mean loss: 190.09
 ---- batch: 040 ----
mean loss: 192.94
train mean loss: 189.69
epoch train time: 0:00:01.677895
elapsed time: 0:01:03.130921
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 22:08:09.069304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.33
 ---- batch: 020 ----
mean loss: 191.55
 ---- batch: 030 ----
mean loss: 177.26
 ---- batch: 040 ----
mean loss: 183.39
train mean loss: 185.08
epoch train time: 0:00:01.780774
elapsed time: 0:01:04.911825
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 22:08:10.850204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.55
 ---- batch: 020 ----
mean loss: 190.21
 ---- batch: 030 ----
mean loss: 187.71
 ---- batch: 040 ----
mean loss: 182.68
train mean loss: 189.33
epoch train time: 0:00:01.690449
elapsed time: 0:01:06.602417
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 22:08:12.540807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.32
 ---- batch: 020 ----
mean loss: 192.66
 ---- batch: 030 ----
mean loss: 186.12
 ---- batch: 040 ----
mean loss: 189.40
train mean loss: 191.31
epoch train time: 0:00:01.679910
elapsed time: 0:01:08.282484
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 22:08:14.220890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.35
 ---- batch: 020 ----
mean loss: 183.11
 ---- batch: 030 ----
mean loss: 186.57
 ---- batch: 040 ----
mean loss: 184.45
train mean loss: 187.35
epoch train time: 0:00:01.802382
elapsed time: 0:01:10.085018
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 22:08:16.023395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.02
 ---- batch: 020 ----
mean loss: 186.93
 ---- batch: 030 ----
mean loss: 180.60
 ---- batch: 040 ----
mean loss: 178.87
train mean loss: 184.98
epoch train time: 0:00:01.688299
elapsed time: 0:01:11.773462
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 22:08:17.711850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.67
 ---- batch: 020 ----
mean loss: 185.21
 ---- batch: 030 ----
mean loss: 181.88
 ---- batch: 040 ----
mean loss: 188.01
train mean loss: 184.13
epoch train time: 0:00:01.786452
elapsed time: 0:01:13.560042
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 22:08:19.498418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.13
 ---- batch: 020 ----
mean loss: 183.73
 ---- batch: 030 ----
mean loss: 185.24
 ---- batch: 040 ----
mean loss: 186.69
train mean loss: 185.88
epoch train time: 0:00:01.678031
elapsed time: 0:01:15.238244
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 22:08:21.176645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.57
 ---- batch: 020 ----
mean loss: 184.71
 ---- batch: 030 ----
mean loss: 187.96
 ---- batch: 040 ----
mean loss: 186.07
train mean loss: 185.41
epoch train time: 0:00:01.678202
elapsed time: 0:01:16.916599
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 22:08:22.854981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.11
 ---- batch: 020 ----
mean loss: 183.72
 ---- batch: 030 ----
mean loss: 188.76
 ---- batch: 040 ----
mean loss: 186.13
train mean loss: 188.05
epoch train time: 0:00:01.774719
elapsed time: 0:01:18.691466
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 22:08:24.629851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.56
 ---- batch: 020 ----
mean loss: 177.00
 ---- batch: 030 ----
mean loss: 184.66
 ---- batch: 040 ----
mean loss: 186.68
train mean loss: 185.03
epoch train time: 0:00:01.686027
elapsed time: 0:01:20.377654
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 22:08:26.316054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.74
 ---- batch: 020 ----
mean loss: 186.02
 ---- batch: 030 ----
mean loss: 181.53
 ---- batch: 040 ----
mean loss: 185.87
train mean loss: 184.16
epoch train time: 0:00:01.715192
elapsed time: 0:01:22.092997
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 22:08:28.031380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.54
 ---- batch: 020 ----
mean loss: 184.99
 ---- batch: 030 ----
mean loss: 184.67
 ---- batch: 040 ----
mean loss: 189.16
train mean loss: 186.35
epoch train time: 0:00:01.687554
elapsed time: 0:01:23.780707
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 22:08:29.719092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.86
 ---- batch: 020 ----
mean loss: 184.72
 ---- batch: 030 ----
mean loss: 199.51
 ---- batch: 040 ----
mean loss: 176.40
train mean loss: 186.90
epoch train time: 0:00:01.694754
elapsed time: 0:01:25.475615
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 22:08:31.414000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.65
 ---- batch: 020 ----
mean loss: 178.57
 ---- batch: 030 ----
mean loss: 188.25
 ---- batch: 040 ----
mean loss: 180.45
train mean loss: 181.70
epoch train time: 0:00:01.712889
elapsed time: 0:01:27.188652
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 22:08:33.127037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.24
 ---- batch: 020 ----
mean loss: 183.72
 ---- batch: 030 ----
mean loss: 172.32
 ---- batch: 040 ----
mean loss: 183.62
train mean loss: 183.02
epoch train time: 0:00:01.679347
elapsed time: 0:01:28.868144
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 22:08:34.806526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.27
 ---- batch: 020 ----
mean loss: 186.72
 ---- batch: 030 ----
mean loss: 182.07
 ---- batch: 040 ----
mean loss: 181.63
train mean loss: 183.84
epoch train time: 0:00:01.792957
elapsed time: 0:01:30.661283
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 22:08:36.599666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.60
 ---- batch: 020 ----
mean loss: 184.13
 ---- batch: 030 ----
mean loss: 179.01
 ---- batch: 040 ----
mean loss: 185.78
train mean loss: 181.22
epoch train time: 0:00:01.683795
elapsed time: 0:01:32.345224
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 22:08:38.283637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.52
 ---- batch: 020 ----
mean loss: 178.43
 ---- batch: 030 ----
mean loss: 181.43
 ---- batch: 040 ----
mean loss: 175.98
train mean loss: 178.78
epoch train time: 0:00:01.707592
elapsed time: 0:01:34.052998
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 22:08:39.991395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.24
 ---- batch: 020 ----
mean loss: 182.74
 ---- batch: 030 ----
mean loss: 187.92
 ---- batch: 040 ----
mean loss: 181.04
train mean loss: 183.85
epoch train time: 0:00:01.694829
elapsed time: 0:01:35.747988
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 22:08:41.686387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.72
 ---- batch: 020 ----
mean loss: 189.17
 ---- batch: 030 ----
mean loss: 178.91
 ---- batch: 040 ----
mean loss: 182.54
train mean loss: 184.45
epoch train time: 0:00:01.679369
elapsed time: 0:01:37.427536
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 22:08:43.365933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.78
 ---- batch: 020 ----
mean loss: 178.30
 ---- batch: 030 ----
mean loss: 174.57
 ---- batch: 040 ----
mean loss: 178.02
train mean loss: 179.67
epoch train time: 0:00:01.786844
elapsed time: 0:01:39.214556
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 22:08:45.152941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.58
 ---- batch: 020 ----
mean loss: 185.17
 ---- batch: 030 ----
mean loss: 174.77
 ---- batch: 040 ----
mean loss: 178.20
train mean loss: 179.41
epoch train time: 0:00:01.679900
elapsed time: 0:01:40.894621
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 22:08:46.833050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.68
 ---- batch: 020 ----
mean loss: 179.56
 ---- batch: 030 ----
mean loss: 179.91
 ---- batch: 040 ----
mean loss: 171.82
train mean loss: 179.06
epoch train time: 0:00:01.793714
elapsed time: 0:01:42.688506
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 22:08:48.626884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.04
 ---- batch: 020 ----
mean loss: 173.21
 ---- batch: 030 ----
mean loss: 175.16
 ---- batch: 040 ----
mean loss: 174.86
train mean loss: 177.33
epoch train time: 0:00:01.689230
elapsed time: 0:01:44.377868
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 22:08:50.316250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.66
 ---- batch: 020 ----
mean loss: 175.75
 ---- batch: 030 ----
mean loss: 178.14
 ---- batch: 040 ----
mean loss: 184.30
train mean loss: 177.97
epoch train time: 0:00:01.789344
elapsed time: 0:01:46.167334
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 22:08:52.105710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.96
 ---- batch: 020 ----
mean loss: 192.48
 ---- batch: 030 ----
mean loss: 183.64
 ---- batch: 040 ----
mean loss: 175.14
train mean loss: 182.68
epoch train time: 0:00:01.696821
elapsed time: 0:01:47.864311
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 22:08:53.802703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.56
 ---- batch: 020 ----
mean loss: 183.78
 ---- batch: 030 ----
mean loss: 174.91
 ---- batch: 040 ----
mean loss: 167.29
train mean loss: 179.73
epoch train time: 0:00:01.677745
elapsed time: 0:01:49.542195
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 22:08:55.480573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.15
 ---- batch: 020 ----
mean loss: 175.47
 ---- batch: 030 ----
mean loss: 171.07
 ---- batch: 040 ----
mean loss: 178.48
train mean loss: 176.91
epoch train time: 0:00:01.792215
elapsed time: 0:01:51.334536
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 22:08:57.272932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.95
 ---- batch: 020 ----
mean loss: 170.57
 ---- batch: 030 ----
mean loss: 179.37
 ---- batch: 040 ----
mean loss: 179.62
train mean loss: 176.80
epoch train time: 0:00:01.684751
elapsed time: 0:01:53.019449
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 22:08:58.957833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.42
 ---- batch: 020 ----
mean loss: 175.64
 ---- batch: 030 ----
mean loss: 172.97
 ---- batch: 040 ----
mean loss: 176.16
train mean loss: 172.78
epoch train time: 0:00:01.779083
elapsed time: 0:01:54.798662
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 22:09:00.737039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.81
 ---- batch: 020 ----
mean loss: 186.49
 ---- batch: 030 ----
mean loss: 176.78
 ---- batch: 040 ----
mean loss: 174.58
train mean loss: 176.97
epoch train time: 0:00:01.692190
elapsed time: 0:01:56.491009
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 22:09:02.429406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.16
 ---- batch: 020 ----
mean loss: 165.86
 ---- batch: 030 ----
mean loss: 177.67
 ---- batch: 040 ----
mean loss: 177.01
train mean loss: 172.60
epoch train time: 0:00:01.777174
elapsed time: 0:01:58.268373
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 22:09:04.206755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.56
 ---- batch: 020 ----
mean loss: 176.42
 ---- batch: 030 ----
mean loss: 170.81
 ---- batch: 040 ----
mean loss: 174.18
train mean loss: 173.12
epoch train time: 0:00:01.693443
elapsed time: 0:01:59.961961
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 22:09:05.900417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.12
 ---- batch: 020 ----
mean loss: 170.11
 ---- batch: 030 ----
mean loss: 173.87
 ---- batch: 040 ----
mean loss: 174.48
train mean loss: 171.28
epoch train time: 0:00:01.680705
elapsed time: 0:02:01.642917
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 22:09:07.581313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.41
 ---- batch: 020 ----
mean loss: 167.33
 ---- batch: 030 ----
mean loss: 177.84
 ---- batch: 040 ----
mean loss: 167.20
train mean loss: 169.66
epoch train time: 0:00:01.790121
elapsed time: 0:02:03.433186
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 22:09:09.371583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.87
 ---- batch: 020 ----
mean loss: 174.27
 ---- batch: 030 ----
mean loss: 171.50
 ---- batch: 040 ----
mean loss: 164.95
train mean loss: 170.55
epoch train time: 0:00:01.688626
elapsed time: 0:02:05.121982
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 22:09:11.060368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.54
 ---- batch: 020 ----
mean loss: 172.87
 ---- batch: 030 ----
mean loss: 164.24
 ---- batch: 040 ----
mean loss: 171.84
train mean loss: 169.44
epoch train time: 0:00:01.777117
elapsed time: 0:02:06.899228
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 22:09:12.837606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.23
 ---- batch: 020 ----
mean loss: 164.28
 ---- batch: 030 ----
mean loss: 173.14
 ---- batch: 040 ----
mean loss: 167.96
train mean loss: 166.96
epoch train time: 0:00:01.689571
elapsed time: 0:02:08.588942
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 22:09:14.527328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.78
 ---- batch: 020 ----
mean loss: 170.97
 ---- batch: 030 ----
mean loss: 174.99
 ---- batch: 040 ----
mean loss: 166.50
train mean loss: 168.83
epoch train time: 0:00:01.676342
elapsed time: 0:02:10.265421
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 22:09:16.203814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.73
 ---- batch: 020 ----
mean loss: 162.90
 ---- batch: 030 ----
mean loss: 164.90
 ---- batch: 040 ----
mean loss: 169.65
train mean loss: 165.32
epoch train time: 0:00:01.790415
elapsed time: 0:02:12.055970
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 22:09:17.994347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.03
 ---- batch: 020 ----
mean loss: 159.20
 ---- batch: 030 ----
mean loss: 159.65
 ---- batch: 040 ----
mean loss: 163.79
train mean loss: 163.17
epoch train time: 0:00:01.688670
elapsed time: 0:02:13.744817
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 22:09:19.683227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.85
 ---- batch: 020 ----
mean loss: 161.83
 ---- batch: 030 ----
mean loss: 165.20
 ---- batch: 040 ----
mean loss: 164.36
train mean loss: 166.31
epoch train time: 0:00:01.781636
elapsed time: 0:02:15.526609
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 22:09:21.465001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.96
 ---- batch: 020 ----
mean loss: 167.93
 ---- batch: 030 ----
mean loss: 161.42
 ---- batch: 040 ----
mean loss: 161.63
train mean loss: 164.29
epoch train time: 0:00:01.687874
elapsed time: 0:02:17.214661
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 22:09:23.153072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.18
 ---- batch: 020 ----
mean loss: 164.84
 ---- batch: 030 ----
mean loss: 156.35
 ---- batch: 040 ----
mean loss: 158.19
train mean loss: 160.48
epoch train time: 0:00:01.789241
elapsed time: 0:02:19.004125
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 22:09:24.942505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.63
 ---- batch: 020 ----
mean loss: 159.29
 ---- batch: 030 ----
mean loss: 162.18
 ---- batch: 040 ----
mean loss: 159.48
train mean loss: 161.27
epoch train time: 0:00:01.682069
elapsed time: 0:02:20.686355
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 22:09:26.624736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.51
 ---- batch: 020 ----
mean loss: 157.82
 ---- batch: 030 ----
mean loss: 162.57
 ---- batch: 040 ----
mean loss: 170.42
train mean loss: 163.31
epoch train time: 0:00:01.683386
elapsed time: 0:02:22.369868
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 22:09:28.308249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.67
 ---- batch: 020 ----
mean loss: 173.82
 ---- batch: 030 ----
mean loss: 164.89
 ---- batch: 040 ----
mean loss: 160.99
train mean loss: 165.08
epoch train time: 0:00:01.724859
elapsed time: 0:02:24.094880
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 22:09:30.033268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.99
 ---- batch: 020 ----
mean loss: 155.76
 ---- batch: 030 ----
mean loss: 156.01
 ---- batch: 040 ----
mean loss: 159.58
train mean loss: 158.25
epoch train time: 0:00:01.674253
elapsed time: 0:02:25.769313
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 22:09:31.707742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.51
 ---- batch: 020 ----
mean loss: 162.99
 ---- batch: 030 ----
mean loss: 159.39
 ---- batch: 040 ----
mean loss: 154.18
train mean loss: 158.61
epoch train time: 0:00:01.785937
elapsed time: 0:02:27.555428
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 22:09:33.493837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.50
 ---- batch: 020 ----
mean loss: 156.05
 ---- batch: 030 ----
mean loss: 159.04
 ---- batch: 040 ----
mean loss: 160.56
train mean loss: 158.74
epoch train time: 0:00:01.691601
elapsed time: 0:02:29.247234
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 22:09:35.185628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.88
 ---- batch: 020 ----
mean loss: 153.36
 ---- batch: 030 ----
mean loss: 159.86
 ---- batch: 040 ----
mean loss: 157.60
train mean loss: 157.21
epoch train time: 0:00:01.731281
elapsed time: 0:02:30.978675
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 22:09:36.917054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.79
 ---- batch: 020 ----
mean loss: 155.65
 ---- batch: 030 ----
mean loss: 155.56
 ---- batch: 040 ----
mean loss: 154.29
train mean loss: 155.79
epoch train time: 0:00:01.740408
elapsed time: 0:02:32.719214
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 22:09:38.657595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.37
 ---- batch: 020 ----
mean loss: 160.33
 ---- batch: 030 ----
mean loss: 155.23
 ---- batch: 040 ----
mean loss: 150.78
train mean loss: 155.98
epoch train time: 0:00:01.688887
elapsed time: 0:02:34.408243
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 22:09:40.346629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.18
 ---- batch: 020 ----
mean loss: 163.99
 ---- batch: 030 ----
mean loss: 159.62
 ---- batch: 040 ----
mean loss: 163.27
train mean loss: 160.85
epoch train time: 0:00:01.790366
elapsed time: 0:02:36.198749
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 22:09:42.137129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.79
 ---- batch: 020 ----
mean loss: 158.61
 ---- batch: 030 ----
mean loss: 154.99
 ---- batch: 040 ----
mean loss: 153.40
train mean loss: 156.85
epoch train time: 0:00:01.687806
elapsed time: 0:02:37.886697
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 22:09:43.825095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.95
 ---- batch: 020 ----
mean loss: 154.87
 ---- batch: 030 ----
mean loss: 155.35
 ---- batch: 040 ----
mean loss: 157.48
train mean loss: 154.15
epoch train time: 0:00:01.785255
elapsed time: 0:02:39.672098
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 22:09:45.610495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.04
 ---- batch: 020 ----
mean loss: 153.29
 ---- batch: 030 ----
mean loss: 154.42
 ---- batch: 040 ----
mean loss: 155.45
train mean loss: 153.48
epoch train time: 0:00:01.687238
elapsed time: 0:02:41.359498
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 22:09:47.297879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.57
 ---- batch: 020 ----
mean loss: 153.64
 ---- batch: 030 ----
mean loss: 149.00
 ---- batch: 040 ----
mean loss: 150.98
train mean loss: 151.24
epoch train time: 0:00:01.677003
elapsed time: 0:02:43.036658
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 22:09:48.975078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.49
 ---- batch: 020 ----
mean loss: 153.84
 ---- batch: 030 ----
mean loss: 147.83
 ---- batch: 040 ----
mean loss: 143.08
train mean loss: 149.00
epoch train time: 0:00:01.782779
elapsed time: 0:02:44.819597
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 22:09:50.757976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.93
 ---- batch: 020 ----
mean loss: 156.62
 ---- batch: 030 ----
mean loss: 146.05
 ---- batch: 040 ----
mean loss: 143.42
train mean loss: 150.38
epoch train time: 0:00:01.686908
elapsed time: 0:02:46.506656
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 22:09:52.445053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.19
 ---- batch: 020 ----
mean loss: 145.53
 ---- batch: 030 ----
mean loss: 150.83
 ---- batch: 040 ----
mean loss: 144.22
train mean loss: 146.24
epoch train time: 0:00:01.786783
elapsed time: 0:02:48.293578
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 22:09:54.231969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.59
 ---- batch: 020 ----
mean loss: 149.00
 ---- batch: 030 ----
mean loss: 146.64
 ---- batch: 040 ----
mean loss: 150.18
train mean loss: 148.24
epoch train time: 0:00:01.684302
elapsed time: 0:02:49.978065
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 22:09:55.916454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.39
 ---- batch: 020 ----
mean loss: 150.67
 ---- batch: 030 ----
mean loss: 147.32
 ---- batch: 040 ----
mean loss: 142.23
train mean loss: 147.87
epoch train time: 0:00:01.780504
elapsed time: 0:02:51.758699
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 22:09:57.697074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.09
 ---- batch: 020 ----
mean loss: 141.55
 ---- batch: 030 ----
mean loss: 146.57
 ---- batch: 040 ----
mean loss: 149.51
train mean loss: 146.51
epoch train time: 0:00:01.699177
elapsed time: 0:02:53.458020
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 22:09:59.396416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.30
 ---- batch: 020 ----
mean loss: 142.38
 ---- batch: 030 ----
mean loss: 139.10
 ---- batch: 040 ----
mean loss: 147.57
train mean loss: 143.88
epoch train time: 0:00:01.674329
elapsed time: 0:02:55.132529
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 22:10:01.070913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.36
 ---- batch: 020 ----
mean loss: 147.21
 ---- batch: 030 ----
mean loss: 146.72
 ---- batch: 040 ----
mean loss: 148.55
train mean loss: 147.75
epoch train time: 0:00:01.795994
elapsed time: 0:02:56.928653
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 22:10:02.867030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.99
 ---- batch: 020 ----
mean loss: 143.27
 ---- batch: 030 ----
mean loss: 140.08
 ---- batch: 040 ----
mean loss: 146.73
train mean loss: 145.01
epoch train time: 0:00:01.684386
elapsed time: 0:02:58.613177
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 22:10:04.551595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.32
 ---- batch: 020 ----
mean loss: 150.88
 ---- batch: 030 ----
mean loss: 142.69
 ---- batch: 040 ----
mean loss: 146.33
train mean loss: 150.00
epoch train time: 0:00:01.786499
elapsed time: 0:03:00.399839
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 22:10:06.338219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.68
 ---- batch: 020 ----
mean loss: 144.90
 ---- batch: 030 ----
mean loss: 143.81
 ---- batch: 040 ----
mean loss: 140.73
train mean loss: 143.39
epoch train time: 0:00:01.689346
elapsed time: 0:03:02.089338
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 22:10:08.027719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.52
 ---- batch: 020 ----
mean loss: 152.44
 ---- batch: 030 ----
mean loss: 147.91
 ---- batch: 040 ----
mean loss: 151.15
train mean loss: 147.52
epoch train time: 0:00:01.679311
elapsed time: 0:03:03.768798
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 22:10:09.707196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.36
 ---- batch: 020 ----
mean loss: 147.68
 ---- batch: 030 ----
mean loss: 140.12
 ---- batch: 040 ----
mean loss: 136.65
train mean loss: 143.11
epoch train time: 0:00:01.798684
elapsed time: 0:03:05.567635
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 22:10:11.506018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.13
 ---- batch: 020 ----
mean loss: 146.13
 ---- batch: 030 ----
mean loss: 145.29
 ---- batch: 040 ----
mean loss: 143.23
train mean loss: 143.00
epoch train time: 0:00:01.683048
elapsed time: 0:03:07.250821
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 22:10:13.189203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.35
 ---- batch: 020 ----
mean loss: 144.99
 ---- batch: 030 ----
mean loss: 138.70
 ---- batch: 040 ----
mean loss: 138.59
train mean loss: 140.45
epoch train time: 0:00:01.780325
elapsed time: 0:03:09.031280
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 22:10:14.969678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.90
 ---- batch: 020 ----
mean loss: 135.83
 ---- batch: 030 ----
mean loss: 142.50
 ---- batch: 040 ----
mean loss: 146.09
train mean loss: 141.14
epoch train time: 0:00:01.687797
elapsed time: 0:03:10.719229
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 22:10:16.657611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.93
 ---- batch: 020 ----
mean loss: 144.34
 ---- batch: 030 ----
mean loss: 139.90
 ---- batch: 040 ----
mean loss: 139.72
train mean loss: 142.18
epoch train time: 0:00:01.782978
elapsed time: 0:03:12.502365
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 22:10:18.440741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.81
 ---- batch: 020 ----
mean loss: 144.10
 ---- batch: 030 ----
mean loss: 152.82
 ---- batch: 040 ----
mean loss: 154.09
train mean loss: 148.57
epoch train time: 0:00:01.686413
elapsed time: 0:03:14.188904
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 22:10:20.127307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.90
 ---- batch: 020 ----
mean loss: 140.95
 ---- batch: 030 ----
mean loss: 140.77
 ---- batch: 040 ----
mean loss: 140.68
train mean loss: 140.99
epoch train time: 0:00:01.682338
elapsed time: 0:03:15.871392
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 22:10:21.809773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.54
 ---- batch: 020 ----
mean loss: 134.41
 ---- batch: 030 ----
mean loss: 140.58
 ---- batch: 040 ----
mean loss: 141.58
train mean loss: 139.73
epoch train time: 0:00:01.789527
elapsed time: 0:03:17.661058
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 22:10:23.599427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.90
 ---- batch: 020 ----
mean loss: 138.13
 ---- batch: 030 ----
mean loss: 142.48
 ---- batch: 040 ----
mean loss: 136.47
train mean loss: 140.29
epoch train time: 0:00:01.689179
elapsed time: 0:03:19.350378
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 22:10:25.288788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.62
 ---- batch: 020 ----
mean loss: 144.94
 ---- batch: 030 ----
mean loss: 134.65
 ---- batch: 040 ----
mean loss: 143.07
train mean loss: 141.04
epoch train time: 0:00:01.802575
elapsed time: 0:03:21.153110
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 22:10:27.091491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.86
 ---- batch: 020 ----
mean loss: 134.41
 ---- batch: 030 ----
mean loss: 140.46
 ---- batch: 040 ----
mean loss: 133.16
train mean loss: 137.60
epoch train time: 0:00:01.688097
elapsed time: 0:03:22.841350
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 22:10:28.779733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.72
 ---- batch: 020 ----
mean loss: 138.23
 ---- batch: 030 ----
mean loss: 136.84
 ---- batch: 040 ----
mean loss: 134.05
train mean loss: 137.43
epoch train time: 0:00:01.782558
elapsed time: 0:03:24.624052
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 22:10:30.562430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.26
 ---- batch: 020 ----
mean loss: 136.17
 ---- batch: 030 ----
mean loss: 140.90
 ---- batch: 040 ----
mean loss: 137.77
train mean loss: 137.52
epoch train time: 0:00:01.686542
elapsed time: 0:03:26.310735
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 22:10:32.249119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.58
 ---- batch: 020 ----
mean loss: 136.01
 ---- batch: 030 ----
mean loss: 145.24
 ---- batch: 040 ----
mean loss: 135.38
train mean loss: 138.36
epoch train time: 0:00:01.784545
elapsed time: 0:03:28.095409
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 22:10:34.033796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.07
 ---- batch: 020 ----
mean loss: 138.63
 ---- batch: 030 ----
mean loss: 136.84
 ---- batch: 040 ----
mean loss: 136.78
train mean loss: 138.01
epoch train time: 0:00:01.682031
elapsed time: 0:03:29.777635
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 22:10:35.716031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.56
 ---- batch: 020 ----
mean loss: 130.95
 ---- batch: 030 ----
mean loss: 135.70
 ---- batch: 040 ----
mean loss: 140.27
train mean loss: 137.00
epoch train time: 0:00:01.778384
elapsed time: 0:03:31.556158
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 22:10:37.494552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.54
 ---- batch: 020 ----
mean loss: 136.35
 ---- batch: 030 ----
mean loss: 135.98
 ---- batch: 040 ----
mean loss: 131.75
train mean loss: 136.18
epoch train time: 0:00:01.689335
elapsed time: 0:03:33.245644
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 22:10:39.184026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.23
 ---- batch: 020 ----
mean loss: 133.24
 ---- batch: 030 ----
mean loss: 136.74
 ---- batch: 040 ----
mean loss: 133.83
train mean loss: 134.38
epoch train time: 0:00:01.780794
elapsed time: 0:03:35.026565
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 22:10:40.964943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.83
 ---- batch: 020 ----
mean loss: 139.80
 ---- batch: 030 ----
mean loss: 135.03
 ---- batch: 040 ----
mean loss: 136.66
train mean loss: 137.56
epoch train time: 0:00:01.691155
elapsed time: 0:03:36.717869
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 22:10:42.656250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.36
 ---- batch: 020 ----
mean loss: 135.35
 ---- batch: 030 ----
mean loss: 135.53
 ---- batch: 040 ----
mean loss: 136.35
train mean loss: 134.88
epoch train time: 0:00:01.676763
elapsed time: 0:03:38.394769
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 22:10:44.333147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.99
 ---- batch: 020 ----
mean loss: 135.00
 ---- batch: 030 ----
mean loss: 134.00
 ---- batch: 040 ----
mean loss: 134.92
train mean loss: 134.43
epoch train time: 0:00:01.797312
elapsed time: 0:03:40.192222
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 22:10:46.130621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.83
 ---- batch: 020 ----
mean loss: 132.59
 ---- batch: 030 ----
mean loss: 137.86
 ---- batch: 040 ----
mean loss: 133.94
train mean loss: 134.16
epoch train time: 0:00:01.689678
elapsed time: 0:03:41.882083
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 22:10:47.820470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.81
 ---- batch: 020 ----
mean loss: 136.53
 ---- batch: 030 ----
mean loss: 137.03
 ---- batch: 040 ----
mean loss: 143.77
train mean loss: 139.84
epoch train time: 0:00:01.790214
elapsed time: 0:03:43.672435
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 22:10:49.610815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.14
 ---- batch: 020 ----
mean loss: 134.69
 ---- batch: 030 ----
mean loss: 139.33
 ---- batch: 040 ----
mean loss: 133.21
train mean loss: 136.61
epoch train time: 0:00:01.686934
elapsed time: 0:03:45.359504
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 22:10:51.297884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.35
 ---- batch: 020 ----
mean loss: 139.71
 ---- batch: 030 ----
mean loss: 136.77
 ---- batch: 040 ----
mean loss: 130.71
train mean loss: 136.95
epoch train time: 0:00:01.782253
elapsed time: 0:03:47.141893
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 22:10:53.080271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.49
 ---- batch: 020 ----
mean loss: 133.30
 ---- batch: 030 ----
mean loss: 125.86
 ---- batch: 040 ----
mean loss: 130.71
train mean loss: 131.77
epoch train time: 0:00:01.690284
elapsed time: 0:03:48.832324
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 22:10:54.770704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.59
 ---- batch: 020 ----
mean loss: 133.45
 ---- batch: 030 ----
mean loss: 134.59
 ---- batch: 040 ----
mean loss: 133.44
train mean loss: 133.77
epoch train time: 0:00:01.782116
elapsed time: 0:03:50.614583
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 22:10:56.552978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.39
 ---- batch: 020 ----
mean loss: 138.43
 ---- batch: 030 ----
mean loss: 140.63
 ---- batch: 040 ----
mean loss: 136.32
train mean loss: 137.21
epoch train time: 0:00:01.691336
elapsed time: 0:03:52.306114
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 22:10:58.244505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.59
 ---- batch: 020 ----
mean loss: 135.39
 ---- batch: 030 ----
mean loss: 130.67
 ---- batch: 040 ----
mean loss: 138.93
train mean loss: 136.00
epoch train time: 0:00:01.789560
elapsed time: 0:03:54.095811
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 22:11:00.034191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.85
 ---- batch: 020 ----
mean loss: 139.00
 ---- batch: 030 ----
mean loss: 137.01
 ---- batch: 040 ----
mean loss: 133.25
train mean loss: 136.50
epoch train time: 0:00:01.692947
elapsed time: 0:03:55.788917
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 22:11:01.727303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.38
 ---- batch: 020 ----
mean loss: 137.01
 ---- batch: 030 ----
mean loss: 136.42
 ---- batch: 040 ----
mean loss: 131.70
train mean loss: 133.88
epoch train time: 0:00:01.781724
elapsed time: 0:03:57.570779
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 22:11:03.509162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.89
 ---- batch: 020 ----
mean loss: 130.64
 ---- batch: 030 ----
mean loss: 130.13
 ---- batch: 040 ----
mean loss: 130.25
train mean loss: 131.22
epoch train time: 0:00:01.694051
elapsed time: 0:03:59.264989
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 22:11:05.203388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.19
 ---- batch: 020 ----
mean loss: 132.69
 ---- batch: 030 ----
mean loss: 131.98
 ---- batch: 040 ----
mean loss: 132.44
train mean loss: 132.03
epoch train time: 0:00:01.685635
elapsed time: 0:04:00.950783
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 22:11:06.889196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.68
 ---- batch: 020 ----
mean loss: 132.78
 ---- batch: 030 ----
mean loss: 140.47
 ---- batch: 040 ----
mean loss: 132.74
train mean loss: 135.05
epoch train time: 0:00:01.790527
elapsed time: 0:04:02.741475
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 22:11:08.679871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.24
 ---- batch: 020 ----
mean loss: 130.97
 ---- batch: 030 ----
mean loss: 132.22
 ---- batch: 040 ----
mean loss: 126.98
train mean loss: 129.39
epoch train time: 0:00:01.689965
elapsed time: 0:04:04.431606
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 22:11:10.369986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.80
 ---- batch: 020 ----
mean loss: 130.85
 ---- batch: 030 ----
mean loss: 131.21
 ---- batch: 040 ----
mean loss: 132.10
train mean loss: 131.68
epoch train time: 0:00:01.797315
elapsed time: 0:04:06.229044
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 22:11:12.167422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.20
 ---- batch: 020 ----
mean loss: 132.88
 ---- batch: 030 ----
mean loss: 131.56
 ---- batch: 040 ----
mean loss: 130.28
train mean loss: 131.04
epoch train time: 0:00:01.692745
elapsed time: 0:04:07.921934
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 22:11:13.860316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.11
 ---- batch: 020 ----
mean loss: 131.67
 ---- batch: 030 ----
mean loss: 124.89
 ---- batch: 040 ----
mean loss: 128.61
train mean loss: 131.07
epoch train time: 0:00:01.781072
elapsed time: 0:04:09.703159
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 22:11:15.641537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.94
 ---- batch: 020 ----
mean loss: 125.81
 ---- batch: 030 ----
mean loss: 131.07
 ---- batch: 040 ----
mean loss: 131.32
train mean loss: 128.98
epoch train time: 0:00:01.690664
elapsed time: 0:04:11.393957
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 22:11:17.332341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.65
 ---- batch: 020 ----
mean loss: 135.24
 ---- batch: 030 ----
mean loss: 129.67
 ---- batch: 040 ----
mean loss: 127.18
train mean loss: 130.42
epoch train time: 0:00:01.786583
elapsed time: 0:04:13.180676
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 22:11:19.119061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.95
 ---- batch: 020 ----
mean loss: 130.86
 ---- batch: 030 ----
mean loss: 129.57
 ---- batch: 040 ----
mean loss: 131.38
train mean loss: 131.27
epoch train time: 0:00:01.679522
elapsed time: 0:04:14.860354
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 22:11:20.798749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.37
 ---- batch: 020 ----
mean loss: 132.27
 ---- batch: 030 ----
mean loss: 131.63
 ---- batch: 040 ----
mean loss: 138.03
train mean loss: 134.30
epoch train time: 0:00:01.794624
elapsed time: 0:04:16.655115
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 22:11:22.593491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.32
 ---- batch: 020 ----
mean loss: 135.49
 ---- batch: 030 ----
mean loss: 131.69
 ---- batch: 040 ----
mean loss: 129.85
train mean loss: 131.52
epoch train time: 0:00:01.695680
elapsed time: 0:04:18.350941
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 22:11:24.289324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.85
 ---- batch: 020 ----
mean loss: 128.32
 ---- batch: 030 ----
mean loss: 130.71
 ---- batch: 040 ----
mean loss: 129.36
train mean loss: 129.02
epoch train time: 0:00:01.787079
elapsed time: 0:04:20.138154
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 22:11:26.076534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.24
 ---- batch: 020 ----
mean loss: 126.81
 ---- batch: 030 ----
mean loss: 135.68
 ---- batch: 040 ----
mean loss: 131.60
train mean loss: 130.35
epoch train time: 0:00:01.692258
elapsed time: 0:04:21.830577
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 22:11:27.768979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.36
 ---- batch: 020 ----
mean loss: 131.79
 ---- batch: 030 ----
mean loss: 135.13
 ---- batch: 040 ----
mean loss: 126.15
train mean loss: 131.06
epoch train time: 0:00:01.718966
elapsed time: 0:04:23.549688
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 22:11:29.488067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.71
 ---- batch: 020 ----
mean loss: 127.21
 ---- batch: 030 ----
mean loss: 125.61
 ---- batch: 040 ----
mean loss: 129.34
train mean loss: 129.48
epoch train time: 0:00:01.756783
elapsed time: 0:04:25.306606
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 22:11:31.245005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.78
 ---- batch: 020 ----
mean loss: 126.99
 ---- batch: 030 ----
mean loss: 130.50
 ---- batch: 040 ----
mean loss: 135.69
train mean loss: 129.93
epoch train time: 0:00:01.681304
elapsed time: 0:04:26.988065
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 22:11:32.926463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.56
 ---- batch: 020 ----
mean loss: 125.30
 ---- batch: 030 ----
mean loss: 128.53
 ---- batch: 040 ----
mean loss: 128.20
train mean loss: 128.39
epoch train time: 0:00:01.798037
elapsed time: 0:04:28.786304
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 22:11:34.724702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.83
 ---- batch: 020 ----
mean loss: 131.34
 ---- batch: 030 ----
mean loss: 130.74
 ---- batch: 040 ----
mean loss: 126.12
train mean loss: 128.95
epoch train time: 0:00:01.689244
elapsed time: 0:04:30.475753
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 22:11:36.414127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.07
 ---- batch: 020 ----
mean loss: 127.69
 ---- batch: 030 ----
mean loss: 126.51
 ---- batch: 040 ----
mean loss: 127.55
train mean loss: 129.57
epoch train time: 0:00:01.783151
elapsed time: 0:04:32.259025
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 22:11:38.197405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.24
 ---- batch: 020 ----
mean loss: 127.64
 ---- batch: 030 ----
mean loss: 130.93
 ---- batch: 040 ----
mean loss: 125.44
train mean loss: 128.19
epoch train time: 0:00:01.686864
elapsed time: 0:04:33.946031
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 22:11:39.884438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.48
 ---- batch: 020 ----
mean loss: 127.42
 ---- batch: 030 ----
mean loss: 129.86
 ---- batch: 040 ----
mean loss: 132.70
train mean loss: 129.68
epoch train time: 0:00:01.780447
elapsed time: 0:04:35.726628
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 22:11:41.665003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.08
 ---- batch: 020 ----
mean loss: 127.69
 ---- batch: 030 ----
mean loss: 128.05
 ---- batch: 040 ----
mean loss: 130.43
train mean loss: 128.70
epoch train time: 0:00:01.685825
elapsed time: 0:04:37.412597
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 22:11:43.350978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.47
 ---- batch: 020 ----
mean loss: 123.77
 ---- batch: 030 ----
mean loss: 124.12
 ---- batch: 040 ----
mean loss: 126.66
train mean loss: 125.75
epoch train time: 0:00:01.784949
elapsed time: 0:04:39.197684
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 22:11:45.136080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.68
 ---- batch: 020 ----
mean loss: 141.16
 ---- batch: 030 ----
mean loss: 129.41
 ---- batch: 040 ----
mean loss: 130.25
train mean loss: 132.30
epoch train time: 0:00:01.704955
elapsed time: 0:04:40.902788
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 22:11:46.841168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.85
 ---- batch: 020 ----
mean loss: 132.14
 ---- batch: 030 ----
mean loss: 132.87
 ---- batch: 040 ----
mean loss: 123.33
train mean loss: 129.13
epoch train time: 0:00:01.678543
elapsed time: 0:04:42.581458
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 22:11:48.519852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.15
 ---- batch: 020 ----
mean loss: 130.27
 ---- batch: 030 ----
mean loss: 121.66
 ---- batch: 040 ----
mean loss: 129.50
train mean loss: 125.90
epoch train time: 0:00:01.794076
elapsed time: 0:04:44.375672
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 22:11:50.314052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.14
 ---- batch: 020 ----
mean loss: 128.96
 ---- batch: 030 ----
mean loss: 129.66
 ---- batch: 040 ----
mean loss: 133.02
train mean loss: 129.50
epoch train time: 0:00:01.691002
elapsed time: 0:04:46.066818
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 22:11:52.005196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.41
 ---- batch: 020 ----
mean loss: 124.21
 ---- batch: 030 ----
mean loss: 129.60
 ---- batch: 040 ----
mean loss: 125.07
train mean loss: 125.34
epoch train time: 0:00:01.783016
elapsed time: 0:04:47.850034
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 22:11:53.788462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.50
 ---- batch: 020 ----
mean loss: 121.84
 ---- batch: 030 ----
mean loss: 126.39
 ---- batch: 040 ----
mean loss: 133.67
train mean loss: 125.91
epoch train time: 0:00:01.690321
elapsed time: 0:04:49.540561
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 22:11:55.478949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.28
 ---- batch: 020 ----
mean loss: 124.25
 ---- batch: 030 ----
mean loss: 128.02
 ---- batch: 040 ----
mean loss: 132.72
train mean loss: 128.25
epoch train time: 0:00:01.678092
elapsed time: 0:04:51.218787
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 22:11:57.157165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.97
 ---- batch: 020 ----
mean loss: 130.00
 ---- batch: 030 ----
mean loss: 124.37
 ---- batch: 040 ----
mean loss: 127.86
train mean loss: 127.13
epoch train time: 0:00:01.787885
elapsed time: 0:04:53.006842
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 22:11:58.945224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.26
 ---- batch: 020 ----
mean loss: 123.21
 ---- batch: 030 ----
mean loss: 124.63
 ---- batch: 040 ----
mean loss: 125.29
train mean loss: 122.85
epoch train time: 0:00:01.687258
elapsed time: 0:04:54.694239
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 22:12:00.632621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.70
 ---- batch: 020 ----
mean loss: 128.10
 ---- batch: 030 ----
mean loss: 130.48
 ---- batch: 040 ----
mean loss: 125.25
train mean loss: 128.93
epoch train time: 0:00:01.794049
elapsed time: 0:04:56.488427
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 22:12:02.426811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.41
 ---- batch: 020 ----
mean loss: 134.63
 ---- batch: 030 ----
mean loss: 129.69
 ---- batch: 040 ----
mean loss: 127.64
train mean loss: 130.60
epoch train time: 0:00:01.695068
elapsed time: 0:04:58.183639
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 22:12:04.122080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.97
 ---- batch: 020 ----
mean loss: 128.63
 ---- batch: 030 ----
mean loss: 119.85
 ---- batch: 040 ----
mean loss: 126.05
train mean loss: 127.66
epoch train time: 0:00:01.787249
elapsed time: 0:04:59.971074
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 22:12:05.909455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.36
 ---- batch: 020 ----
mean loss: 123.87
 ---- batch: 030 ----
mean loss: 125.54
 ---- batch: 040 ----
mean loss: 120.44
train mean loss: 124.46
epoch train time: 0:00:01.698689
elapsed time: 0:05:01.669910
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 22:12:07.608320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.53
 ---- batch: 020 ----
mean loss: 127.96
 ---- batch: 030 ----
mean loss: 124.76
 ---- batch: 040 ----
mean loss: 125.83
train mean loss: 126.90
epoch train time: 0:00:01.687491
elapsed time: 0:05:03.357581
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 22:12:09.295982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.74
 ---- batch: 020 ----
mean loss: 126.70
 ---- batch: 030 ----
mean loss: 125.73
 ---- batch: 040 ----
mean loss: 124.23
train mean loss: 128.12
epoch train time: 0:00:01.783989
elapsed time: 0:05:05.141716
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 22:12:11.080113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.97
 ---- batch: 020 ----
mean loss: 127.79
 ---- batch: 030 ----
mean loss: 127.22
 ---- batch: 040 ----
mean loss: 128.47
train mean loss: 127.15
epoch train time: 0:00:01.675554
elapsed time: 0:05:06.817418
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 22:12:12.755806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.41
 ---- batch: 020 ----
mean loss: 125.01
 ---- batch: 030 ----
mean loss: 125.03
 ---- batch: 040 ----
mean loss: 123.36
train mean loss: 124.46
epoch train time: 0:00:01.778322
elapsed time: 0:05:08.595888
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 22:12:14.534267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.14
 ---- batch: 020 ----
mean loss: 123.35
 ---- batch: 030 ----
mean loss: 129.40
 ---- batch: 040 ----
mean loss: 119.64
train mean loss: 125.04
epoch train time: 0:00:01.691940
elapsed time: 0:05:10.287972
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 22:12:16.226378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.40
 ---- batch: 020 ----
mean loss: 122.86
 ---- batch: 030 ----
mean loss: 128.25
 ---- batch: 040 ----
mean loss: 129.92
train mean loss: 127.22
epoch train time: 0:00:01.685296
elapsed time: 0:05:11.973439
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 22:12:17.911822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.77
 ---- batch: 020 ----
mean loss: 124.47
 ---- batch: 030 ----
mean loss: 124.26
 ---- batch: 040 ----
mean loss: 125.94
train mean loss: 124.33
epoch train time: 0:00:01.788244
elapsed time: 0:05:13.761832
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 22:12:19.700202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.43
 ---- batch: 020 ----
mean loss: 124.60
 ---- batch: 030 ----
mean loss: 122.23
 ---- batch: 040 ----
mean loss: 120.45
train mean loss: 123.27
epoch train time: 0:00:01.684715
elapsed time: 0:05:15.446857
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 22:12:21.385291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.44
 ---- batch: 020 ----
mean loss: 126.17
 ---- batch: 030 ----
mean loss: 121.25
 ---- batch: 040 ----
mean loss: 125.40
train mean loss: 123.33
epoch train time: 0:00:01.786518
elapsed time: 0:05:17.233564
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 22:12:23.171939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.98
 ---- batch: 020 ----
mean loss: 122.94
 ---- batch: 030 ----
mean loss: 124.24
 ---- batch: 040 ----
mean loss: 124.46
train mean loss: 125.00
epoch train time: 0:00:01.682643
elapsed time: 0:05:18.916337
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 22:12:24.854719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.13
 ---- batch: 020 ----
mean loss: 122.07
 ---- batch: 030 ----
mean loss: 123.96
 ---- batch: 040 ----
mean loss: 120.70
train mean loss: 122.60
epoch train time: 0:00:01.787900
elapsed time: 0:05:20.704363
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 22:12:26.642742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.25
 ---- batch: 020 ----
mean loss: 121.38
 ---- batch: 030 ----
mean loss: 122.69
 ---- batch: 040 ----
mean loss: 122.72
train mean loss: 123.95
epoch train time: 0:00:01.690218
elapsed time: 0:05:22.394712
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 22:12:28.333106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.59
 ---- batch: 020 ----
mean loss: 130.14
 ---- batch: 030 ----
mean loss: 129.14
 ---- batch: 040 ----
mean loss: 122.47
train mean loss: 127.62
epoch train time: 0:00:01.679049
elapsed time: 0:05:24.073928
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 22:12:30.012310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.69
 ---- batch: 020 ----
mean loss: 124.86
 ---- batch: 030 ----
mean loss: 120.01
 ---- batch: 040 ----
mean loss: 123.13
train mean loss: 122.94
epoch train time: 0:00:01.785272
elapsed time: 0:05:25.859340
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 22:12:31.797719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.86
 ---- batch: 020 ----
mean loss: 124.78
 ---- batch: 030 ----
mean loss: 122.80
 ---- batch: 040 ----
mean loss: 127.72
train mean loss: 125.59
epoch train time: 0:00:01.680040
elapsed time: 0:05:27.539513
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 22:12:33.477895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.65
 ---- batch: 020 ----
mean loss: 126.05
 ---- batch: 030 ----
mean loss: 121.96
 ---- batch: 040 ----
mean loss: 126.95
train mean loss: 123.65
epoch train time: 0:00:01.806928
elapsed time: 0:05:29.346612
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 22:12:35.284992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.11
 ---- batch: 020 ----
mean loss: 131.23
 ---- batch: 030 ----
mean loss: 125.73
 ---- batch: 040 ----
mean loss: 124.05
train mean loss: 125.99
epoch train time: 0:00:01.681980
elapsed time: 0:05:31.028725
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 22:12:36.967106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.28
 ---- batch: 020 ----
mean loss: 122.47
 ---- batch: 030 ----
mean loss: 123.53
 ---- batch: 040 ----
mean loss: 125.66
train mean loss: 123.71
epoch train time: 0:00:01.780441
elapsed time: 0:05:32.809309
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 22:12:38.747690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.84
 ---- batch: 020 ----
mean loss: 118.33
 ---- batch: 030 ----
mean loss: 123.81
 ---- batch: 040 ----
mean loss: 122.31
train mean loss: 121.73
epoch train time: 0:00:01.689304
elapsed time: 0:05:34.498749
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 22:12:40.437129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.42
 ---- batch: 020 ----
mean loss: 126.38
 ---- batch: 030 ----
mean loss: 120.29
 ---- batch: 040 ----
mean loss: 116.55
train mean loss: 121.04
epoch train time: 0:00:01.674948
elapsed time: 0:05:36.173842
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 22:12:42.112266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.79
 ---- batch: 020 ----
mean loss: 119.04
 ---- batch: 030 ----
mean loss: 121.94
 ---- batch: 040 ----
mean loss: 121.71
train mean loss: 121.41
epoch train time: 0:00:01.794717
elapsed time: 0:05:37.968738
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 22:12:43.907123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.11
 ---- batch: 020 ----
mean loss: 130.73
 ---- batch: 030 ----
mean loss: 124.15
 ---- batch: 040 ----
mean loss: 124.40
train mean loss: 126.63
epoch train time: 0:00:01.688236
elapsed time: 0:05:39.657111
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 22:12:45.595490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.22
 ---- batch: 020 ----
mean loss: 124.60
 ---- batch: 030 ----
mean loss: 120.85
 ---- batch: 040 ----
mean loss: 125.47
train mean loss: 122.61
epoch train time: 0:00:01.788185
elapsed time: 0:05:41.445435
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 22:12:47.383815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.63
 ---- batch: 020 ----
mean loss: 135.54
 ---- batch: 030 ----
mean loss: 123.44
 ---- batch: 040 ----
mean loss: 128.03
train mean loss: 128.00
epoch train time: 0:00:01.687735
elapsed time: 0:05:43.133364
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 22:12:49.071789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.27
 ---- batch: 020 ----
mean loss: 126.71
 ---- batch: 030 ----
mean loss: 130.21
 ---- batch: 040 ----
mean loss: 126.92
train mean loss: 126.97
epoch train time: 0:00:01.679822
elapsed time: 0:05:44.813354
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 22:12:50.751730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.24
 ---- batch: 020 ----
mean loss: 125.42
 ---- batch: 030 ----
mean loss: 121.73
 ---- batch: 040 ----
mean loss: 120.38
train mean loss: 122.74
epoch train time: 0:00:01.788011
elapsed time: 0:05:46.601533
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 22:12:52.539961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.21
 ---- batch: 020 ----
mean loss: 120.32
 ---- batch: 030 ----
mean loss: 124.65
 ---- batch: 040 ----
mean loss: 122.24
train mean loss: 121.75
epoch train time: 0:00:01.680385
elapsed time: 0:05:48.282134
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 22:12:54.220519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.32
 ---- batch: 020 ----
mean loss: 123.51
 ---- batch: 030 ----
mean loss: 121.69
 ---- batch: 040 ----
mean loss: 119.60
train mean loss: 121.33
epoch train time: 0:00:01.780802
elapsed time: 0:05:50.063098
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 22:12:56.001492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.58
 ---- batch: 020 ----
mean loss: 124.27
 ---- batch: 030 ----
mean loss: 123.87
 ---- batch: 040 ----
mean loss: 120.77
train mean loss: 121.53
epoch train time: 0:00:01.685898
elapsed time: 0:05:51.749159
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 22:12:57.687544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.72
 ---- batch: 020 ----
mean loss: 126.62
 ---- batch: 030 ----
mean loss: 117.20
 ---- batch: 040 ----
mean loss: 126.55
train mean loss: 122.90
epoch train time: 0:00:01.778323
elapsed time: 0:05:53.527642
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 22:12:59.466046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.56
 ---- batch: 020 ----
mean loss: 121.04
 ---- batch: 030 ----
mean loss: 120.40
 ---- batch: 040 ----
mean loss: 128.23
train mean loss: 122.82
epoch train time: 0:00:01.687597
elapsed time: 0:05:55.215404
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 22:13:01.153805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.41
 ---- batch: 020 ----
mean loss: 129.25
 ---- batch: 030 ----
mean loss: 126.45
 ---- batch: 040 ----
mean loss: 120.63
train mean loss: 125.75
epoch train time: 0:00:01.682283
elapsed time: 0:05:56.897897
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 22:13:02.836279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.54
 ---- batch: 020 ----
mean loss: 131.46
 ---- batch: 030 ----
mean loss: 132.82
 ---- batch: 040 ----
mean loss: 120.14
train mean loss: 126.35
epoch train time: 0:00:01.784946
elapsed time: 0:05:58.683022
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 22:13:04.621416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.23
 ---- batch: 020 ----
mean loss: 120.22
 ---- batch: 030 ----
mean loss: 121.57
 ---- batch: 040 ----
mean loss: 119.93
train mean loss: 121.22
epoch train time: 0:00:01.688789
elapsed time: 0:06:00.371961
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 22:13:06.310350
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.20
 ---- batch: 020 ----
mean loss: 117.67
 ---- batch: 030 ----
mean loss: 118.17
 ---- batch: 040 ----
mean loss: 116.83
train mean loss: 118.32
epoch train time: 0:00:01.787909
elapsed time: 0:06:02.160048
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 22:13:08.098419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.44
 ---- batch: 020 ----
mean loss: 118.55
 ---- batch: 030 ----
mean loss: 118.23
 ---- batch: 040 ----
mean loss: 113.40
train mean loss: 117.17
epoch train time: 0:00:01.686891
elapsed time: 0:06:03.847068
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 22:13:09.785480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.26
 ---- batch: 020 ----
mean loss: 120.92
 ---- batch: 030 ----
mean loss: 118.00
 ---- batch: 040 ----
mean loss: 116.26
train mean loss: 118.32
epoch train time: 0:00:01.789909
elapsed time: 0:06:05.637170
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 22:13:11.575551
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.97
 ---- batch: 020 ----
mean loss: 116.26
 ---- batch: 030 ----
mean loss: 119.09
 ---- batch: 040 ----
mean loss: 123.22
train mean loss: 119.65
epoch train time: 0:00:01.685207
elapsed time: 0:06:07.322520
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 22:13:13.260904
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.94
 ---- batch: 020 ----
mean loss: 118.21
 ---- batch: 030 ----
mean loss: 117.60
 ---- batch: 040 ----
mean loss: 119.04
train mean loss: 118.25
epoch train time: 0:00:01.773000
elapsed time: 0:06:09.095672
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 22:13:15.034055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.05
 ---- batch: 020 ----
mean loss: 119.76
 ---- batch: 030 ----
mean loss: 122.19
 ---- batch: 040 ----
mean loss: 115.65
train mean loss: 118.92
epoch train time: 0:00:01.692469
elapsed time: 0:06:10.788313
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 22:13:16.726711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.74
 ---- batch: 020 ----
mean loss: 118.30
 ---- batch: 030 ----
mean loss: 114.23
 ---- batch: 040 ----
mean loss: 118.06
train mean loss: 118.00
epoch train time: 0:00:01.680631
elapsed time: 0:06:12.469127
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 22:13:18.407528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.04
 ---- batch: 020 ----
mean loss: 121.08
 ---- batch: 030 ----
mean loss: 114.69
 ---- batch: 040 ----
mean loss: 118.65
train mean loss: 118.25
epoch train time: 0:00:01.791848
elapsed time: 0:06:14.261120
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 22:13:20.199498
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.79
 ---- batch: 020 ----
mean loss: 120.06
 ---- batch: 030 ----
mean loss: 117.07
 ---- batch: 040 ----
mean loss: 120.73
train mean loss: 118.41
epoch train time: 0:00:01.687758
elapsed time: 0:06:15.949027
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 22:13:21.887410
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.08
 ---- batch: 020 ----
mean loss: 121.41
 ---- batch: 030 ----
mean loss: 119.81
 ---- batch: 040 ----
mean loss: 118.73
train mean loss: 119.84
epoch train time: 0:00:01.787710
elapsed time: 0:06:17.736881
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 22:13:23.675263
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.46
 ---- batch: 020 ----
mean loss: 116.93
 ---- batch: 030 ----
mean loss: 115.19
 ---- batch: 040 ----
mean loss: 119.70
train mean loss: 117.44
epoch train time: 0:00:01.690579
elapsed time: 0:06:19.427616
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 22:13:25.366000
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.72
 ---- batch: 020 ----
mean loss: 117.78
 ---- batch: 030 ----
mean loss: 119.13
 ---- batch: 040 ----
mean loss: 115.43
train mean loss: 118.39
epoch train time: 0:00:01.675642
elapsed time: 0:06:21.103401
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 22:13:27.041779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.40
 ---- batch: 020 ----
mean loss: 113.36
 ---- batch: 030 ----
mean loss: 120.38
 ---- batch: 040 ----
mean loss: 119.89
train mean loss: 118.78
epoch train time: 0:00:01.794083
elapsed time: 0:06:22.897616
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 22:13:28.836000
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.34
 ---- batch: 020 ----
mean loss: 117.64
 ---- batch: 030 ----
mean loss: 117.18
 ---- batch: 040 ----
mean loss: 116.12
train mean loss: 116.04
epoch train time: 0:00:01.686281
elapsed time: 0:06:24.584066
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 22:13:30.522489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.43
 ---- batch: 020 ----
mean loss: 115.36
 ---- batch: 030 ----
mean loss: 113.91
 ---- batch: 040 ----
mean loss: 118.71
train mean loss: 116.38
epoch train time: 0:00:01.794989
elapsed time: 0:06:26.379225
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 22:13:32.317603
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.14
 ---- batch: 020 ----
mean loss: 117.76
 ---- batch: 030 ----
mean loss: 122.51
 ---- batch: 040 ----
mean loss: 117.64
train mean loss: 119.16
epoch train time: 0:00:01.681007
elapsed time: 0:06:28.060366
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 22:13:33.998767
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.81
 ---- batch: 020 ----
mean loss: 117.81
 ---- batch: 030 ----
mean loss: 119.07
 ---- batch: 040 ----
mean loss: 119.94
train mean loss: 118.79
epoch train time: 0:00:01.781249
elapsed time: 0:06:29.841784
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 22:13:35.780164
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.91
 ---- batch: 020 ----
mean loss: 121.22
 ---- batch: 030 ----
mean loss: 115.11
 ---- batch: 040 ----
mean loss: 115.21
train mean loss: 116.46
epoch train time: 0:00:01.693956
elapsed time: 0:06:31.535873
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 22:13:37.474265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.61
 ---- batch: 020 ----
mean loss: 116.06
 ---- batch: 030 ----
mean loss: 118.14
 ---- batch: 040 ----
mean loss: 119.57
train mean loss: 117.44
epoch train time: 0:00:01.678558
elapsed time: 0:06:33.214632
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 22:13:39.153015
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.68
 ---- batch: 020 ----
mean loss: 117.15
 ---- batch: 030 ----
mean loss: 116.76
 ---- batch: 040 ----
mean loss: 119.08
train mean loss: 117.33
epoch train time: 0:00:01.782704
elapsed time: 0:06:34.997472
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 22:13:40.935851
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.98
 ---- batch: 020 ----
mean loss: 117.26
 ---- batch: 030 ----
mean loss: 113.38
 ---- batch: 040 ----
mean loss: 120.70
train mean loss: 118.09
epoch train time: 0:00:01.687046
elapsed time: 0:06:36.684643
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 22:13:42.623023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.13
 ---- batch: 020 ----
mean loss: 117.72
 ---- batch: 030 ----
mean loss: 118.51
 ---- batch: 040 ----
mean loss: 115.99
train mean loss: 117.43
epoch train time: 0:00:01.780818
elapsed time: 0:06:38.465596
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 22:13:44.403974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.54
 ---- batch: 020 ----
mean loss: 117.15
 ---- batch: 030 ----
mean loss: 122.91
 ---- batch: 040 ----
mean loss: 113.52
train mean loss: 116.96
epoch train time: 0:00:01.685599
elapsed time: 0:06:40.151320
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 22:13:46.089711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.60
 ---- batch: 020 ----
mean loss: 119.64
 ---- batch: 030 ----
mean loss: 113.61
 ---- batch: 040 ----
mean loss: 119.58
train mean loss: 117.35
epoch train time: 0:00:01.780002
elapsed time: 0:06:41.931456
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 22:13:47.869834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.50
 ---- batch: 020 ----
mean loss: 114.72
 ---- batch: 030 ----
mean loss: 116.87
 ---- batch: 040 ----
mean loss: 119.09
train mean loss: 116.84
epoch train time: 0:00:01.684144
elapsed time: 0:06:43.615732
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 22:13:49.554116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.25
 ---- batch: 020 ----
mean loss: 114.84
 ---- batch: 030 ----
mean loss: 117.62
 ---- batch: 040 ----
mean loss: 117.96
train mean loss: 117.25
epoch train time: 0:00:01.680769
elapsed time: 0:06:45.296644
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 22:13:51.235038
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.41
 ---- batch: 020 ----
mean loss: 118.64
 ---- batch: 030 ----
mean loss: 117.97
 ---- batch: 040 ----
mean loss: 113.70
train mean loss: 117.55
epoch train time: 0:00:01.728634
elapsed time: 0:06:47.025428
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 22:13:52.963808
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.78
 ---- batch: 020 ----
mean loss: 115.24
 ---- batch: 030 ----
mean loss: 115.97
 ---- batch: 040 ----
mean loss: 118.95
train mean loss: 116.50
epoch train time: 0:00:01.682004
elapsed time: 0:06:48.707564
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 22:13:54.645976
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.98
 ---- batch: 020 ----
mean loss: 120.24
 ---- batch: 030 ----
mean loss: 115.98
 ---- batch: 040 ----
mean loss: 118.43
train mean loss: 117.76
epoch train time: 0:00:01.789404
elapsed time: 0:06:50.497175
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 22:13:56.435569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.01
 ---- batch: 020 ----
mean loss: 116.95
 ---- batch: 030 ----
mean loss: 120.04
 ---- batch: 040 ----
mean loss: 114.77
train mean loss: 117.13
epoch train time: 0:00:01.690252
elapsed time: 0:06:52.187601
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 22:13:58.125982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.11
 ---- batch: 020 ----
mean loss: 115.51
 ---- batch: 030 ----
mean loss: 115.67
 ---- batch: 040 ----
mean loss: 117.21
train mean loss: 115.72
epoch train time: 0:00:01.680246
elapsed time: 0:06:53.867995
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 22:13:59.806395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.49
 ---- batch: 020 ----
mean loss: 119.58
 ---- batch: 030 ----
mean loss: 121.16
 ---- batch: 040 ----
mean loss: 115.05
train mean loss: 119.07
epoch train time: 0:00:01.788235
elapsed time: 0:06:55.656371
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 22:14:01.594749
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.58
 ---- batch: 020 ----
mean loss: 120.47
 ---- batch: 030 ----
mean loss: 118.95
 ---- batch: 040 ----
mean loss: 118.00
train mean loss: 118.61
epoch train time: 0:00:01.695048
elapsed time: 0:06:57.351587
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 22:14:03.289989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.42
 ---- batch: 020 ----
mean loss: 117.68
 ---- batch: 030 ----
mean loss: 118.80
 ---- batch: 040 ----
mean loss: 119.35
train mean loss: 117.58
epoch train time: 0:00:01.781429
elapsed time: 0:06:59.133163
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 22:14:05.071544
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.81
 ---- batch: 020 ----
mean loss: 117.25
 ---- batch: 030 ----
mean loss: 117.04
 ---- batch: 040 ----
mean loss: 114.46
train mean loss: 116.44
epoch train time: 0:00:01.682799
elapsed time: 0:07:00.816096
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 22:14:06.754477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.39
 ---- batch: 020 ----
mean loss: 112.50
 ---- batch: 030 ----
mean loss: 117.62
 ---- batch: 040 ----
mean loss: 119.76
train mean loss: 116.91
epoch train time: 0:00:01.780408
elapsed time: 0:07:02.596657
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 22:14:08.535039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.01
 ---- batch: 020 ----
mean loss: 113.86
 ---- batch: 030 ----
mean loss: 113.33
 ---- batch: 040 ----
mean loss: 116.13
train mean loss: 116.04
epoch train time: 0:00:01.695763
elapsed time: 0:07:04.292560
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 22:14:10.230946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.31
 ---- batch: 020 ----
mean loss: 115.33
 ---- batch: 030 ----
mean loss: 117.30
 ---- batch: 040 ----
mean loss: 119.10
train mean loss: 117.24
epoch train time: 0:00:01.701691
elapsed time: 0:07:05.994383
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 22:14:11.932777
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.41
 ---- batch: 020 ----
mean loss: 115.89
 ---- batch: 030 ----
mean loss: 117.66
 ---- batch: 040 ----
mean loss: 121.19
train mean loss: 118.30
epoch train time: 0:00:01.769632
elapsed time: 0:07:07.764172
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 22:14:13.702558
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.42
 ---- batch: 020 ----
mean loss: 117.94
 ---- batch: 030 ----
mean loss: 115.04
 ---- batch: 040 ----
mean loss: 116.82
train mean loss: 117.51
epoch train time: 0:00:01.683576
elapsed time: 0:07:09.447881
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 22:14:15.386261
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.32
 ---- batch: 020 ----
mean loss: 114.92
 ---- batch: 030 ----
mean loss: 119.97
 ---- batch: 040 ----
mean loss: 117.19
train mean loss: 116.52
epoch train time: 0:00:01.786286
elapsed time: 0:07:11.234333
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 22:14:17.172710
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.47
 ---- batch: 020 ----
mean loss: 116.83
 ---- batch: 030 ----
mean loss: 118.50
 ---- batch: 040 ----
mean loss: 115.95
train mean loss: 117.17
epoch train time: 0:00:01.687974
elapsed time: 0:07:12.922451
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 22:14:18.860835
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.02
 ---- batch: 020 ----
mean loss: 116.58
 ---- batch: 030 ----
mean loss: 117.56
 ---- batch: 040 ----
mean loss: 114.83
train mean loss: 116.30
epoch train time: 0:00:01.789784
elapsed time: 0:07:14.712360
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 22:14:20.650738
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.84
 ---- batch: 020 ----
mean loss: 117.76
 ---- batch: 030 ----
mean loss: 118.20
 ---- batch: 040 ----
mean loss: 115.32
train mean loss: 117.16
epoch train time: 0:00:01.680492
elapsed time: 0:07:16.393004
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 22:14:22.331386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.79
 ---- batch: 020 ----
mean loss: 122.77
 ---- batch: 030 ----
mean loss: 117.51
 ---- batch: 040 ----
mean loss: 116.53
train mean loss: 118.36
epoch train time: 0:00:01.720001
elapsed time: 0:07:18.113197
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 22:14:24.051591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.37
 ---- batch: 020 ----
mean loss: 118.05
 ---- batch: 030 ----
mean loss: 119.43
 ---- batch: 040 ----
mean loss: 118.70
train mean loss: 118.78
epoch train time: 0:00:01.688048
elapsed time: 0:07:19.801398
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 22:14:25.739782
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.61
 ---- batch: 020 ----
mean loss: 117.60
 ---- batch: 030 ----
mean loss: 115.47
 ---- batch: 040 ----
mean loss: 116.52
train mean loss: 116.78
epoch train time: 0:00:01.793071
elapsed time: 0:07:21.594623
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 22:14:27.533016
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.84
 ---- batch: 020 ----
mean loss: 117.62
 ---- batch: 030 ----
mean loss: 117.60
 ---- batch: 040 ----
mean loss: 115.31
train mean loss: 118.18
epoch train time: 0:00:01.681007
elapsed time: 0:07:23.275787
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 22:14:29.214182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.70
 ---- batch: 020 ----
mean loss: 115.85
 ---- batch: 030 ----
mean loss: 119.47
 ---- batch: 040 ----
mean loss: 116.48
train mean loss: 117.42
epoch train time: 0:00:01.783758
elapsed time: 0:07:25.062866
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_0/checkpoint.pth.tar
**** end time: 2019-09-20 22:14:31.001217 ****
