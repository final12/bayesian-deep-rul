Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_2', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8528
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistDense3...
Done.
**** start time: 2019-09-20 23:40:21.377577 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
            Linear-2                  [-1, 100]          42,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 62,100
Trainable params: 62,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:40:21.380819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4732.32
 ---- batch: 020 ----
mean loss: 4634.88
 ---- batch: 030 ----
mean loss: 4600.60
 ---- batch: 040 ----
mean loss: 4456.46
train mean loss: 4589.84
epoch train time: 0:00:14.873061
elapsed time: 0:00:14.878438
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:40:36.256062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4402.13
 ---- batch: 020 ----
mean loss: 4274.53
 ---- batch: 030 ----
mean loss: 4189.82
 ---- batch: 040 ----
mean loss: 4220.72
train mean loss: 4259.71
epoch train time: 0:00:00.208277
elapsed time: 0:00:15.086859
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:40:36.464493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4039.04
 ---- batch: 020 ----
mean loss: 4009.22
 ---- batch: 030 ----
mean loss: 4008.37
 ---- batch: 040 ----
mean loss: 3882.62
train mean loss: 3969.41
epoch train time: 0:00:00.205004
elapsed time: 0:00:15.291993
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:40:36.669609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3767.26
 ---- batch: 020 ----
mean loss: 3817.77
 ---- batch: 030 ----
mean loss: 3558.25
 ---- batch: 040 ----
mean loss: 3630.53
train mean loss: 3693.39
epoch train time: 0:00:00.200204
elapsed time: 0:00:15.492323
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:40:36.869940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3550.94
 ---- batch: 020 ----
mean loss: 3449.91
 ---- batch: 030 ----
mean loss: 3427.47
 ---- batch: 040 ----
mean loss: 3341.91
train mean loss: 3433.75
epoch train time: 0:00:00.200081
elapsed time: 0:00:15.692516
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:40:37.070133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3260.45
 ---- batch: 020 ----
mean loss: 3273.79
 ---- batch: 030 ----
mean loss: 3172.38
 ---- batch: 040 ----
mean loss: 3105.34
train mean loss: 3198.05
epoch train time: 0:00:00.199762
elapsed time: 0:00:15.892393
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:40:37.270011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3037.36
 ---- batch: 020 ----
mean loss: 3059.08
 ---- batch: 030 ----
mean loss: 2982.73
 ---- batch: 040 ----
mean loss: 2856.38
train mean loss: 2978.36
epoch train time: 0:00:00.206031
elapsed time: 0:00:16.098541
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:40:37.476158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2861.96
 ---- batch: 020 ----
mean loss: 2750.75
 ---- batch: 030 ----
mean loss: 2780.92
 ---- batch: 040 ----
mean loss: 2697.96
train mean loss: 2768.68
epoch train time: 0:00:00.202095
elapsed time: 0:00:16.300749
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:40:37.678365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2707.58
 ---- batch: 020 ----
mean loss: 2582.22
 ---- batch: 030 ----
mean loss: 2548.32
 ---- batch: 040 ----
mean loss: 2498.35
train mean loss: 2578.41
epoch train time: 0:00:00.207122
elapsed time: 0:00:16.507995
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:40:37.885632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2461.28
 ---- batch: 020 ----
mean loss: 2415.74
 ---- batch: 030 ----
mean loss: 2391.59
 ---- batch: 040 ----
mean loss: 2385.35
train mean loss: 2405.93
epoch train time: 0:00:00.203827
elapsed time: 0:00:16.711959
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:40:38.089606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2290.27
 ---- batch: 020 ----
mean loss: 2265.84
 ---- batch: 030 ----
mean loss: 2235.19
 ---- batch: 040 ----
mean loss: 2144.06
train mean loss: 2234.47
epoch train time: 0:00:00.203142
elapsed time: 0:00:16.915245
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:40:38.292887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2165.26
 ---- batch: 020 ----
mean loss: 2067.87
 ---- batch: 030 ----
mean loss: 2035.09
 ---- batch: 040 ----
mean loss: 2018.18
train mean loss: 2067.95
epoch train time: 0:00:00.199783
elapsed time: 0:00:17.115168
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:40:38.492800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1959.66
 ---- batch: 020 ----
mean loss: 1939.59
 ---- batch: 030 ----
mean loss: 1886.69
 ---- batch: 040 ----
mean loss: 1883.37
train mean loss: 1912.56
epoch train time: 0:00:00.209064
elapsed time: 0:00:17.324362
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:40:38.702010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1818.62
 ---- batch: 020 ----
mean loss: 1789.47
 ---- batch: 030 ----
mean loss: 1750.13
 ---- batch: 040 ----
mean loss: 1734.74
train mean loss: 1771.15
epoch train time: 0:00:00.207171
elapsed time: 0:00:17.531705
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:40:38.909350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1696.86
 ---- batch: 020 ----
mean loss: 1648.92
 ---- batch: 030 ----
mean loss: 1615.50
 ---- batch: 040 ----
mean loss: 1610.54
train mean loss: 1636.60
epoch train time: 0:00:00.209560
elapsed time: 0:00:17.741407
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:40:39.119023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1541.45
 ---- batch: 020 ----
mean loss: 1547.99
 ---- batch: 030 ----
mean loss: 1514.41
 ---- batch: 040 ----
mean loss: 1475.46
train mean loss: 1515.59
epoch train time: 0:00:00.203406
elapsed time: 0:00:17.944928
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:40:39.322546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1429.35
 ---- batch: 020 ----
mean loss: 1417.54
 ---- batch: 030 ----
mean loss: 1404.33
 ---- batch: 040 ----
mean loss: 1387.62
train mean loss: 1409.14
epoch train time: 0:00:00.201802
elapsed time: 0:00:18.146896
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:40:39.524522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1338.99
 ---- batch: 020 ----
mean loss: 1325.70
 ---- batch: 030 ----
mean loss: 1304.50
 ---- batch: 040 ----
mean loss: 1287.09
train mean loss: 1312.03
epoch train time: 0:00:00.204049
elapsed time: 0:00:18.351071
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:40:39.728691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1270.56
 ---- batch: 020 ----
mean loss: 1213.18
 ---- batch: 030 ----
mean loss: 1232.97
 ---- batch: 040 ----
mean loss: 1198.84
train mean loss: 1225.21
epoch train time: 0:00:00.206413
elapsed time: 0:00:18.557619
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:40:39.935260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1163.89
 ---- batch: 020 ----
mean loss: 1164.98
 ---- batch: 030 ----
mean loss: 1158.14
 ---- batch: 040 ----
mean loss: 1111.64
train mean loss: 1149.25
epoch train time: 0:00:00.205716
elapsed time: 0:00:18.763479
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:40:40.141099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1101.07
 ---- batch: 020 ----
mean loss: 1092.57
 ---- batch: 030 ----
mean loss: 1073.20
 ---- batch: 040 ----
mean loss: 1058.53
train mean loss: 1079.40
epoch train time: 0:00:00.210794
elapsed time: 0:00:18.974396
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:40:40.352015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1043.30
 ---- batch: 020 ----
mean loss: 1015.47
 ---- batch: 030 ----
mean loss: 1012.21
 ---- batch: 040 ----
mean loss: 1005.37
train mean loss: 1016.11
epoch train time: 0:00:00.206383
elapsed time: 0:00:19.180900
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:40:40.558521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 936.08
 ---- batch: 020 ----
mean loss: 865.58
 ---- batch: 030 ----
mean loss: 837.96
 ---- batch: 040 ----
mean loss: 804.00
train mean loss: 857.03
epoch train time: 0:00:00.205182
elapsed time: 0:00:19.386203
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:40:40.763822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 788.75
 ---- batch: 020 ----
mean loss: 782.56
 ---- batch: 030 ----
mean loss: 752.17
 ---- batch: 040 ----
mean loss: 731.00
train mean loss: 759.59
epoch train time: 0:00:00.198269
elapsed time: 0:00:19.584593
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:40:40.962226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 727.62
 ---- batch: 020 ----
mean loss: 692.73
 ---- batch: 030 ----
mean loss: 683.21
 ---- batch: 040 ----
mean loss: 664.77
train mean loss: 688.69
epoch train time: 0:00:00.191817
elapsed time: 0:00:19.776548
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:40:41.154163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 651.83
 ---- batch: 020 ----
mean loss: 645.20
 ---- batch: 030 ----
mean loss: 618.56
 ---- batch: 040 ----
mean loss: 599.04
train mean loss: 624.78
epoch train time: 0:00:00.194250
elapsed time: 0:00:19.970945
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:40:41.348580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.42
 ---- batch: 020 ----
mean loss: 575.80
 ---- batch: 030 ----
mean loss: 552.82
 ---- batch: 040 ----
mean loss: 549.98
train mean loss: 566.88
epoch train time: 0:00:00.193426
elapsed time: 0:00:20.164505
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:40:41.542123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.10
 ---- batch: 020 ----
mean loss: 518.67
 ---- batch: 030 ----
mean loss: 516.05
 ---- batch: 040 ----
mean loss: 498.46
train mean loss: 515.01
epoch train time: 0:00:00.194663
elapsed time: 0:00:20.359286
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:40:41.736920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.22
 ---- batch: 020 ----
mean loss: 473.82
 ---- batch: 030 ----
mean loss: 463.24
 ---- batch: 040 ----
mean loss: 453.41
train mean loss: 468.59
epoch train time: 0:00:00.201105
elapsed time: 0:00:20.560523
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:40:41.938140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.44
 ---- batch: 020 ----
mean loss: 424.57
 ---- batch: 030 ----
mean loss: 428.35
 ---- batch: 040 ----
mean loss: 416.31
train mean loss: 426.08
epoch train time: 0:00:00.199340
elapsed time: 0:00:20.759977
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:40:42.137594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.44
 ---- batch: 020 ----
mean loss: 398.23
 ---- batch: 030 ----
mean loss: 389.12
 ---- batch: 040 ----
mean loss: 379.11
train mean loss: 387.82
epoch train time: 0:00:00.199805
elapsed time: 0:00:20.959910
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:40:42.337526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.92
 ---- batch: 020 ----
mean loss: 363.85
 ---- batch: 030 ----
mean loss: 349.95
 ---- batch: 040 ----
mean loss: 342.71
train mean loss: 354.28
epoch train time: 0:00:00.199207
elapsed time: 0:00:21.159268
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:40:42.536902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.89
 ---- batch: 020 ----
mean loss: 331.93
 ---- batch: 030 ----
mean loss: 315.70
 ---- batch: 040 ----
mean loss: 323.77
train mean loss: 323.81
epoch train time: 0:00:00.202046
elapsed time: 0:00:21.361450
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:40:42.739069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.07
 ---- batch: 020 ----
mean loss: 296.66
 ---- batch: 030 ----
mean loss: 291.89
 ---- batch: 040 ----
mean loss: 289.54
train mean loss: 295.76
epoch train time: 0:00:00.205569
elapsed time: 0:00:21.567140
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:40:42.944752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.12
 ---- batch: 020 ----
mean loss: 275.18
 ---- batch: 030 ----
mean loss: 268.88
 ---- batch: 040 ----
mean loss: 258.49
train mean loss: 271.11
epoch train time: 0:00:00.205708
elapsed time: 0:00:21.772958
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:40:43.150576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.32
 ---- batch: 020 ----
mean loss: 251.74
 ---- batch: 030 ----
mean loss: 247.95
 ---- batch: 040 ----
mean loss: 246.22
train mean loss: 248.73
epoch train time: 0:00:00.201909
elapsed time: 0:00:21.974988
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:40:43.352625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.05
 ---- batch: 020 ----
mean loss: 234.02
 ---- batch: 030 ----
mean loss: 227.62
 ---- batch: 040 ----
mean loss: 223.22
train mean loss: 229.19
epoch train time: 0:00:00.201958
elapsed time: 0:00:22.177098
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:40:43.554715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.32
 ---- batch: 020 ----
mean loss: 212.52
 ---- batch: 030 ----
mean loss: 209.15
 ---- batch: 040 ----
mean loss: 205.44
train mean loss: 210.71
epoch train time: 0:00:00.200791
elapsed time: 0:00:22.378007
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:40:43.755640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.23
 ---- batch: 020 ----
mean loss: 197.31
 ---- batch: 030 ----
mean loss: 192.13
 ---- batch: 040 ----
mean loss: 191.06
train mean loss: 194.95
epoch train time: 0:00:00.200672
elapsed time: 0:00:22.578846
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:40:43.956481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.54
 ---- batch: 020 ----
mean loss: 181.97
 ---- batch: 030 ----
mean loss: 179.29
 ---- batch: 040 ----
mean loss: 175.70
train mean loss: 180.45
epoch train time: 0:00:00.200753
elapsed time: 0:00:22.779732
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:40:44.157357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.62
 ---- batch: 020 ----
mean loss: 168.66
 ---- batch: 030 ----
mean loss: 168.44
 ---- batch: 040 ----
mean loss: 164.43
train mean loss: 168.04
epoch train time: 0:00:00.202352
elapsed time: 0:00:22.982204
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:40:44.359821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.01
 ---- batch: 020 ----
mean loss: 152.46
 ---- batch: 030 ----
mean loss: 158.76
 ---- batch: 040 ----
mean loss: 153.32
train mean loss: 156.23
epoch train time: 0:00:00.200681
elapsed time: 0:00:23.183000
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:40:44.560617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.21
 ---- batch: 020 ----
mean loss: 149.05
 ---- batch: 030 ----
mean loss: 146.41
 ---- batch: 040 ----
mean loss: 141.44
train mean loss: 146.33
epoch train time: 0:00:00.207656
elapsed time: 0:00:23.390790
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:40:44.768434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.87
 ---- batch: 020 ----
mean loss: 141.51
 ---- batch: 030 ----
mean loss: 133.81
 ---- batch: 040 ----
mean loss: 131.49
train mean loss: 137.11
epoch train time: 0:00:00.201995
elapsed time: 0:00:23.592945
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:40:44.970578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.70
 ---- batch: 020 ----
mean loss: 132.26
 ---- batch: 030 ----
mean loss: 129.65
 ---- batch: 040 ----
mean loss: 127.54
train mean loss: 130.25
epoch train time: 0:00:00.195408
elapsed time: 0:00:23.788482
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:40:45.166099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.63
 ---- batch: 020 ----
mean loss: 123.55
 ---- batch: 030 ----
mean loss: 121.31
 ---- batch: 040 ----
mean loss: 120.12
train mean loss: 122.18
epoch train time: 0:00:00.196968
elapsed time: 0:00:23.985626
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:40:45.363300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.76
 ---- batch: 020 ----
mean loss: 114.58
 ---- batch: 030 ----
mean loss: 117.01
 ---- batch: 040 ----
mean loss: 113.93
train mean loss: 115.25
epoch train time: 0:00:00.194715
elapsed time: 0:00:24.180512
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:40:45.558127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.52
 ---- batch: 020 ----
mean loss: 109.08
 ---- batch: 030 ----
mean loss: 108.11
 ---- batch: 040 ----
mean loss: 109.44
train mean loss: 109.39
epoch train time: 0:00:00.193787
elapsed time: 0:00:24.374407
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:40:45.752037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.19
 ---- batch: 020 ----
mean loss: 103.27
 ---- batch: 030 ----
mean loss: 102.51
 ---- batch: 040 ----
mean loss: 106.29
train mean loss: 103.74
epoch train time: 0:00:00.208493
elapsed time: 0:00:24.583029
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:40:45.960647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.20
 ---- batch: 020 ----
mean loss: 100.07
 ---- batch: 030 ----
mean loss: 95.05
 ---- batch: 040 ----
mean loss: 98.46
train mean loss: 99.40
epoch train time: 0:00:00.198379
elapsed time: 0:00:24.781517
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:40:46.159132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.14
 ---- batch: 020 ----
mean loss: 97.34
 ---- batch: 030 ----
mean loss: 94.46
 ---- batch: 040 ----
mean loss: 94.97
train mean loss: 95.52
epoch train time: 0:00:00.199831
elapsed time: 0:00:24.981459
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:40:46.359075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.29
 ---- batch: 020 ----
mean loss: 90.54
 ---- batch: 030 ----
mean loss: 91.12
 ---- batch: 040 ----
mean loss: 89.36
train mean loss: 91.37
epoch train time: 0:00:00.199530
elapsed time: 0:00:25.181102
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:40:46.558731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.48
 ---- batch: 020 ----
mean loss: 87.44
 ---- batch: 030 ----
mean loss: 89.83
 ---- batch: 040 ----
mean loss: 87.40
train mean loss: 88.49
epoch train time: 0:00:00.198884
elapsed time: 0:00:25.380112
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:40:46.757730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.03
 ---- batch: 020 ----
mean loss: 83.97
 ---- batch: 030 ----
mean loss: 84.22
 ---- batch: 040 ----
mean loss: 85.70
train mean loss: 85.09
epoch train time: 0:00:00.203541
elapsed time: 0:00:25.583771
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:40:46.961401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.10
 ---- batch: 020 ----
mean loss: 80.05
 ---- batch: 030 ----
mean loss: 84.73
 ---- batch: 040 ----
mean loss: 81.35
train mean loss: 82.69
epoch train time: 0:00:00.204950
elapsed time: 0:00:25.788866
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:40:47.166485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.21
 ---- batch: 020 ----
mean loss: 81.40
 ---- batch: 030 ----
mean loss: 82.54
 ---- batch: 040 ----
mean loss: 77.86
train mean loss: 80.77
epoch train time: 0:00:00.202579
elapsed time: 0:00:25.991562
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:40:47.369180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.56
 ---- batch: 020 ----
mean loss: 77.79
 ---- batch: 030 ----
mean loss: 76.41
 ---- batch: 040 ----
mean loss: 78.33
train mean loss: 78.23
epoch train time: 0:00:00.203277
elapsed time: 0:00:26.194986
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:40:47.572620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.47
 ---- batch: 020 ----
mean loss: 75.15
 ---- batch: 030 ----
mean loss: 77.41
 ---- batch: 040 ----
mean loss: 75.36
train mean loss: 76.17
epoch train time: 0:00:00.205919
elapsed time: 0:00:26.401043
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:40:47.778665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.71
 ---- batch: 020 ----
mean loss: 72.76
 ---- batch: 030 ----
mean loss: 72.03
 ---- batch: 040 ----
mean loss: 75.23
train mean loss: 73.86
epoch train time: 0:00:00.203349
elapsed time: 0:00:26.604517
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:40:47.982137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.98
 ---- batch: 020 ----
mean loss: 74.16
 ---- batch: 030 ----
mean loss: 71.47
 ---- batch: 040 ----
mean loss: 71.80
train mean loss: 72.90
epoch train time: 0:00:00.201669
elapsed time: 0:00:26.806309
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:40:48.183941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.15
 ---- batch: 020 ----
mean loss: 70.84
 ---- batch: 030 ----
mean loss: 72.70
 ---- batch: 040 ----
mean loss: 70.14
train mean loss: 71.02
epoch train time: 0:00:00.203384
elapsed time: 0:00:27.009822
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:40:48.387440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.99
 ---- batch: 020 ----
mean loss: 70.10
 ---- batch: 030 ----
mean loss: 67.88
 ---- batch: 040 ----
mean loss: 69.83
train mean loss: 69.83
epoch train time: 0:00:00.203264
elapsed time: 0:00:27.213225
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:40:48.590842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.63
 ---- batch: 020 ----
mean loss: 70.29
 ---- batch: 030 ----
mean loss: 70.39
 ---- batch: 040 ----
mean loss: 71.01
train mean loss: 69.26
epoch train time: 0:00:00.201781
elapsed time: 0:00:27.415141
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:40:48.792775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.33
 ---- batch: 020 ----
mean loss: 66.99
 ---- batch: 030 ----
mean loss: 70.41
 ---- batch: 040 ----
mean loss: 67.46
train mean loss: 67.93
epoch train time: 0:00:00.202069
elapsed time: 0:00:27.617350
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:40:48.994966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.24
 ---- batch: 020 ----
mean loss: 64.17
 ---- batch: 030 ----
mean loss: 65.67
 ---- batch: 040 ----
mean loss: 68.37
train mean loss: 65.92
epoch train time: 0:00:00.197835
elapsed time: 0:00:27.815314
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:40:49.192929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.59
 ---- batch: 020 ----
mean loss: 65.89
 ---- batch: 030 ----
mean loss: 63.54
 ---- batch: 040 ----
mean loss: 65.02
train mean loss: 65.23
epoch train time: 0:00:00.194556
elapsed time: 0:00:28.009979
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:40:49.387593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.35
 ---- batch: 020 ----
mean loss: 63.78
 ---- batch: 030 ----
mean loss: 65.03
 ---- batch: 040 ----
mean loss: 62.98
train mean loss: 64.12
epoch train time: 0:00:00.193850
elapsed time: 0:00:28.203934
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:40:49.581548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.55
 ---- batch: 020 ----
mean loss: 65.31
 ---- batch: 030 ----
mean loss: 65.58
 ---- batch: 040 ----
mean loss: 61.34
train mean loss: 63.86
epoch train time: 0:00:00.194590
elapsed time: 0:00:28.398650
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:40:49.776266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.46
 ---- batch: 020 ----
mean loss: 61.95
 ---- batch: 030 ----
mean loss: 63.69
 ---- batch: 040 ----
mean loss: 65.14
train mean loss: 64.05
epoch train time: 0:00:00.196450
elapsed time: 0:00:28.595215
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:40:49.972829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.60
 ---- batch: 020 ----
mean loss: 60.07
 ---- batch: 030 ----
mean loss: 59.51
 ---- batch: 040 ----
mean loss: 63.91
train mean loss: 61.92
epoch train time: 0:00:00.194705
elapsed time: 0:00:28.790045
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:40:50.167661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.31
 ---- batch: 020 ----
mean loss: 58.63
 ---- batch: 030 ----
mean loss: 61.38
 ---- batch: 040 ----
mean loss: 62.16
train mean loss: 61.35
epoch train time: 0:00:00.201245
elapsed time: 0:00:28.991412
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:40:50.369027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.08
 ---- batch: 020 ----
mean loss: 62.44
 ---- batch: 030 ----
mean loss: 61.91
 ---- batch: 040 ----
mean loss: 59.07
train mean loss: 60.67
epoch train time: 0:00:00.202593
elapsed time: 0:00:29.194179
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:40:50.571864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.27
 ---- batch: 020 ----
mean loss: 63.11
 ---- batch: 030 ----
mean loss: 59.96
 ---- batch: 040 ----
mean loss: 59.83
train mean loss: 59.82
epoch train time: 0:00:00.195070
elapsed time: 0:00:29.389449
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:40:50.767066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.95
 ---- batch: 020 ----
mean loss: 57.08
 ---- batch: 030 ----
mean loss: 60.22
 ---- batch: 040 ----
mean loss: 61.17
train mean loss: 59.48
epoch train time: 0:00:00.198148
elapsed time: 0:00:29.587741
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:40:50.965362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.66
 ---- batch: 020 ----
mean loss: 59.58
 ---- batch: 030 ----
mean loss: 59.34
 ---- batch: 040 ----
mean loss: 58.24
train mean loss: 59.12
epoch train time: 0:00:00.202037
elapsed time: 0:00:29.789899
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:40:51.167518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.02
 ---- batch: 020 ----
mean loss: 60.68
 ---- batch: 030 ----
mean loss: 58.87
 ---- batch: 040 ----
mean loss: 55.58
train mean loss: 58.93
epoch train time: 0:00:00.207355
elapsed time: 0:00:29.997384
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:40:51.375012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.65
 ---- batch: 020 ----
mean loss: 54.09
 ---- batch: 030 ----
mean loss: 58.79
 ---- batch: 040 ----
mean loss: 59.88
train mean loss: 57.76
epoch train time: 0:00:00.214885
elapsed time: 0:00:30.212396
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:40:51.590015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.77
 ---- batch: 020 ----
mean loss: 59.71
 ---- batch: 030 ----
mean loss: 58.01
 ---- batch: 040 ----
mean loss: 57.01
train mean loss: 57.83
epoch train time: 0:00:00.205862
elapsed time: 0:00:30.418380
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:40:51.795999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.79
 ---- batch: 020 ----
mean loss: 53.52
 ---- batch: 030 ----
mean loss: 59.12
 ---- batch: 040 ----
mean loss: 57.42
train mean loss: 56.91
epoch train time: 0:00:00.207556
elapsed time: 0:00:30.626054
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:40:52.003674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.97
 ---- batch: 020 ----
mean loss: 55.42
 ---- batch: 030 ----
mean loss: 57.60
 ---- batch: 040 ----
mean loss: 57.26
train mean loss: 56.92
epoch train time: 0:00:00.207504
elapsed time: 0:00:30.833685
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:40:52.211319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.04
 ---- batch: 020 ----
mean loss: 54.08
 ---- batch: 030 ----
mean loss: 56.45
 ---- batch: 040 ----
mean loss: 56.23
train mean loss: 56.25
epoch train time: 0:00:00.208362
elapsed time: 0:00:31.042192
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:40:52.419812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.57
 ---- batch: 020 ----
mean loss: 55.45
 ---- batch: 030 ----
mean loss: 56.66
 ---- batch: 040 ----
mean loss: 58.97
train mean loss: 56.08
epoch train time: 0:00:00.207167
elapsed time: 0:00:31.249477
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:40:52.627097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.18
 ---- batch: 020 ----
mean loss: 54.87
 ---- batch: 030 ----
mean loss: 54.36
 ---- batch: 040 ----
mean loss: 56.76
train mean loss: 56.08
epoch train time: 0:00:00.209987
elapsed time: 0:00:31.459596
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:40:52.837257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.31
 ---- batch: 020 ----
mean loss: 57.24
 ---- batch: 030 ----
mean loss: 56.53
 ---- batch: 040 ----
mean loss: 57.47
train mean loss: 56.33
epoch train time: 0:00:00.213320
elapsed time: 0:00:31.673077
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:40:53.050709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.60
 ---- batch: 020 ----
mean loss: 52.58
 ---- batch: 030 ----
mean loss: 54.60
 ---- batch: 040 ----
mean loss: 54.09
train mean loss: 54.59
epoch train time: 0:00:00.206068
elapsed time: 0:00:31.879294
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:40:53.256913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.63
 ---- batch: 020 ----
mean loss: 54.56
 ---- batch: 030 ----
mean loss: 52.69
 ---- batch: 040 ----
mean loss: 55.79
train mean loss: 54.21
epoch train time: 0:00:00.198042
elapsed time: 0:00:32.077450
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:40:53.455066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.74
 ---- batch: 020 ----
mean loss: 53.60
 ---- batch: 030 ----
mean loss: 52.83
 ---- batch: 040 ----
mean loss: 57.09
train mean loss: 54.16
epoch train time: 0:00:00.200752
elapsed time: 0:00:32.278316
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:40:53.655932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.29
 ---- batch: 020 ----
mean loss: 53.44
 ---- batch: 030 ----
mean loss: 54.43
 ---- batch: 040 ----
mean loss: 52.26
train mean loss: 53.87
epoch train time: 0:00:00.201455
elapsed time: 0:00:32.479896
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:40:53.857514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.29
 ---- batch: 020 ----
mean loss: 52.21
 ---- batch: 030 ----
mean loss: 51.35
 ---- batch: 040 ----
mean loss: 52.23
train mean loss: 52.92
epoch train time: 0:00:00.198525
elapsed time: 0:00:32.678554
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:40:54.056175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.36
 ---- batch: 020 ----
mean loss: 51.07
 ---- batch: 030 ----
mean loss: 53.03
 ---- batch: 040 ----
mean loss: 55.95
train mean loss: 53.04
epoch train time: 0:00:00.198920
elapsed time: 0:00:32.877590
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:40:54.255206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.92
 ---- batch: 020 ----
mean loss: 55.81
 ---- batch: 030 ----
mean loss: 51.36
 ---- batch: 040 ----
mean loss: 53.61
train mean loss: 53.42
epoch train time: 0:00:00.195108
elapsed time: 0:00:33.072811
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:40:54.450429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.93
 ---- batch: 020 ----
mean loss: 53.63
 ---- batch: 030 ----
mean loss: 51.99
 ---- batch: 040 ----
mean loss: 50.70
train mean loss: 52.12
epoch train time: 0:00:00.196169
elapsed time: 0:00:33.269093
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:40:54.646708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.42
 ---- batch: 020 ----
mean loss: 51.60
 ---- batch: 030 ----
mean loss: 53.41
 ---- batch: 040 ----
mean loss: 53.52
train mean loss: 52.16
epoch train time: 0:00:00.201810
elapsed time: 0:00:33.471022
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:40:54.848642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.40
 ---- batch: 020 ----
mean loss: 53.44
 ---- batch: 030 ----
mean loss: 50.43
 ---- batch: 040 ----
mean loss: 51.48
train mean loss: 51.85
epoch train time: 0:00:00.201568
elapsed time: 0:00:33.672708
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:40:55.050326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.22
 ---- batch: 020 ----
mean loss: 53.74
 ---- batch: 030 ----
mean loss: 50.79
 ---- batch: 040 ----
mean loss: 53.46
train mean loss: 51.88
epoch train time: 0:00:00.200987
elapsed time: 0:00:33.873808
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:40:55.251456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.72
 ---- batch: 020 ----
mean loss: 52.85
 ---- batch: 030 ----
mean loss: 52.45
 ---- batch: 040 ----
mean loss: 50.58
train mean loss: 52.14
epoch train time: 0:00:00.204057
elapsed time: 0:00:34.078027
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:40:55.455659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.73
 ---- batch: 020 ----
mean loss: 52.25
 ---- batch: 030 ----
mean loss: 47.67
 ---- batch: 040 ----
mean loss: 50.40
train mean loss: 50.72
epoch train time: 0:00:00.212065
elapsed time: 0:00:34.290226
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:40:55.667845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.62
 ---- batch: 020 ----
mean loss: 50.24
 ---- batch: 030 ----
mean loss: 46.83
 ---- batch: 040 ----
mean loss: 51.26
train mean loss: 50.35
epoch train time: 0:00:00.205911
elapsed time: 0:00:34.496252
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:40:55.873869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.69
 ---- batch: 020 ----
mean loss: 50.47
 ---- batch: 030 ----
mean loss: 50.32
 ---- batch: 040 ----
mean loss: 51.97
train mean loss: 50.84
epoch train time: 0:00:00.202755
elapsed time: 0:00:34.699184
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:40:56.076816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.07
 ---- batch: 020 ----
mean loss: 52.95
 ---- batch: 030 ----
mean loss: 53.44
 ---- batch: 040 ----
mean loss: 52.63
train mean loss: 52.68
epoch train time: 0:00:00.203413
elapsed time: 0:00:34.902744
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:40:56.280364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.64
 ---- batch: 020 ----
mean loss: 49.38
 ---- batch: 030 ----
mean loss: 51.02
 ---- batch: 040 ----
mean loss: 50.40
train mean loss: 50.06
epoch train time: 0:00:00.202357
elapsed time: 0:00:35.105228
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:40:56.482857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.56
 ---- batch: 020 ----
mean loss: 48.05
 ---- batch: 030 ----
mean loss: 49.53
 ---- batch: 040 ----
mean loss: 50.89
train mean loss: 49.75
epoch train time: 0:00:00.203537
elapsed time: 0:00:35.308896
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:40:56.686518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.08
 ---- batch: 020 ----
mean loss: 47.32
 ---- batch: 030 ----
mean loss: 50.58
 ---- batch: 040 ----
mean loss: 51.20
train mean loss: 49.39
epoch train time: 0:00:00.204152
elapsed time: 0:00:35.513181
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:40:56.890799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.91
 ---- batch: 020 ----
mean loss: 49.44
 ---- batch: 030 ----
mean loss: 49.12
 ---- batch: 040 ----
mean loss: 47.05
train mean loss: 48.74
epoch train time: 0:00:00.202115
elapsed time: 0:00:35.715415
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:40:57.093032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.02
 ---- batch: 020 ----
mean loss: 49.83
 ---- batch: 030 ----
mean loss: 46.06
 ---- batch: 040 ----
mean loss: 49.63
train mean loss: 48.67
epoch train time: 0:00:00.201500
elapsed time: 0:00:35.917027
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:40:57.294646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.54
 ---- batch: 020 ----
mean loss: 49.77
 ---- batch: 030 ----
mean loss: 49.04
 ---- batch: 040 ----
mean loss: 53.49
train mean loss: 49.79
epoch train time: 0:00:00.201263
elapsed time: 0:00:36.118435
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:40:57.496059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.83
 ---- batch: 020 ----
mean loss: 48.05
 ---- batch: 030 ----
mean loss: 48.85
 ---- batch: 040 ----
mean loss: 48.73
train mean loss: 48.45
epoch train time: 0:00:00.195465
elapsed time: 0:00:36.314042
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:40:57.691650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.57
 ---- batch: 020 ----
mean loss: 48.35
 ---- batch: 030 ----
mean loss: 47.22
 ---- batch: 040 ----
mean loss: 48.53
train mean loss: 48.50
epoch train time: 0:00:00.200552
elapsed time: 0:00:36.514737
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:40:57.892364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.64
 ---- batch: 020 ----
mean loss: 47.02
 ---- batch: 030 ----
mean loss: 46.92
 ---- batch: 040 ----
mean loss: 50.61
train mean loss: 48.23
epoch train time: 0:00:00.200087
elapsed time: 0:00:36.714946
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:40:58.092562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.10
 ---- batch: 020 ----
mean loss: 46.29
 ---- batch: 030 ----
mean loss: 48.93
 ---- batch: 040 ----
mean loss: 44.89
train mean loss: 47.61
epoch train time: 0:00:00.197940
elapsed time: 0:00:36.912997
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:40:58.290629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.12
 ---- batch: 020 ----
mean loss: 46.55
 ---- batch: 030 ----
mean loss: 48.82
 ---- batch: 040 ----
mean loss: 47.65
train mean loss: 47.38
epoch train time: 0:00:00.196249
elapsed time: 0:00:37.109377
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:40:58.486994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.88
 ---- batch: 020 ----
mean loss: 48.92
 ---- batch: 030 ----
mean loss: 46.52
 ---- batch: 040 ----
mean loss: 47.71
train mean loss: 48.10
epoch train time: 0:00:00.200076
elapsed time: 0:00:37.309600
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:40:58.687223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.19
 ---- batch: 020 ----
mean loss: 48.98
 ---- batch: 030 ----
mean loss: 46.97
 ---- batch: 040 ----
mean loss: 46.05
train mean loss: 47.80
epoch train time: 0:00:00.204754
elapsed time: 0:00:37.514474
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:40:58.892107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.31
 ---- batch: 020 ----
mean loss: 48.29
 ---- batch: 030 ----
mean loss: 46.70
 ---- batch: 040 ----
mean loss: 44.53
train mean loss: 46.74
epoch train time: 0:00:00.198891
elapsed time: 0:00:37.713501
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:40:59.091119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.19
 ---- batch: 020 ----
mean loss: 45.15
 ---- batch: 030 ----
mean loss: 48.37
 ---- batch: 040 ----
mean loss: 47.19
train mean loss: 46.78
epoch train time: 0:00:00.196922
elapsed time: 0:00:37.910539
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:40:59.288156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.03
 ---- batch: 020 ----
mean loss: 50.05
 ---- batch: 030 ----
mean loss: 48.53
 ---- batch: 040 ----
mean loss: 48.65
train mean loss: 48.67
epoch train time: 0:00:00.203539
elapsed time: 0:00:38.114204
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:40:59.491824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.04
 ---- batch: 020 ----
mean loss: 46.62
 ---- batch: 030 ----
mean loss: 45.99
 ---- batch: 040 ----
mean loss: 51.58
train mean loss: 47.48
epoch train time: 0:00:00.208955
elapsed time: 0:00:38.323301
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:40:59.700932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.23
 ---- batch: 020 ----
mean loss: 50.66
 ---- batch: 030 ----
mean loss: 46.26
 ---- batch: 040 ----
mean loss: 45.16
train mean loss: 48.49
epoch train time: 0:00:00.208092
elapsed time: 0:00:38.531524
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:40:59.909142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.69
 ---- batch: 020 ----
mean loss: 45.36
 ---- batch: 030 ----
mean loss: 46.32
 ---- batch: 040 ----
mean loss: 47.95
train mean loss: 46.86
epoch train time: 0:00:00.202072
elapsed time: 0:00:38.733714
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:41:00.111332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.00
 ---- batch: 020 ----
mean loss: 46.48
 ---- batch: 030 ----
mean loss: 48.96
 ---- batch: 040 ----
mean loss: 45.26
train mean loss: 46.96
epoch train time: 0:00:00.204060
elapsed time: 0:00:38.937894
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:41:00.315529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.95
 ---- batch: 020 ----
mean loss: 47.00
 ---- batch: 030 ----
mean loss: 46.99
 ---- batch: 040 ----
mean loss: 46.34
train mean loss: 46.20
epoch train time: 0:00:00.211632
elapsed time: 0:00:39.149723
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:41:00.527353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.45
 ---- batch: 020 ----
mean loss: 47.39
 ---- batch: 030 ----
mean loss: 42.89
 ---- batch: 040 ----
mean loss: 49.15
train mean loss: 45.81
epoch train time: 0:00:00.211251
elapsed time: 0:00:39.361108
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:41:00.738752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.15
 ---- batch: 020 ----
mean loss: 45.72
 ---- batch: 030 ----
mean loss: 45.22
 ---- batch: 040 ----
mean loss: 45.69
train mean loss: 45.43
epoch train time: 0:00:00.210298
elapsed time: 0:00:39.571550
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:41:00.949170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.85
 ---- batch: 020 ----
mean loss: 45.17
 ---- batch: 030 ----
mean loss: 46.27
 ---- batch: 040 ----
mean loss: 43.53
train mean loss: 45.13
epoch train time: 0:00:00.206432
elapsed time: 0:00:39.778119
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:41:01.155737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.73
 ---- batch: 020 ----
mean loss: 43.33
 ---- batch: 030 ----
mean loss: 43.91
 ---- batch: 040 ----
mean loss: 44.49
train mean loss: 44.80
epoch train time: 0:00:00.204331
elapsed time: 0:00:39.982561
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:41:01.360185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.63
 ---- batch: 020 ----
mean loss: 44.49
 ---- batch: 030 ----
mean loss: 47.81
 ---- batch: 040 ----
mean loss: 44.01
train mean loss: 44.87
epoch train time: 0:00:00.203526
elapsed time: 0:00:40.186211
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:41:01.563830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.52
 ---- batch: 020 ----
mean loss: 46.09
 ---- batch: 030 ----
mean loss: 45.18
 ---- batch: 040 ----
mean loss: 44.12
train mean loss: 45.72
epoch train time: 0:00:00.203328
elapsed time: 0:00:40.389668
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:41:01.767281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.93
 ---- batch: 020 ----
mean loss: 43.36
 ---- batch: 030 ----
mean loss: 43.90
 ---- batch: 040 ----
mean loss: 45.02
train mean loss: 44.70
epoch train time: 0:00:00.199908
elapsed time: 0:00:40.589705
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:41:01.967323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.16
 ---- batch: 020 ----
mean loss: 44.74
 ---- batch: 030 ----
mean loss: 43.89
 ---- batch: 040 ----
mean loss: 45.91
train mean loss: 44.51
epoch train time: 0:00:00.194221
elapsed time: 0:00:40.784035
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:41:02.161663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.23
 ---- batch: 020 ----
mean loss: 44.73
 ---- batch: 030 ----
mean loss: 46.02
 ---- batch: 040 ----
mean loss: 43.19
train mean loss: 45.03
epoch train time: 0:00:00.193326
elapsed time: 0:00:40.977484
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:41:02.355099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.45
 ---- batch: 020 ----
mean loss: 44.57
 ---- batch: 030 ----
mean loss: 42.18
 ---- batch: 040 ----
mean loss: 45.28
train mean loss: 44.18
epoch train time: 0:00:00.191550
elapsed time: 0:00:41.169142
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:41:02.546774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.27
 ---- batch: 020 ----
mean loss: 45.84
 ---- batch: 030 ----
mean loss: 43.28
 ---- batch: 040 ----
mean loss: 43.57
train mean loss: 45.13
epoch train time: 0:00:00.198046
elapsed time: 0:00:41.367320
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:41:02.744937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.09
 ---- batch: 020 ----
mean loss: 44.49
 ---- batch: 030 ----
mean loss: 43.40
 ---- batch: 040 ----
mean loss: 41.85
train mean loss: 43.95
epoch train time: 0:00:00.201523
elapsed time: 0:00:41.568970
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:41:02.946617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.24
 ---- batch: 020 ----
mean loss: 42.91
 ---- batch: 030 ----
mean loss: 44.11
 ---- batch: 040 ----
mean loss: 44.27
train mean loss: 43.29
epoch train time: 0:00:00.200693
elapsed time: 0:00:41.769808
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:41:03.147456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.68
 ---- batch: 020 ----
mean loss: 45.19
 ---- batch: 030 ----
mean loss: 44.20
 ---- batch: 040 ----
mean loss: 43.27
train mean loss: 44.09
epoch train time: 0:00:00.197334
elapsed time: 0:00:41.967284
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:41:03.344899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.48
 ---- batch: 020 ----
mean loss: 43.22
 ---- batch: 030 ----
mean loss: 43.38
 ---- batch: 040 ----
mean loss: 44.29
train mean loss: 43.34
epoch train time: 0:00:00.193672
elapsed time: 0:00:42.161076
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:41:03.538691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.04
 ---- batch: 020 ----
mean loss: 44.19
 ---- batch: 030 ----
mean loss: 43.29
 ---- batch: 040 ----
mean loss: 42.34
train mean loss: 43.17
epoch train time: 0:00:00.203794
elapsed time: 0:00:42.365038
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:41:03.742669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.00
 ---- batch: 020 ----
mean loss: 43.43
 ---- batch: 030 ----
mean loss: 44.60
 ---- batch: 040 ----
mean loss: 43.68
train mean loss: 43.42
epoch train time: 0:00:00.212933
elapsed time: 0:00:42.578107
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:41:03.955730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.02
 ---- batch: 020 ----
mean loss: 47.37
 ---- batch: 030 ----
mean loss: 44.44
 ---- batch: 040 ----
mean loss: 42.94
train mean loss: 44.32
epoch train time: 0:00:00.207081
elapsed time: 0:00:42.785331
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:41:04.162951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.55
 ---- batch: 020 ----
mean loss: 42.06
 ---- batch: 030 ----
mean loss: 42.11
 ---- batch: 040 ----
mean loss: 42.07
train mean loss: 42.67
epoch train time: 0:00:00.209733
elapsed time: 0:00:42.995181
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:41:04.372821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.57
 ---- batch: 020 ----
mean loss: 39.83
 ---- batch: 030 ----
mean loss: 42.19
 ---- batch: 040 ----
mean loss: 43.28
train mean loss: 42.19
epoch train time: 0:00:00.207492
elapsed time: 0:00:43.202886
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:41:04.580510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.95
 ---- batch: 020 ----
mean loss: 43.41
 ---- batch: 030 ----
mean loss: 45.33
 ---- batch: 040 ----
mean loss: 42.45
train mean loss: 42.67
epoch train time: 0:00:00.207149
elapsed time: 0:00:43.410170
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:41:04.787791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.39
 ---- batch: 020 ----
mean loss: 44.07
 ---- batch: 030 ----
mean loss: 43.59
 ---- batch: 040 ----
mean loss: 41.69
train mean loss: 42.34
epoch train time: 0:00:00.204339
elapsed time: 0:00:43.614627
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:41:04.992243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.39
 ---- batch: 020 ----
mean loss: 43.77
 ---- batch: 030 ----
mean loss: 42.59
 ---- batch: 040 ----
mean loss: 42.81
train mean loss: 42.52
epoch train time: 0:00:00.202907
elapsed time: 0:00:43.817645
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:41:05.195298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.10
 ---- batch: 020 ----
mean loss: 43.51
 ---- batch: 030 ----
mean loss: 42.68
 ---- batch: 040 ----
mean loss: 41.03
train mean loss: 41.91
epoch train time: 0:00:00.208534
elapsed time: 0:00:44.026343
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:41:05.403976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.81
 ---- batch: 020 ----
mean loss: 39.01
 ---- batch: 030 ----
mean loss: 42.20
 ---- batch: 040 ----
mean loss: 42.36
train mean loss: 41.45
epoch train time: 0:00:00.206906
elapsed time: 0:00:44.233383
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:41:05.611019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.29
 ---- batch: 020 ----
mean loss: 43.36
 ---- batch: 030 ----
mean loss: 41.30
 ---- batch: 040 ----
mean loss: 42.58
train mean loss: 42.16
epoch train time: 0:00:00.214095
elapsed time: 0:00:44.447614
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:41:05.825260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.57
 ---- batch: 020 ----
mean loss: 40.95
 ---- batch: 030 ----
mean loss: 42.48
 ---- batch: 040 ----
mean loss: 41.00
train mean loss: 41.89
epoch train time: 0:00:00.201295
elapsed time: 0:00:44.649070
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:41:06.026705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.01
 ---- batch: 020 ----
mean loss: 44.12
 ---- batch: 030 ----
mean loss: 39.99
 ---- batch: 040 ----
mean loss: 39.30
train mean loss: 40.92
epoch train time: 0:00:00.199140
elapsed time: 0:00:44.848349
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:41:06.225958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.79
 ---- batch: 020 ----
mean loss: 40.90
 ---- batch: 030 ----
mean loss: 42.04
 ---- batch: 040 ----
mean loss: 41.78
train mean loss: 41.46
epoch train time: 0:00:00.197403
elapsed time: 0:00:45.045855
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:41:06.423470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.96
 ---- batch: 020 ----
mean loss: 41.99
 ---- batch: 030 ----
mean loss: 42.24
 ---- batch: 040 ----
mean loss: 43.49
train mean loss: 41.97
epoch train time: 0:00:00.196191
elapsed time: 0:00:45.242159
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:41:06.619776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.32
 ---- batch: 020 ----
mean loss: 41.19
 ---- batch: 030 ----
mean loss: 40.47
 ---- batch: 040 ----
mean loss: 40.43
train mean loss: 40.53
epoch train time: 0:00:00.206012
elapsed time: 0:00:45.448285
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:41:06.825901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.24
 ---- batch: 020 ----
mean loss: 39.87
 ---- batch: 030 ----
mean loss: 41.57
 ---- batch: 040 ----
mean loss: 41.63
train mean loss: 40.77
epoch train time: 0:00:00.212543
elapsed time: 0:00:45.660951
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:41:07.038569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.72
 ---- batch: 020 ----
mean loss: 40.96
 ---- batch: 030 ----
mean loss: 37.66
 ---- batch: 040 ----
mean loss: 39.98
train mean loss: 40.36
epoch train time: 0:00:00.198838
elapsed time: 0:00:45.859902
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:41:07.237519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.22
 ---- batch: 020 ----
mean loss: 39.93
 ---- batch: 030 ----
mean loss: 39.92
 ---- batch: 040 ----
mean loss: 41.35
train mean loss: 40.50
epoch train time: 0:00:00.196731
elapsed time: 0:00:46.056780
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:41:07.434396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.36
 ---- batch: 020 ----
mean loss: 39.91
 ---- batch: 030 ----
mean loss: 41.11
 ---- batch: 040 ----
mean loss: 39.23
train mean loss: 39.94
epoch train time: 0:00:00.195115
elapsed time: 0:00:46.252006
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:41:07.629622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.99
 ---- batch: 020 ----
mean loss: 41.01
 ---- batch: 030 ----
mean loss: 41.32
 ---- batch: 040 ----
mean loss: 40.14
train mean loss: 40.22
epoch train time: 0:00:00.209727
elapsed time: 0:00:46.461878
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:41:07.839540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.80
 ---- batch: 020 ----
mean loss: 40.63
 ---- batch: 030 ----
mean loss: 39.73
 ---- batch: 040 ----
mean loss: 40.87
train mean loss: 39.78
epoch train time: 0:00:00.207047
elapsed time: 0:00:46.669143
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:41:08.046816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.28
 ---- batch: 020 ----
mean loss: 39.36
 ---- batch: 030 ----
mean loss: 39.82
 ---- batch: 040 ----
mean loss: 39.06
train mean loss: 39.62
epoch train time: 0:00:00.213397
elapsed time: 0:00:46.882734
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:41:08.260370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.18
 ---- batch: 020 ----
mean loss: 38.42
 ---- batch: 030 ----
mean loss: 40.15
 ---- batch: 040 ----
mean loss: 42.66
train mean loss: 39.88
epoch train time: 0:00:00.204554
elapsed time: 0:00:47.087420
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:41:08.465036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.58
 ---- batch: 020 ----
mean loss: 38.31
 ---- batch: 030 ----
mean loss: 39.18
 ---- batch: 040 ----
mean loss: 40.56
train mean loss: 39.30
epoch train time: 0:00:00.206949
elapsed time: 0:00:47.294481
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:41:08.672097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.87
 ---- batch: 020 ----
mean loss: 37.71
 ---- batch: 030 ----
mean loss: 39.15
 ---- batch: 040 ----
mean loss: 40.10
train mean loss: 39.21
epoch train time: 0:00:00.208544
elapsed time: 0:00:47.503141
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:41:08.880758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.18
 ---- batch: 020 ----
mean loss: 39.36
 ---- batch: 030 ----
mean loss: 37.95
 ---- batch: 040 ----
mean loss: 40.65
train mean loss: 38.80
epoch train time: 0:00:00.202847
elapsed time: 0:00:47.706122
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:41:09.083742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.78
 ---- batch: 020 ----
mean loss: 37.67
 ---- batch: 030 ----
mean loss: 38.42
 ---- batch: 040 ----
mean loss: 40.04
train mean loss: 38.48
epoch train time: 0:00:00.201724
elapsed time: 0:00:47.907969
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:41:09.285589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.14
 ---- batch: 020 ----
mean loss: 39.22
 ---- batch: 030 ----
mean loss: 39.24
 ---- batch: 040 ----
mean loss: 38.29
train mean loss: 38.72
epoch train time: 0:00:00.202363
elapsed time: 0:00:48.110450
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:41:09.488088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.89
 ---- batch: 020 ----
mean loss: 38.85
 ---- batch: 030 ----
mean loss: 38.21
 ---- batch: 040 ----
mean loss: 39.91
train mean loss: 38.88
epoch train time: 0:00:00.205470
elapsed time: 0:00:48.316067
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:41:09.693695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.38
 ---- batch: 020 ----
mean loss: 38.73
 ---- batch: 030 ----
mean loss: 39.15
 ---- batch: 040 ----
mean loss: 36.15
train mean loss: 38.26
epoch train time: 0:00:00.213025
elapsed time: 0:00:48.529223
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:41:09.906859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.39
 ---- batch: 020 ----
mean loss: 39.07
 ---- batch: 030 ----
mean loss: 37.89
 ---- batch: 040 ----
mean loss: 36.80
train mean loss: 38.05
epoch train time: 0:00:00.203258
elapsed time: 0:00:48.732611
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:41:10.110236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.18
 ---- batch: 020 ----
mean loss: 39.22
 ---- batch: 030 ----
mean loss: 42.95
 ---- batch: 040 ----
mean loss: 37.05
train mean loss: 39.43
epoch train time: 0:00:00.201843
elapsed time: 0:00:48.934581
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:41:10.312244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.49
 ---- batch: 020 ----
mean loss: 36.45
 ---- batch: 030 ----
mean loss: 40.41
 ---- batch: 040 ----
mean loss: 39.26
train mean loss: 38.13
epoch train time: 0:00:00.198944
elapsed time: 0:00:49.133684
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:41:10.511300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.98
 ---- batch: 020 ----
mean loss: 36.49
 ---- batch: 030 ----
mean loss: 37.74
 ---- batch: 040 ----
mean loss: 37.82
train mean loss: 37.51
epoch train time: 0:00:00.200362
elapsed time: 0:00:49.334159
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:41:10.711775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.09
 ---- batch: 020 ----
mean loss: 36.04
 ---- batch: 030 ----
mean loss: 39.24
 ---- batch: 040 ----
mean loss: 38.13
train mean loss: 37.54
epoch train time: 0:00:00.199307
elapsed time: 0:00:49.533577
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:41:10.911192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.36
 ---- batch: 020 ----
mean loss: 39.92
 ---- batch: 030 ----
mean loss: 37.79
 ---- batch: 040 ----
mean loss: 40.02
train mean loss: 38.81
epoch train time: 0:00:00.199685
elapsed time: 0:00:49.733373
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:41:11.111022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.62
 ---- batch: 020 ----
mean loss: 39.67
 ---- batch: 030 ----
mean loss: 37.81
 ---- batch: 040 ----
mean loss: 38.27
train mean loss: 38.05
epoch train time: 0:00:00.199964
elapsed time: 0:00:49.933489
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:41:11.311099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.49
 ---- batch: 020 ----
mean loss: 37.31
 ---- batch: 030 ----
mean loss: 37.56
 ---- batch: 040 ----
mean loss: 36.48
train mean loss: 37.37
epoch train time: 0:00:00.198270
elapsed time: 0:00:50.131865
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:41:11.509497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.01
 ---- batch: 020 ----
mean loss: 38.06
 ---- batch: 030 ----
mean loss: 37.30
 ---- batch: 040 ----
mean loss: 37.47
train mean loss: 37.86
epoch train time: 0:00:00.200153
elapsed time: 0:00:50.332145
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:41:11.709773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.31
 ---- batch: 020 ----
mean loss: 37.15
 ---- batch: 030 ----
mean loss: 37.22
 ---- batch: 040 ----
mean loss: 38.82
train mean loss: 37.14
epoch train time: 0:00:00.200325
elapsed time: 0:00:50.532613
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:41:11.910242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.50
 ---- batch: 020 ----
mean loss: 35.22
 ---- batch: 030 ----
mean loss: 35.21
 ---- batch: 040 ----
mean loss: 38.03
train mean loss: 36.67
epoch train time: 0:00:00.207387
elapsed time: 0:00:50.740129
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:41:12.117757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.15
 ---- batch: 020 ----
mean loss: 38.59
 ---- batch: 030 ----
mean loss: 35.63
 ---- batch: 040 ----
mean loss: 37.49
train mean loss: 37.66
epoch train time: 0:00:00.209928
elapsed time: 0:00:50.950187
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:41:12.327838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.39
 ---- batch: 020 ----
mean loss: 42.13
 ---- batch: 030 ----
mean loss: 36.53
 ---- batch: 040 ----
mean loss: 35.36
train mean loss: 38.59
epoch train time: 0:00:00.207622
elapsed time: 0:00:51.157979
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:41:12.535599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.72
 ---- batch: 020 ----
mean loss: 38.23
 ---- batch: 030 ----
mean loss: 35.88
 ---- batch: 040 ----
mean loss: 36.08
train mean loss: 36.68
epoch train time: 0:00:00.207912
elapsed time: 0:00:51.366014
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:41:12.743648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.09
 ---- batch: 020 ----
mean loss: 34.82
 ---- batch: 030 ----
mean loss: 35.39
 ---- batch: 040 ----
mean loss: 36.75
train mean loss: 36.07
epoch train time: 0:00:00.214400
elapsed time: 0:00:51.580551
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:41:12.958172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.08
 ---- batch: 020 ----
mean loss: 35.74
 ---- batch: 030 ----
mean loss: 33.66
 ---- batch: 040 ----
mean loss: 37.71
train mean loss: 35.87
epoch train time: 0:00:00.212455
elapsed time: 0:00:51.793173
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:41:13.170795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.80
 ---- batch: 020 ----
mean loss: 36.18
 ---- batch: 030 ----
mean loss: 40.19
 ---- batch: 040 ----
mean loss: 37.57
train mean loss: 37.27
epoch train time: 0:00:00.209847
elapsed time: 0:00:52.003161
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:41:13.380779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.24
 ---- batch: 020 ----
mean loss: 38.64
 ---- batch: 030 ----
mean loss: 35.09
 ---- batch: 040 ----
mean loss: 38.57
train mean loss: 36.71
epoch train time: 0:00:00.207978
elapsed time: 0:00:52.211260
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:41:13.588880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.52
 ---- batch: 020 ----
mean loss: 34.56
 ---- batch: 030 ----
mean loss: 38.00
 ---- batch: 040 ----
mean loss: 34.96
train mean loss: 35.98
epoch train time: 0:00:00.216169
elapsed time: 0:00:52.427563
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:41:13.805199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.55
 ---- batch: 020 ----
mean loss: 37.11
 ---- batch: 030 ----
mean loss: 37.06
 ---- batch: 040 ----
mean loss: 31.19
train mean loss: 35.70
epoch train time: 0:00:00.205305
elapsed time: 0:00:52.633003
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:41:14.010638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.16
 ---- batch: 020 ----
mean loss: 38.38
 ---- batch: 030 ----
mean loss: 37.25
 ---- batch: 040 ----
mean loss: 38.33
train mean loss: 36.86
epoch train time: 0:00:00.203322
elapsed time: 0:00:52.836459
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:41:14.214078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.85
 ---- batch: 020 ----
mean loss: 34.78
 ---- batch: 030 ----
mean loss: 35.42
 ---- batch: 040 ----
mean loss: 37.08
train mean loss: 35.37
epoch train time: 0:00:00.198551
elapsed time: 0:00:53.035130
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:41:14.412763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.53
 ---- batch: 020 ----
mean loss: 34.87
 ---- batch: 030 ----
mean loss: 36.54
 ---- batch: 040 ----
mean loss: 37.95
train mean loss: 35.63
epoch train time: 0:00:00.195318
elapsed time: 0:00:53.230611
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:41:14.608230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.52
 ---- batch: 020 ----
mean loss: 36.53
 ---- batch: 030 ----
mean loss: 33.51
 ---- batch: 040 ----
mean loss: 35.73
train mean loss: 35.19
epoch train time: 0:00:00.197723
elapsed time: 0:00:53.428446
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:41:14.806061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.95
 ---- batch: 020 ----
mean loss: 33.79
 ---- batch: 030 ----
mean loss: 36.09
 ---- batch: 040 ----
mean loss: 34.49
train mean loss: 34.70
epoch train time: 0:00:00.200581
elapsed time: 0:00:53.629141
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:41:15.006771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.93
 ---- batch: 020 ----
mean loss: 34.10
 ---- batch: 030 ----
mean loss: 36.25
 ---- batch: 040 ----
mean loss: 33.81
train mean loss: 35.46
epoch train time: 0:00:00.204606
elapsed time: 0:00:53.833873
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:41:15.211522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.97
 ---- batch: 020 ----
mean loss: 35.45
 ---- batch: 030 ----
mean loss: 38.65
 ---- batch: 040 ----
mean loss: 35.27
train mean loss: 36.45
epoch train time: 0:00:00.204588
elapsed time: 0:00:54.038626
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:41:15.416245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.77
 ---- batch: 020 ----
mean loss: 36.81
 ---- batch: 030 ----
mean loss: 34.81
 ---- batch: 040 ----
mean loss: 33.87
train mean loss: 35.76
epoch train time: 0:00:00.201296
elapsed time: 0:00:54.240100
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:41:15.617719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.33
 ---- batch: 020 ----
mean loss: 34.26
 ---- batch: 030 ----
mean loss: 36.44
 ---- batch: 040 ----
mean loss: 36.55
train mean loss: 35.27
epoch train time: 0:00:00.196543
elapsed time: 0:00:54.436782
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:41:15.814407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.99
 ---- batch: 020 ----
mean loss: 35.90
 ---- batch: 030 ----
mean loss: 34.81
 ---- batch: 040 ----
mean loss: 35.56
train mean loss: 35.08
epoch train time: 0:00:00.200278
elapsed time: 0:00:54.637184
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:41:16.014803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.92
 ---- batch: 020 ----
mean loss: 34.53
 ---- batch: 030 ----
mean loss: 33.63
 ---- batch: 040 ----
mean loss: 33.81
train mean loss: 34.51
epoch train time: 0:00:00.202178
elapsed time: 0:00:54.839486
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:41:16.217106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.88
 ---- batch: 020 ----
mean loss: 33.68
 ---- batch: 030 ----
mean loss: 34.89
 ---- batch: 040 ----
mean loss: 34.35
train mean loss: 34.10
epoch train time: 0:00:00.210151
elapsed time: 0:00:55.049759
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:41:16.427393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 32.77
 ---- batch: 020 ----
mean loss: 34.18
 ---- batch: 030 ----
mean loss: 34.68
 ---- batch: 040 ----
mean loss: 34.74
train mean loss: 34.23
epoch train time: 0:00:00.212194
elapsed time: 0:00:55.262086
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:41:16.639706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.42
 ---- batch: 020 ----
mean loss: 33.58
 ---- batch: 030 ----
mean loss: 34.85
 ---- batch: 040 ----
mean loss: 34.86
train mean loss: 34.58
epoch train time: 0:00:00.202536
elapsed time: 0:00:55.464758
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:41:16.842376
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.48
 ---- batch: 020 ----
mean loss: 32.15
 ---- batch: 030 ----
mean loss: 32.79
 ---- batch: 040 ----
mean loss: 33.59
train mean loss: 32.75
epoch train time: 0:00:00.203480
elapsed time: 0:00:55.668383
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:41:17.045994
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.40
 ---- batch: 020 ----
mean loss: 33.46
 ---- batch: 030 ----
mean loss: 33.43
 ---- batch: 040 ----
mean loss: 31.68
train mean loss: 32.53
epoch train time: 0:00:00.202935
elapsed time: 0:00:55.871427
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:41:17.249043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.40
 ---- batch: 020 ----
mean loss: 33.13
 ---- batch: 030 ----
mean loss: 31.40
 ---- batch: 040 ----
mean loss: 31.95
train mean loss: 32.46
epoch train time: 0:00:00.208233
elapsed time: 0:00:56.079776
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:41:17.457394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.70
 ---- batch: 020 ----
mean loss: 32.81
 ---- batch: 030 ----
mean loss: 32.42
 ---- batch: 040 ----
mean loss: 33.49
train mean loss: 32.44
epoch train time: 0:00:00.207960
elapsed time: 0:00:56.287856
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:41:17.665492
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.66
 ---- batch: 020 ----
mean loss: 32.08
 ---- batch: 030 ----
mean loss: 32.86
 ---- batch: 040 ----
mean loss: 32.52
train mean loss: 32.49
epoch train time: 0:00:00.214015
elapsed time: 0:00:56.502008
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:41:17.879657
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.96
 ---- batch: 020 ----
mean loss: 32.11
 ---- batch: 030 ----
mean loss: 32.50
 ---- batch: 040 ----
mean loss: 31.99
train mean loss: 32.50
epoch train time: 0:00:00.206274
elapsed time: 0:00:56.708479
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:41:18.086098
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.71
 ---- batch: 020 ----
mean loss: 31.51
 ---- batch: 030 ----
mean loss: 32.29
 ---- batch: 040 ----
mean loss: 32.61
train mean loss: 32.59
epoch train time: 0:00:00.208667
elapsed time: 0:00:56.917262
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:41:18.294879
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.23
 ---- batch: 020 ----
mean loss: 32.69
 ---- batch: 030 ----
mean loss: 31.24
 ---- batch: 040 ----
mean loss: 31.20
train mean loss: 32.39
epoch train time: 0:00:00.205075
elapsed time: 0:00:57.122455
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:41:18.500074
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.33
 ---- batch: 020 ----
mean loss: 32.20
 ---- batch: 030 ----
mean loss: 31.18
 ---- batch: 040 ----
mean loss: 35.14
train mean loss: 32.42
epoch train time: 0:00:00.206360
elapsed time: 0:00:57.328940
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:41:18.706561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.20
 ---- batch: 020 ----
mean loss: 33.45
 ---- batch: 030 ----
mean loss: 32.82
 ---- batch: 040 ----
mean loss: 30.76
train mean loss: 32.39
epoch train time: 0:00:00.211779
elapsed time: 0:00:57.540887
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:41:18.918507
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.83
 ---- batch: 020 ----
mean loss: 31.76
 ---- batch: 030 ----
mean loss: 31.94
 ---- batch: 040 ----
mean loss: 32.20
train mean loss: 32.38
epoch train time: 0:00:00.202102
elapsed time: 0:00:57.743108
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:41:19.120725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.86
 ---- batch: 020 ----
mean loss: 31.54
 ---- batch: 030 ----
mean loss: 31.04
 ---- batch: 040 ----
mean loss: 34.14
train mean loss: 32.38
epoch train time: 0:00:00.202397
elapsed time: 0:00:57.945621
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:41:19.323238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.97
 ---- batch: 020 ----
mean loss: 29.99
 ---- batch: 030 ----
mean loss: 31.07
 ---- batch: 040 ----
mean loss: 33.55
train mean loss: 32.34
epoch train time: 0:00:00.201479
elapsed time: 0:00:58.147231
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:41:19.524867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.86
 ---- batch: 020 ----
mean loss: 32.01
 ---- batch: 030 ----
mean loss: 32.68
 ---- batch: 040 ----
mean loss: 32.72
train mean loss: 32.33
epoch train time: 0:00:00.199538
elapsed time: 0:00:58.346932
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:41:19.724551
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.14
 ---- batch: 020 ----
mean loss: 31.68
 ---- batch: 030 ----
mean loss: 33.26
 ---- batch: 040 ----
mean loss: 32.64
train mean loss: 32.33
epoch train time: 0:00:00.206254
elapsed time: 0:00:58.553303
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:41:19.930920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.65
 ---- batch: 020 ----
mean loss: 31.83
 ---- batch: 030 ----
mean loss: 32.55
 ---- batch: 040 ----
mean loss: 30.16
train mean loss: 32.35
epoch train time: 0:00:00.200185
elapsed time: 0:00:58.753603
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:41:20.131235
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.58
 ---- batch: 020 ----
mean loss: 32.36
 ---- batch: 030 ----
mean loss: 32.69
 ---- batch: 040 ----
mean loss: 31.46
train mean loss: 32.30
epoch train time: 0:00:00.205810
elapsed time: 0:00:58.959565
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:41:20.337186
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.75
 ---- batch: 020 ----
mean loss: 34.08
 ---- batch: 030 ----
mean loss: 31.91
 ---- batch: 040 ----
mean loss: 32.96
train mean loss: 32.27
epoch train time: 0:00:00.211857
elapsed time: 0:00:59.171547
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:41:20.549168
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.24
 ---- batch: 020 ----
mean loss: 33.24
 ---- batch: 030 ----
mean loss: 31.84
 ---- batch: 040 ----
mean loss: 31.24
train mean loss: 32.31
epoch train time: 0:00:00.214162
elapsed time: 0:00:59.385845
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:41:20.763464
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.05
 ---- batch: 020 ----
mean loss: 32.41
 ---- batch: 030 ----
mean loss: 32.96
 ---- batch: 040 ----
mean loss: 32.30
train mean loss: 32.26
epoch train time: 0:00:00.202053
elapsed time: 0:00:59.588019
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:41:20.965640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.54
 ---- batch: 020 ----
mean loss: 32.06
 ---- batch: 030 ----
mean loss: 32.37
 ---- batch: 040 ----
mean loss: 33.08
train mean loss: 32.31
epoch train time: 0:00:00.203001
elapsed time: 0:00:59.791144
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:41:21.168765
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.42
 ---- batch: 020 ----
mean loss: 33.31
 ---- batch: 030 ----
mean loss: 31.56
 ---- batch: 040 ----
mean loss: 32.62
train mean loss: 32.33
epoch train time: 0:00:00.203857
elapsed time: 0:00:59.995123
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:41:21.372742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.29
 ---- batch: 020 ----
mean loss: 31.70
 ---- batch: 030 ----
mean loss: 31.50
 ---- batch: 040 ----
mean loss: 33.00
train mean loss: 32.17
epoch train time: 0:00:00.205257
elapsed time: 0:01:00.200499
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:41:21.578117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.47
 ---- batch: 020 ----
mean loss: 33.29
 ---- batch: 030 ----
mean loss: 31.32
 ---- batch: 040 ----
mean loss: 32.18
train mean loss: 32.27
epoch train time: 0:00:00.205074
elapsed time: 0:01:00.405690
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:41:21.783308
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.81
 ---- batch: 020 ----
mean loss: 32.71
 ---- batch: 030 ----
mean loss: 32.47
 ---- batch: 040 ----
mean loss: 31.53
train mean loss: 32.23
epoch train time: 0:00:00.206685
elapsed time: 0:01:00.612495
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:41:21.990116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.28
 ---- batch: 020 ----
mean loss: 32.45
 ---- batch: 030 ----
mean loss: 32.29
 ---- batch: 040 ----
mean loss: 32.62
train mean loss: 32.15
epoch train time: 0:00:00.206826
elapsed time: 0:01:00.819444
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:41:22.197079
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.36
 ---- batch: 020 ----
mean loss: 32.69
 ---- batch: 030 ----
mean loss: 32.39
 ---- batch: 040 ----
mean loss: 31.41
train mean loss: 32.08
epoch train time: 0:00:00.208520
elapsed time: 0:01:01.028097
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:41:22.405716
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.64
 ---- batch: 020 ----
mean loss: 30.41
 ---- batch: 030 ----
mean loss: 33.43
 ---- batch: 040 ----
mean loss: 32.70
train mean loss: 32.21
epoch train time: 0:00:00.204257
elapsed time: 0:01:01.232487
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:41:22.610134
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.14
 ---- batch: 020 ----
mean loss: 32.82
 ---- batch: 030 ----
mean loss: 32.39
 ---- batch: 040 ----
mean loss: 33.24
train mean loss: 32.12
epoch train time: 0:00:00.205810
elapsed time: 0:01:01.438441
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:41:22.816057
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.40
 ---- batch: 020 ----
mean loss: 31.88
 ---- batch: 030 ----
mean loss: 32.22
 ---- batch: 040 ----
mean loss: 31.53
train mean loss: 32.13
epoch train time: 0:00:00.207146
elapsed time: 0:01:01.645699
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:41:23.023315
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.42
 ---- batch: 020 ----
mean loss: 30.61
 ---- batch: 030 ----
mean loss: 32.08
 ---- batch: 040 ----
mean loss: 33.04
train mean loss: 32.28
epoch train time: 0:00:00.202489
elapsed time: 0:01:01.848327
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:41:23.225946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.12
 ---- batch: 020 ----
mean loss: 32.79
 ---- batch: 030 ----
mean loss: 32.93
 ---- batch: 040 ----
mean loss: 30.04
train mean loss: 32.05
epoch train time: 0:00:00.201655
elapsed time: 0:01:02.050113
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:41:23.427731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.65
 ---- batch: 020 ----
mean loss: 33.87
 ---- batch: 030 ----
mean loss: 31.09
 ---- batch: 040 ----
mean loss: 31.28
train mean loss: 32.23
epoch train time: 0:00:00.199473
elapsed time: 0:01:02.249722
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:41:23.627334
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.63
 ---- batch: 020 ----
mean loss: 32.05
 ---- batch: 030 ----
mean loss: 33.15
 ---- batch: 040 ----
mean loss: 31.87
train mean loss: 32.12
epoch train time: 0:00:00.206858
elapsed time: 0:01:02.456723
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:41:23.834342
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.58
 ---- batch: 020 ----
mean loss: 31.28
 ---- batch: 030 ----
mean loss: 30.53
 ---- batch: 040 ----
mean loss: 32.77
train mean loss: 32.07
epoch train time: 0:00:00.207285
elapsed time: 0:01:02.664152
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:41:24.041772
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.96
 ---- batch: 020 ----
mean loss: 31.87
 ---- batch: 030 ----
mean loss: 32.32
 ---- batch: 040 ----
mean loss: 31.74
train mean loss: 31.93
epoch train time: 0:00:00.206886
elapsed time: 0:01:02.871169
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:41:24.248787
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.85
 ---- batch: 020 ----
mean loss: 29.72
 ---- batch: 030 ----
mean loss: 32.76
 ---- batch: 040 ----
mean loss: 31.66
train mean loss: 31.98
epoch train time: 0:00:00.204054
elapsed time: 0:01:03.075361
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:41:24.452993
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.01
 ---- batch: 020 ----
mean loss: 32.42
 ---- batch: 030 ----
mean loss: 31.22
 ---- batch: 040 ----
mean loss: 32.11
train mean loss: 32.05
epoch train time: 0:00:00.216305
elapsed time: 0:01:03.291803
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:41:24.669424
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.60
 ---- batch: 020 ----
mean loss: 32.42
 ---- batch: 030 ----
mean loss: 32.93
 ---- batch: 040 ----
mean loss: 32.53
train mean loss: 31.93
epoch train time: 0:00:00.215061
elapsed time: 0:01:03.506992
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:41:24.884629
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.58
 ---- batch: 020 ----
mean loss: 33.55
 ---- batch: 030 ----
mean loss: 30.47
 ---- batch: 040 ----
mean loss: 32.06
train mean loss: 31.99
epoch train time: 0:00:00.213341
elapsed time: 0:01:03.720470
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:41:25.098088
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.71
 ---- batch: 020 ----
mean loss: 31.63
 ---- batch: 030 ----
mean loss: 31.90
 ---- batch: 040 ----
mean loss: 32.84
train mean loss: 31.88
epoch train time: 0:00:00.211691
elapsed time: 0:01:03.932281
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:41:25.309916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.27
 ---- batch: 020 ----
mean loss: 31.92
 ---- batch: 030 ----
mean loss: 31.95
 ---- batch: 040 ----
mean loss: 31.34
train mean loss: 31.85
epoch train time: 0:00:00.207728
elapsed time: 0:01:04.140148
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:41:25.517766
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.32
 ---- batch: 020 ----
mean loss: 30.10
 ---- batch: 030 ----
mean loss: 33.10
 ---- batch: 040 ----
mean loss: 32.02
train mean loss: 31.90
epoch train time: 0:00:00.210754
elapsed time: 0:01:04.351023
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:41:25.728645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.76
 ---- batch: 020 ----
mean loss: 31.63
 ---- batch: 030 ----
mean loss: 32.66
 ---- batch: 040 ----
mean loss: 31.43
train mean loss: 31.82
epoch train time: 0:00:00.209676
elapsed time: 0:01:04.560820
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:41:25.938440
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.06
 ---- batch: 020 ----
mean loss: 33.06
 ---- batch: 030 ----
mean loss: 32.16
 ---- batch: 040 ----
mean loss: 32.11
train mean loss: 31.83
epoch train time: 0:00:00.211719
elapsed time: 0:01:04.772671
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:41:26.150293
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.29
 ---- batch: 020 ----
mean loss: 30.11
 ---- batch: 030 ----
mean loss: 33.73
 ---- batch: 040 ----
mean loss: 33.30
train mean loss: 31.86
epoch train time: 0:00:00.214496
elapsed time: 0:01:04.987305
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:41:26.364934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.86
 ---- batch: 020 ----
mean loss: 32.34
 ---- batch: 030 ----
mean loss: 30.89
 ---- batch: 040 ----
mean loss: 32.05
train mean loss: 31.88
epoch train time: 0:00:00.211903
elapsed time: 0:01:05.199342
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:41:26.576963
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.62
 ---- batch: 020 ----
mean loss: 31.33
 ---- batch: 030 ----
mean loss: 32.82
 ---- batch: 040 ----
mean loss: 30.79
train mean loss: 31.92
epoch train time: 0:00:00.209117
elapsed time: 0:01:05.408596
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:41:26.786217
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.84
 ---- batch: 020 ----
mean loss: 32.29
 ---- batch: 030 ----
mean loss: 31.48
 ---- batch: 040 ----
mean loss: 30.97
train mean loss: 31.78
epoch train time: 0:00:00.206650
elapsed time: 0:01:05.618525
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_2/checkpoint.pth.tar
**** end time: 2019-09-20 23:41:26.996118 ****
