Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 10354
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 07:25:21.529159 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 07:25:21.547520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2801.18
 ---- batch: 020 ----
mean loss: 1296.41
 ---- batch: 030 ----
mean loss: 1091.46
 ---- batch: 040 ----
mean loss: 1093.71
train mean loss: 1480.60
epoch train time: 0:00:23.733177
elapsed time: 0:00:23.760330
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 07:25:45.289535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1004.13
 ---- batch: 020 ----
mean loss: 1025.25
 ---- batch: 030 ----
mean loss: 1003.17
 ---- batch: 040 ----
mean loss: 1003.03
train mean loss: 1005.34
epoch train time: 0:00:08.442019
elapsed time: 0:00:32.203350
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 07:25:53.732778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 977.50
 ---- batch: 020 ----
mean loss: 988.51
 ---- batch: 030 ----
mean loss: 965.32
 ---- batch: 040 ----
mean loss: 980.79
train mean loss: 973.35
epoch train time: 0:00:08.484263
elapsed time: 0:00:40.688802
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 07:26:02.218297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.06
 ---- batch: 020 ----
mean loss: 950.82
 ---- batch: 030 ----
mean loss: 947.03
 ---- batch: 040 ----
mean loss: 964.52
train mean loss: 951.83
epoch train time: 0:00:08.452785
elapsed time: 0:00:49.142855
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 07:26:10.672312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 965.19
 ---- batch: 020 ----
mean loss: 942.28
 ---- batch: 030 ----
mean loss: 948.62
 ---- batch: 040 ----
mean loss: 934.23
train mean loss: 945.39
epoch train time: 0:00:08.473766
elapsed time: 0:00:57.617962
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 07:26:19.147550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.19
 ---- batch: 020 ----
mean loss: 940.77
 ---- batch: 030 ----
mean loss: 945.60
 ---- batch: 040 ----
mean loss: 921.48
train mean loss: 933.28
epoch train time: 0:00:08.471381
elapsed time: 0:01:06.090700
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 07:26:27.620246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.01
 ---- batch: 020 ----
mean loss: 929.57
 ---- batch: 030 ----
mean loss: 930.77
 ---- batch: 040 ----
mean loss: 907.72
train mean loss: 927.55
epoch train time: 0:00:08.451466
elapsed time: 0:01:14.543443
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 07:26:36.072904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.21
 ---- batch: 020 ----
mean loss: 913.58
 ---- batch: 030 ----
mean loss: 928.33
 ---- batch: 040 ----
mean loss: 938.65
train mean loss: 925.14
epoch train time: 0:00:08.449684
elapsed time: 0:01:22.994480
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 07:26:44.523976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.82
 ---- batch: 020 ----
mean loss: 940.85
 ---- batch: 030 ----
mean loss: 905.55
 ---- batch: 040 ----
mean loss: 910.69
train mean loss: 919.42
epoch train time: 0:00:08.453593
elapsed time: 0:01:31.449319
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 07:26:52.978761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.34
 ---- batch: 020 ----
mean loss: 919.82
 ---- batch: 030 ----
mean loss: 910.91
 ---- batch: 040 ----
mean loss: 913.36
train mean loss: 916.69
epoch train time: 0:00:08.461325
elapsed time: 0:01:39.911868
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 07:27:01.441376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.76
 ---- batch: 020 ----
mean loss: 906.29
 ---- batch: 030 ----
mean loss: 910.21
 ---- batch: 040 ----
mean loss: 922.75
train mean loss: 910.95
epoch train time: 0:00:08.478636
elapsed time: 0:01:48.391807
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 07:27:09.921259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.43
 ---- batch: 020 ----
mean loss: 915.07
 ---- batch: 030 ----
mean loss: 906.63
 ---- batch: 040 ----
mean loss: 917.44
train mean loss: 909.64
epoch train time: 0:00:08.500396
elapsed time: 0:01:56.893377
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 07:27:18.422840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.99
 ---- batch: 020 ----
mean loss: 895.19
 ---- batch: 030 ----
mean loss: 897.22
 ---- batch: 040 ----
mean loss: 898.15
train mean loss: 903.98
epoch train time: 0:00:08.476103
elapsed time: 0:02:05.370713
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 07:27:26.900198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.73
 ---- batch: 020 ----
mean loss: 904.49
 ---- batch: 030 ----
mean loss: 907.68
 ---- batch: 040 ----
mean loss: 907.29
train mean loss: 906.09
epoch train time: 0:00:08.459274
elapsed time: 0:02:13.831268
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 07:27:35.360744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.74
 ---- batch: 020 ----
mean loss: 917.06
 ---- batch: 030 ----
mean loss: 901.73
 ---- batch: 040 ----
mean loss: 911.32
train mean loss: 907.35
epoch train time: 0:00:08.464383
elapsed time: 0:02:22.296944
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 07:27:43.826432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.84
 ---- batch: 020 ----
mean loss: 893.88
 ---- batch: 030 ----
mean loss: 906.37
 ---- batch: 040 ----
mean loss: 888.46
train mean loss: 894.54
epoch train time: 0:00:08.490136
elapsed time: 0:02:30.788291
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 07:27:52.317761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.25
 ---- batch: 020 ----
mean loss: 911.49
 ---- batch: 030 ----
mean loss: 897.57
 ---- batch: 040 ----
mean loss: 901.05
train mean loss: 892.77
epoch train time: 0:00:08.476530
elapsed time: 0:02:39.266250
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 07:28:00.795740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 889.53
 ---- batch: 030 ----
mean loss: 893.29
 ---- batch: 040 ----
mean loss: 893.00
train mean loss: 889.26
epoch train time: 0:00:08.468734
elapsed time: 0:02:47.736209
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 07:28:09.265746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.69
 ---- batch: 020 ----
mean loss: 889.68
 ---- batch: 030 ----
mean loss: 873.74
 ---- batch: 040 ----
mean loss: 887.15
train mean loss: 882.39
epoch train time: 0:00:08.468957
elapsed time: 0:02:56.206484
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 07:28:17.735991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.78
 ---- batch: 020 ----
mean loss: 861.50
 ---- batch: 030 ----
mean loss: 882.96
 ---- batch: 040 ----
mean loss: 905.30
train mean loss: 878.44
epoch train time: 0:00:08.456909
elapsed time: 0:03:04.664694
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 07:28:26.194174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.82
 ---- batch: 020 ----
mean loss: 888.12
 ---- batch: 030 ----
mean loss: 870.66
 ---- batch: 040 ----
mean loss: 874.34
train mean loss: 872.47
epoch train time: 0:00:08.477406
elapsed time: 0:03:13.143262
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 07:28:34.672721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.71
 ---- batch: 020 ----
mean loss: 860.23
 ---- batch: 030 ----
mean loss: 860.88
 ---- batch: 040 ----
mean loss: 861.90
train mean loss: 864.18
epoch train time: 0:00:08.486514
elapsed time: 0:03:21.630992
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 07:28:43.160415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.99
 ---- batch: 020 ----
mean loss: 851.35
 ---- batch: 030 ----
mean loss: 846.33
 ---- batch: 040 ----
mean loss: 853.68
train mean loss: 847.85
epoch train time: 0:00:08.489020
elapsed time: 0:03:30.121217
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 07:28:51.650692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.71
 ---- batch: 020 ----
mean loss: 829.54
 ---- batch: 030 ----
mean loss: 783.43
 ---- batch: 040 ----
mean loss: 797.46
train mean loss: 806.19
epoch train time: 0:00:08.473790
elapsed time: 0:03:38.596200
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 07:29:00.125692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.09
 ---- batch: 020 ----
mean loss: 731.12
 ---- batch: 030 ----
mean loss: 732.18
 ---- batch: 040 ----
mean loss: 712.74
train mean loss: 729.81
epoch train time: 0:00:08.561844
elapsed time: 0:03:47.159302
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 07:29:08.688780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 705.54
 ---- batch: 020 ----
mean loss: 684.61
 ---- batch: 030 ----
mean loss: 683.10
 ---- batch: 040 ----
mean loss: 652.41
train mean loss: 679.02
epoch train time: 0:00:08.498961
elapsed time: 0:03:55.659493
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 07:29:17.188946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 654.52
 ---- batch: 020 ----
mean loss: 663.78
 ---- batch: 030 ----
mean loss: 655.39
 ---- batch: 040 ----
mean loss: 629.07
train mean loss: 647.69
epoch train time: 0:00:08.466823
elapsed time: 0:04:04.127650
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 07:29:25.657131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 620.28
 ---- batch: 020 ----
mean loss: 612.36
 ---- batch: 030 ----
mean loss: 601.89
 ---- batch: 040 ----
mean loss: 606.83
train mean loss: 612.02
epoch train time: 0:00:08.494304
elapsed time: 0:04:12.623166
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 07:29:34.152646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 595.87
 ---- batch: 020 ----
mean loss: 595.58
 ---- batch: 030 ----
mean loss: 580.33
 ---- batch: 040 ----
mean loss: 580.57
train mean loss: 585.46
epoch train time: 0:00:08.471010
elapsed time: 0:04:21.095407
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 07:29:42.624868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 577.41
 ---- batch: 020 ----
mean loss: 565.76
 ---- batch: 030 ----
mean loss: 555.41
 ---- batch: 040 ----
mean loss: 552.66
train mean loss: 556.53
epoch train time: 0:00:08.482416
elapsed time: 0:04:29.579025
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 07:29:51.108619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.49
 ---- batch: 020 ----
mean loss: 530.06
 ---- batch: 030 ----
mean loss: 534.43
 ---- batch: 040 ----
mean loss: 516.21
train mean loss: 527.03
epoch train time: 0:00:08.468021
elapsed time: 0:04:38.048355
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 07:29:59.577848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.70
 ---- batch: 020 ----
mean loss: 505.31
 ---- batch: 030 ----
mean loss: 496.82
 ---- batch: 040 ----
mean loss: 502.83
train mean loss: 504.61
epoch train time: 0:00:08.522312
elapsed time: 0:04:46.571987
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 07:30:08.101496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.60
 ---- batch: 020 ----
mean loss: 475.50
 ---- batch: 030 ----
mean loss: 480.75
 ---- batch: 040 ----
mean loss: 471.22
train mean loss: 476.88
epoch train time: 0:00:08.457664
elapsed time: 0:04:55.030994
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 07:30:16.560326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.94
 ---- batch: 020 ----
mean loss: 469.79
 ---- batch: 030 ----
mean loss: 459.99
 ---- batch: 040 ----
mean loss: 457.99
train mean loss: 462.26
epoch train time: 0:00:08.471025
elapsed time: 0:05:03.503213
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 07:30:25.032687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.66
 ---- batch: 020 ----
mean loss: 448.37
 ---- batch: 030 ----
mean loss: 433.80
 ---- batch: 040 ----
mean loss: 460.64
train mean loss: 444.79
epoch train time: 0:00:08.491194
elapsed time: 0:05:11.995688
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 07:30:33.525195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.95
 ---- batch: 020 ----
mean loss: 444.24
 ---- batch: 030 ----
mean loss: 442.98
 ---- batch: 040 ----
mean loss: 427.35
train mean loss: 436.11
epoch train time: 0:00:08.464087
elapsed time: 0:05:20.461035
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 07:30:41.990515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.78
 ---- batch: 020 ----
mean loss: 419.50
 ---- batch: 030 ----
mean loss: 424.93
 ---- batch: 040 ----
mean loss: 433.76
train mean loss: 423.93
epoch train time: 0:00:08.463678
elapsed time: 0:05:28.925959
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 07:30:50.455435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.69
 ---- batch: 020 ----
mean loss: 419.09
 ---- batch: 030 ----
mean loss: 410.28
 ---- batch: 040 ----
mean loss: 411.63
train mean loss: 416.34
epoch train time: 0:00:08.474121
elapsed time: 0:05:37.401353
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 07:30:58.930811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.43
 ---- batch: 020 ----
mean loss: 393.71
 ---- batch: 030 ----
mean loss: 400.27
 ---- batch: 040 ----
mean loss: 408.07
train mean loss: 405.84
epoch train time: 0:00:08.461415
elapsed time: 0:05:45.863998
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 07:31:07.393482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.04
 ---- batch: 020 ----
mean loss: 397.95
 ---- batch: 030 ----
mean loss: 408.53
 ---- batch: 040 ----
mean loss: 400.76
train mean loss: 401.07
epoch train time: 0:00:08.443692
elapsed time: 0:05:54.308984
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 07:31:15.838513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.72
 ---- batch: 020 ----
mean loss: 382.15
 ---- batch: 030 ----
mean loss: 384.13
 ---- batch: 040 ----
mean loss: 385.54
train mean loss: 387.75
epoch train time: 0:00:08.488015
elapsed time: 0:06:02.798506
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 07:31:24.327990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.34
 ---- batch: 020 ----
mean loss: 386.35
 ---- batch: 030 ----
mean loss: 390.00
 ---- batch: 040 ----
mean loss: 383.90
train mean loss: 382.98
epoch train time: 0:00:08.484510
elapsed time: 0:06:11.284263
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 07:31:32.813733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.18
 ---- batch: 020 ----
mean loss: 376.63
 ---- batch: 030 ----
mean loss: 385.13
 ---- batch: 040 ----
mean loss: 388.38
train mean loss: 383.47
epoch train time: 0:00:08.468741
elapsed time: 0:06:19.754190
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 07:31:41.283656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.64
 ---- batch: 020 ----
mean loss: 369.24
 ---- batch: 030 ----
mean loss: 361.42
 ---- batch: 040 ----
mean loss: 378.02
train mean loss: 370.65
epoch train time: 0:00:08.512344
elapsed time: 0:06:28.267853
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 07:31:49.797267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.61
 ---- batch: 020 ----
mean loss: 361.24
 ---- batch: 030 ----
mean loss: 369.89
 ---- batch: 040 ----
mean loss: 370.65
train mean loss: 370.46
epoch train time: 0:00:08.465050
elapsed time: 0:06:36.734159
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 07:31:58.263650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.88
 ---- batch: 020 ----
mean loss: 373.71
 ---- batch: 030 ----
mean loss: 357.19
 ---- batch: 040 ----
mean loss: 348.48
train mean loss: 360.59
epoch train time: 0:00:08.476342
elapsed time: 0:06:45.211840
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 07:32:06.741318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.84
 ---- batch: 020 ----
mean loss: 359.41
 ---- batch: 030 ----
mean loss: 359.52
 ---- batch: 040 ----
mean loss: 362.21
train mean loss: 358.80
epoch train time: 0:00:08.462268
elapsed time: 0:06:53.675387
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 07:32:15.204988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.52
 ---- batch: 020 ----
mean loss: 344.80
 ---- batch: 030 ----
mean loss: 358.81
 ---- batch: 040 ----
mean loss: 352.89
train mean loss: 350.55
epoch train time: 0:00:08.465163
elapsed time: 0:07:02.142028
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 07:32:23.671483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.18
 ---- batch: 020 ----
mean loss: 348.58
 ---- batch: 030 ----
mean loss: 344.06
 ---- batch: 040 ----
mean loss: 344.73
train mean loss: 350.50
epoch train time: 0:00:08.446525
elapsed time: 0:07:10.589829
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 07:32:32.119357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.40
 ---- batch: 020 ----
mean loss: 346.24
 ---- batch: 030 ----
mean loss: 353.06
 ---- batch: 040 ----
mean loss: 342.00
train mean loss: 345.57
epoch train time: 0:00:08.485343
elapsed time: 0:07:19.076532
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 07:32:40.606064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.01
 ---- batch: 020 ----
mean loss: 341.60
 ---- batch: 030 ----
mean loss: 342.81
 ---- batch: 040 ----
mean loss: 336.88
train mean loss: 340.41
epoch train time: 0:00:08.449682
elapsed time: 0:07:27.527664
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 07:32:49.057241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.00
 ---- batch: 020 ----
mean loss: 341.30
 ---- batch: 030 ----
mean loss: 327.33
 ---- batch: 040 ----
mean loss: 334.21
train mean loss: 333.10
epoch train time: 0:00:08.463417
elapsed time: 0:07:35.992374
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 07:32:57.521907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.82
 ---- batch: 020 ----
mean loss: 342.13
 ---- batch: 030 ----
mean loss: 338.47
 ---- batch: 040 ----
mean loss: 336.08
train mean loss: 335.65
epoch train time: 0:00:08.474164
elapsed time: 0:07:44.467824
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 07:33:05.997306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.14
 ---- batch: 020 ----
mean loss: 326.99
 ---- batch: 030 ----
mean loss: 328.98
 ---- batch: 040 ----
mean loss: 325.65
train mean loss: 329.01
epoch train time: 0:00:08.481697
elapsed time: 0:07:52.950854
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 07:33:14.480391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.61
 ---- batch: 020 ----
mean loss: 322.41
 ---- batch: 030 ----
mean loss: 329.19
 ---- batch: 040 ----
mean loss: 328.24
train mean loss: 326.54
epoch train time: 0:00:08.500506
elapsed time: 0:08:01.452671
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 07:33:22.982235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.65
 ---- batch: 020 ----
mean loss: 322.46
 ---- batch: 030 ----
mean loss: 317.79
 ---- batch: 040 ----
mean loss: 331.69
train mean loss: 324.79
epoch train time: 0:00:08.475462
elapsed time: 0:08:09.929440
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 07:33:31.458934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.09
 ---- batch: 020 ----
mean loss: 320.62
 ---- batch: 030 ----
mean loss: 319.13
 ---- batch: 040 ----
mean loss: 316.23
train mean loss: 317.65
epoch train time: 0:00:08.487768
elapsed time: 0:08:18.418488
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 07:33:39.947991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.72
 ---- batch: 020 ----
mean loss: 316.47
 ---- batch: 030 ----
mean loss: 311.35
 ---- batch: 040 ----
mean loss: 318.59
train mean loss: 316.20
epoch train time: 0:00:08.475268
elapsed time: 0:08:26.895074
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 07:33:48.424565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.86
 ---- batch: 020 ----
mean loss: 308.14
 ---- batch: 030 ----
mean loss: 320.28
 ---- batch: 040 ----
mean loss: 318.44
train mean loss: 316.82
epoch train time: 0:00:08.495617
elapsed time: 0:08:35.391952
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 07:33:56.921427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.80
 ---- batch: 020 ----
mean loss: 313.88
 ---- batch: 030 ----
mean loss: 318.52
 ---- batch: 040 ----
mean loss: 303.12
train mean loss: 310.42
epoch train time: 0:00:08.458479
elapsed time: 0:08:43.851727
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 07:34:05.381219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.53
 ---- batch: 020 ----
mean loss: 309.24
 ---- batch: 030 ----
mean loss: 299.91
 ---- batch: 040 ----
mean loss: 312.41
train mean loss: 305.00
epoch train time: 0:00:08.456519
elapsed time: 0:08:52.309493
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 07:34:13.838976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.96
 ---- batch: 020 ----
mean loss: 301.98
 ---- batch: 030 ----
mean loss: 312.89
 ---- batch: 040 ----
mean loss: 296.36
train mean loss: 304.34
epoch train time: 0:00:08.543767
elapsed time: 0:09:00.854529
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 07:34:22.384030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.97
 ---- batch: 020 ----
mean loss: 310.12
 ---- batch: 030 ----
mean loss: 314.00
 ---- batch: 040 ----
mean loss: 299.86
train mean loss: 304.54
epoch train time: 0:00:08.464841
elapsed time: 0:09:09.320668
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 07:34:30.850148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.39
 ---- batch: 020 ----
mean loss: 296.65
 ---- batch: 030 ----
mean loss: 306.44
 ---- batch: 040 ----
mean loss: 305.68
train mean loss: 300.73
epoch train time: 0:00:08.480439
elapsed time: 0:09:17.802365
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 07:34:39.331878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.64
 ---- batch: 020 ----
mean loss: 303.66
 ---- batch: 030 ----
mean loss: 289.42
 ---- batch: 040 ----
mean loss: 286.85
train mean loss: 297.38
epoch train time: 0:00:08.475768
elapsed time: 0:09:26.279392
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 07:34:47.808868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.17
 ---- batch: 020 ----
mean loss: 293.65
 ---- batch: 030 ----
mean loss: 291.63
 ---- batch: 040 ----
mean loss: 303.68
train mean loss: 295.07
epoch train time: 0:00:08.468116
elapsed time: 0:09:34.748753
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 07:34:56.278245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.43
 ---- batch: 020 ----
mean loss: 301.10
 ---- batch: 030 ----
mean loss: 280.55
 ---- batch: 040 ----
mean loss: 301.17
train mean loss: 294.60
epoch train time: 0:00:08.464542
elapsed time: 0:09:43.214577
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 07:35:04.744072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.09
 ---- batch: 020 ----
mean loss: 295.18
 ---- batch: 030 ----
mean loss: 302.63
 ---- batch: 040 ----
mean loss: 291.06
train mean loss: 294.30
epoch train time: 0:00:08.528158
elapsed time: 0:09:51.744004
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 07:35:13.273500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.04
 ---- batch: 020 ----
mean loss: 294.35
 ---- batch: 030 ----
mean loss: 289.31
 ---- batch: 040 ----
mean loss: 288.10
train mean loss: 289.15
epoch train time: 0:00:08.493584
elapsed time: 0:10:00.238805
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 07:35:21.768322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.11
 ---- batch: 020 ----
mean loss: 291.60
 ---- batch: 030 ----
mean loss: 299.10
 ---- batch: 040 ----
mean loss: 296.87
train mean loss: 291.92
epoch train time: 0:00:08.481486
elapsed time: 0:10:08.721683
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 07:35:30.251248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.65
 ---- batch: 020 ----
mean loss: 288.75
 ---- batch: 030 ----
mean loss: 284.28
 ---- batch: 040 ----
mean loss: 291.90
train mean loss: 287.75
epoch train time: 0:00:08.483866
elapsed time: 0:10:17.206860
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 07:35:38.736338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.65
 ---- batch: 020 ----
mean loss: 282.29
 ---- batch: 030 ----
mean loss: 291.97
 ---- batch: 040 ----
mean loss: 291.90
train mean loss: 286.85
epoch train time: 0:00:08.479880
elapsed time: 0:10:25.687968
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 07:35:47.217441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.56
 ---- batch: 020 ----
mean loss: 284.74
 ---- batch: 030 ----
mean loss: 280.08
 ---- batch: 040 ----
mean loss: 273.41
train mean loss: 280.55
epoch train time: 0:00:08.460147
elapsed time: 0:10:34.149345
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 07:35:55.678834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.00
 ---- batch: 020 ----
mean loss: 280.06
 ---- batch: 030 ----
mean loss: 289.98
 ---- batch: 040 ----
mean loss: 276.25
train mean loss: 281.58
epoch train time: 0:00:08.483989
elapsed time: 0:10:42.634610
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 07:36:04.164095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.26
 ---- batch: 020 ----
mean loss: 280.35
 ---- batch: 030 ----
mean loss: 284.71
 ---- batch: 040 ----
mean loss: 283.00
train mean loss: 280.78
epoch train time: 0:00:08.466050
elapsed time: 0:10:51.101882
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 07:36:12.631412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.11
 ---- batch: 020 ----
mean loss: 274.66
 ---- batch: 030 ----
mean loss: 279.19
 ---- batch: 040 ----
mean loss: 277.69
train mean loss: 279.36
epoch train time: 0:00:08.462390
elapsed time: 0:10:59.565561
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 07:36:21.095037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.58
 ---- batch: 020 ----
mean loss: 267.89
 ---- batch: 030 ----
mean loss: 277.80
 ---- batch: 040 ----
mean loss: 277.16
train mean loss: 279.12
epoch train time: 0:00:08.473330
elapsed time: 0:11:08.040103
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 07:36:29.569634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.57
 ---- batch: 020 ----
mean loss: 275.99
 ---- batch: 030 ----
mean loss: 283.53
 ---- batch: 040 ----
mean loss: 266.68
train mean loss: 275.47
epoch train time: 0:00:08.452899
elapsed time: 0:11:16.494344
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 07:36:38.023823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.24
 ---- batch: 020 ----
mean loss: 274.91
 ---- batch: 030 ----
mean loss: 281.42
 ---- batch: 040 ----
mean loss: 270.67
train mean loss: 274.13
epoch train time: 0:00:08.491239
elapsed time: 0:11:24.986813
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 07:36:46.516299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.63
 ---- batch: 020 ----
mean loss: 266.13
 ---- batch: 030 ----
mean loss: 273.32
 ---- batch: 040 ----
mean loss: 273.11
train mean loss: 271.87
epoch train time: 0:00:08.446408
elapsed time: 0:11:33.434444
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 07:36:54.963939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.08
 ---- batch: 020 ----
mean loss: 270.02
 ---- batch: 030 ----
mean loss: 266.03
 ---- batch: 040 ----
mean loss: 265.64
train mean loss: 271.00
epoch train time: 0:00:08.451158
elapsed time: 0:11:41.886918
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 07:37:03.416393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.80
 ---- batch: 020 ----
mean loss: 269.27
 ---- batch: 030 ----
mean loss: 264.84
 ---- batch: 040 ----
mean loss: 272.87
train mean loss: 269.92
epoch train time: 0:00:08.445637
elapsed time: 0:11:50.333829
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 07:37:11.863313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.26
 ---- batch: 020 ----
mean loss: 268.63
 ---- batch: 030 ----
mean loss: 271.14
 ---- batch: 040 ----
mean loss: 270.64
train mean loss: 270.35
epoch train time: 0:00:08.444577
elapsed time: 0:11:58.779594
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 07:37:20.309078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.77
 ---- batch: 020 ----
mean loss: 275.71
 ---- batch: 030 ----
mean loss: 283.27
 ---- batch: 040 ----
mean loss: 262.28
train mean loss: 273.55
epoch train time: 0:00:08.452492
elapsed time: 0:12:07.233335
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 07:37:28.762816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.27
 ---- batch: 020 ----
mean loss: 268.41
 ---- batch: 030 ----
mean loss: 270.68
 ---- batch: 040 ----
mean loss: 269.02
train mean loss: 266.56
epoch train time: 0:00:08.453841
elapsed time: 0:12:15.688363
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 07:37:37.217867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.07
 ---- batch: 020 ----
mean loss: 265.70
 ---- batch: 030 ----
mean loss: 268.29
 ---- batch: 040 ----
mean loss: 268.55
train mean loss: 265.27
epoch train time: 0:00:08.461458
elapsed time: 0:12:24.151666
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 07:37:45.681169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.00
 ---- batch: 020 ----
mean loss: 264.75
 ---- batch: 030 ----
mean loss: 268.57
 ---- batch: 040 ----
mean loss: 260.35
train mean loss: 265.64
epoch train time: 0:00:08.464831
elapsed time: 0:12:32.617736
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 07:37:54.147223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.66
 ---- batch: 020 ----
mean loss: 264.33
 ---- batch: 030 ----
mean loss: 263.84
 ---- batch: 040 ----
mean loss: 260.78
train mean loss: 263.48
epoch train time: 0:00:08.460683
elapsed time: 0:12:41.079605
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 07:38:02.609096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.99
 ---- batch: 020 ----
mean loss: 264.77
 ---- batch: 030 ----
mean loss: 263.14
 ---- batch: 040 ----
mean loss: 256.36
train mean loss: 264.24
epoch train time: 0:00:08.453223
elapsed time: 0:12:49.534167
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 07:38:11.063654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.80
 ---- batch: 020 ----
mean loss: 255.31
 ---- batch: 030 ----
mean loss: 263.62
 ---- batch: 040 ----
mean loss: 260.67
train mean loss: 262.54
epoch train time: 0:00:08.442482
elapsed time: 0:12:57.977957
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 07:38:19.507444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.75
 ---- batch: 020 ----
mean loss: 254.38
 ---- batch: 030 ----
mean loss: 250.77
 ---- batch: 040 ----
mean loss: 263.81
train mean loss: 259.39
epoch train time: 0:00:08.438834
elapsed time: 0:13:06.418004
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 07:38:27.947492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.92
 ---- batch: 020 ----
mean loss: 261.68
 ---- batch: 030 ----
mean loss: 265.62
 ---- batch: 040 ----
mean loss: 265.86
train mean loss: 261.82
epoch train time: 0:00:08.435681
elapsed time: 0:13:14.854898
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 07:38:36.384411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.93
 ---- batch: 020 ----
mean loss: 253.20
 ---- batch: 030 ----
mean loss: 266.32
 ---- batch: 040 ----
mean loss: 265.05
train mean loss: 260.00
epoch train time: 0:00:08.457501
elapsed time: 0:13:23.313705
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 07:38:44.843242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.40
 ---- batch: 020 ----
mean loss: 258.45
 ---- batch: 030 ----
mean loss: 259.13
 ---- batch: 040 ----
mean loss: 257.15
train mean loss: 256.43
epoch train time: 0:00:08.462051
elapsed time: 0:13:31.777063
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 07:38:53.306561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.08
 ---- batch: 020 ----
mean loss: 259.21
 ---- batch: 030 ----
mean loss: 243.47
 ---- batch: 040 ----
mean loss: 256.08
train mean loss: 254.64
epoch train time: 0:00:08.457867
elapsed time: 0:13:40.236210
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 07:39:01.765752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.56
 ---- batch: 020 ----
mean loss: 250.21
 ---- batch: 030 ----
mean loss: 258.22
 ---- batch: 040 ----
mean loss: 249.18
train mean loss: 256.96
epoch train time: 0:00:08.487522
elapsed time: 0:13:48.725020
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 07:39:10.254513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.19
 ---- batch: 020 ----
mean loss: 251.99
 ---- batch: 030 ----
mean loss: 258.53
 ---- batch: 040 ----
mean loss: 258.17
train mean loss: 258.75
epoch train time: 0:00:08.502506
elapsed time: 0:13:57.228807
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 07:39:18.758292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.81
 ---- batch: 020 ----
mean loss: 257.34
 ---- batch: 030 ----
mean loss: 254.69
 ---- batch: 040 ----
mean loss: 257.01
train mean loss: 257.17
epoch train time: 0:00:08.513705
elapsed time: 0:14:05.743833
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 07:39:27.273316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.99
 ---- batch: 020 ----
mean loss: 256.72
 ---- batch: 030 ----
mean loss: 255.22
 ---- batch: 040 ----
mean loss: 252.30
train mean loss: 252.62
epoch train time: 0:00:08.523200
elapsed time: 0:14:14.268320
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 07:39:35.797824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.04
 ---- batch: 020 ----
mean loss: 250.97
 ---- batch: 030 ----
mean loss: 261.34
 ---- batch: 040 ----
mean loss: 255.98
train mean loss: 251.63
epoch train time: 0:00:08.464020
elapsed time: 0:14:22.733624
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 07:39:44.263209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.60
 ---- batch: 020 ----
mean loss: 251.70
 ---- batch: 030 ----
mean loss: 244.76
 ---- batch: 040 ----
mean loss: 258.25
train mean loss: 251.28
epoch train time: 0:00:08.463319
elapsed time: 0:14:31.198314
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 07:39:52.727799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.80
 ---- batch: 020 ----
mean loss: 262.63
 ---- batch: 030 ----
mean loss: 253.07
 ---- batch: 040 ----
mean loss: 243.37
train mean loss: 252.85
epoch train time: 0:00:08.469331
elapsed time: 0:14:39.668896
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 07:40:01.198386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.00
 ---- batch: 020 ----
mean loss: 248.31
 ---- batch: 030 ----
mean loss: 254.10
 ---- batch: 040 ----
mean loss: 250.58
train mean loss: 252.02
epoch train time: 0:00:08.439844
elapsed time: 0:14:48.109961
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 07:40:09.639444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.41
 ---- batch: 020 ----
mean loss: 254.39
 ---- batch: 030 ----
mean loss: 246.48
 ---- batch: 040 ----
mean loss: 248.12
train mean loss: 250.28
epoch train time: 0:00:08.496759
elapsed time: 0:14:56.608053
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 07:40:18.137547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.39
 ---- batch: 020 ----
mean loss: 252.82
 ---- batch: 030 ----
mean loss: 252.57
 ---- batch: 040 ----
mean loss: 250.46
train mean loss: 249.41
epoch train time: 0:00:08.446255
elapsed time: 0:15:05.055580
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 07:40:26.585119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.28
 ---- batch: 020 ----
mean loss: 258.05
 ---- batch: 030 ----
mean loss: 250.88
 ---- batch: 040 ----
mean loss: 249.39
train mean loss: 252.55
epoch train time: 0:00:08.442103
elapsed time: 0:15:13.498992
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 07:40:35.028469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.67
 ---- batch: 020 ----
mean loss: 254.18
 ---- batch: 030 ----
mean loss: 247.68
 ---- batch: 040 ----
mean loss: 238.53
train mean loss: 246.35
epoch train time: 0:00:08.447545
elapsed time: 0:15:21.948080
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 07:40:43.477427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.70
 ---- batch: 020 ----
mean loss: 248.33
 ---- batch: 030 ----
mean loss: 249.58
 ---- batch: 040 ----
mean loss: 246.78
train mean loss: 248.18
epoch train time: 0:00:08.461196
elapsed time: 0:15:30.410336
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 07:40:51.939806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.90
 ---- batch: 020 ----
mean loss: 242.50
 ---- batch: 030 ----
mean loss: 248.72
 ---- batch: 040 ----
mean loss: 246.88
train mean loss: 246.09
epoch train time: 0:00:08.458780
elapsed time: 0:15:38.870335
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 07:41:00.399821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.65
 ---- batch: 020 ----
mean loss: 242.66
 ---- batch: 030 ----
mean loss: 238.75
 ---- batch: 040 ----
mean loss: 247.70
train mean loss: 245.50
epoch train time: 0:00:08.473525
elapsed time: 0:15:47.345222
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 07:41:08.874695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.46
 ---- batch: 020 ----
mean loss: 246.90
 ---- batch: 030 ----
mean loss: 246.88
 ---- batch: 040 ----
mean loss: 246.44
train mean loss: 245.36
epoch train time: 0:00:08.473807
elapsed time: 0:15:55.820218
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 07:41:17.349702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.78
 ---- batch: 020 ----
mean loss: 238.86
 ---- batch: 030 ----
mean loss: 246.82
 ---- batch: 040 ----
mean loss: 243.53
train mean loss: 243.10
epoch train time: 0:00:08.490197
elapsed time: 0:16:04.311680
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 07:41:25.841188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.14
 ---- batch: 020 ----
mean loss: 244.76
 ---- batch: 030 ----
mean loss: 245.97
 ---- batch: 040 ----
mean loss: 244.08
train mean loss: 244.60
epoch train time: 0:00:08.465391
elapsed time: 0:16:12.778286
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 07:41:34.307756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.37
 ---- batch: 020 ----
mean loss: 238.00
 ---- batch: 030 ----
mean loss: 236.84
 ---- batch: 040 ----
mean loss: 253.09
train mean loss: 243.33
epoch train time: 0:00:08.482110
elapsed time: 0:16:21.261592
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 07:41:42.791056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.55
 ---- batch: 020 ----
mean loss: 246.05
 ---- batch: 030 ----
mean loss: 238.66
 ---- batch: 040 ----
mean loss: 238.87
train mean loss: 241.15
epoch train time: 0:00:08.471187
elapsed time: 0:16:29.734221
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 07:41:51.263796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.21
 ---- batch: 020 ----
mean loss: 247.68
 ---- batch: 030 ----
mean loss: 245.93
 ---- batch: 040 ----
mean loss: 249.17
train mean loss: 244.39
epoch train time: 0:00:08.460667
elapsed time: 0:16:38.196256
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 07:41:59.725781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.09
 ---- batch: 020 ----
mean loss: 235.38
 ---- batch: 030 ----
mean loss: 240.52
 ---- batch: 040 ----
mean loss: 247.89
train mean loss: 243.26
epoch train time: 0:00:08.441605
elapsed time: 0:16:46.639088
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 07:42:08.168596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.71
 ---- batch: 020 ----
mean loss: 231.40
 ---- batch: 030 ----
mean loss: 237.92
 ---- batch: 040 ----
mean loss: 245.04
train mean loss: 240.58
epoch train time: 0:00:08.444196
elapsed time: 0:16:55.084579
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 07:42:16.614117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.01
 ---- batch: 020 ----
mean loss: 241.47
 ---- batch: 030 ----
mean loss: 243.19
 ---- batch: 040 ----
mean loss: 232.46
train mean loss: 238.63
epoch train time: 0:00:08.464459
elapsed time: 0:17:03.550285
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 07:42:25.079793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.47
 ---- batch: 020 ----
mean loss: 235.29
 ---- batch: 030 ----
mean loss: 240.64
 ---- batch: 040 ----
mean loss: 245.04
train mean loss: 238.82
epoch train time: 0:00:08.462016
elapsed time: 0:17:12.013491
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 07:42:33.542962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.04
 ---- batch: 020 ----
mean loss: 234.23
 ---- batch: 030 ----
mean loss: 242.06
 ---- batch: 040 ----
mean loss: 241.34
train mean loss: 238.10
epoch train time: 0:00:08.448569
elapsed time: 0:17:20.463335
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 07:42:41.992803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.04
 ---- batch: 020 ----
mean loss: 234.78
 ---- batch: 030 ----
mean loss: 239.50
 ---- batch: 040 ----
mean loss: 237.76
train mean loss: 238.59
epoch train time: 0:00:08.451091
elapsed time: 0:17:28.915648
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 07:42:50.445117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.03
 ---- batch: 020 ----
mean loss: 237.67
 ---- batch: 030 ----
mean loss: 239.47
 ---- batch: 040 ----
mean loss: 230.52
train mean loss: 237.31
epoch train time: 0:00:08.480272
elapsed time: 0:17:37.397227
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 07:42:58.926709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.56
 ---- batch: 020 ----
mean loss: 231.14
 ---- batch: 030 ----
mean loss: 237.63
 ---- batch: 040 ----
mean loss: 235.43
train mean loss: 234.34
epoch train time: 0:00:08.453560
elapsed time: 0:17:45.852210
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 07:43:07.381736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.03
 ---- batch: 020 ----
mean loss: 235.68
 ---- batch: 030 ----
mean loss: 227.61
 ---- batch: 040 ----
mean loss: 243.41
train mean loss: 235.46
epoch train time: 0:00:08.440384
elapsed time: 0:17:54.293883
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 07:43:15.823387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.49
 ---- batch: 020 ----
mean loss: 228.98
 ---- batch: 030 ----
mean loss: 231.53
 ---- batch: 040 ----
mean loss: 240.95
train mean loss: 235.55
epoch train time: 0:00:08.476446
elapsed time: 0:18:02.771705
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 07:43:24.301273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.50
 ---- batch: 020 ----
mean loss: 236.53
 ---- batch: 030 ----
mean loss: 244.07
 ---- batch: 040 ----
mean loss: 227.99
train mean loss: 235.40
epoch train time: 0:00:08.463147
elapsed time: 0:18:11.236373
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 07:43:32.765660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.08
 ---- batch: 020 ----
mean loss: 245.66
 ---- batch: 030 ----
mean loss: 233.27
 ---- batch: 040 ----
mean loss: 242.54
train mean loss: 239.21
epoch train time: 0:00:08.466935
elapsed time: 0:18:19.704535
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 07:43:41.233964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.69
 ---- batch: 020 ----
mean loss: 234.46
 ---- batch: 030 ----
mean loss: 229.67
 ---- batch: 040 ----
mean loss: 233.64
train mean loss: 234.92
epoch train time: 0:00:08.461528
elapsed time: 0:18:28.167238
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 07:43:49.696704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.27
 ---- batch: 020 ----
mean loss: 234.29
 ---- batch: 030 ----
mean loss: 231.86
 ---- batch: 040 ----
mean loss: 231.71
train mean loss: 232.04
epoch train time: 0:00:08.470376
elapsed time: 0:18:36.638811
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 07:43:58.168299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.06
 ---- batch: 020 ----
mean loss: 229.88
 ---- batch: 030 ----
mean loss: 229.59
 ---- batch: 040 ----
mean loss: 231.15
train mean loss: 232.83
epoch train time: 0:00:08.452418
elapsed time: 0:18:45.092549
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 07:44:06.622024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.14
 ---- batch: 020 ----
mean loss: 235.80
 ---- batch: 030 ----
mean loss: 240.21
 ---- batch: 040 ----
mean loss: 225.42
train mean loss: 234.06
epoch train time: 0:00:08.441179
elapsed time: 0:18:53.534977
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 07:44:15.064443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.77
 ---- batch: 020 ----
mean loss: 232.72
 ---- batch: 030 ----
mean loss: 226.58
 ---- batch: 040 ----
mean loss: 237.99
train mean loss: 230.48
epoch train time: 0:00:08.443985
elapsed time: 0:19:01.980173
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 07:44:23.509741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.26
 ---- batch: 020 ----
mean loss: 226.67
 ---- batch: 030 ----
mean loss: 228.00
 ---- batch: 040 ----
mean loss: 226.97
train mean loss: 230.18
epoch train time: 0:00:08.441084
elapsed time: 0:19:10.422777
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 07:44:31.952274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.82
 ---- batch: 020 ----
mean loss: 230.36
 ---- batch: 030 ----
mean loss: 223.01
 ---- batch: 040 ----
mean loss: 228.56
train mean loss: 229.22
epoch train time: 0:00:08.502416
elapsed time: 0:19:18.926486
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 07:44:40.455988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.98
 ---- batch: 020 ----
mean loss: 231.60
 ---- batch: 030 ----
mean loss: 235.69
 ---- batch: 040 ----
mean loss: 231.47
train mean loss: 229.60
epoch train time: 0:00:08.485227
elapsed time: 0:19:27.412977
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 07:44:48.942446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.57
 ---- batch: 020 ----
mean loss: 231.84
 ---- batch: 030 ----
mean loss: 220.61
 ---- batch: 040 ----
mean loss: 228.04
train mean loss: 228.31
epoch train time: 0:00:08.452241
elapsed time: 0:19:35.866443
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 07:44:57.395944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.94
 ---- batch: 020 ----
mean loss: 237.97
 ---- batch: 030 ----
mean loss: 234.27
 ---- batch: 040 ----
mean loss: 227.66
train mean loss: 231.40
epoch train time: 0:00:08.461281
elapsed time: 0:19:44.328990
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 07:45:05.858493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.59
 ---- batch: 020 ----
mean loss: 230.03
 ---- batch: 030 ----
mean loss: 240.46
 ---- batch: 040 ----
mean loss: 229.96
train mean loss: 232.31
epoch train time: 0:00:08.463634
elapsed time: 0:19:52.793848
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 07:45:14.323333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.81
 ---- batch: 020 ----
mean loss: 229.75
 ---- batch: 030 ----
mean loss: 223.51
 ---- batch: 040 ----
mean loss: 235.45
train mean loss: 228.33
epoch train time: 0:00:08.505244
elapsed time: 0:20:01.300461
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 07:45:22.829956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.98
 ---- batch: 020 ----
mean loss: 225.68
 ---- batch: 030 ----
mean loss: 223.12
 ---- batch: 040 ----
mean loss: 232.39
train mean loss: 228.50
epoch train time: 0:00:08.468560
elapsed time: 0:20:09.770248
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 07:45:31.299730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.60
 ---- batch: 020 ----
mean loss: 222.91
 ---- batch: 030 ----
mean loss: 226.29
 ---- batch: 040 ----
mean loss: 227.41
train mean loss: 226.67
epoch train time: 0:00:08.442835
elapsed time: 0:20:18.214348
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 07:45:39.743830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.29
 ---- batch: 020 ----
mean loss: 228.16
 ---- batch: 030 ----
mean loss: 229.10
 ---- batch: 040 ----
mean loss: 228.98
train mean loss: 228.18
epoch train time: 0:00:08.451501
elapsed time: 0:20:26.667084
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 07:45:48.196565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.32
 ---- batch: 020 ----
mean loss: 220.84
 ---- batch: 030 ----
mean loss: 230.66
 ---- batch: 040 ----
mean loss: 225.44
train mean loss: 225.73
epoch train time: 0:00:08.474786
elapsed time: 0:20:35.143101
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 07:45:56.672545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.35
 ---- batch: 020 ----
mean loss: 223.16
 ---- batch: 030 ----
mean loss: 218.47
 ---- batch: 040 ----
mean loss: 232.36
train mean loss: 225.17
epoch train time: 0:00:08.474924
elapsed time: 0:20:43.619163
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 07:46:05.148627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.96
 ---- batch: 020 ----
mean loss: 219.47
 ---- batch: 030 ----
mean loss: 233.10
 ---- batch: 040 ----
mean loss: 222.56
train mean loss: 226.18
epoch train time: 0:00:08.456867
elapsed time: 0:20:52.077270
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 07:46:13.606752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.73
 ---- batch: 020 ----
mean loss: 231.31
 ---- batch: 030 ----
mean loss: 228.32
 ---- batch: 040 ----
mean loss: 215.26
train mean loss: 225.35
epoch train time: 0:00:08.460880
elapsed time: 0:21:00.539360
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 07:46:22.068829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.54
 ---- batch: 020 ----
mean loss: 225.95
 ---- batch: 030 ----
mean loss: 222.99
 ---- batch: 040 ----
mean loss: 230.96
train mean loss: 224.83
epoch train time: 0:00:08.452038
elapsed time: 0:21:08.992857
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 07:46:30.522468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.59
 ---- batch: 020 ----
mean loss: 215.36
 ---- batch: 030 ----
mean loss: 227.19
 ---- batch: 040 ----
mean loss: 221.93
train mean loss: 224.29
epoch train time: 0:00:08.448479
elapsed time: 0:21:17.442904
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 07:46:38.972134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.05
 ---- batch: 020 ----
mean loss: 224.71
 ---- batch: 030 ----
mean loss: 220.16
 ---- batch: 040 ----
mean loss: 225.10
train mean loss: 223.76
epoch train time: 0:00:08.488999
elapsed time: 0:21:25.932856
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 07:46:47.462330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.00
 ---- batch: 020 ----
mean loss: 228.81
 ---- batch: 030 ----
mean loss: 228.66
 ---- batch: 040 ----
mean loss: 214.61
train mean loss: 224.35
epoch train time: 0:00:08.443765
elapsed time: 0:21:34.377831
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 07:46:55.907344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.89
 ---- batch: 020 ----
mean loss: 225.82
 ---- batch: 030 ----
mean loss: 220.86
 ---- batch: 040 ----
mean loss: 222.36
train mean loss: 224.95
epoch train time: 0:00:08.445769
elapsed time: 0:21:42.824868
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 07:47:04.354346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.11
 ---- batch: 020 ----
mean loss: 223.16
 ---- batch: 030 ----
mean loss: 220.10
 ---- batch: 040 ----
mean loss: 225.20
train mean loss: 221.78
epoch train time: 0:00:08.464807
elapsed time: 0:21:51.290899
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 07:47:12.820356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.25
 ---- batch: 020 ----
mean loss: 223.69
 ---- batch: 030 ----
mean loss: 225.02
 ---- batch: 040 ----
mean loss: 217.38
train mean loss: 222.57
epoch train time: 0:00:08.450863
elapsed time: 0:21:59.743084
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 07:47:21.272578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.43
 ---- batch: 020 ----
mean loss: 228.17
 ---- batch: 030 ----
mean loss: 221.46
 ---- batch: 040 ----
mean loss: 212.44
train mean loss: 221.27
epoch train time: 0:00:08.438369
elapsed time: 0:22:08.182804
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 07:47:29.712264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.52
 ---- batch: 020 ----
mean loss: 219.82
 ---- batch: 030 ----
mean loss: 218.43
 ---- batch: 040 ----
mean loss: 223.24
train mean loss: 220.42
epoch train time: 0:00:08.426742
elapsed time: 0:22:16.610747
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 07:47:38.140213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.36
 ---- batch: 020 ----
mean loss: 217.98
 ---- batch: 030 ----
mean loss: 217.65
 ---- batch: 040 ----
mean loss: 217.86
train mean loss: 219.96
epoch train time: 0:00:08.443243
elapsed time: 0:22:25.055382
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 07:47:46.584853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.49
 ---- batch: 020 ----
mean loss: 220.91
 ---- batch: 030 ----
mean loss: 216.08
 ---- batch: 040 ----
mean loss: 223.53
train mean loss: 220.84
epoch train time: 0:00:08.439953
elapsed time: 0:22:33.496574
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 07:47:55.026086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.25
 ---- batch: 020 ----
mean loss: 215.74
 ---- batch: 030 ----
mean loss: 213.51
 ---- batch: 040 ----
mean loss: 226.19
train mean loss: 218.40
epoch train time: 0:00:08.460201
elapsed time: 0:22:41.957981
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 07:48:03.487451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.57
 ---- batch: 020 ----
mean loss: 221.95
 ---- batch: 030 ----
mean loss: 219.87
 ---- batch: 040 ----
mean loss: 213.90
train mean loss: 219.32
epoch train time: 0:00:08.451812
elapsed time: 0:22:50.411045
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 07:48:11.940495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.92
 ---- batch: 020 ----
mean loss: 224.16
 ---- batch: 030 ----
mean loss: 219.07
 ---- batch: 040 ----
mean loss: 216.67
train mean loss: 220.50
epoch train time: 0:00:08.446759
elapsed time: 0:22:58.858972
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 07:48:20.388487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.66
 ---- batch: 020 ----
mean loss: 219.44
 ---- batch: 030 ----
mean loss: 220.61
 ---- batch: 040 ----
mean loss: 220.62
train mean loss: 218.03
epoch train time: 0:00:08.450397
elapsed time: 0:23:07.310600
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 07:48:28.840068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.53
 ---- batch: 020 ----
mean loss: 209.25
 ---- batch: 030 ----
mean loss: 221.45
 ---- batch: 040 ----
mean loss: 222.24
train mean loss: 217.42
epoch train time: 0:00:08.451204
elapsed time: 0:23:15.763065
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 07:48:37.292561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.05
 ---- batch: 020 ----
mean loss: 219.57
 ---- batch: 030 ----
mean loss: 227.17
 ---- batch: 040 ----
mean loss: 224.84
train mean loss: 221.67
epoch train time: 0:00:08.440241
elapsed time: 0:23:24.204683
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 07:48:45.734143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.00
 ---- batch: 020 ----
mean loss: 225.43
 ---- batch: 030 ----
mean loss: 216.95
 ---- batch: 040 ----
mean loss: 219.92
train mean loss: 219.57
epoch train time: 0:00:08.448436
elapsed time: 0:23:32.654320
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 07:48:54.183825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.82
 ---- batch: 020 ----
mean loss: 210.95
 ---- batch: 030 ----
mean loss: 215.96
 ---- batch: 040 ----
mean loss: 225.85
train mean loss: 218.22
epoch train time: 0:00:08.452131
elapsed time: 0:23:41.107730
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 07:49:02.637238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.49
 ---- batch: 020 ----
mean loss: 221.23
 ---- batch: 030 ----
mean loss: 217.82
 ---- batch: 040 ----
mean loss: 212.68
train mean loss: 217.43
epoch train time: 0:00:08.463027
elapsed time: 0:23:49.572046
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 07:49:11.101509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.22
 ---- batch: 020 ----
mean loss: 218.84
 ---- batch: 030 ----
mean loss: 212.05
 ---- batch: 040 ----
mean loss: 215.24
train mean loss: 217.20
epoch train time: 0:00:08.464872
elapsed time: 0:23:58.038138
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 07:49:19.567626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.19
 ---- batch: 020 ----
mean loss: 218.69
 ---- batch: 030 ----
mean loss: 223.60
 ---- batch: 040 ----
mean loss: 215.10
train mean loss: 219.06
epoch train time: 0:00:08.455000
elapsed time: 0:24:06.494433
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 07:49:28.023940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.08
 ---- batch: 020 ----
mean loss: 208.19
 ---- batch: 030 ----
mean loss: 219.76
 ---- batch: 040 ----
mean loss: 221.50
train mean loss: 216.18
epoch train time: 0:00:08.454254
elapsed time: 0:24:14.949915
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 07:49:36.479417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.24
 ---- batch: 020 ----
mean loss: 217.21
 ---- batch: 030 ----
mean loss: 207.01
 ---- batch: 040 ----
mean loss: 222.78
train mean loss: 217.13
epoch train time: 0:00:08.465544
elapsed time: 0:24:23.416783
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 07:49:44.946277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.47
 ---- batch: 020 ----
mean loss: 214.59
 ---- batch: 030 ----
mean loss: 216.52
 ---- batch: 040 ----
mean loss: 221.92
train mean loss: 216.34
epoch train time: 0:00:08.519666
elapsed time: 0:24:31.937663
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 07:49:53.467127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.50
 ---- batch: 020 ----
mean loss: 214.11
 ---- batch: 030 ----
mean loss: 220.51
 ---- batch: 040 ----
mean loss: 219.46
train mean loss: 217.17
epoch train time: 0:00:08.432414
elapsed time: 0:24:40.371342
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 07:50:01.900827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.97
 ---- batch: 020 ----
mean loss: 217.01
 ---- batch: 030 ----
mean loss: 212.80
 ---- batch: 040 ----
mean loss: 212.86
train mean loss: 216.29
epoch train time: 0:00:08.431351
elapsed time: 0:24:48.804235
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 07:50:10.333462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.58
 ---- batch: 020 ----
mean loss: 222.01
 ---- batch: 030 ----
mean loss: 214.48
 ---- batch: 040 ----
mean loss: 221.31
train mean loss: 216.92
epoch train time: 0:00:08.428960
elapsed time: 0:24:57.234219
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 07:50:18.763722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.43
 ---- batch: 020 ----
mean loss: 208.19
 ---- batch: 030 ----
mean loss: 217.54
 ---- batch: 040 ----
mean loss: 221.60
train mean loss: 214.32
epoch train time: 0:00:08.492277
elapsed time: 0:25:05.727736
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 07:50:27.257221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.32
 ---- batch: 020 ----
mean loss: 217.35
 ---- batch: 030 ----
mean loss: 214.13
 ---- batch: 040 ----
mean loss: 215.40
train mean loss: 215.02
epoch train time: 0:00:08.428414
elapsed time: 0:25:14.157467
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 07:50:35.686979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.13
 ---- batch: 020 ----
mean loss: 211.72
 ---- batch: 030 ----
mean loss: 209.33
 ---- batch: 040 ----
mean loss: 215.44
train mean loss: 214.19
epoch train time: 0:00:08.438307
elapsed time: 0:25:22.597097
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 07:50:44.126590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.07
 ---- batch: 020 ----
mean loss: 210.54
 ---- batch: 030 ----
mean loss: 214.26
 ---- batch: 040 ----
mean loss: 211.20
train mean loss: 214.22
epoch train time: 0:00:08.439861
elapsed time: 0:25:31.038161
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 07:50:52.567635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.70
 ---- batch: 020 ----
mean loss: 209.25
 ---- batch: 030 ----
mean loss: 217.61
 ---- batch: 040 ----
mean loss: 219.22
train mean loss: 215.01
epoch train time: 0:00:08.451437
elapsed time: 0:25:39.490850
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 07:51:01.020330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.68
 ---- batch: 020 ----
mean loss: 217.28
 ---- batch: 030 ----
mean loss: 217.38
 ---- batch: 040 ----
mean loss: 207.05
train mean loss: 213.00
epoch train time: 0:00:08.450491
elapsed time: 0:25:47.942606
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 07:51:09.472029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.47
 ---- batch: 020 ----
mean loss: 208.58
 ---- batch: 030 ----
mean loss: 213.22
 ---- batch: 040 ----
mean loss: 212.78
train mean loss: 212.22
epoch train time: 0:00:08.458979
elapsed time: 0:25:56.402765
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 07:51:17.932269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.96
 ---- batch: 020 ----
mean loss: 209.92
 ---- batch: 030 ----
mean loss: 210.55
 ---- batch: 040 ----
mean loss: 214.08
train mean loss: 212.88
epoch train time: 0:00:08.443455
elapsed time: 0:26:04.847468
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 07:51:26.377000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.18
 ---- batch: 020 ----
mean loss: 217.03
 ---- batch: 030 ----
mean loss: 216.74
 ---- batch: 040 ----
mean loss: 208.03
train mean loss: 211.43
epoch train time: 0:00:08.455002
elapsed time: 0:26:13.303735
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 07:51:34.833197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.63
 ---- batch: 020 ----
mean loss: 216.12
 ---- batch: 030 ----
mean loss: 214.52
 ---- batch: 040 ----
mean loss: 211.04
train mean loss: 213.11
epoch train time: 0:00:08.442105
elapsed time: 0:26:21.746993
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 07:51:43.276457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.95
 ---- batch: 020 ----
mean loss: 213.20
 ---- batch: 030 ----
mean loss: 211.65
 ---- batch: 040 ----
mean loss: 210.85
train mean loss: 212.74
epoch train time: 0:00:08.445175
elapsed time: 0:26:30.193381
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 07:51:51.722896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.77
 ---- batch: 020 ----
mean loss: 223.15
 ---- batch: 030 ----
mean loss: 209.17
 ---- batch: 040 ----
mean loss: 217.89
train mean loss: 213.91
epoch train time: 0:00:08.474504
elapsed time: 0:26:38.669112
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 07:52:00.198664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.30
 ---- batch: 020 ----
mean loss: 215.10
 ---- batch: 030 ----
mean loss: 217.37
 ---- batch: 040 ----
mean loss: 206.52
train mean loss: 213.05
epoch train time: 0:00:08.445413
elapsed time: 0:26:47.115816
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 07:52:08.645271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.11
 ---- batch: 020 ----
mean loss: 211.52
 ---- batch: 030 ----
mean loss: 206.27
 ---- batch: 040 ----
mean loss: 210.96
train mean loss: 210.32
epoch train time: 0:00:08.440986
elapsed time: 0:26:55.557987
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 07:52:17.087472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.79
 ---- batch: 020 ----
mean loss: 215.56
 ---- batch: 030 ----
mean loss: 215.28
 ---- batch: 040 ----
mean loss: 212.77
train mean loss: 210.45
epoch train time: 0:00:08.437811
elapsed time: 0:27:03.996999
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 07:52:25.526475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.25
 ---- batch: 020 ----
mean loss: 215.43
 ---- batch: 030 ----
mean loss: 206.45
 ---- batch: 040 ----
mean loss: 205.10
train mean loss: 209.98
epoch train time: 0:00:08.487348
elapsed time: 0:27:12.485655
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 07:52:34.015192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.48
 ---- batch: 020 ----
mean loss: 214.85
 ---- batch: 030 ----
mean loss: 211.64
 ---- batch: 040 ----
mean loss: 207.20
train mean loss: 210.52
epoch train time: 0:00:08.443760
elapsed time: 0:27:20.930745
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 07:52:42.460227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.83
 ---- batch: 020 ----
mean loss: 211.05
 ---- batch: 030 ----
mean loss: 215.66
 ---- batch: 040 ----
mean loss: 205.37
train mean loss: 209.92
epoch train time: 0:00:08.450498
elapsed time: 0:27:29.382425
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 07:52:50.911903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.04
 ---- batch: 020 ----
mean loss: 205.80
 ---- batch: 030 ----
mean loss: 208.56
 ---- batch: 040 ----
mean loss: 213.48
train mean loss: 211.58
epoch train time: 0:00:08.431097
elapsed time: 0:27:37.814827
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 07:52:59.344358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.35
 ---- batch: 020 ----
mean loss: 211.40
 ---- batch: 030 ----
mean loss: 209.74
 ---- batch: 040 ----
mean loss: 207.84
train mean loss: 209.71
epoch train time: 0:00:08.448887
elapsed time: 0:27:46.265211
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 07:53:07.794696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.07
 ---- batch: 020 ----
mean loss: 203.29
 ---- batch: 030 ----
mean loss: 208.71
 ---- batch: 040 ----
mean loss: 215.57
train mean loss: 209.61
epoch train time: 0:00:08.473257
elapsed time: 0:27:54.739711
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 07:53:16.269186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.67
 ---- batch: 020 ----
mean loss: 210.43
 ---- batch: 030 ----
mean loss: 206.73
 ---- batch: 040 ----
mean loss: 209.14
train mean loss: 209.18
epoch train time: 0:00:08.471544
elapsed time: 0:28:03.212579
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 07:53:24.742086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.13
 ---- batch: 020 ----
mean loss: 210.51
 ---- batch: 030 ----
mean loss: 205.17
 ---- batch: 040 ----
mean loss: 215.06
train mean loss: 208.63
epoch train time: 0:00:08.479614
elapsed time: 0:28:11.693612
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 07:53:33.223140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.19
 ---- batch: 020 ----
mean loss: 213.42
 ---- batch: 030 ----
mean loss: 205.44
 ---- batch: 040 ----
mean loss: 208.13
train mean loss: 207.86
epoch train time: 0:00:08.446279
elapsed time: 0:28:20.141298
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 07:53:41.670825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.44
 ---- batch: 020 ----
mean loss: 210.78
 ---- batch: 030 ----
mean loss: 212.30
 ---- batch: 040 ----
mean loss: 211.45
train mean loss: 209.73
epoch train time: 0:00:08.452979
elapsed time: 0:28:28.595551
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 07:53:50.125132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.37
 ---- batch: 020 ----
mean loss: 218.74
 ---- batch: 030 ----
mean loss: 206.37
 ---- batch: 040 ----
mean loss: 213.25
train mean loss: 210.71
epoch train time: 0:00:08.460884
elapsed time: 0:28:37.057763
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 07:53:58.587311
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.63
 ---- batch: 020 ----
mean loss: 207.09
 ---- batch: 030 ----
mean loss: 207.27
 ---- batch: 040 ----
mean loss: 211.39
train mean loss: 206.54
epoch train time: 0:00:08.440755
elapsed time: 0:28:45.500129
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 07:54:07.029350
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.06
 ---- batch: 020 ----
mean loss: 198.82
 ---- batch: 030 ----
mean loss: 208.68
 ---- batch: 040 ----
mean loss: 204.49
train mean loss: 206.01
epoch train time: 0:00:08.483108
elapsed time: 0:28:53.984180
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 07:54:15.513644
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.66
 ---- batch: 020 ----
mean loss: 203.60
 ---- batch: 030 ----
mean loss: 209.58
 ---- batch: 040 ----
mean loss: 196.46
train mean loss: 205.56
epoch train time: 0:00:08.453046
elapsed time: 0:29:02.438430
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 07:54:23.967899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.18
 ---- batch: 020 ----
mean loss: 204.96
 ---- batch: 030 ----
mean loss: 209.64
 ---- batch: 040 ----
mean loss: 207.23
train mean loss: 205.40
epoch train time: 0:00:08.462248
elapsed time: 0:29:10.901895
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 07:54:32.431381
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.65
 ---- batch: 020 ----
mean loss: 204.96
 ---- batch: 030 ----
mean loss: 199.14
 ---- batch: 040 ----
mean loss: 207.78
train mean loss: 205.89
epoch train time: 0:00:08.451357
elapsed time: 0:29:19.354503
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 07:54:40.884004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.26
 ---- batch: 020 ----
mean loss: 203.92
 ---- batch: 030 ----
mean loss: 206.26
 ---- batch: 040 ----
mean loss: 203.40
train mean loss: 205.90
epoch train time: 0:00:08.427407
elapsed time: 0:29:27.783212
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 07:54:49.312677
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.51
 ---- batch: 020 ----
mean loss: 201.34
 ---- batch: 030 ----
mean loss: 202.85
 ---- batch: 040 ----
mean loss: 205.50
train mean loss: 205.40
epoch train time: 0:00:08.461835
elapsed time: 0:29:36.246293
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 07:54:57.775766
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.09
 ---- batch: 020 ----
mean loss: 211.79
 ---- batch: 030 ----
mean loss: 205.09
 ---- batch: 040 ----
mean loss: 204.95
train mean loss: 205.97
epoch train time: 0:00:08.492968
elapsed time: 0:29:44.740438
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 07:55:06.269976
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.30
 ---- batch: 020 ----
mean loss: 208.88
 ---- batch: 030 ----
mean loss: 204.02
 ---- batch: 040 ----
mean loss: 209.53
train mean loss: 206.31
epoch train time: 0:00:08.452819
elapsed time: 0:29:53.194598
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 07:55:14.724071
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.56
 ---- batch: 020 ----
mean loss: 212.67
 ---- batch: 030 ----
mean loss: 207.06
 ---- batch: 040 ----
mean loss: 200.65
train mean loss: 205.86
epoch train time: 0:00:08.462712
elapsed time: 0:30:01.658495
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 07:55:23.188022
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.61
 ---- batch: 020 ----
mean loss: 211.62
 ---- batch: 030 ----
mean loss: 208.26
 ---- batch: 040 ----
mean loss: 198.17
train mean loss: 206.43
epoch train time: 0:00:08.486197
elapsed time: 0:30:10.145957
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 07:55:31.675453
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.93
 ---- batch: 020 ----
mean loss: 198.96
 ---- batch: 030 ----
mean loss: 205.55
 ---- batch: 040 ----
mean loss: 204.37
train mean loss: 205.64
epoch train time: 0:00:08.464879
elapsed time: 0:30:18.612027
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 07:55:40.141492
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.21
 ---- batch: 020 ----
mean loss: 206.41
 ---- batch: 030 ----
mean loss: 204.22
 ---- batch: 040 ----
mean loss: 203.23
train mean loss: 205.66
epoch train time: 0:00:08.458771
elapsed time: 0:30:27.072015
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 07:55:48.601481
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.36
 ---- batch: 020 ----
mean loss: 206.00
 ---- batch: 030 ----
mean loss: 201.99
 ---- batch: 040 ----
mean loss: 208.41
train mean loss: 205.76
epoch train time: 0:00:08.455843
elapsed time: 0:30:35.529150
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 07:55:57.058619
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.90
 ---- batch: 020 ----
mean loss: 202.18
 ---- batch: 030 ----
mean loss: 206.70
 ---- batch: 040 ----
mean loss: 210.10
train mean loss: 206.00
epoch train time: 0:00:08.453731
elapsed time: 0:30:43.984141
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 07:56:05.513649
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.55
 ---- batch: 020 ----
mean loss: 202.30
 ---- batch: 030 ----
mean loss: 204.44
 ---- batch: 040 ----
mean loss: 203.17
train mean loss: 205.32
epoch train time: 0:00:08.455273
elapsed time: 0:30:52.440829
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 07:56:13.970361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.30
 ---- batch: 020 ----
mean loss: 201.94
 ---- batch: 030 ----
mean loss: 207.02
 ---- batch: 040 ----
mean loss: 210.33
train mean loss: 206.05
epoch train time: 0:00:08.490813
elapsed time: 0:31:00.933014
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 07:56:22.462574
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.67
 ---- batch: 020 ----
mean loss: 206.79
 ---- batch: 030 ----
mean loss: 209.27
 ---- batch: 040 ----
mean loss: 203.32
train mean loss: 205.86
epoch train time: 0:00:08.444977
elapsed time: 0:31:09.379274
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 07:56:30.908742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.61
 ---- batch: 020 ----
mean loss: 213.56
 ---- batch: 030 ----
mean loss: 209.02
 ---- batch: 040 ----
mean loss: 204.40
train mean loss: 206.12
epoch train time: 0:00:08.455962
elapsed time: 0:31:17.836408
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 07:56:39.365972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.74
 ---- batch: 020 ----
mean loss: 206.12
 ---- batch: 030 ----
mean loss: 202.52
 ---- batch: 040 ----
mean loss: 209.86
train mean loss: 205.59
epoch train time: 0:00:08.458639
elapsed time: 0:31:26.296439
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 07:56:47.826049
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.57
 ---- batch: 020 ----
mean loss: 202.04
 ---- batch: 030 ----
mean loss: 203.73
 ---- batch: 040 ----
mean loss: 205.02
train mean loss: 205.44
epoch train time: 0:00:08.444489
elapsed time: 0:31:34.742286
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 07:56:56.271747
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.87
 ---- batch: 020 ----
mean loss: 201.56
 ---- batch: 030 ----
mean loss: 198.58
 ---- batch: 040 ----
mean loss: 211.44
train mean loss: 205.67
epoch train time: 0:00:08.454458
elapsed time: 0:31:43.197953
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 07:57:04.727460
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.40
 ---- batch: 020 ----
mean loss: 206.38
 ---- batch: 030 ----
mean loss: 206.14
 ---- batch: 040 ----
mean loss: 204.62
train mean loss: 205.85
epoch train time: 0:00:08.696966
elapsed time: 0:31:51.896233
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 07:57:13.425733
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.36
 ---- batch: 020 ----
mean loss: 206.07
 ---- batch: 030 ----
mean loss: 205.81
 ---- batch: 040 ----
mean loss: 203.49
train mean loss: 206.11
epoch train time: 0:00:08.454525
elapsed time: 0:32:00.351999
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 07:57:21.881476
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.48
 ---- batch: 020 ----
mean loss: 212.33
 ---- batch: 030 ----
mean loss: 206.46
 ---- batch: 040 ----
mean loss: 197.85
train mean loss: 206.08
epoch train time: 0:00:08.463132
elapsed time: 0:32:08.816509
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 07:57:30.346072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.61
 ---- batch: 020 ----
mean loss: 200.90
 ---- batch: 030 ----
mean loss: 209.80
 ---- batch: 040 ----
mean loss: 206.35
train mean loss: 204.61
epoch train time: 0:00:08.696072
elapsed time: 0:32:17.513919
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 07:57:39.043424
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.27
 ---- batch: 020 ----
mean loss: 208.26
 ---- batch: 030 ----
mean loss: 204.26
 ---- batch: 040 ----
mean loss: 205.89
train mean loss: 206.01
epoch train time: 0:00:08.659623
elapsed time: 0:32:26.174840
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 07:57:47.704288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.36
 ---- batch: 020 ----
mean loss: 212.12
 ---- batch: 030 ----
mean loss: 205.46
 ---- batch: 040 ----
mean loss: 203.49
train mean loss: 205.43
epoch train time: 0:00:08.475121
elapsed time: 0:32:34.651203
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 07:57:56.180768
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.93
 ---- batch: 020 ----
mean loss: 203.09
 ---- batch: 030 ----
mean loss: 215.86
 ---- batch: 040 ----
mean loss: 200.87
train mean loss: 205.10
epoch train time: 0:00:08.486155
elapsed time: 0:32:43.138707
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 07:58:04.668185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.68
 ---- batch: 020 ----
mean loss: 200.66
 ---- batch: 030 ----
mean loss: 206.14
 ---- batch: 040 ----
mean loss: 205.32
train mean loss: 205.10
epoch train time: 0:00:08.461340
elapsed time: 0:32:51.601340
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 07:58:13.130827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.50
 ---- batch: 020 ----
mean loss: 202.80
 ---- batch: 030 ----
mean loss: 202.47
 ---- batch: 040 ----
mean loss: 203.25
train mean loss: 205.33
epoch train time: 0:00:08.439093
elapsed time: 0:33:00.041608
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 07:58:21.571088
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.94
 ---- batch: 020 ----
mean loss: 209.71
 ---- batch: 030 ----
mean loss: 202.75
 ---- batch: 040 ----
mean loss: 207.83
train mean loss: 205.34
epoch train time: 0:00:08.456608
elapsed time: 0:33:08.499716
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 07:58:30.029200
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.84
 ---- batch: 020 ----
mean loss: 204.21
 ---- batch: 030 ----
mean loss: 204.82
 ---- batch: 040 ----
mean loss: 201.41
train mean loss: 205.06
epoch train time: 0:00:08.653741
elapsed time: 0:33:17.154951
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 07:58:38.684162
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.01
 ---- batch: 020 ----
mean loss: 210.26
 ---- batch: 030 ----
mean loss: 203.21
 ---- batch: 040 ----
mean loss: 206.80
train mean loss: 204.76
epoch train time: 0:00:08.559817
elapsed time: 0:33:25.715753
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 07:58:47.245262
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.72
 ---- batch: 020 ----
mean loss: 204.67
 ---- batch: 030 ----
mean loss: 205.99
 ---- batch: 040 ----
mean loss: 198.63
train mean loss: 204.85
epoch train time: 0:00:08.480763
elapsed time: 0:33:34.197815
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 07:58:55.727301
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.85
 ---- batch: 020 ----
mean loss: 200.66
 ---- batch: 030 ----
mean loss: 208.29
 ---- batch: 040 ----
mean loss: 201.78
train mean loss: 204.98
epoch train time: 0:00:08.479625
elapsed time: 0:33:42.678685
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 07:59:04.208155
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.37
 ---- batch: 020 ----
mean loss: 212.22
 ---- batch: 030 ----
mean loss: 201.48
 ---- batch: 040 ----
mean loss: 196.55
train mean loss: 204.46
epoch train time: 0:00:08.478695
elapsed time: 0:33:51.158671
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 07:59:12.688217
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.86
 ---- batch: 020 ----
mean loss: 206.75
 ---- batch: 030 ----
mean loss: 203.37
 ---- batch: 040 ----
mean loss: 210.00
train mean loss: 204.73
epoch train time: 0:00:08.472576
elapsed time: 0:33:59.632804
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 07:59:21.162368
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.60
 ---- batch: 020 ----
mean loss: 207.39
 ---- batch: 030 ----
mean loss: 202.41
 ---- batch: 040 ----
mean loss: 207.29
train mean loss: 204.56
epoch train time: 0:00:08.486349
elapsed time: 0:34:08.120436
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 07:59:29.649967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.23
 ---- batch: 020 ----
mean loss: 209.82
 ---- batch: 030 ----
mean loss: 204.12
 ---- batch: 040 ----
mean loss: 198.76
train mean loss: 205.40
epoch train time: 0:00:08.463870
elapsed time: 0:34:16.585663
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 07:59:38.115137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.69
 ---- batch: 020 ----
mean loss: 207.88
 ---- batch: 030 ----
mean loss: 205.89
 ---- batch: 040 ----
mean loss: 202.73
train mean loss: 205.71
epoch train time: 0:00:08.480783
elapsed time: 0:34:25.067612
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 07:59:46.597143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.59
 ---- batch: 020 ----
mean loss: 212.65
 ---- batch: 030 ----
mean loss: 205.43
 ---- batch: 040 ----
mean loss: 201.21
train mean loss: 205.00
epoch train time: 0:00:08.464811
elapsed time: 0:34:33.533846
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 07:59:55.063457
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.86
 ---- batch: 020 ----
mean loss: 206.00
 ---- batch: 030 ----
mean loss: 201.91
 ---- batch: 040 ----
mean loss: 211.99
train mean loss: 204.98
epoch train time: 0:00:08.461402
elapsed time: 0:34:41.996596
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 08:00:03.526055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.45
 ---- batch: 020 ----
mean loss: 207.43
 ---- batch: 030 ----
mean loss: 203.82
 ---- batch: 040 ----
mean loss: 213.13
train mean loss: 204.95
epoch train time: 0:00:08.497126
elapsed time: 0:34:50.494975
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 08:00:12.024430
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.58
 ---- batch: 020 ----
mean loss: 200.82
 ---- batch: 030 ----
mean loss: 202.47
 ---- batch: 040 ----
mean loss: 204.72
train mean loss: 205.12
epoch train time: 0:00:08.491179
elapsed time: 0:34:58.987485
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 08:00:20.516959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.25
 ---- batch: 020 ----
mean loss: 206.68
 ---- batch: 030 ----
mean loss: 205.12
 ---- batch: 040 ----
mean loss: 204.30
train mean loss: 204.55
epoch train time: 0:00:08.435447
elapsed time: 0:35:07.424115
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 08:00:28.953623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.87
 ---- batch: 020 ----
mean loss: 202.92
 ---- batch: 030 ----
mean loss: 200.44
 ---- batch: 040 ----
mean loss: 204.54
train mean loss: 205.45
epoch train time: 0:00:08.468160
elapsed time: 0:35:15.893618
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 08:00:37.423091
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.96
 ---- batch: 020 ----
mean loss: 205.24
 ---- batch: 030 ----
mean loss: 200.34
 ---- batch: 040 ----
mean loss: 205.77
train mean loss: 204.43
epoch train time: 0:00:08.433428
elapsed time: 0:35:24.328303
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 08:00:45.857852
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.30
 ---- batch: 020 ----
mean loss: 207.24
 ---- batch: 030 ----
mean loss: 199.03
 ---- batch: 040 ----
mean loss: 202.86
train mean loss: 204.39
epoch train time: 0:00:08.453856
elapsed time: 0:35:32.793654
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_1/checkpoint.pth.tar
**** end time: 2019-09-25 08:00:54.322831 ****
