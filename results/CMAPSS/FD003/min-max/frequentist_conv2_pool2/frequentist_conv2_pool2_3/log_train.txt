Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_3', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8005
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-20 23:28:03.164851 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:28:03.169493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4767.86
 ---- batch: 020 ----
mean loss: 4730.44
 ---- batch: 030 ----
mean loss: 4755.40
 ---- batch: 040 ----
mean loss: 4648.53
train mean loss: 4715.32
epoch train time: 0:00:14.909265
elapsed time: 0:00:14.915123
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:28:18.080013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4616.87
 ---- batch: 020 ----
mean loss: 4492.53
 ---- batch: 030 ----
mean loss: 4405.28
 ---- batch: 040 ----
mean loss: 4432.27
train mean loss: 4473.51
epoch train time: 0:00:00.204036
elapsed time: 0:00:15.119318
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:28:18.284245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4226.30
 ---- batch: 020 ----
mean loss: 4177.43
 ---- batch: 030 ----
mean loss: 4154.83
 ---- batch: 040 ----
mean loss: 4003.73
train mean loss: 4121.64
epoch train time: 0:00:00.201333
elapsed time: 0:00:15.320803
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:28:18.485697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3858.47
 ---- batch: 020 ----
mean loss: 3889.39
 ---- batch: 030 ----
mean loss: 3607.65
 ---- batch: 040 ----
mean loss: 3663.20
train mean loss: 3751.87
epoch train time: 0:00:00.196489
elapsed time: 0:00:15.517439
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:28:18.682342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3562.30
 ---- batch: 020 ----
mean loss: 3447.59
 ---- batch: 030 ----
mean loss: 3411.78
 ---- batch: 040 ----
mean loss: 3313.55
train mean loss: 3423.12
epoch train time: 0:00:00.205251
elapsed time: 0:00:15.722874
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:28:18.887768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3217.75
 ---- batch: 020 ----
mean loss: 3221.15
 ---- batch: 030 ----
mean loss: 3111.58
 ---- batch: 040 ----
mean loss: 3036.23
train mean loss: 3140.37
epoch train time: 0:00:00.198721
elapsed time: 0:00:15.921742
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:28:19.086635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2958.05
 ---- batch: 020 ----
mean loss: 2971.48
 ---- batch: 030 ----
mean loss: 2890.23
 ---- batch: 040 ----
mean loss: 2761.40
train mean loss: 2888.99
epoch train time: 0:00:00.197776
elapsed time: 0:00:16.119649
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:28:19.284555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2761.49
 ---- batch: 020 ----
mean loss: 2649.62
 ---- batch: 030 ----
mean loss: 2675.53
 ---- batch: 040 ----
mean loss: 2590.56
train mean loss: 2664.56
epoch train time: 0:00:00.200022
elapsed time: 0:00:16.319805
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:28:19.484735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2594.48
 ---- batch: 020 ----
mean loss: 2468.63
 ---- batch: 030 ----
mean loss: 2430.33
 ---- batch: 040 ----
mean loss: 2379.44
train mean loss: 2462.12
epoch train time: 0:00:00.199300
elapsed time: 0:00:16.519267
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:28:19.684155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2338.17
 ---- batch: 020 ----
mean loss: 2289.25
 ---- batch: 030 ----
mean loss: 2263.40
 ---- batch: 040 ----
mean loss: 2255.80
train mean loss: 2279.08
epoch train time: 0:00:00.203379
elapsed time: 0:00:16.722765
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:28:19.887657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2163.96
 ---- batch: 020 ----
mean loss: 2142.58
 ---- batch: 030 ----
mean loss: 2114.34
 ---- batch: 040 ----
mean loss: 2030.43
train mean loss: 2113.73
epoch train time: 0:00:00.200910
elapsed time: 0:00:16.923811
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:28:20.088703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2052.20
 ---- batch: 020 ----
mean loss: 1961.00
 ---- batch: 030 ----
mean loss: 1931.70
 ---- batch: 040 ----
mean loss: 1917.91
train mean loss: 1962.39
epoch train time: 0:00:00.198215
elapsed time: 0:00:17.122143
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:28:20.287036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1864.34
 ---- batch: 020 ----
mean loss: 1847.08
 ---- batch: 030 ----
mean loss: 1801.28
 ---- batch: 040 ----
mean loss: 1801.54
train mean loss: 1824.46
epoch train time: 0:00:00.205944
elapsed time: 0:00:17.328240
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:28:20.493134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1739.40
 ---- batch: 020 ----
mean loss: 1716.46
 ---- batch: 030 ----
mean loss: 1684.24
 ---- batch: 040 ----
mean loss: 1670.61
train mean loss: 1701.08
epoch train time: 0:00:00.210504
elapsed time: 0:00:17.538867
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:28:20.703762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1639.01
 ---- batch: 020 ----
mean loss: 1594.84
 ---- batch: 030 ----
mean loss: 1567.95
 ---- batch: 040 ----
mean loss: 1565.62
train mean loss: 1586.32
epoch train time: 0:00:00.207615
elapsed time: 0:00:17.746619
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:28:20.911514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1502.24
 ---- batch: 020 ----
mean loss: 1513.61
 ---- batch: 030 ----
mean loss: 1483.37
 ---- batch: 040 ----
mean loss: 1449.47
train mean loss: 1483.67
epoch train time: 0:00:00.205171
elapsed time: 0:00:17.951920
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:28:21.116823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1408.33
 ---- batch: 020 ----
mean loss: 1397.82
 ---- batch: 030 ----
mean loss: 1388.12
 ---- batch: 040 ----
mean loss: 1372.52
train mean loss: 1391.54
epoch train time: 0:00:00.202857
elapsed time: 0:00:18.154934
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:28:21.319834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1329.11
 ---- batch: 020 ----
mean loss: 1319.14
 ---- batch: 030 ----
mean loss: 1299.95
 ---- batch: 040 ----
mean loss: 1284.79
train mean loss: 1306.72
epoch train time: 0:00:00.205523
elapsed time: 0:00:18.360599
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:28:21.525521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1271.85
 ---- batch: 020 ----
mean loss: 1217.98
 ---- batch: 030 ----
mean loss: 1238.30
 ---- batch: 040 ----
mean loss: 1207.44
train mean loss: 1230.48
epoch train time: 0:00:00.204300
elapsed time: 0:00:18.565047
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:28:21.729939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1174.53
 ---- batch: 020 ----
mean loss: 1177.71
 ---- batch: 030 ----
mean loss: 1173.49
 ---- batch: 040 ----
mean loss: 1129.31
train mean loss: 1163.76
epoch train time: 0:00:00.207003
elapsed time: 0:00:18.772198
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:28:21.937092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1120.30
 ---- batch: 020 ----
mean loss: 1113.14
 ---- batch: 030 ----
mean loss: 1097.02
 ---- batch: 040 ----
mean loss: 1083.50
train mean loss: 1101.86
epoch train time: 0:00:00.206827
elapsed time: 0:00:18.979148
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:28:22.144059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1071.28
 ---- batch: 020 ----
mean loss: 1043.21
 ---- batch: 030 ----
mean loss: 1041.19
 ---- batch: 040 ----
mean loss: 1039.41
train mean loss: 1046.55
epoch train time: 0:00:00.207263
elapsed time: 0:00:19.186550
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:28:22.351456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1019.13
 ---- batch: 020 ----
mean loss: 1010.56
 ---- batch: 030 ----
mean loss: 992.47
 ---- batch: 040 ----
mean loss: 973.31
train mean loss: 997.91
epoch train time: 0:00:00.205460
elapsed time: 0:00:19.392144
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:28:22.557038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 981.84
 ---- batch: 020 ----
mean loss: 968.55
 ---- batch: 030 ----
mean loss: 948.13
 ---- batch: 040 ----
mean loss: 931.96
train mean loss: 954.43
epoch train time: 0:00:00.200386
elapsed time: 0:00:19.592655
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:28:22.757557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.31
 ---- batch: 020 ----
mean loss: 921.93
 ---- batch: 030 ----
mean loss: 917.01
 ---- batch: 040 ----
mean loss: 889.73
train mean loss: 916.14
epoch train time: 0:00:00.204905
elapsed time: 0:00:19.797686
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:28:22.962576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.82
 ---- batch: 020 ----
mean loss: 900.18
 ---- batch: 030 ----
mean loss: 878.76
 ---- batch: 040 ----
mean loss: 867.02
train mean loss: 881.51
epoch train time: 0:00:00.199306
elapsed time: 0:00:19.997157
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:28:23.162053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.59
 ---- batch: 020 ----
mean loss: 843.42
 ---- batch: 030 ----
mean loss: 849.06
 ---- batch: 040 ----
mean loss: 846.93
train mean loss: 851.43
epoch train time: 0:00:00.199433
elapsed time: 0:00:20.196727
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:28:23.361617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 833.58
 ---- batch: 020 ----
mean loss: 830.79
 ---- batch: 030 ----
mean loss: 821.69
 ---- batch: 040 ----
mean loss: 821.45
train mean loss: 825.35
epoch train time: 0:00:00.198036
elapsed time: 0:00:20.394888
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:28:23.559783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 823.34
 ---- batch: 020 ----
mean loss: 793.68
 ---- batch: 030 ----
mean loss: 798.91
 ---- batch: 040 ----
mean loss: 795.73
train mean loss: 802.32
epoch train time: 0:00:00.205019
elapsed time: 0:00:20.600025
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:28:23.764917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 789.53
 ---- batch: 020 ----
mean loss: 771.21
 ---- batch: 030 ----
mean loss: 792.81
 ---- batch: 040 ----
mean loss: 773.72
train mean loss: 782.57
epoch train time: 0:00:00.200543
elapsed time: 0:00:20.800682
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:28:23.965587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 769.65
 ---- batch: 020 ----
mean loss: 761.06
 ---- batch: 030 ----
mean loss: 769.38
 ---- batch: 040 ----
mean loss: 761.92
train mean loss: 764.47
epoch train time: 0:00:00.193692
elapsed time: 0:00:20.994504
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:28:24.159396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 766.89
 ---- batch: 020 ----
mean loss: 741.85
 ---- batch: 030 ----
mean loss: 757.03
 ---- batch: 040 ----
mean loss: 737.72
train mean loss: 749.27
epoch train time: 0:00:00.191927
elapsed time: 0:00:21.186545
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:28:24.351441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 762.30
 ---- batch: 020 ----
mean loss: 721.79
 ---- batch: 030 ----
mean loss: 723.27
 ---- batch: 040 ----
mean loss: 746.13
train mean loss: 736.21
epoch train time: 0:00:00.205908
elapsed time: 0:00:21.392586
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:28:24.557511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 727.10
 ---- batch: 020 ----
mean loss: 726.73
 ---- batch: 030 ----
mean loss: 730.46
 ---- batch: 040 ----
mean loss: 719.94
train mean loss: 724.89
epoch train time: 0:00:00.208976
elapsed time: 0:00:21.601715
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:28:24.766607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 715.81
 ---- batch: 020 ----
mean loss: 717.92
 ---- batch: 030 ----
mean loss: 703.61
 ---- batch: 040 ----
mean loss: 718.00
train mean loss: 715.75
epoch train time: 0:00:00.209543
elapsed time: 0:00:21.811386
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:28:24.976282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 712.00
 ---- batch: 020 ----
mean loss: 698.45
 ---- batch: 030 ----
mean loss: 701.30
 ---- batch: 040 ----
mean loss: 722.98
train mean loss: 706.78
epoch train time: 0:00:00.208705
elapsed time: 0:00:22.020226
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:28:25.185119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.92
 ---- batch: 020 ----
mean loss: 707.43
 ---- batch: 030 ----
mean loss: 701.62
 ---- batch: 040 ----
mean loss: 702.67
train mean loss: 700.29
epoch train time: 0:00:00.205898
elapsed time: 0:00:22.226240
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:28:25.391132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 693.43
 ---- batch: 020 ----
mean loss: 686.66
 ---- batch: 030 ----
mean loss: 685.81
 ---- batch: 040 ----
mean loss: 712.08
train mean loss: 694.50
epoch train time: 0:00:00.205888
elapsed time: 0:00:22.432267
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:28:25.597160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 693.75
 ---- batch: 020 ----
mean loss: 677.11
 ---- batch: 030 ----
mean loss: 708.44
 ---- batch: 040 ----
mean loss: 686.15
train mean loss: 689.19
epoch train time: 0:00:00.206770
elapsed time: 0:00:22.639172
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:28:25.804066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 698.50
 ---- batch: 020 ----
mean loss: 686.65
 ---- batch: 030 ----
mean loss: 684.36
 ---- batch: 040 ----
mean loss: 677.13
train mean loss: 684.62
epoch train time: 0:00:00.203541
elapsed time: 0:00:22.842834
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:28:26.007729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.36
 ---- batch: 020 ----
mean loss: 698.37
 ---- batch: 030 ----
mean loss: 683.94
 ---- batch: 040 ----
mean loss: 674.57
train mean loss: 681.47
epoch train time: 0:00:00.204233
elapsed time: 0:00:23.047203
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:28:26.212096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.49
 ---- batch: 020 ----
mean loss: 679.84
 ---- batch: 030 ----
mean loss: 672.60
 ---- batch: 040 ----
mean loss: 665.52
train mean loss: 678.65
epoch train time: 0:00:00.196810
elapsed time: 0:00:23.244247
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:28:26.409157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 676.47
 ---- batch: 020 ----
mean loss: 676.69
 ---- batch: 030 ----
mean loss: 679.54
 ---- batch: 040 ----
mean loss: 678.13
train mean loss: 676.17
epoch train time: 0:00:00.196237
elapsed time: 0:00:23.440621
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:28:26.605510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 691.41
 ---- batch: 020 ----
mean loss: 663.55
 ---- batch: 030 ----
mean loss: 674.54
 ---- batch: 040 ----
mean loss: 667.92
train mean loss: 674.70
epoch train time: 0:00:00.198388
elapsed time: 0:00:23.639139
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:28:26.804038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 674.91
 ---- batch: 020 ----
mean loss: 647.35
 ---- batch: 030 ----
mean loss: 676.91
 ---- batch: 040 ----
mean loss: 687.21
train mean loss: 672.36
epoch train time: 0:00:00.196638
elapsed time: 0:00:23.835900
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:28:27.000788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.24
 ---- batch: 020 ----
mean loss: 653.92
 ---- batch: 030 ----
mean loss: 670.98
 ---- batch: 040 ----
mean loss: 676.55
train mean loss: 671.14
epoch train time: 0:00:00.196457
elapsed time: 0:00:24.032467
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:28:27.197374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.21
 ---- batch: 020 ----
mean loss: 678.83
 ---- batch: 030 ----
mean loss: 659.77
 ---- batch: 040 ----
mean loss: 685.11
train mean loss: 669.61
epoch train time: 0:00:00.193149
elapsed time: 0:00:24.225759
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:28:27.390648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 666.15
 ---- batch: 020 ----
mean loss: 679.51
 ---- batch: 030 ----
mean loss: 665.33
 ---- batch: 040 ----
mean loss: 673.24
train mean loss: 668.46
epoch train time: 0:00:00.193790
elapsed time: 0:00:24.419677
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:28:27.584567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.71
 ---- batch: 020 ----
mean loss: 678.01
 ---- batch: 030 ----
mean loss: 655.58
 ---- batch: 040 ----
mean loss: 672.50
train mean loss: 666.85
epoch train time: 0:00:00.212201
elapsed time: 0:00:24.631997
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:28:27.796889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 654.59
 ---- batch: 020 ----
mean loss: 657.70
 ---- batch: 030 ----
mean loss: 676.45
 ---- batch: 040 ----
mean loss: 669.23
train mean loss: 662.85
epoch train time: 0:00:00.204106
elapsed time: 0:00:24.836218
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:28:28.001107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 645.75
 ---- batch: 020 ----
mean loss: 638.97
 ---- batch: 030 ----
mean loss: 581.95
 ---- batch: 040 ----
mean loss: 531.08
train mean loss: 592.59
epoch train time: 0:00:00.204551
elapsed time: 0:00:25.040894
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:28:28.205807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.04
 ---- batch: 020 ----
mean loss: 439.09
 ---- batch: 030 ----
mean loss: 436.34
 ---- batch: 040 ----
mean loss: 430.42
train mean loss: 441.07
epoch train time: 0:00:00.208444
elapsed time: 0:00:25.249478
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:28:28.414374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.95
 ---- batch: 020 ----
mean loss: 423.24
 ---- batch: 030 ----
mean loss: 415.35
 ---- batch: 040 ----
mean loss: 407.54
train mean loss: 419.09
epoch train time: 0:00:00.209585
elapsed time: 0:00:25.459188
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:28:28.624081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.48
 ---- batch: 020 ----
mean loss: 410.79
 ---- batch: 030 ----
mean loss: 407.10
 ---- batch: 040 ----
mean loss: 402.55
train mean loss: 406.79
epoch train time: 0:00:00.217838
elapsed time: 0:00:25.677180
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:28:28.842094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.80
 ---- batch: 020 ----
mean loss: 400.45
 ---- batch: 030 ----
mean loss: 398.53
 ---- batch: 040 ----
mean loss: 392.50
train mean loss: 397.36
epoch train time: 0:00:00.208486
elapsed time: 0:00:25.885825
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:28:29.050731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.29
 ---- batch: 020 ----
mean loss: 383.29
 ---- batch: 030 ----
mean loss: 389.56
 ---- batch: 040 ----
mean loss: 384.93
train mean loss: 389.11
epoch train time: 0:00:00.207924
elapsed time: 0:00:26.093883
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:28:29.258777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.89
 ---- batch: 020 ----
mean loss: 376.35
 ---- batch: 030 ----
mean loss: 381.98
 ---- batch: 040 ----
mean loss: 384.80
train mean loss: 382.27
epoch train time: 0:00:00.209251
elapsed time: 0:00:26.303272
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:28:29.468165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.74
 ---- batch: 020 ----
mean loss: 374.56
 ---- batch: 030 ----
mean loss: 375.44
 ---- batch: 040 ----
mean loss: 372.01
train mean loss: 376.01
epoch train time: 0:00:00.212806
elapsed time: 0:00:26.516207
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:28:29.681109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.26
 ---- batch: 020 ----
mean loss: 376.16
 ---- batch: 030 ----
mean loss: 370.62
 ---- batch: 040 ----
mean loss: 369.74
train mean loss: 370.57
epoch train time: 0:00:00.217551
elapsed time: 0:00:26.733896
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:28:29.898790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.61
 ---- batch: 020 ----
mean loss: 370.21
 ---- batch: 030 ----
mean loss: 366.83
 ---- batch: 040 ----
mean loss: 365.25
train mean loss: 365.31
epoch train time: 0:00:00.205373
elapsed time: 0:00:26.939384
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:28:30.104273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.27
 ---- batch: 020 ----
mean loss: 359.63
 ---- batch: 030 ----
mean loss: 366.39
 ---- batch: 040 ----
mean loss: 355.13
train mean loss: 361.13
epoch train time: 0:00:00.194158
elapsed time: 0:00:27.133659
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:28:30.298552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.04
 ---- batch: 020 ----
mean loss: 361.89
 ---- batch: 030 ----
mean loss: 352.82
 ---- batch: 040 ----
mean loss: 361.05
train mean loss: 357.25
epoch train time: 0:00:00.195198
elapsed time: 0:00:27.328979
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:28:30.493871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.65
 ---- batch: 020 ----
mean loss: 355.76
 ---- batch: 030 ----
mean loss: 351.35
 ---- batch: 040 ----
mean loss: 351.98
train mean loss: 353.56
epoch train time: 0:00:00.195357
elapsed time: 0:00:27.524464
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:28:30.689364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.58
 ---- batch: 020 ----
mean loss: 354.15
 ---- batch: 030 ----
mean loss: 354.20
 ---- batch: 040 ----
mean loss: 348.83
train mean loss: 350.37
epoch train time: 0:00:00.202391
elapsed time: 0:00:27.726979
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:28:30.891871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.21
 ---- batch: 020 ----
mean loss: 349.72
 ---- batch: 030 ----
mean loss: 351.38
 ---- batch: 040 ----
mean loss: 335.28
train mean loss: 347.46
epoch train time: 0:00:00.199484
elapsed time: 0:00:27.926579
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:28:31.091470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.76
 ---- batch: 020 ----
mean loss: 346.47
 ---- batch: 030 ----
mean loss: 346.57
 ---- batch: 040 ----
mean loss: 342.08
train mean loss: 345.44
epoch train time: 0:00:00.197650
elapsed time: 0:00:28.124344
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:28:31.289261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.81
 ---- batch: 020 ----
mean loss: 327.48
 ---- batch: 030 ----
mean loss: 346.97
 ---- batch: 040 ----
mean loss: 345.72
train mean loss: 343.00
epoch train time: 0:00:00.193929
elapsed time: 0:00:28.318415
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:28:31.483364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.20
 ---- batch: 020 ----
mean loss: 342.12
 ---- batch: 030 ----
mean loss: 337.04
 ---- batch: 040 ----
mean loss: 341.81
train mean loss: 340.93
epoch train time: 0:00:00.197734
elapsed time: 0:00:28.516341
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:28:31.681287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.81
 ---- batch: 020 ----
mean loss: 327.63
 ---- batch: 030 ----
mean loss: 342.35
 ---- batch: 040 ----
mean loss: 344.97
train mean loss: 339.20
epoch train time: 0:00:00.204637
elapsed time: 0:00:28.721164
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:28:31.886086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.20
 ---- batch: 020 ----
mean loss: 336.12
 ---- batch: 030 ----
mean loss: 328.82
 ---- batch: 040 ----
mean loss: 333.92
train mean loss: 337.79
epoch train time: 0:00:00.209830
elapsed time: 0:00:28.931152
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:28:32.096048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.39
 ---- batch: 020 ----
mean loss: 339.43
 ---- batch: 030 ----
mean loss: 340.88
 ---- batch: 040 ----
mean loss: 329.58
train mean loss: 336.18
epoch train time: 0:00:00.199781
elapsed time: 0:00:29.131061
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:28:32.295956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.45
 ---- batch: 020 ----
mean loss: 338.87
 ---- batch: 030 ----
mean loss: 337.09
 ---- batch: 040 ----
mean loss: 332.07
train mean loss: 335.10
epoch train time: 0:00:00.201790
elapsed time: 0:00:29.332969
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:28:32.497861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.72
 ---- batch: 020 ----
mean loss: 336.90
 ---- batch: 030 ----
mean loss: 332.11
 ---- batch: 040 ----
mean loss: 336.29
train mean loss: 333.59
epoch train time: 0:00:00.202413
elapsed time: 0:00:29.535498
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:28:32.700423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.62
 ---- batch: 020 ----
mean loss: 324.08
 ---- batch: 030 ----
mean loss: 332.60
 ---- batch: 040 ----
mean loss: 333.29
train mean loss: 332.71
epoch train time: 0:00:00.206062
elapsed time: 0:00:29.741709
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:28:32.906600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.57
 ---- batch: 020 ----
mean loss: 331.29
 ---- batch: 030 ----
mean loss: 320.77
 ---- batch: 040 ----
mean loss: 338.37
train mean loss: 332.41
epoch train time: 0:00:00.201326
elapsed time: 0:00:29.943169
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:28:33.108108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.10
 ---- batch: 020 ----
mean loss: 332.44
 ---- batch: 030 ----
mean loss: 326.25
 ---- batch: 040 ----
mean loss: 339.88
train mean loss: 330.70
epoch train time: 0:00:00.209806
elapsed time: 0:00:30.153144
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:28:33.318041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.17
 ---- batch: 020 ----
mean loss: 330.13
 ---- batch: 030 ----
mean loss: 330.28
 ---- batch: 040 ----
mean loss: 331.90
train mean loss: 330.28
epoch train time: 0:00:00.208794
elapsed time: 0:00:30.362066
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:28:33.526961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.22
 ---- batch: 020 ----
mean loss: 334.49
 ---- batch: 030 ----
mean loss: 323.68
 ---- batch: 040 ----
mean loss: 330.48
train mean loss: 329.48
epoch train time: 0:00:00.208074
elapsed time: 0:00:30.570701
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:28:33.735623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.67
 ---- batch: 020 ----
mean loss: 329.26
 ---- batch: 030 ----
mean loss: 325.16
 ---- batch: 040 ----
mean loss: 330.84
train mean loss: 328.71
epoch train time: 0:00:00.206788
elapsed time: 0:00:30.777682
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:28:33.942575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.09
 ---- batch: 020 ----
mean loss: 325.89
 ---- batch: 030 ----
mean loss: 336.32
 ---- batch: 040 ----
mean loss: 319.30
train mean loss: 328.19
epoch train time: 0:00:00.201746
elapsed time: 0:00:30.979584
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:28:34.144480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.28
 ---- batch: 020 ----
mean loss: 329.41
 ---- batch: 030 ----
mean loss: 335.71
 ---- batch: 040 ----
mean loss: 316.42
train mean loss: 327.56
epoch train time: 0:00:00.197604
elapsed time: 0:00:31.177312
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:28:34.342206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.32
 ---- batch: 020 ----
mean loss: 323.07
 ---- batch: 030 ----
mean loss: 323.42
 ---- batch: 040 ----
mean loss: 329.31
train mean loss: 326.62
epoch train time: 0:00:00.198280
elapsed time: 0:00:31.375714
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:28:34.540608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.92
 ---- batch: 020 ----
mean loss: 334.74
 ---- batch: 030 ----
mean loss: 322.26
 ---- batch: 040 ----
mean loss: 330.81
train mean loss: 326.67
epoch train time: 0:00:00.198578
elapsed time: 0:00:31.574450
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:28:34.739342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.52
 ---- batch: 020 ----
mean loss: 329.63
 ---- batch: 030 ----
mean loss: 317.10
 ---- batch: 040 ----
mean loss: 332.84
train mean loss: 325.74
epoch train time: 0:00:00.206525
elapsed time: 0:00:31.781095
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:28:34.945989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.35
 ---- batch: 020 ----
mean loss: 326.99
 ---- batch: 030 ----
mean loss: 322.61
 ---- batch: 040 ----
mean loss: 327.82
train mean loss: 325.46
epoch train time: 0:00:00.200112
elapsed time: 0:00:31.981326
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:28:35.146219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.81
 ---- batch: 020 ----
mean loss: 324.97
 ---- batch: 030 ----
mean loss: 328.84
 ---- batch: 040 ----
mean loss: 331.36
train mean loss: 325.21
epoch train time: 0:00:00.203113
elapsed time: 0:00:32.184559
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:28:35.349453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.47
 ---- batch: 020 ----
mean loss: 328.55
 ---- batch: 030 ----
mean loss: 316.61
 ---- batch: 040 ----
mean loss: 326.99
train mean loss: 325.04
epoch train time: 0:00:00.196393
elapsed time: 0:00:32.381075
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:28:35.545970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.47
 ---- batch: 020 ----
mean loss: 315.91
 ---- batch: 030 ----
mean loss: 333.64
 ---- batch: 040 ----
mean loss: 324.68
train mean loss: 324.54
epoch train time: 0:00:00.204460
elapsed time: 0:00:32.585673
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:28:35.750568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.69
 ---- batch: 020 ----
mean loss: 327.09
 ---- batch: 030 ----
mean loss: 327.58
 ---- batch: 040 ----
mean loss: 320.08
train mean loss: 323.72
epoch train time: 0:00:00.218759
elapsed time: 0:00:32.804559
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:28:35.969467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.76
 ---- batch: 020 ----
mean loss: 318.84
 ---- batch: 030 ----
mean loss: 324.54
 ---- batch: 040 ----
mean loss: 324.96
train mean loss: 323.52
epoch train time: 0:00:00.204774
elapsed time: 0:00:33.009478
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:28:36.174388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.83
 ---- batch: 020 ----
mean loss: 321.41
 ---- batch: 030 ----
mean loss: 322.93
 ---- batch: 040 ----
mean loss: 326.15
train mean loss: 322.99
epoch train time: 0:00:00.202939
elapsed time: 0:00:33.212580
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:28:36.377475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.36
 ---- batch: 020 ----
mean loss: 327.53
 ---- batch: 030 ----
mean loss: 322.94
 ---- batch: 040 ----
mean loss: 323.78
train mean loss: 322.72
epoch train time: 0:00:00.206200
elapsed time: 0:00:33.418903
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:28:36.583796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.95
 ---- batch: 020 ----
mean loss: 328.33
 ---- batch: 030 ----
mean loss: 321.71
 ---- batch: 040 ----
mean loss: 321.06
train mean loss: 322.82
epoch train time: 0:00:00.205669
elapsed time: 0:00:33.624693
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:28:36.789585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.87
 ---- batch: 020 ----
mean loss: 321.59
 ---- batch: 030 ----
mean loss: 311.66
 ---- batch: 040 ----
mean loss: 329.05
train mean loss: 322.10
epoch train time: 0:00:00.209030
elapsed time: 0:00:33.833840
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:28:36.998732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.35
 ---- batch: 020 ----
mean loss: 327.70
 ---- batch: 030 ----
mean loss: 326.84
 ---- batch: 040 ----
mean loss: 316.68
train mean loss: 321.78
epoch train time: 0:00:00.209249
elapsed time: 0:00:34.043239
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:28:37.208134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.77
 ---- batch: 020 ----
mean loss: 323.90
 ---- batch: 030 ----
mean loss: 311.05
 ---- batch: 040 ----
mean loss: 322.43
train mean loss: 321.60
epoch train time: 0:00:00.207807
elapsed time: 0:00:34.251171
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:28:37.416066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.29
 ---- batch: 020 ----
mean loss: 320.57
 ---- batch: 030 ----
mean loss: 317.26
 ---- batch: 040 ----
mean loss: 329.09
train mean loss: 321.03
epoch train time: 0:00:00.205860
elapsed time: 0:00:34.457180
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:28:37.622076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.66
 ---- batch: 020 ----
mean loss: 321.17
 ---- batch: 030 ----
mean loss: 323.14
 ---- batch: 040 ----
mean loss: 321.23
train mean loss: 320.63
epoch train time: 0:00:00.205312
elapsed time: 0:00:34.662623
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:28:37.827526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.63
 ---- batch: 020 ----
mean loss: 321.01
 ---- batch: 030 ----
mean loss: 326.64
 ---- batch: 040 ----
mean loss: 315.97
train mean loss: 320.43
epoch train time: 0:00:00.207654
elapsed time: 0:00:34.870403
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:28:38.035294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.40
 ---- batch: 020 ----
mean loss: 322.21
 ---- batch: 030 ----
mean loss: 315.35
 ---- batch: 040 ----
mean loss: 327.13
train mean loss: 320.10
epoch train time: 0:00:00.201301
elapsed time: 0:00:35.071819
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:28:38.236708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.03
 ---- batch: 020 ----
mean loss: 322.59
 ---- batch: 030 ----
mean loss: 326.22
 ---- batch: 040 ----
mean loss: 316.66
train mean loss: 320.28
epoch train time: 0:00:00.199903
elapsed time: 0:00:35.271837
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:28:38.436729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.87
 ---- batch: 020 ----
mean loss: 321.52
 ---- batch: 030 ----
mean loss: 321.88
 ---- batch: 040 ----
mean loss: 313.43
train mean loss: 320.01
epoch train time: 0:00:00.198504
elapsed time: 0:00:35.470470
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:28:38.635379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.01
 ---- batch: 020 ----
mean loss: 328.08
 ---- batch: 030 ----
mean loss: 325.11
 ---- batch: 040 ----
mean loss: 307.88
train mean loss: 319.29
epoch train time: 0:00:00.205408
elapsed time: 0:00:35.676014
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:28:38.840908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.51
 ---- batch: 020 ----
mean loss: 325.94
 ---- batch: 030 ----
mean loss: 316.78
 ---- batch: 040 ----
mean loss: 316.27
train mean loss: 319.47
epoch train time: 0:00:00.198483
elapsed time: 0:00:35.874610
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:28:39.039499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.33
 ---- batch: 020 ----
mean loss: 320.72
 ---- batch: 030 ----
mean loss: 308.95
 ---- batch: 040 ----
mean loss: 317.73
train mean loss: 319.26
epoch train time: 0:00:00.197599
elapsed time: 0:00:36.072320
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:28:39.237226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.29
 ---- batch: 020 ----
mean loss: 324.15
 ---- batch: 030 ----
mean loss: 318.14
 ---- batch: 040 ----
mean loss: 315.76
train mean loss: 318.91
epoch train time: 0:00:00.198058
elapsed time: 0:00:36.270511
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:28:39.435406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.24
 ---- batch: 020 ----
mean loss: 322.93
 ---- batch: 030 ----
mean loss: 311.75
 ---- batch: 040 ----
mean loss: 316.65
train mean loss: 318.39
epoch train time: 0:00:00.203855
elapsed time: 0:00:36.474516
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:28:39.639402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.74
 ---- batch: 020 ----
mean loss: 331.71
 ---- batch: 030 ----
mean loss: 315.56
 ---- batch: 040 ----
mean loss: 310.21
train mean loss: 318.03
epoch train time: 0:00:00.204280
elapsed time: 0:00:36.678913
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:28:39.843809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.71
 ---- batch: 020 ----
mean loss: 324.83
 ---- batch: 030 ----
mean loss: 310.98
 ---- batch: 040 ----
mean loss: 312.05
train mean loss: 318.17
epoch train time: 0:00:00.206528
elapsed time: 0:00:36.885565
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:28:40.050458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.49
 ---- batch: 020 ----
mean loss: 324.63
 ---- batch: 030 ----
mean loss: 322.13
 ---- batch: 040 ----
mean loss: 312.23
train mean loss: 318.23
epoch train time: 0:00:00.203853
elapsed time: 0:00:37.089539
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:28:40.254433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.45
 ---- batch: 020 ----
mean loss: 303.80
 ---- batch: 030 ----
mean loss: 325.81
 ---- batch: 040 ----
mean loss: 326.02
train mean loss: 317.19
epoch train time: 0:00:00.203870
elapsed time: 0:00:37.293530
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:28:40.458425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.67
 ---- batch: 020 ----
mean loss: 310.40
 ---- batch: 030 ----
mean loss: 318.62
 ---- batch: 040 ----
mean loss: 318.75
train mean loss: 317.21
epoch train time: 0:00:00.205135
elapsed time: 0:00:37.498793
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:28:40.663687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.78
 ---- batch: 020 ----
mean loss: 312.54
 ---- batch: 030 ----
mean loss: 317.43
 ---- batch: 040 ----
mean loss: 321.60
train mean loss: 317.21
epoch train time: 0:00:00.207927
elapsed time: 0:00:37.706843
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:28:40.871738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.27
 ---- batch: 020 ----
mean loss: 325.23
 ---- batch: 030 ----
mean loss: 318.18
 ---- batch: 040 ----
mean loss: 312.98
train mean loss: 316.72
epoch train time: 0:00:00.208763
elapsed time: 0:00:37.915730
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:28:41.080638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.28
 ---- batch: 020 ----
mean loss: 317.13
 ---- batch: 030 ----
mean loss: 312.82
 ---- batch: 040 ----
mean loss: 312.52
train mean loss: 317.03
epoch train time: 0:00:00.204900
elapsed time: 0:00:38.120762
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:28:41.285686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.14
 ---- batch: 020 ----
mean loss: 313.87
 ---- batch: 030 ----
mean loss: 319.29
 ---- batch: 040 ----
mean loss: 315.32
train mean loss: 316.51
epoch train time: 0:00:00.198927
elapsed time: 0:00:38.319838
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:28:41.484731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.19
 ---- batch: 020 ----
mean loss: 316.88
 ---- batch: 030 ----
mean loss: 311.37
 ---- batch: 040 ----
mean loss: 312.63
train mean loss: 316.43
epoch train time: 0:00:00.199762
elapsed time: 0:00:38.519752
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:28:41.684661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.66
 ---- batch: 020 ----
mean loss: 324.93
 ---- batch: 030 ----
mean loss: 316.10
 ---- batch: 040 ----
mean loss: 312.38
train mean loss: 315.94
epoch train time: 0:00:00.208838
elapsed time: 0:00:38.728721
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:28:41.893610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.11
 ---- batch: 020 ----
mean loss: 309.04
 ---- batch: 030 ----
mean loss: 315.10
 ---- batch: 040 ----
mean loss: 315.59
train mean loss: 316.20
epoch train time: 0:00:00.201690
elapsed time: 0:00:38.930521
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:28:42.095412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.81
 ---- batch: 020 ----
mean loss: 315.48
 ---- batch: 030 ----
mean loss: 312.95
 ---- batch: 040 ----
mean loss: 319.64
train mean loss: 315.53
epoch train time: 0:00:00.198787
elapsed time: 0:00:39.129432
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:28:42.294324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.77
 ---- batch: 020 ----
mean loss: 316.29
 ---- batch: 030 ----
mean loss: 314.83
 ---- batch: 040 ----
mean loss: 313.39
train mean loss: 315.50
epoch train time: 0:00:00.203332
elapsed time: 0:00:39.332902
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:28:42.497793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.52
 ---- batch: 020 ----
mean loss: 323.28
 ---- batch: 030 ----
mean loss: 314.70
 ---- batch: 040 ----
mean loss: 311.58
train mean loss: 315.15
epoch train time: 0:00:00.196967
elapsed time: 0:00:39.529983
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:28:42.694875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.16
 ---- batch: 020 ----
mean loss: 311.55
 ---- batch: 030 ----
mean loss: 316.91
 ---- batch: 040 ----
mean loss: 324.15
train mean loss: 314.95
epoch train time: 0:00:00.200408
elapsed time: 0:00:39.730520
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:28:42.895411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.48
 ---- batch: 020 ----
mean loss: 310.47
 ---- batch: 030 ----
mean loss: 313.09
 ---- batch: 040 ----
mean loss: 315.95
train mean loss: 314.91
epoch train time: 0:00:00.199162
elapsed time: 0:00:39.929810
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:28:43.094699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.65
 ---- batch: 020 ----
mean loss: 312.23
 ---- batch: 030 ----
mean loss: 314.36
 ---- batch: 040 ----
mean loss: 312.53
train mean loss: 314.74
epoch train time: 0:00:00.205660
elapsed time: 0:00:40.135605
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:28:43.300499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.12
 ---- batch: 020 ----
mean loss: 319.65
 ---- batch: 030 ----
mean loss: 312.30
 ---- batch: 040 ----
mean loss: 329.67
train mean loss: 314.58
epoch train time: 0:00:00.207684
elapsed time: 0:00:40.343413
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:28:43.508340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.78
 ---- batch: 020 ----
mean loss: 309.01
 ---- batch: 030 ----
mean loss: 320.01
 ---- batch: 040 ----
mean loss: 308.31
train mean loss: 314.34
epoch train time: 0:00:00.207018
elapsed time: 0:00:40.550597
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:28:43.715484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.51
 ---- batch: 020 ----
mean loss: 307.35
 ---- batch: 030 ----
mean loss: 318.28
 ---- batch: 040 ----
mean loss: 315.89
train mean loss: 314.74
epoch train time: 0:00:00.217308
elapsed time: 0:00:40.768035
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:28:43.932930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.45
 ---- batch: 020 ----
mean loss: 310.22
 ---- batch: 030 ----
mean loss: 306.77
 ---- batch: 040 ----
mean loss: 316.14
train mean loss: 313.95
epoch train time: 0:00:00.211087
elapsed time: 0:00:40.979248
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:28:44.144143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.09
 ---- batch: 020 ----
mean loss: 315.02
 ---- batch: 030 ----
mean loss: 321.77
 ---- batch: 040 ----
mean loss: 316.30
train mean loss: 313.77
epoch train time: 0:00:00.209415
elapsed time: 0:00:41.188789
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:28:44.353685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.25
 ---- batch: 020 ----
mean loss: 308.12
 ---- batch: 030 ----
mean loss: 316.46
 ---- batch: 040 ----
mean loss: 309.31
train mean loss: 313.76
epoch train time: 0:00:00.211108
elapsed time: 0:00:41.400023
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:28:44.564916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.14
 ---- batch: 020 ----
mean loss: 313.63
 ---- batch: 030 ----
mean loss: 308.53
 ---- batch: 040 ----
mean loss: 316.12
train mean loss: 313.37
epoch train time: 0:00:00.210511
elapsed time: 0:00:41.610656
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:28:44.775550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.43
 ---- batch: 020 ----
mean loss: 312.83
 ---- batch: 030 ----
mean loss: 319.72
 ---- batch: 040 ----
mean loss: 317.95
train mean loss: 313.49
epoch train time: 0:00:00.204985
elapsed time: 0:00:41.815760
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:28:44.980666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.79
 ---- batch: 020 ----
mean loss: 312.03
 ---- batch: 030 ----
mean loss: 319.44
 ---- batch: 040 ----
mean loss: 305.45
train mean loss: 313.16
epoch train time: 0:00:00.200225
elapsed time: 0:00:42.016131
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:28:45.181020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.37
 ---- batch: 020 ----
mean loss: 316.66
 ---- batch: 030 ----
mean loss: 309.08
 ---- batch: 040 ----
mean loss: 314.43
train mean loss: 313.07
epoch train time: 0:00:00.197249
elapsed time: 0:00:42.213492
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:28:45.378384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.07
 ---- batch: 020 ----
mean loss: 313.68
 ---- batch: 030 ----
mean loss: 309.85
 ---- batch: 040 ----
mean loss: 317.15
train mean loss: 313.00
epoch train time: 0:00:00.197641
elapsed time: 0:00:42.411266
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:28:45.576158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.29
 ---- batch: 020 ----
mean loss: 309.43
 ---- batch: 030 ----
mean loss: 312.58
 ---- batch: 040 ----
mean loss: 314.48
train mean loss: 312.62
epoch train time: 0:00:00.200489
elapsed time: 0:00:42.611878
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:28:45.776771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.27
 ---- batch: 020 ----
mean loss: 318.64
 ---- batch: 030 ----
mean loss: 313.68
 ---- batch: 040 ----
mean loss: 308.13
train mean loss: 312.70
epoch train time: 0:00:00.212818
elapsed time: 0:00:42.824812
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:28:45.989717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.27
 ---- batch: 020 ----
mean loss: 309.23
 ---- batch: 030 ----
mean loss: 316.70
 ---- batch: 040 ----
mean loss: 306.96
train mean loss: 312.12
epoch train time: 0:00:00.194359
elapsed time: 0:00:43.019297
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:28:46.184187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.35
 ---- batch: 020 ----
mean loss: 307.80
 ---- batch: 030 ----
mean loss: 314.26
 ---- batch: 040 ----
mean loss: 309.93
train mean loss: 312.07
epoch train time: 0:00:00.195212
elapsed time: 0:00:43.214618
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:28:46.379507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.14
 ---- batch: 020 ----
mean loss: 312.66
 ---- batch: 030 ----
mean loss: 309.82
 ---- batch: 040 ----
mean loss: 318.69
train mean loss: 311.76
epoch train time: 0:00:00.191140
elapsed time: 0:00:43.405911
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:28:46.570835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.59
 ---- batch: 020 ----
mean loss: 310.67
 ---- batch: 030 ----
mean loss: 313.37
 ---- batch: 040 ----
mean loss: 315.38
train mean loss: 312.06
epoch train time: 0:00:00.195137
elapsed time: 0:00:43.601229
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:28:46.766120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.38
 ---- batch: 020 ----
mean loss: 314.77
 ---- batch: 030 ----
mean loss: 306.25
 ---- batch: 040 ----
mean loss: 306.34
train mean loss: 311.75
epoch train time: 0:00:00.207587
elapsed time: 0:00:43.808970
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:28:46.973874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.43
 ---- batch: 020 ----
mean loss: 307.51
 ---- batch: 030 ----
mean loss: 312.20
 ---- batch: 040 ----
mean loss: 316.38
train mean loss: 311.71
epoch train time: 0:00:00.206731
elapsed time: 0:00:44.015839
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:28:47.180742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.16
 ---- batch: 020 ----
mean loss: 311.86
 ---- batch: 030 ----
mean loss: 311.08
 ---- batch: 040 ----
mean loss: 309.73
train mean loss: 311.23
epoch train time: 0:00:00.210164
elapsed time: 0:00:44.226132
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:28:47.391026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.13
 ---- batch: 020 ----
mean loss: 319.21
 ---- batch: 030 ----
mean loss: 309.99
 ---- batch: 040 ----
mean loss: 306.10
train mean loss: 311.21
epoch train time: 0:00:00.207276
elapsed time: 0:00:44.433531
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:28:47.598425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.13
 ---- batch: 020 ----
mean loss: 316.98
 ---- batch: 030 ----
mean loss: 308.13
 ---- batch: 040 ----
mean loss: 314.13
train mean loss: 310.97
epoch train time: 0:00:00.210437
elapsed time: 0:00:44.644095
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:28:47.808989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.38
 ---- batch: 020 ----
mean loss: 311.58
 ---- batch: 030 ----
mean loss: 310.86
 ---- batch: 040 ----
mean loss: 308.73
train mean loss: 311.44
epoch train time: 0:00:00.205257
elapsed time: 0:00:44.849475
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:28:48.014371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.09
 ---- batch: 020 ----
mean loss: 297.55
 ---- batch: 030 ----
mean loss: 317.53
 ---- batch: 040 ----
mean loss: 311.93
train mean loss: 310.83
epoch train time: 0:00:00.204776
elapsed time: 0:00:45.054397
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:28:48.219291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.19
 ---- batch: 020 ----
mean loss: 308.43
 ---- batch: 030 ----
mean loss: 312.93
 ---- batch: 040 ----
mean loss: 309.93
train mean loss: 310.81
epoch train time: 0:00:00.205288
elapsed time: 0:00:45.259831
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:28:48.424741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.62
 ---- batch: 020 ----
mean loss: 306.98
 ---- batch: 030 ----
mean loss: 319.01
 ---- batch: 040 ----
mean loss: 307.96
train mean loss: 310.55
epoch train time: 0:00:00.201534
elapsed time: 0:00:45.461501
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:28:48.626393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.85
 ---- batch: 020 ----
mean loss: 301.74
 ---- batch: 030 ----
mean loss: 318.72
 ---- batch: 040 ----
mean loss: 310.46
train mean loss: 310.21
epoch train time: 0:00:00.198779
elapsed time: 0:00:45.660613
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:28:48.825516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.47
 ---- batch: 020 ----
mean loss: 307.37
 ---- batch: 030 ----
mean loss: 306.81
 ---- batch: 040 ----
mean loss: 309.45
train mean loss: 310.41
epoch train time: 0:00:00.197518
elapsed time: 0:00:45.858263
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:28:49.023151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.94
 ---- batch: 020 ----
mean loss: 310.45
 ---- batch: 030 ----
mean loss: 303.03
 ---- batch: 040 ----
mean loss: 314.52
train mean loss: 310.25
epoch train time: 0:00:00.195845
elapsed time: 0:00:46.054226
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:28:49.219115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.41
 ---- batch: 020 ----
mean loss: 313.28
 ---- batch: 030 ----
mean loss: 304.24
 ---- batch: 040 ----
mean loss: 318.04
train mean loss: 309.85
epoch train time: 0:00:00.196965
elapsed time: 0:00:46.251303
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:28:49.416194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.85
 ---- batch: 020 ----
mean loss: 306.07
 ---- batch: 030 ----
mean loss: 316.93
 ---- batch: 040 ----
mean loss: 309.28
train mean loss: 309.63
epoch train time: 0:00:00.193904
elapsed time: 0:00:46.445317
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:28:49.610206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.47
 ---- batch: 020 ----
mean loss: 319.34
 ---- batch: 030 ----
mean loss: 307.50
 ---- batch: 040 ----
mean loss: 308.70
train mean loss: 309.06
epoch train time: 0:00:00.209408
elapsed time: 0:00:46.654851
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:28:49.819762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.52
 ---- batch: 020 ----
mean loss: 312.95
 ---- batch: 030 ----
mean loss: 312.36
 ---- batch: 040 ----
mean loss: 297.56
train mean loss: 309.62
epoch train time: 0:00:00.217911
elapsed time: 0:00:46.872908
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:28:50.037801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.60
 ---- batch: 020 ----
mean loss: 313.01
 ---- batch: 030 ----
mean loss: 306.51
 ---- batch: 040 ----
mean loss: 307.62
train mean loss: 309.11
epoch train time: 0:00:00.208087
elapsed time: 0:00:47.081120
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:28:50.246015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.20
 ---- batch: 020 ----
mean loss: 298.27
 ---- batch: 030 ----
mean loss: 306.43
 ---- batch: 040 ----
mean loss: 324.27
train mean loss: 309.04
epoch train time: 0:00:00.205570
elapsed time: 0:00:47.286823
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:28:50.451713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.52
 ---- batch: 020 ----
mean loss: 311.44
 ---- batch: 030 ----
mean loss: 305.84
 ---- batch: 040 ----
mean loss: 306.69
train mean loss: 308.56
epoch train time: 0:00:00.207132
elapsed time: 0:00:47.494090
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:28:50.658999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.77
 ---- batch: 020 ----
mean loss: 307.81
 ---- batch: 030 ----
mean loss: 303.62
 ---- batch: 040 ----
mean loss: 306.95
train mean loss: 308.47
epoch train time: 0:00:00.209433
elapsed time: 0:00:47.703661
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:28:50.868571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.77
 ---- batch: 020 ----
mean loss: 300.37
 ---- batch: 030 ----
mean loss: 310.20
 ---- batch: 040 ----
mean loss: 309.31
train mean loss: 308.67
epoch train time: 0:00:00.214592
elapsed time: 0:00:47.918395
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:28:51.083290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.00
 ---- batch: 020 ----
mean loss: 308.80
 ---- batch: 030 ----
mean loss: 308.34
 ---- batch: 040 ----
mean loss: 311.02
train mean loss: 308.05
epoch train time: 0:00:00.206846
elapsed time: 0:00:48.125360
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:28:51.290261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.82
 ---- batch: 020 ----
mean loss: 310.35
 ---- batch: 030 ----
mean loss: 315.34
 ---- batch: 040 ----
mean loss: 308.11
train mean loss: 308.06
epoch train time: 0:00:00.204728
elapsed time: 0:00:48.330214
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:28:51.495106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.58
 ---- batch: 020 ----
mean loss: 308.01
 ---- batch: 030 ----
mean loss: 304.72
 ---- batch: 040 ----
mean loss: 313.14
train mean loss: 308.32
epoch train time: 0:00:00.203877
elapsed time: 0:00:48.534208
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:28:51.699099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.90
 ---- batch: 020 ----
mean loss: 301.24
 ---- batch: 030 ----
mean loss: 316.29
 ---- batch: 040 ----
mean loss: 309.30
train mean loss: 307.99
epoch train time: 0:00:00.205344
elapsed time: 0:00:48.739667
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:28:51.904558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.23
 ---- batch: 020 ----
mean loss: 312.97
 ---- batch: 030 ----
mean loss: 312.63
 ---- batch: 040 ----
mean loss: 304.76
train mean loss: 307.03
epoch train time: 0:00:00.205425
elapsed time: 0:00:48.945227
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:28:52.110135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.92
 ---- batch: 020 ----
mean loss: 307.09
 ---- batch: 030 ----
mean loss: 307.09
 ---- batch: 040 ----
mean loss: 312.04
train mean loss: 307.61
epoch train time: 0:00:00.203794
elapsed time: 0:00:49.149171
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:28:52.314067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.90
 ---- batch: 020 ----
mean loss: 310.69
 ---- batch: 030 ----
mean loss: 312.50
 ---- batch: 040 ----
mean loss: 297.81
train mean loss: 307.78
epoch train time: 0:00:00.199924
elapsed time: 0:00:49.349214
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:28:52.514105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.85
 ---- batch: 020 ----
mean loss: 305.02
 ---- batch: 030 ----
mean loss: 307.77
 ---- batch: 040 ----
mean loss: 302.70
train mean loss: 307.73
epoch train time: 0:00:00.196285
elapsed time: 0:00:49.545612
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:28:52.710503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.68
 ---- batch: 020 ----
mean loss: 303.16
 ---- batch: 030 ----
mean loss: 309.28
 ---- batch: 040 ----
mean loss: 309.09
train mean loss: 307.36
epoch train time: 0:00:00.200361
elapsed time: 0:00:49.746087
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:28:52.910978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.34
 ---- batch: 020 ----
mean loss: 306.72
 ---- batch: 030 ----
mean loss: 308.27
 ---- batch: 040 ----
mean loss: 313.30
train mean loss: 306.64
epoch train time: 0:00:00.197578
elapsed time: 0:00:49.943780
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:28:53.108675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.23
 ---- batch: 020 ----
mean loss: 308.14
 ---- batch: 030 ----
mean loss: 304.39
 ---- batch: 040 ----
mean loss: 306.10
train mean loss: 306.97
epoch train time: 0:00:00.209451
elapsed time: 0:00:50.153359
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:28:53.318261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.19
 ---- batch: 020 ----
mean loss: 309.42
 ---- batch: 030 ----
mean loss: 310.90
 ---- batch: 040 ----
mean loss: 295.64
train mean loss: 306.86
epoch train time: 0:00:00.195849
elapsed time: 0:00:50.349331
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:28:53.514220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.71
 ---- batch: 020 ----
mean loss: 296.51
 ---- batch: 030 ----
mean loss: 311.94
 ---- batch: 040 ----
mean loss: 317.24
train mean loss: 306.67
epoch train time: 0:00:00.196967
elapsed time: 0:00:50.546437
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:28:53.711340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.63
 ---- batch: 020 ----
mean loss: 312.05
 ---- batch: 030 ----
mean loss: 305.98
 ---- batch: 040 ----
mean loss: 300.54
train mean loss: 306.69
epoch train time: 0:00:00.205024
elapsed time: 0:00:50.751614
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:28:53.916511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.31
 ---- batch: 020 ----
mean loss: 296.90
 ---- batch: 030 ----
mean loss: 306.48
 ---- batch: 040 ----
mean loss: 311.63
train mean loss: 306.22
epoch train time: 0:00:00.196328
elapsed time: 0:00:50.948078
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:28:54.112968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.16
 ---- batch: 020 ----
mean loss: 309.43
 ---- batch: 030 ----
mean loss: 301.97
 ---- batch: 040 ----
mean loss: 303.93
train mean loss: 306.23
epoch train time: 0:00:00.194931
elapsed time: 0:00:51.143144
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:28:54.308039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.77
 ---- batch: 020 ----
mean loss: 298.58
 ---- batch: 030 ----
mean loss: 303.34
 ---- batch: 040 ----
mean loss: 312.77
train mean loss: 305.35
epoch train time: 0:00:00.204564
elapsed time: 0:00:51.347841
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:28:54.512736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.37
 ---- batch: 020 ----
mean loss: 302.82
 ---- batch: 030 ----
mean loss: 307.18
 ---- batch: 040 ----
mean loss: 310.27
train mean loss: 305.28
epoch train time: 0:00:00.203332
elapsed time: 0:00:51.551298
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:28:54.716193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.00
 ---- batch: 020 ----
mean loss: 307.18
 ---- batch: 030 ----
mean loss: 300.23
 ---- batch: 040 ----
mean loss: 307.04
train mean loss: 305.46
epoch train time: 0:00:00.209323
elapsed time: 0:00:51.760742
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:28:54.925637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.24
 ---- batch: 020 ----
mean loss: 311.56
 ---- batch: 030 ----
mean loss: 300.50
 ---- batch: 040 ----
mean loss: 303.31
train mean loss: 305.40
epoch train time: 0:00:00.204295
elapsed time: 0:00:51.965158
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:28:55.130050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.97
 ---- batch: 020 ----
mean loss: 311.12
 ---- batch: 030 ----
mean loss: 298.96
 ---- batch: 040 ----
mean loss: 304.49
train mean loss: 305.16
epoch train time: 0:00:00.204588
elapsed time: 0:00:52.169862
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:28:55.334763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.32
 ---- batch: 020 ----
mean loss: 299.88
 ---- batch: 030 ----
mean loss: 307.40
 ---- batch: 040 ----
mean loss: 310.37
train mean loss: 305.00
epoch train time: 0:00:00.203125
elapsed time: 0:00:52.373130
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:28:55.538024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.97
 ---- batch: 020 ----
mean loss: 303.01
 ---- batch: 030 ----
mean loss: 297.13
 ---- batch: 040 ----
mean loss: 315.70
train mean loss: 304.85
epoch train time: 0:00:00.202770
elapsed time: 0:00:52.576022
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:28:55.740930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.27
 ---- batch: 020 ----
mean loss: 309.65
 ---- batch: 030 ----
mean loss: 307.43
 ---- batch: 040 ----
mean loss: 300.94
train mean loss: 304.94
epoch train time: 0:00:00.204275
elapsed time: 0:00:52.780429
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:28:55.945321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.75
 ---- batch: 020 ----
mean loss: 298.47
 ---- batch: 030 ----
mean loss: 309.74
 ---- batch: 040 ----
mean loss: 308.33
train mean loss: 304.88
epoch train time: 0:00:00.203600
elapsed time: 0:00:52.984150
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:28:56.149043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.94
 ---- batch: 020 ----
mean loss: 304.44
 ---- batch: 030 ----
mean loss: 306.09
 ---- batch: 040 ----
mean loss: 299.35
train mean loss: 304.34
epoch train time: 0:00:00.198199
elapsed time: 0:00:53.182469
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:28:56.347364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.00
 ---- batch: 020 ----
mean loss: 314.85
 ---- batch: 030 ----
mean loss: 296.82
 ---- batch: 040 ----
mean loss: 309.17
train mean loss: 304.69
epoch train time: 0:00:00.195748
elapsed time: 0:00:53.378333
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:28:56.543222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.83
 ---- batch: 020 ----
mean loss: 308.51
 ---- batch: 030 ----
mean loss: 308.26
 ---- batch: 040 ----
mean loss: 293.35
train mean loss: 303.93
epoch train time: 0:00:00.197185
elapsed time: 0:00:53.575636
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:28:56.740531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.87
 ---- batch: 020 ----
mean loss: 302.62
 ---- batch: 030 ----
mean loss: 303.14
 ---- batch: 040 ----
mean loss: 308.24
train mean loss: 303.80
epoch train time: 0:00:00.200546
elapsed time: 0:00:53.776308
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:28:56.941196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.26
 ---- batch: 020 ----
mean loss: 301.42
 ---- batch: 030 ----
mean loss: 298.64
 ---- batch: 040 ----
mean loss: 312.64
train mean loss: 304.06
epoch train time: 0:00:00.198389
elapsed time: 0:00:53.974808
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:28:57.139699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.81
 ---- batch: 020 ----
mean loss: 309.03
 ---- batch: 030 ----
mean loss: 305.04
 ---- batch: 040 ----
mean loss: 309.14
train mean loss: 303.94
epoch train time: 0:00:00.199752
elapsed time: 0:00:54.174677
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:28:57.339571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.57
 ---- batch: 020 ----
mean loss: 315.88
 ---- batch: 030 ----
mean loss: 301.85
 ---- batch: 040 ----
mean loss: 299.07
train mean loss: 303.20
epoch train time: 0:00:00.200030
elapsed time: 0:00:54.374839
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:28:57.539760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.78
 ---- batch: 020 ----
mean loss: 303.66
 ---- batch: 030 ----
mean loss: 302.77
 ---- batch: 040 ----
mean loss: 304.90
train mean loss: 303.11
epoch train time: 0:00:00.197504
elapsed time: 0:00:54.572491
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:28:57.737383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.13
 ---- batch: 020 ----
mean loss: 302.48
 ---- batch: 030 ----
mean loss: 299.03
 ---- batch: 040 ----
mean loss: 306.14
train mean loss: 303.07
epoch train time: 0:00:00.201514
elapsed time: 0:00:54.774137
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:28:57.939066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.70
 ---- batch: 020 ----
mean loss: 304.43
 ---- batch: 030 ----
mean loss: 308.12
 ---- batch: 040 ----
mean loss: 302.05
train mean loss: 303.00
epoch train time: 0:00:00.208289
elapsed time: 0:00:54.982583
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:28:58.147476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.29
 ---- batch: 020 ----
mean loss: 300.17
 ---- batch: 030 ----
mean loss: 298.98
 ---- batch: 040 ----
mean loss: 300.58
train mean loss: 302.64
epoch train time: 0:00:00.207061
elapsed time: 0:00:55.189777
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:28:58.354684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.62
 ---- batch: 020 ----
mean loss: 313.26
 ---- batch: 030 ----
mean loss: 297.32
 ---- batch: 040 ----
mean loss: 307.18
train mean loss: 302.37
epoch train time: 0:00:00.208668
elapsed time: 0:00:55.398580
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:28:58.563501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.38
 ---- batch: 020 ----
mean loss: 299.14
 ---- batch: 030 ----
mean loss: 300.04
 ---- batch: 040 ----
mean loss: 305.71
train mean loss: 302.55
epoch train time: 0:00:00.207089
elapsed time: 0:00:55.605816
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:28:58.770709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.49
 ---- batch: 020 ----
mean loss: 305.19
 ---- batch: 030 ----
mean loss: 300.37
 ---- batch: 040 ----
mean loss: 303.32
train mean loss: 302.47
epoch train time: 0:00:00.208502
elapsed time: 0:00:55.814476
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:28:58.979364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 294.97
 ---- batch: 020 ----
mean loss: 307.70
 ---- batch: 030 ----
mean loss: 301.07
 ---- batch: 040 ----
mean loss: 303.89
train mean loss: 302.06
epoch train time: 0:00:00.207204
elapsed time: 0:00:56.021795
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:28:59.186688
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.62
 ---- batch: 020 ----
mean loss: 310.92
 ---- batch: 030 ----
mean loss: 306.10
 ---- batch: 040 ----
mean loss: 299.54
train mean loss: 302.26
epoch train time: 0:00:00.208204
elapsed time: 0:00:56.230120
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:28:59.395015
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.28
 ---- batch: 020 ----
mean loss: 303.68
 ---- batch: 030 ----
mean loss: 295.34
 ---- batch: 040 ----
mean loss: 310.63
train mean loss: 302.47
epoch train time: 0:00:00.208922
elapsed time: 0:00:56.439163
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:28:59.604055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.96
 ---- batch: 020 ----
mean loss: 305.28
 ---- batch: 030 ----
mean loss: 304.00
 ---- batch: 040 ----
mean loss: 294.73
train mean loss: 302.15
epoch train time: 0:00:00.206403
elapsed time: 0:00:56.645711
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:28:59.810610
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.50
 ---- batch: 020 ----
mean loss: 300.18
 ---- batch: 030 ----
mean loss: 308.86
 ---- batch: 040 ----
mean loss: 300.59
train mean loss: 302.03
epoch train time: 0:00:00.207111
elapsed time: 0:00:56.852946
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:29:00.017840
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.41
 ---- batch: 020 ----
mean loss: 299.07
 ---- batch: 030 ----
mean loss: 307.47
 ---- batch: 040 ----
mean loss: 297.37
train mean loss: 302.50
epoch train time: 0:00:00.201235
elapsed time: 0:00:57.054301
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:29:00.219194
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.70
 ---- batch: 020 ----
mean loss: 309.97
 ---- batch: 030 ----
mean loss: 297.86
 ---- batch: 040 ----
mean loss: 303.33
train mean loss: 302.05
epoch train time: 0:00:00.201254
elapsed time: 0:00:57.255675
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:29:00.420585
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.34
 ---- batch: 020 ----
mean loss: 307.41
 ---- batch: 030 ----
mean loss: 299.22
 ---- batch: 040 ----
mean loss: 305.84
train mean loss: 302.36
epoch train time: 0:00:00.202915
elapsed time: 0:00:57.458734
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:29:00.623630
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.44
 ---- batch: 020 ----
mean loss: 300.24
 ---- batch: 030 ----
mean loss: 297.10
 ---- batch: 040 ----
mean loss: 310.66
train mean loss: 302.19
epoch train time: 0:00:00.204546
elapsed time: 0:00:57.663411
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:29:00.828307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.15
 ---- batch: 020 ----
mean loss: 301.66
 ---- batch: 030 ----
mean loss: 302.90
 ---- batch: 040 ----
mean loss: 306.99
train mean loss: 302.04
epoch train time: 0:00:00.204705
elapsed time: 0:00:57.868239
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:29:01.033131
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.19
 ---- batch: 020 ----
mean loss: 299.89
 ---- batch: 030 ----
mean loss: 299.71
 ---- batch: 040 ----
mean loss: 306.78
train mean loss: 302.51
epoch train time: 0:00:00.202595
elapsed time: 0:00:58.070970
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:29:01.235871
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.59
 ---- batch: 020 ----
mean loss: 304.02
 ---- batch: 030 ----
mean loss: 301.18
 ---- batch: 040 ----
mean loss: 301.87
train mean loss: 302.02
epoch train time: 0:00:00.199120
elapsed time: 0:00:58.270212
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:29:01.435103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.92
 ---- batch: 020 ----
mean loss: 296.12
 ---- batch: 030 ----
mean loss: 302.32
 ---- batch: 040 ----
mean loss: 308.06
train mean loss: 302.20
epoch train time: 0:00:00.198977
elapsed time: 0:00:58.469303
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:29:01.634193
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.95
 ---- batch: 020 ----
mean loss: 309.46
 ---- batch: 030 ----
mean loss: 295.65
 ---- batch: 040 ----
mean loss: 299.76
train mean loss: 302.21
epoch train time: 0:00:00.203305
elapsed time: 0:00:58.672723
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:29:01.837614
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.45
 ---- batch: 020 ----
mean loss: 297.39
 ---- batch: 030 ----
mean loss: 303.67
 ---- batch: 040 ----
mean loss: 299.53
train mean loss: 302.06
epoch train time: 0:00:00.205935
elapsed time: 0:00:58.878778
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:29:02.043672
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.86
 ---- batch: 020 ----
mean loss: 302.69
 ---- batch: 030 ----
mean loss: 295.65
 ---- batch: 040 ----
mean loss: 306.02
train mean loss: 302.35
epoch train time: 0:00:00.202733
elapsed time: 0:00:59.081633
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:29:02.246528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.73
 ---- batch: 020 ----
mean loss: 308.58
 ---- batch: 030 ----
mean loss: 300.91
 ---- batch: 040 ----
mean loss: 295.69
train mean loss: 301.95
epoch train time: 0:00:00.204447
elapsed time: 0:00:59.286199
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:29:02.451092
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.33
 ---- batch: 020 ----
mean loss: 297.60
 ---- batch: 030 ----
mean loss: 296.26
 ---- batch: 040 ----
mean loss: 302.40
train mean loss: 302.14
epoch train time: 0:00:00.204502
elapsed time: 0:00:59.490823
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:29:02.655792
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.02
 ---- batch: 020 ----
mean loss: 302.78
 ---- batch: 030 ----
mean loss: 301.60
 ---- batch: 040 ----
mean loss: 304.83
train mean loss: 302.00
epoch train time: 0:00:00.212209
elapsed time: 0:00:59.703245
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:29:02.868171
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.47
 ---- batch: 020 ----
mean loss: 308.20
 ---- batch: 030 ----
mean loss: 296.80
 ---- batch: 040 ----
mean loss: 288.68
train mean loss: 302.59
epoch train time: 0:00:00.208202
elapsed time: 0:00:59.911636
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:29:03.076530
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.62
 ---- batch: 020 ----
mean loss: 301.16
 ---- batch: 030 ----
mean loss: 307.13
 ---- batch: 040 ----
mean loss: 301.12
train mean loss: 302.29
epoch train time: 0:00:00.206794
elapsed time: 0:01:00.118562
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:29:03.283466
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.10
 ---- batch: 020 ----
mean loss: 305.57
 ---- batch: 030 ----
mean loss: 294.90
 ---- batch: 040 ----
mean loss: 302.09
train mean loss: 301.99
epoch train time: 0:00:00.215466
elapsed time: 0:01:00.334171
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:29:03.499063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.39
 ---- batch: 020 ----
mean loss: 303.68
 ---- batch: 030 ----
mean loss: 302.78
 ---- batch: 040 ----
mean loss: 299.34
train mean loss: 302.13
epoch train time: 0:00:00.203560
elapsed time: 0:01:00.537842
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:29:03.702731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.54
 ---- batch: 020 ----
mean loss: 290.52
 ---- batch: 030 ----
mean loss: 302.86
 ---- batch: 040 ----
mean loss: 306.78
train mean loss: 302.03
epoch train time: 0:00:00.207083
elapsed time: 0:01:00.745035
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:29:03.909924
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.42
 ---- batch: 020 ----
mean loss: 297.47
 ---- batch: 030 ----
mean loss: 306.43
 ---- batch: 040 ----
mean loss: 299.85
train mean loss: 301.74
epoch train time: 0:00:00.199708
elapsed time: 0:01:00.944870
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:29:04.109761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.93
 ---- batch: 020 ----
mean loss: 302.69
 ---- batch: 030 ----
mean loss: 299.15
 ---- batch: 040 ----
mean loss: 298.72
train mean loss: 301.78
epoch train time: 0:00:00.198116
elapsed time: 0:01:01.143099
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:29:04.307989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.05
 ---- batch: 020 ----
mean loss: 297.02
 ---- batch: 030 ----
mean loss: 304.56
 ---- batch: 040 ----
mean loss: 303.55
train mean loss: 301.81
epoch train time: 0:00:00.198669
elapsed time: 0:01:01.341887
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:29:04.506791
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.69
 ---- batch: 020 ----
mean loss: 308.35
 ---- batch: 030 ----
mean loss: 294.56
 ---- batch: 040 ----
mean loss: 302.55
train mean loss: 301.77
epoch train time: 0:00:00.201313
elapsed time: 0:01:01.543346
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:29:04.708239
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.10
 ---- batch: 020 ----
mean loss: 304.99
 ---- batch: 030 ----
mean loss: 305.23
 ---- batch: 040 ----
mean loss: 307.24
train mean loss: 301.36
epoch train time: 0:00:00.205475
elapsed time: 0:01:01.748949
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:29:04.913842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.25
 ---- batch: 020 ----
mean loss: 297.61
 ---- batch: 030 ----
mean loss: 307.52
 ---- batch: 040 ----
mean loss: 303.23
train mean loss: 301.75
epoch train time: 0:00:00.202399
elapsed time: 0:01:01.951485
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:29:05.116379
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.53
 ---- batch: 020 ----
mean loss: 302.67
 ---- batch: 030 ----
mean loss: 306.21
 ---- batch: 040 ----
mean loss: 304.84
train mean loss: 301.89
epoch train time: 0:00:00.203083
elapsed time: 0:01:02.154737
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:29:05.319646
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.45
 ---- batch: 020 ----
mean loss: 308.12
 ---- batch: 030 ----
mean loss: 301.04
 ---- batch: 040 ----
mean loss: 299.97
train mean loss: 301.84
epoch train time: 0:00:00.198910
elapsed time: 0:01:02.353792
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:29:05.518690
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.94
 ---- batch: 020 ----
mean loss: 307.14
 ---- batch: 030 ----
mean loss: 305.57
 ---- batch: 040 ----
mean loss: 294.35
train mean loss: 301.74
epoch train time: 0:00:00.206654
elapsed time: 0:01:02.560572
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:29:05.725466
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.06
 ---- batch: 020 ----
mean loss: 303.81
 ---- batch: 030 ----
mean loss: 301.10
 ---- batch: 040 ----
mean loss: 301.09
train mean loss: 301.44
epoch train time: 0:00:00.207004
elapsed time: 0:01:02.767704
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:29:05.932598
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 307.47
 ---- batch: 020 ----
mean loss: 294.11
 ---- batch: 030 ----
mean loss: 304.87
 ---- batch: 040 ----
mean loss: 295.39
train mean loss: 301.84
epoch train time: 0:00:00.204235
elapsed time: 0:01:02.972065
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:29:06.136959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 310.82
 ---- batch: 020 ----
mean loss: 297.14
 ---- batch: 030 ----
mean loss: 303.03
 ---- batch: 040 ----
mean loss: 299.49
train mean loss: 301.47
epoch train time: 0:00:00.203298
elapsed time: 0:01:03.175487
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:29:06.340414
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.33
 ---- batch: 020 ----
mean loss: 298.15
 ---- batch: 030 ----
mean loss: 299.16
 ---- batch: 040 ----
mean loss: 296.74
train mean loss: 301.75
epoch train time: 0:00:00.202687
elapsed time: 0:01:03.378327
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:29:06.543219
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.13
 ---- batch: 020 ----
mean loss: 302.35
 ---- batch: 030 ----
mean loss: 298.53
 ---- batch: 040 ----
mean loss: 308.25
train mean loss: 301.45
epoch train time: 0:00:00.204404
elapsed time: 0:01:03.582852
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:29:06.747746
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.69
 ---- batch: 020 ----
mean loss: 305.65
 ---- batch: 030 ----
mean loss: 303.07
 ---- batch: 040 ----
mean loss: 303.74
train mean loss: 301.42
epoch train time: 0:00:00.217172
elapsed time: 0:01:03.800148
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:29:06.965059
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.28
 ---- batch: 020 ----
mean loss: 304.55
 ---- batch: 030 ----
mean loss: 305.45
 ---- batch: 040 ----
mean loss: 299.75
train mean loss: 301.85
epoch train time: 0:00:00.209670
elapsed time: 0:01:04.009958
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:29:07.174853
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.13
 ---- batch: 020 ----
mean loss: 304.86
 ---- batch: 030 ----
mean loss: 305.17
 ---- batch: 040 ----
mean loss: 296.35
train mean loss: 301.68
epoch train time: 0:00:00.210104
elapsed time: 0:01:04.220197
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:29:07.385089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.99
 ---- batch: 020 ----
mean loss: 300.85
 ---- batch: 030 ----
mean loss: 299.50
 ---- batch: 040 ----
mean loss: 305.07
train mean loss: 301.47
epoch train time: 0:00:00.206653
elapsed time: 0:01:04.426966
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:29:07.591859
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.03
 ---- batch: 020 ----
mean loss: 303.50
 ---- batch: 030 ----
mean loss: 305.45
 ---- batch: 040 ----
mean loss: 303.65
train mean loss: 301.43
epoch train time: 0:00:00.209465
elapsed time: 0:01:04.636549
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:29:07.801456
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.43
 ---- batch: 020 ----
mean loss: 301.44
 ---- batch: 030 ----
mean loss: 300.65
 ---- batch: 040 ----
mean loss: 302.65
train mean loss: 301.36
epoch train time: 0:00:00.206320
elapsed time: 0:01:04.843020
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:29:08.007936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.92
 ---- batch: 020 ----
mean loss: 301.73
 ---- batch: 030 ----
mean loss: 307.16
 ---- batch: 040 ----
mean loss: 295.15
train mean loss: 301.87
epoch train time: 0:00:00.204538
elapsed time: 0:01:05.047703
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:29:08.212598
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.80
 ---- batch: 020 ----
mean loss: 307.04
 ---- batch: 030 ----
mean loss: 300.21
 ---- batch: 040 ----
mean loss: 297.12
train mean loss: 302.08
epoch train time: 0:00:00.203054
elapsed time: 0:01:05.250903
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:29:08.415798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.82
 ---- batch: 020 ----
mean loss: 295.45
 ---- batch: 030 ----
mean loss: 310.25
 ---- batch: 040 ----
mean loss: 295.02
train mean loss: 301.43
epoch train time: 0:00:00.202571
elapsed time: 0:01:05.453615
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:29:08.618513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.49
 ---- batch: 020 ----
mean loss: 308.00
 ---- batch: 030 ----
mean loss: 299.81
 ---- batch: 040 ----
mean loss: 296.15
train mean loss: 301.41
epoch train time: 0:00:00.209221
elapsed time: 0:01:05.665087
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_3/checkpoint.pth.tar
**** end time: 2019-09-20 23:29:08.829954 ****
