Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_8', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18390
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistDense3...
Done.
**** start time: 2019-09-27 02:29:08.207964 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
            Linear-2                  [-1, 100]          36,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 56,100
Trainable params: 56,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:29:08.211247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4974.49
 ---- batch: 020 ----
mean loss: 4859.38
 ---- batch: 030 ----
mean loss: 4763.30
 ---- batch: 040 ----
mean loss: 4670.72
 ---- batch: 050 ----
mean loss: 4613.58
 ---- batch: 060 ----
mean loss: 4515.23
 ---- batch: 070 ----
mean loss: 4487.96
 ---- batch: 080 ----
mean loss: 4421.68
 ---- batch: 090 ----
mean loss: 4358.22
 ---- batch: 100 ----
mean loss: 4332.16
 ---- batch: 110 ----
mean loss: 4298.56
train mean loss: 4563.17
epoch train time: 0:00:32.369197
elapsed time: 0:00:32.374703
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:29:40.582706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4182.34
 ---- batch: 020 ----
mean loss: 4092.75
 ---- batch: 030 ----
mean loss: 4067.59
 ---- batch: 040 ----
mean loss: 4001.69
 ---- batch: 050 ----
mean loss: 3981.30
 ---- batch: 060 ----
mean loss: 3888.89
 ---- batch: 070 ----
mean loss: 3792.93
 ---- batch: 080 ----
mean loss: 3780.07
 ---- batch: 090 ----
mean loss: 3689.90
 ---- batch: 100 ----
mean loss: 3598.94
 ---- batch: 110 ----
mean loss: 3515.83
train mean loss: 3864.36
epoch train time: 0:00:00.547623
elapsed time: 0:00:32.922494
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:29:41.130541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3497.40
 ---- batch: 020 ----
mean loss: 3440.99
 ---- batch: 030 ----
mean loss: 3414.98
 ---- batch: 040 ----
mean loss: 3356.21
 ---- batch: 050 ----
mean loss: 3294.58
 ---- batch: 060 ----
mean loss: 3230.00
 ---- batch: 070 ----
mean loss: 3201.02
 ---- batch: 080 ----
mean loss: 3113.46
 ---- batch: 090 ----
mean loss: 3050.20
 ---- batch: 100 ----
mean loss: 3045.44
 ---- batch: 110 ----
mean loss: 2915.40
train mean loss: 3226.42
epoch train time: 0:00:00.540617
elapsed time: 0:00:33.463276
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:29:41.671285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2899.99
 ---- batch: 020 ----
mean loss: 2813.11
 ---- batch: 030 ----
mean loss: 2844.21
 ---- batch: 040 ----
mean loss: 2806.39
 ---- batch: 050 ----
mean loss: 2723.38
 ---- batch: 060 ----
mean loss: 2714.98
 ---- batch: 070 ----
mean loss: 2667.46
 ---- batch: 080 ----
mean loss: 2667.40
 ---- batch: 090 ----
mean loss: 2569.06
 ---- batch: 100 ----
mean loss: 2550.99
 ---- batch: 110 ----
mean loss: 2508.75
train mean loss: 2699.97
epoch train time: 0:00:00.554413
elapsed time: 0:00:34.017825
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:29:42.225840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2494.40
 ---- batch: 020 ----
mean loss: 2427.87
 ---- batch: 030 ----
mean loss: 2374.54
 ---- batch: 040 ----
mean loss: 2334.08
 ---- batch: 050 ----
mean loss: 2314.03
 ---- batch: 060 ----
mean loss: 2267.17
 ---- batch: 070 ----
mean loss: 2214.07
 ---- batch: 080 ----
mean loss: 2178.44
 ---- batch: 090 ----
mean loss: 2171.97
 ---- batch: 100 ----
mean loss: 2155.45
 ---- batch: 110 ----
mean loss: 2134.59
train mean loss: 2273.37
epoch train time: 0:00:00.553148
elapsed time: 0:00:34.571111
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:29:42.779120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2091.67
 ---- batch: 020 ----
mean loss: 2056.81
 ---- batch: 030 ----
mean loss: 2020.74
 ---- batch: 040 ----
mean loss: 2027.66
 ---- batch: 050 ----
mean loss: 1946.61
 ---- batch: 060 ----
mean loss: 1919.78
 ---- batch: 070 ----
mean loss: 1888.08
 ---- batch: 080 ----
mean loss: 1857.94
 ---- batch: 090 ----
mean loss: 1850.90
 ---- batch: 100 ----
mean loss: 1846.39
 ---- batch: 110 ----
mean loss: 1806.33
train mean loss: 1934.23
epoch train time: 0:00:00.565025
elapsed time: 0:00:35.136266
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:29:43.344312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1784.95
 ---- batch: 020 ----
mean loss: 1753.98
 ---- batch: 030 ----
mean loss: 1720.07
 ---- batch: 040 ----
mean loss: 1708.54
 ---- batch: 050 ----
mean loss: 1725.09
 ---- batch: 060 ----
mean loss: 1630.32
 ---- batch: 070 ----
mean loss: 1623.06
 ---- batch: 080 ----
mean loss: 1596.74
 ---- batch: 090 ----
mean loss: 1591.95
 ---- batch: 100 ----
mean loss: 1551.58
 ---- batch: 110 ----
mean loss: 1571.36
train mean loss: 1656.35
epoch train time: 0:00:00.552093
elapsed time: 0:00:35.688543
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:29:43.896554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1503.12
 ---- batch: 020 ----
mean loss: 1514.54
 ---- batch: 030 ----
mean loss: 1465.98
 ---- batch: 040 ----
mean loss: 1447.71
 ---- batch: 050 ----
mean loss: 1445.76
 ---- batch: 060 ----
mean loss: 1428.10
 ---- batch: 070 ----
mean loss: 1409.41
 ---- batch: 080 ----
mean loss: 1394.17
 ---- batch: 090 ----
mean loss: 1396.62
 ---- batch: 100 ----
mean loss: 1379.25
 ---- batch: 110 ----
mean loss: 1318.55
train mean loss: 1425.62
epoch train time: 0:00:00.544857
elapsed time: 0:00:36.233531
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:29:44.441542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1307.16
 ---- batch: 020 ----
mean loss: 1314.04
 ---- batch: 030 ----
mean loss: 1281.27
 ---- batch: 040 ----
mean loss: 1260.45
 ---- batch: 050 ----
mean loss: 1262.46
 ---- batch: 060 ----
mean loss: 1260.46
 ---- batch: 070 ----
mean loss: 1242.28
 ---- batch: 080 ----
mean loss: 1224.93
 ---- batch: 090 ----
mean loss: 1208.96
 ---- batch: 100 ----
mean loss: 1207.00
 ---- batch: 110 ----
mean loss: 1199.95
train mean loss: 1249.04
epoch train time: 0:00:00.570488
elapsed time: 0:00:36.804199
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:29:45.012246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1174.62
 ---- batch: 020 ----
mean loss: 1170.70
 ---- batch: 030 ----
mean loss: 1143.13
 ---- batch: 040 ----
mean loss: 1130.47
 ---- batch: 050 ----
mean loss: 1126.37
 ---- batch: 060 ----
mean loss: 1125.38
 ---- batch: 070 ----
mean loss: 1120.11
 ---- batch: 080 ----
mean loss: 1104.04
 ---- batch: 090 ----
mean loss: 1099.30
 ---- batch: 100 ----
mean loss: 1073.66
 ---- batch: 110 ----
mean loss: 1088.33
train mean loss: 1122.17
epoch train time: 0:00:00.536687
elapsed time: 0:00:37.341055
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:29:45.549077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1063.99
 ---- batch: 020 ----
mean loss: 1055.28
 ---- batch: 030 ----
mean loss: 1054.98
 ---- batch: 040 ----
mean loss: 1048.25
 ---- batch: 050 ----
mean loss: 1039.60
 ---- batch: 060 ----
mean loss: 1031.35
 ---- batch: 070 ----
mean loss: 1012.49
 ---- batch: 080 ----
mean loss: 1016.71
 ---- batch: 090 ----
mean loss: 1018.58
 ---- batch: 100 ----
mean loss: 1015.61
 ---- batch: 110 ----
mean loss: 994.34
train mean loss: 1031.07
epoch train time: 0:00:00.541675
elapsed time: 0:00:37.882871
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:29:46.090879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1000.30
 ---- batch: 020 ----
mean loss: 985.25
 ---- batch: 030 ----
mean loss: 979.42
 ---- batch: 040 ----
mean loss: 967.76
 ---- batch: 050 ----
mean loss: 955.69
 ---- batch: 060 ----
mean loss: 962.83
 ---- batch: 070 ----
mean loss: 964.99
 ---- batch: 080 ----
mean loss: 953.43
 ---- batch: 090 ----
mean loss: 953.96
 ---- batch: 100 ----
mean loss: 948.28
 ---- batch: 110 ----
mean loss: 934.14
train mean loss: 964.05
epoch train time: 0:00:00.539363
elapsed time: 0:00:38.422367
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:29:46.630378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.43
 ---- batch: 020 ----
mean loss: 938.29
 ---- batch: 030 ----
mean loss: 930.44
 ---- batch: 040 ----
mean loss: 920.62
 ---- batch: 050 ----
mean loss: 912.16
 ---- batch: 060 ----
mean loss: 896.47
 ---- batch: 070 ----
mean loss: 921.12
 ---- batch: 080 ----
mean loss: 895.83
 ---- batch: 090 ----
mean loss: 901.54
 ---- batch: 100 ----
mean loss: 912.16
 ---- batch: 110 ----
mean loss: 887.33
train mean loss: 914.24
epoch train time: 0:00:00.541748
elapsed time: 0:00:38.964246
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:29:47.172270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.25
 ---- batch: 020 ----
mean loss: 886.67
 ---- batch: 030 ----
mean loss: 887.44
 ---- batch: 040 ----
mean loss: 880.03
 ---- batch: 050 ----
mean loss: 881.79
 ---- batch: 060 ----
mean loss: 892.51
 ---- batch: 070 ----
mean loss: 884.86
 ---- batch: 080 ----
mean loss: 884.72
 ---- batch: 090 ----
mean loss: 873.30
 ---- batch: 100 ----
mean loss: 881.51
 ---- batch: 110 ----
mean loss: 881.14
train mean loss: 883.88
epoch train time: 0:00:00.537759
elapsed time: 0:00:39.502153
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:29:47.710165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 868.23
 ---- batch: 030 ----
mean loss: 884.20
 ---- batch: 040 ----
mean loss: 881.48
 ---- batch: 050 ----
mean loss: 866.74
 ---- batch: 060 ----
mean loss: 859.08
 ---- batch: 070 ----
mean loss: 858.26
 ---- batch: 080 ----
mean loss: 853.88
 ---- batch: 090 ----
mean loss: 862.55
 ---- batch: 100 ----
mean loss: 854.39
 ---- batch: 110 ----
mean loss: 873.64
train mean loss: 866.04
epoch train time: 0:00:00.550738
elapsed time: 0:00:40.053070
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:29:48.261098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.99
 ---- batch: 020 ----
mean loss: 863.50
 ---- batch: 030 ----
mean loss: 854.55
 ---- batch: 040 ----
mean loss: 843.23
 ---- batch: 050 ----
mean loss: 853.42
 ---- batch: 060 ----
mean loss: 859.02
 ---- batch: 070 ----
mean loss: 868.88
 ---- batch: 080 ----
mean loss: 844.93
 ---- batch: 090 ----
mean loss: 848.38
 ---- batch: 100 ----
mean loss: 861.67
 ---- batch: 110 ----
mean loss: 842.14
train mean loss: 855.89
epoch train time: 0:00:00.550437
elapsed time: 0:00:40.603656
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:29:48.811666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.68
 ---- batch: 020 ----
mean loss: 825.13
 ---- batch: 030 ----
mean loss: 850.20
 ---- batch: 040 ----
mean loss: 868.41
 ---- batch: 050 ----
mean loss: 868.31
 ---- batch: 060 ----
mean loss: 864.40
 ---- batch: 070 ----
mean loss: 858.70
 ---- batch: 080 ----
mean loss: 850.19
 ---- batch: 090 ----
mean loss: 836.47
 ---- batch: 100 ----
mean loss: 840.90
 ---- batch: 110 ----
mean loss: 842.12
train mean loss: 850.16
epoch train time: 0:00:00.546532
elapsed time: 0:00:41.150322
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:29:49.358333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.22
 ---- batch: 020 ----
mean loss: 851.61
 ---- batch: 030 ----
mean loss: 835.43
 ---- batch: 040 ----
mean loss: 848.06
 ---- batch: 050 ----
mean loss: 861.82
 ---- batch: 060 ----
mean loss: 827.48
 ---- batch: 070 ----
mean loss: 860.64
 ---- batch: 080 ----
mean loss: 843.11
 ---- batch: 090 ----
mean loss: 843.53
 ---- batch: 100 ----
mean loss: 861.60
 ---- batch: 110 ----
mean loss: 857.23
train mean loss: 846.92
epoch train time: 0:00:00.547058
elapsed time: 0:00:41.697533
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:29:49.905550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.56
 ---- batch: 020 ----
mean loss: 856.89
 ---- batch: 030 ----
mean loss: 844.15
 ---- batch: 040 ----
mean loss: 824.43
 ---- batch: 050 ----
mean loss: 836.42
 ---- batch: 060 ----
mean loss: 852.46
 ---- batch: 070 ----
mean loss: 842.14
 ---- batch: 080 ----
mean loss: 842.98
 ---- batch: 090 ----
mean loss: 852.98
 ---- batch: 100 ----
mean loss: 840.94
 ---- batch: 110 ----
mean loss: 863.37
train mean loss: 845.25
epoch train time: 0:00:00.547409
elapsed time: 0:00:42.245094
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:29:50.453134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.46
 ---- batch: 020 ----
mean loss: 853.19
 ---- batch: 030 ----
mean loss: 854.98
 ---- batch: 040 ----
mean loss: 839.28
 ---- batch: 050 ----
mean loss: 828.85
 ---- batch: 060 ----
mean loss: 851.83
 ---- batch: 070 ----
mean loss: 843.34
 ---- batch: 080 ----
mean loss: 849.01
 ---- batch: 090 ----
mean loss: 838.51
 ---- batch: 100 ----
mean loss: 846.51
 ---- batch: 110 ----
mean loss: 851.56
train mean loss: 844.53
epoch train time: 0:00:00.546739
elapsed time: 0:00:42.791997
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:29:51.000009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.36
 ---- batch: 020 ----
mean loss: 880.67
 ---- batch: 030 ----
mean loss: 843.44
 ---- batch: 040 ----
mean loss: 869.61
 ---- batch: 050 ----
mean loss: 847.90
 ---- batch: 060 ----
mean loss: 859.73
 ---- batch: 070 ----
mean loss: 840.69
 ---- batch: 080 ----
mean loss: 857.37
 ---- batch: 090 ----
mean loss: 830.54
 ---- batch: 100 ----
mean loss: 826.37
 ---- batch: 110 ----
mean loss: 827.99
train mean loss: 844.22
epoch train time: 0:00:00.546930
elapsed time: 0:00:43.339061
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:29:51.547071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.91
 ---- batch: 020 ----
mean loss: 856.41
 ---- batch: 030 ----
mean loss: 828.45
 ---- batch: 040 ----
mean loss: 858.95
 ---- batch: 050 ----
mean loss: 850.57
 ---- batch: 060 ----
mean loss: 841.71
 ---- batch: 070 ----
mean loss: 853.88
 ---- batch: 080 ----
mean loss: 837.69
 ---- batch: 090 ----
mean loss: 835.48
 ---- batch: 100 ----
mean loss: 831.89
 ---- batch: 110 ----
mean loss: 853.09
train mean loss: 844.06
epoch train time: 0:00:00.552709
elapsed time: 0:00:43.891916
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:29:52.099968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.52
 ---- batch: 020 ----
mean loss: 834.72
 ---- batch: 030 ----
mean loss: 819.11
 ---- batch: 040 ----
mean loss: 844.23
 ---- batch: 050 ----
mean loss: 871.39
 ---- batch: 060 ----
mean loss: 835.68
 ---- batch: 070 ----
mean loss: 864.73
 ---- batch: 080 ----
mean loss: 832.28
 ---- batch: 090 ----
mean loss: 846.12
 ---- batch: 100 ----
mean loss: 862.00
 ---- batch: 110 ----
mean loss: 844.22
train mean loss: 844.05
epoch train time: 0:00:00.550719
elapsed time: 0:00:44.442825
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:29:52.650835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.46
 ---- batch: 020 ----
mean loss: 841.64
 ---- batch: 030 ----
mean loss: 856.86
 ---- batch: 040 ----
mean loss: 844.21
 ---- batch: 050 ----
mean loss: 860.68
 ---- batch: 060 ----
mean loss: 834.28
 ---- batch: 070 ----
mean loss: 833.52
 ---- batch: 080 ----
mean loss: 858.24
 ---- batch: 090 ----
mean loss: 841.86
 ---- batch: 100 ----
mean loss: 851.78
 ---- batch: 110 ----
mean loss: 829.28
train mean loss: 844.00
epoch train time: 0:00:00.559525
elapsed time: 0:00:45.002502
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:29:53.210529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.90
 ---- batch: 020 ----
mean loss: 842.30
 ---- batch: 030 ----
mean loss: 827.35
 ---- batch: 040 ----
mean loss: 849.84
 ---- batch: 050 ----
mean loss: 846.92
 ---- batch: 060 ----
mean loss: 858.65
 ---- batch: 070 ----
mean loss: 818.92
 ---- batch: 080 ----
mean loss: 846.64
 ---- batch: 090 ----
mean loss: 854.55
 ---- batch: 100 ----
mean loss: 835.53
 ---- batch: 110 ----
mean loss: 854.68
train mean loss: 844.00
epoch train time: 0:00:00.554658
elapsed time: 0:00:45.557311
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:29:53.765324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.78
 ---- batch: 020 ----
mean loss: 845.51
 ---- batch: 030 ----
mean loss: 843.54
 ---- batch: 040 ----
mean loss: 840.50
 ---- batch: 050 ----
mean loss: 834.65
 ---- batch: 060 ----
mean loss: 852.34
 ---- batch: 070 ----
mean loss: 864.87
 ---- batch: 080 ----
mean loss: 830.97
 ---- batch: 090 ----
mean loss: 848.37
 ---- batch: 100 ----
mean loss: 838.75
 ---- batch: 110 ----
mean loss: 838.40
train mean loss: 843.96
epoch train time: 0:00:00.553106
elapsed time: 0:00:46.110562
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:29:54.318570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.06
 ---- batch: 020 ----
mean loss: 852.37
 ---- batch: 030 ----
mean loss: 858.97
 ---- batch: 040 ----
mean loss: 845.34
 ---- batch: 050 ----
mean loss: 839.65
 ---- batch: 060 ----
mean loss: 819.40
 ---- batch: 070 ----
mean loss: 828.49
 ---- batch: 080 ----
mean loss: 860.86
 ---- batch: 090 ----
mean loss: 863.13
 ---- batch: 100 ----
mean loss: 836.78
 ---- batch: 110 ----
mean loss: 837.83
train mean loss: 843.92
epoch train time: 0:00:00.553016
elapsed time: 0:00:46.663716
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:29:54.871735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.02
 ---- batch: 020 ----
mean loss: 834.65
 ---- batch: 030 ----
mean loss: 854.13
 ---- batch: 040 ----
mean loss: 865.56
 ---- batch: 050 ----
mean loss: 838.79
 ---- batch: 060 ----
mean loss: 839.51
 ---- batch: 070 ----
mean loss: 829.95
 ---- batch: 080 ----
mean loss: 849.25
 ---- batch: 090 ----
mean loss: 852.48
 ---- batch: 100 ----
mean loss: 835.78
 ---- batch: 110 ----
mean loss: 847.17
train mean loss: 843.84
epoch train time: 0:00:00.540155
elapsed time: 0:00:47.204009
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:29:55.412036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.61
 ---- batch: 020 ----
mean loss: 839.61
 ---- batch: 030 ----
mean loss: 857.25
 ---- batch: 040 ----
mean loss: 861.73
 ---- batch: 050 ----
mean loss: 858.48
 ---- batch: 060 ----
mean loss: 838.94
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 844.45
 ---- batch: 090 ----
mean loss: 822.49
 ---- batch: 100 ----
mean loss: 855.87
 ---- batch: 110 ----
mean loss: 837.78
train mean loss: 843.94
epoch train time: 0:00:00.553026
elapsed time: 0:00:47.757257
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:29:55.965269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 823.18
 ---- batch: 030 ----
mean loss: 834.62
 ---- batch: 040 ----
mean loss: 843.74
 ---- batch: 050 ----
mean loss: 847.57
 ---- batch: 060 ----
mean loss: 842.48
 ---- batch: 070 ----
mean loss: 847.41
 ---- batch: 080 ----
mean loss: 859.33
 ---- batch: 090 ----
mean loss: 843.56
 ---- batch: 100 ----
mean loss: 848.51
 ---- batch: 110 ----
mean loss: 841.92
train mean loss: 843.91
epoch train time: 0:00:00.561169
elapsed time: 0:00:48.318563
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:29:56.526573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.91
 ---- batch: 020 ----
mean loss: 839.22
 ---- batch: 030 ----
mean loss: 847.48
 ---- batch: 040 ----
mean loss: 860.20
 ---- batch: 050 ----
mean loss: 844.45
 ---- batch: 060 ----
mean loss: 840.43
 ---- batch: 070 ----
mean loss: 807.10
 ---- batch: 080 ----
mean loss: 851.61
 ---- batch: 090 ----
mean loss: 844.55
 ---- batch: 100 ----
mean loss: 855.06
 ---- batch: 110 ----
mean loss: 856.51
train mean loss: 844.02
epoch train time: 0:00:00.555434
elapsed time: 0:00:48.874165
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:29:57.082177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.92
 ---- batch: 020 ----
mean loss: 833.89
 ---- batch: 030 ----
mean loss: 864.36
 ---- batch: 040 ----
mean loss: 867.12
 ---- batch: 050 ----
mean loss: 818.03
 ---- batch: 060 ----
mean loss: 833.12
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 846.87
 ---- batch: 090 ----
mean loss: 842.99
 ---- batch: 100 ----
mean loss: 845.87
 ---- batch: 110 ----
mean loss: 831.11
train mean loss: 844.04
epoch train time: 0:00:00.543674
elapsed time: 0:00:49.417984
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:29:57.626001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.77
 ---- batch: 020 ----
mean loss: 832.53
 ---- batch: 030 ----
mean loss: 855.60
 ---- batch: 040 ----
mean loss: 864.32
 ---- batch: 050 ----
mean loss: 850.39
 ---- batch: 060 ----
mean loss: 854.95
 ---- batch: 070 ----
mean loss: 825.55
 ---- batch: 080 ----
mean loss: 851.27
 ---- batch: 090 ----
mean loss: 841.01
 ---- batch: 100 ----
mean loss: 851.35
 ---- batch: 110 ----
mean loss: 831.00
train mean loss: 843.87
epoch train time: 0:00:00.557330
elapsed time: 0:00:49.975452
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:29:58.183460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.60
 ---- batch: 020 ----
mean loss: 848.83
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 843.44
 ---- batch: 050 ----
mean loss: 824.87
 ---- batch: 060 ----
mean loss: 836.67
 ---- batch: 070 ----
mean loss: 843.42
 ---- batch: 080 ----
mean loss: 858.18
 ---- batch: 090 ----
mean loss: 855.87
 ---- batch: 100 ----
mean loss: 850.60
 ---- batch: 110 ----
mean loss: 855.83
train mean loss: 843.97
epoch train time: 0:00:00.557754
elapsed time: 0:00:50.533358
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:29:58.741383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.51
 ---- batch: 020 ----
mean loss: 814.72
 ---- batch: 030 ----
mean loss: 838.89
 ---- batch: 040 ----
mean loss: 851.10
 ---- batch: 050 ----
mean loss: 848.75
 ---- batch: 060 ----
mean loss: 852.61
 ---- batch: 070 ----
mean loss: 834.73
 ---- batch: 080 ----
mean loss: 850.42
 ---- batch: 090 ----
mean loss: 854.91
 ---- batch: 100 ----
mean loss: 853.50
 ---- batch: 110 ----
mean loss: 840.83
train mean loss: 844.05
epoch train time: 0:00:00.547436
elapsed time: 0:00:51.080955
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:29:59.288982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.12
 ---- batch: 020 ----
mean loss: 861.63
 ---- batch: 030 ----
mean loss: 838.12
 ---- batch: 040 ----
mean loss: 845.02
 ---- batch: 050 ----
mean loss: 836.21
 ---- batch: 060 ----
mean loss: 835.64
 ---- batch: 070 ----
mean loss: 842.72
 ---- batch: 080 ----
mean loss: 851.19
 ---- batch: 090 ----
mean loss: 833.50
 ---- batch: 100 ----
mean loss: 834.01
 ---- batch: 110 ----
mean loss: 834.96
train mean loss: 844.04
epoch train time: 0:00:00.547224
elapsed time: 0:00:51.628335
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:29:59.836349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.26
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 855.51
 ---- batch: 040 ----
mean loss: 842.87
 ---- batch: 050 ----
mean loss: 826.74
 ---- batch: 060 ----
mean loss: 850.21
 ---- batch: 070 ----
mean loss: 835.43
 ---- batch: 080 ----
mean loss: 827.14
 ---- batch: 090 ----
mean loss: 864.26
 ---- batch: 100 ----
mean loss: 852.65
 ---- batch: 110 ----
mean loss: 827.14
train mean loss: 843.95
epoch train time: 0:00:00.562220
elapsed time: 0:00:52.190696
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:30:00.398708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.02
 ---- batch: 020 ----
mean loss: 855.72
 ---- batch: 030 ----
mean loss: 830.98
 ---- batch: 040 ----
mean loss: 838.38
 ---- batch: 050 ----
mean loss: 868.38
 ---- batch: 060 ----
mean loss: 833.24
 ---- batch: 070 ----
mean loss: 839.23
 ---- batch: 080 ----
mean loss: 829.76
 ---- batch: 090 ----
mean loss: 826.33
 ---- batch: 100 ----
mean loss: 855.07
 ---- batch: 110 ----
mean loss: 855.25
train mean loss: 843.94
epoch train time: 0:00:00.576528
elapsed time: 0:00:52.767364
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:30:00.975377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.59
 ---- batch: 020 ----
mean loss: 836.38
 ---- batch: 030 ----
mean loss: 854.91
 ---- batch: 040 ----
mean loss: 859.66
 ---- batch: 050 ----
mean loss: 817.83
 ---- batch: 060 ----
mean loss: 832.36
 ---- batch: 070 ----
mean loss: 858.04
 ---- batch: 080 ----
mean loss: 854.70
 ---- batch: 090 ----
mean loss: 824.22
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 844.56
train mean loss: 844.09
epoch train time: 0:00:00.571192
elapsed time: 0:00:53.338695
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:30:01.546708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.50
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 842.46
 ---- batch: 040 ----
mean loss: 845.66
 ---- batch: 050 ----
mean loss: 833.50
 ---- batch: 060 ----
mean loss: 835.28
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 852.60
 ---- batch: 090 ----
mean loss: 845.35
 ---- batch: 100 ----
mean loss: 850.67
 ---- batch: 110 ----
mean loss: 849.95
train mean loss: 844.01
epoch train time: 0:00:00.574309
elapsed time: 0:00:53.913153
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:30:02.121194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.99
 ---- batch: 020 ----
mean loss: 855.23
 ---- batch: 030 ----
mean loss: 833.33
 ---- batch: 040 ----
mean loss: 858.46
 ---- batch: 050 ----
mean loss: 855.47
 ---- batch: 060 ----
mean loss: 845.19
 ---- batch: 070 ----
mean loss: 832.66
 ---- batch: 080 ----
mean loss: 846.02
 ---- batch: 090 ----
mean loss: 835.69
 ---- batch: 100 ----
mean loss: 840.27
 ---- batch: 110 ----
mean loss: 845.14
train mean loss: 843.93
epoch train time: 0:00:00.571409
elapsed time: 0:00:54.484742
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:30:02.692772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.49
 ---- batch: 020 ----
mean loss: 825.58
 ---- batch: 030 ----
mean loss: 839.14
 ---- batch: 040 ----
mean loss: 848.79
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 850.23
 ---- batch: 070 ----
mean loss: 845.20
 ---- batch: 080 ----
mean loss: 866.49
 ---- batch: 090 ----
mean loss: 853.99
 ---- batch: 100 ----
mean loss: 844.02
 ---- batch: 110 ----
mean loss: 822.71
train mean loss: 843.96
epoch train time: 0:00:00.567430
elapsed time: 0:00:55.052325
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:30:03.260335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.68
 ---- batch: 020 ----
mean loss: 835.69
 ---- batch: 030 ----
mean loss: 846.90
 ---- batch: 040 ----
mean loss: 835.58
 ---- batch: 050 ----
mean loss: 868.28
 ---- batch: 060 ----
mean loss: 830.60
 ---- batch: 070 ----
mean loss: 843.45
 ---- batch: 080 ----
mean loss: 856.95
 ---- batch: 090 ----
mean loss: 841.46
 ---- batch: 100 ----
mean loss: 842.00
 ---- batch: 110 ----
mean loss: 840.71
train mean loss: 844.08
epoch train time: 0:00:00.564795
elapsed time: 0:00:55.617285
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:30:03.825294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.32
 ---- batch: 020 ----
mean loss: 848.50
 ---- batch: 030 ----
mean loss: 852.80
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 845.94
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 809.15
 ---- batch: 080 ----
mean loss: 856.45
 ---- batch: 090 ----
mean loss: 845.25
 ---- batch: 100 ----
mean loss: 849.24
 ---- batch: 110 ----
mean loss: 851.07
train mean loss: 844.06
epoch train time: 0:00:00.564929
elapsed time: 0:00:56.182348
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:30:04.390360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.24
 ---- batch: 020 ----
mean loss: 835.81
 ---- batch: 030 ----
mean loss: 850.60
 ---- batch: 040 ----
mean loss: 858.13
 ---- batch: 050 ----
mean loss: 833.72
 ---- batch: 060 ----
mean loss: 865.09
 ---- batch: 070 ----
mean loss: 852.62
 ---- batch: 080 ----
mean loss: 833.11
 ---- batch: 090 ----
mean loss: 822.30
 ---- batch: 100 ----
mean loss: 830.74
 ---- batch: 110 ----
mean loss: 843.90
train mean loss: 844.04
epoch train time: 0:00:00.571310
elapsed time: 0:00:56.753792
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:30:04.961803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.00
 ---- batch: 020 ----
mean loss: 869.90
 ---- batch: 030 ----
mean loss: 857.71
 ---- batch: 040 ----
mean loss: 850.94
 ---- batch: 050 ----
mean loss: 831.70
 ---- batch: 060 ----
mean loss: 842.32
 ---- batch: 070 ----
mean loss: 837.32
 ---- batch: 080 ----
mean loss: 844.26
 ---- batch: 090 ----
mean loss: 840.97
 ---- batch: 100 ----
mean loss: 843.34
 ---- batch: 110 ----
mean loss: 853.45
train mean loss: 843.94
epoch train time: 0:00:00.561811
elapsed time: 0:00:57.315736
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:30:05.523746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.24
 ---- batch: 020 ----
mean loss: 826.17
 ---- batch: 030 ----
mean loss: 857.23
 ---- batch: 040 ----
mean loss: 855.75
 ---- batch: 050 ----
mean loss: 829.66
 ---- batch: 060 ----
mean loss: 825.05
 ---- batch: 070 ----
mean loss: 862.07
 ---- batch: 080 ----
mean loss: 821.71
 ---- batch: 090 ----
mean loss: 868.26
 ---- batch: 100 ----
mean loss: 840.49
 ---- batch: 110 ----
mean loss: 854.33
train mean loss: 844.02
epoch train time: 0:00:00.550708
elapsed time: 0:00:57.866582
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:30:06.074593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.59
 ---- batch: 020 ----
mean loss: 839.91
 ---- batch: 030 ----
mean loss: 832.06
 ---- batch: 040 ----
mean loss: 833.44
 ---- batch: 050 ----
mean loss: 843.63
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 865.28
 ---- batch: 080 ----
mean loss: 850.58
 ---- batch: 090 ----
mean loss: 864.03
 ---- batch: 100 ----
mean loss: 829.87
 ---- batch: 110 ----
mean loss: 840.02
train mean loss: 844.02
epoch train time: 0:00:00.545798
elapsed time: 0:00:58.412510
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:30:06.620550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.87
 ---- batch: 020 ----
mean loss: 843.81
 ---- batch: 030 ----
mean loss: 851.02
 ---- batch: 040 ----
mean loss: 856.66
 ---- batch: 050 ----
mean loss: 832.84
 ---- batch: 060 ----
mean loss: 851.55
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 833.22
 ---- batch: 090 ----
mean loss: 833.15
 ---- batch: 100 ----
mean loss: 851.12
 ---- batch: 110 ----
mean loss: 824.81
train mean loss: 844.02
epoch train time: 0:00:00.545301
elapsed time: 0:00:58.957977
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:30:07.165989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.72
 ---- batch: 020 ----
mean loss: 849.35
 ---- batch: 030 ----
mean loss: 854.58
 ---- batch: 040 ----
mean loss: 833.74
 ---- batch: 050 ----
mean loss: 834.99
 ---- batch: 060 ----
mean loss: 847.39
 ---- batch: 070 ----
mean loss: 828.86
 ---- batch: 080 ----
mean loss: 845.62
 ---- batch: 090 ----
mean loss: 847.91
 ---- batch: 100 ----
mean loss: 839.45
 ---- batch: 110 ----
mean loss: 846.27
train mean loss: 844.00
epoch train time: 0:00:00.548240
elapsed time: 0:00:59.506400
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:30:07.714457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.21
 ---- batch: 020 ----
mean loss: 846.50
 ---- batch: 030 ----
mean loss: 862.86
 ---- batch: 040 ----
mean loss: 852.97
 ---- batch: 050 ----
mean loss: 837.18
 ---- batch: 060 ----
mean loss: 840.36
 ---- batch: 070 ----
mean loss: 860.78
 ---- batch: 080 ----
mean loss: 855.69
 ---- batch: 090 ----
mean loss: 833.00
 ---- batch: 100 ----
mean loss: 817.31
 ---- batch: 110 ----
mean loss: 839.15
train mean loss: 843.95
epoch train time: 0:00:00.547774
elapsed time: 0:01:00.054359
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:30:08.262370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.07
 ---- batch: 020 ----
mean loss: 842.82
 ---- batch: 030 ----
mean loss: 842.87
 ---- batch: 040 ----
mean loss: 838.16
 ---- batch: 050 ----
mean loss: 844.73
 ---- batch: 060 ----
mean loss: 863.76
 ---- batch: 070 ----
mean loss: 842.70
 ---- batch: 080 ----
mean loss: 808.24
 ---- batch: 090 ----
mean loss: 833.55
 ---- batch: 100 ----
mean loss: 859.01
 ---- batch: 110 ----
mean loss: 849.06
train mean loss: 844.05
epoch train time: 0:00:00.547197
elapsed time: 0:01:00.601722
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:30:08.809737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.96
 ---- batch: 020 ----
mean loss: 835.16
 ---- batch: 030 ----
mean loss: 837.94
 ---- batch: 040 ----
mean loss: 824.14
 ---- batch: 050 ----
mean loss: 839.35
 ---- batch: 060 ----
mean loss: 857.77
 ---- batch: 070 ----
mean loss: 822.83
 ---- batch: 080 ----
mean loss: 857.42
 ---- batch: 090 ----
mean loss: 835.62
 ---- batch: 100 ----
mean loss: 851.20
 ---- batch: 110 ----
mean loss: 863.65
train mean loss: 844.00
epoch train time: 0:00:00.548019
elapsed time: 0:01:01.149880
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:30:09.357890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.57
 ---- batch: 020 ----
mean loss: 858.19
 ---- batch: 030 ----
mean loss: 852.88
 ---- batch: 040 ----
mean loss: 839.77
 ---- batch: 050 ----
mean loss: 830.18
 ---- batch: 060 ----
mean loss: 841.44
 ---- batch: 070 ----
mean loss: 855.69
 ---- batch: 080 ----
mean loss: 849.65
 ---- batch: 090 ----
mean loss: 832.65
 ---- batch: 100 ----
mean loss: 836.73
 ---- batch: 110 ----
mean loss: 842.08
train mean loss: 843.93
epoch train time: 0:00:00.553501
elapsed time: 0:01:01.703515
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:30:09.911525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.38
 ---- batch: 020 ----
mean loss: 853.11
 ---- batch: 030 ----
mean loss: 847.68
 ---- batch: 040 ----
mean loss: 843.46
 ---- batch: 050 ----
mean loss: 859.03
 ---- batch: 060 ----
mean loss: 816.26
 ---- batch: 070 ----
mean loss: 860.17
 ---- batch: 080 ----
mean loss: 813.18
 ---- batch: 090 ----
mean loss: 834.16
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 857.18
train mean loss: 843.89
epoch train time: 0:00:00.543158
elapsed time: 0:01:02.246825
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:30:10.454855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.13
 ---- batch: 020 ----
mean loss: 837.08
 ---- batch: 030 ----
mean loss: 843.34
 ---- batch: 040 ----
mean loss: 852.88
 ---- batch: 050 ----
mean loss: 846.10
 ---- batch: 060 ----
mean loss: 837.07
 ---- batch: 070 ----
mean loss: 843.34
 ---- batch: 080 ----
mean loss: 827.01
 ---- batch: 090 ----
mean loss: 848.80
 ---- batch: 100 ----
mean loss: 843.75
 ---- batch: 110 ----
mean loss: 850.03
train mean loss: 844.00
epoch train time: 0:00:00.550312
elapsed time: 0:01:02.797291
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:30:11.005303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.49
 ---- batch: 020 ----
mean loss: 850.57
 ---- batch: 030 ----
mean loss: 833.69
 ---- batch: 040 ----
mean loss: 829.32
 ---- batch: 050 ----
mean loss: 839.96
 ---- batch: 060 ----
mean loss: 842.12
 ---- batch: 070 ----
mean loss: 843.92
 ---- batch: 080 ----
mean loss: 846.71
 ---- batch: 090 ----
mean loss: 832.62
 ---- batch: 100 ----
mean loss: 825.49
 ---- batch: 110 ----
mean loss: 814.94
train mean loss: 838.37
epoch train time: 0:00:00.556612
elapsed time: 0:01:03.354035
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:30:11.562043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 782.04
 ---- batch: 020 ----
mean loss: 774.37
 ---- batch: 030 ----
mean loss: 738.65
 ---- batch: 040 ----
mean loss: 736.57
 ---- batch: 050 ----
mean loss: 708.71
 ---- batch: 060 ----
mean loss: 670.45
 ---- batch: 070 ----
mean loss: 621.42
 ---- batch: 080 ----
mean loss: 524.19
 ---- batch: 090 ----
mean loss: 463.38
 ---- batch: 100 ----
mean loss: 429.38
 ---- batch: 110 ----
mean loss: 415.57
train mean loss: 618.17
epoch train time: 0:00:00.545696
elapsed time: 0:01:03.899859
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:30:12.107867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.17
 ---- batch: 020 ----
mean loss: 377.13
 ---- batch: 030 ----
mean loss: 362.47
 ---- batch: 040 ----
mean loss: 350.16
 ---- batch: 050 ----
mean loss: 337.96
 ---- batch: 060 ----
mean loss: 315.85
 ---- batch: 070 ----
mean loss: 318.39
 ---- batch: 080 ----
mean loss: 324.98
 ---- batch: 090 ----
mean loss: 303.88
 ---- batch: 100 ----
mean loss: 307.83
 ---- batch: 110 ----
mean loss: 289.75
train mean loss: 332.95
epoch train time: 0:00:00.546929
elapsed time: 0:01:04.446939
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:30:12.654960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.10
 ---- batch: 020 ----
mean loss: 282.75
 ---- batch: 030 ----
mean loss: 280.73
 ---- batch: 040 ----
mean loss: 279.39
 ---- batch: 050 ----
mean loss: 264.24
 ---- batch: 060 ----
mean loss: 268.01
 ---- batch: 070 ----
mean loss: 259.94
 ---- batch: 080 ----
mean loss: 266.96
 ---- batch: 090 ----
mean loss: 277.07
 ---- batch: 100 ----
mean loss: 271.29
 ---- batch: 110 ----
mean loss: 268.36
train mean loss: 273.20
epoch train time: 0:00:00.568397
elapsed time: 0:01:05.015480
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:30:13.223489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.02
 ---- batch: 020 ----
mean loss: 252.81
 ---- batch: 030 ----
mean loss: 261.53
 ---- batch: 040 ----
mean loss: 257.33
 ---- batch: 050 ----
mean loss: 255.03
 ---- batch: 060 ----
mean loss: 255.37
 ---- batch: 070 ----
mean loss: 246.06
 ---- batch: 080 ----
mean loss: 251.31
 ---- batch: 090 ----
mean loss: 252.68
 ---- batch: 100 ----
mean loss: 246.75
 ---- batch: 110 ----
mean loss: 240.72
train mean loss: 251.79
epoch train time: 0:00:00.549326
elapsed time: 0:01:05.564943
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:30:13.772968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.57
 ---- batch: 020 ----
mean loss: 246.14
 ---- batch: 030 ----
mean loss: 240.45
 ---- batch: 040 ----
mean loss: 231.60
 ---- batch: 050 ----
mean loss: 238.57
 ---- batch: 060 ----
mean loss: 236.12
 ---- batch: 070 ----
mean loss: 241.53
 ---- batch: 080 ----
mean loss: 231.51
 ---- batch: 090 ----
mean loss: 242.10
 ---- batch: 100 ----
mean loss: 226.81
 ---- batch: 110 ----
mean loss: 238.80
train mean loss: 236.60
epoch train time: 0:00:00.546853
elapsed time: 0:01:06.111947
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:30:14.319951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.71
 ---- batch: 020 ----
mean loss: 219.81
 ---- batch: 030 ----
mean loss: 226.02
 ---- batch: 040 ----
mean loss: 233.49
 ---- batch: 050 ----
mean loss: 215.64
 ---- batch: 060 ----
mean loss: 225.39
 ---- batch: 070 ----
mean loss: 215.07
 ---- batch: 080 ----
mean loss: 234.88
 ---- batch: 090 ----
mean loss: 222.51
 ---- batch: 100 ----
mean loss: 227.41
 ---- batch: 110 ----
mean loss: 226.41
train mean loss: 225.55
epoch train time: 0:00:00.570261
elapsed time: 0:01:06.682332
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:30:14.890341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.25
 ---- batch: 020 ----
mean loss: 215.99
 ---- batch: 030 ----
mean loss: 215.68
 ---- batch: 040 ----
mean loss: 220.57
 ---- batch: 050 ----
mean loss: 212.56
 ---- batch: 060 ----
mean loss: 212.90
 ---- batch: 070 ----
mean loss: 222.30
 ---- batch: 080 ----
mean loss: 213.78
 ---- batch: 090 ----
mean loss: 210.70
 ---- batch: 100 ----
mean loss: 215.31
 ---- batch: 110 ----
mean loss: 229.21
train mean loss: 216.91
epoch train time: 0:00:00.548769
elapsed time: 0:01:07.231232
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:30:15.439240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.41
 ---- batch: 020 ----
mean loss: 212.31
 ---- batch: 030 ----
mean loss: 206.75
 ---- batch: 040 ----
mean loss: 196.41
 ---- batch: 050 ----
mean loss: 211.10
 ---- batch: 060 ----
mean loss: 215.78
 ---- batch: 070 ----
mean loss: 213.67
 ---- batch: 080 ----
mean loss: 213.93
 ---- batch: 090 ----
mean loss: 220.73
 ---- batch: 100 ----
mean loss: 211.06
 ---- batch: 110 ----
mean loss: 203.89
train mean loss: 210.76
epoch train time: 0:00:00.550948
elapsed time: 0:01:07.782327
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:30:15.990341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.05
 ---- batch: 020 ----
mean loss: 199.48
 ---- batch: 030 ----
mean loss: 213.77
 ---- batch: 040 ----
mean loss: 213.26
 ---- batch: 050 ----
mean loss: 214.54
 ---- batch: 060 ----
mean loss: 207.38
 ---- batch: 070 ----
mean loss: 215.45
 ---- batch: 080 ----
mean loss: 205.06
 ---- batch: 090 ----
mean loss: 199.40
 ---- batch: 100 ----
mean loss: 199.72
 ---- batch: 110 ----
mean loss: 210.00
train mean loss: 208.22
epoch train time: 0:00:00.548675
elapsed time: 0:01:08.331139
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:30:16.539149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.78
 ---- batch: 020 ----
mean loss: 200.29
 ---- batch: 030 ----
mean loss: 198.44
 ---- batch: 040 ----
mean loss: 198.42
 ---- batch: 050 ----
mean loss: 191.60
 ---- batch: 060 ----
mean loss: 203.39
 ---- batch: 070 ----
mean loss: 199.52
 ---- batch: 080 ----
mean loss: 204.11
 ---- batch: 090 ----
mean loss: 199.08
 ---- batch: 100 ----
mean loss: 199.90
 ---- batch: 110 ----
mean loss: 200.47
train mean loss: 199.66
epoch train time: 0:00:00.572291
elapsed time: 0:01:08.903570
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:30:17.111587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.28
 ---- batch: 020 ----
mean loss: 190.72
 ---- batch: 030 ----
mean loss: 186.00
 ---- batch: 040 ----
mean loss: 198.15
 ---- batch: 050 ----
mean loss: 197.36
 ---- batch: 060 ----
mean loss: 192.37
 ---- batch: 070 ----
mean loss: 199.55
 ---- batch: 080 ----
mean loss: 197.33
 ---- batch: 090 ----
mean loss: 206.85
 ---- batch: 100 ----
mean loss: 199.58
 ---- batch: 110 ----
mean loss: 202.94
train mean loss: 197.31
epoch train time: 0:00:00.559875
elapsed time: 0:01:09.463595
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:30:17.671631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.78
 ---- batch: 020 ----
mean loss: 195.55
 ---- batch: 030 ----
mean loss: 185.10
 ---- batch: 040 ----
mean loss: 194.19
 ---- batch: 050 ----
mean loss: 199.41
 ---- batch: 060 ----
mean loss: 202.36
 ---- batch: 070 ----
mean loss: 199.67
 ---- batch: 080 ----
mean loss: 189.32
 ---- batch: 090 ----
mean loss: 199.15
 ---- batch: 100 ----
mean loss: 184.03
 ---- batch: 110 ----
mean loss: 198.69
train mean loss: 194.54
epoch train time: 0:00:00.560697
elapsed time: 0:01:10.024452
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:30:18.232473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.67
 ---- batch: 020 ----
mean loss: 195.15
 ---- batch: 030 ----
mean loss: 191.60
 ---- batch: 040 ----
mean loss: 184.63
 ---- batch: 050 ----
mean loss: 195.83
 ---- batch: 060 ----
mean loss: 196.21
 ---- batch: 070 ----
mean loss: 187.25
 ---- batch: 080 ----
mean loss: 188.14
 ---- batch: 090 ----
mean loss: 189.24
 ---- batch: 100 ----
mean loss: 186.73
 ---- batch: 110 ----
mean loss: 196.11
train mean loss: 191.72
epoch train time: 0:00:00.555183
elapsed time: 0:01:10.579780
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:30:18.787792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.34
 ---- batch: 020 ----
mean loss: 190.14
 ---- batch: 030 ----
mean loss: 200.07
 ---- batch: 040 ----
mean loss: 189.50
 ---- batch: 050 ----
mean loss: 186.52
 ---- batch: 060 ----
mean loss: 183.92
 ---- batch: 070 ----
mean loss: 191.48
 ---- batch: 080 ----
mean loss: 189.07
 ---- batch: 090 ----
mean loss: 196.19
 ---- batch: 100 ----
mean loss: 185.84
 ---- batch: 110 ----
mean loss: 191.26
train mean loss: 189.45
epoch train time: 0:00:00.547171
elapsed time: 0:01:11.127087
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:30:19.335098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.59
 ---- batch: 020 ----
mean loss: 180.22
 ---- batch: 030 ----
mean loss: 190.07
 ---- batch: 040 ----
mean loss: 187.43
 ---- batch: 050 ----
mean loss: 176.99
 ---- batch: 060 ----
mean loss: 189.17
 ---- batch: 070 ----
mean loss: 190.89
 ---- batch: 080 ----
mean loss: 198.21
 ---- batch: 090 ----
mean loss: 184.73
 ---- batch: 100 ----
mean loss: 185.79
 ---- batch: 110 ----
mean loss: 187.34
train mean loss: 187.05
epoch train time: 0:00:00.561120
elapsed time: 0:01:11.688338
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:30:19.896351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.78
 ---- batch: 020 ----
mean loss: 182.41
 ---- batch: 030 ----
mean loss: 188.78
 ---- batch: 040 ----
mean loss: 186.86
 ---- batch: 050 ----
mean loss: 190.34
 ---- batch: 060 ----
mean loss: 182.26
 ---- batch: 070 ----
mean loss: 186.60
 ---- batch: 080 ----
mean loss: 182.24
 ---- batch: 090 ----
mean loss: 181.16
 ---- batch: 100 ----
mean loss: 191.52
 ---- batch: 110 ----
mean loss: 181.72
train mean loss: 185.29
epoch train time: 0:00:00.544749
elapsed time: 0:01:12.233224
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:30:20.441235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.64
 ---- batch: 020 ----
mean loss: 185.47
 ---- batch: 030 ----
mean loss: 182.94
 ---- batch: 040 ----
mean loss: 176.99
 ---- batch: 050 ----
mean loss: 178.64
 ---- batch: 060 ----
mean loss: 190.95
 ---- batch: 070 ----
mean loss: 180.70
 ---- batch: 080 ----
mean loss: 191.33
 ---- batch: 090 ----
mean loss: 189.26
 ---- batch: 100 ----
mean loss: 187.32
 ---- batch: 110 ----
mean loss: 180.47
train mean loss: 184.30
epoch train time: 0:00:00.554419
elapsed time: 0:01:12.787779
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:30:20.995809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.02
 ---- batch: 020 ----
mean loss: 182.24
 ---- batch: 030 ----
mean loss: 181.86
 ---- batch: 040 ----
mean loss: 177.62
 ---- batch: 050 ----
mean loss: 181.22
 ---- batch: 060 ----
mean loss: 182.66
 ---- batch: 070 ----
mean loss: 188.09
 ---- batch: 080 ----
mean loss: 186.69
 ---- batch: 090 ----
mean loss: 181.17
 ---- batch: 100 ----
mean loss: 178.83
 ---- batch: 110 ----
mean loss: 182.05
train mean loss: 183.15
epoch train time: 0:00:00.550075
elapsed time: 0:01:13.338018
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:30:21.546031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.51
 ---- batch: 020 ----
mean loss: 178.49
 ---- batch: 030 ----
mean loss: 187.66
 ---- batch: 040 ----
mean loss: 180.88
 ---- batch: 050 ----
mean loss: 189.88
 ---- batch: 060 ----
mean loss: 173.75
 ---- batch: 070 ----
mean loss: 177.10
 ---- batch: 080 ----
mean loss: 183.19
 ---- batch: 090 ----
mean loss: 182.89
 ---- batch: 100 ----
mean loss: 178.64
 ---- batch: 110 ----
mean loss: 186.86
train mean loss: 181.52
epoch train time: 0:00:00.554373
elapsed time: 0:01:13.892524
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:30:22.100533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.66
 ---- batch: 020 ----
mean loss: 171.74
 ---- batch: 030 ----
mean loss: 183.39
 ---- batch: 040 ----
mean loss: 185.01
 ---- batch: 050 ----
mean loss: 184.40
 ---- batch: 060 ----
mean loss: 188.08
 ---- batch: 070 ----
mean loss: 185.21
 ---- batch: 080 ----
mean loss: 179.46
 ---- batch: 090 ----
mean loss: 181.44
 ---- batch: 100 ----
mean loss: 172.39
 ---- batch: 110 ----
mean loss: 185.05
train mean loss: 181.88
epoch train time: 0:00:00.552550
elapsed time: 0:01:14.445205
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:30:22.653234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.23
 ---- batch: 020 ----
mean loss: 182.79
 ---- batch: 030 ----
mean loss: 175.33
 ---- batch: 040 ----
mean loss: 179.59
 ---- batch: 050 ----
mean loss: 182.74
 ---- batch: 060 ----
mean loss: 180.89
 ---- batch: 070 ----
mean loss: 176.59
 ---- batch: 080 ----
mean loss: 179.22
 ---- batch: 090 ----
mean loss: 185.04
 ---- batch: 100 ----
mean loss: 183.36
 ---- batch: 110 ----
mean loss: 173.79
train mean loss: 179.98
epoch train time: 0:00:00.558459
elapsed time: 0:01:15.003830
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:30:23.211840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.98
 ---- batch: 020 ----
mean loss: 175.08
 ---- batch: 030 ----
mean loss: 169.07
 ---- batch: 040 ----
mean loss: 178.51
 ---- batch: 050 ----
mean loss: 182.37
 ---- batch: 060 ----
mean loss: 189.32
 ---- batch: 070 ----
mean loss: 178.53
 ---- batch: 080 ----
mean loss: 182.53
 ---- batch: 090 ----
mean loss: 180.09
 ---- batch: 100 ----
mean loss: 177.93
 ---- batch: 110 ----
mean loss: 179.97
train mean loss: 178.24
epoch train time: 0:00:00.553427
elapsed time: 0:01:15.557429
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:30:23.765443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.44
 ---- batch: 020 ----
mean loss: 176.61
 ---- batch: 030 ----
mean loss: 169.67
 ---- batch: 040 ----
mean loss: 182.31
 ---- batch: 050 ----
mean loss: 179.50
 ---- batch: 060 ----
mean loss: 181.08
 ---- batch: 070 ----
mean loss: 179.55
 ---- batch: 080 ----
mean loss: 175.80
 ---- batch: 090 ----
mean loss: 182.01
 ---- batch: 100 ----
mean loss: 172.65
 ---- batch: 110 ----
mean loss: 193.79
train mean loss: 178.09
epoch train time: 0:00:00.544883
elapsed time: 0:01:16.102452
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:30:24.310470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.05
 ---- batch: 020 ----
mean loss: 181.11
 ---- batch: 030 ----
mean loss: 174.99
 ---- batch: 040 ----
mean loss: 174.11
 ---- batch: 050 ----
mean loss: 177.87
 ---- batch: 060 ----
mean loss: 182.95
 ---- batch: 070 ----
mean loss: 182.89
 ---- batch: 080 ----
mean loss: 174.23
 ---- batch: 090 ----
mean loss: 181.43
 ---- batch: 100 ----
mean loss: 180.42
 ---- batch: 110 ----
mean loss: 176.04
train mean loss: 178.02
epoch train time: 0:00:00.549707
elapsed time: 0:01:16.652318
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:30:24.860372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.86
 ---- batch: 020 ----
mean loss: 172.39
 ---- batch: 030 ----
mean loss: 174.54
 ---- batch: 040 ----
mean loss: 177.23
 ---- batch: 050 ----
mean loss: 179.67
 ---- batch: 060 ----
mean loss: 171.56
 ---- batch: 070 ----
mean loss: 170.28
 ---- batch: 080 ----
mean loss: 192.66
 ---- batch: 090 ----
mean loss: 178.98
 ---- batch: 100 ----
mean loss: 164.59
 ---- batch: 110 ----
mean loss: 183.18
train mean loss: 176.14
epoch train time: 0:00:00.549103
elapsed time: 0:01:17.201596
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:30:25.409632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.32
 ---- batch: 020 ----
mean loss: 180.92
 ---- batch: 030 ----
mean loss: 180.78
 ---- batch: 040 ----
mean loss: 178.48
 ---- batch: 050 ----
mean loss: 165.93
 ---- batch: 060 ----
mean loss: 180.05
 ---- batch: 070 ----
mean loss: 184.94
 ---- batch: 080 ----
mean loss: 178.49
 ---- batch: 090 ----
mean loss: 176.98
 ---- batch: 100 ----
mean loss: 173.04
 ---- batch: 110 ----
mean loss: 175.75
train mean loss: 176.52
epoch train time: 0:00:00.552395
elapsed time: 0:01:17.754159
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:30:25.962175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.59
 ---- batch: 020 ----
mean loss: 176.05
 ---- batch: 030 ----
mean loss: 176.84
 ---- batch: 040 ----
mean loss: 169.48
 ---- batch: 050 ----
mean loss: 177.19
 ---- batch: 060 ----
mean loss: 172.79
 ---- batch: 070 ----
mean loss: 174.07
 ---- batch: 080 ----
mean loss: 173.93
 ---- batch: 090 ----
mean loss: 174.09
 ---- batch: 100 ----
mean loss: 176.30
 ---- batch: 110 ----
mean loss: 175.89
train mean loss: 174.77
epoch train time: 0:00:00.557184
elapsed time: 0:01:18.311521
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:30:26.519533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.21
 ---- batch: 020 ----
mean loss: 177.97
 ---- batch: 030 ----
mean loss: 172.62
 ---- batch: 040 ----
mean loss: 170.47
 ---- batch: 050 ----
mean loss: 168.77
 ---- batch: 060 ----
mean loss: 176.01
 ---- batch: 070 ----
mean loss: 185.77
 ---- batch: 080 ----
mean loss: 177.90
 ---- batch: 090 ----
mean loss: 174.15
 ---- batch: 100 ----
mean loss: 180.73
 ---- batch: 110 ----
mean loss: 172.82
train mean loss: 175.31
epoch train time: 0:00:00.553074
elapsed time: 0:01:18.864760
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:30:27.072769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.59
 ---- batch: 020 ----
mean loss: 177.63
 ---- batch: 030 ----
mean loss: 161.95
 ---- batch: 040 ----
mean loss: 173.67
 ---- batch: 050 ----
mean loss: 176.44
 ---- batch: 060 ----
mean loss: 172.40
 ---- batch: 070 ----
mean loss: 172.94
 ---- batch: 080 ----
mean loss: 182.01
 ---- batch: 090 ----
mean loss: 177.78
 ---- batch: 100 ----
mean loss: 178.40
 ---- batch: 110 ----
mean loss: 181.29
train mean loss: 174.46
epoch train time: 0:00:00.550551
elapsed time: 0:01:19.415479
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:30:27.623488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.84
 ---- batch: 020 ----
mean loss: 182.70
 ---- batch: 030 ----
mean loss: 165.74
 ---- batch: 040 ----
mean loss: 168.52
 ---- batch: 050 ----
mean loss: 180.33
 ---- batch: 060 ----
mean loss: 172.48
 ---- batch: 070 ----
mean loss: 170.49
 ---- batch: 080 ----
mean loss: 170.49
 ---- batch: 090 ----
mean loss: 176.34
 ---- batch: 100 ----
mean loss: 176.00
 ---- batch: 110 ----
mean loss: 183.62
train mean loss: 173.53
epoch train time: 0:00:00.556666
elapsed time: 0:01:19.972275
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:30:28.180290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.53
 ---- batch: 020 ----
mean loss: 177.69
 ---- batch: 030 ----
mean loss: 172.09
 ---- batch: 040 ----
mean loss: 162.00
 ---- batch: 050 ----
mean loss: 172.48
 ---- batch: 060 ----
mean loss: 173.04
 ---- batch: 070 ----
mean loss: 172.95
 ---- batch: 080 ----
mean loss: 182.35
 ---- batch: 090 ----
mean loss: 170.78
 ---- batch: 100 ----
mean loss: 166.83
 ---- batch: 110 ----
mean loss: 180.02
train mean loss: 172.77
epoch train time: 0:00:00.548084
elapsed time: 0:01:20.520506
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:30:28.728536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.95
 ---- batch: 020 ----
mean loss: 166.01
 ---- batch: 030 ----
mean loss: 173.38
 ---- batch: 040 ----
mean loss: 169.57
 ---- batch: 050 ----
mean loss: 172.72
 ---- batch: 060 ----
mean loss: 165.28
 ---- batch: 070 ----
mean loss: 178.74
 ---- batch: 080 ----
mean loss: 174.07
 ---- batch: 090 ----
mean loss: 175.11
 ---- batch: 100 ----
mean loss: 178.64
 ---- batch: 110 ----
mean loss: 170.83
train mean loss: 170.99
epoch train time: 0:00:00.547942
elapsed time: 0:01:21.068620
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:30:29.276648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.23
 ---- batch: 020 ----
mean loss: 169.66
 ---- batch: 030 ----
mean loss: 169.84
 ---- batch: 040 ----
mean loss: 174.16
 ---- batch: 050 ----
mean loss: 161.99
 ---- batch: 060 ----
mean loss: 170.97
 ---- batch: 070 ----
mean loss: 171.14
 ---- batch: 080 ----
mean loss: 173.66
 ---- batch: 090 ----
mean loss: 170.66
 ---- batch: 100 ----
mean loss: 171.98
 ---- batch: 110 ----
mean loss: 174.70
train mean loss: 170.62
epoch train time: 0:00:00.553654
elapsed time: 0:01:21.622436
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:30:29.830451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.98
 ---- batch: 020 ----
mean loss: 165.73
 ---- batch: 030 ----
mean loss: 161.93
 ---- batch: 040 ----
mean loss: 167.08
 ---- batch: 050 ----
mean loss: 163.99
 ---- batch: 060 ----
mean loss: 175.07
 ---- batch: 070 ----
mean loss: 182.06
 ---- batch: 080 ----
mean loss: 174.34
 ---- batch: 090 ----
mean loss: 164.52
 ---- batch: 100 ----
mean loss: 174.34
 ---- batch: 110 ----
mean loss: 172.32
train mean loss: 170.49
epoch train time: 0:00:00.551927
elapsed time: 0:01:22.174499
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:30:30.382510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.48
 ---- batch: 020 ----
mean loss: 171.99
 ---- batch: 030 ----
mean loss: 171.83
 ---- batch: 040 ----
mean loss: 162.80
 ---- batch: 050 ----
mean loss: 170.21
 ---- batch: 060 ----
mean loss: 172.87
 ---- batch: 070 ----
mean loss: 170.18
 ---- batch: 080 ----
mean loss: 167.19
 ---- batch: 090 ----
mean loss: 170.56
 ---- batch: 100 ----
mean loss: 172.35
 ---- batch: 110 ----
mean loss: 174.36
train mean loss: 169.97
epoch train time: 0:00:00.557227
elapsed time: 0:01:22.731859
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:30:30.939870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.98
 ---- batch: 020 ----
mean loss: 173.73
 ---- batch: 030 ----
mean loss: 161.64
 ---- batch: 040 ----
mean loss: 171.43
 ---- batch: 050 ----
mean loss: 162.79
 ---- batch: 060 ----
mean loss: 170.04
 ---- batch: 070 ----
mean loss: 172.56
 ---- batch: 080 ----
mean loss: 177.87
 ---- batch: 090 ----
mean loss: 166.54
 ---- batch: 100 ----
mean loss: 170.98
 ---- batch: 110 ----
mean loss: 174.05
train mean loss: 169.97
epoch train time: 0:00:00.549265
elapsed time: 0:01:23.281258
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:30:31.489268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.64
 ---- batch: 020 ----
mean loss: 174.89
 ---- batch: 030 ----
mean loss: 164.92
 ---- batch: 040 ----
mean loss: 171.27
 ---- batch: 050 ----
mean loss: 170.25
 ---- batch: 060 ----
mean loss: 157.20
 ---- batch: 070 ----
mean loss: 171.98
 ---- batch: 080 ----
mean loss: 160.15
 ---- batch: 090 ----
mean loss: 174.78
 ---- batch: 100 ----
mean loss: 169.49
 ---- batch: 110 ----
mean loss: 172.43
train mean loss: 168.52
epoch train time: 0:00:00.558359
elapsed time: 0:01:23.839750
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:30:32.047780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.29
 ---- batch: 020 ----
mean loss: 174.47
 ---- batch: 030 ----
mean loss: 169.19
 ---- batch: 040 ----
mean loss: 163.02
 ---- batch: 050 ----
mean loss: 165.32
 ---- batch: 060 ----
mean loss: 169.03
 ---- batch: 070 ----
mean loss: 175.12
 ---- batch: 080 ----
mean loss: 174.70
 ---- batch: 090 ----
mean loss: 162.00
 ---- batch: 100 ----
mean loss: 172.51
 ---- batch: 110 ----
mean loss: 166.18
train mean loss: 168.61
epoch train time: 0:00:00.552574
elapsed time: 0:01:24.392474
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:30:32.600483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.93
 ---- batch: 020 ----
mean loss: 167.46
 ---- batch: 030 ----
mean loss: 173.47
 ---- batch: 040 ----
mean loss: 164.67
 ---- batch: 050 ----
mean loss: 173.63
 ---- batch: 060 ----
mean loss: 164.40
 ---- batch: 070 ----
mean loss: 166.41
 ---- batch: 080 ----
mean loss: 169.24
 ---- batch: 090 ----
mean loss: 166.70
 ---- batch: 100 ----
mean loss: 164.78
 ---- batch: 110 ----
mean loss: 168.44
train mean loss: 167.83
epoch train time: 0:00:00.563438
elapsed time: 0:01:24.956045
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:30:33.164074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.39
 ---- batch: 020 ----
mean loss: 169.35
 ---- batch: 030 ----
mean loss: 165.11
 ---- batch: 040 ----
mean loss: 176.55
 ---- batch: 050 ----
mean loss: 177.33
 ---- batch: 060 ----
mean loss: 164.36
 ---- batch: 070 ----
mean loss: 163.65
 ---- batch: 080 ----
mean loss: 159.85
 ---- batch: 090 ----
mean loss: 171.43
 ---- batch: 100 ----
mean loss: 170.16
 ---- batch: 110 ----
mean loss: 167.74
train mean loss: 168.27
epoch train time: 0:00:00.558894
elapsed time: 0:01:25.515089
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:30:33.723099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.12
 ---- batch: 020 ----
mean loss: 156.56
 ---- batch: 030 ----
mean loss: 151.19
 ---- batch: 040 ----
mean loss: 168.58
 ---- batch: 050 ----
mean loss: 177.60
 ---- batch: 060 ----
mean loss: 172.21
 ---- batch: 070 ----
mean loss: 171.88
 ---- batch: 080 ----
mean loss: 167.22
 ---- batch: 090 ----
mean loss: 164.12
 ---- batch: 100 ----
mean loss: 170.20
 ---- batch: 110 ----
mean loss: 172.04
train mean loss: 167.24
epoch train time: 0:00:00.554931
elapsed time: 0:01:26.070153
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:30:34.278164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.19
 ---- batch: 020 ----
mean loss: 162.01
 ---- batch: 030 ----
mean loss: 163.74
 ---- batch: 040 ----
mean loss: 167.80
 ---- batch: 050 ----
mean loss: 174.50
 ---- batch: 060 ----
mean loss: 163.10
 ---- batch: 070 ----
mean loss: 160.10
 ---- batch: 080 ----
mean loss: 173.40
 ---- batch: 090 ----
mean loss: 173.96
 ---- batch: 100 ----
mean loss: 170.83
 ---- batch: 110 ----
mean loss: 163.12
train mean loss: 166.64
epoch train time: 0:00:00.550251
elapsed time: 0:01:26.620540
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:30:34.828553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.16
 ---- batch: 020 ----
mean loss: 166.06
 ---- batch: 030 ----
mean loss: 164.93
 ---- batch: 040 ----
mean loss: 162.60
 ---- batch: 050 ----
mean loss: 166.74
 ---- batch: 060 ----
mean loss: 170.57
 ---- batch: 070 ----
mean loss: 168.25
 ---- batch: 080 ----
mean loss: 178.03
 ---- batch: 090 ----
mean loss: 166.43
 ---- batch: 100 ----
mean loss: 167.00
 ---- batch: 110 ----
mean loss: 162.98
train mean loss: 166.35
epoch train time: 0:00:00.553230
elapsed time: 0:01:27.173936
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:30:35.381945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.38
 ---- batch: 020 ----
mean loss: 166.92
 ---- batch: 030 ----
mean loss: 156.75
 ---- batch: 040 ----
mean loss: 166.50
 ---- batch: 050 ----
mean loss: 163.51
 ---- batch: 060 ----
mean loss: 163.65
 ---- batch: 070 ----
mean loss: 157.48
 ---- batch: 080 ----
mean loss: 158.55
 ---- batch: 090 ----
mean loss: 171.16
 ---- batch: 100 ----
mean loss: 171.44
 ---- batch: 110 ----
mean loss: 173.80
train mean loss: 165.35
epoch train time: 0:00:00.553842
elapsed time: 0:01:27.727923
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:30:35.936016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.86
 ---- batch: 020 ----
mean loss: 162.18
 ---- batch: 030 ----
mean loss: 169.33
 ---- batch: 040 ----
mean loss: 158.82
 ---- batch: 050 ----
mean loss: 164.66
 ---- batch: 060 ----
mean loss: 171.98
 ---- batch: 070 ----
mean loss: 163.70
 ---- batch: 080 ----
mean loss: 163.62
 ---- batch: 090 ----
mean loss: 162.12
 ---- batch: 100 ----
mean loss: 171.13
 ---- batch: 110 ----
mean loss: 164.25
train mean loss: 164.50
epoch train time: 0:00:00.547169
elapsed time: 0:01:28.275318
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:30:36.483331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.52
 ---- batch: 020 ----
mean loss: 164.43
 ---- batch: 030 ----
mean loss: 164.86
 ---- batch: 040 ----
mean loss: 158.58
 ---- batch: 050 ----
mean loss: 169.82
 ---- batch: 060 ----
mean loss: 156.87
 ---- batch: 070 ----
mean loss: 165.02
 ---- batch: 080 ----
mean loss: 167.87
 ---- batch: 090 ----
mean loss: 165.89
 ---- batch: 100 ----
mean loss: 156.38
 ---- batch: 110 ----
mean loss: 166.65
train mean loss: 164.12
epoch train time: 0:00:00.563915
elapsed time: 0:01:28.839370
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:30:37.047381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.35
 ---- batch: 020 ----
mean loss: 163.63
 ---- batch: 030 ----
mean loss: 163.84
 ---- batch: 040 ----
mean loss: 165.82
 ---- batch: 050 ----
mean loss: 163.38
 ---- batch: 060 ----
mean loss: 165.04
 ---- batch: 070 ----
mean loss: 158.95
 ---- batch: 080 ----
mean loss: 172.62
 ---- batch: 090 ----
mean loss: 164.41
 ---- batch: 100 ----
mean loss: 168.38
 ---- batch: 110 ----
mean loss: 159.44
train mean loss: 164.62
epoch train time: 0:00:00.549123
elapsed time: 0:01:29.388643
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:30:37.596654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.31
 ---- batch: 020 ----
mean loss: 157.15
 ---- batch: 030 ----
mean loss: 162.26
 ---- batch: 040 ----
mean loss: 168.34
 ---- batch: 050 ----
mean loss: 158.35
 ---- batch: 060 ----
mean loss: 169.63
 ---- batch: 070 ----
mean loss: 161.97
 ---- batch: 080 ----
mean loss: 161.72
 ---- batch: 090 ----
mean loss: 157.97
 ---- batch: 100 ----
mean loss: 166.66
 ---- batch: 110 ----
mean loss: 173.40
train mean loss: 164.08
epoch train time: 0:00:00.551209
elapsed time: 0:01:29.940015
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:30:38.148026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.89
 ---- batch: 020 ----
mean loss: 160.36
 ---- batch: 030 ----
mean loss: 163.93
 ---- batch: 040 ----
mean loss: 161.46
 ---- batch: 050 ----
mean loss: 161.04
 ---- batch: 060 ----
mean loss: 160.70
 ---- batch: 070 ----
mean loss: 161.20
 ---- batch: 080 ----
mean loss: 161.49
 ---- batch: 090 ----
mean loss: 165.85
 ---- batch: 100 ----
mean loss: 169.96
 ---- batch: 110 ----
mean loss: 172.21
train mean loss: 162.82
epoch train time: 0:00:00.548992
elapsed time: 0:01:30.489139
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:30:38.697150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.82
 ---- batch: 020 ----
mean loss: 160.70
 ---- batch: 030 ----
mean loss: 163.84
 ---- batch: 040 ----
mean loss: 155.59
 ---- batch: 050 ----
mean loss: 157.63
 ---- batch: 060 ----
mean loss: 171.02
 ---- batch: 070 ----
mean loss: 164.01
 ---- batch: 080 ----
mean loss: 162.59
 ---- batch: 090 ----
mean loss: 163.78
 ---- batch: 100 ----
mean loss: 170.94
 ---- batch: 110 ----
mean loss: 163.52
train mean loss: 163.69
epoch train time: 0:00:00.563048
elapsed time: 0:01:31.052336
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:30:39.260357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.41
 ---- batch: 020 ----
mean loss: 166.01
 ---- batch: 030 ----
mean loss: 159.75
 ---- batch: 040 ----
mean loss: 157.82
 ---- batch: 050 ----
mean loss: 162.19
 ---- batch: 060 ----
mean loss: 164.16
 ---- batch: 070 ----
mean loss: 160.77
 ---- batch: 080 ----
mean loss: 167.34
 ---- batch: 090 ----
mean loss: 167.97
 ---- batch: 100 ----
mean loss: 157.20
 ---- batch: 110 ----
mean loss: 166.08
train mean loss: 162.32
epoch train time: 0:00:00.555123
elapsed time: 0:01:31.607602
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:30:39.815613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.75
 ---- batch: 020 ----
mean loss: 165.13
 ---- batch: 030 ----
mean loss: 164.22
 ---- batch: 040 ----
mean loss: 157.03
 ---- batch: 050 ----
mean loss: 166.61
 ---- batch: 060 ----
mean loss: 158.91
 ---- batch: 070 ----
mean loss: 169.78
 ---- batch: 080 ----
mean loss: 167.65
 ---- batch: 090 ----
mean loss: 164.86
 ---- batch: 100 ----
mean loss: 152.22
 ---- batch: 110 ----
mean loss: 162.75
train mean loss: 162.40
epoch train time: 0:00:00.556258
elapsed time: 0:01:32.163991
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:30:40.372002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.76
 ---- batch: 020 ----
mean loss: 161.57
 ---- batch: 030 ----
mean loss: 152.89
 ---- batch: 040 ----
mean loss: 159.61
 ---- batch: 050 ----
mean loss: 164.96
 ---- batch: 060 ----
mean loss: 160.64
 ---- batch: 070 ----
mean loss: 169.56
 ---- batch: 080 ----
mean loss: 162.19
 ---- batch: 090 ----
mean loss: 163.40
 ---- batch: 100 ----
mean loss: 165.88
 ---- batch: 110 ----
mean loss: 164.88
train mean loss: 161.84
epoch train time: 0:00:00.554481
elapsed time: 0:01:32.718622
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:30:40.926632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.59
 ---- batch: 020 ----
mean loss: 154.01
 ---- batch: 030 ----
mean loss: 162.67
 ---- batch: 040 ----
mean loss: 156.22
 ---- batch: 050 ----
mean loss: 163.97
 ---- batch: 060 ----
mean loss: 157.14
 ---- batch: 070 ----
mean loss: 168.57
 ---- batch: 080 ----
mean loss: 164.16
 ---- batch: 090 ----
mean loss: 157.71
 ---- batch: 100 ----
mean loss: 160.04
 ---- batch: 110 ----
mean loss: 167.47
train mean loss: 160.74
epoch train time: 0:00:00.548565
elapsed time: 0:01:33.267321
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:30:41.475331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.35
 ---- batch: 020 ----
mean loss: 158.45
 ---- batch: 030 ----
mean loss: 156.03
 ---- batch: 040 ----
mean loss: 154.82
 ---- batch: 050 ----
mean loss: 165.72
 ---- batch: 060 ----
mean loss: 160.06
 ---- batch: 070 ----
mean loss: 163.18
 ---- batch: 080 ----
mean loss: 161.89
 ---- batch: 090 ----
mean loss: 154.46
 ---- batch: 100 ----
mean loss: 159.80
 ---- batch: 110 ----
mean loss: 165.19
train mean loss: 160.19
epoch train time: 0:00:00.551833
elapsed time: 0:01:33.819284
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:30:42.027293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.09
 ---- batch: 020 ----
mean loss: 152.17
 ---- batch: 030 ----
mean loss: 157.61
 ---- batch: 040 ----
mean loss: 159.16
 ---- batch: 050 ----
mean loss: 153.84
 ---- batch: 060 ----
mean loss: 161.95
 ---- batch: 070 ----
mean loss: 165.58
 ---- batch: 080 ----
mean loss: 169.46
 ---- batch: 090 ----
mean loss: 167.35
 ---- batch: 100 ----
mean loss: 155.96
 ---- batch: 110 ----
mean loss: 166.60
train mean loss: 161.00
epoch train time: 0:00:00.555267
elapsed time: 0:01:34.374731
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:30:42.582770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.24
 ---- batch: 020 ----
mean loss: 152.31
 ---- batch: 030 ----
mean loss: 161.10
 ---- batch: 040 ----
mean loss: 149.35
 ---- batch: 050 ----
mean loss: 160.76
 ---- batch: 060 ----
mean loss: 160.60
 ---- batch: 070 ----
mean loss: 153.54
 ---- batch: 080 ----
mean loss: 165.30
 ---- batch: 090 ----
mean loss: 161.22
 ---- batch: 100 ----
mean loss: 163.19
 ---- batch: 110 ----
mean loss: 168.66
train mean loss: 160.09
epoch train time: 0:00:00.550748
elapsed time: 0:01:34.925706
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:30:43.133737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.38
 ---- batch: 020 ----
mean loss: 164.18
 ---- batch: 030 ----
mean loss: 157.08
 ---- batch: 040 ----
mean loss: 156.92
 ---- batch: 050 ----
mean loss: 164.69
 ---- batch: 060 ----
mean loss: 171.63
 ---- batch: 070 ----
mean loss: 153.79
 ---- batch: 080 ----
mean loss: 157.00
 ---- batch: 090 ----
mean loss: 159.70
 ---- batch: 100 ----
mean loss: 164.16
 ---- batch: 110 ----
mean loss: 161.31
train mean loss: 160.57
epoch train time: 0:00:00.554693
elapsed time: 0:01:35.480585
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:30:43.688601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.72
 ---- batch: 020 ----
mean loss: 164.45
 ---- batch: 030 ----
mean loss: 154.55
 ---- batch: 040 ----
mean loss: 152.66
 ---- batch: 050 ----
mean loss: 156.98
 ---- batch: 060 ----
mean loss: 159.02
 ---- batch: 070 ----
mean loss: 160.55
 ---- batch: 080 ----
mean loss: 167.48
 ---- batch: 090 ----
mean loss: 158.45
 ---- batch: 100 ----
mean loss: 148.99
 ---- batch: 110 ----
mean loss: 150.49
train mean loss: 158.01
epoch train time: 0:00:00.558859
elapsed time: 0:01:36.039581
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:30:44.247592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.11
 ---- batch: 020 ----
mean loss: 159.06
 ---- batch: 030 ----
mean loss: 159.26
 ---- batch: 040 ----
mean loss: 158.49
 ---- batch: 050 ----
mean loss: 156.77
 ---- batch: 060 ----
mean loss: 163.02
 ---- batch: 070 ----
mean loss: 166.86
 ---- batch: 080 ----
mean loss: 163.20
 ---- batch: 090 ----
mean loss: 157.49
 ---- batch: 100 ----
mean loss: 164.73
 ---- batch: 110 ----
mean loss: 155.66
train mean loss: 159.65
epoch train time: 0:00:00.556004
elapsed time: 0:01:36.595762
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:30:44.803774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.77
 ---- batch: 020 ----
mean loss: 162.32
 ---- batch: 030 ----
mean loss: 157.19
 ---- batch: 040 ----
mean loss: 154.41
 ---- batch: 050 ----
mean loss: 154.81
 ---- batch: 060 ----
mean loss: 160.33
 ---- batch: 070 ----
mean loss: 161.59
 ---- batch: 080 ----
mean loss: 162.41
 ---- batch: 090 ----
mean loss: 156.60
 ---- batch: 100 ----
mean loss: 160.24
 ---- batch: 110 ----
mean loss: 153.94
train mean loss: 158.67
epoch train time: 0:00:00.563237
elapsed time: 0:01:37.159174
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:30:45.367188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.06
 ---- batch: 020 ----
mean loss: 154.32
 ---- batch: 030 ----
mean loss: 151.57
 ---- batch: 040 ----
mean loss: 158.24
 ---- batch: 050 ----
mean loss: 159.69
 ---- batch: 060 ----
mean loss: 156.70
 ---- batch: 070 ----
mean loss: 159.88
 ---- batch: 080 ----
mean loss: 163.28
 ---- batch: 090 ----
mean loss: 159.02
 ---- batch: 100 ----
mean loss: 160.33
 ---- batch: 110 ----
mean loss: 159.29
train mean loss: 157.93
epoch train time: 0:00:00.572245
elapsed time: 0:01:37.731555
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:30:45.939568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.23
 ---- batch: 020 ----
mean loss: 150.24
 ---- batch: 030 ----
mean loss: 155.69
 ---- batch: 040 ----
mean loss: 155.18
 ---- batch: 050 ----
mean loss: 161.73
 ---- batch: 060 ----
mean loss: 160.56
 ---- batch: 070 ----
mean loss: 159.59
 ---- batch: 080 ----
mean loss: 158.76
 ---- batch: 090 ----
mean loss: 164.57
 ---- batch: 100 ----
mean loss: 160.27
 ---- batch: 110 ----
mean loss: 158.71
train mean loss: 158.02
epoch train time: 0:00:00.557736
elapsed time: 0:01:38.289430
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:30:46.497464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.03
 ---- batch: 020 ----
mean loss: 145.63
 ---- batch: 030 ----
mean loss: 163.86
 ---- batch: 040 ----
mean loss: 159.81
 ---- batch: 050 ----
mean loss: 154.92
 ---- batch: 060 ----
mean loss: 153.82
 ---- batch: 070 ----
mean loss: 164.16
 ---- batch: 080 ----
mean loss: 152.10
 ---- batch: 090 ----
mean loss: 164.53
 ---- batch: 100 ----
mean loss: 156.65
 ---- batch: 110 ----
mean loss: 158.49
train mean loss: 157.31
epoch train time: 0:00:00.562927
elapsed time: 0:01:38.852509
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:30:47.060517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.86
 ---- batch: 020 ----
mean loss: 151.63
 ---- batch: 030 ----
mean loss: 152.80
 ---- batch: 040 ----
mean loss: 152.06
 ---- batch: 050 ----
mean loss: 147.27
 ---- batch: 060 ----
mean loss: 154.05
 ---- batch: 070 ----
mean loss: 158.29
 ---- batch: 080 ----
mean loss: 162.06
 ---- batch: 090 ----
mean loss: 159.96
 ---- batch: 100 ----
mean loss: 167.11
 ---- batch: 110 ----
mean loss: 161.21
train mean loss: 156.74
epoch train time: 0:00:00.543084
elapsed time: 0:01:39.395734
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:30:47.603742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.14
 ---- batch: 020 ----
mean loss: 163.70
 ---- batch: 030 ----
mean loss: 153.64
 ---- batch: 040 ----
mean loss: 151.35
 ---- batch: 050 ----
mean loss: 163.24
 ---- batch: 060 ----
mean loss: 163.52
 ---- batch: 070 ----
mean loss: 163.85
 ---- batch: 080 ----
mean loss: 159.15
 ---- batch: 090 ----
mean loss: 152.54
 ---- batch: 100 ----
mean loss: 158.60
 ---- batch: 110 ----
mean loss: 149.94
train mean loss: 158.10
epoch train time: 0:00:00.552049
elapsed time: 0:01:39.947912
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:30:48.155920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.21
 ---- batch: 020 ----
mean loss: 150.31
 ---- batch: 030 ----
mean loss: 152.66
 ---- batch: 040 ----
mean loss: 160.47
 ---- batch: 050 ----
mean loss: 151.63
 ---- batch: 060 ----
mean loss: 151.35
 ---- batch: 070 ----
mean loss: 155.68
 ---- batch: 080 ----
mean loss: 159.26
 ---- batch: 090 ----
mean loss: 163.12
 ---- batch: 100 ----
mean loss: 160.85
 ---- batch: 110 ----
mean loss: 150.38
train mean loss: 155.36
epoch train time: 0:00:00.560842
elapsed time: 0:01:40.508886
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:30:48.716897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.12
 ---- batch: 020 ----
mean loss: 143.46
 ---- batch: 030 ----
mean loss: 151.66
 ---- batch: 040 ----
mean loss: 157.29
 ---- batch: 050 ----
mean loss: 164.21
 ---- batch: 060 ----
mean loss: 162.50
 ---- batch: 070 ----
mean loss: 165.29
 ---- batch: 080 ----
mean loss: 149.53
 ---- batch: 090 ----
mean loss: 156.46
 ---- batch: 100 ----
mean loss: 154.09
 ---- batch: 110 ----
mean loss: 154.70
train mean loss: 155.61
epoch train time: 0:00:00.560617
elapsed time: 0:01:41.069664
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:30:49.277680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.39
 ---- batch: 020 ----
mean loss: 146.72
 ---- batch: 030 ----
mean loss: 157.39
 ---- batch: 040 ----
mean loss: 160.58
 ---- batch: 050 ----
mean loss: 145.63
 ---- batch: 060 ----
mean loss: 155.38
 ---- batch: 070 ----
mean loss: 163.84
 ---- batch: 080 ----
mean loss: 162.42
 ---- batch: 090 ----
mean loss: 155.55
 ---- batch: 100 ----
mean loss: 151.29
 ---- batch: 110 ----
mean loss: 161.13
train mean loss: 155.69
epoch train time: 0:00:00.562451
elapsed time: 0:01:41.632253
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:30:49.840265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.71
 ---- batch: 020 ----
mean loss: 144.38
 ---- batch: 030 ----
mean loss: 154.36
 ---- batch: 040 ----
mean loss: 157.05
 ---- batch: 050 ----
mean loss: 156.09
 ---- batch: 060 ----
mean loss: 155.51
 ---- batch: 070 ----
mean loss: 155.88
 ---- batch: 080 ----
mean loss: 160.02
 ---- batch: 090 ----
mean loss: 156.56
 ---- batch: 100 ----
mean loss: 154.88
 ---- batch: 110 ----
mean loss: 153.84
train mean loss: 155.01
epoch train time: 0:00:00.566122
elapsed time: 0:01:42.198528
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:30:50.406549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.62
 ---- batch: 020 ----
mean loss: 154.19
 ---- batch: 030 ----
mean loss: 154.37
 ---- batch: 040 ----
mean loss: 138.82
 ---- batch: 050 ----
mean loss: 167.53
 ---- batch: 060 ----
mean loss: 157.37
 ---- batch: 070 ----
mean loss: 154.61
 ---- batch: 080 ----
mean loss: 154.64
 ---- batch: 090 ----
mean loss: 158.30
 ---- batch: 100 ----
mean loss: 152.42
 ---- batch: 110 ----
mean loss: 156.67
train mean loss: 155.29
epoch train time: 0:00:00.550417
elapsed time: 0:01:42.749089
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:30:50.957098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.02
 ---- batch: 020 ----
mean loss: 142.04
 ---- batch: 030 ----
mean loss: 156.48
 ---- batch: 040 ----
mean loss: 153.22
 ---- batch: 050 ----
mean loss: 156.10
 ---- batch: 060 ----
mean loss: 154.09
 ---- batch: 070 ----
mean loss: 157.36
 ---- batch: 080 ----
mean loss: 159.63
 ---- batch: 090 ----
mean loss: 152.67
 ---- batch: 100 ----
mean loss: 166.24
 ---- batch: 110 ----
mean loss: 146.88
train mean loss: 154.91
epoch train time: 0:00:00.553026
elapsed time: 0:01:43.302309
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:30:51.510320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.23
 ---- batch: 020 ----
mean loss: 158.59
 ---- batch: 030 ----
mean loss: 151.65
 ---- batch: 040 ----
mean loss: 151.11
 ---- batch: 050 ----
mean loss: 155.71
 ---- batch: 060 ----
mean loss: 153.51
 ---- batch: 070 ----
mean loss: 146.56
 ---- batch: 080 ----
mean loss: 154.64
 ---- batch: 090 ----
mean loss: 155.24
 ---- batch: 100 ----
mean loss: 155.06
 ---- batch: 110 ----
mean loss: 160.01
train mean loss: 153.83
epoch train time: 0:00:00.557453
elapsed time: 0:01:43.859897
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:30:52.067917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.80
 ---- batch: 020 ----
mean loss: 151.24
 ---- batch: 030 ----
mean loss: 151.87
 ---- batch: 040 ----
mean loss: 158.29
 ---- batch: 050 ----
mean loss: 148.02
 ---- batch: 060 ----
mean loss: 151.17
 ---- batch: 070 ----
mean loss: 162.43
 ---- batch: 080 ----
mean loss: 160.42
 ---- batch: 090 ----
mean loss: 158.74
 ---- batch: 100 ----
mean loss: 151.14
 ---- batch: 110 ----
mean loss: 154.50
train mean loss: 154.20
epoch train time: 0:00:00.570594
elapsed time: 0:01:44.430643
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:30:52.638657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.07
 ---- batch: 020 ----
mean loss: 153.06
 ---- batch: 030 ----
mean loss: 154.40
 ---- batch: 040 ----
mean loss: 155.97
 ---- batch: 050 ----
mean loss: 153.45
 ---- batch: 060 ----
mean loss: 150.81
 ---- batch: 070 ----
mean loss: 154.86
 ---- batch: 080 ----
mean loss: 149.14
 ---- batch: 090 ----
mean loss: 150.01
 ---- batch: 100 ----
mean loss: 155.64
 ---- batch: 110 ----
mean loss: 157.15
train mean loss: 153.44
epoch train time: 0:00:00.549435
elapsed time: 0:01:44.980214
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:30:53.188224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.35
 ---- batch: 020 ----
mean loss: 154.07
 ---- batch: 030 ----
mean loss: 148.94
 ---- batch: 040 ----
mean loss: 154.11
 ---- batch: 050 ----
mean loss: 152.57
 ---- batch: 060 ----
mean loss: 160.26
 ---- batch: 070 ----
mean loss: 158.34
 ---- batch: 080 ----
mean loss: 147.06
 ---- batch: 090 ----
mean loss: 147.63
 ---- batch: 100 ----
mean loss: 152.99
 ---- batch: 110 ----
mean loss: 149.97
train mean loss: 152.65
epoch train time: 0:00:00.546560
elapsed time: 0:01:45.526908
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:30:53.734919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.67
 ---- batch: 020 ----
mean loss: 147.05
 ---- batch: 030 ----
mean loss: 150.05
 ---- batch: 040 ----
mean loss: 150.06
 ---- batch: 050 ----
mean loss: 150.60
 ---- batch: 060 ----
mean loss: 144.58
 ---- batch: 070 ----
mean loss: 153.60
 ---- batch: 080 ----
mean loss: 151.08
 ---- batch: 090 ----
mean loss: 165.52
 ---- batch: 100 ----
mean loss: 147.67
 ---- batch: 110 ----
mean loss: 161.57
train mean loss: 153.65
epoch train time: 0:00:00.546597
elapsed time: 0:01:46.073665
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:30:54.281699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.08
 ---- batch: 020 ----
mean loss: 153.96
 ---- batch: 030 ----
mean loss: 156.05
 ---- batch: 040 ----
mean loss: 153.25
 ---- batch: 050 ----
mean loss: 153.28
 ---- batch: 060 ----
mean loss: 155.19
 ---- batch: 070 ----
mean loss: 153.46
 ---- batch: 080 ----
mean loss: 150.27
 ---- batch: 090 ----
mean loss: 149.64
 ---- batch: 100 ----
mean loss: 158.68
 ---- batch: 110 ----
mean loss: 152.28
train mean loss: 153.07
epoch train time: 0:00:00.574745
elapsed time: 0:01:46.648578
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:30:54.856599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.80
 ---- batch: 020 ----
mean loss: 157.75
 ---- batch: 030 ----
mean loss: 156.41
 ---- batch: 040 ----
mean loss: 162.24
 ---- batch: 050 ----
mean loss: 149.30
 ---- batch: 060 ----
mean loss: 155.70
 ---- batch: 070 ----
mean loss: 150.53
 ---- batch: 080 ----
mean loss: 153.09
 ---- batch: 090 ----
mean loss: 153.42
 ---- batch: 100 ----
mean loss: 146.27
 ---- batch: 110 ----
mean loss: 159.99
train mean loss: 154.23
epoch train time: 0:00:00.578011
elapsed time: 0:01:47.226739
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:30:55.434770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.27
 ---- batch: 020 ----
mean loss: 148.41
 ---- batch: 030 ----
mean loss: 154.14
 ---- batch: 040 ----
mean loss: 149.92
 ---- batch: 050 ----
mean loss: 148.44
 ---- batch: 060 ----
mean loss: 152.20
 ---- batch: 070 ----
mean loss: 158.33
 ---- batch: 080 ----
mean loss: 153.70
 ---- batch: 090 ----
mean loss: 154.87
 ---- batch: 100 ----
mean loss: 148.53
 ---- batch: 110 ----
mean loss: 148.60
train mean loss: 152.63
epoch train time: 0:00:00.563284
elapsed time: 0:01:47.790181
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:30:55.998192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.37
 ---- batch: 020 ----
mean loss: 151.55
 ---- batch: 030 ----
mean loss: 147.56
 ---- batch: 040 ----
mean loss: 151.80
 ---- batch: 050 ----
mean loss: 150.21
 ---- batch: 060 ----
mean loss: 157.02
 ---- batch: 070 ----
mean loss: 148.02
 ---- batch: 080 ----
mean loss: 152.94
 ---- batch: 090 ----
mean loss: 143.82
 ---- batch: 100 ----
mean loss: 160.77
 ---- batch: 110 ----
mean loss: 160.46
train mean loss: 151.25
epoch train time: 0:00:00.552382
elapsed time: 0:01:48.342701
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:30:56.550713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.48
 ---- batch: 020 ----
mean loss: 157.36
 ---- batch: 030 ----
mean loss: 142.14
 ---- batch: 040 ----
mean loss: 157.75
 ---- batch: 050 ----
mean loss: 147.12
 ---- batch: 060 ----
mean loss: 152.71
 ---- batch: 070 ----
mean loss: 148.17
 ---- batch: 080 ----
mean loss: 153.46
 ---- batch: 090 ----
mean loss: 148.89
 ---- batch: 100 ----
mean loss: 156.60
 ---- batch: 110 ----
mean loss: 151.94
train mean loss: 152.07
epoch train time: 0:00:00.548758
elapsed time: 0:01:48.891592
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:30:57.099602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.89
 ---- batch: 020 ----
mean loss: 152.09
 ---- batch: 030 ----
mean loss: 151.94
 ---- batch: 040 ----
mean loss: 158.83
 ---- batch: 050 ----
mean loss: 154.39
 ---- batch: 060 ----
mean loss: 154.59
 ---- batch: 070 ----
mean loss: 149.40
 ---- batch: 080 ----
mean loss: 145.26
 ---- batch: 090 ----
mean loss: 145.47
 ---- batch: 100 ----
mean loss: 143.68
 ---- batch: 110 ----
mean loss: 157.85
train mean loss: 151.03
epoch train time: 0:00:00.548629
elapsed time: 0:01:49.440353
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:30:57.648380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.18
 ---- batch: 020 ----
mean loss: 148.16
 ---- batch: 030 ----
mean loss: 153.04
 ---- batch: 040 ----
mean loss: 156.03
 ---- batch: 050 ----
mean loss: 153.01
 ---- batch: 060 ----
mean loss: 141.61
 ---- batch: 070 ----
mean loss: 151.33
 ---- batch: 080 ----
mean loss: 158.43
 ---- batch: 090 ----
mean loss: 160.68
 ---- batch: 100 ----
mean loss: 150.85
 ---- batch: 110 ----
mean loss: 149.48
train mean loss: 151.31
epoch train time: 0:00:00.568388
elapsed time: 0:01:50.008893
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:30:58.216911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.92
 ---- batch: 020 ----
mean loss: 150.49
 ---- batch: 030 ----
mean loss: 148.68
 ---- batch: 040 ----
mean loss: 147.12
 ---- batch: 050 ----
mean loss: 150.01
 ---- batch: 060 ----
mean loss: 151.34
 ---- batch: 070 ----
mean loss: 149.79
 ---- batch: 080 ----
mean loss: 153.48
 ---- batch: 090 ----
mean loss: 151.12
 ---- batch: 100 ----
mean loss: 150.92
 ---- batch: 110 ----
mean loss: 146.60
train mean loss: 150.62
epoch train time: 0:00:00.558758
elapsed time: 0:01:50.567805
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:30:58.775812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.30
 ---- batch: 020 ----
mean loss: 151.91
 ---- batch: 030 ----
mean loss: 146.46
 ---- batch: 040 ----
mean loss: 148.38
 ---- batch: 050 ----
mean loss: 142.50
 ---- batch: 060 ----
mean loss: 149.24
 ---- batch: 070 ----
mean loss: 150.81
 ---- batch: 080 ----
mean loss: 152.03
 ---- batch: 090 ----
mean loss: 146.85
 ---- batch: 100 ----
mean loss: 158.17
 ---- batch: 110 ----
mean loss: 163.24
train mean loss: 150.56
epoch train time: 0:00:00.548697
elapsed time: 0:01:51.116632
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:30:59.324643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.05
 ---- batch: 020 ----
mean loss: 140.41
 ---- batch: 030 ----
mean loss: 149.21
 ---- batch: 040 ----
mean loss: 150.55
 ---- batch: 050 ----
mean loss: 154.10
 ---- batch: 060 ----
mean loss: 144.13
 ---- batch: 070 ----
mean loss: 159.66
 ---- batch: 080 ----
mean loss: 154.83
 ---- batch: 090 ----
mean loss: 155.68
 ---- batch: 100 ----
mean loss: 156.33
 ---- batch: 110 ----
mean loss: 144.00
train mean loss: 150.98
epoch train time: 0:00:00.556818
elapsed time: 0:01:51.673585
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:30:59.881654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.04
 ---- batch: 020 ----
mean loss: 143.47
 ---- batch: 030 ----
mean loss: 153.64
 ---- batch: 040 ----
mean loss: 152.18
 ---- batch: 050 ----
mean loss: 146.37
 ---- batch: 060 ----
mean loss: 150.80
 ---- batch: 070 ----
mean loss: 144.95
 ---- batch: 080 ----
mean loss: 150.48
 ---- batch: 090 ----
mean loss: 153.06
 ---- batch: 100 ----
mean loss: 154.06
 ---- batch: 110 ----
mean loss: 150.10
train mean loss: 149.70
epoch train time: 0:00:00.551977
elapsed time: 0:01:52.225754
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:31:00.433764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.52
 ---- batch: 020 ----
mean loss: 155.72
 ---- batch: 030 ----
mean loss: 155.59
 ---- batch: 040 ----
mean loss: 151.37
 ---- batch: 050 ----
mean loss: 155.29
 ---- batch: 060 ----
mean loss: 148.95
 ---- batch: 070 ----
mean loss: 147.52
 ---- batch: 080 ----
mean loss: 154.27
 ---- batch: 090 ----
mean loss: 150.41
 ---- batch: 100 ----
mean loss: 145.96
 ---- batch: 110 ----
mean loss: 146.50
train mean loss: 150.26
epoch train time: 0:00:00.562224
elapsed time: 0:01:52.788109
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:31:00.996118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.47
 ---- batch: 020 ----
mean loss: 149.44
 ---- batch: 030 ----
mean loss: 140.94
 ---- batch: 040 ----
mean loss: 149.59
 ---- batch: 050 ----
mean loss: 147.09
 ---- batch: 060 ----
mean loss: 151.74
 ---- batch: 070 ----
mean loss: 154.97
 ---- batch: 080 ----
mean loss: 147.93
 ---- batch: 090 ----
mean loss: 148.04
 ---- batch: 100 ----
mean loss: 146.25
 ---- batch: 110 ----
mean loss: 153.20
train mean loss: 148.97
epoch train time: 0:00:00.562596
elapsed time: 0:01:53.350847
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:31:01.558878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.86
 ---- batch: 020 ----
mean loss: 147.94
 ---- batch: 030 ----
mean loss: 146.22
 ---- batch: 040 ----
mean loss: 147.19
 ---- batch: 050 ----
mean loss: 153.52
 ---- batch: 060 ----
mean loss: 147.33
 ---- batch: 070 ----
mean loss: 152.72
 ---- batch: 080 ----
mean loss: 149.57
 ---- batch: 090 ----
mean loss: 153.06
 ---- batch: 100 ----
mean loss: 160.26
 ---- batch: 110 ----
mean loss: 143.18
train mean loss: 149.65
epoch train time: 0:00:00.560915
elapsed time: 0:01:53.911938
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:31:02.119950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.02
 ---- batch: 020 ----
mean loss: 148.67
 ---- batch: 030 ----
mean loss: 152.53
 ---- batch: 040 ----
mean loss: 145.04
 ---- batch: 050 ----
mean loss: 147.18
 ---- batch: 060 ----
mean loss: 143.83
 ---- batch: 070 ----
mean loss: 150.55
 ---- batch: 080 ----
mean loss: 160.12
 ---- batch: 090 ----
mean loss: 153.60
 ---- batch: 100 ----
mean loss: 154.94
 ---- batch: 110 ----
mean loss: 146.65
train mean loss: 149.59
epoch train time: 0:00:00.549723
elapsed time: 0:01:54.461820
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:31:02.669831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.25
 ---- batch: 020 ----
mean loss: 145.78
 ---- batch: 030 ----
mean loss: 142.40
 ---- batch: 040 ----
mean loss: 137.65
 ---- batch: 050 ----
mean loss: 139.81
 ---- batch: 060 ----
mean loss: 148.88
 ---- batch: 070 ----
mean loss: 153.48
 ---- batch: 080 ----
mean loss: 154.82
 ---- batch: 090 ----
mean loss: 151.99
 ---- batch: 100 ----
mean loss: 154.43
 ---- batch: 110 ----
mean loss: 149.14
train mean loss: 148.44
epoch train time: 0:00:00.568116
elapsed time: 0:01:55.030071
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:31:03.238084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.29
 ---- batch: 020 ----
mean loss: 141.33
 ---- batch: 030 ----
mean loss: 150.14
 ---- batch: 040 ----
mean loss: 143.14
 ---- batch: 050 ----
mean loss: 149.51
 ---- batch: 060 ----
mean loss: 155.08
 ---- batch: 070 ----
mean loss: 145.92
 ---- batch: 080 ----
mean loss: 155.38
 ---- batch: 090 ----
mean loss: 154.88
 ---- batch: 100 ----
mean loss: 142.36
 ---- batch: 110 ----
mean loss: 145.56
train mean loss: 148.06
epoch train time: 0:00:00.551805
elapsed time: 0:01:55.582014
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:31:03.790024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.52
 ---- batch: 020 ----
mean loss: 143.12
 ---- batch: 030 ----
mean loss: 150.03
 ---- batch: 040 ----
mean loss: 144.63
 ---- batch: 050 ----
mean loss: 147.24
 ---- batch: 060 ----
mean loss: 153.22
 ---- batch: 070 ----
mean loss: 150.28
 ---- batch: 080 ----
mean loss: 145.68
 ---- batch: 090 ----
mean loss: 152.34
 ---- batch: 100 ----
mean loss: 152.15
 ---- batch: 110 ----
mean loss: 150.03
train mean loss: 148.99
epoch train time: 0:00:00.555102
elapsed time: 0:01:56.137246
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:31:04.345287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.72
 ---- batch: 020 ----
mean loss: 142.97
 ---- batch: 030 ----
mean loss: 144.97
 ---- batch: 040 ----
mean loss: 141.77
 ---- batch: 050 ----
mean loss: 145.51
 ---- batch: 060 ----
mean loss: 145.51
 ---- batch: 070 ----
mean loss: 145.95
 ---- batch: 080 ----
mean loss: 154.78
 ---- batch: 090 ----
mean loss: 151.95
 ---- batch: 100 ----
mean loss: 149.80
 ---- batch: 110 ----
mean loss: 144.68
train mean loss: 147.83
epoch train time: 0:00:00.550150
elapsed time: 0:01:56.687558
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:31:04.895586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.82
 ---- batch: 020 ----
mean loss: 135.59
 ---- batch: 030 ----
mean loss: 146.42
 ---- batch: 040 ----
mean loss: 145.83
 ---- batch: 050 ----
mean loss: 152.66
 ---- batch: 060 ----
mean loss: 155.39
 ---- batch: 070 ----
mean loss: 150.53
 ---- batch: 080 ----
mean loss: 148.03
 ---- batch: 090 ----
mean loss: 145.25
 ---- batch: 100 ----
mean loss: 146.34
 ---- batch: 110 ----
mean loss: 153.51
train mean loss: 148.30
epoch train time: 0:00:00.549469
elapsed time: 0:01:57.237173
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:31:05.445181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.61
 ---- batch: 020 ----
mean loss: 144.82
 ---- batch: 030 ----
mean loss: 146.00
 ---- batch: 040 ----
mean loss: 147.05
 ---- batch: 050 ----
mean loss: 147.73
 ---- batch: 060 ----
mean loss: 145.32
 ---- batch: 070 ----
mean loss: 146.27
 ---- batch: 080 ----
mean loss: 145.83
 ---- batch: 090 ----
mean loss: 153.18
 ---- batch: 100 ----
mean loss: 145.94
 ---- batch: 110 ----
mean loss: 152.75
train mean loss: 146.95
epoch train time: 0:00:00.547774
elapsed time: 0:01:57.785076
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:31:05.993086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.93
 ---- batch: 020 ----
mean loss: 144.22
 ---- batch: 030 ----
mean loss: 152.38
 ---- batch: 040 ----
mean loss: 146.04
 ---- batch: 050 ----
mean loss: 144.18
 ---- batch: 060 ----
mean loss: 144.67
 ---- batch: 070 ----
mean loss: 155.94
 ---- batch: 080 ----
mean loss: 148.06
 ---- batch: 090 ----
mean loss: 149.74
 ---- batch: 100 ----
mean loss: 149.53
 ---- batch: 110 ----
mean loss: 155.54
train mean loss: 148.39
epoch train time: 0:00:00.549937
elapsed time: 0:01:58.335145
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:31:06.543154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.93
 ---- batch: 020 ----
mean loss: 141.96
 ---- batch: 030 ----
mean loss: 153.54
 ---- batch: 040 ----
mean loss: 156.61
 ---- batch: 050 ----
mean loss: 149.65
 ---- batch: 060 ----
mean loss: 148.02
 ---- batch: 070 ----
mean loss: 152.22
 ---- batch: 080 ----
mean loss: 147.32
 ---- batch: 090 ----
mean loss: 141.38
 ---- batch: 100 ----
mean loss: 153.35
 ---- batch: 110 ----
mean loss: 147.04
train mean loss: 148.87
epoch train time: 0:00:00.548760
elapsed time: 0:01:58.884041
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:31:07.092051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.86
 ---- batch: 020 ----
mean loss: 140.60
 ---- batch: 030 ----
mean loss: 152.02
 ---- batch: 040 ----
mean loss: 146.73
 ---- batch: 050 ----
mean loss: 136.74
 ---- batch: 060 ----
mean loss: 153.04
 ---- batch: 070 ----
mean loss: 137.41
 ---- batch: 080 ----
mean loss: 146.99
 ---- batch: 090 ----
mean loss: 154.86
 ---- batch: 100 ----
mean loss: 148.33
 ---- batch: 110 ----
mean loss: 154.76
train mean loss: 146.69
epoch train time: 0:00:00.544115
elapsed time: 0:01:59.428289
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:31:07.636300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.06
 ---- batch: 020 ----
mean loss: 143.18
 ---- batch: 030 ----
mean loss: 144.20
 ---- batch: 040 ----
mean loss: 145.89
 ---- batch: 050 ----
mean loss: 148.16
 ---- batch: 060 ----
mean loss: 141.33
 ---- batch: 070 ----
mean loss: 149.73
 ---- batch: 080 ----
mean loss: 146.67
 ---- batch: 090 ----
mean loss: 136.02
 ---- batch: 100 ----
mean loss: 151.23
 ---- batch: 110 ----
mean loss: 151.52
train mean loss: 146.19
epoch train time: 0:00:00.570264
elapsed time: 0:01:59.998688
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:31:08.206698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.45
 ---- batch: 020 ----
mean loss: 145.71
 ---- batch: 030 ----
mean loss: 138.15
 ---- batch: 040 ----
mean loss: 152.03
 ---- batch: 050 ----
mean loss: 149.60
 ---- batch: 060 ----
mean loss: 137.82
 ---- batch: 070 ----
mean loss: 145.39
 ---- batch: 080 ----
mean loss: 148.42
 ---- batch: 090 ----
mean loss: 144.04
 ---- batch: 100 ----
mean loss: 155.65
 ---- batch: 110 ----
mean loss: 144.10
train mean loss: 146.56
epoch train time: 0:00:00.552453
elapsed time: 0:02:00.551274
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:31:08.759306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.67
 ---- batch: 020 ----
mean loss: 144.43
 ---- batch: 030 ----
mean loss: 139.95
 ---- batch: 040 ----
mean loss: 151.61
 ---- batch: 050 ----
mean loss: 153.75
 ---- batch: 060 ----
mean loss: 141.23
 ---- batch: 070 ----
mean loss: 135.48
 ---- batch: 080 ----
mean loss: 156.45
 ---- batch: 090 ----
mean loss: 155.96
 ---- batch: 100 ----
mean loss: 139.22
 ---- batch: 110 ----
mean loss: 142.51
train mean loss: 145.69
epoch train time: 0:00:00.552536
elapsed time: 0:02:01.103965
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:31:09.311976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.87
 ---- batch: 020 ----
mean loss: 146.37
 ---- batch: 030 ----
mean loss: 145.02
 ---- batch: 040 ----
mean loss: 149.18
 ---- batch: 050 ----
mean loss: 153.64
 ---- batch: 060 ----
mean loss: 146.34
 ---- batch: 070 ----
mean loss: 143.47
 ---- batch: 080 ----
mean loss: 145.82
 ---- batch: 090 ----
mean loss: 142.41
 ---- batch: 100 ----
mean loss: 149.82
 ---- batch: 110 ----
mean loss: 146.77
train mean loss: 146.08
epoch train time: 0:00:00.557590
elapsed time: 0:02:01.661726
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:31:09.869749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.69
 ---- batch: 020 ----
mean loss: 148.44
 ---- batch: 030 ----
mean loss: 146.72
 ---- batch: 040 ----
mean loss: 137.85
 ---- batch: 050 ----
mean loss: 156.57
 ---- batch: 060 ----
mean loss: 147.05
 ---- batch: 070 ----
mean loss: 143.34
 ---- batch: 080 ----
mean loss: 127.97
 ---- batch: 090 ----
mean loss: 136.97
 ---- batch: 100 ----
mean loss: 151.30
 ---- batch: 110 ----
mean loss: 152.25
train mean loss: 145.11
epoch train time: 0:00:00.580172
elapsed time: 0:02:02.242079
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:31:10.450092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.58
 ---- batch: 020 ----
mean loss: 145.68
 ---- batch: 030 ----
mean loss: 149.15
 ---- batch: 040 ----
mean loss: 151.44
 ---- batch: 050 ----
mean loss: 146.16
 ---- batch: 060 ----
mean loss: 146.75
 ---- batch: 070 ----
mean loss: 141.91
 ---- batch: 080 ----
mean loss: 146.56
 ---- batch: 090 ----
mean loss: 142.41
 ---- batch: 100 ----
mean loss: 138.12
 ---- batch: 110 ----
mean loss: 147.41
train mean loss: 145.50
epoch train time: 0:00:00.553833
elapsed time: 0:02:02.796049
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:31:11.004060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.49
 ---- batch: 020 ----
mean loss: 149.09
 ---- batch: 030 ----
mean loss: 143.16
 ---- batch: 040 ----
mean loss: 145.39
 ---- batch: 050 ----
mean loss: 146.07
 ---- batch: 060 ----
mean loss: 140.42
 ---- batch: 070 ----
mean loss: 154.04
 ---- batch: 080 ----
mean loss: 145.33
 ---- batch: 090 ----
mean loss: 137.66
 ---- batch: 100 ----
mean loss: 148.39
 ---- batch: 110 ----
mean loss: 141.61
train mean loss: 144.92
epoch train time: 0:00:00.551875
elapsed time: 0:02:03.348090
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:31:11.556100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.78
 ---- batch: 020 ----
mean loss: 136.94
 ---- batch: 030 ----
mean loss: 145.77
 ---- batch: 040 ----
mean loss: 143.57
 ---- batch: 050 ----
mean loss: 158.25
 ---- batch: 060 ----
mean loss: 144.22
 ---- batch: 070 ----
mean loss: 147.39
 ---- batch: 080 ----
mean loss: 149.44
 ---- batch: 090 ----
mean loss: 152.75
 ---- batch: 100 ----
mean loss: 140.09
 ---- batch: 110 ----
mean loss: 135.27
train mean loss: 145.76
epoch train time: 0:00:00.559366
elapsed time: 0:02:03.907620
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:31:12.115663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.54
 ---- batch: 020 ----
mean loss: 143.47
 ---- batch: 030 ----
mean loss: 140.77
 ---- batch: 040 ----
mean loss: 146.36
 ---- batch: 050 ----
mean loss: 139.11
 ---- batch: 060 ----
mean loss: 144.02
 ---- batch: 070 ----
mean loss: 132.38
 ---- batch: 080 ----
mean loss: 153.62
 ---- batch: 090 ----
mean loss: 147.00
 ---- batch: 100 ----
mean loss: 157.16
 ---- batch: 110 ----
mean loss: 142.09
train mean loss: 144.31
epoch train time: 0:00:00.550608
elapsed time: 0:02:04.458423
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:31:12.666433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.94
 ---- batch: 020 ----
mean loss: 142.69
 ---- batch: 030 ----
mean loss: 138.52
 ---- batch: 040 ----
mean loss: 136.98
 ---- batch: 050 ----
mean loss: 145.79
 ---- batch: 060 ----
mean loss: 142.26
 ---- batch: 070 ----
mean loss: 145.47
 ---- batch: 080 ----
mean loss: 140.65
 ---- batch: 090 ----
mean loss: 153.27
 ---- batch: 100 ----
mean loss: 151.10
 ---- batch: 110 ----
mean loss: 148.91
train mean loss: 144.25
epoch train time: 0:00:00.553411
elapsed time: 0:02:05.011979
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:31:13.219989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.22
 ---- batch: 020 ----
mean loss: 142.78
 ---- batch: 030 ----
mean loss: 141.17
 ---- batch: 040 ----
mean loss: 155.74
 ---- batch: 050 ----
mean loss: 153.35
 ---- batch: 060 ----
mean loss: 139.39
 ---- batch: 070 ----
mean loss: 136.39
 ---- batch: 080 ----
mean loss: 146.39
 ---- batch: 090 ----
mean loss: 145.66
 ---- batch: 100 ----
mean loss: 139.60
 ---- batch: 110 ----
mean loss: 147.08
train mean loss: 144.02
epoch train time: 0:00:00.556932
elapsed time: 0:02:05.569048
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:31:13.777062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.48
 ---- batch: 020 ----
mean loss: 145.49
 ---- batch: 030 ----
mean loss: 132.96
 ---- batch: 040 ----
mean loss: 151.52
 ---- batch: 050 ----
mean loss: 149.98
 ---- batch: 060 ----
mean loss: 147.67
 ---- batch: 070 ----
mean loss: 144.92
 ---- batch: 080 ----
mean loss: 150.19
 ---- batch: 090 ----
mean loss: 143.34
 ---- batch: 100 ----
mean loss: 148.43
 ---- batch: 110 ----
mean loss: 139.33
train mean loss: 144.32
epoch train time: 0:00:00.562977
elapsed time: 0:02:06.132158
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:31:14.340167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.98
 ---- batch: 020 ----
mean loss: 143.78
 ---- batch: 030 ----
mean loss: 139.59
 ---- batch: 040 ----
mean loss: 141.01
 ---- batch: 050 ----
mean loss: 145.41
 ---- batch: 060 ----
mean loss: 149.01
 ---- batch: 070 ----
mean loss: 141.96
 ---- batch: 080 ----
mean loss: 148.84
 ---- batch: 090 ----
mean loss: 152.23
 ---- batch: 100 ----
mean loss: 141.04
 ---- batch: 110 ----
mean loss: 141.69
train mean loss: 144.56
epoch train time: 0:00:00.556006
elapsed time: 0:02:06.688296
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:31:14.896306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.94
 ---- batch: 020 ----
mean loss: 150.04
 ---- batch: 030 ----
mean loss: 148.80
 ---- batch: 040 ----
mean loss: 142.79
 ---- batch: 050 ----
mean loss: 136.98
 ---- batch: 060 ----
mean loss: 138.40
 ---- batch: 070 ----
mean loss: 152.45
 ---- batch: 080 ----
mean loss: 139.52
 ---- batch: 090 ----
mean loss: 147.95
 ---- batch: 100 ----
mean loss: 147.47
 ---- batch: 110 ----
mean loss: 143.50
train mean loss: 143.93
epoch train time: 0:00:00.548205
elapsed time: 0:02:07.236639
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:31:15.444654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.72
 ---- batch: 020 ----
mean loss: 145.40
 ---- batch: 030 ----
mean loss: 141.75
 ---- batch: 040 ----
mean loss: 139.36
 ---- batch: 050 ----
mean loss: 143.65
 ---- batch: 060 ----
mean loss: 144.06
 ---- batch: 070 ----
mean loss: 138.94
 ---- batch: 080 ----
mean loss: 145.26
 ---- batch: 090 ----
mean loss: 149.92
 ---- batch: 100 ----
mean loss: 145.12
 ---- batch: 110 ----
mean loss: 150.02
train mean loss: 143.70
epoch train time: 0:00:00.552595
elapsed time: 0:02:07.789402
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:31:15.997431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.06
 ---- batch: 020 ----
mean loss: 139.52
 ---- batch: 030 ----
mean loss: 146.34
 ---- batch: 040 ----
mean loss: 141.27
 ---- batch: 050 ----
mean loss: 133.54
 ---- batch: 060 ----
mean loss: 139.60
 ---- batch: 070 ----
mean loss: 145.36
 ---- batch: 080 ----
mean loss: 140.03
 ---- batch: 090 ----
mean loss: 149.64
 ---- batch: 100 ----
mean loss: 140.23
 ---- batch: 110 ----
mean loss: 151.67
train mean loss: 142.72
epoch train time: 0:00:00.546351
elapsed time: 0:02:08.335915
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:31:16.543917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.25
 ---- batch: 020 ----
mean loss: 145.78
 ---- batch: 030 ----
mean loss: 139.67
 ---- batch: 040 ----
mean loss: 146.07
 ---- batch: 050 ----
mean loss: 147.43
 ---- batch: 060 ----
mean loss: 141.01
 ---- batch: 070 ----
mean loss: 141.43
 ---- batch: 080 ----
mean loss: 142.83
 ---- batch: 090 ----
mean loss: 150.46
 ---- batch: 100 ----
mean loss: 148.61
 ---- batch: 110 ----
mean loss: 145.54
train mean loss: 143.67
epoch train time: 0:00:00.554385
elapsed time: 0:02:08.890447
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:31:17.098460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.98
 ---- batch: 020 ----
mean loss: 142.64
 ---- batch: 030 ----
mean loss: 142.65
 ---- batch: 040 ----
mean loss: 140.04
 ---- batch: 050 ----
mean loss: 141.99
 ---- batch: 060 ----
mean loss: 143.13
 ---- batch: 070 ----
mean loss: 149.93
 ---- batch: 080 ----
mean loss: 141.25
 ---- batch: 090 ----
mean loss: 144.83
 ---- batch: 100 ----
mean loss: 143.86
 ---- batch: 110 ----
mean loss: 140.40
train mean loss: 143.37
epoch train time: 0:00:00.557280
elapsed time: 0:02:09.447863
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:31:17.655875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.49
 ---- batch: 020 ----
mean loss: 143.16
 ---- batch: 030 ----
mean loss: 135.55
 ---- batch: 040 ----
mean loss: 151.54
 ---- batch: 050 ----
mean loss: 145.69
 ---- batch: 060 ----
mean loss: 145.02
 ---- batch: 070 ----
mean loss: 139.12
 ---- batch: 080 ----
mean loss: 146.93
 ---- batch: 090 ----
mean loss: 136.77
 ---- batch: 100 ----
mean loss: 143.07
 ---- batch: 110 ----
mean loss: 140.60
train mean loss: 142.71
epoch train time: 0:00:00.556371
elapsed time: 0:02:10.004397
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:31:18.212414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.74
 ---- batch: 020 ----
mean loss: 146.65
 ---- batch: 030 ----
mean loss: 147.01
 ---- batch: 040 ----
mean loss: 137.15
 ---- batch: 050 ----
mean loss: 143.80
 ---- batch: 060 ----
mean loss: 142.86
 ---- batch: 070 ----
mean loss: 139.94
 ---- batch: 080 ----
mean loss: 142.55
 ---- batch: 090 ----
mean loss: 142.43
 ---- batch: 100 ----
mean loss: 143.39
 ---- batch: 110 ----
mean loss: 141.27
train mean loss: 142.81
epoch train time: 0:00:00.549462
elapsed time: 0:02:10.554006
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:31:18.762018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.88
 ---- batch: 020 ----
mean loss: 148.49
 ---- batch: 030 ----
mean loss: 141.16
 ---- batch: 040 ----
mean loss: 133.56
 ---- batch: 050 ----
mean loss: 140.96
 ---- batch: 060 ----
mean loss: 151.47
 ---- batch: 070 ----
mean loss: 136.67
 ---- batch: 080 ----
mean loss: 141.07
 ---- batch: 090 ----
mean loss: 146.31
 ---- batch: 100 ----
mean loss: 138.56
 ---- batch: 110 ----
mean loss: 144.19
train mean loss: 142.23
epoch train time: 0:00:00.549072
elapsed time: 0:02:11.103214
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:31:19.311222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.32
 ---- batch: 020 ----
mean loss: 139.34
 ---- batch: 030 ----
mean loss: 137.11
 ---- batch: 040 ----
mean loss: 147.89
 ---- batch: 050 ----
mean loss: 142.52
 ---- batch: 060 ----
mean loss: 135.48
 ---- batch: 070 ----
mean loss: 145.80
 ---- batch: 080 ----
mean loss: 145.71
 ---- batch: 090 ----
mean loss: 139.92
 ---- batch: 100 ----
mean loss: 144.07
 ---- batch: 110 ----
mean loss: 147.66
train mean loss: 142.19
epoch train time: 0:00:00.549213
elapsed time: 0:02:11.652557
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:31:19.860598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.49
 ---- batch: 020 ----
mean loss: 142.40
 ---- batch: 030 ----
mean loss: 135.94
 ---- batch: 040 ----
mean loss: 139.44
 ---- batch: 050 ----
mean loss: 152.10
 ---- batch: 060 ----
mean loss: 142.87
 ---- batch: 070 ----
mean loss: 146.85
 ---- batch: 080 ----
mean loss: 144.32
 ---- batch: 090 ----
mean loss: 139.26
 ---- batch: 100 ----
mean loss: 143.97
 ---- batch: 110 ----
mean loss: 140.70
train mean loss: 141.96
epoch train time: 0:00:00.568479
elapsed time: 0:02:12.221200
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:31:20.429212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.97
 ---- batch: 020 ----
mean loss: 141.73
 ---- batch: 030 ----
mean loss: 148.69
 ---- batch: 040 ----
mean loss: 129.25
 ---- batch: 050 ----
mean loss: 143.18
 ---- batch: 060 ----
mean loss: 137.36
 ---- batch: 070 ----
mean loss: 142.33
 ---- batch: 080 ----
mean loss: 145.48
 ---- batch: 090 ----
mean loss: 146.82
 ---- batch: 100 ----
mean loss: 146.83
 ---- batch: 110 ----
mean loss: 149.29
train mean loss: 142.39
epoch train time: 0:00:00.564364
elapsed time: 0:02:12.785720
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:31:20.993764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.44
 ---- batch: 020 ----
mean loss: 144.78
 ---- batch: 030 ----
mean loss: 141.48
 ---- batch: 040 ----
mean loss: 145.41
 ---- batch: 050 ----
mean loss: 144.54
 ---- batch: 060 ----
mean loss: 144.07
 ---- batch: 070 ----
mean loss: 139.48
 ---- batch: 080 ----
mean loss: 141.59
 ---- batch: 090 ----
mean loss: 144.87
 ---- batch: 100 ----
mean loss: 147.12
 ---- batch: 110 ----
mean loss: 139.90
train mean loss: 143.03
epoch train time: 0:00:00.543203
elapsed time: 0:02:13.329098
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:31:21.537107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.13
 ---- batch: 020 ----
mean loss: 145.15
 ---- batch: 030 ----
mean loss: 132.81
 ---- batch: 040 ----
mean loss: 139.59
 ---- batch: 050 ----
mean loss: 131.25
 ---- batch: 060 ----
mean loss: 136.41
 ---- batch: 070 ----
mean loss: 145.36
 ---- batch: 080 ----
mean loss: 147.51
 ---- batch: 090 ----
mean loss: 154.04
 ---- batch: 100 ----
mean loss: 140.67
 ---- batch: 110 ----
mean loss: 141.84
train mean loss: 141.51
epoch train time: 0:00:00.563579
elapsed time: 0:02:13.892812
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:31:22.100823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.15
 ---- batch: 020 ----
mean loss: 144.89
 ---- batch: 030 ----
mean loss: 144.57
 ---- batch: 040 ----
mean loss: 136.81
 ---- batch: 050 ----
mean loss: 145.26
 ---- batch: 060 ----
mean loss: 142.45
 ---- batch: 070 ----
mean loss: 141.78
 ---- batch: 080 ----
mean loss: 141.25
 ---- batch: 090 ----
mean loss: 131.50
 ---- batch: 100 ----
mean loss: 143.68
 ---- batch: 110 ----
mean loss: 151.50
train mean loss: 141.72
epoch train time: 0:00:00.550590
elapsed time: 0:02:14.443535
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:31:22.651545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.30
 ---- batch: 020 ----
mean loss: 134.83
 ---- batch: 030 ----
mean loss: 148.61
 ---- batch: 040 ----
mean loss: 140.73
 ---- batch: 050 ----
mean loss: 141.18
 ---- batch: 060 ----
mean loss: 142.50
 ---- batch: 070 ----
mean loss: 145.68
 ---- batch: 080 ----
mean loss: 137.18
 ---- batch: 090 ----
mean loss: 140.32
 ---- batch: 100 ----
mean loss: 142.59
 ---- batch: 110 ----
mean loss: 145.13
train mean loss: 141.69
epoch train time: 0:00:00.554237
elapsed time: 0:02:14.997907
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:31:23.205944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.40
 ---- batch: 020 ----
mean loss: 147.21
 ---- batch: 030 ----
mean loss: 146.55
 ---- batch: 040 ----
mean loss: 136.96
 ---- batch: 050 ----
mean loss: 140.97
 ---- batch: 060 ----
mean loss: 140.20
 ---- batch: 070 ----
mean loss: 140.85
 ---- batch: 080 ----
mean loss: 141.08
 ---- batch: 090 ----
mean loss: 139.18
 ---- batch: 100 ----
mean loss: 140.50
 ---- batch: 110 ----
mean loss: 143.44
train mean loss: 140.84
epoch train time: 0:00:00.551572
elapsed time: 0:02:15.549637
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:31:23.757673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.28
 ---- batch: 020 ----
mean loss: 137.05
 ---- batch: 030 ----
mean loss: 134.95
 ---- batch: 040 ----
mean loss: 146.14
 ---- batch: 050 ----
mean loss: 144.59
 ---- batch: 060 ----
mean loss: 146.16
 ---- batch: 070 ----
mean loss: 132.70
 ---- batch: 080 ----
mean loss: 136.25
 ---- batch: 090 ----
mean loss: 145.10
 ---- batch: 100 ----
mean loss: 144.11
 ---- batch: 110 ----
mean loss: 141.69
train mean loss: 140.84
epoch train time: 0:00:00.552725
elapsed time: 0:02:16.102526
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:31:24.310538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.21
 ---- batch: 020 ----
mean loss: 142.61
 ---- batch: 030 ----
mean loss: 147.22
 ---- batch: 040 ----
mean loss: 134.70
 ---- batch: 050 ----
mean loss: 144.32
 ---- batch: 060 ----
mean loss: 133.97
 ---- batch: 070 ----
mean loss: 143.80
 ---- batch: 080 ----
mean loss: 140.16
 ---- batch: 090 ----
mean loss: 145.17
 ---- batch: 100 ----
mean loss: 139.86
 ---- batch: 110 ----
mean loss: 133.94
train mean loss: 140.40
epoch train time: 0:00:00.582077
elapsed time: 0:02:16.684739
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:31:24.892750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.19
 ---- batch: 020 ----
mean loss: 139.77
 ---- batch: 030 ----
mean loss: 153.16
 ---- batch: 040 ----
mean loss: 140.15
 ---- batch: 050 ----
mean loss: 134.00
 ---- batch: 060 ----
mean loss: 142.33
 ---- batch: 070 ----
mean loss: 136.30
 ---- batch: 080 ----
mean loss: 142.26
 ---- batch: 090 ----
mean loss: 142.97
 ---- batch: 100 ----
mean loss: 135.57
 ---- batch: 110 ----
mean loss: 141.44
train mean loss: 140.86
epoch train time: 0:00:00.546513
elapsed time: 0:02:17.231383
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:31:25.439395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.87
 ---- batch: 020 ----
mean loss: 131.39
 ---- batch: 030 ----
mean loss: 139.05
 ---- batch: 040 ----
mean loss: 135.60
 ---- batch: 050 ----
mean loss: 137.18
 ---- batch: 060 ----
mean loss: 147.76
 ---- batch: 070 ----
mean loss: 151.97
 ---- batch: 080 ----
mean loss: 137.58
 ---- batch: 090 ----
mean loss: 138.80
 ---- batch: 100 ----
mean loss: 144.18
 ---- batch: 110 ----
mean loss: 146.38
train mean loss: 140.28
epoch train time: 0:00:00.559955
elapsed time: 0:02:17.791472
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:31:25.999484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.89
 ---- batch: 020 ----
mean loss: 129.73
 ---- batch: 030 ----
mean loss: 139.60
 ---- batch: 040 ----
mean loss: 136.60
 ---- batch: 050 ----
mean loss: 140.94
 ---- batch: 060 ----
mean loss: 146.32
 ---- batch: 070 ----
mean loss: 135.49
 ---- batch: 080 ----
mean loss: 138.38
 ---- batch: 090 ----
mean loss: 144.60
 ---- batch: 100 ----
mean loss: 144.64
 ---- batch: 110 ----
mean loss: 138.84
train mean loss: 139.66
epoch train time: 0:00:00.540810
elapsed time: 0:02:18.332444
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:31:26.540453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.12
 ---- batch: 020 ----
mean loss: 140.59
 ---- batch: 030 ----
mean loss: 138.89
 ---- batch: 040 ----
mean loss: 138.00
 ---- batch: 050 ----
mean loss: 142.82
 ---- batch: 060 ----
mean loss: 146.70
 ---- batch: 070 ----
mean loss: 133.18
 ---- batch: 080 ----
mean loss: 136.05
 ---- batch: 090 ----
mean loss: 137.11
 ---- batch: 100 ----
mean loss: 140.03
 ---- batch: 110 ----
mean loss: 144.04
train mean loss: 139.40
epoch train time: 0:00:00.556527
elapsed time: 0:02:18.889123
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:31:27.097156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.86
 ---- batch: 020 ----
mean loss: 140.68
 ---- batch: 030 ----
mean loss: 136.16
 ---- batch: 040 ----
mean loss: 141.06
 ---- batch: 050 ----
mean loss: 142.53
 ---- batch: 060 ----
mean loss: 136.75
 ---- batch: 070 ----
mean loss: 138.83
 ---- batch: 080 ----
mean loss: 135.51
 ---- batch: 090 ----
mean loss: 133.05
 ---- batch: 100 ----
mean loss: 149.91
 ---- batch: 110 ----
mean loss: 149.63
train mean loss: 140.30
epoch train time: 0:00:00.563992
elapsed time: 0:02:19.453274
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:31:27.661285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.77
 ---- batch: 020 ----
mean loss: 150.52
 ---- batch: 030 ----
mean loss: 139.29
 ---- batch: 040 ----
mean loss: 137.50
 ---- batch: 050 ----
mean loss: 131.57
 ---- batch: 060 ----
mean loss: 135.02
 ---- batch: 070 ----
mean loss: 145.01
 ---- batch: 080 ----
mean loss: 140.38
 ---- batch: 090 ----
mean loss: 141.43
 ---- batch: 100 ----
mean loss: 141.20
 ---- batch: 110 ----
mean loss: 136.28
train mean loss: 139.58
epoch train time: 0:00:00.564483
elapsed time: 0:02:20.017901
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:31:28.225907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.24
 ---- batch: 020 ----
mean loss: 145.06
 ---- batch: 030 ----
mean loss: 146.31
 ---- batch: 040 ----
mean loss: 142.15
 ---- batch: 050 ----
mean loss: 130.08
 ---- batch: 060 ----
mean loss: 133.69
 ---- batch: 070 ----
mean loss: 143.04
 ---- batch: 080 ----
mean loss: 136.49
 ---- batch: 090 ----
mean loss: 140.76
 ---- batch: 100 ----
mean loss: 136.42
 ---- batch: 110 ----
mean loss: 143.01
train mean loss: 138.99
epoch train time: 0:00:00.561808
elapsed time: 0:02:20.579867
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:31:28.787909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.52
 ---- batch: 020 ----
mean loss: 134.82
 ---- batch: 030 ----
mean loss: 133.61
 ---- batch: 040 ----
mean loss: 147.00
 ---- batch: 050 ----
mean loss: 133.79
 ---- batch: 060 ----
mean loss: 134.15
 ---- batch: 070 ----
mean loss: 143.53
 ---- batch: 080 ----
mean loss: 142.67
 ---- batch: 090 ----
mean loss: 141.28
 ---- batch: 100 ----
mean loss: 133.08
 ---- batch: 110 ----
mean loss: 144.34
train mean loss: 139.41
epoch train time: 0:00:00.551759
elapsed time: 0:02:21.131789
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:31:29.339799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.49
 ---- batch: 020 ----
mean loss: 133.94
 ---- batch: 030 ----
mean loss: 141.25
 ---- batch: 040 ----
mean loss: 137.16
 ---- batch: 050 ----
mean loss: 141.29
 ---- batch: 060 ----
mean loss: 131.41
 ---- batch: 070 ----
mean loss: 148.95
 ---- batch: 080 ----
mean loss: 142.86
 ---- batch: 090 ----
mean loss: 141.47
 ---- batch: 100 ----
mean loss: 141.71
 ---- batch: 110 ----
mean loss: 136.05
train mean loss: 139.20
epoch train time: 0:00:00.567590
elapsed time: 0:02:21.699539
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:31:29.907548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.57
 ---- batch: 020 ----
mean loss: 130.64
 ---- batch: 030 ----
mean loss: 140.05
 ---- batch: 040 ----
mean loss: 141.02
 ---- batch: 050 ----
mean loss: 139.96
 ---- batch: 060 ----
mean loss: 143.18
 ---- batch: 070 ----
mean loss: 131.80
 ---- batch: 080 ----
mean loss: 137.94
 ---- batch: 090 ----
mean loss: 135.76
 ---- batch: 100 ----
mean loss: 136.35
 ---- batch: 110 ----
mean loss: 142.18
train mean loss: 138.45
epoch train time: 0:00:00.569502
elapsed time: 0:02:22.269171
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:31:30.477200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.50
 ---- batch: 020 ----
mean loss: 134.13
 ---- batch: 030 ----
mean loss: 144.39
 ---- batch: 040 ----
mean loss: 130.34
 ---- batch: 050 ----
mean loss: 136.39
 ---- batch: 060 ----
mean loss: 147.66
 ---- batch: 070 ----
mean loss: 143.56
 ---- batch: 080 ----
mean loss: 140.01
 ---- batch: 090 ----
mean loss: 136.50
 ---- batch: 100 ----
mean loss: 136.44
 ---- batch: 110 ----
mean loss: 142.60
train mean loss: 139.62
epoch train time: 0:00:00.561487
elapsed time: 0:02:22.830810
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:31:31.038820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.53
 ---- batch: 020 ----
mean loss: 129.81
 ---- batch: 030 ----
mean loss: 136.07
 ---- batch: 040 ----
mean loss: 136.83
 ---- batch: 050 ----
mean loss: 140.11
 ---- batch: 060 ----
mean loss: 142.14
 ---- batch: 070 ----
mean loss: 142.84
 ---- batch: 080 ----
mean loss: 138.19
 ---- batch: 090 ----
mean loss: 149.00
 ---- batch: 100 ----
mean loss: 140.42
 ---- batch: 110 ----
mean loss: 148.04
train mean loss: 139.75
epoch train time: 0:00:00.549817
elapsed time: 0:02:23.380769
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:31:31.588780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.03
 ---- batch: 020 ----
mean loss: 134.86
 ---- batch: 030 ----
mean loss: 141.19
 ---- batch: 040 ----
mean loss: 131.75
 ---- batch: 050 ----
mean loss: 128.23
 ---- batch: 060 ----
mean loss: 128.81
 ---- batch: 070 ----
mean loss: 124.69
 ---- batch: 080 ----
mean loss: 134.87
 ---- batch: 090 ----
mean loss: 135.68
 ---- batch: 100 ----
mean loss: 135.06
 ---- batch: 110 ----
mean loss: 133.34
train mean loss: 132.61
epoch train time: 0:00:00.551822
elapsed time: 0:02:23.932752
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:31:32.140771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.54
 ---- batch: 020 ----
mean loss: 128.26
 ---- batch: 030 ----
mean loss: 127.81
 ---- batch: 040 ----
mean loss: 131.99
 ---- batch: 050 ----
mean loss: 130.51
 ---- batch: 060 ----
mean loss: 134.27
 ---- batch: 070 ----
mean loss: 125.44
 ---- batch: 080 ----
mean loss: 139.71
 ---- batch: 090 ----
mean loss: 133.08
 ---- batch: 100 ----
mean loss: 132.57
 ---- batch: 110 ----
mean loss: 129.67
train mean loss: 131.86
epoch train time: 0:00:00.551254
elapsed time: 0:02:24.484149
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:31:32.692159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.92
 ---- batch: 020 ----
mean loss: 131.02
 ---- batch: 030 ----
mean loss: 133.25
 ---- batch: 040 ----
mean loss: 129.92
 ---- batch: 050 ----
mean loss: 133.69
 ---- batch: 060 ----
mean loss: 134.30
 ---- batch: 070 ----
mean loss: 128.33
 ---- batch: 080 ----
mean loss: 138.54
 ---- batch: 090 ----
mean loss: 132.99
 ---- batch: 100 ----
mean loss: 128.55
 ---- batch: 110 ----
mean loss: 129.63
train mean loss: 131.59
epoch train time: 0:00:00.549305
elapsed time: 0:02:25.033607
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:31:33.241644
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.24
 ---- batch: 020 ----
mean loss: 124.71
 ---- batch: 030 ----
mean loss: 129.26
 ---- batch: 040 ----
mean loss: 128.49
 ---- batch: 050 ----
mean loss: 135.75
 ---- batch: 060 ----
mean loss: 129.82
 ---- batch: 070 ----
mean loss: 136.29
 ---- batch: 080 ----
mean loss: 131.81
 ---- batch: 090 ----
mean loss: 136.82
 ---- batch: 100 ----
mean loss: 129.55
 ---- batch: 110 ----
mean loss: 128.33
train mean loss: 131.35
epoch train time: 0:00:00.545072
elapsed time: 0:02:25.578839
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:31:33.786866
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 139.47
 ---- batch: 020 ----
mean loss: 127.54
 ---- batch: 030 ----
mean loss: 128.65
 ---- batch: 040 ----
mean loss: 127.21
 ---- batch: 050 ----
mean loss: 131.21
 ---- batch: 060 ----
mean loss: 131.80
 ---- batch: 070 ----
mean loss: 133.88
 ---- batch: 080 ----
mean loss: 140.40
 ---- batch: 090 ----
mean loss: 123.59
 ---- batch: 100 ----
mean loss: 129.08
 ---- batch: 110 ----
mean loss: 136.26
train mean loss: 131.49
epoch train time: 0:00:00.554088
elapsed time: 0:02:26.133076
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:31:34.341104
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.98
 ---- batch: 020 ----
mean loss: 125.85
 ---- batch: 030 ----
mean loss: 130.36
 ---- batch: 040 ----
mean loss: 136.47
 ---- batch: 050 ----
mean loss: 123.42
 ---- batch: 060 ----
mean loss: 140.14
 ---- batch: 070 ----
mean loss: 135.14
 ---- batch: 080 ----
mean loss: 133.95
 ---- batch: 090 ----
mean loss: 132.79
 ---- batch: 100 ----
mean loss: 125.63
 ---- batch: 110 ----
mean loss: 132.04
train mean loss: 131.31
epoch train time: 0:00:00.554268
elapsed time: 0:02:26.687518
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:31:34.895543
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.86
 ---- batch: 020 ----
mean loss: 132.08
 ---- batch: 030 ----
mean loss: 134.59
 ---- batch: 040 ----
mean loss: 140.87
 ---- batch: 050 ----
mean loss: 127.50
 ---- batch: 060 ----
mean loss: 132.64
 ---- batch: 070 ----
mean loss: 123.50
 ---- batch: 080 ----
mean loss: 135.96
 ---- batch: 090 ----
mean loss: 130.60
 ---- batch: 100 ----
mean loss: 135.10
 ---- batch: 110 ----
mean loss: 129.76
train mean loss: 131.35
epoch train time: 0:00:00.545669
elapsed time: 0:02:27.233338
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:31:35.441350
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.35
 ---- batch: 020 ----
mean loss: 128.19
 ---- batch: 030 ----
mean loss: 129.24
 ---- batch: 040 ----
mean loss: 130.18
 ---- batch: 050 ----
mean loss: 134.91
 ---- batch: 060 ----
mean loss: 137.01
 ---- batch: 070 ----
mean loss: 138.50
 ---- batch: 080 ----
mean loss: 131.82
 ---- batch: 090 ----
mean loss: 130.49
 ---- batch: 100 ----
mean loss: 130.61
 ---- batch: 110 ----
mean loss: 130.95
train mean loss: 131.29
epoch train time: 0:00:00.552678
elapsed time: 0:02:27.786151
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:31:35.994160
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.81
 ---- batch: 020 ----
mean loss: 124.05
 ---- batch: 030 ----
mean loss: 134.36
 ---- batch: 040 ----
mean loss: 131.64
 ---- batch: 050 ----
mean loss: 128.10
 ---- batch: 060 ----
mean loss: 134.49
 ---- batch: 070 ----
mean loss: 133.53
 ---- batch: 080 ----
mean loss: 120.91
 ---- batch: 090 ----
mean loss: 133.62
 ---- batch: 100 ----
mean loss: 128.86
 ---- batch: 110 ----
mean loss: 143.74
train mean loss: 131.05
epoch train time: 0:00:00.553171
elapsed time: 0:02:28.339451
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:31:36.547461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.06
 ---- batch: 020 ----
mean loss: 127.54
 ---- batch: 030 ----
mean loss: 129.62
 ---- batch: 040 ----
mean loss: 136.75
 ---- batch: 050 ----
mean loss: 132.76
 ---- batch: 060 ----
mean loss: 131.87
 ---- batch: 070 ----
mean loss: 128.91
 ---- batch: 080 ----
mean loss: 125.61
 ---- batch: 090 ----
mean loss: 129.06
 ---- batch: 100 ----
mean loss: 130.18
 ---- batch: 110 ----
mean loss: 128.01
train mean loss: 131.11
epoch train time: 0:00:00.560818
elapsed time: 0:02:28.900407
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:31:37.108422
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.99
 ---- batch: 020 ----
mean loss: 133.45
 ---- batch: 030 ----
mean loss: 135.81
 ---- batch: 040 ----
mean loss: 131.23
 ---- batch: 050 ----
mean loss: 135.86
 ---- batch: 060 ----
mean loss: 127.63
 ---- batch: 070 ----
mean loss: 125.49
 ---- batch: 080 ----
mean loss: 128.46
 ---- batch: 090 ----
mean loss: 136.82
 ---- batch: 100 ----
mean loss: 124.56
 ---- batch: 110 ----
mean loss: 131.43
train mean loss: 131.20
epoch train time: 0:00:00.552351
elapsed time: 0:02:29.452898
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:31:37.660934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.19
 ---- batch: 020 ----
mean loss: 126.21
 ---- batch: 030 ----
mean loss: 130.31
 ---- batch: 040 ----
mean loss: 132.14
 ---- batch: 050 ----
mean loss: 130.03
 ---- batch: 060 ----
mean loss: 133.80
 ---- batch: 070 ----
mean loss: 133.35
 ---- batch: 080 ----
mean loss: 131.93
 ---- batch: 090 ----
mean loss: 127.44
 ---- batch: 100 ----
mean loss: 137.58
 ---- batch: 110 ----
mean loss: 129.86
train mean loss: 131.03
epoch train time: 0:00:00.557585
elapsed time: 0:02:30.010647
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:31:38.218659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.97
 ---- batch: 020 ----
mean loss: 128.50
 ---- batch: 030 ----
mean loss: 121.18
 ---- batch: 040 ----
mean loss: 140.25
 ---- batch: 050 ----
mean loss: 133.29
 ---- batch: 060 ----
mean loss: 133.23
 ---- batch: 070 ----
mean loss: 123.96
 ---- batch: 080 ----
mean loss: 130.47
 ---- batch: 090 ----
mean loss: 124.92
 ---- batch: 100 ----
mean loss: 136.73
 ---- batch: 110 ----
mean loss: 139.31
train mean loss: 131.03
epoch train time: 0:00:00.559084
elapsed time: 0:02:30.569865
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:31:38.777874
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.02
 ---- batch: 020 ----
mean loss: 126.78
 ---- batch: 030 ----
mean loss: 123.62
 ---- batch: 040 ----
mean loss: 127.28
 ---- batch: 050 ----
mean loss: 135.63
 ---- batch: 060 ----
mean loss: 126.61
 ---- batch: 070 ----
mean loss: 139.53
 ---- batch: 080 ----
mean loss: 130.98
 ---- batch: 090 ----
mean loss: 130.47
 ---- batch: 100 ----
mean loss: 133.69
 ---- batch: 110 ----
mean loss: 137.40
train mean loss: 131.08
epoch train time: 0:00:00.545823
elapsed time: 0:02:31.115819
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:31:39.323828
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.26
 ---- batch: 020 ----
mean loss: 129.24
 ---- batch: 030 ----
mean loss: 135.66
 ---- batch: 040 ----
mean loss: 135.15
 ---- batch: 050 ----
mean loss: 125.55
 ---- batch: 060 ----
mean loss: 127.96
 ---- batch: 070 ----
mean loss: 137.23
 ---- batch: 080 ----
mean loss: 126.41
 ---- batch: 090 ----
mean loss: 123.04
 ---- batch: 100 ----
mean loss: 130.46
 ---- batch: 110 ----
mean loss: 129.85
train mean loss: 131.08
epoch train time: 0:00:00.553061
elapsed time: 0:02:31.669016
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:31:39.877023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.28
 ---- batch: 020 ----
mean loss: 130.76
 ---- batch: 030 ----
mean loss: 131.08
 ---- batch: 040 ----
mean loss: 129.19
 ---- batch: 050 ----
mean loss: 127.38
 ---- batch: 060 ----
mean loss: 129.86
 ---- batch: 070 ----
mean loss: 129.70
 ---- batch: 080 ----
mean loss: 134.82
 ---- batch: 090 ----
mean loss: 133.41
 ---- batch: 100 ----
mean loss: 127.46
 ---- batch: 110 ----
mean loss: 130.55
train mean loss: 130.97
epoch train time: 0:00:00.553230
elapsed time: 0:02:32.222391
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:31:40.430408
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.76
 ---- batch: 020 ----
mean loss: 129.57
 ---- batch: 030 ----
mean loss: 132.12
 ---- batch: 040 ----
mean loss: 129.14
 ---- batch: 050 ----
mean loss: 130.04
 ---- batch: 060 ----
mean loss: 133.09
 ---- batch: 070 ----
mean loss: 130.34
 ---- batch: 080 ----
mean loss: 132.61
 ---- batch: 090 ----
mean loss: 133.33
 ---- batch: 100 ----
mean loss: 132.87
 ---- batch: 110 ----
mean loss: 130.75
train mean loss: 131.03
epoch train time: 0:00:00.561856
elapsed time: 0:02:32.784394
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:31:40.992430
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.77
 ---- batch: 020 ----
mean loss: 132.65
 ---- batch: 030 ----
mean loss: 134.48
 ---- batch: 040 ----
mean loss: 130.83
 ---- batch: 050 ----
mean loss: 130.80
 ---- batch: 060 ----
mean loss: 129.17
 ---- batch: 070 ----
mean loss: 131.11
 ---- batch: 080 ----
mean loss: 128.46
 ---- batch: 090 ----
mean loss: 130.76
 ---- batch: 100 ----
mean loss: 132.81
 ---- batch: 110 ----
mean loss: 127.33
train mean loss: 131.05
epoch train time: 0:00:00.554842
elapsed time: 0:02:33.339396
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:31:41.547438
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.32
 ---- batch: 020 ----
mean loss: 125.02
 ---- batch: 030 ----
mean loss: 127.89
 ---- batch: 040 ----
mean loss: 128.52
 ---- batch: 050 ----
mean loss: 131.33
 ---- batch: 060 ----
mean loss: 124.37
 ---- batch: 070 ----
mean loss: 131.46
 ---- batch: 080 ----
mean loss: 138.28
 ---- batch: 090 ----
mean loss: 132.51
 ---- batch: 100 ----
mean loss: 132.76
 ---- batch: 110 ----
mean loss: 135.59
train mean loss: 131.00
epoch train time: 0:00:00.573428
elapsed time: 0:02:33.913000
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:31:42.121011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.46
 ---- batch: 020 ----
mean loss: 134.40
 ---- batch: 030 ----
mean loss: 135.77
 ---- batch: 040 ----
mean loss: 130.24
 ---- batch: 050 ----
mean loss: 135.75
 ---- batch: 060 ----
mean loss: 129.91
 ---- batch: 070 ----
mean loss: 124.49
 ---- batch: 080 ----
mean loss: 131.13
 ---- batch: 090 ----
mean loss: 129.84
 ---- batch: 100 ----
mean loss: 134.56
 ---- batch: 110 ----
mean loss: 132.95
train mean loss: 130.86
epoch train time: 0:00:00.553494
elapsed time: 0:02:34.466654
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:31:42.674662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.73
 ---- batch: 020 ----
mean loss: 125.37
 ---- batch: 030 ----
mean loss: 134.62
 ---- batch: 040 ----
mean loss: 135.29
 ---- batch: 050 ----
mean loss: 127.26
 ---- batch: 060 ----
mean loss: 127.29
 ---- batch: 070 ----
mean loss: 134.91
 ---- batch: 080 ----
mean loss: 136.18
 ---- batch: 090 ----
mean loss: 127.24
 ---- batch: 100 ----
mean loss: 127.53
 ---- batch: 110 ----
mean loss: 134.38
train mean loss: 130.79
epoch train time: 0:00:00.546937
elapsed time: 0:02:35.013757
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:31:43.221766
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.93
 ---- batch: 020 ----
mean loss: 133.10
 ---- batch: 030 ----
mean loss: 135.62
 ---- batch: 040 ----
mean loss: 124.35
 ---- batch: 050 ----
mean loss: 131.55
 ---- batch: 060 ----
mean loss: 130.12
 ---- batch: 070 ----
mean loss: 129.92
 ---- batch: 080 ----
mean loss: 127.90
 ---- batch: 090 ----
mean loss: 128.28
 ---- batch: 100 ----
mean loss: 131.21
 ---- batch: 110 ----
mean loss: 128.36
train mean loss: 130.90
epoch train time: 0:00:00.544077
elapsed time: 0:02:35.558014
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:31:43.766024
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.44
 ---- batch: 020 ----
mean loss: 127.95
 ---- batch: 030 ----
mean loss: 127.90
 ---- batch: 040 ----
mean loss: 128.61
 ---- batch: 050 ----
mean loss: 135.37
 ---- batch: 060 ----
mean loss: 130.12
 ---- batch: 070 ----
mean loss: 130.84
 ---- batch: 080 ----
mean loss: 132.81
 ---- batch: 090 ----
mean loss: 134.46
 ---- batch: 100 ----
mean loss: 131.08
 ---- batch: 110 ----
mean loss: 128.76
train mean loss: 130.84
epoch train time: 0:00:00.576835
elapsed time: 0:02:36.134984
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:31:44.342995
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.57
 ---- batch: 020 ----
mean loss: 129.20
 ---- batch: 030 ----
mean loss: 131.07
 ---- batch: 040 ----
mean loss: 130.10
 ---- batch: 050 ----
mean loss: 133.53
 ---- batch: 060 ----
mean loss: 134.86
 ---- batch: 070 ----
mean loss: 130.04
 ---- batch: 080 ----
mean loss: 138.97
 ---- batch: 090 ----
mean loss: 130.00
 ---- batch: 100 ----
mean loss: 121.84
 ---- batch: 110 ----
mean loss: 133.86
train mean loss: 130.98
epoch train time: 0:00:00.554391
elapsed time: 0:02:36.689541
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:31:44.897553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.20
 ---- batch: 020 ----
mean loss: 127.15
 ---- batch: 030 ----
mean loss: 134.48
 ---- batch: 040 ----
mean loss: 135.70
 ---- batch: 050 ----
mean loss: 124.87
 ---- batch: 060 ----
mean loss: 133.03
 ---- batch: 070 ----
mean loss: 128.08
 ---- batch: 080 ----
mean loss: 122.01
 ---- batch: 090 ----
mean loss: 134.05
 ---- batch: 100 ----
mean loss: 135.39
 ---- batch: 110 ----
mean loss: 133.56
train mean loss: 130.90
epoch train time: 0:00:00.563169
elapsed time: 0:02:37.252844
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:31:45.460878
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.99
 ---- batch: 020 ----
mean loss: 138.42
 ---- batch: 030 ----
mean loss: 131.95
 ---- batch: 040 ----
mean loss: 128.53
 ---- batch: 050 ----
mean loss: 129.85
 ---- batch: 060 ----
mean loss: 118.18
 ---- batch: 070 ----
mean loss: 142.82
 ---- batch: 080 ----
mean loss: 127.31
 ---- batch: 090 ----
mean loss: 134.28
 ---- batch: 100 ----
mean loss: 131.57
 ---- batch: 110 ----
mean loss: 131.97
train mean loss: 130.60
epoch train time: 0:00:00.552591
elapsed time: 0:02:37.805592
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:31:46.013623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.70
 ---- batch: 020 ----
mean loss: 128.79
 ---- batch: 030 ----
mean loss: 134.05
 ---- batch: 040 ----
mean loss: 124.23
 ---- batch: 050 ----
mean loss: 131.77
 ---- batch: 060 ----
mean loss: 132.02
 ---- batch: 070 ----
mean loss: 136.29
 ---- batch: 080 ----
mean loss: 136.80
 ---- batch: 090 ----
mean loss: 133.67
 ---- batch: 100 ----
mean loss: 130.79
 ---- batch: 110 ----
mean loss: 131.28
train mean loss: 130.66
epoch train time: 0:00:00.545642
elapsed time: 0:02:38.351383
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:31:46.559391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.73
 ---- batch: 020 ----
mean loss: 129.00
 ---- batch: 030 ----
mean loss: 130.41
 ---- batch: 040 ----
mean loss: 131.92
 ---- batch: 050 ----
mean loss: 128.09
 ---- batch: 060 ----
mean loss: 130.88
 ---- batch: 070 ----
mean loss: 136.37
 ---- batch: 080 ----
mean loss: 131.89
 ---- batch: 090 ----
mean loss: 128.00
 ---- batch: 100 ----
mean loss: 129.19
 ---- batch: 110 ----
mean loss: 128.69
train mean loss: 130.59
epoch train time: 0:00:00.576135
elapsed time: 0:02:38.927651
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:31:47.135661
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.99
 ---- batch: 020 ----
mean loss: 134.44
 ---- batch: 030 ----
mean loss: 139.67
 ---- batch: 040 ----
mean loss: 138.32
 ---- batch: 050 ----
mean loss: 129.75
 ---- batch: 060 ----
mean loss: 132.07
 ---- batch: 070 ----
mean loss: 125.06
 ---- batch: 080 ----
mean loss: 120.26
 ---- batch: 090 ----
mean loss: 130.93
 ---- batch: 100 ----
mean loss: 132.30
 ---- batch: 110 ----
mean loss: 129.89
train mean loss: 130.61
epoch train time: 0:00:00.552931
elapsed time: 0:02:39.480743
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:31:47.688764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.89
 ---- batch: 020 ----
mean loss: 131.38
 ---- batch: 030 ----
mean loss: 123.44
 ---- batch: 040 ----
mean loss: 140.72
 ---- batch: 050 ----
mean loss: 126.77
 ---- batch: 060 ----
mean loss: 130.93
 ---- batch: 070 ----
mean loss: 126.89
 ---- batch: 080 ----
mean loss: 129.38
 ---- batch: 090 ----
mean loss: 129.81
 ---- batch: 100 ----
mean loss: 130.04
 ---- batch: 110 ----
mean loss: 135.14
train mean loss: 130.74
epoch train time: 0:00:00.557376
elapsed time: 0:02:40.038267
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:31:48.246280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.06
 ---- batch: 020 ----
mean loss: 121.73
 ---- batch: 030 ----
mean loss: 127.19
 ---- batch: 040 ----
mean loss: 138.49
 ---- batch: 050 ----
mean loss: 131.44
 ---- batch: 060 ----
mean loss: 121.71
 ---- batch: 070 ----
mean loss: 134.05
 ---- batch: 080 ----
mean loss: 134.89
 ---- batch: 090 ----
mean loss: 131.22
 ---- batch: 100 ----
mean loss: 139.88
 ---- batch: 110 ----
mean loss: 134.81
train mean loss: 130.67
epoch train time: 0:00:00.559377
elapsed time: 0:02:40.597791
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:31:48.805830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.02
 ---- batch: 020 ----
mean loss: 128.81
 ---- batch: 030 ----
mean loss: 132.78
 ---- batch: 040 ----
mean loss: 132.75
 ---- batch: 050 ----
mean loss: 122.17
 ---- batch: 060 ----
mean loss: 135.38
 ---- batch: 070 ----
mean loss: 130.68
 ---- batch: 080 ----
mean loss: 132.92
 ---- batch: 090 ----
mean loss: 131.96
 ---- batch: 100 ----
mean loss: 127.53
 ---- batch: 110 ----
mean loss: 128.17
train mean loss: 130.55
epoch train time: 0:00:00.555535
elapsed time: 0:02:41.153486
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:31:49.361499
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.55
 ---- batch: 020 ----
mean loss: 123.88
 ---- batch: 030 ----
mean loss: 130.26
 ---- batch: 040 ----
mean loss: 133.85
 ---- batch: 050 ----
mean loss: 126.85
 ---- batch: 060 ----
mean loss: 135.34
 ---- batch: 070 ----
mean loss: 134.63
 ---- batch: 080 ----
mean loss: 133.69
 ---- batch: 090 ----
mean loss: 132.05
 ---- batch: 100 ----
mean loss: 129.59
 ---- batch: 110 ----
mean loss: 131.02
train mean loss: 130.67
epoch train time: 0:00:00.564367
elapsed time: 0:02:41.718029
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:31:49.926048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.07
 ---- batch: 020 ----
mean loss: 129.89
 ---- batch: 030 ----
mean loss: 125.08
 ---- batch: 040 ----
mean loss: 131.46
 ---- batch: 050 ----
mean loss: 128.11
 ---- batch: 060 ----
mean loss: 133.30
 ---- batch: 070 ----
mean loss: 130.41
 ---- batch: 080 ----
mean loss: 132.14
 ---- batch: 090 ----
mean loss: 133.84
 ---- batch: 100 ----
mean loss: 132.23
 ---- batch: 110 ----
mean loss: 128.16
train mean loss: 130.52
epoch train time: 0:00:00.556192
elapsed time: 0:02:42.274366
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:31:50.482380
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.02
 ---- batch: 020 ----
mean loss: 125.50
 ---- batch: 030 ----
mean loss: 126.58
 ---- batch: 040 ----
mean loss: 139.29
 ---- batch: 050 ----
mean loss: 127.44
 ---- batch: 060 ----
mean loss: 134.96
 ---- batch: 070 ----
mean loss: 132.86
 ---- batch: 080 ----
mean loss: 124.62
 ---- batch: 090 ----
mean loss: 132.29
 ---- batch: 100 ----
mean loss: 133.54
 ---- batch: 110 ----
mean loss: 125.16
train mean loss: 130.51
epoch train time: 0:00:00.550607
elapsed time: 0:02:42.825117
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:31:51.033129
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.08
 ---- batch: 020 ----
mean loss: 131.52
 ---- batch: 030 ----
mean loss: 131.31
 ---- batch: 040 ----
mean loss: 134.76
 ---- batch: 050 ----
mean loss: 130.34
 ---- batch: 060 ----
mean loss: 140.56
 ---- batch: 070 ----
mean loss: 131.23
 ---- batch: 080 ----
mean loss: 127.37
 ---- batch: 090 ----
mean loss: 125.81
 ---- batch: 100 ----
mean loss: 129.29
 ---- batch: 110 ----
mean loss: 124.23
train mean loss: 130.53
epoch train time: 0:00:00.547787
elapsed time: 0:02:43.373038
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:31:51.581046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.58
 ---- batch: 020 ----
mean loss: 138.58
 ---- batch: 030 ----
mean loss: 129.73
 ---- batch: 040 ----
mean loss: 128.21
 ---- batch: 050 ----
mean loss: 130.73
 ---- batch: 060 ----
mean loss: 129.79
 ---- batch: 070 ----
mean loss: 121.78
 ---- batch: 080 ----
mean loss: 129.47
 ---- batch: 090 ----
mean loss: 129.30
 ---- batch: 100 ----
mean loss: 131.09
 ---- batch: 110 ----
mean loss: 128.62
train mean loss: 130.48
epoch train time: 0:00:00.559346
elapsed time: 0:02:43.932515
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:31:52.140525
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.39
 ---- batch: 020 ----
mean loss: 132.05
 ---- batch: 030 ----
mean loss: 127.15
 ---- batch: 040 ----
mean loss: 134.12
 ---- batch: 050 ----
mean loss: 130.23
 ---- batch: 060 ----
mean loss: 131.68
 ---- batch: 070 ----
mean loss: 139.89
 ---- batch: 080 ----
mean loss: 122.32
 ---- batch: 090 ----
mean loss: 125.27
 ---- batch: 100 ----
mean loss: 135.54
 ---- batch: 110 ----
mean loss: 124.81
train mean loss: 130.40
epoch train time: 0:00:00.548308
elapsed time: 0:02:44.480954
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:31:52.688981
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.51
 ---- batch: 020 ----
mean loss: 136.22
 ---- batch: 030 ----
mean loss: 130.88
 ---- batch: 040 ----
mean loss: 132.74
 ---- batch: 050 ----
mean loss: 128.78
 ---- batch: 060 ----
mean loss: 141.83
 ---- batch: 070 ----
mean loss: 135.48
 ---- batch: 080 ----
mean loss: 132.45
 ---- batch: 090 ----
mean loss: 125.48
 ---- batch: 100 ----
mean loss: 127.40
 ---- batch: 110 ----
mean loss: 123.60
train mean loss: 130.50
epoch train time: 0:00:00.551218
elapsed time: 0:02:45.032322
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:31:53.240336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.39
 ---- batch: 020 ----
mean loss: 124.59
 ---- batch: 030 ----
mean loss: 130.59
 ---- batch: 040 ----
mean loss: 126.28
 ---- batch: 050 ----
mean loss: 128.96
 ---- batch: 060 ----
mean loss: 128.69
 ---- batch: 070 ----
mean loss: 131.32
 ---- batch: 080 ----
mean loss: 130.90
 ---- batch: 090 ----
mean loss: 147.18
 ---- batch: 100 ----
mean loss: 124.37
 ---- batch: 110 ----
mean loss: 133.24
train mean loss: 130.39
epoch train time: 0:00:00.551555
elapsed time: 0:02:45.584014
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:31:53.792025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.87
 ---- batch: 020 ----
mean loss: 124.88
 ---- batch: 030 ----
mean loss: 128.20
 ---- batch: 040 ----
mean loss: 128.13
 ---- batch: 050 ----
mean loss: 134.64
 ---- batch: 060 ----
mean loss: 134.41
 ---- batch: 070 ----
mean loss: 132.78
 ---- batch: 080 ----
mean loss: 127.29
 ---- batch: 090 ----
mean loss: 131.06
 ---- batch: 100 ----
mean loss: 138.38
 ---- batch: 110 ----
mean loss: 125.45
train mean loss: 130.32
epoch train time: 0:00:00.550933
elapsed time: 0:02:46.135087
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:31:54.343099
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.33
 ---- batch: 020 ----
mean loss: 137.88
 ---- batch: 030 ----
mean loss: 136.71
 ---- batch: 040 ----
mean loss: 121.30
 ---- batch: 050 ----
mean loss: 125.83
 ---- batch: 060 ----
mean loss: 137.73
 ---- batch: 070 ----
mean loss: 132.58
 ---- batch: 080 ----
mean loss: 116.14
 ---- batch: 090 ----
mean loss: 123.62
 ---- batch: 100 ----
mean loss: 133.73
 ---- batch: 110 ----
mean loss: 137.35
train mean loss: 130.44
epoch train time: 0:00:00.551695
elapsed time: 0:02:46.686923
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:31:54.894936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.66
 ---- batch: 020 ----
mean loss: 139.06
 ---- batch: 030 ----
mean loss: 123.78
 ---- batch: 040 ----
mean loss: 136.61
 ---- batch: 050 ----
mean loss: 135.18
 ---- batch: 060 ----
mean loss: 127.56
 ---- batch: 070 ----
mean loss: 136.54
 ---- batch: 080 ----
mean loss: 121.22
 ---- batch: 090 ----
mean loss: 127.43
 ---- batch: 100 ----
mean loss: 135.38
 ---- batch: 110 ----
mean loss: 127.65
train mean loss: 130.35
epoch train time: 0:00:00.563577
elapsed time: 0:02:47.250642
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:31:55.458658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.28
 ---- batch: 020 ----
mean loss: 125.31
 ---- batch: 030 ----
mean loss: 125.58
 ---- batch: 040 ----
mean loss: 129.74
 ---- batch: 050 ----
mean loss: 129.53
 ---- batch: 060 ----
mean loss: 126.36
 ---- batch: 070 ----
mean loss: 136.95
 ---- batch: 080 ----
mean loss: 133.52
 ---- batch: 090 ----
mean loss: 131.27
 ---- batch: 100 ----
mean loss: 133.93
 ---- batch: 110 ----
mean loss: 136.53
train mean loss: 130.31
epoch train time: 0:00:00.565072
elapsed time: 0:02:47.815853
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:31:56.023881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.84
 ---- batch: 020 ----
mean loss: 127.09
 ---- batch: 030 ----
mean loss: 131.96
 ---- batch: 040 ----
mean loss: 129.79
 ---- batch: 050 ----
mean loss: 124.57
 ---- batch: 060 ----
mean loss: 135.93
 ---- batch: 070 ----
mean loss: 131.68
 ---- batch: 080 ----
mean loss: 127.03
 ---- batch: 090 ----
mean loss: 127.14
 ---- batch: 100 ----
mean loss: 138.97
 ---- batch: 110 ----
mean loss: 136.79
train mean loss: 130.36
epoch train time: 0:00:00.553638
elapsed time: 0:02:48.369662
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:31:56.577677
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.09
 ---- batch: 020 ----
mean loss: 132.17
 ---- batch: 030 ----
mean loss: 125.88
 ---- batch: 040 ----
mean loss: 127.00
 ---- batch: 050 ----
mean loss: 131.41
 ---- batch: 060 ----
mean loss: 127.78
 ---- batch: 070 ----
mean loss: 134.28
 ---- batch: 080 ----
mean loss: 130.41
 ---- batch: 090 ----
mean loss: 128.37
 ---- batch: 100 ----
mean loss: 131.67
 ---- batch: 110 ----
mean loss: 128.21
train mean loss: 130.23
epoch train time: 0:00:00.555516
elapsed time: 0:02:48.925316
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:31:57.133326
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.79
 ---- batch: 020 ----
mean loss: 130.39
 ---- batch: 030 ----
mean loss: 128.38
 ---- batch: 040 ----
mean loss: 126.93
 ---- batch: 050 ----
mean loss: 130.27
 ---- batch: 060 ----
mean loss: 129.84
 ---- batch: 070 ----
mean loss: 132.64
 ---- batch: 080 ----
mean loss: 125.30
 ---- batch: 090 ----
mean loss: 129.74
 ---- batch: 100 ----
mean loss: 137.88
 ---- batch: 110 ----
mean loss: 139.03
train mean loss: 130.15
epoch train time: 0:00:00.549794
elapsed time: 0:02:49.475248
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:31:57.683259
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.88
 ---- batch: 020 ----
mean loss: 128.94
 ---- batch: 030 ----
mean loss: 128.45
 ---- batch: 040 ----
mean loss: 129.76
 ---- batch: 050 ----
mean loss: 127.43
 ---- batch: 060 ----
mean loss: 134.60
 ---- batch: 070 ----
mean loss: 125.70
 ---- batch: 080 ----
mean loss: 129.90
 ---- batch: 090 ----
mean loss: 130.79
 ---- batch: 100 ----
mean loss: 135.85
 ---- batch: 110 ----
mean loss: 135.36
train mean loss: 130.32
epoch train time: 0:00:00.568731
elapsed time: 0:02:50.044120
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:31:58.252148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.68
 ---- batch: 020 ----
mean loss: 127.21
 ---- batch: 030 ----
mean loss: 122.74
 ---- batch: 040 ----
mean loss: 131.64
 ---- batch: 050 ----
mean loss: 127.04
 ---- batch: 060 ----
mean loss: 136.01
 ---- batch: 070 ----
mean loss: 126.78
 ---- batch: 080 ----
mean loss: 135.33
 ---- batch: 090 ----
mean loss: 134.65
 ---- batch: 100 ----
mean loss: 134.14
 ---- batch: 110 ----
mean loss: 130.55
train mean loss: 130.20
epoch train time: 0:00:00.565359
elapsed time: 0:02:50.612751
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_8/checkpoint.pth.tar
**** end time: 2019-09-27 02:31:58.820731 ****
