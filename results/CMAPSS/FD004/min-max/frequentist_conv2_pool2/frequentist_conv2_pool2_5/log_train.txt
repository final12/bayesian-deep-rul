Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_5', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17376
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 01:44:45.073422 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 11, 11]             560
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
            Conv2d-4            [-1, 14, 4, 11]             224
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
            Linear-8                    [-1, 1]             308
================================================================
Total params: 1,092
Trainable params: 1,092
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 01:44:45.078770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4978.61
 ---- batch: 020 ----
mean loss: 4825.03
 ---- batch: 030 ----
mean loss: 4676.55
 ---- batch: 040 ----
mean loss: 4498.18
 ---- batch: 050 ----
mean loss: 4328.94
 ---- batch: 060 ----
mean loss: 4103.49
 ---- batch: 070 ----
mean loss: 3935.42
 ---- batch: 080 ----
mean loss: 3720.18
 ---- batch: 090 ----
mean loss: 3502.42
 ---- batch: 100 ----
mean loss: 3313.04
 ---- batch: 110 ----
mean loss: 3112.75
train mean loss: 4060.16
epoch train time: 0:00:32.984045
elapsed time: 0:00:32.990829
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 01:45:18.064293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2815.01
 ---- batch: 020 ----
mean loss: 2596.95
 ---- batch: 030 ----
mean loss: 2430.23
 ---- batch: 040 ----
mean loss: 2255.77
 ---- batch: 050 ----
mean loss: 2114.99
 ---- batch: 060 ----
mean loss: 1955.45
 ---- batch: 070 ----
mean loss: 1817.19
 ---- batch: 080 ----
mean loss: 1712.63
 ---- batch: 090 ----
mean loss: 1599.67
 ---- batch: 100 ----
mean loss: 1492.57
 ---- batch: 110 ----
mean loss: 1403.06
train mean loss: 2000.77
epoch train time: 0:00:00.727893
elapsed time: 0:00:33.718861
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 01:45:18.792328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1315.33
 ---- batch: 020 ----
mean loss: 1262.26
 ---- batch: 030 ----
mean loss: 1207.44
 ---- batch: 040 ----
mean loss: 1159.12
 ---- batch: 050 ----
mean loss: 1101.47
 ---- batch: 060 ----
mean loss: 1056.41
 ---- batch: 070 ----
mean loss: 1052.03
 ---- batch: 080 ----
mean loss: 1009.68
 ---- batch: 090 ----
mean loss: 985.43
 ---- batch: 100 ----
mean loss: 963.26
 ---- batch: 110 ----
mean loss: 946.18
train mean loss: 1091.69
epoch train time: 0:00:00.722573
elapsed time: 0:00:34.441597
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 01:45:19.515065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.43
 ---- batch: 020 ----
mean loss: 916.64
 ---- batch: 030 ----
mean loss: 914.05
 ---- batch: 040 ----
mean loss: 889.38
 ---- batch: 050 ----
mean loss: 871.80
 ---- batch: 060 ----
mean loss: 873.37
 ---- batch: 070 ----
mean loss: 874.38
 ---- batch: 080 ----
mean loss: 847.96
 ---- batch: 090 ----
mean loss: 871.26
 ---- batch: 100 ----
mean loss: 873.38
 ---- batch: 110 ----
mean loss: 852.89
train mean loss: 882.80
epoch train time: 0:00:00.726439
elapsed time: 0:00:35.168202
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 01:45:20.241702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.95
 ---- batch: 020 ----
mean loss: 848.41
 ---- batch: 030 ----
mean loss: 854.50
 ---- batch: 040 ----
mean loss: 860.41
 ---- batch: 050 ----
mean loss: 853.84
 ---- batch: 060 ----
mean loss: 839.42
 ---- batch: 070 ----
mean loss: 842.80
 ---- batch: 080 ----
mean loss: 839.50
 ---- batch: 090 ----
mean loss: 840.57
 ---- batch: 100 ----
mean loss: 855.60
 ---- batch: 110 ----
mean loss: 854.01
train mean loss: 848.78
epoch train time: 0:00:00.726333
elapsed time: 0:00:35.894703
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 01:45:20.968173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.10
 ---- batch: 020 ----
mean loss: 842.68
 ---- batch: 030 ----
mean loss: 847.75
 ---- batch: 040 ----
mean loss: 822.45
 ---- batch: 050 ----
mean loss: 836.73
 ---- batch: 060 ----
mean loss: 857.90
 ---- batch: 070 ----
mean loss: 835.99
 ---- batch: 080 ----
mean loss: 857.07
 ---- batch: 090 ----
mean loss: 836.86
 ---- batch: 100 ----
mean loss: 844.09
 ---- batch: 110 ----
mean loss: 842.29
train mean loss: 841.97
epoch train time: 0:00:00.733765
elapsed time: 0:00:36.628648
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 01:45:21.702124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.39
 ---- batch: 020 ----
mean loss: 824.92
 ---- batch: 030 ----
mean loss: 828.30
 ---- batch: 040 ----
mean loss: 838.16
 ---- batch: 050 ----
mean loss: 811.03
 ---- batch: 060 ----
mean loss: 839.51
 ---- batch: 070 ----
mean loss: 830.08
 ---- batch: 080 ----
mean loss: 853.48
 ---- batch: 090 ----
mean loss: 848.94
 ---- batch: 100 ----
mean loss: 853.54
 ---- batch: 110 ----
mean loss: 840.72
train mean loss: 838.08
epoch train time: 0:00:00.742311
elapsed time: 0:00:37.371108
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 01:45:22.444581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.85
 ---- batch: 020 ----
mean loss: 820.50
 ---- batch: 030 ----
mean loss: 855.71
 ---- batch: 040 ----
mean loss: 836.36
 ---- batch: 050 ----
mean loss: 837.07
 ---- batch: 060 ----
mean loss: 842.38
 ---- batch: 070 ----
mean loss: 813.40
 ---- batch: 080 ----
mean loss: 832.43
 ---- batch: 090 ----
mean loss: 836.20
 ---- batch: 100 ----
mean loss: 843.41
 ---- batch: 110 ----
mean loss: 838.78
train mean loss: 834.56
epoch train time: 0:00:00.736020
elapsed time: 0:00:38.107267
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 01:45:23.180751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.02
 ---- batch: 020 ----
mean loss: 837.70
 ---- batch: 030 ----
mean loss: 818.69
 ---- batch: 040 ----
mean loss: 818.14
 ---- batch: 050 ----
mean loss: 835.11
 ---- batch: 060 ----
mean loss: 817.86
 ---- batch: 070 ----
mean loss: 841.07
 ---- batch: 080 ----
mean loss: 825.48
 ---- batch: 090 ----
mean loss: 824.41
 ---- batch: 100 ----
mean loss: 849.26
 ---- batch: 110 ----
mean loss: 837.93
train mean loss: 831.19
epoch train time: 0:00:00.734931
elapsed time: 0:00:38.842357
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 01:45:23.915831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 823.67
 ---- batch: 020 ----
mean loss: 837.04
 ---- batch: 030 ----
mean loss: 808.59
 ---- batch: 040 ----
mean loss: 841.53
 ---- batch: 050 ----
mean loss: 821.79
 ---- batch: 060 ----
mean loss: 828.84
 ---- batch: 070 ----
mean loss: 822.85
 ---- batch: 080 ----
mean loss: 857.56
 ---- batch: 090 ----
mean loss: 818.27
 ---- batch: 100 ----
mean loss: 812.06
 ---- batch: 110 ----
mean loss: 833.89
train mean loss: 827.22
epoch train time: 0:00:00.735552
elapsed time: 0:00:39.578052
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 01:45:24.651541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.65
 ---- batch: 020 ----
mean loss: 808.72
 ---- batch: 030 ----
mean loss: 824.40
 ---- batch: 040 ----
mean loss: 840.90
 ---- batch: 050 ----
mean loss: 815.76
 ---- batch: 060 ----
mean loss: 820.22
 ---- batch: 070 ----
mean loss: 825.09
 ---- batch: 080 ----
mean loss: 817.42
 ---- batch: 090 ----
mean loss: 832.90
 ---- batch: 100 ----
mean loss: 817.09
 ---- batch: 110 ----
mean loss: 819.68
train mean loss: 822.87
epoch train time: 0:00:00.726294
elapsed time: 0:00:40.304503
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 01:45:25.377973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.64
 ---- batch: 020 ----
mean loss: 800.29
 ---- batch: 030 ----
mean loss: 835.23
 ---- batch: 040 ----
mean loss: 828.21
 ---- batch: 050 ----
mean loss: 818.87
 ---- batch: 060 ----
mean loss: 811.86
 ---- batch: 070 ----
mean loss: 816.10
 ---- batch: 080 ----
mean loss: 808.92
 ---- batch: 090 ----
mean loss: 824.26
 ---- batch: 100 ----
mean loss: 813.78
 ---- batch: 110 ----
mean loss: 786.60
train mean loss: 817.94
epoch train time: 0:00:00.725621
elapsed time: 0:00:41.030308
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 01:45:26.103809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.54
 ---- batch: 020 ----
mean loss: 821.79
 ---- batch: 030 ----
mean loss: 819.21
 ---- batch: 040 ----
mean loss: 808.23
 ---- batch: 050 ----
mean loss: 810.97
 ---- batch: 060 ----
mean loss: 806.36
 ---- batch: 070 ----
mean loss: 822.35
 ---- batch: 080 ----
mean loss: 787.86
 ---- batch: 090 ----
mean loss: 808.92
 ---- batch: 100 ----
mean loss: 821.58
 ---- batch: 110 ----
mean loss: 794.09
train mean loss: 812.54
epoch train time: 0:00:00.724685
elapsed time: 0:00:41.755172
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 01:45:26.828670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 796.31
 ---- batch: 020 ----
mean loss: 810.74
 ---- batch: 030 ----
mean loss: 802.40
 ---- batch: 040 ----
mean loss: 791.71
 ---- batch: 050 ----
mean loss: 797.67
 ---- batch: 060 ----
mean loss: 812.81
 ---- batch: 070 ----
mean loss: 810.06
 ---- batch: 080 ----
mean loss: 812.47
 ---- batch: 090 ----
mean loss: 797.85
 ---- batch: 100 ----
mean loss: 814.40
 ---- batch: 110 ----
mean loss: 816.88
train mean loss: 806.49
epoch train time: 0:00:00.726857
elapsed time: 0:00:42.482194
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 01:45:27.555665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 805.60
 ---- batch: 020 ----
mean loss: 795.40
 ---- batch: 030 ----
mean loss: 811.88
 ---- batch: 040 ----
mean loss: 812.76
 ---- batch: 050 ----
mean loss: 804.38
 ---- batch: 060 ----
mean loss: 792.67
 ---- batch: 070 ----
mean loss: 797.23
 ---- batch: 080 ----
mean loss: 788.12
 ---- batch: 090 ----
mean loss: 796.84
 ---- batch: 100 ----
mean loss: 796.45
 ---- batch: 110 ----
mean loss: 815.51
train mean loss: 800.50
epoch train time: 0:00:00.731679
elapsed time: 0:00:43.214017
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 01:45:28.287487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 800.85
 ---- batch: 020 ----
mean loss: 803.49
 ---- batch: 030 ----
mean loss: 792.63
 ---- batch: 040 ----
mean loss: 780.62
 ---- batch: 050 ----
mean loss: 792.65
 ---- batch: 060 ----
mean loss: 803.27
 ---- batch: 070 ----
mean loss: 805.76
 ---- batch: 080 ----
mean loss: 782.39
 ---- batch: 090 ----
mean loss: 785.00
 ---- batch: 100 ----
mean loss: 805.89
 ---- batch: 110 ----
mean loss: 781.86
train mean loss: 794.94
epoch train time: 0:00:00.726037
elapsed time: 0:00:43.940306
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 01:45:29.013813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 786.15
 ---- batch: 020 ----
mean loss: 760.89
 ---- batch: 030 ----
mean loss: 787.90
 ---- batch: 040 ----
mean loss: 810.53
 ---- batch: 050 ----
mean loss: 808.24
 ---- batch: 060 ----
mean loss: 804.77
 ---- batch: 070 ----
mean loss: 801.33
 ---- batch: 080 ----
mean loss: 787.96
 ---- batch: 090 ----
mean loss: 774.67
 ---- batch: 100 ----
mean loss: 782.41
 ---- batch: 110 ----
mean loss: 780.07
train mean loss: 789.58
epoch train time: 0:00:00.724784
elapsed time: 0:00:44.665270
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 01:45:29.738755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 766.79
 ---- batch: 020 ----
mean loss: 791.70
 ---- batch: 030 ----
mean loss: 771.95
 ---- batch: 040 ----
mean loss: 790.33
 ---- batch: 050 ----
mean loss: 800.52
 ---- batch: 060 ----
mean loss: 767.61
 ---- batch: 070 ----
mean loss: 795.25
 ---- batch: 080 ----
mean loss: 778.64
 ---- batch: 090 ----
mean loss: 779.32
 ---- batch: 100 ----
mean loss: 796.46
 ---- batch: 110 ----
mean loss: 791.70
train mean loss: 784.11
epoch train time: 0:00:00.722414
elapsed time: 0:00:45.387836
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 01:45:30.461305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 780.48
 ---- batch: 020 ----
mean loss: 792.12
 ---- batch: 030 ----
mean loss: 779.42
 ---- batch: 040 ----
mean loss: 761.04
 ---- batch: 050 ----
mean loss: 769.55
 ---- batch: 060 ----
mean loss: 784.88
 ---- batch: 070 ----
mean loss: 777.68
 ---- batch: 080 ----
mean loss: 778.01
 ---- batch: 090 ----
mean loss: 783.36
 ---- batch: 100 ----
mean loss: 773.19
 ---- batch: 110 ----
mean loss: 795.71
train mean loss: 778.67
epoch train time: 0:00:00.735561
elapsed time: 0:00:46.123557
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 01:45:31.197028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.03
 ---- batch: 020 ----
mean loss: 781.61
 ---- batch: 030 ----
mean loss: 785.26
 ---- batch: 040 ----
mean loss: 768.48
 ---- batch: 050 ----
mean loss: 760.37
 ---- batch: 060 ----
mean loss: 781.07
 ---- batch: 070 ----
mean loss: 774.67
 ---- batch: 080 ----
mean loss: 776.64
 ---- batch: 090 ----
mean loss: 766.98
 ---- batch: 100 ----
mean loss: 772.59
 ---- batch: 110 ----
mean loss: 774.43
train mean loss: 773.22
epoch train time: 0:00:00.726913
elapsed time: 0:00:46.850601
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 01:45:31.924069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.91
 ---- batch: 020 ----
mean loss: 805.68
 ---- batch: 030 ----
mean loss: 770.74
 ---- batch: 040 ----
mean loss: 790.67
 ---- batch: 050 ----
mean loss: 773.81
 ---- batch: 060 ----
mean loss: 775.07
 ---- batch: 070 ----
mean loss: 764.14
 ---- batch: 080 ----
mean loss: 773.93
 ---- batch: 090 ----
mean loss: 755.77
 ---- batch: 100 ----
mean loss: 752.19
 ---- batch: 110 ----
mean loss: 746.96
train mean loss: 767.59
epoch train time: 0:00:00.721177
elapsed time: 0:00:47.571922
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 01:45:32.645391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.86
 ---- batch: 020 ----
mean loss: 777.87
 ---- batch: 030 ----
mean loss: 753.84
 ---- batch: 040 ----
mean loss: 778.78
 ---- batch: 050 ----
mean loss: 765.77
 ---- batch: 060 ----
mean loss: 758.34
 ---- batch: 070 ----
mean loss: 770.46
 ---- batch: 080 ----
mean loss: 756.76
 ---- batch: 090 ----
mean loss: 753.10
 ---- batch: 100 ----
mean loss: 750.71
 ---- batch: 110 ----
mean loss: 764.99
train mean loss: 761.88
epoch train time: 0:00:00.736112
elapsed time: 0:00:48.308172
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 01:45:33.381679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 742.55
 ---- batch: 020 ----
mean loss: 744.71
 ---- batch: 030 ----
mean loss: 736.86
 ---- batch: 040 ----
mean loss: 761.79
 ---- batch: 050 ----
mean loss: 775.25
 ---- batch: 060 ----
mean loss: 752.85
 ---- batch: 070 ----
mean loss: 775.97
 ---- batch: 080 ----
mean loss: 745.83
 ---- batch: 090 ----
mean loss: 749.64
 ---- batch: 100 ----
mean loss: 778.52
 ---- batch: 110 ----
mean loss: 748.03
train mean loss: 756.16
epoch train time: 0:00:00.727591
elapsed time: 0:00:49.035935
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 01:45:34.109449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 734.71
 ---- batch: 020 ----
mean loss: 752.53
 ---- batch: 030 ----
mean loss: 766.08
 ---- batch: 040 ----
mean loss: 746.82
 ---- batch: 050 ----
mean loss: 764.71
 ---- batch: 060 ----
mean loss: 744.29
 ---- batch: 070 ----
mean loss: 746.96
 ---- batch: 080 ----
mean loss: 759.68
 ---- batch: 090 ----
mean loss: 746.06
 ---- batch: 100 ----
mean loss: 755.44
 ---- batch: 110 ----
mean loss: 734.76
train mean loss: 750.32
epoch train time: 0:00:00.722700
elapsed time: 0:00:49.758848
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 01:45:34.832363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 744.58
 ---- batch: 020 ----
mean loss: 746.10
 ---- batch: 030 ----
mean loss: 732.96
 ---- batch: 040 ----
mean loss: 752.03
 ---- batch: 050 ----
mean loss: 745.89
 ---- batch: 060 ----
mean loss: 754.06
 ---- batch: 070 ----
mean loss: 727.47
 ---- batch: 080 ----
mean loss: 746.24
 ---- batch: 090 ----
mean loss: 753.01
 ---- batch: 100 ----
mean loss: 731.58
 ---- batch: 110 ----
mean loss: 752.04
train mean loss: 744.47
epoch train time: 0:00:00.729176
elapsed time: 0:00:50.488228
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 01:45:35.561719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.59
 ---- batch: 020 ----
mean loss: 739.92
 ---- batch: 030 ----
mean loss: 732.32
 ---- batch: 040 ----
mean loss: 739.66
 ---- batch: 050 ----
mean loss: 739.24
 ---- batch: 060 ----
mean loss: 745.94
 ---- batch: 070 ----
mean loss: 751.47
 ---- batch: 080 ----
mean loss: 724.40
 ---- batch: 090 ----
mean loss: 746.72
 ---- batch: 100 ----
mean loss: 732.35
 ---- batch: 110 ----
mean loss: 734.77
train mean loss: 738.43
epoch train time: 0:00:00.728981
elapsed time: 0:00:51.217373
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 01:45:36.290845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 738.91
 ---- batch: 020 ----
mean loss: 739.17
 ---- batch: 030 ----
mean loss: 744.82
 ---- batch: 040 ----
mean loss: 738.73
 ---- batch: 050 ----
mean loss: 728.02
 ---- batch: 060 ----
mean loss: 718.02
 ---- batch: 070 ----
mean loss: 715.88
 ---- batch: 080 ----
mean loss: 750.03
 ---- batch: 090 ----
mean loss: 744.49
 ---- batch: 100 ----
mean loss: 718.98
 ---- batch: 110 ----
mean loss: 723.78
train mean loss: 732.35
epoch train time: 0:00:00.730312
elapsed time: 0:00:51.947828
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 01:45:37.021300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 726.77
 ---- batch: 020 ----
mean loss: 717.69
 ---- batch: 030 ----
mean loss: 743.87
 ---- batch: 040 ----
mean loss: 743.70
 ---- batch: 050 ----
mean loss: 723.36
 ---- batch: 060 ----
mean loss: 723.43
 ---- batch: 070 ----
mean loss: 720.92
 ---- batch: 080 ----
mean loss: 722.45
 ---- batch: 090 ----
mean loss: 730.85
 ---- batch: 100 ----
mean loss: 715.96
 ---- batch: 110 ----
mean loss: 724.61
train mean loss: 725.85
epoch train time: 0:00:00.724236
elapsed time: 0:00:52.672243
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 01:45:37.745732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 687.91
 ---- batch: 020 ----
mean loss: 718.62
 ---- batch: 030 ----
mean loss: 730.65
 ---- batch: 040 ----
mean loss: 740.18
 ---- batch: 050 ----
mean loss: 739.66
 ---- batch: 060 ----
mean loss: 710.78
 ---- batch: 070 ----
mean loss: 719.52
 ---- batch: 080 ----
mean loss: 719.69
 ---- batch: 090 ----
mean loss: 704.79
 ---- batch: 100 ----
mean loss: 722.71
 ---- batch: 110 ----
mean loss: 714.92
train mean loss: 719.32
epoch train time: 0:00:00.725132
elapsed time: 0:00:53.397526
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 01:45:38.470994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.87
 ---- batch: 020 ----
mean loss: 698.77
 ---- batch: 030 ----
mean loss: 709.42
 ---- batch: 040 ----
mean loss: 708.27
 ---- batch: 050 ----
mean loss: 721.55
 ---- batch: 060 ----
mean loss: 709.75
 ---- batch: 070 ----
mean loss: 711.18
 ---- batch: 080 ----
mean loss: 721.73
 ---- batch: 090 ----
mean loss: 712.37
 ---- batch: 100 ----
mean loss: 718.84
 ---- batch: 110 ----
mean loss: 711.34
train mean loss: 712.43
epoch train time: 0:00:00.719191
elapsed time: 0:00:54.116850
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 01:45:39.190340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.17
 ---- batch: 020 ----
mean loss: 700.82
 ---- batch: 030 ----
mean loss: 711.12
 ---- batch: 040 ----
mean loss: 711.95
 ---- batch: 050 ----
mean loss: 708.15
 ---- batch: 060 ----
mean loss: 707.31
 ---- batch: 070 ----
mean loss: 673.46
 ---- batch: 080 ----
mean loss: 714.64
 ---- batch: 090 ----
mean loss: 706.02
 ---- batch: 100 ----
mean loss: 706.84
 ---- batch: 110 ----
mean loss: 710.34
train mean loss: 705.34
epoch train time: 0:00:00.726024
elapsed time: 0:00:54.843027
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 01:45:39.916497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 705.24
 ---- batch: 020 ----
mean loss: 685.86
 ---- batch: 030 ----
mean loss: 713.90
 ---- batch: 040 ----
mean loss: 717.87
 ---- batch: 050 ----
mean loss: 681.93
 ---- batch: 060 ----
mean loss: 702.61
 ---- batch: 070 ----
mean loss: 698.56
 ---- batch: 080 ----
mean loss: 698.45
 ---- batch: 090 ----
mean loss: 692.66
 ---- batch: 100 ----
mean loss: 696.92
 ---- batch: 110 ----
mean loss: 684.42
train mean loss: 697.92
epoch train time: 0:00:00.724713
elapsed time: 0:00:55.567877
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 01:45:40.641363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.75
 ---- batch: 020 ----
mean loss: 683.65
 ---- batch: 030 ----
mean loss: 701.92
 ---- batch: 040 ----
mean loss: 703.75
 ---- batch: 050 ----
mean loss: 693.12
 ---- batch: 060 ----
mean loss: 699.17
 ---- batch: 070 ----
mean loss: 665.27
 ---- batch: 080 ----
mean loss: 697.68
 ---- batch: 090 ----
mean loss: 686.03
 ---- batch: 100 ----
mean loss: 695.17
 ---- batch: 110 ----
mean loss: 679.16
train mean loss: 689.58
epoch train time: 0:00:00.729510
elapsed time: 0:00:56.297541
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 01:45:41.371010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 678.64
 ---- batch: 020 ----
mean loss: 686.16
 ---- batch: 030 ----
mean loss: 675.93
 ---- batch: 040 ----
mean loss: 682.79
 ---- batch: 050 ----
mean loss: 666.93
 ---- batch: 060 ----
mean loss: 674.43
 ---- batch: 070 ----
mean loss: 689.17
 ---- batch: 080 ----
mean loss: 687.71
 ---- batch: 090 ----
mean loss: 687.07
 ---- batch: 100 ----
mean loss: 685.83
 ---- batch: 110 ----
mean loss: 680.74
train mean loss: 680.79
epoch train time: 0:00:00.728353
elapsed time: 0:00:57.026036
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 01:45:42.099507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.68
 ---- batch: 020 ----
mean loss: 648.16
 ---- batch: 030 ----
mean loss: 668.56
 ---- batch: 040 ----
mean loss: 685.61
 ---- batch: 050 ----
mean loss: 671.89
 ---- batch: 060 ----
mean loss: 680.84
 ---- batch: 070 ----
mean loss: 655.46
 ---- batch: 080 ----
mean loss: 678.80
 ---- batch: 090 ----
mean loss: 677.89
 ---- batch: 100 ----
mean loss: 678.02
 ---- batch: 110 ----
mean loss: 673.53
train mean loss: 671.23
epoch train time: 0:00:00.727412
elapsed time: 0:00:57.753606
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 01:45:42.827080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 691.43
 ---- batch: 020 ----
mean loss: 673.11
 ---- batch: 030 ----
mean loss: 659.83
 ---- batch: 040 ----
mean loss: 659.13
 ---- batch: 050 ----
mean loss: 660.39
 ---- batch: 060 ----
mean loss: 652.96
 ---- batch: 070 ----
mean loss: 655.78
 ---- batch: 080 ----
mean loss: 656.12
 ---- batch: 090 ----
mean loss: 643.52
 ---- batch: 100 ----
mean loss: 655.58
 ---- batch: 110 ----
mean loss: 647.62
train mean loss: 659.34
epoch train time: 0:00:00.717443
elapsed time: 0:00:58.471193
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 01:45:43.544686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 661.04
 ---- batch: 020 ----
mean loss: 645.98
 ---- batch: 030 ----
mean loss: 653.44
 ---- batch: 040 ----
mean loss: 645.85
 ---- batch: 050 ----
mean loss: 636.81
 ---- batch: 060 ----
mean loss: 649.98
 ---- batch: 070 ----
mean loss: 632.95
 ---- batch: 080 ----
mean loss: 630.13
 ---- batch: 090 ----
mean loss: 645.81
 ---- batch: 100 ----
mean loss: 638.52
 ---- batch: 110 ----
mean loss: 616.68
train mean loss: 641.35
epoch train time: 0:00:00.724235
elapsed time: 0:00:59.195589
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 01:45:44.269096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 629.86
 ---- batch: 020 ----
mean loss: 632.69
 ---- batch: 030 ----
mean loss: 628.57
 ---- batch: 040 ----
mean loss: 624.07
 ---- batch: 050 ----
mean loss: 636.64
 ---- batch: 060 ----
mean loss: 603.82
 ---- batch: 070 ----
mean loss: 608.79
 ---- batch: 080 ----
mean loss: 609.33
 ---- batch: 090 ----
mean loss: 603.16
 ---- batch: 100 ----
mean loss: 608.24
 ---- batch: 110 ----
mean loss: 622.44
train mean loss: 618.36
epoch train time: 0:00:00.729056
elapsed time: 0:00:59.924833
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 01:45:44.998317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 612.34
 ---- batch: 020 ----
mean loss: 607.35
 ---- batch: 030 ----
mean loss: 605.64
 ---- batch: 040 ----
mean loss: 596.67
 ---- batch: 050 ----
mean loss: 581.05
 ---- batch: 060 ----
mean loss: 590.23
 ---- batch: 070 ----
mean loss: 586.24
 ---- batch: 080 ----
mean loss: 599.60
 ---- batch: 090 ----
mean loss: 587.64
 ---- batch: 100 ----
mean loss: 580.41
 ---- batch: 110 ----
mean loss: 589.44
train mean loss: 594.36
epoch train time: 0:00:00.723061
elapsed time: 0:01:00.648048
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 01:45:45.721535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.14
 ---- batch: 020 ----
mean loss: 582.96
 ---- batch: 030 ----
mean loss: 561.80
 ---- batch: 040 ----
mean loss: 572.52
 ---- batch: 050 ----
mean loss: 578.23
 ---- batch: 060 ----
mean loss: 565.59
 ---- batch: 070 ----
mean loss: 567.38
 ---- batch: 080 ----
mean loss: 572.37
 ---- batch: 090 ----
mean loss: 560.16
 ---- batch: 100 ----
mean loss: 569.58
 ---- batch: 110 ----
mean loss: 563.83
train mean loss: 569.05
epoch train time: 0:00:00.727372
elapsed time: 0:01:01.375577
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 01:45:46.449047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.53
 ---- batch: 020 ----
mean loss: 560.54
 ---- batch: 030 ----
mean loss: 541.94
 ---- batch: 040 ----
mean loss: 551.09
 ---- batch: 050 ----
mean loss: 546.19
 ---- batch: 060 ----
mean loss: 545.58
 ---- batch: 070 ----
mean loss: 546.20
 ---- batch: 080 ----
mean loss: 544.79
 ---- batch: 090 ----
mean loss: 524.36
 ---- batch: 100 ----
mean loss: 532.49
 ---- batch: 110 ----
mean loss: 535.78
train mean loss: 542.68
epoch train time: 0:00:00.730676
elapsed time: 0:01:02.106387
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 01:45:47.179856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 532.99
 ---- batch: 020 ----
mean loss: 520.66
 ---- batch: 030 ----
mean loss: 529.32
 ---- batch: 040 ----
mean loss: 523.82
 ---- batch: 050 ----
mean loss: 513.94
 ---- batch: 060 ----
mean loss: 507.35
 ---- batch: 070 ----
mean loss: 532.27
 ---- batch: 080 ----
mean loss: 528.86
 ---- batch: 090 ----
mean loss: 516.98
 ---- batch: 100 ----
mean loss: 508.81
 ---- batch: 110 ----
mean loss: 504.52
train mean loss: 518.81
epoch train time: 0:00:00.725703
elapsed time: 0:01:02.832357
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 01:45:47.905874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.29
 ---- batch: 020 ----
mean loss: 500.93
 ---- batch: 030 ----
mean loss: 503.23
 ---- batch: 040 ----
mean loss: 510.91
 ---- batch: 050 ----
mean loss: 500.25
 ---- batch: 060 ----
mean loss: 501.73
 ---- batch: 070 ----
mean loss: 500.74
 ---- batch: 080 ----
mean loss: 488.74
 ---- batch: 090 ----
mean loss: 490.22
 ---- batch: 100 ----
mean loss: 476.42
 ---- batch: 110 ----
mean loss: 492.60
train mean loss: 497.32
epoch train time: 0:00:00.749605
elapsed time: 0:01:03.582165
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 01:45:48.655694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.89
 ---- batch: 020 ----
mean loss: 474.42
 ---- batch: 030 ----
mean loss: 490.19
 ---- batch: 040 ----
mean loss: 485.92
 ---- batch: 050 ----
mean loss: 478.13
 ---- batch: 060 ----
mean loss: 478.68
 ---- batch: 070 ----
mean loss: 469.52
 ---- batch: 080 ----
mean loss: 478.85
 ---- batch: 090 ----
mean loss: 464.59
 ---- batch: 100 ----
mean loss: 473.64
 ---- batch: 110 ----
mean loss: 461.70
train mean loss: 476.40
epoch train time: 0:00:00.721967
elapsed time: 0:01:04.304349
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 01:45:49.377820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.00
 ---- batch: 020 ----
mean loss: 462.32
 ---- batch: 030 ----
mean loss: 452.83
 ---- batch: 040 ----
mean loss: 461.07
 ---- batch: 050 ----
mean loss: 459.37
 ---- batch: 060 ----
mean loss: 483.87
 ---- batch: 070 ----
mean loss: 460.81
 ---- batch: 080 ----
mean loss: 447.25
 ---- batch: 090 ----
mean loss: 456.03
 ---- batch: 100 ----
mean loss: 437.86
 ---- batch: 110 ----
mean loss: 444.63
train mean loss: 456.14
epoch train time: 0:00:00.727077
elapsed time: 0:01:05.031571
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 01:45:50.105040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.05
 ---- batch: 020 ----
mean loss: 456.21
 ---- batch: 030 ----
mean loss: 447.10
 ---- batch: 040 ----
mean loss: 447.26
 ---- batch: 050 ----
mean loss: 437.44
 ---- batch: 060 ----
mean loss: 431.66
 ---- batch: 070 ----
mean loss: 433.48
 ---- batch: 080 ----
mean loss: 435.55
 ---- batch: 090 ----
mean loss: 423.13
 ---- batch: 100 ----
mean loss: 431.73
 ---- batch: 110 ----
mean loss: 431.30
train mean loss: 437.13
epoch train time: 0:00:00.727879
elapsed time: 0:01:05.759609
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 01:45:50.833089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.03
 ---- batch: 020 ----
mean loss: 432.13
 ---- batch: 030 ----
mean loss: 427.36
 ---- batch: 040 ----
mean loss: 413.91
 ---- batch: 050 ----
mean loss: 425.65
 ---- batch: 060 ----
mean loss: 408.85
 ---- batch: 070 ----
mean loss: 427.71
 ---- batch: 080 ----
mean loss: 419.58
 ---- batch: 090 ----
mean loss: 407.11
 ---- batch: 100 ----
mean loss: 409.87
 ---- batch: 110 ----
mean loss: 421.31
train mean loss: 419.73
epoch train time: 0:00:00.729699
elapsed time: 0:01:06.489461
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 01:45:51.562941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.77
 ---- batch: 020 ----
mean loss: 419.50
 ---- batch: 030 ----
mean loss: 406.47
 ---- batch: 040 ----
mean loss: 402.15
 ---- batch: 050 ----
mean loss: 411.89
 ---- batch: 060 ----
mean loss: 401.89
 ---- batch: 070 ----
mean loss: 414.38
 ---- batch: 080 ----
mean loss: 398.83
 ---- batch: 090 ----
mean loss: 404.74
 ---- batch: 100 ----
mean loss: 382.75
 ---- batch: 110 ----
mean loss: 396.45
train mean loss: 404.02
epoch train time: 0:00:00.725456
elapsed time: 0:01:07.215063
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 01:45:52.288534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.84
 ---- batch: 020 ----
mean loss: 394.69
 ---- batch: 030 ----
mean loss: 392.45
 ---- batch: 040 ----
mean loss: 380.12
 ---- batch: 050 ----
mean loss: 386.56
 ---- batch: 060 ----
mean loss: 395.96
 ---- batch: 070 ----
mean loss: 394.62
 ---- batch: 080 ----
mean loss: 394.53
 ---- batch: 090 ----
mean loss: 386.77
 ---- batch: 100 ----
mean loss: 383.14
 ---- batch: 110 ----
mean loss: 381.33
train mean loss: 390.01
epoch train time: 0:00:00.730575
elapsed time: 0:01:07.945791
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 01:45:53.019264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.01
 ---- batch: 020 ----
mean loss: 385.34
 ---- batch: 030 ----
mean loss: 374.65
 ---- batch: 040 ----
mean loss: 385.26
 ---- batch: 050 ----
mean loss: 377.64
 ---- batch: 060 ----
mean loss: 373.04
 ---- batch: 070 ----
mean loss: 374.34
 ---- batch: 080 ----
mean loss: 378.45
 ---- batch: 090 ----
mean loss: 376.55
 ---- batch: 100 ----
mean loss: 370.87
 ---- batch: 110 ----
mean loss: 361.39
train mean loss: 377.09
epoch train time: 0:00:00.723585
elapsed time: 0:01:08.669516
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 01:45:53.743004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.98
 ---- batch: 020 ----
mean loss: 367.47
 ---- batch: 030 ----
mean loss: 363.90
 ---- batch: 040 ----
mean loss: 371.10
 ---- batch: 050 ----
mean loss: 369.54
 ---- batch: 060 ----
mean loss: 364.89
 ---- batch: 070 ----
mean loss: 366.41
 ---- batch: 080 ----
mean loss: 358.10
 ---- batch: 090 ----
mean loss: 365.16
 ---- batch: 100 ----
mean loss: 362.92
 ---- batch: 110 ----
mean loss: 365.34
train mean loss: 365.80
epoch train time: 0:00:00.721802
elapsed time: 0:01:09.391487
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 01:45:54.464954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.57
 ---- batch: 020 ----
mean loss: 366.75
 ---- batch: 030 ----
mean loss: 373.93
 ---- batch: 040 ----
mean loss: 348.82
 ---- batch: 050 ----
mean loss: 348.01
 ---- batch: 060 ----
mean loss: 354.88
 ---- batch: 070 ----
mean loss: 351.80
 ---- batch: 080 ----
mean loss: 357.52
 ---- batch: 090 ----
mean loss: 347.75
 ---- batch: 100 ----
mean loss: 357.21
 ---- batch: 110 ----
mean loss: 352.07
train mean loss: 355.35
epoch train time: 0:00:00.732719
elapsed time: 0:01:10.124339
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 01:45:55.197808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.95
 ---- batch: 020 ----
mean loss: 359.96
 ---- batch: 030 ----
mean loss: 349.13
 ---- batch: 040 ----
mean loss: 340.51
 ---- batch: 050 ----
mean loss: 337.80
 ---- batch: 060 ----
mean loss: 347.26
 ---- batch: 070 ----
mean loss: 343.14
 ---- batch: 080 ----
mean loss: 346.04
 ---- batch: 090 ----
mean loss: 339.26
 ---- batch: 100 ----
mean loss: 336.47
 ---- batch: 110 ----
mean loss: 348.44
train mean loss: 346.07
epoch train time: 0:00:00.730119
elapsed time: 0:01:10.854618
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 01:45:55.928116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.45
 ---- batch: 020 ----
mean loss: 341.01
 ---- batch: 030 ----
mean loss: 347.44
 ---- batch: 040 ----
mean loss: 333.62
 ---- batch: 050 ----
mean loss: 335.83
 ---- batch: 060 ----
mean loss: 340.57
 ---- batch: 070 ----
mean loss: 337.61
 ---- batch: 080 ----
mean loss: 332.40
 ---- batch: 090 ----
mean loss: 327.75
 ---- batch: 100 ----
mean loss: 339.02
 ---- batch: 110 ----
mean loss: 335.14
train mean loss: 337.43
epoch train time: 0:00:00.728319
elapsed time: 0:01:11.583105
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 01:45:56.656576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.67
 ---- batch: 020 ----
mean loss: 330.35
 ---- batch: 030 ----
mean loss: 342.17
 ---- batch: 040 ----
mean loss: 336.28
 ---- batch: 050 ----
mean loss: 334.49
 ---- batch: 060 ----
mean loss: 314.07
 ---- batch: 070 ----
mean loss: 319.56
 ---- batch: 080 ----
mean loss: 325.29
 ---- batch: 090 ----
mean loss: 324.13
 ---- batch: 100 ----
mean loss: 341.76
 ---- batch: 110 ----
mean loss: 327.49
train mean loss: 329.71
epoch train time: 0:00:00.736965
elapsed time: 0:01:12.320245
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 01:45:57.393736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.83
 ---- batch: 020 ----
mean loss: 322.67
 ---- batch: 030 ----
mean loss: 331.05
 ---- batch: 040 ----
mean loss: 323.65
 ---- batch: 050 ----
mean loss: 322.59
 ---- batch: 060 ----
mean loss: 319.07
 ---- batch: 070 ----
mean loss: 314.63
 ---- batch: 080 ----
mean loss: 324.68
 ---- batch: 090 ----
mean loss: 325.38
 ---- batch: 100 ----
mean loss: 312.58
 ---- batch: 110 ----
mean loss: 319.12
train mean loss: 322.67
epoch train time: 0:00:00.736928
elapsed time: 0:01:13.057334
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 01:45:58.130825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.09
 ---- batch: 020 ----
mean loss: 321.53
 ---- batch: 030 ----
mean loss: 322.70
 ---- batch: 040 ----
mean loss: 314.96
 ---- batch: 050 ----
mean loss: 326.14
 ---- batch: 060 ----
mean loss: 324.84
 ---- batch: 070 ----
mean loss: 308.91
 ---- batch: 080 ----
mean loss: 305.31
 ---- batch: 090 ----
mean loss: 310.62
 ---- batch: 100 ----
mean loss: 309.13
 ---- batch: 110 ----
mean loss: 312.22
train mean loss: 316.09
epoch train time: 0:00:00.755633
elapsed time: 0:01:13.813138
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 01:45:58.886619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.40
 ---- batch: 020 ----
mean loss: 310.09
 ---- batch: 030 ----
mean loss: 303.88
 ---- batch: 040 ----
mean loss: 306.11
 ---- batch: 050 ----
mean loss: 311.81
 ---- batch: 060 ----
mean loss: 310.83
 ---- batch: 070 ----
mean loss: 310.59
 ---- batch: 080 ----
mean loss: 302.21
 ---- batch: 090 ----
mean loss: 316.74
 ---- batch: 100 ----
mean loss: 304.94
 ---- batch: 110 ----
mean loss: 314.35
train mean loss: 310.13
epoch train time: 0:00:00.729904
elapsed time: 0:01:14.543206
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 01:45:59.616692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.80
 ---- batch: 020 ----
mean loss: 312.00
 ---- batch: 030 ----
mean loss: 311.42
 ---- batch: 040 ----
mean loss: 304.14
 ---- batch: 050 ----
mean loss: 296.73
 ---- batch: 060 ----
mean loss: 289.46
 ---- batch: 070 ----
mean loss: 302.95
 ---- batch: 080 ----
mean loss: 311.93
 ---- batch: 090 ----
mean loss: 300.32
 ---- batch: 100 ----
mean loss: 311.62
 ---- batch: 110 ----
mean loss: 301.91
train mean loss: 304.43
epoch train time: 0:00:00.727497
elapsed time: 0:01:15.270873
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 01:46:00.344359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.38
 ---- batch: 020 ----
mean loss: 296.97
 ---- batch: 030 ----
mean loss: 312.28
 ---- batch: 040 ----
mean loss: 297.24
 ---- batch: 050 ----
mean loss: 293.50
 ---- batch: 060 ----
mean loss: 296.71
 ---- batch: 070 ----
mean loss: 286.36
 ---- batch: 080 ----
mean loss: 299.75
 ---- batch: 090 ----
mean loss: 306.18
 ---- batch: 100 ----
mean loss: 302.48
 ---- batch: 110 ----
mean loss: 300.43
train mean loss: 299.54
epoch train time: 0:00:00.728825
elapsed time: 0:01:15.999857
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 01:46:01.073329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.65
 ---- batch: 020 ----
mean loss: 286.07
 ---- batch: 030 ----
mean loss: 301.74
 ---- batch: 040 ----
mean loss: 295.56
 ---- batch: 050 ----
mean loss: 301.57
 ---- batch: 060 ----
mean loss: 290.26
 ---- batch: 070 ----
mean loss: 288.27
 ---- batch: 080 ----
mean loss: 294.71
 ---- batch: 090 ----
mean loss: 295.78
 ---- batch: 100 ----
mean loss: 293.70
 ---- batch: 110 ----
mean loss: 291.97
train mean loss: 294.75
epoch train time: 0:00:00.732901
elapsed time: 0:01:16.732903
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 01:46:01.806376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.63
 ---- batch: 020 ----
mean loss: 298.13
 ---- batch: 030 ----
mean loss: 288.89
 ---- batch: 040 ----
mean loss: 282.66
 ---- batch: 050 ----
mean loss: 289.72
 ---- batch: 060 ----
mean loss: 290.77
 ---- batch: 070 ----
mean loss: 299.96
 ---- batch: 080 ----
mean loss: 288.49
 ---- batch: 090 ----
mean loss: 298.50
 ---- batch: 100 ----
mean loss: 281.81
 ---- batch: 110 ----
mean loss: 297.53
train mean loss: 290.50
epoch train time: 0:00:00.726151
elapsed time: 0:01:17.459223
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 01:46:02.532730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.85
 ---- batch: 020 ----
mean loss: 277.12
 ---- batch: 030 ----
mean loss: 281.27
 ---- batch: 040 ----
mean loss: 294.05
 ---- batch: 050 ----
mean loss: 283.31
 ---- batch: 060 ----
mean loss: 284.52
 ---- batch: 070 ----
mean loss: 274.50
 ---- batch: 080 ----
mean loss: 292.60
 ---- batch: 090 ----
mean loss: 288.61
 ---- batch: 100 ----
mean loss: 289.25
 ---- batch: 110 ----
mean loss: 288.37
train mean loss: 286.35
epoch train time: 0:00:00.730151
elapsed time: 0:01:18.189558
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 01:46:03.263050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.26
 ---- batch: 020 ----
mean loss: 277.99
 ---- batch: 030 ----
mean loss: 283.94
 ---- batch: 040 ----
mean loss: 285.09
 ---- batch: 050 ----
mean loss: 277.61
 ---- batch: 060 ----
mean loss: 279.89
 ---- batch: 070 ----
mean loss: 292.42
 ---- batch: 080 ----
mean loss: 279.35
 ---- batch: 090 ----
mean loss: 278.18
 ---- batch: 100 ----
mean loss: 282.26
 ---- batch: 110 ----
mean loss: 292.84
train mean loss: 282.49
epoch train time: 0:00:00.733005
elapsed time: 0:01:18.922723
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 01:46:03.996194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.38
 ---- batch: 020 ----
mean loss: 283.44
 ---- batch: 030 ----
mean loss: 277.87
 ---- batch: 040 ----
mean loss: 263.96
 ---- batch: 050 ----
mean loss: 279.24
 ---- batch: 060 ----
mean loss: 282.26
 ---- batch: 070 ----
mean loss: 280.92
 ---- batch: 080 ----
mean loss: 281.26
 ---- batch: 090 ----
mean loss: 288.28
 ---- batch: 100 ----
mean loss: 274.45
 ---- batch: 110 ----
mean loss: 273.77
train mean loss: 279.12
epoch train time: 0:00:00.727893
elapsed time: 0:01:19.650752
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 01:46:04.724221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.60
 ---- batch: 020 ----
mean loss: 270.08
 ---- batch: 030 ----
mean loss: 275.52
 ---- batch: 040 ----
mean loss: 281.09
 ---- batch: 050 ----
mean loss: 283.55
 ---- batch: 060 ----
mean loss: 270.77
 ---- batch: 070 ----
mean loss: 280.13
 ---- batch: 080 ----
mean loss: 271.98
 ---- batch: 090 ----
mean loss: 268.67
 ---- batch: 100 ----
mean loss: 269.62
 ---- batch: 110 ----
mean loss: 276.96
train mean loss: 275.73
epoch train time: 0:00:00.730419
elapsed time: 0:01:20.381322
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 01:46:05.454790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.81
 ---- batch: 020 ----
mean loss: 275.44
 ---- batch: 030 ----
mean loss: 274.30
 ---- batch: 040 ----
mean loss: 267.83
 ---- batch: 050 ----
mean loss: 267.23
 ---- batch: 060 ----
mean loss: 282.29
 ---- batch: 070 ----
mean loss: 272.52
 ---- batch: 080 ----
mean loss: 276.25
 ---- batch: 090 ----
mean loss: 270.26
 ---- batch: 100 ----
mean loss: 272.41
 ---- batch: 110 ----
mean loss: 268.70
train mean loss: 272.73
epoch train time: 0:00:00.726981
elapsed time: 0:01:21.108439
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 01:46:06.181911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.11
 ---- batch: 020 ----
mean loss: 271.11
 ---- batch: 030 ----
mean loss: 261.03
 ---- batch: 040 ----
mean loss: 272.20
 ---- batch: 050 ----
mean loss: 272.66
 ---- batch: 060 ----
mean loss: 262.88
 ---- batch: 070 ----
mean loss: 264.35
 ---- batch: 080 ----
mean loss: 267.41
 ---- batch: 090 ----
mean loss: 271.53
 ---- batch: 100 ----
mean loss: 271.65
 ---- batch: 110 ----
mean loss: 277.09
train mean loss: 269.83
epoch train time: 0:00:00.719611
elapsed time: 0:01:21.828226
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 01:46:06.901717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.55
 ---- batch: 020 ----
mean loss: 272.33
 ---- batch: 030 ----
mean loss: 260.97
 ---- batch: 040 ----
mean loss: 270.24
 ---- batch: 050 ----
mean loss: 271.91
 ---- batch: 060 ----
mean loss: 274.14
 ---- batch: 070 ----
mean loss: 274.35
 ---- batch: 080 ----
mean loss: 259.25
 ---- batch: 090 ----
mean loss: 264.25
 ---- batch: 100 ----
mean loss: 255.60
 ---- batch: 110 ----
mean loss: 271.18
train mean loss: 266.91
epoch train time: 0:00:00.720425
elapsed time: 0:01:22.548813
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 01:46:07.622321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.80
 ---- batch: 020 ----
mean loss: 263.49
 ---- batch: 030 ----
mean loss: 265.98
 ---- batch: 040 ----
mean loss: 254.94
 ---- batch: 050 ----
mean loss: 260.93
 ---- batch: 060 ----
mean loss: 271.54
 ---- batch: 070 ----
mean loss: 265.87
 ---- batch: 080 ----
mean loss: 261.13
 ---- batch: 090 ----
mean loss: 263.82
 ---- batch: 100 ----
mean loss: 256.52
 ---- batch: 110 ----
mean loss: 270.45
train mean loss: 264.34
epoch train time: 0:00:00.721475
elapsed time: 0:01:23.270459
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 01:46:08.343927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.95
 ---- batch: 020 ----
mean loss: 264.92
 ---- batch: 030 ----
mean loss: 265.92
 ---- batch: 040 ----
mean loss: 266.78
 ---- batch: 050 ----
mean loss: 255.47
 ---- batch: 060 ----
mean loss: 251.10
 ---- batch: 070 ----
mean loss: 264.15
 ---- batch: 080 ----
mean loss: 262.78
 ---- batch: 090 ----
mean loss: 263.89
 ---- batch: 100 ----
mean loss: 263.92
 ---- batch: 110 ----
mean loss: 261.98
train mean loss: 261.70
epoch train time: 0:00:00.724239
elapsed time: 0:01:23.994845
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 01:46:09.068316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.63
 ---- batch: 020 ----
mean loss: 256.70
 ---- batch: 030 ----
mean loss: 262.66
 ---- batch: 040 ----
mean loss: 258.54
 ---- batch: 050 ----
mean loss: 242.71
 ---- batch: 060 ----
mean loss: 261.23
 ---- batch: 070 ----
mean loss: 263.25
 ---- batch: 080 ----
mean loss: 267.71
 ---- batch: 090 ----
mean loss: 257.91
 ---- batch: 100 ----
mean loss: 263.72
 ---- batch: 110 ----
mean loss: 257.23
train mean loss: 259.31
epoch train time: 0:00:00.717751
elapsed time: 0:01:24.712738
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 01:46:09.786208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.76
 ---- batch: 020 ----
mean loss: 256.87
 ---- batch: 030 ----
mean loss: 258.55
 ---- batch: 040 ----
mean loss: 262.29
 ---- batch: 050 ----
mean loss: 263.77
 ---- batch: 060 ----
mean loss: 254.41
 ---- batch: 070 ----
mean loss: 255.42
 ---- batch: 080 ----
mean loss: 256.36
 ---- batch: 090 ----
mean loss: 259.28
 ---- batch: 100 ----
mean loss: 259.14
 ---- batch: 110 ----
mean loss: 255.02
train mean loss: 257.01
epoch train time: 0:00:00.722462
elapsed time: 0:01:25.435337
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 01:46:10.508806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.97
 ---- batch: 020 ----
mean loss: 258.46
 ---- batch: 030 ----
mean loss: 249.97
 ---- batch: 040 ----
mean loss: 254.74
 ---- batch: 050 ----
mean loss: 249.45
 ---- batch: 060 ----
mean loss: 262.04
 ---- batch: 070 ----
mean loss: 252.83
 ---- batch: 080 ----
mean loss: 261.86
 ---- batch: 090 ----
mean loss: 254.00
 ---- batch: 100 ----
mean loss: 252.38
 ---- batch: 110 ----
mean loss: 253.23
train mean loss: 255.01
epoch train time: 0:00:00.736238
elapsed time: 0:01:26.171710
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 01:46:11.245180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.77
 ---- batch: 020 ----
mean loss: 247.32
 ---- batch: 030 ----
mean loss: 247.13
 ---- batch: 040 ----
mean loss: 250.52
 ---- batch: 050 ----
mean loss: 247.75
 ---- batch: 060 ----
mean loss: 261.89
 ---- batch: 070 ----
mean loss: 257.06
 ---- batch: 080 ----
mean loss: 256.25
 ---- batch: 090 ----
mean loss: 250.52
 ---- batch: 100 ----
mean loss: 256.22
 ---- batch: 110 ----
mean loss: 251.23
train mean loss: 253.12
epoch train time: 0:00:00.735989
elapsed time: 0:01:26.907855
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 01:46:11.981327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.29
 ---- batch: 020 ----
mean loss: 253.01
 ---- batch: 030 ----
mean loss: 259.70
 ---- batch: 040 ----
mean loss: 257.17
 ---- batch: 050 ----
mean loss: 261.31
 ---- batch: 060 ----
mean loss: 246.59
 ---- batch: 070 ----
mean loss: 241.81
 ---- batch: 080 ----
mean loss: 245.38
 ---- batch: 090 ----
mean loss: 250.62
 ---- batch: 100 ----
mean loss: 246.97
 ---- batch: 110 ----
mean loss: 249.94
train mean loss: 251.11
epoch train time: 0:00:00.735507
elapsed time: 0:01:27.643508
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 01:46:12.716997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.95
 ---- batch: 020 ----
mean loss: 246.85
 ---- batch: 030 ----
mean loss: 253.73
 ---- batch: 040 ----
mean loss: 254.24
 ---- batch: 050 ----
mean loss: 248.97
 ---- batch: 060 ----
mean loss: 251.34
 ---- batch: 070 ----
mean loss: 250.05
 ---- batch: 080 ----
mean loss: 246.42
 ---- batch: 090 ----
mean loss: 247.10
 ---- batch: 100 ----
mean loss: 241.48
 ---- batch: 110 ----
mean loss: 251.85
train mean loss: 249.16
epoch train time: 0:00:00.723769
elapsed time: 0:01:28.367430
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 01:46:13.440902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.35
 ---- batch: 020 ----
mean loss: 252.71
 ---- batch: 030 ----
mean loss: 241.15
 ---- batch: 040 ----
mean loss: 249.54
 ---- batch: 050 ----
mean loss: 254.28
 ---- batch: 060 ----
mean loss: 248.88
 ---- batch: 070 ----
mean loss: 242.27
 ---- batch: 080 ----
mean loss: 246.43
 ---- batch: 090 ----
mean loss: 252.24
 ---- batch: 100 ----
mean loss: 243.82
 ---- batch: 110 ----
mean loss: 241.23
train mean loss: 247.59
epoch train time: 0:00:00.740267
elapsed time: 0:01:29.107838
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 01:46:14.181311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.43
 ---- batch: 020 ----
mean loss: 246.51
 ---- batch: 030 ----
mean loss: 238.78
 ---- batch: 040 ----
mean loss: 247.31
 ---- batch: 050 ----
mean loss: 247.32
 ---- batch: 060 ----
mean loss: 254.25
 ---- batch: 070 ----
mean loss: 249.49
 ---- batch: 080 ----
mean loss: 252.17
 ---- batch: 090 ----
mean loss: 242.90
 ---- batch: 100 ----
mean loss: 242.80
 ---- batch: 110 ----
mean loss: 242.89
train mean loss: 245.85
epoch train time: 0:00:00.744331
elapsed time: 0:01:29.852333
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 01:46:14.925805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.36
 ---- batch: 020 ----
mean loss: 242.64
 ---- batch: 030 ----
mean loss: 244.22
 ---- batch: 040 ----
mean loss: 252.40
 ---- batch: 050 ----
mean loss: 248.40
 ---- batch: 060 ----
mean loss: 241.03
 ---- batch: 070 ----
mean loss: 245.08
 ---- batch: 080 ----
mean loss: 241.09
 ---- batch: 090 ----
mean loss: 254.58
 ---- batch: 100 ----
mean loss: 226.80
 ---- batch: 110 ----
mean loss: 252.55
train mean loss: 244.34
epoch train time: 0:00:00.745371
elapsed time: 0:01:30.597845
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 01:46:15.671316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.88
 ---- batch: 020 ----
mean loss: 249.31
 ---- batch: 030 ----
mean loss: 239.05
 ---- batch: 040 ----
mean loss: 236.40
 ---- batch: 050 ----
mean loss: 247.22
 ---- batch: 060 ----
mean loss: 242.61
 ---- batch: 070 ----
mean loss: 252.11
 ---- batch: 080 ----
mean loss: 238.05
 ---- batch: 090 ----
mean loss: 243.85
 ---- batch: 100 ----
mean loss: 248.56
 ---- batch: 110 ----
mean loss: 235.46
train mean loss: 242.87
epoch train time: 0:00:00.748616
elapsed time: 0:01:31.346600
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 01:46:16.420071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.23
 ---- batch: 020 ----
mean loss: 243.78
 ---- batch: 030 ----
mean loss: 239.65
 ---- batch: 040 ----
mean loss: 239.22
 ---- batch: 050 ----
mean loss: 242.16
 ---- batch: 060 ----
mean loss: 239.44
 ---- batch: 070 ----
mean loss: 238.58
 ---- batch: 080 ----
mean loss: 250.64
 ---- batch: 090 ----
mean loss: 243.87
 ---- batch: 100 ----
mean loss: 231.00
 ---- batch: 110 ----
mean loss: 250.02
train mean loss: 241.39
epoch train time: 0:00:00.742154
elapsed time: 0:01:32.088891
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 01:46:17.162376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.89
 ---- batch: 020 ----
mean loss: 246.84
 ---- batch: 030 ----
mean loss: 247.36
 ---- batch: 040 ----
mean loss: 245.16
 ---- batch: 050 ----
mean loss: 235.96
 ---- batch: 060 ----
mean loss: 238.41
 ---- batch: 070 ----
mean loss: 240.91
 ---- batch: 080 ----
mean loss: 238.50
 ---- batch: 090 ----
mean loss: 235.21
 ---- batch: 100 ----
mean loss: 238.51
 ---- batch: 110 ----
mean loss: 241.37
train mean loss: 240.40
epoch train time: 0:00:00.744430
elapsed time: 0:01:32.833471
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 01:46:17.906991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.54
 ---- batch: 020 ----
mean loss: 243.30
 ---- batch: 030 ----
mean loss: 233.88
 ---- batch: 040 ----
mean loss: 235.17
 ---- batch: 050 ----
mean loss: 241.31
 ---- batch: 060 ----
mean loss: 239.06
 ---- batch: 070 ----
mean loss: 228.50
 ---- batch: 080 ----
mean loss: 238.88
 ---- batch: 090 ----
mean loss: 237.48
 ---- batch: 100 ----
mean loss: 245.36
 ---- batch: 110 ----
mean loss: 237.74
train mean loss: 238.73
epoch train time: 0:00:00.736142
elapsed time: 0:01:33.569823
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 01:46:18.643293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.59
 ---- batch: 020 ----
mean loss: 240.95
 ---- batch: 030 ----
mean loss: 235.95
 ---- batch: 040 ----
mean loss: 232.45
 ---- batch: 050 ----
mean loss: 230.33
 ---- batch: 060 ----
mean loss: 240.11
 ---- batch: 070 ----
mean loss: 247.23
 ---- batch: 080 ----
mean loss: 245.83
 ---- batch: 090 ----
mean loss: 237.01
 ---- batch: 100 ----
mean loss: 239.57
 ---- batch: 110 ----
mean loss: 232.97
train mean loss: 237.69
epoch train time: 0:00:00.745838
elapsed time: 0:01:34.315799
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 01:46:19.389271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.41
 ---- batch: 020 ----
mean loss: 243.95
 ---- batch: 030 ----
mean loss: 230.11
 ---- batch: 040 ----
mean loss: 242.60
 ---- batch: 050 ----
mean loss: 240.05
 ---- batch: 060 ----
mean loss: 236.61
 ---- batch: 070 ----
mean loss: 237.06
 ---- batch: 080 ----
mean loss: 242.29
 ---- batch: 090 ----
mean loss: 237.61
 ---- batch: 100 ----
mean loss: 233.12
 ---- batch: 110 ----
mean loss: 230.58
train mean loss: 236.39
epoch train time: 0:00:00.749385
elapsed time: 0:01:35.065326
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 01:46:20.138797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.82
 ---- batch: 020 ----
mean loss: 244.64
 ---- batch: 030 ----
mean loss: 230.90
 ---- batch: 040 ----
mean loss: 233.26
 ---- batch: 050 ----
mean loss: 237.30
 ---- batch: 060 ----
mean loss: 230.43
 ---- batch: 070 ----
mean loss: 228.63
 ---- batch: 080 ----
mean loss: 228.87
 ---- batch: 090 ----
mean loss: 238.95
 ---- batch: 100 ----
mean loss: 238.87
 ---- batch: 110 ----
mean loss: 240.37
train mean loss: 235.34
epoch train time: 0:00:00.746101
elapsed time: 0:01:35.811568
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 01:46:20.885036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.01
 ---- batch: 020 ----
mean loss: 238.61
 ---- batch: 030 ----
mean loss: 230.54
 ---- batch: 040 ----
mean loss: 226.62
 ---- batch: 050 ----
mean loss: 235.05
 ---- batch: 060 ----
mean loss: 232.99
 ---- batch: 070 ----
mean loss: 233.05
 ---- batch: 080 ----
mean loss: 240.75
 ---- batch: 090 ----
mean loss: 232.95
 ---- batch: 100 ----
mean loss: 228.53
 ---- batch: 110 ----
mean loss: 235.26
train mean loss: 234.28
epoch train time: 0:00:00.740266
elapsed time: 0:01:36.552005
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 01:46:21.625479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.90
 ---- batch: 020 ----
mean loss: 229.02
 ---- batch: 030 ----
mean loss: 237.71
 ---- batch: 040 ----
mean loss: 234.31
 ---- batch: 050 ----
mean loss: 239.88
 ---- batch: 060 ----
mean loss: 225.94
 ---- batch: 070 ----
mean loss: 231.40
 ---- batch: 080 ----
mean loss: 238.84
 ---- batch: 090 ----
mean loss: 237.01
 ---- batch: 100 ----
mean loss: 235.93
 ---- batch: 110 ----
mean loss: 231.76
train mean loss: 233.16
epoch train time: 0:00:00.745842
elapsed time: 0:01:37.297991
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 01:46:22.371489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.04
 ---- batch: 020 ----
mean loss: 233.41
 ---- batch: 030 ----
mean loss: 236.01
 ---- batch: 040 ----
mean loss: 237.64
 ---- batch: 050 ----
mean loss: 228.30
 ---- batch: 060 ----
mean loss: 230.37
 ---- batch: 070 ----
mean loss: 235.14
 ---- batch: 080 ----
mean loss: 231.76
 ---- batch: 090 ----
mean loss: 233.62
 ---- batch: 100 ----
mean loss: 228.93
 ---- batch: 110 ----
mean loss: 224.48
train mean loss: 232.46
epoch train time: 0:00:00.752318
elapsed time: 0:01:38.050478
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 01:46:23.123951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.39
 ---- batch: 020 ----
mean loss: 228.64
 ---- batch: 030 ----
mean loss: 222.50
 ---- batch: 040 ----
mean loss: 229.77
 ---- batch: 050 ----
mean loss: 230.14
 ---- batch: 060 ----
mean loss: 237.94
 ---- batch: 070 ----
mean loss: 240.52
 ---- batch: 080 ----
mean loss: 236.38
 ---- batch: 090 ----
mean loss: 228.65
 ---- batch: 100 ----
mean loss: 230.59
 ---- batch: 110 ----
mean loss: 232.60
train mean loss: 231.48
epoch train time: 0:00:00.743254
elapsed time: 0:01:38.793879
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 01:46:23.867356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.16
 ---- batch: 020 ----
mean loss: 239.66
 ---- batch: 030 ----
mean loss: 237.50
 ---- batch: 040 ----
mean loss: 226.08
 ---- batch: 050 ----
mean loss: 231.32
 ---- batch: 060 ----
mean loss: 231.69
 ---- batch: 070 ----
mean loss: 231.20
 ---- batch: 080 ----
mean loss: 225.68
 ---- batch: 090 ----
mean loss: 228.07
 ---- batch: 100 ----
mean loss: 220.65
 ---- batch: 110 ----
mean loss: 234.22
train mean loss: 230.72
epoch train time: 0:00:00.731732
elapsed time: 0:01:39.525771
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 01:46:24.599240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.56
 ---- batch: 020 ----
mean loss: 234.95
 ---- batch: 030 ----
mean loss: 229.98
 ---- batch: 040 ----
mean loss: 236.16
 ---- batch: 050 ----
mean loss: 219.87
 ---- batch: 060 ----
mean loss: 228.45
 ---- batch: 070 ----
mean loss: 234.71
 ---- batch: 080 ----
mean loss: 227.34
 ---- batch: 090 ----
mean loss: 223.41
 ---- batch: 100 ----
mean loss: 225.39
 ---- batch: 110 ----
mean loss: 234.52
train mean loss: 230.00
epoch train time: 0:00:00.732873
elapsed time: 0:01:40.258795
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 01:46:25.332266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.02
 ---- batch: 020 ----
mean loss: 233.33
 ---- batch: 030 ----
mean loss: 230.46
 ---- batch: 040 ----
mean loss: 234.42
 ---- batch: 050 ----
mean loss: 225.88
 ---- batch: 060 ----
mean loss: 221.25
 ---- batch: 070 ----
mean loss: 229.13
 ---- batch: 080 ----
mean loss: 222.48
 ---- batch: 090 ----
mean loss: 234.64
 ---- batch: 100 ----
mean loss: 230.95
 ---- batch: 110 ----
mean loss: 229.54
train mean loss: 229.16
epoch train time: 0:00:00.735441
elapsed time: 0:01:40.994375
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 01:46:26.067844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.42
 ---- batch: 020 ----
mean loss: 237.37
 ---- batch: 030 ----
mean loss: 223.99
 ---- batch: 040 ----
mean loss: 226.88
 ---- batch: 050 ----
mean loss: 221.74
 ---- batch: 060 ----
mean loss: 224.44
 ---- batch: 070 ----
mean loss: 233.81
 ---- batch: 080 ----
mean loss: 236.44
 ---- batch: 090 ----
mean loss: 228.44
 ---- batch: 100 ----
mean loss: 228.00
 ---- batch: 110 ----
mean loss: 227.77
train mean loss: 228.70
epoch train time: 0:00:00.719565
elapsed time: 0:01:41.714077
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 01:46:26.787547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.74
 ---- batch: 020 ----
mean loss: 229.78
 ---- batch: 030 ----
mean loss: 229.96
 ---- batch: 040 ----
mean loss: 227.82
 ---- batch: 050 ----
mean loss: 232.05
 ---- batch: 060 ----
mean loss: 227.62
 ---- batch: 070 ----
mean loss: 231.85
 ---- batch: 080 ----
mean loss: 225.55
 ---- batch: 090 ----
mean loss: 232.78
 ---- batch: 100 ----
mean loss: 225.37
 ---- batch: 110 ----
mean loss: 221.38
train mean loss: 227.99
epoch train time: 0:00:00.720610
elapsed time: 0:01:42.434829
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 01:46:27.508323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.44
 ---- batch: 020 ----
mean loss: 233.48
 ---- batch: 030 ----
mean loss: 229.55
 ---- batch: 040 ----
mean loss: 231.07
 ---- batch: 050 ----
mean loss: 234.79
 ---- batch: 060 ----
mean loss: 219.73
 ---- batch: 070 ----
mean loss: 218.49
 ---- batch: 080 ----
mean loss: 222.24
 ---- batch: 090 ----
mean loss: 226.30
 ---- batch: 100 ----
mean loss: 226.59
 ---- batch: 110 ----
mean loss: 228.70
train mean loss: 227.36
epoch train time: 0:00:00.722933
elapsed time: 0:01:43.157920
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 01:46:28.231389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.02
 ---- batch: 020 ----
mean loss: 221.02
 ---- batch: 030 ----
mean loss: 204.73
 ---- batch: 040 ----
mean loss: 230.79
 ---- batch: 050 ----
mean loss: 242.84
 ---- batch: 060 ----
mean loss: 232.18
 ---- batch: 070 ----
mean loss: 227.29
 ---- batch: 080 ----
mean loss: 226.18
 ---- batch: 090 ----
mean loss: 225.99
 ---- batch: 100 ----
mean loss: 227.59
 ---- batch: 110 ----
mean loss: 233.07
train mean loss: 226.77
epoch train time: 0:00:00.726353
elapsed time: 0:01:43.884411
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 01:46:28.957906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.86
 ---- batch: 020 ----
mean loss: 217.38
 ---- batch: 030 ----
mean loss: 224.47
 ---- batch: 040 ----
mean loss: 224.86
 ---- batch: 050 ----
mean loss: 238.84
 ---- batch: 060 ----
mean loss: 221.86
 ---- batch: 070 ----
mean loss: 223.60
 ---- batch: 080 ----
mean loss: 234.34
 ---- batch: 090 ----
mean loss: 228.54
 ---- batch: 100 ----
mean loss: 229.97
 ---- batch: 110 ----
mean loss: 226.19
train mean loss: 226.25
epoch train time: 0:00:00.723539
elapsed time: 0:01:44.608115
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 01:46:29.681585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.88
 ---- batch: 020 ----
mean loss: 225.05
 ---- batch: 030 ----
mean loss: 226.63
 ---- batch: 040 ----
mean loss: 216.82
 ---- batch: 050 ----
mean loss: 223.91
 ---- batch: 060 ----
mean loss: 231.29
 ---- batch: 070 ----
mean loss: 227.44
 ---- batch: 080 ----
mean loss: 237.12
 ---- batch: 090 ----
mean loss: 228.10
 ---- batch: 100 ----
mean loss: 220.51
 ---- batch: 110 ----
mean loss: 222.11
train mean loss: 225.65
epoch train time: 0:00:00.726014
elapsed time: 0:01:45.334290
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 01:46:30.407767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.29
 ---- batch: 020 ----
mean loss: 233.09
 ---- batch: 030 ----
mean loss: 219.19
 ---- batch: 040 ----
mean loss: 230.61
 ---- batch: 050 ----
mean loss: 218.07
 ---- batch: 060 ----
mean loss: 227.51
 ---- batch: 070 ----
mean loss: 219.47
 ---- batch: 080 ----
mean loss: 214.20
 ---- batch: 090 ----
mean loss: 222.10
 ---- batch: 100 ----
mean loss: 229.32
 ---- batch: 110 ----
mean loss: 234.01
train mean loss: 225.11
epoch train time: 0:00:00.724487
elapsed time: 0:01:46.058927
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 01:46:31.132398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.98
 ---- batch: 020 ----
mean loss: 225.61
 ---- batch: 030 ----
mean loss: 234.42
 ---- batch: 040 ----
mean loss: 220.56
 ---- batch: 050 ----
mean loss: 226.93
 ---- batch: 060 ----
mean loss: 226.85
 ---- batch: 070 ----
mean loss: 221.38
 ---- batch: 080 ----
mean loss: 223.83
 ---- batch: 090 ----
mean loss: 217.01
 ---- batch: 100 ----
mean loss: 232.96
 ---- batch: 110 ----
mean loss: 224.44
train mean loss: 224.68
epoch train time: 0:00:00.724386
elapsed time: 0:01:46.783486
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 01:46:31.856992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.88
 ---- batch: 020 ----
mean loss: 228.25
 ---- batch: 030 ----
mean loss: 223.93
 ---- batch: 040 ----
mean loss: 221.97
 ---- batch: 050 ----
mean loss: 227.78
 ---- batch: 060 ----
mean loss: 217.21
 ---- batch: 070 ----
mean loss: 223.31
 ---- batch: 080 ----
mean loss: 229.04
 ---- batch: 090 ----
mean loss: 223.49
 ---- batch: 100 ----
mean loss: 216.79
 ---- batch: 110 ----
mean loss: 224.48
train mean loss: 224.19
epoch train time: 0:00:00.725048
elapsed time: 0:01:47.508710
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 01:46:32.582182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.78
 ---- batch: 020 ----
mean loss: 225.66
 ---- batch: 030 ----
mean loss: 229.02
 ---- batch: 040 ----
mean loss: 224.11
 ---- batch: 050 ----
mean loss: 222.11
 ---- batch: 060 ----
mean loss: 225.76
 ---- batch: 070 ----
mean loss: 217.84
 ---- batch: 080 ----
mean loss: 228.64
 ---- batch: 090 ----
mean loss: 216.90
 ---- batch: 100 ----
mean loss: 221.18
 ---- batch: 110 ----
mean loss: 220.72
train mean loss: 223.78
epoch train time: 0:00:00.732744
elapsed time: 0:01:48.241596
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 01:46:33.315070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.35
 ---- batch: 020 ----
mean loss: 220.05
 ---- batch: 030 ----
mean loss: 224.19
 ---- batch: 040 ----
mean loss: 222.59
 ---- batch: 050 ----
mean loss: 216.43
 ---- batch: 060 ----
mean loss: 224.82
 ---- batch: 070 ----
mean loss: 220.69
 ---- batch: 080 ----
mean loss: 218.76
 ---- batch: 090 ----
mean loss: 218.96
 ---- batch: 100 ----
mean loss: 226.58
 ---- batch: 110 ----
mean loss: 231.15
train mean loss: 223.33
epoch train time: 0:00:00.730314
elapsed time: 0:01:48.972056
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 01:46:34.045527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.70
 ---- batch: 020 ----
mean loss: 225.76
 ---- batch: 030 ----
mean loss: 223.16
 ---- batch: 040 ----
mean loss: 221.48
 ---- batch: 050 ----
mean loss: 220.55
 ---- batch: 060 ----
mean loss: 216.44
 ---- batch: 070 ----
mean loss: 228.36
 ---- batch: 080 ----
mean loss: 218.69
 ---- batch: 090 ----
mean loss: 222.80
 ---- batch: 100 ----
mean loss: 228.09
 ---- batch: 110 ----
mean loss: 230.19
train mean loss: 223.00
epoch train time: 0:00:00.727039
elapsed time: 0:01:49.699246
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 01:46:34.772716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.45
 ---- batch: 020 ----
mean loss: 223.54
 ---- batch: 030 ----
mean loss: 219.32
 ---- batch: 040 ----
mean loss: 215.55
 ---- batch: 050 ----
mean loss: 214.05
 ---- batch: 060 ----
mean loss: 227.43
 ---- batch: 070 ----
mean loss: 222.29
 ---- batch: 080 ----
mean loss: 220.17
 ---- batch: 090 ----
mean loss: 222.22
 ---- batch: 100 ----
mean loss: 233.63
 ---- batch: 110 ----
mean loss: 220.81
train mean loss: 222.54
epoch train time: 0:00:00.725145
elapsed time: 0:01:50.424571
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 01:46:35.498031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.19
 ---- batch: 020 ----
mean loss: 231.48
 ---- batch: 030 ----
mean loss: 215.07
 ---- batch: 040 ----
mean loss: 221.71
 ---- batch: 050 ----
mean loss: 226.90
 ---- batch: 060 ----
mean loss: 220.07
 ---- batch: 070 ----
mean loss: 218.61
 ---- batch: 080 ----
mean loss: 225.87
 ---- batch: 090 ----
mean loss: 226.93
 ---- batch: 100 ----
mean loss: 212.99
 ---- batch: 110 ----
mean loss: 226.82
train mean loss: 222.27
epoch train time: 0:00:00.726186
elapsed time: 0:01:51.150884
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 01:46:36.224353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.39
 ---- batch: 020 ----
mean loss: 223.66
 ---- batch: 030 ----
mean loss: 225.79
 ---- batch: 040 ----
mean loss: 220.45
 ---- batch: 050 ----
mean loss: 229.37
 ---- batch: 060 ----
mean loss: 215.96
 ---- batch: 070 ----
mean loss: 232.35
 ---- batch: 080 ----
mean loss: 219.81
 ---- batch: 090 ----
mean loss: 221.59
 ---- batch: 100 ----
mean loss: 211.16
 ---- batch: 110 ----
mean loss: 215.29
train mean loss: 221.82
epoch train time: 0:00:00.728162
elapsed time: 0:01:51.879181
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 01:46:36.952687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.49
 ---- batch: 020 ----
mean loss: 228.03
 ---- batch: 030 ----
mean loss: 211.88
 ---- batch: 040 ----
mean loss: 218.03
 ---- batch: 050 ----
mean loss: 221.71
 ---- batch: 060 ----
mean loss: 219.20
 ---- batch: 070 ----
mean loss: 219.94
 ---- batch: 080 ----
mean loss: 222.55
 ---- batch: 090 ----
mean loss: 226.51
 ---- batch: 100 ----
mean loss: 225.14
 ---- batch: 110 ----
mean loss: 224.53
train mean loss: 221.53
epoch train time: 0:00:00.729055
elapsed time: 0:01:52.608410
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 01:46:37.681880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.55
 ---- batch: 020 ----
mean loss: 215.47
 ---- batch: 030 ----
mean loss: 225.92
 ---- batch: 040 ----
mean loss: 213.01
 ---- batch: 050 ----
mean loss: 224.38
 ---- batch: 060 ----
mean loss: 215.76
 ---- batch: 070 ----
mean loss: 229.38
 ---- batch: 080 ----
mean loss: 223.15
 ---- batch: 090 ----
mean loss: 216.12
 ---- batch: 100 ----
mean loss: 224.60
 ---- batch: 110 ----
mean loss: 224.73
train mean loss: 221.17
epoch train time: 0:00:00.729263
elapsed time: 0:01:53.337815
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 01:46:38.411286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.26
 ---- batch: 020 ----
mean loss: 220.64
 ---- batch: 030 ----
mean loss: 219.27
 ---- batch: 040 ----
mean loss: 212.02
 ---- batch: 050 ----
mean loss: 226.64
 ---- batch: 060 ----
mean loss: 219.42
 ---- batch: 070 ----
mean loss: 222.41
 ---- batch: 080 ----
mean loss: 229.49
 ---- batch: 090 ----
mean loss: 219.52
 ---- batch: 100 ----
mean loss: 214.69
 ---- batch: 110 ----
mean loss: 224.63
train mean loss: 221.03
epoch train time: 0:00:00.735581
elapsed time: 0:01:54.073534
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 01:46:39.147004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.20
 ---- batch: 020 ----
mean loss: 219.92
 ---- batch: 030 ----
mean loss: 221.42
 ---- batch: 040 ----
mean loss: 217.42
 ---- batch: 050 ----
mean loss: 213.47
 ---- batch: 060 ----
mean loss: 225.71
 ---- batch: 070 ----
mean loss: 221.84
 ---- batch: 080 ----
mean loss: 225.70
 ---- batch: 090 ----
mean loss: 228.62
 ---- batch: 100 ----
mean loss: 210.35
 ---- batch: 110 ----
mean loss: 221.67
train mean loss: 220.71
epoch train time: 0:00:00.727012
elapsed time: 0:01:54.800683
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 01:46:39.874154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.19
 ---- batch: 020 ----
mean loss: 206.74
 ---- batch: 030 ----
mean loss: 224.82
 ---- batch: 040 ----
mean loss: 207.56
 ---- batch: 050 ----
mean loss: 219.54
 ---- batch: 060 ----
mean loss: 225.08
 ---- batch: 070 ----
mean loss: 219.85
 ---- batch: 080 ----
mean loss: 226.02
 ---- batch: 090 ----
mean loss: 222.90
 ---- batch: 100 ----
mean loss: 223.67
 ---- batch: 110 ----
mean loss: 226.47
train mean loss: 220.23
epoch train time: 0:00:00.716924
elapsed time: 0:01:55.517753
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 01:46:40.591223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.17
 ---- batch: 020 ----
mean loss: 229.99
 ---- batch: 030 ----
mean loss: 214.55
 ---- batch: 040 ----
mean loss: 219.68
 ---- batch: 050 ----
mean loss: 226.89
 ---- batch: 060 ----
mean loss: 236.21
 ---- batch: 070 ----
mean loss: 222.27
 ---- batch: 080 ----
mean loss: 209.88
 ---- batch: 090 ----
mean loss: 215.98
 ---- batch: 100 ----
mean loss: 219.60
 ---- batch: 110 ----
mean loss: 218.16
train mean loss: 220.51
epoch train time: 0:00:00.732289
elapsed time: 0:01:56.250183
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 01:46:41.323673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.92
 ---- batch: 020 ----
mean loss: 228.76
 ---- batch: 030 ----
mean loss: 222.55
 ---- batch: 040 ----
mean loss: 211.78
 ---- batch: 050 ----
mean loss: 218.89
 ---- batch: 060 ----
mean loss: 217.66
 ---- batch: 070 ----
mean loss: 215.23
 ---- batch: 080 ----
mean loss: 224.59
 ---- batch: 090 ----
mean loss: 224.82
 ---- batch: 100 ----
mean loss: 212.09
 ---- batch: 110 ----
mean loss: 214.02
train mean loss: 219.87
epoch train time: 0:00:00.739210
elapsed time: 0:01:56.989552
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 01:46:42.063022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.49
 ---- batch: 020 ----
mean loss: 217.85
 ---- batch: 030 ----
mean loss: 216.70
 ---- batch: 040 ----
mean loss: 217.74
 ---- batch: 050 ----
mean loss: 218.62
 ---- batch: 060 ----
mean loss: 220.55
 ---- batch: 070 ----
mean loss: 227.68
 ---- batch: 080 ----
mean loss: 222.93
 ---- batch: 090 ----
mean loss: 219.13
 ---- batch: 100 ----
mean loss: 225.59
 ---- batch: 110 ----
mean loss: 215.19
train mean loss: 219.61
epoch train time: 0:00:00.731769
elapsed time: 0:01:57.721488
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 01:46:42.794967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.49
 ---- batch: 020 ----
mean loss: 223.16
 ---- batch: 030 ----
mean loss: 218.74
 ---- batch: 040 ----
mean loss: 217.50
 ---- batch: 050 ----
mean loss: 213.42
 ---- batch: 060 ----
mean loss: 218.02
 ---- batch: 070 ----
mean loss: 219.26
 ---- batch: 080 ----
mean loss: 229.76
 ---- batch: 090 ----
mean loss: 215.97
 ---- batch: 100 ----
mean loss: 220.17
 ---- batch: 110 ----
mean loss: 213.68
train mean loss: 219.20
epoch train time: 0:00:00.721082
elapsed time: 0:01:58.442719
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 01:46:43.516190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.46
 ---- batch: 020 ----
mean loss: 218.92
 ---- batch: 030 ----
mean loss: 220.65
 ---- batch: 040 ----
mean loss: 213.71
 ---- batch: 050 ----
mean loss: 219.77
 ---- batch: 060 ----
mean loss: 216.73
 ---- batch: 070 ----
mean loss: 215.90
 ---- batch: 080 ----
mean loss: 224.04
 ---- batch: 090 ----
mean loss: 225.22
 ---- batch: 100 ----
mean loss: 221.86
 ---- batch: 110 ----
mean loss: 215.88
train mean loss: 218.92
epoch train time: 0:00:00.733848
elapsed time: 0:01:59.176707
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 01:46:44.250179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.13
 ---- batch: 020 ----
mean loss: 209.71
 ---- batch: 030 ----
mean loss: 216.41
 ---- batch: 040 ----
mean loss: 225.18
 ---- batch: 050 ----
mean loss: 222.12
 ---- batch: 060 ----
mean loss: 227.47
 ---- batch: 070 ----
mean loss: 216.77
 ---- batch: 080 ----
mean loss: 217.45
 ---- batch: 090 ----
mean loss: 226.32
 ---- batch: 100 ----
mean loss: 216.15
 ---- batch: 110 ----
mean loss: 218.88
train mean loss: 218.91
epoch train time: 0:00:00.741028
elapsed time: 0:01:59.917877
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 01:46:44.991348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.28
 ---- batch: 020 ----
mean loss: 204.35
 ---- batch: 030 ----
mean loss: 224.59
 ---- batch: 040 ----
mean loss: 221.82
 ---- batch: 050 ----
mean loss: 216.53
 ---- batch: 060 ----
mean loss: 215.53
 ---- batch: 070 ----
mean loss: 222.36
 ---- batch: 080 ----
mean loss: 208.04
 ---- batch: 090 ----
mean loss: 224.20
 ---- batch: 100 ----
mean loss: 218.04
 ---- batch: 110 ----
mean loss: 222.42
train mean loss: 218.48
epoch train time: 0:00:00.727823
elapsed time: 0:02:00.645848
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 01:46:45.719318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.13
 ---- batch: 020 ----
mean loss: 215.53
 ---- batch: 030 ----
mean loss: 217.33
 ---- batch: 040 ----
mean loss: 221.10
 ---- batch: 050 ----
mean loss: 210.66
 ---- batch: 060 ----
mean loss: 218.81
 ---- batch: 070 ----
mean loss: 215.69
 ---- batch: 080 ----
mean loss: 218.43
 ---- batch: 090 ----
mean loss: 218.35
 ---- batch: 100 ----
mean loss: 223.32
 ---- batch: 110 ----
mean loss: 220.25
train mean loss: 218.14
epoch train time: 0:00:00.738381
elapsed time: 0:02:01.384372
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 01:46:46.457862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.85
 ---- batch: 020 ----
mean loss: 224.74
 ---- batch: 030 ----
mean loss: 213.32
 ---- batch: 040 ----
mean loss: 223.21
 ---- batch: 050 ----
mean loss: 224.81
 ---- batch: 060 ----
mean loss: 218.05
 ---- batch: 070 ----
mean loss: 220.02
 ---- batch: 080 ----
mean loss: 223.03
 ---- batch: 090 ----
mean loss: 202.15
 ---- batch: 100 ----
mean loss: 224.41
 ---- batch: 110 ----
mean loss: 205.76
train mean loss: 217.98
epoch train time: 0:00:00.828749
elapsed time: 0:02:02.213349
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 01:46:47.286831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.94
 ---- batch: 020 ----
mean loss: 209.02
 ---- batch: 030 ----
mean loss: 216.33
 ---- batch: 040 ----
mean loss: 219.94
 ---- batch: 050 ----
mean loss: 218.56
 ---- batch: 060 ----
mean loss: 218.40
 ---- batch: 070 ----
mean loss: 217.85
 ---- batch: 080 ----
mean loss: 219.35
 ---- batch: 090 ----
mean loss: 228.18
 ---- batch: 100 ----
mean loss: 215.70
 ---- batch: 110 ----
mean loss: 211.75
train mean loss: 217.59
epoch train time: 0:00:00.842350
elapsed time: 0:02:03.055849
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 01:46:48.129323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.56
 ---- batch: 020 ----
mean loss: 215.98
 ---- batch: 030 ----
mean loss: 218.95
 ---- batch: 040 ----
mean loss: 224.95
 ---- batch: 050 ----
mean loss: 224.97
 ---- batch: 060 ----
mean loss: 221.18
 ---- batch: 070 ----
mean loss: 219.16
 ---- batch: 080 ----
mean loss: 213.20
 ---- batch: 090 ----
mean loss: 214.96
 ---- batch: 100 ----
mean loss: 209.00
 ---- batch: 110 ----
mean loss: 211.80
train mean loss: 217.33
epoch train time: 0:00:00.733412
elapsed time: 0:02:03.789449
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 01:46:48.862922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.29
 ---- batch: 020 ----
mean loss: 211.11
 ---- batch: 030 ----
mean loss: 223.29
 ---- batch: 040 ----
mean loss: 225.09
 ---- batch: 050 ----
mean loss: 211.10
 ---- batch: 060 ----
mean loss: 218.55
 ---- batch: 070 ----
mean loss: 224.02
 ---- batch: 080 ----
mean loss: 222.45
 ---- batch: 090 ----
mean loss: 210.41
 ---- batch: 100 ----
mean loss: 209.49
 ---- batch: 110 ----
mean loss: 218.16
train mean loss: 217.22
epoch train time: 0:00:00.738520
elapsed time: 0:02:04.528124
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 01:46:49.601599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.37
 ---- batch: 020 ----
mean loss: 213.29
 ---- batch: 030 ----
mean loss: 213.98
 ---- batch: 040 ----
mean loss: 220.33
 ---- batch: 050 ----
mean loss: 216.49
 ---- batch: 060 ----
mean loss: 213.60
 ---- batch: 070 ----
mean loss: 220.40
 ---- batch: 080 ----
mean loss: 221.50
 ---- batch: 090 ----
mean loss: 216.13
 ---- batch: 100 ----
mean loss: 217.69
 ---- batch: 110 ----
mean loss: 214.25
train mean loss: 217.08
epoch train time: 0:00:00.746186
elapsed time: 0:02:05.274464
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 01:46:50.347925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.58
 ---- batch: 020 ----
mean loss: 217.08
 ---- batch: 030 ----
mean loss: 218.10
 ---- batch: 040 ----
mean loss: 202.12
 ---- batch: 050 ----
mean loss: 227.88
 ---- batch: 060 ----
mean loss: 215.27
 ---- batch: 070 ----
mean loss: 215.58
 ---- batch: 080 ----
mean loss: 214.85
 ---- batch: 090 ----
mean loss: 220.38
 ---- batch: 100 ----
mean loss: 214.32
 ---- batch: 110 ----
mean loss: 217.12
train mean loss: 216.71
epoch train time: 0:00:00.728562
elapsed time: 0:02:06.003186
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 01:46:51.076657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.06
 ---- batch: 020 ----
mean loss: 211.92
 ---- batch: 030 ----
mean loss: 214.87
 ---- batch: 040 ----
mean loss: 216.88
 ---- batch: 050 ----
mean loss: 223.86
 ---- batch: 060 ----
mean loss: 215.31
 ---- batch: 070 ----
mean loss: 219.04
 ---- batch: 080 ----
mean loss: 215.83
 ---- batch: 090 ----
mean loss: 208.76
 ---- batch: 100 ----
mean loss: 222.79
 ---- batch: 110 ----
mean loss: 210.64
train mean loss: 216.50
epoch train time: 0:00:00.724326
elapsed time: 0:02:06.727651
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 01:46:51.801149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.64
 ---- batch: 020 ----
mean loss: 225.20
 ---- batch: 030 ----
mean loss: 215.22
 ---- batch: 040 ----
mean loss: 216.49
 ---- batch: 050 ----
mean loss: 219.74
 ---- batch: 060 ----
mean loss: 221.07
 ---- batch: 070 ----
mean loss: 207.36
 ---- batch: 080 ----
mean loss: 215.48
 ---- batch: 090 ----
mean loss: 216.24
 ---- batch: 100 ----
mean loss: 215.93
 ---- batch: 110 ----
mean loss: 214.76
train mean loss: 216.25
epoch train time: 0:00:00.720565
elapsed time: 0:02:07.448382
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 01:46:52.521864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.46
 ---- batch: 020 ----
mean loss: 215.96
 ---- batch: 030 ----
mean loss: 213.97
 ---- batch: 040 ----
mean loss: 225.36
 ---- batch: 050 ----
mean loss: 211.94
 ---- batch: 060 ----
mean loss: 214.13
 ---- batch: 070 ----
mean loss: 221.88
 ---- batch: 080 ----
mean loss: 218.21
 ---- batch: 090 ----
mean loss: 217.99
 ---- batch: 100 ----
mean loss: 211.92
 ---- batch: 110 ----
mean loss: 212.01
train mean loss: 215.96
epoch train time: 0:00:00.720353
elapsed time: 0:02:08.168887
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 01:46:53.242360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.01
 ---- batch: 020 ----
mean loss: 215.39
 ---- batch: 030 ----
mean loss: 221.79
 ---- batch: 040 ----
mean loss: 215.52
 ---- batch: 050 ----
mean loss: 216.12
 ---- batch: 060 ----
mean loss: 215.83
 ---- batch: 070 ----
mean loss: 217.46
 ---- batch: 080 ----
mean loss: 213.75
 ---- batch: 090 ----
mean loss: 211.59
 ---- batch: 100 ----
mean loss: 219.35
 ---- batch: 110 ----
mean loss: 212.51
train mean loss: 216.09
epoch train time: 0:00:00.730623
elapsed time: 0:02:08.899651
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 01:46:53.973123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.03
 ---- batch: 020 ----
mean loss: 222.51
 ---- batch: 030 ----
mean loss: 218.77
 ---- batch: 040 ----
mean loss: 220.47
 ---- batch: 050 ----
mean loss: 212.53
 ---- batch: 060 ----
mean loss: 215.17
 ---- batch: 070 ----
mean loss: 222.65
 ---- batch: 080 ----
mean loss: 212.55
 ---- batch: 090 ----
mean loss: 205.56
 ---- batch: 100 ----
mean loss: 213.13
 ---- batch: 110 ----
mean loss: 211.47
train mean loss: 215.49
epoch train time: 0:00:00.731780
elapsed time: 0:02:09.631579
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 01:46:54.705051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.38
 ---- batch: 020 ----
mean loss: 216.19
 ---- batch: 030 ----
mean loss: 216.06
 ---- batch: 040 ----
mean loss: 208.49
 ---- batch: 050 ----
mean loss: 216.20
 ---- batch: 060 ----
mean loss: 206.08
 ---- batch: 070 ----
mean loss: 213.99
 ---- batch: 080 ----
mean loss: 211.62
 ---- batch: 090 ----
mean loss: 224.53
 ---- batch: 100 ----
mean loss: 204.17
 ---- batch: 110 ----
mean loss: 219.66
train mean loss: 215.26
epoch train time: 0:00:00.729538
elapsed time: 0:02:10.361256
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 01:46:55.434726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.42
 ---- batch: 020 ----
mean loss: 217.14
 ---- batch: 030 ----
mean loss: 223.39
 ---- batch: 040 ----
mean loss: 214.16
 ---- batch: 050 ----
mean loss: 214.60
 ---- batch: 060 ----
mean loss: 219.30
 ---- batch: 070 ----
mean loss: 215.16
 ---- batch: 080 ----
mean loss: 210.89
 ---- batch: 090 ----
mean loss: 211.23
 ---- batch: 100 ----
mean loss: 214.11
 ---- batch: 110 ----
mean loss: 210.00
train mean loss: 215.21
epoch train time: 0:00:00.744252
elapsed time: 0:02:11.105670
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 01:46:56.179170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.76
 ---- batch: 020 ----
mean loss: 219.64
 ---- batch: 030 ----
mean loss: 223.37
 ---- batch: 040 ----
mean loss: 214.31
 ---- batch: 050 ----
mean loss: 212.58
 ---- batch: 060 ----
mean loss: 210.56
 ---- batch: 070 ----
mean loss: 213.20
 ---- batch: 080 ----
mean loss: 212.98
 ---- batch: 090 ----
mean loss: 221.74
 ---- batch: 100 ----
mean loss: 212.05
 ---- batch: 110 ----
mean loss: 212.50
train mean loss: 215.00
epoch train time: 0:00:00.754025
elapsed time: 0:02:11.859863
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 01:46:56.933334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.86
 ---- batch: 020 ----
mean loss: 222.65
 ---- batch: 030 ----
mean loss: 214.97
 ---- batch: 040 ----
mean loss: 216.51
 ---- batch: 050 ----
mean loss: 207.00
 ---- batch: 060 ----
mean loss: 210.62
 ---- batch: 070 ----
mean loss: 219.73
 ---- batch: 080 ----
mean loss: 210.82
 ---- batch: 090 ----
mean loss: 216.58
 ---- batch: 100 ----
mean loss: 206.52
 ---- batch: 110 ----
mean loss: 207.82
train mean loss: 214.72
epoch train time: 0:00:00.732631
elapsed time: 0:02:12.592669
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 01:46:57.666138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.99
 ---- batch: 020 ----
mean loss: 219.35
 ---- batch: 030 ----
mean loss: 216.18
 ---- batch: 040 ----
mean loss: 216.58
 ---- batch: 050 ----
mean loss: 219.72
 ---- batch: 060 ----
mean loss: 217.50
 ---- batch: 070 ----
mean loss: 204.02
 ---- batch: 080 ----
mean loss: 215.33
 ---- batch: 090 ----
mean loss: 205.49
 ---- batch: 100 ----
mean loss: 223.00
 ---- batch: 110 ----
mean loss: 218.47
train mean loss: 214.39
epoch train time: 0:00:00.732785
elapsed time: 0:02:13.325591
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 01:46:58.399073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.36
 ---- batch: 020 ----
mean loss: 219.84
 ---- batch: 030 ----
mean loss: 210.00
 ---- batch: 040 ----
mean loss: 218.00
 ---- batch: 050 ----
mean loss: 204.51
 ---- batch: 060 ----
mean loss: 221.44
 ---- batch: 070 ----
mean loss: 203.66
 ---- batch: 080 ----
mean loss: 215.57
 ---- batch: 090 ----
mean loss: 208.39
 ---- batch: 100 ----
mean loss: 219.04
 ---- batch: 110 ----
mean loss: 213.28
train mean loss: 214.15
epoch train time: 0:00:00.732017
elapsed time: 0:02:14.057757
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 01:46:59.131228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.48
 ---- batch: 020 ----
mean loss: 222.72
 ---- batch: 030 ----
mean loss: 217.85
 ---- batch: 040 ----
mean loss: 221.42
 ---- batch: 050 ----
mean loss: 215.24
 ---- batch: 060 ----
mean loss: 215.54
 ---- batch: 070 ----
mean loss: 209.82
 ---- batch: 080 ----
mean loss: 206.81
 ---- batch: 090 ----
mean loss: 215.84
 ---- batch: 100 ----
mean loss: 202.38
 ---- batch: 110 ----
mean loss: 216.17
train mean loss: 214.01
epoch train time: 0:00:00.726228
elapsed time: 0:02:14.784121
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 01:46:59.857589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.80
 ---- batch: 020 ----
mean loss: 212.84
 ---- batch: 030 ----
mean loss: 221.98
 ---- batch: 040 ----
mean loss: 220.60
 ---- batch: 050 ----
mean loss: 222.24
 ---- batch: 060 ----
mean loss: 205.27
 ---- batch: 070 ----
mean loss: 217.53
 ---- batch: 080 ----
mean loss: 212.91
 ---- batch: 090 ----
mean loss: 222.57
 ---- batch: 100 ----
mean loss: 209.65
 ---- batch: 110 ----
mean loss: 204.02
train mean loss: 213.97
epoch train time: 0:00:00.724857
elapsed time: 0:02:15.509116
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 01:47:00.582588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.67
 ---- batch: 020 ----
mean loss: 221.90
 ---- batch: 030 ----
mean loss: 215.91
 ---- batch: 040 ----
mean loss: 207.54
 ---- batch: 050 ----
mean loss: 206.05
 ---- batch: 060 ----
mean loss: 214.40
 ---- batch: 070 ----
mean loss: 214.67
 ---- batch: 080 ----
mean loss: 218.30
 ---- batch: 090 ----
mean loss: 213.89
 ---- batch: 100 ----
mean loss: 211.10
 ---- batch: 110 ----
mean loss: 208.49
train mean loss: 213.54
epoch train time: 0:00:00.727983
elapsed time: 0:02:16.237236
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 01:47:01.310724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.77
 ---- batch: 020 ----
mean loss: 224.44
 ---- batch: 030 ----
mean loss: 210.62
 ---- batch: 040 ----
mean loss: 212.83
 ---- batch: 050 ----
mean loss: 208.00
 ---- batch: 060 ----
mean loss: 210.54
 ---- batch: 070 ----
mean loss: 211.22
 ---- batch: 080 ----
mean loss: 211.50
 ---- batch: 090 ----
mean loss: 204.53
 ---- batch: 100 ----
mean loss: 218.97
 ---- batch: 110 ----
mean loss: 222.00
train mean loss: 213.46
epoch train time: 0:00:00.732290
elapsed time: 0:02:16.969697
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 01:47:02.043166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.34
 ---- batch: 020 ----
mean loss: 206.46
 ---- batch: 030 ----
mean loss: 204.50
 ---- batch: 040 ----
mean loss: 211.48
 ---- batch: 050 ----
mean loss: 221.24
 ---- batch: 060 ----
mean loss: 202.96
 ---- batch: 070 ----
mean loss: 216.81
 ---- batch: 080 ----
mean loss: 223.37
 ---- batch: 090 ----
mean loss: 216.81
 ---- batch: 100 ----
mean loss: 218.96
 ---- batch: 110 ----
mean loss: 207.08
train mean loss: 213.18
epoch train time: 0:00:00.726223
elapsed time: 0:02:17.696081
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 01:47:02.769551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.71
 ---- batch: 020 ----
mean loss: 210.33
 ---- batch: 030 ----
mean loss: 215.59
 ---- batch: 040 ----
mean loss: 219.97
 ---- batch: 050 ----
mean loss: 210.57
 ---- batch: 060 ----
mean loss: 217.91
 ---- batch: 070 ----
mean loss: 207.25
 ---- batch: 080 ----
mean loss: 221.84
 ---- batch: 090 ----
mean loss: 209.87
 ---- batch: 100 ----
mean loss: 214.25
 ---- batch: 110 ----
mean loss: 208.05
train mean loss: 213.04
epoch train time: 0:00:00.730824
elapsed time: 0:02:18.427051
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 01:47:03.500542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.67
 ---- batch: 020 ----
mean loss: 217.62
 ---- batch: 030 ----
mean loss: 222.27
 ---- batch: 040 ----
mean loss: 208.51
 ---- batch: 050 ----
mean loss: 218.69
 ---- batch: 060 ----
mean loss: 217.40
 ---- batch: 070 ----
mean loss: 211.56
 ---- batch: 080 ----
mean loss: 213.76
 ---- batch: 090 ----
mean loss: 209.71
 ---- batch: 100 ----
mean loss: 209.55
 ---- batch: 110 ----
mean loss: 212.39
train mean loss: 212.65
epoch train time: 0:00:00.730252
elapsed time: 0:02:19.157464
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 01:47:04.230933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.26
 ---- batch: 020 ----
mean loss: 218.09
 ---- batch: 030 ----
mean loss: 205.34
 ---- batch: 040 ----
mean loss: 215.48
 ---- batch: 050 ----
mean loss: 206.95
 ---- batch: 060 ----
mean loss: 211.23
 ---- batch: 070 ----
mean loss: 220.32
 ---- batch: 080 ----
mean loss: 206.87
 ---- batch: 090 ----
mean loss: 216.58
 ---- batch: 100 ----
mean loss: 206.09
 ---- batch: 110 ----
mean loss: 212.58
train mean loss: 212.66
epoch train time: 0:00:00.737490
elapsed time: 0:02:19.895091
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 01:47:04.968562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.96
 ---- batch: 020 ----
mean loss: 214.98
 ---- batch: 030 ----
mean loss: 211.49
 ---- batch: 040 ----
mean loss: 212.04
 ---- batch: 050 ----
mean loss: 218.50
 ---- batch: 060 ----
mean loss: 207.39
 ---- batch: 070 ----
mean loss: 214.08
 ---- batch: 080 ----
mean loss: 211.04
 ---- batch: 090 ----
mean loss: 212.10
 ---- batch: 100 ----
mean loss: 222.68
 ---- batch: 110 ----
mean loss: 201.24
train mean loss: 212.27
epoch train time: 0:00:00.735332
elapsed time: 0:02:20.630563
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 01:47:05.704055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.35
 ---- batch: 020 ----
mean loss: 216.52
 ---- batch: 030 ----
mean loss: 219.77
 ---- batch: 040 ----
mean loss: 207.29
 ---- batch: 050 ----
mean loss: 213.27
 ---- batch: 060 ----
mean loss: 201.93
 ---- batch: 070 ----
mean loss: 210.19
 ---- batch: 080 ----
mean loss: 222.84
 ---- batch: 090 ----
mean loss: 215.71
 ---- batch: 100 ----
mean loss: 218.26
 ---- batch: 110 ----
mean loss: 202.91
train mean loss: 212.09
epoch train time: 0:00:00.728010
elapsed time: 0:02:21.358745
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 01:47:06.432206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.25
 ---- batch: 020 ----
mean loss: 209.32
 ---- batch: 030 ----
mean loss: 203.17
 ---- batch: 040 ----
mean loss: 199.53
 ---- batch: 050 ----
mean loss: 210.60
 ---- batch: 060 ----
mean loss: 213.10
 ---- batch: 070 ----
mean loss: 216.00
 ---- batch: 080 ----
mean loss: 216.79
 ---- batch: 090 ----
mean loss: 213.47
 ---- batch: 100 ----
mean loss: 216.79
 ---- batch: 110 ----
mean loss: 210.13
train mean loss: 211.86
epoch train time: 0:00:00.737572
elapsed time: 0:02:22.096445
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 01:47:07.169919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.15
 ---- batch: 020 ----
mean loss: 206.88
 ---- batch: 030 ----
mean loss: 215.32
 ---- batch: 040 ----
mean loss: 204.46
 ---- batch: 050 ----
mean loss: 210.75
 ---- batch: 060 ----
mean loss: 220.45
 ---- batch: 070 ----
mean loss: 207.26
 ---- batch: 080 ----
mean loss: 216.49
 ---- batch: 090 ----
mean loss: 215.27
 ---- batch: 100 ----
mean loss: 205.24
 ---- batch: 110 ----
mean loss: 210.64
train mean loss: 211.80
epoch train time: 0:00:00.733070
elapsed time: 0:02:22.829658
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 01:47:07.903132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.83
 ---- batch: 020 ----
mean loss: 206.00
 ---- batch: 030 ----
mean loss: 214.60
 ---- batch: 040 ----
mean loss: 213.83
 ---- batch: 050 ----
mean loss: 210.73
 ---- batch: 060 ----
mean loss: 215.97
 ---- batch: 070 ----
mean loss: 212.83
 ---- batch: 080 ----
mean loss: 208.74
 ---- batch: 090 ----
mean loss: 211.83
 ---- batch: 100 ----
mean loss: 218.73
 ---- batch: 110 ----
mean loss: 203.59
train mean loss: 211.76
epoch train time: 0:00:00.727071
elapsed time: 0:02:23.556874
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 01:47:08.630346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.57
 ---- batch: 020 ----
mean loss: 208.17
 ---- batch: 030 ----
mean loss: 207.90
 ---- batch: 040 ----
mean loss: 203.50
 ---- batch: 050 ----
mean loss: 214.93
 ---- batch: 060 ----
mean loss: 206.54
 ---- batch: 070 ----
mean loss: 205.16
 ---- batch: 080 ----
mean loss: 219.29
 ---- batch: 090 ----
mean loss: 218.37
 ---- batch: 100 ----
mean loss: 211.07
 ---- batch: 110 ----
mean loss: 205.24
train mean loss: 211.26
epoch train time: 0:00:00.733372
elapsed time: 0:02:24.290391
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 01:47:09.363881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.52
 ---- batch: 020 ----
mean loss: 199.16
 ---- batch: 030 ----
mean loss: 211.05
 ---- batch: 040 ----
mean loss: 207.79
 ---- batch: 050 ----
mean loss: 218.15
 ---- batch: 060 ----
mean loss: 214.93
 ---- batch: 070 ----
mean loss: 214.64
 ---- batch: 080 ----
mean loss: 209.50
 ---- batch: 090 ----
mean loss: 208.59
 ---- batch: 100 ----
mean loss: 209.83
 ---- batch: 110 ----
mean loss: 213.28
train mean loss: 211.11
epoch train time: 0:00:00.728694
elapsed time: 0:02:25.019239
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 01:47:10.092708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.45
 ---- batch: 020 ----
mean loss: 207.71
 ---- batch: 030 ----
mean loss: 214.85
 ---- batch: 040 ----
mean loss: 209.27
 ---- batch: 050 ----
mean loss: 211.07
 ---- batch: 060 ----
mean loss: 206.29
 ---- batch: 070 ----
mean loss: 209.26
 ---- batch: 080 ----
mean loss: 209.41
 ---- batch: 090 ----
mean loss: 216.31
 ---- batch: 100 ----
mean loss: 207.72
 ---- batch: 110 ----
mean loss: 216.78
train mean loss: 210.74
epoch train time: 0:00:00.737235
elapsed time: 0:02:25.756618
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 01:47:10.830113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.50
 ---- batch: 020 ----
mean loss: 207.79
 ---- batch: 030 ----
mean loss: 207.06
 ---- batch: 040 ----
mean loss: 212.36
 ---- batch: 050 ----
mean loss: 204.52
 ---- batch: 060 ----
mean loss: 207.86
 ---- batch: 070 ----
mean loss: 217.94
 ---- batch: 080 ----
mean loss: 209.04
 ---- batch: 090 ----
mean loss: 213.73
 ---- batch: 100 ----
mean loss: 211.23
 ---- batch: 110 ----
mean loss: 213.80
train mean loss: 210.77
epoch train time: 0:00:00.724143
elapsed time: 0:02:26.480921
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 01:47:11.554388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.48
 ---- batch: 020 ----
mean loss: 204.87
 ---- batch: 030 ----
mean loss: 213.23
 ---- batch: 040 ----
mean loss: 222.87
 ---- batch: 050 ----
mean loss: 211.20
 ---- batch: 060 ----
mean loss: 211.68
 ---- batch: 070 ----
mean loss: 209.75
 ---- batch: 080 ----
mean loss: 206.80
 ---- batch: 090 ----
mean loss: 204.56
 ---- batch: 100 ----
mean loss: 212.04
 ---- batch: 110 ----
mean loss: 208.22
train mean loss: 210.65
epoch train time: 0:00:00.739272
elapsed time: 0:02:27.220329
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 01:47:12.293802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.68
 ---- batch: 020 ----
mean loss: 209.52
 ---- batch: 030 ----
mean loss: 212.45
 ---- batch: 040 ----
mean loss: 210.48
 ---- batch: 050 ----
mean loss: 198.94
 ---- batch: 060 ----
mean loss: 217.34
 ---- batch: 070 ----
mean loss: 202.41
 ---- batch: 080 ----
mean loss: 209.43
 ---- batch: 090 ----
mean loss: 215.59
 ---- batch: 100 ----
mean loss: 212.77
 ---- batch: 110 ----
mean loss: 216.78
train mean loss: 210.37
epoch train time: 0:00:00.743366
elapsed time: 0:02:27.963833
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 01:47:13.037305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.45
 ---- batch: 020 ----
mean loss: 204.69
 ---- batch: 030 ----
mean loss: 214.43
 ---- batch: 040 ----
mean loss: 202.87
 ---- batch: 050 ----
mean loss: 207.80
 ---- batch: 060 ----
mean loss: 207.95
 ---- batch: 070 ----
mean loss: 210.80
 ---- batch: 080 ----
mean loss: 206.52
 ---- batch: 090 ----
mean loss: 203.75
 ---- batch: 100 ----
mean loss: 215.05
 ---- batch: 110 ----
mean loss: 219.63
train mean loss: 210.07
epoch train time: 0:00:00.730290
elapsed time: 0:02:28.694263
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 01:47:13.767734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.05
 ---- batch: 020 ----
mean loss: 207.51
 ---- batch: 030 ----
mean loss: 202.46
 ---- batch: 040 ----
mean loss: 213.01
 ---- batch: 050 ----
mean loss: 212.41
 ---- batch: 060 ----
mean loss: 204.29
 ---- batch: 070 ----
mean loss: 215.97
 ---- batch: 080 ----
mean loss: 210.62
 ---- batch: 090 ----
mean loss: 204.89
 ---- batch: 100 ----
mean loss: 221.57
 ---- batch: 110 ----
mean loss: 207.97
train mean loss: 210.11
epoch train time: 0:00:00.729758
elapsed time: 0:02:29.424168
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 01:47:14.497679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.14
 ---- batch: 020 ----
mean loss: 207.71
 ---- batch: 030 ----
mean loss: 206.09
 ---- batch: 040 ----
mean loss: 207.52
 ---- batch: 050 ----
mean loss: 227.86
 ---- batch: 060 ----
mean loss: 203.09
 ---- batch: 070 ----
mean loss: 208.32
 ---- batch: 080 ----
mean loss: 216.90
 ---- batch: 090 ----
mean loss: 213.54
 ---- batch: 100 ----
mean loss: 199.65
 ---- batch: 110 ----
mean loss: 204.09
train mean loss: 209.64
epoch train time: 0:00:00.736382
elapsed time: 0:02:30.160738
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 01:47:15.234244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.33
 ---- batch: 020 ----
mean loss: 212.90
 ---- batch: 030 ----
mean loss: 210.97
 ---- batch: 040 ----
mean loss: 211.17
 ---- batch: 050 ----
mean loss: 220.03
 ---- batch: 060 ----
mean loss: 208.63
 ---- batch: 070 ----
mean loss: 209.73
 ---- batch: 080 ----
mean loss: 210.01
 ---- batch: 090 ----
mean loss: 208.43
 ---- batch: 100 ----
mean loss: 214.13
 ---- batch: 110 ----
mean loss: 206.65
train mean loss: 209.56
epoch train time: 0:00:00.731103
elapsed time: 0:02:30.892022
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 01:47:15.965495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.22
 ---- batch: 020 ----
mean loss: 212.19
 ---- batch: 030 ----
mean loss: 216.26
 ---- batch: 040 ----
mean loss: 200.58
 ---- batch: 050 ----
mean loss: 219.08
 ---- batch: 060 ----
mean loss: 211.39
 ---- batch: 070 ----
mean loss: 198.75
 ---- batch: 080 ----
mean loss: 191.38
 ---- batch: 090 ----
mean loss: 208.32
 ---- batch: 100 ----
mean loss: 215.28
 ---- batch: 110 ----
mean loss: 214.43
train mean loss: 209.52
epoch train time: 0:00:00.737387
elapsed time: 0:02:31.629625
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 01:47:16.703121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.38
 ---- batch: 020 ----
mean loss: 211.99
 ---- batch: 030 ----
mean loss: 216.13
 ---- batch: 040 ----
mean loss: 218.69
 ---- batch: 050 ----
mean loss: 209.88
 ---- batch: 060 ----
mean loss: 206.78
 ---- batch: 070 ----
mean loss: 208.78
 ---- batch: 080 ----
mean loss: 209.82
 ---- batch: 090 ----
mean loss: 199.28
 ---- batch: 100 ----
mean loss: 198.02
 ---- batch: 110 ----
mean loss: 211.05
train mean loss: 209.18
epoch train time: 0:00:00.729467
elapsed time: 0:02:32.359275
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 01:47:17.432745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.19
 ---- batch: 020 ----
mean loss: 214.45
 ---- batch: 030 ----
mean loss: 205.13
 ---- batch: 040 ----
mean loss: 209.06
 ---- batch: 050 ----
mean loss: 210.72
 ---- batch: 060 ----
mean loss: 205.11
 ---- batch: 070 ----
mean loss: 224.77
 ---- batch: 080 ----
mean loss: 207.03
 ---- batch: 090 ----
mean loss: 201.90
 ---- batch: 100 ----
mean loss: 210.28
 ---- batch: 110 ----
mean loss: 206.19
train mean loss: 209.15
epoch train time: 0:00:00.741591
elapsed time: 0:02:33.101047
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 01:47:18.174537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.31
 ---- batch: 020 ----
mean loss: 204.65
 ---- batch: 030 ----
mean loss: 213.54
 ---- batch: 040 ----
mean loss: 207.90
 ---- batch: 050 ----
mean loss: 221.49
 ---- batch: 060 ----
mean loss: 208.11
 ---- batch: 070 ----
mean loss: 206.99
 ---- batch: 080 ----
mean loss: 210.58
 ---- batch: 090 ----
mean loss: 209.91
 ---- batch: 100 ----
mean loss: 202.59
 ---- batch: 110 ----
mean loss: 200.07
train mean loss: 208.89
epoch train time: 0:00:00.744966
elapsed time: 0:02:33.846173
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 01:47:18.919659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.23
 ---- batch: 020 ----
mean loss: 208.98
 ---- batch: 030 ----
mean loss: 207.81
 ---- batch: 040 ----
mean loss: 202.02
 ---- batch: 050 ----
mean loss: 201.40
 ---- batch: 060 ----
mean loss: 212.14
 ---- batch: 070 ----
mean loss: 196.34
 ---- batch: 080 ----
mean loss: 218.88
 ---- batch: 090 ----
mean loss: 215.48
 ---- batch: 100 ----
mean loss: 222.92
 ---- batch: 110 ----
mean loss: 203.15
train mean loss: 208.85
epoch train time: 0:00:00.740601
elapsed time: 0:02:34.586935
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 01:47:19.660416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.18
 ---- batch: 020 ----
mean loss: 207.09
 ---- batch: 030 ----
mean loss: 205.13
 ---- batch: 040 ----
mean loss: 201.21
 ---- batch: 050 ----
mean loss: 210.39
 ---- batch: 060 ----
mean loss: 206.04
 ---- batch: 070 ----
mean loss: 206.70
 ---- batch: 080 ----
mean loss: 203.66
 ---- batch: 090 ----
mean loss: 216.28
 ---- batch: 100 ----
mean loss: 211.99
 ---- batch: 110 ----
mean loss: 212.15
train mean loss: 208.46
epoch train time: 0:00:00.747454
elapsed time: 0:02:35.334541
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 01:47:20.408037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.83
 ---- batch: 020 ----
mean loss: 216.04
 ---- batch: 030 ----
mean loss: 207.87
 ---- batch: 040 ----
mean loss: 220.42
 ---- batch: 050 ----
mean loss: 217.79
 ---- batch: 060 ----
mean loss: 205.09
 ---- batch: 070 ----
mean loss: 197.16
 ---- batch: 080 ----
mean loss: 206.82
 ---- batch: 090 ----
mean loss: 201.58
 ---- batch: 100 ----
mean loss: 205.43
 ---- batch: 110 ----
mean loss: 201.94
train mean loss: 208.49
epoch train time: 0:00:00.747224
elapsed time: 0:02:36.081929
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 01:47:21.155402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.20
 ---- batch: 020 ----
mean loss: 208.69
 ---- batch: 030 ----
mean loss: 197.64
 ---- batch: 040 ----
mean loss: 219.18
 ---- batch: 050 ----
mean loss: 211.89
 ---- batch: 060 ----
mean loss: 211.24
 ---- batch: 070 ----
mean loss: 209.11
 ---- batch: 080 ----
mean loss: 214.72
 ---- batch: 090 ----
mean loss: 202.19
 ---- batch: 100 ----
mean loss: 210.46
 ---- batch: 110 ----
mean loss: 203.88
train mean loss: 208.40
epoch train time: 0:00:00.733414
elapsed time: 0:02:36.815509
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 01:47:21.888977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.52
 ---- batch: 020 ----
mean loss: 214.01
 ---- batch: 030 ----
mean loss: 215.43
 ---- batch: 040 ----
mean loss: 204.71
 ---- batch: 050 ----
mean loss: 200.70
 ---- batch: 060 ----
mean loss: 213.02
 ---- batch: 070 ----
mean loss: 206.32
 ---- batch: 080 ----
mean loss: 211.26
 ---- batch: 090 ----
mean loss: 211.39
 ---- batch: 100 ----
mean loss: 201.47
 ---- batch: 110 ----
mean loss: 201.16
train mean loss: 208.14
epoch train time: 0:00:00.735156
elapsed time: 0:02:37.550800
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 01:47:22.624270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.54
 ---- batch: 020 ----
mean loss: 219.89
 ---- batch: 030 ----
mean loss: 213.34
 ---- batch: 040 ----
mean loss: 205.43
 ---- batch: 050 ----
mean loss: 210.06
 ---- batch: 060 ----
mean loss: 200.28
 ---- batch: 070 ----
mean loss: 216.24
 ---- batch: 080 ----
mean loss: 202.57
 ---- batch: 090 ----
mean loss: 210.37
 ---- batch: 100 ----
mean loss: 204.26
 ---- batch: 110 ----
mean loss: 206.73
train mean loss: 207.91
epoch train time: 0:00:00.738838
elapsed time: 0:02:38.289786
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 01:47:23.363262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.70
 ---- batch: 020 ----
mean loss: 213.82
 ---- batch: 030 ----
mean loss: 206.87
 ---- batch: 040 ----
mean loss: 200.52
 ---- batch: 050 ----
mean loss: 208.14
 ---- batch: 060 ----
mean loss: 206.11
 ---- batch: 070 ----
mean loss: 197.09
 ---- batch: 080 ----
mean loss: 209.59
 ---- batch: 090 ----
mean loss: 215.74
 ---- batch: 100 ----
mean loss: 204.99
 ---- batch: 110 ----
mean loss: 214.78
train mean loss: 207.68
epoch train time: 0:00:00.737599
elapsed time: 0:02:39.027567
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 01:47:24.101040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.69
 ---- batch: 020 ----
mean loss: 205.87
 ---- batch: 030 ----
mean loss: 215.89
 ---- batch: 040 ----
mean loss: 209.87
 ---- batch: 050 ----
mean loss: 202.86
 ---- batch: 060 ----
mean loss: 204.67
 ---- batch: 070 ----
mean loss: 207.96
 ---- batch: 080 ----
mean loss: 201.82
 ---- batch: 090 ----
mean loss: 215.31
 ---- batch: 100 ----
mean loss: 197.73
 ---- batch: 110 ----
mean loss: 211.84
train mean loss: 207.61
epoch train time: 0:00:00.730746
elapsed time: 0:02:39.758467
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 01:47:24.831953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.34
 ---- batch: 020 ----
mean loss: 216.63
 ---- batch: 030 ----
mean loss: 209.52
 ---- batch: 040 ----
mean loss: 207.22
 ---- batch: 050 ----
mean loss: 210.84
 ---- batch: 060 ----
mean loss: 206.62
 ---- batch: 070 ----
mean loss: 205.87
 ---- batch: 080 ----
mean loss: 199.23
 ---- batch: 090 ----
mean loss: 205.77
 ---- batch: 100 ----
mean loss: 208.75
 ---- batch: 110 ----
mean loss: 205.21
train mean loss: 207.28
epoch train time: 0:00:00.727425
elapsed time: 0:02:40.486055
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 01:47:25.559547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.96
 ---- batch: 020 ----
mean loss: 205.73
 ---- batch: 030 ----
mean loss: 202.50
 ---- batch: 040 ----
mean loss: 204.75
 ---- batch: 050 ----
mean loss: 202.84
 ---- batch: 060 ----
mean loss: 210.32
 ---- batch: 070 ----
mean loss: 212.18
 ---- batch: 080 ----
mean loss: 202.14
 ---- batch: 090 ----
mean loss: 213.39
 ---- batch: 100 ----
mean loss: 208.38
 ---- batch: 110 ----
mean loss: 200.66
train mean loss: 207.30
epoch train time: 0:00:00.732007
elapsed time: 0:02:41.218229
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 01:47:26.291701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.18
 ---- batch: 020 ----
mean loss: 211.32
 ---- batch: 030 ----
mean loss: 194.72
 ---- batch: 040 ----
mean loss: 218.10
 ---- batch: 050 ----
mean loss: 212.13
 ---- batch: 060 ----
mean loss: 207.36
 ---- batch: 070 ----
mean loss: 207.20
 ---- batch: 080 ----
mean loss: 209.61
 ---- batch: 090 ----
mean loss: 200.32
 ---- batch: 100 ----
mean loss: 206.05
 ---- batch: 110 ----
mean loss: 201.53
train mean loss: 207.01
epoch train time: 0:00:00.732540
elapsed time: 0:02:41.950912
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 01:47:27.024384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.81
 ---- batch: 020 ----
mean loss: 221.43
 ---- batch: 030 ----
mean loss: 211.18
 ---- batch: 040 ----
mean loss: 198.06
 ---- batch: 050 ----
mean loss: 206.57
 ---- batch: 060 ----
mean loss: 205.80
 ---- batch: 070 ----
mean loss: 198.45
 ---- batch: 080 ----
mean loss: 209.42
 ---- batch: 090 ----
mean loss: 206.58
 ---- batch: 100 ----
mean loss: 206.35
 ---- batch: 110 ----
mean loss: 195.89
train mean loss: 206.89
epoch train time: 0:00:00.733923
elapsed time: 0:02:42.684996
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 01:47:27.758503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.16
 ---- batch: 020 ----
mean loss: 215.16
 ---- batch: 030 ----
mean loss: 207.37
 ---- batch: 040 ----
mean loss: 200.69
 ---- batch: 050 ----
mean loss: 200.62
 ---- batch: 060 ----
mean loss: 210.25
 ---- batch: 070 ----
mean loss: 204.11
 ---- batch: 080 ----
mean loss: 201.84
 ---- batch: 090 ----
mean loss: 209.80
 ---- batch: 100 ----
mean loss: 204.21
 ---- batch: 110 ----
mean loss: 213.01
train mean loss: 206.81
epoch train time: 0:00:00.739421
elapsed time: 0:02:43.424620
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 01:47:28.498096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.08
 ---- batch: 020 ----
mean loss: 199.03
 ---- batch: 030 ----
mean loss: 210.04
 ---- batch: 040 ----
mean loss: 212.00
 ---- batch: 050 ----
mean loss: 211.78
 ---- batch: 060 ----
mean loss: 199.57
 ---- batch: 070 ----
mean loss: 212.62
 ---- batch: 080 ----
mean loss: 208.09
 ---- batch: 090 ----
mean loss: 204.14
 ---- batch: 100 ----
mean loss: 202.90
 ---- batch: 110 ----
mean loss: 216.35
train mean loss: 206.86
epoch train time: 0:00:00.732177
elapsed time: 0:02:44.156948
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 01:47:29.230428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.52
 ---- batch: 020 ----
mean loss: 212.96
 ---- batch: 030 ----
mean loss: 201.35
 ---- batch: 040 ----
mean loss: 199.29
 ---- batch: 050 ----
mean loss: 212.72
 ---- batch: 060 ----
mean loss: 211.82
 ---- batch: 070 ----
mean loss: 204.96
 ---- batch: 080 ----
mean loss: 209.61
 ---- batch: 090 ----
mean loss: 201.30
 ---- batch: 100 ----
mean loss: 208.96
 ---- batch: 110 ----
mean loss: 208.36
train mean loss: 206.32
epoch train time: 0:00:00.734276
elapsed time: 0:02:44.891374
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 01:47:29.964865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.68
 ---- batch: 020 ----
mean loss: 205.21
 ---- batch: 030 ----
mean loss: 218.33
 ---- batch: 040 ----
mean loss: 195.12
 ---- batch: 050 ----
mean loss: 202.17
 ---- batch: 060 ----
mean loss: 199.33
 ---- batch: 070 ----
mean loss: 200.08
 ---- batch: 080 ----
mean loss: 206.68
 ---- batch: 090 ----
mean loss: 212.44
 ---- batch: 100 ----
mean loss: 210.24
 ---- batch: 110 ----
mean loss: 206.51
train mean loss: 206.18
epoch train time: 0:00:00.732360
elapsed time: 0:02:45.623894
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 01:47:30.697365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.02
 ---- batch: 020 ----
mean loss: 213.66
 ---- batch: 030 ----
mean loss: 204.72
 ---- batch: 040 ----
mean loss: 211.68
 ---- batch: 050 ----
mean loss: 201.93
 ---- batch: 060 ----
mean loss: 208.37
 ---- batch: 070 ----
mean loss: 205.18
 ---- batch: 080 ----
mean loss: 201.07
 ---- batch: 090 ----
mean loss: 206.24
 ---- batch: 100 ----
mean loss: 200.91
 ---- batch: 110 ----
mean loss: 196.99
train mean loss: 205.91
epoch train time: 0:00:00.732921
elapsed time: 0:02:46.356957
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 01:47:31.430437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.66
 ---- batch: 020 ----
mean loss: 208.89
 ---- batch: 030 ----
mean loss: 197.51
 ---- batch: 040 ----
mean loss: 209.49
 ---- batch: 050 ----
mean loss: 195.70
 ---- batch: 060 ----
mean loss: 200.91
 ---- batch: 070 ----
mean loss: 204.98
 ---- batch: 080 ----
mean loss: 206.67
 ---- batch: 090 ----
mean loss: 217.95
 ---- batch: 100 ----
mean loss: 209.72
 ---- batch: 110 ----
mean loss: 202.24
train mean loss: 205.45
epoch train time: 0:00:00.729331
elapsed time: 0:02:47.086436
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 01:47:32.159905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.60
 ---- batch: 020 ----
mean loss: 209.09
 ---- batch: 030 ----
mean loss: 207.60
 ---- batch: 040 ----
mean loss: 201.64
 ---- batch: 050 ----
mean loss: 209.39
 ---- batch: 060 ----
mean loss: 211.93
 ---- batch: 070 ----
mean loss: 206.08
 ---- batch: 080 ----
mean loss: 202.98
 ---- batch: 090 ----
mean loss: 197.73
 ---- batch: 100 ----
mean loss: 201.75
 ---- batch: 110 ----
mean loss: 210.79
train mean loss: 205.19
epoch train time: 0:00:00.739740
elapsed time: 0:02:47.826314
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 01:47:32.899784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.64
 ---- batch: 020 ----
mean loss: 196.08
 ---- batch: 030 ----
mean loss: 212.80
 ---- batch: 040 ----
mean loss: 204.55
 ---- batch: 050 ----
mean loss: 206.56
 ---- batch: 060 ----
mean loss: 206.32
 ---- batch: 070 ----
mean loss: 205.55
 ---- batch: 080 ----
mean loss: 199.66
 ---- batch: 090 ----
mean loss: 201.44
 ---- batch: 100 ----
mean loss: 209.33
 ---- batch: 110 ----
mean loss: 209.62
train mean loss: 205.12
epoch train time: 0:00:00.731940
elapsed time: 0:02:48.558390
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 01:47:33.631862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.30
 ---- batch: 020 ----
mean loss: 215.45
 ---- batch: 030 ----
mean loss: 209.40
 ---- batch: 040 ----
mean loss: 199.81
 ---- batch: 050 ----
mean loss: 204.72
 ---- batch: 060 ----
mean loss: 205.05
 ---- batch: 070 ----
mean loss: 205.62
 ---- batch: 080 ----
mean loss: 202.59
 ---- batch: 090 ----
mean loss: 195.63
 ---- batch: 100 ----
mean loss: 199.00
 ---- batch: 110 ----
mean loss: 205.80
train mean loss: 204.83
epoch train time: 0:00:00.734253
elapsed time: 0:02:49.292785
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 01:47:34.366257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.01
 ---- batch: 020 ----
mean loss: 200.37
 ---- batch: 030 ----
mean loss: 204.94
 ---- batch: 040 ----
mean loss: 207.25
 ---- batch: 050 ----
mean loss: 208.45
 ---- batch: 060 ----
mean loss: 212.83
 ---- batch: 070 ----
mean loss: 200.37
 ---- batch: 080 ----
mean loss: 201.21
 ---- batch: 090 ----
mean loss: 213.95
 ---- batch: 100 ----
mean loss: 203.90
 ---- batch: 110 ----
mean loss: 202.29
train mean loss: 204.82
epoch train time: 0:00:00.727146
elapsed time: 0:02:50.020067
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 01:47:35.093554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.69
 ---- batch: 020 ----
mean loss: 205.85
 ---- batch: 030 ----
mean loss: 214.41
 ---- batch: 040 ----
mean loss: 196.64
 ---- batch: 050 ----
mean loss: 205.40
 ---- batch: 060 ----
mean loss: 200.00
 ---- batch: 070 ----
mean loss: 206.43
 ---- batch: 080 ----
mean loss: 199.91
 ---- batch: 090 ----
mean loss: 206.68
 ---- batch: 100 ----
mean loss: 205.91
 ---- batch: 110 ----
mean loss: 200.64
train mean loss: 204.35
epoch train time: 0:00:00.734685
elapsed time: 0:02:50.754911
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 01:47:35.828384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.90
 ---- batch: 020 ----
mean loss: 206.06
 ---- batch: 030 ----
mean loss: 216.43
 ---- batch: 040 ----
mean loss: 206.13
 ---- batch: 050 ----
mean loss: 204.06
 ---- batch: 060 ----
mean loss: 203.40
 ---- batch: 070 ----
mean loss: 202.19
 ---- batch: 080 ----
mean loss: 210.15
 ---- batch: 090 ----
mean loss: 204.78
 ---- batch: 100 ----
mean loss: 194.05
 ---- batch: 110 ----
mean loss: 197.11
train mean loss: 204.20
epoch train time: 0:00:00.740086
elapsed time: 0:02:51.495140
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 01:47:36.568612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.64
 ---- batch: 020 ----
mean loss: 198.68
 ---- batch: 030 ----
mean loss: 213.24
 ---- batch: 040 ----
mean loss: 202.38
 ---- batch: 050 ----
mean loss: 195.73
 ---- batch: 060 ----
mean loss: 210.49
 ---- batch: 070 ----
mean loss: 212.00
 ---- batch: 080 ----
mean loss: 200.88
 ---- batch: 090 ----
mean loss: 201.29
 ---- batch: 100 ----
mean loss: 206.96
 ---- batch: 110 ----
mean loss: 210.51
train mean loss: 204.13
epoch train time: 0:00:00.733032
elapsed time: 0:02:52.228312
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 01:47:37.301787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.65
 ---- batch: 020 ----
mean loss: 196.81
 ---- batch: 030 ----
mean loss: 201.29
 ---- batch: 040 ----
mean loss: 206.79
 ---- batch: 050 ----
mean loss: 212.63
 ---- batch: 060 ----
mean loss: 205.42
 ---- batch: 070 ----
mean loss: 201.18
 ---- batch: 080 ----
mean loss: 198.88
 ---- batch: 090 ----
mean loss: 206.99
 ---- batch: 100 ----
mean loss: 208.39
 ---- batch: 110 ----
mean loss: 201.14
train mean loss: 203.86
epoch train time: 0:00:00.737284
elapsed time: 0:02:52.965746
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 01:47:38.039218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.61
 ---- batch: 020 ----
mean loss: 201.42
 ---- batch: 030 ----
mean loss: 201.76
 ---- batch: 040 ----
mean loss: 202.23
 ---- batch: 050 ----
mean loss: 210.27
 ---- batch: 060 ----
mean loss: 213.31
 ---- batch: 070 ----
mean loss: 196.91
 ---- batch: 080 ----
mean loss: 204.96
 ---- batch: 090 ----
mean loss: 198.51
 ---- batch: 100 ----
mean loss: 197.49
 ---- batch: 110 ----
mean loss: 200.75
train mean loss: 203.61
epoch train time: 0:00:00.732333
elapsed time: 0:02:53.698224
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 01:47:38.771695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.64
 ---- batch: 020 ----
mean loss: 207.78
 ---- batch: 030 ----
mean loss: 203.84
 ---- batch: 040 ----
mean loss: 208.09
 ---- batch: 050 ----
mean loss: 209.36
 ---- batch: 060 ----
mean loss: 195.28
 ---- batch: 070 ----
mean loss: 194.15
 ---- batch: 080 ----
mean loss: 200.60
 ---- batch: 090 ----
mean loss: 196.28
 ---- batch: 100 ----
mean loss: 208.90
 ---- batch: 110 ----
mean loss: 210.48
train mean loss: 203.37
epoch train time: 0:00:00.745226
elapsed time: 0:02:54.443600
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 01:47:39.517093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.31
 ---- batch: 020 ----
mean loss: 211.96
 ---- batch: 030 ----
mean loss: 209.12
 ---- batch: 040 ----
mean loss: 201.34
 ---- batch: 050 ----
mean loss: 195.83
 ---- batch: 060 ----
mean loss: 198.41
 ---- batch: 070 ----
mean loss: 212.32
 ---- batch: 080 ----
mean loss: 196.76
 ---- batch: 090 ----
mean loss: 205.26
 ---- batch: 100 ----
mean loss: 209.92
 ---- batch: 110 ----
mean loss: 193.95
train mean loss: 203.11
epoch train time: 0:00:00.737467
elapsed time: 0:02:55.181230
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 01:47:40.254701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.47
 ---- batch: 020 ----
mean loss: 210.71
 ---- batch: 030 ----
mean loss: 208.51
 ---- batch: 040 ----
mean loss: 206.63
 ---- batch: 050 ----
mean loss: 193.08
 ---- batch: 060 ----
mean loss: 200.09
 ---- batch: 070 ----
mean loss: 202.52
 ---- batch: 080 ----
mean loss: 199.70
 ---- batch: 090 ----
mean loss: 207.82
 ---- batch: 100 ----
mean loss: 200.68
 ---- batch: 110 ----
mean loss: 207.32
train mean loss: 203.02
epoch train time: 0:00:00.737984
elapsed time: 0:02:55.919387
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 01:47:40.992872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.42
 ---- batch: 020 ----
mean loss: 209.08
 ---- batch: 030 ----
mean loss: 196.31
 ---- batch: 040 ----
mean loss: 212.11
 ---- batch: 050 ----
mean loss: 199.48
 ---- batch: 060 ----
mean loss: 202.82
 ---- batch: 070 ----
mean loss: 204.94
 ---- batch: 080 ----
mean loss: 202.71
 ---- batch: 090 ----
mean loss: 199.50
 ---- batch: 100 ----
mean loss: 190.75
 ---- batch: 110 ----
mean loss: 207.82
train mean loss: 202.64
epoch train time: 0:00:00.733301
elapsed time: 0:02:56.652841
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 01:47:41.726309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.32
 ---- batch: 020 ----
mean loss: 201.34
 ---- batch: 030 ----
mean loss: 207.47
 ---- batch: 040 ----
mean loss: 195.53
 ---- batch: 050 ----
mean loss: 210.27
 ---- batch: 060 ----
mean loss: 197.53
 ---- batch: 070 ----
mean loss: 215.56
 ---- batch: 080 ----
mean loss: 206.60
 ---- batch: 090 ----
mean loss: 198.48
 ---- batch: 100 ----
mean loss: 200.08
 ---- batch: 110 ----
mean loss: 197.46
train mean loss: 202.68
epoch train time: 0:00:00.734642
elapsed time: 0:02:57.387619
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 01:47:42.461090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.60
 ---- batch: 020 ----
mean loss: 200.48
 ---- batch: 030 ----
mean loss: 202.81
 ---- batch: 040 ----
mean loss: 202.92
 ---- batch: 050 ----
mean loss: 199.79
 ---- batch: 060 ----
mean loss: 209.21
 ---- batch: 070 ----
mean loss: 195.50
 ---- batch: 080 ----
mean loss: 197.65
 ---- batch: 090 ----
mean loss: 200.43
 ---- batch: 100 ----
mean loss: 191.76
 ---- batch: 110 ----
mean loss: 209.36
train mean loss: 202.45
epoch train time: 0:00:00.735002
elapsed time: 0:02:58.122764
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 01:47:43.196238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.87
 ---- batch: 020 ----
mean loss: 195.91
 ---- batch: 030 ----
mean loss: 216.50
 ---- batch: 040 ----
mean loss: 193.37
 ---- batch: 050 ----
mean loss: 205.91
 ---- batch: 060 ----
mean loss: 213.06
 ---- batch: 070 ----
mean loss: 201.43
 ---- batch: 080 ----
mean loss: 202.18
 ---- batch: 090 ----
mean loss: 192.54
 ---- batch: 100 ----
mean loss: 199.29
 ---- batch: 110 ----
mean loss: 203.45
train mean loss: 202.19
epoch train time: 0:00:00.735309
elapsed time: 0:02:58.858215
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 01:47:43.931686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.68
 ---- batch: 020 ----
mean loss: 198.49
 ---- batch: 030 ----
mean loss: 203.05
 ---- batch: 040 ----
mean loss: 198.22
 ---- batch: 050 ----
mean loss: 205.07
 ---- batch: 060 ----
mean loss: 207.45
 ---- batch: 070 ----
mean loss: 201.89
 ---- batch: 080 ----
mean loss: 193.01
 ---- batch: 090 ----
mean loss: 201.21
 ---- batch: 100 ----
mean loss: 200.93
 ---- batch: 110 ----
mean loss: 212.86
train mean loss: 202.17
epoch train time: 0:00:00.730475
elapsed time: 0:02:59.588840
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 01:47:44.662344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.94
 ---- batch: 020 ----
mean loss: 198.92
 ---- batch: 030 ----
mean loss: 212.67
 ---- batch: 040 ----
mean loss: 202.23
 ---- batch: 050 ----
mean loss: 199.31
 ---- batch: 060 ----
mean loss: 202.25
 ---- batch: 070 ----
mean loss: 197.26
 ---- batch: 080 ----
mean loss: 202.00
 ---- batch: 090 ----
mean loss: 202.82
 ---- batch: 100 ----
mean loss: 195.98
 ---- batch: 110 ----
mean loss: 200.72
train mean loss: 201.59
epoch train time: 0:00:00.740659
elapsed time: 0:03:00.329693
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 01:47:45.403164
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.86
 ---- batch: 020 ----
mean loss: 196.94
 ---- batch: 030 ----
mean loss: 194.63
 ---- batch: 040 ----
mean loss: 206.09
 ---- batch: 050 ----
mean loss: 201.89
 ---- batch: 060 ----
mean loss: 199.75
 ---- batch: 070 ----
mean loss: 197.73
 ---- batch: 080 ----
mean loss: 212.47
 ---- batch: 090 ----
mean loss: 205.57
 ---- batch: 100 ----
mean loss: 200.84
 ---- batch: 110 ----
mean loss: 195.73
train mean loss: 201.56
epoch train time: 0:00:00.733599
elapsed time: 0:03:01.063445
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 01:47:46.136913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.18
 ---- batch: 020 ----
mean loss: 203.48
 ---- batch: 030 ----
mean loss: 203.67
 ---- batch: 040 ----
mean loss: 204.42
 ---- batch: 050 ----
mean loss: 207.00
 ---- batch: 060 ----
mean loss: 202.49
 ---- batch: 070 ----
mean loss: 191.54
 ---- batch: 080 ----
mean loss: 210.35
 ---- batch: 090 ----
mean loss: 203.06
 ---- batch: 100 ----
mean loss: 198.21
 ---- batch: 110 ----
mean loss: 196.39
train mean loss: 201.48
epoch train time: 0:00:00.730962
elapsed time: 0:03:01.794540
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 01:47:46.868025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.10
 ---- batch: 020 ----
mean loss: 204.22
 ---- batch: 030 ----
mean loss: 198.31
 ---- batch: 040 ----
mean loss: 198.81
 ---- batch: 050 ----
mean loss: 204.56
 ---- batch: 060 ----
mean loss: 201.41
 ---- batch: 070 ----
mean loss: 204.82
 ---- batch: 080 ----
mean loss: 205.42
 ---- batch: 090 ----
mean loss: 203.26
 ---- batch: 100 ----
mean loss: 196.38
 ---- batch: 110 ----
mean loss: 197.98
train mean loss: 201.45
epoch train time: 0:00:00.719177
elapsed time: 0:03:02.513884
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 01:47:47.587356
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.94
 ---- batch: 020 ----
mean loss: 196.40
 ---- batch: 030 ----
mean loss: 202.53
 ---- batch: 040 ----
mean loss: 197.97
 ---- batch: 050 ----
mean loss: 202.43
 ---- batch: 060 ----
mean loss: 207.25
 ---- batch: 070 ----
mean loss: 204.72
 ---- batch: 080 ----
mean loss: 207.76
 ---- batch: 090 ----
mean loss: 196.60
 ---- batch: 100 ----
mean loss: 194.50
 ---- batch: 110 ----
mean loss: 206.28
train mean loss: 201.49
epoch train time: 0:00:00.739673
elapsed time: 0:03:03.253699
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 01:47:48.327175
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.73
 ---- batch: 020 ----
mean loss: 198.67
 ---- batch: 030 ----
mean loss: 199.73
 ---- batch: 040 ----
mean loss: 204.30
 ---- batch: 050 ----
mean loss: 195.01
 ---- batch: 060 ----
mean loss: 202.95
 ---- batch: 070 ----
mean loss: 204.00
 ---- batch: 080 ----
mean loss: 206.01
 ---- batch: 090 ----
mean loss: 200.95
 ---- batch: 100 ----
mean loss: 192.50
 ---- batch: 110 ----
mean loss: 208.62
train mean loss: 201.53
epoch train time: 0:00:00.735174
elapsed time: 0:03:03.989018
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 01:47:49.062511
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.51
 ---- batch: 020 ----
mean loss: 203.59
 ---- batch: 030 ----
mean loss: 203.66
 ---- batch: 040 ----
mean loss: 212.36
 ---- batch: 050 ----
mean loss: 189.16
 ---- batch: 060 ----
mean loss: 202.24
 ---- batch: 070 ----
mean loss: 193.37
 ---- batch: 080 ----
mean loss: 204.30
 ---- batch: 090 ----
mean loss: 197.32
 ---- batch: 100 ----
mean loss: 212.80
 ---- batch: 110 ----
mean loss: 196.71
train mean loss: 201.52
epoch train time: 0:00:00.738513
elapsed time: 0:03:04.727709
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 01:47:49.801182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.89
 ---- batch: 020 ----
mean loss: 195.76
 ---- batch: 030 ----
mean loss: 194.91
 ---- batch: 040 ----
mean loss: 199.12
 ---- batch: 050 ----
mean loss: 203.07
 ---- batch: 060 ----
mean loss: 212.01
 ---- batch: 070 ----
mean loss: 207.76
 ---- batch: 080 ----
mean loss: 208.83
 ---- batch: 090 ----
mean loss: 198.13
 ---- batch: 100 ----
mean loss: 198.05
 ---- batch: 110 ----
mean loss: 205.12
train mean loss: 201.51
epoch train time: 0:00:00.730533
elapsed time: 0:03:05.458399
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 01:47:50.531912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.22
 ---- batch: 020 ----
mean loss: 185.42
 ---- batch: 030 ----
mean loss: 204.76
 ---- batch: 040 ----
mean loss: 196.88
 ---- batch: 050 ----
mean loss: 197.24
 ---- batch: 060 ----
mean loss: 210.08
 ---- batch: 070 ----
mean loss: 201.82
 ---- batch: 080 ----
mean loss: 198.85
 ---- batch: 090 ----
mean loss: 200.13
 ---- batch: 100 ----
mean loss: 202.88
 ---- batch: 110 ----
mean loss: 215.78
train mean loss: 201.40
epoch train time: 0:00:00.733089
elapsed time: 0:03:06.191670
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 01:47:51.265139
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.92
 ---- batch: 020 ----
mean loss: 199.89
 ---- batch: 030 ----
mean loss: 203.98
 ---- batch: 040 ----
mean loss: 207.70
 ---- batch: 050 ----
mean loss: 206.02
 ---- batch: 060 ----
mean loss: 206.96
 ---- batch: 070 ----
mean loss: 195.12
 ---- batch: 080 ----
mean loss: 192.62
 ---- batch: 090 ----
mean loss: 190.97
 ---- batch: 100 ----
mean loss: 192.29
 ---- batch: 110 ----
mean loss: 203.71
train mean loss: 201.39
epoch train time: 0:00:00.728063
elapsed time: 0:03:06.919895
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 01:47:51.993364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.56
 ---- batch: 020 ----
mean loss: 208.40
 ---- batch: 030 ----
mean loss: 201.42
 ---- batch: 040 ----
mean loss: 204.75
 ---- batch: 050 ----
mean loss: 208.32
 ---- batch: 060 ----
mean loss: 202.31
 ---- batch: 070 ----
mean loss: 192.72
 ---- batch: 080 ----
mean loss: 193.54
 ---- batch: 090 ----
mean loss: 208.07
 ---- batch: 100 ----
mean loss: 191.93
 ---- batch: 110 ----
mean loss: 201.19
train mean loss: 201.37
epoch train time: 0:00:00.728756
elapsed time: 0:03:07.648807
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 01:47:52.722279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.40
 ---- batch: 020 ----
mean loss: 198.77
 ---- batch: 030 ----
mean loss: 199.99
 ---- batch: 040 ----
mean loss: 203.55
 ---- batch: 050 ----
mean loss: 206.93
 ---- batch: 060 ----
mean loss: 205.88
 ---- batch: 070 ----
mean loss: 204.95
 ---- batch: 080 ----
mean loss: 196.99
 ---- batch: 090 ----
mean loss: 203.16
 ---- batch: 100 ----
mean loss: 199.19
 ---- batch: 110 ----
mean loss: 200.88
train mean loss: 201.31
epoch train time: 0:00:00.738131
elapsed time: 0:03:08.387106
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 01:47:53.460579
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.50
 ---- batch: 020 ----
mean loss: 199.68
 ---- batch: 030 ----
mean loss: 190.85
 ---- batch: 040 ----
mean loss: 208.23
 ---- batch: 050 ----
mean loss: 206.84
 ---- batch: 060 ----
mean loss: 204.36
 ---- batch: 070 ----
mean loss: 197.40
 ---- batch: 080 ----
mean loss: 198.83
 ---- batch: 090 ----
mean loss: 189.22
 ---- batch: 100 ----
mean loss: 207.19
 ---- batch: 110 ----
mean loss: 207.02
train mean loss: 201.29
epoch train time: 0:00:00.729152
elapsed time: 0:03:09.116400
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 01:47:54.189907
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.65
 ---- batch: 020 ----
mean loss: 201.36
 ---- batch: 030 ----
mean loss: 203.55
 ---- batch: 040 ----
mean loss: 189.91
 ---- batch: 050 ----
mean loss: 205.93
 ---- batch: 060 ----
mean loss: 198.75
 ---- batch: 070 ----
mean loss: 210.24
 ---- batch: 080 ----
mean loss: 205.04
 ---- batch: 090 ----
mean loss: 197.74
 ---- batch: 100 ----
mean loss: 199.51
 ---- batch: 110 ----
mean loss: 203.77
train mean loss: 201.37
epoch train time: 0:00:00.734182
elapsed time: 0:03:09.850757
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 01:47:54.924226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.95
 ---- batch: 020 ----
mean loss: 206.42
 ---- batch: 030 ----
mean loss: 202.07
 ---- batch: 040 ----
mean loss: 205.68
 ---- batch: 050 ----
mean loss: 196.22
 ---- batch: 060 ----
mean loss: 194.80
 ---- batch: 070 ----
mean loss: 209.71
 ---- batch: 080 ----
mean loss: 195.58
 ---- batch: 090 ----
mean loss: 187.31
 ---- batch: 100 ----
mean loss: 200.10
 ---- batch: 110 ----
mean loss: 203.82
train mean loss: 201.31
epoch train time: 0:00:00.722386
elapsed time: 0:03:10.573286
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 01:47:55.646773
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.88
 ---- batch: 020 ----
mean loss: 199.66
 ---- batch: 030 ----
mean loss: 204.98
 ---- batch: 040 ----
mean loss: 201.49
 ---- batch: 050 ----
mean loss: 200.07
 ---- batch: 060 ----
mean loss: 199.37
 ---- batch: 070 ----
mean loss: 194.51
 ---- batch: 080 ----
mean loss: 205.43
 ---- batch: 090 ----
mean loss: 201.45
 ---- batch: 100 ----
mean loss: 204.65
 ---- batch: 110 ----
mean loss: 200.28
train mean loss: 201.31
epoch train time: 0:00:00.728905
elapsed time: 0:03:11.302348
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 01:47:56.375820
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.81
 ---- batch: 020 ----
mean loss: 200.86
 ---- batch: 030 ----
mean loss: 199.74
 ---- batch: 040 ----
mean loss: 201.45
 ---- batch: 050 ----
mean loss: 197.87
 ---- batch: 060 ----
mean loss: 206.21
 ---- batch: 070 ----
mean loss: 198.12
 ---- batch: 080 ----
mean loss: 204.67
 ---- batch: 090 ----
mean loss: 201.71
 ---- batch: 100 ----
mean loss: 208.95
 ---- batch: 110 ----
mean loss: 200.77
train mean loss: 201.31
epoch train time: 0:00:00.728735
elapsed time: 0:03:12.031226
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 01:47:57.104699
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.66
 ---- batch: 020 ----
mean loss: 208.19
 ---- batch: 030 ----
mean loss: 207.46
 ---- batch: 040 ----
mean loss: 204.18
 ---- batch: 050 ----
mean loss: 199.56
 ---- batch: 060 ----
mean loss: 203.22
 ---- batch: 070 ----
mean loss: 202.05
 ---- batch: 080 ----
mean loss: 189.54
 ---- batch: 090 ----
mean loss: 198.37
 ---- batch: 100 ----
mean loss: 199.09
 ---- batch: 110 ----
mean loss: 201.12
train mean loss: 201.23
epoch train time: 0:00:00.736290
elapsed time: 0:03:12.767663
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 01:47:57.841150
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.43
 ---- batch: 020 ----
mean loss: 206.65
 ---- batch: 030 ----
mean loss: 196.96
 ---- batch: 040 ----
mean loss: 199.65
 ---- batch: 050 ----
mean loss: 201.84
 ---- batch: 060 ----
mean loss: 194.30
 ---- batch: 070 ----
mean loss: 199.27
 ---- batch: 080 ----
mean loss: 208.15
 ---- batch: 090 ----
mean loss: 202.61
 ---- batch: 100 ----
mean loss: 201.03
 ---- batch: 110 ----
mean loss: 202.60
train mean loss: 201.22
epoch train time: 0:00:00.723276
elapsed time: 0:03:13.491099
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 01:47:58.564575
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.60
 ---- batch: 020 ----
mean loss: 204.59
 ---- batch: 030 ----
mean loss: 203.68
 ---- batch: 040 ----
mean loss: 202.12
 ---- batch: 050 ----
mean loss: 210.70
 ---- batch: 060 ----
mean loss: 194.79
 ---- batch: 070 ----
mean loss: 200.10
 ---- batch: 080 ----
mean loss: 201.78
 ---- batch: 090 ----
mean loss: 200.16
 ---- batch: 100 ----
mean loss: 205.43
 ---- batch: 110 ----
mean loss: 198.25
train mean loss: 201.18
epoch train time: 0:00:00.726908
elapsed time: 0:03:14.218155
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 01:47:59.291646
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.73
 ---- batch: 020 ----
mean loss: 196.88
 ---- batch: 030 ----
mean loss: 204.36
 ---- batch: 040 ----
mean loss: 211.79
 ---- batch: 050 ----
mean loss: 195.63
 ---- batch: 060 ----
mean loss: 197.68
 ---- batch: 070 ----
mean loss: 208.61
 ---- batch: 080 ----
mean loss: 204.43
 ---- batch: 090 ----
mean loss: 197.45
 ---- batch: 100 ----
mean loss: 199.18
 ---- batch: 110 ----
mean loss: 204.17
train mean loss: 201.14
epoch train time: 0:00:00.745847
elapsed time: 0:03:14.964162
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 01:48:00.037658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.31
 ---- batch: 020 ----
mean loss: 208.21
 ---- batch: 030 ----
mean loss: 212.59
 ---- batch: 040 ----
mean loss: 196.33
 ---- batch: 050 ----
mean loss: 199.56
 ---- batch: 060 ----
mean loss: 196.63
 ---- batch: 070 ----
mean loss: 204.42
 ---- batch: 080 ----
mean loss: 195.73
 ---- batch: 090 ----
mean loss: 197.62
 ---- batch: 100 ----
mean loss: 202.71
 ---- batch: 110 ----
mean loss: 196.06
train mean loss: 201.20
epoch train time: 0:00:00.727369
elapsed time: 0:03:15.691691
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 01:48:00.765164
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.23
 ---- batch: 020 ----
mean loss: 200.82
 ---- batch: 030 ----
mean loss: 196.34
 ---- batch: 040 ----
mean loss: 205.62
 ---- batch: 050 ----
mean loss: 211.01
 ---- batch: 060 ----
mean loss: 197.83
 ---- batch: 070 ----
mean loss: 199.85
 ---- batch: 080 ----
mean loss: 196.12
 ---- batch: 090 ----
mean loss: 205.38
 ---- batch: 100 ----
mean loss: 201.01
 ---- batch: 110 ----
mean loss: 199.28
train mean loss: 201.18
epoch train time: 0:00:00.725015
elapsed time: 0:03:16.416846
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 01:48:01.490319
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.90
 ---- batch: 020 ----
mean loss: 201.01
 ---- batch: 030 ----
mean loss: 203.00
 ---- batch: 040 ----
mean loss: 204.26
 ---- batch: 050 ----
mean loss: 199.45
 ---- batch: 060 ----
mean loss: 211.94
 ---- batch: 070 ----
mean loss: 198.36
 ---- batch: 080 ----
mean loss: 204.43
 ---- batch: 090 ----
mean loss: 205.57
 ---- batch: 100 ----
mean loss: 188.35
 ---- batch: 110 ----
mean loss: 197.86
train mean loss: 201.16
epoch train time: 0:00:00.722861
elapsed time: 0:03:17.139845
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 01:48:02.213318
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.43
 ---- batch: 020 ----
mean loss: 196.92
 ---- batch: 030 ----
mean loss: 203.23
 ---- batch: 040 ----
mean loss: 207.98
 ---- batch: 050 ----
mean loss: 189.85
 ---- batch: 060 ----
mean loss: 203.14
 ---- batch: 070 ----
mean loss: 195.84
 ---- batch: 080 ----
mean loss: 197.38
 ---- batch: 090 ----
mean loss: 207.50
 ---- batch: 100 ----
mean loss: 205.86
 ---- batch: 110 ----
mean loss: 197.91
train mean loss: 201.16
epoch train time: 0:00:00.724150
elapsed time: 0:03:17.864138
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 01:48:02.937637
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.85
 ---- batch: 020 ----
mean loss: 208.16
 ---- batch: 030 ----
mean loss: 201.07
 ---- batch: 040 ----
mean loss: 199.99
 ---- batch: 050 ----
mean loss: 201.96
 ---- batch: 060 ----
mean loss: 187.22
 ---- batch: 070 ----
mean loss: 206.30
 ---- batch: 080 ----
mean loss: 195.94
 ---- batch: 090 ----
mean loss: 209.67
 ---- batch: 100 ----
mean loss: 206.60
 ---- batch: 110 ----
mean loss: 202.23
train mean loss: 201.04
epoch train time: 0:00:00.728129
elapsed time: 0:03:18.592436
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 01:48:03.665954
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.84
 ---- batch: 020 ----
mean loss: 203.58
 ---- batch: 030 ----
mean loss: 201.73
 ---- batch: 040 ----
mean loss: 197.56
 ---- batch: 050 ----
mean loss: 195.60
 ---- batch: 060 ----
mean loss: 204.68
 ---- batch: 070 ----
mean loss: 210.04
 ---- batch: 080 ----
mean loss: 205.24
 ---- batch: 090 ----
mean loss: 205.17
 ---- batch: 100 ----
mean loss: 201.66
 ---- batch: 110 ----
mean loss: 195.34
train mean loss: 201.05
epoch train time: 0:00:00.737720
elapsed time: 0:03:19.330343
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 01:48:04.403817
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.12
 ---- batch: 020 ----
mean loss: 196.45
 ---- batch: 030 ----
mean loss: 197.00
 ---- batch: 040 ----
mean loss: 203.53
 ---- batch: 050 ----
mean loss: 202.21
 ---- batch: 060 ----
mean loss: 202.89
 ---- batch: 070 ----
mean loss: 207.34
 ---- batch: 080 ----
mean loss: 203.93
 ---- batch: 090 ----
mean loss: 194.37
 ---- batch: 100 ----
mean loss: 199.13
 ---- batch: 110 ----
mean loss: 202.58
train mean loss: 201.09
epoch train time: 0:00:00.732419
elapsed time: 0:03:20.062907
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 01:48:05.136384
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.68
 ---- batch: 020 ----
mean loss: 205.56
 ---- batch: 030 ----
mean loss: 209.37
 ---- batch: 040 ----
mean loss: 200.93
 ---- batch: 050 ----
mean loss: 200.24
 ---- batch: 060 ----
mean loss: 202.66
 ---- batch: 070 ----
mean loss: 199.00
 ---- batch: 080 ----
mean loss: 189.88
 ---- batch: 090 ----
mean loss: 205.50
 ---- batch: 100 ----
mean loss: 206.26
 ---- batch: 110 ----
mean loss: 200.51
train mean loss: 201.02
epoch train time: 0:00:00.740677
elapsed time: 0:03:20.803751
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 01:48:05.877233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.53
 ---- batch: 020 ----
mean loss: 194.12
 ---- batch: 030 ----
mean loss: 193.63
 ---- batch: 040 ----
mean loss: 215.84
 ---- batch: 050 ----
mean loss: 194.35
 ---- batch: 060 ----
mean loss: 198.48
 ---- batch: 070 ----
mean loss: 203.66
 ---- batch: 080 ----
mean loss: 202.47
 ---- batch: 090 ----
mean loss: 202.09
 ---- batch: 100 ----
mean loss: 193.73
 ---- batch: 110 ----
mean loss: 205.00
train mean loss: 201.09
epoch train time: 0:00:00.725436
elapsed time: 0:03:21.529337
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 01:48:06.602815
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.41
 ---- batch: 020 ----
mean loss: 191.10
 ---- batch: 030 ----
mean loss: 191.36
 ---- batch: 040 ----
mean loss: 206.60
 ---- batch: 050 ----
mean loss: 204.54
 ---- batch: 060 ----
mean loss: 192.94
 ---- batch: 070 ----
mean loss: 204.13
 ---- batch: 080 ----
mean loss: 214.33
 ---- batch: 090 ----
mean loss: 198.20
 ---- batch: 100 ----
mean loss: 210.28
 ---- batch: 110 ----
mean loss: 201.41
train mean loss: 200.99
epoch train time: 0:00:00.726923
elapsed time: 0:03:22.256407
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 01:48:07.329881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.86
 ---- batch: 020 ----
mean loss: 197.69
 ---- batch: 030 ----
mean loss: 204.86
 ---- batch: 040 ----
mean loss: 205.60
 ---- batch: 050 ----
mean loss: 188.62
 ---- batch: 060 ----
mean loss: 208.24
 ---- batch: 070 ----
mean loss: 203.98
 ---- batch: 080 ----
mean loss: 209.51
 ---- batch: 090 ----
mean loss: 202.22
 ---- batch: 100 ----
mean loss: 195.44
 ---- batch: 110 ----
mean loss: 197.48
train mean loss: 200.92
epoch train time: 0:00:00.735798
elapsed time: 0:03:22.992353
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 01:48:08.065826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.57
 ---- batch: 020 ----
mean loss: 192.94
 ---- batch: 030 ----
mean loss: 197.79
 ---- batch: 040 ----
mean loss: 203.01
 ---- batch: 050 ----
mean loss: 201.56
 ---- batch: 060 ----
mean loss: 210.89
 ---- batch: 070 ----
mean loss: 204.76
 ---- batch: 080 ----
mean loss: 202.27
 ---- batch: 090 ----
mean loss: 197.98
 ---- batch: 100 ----
mean loss: 204.52
 ---- batch: 110 ----
mean loss: 197.01
train mean loss: 200.92
epoch train time: 0:00:00.740307
elapsed time: 0:03:23.732821
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 01:48:08.806305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.10
 ---- batch: 020 ----
mean loss: 197.52
 ---- batch: 030 ----
mean loss: 187.43
 ---- batch: 040 ----
mean loss: 202.78
 ---- batch: 050 ----
mean loss: 195.41
 ---- batch: 060 ----
mean loss: 205.37
 ---- batch: 070 ----
mean loss: 198.03
 ---- batch: 080 ----
mean loss: 211.35
 ---- batch: 090 ----
mean loss: 207.51
 ---- batch: 100 ----
mean loss: 204.45
 ---- batch: 110 ----
mean loss: 198.06
train mean loss: 200.91
epoch train time: 0:00:00.726019
elapsed time: 0:03:24.458990
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 01:48:09.532483
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.06
 ---- batch: 020 ----
mean loss: 195.68
 ---- batch: 030 ----
mean loss: 199.71
 ---- batch: 040 ----
mean loss: 207.03
 ---- batch: 050 ----
mean loss: 196.17
 ---- batch: 060 ----
mean loss: 209.11
 ---- batch: 070 ----
mean loss: 204.98
 ---- batch: 080 ----
mean loss: 195.54
 ---- batch: 090 ----
mean loss: 195.79
 ---- batch: 100 ----
mean loss: 205.34
 ---- batch: 110 ----
mean loss: 195.89
train mean loss: 200.91
epoch train time: 0:00:00.738242
elapsed time: 0:03:25.197395
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 01:48:10.270867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.05
 ---- batch: 020 ----
mean loss: 203.25
 ---- batch: 030 ----
mean loss: 196.34
 ---- batch: 040 ----
mean loss: 203.18
 ---- batch: 050 ----
mean loss: 201.68
 ---- batch: 060 ----
mean loss: 213.99
 ---- batch: 070 ----
mean loss: 203.59
 ---- batch: 080 ----
mean loss: 195.13
 ---- batch: 090 ----
mean loss: 199.23
 ---- batch: 100 ----
mean loss: 197.61
 ---- batch: 110 ----
mean loss: 196.48
train mean loss: 200.92
epoch train time: 0:00:00.737485
elapsed time: 0:03:25.935058
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 01:48:11.008532
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.42
 ---- batch: 020 ----
mean loss: 212.30
 ---- batch: 030 ----
mean loss: 200.71
 ---- batch: 040 ----
mean loss: 197.41
 ---- batch: 050 ----
mean loss: 202.47
 ---- batch: 060 ----
mean loss: 203.78
 ---- batch: 070 ----
mean loss: 189.92
 ---- batch: 080 ----
mean loss: 195.97
 ---- batch: 090 ----
mean loss: 207.64
 ---- batch: 100 ----
mean loss: 200.93
 ---- batch: 110 ----
mean loss: 193.94
train mean loss: 200.86
epoch train time: 0:00:00.729999
elapsed time: 0:03:26.665232
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 01:48:11.738704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.63
 ---- batch: 020 ----
mean loss: 205.00
 ---- batch: 030 ----
mean loss: 202.28
 ---- batch: 040 ----
mean loss: 203.03
 ---- batch: 050 ----
mean loss: 197.81
 ---- batch: 060 ----
mean loss: 198.76
 ---- batch: 070 ----
mean loss: 211.56
 ---- batch: 080 ----
mean loss: 195.15
 ---- batch: 090 ----
mean loss: 192.08
 ---- batch: 100 ----
mean loss: 207.33
 ---- batch: 110 ----
mean loss: 197.22
train mean loss: 200.84
epoch train time: 0:00:00.731295
elapsed time: 0:03:27.396678
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 01:48:12.470153
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.33
 ---- batch: 020 ----
mean loss: 203.23
 ---- batch: 030 ----
mean loss: 204.68
 ---- batch: 040 ----
mean loss: 206.84
 ---- batch: 050 ----
mean loss: 194.80
 ---- batch: 060 ----
mean loss: 210.11
 ---- batch: 070 ----
mean loss: 202.61
 ---- batch: 080 ----
mean loss: 206.49
 ---- batch: 090 ----
mean loss: 195.10
 ---- batch: 100 ----
mean loss: 198.93
 ---- batch: 110 ----
mean loss: 193.21
train mean loss: 200.85
epoch train time: 0:00:00.734039
elapsed time: 0:03:28.130858
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 01:48:13.204348
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.18
 ---- batch: 020 ----
mean loss: 198.13
 ---- batch: 030 ----
mean loss: 204.66
 ---- batch: 040 ----
mean loss: 191.49
 ---- batch: 050 ----
mean loss: 200.57
 ---- batch: 060 ----
mean loss: 202.71
 ---- batch: 070 ----
mean loss: 202.84
 ---- batch: 080 ----
mean loss: 200.71
 ---- batch: 090 ----
mean loss: 212.95
 ---- batch: 100 ----
mean loss: 200.06
 ---- batch: 110 ----
mean loss: 200.79
train mean loss: 200.86
epoch train time: 0:00:00.729195
elapsed time: 0:03:28.860260
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 01:48:13.933734
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.16
 ---- batch: 020 ----
mean loss: 199.23
 ---- batch: 030 ----
mean loss: 197.28
 ---- batch: 040 ----
mean loss: 198.90
 ---- batch: 050 ----
mean loss: 202.98
 ---- batch: 060 ----
mean loss: 208.49
 ---- batch: 070 ----
mean loss: 202.31
 ---- batch: 080 ----
mean loss: 201.17
 ---- batch: 090 ----
mean loss: 204.55
 ---- batch: 100 ----
mean loss: 209.10
 ---- batch: 110 ----
mean loss: 196.31
train mean loss: 200.77
epoch train time: 0:00:00.720597
elapsed time: 0:03:29.581004
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 01:48:14.654474
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.37
 ---- batch: 020 ----
mean loss: 217.57
 ---- batch: 030 ----
mean loss: 208.17
 ---- batch: 040 ----
mean loss: 191.46
 ---- batch: 050 ----
mean loss: 197.57
 ---- batch: 060 ----
mean loss: 200.97
 ---- batch: 070 ----
mean loss: 204.45
 ---- batch: 080 ----
mean loss: 185.14
 ---- batch: 090 ----
mean loss: 198.25
 ---- batch: 100 ----
mean loss: 206.14
 ---- batch: 110 ----
mean loss: 198.57
train mean loss: 200.75
epoch train time: 0:00:00.733468
elapsed time: 0:03:30.314609
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 01:48:15.388079
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.02
 ---- batch: 020 ----
mean loss: 208.03
 ---- batch: 030 ----
mean loss: 197.07
 ---- batch: 040 ----
mean loss: 207.83
 ---- batch: 050 ----
mean loss: 206.60
 ---- batch: 060 ----
mean loss: 196.98
 ---- batch: 070 ----
mean loss: 208.65
 ---- batch: 080 ----
mean loss: 191.44
 ---- batch: 090 ----
mean loss: 192.53
 ---- batch: 100 ----
mean loss: 207.32
 ---- batch: 110 ----
mean loss: 197.73
train mean loss: 200.76
epoch train time: 0:00:00.722528
elapsed time: 0:03:31.037273
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 01:48:16.110742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.74
 ---- batch: 020 ----
mean loss: 201.57
 ---- batch: 030 ----
mean loss: 198.98
 ---- batch: 040 ----
mean loss: 202.27
 ---- batch: 050 ----
mean loss: 200.56
 ---- batch: 060 ----
mean loss: 195.38
 ---- batch: 070 ----
mean loss: 204.26
 ---- batch: 080 ----
mean loss: 209.73
 ---- batch: 090 ----
mean loss: 199.43
 ---- batch: 100 ----
mean loss: 200.02
 ---- batch: 110 ----
mean loss: 204.27
train mean loss: 200.70
epoch train time: 0:00:00.739610
elapsed time: 0:03:31.777088
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 01:48:16.850559
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.14
 ---- batch: 020 ----
mean loss: 199.24
 ---- batch: 030 ----
mean loss: 203.22
 ---- batch: 040 ----
mean loss: 197.69
 ---- batch: 050 ----
mean loss: 198.47
 ---- batch: 060 ----
mean loss: 201.66
 ---- batch: 070 ----
mean loss: 206.30
 ---- batch: 080 ----
mean loss: 198.51
 ---- batch: 090 ----
mean loss: 200.25
 ---- batch: 100 ----
mean loss: 204.69
 ---- batch: 110 ----
mean loss: 204.08
train mean loss: 200.71
epoch train time: 0:00:00.719160
elapsed time: 0:03:32.496390
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 01:48:17.569859
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.19
 ---- batch: 020 ----
mean loss: 198.75
 ---- batch: 030 ----
mean loss: 199.29
 ---- batch: 040 ----
mean loss: 200.12
 ---- batch: 050 ----
mean loss: 202.65
 ---- batch: 060 ----
mean loss: 198.26
 ---- batch: 070 ----
mean loss: 199.06
 ---- batch: 080 ----
mean loss: 199.21
 ---- batch: 090 ----
mean loss: 199.02
 ---- batch: 100 ----
mean loss: 201.71
 ---- batch: 110 ----
mean loss: 203.30
train mean loss: 200.74
epoch train time: 0:00:00.725102
elapsed time: 0:03:33.221675
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 01:48:18.295197
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.07
 ---- batch: 020 ----
mean loss: 208.43
 ---- batch: 030 ----
mean loss: 202.31
 ---- batch: 040 ----
mean loss: 201.18
 ---- batch: 050 ----
mean loss: 198.39
 ---- batch: 060 ----
mean loss: 202.76
 ---- batch: 070 ----
mean loss: 201.99
 ---- batch: 080 ----
mean loss: 194.34
 ---- batch: 090 ----
mean loss: 201.76
 ---- batch: 100 ----
mean loss: 206.82
 ---- batch: 110 ----
mean loss: 206.35
train mean loss: 200.64
epoch train time: 0:00:00.727758
elapsed time: 0:03:33.949626
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 01:48:19.023096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.41
 ---- batch: 020 ----
mean loss: 198.92
 ---- batch: 030 ----
mean loss: 197.91
 ---- batch: 040 ----
mean loss: 200.71
 ---- batch: 050 ----
mean loss: 196.71
 ---- batch: 060 ----
mean loss: 196.19
 ---- batch: 070 ----
mean loss: 197.61
 ---- batch: 080 ----
mean loss: 200.89
 ---- batch: 090 ----
mean loss: 201.97
 ---- batch: 100 ----
mean loss: 208.81
 ---- batch: 110 ----
mean loss: 204.93
train mean loss: 200.57
epoch train time: 0:00:00.728184
elapsed time: 0:03:34.677949
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 01:48:19.751444
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.96
 ---- batch: 020 ----
mean loss: 201.74
 ---- batch: 030 ----
mean loss: 199.08
 ---- batch: 040 ----
mean loss: 199.41
 ---- batch: 050 ----
mean loss: 197.75
 ---- batch: 060 ----
mean loss: 207.18
 ---- batch: 070 ----
mean loss: 199.78
 ---- batch: 080 ----
mean loss: 200.58
 ---- batch: 090 ----
mean loss: 201.85
 ---- batch: 100 ----
mean loss: 203.61
 ---- batch: 110 ----
mean loss: 200.89
train mean loss: 200.60
epoch train time: 0:00:00.724209
elapsed time: 0:03:35.404450
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_5/checkpoint.pth.tar
**** end time: 2019-09-27 01:48:20.477888 ****
