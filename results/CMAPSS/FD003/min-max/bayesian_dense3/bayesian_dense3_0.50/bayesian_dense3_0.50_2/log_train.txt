Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 22584
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-10-01 14:24:58.578045 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-10-01 14:24:58.588315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4276.75
 ---- batch: 020 ----
mean loss: 3974.26
train mean loss: 4108.03
epoch train time: 0:00:07.937486
elapsed time: 0:00:07.954355
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-10-01 14:25:06.532449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3728.68
 ---- batch: 020 ----
mean loss: 3495.79
train mean loss: 3591.92
epoch train time: 0:00:00.384798
elapsed time: 0:00:08.339318
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-10-01 14:25:06.917429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3261.87
 ---- batch: 020 ----
mean loss: 3198.09
train mean loss: 3222.97
epoch train time: 0:00:00.366272
elapsed time: 0:00:08.705797
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-10-01 14:25:07.283919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3013.78
 ---- batch: 020 ----
mean loss: 2889.82
train mean loss: 2933.81
epoch train time: 0:00:00.364627
elapsed time: 0:00:09.070617
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-10-01 14:25:07.648725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2762.47
 ---- batch: 020 ----
mean loss: 2651.98
train mean loss: 2705.18
epoch train time: 0:00:00.359516
elapsed time: 0:00:09.430315
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-10-01 14:25:08.008425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2579.74
 ---- batch: 020 ----
mean loss: 2470.29
train mean loss: 2512.11
epoch train time: 0:00:00.353274
elapsed time: 0:00:09.783788
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-10-01 14:25:08.361896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2386.24
 ---- batch: 020 ----
mean loss: 2335.15
train mean loss: 2357.54
epoch train time: 0:00:00.345654
elapsed time: 0:00:10.129645
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-10-01 14:25:08.707749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2284.89
 ---- batch: 020 ----
mean loss: 2217.73
train mean loss: 2246.59
epoch train time: 0:00:00.360980
elapsed time: 0:00:10.490817
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-10-01 14:25:09.068927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2178.04
 ---- batch: 020 ----
mean loss: 2121.39
train mean loss: 2140.61
epoch train time: 0:00:00.350980
elapsed time: 0:00:10.841974
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-10-01 14:25:09.420081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2089.27
 ---- batch: 020 ----
mean loss: 2036.28
train mean loss: 2049.04
epoch train time: 0:00:00.347336
elapsed time: 0:00:11.189488
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-10-01 14:25:09.767594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1961.39
 ---- batch: 020 ----
mean loss: 1985.70
train mean loss: 1974.94
epoch train time: 0:00:00.364691
elapsed time: 0:00:11.554358
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-10-01 14:25:10.132486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1919.06
 ---- batch: 020 ----
mean loss: 1862.91
train mean loss: 1895.89
epoch train time: 0:00:00.356287
elapsed time: 0:00:11.910861
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-10-01 14:25:10.488970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1846.63
 ---- batch: 020 ----
mean loss: 1845.89
train mean loss: 1841.50
epoch train time: 0:00:00.365427
elapsed time: 0:00:12.276538
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-10-01 14:25:10.854660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1775.17
 ---- batch: 020 ----
mean loss: 1745.56
train mean loss: 1757.51
epoch train time: 0:00:00.382482
elapsed time: 0:00:12.659239
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-10-01 14:25:11.237348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1740.02
 ---- batch: 020 ----
mean loss: 1668.79
train mean loss: 1709.34
epoch train time: 0:00:00.356633
elapsed time: 0:00:13.016067
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-10-01 14:25:11.594173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1677.57
 ---- batch: 020 ----
mean loss: 1619.67
train mean loss: 1641.86
epoch train time: 0:00:00.359877
elapsed time: 0:00:13.376128
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-10-01 14:25:11.954233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1603.35
 ---- batch: 020 ----
mean loss: 1575.24
train mean loss: 1581.81
epoch train time: 0:00:00.356916
elapsed time: 0:00:13.733237
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-10-01 14:25:12.311339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1539.16
 ---- batch: 020 ----
mean loss: 1493.06
train mean loss: 1516.61
epoch train time: 0:00:00.353359
elapsed time: 0:00:14.086765
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-10-01 14:25:12.664878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1453.92
 ---- batch: 020 ----
mean loss: 1451.29
train mean loss: 1452.24
epoch train time: 0:00:00.375342
elapsed time: 0:00:14.462308
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-10-01 14:25:13.040427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1395.67
 ---- batch: 020 ----
mean loss: 1395.87
train mean loss: 1388.08
epoch train time: 0:00:00.359875
elapsed time: 0:00:14.822379
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-10-01 14:25:13.400486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1365.78
 ---- batch: 020 ----
mean loss: 1326.57
train mean loss: 1340.01
epoch train time: 0:00:00.357440
elapsed time: 0:00:15.180004
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-10-01 14:25:13.758107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1310.73
 ---- batch: 020 ----
mean loss: 1271.10
train mean loss: 1283.51
epoch train time: 0:00:00.356056
elapsed time: 0:00:15.536255
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-10-01 14:25:14.114360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1248.41
 ---- batch: 020 ----
mean loss: 1233.56
train mean loss: 1231.71
epoch train time: 0:00:00.359226
elapsed time: 0:00:15.895662
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-10-01 14:25:14.473825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1202.92
 ---- batch: 020 ----
mean loss: 1194.80
train mean loss: 1195.65
epoch train time: 0:00:00.363923
elapsed time: 0:00:16.259865
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-10-01 14:25:14.838058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1160.98
 ---- batch: 020 ----
mean loss: 1150.96
train mean loss: 1161.30
epoch train time: 0:00:00.355605
elapsed time: 0:00:16.615787
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-10-01 14:25:15.193898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1119.10
 ---- batch: 020 ----
mean loss: 1099.90
train mean loss: 1102.43
epoch train time: 0:00:00.356974
elapsed time: 0:00:16.972958
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-10-01 14:25:15.551078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1080.07
 ---- batch: 020 ----
mean loss: 1064.39
train mean loss: 1066.27
epoch train time: 0:00:00.349136
elapsed time: 0:00:17.322336
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-10-01 14:25:15.900439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1050.06
 ---- batch: 020 ----
mean loss: 1014.65
train mean loss: 1032.96
epoch train time: 0:00:00.346787
elapsed time: 0:00:17.669308
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-10-01 14:25:16.247411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1020.21
 ---- batch: 020 ----
mean loss: 991.46
train mean loss: 1000.84
epoch train time: 0:00:00.350510
elapsed time: 0:00:18.019995
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-10-01 14:25:16.598101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 956.56
 ---- batch: 020 ----
mean loss: 944.60
train mean loss: 954.14
epoch train time: 0:00:00.354065
elapsed time: 0:00:18.374245
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-10-01 14:25:16.952351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.21
 ---- batch: 020 ----
mean loss: 910.18
train mean loss: 918.19
epoch train time: 0:00:00.354478
elapsed time: 0:00:18.728916
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-10-01 14:25:17.307037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.36
 ---- batch: 020 ----
mean loss: 885.18
train mean loss: 885.29
epoch train time: 0:00:00.363149
elapsed time: 0:00:19.092268
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-10-01 14:25:17.670376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.38
 ---- batch: 020 ----
mean loss: 853.59
train mean loss: 858.19
epoch train time: 0:00:00.369052
elapsed time: 0:00:19.461508
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-10-01 14:25:18.039615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.18
 ---- batch: 020 ----
mean loss: 827.64
train mean loss: 821.01
epoch train time: 0:00:00.359576
elapsed time: 0:00:19.821263
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-10-01 14:25:18.399369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 794.51
 ---- batch: 020 ----
mean loss: 790.08
train mean loss: 794.72
epoch train time: 0:00:00.379521
elapsed time: 0:00:20.200968
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-10-01 14:25:18.779076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 770.75
 ---- batch: 020 ----
mean loss: 757.64
train mean loss: 764.64
epoch train time: 0:00:00.355936
elapsed time: 0:00:20.557087
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-10-01 14:25:19.135216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 750.69
 ---- batch: 020 ----
mean loss: 722.31
train mean loss: 734.60
epoch train time: 0:00:00.347714
elapsed time: 0:00:20.905009
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-10-01 14:25:19.483130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 717.91
 ---- batch: 020 ----
mean loss: 691.42
train mean loss: 701.51
epoch train time: 0:00:00.355381
elapsed time: 0:00:21.260582
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-10-01 14:25:19.838686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 677.36
 ---- batch: 020 ----
mean loss: 674.14
train mean loss: 677.26
epoch train time: 0:00:00.352792
elapsed time: 0:00:21.613607
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-10-01 14:25:20.191710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 649.34
 ---- batch: 020 ----
mean loss: 656.73
train mean loss: 656.94
epoch train time: 0:00:00.355188
elapsed time: 0:00:21.968972
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-10-01 14:25:20.547077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 646.30
 ---- batch: 020 ----
mean loss: 619.51
train mean loss: 631.56
epoch train time: 0:00:00.361349
elapsed time: 0:00:22.330499
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-10-01 14:25:20.908608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 606.53
 ---- batch: 020 ----
mean loss: 607.61
train mean loss: 606.31
epoch train time: 0:00:00.355740
elapsed time: 0:00:22.686443
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-10-01 14:25:21.264571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 600.78
 ---- batch: 020 ----
mean loss: 576.04
train mean loss: 584.83
epoch train time: 0:00:00.355268
elapsed time: 0:00:23.041910
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-10-01 14:25:21.620017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 574.05
 ---- batch: 020 ----
mean loss: 562.71
train mean loss: 565.71
epoch train time: 0:00:00.362443
elapsed time: 0:00:23.404538
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-10-01 14:25:21.982643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.03
 ---- batch: 020 ----
mean loss: 538.50
train mean loss: 545.42
epoch train time: 0:00:00.359835
elapsed time: 0:00:23.764549
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-10-01 14:25:22.342655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.58
 ---- batch: 020 ----
mean loss: 512.33
train mean loss: 522.71
epoch train time: 0:00:00.435451
elapsed time: 0:00:24.200216
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-10-01 14:25:22.778345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.82
 ---- batch: 020 ----
mean loss: 496.15
train mean loss: 503.70
epoch train time: 0:00:00.355618
elapsed time: 0:00:24.556036
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-10-01 14:25:23.134138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.08
 ---- batch: 020 ----
mean loss: 480.71
train mean loss: 485.81
epoch train time: 0:00:00.346497
elapsed time: 0:00:24.902735
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-10-01 14:25:23.480852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.30
 ---- batch: 020 ----
mean loss: 476.60
train mean loss: 470.17
epoch train time: 0:00:00.364246
elapsed time: 0:00:25.267166
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-10-01 14:25:23.845272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.88
 ---- batch: 020 ----
mean loss: 444.95
train mean loss: 450.15
epoch train time: 0:00:00.352040
elapsed time: 0:00:25.619382
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-10-01 14:25:24.197488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.20
 ---- batch: 020 ----
mean loss: 426.22
train mean loss: 433.84
epoch train time: 0:00:00.353889
elapsed time: 0:00:25.973477
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-10-01 14:25:24.551581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.81
 ---- batch: 020 ----
mean loss: 406.82
train mean loss: 419.16
epoch train time: 0:00:00.359279
elapsed time: 0:00:26.332930
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-10-01 14:25:24.911053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.81
 ---- batch: 020 ----
mean loss: 403.11
train mean loss: 404.47
epoch train time: 0:00:00.355552
elapsed time: 0:00:26.688674
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-10-01 14:25:25.266777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.88
 ---- batch: 020 ----
mean loss: 390.59
train mean loss: 394.55
epoch train time: 0:00:00.358796
elapsed time: 0:00:27.047654
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-10-01 14:25:25.625779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.18
 ---- batch: 020 ----
mean loss: 376.11
train mean loss: 375.95
epoch train time: 0:00:00.365547
elapsed time: 0:00:27.413398
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-10-01 14:25:25.991505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.49
 ---- batch: 020 ----
mean loss: 356.36
train mean loss: 364.33
epoch train time: 0:00:00.352108
elapsed time: 0:00:27.765687
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-10-01 14:25:26.343841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.02
 ---- batch: 020 ----
mean loss: 349.50
train mean loss: 355.84
epoch train time: 0:00:00.349597
elapsed time: 0:00:28.115508
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-10-01 14:25:26.693628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.74
 ---- batch: 020 ----
mean loss: 338.17
train mean loss: 342.42
epoch train time: 0:00:00.362753
elapsed time: 0:00:28.478451
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-10-01 14:25:27.056556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.60
 ---- batch: 020 ----
mean loss: 327.45
train mean loss: 329.80
epoch train time: 0:00:00.360891
elapsed time: 0:00:28.839539
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-10-01 14:25:27.417641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.65
 ---- batch: 020 ----
mean loss: 314.36
train mean loss: 316.08
epoch train time: 0:00:00.364951
elapsed time: 0:00:29.204700
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-10-01 14:25:27.782864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.86
 ---- batch: 020 ----
mean loss: 304.31
train mean loss: 306.48
epoch train time: 0:00:00.365761
elapsed time: 0:00:29.570711
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-10-01 14:25:28.148844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.28
 ---- batch: 020 ----
mean loss: 299.76
train mean loss: 298.68
epoch train time: 0:00:00.357328
elapsed time: 0:00:29.928243
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-10-01 14:25:28.506358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.11
 ---- batch: 020 ----
mean loss: 280.70
train mean loss: 286.20
epoch train time: 0:00:00.371644
elapsed time: 0:00:30.300076
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-10-01 14:25:28.878179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.66
 ---- batch: 020 ----
mean loss: 276.53
train mean loss: 281.48
epoch train time: 0:00:00.360102
elapsed time: 0:00:30.660370
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-10-01 14:25:29.238477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.31
 ---- batch: 020 ----
mean loss: 272.03
train mean loss: 269.70
epoch train time: 0:00:00.361041
elapsed time: 0:00:31.021593
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-10-01 14:25:29.599702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.78
 ---- batch: 020 ----
mean loss: 262.60
train mean loss: 264.68
epoch train time: 0:00:00.358758
elapsed time: 0:00:31.380547
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-10-01 14:25:29.958658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.80
 ---- batch: 020 ----
mean loss: 253.31
train mean loss: 254.61
epoch train time: 0:00:00.354929
elapsed time: 0:00:31.735732
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-10-01 14:25:30.313841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.76
 ---- batch: 020 ----
mean loss: 246.03
train mean loss: 250.11
epoch train time: 0:00:00.350422
elapsed time: 0:00:32.086355
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-10-01 14:25:30.664457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.31
 ---- batch: 020 ----
mean loss: 243.33
train mean loss: 242.24
epoch train time: 0:00:00.361397
elapsed time: 0:00:32.447940
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-10-01 14:25:31.026096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.63
 ---- batch: 020 ----
mean loss: 230.59
train mean loss: 233.49
epoch train time: 0:00:00.365062
elapsed time: 0:00:32.813233
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-10-01 14:25:31.391341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.57
 ---- batch: 020 ----
mean loss: 227.15
train mean loss: 228.41
epoch train time: 0:00:00.360624
elapsed time: 0:00:33.174038
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-10-01 14:25:31.752178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.18
 ---- batch: 020 ----
mean loss: 219.65
train mean loss: 221.80
epoch train time: 0:00:00.362792
elapsed time: 0:00:33.537055
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-10-01 14:25:32.115176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.84
 ---- batch: 020 ----
mean loss: 215.08
train mean loss: 214.25
epoch train time: 0:00:00.364817
elapsed time: 0:00:33.902098
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-10-01 14:25:32.480233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.55
 ---- batch: 020 ----
mean loss: 208.73
train mean loss: 209.12
epoch train time: 0:00:00.377887
elapsed time: 0:00:34.280201
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-10-01 14:25:32.858321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.95
 ---- batch: 020 ----
mean loss: 198.92
train mean loss: 200.82
epoch train time: 0:00:00.357863
elapsed time: 0:00:34.638264
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-10-01 14:25:33.216369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.93
 ---- batch: 020 ----
mean loss: 197.98
train mean loss: 198.77
epoch train time: 0:00:00.353099
elapsed time: 0:00:34.991610
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-10-01 14:25:33.569769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.67
 ---- batch: 020 ----
mean loss: 192.58
train mean loss: 191.60
epoch train time: 0:00:00.363878
elapsed time: 0:00:35.355740
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-10-01 14:25:33.933858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.86
 ---- batch: 020 ----
mean loss: 187.80
train mean loss: 185.13
epoch train time: 0:00:00.360653
elapsed time: 0:00:35.716587
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-10-01 14:25:34.294695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.03
 ---- batch: 020 ----
mean loss: 184.56
train mean loss: 183.69
epoch train time: 0:00:00.355638
elapsed time: 0:00:36.072408
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-10-01 14:25:34.650518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.38
 ---- batch: 020 ----
mean loss: 178.21
train mean loss: 178.78
epoch train time: 0:00:00.377181
elapsed time: 0:00:36.449776
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-10-01 14:25:35.027883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.79
 ---- batch: 020 ----
mean loss: 173.93
train mean loss: 173.25
epoch train time: 0:00:00.356430
elapsed time: 0:00:36.806400
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-10-01 14:25:35.384541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.35
 ---- batch: 020 ----
mean loss: 169.80
train mean loss: 171.36
epoch train time: 0:00:00.356962
elapsed time: 0:00:37.163589
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-10-01 14:25:35.741701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.28
 ---- batch: 020 ----
mean loss: 167.52
train mean loss: 166.63
epoch train time: 0:00:00.355602
elapsed time: 0:00:37.519374
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-10-01 14:25:36.097481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.94
 ---- batch: 020 ----
mean loss: 158.67
train mean loss: 160.87
epoch train time: 0:00:00.358857
elapsed time: 0:00:37.878459
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-10-01 14:25:36.456617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.39
 ---- batch: 020 ----
mean loss: 157.31
train mean loss: 158.83
epoch train time: 0:00:00.374000
elapsed time: 0:00:38.252690
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-10-01 14:25:36.830794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.95
 ---- batch: 020 ----
mean loss: 155.42
train mean loss: 153.52
epoch train time: 0:00:00.362817
elapsed time: 0:00:38.615731
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-10-01 14:25:37.193866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.84
 ---- batch: 020 ----
mean loss: 152.50
train mean loss: 152.09
epoch train time: 0:00:00.367631
elapsed time: 0:00:38.983605
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-10-01 14:25:37.561728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.95
 ---- batch: 020 ----
mean loss: 148.20
train mean loss: 148.51
epoch train time: 0:00:00.363795
elapsed time: 0:00:39.347595
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-10-01 14:25:37.925703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.75
 ---- batch: 020 ----
mean loss: 145.53
train mean loss: 146.03
epoch train time: 0:00:00.355922
elapsed time: 0:00:39.703698
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-10-01 14:25:38.281838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.09
 ---- batch: 020 ----
mean loss: 142.79
train mean loss: 145.50
epoch train time: 0:00:00.354534
elapsed time: 0:00:40.058452
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-10-01 14:25:38.636559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.82
 ---- batch: 020 ----
mean loss: 140.22
train mean loss: 140.11
epoch train time: 0:00:00.375304
elapsed time: 0:00:40.433951
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-10-01 14:25:39.012063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.01
 ---- batch: 020 ----
mean loss: 134.25
train mean loss: 136.89
epoch train time: 0:00:00.373182
elapsed time: 0:00:40.807321
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-10-01 14:25:39.385429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.44
 ---- batch: 020 ----
mean loss: 132.28
train mean loss: 133.68
epoch train time: 0:00:00.374688
elapsed time: 0:00:41.182222
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-10-01 14:25:39.760328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.64
 ---- batch: 020 ----
mean loss: 129.35
train mean loss: 132.78
epoch train time: 0:00:00.361730
elapsed time: 0:00:41.544133
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-10-01 14:25:40.122258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.82
 ---- batch: 020 ----
mean loss: 126.98
train mean loss: 131.00
epoch train time: 0:00:00.362917
elapsed time: 0:00:41.907261
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-10-01 14:25:40.485373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.82
 ---- batch: 020 ----
mean loss: 129.16
train mean loss: 130.10
epoch train time: 0:00:00.364885
elapsed time: 0:00:42.272378
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-10-01 14:25:40.850490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.92
 ---- batch: 020 ----
mean loss: 123.91
train mean loss: 124.89
epoch train time: 0:00:00.365287
elapsed time: 0:00:42.637853
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-10-01 14:25:41.215962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.99
 ---- batch: 020 ----
mean loss: 126.16
train mean loss: 122.87
epoch train time: 0:00:00.358965
elapsed time: 0:00:42.997014
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-10-01 14:25:41.575125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.03
 ---- batch: 020 ----
mean loss: 120.14
train mean loss: 120.85
epoch train time: 0:00:00.362807
elapsed time: 0:00:43.360026
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-10-01 14:25:41.938135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.29
 ---- batch: 020 ----
mean loss: 121.01
train mean loss: 121.54
epoch train time: 0:00:00.353980
elapsed time: 0:00:43.714185
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-10-01 14:25:42.292293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.09
 ---- batch: 020 ----
mean loss: 119.52
train mean loss: 118.74
epoch train time: 0:00:00.360195
elapsed time: 0:00:44.074559
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-10-01 14:25:42.652677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.59
 ---- batch: 020 ----
mean loss: 114.96
train mean loss: 117.54
epoch train time: 0:00:00.356430
elapsed time: 0:00:44.431183
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-10-01 14:25:43.009295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.95
 ---- batch: 020 ----
mean loss: 117.80
train mean loss: 114.36
epoch train time: 0:00:00.357163
elapsed time: 0:00:44.788529
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-10-01 14:25:43.366633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.64
 ---- batch: 020 ----
mean loss: 113.30
train mean loss: 113.38
epoch train time: 0:00:00.360358
elapsed time: 0:00:45.149058
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-10-01 14:25:43.727183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.06
 ---- batch: 020 ----
mean loss: 115.67
train mean loss: 111.68
epoch train time: 0:00:00.357348
elapsed time: 0:00:45.506604
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-10-01 14:25:44.084713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.12
 ---- batch: 020 ----
mean loss: 111.03
train mean loss: 109.36
epoch train time: 0:00:00.362482
elapsed time: 0:00:45.869307
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-10-01 14:25:44.447428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.41
 ---- batch: 020 ----
mean loss: 107.41
train mean loss: 107.43
epoch train time: 0:00:00.370207
elapsed time: 0:00:46.239773
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-10-01 14:25:44.817863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.70
 ---- batch: 020 ----
mean loss: 110.42
train mean loss: 108.61
epoch train time: 0:00:00.361865
elapsed time: 0:00:46.601800
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-10-01 14:25:45.179904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.16
 ---- batch: 020 ----
mean loss: 104.27
train mean loss: 107.00
epoch train time: 0:00:00.358612
elapsed time: 0:00:46.960606
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-10-01 14:25:45.538708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.97
 ---- batch: 020 ----
mean loss: 107.80
train mean loss: 106.52
epoch train time: 0:00:00.358204
elapsed time: 0:00:47.318988
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-10-01 14:25:45.897108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.00
 ---- batch: 020 ----
mean loss: 106.46
train mean loss: 105.21
epoch train time: 0:00:00.363191
elapsed time: 0:00:47.682436
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-10-01 14:25:46.260541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.80
 ---- batch: 020 ----
mean loss: 99.82
train mean loss: 103.58
epoch train time: 0:00:00.363099
elapsed time: 0:00:48.045754
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-10-01 14:25:46.623855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.82
 ---- batch: 020 ----
mean loss: 97.63
train mean loss: 98.72
epoch train time: 0:00:00.375946
elapsed time: 0:00:48.421879
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-10-01 14:25:46.999983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.63
 ---- batch: 020 ----
mean loss: 104.33
train mean loss: 101.44
epoch train time: 0:00:00.360375
elapsed time: 0:00:48.782431
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-10-01 14:25:47.360538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.77
 ---- batch: 020 ----
mean loss: 99.87
train mean loss: 100.61
epoch train time: 0:00:00.358729
elapsed time: 0:00:49.141338
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-10-01 14:25:47.719445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.22
 ---- batch: 020 ----
mean loss: 100.94
train mean loss: 96.87
epoch train time: 0:00:00.364659
elapsed time: 0:00:49.506234
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-10-01 14:25:48.084367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.10
 ---- batch: 020 ----
mean loss: 92.30
train mean loss: 95.28
epoch train time: 0:00:00.366214
elapsed time: 0:00:49.872671
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-10-01 14:25:48.450793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.39
 ---- batch: 020 ----
mean loss: 93.94
train mean loss: 93.88
epoch train time: 0:00:00.362128
elapsed time: 0:00:50.235030
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-10-01 14:25:48.813137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.71
 ---- batch: 020 ----
mean loss: 95.01
train mean loss: 95.72
epoch train time: 0:00:00.364249
elapsed time: 0:00:50.599456
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-10-01 14:25:49.177563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.67
 ---- batch: 020 ----
mean loss: 95.53
train mean loss: 94.52
epoch train time: 0:00:00.359140
elapsed time: 0:00:50.958790
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-10-01 14:25:49.536905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.70
 ---- batch: 020 ----
mean loss: 93.47
train mean loss: 92.10
epoch train time: 0:00:00.367233
elapsed time: 0:00:51.326227
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-10-01 14:25:49.904338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.82
 ---- batch: 020 ----
mean loss: 94.50
train mean loss: 93.11
epoch train time: 0:00:00.354776
elapsed time: 0:00:51.681229
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-10-01 14:25:50.259332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.10
 ---- batch: 020 ----
mean loss: 92.07
train mean loss: 90.33
epoch train time: 0:00:00.351963
elapsed time: 0:00:52.033359
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-10-01 14:25:50.611457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.53
 ---- batch: 020 ----
mean loss: 88.84
train mean loss: 90.53
epoch train time: 0:00:00.363521
elapsed time: 0:00:52.397086
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-10-01 14:25:50.975192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.36
 ---- batch: 020 ----
mean loss: 92.00
train mean loss: 90.69
epoch train time: 0:00:00.365224
elapsed time: 0:00:52.762503
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-10-01 14:25:51.340609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.17
 ---- batch: 020 ----
mean loss: 88.75
train mean loss: 89.30
epoch train time: 0:00:00.365000
elapsed time: 0:00:53.127853
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-10-01 14:25:51.705982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.88
 ---- batch: 020 ----
mean loss: 87.80
train mean loss: 88.35
epoch train time: 0:00:00.380084
elapsed time: 0:00:53.508168
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-10-01 14:25:52.086261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.25
 ---- batch: 020 ----
mean loss: 84.63
train mean loss: 86.64
epoch train time: 0:00:00.366295
elapsed time: 0:00:53.874695
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-10-01 14:25:52.452832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.07
 ---- batch: 020 ----
mean loss: 85.65
train mean loss: 86.01
epoch train time: 0:00:00.362484
elapsed time: 0:00:54.237423
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-10-01 14:25:52.815543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.02
 ---- batch: 020 ----
mean loss: 85.37
train mean loss: 83.92
epoch train time: 0:00:00.376520
elapsed time: 0:00:54.614136
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-10-01 14:25:53.192268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.08
 ---- batch: 020 ----
mean loss: 88.50
train mean loss: 86.64
epoch train time: 0:00:00.359547
elapsed time: 0:00:54.973899
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-10-01 14:25:53.552005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.37
 ---- batch: 020 ----
mean loss: 82.72
train mean loss: 84.20
epoch train time: 0:00:00.365717
elapsed time: 0:00:55.339830
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-10-01 14:25:53.917986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.17
 ---- batch: 020 ----
mean loss: 85.32
train mean loss: 83.83
epoch train time: 0:00:00.351549
elapsed time: 0:00:55.691636
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-10-01 14:25:54.269741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.10
 ---- batch: 020 ----
mean loss: 86.13
train mean loss: 84.38
epoch train time: 0:00:00.350179
elapsed time: 0:00:56.041998
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-10-01 14:25:54.620118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.42
 ---- batch: 020 ----
mean loss: 81.79
train mean loss: 81.52
epoch train time: 0:00:00.366005
elapsed time: 0:00:56.408208
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-10-01 14:25:54.986315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.16
 ---- batch: 020 ----
mean loss: 86.51
train mean loss: 81.57
epoch train time: 0:00:00.363961
elapsed time: 0:00:56.772363
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-10-01 14:25:55.350483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.16
 ---- batch: 020 ----
mean loss: 79.45
train mean loss: 80.67
epoch train time: 0:00:00.362898
elapsed time: 0:00:57.135470
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-10-01 14:25:55.713579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.42
 ---- batch: 020 ----
mean loss: 80.15
train mean loss: 79.74
epoch train time: 0:00:00.367512
elapsed time: 0:00:57.503165
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-10-01 14:25:56.081293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.10
 ---- batch: 020 ----
mean loss: 78.70
train mean loss: 79.72
epoch train time: 0:00:00.363942
elapsed time: 0:00:57.867326
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-10-01 14:25:56.445431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.62
 ---- batch: 020 ----
mean loss: 78.00
train mean loss: 78.47
epoch train time: 0:00:00.357501
elapsed time: 0:00:58.225018
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-10-01 14:25:56.803125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.39
 ---- batch: 020 ----
mean loss: 82.71
train mean loss: 78.42
epoch train time: 0:00:00.362113
elapsed time: 0:00:58.587313
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-10-01 14:25:57.165419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.57
 ---- batch: 020 ----
mean loss: 78.12
train mean loss: 79.73
epoch train time: 0:00:00.359760
elapsed time: 0:00:58.947265
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-10-01 14:25:57.525373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.33
 ---- batch: 020 ----
mean loss: 76.35
train mean loss: 79.02
epoch train time: 0:00:00.364476
elapsed time: 0:00:59.311944
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-10-01 14:25:57.890054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.46
 ---- batch: 020 ----
mean loss: 76.97
train mean loss: 78.46
epoch train time: 0:00:00.364971
elapsed time: 0:00:59.677099
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-10-01 14:25:58.255210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.02
 ---- batch: 020 ----
mean loss: 80.80
train mean loss: 78.66
epoch train time: 0:00:00.358411
elapsed time: 0:01:00.035706
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-10-01 14:25:58.613847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.93
 ---- batch: 020 ----
mean loss: 76.24
train mean loss: 76.36
epoch train time: 0:00:00.368642
elapsed time: 0:01:00.404584
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-10-01 14:25:58.982710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.68
 ---- batch: 020 ----
mean loss: 74.21
train mean loss: 74.98
epoch train time: 0:00:00.366107
elapsed time: 0:01:00.770894
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-10-01 14:25:59.349006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.48
 ---- batch: 020 ----
mean loss: 69.88
train mean loss: 74.96
epoch train time: 0:00:00.364502
elapsed time: 0:01:01.135614
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-10-01 14:25:59.713739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.60
 ---- batch: 020 ----
mean loss: 73.71
train mean loss: 75.50
epoch train time: 0:00:00.389315
elapsed time: 0:01:01.525158
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-10-01 14:26:00.103255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.22
 ---- batch: 020 ----
mean loss: 73.51
train mean loss: 74.53
epoch train time: 0:00:00.363592
elapsed time: 0:01:01.888940
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-10-01 14:26:00.467048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.60
 ---- batch: 020 ----
mean loss: 75.03
train mean loss: 74.78
epoch train time: 0:00:00.361736
elapsed time: 0:01:02.250871
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-10-01 14:26:00.828994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.93
 ---- batch: 020 ----
mean loss: 73.32
train mean loss: 73.07
epoch train time: 0:00:00.369863
elapsed time: 0:01:02.620930
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-10-01 14:26:01.199060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.77
 ---- batch: 020 ----
mean loss: 75.12
train mean loss: 73.01
epoch train time: 0:00:00.367261
elapsed time: 0:01:02.988437
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-10-01 14:26:01.566554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.27
 ---- batch: 020 ----
mean loss: 72.08
train mean loss: 73.76
epoch train time: 0:00:00.369359
elapsed time: 0:01:03.357990
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-10-01 14:26:01.936100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.42
 ---- batch: 020 ----
mean loss: 73.62
train mean loss: 73.04
epoch train time: 0:00:00.363103
elapsed time: 0:01:03.721287
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-10-01 14:26:02.299410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.22
 ---- batch: 020 ----
mean loss: 75.00
train mean loss: 74.13
epoch train time: 0:00:00.359008
elapsed time: 0:01:04.080499
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-10-01 14:26:02.658611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.83
 ---- batch: 020 ----
mean loss: 72.09
train mean loss: 70.92
epoch train time: 0:00:00.367707
elapsed time: 0:01:04.448431
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-10-01 14:26:03.026543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.44
 ---- batch: 020 ----
mean loss: 71.06
train mean loss: 72.30
epoch train time: 0:00:00.357858
elapsed time: 0:01:04.806515
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-10-01 14:26:03.384672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.45
 ---- batch: 020 ----
mean loss: 69.21
train mean loss: 70.89
epoch train time: 0:00:00.365665
elapsed time: 0:01:05.172444
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-10-01 14:26:03.750562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.36
 ---- batch: 020 ----
mean loss: 69.18
train mean loss: 70.92
epoch train time: 0:00:00.360590
elapsed time: 0:01:05.533238
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-10-01 14:26:04.111355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.72
 ---- batch: 020 ----
mean loss: 70.88
train mean loss: 71.62
epoch train time: 0:00:00.358529
elapsed time: 0:01:05.891959
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-10-01 14:26:04.470065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.63
 ---- batch: 020 ----
mean loss: 70.92
train mean loss: 69.36
epoch train time: 0:00:00.363940
elapsed time: 0:01:06.256096
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-10-01 14:26:04.834230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.07
 ---- batch: 020 ----
mean loss: 72.60
train mean loss: 69.91
epoch train time: 0:00:00.358621
elapsed time: 0:01:06.614922
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-10-01 14:26:05.193046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.78
 ---- batch: 020 ----
mean loss: 68.77
train mean loss: 70.25
epoch train time: 0:00:00.353233
elapsed time: 0:01:06.968350
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-10-01 14:26:05.546457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.84
 ---- batch: 020 ----
mean loss: 68.52
train mean loss: 69.67
epoch train time: 0:00:00.372585
elapsed time: 0:01:07.341122
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-10-01 14:26:05.919256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.39
 ---- batch: 020 ----
mean loss: 67.48
train mean loss: 68.66
epoch train time: 0:00:00.369582
elapsed time: 0:01:07.710912
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-10-01 14:26:06.289015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.66
 ---- batch: 020 ----
mean loss: 65.00
train mean loss: 67.97
epoch train time: 0:00:00.369062
elapsed time: 0:01:08.080177
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-10-01 14:26:06.658316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.97
 ---- batch: 020 ----
mean loss: 66.80
train mean loss: 69.49
epoch train time: 0:00:00.401259
elapsed time: 0:01:08.481651
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-10-01 14:26:07.059770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.87
 ---- batch: 020 ----
mean loss: 66.31
train mean loss: 68.16
epoch train time: 0:00:00.360532
elapsed time: 0:01:08.842374
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-10-01 14:26:07.420478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.37
 ---- batch: 020 ----
mean loss: 67.28
train mean loss: 67.46
epoch train time: 0:00:00.361894
elapsed time: 0:01:09.204450
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-10-01 14:26:07.782555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.08
 ---- batch: 020 ----
mean loss: 69.48
train mean loss: 68.17
epoch train time: 0:00:00.356379
elapsed time: 0:01:09.561017
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-10-01 14:26:08.139154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.49
 ---- batch: 020 ----
mean loss: 66.29
train mean loss: 66.19
epoch train time: 0:00:00.391142
elapsed time: 0:01:09.952370
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-10-01 14:26:08.530480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.00
 ---- batch: 020 ----
mean loss: 69.35
train mean loss: 67.38
epoch train time: 0:00:00.363215
elapsed time: 0:01:10.315788
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-10-01 14:26:08.893899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.75
 ---- batch: 020 ----
mean loss: 64.49
train mean loss: 65.89
epoch train time: 0:00:00.353294
elapsed time: 0:01:10.669296
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-10-01 14:26:09.247379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.48
 ---- batch: 020 ----
mean loss: 66.64
train mean loss: 65.18
epoch train time: 0:00:00.352421
elapsed time: 0:01:11.021872
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-10-01 14:26:09.599977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.36
 ---- batch: 020 ----
mean loss: 67.57
train mean loss: 65.46
epoch train time: 0:00:00.358343
elapsed time: 0:01:11.380395
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-10-01 14:26:09.958519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.23
 ---- batch: 020 ----
mean loss: 65.42
train mean loss: 65.13
epoch train time: 0:00:00.356304
elapsed time: 0:01:11.736900
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-10-01 14:26:10.315002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.19
 ---- batch: 020 ----
mean loss: 66.53
train mean loss: 64.53
epoch train time: 0:00:00.356136
elapsed time: 0:01:12.093213
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-10-01 14:26:10.671329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.81
 ---- batch: 020 ----
mean loss: 65.38
train mean loss: 63.69
epoch train time: 0:00:00.348186
elapsed time: 0:01:12.441618
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-10-01 14:26:11.019719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.40
 ---- batch: 020 ----
mean loss: 62.38
train mean loss: 64.28
epoch train time: 0:00:00.351256
elapsed time: 0:01:12.793063
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-10-01 14:26:11.371173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.67
 ---- batch: 020 ----
mean loss: 65.25
train mean loss: 63.69
epoch train time: 0:00:00.350074
elapsed time: 0:01:13.143334
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-10-01 14:26:11.721440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.71
 ---- batch: 020 ----
mean loss: 62.70
train mean loss: 63.55
epoch train time: 0:00:00.357718
elapsed time: 0:01:13.501234
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-10-01 14:26:12.079342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.60
 ---- batch: 020 ----
mean loss: 64.48
train mean loss: 63.74
epoch train time: 0:00:00.360196
elapsed time: 0:01:13.861625
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-10-01 14:26:12.439780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.55
 ---- batch: 020 ----
mean loss: 62.98
train mean loss: 63.34
epoch train time: 0:00:00.359481
elapsed time: 0:01:14.221336
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-10-01 14:26:12.799444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.68
 ---- batch: 020 ----
mean loss: 63.37
train mean loss: 62.85
epoch train time: 0:00:00.360761
elapsed time: 0:01:14.582289
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-10-01 14:26:13.160400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.88
 ---- batch: 020 ----
mean loss: 60.85
train mean loss: 61.52
epoch train time: 0:00:00.379596
elapsed time: 0:01:14.962070
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-10-01 14:26:13.540180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.59
 ---- batch: 020 ----
mean loss: 63.06
train mean loss: 62.14
epoch train time: 0:00:00.360976
elapsed time: 0:01:15.323230
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-10-01 14:26:13.901336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.99
 ---- batch: 020 ----
mean loss: 61.74
train mean loss: 61.68
epoch train time: 0:00:00.352330
elapsed time: 0:01:15.675790
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-10-01 14:26:14.253919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.68
 ---- batch: 020 ----
mean loss: 62.47
train mean loss: 62.76
epoch train time: 0:00:00.352937
elapsed time: 0:01:16.028928
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-10-01 14:26:14.607036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.53
 ---- batch: 020 ----
mean loss: 60.68
train mean loss: 60.90
epoch train time: 0:00:00.372229
elapsed time: 0:01:16.401350
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-10-01 14:26:14.979457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.30
 ---- batch: 020 ----
mean loss: 61.96
train mean loss: 61.22
epoch train time: 0:00:00.362773
elapsed time: 0:01:16.764305
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-10-01 14:26:15.342424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.85
 ---- batch: 020 ----
mean loss: 60.38
train mean loss: 61.21
epoch train time: 0:00:00.366439
elapsed time: 0:01:17.130933
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-10-01 14:26:15.709042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.67
 ---- batch: 020 ----
mean loss: 58.13
train mean loss: 60.23
epoch train time: 0:00:00.369399
elapsed time: 0:01:17.500518
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-10-01 14:26:16.078625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.81
 ---- batch: 020 ----
mean loss: 60.96
train mean loss: 59.97
epoch train time: 0:00:00.363355
elapsed time: 0:01:17.864059
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-10-01 14:26:16.442165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.45
 ---- batch: 020 ----
mean loss: 59.49
train mean loss: 58.47
epoch train time: 0:00:00.381968
elapsed time: 0:01:18.246215
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-10-01 14:26:16.824325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.50
 ---- batch: 020 ----
mean loss: 58.91
train mean loss: 60.16
epoch train time: 0:00:00.359592
elapsed time: 0:01:18.606017
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-10-01 14:26:17.184139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.68
 ---- batch: 020 ----
mean loss: 58.93
train mean loss: 58.03
epoch train time: 0:00:00.358307
elapsed time: 0:01:18.964518
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-10-01 14:26:17.542622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.96
 ---- batch: 020 ----
mean loss: 60.36
train mean loss: 58.58
epoch train time: 0:00:00.362580
elapsed time: 0:01:19.327365
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-10-01 14:26:17.905478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.83
 ---- batch: 020 ----
mean loss: 59.07
train mean loss: 58.58
epoch train time: 0:00:00.360031
elapsed time: 0:01:19.687586
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-10-01 14:26:18.265712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.47
 ---- batch: 020 ----
mean loss: 59.59
train mean loss: 58.93
epoch train time: 0:00:00.361152
elapsed time: 0:01:20.048980
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-10-01 14:26:18.627092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.30
 ---- batch: 020 ----
mean loss: 58.26
train mean loss: 57.56
epoch train time: 0:00:00.367174
elapsed time: 0:01:20.416344
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-10-01 14:26:18.994455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.35
 ---- batch: 020 ----
mean loss: 56.67
train mean loss: 57.33
epoch train time: 0:00:00.356602
elapsed time: 0:01:20.773156
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-10-01 14:26:19.351240
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.39
 ---- batch: 020 ----
mean loss: 59.97
train mean loss: 57.23
epoch train time: 0:00:00.366318
elapsed time: 0:01:21.139641
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-10-01 14:26:19.717771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.17
 ---- batch: 020 ----
mean loss: 56.58
train mean loss: 56.57
epoch train time: 0:00:00.364319
elapsed time: 0:01:21.504200
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-10-01 14:26:20.082311
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.32
 ---- batch: 020 ----
mean loss: 58.64
train mean loss: 57.30
epoch train time: 0:00:00.361310
elapsed time: 0:01:21.865708
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-10-01 14:26:20.443816
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.06
 ---- batch: 020 ----
mean loss: 56.61
train mean loss: 57.31
epoch train time: 0:00:00.363157
elapsed time: 0:01:22.229058
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-10-01 14:26:20.807165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.23
 ---- batch: 020 ----
mean loss: 56.94
train mean loss: 55.55
epoch train time: 0:00:00.364321
elapsed time: 0:01:22.593560
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-10-01 14:26:21.171667
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.88
 ---- batch: 020 ----
mean loss: 57.34
train mean loss: 56.98
epoch train time: 0:00:00.357877
elapsed time: 0:01:22.951612
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-10-01 14:26:21.529738
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.72
 ---- batch: 020 ----
mean loss: 56.68
train mean loss: 56.84
epoch train time: 0:00:00.359997
elapsed time: 0:01:23.311830
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-10-01 14:26:21.889939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.54
 ---- batch: 020 ----
mean loss: 57.10
train mean loss: 56.87
epoch train time: 0:00:00.366142
elapsed time: 0:01:23.678157
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-10-01 14:26:22.256276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.74
 ---- batch: 020 ----
mean loss: 58.11
train mean loss: 57.06
epoch train time: 0:00:00.363690
elapsed time: 0:01:24.042039
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-10-01 14:26:22.620151
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.19
 ---- batch: 020 ----
mean loss: 55.24
train mean loss: 55.74
epoch train time: 0:00:00.380449
elapsed time: 0:01:24.422689
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-10-01 14:26:23.000806
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.37
 ---- batch: 020 ----
mean loss: 55.11
train mean loss: 56.53
epoch train time: 0:00:00.361101
elapsed time: 0:01:24.784013
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-10-01 14:26:23.362123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.24
 ---- batch: 020 ----
mean loss: 57.37
train mean loss: 55.77
epoch train time: 0:00:00.366454
elapsed time: 0:01:25.150657
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-10-01 14:26:23.728816
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.50
 ---- batch: 020 ----
mean loss: 56.47
train mean loss: 56.36
epoch train time: 0:00:00.368899
elapsed time: 0:01:25.519809
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-10-01 14:26:24.097917
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.99
 ---- batch: 020 ----
mean loss: 54.63
train mean loss: 55.41
epoch train time: 0:00:00.364454
elapsed time: 0:01:25.884456
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-10-01 14:26:24.462565
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.64
 ---- batch: 020 ----
mean loss: 58.62
train mean loss: 56.30
epoch train time: 0:00:00.384342
elapsed time: 0:01:26.269009
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-10-01 14:26:24.847116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.87
 ---- batch: 020 ----
mean loss: 57.01
train mean loss: 56.20
epoch train time: 0:00:00.367371
elapsed time: 0:01:26.636562
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-10-01 14:26:25.214709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.44
 ---- batch: 020 ----
mean loss: 56.99
train mean loss: 56.90
epoch train time: 0:00:00.376743
elapsed time: 0:01:27.013528
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-10-01 14:26:25.591653
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.95
 ---- batch: 020 ----
mean loss: 56.47
train mean loss: 56.12
epoch train time: 0:00:00.374691
elapsed time: 0:01:27.388420
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-10-01 14:26:25.966529
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.49
 ---- batch: 020 ----
mean loss: 55.70
train mean loss: 55.90
epoch train time: 0:00:00.367412
elapsed time: 0:01:27.756015
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-10-01 14:26:26.334118
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.55
 ---- batch: 020 ----
mean loss: 57.40
train mean loss: 56.37
epoch train time: 0:00:00.357613
elapsed time: 0:01:28.113811
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-10-01 14:26:26.691917
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.44
 ---- batch: 020 ----
mean loss: 58.45
train mean loss: 57.26
epoch train time: 0:00:00.387741
elapsed time: 0:01:28.501735
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-10-01 14:26:27.079839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.69
 ---- batch: 020 ----
mean loss: 56.80
train mean loss: 56.09
epoch train time: 0:00:00.366817
elapsed time: 0:01:28.868741
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-10-01 14:26:27.446880
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.08
 ---- batch: 020 ----
mean loss: 55.71
train mean loss: 55.61
epoch train time: 0:00:00.365970
elapsed time: 0:01:29.234942
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-10-01 14:26:27.813049
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.32
 ---- batch: 020 ----
mean loss: 57.98
train mean loss: 56.29
epoch train time: 0:00:00.362881
elapsed time: 0:01:29.598045
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-10-01 14:26:28.176149
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.25
 ---- batch: 020 ----
mean loss: 57.58
train mean loss: 56.21
epoch train time: 0:00:00.362832
elapsed time: 0:01:29.961894
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-10-01 14:26:28.540038
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.80
 ---- batch: 020 ----
mean loss: 55.81
train mean loss: 55.63
epoch train time: 0:00:00.370016
elapsed time: 0:01:30.332134
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-10-01 14:26:28.910241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.14
 ---- batch: 020 ----
mean loss: 57.49
train mean loss: 55.50
epoch train time: 0:00:00.366744
elapsed time: 0:01:30.699065
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-10-01 14:26:29.277172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.66
 ---- batch: 020 ----
mean loss: 58.30
train mean loss: 56.51
epoch train time: 0:00:00.370130
elapsed time: 0:01:31.069389
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-10-01 14:26:29.647514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.43
 ---- batch: 020 ----
mean loss: 57.58
train mean loss: 56.24
epoch train time: 0:00:00.370352
elapsed time: 0:01:31.439949
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-10-01 14:26:30.018056
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.28
 ---- batch: 020 ----
mean loss: 57.09
train mean loss: 55.20
epoch train time: 0:00:00.367396
elapsed time: 0:01:31.807531
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-10-01 14:26:30.385639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.26
 ---- batch: 020 ----
mean loss: 53.90
train mean loss: 55.60
epoch train time: 0:00:00.373620
elapsed time: 0:01:32.181350
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-10-01 14:26:30.759471
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.54
 ---- batch: 020 ----
mean loss: 55.11
train mean loss: 56.60
epoch train time: 0:00:00.364510
elapsed time: 0:01:32.546108
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-10-01 14:26:31.124193
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.33
 ---- batch: 020 ----
mean loss: 57.16
train mean loss: 55.11
epoch train time: 0:00:00.355077
elapsed time: 0:01:32.901336
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-10-01 14:26:31.479438
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.59
 ---- batch: 020 ----
mean loss: 55.99
train mean loss: 55.31
epoch train time: 0:00:00.362703
elapsed time: 0:01:33.264227
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-10-01 14:26:31.842333
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.54
 ---- batch: 020 ----
mean loss: 57.20
train mean loss: 55.92
epoch train time: 0:00:00.355854
elapsed time: 0:01:33.620262
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-10-01 14:26:32.198364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.90
 ---- batch: 020 ----
mean loss: 56.15
train mean loss: 54.92
epoch train time: 0:00:00.355969
elapsed time: 0:01:33.976410
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-10-01 14:26:32.554517
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.89
 ---- batch: 020 ----
mean loss: 52.58
train mean loss: 55.08
epoch train time: 0:00:00.355555
elapsed time: 0:01:34.332145
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-10-01 14:26:32.910252
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.48
 ---- batch: 020 ----
mean loss: 56.21
train mean loss: 55.35
epoch train time: 0:00:00.353475
elapsed time: 0:01:34.685818
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-10-01 14:26:33.263925
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.02
 ---- batch: 020 ----
mean loss: 55.62
train mean loss: 55.76
epoch train time: 0:00:00.359834
elapsed time: 0:01:35.045829
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-10-01 14:26:33.623982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.56
 ---- batch: 020 ----
mean loss: 55.98
train mean loss: 55.49
epoch train time: 0:00:00.367007
elapsed time: 0:01:35.413061
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-10-01 14:26:33.991167
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.97
 ---- batch: 020 ----
mean loss: 55.73
train mean loss: 55.15
epoch train time: 0:00:00.362745
elapsed time: 0:01:35.775987
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-10-01 14:26:34.354094
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.60
 ---- batch: 020 ----
mean loss: 56.67
train mean loss: 56.34
epoch train time: 0:00:00.355198
elapsed time: 0:01:36.131362
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-10-01 14:26:34.709467
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.18
 ---- batch: 020 ----
mean loss: 57.84
train mean loss: 55.24
epoch train time: 0:00:00.360139
elapsed time: 0:01:36.491699
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-10-01 14:26:35.069831
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.09
 ---- batch: 020 ----
mean loss: 56.46
train mean loss: 54.95
epoch train time: 0:00:00.362373
elapsed time: 0:01:36.854287
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-10-01 14:26:35.432429
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.05
 ---- batch: 020 ----
mean loss: 53.67
train mean loss: 55.24
epoch train time: 0:00:00.369649
elapsed time: 0:01:37.224162
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-10-01 14:26:35.802272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.33
 ---- batch: 020 ----
mean loss: 55.45
train mean loss: 55.99
epoch train time: 0:00:00.362823
elapsed time: 0:01:37.587172
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-10-01 14:26:36.165282
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.24
 ---- batch: 020 ----
mean loss: 54.85
train mean loss: 54.62
epoch train time: 0:00:00.361289
elapsed time: 0:01:37.948682
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-10-01 14:26:36.526798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.25
 ---- batch: 020 ----
mean loss: 56.67
train mean loss: 55.81
epoch train time: 0:00:00.359848
elapsed time: 0:01:38.316543
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_2/checkpoint.pth.tar
**** end time: 2019-10-01 14:26:36.894604 ****
