Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_6', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 14892
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianDense3...
Done.
**** start time: 2019-09-26 23:12:42.014704 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
    BayesianLinear-2                  [-1, 100]          72,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 112,200
Trainable params: 112,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 23:12:42.024441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4580.82
 ---- batch: 020 ----
mean loss: 4344.79
 ---- batch: 030 ----
mean loss: 4111.07
 ---- batch: 040 ----
mean loss: 3892.85
 ---- batch: 050 ----
mean loss: 3696.81
 ---- batch: 060 ----
mean loss: 3490.09
 ---- batch: 070 ----
mean loss: 3358.12
 ---- batch: 080 ----
mean loss: 3215.75
 ---- batch: 090 ----
mean loss: 3086.68
 ---- batch: 100 ----
mean loss: 2990.83
 ---- batch: 110 ----
mean loss: 2914.02
train mean loss: 3586.40
epoch train time: 0:00:34.680258
elapsed time: 0:00:34.696385
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 23:13:16.711138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2756.96
 ---- batch: 020 ----
mean loss: 2654.76
 ---- batch: 030 ----
mean loss: 2604.33
 ---- batch: 040 ----
mean loss: 2534.82
 ---- batch: 050 ----
mean loss: 2495.76
 ---- batch: 060 ----
mean loss: 2421.02
 ---- batch: 070 ----
mean loss: 2353.73
 ---- batch: 080 ----
mean loss: 2327.54
 ---- batch: 090 ----
mean loss: 2270.91
 ---- batch: 100 ----
mean loss: 2200.19
 ---- batch: 110 ----
mean loss: 2148.86
train mean loss: 2427.29
epoch train time: 0:00:01.939632
elapsed time: 0:00:36.636329
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 23:13:18.651503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2133.35
 ---- batch: 020 ----
mean loss: 2099.18
 ---- batch: 030 ----
mean loss: 2071.41
 ---- batch: 040 ----
mean loss: 2049.12
 ---- batch: 050 ----
mean loss: 1996.62
 ---- batch: 060 ----
mean loss: 1959.08
 ---- batch: 070 ----
mean loss: 1959.06
 ---- batch: 080 ----
mean loss: 1905.88
 ---- batch: 090 ----
mean loss: 1873.77
 ---- batch: 100 ----
mean loss: 1874.24
 ---- batch: 110 ----
mean loss: 1795.74
train mean loss: 1971.13
epoch train time: 0:00:01.893494
elapsed time: 0:00:38.530565
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 23:13:20.545654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1797.02
 ---- batch: 020 ----
mean loss: 1746.28
 ---- batch: 030 ----
mean loss: 1765.46
 ---- batch: 040 ----
mean loss: 1738.47
 ---- batch: 050 ----
mean loss: 1689.70
 ---- batch: 060 ----
mean loss: 1689.22
 ---- batch: 070 ----
mean loss: 1668.12
 ---- batch: 080 ----
mean loss: 1658.91
 ---- batch: 090 ----
mean loss: 1608.37
 ---- batch: 100 ----
mean loss: 1611.21
 ---- batch: 110 ----
mean loss: 1576.12
train mean loss: 1682.85
epoch train time: 0:00:01.947225
elapsed time: 0:00:40.478359
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 23:13:22.493460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1579.65
 ---- batch: 020 ----
mean loss: 1536.70
 ---- batch: 030 ----
mean loss: 1517.80
 ---- batch: 040 ----
mean loss: 1492.26
 ---- batch: 050 ----
mean loss: 1487.57
 ---- batch: 060 ----
mean loss: 1458.08
 ---- batch: 070 ----
mean loss: 1430.02
 ---- batch: 080 ----
mean loss: 1409.49
 ---- batch: 090 ----
mean loss: 1414.70
 ---- batch: 100 ----
mean loss: 1416.09
 ---- batch: 110 ----
mean loss: 1413.30
train mean loss: 1466.06
epoch train time: 0:00:01.926220
elapsed time: 0:00:42.405161
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 23:13:24.420245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1376.30
 ---- batch: 020 ----
mean loss: 1362.11
 ---- batch: 030 ----
mean loss: 1338.13
 ---- batch: 040 ----
mean loss: 1349.80
 ---- batch: 050 ----
mean loss: 1299.47
 ---- batch: 060 ----
mean loss: 1303.47
 ---- batch: 070 ----
mean loss: 1271.46
 ---- batch: 080 ----
mean loss: 1266.43
 ---- batch: 090 ----
mean loss: 1255.84
 ---- batch: 100 ----
mean loss: 1258.06
 ---- batch: 110 ----
mean loss: 1240.15
train mean loss: 1300.78
epoch train time: 0:00:01.935817
elapsed time: 0:00:44.341552
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 23:13:26.356629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1235.39
 ---- batch: 020 ----
mean loss: 1213.11
 ---- batch: 030 ----
mean loss: 1190.55
 ---- batch: 040 ----
mean loss: 1201.38
 ---- batch: 050 ----
mean loss: 1206.55
 ---- batch: 060 ----
mean loss: 1160.00
 ---- batch: 070 ----
mean loss: 1159.28
 ---- batch: 080 ----
mean loss: 1153.65
 ---- batch: 090 ----
mean loss: 1155.91
 ---- batch: 100 ----
mean loss: 1137.06
 ---- batch: 110 ----
mean loss: 1158.49
train mean loss: 1178.32
epoch train time: 0:00:01.882021
elapsed time: 0:00:46.224175
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 23:13:28.239317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1113.04
 ---- batch: 020 ----
mean loss: 1118.06
 ---- batch: 030 ----
mean loss: 1095.17
 ---- batch: 040 ----
mean loss: 1085.76
 ---- batch: 050 ----
mean loss: 1096.40
 ---- batch: 060 ----
mean loss: 1081.28
 ---- batch: 070 ----
mean loss: 1072.40
 ---- batch: 080 ----
mean loss: 1070.65
 ---- batch: 090 ----
mean loss: 1080.25
 ---- batch: 100 ----
mean loss: 1069.58
 ---- batch: 110 ----
mean loss: 1036.39
train mean loss: 1082.71
epoch train time: 0:00:01.904284
elapsed time: 0:00:48.129072
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 23:13:30.144168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1037.59
 ---- batch: 020 ----
mean loss: 1046.97
 ---- batch: 030 ----
mean loss: 1014.77
 ---- batch: 040 ----
mean loss: 1011.13
 ---- batch: 050 ----
mean loss: 1019.36
 ---- batch: 060 ----
mean loss: 1009.93
 ---- batch: 070 ----
mean loss: 1017.47
 ---- batch: 080 ----
mean loss: 1007.24
 ---- batch: 090 ----
mean loss: 995.89
 ---- batch: 100 ----
mean loss: 1006.41
 ---- batch: 110 ----
mean loss: 997.22
train mean loss: 1013.79
epoch train time: 0:00:01.924810
elapsed time: 0:00:50.054456
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 23:13:32.069589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 984.78
 ---- batch: 020 ----
mean loss: 989.26
 ---- batch: 030 ----
mean loss: 958.77
 ---- batch: 040 ----
mean loss: 968.34
 ---- batch: 050 ----
mean loss: 965.85
 ---- batch: 060 ----
mean loss: 965.96
 ---- batch: 070 ----
mean loss: 959.81
 ---- batch: 080 ----
mean loss: 967.37
 ---- batch: 090 ----
mean loss: 952.29
 ---- batch: 100 ----
mean loss: 933.59
 ---- batch: 110 ----
mean loss: 960.19
train mean loss: 963.45
epoch train time: 0:00:01.930514
elapsed time: 0:00:51.985598
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 23:13:34.000730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.36
 ---- batch: 020 ----
mean loss: 931.63
 ---- batch: 030 ----
mean loss: 938.61
 ---- batch: 040 ----
mean loss: 938.80
 ---- batch: 050 ----
mean loss: 927.48
 ---- batch: 060 ----
mean loss: 927.90
 ---- batch: 070 ----
mean loss: 912.47
 ---- batch: 080 ----
mean loss: 919.66
 ---- batch: 090 ----
mean loss: 926.93
 ---- batch: 100 ----
mean loss: 916.28
 ---- batch: 110 ----
mean loss: 905.95
train mean loss: 925.31
epoch train time: 0:00:01.911527
elapsed time: 0:00:53.897766
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 23:13:35.912649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.40
 ---- batch: 020 ----
mean loss: 897.14
 ---- batch: 030 ----
mean loss: 916.33
 ---- batch: 040 ----
mean loss: 904.07
 ---- batch: 050 ----
mean loss: 889.86
 ---- batch: 060 ----
mean loss: 896.59
 ---- batch: 070 ----
mean loss: 902.31
 ---- batch: 080 ----
mean loss: 888.00
 ---- batch: 090 ----
mean loss: 899.82
 ---- batch: 100 ----
mean loss: 894.49
 ---- batch: 110 ----
mean loss: 876.58
train mean loss: 899.10
epoch train time: 0:00:01.952607
elapsed time: 0:00:55.850779
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 23:13:37.865878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 910.54
 ---- batch: 020 ----
mean loss: 893.78
 ---- batch: 030 ----
mean loss: 890.15
 ---- batch: 040 ----
mean loss: 879.52
 ---- batch: 050 ----
mean loss: 873.11
 ---- batch: 060 ----
mean loss: 871.40
 ---- batch: 070 ----
mean loss: 885.80
 ---- batch: 080 ----
mean loss: 865.94
 ---- batch: 090 ----
mean loss: 872.33
 ---- batch: 100 ----
mean loss: 884.36
 ---- batch: 110 ----
mean loss: 862.56
train mean loss: 880.35
epoch train time: 0:00:01.923147
elapsed time: 0:00:57.774563
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 23:13:39.789685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.96
 ---- batch: 020 ----
mean loss: 873.17
 ---- batch: 030 ----
mean loss: 869.95
 ---- batch: 040 ----
mean loss: 861.15
 ---- batch: 050 ----
mean loss: 865.16
 ---- batch: 060 ----
mean loss: 878.69
 ---- batch: 070 ----
mean loss: 871.60
 ---- batch: 080 ----
mean loss: 875.69
 ---- batch: 090 ----
mean loss: 855.36
 ---- batch: 100 ----
mean loss: 881.74
 ---- batch: 110 ----
mean loss: 870.98
train mean loss: 870.33
epoch train time: 0:00:01.883663
elapsed time: 0:00:59.658868
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 23:13:41.673986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.92
 ---- batch: 020 ----
mean loss: 863.11
 ---- batch: 030 ----
mean loss: 879.72
 ---- batch: 040 ----
mean loss: 876.44
 ---- batch: 050 ----
mean loss: 863.81
 ---- batch: 060 ----
mean loss: 854.43
 ---- batch: 070 ----
mean loss: 854.52
 ---- batch: 080 ----
mean loss: 857.18
 ---- batch: 090 ----
mean loss: 859.62
 ---- batch: 100 ----
mean loss: 853.01
 ---- batch: 110 ----
mean loss: 878.75
train mean loss: 864.46
epoch train time: 0:00:01.920809
elapsed time: 0:01:01.580296
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 23:13:43.595393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.84
 ---- batch: 020 ----
mean loss: 863.04
 ---- batch: 030 ----
mean loss: 854.54
 ---- batch: 040 ----
mean loss: 842.01
 ---- batch: 050 ----
mean loss: 858.85
 ---- batch: 060 ----
mean loss: 862.61
 ---- batch: 070 ----
mean loss: 871.34
 ---- batch: 080 ----
mean loss: 848.31
 ---- batch: 090 ----
mean loss: 852.86
 ---- batch: 100 ----
mean loss: 864.68
 ---- batch: 110 ----
mean loss: 849.63
train mean loss: 858.69
epoch train time: 0:00:01.949287
elapsed time: 0:01:03.530255
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 23:13:45.545657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.73
 ---- batch: 020 ----
mean loss: 826.86
 ---- batch: 030 ----
mean loss: 856.09
 ---- batch: 040 ----
mean loss: 874.64
 ---- batch: 050 ----
mean loss: 880.35
 ---- batch: 060 ----
mean loss: 865.68
 ---- batch: 070 ----
mean loss: 862.10
 ---- batch: 080 ----
mean loss: 859.12
 ---- batch: 090 ----
mean loss: 843.85
 ---- batch: 100 ----
mean loss: 846.97
 ---- batch: 110 ----
mean loss: 850.57
train mean loss: 856.12
epoch train time: 0:00:01.915069
elapsed time: 0:01:05.446226
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 23:13:47.461346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.35
 ---- batch: 020 ----
mean loss: 861.68
 ---- batch: 030 ----
mean loss: 841.53
 ---- batch: 040 ----
mean loss: 854.94
 ---- batch: 050 ----
mean loss: 873.02
 ---- batch: 060 ----
mean loss: 835.68
 ---- batch: 070 ----
mean loss: 865.46
 ---- batch: 080 ----
mean loss: 851.73
 ---- batch: 090 ----
mean loss: 852.45
 ---- batch: 100 ----
mean loss: 870.16
 ---- batch: 110 ----
mean loss: 866.95
train mean loss: 855.16
epoch train time: 0:00:01.909582
elapsed time: 0:01:07.356437
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 23:13:49.371517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.55
 ---- batch: 020 ----
mean loss: 861.86
 ---- batch: 030 ----
mean loss: 850.70
 ---- batch: 040 ----
mean loss: 830.31
 ---- batch: 050 ----
mean loss: 843.91
 ---- batch: 060 ----
mean loss: 863.95
 ---- batch: 070 ----
mean loss: 850.96
 ---- batch: 080 ----
mean loss: 848.59
 ---- batch: 090 ----
mean loss: 857.34
 ---- batch: 100 ----
mean loss: 849.13
 ---- batch: 110 ----
mean loss: 873.53
train mean loss: 853.24
epoch train time: 0:00:01.922719
elapsed time: 0:01:09.279735
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 23:13:51.294806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.63
 ---- batch: 020 ----
mean loss: 859.20
 ---- batch: 030 ----
mean loss: 860.20
 ---- batch: 040 ----
mean loss: 843.61
 ---- batch: 050 ----
mean loss: 832.84
 ---- batch: 060 ----
mean loss: 862.34
 ---- batch: 070 ----
mean loss: 850.51
 ---- batch: 080 ----
mean loss: 857.95
 ---- batch: 090 ----
mean loss: 850.83
 ---- batch: 100 ----
mean loss: 851.80
 ---- batch: 110 ----
mean loss: 859.32
train mean loss: 851.45
epoch train time: 0:00:01.919370
elapsed time: 0:01:11.199683
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 23:13:53.214744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.97
 ---- batch: 020 ----
mean loss: 889.51
 ---- batch: 030 ----
mean loss: 847.73
 ---- batch: 040 ----
mean loss: 877.47
 ---- batch: 050 ----
mean loss: 853.87
 ---- batch: 060 ----
mean loss: 866.63
 ---- batch: 070 ----
mean loss: 848.63
 ---- batch: 080 ----
mean loss: 863.94
 ---- batch: 090 ----
mean loss: 838.31
 ---- batch: 100 ----
mean loss: 833.43
 ---- batch: 110 ----
mean loss: 833.45
train mean loss: 851.36
epoch train time: 0:00:01.927194
elapsed time: 0:01:13.127418
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 23:13:55.142306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.70
 ---- batch: 020 ----
mean loss: 863.53
 ---- batch: 030 ----
mean loss: 835.34
 ---- batch: 040 ----
mean loss: 870.52
 ---- batch: 050 ----
mean loss: 856.04
 ---- batch: 060 ----
mean loss: 847.10
 ---- batch: 070 ----
mean loss: 863.59
 ---- batch: 080 ----
mean loss: 846.32
 ---- batch: 090 ----
mean loss: 842.44
 ---- batch: 100 ----
mean loss: 843.20
 ---- batch: 110 ----
mean loss: 860.87
train mean loss: 851.81
epoch train time: 0:00:01.897138
elapsed time: 0:01:15.024962
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 23:13:57.040063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.86
 ---- batch: 020 ----
mean loss: 842.22
 ---- batch: 030 ----
mean loss: 821.81
 ---- batch: 040 ----
mean loss: 850.57
 ---- batch: 050 ----
mean loss: 879.41
 ---- batch: 060 ----
mean loss: 841.43
 ---- batch: 070 ----
mean loss: 871.77
 ---- batch: 080 ----
mean loss: 840.25
 ---- batch: 090 ----
mean loss: 851.29
 ---- batch: 100 ----
mean loss: 866.89
 ---- batch: 110 ----
mean loss: 848.53
train mean loss: 850.33
epoch train time: 0:00:01.947864
elapsed time: 0:01:16.973425
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 23:13:58.988512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.89
 ---- batch: 020 ----
mean loss: 853.09
 ---- batch: 030 ----
mean loss: 864.55
 ---- batch: 040 ----
mean loss: 852.93
 ---- batch: 050 ----
mean loss: 867.82
 ---- batch: 060 ----
mean loss: 842.24
 ---- batch: 070 ----
mean loss: 840.27
 ---- batch: 080 ----
mean loss: 862.78
 ---- batch: 090 ----
mean loss: 851.55
 ---- batch: 100 ----
mean loss: 856.15
 ---- batch: 110 ----
mean loss: 835.18
train mean loss: 851.29
epoch train time: 0:00:01.902839
elapsed time: 0:01:18.876941
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 23:14:00.892077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.34
 ---- batch: 020 ----
mean loss: 850.01
 ---- batch: 030 ----
mean loss: 831.64
 ---- batch: 040 ----
mean loss: 854.98
 ---- batch: 050 ----
mean loss: 852.37
 ---- batch: 060 ----
mean loss: 868.43
 ---- batch: 070 ----
mean loss: 824.78
 ---- batch: 080 ----
mean loss: 855.37
 ---- batch: 090 ----
mean loss: 859.97
 ---- batch: 100 ----
mean loss: 840.11
 ---- batch: 110 ----
mean loss: 861.20
train mean loss: 850.66
epoch train time: 0:00:01.926754
elapsed time: 0:01:20.804361
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 23:14:02.819454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.71
 ---- batch: 020 ----
mean loss: 851.76
 ---- batch: 030 ----
mean loss: 851.22
 ---- batch: 040 ----
mean loss: 847.88
 ---- batch: 050 ----
mean loss: 836.33
 ---- batch: 060 ----
mean loss: 857.75
 ---- batch: 070 ----
mean loss: 868.91
 ---- batch: 080 ----
mean loss: 835.47
 ---- batch: 090 ----
mean loss: 852.89
 ---- batch: 100 ----
mean loss: 845.84
 ---- batch: 110 ----
mean loss: 844.28
train mean loss: 849.51
epoch train time: 0:00:01.916335
elapsed time: 0:01:22.721341
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 23:14:04.736499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.49
 ---- batch: 020 ----
mean loss: 858.75
 ---- batch: 030 ----
mean loss: 861.40
 ---- batch: 040 ----
mean loss: 844.84
 ---- batch: 050 ----
mean loss: 845.74
 ---- batch: 060 ----
mean loss: 828.51
 ---- batch: 070 ----
mean loss: 830.27
 ---- batch: 080 ----
mean loss: 864.83
 ---- batch: 090 ----
mean loss: 865.96
 ---- batch: 100 ----
mean loss: 841.27
 ---- batch: 110 ----
mean loss: 843.61
train mean loss: 848.46
epoch train time: 0:00:01.883405
elapsed time: 0:01:24.605413
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 23:14:06.620464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.54
 ---- batch: 020 ----
mean loss: 840.25
 ---- batch: 030 ----
mean loss: 862.82
 ---- batch: 040 ----
mean loss: 871.06
 ---- batch: 050 ----
mean loss: 839.82
 ---- batch: 060 ----
mean loss: 845.21
 ---- batch: 070 ----
mean loss: 838.31
 ---- batch: 080 ----
mean loss: 854.90
 ---- batch: 090 ----
mean loss: 859.37
 ---- batch: 100 ----
mean loss: 839.60
 ---- batch: 110 ----
mean loss: 852.68
train mean loss: 849.67
epoch train time: 0:00:01.890596
elapsed time: 0:01:26.496568
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 23:14:08.511636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 813.08
 ---- batch: 020 ----
mean loss: 842.13
 ---- batch: 030 ----
mean loss: 863.05
 ---- batch: 040 ----
mean loss: 870.38
 ---- batch: 050 ----
mean loss: 862.32
 ---- batch: 060 ----
mean loss: 840.83
 ---- batch: 070 ----
mean loss: 860.16
 ---- batch: 080 ----
mean loss: 852.28
 ---- batch: 090 ----
mean loss: 828.57
 ---- batch: 100 ----
mean loss: 860.76
 ---- batch: 110 ----
mean loss: 845.13
train mean loss: 849.74
epoch train time: 0:00:01.914803
elapsed time: 0:01:28.411915
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 23:14:10.426992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.80
 ---- batch: 020 ----
mean loss: 829.25
 ---- batch: 030 ----
mean loss: 839.73
 ---- batch: 040 ----
mean loss: 850.40
 ---- batch: 050 ----
mean loss: 848.86
 ---- batch: 060 ----
mean loss: 849.34
 ---- batch: 070 ----
mean loss: 851.12
 ---- batch: 080 ----
mean loss: 862.03
 ---- batch: 090 ----
mean loss: 849.53
 ---- batch: 100 ----
mean loss: 852.20
 ---- batch: 110 ----
mean loss: 844.68
train mean loss: 848.38
epoch train time: 0:00:01.923222
elapsed time: 0:01:30.335721
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 23:14:12.350831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.32
 ---- batch: 020 ----
mean loss: 845.25
 ---- batch: 030 ----
mean loss: 854.60
 ---- batch: 040 ----
mean loss: 862.82
 ---- batch: 050 ----
mean loss: 847.43
 ---- batch: 060 ----
mean loss: 843.43
 ---- batch: 070 ----
mean loss: 809.17
 ---- batch: 080 ----
mean loss: 857.05
 ---- batch: 090 ----
mean loss: 847.76
 ---- batch: 100 ----
mean loss: 858.30
 ---- batch: 110 ----
mean loss: 860.77
train mean loss: 848.17
epoch train time: 0:00:01.960500
elapsed time: 0:01:32.296825
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 23:14:14.311907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.57
 ---- batch: 020 ----
mean loss: 838.37
 ---- batch: 030 ----
mean loss: 867.36
 ---- batch: 040 ----
mean loss: 872.76
 ---- batch: 050 ----
mean loss: 821.07
 ---- batch: 060 ----
mean loss: 834.03
 ---- batch: 070 ----
mean loss: 859.18
 ---- batch: 080 ----
mean loss: 851.32
 ---- batch: 090 ----
mean loss: 848.15
 ---- batch: 100 ----
mean loss: 848.65
 ---- batch: 110 ----
mean loss: 835.46
train mean loss: 847.68
epoch train time: 0:00:01.918202
elapsed time: 0:01:34.215603
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 23:14:16.230792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.12
 ---- batch: 020 ----
mean loss: 839.54
 ---- batch: 030 ----
mean loss: 857.24
 ---- batch: 040 ----
mean loss: 870.11
 ---- batch: 050 ----
mean loss: 853.65
 ---- batch: 060 ----
mean loss: 858.53
 ---- batch: 070 ----
mean loss: 833.36
 ---- batch: 080 ----
mean loss: 855.92
 ---- batch: 090 ----
mean loss: 845.36
 ---- batch: 100 ----
mean loss: 854.90
 ---- batch: 110 ----
mean loss: 836.04
train mean loss: 848.60
epoch train time: 0:00:01.934830
elapsed time: 0:01:36.151128
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 23:14:18.166211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.54
 ---- batch: 020 ----
mean loss: 853.29
 ---- batch: 030 ----
mean loss: 842.65
 ---- batch: 040 ----
mean loss: 845.22
 ---- batch: 050 ----
mean loss: 829.63
 ---- batch: 060 ----
mean loss: 841.30
 ---- batch: 070 ----
mean loss: 846.44
 ---- batch: 080 ----
mean loss: 863.78
 ---- batch: 090 ----
mean loss: 860.36
 ---- batch: 100 ----
mean loss: 854.96
 ---- batch: 110 ----
mean loss: 859.32
train mean loss: 848.11
epoch train time: 0:00:01.929429
elapsed time: 0:01:38.081148
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 23:14:20.096043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.73
 ---- batch: 020 ----
mean loss: 819.05
 ---- batch: 030 ----
mean loss: 843.63
 ---- batch: 040 ----
mean loss: 856.06
 ---- batch: 050 ----
mean loss: 852.96
 ---- batch: 060 ----
mean loss: 855.56
 ---- batch: 070 ----
mean loss: 838.45
 ---- batch: 080 ----
mean loss: 855.22
 ---- batch: 090 ----
mean loss: 859.02
 ---- batch: 100 ----
mean loss: 857.58
 ---- batch: 110 ----
mean loss: 844.48
train mean loss: 848.09
epoch train time: 0:00:01.928810
elapsed time: 0:01:40.010345
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 23:14:22.025442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.74
 ---- batch: 020 ----
mean loss: 866.24
 ---- batch: 030 ----
mean loss: 838.88
 ---- batch: 040 ----
mean loss: 849.07
 ---- batch: 050 ----
mean loss: 837.27
 ---- batch: 060 ----
mean loss: 841.77
 ---- batch: 070 ----
mean loss: 846.65
 ---- batch: 080 ----
mean loss: 857.62
 ---- batch: 090 ----
mean loss: 834.45
 ---- batch: 100 ----
mean loss: 839.56
 ---- batch: 110 ----
mean loss: 837.84
train mean loss: 847.61
epoch train time: 0:00:01.959563
elapsed time: 0:01:41.970561
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 23:14:23.985668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.64
 ---- batch: 020 ----
mean loss: 844.16
 ---- batch: 030 ----
mean loss: 856.93
 ---- batch: 040 ----
mean loss: 845.22
 ---- batch: 050 ----
mean loss: 831.25
 ---- batch: 060 ----
mean loss: 853.78
 ---- batch: 070 ----
mean loss: 838.67
 ---- batch: 080 ----
mean loss: 830.26
 ---- batch: 090 ----
mean loss: 865.48
 ---- batch: 100 ----
mean loss: 856.70
 ---- batch: 110 ----
mean loss: 827.82
train mean loss: 846.28
epoch train time: 0:00:01.949799
elapsed time: 0:01:43.920949
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 23:14:25.936019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.91
 ---- batch: 020 ----
mean loss: 861.31
 ---- batch: 030 ----
mean loss: 833.11
 ---- batch: 040 ----
mean loss: 841.83
 ---- batch: 050 ----
mean loss: 871.82
 ---- batch: 060 ----
mean loss: 836.70
 ---- batch: 070 ----
mean loss: 838.70
 ---- batch: 080 ----
mean loss: 835.61
 ---- batch: 090 ----
mean loss: 830.03
 ---- batch: 100 ----
mean loss: 859.11
 ---- batch: 110 ----
mean loss: 858.11
train mean loss: 847.03
epoch train time: 0:00:01.893266
elapsed time: 0:01:45.814800
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 23:14:27.829899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.55
 ---- batch: 020 ----
mean loss: 839.60
 ---- batch: 030 ----
mean loss: 856.77
 ---- batch: 040 ----
mean loss: 864.11
 ---- batch: 050 ----
mean loss: 819.66
 ---- batch: 060 ----
mean loss: 833.62
 ---- batch: 070 ----
mean loss: 862.64
 ---- batch: 080 ----
mean loss: 858.34
 ---- batch: 090 ----
mean loss: 827.77
 ---- batch: 100 ----
mean loss: 846.20
 ---- batch: 110 ----
mean loss: 846.50
train mean loss: 847.01
epoch train time: 0:00:01.930086
elapsed time: 0:01:47.745492
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 23:14:29.760577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.18
 ---- batch: 020 ----
mean loss: 847.67
 ---- batch: 030 ----
mean loss: 845.05
 ---- batch: 040 ----
mean loss: 851.25
 ---- batch: 050 ----
mean loss: 837.64
 ---- batch: 060 ----
mean loss: 836.87
 ---- batch: 070 ----
mean loss: 842.78
 ---- batch: 080 ----
mean loss: 854.19
 ---- batch: 090 ----
mean loss: 846.19
 ---- batch: 100 ----
mean loss: 851.45
 ---- batch: 110 ----
mean loss: 850.88
train mean loss: 846.54
epoch train time: 0:00:01.904206
elapsed time: 0:01:49.650327
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 23:14:31.665419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.29
 ---- batch: 020 ----
mean loss: 857.44
 ---- batch: 030 ----
mean loss: 836.13
 ---- batch: 040 ----
mean loss: 859.84
 ---- batch: 050 ----
mean loss: 858.05
 ---- batch: 060 ----
mean loss: 849.30
 ---- batch: 070 ----
mean loss: 835.69
 ---- batch: 080 ----
mean loss: 849.01
 ---- batch: 090 ----
mean loss: 837.62
 ---- batch: 100 ----
mean loss: 840.96
 ---- batch: 110 ----
mean loss: 845.24
train mean loss: 846.00
epoch train time: 0:00:01.946764
elapsed time: 0:01:51.597682
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 23:14:33.612753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.52
 ---- batch: 020 ----
mean loss: 828.16
 ---- batch: 030 ----
mean loss: 842.14
 ---- batch: 040 ----
mean loss: 851.16
 ---- batch: 050 ----
mean loss: 840.52
 ---- batch: 060 ----
mean loss: 853.76
 ---- batch: 070 ----
mean loss: 849.26
 ---- batch: 080 ----
mean loss: 870.91
 ---- batch: 090 ----
mean loss: 855.54
 ---- batch: 100 ----
mean loss: 848.20
 ---- batch: 110 ----
mean loss: 825.69
train mean loss: 847.35
epoch train time: 0:00:01.906974
elapsed time: 0:01:53.505226
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 23:14:35.520299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.10
 ---- batch: 020 ----
mean loss: 835.70
 ---- batch: 030 ----
mean loss: 849.98
 ---- batch: 040 ----
mean loss: 838.41
 ---- batch: 050 ----
mean loss: 871.70
 ---- batch: 060 ----
mean loss: 833.30
 ---- batch: 070 ----
mean loss: 846.71
 ---- batch: 080 ----
mean loss: 859.26
 ---- batch: 090 ----
mean loss: 845.22
 ---- batch: 100 ----
mean loss: 846.36
 ---- batch: 110 ----
mean loss: 842.61
train mean loss: 846.79
epoch train time: 0:00:01.943783
elapsed time: 0:01:55.449561
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 23:14:37.464659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.16
 ---- batch: 020 ----
mean loss: 851.59
 ---- batch: 030 ----
mean loss: 856.56
 ---- batch: 040 ----
mean loss: 839.78
 ---- batch: 050 ----
mean loss: 847.97
 ---- batch: 060 ----
mean loss: 844.81
 ---- batch: 070 ----
mean loss: 811.25
 ---- batch: 080 ----
mean loss: 856.34
 ---- batch: 090 ----
mean loss: 843.72
 ---- batch: 100 ----
mean loss: 852.14
 ---- batch: 110 ----
mean loss: 852.42
train mean loss: 846.24
epoch train time: 0:00:01.904546
elapsed time: 0:01:57.354697
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 23:14:39.369767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.77
 ---- batch: 020 ----
mean loss: 837.41
 ---- batch: 030 ----
mean loss: 851.69
 ---- batch: 040 ----
mean loss: 859.14
 ---- batch: 050 ----
mean loss: 835.91
 ---- batch: 060 ----
mean loss: 868.43
 ---- batch: 070 ----
mean loss: 854.78
 ---- batch: 080 ----
mean loss: 833.96
 ---- batch: 090 ----
mean loss: 822.59
 ---- batch: 100 ----
mean loss: 832.67
 ---- batch: 110 ----
mean loss: 847.35
train mean loss: 845.90
epoch train time: 0:00:01.941404
elapsed time: 0:01:59.296731
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 23:14:41.311791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.09
 ---- batch: 020 ----
mean loss: 870.50
 ---- batch: 030 ----
mean loss: 858.74
 ---- batch: 040 ----
mean loss: 853.29
 ---- batch: 050 ----
mean loss: 834.67
 ---- batch: 060 ----
mean loss: 843.08
 ---- batch: 070 ----
mean loss: 840.36
 ---- batch: 080 ----
mean loss: 846.30
 ---- batch: 090 ----
mean loss: 843.18
 ---- batch: 100 ----
mean loss: 843.45
 ---- batch: 110 ----
mean loss: 855.30
train mean loss: 845.81
epoch train time: 0:00:01.918329
elapsed time: 0:02:01.215611
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 23:14:43.230734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.91
 ---- batch: 020 ----
mean loss: 827.78
 ---- batch: 030 ----
mean loss: 858.24
 ---- batch: 040 ----
mean loss: 856.35
 ---- batch: 050 ----
mean loss: 829.82
 ---- batch: 060 ----
mean loss: 825.39
 ---- batch: 070 ----
mean loss: 866.45
 ---- batch: 080 ----
mean loss: 822.73
 ---- batch: 090 ----
mean loss: 868.99
 ---- batch: 100 ----
mean loss: 843.57
 ---- batch: 110 ----
mean loss: 854.49
train mean loss: 845.55
epoch train time: 0:00:01.921166
elapsed time: 0:02:03.137429
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 23:14:45.152516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.57
 ---- batch: 020 ----
mean loss: 841.38
 ---- batch: 030 ----
mean loss: 835.25
 ---- batch: 040 ----
mean loss: 834.65
 ---- batch: 050 ----
mean loss: 844.12
 ---- batch: 060 ----
mean loss: 843.00
 ---- batch: 070 ----
mean loss: 867.15
 ---- batch: 080 ----
mean loss: 853.43
 ---- batch: 090 ----
mean loss: 866.96
 ---- batch: 100 ----
mean loss: 831.38
 ---- batch: 110 ----
mean loss: 842.95
train mean loss: 845.84
epoch train time: 0:00:01.911791
elapsed time: 0:02:05.049840
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 23:14:47.064902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.24
 ---- batch: 020 ----
mean loss: 847.21
 ---- batch: 030 ----
mean loss: 852.22
 ---- batch: 040 ----
mean loss: 857.99
 ---- batch: 050 ----
mean loss: 836.14
 ---- batch: 060 ----
mean loss: 852.60
 ---- batch: 070 ----
mean loss: 866.24
 ---- batch: 080 ----
mean loss: 836.10
 ---- batch: 090 ----
mean loss: 834.66
 ---- batch: 100 ----
mean loss: 852.59
 ---- batch: 110 ----
mean loss: 827.00
train mean loss: 846.04
epoch train time: 0:00:01.917367
elapsed time: 0:02:06.967760
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 23:14:48.982818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.01
 ---- batch: 020 ----
mean loss: 849.90
 ---- batch: 030 ----
mean loss: 858.32
 ---- batch: 040 ----
mean loss: 834.90
 ---- batch: 050 ----
mean loss: 836.17
 ---- batch: 060 ----
mean loss: 849.23
 ---- batch: 070 ----
mean loss: 829.82
 ---- batch: 080 ----
mean loss: 846.33
 ---- batch: 090 ----
mean loss: 848.80
 ---- batch: 100 ----
mean loss: 840.11
 ---- batch: 110 ----
mean loss: 847.28
train mean loss: 845.43
epoch train time: 0:00:01.906661
elapsed time: 0:02:08.875016
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 23:14:50.890148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.70
 ---- batch: 020 ----
mean loss: 847.42
 ---- batch: 030 ----
mean loss: 865.65
 ---- batch: 040 ----
mean loss: 853.84
 ---- batch: 050 ----
mean loss: 839.54
 ---- batch: 060 ----
mean loss: 843.62
 ---- batch: 070 ----
mean loss: 863.54
 ---- batch: 080 ----
mean loss: 858.35
 ---- batch: 090 ----
mean loss: 834.23
 ---- batch: 100 ----
mean loss: 821.25
 ---- batch: 110 ----
mean loss: 841.63
train mean loss: 845.94
epoch train time: 0:00:01.935094
elapsed time: 0:02:10.810763
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 23:14:52.825857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.89
 ---- batch: 020 ----
mean loss: 845.74
 ---- batch: 030 ----
mean loss: 844.72
 ---- batch: 040 ----
mean loss: 838.90
 ---- batch: 050 ----
mean loss: 847.08
 ---- batch: 060 ----
mean loss: 865.43
 ---- batch: 070 ----
mean loss: 844.28
 ---- batch: 080 ----
mean loss: 810.56
 ---- batch: 090 ----
mean loss: 836.90
 ---- batch: 100 ----
mean loss: 861.97
 ---- batch: 110 ----
mean loss: 849.24
train mean loss: 845.89
epoch train time: 0:00:01.944491
elapsed time: 0:02:12.755922
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 23:14:54.771072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.54
 ---- batch: 020 ----
mean loss: 834.31
 ---- batch: 030 ----
mean loss: 839.87
 ---- batch: 040 ----
mean loss: 826.11
 ---- batch: 050 ----
mean loss: 839.25
 ---- batch: 060 ----
mean loss: 858.29
 ---- batch: 070 ----
mean loss: 823.96
 ---- batch: 080 ----
mean loss: 858.52
 ---- batch: 090 ----
mean loss: 835.99
 ---- batch: 100 ----
mean loss: 853.20
 ---- batch: 110 ----
mean loss: 865.41
train mean loss: 844.85
epoch train time: 0:00:01.946601
elapsed time: 0:02:14.703233
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 23:14:56.718393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.72
 ---- batch: 020 ----
mean loss: 859.03
 ---- batch: 030 ----
mean loss: 856.35
 ---- batch: 040 ----
mean loss: 839.46
 ---- batch: 050 ----
mean loss: 831.52
 ---- batch: 060 ----
mean loss: 842.82
 ---- batch: 070 ----
mean loss: 854.36
 ---- batch: 080 ----
mean loss: 849.27
 ---- batch: 090 ----
mean loss: 832.26
 ---- batch: 100 ----
mean loss: 832.81
 ---- batch: 110 ----
mean loss: 835.25
train mean loss: 843.34
epoch train time: 0:00:01.900628
elapsed time: 0:02:16.604569
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 23:14:58.619668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.42
 ---- batch: 020 ----
mean loss: 821.93
 ---- batch: 030 ----
mean loss: 796.37
 ---- batch: 040 ----
mean loss: 758.30
 ---- batch: 050 ----
mean loss: 715.83
 ---- batch: 060 ----
mean loss: 630.95
 ---- batch: 070 ----
mean loss: 581.47
 ---- batch: 080 ----
mean loss: 530.66
 ---- batch: 090 ----
mean loss: 491.96
 ---- batch: 100 ----
mean loss: 467.40
 ---- batch: 110 ----
mean loss: 460.36
train mean loss: 639.64
epoch train time: 0:00:01.929417
elapsed time: 0:02:18.534603
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 23:15:00.549777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.03
 ---- batch: 020 ----
mean loss: 425.82
 ---- batch: 030 ----
mean loss: 422.58
 ---- batch: 040 ----
mean loss: 409.45
 ---- batch: 050 ----
mean loss: 396.24
 ---- batch: 060 ----
mean loss: 397.06
 ---- batch: 070 ----
mean loss: 363.22
 ---- batch: 080 ----
mean loss: 369.25
 ---- batch: 090 ----
mean loss: 371.58
 ---- batch: 100 ----
mean loss: 359.17
 ---- batch: 110 ----
mean loss: 356.70
train mean loss: 391.78
epoch train time: 0:00:01.918218
elapsed time: 0:02:20.453475
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 23:15:02.468286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.92
 ---- batch: 020 ----
mean loss: 361.86
 ---- batch: 030 ----
mean loss: 359.17
 ---- batch: 040 ----
mean loss: 345.12
 ---- batch: 050 ----
mean loss: 355.37
 ---- batch: 060 ----
mean loss: 353.65
 ---- batch: 070 ----
mean loss: 336.33
 ---- batch: 080 ----
mean loss: 332.21
 ---- batch: 090 ----
mean loss: 342.81
 ---- batch: 100 ----
mean loss: 327.83
 ---- batch: 110 ----
mean loss: 325.73
train mean loss: 344.26
epoch train time: 0:00:01.935282
elapsed time: 0:02:22.389062
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 23:15:04.404127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.91
 ---- batch: 020 ----
mean loss: 317.68
 ---- batch: 030 ----
mean loss: 314.38
 ---- batch: 040 ----
mean loss: 313.63
 ---- batch: 050 ----
mean loss: 331.03
 ---- batch: 060 ----
mean loss: 317.78
 ---- batch: 070 ----
mean loss: 329.34
 ---- batch: 080 ----
mean loss: 322.25
 ---- batch: 090 ----
mean loss: 318.31
 ---- batch: 100 ----
mean loss: 317.28
 ---- batch: 110 ----
mean loss: 321.43
train mean loss: 320.66
epoch train time: 0:00:01.890966
elapsed time: 0:02:24.280605
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 23:15:06.295738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.66
 ---- batch: 020 ----
mean loss: 311.63
 ---- batch: 030 ----
mean loss: 305.89
 ---- batch: 040 ----
mean loss: 303.35
 ---- batch: 050 ----
mean loss: 302.09
 ---- batch: 060 ----
mean loss: 291.42
 ---- batch: 070 ----
mean loss: 299.55
 ---- batch: 080 ----
mean loss: 320.28
 ---- batch: 090 ----
mean loss: 300.29
 ---- batch: 100 ----
mean loss: 307.84
 ---- batch: 110 ----
mean loss: 297.65
train mean loss: 303.32
epoch train time: 0:00:01.932285
elapsed time: 0:02:26.213539
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 23:15:08.228646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.86
 ---- batch: 020 ----
mean loss: 292.38
 ---- batch: 030 ----
mean loss: 297.51
 ---- batch: 040 ----
mean loss: 294.72
 ---- batch: 050 ----
mean loss: 277.75
 ---- batch: 060 ----
mean loss: 282.13
 ---- batch: 070 ----
mean loss: 277.35
 ---- batch: 080 ----
mean loss: 288.41
 ---- batch: 090 ----
mean loss: 302.40
 ---- batch: 100 ----
mean loss: 292.48
 ---- batch: 110 ----
mean loss: 298.61
train mean loss: 290.56
epoch train time: 0:00:01.930092
elapsed time: 0:02:28.144233
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 23:15:10.159299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.98
 ---- batch: 020 ----
mean loss: 278.15
 ---- batch: 030 ----
mean loss: 294.88
 ---- batch: 040 ----
mean loss: 286.79
 ---- batch: 050 ----
mean loss: 284.91
 ---- batch: 060 ----
mean loss: 283.52
 ---- batch: 070 ----
mean loss: 277.96
 ---- batch: 080 ----
mean loss: 282.78
 ---- batch: 090 ----
mean loss: 286.32
 ---- batch: 100 ----
mean loss: 282.08
 ---- batch: 110 ----
mean loss: 273.10
train mean loss: 282.76
epoch train time: 0:00:01.913000
elapsed time: 0:02:30.057778
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 23:15:12.072842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.73
 ---- batch: 020 ----
mean loss: 280.34
 ---- batch: 030 ----
mean loss: 272.40
 ---- batch: 040 ----
mean loss: 266.28
 ---- batch: 050 ----
mean loss: 283.66
 ---- batch: 060 ----
mean loss: 270.65
 ---- batch: 070 ----
mean loss: 280.25
 ---- batch: 080 ----
mean loss: 270.50
 ---- batch: 090 ----
mean loss: 282.45
 ---- batch: 100 ----
mean loss: 263.34
 ---- batch: 110 ----
mean loss: 283.78
train mean loss: 274.20
epoch train time: 0:00:01.932900
elapsed time: 0:02:31.991257
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 23:15:14.006316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.85
 ---- batch: 020 ----
mean loss: 263.23
 ---- batch: 030 ----
mean loss: 264.78
 ---- batch: 040 ----
mean loss: 276.50
 ---- batch: 050 ----
mean loss: 263.79
 ---- batch: 060 ----
mean loss: 266.00
 ---- batch: 070 ----
mean loss: 264.87
 ---- batch: 080 ----
mean loss: 278.80
 ---- batch: 090 ----
mean loss: 264.56
 ---- batch: 100 ----
mean loss: 267.16
 ---- batch: 110 ----
mean loss: 266.24
train mean loss: 268.10
epoch train time: 0:00:01.906701
elapsed time: 0:02:33.898562
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 23:15:15.913692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.74
 ---- batch: 020 ----
mean loss: 257.68
 ---- batch: 030 ----
mean loss: 257.85
 ---- batch: 040 ----
mean loss: 258.88
 ---- batch: 050 ----
mean loss: 258.29
 ---- batch: 060 ----
mean loss: 256.70
 ---- batch: 070 ----
mean loss: 268.47
 ---- batch: 080 ----
mean loss: 255.61
 ---- batch: 090 ----
mean loss: 253.88
 ---- batch: 100 ----
mean loss: 265.37
 ---- batch: 110 ----
mean loss: 273.71
train mean loss: 261.38
epoch train time: 0:00:01.931528
elapsed time: 0:02:35.830726
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 23:15:17.845863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.62
 ---- batch: 020 ----
mean loss: 259.20
 ---- batch: 030 ----
mean loss: 250.79
 ---- batch: 040 ----
mean loss: 243.55
 ---- batch: 050 ----
mean loss: 246.65
 ---- batch: 060 ----
mean loss: 260.51
 ---- batch: 070 ----
mean loss: 266.83
 ---- batch: 080 ----
mean loss: 261.21
 ---- batch: 090 ----
mean loss: 264.96
 ---- batch: 100 ----
mean loss: 250.77
 ---- batch: 110 ----
mean loss: 257.66
train mean loss: 257.09
epoch train time: 0:00:01.939035
elapsed time: 0:02:37.770412
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 23:15:19.785515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.07
 ---- batch: 020 ----
mean loss: 242.69
 ---- batch: 030 ----
mean loss: 255.16
 ---- batch: 040 ----
mean loss: 253.76
 ---- batch: 050 ----
mean loss: 259.50
 ---- batch: 060 ----
mean loss: 250.93
 ---- batch: 070 ----
mean loss: 257.29
 ---- batch: 080 ----
mean loss: 244.63
 ---- batch: 090 ----
mean loss: 245.82
 ---- batch: 100 ----
mean loss: 248.71
 ---- batch: 110 ----
mean loss: 253.75
train mean loss: 251.84
epoch train time: 0:00:01.934413
elapsed time: 0:02:39.705441
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 23:15:21.720526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.23
 ---- batch: 020 ----
mean loss: 246.50
 ---- batch: 030 ----
mean loss: 248.32
 ---- batch: 040 ----
mean loss: 244.93
 ---- batch: 050 ----
mean loss: 240.22
 ---- batch: 060 ----
mean loss: 257.30
 ---- batch: 070 ----
mean loss: 244.61
 ---- batch: 080 ----
mean loss: 251.34
 ---- batch: 090 ----
mean loss: 244.00
 ---- batch: 100 ----
mean loss: 248.85
 ---- batch: 110 ----
mean loss: 240.29
train mean loss: 247.28
epoch train time: 0:00:01.900889
elapsed time: 0:02:41.606955
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 23:15:23.622098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.17
 ---- batch: 020 ----
mean loss: 236.42
 ---- batch: 030 ----
mean loss: 230.01
 ---- batch: 040 ----
mean loss: 243.16
 ---- batch: 050 ----
mean loss: 248.49
 ---- batch: 060 ----
mean loss: 234.62
 ---- batch: 070 ----
mean loss: 242.15
 ---- batch: 080 ----
mean loss: 234.90
 ---- batch: 090 ----
mean loss: 241.07
 ---- batch: 100 ----
mean loss: 244.06
 ---- batch: 110 ----
mean loss: 245.90
train mean loss: 240.94
epoch train time: 0:00:01.912278
elapsed time: 0:02:43.519887
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 23:15:25.534977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.49
 ---- batch: 020 ----
mean loss: 238.90
 ---- batch: 030 ----
mean loss: 230.27
 ---- batch: 040 ----
mean loss: 240.41
 ---- batch: 050 ----
mean loss: 237.90
 ---- batch: 060 ----
mean loss: 245.51
 ---- batch: 070 ----
mean loss: 246.58
 ---- batch: 080 ----
mean loss: 232.02
 ---- batch: 090 ----
mean loss: 244.19
 ---- batch: 100 ----
mean loss: 227.86
 ---- batch: 110 ----
mean loss: 243.85
train mean loss: 238.52
epoch train time: 0:00:01.910288
elapsed time: 0:02:45.430748
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 23:15:27.445854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.43
 ---- batch: 020 ----
mean loss: 235.06
 ---- batch: 030 ----
mean loss: 236.45
 ---- batch: 040 ----
mean loss: 230.33
 ---- batch: 050 ----
mean loss: 232.49
 ---- batch: 060 ----
mean loss: 239.27
 ---- batch: 070 ----
mean loss: 231.18
 ---- batch: 080 ----
mean loss: 232.44
 ---- batch: 090 ----
mean loss: 229.96
 ---- batch: 100 ----
mean loss: 235.03
 ---- batch: 110 ----
mean loss: 239.20
train mean loss: 235.28
epoch train time: 0:00:01.924899
elapsed time: 0:02:47.356277
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 23:15:29.371452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.09
 ---- batch: 020 ----
mean loss: 232.48
 ---- batch: 030 ----
mean loss: 240.89
 ---- batch: 040 ----
mean loss: 231.81
 ---- batch: 050 ----
mean loss: 225.58
 ---- batch: 060 ----
mean loss: 222.29
 ---- batch: 070 ----
mean loss: 235.27
 ---- batch: 080 ----
mean loss: 223.93
 ---- batch: 090 ----
mean loss: 231.30
 ---- batch: 100 ----
mean loss: 226.96
 ---- batch: 110 ----
mean loss: 237.15
train mean loss: 229.87
epoch train time: 0:00:01.925643
elapsed time: 0:02:49.282644
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 23:15:31.297762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.42
 ---- batch: 020 ----
mean loss: 221.94
 ---- batch: 030 ----
mean loss: 226.65
 ---- batch: 040 ----
mean loss: 228.99
 ---- batch: 050 ----
mean loss: 213.45
 ---- batch: 060 ----
mean loss: 227.64
 ---- batch: 070 ----
mean loss: 231.76
 ---- batch: 080 ----
mean loss: 238.15
 ---- batch: 090 ----
mean loss: 224.05
 ---- batch: 100 ----
mean loss: 232.59
 ---- batch: 110 ----
mean loss: 226.99
train mean loss: 227.33
epoch train time: 0:00:01.922934
elapsed time: 0:02:51.206236
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 23:15:33.221309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.82
 ---- batch: 020 ----
mean loss: 220.17
 ---- batch: 030 ----
mean loss: 227.57
 ---- batch: 040 ----
mean loss: 230.00
 ---- batch: 050 ----
mean loss: 231.41
 ---- batch: 060 ----
mean loss: 221.45
 ---- batch: 070 ----
mean loss: 218.75
 ---- batch: 080 ----
mean loss: 217.12
 ---- batch: 090 ----
mean loss: 221.92
 ---- batch: 100 ----
mean loss: 230.93
 ---- batch: 110 ----
mean loss: 220.59
train mean loss: 223.13
epoch train time: 0:00:01.939217
elapsed time: 0:02:53.146021
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 23:15:35.161158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.17
 ---- batch: 020 ----
mean loss: 226.12
 ---- batch: 030 ----
mean loss: 218.40
 ---- batch: 040 ----
mean loss: 216.92
 ---- batch: 050 ----
mean loss: 216.45
 ---- batch: 060 ----
mean loss: 226.48
 ---- batch: 070 ----
mean loss: 219.30
 ---- batch: 080 ----
mean loss: 222.48
 ---- batch: 090 ----
mean loss: 221.21
 ---- batch: 100 ----
mean loss: 220.79
 ---- batch: 110 ----
mean loss: 218.26
train mean loss: 220.84
epoch train time: 0:00:01.924274
elapsed time: 0:02:55.070939
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 23:15:37.086023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.48
 ---- batch: 020 ----
mean loss: 215.49
 ---- batch: 030 ----
mean loss: 211.30
 ---- batch: 040 ----
mean loss: 215.60
 ---- batch: 050 ----
mean loss: 211.01
 ---- batch: 060 ----
mean loss: 223.15
 ---- batch: 070 ----
mean loss: 218.04
 ---- batch: 080 ----
mean loss: 222.03
 ---- batch: 090 ----
mean loss: 216.02
 ---- batch: 100 ----
mean loss: 220.00
 ---- batch: 110 ----
mean loss: 211.88
train mean loss: 217.58
epoch train time: 0:00:01.932891
elapsed time: 0:02:57.004416
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 23:15:39.019539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.08
 ---- batch: 020 ----
mean loss: 211.23
 ---- batch: 030 ----
mean loss: 224.31
 ---- batch: 040 ----
mean loss: 217.14
 ---- batch: 050 ----
mean loss: 222.90
 ---- batch: 060 ----
mean loss: 210.73
 ---- batch: 070 ----
mean loss: 208.43
 ---- batch: 080 ----
mean loss: 216.71
 ---- batch: 090 ----
mean loss: 209.84
 ---- batch: 100 ----
mean loss: 212.07
 ---- batch: 110 ----
mean loss: 218.92
train mean loss: 214.98
epoch train time: 0:00:01.971723
elapsed time: 0:02:58.976784
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 23:15:40.991883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.83
 ---- batch: 020 ----
mean loss: 207.99
 ---- batch: 030 ----
mean loss: 216.93
 ---- batch: 040 ----
mean loss: 216.33
 ---- batch: 050 ----
mean loss: 213.70
 ---- batch: 060 ----
mean loss: 212.14
 ---- batch: 070 ----
mean loss: 216.37
 ---- batch: 080 ----
mean loss: 209.88
 ---- batch: 090 ----
mean loss: 210.68
 ---- batch: 100 ----
mean loss: 206.10
 ---- batch: 110 ----
mean loss: 214.53
train mean loss: 212.43
epoch train time: 0:00:01.952182
elapsed time: 0:03:00.929569
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 23:15:42.944689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.29
 ---- batch: 020 ----
mean loss: 216.89
 ---- batch: 030 ----
mean loss: 203.87
 ---- batch: 040 ----
mean loss: 204.89
 ---- batch: 050 ----
mean loss: 216.16
 ---- batch: 060 ----
mean loss: 208.11
 ---- batch: 070 ----
mean loss: 209.23
 ---- batch: 080 ----
mean loss: 206.84
 ---- batch: 090 ----
mean loss: 216.69
 ---- batch: 100 ----
mean loss: 209.00
 ---- batch: 110 ----
mean loss: 201.50
train mean loss: 209.22
epoch train time: 0:00:01.949706
elapsed time: 0:03:02.879896
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 23:15:44.894962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.25
 ---- batch: 020 ----
mean loss: 203.68
 ---- batch: 030 ----
mean loss: 199.32
 ---- batch: 040 ----
mean loss: 207.53
 ---- batch: 050 ----
mean loss: 204.39
 ---- batch: 060 ----
mean loss: 216.87
 ---- batch: 070 ----
mean loss: 211.11
 ---- batch: 080 ----
mean loss: 210.04
 ---- batch: 090 ----
mean loss: 205.42
 ---- batch: 100 ----
mean loss: 203.51
 ---- batch: 110 ----
mean loss: 205.80
train mean loss: 206.12
epoch train time: 0:00:01.919639
elapsed time: 0:03:04.800123
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 23:15:46.815204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.36
 ---- batch: 020 ----
mean loss: 205.72
 ---- batch: 030 ----
mean loss: 197.64
 ---- batch: 040 ----
mean loss: 209.54
 ---- batch: 050 ----
mean loss: 204.40
 ---- batch: 060 ----
mean loss: 203.48
 ---- batch: 070 ----
mean loss: 207.90
 ---- batch: 080 ----
mean loss: 203.09
 ---- batch: 090 ----
mean loss: 215.42
 ---- batch: 100 ----
mean loss: 195.74
 ---- batch: 110 ----
mean loss: 215.55
train mean loss: 204.93
epoch train time: 0:00:01.881840
elapsed time: 0:03:06.682634
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 23:15:48.697791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.66
 ---- batch: 020 ----
mean loss: 206.69
 ---- batch: 030 ----
mean loss: 201.71
 ---- batch: 040 ----
mean loss: 199.59
 ---- batch: 050 ----
mean loss: 204.97
 ---- batch: 060 ----
mean loss: 203.58
 ---- batch: 070 ----
mean loss: 207.46
 ---- batch: 080 ----
mean loss: 197.94
 ---- batch: 090 ----
mean loss: 204.57
 ---- batch: 100 ----
mean loss: 203.77
 ---- batch: 110 ----
mean loss: 196.92
train mean loss: 202.50
epoch train time: 0:00:01.932063
elapsed time: 0:03:08.615366
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 23:15:50.630487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.96
 ---- batch: 020 ----
mean loss: 197.54
 ---- batch: 030 ----
mean loss: 193.09
 ---- batch: 040 ----
mean loss: 200.64
 ---- batch: 050 ----
mean loss: 204.45
 ---- batch: 060 ----
mean loss: 195.40
 ---- batch: 070 ----
mean loss: 198.04
 ---- batch: 080 ----
mean loss: 217.25
 ---- batch: 090 ----
mean loss: 205.30
 ---- batch: 100 ----
mean loss: 186.16
 ---- batch: 110 ----
mean loss: 207.32
train mean loss: 199.80
epoch train time: 0:00:01.925755
elapsed time: 0:03:10.541770
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 23:15:52.556882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.51
 ---- batch: 020 ----
mean loss: 202.83
 ---- batch: 030 ----
mean loss: 197.74
 ---- batch: 040 ----
mean loss: 203.07
 ---- batch: 050 ----
mean loss: 190.57
 ---- batch: 060 ----
mean loss: 197.26
 ---- batch: 070 ----
mean loss: 200.31
 ---- batch: 080 ----
mean loss: 198.51
 ---- batch: 090 ----
mean loss: 199.76
 ---- batch: 100 ----
mean loss: 199.02
 ---- batch: 110 ----
mean loss: 194.41
train mean loss: 197.96
epoch train time: 0:00:01.947124
elapsed time: 0:03:12.489488
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 23:15:54.504632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.32
 ---- batch: 020 ----
mean loss: 199.53
 ---- batch: 030 ----
mean loss: 195.77
 ---- batch: 040 ----
mean loss: 186.92
 ---- batch: 050 ----
mean loss: 197.96
 ---- batch: 060 ----
mean loss: 194.98
 ---- batch: 070 ----
mean loss: 192.32
 ---- batch: 080 ----
mean loss: 191.60
 ---- batch: 090 ----
mean loss: 193.56
 ---- batch: 100 ----
mean loss: 201.07
 ---- batch: 110 ----
mean loss: 197.66
train mean loss: 194.83
epoch train time: 0:00:01.889582
elapsed time: 0:03:14.379703
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 23:15:56.394763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.23
 ---- batch: 020 ----
mean loss: 195.28
 ---- batch: 030 ----
mean loss: 191.42
 ---- batch: 040 ----
mean loss: 187.12
 ---- batch: 050 ----
mean loss: 186.82
 ---- batch: 060 ----
mean loss: 191.85
 ---- batch: 070 ----
mean loss: 206.50
 ---- batch: 080 ----
mean loss: 206.98
 ---- batch: 090 ----
mean loss: 194.12
 ---- batch: 100 ----
mean loss: 202.42
 ---- batch: 110 ----
mean loss: 188.15
train mean loss: 194.61
epoch train time: 0:00:01.949891
elapsed time: 0:03:16.330160
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 23:15:58.345280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.28
 ---- batch: 020 ----
mean loss: 200.87
 ---- batch: 030 ----
mean loss: 178.66
 ---- batch: 040 ----
mean loss: 195.44
 ---- batch: 050 ----
mean loss: 193.12
 ---- batch: 060 ----
mean loss: 194.54
 ---- batch: 070 ----
mean loss: 194.53
 ---- batch: 080 ----
mean loss: 200.42
 ---- batch: 090 ----
mean loss: 194.67
 ---- batch: 100 ----
mean loss: 193.62
 ---- batch: 110 ----
mean loss: 192.73
train mean loss: 192.77
epoch train time: 0:00:01.911469
elapsed time: 0:03:18.242246
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 23:16:00.257328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.21
 ---- batch: 020 ----
mean loss: 198.96
 ---- batch: 030 ----
mean loss: 188.04
 ---- batch: 040 ----
mean loss: 186.12
 ---- batch: 050 ----
mean loss: 197.24
 ---- batch: 060 ----
mean loss: 187.04
 ---- batch: 070 ----
mean loss: 190.89
 ---- batch: 080 ----
mean loss: 185.99
 ---- batch: 090 ----
mean loss: 197.21
 ---- batch: 100 ----
mean loss: 196.13
 ---- batch: 110 ----
mean loss: 197.98
train mean loss: 191.54
epoch train time: 0:00:01.912666
elapsed time: 0:03:20.155470
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 23:16:02.170548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.72
 ---- batch: 020 ----
mean loss: 192.01
 ---- batch: 030 ----
mean loss: 189.38
 ---- batch: 040 ----
mean loss: 180.45
 ---- batch: 050 ----
mean loss: 184.13
 ---- batch: 060 ----
mean loss: 181.47
 ---- batch: 070 ----
mean loss: 186.93
 ---- batch: 080 ----
mean loss: 201.04
 ---- batch: 090 ----
mean loss: 191.92
 ---- batch: 100 ----
mean loss: 178.53
 ---- batch: 110 ----
mean loss: 194.59
train mean loss: 188.04
epoch train time: 0:00:01.950893
elapsed time: 0:03:22.106962
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 23:16:04.122128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.92
 ---- batch: 020 ----
mean loss: 179.22
 ---- batch: 030 ----
mean loss: 188.17
 ---- batch: 040 ----
mean loss: 183.77
 ---- batch: 050 ----
mean loss: 189.74
 ---- batch: 060 ----
mean loss: 180.61
 ---- batch: 070 ----
mean loss: 194.85
 ---- batch: 080 ----
mean loss: 186.85
 ---- batch: 090 ----
mean loss: 191.87
 ---- batch: 100 ----
mean loss: 194.01
 ---- batch: 110 ----
mean loss: 187.40
train mean loss: 186.41
epoch train time: 0:00:01.965301
elapsed time: 0:03:24.072953
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 23:16:06.088056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.70
 ---- batch: 020 ----
mean loss: 182.14
 ---- batch: 030 ----
mean loss: 188.54
 ---- batch: 040 ----
mean loss: 193.76
 ---- batch: 050 ----
mean loss: 177.83
 ---- batch: 060 ----
mean loss: 182.75
 ---- batch: 070 ----
mean loss: 188.86
 ---- batch: 080 ----
mean loss: 189.90
 ---- batch: 090 ----
mean loss: 180.55
 ---- batch: 100 ----
mean loss: 187.65
 ---- batch: 110 ----
mean loss: 185.92
train mean loss: 185.19
epoch train time: 0:00:01.986969
elapsed time: 0:03:26.060573
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 23:16:08.075665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.49
 ---- batch: 020 ----
mean loss: 183.93
 ---- batch: 030 ----
mean loss: 181.21
 ---- batch: 040 ----
mean loss: 181.38
 ---- batch: 050 ----
mean loss: 175.43
 ---- batch: 060 ----
mean loss: 191.02
 ---- batch: 070 ----
mean loss: 193.03
 ---- batch: 080 ----
mean loss: 187.20
 ---- batch: 090 ----
mean loss: 177.76
 ---- batch: 100 ----
mean loss: 188.63
 ---- batch: 110 ----
mean loss: 188.38
train mean loss: 184.90
epoch train time: 0:00:01.969168
elapsed time: 0:03:28.030339
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 23:16:10.045426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.26
 ---- batch: 020 ----
mean loss: 188.68
 ---- batch: 030 ----
mean loss: 183.30
 ---- batch: 040 ----
mean loss: 177.45
 ---- batch: 050 ----
mean loss: 184.43
 ---- batch: 060 ----
mean loss: 189.78
 ---- batch: 070 ----
mean loss: 182.53
 ---- batch: 080 ----
mean loss: 181.50
 ---- batch: 090 ----
mean loss: 182.08
 ---- batch: 100 ----
mean loss: 181.98
 ---- batch: 110 ----
mean loss: 189.09
train mean loss: 183.79
epoch train time: 0:00:01.959947
elapsed time: 0:03:29.990875
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 23:16:12.005940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.65
 ---- batch: 020 ----
mean loss: 179.99
 ---- batch: 030 ----
mean loss: 175.79
 ---- batch: 040 ----
mean loss: 185.95
 ---- batch: 050 ----
mean loss: 174.38
 ---- batch: 060 ----
mean loss: 179.35
 ---- batch: 070 ----
mean loss: 185.21
 ---- batch: 080 ----
mean loss: 183.74
 ---- batch: 090 ----
mean loss: 176.82
 ---- batch: 100 ----
mean loss: 180.01
 ---- batch: 110 ----
mean loss: 188.61
train mean loss: 181.01
epoch train time: 0:00:01.916164
elapsed time: 0:03:31.907604
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 23:16:13.922673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.49
 ---- batch: 020 ----
mean loss: 188.53
 ---- batch: 030 ----
mean loss: 177.32
 ---- batch: 040 ----
mean loss: 184.01
 ---- batch: 050 ----
mean loss: 180.55
 ---- batch: 060 ----
mean loss: 168.04
 ---- batch: 070 ----
mean loss: 180.94
 ---- batch: 080 ----
mean loss: 170.37
 ---- batch: 090 ----
mean loss: 181.11
 ---- batch: 100 ----
mean loss: 185.10
 ---- batch: 110 ----
mean loss: 181.63
train mean loss: 179.79
epoch train time: 0:00:01.909865
elapsed time: 0:03:33.818073
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 23:16:15.833337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.79
 ---- batch: 020 ----
mean loss: 183.40
 ---- batch: 030 ----
mean loss: 179.30
 ---- batch: 040 ----
mean loss: 175.59
 ---- batch: 050 ----
mean loss: 178.10
 ---- batch: 060 ----
mean loss: 179.65
 ---- batch: 070 ----
mean loss: 183.70
 ---- batch: 080 ----
mean loss: 183.73
 ---- batch: 090 ----
mean loss: 174.44
 ---- batch: 100 ----
mean loss: 180.58
 ---- batch: 110 ----
mean loss: 174.28
train mean loss: 178.53
epoch train time: 0:00:01.930595
elapsed time: 0:03:35.749430
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 23:16:17.764440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.62
 ---- batch: 020 ----
mean loss: 181.12
 ---- batch: 030 ----
mean loss: 184.90
 ---- batch: 040 ----
mean loss: 175.43
 ---- batch: 050 ----
mean loss: 180.14
 ---- batch: 060 ----
mean loss: 173.57
 ---- batch: 070 ----
mean loss: 176.98
 ---- batch: 080 ----
mean loss: 179.74
 ---- batch: 090 ----
mean loss: 176.51
 ---- batch: 100 ----
mean loss: 175.08
 ---- batch: 110 ----
mean loss: 175.89
train mean loss: 177.77
epoch train time: 0:00:01.909857
elapsed time: 0:03:37.659821
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 23:16:19.674932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.67
 ---- batch: 020 ----
mean loss: 174.59
 ---- batch: 030 ----
mean loss: 174.78
 ---- batch: 040 ----
mean loss: 185.60
 ---- batch: 050 ----
mean loss: 183.02
 ---- batch: 060 ----
mean loss: 172.63
 ---- batch: 070 ----
mean loss: 169.19
 ---- batch: 080 ----
mean loss: 171.15
 ---- batch: 090 ----
mean loss: 179.96
 ---- batch: 100 ----
mean loss: 176.83
 ---- batch: 110 ----
mean loss: 175.68
train mean loss: 176.59
epoch train time: 0:00:01.900133
elapsed time: 0:03:39.560568
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 23:16:21.575687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.66
 ---- batch: 020 ----
mean loss: 167.10
 ---- batch: 030 ----
mean loss: 157.82
 ---- batch: 040 ----
mean loss: 177.40
 ---- batch: 050 ----
mean loss: 187.27
 ---- batch: 060 ----
mean loss: 177.82
 ---- batch: 070 ----
mean loss: 178.59
 ---- batch: 080 ----
mean loss: 171.86
 ---- batch: 090 ----
mean loss: 173.57
 ---- batch: 100 ----
mean loss: 175.76
 ---- batch: 110 ----
mean loss: 181.18
train mean loss: 175.15
epoch train time: 0:00:01.901103
elapsed time: 0:03:41.462293
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 23:16:23.477086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.07
 ---- batch: 020 ----
mean loss: 171.93
 ---- batch: 030 ----
mean loss: 170.72
 ---- batch: 040 ----
mean loss: 170.55
 ---- batch: 050 ----
mean loss: 184.83
 ---- batch: 060 ----
mean loss: 170.72
 ---- batch: 070 ----
mean loss: 166.89
 ---- batch: 080 ----
mean loss: 176.92
 ---- batch: 090 ----
mean loss: 181.02
 ---- batch: 100 ----
mean loss: 175.94
 ---- batch: 110 ----
mean loss: 173.89
train mean loss: 173.72
epoch train time: 0:00:01.955234
elapsed time: 0:03:43.417831
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 23:16:25.432971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.38
 ---- batch: 020 ----
mean loss: 172.28
 ---- batch: 030 ----
mean loss: 172.17
 ---- batch: 040 ----
mean loss: 166.98
 ---- batch: 050 ----
mean loss: 177.87
 ---- batch: 060 ----
mean loss: 178.86
 ---- batch: 070 ----
mean loss: 174.98
 ---- batch: 080 ----
mean loss: 182.04
 ---- batch: 090 ----
mean loss: 173.02
 ---- batch: 100 ----
mean loss: 172.20
 ---- batch: 110 ----
mean loss: 169.78
train mean loss: 172.92
epoch train time: 0:00:01.967021
elapsed time: 0:03:45.385504
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 23:16:27.400583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.99
 ---- batch: 020 ----
mean loss: 177.87
 ---- batch: 030 ----
mean loss: 161.67
 ---- batch: 040 ----
mean loss: 176.55
 ---- batch: 050 ----
mean loss: 166.16
 ---- batch: 060 ----
mean loss: 170.37
 ---- batch: 070 ----
mean loss: 163.26
 ---- batch: 080 ----
mean loss: 163.62
 ---- batch: 090 ----
mean loss: 171.44
 ---- batch: 100 ----
mean loss: 178.62
 ---- batch: 110 ----
mean loss: 183.48
train mean loss: 171.63
epoch train time: 0:00:01.965965
elapsed time: 0:03:47.352064
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 23:16:29.367136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.30
 ---- batch: 020 ----
mean loss: 171.08
 ---- batch: 030 ----
mean loss: 172.55
 ---- batch: 040 ----
mean loss: 165.08
 ---- batch: 050 ----
mean loss: 171.53
 ---- batch: 060 ----
mean loss: 179.26
 ---- batch: 070 ----
mean loss: 171.36
 ---- batch: 080 ----
mean loss: 167.95
 ---- batch: 090 ----
mean loss: 166.57
 ---- batch: 100 ----
mean loss: 177.40
 ---- batch: 110 ----
mean loss: 173.49
train mean loss: 170.87
epoch train time: 0:00:01.911978
elapsed time: 0:03:49.264593
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 23:16:31.279671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.59
 ---- batch: 020 ----
mean loss: 170.25
 ---- batch: 030 ----
mean loss: 170.19
 ---- batch: 040 ----
mean loss: 166.05
 ---- batch: 050 ----
mean loss: 176.94
 ---- batch: 060 ----
mean loss: 163.81
 ---- batch: 070 ----
mean loss: 171.45
 ---- batch: 080 ----
mean loss: 169.99
 ---- batch: 090 ----
mean loss: 170.86
 ---- batch: 100 ----
mean loss: 165.51
 ---- batch: 110 ----
mean loss: 173.20
train mean loss: 170.51
epoch train time: 0:00:01.886124
elapsed time: 0:03:51.151278
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 23:16:33.166348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.65
 ---- batch: 020 ----
mean loss: 172.88
 ---- batch: 030 ----
mean loss: 169.55
 ---- batch: 040 ----
mean loss: 170.55
 ---- batch: 050 ----
mean loss: 163.10
 ---- batch: 060 ----
mean loss: 169.85
 ---- batch: 070 ----
mean loss: 167.01
 ---- batch: 080 ----
mean loss: 172.28
 ---- batch: 090 ----
mean loss: 165.72
 ---- batch: 100 ----
mean loss: 172.73
 ---- batch: 110 ----
mean loss: 167.82
train mean loss: 169.62
epoch train time: 0:00:01.905228
elapsed time: 0:03:53.057076
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 23:16:35.072192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.84
 ---- batch: 020 ----
mean loss: 161.51
 ---- batch: 030 ----
mean loss: 167.39
 ---- batch: 040 ----
mean loss: 167.26
 ---- batch: 050 ----
mean loss: 164.34
 ---- batch: 060 ----
mean loss: 173.18
 ---- batch: 070 ----
mean loss: 166.84
 ---- batch: 080 ----
mean loss: 165.20
 ---- batch: 090 ----
mean loss: 162.57
 ---- batch: 100 ----
mean loss: 173.78
 ---- batch: 110 ----
mean loss: 174.11
train mean loss: 168.02
epoch train time: 0:00:01.906987
elapsed time: 0:03:54.964654
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 23:16:36.979701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.23
 ---- batch: 020 ----
mean loss: 168.99
 ---- batch: 030 ----
mean loss: 169.06
 ---- batch: 040 ----
mean loss: 167.10
 ---- batch: 050 ----
mean loss: 169.42
 ---- batch: 060 ----
mean loss: 164.50
 ---- batch: 070 ----
mean loss: 166.10
 ---- batch: 080 ----
mean loss: 163.54
 ---- batch: 090 ----
mean loss: 165.36
 ---- batch: 100 ----
mean loss: 176.40
 ---- batch: 110 ----
mean loss: 180.11
train mean loss: 167.80
epoch train time: 0:00:01.904287
elapsed time: 0:03:56.869472
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 23:16:38.884549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.41
 ---- batch: 020 ----
mean loss: 162.93
 ---- batch: 030 ----
mean loss: 168.58
 ---- batch: 040 ----
mean loss: 163.21
 ---- batch: 050 ----
mean loss: 160.07
 ---- batch: 060 ----
mean loss: 169.56
 ---- batch: 070 ----
mean loss: 167.11
 ---- batch: 080 ----
mean loss: 163.23
 ---- batch: 090 ----
mean loss: 170.01
 ---- batch: 100 ----
mean loss: 174.94
 ---- batch: 110 ----
mean loss: 168.35
train mean loss: 167.25
epoch train time: 0:00:01.890898
elapsed time: 0:03:58.761191
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 23:16:40.776064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.00
 ---- batch: 020 ----
mean loss: 169.63
 ---- batch: 030 ----
mean loss: 161.50
 ---- batch: 040 ----
mean loss: 162.71
 ---- batch: 050 ----
mean loss: 164.90
 ---- batch: 060 ----
mean loss: 162.53
 ---- batch: 070 ----
mean loss: 164.26
 ---- batch: 080 ----
mean loss: 172.28
 ---- batch: 090 ----
mean loss: 170.33
 ---- batch: 100 ----
mean loss: 161.31
 ---- batch: 110 ----
mean loss: 167.88
train mean loss: 165.43
epoch train time: 0:00:01.943286
elapsed time: 0:04:00.704883
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 23:16:42.720027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.03
 ---- batch: 020 ----
mean loss: 168.86
 ---- batch: 030 ----
mean loss: 160.72
 ---- batch: 040 ----
mean loss: 159.38
 ---- batch: 050 ----
mean loss: 170.43
 ---- batch: 060 ----
mean loss: 161.84
 ---- batch: 070 ----
mean loss: 174.77
 ---- batch: 080 ----
mean loss: 168.70
 ---- batch: 090 ----
mean loss: 169.10
 ---- batch: 100 ----
mean loss: 157.23
 ---- batch: 110 ----
mean loss: 166.72
train mean loss: 165.29
epoch train time: 0:00:01.937107
elapsed time: 0:04:02.642634
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 23:16:44.657767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.02
 ---- batch: 020 ----
mean loss: 167.60
 ---- batch: 030 ----
mean loss: 158.88
 ---- batch: 040 ----
mean loss: 157.33
 ---- batch: 050 ----
mean loss: 164.74
 ---- batch: 060 ----
mean loss: 162.72
 ---- batch: 070 ----
mean loss: 168.63
 ---- batch: 080 ----
mean loss: 163.14
 ---- batch: 090 ----
mean loss: 171.61
 ---- batch: 100 ----
mean loss: 168.64
 ---- batch: 110 ----
mean loss: 162.04
train mean loss: 164.15
epoch train time: 0:00:01.964746
elapsed time: 0:04:04.608019
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 23:16:46.623178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.40
 ---- batch: 020 ----
mean loss: 156.54
 ---- batch: 030 ----
mean loss: 164.13
 ---- batch: 040 ----
mean loss: 157.40
 ---- batch: 050 ----
mean loss: 168.75
 ---- batch: 060 ----
mean loss: 164.17
 ---- batch: 070 ----
mean loss: 169.87
 ---- batch: 080 ----
mean loss: 164.15
 ---- batch: 090 ----
mean loss: 155.40
 ---- batch: 100 ----
mean loss: 159.16
 ---- batch: 110 ----
mean loss: 169.78
train mean loss: 162.72
epoch train time: 0:00:01.927865
elapsed time: 0:04:06.536525
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 23:16:48.551614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.07
 ---- batch: 020 ----
mean loss: 159.72
 ---- batch: 030 ----
mean loss: 159.60
 ---- batch: 040 ----
mean loss: 151.43
 ---- batch: 050 ----
mean loss: 168.92
 ---- batch: 060 ----
mean loss: 160.43
 ---- batch: 070 ----
mean loss: 166.56
 ---- batch: 080 ----
mean loss: 166.67
 ---- batch: 090 ----
mean loss: 160.53
 ---- batch: 100 ----
mean loss: 162.58
 ---- batch: 110 ----
mean loss: 165.91
train mean loss: 162.86
epoch train time: 0:00:01.921896
elapsed time: 0:04:08.459023
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 23:16:50.474102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.09
 ---- batch: 020 ----
mean loss: 155.00
 ---- batch: 030 ----
mean loss: 160.63
 ---- batch: 040 ----
mean loss: 164.33
 ---- batch: 050 ----
mean loss: 158.42
 ---- batch: 060 ----
mean loss: 164.62
 ---- batch: 070 ----
mean loss: 161.97
 ---- batch: 080 ----
mean loss: 169.13
 ---- batch: 090 ----
mean loss: 169.10
 ---- batch: 100 ----
mean loss: 153.61
 ---- batch: 110 ----
mean loss: 163.87
train mean loss: 162.21
epoch train time: 0:00:01.935389
elapsed time: 0:04:10.395026
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 23:16:52.410089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.83
 ---- batch: 020 ----
mean loss: 151.75
 ---- batch: 030 ----
mean loss: 158.62
 ---- batch: 040 ----
mean loss: 151.89
 ---- batch: 050 ----
mean loss: 162.69
 ---- batch: 060 ----
mean loss: 164.63
 ---- batch: 070 ----
mean loss: 153.43
 ---- batch: 080 ----
mean loss: 166.97
 ---- batch: 090 ----
mean loss: 164.76
 ---- batch: 100 ----
mean loss: 165.47
 ---- batch: 110 ----
mean loss: 168.80
train mean loss: 161.75
epoch train time: 0:00:01.909525
elapsed time: 0:04:12.305180
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 23:16:54.320250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.18
 ---- batch: 020 ----
mean loss: 162.63
 ---- batch: 030 ----
mean loss: 159.13
 ---- batch: 040 ----
mean loss: 162.12
 ---- batch: 050 ----
mean loss: 163.63
 ---- batch: 060 ----
mean loss: 172.24
 ---- batch: 070 ----
mean loss: 155.20
 ---- batch: 080 ----
mean loss: 155.97
 ---- batch: 090 ----
mean loss: 157.50
 ---- batch: 100 ----
mean loss: 160.50
 ---- batch: 110 ----
mean loss: 162.26
train mean loss: 160.67
epoch train time: 0:00:01.968193
elapsed time: 0:04:14.273944
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 23:16:56.289054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.53
 ---- batch: 020 ----
mean loss: 163.60
 ---- batch: 030 ----
mean loss: 161.29
 ---- batch: 040 ----
mean loss: 155.79
 ---- batch: 050 ----
mean loss: 160.21
 ---- batch: 060 ----
mean loss: 159.51
 ---- batch: 070 ----
mean loss: 163.47
 ---- batch: 080 ----
mean loss: 166.25
 ---- batch: 090 ----
mean loss: 160.04
 ---- batch: 100 ----
mean loss: 151.49
 ---- batch: 110 ----
mean loss: 150.20
train mean loss: 159.79
epoch train time: 0:00:01.893769
elapsed time: 0:04:16.168306
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 23:16:58.183396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.71
 ---- batch: 020 ----
mean loss: 157.69
 ---- batch: 030 ----
mean loss: 160.06
 ---- batch: 040 ----
mean loss: 162.16
 ---- batch: 050 ----
mean loss: 156.55
 ---- batch: 060 ----
mean loss: 161.77
 ---- batch: 070 ----
mean loss: 162.03
 ---- batch: 080 ----
mean loss: 168.55
 ---- batch: 090 ----
mean loss: 156.71
 ---- batch: 100 ----
mean loss: 168.81
 ---- batch: 110 ----
mean loss: 152.63
train mean loss: 160.07
epoch train time: 0:00:01.906112
elapsed time: 0:04:18.075006
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 23:17:00.090072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.79
 ---- batch: 020 ----
mean loss: 160.25
 ---- batch: 030 ----
mean loss: 159.08
 ---- batch: 040 ----
mean loss: 154.37
 ---- batch: 050 ----
mean loss: 156.71
 ---- batch: 060 ----
mean loss: 164.34
 ---- batch: 070 ----
mean loss: 164.58
 ---- batch: 080 ----
mean loss: 162.66
 ---- batch: 090 ----
mean loss: 156.23
 ---- batch: 100 ----
mean loss: 159.97
 ---- batch: 110 ----
mean loss: 154.96
train mean loss: 159.30
epoch train time: 0:00:01.915375
elapsed time: 0:04:19.991262
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 23:17:02.006409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.80
 ---- batch: 020 ----
mean loss: 150.24
 ---- batch: 030 ----
mean loss: 154.64
 ---- batch: 040 ----
mean loss: 154.86
 ---- batch: 050 ----
mean loss: 157.29
 ---- batch: 060 ----
mean loss: 157.13
 ---- batch: 070 ----
mean loss: 161.91
 ---- batch: 080 ----
mean loss: 161.59
 ---- batch: 090 ----
mean loss: 159.37
 ---- batch: 100 ----
mean loss: 160.95
 ---- batch: 110 ----
mean loss: 162.90
train mean loss: 158.08
epoch train time: 0:00:01.907076
elapsed time: 0:04:21.899054
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 23:17:03.914150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.03
 ---- batch: 020 ----
mean loss: 155.46
 ---- batch: 030 ----
mean loss: 147.21
 ---- batch: 040 ----
mean loss: 156.10
 ---- batch: 050 ----
mean loss: 159.42
 ---- batch: 060 ----
mean loss: 158.51
 ---- batch: 070 ----
mean loss: 158.46
 ---- batch: 080 ----
mean loss: 160.80
 ---- batch: 090 ----
mean loss: 165.95
 ---- batch: 100 ----
mean loss: 159.30
 ---- batch: 110 ----
mean loss: 160.81
train mean loss: 158.11
epoch train time: 0:00:01.906480
elapsed time: 0:04:23.806136
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 23:17:05.821204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.21
 ---- batch: 020 ----
mean loss: 144.41
 ---- batch: 030 ----
mean loss: 160.58
 ---- batch: 040 ----
mean loss: 153.61
 ---- batch: 050 ----
mean loss: 150.48
 ---- batch: 060 ----
mean loss: 154.36
 ---- batch: 070 ----
mean loss: 165.29
 ---- batch: 080 ----
mean loss: 151.27
 ---- batch: 090 ----
mean loss: 164.88
 ---- batch: 100 ----
mean loss: 157.63
 ---- batch: 110 ----
mean loss: 157.71
train mean loss: 156.27
epoch train time: 0:00:01.918483
elapsed time: 0:04:25.725241
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 23:17:07.740297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.41
 ---- batch: 020 ----
mean loss: 152.32
 ---- batch: 030 ----
mean loss: 159.11
 ---- batch: 040 ----
mean loss: 154.53
 ---- batch: 050 ----
mean loss: 148.46
 ---- batch: 060 ----
mean loss: 157.61
 ---- batch: 070 ----
mean loss: 162.62
 ---- batch: 080 ----
mean loss: 159.31
 ---- batch: 090 ----
mean loss: 155.07
 ---- batch: 100 ----
mean loss: 161.35
 ---- batch: 110 ----
mean loss: 157.76
train mean loss: 156.77
epoch train time: 0:00:01.895579
elapsed time: 0:04:27.621382
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 23:17:09.636479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.73
 ---- batch: 020 ----
mean loss: 161.18
 ---- batch: 030 ----
mean loss: 146.30
 ---- batch: 040 ----
mean loss: 152.33
 ---- batch: 050 ----
mean loss: 157.53
 ---- batch: 060 ----
mean loss: 159.89
 ---- batch: 070 ----
mean loss: 162.76
 ---- batch: 080 ----
mean loss: 158.06
 ---- batch: 090 ----
mean loss: 154.99
 ---- batch: 100 ----
mean loss: 158.34
 ---- batch: 110 ----
mean loss: 152.66
train mean loss: 156.23
epoch train time: 0:00:01.944724
elapsed time: 0:04:29.566754
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 23:17:11.581857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.38
 ---- batch: 020 ----
mean loss: 146.06
 ---- batch: 030 ----
mean loss: 153.16
 ---- batch: 040 ----
mean loss: 159.57
 ---- batch: 050 ----
mean loss: 158.48
 ---- batch: 060 ----
mean loss: 157.31
 ---- batch: 070 ----
mean loss: 153.46
 ---- batch: 080 ----
mean loss: 159.51
 ---- batch: 090 ----
mean loss: 158.70
 ---- batch: 100 ----
mean loss: 158.64
 ---- batch: 110 ----
mean loss: 148.56
train mean loss: 155.16
epoch train time: 0:00:01.939604
elapsed time: 0:04:31.506975
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 23:17:13.522066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.16
 ---- batch: 020 ----
mean loss: 142.19
 ---- batch: 030 ----
mean loss: 155.74
 ---- batch: 040 ----
mean loss: 155.99
 ---- batch: 050 ----
mean loss: 160.69
 ---- batch: 060 ----
mean loss: 165.08
 ---- batch: 070 ----
mean loss: 164.40
 ---- batch: 080 ----
mean loss: 152.37
 ---- batch: 090 ----
mean loss: 153.78
 ---- batch: 100 ----
mean loss: 150.94
 ---- batch: 110 ----
mean loss: 150.20
train mean loss: 155.21
epoch train time: 0:00:01.928806
elapsed time: 0:04:33.436374
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 23:17:15.451460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.05
 ---- batch: 020 ----
mean loss: 149.69
 ---- batch: 030 ----
mean loss: 156.97
 ---- batch: 040 ----
mean loss: 157.36
 ---- batch: 050 ----
mean loss: 144.96
 ---- batch: 060 ----
mean loss: 153.34
 ---- batch: 070 ----
mean loss: 160.16
 ---- batch: 080 ----
mean loss: 159.12
 ---- batch: 090 ----
mean loss: 153.52
 ---- batch: 100 ----
mean loss: 155.18
 ---- batch: 110 ----
mean loss: 159.02
train mean loss: 154.57
epoch train time: 0:00:01.935191
elapsed time: 0:04:35.372127
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 23:17:17.387206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.84
 ---- batch: 020 ----
mean loss: 147.38
 ---- batch: 030 ----
mean loss: 153.61
 ---- batch: 040 ----
mean loss: 156.10
 ---- batch: 050 ----
mean loss: 152.09
 ---- batch: 060 ----
mean loss: 152.50
 ---- batch: 070 ----
mean loss: 153.46
 ---- batch: 080 ----
mean loss: 159.20
 ---- batch: 090 ----
mean loss: 155.48
 ---- batch: 100 ----
mean loss: 151.25
 ---- batch: 110 ----
mean loss: 153.11
train mean loss: 153.48
epoch train time: 0:00:01.914689
elapsed time: 0:04:37.287620
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 23:17:19.302473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.38
 ---- batch: 020 ----
mean loss: 151.16
 ---- batch: 030 ----
mean loss: 150.12
 ---- batch: 040 ----
mean loss: 136.39
 ---- batch: 050 ----
mean loss: 164.47
 ---- batch: 060 ----
mean loss: 157.11
 ---- batch: 070 ----
mean loss: 154.57
 ---- batch: 080 ----
mean loss: 156.04
 ---- batch: 090 ----
mean loss: 158.47
 ---- batch: 100 ----
mean loss: 150.11
 ---- batch: 110 ----
mean loss: 154.73
train mean loss: 153.17
epoch train time: 0:00:01.940608
elapsed time: 0:04:39.228577
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 23:17:21.243645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.57
 ---- batch: 020 ----
mean loss: 140.78
 ---- batch: 030 ----
mean loss: 153.31
 ---- batch: 040 ----
mean loss: 149.65
 ---- batch: 050 ----
mean loss: 154.87
 ---- batch: 060 ----
mean loss: 154.54
 ---- batch: 070 ----
mean loss: 155.76
 ---- batch: 080 ----
mean loss: 158.32
 ---- batch: 090 ----
mean loss: 149.77
 ---- batch: 100 ----
mean loss: 162.21
 ---- batch: 110 ----
mean loss: 146.24
train mean loss: 153.02
epoch train time: 0:00:01.913618
elapsed time: 0:04:41.142742
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 23:17:23.157825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.95
 ---- batch: 020 ----
mean loss: 157.99
 ---- batch: 030 ----
mean loss: 149.94
 ---- batch: 040 ----
mean loss: 149.29
 ---- batch: 050 ----
mean loss: 154.95
 ---- batch: 060 ----
mean loss: 150.17
 ---- batch: 070 ----
mean loss: 143.13
 ---- batch: 080 ----
mean loss: 151.76
 ---- batch: 090 ----
mean loss: 154.36
 ---- batch: 100 ----
mean loss: 152.96
 ---- batch: 110 ----
mean loss: 157.16
train mean loss: 152.34
epoch train time: 0:00:01.909173
elapsed time: 0:04:43.052531
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 23:17:25.067616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.29
 ---- batch: 020 ----
mean loss: 147.68
 ---- batch: 030 ----
mean loss: 152.89
 ---- batch: 040 ----
mean loss: 158.52
 ---- batch: 050 ----
mean loss: 144.55
 ---- batch: 060 ----
mean loss: 147.13
 ---- batch: 070 ----
mean loss: 157.70
 ---- batch: 080 ----
mean loss: 153.62
 ---- batch: 090 ----
mean loss: 154.00
 ---- batch: 100 ----
mean loss: 147.35
 ---- batch: 110 ----
mean loss: 153.26
train mean loss: 151.60
epoch train time: 0:00:01.920125
elapsed time: 0:04:44.973231
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 23:17:26.988326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.48
 ---- batch: 020 ----
mean loss: 149.29
 ---- batch: 030 ----
mean loss: 150.68
 ---- batch: 040 ----
mean loss: 153.53
 ---- batch: 050 ----
mean loss: 152.03
 ---- batch: 060 ----
mean loss: 153.96
 ---- batch: 070 ----
mean loss: 152.94
 ---- batch: 080 ----
mean loss: 144.90
 ---- batch: 090 ----
mean loss: 147.77
 ---- batch: 100 ----
mean loss: 159.15
 ---- batch: 110 ----
mean loss: 156.59
train mean loss: 151.98
epoch train time: 0:00:01.933221
elapsed time: 0:04:46.907051
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 23:17:28.922112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.30
 ---- batch: 020 ----
mean loss: 151.65
 ---- batch: 030 ----
mean loss: 143.88
 ---- batch: 040 ----
mean loss: 148.90
 ---- batch: 050 ----
mean loss: 149.31
 ---- batch: 060 ----
mean loss: 154.92
 ---- batch: 070 ----
mean loss: 157.74
 ---- batch: 080 ----
mean loss: 142.55
 ---- batch: 090 ----
mean loss: 144.34
 ---- batch: 100 ----
mean loss: 149.21
 ---- batch: 110 ----
mean loss: 148.29
train mean loss: 149.58
epoch train time: 0:00:01.897624
elapsed time: 0:04:48.805263
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 23:17:30.820375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.55
 ---- batch: 020 ----
mean loss: 147.56
 ---- batch: 030 ----
mean loss: 146.47
 ---- batch: 040 ----
mean loss: 145.95
 ---- batch: 050 ----
mean loss: 150.60
 ---- batch: 060 ----
mean loss: 146.76
 ---- batch: 070 ----
mean loss: 147.83
 ---- batch: 080 ----
mean loss: 148.08
 ---- batch: 090 ----
mean loss: 163.03
 ---- batch: 100 ----
mean loss: 141.46
 ---- batch: 110 ----
mean loss: 154.01
train mean loss: 150.49
epoch train time: 0:00:01.921495
elapsed time: 0:04:50.727394
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 23:17:32.742482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.21
 ---- batch: 020 ----
mean loss: 150.19
 ---- batch: 030 ----
mean loss: 156.18
 ---- batch: 040 ----
mean loss: 150.14
 ---- batch: 050 ----
mean loss: 153.87
 ---- batch: 060 ----
mean loss: 148.07
 ---- batch: 070 ----
mean loss: 148.33
 ---- batch: 080 ----
mean loss: 147.36
 ---- batch: 090 ----
mean loss: 144.54
 ---- batch: 100 ----
mean loss: 156.92
 ---- batch: 110 ----
mean loss: 147.60
train mean loss: 150.12
epoch train time: 0:00:01.915622
elapsed time: 0:04:52.643602
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 23:17:34.658691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.92
 ---- batch: 020 ----
mean loss: 148.93
 ---- batch: 030 ----
mean loss: 151.76
 ---- batch: 040 ----
mean loss: 154.18
 ---- batch: 050 ----
mean loss: 143.44
 ---- batch: 060 ----
mean loss: 153.94
 ---- batch: 070 ----
mean loss: 149.28
 ---- batch: 080 ----
mean loss: 150.61
 ---- batch: 090 ----
mean loss: 152.36
 ---- batch: 100 ----
mean loss: 143.06
 ---- batch: 110 ----
mean loss: 156.31
train mean loss: 150.20
epoch train time: 0:00:01.933252
elapsed time: 0:04:54.577448
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 23:17:36.592533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.25
 ---- batch: 020 ----
mean loss: 149.41
 ---- batch: 030 ----
mean loss: 152.30
 ---- batch: 040 ----
mean loss: 144.70
 ---- batch: 050 ----
mean loss: 143.20
 ---- batch: 060 ----
mean loss: 146.43
 ---- batch: 070 ----
mean loss: 153.33
 ---- batch: 080 ----
mean loss: 148.20
 ---- batch: 090 ----
mean loss: 151.02
 ---- batch: 100 ----
mean loss: 148.00
 ---- batch: 110 ----
mean loss: 142.50
train mean loss: 148.90
epoch train time: 0:00:01.934478
elapsed time: 0:04:56.512482
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 23:17:38.527571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.88
 ---- batch: 020 ----
mean loss: 146.88
 ---- batch: 030 ----
mean loss: 146.39
 ---- batch: 040 ----
mean loss: 149.19
 ---- batch: 050 ----
mean loss: 150.09
 ---- batch: 060 ----
mean loss: 150.38
 ---- batch: 070 ----
mean loss: 145.72
 ---- batch: 080 ----
mean loss: 147.15
 ---- batch: 090 ----
mean loss: 145.00
 ---- batch: 100 ----
mean loss: 155.77
 ---- batch: 110 ----
mean loss: 156.24
train mean loss: 147.97
epoch train time: 0:00:01.919291
elapsed time: 0:04:58.432368
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 23:17:40.447426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.15
 ---- batch: 020 ----
mean loss: 154.52
 ---- batch: 030 ----
mean loss: 139.24
 ---- batch: 040 ----
mean loss: 152.95
 ---- batch: 050 ----
mean loss: 145.07
 ---- batch: 060 ----
mean loss: 153.20
 ---- batch: 070 ----
mean loss: 146.06
 ---- batch: 080 ----
mean loss: 145.62
 ---- batch: 090 ----
mean loss: 141.34
 ---- batch: 100 ----
mean loss: 152.62
 ---- batch: 110 ----
mean loss: 146.14
train mean loss: 148.01
epoch train time: 0:00:01.909804
elapsed time: 0:05:00.342726
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 23:17:42.357810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.36
 ---- batch: 020 ----
mean loss: 147.86
 ---- batch: 030 ----
mean loss: 150.00
 ---- batch: 040 ----
mean loss: 156.89
 ---- batch: 050 ----
mean loss: 149.43
 ---- batch: 060 ----
mean loss: 153.55
 ---- batch: 070 ----
mean loss: 145.18
 ---- batch: 080 ----
mean loss: 143.58
 ---- batch: 090 ----
mean loss: 152.43
 ---- batch: 100 ----
mean loss: 136.56
 ---- batch: 110 ----
mean loss: 150.51
train mean loss: 147.97
epoch train time: 0:00:01.917565
elapsed time: 0:05:02.260899
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 23:17:44.275999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.39
 ---- batch: 020 ----
mean loss: 142.77
 ---- batch: 030 ----
mean loss: 151.43
 ---- batch: 040 ----
mean loss: 155.43
 ---- batch: 050 ----
mean loss: 147.20
 ---- batch: 060 ----
mean loss: 140.37
 ---- batch: 070 ----
mean loss: 144.89
 ---- batch: 080 ----
mean loss: 150.59
 ---- batch: 090 ----
mean loss: 157.67
 ---- batch: 100 ----
mean loss: 143.34
 ---- batch: 110 ----
mean loss: 146.28
train mean loss: 147.15
epoch train time: 0:00:01.941918
elapsed time: 0:05:04.203533
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 23:17:46.218747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.21
 ---- batch: 020 ----
mean loss: 150.97
 ---- batch: 030 ----
mean loss: 143.93
 ---- batch: 040 ----
mean loss: 142.57
 ---- batch: 050 ----
mean loss: 147.55
 ---- batch: 060 ----
mean loss: 147.76
 ---- batch: 070 ----
mean loss: 142.28
 ---- batch: 080 ----
mean loss: 148.47
 ---- batch: 090 ----
mean loss: 147.44
 ---- batch: 100 ----
mean loss: 146.78
 ---- batch: 110 ----
mean loss: 143.66
train mean loss: 146.91
epoch train time: 0:00:01.963351
elapsed time: 0:05:06.167583
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 23:17:48.182697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.67
 ---- batch: 020 ----
mean loss: 149.43
 ---- batch: 030 ----
mean loss: 142.43
 ---- batch: 040 ----
mean loss: 145.40
 ---- batch: 050 ----
mean loss: 138.28
 ---- batch: 060 ----
mean loss: 147.60
 ---- batch: 070 ----
mean loss: 144.94
 ---- batch: 080 ----
mean loss: 150.58
 ---- batch: 090 ----
mean loss: 144.13
 ---- batch: 100 ----
mean loss: 153.02
 ---- batch: 110 ----
mean loss: 157.36
train mean loss: 146.93
epoch train time: 0:00:01.928190
elapsed time: 0:05:08.096384
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 23:17:50.111470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.80
 ---- batch: 020 ----
mean loss: 138.12
 ---- batch: 030 ----
mean loss: 139.81
 ---- batch: 040 ----
mean loss: 146.09
 ---- batch: 050 ----
mean loss: 150.60
 ---- batch: 060 ----
mean loss: 141.50
 ---- batch: 070 ----
mean loss: 147.95
 ---- batch: 080 ----
mean loss: 151.29
 ---- batch: 090 ----
mean loss: 156.97
 ---- batch: 100 ----
mean loss: 149.98
 ---- batch: 110 ----
mean loss: 139.01
train mean loss: 146.04
epoch train time: 0:00:01.900121
elapsed time: 0:05:09.997091
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 23:17:52.012170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.72
 ---- batch: 020 ----
mean loss: 139.59
 ---- batch: 030 ----
mean loss: 148.93
 ---- batch: 040 ----
mean loss: 151.94
 ---- batch: 050 ----
mean loss: 140.83
 ---- batch: 060 ----
mean loss: 148.63
 ---- batch: 070 ----
mean loss: 143.86
 ---- batch: 080 ----
mean loss: 148.73
 ---- batch: 090 ----
mean loss: 147.70
 ---- batch: 100 ----
mean loss: 145.46
 ---- batch: 110 ----
mean loss: 150.95
train mean loss: 145.93
epoch train time: 0:00:01.924499
elapsed time: 0:05:11.922178
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 23:17:53.937256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.79
 ---- batch: 020 ----
mean loss: 146.34
 ---- batch: 030 ----
mean loss: 144.33
 ---- batch: 040 ----
mean loss: 141.31
 ---- batch: 050 ----
mean loss: 152.57
 ---- batch: 060 ----
mean loss: 147.64
 ---- batch: 070 ----
mean loss: 147.76
 ---- batch: 080 ----
mean loss: 148.91
 ---- batch: 090 ----
mean loss: 145.24
 ---- batch: 100 ----
mean loss: 142.43
 ---- batch: 110 ----
mean loss: 144.24
train mean loss: 145.12
epoch train time: 0:00:01.937051
elapsed time: 0:05:13.859817
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 23:17:55.874908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.02
 ---- batch: 020 ----
mean loss: 145.92
 ---- batch: 030 ----
mean loss: 137.76
 ---- batch: 040 ----
mean loss: 146.86
 ---- batch: 050 ----
mean loss: 141.71
 ---- batch: 060 ----
mean loss: 146.17
 ---- batch: 070 ----
mean loss: 148.30
 ---- batch: 080 ----
mean loss: 144.67
 ---- batch: 090 ----
mean loss: 141.26
 ---- batch: 100 ----
mean loss: 141.28
 ---- batch: 110 ----
mean loss: 148.87
train mean loss: 144.47
epoch train time: 0:00:01.981805
elapsed time: 0:05:15.842218
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 23:17:57.857311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.37
 ---- batch: 020 ----
mean loss: 144.42
 ---- batch: 030 ----
mean loss: 140.08
 ---- batch: 040 ----
mean loss: 140.64
 ---- batch: 050 ----
mean loss: 147.09
 ---- batch: 060 ----
mean loss: 145.03
 ---- batch: 070 ----
mean loss: 144.87
 ---- batch: 080 ----
mean loss: 144.76
 ---- batch: 090 ----
mean loss: 151.68
 ---- batch: 100 ----
mean loss: 154.08
 ---- batch: 110 ----
mean loss: 139.53
train mean loss: 144.93
epoch train time: 0:00:01.937363
elapsed time: 0:05:17.780188
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 23:17:59.795314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.43
 ---- batch: 020 ----
mean loss: 147.43
 ---- batch: 030 ----
mean loss: 147.99
 ---- batch: 040 ----
mean loss: 137.81
 ---- batch: 050 ----
mean loss: 142.52
 ---- batch: 060 ----
mean loss: 136.04
 ---- batch: 070 ----
mean loss: 147.39
 ---- batch: 080 ----
mean loss: 154.83
 ---- batch: 090 ----
mean loss: 141.24
 ---- batch: 100 ----
mean loss: 149.32
 ---- batch: 110 ----
mean loss: 144.55
train mean loss: 144.33
epoch train time: 0:00:01.918665
elapsed time: 0:05:19.699914
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 23:18:01.714668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.51
 ---- batch: 020 ----
mean loss: 139.89
 ---- batch: 030 ----
mean loss: 133.29
 ---- batch: 040 ----
mean loss: 134.57
 ---- batch: 050 ----
mean loss: 139.34
 ---- batch: 060 ----
mean loss: 148.24
 ---- batch: 070 ----
mean loss: 148.17
 ---- batch: 080 ----
mean loss: 149.22
 ---- batch: 090 ----
mean loss: 150.10
 ---- batch: 100 ----
mean loss: 150.04
 ---- batch: 110 ----
mean loss: 143.22
train mean loss: 144.19
epoch train time: 0:00:01.912161
elapsed time: 0:05:21.612324
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 23:18:03.627437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.69
 ---- batch: 020 ----
mean loss: 137.09
 ---- batch: 030 ----
mean loss: 143.37
 ---- batch: 040 ----
mean loss: 140.80
 ---- batch: 050 ----
mean loss: 142.30
 ---- batch: 060 ----
mean loss: 147.74
 ---- batch: 070 ----
mean loss: 143.18
 ---- batch: 080 ----
mean loss: 149.04
 ---- batch: 090 ----
mean loss: 150.68
 ---- batch: 100 ----
mean loss: 138.31
 ---- batch: 110 ----
mean loss: 141.52
train mean loss: 143.41
epoch train time: 0:00:01.904862
elapsed time: 0:05:23.517791
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 23:18:05.532581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.77
 ---- batch: 020 ----
mean loss: 138.41
 ---- batch: 030 ----
mean loss: 143.50
 ---- batch: 040 ----
mean loss: 143.29
 ---- batch: 050 ----
mean loss: 134.34
 ---- batch: 060 ----
mean loss: 143.26
 ---- batch: 070 ----
mean loss: 147.06
 ---- batch: 080 ----
mean loss: 144.60
 ---- batch: 090 ----
mean loss: 148.43
 ---- batch: 100 ----
mean loss: 148.77
 ---- batch: 110 ----
mean loss: 140.75
train mean loss: 143.46
epoch train time: 0:00:01.926235
elapsed time: 0:05:25.444295
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 23:18:07.459370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.82
 ---- batch: 020 ----
mean loss: 137.89
 ---- batch: 030 ----
mean loss: 136.52
 ---- batch: 040 ----
mean loss: 136.41
 ---- batch: 050 ----
mean loss: 145.96
 ---- batch: 060 ----
mean loss: 143.17
 ---- batch: 070 ----
mean loss: 137.05
 ---- batch: 080 ----
mean loss: 148.30
 ---- batch: 090 ----
mean loss: 147.37
 ---- batch: 100 ----
mean loss: 146.31
 ---- batch: 110 ----
mean loss: 140.34
train mean loss: 142.70
epoch train time: 0:00:01.952582
elapsed time: 0:05:27.397451
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 23:18:09.412547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.35
 ---- batch: 020 ----
mean loss: 130.20
 ---- batch: 030 ----
mean loss: 136.64
 ---- batch: 040 ----
mean loss: 140.05
 ---- batch: 050 ----
mean loss: 146.16
 ---- batch: 060 ----
mean loss: 146.32
 ---- batch: 070 ----
mean loss: 150.30
 ---- batch: 080 ----
mean loss: 143.87
 ---- batch: 090 ----
mean loss: 136.34
 ---- batch: 100 ----
mean loss: 144.82
 ---- batch: 110 ----
mean loss: 144.50
train mean loss: 142.46
epoch train time: 0:00:01.925270
elapsed time: 0:05:29.323306
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 23:18:11.338376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.52
 ---- batch: 020 ----
mean loss: 135.73
 ---- batch: 030 ----
mean loss: 140.81
 ---- batch: 040 ----
mean loss: 142.52
 ---- batch: 050 ----
mean loss: 142.56
 ---- batch: 060 ----
mean loss: 142.51
 ---- batch: 070 ----
mean loss: 142.00
 ---- batch: 080 ----
mean loss: 137.12
 ---- batch: 090 ----
mean loss: 144.28
 ---- batch: 100 ----
mean loss: 140.32
 ---- batch: 110 ----
mean loss: 148.21
train mean loss: 141.50
epoch train time: 0:00:01.908420
elapsed time: 0:05:31.232302
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 23:18:13.247380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.37
 ---- batch: 020 ----
mean loss: 138.17
 ---- batch: 030 ----
mean loss: 143.18
 ---- batch: 040 ----
mean loss: 143.24
 ---- batch: 050 ----
mean loss: 138.10
 ---- batch: 060 ----
mean loss: 135.79
 ---- batch: 070 ----
mean loss: 150.99
 ---- batch: 080 ----
mean loss: 144.10
 ---- batch: 090 ----
mean loss: 145.62
 ---- batch: 100 ----
mean loss: 140.62
 ---- batch: 110 ----
mean loss: 144.40
train mean loss: 142.22
epoch train time: 0:00:01.890917
elapsed time: 0:05:33.123806
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 23:18:15.138863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.61
 ---- batch: 020 ----
mean loss: 136.71
 ---- batch: 030 ----
mean loss: 144.71
 ---- batch: 040 ----
mean loss: 151.37
 ---- batch: 050 ----
mean loss: 140.94
 ---- batch: 060 ----
mean loss: 139.16
 ---- batch: 070 ----
mean loss: 143.92
 ---- batch: 080 ----
mean loss: 139.90
 ---- batch: 090 ----
mean loss: 138.06
 ---- batch: 100 ----
mean loss: 146.74
 ---- batch: 110 ----
mean loss: 141.92
train mean loss: 142.12
epoch train time: 0:00:01.933897
elapsed time: 0:05:35.058260
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 23:18:17.073339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.22
 ---- batch: 020 ----
mean loss: 138.70
 ---- batch: 030 ----
mean loss: 145.49
 ---- batch: 040 ----
mean loss: 135.53
 ---- batch: 050 ----
mean loss: 135.55
 ---- batch: 060 ----
mean loss: 141.89
 ---- batch: 070 ----
mean loss: 134.98
 ---- batch: 080 ----
mean loss: 138.43
 ---- batch: 090 ----
mean loss: 151.74
 ---- batch: 100 ----
mean loss: 143.12
 ---- batch: 110 ----
mean loss: 148.06
train mean loss: 141.01
epoch train time: 0:00:01.932366
elapsed time: 0:05:36.991199
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 23:18:19.006272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.99
 ---- batch: 020 ----
mean loss: 138.19
 ---- batch: 030 ----
mean loss: 141.12
 ---- batch: 040 ----
mean loss: 137.37
 ---- batch: 050 ----
mean loss: 140.02
 ---- batch: 060 ----
mean loss: 135.40
 ---- batch: 070 ----
mean loss: 145.79
 ---- batch: 080 ----
mean loss: 137.18
 ---- batch: 090 ----
mean loss: 131.96
 ---- batch: 100 ----
mean loss: 146.82
 ---- batch: 110 ----
mean loss: 148.37
train mean loss: 140.85
epoch train time: 0:00:01.898981
elapsed time: 0:05:38.890751
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 23:18:20.905926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.78
 ---- batch: 020 ----
mean loss: 138.57
 ---- batch: 030 ----
mean loss: 133.68
 ---- batch: 040 ----
mean loss: 146.02
 ---- batch: 050 ----
mean loss: 143.73
 ---- batch: 060 ----
mean loss: 134.41
 ---- batch: 070 ----
mean loss: 143.31
 ---- batch: 080 ----
mean loss: 141.17
 ---- batch: 090 ----
mean loss: 134.29
 ---- batch: 100 ----
mean loss: 148.70
 ---- batch: 110 ----
mean loss: 137.69
train mean loss: 140.33
epoch train time: 0:00:01.926814
elapsed time: 0:05:40.818270
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 23:18:22.833482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.90
 ---- batch: 020 ----
mean loss: 136.86
 ---- batch: 030 ----
mean loss: 133.07
 ---- batch: 040 ----
mean loss: 144.63
 ---- batch: 050 ----
mean loss: 154.14
 ---- batch: 060 ----
mean loss: 137.98
 ---- batch: 070 ----
mean loss: 131.18
 ---- batch: 080 ----
mean loss: 151.64
 ---- batch: 090 ----
mean loss: 143.20
 ---- batch: 100 ----
mean loss: 133.48
 ---- batch: 110 ----
mean loss: 133.29
train mean loss: 140.10
epoch train time: 0:00:01.922925
elapsed time: 0:05:42.741918
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 23:18:24.757040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.38
 ---- batch: 020 ----
mean loss: 135.88
 ---- batch: 030 ----
mean loss: 133.75
 ---- batch: 040 ----
mean loss: 141.18
 ---- batch: 050 ----
mean loss: 146.72
 ---- batch: 060 ----
mean loss: 142.98
 ---- batch: 070 ----
mean loss: 137.03
 ---- batch: 080 ----
mean loss: 140.66
 ---- batch: 090 ----
mean loss: 140.91
 ---- batch: 100 ----
mean loss: 147.63
 ---- batch: 110 ----
mean loss: 142.03
train mean loss: 140.13
epoch train time: 0:00:01.938382
elapsed time: 0:05:44.680927
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 23:18:26.696011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.61
 ---- batch: 020 ----
mean loss: 143.71
 ---- batch: 030 ----
mean loss: 140.31
 ---- batch: 040 ----
mean loss: 132.70
 ---- batch: 050 ----
mean loss: 149.20
 ---- batch: 060 ----
mean loss: 140.88
 ---- batch: 070 ----
mean loss: 135.91
 ---- batch: 080 ----
mean loss: 121.38
 ---- batch: 090 ----
mean loss: 138.70
 ---- batch: 100 ----
mean loss: 146.72
 ---- batch: 110 ----
mean loss: 146.26
train mean loss: 139.42
epoch train time: 0:00:01.943645
elapsed time: 0:05:46.625159
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 23:18:28.640300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.94
 ---- batch: 020 ----
mean loss: 137.21
 ---- batch: 030 ----
mean loss: 142.21
 ---- batch: 040 ----
mean loss: 139.39
 ---- batch: 050 ----
mean loss: 137.51
 ---- batch: 060 ----
mean loss: 138.18
 ---- batch: 070 ----
mean loss: 141.93
 ---- batch: 080 ----
mean loss: 138.57
 ---- batch: 090 ----
mean loss: 139.58
 ---- batch: 100 ----
mean loss: 131.87
 ---- batch: 110 ----
mean loss: 141.05
train mean loss: 138.73
epoch train time: 0:00:01.957320
elapsed time: 0:05:48.583171
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 23:18:30.598271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.01
 ---- batch: 020 ----
mean loss: 140.63
 ---- batch: 030 ----
mean loss: 140.76
 ---- batch: 040 ----
mean loss: 139.24
 ---- batch: 050 ----
mean loss: 138.98
 ---- batch: 060 ----
mean loss: 132.44
 ---- batch: 070 ----
mean loss: 148.02
 ---- batch: 080 ----
mean loss: 136.82
 ---- batch: 090 ----
mean loss: 132.61
 ---- batch: 100 ----
mean loss: 143.19
 ---- batch: 110 ----
mean loss: 141.44
train mean loss: 139.01
epoch train time: 0:00:01.905316
elapsed time: 0:05:50.489122
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 23:18:32.504253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.77
 ---- batch: 020 ----
mean loss: 132.95
 ---- batch: 030 ----
mean loss: 140.64
 ---- batch: 040 ----
mean loss: 136.50
 ---- batch: 050 ----
mean loss: 149.38
 ---- batch: 060 ----
mean loss: 134.31
 ---- batch: 070 ----
mean loss: 139.51
 ---- batch: 080 ----
mean loss: 139.03
 ---- batch: 090 ----
mean loss: 143.01
 ---- batch: 100 ----
mean loss: 137.56
 ---- batch: 110 ----
mean loss: 128.41
train mean loss: 138.58
epoch train time: 0:00:01.929170
elapsed time: 0:05:52.418951
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 23:18:34.434056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.46
 ---- batch: 020 ----
mean loss: 136.02
 ---- batch: 030 ----
mean loss: 135.15
 ---- batch: 040 ----
mean loss: 135.72
 ---- batch: 050 ----
mean loss: 134.81
 ---- batch: 060 ----
mean loss: 135.78
 ---- batch: 070 ----
mean loss: 130.73
 ---- batch: 080 ----
mean loss: 154.70
 ---- batch: 090 ----
mean loss: 140.09
 ---- batch: 100 ----
mean loss: 153.93
 ---- batch: 110 ----
mean loss: 134.54
train mean loss: 138.53
epoch train time: 0:00:01.947324
elapsed time: 0:05:54.366920
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 23:18:36.382012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.15
 ---- batch: 020 ----
mean loss: 134.94
 ---- batch: 030 ----
mean loss: 131.41
 ---- batch: 040 ----
mean loss: 133.61
 ---- batch: 050 ----
mean loss: 134.56
 ---- batch: 060 ----
mean loss: 132.89
 ---- batch: 070 ----
mean loss: 140.01
 ---- batch: 080 ----
mean loss: 133.95
 ---- batch: 090 ----
mean loss: 144.73
 ---- batch: 100 ----
mean loss: 146.30
 ---- batch: 110 ----
mean loss: 146.70
train mean loss: 138.00
epoch train time: 0:00:01.924699
elapsed time: 0:05:56.292230
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 23:18:38.307332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.47
 ---- batch: 020 ----
mean loss: 140.57
 ---- batch: 030 ----
mean loss: 135.17
 ---- batch: 040 ----
mean loss: 146.86
 ---- batch: 050 ----
mean loss: 144.97
 ---- batch: 060 ----
mean loss: 130.18
 ---- batch: 070 ----
mean loss: 128.20
 ---- batch: 080 ----
mean loss: 138.68
 ---- batch: 090 ----
mean loss: 138.63
 ---- batch: 100 ----
mean loss: 134.90
 ---- batch: 110 ----
mean loss: 138.46
train mean loss: 137.09
epoch train time: 0:00:01.958121
elapsed time: 0:05:58.251030
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 23:18:40.266136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.30
 ---- batch: 020 ----
mean loss: 136.98
 ---- batch: 030 ----
mean loss: 128.32
 ---- batch: 040 ----
mean loss: 144.35
 ---- batch: 050 ----
mean loss: 137.40
 ---- batch: 060 ----
mean loss: 140.13
 ---- batch: 070 ----
mean loss: 139.09
 ---- batch: 080 ----
mean loss: 142.53
 ---- batch: 090 ----
mean loss: 135.73
 ---- batch: 100 ----
mean loss: 143.27
 ---- batch: 110 ----
mean loss: 132.39
train mean loss: 137.27
epoch train time: 0:00:01.916400
elapsed time: 0:06:00.168076
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 23:18:42.183206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.50
 ---- batch: 020 ----
mean loss: 136.74
 ---- batch: 030 ----
mean loss: 135.52
 ---- batch: 040 ----
mean loss: 137.19
 ---- batch: 050 ----
mean loss: 139.09
 ---- batch: 060 ----
mean loss: 142.20
 ---- batch: 070 ----
mean loss: 132.29
 ---- batch: 080 ----
mean loss: 138.93
 ---- batch: 090 ----
mean loss: 147.31
 ---- batch: 100 ----
mean loss: 133.01
 ---- batch: 110 ----
mean loss: 134.69
train mean loss: 137.65
epoch train time: 0:00:01.910788
elapsed time: 0:06:02.079498
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 23:18:44.094674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.29
 ---- batch: 020 ----
mean loss: 142.23
 ---- batch: 030 ----
mean loss: 143.52
 ---- batch: 040 ----
mean loss: 135.39
 ---- batch: 050 ----
mean loss: 136.51
 ---- batch: 060 ----
mean loss: 127.47
 ---- batch: 070 ----
mean loss: 143.20
 ---- batch: 080 ----
mean loss: 135.82
 ---- batch: 090 ----
mean loss: 140.32
 ---- batch: 100 ----
mean loss: 139.52
 ---- batch: 110 ----
mean loss: 138.66
train mean loss: 137.35
epoch train time: 0:00:01.902305
elapsed time: 0:06:03.982490
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 23:18:45.997562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.73
 ---- batch: 020 ----
mean loss: 136.24
 ---- batch: 030 ----
mean loss: 133.60
 ---- batch: 040 ----
mean loss: 129.64
 ---- batch: 050 ----
mean loss: 136.51
 ---- batch: 060 ----
mean loss: 136.88
 ---- batch: 070 ----
mean loss: 131.13
 ---- batch: 080 ----
mean loss: 139.73
 ---- batch: 090 ----
mean loss: 141.31
 ---- batch: 100 ----
mean loss: 137.59
 ---- batch: 110 ----
mean loss: 143.19
train mean loss: 136.10
epoch train time: 0:00:01.902417
elapsed time: 0:06:05.885471
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 23:18:47.900611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.61
 ---- batch: 020 ----
mean loss: 134.77
 ---- batch: 030 ----
mean loss: 140.48
 ---- batch: 040 ----
mean loss: 134.35
 ---- batch: 050 ----
mean loss: 131.00
 ---- batch: 060 ----
mean loss: 132.86
 ---- batch: 070 ----
mean loss: 139.62
 ---- batch: 080 ----
mean loss: 132.04
 ---- batch: 090 ----
mean loss: 144.34
 ---- batch: 100 ----
mean loss: 130.33
 ---- batch: 110 ----
mean loss: 142.00
train mean loss: 136.20
epoch train time: 0:00:01.916276
elapsed time: 0:06:07.802804
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 23:18:49.817679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.96
 ---- batch: 020 ----
mean loss: 135.94
 ---- batch: 030 ----
mean loss: 135.91
 ---- batch: 040 ----
mean loss: 136.74
 ---- batch: 050 ----
mean loss: 140.31
 ---- batch: 060 ----
mean loss: 134.20
 ---- batch: 070 ----
mean loss: 134.85
 ---- batch: 080 ----
mean loss: 134.49
 ---- batch: 090 ----
mean loss: 140.47
 ---- batch: 100 ----
mean loss: 142.51
 ---- batch: 110 ----
mean loss: 137.06
train mean loss: 135.82
epoch train time: 0:00:01.919629
elapsed time: 0:06:09.722828
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 23:18:51.737974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.64
 ---- batch: 020 ----
mean loss: 131.45
 ---- batch: 030 ----
mean loss: 135.73
 ---- batch: 040 ----
mean loss: 134.35
 ---- batch: 050 ----
mean loss: 133.45
 ---- batch: 060 ----
mean loss: 138.86
 ---- batch: 070 ----
mean loss: 141.04
 ---- batch: 080 ----
mean loss: 136.67
 ---- batch: 090 ----
mean loss: 136.47
 ---- batch: 100 ----
mean loss: 136.59
 ---- batch: 110 ----
mean loss: 132.82
train mean loss: 136.00
epoch train time: 0:00:01.893130
elapsed time: 0:06:11.616582
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 23:18:53.631669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.82
 ---- batch: 020 ----
mean loss: 136.04
 ---- batch: 030 ----
mean loss: 126.38
 ---- batch: 040 ----
mean loss: 141.13
 ---- batch: 050 ----
mean loss: 139.72
 ---- batch: 060 ----
mean loss: 134.12
 ---- batch: 070 ----
mean loss: 133.91
 ---- batch: 080 ----
mean loss: 142.41
 ---- batch: 090 ----
mean loss: 125.54
 ---- batch: 100 ----
mean loss: 140.36
 ---- batch: 110 ----
mean loss: 133.95
train mean loss: 135.17
epoch train time: 0:00:01.923662
elapsed time: 0:06:13.540835
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 23:18:55.555924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.78
 ---- batch: 020 ----
mean loss: 136.63
 ---- batch: 030 ----
mean loss: 136.87
 ---- batch: 040 ----
mean loss: 126.84
 ---- batch: 050 ----
mean loss: 136.49
 ---- batch: 060 ----
mean loss: 135.52
 ---- batch: 070 ----
mean loss: 130.96
 ---- batch: 080 ----
mean loss: 134.20
 ---- batch: 090 ----
mean loss: 140.18
 ---- batch: 100 ----
mean loss: 141.86
 ---- batch: 110 ----
mean loss: 134.25
train mean loss: 135.14
epoch train time: 0:00:01.931018
elapsed time: 0:06:15.472456
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 23:18:57.487530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.31
 ---- batch: 020 ----
mean loss: 141.73
 ---- batch: 030 ----
mean loss: 135.54
 ---- batch: 040 ----
mean loss: 126.91
 ---- batch: 050 ----
mean loss: 131.19
 ---- batch: 060 ----
mean loss: 140.42
 ---- batch: 070 ----
mean loss: 130.19
 ---- batch: 080 ----
mean loss: 134.82
 ---- batch: 090 ----
mean loss: 141.59
 ---- batch: 100 ----
mean loss: 129.64
 ---- batch: 110 ----
mean loss: 136.73
train mean loss: 134.89
epoch train time: 0:00:01.925135
elapsed time: 0:06:17.398145
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 23:18:59.413233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.84
 ---- batch: 020 ----
mean loss: 131.54
 ---- batch: 030 ----
mean loss: 129.83
 ---- batch: 040 ----
mean loss: 140.75
 ---- batch: 050 ----
mean loss: 136.66
 ---- batch: 060 ----
mean loss: 127.45
 ---- batch: 070 ----
mean loss: 138.80
 ---- batch: 080 ----
mean loss: 136.45
 ---- batch: 090 ----
mean loss: 135.32
 ---- batch: 100 ----
mean loss: 133.06
 ---- batch: 110 ----
mean loss: 142.52
train mean loss: 134.72
epoch train time: 0:00:01.959248
elapsed time: 0:06:19.357968
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 23:19:01.373041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.04
 ---- batch: 020 ----
mean loss: 135.42
 ---- batch: 030 ----
mean loss: 128.22
 ---- batch: 040 ----
mean loss: 133.02
 ---- batch: 050 ----
mean loss: 140.72
 ---- batch: 060 ----
mean loss: 138.07
 ---- batch: 070 ----
mean loss: 137.12
 ---- batch: 080 ----
mean loss: 135.46
 ---- batch: 090 ----
mean loss: 129.54
 ---- batch: 100 ----
mean loss: 135.48
 ---- batch: 110 ----
mean loss: 138.42
train mean loss: 134.43
epoch train time: 0:00:01.930228
elapsed time: 0:06:21.288762
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 23:19:03.303856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.95
 ---- batch: 020 ----
mean loss: 130.69
 ---- batch: 030 ----
mean loss: 142.76
 ---- batch: 040 ----
mean loss: 121.94
 ---- batch: 050 ----
mean loss: 134.39
 ---- batch: 060 ----
mean loss: 128.54
 ---- batch: 070 ----
mean loss: 132.02
 ---- batch: 080 ----
mean loss: 136.20
 ---- batch: 090 ----
mean loss: 138.39
 ---- batch: 100 ----
mean loss: 140.96
 ---- batch: 110 ----
mean loss: 136.81
train mean loss: 134.03
epoch train time: 0:00:01.914018
elapsed time: 0:06:23.203359
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 23:19:05.218416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.00
 ---- batch: 020 ----
mean loss: 136.86
 ---- batch: 030 ----
mean loss: 130.53
 ---- batch: 040 ----
mean loss: 134.99
 ---- batch: 050 ----
mean loss: 131.26
 ---- batch: 060 ----
mean loss: 134.44
 ---- batch: 070 ----
mean loss: 134.36
 ---- batch: 080 ----
mean loss: 133.76
 ---- batch: 090 ----
mean loss: 135.60
 ---- batch: 100 ----
mean loss: 135.49
 ---- batch: 110 ----
mean loss: 131.13
train mean loss: 133.89
epoch train time: 0:00:01.932029
elapsed time: 0:06:25.135919
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 23:19:07.151017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.51
 ---- batch: 020 ----
mean loss: 136.09
 ---- batch: 030 ----
mean loss: 121.19
 ---- batch: 040 ----
mean loss: 135.25
 ---- batch: 050 ----
mean loss: 129.66
 ---- batch: 060 ----
mean loss: 130.09
 ---- batch: 070 ----
mean loss: 136.64
 ---- batch: 080 ----
mean loss: 136.39
 ---- batch: 090 ----
mean loss: 145.55
 ---- batch: 100 ----
mean loss: 130.44
 ---- batch: 110 ----
mean loss: 136.33
train mean loss: 133.62
epoch train time: 0:00:01.911853
elapsed time: 0:06:27.048392
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 23:19:09.063475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.83
 ---- batch: 020 ----
mean loss: 131.09
 ---- batch: 030 ----
mean loss: 131.41
 ---- batch: 040 ----
mean loss: 127.19
 ---- batch: 050 ----
mean loss: 137.70
 ---- batch: 060 ----
mean loss: 134.13
 ---- batch: 070 ----
mean loss: 134.84
 ---- batch: 080 ----
mean loss: 135.52
 ---- batch: 090 ----
mean loss: 127.77
 ---- batch: 100 ----
mean loss: 136.65
 ---- batch: 110 ----
mean loss: 139.72
train mean loss: 132.97
epoch train time: 0:00:01.877308
elapsed time: 0:06:28.926285
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 23:19:10.941390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.77
 ---- batch: 020 ----
mean loss: 128.25
 ---- batch: 030 ----
mean loss: 137.02
 ---- batch: 040 ----
mean loss: 129.31
 ---- batch: 050 ----
mean loss: 131.59
 ---- batch: 060 ----
mean loss: 132.94
 ---- batch: 070 ----
mean loss: 141.16
 ---- batch: 080 ----
mean loss: 129.37
 ---- batch: 090 ----
mean loss: 131.42
 ---- batch: 100 ----
mean loss: 134.01
 ---- batch: 110 ----
mean loss: 141.50
train mean loss: 132.90
epoch train time: 0:00:01.892392
elapsed time: 0:06:30.819333
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 23:19:12.834496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.45
 ---- batch: 020 ----
mean loss: 135.07
 ---- batch: 030 ----
mean loss: 133.52
 ---- batch: 040 ----
mean loss: 132.17
 ---- batch: 050 ----
mean loss: 130.97
 ---- batch: 060 ----
mean loss: 136.31
 ---- batch: 070 ----
mean loss: 135.93
 ---- batch: 080 ----
mean loss: 131.56
 ---- batch: 090 ----
mean loss: 129.42
 ---- batch: 100 ----
mean loss: 129.71
 ---- batch: 110 ----
mean loss: 133.15
train mean loss: 132.40
epoch train time: 0:00:01.917734
elapsed time: 0:06:32.737717
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 23:19:14.752795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.59
 ---- batch: 020 ----
mean loss: 125.45
 ---- batch: 030 ----
mean loss: 127.90
 ---- batch: 040 ----
mean loss: 135.46
 ---- batch: 050 ----
mean loss: 136.63
 ---- batch: 060 ----
mean loss: 139.34
 ---- batch: 070 ----
mean loss: 131.81
 ---- batch: 080 ----
mean loss: 126.49
 ---- batch: 090 ----
mean loss: 137.87
 ---- batch: 100 ----
mean loss: 136.24
 ---- batch: 110 ----
mean loss: 132.67
train mean loss: 132.28
epoch train time: 0:00:01.899606
elapsed time: 0:06:34.637904
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 23:19:16.652996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.96
 ---- batch: 020 ----
mean loss: 133.48
 ---- batch: 030 ----
mean loss: 141.91
 ---- batch: 040 ----
mean loss: 122.19
 ---- batch: 050 ----
mean loss: 133.05
 ---- batch: 060 ----
mean loss: 132.11
 ---- batch: 070 ----
mean loss: 130.62
 ---- batch: 080 ----
mean loss: 135.01
 ---- batch: 090 ----
mean loss: 131.55
 ---- batch: 100 ----
mean loss: 131.68
 ---- batch: 110 ----
mean loss: 126.31
train mean loss: 131.84
epoch train time: 0:00:01.922954
elapsed time: 0:06:36.561495
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 23:19:18.576569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.48
 ---- batch: 020 ----
mean loss: 130.34
 ---- batch: 030 ----
mean loss: 144.62
 ---- batch: 040 ----
mean loss: 131.54
 ---- batch: 050 ----
mean loss: 125.11
 ---- batch: 060 ----
mean loss: 133.12
 ---- batch: 070 ----
mean loss: 128.16
 ---- batch: 080 ----
mean loss: 138.76
 ---- batch: 090 ----
mean loss: 129.62
 ---- batch: 100 ----
mean loss: 123.00
 ---- batch: 110 ----
mean loss: 133.81
train mean loss: 131.65
epoch train time: 0:00:01.932329
elapsed time: 0:06:38.494409
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 23:19:20.509492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.95
 ---- batch: 020 ----
mean loss: 120.65
 ---- batch: 030 ----
mean loss: 132.01
 ---- batch: 040 ----
mean loss: 126.38
 ---- batch: 050 ----
mean loss: 128.54
 ---- batch: 060 ----
mean loss: 136.30
 ---- batch: 070 ----
mean loss: 140.61
 ---- batch: 080 ----
mean loss: 129.36
 ---- batch: 090 ----
mean loss: 128.97
 ---- batch: 100 ----
mean loss: 134.40
 ---- batch: 110 ----
mean loss: 140.02
train mean loss: 131.24
epoch train time: 0:00:01.898381
elapsed time: 0:06:40.393357
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 23:19:22.408429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.03
 ---- batch: 020 ----
mean loss: 126.73
 ---- batch: 030 ----
mean loss: 129.16
 ---- batch: 040 ----
mean loss: 134.01
 ---- batch: 050 ----
mean loss: 133.20
 ---- batch: 060 ----
mean loss: 132.59
 ---- batch: 070 ----
mean loss: 129.47
 ---- batch: 080 ----
mean loss: 127.00
 ---- batch: 090 ----
mean loss: 135.26
 ---- batch: 100 ----
mean loss: 135.65
 ---- batch: 110 ----
mean loss: 133.26
train mean loss: 131.24
epoch train time: 0:00:01.947991
elapsed time: 0:06:42.341920
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 23:19:24.357016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.49
 ---- batch: 020 ----
mean loss: 130.53
 ---- batch: 030 ----
mean loss: 133.23
 ---- batch: 040 ----
mean loss: 128.13
 ---- batch: 050 ----
mean loss: 130.90
 ---- batch: 060 ----
mean loss: 132.15
 ---- batch: 070 ----
mean loss: 125.61
 ---- batch: 080 ----
mean loss: 129.38
 ---- batch: 090 ----
mean loss: 128.08
 ---- batch: 100 ----
mean loss: 130.24
 ---- batch: 110 ----
mean loss: 135.34
train mean loss: 131.15
epoch train time: 0:00:01.934799
elapsed time: 0:06:44.277332
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 23:19:26.292494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.27
 ---- batch: 020 ----
mean loss: 131.87
 ---- batch: 030 ----
mean loss: 131.28
 ---- batch: 040 ----
mean loss: 130.82
 ---- batch: 050 ----
mean loss: 130.81
 ---- batch: 060 ----
mean loss: 125.94
 ---- batch: 070 ----
mean loss: 124.73
 ---- batch: 080 ----
mean loss: 129.51
 ---- batch: 090 ----
mean loss: 129.82
 ---- batch: 100 ----
mean loss: 140.04
 ---- batch: 110 ----
mean loss: 136.94
train mean loss: 130.95
epoch train time: 0:00:01.933886
elapsed time: 0:06:46.211868
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 23:19:28.226974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.44
 ---- batch: 020 ----
mean loss: 139.77
 ---- batch: 030 ----
mean loss: 134.04
 ---- batch: 040 ----
mean loss: 129.48
 ---- batch: 050 ----
mean loss: 125.03
 ---- batch: 060 ----
mean loss: 124.16
 ---- batch: 070 ----
mean loss: 134.16
 ---- batch: 080 ----
mean loss: 127.94
 ---- batch: 090 ----
mean loss: 129.90
 ---- batch: 100 ----
mean loss: 136.61
 ---- batch: 110 ----
mean loss: 127.40
train mean loss: 130.66
epoch train time: 0:00:01.906816
elapsed time: 0:06:48.119310
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 23:19:30.134397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.66
 ---- batch: 020 ----
mean loss: 133.06
 ---- batch: 030 ----
mean loss: 137.01
 ---- batch: 040 ----
mean loss: 129.24
 ---- batch: 050 ----
mean loss: 124.83
 ---- batch: 060 ----
mean loss: 127.32
 ---- batch: 070 ----
mean loss: 130.93
 ---- batch: 080 ----
mean loss: 128.12
 ---- batch: 090 ----
mean loss: 133.19
 ---- batch: 100 ----
mean loss: 131.04
 ---- batch: 110 ----
mean loss: 133.39
train mean loss: 130.51
epoch train time: 0:00:01.929116
elapsed time: 0:06:50.049102
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 23:19:32.064225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.25
 ---- batch: 020 ----
mean loss: 128.80
 ---- batch: 030 ----
mean loss: 128.92
 ---- batch: 040 ----
mean loss: 136.08
 ---- batch: 050 ----
mean loss: 122.13
 ---- batch: 060 ----
mean loss: 128.02
 ---- batch: 070 ----
mean loss: 133.33
 ---- batch: 080 ----
mean loss: 129.77
 ---- batch: 090 ----
mean loss: 129.00
 ---- batch: 100 ----
mean loss: 124.72
 ---- batch: 110 ----
mean loss: 136.76
train mean loss: 130.47
epoch train time: 0:00:01.923219
elapsed time: 0:06:51.972934
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 23:19:33.988043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.63
 ---- batch: 020 ----
mean loss: 126.96
 ---- batch: 030 ----
mean loss: 125.71
 ---- batch: 040 ----
mean loss: 121.85
 ---- batch: 050 ----
mean loss: 128.74
 ---- batch: 060 ----
mean loss: 118.81
 ---- batch: 070 ----
mean loss: 143.01
 ---- batch: 080 ----
mean loss: 135.89
 ---- batch: 090 ----
mean loss: 133.27
 ---- batch: 100 ----
mean loss: 132.82
 ---- batch: 110 ----
mean loss: 127.90
train mean loss: 129.56
epoch train time: 0:00:01.889175
elapsed time: 0:06:53.862758
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 23:19:35.877855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.85
 ---- batch: 020 ----
mean loss: 119.74
 ---- batch: 030 ----
mean loss: 130.93
 ---- batch: 040 ----
mean loss: 125.63
 ---- batch: 050 ----
mean loss: 127.45
 ---- batch: 060 ----
mean loss: 137.10
 ---- batch: 070 ----
mean loss: 120.24
 ---- batch: 080 ----
mean loss: 136.77
 ---- batch: 090 ----
mean loss: 127.49
 ---- batch: 100 ----
mean loss: 130.97
 ---- batch: 110 ----
mean loss: 137.13
train mean loss: 129.54
epoch train time: 0:00:01.910483
elapsed time: 0:06:55.773909
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 23:19:37.789002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.01
 ---- batch: 020 ----
mean loss: 124.92
 ---- batch: 030 ----
mean loss: 133.26
 ---- batch: 040 ----
mean loss: 122.79
 ---- batch: 050 ----
mean loss: 126.26
 ---- batch: 060 ----
mean loss: 139.85
 ---- batch: 070 ----
mean loss: 130.00
 ---- batch: 080 ----
mean loss: 131.05
 ---- batch: 090 ----
mean loss: 126.30
 ---- batch: 100 ----
mean loss: 123.80
 ---- batch: 110 ----
mean loss: 134.04
train mean loss: 129.29
epoch train time: 0:00:01.890661
elapsed time: 0:06:57.665210
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 23:19:39.680294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.00
 ---- batch: 020 ----
mean loss: 121.98
 ---- batch: 030 ----
mean loss: 125.25
 ---- batch: 040 ----
mean loss: 123.60
 ---- batch: 050 ----
mean loss: 130.36
 ---- batch: 060 ----
mean loss: 130.51
 ---- batch: 070 ----
mean loss: 126.95
 ---- batch: 080 ----
mean loss: 125.29
 ---- batch: 090 ----
mean loss: 132.99
 ---- batch: 100 ----
mean loss: 130.69
 ---- batch: 110 ----
mean loss: 139.23
train mean loss: 128.95
epoch train time: 0:00:01.887739
elapsed time: 0:06:59.553530
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 23:19:41.568617
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.21
 ---- batch: 020 ----
mean loss: 124.44
 ---- batch: 030 ----
mean loss: 132.62
 ---- batch: 040 ----
mean loss: 122.16
 ---- batch: 050 ----
mean loss: 118.87
 ---- batch: 060 ----
mean loss: 119.74
 ---- batch: 070 ----
mean loss: 117.20
 ---- batch: 080 ----
mean loss: 123.20
 ---- batch: 090 ----
mean loss: 130.41
 ---- batch: 100 ----
mean loss: 127.81
 ---- batch: 110 ----
mean loss: 124.11
train mean loss: 123.93
epoch train time: 0:00:01.930726
elapsed time: 0:07:01.485237
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 23:19:43.499991
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.37
 ---- batch: 020 ----
mean loss: 118.70
 ---- batch: 030 ----
mean loss: 117.74
 ---- batch: 040 ----
mean loss: 124.17
 ---- batch: 050 ----
mean loss: 123.01
 ---- batch: 060 ----
mean loss: 124.40
 ---- batch: 070 ----
mean loss: 118.40
 ---- batch: 080 ----
mean loss: 132.70
 ---- batch: 090 ----
mean loss: 124.05
 ---- batch: 100 ----
mean loss: 123.93
 ---- batch: 110 ----
mean loss: 117.33
train mean loss: 123.28
epoch train time: 0:00:01.926033
elapsed time: 0:07:03.411519
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 23:19:45.426598
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.68
 ---- batch: 020 ----
mean loss: 123.34
 ---- batch: 030 ----
mean loss: 127.09
 ---- batch: 040 ----
mean loss: 121.95
 ---- batch: 050 ----
mean loss: 124.65
 ---- batch: 060 ----
mean loss: 121.54
 ---- batch: 070 ----
mean loss: 119.36
 ---- batch: 080 ----
mean loss: 126.24
 ---- batch: 090 ----
mean loss: 122.10
 ---- batch: 100 ----
mean loss: 124.35
 ---- batch: 110 ----
mean loss: 121.59
train mean loss: 123.01
epoch train time: 0:00:01.915771
elapsed time: 0:07:05.327872
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 23:19:47.342660
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.00
 ---- batch: 020 ----
mean loss: 116.89
 ---- batch: 030 ----
mean loss: 117.10
 ---- batch: 040 ----
mean loss: 117.68
 ---- batch: 050 ----
mean loss: 131.08
 ---- batch: 060 ----
mean loss: 123.45
 ---- batch: 070 ----
mean loss: 131.43
 ---- batch: 080 ----
mean loss: 124.70
 ---- batch: 090 ----
mean loss: 128.09
 ---- batch: 100 ----
mean loss: 118.16
 ---- batch: 110 ----
mean loss: 117.48
train mean loss: 122.90
epoch train time: 0:00:01.928701
elapsed time: 0:07:07.256857
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 23:19:49.271939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.00
 ---- batch: 020 ----
mean loss: 121.99
 ---- batch: 030 ----
mean loss: 122.47
 ---- batch: 040 ----
mean loss: 122.69
 ---- batch: 050 ----
mean loss: 121.38
 ---- batch: 060 ----
mean loss: 122.91
 ---- batch: 070 ----
mean loss: 121.75
 ---- batch: 080 ----
mean loss: 130.67
 ---- batch: 090 ----
mean loss: 115.36
 ---- batch: 100 ----
mean loss: 120.48
 ---- batch: 110 ----
mean loss: 128.30
train mean loss: 122.78
epoch train time: 0:00:01.952920
elapsed time: 0:07:09.210349
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 23:19:51.225438
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.00
 ---- batch: 020 ----
mean loss: 119.64
 ---- batch: 030 ----
mean loss: 122.01
 ---- batch: 040 ----
mean loss: 123.90
 ---- batch: 050 ----
mean loss: 115.14
 ---- batch: 060 ----
mean loss: 127.96
 ---- batch: 070 ----
mean loss: 128.30
 ---- batch: 080 ----
mean loss: 124.19
 ---- batch: 090 ----
mean loss: 121.71
 ---- batch: 100 ----
mean loss: 118.53
 ---- batch: 110 ----
mean loss: 122.53
train mean loss: 122.74
epoch train time: 0:00:01.952352
elapsed time: 0:07:11.163268
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 23:19:53.178359
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.38
 ---- batch: 020 ----
mean loss: 124.54
 ---- batch: 030 ----
mean loss: 120.95
 ---- batch: 040 ----
mean loss: 128.58
 ---- batch: 050 ----
mean loss: 120.77
 ---- batch: 060 ----
mean loss: 121.90
 ---- batch: 070 ----
mean loss: 116.48
 ---- batch: 080 ----
mean loss: 128.89
 ---- batch: 090 ----
mean loss: 122.09
 ---- batch: 100 ----
mean loss: 129.46
 ---- batch: 110 ----
mean loss: 118.53
train mean loss: 122.72
epoch train time: 0:00:01.978705
elapsed time: 0:07:13.142601
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 23:19:55.157698
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.68
 ---- batch: 020 ----
mean loss: 123.67
 ---- batch: 030 ----
mean loss: 118.95
 ---- batch: 040 ----
mean loss: 120.05
 ---- batch: 050 ----
mean loss: 125.94
 ---- batch: 060 ----
mean loss: 127.76
 ---- batch: 070 ----
mean loss: 128.61
 ---- batch: 080 ----
mean loss: 124.76
 ---- batch: 090 ----
mean loss: 126.29
 ---- batch: 100 ----
mean loss: 119.11
 ---- batch: 110 ----
mean loss: 121.59
train mean loss: 122.80
epoch train time: 0:00:01.909023
elapsed time: 0:07:15.052217
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 23:19:57.067295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.41
 ---- batch: 020 ----
mean loss: 115.59
 ---- batch: 030 ----
mean loss: 126.51
 ---- batch: 040 ----
mean loss: 121.78
 ---- batch: 050 ----
mean loss: 118.77
 ---- batch: 060 ----
mean loss: 125.69
 ---- batch: 070 ----
mean loss: 126.49
 ---- batch: 080 ----
mean loss: 117.28
 ---- batch: 090 ----
mean loss: 121.82
 ---- batch: 100 ----
mean loss: 120.88
 ---- batch: 110 ----
mean loss: 137.66
train mean loss: 122.60
epoch train time: 0:00:01.932995
elapsed time: 0:07:16.985805
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 23:19:59.000879
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.13
 ---- batch: 020 ----
mean loss: 121.89
 ---- batch: 030 ----
mean loss: 122.36
 ---- batch: 040 ----
mean loss: 124.97
 ---- batch: 050 ----
mean loss: 124.70
 ---- batch: 060 ----
mean loss: 122.28
 ---- batch: 070 ----
mean loss: 116.09
 ---- batch: 080 ----
mean loss: 118.78
 ---- batch: 090 ----
mean loss: 120.82
 ---- batch: 100 ----
mean loss: 121.40
 ---- batch: 110 ----
mean loss: 124.64
train mean loss: 122.61
epoch train time: 0:00:01.918926
elapsed time: 0:07:18.905336
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 23:20:00.920408
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.65
 ---- batch: 020 ----
mean loss: 126.77
 ---- batch: 030 ----
mean loss: 125.32
 ---- batch: 040 ----
mean loss: 120.87
 ---- batch: 050 ----
mean loss: 126.12
 ---- batch: 060 ----
mean loss: 122.10
 ---- batch: 070 ----
mean loss: 119.48
 ---- batch: 080 ----
mean loss: 119.44
 ---- batch: 090 ----
mean loss: 128.82
 ---- batch: 100 ----
mean loss: 115.24
 ---- batch: 110 ----
mean loss: 121.45
train mean loss: 122.55
epoch train time: 0:00:01.914851
elapsed time: 0:07:20.820814
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 23:20:02.835895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.55
 ---- batch: 020 ----
mean loss: 117.65
 ---- batch: 030 ----
mean loss: 126.61
 ---- batch: 040 ----
mean loss: 128.86
 ---- batch: 050 ----
mean loss: 126.45
 ---- batch: 060 ----
mean loss: 122.23
 ---- batch: 070 ----
mean loss: 121.32
 ---- batch: 080 ----
mean loss: 119.82
 ---- batch: 090 ----
mean loss: 122.17
 ---- batch: 100 ----
mean loss: 126.31
 ---- batch: 110 ----
mean loss: 119.25
train mean loss: 122.59
epoch train time: 0:00:01.931982
elapsed time: 0:07:22.753394
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 23:20:04.768477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.44
 ---- batch: 020 ----
mean loss: 120.34
 ---- batch: 030 ----
mean loss: 114.05
 ---- batch: 040 ----
mean loss: 129.13
 ---- batch: 050 ----
mean loss: 121.69
 ---- batch: 060 ----
mean loss: 126.43
 ---- batch: 070 ----
mean loss: 118.71
 ---- batch: 080 ----
mean loss: 122.95
 ---- batch: 090 ----
mean loss: 115.83
 ---- batch: 100 ----
mean loss: 125.25
 ---- batch: 110 ----
mean loss: 129.88
train mean loss: 122.57
epoch train time: 0:00:01.921679
elapsed time: 0:07:24.675693
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 23:20:06.690970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.36
 ---- batch: 020 ----
mean loss: 115.49
 ---- batch: 030 ----
mean loss: 119.72
 ---- batch: 040 ----
mean loss: 117.88
 ---- batch: 050 ----
mean loss: 124.27
 ---- batch: 060 ----
mean loss: 116.50
 ---- batch: 070 ----
mean loss: 131.36
 ---- batch: 080 ----
mean loss: 123.00
 ---- batch: 090 ----
mean loss: 122.60
 ---- batch: 100 ----
mean loss: 126.29
 ---- batch: 110 ----
mean loss: 129.31
train mean loss: 122.49
epoch train time: 0:00:01.944913
elapsed time: 0:07:26.621378
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 23:20:08.636467
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.77
 ---- batch: 020 ----
mean loss: 120.75
 ---- batch: 030 ----
mean loss: 126.54
 ---- batch: 040 ----
mean loss: 124.80
 ---- batch: 050 ----
mean loss: 116.30
 ---- batch: 060 ----
mean loss: 120.16
 ---- batch: 070 ----
mean loss: 125.38
 ---- batch: 080 ----
mean loss: 121.39
 ---- batch: 090 ----
mean loss: 111.54
 ---- batch: 100 ----
mean loss: 125.25
 ---- batch: 110 ----
mean loss: 123.68
train mean loss: 122.47
epoch train time: 0:00:01.928508
elapsed time: 0:07:28.550462
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 23:20:10.565554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.32
 ---- batch: 020 ----
mean loss: 121.52
 ---- batch: 030 ----
mean loss: 124.10
 ---- batch: 040 ----
mean loss: 119.85
 ---- batch: 050 ----
mean loss: 116.97
 ---- batch: 060 ----
mean loss: 125.14
 ---- batch: 070 ----
mean loss: 122.21
 ---- batch: 080 ----
mean loss: 125.09
 ---- batch: 090 ----
mean loss: 123.08
 ---- batch: 100 ----
mean loss: 121.95
 ---- batch: 110 ----
mean loss: 120.93
train mean loss: 122.43
epoch train time: 0:00:01.903907
elapsed time: 0:07:30.454967
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 23:20:12.470041
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.76
 ---- batch: 020 ----
mean loss: 120.59
 ---- batch: 030 ----
mean loss: 126.22
 ---- batch: 040 ----
mean loss: 119.39
 ---- batch: 050 ----
mean loss: 122.09
 ---- batch: 060 ----
mean loss: 124.28
 ---- batch: 070 ----
mean loss: 121.09
 ---- batch: 080 ----
mean loss: 123.69
 ---- batch: 090 ----
mean loss: 122.23
 ---- batch: 100 ----
mean loss: 127.10
 ---- batch: 110 ----
mean loss: 122.38
train mean loss: 122.36
epoch train time: 0:00:01.909440
elapsed time: 0:07:32.364962
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 23:20:14.380048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.07
 ---- batch: 020 ----
mean loss: 125.18
 ---- batch: 030 ----
mean loss: 126.46
 ---- batch: 040 ----
mean loss: 124.56
 ---- batch: 050 ----
mean loss: 120.24
 ---- batch: 060 ----
mean loss: 122.92
 ---- batch: 070 ----
mean loss: 120.41
 ---- batch: 080 ----
mean loss: 118.96
 ---- batch: 090 ----
mean loss: 124.15
 ---- batch: 100 ----
mean loss: 124.05
 ---- batch: 110 ----
mean loss: 116.13
train mean loss: 122.40
epoch train time: 0:00:01.950585
elapsed time: 0:07:34.316146
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 23:20:16.331209
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.67
 ---- batch: 020 ----
mean loss: 117.02
 ---- batch: 030 ----
mean loss: 119.63
 ---- batch: 040 ----
mean loss: 119.97
 ---- batch: 050 ----
mean loss: 123.75
 ---- batch: 060 ----
mean loss: 116.04
 ---- batch: 070 ----
mean loss: 122.04
 ---- batch: 080 ----
mean loss: 127.49
 ---- batch: 090 ----
mean loss: 125.85
 ---- batch: 100 ----
mean loss: 125.06
 ---- batch: 110 ----
mean loss: 125.16
train mean loss: 122.32
epoch train time: 0:00:01.894271
elapsed time: 0:07:36.211001
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 23:20:18.226104
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.67
 ---- batch: 020 ----
mean loss: 122.73
 ---- batch: 030 ----
mean loss: 127.18
 ---- batch: 040 ----
mean loss: 122.21
 ---- batch: 050 ----
mean loss: 126.54
 ---- batch: 060 ----
mean loss: 118.88
 ---- batch: 070 ----
mean loss: 115.98
 ---- batch: 080 ----
mean loss: 122.87
 ---- batch: 090 ----
mean loss: 117.66
 ---- batch: 100 ----
mean loss: 128.45
 ---- batch: 110 ----
mean loss: 124.18
train mean loss: 122.26
epoch train time: 0:00:01.915919
elapsed time: 0:07:38.127572
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 23:20:20.142674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.56
 ---- batch: 020 ----
mean loss: 117.32
 ---- batch: 030 ----
mean loss: 124.29
 ---- batch: 040 ----
mean loss: 121.84
 ---- batch: 050 ----
mean loss: 119.50
 ---- batch: 060 ----
mean loss: 120.18
 ---- batch: 070 ----
mean loss: 126.07
 ---- batch: 080 ----
mean loss: 131.77
 ---- batch: 090 ----
mean loss: 119.78
 ---- batch: 100 ----
mean loss: 118.79
 ---- batch: 110 ----
mean loss: 127.06
train mean loss: 122.20
epoch train time: 0:00:01.916769
elapsed time: 0:07:40.044927
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 23:20:22.059991
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.82
 ---- batch: 020 ----
mean loss: 124.36
 ---- batch: 030 ----
mean loss: 127.81
 ---- batch: 040 ----
mean loss: 116.15
 ---- batch: 050 ----
mean loss: 121.39
 ---- batch: 060 ----
mean loss: 122.01
 ---- batch: 070 ----
mean loss: 118.40
 ---- batch: 080 ----
mean loss: 122.04
 ---- batch: 090 ----
mean loss: 115.97
 ---- batch: 100 ----
mean loss: 125.62
 ---- batch: 110 ----
mean loss: 121.61
train mean loss: 122.30
epoch train time: 0:00:01.903367
elapsed time: 0:07:41.948847
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 23:20:23.963902
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.55
 ---- batch: 020 ----
mean loss: 123.19
 ---- batch: 030 ----
mean loss: 116.71
 ---- batch: 040 ----
mean loss: 120.05
 ---- batch: 050 ----
mean loss: 127.82
 ---- batch: 060 ----
mean loss: 124.92
 ---- batch: 070 ----
mean loss: 123.10
 ---- batch: 080 ----
mean loss: 125.42
 ---- batch: 090 ----
mean loss: 126.92
 ---- batch: 100 ----
mean loss: 120.86
 ---- batch: 110 ----
mean loss: 116.05
train mean loss: 122.23
epoch train time: 0:00:01.920470
elapsed time: 0:07:43.869873
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 23:20:25.884966
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.21
 ---- batch: 020 ----
mean loss: 122.61
 ---- batch: 030 ----
mean loss: 119.92
 ---- batch: 040 ----
mean loss: 121.74
 ---- batch: 050 ----
mean loss: 123.71
 ---- batch: 060 ----
mean loss: 127.38
 ---- batch: 070 ----
mean loss: 122.38
 ---- batch: 080 ----
mean loss: 128.65
 ---- batch: 090 ----
mean loss: 121.21
 ---- batch: 100 ----
mean loss: 112.83
 ---- batch: 110 ----
mean loss: 126.53
train mean loss: 122.22
epoch train time: 0:00:01.871909
elapsed time: 0:07:45.742366
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 23:20:27.757455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.12
 ---- batch: 020 ----
mean loss: 118.29
 ---- batch: 030 ----
mean loss: 122.19
 ---- batch: 040 ----
mean loss: 123.64
 ---- batch: 050 ----
mean loss: 117.61
 ---- batch: 060 ----
mean loss: 127.44
 ---- batch: 070 ----
mean loss: 116.20
 ---- batch: 080 ----
mean loss: 117.00
 ---- batch: 090 ----
mean loss: 123.52
 ---- batch: 100 ----
mean loss: 131.65
 ---- batch: 110 ----
mean loss: 121.46
train mean loss: 122.25
epoch train time: 0:00:01.944709
elapsed time: 0:07:47.687681
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 23:20:29.702775
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.25
 ---- batch: 020 ----
mean loss: 130.22
 ---- batch: 030 ----
mean loss: 124.72
 ---- batch: 040 ----
mean loss: 120.33
 ---- batch: 050 ----
mean loss: 122.58
 ---- batch: 060 ----
mean loss: 114.34
 ---- batch: 070 ----
mean loss: 132.58
 ---- batch: 080 ----
mean loss: 116.04
 ---- batch: 090 ----
mean loss: 121.13
 ---- batch: 100 ----
mean loss: 120.85
 ---- batch: 110 ----
mean loss: 122.51
train mean loss: 122.03
epoch train time: 0:00:01.923872
elapsed time: 0:07:49.612147
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 23:20:31.627234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.10
 ---- batch: 020 ----
mean loss: 117.89
 ---- batch: 030 ----
mean loss: 122.38
 ---- batch: 040 ----
mean loss: 116.08
 ---- batch: 050 ----
mean loss: 119.69
 ---- batch: 060 ----
mean loss: 124.89
 ---- batch: 070 ----
mean loss: 126.23
 ---- batch: 080 ----
mean loss: 131.50
 ---- batch: 090 ----
mean loss: 123.74
 ---- batch: 100 ----
mean loss: 123.19
 ---- batch: 110 ----
mean loss: 123.72
train mean loss: 122.00
epoch train time: 0:00:01.925527
elapsed time: 0:07:51.538279
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 23:20:33.553374
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.70
 ---- batch: 020 ----
mean loss: 119.08
 ---- batch: 030 ----
mean loss: 120.71
 ---- batch: 040 ----
mean loss: 126.96
 ---- batch: 050 ----
mean loss: 123.80
 ---- batch: 060 ----
mean loss: 121.99
 ---- batch: 070 ----
mean loss: 128.09
 ---- batch: 080 ----
mean loss: 118.99
 ---- batch: 090 ----
mean loss: 119.56
 ---- batch: 100 ----
mean loss: 122.12
 ---- batch: 110 ----
mean loss: 120.66
train mean loss: 121.98
epoch train time: 0:00:01.951738
elapsed time: 0:07:53.490621
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 23:20:35.505734
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.19
 ---- batch: 020 ----
mean loss: 122.96
 ---- batch: 030 ----
mean loss: 126.77
 ---- batch: 040 ----
mean loss: 130.74
 ---- batch: 050 ----
mean loss: 120.23
 ---- batch: 060 ----
mean loss: 121.78
 ---- batch: 070 ----
mean loss: 116.60
 ---- batch: 080 ----
mean loss: 114.21
 ---- batch: 090 ----
mean loss: 122.94
 ---- batch: 100 ----
mean loss: 124.50
 ---- batch: 110 ----
mean loss: 122.12
train mean loss: 121.95
epoch train time: 0:00:01.936437
elapsed time: 0:07:55.427670
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 23:20:37.442737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.58
 ---- batch: 020 ----
mean loss: 115.72
 ---- batch: 030 ----
mean loss: 117.16
 ---- batch: 040 ----
mean loss: 129.12
 ---- batch: 050 ----
mean loss: 122.30
 ---- batch: 060 ----
mean loss: 121.60
 ---- batch: 070 ----
mean loss: 122.96
 ---- batch: 080 ----
mean loss: 121.08
 ---- batch: 090 ----
mean loss: 119.51
 ---- batch: 100 ----
mean loss: 120.44
 ---- batch: 110 ----
mean loss: 127.35
train mean loss: 122.06
epoch train time: 0:00:01.922234
elapsed time: 0:07:57.350478
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 23:20:39.365542
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.80
 ---- batch: 020 ----
mean loss: 117.52
 ---- batch: 030 ----
mean loss: 118.12
 ---- batch: 040 ----
mean loss: 126.85
 ---- batch: 050 ----
mean loss: 122.70
 ---- batch: 060 ----
mean loss: 113.09
 ---- batch: 070 ----
mean loss: 122.83
 ---- batch: 080 ----
mean loss: 125.16
 ---- batch: 090 ----
mean loss: 125.41
 ---- batch: 100 ----
mean loss: 131.83
 ---- batch: 110 ----
mean loss: 124.88
train mean loss: 121.83
epoch train time: 0:00:01.939836
elapsed time: 0:07:59.290889
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 23:20:41.306021
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.76
 ---- batch: 020 ----
mean loss: 123.91
 ---- batch: 030 ----
mean loss: 125.03
 ---- batch: 040 ----
mean loss: 123.93
 ---- batch: 050 ----
mean loss: 111.09
 ---- batch: 060 ----
mean loss: 123.63
 ---- batch: 070 ----
mean loss: 123.92
 ---- batch: 080 ----
mean loss: 122.83
 ---- batch: 090 ----
mean loss: 124.67
 ---- batch: 100 ----
mean loss: 117.21
 ---- batch: 110 ----
mean loss: 122.71
train mean loss: 121.92
epoch train time: 0:00:01.934323
elapsed time: 0:08:01.225833
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 23:20:43.240902
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.21
 ---- batch: 020 ----
mean loss: 112.21
 ---- batch: 030 ----
mean loss: 120.99
 ---- batch: 040 ----
mean loss: 123.38
 ---- batch: 050 ----
mean loss: 115.71
 ---- batch: 060 ----
mean loss: 125.06
 ---- batch: 070 ----
mean loss: 124.50
 ---- batch: 080 ----
mean loss: 125.86
 ---- batch: 090 ----
mean loss: 121.42
 ---- batch: 100 ----
mean loss: 126.93
 ---- batch: 110 ----
mean loss: 125.48
train mean loss: 121.84
epoch train time: 0:00:01.934942
elapsed time: 0:08:03.161671
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 23:20:45.176461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.52
 ---- batch: 020 ----
mean loss: 120.47
 ---- batch: 030 ----
mean loss: 119.57
 ---- batch: 040 ----
mean loss: 121.18
 ---- batch: 050 ----
mean loss: 120.88
 ---- batch: 060 ----
mean loss: 124.46
 ---- batch: 070 ----
mean loss: 116.78
 ---- batch: 080 ----
mean loss: 125.00
 ---- batch: 090 ----
mean loss: 127.82
 ---- batch: 100 ----
mean loss: 124.08
 ---- batch: 110 ----
mean loss: 119.74
train mean loss: 121.78
epoch train time: 0:00:01.939014
elapsed time: 0:08:05.100962
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 23:20:47.116021
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.80
 ---- batch: 020 ----
mean loss: 116.51
 ---- batch: 030 ----
mean loss: 117.38
 ---- batch: 040 ----
mean loss: 134.33
 ---- batch: 050 ----
mean loss: 118.80
 ---- batch: 060 ----
mean loss: 120.87
 ---- batch: 070 ----
mean loss: 123.86
 ---- batch: 080 ----
mean loss: 116.70
 ---- batch: 090 ----
mean loss: 123.79
 ---- batch: 100 ----
mean loss: 123.33
 ---- batch: 110 ----
mean loss: 119.47
train mean loss: 121.79
epoch train time: 0:00:01.909512
elapsed time: 0:08:07.011032
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 23:20:49.026102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.05
 ---- batch: 020 ----
mean loss: 122.56
 ---- batch: 030 ----
mean loss: 125.01
 ---- batch: 040 ----
mean loss: 126.16
 ---- batch: 050 ----
mean loss: 121.63
 ---- batch: 060 ----
mean loss: 132.80
 ---- batch: 070 ----
mean loss: 123.57
 ---- batch: 080 ----
mean loss: 117.02
 ---- batch: 090 ----
mean loss: 117.18
 ---- batch: 100 ----
mean loss: 122.67
 ---- batch: 110 ----
mean loss: 114.14
train mean loss: 121.78
epoch train time: 0:00:01.912613
elapsed time: 0:08:08.924222
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 23:20:50.939285
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.36
 ---- batch: 020 ----
mean loss: 130.39
 ---- batch: 030 ----
mean loss: 116.78
 ---- batch: 040 ----
mean loss: 120.27
 ---- batch: 050 ----
mean loss: 120.10
 ---- batch: 060 ----
mean loss: 123.63
 ---- batch: 070 ----
mean loss: 117.46
 ---- batch: 080 ----
mean loss: 120.23
 ---- batch: 090 ----
mean loss: 122.84
 ---- batch: 100 ----
mean loss: 122.25
 ---- batch: 110 ----
mean loss: 119.07
train mean loss: 121.79
epoch train time: 0:00:01.910594
elapsed time: 0:08:10.835406
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 23:20:52.850476
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.38
 ---- batch: 020 ----
mean loss: 127.25
 ---- batch: 030 ----
mean loss: 115.83
 ---- batch: 040 ----
mean loss: 118.96
 ---- batch: 050 ----
mean loss: 125.92
 ---- batch: 060 ----
mean loss: 125.06
 ---- batch: 070 ----
mean loss: 130.81
 ---- batch: 080 ----
mean loss: 114.14
 ---- batch: 090 ----
mean loss: 117.31
 ---- batch: 100 ----
mean loss: 124.78
 ---- batch: 110 ----
mean loss: 118.57
train mean loss: 121.81
epoch train time: 0:00:01.900893
elapsed time: 0:08:12.736885
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 23:20:54.751921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.95
 ---- batch: 020 ----
mean loss: 124.49
 ---- batch: 030 ----
mean loss: 124.59
 ---- batch: 040 ----
mean loss: 123.32
 ---- batch: 050 ----
mean loss: 119.81
 ---- batch: 060 ----
mean loss: 130.18
 ---- batch: 070 ----
mean loss: 124.24
 ---- batch: 080 ----
mean loss: 123.81
 ---- batch: 090 ----
mean loss: 119.25
 ---- batch: 100 ----
mean loss: 118.82
 ---- batch: 110 ----
mean loss: 115.65
train mean loss: 121.56
epoch train time: 0:00:01.900944
elapsed time: 0:08:14.638357
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 23:20:56.653439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.59
 ---- batch: 020 ----
mean loss: 119.64
 ---- batch: 030 ----
mean loss: 126.10
 ---- batch: 040 ----
mean loss: 117.81
 ---- batch: 050 ----
mean loss: 119.08
 ---- batch: 060 ----
mean loss: 121.25
 ---- batch: 070 ----
mean loss: 123.78
 ---- batch: 080 ----
mean loss: 121.52
 ---- batch: 090 ----
mean loss: 134.48
 ---- batch: 100 ----
mean loss: 113.24
 ---- batch: 110 ----
mean loss: 122.51
train mean loss: 121.70
epoch train time: 0:00:01.937164
elapsed time: 0:08:16.576106
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 23:20:58.591195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.16
 ---- batch: 020 ----
mean loss: 117.50
 ---- batch: 030 ----
mean loss: 116.38
 ---- batch: 040 ----
mean loss: 121.10
 ---- batch: 050 ----
mean loss: 126.03
 ---- batch: 060 ----
mean loss: 122.45
 ---- batch: 070 ----
mean loss: 125.28
 ---- batch: 080 ----
mean loss: 125.78
 ---- batch: 090 ----
mean loss: 123.41
 ---- batch: 100 ----
mean loss: 126.45
 ---- batch: 110 ----
mean loss: 117.65
train mean loss: 121.52
epoch train time: 0:00:01.903878
elapsed time: 0:08:18.480581
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 23:21:00.495711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.25
 ---- batch: 020 ----
mean loss: 130.05
 ---- batch: 030 ----
mean loss: 124.29
 ---- batch: 040 ----
mean loss: 114.41
 ---- batch: 050 ----
mean loss: 119.50
 ---- batch: 060 ----
mean loss: 126.99
 ---- batch: 070 ----
mean loss: 123.62
 ---- batch: 080 ----
mean loss: 113.47
 ---- batch: 090 ----
mean loss: 112.30
 ---- batch: 100 ----
mean loss: 125.90
 ---- batch: 110 ----
mean loss: 128.47
train mean loss: 121.60
epoch train time: 0:00:01.924039
elapsed time: 0:08:20.405242
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 23:21:02.420298
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.93
 ---- batch: 020 ----
mean loss: 126.90
 ---- batch: 030 ----
mean loss: 114.56
 ---- batch: 040 ----
mean loss: 126.99
 ---- batch: 050 ----
mean loss: 123.71
 ---- batch: 060 ----
mean loss: 116.65
 ---- batch: 070 ----
mean loss: 125.65
 ---- batch: 080 ----
mean loss: 118.71
 ---- batch: 090 ----
mean loss: 118.47
 ---- batch: 100 ----
mean loss: 125.26
 ---- batch: 110 ----
mean loss: 120.73
train mean loss: 121.60
epoch train time: 0:00:01.941475
elapsed time: 0:08:22.347269
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 23:21:04.362385
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.69
 ---- batch: 020 ----
mean loss: 112.72
 ---- batch: 030 ----
mean loss: 118.76
 ---- batch: 040 ----
mean loss: 125.56
 ---- batch: 050 ----
mean loss: 122.57
 ---- batch: 060 ----
mean loss: 122.85
 ---- batch: 070 ----
mean loss: 127.34
 ---- batch: 080 ----
mean loss: 122.35
 ---- batch: 090 ----
mean loss: 118.88
 ---- batch: 100 ----
mean loss: 126.30
 ---- batch: 110 ----
mean loss: 120.68
train mean loss: 121.50
epoch train time: 0:00:01.914906
elapsed time: 0:08:24.262792
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 23:21:06.277892
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.91
 ---- batch: 020 ----
mean loss: 121.57
 ---- batch: 030 ----
mean loss: 124.04
 ---- batch: 040 ----
mean loss: 119.95
 ---- batch: 050 ----
mean loss: 114.90
 ---- batch: 060 ----
mean loss: 124.26
 ---- batch: 070 ----
mean loss: 122.74
 ---- batch: 080 ----
mean loss: 115.14
 ---- batch: 090 ----
mean loss: 120.40
 ---- batch: 100 ----
mean loss: 127.78
 ---- batch: 110 ----
mean loss: 129.54
train mean loss: 121.52
epoch train time: 0:00:01.919792
elapsed time: 0:08:26.183196
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 23:21:08.198267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.00
 ---- batch: 020 ----
mean loss: 120.52
 ---- batch: 030 ----
mean loss: 120.12
 ---- batch: 040 ----
mean loss: 118.85
 ---- batch: 050 ----
mean loss: 121.76
 ---- batch: 060 ----
mean loss: 119.95
 ---- batch: 070 ----
mean loss: 126.93
 ---- batch: 080 ----
mean loss: 122.23
 ---- batch: 090 ----
mean loss: 122.69
 ---- batch: 100 ----
mean loss: 125.10
 ---- batch: 110 ----
mean loss: 115.24
train mean loss: 121.50
epoch train time: 0:00:01.943255
elapsed time: 0:08:28.127051
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 23:21:10.142190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.22
 ---- batch: 020 ----
mean loss: 120.27
 ---- batch: 030 ----
mean loss: 126.36
 ---- batch: 040 ----
mean loss: 119.93
 ---- batch: 050 ----
mean loss: 123.31
 ---- batch: 060 ----
mean loss: 122.46
 ---- batch: 070 ----
mean loss: 122.51
 ---- batch: 080 ----
mean loss: 118.14
 ---- batch: 090 ----
mean loss: 118.52
 ---- batch: 100 ----
mean loss: 124.69
 ---- batch: 110 ----
mean loss: 129.40
train mean loss: 121.40
epoch train time: 0:00:01.930233
elapsed time: 0:08:30.057911
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 23:21:12.072986
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.65
 ---- batch: 020 ----
mean loss: 118.87
 ---- batch: 030 ----
mean loss: 119.03
 ---- batch: 040 ----
mean loss: 120.17
 ---- batch: 050 ----
mean loss: 118.41
 ---- batch: 060 ----
mean loss: 120.46
 ---- batch: 070 ----
mean loss: 117.74
 ---- batch: 080 ----
mean loss: 120.98
 ---- batch: 090 ----
mean loss: 126.38
 ---- batch: 100 ----
mean loss: 126.40
 ---- batch: 110 ----
mean loss: 125.18
train mean loss: 121.37
epoch train time: 0:00:01.943760
elapsed time: 0:08:32.002320
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 23:21:14.017417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.11
 ---- batch: 020 ----
mean loss: 120.48
 ---- batch: 030 ----
mean loss: 116.99
 ---- batch: 040 ----
mean loss: 119.94
 ---- batch: 050 ----
mean loss: 117.80
 ---- batch: 060 ----
mean loss: 124.82
 ---- batch: 070 ----
mean loss: 118.11
 ---- batch: 080 ----
mean loss: 124.69
 ---- batch: 090 ----
mean loss: 122.95
 ---- batch: 100 ----
mean loss: 128.52
 ---- batch: 110 ----
mean loss: 119.84
train mean loss: 121.35
epoch train time: 0:00:01.935241
elapsed time: 0:08:33.945649
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_6/checkpoint.pth.tar
**** end time: 2019-09-26 23:21:15.960371 ****
