Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_4', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18093
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistDense3...
Done.
**** start time: 2019-09-27 02:16:28.295568 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
            Linear-2                  [-1, 100]          36,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 56,100
Trainable params: 56,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:16:28.298886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4932.64
 ---- batch: 020 ----
mean loss: 4808.74
 ---- batch: 030 ----
mean loss: 4693.15
 ---- batch: 040 ----
mean loss: 4576.69
 ---- batch: 050 ----
mean loss: 4501.46
 ---- batch: 060 ----
mean loss: 4391.51
 ---- batch: 070 ----
mean loss: 4356.72
 ---- batch: 080 ----
mean loss: 4287.07
 ---- batch: 090 ----
mean loss: 4222.04
 ---- batch: 100 ----
mean loss: 4195.10
 ---- batch: 110 ----
mean loss: 4161.89
train mean loss: 4456.18
epoch train time: 0:00:33.176534
elapsed time: 0:00:33.182408
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:17:01.478033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4046.98
 ---- batch: 020 ----
mean loss: 3958.90
 ---- batch: 030 ----
mean loss: 3934.13
 ---- batch: 040 ----
mean loss: 3870.38
 ---- batch: 050 ----
mean loss: 3850.41
 ---- batch: 060 ----
mean loss: 3763.08
 ---- batch: 070 ----
mean loss: 3676.02
 ---- batch: 080 ----
mean loss: 3668.29
 ---- batch: 090 ----
mean loss: 3583.57
 ---- batch: 100 ----
mean loss: 3495.05
 ---- batch: 110 ----
mean loss: 3412.44
train mean loss: 3743.57
epoch train time: 0:00:00.579317
elapsed time: 0:00:33.761881
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:17:02.057494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3391.37
 ---- batch: 020 ----
mean loss: 3334.36
 ---- batch: 030 ----
mean loss: 3307.18
 ---- batch: 040 ----
mean loss: 3246.98
 ---- batch: 050 ----
mean loss: 3183.76
 ---- batch: 060 ----
mean loss: 3120.16
 ---- batch: 070 ----
mean loss: 3092.92
 ---- batch: 080 ----
mean loss: 3008.20
 ---- batch: 090 ----
mean loss: 2948.38
 ---- batch: 100 ----
mean loss: 2944.78
 ---- batch: 110 ----
mean loss: 2818.27
train mean loss: 3120.79
epoch train time: 0:00:00.563998
elapsed time: 0:00:34.326011
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:17:02.621662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2801.47
 ---- batch: 020 ----
mean loss: 2715.63
 ---- batch: 030 ----
mean loss: 2744.70
 ---- batch: 040 ----
mean loss: 2706.35
 ---- batch: 050 ----
mean loss: 2624.52
 ---- batch: 060 ----
mean loss: 2615.52
 ---- batch: 070 ----
mean loss: 2568.73
 ---- batch: 080 ----
mean loss: 2567.12
 ---- batch: 090 ----
mean loss: 2472.04
 ---- batch: 100 ----
mean loss: 2455.42
 ---- batch: 110 ----
mean loss: 2415.71
train mean loss: 2602.10
epoch train time: 0:00:00.565729
elapsed time: 0:00:34.891913
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:17:03.187529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2403.54
 ---- batch: 020 ----
mean loss: 2340.07
 ---- batch: 030 ----
mean loss: 2289.41
 ---- batch: 040 ----
mean loss: 2251.05
 ---- batch: 050 ----
mean loss: 2232.05
 ---- batch: 060 ----
mean loss: 2186.67
 ---- batch: 070 ----
mean loss: 2135.74
 ---- batch: 080 ----
mean loss: 2101.41
 ---- batch: 090 ----
mean loss: 2095.50
 ---- batch: 100 ----
mean loss: 2080.23
 ---- batch: 110 ----
mean loss: 2060.20
train mean loss: 2192.61
epoch train time: 0:00:00.559563
elapsed time: 0:00:35.451611
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:17:03.747226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2018.28
 ---- batch: 020 ----
mean loss: 1984.90
 ---- batch: 030 ----
mean loss: 1950.35
 ---- batch: 040 ----
mean loss: 1956.37
 ---- batch: 050 ----
mean loss: 1878.51
 ---- batch: 060 ----
mean loss: 1853.35
 ---- batch: 070 ----
mean loss: 1822.10
 ---- batch: 080 ----
mean loss: 1793.71
 ---- batch: 090 ----
mean loss: 1786.37
 ---- batch: 100 ----
mean loss: 1782.21
 ---- batch: 110 ----
mean loss: 1743.56
train mean loss: 1866.79
epoch train time: 0:00:00.567085
elapsed time: 0:00:36.018834
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:17:04.314471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1722.59
 ---- batch: 020 ----
mean loss: 1690.87
 ---- batch: 030 ----
mean loss: 1658.15
 ---- batch: 040 ----
mean loss: 1647.79
 ---- batch: 050 ----
mean loss: 1663.28
 ---- batch: 060 ----
mean loss: 1574.43
 ---- batch: 070 ----
mean loss: 1569.01
 ---- batch: 080 ----
mean loss: 1546.06
 ---- batch: 090 ----
mean loss: 1542.76
 ---- batch: 100 ----
mean loss: 1505.37
 ---- batch: 110 ----
mean loss: 1525.28
train mean loss: 1601.04
epoch train time: 0:00:00.583199
elapsed time: 0:00:36.602195
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:17:04.897830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1460.33
 ---- batch: 020 ----
mean loss: 1471.91
 ---- batch: 030 ----
mean loss: 1426.70
 ---- batch: 040 ----
mean loss: 1407.82
 ---- batch: 050 ----
mean loss: 1405.18
 ---- batch: 060 ----
mean loss: 1387.95
 ---- batch: 070 ----
mean loss: 1369.75
 ---- batch: 080 ----
mean loss: 1356.42
 ---- batch: 090 ----
mean loss: 1359.90
 ---- batch: 100 ----
mean loss: 1344.13
 ---- batch: 110 ----
mean loss: 1286.17
train mean loss: 1386.93
epoch train time: 0:00:00.568549
elapsed time: 0:00:37.170913
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:17:05.466530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1275.96
 ---- batch: 020 ----
mean loss: 1283.26
 ---- batch: 030 ----
mean loss: 1251.45
 ---- batch: 040 ----
mean loss: 1231.74
 ---- batch: 050 ----
mean loss: 1234.73
 ---- batch: 060 ----
mean loss: 1232.70
 ---- batch: 070 ----
mean loss: 1216.50
 ---- batch: 080 ----
mean loss: 1199.46
 ---- batch: 090 ----
mean loss: 1184.36
 ---- batch: 100 ----
mean loss: 1183.76
 ---- batch: 110 ----
mean loss: 1176.95
train mean loss: 1222.08
epoch train time: 0:00:00.579391
elapsed time: 0:00:37.750440
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:17:06.046058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1152.31
 ---- batch: 020 ----
mean loss: 1149.39
 ---- batch: 030 ----
mean loss: 1122.02
 ---- batch: 040 ----
mean loss: 1111.26
 ---- batch: 050 ----
mean loss: 1106.89
 ---- batch: 060 ----
mean loss: 1106.52
 ---- batch: 070 ----
mean loss: 1101.45
 ---- batch: 080 ----
mean loss: 1087.47
 ---- batch: 090 ----
mean loss: 1081.66
 ---- batch: 100 ----
mean loss: 1056.92
 ---- batch: 110 ----
mean loss: 1072.14
train mean loss: 1103.31
epoch train time: 0:00:00.559074
elapsed time: 0:00:38.309680
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:17:06.605297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1048.75
 ---- batch: 020 ----
mean loss: 1039.88
 ---- batch: 030 ----
mean loss: 1040.21
 ---- batch: 040 ----
mean loss: 1034.53
 ---- batch: 050 ----
mean loss: 1025.55
 ---- batch: 060 ----
mean loss: 1018.06
 ---- batch: 070 ----
mean loss: 1000.22
 ---- batch: 080 ----
mean loss: 1004.01
 ---- batch: 090 ----
mean loss: 1006.65
 ---- batch: 100 ----
mean loss: 1003.30
 ---- batch: 110 ----
mean loss: 983.27
train mean loss: 1017.78
epoch train time: 0:00:00.564723
elapsed time: 0:00:38.874537
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:17:07.170182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 990.32
 ---- batch: 020 ----
mean loss: 974.19
 ---- batch: 030 ----
mean loss: 970.39
 ---- batch: 040 ----
mean loss: 959.70
 ---- batch: 050 ----
mean loss: 948.48
 ---- batch: 060 ----
mean loss: 955.97
 ---- batch: 070 ----
mean loss: 958.91
 ---- batch: 080 ----
mean loss: 948.13
 ---- batch: 090 ----
mean loss: 949.57
 ---- batch: 100 ----
mean loss: 944.33
 ---- batch: 110 ----
mean loss: 930.66
train mean loss: 957.31
epoch train time: 0:00:00.554913
elapsed time: 0:00:39.429627
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:17:07.725239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.81
 ---- batch: 020 ----
mean loss: 937.38
 ---- batch: 030 ----
mean loss: 930.32
 ---- batch: 040 ----
mean loss: 921.21
 ---- batch: 050 ----
mean loss: 913.30
 ---- batch: 060 ----
mean loss: 898.05
 ---- batch: 070 ----
mean loss: 923.23
 ---- batch: 080 ----
mean loss: 898.57
 ---- batch: 090 ----
mean loss: 904.41
 ---- batch: 100 ----
mean loss: 915.30
 ---- batch: 110 ----
mean loss: 890.89
train mean loss: 915.68
epoch train time: 0:00:00.578645
elapsed time: 0:00:40.008406
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:17:08.304057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.09
 ---- batch: 020 ----
mean loss: 890.14
 ---- batch: 030 ----
mean loss: 891.52
 ---- batch: 040 ----
mean loss: 884.53
 ---- batch: 050 ----
mean loss: 886.31
 ---- batch: 060 ----
mean loss: 896.75
 ---- batch: 070 ----
mean loss: 889.32
 ---- batch: 080 ----
mean loss: 888.77
 ---- batch: 090 ----
mean loss: 877.69
 ---- batch: 100 ----
mean loss: 885.63
 ---- batch: 110 ----
mean loss: 885.39
train mean loss: 888.04
epoch train time: 0:00:00.573749
elapsed time: 0:00:40.582332
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:17:08.877949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.76
 ---- batch: 020 ----
mean loss: 872.62
 ---- batch: 030 ----
mean loss: 888.53
 ---- batch: 040 ----
mean loss: 885.88
 ---- batch: 050 ----
mean loss: 870.61
 ---- batch: 060 ----
mean loss: 863.36
 ---- batch: 070 ----
mean loss: 862.24
 ---- batch: 080 ----
mean loss: 857.97
 ---- batch: 090 ----
mean loss: 866.62
 ---- batch: 100 ----
mean loss: 858.08
 ---- batch: 110 ----
mean loss: 876.81
train mean loss: 870.08
epoch train time: 0:00:00.561485
elapsed time: 0:00:41.143948
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:17:09.439561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.79
 ---- batch: 020 ----
mean loss: 866.55
 ---- batch: 030 ----
mean loss: 857.85
 ---- batch: 040 ----
mean loss: 846.82
 ---- batch: 050 ----
mean loss: 856.85
 ---- batch: 060 ----
mean loss: 861.84
 ---- batch: 070 ----
mean loss: 871.83
 ---- batch: 080 ----
mean loss: 848.40
 ---- batch: 090 ----
mean loss: 851.49
 ---- batch: 100 ----
mean loss: 864.12
 ---- batch: 110 ----
mean loss: 845.12
train mean loss: 859.01
epoch train time: 0:00:00.569703
elapsed time: 0:00:41.713782
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:17:10.009406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.30
 ---- batch: 020 ----
mean loss: 828.26
 ---- batch: 030 ----
mean loss: 852.91
 ---- batch: 040 ----
mean loss: 870.42
 ---- batch: 050 ----
mean loss: 869.84
 ---- batch: 060 ----
mean loss: 866.43
 ---- batch: 070 ----
mean loss: 860.38
 ---- batch: 080 ----
mean loss: 852.20
 ---- batch: 090 ----
mean loss: 838.76
 ---- batch: 100 ----
mean loss: 842.54
 ---- batch: 110 ----
mean loss: 844.24
train mean loss: 852.32
epoch train time: 0:00:00.560779
elapsed time: 0:00:42.274715
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:17:10.570329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.33
 ---- batch: 020 ----
mean loss: 853.19
 ---- batch: 030 ----
mean loss: 837.09
 ---- batch: 040 ----
mean loss: 849.41
 ---- batch: 050 ----
mean loss: 862.70
 ---- batch: 060 ----
mean loss: 829.34
 ---- batch: 070 ----
mean loss: 861.78
 ---- batch: 080 ----
mean loss: 844.45
 ---- batch: 090 ----
mean loss: 844.85
 ---- batch: 100 ----
mean loss: 862.54
 ---- batch: 110 ----
mean loss: 857.97
train mean loss: 848.28
epoch train time: 0:00:00.565293
elapsed time: 0:00:42.840154
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:17:11.135784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.38
 ---- batch: 020 ----
mean loss: 857.78
 ---- batch: 030 ----
mean loss: 845.00
 ---- batch: 040 ----
mean loss: 825.89
 ---- batch: 050 ----
mean loss: 837.29
 ---- batch: 060 ----
mean loss: 852.96
 ---- batch: 070 ----
mean loss: 842.83
 ---- batch: 080 ----
mean loss: 843.95
 ---- batch: 090 ----
mean loss: 853.49
 ---- batch: 100 ----
mean loss: 841.60
 ---- batch: 110 ----
mean loss: 863.79
train mean loss: 846.05
epoch train time: 0:00:00.570355
elapsed time: 0:00:43.410658
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:17:11.706274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.00
 ---- batch: 020 ----
mean loss: 853.64
 ---- batch: 030 ----
mean loss: 855.33
 ---- batch: 040 ----
mean loss: 839.92
 ---- batch: 050 ----
mean loss: 829.38
 ---- batch: 060 ----
mean loss: 852.12
 ---- batch: 070 ----
mean loss: 843.73
 ---- batch: 080 ----
mean loss: 849.16
 ---- batch: 090 ----
mean loss: 839.05
 ---- batch: 100 ----
mean loss: 846.99
 ---- batch: 110 ----
mean loss: 851.97
train mean loss: 844.97
epoch train time: 0:00:00.559590
elapsed time: 0:00:43.970390
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:17:12.266022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.95
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 843.68
 ---- batch: 040 ----
mean loss: 869.52
 ---- batch: 050 ----
mean loss: 848.04
 ---- batch: 060 ----
mean loss: 859.88
 ---- batch: 070 ----
mean loss: 840.79
 ---- batch: 080 ----
mean loss: 857.60
 ---- batch: 090 ----
mean loss: 831.03
 ---- batch: 100 ----
mean loss: 826.70
 ---- batch: 110 ----
mean loss: 828.33
train mean loss: 844.44
epoch train time: 0:00:00.556303
elapsed time: 0:00:44.526839
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:17:12.822452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.22
 ---- batch: 020 ----
mean loss: 856.54
 ---- batch: 030 ----
mean loss: 828.48
 ---- batch: 040 ----
mean loss: 858.93
 ---- batch: 050 ----
mean loss: 850.68
 ---- batch: 060 ----
mean loss: 841.78
 ---- batch: 070 ----
mean loss: 854.03
 ---- batch: 080 ----
mean loss: 837.87
 ---- batch: 090 ----
mean loss: 835.49
 ---- batch: 100 ----
mean loss: 831.99
 ---- batch: 110 ----
mean loss: 853.14
train mean loss: 844.17
epoch train time: 0:00:00.573481
elapsed time: 0:00:45.100456
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:17:13.396082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.62
 ---- batch: 020 ----
mean loss: 834.91
 ---- batch: 030 ----
mean loss: 819.32
 ---- batch: 040 ----
mean loss: 844.26
 ---- batch: 050 ----
mean loss: 871.31
 ---- batch: 060 ----
mean loss: 835.78
 ---- batch: 070 ----
mean loss: 864.70
 ---- batch: 080 ----
mean loss: 832.32
 ---- batch: 090 ----
mean loss: 846.11
 ---- batch: 100 ----
mean loss: 861.94
 ---- batch: 110 ----
mean loss: 844.30
train mean loss: 844.10
epoch train time: 0:00:00.577181
elapsed time: 0:00:45.677796
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:17:13.973421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.55
 ---- batch: 020 ----
mean loss: 841.70
 ---- batch: 030 ----
mean loss: 856.80
 ---- batch: 040 ----
mean loss: 844.16
 ---- batch: 050 ----
mean loss: 860.66
 ---- batch: 060 ----
mean loss: 834.29
 ---- batch: 070 ----
mean loss: 833.57
 ---- batch: 080 ----
mean loss: 858.24
 ---- batch: 090 ----
mean loss: 841.91
 ---- batch: 100 ----
mean loss: 851.77
 ---- batch: 110 ----
mean loss: 829.36
train mean loss: 844.02
epoch train time: 0:00:00.563373
elapsed time: 0:00:46.241332
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:17:14.536947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.91
 ---- batch: 020 ----
mean loss: 842.29
 ---- batch: 030 ----
mean loss: 827.41
 ---- batch: 040 ----
mean loss: 849.84
 ---- batch: 050 ----
mean loss: 846.95
 ---- batch: 060 ----
mean loss: 858.62
 ---- batch: 070 ----
mean loss: 818.97
 ---- batch: 080 ----
mean loss: 846.60
 ---- batch: 090 ----
mean loss: 854.55
 ---- batch: 100 ----
mean loss: 835.60
 ---- batch: 110 ----
mean loss: 854.64
train mean loss: 844.01
epoch train time: 0:00:00.572888
elapsed time: 0:00:46.814354
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:17:15.109969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.81
 ---- batch: 020 ----
mean loss: 845.54
 ---- batch: 030 ----
mean loss: 843.54
 ---- batch: 040 ----
mean loss: 840.51
 ---- batch: 050 ----
mean loss: 834.66
 ---- batch: 060 ----
mean loss: 852.34
 ---- batch: 070 ----
mean loss: 864.83
 ---- batch: 080 ----
mean loss: 830.97
 ---- batch: 090 ----
mean loss: 848.36
 ---- batch: 100 ----
mean loss: 838.76
 ---- batch: 110 ----
mean loss: 838.40
train mean loss: 843.97
epoch train time: 0:00:00.564857
elapsed time: 0:00:47.379408
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:17:15.675049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.06
 ---- batch: 020 ----
mean loss: 852.36
 ---- batch: 030 ----
mean loss: 858.96
 ---- batch: 040 ----
mean loss: 845.34
 ---- batch: 050 ----
mean loss: 839.66
 ---- batch: 060 ----
mean loss: 819.42
 ---- batch: 070 ----
mean loss: 828.52
 ---- batch: 080 ----
mean loss: 860.84
 ---- batch: 090 ----
mean loss: 863.11
 ---- batch: 100 ----
mean loss: 836.78
 ---- batch: 110 ----
mean loss: 837.83
train mean loss: 843.92
epoch train time: 0:00:00.564668
elapsed time: 0:00:47.944237
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:17:16.239853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.02
 ---- batch: 020 ----
mean loss: 834.66
 ---- batch: 030 ----
mean loss: 854.12
 ---- batch: 040 ----
mean loss: 865.55
 ---- batch: 050 ----
mean loss: 838.79
 ---- batch: 060 ----
mean loss: 839.51
 ---- batch: 070 ----
mean loss: 829.95
 ---- batch: 080 ----
mean loss: 849.24
 ---- batch: 090 ----
mean loss: 852.49
 ---- batch: 100 ----
mean loss: 835.78
 ---- batch: 110 ----
mean loss: 847.17
train mean loss: 843.84
epoch train time: 0:00:00.562815
elapsed time: 0:00:48.507189
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:17:16.802806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.63
 ---- batch: 020 ----
mean loss: 839.61
 ---- batch: 030 ----
mean loss: 857.26
 ---- batch: 040 ----
mean loss: 861.71
 ---- batch: 050 ----
mean loss: 858.47
 ---- batch: 060 ----
mean loss: 838.94
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 844.45
 ---- batch: 090 ----
mean loss: 822.49
 ---- batch: 100 ----
mean loss: 855.87
 ---- batch: 110 ----
mean loss: 837.78
train mean loss: 843.94
epoch train time: 0:00:00.567084
elapsed time: 0:00:49.074410
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:17:17.370043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 823.18
 ---- batch: 030 ----
mean loss: 834.62
 ---- batch: 040 ----
mean loss: 843.74
 ---- batch: 050 ----
mean loss: 847.57
 ---- batch: 060 ----
mean loss: 842.48
 ---- batch: 070 ----
mean loss: 847.41
 ---- batch: 080 ----
mean loss: 859.33
 ---- batch: 090 ----
mean loss: 843.55
 ---- batch: 100 ----
mean loss: 848.51
 ---- batch: 110 ----
mean loss: 841.92
train mean loss: 843.91
epoch train time: 0:00:00.566769
elapsed time: 0:00:49.641332
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:17:17.936964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.91
 ---- batch: 020 ----
mean loss: 839.22
 ---- batch: 030 ----
mean loss: 847.48
 ---- batch: 040 ----
mean loss: 860.20
 ---- batch: 050 ----
mean loss: 844.45
 ---- batch: 060 ----
mean loss: 840.43
 ---- batch: 070 ----
mean loss: 807.10
 ---- batch: 080 ----
mean loss: 851.61
 ---- batch: 090 ----
mean loss: 844.55
 ---- batch: 100 ----
mean loss: 855.05
 ---- batch: 110 ----
mean loss: 856.51
train mean loss: 844.02
epoch train time: 0:00:00.575695
elapsed time: 0:00:50.217180
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:17:18.512798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.92
 ---- batch: 020 ----
mean loss: 833.89
 ---- batch: 030 ----
mean loss: 864.36
 ---- batch: 040 ----
mean loss: 867.12
 ---- batch: 050 ----
mean loss: 818.03
 ---- batch: 060 ----
mean loss: 833.12
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 846.87
 ---- batch: 090 ----
mean loss: 842.99
 ---- batch: 100 ----
mean loss: 845.87
 ---- batch: 110 ----
mean loss: 831.11
train mean loss: 844.04
epoch train time: 0:00:00.583863
elapsed time: 0:00:50.801183
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:17:19.096801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.77
 ---- batch: 020 ----
mean loss: 832.53
 ---- batch: 030 ----
mean loss: 855.60
 ---- batch: 040 ----
mean loss: 864.32
 ---- batch: 050 ----
mean loss: 850.39
 ---- batch: 060 ----
mean loss: 854.95
 ---- batch: 070 ----
mean loss: 825.55
 ---- batch: 080 ----
mean loss: 851.27
 ---- batch: 090 ----
mean loss: 841.00
 ---- batch: 100 ----
mean loss: 851.35
 ---- batch: 110 ----
mean loss: 831.00
train mean loss: 843.87
epoch train time: 0:00:00.585359
elapsed time: 0:00:51.386680
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:17:19.682314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.60
 ---- batch: 020 ----
mean loss: 848.83
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 843.44
 ---- batch: 050 ----
mean loss: 824.87
 ---- batch: 060 ----
mean loss: 836.67
 ---- batch: 070 ----
mean loss: 843.42
 ---- batch: 080 ----
mean loss: 858.18
 ---- batch: 090 ----
mean loss: 855.87
 ---- batch: 100 ----
mean loss: 850.60
 ---- batch: 110 ----
mean loss: 855.83
train mean loss: 843.97
epoch train time: 0:00:00.591125
elapsed time: 0:00:51.977961
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:17:20.273584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.51
 ---- batch: 020 ----
mean loss: 814.73
 ---- batch: 030 ----
mean loss: 838.88
 ---- batch: 040 ----
mean loss: 851.10
 ---- batch: 050 ----
mean loss: 848.75
 ---- batch: 060 ----
mean loss: 852.61
 ---- batch: 070 ----
mean loss: 834.72
 ---- batch: 080 ----
mean loss: 850.42
 ---- batch: 090 ----
mean loss: 854.91
 ---- batch: 100 ----
mean loss: 853.50
 ---- batch: 110 ----
mean loss: 840.83
train mean loss: 844.05
epoch train time: 0:00:00.593138
elapsed time: 0:00:52.571240
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:17:20.866857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.12
 ---- batch: 020 ----
mean loss: 861.64
 ---- batch: 030 ----
mean loss: 838.11
 ---- batch: 040 ----
mean loss: 845.01
 ---- batch: 050 ----
mean loss: 836.21
 ---- batch: 060 ----
mean loss: 835.64
 ---- batch: 070 ----
mean loss: 842.72
 ---- batch: 080 ----
mean loss: 851.18
 ---- batch: 090 ----
mean loss: 833.50
 ---- batch: 100 ----
mean loss: 834.01
 ---- batch: 110 ----
mean loss: 834.96
train mean loss: 844.04
epoch train time: 0:00:00.572643
elapsed time: 0:00:53.144027
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:17:21.439636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.26
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 855.51
 ---- batch: 040 ----
mean loss: 842.87
 ---- batch: 050 ----
mean loss: 826.74
 ---- batch: 060 ----
mean loss: 850.21
 ---- batch: 070 ----
mean loss: 835.43
 ---- batch: 080 ----
mean loss: 827.14
 ---- batch: 090 ----
mean loss: 864.25
 ---- batch: 100 ----
mean loss: 852.65
 ---- batch: 110 ----
mean loss: 827.14
train mean loss: 843.95
epoch train time: 0:00:00.581206
elapsed time: 0:00:53.725374
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:17:22.020990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.02
 ---- batch: 020 ----
mean loss: 855.72
 ---- batch: 030 ----
mean loss: 830.98
 ---- batch: 040 ----
mean loss: 838.38
 ---- batch: 050 ----
mean loss: 868.38
 ---- batch: 060 ----
mean loss: 833.24
 ---- batch: 070 ----
mean loss: 839.23
 ---- batch: 080 ----
mean loss: 829.76
 ---- batch: 090 ----
mean loss: 826.33
 ---- batch: 100 ----
mean loss: 855.06
 ---- batch: 110 ----
mean loss: 855.24
train mean loss: 843.94
epoch train time: 0:00:00.582114
elapsed time: 0:00:54.307631
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:17:22.603253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.59
 ---- batch: 020 ----
mean loss: 836.38
 ---- batch: 030 ----
mean loss: 854.91
 ---- batch: 040 ----
mean loss: 859.66
 ---- batch: 050 ----
mean loss: 817.83
 ---- batch: 060 ----
mean loss: 832.36
 ---- batch: 070 ----
mean loss: 858.04
 ---- batch: 080 ----
mean loss: 854.70
 ---- batch: 090 ----
mean loss: 824.21
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 844.56
train mean loss: 844.09
epoch train time: 0:00:00.608152
elapsed time: 0:00:54.915926
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:17:23.211542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.50
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 842.46
 ---- batch: 040 ----
mean loss: 845.66
 ---- batch: 050 ----
mean loss: 833.50
 ---- batch: 060 ----
mean loss: 835.28
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 852.60
 ---- batch: 090 ----
mean loss: 845.35
 ---- batch: 100 ----
mean loss: 850.67
 ---- batch: 110 ----
mean loss: 849.94
train mean loss: 844.01
epoch train time: 0:00:00.591739
elapsed time: 0:00:55.507798
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:17:23.803435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.99
 ---- batch: 020 ----
mean loss: 855.23
 ---- batch: 030 ----
mean loss: 833.32
 ---- batch: 040 ----
mean loss: 858.46
 ---- batch: 050 ----
mean loss: 855.47
 ---- batch: 060 ----
mean loss: 845.19
 ---- batch: 070 ----
mean loss: 832.65
 ---- batch: 080 ----
mean loss: 846.02
 ---- batch: 090 ----
mean loss: 835.69
 ---- batch: 100 ----
mean loss: 840.27
 ---- batch: 110 ----
mean loss: 845.14
train mean loss: 843.93
epoch train time: 0:00:00.576722
elapsed time: 0:00:56.084673
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:17:24.380286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.49
 ---- batch: 020 ----
mean loss: 825.58
 ---- batch: 030 ----
mean loss: 839.14
 ---- batch: 040 ----
mean loss: 848.79
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 850.23
 ---- batch: 070 ----
mean loss: 845.20
 ---- batch: 080 ----
mean loss: 866.50
 ---- batch: 090 ----
mean loss: 853.99
 ---- batch: 100 ----
mean loss: 844.02
 ---- batch: 110 ----
mean loss: 822.70
train mean loss: 843.96
epoch train time: 0:00:00.582282
elapsed time: 0:00:56.667091
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:17:24.962706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.68
 ---- batch: 020 ----
mean loss: 835.69
 ---- batch: 030 ----
mean loss: 846.90
 ---- batch: 040 ----
mean loss: 835.58
 ---- batch: 050 ----
mean loss: 868.28
 ---- batch: 060 ----
mean loss: 830.60
 ---- batch: 070 ----
mean loss: 843.45
 ---- batch: 080 ----
mean loss: 856.95
 ---- batch: 090 ----
mean loss: 841.46
 ---- batch: 100 ----
mean loss: 841.99
 ---- batch: 110 ----
mean loss: 840.71
train mean loss: 844.08
epoch train time: 0:00:00.581485
elapsed time: 0:00:57.248715
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:17:25.544333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.32
 ---- batch: 020 ----
mean loss: 848.50
 ---- batch: 030 ----
mean loss: 852.79
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 845.94
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 809.14
 ---- batch: 080 ----
mean loss: 856.46
 ---- batch: 090 ----
mean loss: 845.25
 ---- batch: 100 ----
mean loss: 849.24
 ---- batch: 110 ----
mean loss: 851.07
train mean loss: 844.06
epoch train time: 0:00:00.576776
elapsed time: 0:00:57.825640
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:17:26.121288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.24
 ---- batch: 020 ----
mean loss: 835.81
 ---- batch: 030 ----
mean loss: 850.60
 ---- batch: 040 ----
mean loss: 858.13
 ---- batch: 050 ----
mean loss: 833.71
 ---- batch: 060 ----
mean loss: 865.10
 ---- batch: 070 ----
mean loss: 852.62
 ---- batch: 080 ----
mean loss: 833.10
 ---- batch: 090 ----
mean loss: 822.29
 ---- batch: 100 ----
mean loss: 830.74
 ---- batch: 110 ----
mean loss: 843.89
train mean loss: 844.04
epoch train time: 0:00:00.573764
elapsed time: 0:00:58.399606
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:17:26.695226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.01
 ---- batch: 020 ----
mean loss: 869.89
 ---- batch: 030 ----
mean loss: 857.71
 ---- batch: 040 ----
mean loss: 850.94
 ---- batch: 050 ----
mean loss: 831.70
 ---- batch: 060 ----
mean loss: 842.31
 ---- batch: 070 ----
mean loss: 837.32
 ---- batch: 080 ----
mean loss: 844.26
 ---- batch: 090 ----
mean loss: 840.97
 ---- batch: 100 ----
mean loss: 843.34
 ---- batch: 110 ----
mean loss: 853.45
train mean loss: 843.94
epoch train time: 0:00:00.580287
elapsed time: 0:00:58.980031
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:17:27.275663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.23
 ---- batch: 020 ----
mean loss: 826.17
 ---- batch: 030 ----
mean loss: 857.23
 ---- batch: 040 ----
mean loss: 855.75
 ---- batch: 050 ----
mean loss: 829.66
 ---- batch: 060 ----
mean loss: 825.05
 ---- batch: 070 ----
mean loss: 862.07
 ---- batch: 080 ----
mean loss: 821.71
 ---- batch: 090 ----
mean loss: 868.25
 ---- batch: 100 ----
mean loss: 840.49
 ---- batch: 110 ----
mean loss: 854.33
train mean loss: 844.02
epoch train time: 0:00:00.590579
elapsed time: 0:00:59.570766
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:17:27.866384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.59
 ---- batch: 020 ----
mean loss: 839.91
 ---- batch: 030 ----
mean loss: 832.06
 ---- batch: 040 ----
mean loss: 833.44
 ---- batch: 050 ----
mean loss: 843.63
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 865.28
 ---- batch: 080 ----
mean loss: 850.59
 ---- batch: 090 ----
mean loss: 864.03
 ---- batch: 100 ----
mean loss: 829.86
 ---- batch: 110 ----
mean loss: 840.02
train mean loss: 844.02
epoch train time: 0:00:00.605253
elapsed time: 0:01:00.176162
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:17:28.471783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.87
 ---- batch: 020 ----
mean loss: 843.81
 ---- batch: 030 ----
mean loss: 851.01
 ---- batch: 040 ----
mean loss: 856.66
 ---- batch: 050 ----
mean loss: 832.84
 ---- batch: 060 ----
mean loss: 851.55
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 833.21
 ---- batch: 090 ----
mean loss: 833.15
 ---- batch: 100 ----
mean loss: 851.12
 ---- batch: 110 ----
mean loss: 824.81
train mean loss: 844.02
epoch train time: 0:00:00.572222
elapsed time: 0:01:00.748525
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:17:29.044134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.72
 ---- batch: 020 ----
mean loss: 849.35
 ---- batch: 030 ----
mean loss: 854.58
 ---- batch: 040 ----
mean loss: 833.74
 ---- batch: 050 ----
mean loss: 834.99
 ---- batch: 060 ----
mean loss: 847.39
 ---- batch: 070 ----
mean loss: 828.86
 ---- batch: 080 ----
mean loss: 845.62
 ---- batch: 090 ----
mean loss: 847.91
 ---- batch: 100 ----
mean loss: 839.45
 ---- batch: 110 ----
mean loss: 846.27
train mean loss: 844.00
epoch train time: 0:00:00.561808
elapsed time: 0:01:01.310460
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:17:29.606095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.21
 ---- batch: 020 ----
mean loss: 846.50
 ---- batch: 030 ----
mean loss: 862.86
 ---- batch: 040 ----
mean loss: 852.97
 ---- batch: 050 ----
mean loss: 837.18
 ---- batch: 060 ----
mean loss: 840.36
 ---- batch: 070 ----
mean loss: 860.77
 ---- batch: 080 ----
mean loss: 855.69
 ---- batch: 090 ----
mean loss: 833.00
 ---- batch: 100 ----
mean loss: 817.31
 ---- batch: 110 ----
mean loss: 839.15
train mean loss: 843.95
epoch train time: 0:00:00.563710
elapsed time: 0:01:01.874322
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:17:30.169942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.07
 ---- batch: 020 ----
mean loss: 842.83
 ---- batch: 030 ----
mean loss: 842.87
 ---- batch: 040 ----
mean loss: 838.16
 ---- batch: 050 ----
mean loss: 844.73
 ---- batch: 060 ----
mean loss: 863.76
 ---- batch: 070 ----
mean loss: 842.70
 ---- batch: 080 ----
mean loss: 808.24
 ---- batch: 090 ----
mean loss: 833.55
 ---- batch: 100 ----
mean loss: 859.01
 ---- batch: 110 ----
mean loss: 849.06
train mean loss: 844.05
epoch train time: 0:00:00.558690
elapsed time: 0:01:02.433153
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:17:30.728772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.97
 ---- batch: 020 ----
mean loss: 835.15
 ---- batch: 030 ----
mean loss: 837.94
 ---- batch: 040 ----
mean loss: 824.13
 ---- batch: 050 ----
mean loss: 839.35
 ---- batch: 060 ----
mean loss: 857.77
 ---- batch: 070 ----
mean loss: 822.83
 ---- batch: 080 ----
mean loss: 857.42
 ---- batch: 090 ----
mean loss: 835.62
 ---- batch: 100 ----
mean loss: 851.20
 ---- batch: 110 ----
mean loss: 863.65
train mean loss: 844.00
epoch train time: 0:00:00.561560
elapsed time: 0:01:02.994874
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:17:31.290528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.57
 ---- batch: 020 ----
mean loss: 858.19
 ---- batch: 030 ----
mean loss: 852.89
 ---- batch: 040 ----
mean loss: 839.76
 ---- batch: 050 ----
mean loss: 830.17
 ---- batch: 060 ----
mean loss: 841.44
 ---- batch: 070 ----
mean loss: 855.69
 ---- batch: 080 ----
mean loss: 849.65
 ---- batch: 090 ----
mean loss: 832.65
 ---- batch: 100 ----
mean loss: 836.73
 ---- batch: 110 ----
mean loss: 842.08
train mean loss: 843.93
epoch train time: 0:00:00.559990
elapsed time: 0:01:03.555036
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:17:31.850653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.38
 ---- batch: 020 ----
mean loss: 853.11
 ---- batch: 030 ----
mean loss: 847.68
 ---- batch: 040 ----
mean loss: 843.46
 ---- batch: 050 ----
mean loss: 859.03
 ---- batch: 060 ----
mean loss: 816.25
 ---- batch: 070 ----
mean loss: 860.16
 ---- batch: 080 ----
mean loss: 813.18
 ---- batch: 090 ----
mean loss: 834.16
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 857.17
train mean loss: 843.89
epoch train time: 0:00:00.555501
elapsed time: 0:01:04.110674
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:17:32.406306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.12
 ---- batch: 020 ----
mean loss: 837.08
 ---- batch: 030 ----
mean loss: 843.34
 ---- batch: 040 ----
mean loss: 852.88
 ---- batch: 050 ----
mean loss: 846.10
 ---- batch: 060 ----
mean loss: 837.07
 ---- batch: 070 ----
mean loss: 843.34
 ---- batch: 080 ----
mean loss: 827.01
 ---- batch: 090 ----
mean loss: 848.79
 ---- batch: 100 ----
mean loss: 843.75
 ---- batch: 110 ----
mean loss: 850.03
train mean loss: 843.99
epoch train time: 0:00:00.556257
elapsed time: 0:01:04.667080
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:17:32.962694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.49
 ---- batch: 020 ----
mean loss: 850.58
 ---- batch: 030 ----
mean loss: 833.68
 ---- batch: 040 ----
mean loss: 829.32
 ---- batch: 050 ----
mean loss: 839.98
 ---- batch: 060 ----
mean loss: 842.15
 ---- batch: 070 ----
mean loss: 844.11
 ---- batch: 080 ----
mean loss: 847.44
 ---- batch: 090 ----
mean loss: 835.43
 ---- batch: 100 ----
mean loss: 836.29
 ---- batch: 110 ----
mean loss: 848.06
train mean loss: 843.98
epoch train time: 0:00:00.553928
elapsed time: 0:01:05.221137
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:17:33.516750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.99
 ---- batch: 020 ----
mean loss: 850.88
 ---- batch: 030 ----
mean loss: 821.31
 ---- batch: 040 ----
mean loss: 824.54
 ---- batch: 050 ----
mean loss: 800.95
 ---- batch: 060 ----
mean loss: 786.46
 ---- batch: 070 ----
mean loss: 770.07
 ---- batch: 080 ----
mean loss: 738.37
 ---- batch: 090 ----
mean loss: 771.28
 ---- batch: 100 ----
mean loss: 752.03
 ---- batch: 110 ----
mean loss: 723.93
train mean loss: 787.46
epoch train time: 0:00:00.574971
elapsed time: 0:01:05.796241
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:17:34.091856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 704.30
 ---- batch: 020 ----
mean loss: 677.04
 ---- batch: 030 ----
mean loss: 663.85
 ---- batch: 040 ----
mean loss: 624.49
 ---- batch: 050 ----
mean loss: 546.16
 ---- batch: 060 ----
mean loss: 451.10
 ---- batch: 070 ----
mean loss: 410.97
 ---- batch: 080 ----
mean loss: 402.53
 ---- batch: 090 ----
mean loss: 388.74
 ---- batch: 100 ----
mean loss: 381.40
 ---- batch: 110 ----
mean loss: 360.65
train mean loss: 505.76
epoch train time: 0:00:00.566176
elapsed time: 0:01:06.362551
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:17:34.658217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.79
 ---- batch: 020 ----
mean loss: 333.75
 ---- batch: 030 ----
mean loss: 332.14
 ---- batch: 040 ----
mean loss: 323.18
 ---- batch: 050 ----
mean loss: 304.34
 ---- batch: 060 ----
mean loss: 312.56
 ---- batch: 070 ----
mean loss: 293.24
 ---- batch: 080 ----
mean loss: 303.37
 ---- batch: 090 ----
mean loss: 316.22
 ---- batch: 100 ----
mean loss: 304.77
 ---- batch: 110 ----
mean loss: 297.42
train mean loss: 315.59
epoch train time: 0:00:00.566894
elapsed time: 0:01:06.929703
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:17:35.225334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.08
 ---- batch: 020 ----
mean loss: 277.53
 ---- batch: 030 ----
mean loss: 284.88
 ---- batch: 040 ----
mean loss: 277.75
 ---- batch: 050 ----
mean loss: 272.75
 ---- batch: 060 ----
mean loss: 272.92
 ---- batch: 070 ----
mean loss: 264.25
 ---- batch: 080 ----
mean loss: 271.13
 ---- batch: 090 ----
mean loss: 266.49
 ---- batch: 100 ----
mean loss: 263.43
 ---- batch: 110 ----
mean loss: 262.95
train mean loss: 271.70
epoch train time: 0:00:00.575446
elapsed time: 0:01:07.505297
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:17:35.800921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.68
 ---- batch: 020 ----
mean loss: 262.44
 ---- batch: 030 ----
mean loss: 255.51
 ---- batch: 040 ----
mean loss: 244.59
 ---- batch: 050 ----
mean loss: 255.35
 ---- batch: 060 ----
mean loss: 251.77
 ---- batch: 070 ----
mean loss: 256.80
 ---- batch: 080 ----
mean loss: 244.51
 ---- batch: 090 ----
mean loss: 252.44
 ---- batch: 100 ----
mean loss: 239.03
 ---- batch: 110 ----
mean loss: 254.11
train mean loss: 250.83
epoch train time: 0:00:00.558650
elapsed time: 0:01:08.064087
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:17:36.359701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.36
 ---- batch: 020 ----
mean loss: 230.80
 ---- batch: 030 ----
mean loss: 238.37
 ---- batch: 040 ----
mean loss: 248.53
 ---- batch: 050 ----
mean loss: 232.71
 ---- batch: 060 ----
mean loss: 236.11
 ---- batch: 070 ----
mean loss: 225.56
 ---- batch: 080 ----
mean loss: 247.31
 ---- batch: 090 ----
mean loss: 234.06
 ---- batch: 100 ----
mean loss: 237.89
 ---- batch: 110 ----
mean loss: 236.56
train mean loss: 237.61
epoch train time: 0:00:00.562203
elapsed time: 0:01:08.626431
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:17:36.922048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.09
 ---- batch: 020 ----
mean loss: 227.15
 ---- batch: 030 ----
mean loss: 225.37
 ---- batch: 040 ----
mean loss: 232.86
 ---- batch: 050 ----
mean loss: 222.92
 ---- batch: 060 ----
mean loss: 222.34
 ---- batch: 070 ----
mean loss: 233.94
 ---- batch: 080 ----
mean loss: 224.88
 ---- batch: 090 ----
mean loss: 222.69
 ---- batch: 100 ----
mean loss: 225.75
 ---- batch: 110 ----
mean loss: 237.65
train mean loss: 227.79
epoch train time: 0:00:00.562842
elapsed time: 0:01:09.189412
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:17:37.485028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.41
 ---- batch: 020 ----
mean loss: 224.39
 ---- batch: 030 ----
mean loss: 217.28
 ---- batch: 040 ----
mean loss: 207.20
 ---- batch: 050 ----
mean loss: 221.37
 ---- batch: 060 ----
mean loss: 222.52
 ---- batch: 070 ----
mean loss: 224.81
 ---- batch: 080 ----
mean loss: 226.78
 ---- batch: 090 ----
mean loss: 231.04
 ---- batch: 100 ----
mean loss: 220.69
 ---- batch: 110 ----
mean loss: 212.64
train mean loss: 220.92
epoch train time: 0:00:00.565102
elapsed time: 0:01:09.754650
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:17:38.050265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.55
 ---- batch: 020 ----
mean loss: 207.61
 ---- batch: 030 ----
mean loss: 223.93
 ---- batch: 040 ----
mean loss: 219.31
 ---- batch: 050 ----
mean loss: 222.91
 ---- batch: 060 ----
mean loss: 219.16
 ---- batch: 070 ----
mean loss: 222.37
 ---- batch: 080 ----
mean loss: 211.01
 ---- batch: 090 ----
mean loss: 207.03
 ---- batch: 100 ----
mean loss: 209.10
 ---- batch: 110 ----
mean loss: 218.58
train mean loss: 216.75
epoch train time: 0:00:00.559086
elapsed time: 0:01:10.313874
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:17:38.609503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.60
 ---- batch: 020 ----
mean loss: 207.45
 ---- batch: 030 ----
mean loss: 209.54
 ---- batch: 040 ----
mean loss: 207.60
 ---- batch: 050 ----
mean loss: 201.40
 ---- batch: 060 ----
mean loss: 215.41
 ---- batch: 070 ----
mean loss: 208.93
 ---- batch: 080 ----
mean loss: 215.60
 ---- batch: 090 ----
mean loss: 207.75
 ---- batch: 100 ----
mean loss: 206.96
 ---- batch: 110 ----
mean loss: 207.13
train mean loss: 208.86
epoch train time: 0:00:00.566162
elapsed time: 0:01:10.880184
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:17:39.175798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.50
 ---- batch: 020 ----
mean loss: 205.08
 ---- batch: 030 ----
mean loss: 193.63
 ---- batch: 040 ----
mean loss: 205.04
 ---- batch: 050 ----
mean loss: 206.29
 ---- batch: 060 ----
mean loss: 199.83
 ---- batch: 070 ----
mean loss: 210.39
 ---- batch: 080 ----
mean loss: 203.77
 ---- batch: 090 ----
mean loss: 212.48
 ---- batch: 100 ----
mean loss: 208.47
 ---- batch: 110 ----
mean loss: 209.30
train mean loss: 205.78
epoch train time: 0:00:00.563991
elapsed time: 0:01:11.444310
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:17:39.739918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.77
 ---- batch: 020 ----
mean loss: 202.61
 ---- batch: 030 ----
mean loss: 192.89
 ---- batch: 040 ----
mean loss: 199.53
 ---- batch: 050 ----
mean loss: 203.51
 ---- batch: 060 ----
mean loss: 207.87
 ---- batch: 070 ----
mean loss: 205.49
 ---- batch: 080 ----
mean loss: 201.40
 ---- batch: 090 ----
mean loss: 209.75
 ---- batch: 100 ----
mean loss: 190.97
 ---- batch: 110 ----
mean loss: 207.54
train mean loss: 201.81
epoch train time: 0:00:00.563174
elapsed time: 0:01:12.007607
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:17:40.303219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.71
 ---- batch: 020 ----
mean loss: 201.64
 ---- batch: 030 ----
mean loss: 202.76
 ---- batch: 040 ----
mean loss: 192.29
 ---- batch: 050 ----
mean loss: 200.72
 ---- batch: 060 ----
mean loss: 201.02
 ---- batch: 070 ----
mean loss: 191.22
 ---- batch: 080 ----
mean loss: 192.32
 ---- batch: 090 ----
mean loss: 196.80
 ---- batch: 100 ----
mean loss: 194.39
 ---- batch: 110 ----
mean loss: 202.53
train mean loss: 198.11
epoch train time: 0:00:00.573655
elapsed time: 0:01:12.581389
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:17:40.877005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.74
 ---- batch: 020 ----
mean loss: 196.20
 ---- batch: 030 ----
mean loss: 202.59
 ---- batch: 040 ----
mean loss: 193.73
 ---- batch: 050 ----
mean loss: 191.67
 ---- batch: 060 ----
mean loss: 190.02
 ---- batch: 070 ----
mean loss: 198.78
 ---- batch: 080 ----
mean loss: 193.90
 ---- batch: 090 ----
mean loss: 199.43
 ---- batch: 100 ----
mean loss: 193.13
 ---- batch: 110 ----
mean loss: 195.53
train mean loss: 194.66
epoch train time: 0:00:00.562973
elapsed time: 0:01:13.144496
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:17:41.440110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.39
 ---- batch: 020 ----
mean loss: 184.16
 ---- batch: 030 ----
mean loss: 194.70
 ---- batch: 040 ----
mean loss: 194.29
 ---- batch: 050 ----
mean loss: 179.16
 ---- batch: 060 ----
mean loss: 193.50
 ---- batch: 070 ----
mean loss: 195.09
 ---- batch: 080 ----
mean loss: 202.10
 ---- batch: 090 ----
mean loss: 190.32
 ---- batch: 100 ----
mean loss: 190.61
 ---- batch: 110 ----
mean loss: 192.91
train mean loss: 191.82
epoch train time: 0:00:00.562564
elapsed time: 0:01:13.707194
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:17:42.002829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.36
 ---- batch: 020 ----
mean loss: 185.92
 ---- batch: 030 ----
mean loss: 191.53
 ---- batch: 040 ----
mean loss: 192.06
 ---- batch: 050 ----
mean loss: 194.58
 ---- batch: 060 ----
mean loss: 186.84
 ---- batch: 070 ----
mean loss: 190.69
 ---- batch: 080 ----
mean loss: 186.95
 ---- batch: 090 ----
mean loss: 187.65
 ---- batch: 100 ----
mean loss: 196.86
 ---- batch: 110 ----
mean loss: 186.43
train mean loss: 189.46
epoch train time: 0:00:00.570753
elapsed time: 0:01:14.278125
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:17:42.573743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.02
 ---- batch: 020 ----
mean loss: 190.00
 ---- batch: 030 ----
mean loss: 183.49
 ---- batch: 040 ----
mean loss: 183.33
 ---- batch: 050 ----
mean loss: 182.33
 ---- batch: 060 ----
mean loss: 195.15
 ---- batch: 070 ----
mean loss: 185.08
 ---- batch: 080 ----
mean loss: 193.22
 ---- batch: 090 ----
mean loss: 190.36
 ---- batch: 100 ----
mean loss: 193.34
 ---- batch: 110 ----
mean loss: 184.00
train mean loss: 187.95
epoch train time: 0:00:00.559202
elapsed time: 0:01:14.837481
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:17:43.133125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.25
 ---- batch: 020 ----
mean loss: 184.38
 ---- batch: 030 ----
mean loss: 182.04
 ---- batch: 040 ----
mean loss: 179.78
 ---- batch: 050 ----
mean loss: 186.08
 ---- batch: 060 ----
mean loss: 188.89
 ---- batch: 070 ----
mean loss: 189.44
 ---- batch: 080 ----
mean loss: 191.35
 ---- batch: 090 ----
mean loss: 183.48
 ---- batch: 100 ----
mean loss: 185.88
 ---- batch: 110 ----
mean loss: 185.01
train mean loss: 186.15
epoch train time: 0:00:00.560021
elapsed time: 0:01:15.397663
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:17:43.693277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.13
 ---- batch: 020 ----
mean loss: 182.08
 ---- batch: 030 ----
mean loss: 192.35
 ---- batch: 040 ----
mean loss: 187.17
 ---- batch: 050 ----
mean loss: 191.23
 ---- batch: 060 ----
mean loss: 179.45
 ---- batch: 070 ----
mean loss: 181.68
 ---- batch: 080 ----
mean loss: 186.23
 ---- batch: 090 ----
mean loss: 186.76
 ---- batch: 100 ----
mean loss: 178.65
 ---- batch: 110 ----
mean loss: 190.99
train mean loss: 185.25
epoch train time: 0:00:00.567699
elapsed time: 0:01:15.965493
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:17:44.261108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.50
 ---- batch: 020 ----
mean loss: 175.81
 ---- batch: 030 ----
mean loss: 187.11
 ---- batch: 040 ----
mean loss: 187.50
 ---- batch: 050 ----
mean loss: 187.72
 ---- batch: 060 ----
mean loss: 188.59
 ---- batch: 070 ----
mean loss: 188.20
 ---- batch: 080 ----
mean loss: 183.23
 ---- batch: 090 ----
mean loss: 181.42
 ---- batch: 100 ----
mean loss: 172.20
 ---- batch: 110 ----
mean loss: 186.92
train mean loss: 184.08
epoch train time: 0:00:00.565416
elapsed time: 0:01:16.531043
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:17:44.826691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.98
 ---- batch: 020 ----
mean loss: 185.10
 ---- batch: 030 ----
mean loss: 175.36
 ---- batch: 040 ----
mean loss: 181.61
 ---- batch: 050 ----
mean loss: 189.34
 ---- batch: 060 ----
mean loss: 182.80
 ---- batch: 070 ----
mean loss: 182.32
 ---- batch: 080 ----
mean loss: 182.88
 ---- batch: 090 ----
mean loss: 185.92
 ---- batch: 100 ----
mean loss: 188.41
 ---- batch: 110 ----
mean loss: 176.45
train mean loss: 183.10
epoch train time: 0:00:00.559452
elapsed time: 0:01:17.090689
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:17:45.386307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.31
 ---- batch: 020 ----
mean loss: 178.15
 ---- batch: 030 ----
mean loss: 172.82
 ---- batch: 040 ----
mean loss: 181.84
 ---- batch: 050 ----
mean loss: 183.48
 ---- batch: 060 ----
mean loss: 191.90
 ---- batch: 070 ----
mean loss: 180.88
 ---- batch: 080 ----
mean loss: 184.82
 ---- batch: 090 ----
mean loss: 181.14
 ---- batch: 100 ----
mean loss: 179.29
 ---- batch: 110 ----
mean loss: 183.55
train mean loss: 180.85
epoch train time: 0:00:00.573644
elapsed time: 0:01:17.664520
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:17:45.960156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.65
 ---- batch: 020 ----
mean loss: 180.46
 ---- batch: 030 ----
mean loss: 174.45
 ---- batch: 040 ----
mean loss: 186.21
 ---- batch: 050 ----
mean loss: 182.46
 ---- batch: 060 ----
mean loss: 181.76
 ---- batch: 070 ----
mean loss: 179.20
 ---- batch: 080 ----
mean loss: 175.95
 ---- batch: 090 ----
mean loss: 185.91
 ---- batch: 100 ----
mean loss: 173.24
 ---- batch: 110 ----
mean loss: 195.49
train mean loss: 180.61
epoch train time: 0:00:00.564023
elapsed time: 0:01:18.228707
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:17:46.524320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.06
 ---- batch: 020 ----
mean loss: 183.47
 ---- batch: 030 ----
mean loss: 173.82
 ---- batch: 040 ----
mean loss: 177.21
 ---- batch: 050 ----
mean loss: 179.30
 ---- batch: 060 ----
mean loss: 183.95
 ---- batch: 070 ----
mean loss: 186.31
 ---- batch: 080 ----
mean loss: 177.09
 ---- batch: 090 ----
mean loss: 181.20
 ---- batch: 100 ----
mean loss: 181.13
 ---- batch: 110 ----
mean loss: 178.22
train mean loss: 179.69
epoch train time: 0:00:00.566789
elapsed time: 0:01:18.795662
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:17:47.091296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.50
 ---- batch: 020 ----
mean loss: 173.63
 ---- batch: 030 ----
mean loss: 176.36
 ---- batch: 040 ----
mean loss: 179.19
 ---- batch: 050 ----
mean loss: 181.28
 ---- batch: 060 ----
mean loss: 174.13
 ---- batch: 070 ----
mean loss: 175.71
 ---- batch: 080 ----
mean loss: 194.21
 ---- batch: 090 ----
mean loss: 179.96
 ---- batch: 100 ----
mean loss: 166.89
 ---- batch: 110 ----
mean loss: 188.23
train mean loss: 178.34
epoch train time: 0:00:00.566191
elapsed time: 0:01:19.362006
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:17:47.657644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.00
 ---- batch: 020 ----
mean loss: 182.68
 ---- batch: 030 ----
mean loss: 183.94
 ---- batch: 040 ----
mean loss: 178.97
 ---- batch: 050 ----
mean loss: 170.69
 ---- batch: 060 ----
mean loss: 178.66
 ---- batch: 070 ----
mean loss: 187.78
 ---- batch: 080 ----
mean loss: 178.62
 ---- batch: 090 ----
mean loss: 180.16
 ---- batch: 100 ----
mean loss: 176.04
 ---- batch: 110 ----
mean loss: 176.98
train mean loss: 178.78
epoch train time: 0:00:00.574586
elapsed time: 0:01:19.936754
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:17:48.232372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.55
 ---- batch: 020 ----
mean loss: 175.39
 ---- batch: 030 ----
mean loss: 176.63
 ---- batch: 040 ----
mean loss: 172.96
 ---- batch: 050 ----
mean loss: 176.65
 ---- batch: 060 ----
mean loss: 175.62
 ---- batch: 070 ----
mean loss: 175.94
 ---- batch: 080 ----
mean loss: 175.46
 ---- batch: 090 ----
mean loss: 176.80
 ---- batch: 100 ----
mean loss: 178.06
 ---- batch: 110 ----
mean loss: 179.50
train mean loss: 176.69
epoch train time: 0:00:00.578588
elapsed time: 0:01:20.515487
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:17:48.811110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.53
 ---- batch: 020 ----
mean loss: 181.94
 ---- batch: 030 ----
mean loss: 174.18
 ---- batch: 040 ----
mean loss: 171.06
 ---- batch: 050 ----
mean loss: 169.02
 ---- batch: 060 ----
mean loss: 177.31
 ---- batch: 070 ----
mean loss: 185.86
 ---- batch: 080 ----
mean loss: 182.48
 ---- batch: 090 ----
mean loss: 176.22
 ---- batch: 100 ----
mean loss: 183.18
 ---- batch: 110 ----
mean loss: 173.04
train mean loss: 177.30
epoch train time: 0:00:00.562053
elapsed time: 0:01:21.077682
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:17:49.373298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.26
 ---- batch: 020 ----
mean loss: 179.11
 ---- batch: 030 ----
mean loss: 163.45
 ---- batch: 040 ----
mean loss: 177.91
 ---- batch: 050 ----
mean loss: 176.05
 ---- batch: 060 ----
mean loss: 173.72
 ---- batch: 070 ----
mean loss: 172.60
 ---- batch: 080 ----
mean loss: 181.90
 ---- batch: 090 ----
mean loss: 179.88
 ---- batch: 100 ----
mean loss: 181.02
 ---- batch: 110 ----
mean loss: 180.66
train mean loss: 175.88
epoch train time: 0:00:00.562011
elapsed time: 0:01:21.639828
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:17:49.935464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.93
 ---- batch: 020 ----
mean loss: 183.45
 ---- batch: 030 ----
mean loss: 166.17
 ---- batch: 040 ----
mean loss: 168.94
 ---- batch: 050 ----
mean loss: 180.04
 ---- batch: 060 ----
mean loss: 174.68
 ---- batch: 070 ----
mean loss: 171.89
 ---- batch: 080 ----
mean loss: 170.95
 ---- batch: 090 ----
mean loss: 176.09
 ---- batch: 100 ----
mean loss: 180.22
 ---- batch: 110 ----
mean loss: 184.46
train mean loss: 174.49
epoch train time: 0:00:00.557062
elapsed time: 0:01:22.197042
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:17:50.492666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.58
 ---- batch: 020 ----
mean loss: 175.64
 ---- batch: 030 ----
mean loss: 172.55
 ---- batch: 040 ----
mean loss: 163.83
 ---- batch: 050 ----
mean loss: 174.37
 ---- batch: 060 ----
mean loss: 174.42
 ---- batch: 070 ----
mean loss: 175.96
 ---- batch: 080 ----
mean loss: 182.01
 ---- batch: 090 ----
mean loss: 174.44
 ---- batch: 100 ----
mean loss: 169.95
 ---- batch: 110 ----
mean loss: 180.11
train mean loss: 174.28
epoch train time: 0:00:00.562585
elapsed time: 0:01:22.759771
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:17:51.055390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.92
 ---- batch: 020 ----
mean loss: 167.01
 ---- batch: 030 ----
mean loss: 176.12
 ---- batch: 040 ----
mean loss: 172.29
 ---- batch: 050 ----
mean loss: 175.41
 ---- batch: 060 ----
mean loss: 169.27
 ---- batch: 070 ----
mean loss: 177.43
 ---- batch: 080 ----
mean loss: 172.65
 ---- batch: 090 ----
mean loss: 173.62
 ---- batch: 100 ----
mean loss: 180.67
 ---- batch: 110 ----
mean loss: 172.56
train mean loss: 172.60
epoch train time: 0:00:00.567132
elapsed time: 0:01:23.327043
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:17:51.622659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.61
 ---- batch: 020 ----
mean loss: 171.13
 ---- batch: 030 ----
mean loss: 170.05
 ---- batch: 040 ----
mean loss: 176.34
 ---- batch: 050 ----
mean loss: 166.49
 ---- batch: 060 ----
mean loss: 171.44
 ---- batch: 070 ----
mean loss: 172.30
 ---- batch: 080 ----
mean loss: 176.88
 ---- batch: 090 ----
mean loss: 171.63
 ---- batch: 100 ----
mean loss: 173.75
 ---- batch: 110 ----
mean loss: 172.86
train mean loss: 172.15
epoch train time: 0:00:00.565561
elapsed time: 0:01:23.892737
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:17:52.188353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.59
 ---- batch: 020 ----
mean loss: 167.13
 ---- batch: 030 ----
mean loss: 163.76
 ---- batch: 040 ----
mean loss: 166.90
 ---- batch: 050 ----
mean loss: 163.58
 ---- batch: 060 ----
mean loss: 177.49
 ---- batch: 070 ----
mean loss: 182.00
 ---- batch: 080 ----
mean loss: 173.39
 ---- batch: 090 ----
mean loss: 169.24
 ---- batch: 100 ----
mean loss: 175.64
 ---- batch: 110 ----
mean loss: 174.86
train mean loss: 171.94
epoch train time: 0:00:00.555197
elapsed time: 0:01:24.448084
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:17:52.743700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.85
 ---- batch: 020 ----
mean loss: 169.14
 ---- batch: 030 ----
mean loss: 170.39
 ---- batch: 040 ----
mean loss: 165.31
 ---- batch: 050 ----
mean loss: 169.41
 ---- batch: 060 ----
mean loss: 177.38
 ---- batch: 070 ----
mean loss: 171.67
 ---- batch: 080 ----
mean loss: 170.17
 ---- batch: 090 ----
mean loss: 174.10
 ---- batch: 100 ----
mean loss: 172.30
 ---- batch: 110 ----
mean loss: 175.33
train mean loss: 171.10
epoch train time: 0:00:00.559859
elapsed time: 0:01:25.008091
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:17:53.303723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.99
 ---- batch: 020 ----
mean loss: 176.77
 ---- batch: 030 ----
mean loss: 161.73
 ---- batch: 040 ----
mean loss: 175.59
 ---- batch: 050 ----
mean loss: 159.96
 ---- batch: 060 ----
mean loss: 170.77
 ---- batch: 070 ----
mean loss: 177.54
 ---- batch: 080 ----
mean loss: 180.76
 ---- batch: 090 ----
mean loss: 169.05
 ---- batch: 100 ----
mean loss: 169.89
 ---- batch: 110 ----
mean loss: 175.17
train mean loss: 171.17
epoch train time: 0:00:00.566510
elapsed time: 0:01:25.574750
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:17:53.870364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.51
 ---- batch: 020 ----
mean loss: 174.94
 ---- batch: 030 ----
mean loss: 169.42
 ---- batch: 040 ----
mean loss: 173.54
 ---- batch: 050 ----
mean loss: 169.82
 ---- batch: 060 ----
mean loss: 158.15
 ---- batch: 070 ----
mean loss: 172.57
 ---- batch: 080 ----
mean loss: 162.15
 ---- batch: 090 ----
mean loss: 174.41
 ---- batch: 100 ----
mean loss: 171.05
 ---- batch: 110 ----
mean loss: 172.21
train mean loss: 169.84
epoch train time: 0:00:00.556327
elapsed time: 0:01:26.131210
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:17:54.426824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.16
 ---- batch: 020 ----
mean loss: 175.37
 ---- batch: 030 ----
mean loss: 171.62
 ---- batch: 040 ----
mean loss: 162.98
 ---- batch: 050 ----
mean loss: 165.59
 ---- batch: 060 ----
mean loss: 171.27
 ---- batch: 070 ----
mean loss: 178.64
 ---- batch: 080 ----
mean loss: 177.64
 ---- batch: 090 ----
mean loss: 163.52
 ---- batch: 100 ----
mean loss: 171.87
 ---- batch: 110 ----
mean loss: 166.90
train mean loss: 169.83
epoch train time: 0:00:00.558208
elapsed time: 0:01:26.689551
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:17:54.985166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.23
 ---- batch: 020 ----
mean loss: 167.67
 ---- batch: 030 ----
mean loss: 175.74
 ---- batch: 040 ----
mean loss: 168.84
 ---- batch: 050 ----
mean loss: 171.90
 ---- batch: 060 ----
mean loss: 170.18
 ---- batch: 070 ----
mean loss: 166.25
 ---- batch: 080 ----
mean loss: 170.30
 ---- batch: 090 ----
mean loss: 164.68
 ---- batch: 100 ----
mean loss: 165.56
 ---- batch: 110 ----
mean loss: 171.56
train mean loss: 169.13
epoch train time: 0:00:00.560109
elapsed time: 0:01:27.249789
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:17:55.545401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.24
 ---- batch: 020 ----
mean loss: 170.90
 ---- batch: 030 ----
mean loss: 166.77
 ---- batch: 040 ----
mean loss: 177.02
 ---- batch: 050 ----
mean loss: 176.33
 ---- batch: 060 ----
mean loss: 161.33
 ---- batch: 070 ----
mean loss: 161.62
 ---- batch: 080 ----
mean loss: 161.36
 ---- batch: 090 ----
mean loss: 171.54
 ---- batch: 100 ----
mean loss: 170.46
 ---- batch: 110 ----
mean loss: 168.54
train mean loss: 169.23
epoch train time: 0:00:00.559275
elapsed time: 0:01:27.809216
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:17:56.104851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.22
 ---- batch: 020 ----
mean loss: 158.06
 ---- batch: 030 ----
mean loss: 156.02
 ---- batch: 040 ----
mean loss: 169.91
 ---- batch: 050 ----
mean loss: 177.17
 ---- batch: 060 ----
mean loss: 172.11
 ---- batch: 070 ----
mean loss: 170.71
 ---- batch: 080 ----
mean loss: 169.33
 ---- batch: 090 ----
mean loss: 164.06
 ---- batch: 100 ----
mean loss: 170.88
 ---- batch: 110 ----
mean loss: 171.78
train mean loss: 168.30
epoch train time: 0:00:00.556014
elapsed time: 0:01:28.365389
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:17:56.661006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.69
 ---- batch: 020 ----
mean loss: 162.78
 ---- batch: 030 ----
mean loss: 162.99
 ---- batch: 040 ----
mean loss: 166.31
 ---- batch: 050 ----
mean loss: 177.84
 ---- batch: 060 ----
mean loss: 164.63
 ---- batch: 070 ----
mean loss: 160.45
 ---- batch: 080 ----
mean loss: 174.09
 ---- batch: 090 ----
mean loss: 175.55
 ---- batch: 100 ----
mean loss: 171.67
 ---- batch: 110 ----
mean loss: 165.88
train mean loss: 167.77
epoch train time: 0:00:00.573549
elapsed time: 0:01:28.939083
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:17:57.234720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.01
 ---- batch: 020 ----
mean loss: 166.67
 ---- batch: 030 ----
mean loss: 167.44
 ---- batch: 040 ----
mean loss: 164.89
 ---- batch: 050 ----
mean loss: 169.12
 ---- batch: 060 ----
mean loss: 168.52
 ---- batch: 070 ----
mean loss: 170.01
 ---- batch: 080 ----
mean loss: 174.74
 ---- batch: 090 ----
mean loss: 166.11
 ---- batch: 100 ----
mean loss: 167.08
 ---- batch: 110 ----
mean loss: 162.62
train mean loss: 166.91
epoch train time: 0:00:00.576849
elapsed time: 0:01:29.516084
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:17:57.811695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.95
 ---- batch: 020 ----
mean loss: 169.36
 ---- batch: 030 ----
mean loss: 157.81
 ---- batch: 040 ----
mean loss: 168.95
 ---- batch: 050 ----
mean loss: 159.93
 ---- batch: 060 ----
mean loss: 165.56
 ---- batch: 070 ----
mean loss: 158.81
 ---- batch: 080 ----
mean loss: 160.39
 ---- batch: 090 ----
mean loss: 168.67
 ---- batch: 100 ----
mean loss: 174.65
 ---- batch: 110 ----
mean loss: 178.29
train mean loss: 166.56
epoch train time: 0:00:00.563533
elapsed time: 0:01:30.079747
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:17:58.375362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.00
 ---- batch: 020 ----
mean loss: 162.84
 ---- batch: 030 ----
mean loss: 170.08
 ---- batch: 040 ----
mean loss: 160.18
 ---- batch: 050 ----
mean loss: 166.16
 ---- batch: 060 ----
mean loss: 172.56
 ---- batch: 070 ----
mean loss: 163.40
 ---- batch: 080 ----
mean loss: 166.68
 ---- batch: 090 ----
mean loss: 164.54
 ---- batch: 100 ----
mean loss: 172.05
 ---- batch: 110 ----
mean loss: 165.48
train mean loss: 165.57
epoch train time: 0:00:00.589215
elapsed time: 0:01:30.669097
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:17:58.964715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.24
 ---- batch: 020 ----
mean loss: 165.50
 ---- batch: 030 ----
mean loss: 165.12
 ---- batch: 040 ----
mean loss: 157.86
 ---- batch: 050 ----
mean loss: 167.82
 ---- batch: 060 ----
mean loss: 157.58
 ---- batch: 070 ----
mean loss: 166.81
 ---- batch: 080 ----
mean loss: 169.69
 ---- batch: 090 ----
mean loss: 166.02
 ---- batch: 100 ----
mean loss: 157.85
 ---- batch: 110 ----
mean loss: 169.47
train mean loss: 164.87
epoch train time: 0:00:00.563321
elapsed time: 0:01:31.232555
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:17:59.528172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.06
 ---- batch: 020 ----
mean loss: 162.45
 ---- batch: 030 ----
mean loss: 165.53
 ---- batch: 040 ----
mean loss: 168.49
 ---- batch: 050 ----
mean loss: 164.53
 ---- batch: 060 ----
mean loss: 167.19
 ---- batch: 070 ----
mean loss: 165.17
 ---- batch: 080 ----
mean loss: 173.04
 ---- batch: 090 ----
mean loss: 161.91
 ---- batch: 100 ----
mean loss: 166.11
 ---- batch: 110 ----
mean loss: 159.85
train mean loss: 165.75
epoch train time: 0:00:00.562332
elapsed time: 0:01:31.795017
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:18:00.090629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.50
 ---- batch: 020 ----
mean loss: 158.77
 ---- batch: 030 ----
mean loss: 163.50
 ---- batch: 040 ----
mean loss: 164.06
 ---- batch: 050 ----
mean loss: 158.59
 ---- batch: 060 ----
mean loss: 170.01
 ---- batch: 070 ----
mean loss: 164.45
 ---- batch: 080 ----
mean loss: 160.56
 ---- batch: 090 ----
mean loss: 159.91
 ---- batch: 100 ----
mean loss: 165.35
 ---- batch: 110 ----
mean loss: 172.51
train mean loss: 164.23
epoch train time: 0:00:00.555058
elapsed time: 0:01:32.350209
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:18:00.645843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.03
 ---- batch: 020 ----
mean loss: 162.31
 ---- batch: 030 ----
mean loss: 166.48
 ---- batch: 040 ----
mean loss: 164.37
 ---- batch: 050 ----
mean loss: 161.92
 ---- batch: 060 ----
mean loss: 163.14
 ---- batch: 070 ----
mean loss: 161.27
 ---- batch: 080 ----
mean loss: 159.76
 ---- batch: 090 ----
mean loss: 164.84
 ---- batch: 100 ----
mean loss: 171.37
 ---- batch: 110 ----
mean loss: 173.41
train mean loss: 163.66
epoch train time: 0:00:00.558289
elapsed time: 0:01:32.908668
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:18:01.204287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.15
 ---- batch: 020 ----
mean loss: 160.85
 ---- batch: 030 ----
mean loss: 163.44
 ---- batch: 040 ----
mean loss: 157.58
 ---- batch: 050 ----
mean loss: 158.10
 ---- batch: 060 ----
mean loss: 167.68
 ---- batch: 070 ----
mean loss: 165.85
 ---- batch: 080 ----
mean loss: 165.74
 ---- batch: 090 ----
mean loss: 164.04
 ---- batch: 100 ----
mean loss: 170.55
 ---- batch: 110 ----
mean loss: 159.00
train mean loss: 163.82
epoch train time: 0:00:00.560367
elapsed time: 0:01:33.469182
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:18:01.764804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.32
 ---- batch: 020 ----
mean loss: 167.23
 ---- batch: 030 ----
mean loss: 159.71
 ---- batch: 040 ----
mean loss: 158.16
 ---- batch: 050 ----
mean loss: 159.16
 ---- batch: 060 ----
mean loss: 163.08
 ---- batch: 070 ----
mean loss: 160.03
 ---- batch: 080 ----
mean loss: 168.01
 ---- batch: 090 ----
mean loss: 169.25
 ---- batch: 100 ----
mean loss: 159.68
 ---- batch: 110 ----
mean loss: 167.93
train mean loss: 162.85
epoch train time: 0:00:00.559652
elapsed time: 0:01:34.028974
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:18:02.324599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.43
 ---- batch: 020 ----
mean loss: 165.50
 ---- batch: 030 ----
mean loss: 165.02
 ---- batch: 040 ----
mean loss: 162.77
 ---- batch: 050 ----
mean loss: 166.07
 ---- batch: 060 ----
mean loss: 156.13
 ---- batch: 070 ----
mean loss: 171.21
 ---- batch: 080 ----
mean loss: 166.12
 ---- batch: 090 ----
mean loss: 165.26
 ---- batch: 100 ----
mean loss: 153.58
 ---- batch: 110 ----
mean loss: 166.34
train mean loss: 163.26
epoch train time: 0:00:00.563151
elapsed time: 0:01:34.592272
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:18:02.887889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.08
 ---- batch: 020 ----
mean loss: 160.33
 ---- batch: 030 ----
mean loss: 152.48
 ---- batch: 040 ----
mean loss: 158.62
 ---- batch: 050 ----
mean loss: 164.54
 ---- batch: 060 ----
mean loss: 159.74
 ---- batch: 070 ----
mean loss: 167.56
 ---- batch: 080 ----
mean loss: 164.34
 ---- batch: 090 ----
mean loss: 164.18
 ---- batch: 100 ----
mean loss: 168.46
 ---- batch: 110 ----
mean loss: 162.08
train mean loss: 162.06
epoch train time: 0:00:00.574488
elapsed time: 0:01:35.166900
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:18:03.462515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.20
 ---- batch: 020 ----
mean loss: 152.66
 ---- batch: 030 ----
mean loss: 162.91
 ---- batch: 040 ----
mean loss: 159.09
 ---- batch: 050 ----
mean loss: 164.70
 ---- batch: 060 ----
mean loss: 159.47
 ---- batch: 070 ----
mean loss: 168.55
 ---- batch: 080 ----
mean loss: 166.91
 ---- batch: 090 ----
mean loss: 155.02
 ---- batch: 100 ----
mean loss: 160.97
 ---- batch: 110 ----
mean loss: 164.85
train mean loss: 161.16
epoch train time: 0:00:00.574310
elapsed time: 0:01:35.741343
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:18:04.036972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.18
 ---- batch: 020 ----
mean loss: 159.92
 ---- batch: 030 ----
mean loss: 156.83
 ---- batch: 040 ----
mean loss: 151.69
 ---- batch: 050 ----
mean loss: 167.01
 ---- batch: 060 ----
mean loss: 160.36
 ---- batch: 070 ----
mean loss: 164.27
 ---- batch: 080 ----
mean loss: 163.17
 ---- batch: 090 ----
mean loss: 155.97
 ---- batch: 100 ----
mean loss: 161.24
 ---- batch: 110 ----
mean loss: 166.26
train mean loss: 160.83
epoch train time: 0:00:00.562394
elapsed time: 0:01:36.303885
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:18:04.599535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.89
 ---- batch: 020 ----
mean loss: 151.07
 ---- batch: 030 ----
mean loss: 159.26
 ---- batch: 040 ----
mean loss: 161.66
 ---- batch: 050 ----
mean loss: 156.52
 ---- batch: 060 ----
mean loss: 163.01
 ---- batch: 070 ----
mean loss: 165.67
 ---- batch: 080 ----
mean loss: 170.75
 ---- batch: 090 ----
mean loss: 169.01
 ---- batch: 100 ----
mean loss: 154.39
 ---- batch: 110 ----
mean loss: 163.02
train mean loss: 161.59
epoch train time: 0:00:00.563442
elapsed time: 0:01:36.867509
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:18:05.163144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.54
 ---- batch: 020 ----
mean loss: 154.93
 ---- batch: 030 ----
mean loss: 160.45
 ---- batch: 040 ----
mean loss: 152.51
 ---- batch: 050 ----
mean loss: 159.87
 ---- batch: 060 ----
mean loss: 159.58
 ---- batch: 070 ----
mean loss: 152.90
 ---- batch: 080 ----
mean loss: 166.01
 ---- batch: 090 ----
mean loss: 164.27
 ---- batch: 100 ----
mean loss: 161.89
 ---- batch: 110 ----
mean loss: 168.96
train mean loss: 160.44
epoch train time: 0:00:00.564250
elapsed time: 0:01:37.431914
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:18:05.727529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.15
 ---- batch: 020 ----
mean loss: 169.27
 ---- batch: 030 ----
mean loss: 158.59
 ---- batch: 040 ----
mean loss: 158.25
 ---- batch: 050 ----
mean loss: 161.08
 ---- batch: 060 ----
mean loss: 171.28
 ---- batch: 070 ----
mean loss: 156.76
 ---- batch: 080 ----
mean loss: 155.70
 ---- batch: 090 ----
mean loss: 161.26
 ---- batch: 100 ----
mean loss: 160.71
 ---- batch: 110 ----
mean loss: 159.41
train mean loss: 161.01
epoch train time: 0:00:00.561104
elapsed time: 0:01:37.993153
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:18:06.288773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.96
 ---- batch: 020 ----
mean loss: 167.02
 ---- batch: 030 ----
mean loss: 158.36
 ---- batch: 040 ----
mean loss: 154.49
 ---- batch: 050 ----
mean loss: 161.21
 ---- batch: 060 ----
mean loss: 159.37
 ---- batch: 070 ----
mean loss: 156.53
 ---- batch: 080 ----
mean loss: 166.42
 ---- batch: 090 ----
mean loss: 159.34
 ---- batch: 100 ----
mean loss: 150.35
 ---- batch: 110 ----
mean loss: 151.81
train mean loss: 159.21
epoch train time: 0:00:00.561447
elapsed time: 0:01:38.554741
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:18:06.850357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.26
 ---- batch: 020 ----
mean loss: 158.30
 ---- batch: 030 ----
mean loss: 158.62
 ---- batch: 040 ----
mean loss: 160.97
 ---- batch: 050 ----
mean loss: 161.81
 ---- batch: 060 ----
mean loss: 164.63
 ---- batch: 070 ----
mean loss: 166.82
 ---- batch: 080 ----
mean loss: 164.50
 ---- batch: 090 ----
mean loss: 156.62
 ---- batch: 100 ----
mean loss: 162.70
 ---- batch: 110 ----
mean loss: 158.04
train mean loss: 160.55
epoch train time: 0:00:00.562919
elapsed time: 0:01:39.117792
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:18:07.413417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.02
 ---- batch: 020 ----
mean loss: 161.15
 ---- batch: 030 ----
mean loss: 159.64
 ---- batch: 040 ----
mean loss: 156.75
 ---- batch: 050 ----
mean loss: 154.75
 ---- batch: 060 ----
mean loss: 161.48
 ---- batch: 070 ----
mean loss: 161.03
 ---- batch: 080 ----
mean loss: 161.31
 ---- batch: 090 ----
mean loss: 159.58
 ---- batch: 100 ----
mean loss: 162.00
 ---- batch: 110 ----
mean loss: 155.59
train mean loss: 159.44
epoch train time: 0:00:00.567138
elapsed time: 0:01:39.685073
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:18:07.980709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.96
 ---- batch: 020 ----
mean loss: 155.79
 ---- batch: 030 ----
mean loss: 155.35
 ---- batch: 040 ----
mean loss: 154.99
 ---- batch: 050 ----
mean loss: 161.21
 ---- batch: 060 ----
mean loss: 154.43
 ---- batch: 070 ----
mean loss: 157.55
 ---- batch: 080 ----
mean loss: 164.07
 ---- batch: 090 ----
mean loss: 163.37
 ---- batch: 100 ----
mean loss: 160.98
 ---- batch: 110 ----
mean loss: 156.72
train mean loss: 158.28
epoch train time: 0:00:00.563617
elapsed time: 0:01:40.248847
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:18:08.544472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.46
 ---- batch: 020 ----
mean loss: 152.35
 ---- batch: 030 ----
mean loss: 155.89
 ---- batch: 040 ----
mean loss: 157.94
 ---- batch: 050 ----
mean loss: 161.94
 ---- batch: 060 ----
mean loss: 161.24
 ---- batch: 070 ----
mean loss: 160.49
 ---- batch: 080 ----
mean loss: 158.22
 ---- batch: 090 ----
mean loss: 165.39
 ---- batch: 100 ----
mean loss: 158.68
 ---- batch: 110 ----
mean loss: 158.06
train mean loss: 158.64
epoch train time: 0:00:00.570021
elapsed time: 0:01:40.819013
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:18:09.114628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.23
 ---- batch: 020 ----
mean loss: 144.85
 ---- batch: 030 ----
mean loss: 162.66
 ---- batch: 040 ----
mean loss: 157.88
 ---- batch: 050 ----
mean loss: 154.17
 ---- batch: 060 ----
mean loss: 151.35
 ---- batch: 070 ----
mean loss: 164.34
 ---- batch: 080 ----
mean loss: 150.98
 ---- batch: 090 ----
mean loss: 164.14
 ---- batch: 100 ----
mean loss: 159.44
 ---- batch: 110 ----
mean loss: 159.39
train mean loss: 157.31
epoch train time: 0:00:00.562633
elapsed time: 0:01:41.381807
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:18:09.677461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.67
 ---- batch: 020 ----
mean loss: 150.41
 ---- batch: 030 ----
mean loss: 154.99
 ---- batch: 040 ----
mean loss: 154.02
 ---- batch: 050 ----
mean loss: 146.00
 ---- batch: 060 ----
mean loss: 158.08
 ---- batch: 070 ----
mean loss: 162.32
 ---- batch: 080 ----
mean loss: 159.14
 ---- batch: 090 ----
mean loss: 159.53
 ---- batch: 100 ----
mean loss: 164.04
 ---- batch: 110 ----
mean loss: 163.48
train mean loss: 157.15
epoch train time: 0:00:00.561101
elapsed time: 0:01:41.943084
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:18:10.238702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.71
 ---- batch: 020 ----
mean loss: 159.49
 ---- batch: 030 ----
mean loss: 151.42
 ---- batch: 040 ----
mean loss: 155.98
 ---- batch: 050 ----
mean loss: 166.90
 ---- batch: 060 ----
mean loss: 160.69
 ---- batch: 070 ----
mean loss: 164.65
 ---- batch: 080 ----
mean loss: 159.82
 ---- batch: 090 ----
mean loss: 157.15
 ---- batch: 100 ----
mean loss: 159.79
 ---- batch: 110 ----
mean loss: 150.81
train mean loss: 158.42
epoch train time: 0:00:00.567091
elapsed time: 0:01:42.510347
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:18:10.805996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.39
 ---- batch: 020 ----
mean loss: 151.73
 ---- batch: 030 ----
mean loss: 153.84
 ---- batch: 040 ----
mean loss: 156.24
 ---- batch: 050 ----
mean loss: 156.30
 ---- batch: 060 ----
mean loss: 157.70
 ---- batch: 070 ----
mean loss: 156.21
 ---- batch: 080 ----
mean loss: 158.67
 ---- batch: 090 ----
mean loss: 166.12
 ---- batch: 100 ----
mean loss: 159.75
 ---- batch: 110 ----
mean loss: 150.61
train mean loss: 156.35
epoch train time: 0:00:00.584302
elapsed time: 0:01:43.094831
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:18:11.390454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.73
 ---- batch: 020 ----
mean loss: 146.13
 ---- batch: 030 ----
mean loss: 158.74
 ---- batch: 040 ----
mean loss: 160.32
 ---- batch: 050 ----
mean loss: 164.70
 ---- batch: 060 ----
mean loss: 162.02
 ---- batch: 070 ----
mean loss: 164.21
 ---- batch: 080 ----
mean loss: 154.60
 ---- batch: 090 ----
mean loss: 158.47
 ---- batch: 100 ----
mean loss: 154.87
 ---- batch: 110 ----
mean loss: 152.37
train mean loss: 156.87
epoch train time: 0:00:00.578161
elapsed time: 0:01:43.673135
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:18:11.968769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.56
 ---- batch: 020 ----
mean loss: 147.32
 ---- batch: 030 ----
mean loss: 161.13
 ---- batch: 040 ----
mean loss: 164.02
 ---- batch: 050 ----
mean loss: 149.94
 ---- batch: 060 ----
mean loss: 156.75
 ---- batch: 070 ----
mean loss: 163.80
 ---- batch: 080 ----
mean loss: 161.07
 ---- batch: 090 ----
mean loss: 153.71
 ---- batch: 100 ----
mean loss: 157.11
 ---- batch: 110 ----
mean loss: 157.59
train mean loss: 157.18
epoch train time: 0:00:00.573266
elapsed time: 0:01:44.246645
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:18:12.542262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.11
 ---- batch: 020 ----
mean loss: 149.53
 ---- batch: 030 ----
mean loss: 154.66
 ---- batch: 040 ----
mean loss: 158.54
 ---- batch: 050 ----
mean loss: 157.00
 ---- batch: 060 ----
mean loss: 157.66
 ---- batch: 070 ----
mean loss: 153.13
 ---- batch: 080 ----
mean loss: 158.20
 ---- batch: 090 ----
mean loss: 155.38
 ---- batch: 100 ----
mean loss: 160.27
 ---- batch: 110 ----
mean loss: 153.82
train mean loss: 155.69
epoch train time: 0:00:00.577533
elapsed time: 0:01:44.824327
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:18:13.119936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.98
 ---- batch: 020 ----
mean loss: 152.21
 ---- batch: 030 ----
mean loss: 155.27
 ---- batch: 040 ----
mean loss: 141.97
 ---- batch: 050 ----
mean loss: 168.63
 ---- batch: 060 ----
mean loss: 156.41
 ---- batch: 070 ----
mean loss: 153.34
 ---- batch: 080 ----
mean loss: 155.04
 ---- batch: 090 ----
mean loss: 160.87
 ---- batch: 100 ----
mean loss: 156.64
 ---- batch: 110 ----
mean loss: 158.95
train mean loss: 156.18
epoch train time: 0:00:00.561848
elapsed time: 0:01:45.386301
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:18:13.681923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.78
 ---- batch: 020 ----
mean loss: 144.94
 ---- batch: 030 ----
mean loss: 155.83
 ---- batch: 040 ----
mean loss: 154.61
 ---- batch: 050 ----
mean loss: 157.49
 ---- batch: 060 ----
mean loss: 153.78
 ---- batch: 070 ----
mean loss: 155.70
 ---- batch: 080 ----
mean loss: 161.55
 ---- batch: 090 ----
mean loss: 150.98
 ---- batch: 100 ----
mean loss: 165.32
 ---- batch: 110 ----
mean loss: 149.44
train mean loss: 155.62
epoch train time: 0:00:00.567546
elapsed time: 0:01:45.954005
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:18:14.249647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.82
 ---- batch: 020 ----
mean loss: 159.29
 ---- batch: 030 ----
mean loss: 153.35
 ---- batch: 040 ----
mean loss: 151.80
 ---- batch: 050 ----
mean loss: 155.97
 ---- batch: 060 ----
mean loss: 156.65
 ---- batch: 070 ----
mean loss: 147.22
 ---- batch: 080 ----
mean loss: 155.08
 ---- batch: 090 ----
mean loss: 156.88
 ---- batch: 100 ----
mean loss: 156.32
 ---- batch: 110 ----
mean loss: 158.79
train mean loss: 155.11
epoch train time: 0:00:00.562089
elapsed time: 0:01:46.516255
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:18:14.811871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.81
 ---- batch: 020 ----
mean loss: 150.03
 ---- batch: 030 ----
mean loss: 154.52
 ---- batch: 040 ----
mean loss: 161.12
 ---- batch: 050 ----
mean loss: 148.35
 ---- batch: 060 ----
mean loss: 154.41
 ---- batch: 070 ----
mean loss: 162.32
 ---- batch: 080 ----
mean loss: 160.44
 ---- batch: 090 ----
mean loss: 162.56
 ---- batch: 100 ----
mean loss: 150.18
 ---- batch: 110 ----
mean loss: 153.93
train mean loss: 155.31
epoch train time: 0:00:00.570055
elapsed time: 0:01:47.086445
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:18:15.382066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.30
 ---- batch: 020 ----
mean loss: 154.16
 ---- batch: 030 ----
mean loss: 155.69
 ---- batch: 040 ----
mean loss: 155.72
 ---- batch: 050 ----
mean loss: 152.01
 ---- batch: 060 ----
mean loss: 150.59
 ---- batch: 070 ----
mean loss: 154.21
 ---- batch: 080 ----
mean loss: 149.95
 ---- batch: 090 ----
mean loss: 151.14
 ---- batch: 100 ----
mean loss: 159.53
 ---- batch: 110 ----
mean loss: 160.72
train mean loss: 154.41
epoch train time: 0:00:00.572208
elapsed time: 0:01:47.658793
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:18:15.954443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.61
 ---- batch: 020 ----
mean loss: 156.04
 ---- batch: 030 ----
mean loss: 150.33
 ---- batch: 040 ----
mean loss: 157.37
 ---- batch: 050 ----
mean loss: 155.11
 ---- batch: 060 ----
mean loss: 155.39
 ---- batch: 070 ----
mean loss: 159.07
 ---- batch: 080 ----
mean loss: 151.14
 ---- batch: 090 ----
mean loss: 147.99
 ---- batch: 100 ----
mean loss: 151.70
 ---- batch: 110 ----
mean loss: 150.64
train mean loss: 153.54
epoch train time: 0:00:00.564445
elapsed time: 0:01:48.223412
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:18:16.519030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.37
 ---- batch: 020 ----
mean loss: 147.05
 ---- batch: 030 ----
mean loss: 151.63
 ---- batch: 040 ----
mean loss: 147.53
 ---- batch: 050 ----
mean loss: 153.83
 ---- batch: 060 ----
mean loss: 147.88
 ---- batch: 070 ----
mean loss: 154.61
 ---- batch: 080 ----
mean loss: 152.57
 ---- batch: 090 ----
mean loss: 168.70
 ---- batch: 100 ----
mean loss: 150.19
 ---- batch: 110 ----
mean loss: 161.91
train mean loss: 154.36
epoch train time: 0:00:00.564079
elapsed time: 0:01:48.787624
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:18:17.083239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.56
 ---- batch: 020 ----
mean loss: 159.80
 ---- batch: 030 ----
mean loss: 162.97
 ---- batch: 040 ----
mean loss: 155.97
 ---- batch: 050 ----
mean loss: 156.81
 ---- batch: 060 ----
mean loss: 152.61
 ---- batch: 070 ----
mean loss: 151.97
 ---- batch: 080 ----
mean loss: 148.23
 ---- batch: 090 ----
mean loss: 148.31
 ---- batch: 100 ----
mean loss: 157.70
 ---- batch: 110 ----
mean loss: 153.87
train mean loss: 154.84
epoch train time: 0:00:00.559785
elapsed time: 0:01:49.347543
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:18:17.643159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.28
 ---- batch: 020 ----
mean loss: 156.33
 ---- batch: 030 ----
mean loss: 156.51
 ---- batch: 040 ----
mean loss: 157.44
 ---- batch: 050 ----
mean loss: 145.27
 ---- batch: 060 ----
mean loss: 155.64
 ---- batch: 070 ----
mean loss: 153.09
 ---- batch: 080 ----
mean loss: 154.34
 ---- batch: 090 ----
mean loss: 155.45
 ---- batch: 100 ----
mean loss: 148.15
 ---- batch: 110 ----
mean loss: 160.16
train mean loss: 153.74
epoch train time: 0:00:00.570927
elapsed time: 0:01:49.918604
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:18:18.214219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.28
 ---- batch: 020 ----
mean loss: 150.65
 ---- batch: 030 ----
mean loss: 157.15
 ---- batch: 040 ----
mean loss: 149.33
 ---- batch: 050 ----
mean loss: 149.48
 ---- batch: 060 ----
mean loss: 152.37
 ---- batch: 070 ----
mean loss: 159.36
 ---- batch: 080 ----
mean loss: 151.71
 ---- batch: 090 ----
mean loss: 157.76
 ---- batch: 100 ----
mean loss: 148.38
 ---- batch: 110 ----
mean loss: 149.84
train mean loss: 153.37
epoch train time: 0:00:00.564015
elapsed time: 0:01:50.482756
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:18:18.778373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.80
 ---- batch: 020 ----
mean loss: 146.99
 ---- batch: 030 ----
mean loss: 149.65
 ---- batch: 040 ----
mean loss: 152.60
 ---- batch: 050 ----
mean loss: 155.52
 ---- batch: 060 ----
mean loss: 156.10
 ---- batch: 070 ----
mean loss: 149.69
 ---- batch: 080 ----
mean loss: 155.07
 ---- batch: 090 ----
mean loss: 143.74
 ---- batch: 100 ----
mean loss: 160.88
 ---- batch: 110 ----
mean loss: 157.93
train mean loss: 151.71
epoch train time: 0:00:00.555409
elapsed time: 0:01:51.038359
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:18:19.333989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.20
 ---- batch: 020 ----
mean loss: 156.88
 ---- batch: 030 ----
mean loss: 142.23
 ---- batch: 040 ----
mean loss: 160.86
 ---- batch: 050 ----
mean loss: 145.85
 ---- batch: 060 ----
mean loss: 157.17
 ---- batch: 070 ----
mean loss: 146.89
 ---- batch: 080 ----
mean loss: 154.89
 ---- batch: 090 ----
mean loss: 147.23
 ---- batch: 100 ----
mean loss: 158.04
 ---- batch: 110 ----
mean loss: 152.31
train mean loss: 152.64
epoch train time: 0:00:00.559341
elapsed time: 0:01:51.597862
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:18:19.893476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.26
 ---- batch: 020 ----
mean loss: 150.45
 ---- batch: 030 ----
mean loss: 153.80
 ---- batch: 040 ----
mean loss: 162.31
 ---- batch: 050 ----
mean loss: 154.99
 ---- batch: 060 ----
mean loss: 157.23
 ---- batch: 070 ----
mean loss: 150.87
 ---- batch: 080 ----
mean loss: 149.49
 ---- batch: 090 ----
mean loss: 155.17
 ---- batch: 100 ----
mean loss: 145.80
 ---- batch: 110 ----
mean loss: 156.20
train mean loss: 152.64
epoch train time: 0:00:00.555928
elapsed time: 0:01:52.153928
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:18:20.449547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.31
 ---- batch: 020 ----
mean loss: 148.45
 ---- batch: 030 ----
mean loss: 152.67
 ---- batch: 040 ----
mean loss: 155.68
 ---- batch: 050 ----
mean loss: 151.38
 ---- batch: 060 ----
mean loss: 142.91
 ---- batch: 070 ----
mean loss: 153.15
 ---- batch: 080 ----
mean loss: 161.38
 ---- batch: 090 ----
mean loss: 162.06
 ---- batch: 100 ----
mean loss: 157.29
 ---- batch: 110 ----
mean loss: 147.07
train mean loss: 152.56
epoch train time: 0:00:00.561817
elapsed time: 0:01:52.715890
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:18:21.011508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.83
 ---- batch: 020 ----
mean loss: 154.94
 ---- batch: 030 ----
mean loss: 151.50
 ---- batch: 040 ----
mean loss: 145.75
 ---- batch: 050 ----
mean loss: 151.77
 ---- batch: 060 ----
mean loss: 150.66
 ---- batch: 070 ----
mean loss: 150.36
 ---- batch: 080 ----
mean loss: 153.27
 ---- batch: 090 ----
mean loss: 151.73
 ---- batch: 100 ----
mean loss: 149.70
 ---- batch: 110 ----
mean loss: 148.78
train mean loss: 151.46
epoch train time: 0:00:00.557402
elapsed time: 0:01:53.273430
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:18:21.569045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.88
 ---- batch: 020 ----
mean loss: 154.99
 ---- batch: 030 ----
mean loss: 147.46
 ---- batch: 040 ----
mean loss: 151.21
 ---- batch: 050 ----
mean loss: 146.81
 ---- batch: 060 ----
mean loss: 152.45
 ---- batch: 070 ----
mean loss: 148.63
 ---- batch: 080 ----
mean loss: 153.31
 ---- batch: 090 ----
mean loss: 143.48
 ---- batch: 100 ----
mean loss: 162.49
 ---- batch: 110 ----
mean loss: 162.72
train mean loss: 151.55
epoch train time: 0:00:00.578444
elapsed time: 0:01:53.852017
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:18:22.147640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.82
 ---- batch: 020 ----
mean loss: 141.98
 ---- batch: 030 ----
mean loss: 149.38
 ---- batch: 040 ----
mean loss: 153.21
 ---- batch: 050 ----
mean loss: 152.25
 ---- batch: 060 ----
mean loss: 143.35
 ---- batch: 070 ----
mean loss: 154.84
 ---- batch: 080 ----
mean loss: 156.69
 ---- batch: 090 ----
mean loss: 161.86
 ---- batch: 100 ----
mean loss: 157.55
 ---- batch: 110 ----
mean loss: 146.18
train mean loss: 151.74
epoch train time: 0:00:00.566799
elapsed time: 0:01:54.418958
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:18:22.714575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.36
 ---- batch: 020 ----
mean loss: 146.84
 ---- batch: 030 ----
mean loss: 158.07
 ---- batch: 040 ----
mean loss: 155.40
 ---- batch: 050 ----
mean loss: 147.02
 ---- batch: 060 ----
mean loss: 150.21
 ---- batch: 070 ----
mean loss: 148.09
 ---- batch: 080 ----
mean loss: 149.42
 ---- batch: 090 ----
mean loss: 150.97
 ---- batch: 100 ----
mean loss: 154.45
 ---- batch: 110 ----
mean loss: 151.10
train mean loss: 150.91
epoch train time: 0:00:00.561312
elapsed time: 0:01:54.980404
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:18:23.276039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.98
 ---- batch: 020 ----
mean loss: 152.60
 ---- batch: 030 ----
mean loss: 149.65
 ---- batch: 040 ----
mean loss: 146.55
 ---- batch: 050 ----
mean loss: 154.35
 ---- batch: 060 ----
mean loss: 151.19
 ---- batch: 070 ----
mean loss: 146.43
 ---- batch: 080 ----
mean loss: 155.64
 ---- batch: 090 ----
mean loss: 152.28
 ---- batch: 100 ----
mean loss: 148.14
 ---- batch: 110 ----
mean loss: 151.72
train mean loss: 150.04
epoch train time: 0:00:00.565890
elapsed time: 0:01:55.546450
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:18:23.842083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.85
 ---- batch: 020 ----
mean loss: 151.69
 ---- batch: 030 ----
mean loss: 143.28
 ---- batch: 040 ----
mean loss: 150.21
 ---- batch: 050 ----
mean loss: 146.17
 ---- batch: 060 ----
mean loss: 147.49
 ---- batch: 070 ----
mean loss: 153.40
 ---- batch: 080 ----
mean loss: 147.03
 ---- batch: 090 ----
mean loss: 149.79
 ---- batch: 100 ----
mean loss: 151.79
 ---- batch: 110 ----
mean loss: 154.93
train mean loss: 149.97
epoch train time: 0:00:00.559099
elapsed time: 0:01:56.105696
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:18:24.401307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.46
 ---- batch: 020 ----
mean loss: 150.60
 ---- batch: 030 ----
mean loss: 147.53
 ---- batch: 040 ----
mean loss: 147.82
 ---- batch: 050 ----
mean loss: 149.66
 ---- batch: 060 ----
mean loss: 148.16
 ---- batch: 070 ----
mean loss: 154.04
 ---- batch: 080 ----
mean loss: 151.07
 ---- batch: 090 ----
mean loss: 153.42
 ---- batch: 100 ----
mean loss: 159.35
 ---- batch: 110 ----
mean loss: 145.70
train mean loss: 150.42
epoch train time: 0:00:00.567488
elapsed time: 0:01:56.673316
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:18:24.968939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.34
 ---- batch: 020 ----
mean loss: 152.29
 ---- batch: 030 ----
mean loss: 152.71
 ---- batch: 040 ----
mean loss: 148.67
 ---- batch: 050 ----
mean loss: 148.73
 ---- batch: 060 ----
mean loss: 141.56
 ---- batch: 070 ----
mean loss: 151.68
 ---- batch: 080 ----
mean loss: 156.50
 ---- batch: 090 ----
mean loss: 151.65
 ---- batch: 100 ----
mean loss: 152.90
 ---- batch: 110 ----
mean loss: 148.74
train mean loss: 149.66
epoch train time: 0:00:00.555391
elapsed time: 0:01:57.228898
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:18:25.524504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.28
 ---- batch: 020 ----
mean loss: 145.13
 ---- batch: 030 ----
mean loss: 141.47
 ---- batch: 040 ----
mean loss: 141.58
 ---- batch: 050 ----
mean loss: 142.39
 ---- batch: 060 ----
mean loss: 153.47
 ---- batch: 070 ----
mean loss: 153.41
 ---- batch: 080 ----
mean loss: 154.90
 ---- batch: 090 ----
mean loss: 152.04
 ---- batch: 100 ----
mean loss: 155.09
 ---- batch: 110 ----
mean loss: 151.61
train mean loss: 150.00
epoch train time: 0:00:00.568829
elapsed time: 0:01:57.797854
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:18:26.093471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.24
 ---- batch: 020 ----
mean loss: 140.16
 ---- batch: 030 ----
mean loss: 149.42
 ---- batch: 040 ----
mean loss: 145.42
 ---- batch: 050 ----
mean loss: 149.79
 ---- batch: 060 ----
mean loss: 158.26
 ---- batch: 070 ----
mean loss: 148.77
 ---- batch: 080 ----
mean loss: 155.02
 ---- batch: 090 ----
mean loss: 153.71
 ---- batch: 100 ----
mean loss: 147.59
 ---- batch: 110 ----
mean loss: 147.38
train mean loss: 149.17
epoch train time: 0:00:00.580038
elapsed time: 0:01:58.378082
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:18:26.673718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.86
 ---- batch: 020 ----
mean loss: 142.24
 ---- batch: 030 ----
mean loss: 153.69
 ---- batch: 040 ----
mean loss: 149.47
 ---- batch: 050 ----
mean loss: 146.86
 ---- batch: 060 ----
mean loss: 154.56
 ---- batch: 070 ----
mean loss: 149.97
 ---- batch: 080 ----
mean loss: 149.75
 ---- batch: 090 ----
mean loss: 154.89
 ---- batch: 100 ----
mean loss: 153.67
 ---- batch: 110 ----
mean loss: 148.81
train mean loss: 150.24
epoch train time: 0:00:00.564991
elapsed time: 0:01:58.943227
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:18:27.238856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.90
 ---- batch: 020 ----
mean loss: 141.82
 ---- batch: 030 ----
mean loss: 147.44
 ---- batch: 040 ----
mean loss: 144.35
 ---- batch: 050 ----
mean loss: 148.73
 ---- batch: 060 ----
mean loss: 148.18
 ---- batch: 070 ----
mean loss: 149.01
 ---- batch: 080 ----
mean loss: 153.98
 ---- batch: 090 ----
mean loss: 152.42
 ---- batch: 100 ----
mean loss: 154.89
 ---- batch: 110 ----
mean loss: 146.44
train mean loss: 149.35
epoch train time: 0:00:00.563533
elapsed time: 0:01:59.506923
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:18:27.802541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.16
 ---- batch: 020 ----
mean loss: 138.52
 ---- batch: 030 ----
mean loss: 148.36
 ---- batch: 040 ----
mean loss: 149.00
 ---- batch: 050 ----
mean loss: 153.08
 ---- batch: 060 ----
mean loss: 152.07
 ---- batch: 070 ----
mean loss: 154.44
 ---- batch: 080 ----
mean loss: 146.61
 ---- batch: 090 ----
mean loss: 143.47
 ---- batch: 100 ----
mean loss: 148.32
 ---- batch: 110 ----
mean loss: 154.01
train mean loss: 148.93
epoch train time: 0:00:00.556030
elapsed time: 0:02:00.063099
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:18:28.358714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.91
 ---- batch: 020 ----
mean loss: 140.98
 ---- batch: 030 ----
mean loss: 148.48
 ---- batch: 040 ----
mean loss: 149.74
 ---- batch: 050 ----
mean loss: 150.07
 ---- batch: 060 ----
mean loss: 147.56
 ---- batch: 070 ----
mean loss: 145.43
 ---- batch: 080 ----
mean loss: 145.86
 ---- batch: 090 ----
mean loss: 155.27
 ---- batch: 100 ----
mean loss: 144.89
 ---- batch: 110 ----
mean loss: 153.96
train mean loss: 148.01
epoch train time: 0:00:00.558307
elapsed time: 0:02:00.621542
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:18:28.917158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.16
 ---- batch: 020 ----
mean loss: 142.27
 ---- batch: 030 ----
mean loss: 149.11
 ---- batch: 040 ----
mean loss: 147.29
 ---- batch: 050 ----
mean loss: 147.45
 ---- batch: 060 ----
mean loss: 148.15
 ---- batch: 070 ----
mean loss: 153.72
 ---- batch: 080 ----
mean loss: 147.43
 ---- batch: 090 ----
mean loss: 149.00
 ---- batch: 100 ----
mean loss: 150.49
 ---- batch: 110 ----
mean loss: 154.01
train mean loss: 148.89
epoch train time: 0:00:00.553887
elapsed time: 0:02:01.175567
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:18:29.471182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.46
 ---- batch: 020 ----
mean loss: 142.60
 ---- batch: 030 ----
mean loss: 153.22
 ---- batch: 040 ----
mean loss: 157.96
 ---- batch: 050 ----
mean loss: 149.12
 ---- batch: 060 ----
mean loss: 152.18
 ---- batch: 070 ----
mean loss: 153.26
 ---- batch: 080 ----
mean loss: 146.63
 ---- batch: 090 ----
mean loss: 145.22
 ---- batch: 100 ----
mean loss: 151.75
 ---- batch: 110 ----
mean loss: 146.61
train mean loss: 149.45
epoch train time: 0:00:00.566136
elapsed time: 0:02:01.741840
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:18:30.037456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.34
 ---- batch: 020 ----
mean loss: 143.47
 ---- batch: 030 ----
mean loss: 152.64
 ---- batch: 040 ----
mean loss: 148.24
 ---- batch: 050 ----
mean loss: 140.26
 ---- batch: 060 ----
mean loss: 154.53
 ---- batch: 070 ----
mean loss: 138.67
 ---- batch: 080 ----
mean loss: 146.45
 ---- batch: 090 ----
mean loss: 157.46
 ---- batch: 100 ----
mean loss: 147.45
 ---- batch: 110 ----
mean loss: 156.17
train mean loss: 147.91
epoch train time: 0:00:00.563139
elapsed time: 0:02:02.305115
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:18:30.600749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.16
 ---- batch: 020 ----
mean loss: 139.61
 ---- batch: 030 ----
mean loss: 149.48
 ---- batch: 040 ----
mean loss: 146.31
 ---- batch: 050 ----
mean loss: 147.50
 ---- batch: 060 ----
mean loss: 143.01
 ---- batch: 070 ----
mean loss: 153.94
 ---- batch: 080 ----
mean loss: 144.56
 ---- batch: 090 ----
mean loss: 143.37
 ---- batch: 100 ----
mean loss: 155.44
 ---- batch: 110 ----
mean loss: 151.14
train mean loss: 147.90
epoch train time: 0:00:00.565014
elapsed time: 0:02:02.870283
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:18:31.165900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.79
 ---- batch: 020 ----
mean loss: 145.25
 ---- batch: 030 ----
mean loss: 140.11
 ---- batch: 040 ----
mean loss: 155.23
 ---- batch: 050 ----
mean loss: 146.74
 ---- batch: 060 ----
mean loss: 144.17
 ---- batch: 070 ----
mean loss: 151.67
 ---- batch: 080 ----
mean loss: 149.21
 ---- batch: 090 ----
mean loss: 144.98
 ---- batch: 100 ----
mean loss: 151.40
 ---- batch: 110 ----
mean loss: 147.81
train mean loss: 148.01
epoch train time: 0:00:00.556628
elapsed time: 0:02:03.427057
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:18:31.722670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.16
 ---- batch: 020 ----
mean loss: 146.30
 ---- batch: 030 ----
mean loss: 143.08
 ---- batch: 040 ----
mean loss: 154.29
 ---- batch: 050 ----
mean loss: 157.97
 ---- batch: 060 ----
mean loss: 142.13
 ---- batch: 070 ----
mean loss: 138.56
 ---- batch: 080 ----
mean loss: 159.53
 ---- batch: 090 ----
mean loss: 152.67
 ---- batch: 100 ----
mean loss: 137.63
 ---- batch: 110 ----
mean loss: 143.88
train mean loss: 147.29
epoch train time: 0:00:00.576103
elapsed time: 0:02:04.003313
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:18:32.298929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.61
 ---- batch: 020 ----
mean loss: 146.39
 ---- batch: 030 ----
mean loss: 145.14
 ---- batch: 040 ----
mean loss: 150.78
 ---- batch: 050 ----
mean loss: 157.31
 ---- batch: 060 ----
mean loss: 149.07
 ---- batch: 070 ----
mean loss: 143.41
 ---- batch: 080 ----
mean loss: 145.82
 ---- batch: 090 ----
mean loss: 143.02
 ---- batch: 100 ----
mean loss: 154.40
 ---- batch: 110 ----
mean loss: 148.74
train mean loss: 147.76
epoch train time: 0:00:00.576769
elapsed time: 0:02:04.580234
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:18:32.875865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.35
 ---- batch: 020 ----
mean loss: 151.81
 ---- batch: 030 ----
mean loss: 145.39
 ---- batch: 040 ----
mean loss: 139.03
 ---- batch: 050 ----
mean loss: 159.19
 ---- batch: 060 ----
mean loss: 146.17
 ---- batch: 070 ----
mean loss: 141.09
 ---- batch: 080 ----
mean loss: 132.60
 ---- batch: 090 ----
mean loss: 143.88
 ---- batch: 100 ----
mean loss: 152.50
 ---- batch: 110 ----
mean loss: 152.08
train mean loss: 146.78
epoch train time: 0:00:00.555647
elapsed time: 0:02:05.136029
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:18:33.431642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.41
 ---- batch: 020 ----
mean loss: 145.58
 ---- batch: 030 ----
mean loss: 152.01
 ---- batch: 040 ----
mean loss: 153.94
 ---- batch: 050 ----
mean loss: 150.16
 ---- batch: 060 ----
mean loss: 146.94
 ---- batch: 070 ----
mean loss: 143.73
 ---- batch: 080 ----
mean loss: 145.34
 ---- batch: 090 ----
mean loss: 145.02
 ---- batch: 100 ----
mean loss: 139.93
 ---- batch: 110 ----
mean loss: 145.12
train mean loss: 147.07
epoch train time: 0:00:00.552779
elapsed time: 0:02:05.688965
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:18:33.984615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.32
 ---- batch: 020 ----
mean loss: 146.75
 ---- batch: 030 ----
mean loss: 144.76
 ---- batch: 040 ----
mean loss: 148.77
 ---- batch: 050 ----
mean loss: 146.84
 ---- batch: 060 ----
mean loss: 145.28
 ---- batch: 070 ----
mean loss: 158.47
 ---- batch: 080 ----
mean loss: 142.97
 ---- batch: 090 ----
mean loss: 138.27
 ---- batch: 100 ----
mean loss: 147.86
 ---- batch: 110 ----
mean loss: 146.17
train mean loss: 146.16
epoch train time: 0:00:00.556706
elapsed time: 0:02:06.245838
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:18:34.541451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.93
 ---- batch: 020 ----
mean loss: 140.80
 ---- batch: 030 ----
mean loss: 150.79
 ---- batch: 040 ----
mean loss: 147.87
 ---- batch: 050 ----
mean loss: 157.74
 ---- batch: 060 ----
mean loss: 142.75
 ---- batch: 070 ----
mean loss: 146.44
 ---- batch: 080 ----
mean loss: 150.36
 ---- batch: 090 ----
mean loss: 150.82
 ---- batch: 100 ----
mean loss: 143.69
 ---- batch: 110 ----
mean loss: 135.16
train mean loss: 146.73
epoch train time: 0:00:00.566804
elapsed time: 0:02:06.812780
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:18:35.108395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.30
 ---- batch: 020 ----
mean loss: 147.42
 ---- batch: 030 ----
mean loss: 144.40
 ---- batch: 040 ----
mean loss: 143.18
 ---- batch: 050 ----
mean loss: 140.30
 ---- batch: 060 ----
mean loss: 146.31
 ---- batch: 070 ----
mean loss: 133.06
 ---- batch: 080 ----
mean loss: 158.00
 ---- batch: 090 ----
mean loss: 150.54
 ---- batch: 100 ----
mean loss: 159.13
 ---- batch: 110 ----
mean loss: 142.71
train mean loss: 146.13
epoch train time: 0:00:00.569516
elapsed time: 0:02:07.382433
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:18:35.678056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.76
 ---- batch: 020 ----
mean loss: 141.15
 ---- batch: 030 ----
mean loss: 139.38
 ---- batch: 040 ----
mean loss: 138.03
 ---- batch: 050 ----
mean loss: 147.19
 ---- batch: 060 ----
mean loss: 143.75
 ---- batch: 070 ----
mean loss: 145.28
 ---- batch: 080 ----
mean loss: 139.95
 ---- batch: 090 ----
mean loss: 154.02
 ---- batch: 100 ----
mean loss: 150.07
 ---- batch: 110 ----
mean loss: 152.89
train mean loss: 145.30
epoch train time: 0:00:00.567964
elapsed time: 0:02:07.950538
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:18:36.246155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.70
 ---- batch: 020 ----
mean loss: 146.82
 ---- batch: 030 ----
mean loss: 140.75
 ---- batch: 040 ----
mean loss: 156.88
 ---- batch: 050 ----
mean loss: 156.65
 ---- batch: 060 ----
mean loss: 138.87
 ---- batch: 070 ----
mean loss: 135.03
 ---- batch: 080 ----
mean loss: 145.90
 ---- batch: 090 ----
mean loss: 149.04
 ---- batch: 100 ----
mean loss: 143.80
 ---- batch: 110 ----
mean loss: 147.52
train mean loss: 145.52
epoch train time: 0:00:00.574885
elapsed time: 0:02:08.525569
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:18:36.821214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.57
 ---- batch: 020 ----
mean loss: 145.17
 ---- batch: 030 ----
mean loss: 134.34
 ---- batch: 040 ----
mean loss: 150.76
 ---- batch: 050 ----
mean loss: 152.87
 ---- batch: 060 ----
mean loss: 148.36
 ---- batch: 070 ----
mean loss: 147.90
 ---- batch: 080 ----
mean loss: 151.45
 ---- batch: 090 ----
mean loss: 143.72
 ---- batch: 100 ----
mean loss: 151.86
 ---- batch: 110 ----
mean loss: 138.16
train mean loss: 145.76
epoch train time: 0:00:00.566326
elapsed time: 0:02:09.092058
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:18:37.387674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.63
 ---- batch: 020 ----
mean loss: 143.85
 ---- batch: 030 ----
mean loss: 147.55
 ---- batch: 040 ----
mean loss: 145.86
 ---- batch: 050 ----
mean loss: 143.44
 ---- batch: 060 ----
mean loss: 150.98
 ---- batch: 070 ----
mean loss: 141.52
 ---- batch: 080 ----
mean loss: 151.04
 ---- batch: 090 ----
mean loss: 151.39
 ---- batch: 100 ----
mean loss: 138.02
 ---- batch: 110 ----
mean loss: 140.36
train mean loss: 145.75
epoch train time: 0:00:00.567776
elapsed time: 0:02:09.659968
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:18:37.955602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.95
 ---- batch: 020 ----
mean loss: 153.28
 ---- batch: 030 ----
mean loss: 151.80
 ---- batch: 040 ----
mean loss: 140.06
 ---- batch: 050 ----
mean loss: 138.16
 ---- batch: 060 ----
mean loss: 138.35
 ---- batch: 070 ----
mean loss: 153.90
 ---- batch: 080 ----
mean loss: 143.20
 ---- batch: 090 ----
mean loss: 148.83
 ---- batch: 100 ----
mean loss: 146.03
 ---- batch: 110 ----
mean loss: 145.92
train mean loss: 144.93
epoch train time: 0:00:00.559075
elapsed time: 0:02:10.219229
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:18:38.514841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.32
 ---- batch: 020 ----
mean loss: 144.83
 ---- batch: 030 ----
mean loss: 145.88
 ---- batch: 040 ----
mean loss: 139.94
 ---- batch: 050 ----
mean loss: 146.50
 ---- batch: 060 ----
mean loss: 143.16
 ---- batch: 070 ----
mean loss: 136.31
 ---- batch: 080 ----
mean loss: 146.28
 ---- batch: 090 ----
mean loss: 148.66
 ---- batch: 100 ----
mean loss: 147.60
 ---- batch: 110 ----
mean loss: 156.18
train mean loss: 145.12
epoch train time: 0:00:00.573804
elapsed time: 0:02:10.793160
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:18:39.088777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.21
 ---- batch: 020 ----
mean loss: 141.88
 ---- batch: 030 ----
mean loss: 146.24
 ---- batch: 040 ----
mean loss: 143.48
 ---- batch: 050 ----
mean loss: 139.73
 ---- batch: 060 ----
mean loss: 142.02
 ---- batch: 070 ----
mean loss: 150.19
 ---- batch: 080 ----
mean loss: 137.20
 ---- batch: 090 ----
mean loss: 154.83
 ---- batch: 100 ----
mean loss: 136.60
 ---- batch: 110 ----
mean loss: 150.30
train mean loss: 144.79
epoch train time: 0:00:00.571032
elapsed time: 0:02:11.364344
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:18:39.659949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.86
 ---- batch: 020 ----
mean loss: 150.29
 ---- batch: 030 ----
mean loss: 142.32
 ---- batch: 040 ----
mean loss: 147.69
 ---- batch: 050 ----
mean loss: 150.79
 ---- batch: 060 ----
mean loss: 141.61
 ---- batch: 070 ----
mean loss: 147.09
 ---- batch: 080 ----
mean loss: 140.43
 ---- batch: 090 ----
mean loss: 153.14
 ---- batch: 100 ----
mean loss: 148.98
 ---- batch: 110 ----
mean loss: 143.36
train mean loss: 145.71
epoch train time: 0:00:00.558541
elapsed time: 0:02:11.923039
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:18:40.218652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.26
 ---- batch: 020 ----
mean loss: 141.49
 ---- batch: 030 ----
mean loss: 140.51
 ---- batch: 040 ----
mean loss: 142.18
 ---- batch: 050 ----
mean loss: 143.50
 ---- batch: 060 ----
mean loss: 146.47
 ---- batch: 070 ----
mean loss: 152.90
 ---- batch: 080 ----
mean loss: 144.28
 ---- batch: 090 ----
mean loss: 143.84
 ---- batch: 100 ----
mean loss: 143.35
 ---- batch: 110 ----
mean loss: 143.18
train mean loss: 144.43
epoch train time: 0:00:00.554227
elapsed time: 0:02:12.477409
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:18:40.773020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.62
 ---- batch: 020 ----
mean loss: 143.24
 ---- batch: 030 ----
mean loss: 135.14
 ---- batch: 040 ----
mean loss: 147.73
 ---- batch: 050 ----
mean loss: 149.75
 ---- batch: 060 ----
mean loss: 145.91
 ---- batch: 070 ----
mean loss: 143.68
 ---- batch: 080 ----
mean loss: 148.95
 ---- batch: 090 ----
mean loss: 138.14
 ---- batch: 100 ----
mean loss: 145.73
 ---- batch: 110 ----
mean loss: 144.17
train mean loss: 144.12
epoch train time: 0:00:00.559365
elapsed time: 0:02:13.036940
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:18:41.332572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.55
 ---- batch: 020 ----
mean loss: 148.86
 ---- batch: 030 ----
mean loss: 145.92
 ---- batch: 040 ----
mean loss: 139.13
 ---- batch: 050 ----
mean loss: 143.04
 ---- batch: 060 ----
mean loss: 145.10
 ---- batch: 070 ----
mean loss: 138.60
 ---- batch: 080 ----
mean loss: 142.80
 ---- batch: 090 ----
mean loss: 144.82
 ---- batch: 100 ----
mean loss: 147.84
 ---- batch: 110 ----
mean loss: 140.14
train mean loss: 143.78
epoch train time: 0:00:00.555323
elapsed time: 0:02:13.592451
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:18:41.888083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.87
 ---- batch: 020 ----
mean loss: 150.08
 ---- batch: 030 ----
mean loss: 146.10
 ---- batch: 040 ----
mean loss: 136.04
 ---- batch: 050 ----
mean loss: 136.41
 ---- batch: 060 ----
mean loss: 146.51
 ---- batch: 070 ----
mean loss: 137.80
 ---- batch: 080 ----
mean loss: 139.86
 ---- batch: 090 ----
mean loss: 148.94
 ---- batch: 100 ----
mean loss: 138.97
 ---- batch: 110 ----
mean loss: 147.95
train mean loss: 143.48
epoch train time: 0:00:00.562256
elapsed time: 0:02:14.154859
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:18:42.450475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.34
 ---- batch: 020 ----
mean loss: 139.34
 ---- batch: 030 ----
mean loss: 138.57
 ---- batch: 040 ----
mean loss: 146.25
 ---- batch: 050 ----
mean loss: 145.56
 ---- batch: 060 ----
mean loss: 139.28
 ---- batch: 070 ----
mean loss: 146.94
 ---- batch: 080 ----
mean loss: 149.32
 ---- batch: 090 ----
mean loss: 140.95
 ---- batch: 100 ----
mean loss: 146.43
 ---- batch: 110 ----
mean loss: 151.97
train mean loss: 144.32
epoch train time: 0:00:00.562269
elapsed time: 0:02:14.717261
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:18:43.012874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.45
 ---- batch: 020 ----
mean loss: 147.03
 ---- batch: 030 ----
mean loss: 135.39
 ---- batch: 040 ----
mean loss: 140.30
 ---- batch: 050 ----
mean loss: 149.93
 ---- batch: 060 ----
mean loss: 143.11
 ---- batch: 070 ----
mean loss: 148.01
 ---- batch: 080 ----
mean loss: 147.21
 ---- batch: 090 ----
mean loss: 139.22
 ---- batch: 100 ----
mean loss: 146.39
 ---- batch: 110 ----
mean loss: 145.44
train mean loss: 143.30
epoch train time: 0:00:00.559365
elapsed time: 0:02:15.276761
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:18:43.572371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.81
 ---- batch: 020 ----
mean loss: 142.76
 ---- batch: 030 ----
mean loss: 151.93
 ---- batch: 040 ----
mean loss: 133.00
 ---- batch: 050 ----
mean loss: 145.58
 ---- batch: 060 ----
mean loss: 134.32
 ---- batch: 070 ----
mean loss: 143.39
 ---- batch: 080 ----
mean loss: 143.80
 ---- batch: 090 ----
mean loss: 148.75
 ---- batch: 100 ----
mean loss: 147.07
 ---- batch: 110 ----
mean loss: 150.24
train mean loss: 143.47
epoch train time: 0:00:00.565859
elapsed time: 0:02:15.842751
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:18:44.138365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.87
 ---- batch: 020 ----
mean loss: 145.95
 ---- batch: 030 ----
mean loss: 142.53
 ---- batch: 040 ----
mean loss: 142.96
 ---- batch: 050 ----
mean loss: 145.83
 ---- batch: 060 ----
mean loss: 146.30
 ---- batch: 070 ----
mean loss: 141.81
 ---- batch: 080 ----
mean loss: 140.92
 ---- batch: 090 ----
mean loss: 145.16
 ---- batch: 100 ----
mean loss: 148.29
 ---- batch: 110 ----
mean loss: 142.33
train mean loss: 144.61
epoch train time: 0:00:00.567891
elapsed time: 0:02:16.410775
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:18:44.706421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.71
 ---- batch: 020 ----
mean loss: 142.84
 ---- batch: 030 ----
mean loss: 134.39
 ---- batch: 040 ----
mean loss: 139.43
 ---- batch: 050 ----
mean loss: 134.85
 ---- batch: 060 ----
mean loss: 135.50
 ---- batch: 070 ----
mean loss: 145.05
 ---- batch: 080 ----
mean loss: 148.93
 ---- batch: 090 ----
mean loss: 155.67
 ---- batch: 100 ----
mean loss: 146.11
 ---- batch: 110 ----
mean loss: 146.08
train mean loss: 142.65
epoch train time: 0:00:00.608549
elapsed time: 0:02:17.019523
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:18:45.315170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.96
 ---- batch: 020 ----
mean loss: 144.87
 ---- batch: 030 ----
mean loss: 144.14
 ---- batch: 040 ----
mean loss: 138.80
 ---- batch: 050 ----
mean loss: 147.03
 ---- batch: 060 ----
mean loss: 142.50
 ---- batch: 070 ----
mean loss: 143.15
 ---- batch: 080 ----
mean loss: 146.46
 ---- batch: 090 ----
mean loss: 137.96
 ---- batch: 100 ----
mean loss: 146.37
 ---- batch: 110 ----
mean loss: 151.45
train mean loss: 143.33
epoch train time: 0:00:00.583957
elapsed time: 0:02:17.603642
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:18:45.899268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.77
 ---- batch: 020 ----
mean loss: 136.11
 ---- batch: 030 ----
mean loss: 147.91
 ---- batch: 040 ----
mean loss: 141.80
 ---- batch: 050 ----
mean loss: 145.93
 ---- batch: 060 ----
mean loss: 143.97
 ---- batch: 070 ----
mean loss: 145.60
 ---- batch: 080 ----
mean loss: 140.00
 ---- batch: 090 ----
mean loss: 140.13
 ---- batch: 100 ----
mean loss: 144.57
 ---- batch: 110 ----
mean loss: 146.91
train mean loss: 142.81
epoch train time: 0:00:00.558940
elapsed time: 0:02:18.162729
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:18:46.458348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.95
 ---- batch: 020 ----
mean loss: 147.06
 ---- batch: 030 ----
mean loss: 148.05
 ---- batch: 040 ----
mean loss: 139.38
 ---- batch: 050 ----
mean loss: 145.12
 ---- batch: 060 ----
mean loss: 142.05
 ---- batch: 070 ----
mean loss: 143.21
 ---- batch: 080 ----
mean loss: 143.91
 ---- batch: 090 ----
mean loss: 140.10
 ---- batch: 100 ----
mean loss: 139.49
 ---- batch: 110 ----
mean loss: 142.79
train mean loss: 142.51
epoch train time: 0:00:00.560425
elapsed time: 0:02:18.723295
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:18:47.018910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.24
 ---- batch: 020 ----
mean loss: 138.90
 ---- batch: 030 ----
mean loss: 138.14
 ---- batch: 040 ----
mean loss: 146.72
 ---- batch: 050 ----
mean loss: 146.67
 ---- batch: 060 ----
mean loss: 148.86
 ---- batch: 070 ----
mean loss: 137.40
 ---- batch: 080 ----
mean loss: 136.40
 ---- batch: 090 ----
mean loss: 147.55
 ---- batch: 100 ----
mean loss: 144.09
 ---- batch: 110 ----
mean loss: 144.23
train mean loss: 142.39
epoch train time: 0:00:00.560574
elapsed time: 0:02:19.284017
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:18:47.579633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.01
 ---- batch: 020 ----
mean loss: 143.57
 ---- batch: 030 ----
mean loss: 147.81
 ---- batch: 040 ----
mean loss: 134.70
 ---- batch: 050 ----
mean loss: 143.28
 ---- batch: 060 ----
mean loss: 137.76
 ---- batch: 070 ----
mean loss: 140.35
 ---- batch: 080 ----
mean loss: 142.23
 ---- batch: 090 ----
mean loss: 147.90
 ---- batch: 100 ----
mean loss: 144.31
 ---- batch: 110 ----
mean loss: 138.58
train mean loss: 141.58
epoch train time: 0:00:00.566179
elapsed time: 0:02:19.850328
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:18:48.145948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.05
 ---- batch: 020 ----
mean loss: 140.52
 ---- batch: 030 ----
mean loss: 155.38
 ---- batch: 040 ----
mean loss: 140.21
 ---- batch: 050 ----
mean loss: 134.03
 ---- batch: 060 ----
mean loss: 138.27
 ---- batch: 070 ----
mean loss: 132.98
 ---- batch: 080 ----
mean loss: 149.43
 ---- batch: 090 ----
mean loss: 145.76
 ---- batch: 100 ----
mean loss: 138.99
 ---- batch: 110 ----
mean loss: 142.01
train mean loss: 141.68
epoch train time: 0:00:00.553786
elapsed time: 0:02:20.404264
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:18:48.699879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.34
 ---- batch: 020 ----
mean loss: 136.99
 ---- batch: 030 ----
mean loss: 144.69
 ---- batch: 040 ----
mean loss: 134.30
 ---- batch: 050 ----
mean loss: 138.89
 ---- batch: 060 ----
mean loss: 145.95
 ---- batch: 070 ----
mean loss: 152.02
 ---- batch: 080 ----
mean loss: 140.59
 ---- batch: 090 ----
mean loss: 144.29
 ---- batch: 100 ----
mean loss: 146.86
 ---- batch: 110 ----
mean loss: 148.21
train mean loss: 142.52
epoch train time: 0:00:00.568072
elapsed time: 0:02:20.972501
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:18:49.268165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.29
 ---- batch: 020 ----
mean loss: 132.61
 ---- batch: 030 ----
mean loss: 141.44
 ---- batch: 040 ----
mean loss: 142.01
 ---- batch: 050 ----
mean loss: 145.11
 ---- batch: 060 ----
mean loss: 146.17
 ---- batch: 070 ----
mean loss: 138.34
 ---- batch: 080 ----
mean loss: 136.53
 ---- batch: 090 ----
mean loss: 144.68
 ---- batch: 100 ----
mean loss: 144.10
 ---- batch: 110 ----
mean loss: 141.61
train mean loss: 140.84
epoch train time: 0:00:00.588651
elapsed time: 0:02:21.561374
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:18:49.857025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.11
 ---- batch: 020 ----
mean loss: 141.63
 ---- batch: 030 ----
mean loss: 144.31
 ---- batch: 040 ----
mean loss: 140.41
 ---- batch: 050 ----
mean loss: 147.29
 ---- batch: 060 ----
mean loss: 144.24
 ---- batch: 070 ----
mean loss: 131.48
 ---- batch: 080 ----
mean loss: 140.14
 ---- batch: 090 ----
mean loss: 136.33
 ---- batch: 100 ----
mean loss: 137.27
 ---- batch: 110 ----
mean loss: 143.50
train mean loss: 141.19
epoch train time: 0:00:00.583205
elapsed time: 0:02:22.144748
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:18:50.440363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.40
 ---- batch: 020 ----
mean loss: 141.64
 ---- batch: 030 ----
mean loss: 136.22
 ---- batch: 040 ----
mean loss: 141.98
 ---- batch: 050 ----
mean loss: 144.68
 ---- batch: 060 ----
mean loss: 138.39
 ---- batch: 070 ----
mean loss: 142.91
 ---- batch: 080 ----
mean loss: 137.74
 ---- batch: 090 ----
mean loss: 136.08
 ---- batch: 100 ----
mean loss: 148.73
 ---- batch: 110 ----
mean loss: 153.25
train mean loss: 142.05
epoch train time: 0:00:00.580266
elapsed time: 0:02:22.725147
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:18:51.020767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.52
 ---- batch: 020 ----
mean loss: 148.79
 ---- batch: 030 ----
mean loss: 141.71
 ---- batch: 040 ----
mean loss: 142.33
 ---- batch: 050 ----
mean loss: 135.94
 ---- batch: 060 ----
mean loss: 133.68
 ---- batch: 070 ----
mean loss: 142.87
 ---- batch: 080 ----
mean loss: 140.14
 ---- batch: 090 ----
mean loss: 141.72
 ---- batch: 100 ----
mean loss: 144.55
 ---- batch: 110 ----
mean loss: 140.64
train mean loss: 140.92
epoch train time: 0:00:00.574547
elapsed time: 0:02:23.299834
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:18:51.595459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.15
 ---- batch: 020 ----
mean loss: 147.06
 ---- batch: 030 ----
mean loss: 150.21
 ---- batch: 040 ----
mean loss: 142.75
 ---- batch: 050 ----
mean loss: 134.00
 ---- batch: 060 ----
mean loss: 137.19
 ---- batch: 070 ----
mean loss: 143.63
 ---- batch: 080 ----
mean loss: 134.68
 ---- batch: 090 ----
mean loss: 149.09
 ---- batch: 100 ----
mean loss: 137.58
 ---- batch: 110 ----
mean loss: 141.61
train mean loss: 141.10
epoch train time: 0:00:00.574780
elapsed time: 0:02:23.874758
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:18:52.170373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.53
 ---- batch: 020 ----
mean loss: 137.57
 ---- batch: 030 ----
mean loss: 138.26
 ---- batch: 040 ----
mean loss: 150.72
 ---- batch: 050 ----
mean loss: 134.10
 ---- batch: 060 ----
mean loss: 133.67
 ---- batch: 070 ----
mean loss: 144.40
 ---- batch: 080 ----
mean loss: 139.95
 ---- batch: 090 ----
mean loss: 140.95
 ---- batch: 100 ----
mean loss: 138.15
 ---- batch: 110 ----
mean loss: 145.83
train mean loss: 140.78
epoch train time: 0:00:00.582612
elapsed time: 0:02:24.457534
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:18:52.753192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.23
 ---- batch: 020 ----
mean loss: 136.88
 ---- batch: 030 ----
mean loss: 141.92
 ---- batch: 040 ----
mean loss: 134.11
 ---- batch: 050 ----
mean loss: 139.70
 ---- batch: 060 ----
mean loss: 134.81
 ---- batch: 070 ----
mean loss: 156.93
 ---- batch: 080 ----
mean loss: 145.44
 ---- batch: 090 ----
mean loss: 143.60
 ---- batch: 100 ----
mean loss: 141.30
 ---- batch: 110 ----
mean loss: 133.54
train mean loss: 140.58
epoch train time: 0:00:00.583609
elapsed time: 0:02:25.041337
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:18:53.336952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.43
 ---- batch: 020 ----
mean loss: 136.33
 ---- batch: 030 ----
mean loss: 140.67
 ---- batch: 040 ----
mean loss: 136.16
 ---- batch: 050 ----
mean loss: 136.91
 ---- batch: 060 ----
mean loss: 151.76
 ---- batch: 070 ----
mean loss: 135.70
 ---- batch: 080 ----
mean loss: 145.24
 ---- batch: 090 ----
mean loss: 140.39
 ---- batch: 100 ----
mean loss: 136.80
 ---- batch: 110 ----
mean loss: 145.37
train mean loss: 140.82
epoch train time: 0:00:00.580260
elapsed time: 0:02:25.621748
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:18:53.917379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.44
 ---- batch: 020 ----
mean loss: 133.30
 ---- batch: 030 ----
mean loss: 147.00
 ---- batch: 040 ----
mean loss: 129.57
 ---- batch: 050 ----
mean loss: 138.11
 ---- batch: 060 ----
mean loss: 149.63
 ---- batch: 070 ----
mean loss: 143.01
 ---- batch: 080 ----
mean loss: 139.00
 ---- batch: 090 ----
mean loss: 137.71
 ---- batch: 100 ----
mean loss: 139.58
 ---- batch: 110 ----
mean loss: 144.45
train mean loss: 140.11
epoch train time: 0:00:00.570974
elapsed time: 0:02:26.192867
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:18:54.488481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.48
 ---- batch: 020 ----
mean loss: 133.54
 ---- batch: 030 ----
mean loss: 133.05
 ---- batch: 040 ----
mean loss: 135.30
 ---- batch: 050 ----
mean loss: 142.17
 ---- batch: 060 ----
mean loss: 141.24
 ---- batch: 070 ----
mean loss: 139.99
 ---- batch: 080 ----
mean loss: 135.69
 ---- batch: 090 ----
mean loss: 142.09
 ---- batch: 100 ----
mean loss: 141.16
 ---- batch: 110 ----
mean loss: 152.27
train mean loss: 139.98
epoch train time: 0:00:00.576772
elapsed time: 0:02:26.769774
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:18:55.065391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.47
 ---- batch: 020 ----
mean loss: 133.98
 ---- batch: 030 ----
mean loss: 144.58
 ---- batch: 040 ----
mean loss: 132.47
 ---- batch: 050 ----
mean loss: 129.20
 ---- batch: 060 ----
mean loss: 131.89
 ---- batch: 070 ----
mean loss: 127.70
 ---- batch: 080 ----
mean loss: 137.96
 ---- batch: 090 ----
mean loss: 138.05
 ---- batch: 100 ----
mean loss: 138.52
 ---- batch: 110 ----
mean loss: 129.22
train mean loss: 134.57
epoch train time: 0:00:00.578557
elapsed time: 0:02:27.348489
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:18:55.644099
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.89
 ---- batch: 020 ----
mean loss: 132.26
 ---- batch: 030 ----
mean loss: 127.91
 ---- batch: 040 ----
mean loss: 136.88
 ---- batch: 050 ----
mean loss: 133.03
 ---- batch: 060 ----
mean loss: 136.82
 ---- batch: 070 ----
mean loss: 125.06
 ---- batch: 080 ----
mean loss: 141.47
 ---- batch: 090 ----
mean loss: 133.70
 ---- batch: 100 ----
mean loss: 135.15
 ---- batch: 110 ----
mean loss: 128.27
train mean loss: 133.71
epoch train time: 0:00:00.608028
elapsed time: 0:02:27.956659
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:18:56.252306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.27
 ---- batch: 020 ----
mean loss: 128.91
 ---- batch: 030 ----
mean loss: 137.97
 ---- batch: 040 ----
mean loss: 129.84
 ---- batch: 050 ----
mean loss: 135.58
 ---- batch: 060 ----
mean loss: 135.18
 ---- batch: 070 ----
mean loss: 129.92
 ---- batch: 080 ----
mean loss: 141.27
 ---- batch: 090 ----
mean loss: 135.93
 ---- batch: 100 ----
mean loss: 130.67
 ---- batch: 110 ----
mean loss: 134.72
train mean loss: 133.42
epoch train time: 0:00:00.565114
elapsed time: 0:02:28.521940
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:18:56.817576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.08
 ---- batch: 020 ----
mean loss: 129.14
 ---- batch: 030 ----
mean loss: 128.82
 ---- batch: 040 ----
mean loss: 129.83
 ---- batch: 050 ----
mean loss: 140.35
 ---- batch: 060 ----
mean loss: 132.66
 ---- batch: 070 ----
mean loss: 140.23
 ---- batch: 080 ----
mean loss: 132.17
 ---- batch: 090 ----
mean loss: 139.78
 ---- batch: 100 ----
mean loss: 129.31
 ---- batch: 110 ----
mean loss: 128.92
train mean loss: 133.30
epoch train time: 0:00:00.567109
elapsed time: 0:02:29.089275
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:18:57.384892
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 139.63
 ---- batch: 020 ----
mean loss: 133.52
 ---- batch: 030 ----
mean loss: 133.21
 ---- batch: 040 ----
mean loss: 132.21
 ---- batch: 050 ----
mean loss: 129.99
 ---- batch: 060 ----
mean loss: 137.56
 ---- batch: 070 ----
mean loss: 134.74
 ---- batch: 080 ----
mean loss: 141.35
 ---- batch: 090 ----
mean loss: 124.80
 ---- batch: 100 ----
mean loss: 128.57
 ---- batch: 110 ----
mean loss: 133.15
train mean loss: 133.32
epoch train time: 0:00:00.600546
elapsed time: 0:02:29.689967
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:18:57.985586
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.70
 ---- batch: 020 ----
mean loss: 129.71
 ---- batch: 030 ----
mean loss: 131.75
 ---- batch: 040 ----
mean loss: 137.26
 ---- batch: 050 ----
mean loss: 125.79
 ---- batch: 060 ----
mean loss: 139.30
 ---- batch: 070 ----
mean loss: 135.03
 ---- batch: 080 ----
mean loss: 134.99
 ---- batch: 090 ----
mean loss: 133.17
 ---- batch: 100 ----
mean loss: 126.20
 ---- batch: 110 ----
mean loss: 135.98
train mean loss: 133.19
epoch train time: 0:00:00.601349
elapsed time: 0:02:30.291458
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:18:58.587073
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.57
 ---- batch: 020 ----
mean loss: 133.67
 ---- batch: 030 ----
mean loss: 134.15
 ---- batch: 040 ----
mean loss: 139.05
 ---- batch: 050 ----
mean loss: 131.11
 ---- batch: 060 ----
mean loss: 133.47
 ---- batch: 070 ----
mean loss: 126.31
 ---- batch: 080 ----
mean loss: 139.10
 ---- batch: 090 ----
mean loss: 130.94
 ---- batch: 100 ----
mean loss: 138.66
 ---- batch: 110 ----
mean loss: 128.57
train mean loss: 133.11
epoch train time: 0:00:00.573862
elapsed time: 0:02:30.865455
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:18:59.161070
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.47
 ---- batch: 020 ----
mean loss: 130.57
 ---- batch: 030 ----
mean loss: 131.73
 ---- batch: 040 ----
mean loss: 131.57
 ---- batch: 050 ----
mean loss: 138.89
 ---- batch: 060 ----
mean loss: 136.69
 ---- batch: 070 ----
mean loss: 140.03
 ---- batch: 080 ----
mean loss: 135.93
 ---- batch: 090 ----
mean loss: 128.73
 ---- batch: 100 ----
mean loss: 126.25
 ---- batch: 110 ----
mean loss: 134.00
train mean loss: 133.15
epoch train time: 0:00:00.561206
elapsed time: 0:02:31.426792
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:18:59.722407
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.36
 ---- batch: 020 ----
mean loss: 120.55
 ---- batch: 030 ----
mean loss: 138.00
 ---- batch: 040 ----
mean loss: 131.38
 ---- batch: 050 ----
mean loss: 127.20
 ---- batch: 060 ----
mean loss: 138.12
 ---- batch: 070 ----
mean loss: 135.51
 ---- batch: 080 ----
mean loss: 129.22
 ---- batch: 090 ----
mean loss: 137.19
 ---- batch: 100 ----
mean loss: 132.80
 ---- batch: 110 ----
mean loss: 144.11
train mean loss: 133.03
epoch train time: 0:00:00.617563
elapsed time: 0:02:32.044526
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:19:00.340172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 139.39
 ---- batch: 020 ----
mean loss: 129.60
 ---- batch: 030 ----
mean loss: 132.29
 ---- batch: 040 ----
mean loss: 139.45
 ---- batch: 050 ----
mean loss: 133.64
 ---- batch: 060 ----
mean loss: 131.72
 ---- batch: 070 ----
mean loss: 130.61
 ---- batch: 080 ----
mean loss: 129.13
 ---- batch: 090 ----
mean loss: 130.37
 ---- batch: 100 ----
mean loss: 129.43
 ---- batch: 110 ----
mean loss: 129.53
train mean loss: 133.07
epoch train time: 0:00:00.613845
elapsed time: 0:02:32.658537
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:19:00.954153
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.24
 ---- batch: 020 ----
mean loss: 137.27
 ---- batch: 030 ----
mean loss: 138.56
 ---- batch: 040 ----
mean loss: 132.53
 ---- batch: 050 ----
mean loss: 136.67
 ---- batch: 060 ----
mean loss: 128.16
 ---- batch: 070 ----
mean loss: 128.22
 ---- batch: 080 ----
mean loss: 131.36
 ---- batch: 090 ----
mean loss: 140.29
 ---- batch: 100 ----
mean loss: 127.52
 ---- batch: 110 ----
mean loss: 131.65
train mean loss: 133.07
epoch train time: 0:00:00.569302
elapsed time: 0:02:33.227978
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:19:01.523596
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.57
 ---- batch: 020 ----
mean loss: 129.19
 ---- batch: 030 ----
mean loss: 134.17
 ---- batch: 040 ----
mean loss: 137.14
 ---- batch: 050 ----
mean loss: 132.66
 ---- batch: 060 ----
mean loss: 134.87
 ---- batch: 070 ----
mean loss: 135.80
 ---- batch: 080 ----
mean loss: 130.13
 ---- batch: 090 ----
mean loss: 129.91
 ---- batch: 100 ----
mean loss: 137.35
 ---- batch: 110 ----
mean loss: 132.83
train mean loss: 133.01
epoch train time: 0:00:00.599808
elapsed time: 0:02:33.827936
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:19:02.123554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.03
 ---- batch: 020 ----
mean loss: 130.78
 ---- batch: 030 ----
mean loss: 123.95
 ---- batch: 040 ----
mean loss: 141.73
 ---- batch: 050 ----
mean loss: 134.18
 ---- batch: 060 ----
mean loss: 139.07
 ---- batch: 070 ----
mean loss: 126.60
 ---- batch: 080 ----
mean loss: 131.88
 ---- batch: 090 ----
mean loss: 124.84
 ---- batch: 100 ----
mean loss: 136.65
 ---- batch: 110 ----
mean loss: 138.75
train mean loss: 133.02
epoch train time: 0:00:00.583242
elapsed time: 0:02:34.411317
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:19:02.706943
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.37
 ---- batch: 020 ----
mean loss: 128.07
 ---- batch: 030 ----
mean loss: 128.50
 ---- batch: 040 ----
mean loss: 125.72
 ---- batch: 050 ----
mean loss: 136.05
 ---- batch: 060 ----
mean loss: 132.28
 ---- batch: 070 ----
mean loss: 141.66
 ---- batch: 080 ----
mean loss: 132.75
 ---- batch: 090 ----
mean loss: 132.98
 ---- batch: 100 ----
mean loss: 136.82
 ---- batch: 110 ----
mean loss: 138.05
train mean loss: 133.02
epoch train time: 0:00:00.576967
elapsed time: 0:02:34.988433
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:19:03.284043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.61
 ---- batch: 020 ----
mean loss: 130.41
 ---- batch: 030 ----
mean loss: 139.19
 ---- batch: 040 ----
mean loss: 133.55
 ---- batch: 050 ----
mean loss: 129.27
 ---- batch: 060 ----
mean loss: 129.22
 ---- batch: 070 ----
mean loss: 138.11
 ---- batch: 080 ----
mean loss: 130.71
 ---- batch: 090 ----
mean loss: 122.64
 ---- batch: 100 ----
mean loss: 133.38
 ---- batch: 110 ----
mean loss: 135.14
train mean loss: 132.99
epoch train time: 0:00:00.571296
elapsed time: 0:02:35.559878
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:19:03.855494
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 141.15
 ---- batch: 020 ----
mean loss: 135.31
 ---- batch: 030 ----
mean loss: 132.93
 ---- batch: 040 ----
mean loss: 129.93
 ---- batch: 050 ----
mean loss: 130.29
 ---- batch: 060 ----
mean loss: 135.03
 ---- batch: 070 ----
mean loss: 130.39
 ---- batch: 080 ----
mean loss: 136.33
 ---- batch: 090 ----
mean loss: 131.05
 ---- batch: 100 ----
mean loss: 129.48
 ---- batch: 110 ----
mean loss: 132.75
train mean loss: 132.93
epoch train time: 0:00:00.574016
elapsed time: 0:02:36.134032
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:19:04.429686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.18
 ---- batch: 020 ----
mean loss: 131.69
 ---- batch: 030 ----
mean loss: 132.27
 ---- batch: 040 ----
mean loss: 135.30
 ---- batch: 050 ----
mean loss: 129.92
 ---- batch: 060 ----
mean loss: 135.05
 ---- batch: 070 ----
mean loss: 129.50
 ---- batch: 080 ----
mean loss: 138.86
 ---- batch: 090 ----
mean loss: 136.50
 ---- batch: 100 ----
mean loss: 137.46
 ---- batch: 110 ----
mean loss: 130.93
train mean loss: 133.01
epoch train time: 0:00:00.579213
elapsed time: 0:02:36.713424
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:19:05.009044
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.66
 ---- batch: 020 ----
mean loss: 134.77
 ---- batch: 030 ----
mean loss: 133.36
 ---- batch: 040 ----
mean loss: 133.86
 ---- batch: 050 ----
mean loss: 127.68
 ---- batch: 060 ----
mean loss: 132.33
 ---- batch: 070 ----
mean loss: 129.87
 ---- batch: 080 ----
mean loss: 133.08
 ---- batch: 090 ----
mean loss: 130.20
 ---- batch: 100 ----
mean loss: 141.21
 ---- batch: 110 ----
mean loss: 129.30
train mean loss: 132.87
epoch train time: 0:00:00.581683
elapsed time: 0:02:37.295277
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:19:05.590906
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.61
 ---- batch: 020 ----
mean loss: 131.58
 ---- batch: 030 ----
mean loss: 129.71
 ---- batch: 040 ----
mean loss: 132.47
 ---- batch: 050 ----
mean loss: 133.50
 ---- batch: 060 ----
mean loss: 123.91
 ---- batch: 070 ----
mean loss: 133.13
 ---- batch: 080 ----
mean loss: 141.72
 ---- batch: 090 ----
mean loss: 133.54
 ---- batch: 100 ----
mean loss: 132.99
 ---- batch: 110 ----
mean loss: 135.84
train mean loss: 132.88
epoch train time: 0:00:00.576733
elapsed time: 0:02:37.872162
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:19:06.167780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.75
 ---- batch: 020 ----
mean loss: 132.64
 ---- batch: 030 ----
mean loss: 139.99
 ---- batch: 040 ----
mean loss: 134.75
 ---- batch: 050 ----
mean loss: 139.47
 ---- batch: 060 ----
mean loss: 128.84
 ---- batch: 070 ----
mean loss: 128.05
 ---- batch: 080 ----
mean loss: 133.34
 ---- batch: 090 ----
mean loss: 131.01
 ---- batch: 100 ----
mean loss: 136.93
 ---- batch: 110 ----
mean loss: 134.18
train mean loss: 132.82
epoch train time: 0:00:00.568479
elapsed time: 0:02:38.440793
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:19:06.736409
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.18
 ---- batch: 020 ----
mean loss: 130.74
 ---- batch: 030 ----
mean loss: 137.55
 ---- batch: 040 ----
mean loss: 134.44
 ---- batch: 050 ----
mean loss: 128.44
 ---- batch: 060 ----
mean loss: 129.40
 ---- batch: 070 ----
mean loss: 139.68
 ---- batch: 080 ----
mean loss: 144.13
 ---- batch: 090 ----
mean loss: 127.17
 ---- batch: 100 ----
mean loss: 128.65
 ---- batch: 110 ----
mean loss: 131.81
train mean loss: 132.73
epoch train time: 0:00:00.569959
elapsed time: 0:02:39.010928
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:19:07.306575
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.93
 ---- batch: 020 ----
mean loss: 133.34
 ---- batch: 030 ----
mean loss: 139.61
 ---- batch: 040 ----
mean loss: 125.68
 ---- batch: 050 ----
mean loss: 130.98
 ---- batch: 060 ----
mean loss: 132.87
 ---- batch: 070 ----
mean loss: 132.59
 ---- batch: 080 ----
mean loss: 124.97
 ---- batch: 090 ----
mean loss: 129.83
 ---- batch: 100 ----
mean loss: 136.83
 ---- batch: 110 ----
mean loss: 132.67
train mean loss: 132.86
epoch train time: 0:00:00.565878
elapsed time: 0:02:39.576980
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:19:07.872600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.91
 ---- batch: 020 ----
mean loss: 135.41
 ---- batch: 030 ----
mean loss: 127.79
 ---- batch: 040 ----
mean loss: 129.55
 ---- batch: 050 ----
mean loss: 137.34
 ---- batch: 060 ----
mean loss: 138.23
 ---- batch: 070 ----
mean loss: 132.85
 ---- batch: 080 ----
mean loss: 131.60
 ---- batch: 090 ----
mean loss: 138.94
 ---- batch: 100 ----
mean loss: 129.82
 ---- batch: 110 ----
mean loss: 130.96
train mean loss: 132.78
epoch train time: 0:00:00.581956
elapsed time: 0:02:40.159075
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:19:08.454709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.90
 ---- batch: 020 ----
mean loss: 135.50
 ---- batch: 030 ----
mean loss: 128.89
 ---- batch: 040 ----
mean loss: 130.83
 ---- batch: 050 ----
mean loss: 136.32
 ---- batch: 060 ----
mean loss: 134.53
 ---- batch: 070 ----
mean loss: 135.69
 ---- batch: 080 ----
mean loss: 141.79
 ---- batch: 090 ----
mean loss: 131.93
 ---- batch: 100 ----
mean loss: 122.78
 ---- batch: 110 ----
mean loss: 131.01
train mean loss: 132.84
epoch train time: 0:00:00.581212
elapsed time: 0:02:40.740447
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:19:09.036072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.78
 ---- batch: 020 ----
mean loss: 126.62
 ---- batch: 030 ----
mean loss: 133.79
 ---- batch: 040 ----
mean loss: 134.89
 ---- batch: 050 ----
mean loss: 124.41
 ---- batch: 060 ----
mean loss: 136.36
 ---- batch: 070 ----
mean loss: 131.57
 ---- batch: 080 ----
mean loss: 125.12
 ---- batch: 090 ----
mean loss: 137.23
 ---- batch: 100 ----
mean loss: 138.18
 ---- batch: 110 ----
mean loss: 135.98
train mean loss: 132.80
epoch train time: 0:00:00.600550
elapsed time: 0:02:41.341161
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:19:09.636777
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.11
 ---- batch: 020 ----
mean loss: 143.62
 ---- batch: 030 ----
mean loss: 133.09
 ---- batch: 040 ----
mean loss: 130.32
 ---- batch: 050 ----
mean loss: 130.55
 ---- batch: 060 ----
mean loss: 124.48
 ---- batch: 070 ----
mean loss: 141.97
 ---- batch: 080 ----
mean loss: 126.39
 ---- batch: 090 ----
mean loss: 135.57
 ---- batch: 100 ----
mean loss: 134.34
 ---- batch: 110 ----
mean loss: 132.39
train mean loss: 132.56
epoch train time: 0:00:00.567665
elapsed time: 0:02:41.908992
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:19:10.204607
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.62
 ---- batch: 020 ----
mean loss: 128.66
 ---- batch: 030 ----
mean loss: 132.79
 ---- batch: 040 ----
mean loss: 126.35
 ---- batch: 050 ----
mean loss: 131.30
 ---- batch: 060 ----
mean loss: 134.60
 ---- batch: 070 ----
mean loss: 138.65
 ---- batch: 080 ----
mean loss: 138.12
 ---- batch: 090 ----
mean loss: 131.56
 ---- batch: 100 ----
mean loss: 135.96
 ---- batch: 110 ----
mean loss: 135.32
train mean loss: 132.62
epoch train time: 0:00:00.578644
elapsed time: 0:02:42.487777
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:19:10.783396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.28
 ---- batch: 020 ----
mean loss: 131.03
 ---- batch: 030 ----
mean loss: 131.83
 ---- batch: 040 ----
mean loss: 136.08
 ---- batch: 050 ----
mean loss: 132.50
 ---- batch: 060 ----
mean loss: 133.57
 ---- batch: 070 ----
mean loss: 135.55
 ---- batch: 080 ----
mean loss: 132.85
 ---- batch: 090 ----
mean loss: 131.07
 ---- batch: 100 ----
mean loss: 129.58
 ---- batch: 110 ----
mean loss: 133.64
train mean loss: 132.54
epoch train time: 0:00:00.575434
elapsed time: 0:02:43.063353
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:19:11.358982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.45
 ---- batch: 020 ----
mean loss: 137.44
 ---- batch: 030 ----
mean loss: 138.93
 ---- batch: 040 ----
mean loss: 138.01
 ---- batch: 050 ----
mean loss: 132.29
 ---- batch: 060 ----
mean loss: 135.96
 ---- batch: 070 ----
mean loss: 128.90
 ---- batch: 080 ----
mean loss: 121.83
 ---- batch: 090 ----
mean loss: 135.92
 ---- batch: 100 ----
mean loss: 133.89
 ---- batch: 110 ----
mean loss: 132.33
train mean loss: 132.60
epoch train time: 0:00:00.574697
elapsed time: 0:02:43.638205
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:19:11.933844
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.51
 ---- batch: 020 ----
mean loss: 131.20
 ---- batch: 030 ----
mean loss: 127.70
 ---- batch: 040 ----
mean loss: 141.17
 ---- batch: 050 ----
mean loss: 130.12
 ---- batch: 060 ----
mean loss: 133.28
 ---- batch: 070 ----
mean loss: 129.31
 ---- batch: 080 ----
mean loss: 131.77
 ---- batch: 090 ----
mean loss: 130.12
 ---- batch: 100 ----
mean loss: 135.34
 ---- batch: 110 ----
mean loss: 137.46
train mean loss: 132.79
epoch train time: 0:00:00.638494
elapsed time: 0:02:44.276885
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:19:12.572505
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.19
 ---- batch: 020 ----
mean loss: 123.35
 ---- batch: 030 ----
mean loss: 128.30
 ---- batch: 040 ----
mean loss: 141.92
 ---- batch: 050 ----
mean loss: 136.30
 ---- batch: 060 ----
mean loss: 124.31
 ---- batch: 070 ----
mean loss: 131.67
 ---- batch: 080 ----
mean loss: 141.63
 ---- batch: 090 ----
mean loss: 129.14
 ---- batch: 100 ----
mean loss: 141.62
 ---- batch: 110 ----
mean loss: 135.90
train mean loss: 132.64
epoch train time: 0:00:00.575229
elapsed time: 0:02:44.852248
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:19:13.147862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.30
 ---- batch: 020 ----
mean loss: 131.39
 ---- batch: 030 ----
mean loss: 134.38
 ---- batch: 040 ----
mean loss: 130.07
 ---- batch: 050 ----
mean loss: 122.69
 ---- batch: 060 ----
mean loss: 141.95
 ---- batch: 070 ----
mean loss: 139.29
 ---- batch: 080 ----
mean loss: 135.37
 ---- batch: 090 ----
mean loss: 134.92
 ---- batch: 100 ----
mean loss: 125.62
 ---- batch: 110 ----
mean loss: 129.93
train mean loss: 132.49
epoch train time: 0:00:00.560205
elapsed time: 0:02:45.412598
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:19:13.708226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.88
 ---- batch: 020 ----
mean loss: 124.07
 ---- batch: 030 ----
mean loss: 131.26
 ---- batch: 040 ----
mean loss: 135.85
 ---- batch: 050 ----
mean loss: 129.10
 ---- batch: 060 ----
mean loss: 134.68
 ---- batch: 070 ----
mean loss: 137.94
 ---- batch: 080 ----
mean loss: 137.41
 ---- batch: 090 ----
mean loss: 133.43
 ---- batch: 100 ----
mean loss: 133.98
 ---- batch: 110 ----
mean loss: 133.32
train mean loss: 132.55
epoch train time: 0:00:00.578233
elapsed time: 0:02:45.990997
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:19:14.286605
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.06
 ---- batch: 020 ----
mean loss: 130.85
 ---- batch: 030 ----
mean loss: 127.08
 ---- batch: 040 ----
mean loss: 139.07
 ---- batch: 050 ----
mean loss: 129.17
 ---- batch: 060 ----
mean loss: 132.75
 ---- batch: 070 ----
mean loss: 132.55
 ---- batch: 080 ----
mean loss: 135.41
 ---- batch: 090 ----
mean loss: 138.21
 ---- batch: 100 ----
mean loss: 134.98
 ---- batch: 110 ----
mean loss: 129.93
train mean loss: 132.43
epoch train time: 0:00:00.566121
elapsed time: 0:02:46.557250
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:19:14.852865
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.35
 ---- batch: 020 ----
mean loss: 129.85
 ---- batch: 030 ----
mean loss: 128.54
 ---- batch: 040 ----
mean loss: 139.27
 ---- batch: 050 ----
mean loss: 128.15
 ---- batch: 060 ----
mean loss: 138.92
 ---- batch: 070 ----
mean loss: 136.35
 ---- batch: 080 ----
mean loss: 128.83
 ---- batch: 090 ----
mean loss: 130.75
 ---- batch: 100 ----
mean loss: 132.65
 ---- batch: 110 ----
mean loss: 129.33
train mean loss: 132.51
epoch train time: 0:00:00.567239
elapsed time: 0:02:47.124624
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:19:15.420242
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.57
 ---- batch: 020 ----
mean loss: 131.09
 ---- batch: 030 ----
mean loss: 133.92
 ---- batch: 040 ----
mean loss: 137.99
 ---- batch: 050 ----
mean loss: 133.07
 ---- batch: 060 ----
mean loss: 143.39
 ---- batch: 070 ----
mean loss: 131.34
 ---- batch: 080 ----
mean loss: 128.83
 ---- batch: 090 ----
mean loss: 126.24
 ---- batch: 100 ----
mean loss: 133.62
 ---- batch: 110 ----
mean loss: 126.71
train mean loss: 132.45
epoch train time: 0:00:00.567391
elapsed time: 0:02:47.692157
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:19:15.987795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.29
 ---- batch: 020 ----
mean loss: 142.60
 ---- batch: 030 ----
mean loss: 129.11
 ---- batch: 040 ----
mean loss: 131.12
 ---- batch: 050 ----
mean loss: 130.10
 ---- batch: 060 ----
mean loss: 135.26
 ---- batch: 070 ----
mean loss: 123.22
 ---- batch: 080 ----
mean loss: 129.76
 ---- batch: 090 ----
mean loss: 135.35
 ---- batch: 100 ----
mean loss: 132.59
 ---- batch: 110 ----
mean loss: 126.74
train mean loss: 132.43
epoch train time: 0:00:00.598429
elapsed time: 0:02:48.290749
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:19:16.586369
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.10
 ---- batch: 020 ----
mean loss: 136.95
 ---- batch: 030 ----
mean loss: 126.29
 ---- batch: 040 ----
mean loss: 135.93
 ---- batch: 050 ----
mean loss: 136.93
 ---- batch: 060 ----
mean loss: 134.28
 ---- batch: 070 ----
mean loss: 140.81
 ---- batch: 080 ----
mean loss: 124.06
 ---- batch: 090 ----
mean loss: 126.54
 ---- batch: 100 ----
mean loss: 134.85
 ---- batch: 110 ----
mean loss: 131.26
train mean loss: 132.39
epoch train time: 0:00:00.578137
elapsed time: 0:02:48.869032
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:19:17.164650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.05
 ---- batch: 020 ----
mean loss: 136.37
 ---- batch: 030 ----
mean loss: 134.13
 ---- batch: 040 ----
mean loss: 137.40
 ---- batch: 050 ----
mean loss: 130.42
 ---- batch: 060 ----
mean loss: 142.56
 ---- batch: 070 ----
mean loss: 135.69
 ---- batch: 080 ----
mean loss: 137.54
 ---- batch: 090 ----
mean loss: 126.24
 ---- batch: 100 ----
mean loss: 128.07
 ---- batch: 110 ----
mean loss: 123.04
train mean loss: 132.47
epoch train time: 0:00:00.593896
elapsed time: 0:02:49.463081
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:19:17.758706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.75
 ---- batch: 020 ----
mean loss: 131.13
 ---- batch: 030 ----
mean loss: 129.86
 ---- batch: 040 ----
mean loss: 129.63
 ---- batch: 050 ----
mean loss: 130.06
 ---- batch: 060 ----
mean loss: 131.97
 ---- batch: 070 ----
mean loss: 132.68
 ---- batch: 080 ----
mean loss: 134.18
 ---- batch: 090 ----
mean loss: 145.94
 ---- batch: 100 ----
mean loss: 128.25
 ---- batch: 110 ----
mean loss: 134.75
train mean loss: 132.39
epoch train time: 0:00:00.599040
elapsed time: 0:02:50.062278
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:19:18.357896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.68
 ---- batch: 020 ----
mean loss: 129.16
 ---- batch: 030 ----
mean loss: 126.05
 ---- batch: 040 ----
mean loss: 131.02
 ---- batch: 050 ----
mean loss: 133.52
 ---- batch: 060 ----
mean loss: 136.94
 ---- batch: 070 ----
mean loss: 135.80
 ---- batch: 080 ----
mean loss: 129.77
 ---- batch: 090 ----
mean loss: 135.20
 ---- batch: 100 ----
mean loss: 140.05
 ---- batch: 110 ----
mean loss: 127.70
train mean loss: 132.25
epoch train time: 0:00:00.587356
elapsed time: 0:02:50.649775
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:19:18.945394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.43
 ---- batch: 020 ----
mean loss: 141.81
 ---- batch: 030 ----
mean loss: 138.55
 ---- batch: 040 ----
mean loss: 125.46
 ---- batch: 050 ----
mean loss: 128.97
 ---- batch: 060 ----
mean loss: 136.25
 ---- batch: 070 ----
mean loss: 132.67
 ---- batch: 080 ----
mean loss: 120.37
 ---- batch: 090 ----
mean loss: 129.96
 ---- batch: 100 ----
mean loss: 137.69
 ---- batch: 110 ----
mean loss: 134.49
train mean loss: 132.45
epoch train time: 0:00:00.582360
elapsed time: 0:02:51.232273
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:19:19.527909
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.46
 ---- batch: 020 ----
mean loss: 140.04
 ---- batch: 030 ----
mean loss: 126.04
 ---- batch: 040 ----
mean loss: 138.91
 ---- batch: 050 ----
mean loss: 136.85
 ---- batch: 060 ----
mean loss: 128.75
 ---- batch: 070 ----
mean loss: 139.97
 ---- batch: 080 ----
mean loss: 123.40
 ---- batch: 090 ----
mean loss: 126.49
 ---- batch: 100 ----
mean loss: 138.34
 ---- batch: 110 ----
mean loss: 130.30
train mean loss: 132.29
epoch train time: 0:00:00.584528
elapsed time: 0:02:51.816953
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:19:20.112582
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.89
 ---- batch: 020 ----
mean loss: 125.63
 ---- batch: 030 ----
mean loss: 130.96
 ---- batch: 040 ----
mean loss: 136.98
 ---- batch: 050 ----
mean loss: 128.86
 ---- batch: 060 ----
mean loss: 130.34
 ---- batch: 070 ----
mean loss: 137.22
 ---- batch: 080 ----
mean loss: 135.43
 ---- batch: 090 ----
mean loss: 131.71
 ---- batch: 100 ----
mean loss: 134.47
 ---- batch: 110 ----
mean loss: 136.59
train mean loss: 132.31
epoch train time: 0:00:00.566912
elapsed time: 0:02:52.384012
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:19:20.679627
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.66
 ---- batch: 020 ----
mean loss: 131.43
 ---- batch: 030 ----
mean loss: 131.89
 ---- batch: 040 ----
mean loss: 129.14
 ---- batch: 050 ----
mean loss: 129.54
 ---- batch: 060 ----
mean loss: 136.15
 ---- batch: 070 ----
mean loss: 131.46
 ---- batch: 080 ----
mean loss: 127.34
 ---- batch: 090 ----
mean loss: 132.07
 ---- batch: 100 ----
mean loss: 138.79
 ---- batch: 110 ----
mean loss: 139.71
train mean loss: 132.29
epoch train time: 0:00:00.582886
elapsed time: 0:02:52.967051
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:19:21.262669
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.69
 ---- batch: 020 ----
mean loss: 132.71
 ---- batch: 030 ----
mean loss: 128.93
 ---- batch: 040 ----
mean loss: 128.64
 ---- batch: 050 ----
mean loss: 136.56
 ---- batch: 060 ----
mean loss: 130.74
 ---- batch: 070 ----
mean loss: 134.15
 ---- batch: 080 ----
mean loss: 132.43
 ---- batch: 090 ----
mean loss: 131.16
 ---- batch: 100 ----
mean loss: 133.31
 ---- batch: 110 ----
mean loss: 128.77
train mean loss: 132.29
epoch train time: 0:00:00.599756
elapsed time: 0:02:53.566956
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:19:21.862576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.24
 ---- batch: 020 ----
mean loss: 131.92
 ---- batch: 030 ----
mean loss: 133.24
 ---- batch: 040 ----
mean loss: 129.12
 ---- batch: 050 ----
mean loss: 131.20
 ---- batch: 060 ----
mean loss: 130.41
 ---- batch: 070 ----
mean loss: 133.65
 ---- batch: 080 ----
mean loss: 128.75
 ---- batch: 090 ----
mean loss: 130.29
 ---- batch: 100 ----
mean loss: 141.17
 ---- batch: 110 ----
mean loss: 138.89
train mean loss: 132.05
epoch train time: 0:00:00.578250
elapsed time: 0:02:54.145357
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:19:22.440975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.14
 ---- batch: 020 ----
mean loss: 130.49
 ---- batch: 030 ----
mean loss: 126.49
 ---- batch: 040 ----
mean loss: 133.62
 ---- batch: 050 ----
mean loss: 129.87
 ---- batch: 060 ----
mean loss: 128.37
 ---- batch: 070 ----
mean loss: 128.78
 ---- batch: 080 ----
mean loss: 129.24
 ---- batch: 090 ----
mean loss: 136.68
 ---- batch: 100 ----
mean loss: 139.70
 ---- batch: 110 ----
mean loss: 136.38
train mean loss: 132.15
epoch train time: 0:00:00.584021
elapsed time: 0:02:54.729531
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:19:23.025145
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.82
 ---- batch: 020 ----
mean loss: 130.44
 ---- batch: 030 ----
mean loss: 128.28
 ---- batch: 040 ----
mean loss: 129.88
 ---- batch: 050 ----
mean loss: 128.69
 ---- batch: 060 ----
mean loss: 136.03
 ---- batch: 070 ----
mean loss: 131.80
 ---- batch: 080 ----
mean loss: 135.98
 ---- batch: 090 ----
mean loss: 134.24
 ---- batch: 100 ----
mean loss: 136.11
 ---- batch: 110 ----
mean loss: 132.93
train mean loss: 132.18
epoch train time: 0:00:00.556739
elapsed time: 0:02:55.289305
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_4/checkpoint.pth.tar
**** end time: 2019-09-27 02:19:23.584890 ****
