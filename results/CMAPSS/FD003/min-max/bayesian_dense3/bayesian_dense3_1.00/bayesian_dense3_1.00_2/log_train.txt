Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 5754
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-09-20 21:40:26.456338 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 21:40:26.465144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4365.92
 ---- batch: 020 ----
mean loss: 4032.38
 ---- batch: 030 ----
mean loss: 3790.03
 ---- batch: 040 ----
mean loss: 3490.74
train mean loss: 3878.61
epoch train time: 0:00:14.682133
elapsed time: 0:00:14.696604
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 21:40:41.152977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3268.98
 ---- batch: 020 ----
mean loss: 3072.93
 ---- batch: 030 ----
mean loss: 2916.26
 ---- batch: 040 ----
mean loss: 2863.55
train mean loss: 3010.32
epoch train time: 0:00:00.652786
elapsed time: 0:00:15.349573
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 21:40:41.806012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2660.97
 ---- batch: 020 ----
mean loss: 2599.59
 ---- batch: 030 ----
mean loss: 2568.61
 ---- batch: 040 ----
mean loss: 2474.41
train mean loss: 2562.79
epoch train time: 0:00:00.629000
elapsed time: 0:00:15.978798
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 21:40:42.435197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2371.83
 ---- batch: 020 ----
mean loss: 2391.57
 ---- batch: 030 ----
mean loss: 2226.08
 ---- batch: 040 ----
mean loss: 2273.03
train mean loss: 2316.53
epoch train time: 0:00:00.659061
elapsed time: 0:00:16.638056
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 21:40:43.094457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2216.72
 ---- batch: 020 ----
mean loss: 2153.92
 ---- batch: 030 ----
mean loss: 2153.25
 ---- batch: 040 ----
mean loss: 2093.16
train mean loss: 2149.97
epoch train time: 0:00:00.668420
elapsed time: 0:00:17.306718
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 21:40:43.763132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2044.83
 ---- batch: 020 ----
mean loss: 2068.16
 ---- batch: 030 ----
mean loss: 2004.58
 ---- batch: 040 ----
mean loss: 1976.61
train mean loss: 2020.59
epoch train time: 0:00:00.685704
elapsed time: 0:00:17.992686
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 21:40:44.449131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1931.30
 ---- batch: 020 ----
mean loss: 1958.93
 ---- batch: 030 ----
mean loss: 1906.91
 ---- batch: 040 ----
mean loss: 1833.81
train mean loss: 1904.71
epoch train time: 0:00:00.680154
elapsed time: 0:00:18.673109
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 21:40:45.129518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1854.70
 ---- batch: 020 ----
mean loss: 1783.13
 ---- batch: 030 ----
mean loss: 1816.10
 ---- batch: 040 ----
mean loss: 1764.87
train mean loss: 1803.50
epoch train time: 0:00:00.664316
elapsed time: 0:00:19.337687
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 21:40:45.794107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1775.48
 ---- batch: 020 ----
mean loss: 1706.05
 ---- batch: 030 ----
mean loss: 1683.57
 ---- batch: 040 ----
mean loss: 1656.87
train mean loss: 1702.30
epoch train time: 0:00:00.644241
elapsed time: 0:00:19.982143
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 21:40:46.438550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1649.48
 ---- batch: 020 ----
mean loss: 1620.45
 ---- batch: 030 ----
mean loss: 1600.99
 ---- batch: 040 ----
mean loss: 1613.34
train mean loss: 1616.47
epoch train time: 0:00:00.673299
elapsed time: 0:00:20.655702
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 21:40:47.112111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1546.35
 ---- batch: 020 ----
mean loss: 1544.92
 ---- batch: 030 ----
mean loss: 1536.01
 ---- batch: 040 ----
mean loss: 1478.78
train mean loss: 1527.59
epoch train time: 0:00:00.694216
elapsed time: 0:00:21.350180
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 21:40:47.806626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1492.51
 ---- batch: 020 ----
mean loss: 1434.13
 ---- batch: 030 ----
mean loss: 1425.14
 ---- batch: 040 ----
mean loss: 1420.86
train mean loss: 1442.38
epoch train time: 0:00:00.705554
elapsed time: 0:00:22.056016
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 21:40:48.512448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1392.25
 ---- batch: 020 ----
mean loss: 1379.61
 ---- batch: 030 ----
mean loss: 1355.24
 ---- batch: 040 ----
mean loss: 1362.73
train mean loss: 1370.16
epoch train time: 0:00:00.668664
elapsed time: 0:00:22.724938
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 21:40:49.181359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1315.75
 ---- batch: 020 ----
mean loss: 1300.96
 ---- batch: 030 ----
mean loss: 1281.43
 ---- batch: 040 ----
mean loss: 1258.04
train mean loss: 1287.31
epoch train time: 0:00:00.658661
elapsed time: 0:00:23.383894
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 21:40:49.840330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1236.72
 ---- batch: 020 ----
mean loss: 1196.68
 ---- batch: 030 ----
mean loss: 1163.68
 ---- batch: 040 ----
mean loss: 1162.37
train mean loss: 1185.14
epoch train time: 0:00:00.644723
elapsed time: 0:00:24.028856
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 21:40:50.485261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1108.41
 ---- batch: 020 ----
mean loss: 1117.50
 ---- batch: 030 ----
mean loss: 1093.78
 ---- batch: 040 ----
mean loss: 1074.78
train mean loss: 1095.50
epoch train time: 0:00:00.699694
elapsed time: 0:00:24.728796
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 21:40:51.185206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1035.23
 ---- batch: 020 ----
mean loss: 1027.14
 ---- batch: 030 ----
mean loss: 1021.31
 ---- batch: 040 ----
mean loss: 1012.18
train mean loss: 1024.31
epoch train time: 0:00:00.694483
elapsed time: 0:00:25.423517
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 21:40:51.879935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.89
 ---- batch: 020 ----
mean loss: 957.55
 ---- batch: 030 ----
mean loss: 953.48
 ---- batch: 040 ----
mean loss: 935.59
train mean loss: 955.10
epoch train time: 0:00:00.689312
elapsed time: 0:00:26.113069
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 21:40:52.569499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.34
 ---- batch: 020 ----
mean loss: 882.24
 ---- batch: 030 ----
mean loss: 909.78
 ---- batch: 040 ----
mean loss: 877.19
train mean loss: 897.22
epoch train time: 0:00:00.647622
elapsed time: 0:00:26.760938
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 21:40:53.217331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.30
 ---- batch: 020 ----
mean loss: 851.70
 ---- batch: 030 ----
mean loss: 836.66
 ---- batch: 040 ----
mean loss: 803.62
train mean loss: 833.82
epoch train time: 0:00:00.643576
elapsed time: 0:00:27.404706
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 21:40:53.861126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 794.74
 ---- batch: 020 ----
mean loss: 797.28
 ---- batch: 030 ----
mean loss: 780.07
 ---- batch: 040 ----
mean loss: 765.94
train mean loss: 783.22
epoch train time: 0:00:00.650882
elapsed time: 0:00:28.055867
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 21:40:54.512254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.32
 ---- batch: 020 ----
mean loss: 723.28
 ---- batch: 030 ----
mean loss: 732.39
 ---- batch: 040 ----
mean loss: 721.83
train mean loss: 731.30
epoch train time: 0:00:00.680220
elapsed time: 0:00:28.736295
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 21:40:55.192702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 703.66
 ---- batch: 020 ----
mean loss: 690.71
 ---- batch: 030 ----
mean loss: 678.38
 ---- batch: 040 ----
mean loss: 662.88
train mean loss: 682.52
epoch train time: 0:00:00.680526
elapsed time: 0:00:29.417112
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 21:40:55.873505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 658.32
 ---- batch: 020 ----
mean loss: 655.07
 ---- batch: 030 ----
mean loss: 635.42
 ---- batch: 040 ----
mean loss: 620.56
train mean loss: 638.98
epoch train time: 0:00:00.664722
elapsed time: 0:00:30.082025
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 21:40:56.538466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 623.94
 ---- batch: 020 ----
mean loss: 598.72
 ---- batch: 030 ----
mean loss: 596.40
 ---- batch: 040 ----
mean loss: 584.27
train mean loss: 597.84
epoch train time: 0:00:00.662476
elapsed time: 0:00:30.744762
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 21:40:57.201179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.04
 ---- batch: 020 ----
mean loss: 572.42
 ---- batch: 030 ----
mean loss: 556.39
 ---- batch: 040 ----
mean loss: 538.87
train mean loss: 557.23
epoch train time: 0:00:00.638728
elapsed time: 0:00:31.383693
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 21:40:57.840138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 542.68
 ---- batch: 020 ----
mean loss: 525.86
 ---- batch: 030 ----
mean loss: 510.64
 ---- batch: 040 ----
mean loss: 510.88
train mean loss: 521.13
epoch train time: 0:00:00.672265
elapsed time: 0:00:32.056260
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 21:40:58.512667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.56
 ---- batch: 020 ----
mean loss: 491.48
 ---- batch: 030 ----
mean loss: 491.13
 ---- batch: 040 ----
mean loss: 477.44
train mean loss: 489.69
epoch train time: 0:00:00.682146
elapsed time: 0:00:32.738639
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 21:40:59.195048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.28
 ---- batch: 020 ----
mean loss: 463.55
 ---- batch: 030 ----
mean loss: 455.63
 ---- batch: 040 ----
mean loss: 451.87
train mean loss: 460.37
epoch train time: 0:00:00.711416
elapsed time: 0:00:33.450313
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 21:40:59.906725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.41
 ---- batch: 020 ----
mean loss: 426.33
 ---- batch: 030 ----
mean loss: 431.16
 ---- batch: 040 ----
mean loss: 422.34
train mean loss: 429.02
epoch train time: 0:00:00.656582
elapsed time: 0:00:34.107151
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 21:41:00.563577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.85
 ---- batch: 020 ----
mean loss: 408.64
 ---- batch: 030 ----
mean loss: 400.70
 ---- batch: 040 ----
mean loss: 397.58
train mean loss: 400.98
epoch train time: 0:00:00.673255
elapsed time: 0:00:34.780650
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 21:41:01.237053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.09
 ---- batch: 020 ----
mean loss: 383.41
 ---- batch: 030 ----
mean loss: 375.17
 ---- batch: 040 ----
mean loss: 366.79
train mean loss: 376.65
epoch train time: 0:00:00.666723
elapsed time: 0:00:35.447595
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 21:41:01.904008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.61
 ---- batch: 020 ----
mean loss: 360.21
 ---- batch: 030 ----
mean loss: 345.06
 ---- batch: 040 ----
mean loss: 355.36
train mean loss: 353.06
epoch train time: 0:00:00.698790
elapsed time: 0:00:36.146653
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 21:41:02.603068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.70
 ---- batch: 020 ----
mean loss: 330.65
 ---- batch: 030 ----
mean loss: 328.61
 ---- batch: 040 ----
mean loss: 327.10
train mean loss: 331.44
epoch train time: 0:00:00.690426
elapsed time: 0:00:36.837370
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 21:41:03.293782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.87
 ---- batch: 020 ----
mean loss: 317.90
 ---- batch: 030 ----
mean loss: 308.40
 ---- batch: 040 ----
mean loss: 299.64
train mean loss: 310.68
epoch train time: 0:00:00.676933
elapsed time: 0:00:37.514526
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 21:41:03.970948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.23
 ---- batch: 020 ----
mean loss: 294.36
 ---- batch: 030 ----
mean loss: 292.24
 ---- batch: 040 ----
mean loss: 291.98
train mean loss: 293.59
epoch train time: 0:00:00.661454
elapsed time: 0:00:38.176207
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 21:41:04.632612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.46
 ---- batch: 020 ----
mean loss: 278.22
 ---- batch: 030 ----
mean loss: 275.77
 ---- batch: 040 ----
mean loss: 271.35
train mean loss: 276.13
epoch train time: 0:00:00.658700
elapsed time: 0:00:38.835141
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 21:41:05.291545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.89
 ---- batch: 020 ----
mean loss: 260.24
 ---- batch: 030 ----
mean loss: 258.76
 ---- batch: 040 ----
mean loss: 256.80
train mean loss: 259.98
epoch train time: 0:00:00.665983
elapsed time: 0:00:39.501358
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 21:41:05.957780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.39
 ---- batch: 020 ----
mean loss: 248.59
 ---- batch: 030 ----
mean loss: 240.70
 ---- batch: 040 ----
mean loss: 242.82
train mean loss: 245.78
epoch train time: 0:00:00.697047
elapsed time: 0:00:40.198685
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 21:41:06.655104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.31
 ---- batch: 020 ----
mean loss: 228.62
 ---- batch: 030 ----
mean loss: 229.70
 ---- batch: 040 ----
mean loss: 225.78
train mean loss: 229.69
epoch train time: 0:00:00.706957
elapsed time: 0:00:40.905897
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 21:41:07.362324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.45
 ---- batch: 020 ----
mean loss: 218.79
 ---- batch: 030 ----
mean loss: 217.95
 ---- batch: 040 ----
mean loss: 213.82
train mean loss: 217.91
epoch train time: 0:00:00.672678
elapsed time: 0:00:41.579150
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 21:41:08.035568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.77
 ---- batch: 020 ----
mean loss: 203.58
 ---- batch: 030 ----
mean loss: 211.73
 ---- batch: 040 ----
mean loss: 203.42
train mean loss: 207.47
epoch train time: 0:00:00.673709
elapsed time: 0:00:42.253147
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 21:41:08.709568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.26
 ---- batch: 020 ----
mean loss: 196.37
 ---- batch: 030 ----
mean loss: 196.89
 ---- batch: 040 ----
mean loss: 194.22
train mean loss: 196.79
epoch train time: 0:00:00.686062
elapsed time: 0:00:42.939424
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 21:41:09.395840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.34
 ---- batch: 020 ----
mean loss: 192.46
 ---- batch: 030 ----
mean loss: 183.74
 ---- batch: 040 ----
mean loss: 179.96
train mean loss: 187.01
epoch train time: 0:00:00.686789
elapsed time: 0:00:43.626541
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 21:41:10.082971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.98
 ---- batch: 020 ----
mean loss: 178.18
 ---- batch: 030 ----
mean loss: 176.87
 ---- batch: 040 ----
mean loss: 174.31
train mean loss: 177.51
epoch train time: 0:00:00.707462
elapsed time: 0:00:44.334283
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 21:41:10.790699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.50
 ---- batch: 020 ----
mean loss: 174.75
 ---- batch: 030 ----
mean loss: 169.03
 ---- batch: 040 ----
mean loss: 167.75
train mean loss: 171.12
epoch train time: 0:00:00.668407
elapsed time: 0:00:45.002893
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 21:41:11.459311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.21
 ---- batch: 020 ----
mean loss: 161.29
 ---- batch: 030 ----
mean loss: 164.61
 ---- batch: 040 ----
mean loss: 160.26
train mean loss: 161.87
epoch train time: 0:00:00.639300
elapsed time: 0:00:45.642400
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 21:41:12.098803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.06
 ---- batch: 020 ----
mean loss: 154.45
 ---- batch: 030 ----
mean loss: 152.07
 ---- batch: 040 ----
mean loss: 152.86
train mean loss: 154.54
epoch train time: 0:00:00.640575
elapsed time: 0:00:46.283199
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 21:41:12.739607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.38
 ---- batch: 020 ----
mean loss: 146.80
 ---- batch: 030 ----
mean loss: 146.07
 ---- batch: 040 ----
mean loss: 149.80
train mean loss: 147.55
epoch train time: 0:00:00.669098
elapsed time: 0:00:46.952534
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 21:41:13.408960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.95
 ---- batch: 020 ----
mean loss: 144.78
 ---- batch: 030 ----
mean loss: 138.83
 ---- batch: 040 ----
mean loss: 140.07
train mean loss: 142.54
epoch train time: 0:00:00.692285
elapsed time: 0:00:47.645118
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 21:41:14.101539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.40
 ---- batch: 020 ----
mean loss: 136.45
 ---- batch: 030 ----
mean loss: 136.93
 ---- batch: 040 ----
mean loss: 136.36
train mean loss: 136.85
epoch train time: 0:00:00.697066
elapsed time: 0:00:48.342417
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 21:41:14.798842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.11
 ---- batch: 020 ----
mean loss: 128.37
 ---- batch: 030 ----
mean loss: 131.75
 ---- batch: 040 ----
mean loss: 131.14
train mean loss: 130.94
epoch train time: 0:00:00.673921
elapsed time: 0:00:49.016583
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 21:41:15.473008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.12
 ---- batch: 020 ----
mean loss: 127.08
 ---- batch: 030 ----
mean loss: 126.28
 ---- batch: 040 ----
mean loss: 124.03
train mean loss: 127.36
epoch train time: 0:00:00.652175
elapsed time: 0:00:49.668974
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 21:41:16.125388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.74
 ---- batch: 020 ----
mean loss: 121.47
 ---- batch: 030 ----
mean loss: 122.65
 ---- batch: 040 ----
mean loss: 123.03
train mean loss: 122.87
epoch train time: 0:00:00.643675
elapsed time: 0:00:50.312900
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 21:41:16.769311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.64
 ---- batch: 020 ----
mean loss: 120.15
 ---- batch: 030 ----
mean loss: 120.82
 ---- batch: 040 ----
mean loss: 118.49
train mean loss: 120.22
epoch train time: 0:00:00.694214
elapsed time: 0:00:51.007362
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 21:41:17.463808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.95
 ---- batch: 020 ----
mean loss: 117.53
 ---- batch: 030 ----
mean loss: 115.08
 ---- batch: 040 ----
mean loss: 109.71
train mean loss: 114.43
epoch train time: 0:00:00.691352
elapsed time: 0:00:51.699038
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 21:41:18.155445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.72
 ---- batch: 020 ----
mean loss: 111.08
 ---- batch: 030 ----
mean loss: 107.90
 ---- batch: 040 ----
mean loss: 111.71
train mean loss: 111.03
epoch train time: 0:00:00.678673
elapsed time: 0:00:52.377946
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 21:41:18.834374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.26
 ---- batch: 020 ----
mean loss: 108.78
 ---- batch: 030 ----
mean loss: 108.39
 ---- batch: 040 ----
mean loss: 106.89
train mean loss: 108.13
epoch train time: 0:00:00.655488
elapsed time: 0:00:53.033665
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 21:41:19.490074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.18
 ---- batch: 020 ----
mean loss: 105.48
 ---- batch: 030 ----
mean loss: 104.01
 ---- batch: 040 ----
mean loss: 107.74
train mean loss: 106.07
epoch train time: 0:00:00.652275
elapsed time: 0:00:53.686166
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 21:41:20.142593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.41
 ---- batch: 020 ----
mean loss: 102.63
 ---- batch: 030 ----
mean loss: 102.67
 ---- batch: 040 ----
mean loss: 100.84
train mean loss: 103.10
epoch train time: 0:00:00.649789
elapsed time: 0:00:54.336222
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 21:41:20.792637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.67
 ---- batch: 020 ----
mean loss: 98.93
 ---- batch: 030 ----
mean loss: 101.47
 ---- batch: 040 ----
mean loss: 100.28
train mean loss: 99.57
epoch train time: 0:00:00.678366
elapsed time: 0:00:55.014865
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 21:41:21.471305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.53
 ---- batch: 020 ----
mean loss: 98.58
 ---- batch: 030 ----
mean loss: 94.80
 ---- batch: 040 ----
mean loss: 97.81
train mean loss: 97.35
epoch train time: 0:00:00.696248
elapsed time: 0:00:55.711378
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 21:41:22.167813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.35
 ---- batch: 020 ----
mean loss: 96.74
 ---- batch: 030 ----
mean loss: 96.47
 ---- batch: 040 ----
mean loss: 97.08
train mean loss: 96.05
epoch train time: 0:00:00.682430
elapsed time: 0:00:56.394124
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 21:41:22.850537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.47
 ---- batch: 020 ----
mean loss: 90.56
 ---- batch: 030 ----
mean loss: 97.32
 ---- batch: 040 ----
mean loss: 85.93
train mean loss: 91.64
epoch train time: 0:00:00.629638
elapsed time: 0:00:57.023989
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 21:41:23.480438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.59
 ---- batch: 020 ----
mean loss: 90.17
 ---- batch: 030 ----
mean loss: 91.83
 ---- batch: 040 ----
mean loss: 93.42
train mean loss: 91.74
epoch train time: 0:00:00.666488
elapsed time: 0:00:57.690734
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 21:41:24.147149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.74
 ---- batch: 020 ----
mean loss: 91.71
 ---- batch: 030 ----
mean loss: 88.92
 ---- batch: 040 ----
mean loss: 91.90
train mean loss: 90.97
epoch train time: 0:00:00.691522
elapsed time: 0:00:58.382554
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 21:41:24.838989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.93
 ---- batch: 020 ----
mean loss: 88.21
 ---- batch: 030 ----
mean loss: 89.48
 ---- batch: 040 ----
mean loss: 88.69
train mean loss: 87.95
epoch train time: 0:00:00.709924
elapsed time: 0:00:59.092768
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 21:41:25.549200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.52
 ---- batch: 020 ----
mean loss: 87.99
 ---- batch: 030 ----
mean loss: 89.91
 ---- batch: 040 ----
mean loss: 86.84
train mean loss: 87.48
epoch train time: 0:00:00.680472
elapsed time: 0:00:59.773562
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 21:41:26.229994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.86
 ---- batch: 020 ----
mean loss: 84.69
 ---- batch: 030 ----
mean loss: 85.01
 ---- batch: 040 ----
mean loss: 84.72
train mean loss: 85.44
epoch train time: 0:00:00.659651
elapsed time: 0:01:00.433458
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 21:41:26.889872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.49
 ---- batch: 020 ----
mean loss: 84.17
 ---- batch: 030 ----
mean loss: 82.39
 ---- batch: 040 ----
mean loss: 86.26
train mean loss: 84.38
epoch train time: 0:00:00.651425
elapsed time: 0:01:01.085089
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 21:41:27.541494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.13
 ---- batch: 020 ----
mean loss: 81.96
 ---- batch: 030 ----
mean loss: 84.07
 ---- batch: 040 ----
mean loss: 83.40
train mean loss: 82.90
epoch train time: 0:00:00.684234
elapsed time: 0:01:01.769564
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 21:41:28.225980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.05
 ---- batch: 020 ----
mean loss: 81.50
 ---- batch: 030 ----
mean loss: 81.48
 ---- batch: 040 ----
mean loss: 78.70
train mean loss: 80.43
epoch train time: 0:00:00.734085
elapsed time: 0:01:02.503934
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 21:41:28.960362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.55
 ---- batch: 020 ----
mean loss: 84.42
 ---- batch: 030 ----
mean loss: 78.79
 ---- batch: 040 ----
mean loss: 79.12
train mean loss: 79.43
epoch train time: 0:00:00.692768
elapsed time: 0:01:03.196964
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 21:41:29.653428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.59
 ---- batch: 020 ----
mean loss: 76.42
 ---- batch: 030 ----
mean loss: 79.75
 ---- batch: 040 ----
mean loss: 79.10
train mean loss: 78.86
epoch train time: 0:00:00.649945
elapsed time: 0:01:03.847182
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 21:41:30.303605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.84
 ---- batch: 020 ----
mean loss: 79.21
 ---- batch: 030 ----
mean loss: 79.81
 ---- batch: 040 ----
mean loss: 78.13
train mean loss: 78.60
epoch train time: 0:00:00.664435
elapsed time: 0:01:04.511829
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 21:41:30.968246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.55
 ---- batch: 020 ----
mean loss: 78.79
 ---- batch: 030 ----
mean loss: 77.57
 ---- batch: 040 ----
mean loss: 75.48
train mean loss: 77.74
epoch train time: 0:00:00.629931
elapsed time: 0:01:05.141966
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 21:41:31.598370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.07
 ---- batch: 020 ----
mean loss: 71.81
 ---- batch: 030 ----
mean loss: 78.02
 ---- batch: 040 ----
mean loss: 77.11
train mean loss: 75.42
epoch train time: 0:00:00.684693
elapsed time: 0:01:05.826877
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 21:41:32.283318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.52
 ---- batch: 020 ----
mean loss: 76.49
 ---- batch: 030 ----
mean loss: 76.26
 ---- batch: 040 ----
mean loss: 77.11
train mean loss: 75.87
epoch train time: 0:00:00.707785
elapsed time: 0:01:06.534930
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 21:41:32.991338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.82
 ---- batch: 020 ----
mean loss: 74.05
 ---- batch: 030 ----
mean loss: 76.29
 ---- batch: 040 ----
mean loss: 74.86
train mean loss: 74.87
epoch train time: 0:00:00.687578
elapsed time: 0:01:07.222708
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 21:41:33.679115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.43
 ---- batch: 020 ----
mean loss: 71.35
 ---- batch: 030 ----
mean loss: 76.57
 ---- batch: 040 ----
mean loss: 76.44
train mean loss: 74.30
epoch train time: 0:00:00.665235
elapsed time: 0:01:07.888185
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 21:41:34.344607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.11
 ---- batch: 020 ----
mean loss: 71.64
 ---- batch: 030 ----
mean loss: 73.76
 ---- batch: 040 ----
mean loss: 74.05
train mean loss: 73.89
epoch train time: 0:00:00.673442
elapsed time: 0:01:08.561853
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 21:41:35.018260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.84
 ---- batch: 020 ----
mean loss: 72.31
 ---- batch: 030 ----
mean loss: 72.65
 ---- batch: 040 ----
mean loss: 74.05
train mean loss: 72.50
epoch train time: 0:00:00.658794
elapsed time: 0:01:09.220912
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 21:41:35.677339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.71
 ---- batch: 020 ----
mean loss: 68.68
 ---- batch: 030 ----
mean loss: 70.99
 ---- batch: 040 ----
mean loss: 72.70
train mean loss: 72.01
epoch train time: 0:00:00.707689
elapsed time: 0:01:09.928845
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 21:41:36.385251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.87
 ---- batch: 020 ----
mean loss: 72.90
 ---- batch: 030 ----
mean loss: 71.12
 ---- batch: 040 ----
mean loss: 74.88
train mean loss: 72.23
epoch train time: 0:00:00.699485
elapsed time: 0:01:10.628553
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 21:41:37.084976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.85
 ---- batch: 020 ----
mean loss: 71.21
 ---- batch: 030 ----
mean loss: 73.58
 ---- batch: 040 ----
mean loss: 68.73
train mean loss: 71.90
epoch train time: 0:00:00.686569
elapsed time: 0:01:11.315342
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 21:41:37.771749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.06
 ---- batch: 020 ----
mean loss: 70.88
 ---- batch: 030 ----
mean loss: 70.45
 ---- batch: 040 ----
mean loss: 71.30
train mean loss: 71.12
epoch train time: 0:00:00.636838
elapsed time: 0:01:11.952385
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 21:41:38.408806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.83
 ---- batch: 020 ----
mean loss: 71.57
 ---- batch: 030 ----
mean loss: 67.09
 ---- batch: 040 ----
mean loss: 70.31
train mean loss: 69.88
epoch train time: 0:00:00.663809
elapsed time: 0:01:12.616412
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 21:41:39.072829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.32
 ---- batch: 020 ----
mean loss: 67.93
 ---- batch: 030 ----
mean loss: 71.10
 ---- batch: 040 ----
mean loss: 67.45
train mean loss: 69.49
epoch train time: 0:00:00.686261
elapsed time: 0:01:13.302915
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 21:41:39.759338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.60
 ---- batch: 020 ----
mean loss: 70.96
 ---- batch: 030 ----
mean loss: 67.34
 ---- batch: 040 ----
mean loss: 68.97
train mean loss: 69.59
epoch train time: 0:00:00.685855
elapsed time: 0:01:13.989065
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 21:41:40.445482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.49
 ---- batch: 020 ----
mean loss: 67.60
 ---- batch: 030 ----
mean loss: 69.22
 ---- batch: 040 ----
mean loss: 70.76
train mean loss: 69.00
epoch train time: 0:00:00.676495
elapsed time: 0:01:14.665814
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 21:41:41.122228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.62
 ---- batch: 020 ----
mean loss: 71.75
 ---- batch: 030 ----
mean loss: 63.92
 ---- batch: 040 ----
mean loss: 69.98
train mean loss: 68.34
epoch train time: 0:00:00.632016
elapsed time: 0:01:15.298040
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 21:41:41.754444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.59
 ---- batch: 020 ----
mean loss: 70.20
 ---- batch: 030 ----
mean loss: 66.31
 ---- batch: 040 ----
mean loss: 66.83
train mean loss: 68.38
epoch train time: 0:00:00.634292
elapsed time: 0:01:15.932525
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 21:41:42.388929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.65
 ---- batch: 020 ----
mean loss: 66.70
 ---- batch: 030 ----
mean loss: 67.86
 ---- batch: 040 ----
mean loss: 67.80
train mean loss: 66.78
epoch train time: 0:00:00.658557
elapsed time: 0:01:16.591312
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 21:41:43.047723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.20
 ---- batch: 020 ----
mean loss: 67.59
 ---- batch: 030 ----
mean loss: 64.37
 ---- batch: 040 ----
mean loss: 66.56
train mean loss: 66.18
epoch train time: 0:00:00.683437
elapsed time: 0:01:17.275020
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 21:41:43.731454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.17
 ---- batch: 020 ----
mean loss: 67.62
 ---- batch: 030 ----
mean loss: 65.20
 ---- batch: 040 ----
mean loss: 66.41
train mean loss: 65.97
epoch train time: 0:00:00.679185
elapsed time: 0:01:17.954658
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 21:41:44.411094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.91
 ---- batch: 020 ----
mean loss: 66.75
 ---- batch: 030 ----
mean loss: 66.72
 ---- batch: 040 ----
mean loss: 65.43
train mean loss: 66.76
epoch train time: 0:00:00.666256
elapsed time: 0:01:18.621153
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 21:41:45.077561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.19
 ---- batch: 020 ----
mean loss: 68.22
 ---- batch: 030 ----
mean loss: 62.86
 ---- batch: 040 ----
mean loss: 64.78
train mean loss: 65.28
epoch train time: 0:00:00.652840
elapsed time: 0:01:19.274217
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 21:41:45.730639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.85
 ---- batch: 020 ----
mean loss: 64.31
 ---- batch: 030 ----
mean loss: 64.81
 ---- batch: 040 ----
mean loss: 64.97
train mean loss: 65.62
epoch train time: 0:00:00.648101
elapsed time: 0:01:19.922588
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 21:41:46.379040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.87
 ---- batch: 020 ----
mean loss: 65.49
 ---- batch: 030 ----
mean loss: 63.94
 ---- batch: 040 ----
mean loss: 67.41
train mean loss: 64.92
epoch train time: 0:00:00.689047
elapsed time: 0:01:20.611908
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 21:41:47.068345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.28
 ---- batch: 020 ----
mean loss: 66.43
 ---- batch: 030 ----
mean loss: 63.58
 ---- batch: 040 ----
mean loss: 62.27
train mean loss: 64.32
epoch train time: 0:00:00.682175
elapsed time: 0:01:21.294356
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 21:41:47.750774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.97
 ---- batch: 020 ----
mean loss: 64.18
 ---- batch: 030 ----
mean loss: 65.28
 ---- batch: 040 ----
mean loss: 63.71
train mean loss: 63.83
epoch train time: 0:00:00.670314
elapsed time: 0:01:21.964895
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 21:41:48.421306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.18
 ---- batch: 020 ----
mean loss: 62.19
 ---- batch: 030 ----
mean loss: 62.22
 ---- batch: 040 ----
mean loss: 64.24
train mean loss: 63.80
epoch train time: 0:00:00.668899
elapsed time: 0:01:22.634027
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 21:41:49.090456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.43
 ---- batch: 020 ----
mean loss: 62.55
 ---- batch: 030 ----
mean loss: 64.01
 ---- batch: 040 ----
mean loss: 64.62
train mean loss: 63.51
epoch train time: 0:00:00.654089
elapsed time: 0:01:23.288397
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 21:41:49.744825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.77
 ---- batch: 020 ----
mean loss: 64.31
 ---- batch: 030 ----
mean loss: 61.10
 ---- batch: 040 ----
mean loss: 60.61
train mean loss: 62.67
epoch train time: 0:00:00.647445
elapsed time: 0:01:23.936083
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 21:41:50.392491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.22
 ---- batch: 020 ----
mean loss: 65.01
 ---- batch: 030 ----
mean loss: 61.57
 ---- batch: 040 ----
mean loss: 66.65
train mean loss: 64.38
epoch train time: 0:00:00.682888
elapsed time: 0:01:24.619226
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 21:41:51.075638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.13
 ---- batch: 020 ----
mean loss: 63.61
 ---- batch: 030 ----
mean loss: 61.50
 ---- batch: 040 ----
mean loss: 64.17
train mean loss: 63.01
epoch train time: 0:00:00.685810
elapsed time: 0:01:25.305289
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 21:41:51.761706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.03
 ---- batch: 020 ----
mean loss: 60.54
 ---- batch: 030 ----
mean loss: 61.87
 ---- batch: 040 ----
mean loss: 63.02
train mean loss: 61.78
epoch train time: 0:00:00.680057
elapsed time: 0:01:25.985626
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 21:41:52.442002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.66
 ---- batch: 020 ----
mean loss: 61.15
 ---- batch: 030 ----
mean loss: 62.51
 ---- batch: 040 ----
mean loss: 61.86
train mean loss: 62.09
epoch train time: 0:00:00.659331
elapsed time: 0:01:26.645145
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 21:41:53.101553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.92
 ---- batch: 020 ----
mean loss: 59.36
 ---- batch: 030 ----
mean loss: 60.51
 ---- batch: 040 ----
mean loss: 65.13
train mean loss: 61.64
epoch train time: 0:00:00.650441
elapsed time: 0:01:27.295787
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 21:41:53.752193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.71
 ---- batch: 020 ----
mean loss: 60.08
 ---- batch: 030 ----
mean loss: 62.51
 ---- batch: 040 ----
mean loss: 58.51
train mean loss: 61.40
epoch train time: 0:00:00.651218
elapsed time: 0:01:27.947229
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 21:41:54.403678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.26
 ---- batch: 020 ----
mean loss: 59.28
 ---- batch: 030 ----
mean loss: 63.29
 ---- batch: 040 ----
mean loss: 60.51
train mean loss: 60.87
epoch train time: 0:00:00.683330
elapsed time: 0:01:28.630825
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 21:41:55.087234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.14
 ---- batch: 020 ----
mean loss: 61.45
 ---- batch: 030 ----
mean loss: 61.48
 ---- batch: 040 ----
mean loss: 58.31
train mean loss: 60.41
epoch train time: 0:00:00.678109
elapsed time: 0:01:29.309204
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 21:41:55.765656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.18
 ---- batch: 020 ----
mean loss: 61.31
 ---- batch: 030 ----
mean loss: 60.98
 ---- batch: 040 ----
mean loss: 58.00
train mean loss: 60.74
epoch train time: 0:00:00.650572
elapsed time: 0:01:29.960109
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 21:41:56.416555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.71
 ---- batch: 020 ----
mean loss: 60.94
 ---- batch: 030 ----
mean loss: 60.63
 ---- batch: 040 ----
mean loss: 57.94
train mean loss: 59.80
epoch train time: 0:00:00.650190
elapsed time: 0:01:30.610539
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 21:41:57.066947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.86
 ---- batch: 020 ----
mean loss: 57.58
 ---- batch: 030 ----
mean loss: 61.75
 ---- batch: 040 ----
mean loss: 59.94
train mean loss: 59.96
epoch train time: 0:00:00.631122
elapsed time: 0:01:31.241957
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 21:41:57.698367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.03
 ---- batch: 020 ----
mean loss: 59.32
 ---- batch: 030 ----
mean loss: 59.02
 ---- batch: 040 ----
mean loss: 57.28
train mean loss: 59.89
epoch train time: 0:00:00.688431
elapsed time: 0:01:31.930623
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 21:41:58.387036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.80
 ---- batch: 020 ----
mean loss: 59.53
 ---- batch: 030 ----
mean loss: 58.99
 ---- batch: 040 ----
mean loss: 60.84
train mean loss: 59.03
epoch train time: 0:00:00.695937
elapsed time: 0:01:32.626809
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 21:41:59.083226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.53
 ---- batch: 020 ----
mean loss: 59.35
 ---- batch: 030 ----
mean loss: 57.63
 ---- batch: 040 ----
mean loss: 56.68
train mean loss: 58.44
epoch train time: 0:00:00.669600
elapsed time: 0:01:33.296640
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 21:41:59.753062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.56
 ---- batch: 020 ----
mean loss: 57.39
 ---- batch: 030 ----
mean loss: 58.79
 ---- batch: 040 ----
mean loss: 58.13
train mean loss: 58.48
epoch train time: 0:00:00.646768
elapsed time: 0:01:33.943619
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 21:42:00.400022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.74
 ---- batch: 020 ----
mean loss: 57.77
 ---- batch: 030 ----
mean loss: 60.45
 ---- batch: 040 ----
mean loss: 56.11
train mean loss: 57.99
epoch train time: 0:00:00.657847
elapsed time: 0:01:34.601696
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 21:42:01.058102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.78
 ---- batch: 020 ----
mean loss: 58.04
 ---- batch: 030 ----
mean loss: 59.59
 ---- batch: 040 ----
mean loss: 58.37
train mean loss: 57.79
epoch train time: 0:00:00.641081
elapsed time: 0:01:35.243028
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 21:42:01.699440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.56
 ---- batch: 020 ----
mean loss: 59.33
 ---- batch: 030 ----
mean loss: 54.70
 ---- batch: 040 ----
mean loss: 59.27
train mean loss: 57.61
epoch train time: 0:00:00.679885
elapsed time: 0:01:35.923135
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 21:42:02.379562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.36
 ---- batch: 020 ----
mean loss: 59.22
 ---- batch: 030 ----
mean loss: 57.34
 ---- batch: 040 ----
mean loss: 56.41
train mean loss: 57.76
epoch train time: 0:00:00.690069
elapsed time: 0:01:36.613458
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 21:42:03.069878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.97
 ---- batch: 020 ----
mean loss: 57.40
 ---- batch: 030 ----
mean loss: 58.51
 ---- batch: 040 ----
mean loss: 55.60
train mean loss: 57.18
epoch train time: 0:00:00.684281
elapsed time: 0:01:37.297965
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 21:42:03.754376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.82
 ---- batch: 020 ----
mean loss: 54.89
 ---- batch: 030 ----
mean loss: 53.74
 ---- batch: 040 ----
mean loss: 55.69
train mean loss: 55.96
epoch train time: 0:00:00.628357
elapsed time: 0:01:37.926533
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 21:42:04.382962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.82
 ---- batch: 020 ----
mean loss: 55.96
 ---- batch: 030 ----
mean loss: 59.12
 ---- batch: 040 ----
mean loss: 56.01
train mean loss: 56.21
epoch train time: 0:00:00.627598
elapsed time: 0:01:38.554351
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 21:42:05.010754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.98
 ---- batch: 020 ----
mean loss: 55.88
 ---- batch: 030 ----
mean loss: 56.18
 ---- batch: 040 ----
mean loss: 55.05
train mean loss: 55.90
epoch train time: 0:00:00.640349
elapsed time: 0:01:39.194987
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 21:42:05.651378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.88
 ---- batch: 020 ----
mean loss: 53.71
 ---- batch: 030 ----
mean loss: 56.78
 ---- batch: 040 ----
mean loss: 56.55
train mean loss: 56.29
epoch train time: 0:00:00.706172
elapsed time: 0:01:39.901361
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 21:42:06.357769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.45
 ---- batch: 020 ----
mean loss: 56.48
 ---- batch: 030 ----
mean loss: 56.29
 ---- batch: 040 ----
mean loss: 56.01
train mean loss: 55.99
epoch train time: 0:00:00.690799
elapsed time: 0:01:40.592381
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 21:42:07.048815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.01
 ---- batch: 020 ----
mean loss: 54.58
 ---- batch: 030 ----
mean loss: 56.84
 ---- batch: 040 ----
mean loss: 53.29
train mean loss: 55.76
epoch train time: 0:00:00.682135
elapsed time: 0:01:41.274787
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 21:42:07.731231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.73
 ---- batch: 020 ----
mean loss: 56.46
 ---- batch: 030 ----
mean loss: 53.49
 ---- batch: 040 ----
mean loss: 56.74
train mean loss: 55.52
epoch train time: 0:00:00.644423
elapsed time: 0:01:41.919452
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 21:42:08.375899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.34
 ---- batch: 020 ----
mean loss: 53.71
 ---- batch: 030 ----
mean loss: 53.01
 ---- batch: 040 ----
mean loss: 55.46
train mean loss: 54.91
epoch train time: 0:00:00.670987
elapsed time: 0:01:42.590676
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 21:42:09.047088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.83
 ---- batch: 020 ----
mean loss: 56.35
 ---- batch: 030 ----
mean loss: 53.49
 ---- batch: 040 ----
mean loss: 52.43
train mean loss: 55.04
epoch train time: 0:00:00.668992
elapsed time: 0:01:43.259897
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 21:42:09.716306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.97
 ---- batch: 020 ----
mean loss: 53.66
 ---- batch: 030 ----
mean loss: 55.99
 ---- batch: 040 ----
mean loss: 53.89
train mean loss: 53.76
epoch train time: 0:00:00.689820
elapsed time: 0:01:43.949929
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 21:42:10.406342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.54
 ---- batch: 020 ----
mean loss: 55.53
 ---- batch: 030 ----
mean loss: 54.84
 ---- batch: 040 ----
mean loss: 53.42
train mean loss: 54.24
epoch train time: 0:00:00.680548
elapsed time: 0:01:44.630715
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 21:42:11.087155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.93
 ---- batch: 020 ----
mean loss: 52.88
 ---- batch: 030 ----
mean loss: 51.63
 ---- batch: 040 ----
mean loss: 53.84
train mean loss: 52.92
epoch train time: 0:00:00.715026
elapsed time: 0:01:45.346008
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 21:42:11.802426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.28
 ---- batch: 020 ----
mean loss: 54.32
 ---- batch: 030 ----
mean loss: 53.95
 ---- batch: 040 ----
mean loss: 52.42
train mean loss: 53.74
epoch train time: 0:00:00.688604
elapsed time: 0:01:46.034838
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 21:42:12.491248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.63
 ---- batch: 020 ----
mean loss: 52.80
 ---- batch: 030 ----
mean loss: 53.35
 ---- batch: 040 ----
mean loss: 53.61
train mean loss: 53.45
epoch train time: 0:00:00.671120
elapsed time: 0:01:46.706225
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 21:42:13.162654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.37
 ---- batch: 020 ----
mean loss: 58.96
 ---- batch: 030 ----
mean loss: 52.80
 ---- batch: 040 ----
mean loss: 51.80
train mean loss: 53.23
epoch train time: 0:00:00.692877
elapsed time: 0:01:47.399426
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 21:42:13.855851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.77
 ---- batch: 020 ----
mean loss: 51.48
 ---- batch: 030 ----
mean loss: 53.79
 ---- batch: 040 ----
mean loss: 53.25
train mean loss: 53.02
epoch train time: 0:00:00.697177
elapsed time: 0:01:48.096859
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 21:42:14.553271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.23
 ---- batch: 020 ----
mean loss: 48.80
 ---- batch: 030 ----
mean loss: 53.31
 ---- batch: 040 ----
mean loss: 54.27
train mean loss: 52.59
epoch train time: 0:00:00.680401
elapsed time: 0:01:48.777474
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 21:42:15.233877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.04
 ---- batch: 020 ----
mean loss: 53.38
 ---- batch: 030 ----
mean loss: 52.36
 ---- batch: 040 ----
mean loss: 51.89
train mean loss: 51.88
epoch train time: 0:00:00.658567
elapsed time: 0:01:49.436255
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 21:42:15.892677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.22
 ---- batch: 020 ----
mean loss: 56.32
 ---- batch: 030 ----
mean loss: 53.15
 ---- batch: 040 ----
mean loss: 49.96
train mean loss: 52.06
epoch train time: 0:00:00.659191
elapsed time: 0:01:50.095704
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 21:42:16.552145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.64
 ---- batch: 020 ----
mean loss: 51.67
 ---- batch: 030 ----
mean loss: 51.86
 ---- batch: 040 ----
mean loss: 53.05
train mean loss: 51.15
epoch train time: 0:00:00.683494
elapsed time: 0:01:50.779462
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 21:42:17.235921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.81
 ---- batch: 020 ----
mean loss: 53.08
 ---- batch: 030 ----
mean loss: 52.09
 ---- batch: 040 ----
mean loss: 50.57
train mean loss: 51.72
epoch train time: 0:00:00.682581
elapsed time: 0:01:51.462347
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 21:42:17.918777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.87
 ---- batch: 020 ----
mean loss: 48.13
 ---- batch: 030 ----
mean loss: 50.42
 ---- batch: 040 ----
mean loss: 52.41
train mean loss: 51.10
epoch train time: 0:00:00.683024
elapsed time: 0:01:52.145608
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 21:42:18.602082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.22
 ---- batch: 020 ----
mean loss: 51.94
 ---- batch: 030 ----
mean loss: 50.58
 ---- batch: 040 ----
mean loss: 52.42
train mean loss: 51.25
epoch train time: 0:00:00.684248
elapsed time: 0:01:52.830118
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 21:42:19.286536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.21
 ---- batch: 020 ----
mean loss: 49.79
 ---- batch: 030 ----
mean loss: 51.47
 ---- batch: 040 ----
mean loss: 50.76
train mean loss: 51.20
epoch train time: 0:00:00.658504
elapsed time: 0:01:53.488887
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 21:42:19.945305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.33
 ---- batch: 020 ----
mean loss: 54.00
 ---- batch: 030 ----
mean loss: 49.30
 ---- batch: 040 ----
mean loss: 48.78
train mean loss: 50.14
epoch train time: 0:00:00.652919
elapsed time: 0:01:54.142080
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 21:42:20.598454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.91
 ---- batch: 020 ----
mean loss: 49.76
 ---- batch: 030 ----
mean loss: 49.41
 ---- batch: 040 ----
mean loss: 49.41
train mean loss: 49.61
epoch train time: 0:00:00.685491
elapsed time: 0:01:54.827807
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 21:42:21.284225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.35
 ---- batch: 020 ----
mean loss: 49.47
 ---- batch: 030 ----
mean loss: 51.31
 ---- batch: 040 ----
mean loss: 50.98
train mean loss: 50.33
epoch train time: 0:00:00.673148
elapsed time: 0:01:55.501226
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 21:42:21.957666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.78
 ---- batch: 020 ----
mean loss: 48.86
 ---- batch: 030 ----
mean loss: 49.44
 ---- batch: 040 ----
mean loss: 51.22
train mean loss: 49.52
epoch train time: 0:00:00.644457
elapsed time: 0:01:56.145952
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 21:42:22.602415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.78
 ---- batch: 020 ----
mean loss: 51.29
 ---- batch: 030 ----
mean loss: 48.63
 ---- batch: 040 ----
mean loss: 49.38
train mean loss: 49.73
epoch train time: 0:00:00.651890
elapsed time: 0:01:56.798108
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 21:42:23.254510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.41
 ---- batch: 020 ----
mean loss: 50.58
 ---- batch: 030 ----
mean loss: 46.75
 ---- batch: 040 ----
mean loss: 47.53
train mean loss: 48.69
epoch train time: 0:00:00.640416
elapsed time: 0:01:57.438719
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 21:42:23.895139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.80
 ---- batch: 020 ----
mean loss: 47.64
 ---- batch: 030 ----
mean loss: 48.33
 ---- batch: 040 ----
mean loss: 49.60
train mean loss: 48.48
epoch train time: 0:00:00.676765
elapsed time: 0:01:58.115732
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 21:42:24.572171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.04
 ---- batch: 020 ----
mean loss: 49.13
 ---- batch: 030 ----
mean loss: 49.98
 ---- batch: 040 ----
mean loss: 49.28
train mean loss: 48.53
epoch train time: 0:00:00.701348
elapsed time: 0:01:58.817338
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 21:42:25.273751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.58
 ---- batch: 020 ----
mean loss: 47.53
 ---- batch: 030 ----
mean loss: 48.37
 ---- batch: 040 ----
mean loss: 47.03
train mean loss: 47.52
epoch train time: 0:00:00.683686
elapsed time: 0:01:59.501245
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 21:42:25.957670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.83
 ---- batch: 020 ----
mean loss: 48.68
 ---- batch: 030 ----
mean loss: 49.01
 ---- batch: 040 ----
mean loss: 48.73
train mean loss: 47.87
epoch train time: 0:00:00.652170
elapsed time: 0:02:00.153636
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 21:42:26.610069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.10
 ---- batch: 020 ----
mean loss: 48.01
 ---- batch: 030 ----
mean loss: 47.56
 ---- batch: 040 ----
mean loss: 46.27
train mean loss: 47.40
epoch train time: 0:00:00.653825
elapsed time: 0:02:00.807686
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 21:42:27.264106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.01
 ---- batch: 020 ----
mean loss: 48.00
 ---- batch: 030 ----
mean loss: 45.98
 ---- batch: 040 ----
mean loss: 49.36
train mean loss: 47.30
epoch train time: 0:00:00.653519
elapsed time: 0:02:01.461438
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 21:42:27.917871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.22
 ---- batch: 020 ----
mean loss: 47.20
 ---- batch: 030 ----
mean loss: 46.00
 ---- batch: 040 ----
mean loss: 47.33
train mean loss: 46.65
epoch train time: 0:00:00.680027
elapsed time: 0:02:02.141777
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 21:42:28.598192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.01
 ---- batch: 020 ----
mean loss: 44.88
 ---- batch: 030 ----
mean loss: 47.54
 ---- batch: 040 ----
mean loss: 49.06
train mean loss: 46.79
epoch train time: 0:00:00.697238
elapsed time: 0:02:02.839265
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 21:42:29.295680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.83
 ---- batch: 020 ----
mean loss: 47.18
 ---- batch: 030 ----
mean loss: 45.05
 ---- batch: 040 ----
mean loss: 48.18
train mean loss: 46.18
epoch train time: 0:00:00.695941
elapsed time: 0:02:03.535453
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 21:42:29.991884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.05
 ---- batch: 020 ----
mean loss: 45.04
 ---- batch: 030 ----
mean loss: 46.34
 ---- batch: 040 ----
mean loss: 47.77
train mean loss: 45.91
epoch train time: 0:00:00.657441
elapsed time: 0:02:04.193129
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 21:42:30.649556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.94
 ---- batch: 020 ----
mean loss: 46.86
 ---- batch: 030 ----
mean loss: 47.36
 ---- batch: 040 ----
mean loss: 46.22
train mean loss: 46.27
epoch train time: 0:00:00.642426
elapsed time: 0:02:04.835802
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 21:42:31.292213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.39
 ---- batch: 020 ----
mean loss: 45.96
 ---- batch: 030 ----
mean loss: 45.20
 ---- batch: 040 ----
mean loss: 46.50
train mean loss: 46.17
epoch train time: 0:00:00.693264
elapsed time: 0:02:05.529309
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 21:42:31.985736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.56
 ---- batch: 020 ----
mean loss: 44.66
 ---- batch: 030 ----
mean loss: 46.38
 ---- batch: 040 ----
mean loss: 42.47
train mean loss: 45.04
epoch train time: 0:00:00.690392
elapsed time: 0:02:06.219973
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 21:42:32.676415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.40
 ---- batch: 020 ----
mean loss: 43.74
 ---- batch: 030 ----
mean loss: 44.39
 ---- batch: 040 ----
mean loss: 44.96
train mean loss: 44.96
epoch train time: 0:00:00.692861
elapsed time: 0:02:06.913121
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 21:42:33.369529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.48
 ---- batch: 020 ----
mean loss: 44.97
 ---- batch: 030 ----
mean loss: 46.19
 ---- batch: 040 ----
mean loss: 42.56
train mean loss: 44.50
epoch train time: 0:00:00.674493
elapsed time: 0:02:07.587852
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 21:42:34.044257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.72
 ---- batch: 020 ----
mean loss: 41.50
 ---- batch: 030 ----
mean loss: 46.04
 ---- batch: 040 ----
mean loss: 46.57
train mean loss: 44.34
epoch train time: 0:00:00.661781
elapsed time: 0:02:08.249841
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 21:42:34.706264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.85
 ---- batch: 020 ----
mean loss: 44.37
 ---- batch: 030 ----
mean loss: 43.56
 ---- batch: 040 ----
mean loss: 44.03
train mean loss: 44.53
epoch train time: 0:00:00.659637
elapsed time: 0:02:08.909698
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 21:42:35.366122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.65
 ---- batch: 020 ----
mean loss: 41.65
 ---- batch: 030 ----
mean loss: 46.02
 ---- batch: 040 ----
mean loss: 43.48
train mean loss: 43.89
epoch train time: 0:00:00.680391
elapsed time: 0:02:09.590376
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 21:42:36.046789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.13
 ---- batch: 020 ----
mean loss: 45.28
 ---- batch: 030 ----
mean loss: 45.01
 ---- batch: 040 ----
mean loss: 45.79
train mean loss: 44.48
epoch train time: 0:00:00.676522
elapsed time: 0:02:10.267228
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 21:42:36.723670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.65
 ---- batch: 020 ----
mean loss: 45.71
 ---- batch: 030 ----
mean loss: 43.73
 ---- batch: 040 ----
mean loss: 42.76
train mean loss: 43.94
epoch train time: 0:00:00.677956
elapsed time: 0:02:10.945501
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 21:42:37.401877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.31
 ---- batch: 020 ----
mean loss: 43.22
 ---- batch: 030 ----
mean loss: 42.95
 ---- batch: 040 ----
mean loss: 41.86
train mean loss: 42.69
epoch train time: 0:00:00.667315
elapsed time: 0:02:11.613011
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 21:42:38.069425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.15
 ---- batch: 020 ----
mean loss: 42.62
 ---- batch: 030 ----
mean loss: 42.85
 ---- batch: 040 ----
mean loss: 41.85
train mean loss: 42.65
epoch train time: 0:00:00.653112
elapsed time: 0:02:12.266333
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 21:42:38.722740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.05
 ---- batch: 020 ----
mean loss: 42.40
 ---- batch: 030 ----
mean loss: 41.97
 ---- batch: 040 ----
mean loss: 44.42
train mean loss: 42.24
epoch train time: 0:00:00.654189
elapsed time: 0:02:12.920736
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 21:42:39.377147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.01
 ---- batch: 020 ----
mean loss: 41.42
 ---- batch: 030 ----
mean loss: 41.69
 ---- batch: 040 ----
mean loss: 43.18
train mean loss: 42.11
epoch train time: 0:00:00.688859
elapsed time: 0:02:13.609850
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 21:42:40.066269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.19
 ---- batch: 020 ----
mean loss: 41.29
 ---- batch: 030 ----
mean loss: 40.47
 ---- batch: 040 ----
mean loss: 42.62
train mean loss: 41.97
epoch train time: 0:00:00.679556
elapsed time: 0:02:14.289634
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 21:42:40.746039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.85
 ---- batch: 020 ----
mean loss: 44.17
 ---- batch: 030 ----
mean loss: 42.38
 ---- batch: 040 ----
mean loss: 40.09
train mean loss: 42.06
epoch train time: 0:00:00.655306
elapsed time: 0:02:14.945149
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 21:42:41.401575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.37
 ---- batch: 020 ----
mean loss: 42.94
 ---- batch: 030 ----
mean loss: 40.01
 ---- batch: 040 ----
mean loss: 40.76
train mean loss: 41.12
epoch train time: 0:00:00.684687
elapsed time: 0:02:15.630122
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 21:42:42.086576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.65
 ---- batch: 020 ----
mean loss: 40.73
 ---- batch: 030 ----
mean loss: 39.75
 ---- batch: 040 ----
mean loss: 43.18
train mean loss: 41.27
epoch train time: 0:00:00.658581
elapsed time: 0:02:16.288971
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 21:42:42.745378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.58
 ---- batch: 020 ----
mean loss: 40.57
 ---- batch: 030 ----
mean loss: 38.71
 ---- batch: 040 ----
mean loss: 42.20
train mean loss: 40.73
epoch train time: 0:00:00.659858
elapsed time: 0:02:16.949048
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 21:42:43.405492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.97
 ---- batch: 020 ----
mean loss: 40.86
 ---- batch: 030 ----
mean loss: 41.22
 ---- batch: 040 ----
mean loss: 40.53
train mean loss: 40.59
epoch train time: 0:00:00.701645
elapsed time: 0:02:17.650970
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 21:42:44.107382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.31
 ---- batch: 020 ----
mean loss: 41.27
 ---- batch: 030 ----
mean loss: 38.10
 ---- batch: 040 ----
mean loss: 41.48
train mean loss: 39.87
epoch train time: 0:00:00.692620
elapsed time: 0:02:18.343851
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 21:42:44.800268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.81
 ---- batch: 020 ----
mean loss: 39.23
 ---- batch: 030 ----
mean loss: 41.57
 ---- batch: 040 ----
mean loss: 39.96
train mean loss: 40.06
epoch train time: 0:00:00.670291
elapsed time: 0:02:19.014397
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 21:42:45.470806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.66
 ---- batch: 020 ----
mean loss: 42.10
 ---- batch: 030 ----
mean loss: 41.60
 ---- batch: 040 ----
mean loss: 35.93
train mean loss: 40.43
epoch train time: 0:00:00.684064
elapsed time: 0:02:19.698696
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 21:42:46.155099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.54
 ---- batch: 020 ----
mean loss: 40.12
 ---- batch: 030 ----
mean loss: 38.42
 ---- batch: 040 ----
mean loss: 41.46
train mean loss: 40.05
epoch train time: 0:00:00.648612
elapsed time: 0:02:20.347537
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 21:42:46.803961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.87
 ---- batch: 020 ----
mean loss: 38.36
 ---- batch: 030 ----
mean loss: 39.37
 ---- batch: 040 ----
mean loss: 40.02
train mean loss: 38.67
epoch train time: 0:00:00.667794
elapsed time: 0:02:21.015613
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 21:42:47.472036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.69
 ---- batch: 020 ----
mean loss: 38.30
 ---- batch: 030 ----
mean loss: 41.49
 ---- batch: 040 ----
mean loss: 40.73
train mean loss: 39.26
epoch train time: 0:00:00.686866
elapsed time: 0:02:21.702731
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 21:42:48.159158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.47
 ---- batch: 020 ----
mean loss: 38.71
 ---- batch: 030 ----
mean loss: 36.51
 ---- batch: 040 ----
mean loss: 39.42
train mean loss: 38.27
epoch train time: 0:00:00.684066
elapsed time: 0:02:22.387045
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 21:42:48.843461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.39
 ---- batch: 020 ----
mean loss: 37.20
 ---- batch: 030 ----
mean loss: 39.76
 ---- batch: 040 ----
mean loss: 37.22
train mean loss: 37.87
epoch train time: 0:00:00.651508
elapsed time: 0:02:23.038762
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 21:42:49.495167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.42
 ---- batch: 020 ----
mean loss: 37.22
 ---- batch: 030 ----
mean loss: 39.32
 ---- batch: 040 ----
mean loss: 37.23
train mean loss: 38.18
epoch train time: 0:00:00.646821
elapsed time: 0:02:23.685876
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 21:42:50.142300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.89
 ---- batch: 020 ----
mean loss: 36.28
 ---- batch: 030 ----
mean loss: 39.33
 ---- batch: 040 ----
mean loss: 37.93
train mean loss: 38.06
epoch train time: 0:00:00.656193
elapsed time: 0:02:24.342372
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 21:42:50.798826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.76
 ---- batch: 020 ----
mean loss: 38.15
 ---- batch: 030 ----
mean loss: 37.93
 ---- batch: 040 ----
mean loss: 36.06
train mean loss: 37.54
epoch train time: 0:00:00.699841
elapsed time: 0:02:25.042491
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 21:42:51.498918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.17
 ---- batch: 020 ----
mean loss: 37.72
 ---- batch: 030 ----
mean loss: 38.46
 ---- batch: 040 ----
mean loss: 37.43
train mean loss: 37.39
epoch train time: 0:00:00.691759
elapsed time: 0:02:25.734517
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 21:42:52.190946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.62
 ---- batch: 020 ----
mean loss: 37.51
 ---- batch: 030 ----
mean loss: 36.48
 ---- batch: 040 ----
mean loss: 37.34
train mean loss: 36.58
epoch train time: 0:00:00.674182
elapsed time: 0:02:26.408939
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 21:42:52.865352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.27
 ---- batch: 020 ----
mean loss: 37.32
 ---- batch: 030 ----
mean loss: 36.77
 ---- batch: 040 ----
mean loss: 36.26
train mean loss: 36.86
epoch train time: 0:00:00.657797
elapsed time: 0:02:27.066956
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 21:42:53.523368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.30
 ---- batch: 020 ----
mean loss: 36.14
 ---- batch: 030 ----
mean loss: 37.39
 ---- batch: 040 ----
mean loss: 36.01
train mean loss: 36.33
epoch train time: 0:00:00.666465
elapsed time: 0:02:27.733641
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 21:42:54.190085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.24
 ---- batch: 020 ----
mean loss: 34.84
 ---- batch: 030 ----
mean loss: 37.08
 ---- batch: 040 ----
mean loss: 36.97
train mean loss: 35.85
epoch train time: 0:00:00.703186
elapsed time: 0:02:28.437150
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 21:42:54.893561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.15
 ---- batch: 020 ----
mean loss: 34.95
 ---- batch: 030 ----
mean loss: 36.50
 ---- batch: 040 ----
mean loss: 36.64
train mean loss: 35.79
epoch train time: 0:00:00.669146
elapsed time: 0:02:29.106564
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 21:42:55.562977
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.04
 ---- batch: 020 ----
mean loss: 34.42
 ---- batch: 030 ----
mean loss: 34.71
 ---- batch: 040 ----
mean loss: 36.11
train mean loss: 34.56
epoch train time: 0:00:00.680425
elapsed time: 0:02:29.787301
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 21:42:56.243678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.94
 ---- batch: 020 ----
mean loss: 35.90
 ---- batch: 030 ----
mean loss: 33.72
 ---- batch: 040 ----
mean loss: 34.58
train mean loss: 34.59
epoch train time: 0:00:00.679611
elapsed time: 0:02:30.467170
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 21:42:56.923601
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.39
 ---- batch: 020 ----
mean loss: 34.68
 ---- batch: 030 ----
mean loss: 33.96
 ---- batch: 040 ----
mean loss: 33.88
train mean loss: 34.50
epoch train time: 0:00:00.636620
elapsed time: 0:02:31.104030
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 21:42:57.560471
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.69
 ---- batch: 020 ----
mean loss: 34.21
 ---- batch: 030 ----
mean loss: 33.95
 ---- batch: 040 ----
mean loss: 35.41
train mean loss: 34.22
epoch train time: 0:00:00.681373
elapsed time: 0:02:31.785645
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 21:42:58.242107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.46
 ---- batch: 020 ----
mean loss: 32.61
 ---- batch: 030 ----
mean loss: 34.66
 ---- batch: 040 ----
mean loss: 35.25
train mean loss: 34.56
epoch train time: 0:00:00.685057
elapsed time: 0:02:32.470998
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 21:42:58.927419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.89
 ---- batch: 020 ----
mean loss: 33.79
 ---- batch: 030 ----
mean loss: 35.62
 ---- batch: 040 ----
mean loss: 32.34
train mean loss: 34.37
epoch train time: 0:00:00.704846
elapsed time: 0:02:33.176110
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 21:42:59.632563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.42
 ---- batch: 020 ----
mean loss: 34.02
 ---- batch: 030 ----
mean loss: 34.32
 ---- batch: 040 ----
mean loss: 33.81
train mean loss: 34.56
epoch train time: 0:00:00.695480
elapsed time: 0:02:33.871877
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 21:43:00.328291
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.92
 ---- batch: 020 ----
mean loss: 34.07
 ---- batch: 030 ----
mean loss: 32.98
 ---- batch: 040 ----
mean loss: 32.71
train mean loss: 34.08
epoch train time: 0:00:00.670630
elapsed time: 0:02:34.542757
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 21:43:00.999166
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.32
 ---- batch: 020 ----
mean loss: 35.56
 ---- batch: 030 ----
mean loss: 31.96
 ---- batch: 040 ----
mean loss: 35.73
train mean loss: 34.17
epoch train time: 0:00:00.667917
elapsed time: 0:02:35.210886
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 21:43:01.667297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.80
 ---- batch: 020 ----
mean loss: 35.18
 ---- batch: 030 ----
mean loss: 34.59
 ---- batch: 040 ----
mean loss: 31.95
train mean loss: 34.20
epoch train time: 0:00:00.675767
elapsed time: 0:02:35.886894
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 21:43:02.343302
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.42
 ---- batch: 020 ----
mean loss: 35.25
 ---- batch: 030 ----
mean loss: 33.00
 ---- batch: 040 ----
mean loss: 33.23
train mean loss: 34.25
epoch train time: 0:00:00.699608
elapsed time: 0:02:36.586734
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 21:43:03.043145
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.26
 ---- batch: 020 ----
mean loss: 33.57
 ---- batch: 030 ----
mean loss: 33.48
 ---- batch: 040 ----
mean loss: 34.26
train mean loss: 34.40
epoch train time: 0:00:00.676301
elapsed time: 0:02:37.263313
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 21:43:03.719729
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.84
 ---- batch: 020 ----
mean loss: 32.50
 ---- batch: 030 ----
mean loss: 33.13
 ---- batch: 040 ----
mean loss: 35.08
train mean loss: 34.07
epoch train time: 0:00:00.692080
elapsed time: 0:02:37.955703
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 21:43:04.412132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.60
 ---- batch: 020 ----
mean loss: 34.37
 ---- batch: 030 ----
mean loss: 34.52
 ---- batch: 040 ----
mean loss: 34.29
train mean loss: 34.34
epoch train time: 0:00:00.665127
elapsed time: 0:02:38.621103
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 21:43:05.077531
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.88
 ---- batch: 020 ----
mean loss: 32.60
 ---- batch: 030 ----
mean loss: 34.48
 ---- batch: 040 ----
mean loss: 34.35
train mean loss: 34.03
epoch train time: 0:00:00.635831
elapsed time: 0:02:39.257181
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 21:43:05.713609
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.16
 ---- batch: 020 ----
mean loss: 33.86
 ---- batch: 030 ----
mean loss: 32.74
 ---- batch: 040 ----
mean loss: 32.27
train mean loss: 33.72
epoch train time: 0:00:00.633705
elapsed time: 0:02:39.891126
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 21:43:06.347531
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.45
 ---- batch: 020 ----
mean loss: 33.85
 ---- batch: 030 ----
mean loss: 34.03
 ---- batch: 040 ----
mean loss: 33.59
train mean loss: 34.04
epoch train time: 0:00:00.688589
elapsed time: 0:02:40.579978
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 21:43:07.036422
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.87
 ---- batch: 020 ----
mean loss: 35.74
 ---- batch: 030 ----
mean loss: 33.73
 ---- batch: 040 ----
mean loss: 34.27
train mean loss: 34.05
epoch train time: 0:00:00.681678
elapsed time: 0:02:41.261926
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 21:43:07.718409
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.64
 ---- batch: 020 ----
mean loss: 35.41
 ---- batch: 030 ----
mean loss: 33.64
 ---- batch: 040 ----
mean loss: 32.69
train mean loss: 34.09
epoch train time: 0:00:00.690203
elapsed time: 0:02:41.952437
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 21:43:08.408890
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.88
 ---- batch: 020 ----
mean loss: 35.28
 ---- batch: 030 ----
mean loss: 34.53
 ---- batch: 040 ----
mean loss: 33.75
train mean loss: 34.04
epoch train time: 0:00:00.657949
elapsed time: 0:02:42.610693
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 21:43:09.067100
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.96
 ---- batch: 020 ----
mean loss: 33.28
 ---- batch: 030 ----
mean loss: 34.05
 ---- batch: 040 ----
mean loss: 35.09
train mean loss: 33.97
epoch train time: 0:00:00.625348
elapsed time: 0:02:43.236248
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 21:43:09.692652
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.08
 ---- batch: 020 ----
mean loss: 34.83
 ---- batch: 030 ----
mean loss: 33.10
 ---- batch: 040 ----
mean loss: 35.06
train mean loss: 33.92
epoch train time: 0:00:00.681643
elapsed time: 0:02:43.918092
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 21:43:10.374517
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.31
 ---- batch: 020 ----
mean loss: 33.35
 ---- batch: 030 ----
mean loss: 33.73
 ---- batch: 040 ----
mean loss: 34.62
train mean loss: 34.02
epoch train time: 0:00:00.692407
elapsed time: 0:02:44.610783
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 21:43:11.067195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.95
 ---- batch: 020 ----
mean loss: 34.36
 ---- batch: 030 ----
mean loss: 32.78
 ---- batch: 040 ----
mean loss: 34.30
train mean loss: 33.96
epoch train time: 0:00:00.687588
elapsed time: 0:02:45.298634
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 21:43:11.755099
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.70
 ---- batch: 020 ----
mean loss: 33.55
 ---- batch: 030 ----
mean loss: 34.47
 ---- batch: 040 ----
mean loss: 33.97
train mean loss: 33.80
epoch train time: 0:00:00.669200
elapsed time: 0:02:45.968157
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 21:43:12.424574
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.60
 ---- batch: 020 ----
mean loss: 34.43
 ---- batch: 030 ----
mean loss: 34.23
 ---- batch: 040 ----
mean loss: 34.31
train mean loss: 33.98
epoch train time: 0:00:00.649418
elapsed time: 0:02:46.617784
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 21:43:13.074189
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.32
 ---- batch: 020 ----
mean loss: 34.15
 ---- batch: 030 ----
mean loss: 34.23
 ---- batch: 040 ----
mean loss: 32.68
train mean loss: 33.53
epoch train time: 0:00:00.639763
elapsed time: 0:02:47.257748
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 21:43:13.714154
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.52
 ---- batch: 020 ----
mean loss: 31.85
 ---- batch: 030 ----
mean loss: 35.98
 ---- batch: 040 ----
mean loss: 34.43
train mean loss: 33.85
epoch train time: 0:00:00.659897
elapsed time: 0:02:47.917873
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 21:43:14.374295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.95
 ---- batch: 020 ----
mean loss: 34.12
 ---- batch: 030 ----
mean loss: 33.92
 ---- batch: 040 ----
mean loss: 34.57
train mean loss: 33.70
epoch train time: 0:00:00.679966
elapsed time: 0:02:48.598099
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 21:43:15.054514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.87
 ---- batch: 020 ----
mean loss: 35.00
 ---- batch: 030 ----
mean loss: 34.57
 ---- batch: 040 ----
mean loss: 32.17
train mean loss: 33.87
epoch train time: 0:00:00.693579
elapsed time: 0:02:49.291954
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 21:43:15.748425
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.57
 ---- batch: 020 ----
mean loss: 31.06
 ---- batch: 030 ----
mean loss: 34.28
 ---- batch: 040 ----
mean loss: 35.32
train mean loss: 33.76
epoch train time: 0:00:00.681996
elapsed time: 0:02:49.974252
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 21:43:16.430661
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.51
 ---- batch: 020 ----
mean loss: 34.45
 ---- batch: 030 ----
mean loss: 33.32
 ---- batch: 040 ----
mean loss: 32.38
train mean loss: 33.60
epoch train time: 0:00:00.667649
elapsed time: 0:02:50.642103
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 21:43:17.098508
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.32
 ---- batch: 020 ----
mean loss: 35.18
 ---- batch: 030 ----
mean loss: 32.21
 ---- batch: 040 ----
mean loss: 32.93
train mean loss: 33.60
epoch train time: 0:00:00.653402
elapsed time: 0:02:51.295746
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 21:43:17.752121
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.23
 ---- batch: 020 ----
mean loss: 34.29
 ---- batch: 030 ----
mean loss: 34.38
 ---- batch: 040 ----
mean loss: 32.26
train mean loss: 33.56
epoch train time: 0:00:00.669543
elapsed time: 0:02:51.965474
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 21:43:18.421880
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.73
 ---- batch: 020 ----
mean loss: 32.97
 ---- batch: 030 ----
mean loss: 32.40
 ---- batch: 040 ----
mean loss: 34.40
train mean loss: 33.60
epoch train time: 0:00:00.686294
elapsed time: 0:02:52.652008
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 21:43:19.108434
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.58
 ---- batch: 020 ----
mean loss: 33.62
 ---- batch: 030 ----
mean loss: 33.25
 ---- batch: 040 ----
mean loss: 32.67
train mean loss: 33.30
epoch train time: 0:00:00.673787
elapsed time: 0:02:53.326079
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 21:43:19.782507
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.40
 ---- batch: 020 ----
mean loss: 31.53
 ---- batch: 030 ----
mean loss: 34.07
 ---- batch: 040 ----
mean loss: 33.18
train mean loss: 33.39
epoch train time: 0:00:00.671510
elapsed time: 0:02:53.997842
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 21:43:20.454267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.60
 ---- batch: 020 ----
mean loss: 34.19
 ---- batch: 030 ----
mean loss: 32.35
 ---- batch: 040 ----
mean loss: 33.94
train mean loss: 33.25
epoch train time: 0:00:00.672471
elapsed time: 0:02:54.670534
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 21:43:21.126935
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.14
 ---- batch: 020 ----
mean loss: 34.60
 ---- batch: 030 ----
mean loss: 34.59
 ---- batch: 040 ----
mean loss: 34.12
train mean loss: 33.42
epoch train time: 0:00:00.651930
elapsed time: 0:02:55.322695
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 21:43:21.779100
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.94
 ---- batch: 020 ----
mean loss: 34.59
 ---- batch: 030 ----
mean loss: 31.76
 ---- batch: 040 ----
mean loss: 33.42
train mean loss: 33.31
epoch train time: 0:00:00.651656
elapsed time: 0:02:55.974607
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 21:43:22.431027
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.73
 ---- batch: 020 ----
mean loss: 32.45
 ---- batch: 030 ----
mean loss: 32.79
 ---- batch: 040 ----
mean loss: 33.79
train mean loss: 33.15
epoch train time: 0:00:00.677128
elapsed time: 0:02:56.651978
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 21:43:23.108406
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.22
 ---- batch: 020 ----
mean loss: 33.36
 ---- batch: 030 ----
mean loss: 33.20
 ---- batch: 040 ----
mean loss: 32.01
train mean loss: 33.19
epoch train time: 0:00:00.690000
elapsed time: 0:02:57.342228
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 21:43:23.798671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.76
 ---- batch: 020 ----
mean loss: 31.78
 ---- batch: 030 ----
mean loss: 34.34
 ---- batch: 040 ----
mean loss: 33.31
train mean loss: 33.35
epoch train time: 0:00:00.690785
elapsed time: 0:02:58.033277
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 21:43:24.489700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.51
 ---- batch: 020 ----
mean loss: 33.41
 ---- batch: 030 ----
mean loss: 33.52
 ---- batch: 040 ----
mean loss: 32.62
train mean loss: 32.96
epoch train time: 0:00:00.680207
elapsed time: 0:02:58.713750
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 21:43:25.170165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.16
 ---- batch: 020 ----
mean loss: 33.60
 ---- batch: 030 ----
mean loss: 33.63
 ---- batch: 040 ----
mean loss: 34.24
train mean loss: 33.17
epoch train time: 0:00:00.659464
elapsed time: 0:02:59.373423
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 21:43:25.829827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.33
 ---- batch: 020 ----
mean loss: 31.82
 ---- batch: 030 ----
mean loss: 33.91
 ---- batch: 040 ----
mean loss: 35.19
train mean loss: 33.20
epoch train time: 0:00:00.646303
elapsed time: 0:03:00.019920
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 21:43:26.476324
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.29
 ---- batch: 020 ----
mean loss: 33.87
 ---- batch: 030 ----
mean loss: 33.25
 ---- batch: 040 ----
mean loss: 33.94
train mean loss: 33.33
epoch train time: 0:00:00.653394
elapsed time: 0:03:00.673536
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 21:43:27.129945
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.17
 ---- batch: 020 ----
mean loss: 33.73
 ---- batch: 030 ----
mean loss: 33.15
 ---- batch: 040 ----
mean loss: 31.40
train mean loss: 33.16
epoch train time: 0:00:00.690226
elapsed time: 0:03:01.364033
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 21:43:27.820460
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.67
 ---- batch: 020 ----
mean loss: 33.61
 ---- batch: 030 ----
mean loss: 32.45
 ---- batch: 040 ----
mean loss: 31.74
train mean loss: 32.73
epoch train time: 0:00:00.682114
elapsed time: 0:03:02.053838
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_2/checkpoint.pth.tar
**** end time: 2019-09-20 21:43:28.510194 ****
