Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_4', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 16042
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-27 00:26:49.286006 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 16, 24]             100
              Tanh-2           [-1, 10, 16, 24]               0
            Conv2d-3           [-1, 10, 15, 24]           1,000
              Tanh-4           [-1, 10, 15, 24]               0
            Conv2d-5           [-1, 10, 16, 24]           1,000
              Tanh-6           [-1, 10, 16, 24]               0
            Conv2d-7           [-1, 10, 15, 24]           1,000
              Tanh-8           [-1, 10, 15, 24]               0
            Conv2d-9            [-1, 1, 15, 24]              30
             Tanh-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
          Dropout-12                  [-1, 360]               0
           Linear-13                  [-1, 100]          36,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 39,230
Trainable params: 39,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 00:26:49.294464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4701.47
 ---- batch: 020 ----
mean loss: 2956.43
 ---- batch: 030 ----
mean loss: 1524.22
 ---- batch: 040 ----
mean loss: 1355.68
 ---- batch: 050 ----
mean loss: 1201.92
 ---- batch: 060 ----
mean loss: 1111.32
 ---- batch: 070 ----
mean loss: 1077.82
 ---- batch: 080 ----
mean loss: 1041.99
 ---- batch: 090 ----
mean loss: 996.57
 ---- batch: 100 ----
mean loss: 985.05
 ---- batch: 110 ----
mean loss: 944.92
train mean loss: 1608.91
epoch train time: 0:00:33.932996
elapsed time: 0:00:33.944251
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 00:27:23.230302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.84
 ---- batch: 020 ----
mean loss: 899.17
 ---- batch: 030 ----
mean loss: 855.54
 ---- batch: 040 ----
mean loss: 851.75
 ---- batch: 050 ----
mean loss: 804.27
 ---- batch: 060 ----
mean loss: 810.69
 ---- batch: 070 ----
mean loss: 813.01
 ---- batch: 080 ----
mean loss: 792.16
 ---- batch: 090 ----
mean loss: 797.98
 ---- batch: 100 ----
mean loss: 786.37
 ---- batch: 110 ----
mean loss: 800.87
train mean loss: 827.77
epoch train time: 0:00:02.227881
elapsed time: 0:00:36.172288
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 00:27:25.458367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.08
 ---- batch: 020 ----
mean loss: 749.53
 ---- batch: 030 ----
mean loss: 726.02
 ---- batch: 040 ----
mean loss: 732.25
 ---- batch: 050 ----
mean loss: 727.44
 ---- batch: 060 ----
mean loss: 711.25
 ---- batch: 070 ----
mean loss: 737.90
 ---- batch: 080 ----
mean loss: 696.63
 ---- batch: 090 ----
mean loss: 704.48
 ---- batch: 100 ----
mean loss: 695.15
 ---- batch: 110 ----
mean loss: 694.66
train mean loss: 717.83
epoch train time: 0:00:02.162853
elapsed time: 0:00:38.335336
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 00:27:27.621399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.34
 ---- batch: 020 ----
mean loss: 662.52
 ---- batch: 030 ----
mean loss: 684.16
 ---- batch: 040 ----
mean loss: 656.55
 ---- batch: 050 ----
mean loss: 641.90
 ---- batch: 060 ----
mean loss: 643.10
 ---- batch: 070 ----
mean loss: 662.01
 ---- batch: 080 ----
mean loss: 645.40
 ---- batch: 090 ----
mean loss: 645.12
 ---- batch: 100 ----
mean loss: 655.42
 ---- batch: 110 ----
mean loss: 640.85
train mean loss: 656.82
epoch train time: 0:00:02.166796
elapsed time: 0:00:40.502321
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 00:27:29.788383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 619.34
 ---- batch: 020 ----
mean loss: 640.70
 ---- batch: 030 ----
mean loss: 625.01
 ---- batch: 040 ----
mean loss: 640.36
 ---- batch: 050 ----
mean loss: 625.14
 ---- batch: 060 ----
mean loss: 617.76
 ---- batch: 070 ----
mean loss: 629.39
 ---- batch: 080 ----
mean loss: 631.36
 ---- batch: 090 ----
mean loss: 622.14
 ---- batch: 100 ----
mean loss: 615.00
 ---- batch: 110 ----
mean loss: 626.27
train mean loss: 625.95
epoch train time: 0:00:02.176875
elapsed time: 0:00:42.679368
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 00:27:31.965430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.54
 ---- batch: 020 ----
mean loss: 605.93
 ---- batch: 030 ----
mean loss: 603.98
 ---- batch: 040 ----
mean loss: 605.45
 ---- batch: 050 ----
mean loss: 604.21
 ---- batch: 060 ----
mean loss: 619.76
 ---- batch: 070 ----
mean loss: 599.21
 ---- batch: 080 ----
mean loss: 614.69
 ---- batch: 090 ----
mean loss: 594.68
 ---- batch: 100 ----
mean loss: 589.36
 ---- batch: 110 ----
mean loss: 609.38
train mean loss: 604.98
epoch train time: 0:00:02.178981
elapsed time: 0:00:44.858528
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 00:27:34.144594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 594.06
 ---- batch: 020 ----
mean loss: 598.09
 ---- batch: 030 ----
mean loss: 580.96
 ---- batch: 040 ----
mean loss: 587.81
 ---- batch: 050 ----
mean loss: 579.20
 ---- batch: 060 ----
mean loss: 588.65
 ---- batch: 070 ----
mean loss: 582.33
 ---- batch: 080 ----
mean loss: 600.51
 ---- batch: 090 ----
mean loss: 580.71
 ---- batch: 100 ----
mean loss: 594.79
 ---- batch: 110 ----
mean loss: 579.22
train mean loss: 588.56
epoch train time: 0:00:02.177115
elapsed time: 0:00:47.035827
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 00:27:36.321886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.46
 ---- batch: 020 ----
mean loss: 567.51
 ---- batch: 030 ----
mean loss: 614.16
 ---- batch: 040 ----
mean loss: 594.80
 ---- batch: 050 ----
mean loss: 591.20
 ---- batch: 060 ----
mean loss: 594.94
 ---- batch: 070 ----
mean loss: 604.52
 ---- batch: 080 ----
mean loss: 584.03
 ---- batch: 090 ----
mean loss: 584.00
 ---- batch: 100 ----
mean loss: 577.30
 ---- batch: 110 ----
mean loss: 592.60
train mean loss: 590.24
epoch train time: 0:00:02.173793
elapsed time: 0:00:49.209804
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 00:27:38.495868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.43
 ---- batch: 020 ----
mean loss: 589.02
 ---- batch: 030 ----
mean loss: 565.77
 ---- batch: 040 ----
mean loss: 568.66
 ---- batch: 050 ----
mean loss: 566.08
 ---- batch: 060 ----
mean loss: 583.30
 ---- batch: 070 ----
mean loss: 567.90
 ---- batch: 080 ----
mean loss: 570.80
 ---- batch: 090 ----
mean loss: 577.74
 ---- batch: 100 ----
mean loss: 582.08
 ---- batch: 110 ----
mean loss: 583.17
train mean loss: 576.74
epoch train time: 0:00:02.184844
elapsed time: 0:00:51.394858
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 00:27:40.680957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.57
 ---- batch: 020 ----
mean loss: 581.09
 ---- batch: 030 ----
mean loss: 569.08
 ---- batch: 040 ----
mean loss: 572.12
 ---- batch: 050 ----
mean loss: 592.64
 ---- batch: 060 ----
mean loss: 563.72
 ---- batch: 070 ----
mean loss: 572.14
 ---- batch: 080 ----
mean loss: 577.22
 ---- batch: 090 ----
mean loss: 566.07
 ---- batch: 100 ----
mean loss: 565.21
 ---- batch: 110 ----
mean loss: 575.58
train mean loss: 573.71
epoch train time: 0:00:02.171668
elapsed time: 0:00:53.566762
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 00:27:42.852865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.62
 ---- batch: 020 ----
mean loss: 543.63
 ---- batch: 030 ----
mean loss: 574.24
 ---- batch: 040 ----
mean loss: 571.43
 ---- batch: 050 ----
mean loss: 579.68
 ---- batch: 060 ----
mean loss: 577.34
 ---- batch: 070 ----
mean loss: 584.40
 ---- batch: 080 ----
mean loss: 559.87
 ---- batch: 090 ----
mean loss: 582.77
 ---- batch: 100 ----
mean loss: 556.82
 ---- batch: 110 ----
mean loss: 568.71
train mean loss: 569.67
epoch train time: 0:00:02.184468
elapsed time: 0:00:55.751441
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 00:27:45.037502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 580.52
 ---- batch: 020 ----
mean loss: 564.38
 ---- batch: 030 ----
mean loss: 568.06
 ---- batch: 040 ----
mean loss: 578.60
 ---- batch: 050 ----
mean loss: 565.75
 ---- batch: 060 ----
mean loss: 537.64
 ---- batch: 070 ----
mean loss: 554.03
 ---- batch: 080 ----
mean loss: 563.62
 ---- batch: 090 ----
mean loss: 551.66
 ---- batch: 100 ----
mean loss: 551.54
 ---- batch: 110 ----
mean loss: 557.58
train mean loss: 561.00
epoch train time: 0:00:02.182342
elapsed time: 0:00:57.933949
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 00:27:47.220009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 582.86
 ---- batch: 020 ----
mean loss: 565.57
 ---- batch: 030 ----
mean loss: 543.39
 ---- batch: 040 ----
mean loss: 557.63
 ---- batch: 050 ----
mean loss: 558.31
 ---- batch: 060 ----
mean loss: 574.94
 ---- batch: 070 ----
mean loss: 556.01
 ---- batch: 080 ----
mean loss: 563.85
 ---- batch: 090 ----
mean loss: 563.42
 ---- batch: 100 ----
mean loss: 580.36
 ---- batch: 110 ----
mean loss: 565.41
train mean loss: 564.80
epoch train time: 0:00:02.180432
elapsed time: 0:01:00.114546
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 00:27:49.400609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.81
 ---- batch: 020 ----
mean loss: 565.00
 ---- batch: 030 ----
mean loss: 571.07
 ---- batch: 040 ----
mean loss: 555.49
 ---- batch: 050 ----
mean loss: 564.98
 ---- batch: 060 ----
mean loss: 553.53
 ---- batch: 070 ----
mean loss: 566.47
 ---- batch: 080 ----
mean loss: 568.18
 ---- batch: 090 ----
mean loss: 585.66
 ---- batch: 100 ----
mean loss: 585.62
 ---- batch: 110 ----
mean loss: 560.57
train mean loss: 567.38
epoch train time: 0:00:02.168716
elapsed time: 0:01:02.283430
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 00:27:51.569491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 550.13
 ---- batch: 020 ----
mean loss: 575.02
 ---- batch: 030 ----
mean loss: 570.26
 ---- batch: 040 ----
mean loss: 584.30
 ---- batch: 050 ----
mean loss: 564.59
 ---- batch: 060 ----
mean loss: 556.65
 ---- batch: 070 ----
mean loss: 560.08
 ---- batch: 080 ----
mean loss: 570.61
 ---- batch: 090 ----
mean loss: 569.25
 ---- batch: 100 ----
mean loss: 556.59
 ---- batch: 110 ----
mean loss: 586.92
train mean loss: 567.51
epoch train time: 0:00:02.169452
elapsed time: 0:01:04.453070
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 00:27:53.739153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.00
 ---- batch: 020 ----
mean loss: 576.68
 ---- batch: 030 ----
mean loss: 563.11
 ---- batch: 040 ----
mean loss: 566.82
 ---- batch: 050 ----
mean loss: 559.25
 ---- batch: 060 ----
mean loss: 569.92
 ---- batch: 070 ----
mean loss: 560.94
 ---- batch: 080 ----
mean loss: 558.55
 ---- batch: 090 ----
mean loss: 561.63
 ---- batch: 100 ----
mean loss: 564.78
 ---- batch: 110 ----
mean loss: 549.06
train mean loss: 565.35
epoch train time: 0:00:02.175575
elapsed time: 0:01:06.628843
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 00:27:55.914905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 546.00
 ---- batch: 020 ----
mean loss: 538.92
 ---- batch: 030 ----
mean loss: 559.78
 ---- batch: 040 ----
mean loss: 577.31
 ---- batch: 050 ----
mean loss: 563.30
 ---- batch: 060 ----
mean loss: 576.39
 ---- batch: 070 ----
mean loss: 573.96
 ---- batch: 080 ----
mean loss: 579.46
 ---- batch: 090 ----
mean loss: 548.55
 ---- batch: 100 ----
mean loss: 562.62
 ---- batch: 110 ----
mean loss: 561.15
train mean loss: 562.60
epoch train time: 0:00:02.162751
elapsed time: 0:01:08.791788
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 00:27:58.077855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 552.17
 ---- batch: 020 ----
mean loss: 565.08
 ---- batch: 030 ----
mean loss: 548.20
 ---- batch: 040 ----
mean loss: 575.20
 ---- batch: 050 ----
mean loss: 556.41
 ---- batch: 060 ----
mean loss: 538.44
 ---- batch: 070 ----
mean loss: 564.93
 ---- batch: 080 ----
mean loss: 561.11
 ---- batch: 090 ----
mean loss: 534.48
 ---- batch: 100 ----
mean loss: 572.03
 ---- batch: 110 ----
mean loss: 568.10
train mean loss: 557.05
epoch train time: 0:00:02.172226
elapsed time: 0:01:10.964212
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 00:28:00.250292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 547.53
 ---- batch: 020 ----
mean loss: 564.01
 ---- batch: 030 ----
mean loss: 561.04
 ---- batch: 040 ----
mean loss: 542.91
 ---- batch: 050 ----
mean loss: 552.26
 ---- batch: 060 ----
mean loss: 554.72
 ---- batch: 070 ----
mean loss: 573.43
 ---- batch: 080 ----
mean loss: 574.45
 ---- batch: 090 ----
mean loss: 552.84
 ---- batch: 100 ----
mean loss: 569.14
 ---- batch: 110 ----
mean loss: 560.00
train mean loss: 559.01
epoch train time: 0:00:02.170223
elapsed time: 0:01:13.134633
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 00:28:02.420703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.78
 ---- batch: 020 ----
mean loss: 561.12
 ---- batch: 030 ----
mean loss: 564.35
 ---- batch: 040 ----
mean loss: 553.93
 ---- batch: 050 ----
mean loss: 539.21
 ---- batch: 060 ----
mean loss: 578.50
 ---- batch: 070 ----
mean loss: 561.17
 ---- batch: 080 ----
mean loss: 559.21
 ---- batch: 090 ----
mean loss: 561.78
 ---- batch: 100 ----
mean loss: 552.48
 ---- batch: 110 ----
mean loss: 547.98
train mean loss: 559.31
epoch train time: 0:00:02.171685
elapsed time: 0:01:15.306493
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 00:28:04.592558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 548.47
 ---- batch: 020 ----
mean loss: 555.48
 ---- batch: 030 ----
mean loss: 557.38
 ---- batch: 040 ----
mean loss: 549.55
 ---- batch: 050 ----
mean loss: 569.37
 ---- batch: 060 ----
mean loss: 565.12
 ---- batch: 070 ----
mean loss: 563.18
 ---- batch: 080 ----
mean loss: 561.39
 ---- batch: 090 ----
mean loss: 557.19
 ---- batch: 100 ----
mean loss: 550.35
 ---- batch: 110 ----
mean loss: 536.28
train mean loss: 555.09
epoch train time: 0:00:02.169992
elapsed time: 0:01:17.476701
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 00:28:06.762766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 543.10
 ---- batch: 020 ----
mean loss: 570.12
 ---- batch: 030 ----
mean loss: 554.01
 ---- batch: 040 ----
mean loss: 553.73
 ---- batch: 050 ----
mean loss: 554.48
 ---- batch: 060 ----
mean loss: 558.82
 ---- batch: 070 ----
mean loss: 569.87
 ---- batch: 080 ----
mean loss: 538.62
 ---- batch: 090 ----
mean loss: 541.78
 ---- batch: 100 ----
mean loss: 559.27
 ---- batch: 110 ----
mean loss: 546.98
train mean loss: 554.05
epoch train time: 0:00:02.170914
elapsed time: 0:01:19.647791
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 00:28:08.933873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 550.57
 ---- batch: 020 ----
mean loss: 559.51
 ---- batch: 030 ----
mean loss: 554.15
 ---- batch: 040 ----
mean loss: 550.66
 ---- batch: 050 ----
mean loss: 571.29
 ---- batch: 060 ----
mean loss: 572.57
 ---- batch: 070 ----
mean loss: 567.83
 ---- batch: 080 ----
mean loss: 557.04
 ---- batch: 090 ----
mean loss: 538.93
 ---- batch: 100 ----
mean loss: 549.37
 ---- batch: 110 ----
mean loss: 549.70
train mean loss: 556.71
epoch train time: 0:00:02.167792
elapsed time: 0:01:21.815766
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 00:28:11.101827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 553.12
 ---- batch: 020 ----
mean loss: 546.01
 ---- batch: 030 ----
mean loss: 569.47
 ---- batch: 040 ----
mean loss: 556.30
 ---- batch: 050 ----
mean loss: 560.17
 ---- batch: 060 ----
mean loss: 560.34
 ---- batch: 070 ----
mean loss: 550.80
 ---- batch: 080 ----
mean loss: 556.01
 ---- batch: 090 ----
mean loss: 558.82
 ---- batch: 100 ----
mean loss: 574.57
 ---- batch: 110 ----
mean loss: 570.31
train mean loss: 559.51
epoch train time: 0:00:02.171985
elapsed time: 0:01:23.987915
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 00:28:13.273975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 555.60
 ---- batch: 020 ----
mean loss: 549.07
 ---- batch: 030 ----
mean loss: 547.78
 ---- batch: 040 ----
mean loss: 542.65
 ---- batch: 050 ----
mean loss: 563.18
 ---- batch: 060 ----
mean loss: 554.86
 ---- batch: 070 ----
mean loss: 561.49
 ---- batch: 080 ----
mean loss: 539.99
 ---- batch: 090 ----
mean loss: 554.54
 ---- batch: 100 ----
mean loss: 537.81
 ---- batch: 110 ----
mean loss: 550.82
train mean loss: 550.59
epoch train time: 0:00:02.176309
elapsed time: 0:01:26.164404
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 00:28:15.450468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 553.79
 ---- batch: 020 ----
mean loss: 565.56
 ---- batch: 030 ----
mean loss: 546.90
 ---- batch: 040 ----
mean loss: 552.67
 ---- batch: 050 ----
mean loss: 551.14
 ---- batch: 060 ----
mean loss: 557.51
 ---- batch: 070 ----
mean loss: 562.12
 ---- batch: 080 ----
mean loss: 563.41
 ---- batch: 090 ----
mean loss: 549.06
 ---- batch: 100 ----
mean loss: 560.81
 ---- batch: 110 ----
mean loss: 547.44
train mean loss: 555.49
epoch train time: 0:00:02.175926
elapsed time: 0:01:28.340514
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 00:28:17.626575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 555.82
 ---- batch: 020 ----
mean loss: 568.77
 ---- batch: 030 ----
mean loss: 566.78
 ---- batch: 040 ----
mean loss: 560.81
 ---- batch: 050 ----
mean loss: 546.19
 ---- batch: 060 ----
mean loss: 529.03
 ---- batch: 070 ----
mean loss: 539.87
 ---- batch: 080 ----
mean loss: 549.44
 ---- batch: 090 ----
mean loss: 554.98
 ---- batch: 100 ----
mean loss: 555.48
 ---- batch: 110 ----
mean loss: 540.61
train mean loss: 551.62
epoch train time: 0:00:02.173426
elapsed time: 0:01:30.514118
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 00:28:19.800190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 562.18
 ---- batch: 020 ----
mean loss: 532.54
 ---- batch: 030 ----
mean loss: 564.92
 ---- batch: 040 ----
mean loss: 552.78
 ---- batch: 050 ----
mean loss: 548.92
 ---- batch: 060 ----
mean loss: 528.69
 ---- batch: 070 ----
mean loss: 527.62
 ---- batch: 080 ----
mean loss: 552.23
 ---- batch: 090 ----
mean loss: 538.44
 ---- batch: 100 ----
mean loss: 539.20
 ---- batch: 110 ----
mean loss: 535.77
train mean loss: 543.27
epoch train time: 0:00:02.171990
elapsed time: 0:01:32.686309
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 00:28:21.972396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.42
 ---- batch: 020 ----
mean loss: 562.84
 ---- batch: 030 ----
mean loss: 550.63
 ---- batch: 040 ----
mean loss: 548.03
 ---- batch: 050 ----
mean loss: 542.47
 ---- batch: 060 ----
mean loss: 543.83
 ---- batch: 070 ----
mean loss: 543.39
 ---- batch: 080 ----
mean loss: 539.89
 ---- batch: 090 ----
mean loss: 538.59
 ---- batch: 100 ----
mean loss: 540.20
 ---- batch: 110 ----
mean loss: 543.41
train mean loss: 542.86
epoch train time: 0:00:02.168965
elapsed time: 0:01:34.855475
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 00:28:24.141538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.67
 ---- batch: 020 ----
mean loss: 542.02
 ---- batch: 030 ----
mean loss: 545.28
 ---- batch: 040 ----
mean loss: 540.81
 ---- batch: 050 ----
mean loss: 545.88
 ---- batch: 060 ----
mean loss: 521.07
 ---- batch: 070 ----
mean loss: 551.44
 ---- batch: 080 ----
mean loss: 529.15
 ---- batch: 090 ----
mean loss: 526.11
 ---- batch: 100 ----
mean loss: 546.31
 ---- batch: 110 ----
mean loss: 538.03
train mean loss: 536.54
epoch train time: 0:00:02.169819
elapsed time: 0:01:37.025462
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 00:28:26.311539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.58
 ---- batch: 020 ----
mean loss: 528.28
 ---- batch: 030 ----
mean loss: 527.86
 ---- batch: 040 ----
mean loss: 546.14
 ---- batch: 050 ----
mean loss: 530.19
 ---- batch: 060 ----
mean loss: 542.48
 ---- batch: 070 ----
mean loss: 521.66
 ---- batch: 080 ----
mean loss: 550.18
 ---- batch: 090 ----
mean loss: 522.75
 ---- batch: 100 ----
mean loss: 544.18
 ---- batch: 110 ----
mean loss: 529.23
train mean loss: 533.42
epoch train time: 0:00:02.170324
elapsed time: 0:01:39.195989
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 00:28:28.482052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 541.23
 ---- batch: 020 ----
mean loss: 567.31
 ---- batch: 030 ----
mean loss: 546.32
 ---- batch: 040 ----
mean loss: 556.06
 ---- batch: 050 ----
mean loss: 528.95
 ---- batch: 060 ----
mean loss: 536.54
 ---- batch: 070 ----
mean loss: 518.20
 ---- batch: 080 ----
mean loss: 533.59
 ---- batch: 090 ----
mean loss: 533.59
 ---- batch: 100 ----
mean loss: 524.47
 ---- batch: 110 ----
mean loss: 524.18
train mean loss: 537.61
epoch train time: 0:00:02.171537
elapsed time: 0:01:41.367717
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 00:28:30.653781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 523.49
 ---- batch: 020 ----
mean loss: 519.02
 ---- batch: 030 ----
mean loss: 514.13
 ---- batch: 040 ----
mean loss: 516.17
 ---- batch: 050 ----
mean loss: 524.94
 ---- batch: 060 ----
mean loss: 545.81
 ---- batch: 070 ----
mean loss: 527.48
 ---- batch: 080 ----
mean loss: 544.04
 ---- batch: 090 ----
mean loss: 538.44
 ---- batch: 100 ----
mean loss: 518.97
 ---- batch: 110 ----
mean loss: 545.17
train mean loss: 529.57
epoch train time: 0:00:02.168267
elapsed time: 0:01:43.536204
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 00:28:32.822276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 539.16
 ---- batch: 020 ----
mean loss: 525.70
 ---- batch: 030 ----
mean loss: 540.18
 ---- batch: 040 ----
mean loss: 521.19
 ---- batch: 050 ----
mean loss: 505.47
 ---- batch: 060 ----
mean loss: 502.30
 ---- batch: 070 ----
mean loss: 534.93
 ---- batch: 080 ----
mean loss: 515.21
 ---- batch: 090 ----
mean loss: 535.87
 ---- batch: 100 ----
mean loss: 525.89
 ---- batch: 110 ----
mean loss: 533.37
train mean loss: 525.84
epoch train time: 0:00:02.184931
elapsed time: 0:01:45.721333
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 00:28:35.007409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.27
 ---- batch: 020 ----
mean loss: 518.05
 ---- batch: 030 ----
mean loss: 517.03
 ---- batch: 040 ----
mean loss: 553.93
 ---- batch: 050 ----
mean loss: 536.11
 ---- batch: 060 ----
mean loss: 536.82
 ---- batch: 070 ----
mean loss: 523.91
 ---- batch: 080 ----
mean loss: 541.05
 ---- batch: 090 ----
mean loss: 519.74
 ---- batch: 100 ----
mean loss: 526.42
 ---- batch: 110 ----
mean loss: 530.49
train mean loss: 528.70
epoch train time: 0:00:02.181030
elapsed time: 0:01:47.902538
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 00:28:37.188634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 536.95
 ---- batch: 020 ----
mean loss: 510.85
 ---- batch: 030 ----
mean loss: 513.77
 ---- batch: 040 ----
mean loss: 510.89
 ---- batch: 050 ----
mean loss: 522.69
 ---- batch: 060 ----
mean loss: 514.76
 ---- batch: 070 ----
mean loss: 504.60
 ---- batch: 080 ----
mean loss: 530.61
 ---- batch: 090 ----
mean loss: 509.03
 ---- batch: 100 ----
mean loss: 541.02
 ---- batch: 110 ----
mean loss: 515.50
train mean loss: 518.49
epoch train time: 0:00:02.187948
elapsed time: 0:01:50.090695
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 00:28:39.376759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 535.35
 ---- batch: 020 ----
mean loss: 524.09
 ---- batch: 030 ----
mean loss: 535.01
 ---- batch: 040 ----
mean loss: 519.10
 ---- batch: 050 ----
mean loss: 514.09
 ---- batch: 060 ----
mean loss: 521.50
 ---- batch: 070 ----
mean loss: 516.86
 ---- batch: 080 ----
mean loss: 507.97
 ---- batch: 090 ----
mean loss: 520.08
 ---- batch: 100 ----
mean loss: 516.91
 ---- batch: 110 ----
mean loss: 520.17
train mean loss: 521.61
epoch train time: 0:00:02.180210
elapsed time: 0:01:52.271076
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 00:28:41.557138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.62
 ---- batch: 020 ----
mean loss: 514.29
 ---- batch: 030 ----
mean loss: 523.45
 ---- batch: 040 ----
mean loss: 533.03
 ---- batch: 050 ----
mean loss: 515.53
 ---- batch: 060 ----
mean loss: 524.27
 ---- batch: 070 ----
mean loss: 523.07
 ---- batch: 080 ----
mean loss: 522.84
 ---- batch: 090 ----
mean loss: 505.32
 ---- batch: 100 ----
mean loss: 516.80
 ---- batch: 110 ----
mean loss: 525.63
train mean loss: 519.86
epoch train time: 0:00:02.180988
elapsed time: 0:01:54.452268
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 00:28:43.738329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.15
 ---- batch: 020 ----
mean loss: 534.28
 ---- batch: 030 ----
mean loss: 523.47
 ---- batch: 040 ----
mean loss: 514.55
 ---- batch: 050 ----
mean loss: 503.37
 ---- batch: 060 ----
mean loss: 534.41
 ---- batch: 070 ----
mean loss: 518.46
 ---- batch: 080 ----
mean loss: 530.15
 ---- batch: 090 ----
mean loss: 512.60
 ---- batch: 100 ----
mean loss: 515.94
 ---- batch: 110 ----
mean loss: 511.38
train mean loss: 519.83
epoch train time: 0:00:02.177685
elapsed time: 0:01:56.630125
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 00:28:45.916186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.66
 ---- batch: 020 ----
mean loss: 528.26
 ---- batch: 030 ----
mean loss: 508.13
 ---- batch: 040 ----
mean loss: 510.00
 ---- batch: 050 ----
mean loss: 505.58
 ---- batch: 060 ----
mean loss: 524.23
 ---- batch: 070 ----
mean loss: 523.20
 ---- batch: 080 ----
mean loss: 518.95
 ---- batch: 090 ----
mean loss: 545.34
 ---- batch: 100 ----
mean loss: 524.94
 ---- batch: 110 ----
mean loss: 521.89
train mean loss: 520.02
epoch train time: 0:00:02.172494
elapsed time: 0:01:58.802790
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 00:28:48.088851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.65
 ---- batch: 020 ----
mean loss: 515.67
 ---- batch: 030 ----
mean loss: 501.86
 ---- batch: 040 ----
mean loss: 524.09
 ---- batch: 050 ----
mean loss: 515.72
 ---- batch: 060 ----
mean loss: 514.46
 ---- batch: 070 ----
mean loss: 514.90
 ---- batch: 080 ----
mean loss: 520.42
 ---- batch: 090 ----
mean loss: 501.49
 ---- batch: 100 ----
mean loss: 514.36
 ---- batch: 110 ----
mean loss: 526.25
train mean loss: 515.66
epoch train time: 0:00:02.174609
elapsed time: 0:02:00.977593
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 00:28:50.263680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.48
 ---- batch: 020 ----
mean loss: 521.55
 ---- batch: 030 ----
mean loss: 517.37
 ---- batch: 040 ----
mean loss: 521.63
 ---- batch: 050 ----
mean loss: 516.15
 ---- batch: 060 ----
mean loss: 507.08
 ---- batch: 070 ----
mean loss: 541.62
 ---- batch: 080 ----
mean loss: 522.14
 ---- batch: 090 ----
mean loss: 526.06
 ---- batch: 100 ----
mean loss: 515.42
 ---- batch: 110 ----
mean loss: 510.22
train mean loss: 520.62
epoch train time: 0:00:02.171989
elapsed time: 0:02:03.149787
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 00:28:52.435851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.32
 ---- batch: 020 ----
mean loss: 513.22
 ---- batch: 030 ----
mean loss: 514.68
 ---- batch: 040 ----
mean loss: 526.80
 ---- batch: 050 ----
mean loss: 513.87
 ---- batch: 060 ----
mean loss: 515.84
 ---- batch: 070 ----
mean loss: 526.89
 ---- batch: 080 ----
mean loss: 516.48
 ---- batch: 090 ----
mean loss: 508.68
 ---- batch: 100 ----
mean loss: 511.35
 ---- batch: 110 ----
mean loss: 519.60
train mean loss: 517.81
epoch train time: 0:00:02.165927
elapsed time: 0:02:05.315873
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 00:28:54.601959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 523.09
 ---- batch: 020 ----
mean loss: 511.04
 ---- batch: 030 ----
mean loss: 537.16
 ---- batch: 040 ----
mean loss: 519.76
 ---- batch: 050 ----
mean loss: 533.55
 ---- batch: 060 ----
mean loss: 513.63
 ---- batch: 070 ----
mean loss: 506.79
 ---- batch: 080 ----
mean loss: 535.64
 ---- batch: 090 ----
mean loss: 509.92
 ---- batch: 100 ----
mean loss: 515.51
 ---- batch: 110 ----
mean loss: 511.00
train mean loss: 519.83
epoch train time: 0:00:02.168416
elapsed time: 0:02:07.484476
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 00:28:56.770539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.01
 ---- batch: 020 ----
mean loss: 514.43
 ---- batch: 030 ----
mean loss: 504.85
 ---- batch: 040 ----
mean loss: 519.24
 ---- batch: 050 ----
mean loss: 529.14
 ---- batch: 060 ----
mean loss: 522.85
 ---- batch: 070 ----
mean loss: 527.18
 ---- batch: 080 ----
mean loss: 521.35
 ---- batch: 090 ----
mean loss: 521.17
 ---- batch: 100 ----
mean loss: 504.33
 ---- batch: 110 ----
mean loss: 501.57
train mean loss: 516.14
epoch train time: 0:00:02.160591
elapsed time: 0:02:09.645238
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 00:28:58.931301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.74
 ---- batch: 020 ----
mean loss: 519.82
 ---- batch: 030 ----
mean loss: 521.10
 ---- batch: 040 ----
mean loss: 515.44
 ---- batch: 050 ----
mean loss: 501.95
 ---- batch: 060 ----
mean loss: 511.41
 ---- batch: 070 ----
mean loss: 500.22
 ---- batch: 080 ----
mean loss: 512.20
 ---- batch: 090 ----
mean loss: 498.86
 ---- batch: 100 ----
mean loss: 515.98
 ---- batch: 110 ----
mean loss: 496.44
train mean loss: 509.42
epoch train time: 0:00:02.143586
elapsed time: 0:02:11.788998
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 00:29:01.075065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.86
 ---- batch: 020 ----
mean loss: 525.68
 ---- batch: 030 ----
mean loss: 516.18
 ---- batch: 040 ----
mean loss: 512.59
 ---- batch: 050 ----
mean loss: 518.50
 ---- batch: 060 ----
mean loss: 505.84
 ---- batch: 070 ----
mean loss: 515.66
 ---- batch: 080 ----
mean loss: 507.38
 ---- batch: 090 ----
mean loss: 511.89
 ---- batch: 100 ----
mean loss: 508.85
 ---- batch: 110 ----
mean loss: 522.86
train mean loss: 514.94
epoch train time: 0:00:02.154359
elapsed time: 0:02:13.943537
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 00:29:03.229597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.59
 ---- batch: 020 ----
mean loss: 522.58
 ---- batch: 030 ----
mean loss: 516.87
 ---- batch: 040 ----
mean loss: 519.83
 ---- batch: 050 ----
mean loss: 523.95
 ---- batch: 060 ----
mean loss: 521.31
 ---- batch: 070 ----
mean loss: 519.91
 ---- batch: 080 ----
mean loss: 504.45
 ---- batch: 090 ----
mean loss: 518.15
 ---- batch: 100 ----
mean loss: 511.50
 ---- batch: 110 ----
mean loss: 507.82
train mean loss: 514.46
epoch train time: 0:00:02.147520
elapsed time: 0:02:16.091208
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 00:29:05.377307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.88
 ---- batch: 020 ----
mean loss: 512.04
 ---- batch: 030 ----
mean loss: 515.37
 ---- batch: 040 ----
mean loss: 513.15
 ---- batch: 050 ----
mean loss: 505.26
 ---- batch: 060 ----
mean loss: 523.48
 ---- batch: 070 ----
mean loss: 520.04
 ---- batch: 080 ----
mean loss: 523.90
 ---- batch: 090 ----
mean loss: 520.89
 ---- batch: 100 ----
mean loss: 509.48
 ---- batch: 110 ----
mean loss: 513.74
train mean loss: 515.89
epoch train time: 0:00:02.143267
elapsed time: 0:02:18.234663
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 00:29:07.520721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 536.36
 ---- batch: 020 ----
mean loss: 513.34
 ---- batch: 030 ----
mean loss: 500.83
 ---- batch: 040 ----
mean loss: 520.06
 ---- batch: 050 ----
mean loss: 513.09
 ---- batch: 060 ----
mean loss: 507.59
 ---- batch: 070 ----
mean loss: 512.59
 ---- batch: 080 ----
mean loss: 525.11
 ---- batch: 090 ----
mean loss: 525.33
 ---- batch: 100 ----
mean loss: 518.33
 ---- batch: 110 ----
mean loss: 507.65
train mean loss: 515.89
epoch train time: 0:00:02.146003
elapsed time: 0:02:20.380837
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 00:29:09.666904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.49
 ---- batch: 020 ----
mean loss: 515.47
 ---- batch: 030 ----
mean loss: 508.32
 ---- batch: 040 ----
mean loss: 500.22
 ---- batch: 050 ----
mean loss: 521.87
 ---- batch: 060 ----
mean loss: 497.36
 ---- batch: 070 ----
mean loss: 528.99
 ---- batch: 080 ----
mean loss: 518.36
 ---- batch: 090 ----
mean loss: 519.73
 ---- batch: 100 ----
mean loss: 498.20
 ---- batch: 110 ----
mean loss: 520.79
train mean loss: 510.74
epoch train time: 0:00:02.148819
elapsed time: 0:02:22.529846
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 00:29:11.815911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.41
 ---- batch: 020 ----
mean loss: 519.90
 ---- batch: 030 ----
mean loss: 522.10
 ---- batch: 040 ----
mean loss: 507.48
 ---- batch: 050 ----
mean loss: 488.87
 ---- batch: 060 ----
mean loss: 523.22
 ---- batch: 070 ----
mean loss: 503.13
 ---- batch: 080 ----
mean loss: 495.70
 ---- batch: 090 ----
mean loss: 519.10
 ---- batch: 100 ----
mean loss: 519.23
 ---- batch: 110 ----
mean loss: 515.12
train mean loss: 510.40
epoch train time: 0:00:02.150643
elapsed time: 0:02:24.680665
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 00:29:13.966742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.26
 ---- batch: 020 ----
mean loss: 507.16
 ---- batch: 030 ----
mean loss: 501.77
 ---- batch: 040 ----
mean loss: 506.83
 ---- batch: 050 ----
mean loss: 519.01
 ---- batch: 060 ----
mean loss: 501.81
 ---- batch: 070 ----
mean loss: 508.88
 ---- batch: 080 ----
mean loss: 519.71
 ---- batch: 090 ----
mean loss: 510.71
 ---- batch: 100 ----
mean loss: 504.31
 ---- batch: 110 ----
mean loss: 514.32
train mean loss: 511.23
epoch train time: 0:00:02.152088
elapsed time: 0:02:26.832975
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 00:29:16.119038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.95
 ---- batch: 020 ----
mean loss: 530.11
 ---- batch: 030 ----
mean loss: 520.05
 ---- batch: 040 ----
mean loss: 517.93
 ---- batch: 050 ----
mean loss: 502.01
 ---- batch: 060 ----
mean loss: 510.08
 ---- batch: 070 ----
mean loss: 508.98
 ---- batch: 080 ----
mean loss: 520.79
 ---- batch: 090 ----
mean loss: 507.31
 ---- batch: 100 ----
mean loss: 503.57
 ---- batch: 110 ----
mean loss: 508.91
train mean loss: 512.84
epoch train time: 0:00:02.151973
elapsed time: 0:02:28.985116
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 00:29:18.271191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.20
 ---- batch: 020 ----
mean loss: 499.45
 ---- batch: 030 ----
mean loss: 527.10
 ---- batch: 040 ----
mean loss: 512.07
 ---- batch: 050 ----
mean loss: 505.72
 ---- batch: 060 ----
mean loss: 489.95
 ---- batch: 070 ----
mean loss: 504.39
 ---- batch: 080 ----
mean loss: 500.01
 ---- batch: 090 ----
mean loss: 511.52
 ---- batch: 100 ----
mean loss: 514.81
 ---- batch: 110 ----
mean loss: 521.96
train mean loss: 509.34
epoch train time: 0:00:02.151024
elapsed time: 0:02:31.136311
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 00:29:20.422393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.01
 ---- batch: 020 ----
mean loss: 513.23
 ---- batch: 030 ----
mean loss: 500.11
 ---- batch: 040 ----
mean loss: 497.94
 ---- batch: 050 ----
mean loss: 508.95
 ---- batch: 060 ----
mean loss: 514.66
 ---- batch: 070 ----
mean loss: 510.47
 ---- batch: 080 ----
mean loss: 509.11
 ---- batch: 090 ----
mean loss: 516.88
 ---- batch: 100 ----
mean loss: 519.61
 ---- batch: 110 ----
mean loss: 503.93
train mean loss: 509.69
epoch train time: 0:00:02.153019
elapsed time: 0:02:33.289523
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 00:29:22.575605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.91
 ---- batch: 020 ----
mean loss: 507.53
 ---- batch: 030 ----
mean loss: 525.12
 ---- batch: 040 ----
mean loss: 502.96
 ---- batch: 050 ----
mean loss: 503.03
 ---- batch: 060 ----
mean loss: 516.69
 ---- batch: 070 ----
mean loss: 509.78
 ---- batch: 080 ----
mean loss: 516.24
 ---- batch: 090 ----
mean loss: 519.47
 ---- batch: 100 ----
mean loss: 501.27
 ---- batch: 110 ----
mean loss: 507.56
train mean loss: 512.91
epoch train time: 0:00:02.149954
elapsed time: 0:02:35.439714
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 00:29:24.725775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.49
 ---- batch: 020 ----
mean loss: 509.58
 ---- batch: 030 ----
mean loss: 506.14
 ---- batch: 040 ----
mean loss: 495.11
 ---- batch: 050 ----
mean loss: 512.75
 ---- batch: 060 ----
mean loss: 523.38
 ---- batch: 070 ----
mean loss: 514.95
 ---- batch: 080 ----
mean loss: 508.36
 ---- batch: 090 ----
mean loss: 517.49
 ---- batch: 100 ----
mean loss: 513.39
 ---- batch: 110 ----
mean loss: 519.62
train mean loss: 512.06
epoch train time: 0:00:02.141876
elapsed time: 0:02:37.581798
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 00:29:26.867887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.84
 ---- batch: 020 ----
mean loss: 505.08
 ---- batch: 030 ----
mean loss: 517.17
 ---- batch: 040 ----
mean loss: 505.91
 ---- batch: 050 ----
mean loss: 502.53
 ---- batch: 060 ----
mean loss: 498.75
 ---- batch: 070 ----
mean loss: 512.71
 ---- batch: 080 ----
mean loss: 512.49
 ---- batch: 090 ----
mean loss: 506.80
 ---- batch: 100 ----
mean loss: 527.98
 ---- batch: 110 ----
mean loss: 519.20
train mean loss: 509.64
epoch train time: 0:00:02.149523
elapsed time: 0:02:39.731511
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 00:29:29.017575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.25
 ---- batch: 020 ----
mean loss: 524.18
 ---- batch: 030 ----
mean loss: 511.88
 ---- batch: 040 ----
mean loss: 505.58
 ---- batch: 050 ----
mean loss: 494.67
 ---- batch: 060 ----
mean loss: 495.98
 ---- batch: 070 ----
mean loss: 501.09
 ---- batch: 080 ----
mean loss: 491.37
 ---- batch: 090 ----
mean loss: 521.00
 ---- batch: 100 ----
mean loss: 508.77
 ---- batch: 110 ----
mean loss: 501.50
train mean loss: 505.73
epoch train time: 0:00:02.144100
elapsed time: 0:02:41.875774
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 00:29:31.161839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.24
 ---- batch: 020 ----
mean loss: 506.92
 ---- batch: 030 ----
mean loss: 531.02
 ---- batch: 040 ----
mean loss: 504.53
 ---- batch: 050 ----
mean loss: 517.81
 ---- batch: 060 ----
mean loss: 506.33
 ---- batch: 070 ----
mean loss: 502.85
 ---- batch: 080 ----
mean loss: 504.03
 ---- batch: 090 ----
mean loss: 504.44
 ---- batch: 100 ----
mean loss: 499.59
 ---- batch: 110 ----
mean loss: 500.46
train mean loss: 508.44
epoch train time: 0:00:02.139548
elapsed time: 0:02:44.015483
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 00:29:33.301566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.67
 ---- batch: 020 ----
mean loss: 499.71
 ---- batch: 030 ----
mean loss: 494.35
 ---- batch: 040 ----
mean loss: 508.72
 ---- batch: 050 ----
mean loss: 506.94
 ---- batch: 060 ----
mean loss: 493.99
 ---- batch: 070 ----
mean loss: 509.64
 ---- batch: 080 ----
mean loss: 518.34
 ---- batch: 090 ----
mean loss: 529.12
 ---- batch: 100 ----
mean loss: 501.41
 ---- batch: 110 ----
mean loss: 511.41
train mean loss: 505.89
epoch train time: 0:00:02.137483
elapsed time: 0:02:46.153141
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 00:29:35.439215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.44
 ---- batch: 020 ----
mean loss: 504.69
 ---- batch: 030 ----
mean loss: 504.83
 ---- batch: 040 ----
mean loss: 505.56
 ---- batch: 050 ----
mean loss: 510.54
 ---- batch: 060 ----
mean loss: 503.27
 ---- batch: 070 ----
mean loss: 493.91
 ---- batch: 080 ----
mean loss: 510.74
 ---- batch: 090 ----
mean loss: 499.95
 ---- batch: 100 ----
mean loss: 498.75
 ---- batch: 110 ----
mean loss: 507.82
train mean loss: 505.39
epoch train time: 0:00:02.145146
elapsed time: 0:02:48.298483
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 00:29:37.584545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.85
 ---- batch: 020 ----
mean loss: 492.71
 ---- batch: 030 ----
mean loss: 499.31
 ---- batch: 040 ----
mean loss: 498.17
 ---- batch: 050 ----
mean loss: 505.62
 ---- batch: 060 ----
mean loss: 507.16
 ---- batch: 070 ----
mean loss: 525.22
 ---- batch: 080 ----
mean loss: 516.96
 ---- batch: 090 ----
mean loss: 490.86
 ---- batch: 100 ----
mean loss: 510.46
 ---- batch: 110 ----
mean loss: 519.35
train mean loss: 507.16
epoch train time: 0:00:02.153884
elapsed time: 0:02:50.452526
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 00:29:39.738589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.14
 ---- batch: 020 ----
mean loss: 503.02
 ---- batch: 030 ----
mean loss: 510.22
 ---- batch: 040 ----
mean loss: 502.91
 ---- batch: 050 ----
mean loss: 503.65
 ---- batch: 060 ----
mean loss: 519.89
 ---- batch: 070 ----
mean loss: 502.86
 ---- batch: 080 ----
mean loss: 494.02
 ---- batch: 090 ----
mean loss: 521.55
 ---- batch: 100 ----
mean loss: 503.29
 ---- batch: 110 ----
mean loss: 495.06
train mean loss: 506.33
epoch train time: 0:00:02.160138
elapsed time: 0:02:52.612850
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 00:29:41.898916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.49
 ---- batch: 020 ----
mean loss: 501.35
 ---- batch: 030 ----
mean loss: 499.70
 ---- batch: 040 ----
mean loss: 504.25
 ---- batch: 050 ----
mean loss: 508.84
 ---- batch: 060 ----
mean loss: 487.87
 ---- batch: 070 ----
mean loss: 505.38
 ---- batch: 080 ----
mean loss: 510.68
 ---- batch: 090 ----
mean loss: 504.63
 ---- batch: 100 ----
mean loss: 503.14
 ---- batch: 110 ----
mean loss: 509.52
train mean loss: 504.49
epoch train time: 0:00:02.161727
elapsed time: 0:02:54.774733
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 00:29:44.060809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.99
 ---- batch: 020 ----
mean loss: 510.03
 ---- batch: 030 ----
mean loss: 527.22
 ---- batch: 040 ----
mean loss: 505.59
 ---- batch: 050 ----
mean loss: 496.24
 ---- batch: 060 ----
mean loss: 512.71
 ---- batch: 070 ----
mean loss: 501.63
 ---- batch: 080 ----
mean loss: 527.04
 ---- batch: 090 ----
mean loss: 522.90
 ---- batch: 100 ----
mean loss: 511.30
 ---- batch: 110 ----
mean loss: 512.22
train mean loss: 512.02
epoch train time: 0:00:02.153939
elapsed time: 0:02:56.928867
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 00:29:46.214931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.76
 ---- batch: 020 ----
mean loss: 503.19
 ---- batch: 030 ----
mean loss: 507.20
 ---- batch: 040 ----
mean loss: 496.90
 ---- batch: 050 ----
mean loss: 535.02
 ---- batch: 060 ----
mean loss: 516.67
 ---- batch: 070 ----
mean loss: 513.56
 ---- batch: 080 ----
mean loss: 502.08
 ---- batch: 090 ----
mean loss: 512.53
 ---- batch: 100 ----
mean loss: 504.40
 ---- batch: 110 ----
mean loss: 521.98
train mean loss: 511.63
epoch train time: 0:00:02.160273
elapsed time: 0:02:59.089351
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 00:29:48.375437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.88
 ---- batch: 020 ----
mean loss: 509.08
 ---- batch: 030 ----
mean loss: 517.89
 ---- batch: 040 ----
mean loss: 501.45
 ---- batch: 050 ----
mean loss: 510.76
 ---- batch: 060 ----
mean loss: 509.37
 ---- batch: 070 ----
mean loss: 522.67
 ---- batch: 080 ----
mean loss: 511.18
 ---- batch: 090 ----
mean loss: 507.27
 ---- batch: 100 ----
mean loss: 497.50
 ---- batch: 110 ----
mean loss: 515.52
train mean loss: 509.47
epoch train time: 0:00:02.157673
elapsed time: 0:03:01.247220
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 00:29:50.533283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.98
 ---- batch: 020 ----
mean loss: 487.42
 ---- batch: 030 ----
mean loss: 520.34
 ---- batch: 040 ----
mean loss: 493.72
 ---- batch: 050 ----
mean loss: 507.90
 ---- batch: 060 ----
mean loss: 490.67
 ---- batch: 070 ----
mean loss: 498.42
 ---- batch: 080 ----
mean loss: 507.30
 ---- batch: 090 ----
mean loss: 499.88
 ---- batch: 100 ----
mean loss: 497.44
 ---- batch: 110 ----
mean loss: 509.72
train mean loss: 501.60
epoch train time: 0:00:02.161670
elapsed time: 0:03:03.409083
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 00:29:52.695165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.90
 ---- batch: 020 ----
mean loss: 502.65
 ---- batch: 030 ----
mean loss: 496.96
 ---- batch: 040 ----
mean loss: 513.59
 ---- batch: 050 ----
mean loss: 489.62
 ---- batch: 060 ----
mean loss: 498.66
 ---- batch: 070 ----
mean loss: 503.99
 ---- batch: 080 ----
mean loss: 497.59
 ---- batch: 090 ----
mean loss: 506.69
 ---- batch: 100 ----
mean loss: 501.52
 ---- batch: 110 ----
mean loss: 500.18
train mean loss: 501.12
epoch train time: 0:00:02.162027
elapsed time: 0:03:05.571303
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 00:29:54.857380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.03
 ---- batch: 020 ----
mean loss: 512.32
 ---- batch: 030 ----
mean loss: 522.89
 ---- batch: 040 ----
mean loss: 481.71
 ---- batch: 050 ----
mean loss: 483.48
 ---- batch: 060 ----
mean loss: 507.98
 ---- batch: 070 ----
mean loss: 506.60
 ---- batch: 080 ----
mean loss: 529.76
 ---- batch: 090 ----
mean loss: 506.29
 ---- batch: 100 ----
mean loss: 508.88
 ---- batch: 110 ----
mean loss: 511.69
train mean loss: 506.57
epoch train time: 0:00:02.156192
elapsed time: 0:03:07.727698
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 00:29:57.013765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.17
 ---- batch: 020 ----
mean loss: 512.56
 ---- batch: 030 ----
mean loss: 492.81
 ---- batch: 040 ----
mean loss: 521.39
 ---- batch: 050 ----
mean loss: 504.31
 ---- batch: 060 ----
mean loss: 501.98
 ---- batch: 070 ----
mean loss: 484.46
 ---- batch: 080 ----
mean loss: 517.02
 ---- batch: 090 ----
mean loss: 512.33
 ---- batch: 100 ----
mean loss: 505.20
 ---- batch: 110 ----
mean loss: 501.64
train mean loss: 503.92
epoch train time: 0:00:02.156101
elapsed time: 0:03:09.883988
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 00:29:59.170050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 510.80
 ---- batch: 020 ----
mean loss: 484.96
 ---- batch: 030 ----
mean loss: 498.01
 ---- batch: 040 ----
mean loss: 492.61
 ---- batch: 050 ----
mean loss: 497.16
 ---- batch: 060 ----
mean loss: 495.17
 ---- batch: 070 ----
mean loss: 493.98
 ---- batch: 080 ----
mean loss: 505.89
 ---- batch: 090 ----
mean loss: 513.94
 ---- batch: 100 ----
mean loss: 490.30
 ---- batch: 110 ----
mean loss: 503.49
train mean loss: 498.63
epoch train time: 0:00:02.153264
elapsed time: 0:03:12.037431
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 00:30:01.323512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.51
 ---- batch: 020 ----
mean loss: 499.03
 ---- batch: 030 ----
mean loss: 491.96
 ---- batch: 040 ----
mean loss: 485.97
 ---- batch: 050 ----
mean loss: 502.66
 ---- batch: 060 ----
mean loss: 506.88
 ---- batch: 070 ----
mean loss: 503.28
 ---- batch: 080 ----
mean loss: 495.70
 ---- batch: 090 ----
mean loss: 507.39
 ---- batch: 100 ----
mean loss: 497.68
 ---- batch: 110 ----
mean loss: 507.93
train mean loss: 498.48
epoch train time: 0:00:02.156837
elapsed time: 0:03:14.194455
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 00:30:03.480517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.57
 ---- batch: 020 ----
mean loss: 497.74
 ---- batch: 030 ----
mean loss: 514.62
 ---- batch: 040 ----
mean loss: 503.13
 ---- batch: 050 ----
mean loss: 501.19
 ---- batch: 060 ----
mean loss: 517.59
 ---- batch: 070 ----
mean loss: 495.18
 ---- batch: 080 ----
mean loss: 492.25
 ---- batch: 090 ----
mean loss: 491.62
 ---- batch: 100 ----
mean loss: 502.38
 ---- batch: 110 ----
mean loss: 492.99
train mean loss: 500.74
epoch train time: 0:00:02.168539
elapsed time: 0:03:16.363160
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 00:30:05.649219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.28
 ---- batch: 020 ----
mean loss: 487.62
 ---- batch: 030 ----
mean loss: 499.29
 ---- batch: 040 ----
mean loss: 505.66
 ---- batch: 050 ----
mean loss: 502.02
 ---- batch: 060 ----
mean loss: 495.33
 ---- batch: 070 ----
mean loss: 502.36
 ---- batch: 080 ----
mean loss: 513.65
 ---- batch: 090 ----
mean loss: 499.80
 ---- batch: 100 ----
mean loss: 507.00
 ---- batch: 110 ----
mean loss: 503.76
train mean loss: 500.74
epoch train time: 0:00:02.171337
elapsed time: 0:03:18.534719
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 00:30:07.820794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.21
 ---- batch: 020 ----
mean loss: 501.52
 ---- batch: 030 ----
mean loss: 495.25
 ---- batch: 040 ----
mean loss: 494.95
 ---- batch: 050 ----
mean loss: 519.34
 ---- batch: 060 ----
mean loss: 514.71
 ---- batch: 070 ----
mean loss: 498.07
 ---- batch: 080 ----
mean loss: 484.94
 ---- batch: 090 ----
mean loss: 503.54
 ---- batch: 100 ----
mean loss: 502.95
 ---- batch: 110 ----
mean loss: 502.92
train mean loss: 500.69
epoch train time: 0:00:02.173893
elapsed time: 0:03:20.708790
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 00:30:09.994848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.62
 ---- batch: 020 ----
mean loss: 506.57
 ---- batch: 030 ----
mean loss: 496.22
 ---- batch: 040 ----
mean loss: 503.81
 ---- batch: 050 ----
mean loss: 507.21
 ---- batch: 060 ----
mean loss: 513.53
 ---- batch: 070 ----
mean loss: 514.40
 ---- batch: 080 ----
mean loss: 507.56
 ---- batch: 090 ----
mean loss: 483.74
 ---- batch: 100 ----
mean loss: 498.21
 ---- batch: 110 ----
mean loss: 493.37
train mean loss: 501.34
epoch train time: 0:00:02.179519
elapsed time: 0:03:22.888486
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 00:30:12.174549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.05
 ---- batch: 020 ----
mean loss: 498.31
 ---- batch: 030 ----
mean loss: 481.17
 ---- batch: 040 ----
mean loss: 507.56
 ---- batch: 050 ----
mean loss: 502.76
 ---- batch: 060 ----
mean loss: 507.71
 ---- batch: 070 ----
mean loss: 484.35
 ---- batch: 080 ----
mean loss: 497.00
 ---- batch: 090 ----
mean loss: 501.58
 ---- batch: 100 ----
mean loss: 486.14
 ---- batch: 110 ----
mean loss: 502.82
train mean loss: 496.62
epoch train time: 0:00:02.167109
elapsed time: 0:03:25.055771
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 00:30:14.341835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.85
 ---- batch: 020 ----
mean loss: 514.10
 ---- batch: 030 ----
mean loss: 494.61
 ---- batch: 040 ----
mean loss: 495.56
 ---- batch: 050 ----
mean loss: 505.65
 ---- batch: 060 ----
mean loss: 488.20
 ---- batch: 070 ----
mean loss: 505.01
 ---- batch: 080 ----
mean loss: 490.34
 ---- batch: 090 ----
mean loss: 491.66
 ---- batch: 100 ----
mean loss: 514.55
 ---- batch: 110 ----
mean loss: 483.44
train mean loss: 497.74
epoch train time: 0:00:02.174508
elapsed time: 0:03:27.230470
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 00:30:16.516559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.29
 ---- batch: 020 ----
mean loss: 499.19
 ---- batch: 030 ----
mean loss: 488.55
 ---- batch: 040 ----
mean loss: 482.54
 ---- batch: 050 ----
mean loss: 485.43
 ---- batch: 060 ----
mean loss: 484.34
 ---- batch: 070 ----
mean loss: 493.94
 ---- batch: 080 ----
mean loss: 510.20
 ---- batch: 090 ----
mean loss: 497.93
 ---- batch: 100 ----
mean loss: 485.56
 ---- batch: 110 ----
mean loss: 501.79
train mean loss: 493.32
epoch train time: 0:00:02.176441
elapsed time: 0:03:29.407118
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 00:30:18.693188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.23
 ---- batch: 020 ----
mean loss: 497.13
 ---- batch: 030 ----
mean loss: 506.02
 ---- batch: 040 ----
mean loss: 492.34
 ---- batch: 050 ----
mean loss: 491.30
 ---- batch: 060 ----
mean loss: 499.72
 ---- batch: 070 ----
mean loss: 501.98
 ---- batch: 080 ----
mean loss: 493.10
 ---- batch: 090 ----
mean loss: 496.92
 ---- batch: 100 ----
mean loss: 513.05
 ---- batch: 110 ----
mean loss: 501.62
train mean loss: 498.19
epoch train time: 0:00:02.180151
elapsed time: 0:03:31.587447
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 00:30:20.873509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.08
 ---- batch: 020 ----
mean loss: 497.39
 ---- batch: 030 ----
mean loss: 487.97
 ---- batch: 040 ----
mean loss: 502.11
 ---- batch: 050 ----
mean loss: 502.39
 ---- batch: 060 ----
mean loss: 491.91
 ---- batch: 070 ----
mean loss: 506.22
 ---- batch: 080 ----
mean loss: 501.76
 ---- batch: 090 ----
mean loss: 523.41
 ---- batch: 100 ----
mean loss: 493.41
 ---- batch: 110 ----
mean loss: 490.34
train mean loss: 499.87
epoch train time: 0:00:02.180014
elapsed time: 0:03:33.767688
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 00:30:23.053757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.45
 ---- batch: 020 ----
mean loss: 482.51
 ---- batch: 030 ----
mean loss: 497.79
 ---- batch: 040 ----
mean loss: 478.54
 ---- batch: 050 ----
mean loss: 478.01
 ---- batch: 060 ----
mean loss: 495.88
 ---- batch: 070 ----
mean loss: 495.72
 ---- batch: 080 ----
mean loss: 495.34
 ---- batch: 090 ----
mean loss: 495.56
 ---- batch: 100 ----
mean loss: 504.69
 ---- batch: 110 ----
mean loss: 500.43
train mean loss: 492.15
epoch train time: 0:00:02.185173
elapsed time: 0:03:35.953030
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 00:30:25.239097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.69
 ---- batch: 020 ----
mean loss: 510.65
 ---- batch: 030 ----
mean loss: 492.16
 ---- batch: 040 ----
mean loss: 494.18
 ---- batch: 050 ----
mean loss: 497.27
 ---- batch: 060 ----
mean loss: 502.20
 ---- batch: 070 ----
mean loss: 484.05
 ---- batch: 080 ----
mean loss: 503.46
 ---- batch: 090 ----
mean loss: 498.67
 ---- batch: 100 ----
mean loss: 495.19
 ---- batch: 110 ----
mean loss: 487.83
train mean loss: 493.48
epoch train time: 0:00:02.178630
elapsed time: 0:03:38.131846
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 00:30:27.417908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.31
 ---- batch: 020 ----
mean loss: 494.99
 ---- batch: 030 ----
mean loss: 496.19
 ---- batch: 040 ----
mean loss: 494.54
 ---- batch: 050 ----
mean loss: 499.96
 ---- batch: 060 ----
mean loss: 499.52
 ---- batch: 070 ----
mean loss: 492.21
 ---- batch: 080 ----
mean loss: 486.03
 ---- batch: 090 ----
mean loss: 499.09
 ---- batch: 100 ----
mean loss: 501.82
 ---- batch: 110 ----
mean loss: 483.29
train mean loss: 492.74
epoch train time: 0:00:02.184798
elapsed time: 0:03:40.316844
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 00:30:29.602934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.66
 ---- batch: 020 ----
mean loss: 510.89
 ---- batch: 030 ----
mean loss: 499.94
 ---- batch: 040 ----
mean loss: 486.74
 ---- batch: 050 ----
mean loss: 496.26
 ---- batch: 060 ----
mean loss: 518.42
 ---- batch: 070 ----
mean loss: 481.92
 ---- batch: 080 ----
mean loss: 496.43
 ---- batch: 090 ----
mean loss: 480.75
 ---- batch: 100 ----
mean loss: 478.61
 ---- batch: 110 ----
mean loss: 490.27
train mean loss: 493.38
epoch train time: 0:00:02.175904
elapsed time: 0:03:42.492947
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 00:30:31.779011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.55
 ---- batch: 020 ----
mean loss: 494.84
 ---- batch: 030 ----
mean loss: 486.69
 ---- batch: 040 ----
mean loss: 500.77
 ---- batch: 050 ----
mean loss: 497.57
 ---- batch: 060 ----
mean loss: 489.67
 ---- batch: 070 ----
mean loss: 485.99
 ---- batch: 080 ----
mean loss: 494.66
 ---- batch: 090 ----
mean loss: 500.11
 ---- batch: 100 ----
mean loss: 494.85
 ---- batch: 110 ----
mean loss: 482.05
train mean loss: 492.63
epoch train time: 0:00:02.154661
elapsed time: 0:03:44.647780
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 00:30:33.933864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.37
 ---- batch: 020 ----
mean loss: 491.30
 ---- batch: 030 ----
mean loss: 476.10
 ---- batch: 040 ----
mean loss: 505.91
 ---- batch: 050 ----
mean loss: 481.31
 ---- batch: 060 ----
mean loss: 491.09
 ---- batch: 070 ----
mean loss: 491.62
 ---- batch: 080 ----
mean loss: 489.63
 ---- batch: 090 ----
mean loss: 503.82
 ---- batch: 100 ----
mean loss: 498.86
 ---- batch: 110 ----
mean loss: 483.32
train mean loss: 490.26
epoch train time: 0:00:02.165669
elapsed time: 0:03:46.813655
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 00:30:36.099715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.24
 ---- batch: 020 ----
mean loss: 493.34
 ---- batch: 030 ----
mean loss: 491.02
 ---- batch: 040 ----
mean loss: 499.98
 ---- batch: 050 ----
mean loss: 496.69
 ---- batch: 060 ----
mean loss: 500.30
 ---- batch: 070 ----
mean loss: 489.37
 ---- batch: 080 ----
mean loss: 495.91
 ---- batch: 090 ----
mean loss: 511.69
 ---- batch: 100 ----
mean loss: 493.47
 ---- batch: 110 ----
mean loss: 505.82
train mean loss: 495.15
epoch train time: 0:00:02.157476
elapsed time: 0:03:48.971293
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 00:30:38.257358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.37
 ---- batch: 020 ----
mean loss: 493.74
 ---- batch: 030 ----
mean loss: 507.68
 ---- batch: 040 ----
mean loss: 482.55
 ---- batch: 050 ----
mean loss: 486.12
 ---- batch: 060 ----
mean loss: 493.71
 ---- batch: 070 ----
mean loss: 481.24
 ---- batch: 080 ----
mean loss: 496.38
 ---- batch: 090 ----
mean loss: 503.59
 ---- batch: 100 ----
mean loss: 498.64
 ---- batch: 110 ----
mean loss: 494.94
train mean loss: 494.31
epoch train time: 0:00:02.156718
elapsed time: 0:03:51.128179
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 00:30:40.414237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.39
 ---- batch: 020 ----
mean loss: 499.65
 ---- batch: 030 ----
mean loss: 486.38
 ---- batch: 040 ----
mean loss: 491.29
 ---- batch: 050 ----
mean loss: 486.19
 ---- batch: 060 ----
mean loss: 483.30
 ---- batch: 070 ----
mean loss: 498.46
 ---- batch: 080 ----
mean loss: 483.18
 ---- batch: 090 ----
mean loss: 489.18
 ---- batch: 100 ----
mean loss: 475.93
 ---- batch: 110 ----
mean loss: 488.72
train mean loss: 489.47
epoch train time: 0:00:02.159762
elapsed time: 0:03:53.288127
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 00:30:42.574200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.12
 ---- batch: 020 ----
mean loss: 512.05
 ---- batch: 030 ----
mean loss: 497.34
 ---- batch: 040 ----
mean loss: 502.37
 ---- batch: 050 ----
mean loss: 490.77
 ---- batch: 060 ----
mean loss: 476.35
 ---- batch: 070 ----
mean loss: 485.50
 ---- batch: 080 ----
mean loss: 491.75
 ---- batch: 090 ----
mean loss: 483.13
 ---- batch: 100 ----
mean loss: 492.80
 ---- batch: 110 ----
mean loss: 473.44
train mean loss: 490.16
epoch train time: 0:00:02.161063
elapsed time: 0:03:55.449363
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 00:30:44.735465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.84
 ---- batch: 020 ----
mean loss: 485.17
 ---- batch: 030 ----
mean loss: 481.73
 ---- batch: 040 ----
mean loss: 480.09
 ---- batch: 050 ----
mean loss: 495.19
 ---- batch: 060 ----
mean loss: 488.94
 ---- batch: 070 ----
mean loss: 501.15
 ---- batch: 080 ----
mean loss: 473.89
 ---- batch: 090 ----
mean loss: 473.58
 ---- batch: 100 ----
mean loss: 491.59
 ---- batch: 110 ----
mean loss: 491.25
train mean loss: 487.44
epoch train time: 0:00:02.156910
elapsed time: 0:03:57.606490
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 00:30:46.892557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.59
 ---- batch: 020 ----
mean loss: 496.29
 ---- batch: 030 ----
mean loss: 488.48
 ---- batch: 040 ----
mean loss: 479.31
 ---- batch: 050 ----
mean loss: 486.57
 ---- batch: 060 ----
mean loss: 490.72
 ---- batch: 070 ----
mean loss: 500.47
 ---- batch: 080 ----
mean loss: 475.29
 ---- batch: 090 ----
mean loss: 499.77
 ---- batch: 100 ----
mean loss: 477.89
 ---- batch: 110 ----
mean loss: 493.69
train mean loss: 488.62
epoch train time: 0:00:02.159096
elapsed time: 0:03:59.765769
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 00:30:49.051834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.04
 ---- batch: 020 ----
mean loss: 490.34
 ---- batch: 030 ----
mean loss: 496.74
 ---- batch: 040 ----
mean loss: 492.09
 ---- batch: 050 ----
mean loss: 485.77
 ---- batch: 060 ----
mean loss: 490.02
 ---- batch: 070 ----
mean loss: 486.89
 ---- batch: 080 ----
mean loss: 481.27
 ---- batch: 090 ----
mean loss: 482.26
 ---- batch: 100 ----
mean loss: 496.68
 ---- batch: 110 ----
mean loss: 476.48
train mean loss: 488.69
epoch train time: 0:00:02.162194
elapsed time: 0:04:01.928139
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 00:30:51.214200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.93
 ---- batch: 020 ----
mean loss: 473.83
 ---- batch: 030 ----
mean loss: 474.25
 ---- batch: 040 ----
mean loss: 485.18
 ---- batch: 050 ----
mean loss: 517.76
 ---- batch: 060 ----
mean loss: 489.89
 ---- batch: 070 ----
mean loss: 496.13
 ---- batch: 080 ----
mean loss: 487.55
 ---- batch: 090 ----
mean loss: 483.25
 ---- batch: 100 ----
mean loss: 490.01
 ---- batch: 110 ----
mean loss: 478.46
train mean loss: 486.85
epoch train time: 0:00:02.153614
elapsed time: 0:04:04.081934
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 00:30:53.367997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.36
 ---- batch: 020 ----
mean loss: 483.66
 ---- batch: 030 ----
mean loss: 485.82
 ---- batch: 040 ----
mean loss: 477.17
 ---- batch: 050 ----
mean loss: 489.66
 ---- batch: 060 ----
mean loss: 477.74
 ---- batch: 070 ----
mean loss: 482.58
 ---- batch: 080 ----
mean loss: 503.17
 ---- batch: 090 ----
mean loss: 489.29
 ---- batch: 100 ----
mean loss: 482.90
 ---- batch: 110 ----
mean loss: 483.34
train mean loss: 483.97
epoch train time: 0:00:02.158533
elapsed time: 0:04:06.240641
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 00:30:55.526699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.43
 ---- batch: 020 ----
mean loss: 483.34
 ---- batch: 030 ----
mean loss: 483.59
 ---- batch: 040 ----
mean loss: 471.04
 ---- batch: 050 ----
mean loss: 488.15
 ---- batch: 060 ----
mean loss: 490.80
 ---- batch: 070 ----
mean loss: 496.40
 ---- batch: 080 ----
mean loss: 489.61
 ---- batch: 090 ----
mean loss: 487.45
 ---- batch: 100 ----
mean loss: 493.36
 ---- batch: 110 ----
mean loss: 471.58
train mean loss: 485.25
epoch train time: 0:00:02.153569
elapsed time: 0:04:08.394383
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 00:30:57.680445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.11
 ---- batch: 020 ----
mean loss: 482.53
 ---- batch: 030 ----
mean loss: 487.69
 ---- batch: 040 ----
mean loss: 490.69
 ---- batch: 050 ----
mean loss: 465.21
 ---- batch: 060 ----
mean loss: 483.59
 ---- batch: 070 ----
mean loss: 480.65
 ---- batch: 080 ----
mean loss: 469.26
 ---- batch: 090 ----
mean loss: 482.93
 ---- batch: 100 ----
mean loss: 490.27
 ---- batch: 110 ----
mean loss: 497.76
train mean loss: 484.05
epoch train time: 0:00:02.164320
elapsed time: 0:04:10.558869
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 00:30:59.844965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.63
 ---- batch: 020 ----
mean loss: 484.15
 ---- batch: 030 ----
mean loss: 492.75
 ---- batch: 040 ----
mean loss: 490.09
 ---- batch: 050 ----
mean loss: 477.11
 ---- batch: 060 ----
mean loss: 483.44
 ---- batch: 070 ----
mean loss: 485.19
 ---- batch: 080 ----
mean loss: 489.70
 ---- batch: 090 ----
mean loss: 478.18
 ---- batch: 100 ----
mean loss: 487.97
 ---- batch: 110 ----
mean loss: 492.55
train mean loss: 486.58
epoch train time: 0:00:02.161999
elapsed time: 0:04:12.721081
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 00:31:02.007144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.77
 ---- batch: 020 ----
mean loss: 503.83
 ---- batch: 030 ----
mean loss: 498.96
 ---- batch: 040 ----
mean loss: 484.81
 ---- batch: 050 ----
mean loss: 496.50
 ---- batch: 060 ----
mean loss: 472.82
 ---- batch: 070 ----
mean loss: 508.07
 ---- batch: 080 ----
mean loss: 501.00
 ---- batch: 090 ----
mean loss: 489.62
 ---- batch: 100 ----
mean loss: 505.29
 ---- batch: 110 ----
mean loss: 483.28
train mean loss: 494.63
epoch train time: 0:00:02.166303
elapsed time: 0:04:14.887546
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 00:31:04.173633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.52
 ---- batch: 020 ----
mean loss: 484.27
 ---- batch: 030 ----
mean loss: 486.91
 ---- batch: 040 ----
mean loss: 482.50
 ---- batch: 050 ----
mean loss: 497.89
 ---- batch: 060 ----
mean loss: 495.52
 ---- batch: 070 ----
mean loss: 477.90
 ---- batch: 080 ----
mean loss: 495.18
 ---- batch: 090 ----
mean loss: 501.36
 ---- batch: 100 ----
mean loss: 478.71
 ---- batch: 110 ----
mean loss: 486.79
train mean loss: 487.78
epoch train time: 0:00:02.178916
elapsed time: 0:04:17.066693
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 00:31:06.352757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.23
 ---- batch: 020 ----
mean loss: 475.91
 ---- batch: 030 ----
mean loss: 481.79
 ---- batch: 040 ----
mean loss: 487.68
 ---- batch: 050 ----
mean loss: 487.33
 ---- batch: 060 ----
mean loss: 483.46
 ---- batch: 070 ----
mean loss: 483.02
 ---- batch: 080 ----
mean loss: 470.66
 ---- batch: 090 ----
mean loss: 478.84
 ---- batch: 100 ----
mean loss: 477.71
 ---- batch: 110 ----
mean loss: 489.54
train mean loss: 482.42
epoch train time: 0:00:02.174106
elapsed time: 0:04:19.240970
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 00:31:08.527034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.64
 ---- batch: 020 ----
mean loss: 501.30
 ---- batch: 030 ----
mean loss: 482.43
 ---- batch: 040 ----
mean loss: 466.23
 ---- batch: 050 ----
mean loss: 474.49
 ---- batch: 060 ----
mean loss: 471.39
 ---- batch: 070 ----
mean loss: 476.77
 ---- batch: 080 ----
mean loss: 467.73
 ---- batch: 090 ----
mean loss: 472.50
 ---- batch: 100 ----
mean loss: 489.83
 ---- batch: 110 ----
mean loss: 482.93
train mean loss: 478.53
epoch train time: 0:00:02.176377
elapsed time: 0:04:21.417528
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 00:31:10.703592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.14
 ---- batch: 020 ----
mean loss: 485.51
 ---- batch: 030 ----
mean loss: 500.51
 ---- batch: 040 ----
mean loss: 470.85
 ---- batch: 050 ----
mean loss: 464.62
 ---- batch: 060 ----
mean loss: 490.26
 ---- batch: 070 ----
mean loss: 472.26
 ---- batch: 080 ----
mean loss: 479.38
 ---- batch: 090 ----
mean loss: 479.55
 ---- batch: 100 ----
mean loss: 477.88
 ---- batch: 110 ----
mean loss: 492.57
train mean loss: 481.20
epoch train time: 0:00:02.178321
elapsed time: 0:04:23.596044
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 00:31:12.882113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.60
 ---- batch: 020 ----
mean loss: 493.76
 ---- batch: 030 ----
mean loss: 476.31
 ---- batch: 040 ----
mean loss: 474.35
 ---- batch: 050 ----
mean loss: 500.29
 ---- batch: 060 ----
mean loss: 486.49
 ---- batch: 070 ----
mean loss: 482.21
 ---- batch: 080 ----
mean loss: 488.93
 ---- batch: 090 ----
mean loss: 494.00
 ---- batch: 100 ----
mean loss: 475.26
 ---- batch: 110 ----
mean loss: 492.31
train mean loss: 485.94
epoch train time: 0:00:02.180677
elapsed time: 0:04:25.776888
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 00:31:15.062948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.38
 ---- batch: 020 ----
mean loss: 476.85
 ---- batch: 030 ----
mean loss: 481.52
 ---- batch: 040 ----
mean loss: 467.12
 ---- batch: 050 ----
mean loss: 493.43
 ---- batch: 060 ----
mean loss: 486.81
 ---- batch: 070 ----
mean loss: 478.82
 ---- batch: 080 ----
mean loss: 487.22
 ---- batch: 090 ----
mean loss: 498.47
 ---- batch: 100 ----
mean loss: 476.47
 ---- batch: 110 ----
mean loss: 466.00
train mean loss: 480.95
epoch train time: 0:00:02.180275
elapsed time: 0:04:27.957334
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 00:31:17.243393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.93
 ---- batch: 020 ----
mean loss: 488.48
 ---- batch: 030 ----
mean loss: 474.17
 ---- batch: 040 ----
mean loss: 460.42
 ---- batch: 050 ----
mean loss: 486.48
 ---- batch: 060 ----
mean loss: 472.46
 ---- batch: 070 ----
mean loss: 489.43
 ---- batch: 080 ----
mean loss: 469.13
 ---- batch: 090 ----
mean loss: 480.96
 ---- batch: 100 ----
mean loss: 474.42
 ---- batch: 110 ----
mean loss: 487.91
train mean loss: 478.20
epoch train time: 0:00:02.180743
elapsed time: 0:04:30.138271
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 00:31:19.424333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.81
 ---- batch: 020 ----
mean loss: 470.33
 ---- batch: 030 ----
mean loss: 490.60
 ---- batch: 040 ----
mean loss: 476.63
 ---- batch: 050 ----
mean loss: 473.14
 ---- batch: 060 ----
mean loss: 451.66
 ---- batch: 070 ----
mean loss: 490.05
 ---- batch: 080 ----
mean loss: 478.17
 ---- batch: 090 ----
mean loss: 470.98
 ---- batch: 100 ----
mean loss: 470.32
 ---- batch: 110 ----
mean loss: 493.39
train mean loss: 477.65
epoch train time: 0:00:02.177411
elapsed time: 0:04:32.315852
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 00:31:21.601913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.01
 ---- batch: 020 ----
mean loss: 481.49
 ---- batch: 030 ----
mean loss: 484.72
 ---- batch: 040 ----
mean loss: 462.70
 ---- batch: 050 ----
mean loss: 492.00
 ---- batch: 060 ----
mean loss: 461.68
 ---- batch: 070 ----
mean loss: 451.57
 ---- batch: 080 ----
mean loss: 484.40
 ---- batch: 090 ----
mean loss: 475.66
 ---- batch: 100 ----
mean loss: 488.07
 ---- batch: 110 ----
mean loss: 478.84
train mean loss: 475.36
epoch train time: 0:00:02.183231
elapsed time: 0:04:34.499271
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 00:31:23.785349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.85
 ---- batch: 020 ----
mean loss: 471.88
 ---- batch: 030 ----
mean loss: 488.97
 ---- batch: 040 ----
mean loss: 476.89
 ---- batch: 050 ----
mean loss: 462.73
 ---- batch: 060 ----
mean loss: 475.47
 ---- batch: 070 ----
mean loss: 460.71
 ---- batch: 080 ----
mean loss: 479.50
 ---- batch: 090 ----
mean loss: 474.84
 ---- batch: 100 ----
mean loss: 484.25
 ---- batch: 110 ----
mean loss: 463.77
train mean loss: 475.27
epoch train time: 0:00:02.181198
elapsed time: 0:04:36.680678
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 00:31:25.966743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.31
 ---- batch: 020 ----
mean loss: 466.09
 ---- batch: 030 ----
mean loss: 481.93
 ---- batch: 040 ----
mean loss: 477.24
 ---- batch: 050 ----
mean loss: 477.89
 ---- batch: 060 ----
mean loss: 464.50
 ---- batch: 070 ----
mean loss: 478.23
 ---- batch: 080 ----
mean loss: 478.97
 ---- batch: 090 ----
mean loss: 472.32
 ---- batch: 100 ----
mean loss: 468.44
 ---- batch: 110 ----
mean loss: 482.48
train mean loss: 473.52
epoch train time: 0:00:02.180062
elapsed time: 0:04:38.860938
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 00:31:28.147033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.97
 ---- batch: 020 ----
mean loss: 491.54
 ---- batch: 030 ----
mean loss: 462.03
 ---- batch: 040 ----
mean loss: 475.74
 ---- batch: 050 ----
mean loss: 471.10
 ---- batch: 060 ----
mean loss: 473.77
 ---- batch: 070 ----
mean loss: 469.79
 ---- batch: 080 ----
mean loss: 482.55
 ---- batch: 090 ----
mean loss: 462.66
 ---- batch: 100 ----
mean loss: 473.61
 ---- batch: 110 ----
mean loss: 484.26
train mean loss: 475.20
epoch train time: 0:00:02.181474
elapsed time: 0:04:41.042613
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 00:31:30.328673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.64
 ---- batch: 020 ----
mean loss: 465.46
 ---- batch: 030 ----
mean loss: 488.02
 ---- batch: 040 ----
mean loss: 464.09
 ---- batch: 050 ----
mean loss: 456.13
 ---- batch: 060 ----
mean loss: 460.15
 ---- batch: 070 ----
mean loss: 469.15
 ---- batch: 080 ----
mean loss: 465.74
 ---- batch: 090 ----
mean loss: 470.72
 ---- batch: 100 ----
mean loss: 457.97
 ---- batch: 110 ----
mean loss: 477.67
train mean loss: 466.75
epoch train time: 0:00:02.177721
elapsed time: 0:04:43.220524
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 00:31:32.506586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.76
 ---- batch: 020 ----
mean loss: 460.49
 ---- batch: 030 ----
mean loss: 480.25
 ---- batch: 040 ----
mean loss: 472.70
 ---- batch: 050 ----
mean loss: 460.00
 ---- batch: 060 ----
mean loss: 478.41
 ---- batch: 070 ----
mean loss: 470.86
 ---- batch: 080 ----
mean loss: 468.75
 ---- batch: 090 ----
mean loss: 462.70
 ---- batch: 100 ----
mean loss: 490.19
 ---- batch: 110 ----
mean loss: 456.48
train mean loss: 470.08
epoch train time: 0:00:02.178689
elapsed time: 0:04:45.399382
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 00:31:34.685448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.98
 ---- batch: 020 ----
mean loss: 473.51
 ---- batch: 030 ----
mean loss: 476.39
 ---- batch: 040 ----
mean loss: 468.53
 ---- batch: 050 ----
mean loss: 465.18
 ---- batch: 060 ----
mean loss: 481.05
 ---- batch: 070 ----
mean loss: 474.49
 ---- batch: 080 ----
mean loss: 491.88
 ---- batch: 090 ----
mean loss: 473.30
 ---- batch: 100 ----
mean loss: 465.50
 ---- batch: 110 ----
mean loss: 466.64
train mean loss: 473.19
epoch train time: 0:00:02.181517
elapsed time: 0:04:47.581091
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 00:31:36.867152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.42
 ---- batch: 020 ----
mean loss: 456.95
 ---- batch: 030 ----
mean loss: 469.22
 ---- batch: 040 ----
mean loss: 453.60
 ---- batch: 050 ----
mean loss: 477.71
 ---- batch: 060 ----
mean loss: 462.84
 ---- batch: 070 ----
mean loss: 455.12
 ---- batch: 080 ----
mean loss: 480.68
 ---- batch: 090 ----
mean loss: 471.92
 ---- batch: 100 ----
mean loss: 466.03
 ---- batch: 110 ----
mean loss: 457.48
train mean loss: 464.84
epoch train time: 0:00:02.177976
elapsed time: 0:04:49.759246
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 00:31:39.045309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.77
 ---- batch: 020 ----
mean loss: 469.95
 ---- batch: 030 ----
mean loss: 461.45
 ---- batch: 040 ----
mean loss: 447.87
 ---- batch: 050 ----
mean loss: 468.68
 ---- batch: 060 ----
mean loss: 473.23
 ---- batch: 070 ----
mean loss: 482.91
 ---- batch: 080 ----
mean loss: 463.13
 ---- batch: 090 ----
mean loss: 470.90
 ---- batch: 100 ----
mean loss: 447.09
 ---- batch: 110 ----
mean loss: 465.96
train mean loss: 463.71
epoch train time: 0:00:02.180113
elapsed time: 0:04:51.939548
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 00:31:41.225666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.20
 ---- batch: 020 ----
mean loss: 450.79
 ---- batch: 030 ----
mean loss: 465.27
 ---- batch: 040 ----
mean loss: 466.69
 ---- batch: 050 ----
mean loss: 463.13
 ---- batch: 060 ----
mean loss: 459.52
 ---- batch: 070 ----
mean loss: 449.42
 ---- batch: 080 ----
mean loss: 465.45
 ---- batch: 090 ----
mean loss: 471.91
 ---- batch: 100 ----
mean loss: 467.97
 ---- batch: 110 ----
mean loss: 465.85
train mean loss: 461.92
epoch train time: 0:00:02.182704
elapsed time: 0:04:54.122505
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 00:31:43.408571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.60
 ---- batch: 020 ----
mean loss: 456.87
 ---- batch: 030 ----
mean loss: 459.07
 ---- batch: 040 ----
mean loss: 457.22
 ---- batch: 050 ----
mean loss: 447.37
 ---- batch: 060 ----
mean loss: 466.08
 ---- batch: 070 ----
mean loss: 462.72
 ---- batch: 080 ----
mean loss: 455.77
 ---- batch: 090 ----
mean loss: 449.43
 ---- batch: 100 ----
mean loss: 453.39
 ---- batch: 110 ----
mean loss: 445.59
train mean loss: 456.03
epoch train time: 0:00:02.175909
elapsed time: 0:04:56.298614
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 00:31:45.584680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.91
 ---- batch: 020 ----
mean loss: 455.99
 ---- batch: 030 ----
mean loss: 456.25
 ---- batch: 040 ----
mean loss: 446.06
 ---- batch: 050 ----
mean loss: 461.68
 ---- batch: 060 ----
mean loss: 443.88
 ---- batch: 070 ----
mean loss: 462.27
 ---- batch: 080 ----
mean loss: 460.68
 ---- batch: 090 ----
mean loss: 428.44
 ---- batch: 100 ----
mean loss: 451.20
 ---- batch: 110 ----
mean loss: 439.29
train mean loss: 449.65
epoch train time: 0:00:02.180786
elapsed time: 0:04:58.479584
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 00:31:47.765676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.54
 ---- batch: 020 ----
mean loss: 444.64
 ---- batch: 030 ----
mean loss: 437.36
 ---- batch: 040 ----
mean loss: 455.22
 ---- batch: 050 ----
mean loss: 448.25
 ---- batch: 060 ----
mean loss: 447.38
 ---- batch: 070 ----
mean loss: 439.38
 ---- batch: 080 ----
mean loss: 449.49
 ---- batch: 090 ----
mean loss: 453.18
 ---- batch: 100 ----
mean loss: 445.36
 ---- batch: 110 ----
mean loss: 439.42
train mean loss: 446.86
epoch train time: 0:00:02.183064
elapsed time: 0:05:00.662842
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 00:31:49.948906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.70
 ---- batch: 020 ----
mean loss: 430.65
 ---- batch: 030 ----
mean loss: 453.66
 ---- batch: 040 ----
mean loss: 451.88
 ---- batch: 050 ----
mean loss: 438.00
 ---- batch: 060 ----
mean loss: 438.55
 ---- batch: 070 ----
mean loss: 441.79
 ---- batch: 080 ----
mean loss: 446.49
 ---- batch: 090 ----
mean loss: 432.59
 ---- batch: 100 ----
mean loss: 435.01
 ---- batch: 110 ----
mean loss: 436.60
train mean loss: 441.74
epoch train time: 0:00:02.178363
elapsed time: 0:05:02.841410
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 00:31:52.127475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.00
 ---- batch: 020 ----
mean loss: 437.28
 ---- batch: 030 ----
mean loss: 446.11
 ---- batch: 040 ----
mean loss: 447.84
 ---- batch: 050 ----
mean loss: 439.27
 ---- batch: 060 ----
mean loss: 430.63
 ---- batch: 070 ----
mean loss: 430.02
 ---- batch: 080 ----
mean loss: 448.99
 ---- batch: 090 ----
mean loss: 428.76
 ---- batch: 100 ----
mean loss: 438.71
 ---- batch: 110 ----
mean loss: 437.76
train mean loss: 436.99
epoch train time: 0:00:02.182414
elapsed time: 0:05:05.024007
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 00:31:54.310077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.56
 ---- batch: 020 ----
mean loss: 434.11
 ---- batch: 030 ----
mean loss: 429.07
 ---- batch: 040 ----
mean loss: 430.29
 ---- batch: 050 ----
mean loss: 425.56
 ---- batch: 060 ----
mean loss: 427.45
 ---- batch: 070 ----
mean loss: 436.95
 ---- batch: 080 ----
mean loss: 445.18
 ---- batch: 090 ----
mean loss: 435.57
 ---- batch: 100 ----
mean loss: 439.87
 ---- batch: 110 ----
mean loss: 435.19
train mean loss: 434.73
epoch train time: 0:00:02.176138
elapsed time: 0:05:07.200396
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 00:31:56.486468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.90
 ---- batch: 020 ----
mean loss: 428.67
 ---- batch: 030 ----
mean loss: 415.35
 ---- batch: 040 ----
mean loss: 426.25
 ---- batch: 050 ----
mean loss: 439.78
 ---- batch: 060 ----
mean loss: 428.06
 ---- batch: 070 ----
mean loss: 429.48
 ---- batch: 080 ----
mean loss: 438.33
 ---- batch: 090 ----
mean loss: 431.37
 ---- batch: 100 ----
mean loss: 433.07
 ---- batch: 110 ----
mean loss: 432.18
train mean loss: 431.52
epoch train time: 0:00:02.182336
elapsed time: 0:05:09.382964
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 00:31:58.669031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.27
 ---- batch: 020 ----
mean loss: 429.65
 ---- batch: 030 ----
mean loss: 436.89
 ---- batch: 040 ----
mean loss: 430.88
 ---- batch: 050 ----
mean loss: 435.09
 ---- batch: 060 ----
mean loss: 429.87
 ---- batch: 070 ----
mean loss: 431.32
 ---- batch: 080 ----
mean loss: 436.98
 ---- batch: 090 ----
mean loss: 413.75
 ---- batch: 100 ----
mean loss: 436.52
 ---- batch: 110 ----
mean loss: 432.45
train mean loss: 429.74
epoch train time: 0:00:02.183566
elapsed time: 0:05:11.566713
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 00:32:00.852777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.21
 ---- batch: 020 ----
mean loss: 431.47
 ---- batch: 030 ----
mean loss: 421.26
 ---- batch: 040 ----
mean loss: 443.87
 ---- batch: 050 ----
mean loss: 439.39
 ---- batch: 060 ----
mean loss: 436.62
 ---- batch: 070 ----
mean loss: 413.55
 ---- batch: 080 ----
mean loss: 426.93
 ---- batch: 090 ----
mean loss: 432.15
 ---- batch: 100 ----
mean loss: 423.62
 ---- batch: 110 ----
mean loss: 421.18
train mean loss: 428.53
epoch train time: 0:00:02.185818
elapsed time: 0:05:13.752715
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 00:32:03.038776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.48
 ---- batch: 020 ----
mean loss: 412.02
 ---- batch: 030 ----
mean loss: 419.57
 ---- batch: 040 ----
mean loss: 432.24
 ---- batch: 050 ----
mean loss: 415.11
 ---- batch: 060 ----
mean loss: 417.22
 ---- batch: 070 ----
mean loss: 426.52
 ---- batch: 080 ----
mean loss: 419.40
 ---- batch: 090 ----
mean loss: 427.08
 ---- batch: 100 ----
mean loss: 423.55
 ---- batch: 110 ----
mean loss: 422.48
train mean loss: 420.10
epoch train time: 0:00:02.176059
elapsed time: 0:05:15.928938
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 00:32:05.215004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.66
 ---- batch: 020 ----
mean loss: 411.52
 ---- batch: 030 ----
mean loss: 431.06
 ---- batch: 040 ----
mean loss: 412.52
 ---- batch: 050 ----
mean loss: 429.67
 ---- batch: 060 ----
mean loss: 419.45
 ---- batch: 070 ----
mean loss: 433.67
 ---- batch: 080 ----
mean loss: 426.69
 ---- batch: 090 ----
mean loss: 436.99
 ---- batch: 100 ----
mean loss: 427.37
 ---- batch: 110 ----
mean loss: 424.18
train mean loss: 425.28
epoch train time: 0:00:02.176277
elapsed time: 0:05:18.105380
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 00:32:07.391441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.04
 ---- batch: 020 ----
mean loss: 428.49
 ---- batch: 030 ----
mean loss: 423.38
 ---- batch: 040 ----
mean loss: 417.99
 ---- batch: 050 ----
mean loss: 429.43
 ---- batch: 060 ----
mean loss: 421.95
 ---- batch: 070 ----
mean loss: 419.63
 ---- batch: 080 ----
mean loss: 419.08
 ---- batch: 090 ----
mean loss: 407.91
 ---- batch: 100 ----
mean loss: 420.74
 ---- batch: 110 ----
mean loss: 437.67
train mean loss: 423.84
epoch train time: 0:00:02.180939
elapsed time: 0:05:20.286491
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 00:32:09.572554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.75
 ---- batch: 020 ----
mean loss: 417.54
 ---- batch: 030 ----
mean loss: 416.42
 ---- batch: 040 ----
mean loss: 432.72
 ---- batch: 050 ----
mean loss: 403.19
 ---- batch: 060 ----
mean loss: 414.76
 ---- batch: 070 ----
mean loss: 414.01
 ---- batch: 080 ----
mean loss: 424.08
 ---- batch: 090 ----
mean loss: 415.22
 ---- batch: 100 ----
mean loss: 409.06
 ---- batch: 110 ----
mean loss: 429.30
train mean loss: 418.18
epoch train time: 0:00:02.184148
elapsed time: 0:05:22.470822
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 00:32:11.756906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.91
 ---- batch: 020 ----
mean loss: 423.55
 ---- batch: 030 ----
mean loss: 417.40
 ---- batch: 040 ----
mean loss: 423.75
 ---- batch: 050 ----
mean loss: 426.74
 ---- batch: 060 ----
mean loss: 424.49
 ---- batch: 070 ----
mean loss: 422.29
 ---- batch: 080 ----
mean loss: 406.88
 ---- batch: 090 ----
mean loss: 411.97
 ---- batch: 100 ----
mean loss: 411.17
 ---- batch: 110 ----
mean loss: 413.72
train mean loss: 417.41
epoch train time: 0:00:02.181139
elapsed time: 0:05:24.652152
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 00:32:13.938240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.65
 ---- batch: 020 ----
mean loss: 426.55
 ---- batch: 030 ----
mean loss: 429.00
 ---- batch: 040 ----
mean loss: 414.69
 ---- batch: 050 ----
mean loss: 408.96
 ---- batch: 060 ----
mean loss: 406.99
 ---- batch: 070 ----
mean loss: 400.01
 ---- batch: 080 ----
mean loss: 416.17
 ---- batch: 090 ----
mean loss: 431.26
 ---- batch: 100 ----
mean loss: 408.84
 ---- batch: 110 ----
mean loss: 417.33
train mean loss: 415.22
epoch train time: 0:00:02.178971
elapsed time: 0:05:26.831318
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 00:32:16.117380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.60
 ---- batch: 020 ----
mean loss: 417.62
 ---- batch: 030 ----
mean loss: 419.20
 ---- batch: 040 ----
mean loss: 404.47
 ---- batch: 050 ----
mean loss: 400.86
 ---- batch: 060 ----
mean loss: 414.92
 ---- batch: 070 ----
mean loss: 416.08
 ---- batch: 080 ----
mean loss: 422.44
 ---- batch: 090 ----
mean loss: 423.61
 ---- batch: 100 ----
mean loss: 410.99
 ---- batch: 110 ----
mean loss: 410.82
train mean loss: 415.35
epoch train time: 0:00:02.178751
elapsed time: 0:05:29.010240
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 00:32:18.296305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.81
 ---- batch: 020 ----
mean loss: 416.58
 ---- batch: 030 ----
mean loss: 407.13
 ---- batch: 040 ----
mean loss: 410.51
 ---- batch: 050 ----
mean loss: 410.40
 ---- batch: 060 ----
mean loss: 416.56
 ---- batch: 070 ----
mean loss: 411.65
 ---- batch: 080 ----
mean loss: 420.08
 ---- batch: 090 ----
mean loss: 393.39
 ---- batch: 100 ----
mean loss: 416.55
 ---- batch: 110 ----
mean loss: 406.30
train mean loss: 410.38
epoch train time: 0:00:02.177551
elapsed time: 0:05:31.187978
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 00:32:20.474050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.10
 ---- batch: 020 ----
mean loss: 402.75
 ---- batch: 030 ----
mean loss: 404.94
 ---- batch: 040 ----
mean loss: 410.17
 ---- batch: 050 ----
mean loss: 409.16
 ---- batch: 060 ----
mean loss: 408.68
 ---- batch: 070 ----
mean loss: 393.86
 ---- batch: 080 ----
mean loss: 400.61
 ---- batch: 090 ----
mean loss: 409.90
 ---- batch: 100 ----
mean loss: 420.01
 ---- batch: 110 ----
mean loss: 411.37
train mean loss: 408.18
epoch train time: 0:00:02.177585
elapsed time: 0:05:33.365733
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 00:32:22.651795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.30
 ---- batch: 020 ----
mean loss: 410.21
 ---- batch: 030 ----
mean loss: 423.82
 ---- batch: 040 ----
mean loss: 413.77
 ---- batch: 050 ----
mean loss: 409.25
 ---- batch: 060 ----
mean loss: 402.41
 ---- batch: 070 ----
mean loss: 394.56
 ---- batch: 080 ----
mean loss: 399.44
 ---- batch: 090 ----
mean loss: 403.33
 ---- batch: 100 ----
mean loss: 400.40
 ---- batch: 110 ----
mean loss: 407.83
train mean loss: 406.90
epoch train time: 0:00:02.174472
elapsed time: 0:05:35.540370
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 00:32:24.826456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.21
 ---- batch: 020 ----
mean loss: 407.58
 ---- batch: 030 ----
mean loss: 416.00
 ---- batch: 040 ----
mean loss: 405.36
 ---- batch: 050 ----
mean loss: 420.79
 ---- batch: 060 ----
mean loss: 396.53
 ---- batch: 070 ----
mean loss: 407.68
 ---- batch: 080 ----
mean loss: 410.74
 ---- batch: 090 ----
mean loss: 407.25
 ---- batch: 100 ----
mean loss: 400.18
 ---- batch: 110 ----
mean loss: 406.24
train mean loss: 405.73
epoch train time: 0:00:02.181407
elapsed time: 0:05:37.721993
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 00:32:27.008057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.43
 ---- batch: 020 ----
mean loss: 403.81
 ---- batch: 030 ----
mean loss: 397.80
 ---- batch: 040 ----
mean loss: 399.06
 ---- batch: 050 ----
mean loss: 393.28
 ---- batch: 060 ----
mean loss: 396.49
 ---- batch: 070 ----
mean loss: 419.15
 ---- batch: 080 ----
mean loss: 392.48
 ---- batch: 090 ----
mean loss: 401.23
 ---- batch: 100 ----
mean loss: 404.67
 ---- batch: 110 ----
mean loss: 407.33
train mean loss: 401.61
epoch train time: 0:00:02.181257
elapsed time: 0:05:39.903429
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 00:32:29.189491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.79
 ---- batch: 020 ----
mean loss: 413.10
 ---- batch: 030 ----
mean loss: 405.66
 ---- batch: 040 ----
mean loss: 400.28
 ---- batch: 050 ----
mean loss: 398.92
 ---- batch: 060 ----
mean loss: 412.19
 ---- batch: 070 ----
mean loss: 401.32
 ---- batch: 080 ----
mean loss: 386.56
 ---- batch: 090 ----
mean loss: 401.01
 ---- batch: 100 ----
mean loss: 411.82
 ---- batch: 110 ----
mean loss: 411.13
train mean loss: 402.70
epoch train time: 0:00:02.164505
elapsed time: 0:05:42.068127
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 00:32:31.354190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.91
 ---- batch: 020 ----
mean loss: 394.21
 ---- batch: 030 ----
mean loss: 394.06
 ---- batch: 040 ----
mean loss: 390.32
 ---- batch: 050 ----
mean loss: 402.88
 ---- batch: 060 ----
mean loss: 389.47
 ---- batch: 070 ----
mean loss: 397.43
 ---- batch: 080 ----
mean loss: 415.02
 ---- batch: 090 ----
mean loss: 403.25
 ---- batch: 100 ----
mean loss: 399.93
 ---- batch: 110 ----
mean loss: 383.90
train mean loss: 397.42
epoch train time: 0:00:02.156085
elapsed time: 0:05:44.224401
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 00:32:33.510476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.30
 ---- batch: 020 ----
mean loss: 397.83
 ---- batch: 030 ----
mean loss: 400.50
 ---- batch: 040 ----
mean loss: 403.55
 ---- batch: 050 ----
mean loss: 400.43
 ---- batch: 060 ----
mean loss: 389.54
 ---- batch: 070 ----
mean loss: 396.43
 ---- batch: 080 ----
mean loss: 394.34
 ---- batch: 090 ----
mean loss: 383.10
 ---- batch: 100 ----
mean loss: 410.02
 ---- batch: 110 ----
mean loss: 398.06
train mean loss: 396.90
epoch train time: 0:00:02.163162
elapsed time: 0:05:46.387753
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 00:32:35.673816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.07
 ---- batch: 020 ----
mean loss: 384.42
 ---- batch: 030 ----
mean loss: 406.86
 ---- batch: 040 ----
mean loss: 398.25
 ---- batch: 050 ----
mean loss: 403.89
 ---- batch: 060 ----
mean loss: 389.65
 ---- batch: 070 ----
mean loss: 393.79
 ---- batch: 080 ----
mean loss: 397.14
 ---- batch: 090 ----
mean loss: 390.94
 ---- batch: 100 ----
mean loss: 381.74
 ---- batch: 110 ----
mean loss: 389.41
train mean loss: 393.89
epoch train time: 0:00:02.162620
elapsed time: 0:05:48.550534
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 00:32:37.836593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.15
 ---- batch: 020 ----
mean loss: 387.50
 ---- batch: 030 ----
mean loss: 384.97
 ---- batch: 040 ----
mean loss: 394.38
 ---- batch: 050 ----
mean loss: 384.53
 ---- batch: 060 ----
mean loss: 395.75
 ---- batch: 070 ----
mean loss: 386.02
 ---- batch: 080 ----
mean loss: 403.01
 ---- batch: 090 ----
mean loss: 390.10
 ---- batch: 100 ----
mean loss: 388.92
 ---- batch: 110 ----
mean loss: 381.69
train mean loss: 392.27
epoch train time: 0:00:02.159551
elapsed time: 0:05:50.710236
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 00:32:39.996314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.79
 ---- batch: 020 ----
mean loss: 387.55
 ---- batch: 030 ----
mean loss: 377.48
 ---- batch: 040 ----
mean loss: 386.07
 ---- batch: 050 ----
mean loss: 385.69
 ---- batch: 060 ----
mean loss: 372.94
 ---- batch: 070 ----
mean loss: 392.42
 ---- batch: 080 ----
mean loss: 387.61
 ---- batch: 090 ----
mean loss: 402.48
 ---- batch: 100 ----
mean loss: 390.11
 ---- batch: 110 ----
mean loss: 373.90
train mean loss: 384.84
epoch train time: 0:00:02.164480
elapsed time: 0:05:52.874925
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 00:32:42.160988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.17
 ---- batch: 020 ----
mean loss: 393.33
 ---- batch: 030 ----
mean loss: 387.27
 ---- batch: 040 ----
mean loss: 383.38
 ---- batch: 050 ----
mean loss: 393.07
 ---- batch: 060 ----
mean loss: 382.84
 ---- batch: 070 ----
mean loss: 389.34
 ---- batch: 080 ----
mean loss: 385.65
 ---- batch: 090 ----
mean loss: 383.33
 ---- batch: 100 ----
mean loss: 377.78
 ---- batch: 110 ----
mean loss: 372.60
train mean loss: 382.98
epoch train time: 0:00:02.164585
elapsed time: 0:05:55.039724
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 00:32:44.325789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.37
 ---- batch: 020 ----
mean loss: 380.49
 ---- batch: 030 ----
mean loss: 379.85
 ---- batch: 040 ----
mean loss: 358.01
 ---- batch: 050 ----
mean loss: 363.73
 ---- batch: 060 ----
mean loss: 379.63
 ---- batch: 070 ----
mean loss: 389.03
 ---- batch: 080 ----
mean loss: 395.67
 ---- batch: 090 ----
mean loss: 389.86
 ---- batch: 100 ----
mean loss: 385.84
 ---- batch: 110 ----
mean loss: 370.18
train mean loss: 379.33
epoch train time: 0:00:02.161750
elapsed time: 0:05:57.201638
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 00:32:46.487698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.73
 ---- batch: 020 ----
mean loss: 373.01
 ---- batch: 030 ----
mean loss: 374.77
 ---- batch: 040 ----
mean loss: 380.27
 ---- batch: 050 ----
mean loss: 385.53
 ---- batch: 060 ----
mean loss: 380.39
 ---- batch: 070 ----
mean loss: 377.98
 ---- batch: 080 ----
mean loss: 379.67
 ---- batch: 090 ----
mean loss: 374.10
 ---- batch: 100 ----
mean loss: 378.37
 ---- batch: 110 ----
mean loss: 382.26
train mean loss: 378.21
epoch train time: 0:00:02.161010
elapsed time: 0:05:59.362833
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 00:32:48.648899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.63
 ---- batch: 020 ----
mean loss: 376.67
 ---- batch: 030 ----
mean loss: 380.92
 ---- batch: 040 ----
mean loss: 373.04
 ---- batch: 050 ----
mean loss: 371.71
 ---- batch: 060 ----
mean loss: 369.79
 ---- batch: 070 ----
mean loss: 383.39
 ---- batch: 080 ----
mean loss: 377.72
 ---- batch: 090 ----
mean loss: 380.53
 ---- batch: 100 ----
mean loss: 377.23
 ---- batch: 110 ----
mean loss: 366.08
train mean loss: 376.14
epoch train time: 0:00:02.179984
elapsed time: 0:06:01.542995
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 00:32:50.829062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.20
 ---- batch: 020 ----
mean loss: 378.14
 ---- batch: 030 ----
mean loss: 375.02
 ---- batch: 040 ----
mean loss: 383.65
 ---- batch: 050 ----
mean loss: 378.04
 ---- batch: 060 ----
mean loss: 371.81
 ---- batch: 070 ----
mean loss: 375.06
 ---- batch: 080 ----
mean loss: 375.10
 ---- batch: 090 ----
mean loss: 381.18
 ---- batch: 100 ----
mean loss: 370.04
 ---- batch: 110 ----
mean loss: 360.95
train mean loss: 376.98
epoch train time: 0:00:02.169542
elapsed time: 0:06:03.712724
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 00:32:52.998820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.28
 ---- batch: 020 ----
mean loss: 362.21
 ---- batch: 030 ----
mean loss: 369.54
 ---- batch: 040 ----
mean loss: 373.61
 ---- batch: 050 ----
mean loss: 381.86
 ---- batch: 060 ----
mean loss: 382.09
 ---- batch: 070 ----
mean loss: 371.26
 ---- batch: 080 ----
mean loss: 361.54
 ---- batch: 090 ----
mean loss: 375.66
 ---- batch: 100 ----
mean loss: 369.34
 ---- batch: 110 ----
mean loss: 381.36
train mean loss: 372.01
epoch train time: 0:00:02.162445
elapsed time: 0:06:05.875355
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 00:32:55.161422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.31
 ---- batch: 020 ----
mean loss: 370.38
 ---- batch: 030 ----
mean loss: 370.46
 ---- batch: 040 ----
mean loss: 365.76
 ---- batch: 050 ----
mean loss: 378.95
 ---- batch: 060 ----
mean loss: 371.19
 ---- batch: 070 ----
mean loss: 364.07
 ---- batch: 080 ----
mean loss: 369.92
 ---- batch: 090 ----
mean loss: 388.43
 ---- batch: 100 ----
mean loss: 363.24
 ---- batch: 110 ----
mean loss: 373.36
train mean loss: 371.60
epoch train time: 0:00:02.158221
elapsed time: 0:06:08.033751
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 00:32:57.319813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.02
 ---- batch: 020 ----
mean loss: 374.84
 ---- batch: 030 ----
mean loss: 362.99
 ---- batch: 040 ----
mean loss: 379.23
 ---- batch: 050 ----
mean loss: 365.75
 ---- batch: 060 ----
mean loss: 374.97
 ---- batch: 070 ----
mean loss: 379.61
 ---- batch: 080 ----
mean loss: 364.27
 ---- batch: 090 ----
mean loss: 367.31
 ---- batch: 100 ----
mean loss: 366.04
 ---- batch: 110 ----
mean loss: 371.19
train mean loss: 370.15
epoch train time: 0:00:02.158181
elapsed time: 0:06:10.192108
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 00:32:59.478169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.63
 ---- batch: 020 ----
mean loss: 359.34
 ---- batch: 030 ----
mean loss: 370.27
 ---- batch: 040 ----
mean loss: 369.84
 ---- batch: 050 ----
mean loss: 366.05
 ---- batch: 060 ----
mean loss: 372.85
 ---- batch: 070 ----
mean loss: 369.36
 ---- batch: 080 ----
mean loss: 377.17
 ---- batch: 090 ----
mean loss: 353.25
 ---- batch: 100 ----
mean loss: 360.57
 ---- batch: 110 ----
mean loss: 358.55
train mean loss: 366.70
epoch train time: 0:00:02.172643
elapsed time: 0:06:12.364943
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 00:33:01.651008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.57
 ---- batch: 020 ----
mean loss: 368.06
 ---- batch: 030 ----
mean loss: 375.59
 ---- batch: 040 ----
mean loss: 357.17
 ---- batch: 050 ----
mean loss: 362.97
 ---- batch: 060 ----
mean loss: 356.86
 ---- batch: 070 ----
mean loss: 349.34
 ---- batch: 080 ----
mean loss: 365.96
 ---- batch: 090 ----
mean loss: 369.92
 ---- batch: 100 ----
mean loss: 373.87
 ---- batch: 110 ----
mean loss: 365.86
train mean loss: 365.25
epoch train time: 0:00:02.171624
elapsed time: 0:06:14.536770
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 00:33:03.822834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.12
 ---- batch: 020 ----
mean loss: 370.59
 ---- batch: 030 ----
mean loss: 362.04
 ---- batch: 040 ----
mean loss: 353.92
 ---- batch: 050 ----
mean loss: 367.44
 ---- batch: 060 ----
mean loss: 364.60
 ---- batch: 070 ----
mean loss: 366.03
 ---- batch: 080 ----
mean loss: 367.23
 ---- batch: 090 ----
mean loss: 349.97
 ---- batch: 100 ----
mean loss: 362.66
 ---- batch: 110 ----
mean loss: 361.19
train mean loss: 362.87
epoch train time: 0:00:02.184340
elapsed time: 0:06:16.721301
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 00:33:06.007362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.30
 ---- batch: 020 ----
mean loss: 359.15
 ---- batch: 030 ----
mean loss: 356.50
 ---- batch: 040 ----
mean loss: 373.96
 ---- batch: 050 ----
mean loss: 354.74
 ---- batch: 060 ----
mean loss: 354.86
 ---- batch: 070 ----
mean loss: 363.16
 ---- batch: 080 ----
mean loss: 360.45
 ---- batch: 090 ----
mean loss: 354.27
 ---- batch: 100 ----
mean loss: 378.63
 ---- batch: 110 ----
mean loss: 370.64
train mean loss: 362.50
epoch train time: 0:00:02.171943
elapsed time: 0:06:18.893460
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 00:33:08.179546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.01
 ---- batch: 020 ----
mean loss: 356.76
 ---- batch: 030 ----
mean loss: 345.76
 ---- batch: 040 ----
mean loss: 362.13
 ---- batch: 050 ----
mean loss: 372.26
 ---- batch: 060 ----
mean loss: 355.57
 ---- batch: 070 ----
mean loss: 350.69
 ---- batch: 080 ----
mean loss: 366.42
 ---- batch: 090 ----
mean loss: 358.47
 ---- batch: 100 ----
mean loss: 345.51
 ---- batch: 110 ----
mean loss: 341.68
train mean loss: 356.34
epoch train time: 0:00:02.182973
elapsed time: 0:06:21.076655
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 00:33:10.362767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.82
 ---- batch: 020 ----
mean loss: 357.35
 ---- batch: 030 ----
mean loss: 349.76
 ---- batch: 040 ----
mean loss: 350.15
 ---- batch: 050 ----
mean loss: 369.91
 ---- batch: 060 ----
mean loss: 364.17
 ---- batch: 070 ----
mean loss: 349.22
 ---- batch: 080 ----
mean loss: 348.15
 ---- batch: 090 ----
mean loss: 363.06
 ---- batch: 100 ----
mean loss: 361.42
 ---- batch: 110 ----
mean loss: 351.06
train mean loss: 355.73
epoch train time: 0:00:02.180396
elapsed time: 0:06:23.257259
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 00:33:12.543323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.85
 ---- batch: 020 ----
mean loss: 357.38
 ---- batch: 030 ----
mean loss: 363.50
 ---- batch: 040 ----
mean loss: 342.19
 ---- batch: 050 ----
mean loss: 362.97
 ---- batch: 060 ----
mean loss: 344.28
 ---- batch: 070 ----
mean loss: 346.39
 ---- batch: 080 ----
mean loss: 335.81
 ---- batch: 090 ----
mean loss: 351.63
 ---- batch: 100 ----
mean loss: 361.03
 ---- batch: 110 ----
mean loss: 356.22
train mean loss: 352.28
epoch train time: 0:00:02.177887
elapsed time: 0:06:25.435344
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 00:33:14.721432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.22
 ---- batch: 020 ----
mean loss: 350.83
 ---- batch: 030 ----
mean loss: 352.64
 ---- batch: 040 ----
mean loss: 352.40
 ---- batch: 050 ----
mean loss: 346.82
 ---- batch: 060 ----
mean loss: 348.19
 ---- batch: 070 ----
mean loss: 351.68
 ---- batch: 080 ----
mean loss: 354.38
 ---- batch: 090 ----
mean loss: 345.19
 ---- batch: 100 ----
mean loss: 341.58
 ---- batch: 110 ----
mean loss: 354.66
train mean loss: 350.67
epoch train time: 0:00:02.179098
elapsed time: 0:06:27.614663
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 00:33:16.900730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.33
 ---- batch: 020 ----
mean loss: 350.07
 ---- batch: 030 ----
mean loss: 353.09
 ---- batch: 040 ----
mean loss: 354.70
 ---- batch: 050 ----
mean loss: 344.73
 ---- batch: 060 ----
mean loss: 341.62
 ---- batch: 070 ----
mean loss: 351.16
 ---- batch: 080 ----
mean loss: 335.28
 ---- batch: 090 ----
mean loss: 345.89
 ---- batch: 100 ----
mean loss: 346.80
 ---- batch: 110 ----
mean loss: 347.92
train mean loss: 346.42
epoch train time: 0:00:02.181965
elapsed time: 0:06:29.796815
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 00:33:19.082875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.85
 ---- batch: 020 ----
mean loss: 342.89
 ---- batch: 030 ----
mean loss: 350.35
 ---- batch: 040 ----
mean loss: 346.41
 ---- batch: 050 ----
mean loss: 356.14
 ---- batch: 060 ----
mean loss: 345.43
 ---- batch: 070 ----
mean loss: 336.44
 ---- batch: 080 ----
mean loss: 344.33
 ---- batch: 090 ----
mean loss: 352.59
 ---- batch: 100 ----
mean loss: 331.60
 ---- batch: 110 ----
mean loss: 345.64
train mean loss: 346.73
epoch train time: 0:00:02.179474
elapsed time: 0:06:31.976468
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 00:33:21.262555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.47
 ---- batch: 020 ----
mean loss: 338.46
 ---- batch: 030 ----
mean loss: 335.53
 ---- batch: 040 ----
mean loss: 340.94
 ---- batch: 050 ----
mean loss: 341.35
 ---- batch: 060 ----
mean loss: 341.78
 ---- batch: 070 ----
mean loss: 329.84
 ---- batch: 080 ----
mean loss: 345.91
 ---- batch: 090 ----
mean loss: 346.50
 ---- batch: 100 ----
mean loss: 361.01
 ---- batch: 110 ----
mean loss: 336.40
train mean loss: 342.66
epoch train time: 0:00:02.178670
elapsed time: 0:06:34.155339
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 00:33:23.441403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.49
 ---- batch: 020 ----
mean loss: 336.16
 ---- batch: 030 ----
mean loss: 332.04
 ---- batch: 040 ----
mean loss: 323.90
 ---- batch: 050 ----
mean loss: 337.08
 ---- batch: 060 ----
mean loss: 330.69
 ---- batch: 070 ----
mean loss: 346.34
 ---- batch: 080 ----
mean loss: 334.90
 ---- batch: 090 ----
mean loss: 351.76
 ---- batch: 100 ----
mean loss: 341.62
 ---- batch: 110 ----
mean loss: 343.42
train mean loss: 338.71
epoch train time: 0:00:02.182425
elapsed time: 0:06:36.337949
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 00:33:25.624015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.76
 ---- batch: 020 ----
mean loss: 338.08
 ---- batch: 030 ----
mean loss: 335.81
 ---- batch: 040 ----
mean loss: 346.21
 ---- batch: 050 ----
mean loss: 344.44
 ---- batch: 060 ----
mean loss: 334.14
 ---- batch: 070 ----
mean loss: 329.37
 ---- batch: 080 ----
mean loss: 339.27
 ---- batch: 090 ----
mean loss: 323.22
 ---- batch: 100 ----
mean loss: 336.64
 ---- batch: 110 ----
mean loss: 325.58
train mean loss: 335.35
epoch train time: 0:00:02.185203
elapsed time: 0:06:38.523339
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 00:33:27.809405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.91
 ---- batch: 020 ----
mean loss: 332.53
 ---- batch: 030 ----
mean loss: 329.44
 ---- batch: 040 ----
mean loss: 339.66
 ---- batch: 050 ----
mean loss: 338.99
 ---- batch: 060 ----
mean loss: 337.86
 ---- batch: 070 ----
mean loss: 330.39
 ---- batch: 080 ----
mean loss: 338.98
 ---- batch: 090 ----
mean loss: 337.12
 ---- batch: 100 ----
mean loss: 333.23
 ---- batch: 110 ----
mean loss: 330.56
train mean loss: 333.56
epoch train time: 0:00:02.173213
elapsed time: 0:06:40.696728
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 00:33:29.982790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.62
 ---- batch: 020 ----
mean loss: 338.46
 ---- batch: 030 ----
mean loss: 336.72
 ---- batch: 040 ----
mean loss: 327.63
 ---- batch: 050 ----
mean loss: 328.78
 ---- batch: 060 ----
mean loss: 346.74
 ---- batch: 070 ----
mean loss: 323.04
 ---- batch: 080 ----
mean loss: 332.18
 ---- batch: 090 ----
mean loss: 337.62
 ---- batch: 100 ----
mean loss: 323.89
 ---- batch: 110 ----
mean loss: 324.46
train mean loss: 332.46
epoch train time: 0:00:02.181382
elapsed time: 0:06:42.878304
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 00:33:32.164383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.55
 ---- batch: 020 ----
mean loss: 329.42
 ---- batch: 030 ----
mean loss: 328.13
 ---- batch: 040 ----
mean loss: 323.09
 ---- batch: 050 ----
mean loss: 326.85
 ---- batch: 060 ----
mean loss: 323.67
 ---- batch: 070 ----
mean loss: 339.17
 ---- batch: 080 ----
mean loss: 333.98
 ---- batch: 090 ----
mean loss: 332.97
 ---- batch: 100 ----
mean loss: 315.28
 ---- batch: 110 ----
mean loss: 317.28
train mean loss: 325.77
epoch train time: 0:00:02.181750
elapsed time: 0:06:45.060249
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 00:33:34.346311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.30
 ---- batch: 020 ----
mean loss: 330.95
 ---- batch: 030 ----
mean loss: 324.51
 ---- batch: 040 ----
mean loss: 324.99
 ---- batch: 050 ----
mean loss: 329.34
 ---- batch: 060 ----
mean loss: 336.79
 ---- batch: 070 ----
mean loss: 314.89
 ---- batch: 080 ----
mean loss: 325.54
 ---- batch: 090 ----
mean loss: 330.71
 ---- batch: 100 ----
mean loss: 328.08
 ---- batch: 110 ----
mean loss: 331.59
train mean loss: 327.43
epoch train time: 0:00:02.180547
elapsed time: 0:06:47.240988
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 00:33:36.527083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.66
 ---- batch: 020 ----
mean loss: 325.78
 ---- batch: 030 ----
mean loss: 337.99
 ---- batch: 040 ----
mean loss: 320.13
 ---- batch: 050 ----
mean loss: 310.73
 ---- batch: 060 ----
mean loss: 319.34
 ---- batch: 070 ----
mean loss: 322.72
 ---- batch: 080 ----
mean loss: 322.88
 ---- batch: 090 ----
mean loss: 335.66
 ---- batch: 100 ----
mean loss: 309.69
 ---- batch: 110 ----
mean loss: 319.26
train mean loss: 321.78
epoch train time: 0:00:02.177678
elapsed time: 0:06:49.418890
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 00:33:38.704945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.10
 ---- batch: 020 ----
mean loss: 326.00
 ---- batch: 030 ----
mean loss: 324.52
 ---- batch: 040 ----
mean loss: 326.27
 ---- batch: 050 ----
mean loss: 320.75
 ---- batch: 060 ----
mean loss: 311.92
 ---- batch: 070 ----
mean loss: 319.70
 ---- batch: 080 ----
mean loss: 310.82
 ---- batch: 090 ----
mean loss: 317.17
 ---- batch: 100 ----
mean loss: 317.55
 ---- batch: 110 ----
mean loss: 320.23
train mean loss: 319.08
epoch train time: 0:00:02.183580
elapsed time: 0:06:51.602652
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 00:33:40.888715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.55
 ---- batch: 020 ----
mean loss: 309.53
 ---- batch: 030 ----
mean loss: 318.15
 ---- batch: 040 ----
mean loss: 314.78
 ---- batch: 050 ----
mean loss: 313.08
 ---- batch: 060 ----
mean loss: 316.40
 ---- batch: 070 ----
mean loss: 322.41
 ---- batch: 080 ----
mean loss: 311.08
 ---- batch: 090 ----
mean loss: 325.46
 ---- batch: 100 ----
mean loss: 311.28
 ---- batch: 110 ----
mean loss: 314.25
train mean loss: 316.87
epoch train time: 0:00:02.183481
elapsed time: 0:06:53.786307
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 00:33:43.072372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.26
 ---- batch: 020 ----
mean loss: 319.64
 ---- batch: 030 ----
mean loss: 307.13
 ---- batch: 040 ----
mean loss: 324.21
 ---- batch: 050 ----
mean loss: 317.78
 ---- batch: 060 ----
mean loss: 314.80
 ---- batch: 070 ----
mean loss: 319.96
 ---- batch: 080 ----
mean loss: 314.51
 ---- batch: 090 ----
mean loss: 306.54
 ---- batch: 100 ----
mean loss: 313.54
 ---- batch: 110 ----
mean loss: 319.20
train mean loss: 315.92
epoch train time: 0:00:02.194480
elapsed time: 0:06:55.980963
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 00:33:45.267024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.71
 ---- batch: 020 ----
mean loss: 313.24
 ---- batch: 030 ----
mean loss: 312.78
 ---- batch: 040 ----
mean loss: 301.40
 ---- batch: 050 ----
mean loss: 309.07
 ---- batch: 060 ----
mean loss: 310.17
 ---- batch: 070 ----
mean loss: 303.76
 ---- batch: 080 ----
mean loss: 314.67
 ---- batch: 090 ----
mean loss: 312.46
 ---- batch: 100 ----
mean loss: 308.38
 ---- batch: 110 ----
mean loss: 299.06
train mean loss: 309.97
epoch train time: 0:00:02.195323
elapsed time: 0:06:58.176457
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 00:33:47.462528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.90
 ---- batch: 020 ----
mean loss: 319.69
 ---- batch: 030 ----
mean loss: 314.30
 ---- batch: 040 ----
mean loss: 300.97
 ---- batch: 050 ----
mean loss: 304.22
 ---- batch: 060 ----
mean loss: 309.51
 ---- batch: 070 ----
mean loss: 304.72
 ---- batch: 080 ----
mean loss: 299.89
 ---- batch: 090 ----
mean loss: 308.12
 ---- batch: 100 ----
mean loss: 316.91
 ---- batch: 110 ----
mean loss: 305.37
train mean loss: 308.62
epoch train time: 0:00:02.187219
elapsed time: 0:07:00.363858
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 00:33:49.649922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.94
 ---- batch: 020 ----
mean loss: 294.46
 ---- batch: 030 ----
mean loss: 310.04
 ---- batch: 040 ----
mean loss: 310.32
 ---- batch: 050 ----
mean loss: 305.75
 ---- batch: 060 ----
mean loss: 297.67
 ---- batch: 070 ----
mean loss: 310.14
 ---- batch: 080 ----
mean loss: 308.98
 ---- batch: 090 ----
mean loss: 310.54
 ---- batch: 100 ----
mean loss: 300.47
 ---- batch: 110 ----
mean loss: 314.09
train mean loss: 305.79
epoch train time: 0:00:02.196042
elapsed time: 0:07:02.560088
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 00:33:51.846170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.30
 ---- batch: 020 ----
mean loss: 312.25
 ---- batch: 030 ----
mean loss: 295.32
 ---- batch: 040 ----
mean loss: 307.94
 ---- batch: 050 ----
mean loss: 316.04
 ---- batch: 060 ----
mean loss: 306.55
 ---- batch: 070 ----
mean loss: 299.18
 ---- batch: 080 ----
mean loss: 316.41
 ---- batch: 090 ----
mean loss: 304.40
 ---- batch: 100 ----
mean loss: 304.70
 ---- batch: 110 ----
mean loss: 310.10
train mean loss: 307.22
epoch train time: 0:00:02.183075
elapsed time: 0:07:04.743351
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 00:33:54.029434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.98
 ---- batch: 020 ----
mean loss: 303.05
 ---- batch: 030 ----
mean loss: 310.34
 ---- batch: 040 ----
mean loss: 297.70
 ---- batch: 050 ----
mean loss: 308.28
 ---- batch: 060 ----
mean loss: 295.37
 ---- batch: 070 ----
mean loss: 307.15
 ---- batch: 080 ----
mean loss: 302.66
 ---- batch: 090 ----
mean loss: 297.34
 ---- batch: 100 ----
mean loss: 313.50
 ---- batch: 110 ----
mean loss: 307.00
train mean loss: 304.47
epoch train time: 0:00:02.179068
elapsed time: 0:07:06.922597
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 00:33:56.208657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.85
 ---- batch: 020 ----
mean loss: 322.66
 ---- batch: 030 ----
mean loss: 314.59
 ---- batch: 040 ----
mean loss: 313.70
 ---- batch: 050 ----
mean loss: 303.10
 ---- batch: 060 ----
mean loss: 310.00
 ---- batch: 070 ----
mean loss: 304.38
 ---- batch: 080 ----
mean loss: 303.70
 ---- batch: 090 ----
mean loss: 308.17
 ---- batch: 100 ----
mean loss: 300.24
 ---- batch: 110 ----
mean loss: 294.55
train mean loss: 306.81
epoch train time: 0:00:02.185150
elapsed time: 0:07:09.107924
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 00:33:58.393989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.24
 ---- batch: 020 ----
mean loss: 309.73
 ---- batch: 030 ----
mean loss: 291.34
 ---- batch: 040 ----
mean loss: 303.17
 ---- batch: 050 ----
mean loss: 297.26
 ---- batch: 060 ----
mean loss: 305.17
 ---- batch: 070 ----
mean loss: 300.69
 ---- batch: 080 ----
mean loss: 299.78
 ---- batch: 090 ----
mean loss: 317.93
 ---- batch: 100 ----
mean loss: 295.97
 ---- batch: 110 ----
mean loss: 294.77
train mean loss: 301.85
epoch train time: 0:00:02.182138
elapsed time: 0:07:11.290230
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 00:34:00.576293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.58
 ---- batch: 020 ----
mean loss: 307.10
 ---- batch: 030 ----
mean loss: 305.81
 ---- batch: 040 ----
mean loss: 296.75
 ---- batch: 050 ----
mean loss: 300.80
 ---- batch: 060 ----
mean loss: 305.34
 ---- batch: 070 ----
mean loss: 300.85
 ---- batch: 080 ----
mean loss: 289.87
 ---- batch: 090 ----
mean loss: 296.22
 ---- batch: 100 ----
mean loss: 302.41
 ---- batch: 110 ----
mean loss: 305.99
train mean loss: 300.53
epoch train time: 0:00:02.175661
elapsed time: 0:07:13.466079
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 00:34:02.752142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.99
 ---- batch: 020 ----
mean loss: 288.50
 ---- batch: 030 ----
mean loss: 300.41
 ---- batch: 040 ----
mean loss: 296.02
 ---- batch: 050 ----
mean loss: 301.34
 ---- batch: 060 ----
mean loss: 297.92
 ---- batch: 070 ----
mean loss: 300.24
 ---- batch: 080 ----
mean loss: 297.51
 ---- batch: 090 ----
mean loss: 299.71
 ---- batch: 100 ----
mean loss: 293.05
 ---- batch: 110 ----
mean loss: 304.41
train mean loss: 298.21
epoch train time: 0:00:02.187663
elapsed time: 0:07:15.653917
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 00:34:04.939995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.19
 ---- batch: 020 ----
mean loss: 303.20
 ---- batch: 030 ----
mean loss: 317.53
 ---- batch: 040 ----
mean loss: 294.87
 ---- batch: 050 ----
mean loss: 298.02
 ---- batch: 060 ----
mean loss: 300.23
 ---- batch: 070 ----
mean loss: 295.61
 ---- batch: 080 ----
mean loss: 288.54
 ---- batch: 090 ----
mean loss: 281.97
 ---- batch: 100 ----
mean loss: 295.17
 ---- batch: 110 ----
mean loss: 296.54
train mean loss: 297.89
epoch train time: 0:00:02.182118
elapsed time: 0:07:17.836266
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 00:34:07.122331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.50
 ---- batch: 020 ----
mean loss: 294.32
 ---- batch: 030 ----
mean loss: 293.88
 ---- batch: 040 ----
mean loss: 291.79
 ---- batch: 050 ----
mean loss: 302.11
 ---- batch: 060 ----
mean loss: 299.65
 ---- batch: 070 ----
mean loss: 298.11
 ---- batch: 080 ----
mean loss: 295.34
 ---- batch: 090 ----
mean loss: 299.46
 ---- batch: 100 ----
mean loss: 295.10
 ---- batch: 110 ----
mean loss: 293.51
train mean loss: 296.01
epoch train time: 0:00:02.174596
elapsed time: 0:07:20.011034
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 00:34:09.297124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.48
 ---- batch: 020 ----
mean loss: 301.67
 ---- batch: 030 ----
mean loss: 296.58
 ---- batch: 040 ----
mean loss: 280.77
 ---- batch: 050 ----
mean loss: 286.98
 ---- batch: 060 ----
mean loss: 289.89
 ---- batch: 070 ----
mean loss: 292.23
 ---- batch: 080 ----
mean loss: 281.65
 ---- batch: 090 ----
mean loss: 305.06
 ---- batch: 100 ----
mean loss: 292.05
 ---- batch: 110 ----
mean loss: 290.31
train mean loss: 291.62
epoch train time: 0:00:02.162730
elapsed time: 0:07:22.173962
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 00:34:11.460021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.10
 ---- batch: 020 ----
mean loss: 295.60
 ---- batch: 030 ----
mean loss: 309.74
 ---- batch: 040 ----
mean loss: 300.31
 ---- batch: 050 ----
mean loss: 296.72
 ---- batch: 060 ----
mean loss: 291.16
 ---- batch: 070 ----
mean loss: 280.83
 ---- batch: 080 ----
mean loss: 295.37
 ---- batch: 090 ----
mean loss: 289.81
 ---- batch: 100 ----
mean loss: 290.44
 ---- batch: 110 ----
mean loss: 281.48
train mean loss: 293.05
epoch train time: 0:00:02.156065
elapsed time: 0:07:24.330204
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 00:34:13.616264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.32
 ---- batch: 020 ----
mean loss: 289.06
 ---- batch: 030 ----
mean loss: 302.29
 ---- batch: 040 ----
mean loss: 295.67
 ---- batch: 050 ----
mean loss: 285.73
 ---- batch: 060 ----
mean loss: 306.81
 ---- batch: 070 ----
mean loss: 301.03
 ---- batch: 080 ----
mean loss: 289.22
 ---- batch: 090 ----
mean loss: 291.53
 ---- batch: 100 ----
mean loss: 290.43
 ---- batch: 110 ----
mean loss: 306.16
train mean loss: 294.34
epoch train time: 0:00:02.146338
elapsed time: 0:07:26.476690
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 00:34:15.762747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.33
 ---- batch: 020 ----
mean loss: 282.32
 ---- batch: 030 ----
mean loss: 285.68
 ---- batch: 040 ----
mean loss: 299.04
 ---- batch: 050 ----
mean loss: 295.78
 ---- batch: 060 ----
mean loss: 298.02
 ---- batch: 070 ----
mean loss: 289.70
 ---- batch: 080 ----
mean loss: 290.82
 ---- batch: 090 ----
mean loss: 294.25
 ---- batch: 100 ----
mean loss: 286.95
 ---- batch: 110 ----
mean loss: 296.09
train mean loss: 291.72
epoch train time: 0:00:02.149964
elapsed time: 0:07:28.626799
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 00:34:17.912858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.07
 ---- batch: 020 ----
mean loss: 285.43
 ---- batch: 030 ----
mean loss: 298.43
 ---- batch: 040 ----
mean loss: 297.22
 ---- batch: 050 ----
mean loss: 298.87
 ---- batch: 060 ----
mean loss: 313.31
 ---- batch: 070 ----
mean loss: 285.61
 ---- batch: 080 ----
mean loss: 296.22
 ---- batch: 090 ----
mean loss: 271.30
 ---- batch: 100 ----
mean loss: 275.58
 ---- batch: 110 ----
mean loss: 283.16
train mean loss: 291.24
epoch train time: 0:00:02.143646
elapsed time: 0:07:30.770595
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 00:34:20.056671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.11
 ---- batch: 020 ----
mean loss: 290.89
 ---- batch: 030 ----
mean loss: 289.21
 ---- batch: 040 ----
mean loss: 286.50
 ---- batch: 050 ----
mean loss: 290.18
 ---- batch: 060 ----
mean loss: 277.25
 ---- batch: 070 ----
mean loss: 275.33
 ---- batch: 080 ----
mean loss: 297.19
 ---- batch: 090 ----
mean loss: 276.52
 ---- batch: 100 ----
mean loss: 293.26
 ---- batch: 110 ----
mean loss: 293.99
train mean loss: 287.87
epoch train time: 0:00:02.141558
elapsed time: 0:07:32.912321
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 00:34:22.198381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.66
 ---- batch: 020 ----
mean loss: 309.35
 ---- batch: 030 ----
mean loss: 290.56
 ---- batch: 040 ----
mean loss: 295.52
 ---- batch: 050 ----
mean loss: 277.45
 ---- batch: 060 ----
mean loss: 281.44
 ---- batch: 070 ----
mean loss: 290.91
 ---- batch: 080 ----
mean loss: 278.72
 ---- batch: 090 ----
mean loss: 284.56
 ---- batch: 100 ----
mean loss: 297.96
 ---- batch: 110 ----
mean loss: 280.20
train mean loss: 289.30
epoch train time: 0:00:02.140171
elapsed time: 0:07:35.052645
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 00:34:24.338721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.06
 ---- batch: 020 ----
mean loss: 304.39
 ---- batch: 030 ----
mean loss: 306.65
 ---- batch: 040 ----
mean loss: 291.90
 ---- batch: 050 ----
mean loss: 283.09
 ---- batch: 060 ----
mean loss: 289.70
 ---- batch: 070 ----
mean loss: 287.47
 ---- batch: 080 ----
mean loss: 294.99
 ---- batch: 090 ----
mean loss: 295.07
 ---- batch: 100 ----
mean loss: 284.09
 ---- batch: 110 ----
mean loss: 290.70
train mean loss: 292.05
epoch train time: 0:00:02.141446
elapsed time: 0:07:37.194254
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 00:34:26.480311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.11
 ---- batch: 020 ----
mean loss: 290.57
 ---- batch: 030 ----
mean loss: 288.17
 ---- batch: 040 ----
mean loss: 298.29
 ---- batch: 050 ----
mean loss: 290.12
 ---- batch: 060 ----
mean loss: 283.40
 ---- batch: 070 ----
mean loss: 289.96
 ---- batch: 080 ----
mean loss: 286.36
 ---- batch: 090 ----
mean loss: 285.14
 ---- batch: 100 ----
mean loss: 276.78
 ---- batch: 110 ----
mean loss: 286.32
train mean loss: 287.12
epoch train time: 0:00:02.143864
elapsed time: 0:07:39.338267
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 00:34:28.624329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.19
 ---- batch: 020 ----
mean loss: 299.32
 ---- batch: 030 ----
mean loss: 290.78
 ---- batch: 040 ----
mean loss: 276.64
 ---- batch: 050 ----
mean loss: 289.83
 ---- batch: 060 ----
mean loss: 277.36
 ---- batch: 070 ----
mean loss: 294.55
 ---- batch: 080 ----
mean loss: 285.13
 ---- batch: 090 ----
mean loss: 286.66
 ---- batch: 100 ----
mean loss: 289.71
 ---- batch: 110 ----
mean loss: 284.76
train mean loss: 287.33
epoch train time: 0:00:02.141749
elapsed time: 0:07:41.480183
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 00:34:30.766241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.90
 ---- batch: 020 ----
mean loss: 283.40
 ---- batch: 030 ----
mean loss: 293.23
 ---- batch: 040 ----
mean loss: 284.73
 ---- batch: 050 ----
mean loss: 284.63
 ---- batch: 060 ----
mean loss: 293.77
 ---- batch: 070 ----
mean loss: 278.72
 ---- batch: 080 ----
mean loss: 284.52
 ---- batch: 090 ----
mean loss: 279.46
 ---- batch: 100 ----
mean loss: 280.12
 ---- batch: 110 ----
mean loss: 290.38
train mean loss: 285.48
epoch train time: 0:00:02.140042
elapsed time: 0:07:43.620373
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 00:34:32.906458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.79
 ---- batch: 020 ----
mean loss: 279.14
 ---- batch: 030 ----
mean loss: 295.19
 ---- batch: 040 ----
mean loss: 280.07
 ---- batch: 050 ----
mean loss: 280.50
 ---- batch: 060 ----
mean loss: 299.76
 ---- batch: 070 ----
mean loss: 286.29
 ---- batch: 080 ----
mean loss: 290.11
 ---- batch: 090 ----
mean loss: 286.01
 ---- batch: 100 ----
mean loss: 279.63
 ---- batch: 110 ----
mean loss: 279.91
train mean loss: 284.84
epoch train time: 0:00:02.140720
elapsed time: 0:07:45.761275
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 00:34:35.047334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.42
 ---- batch: 020 ----
mean loss: 280.13
 ---- batch: 030 ----
mean loss: 279.43
 ---- batch: 040 ----
mean loss: 287.07
 ---- batch: 050 ----
mean loss: 292.57
 ---- batch: 060 ----
mean loss: 293.87
 ---- batch: 070 ----
mean loss: 277.21
 ---- batch: 080 ----
mean loss: 284.24
 ---- batch: 090 ----
mean loss: 277.90
 ---- batch: 100 ----
mean loss: 284.59
 ---- batch: 110 ----
mean loss: 291.39
train mean loss: 284.96
epoch train time: 0:00:02.132738
elapsed time: 0:07:47.894190
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 00:34:37.180249
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.20
 ---- batch: 020 ----
mean loss: 277.20
 ---- batch: 030 ----
mean loss: 285.70
 ---- batch: 040 ----
mean loss: 276.98
 ---- batch: 050 ----
mean loss: 281.73
 ---- batch: 060 ----
mean loss: 281.62
 ---- batch: 070 ----
mean loss: 269.45
 ---- batch: 080 ----
mean loss: 277.94
 ---- batch: 090 ----
mean loss: 282.88
 ---- batch: 100 ----
mean loss: 276.02
 ---- batch: 110 ----
mean loss: 281.35
train mean loss: 279.54
epoch train time: 0:00:02.144926
elapsed time: 0:07:50.039287
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 00:34:39.325347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.43
 ---- batch: 020 ----
mean loss: 272.07
 ---- batch: 030 ----
mean loss: 273.20
 ---- batch: 040 ----
mean loss: 280.99
 ---- batch: 050 ----
mean loss: 278.44
 ---- batch: 060 ----
mean loss: 284.71
 ---- batch: 070 ----
mean loss: 274.57
 ---- batch: 080 ----
mean loss: 289.35
 ---- batch: 090 ----
mean loss: 283.82
 ---- batch: 100 ----
mean loss: 276.23
 ---- batch: 110 ----
mean loss: 278.49
train mean loss: 280.09
epoch train time: 0:00:02.159526
elapsed time: 0:07:52.198972
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 00:34:41.485038
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.92
 ---- batch: 020 ----
mean loss: 275.95
 ---- batch: 030 ----
mean loss: 280.25
 ---- batch: 040 ----
mean loss: 277.11
 ---- batch: 050 ----
mean loss: 287.30
 ---- batch: 060 ----
mean loss: 281.37
 ---- batch: 070 ----
mean loss: 275.02
 ---- batch: 080 ----
mean loss: 283.57
 ---- batch: 090 ----
mean loss: 269.10
 ---- batch: 100 ----
mean loss: 276.64
 ---- batch: 110 ----
mean loss: 276.61
train mean loss: 278.12
epoch train time: 0:00:02.174619
elapsed time: 0:07:54.373780
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 00:34:43.659858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.52
 ---- batch: 020 ----
mean loss: 277.04
 ---- batch: 030 ----
mean loss: 275.86
 ---- batch: 040 ----
mean loss: 276.82
 ---- batch: 050 ----
mean loss: 281.00
 ---- batch: 060 ----
mean loss: 282.03
 ---- batch: 070 ----
mean loss: 286.26
 ---- batch: 080 ----
mean loss: 284.24
 ---- batch: 090 ----
mean loss: 271.36
 ---- batch: 100 ----
mean loss: 269.43
 ---- batch: 110 ----
mean loss: 268.27
train mean loss: 277.56
epoch train time: 0:00:02.169373
elapsed time: 0:07:56.543331
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 00:34:45.829392
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.85
 ---- batch: 020 ----
mean loss: 284.59
 ---- batch: 030 ----
mean loss: 282.73
 ---- batch: 040 ----
mean loss: 268.67
 ---- batch: 050 ----
mean loss: 279.14
 ---- batch: 060 ----
mean loss: 281.49
 ---- batch: 070 ----
mean loss: 279.51
 ---- batch: 080 ----
mean loss: 278.20
 ---- batch: 090 ----
mean loss: 270.42
 ---- batch: 100 ----
mean loss: 269.05
 ---- batch: 110 ----
mean loss: 283.09
train mean loss: 278.05
epoch train time: 0:00:02.174368
elapsed time: 0:07:58.717873
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 00:34:48.003954
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.51
 ---- batch: 020 ----
mean loss: 277.79
 ---- batch: 030 ----
mean loss: 275.96
 ---- batch: 040 ----
mean loss: 274.54
 ---- batch: 050 ----
mean loss: 271.99
 ---- batch: 060 ----
mean loss: 272.96
 ---- batch: 070 ----
mean loss: 282.70
 ---- batch: 080 ----
mean loss: 284.27
 ---- batch: 090 ----
mean loss: 274.54
 ---- batch: 100 ----
mean loss: 282.05
 ---- batch: 110 ----
mean loss: 278.34
train mean loss: 277.83
epoch train time: 0:00:02.191271
elapsed time: 0:08:00.909348
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 00:34:50.195408
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.47
 ---- batch: 020 ----
mean loss: 278.64
 ---- batch: 030 ----
mean loss: 279.81
 ---- batch: 040 ----
mean loss: 283.84
 ---- batch: 050 ----
mean loss: 268.90
 ---- batch: 060 ----
mean loss: 271.98
 ---- batch: 070 ----
mean loss: 271.00
 ---- batch: 080 ----
mean loss: 280.39
 ---- batch: 090 ----
mean loss: 272.40
 ---- batch: 100 ----
mean loss: 289.72
 ---- batch: 110 ----
mean loss: 273.57
train mean loss: 277.00
epoch train time: 0:00:02.194544
elapsed time: 0:08:03.104089
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 00:34:52.390153
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.88
 ---- batch: 020 ----
mean loss: 277.44
 ---- batch: 030 ----
mean loss: 270.93
 ---- batch: 040 ----
mean loss: 281.50
 ---- batch: 050 ----
mean loss: 272.18
 ---- batch: 060 ----
mean loss: 282.62
 ---- batch: 070 ----
mean loss: 287.77
 ---- batch: 080 ----
mean loss: 273.00
 ---- batch: 090 ----
mean loss: 270.49
 ---- batch: 100 ----
mean loss: 272.78
 ---- batch: 110 ----
mean loss: 274.87
train mean loss: 276.30
epoch train time: 0:00:02.192631
elapsed time: 0:08:05.296890
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 00:34:54.582952
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 272.00
 ---- batch: 020 ----
mean loss: 265.49
 ---- batch: 030 ----
mean loss: 289.23
 ---- batch: 040 ----
mean loss: 271.79
 ---- batch: 050 ----
mean loss: 277.52
 ---- batch: 060 ----
mean loss: 277.91
 ---- batch: 070 ----
mean loss: 283.57
 ---- batch: 080 ----
mean loss: 273.70
 ---- batch: 090 ----
mean loss: 279.78
 ---- batch: 100 ----
mean loss: 273.86
 ---- batch: 110 ----
mean loss: 292.49
train mean loss: 277.85
epoch train time: 0:00:02.190088
elapsed time: 0:08:07.487184
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 00:34:56.773260
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.25
 ---- batch: 020 ----
mean loss: 271.67
 ---- batch: 030 ----
mean loss: 277.05
 ---- batch: 040 ----
mean loss: 272.75
 ---- batch: 050 ----
mean loss: 286.75
 ---- batch: 060 ----
mean loss: 285.11
 ---- batch: 070 ----
mean loss: 268.21
 ---- batch: 080 ----
mean loss: 273.79
 ---- batch: 090 ----
mean loss: 265.57
 ---- batch: 100 ----
mean loss: 281.01
 ---- batch: 110 ----
mean loss: 274.15
train mean loss: 276.77
epoch train time: 0:00:02.191575
elapsed time: 0:08:09.678942
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 00:34:58.965013
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.38
 ---- batch: 020 ----
mean loss: 277.19
 ---- batch: 030 ----
mean loss: 277.39
 ---- batch: 040 ----
mean loss: 282.47
 ---- batch: 050 ----
mean loss: 281.16
 ---- batch: 060 ----
mean loss: 279.53
 ---- batch: 070 ----
mean loss: 266.19
 ---- batch: 080 ----
mean loss: 272.30
 ---- batch: 090 ----
mean loss: 281.02
 ---- batch: 100 ----
mean loss: 272.16
 ---- batch: 110 ----
mean loss: 279.84
train mean loss: 278.01
epoch train time: 0:00:02.183689
elapsed time: 0:08:11.862802
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 00:35:01.148882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.72
 ---- batch: 020 ----
mean loss: 265.70
 ---- batch: 030 ----
mean loss: 273.91
 ---- batch: 040 ----
mean loss: 280.22
 ---- batch: 050 ----
mean loss: 283.44
 ---- batch: 060 ----
mean loss: 272.05
 ---- batch: 070 ----
mean loss: 272.82
 ---- batch: 080 ----
mean loss: 274.50
 ---- batch: 090 ----
mean loss: 270.59
 ---- batch: 100 ----
mean loss: 276.71
 ---- batch: 110 ----
mean loss: 272.25
train mean loss: 274.62
epoch train time: 0:00:02.185118
elapsed time: 0:08:14.048111
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 00:35:03.334174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 277.92
 ---- batch: 020 ----
mean loss: 269.06
 ---- batch: 030 ----
mean loss: 268.30
 ---- batch: 040 ----
mean loss: 279.26
 ---- batch: 050 ----
mean loss: 275.46
 ---- batch: 060 ----
mean loss: 279.17
 ---- batch: 070 ----
mean loss: 275.08
 ---- batch: 080 ----
mean loss: 274.62
 ---- batch: 090 ----
mean loss: 262.85
 ---- batch: 100 ----
mean loss: 283.79
 ---- batch: 110 ----
mean loss: 286.61
train mean loss: 275.74
epoch train time: 0:00:02.192092
elapsed time: 0:08:16.240432
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 00:35:05.526499
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 274.32
 ---- batch: 020 ----
mean loss: 281.23
 ---- batch: 030 ----
mean loss: 278.85
 ---- batch: 040 ----
mean loss: 271.88
 ---- batch: 050 ----
mean loss: 277.91
 ---- batch: 060 ----
mean loss: 271.36
 ---- batch: 070 ----
mean loss: 292.19
 ---- batch: 080 ----
mean loss: 280.11
 ---- batch: 090 ----
mean loss: 270.41
 ---- batch: 100 ----
mean loss: 269.62
 ---- batch: 110 ----
mean loss: 285.26
train mean loss: 277.83
epoch train time: 0:00:02.193662
elapsed time: 0:08:18.434322
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 00:35:07.720390
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.81
 ---- batch: 020 ----
mean loss: 284.47
 ---- batch: 030 ----
mean loss: 283.45
 ---- batch: 040 ----
mean loss: 281.79
 ---- batch: 050 ----
mean loss: 275.86
 ---- batch: 060 ----
mean loss: 273.16
 ---- batch: 070 ----
mean loss: 277.97
 ---- batch: 080 ----
mean loss: 274.14
 ---- batch: 090 ----
mean loss: 259.20
 ---- batch: 100 ----
mean loss: 265.56
 ---- batch: 110 ----
mean loss: 278.60
train mean loss: 276.12
epoch train time: 0:00:02.190253
elapsed time: 0:08:20.624746
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 00:35:09.910809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 272.50
 ---- batch: 020 ----
mean loss: 273.78
 ---- batch: 030 ----
mean loss: 277.99
 ---- batch: 040 ----
mean loss: 281.37
 ---- batch: 050 ----
mean loss: 276.42
 ---- batch: 060 ----
mean loss: 274.02
 ---- batch: 070 ----
mean loss: 270.62
 ---- batch: 080 ----
mean loss: 283.64
 ---- batch: 090 ----
mean loss: 282.34
 ---- batch: 100 ----
mean loss: 285.47
 ---- batch: 110 ----
mean loss: 266.02
train mean loss: 276.71
epoch train time: 0:00:02.191478
elapsed time: 0:08:22.816417
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 00:35:12.102480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 273.90
 ---- batch: 020 ----
mean loss: 268.83
 ---- batch: 030 ----
mean loss: 280.34
 ---- batch: 040 ----
mean loss: 285.34
 ---- batch: 050 ----
mean loss: 279.12
 ---- batch: 060 ----
mean loss: 274.63
 ---- batch: 070 ----
mean loss: 265.30
 ---- batch: 080 ----
mean loss: 280.46
 ---- batch: 090 ----
mean loss: 271.32
 ---- batch: 100 ----
mean loss: 273.84
 ---- batch: 110 ----
mean loss: 276.58
train mean loss: 274.82
epoch train time: 0:00:02.180659
elapsed time: 0:08:24.997273
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 00:35:14.283338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 277.78
 ---- batch: 020 ----
mean loss: 278.01
 ---- batch: 030 ----
mean loss: 277.64
 ---- batch: 040 ----
mean loss: 285.20
 ---- batch: 050 ----
mean loss: 275.45
 ---- batch: 060 ----
mean loss: 282.00
 ---- batch: 070 ----
mean loss: 271.59
 ---- batch: 080 ----
mean loss: 275.23
 ---- batch: 090 ----
mean loss: 272.40
 ---- batch: 100 ----
mean loss: 275.05
 ---- batch: 110 ----
mean loss: 275.92
train mean loss: 276.70
epoch train time: 0:00:02.192616
elapsed time: 0:08:27.190073
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 00:35:16.476154
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 270.53
 ---- batch: 020 ----
mean loss: 274.53
 ---- batch: 030 ----
mean loss: 276.69
 ---- batch: 040 ----
mean loss: 274.77
 ---- batch: 050 ----
mean loss: 279.12
 ---- batch: 060 ----
mean loss: 271.35
 ---- batch: 070 ----
mean loss: 274.63
 ---- batch: 080 ----
mean loss: 289.12
 ---- batch: 090 ----
mean loss: 274.56
 ---- batch: 100 ----
mean loss: 269.33
 ---- batch: 110 ----
mean loss: 271.93
train mean loss: 275.24
epoch train time: 0:00:02.190453
elapsed time: 0:08:29.380729
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 00:35:18.666803
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 274.26
 ---- batch: 020 ----
mean loss: 277.13
 ---- batch: 030 ----
mean loss: 279.69
 ---- batch: 040 ----
mean loss: 274.89
 ---- batch: 050 ----
mean loss: 288.20
 ---- batch: 060 ----
mean loss: 266.21
 ---- batch: 070 ----
mean loss: 272.45
 ---- batch: 080 ----
mean loss: 276.44
 ---- batch: 090 ----
mean loss: 272.53
 ---- batch: 100 ----
mean loss: 278.83
 ---- batch: 110 ----
mean loss: 274.24
train mean loss: 275.42
epoch train time: 0:00:02.191102
elapsed time: 0:08:31.572004
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 00:35:20.858065
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 269.79
 ---- batch: 020 ----
mean loss: 276.52
 ---- batch: 030 ----
mean loss: 280.75
 ---- batch: 040 ----
mean loss: 281.90
 ---- batch: 050 ----
mean loss: 273.40
 ---- batch: 060 ----
mean loss: 269.28
 ---- batch: 070 ----
mean loss: 276.99
 ---- batch: 080 ----
mean loss: 283.69
 ---- batch: 090 ----
mean loss: 272.75
 ---- batch: 100 ----
mean loss: 271.45
 ---- batch: 110 ----
mean loss: 279.98
train mean loss: 275.68
epoch train time: 0:00:02.187813
elapsed time: 0:08:33.760014
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 00:35:23.046085
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.52
 ---- batch: 020 ----
mean loss: 280.79
 ---- batch: 030 ----
mean loss: 284.81
 ---- batch: 040 ----
mean loss: 278.33
 ---- batch: 050 ----
mean loss: 281.05
 ---- batch: 060 ----
mean loss: 276.17
 ---- batch: 070 ----
mean loss: 271.36
 ---- batch: 080 ----
mean loss: 269.42
 ---- batch: 090 ----
mean loss: 273.14
 ---- batch: 100 ----
mean loss: 279.10
 ---- batch: 110 ----
mean loss: 273.32
train mean loss: 277.12
epoch train time: 0:00:02.188452
elapsed time: 0:08:35.948681
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 00:35:25.234743
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 274.62
 ---- batch: 020 ----
mean loss: 272.48
 ---- batch: 030 ----
mean loss: 273.23
 ---- batch: 040 ----
mean loss: 276.48
 ---- batch: 050 ----
mean loss: 281.52
 ---- batch: 060 ----
mean loss: 272.26
 ---- batch: 070 ----
mean loss: 278.93
 ---- batch: 080 ----
mean loss: 275.07
 ---- batch: 090 ----
mean loss: 271.11
 ---- batch: 100 ----
mean loss: 287.77
 ---- batch: 110 ----
mean loss: 271.05
train mean loss: 275.74
epoch train time: 0:00:02.189774
elapsed time: 0:08:38.138634
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 00:35:27.424698
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 270.69
 ---- batch: 020 ----
mean loss: 277.31
 ---- batch: 030 ----
mean loss: 272.92
 ---- batch: 040 ----
mean loss: 276.94
 ---- batch: 050 ----
mean loss: 277.81
 ---- batch: 060 ----
mean loss: 288.11
 ---- batch: 070 ----
mean loss: 273.47
 ---- batch: 080 ----
mean loss: 280.80
 ---- batch: 090 ----
mean loss: 281.89
 ---- batch: 100 ----
mean loss: 270.10
 ---- batch: 110 ----
mean loss: 271.26
train mean loss: 276.71
epoch train time: 0:00:02.186522
elapsed time: 0:08:40.325370
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 00:35:29.611439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.31
 ---- batch: 020 ----
mean loss: 272.91
 ---- batch: 030 ----
mean loss: 283.95
 ---- batch: 040 ----
mean loss: 283.61
 ---- batch: 050 ----
mean loss: 269.24
 ---- batch: 060 ----
mean loss: 285.48
 ---- batch: 070 ----
mean loss: 270.86
 ---- batch: 080 ----
mean loss: 268.73
 ---- batch: 090 ----
mean loss: 277.74
 ---- batch: 100 ----
mean loss: 283.09
 ---- batch: 110 ----
mean loss: 265.36
train mean loss: 276.51
epoch train time: 0:00:02.192073
elapsed time: 0:08:42.517655
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 00:35:31.803741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.22
 ---- batch: 020 ----
mean loss: 284.49
 ---- batch: 030 ----
mean loss: 274.10
 ---- batch: 040 ----
mean loss: 270.39
 ---- batch: 050 ----
mean loss: 273.99
 ---- batch: 060 ----
mean loss: 264.63
 ---- batch: 070 ----
mean loss: 285.29
 ---- batch: 080 ----
mean loss: 267.37
 ---- batch: 090 ----
mean loss: 285.86
 ---- batch: 100 ----
mean loss: 278.86
 ---- batch: 110 ----
mean loss: 267.83
train mean loss: 275.36
epoch train time: 0:00:02.176958
elapsed time: 0:08:44.694809
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 00:35:33.980899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.77
 ---- batch: 020 ----
mean loss: 275.48
 ---- batch: 030 ----
mean loss: 278.52
 ---- batch: 040 ----
mean loss: 273.15
 ---- batch: 050 ----
mean loss: 276.26
 ---- batch: 060 ----
mean loss: 277.62
 ---- batch: 070 ----
mean loss: 295.58
 ---- batch: 080 ----
mean loss: 278.98
 ---- batch: 090 ----
mean loss: 273.99
 ---- batch: 100 ----
mean loss: 277.83
 ---- batch: 110 ----
mean loss: 269.56
train mean loss: 276.27
epoch train time: 0:00:02.163473
elapsed time: 0:08:46.858480
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 00:35:36.144543
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.26
 ---- batch: 020 ----
mean loss: 271.46
 ---- batch: 030 ----
mean loss: 269.08
 ---- batch: 040 ----
mean loss: 277.73
 ---- batch: 050 ----
mean loss: 277.58
 ---- batch: 060 ----
mean loss: 288.20
 ---- batch: 070 ----
mean loss: 282.40
 ---- batch: 080 ----
mean loss: 274.99
 ---- batch: 090 ----
mean loss: 279.47
 ---- batch: 100 ----
mean loss: 264.28
 ---- batch: 110 ----
mean loss: 273.36
train mean loss: 275.92
epoch train time: 0:00:02.169774
elapsed time: 0:08:49.028434
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 00:35:38.314506
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.89
 ---- batch: 020 ----
mean loss: 288.27
 ---- batch: 030 ----
mean loss: 282.27
 ---- batch: 040 ----
mean loss: 275.81
 ---- batch: 050 ----
mean loss: 274.75
 ---- batch: 060 ----
mean loss: 278.68
 ---- batch: 070 ----
mean loss: 277.84
 ---- batch: 080 ----
mean loss: 264.66
 ---- batch: 090 ----
mean loss: 274.01
 ---- batch: 100 ----
mean loss: 277.62
 ---- batch: 110 ----
mean loss: 262.29
train mean loss: 274.74
epoch train time: 0:00:02.168355
elapsed time: 0:08:51.196986
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 00:35:40.483051
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 271.27
 ---- batch: 020 ----
mean loss: 269.90
 ---- batch: 030 ----
mean loss: 266.98
 ---- batch: 040 ----
mean loss: 293.61
 ---- batch: 050 ----
mean loss: 268.85
 ---- batch: 060 ----
mean loss: 264.61
 ---- batch: 070 ----
mean loss: 278.37
 ---- batch: 080 ----
mean loss: 276.32
 ---- batch: 090 ----
mean loss: 273.58
 ---- batch: 100 ----
mean loss: 272.10
 ---- batch: 110 ----
mean loss: 280.39
train mean loss: 274.71
epoch train time: 0:00:02.169200
elapsed time: 0:08:53.366366
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 00:35:42.652426
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 269.48
 ---- batch: 020 ----
mean loss: 264.91
 ---- batch: 030 ----
mean loss: 259.90
 ---- batch: 040 ----
mean loss: 278.99
 ---- batch: 050 ----
mean loss: 280.00
 ---- batch: 060 ----
mean loss: 276.90
 ---- batch: 070 ----
mean loss: 276.62
 ---- batch: 080 ----
mean loss: 280.40
 ---- batch: 090 ----
mean loss: 277.40
 ---- batch: 100 ----
mean loss: 277.96
 ---- batch: 110 ----
mean loss: 278.06
train mean loss: 274.29
epoch train time: 0:00:02.170239
elapsed time: 0:08:55.536767
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 00:35:44.822858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.23
 ---- batch: 020 ----
mean loss: 270.16
 ---- batch: 030 ----
mean loss: 277.65
 ---- batch: 040 ----
mean loss: 278.43
 ---- batch: 050 ----
mean loss: 263.47
 ---- batch: 060 ----
mean loss: 274.34
 ---- batch: 070 ----
mean loss: 272.40
 ---- batch: 080 ----
mean loss: 277.20
 ---- batch: 090 ----
mean loss: 280.76
 ---- batch: 100 ----
mean loss: 270.29
 ---- batch: 110 ----
mean loss: 271.50
train mean loss: 273.45
epoch train time: 0:00:02.164026
elapsed time: 0:08:57.701032
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 00:35:46.987098
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 271.99
 ---- batch: 020 ----
mean loss: 261.90
 ---- batch: 030 ----
mean loss: 280.42
 ---- batch: 040 ----
mean loss: 276.87
 ---- batch: 050 ----
mean loss: 270.19
 ---- batch: 060 ----
mean loss: 278.29
 ---- batch: 070 ----
mean loss: 279.79
 ---- batch: 080 ----
mean loss: 272.21
 ---- batch: 090 ----
mean loss: 265.81
 ---- batch: 100 ----
mean loss: 281.52
 ---- batch: 110 ----
mean loss: 278.83
train mean loss: 274.09
epoch train time: 0:00:02.173404
elapsed time: 0:08:59.874660
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 00:35:49.160712
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 276.47
 ---- batch: 020 ----
mean loss: 269.19
 ---- batch: 030 ----
mean loss: 264.60
 ---- batch: 040 ----
mean loss: 276.31
 ---- batch: 050 ----
mean loss: 274.33
 ---- batch: 060 ----
mean loss: 275.95
 ---- batch: 070 ----
mean loss: 277.80
 ---- batch: 080 ----
mean loss: 277.86
 ---- batch: 090 ----
mean loss: 275.44
 ---- batch: 100 ----
mean loss: 280.24
 ---- batch: 110 ----
mean loss: 274.23
train mean loss: 274.98
epoch train time: 0:00:02.170077
elapsed time: 0:09:02.044901
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 00:35:51.330962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 279.81
 ---- batch: 020 ----
mean loss: 268.47
 ---- batch: 030 ----
mean loss: 267.55
 ---- batch: 040 ----
mean loss: 285.64
 ---- batch: 050 ----
mean loss: 273.00
 ---- batch: 060 ----
mean loss: 279.67
 ---- batch: 070 ----
mean loss: 275.07
 ---- batch: 080 ----
mean loss: 269.47
 ---- batch: 090 ----
mean loss: 272.38
 ---- batch: 100 ----
mean loss: 272.20
 ---- batch: 110 ----
mean loss: 272.28
train mean loss: 274.61
epoch train time: 0:00:02.172458
elapsed time: 0:09:04.217518
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 00:35:53.503596
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 277.11
 ---- batch: 020 ----
mean loss: 280.00
 ---- batch: 030 ----
mean loss: 277.35
 ---- batch: 040 ----
mean loss: 277.25
 ---- batch: 050 ----
mean loss: 271.77
 ---- batch: 060 ----
mean loss: 289.62
 ---- batch: 070 ----
mean loss: 282.97
 ---- batch: 080 ----
mean loss: 274.12
 ---- batch: 090 ----
mean loss: 269.97
 ---- batch: 100 ----
mean loss: 266.57
 ---- batch: 110 ----
mean loss: 275.30
train mean loss: 276.49
epoch train time: 0:00:02.161070
elapsed time: 0:09:06.378809
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 00:35:55.664886
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 273.38
 ---- batch: 020 ----
mean loss: 279.29
 ---- batch: 030 ----
mean loss: 269.41
 ---- batch: 040 ----
mean loss: 270.65
 ---- batch: 050 ----
mean loss: 271.90
 ---- batch: 060 ----
mean loss: 276.49
 ---- batch: 070 ----
mean loss: 269.52
 ---- batch: 080 ----
mean loss: 266.10
 ---- batch: 090 ----
mean loss: 274.56
 ---- batch: 100 ----
mean loss: 277.69
 ---- batch: 110 ----
mean loss: 269.10
train mean loss: 272.97
epoch train time: 0:00:02.166722
elapsed time: 0:09:08.545709
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 00:35:57.831769
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.93
 ---- batch: 020 ----
mean loss: 273.84
 ---- batch: 030 ----
mean loss: 273.35
 ---- batch: 040 ----
mean loss: 279.13
 ---- batch: 050 ----
mean loss: 270.53
 ---- batch: 060 ----
mean loss: 280.81
 ---- batch: 070 ----
mean loss: 287.40
 ---- batch: 080 ----
mean loss: 264.56
 ---- batch: 090 ----
mean loss: 259.43
 ---- batch: 100 ----
mean loss: 280.62
 ---- batch: 110 ----
mean loss: 266.42
train mean loss: 273.61
epoch train time: 0:00:02.171629
elapsed time: 0:09:10.717508
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 00:36:00.003591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 274.34
 ---- batch: 020 ----
mean loss: 274.65
 ---- batch: 030 ----
mean loss: 280.05
 ---- batch: 040 ----
mean loss: 268.37
 ---- batch: 050 ----
mean loss: 279.78
 ---- batch: 060 ----
mean loss: 279.15
 ---- batch: 070 ----
mean loss: 275.82
 ---- batch: 080 ----
mean loss: 282.28
 ---- batch: 090 ----
mean loss: 271.82
 ---- batch: 100 ----
mean loss: 270.04
 ---- batch: 110 ----
mean loss: 269.72
train mean loss: 275.18
epoch train time: 0:00:02.168294
elapsed time: 0:09:12.886000
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 00:36:02.172061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 271.29
 ---- batch: 020 ----
mean loss: 269.28
 ---- batch: 030 ----
mean loss: 270.14
 ---- batch: 040 ----
mean loss: 269.11
 ---- batch: 050 ----
mean loss: 263.52
 ---- batch: 060 ----
mean loss: 271.63
 ---- batch: 070 ----
mean loss: 271.73
 ---- batch: 080 ----
mean loss: 268.58
 ---- batch: 090 ----
mean loss: 284.49
 ---- batch: 100 ----
mean loss: 275.26
 ---- batch: 110 ----
mean loss: 279.64
train mean loss: 272.28
epoch train time: 0:00:02.177252
elapsed time: 0:09:15.063429
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 00:36:04.349491
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.42
 ---- batch: 020 ----
mean loss: 274.94
 ---- batch: 030 ----
mean loss: 283.15
 ---- batch: 040 ----
mean loss: 270.10
 ---- batch: 050 ----
mean loss: 271.70
 ---- batch: 060 ----
mean loss: 281.00
 ---- batch: 070 ----
mean loss: 272.88
 ---- batch: 080 ----
mean loss: 271.02
 ---- batch: 090 ----
mean loss: 277.11
 ---- batch: 100 ----
mean loss: 288.58
 ---- batch: 110 ----
mean loss: 271.05
train mean loss: 274.89
epoch train time: 0:00:02.175017
elapsed time: 0:09:17.238611
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 00:36:06.524674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 279.17
 ---- batch: 020 ----
mean loss: 289.79
 ---- batch: 030 ----
mean loss: 289.33
 ---- batch: 040 ----
mean loss: 265.44
 ---- batch: 050 ----
mean loss: 268.40
 ---- batch: 060 ----
mean loss: 275.25
 ---- batch: 070 ----
mean loss: 277.50
 ---- batch: 080 ----
mean loss: 257.17
 ---- batch: 090 ----
mean loss: 271.92
 ---- batch: 100 ----
mean loss: 277.25
 ---- batch: 110 ----
mean loss: 266.39
train mean loss: 274.04
epoch train time: 0:00:02.189185
elapsed time: 0:09:19.427996
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 00:36:08.714074
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 271.43
 ---- batch: 020 ----
mean loss: 287.93
 ---- batch: 030 ----
mean loss: 273.36
 ---- batch: 040 ----
mean loss: 286.02
 ---- batch: 050 ----
mean loss: 282.42
 ---- batch: 060 ----
mean loss: 274.84
 ---- batch: 070 ----
mean loss: 280.02
 ---- batch: 080 ----
mean loss: 272.57
 ---- batch: 090 ----
mean loss: 261.63
 ---- batch: 100 ----
mean loss: 276.69
 ---- batch: 110 ----
mean loss: 266.50
train mean loss: 275.37
epoch train time: 0:00:02.178572
elapsed time: 0:09:21.606750
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 00:36:10.892812
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.68
 ---- batch: 020 ----
mean loss: 270.91
 ---- batch: 030 ----
mean loss: 264.98
 ---- batch: 040 ----
mean loss: 273.92
 ---- batch: 050 ----
mean loss: 284.35
 ---- batch: 060 ----
mean loss: 268.36
 ---- batch: 070 ----
mean loss: 287.12
 ---- batch: 080 ----
mean loss: 276.01
 ---- batch: 090 ----
mean loss: 271.74
 ---- batch: 100 ----
mean loss: 273.70
 ---- batch: 110 ----
mean loss: 273.45
train mean loss: 273.52
epoch train time: 0:00:02.186662
elapsed time: 0:09:23.793590
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 00:36:13.079672
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 273.86
 ---- batch: 020 ----
mean loss: 277.12
 ---- batch: 030 ----
mean loss: 275.15
 ---- batch: 040 ----
mean loss: 274.22
 ---- batch: 050 ----
mean loss: 268.60
 ---- batch: 060 ----
mean loss: 275.32
 ---- batch: 070 ----
mean loss: 278.49
 ---- batch: 080 ----
mean loss: 270.81
 ---- batch: 090 ----
mean loss: 275.47
 ---- batch: 100 ----
mean loss: 276.06
 ---- batch: 110 ----
mean loss: 271.27
train mean loss: 273.99
epoch train time: 0:00:02.189707
elapsed time: 0:09:25.983497
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 00:36:15.269562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.37
 ---- batch: 020 ----
mean loss: 270.62
 ---- batch: 030 ----
mean loss: 272.29
 ---- batch: 040 ----
mean loss: 274.52
 ---- batch: 050 ----
mean loss: 276.47
 ---- batch: 060 ----
mean loss: 267.48
 ---- batch: 070 ----
mean loss: 271.75
 ---- batch: 080 ----
mean loss: 272.48
 ---- batch: 090 ----
mean loss: 275.24
 ---- batch: 100 ----
mean loss: 273.00
 ---- batch: 110 ----
mean loss: 276.61
train mean loss: 273.92
epoch train time: 0:00:02.188502
elapsed time: 0:09:28.172212
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 00:36:17.458271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.50
 ---- batch: 020 ----
mean loss: 280.50
 ---- batch: 030 ----
mean loss: 271.38
 ---- batch: 040 ----
mean loss: 277.21
 ---- batch: 050 ----
mean loss: 277.25
 ---- batch: 060 ----
mean loss: 278.94
 ---- batch: 070 ----
mean loss: 273.98
 ---- batch: 080 ----
mean loss: 270.47
 ---- batch: 090 ----
mean loss: 271.51
 ---- batch: 100 ----
mean loss: 279.00
 ---- batch: 110 ----
mean loss: 285.25
train mean loss: 274.97
epoch train time: 0:00:02.187959
elapsed time: 0:09:30.360334
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 00:36:19.646394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.14
 ---- batch: 020 ----
mean loss: 267.67
 ---- batch: 030 ----
mean loss: 271.13
 ---- batch: 040 ----
mean loss: 260.67
 ---- batch: 050 ----
mean loss: 272.76
 ---- batch: 060 ----
mean loss: 266.59
 ---- batch: 070 ----
mean loss: 277.69
 ---- batch: 080 ----
mean loss: 272.00
 ---- batch: 090 ----
mean loss: 273.99
 ---- batch: 100 ----
mean loss: 281.55
 ---- batch: 110 ----
mean loss: 271.59
train mean loss: 271.37
epoch train time: 0:00:02.184282
elapsed time: 0:09:32.544788
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 00:36:21.830852
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 270.10
 ---- batch: 020 ----
mean loss: 272.61
 ---- batch: 030 ----
mean loss: 269.97
 ---- batch: 040 ----
mean loss: 270.87
 ---- batch: 050 ----
mean loss: 268.21
 ---- batch: 060 ----
mean loss: 284.79
 ---- batch: 070 ----
mean loss: 268.29
 ---- batch: 080 ----
mean loss: 279.85
 ---- batch: 090 ----
mean loss: 281.59
 ---- batch: 100 ----
mean loss: 276.08
 ---- batch: 110 ----
mean loss: 270.43
train mean loss: 273.82
epoch train time: 0:00:02.175118
elapsed time: 0:09:34.723453
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_4/checkpoint.pth.tar
**** end time: 2019-09-27 00:36:24.009476 ****
