Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_9', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17696
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 02:00:05.960337 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 11, 11]             560
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
            Conv2d-4            [-1, 14, 4, 11]             224
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
            Linear-8                    [-1, 1]             308
================================================================
Total params: 1,092
Trainable params: 1,092
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:00:05.966095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4924.60
 ---- batch: 020 ----
mean loss: 4776.40
 ---- batch: 030 ----
mean loss: 4629.24
 ---- batch: 040 ----
mean loss: 4451.02
 ---- batch: 050 ----
mean loss: 4281.46
 ---- batch: 060 ----
mean loss: 4055.58
 ---- batch: 070 ----
mean loss: 3886.50
 ---- batch: 080 ----
mean loss: 3670.59
 ---- batch: 090 ----
mean loss: 3452.06
 ---- batch: 100 ----
mean loss: 3261.93
 ---- batch: 110 ----
mean loss: 3061.11
train mean loss: 4010.65
epoch train time: 0:00:32.933864
elapsed time: 0:00:32.940878
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:00:38.901257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2764.44
 ---- batch: 020 ----
mean loss: 2547.67
 ---- batch: 030 ----
mean loss: 2381.59
 ---- batch: 040 ----
mean loss: 2208.74
 ---- batch: 050 ----
mean loss: 2068.79
 ---- batch: 060 ----
mean loss: 1911.79
 ---- batch: 070 ----
mean loss: 1776.12
 ---- batch: 080 ----
mean loss: 1672.71
 ---- batch: 090 ----
mean loss: 1562.73
 ---- batch: 100 ----
mean loss: 1458.68
 ---- batch: 110 ----
mean loss: 1372.16
train mean loss: 1958.52
epoch train time: 0:00:00.708005
elapsed time: 0:00:33.649013
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:00:39.609391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1286.07
 ---- batch: 020 ----
mean loss: 1235.69
 ---- batch: 030 ----
mean loss: 1182.52
 ---- batch: 040 ----
mean loss: 1136.56
 ---- batch: 050 ----
mean loss: 1081.04
 ---- batch: 060 ----
mean loss: 1037.93
 ---- batch: 070 ----
mean loss: 1035.69
 ---- batch: 080 ----
mean loss: 995.54
 ---- batch: 090 ----
mean loss: 973.04
 ---- batch: 100 ----
mean loss: 951.53
 ---- batch: 110 ----
mean loss: 937.22
train mean loss: 1073.21
epoch train time: 0:00:00.714283
elapsed time: 0:00:34.363432
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:00:40.323827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.53
 ---- batch: 020 ----
mean loss: 910.22
 ---- batch: 030 ----
mean loss: 907.78
 ---- batch: 040 ----
mean loss: 883.66
 ---- batch: 050 ----
mean loss: 867.16
 ---- batch: 060 ----
mean loss: 869.33
 ---- batch: 070 ----
mean loss: 871.06
 ---- batch: 080 ----
mean loss: 844.46
 ---- batch: 090 ----
mean loss: 869.23
 ---- batch: 100 ----
mean loss: 871.54
 ---- batch: 110 ----
mean loss: 851.35
train mean loss: 878.60
epoch train time: 0:00:00.714168
elapsed time: 0:00:35.077766
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:00:41.038153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.40
 ---- batch: 020 ----
mean loss: 847.39
 ---- batch: 030 ----
mean loss: 853.97
 ---- batch: 040 ----
mean loss: 860.15
 ---- batch: 050 ----
mean loss: 853.44
 ---- batch: 060 ----
mean loss: 839.16
 ---- batch: 070 ----
mean loss: 842.64
 ---- batch: 080 ----
mean loss: 839.61
 ---- batch: 090 ----
mean loss: 840.59
 ---- batch: 100 ----
mean loss: 855.68
 ---- batch: 110 ----
mean loss: 854.03
train mean loss: 848.44
epoch train time: 0:00:00.714775
elapsed time: 0:00:35.792677
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:00:41.753061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.24
 ---- batch: 020 ----
mean loss: 842.92
 ---- batch: 030 ----
mean loss: 847.97
 ---- batch: 040 ----
mean loss: 822.52
 ---- batch: 050 ----
mean loss: 836.91
 ---- batch: 060 ----
mean loss: 858.24
 ---- batch: 070 ----
mean loss: 836.27
 ---- batch: 080 ----
mean loss: 857.41
 ---- batch: 090 ----
mean loss: 837.13
 ---- batch: 100 ----
mean loss: 844.45
 ---- batch: 110 ----
mean loss: 842.55
train mean loss: 842.21
epoch train time: 0:00:00.718631
elapsed time: 0:00:36.511440
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:00:42.471842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.58
 ---- batch: 020 ----
mean loss: 825.18
 ---- batch: 030 ----
mean loss: 828.49
 ---- batch: 040 ----
mean loss: 838.21
 ---- batch: 050 ----
mean loss: 811.08
 ---- batch: 060 ----
mean loss: 839.56
 ---- batch: 070 ----
mean loss: 830.04
 ---- batch: 080 ----
mean loss: 853.53
 ---- batch: 090 ----
mean loss: 848.88
 ---- batch: 100 ----
mean loss: 853.19
 ---- batch: 110 ----
mean loss: 840.38
train mean loss: 838.07
epoch train time: 0:00:00.715508
elapsed time: 0:00:37.227114
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:00:43.187497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.34
 ---- batch: 020 ----
mean loss: 820.16
 ---- batch: 030 ----
mean loss: 855.11
 ---- batch: 040 ----
mean loss: 836.00
 ---- batch: 050 ----
mean loss: 836.49
 ---- batch: 060 ----
mean loss: 841.87
 ---- batch: 070 ----
mean loss: 812.64
 ---- batch: 080 ----
mean loss: 831.95
 ---- batch: 090 ----
mean loss: 835.48
 ---- batch: 100 ----
mean loss: 842.70
 ---- batch: 110 ----
mean loss: 837.86
train mean loss: 833.96
epoch train time: 0:00:00.708587
elapsed time: 0:00:37.935832
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:00:43.896235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.04
 ---- batch: 020 ----
mean loss: 836.52
 ---- batch: 030 ----
mean loss: 817.65
 ---- batch: 040 ----
mean loss: 817.10
 ---- batch: 050 ----
mean loss: 833.98
 ---- batch: 060 ----
mean loss: 816.92
 ---- batch: 070 ----
mean loss: 839.80
 ---- batch: 080 ----
mean loss: 824.34
 ---- batch: 090 ----
mean loss: 823.21
 ---- batch: 100 ----
mean loss: 848.02
 ---- batch: 110 ----
mean loss: 836.71
train mean loss: 830.05
epoch train time: 0:00:00.707218
elapsed time: 0:00:38.643204
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:00:44.603602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 822.27
 ---- batch: 020 ----
mean loss: 835.59
 ---- batch: 030 ----
mean loss: 807.25
 ---- batch: 040 ----
mean loss: 840.15
 ---- batch: 050 ----
mean loss: 820.43
 ---- batch: 060 ----
mean loss: 827.39
 ---- batch: 070 ----
mean loss: 821.42
 ---- batch: 080 ----
mean loss: 855.90
 ---- batch: 090 ----
mean loss: 816.74
 ---- batch: 100 ----
mean loss: 810.44
 ---- batch: 110 ----
mean loss: 832.29
train mean loss: 825.74
epoch train time: 0:00:00.710496
elapsed time: 0:00:39.353847
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:00:45.314229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 827.17
 ---- batch: 020 ----
mean loss: 807.25
 ---- batch: 030 ----
mean loss: 822.71
 ---- batch: 040 ----
mean loss: 839.27
 ---- batch: 050 ----
mean loss: 814.02
 ---- batch: 060 ----
mean loss: 818.52
 ---- batch: 070 ----
mean loss: 823.36
 ---- batch: 080 ----
mean loss: 815.71
 ---- batch: 090 ----
mean loss: 831.10
 ---- batch: 100 ----
mean loss: 815.38
 ---- batch: 110 ----
mean loss: 818.00
train mean loss: 821.21
epoch train time: 0:00:00.709591
elapsed time: 0:00:40.063616
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:00:46.023999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.93
 ---- batch: 020 ----
mean loss: 798.49
 ---- batch: 030 ----
mean loss: 833.42
 ---- batch: 040 ----
mean loss: 826.49
 ---- batch: 050 ----
mean loss: 816.90
 ---- batch: 060 ----
mean loss: 809.93
 ---- batch: 070 ----
mean loss: 814.28
 ---- batch: 080 ----
mean loss: 807.12
 ---- batch: 090 ----
mean loss: 822.42
 ---- batch: 100 ----
mean loss: 811.86
 ---- batch: 110 ----
mean loss: 784.78
train mean loss: 816.12
epoch train time: 0:00:00.712329
elapsed time: 0:00:40.776080
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:00:46.736484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.74
 ---- batch: 020 ----
mean loss: 819.99
 ---- batch: 030 ----
mean loss: 817.56
 ---- batch: 040 ----
mean loss: 806.24
 ---- batch: 050 ----
mean loss: 809.31
 ---- batch: 060 ----
mean loss: 804.83
 ---- batch: 070 ----
mean loss: 820.84
 ---- batch: 080 ----
mean loss: 786.61
 ---- batch: 090 ----
mean loss: 807.57
 ---- batch: 100 ----
mean loss: 820.16
 ---- batch: 110 ----
mean loss: 793.05
train mean loss: 811.00
epoch train time: 0:00:00.714895
elapsed time: 0:00:41.491126
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:00:47.451542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 795.13
 ---- batch: 020 ----
mean loss: 809.81
 ---- batch: 030 ----
mean loss: 801.55
 ---- batch: 040 ----
mean loss: 790.77
 ---- batch: 050 ----
mean loss: 796.80
 ---- batch: 060 ----
mean loss: 812.19
 ---- batch: 070 ----
mean loss: 809.35
 ---- batch: 080 ----
mean loss: 811.74
 ---- batch: 090 ----
mean loss: 798.08
 ---- batch: 100 ----
mean loss: 813.88
 ---- batch: 110 ----
mean loss: 816.05
train mean loss: 805.77
epoch train time: 0:00:00.709645
elapsed time: 0:00:42.200935
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:00:48.161318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 805.58
 ---- batch: 020 ----
mean loss: 795.58
 ---- batch: 030 ----
mean loss: 811.83
 ---- batch: 040 ----
mean loss: 812.70
 ---- batch: 050 ----
mean loss: 804.22
 ---- batch: 060 ----
mean loss: 792.78
 ---- batch: 070 ----
mean loss: 796.97
 ---- batch: 080 ----
mean loss: 788.03
 ---- batch: 090 ----
mean loss: 797.11
 ---- batch: 100 ----
mean loss: 796.44
 ---- batch: 110 ----
mean loss: 815.82
train mean loss: 800.54
epoch train time: 0:00:00.710046
elapsed time: 0:00:42.911113
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:00:48.871518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 801.39
 ---- batch: 020 ----
mean loss: 803.74
 ---- batch: 030 ----
mean loss: 793.14
 ---- batch: 040 ----
mean loss: 780.96
 ---- batch: 050 ----
mean loss: 792.91
 ---- batch: 060 ----
mean loss: 803.50
 ---- batch: 070 ----
mean loss: 806.51
 ---- batch: 080 ----
mean loss: 782.27
 ---- batch: 090 ----
mean loss: 785.41
 ---- batch: 100 ----
mean loss: 805.92
 ---- batch: 110 ----
mean loss: 782.34
train mean loss: 795.31
epoch train time: 0:00:00.708691
elapsed time: 0:00:43.619979
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:00:49.580363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 786.46
 ---- batch: 020 ----
mean loss: 761.68
 ---- batch: 030 ----
mean loss: 788.43
 ---- batch: 040 ----
mean loss: 810.76
 ---- batch: 050 ----
mean loss: 809.12
 ---- batch: 060 ----
mean loss: 804.84
 ---- batch: 070 ----
mean loss: 801.41
 ---- batch: 080 ----
mean loss: 788.77
 ---- batch: 090 ----
mean loss: 774.93
 ---- batch: 100 ----
mean loss: 782.50
 ---- batch: 110 ----
mean loss: 780.23
train mean loss: 789.93
epoch train time: 0:00:00.713015
elapsed time: 0:00:44.333133
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:00:50.293545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 766.51
 ---- batch: 020 ----
mean loss: 792.13
 ---- batch: 030 ----
mean loss: 772.52
 ---- batch: 040 ----
mean loss: 790.57
 ---- batch: 050 ----
mean loss: 800.89
 ---- batch: 060 ----
mean loss: 767.72
 ---- batch: 070 ----
mean loss: 795.69
 ---- batch: 080 ----
mean loss: 779.43
 ---- batch: 090 ----
mean loss: 779.10
 ---- batch: 100 ----
mean loss: 795.86
 ---- batch: 110 ----
mean loss: 791.83
train mean loss: 784.33
epoch train time: 0:00:00.712741
elapsed time: 0:00:45.046033
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:00:51.006414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 780.79
 ---- batch: 020 ----
mean loss: 792.94
 ---- batch: 030 ----
mean loss: 779.87
 ---- batch: 040 ----
mean loss: 761.01
 ---- batch: 050 ----
mean loss: 769.63
 ---- batch: 060 ----
mean loss: 784.26
 ---- batch: 070 ----
mean loss: 777.69
 ---- batch: 080 ----
mean loss: 777.65
 ---- batch: 090 ----
mean loss: 783.40
 ---- batch: 100 ----
mean loss: 773.26
 ---- batch: 110 ----
mean loss: 795.71
train mean loss: 778.75
epoch train time: 0:00:00.724993
elapsed time: 0:00:45.771169
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:00:51.731581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.26
 ---- batch: 020 ----
mean loss: 781.64
 ---- batch: 030 ----
mean loss: 784.75
 ---- batch: 040 ----
mean loss: 768.51
 ---- batch: 050 ----
mean loss: 760.96
 ---- batch: 060 ----
mean loss: 780.77
 ---- batch: 070 ----
mean loss: 774.70
 ---- batch: 080 ----
mean loss: 777.04
 ---- batch: 090 ----
mean loss: 766.66
 ---- batch: 100 ----
mean loss: 772.41
 ---- batch: 110 ----
mean loss: 774.87
train mean loss: 773.26
epoch train time: 0:00:00.717179
elapsed time: 0:00:46.488526
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:00:52.448930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.92
 ---- batch: 020 ----
mean loss: 805.89
 ---- batch: 030 ----
mean loss: 770.55
 ---- batch: 040 ----
mean loss: 791.63
 ---- batch: 050 ----
mean loss: 773.46
 ---- batch: 060 ----
mean loss: 775.52
 ---- batch: 070 ----
mean loss: 765.61
 ---- batch: 080 ----
mean loss: 774.88
 ---- batch: 090 ----
mean loss: 755.49
 ---- batch: 100 ----
mean loss: 752.98
 ---- batch: 110 ----
mean loss: 747.21
train mean loss: 767.95
epoch train time: 0:00:00.718889
elapsed time: 0:00:47.207569
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:00:53.167952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 757.03
 ---- batch: 020 ----
mean loss: 778.77
 ---- batch: 030 ----
mean loss: 754.42
 ---- batch: 040 ----
mean loss: 779.54
 ---- batch: 050 ----
mean loss: 767.23
 ---- batch: 060 ----
mean loss: 759.83
 ---- batch: 070 ----
mean loss: 771.55
 ---- batch: 080 ----
mean loss: 757.04
 ---- batch: 090 ----
mean loss: 753.97
 ---- batch: 100 ----
mean loss: 751.01
 ---- batch: 110 ----
mean loss: 766.65
train mean loss: 762.86
epoch train time: 0:00:00.724335
elapsed time: 0:00:47.932036
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:00:53.892417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.46
 ---- batch: 020 ----
mean loss: 746.47
 ---- batch: 030 ----
mean loss: 737.82
 ---- batch: 040 ----
mean loss: 762.91
 ---- batch: 050 ----
mean loss: 777.63
 ---- batch: 060 ----
mean loss: 753.96
 ---- batch: 070 ----
mean loss: 776.97
 ---- batch: 080 ----
mean loss: 747.62
 ---- batch: 090 ----
mean loss: 752.35
 ---- batch: 100 ----
mean loss: 779.55
 ---- batch: 110 ----
mean loss: 750.69
train mean loss: 757.74
epoch train time: 0:00:00.707722
elapsed time: 0:00:48.639886
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:00:54.600267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.56
 ---- batch: 020 ----
mean loss: 753.89
 ---- batch: 030 ----
mean loss: 767.98
 ---- batch: 040 ----
mean loss: 749.99
 ---- batch: 050 ----
mean loss: 767.17
 ---- batch: 060 ----
mean loss: 745.86
 ---- batch: 070 ----
mean loss: 748.38
 ---- batch: 080 ----
mean loss: 762.63
 ---- batch: 090 ----
mean loss: 747.93
 ---- batch: 100 ----
mean loss: 757.96
 ---- batch: 110 ----
mean loss: 736.36
train mean loss: 752.47
epoch train time: 0:00:00.713135
elapsed time: 0:00:49.353181
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:00:55.313578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 747.61
 ---- batch: 020 ----
mean loss: 748.76
 ---- batch: 030 ----
mean loss: 735.57
 ---- batch: 040 ----
mean loss: 754.58
 ---- batch: 050 ----
mean loss: 747.99
 ---- batch: 060 ----
mean loss: 757.09
 ---- batch: 070 ----
mean loss: 729.34
 ---- batch: 080 ----
mean loss: 749.12
 ---- batch: 090 ----
mean loss: 755.56
 ---- batch: 100 ----
mean loss: 734.37
 ---- batch: 110 ----
mean loss: 754.81
train mean loss: 747.10
epoch train time: 0:00:00.715621
elapsed time: 0:00:50.068946
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:00:56.029330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 747.00
 ---- batch: 020 ----
mean loss: 742.73
 ---- batch: 030 ----
mean loss: 736.33
 ---- batch: 040 ----
mean loss: 742.06
 ---- batch: 050 ----
mean loss: 741.04
 ---- batch: 060 ----
mean loss: 749.01
 ---- batch: 070 ----
mean loss: 754.91
 ---- batch: 080 ----
mean loss: 727.74
 ---- batch: 090 ----
mean loss: 749.67
 ---- batch: 100 ----
mean loss: 735.32
 ---- batch: 110 ----
mean loss: 737.73
train mean loss: 741.46
epoch train time: 0:00:00.719665
elapsed time: 0:00:50.788745
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:00:56.749145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 742.03
 ---- batch: 020 ----
mean loss: 742.35
 ---- batch: 030 ----
mean loss: 748.74
 ---- batch: 040 ----
mean loss: 742.25
 ---- batch: 050 ----
mean loss: 731.60
 ---- batch: 060 ----
mean loss: 720.26
 ---- batch: 070 ----
mean loss: 719.36
 ---- batch: 080 ----
mean loss: 753.62
 ---- batch: 090 ----
mean loss: 748.21
 ---- batch: 100 ----
mean loss: 723.41
 ---- batch: 110 ----
mean loss: 727.49
train mean loss: 735.83
epoch train time: 0:00:00.705792
elapsed time: 0:00:51.494682
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:00:57.455061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 730.39
 ---- batch: 020 ----
mean loss: 722.37
 ---- batch: 030 ----
mean loss: 746.99
 ---- batch: 040 ----
mean loss: 747.94
 ---- batch: 050 ----
mean loss: 727.06
 ---- batch: 060 ----
mean loss: 727.47
 ---- batch: 070 ----
mean loss: 724.22
 ---- batch: 080 ----
mean loss: 727.11
 ---- batch: 090 ----
mean loss: 735.06
 ---- batch: 100 ----
mean loss: 720.08
 ---- batch: 110 ----
mean loss: 729.26
train mean loss: 729.90
epoch train time: 0:00:00.707610
elapsed time: 0:00:52.202450
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:00:58.162847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 692.43
 ---- batch: 020 ----
mean loss: 722.73
 ---- batch: 030 ----
mean loss: 735.12
 ---- batch: 040 ----
mean loss: 744.71
 ---- batch: 050 ----
mean loss: 743.81
 ---- batch: 060 ----
mean loss: 715.60
 ---- batch: 070 ----
mean loss: 725.06
 ---- batch: 080 ----
mean loss: 724.79
 ---- batch: 090 ----
mean loss: 708.86
 ---- batch: 100 ----
mean loss: 727.87
 ---- batch: 110 ----
mean loss: 719.65
train mean loss: 724.02
epoch train time: 0:00:00.732876
elapsed time: 0:00:52.935488
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:00:58.895875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 726.15
 ---- batch: 020 ----
mean loss: 704.26
 ---- batch: 030 ----
mean loss: 713.58
 ---- batch: 040 ----
mean loss: 713.87
 ---- batch: 050 ----
mean loss: 726.28
 ---- batch: 060 ----
mean loss: 715.19
 ---- batch: 070 ----
mean loss: 717.59
 ---- batch: 080 ----
mean loss: 727.94
 ---- batch: 090 ----
mean loss: 718.09
 ---- batch: 100 ----
mean loss: 724.09
 ---- batch: 110 ----
mean loss: 716.41
train mean loss: 717.83
epoch train time: 0:00:00.712375
elapsed time: 0:00:53.648000
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:00:59.608383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.15
 ---- batch: 020 ----
mean loss: 706.86
 ---- batch: 030 ----
mean loss: 717.61
 ---- batch: 040 ----
mean loss: 718.95
 ---- batch: 050 ----
mean loss: 713.92
 ---- batch: 060 ----
mean loss: 712.04
 ---- batch: 070 ----
mean loss: 679.89
 ---- batch: 080 ----
mean loss: 720.43
 ---- batch: 090 ----
mean loss: 712.26
 ---- batch: 100 ----
mean loss: 714.29
 ---- batch: 110 ----
mean loss: 717.26
train mean loss: 711.53
epoch train time: 0:00:00.717318
elapsed time: 0:00:54.365486
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:01:00.325870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.78
 ---- batch: 020 ----
mean loss: 693.33
 ---- batch: 030 ----
mean loss: 721.11
 ---- batch: 040 ----
mean loss: 725.02
 ---- batch: 050 ----
mean loss: 688.21
 ---- batch: 060 ----
mean loss: 707.91
 ---- batch: 070 ----
mean loss: 707.22
 ---- batch: 080 ----
mean loss: 705.45
 ---- batch: 090 ----
mean loss: 700.83
 ---- batch: 100 ----
mean loss: 704.50
 ---- batch: 110 ----
mean loss: 691.75
train mean loss: 705.05
epoch train time: 0:00:00.720084
elapsed time: 0:00:55.085704
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:01:01.046105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.92
 ---- batch: 020 ----
mean loss: 691.93
 ---- batch: 030 ----
mean loss: 709.88
 ---- batch: 040 ----
mean loss: 712.49
 ---- batch: 050 ----
mean loss: 701.61
 ---- batch: 060 ----
mean loss: 707.37
 ---- batch: 070 ----
mean loss: 674.84
 ---- batch: 080 ----
mean loss: 706.30
 ---- batch: 090 ----
mean loss: 694.89
 ---- batch: 100 ----
mean loss: 703.17
 ---- batch: 110 ----
mean loss: 687.93
train mean loss: 697.98
epoch train time: 0:00:00.710215
elapsed time: 0:00:55.796064
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:01:01.756444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 687.95
 ---- batch: 020 ----
mean loss: 695.54
 ---- batch: 030 ----
mean loss: 685.78
 ---- batch: 040 ----
mean loss: 692.38
 ---- batch: 050 ----
mean loss: 676.14
 ---- batch: 060 ----
mean loss: 684.30
 ---- batch: 070 ----
mean loss: 697.88
 ---- batch: 080 ----
mean loss: 699.42
 ---- batch: 090 ----
mean loss: 697.77
 ---- batch: 100 ----
mean loss: 696.04
 ---- batch: 110 ----
mean loss: 693.20
train mean loss: 690.88
epoch train time: 0:00:00.714991
elapsed time: 0:00:56.511182
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:01:02.471578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 683.50
 ---- batch: 020 ----
mean loss: 659.98
 ---- batch: 030 ----
mean loss: 680.07
 ---- batch: 040 ----
mean loss: 696.94
 ---- batch: 050 ----
mean loss: 684.63
 ---- batch: 060 ----
mean loss: 692.15
 ---- batch: 070 ----
mean loss: 669.42
 ---- batch: 080 ----
mean loss: 691.53
 ---- batch: 090 ----
mean loss: 690.88
 ---- batch: 100 ----
mean loss: 690.27
 ---- batch: 110 ----
mean loss: 685.22
train mean loss: 683.50
epoch train time: 0:00:00.715877
elapsed time: 0:00:57.227206
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:01:03.187591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 704.97
 ---- batch: 020 ----
mean loss: 688.75
 ---- batch: 030 ----
mean loss: 674.49
 ---- batch: 040 ----
mean loss: 675.03
 ---- batch: 050 ----
mean loss: 674.99
 ---- batch: 060 ----
mean loss: 669.08
 ---- batch: 070 ----
mean loss: 672.06
 ---- batch: 080 ----
mean loss: 675.34
 ---- batch: 090 ----
mean loss: 661.94
 ---- batch: 100 ----
mean loss: 672.47
 ---- batch: 110 ----
mean loss: 665.64
train mean loss: 675.86
epoch train time: 0:00:00.711782
elapsed time: 0:00:57.939132
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:01:03.899516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.41
 ---- batch: 020 ----
mean loss: 667.98
 ---- batch: 030 ----
mean loss: 677.14
 ---- batch: 040 ----
mean loss: 669.37
 ---- batch: 050 ----
mean loss: 659.78
 ---- batch: 060 ----
mean loss: 675.67
 ---- batch: 070 ----
mean loss: 659.94
 ---- batch: 080 ----
mean loss: 656.41
 ---- batch: 090 ----
mean loss: 677.29
 ---- batch: 100 ----
mean loss: 670.36
 ---- batch: 110 ----
mean loss: 650.21
train mean loss: 667.82
epoch train time: 0:00:00.714984
elapsed time: 0:00:58.654250
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:01:04.614649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.06
 ---- batch: 020 ----
mean loss: 670.03
 ---- batch: 030 ----
mean loss: 661.75
 ---- batch: 040 ----
mean loss: 659.68
 ---- batch: 050 ----
mean loss: 678.01
 ---- batch: 060 ----
mean loss: 646.33
 ---- batch: 070 ----
mean loss: 651.51
 ---- batch: 080 ----
mean loss: 650.00
 ---- batch: 090 ----
mean loss: 645.49
 ---- batch: 100 ----
mean loss: 658.44
 ---- batch: 110 ----
mean loss: 670.95
train mean loss: 659.62
epoch train time: 0:00:00.718793
elapsed time: 0:00:59.373193
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:01:05.333578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 662.45
 ---- batch: 020 ----
mean loss: 655.77
 ---- batch: 030 ----
mean loss: 659.77
 ---- batch: 040 ----
mean loss: 655.66
 ---- batch: 050 ----
mean loss: 632.44
 ---- batch: 060 ----
mean loss: 645.18
 ---- batch: 070 ----
mean loss: 649.35
 ---- batch: 080 ----
mean loss: 659.41
 ---- batch: 090 ----
mean loss: 642.91
 ---- batch: 100 ----
mean loss: 643.82
 ---- batch: 110 ----
mean loss: 651.21
train mean loss: 651.05
epoch train time: 0:00:00.725962
elapsed time: 0:01:00.099306
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:01:06.059731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 645.55
 ---- batch: 020 ----
mean loss: 646.49
 ---- batch: 030 ----
mean loss: 634.26
 ---- batch: 040 ----
mean loss: 643.63
 ---- batch: 050 ----
mean loss: 643.72
 ---- batch: 060 ----
mean loss: 636.59
 ---- batch: 070 ----
mean loss: 639.93
 ---- batch: 080 ----
mean loss: 649.47
 ---- batch: 090 ----
mean loss: 640.22
 ---- batch: 100 ----
mean loss: 646.86
 ---- batch: 110 ----
mean loss: 643.62
train mean loss: 641.94
epoch train time: 0:00:00.716692
elapsed time: 0:01:00.816172
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:01:06.776555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 630.45
 ---- batch: 020 ----
mean loss: 645.79
 ---- batch: 030 ----
mean loss: 624.91
 ---- batch: 040 ----
mean loss: 642.00
 ---- batch: 050 ----
mean loss: 639.59
 ---- batch: 060 ----
mean loss: 633.83
 ---- batch: 070 ----
mean loss: 631.78
 ---- batch: 080 ----
mean loss: 634.88
 ---- batch: 090 ----
mean loss: 620.12
 ---- batch: 100 ----
mean loss: 627.83
 ---- batch: 110 ----
mean loss: 631.92
train mean loss: 632.60
epoch train time: 0:00:00.717207
elapsed time: 0:01:01.533511
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:01:07.493893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 633.42
 ---- batch: 020 ----
mean loss: 616.12
 ---- batch: 030 ----
mean loss: 627.94
 ---- batch: 040 ----
mean loss: 626.05
 ---- batch: 050 ----
mean loss: 615.68
 ---- batch: 060 ----
mean loss: 615.79
 ---- batch: 070 ----
mean loss: 634.60
 ---- batch: 080 ----
mean loss: 638.15
 ---- batch: 090 ----
mean loss: 625.20
 ---- batch: 100 ----
mean loss: 618.84
 ---- batch: 110 ----
mean loss: 607.09
train mean loss: 622.74
epoch train time: 0:00:00.723695
elapsed time: 0:01:02.257361
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:01:08.217747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 617.88
 ---- batch: 020 ----
mean loss: 611.36
 ---- batch: 030 ----
mean loss: 616.52
 ---- batch: 040 ----
mean loss: 618.28
 ---- batch: 050 ----
mean loss: 620.31
 ---- batch: 060 ----
mean loss: 612.40
 ---- batch: 070 ----
mean loss: 614.54
 ---- batch: 080 ----
mean loss: 612.52
 ---- batch: 090 ----
mean loss: 607.79
 ---- batch: 100 ----
mean loss: 597.72
 ---- batch: 110 ----
mean loss: 609.54
train mean loss: 612.63
epoch train time: 0:00:00.716456
elapsed time: 0:01:02.973964
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:01:08.934344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 601.02
 ---- batch: 020 ----
mean loss: 599.31
 ---- batch: 030 ----
mean loss: 615.28
 ---- batch: 040 ----
mean loss: 605.21
 ---- batch: 050 ----
mean loss: 604.52
 ---- batch: 060 ----
mean loss: 600.88
 ---- batch: 070 ----
mean loss: 586.78
 ---- batch: 080 ----
mean loss: 609.69
 ---- batch: 090 ----
mean loss: 594.32
 ---- batch: 100 ----
mean loss: 602.66
 ---- batch: 110 ----
mean loss: 596.14
train mean loss: 601.91
epoch train time: 0:00:00.735816
elapsed time: 0:01:03.709910
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:01:09.670308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 590.32
 ---- batch: 020 ----
mean loss: 592.94
 ---- batch: 030 ----
mean loss: 588.71
 ---- batch: 040 ----
mean loss: 598.88
 ---- batch: 050 ----
mean loss: 590.19
 ---- batch: 060 ----
mean loss: 617.31
 ---- batch: 070 ----
mean loss: 595.64
 ---- batch: 080 ----
mean loss: 582.59
 ---- batch: 090 ----
mean loss: 583.47
 ---- batch: 100 ----
mean loss: 572.15
 ---- batch: 110 ----
mean loss: 585.84
train mean loss: 590.77
epoch train time: 0:00:00.722534
elapsed time: 0:01:04.432597
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:01:10.393034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 568.87
 ---- batch: 020 ----
mean loss: 602.39
 ---- batch: 030 ----
mean loss: 590.98
 ---- batch: 040 ----
mean loss: 587.44
 ---- batch: 050 ----
mean loss: 573.41
 ---- batch: 060 ----
mean loss: 574.72
 ---- batch: 070 ----
mean loss: 572.86
 ---- batch: 080 ----
mean loss: 578.68
 ---- batch: 090 ----
mean loss: 568.01
 ---- batch: 100 ----
mean loss: 580.41
 ---- batch: 110 ----
mean loss: 578.37
train mean loss: 579.20
epoch train time: 0:00:00.722486
elapsed time: 0:01:05.155288
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:01:11.115673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.26
 ---- batch: 020 ----
mean loss: 570.79
 ---- batch: 030 ----
mean loss: 575.18
 ---- batch: 040 ----
mean loss: 567.22
 ---- batch: 050 ----
mean loss: 567.17
 ---- batch: 060 ----
mean loss: 552.22
 ---- batch: 070 ----
mean loss: 581.76
 ---- batch: 080 ----
mean loss: 562.56
 ---- batch: 090 ----
mean loss: 569.45
 ---- batch: 100 ----
mean loss: 557.63
 ---- batch: 110 ----
mean loss: 569.29
train mean loss: 567.64
epoch train time: 0:00:00.714503
elapsed time: 0:01:05.869929
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:01:11.830311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.41
 ---- batch: 020 ----
mean loss: 565.93
 ---- batch: 030 ----
mean loss: 551.94
 ---- batch: 040 ----
mean loss: 545.89
 ---- batch: 050 ----
mean loss: 564.06
 ---- batch: 060 ----
mean loss: 552.51
 ---- batch: 070 ----
mean loss: 570.32
 ---- batch: 080 ----
mean loss: 553.46
 ---- batch: 090 ----
mean loss: 562.78
 ---- batch: 100 ----
mean loss: 536.40
 ---- batch: 110 ----
mean loss: 550.89
train mean loss: 555.60
epoch train time: 0:00:00.711653
elapsed time: 0:01:06.581726
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:01:12.542108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.52
 ---- batch: 020 ----
mean loss: 547.74
 ---- batch: 030 ----
mean loss: 551.04
 ---- batch: 040 ----
mean loss: 537.21
 ---- batch: 050 ----
mean loss: 537.12
 ---- batch: 060 ----
mean loss: 552.09
 ---- batch: 070 ----
mean loss: 554.22
 ---- batch: 080 ----
mean loss: 543.20
 ---- batch: 090 ----
mean loss: 536.39
 ---- batch: 100 ----
mean loss: 540.57
 ---- batch: 110 ----
mean loss: 530.22
train mean loss: 543.79
epoch train time: 0:00:00.713870
elapsed time: 0:01:07.295727
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:01:13.256110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.75
 ---- batch: 020 ----
mean loss: 542.02
 ---- batch: 030 ----
mean loss: 536.62
 ---- batch: 040 ----
mean loss: 535.28
 ---- batch: 050 ----
mean loss: 525.81
 ---- batch: 060 ----
mean loss: 530.02
 ---- batch: 070 ----
mean loss: 524.68
 ---- batch: 080 ----
mean loss: 533.40
 ---- batch: 090 ----
mean loss: 533.66
 ---- batch: 100 ----
mean loss: 522.50
 ---- batch: 110 ----
mean loss: 521.88
train mean loss: 531.73
epoch train time: 0:00:00.724442
elapsed time: 0:01:08.020297
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:01:13.980683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 526.96
 ---- batch: 020 ----
mean loss: 525.80
 ---- batch: 030 ----
mean loss: 523.01
 ---- batch: 040 ----
mean loss: 525.38
 ---- batch: 050 ----
mean loss: 522.06
 ---- batch: 060 ----
mean loss: 520.55
 ---- batch: 070 ----
mean loss: 525.66
 ---- batch: 080 ----
mean loss: 519.04
 ---- batch: 090 ----
mean loss: 512.00
 ---- batch: 100 ----
mean loss: 506.97
 ---- batch: 110 ----
mean loss: 512.48
train mean loss: 519.95
epoch train time: 0:00:00.710387
elapsed time: 0:01:08.730818
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:01:14.691217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.01
 ---- batch: 020 ----
mean loss: 518.75
 ---- batch: 030 ----
mean loss: 524.29
 ---- batch: 040 ----
mean loss: 505.39
 ---- batch: 050 ----
mean loss: 501.69
 ---- batch: 060 ----
mean loss: 512.91
 ---- batch: 070 ----
mean loss: 505.22
 ---- batch: 080 ----
mean loss: 495.89
 ---- batch: 090 ----
mean loss: 497.06
 ---- batch: 100 ----
mean loss: 510.74
 ---- batch: 110 ----
mean loss: 506.16
train mean loss: 508.12
epoch train time: 0:00:00.716945
elapsed time: 0:01:09.447912
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:01:15.408316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.27
 ---- batch: 020 ----
mean loss: 508.29
 ---- batch: 030 ----
mean loss: 494.74
 ---- batch: 040 ----
mean loss: 489.86
 ---- batch: 050 ----
mean loss: 486.80
 ---- batch: 060 ----
mean loss: 500.22
 ---- batch: 070 ----
mean loss: 495.15
 ---- batch: 080 ----
mean loss: 498.24
 ---- batch: 090 ----
mean loss: 492.21
 ---- batch: 100 ----
mean loss: 485.95
 ---- batch: 110 ----
mean loss: 501.76
train mean loss: 496.43
epoch train time: 0:00:00.711441
elapsed time: 0:01:10.159506
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:01:16.119891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.75
 ---- batch: 020 ----
mean loss: 492.36
 ---- batch: 030 ----
mean loss: 492.42
 ---- batch: 040 ----
mean loss: 481.10
 ---- batch: 050 ----
mean loss: 474.26
 ---- batch: 060 ----
mean loss: 489.09
 ---- batch: 070 ----
mean loss: 488.61
 ---- batch: 080 ----
mean loss: 487.85
 ---- batch: 090 ----
mean loss: 471.01
 ---- batch: 100 ----
mean loss: 487.34
 ---- batch: 110 ----
mean loss: 478.94
train mean loss: 484.94
epoch train time: 0:00:00.713160
elapsed time: 0:01:10.872797
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:01:16.833180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.38
 ---- batch: 020 ----
mean loss: 478.71
 ---- batch: 030 ----
mean loss: 482.58
 ---- batch: 040 ----
mean loss: 480.22
 ---- batch: 050 ----
mean loss: 481.95
 ---- batch: 060 ----
mean loss: 456.23
 ---- batch: 070 ----
mean loss: 467.29
 ---- batch: 080 ----
mean loss: 465.41
 ---- batch: 090 ----
mean loss: 463.04
 ---- batch: 100 ----
mean loss: 480.66
 ---- batch: 110 ----
mean loss: 470.53
train mean loss: 473.41
epoch train time: 0:00:00.704852
elapsed time: 0:01:11.577783
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:01:17.538166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.48
 ---- batch: 020 ----
mean loss: 466.44
 ---- batch: 030 ----
mean loss: 477.07
 ---- batch: 040 ----
mean loss: 461.02
 ---- batch: 050 ----
mean loss: 460.90
 ---- batch: 060 ----
mean loss: 451.55
 ---- batch: 070 ----
mean loss: 453.62
 ---- batch: 080 ----
mean loss: 455.99
 ---- batch: 090 ----
mean loss: 464.49
 ---- batch: 100 ----
mean loss: 452.41
 ---- batch: 110 ----
mean loss: 460.86
train mean loss: 462.18
epoch train time: 0:00:00.720108
elapsed time: 0:01:12.298038
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:01:18.258430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.62
 ---- batch: 020 ----
mean loss: 456.55
 ---- batch: 030 ----
mean loss: 457.81
 ---- batch: 040 ----
mean loss: 452.57
 ---- batch: 050 ----
mean loss: 459.56
 ---- batch: 060 ----
mean loss: 463.45
 ---- batch: 070 ----
mean loss: 436.29
 ---- batch: 080 ----
mean loss: 438.19
 ---- batch: 090 ----
mean loss: 443.96
 ---- batch: 100 ----
mean loss: 443.29
 ---- batch: 110 ----
mean loss: 445.67
train mean loss: 451.19
epoch train time: 0:00:00.717055
elapsed time: 0:01:13.015266
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:01:18.975663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.24
 ---- batch: 020 ----
mean loss: 451.92
 ---- batch: 030 ----
mean loss: 430.80
 ---- batch: 040 ----
mean loss: 438.47
 ---- batch: 050 ----
mean loss: 437.82
 ---- batch: 060 ----
mean loss: 436.23
 ---- batch: 070 ----
mean loss: 438.25
 ---- batch: 080 ----
mean loss: 421.53
 ---- batch: 090 ----
mean loss: 447.57
 ---- batch: 100 ----
mean loss: 445.62
 ---- batch: 110 ----
mean loss: 444.19
train mean loss: 440.31
epoch train time: 0:00:00.708336
elapsed time: 0:01:13.723750
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:01:19.684134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.77
 ---- batch: 020 ----
mean loss: 434.29
 ---- batch: 030 ----
mean loss: 437.27
 ---- batch: 040 ----
mean loss: 435.00
 ---- batch: 050 ----
mean loss: 431.23
 ---- batch: 060 ----
mean loss: 411.95
 ---- batch: 070 ----
mean loss: 427.37
 ---- batch: 080 ----
mean loss: 431.22
 ---- batch: 090 ----
mean loss: 424.98
 ---- batch: 100 ----
mean loss: 428.12
 ---- batch: 110 ----
mean loss: 428.95
train mean loss: 428.97
epoch train time: 0:00:00.708456
elapsed time: 0:01:14.432352
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:01:20.392767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.04
 ---- batch: 020 ----
mean loss: 425.43
 ---- batch: 030 ----
mean loss: 428.54
 ---- batch: 040 ----
mean loss: 414.18
 ---- batch: 050 ----
mean loss: 412.46
 ---- batch: 060 ----
mean loss: 413.47
 ---- batch: 070 ----
mean loss: 400.30
 ---- batch: 080 ----
mean loss: 408.86
 ---- batch: 090 ----
mean loss: 421.98
 ---- batch: 100 ----
mean loss: 413.00
 ---- batch: 110 ----
mean loss: 407.76
train mean loss: 415.87
epoch train time: 0:00:00.707617
elapsed time: 0:01:15.140141
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:01:21.100523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.37
 ---- batch: 020 ----
mean loss: 400.37
 ---- batch: 030 ----
mean loss: 407.36
 ---- batch: 040 ----
mean loss: 398.85
 ---- batch: 050 ----
mean loss: 405.03
 ---- batch: 060 ----
mean loss: 382.01
 ---- batch: 070 ----
mean loss: 383.18
 ---- batch: 080 ----
mean loss: 391.10
 ---- batch: 090 ----
mean loss: 381.56
 ---- batch: 100 ----
mean loss: 385.52
 ---- batch: 110 ----
mean loss: 381.53
train mean loss: 393.19
epoch train time: 0:00:00.717603
elapsed time: 0:01:15.857920
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:01:21.818314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.58
 ---- batch: 020 ----
mean loss: 377.79
 ---- batch: 030 ----
mean loss: 374.28
 ---- batch: 040 ----
mean loss: 362.56
 ---- batch: 050 ----
mean loss: 367.44
 ---- batch: 060 ----
mean loss: 368.94
 ---- batch: 070 ----
mean loss: 368.29
 ---- batch: 080 ----
mean loss: 355.67
 ---- batch: 090 ----
mean loss: 364.11
 ---- batch: 100 ----
mean loss: 348.74
 ---- batch: 110 ----
mean loss: 364.00
train mean loss: 364.52
epoch train time: 0:00:00.718208
elapsed time: 0:01:16.576291
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:01:22.536705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.78
 ---- batch: 020 ----
mean loss: 340.16
 ---- batch: 030 ----
mean loss: 342.08
 ---- batch: 040 ----
mean loss: 352.93
 ---- batch: 050 ----
mean loss: 346.08
 ---- batch: 060 ----
mean loss: 342.97
 ---- batch: 070 ----
mean loss: 325.19
 ---- batch: 080 ----
mean loss: 340.75
 ---- batch: 090 ----
mean loss: 343.73
 ---- batch: 100 ----
mean loss: 337.96
 ---- batch: 110 ----
mean loss: 343.09
train mean loss: 342.42
epoch train time: 0:00:00.715751
elapsed time: 0:01:17.292219
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:01:23.252662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.79
 ---- batch: 020 ----
mean loss: 325.01
 ---- batch: 030 ----
mean loss: 328.94
 ---- batch: 040 ----
mean loss: 332.20
 ---- batch: 050 ----
mean loss: 321.08
 ---- batch: 060 ----
mean loss: 321.38
 ---- batch: 070 ----
mean loss: 333.35
 ---- batch: 080 ----
mean loss: 321.92
 ---- batch: 090 ----
mean loss: 321.62
 ---- batch: 100 ----
mean loss: 326.78
 ---- batch: 110 ----
mean loss: 337.54
train mean loss: 326.58
epoch train time: 0:00:00.717193
elapsed time: 0:01:18.009618
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:01:23.970000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.51
 ---- batch: 020 ----
mean loss: 321.41
 ---- batch: 030 ----
mean loss: 318.61
 ---- batch: 040 ----
mean loss: 298.63
 ---- batch: 050 ----
mean loss: 312.55
 ---- batch: 060 ----
mean loss: 315.49
 ---- batch: 070 ----
mean loss: 318.80
 ---- batch: 080 ----
mean loss: 315.20
 ---- batch: 090 ----
mean loss: 326.43
 ---- batch: 100 ----
mean loss: 311.12
 ---- batch: 110 ----
mean loss: 308.04
train mean loss: 315.39
epoch train time: 0:00:00.709271
elapsed time: 0:01:18.719035
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:01:24.679464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.38
 ---- batch: 020 ----
mean loss: 299.35
 ---- batch: 030 ----
mean loss: 306.42
 ---- batch: 040 ----
mean loss: 309.30
 ---- batch: 050 ----
mean loss: 320.25
 ---- batch: 060 ----
mean loss: 304.76
 ---- batch: 070 ----
mean loss: 308.29
 ---- batch: 080 ----
mean loss: 301.24
 ---- batch: 090 ----
mean loss: 300.98
 ---- batch: 100 ----
mean loss: 294.08
 ---- batch: 110 ----
mean loss: 304.84
train mean loss: 306.43
epoch train time: 0:00:00.708862
elapsed time: 0:01:19.428074
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:01:25.388456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.86
 ---- batch: 020 ----
mean loss: 301.41
 ---- batch: 030 ----
mean loss: 299.26
 ---- batch: 040 ----
mean loss: 299.78
 ---- batch: 050 ----
mean loss: 294.73
 ---- batch: 060 ----
mean loss: 310.08
 ---- batch: 070 ----
mean loss: 300.90
 ---- batch: 080 ----
mean loss: 302.35
 ---- batch: 090 ----
mean loss: 291.57
 ---- batch: 100 ----
mean loss: 296.83
 ---- batch: 110 ----
mean loss: 288.84
train mean loss: 298.79
epoch train time: 0:00:00.710195
elapsed time: 0:01:20.138410
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:01:26.098806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.12
 ---- batch: 020 ----
mean loss: 294.31
 ---- batch: 030 ----
mean loss: 283.31
 ---- batch: 040 ----
mean loss: 293.08
 ---- batch: 050 ----
mean loss: 292.75
 ---- batch: 060 ----
mean loss: 283.70
 ---- batch: 070 ----
mean loss: 284.83
 ---- batch: 080 ----
mean loss: 289.08
 ---- batch: 090 ----
mean loss: 292.48
 ---- batch: 100 ----
mean loss: 290.95
 ---- batch: 110 ----
mean loss: 296.50
train mean loss: 291.01
epoch train time: 0:00:00.712165
elapsed time: 0:01:20.850719
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:01:26.811101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.81
 ---- batch: 020 ----
mean loss: 291.76
 ---- batch: 030 ----
mean loss: 277.28
 ---- batch: 040 ----
mean loss: 289.82
 ---- batch: 050 ----
mean loss: 289.25
 ---- batch: 060 ----
mean loss: 290.91
 ---- batch: 070 ----
mean loss: 287.14
 ---- batch: 080 ----
mean loss: 276.60
 ---- batch: 090 ----
mean loss: 278.15
 ---- batch: 100 ----
mean loss: 266.27
 ---- batch: 110 ----
mean loss: 287.79
train mean loss: 283.23
epoch train time: 0:00:00.716528
elapsed time: 0:01:21.567376
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:01:27.527775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.37
 ---- batch: 020 ----
mean loss: 274.44
 ---- batch: 030 ----
mean loss: 279.54
 ---- batch: 040 ----
mean loss: 268.84
 ---- batch: 050 ----
mean loss: 276.92
 ---- batch: 060 ----
mean loss: 286.92
 ---- batch: 070 ----
mean loss: 277.08
 ---- batch: 080 ----
mean loss: 271.94
 ---- batch: 090 ----
mean loss: 277.03
 ---- batch: 100 ----
mean loss: 267.56
 ---- batch: 110 ----
mean loss: 279.80
train mean loss: 276.89
epoch train time: 0:00:00.727532
elapsed time: 0:01:22.295078
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:01:28.255466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.65
 ---- batch: 020 ----
mean loss: 274.58
 ---- batch: 030 ----
mean loss: 280.17
 ---- batch: 040 ----
mean loss: 276.32
 ---- batch: 050 ----
mean loss: 267.07
 ---- batch: 060 ----
mean loss: 259.99
 ---- batch: 070 ----
mean loss: 273.16
 ---- batch: 080 ----
mean loss: 274.01
 ---- batch: 090 ----
mean loss: 272.91
 ---- batch: 100 ----
mean loss: 271.29
 ---- batch: 110 ----
mean loss: 271.00
train mean loss: 271.23
epoch train time: 0:00:00.726156
elapsed time: 0:01:23.021389
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:01:28.981805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.40
 ---- batch: 020 ----
mean loss: 265.54
 ---- batch: 030 ----
mean loss: 272.57
 ---- batch: 040 ----
mean loss: 266.11
 ---- batch: 050 ----
mean loss: 250.82
 ---- batch: 060 ----
mean loss: 266.22
 ---- batch: 070 ----
mean loss: 271.60
 ---- batch: 080 ----
mean loss: 275.95
 ---- batch: 090 ----
mean loss: 263.28
 ---- batch: 100 ----
mean loss: 267.33
 ---- batch: 110 ----
mean loss: 263.94
train mean loss: 266.34
epoch train time: 0:00:00.721699
elapsed time: 0:01:23.743274
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:01:29.703689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.52
 ---- batch: 020 ----
mean loss: 262.12
 ---- batch: 030 ----
mean loss: 262.89
 ---- batch: 040 ----
mean loss: 266.57
 ---- batch: 050 ----
mean loss: 268.52
 ---- batch: 060 ----
mean loss: 256.79
 ---- batch: 070 ----
mean loss: 262.47
 ---- batch: 080 ----
mean loss: 261.67
 ---- batch: 090 ----
mean loss: 265.97
 ---- batch: 100 ----
mean loss: 262.22
 ---- batch: 110 ----
mean loss: 261.63
train mean loss: 261.92
epoch train time: 0:00:00.728136
elapsed time: 0:01:24.471598
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:01:30.431982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.08
 ---- batch: 020 ----
mean loss: 262.15
 ---- batch: 030 ----
mean loss: 254.00
 ---- batch: 040 ----
mean loss: 256.16
 ---- batch: 050 ----
mean loss: 255.01
 ---- batch: 060 ----
mean loss: 262.75
 ---- batch: 070 ----
mean loss: 255.44
 ---- batch: 080 ----
mean loss: 264.62
 ---- batch: 090 ----
mean loss: 258.97
 ---- batch: 100 ----
mean loss: 257.65
 ---- batch: 110 ----
mean loss: 254.22
train mean loss: 258.03
epoch train time: 0:00:00.716465
elapsed time: 0:01:25.188198
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:01:31.148584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.73
 ---- batch: 020 ----
mean loss: 254.37
 ---- batch: 030 ----
mean loss: 249.56
 ---- batch: 040 ----
mean loss: 251.09
 ---- batch: 050 ----
mean loss: 250.31
 ---- batch: 060 ----
mean loss: 261.05
 ---- batch: 070 ----
mean loss: 258.84
 ---- batch: 080 ----
mean loss: 255.23
 ---- batch: 090 ----
mean loss: 251.42
 ---- batch: 100 ----
mean loss: 256.82
 ---- batch: 110 ----
mean loss: 250.85
train mean loss: 254.68
epoch train time: 0:00:00.724241
elapsed time: 0:01:25.912571
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:01:31.872961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.18
 ---- batch: 020 ----
mean loss: 251.92
 ---- batch: 030 ----
mean loss: 260.61
 ---- batch: 040 ----
mean loss: 256.62
 ---- batch: 050 ----
mean loss: 259.37
 ---- batch: 060 ----
mean loss: 244.58
 ---- batch: 070 ----
mean loss: 244.94
 ---- batch: 080 ----
mean loss: 247.36
 ---- batch: 090 ----
mean loss: 251.26
 ---- batch: 100 ----
mean loss: 244.80
 ---- batch: 110 ----
mean loss: 253.61
train mean loss: 251.33
epoch train time: 0:00:00.715305
elapsed time: 0:01:26.628015
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:01:32.588398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.94
 ---- batch: 020 ----
mean loss: 245.70
 ---- batch: 030 ----
mean loss: 251.53
 ---- batch: 040 ----
mean loss: 252.37
 ---- batch: 050 ----
mean loss: 249.00
 ---- batch: 060 ----
mean loss: 252.31
 ---- batch: 070 ----
mean loss: 248.33
 ---- batch: 080 ----
mean loss: 247.26
 ---- batch: 090 ----
mean loss: 247.57
 ---- batch: 100 ----
mean loss: 241.92
 ---- batch: 110 ----
mean loss: 249.51
train mean loss: 248.27
epoch train time: 0:00:00.713245
elapsed time: 0:01:27.341437
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:01:33.301820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.26
 ---- batch: 020 ----
mean loss: 251.85
 ---- batch: 030 ----
mean loss: 240.81
 ---- batch: 040 ----
mean loss: 246.62
 ---- batch: 050 ----
mean loss: 252.76
 ---- batch: 060 ----
mean loss: 247.49
 ---- batch: 070 ----
mean loss: 239.96
 ---- batch: 080 ----
mean loss: 240.65
 ---- batch: 090 ----
mean loss: 251.66
 ---- batch: 100 ----
mean loss: 245.20
 ---- batch: 110 ----
mean loss: 238.95
train mean loss: 245.67
epoch train time: 0:00:00.712102
elapsed time: 0:01:28.053672
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:01:34.014073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.86
 ---- batch: 020 ----
mean loss: 243.09
 ---- batch: 030 ----
mean loss: 238.81
 ---- batch: 040 ----
mean loss: 242.17
 ---- batch: 050 ----
mean loss: 244.76
 ---- batch: 060 ----
mean loss: 249.27
 ---- batch: 070 ----
mean loss: 247.15
 ---- batch: 080 ----
mean loss: 248.58
 ---- batch: 090 ----
mean loss: 240.42
 ---- batch: 100 ----
mean loss: 240.89
 ---- batch: 110 ----
mean loss: 240.83
train mean loss: 243.12
epoch train time: 0:00:00.711435
elapsed time: 0:01:28.765313
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:01:34.725707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.57
 ---- batch: 020 ----
mean loss: 241.60
 ---- batch: 030 ----
mean loss: 242.88
 ---- batch: 040 ----
mean loss: 248.17
 ---- batch: 050 ----
mean loss: 240.89
 ---- batch: 060 ----
mean loss: 240.97
 ---- batch: 070 ----
mean loss: 240.51
 ---- batch: 080 ----
mean loss: 235.56
 ---- batch: 090 ----
mean loss: 251.34
 ---- batch: 100 ----
mean loss: 223.89
 ---- batch: 110 ----
mean loss: 250.16
train mean loss: 240.94
epoch train time: 0:00:00.717563
elapsed time: 0:01:29.483037
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:01:35.443421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.06
 ---- batch: 020 ----
mean loss: 244.65
 ---- batch: 030 ----
mean loss: 236.52
 ---- batch: 040 ----
mean loss: 233.45
 ---- batch: 050 ----
mean loss: 246.28
 ---- batch: 060 ----
mean loss: 238.09
 ---- batch: 070 ----
mean loss: 246.72
 ---- batch: 080 ----
mean loss: 233.17
 ---- batch: 090 ----
mean loss: 239.49
 ---- batch: 100 ----
mean loss: 242.19
 ---- batch: 110 ----
mean loss: 230.83
train mean loss: 238.90
epoch train time: 0:00:00.715323
elapsed time: 0:01:30.198508
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:01:36.158905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.73
 ---- batch: 020 ----
mean loss: 238.11
 ---- batch: 030 ----
mean loss: 232.39
 ---- batch: 040 ----
mean loss: 234.81
 ---- batch: 050 ----
mean loss: 236.87
 ---- batch: 060 ----
mean loss: 236.06
 ---- batch: 070 ----
mean loss: 235.24
 ---- batch: 080 ----
mean loss: 248.12
 ---- batch: 090 ----
mean loss: 242.39
 ---- batch: 100 ----
mean loss: 227.42
 ---- batch: 110 ----
mean loss: 242.00
train mean loss: 236.95
epoch train time: 0:00:00.712185
elapsed time: 0:01:30.910835
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:01:36.871216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.53
 ---- batch: 020 ----
mean loss: 243.18
 ---- batch: 030 ----
mean loss: 240.03
 ---- batch: 040 ----
mean loss: 237.75
 ---- batch: 050 ----
mean loss: 232.64
 ---- batch: 060 ----
mean loss: 236.50
 ---- batch: 070 ----
mean loss: 232.45
 ---- batch: 080 ----
mean loss: 230.93
 ---- batch: 090 ----
mean loss: 232.16
 ---- batch: 100 ----
mean loss: 234.67
 ---- batch: 110 ----
mean loss: 238.58
train mean loss: 235.60
epoch train time: 0:00:00.707818
elapsed time: 0:01:31.618805
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:01:37.579223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.30
 ---- batch: 020 ----
mean loss: 237.54
 ---- batch: 030 ----
mean loss: 230.90
 ---- batch: 040 ----
mean loss: 229.22
 ---- batch: 050 ----
mean loss: 239.85
 ---- batch: 060 ----
mean loss: 230.29
 ---- batch: 070 ----
mean loss: 222.85
 ---- batch: 080 ----
mean loss: 232.41
 ---- batch: 090 ----
mean loss: 235.83
 ---- batch: 100 ----
mean loss: 239.20
 ---- batch: 110 ----
mean loss: 233.33
train mean loss: 233.71
epoch train time: 0:00:00.709622
elapsed time: 0:01:32.328604
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:01:38.288989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.14
 ---- batch: 020 ----
mean loss: 239.12
 ---- batch: 030 ----
mean loss: 232.71
 ---- batch: 040 ----
mean loss: 226.91
 ---- batch: 050 ----
mean loss: 226.27
 ---- batch: 060 ----
mean loss: 233.29
 ---- batch: 070 ----
mean loss: 239.37
 ---- batch: 080 ----
mean loss: 240.18
 ---- batch: 090 ----
mean loss: 230.65
 ---- batch: 100 ----
mean loss: 234.40
 ---- batch: 110 ----
mean loss: 227.41
train mean loss: 232.37
epoch train time: 0:00:00.710118
elapsed time: 0:01:33.038856
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:01:38.999257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.20
 ---- batch: 020 ----
mean loss: 238.43
 ---- batch: 030 ----
mean loss: 223.31
 ---- batch: 040 ----
mean loss: 235.87
 ---- batch: 050 ----
mean loss: 236.53
 ---- batch: 060 ----
mean loss: 229.17
 ---- batch: 070 ----
mean loss: 230.32
 ---- batch: 080 ----
mean loss: 237.38
 ---- batch: 090 ----
mean loss: 231.15
 ---- batch: 100 ----
mean loss: 229.50
 ---- batch: 110 ----
mean loss: 226.49
train mean loss: 230.95
epoch train time: 0:00:00.718924
elapsed time: 0:01:33.757938
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:01:39.718324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.55
 ---- batch: 020 ----
mean loss: 240.43
 ---- batch: 030 ----
mean loss: 224.53
 ---- batch: 040 ----
mean loss: 229.89
 ---- batch: 050 ----
mean loss: 231.61
 ---- batch: 060 ----
mean loss: 224.53
 ---- batch: 070 ----
mean loss: 223.18
 ---- batch: 080 ----
mean loss: 222.16
 ---- batch: 090 ----
mean loss: 233.46
 ---- batch: 100 ----
mean loss: 232.19
 ---- batch: 110 ----
mean loss: 232.30
train mean loss: 229.79
epoch train time: 0:00:00.713775
elapsed time: 0:01:34.471846
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:01:40.432228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.53
 ---- batch: 020 ----
mean loss: 230.41
 ---- batch: 030 ----
mean loss: 224.75
 ---- batch: 040 ----
mean loss: 223.21
 ---- batch: 050 ----
mean loss: 229.12
 ---- batch: 060 ----
mean loss: 228.58
 ---- batch: 070 ----
mean loss: 226.50
 ---- batch: 080 ----
mean loss: 235.73
 ---- batch: 090 ----
mean loss: 227.18
 ---- batch: 100 ----
mean loss: 221.16
 ---- batch: 110 ----
mean loss: 228.74
train mean loss: 228.55
epoch train time: 0:00:00.718097
elapsed time: 0:01:35.190098
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:01:41.150484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.64
 ---- batch: 020 ----
mean loss: 224.74
 ---- batch: 030 ----
mean loss: 231.20
 ---- batch: 040 ----
mean loss: 227.35
 ---- batch: 050 ----
mean loss: 233.87
 ---- batch: 060 ----
mean loss: 220.21
 ---- batch: 070 ----
mean loss: 228.03
 ---- batch: 080 ----
mean loss: 231.62
 ---- batch: 090 ----
mean loss: 231.59
 ---- batch: 100 ----
mean loss: 230.39
 ---- batch: 110 ----
mean loss: 225.80
train mean loss: 227.38
epoch train time: 0:00:00.718709
elapsed time: 0:01:35.908974
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:01:41.869357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.70
 ---- batch: 020 ----
mean loss: 227.22
 ---- batch: 030 ----
mean loss: 230.64
 ---- batch: 040 ----
mean loss: 232.18
 ---- batch: 050 ----
mean loss: 221.41
 ---- batch: 060 ----
mean loss: 223.85
 ---- batch: 070 ----
mean loss: 229.70
 ---- batch: 080 ----
mean loss: 226.28
 ---- batch: 090 ----
mean loss: 227.16
 ---- batch: 100 ----
mean loss: 222.46
 ---- batch: 110 ----
mean loss: 220.90
train mean loss: 226.65
epoch train time: 0:00:00.707308
elapsed time: 0:01:36.616427
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:01:42.576810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.78
 ---- batch: 020 ----
mean loss: 224.41
 ---- batch: 030 ----
mean loss: 217.27
 ---- batch: 040 ----
mean loss: 225.03
 ---- batch: 050 ----
mean loss: 222.06
 ---- batch: 060 ----
mean loss: 232.80
 ---- batch: 070 ----
mean loss: 230.90
 ---- batch: 080 ----
mean loss: 230.47
 ---- batch: 090 ----
mean loss: 224.55
 ---- batch: 100 ----
mean loss: 223.43
 ---- batch: 110 ----
mean loss: 228.70
train mean loss: 225.58
epoch train time: 0:00:00.711546
elapsed time: 0:01:37.328171
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:01:43.288590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.71
 ---- batch: 020 ----
mean loss: 232.70
 ---- batch: 030 ----
mean loss: 231.84
 ---- batch: 040 ----
mean loss: 223.32
 ---- batch: 050 ----
mean loss: 224.59
 ---- batch: 060 ----
mean loss: 224.72
 ---- batch: 070 ----
mean loss: 223.57
 ---- batch: 080 ----
mean loss: 220.63
 ---- batch: 090 ----
mean loss: 221.67
 ---- batch: 100 ----
mean loss: 215.85
 ---- batch: 110 ----
mean loss: 226.96
train mean loss: 224.79
epoch train time: 0:00:00.717232
elapsed time: 0:01:38.045582
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:01:44.005968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.40
 ---- batch: 020 ----
mean loss: 226.26
 ---- batch: 030 ----
mean loss: 220.85
 ---- batch: 040 ----
mean loss: 233.60
 ---- batch: 050 ----
mean loss: 214.41
 ---- batch: 060 ----
mean loss: 219.49
 ---- batch: 070 ----
mean loss: 227.61
 ---- batch: 080 ----
mean loss: 220.77
 ---- batch: 090 ----
mean loss: 219.37
 ---- batch: 100 ----
mean loss: 221.01
 ---- batch: 110 ----
mean loss: 230.69
train mean loss: 223.97
epoch train time: 0:00:00.719069
elapsed time: 0:01:38.764782
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:01:44.725168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.59
 ---- batch: 020 ----
mean loss: 225.25
 ---- batch: 030 ----
mean loss: 224.15
 ---- batch: 040 ----
mean loss: 229.60
 ---- batch: 050 ----
mean loss: 220.74
 ---- batch: 060 ----
mean loss: 215.59
 ---- batch: 070 ----
mean loss: 222.90
 ---- batch: 080 ----
mean loss: 218.62
 ---- batch: 090 ----
mean loss: 227.78
 ---- batch: 100 ----
mean loss: 225.29
 ---- batch: 110 ----
mean loss: 222.29
train mean loss: 223.14
epoch train time: 0:00:00.709280
elapsed time: 0:01:39.474195
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:01:45.434575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.30
 ---- batch: 020 ----
mean loss: 227.98
 ---- batch: 030 ----
mean loss: 217.63
 ---- batch: 040 ----
mean loss: 223.34
 ---- batch: 050 ----
mean loss: 218.46
 ---- batch: 060 ----
mean loss: 221.29
 ---- batch: 070 ----
mean loss: 228.00
 ---- batch: 080 ----
mean loss: 227.94
 ---- batch: 090 ----
mean loss: 222.50
 ---- batch: 100 ----
mean loss: 220.69
 ---- batch: 110 ----
mean loss: 220.94
train mean loss: 222.53
epoch train time: 0:00:00.719243
elapsed time: 0:01:40.193575
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:01:46.153969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.70
 ---- batch: 020 ----
mean loss: 223.13
 ---- batch: 030 ----
mean loss: 225.78
 ---- batch: 040 ----
mean loss: 221.43
 ---- batch: 050 ----
mean loss: 224.43
 ---- batch: 060 ----
mean loss: 222.09
 ---- batch: 070 ----
mean loss: 227.17
 ---- batch: 080 ----
mean loss: 220.34
 ---- batch: 090 ----
mean loss: 226.79
 ---- batch: 100 ----
mean loss: 219.41
 ---- batch: 110 ----
mean loss: 214.69
train mean loss: 221.63
epoch train time: 0:00:00.727783
elapsed time: 0:01:40.921509
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:01:46.881896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.22
 ---- batch: 020 ----
mean loss: 223.32
 ---- batch: 030 ----
mean loss: 225.15
 ---- batch: 040 ----
mean loss: 225.11
 ---- batch: 050 ----
mean loss: 225.70
 ---- batch: 060 ----
mean loss: 215.56
 ---- batch: 070 ----
mean loss: 213.35
 ---- batch: 080 ----
mean loss: 216.35
 ---- batch: 090 ----
mean loss: 221.50
 ---- batch: 100 ----
mean loss: 215.64
 ---- batch: 110 ----
mean loss: 223.98
train mean loss: 221.08
epoch train time: 0:00:00.739056
elapsed time: 0:01:41.660701
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:01:47.621084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.15
 ---- batch: 020 ----
mean loss: 213.77
 ---- batch: 030 ----
mean loss: 201.90
 ---- batch: 040 ----
mean loss: 222.94
 ---- batch: 050 ----
mean loss: 235.84
 ---- batch: 060 ----
mean loss: 225.07
 ---- batch: 070 ----
mean loss: 221.62
 ---- batch: 080 ----
mean loss: 219.50
 ---- batch: 090 ----
mean loss: 219.02
 ---- batch: 100 ----
mean loss: 219.80
 ---- batch: 110 ----
mean loss: 227.49
train mean loss: 220.38
epoch train time: 0:00:00.741846
elapsed time: 0:01:42.402681
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:01:48.363085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.62
 ---- batch: 020 ----
mean loss: 212.54
 ---- batch: 030 ----
mean loss: 217.81
 ---- batch: 040 ----
mean loss: 218.27
 ---- batch: 050 ----
mean loss: 231.29
 ---- batch: 060 ----
mean loss: 213.93
 ---- batch: 070 ----
mean loss: 218.34
 ---- batch: 080 ----
mean loss: 227.68
 ---- batch: 090 ----
mean loss: 223.90
 ---- batch: 100 ----
mean loss: 224.26
 ---- batch: 110 ----
mean loss: 219.63
train mean loss: 219.70
epoch train time: 0:00:00.737299
elapsed time: 0:01:43.140214
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:01:49.100734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.18
 ---- batch: 020 ----
mean loss: 219.10
 ---- batch: 030 ----
mean loss: 218.62
 ---- batch: 040 ----
mean loss: 211.95
 ---- batch: 050 ----
mean loss: 219.87
 ---- batch: 060 ----
mean loss: 226.63
 ---- batch: 070 ----
mean loss: 218.60
 ---- batch: 080 ----
mean loss: 230.63
 ---- batch: 090 ----
mean loss: 218.41
 ---- batch: 100 ----
mean loss: 216.32
 ---- batch: 110 ----
mean loss: 216.21
train mean loss: 219.04
epoch train time: 0:00:00.722961
elapsed time: 0:01:43.863462
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:01:49.823846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.14
 ---- batch: 020 ----
mean loss: 230.49
 ---- batch: 030 ----
mean loss: 211.81
 ---- batch: 040 ----
mean loss: 223.78
 ---- batch: 050 ----
mean loss: 212.55
 ---- batch: 060 ----
mean loss: 220.04
 ---- batch: 070 ----
mean loss: 210.06
 ---- batch: 080 ----
mean loss: 208.64
 ---- batch: 090 ----
mean loss: 216.83
 ---- batch: 100 ----
mean loss: 219.07
 ---- batch: 110 ----
mean loss: 226.68
train mean loss: 218.48
epoch train time: 0:00:00.740690
elapsed time: 0:01:44.604296
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:01:50.564682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.72
 ---- batch: 020 ----
mean loss: 217.71
 ---- batch: 030 ----
mean loss: 225.98
 ---- batch: 040 ----
mean loss: 217.49
 ---- batch: 050 ----
mean loss: 219.70
 ---- batch: 060 ----
mean loss: 218.95
 ---- batch: 070 ----
mean loss: 215.20
 ---- batch: 080 ----
mean loss: 217.56
 ---- batch: 090 ----
mean loss: 209.16
 ---- batch: 100 ----
mean loss: 226.27
 ---- batch: 110 ----
mean loss: 218.66
train mean loss: 217.93
epoch train time: 0:00:00.728999
elapsed time: 0:01:45.333446
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:01:51.293828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.71
 ---- batch: 020 ----
mean loss: 221.85
 ---- batch: 030 ----
mean loss: 219.13
 ---- batch: 040 ----
mean loss: 214.75
 ---- batch: 050 ----
mean loss: 220.23
 ---- batch: 060 ----
mean loss: 210.87
 ---- batch: 070 ----
mean loss: 215.55
 ---- batch: 080 ----
mean loss: 220.99
 ---- batch: 090 ----
mean loss: 220.50
 ---- batch: 100 ----
mean loss: 209.12
 ---- batch: 110 ----
mean loss: 216.76
train mean loss: 217.40
epoch train time: 0:00:00.728367
elapsed time: 0:01:46.061951
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:01:52.022351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.29
 ---- batch: 020 ----
mean loss: 218.00
 ---- batch: 030 ----
mean loss: 222.72
 ---- batch: 040 ----
mean loss: 218.04
 ---- batch: 050 ----
mean loss: 215.60
 ---- batch: 060 ----
mean loss: 219.52
 ---- batch: 070 ----
mean loss: 211.50
 ---- batch: 080 ----
mean loss: 220.15
 ---- batch: 090 ----
mean loss: 211.17
 ---- batch: 100 ----
mean loss: 213.96
 ---- batch: 110 ----
mean loss: 211.74
train mean loss: 216.77
epoch train time: 0:00:00.726459
elapsed time: 0:01:46.788558
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:01:52.748950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.68
 ---- batch: 020 ----
mean loss: 214.06
 ---- batch: 030 ----
mean loss: 216.55
 ---- batch: 040 ----
mean loss: 212.16
 ---- batch: 050 ----
mean loss: 207.50
 ---- batch: 060 ----
mean loss: 219.96
 ---- batch: 070 ----
mean loss: 214.69
 ---- batch: 080 ----
mean loss: 211.69
 ---- batch: 090 ----
mean loss: 213.17
 ---- batch: 100 ----
mean loss: 218.35
 ---- batch: 110 ----
mean loss: 223.45
train mean loss: 216.27
epoch train time: 0:00:00.731314
elapsed time: 0:01:47.520015
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:01:53.480424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.89
 ---- batch: 020 ----
mean loss: 217.81
 ---- batch: 030 ----
mean loss: 215.20
 ---- batch: 040 ----
mean loss: 213.43
 ---- batch: 050 ----
mean loss: 215.20
 ---- batch: 060 ----
mean loss: 208.81
 ---- batch: 070 ----
mean loss: 222.25
 ---- batch: 080 ----
mean loss: 211.04
 ---- batch: 090 ----
mean loss: 215.37
 ---- batch: 100 ----
mean loss: 220.04
 ---- batch: 110 ----
mean loss: 223.88
train mean loss: 215.74
epoch train time: 0:00:00.739589
elapsed time: 0:01:48.259765
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:01:54.220148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.77
 ---- batch: 020 ----
mean loss: 213.67
 ---- batch: 030 ----
mean loss: 216.14
 ---- batch: 040 ----
mean loss: 208.21
 ---- batch: 050 ----
mean loss: 206.63
 ---- batch: 060 ----
mean loss: 221.60
 ---- batch: 070 ----
mean loss: 216.48
 ---- batch: 080 ----
mean loss: 212.85
 ---- batch: 090 ----
mean loss: 214.37
 ---- batch: 100 ----
mean loss: 223.74
 ---- batch: 110 ----
mean loss: 213.43
train mean loss: 215.24
epoch train time: 0:00:00.734966
elapsed time: 0:01:48.994940
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:01:54.955319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.47
 ---- batch: 020 ----
mean loss: 225.44
 ---- batch: 030 ----
mean loss: 206.66
 ---- batch: 040 ----
mean loss: 216.38
 ---- batch: 050 ----
mean loss: 216.75
 ---- batch: 060 ----
mean loss: 213.28
 ---- batch: 070 ----
mean loss: 211.37
 ---- batch: 080 ----
mean loss: 217.65
 ---- batch: 090 ----
mean loss: 220.38
 ---- batch: 100 ----
mean loss: 208.11
 ---- batch: 110 ----
mean loss: 218.11
train mean loss: 214.86
epoch train time: 0:00:00.722356
elapsed time: 0:01:49.717430
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:01:55.677816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.43
 ---- batch: 020 ----
mean loss: 218.32
 ---- batch: 030 ----
mean loss: 219.14
 ---- batch: 040 ----
mean loss: 208.78
 ---- batch: 050 ----
mean loss: 223.29
 ---- batch: 060 ----
mean loss: 207.84
 ---- batch: 070 ----
mean loss: 224.11
 ---- batch: 080 ----
mean loss: 214.83
 ---- batch: 090 ----
mean loss: 217.04
 ---- batch: 100 ----
mean loss: 202.49
 ---- batch: 110 ----
mean loss: 206.61
train mean loss: 214.30
epoch train time: 0:00:00.728324
elapsed time: 0:01:50.445900
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:01:56.406285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.80
 ---- batch: 020 ----
mean loss: 219.01
 ---- batch: 030 ----
mean loss: 203.79
 ---- batch: 040 ----
mean loss: 212.20
 ---- batch: 050 ----
mean loss: 215.40
 ---- batch: 060 ----
mean loss: 211.95
 ---- batch: 070 ----
mean loss: 214.78
 ---- batch: 080 ----
mean loss: 212.68
 ---- batch: 090 ----
mean loss: 218.97
 ---- batch: 100 ----
mean loss: 219.90
 ---- batch: 110 ----
mean loss: 215.65
train mean loss: 213.90
epoch train time: 0:00:00.716904
elapsed time: 0:01:51.162965
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:01:57.123349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.99
 ---- batch: 020 ----
mean loss: 206.46
 ---- batch: 030 ----
mean loss: 218.45
 ---- batch: 040 ----
mean loss: 205.50
 ---- batch: 050 ----
mean loss: 217.77
 ---- batch: 060 ----
mean loss: 208.52
 ---- batch: 070 ----
mean loss: 221.13
 ---- batch: 080 ----
mean loss: 214.54
 ---- batch: 090 ----
mean loss: 208.10
 ---- batch: 100 ----
mean loss: 215.69
 ---- batch: 110 ----
mean loss: 219.30
train mean loss: 213.46
epoch train time: 0:00:00.717577
elapsed time: 0:01:51.880686
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:01:57.841101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.18
 ---- batch: 020 ----
mean loss: 213.63
 ---- batch: 030 ----
mean loss: 211.51
 ---- batch: 040 ----
mean loss: 205.32
 ---- batch: 050 ----
mean loss: 220.01
 ---- batch: 060 ----
mean loss: 210.80
 ---- batch: 070 ----
mean loss: 213.39
 ---- batch: 080 ----
mean loss: 221.10
 ---- batch: 090 ----
mean loss: 210.19
 ---- batch: 100 ----
mean loss: 204.87
 ---- batch: 110 ----
mean loss: 216.88
train mean loss: 213.18
epoch train time: 0:00:00.715386
elapsed time: 0:01:52.596244
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:01:58.556638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.46
 ---- batch: 020 ----
mean loss: 208.46
 ---- batch: 030 ----
mean loss: 214.58
 ---- batch: 040 ----
mean loss: 210.95
 ---- batch: 050 ----
mean loss: 204.91
 ---- batch: 060 ----
mean loss: 219.12
 ---- batch: 070 ----
mean loss: 214.43
 ---- batch: 080 ----
mean loss: 216.27
 ---- batch: 090 ----
mean loss: 220.93
 ---- batch: 100 ----
mean loss: 203.96
 ---- batch: 110 ----
mean loss: 213.94
train mean loss: 212.83
epoch train time: 0:00:00.718660
elapsed time: 0:01:53.315052
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:01:59.275436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.93
 ---- batch: 020 ----
mean loss: 200.68
 ---- batch: 030 ----
mean loss: 214.42
 ---- batch: 040 ----
mean loss: 200.18
 ---- batch: 050 ----
mean loss: 213.08
 ---- batch: 060 ----
mean loss: 215.86
 ---- batch: 070 ----
mean loss: 213.58
 ---- batch: 080 ----
mean loss: 216.57
 ---- batch: 090 ----
mean loss: 214.63
 ---- batch: 100 ----
mean loss: 215.61
 ---- batch: 110 ----
mean loss: 218.82
train mean loss: 212.23
epoch train time: 0:00:00.717270
elapsed time: 0:01:54.032457
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:01:59.992842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.69
 ---- batch: 020 ----
mean loss: 222.85
 ---- batch: 030 ----
mean loss: 205.94
 ---- batch: 040 ----
mean loss: 211.71
 ---- batch: 050 ----
mean loss: 218.79
 ---- batch: 060 ----
mean loss: 226.83
 ---- batch: 070 ----
mean loss: 215.55
 ---- batch: 080 ----
mean loss: 203.68
 ---- batch: 090 ----
mean loss: 207.93
 ---- batch: 100 ----
mean loss: 212.87
 ---- batch: 110 ----
mean loss: 207.77
train mean loss: 212.38
epoch train time: 0:00:00.717769
elapsed time: 0:01:54.750369
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:02:00.710748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.66
 ---- batch: 020 ----
mean loss: 218.66
 ---- batch: 030 ----
mean loss: 214.46
 ---- batch: 040 ----
mean loss: 204.48
 ---- batch: 050 ----
mean loss: 211.39
 ---- batch: 060 ----
mean loss: 211.11
 ---- batch: 070 ----
mean loss: 209.62
 ---- batch: 080 ----
mean loss: 217.46
 ---- batch: 090 ----
mean loss: 213.87
 ---- batch: 100 ----
mean loss: 204.44
 ---- batch: 110 ----
mean loss: 205.42
train mean loss: 211.66
epoch train time: 0:00:00.729032
elapsed time: 0:01:55.479541
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:02:01.439944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.69
 ---- batch: 020 ----
mean loss: 212.20
 ---- batch: 030 ----
mean loss: 205.70
 ---- batch: 040 ----
mean loss: 207.52
 ---- batch: 050 ----
mean loss: 208.87
 ---- batch: 060 ----
mean loss: 212.88
 ---- batch: 070 ----
mean loss: 216.91
 ---- batch: 080 ----
mean loss: 216.08
 ---- batch: 090 ----
mean loss: 210.80
 ---- batch: 100 ----
mean loss: 218.05
 ---- batch: 110 ----
mean loss: 207.80
train mean loss: 211.34
epoch train time: 0:00:00.716283
elapsed time: 0:01:56.195978
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:02:02.156361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.19
 ---- batch: 020 ----
mean loss: 213.01
 ---- batch: 030 ----
mean loss: 211.25
 ---- batch: 040 ----
mean loss: 207.77
 ---- batch: 050 ----
mean loss: 208.30
 ---- batch: 060 ----
mean loss: 211.27
 ---- batch: 070 ----
mean loss: 208.56
 ---- batch: 080 ----
mean loss: 220.30
 ---- batch: 090 ----
mean loss: 208.93
 ---- batch: 100 ----
mean loss: 212.30
 ---- batch: 110 ----
mean loss: 203.74
train mean loss: 210.90
epoch train time: 0:00:00.712047
elapsed time: 0:01:56.908163
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:02:02.868589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.26
 ---- batch: 020 ----
mean loss: 209.96
 ---- batch: 030 ----
mean loss: 213.75
 ---- batch: 040 ----
mean loss: 208.90
 ---- batch: 050 ----
mean loss: 211.87
 ---- batch: 060 ----
mean loss: 208.47
 ---- batch: 070 ----
mean loss: 208.60
 ---- batch: 080 ----
mean loss: 212.78
 ---- batch: 090 ----
mean loss: 215.25
 ---- batch: 100 ----
mean loss: 211.67
 ---- batch: 110 ----
mean loss: 209.16
train mean loss: 210.53
epoch train time: 0:00:00.718090
elapsed time: 0:01:57.626448
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:02:03.586837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.33
 ---- batch: 020 ----
mean loss: 205.21
 ---- batch: 030 ----
mean loss: 208.41
 ---- batch: 040 ----
mean loss: 213.11
 ---- batch: 050 ----
mean loss: 212.16
 ---- batch: 060 ----
mean loss: 218.49
 ---- batch: 070 ----
mean loss: 209.15
 ---- batch: 080 ----
mean loss: 209.04
 ---- batch: 090 ----
mean loss: 216.88
 ---- batch: 100 ----
mean loss: 207.81
 ---- batch: 110 ----
mean loss: 209.68
train mean loss: 210.41
epoch train time: 0:00:00.715953
elapsed time: 0:01:58.342541
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:02:04.302935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.98
 ---- batch: 020 ----
mean loss: 196.43
 ---- batch: 030 ----
mean loss: 215.60
 ---- batch: 040 ----
mean loss: 211.90
 ---- batch: 050 ----
mean loss: 207.25
 ---- batch: 060 ----
mean loss: 207.52
 ---- batch: 070 ----
mean loss: 214.48
 ---- batch: 080 ----
mean loss: 200.08
 ---- batch: 090 ----
mean loss: 218.32
 ---- batch: 100 ----
mean loss: 210.74
 ---- batch: 110 ----
mean loss: 211.94
train mean loss: 209.97
epoch train time: 0:00:00.723409
elapsed time: 0:01:59.066141
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:02:05.026541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.99
 ---- batch: 020 ----
mean loss: 207.93
 ---- batch: 030 ----
mean loss: 208.07
 ---- batch: 040 ----
mean loss: 213.28
 ---- batch: 050 ----
mean loss: 203.00
 ---- batch: 060 ----
mean loss: 210.18
 ---- batch: 070 ----
mean loss: 209.39
 ---- batch: 080 ----
mean loss: 207.81
 ---- batch: 090 ----
mean loss: 209.41
 ---- batch: 100 ----
mean loss: 214.79
 ---- batch: 110 ----
mean loss: 211.71
train mean loss: 209.64
epoch train time: 0:00:00.712557
elapsed time: 0:01:59.778848
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:02:05.739230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.31
 ---- batch: 020 ----
mean loss: 216.38
 ---- batch: 030 ----
mean loss: 205.53
 ---- batch: 040 ----
mean loss: 212.52
 ---- batch: 050 ----
mean loss: 215.35
 ---- batch: 060 ----
mean loss: 207.83
 ---- batch: 070 ----
mean loss: 213.80
 ---- batch: 080 ----
mean loss: 212.59
 ---- batch: 090 ----
mean loss: 195.66
 ---- batch: 100 ----
mean loss: 213.16
 ---- batch: 110 ----
mean loss: 200.68
train mean loss: 209.35
epoch train time: 0:00:00.719346
elapsed time: 0:02:00.498323
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:02:06.458705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.38
 ---- batch: 020 ----
mean loss: 202.32
 ---- batch: 030 ----
mean loss: 209.10
 ---- batch: 040 ----
mean loss: 209.64
 ---- batch: 050 ----
mean loss: 208.92
 ---- batch: 060 ----
mean loss: 209.90
 ---- batch: 070 ----
mean loss: 208.19
 ---- batch: 080 ----
mean loss: 211.84
 ---- batch: 090 ----
mean loss: 215.35
 ---- batch: 100 ----
mean loss: 209.63
 ---- batch: 110 ----
mean loss: 202.68
train mean loss: 208.92
epoch train time: 0:00:00.727282
elapsed time: 0:02:01.225739
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:02:07.186156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.00
 ---- batch: 020 ----
mean loss: 206.18
 ---- batch: 030 ----
mean loss: 208.17
 ---- batch: 040 ----
mean loss: 213.93
 ---- batch: 050 ----
mean loss: 218.32
 ---- batch: 060 ----
mean loss: 212.98
 ---- batch: 070 ----
mean loss: 210.90
 ---- batch: 080 ----
mean loss: 205.57
 ---- batch: 090 ----
mean loss: 208.05
 ---- batch: 100 ----
mean loss: 197.82
 ---- batch: 110 ----
mean loss: 205.17
train mean loss: 208.70
epoch train time: 0:00:00.721402
elapsed time: 0:02:01.947305
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:02:07.907687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.89
 ---- batch: 020 ----
mean loss: 202.92
 ---- batch: 030 ----
mean loss: 212.72
 ---- batch: 040 ----
mean loss: 214.74
 ---- batch: 050 ----
mean loss: 202.98
 ---- batch: 060 ----
mean loss: 209.22
 ---- batch: 070 ----
mean loss: 214.92
 ---- batch: 080 ----
mean loss: 213.58
 ---- batch: 090 ----
mean loss: 202.74
 ---- batch: 100 ----
mean loss: 200.94
 ---- batch: 110 ----
mean loss: 211.50
train mean loss: 208.43
epoch train time: 0:00:00.712946
elapsed time: 0:02:02.660382
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:02:08.620768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.38
 ---- batch: 020 ----
mean loss: 204.16
 ---- batch: 030 ----
mean loss: 204.09
 ---- batch: 040 ----
mean loss: 211.75
 ---- batch: 050 ----
mean loss: 207.48
 ---- batch: 060 ----
mean loss: 204.08
 ---- batch: 070 ----
mean loss: 210.62
 ---- batch: 080 ----
mean loss: 211.12
 ---- batch: 090 ----
mean loss: 209.13
 ---- batch: 100 ----
mean loss: 208.40
 ---- batch: 110 ----
mean loss: 206.53
train mean loss: 208.27
epoch train time: 0:00:00.719167
elapsed time: 0:02:03.379701
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:02:09.340078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.23
 ---- batch: 020 ----
mean loss: 207.63
 ---- batch: 030 ----
mean loss: 208.38
 ---- batch: 040 ----
mean loss: 193.63
 ---- batch: 050 ----
mean loss: 219.41
 ---- batch: 060 ----
mean loss: 208.35
 ---- batch: 070 ----
mean loss: 206.56
 ---- batch: 080 ----
mean loss: 207.05
 ---- batch: 090 ----
mean loss: 209.97
 ---- batch: 100 ----
mean loss: 206.13
 ---- batch: 110 ----
mean loss: 207.65
train mean loss: 207.85
epoch train time: 0:00:00.716750
elapsed time: 0:02:04.096580
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:02:10.056978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.18
 ---- batch: 020 ----
mean loss: 203.22
 ---- batch: 030 ----
mean loss: 205.20
 ---- batch: 040 ----
mean loss: 209.45
 ---- batch: 050 ----
mean loss: 213.75
 ---- batch: 060 ----
mean loss: 205.20
 ---- batch: 070 ----
mean loss: 209.12
 ---- batch: 080 ----
mean loss: 210.19
 ---- batch: 090 ----
mean loss: 200.14
 ---- batch: 100 ----
mean loss: 213.18
 ---- batch: 110 ----
mean loss: 198.92
train mean loss: 207.61
epoch train time: 0:00:00.723741
elapsed time: 0:02:04.820469
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:02:10.780878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.80
 ---- batch: 020 ----
mean loss: 214.23
 ---- batch: 030 ----
mean loss: 207.32
 ---- batch: 040 ----
mean loss: 208.23
 ---- batch: 050 ----
mean loss: 212.90
 ---- batch: 060 ----
mean loss: 208.22
 ---- batch: 070 ----
mean loss: 200.57
 ---- batch: 080 ----
mean loss: 209.80
 ---- batch: 090 ----
mean loss: 205.24
 ---- batch: 100 ----
mean loss: 206.46
 ---- batch: 110 ----
mean loss: 205.71
train mean loss: 207.36
epoch train time: 0:00:00.738547
elapsed time: 0:02:05.559175
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:02:11.519558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.50
 ---- batch: 020 ----
mean loss: 205.74
 ---- batch: 030 ----
mean loss: 205.92
 ---- batch: 040 ----
mean loss: 217.89
 ---- batch: 050 ----
mean loss: 202.32
 ---- batch: 060 ----
mean loss: 204.24
 ---- batch: 070 ----
mean loss: 213.29
 ---- batch: 080 ----
mean loss: 210.01
 ---- batch: 090 ----
mean loss: 209.24
 ---- batch: 100 ----
mean loss: 203.07
 ---- batch: 110 ----
mean loss: 200.96
train mean loss: 207.00
epoch train time: 0:00:00.731146
elapsed time: 0:02:06.290459
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:02:12.250864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.41
 ---- batch: 020 ----
mean loss: 206.01
 ---- batch: 030 ----
mean loss: 213.81
 ---- batch: 040 ----
mean loss: 206.65
 ---- batch: 050 ----
mean loss: 207.78
 ---- batch: 060 ----
mean loss: 207.95
 ---- batch: 070 ----
mean loss: 209.20
 ---- batch: 080 ----
mean loss: 203.90
 ---- batch: 090 ----
mean loss: 202.88
 ---- batch: 100 ----
mean loss: 208.66
 ---- batch: 110 ----
mean loss: 202.67
train mean loss: 207.09
epoch train time: 0:00:00.718833
elapsed time: 0:02:07.009448
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:02:12.969831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.86
 ---- batch: 020 ----
mean loss: 213.04
 ---- batch: 030 ----
mean loss: 208.91
 ---- batch: 040 ----
mean loss: 211.65
 ---- batch: 050 ----
mean loss: 204.03
 ---- batch: 060 ----
mean loss: 207.15
 ---- batch: 070 ----
mean loss: 214.81
 ---- batch: 080 ----
mean loss: 203.66
 ---- batch: 090 ----
mean loss: 194.97
 ---- batch: 100 ----
mean loss: 203.99
 ---- batch: 110 ----
mean loss: 203.15
train mean loss: 206.49
epoch train time: 0:00:00.715460
elapsed time: 0:02:07.725044
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:02:13.685437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.77
 ---- batch: 020 ----
mean loss: 201.91
 ---- batch: 030 ----
mean loss: 207.34
 ---- batch: 040 ----
mean loss: 200.51
 ---- batch: 050 ----
mean loss: 206.34
 ---- batch: 060 ----
mean loss: 198.88
 ---- batch: 070 ----
mean loss: 205.15
 ---- batch: 080 ----
mean loss: 205.42
 ---- batch: 090 ----
mean loss: 215.33
 ---- batch: 100 ----
mean loss: 195.02
 ---- batch: 110 ----
mean loss: 209.95
train mean loss: 206.25
epoch train time: 0:00:00.711004
elapsed time: 0:02:08.436193
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:02:14.396580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.74
 ---- batch: 020 ----
mean loss: 209.09
 ---- batch: 030 ----
mean loss: 214.39
 ---- batch: 040 ----
mean loss: 206.00
 ---- batch: 050 ----
mean loss: 205.24
 ---- batch: 060 ----
mean loss: 210.04
 ---- batch: 070 ----
mean loss: 205.81
 ---- batch: 080 ----
mean loss: 199.44
 ---- batch: 090 ----
mean loss: 202.16
 ---- batch: 100 ----
mean loss: 207.53
 ---- batch: 110 ----
mean loss: 200.59
train mean loss: 206.12
epoch train time: 0:00:00.712775
elapsed time: 0:02:09.149106
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:02:15.109488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.67
 ---- batch: 020 ----
mean loss: 213.95
 ---- batch: 030 ----
mean loss: 211.59
 ---- batch: 040 ----
mean loss: 206.81
 ---- batch: 050 ----
mean loss: 202.28
 ---- batch: 060 ----
mean loss: 203.24
 ---- batch: 070 ----
mean loss: 203.01
 ---- batch: 080 ----
mean loss: 202.16
 ---- batch: 090 ----
mean loss: 213.02
 ---- batch: 100 ----
mean loss: 200.32
 ---- batch: 110 ----
mean loss: 204.30
train mean loss: 205.91
epoch train time: 0:00:00.712436
elapsed time: 0:02:09.861676
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:02:15.822061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.94
 ---- batch: 020 ----
mean loss: 211.74
 ---- batch: 030 ----
mean loss: 207.10
 ---- batch: 040 ----
mean loss: 205.25
 ---- batch: 050 ----
mean loss: 199.96
 ---- batch: 060 ----
mean loss: 200.50
 ---- batch: 070 ----
mean loss: 207.60
 ---- batch: 080 ----
mean loss: 202.32
 ---- batch: 090 ----
mean loss: 210.84
 ---- batch: 100 ----
mean loss: 197.14
 ---- batch: 110 ----
mean loss: 200.23
train mean loss: 205.62
epoch train time: 0:00:00.724360
elapsed time: 0:02:10.586168
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:02:16.546591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.61
 ---- batch: 020 ----
mean loss: 210.94
 ---- batch: 030 ----
mean loss: 208.33
 ---- batch: 040 ----
mean loss: 205.78
 ---- batch: 050 ----
mean loss: 210.65
 ---- batch: 060 ----
mean loss: 209.55
 ---- batch: 070 ----
mean loss: 195.81
 ---- batch: 080 ----
mean loss: 207.89
 ---- batch: 090 ----
mean loss: 194.71
 ---- batch: 100 ----
mean loss: 212.38
 ---- batch: 110 ----
mean loss: 206.53
train mean loss: 205.33
epoch train time: 0:00:00.717877
elapsed time: 0:02:11.304232
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:02:17.264615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.97
 ---- batch: 020 ----
mean loss: 211.39
 ---- batch: 030 ----
mean loss: 200.52
 ---- batch: 040 ----
mean loss: 209.50
 ---- batch: 050 ----
mean loss: 193.46
 ---- batch: 060 ----
mean loss: 211.98
 ---- batch: 070 ----
mean loss: 196.09
 ---- batch: 080 ----
mean loss: 207.20
 ---- batch: 090 ----
mean loss: 199.69
 ---- batch: 100 ----
mean loss: 210.90
 ---- batch: 110 ----
mean loss: 201.50
train mean loss: 204.98
epoch train time: 0:00:00.717328
elapsed time: 0:02:12.021696
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:02:17.982080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.22
 ---- batch: 020 ----
mean loss: 213.76
 ---- batch: 030 ----
mean loss: 206.93
 ---- batch: 040 ----
mean loss: 211.01
 ---- batch: 050 ----
mean loss: 206.83
 ---- batch: 060 ----
mean loss: 207.77
 ---- batch: 070 ----
mean loss: 199.11
 ---- batch: 080 ----
mean loss: 198.71
 ---- batch: 090 ----
mean loss: 203.88
 ---- batch: 100 ----
mean loss: 194.28
 ---- batch: 110 ----
mean loss: 208.02
train mean loss: 204.87
epoch train time: 0:00:00.720468
elapsed time: 0:02:12.742335
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:02:18.702722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.64
 ---- batch: 020 ----
mean loss: 203.38
 ---- batch: 030 ----
mean loss: 214.75
 ---- batch: 040 ----
mean loss: 210.41
 ---- batch: 050 ----
mean loss: 211.94
 ---- batch: 060 ----
mean loss: 195.81
 ---- batch: 070 ----
mean loss: 209.54
 ---- batch: 080 ----
mean loss: 203.02
 ---- batch: 090 ----
mean loss: 212.92
 ---- batch: 100 ----
mean loss: 202.20
 ---- batch: 110 ----
mean loss: 193.08
train mean loss: 204.81
epoch train time: 0:00:00.728476
elapsed time: 0:02:13.470949
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:02:19.431332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.83
 ---- batch: 020 ----
mean loss: 212.59
 ---- batch: 030 ----
mean loss: 204.73
 ---- batch: 040 ----
mean loss: 196.70
 ---- batch: 050 ----
mean loss: 199.62
 ---- batch: 060 ----
mean loss: 206.83
 ---- batch: 070 ----
mean loss: 203.58
 ---- batch: 080 ----
mean loss: 208.97
 ---- batch: 090 ----
mean loss: 206.59
 ---- batch: 100 ----
mean loss: 200.55
 ---- batch: 110 ----
mean loss: 200.30
train mean loss: 204.42
epoch train time: 0:00:00.717183
elapsed time: 0:02:14.188263
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:02:20.148646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.03
 ---- batch: 020 ----
mean loss: 215.37
 ---- batch: 030 ----
mean loss: 200.74
 ---- batch: 040 ----
mean loss: 203.10
 ---- batch: 050 ----
mean loss: 200.10
 ---- batch: 060 ----
mean loss: 203.04
 ---- batch: 070 ----
mean loss: 203.43
 ---- batch: 080 ----
mean loss: 202.07
 ---- batch: 090 ----
mean loss: 195.57
 ---- batch: 100 ----
mean loss: 208.93
 ---- batch: 110 ----
mean loss: 212.43
train mean loss: 204.38
epoch train time: 0:00:00.715766
elapsed time: 0:02:14.904172
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:02:20.864557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.03
 ---- batch: 020 ----
mean loss: 198.87
 ---- batch: 030 ----
mean loss: 198.12
 ---- batch: 040 ----
mean loss: 201.82
 ---- batch: 050 ----
mean loss: 211.53
 ---- batch: 060 ----
mean loss: 195.67
 ---- batch: 070 ----
mean loss: 207.34
 ---- batch: 080 ----
mean loss: 213.62
 ---- batch: 090 ----
mean loss: 207.51
 ---- batch: 100 ----
mean loss: 209.63
 ---- batch: 110 ----
mean loss: 194.98
train mean loss: 203.98
epoch train time: 0:00:00.713986
elapsed time: 0:02:15.618293
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:02:21.578696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.15
 ---- batch: 020 ----
mean loss: 202.28
 ---- batch: 030 ----
mean loss: 205.25
 ---- batch: 040 ----
mean loss: 212.39
 ---- batch: 050 ----
mean loss: 201.41
 ---- batch: 060 ----
mean loss: 206.63
 ---- batch: 070 ----
mean loss: 198.60
 ---- batch: 080 ----
mean loss: 209.83
 ---- batch: 090 ----
mean loss: 199.37
 ---- batch: 100 ----
mean loss: 203.99
 ---- batch: 110 ----
mean loss: 201.77
train mean loss: 203.80
epoch train time: 0:00:00.717530
elapsed time: 0:02:16.335978
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:02:22.296359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.70
 ---- batch: 020 ----
mean loss: 210.01
 ---- batch: 030 ----
mean loss: 211.90
 ---- batch: 040 ----
mean loss: 199.33
 ---- batch: 050 ----
mean loss: 207.18
 ---- batch: 060 ----
mean loss: 206.55
 ---- batch: 070 ----
mean loss: 204.53
 ---- batch: 080 ----
mean loss: 202.89
 ---- batch: 090 ----
mean loss: 200.71
 ---- batch: 100 ----
mean loss: 200.68
 ---- batch: 110 ----
mean loss: 205.87
train mean loss: 203.53
epoch train time: 0:00:00.714117
elapsed time: 0:02:17.050228
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:02:23.010612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.85
 ---- batch: 020 ----
mean loss: 208.17
 ---- batch: 030 ----
mean loss: 198.26
 ---- batch: 040 ----
mean loss: 206.15
 ---- batch: 050 ----
mean loss: 197.52
 ---- batch: 060 ----
mean loss: 205.30
 ---- batch: 070 ----
mean loss: 207.24
 ---- batch: 080 ----
mean loss: 199.52
 ---- batch: 090 ----
mean loss: 204.55
 ---- batch: 100 ----
mean loss: 197.51
 ---- batch: 110 ----
mean loss: 205.72
train mean loss: 203.45
epoch train time: 0:00:00.711757
elapsed time: 0:02:17.762116
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:02:23.722499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.61
 ---- batch: 020 ----
mean loss: 208.33
 ---- batch: 030 ----
mean loss: 200.54
 ---- batch: 040 ----
mean loss: 202.64
 ---- batch: 050 ----
mean loss: 209.55
 ---- batch: 060 ----
mean loss: 198.50
 ---- batch: 070 ----
mean loss: 205.12
 ---- batch: 080 ----
mean loss: 202.39
 ---- batch: 090 ----
mean loss: 203.72
 ---- batch: 100 ----
mean loss: 212.01
 ---- batch: 110 ----
mean loss: 192.17
train mean loss: 203.10
epoch train time: 0:00:00.711462
elapsed time: 0:02:18.473735
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:02:24.434122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.37
 ---- batch: 020 ----
mean loss: 205.00
 ---- batch: 030 ----
mean loss: 208.98
 ---- batch: 040 ----
mean loss: 198.24
 ---- batch: 050 ----
mean loss: 206.02
 ---- batch: 060 ----
mean loss: 194.17
 ---- batch: 070 ----
mean loss: 198.73
 ---- batch: 080 ----
mean loss: 211.91
 ---- batch: 090 ----
mean loss: 207.10
 ---- batch: 100 ----
mean loss: 207.97
 ---- batch: 110 ----
mean loss: 194.79
train mean loss: 202.91
epoch train time: 0:00:00.713101
elapsed time: 0:02:19.186983
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:02:25.147356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.09
 ---- batch: 020 ----
mean loss: 198.49
 ---- batch: 030 ----
mean loss: 195.70
 ---- batch: 040 ----
mean loss: 190.23
 ---- batch: 050 ----
mean loss: 200.05
 ---- batch: 060 ----
mean loss: 204.59
 ---- batch: 070 ----
mean loss: 207.60
 ---- batch: 080 ----
mean loss: 207.67
 ---- batch: 090 ----
mean loss: 207.58
 ---- batch: 100 ----
mean loss: 205.05
 ---- batch: 110 ----
mean loss: 202.23
train mean loss: 202.70
epoch train time: 0:00:00.710407
elapsed time: 0:02:19.897532
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:02:25.857917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.19
 ---- batch: 020 ----
mean loss: 200.41
 ---- batch: 030 ----
mean loss: 207.24
 ---- batch: 040 ----
mean loss: 196.78
 ---- batch: 050 ----
mean loss: 203.24
 ---- batch: 060 ----
mean loss: 211.92
 ---- batch: 070 ----
mean loss: 198.58
 ---- batch: 080 ----
mean loss: 205.98
 ---- batch: 090 ----
mean loss: 203.55
 ---- batch: 100 ----
mean loss: 196.06
 ---- batch: 110 ----
mean loss: 200.67
train mean loss: 202.68
epoch train time: 0:00:00.713218
elapsed time: 0:02:20.610883
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:02:26.571280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.14
 ---- batch: 020 ----
mean loss: 197.80
 ---- batch: 030 ----
mean loss: 205.46
 ---- batch: 040 ----
mean loss: 203.93
 ---- batch: 050 ----
mean loss: 202.52
 ---- batch: 060 ----
mean loss: 204.50
 ---- batch: 070 ----
mean loss: 203.51
 ---- batch: 080 ----
mean loss: 200.73
 ---- batch: 090 ----
mean loss: 201.88
 ---- batch: 100 ----
mean loss: 206.97
 ---- batch: 110 ----
mean loss: 196.27
train mean loss: 202.48
epoch train time: 0:00:00.711981
elapsed time: 0:02:21.323006
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:02:27.283387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.80
 ---- batch: 020 ----
mean loss: 200.63
 ---- batch: 030 ----
mean loss: 200.44
 ---- batch: 040 ----
mean loss: 194.01
 ---- batch: 050 ----
mean loss: 205.07
 ---- batch: 060 ----
mean loss: 198.30
 ---- batch: 070 ----
mean loss: 196.03
 ---- batch: 080 ----
mean loss: 209.97
 ---- batch: 090 ----
mean loss: 208.65
 ---- batch: 100 ----
mean loss: 200.81
 ---- batch: 110 ----
mean loss: 196.58
train mean loss: 202.06
epoch train time: 0:00:00.720762
elapsed time: 0:02:22.043904
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:02:28.004288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.08
 ---- batch: 020 ----
mean loss: 191.31
 ---- batch: 030 ----
mean loss: 200.53
 ---- batch: 040 ----
mean loss: 198.10
 ---- batch: 050 ----
mean loss: 206.44
 ---- batch: 060 ----
mean loss: 206.75
 ---- batch: 070 ----
mean loss: 205.29
 ---- batch: 080 ----
mean loss: 200.06
 ---- batch: 090 ----
mean loss: 198.70
 ---- batch: 100 ----
mean loss: 202.31
 ---- batch: 110 ----
mean loss: 204.50
train mean loss: 201.87
epoch train time: 0:00:00.721431
elapsed time: 0:02:22.765468
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:02:28.725852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.16
 ---- batch: 020 ----
mean loss: 196.93
 ---- batch: 030 ----
mean loss: 203.37
 ---- batch: 040 ----
mean loss: 198.58
 ---- batch: 050 ----
mean loss: 200.63
 ---- batch: 060 ----
mean loss: 200.38
 ---- batch: 070 ----
mean loss: 204.36
 ---- batch: 080 ----
mean loss: 199.73
 ---- batch: 090 ----
mean loss: 209.48
 ---- batch: 100 ----
mean loss: 194.96
 ---- batch: 110 ----
mean loss: 208.99
train mean loss: 201.56
epoch train time: 0:00:00.715019
elapsed time: 0:02:23.480627
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:02:29.441012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.60
 ---- batch: 020 ----
mean loss: 201.38
 ---- batch: 030 ----
mean loss: 200.79
 ---- batch: 040 ----
mean loss: 202.48
 ---- batch: 050 ----
mean loss: 194.88
 ---- batch: 060 ----
mean loss: 196.51
 ---- batch: 070 ----
mean loss: 206.08
 ---- batch: 080 ----
mean loss: 199.20
 ---- batch: 090 ----
mean loss: 206.33
 ---- batch: 100 ----
mean loss: 201.11
 ---- batch: 110 ----
mean loss: 206.80
train mean loss: 201.63
epoch train time: 0:00:00.722355
elapsed time: 0:02:24.203117
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:02:30.163497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.24
 ---- batch: 020 ----
mean loss: 195.77
 ---- batch: 030 ----
mean loss: 206.25
 ---- batch: 040 ----
mean loss: 214.35
 ---- batch: 050 ----
mean loss: 198.60
 ---- batch: 060 ----
mean loss: 201.90
 ---- batch: 070 ----
mean loss: 202.44
 ---- batch: 080 ----
mean loss: 197.07
 ---- batch: 090 ----
mean loss: 194.03
 ---- batch: 100 ----
mean loss: 202.84
 ---- batch: 110 ----
mean loss: 198.55
train mean loss: 201.50
epoch train time: 0:00:00.725747
elapsed time: 0:02:24.928995
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:02:30.889396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.20
 ---- batch: 020 ----
mean loss: 200.93
 ---- batch: 030 ----
mean loss: 204.19
 ---- batch: 040 ----
mean loss: 198.93
 ---- batch: 050 ----
mean loss: 191.63
 ---- batch: 060 ----
mean loss: 206.77
 ---- batch: 070 ----
mean loss: 195.64
 ---- batch: 080 ----
mean loss: 201.53
 ---- batch: 090 ----
mean loss: 204.17
 ---- batch: 100 ----
mean loss: 204.34
 ---- batch: 110 ----
mean loss: 206.37
train mean loss: 201.20
epoch train time: 0:00:00.711223
elapsed time: 0:02:25.640386
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:02:31.600768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.96
 ---- batch: 020 ----
mean loss: 197.85
 ---- batch: 030 ----
mean loss: 204.17
 ---- batch: 040 ----
mean loss: 193.51
 ---- batch: 050 ----
mean loss: 200.11
 ---- batch: 060 ----
mean loss: 197.52
 ---- batch: 070 ----
mean loss: 202.03
 ---- batch: 080 ----
mean loss: 196.71
 ---- batch: 090 ----
mean loss: 194.41
 ---- batch: 100 ----
mean loss: 205.55
 ---- batch: 110 ----
mean loss: 208.25
train mean loss: 200.88
epoch train time: 0:00:00.721336
elapsed time: 0:02:26.361860
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:02:32.322244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.89
 ---- batch: 020 ----
mean loss: 199.15
 ---- batch: 030 ----
mean loss: 195.14
 ---- batch: 040 ----
mean loss: 203.86
 ---- batch: 050 ----
mean loss: 203.42
 ---- batch: 060 ----
mean loss: 191.74
 ---- batch: 070 ----
mean loss: 207.86
 ---- batch: 080 ----
mean loss: 198.64
 ---- batch: 090 ----
mean loss: 196.27
 ---- batch: 100 ----
mean loss: 210.97
 ---- batch: 110 ----
mean loss: 198.57
train mean loss: 200.88
epoch train time: 0:00:00.721101
elapsed time: 0:02:27.083096
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:02:33.043481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.28
 ---- batch: 020 ----
mean loss: 198.11
 ---- batch: 030 ----
mean loss: 195.08
 ---- batch: 040 ----
mean loss: 200.32
 ---- batch: 050 ----
mean loss: 217.40
 ---- batch: 060 ----
mean loss: 194.62
 ---- batch: 070 ----
mean loss: 196.62
 ---- batch: 080 ----
mean loss: 209.00
 ---- batch: 090 ----
mean loss: 204.81
 ---- batch: 100 ----
mean loss: 190.74
 ---- batch: 110 ----
mean loss: 195.36
train mean loss: 200.45
epoch train time: 0:00:00.721008
elapsed time: 0:02:27.804259
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:02:33.764650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.66
 ---- batch: 020 ----
mean loss: 202.52
 ---- batch: 030 ----
mean loss: 202.59
 ---- batch: 040 ----
mean loss: 201.34
 ---- batch: 050 ----
mean loss: 211.28
 ---- batch: 060 ----
mean loss: 199.98
 ---- batch: 070 ----
mean loss: 196.34
 ---- batch: 080 ----
mean loss: 198.38
 ---- batch: 090 ----
mean loss: 201.95
 ---- batch: 100 ----
mean loss: 204.77
 ---- batch: 110 ----
mean loss: 197.93
train mean loss: 200.29
epoch train time: 0:00:00.715248
elapsed time: 0:02:28.519660
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:02:34.480045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.53
 ---- batch: 020 ----
mean loss: 205.62
 ---- batch: 030 ----
mean loss: 204.72
 ---- batch: 040 ----
mean loss: 192.53
 ---- batch: 050 ----
mean loss: 208.56
 ---- batch: 060 ----
mean loss: 202.20
 ---- batch: 070 ----
mean loss: 191.69
 ---- batch: 080 ----
mean loss: 183.28
 ---- batch: 090 ----
mean loss: 199.22
 ---- batch: 100 ----
mean loss: 204.92
 ---- batch: 110 ----
mean loss: 205.96
train mean loss: 200.27
epoch train time: 0:00:00.712291
elapsed time: 0:02:29.232087
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:02:35.192470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.79
 ---- batch: 020 ----
mean loss: 202.31
 ---- batch: 030 ----
mean loss: 207.60
 ---- batch: 040 ----
mean loss: 209.39
 ---- batch: 050 ----
mean loss: 199.97
 ---- batch: 060 ----
mean loss: 198.94
 ---- batch: 070 ----
mean loss: 199.80
 ---- batch: 080 ----
mean loss: 200.27
 ---- batch: 090 ----
mean loss: 190.76
 ---- batch: 100 ----
mean loss: 190.93
 ---- batch: 110 ----
mean loss: 198.42
train mean loss: 199.89
epoch train time: 0:00:00.715343
elapsed time: 0:02:29.947576
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:02:35.908017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.43
 ---- batch: 020 ----
mean loss: 204.40
 ---- batch: 030 ----
mean loss: 197.10
 ---- batch: 040 ----
mean loss: 198.93
 ---- batch: 050 ----
mean loss: 204.33
 ---- batch: 060 ----
mean loss: 195.04
 ---- batch: 070 ----
mean loss: 213.19
 ---- batch: 080 ----
mean loss: 198.72
 ---- batch: 090 ----
mean loss: 191.70
 ---- batch: 100 ----
mean loss: 198.88
 ---- batch: 110 ----
mean loss: 197.50
train mean loss: 199.83
epoch train time: 0:00:00.710443
elapsed time: 0:02:30.658209
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:02:36.618591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.54
 ---- batch: 020 ----
mean loss: 192.08
 ---- batch: 030 ----
mean loss: 203.97
 ---- batch: 040 ----
mean loss: 199.90
 ---- batch: 050 ----
mean loss: 212.89
 ---- batch: 060 ----
mean loss: 199.78
 ---- batch: 070 ----
mean loss: 197.98
 ---- batch: 080 ----
mean loss: 201.80
 ---- batch: 090 ----
mean loss: 200.84
 ---- batch: 100 ----
mean loss: 194.15
 ---- batch: 110 ----
mean loss: 190.19
train mean loss: 199.74
epoch train time: 0:00:00.714183
elapsed time: 0:02:31.372523
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:02:37.332922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.71
 ---- batch: 020 ----
mean loss: 201.83
 ---- batch: 030 ----
mean loss: 195.48
 ---- batch: 040 ----
mean loss: 193.72
 ---- batch: 050 ----
mean loss: 193.32
 ---- batch: 060 ----
mean loss: 200.13
 ---- batch: 070 ----
mean loss: 187.29
 ---- batch: 080 ----
mean loss: 210.97
 ---- batch: 090 ----
mean loss: 205.51
 ---- batch: 100 ----
mean loss: 214.48
 ---- batch: 110 ----
mean loss: 193.28
train mean loss: 199.63
epoch train time: 0:00:00.728718
elapsed time: 0:02:32.101392
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:02:38.061779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.08
 ---- batch: 020 ----
mean loss: 197.04
 ---- batch: 030 ----
mean loss: 195.01
 ---- batch: 040 ----
mean loss: 193.03
 ---- batch: 050 ----
mean loss: 203.10
 ---- batch: 060 ----
mean loss: 194.94
 ---- batch: 070 ----
mean loss: 198.68
 ---- batch: 080 ----
mean loss: 195.61
 ---- batch: 090 ----
mean loss: 207.37
 ---- batch: 100 ----
mean loss: 205.56
 ---- batch: 110 ----
mean loss: 201.67
train mean loss: 199.23
epoch train time: 0:00:00.720192
elapsed time: 0:02:32.821725
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:02:38.782113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.13
 ---- batch: 020 ----
mean loss: 205.58
 ---- batch: 030 ----
mean loss: 201.46
 ---- batch: 040 ----
mean loss: 210.43
 ---- batch: 050 ----
mean loss: 208.26
 ---- batch: 060 ----
mean loss: 194.73
 ---- batch: 070 ----
mean loss: 187.85
 ---- batch: 080 ----
mean loss: 199.01
 ---- batch: 090 ----
mean loss: 193.52
 ---- batch: 100 ----
mean loss: 191.20
 ---- batch: 110 ----
mean loss: 192.29
train mean loss: 199.16
epoch train time: 0:00:00.719030
elapsed time: 0:02:33.540892
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:02:39.501276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.64
 ---- batch: 020 ----
mean loss: 201.70
 ---- batch: 030 ----
mean loss: 188.65
 ---- batch: 040 ----
mean loss: 210.44
 ---- batch: 050 ----
mean loss: 203.44
 ---- batch: 060 ----
mean loss: 202.83
 ---- batch: 070 ----
mean loss: 199.10
 ---- batch: 080 ----
mean loss: 203.99
 ---- batch: 090 ----
mean loss: 192.77
 ---- batch: 100 ----
mean loss: 200.57
 ---- batch: 110 ----
mean loss: 192.54
train mean loss: 199.08
epoch train time: 0:00:00.723797
elapsed time: 0:02:34.264841
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:02:40.225273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.15
 ---- batch: 020 ----
mean loss: 204.84
 ---- batch: 030 ----
mean loss: 204.70
 ---- batch: 040 ----
mean loss: 196.70
 ---- batch: 050 ----
mean loss: 193.26
 ---- batch: 060 ----
mean loss: 204.21
 ---- batch: 070 ----
mean loss: 195.79
 ---- batch: 080 ----
mean loss: 199.35
 ---- batch: 090 ----
mean loss: 201.98
 ---- batch: 100 ----
mean loss: 193.18
 ---- batch: 110 ----
mean loss: 193.47
train mean loss: 198.79
epoch train time: 0:00:00.716358
elapsed time: 0:02:34.981385
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:02:40.941768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.22
 ---- batch: 020 ----
mean loss: 209.26
 ---- batch: 030 ----
mean loss: 204.42
 ---- batch: 040 ----
mean loss: 197.70
 ---- batch: 050 ----
mean loss: 197.59
 ---- batch: 060 ----
mean loss: 192.83
 ---- batch: 070 ----
mean loss: 205.09
 ---- batch: 080 ----
mean loss: 195.36
 ---- batch: 090 ----
mean loss: 201.71
 ---- batch: 100 ----
mean loss: 196.09
 ---- batch: 110 ----
mean loss: 198.38
train mean loss: 198.66
epoch train time: 0:00:00.709919
elapsed time: 0:02:35.691472
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:02:41.651857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.94
 ---- batch: 020 ----
mean loss: 206.32
 ---- batch: 030 ----
mean loss: 197.34
 ---- batch: 040 ----
mean loss: 191.75
 ---- batch: 050 ----
mean loss: 198.72
 ---- batch: 060 ----
mean loss: 198.12
 ---- batch: 070 ----
mean loss: 189.38
 ---- batch: 080 ----
mean loss: 200.44
 ---- batch: 090 ----
mean loss: 205.94
 ---- batch: 100 ----
mean loss: 195.14
 ---- batch: 110 ----
mean loss: 203.11
train mean loss: 198.42
epoch train time: 0:00:00.716042
elapsed time: 0:02:36.407647
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:02:42.368031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.02
 ---- batch: 020 ----
mean loss: 197.24
 ---- batch: 030 ----
mean loss: 204.93
 ---- batch: 040 ----
mean loss: 199.81
 ---- batch: 050 ----
mean loss: 191.43
 ---- batch: 060 ----
mean loss: 194.76
 ---- batch: 070 ----
mean loss: 199.48
 ---- batch: 080 ----
mean loss: 194.49
 ---- batch: 090 ----
mean loss: 206.68
 ---- batch: 100 ----
mean loss: 189.93
 ---- batch: 110 ----
mean loss: 201.33
train mean loss: 198.32
epoch train time: 0:00:00.714208
elapsed time: 0:02:37.121999
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:02:43.082372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.13
 ---- batch: 020 ----
mean loss: 207.08
 ---- batch: 030 ----
mean loss: 197.41
 ---- batch: 040 ----
mean loss: 197.64
 ---- batch: 050 ----
mean loss: 204.69
 ---- batch: 060 ----
mean loss: 196.62
 ---- batch: 070 ----
mean loss: 195.32
 ---- batch: 080 ----
mean loss: 188.61
 ---- batch: 090 ----
mean loss: 197.13
 ---- batch: 100 ----
mean loss: 200.20
 ---- batch: 110 ----
mean loss: 196.65
train mean loss: 197.98
epoch train time: 0:00:00.717607
elapsed time: 0:02:37.839735
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:02:43.800139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.63
 ---- batch: 020 ----
mean loss: 197.38
 ---- batch: 030 ----
mean loss: 195.05
 ---- batch: 040 ----
mean loss: 195.63
 ---- batch: 050 ----
mean loss: 193.58
 ---- batch: 060 ----
mean loss: 200.22
 ---- batch: 070 ----
mean loss: 200.24
 ---- batch: 080 ----
mean loss: 194.67
 ---- batch: 090 ----
mean loss: 203.27
 ---- batch: 100 ----
mean loss: 200.32
 ---- batch: 110 ----
mean loss: 192.33
train mean loss: 198.02
epoch train time: 0:00:00.716917
elapsed time: 0:02:38.556804
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:02:44.517187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.57
 ---- batch: 020 ----
mean loss: 199.45
 ---- batch: 030 ----
mean loss: 185.94
 ---- batch: 040 ----
mean loss: 209.55
 ---- batch: 050 ----
mean loss: 202.90
 ---- batch: 060 ----
mean loss: 199.08
 ---- batch: 070 ----
mean loss: 198.00
 ---- batch: 080 ----
mean loss: 199.08
 ---- batch: 090 ----
mean loss: 192.00
 ---- batch: 100 ----
mean loss: 198.27
 ---- batch: 110 ----
mean loss: 192.18
train mean loss: 197.76
epoch train time: 0:00:00.713345
elapsed time: 0:02:39.270294
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:02:45.230692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.30
 ---- batch: 020 ----
mean loss: 210.55
 ---- batch: 030 ----
mean loss: 203.34
 ---- batch: 040 ----
mean loss: 187.37
 ---- batch: 050 ----
mean loss: 196.48
 ---- batch: 060 ----
mean loss: 194.15
 ---- batch: 070 ----
mean loss: 188.24
 ---- batch: 080 ----
mean loss: 200.49
 ---- batch: 090 ----
mean loss: 201.21
 ---- batch: 100 ----
mean loss: 198.30
 ---- batch: 110 ----
mean loss: 188.14
train mean loss: 197.61
epoch train time: 0:00:00.719962
elapsed time: 0:02:39.990404
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:02:45.950793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.59
 ---- batch: 020 ----
mean loss: 206.12
 ---- batch: 030 ----
mean loss: 198.24
 ---- batch: 040 ----
mean loss: 191.47
 ---- batch: 050 ----
mean loss: 190.46
 ---- batch: 060 ----
mean loss: 202.01
 ---- batch: 070 ----
mean loss: 196.41
 ---- batch: 080 ----
mean loss: 192.78
 ---- batch: 090 ----
mean loss: 201.35
 ---- batch: 100 ----
mean loss: 192.73
 ---- batch: 110 ----
mean loss: 203.94
train mean loss: 197.49
epoch train time: 0:00:00.721055
elapsed time: 0:02:40.711608
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:02:46.671996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.06
 ---- batch: 020 ----
mean loss: 190.50
 ---- batch: 030 ----
mean loss: 199.47
 ---- batch: 040 ----
mean loss: 202.14
 ---- batch: 050 ----
mean loss: 204.28
 ---- batch: 060 ----
mean loss: 192.86
 ---- batch: 070 ----
mean loss: 199.89
 ---- batch: 080 ----
mean loss: 198.22
 ---- batch: 090 ----
mean loss: 194.36
 ---- batch: 100 ----
mean loss: 193.48
 ---- batch: 110 ----
mean loss: 207.14
train mean loss: 197.50
epoch train time: 0:00:00.715531
elapsed time: 0:02:41.427272
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:02:47.387655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.87
 ---- batch: 020 ----
mean loss: 204.96
 ---- batch: 030 ----
mean loss: 192.63
 ---- batch: 040 ----
mean loss: 189.03
 ---- batch: 050 ----
mean loss: 202.08
 ---- batch: 060 ----
mean loss: 203.78
 ---- batch: 070 ----
mean loss: 198.29
 ---- batch: 080 ----
mean loss: 200.04
 ---- batch: 090 ----
mean loss: 188.53
 ---- batch: 100 ----
mean loss: 198.12
 ---- batch: 110 ----
mean loss: 199.73
train mean loss: 197.15
epoch train time: 0:00:00.715873
elapsed time: 0:02:42.143292
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:02:48.103678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.46
 ---- batch: 020 ----
mean loss: 197.32
 ---- batch: 030 ----
mean loss: 210.88
 ---- batch: 040 ----
mean loss: 187.90
 ---- batch: 050 ----
mean loss: 193.24
 ---- batch: 060 ----
mean loss: 190.01
 ---- batch: 070 ----
mean loss: 189.88
 ---- batch: 080 ----
mean loss: 195.94
 ---- batch: 090 ----
mean loss: 201.72
 ---- batch: 100 ----
mean loss: 200.69
 ---- batch: 110 ----
mean loss: 198.66
train mean loss: 197.06
epoch train time: 0:00:00.727292
elapsed time: 0:02:42.870727
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:02:48.831116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.74
 ---- batch: 020 ----
mean loss: 202.17
 ---- batch: 030 ----
mean loss: 194.83
 ---- batch: 040 ----
mean loss: 203.00
 ---- batch: 050 ----
mean loss: 194.49
 ---- batch: 060 ----
mean loss: 199.92
 ---- batch: 070 ----
mean loss: 196.07
 ---- batch: 080 ----
mean loss: 191.37
 ---- batch: 090 ----
mean loss: 196.85
 ---- batch: 100 ----
mean loss: 191.54
 ---- batch: 110 ----
mean loss: 192.52
train mean loss: 196.90
epoch train time: 0:00:00.725557
elapsed time: 0:02:43.596442
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:02:49.556854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.89
 ---- batch: 020 ----
mean loss: 200.86
 ---- batch: 030 ----
mean loss: 189.36
 ---- batch: 040 ----
mean loss: 199.63
 ---- batch: 050 ----
mean loss: 185.46
 ---- batch: 060 ----
mean loss: 194.84
 ---- batch: 070 ----
mean loss: 196.99
 ---- batch: 080 ----
mean loss: 196.82
 ---- batch: 090 ----
mean loss: 208.77
 ---- batch: 100 ----
mean loss: 200.36
 ---- batch: 110 ----
mean loss: 193.67
train mean loss: 196.59
epoch train time: 0:00:00.729075
elapsed time: 0:02:44.325728
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:02:50.286144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.65
 ---- batch: 020 ----
mean loss: 199.40
 ---- batch: 030 ----
mean loss: 199.10
 ---- batch: 040 ----
mean loss: 194.00
 ---- batch: 050 ----
mean loss: 200.53
 ---- batch: 060 ----
mean loss: 203.17
 ---- batch: 070 ----
mean loss: 195.10
 ---- batch: 080 ----
mean loss: 195.52
 ---- batch: 090 ----
mean loss: 188.75
 ---- batch: 100 ----
mean loss: 193.43
 ---- batch: 110 ----
mean loss: 201.98
train mean loss: 196.44
epoch train time: 0:00:00.718217
elapsed time: 0:02:45.044114
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:02:51.004500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.90
 ---- batch: 020 ----
mean loss: 187.43
 ---- batch: 030 ----
mean loss: 204.05
 ---- batch: 040 ----
mean loss: 193.00
 ---- batch: 050 ----
mean loss: 198.90
 ---- batch: 060 ----
mean loss: 199.05
 ---- batch: 070 ----
mean loss: 197.42
 ---- batch: 080 ----
mean loss: 190.63
 ---- batch: 090 ----
mean loss: 191.85
 ---- batch: 100 ----
mean loss: 200.71
 ---- batch: 110 ----
mean loss: 203.71
train mean loss: 196.51
epoch train time: 0:00:00.719122
elapsed time: 0:02:45.763376
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:02:51.723766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.55
 ---- batch: 020 ----
mean loss: 206.59
 ---- batch: 030 ----
mean loss: 200.68
 ---- batch: 040 ----
mean loss: 191.44
 ---- batch: 050 ----
mean loss: 194.86
 ---- batch: 060 ----
mean loss: 195.65
 ---- batch: 070 ----
mean loss: 197.68
 ---- batch: 080 ----
mean loss: 193.73
 ---- batch: 090 ----
mean loss: 187.40
 ---- batch: 100 ----
mean loss: 193.32
 ---- batch: 110 ----
mean loss: 197.03
train mean loss: 196.26
epoch train time: 0:00:00.720029
elapsed time: 0:02:46.483546
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:02:52.443931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.81
 ---- batch: 020 ----
mean loss: 193.70
 ---- batch: 030 ----
mean loss: 195.80
 ---- batch: 040 ----
mean loss: 199.34
 ---- batch: 050 ----
mean loss: 197.01
 ---- batch: 060 ----
mean loss: 204.82
 ---- batch: 070 ----
mean loss: 194.35
 ---- batch: 080 ----
mean loss: 190.53
 ---- batch: 090 ----
mean loss: 201.72
 ---- batch: 100 ----
mean loss: 195.56
 ---- batch: 110 ----
mean loss: 195.36
train mean loss: 196.26
epoch train time: 0:00:00.721562
elapsed time: 0:02:47.205258
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:02:53.165663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.23
 ---- batch: 020 ----
mean loss: 196.36
 ---- batch: 030 ----
mean loss: 205.60
 ---- batch: 040 ----
mean loss: 188.10
 ---- batch: 050 ----
mean loss: 197.67
 ---- batch: 060 ----
mean loss: 190.34
 ---- batch: 070 ----
mean loss: 199.28
 ---- batch: 080 ----
mean loss: 192.79
 ---- batch: 090 ----
mean loss: 198.65
 ---- batch: 100 ----
mean loss: 196.54
 ---- batch: 110 ----
mean loss: 193.73
train mean loss: 195.96
epoch train time: 0:00:00.716616
elapsed time: 0:02:47.922026
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:02:53.882410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.35
 ---- batch: 020 ----
mean loss: 198.00
 ---- batch: 030 ----
mean loss: 207.74
 ---- batch: 040 ----
mean loss: 198.89
 ---- batch: 050 ----
mean loss: 194.57
 ---- batch: 060 ----
mean loss: 197.19
 ---- batch: 070 ----
mean loss: 193.42
 ---- batch: 080 ----
mean loss: 202.06
 ---- batch: 090 ----
mean loss: 193.58
 ---- batch: 100 ----
mean loss: 188.72
 ---- batch: 110 ----
mean loss: 187.49
train mean loss: 195.87
epoch train time: 0:00:00.708125
elapsed time: 0:02:48.630284
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:02:54.590696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.97
 ---- batch: 020 ----
mean loss: 192.74
 ---- batch: 030 ----
mean loss: 203.38
 ---- batch: 040 ----
mean loss: 191.99
 ---- batch: 050 ----
mean loss: 191.18
 ---- batch: 060 ----
mean loss: 202.12
 ---- batch: 070 ----
mean loss: 203.85
 ---- batch: 080 ----
mean loss: 191.46
 ---- batch: 090 ----
mean loss: 192.07
 ---- batch: 100 ----
mean loss: 196.94
 ---- batch: 110 ----
mean loss: 203.19
train mean loss: 195.86
epoch train time: 0:00:00.713910
elapsed time: 0:02:49.344354
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:02:55.304750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.66
 ---- batch: 020 ----
mean loss: 189.80
 ---- batch: 030 ----
mean loss: 195.39
 ---- batch: 040 ----
mean loss: 197.08
 ---- batch: 050 ----
mean loss: 205.15
 ---- batch: 060 ----
mean loss: 198.25
 ---- batch: 070 ----
mean loss: 191.28
 ---- batch: 080 ----
mean loss: 190.04
 ---- batch: 090 ----
mean loss: 198.40
 ---- batch: 100 ----
mean loss: 199.14
 ---- batch: 110 ----
mean loss: 190.31
train mean loss: 195.53
epoch train time: 0:00:00.712532
elapsed time: 0:02:50.057044
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:02:56.017432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.35
 ---- batch: 020 ----
mean loss: 193.00
 ---- batch: 030 ----
mean loss: 190.86
 ---- batch: 040 ----
mean loss: 197.51
 ---- batch: 050 ----
mean loss: 202.14
 ---- batch: 060 ----
mean loss: 204.01
 ---- batch: 070 ----
mean loss: 190.77
 ---- batch: 080 ----
mean loss: 193.56
 ---- batch: 090 ----
mean loss: 191.18
 ---- batch: 100 ----
mean loss: 191.33
 ---- batch: 110 ----
mean loss: 194.70
train mean loss: 195.43
epoch train time: 0:00:00.708882
elapsed time: 0:02:50.766082
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:02:56.726481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.81
 ---- batch: 020 ----
mean loss: 200.77
 ---- batch: 030 ----
mean loss: 195.80
 ---- batch: 040 ----
mean loss: 197.94
 ---- batch: 050 ----
mean loss: 201.89
 ---- batch: 060 ----
mean loss: 188.78
 ---- batch: 070 ----
mean loss: 187.67
 ---- batch: 080 ----
mean loss: 192.57
 ---- batch: 090 ----
mean loss: 187.05
 ---- batch: 100 ----
mean loss: 199.33
 ---- batch: 110 ----
mean loss: 202.71
train mean loss: 195.24
epoch train time: 0:00:00.711669
elapsed time: 0:02:51.477901
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:02:57.438283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.11
 ---- batch: 020 ----
mean loss: 203.23
 ---- batch: 030 ----
mean loss: 200.89
 ---- batch: 040 ----
mean loss: 192.05
 ---- batch: 050 ----
mean loss: 187.55
 ---- batch: 060 ----
mean loss: 191.83
 ---- batch: 070 ----
mean loss: 205.02
 ---- batch: 080 ----
mean loss: 190.33
 ---- batch: 090 ----
mean loss: 197.15
 ---- batch: 100 ----
mean loss: 200.29
 ---- batch: 110 ----
mean loss: 186.20
train mean loss: 195.06
epoch train time: 0:00:00.721943
elapsed time: 0:02:52.199974
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:02:58.160356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.48
 ---- batch: 020 ----
mean loss: 203.57
 ---- batch: 030 ----
mean loss: 201.42
 ---- batch: 040 ----
mean loss: 200.70
 ---- batch: 050 ----
mean loss: 185.85
 ---- batch: 060 ----
mean loss: 190.91
 ---- batch: 070 ----
mean loss: 194.87
 ---- batch: 080 ----
mean loss: 190.22
 ---- batch: 090 ----
mean loss: 198.32
 ---- batch: 100 ----
mean loss: 193.49
 ---- batch: 110 ----
mean loss: 200.16
train mean loss: 194.98
epoch train time: 0:00:00.720428
elapsed time: 0:02:52.920533
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:02:58.880934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.55
 ---- batch: 020 ----
mean loss: 200.06
 ---- batch: 030 ----
mean loss: 186.98
 ---- batch: 040 ----
mean loss: 205.33
 ---- batch: 050 ----
mean loss: 191.17
 ---- batch: 060 ----
mean loss: 193.63
 ---- batch: 070 ----
mean loss: 195.93
 ---- batch: 080 ----
mean loss: 195.17
 ---- batch: 090 ----
mean loss: 191.44
 ---- batch: 100 ----
mean loss: 182.70
 ---- batch: 110 ----
mean loss: 201.01
train mean loss: 194.68
epoch train time: 0:00:00.707748
elapsed time: 0:02:53.628446
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:02:59.588848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.32
 ---- batch: 020 ----
mean loss: 193.01
 ---- batch: 030 ----
mean loss: 199.51
 ---- batch: 040 ----
mean loss: 189.05
 ---- batch: 050 ----
mean loss: 200.30
 ---- batch: 060 ----
mean loss: 188.86
 ---- batch: 070 ----
mean loss: 208.76
 ---- batch: 080 ----
mean loss: 200.13
 ---- batch: 090 ----
mean loss: 192.68
 ---- batch: 100 ----
mean loss: 189.94
 ---- batch: 110 ----
mean loss: 190.62
train mean loss: 194.76
epoch train time: 0:00:00.716542
elapsed time: 0:02:54.345147
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:03:00.305532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.87
 ---- batch: 020 ----
mean loss: 191.20
 ---- batch: 030 ----
mean loss: 196.28
 ---- batch: 040 ----
mean loss: 195.03
 ---- batch: 050 ----
mean loss: 190.69
 ---- batch: 060 ----
mean loss: 200.92
 ---- batch: 070 ----
mean loss: 185.72
 ---- batch: 080 ----
mean loss: 192.35
 ---- batch: 090 ----
mean loss: 192.81
 ---- batch: 100 ----
mean loss: 186.09
 ---- batch: 110 ----
mean loss: 203.57
train mean loss: 194.59
epoch train time: 0:00:00.717221
elapsed time: 0:02:55.062503
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:03:01.022887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.54
 ---- batch: 020 ----
mean loss: 188.69
 ---- batch: 030 ----
mean loss: 207.48
 ---- batch: 040 ----
mean loss: 186.30
 ---- batch: 050 ----
mean loss: 197.99
 ---- batch: 060 ----
mean loss: 206.27
 ---- batch: 070 ----
mean loss: 194.20
 ---- batch: 080 ----
mean loss: 193.65
 ---- batch: 090 ----
mean loss: 185.25
 ---- batch: 100 ----
mean loss: 191.77
 ---- batch: 110 ----
mean loss: 194.33
train mean loss: 194.43
epoch train time: 0:00:00.714291
elapsed time: 0:02:55.776977
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:03:01.737359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.19
 ---- batch: 020 ----
mean loss: 192.30
 ---- batch: 030 ----
mean loss: 192.34
 ---- batch: 040 ----
mean loss: 189.24
 ---- batch: 050 ----
mean loss: 197.25
 ---- batch: 060 ----
mean loss: 199.77
 ---- batch: 070 ----
mean loss: 195.97
 ---- batch: 080 ----
mean loss: 184.01
 ---- batch: 090 ----
mean loss: 194.38
 ---- batch: 100 ----
mean loss: 196.69
 ---- batch: 110 ----
mean loss: 202.24
train mean loss: 194.45
epoch train time: 0:00:00.711275
elapsed time: 0:02:56.488380
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:03:02.448764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.44
 ---- batch: 020 ----
mean loss: 192.22
 ---- batch: 030 ----
mean loss: 203.18
 ---- batch: 040 ----
mean loss: 195.51
 ---- batch: 050 ----
mean loss: 189.08
 ---- batch: 060 ----
mean loss: 193.49
 ---- batch: 070 ----
mean loss: 189.55
 ---- batch: 080 ----
mean loss: 195.87
 ---- batch: 090 ----
mean loss: 195.20
 ---- batch: 100 ----
mean loss: 188.12
 ---- batch: 110 ----
mean loss: 192.88
train mean loss: 193.87
epoch train time: 0:00:00.715540
elapsed time: 0:02:57.204073
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:03:03.164448
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.91
 ---- batch: 020 ----
mean loss: 187.58
 ---- batch: 030 ----
mean loss: 186.61
 ---- batch: 040 ----
mean loss: 197.46
 ---- batch: 050 ----
mean loss: 196.24
 ---- batch: 060 ----
mean loss: 193.98
 ---- batch: 070 ----
mean loss: 188.96
 ---- batch: 080 ----
mean loss: 202.86
 ---- batch: 090 ----
mean loss: 198.92
 ---- batch: 100 ----
mean loss: 191.49
 ---- batch: 110 ----
mean loss: 188.01
train mean loss: 193.83
epoch train time: 0:00:00.715756
elapsed time: 0:02:57.919981
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:03:03.880384
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.98
 ---- batch: 020 ----
mean loss: 197.71
 ---- batch: 030 ----
mean loss: 198.37
 ---- batch: 040 ----
mean loss: 195.58
 ---- batch: 050 ----
mean loss: 198.34
 ---- batch: 060 ----
mean loss: 191.11
 ---- batch: 070 ----
mean loss: 185.45
 ---- batch: 080 ----
mean loss: 202.20
 ---- batch: 090 ----
mean loss: 194.46
 ---- batch: 100 ----
mean loss: 191.52
 ---- batch: 110 ----
mean loss: 189.53
train mean loss: 193.77
epoch train time: 0:00:00.706323
elapsed time: 0:02:58.626500
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:03:04.586912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.10
 ---- batch: 020 ----
mean loss: 196.10
 ---- batch: 030 ----
mean loss: 191.39
 ---- batch: 040 ----
mean loss: 193.24
 ---- batch: 050 ----
mean loss: 196.43
 ---- batch: 060 ----
mean loss: 191.06
 ---- batch: 070 ----
mean loss: 197.55
 ---- batch: 080 ----
mean loss: 199.96
 ---- batch: 090 ----
mean loss: 197.26
 ---- batch: 100 ----
mean loss: 186.12
 ---- batch: 110 ----
mean loss: 189.15
train mean loss: 193.75
epoch train time: 0:00:00.722105
elapsed time: 0:02:59.348787
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:03:05.309201
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.05
 ---- batch: 020 ----
mean loss: 190.63
 ---- batch: 030 ----
mean loss: 193.96
 ---- batch: 040 ----
mean loss: 191.04
 ---- batch: 050 ----
mean loss: 194.57
 ---- batch: 060 ----
mean loss: 198.93
 ---- batch: 070 ----
mean loss: 196.49
 ---- batch: 080 ----
mean loss: 199.23
 ---- batch: 090 ----
mean loss: 187.24
 ---- batch: 100 ----
mean loss: 186.53
 ---- batch: 110 ----
mean loss: 200.46
train mean loss: 193.79
epoch train time: 0:00:00.718088
elapsed time: 0:03:00.067051
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:03:06.027432
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.80
 ---- batch: 020 ----
mean loss: 192.64
 ---- batch: 030 ----
mean loss: 192.17
 ---- batch: 040 ----
mean loss: 194.82
 ---- batch: 050 ----
mean loss: 188.35
 ---- batch: 060 ----
mean loss: 195.45
 ---- batch: 070 ----
mean loss: 197.54
 ---- batch: 080 ----
mean loss: 199.17
 ---- batch: 090 ----
mean loss: 191.61
 ---- batch: 100 ----
mean loss: 185.64
 ---- batch: 110 ----
mean loss: 199.17
train mean loss: 193.83
epoch train time: 0:00:00.707780
elapsed time: 0:03:00.774979
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:03:06.735364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.00
 ---- batch: 020 ----
mean loss: 193.08
 ---- batch: 030 ----
mean loss: 197.26
 ---- batch: 040 ----
mean loss: 207.86
 ---- batch: 050 ----
mean loss: 179.89
 ---- batch: 060 ----
mean loss: 196.07
 ---- batch: 070 ----
mean loss: 183.49
 ---- batch: 080 ----
mean loss: 197.75
 ---- batch: 090 ----
mean loss: 188.80
 ---- batch: 100 ----
mean loss: 206.43
 ---- batch: 110 ----
mean loss: 188.68
train mean loss: 193.83
epoch train time: 0:00:00.715150
elapsed time: 0:03:01.490317
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:03:07.450700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.28
 ---- batch: 020 ----
mean loss: 190.23
 ---- batch: 030 ----
mean loss: 185.64
 ---- batch: 040 ----
mean loss: 190.22
 ---- batch: 050 ----
mean loss: 197.44
 ---- batch: 060 ----
mean loss: 206.20
 ---- batch: 070 ----
mean loss: 200.18
 ---- batch: 080 ----
mean loss: 200.72
 ---- batch: 090 ----
mean loss: 191.10
 ---- batch: 100 ----
mean loss: 190.28
 ---- batch: 110 ----
mean loss: 195.70
train mean loss: 193.83
epoch train time: 0:00:00.717844
elapsed time: 0:03:02.208293
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:03:08.168691
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.07
 ---- batch: 020 ----
mean loss: 177.48
 ---- batch: 030 ----
mean loss: 197.65
 ---- batch: 040 ----
mean loss: 189.02
 ---- batch: 050 ----
mean loss: 190.36
 ---- batch: 060 ----
mean loss: 202.14
 ---- batch: 070 ----
mean loss: 193.41
 ---- batch: 080 ----
mean loss: 191.55
 ---- batch: 090 ----
mean loss: 191.95
 ---- batch: 100 ----
mean loss: 193.95
 ---- batch: 110 ----
mean loss: 207.50
train mean loss: 193.73
epoch train time: 0:00:00.722576
elapsed time: 0:03:02.931016
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:03:08.891426
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.91
 ---- batch: 020 ----
mean loss: 191.53
 ---- batch: 030 ----
mean loss: 193.99
 ---- batch: 040 ----
mean loss: 200.74
 ---- batch: 050 ----
mean loss: 196.00
 ---- batch: 060 ----
mean loss: 197.62
 ---- batch: 070 ----
mean loss: 188.95
 ---- batch: 080 ----
mean loss: 186.60
 ---- batch: 090 ----
mean loss: 186.92
 ---- batch: 100 ----
mean loss: 187.54
 ---- batch: 110 ----
mean loss: 194.29
train mean loss: 193.72
epoch train time: 0:00:00.718765
elapsed time: 0:03:03.649993
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:03:09.610377
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.40
 ---- batch: 020 ----
mean loss: 201.21
 ---- batch: 030 ----
mean loss: 192.99
 ---- batch: 040 ----
mean loss: 197.46
 ---- batch: 050 ----
mean loss: 200.62
 ---- batch: 060 ----
mean loss: 192.52
 ---- batch: 070 ----
mean loss: 187.08
 ---- batch: 080 ----
mean loss: 187.03
 ---- batch: 090 ----
mean loss: 199.85
 ---- batch: 100 ----
mean loss: 184.96
 ---- batch: 110 ----
mean loss: 190.47
train mean loss: 193.71
epoch train time: 0:00:00.708776
elapsed time: 0:03:04.358899
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:03:10.319280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.08
 ---- batch: 020 ----
mean loss: 191.86
 ---- batch: 030 ----
mean loss: 192.32
 ---- batch: 040 ----
mean loss: 198.01
 ---- batch: 050 ----
mean loss: 197.20
 ---- batch: 060 ----
mean loss: 196.18
 ---- batch: 070 ----
mean loss: 199.58
 ---- batch: 080 ----
mean loss: 188.94
 ---- batch: 090 ----
mean loss: 193.03
 ---- batch: 100 ----
mean loss: 192.22
 ---- batch: 110 ----
mean loss: 194.04
train mean loss: 193.65
epoch train time: 0:00:00.710699
elapsed time: 0:03:05.069728
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:03:11.030111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.66
 ---- batch: 020 ----
mean loss: 190.64
 ---- batch: 030 ----
mean loss: 183.93
 ---- batch: 040 ----
mean loss: 199.30
 ---- batch: 050 ----
mean loss: 197.74
 ---- batch: 060 ----
mean loss: 198.02
 ---- batch: 070 ----
mean loss: 188.08
 ---- batch: 080 ----
mean loss: 192.43
 ---- batch: 090 ----
mean loss: 184.52
 ---- batch: 100 ----
mean loss: 198.94
 ---- batch: 110 ----
mean loss: 201.13
train mean loss: 193.63
epoch train time: 0:00:00.713977
elapsed time: 0:03:05.783838
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:03:11.744250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.93
 ---- batch: 020 ----
mean loss: 192.18
 ---- batch: 030 ----
mean loss: 194.37
 ---- batch: 040 ----
mean loss: 181.61
 ---- batch: 050 ----
mean loss: 199.12
 ---- batch: 060 ----
mean loss: 192.44
 ---- batch: 070 ----
mean loss: 202.05
 ---- batch: 080 ----
mean loss: 196.01
 ---- batch: 090 ----
mean loss: 188.83
 ---- batch: 100 ----
mean loss: 194.53
 ---- batch: 110 ----
mean loss: 196.97
train mean loss: 193.72
epoch train time: 0:00:00.715960
elapsed time: 0:03:06.499958
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:03:12.460340
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.01
 ---- batch: 020 ----
mean loss: 199.47
 ---- batch: 030 ----
mean loss: 193.34
 ---- batch: 040 ----
mean loss: 198.10
 ---- batch: 050 ----
mean loss: 188.56
 ---- batch: 060 ----
mean loss: 188.90
 ---- batch: 070 ----
mean loss: 201.56
 ---- batch: 080 ----
mean loss: 184.24
 ---- batch: 090 ----
mean loss: 182.63
 ---- batch: 100 ----
mean loss: 190.82
 ---- batch: 110 ----
mean loss: 194.55
train mean loss: 193.67
epoch train time: 0:00:00.712869
elapsed time: 0:03:07.212957
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:03:13.173389
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.97
 ---- batch: 020 ----
mean loss: 195.82
 ---- batch: 030 ----
mean loss: 195.06
 ---- batch: 040 ----
mean loss: 194.11
 ---- batch: 050 ----
mean loss: 192.33
 ---- batch: 060 ----
mean loss: 192.91
 ---- batch: 070 ----
mean loss: 186.99
 ---- batch: 080 ----
mean loss: 196.40
 ---- batch: 090 ----
mean loss: 192.25
 ---- batch: 100 ----
mean loss: 199.25
 ---- batch: 110 ----
mean loss: 191.23
train mean loss: 193.67
epoch train time: 0:00:00.715106
elapsed time: 0:03:07.928253
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:03:13.888645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.15
 ---- batch: 020 ----
mean loss: 191.99
 ---- batch: 030 ----
mean loss: 194.11
 ---- batch: 040 ----
mean loss: 192.82
 ---- batch: 050 ----
mean loss: 191.65
 ---- batch: 060 ----
mean loss: 194.89
 ---- batch: 070 ----
mean loss: 191.54
 ---- batch: 080 ----
mean loss: 198.02
 ---- batch: 090 ----
mean loss: 192.11
 ---- batch: 100 ----
mean loss: 201.13
 ---- batch: 110 ----
mean loss: 193.63
train mean loss: 193.69
epoch train time: 0:00:00.714558
elapsed time: 0:03:08.642950
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:03:14.603332
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.54
 ---- batch: 020 ----
mean loss: 197.60
 ---- batch: 030 ----
mean loss: 199.73
 ---- batch: 040 ----
mean loss: 195.90
 ---- batch: 050 ----
mean loss: 191.80
 ---- batch: 060 ----
mean loss: 195.65
 ---- batch: 070 ----
mean loss: 195.67
 ---- batch: 080 ----
mean loss: 183.69
 ---- batch: 090 ----
mean loss: 192.39
 ---- batch: 100 ----
mean loss: 193.05
 ---- batch: 110 ----
mean loss: 193.49
train mean loss: 193.60
epoch train time: 0:00:00.721589
elapsed time: 0:03:09.364680
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:03:15.325098
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.72
 ---- batch: 020 ----
mean loss: 196.54
 ---- batch: 030 ----
mean loss: 188.75
 ---- batch: 040 ----
mean loss: 191.35
 ---- batch: 050 ----
mean loss: 194.01
 ---- batch: 060 ----
mean loss: 186.84
 ---- batch: 070 ----
mean loss: 192.47
 ---- batch: 080 ----
mean loss: 201.18
 ---- batch: 090 ----
mean loss: 194.94
 ---- batch: 100 ----
mean loss: 195.15
 ---- batch: 110 ----
mean loss: 196.20
train mean loss: 193.61
epoch train time: 0:00:00.722871
elapsed time: 0:03:10.087719
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:03:16.048103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.41
 ---- batch: 020 ----
mean loss: 197.58
 ---- batch: 030 ----
mean loss: 195.95
 ---- batch: 040 ----
mean loss: 193.50
 ---- batch: 050 ----
mean loss: 201.89
 ---- batch: 060 ----
mean loss: 187.89
 ---- batch: 070 ----
mean loss: 194.11
 ---- batch: 080 ----
mean loss: 194.31
 ---- batch: 090 ----
mean loss: 194.33
 ---- batch: 100 ----
mean loss: 195.98
 ---- batch: 110 ----
mean loss: 190.68
train mean loss: 193.55
epoch train time: 0:00:00.716826
elapsed time: 0:03:10.804681
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:03:16.765077
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.10
 ---- batch: 020 ----
mean loss: 186.89
 ---- batch: 030 ----
mean loss: 198.52
 ---- batch: 040 ----
mean loss: 202.38
 ---- batch: 050 ----
mean loss: 186.66
 ---- batch: 060 ----
mean loss: 192.54
 ---- batch: 070 ----
mean loss: 199.71
 ---- batch: 080 ----
mean loss: 197.65
 ---- batch: 090 ----
mean loss: 190.29
 ---- batch: 100 ----
mean loss: 190.68
 ---- batch: 110 ----
mean loss: 196.68
train mean loss: 193.54
epoch train time: 0:00:00.705680
elapsed time: 0:03:11.510505
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:03:17.470886
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.06
 ---- batch: 020 ----
mean loss: 199.89
 ---- batch: 030 ----
mean loss: 204.97
 ---- batch: 040 ----
mean loss: 187.37
 ---- batch: 050 ----
mean loss: 191.67
 ---- batch: 060 ----
mean loss: 190.82
 ---- batch: 070 ----
mean loss: 195.41
 ---- batch: 080 ----
mean loss: 188.33
 ---- batch: 090 ----
mean loss: 188.45
 ---- batch: 100 ----
mean loss: 195.24
 ---- batch: 110 ----
mean loss: 190.26
train mean loss: 193.61
epoch train time: 0:00:00.719215
elapsed time: 0:03:12.229855
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:03:18.190258
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.70
 ---- batch: 020 ----
mean loss: 192.77
 ---- batch: 030 ----
mean loss: 187.70
 ---- batch: 040 ----
mean loss: 197.06
 ---- batch: 050 ----
mean loss: 203.50
 ---- batch: 060 ----
mean loss: 192.17
 ---- batch: 070 ----
mean loss: 190.83
 ---- batch: 080 ----
mean loss: 191.36
 ---- batch: 090 ----
mean loss: 197.10
 ---- batch: 100 ----
mean loss: 193.04
 ---- batch: 110 ----
mean loss: 192.90
train mean loss: 193.56
epoch train time: 0:00:00.717100
elapsed time: 0:03:12.947126
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:03:18.907512
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.54
 ---- batch: 020 ----
mean loss: 194.38
 ---- batch: 030 ----
mean loss: 194.08
 ---- batch: 040 ----
mean loss: 194.57
 ---- batch: 050 ----
mean loss: 192.46
 ---- batch: 060 ----
mean loss: 203.25
 ---- batch: 070 ----
mean loss: 192.33
 ---- batch: 080 ----
mean loss: 194.84
 ---- batch: 090 ----
mean loss: 197.48
 ---- batch: 100 ----
mean loss: 179.90
 ---- batch: 110 ----
mean loss: 191.99
train mean loss: 193.56
epoch train time: 0:00:00.713335
elapsed time: 0:03:13.660596
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:03:19.620979
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.35
 ---- batch: 020 ----
mean loss: 189.57
 ---- batch: 030 ----
mean loss: 198.59
 ---- batch: 040 ----
mean loss: 198.85
 ---- batch: 050 ----
mean loss: 182.90
 ---- batch: 060 ----
mean loss: 195.53
 ---- batch: 070 ----
mean loss: 186.67
 ---- batch: 080 ----
mean loss: 189.10
 ---- batch: 090 ----
mean loss: 199.05
 ---- batch: 100 ----
mean loss: 198.73
 ---- batch: 110 ----
mean loss: 191.26
train mean loss: 193.59
epoch train time: 0:00:00.714795
elapsed time: 0:03:14.375526
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:03:20.335919
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.12
 ---- batch: 020 ----
mean loss: 201.79
 ---- batch: 030 ----
mean loss: 193.94
 ---- batch: 040 ----
mean loss: 190.53
 ---- batch: 050 ----
mean loss: 194.48
 ---- batch: 060 ----
mean loss: 180.25
 ---- batch: 070 ----
mean loss: 200.48
 ---- batch: 080 ----
mean loss: 190.21
 ---- batch: 090 ----
mean loss: 201.18
 ---- batch: 100 ----
mean loss: 198.69
 ---- batch: 110 ----
mean loss: 194.07
train mean loss: 193.47
epoch train time: 0:00:00.718212
elapsed time: 0:03:15.093926
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:03:21.054310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.91
 ---- batch: 020 ----
mean loss: 194.07
 ---- batch: 030 ----
mean loss: 195.78
 ---- batch: 040 ----
mean loss: 190.35
 ---- batch: 050 ----
mean loss: 189.31
 ---- batch: 060 ----
mean loss: 195.44
 ---- batch: 070 ----
mean loss: 200.93
 ---- batch: 080 ----
mean loss: 200.63
 ---- batch: 090 ----
mean loss: 197.33
 ---- batch: 100 ----
mean loss: 195.19
 ---- batch: 110 ----
mean loss: 187.04
train mean loss: 193.49
epoch train time: 0:00:00.716863
elapsed time: 0:03:15.810925
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:03:21.771307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.62
 ---- batch: 020 ----
mean loss: 187.76
 ---- batch: 030 ----
mean loss: 189.25
 ---- batch: 040 ----
mean loss: 195.89
 ---- batch: 050 ----
mean loss: 196.60
 ---- batch: 060 ----
mean loss: 197.67
 ---- batch: 070 ----
mean loss: 200.85
 ---- batch: 080 ----
mean loss: 192.82
 ---- batch: 090 ----
mean loss: 188.34
 ---- batch: 100 ----
mean loss: 192.96
 ---- batch: 110 ----
mean loss: 191.11
train mean loss: 193.51
epoch train time: 0:00:00.722889
elapsed time: 0:03:16.533950
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:03:22.494369
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.81
 ---- batch: 020 ----
mean loss: 195.62
 ---- batch: 030 ----
mean loss: 201.52
 ---- batch: 040 ----
mean loss: 197.20
 ---- batch: 050 ----
mean loss: 192.77
 ---- batch: 060 ----
mean loss: 195.16
 ---- batch: 070 ----
mean loss: 189.81
 ---- batch: 080 ----
mean loss: 183.19
 ---- batch: 090 ----
mean loss: 198.02
 ---- batch: 100 ----
mean loss: 196.51
 ---- batch: 110 ----
mean loss: 193.96
train mean loss: 193.44
epoch train time: 0:00:00.722565
elapsed time: 0:03:17.256685
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:03:23.217070
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.74
 ---- batch: 020 ----
mean loss: 188.54
 ---- batch: 030 ----
mean loss: 187.19
 ---- batch: 040 ----
mean loss: 205.47
 ---- batch: 050 ----
mean loss: 185.78
 ---- batch: 060 ----
mean loss: 190.87
 ---- batch: 070 ----
mean loss: 196.86
 ---- batch: 080 ----
mean loss: 197.72
 ---- batch: 090 ----
mean loss: 190.64
 ---- batch: 100 ----
mean loss: 185.25
 ---- batch: 110 ----
mean loss: 198.33
train mean loss: 193.53
epoch train time: 0:00:00.720568
elapsed time: 0:03:17.977385
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:03:23.937771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.78
 ---- batch: 020 ----
mean loss: 183.68
 ---- batch: 030 ----
mean loss: 185.08
 ---- batch: 040 ----
mean loss: 200.95
 ---- batch: 050 ----
mean loss: 196.70
 ---- batch: 060 ----
mean loss: 184.80
 ---- batch: 070 ----
mean loss: 196.01
 ---- batch: 080 ----
mean loss: 204.68
 ---- batch: 090 ----
mean loss: 189.26
 ---- batch: 100 ----
mean loss: 202.44
 ---- batch: 110 ----
mean loss: 194.02
train mean loss: 193.42
epoch train time: 0:00:00.714922
elapsed time: 0:03:18.692445
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:03:24.652829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.27
 ---- batch: 020 ----
mean loss: 189.06
 ---- batch: 030 ----
mean loss: 199.83
 ---- batch: 040 ----
mean loss: 198.06
 ---- batch: 050 ----
mean loss: 181.28
 ---- batch: 060 ----
mean loss: 199.50
 ---- batch: 070 ----
mean loss: 195.03
 ---- batch: 080 ----
mean loss: 199.59
 ---- batch: 090 ----
mean loss: 194.15
 ---- batch: 100 ----
mean loss: 187.60
 ---- batch: 110 ----
mean loss: 191.32
train mean loss: 193.37
epoch train time: 0:00:00.711957
elapsed time: 0:03:19.404564
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:03:25.364964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.47
 ---- batch: 020 ----
mean loss: 187.15
 ---- batch: 030 ----
mean loss: 187.95
 ---- batch: 040 ----
mean loss: 195.86
 ---- batch: 050 ----
mean loss: 194.07
 ---- batch: 060 ----
mean loss: 202.33
 ---- batch: 070 ----
mean loss: 194.68
 ---- batch: 080 ----
mean loss: 196.26
 ---- batch: 090 ----
mean loss: 191.72
 ---- batch: 100 ----
mean loss: 198.38
 ---- batch: 110 ----
mean loss: 189.18
train mean loss: 193.39
epoch train time: 0:00:00.715310
elapsed time: 0:03:20.120080
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:03:26.080464
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.40
 ---- batch: 020 ----
mean loss: 188.90
 ---- batch: 030 ----
mean loss: 182.78
 ---- batch: 040 ----
mean loss: 192.93
 ---- batch: 050 ----
mean loss: 188.49
 ---- batch: 060 ----
mean loss: 196.86
 ---- batch: 070 ----
mean loss: 191.72
 ---- batch: 080 ----
mean loss: 203.99
 ---- batch: 090 ----
mean loss: 199.85
 ---- batch: 100 ----
mean loss: 195.80
 ---- batch: 110 ----
mean loss: 189.54
train mean loss: 193.37
epoch train time: 0:00:00.713789
elapsed time: 0:03:20.834006
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:03:26.794391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.86
 ---- batch: 020 ----
mean loss: 187.98
 ---- batch: 030 ----
mean loss: 189.37
 ---- batch: 040 ----
mean loss: 200.39
 ---- batch: 050 ----
mean loss: 189.51
 ---- batch: 060 ----
mean loss: 199.73
 ---- batch: 070 ----
mean loss: 197.71
 ---- batch: 080 ----
mean loss: 185.04
 ---- batch: 090 ----
mean loss: 191.36
 ---- batch: 100 ----
mean loss: 197.66
 ---- batch: 110 ----
mean loss: 189.88
train mean loss: 193.39
epoch train time: 0:00:00.712344
elapsed time: 0:03:21.546489
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:03:27.506893
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.89
 ---- batch: 020 ----
mean loss: 195.17
 ---- batch: 030 ----
mean loss: 188.48
 ---- batch: 040 ----
mean loss: 195.43
 ---- batch: 050 ----
mean loss: 192.95
 ---- batch: 060 ----
mean loss: 206.90
 ---- batch: 070 ----
mean loss: 196.42
 ---- batch: 080 ----
mean loss: 188.78
 ---- batch: 090 ----
mean loss: 191.09
 ---- batch: 100 ----
mean loss: 188.86
 ---- batch: 110 ----
mean loss: 191.02
train mean loss: 193.40
epoch train time: 0:00:00.724226
elapsed time: 0:03:22.270898
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:03:28.231281
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.03
 ---- batch: 020 ----
mean loss: 204.62
 ---- batch: 030 ----
mean loss: 193.86
 ---- batch: 040 ----
mean loss: 189.33
 ---- batch: 050 ----
mean loss: 193.62
 ---- batch: 060 ----
mean loss: 195.92
 ---- batch: 070 ----
mean loss: 184.10
 ---- batch: 080 ----
mean loss: 187.35
 ---- batch: 090 ----
mean loss: 200.31
 ---- batch: 100 ----
mean loss: 195.60
 ---- batch: 110 ----
mean loss: 184.88
train mean loss: 193.35
epoch train time: 0:00:00.718851
elapsed time: 0:03:22.989883
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:03:28.950268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.29
 ---- batch: 020 ----
mean loss: 197.21
 ---- batch: 030 ----
mean loss: 192.34
 ---- batch: 040 ----
mean loss: 194.53
 ---- batch: 050 ----
mean loss: 191.91
 ---- batch: 060 ----
mean loss: 193.51
 ---- batch: 070 ----
mean loss: 203.60
 ---- batch: 080 ----
mean loss: 188.61
 ---- batch: 090 ----
mean loss: 184.73
 ---- batch: 100 ----
mean loss: 200.65
 ---- batch: 110 ----
mean loss: 188.85
train mean loss: 193.33
epoch train time: 0:00:00.718371
elapsed time: 0:03:23.708386
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:03:29.668784
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.05
 ---- batch: 020 ----
mean loss: 195.93
 ---- batch: 030 ----
mean loss: 194.79
 ---- batch: 040 ----
mean loss: 198.20
 ---- batch: 050 ----
mean loss: 187.98
 ---- batch: 060 ----
mean loss: 202.60
 ---- batch: 070 ----
mean loss: 196.60
 ---- batch: 080 ----
mean loss: 201.98
 ---- batch: 090 ----
mean loss: 185.69
 ---- batch: 100 ----
mean loss: 192.20
 ---- batch: 110 ----
mean loss: 184.70
train mean loss: 193.35
epoch train time: 0:00:00.714485
elapsed time: 0:03:24.423023
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:03:30.383433
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.19
 ---- batch: 020 ----
mean loss: 187.61
 ---- batch: 030 ----
mean loss: 198.23
 ---- batch: 040 ----
mean loss: 184.08
 ---- batch: 050 ----
mean loss: 192.06
 ---- batch: 060 ----
mean loss: 194.75
 ---- batch: 070 ----
mean loss: 195.20
 ---- batch: 080 ----
mean loss: 192.50
 ---- batch: 090 ----
mean loss: 205.05
 ---- batch: 100 ----
mean loss: 192.08
 ---- batch: 110 ----
mean loss: 198.21
train mean loss: 193.36
epoch train time: 0:00:00.724257
elapsed time: 0:03:25.147441
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:03:31.107827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.99
 ---- batch: 020 ----
mean loss: 189.25
 ---- batch: 030 ----
mean loss: 190.26
 ---- batch: 040 ----
mean loss: 191.91
 ---- batch: 050 ----
mean loss: 199.10
 ---- batch: 060 ----
mean loss: 200.33
 ---- batch: 070 ----
mean loss: 196.23
 ---- batch: 080 ----
mean loss: 193.14
 ---- batch: 090 ----
mean loss: 196.55
 ---- batch: 100 ----
mean loss: 199.80
 ---- batch: 110 ----
mean loss: 187.41
train mean loss: 193.31
epoch train time: 0:00:00.714434
elapsed time: 0:03:25.862028
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:03:31.822411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.05
 ---- batch: 020 ----
mean loss: 208.86
 ---- batch: 030 ----
mean loss: 200.79
 ---- batch: 040 ----
mean loss: 186.62
 ---- batch: 050 ----
mean loss: 189.60
 ---- batch: 060 ----
mean loss: 194.05
 ---- batch: 070 ----
mean loss: 195.88
 ---- batch: 080 ----
mean loss: 179.09
 ---- batch: 090 ----
mean loss: 191.77
 ---- batch: 100 ----
mean loss: 198.66
 ---- batch: 110 ----
mean loss: 188.83
train mean loss: 193.27
epoch train time: 0:00:00.716393
elapsed time: 0:03:26.578579
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:03:32.538983
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.15
 ---- batch: 020 ----
mean loss: 200.31
 ---- batch: 030 ----
mean loss: 186.08
 ---- batch: 040 ----
mean loss: 203.57
 ---- batch: 050 ----
mean loss: 197.46
 ---- batch: 060 ----
mean loss: 190.43
 ---- batch: 070 ----
mean loss: 199.82
 ---- batch: 080 ----
mean loss: 185.35
 ---- batch: 090 ----
mean loss: 185.26
 ---- batch: 100 ----
mean loss: 197.72
 ---- batch: 110 ----
mean loss: 191.40
train mean loss: 193.29
epoch train time: 0:00:00.713833
elapsed time: 0:03:27.292580
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:03:33.252962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.57
 ---- batch: 020 ----
mean loss: 192.75
 ---- batch: 030 ----
mean loss: 190.93
 ---- batch: 040 ----
mean loss: 193.85
 ---- batch: 050 ----
mean loss: 192.26
 ---- batch: 060 ----
mean loss: 188.02
 ---- batch: 070 ----
mean loss: 197.26
 ---- batch: 080 ----
mean loss: 200.75
 ---- batch: 090 ----
mean loss: 193.34
 ---- batch: 100 ----
mean loss: 195.63
 ---- batch: 110 ----
mean loss: 197.78
train mean loss: 193.24
epoch train time: 0:00:00.712479
elapsed time: 0:03:28.005208
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:03:33.965640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.31
 ---- batch: 020 ----
mean loss: 193.21
 ---- batch: 030 ----
mean loss: 193.48
 ---- batch: 040 ----
mean loss: 190.93
 ---- batch: 050 ----
mean loss: 190.07
 ---- batch: 060 ----
mean loss: 197.35
 ---- batch: 070 ----
mean loss: 198.08
 ---- batch: 080 ----
mean loss: 187.97
 ---- batch: 090 ----
mean loss: 191.46
 ---- batch: 100 ----
mean loss: 197.99
 ---- batch: 110 ----
mean loss: 198.92
train mean loss: 193.26
epoch train time: 0:00:00.716046
elapsed time: 0:03:28.721440
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:03:34.681824
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.13
 ---- batch: 020 ----
mean loss: 191.31
 ---- batch: 030 ----
mean loss: 193.43
 ---- batch: 040 ----
mean loss: 191.70
 ---- batch: 050 ----
mean loss: 191.72
 ---- batch: 060 ----
mean loss: 192.21
 ---- batch: 070 ----
mean loss: 193.15
 ---- batch: 080 ----
mean loss: 190.12
 ---- batch: 090 ----
mean loss: 191.63
 ---- batch: 100 ----
mean loss: 195.54
 ---- batch: 110 ----
mean loss: 196.37
train mean loss: 193.29
epoch train time: 0:00:00.716966
elapsed time: 0:03:29.438537
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:03:35.398928
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.88
 ---- batch: 020 ----
mean loss: 201.47
 ---- batch: 030 ----
mean loss: 194.59
 ---- batch: 040 ----
mean loss: 194.28
 ---- batch: 050 ----
mean loss: 189.26
 ---- batch: 060 ----
mean loss: 194.83
 ---- batch: 070 ----
mean loss: 195.79
 ---- batch: 080 ----
mean loss: 189.37
 ---- batch: 090 ----
mean loss: 194.37
 ---- batch: 100 ----
mean loss: 196.67
 ---- batch: 110 ----
mean loss: 199.96
train mean loss: 193.21
epoch train time: 0:00:00.714250
elapsed time: 0:03:30.152922
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:03:36.113302
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.33
 ---- batch: 020 ----
mean loss: 193.90
 ---- batch: 030 ----
mean loss: 188.18
 ---- batch: 040 ----
mean loss: 193.08
 ---- batch: 050 ----
mean loss: 190.67
 ---- batch: 060 ----
mean loss: 190.33
 ---- batch: 070 ----
mean loss: 188.55
 ---- batch: 080 ----
mean loss: 194.67
 ---- batch: 090 ----
mean loss: 193.63
 ---- batch: 100 ----
mean loss: 200.89
 ---- batch: 110 ----
mean loss: 196.98
train mean loss: 193.16
epoch train time: 0:00:00.713357
elapsed time: 0:03:30.866425
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:03:36.826841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.31
 ---- batch: 020 ----
mean loss: 197.56
 ---- batch: 030 ----
mean loss: 191.53
 ---- batch: 040 ----
mean loss: 191.45
 ---- batch: 050 ----
mean loss: 189.94
 ---- batch: 060 ----
mean loss: 197.93
 ---- batch: 070 ----
mean loss: 194.77
 ---- batch: 080 ----
mean loss: 192.09
 ---- batch: 090 ----
mean loss: 191.05
 ---- batch: 100 ----
mean loss: 196.91
 ---- batch: 110 ----
mean loss: 194.56
train mean loss: 193.17
epoch train time: 0:00:00.713772
elapsed time: 0:03:31.582347
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_9/checkpoint.pth.tar
**** end time: 2019-09-27 02:03:37.542699 ****
