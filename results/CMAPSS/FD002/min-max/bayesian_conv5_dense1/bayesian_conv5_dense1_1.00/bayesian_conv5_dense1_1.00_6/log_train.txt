Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_6', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 6476
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 02:06:15.995820 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 02:06:16.014493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2436.18
 ---- batch: 020 ----
mean loss: 1692.59
 ---- batch: 030 ----
mean loss: 1401.83
 ---- batch: 040 ----
mean loss: 1278.11
 ---- batch: 050 ----
mean loss: 1244.65
 ---- batch: 060 ----
mean loss: 1132.62
 ---- batch: 070 ----
mean loss: 1144.01
 ---- batch: 080 ----
mean loss: 1119.27
 ---- batch: 090 ----
mean loss: 1095.11
train mean loss: 1373.31
epoch train time: 0:00:47.503241
elapsed time: 0:00:47.531113
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 02:07:03.526979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1076.71
 ---- batch: 020 ----
mean loss: 1064.34
 ---- batch: 030 ----
mean loss: 1042.92
 ---- batch: 040 ----
mean loss: 1040.54
 ---- batch: 050 ----
mean loss: 1027.08
 ---- batch: 060 ----
mean loss: 1020.52
 ---- batch: 070 ----
mean loss: 1041.67
 ---- batch: 080 ----
mean loss: 1050.14
 ---- batch: 090 ----
mean loss: 1053.69
train mean loss: 1043.95
epoch train time: 0:00:17.012450
elapsed time: 0:01:04.544287
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 02:07:20.540672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1021.60
 ---- batch: 020 ----
mean loss: 1006.02
 ---- batch: 030 ----
mean loss: 1025.76
 ---- batch: 040 ----
mean loss: 1040.66
 ---- batch: 050 ----
mean loss: 1000.58
 ---- batch: 060 ----
mean loss: 1006.21
 ---- batch: 070 ----
mean loss: 1027.24
 ---- batch: 080 ----
mean loss: 977.92
 ---- batch: 090 ----
mean loss: 1025.93
train mean loss: 1013.71
epoch train time: 0:00:16.990291
elapsed time: 0:01:21.535760
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 02:07:37.532136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1023.35
 ---- batch: 020 ----
mean loss: 993.42
 ---- batch: 030 ----
mean loss: 998.59
 ---- batch: 040 ----
mean loss: 1000.10
 ---- batch: 050 ----
mean loss: 1003.81
 ---- batch: 060 ----
mean loss: 991.82
 ---- batch: 070 ----
mean loss: 998.93
 ---- batch: 080 ----
mean loss: 1007.91
 ---- batch: 090 ----
mean loss: 981.15
train mean loss: 1000.09
epoch train time: 0:00:17.032934
elapsed time: 0:01:38.569911
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 02:07:54.566282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 995.98
 ---- batch: 020 ----
mean loss: 975.11
 ---- batch: 030 ----
mean loss: 987.68
 ---- batch: 040 ----
mean loss: 993.05
 ---- batch: 050 ----
mean loss: 968.91
 ---- batch: 060 ----
mean loss: 980.52
 ---- batch: 070 ----
mean loss: 1002.95
 ---- batch: 080 ----
mean loss: 978.85
 ---- batch: 090 ----
mean loss: 960.59
train mean loss: 982.24
epoch train time: 0:00:17.004030
elapsed time: 0:01:55.575106
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 02:08:11.571483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 966.07
 ---- batch: 020 ----
mean loss: 959.61
 ---- batch: 030 ----
mean loss: 974.48
 ---- batch: 040 ----
mean loss: 982.54
 ---- batch: 050 ----
mean loss: 983.38
 ---- batch: 060 ----
mean loss: 940.26
 ---- batch: 070 ----
mean loss: 998.60
 ---- batch: 080 ----
mean loss: 970.47
 ---- batch: 090 ----
mean loss: 962.36
train mean loss: 971.37
epoch train time: 0:00:17.017508
elapsed time: 0:02:12.593920
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 02:08:28.590308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.09
 ---- batch: 020 ----
mean loss: 982.34
 ---- batch: 030 ----
mean loss: 978.45
 ---- batch: 040 ----
mean loss: 977.68
 ---- batch: 050 ----
mean loss: 987.78
 ---- batch: 060 ----
mean loss: 957.32
 ---- batch: 070 ----
mean loss: 974.15
 ---- batch: 080 ----
mean loss: 964.81
 ---- batch: 090 ----
mean loss: 966.89
train mean loss: 968.66
epoch train time: 0:00:17.092297
elapsed time: 0:02:29.687435
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 02:08:45.683808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.92
 ---- batch: 020 ----
mean loss: 973.74
 ---- batch: 030 ----
mean loss: 998.88
 ---- batch: 040 ----
mean loss: 985.67
 ---- batch: 050 ----
mean loss: 954.95
 ---- batch: 060 ----
mean loss: 956.11
 ---- batch: 070 ----
mean loss: 968.69
 ---- batch: 080 ----
mean loss: 953.87
 ---- batch: 090 ----
mean loss: 945.54
train mean loss: 962.02
epoch train time: 0:00:17.139122
elapsed time: 0:02:46.827719
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 02:09:02.824096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 953.85
 ---- batch: 020 ----
mean loss: 944.61
 ---- batch: 030 ----
mean loss: 967.13
 ---- batch: 040 ----
mean loss: 985.08
 ---- batch: 050 ----
mean loss: 929.04
 ---- batch: 060 ----
mean loss: 943.79
 ---- batch: 070 ----
mean loss: 954.38
 ---- batch: 080 ----
mean loss: 943.36
 ---- batch: 090 ----
mean loss: 966.48
train mean loss: 954.53
epoch train time: 0:00:17.155093
elapsed time: 0:03:03.983979
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 02:09:19.980372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 957.60
 ---- batch: 020 ----
mean loss: 950.32
 ---- batch: 030 ----
mean loss: 932.76
 ---- batch: 040 ----
mean loss: 940.36
 ---- batch: 050 ----
mean loss: 954.15
 ---- batch: 060 ----
mean loss: 967.87
 ---- batch: 070 ----
mean loss: 926.73
 ---- batch: 080 ----
mean loss: 933.56
 ---- batch: 090 ----
mean loss: 948.91
train mean loss: 945.16
epoch train time: 0:00:17.008926
elapsed time: 0:03:20.994095
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 02:09:36.990470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.11
 ---- batch: 020 ----
mean loss: 952.23
 ---- batch: 030 ----
mean loss: 959.69
 ---- batch: 040 ----
mean loss: 941.50
 ---- batch: 050 ----
mean loss: 932.78
 ---- batch: 060 ----
mean loss: 935.91
 ---- batch: 070 ----
mean loss: 937.02
 ---- batch: 080 ----
mean loss: 920.43
 ---- batch: 090 ----
mean loss: 939.38
train mean loss: 938.98
epoch train time: 0:00:17.003484
elapsed time: 0:03:37.998730
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 02:09:53.995130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 904.32
 ---- batch: 020 ----
mean loss: 919.82
 ---- batch: 030 ----
mean loss: 935.03
 ---- batch: 040 ----
mean loss: 944.12
 ---- batch: 050 ----
mean loss: 957.30
 ---- batch: 060 ----
mean loss: 933.85
 ---- batch: 070 ----
mean loss: 940.36
 ---- batch: 080 ----
mean loss: 926.35
 ---- batch: 090 ----
mean loss: 925.96
train mean loss: 930.82
epoch train time: 0:00:16.981621
elapsed time: 0:03:54.981535
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 02:10:10.977944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.27
 ---- batch: 020 ----
mean loss: 933.26
 ---- batch: 030 ----
mean loss: 918.38
 ---- batch: 040 ----
mean loss: 898.13
 ---- batch: 050 ----
mean loss: 915.44
 ---- batch: 060 ----
mean loss: 923.00
 ---- batch: 070 ----
mean loss: 908.60
 ---- batch: 080 ----
mean loss: 900.75
 ---- batch: 090 ----
mean loss: 927.80
train mean loss: 917.07
epoch train time: 0:00:16.965436
elapsed time: 0:04:11.948128
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 02:10:27.944496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.71
 ---- batch: 020 ----
mean loss: 926.57
 ---- batch: 030 ----
mean loss: 917.46
 ---- batch: 040 ----
mean loss: 878.77
 ---- batch: 050 ----
mean loss: 905.92
 ---- batch: 060 ----
mean loss: 911.42
 ---- batch: 070 ----
mean loss: 923.43
 ---- batch: 080 ----
mean loss: 894.17
 ---- batch: 090 ----
mean loss: 914.74
train mean loss: 909.07
epoch train time: 0:00:16.979724
elapsed time: 0:04:28.929170
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 02:10:44.925559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.36
 ---- batch: 020 ----
mean loss: 889.66
 ---- batch: 030 ----
mean loss: 895.97
 ---- batch: 040 ----
mean loss: 891.25
 ---- batch: 050 ----
mean loss: 867.70
 ---- batch: 060 ----
mean loss: 872.26
 ---- batch: 070 ----
mean loss: 870.88
 ---- batch: 080 ----
mean loss: 887.09
 ---- batch: 090 ----
mean loss: 876.96
train mean loss: 883.36
epoch train time: 0:00:16.981662
elapsed time: 0:04:45.911999
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 02:11:01.908352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.75
 ---- batch: 020 ----
mean loss: 861.55
 ---- batch: 030 ----
mean loss: 829.53
 ---- batch: 040 ----
mean loss: 826.21
 ---- batch: 050 ----
mean loss: 840.11
 ---- batch: 060 ----
mean loss: 806.41
 ---- batch: 070 ----
mean loss: 774.89
 ---- batch: 080 ----
mean loss: 788.80
 ---- batch: 090 ----
mean loss: 792.25
train mean loss: 815.76
epoch train time: 0:00:16.972133
elapsed time: 0:05:02.885290
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 02:11:18.881702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 780.41
 ---- batch: 020 ----
mean loss: 737.34
 ---- batch: 030 ----
mean loss: 764.26
 ---- batch: 040 ----
mean loss: 749.41
 ---- batch: 050 ----
mean loss: 725.62
 ---- batch: 060 ----
mean loss: 730.58
 ---- batch: 070 ----
mean loss: 744.27
 ---- batch: 080 ----
mean loss: 743.18
 ---- batch: 090 ----
mean loss: 727.18
train mean loss: 742.72
epoch train time: 0:00:16.968912
elapsed time: 0:05:19.855388
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 02:11:35.851867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 728.31
 ---- batch: 020 ----
mean loss: 706.93
 ---- batch: 030 ----
mean loss: 706.13
 ---- batch: 040 ----
mean loss: 710.02
 ---- batch: 050 ----
mean loss: 687.37
 ---- batch: 060 ----
mean loss: 677.82
 ---- batch: 070 ----
mean loss: 683.41
 ---- batch: 080 ----
mean loss: 678.10
 ---- batch: 090 ----
mean loss: 675.66
train mean loss: 693.09
epoch train time: 0:00:16.985049
elapsed time: 0:05:36.841774
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 02:11:52.838255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 696.23
 ---- batch: 020 ----
mean loss: 681.12
 ---- batch: 030 ----
mean loss: 663.15
 ---- batch: 040 ----
mean loss: 679.50
 ---- batch: 050 ----
mean loss: 681.83
 ---- batch: 060 ----
mean loss: 646.27
 ---- batch: 070 ----
mean loss: 628.75
 ---- batch: 080 ----
mean loss: 636.07
 ---- batch: 090 ----
mean loss: 658.53
train mean loss: 662.16
epoch train time: 0:00:17.007925
elapsed time: 0:05:53.850999
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 02:12:09.847365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 625.76
 ---- batch: 020 ----
mean loss: 642.79
 ---- batch: 030 ----
mean loss: 637.95
 ---- batch: 040 ----
mean loss: 631.03
 ---- batch: 050 ----
mean loss: 617.37
 ---- batch: 060 ----
mean loss: 612.16
 ---- batch: 070 ----
mean loss: 610.60
 ---- batch: 080 ----
mean loss: 604.29
 ---- batch: 090 ----
mean loss: 613.44
train mean loss: 620.97
epoch train time: 0:00:16.953533
elapsed time: 0:06:10.805734
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 02:12:26.802126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 593.75
 ---- batch: 020 ----
mean loss: 591.30
 ---- batch: 030 ----
mean loss: 594.51
 ---- batch: 040 ----
mean loss: 583.86
 ---- batch: 050 ----
mean loss: 572.46
 ---- batch: 060 ----
mean loss: 591.70
 ---- batch: 070 ----
mean loss: 569.22
 ---- batch: 080 ----
mean loss: 577.27
 ---- batch: 090 ----
mean loss: 566.78
train mean loss: 582.48
epoch train time: 0:00:16.915380
elapsed time: 0:06:27.722379
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 02:12:43.718780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 565.98
 ---- batch: 020 ----
mean loss: 575.44
 ---- batch: 030 ----
mean loss: 567.05
 ---- batch: 040 ----
mean loss: 543.63
 ---- batch: 050 ----
mean loss: 549.68
 ---- batch: 060 ----
mean loss: 548.59
 ---- batch: 070 ----
mean loss: 541.61
 ---- batch: 080 ----
mean loss: 537.59
 ---- batch: 090 ----
mean loss: 546.46
train mean loss: 551.48
epoch train time: 0:00:16.959184
elapsed time: 0:06:44.682746
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 02:13:00.678982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 539.81
 ---- batch: 020 ----
mean loss: 525.85
 ---- batch: 030 ----
mean loss: 517.81
 ---- batch: 040 ----
mean loss: 522.90
 ---- batch: 050 ----
mean loss: 516.75
 ---- batch: 060 ----
mean loss: 515.14
 ---- batch: 070 ----
mean loss: 522.34
 ---- batch: 080 ----
mean loss: 518.61
 ---- batch: 090 ----
mean loss: 519.61
train mean loss: 520.38
epoch train time: 0:00:17.087898
elapsed time: 0:07:01.771705
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 02:13:17.768083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.63
 ---- batch: 020 ----
mean loss: 508.19
 ---- batch: 030 ----
mean loss: 498.75
 ---- batch: 040 ----
mean loss: 492.08
 ---- batch: 050 ----
mean loss: 498.07
 ---- batch: 060 ----
mean loss: 489.73
 ---- batch: 070 ----
mean loss: 489.70
 ---- batch: 080 ----
mean loss: 495.90
 ---- batch: 090 ----
mean loss: 475.59
train mean loss: 497.17
epoch train time: 0:00:16.982475
elapsed time: 0:07:18.755344
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 02:13:34.751768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.83
 ---- batch: 020 ----
mean loss: 487.97
 ---- batch: 030 ----
mean loss: 473.35
 ---- batch: 040 ----
mean loss: 476.40
 ---- batch: 050 ----
mean loss: 476.28
 ---- batch: 060 ----
mean loss: 457.57
 ---- batch: 070 ----
mean loss: 462.35
 ---- batch: 080 ----
mean loss: 473.06
 ---- batch: 090 ----
mean loss: 464.01
train mean loss: 472.07
epoch train time: 0:00:16.987910
elapsed time: 0:07:35.744446
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 02:13:51.740804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.21
 ---- batch: 020 ----
mean loss: 466.93
 ---- batch: 030 ----
mean loss: 469.65
 ---- batch: 040 ----
mean loss: 455.27
 ---- batch: 050 ----
mean loss: 463.87
 ---- batch: 060 ----
mean loss: 456.44
 ---- batch: 070 ----
mean loss: 447.22
 ---- batch: 080 ----
mean loss: 459.77
 ---- batch: 090 ----
mean loss: 448.26
train mean loss: 456.61
epoch train time: 0:00:17.039956
elapsed time: 0:07:52.785582
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 02:14:08.782001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.95
 ---- batch: 020 ----
mean loss: 454.59
 ---- batch: 030 ----
mean loss: 447.96
 ---- batch: 040 ----
mean loss: 447.44
 ---- batch: 050 ----
mean loss: 431.24
 ---- batch: 060 ----
mean loss: 439.40
 ---- batch: 070 ----
mean loss: 430.65
 ---- batch: 080 ----
mean loss: 443.20
 ---- batch: 090 ----
mean loss: 438.12
train mean loss: 440.61
epoch train time: 0:00:17.060340
elapsed time: 0:08:09.847183
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 02:14:25.843552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.20
 ---- batch: 020 ----
mean loss: 428.94
 ---- batch: 030 ----
mean loss: 423.20
 ---- batch: 040 ----
mean loss: 433.72
 ---- batch: 050 ----
mean loss: 426.19
 ---- batch: 060 ----
mean loss: 413.02
 ---- batch: 070 ----
mean loss: 413.61
 ---- batch: 080 ----
mean loss: 432.77
 ---- batch: 090 ----
mean loss: 414.42
train mean loss: 422.56
epoch train time: 0:00:17.088438
elapsed time: 0:08:26.936863
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 02:14:42.933268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.77
 ---- batch: 020 ----
mean loss: 417.31
 ---- batch: 030 ----
mean loss: 398.27
 ---- batch: 040 ----
mean loss: 413.75
 ---- batch: 050 ----
mean loss: 407.43
 ---- batch: 060 ----
mean loss: 405.25
 ---- batch: 070 ----
mean loss: 425.64
 ---- batch: 080 ----
mean loss: 405.00
 ---- batch: 090 ----
mean loss: 411.78
train mean loss: 410.03
epoch train time: 0:00:17.044005
elapsed time: 0:08:43.982068
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 02:14:59.978426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.97
 ---- batch: 020 ----
mean loss: 391.76
 ---- batch: 030 ----
mean loss: 394.87
 ---- batch: 040 ----
mean loss: 392.01
 ---- batch: 050 ----
mean loss: 411.58
 ---- batch: 060 ----
mean loss: 410.12
 ---- batch: 070 ----
mean loss: 391.77
 ---- batch: 080 ----
mean loss: 395.48
 ---- batch: 090 ----
mean loss: 388.96
train mean loss: 397.24
epoch train time: 0:00:17.062930
elapsed time: 0:09:01.046248
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 02:15:17.042611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.55
 ---- batch: 020 ----
mean loss: 393.92
 ---- batch: 030 ----
mean loss: 392.72
 ---- batch: 040 ----
mean loss: 396.26
 ---- batch: 050 ----
mean loss: 393.05
 ---- batch: 060 ----
mean loss: 387.27
 ---- batch: 070 ----
mean loss: 385.54
 ---- batch: 080 ----
mean loss: 385.02
 ---- batch: 090 ----
mean loss: 377.40
train mean loss: 387.60
epoch train time: 0:00:17.059719
elapsed time: 0:09:18.107223
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 02:15:34.103611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.09
 ---- batch: 020 ----
mean loss: 380.39
 ---- batch: 030 ----
mean loss: 377.10
 ---- batch: 040 ----
mean loss: 365.38
 ---- batch: 050 ----
mean loss: 370.15
 ---- batch: 060 ----
mean loss: 387.34
 ---- batch: 070 ----
mean loss: 371.90
 ---- batch: 080 ----
mean loss: 379.10
 ---- batch: 090 ----
mean loss: 374.04
train mean loss: 377.36
epoch train time: 0:00:17.056899
elapsed time: 0:09:35.165513
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 02:15:51.162001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.66
 ---- batch: 020 ----
mean loss: 374.83
 ---- batch: 030 ----
mean loss: 373.28
 ---- batch: 040 ----
mean loss: 364.77
 ---- batch: 050 ----
mean loss: 368.09
 ---- batch: 060 ----
mean loss: 363.51
 ---- batch: 070 ----
mean loss: 373.59
 ---- batch: 080 ----
mean loss: 359.99
 ---- batch: 090 ----
mean loss: 355.73
train mean loss: 367.23
epoch train time: 0:00:17.064655
elapsed time: 0:09:52.231503
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 02:16:08.227905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.99
 ---- batch: 020 ----
mean loss: 374.10
 ---- batch: 030 ----
mean loss: 368.29
 ---- batch: 040 ----
mean loss: 354.26
 ---- batch: 050 ----
mean loss: 354.74
 ---- batch: 060 ----
mean loss: 362.65
 ---- batch: 070 ----
mean loss: 364.10
 ---- batch: 080 ----
mean loss: 363.20
 ---- batch: 090 ----
mean loss: 360.12
train mean loss: 362.14
epoch train time: 0:00:17.057546
elapsed time: 0:10:09.290300
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 02:16:25.286692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.36
 ---- batch: 020 ----
mean loss: 353.14
 ---- batch: 030 ----
mean loss: 355.25
 ---- batch: 040 ----
mean loss: 357.49
 ---- batch: 050 ----
mean loss: 353.96
 ---- batch: 060 ----
mean loss: 352.81
 ---- batch: 070 ----
mean loss: 356.04
 ---- batch: 080 ----
mean loss: 343.59
 ---- batch: 090 ----
mean loss: 354.36
train mean loss: 353.58
epoch train time: 0:00:16.975391
elapsed time: 0:10:26.266940
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 02:16:42.263438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.26
 ---- batch: 020 ----
mean loss: 353.70
 ---- batch: 030 ----
mean loss: 335.91
 ---- batch: 040 ----
mean loss: 348.19
 ---- batch: 050 ----
mean loss: 340.07
 ---- batch: 060 ----
mean loss: 331.80
 ---- batch: 070 ----
mean loss: 339.96
 ---- batch: 080 ----
mean loss: 339.47
 ---- batch: 090 ----
mean loss: 341.39
train mean loss: 343.93
epoch train time: 0:00:17.046734
elapsed time: 0:10:43.315009
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 02:16:59.311347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.99
 ---- batch: 020 ----
mean loss: 341.48
 ---- batch: 030 ----
mean loss: 335.19
 ---- batch: 040 ----
mean loss: 341.43
 ---- batch: 050 ----
mean loss: 340.64
 ---- batch: 060 ----
mean loss: 323.79
 ---- batch: 070 ----
mean loss: 335.08
 ---- batch: 080 ----
mean loss: 331.49
 ---- batch: 090 ----
mean loss: 338.59
train mean loss: 337.36
epoch train time: 0:00:17.071018
elapsed time: 0:11:00.387194
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 02:17:16.383578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.68
 ---- batch: 020 ----
mean loss: 334.08
 ---- batch: 030 ----
mean loss: 335.92
 ---- batch: 040 ----
mean loss: 340.24
 ---- batch: 050 ----
mean loss: 338.05
 ---- batch: 060 ----
mean loss: 329.83
 ---- batch: 070 ----
mean loss: 336.90
 ---- batch: 080 ----
mean loss: 335.33
 ---- batch: 090 ----
mean loss: 325.05
train mean loss: 335.24
epoch train time: 0:00:17.089215
elapsed time: 0:11:17.477588
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 02:17:33.474017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.33
 ---- batch: 020 ----
mean loss: 317.35
 ---- batch: 030 ----
mean loss: 329.91
 ---- batch: 040 ----
mean loss: 333.03
 ---- batch: 050 ----
mean loss: 326.57
 ---- batch: 060 ----
mean loss: 327.16
 ---- batch: 070 ----
mean loss: 323.58
 ---- batch: 080 ----
mean loss: 336.08
 ---- batch: 090 ----
mean loss: 318.81
train mean loss: 325.35
epoch train time: 0:00:17.103873
elapsed time: 0:11:34.582751
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 02:17:50.579175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.29
 ---- batch: 020 ----
mean loss: 318.86
 ---- batch: 030 ----
mean loss: 321.77
 ---- batch: 040 ----
mean loss: 326.03
 ---- batch: 050 ----
mean loss: 326.53
 ---- batch: 060 ----
mean loss: 312.15
 ---- batch: 070 ----
mean loss: 320.45
 ---- batch: 080 ----
mean loss: 319.66
 ---- batch: 090 ----
mean loss: 313.25
train mean loss: 318.98
epoch train time: 0:00:17.116394
elapsed time: 0:11:51.700363
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 02:18:07.696725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.43
 ---- batch: 020 ----
mean loss: 320.15
 ---- batch: 030 ----
mean loss: 308.27
 ---- batch: 040 ----
mean loss: 313.50
 ---- batch: 050 ----
mean loss: 314.90
 ---- batch: 060 ----
mean loss: 319.74
 ---- batch: 070 ----
mean loss: 314.06
 ---- batch: 080 ----
mean loss: 320.15
 ---- batch: 090 ----
mean loss: 314.18
train mean loss: 316.71
epoch train time: 0:00:17.161828
elapsed time: 0:12:08.863339
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 02:18:24.859734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.24
 ---- batch: 020 ----
mean loss: 305.66
 ---- batch: 030 ----
mean loss: 314.44
 ---- batch: 040 ----
mean loss: 313.91
 ---- batch: 050 ----
mean loss: 315.71
 ---- batch: 060 ----
mean loss: 316.02
 ---- batch: 070 ----
mean loss: 311.37
 ---- batch: 080 ----
mean loss: 311.07
 ---- batch: 090 ----
mean loss: 316.79
train mean loss: 312.37
epoch train time: 0:00:17.112703
elapsed time: 0:12:25.977205
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 02:18:41.973692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.33
 ---- batch: 020 ----
mean loss: 302.45
 ---- batch: 030 ----
mean loss: 310.44
 ---- batch: 040 ----
mean loss: 306.13
 ---- batch: 050 ----
mean loss: 326.28
 ---- batch: 060 ----
mean loss: 310.83
 ---- batch: 070 ----
mean loss: 304.83
 ---- batch: 080 ----
mean loss: 316.36
 ---- batch: 090 ----
mean loss: 308.59
train mean loss: 310.33
epoch train time: 0:00:17.141982
elapsed time: 0:12:43.120518
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 02:18:59.116921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.87
 ---- batch: 020 ----
mean loss: 314.68
 ---- batch: 030 ----
mean loss: 302.33
 ---- batch: 040 ----
mean loss: 311.18
 ---- batch: 050 ----
mean loss: 294.45
 ---- batch: 060 ----
mean loss: 299.56
 ---- batch: 070 ----
mean loss: 299.20
 ---- batch: 080 ----
mean loss: 300.67
 ---- batch: 090 ----
mean loss: 302.31
train mean loss: 302.33
epoch train time: 0:00:17.147099
elapsed time: 0:13:00.268795
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 02:19:16.265159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.13
 ---- batch: 020 ----
mean loss: 283.53
 ---- batch: 030 ----
mean loss: 316.40
 ---- batch: 040 ----
mean loss: 297.98
 ---- batch: 050 ----
mean loss: 296.80
 ---- batch: 060 ----
mean loss: 301.51
 ---- batch: 070 ----
mean loss: 303.09
 ---- batch: 080 ----
mean loss: 299.77
 ---- batch: 090 ----
mean loss: 295.90
train mean loss: 299.29
epoch train time: 0:00:17.122402
elapsed time: 0:13:17.392442
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 02:19:33.388855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.06
 ---- batch: 020 ----
mean loss: 301.33
 ---- batch: 030 ----
mean loss: 287.42
 ---- batch: 040 ----
mean loss: 293.66
 ---- batch: 050 ----
mean loss: 289.84
 ---- batch: 060 ----
mean loss: 292.41
 ---- batch: 070 ----
mean loss: 292.58
 ---- batch: 080 ----
mean loss: 292.90
 ---- batch: 090 ----
mean loss: 298.93
train mean loss: 294.18
epoch train time: 0:00:17.085392
elapsed time: 0:13:34.479075
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 02:19:50.475549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.75
 ---- batch: 020 ----
mean loss: 295.85
 ---- batch: 030 ----
mean loss: 303.07
 ---- batch: 040 ----
mean loss: 298.70
 ---- batch: 050 ----
mean loss: 287.08
 ---- batch: 060 ----
mean loss: 291.90
 ---- batch: 070 ----
mean loss: 286.39
 ---- batch: 080 ----
mean loss: 287.36
 ---- batch: 090 ----
mean loss: 287.16
train mean loss: 292.63
epoch train time: 0:00:17.127367
elapsed time: 0:13:51.607762
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 02:20:07.604152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.32
 ---- batch: 020 ----
mean loss: 293.42
 ---- batch: 030 ----
mean loss: 288.32
 ---- batch: 040 ----
mean loss: 284.18
 ---- batch: 050 ----
mean loss: 290.89
 ---- batch: 060 ----
mean loss: 287.97
 ---- batch: 070 ----
mean loss: 290.73
 ---- batch: 080 ----
mean loss: 280.08
 ---- batch: 090 ----
mean loss: 281.01
train mean loss: 288.36
epoch train time: 0:00:17.083655
elapsed time: 0:14:08.692580
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 02:20:24.689003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.08
 ---- batch: 020 ----
mean loss: 280.05
 ---- batch: 030 ----
mean loss: 291.06
 ---- batch: 040 ----
mean loss: 283.42
 ---- batch: 050 ----
mean loss: 282.50
 ---- batch: 060 ----
mean loss: 282.88
 ---- batch: 070 ----
mean loss: 289.88
 ---- batch: 080 ----
mean loss: 292.73
 ---- batch: 090 ----
mean loss: 288.71
train mean loss: 286.22
epoch train time: 0:00:17.060043
elapsed time: 0:14:25.753874
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 02:20:41.750283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.24
 ---- batch: 020 ----
mean loss: 275.21
 ---- batch: 030 ----
mean loss: 294.66
 ---- batch: 040 ----
mean loss: 271.39
 ---- batch: 050 ----
mean loss: 284.34
 ---- batch: 060 ----
mean loss: 295.17
 ---- batch: 070 ----
mean loss: 280.12
 ---- batch: 080 ----
mean loss: 286.58
 ---- batch: 090 ----
mean loss: 282.33
train mean loss: 283.20
epoch train time: 0:00:17.135082
elapsed time: 0:14:42.890157
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 02:20:58.886519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.38
 ---- batch: 020 ----
mean loss: 274.84
 ---- batch: 030 ----
mean loss: 280.15
 ---- batch: 040 ----
mean loss: 272.49
 ---- batch: 050 ----
mean loss: 282.72
 ---- batch: 060 ----
mean loss: 286.88
 ---- batch: 070 ----
mean loss: 280.73
 ---- batch: 080 ----
mean loss: 274.86
 ---- batch: 090 ----
mean loss: 275.61
train mean loss: 278.93
epoch train time: 0:00:17.092329
elapsed time: 0:14:59.983656
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 02:21:15.980059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.95
 ---- batch: 020 ----
mean loss: 270.22
 ---- batch: 030 ----
mean loss: 281.58
 ---- batch: 040 ----
mean loss: 284.09
 ---- batch: 050 ----
mean loss: 268.93
 ---- batch: 060 ----
mean loss: 276.29
 ---- batch: 070 ----
mean loss: 267.29
 ---- batch: 080 ----
mean loss: 275.96
 ---- batch: 090 ----
mean loss: 281.45
train mean loss: 276.41
epoch train time: 0:00:17.114667
elapsed time: 0:15:17.099521
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 02:21:33.095882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.07
 ---- batch: 020 ----
mean loss: 278.02
 ---- batch: 030 ----
mean loss: 277.16
 ---- batch: 040 ----
mean loss: 276.37
 ---- batch: 050 ----
mean loss: 276.36
 ---- batch: 060 ----
mean loss: 278.82
 ---- batch: 070 ----
mean loss: 262.99
 ---- batch: 080 ----
mean loss: 266.10
 ---- batch: 090 ----
mean loss: 280.85
train mean loss: 275.16
epoch train time: 0:00:17.089279
elapsed time: 0:15:34.190073
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 02:21:50.186469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.04
 ---- batch: 020 ----
mean loss: 274.32
 ---- batch: 030 ----
mean loss: 281.37
 ---- batch: 040 ----
mean loss: 273.63
 ---- batch: 050 ----
mean loss: 269.98
 ---- batch: 060 ----
mean loss: 269.43
 ---- batch: 070 ----
mean loss: 266.37
 ---- batch: 080 ----
mean loss: 281.12
 ---- batch: 090 ----
mean loss: 263.97
train mean loss: 272.31
epoch train time: 0:00:17.086880
elapsed time: 0:15:51.278349
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 02:22:07.274842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.40
 ---- batch: 020 ----
mean loss: 269.13
 ---- batch: 030 ----
mean loss: 269.14
 ---- batch: 040 ----
mean loss: 266.72
 ---- batch: 050 ----
mean loss: 271.17
 ---- batch: 060 ----
mean loss: 263.68
 ---- batch: 070 ----
mean loss: 270.90
 ---- batch: 080 ----
mean loss: 257.90
 ---- batch: 090 ----
mean loss: 269.21
train mean loss: 268.09
epoch train time: 0:00:17.011068
elapsed time: 0:16:08.290743
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 02:22:24.287228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.11
 ---- batch: 020 ----
mean loss: 264.86
 ---- batch: 030 ----
mean loss: 269.25
 ---- batch: 040 ----
mean loss: 270.93
 ---- batch: 050 ----
mean loss: 262.22
 ---- batch: 060 ----
mean loss: 269.46
 ---- batch: 070 ----
mean loss: 270.49
 ---- batch: 080 ----
mean loss: 260.72
 ---- batch: 090 ----
mean loss: 261.48
train mean loss: 267.28
epoch train time: 0:00:17.047144
elapsed time: 0:16:25.339272
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 02:22:41.335705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.86
 ---- batch: 020 ----
mean loss: 260.21
 ---- batch: 030 ----
mean loss: 270.93
 ---- batch: 040 ----
mean loss: 267.08
 ---- batch: 050 ----
mean loss: 263.55
 ---- batch: 060 ----
mean loss: 263.44
 ---- batch: 070 ----
mean loss: 270.43
 ---- batch: 080 ----
mean loss: 262.66
 ---- batch: 090 ----
mean loss: 263.79
train mean loss: 264.93
epoch train time: 0:00:17.037993
elapsed time: 0:16:42.378514
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 02:22:58.374892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.18
 ---- batch: 020 ----
mean loss: 263.68
 ---- batch: 030 ----
mean loss: 262.77
 ---- batch: 040 ----
mean loss: 256.54
 ---- batch: 050 ----
mean loss: 265.77
 ---- batch: 060 ----
mean loss: 268.44
 ---- batch: 070 ----
mean loss: 255.26
 ---- batch: 080 ----
mean loss: 261.11
 ---- batch: 090 ----
mean loss: 263.85
train mean loss: 261.91
epoch train time: 0:00:17.069459
elapsed time: 0:16:59.449195
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 02:23:15.445622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.43
 ---- batch: 020 ----
mean loss: 258.49
 ---- batch: 030 ----
mean loss: 260.05
 ---- batch: 040 ----
mean loss: 263.11
 ---- batch: 050 ----
mean loss: 257.63
 ---- batch: 060 ----
mean loss: 263.03
 ---- batch: 070 ----
mean loss: 259.22
 ---- batch: 080 ----
mean loss: 261.57
 ---- batch: 090 ----
mean loss: 261.13
train mean loss: 261.10
epoch train time: 0:00:17.138419
elapsed time: 0:17:16.588809
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 02:23:32.585196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.09
 ---- batch: 020 ----
mean loss: 257.93
 ---- batch: 030 ----
mean loss: 253.37
 ---- batch: 040 ----
mean loss: 261.99
 ---- batch: 050 ----
mean loss: 262.46
 ---- batch: 060 ----
mean loss: 258.12
 ---- batch: 070 ----
mean loss: 258.04
 ---- batch: 080 ----
mean loss: 250.77
 ---- batch: 090 ----
mean loss: 261.32
train mean loss: 258.32
epoch train time: 0:00:17.004009
elapsed time: 0:17:33.594022
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 02:23:49.590400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.33
 ---- batch: 020 ----
mean loss: 258.01
 ---- batch: 030 ----
mean loss: 251.53
 ---- batch: 040 ----
mean loss: 262.84
 ---- batch: 050 ----
mean loss: 251.02
 ---- batch: 060 ----
mean loss: 252.95
 ---- batch: 070 ----
mean loss: 264.10
 ---- batch: 080 ----
mean loss: 262.61
 ---- batch: 090 ----
mean loss: 256.01
train mean loss: 256.76
epoch train time: 0:00:17.016350
elapsed time: 0:17:50.611545
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 02:24:06.607897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.73
 ---- batch: 020 ----
mean loss: 252.60
 ---- batch: 030 ----
mean loss: 251.26
 ---- batch: 040 ----
mean loss: 247.73
 ---- batch: 050 ----
mean loss: 251.82
 ---- batch: 060 ----
mean loss: 261.23
 ---- batch: 070 ----
mean loss: 253.42
 ---- batch: 080 ----
mean loss: 248.81
 ---- batch: 090 ----
mean loss: 266.49
train mean loss: 254.73
epoch train time: 0:00:17.032174
elapsed time: 0:18:07.644954
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 02:24:23.641466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.76
 ---- batch: 020 ----
mean loss: 248.66
 ---- batch: 030 ----
mean loss: 256.14
 ---- batch: 040 ----
mean loss: 248.74
 ---- batch: 050 ----
mean loss: 258.95
 ---- batch: 060 ----
mean loss: 256.80
 ---- batch: 070 ----
mean loss: 254.97
 ---- batch: 080 ----
mean loss: 254.16
 ---- batch: 090 ----
mean loss: 251.27
train mean loss: 253.81
epoch train time: 0:00:17.027872
elapsed time: 0:18:24.674316
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 02:24:40.670792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.78
 ---- batch: 020 ----
mean loss: 253.81
 ---- batch: 030 ----
mean loss: 249.46
 ---- batch: 040 ----
mean loss: 250.19
 ---- batch: 050 ----
mean loss: 265.28
 ---- batch: 060 ----
mean loss: 245.01
 ---- batch: 070 ----
mean loss: 249.72
 ---- batch: 080 ----
mean loss: 245.05
 ---- batch: 090 ----
mean loss: 245.53
train mean loss: 250.08
epoch train time: 0:00:17.048792
elapsed time: 0:18:41.724532
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 02:24:57.720905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.60
 ---- batch: 020 ----
mean loss: 248.95
 ---- batch: 030 ----
mean loss: 253.46
 ---- batch: 040 ----
mean loss: 253.66
 ---- batch: 050 ----
mean loss: 255.81
 ---- batch: 060 ----
mean loss: 251.55
 ---- batch: 070 ----
mean loss: 245.57
 ---- batch: 080 ----
mean loss: 240.46
 ---- batch: 090 ----
mean loss: 244.78
train mean loss: 250.13
epoch train time: 0:00:17.070821
elapsed time: 0:18:58.796543
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 02:25:14.792924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.53
 ---- batch: 020 ----
mean loss: 251.38
 ---- batch: 030 ----
mean loss: 254.22
 ---- batch: 040 ----
mean loss: 244.10
 ---- batch: 050 ----
mean loss: 250.87
 ---- batch: 060 ----
mean loss: 248.97
 ---- batch: 070 ----
mean loss: 245.68
 ---- batch: 080 ----
mean loss: 254.22
 ---- batch: 090 ----
mean loss: 248.05
train mean loss: 249.04
epoch train time: 0:00:17.058183
elapsed time: 0:19:15.855926
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 02:25:31.852294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.94
 ---- batch: 020 ----
mean loss: 247.16
 ---- batch: 030 ----
mean loss: 250.08
 ---- batch: 040 ----
mean loss: 252.66
 ---- batch: 050 ----
mean loss: 248.32
 ---- batch: 060 ----
mean loss: 249.25
 ---- batch: 070 ----
mean loss: 238.87
 ---- batch: 080 ----
mean loss: 241.73
 ---- batch: 090 ----
mean loss: 239.07
train mean loss: 245.78
epoch train time: 0:00:17.056897
elapsed time: 0:19:32.914185
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 02:25:48.910597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.75
 ---- batch: 020 ----
mean loss: 241.87
 ---- batch: 030 ----
mean loss: 237.50
 ---- batch: 040 ----
mean loss: 239.52
 ---- batch: 050 ----
mean loss: 240.34
 ---- batch: 060 ----
mean loss: 241.21
 ---- batch: 070 ----
mean loss: 245.41
 ---- batch: 080 ----
mean loss: 248.29
 ---- batch: 090 ----
mean loss: 237.97
train mean loss: 241.84
epoch train time: 0:00:17.088503
elapsed time: 0:19:50.003921
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 02:26:06.000301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.87
 ---- batch: 020 ----
mean loss: 245.92
 ---- batch: 030 ----
mean loss: 234.92
 ---- batch: 040 ----
mean loss: 239.65
 ---- batch: 050 ----
mean loss: 241.54
 ---- batch: 060 ----
mean loss: 238.45
 ---- batch: 070 ----
mean loss: 249.92
 ---- batch: 080 ----
mean loss: 240.03
 ---- batch: 090 ----
mean loss: 246.42
train mean loss: 242.70
epoch train time: 0:00:17.066792
elapsed time: 0:20:07.071881
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 02:26:23.068234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.51
 ---- batch: 020 ----
mean loss: 236.87
 ---- batch: 030 ----
mean loss: 238.23
 ---- batch: 040 ----
mean loss: 243.59
 ---- batch: 050 ----
mean loss: 240.33
 ---- batch: 060 ----
mean loss: 234.12
 ---- batch: 070 ----
mean loss: 230.75
 ---- batch: 080 ----
mean loss: 246.80
 ---- batch: 090 ----
mean loss: 245.31
train mean loss: 241.16
epoch train time: 0:00:17.092657
elapsed time: 0:20:24.165673
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 02:26:40.162046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.62
 ---- batch: 020 ----
mean loss: 236.98
 ---- batch: 030 ----
mean loss: 238.41
 ---- batch: 040 ----
mean loss: 244.82
 ---- batch: 050 ----
mean loss: 245.19
 ---- batch: 060 ----
mean loss: 237.67
 ---- batch: 070 ----
mean loss: 248.46
 ---- batch: 080 ----
mean loss: 235.80
 ---- batch: 090 ----
mean loss: 240.59
train mean loss: 240.07
epoch train time: 0:00:17.072588
elapsed time: 0:20:41.239477
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 02:26:57.235847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.81
 ---- batch: 020 ----
mean loss: 232.55
 ---- batch: 030 ----
mean loss: 243.61
 ---- batch: 040 ----
mean loss: 235.84
 ---- batch: 050 ----
mean loss: 237.82
 ---- batch: 060 ----
mean loss: 243.49
 ---- batch: 070 ----
mean loss: 230.91
 ---- batch: 080 ----
mean loss: 237.14
 ---- batch: 090 ----
mean loss: 235.83
train mean loss: 237.37
epoch train time: 0:00:16.877822
elapsed time: 0:20:58.118470
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 02:27:14.114854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.02
 ---- batch: 020 ----
mean loss: 222.75
 ---- batch: 030 ----
mean loss: 235.05
 ---- batch: 040 ----
mean loss: 234.99
 ---- batch: 050 ----
mean loss: 243.18
 ---- batch: 060 ----
mean loss: 235.78
 ---- batch: 070 ----
mean loss: 235.59
 ---- batch: 080 ----
mean loss: 246.79
 ---- batch: 090 ----
mean loss: 240.76
train mean loss: 235.98
epoch train time: 0:00:16.880668
elapsed time: 0:21:15.000339
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 02:27:30.996709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.89
 ---- batch: 020 ----
mean loss: 239.25
 ---- batch: 030 ----
mean loss: 240.69
 ---- batch: 040 ----
mean loss: 230.91
 ---- batch: 050 ----
mean loss: 229.70
 ---- batch: 060 ----
mean loss: 238.47
 ---- batch: 070 ----
mean loss: 232.11
 ---- batch: 080 ----
mean loss: 235.19
 ---- batch: 090 ----
mean loss: 234.25
train mean loss: 235.44
epoch train time: 0:00:16.859859
elapsed time: 0:21:31.861351
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 02:27:47.857731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.40
 ---- batch: 020 ----
mean loss: 239.70
 ---- batch: 030 ----
mean loss: 239.33
 ---- batch: 040 ----
mean loss: 241.48
 ---- batch: 050 ----
mean loss: 231.08
 ---- batch: 060 ----
mean loss: 233.39
 ---- batch: 070 ----
mean loss: 229.43
 ---- batch: 080 ----
mean loss: 225.99
 ---- batch: 090 ----
mean loss: 232.85
train mean loss: 234.40
epoch train time: 0:00:16.892686
elapsed time: 0:21:48.755204
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 02:28:04.751561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.22
 ---- batch: 020 ----
mean loss: 229.71
 ---- batch: 030 ----
mean loss: 237.34
 ---- batch: 040 ----
mean loss: 239.78
 ---- batch: 050 ----
mean loss: 227.99
 ---- batch: 060 ----
mean loss: 229.03
 ---- batch: 070 ----
mean loss: 233.57
 ---- batch: 080 ----
mean loss: 232.21
 ---- batch: 090 ----
mean loss: 233.86
train mean loss: 232.16
epoch train time: 0:00:17.042275
elapsed time: 0:22:05.798690
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 02:28:21.795074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.87
 ---- batch: 020 ----
mean loss: 233.90
 ---- batch: 030 ----
mean loss: 227.98
 ---- batch: 040 ----
mean loss: 236.66
 ---- batch: 050 ----
mean loss: 244.18
 ---- batch: 060 ----
mean loss: 232.41
 ---- batch: 070 ----
mean loss: 235.19
 ---- batch: 080 ----
mean loss: 236.23
 ---- batch: 090 ----
mean loss: 233.47
train mean loss: 234.43
epoch train time: 0:00:17.046155
elapsed time: 0:22:22.846145
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 02:28:38.842584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.36
 ---- batch: 020 ----
mean loss: 232.97
 ---- batch: 030 ----
mean loss: 227.52
 ---- batch: 040 ----
mean loss: 244.63
 ---- batch: 050 ----
mean loss: 225.54
 ---- batch: 060 ----
mean loss: 224.43
 ---- batch: 070 ----
mean loss: 230.91
 ---- batch: 080 ----
mean loss: 234.70
 ---- batch: 090 ----
mean loss: 229.64
train mean loss: 231.00
epoch train time: 0:00:17.057085
elapsed time: 0:22:39.904521
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 02:28:55.900891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.70
 ---- batch: 020 ----
mean loss: 230.23
 ---- batch: 030 ----
mean loss: 225.23
 ---- batch: 040 ----
mean loss: 238.58
 ---- batch: 050 ----
mean loss: 230.90
 ---- batch: 060 ----
mean loss: 224.44
 ---- batch: 070 ----
mean loss: 243.74
 ---- batch: 080 ----
mean loss: 227.85
 ---- batch: 090 ----
mean loss: 228.37
train mean loss: 230.69
epoch train time: 0:00:17.043083
elapsed time: 0:22:56.948871
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 02:29:12.945134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.30
 ---- batch: 020 ----
mean loss: 224.50
 ---- batch: 030 ----
mean loss: 226.89
 ---- batch: 040 ----
mean loss: 238.20
 ---- batch: 050 ----
mean loss: 224.02
 ---- batch: 060 ----
mean loss: 228.87
 ---- batch: 070 ----
mean loss: 218.35
 ---- batch: 080 ----
mean loss: 228.20
 ---- batch: 090 ----
mean loss: 229.22
train mean loss: 226.86
epoch train time: 0:00:17.084987
elapsed time: 0:23:14.035047
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 02:29:30.031399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.69
 ---- batch: 020 ----
mean loss: 220.89
 ---- batch: 030 ----
mean loss: 227.17
 ---- batch: 040 ----
mean loss: 224.99
 ---- batch: 050 ----
mean loss: 223.72
 ---- batch: 060 ----
mean loss: 226.98
 ---- batch: 070 ----
mean loss: 225.12
 ---- batch: 080 ----
mean loss: 230.07
 ---- batch: 090 ----
mean loss: 234.31
train mean loss: 226.79
epoch train time: 0:00:17.022283
elapsed time: 0:23:31.058472
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 02:29:47.054867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.86
 ---- batch: 020 ----
mean loss: 228.52
 ---- batch: 030 ----
mean loss: 232.37
 ---- batch: 040 ----
mean loss: 224.42
 ---- batch: 050 ----
mean loss: 231.33
 ---- batch: 060 ----
mean loss: 225.07
 ---- batch: 070 ----
mean loss: 225.43
 ---- batch: 080 ----
mean loss: 220.30
 ---- batch: 090 ----
mean loss: 224.69
train mean loss: 226.36
epoch train time: 0:00:17.011358
elapsed time: 0:23:48.071045
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 02:30:04.067446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.85
 ---- batch: 020 ----
mean loss: 223.04
 ---- batch: 030 ----
mean loss: 223.18
 ---- batch: 040 ----
mean loss: 224.55
 ---- batch: 050 ----
mean loss: 227.24
 ---- batch: 060 ----
mean loss: 235.35
 ---- batch: 070 ----
mean loss: 218.40
 ---- batch: 080 ----
mean loss: 231.49
 ---- batch: 090 ----
mean loss: 217.76
train mean loss: 224.78
epoch train time: 0:00:16.996647
elapsed time: 0:24:05.068888
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 02:30:21.065266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.40
 ---- batch: 020 ----
mean loss: 219.55
 ---- batch: 030 ----
mean loss: 225.40
 ---- batch: 040 ----
mean loss: 220.91
 ---- batch: 050 ----
mean loss: 223.27
 ---- batch: 060 ----
mean loss: 221.08
 ---- batch: 070 ----
mean loss: 224.99
 ---- batch: 080 ----
mean loss: 221.12
 ---- batch: 090 ----
mean loss: 227.38
train mean loss: 223.37
epoch train time: 0:00:17.017214
elapsed time: 0:24:22.087416
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 02:30:38.083817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.95
 ---- batch: 020 ----
mean loss: 220.64
 ---- batch: 030 ----
mean loss: 223.16
 ---- batch: 040 ----
mean loss: 218.40
 ---- batch: 050 ----
mean loss: 225.13
 ---- batch: 060 ----
mean loss: 223.68
 ---- batch: 070 ----
mean loss: 215.07
 ---- batch: 080 ----
mean loss: 222.50
 ---- batch: 090 ----
mean loss: 228.07
train mean loss: 222.36
epoch train time: 0:00:16.998863
elapsed time: 0:24:39.087491
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 02:30:55.083835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.11
 ---- batch: 020 ----
mean loss: 229.78
 ---- batch: 030 ----
mean loss: 223.04
 ---- batch: 040 ----
mean loss: 229.75
 ---- batch: 050 ----
mean loss: 222.47
 ---- batch: 060 ----
mean loss: 214.73
 ---- batch: 070 ----
mean loss: 212.99
 ---- batch: 080 ----
mean loss: 216.38
 ---- batch: 090 ----
mean loss: 231.08
train mean loss: 223.20
epoch train time: 0:00:16.973961
elapsed time: 0:24:56.062602
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 02:31:12.058997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.77
 ---- batch: 020 ----
mean loss: 218.62
 ---- batch: 030 ----
mean loss: 225.85
 ---- batch: 040 ----
mean loss: 225.52
 ---- batch: 050 ----
mean loss: 220.07
 ---- batch: 060 ----
mean loss: 223.94
 ---- batch: 070 ----
mean loss: 220.47
 ---- batch: 080 ----
mean loss: 218.26
 ---- batch: 090 ----
mean loss: 213.06
train mean loss: 220.88
epoch train time: 0:00:16.989888
elapsed time: 0:25:13.053717
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 02:31:29.050083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.03
 ---- batch: 020 ----
mean loss: 220.81
 ---- batch: 030 ----
mean loss: 219.42
 ---- batch: 040 ----
mean loss: 222.55
 ---- batch: 050 ----
mean loss: 205.29
 ---- batch: 060 ----
mean loss: 224.08
 ---- batch: 070 ----
mean loss: 223.73
 ---- batch: 080 ----
mean loss: 222.84
 ---- batch: 090 ----
mean loss: 218.67
train mean loss: 219.83
epoch train time: 0:00:16.949371
elapsed time: 0:25:30.004311
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 02:31:46.000765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.96
 ---- batch: 020 ----
mean loss: 217.45
 ---- batch: 030 ----
mean loss: 218.31
 ---- batch: 040 ----
mean loss: 220.50
 ---- batch: 050 ----
mean loss: 212.79
 ---- batch: 060 ----
mean loss: 227.58
 ---- batch: 070 ----
mean loss: 216.28
 ---- batch: 080 ----
mean loss: 219.55
 ---- batch: 090 ----
mean loss: 212.28
train mean loss: 218.92
epoch train time: 0:00:16.983012
elapsed time: 0:25:46.988685
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 02:32:02.985084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.50
 ---- batch: 020 ----
mean loss: 222.21
 ---- batch: 030 ----
mean loss: 220.52
 ---- batch: 040 ----
mean loss: 219.48
 ---- batch: 050 ----
mean loss: 220.70
 ---- batch: 060 ----
mean loss: 216.74
 ---- batch: 070 ----
mean loss: 215.09
 ---- batch: 080 ----
mean loss: 216.65
 ---- batch: 090 ----
mean loss: 219.51
train mean loss: 219.07
epoch train time: 0:00:16.996667
elapsed time: 0:26:03.986654
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 02:32:19.983135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.06
 ---- batch: 020 ----
mean loss: 213.55
 ---- batch: 030 ----
mean loss: 208.48
 ---- batch: 040 ----
mean loss: 218.69
 ---- batch: 050 ----
mean loss: 227.00
 ---- batch: 060 ----
mean loss: 227.83
 ---- batch: 070 ----
mean loss: 214.80
 ---- batch: 080 ----
mean loss: 221.46
 ---- batch: 090 ----
mean loss: 215.68
train mean loss: 218.25
epoch train time: 0:00:16.984443
elapsed time: 0:26:20.972321
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 02:32:36.968765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.06
 ---- batch: 020 ----
mean loss: 223.38
 ---- batch: 030 ----
mean loss: 214.19
 ---- batch: 040 ----
mean loss: 221.49
 ---- batch: 050 ----
mean loss: 228.50
 ---- batch: 060 ----
mean loss: 226.07
 ---- batch: 070 ----
mean loss: 226.79
 ---- batch: 080 ----
mean loss: 218.55
 ---- batch: 090 ----
mean loss: 208.11
train mean loss: 219.54
epoch train time: 0:00:16.951632
elapsed time: 0:26:37.925181
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 02:32:53.921560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.81
 ---- batch: 020 ----
mean loss: 225.60
 ---- batch: 030 ----
mean loss: 216.53
 ---- batch: 040 ----
mean loss: 217.10
 ---- batch: 050 ----
mean loss: 210.83
 ---- batch: 060 ----
mean loss: 218.56
 ---- batch: 070 ----
mean loss: 215.30
 ---- batch: 080 ----
mean loss: 223.28
 ---- batch: 090 ----
mean loss: 210.13
train mean loss: 216.95
epoch train time: 0:00:16.997046
elapsed time: 0:26:54.923472
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 02:33:10.919916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.59
 ---- batch: 020 ----
mean loss: 220.48
 ---- batch: 030 ----
mean loss: 216.74
 ---- batch: 040 ----
mean loss: 218.41
 ---- batch: 050 ----
mean loss: 214.63
 ---- batch: 060 ----
mean loss: 224.00
 ---- batch: 070 ----
mean loss: 224.53
 ---- batch: 080 ----
mean loss: 216.85
 ---- batch: 090 ----
mean loss: 212.21
train mean loss: 218.04
epoch train time: 0:00:16.970954
elapsed time: 0:27:11.895740
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 02:33:27.892105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.92
 ---- batch: 020 ----
mean loss: 209.94
 ---- batch: 030 ----
mean loss: 222.91
 ---- batch: 040 ----
mean loss: 205.82
 ---- batch: 050 ----
mean loss: 212.37
 ---- batch: 060 ----
mean loss: 226.03
 ---- batch: 070 ----
mean loss: 219.50
 ---- batch: 080 ----
mean loss: 215.00
 ---- batch: 090 ----
mean loss: 220.36
train mean loss: 216.06
epoch train time: 0:00:17.002473
elapsed time: 0:27:28.899390
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 02:33:44.895767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.65
 ---- batch: 020 ----
mean loss: 219.57
 ---- batch: 030 ----
mean loss: 217.74
 ---- batch: 040 ----
mean loss: 210.11
 ---- batch: 050 ----
mean loss: 214.93
 ---- batch: 060 ----
mean loss: 212.60
 ---- batch: 070 ----
mean loss: 209.07
 ---- batch: 080 ----
mean loss: 215.45
 ---- batch: 090 ----
mean loss: 216.72
train mean loss: 214.50
epoch train time: 0:00:17.088821
elapsed time: 0:27:45.989379
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 02:34:01.985776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.17
 ---- batch: 020 ----
mean loss: 212.86
 ---- batch: 030 ----
mean loss: 214.80
 ---- batch: 040 ----
mean loss: 216.61
 ---- batch: 050 ----
mean loss: 210.78
 ---- batch: 060 ----
mean loss: 211.37
 ---- batch: 070 ----
mean loss: 220.19
 ---- batch: 080 ----
mean loss: 211.02
 ---- batch: 090 ----
mean loss: 215.99
train mean loss: 215.21
epoch train time: 0:00:16.983965
elapsed time: 0:28:02.974629
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 02:34:18.971028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.19
 ---- batch: 020 ----
mean loss: 209.85
 ---- batch: 030 ----
mean loss: 211.24
 ---- batch: 040 ----
mean loss: 212.94
 ---- batch: 050 ----
mean loss: 211.40
 ---- batch: 060 ----
mean loss: 213.70
 ---- batch: 070 ----
mean loss: 223.01
 ---- batch: 080 ----
mean loss: 216.48
 ---- batch: 090 ----
mean loss: 220.19
train mean loss: 213.95
epoch train time: 0:00:16.989602
elapsed time: 0:28:19.965420
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 02:34:35.961835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.33
 ---- batch: 020 ----
mean loss: 216.67
 ---- batch: 030 ----
mean loss: 207.46
 ---- batch: 040 ----
mean loss: 210.80
 ---- batch: 050 ----
mean loss: 206.10
 ---- batch: 060 ----
mean loss: 215.63
 ---- batch: 070 ----
mean loss: 215.99
 ---- batch: 080 ----
mean loss: 217.48
 ---- batch: 090 ----
mean loss: 209.97
train mean loss: 213.05
epoch train time: 0:00:16.973575
elapsed time: 0:28:36.940218
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 02:34:52.936616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.03
 ---- batch: 020 ----
mean loss: 207.64
 ---- batch: 030 ----
mean loss: 210.74
 ---- batch: 040 ----
mean loss: 207.28
 ---- batch: 050 ----
mean loss: 218.21
 ---- batch: 060 ----
mean loss: 213.69
 ---- batch: 070 ----
mean loss: 208.22
 ---- batch: 080 ----
mean loss: 212.47
 ---- batch: 090 ----
mean loss: 212.15
train mean loss: 212.10
epoch train time: 0:00:16.930022
elapsed time: 0:28:53.871491
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 02:35:09.867892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.99
 ---- batch: 020 ----
mean loss: 209.99
 ---- batch: 030 ----
mean loss: 210.95
 ---- batch: 040 ----
mean loss: 211.42
 ---- batch: 050 ----
mean loss: 222.51
 ---- batch: 060 ----
mean loss: 205.51
 ---- batch: 070 ----
mean loss: 207.87
 ---- batch: 080 ----
mean loss: 212.85
 ---- batch: 090 ----
mean loss: 217.67
train mean loss: 213.28
epoch train time: 0:00:16.966712
elapsed time: 0:29:10.839445
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 02:35:26.835862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.24
 ---- batch: 020 ----
mean loss: 211.55
 ---- batch: 030 ----
mean loss: 214.05
 ---- batch: 040 ----
mean loss: 207.07
 ---- batch: 050 ----
mean loss: 215.76
 ---- batch: 060 ----
mean loss: 214.19
 ---- batch: 070 ----
mean loss: 207.77
 ---- batch: 080 ----
mean loss: 209.06
 ---- batch: 090 ----
mean loss: 210.95
train mean loss: 211.31
epoch train time: 0:00:16.990301
elapsed time: 0:29:27.831023
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 02:35:43.827429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.33
 ---- batch: 020 ----
mean loss: 207.03
 ---- batch: 030 ----
mean loss: 208.82
 ---- batch: 040 ----
mean loss: 207.90
 ---- batch: 050 ----
mean loss: 205.21
 ---- batch: 060 ----
mean loss: 213.71
 ---- batch: 070 ----
mean loss: 209.07
 ---- batch: 080 ----
mean loss: 207.61
 ---- batch: 090 ----
mean loss: 218.10
train mean loss: 211.02
epoch train time: 0:00:17.000691
elapsed time: 0:29:44.833002
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 02:36:00.829371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.81
 ---- batch: 020 ----
mean loss: 211.58
 ---- batch: 030 ----
mean loss: 210.77
 ---- batch: 040 ----
mean loss: 207.03
 ---- batch: 050 ----
mean loss: 209.89
 ---- batch: 060 ----
mean loss: 209.58
 ---- batch: 070 ----
mean loss: 216.99
 ---- batch: 080 ----
mean loss: 200.37
 ---- batch: 090 ----
mean loss: 209.13
train mean loss: 210.16
epoch train time: 0:00:16.977838
elapsed time: 0:30:01.812156
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 02:36:17.808610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.93
 ---- batch: 020 ----
mean loss: 217.52
 ---- batch: 030 ----
mean loss: 209.18
 ---- batch: 040 ----
mean loss: 215.36
 ---- batch: 050 ----
mean loss: 214.42
 ---- batch: 060 ----
mean loss: 212.50
 ---- batch: 070 ----
mean loss: 200.34
 ---- batch: 080 ----
mean loss: 205.05
 ---- batch: 090 ----
mean loss: 204.88
train mean loss: 210.38
epoch train time: 0:00:17.001753
elapsed time: 0:30:18.815142
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 02:36:34.811515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.72
 ---- batch: 020 ----
mean loss: 205.96
 ---- batch: 030 ----
mean loss: 216.13
 ---- batch: 040 ----
mean loss: 210.83
 ---- batch: 050 ----
mean loss: 205.54
 ---- batch: 060 ----
mean loss: 212.40
 ---- batch: 070 ----
mean loss: 208.99
 ---- batch: 080 ----
mean loss: 212.83
 ---- batch: 090 ----
mean loss: 214.91
train mean loss: 210.11
epoch train time: 0:00:16.986698
elapsed time: 0:30:35.803008
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 02:36:51.799451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.03
 ---- batch: 020 ----
mean loss: 212.65
 ---- batch: 030 ----
mean loss: 209.89
 ---- batch: 040 ----
mean loss: 199.46
 ---- batch: 050 ----
mean loss: 211.70
 ---- batch: 060 ----
mean loss: 210.94
 ---- batch: 070 ----
mean loss: 207.69
 ---- batch: 080 ----
mean loss: 211.50
 ---- batch: 090 ----
mean loss: 208.41
train mean loss: 208.66
epoch train time: 0:00:16.901547
elapsed time: 0:30:52.706213
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 02:37:08.702516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.88
 ---- batch: 020 ----
mean loss: 211.46
 ---- batch: 030 ----
mean loss: 205.04
 ---- batch: 040 ----
mean loss: 211.17
 ---- batch: 050 ----
mean loss: 205.49
 ---- batch: 060 ----
mean loss: 211.98
 ---- batch: 070 ----
mean loss: 209.77
 ---- batch: 080 ----
mean loss: 215.68
 ---- batch: 090 ----
mean loss: 209.45
train mean loss: 208.81
epoch train time: 0:00:16.821873
elapsed time: 0:31:09.529208
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 02:37:25.525563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.70
 ---- batch: 020 ----
mean loss: 212.48
 ---- batch: 030 ----
mean loss: 202.93
 ---- batch: 040 ----
mean loss: 210.04
 ---- batch: 050 ----
mean loss: 206.81
 ---- batch: 060 ----
mean loss: 215.01
 ---- batch: 070 ----
mean loss: 216.35
 ---- batch: 080 ----
mean loss: 211.74
 ---- batch: 090 ----
mean loss: 208.76
train mean loss: 211.29
epoch train time: 0:00:16.853823
elapsed time: 0:31:26.384173
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 02:37:42.380556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.72
 ---- batch: 020 ----
mean loss: 207.27
 ---- batch: 030 ----
mean loss: 209.13
 ---- batch: 040 ----
mean loss: 201.46
 ---- batch: 050 ----
mean loss: 205.16
 ---- batch: 060 ----
mean loss: 210.57
 ---- batch: 070 ----
mean loss: 210.08
 ---- batch: 080 ----
mean loss: 207.01
 ---- batch: 090 ----
mean loss: 208.58
train mean loss: 208.34
epoch train time: 0:00:16.820953
elapsed time: 0:31:43.206339
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 02:37:59.202723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.45
 ---- batch: 020 ----
mean loss: 206.99
 ---- batch: 030 ----
mean loss: 211.08
 ---- batch: 040 ----
mean loss: 206.70
 ---- batch: 050 ----
mean loss: 207.31
 ---- batch: 060 ----
mean loss: 214.87
 ---- batch: 070 ----
mean loss: 205.08
 ---- batch: 080 ----
mean loss: 204.16
 ---- batch: 090 ----
mean loss: 206.40
train mean loss: 208.95
epoch train time: 0:00:16.771013
elapsed time: 0:31:59.978521
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 02:38:15.974897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.52
 ---- batch: 020 ----
mean loss: 207.01
 ---- batch: 030 ----
mean loss: 216.11
 ---- batch: 040 ----
mean loss: 211.74
 ---- batch: 050 ----
mean loss: 208.28
 ---- batch: 060 ----
mean loss: 205.23
 ---- batch: 070 ----
mean loss: 211.16
 ---- batch: 080 ----
mean loss: 201.25
 ---- batch: 090 ----
mean loss: 203.68
train mean loss: 206.54
epoch train time: 0:00:16.797623
elapsed time: 0:32:16.777270
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 02:38:32.773702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.33
 ---- batch: 020 ----
mean loss: 210.13
 ---- batch: 030 ----
mean loss: 203.61
 ---- batch: 040 ----
mean loss: 204.87
 ---- batch: 050 ----
mean loss: 201.93
 ---- batch: 060 ----
mean loss: 211.16
 ---- batch: 070 ----
mean loss: 216.25
 ---- batch: 080 ----
mean loss: 210.65
 ---- batch: 090 ----
mean loss: 211.05
train mean loss: 207.72
epoch train time: 0:00:16.773903
elapsed time: 0:32:33.552675
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 02:38:49.549065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.05
 ---- batch: 020 ----
mean loss: 209.57
 ---- batch: 030 ----
mean loss: 207.01
 ---- batch: 040 ----
mean loss: 212.86
 ---- batch: 050 ----
mean loss: 209.15
 ---- batch: 060 ----
mean loss: 216.50
 ---- batch: 070 ----
mean loss: 200.23
 ---- batch: 080 ----
mean loss: 206.48
 ---- batch: 090 ----
mean loss: 205.05
train mean loss: 208.84
epoch train time: 0:00:16.864211
elapsed time: 0:32:50.418143
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 02:39:06.414497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.60
 ---- batch: 020 ----
mean loss: 212.92
 ---- batch: 030 ----
mean loss: 202.51
 ---- batch: 040 ----
mean loss: 204.86
 ---- batch: 050 ----
mean loss: 205.60
 ---- batch: 060 ----
mean loss: 208.27
 ---- batch: 070 ----
mean loss: 209.80
 ---- batch: 080 ----
mean loss: 195.75
 ---- batch: 090 ----
mean loss: 210.37
train mean loss: 206.44
epoch train time: 0:00:16.778436
elapsed time: 0:33:07.197868
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 02:39:23.194268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.68
 ---- batch: 020 ----
mean loss: 217.28
 ---- batch: 030 ----
mean loss: 212.75
 ---- batch: 040 ----
mean loss: 201.36
 ---- batch: 050 ----
mean loss: 215.81
 ---- batch: 060 ----
mean loss: 208.78
 ---- batch: 070 ----
mean loss: 210.23
 ---- batch: 080 ----
mean loss: 202.24
 ---- batch: 090 ----
mean loss: 204.28
train mean loss: 208.71
epoch train time: 0:00:16.840973
elapsed time: 0:33:24.040267
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 02:39:40.036572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.10
 ---- batch: 020 ----
mean loss: 201.95
 ---- batch: 030 ----
mean loss: 205.72
 ---- batch: 040 ----
mean loss: 202.34
 ---- batch: 050 ----
mean loss: 204.79
 ---- batch: 060 ----
mean loss: 212.04
 ---- batch: 070 ----
mean loss: 216.11
 ---- batch: 080 ----
mean loss: 211.58
 ---- batch: 090 ----
mean loss: 210.86
train mean loss: 206.44
epoch train time: 0:00:16.730331
elapsed time: 0:33:40.771791
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 02:39:56.768181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.90
 ---- batch: 020 ----
mean loss: 201.22
 ---- batch: 030 ----
mean loss: 208.71
 ---- batch: 040 ----
mean loss: 204.72
 ---- batch: 050 ----
mean loss: 203.63
 ---- batch: 060 ----
mean loss: 203.75
 ---- batch: 070 ----
mean loss: 215.88
 ---- batch: 080 ----
mean loss: 211.18
 ---- batch: 090 ----
mean loss: 219.06
train mean loss: 208.24
epoch train time: 0:00:16.735540
elapsed time: 0:33:57.508508
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 02:40:13.504865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.93
 ---- batch: 020 ----
mean loss: 207.53
 ---- batch: 030 ----
mean loss: 207.68
 ---- batch: 040 ----
mean loss: 211.52
 ---- batch: 050 ----
mean loss: 207.48
 ---- batch: 060 ----
mean loss: 204.94
 ---- batch: 070 ----
mean loss: 201.92
 ---- batch: 080 ----
mean loss: 206.28
 ---- batch: 090 ----
mean loss: 196.23
train mean loss: 204.84
epoch train time: 0:00:16.715498
elapsed time: 0:34:14.225164
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 02:40:30.221529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.10
 ---- batch: 020 ----
mean loss: 204.47
 ---- batch: 030 ----
mean loss: 202.94
 ---- batch: 040 ----
mean loss: 210.78
 ---- batch: 050 ----
mean loss: 200.63
 ---- batch: 060 ----
mean loss: 207.73
 ---- batch: 070 ----
mean loss: 204.07
 ---- batch: 080 ----
mean loss: 207.42
 ---- batch: 090 ----
mean loss: 202.75
train mean loss: 205.54
epoch train time: 0:00:16.710850
elapsed time: 0:34:30.937170
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 02:40:46.933503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.91
 ---- batch: 020 ----
mean loss: 204.19
 ---- batch: 030 ----
mean loss: 198.93
 ---- batch: 040 ----
mean loss: 210.92
 ---- batch: 050 ----
mean loss: 207.95
 ---- batch: 060 ----
mean loss: 204.35
 ---- batch: 070 ----
mean loss: 196.28
 ---- batch: 080 ----
mean loss: 210.04
 ---- batch: 090 ----
mean loss: 199.01
train mean loss: 204.21
epoch train time: 0:00:16.685345
elapsed time: 0:34:47.623628
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 02:41:03.619880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.64
 ---- batch: 020 ----
mean loss: 204.95
 ---- batch: 030 ----
mean loss: 210.10
 ---- batch: 040 ----
mean loss: 208.92
 ---- batch: 050 ----
mean loss: 214.56
 ---- batch: 060 ----
mean loss: 202.36
 ---- batch: 070 ----
mean loss: 209.29
 ---- batch: 080 ----
mean loss: 211.36
 ---- batch: 090 ----
mean loss: 201.09
train mean loss: 206.06
epoch train time: 0:00:16.719266
elapsed time: 0:35:04.343911
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 02:41:20.340246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.67
 ---- batch: 020 ----
mean loss: 205.99
 ---- batch: 030 ----
mean loss: 200.23
 ---- batch: 040 ----
mean loss: 208.87
 ---- batch: 050 ----
mean loss: 202.15
 ---- batch: 060 ----
mean loss: 202.77
 ---- batch: 070 ----
mean loss: 196.15
 ---- batch: 080 ----
mean loss: 201.62
 ---- batch: 090 ----
mean loss: 198.14
train mean loss: 202.60
epoch train time: 0:00:16.723151
elapsed time: 0:35:21.068140
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 02:41:37.064508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.76
 ---- batch: 020 ----
mean loss: 194.80
 ---- batch: 030 ----
mean loss: 197.55
 ---- batch: 040 ----
mean loss: 206.85
 ---- batch: 050 ----
mean loss: 207.36
 ---- batch: 060 ----
mean loss: 205.44
 ---- batch: 070 ----
mean loss: 206.47
 ---- batch: 080 ----
mean loss: 203.80
 ---- batch: 090 ----
mean loss: 200.77
train mean loss: 203.09
epoch train time: 0:00:16.677524
elapsed time: 0:35:37.746863
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 02:41:53.743277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.78
 ---- batch: 020 ----
mean loss: 201.39
 ---- batch: 030 ----
mean loss: 196.64
 ---- batch: 040 ----
mean loss: 204.84
 ---- batch: 050 ----
mean loss: 202.02
 ---- batch: 060 ----
mean loss: 207.43
 ---- batch: 070 ----
mean loss: 207.60
 ---- batch: 080 ----
mean loss: 204.57
 ---- batch: 090 ----
mean loss: 205.14
train mean loss: 203.25
epoch train time: 0:00:16.705726
elapsed time: 0:35:54.453855
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 02:42:10.450232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.79
 ---- batch: 020 ----
mean loss: 201.88
 ---- batch: 030 ----
mean loss: 197.84
 ---- batch: 040 ----
mean loss: 200.10
 ---- batch: 050 ----
mean loss: 202.54
 ---- batch: 060 ----
mean loss: 200.35
 ---- batch: 070 ----
mean loss: 202.65
 ---- batch: 080 ----
mean loss: 208.40
 ---- batch: 090 ----
mean loss: 207.25
train mean loss: 202.30
epoch train time: 0:00:16.665199
elapsed time: 0:36:11.120216
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 02:42:27.116697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.13
 ---- batch: 020 ----
mean loss: 201.49
 ---- batch: 030 ----
mean loss: 206.79
 ---- batch: 040 ----
mean loss: 205.86
 ---- batch: 050 ----
mean loss: 199.78
 ---- batch: 060 ----
mean loss: 201.23
 ---- batch: 070 ----
mean loss: 197.70
 ---- batch: 080 ----
mean loss: 205.02
 ---- batch: 090 ----
mean loss: 206.19
train mean loss: 202.06
epoch train time: 0:00:16.853687
elapsed time: 0:36:27.975608
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 02:42:43.971909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.81
 ---- batch: 020 ----
mean loss: 200.50
 ---- batch: 030 ----
mean loss: 200.48
 ---- batch: 040 ----
mean loss: 205.09
 ---- batch: 050 ----
mean loss: 201.80
 ---- batch: 060 ----
mean loss: 202.65
 ---- batch: 070 ----
mean loss: 194.95
 ---- batch: 080 ----
mean loss: 198.18
 ---- batch: 090 ----
mean loss: 210.31
train mean loss: 201.43
epoch train time: 0:00:16.811324
elapsed time: 0:36:44.788195
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 02:43:00.784653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.65
 ---- batch: 020 ----
mean loss: 196.91
 ---- batch: 030 ----
mean loss: 202.05
 ---- batch: 040 ----
mean loss: 204.53
 ---- batch: 050 ----
mean loss: 200.18
 ---- batch: 060 ----
mean loss: 196.97
 ---- batch: 070 ----
mean loss: 203.37
 ---- batch: 080 ----
mean loss: 206.51
 ---- batch: 090 ----
mean loss: 211.53
train mean loss: 203.45
epoch train time: 0:00:16.833626
elapsed time: 0:37:01.623098
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 02:43:17.619547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.94
 ---- batch: 020 ----
mean loss: 200.44
 ---- batch: 030 ----
mean loss: 200.34
 ---- batch: 040 ----
mean loss: 203.09
 ---- batch: 050 ----
mean loss: 198.59
 ---- batch: 060 ----
mean loss: 198.51
 ---- batch: 070 ----
mean loss: 204.01
 ---- batch: 080 ----
mean loss: 207.16
 ---- batch: 090 ----
mean loss: 205.36
train mean loss: 202.53
epoch train time: 0:00:16.834890
elapsed time: 0:37:18.459268
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 02:43:34.455629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.36
 ---- batch: 020 ----
mean loss: 199.98
 ---- batch: 030 ----
mean loss: 198.99
 ---- batch: 040 ----
mean loss: 204.93
 ---- batch: 050 ----
mean loss: 200.45
 ---- batch: 060 ----
mean loss: 201.48
 ---- batch: 070 ----
mean loss: 200.74
 ---- batch: 080 ----
mean loss: 199.28
 ---- batch: 090 ----
mean loss: 210.44
train mean loss: 201.69
epoch train time: 0:00:16.847586
elapsed time: 0:37:35.308060
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 02:43:51.304444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.86
 ---- batch: 020 ----
mean loss: 200.48
 ---- batch: 030 ----
mean loss: 210.87
 ---- batch: 040 ----
mean loss: 213.48
 ---- batch: 050 ----
mean loss: 208.04
 ---- batch: 060 ----
mean loss: 194.15
 ---- batch: 070 ----
mean loss: 203.03
 ---- batch: 080 ----
mean loss: 209.16
 ---- batch: 090 ----
mean loss: 202.27
train mean loss: 205.76
epoch train time: 0:00:16.894940
elapsed time: 0:37:52.204266
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 02:44:08.200692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.69
 ---- batch: 020 ----
mean loss: 204.30
 ---- batch: 030 ----
mean loss: 198.62
 ---- batch: 040 ----
mean loss: 196.81
 ---- batch: 050 ----
mean loss: 201.40
 ---- batch: 060 ----
mean loss: 203.16
 ---- batch: 070 ----
mean loss: 198.46
 ---- batch: 080 ----
mean loss: 206.14
 ---- batch: 090 ----
mean loss: 195.54
train mean loss: 200.80
epoch train time: 0:00:16.918027
elapsed time: 0:38:09.123572
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 02:44:25.119949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.00
 ---- batch: 020 ----
mean loss: 191.46
 ---- batch: 030 ----
mean loss: 197.19
 ---- batch: 040 ----
mean loss: 201.30
 ---- batch: 050 ----
mean loss: 203.00
 ---- batch: 060 ----
mean loss: 194.74
 ---- batch: 070 ----
mean loss: 201.94
 ---- batch: 080 ----
mean loss: 202.66
 ---- batch: 090 ----
mean loss: 203.44
train mean loss: 200.77
epoch train time: 0:00:16.908177
elapsed time: 0:38:26.032942
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 02:44:42.029305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.52
 ---- batch: 020 ----
mean loss: 202.17
 ---- batch: 030 ----
mean loss: 199.52
 ---- batch: 040 ----
mean loss: 198.16
 ---- batch: 050 ----
mean loss: 201.76
 ---- batch: 060 ----
mean loss: 210.97
 ---- batch: 070 ----
mean loss: 201.65
 ---- batch: 080 ----
mean loss: 197.76
 ---- batch: 090 ----
mean loss: 196.46
train mean loss: 200.93
epoch train time: 0:00:16.864792
elapsed time: 0:38:42.898900
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 02:44:58.895275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.11
 ---- batch: 020 ----
mean loss: 200.37
 ---- batch: 030 ----
mean loss: 198.76
 ---- batch: 040 ----
mean loss: 202.86
 ---- batch: 050 ----
mean loss: 193.95
 ---- batch: 060 ----
mean loss: 196.41
 ---- batch: 070 ----
mean loss: 196.33
 ---- batch: 080 ----
mean loss: 200.72
 ---- batch: 090 ----
mean loss: 206.27
train mean loss: 200.31
epoch train time: 0:00:16.887555
elapsed time: 0:38:59.787663
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 02:45:15.784014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.82
 ---- batch: 020 ----
mean loss: 197.00
 ---- batch: 030 ----
mean loss: 205.52
 ---- batch: 040 ----
mean loss: 199.16
 ---- batch: 050 ----
mean loss: 197.69
 ---- batch: 060 ----
mean loss: 202.81
 ---- batch: 070 ----
mean loss: 198.69
 ---- batch: 080 ----
mean loss: 197.12
 ---- batch: 090 ----
mean loss: 199.06
train mean loss: 199.85
epoch train time: 0:00:16.895645
elapsed time: 0:39:16.684451
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 02:45:32.680875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.12
 ---- batch: 020 ----
mean loss: 200.86
 ---- batch: 030 ----
mean loss: 197.95
 ---- batch: 040 ----
mean loss: 201.53
 ---- batch: 050 ----
mean loss: 194.88
 ---- batch: 060 ----
mean loss: 201.16
 ---- batch: 070 ----
mean loss: 206.86
 ---- batch: 080 ----
mean loss: 203.25
 ---- batch: 090 ----
mean loss: 196.72
train mean loss: 200.52
epoch train time: 0:00:16.928377
elapsed time: 0:39:33.614177
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 02:45:49.610615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.77
 ---- batch: 020 ----
mean loss: 204.17
 ---- batch: 030 ----
mean loss: 202.91
 ---- batch: 040 ----
mean loss: 196.88
 ---- batch: 050 ----
mean loss: 205.88
 ---- batch: 060 ----
mean loss: 198.55
 ---- batch: 070 ----
mean loss: 199.41
 ---- batch: 080 ----
mean loss: 197.82
 ---- batch: 090 ----
mean loss: 206.80
train mean loss: 201.68
epoch train time: 0:00:16.922089
elapsed time: 0:39:50.537603
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 02:46:06.534025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.91
 ---- batch: 020 ----
mean loss: 196.22
 ---- batch: 030 ----
mean loss: 201.83
 ---- batch: 040 ----
mean loss: 198.57
 ---- batch: 050 ----
mean loss: 193.53
 ---- batch: 060 ----
mean loss: 199.55
 ---- batch: 070 ----
mean loss: 201.04
 ---- batch: 080 ----
mean loss: 207.08
 ---- batch: 090 ----
mean loss: 194.54
train mean loss: 198.23
epoch train time: 0:00:16.908414
elapsed time: 0:40:07.447272
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 02:46:23.443647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.22
 ---- batch: 020 ----
mean loss: 197.63
 ---- batch: 030 ----
mean loss: 194.85
 ---- batch: 040 ----
mean loss: 199.61
 ---- batch: 050 ----
mean loss: 200.45
 ---- batch: 060 ----
mean loss: 198.85
 ---- batch: 070 ----
mean loss: 199.19
 ---- batch: 080 ----
mean loss: 198.11
 ---- batch: 090 ----
mean loss: 204.22
train mean loss: 198.42
epoch train time: 0:00:16.884245
elapsed time: 0:40:24.332813
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 02:46:40.329206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.08
 ---- batch: 020 ----
mean loss: 201.11
 ---- batch: 030 ----
mean loss: 207.24
 ---- batch: 040 ----
mean loss: 187.02
 ---- batch: 050 ----
mean loss: 202.78
 ---- batch: 060 ----
mean loss: 197.44
 ---- batch: 070 ----
mean loss: 199.62
 ---- batch: 080 ----
mean loss: 194.82
 ---- batch: 090 ----
mean loss: 202.81
train mean loss: 198.64
epoch train time: 0:00:16.882192
elapsed time: 0:40:41.216186
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 02:46:57.212720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.34
 ---- batch: 020 ----
mean loss: 195.97
 ---- batch: 030 ----
mean loss: 204.37
 ---- batch: 040 ----
mean loss: 206.33
 ---- batch: 050 ----
mean loss: 206.30
 ---- batch: 060 ----
mean loss: 201.78
 ---- batch: 070 ----
mean loss: 202.88
 ---- batch: 080 ----
mean loss: 212.09
 ---- batch: 090 ----
mean loss: 195.20
train mean loss: 202.60
epoch train time: 0:00:16.745222
elapsed time: 0:40:57.962715
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 02:47:13.959134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.89
 ---- batch: 020 ----
mean loss: 202.91
 ---- batch: 030 ----
mean loss: 191.79
 ---- batch: 040 ----
mean loss: 203.77
 ---- batch: 050 ----
mean loss: 193.80
 ---- batch: 060 ----
mean loss: 192.02
 ---- batch: 070 ----
mean loss: 199.17
 ---- batch: 080 ----
mean loss: 192.74
 ---- batch: 090 ----
mean loss: 192.86
train mean loss: 197.62
epoch train time: 0:00:16.761936
elapsed time: 0:41:14.725898
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 02:47:30.722247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.46
 ---- batch: 020 ----
mean loss: 205.14
 ---- batch: 030 ----
mean loss: 197.89
 ---- batch: 040 ----
mean loss: 197.87
 ---- batch: 050 ----
mean loss: 189.34
 ---- batch: 060 ----
mean loss: 204.33
 ---- batch: 070 ----
mean loss: 199.94
 ---- batch: 080 ----
mean loss: 196.53
 ---- batch: 090 ----
mean loss: 200.04
train mean loss: 198.45
epoch train time: 0:00:16.763347
elapsed time: 0:41:31.490365
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 02:47:47.486738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.08
 ---- batch: 020 ----
mean loss: 203.05
 ---- batch: 030 ----
mean loss: 192.68
 ---- batch: 040 ----
mean loss: 200.75
 ---- batch: 050 ----
mean loss: 202.55
 ---- batch: 060 ----
mean loss: 201.06
 ---- batch: 070 ----
mean loss: 192.93
 ---- batch: 080 ----
mean loss: 202.37
 ---- batch: 090 ----
mean loss: 196.70
train mean loss: 197.45
epoch train time: 0:00:16.848503
elapsed time: 0:41:48.340048
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 02:48:04.336458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.51
 ---- batch: 020 ----
mean loss: 200.45
 ---- batch: 030 ----
mean loss: 201.34
 ---- batch: 040 ----
mean loss: 201.10
 ---- batch: 050 ----
mean loss: 197.03
 ---- batch: 060 ----
mean loss: 197.85
 ---- batch: 070 ----
mean loss: 195.72
 ---- batch: 080 ----
mean loss: 190.61
 ---- batch: 090 ----
mean loss: 195.00
train mean loss: 197.35
epoch train time: 0:00:16.905313
elapsed time: 0:42:05.246690
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 02:48:21.242977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.79
 ---- batch: 020 ----
mean loss: 200.50
 ---- batch: 030 ----
mean loss: 197.28
 ---- batch: 040 ----
mean loss: 196.56
 ---- batch: 050 ----
mean loss: 198.04
 ---- batch: 060 ----
mean loss: 201.24
 ---- batch: 070 ----
mean loss: 192.36
 ---- batch: 080 ----
mean loss: 200.77
 ---- batch: 090 ----
mean loss: 200.22
train mean loss: 197.53
epoch train time: 0:00:16.781636
elapsed time: 0:42:22.029510
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 02:48:38.026030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.97
 ---- batch: 020 ----
mean loss: 196.16
 ---- batch: 030 ----
mean loss: 198.42
 ---- batch: 040 ----
mean loss: 197.95
 ---- batch: 050 ----
mean loss: 189.12
 ---- batch: 060 ----
mean loss: 191.81
 ---- batch: 070 ----
mean loss: 199.71
 ---- batch: 080 ----
mean loss: 199.96
 ---- batch: 090 ----
mean loss: 197.72
train mean loss: 196.55
epoch train time: 0:00:16.732744
elapsed time: 0:42:38.764135
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 02:48:54.760179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.53
 ---- batch: 020 ----
mean loss: 202.33
 ---- batch: 030 ----
mean loss: 196.59
 ---- batch: 040 ----
mean loss: 201.63
 ---- batch: 050 ----
mean loss: 197.42
 ---- batch: 060 ----
mean loss: 202.66
 ---- batch: 070 ----
mean loss: 201.13
 ---- batch: 080 ----
mean loss: 193.08
 ---- batch: 090 ----
mean loss: 196.24
train mean loss: 198.29
epoch train time: 0:00:16.741479
elapsed time: 0:42:55.506472
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 02:49:11.502840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.54
 ---- batch: 020 ----
mean loss: 203.33
 ---- batch: 030 ----
mean loss: 194.50
 ---- batch: 040 ----
mean loss: 204.38
 ---- batch: 050 ----
mean loss: 195.84
 ---- batch: 060 ----
mean loss: 198.92
 ---- batch: 070 ----
mean loss: 186.57
 ---- batch: 080 ----
mean loss: 204.99
 ---- batch: 090 ----
mean loss: 196.03
train mean loss: 198.61
epoch train time: 0:00:16.995997
elapsed time: 0:43:12.503596
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 02:49:28.499967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.26
 ---- batch: 020 ----
mean loss: 194.43
 ---- batch: 030 ----
mean loss: 192.92
 ---- batch: 040 ----
mean loss: 195.32
 ---- batch: 050 ----
mean loss: 195.91
 ---- batch: 060 ----
mean loss: 210.15
 ---- batch: 070 ----
mean loss: 201.11
 ---- batch: 080 ----
mean loss: 198.15
 ---- batch: 090 ----
mean loss: 204.16
train mean loss: 198.09
epoch train time: 0:00:16.921428
elapsed time: 0:43:29.426213
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 02:49:45.422611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.70
 ---- batch: 020 ----
mean loss: 202.10
 ---- batch: 030 ----
mean loss: 202.58
 ---- batch: 040 ----
mean loss: 190.99
 ---- batch: 050 ----
mean loss: 195.66
 ---- batch: 060 ----
mean loss: 197.56
 ---- batch: 070 ----
mean loss: 199.60
 ---- batch: 080 ----
mean loss: 191.35
 ---- batch: 090 ----
mean loss: 197.02
train mean loss: 197.05
epoch train time: 0:00:16.897928
elapsed time: 0:43:46.325285
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 02:50:02.321713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.28
 ---- batch: 020 ----
mean loss: 192.80
 ---- batch: 030 ----
mean loss: 202.70
 ---- batch: 040 ----
mean loss: 203.25
 ---- batch: 050 ----
mean loss: 197.88
 ---- batch: 060 ----
mean loss: 192.99
 ---- batch: 070 ----
mean loss: 192.69
 ---- batch: 080 ----
mean loss: 191.12
 ---- batch: 090 ----
mean loss: 194.20
train mean loss: 196.69
epoch train time: 0:00:16.878198
elapsed time: 0:44:03.204798
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 02:50:19.201189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.63
 ---- batch: 020 ----
mean loss: 197.53
 ---- batch: 030 ----
mean loss: 192.44
 ---- batch: 040 ----
mean loss: 196.95
 ---- batch: 050 ----
mean loss: 197.08
 ---- batch: 060 ----
mean loss: 206.20
 ---- batch: 070 ----
mean loss: 198.55
 ---- batch: 080 ----
mean loss: 198.45
 ---- batch: 090 ----
mean loss: 195.46
train mean loss: 197.13
epoch train time: 0:00:16.914549
elapsed time: 0:44:20.120564
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 02:50:36.117015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.74
 ---- batch: 020 ----
mean loss: 197.04
 ---- batch: 030 ----
mean loss: 196.97
 ---- batch: 040 ----
mean loss: 190.87
 ---- batch: 050 ----
mean loss: 190.41
 ---- batch: 060 ----
mean loss: 203.12
 ---- batch: 070 ----
mean loss: 194.90
 ---- batch: 080 ----
mean loss: 190.97
 ---- batch: 090 ----
mean loss: 191.86
train mean loss: 194.89
epoch train time: 0:00:16.870479
elapsed time: 0:44:36.992287
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 02:50:52.988701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.49
 ---- batch: 020 ----
mean loss: 192.80
 ---- batch: 030 ----
mean loss: 207.47
 ---- batch: 040 ----
mean loss: 202.39
 ---- batch: 050 ----
mean loss: 187.01
 ---- batch: 060 ----
mean loss: 195.58
 ---- batch: 070 ----
mean loss: 201.47
 ---- batch: 080 ----
mean loss: 214.75
 ---- batch: 090 ----
mean loss: 196.53
train mean loss: 198.96
epoch train time: 0:00:16.891624
elapsed time: 0:44:53.885102
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 02:51:09.881477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.69
 ---- batch: 020 ----
mean loss: 203.20
 ---- batch: 030 ----
mean loss: 197.30
 ---- batch: 040 ----
mean loss: 200.68
 ---- batch: 050 ----
mean loss: 199.76
 ---- batch: 060 ----
mean loss: 200.01
 ---- batch: 070 ----
mean loss: 192.94
 ---- batch: 080 ----
mean loss: 190.53
 ---- batch: 090 ----
mean loss: 191.29
train mean loss: 196.85
epoch train time: 0:00:16.871507
elapsed time: 0:45:10.757750
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 02:51:26.754132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.03
 ---- batch: 020 ----
mean loss: 192.65
 ---- batch: 030 ----
mean loss: 193.07
 ---- batch: 040 ----
mean loss: 189.82
 ---- batch: 050 ----
mean loss: 189.61
 ---- batch: 060 ----
mean loss: 190.31
 ---- batch: 070 ----
mean loss: 194.95
 ---- batch: 080 ----
mean loss: 200.51
 ---- batch: 090 ----
mean loss: 194.40
train mean loss: 194.76
epoch train time: 0:00:16.893616
elapsed time: 0:45:27.652560
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 02:51:43.648956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.45
 ---- batch: 020 ----
mean loss: 184.23
 ---- batch: 030 ----
mean loss: 197.83
 ---- batch: 040 ----
mean loss: 199.50
 ---- batch: 050 ----
mean loss: 198.56
 ---- batch: 060 ----
mean loss: 193.94
 ---- batch: 070 ----
mean loss: 198.97
 ---- batch: 080 ----
mean loss: 196.89
 ---- batch: 090 ----
mean loss: 194.39
train mean loss: 194.89
epoch train time: 0:00:16.902299
elapsed time: 0:45:44.556050
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 02:52:00.552473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.80
 ---- batch: 020 ----
mean loss: 195.45
 ---- batch: 030 ----
mean loss: 191.32
 ---- batch: 040 ----
mean loss: 198.17
 ---- batch: 050 ----
mean loss: 197.24
 ---- batch: 060 ----
mean loss: 195.89
 ---- batch: 070 ----
mean loss: 203.57
 ---- batch: 080 ----
mean loss: 194.14
 ---- batch: 090 ----
mean loss: 191.84
train mean loss: 195.33
epoch train time: 0:00:16.917273
elapsed time: 0:46:01.474532
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 02:52:17.470972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.22
 ---- batch: 020 ----
mean loss: 190.12
 ---- batch: 030 ----
mean loss: 196.91
 ---- batch: 040 ----
mean loss: 193.31
 ---- batch: 050 ----
mean loss: 194.37
 ---- batch: 060 ----
mean loss: 206.33
 ---- batch: 070 ----
mean loss: 202.59
 ---- batch: 080 ----
mean loss: 194.63
 ---- batch: 090 ----
mean loss: 195.94
train mean loss: 196.60
epoch train time: 0:00:16.897789
elapsed time: 0:46:18.373566
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 02:52:34.370049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.96
 ---- batch: 020 ----
mean loss: 201.84
 ---- batch: 030 ----
mean loss: 200.65
 ---- batch: 040 ----
mean loss: 201.85
 ---- batch: 050 ----
mean loss: 191.19
 ---- batch: 060 ----
mean loss: 197.22
 ---- batch: 070 ----
mean loss: 196.60
 ---- batch: 080 ----
mean loss: 196.43
 ---- batch: 090 ----
mean loss: 188.28
train mean loss: 195.57
epoch train time: 0:00:16.894992
elapsed time: 0:46:35.269940
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 02:52:51.266221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.38
 ---- batch: 020 ----
mean loss: 185.89
 ---- batch: 030 ----
mean loss: 191.94
 ---- batch: 040 ----
mean loss: 195.21
 ---- batch: 050 ----
mean loss: 193.87
 ---- batch: 060 ----
mean loss: 197.01
 ---- batch: 070 ----
mean loss: 195.57
 ---- batch: 080 ----
mean loss: 187.30
 ---- batch: 090 ----
mean loss: 195.81
train mean loss: 193.65
epoch train time: 0:00:16.730654
elapsed time: 0:46:52.001650
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 02:53:07.998075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.73
 ---- batch: 020 ----
mean loss: 189.67
 ---- batch: 030 ----
mean loss: 191.57
 ---- batch: 040 ----
mean loss: 187.22
 ---- batch: 050 ----
mean loss: 199.81
 ---- batch: 060 ----
mean loss: 196.91
 ---- batch: 070 ----
mean loss: 198.63
 ---- batch: 080 ----
mean loss: 194.29
 ---- batch: 090 ----
mean loss: 200.71
train mean loss: 194.45
epoch train time: 0:00:16.828895
elapsed time: 0:47:08.831797
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 02:53:24.828179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.61
 ---- batch: 020 ----
mean loss: 196.44
 ---- batch: 030 ----
mean loss: 188.82
 ---- batch: 040 ----
mean loss: 202.69
 ---- batch: 050 ----
mean loss: 193.80
 ---- batch: 060 ----
mean loss: 204.80
 ---- batch: 070 ----
mean loss: 193.92
 ---- batch: 080 ----
mean loss: 193.62
 ---- batch: 090 ----
mean loss: 198.08
train mean loss: 195.75
epoch train time: 0:00:16.894380
elapsed time: 0:47:25.727364
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 02:53:41.723767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.56
 ---- batch: 020 ----
mean loss: 193.79
 ---- batch: 030 ----
mean loss: 196.75
 ---- batch: 040 ----
mean loss: 193.81
 ---- batch: 050 ----
mean loss: 192.44
 ---- batch: 060 ----
mean loss: 190.75
 ---- batch: 070 ----
mean loss: 183.37
 ---- batch: 080 ----
mean loss: 197.16
 ---- batch: 090 ----
mean loss: 199.52
train mean loss: 194.04
epoch train time: 0:00:16.717767
elapsed time: 0:47:42.446325
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 02:53:58.442775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.26
 ---- batch: 020 ----
mean loss: 192.14
 ---- batch: 030 ----
mean loss: 197.36
 ---- batch: 040 ----
mean loss: 188.24
 ---- batch: 050 ----
mean loss: 193.46
 ---- batch: 060 ----
mean loss: 192.27
 ---- batch: 070 ----
mean loss: 197.67
 ---- batch: 080 ----
mean loss: 196.48
 ---- batch: 090 ----
mean loss: 191.88
train mean loss: 193.79
epoch train time: 0:00:16.656853
elapsed time: 0:47:59.104387
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 02:54:15.100739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.34
 ---- batch: 020 ----
mean loss: 197.15
 ---- batch: 030 ----
mean loss: 187.91
 ---- batch: 040 ----
mean loss: 198.31
 ---- batch: 050 ----
mean loss: 199.58
 ---- batch: 060 ----
mean loss: 204.21
 ---- batch: 070 ----
mean loss: 190.92
 ---- batch: 080 ----
mean loss: 196.07
 ---- batch: 090 ----
mean loss: 194.32
train mean loss: 196.23
epoch train time: 0:00:16.689702
elapsed time: 0:48:15.795219
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 02:54:31.791600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.34
 ---- batch: 020 ----
mean loss: 190.62
 ---- batch: 030 ----
mean loss: 188.02
 ---- batch: 040 ----
mean loss: 192.06
 ---- batch: 050 ----
mean loss: 194.86
 ---- batch: 060 ----
mean loss: 198.69
 ---- batch: 070 ----
mean loss: 198.27
 ---- batch: 080 ----
mean loss: 196.59
 ---- batch: 090 ----
mean loss: 188.98
train mean loss: 193.73
epoch train time: 0:00:16.689571
elapsed time: 0:48:32.486262
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 02:54:48.482652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.82
 ---- batch: 020 ----
mean loss: 192.52
 ---- batch: 030 ----
mean loss: 190.55
 ---- batch: 040 ----
mean loss: 192.98
 ---- batch: 050 ----
mean loss: 201.90
 ---- batch: 060 ----
mean loss: 191.56
 ---- batch: 070 ----
mean loss: 193.96
 ---- batch: 080 ----
mean loss: 183.28
 ---- batch: 090 ----
mean loss: 195.02
train mean loss: 194.06
epoch train time: 0:00:16.699519
elapsed time: 0:48:49.186924
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 02:55:05.183439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.44
 ---- batch: 020 ----
mean loss: 192.70
 ---- batch: 030 ----
mean loss: 189.16
 ---- batch: 040 ----
mean loss: 195.73
 ---- batch: 050 ----
mean loss: 190.76
 ---- batch: 060 ----
mean loss: 194.77
 ---- batch: 070 ----
mean loss: 199.66
 ---- batch: 080 ----
mean loss: 189.65
 ---- batch: 090 ----
mean loss: 200.74
train mean loss: 193.44
epoch train time: 0:00:16.640456
elapsed time: 0:49:05.828807
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 02:55:21.825291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.79
 ---- batch: 020 ----
mean loss: 191.10
 ---- batch: 030 ----
mean loss: 192.89
 ---- batch: 040 ----
mean loss: 196.77
 ---- batch: 050 ----
mean loss: 189.60
 ---- batch: 060 ----
mean loss: 190.78
 ---- batch: 070 ----
mean loss: 195.99
 ---- batch: 080 ----
mean loss: 192.31
 ---- batch: 090 ----
mean loss: 189.83
train mean loss: 193.53
epoch train time: 0:00:16.689043
elapsed time: 0:49:22.519181
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 02:55:38.515739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.62
 ---- batch: 020 ----
mean loss: 187.83
 ---- batch: 030 ----
mean loss: 196.12
 ---- batch: 040 ----
mean loss: 186.67
 ---- batch: 050 ----
mean loss: 189.22
 ---- batch: 060 ----
mean loss: 196.24
 ---- batch: 070 ----
mean loss: 197.04
 ---- batch: 080 ----
mean loss: 203.14
 ---- batch: 090 ----
mean loss: 188.35
train mean loss: 192.71
epoch train time: 0:00:16.696862
elapsed time: 0:49:39.217621
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 02:55:55.213637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.97
 ---- batch: 020 ----
mean loss: 198.17
 ---- batch: 030 ----
mean loss: 200.97
 ---- batch: 040 ----
mean loss: 185.57
 ---- batch: 050 ----
mean loss: 189.99
 ---- batch: 060 ----
mean loss: 200.88
 ---- batch: 070 ----
mean loss: 189.97
 ---- batch: 080 ----
mean loss: 192.09
 ---- batch: 090 ----
mean loss: 190.72
train mean loss: 192.99
epoch train time: 0:00:16.671459
elapsed time: 0:49:55.889902
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 02:56:11.886289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.87
 ---- batch: 020 ----
mean loss: 188.40
 ---- batch: 030 ----
mean loss: 190.88
 ---- batch: 040 ----
mean loss: 193.74
 ---- batch: 050 ----
mean loss: 193.55
 ---- batch: 060 ----
mean loss: 195.37
 ---- batch: 070 ----
mean loss: 196.10
 ---- batch: 080 ----
mean loss: 185.72
 ---- batch: 090 ----
mean loss: 191.97
train mean loss: 192.34
epoch train time: 0:00:16.713525
elapsed time: 0:50:12.604615
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 02:56:28.601055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.79
 ---- batch: 020 ----
mean loss: 193.31
 ---- batch: 030 ----
mean loss: 191.82
 ---- batch: 040 ----
mean loss: 194.58
 ---- batch: 050 ----
mean loss: 196.24
 ---- batch: 060 ----
mean loss: 191.32
 ---- batch: 070 ----
mean loss: 184.85
 ---- batch: 080 ----
mean loss: 185.48
 ---- batch: 090 ----
mean loss: 196.80
train mean loss: 191.41
epoch train time: 0:00:16.692598
elapsed time: 0:50:29.298419
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 02:56:45.294850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.46
 ---- batch: 020 ----
mean loss: 195.35
 ---- batch: 030 ----
mean loss: 193.92
 ---- batch: 040 ----
mean loss: 186.63
 ---- batch: 050 ----
mean loss: 186.91
 ---- batch: 060 ----
mean loss: 193.70
 ---- batch: 070 ----
mean loss: 198.76
 ---- batch: 080 ----
mean loss: 187.35
 ---- batch: 090 ----
mean loss: 188.20
train mean loss: 191.63
epoch train time: 0:00:16.678119
elapsed time: 0:50:45.977883
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 02:57:01.974282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.40
 ---- batch: 020 ----
mean loss: 192.70
 ---- batch: 030 ----
mean loss: 193.24
 ---- batch: 040 ----
mean loss: 191.96
 ---- batch: 050 ----
mean loss: 189.03
 ---- batch: 060 ----
mean loss: 193.77
 ---- batch: 070 ----
mean loss: 191.29
 ---- batch: 080 ----
mean loss: 182.01
 ---- batch: 090 ----
mean loss: 193.54
train mean loss: 191.34
epoch train time: 0:00:16.711918
elapsed time: 0:51:02.690986
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 02:57:18.687413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.78
 ---- batch: 020 ----
mean loss: 190.96
 ---- batch: 030 ----
mean loss: 194.15
 ---- batch: 040 ----
mean loss: 188.29
 ---- batch: 050 ----
mean loss: 194.63
 ---- batch: 060 ----
mean loss: 193.09
 ---- batch: 070 ----
mean loss: 189.48
 ---- batch: 080 ----
mean loss: 192.66
 ---- batch: 090 ----
mean loss: 192.50
train mean loss: 191.97
epoch train time: 0:00:16.702016
elapsed time: 0:51:19.394360
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 02:57:35.390857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.77
 ---- batch: 020 ----
mean loss: 191.79
 ---- batch: 030 ----
mean loss: 192.40
 ---- batch: 040 ----
mean loss: 194.37
 ---- batch: 050 ----
mean loss: 190.94
 ---- batch: 060 ----
mean loss: 184.10
 ---- batch: 070 ----
mean loss: 187.94
 ---- batch: 080 ----
mean loss: 189.61
 ---- batch: 090 ----
mean loss: 189.45
train mean loss: 190.94
epoch train time: 0:00:16.675081
elapsed time: 0:51:36.070740
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 02:57:52.067157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.92
 ---- batch: 020 ----
mean loss: 196.04
 ---- batch: 030 ----
mean loss: 192.59
 ---- batch: 040 ----
mean loss: 191.83
 ---- batch: 050 ----
mean loss: 189.80
 ---- batch: 060 ----
mean loss: 194.72
 ---- batch: 070 ----
mean loss: 194.12
 ---- batch: 080 ----
mean loss: 190.37
 ---- batch: 090 ----
mean loss: 189.06
train mean loss: 192.38
epoch train time: 0:00:16.687216
elapsed time: 0:51:52.759141
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 02:58:08.755529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.33
 ---- batch: 020 ----
mean loss: 190.55
 ---- batch: 030 ----
mean loss: 197.07
 ---- batch: 040 ----
mean loss: 193.54
 ---- batch: 050 ----
mean loss: 185.43
 ---- batch: 060 ----
mean loss: 195.34
 ---- batch: 070 ----
mean loss: 189.18
 ---- batch: 080 ----
mean loss: 192.97
 ---- batch: 090 ----
mean loss: 181.89
train mean loss: 191.14
epoch train time: 0:00:16.665386
elapsed time: 0:52:09.425685
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 02:58:25.422057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.36
 ---- batch: 020 ----
mean loss: 194.22
 ---- batch: 030 ----
mean loss: 191.11
 ---- batch: 040 ----
mean loss: 196.34
 ---- batch: 050 ----
mean loss: 195.20
 ---- batch: 060 ----
mean loss: 190.09
 ---- batch: 070 ----
mean loss: 189.20
 ---- batch: 080 ----
mean loss: 194.18
 ---- batch: 090 ----
mean loss: 188.80
train mean loss: 192.34
epoch train time: 0:00:16.698998
elapsed time: 0:52:26.125888
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 02:58:42.122345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.75
 ---- batch: 020 ----
mean loss: 193.07
 ---- batch: 030 ----
mean loss: 189.42
 ---- batch: 040 ----
mean loss: 193.62
 ---- batch: 050 ----
mean loss: 191.10
 ---- batch: 060 ----
mean loss: 192.73
 ---- batch: 070 ----
mean loss: 188.93
 ---- batch: 080 ----
mean loss: 187.21
 ---- batch: 090 ----
mean loss: 193.81
train mean loss: 191.56
epoch train time: 0:00:16.697916
elapsed time: 0:52:42.825128
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 02:58:58.821629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.03
 ---- batch: 020 ----
mean loss: 192.45
 ---- batch: 030 ----
mean loss: 187.85
 ---- batch: 040 ----
mean loss: 195.22
 ---- batch: 050 ----
mean loss: 198.69
 ---- batch: 060 ----
mean loss: 191.17
 ---- batch: 070 ----
mean loss: 191.79
 ---- batch: 080 ----
mean loss: 189.76
 ---- batch: 090 ----
mean loss: 193.62
train mean loss: 192.91
epoch train time: 0:00:16.690761
elapsed time: 0:52:59.517164
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 02:59:15.513532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.77
 ---- batch: 020 ----
mean loss: 184.34
 ---- batch: 030 ----
mean loss: 195.08
 ---- batch: 040 ----
mean loss: 191.22
 ---- batch: 050 ----
mean loss: 187.89
 ---- batch: 060 ----
mean loss: 193.02
 ---- batch: 070 ----
mean loss: 197.03
 ---- batch: 080 ----
mean loss: 189.56
 ---- batch: 090 ----
mean loss: 194.88
train mean loss: 191.07
epoch train time: 0:00:16.714760
elapsed time: 0:53:16.233058
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 02:59:32.229469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.40
 ---- batch: 020 ----
mean loss: 197.66
 ---- batch: 030 ----
mean loss: 191.27
 ---- batch: 040 ----
mean loss: 192.18
 ---- batch: 050 ----
mean loss: 191.87
 ---- batch: 060 ----
mean loss: 191.01
 ---- batch: 070 ----
mean loss: 192.23
 ---- batch: 080 ----
mean loss: 191.41
 ---- batch: 090 ----
mean loss: 190.51
train mean loss: 191.78
epoch train time: 0:00:16.768000
elapsed time: 0:53:33.002322
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 02:59:48.998681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.42
 ---- batch: 020 ----
mean loss: 199.43
 ---- batch: 030 ----
mean loss: 187.08
 ---- batch: 040 ----
mean loss: 186.54
 ---- batch: 050 ----
mean loss: 190.28
 ---- batch: 060 ----
mean loss: 193.15
 ---- batch: 070 ----
mean loss: 193.76
 ---- batch: 080 ----
mean loss: 194.89
 ---- batch: 090 ----
mean loss: 191.40
train mean loss: 192.23
epoch train time: 0:00:16.743763
elapsed time: 0:53:49.747238
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 03:00:05.743681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.87
 ---- batch: 020 ----
mean loss: 194.21
 ---- batch: 030 ----
mean loss: 204.76
 ---- batch: 040 ----
mean loss: 189.25
 ---- batch: 050 ----
mean loss: 187.91
 ---- batch: 060 ----
mean loss: 177.85
 ---- batch: 070 ----
mean loss: 187.48
 ---- batch: 080 ----
mean loss: 197.93
 ---- batch: 090 ----
mean loss: 198.58
train mean loss: 192.40
epoch train time: 0:00:16.628236
elapsed time: 0:54:06.376693
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 03:00:22.373062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.46
 ---- batch: 020 ----
mean loss: 187.49
 ---- batch: 030 ----
mean loss: 194.50
 ---- batch: 040 ----
mean loss: 204.53
 ---- batch: 050 ----
mean loss: 202.32
 ---- batch: 060 ----
mean loss: 190.44
 ---- batch: 070 ----
mean loss: 191.14
 ---- batch: 080 ----
mean loss: 194.73
 ---- batch: 090 ----
mean loss: 183.38
train mean loss: 192.59
epoch train time: 0:00:16.670619
elapsed time: 0:54:23.048476
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 03:00:39.044857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.74
 ---- batch: 020 ----
mean loss: 192.76
 ---- batch: 030 ----
mean loss: 193.88
 ---- batch: 040 ----
mean loss: 185.70
 ---- batch: 050 ----
mean loss: 189.22
 ---- batch: 060 ----
mean loss: 195.46
 ---- batch: 070 ----
mean loss: 190.61
 ---- batch: 080 ----
mean loss: 185.26
 ---- batch: 090 ----
mean loss: 190.36
train mean loss: 190.26
epoch train time: 0:00:16.711522
elapsed time: 0:54:39.761184
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 03:00:55.757658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.17
 ---- batch: 020 ----
mean loss: 195.95
 ---- batch: 030 ----
mean loss: 188.85
 ---- batch: 040 ----
mean loss: 188.69
 ---- batch: 050 ----
mean loss: 190.32
 ---- batch: 060 ----
mean loss: 194.49
 ---- batch: 070 ----
mean loss: 182.46
 ---- batch: 080 ----
mean loss: 184.02
 ---- batch: 090 ----
mean loss: 187.88
train mean loss: 189.24
epoch train time: 0:00:16.658628
elapsed time: 0:54:56.421086
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 03:01:12.417469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.64
 ---- batch: 020 ----
mean loss: 183.39
 ---- batch: 030 ----
mean loss: 189.41
 ---- batch: 040 ----
mean loss: 188.59
 ---- batch: 050 ----
mean loss: 185.24
 ---- batch: 060 ----
mean loss: 180.10
 ---- batch: 070 ----
mean loss: 197.34
 ---- batch: 080 ----
mean loss: 193.10
 ---- batch: 090 ----
mean loss: 196.59
train mean loss: 189.59
epoch train time: 0:00:16.663007
elapsed time: 0:55:13.085256
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 03:01:29.081807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.39
 ---- batch: 020 ----
mean loss: 190.62
 ---- batch: 030 ----
mean loss: 189.07
 ---- batch: 040 ----
mean loss: 190.21
 ---- batch: 050 ----
mean loss: 182.86
 ---- batch: 060 ----
mean loss: 191.19
 ---- batch: 070 ----
mean loss: 189.30
 ---- batch: 080 ----
mean loss: 192.41
 ---- batch: 090 ----
mean loss: 188.35
train mean loss: 189.52
epoch train time: 0:00:16.639707
elapsed time: 0:55:29.726311
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 03:01:45.722684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.01
 ---- batch: 020 ----
mean loss: 192.01
 ---- batch: 030 ----
mean loss: 194.15
 ---- batch: 040 ----
mean loss: 188.41
 ---- batch: 050 ----
mean loss: 187.19
 ---- batch: 060 ----
mean loss: 191.71
 ---- batch: 070 ----
mean loss: 191.77
 ---- batch: 080 ----
mean loss: 187.88
 ---- batch: 090 ----
mean loss: 186.43
train mean loss: 189.22
epoch train time: 0:00:16.642167
elapsed time: 0:55:46.369624
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 03:02:02.366050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.57
 ---- batch: 020 ----
mean loss: 193.39
 ---- batch: 030 ----
mean loss: 187.36
 ---- batch: 040 ----
mean loss: 189.17
 ---- batch: 050 ----
mean loss: 189.57
 ---- batch: 060 ----
mean loss: 187.15
 ---- batch: 070 ----
mean loss: 188.73
 ---- batch: 080 ----
mean loss: 182.30
 ---- batch: 090 ----
mean loss: 190.99
train mean loss: 189.17
epoch train time: 0:00:16.627259
elapsed time: 0:56:02.998065
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 03:02:18.994518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.44
 ---- batch: 020 ----
mean loss: 191.11
 ---- batch: 030 ----
mean loss: 190.48
 ---- batch: 040 ----
mean loss: 187.97
 ---- batch: 050 ----
mean loss: 187.52
 ---- batch: 060 ----
mean loss: 195.72
 ---- batch: 070 ----
mean loss: 183.99
 ---- batch: 080 ----
mean loss: 186.05
 ---- batch: 090 ----
mean loss: 195.03
train mean loss: 189.36
epoch train time: 0:00:16.654353
elapsed time: 0:56:19.653657
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 03:02:35.650095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.82
 ---- batch: 020 ----
mean loss: 184.22
 ---- batch: 030 ----
mean loss: 187.48
 ---- batch: 040 ----
mean loss: 195.59
 ---- batch: 050 ----
mean loss: 196.25
 ---- batch: 060 ----
mean loss: 192.07
 ---- batch: 070 ----
mean loss: 190.44
 ---- batch: 080 ----
mean loss: 193.39
 ---- batch: 090 ----
mean loss: 185.59
train mean loss: 190.07
epoch train time: 0:00:16.653753
elapsed time: 0:56:36.308809
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 03:02:52.305187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.99
 ---- batch: 020 ----
mean loss: 180.76
 ---- batch: 030 ----
mean loss: 192.10
 ---- batch: 040 ----
mean loss: 187.39
 ---- batch: 050 ----
mean loss: 187.54
 ---- batch: 060 ----
mean loss: 192.22
 ---- batch: 070 ----
mean loss: 188.78
 ---- batch: 080 ----
mean loss: 191.11
 ---- batch: 090 ----
mean loss: 185.98
train mean loss: 188.13
epoch train time: 0:00:16.641162
elapsed time: 0:56:52.951120
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 03:03:08.947458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.67
 ---- batch: 020 ----
mean loss: 187.79
 ---- batch: 030 ----
mean loss: 179.68
 ---- batch: 040 ----
mean loss: 189.18
 ---- batch: 050 ----
mean loss: 192.31
 ---- batch: 060 ----
mean loss: 191.16
 ---- batch: 070 ----
mean loss: 194.33
 ---- batch: 080 ----
mean loss: 193.65
 ---- batch: 090 ----
mean loss: 195.69
train mean loss: 190.35
epoch train time: 0:00:16.609734
elapsed time: 0:57:09.561947
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 03:03:25.558412
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.37
 ---- batch: 020 ----
mean loss: 186.79
 ---- batch: 030 ----
mean loss: 184.14
 ---- batch: 040 ----
mean loss: 193.83
 ---- batch: 050 ----
mean loss: 185.73
 ---- batch: 060 ----
mean loss: 180.96
 ---- batch: 070 ----
mean loss: 185.96
 ---- batch: 080 ----
mean loss: 189.13
 ---- batch: 090 ----
mean loss: 190.38
train mean loss: 188.37
epoch train time: 0:00:16.697331
elapsed time: 0:57:26.261178
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 03:03:42.257193
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.63
 ---- batch: 020 ----
mean loss: 183.21
 ---- batch: 030 ----
mean loss: 185.09
 ---- batch: 040 ----
mean loss: 179.76
 ---- batch: 050 ----
mean loss: 189.27
 ---- batch: 060 ----
mean loss: 185.58
 ---- batch: 070 ----
mean loss: 183.34
 ---- batch: 080 ----
mean loss: 188.32
 ---- batch: 090 ----
mean loss: 184.42
train mean loss: 185.90
epoch train time: 0:00:16.697812
elapsed time: 0:57:42.959785
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 03:03:58.956172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.60
 ---- batch: 020 ----
mean loss: 190.34
 ---- batch: 030 ----
mean loss: 181.62
 ---- batch: 040 ----
mean loss: 188.86
 ---- batch: 050 ----
mean loss: 188.42
 ---- batch: 060 ----
mean loss: 178.22
 ---- batch: 070 ----
mean loss: 190.31
 ---- batch: 080 ----
mean loss: 185.75
 ---- batch: 090 ----
mean loss: 184.86
train mean loss: 185.67
epoch train time: 0:00:16.682673
elapsed time: 0:57:59.643611
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 03:04:15.640047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.96
 ---- batch: 020 ----
mean loss: 186.99
 ---- batch: 030 ----
mean loss: 191.04
 ---- batch: 040 ----
mean loss: 183.65
 ---- batch: 050 ----
mean loss: 180.91
 ---- batch: 060 ----
mean loss: 185.62
 ---- batch: 070 ----
mean loss: 185.18
 ---- batch: 080 ----
mean loss: 196.32
 ---- batch: 090 ----
mean loss: 181.59
train mean loss: 185.54
epoch train time: 0:00:16.680125
elapsed time: 0:58:16.325001
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 03:04:32.321380
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.22
 ---- batch: 020 ----
mean loss: 188.34
 ---- batch: 030 ----
mean loss: 193.88
 ---- batch: 040 ----
mean loss: 175.72
 ---- batch: 050 ----
mean loss: 187.30
 ---- batch: 060 ----
mean loss: 185.25
 ---- batch: 070 ----
mean loss: 182.03
 ---- batch: 080 ----
mean loss: 195.15
 ---- batch: 090 ----
mean loss: 181.85
train mean loss: 185.66
epoch train time: 0:00:16.645503
elapsed time: 0:58:32.971648
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 03:04:48.968061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.35
 ---- batch: 020 ----
mean loss: 181.47
 ---- batch: 030 ----
mean loss: 183.27
 ---- batch: 040 ----
mean loss: 190.75
 ---- batch: 050 ----
mean loss: 184.64
 ---- batch: 060 ----
mean loss: 184.51
 ---- batch: 070 ----
mean loss: 191.80
 ---- batch: 080 ----
mean loss: 185.32
 ---- batch: 090 ----
mean loss: 186.92
train mean loss: 185.69
epoch train time: 0:00:16.828462
elapsed time: 0:58:49.801361
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 03:05:05.797833
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.81
 ---- batch: 020 ----
mean loss: 179.77
 ---- batch: 030 ----
mean loss: 186.69
 ---- batch: 040 ----
mean loss: 189.41
 ---- batch: 050 ----
mean loss: 188.53
 ---- batch: 060 ----
mean loss: 182.79
 ---- batch: 070 ----
mean loss: 185.31
 ---- batch: 080 ----
mean loss: 181.56
 ---- batch: 090 ----
mean loss: 186.74
train mean loss: 185.78
epoch train time: 0:00:16.619473
elapsed time: 0:59:06.422096
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 03:05:22.418472
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.56
 ---- batch: 020 ----
mean loss: 186.71
 ---- batch: 030 ----
mean loss: 186.20
 ---- batch: 040 ----
mean loss: 194.78
 ---- batch: 050 ----
mean loss: 186.40
 ---- batch: 060 ----
mean loss: 182.81
 ---- batch: 070 ----
mean loss: 184.46
 ---- batch: 080 ----
mean loss: 189.35
 ---- batch: 090 ----
mean loss: 179.81
train mean loss: 185.80
epoch train time: 0:00:16.623489
elapsed time: 0:59:23.046730
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 03:05:39.043082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.35
 ---- batch: 020 ----
mean loss: 191.82
 ---- batch: 030 ----
mean loss: 187.10
 ---- batch: 040 ----
mean loss: 177.84
 ---- batch: 050 ----
mean loss: 193.29
 ---- batch: 060 ----
mean loss: 191.81
 ---- batch: 070 ----
mean loss: 183.37
 ---- batch: 080 ----
mean loss: 181.64
 ---- batch: 090 ----
mean loss: 182.66
train mean loss: 185.84
epoch train time: 0:00:16.657704
elapsed time: 0:59:39.705678
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 03:05:55.702124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.89
 ---- batch: 020 ----
mean loss: 188.15
 ---- batch: 030 ----
mean loss: 188.39
 ---- batch: 040 ----
mean loss: 183.80
 ---- batch: 050 ----
mean loss: 180.24
 ---- batch: 060 ----
mean loss: 191.83
 ---- batch: 070 ----
mean loss: 184.90
 ---- batch: 080 ----
mean loss: 184.55
 ---- batch: 090 ----
mean loss: 183.41
train mean loss: 185.39
epoch train time: 0:00:16.632071
elapsed time: 0:59:56.339009
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 03:06:12.335413
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.09
 ---- batch: 020 ----
mean loss: 180.33
 ---- batch: 030 ----
mean loss: 188.08
 ---- batch: 040 ----
mean loss: 184.88
 ---- batch: 050 ----
mean loss: 183.39
 ---- batch: 060 ----
mean loss: 183.87
 ---- batch: 070 ----
mean loss: 189.99
 ---- batch: 080 ----
mean loss: 187.67
 ---- batch: 090 ----
mean loss: 188.74
train mean loss: 185.61
epoch train time: 0:00:16.638995
elapsed time: 1:00:12.979193
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 03:06:28.975636
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.04
 ---- batch: 020 ----
mean loss: 184.07
 ---- batch: 030 ----
mean loss: 183.80
 ---- batch: 040 ----
mean loss: 191.94
 ---- batch: 050 ----
mean loss: 184.15
 ---- batch: 060 ----
mean loss: 177.62
 ---- batch: 070 ----
mean loss: 188.48
 ---- batch: 080 ----
mean loss: 188.75
 ---- batch: 090 ----
mean loss: 184.21
train mean loss: 185.63
epoch train time: 0:00:16.604804
elapsed time: 1:00:29.585197
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 03:06:45.581567
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.48
 ---- batch: 020 ----
mean loss: 189.08
 ---- batch: 030 ----
mean loss: 181.17
 ---- batch: 040 ----
mean loss: 190.08
 ---- batch: 050 ----
mean loss: 184.94
 ---- batch: 060 ----
mean loss: 188.54
 ---- batch: 070 ----
mean loss: 182.26
 ---- batch: 080 ----
mean loss: 184.91
 ---- batch: 090 ----
mean loss: 185.55
train mean loss: 185.24
epoch train time: 0:00:16.638034
elapsed time: 1:00:46.224425
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 03:07:02.220849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.82
 ---- batch: 020 ----
mean loss: 173.76
 ---- batch: 030 ----
mean loss: 182.29
 ---- batch: 040 ----
mean loss: 186.55
 ---- batch: 050 ----
mean loss: 184.18
 ---- batch: 060 ----
mean loss: 189.73
 ---- batch: 070 ----
mean loss: 192.88
 ---- batch: 080 ----
mean loss: 193.27
 ---- batch: 090 ----
mean loss: 186.88
train mean loss: 185.74
epoch train time: 0:00:16.667715
elapsed time: 1:01:02.893349
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 03:07:18.889802
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.27
 ---- batch: 020 ----
mean loss: 187.59
 ---- batch: 030 ----
mean loss: 183.48
 ---- batch: 040 ----
mean loss: 184.35
 ---- batch: 050 ----
mean loss: 184.85
 ---- batch: 060 ----
mean loss: 187.14
 ---- batch: 070 ----
mean loss: 183.33
 ---- batch: 080 ----
mean loss: 187.22
 ---- batch: 090 ----
mean loss: 183.54
train mean loss: 185.95
epoch train time: 0:00:16.646512
elapsed time: 1:01:19.541131
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 03:07:35.537641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.38
 ---- batch: 020 ----
mean loss: 181.74
 ---- batch: 030 ----
mean loss: 190.34
 ---- batch: 040 ----
mean loss: 186.59
 ---- batch: 050 ----
mean loss: 184.89
 ---- batch: 060 ----
mean loss: 186.75
 ---- batch: 070 ----
mean loss: 181.77
 ---- batch: 080 ----
mean loss: 189.81
 ---- batch: 090 ----
mean loss: 182.24
train mean loss: 185.57
epoch train time: 0:00:16.642998
elapsed time: 1:01:36.185433
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 03:07:52.181833
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.61
 ---- batch: 020 ----
mean loss: 184.56
 ---- batch: 030 ----
mean loss: 188.71
 ---- batch: 040 ----
mean loss: 185.46
 ---- batch: 050 ----
mean loss: 190.27
 ---- batch: 060 ----
mean loss: 182.10
 ---- batch: 070 ----
mean loss: 181.75
 ---- batch: 080 ----
mean loss: 189.87
 ---- batch: 090 ----
mean loss: 185.60
train mean loss: 185.74
epoch train time: 0:00:16.673793
elapsed time: 1:01:52.860429
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 03:08:08.856897
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.09
 ---- batch: 020 ----
mean loss: 186.92
 ---- batch: 030 ----
mean loss: 189.40
 ---- batch: 040 ----
mean loss: 186.28
 ---- batch: 050 ----
mean loss: 182.80
 ---- batch: 060 ----
mean loss: 187.51
 ---- batch: 070 ----
mean loss: 179.67
 ---- batch: 080 ----
mean loss: 189.27
 ---- batch: 090 ----
mean loss: 188.12
train mean loss: 185.43
epoch train time: 0:00:16.583173
elapsed time: 1:02:09.444932
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 03:08:25.441301
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.59
 ---- batch: 020 ----
mean loss: 184.87
 ---- batch: 030 ----
mean loss: 187.92
 ---- batch: 040 ----
mean loss: 186.53
 ---- batch: 050 ----
mean loss: 191.80
 ---- batch: 060 ----
mean loss: 180.13
 ---- batch: 070 ----
mean loss: 179.53
 ---- batch: 080 ----
mean loss: 190.35
 ---- batch: 090 ----
mean loss: 181.47
train mean loss: 185.11
epoch train time: 0:00:16.592184
elapsed time: 1:02:26.038299
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 03:08:42.034704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.06
 ---- batch: 020 ----
mean loss: 181.00
 ---- batch: 030 ----
mean loss: 185.25
 ---- batch: 040 ----
mean loss: 186.75
 ---- batch: 050 ----
mean loss: 190.27
 ---- batch: 060 ----
mean loss: 182.60
 ---- batch: 070 ----
mean loss: 185.10
 ---- batch: 080 ----
mean loss: 188.37
 ---- batch: 090 ----
mean loss: 185.29
train mean loss: 185.43
epoch train time: 0:00:16.617337
elapsed time: 1:02:42.656763
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 03:08:58.653105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.61
 ---- batch: 020 ----
mean loss: 184.18
 ---- batch: 030 ----
mean loss: 182.95
 ---- batch: 040 ----
mean loss: 185.84
 ---- batch: 050 ----
mean loss: 182.53
 ---- batch: 060 ----
mean loss: 192.54
 ---- batch: 070 ----
mean loss: 184.74
 ---- batch: 080 ----
mean loss: 190.39
 ---- batch: 090 ----
mean loss: 180.30
train mean loss: 185.24
epoch train time: 0:00:16.609119
elapsed time: 1:02:59.267011
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 03:09:15.263385
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.80
 ---- batch: 020 ----
mean loss: 191.95
 ---- batch: 030 ----
mean loss: 183.76
 ---- batch: 040 ----
mean loss: 181.82
 ---- batch: 050 ----
mean loss: 180.04
 ---- batch: 060 ----
mean loss: 186.30
 ---- batch: 070 ----
mean loss: 183.97
 ---- batch: 080 ----
mean loss: 190.86
 ---- batch: 090 ----
mean loss: 187.71
train mean loss: 185.37
epoch train time: 0:00:16.581000
elapsed time: 1:03:15.849216
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 03:09:31.845594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.89
 ---- batch: 020 ----
mean loss: 186.62
 ---- batch: 030 ----
mean loss: 183.79
 ---- batch: 040 ----
mean loss: 183.05
 ---- batch: 050 ----
mean loss: 187.96
 ---- batch: 060 ----
mean loss: 187.46
 ---- batch: 070 ----
mean loss: 185.98
 ---- batch: 080 ----
mean loss: 177.88
 ---- batch: 090 ----
mean loss: 191.41
train mean loss: 185.37
epoch train time: 0:00:16.585869
elapsed time: 1:03:32.436240
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 03:09:48.432616
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.70
 ---- batch: 020 ----
mean loss: 180.35
 ---- batch: 030 ----
mean loss: 183.46
 ---- batch: 040 ----
mean loss: 181.48
 ---- batch: 050 ----
mean loss: 183.47
 ---- batch: 060 ----
mean loss: 181.99
 ---- batch: 070 ----
mean loss: 186.63
 ---- batch: 080 ----
mean loss: 199.75
 ---- batch: 090 ----
mean loss: 190.74
train mean loss: 185.62
epoch train time: 0:00:16.651968
elapsed time: 1:03:49.089396
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 03:10:05.085827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.88
 ---- batch: 020 ----
mean loss: 188.37
 ---- batch: 030 ----
mean loss: 183.81
 ---- batch: 040 ----
mean loss: 188.32
 ---- batch: 050 ----
mean loss: 184.98
 ---- batch: 060 ----
mean loss: 183.85
 ---- batch: 070 ----
mean loss: 181.06
 ---- batch: 080 ----
mean loss: 181.61
 ---- batch: 090 ----
mean loss: 187.55
train mean loss: 185.45
epoch train time: 0:00:16.718598
elapsed time: 1:04:05.809193
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 03:10:21.805552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.50
 ---- batch: 020 ----
mean loss: 190.50
 ---- batch: 030 ----
mean loss: 185.96
 ---- batch: 040 ----
mean loss: 180.65
 ---- batch: 050 ----
mean loss: 181.26
 ---- batch: 060 ----
mean loss: 184.31
 ---- batch: 070 ----
mean loss: 179.38
 ---- batch: 080 ----
mean loss: 189.64
 ---- batch: 090 ----
mean loss: 193.46
train mean loss: 185.33
epoch train time: 0:00:16.615748
elapsed time: 1:04:22.426105
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 03:10:38.422329
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.63
 ---- batch: 020 ----
mean loss: 187.56
 ---- batch: 030 ----
mean loss: 183.46
 ---- batch: 040 ----
mean loss: 189.89
 ---- batch: 050 ----
mean loss: 193.28
 ---- batch: 060 ----
mean loss: 182.32
 ---- batch: 070 ----
mean loss: 185.08
 ---- batch: 080 ----
mean loss: 185.04
 ---- batch: 090 ----
mean loss: 182.34
train mean loss: 185.27
epoch train time: 0:00:16.639103
elapsed time: 1:04:39.066214
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 03:10:55.062563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.59
 ---- batch: 020 ----
mean loss: 179.99
 ---- batch: 030 ----
mean loss: 187.71
 ---- batch: 040 ----
mean loss: 187.18
 ---- batch: 050 ----
mean loss: 180.89
 ---- batch: 060 ----
mean loss: 183.91
 ---- batch: 070 ----
mean loss: 188.31
 ---- batch: 080 ----
mean loss: 180.56
 ---- batch: 090 ----
mean loss: 188.65
train mean loss: 185.07
epoch train time: 0:00:16.699830
elapsed time: 1:04:55.767156
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 03:11:11.763628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.03
 ---- batch: 020 ----
mean loss: 183.47
 ---- batch: 030 ----
mean loss: 184.38
 ---- batch: 040 ----
mean loss: 191.51
 ---- batch: 050 ----
mean loss: 190.76
 ---- batch: 060 ----
mean loss: 181.72
 ---- batch: 070 ----
mean loss: 186.60
 ---- batch: 080 ----
mean loss: 179.54
 ---- batch: 090 ----
mean loss: 188.10
train mean loss: 185.44
epoch train time: 0:00:16.683413
elapsed time: 1:05:12.451816
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 03:11:28.448204
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.82
 ---- batch: 020 ----
mean loss: 188.01
 ---- batch: 030 ----
mean loss: 188.01
 ---- batch: 040 ----
mean loss: 182.34
 ---- batch: 050 ----
mean loss: 182.94
 ---- batch: 060 ----
mean loss: 183.84
 ---- batch: 070 ----
mean loss: 186.86
 ---- batch: 080 ----
mean loss: 184.97
 ---- batch: 090 ----
mean loss: 182.72
train mean loss: 185.33
epoch train time: 0:00:16.618662
elapsed time: 1:05:29.071629
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 03:11:45.068060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.28
 ---- batch: 020 ----
mean loss: 186.06
 ---- batch: 030 ----
mean loss: 186.01
 ---- batch: 040 ----
mean loss: 186.36
 ---- batch: 050 ----
mean loss: 182.51
 ---- batch: 060 ----
mean loss: 176.64
 ---- batch: 070 ----
mean loss: 184.87
 ---- batch: 080 ----
mean loss: 188.60
 ---- batch: 090 ----
mean loss: 188.38
train mean loss: 185.35
epoch train time: 0:00:16.701311
elapsed time: 1:05:45.774212
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 03:12:01.770645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.60
 ---- batch: 020 ----
mean loss: 183.23
 ---- batch: 030 ----
mean loss: 186.76
 ---- batch: 040 ----
mean loss: 181.09
 ---- batch: 050 ----
mean loss: 180.79
 ---- batch: 060 ----
mean loss: 188.11
 ---- batch: 070 ----
mean loss: 186.25
 ---- batch: 080 ----
mean loss: 180.68
 ---- batch: 090 ----
mean loss: 187.97
train mean loss: 184.77
epoch train time: 0:00:16.651428
elapsed time: 1:06:02.426830
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 03:12:18.423426
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.29
 ---- batch: 020 ----
mean loss: 184.00
 ---- batch: 030 ----
mean loss: 182.70
 ---- batch: 040 ----
mean loss: 181.49
 ---- batch: 050 ----
mean loss: 187.00
 ---- batch: 060 ----
mean loss: 185.05
 ---- batch: 070 ----
mean loss: 179.01
 ---- batch: 080 ----
mean loss: 186.61
 ---- batch: 090 ----
mean loss: 185.43
train mean loss: 185.43
epoch train time: 0:00:16.672456
elapsed time: 1:06:19.101441
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 03:12:35.097317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.79
 ---- batch: 020 ----
mean loss: 188.20
 ---- batch: 030 ----
mean loss: 181.97
 ---- batch: 040 ----
mean loss: 189.99
 ---- batch: 050 ----
mean loss: 186.63
 ---- batch: 060 ----
mean loss: 184.75
 ---- batch: 070 ----
mean loss: 189.01
 ---- batch: 080 ----
mean loss: 183.51
 ---- batch: 090 ----
mean loss: 181.87
train mean loss: 184.93
epoch train time: 0:00:16.652203
elapsed time: 1:06:35.754292
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 03:12:51.750725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.27
 ---- batch: 020 ----
mean loss: 185.60
 ---- batch: 030 ----
mean loss: 181.66
 ---- batch: 040 ----
mean loss: 182.31
 ---- batch: 050 ----
mean loss: 187.56
 ---- batch: 060 ----
mean loss: 188.58
 ---- batch: 070 ----
mean loss: 184.39
 ---- batch: 080 ----
mean loss: 186.34
 ---- batch: 090 ----
mean loss: 184.17
train mean loss: 185.16
epoch train time: 0:00:16.624385
elapsed time: 1:06:52.379924
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 03:13:08.376261
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.23
 ---- batch: 020 ----
mean loss: 185.44
 ---- batch: 030 ----
mean loss: 185.59
 ---- batch: 040 ----
mean loss: 188.05
 ---- batch: 050 ----
mean loss: 179.94
 ---- batch: 060 ----
mean loss: 180.50
 ---- batch: 070 ----
mean loss: 188.79
 ---- batch: 080 ----
mean loss: 186.34
 ---- batch: 090 ----
mean loss: 187.88
train mean loss: 184.88
epoch train time: 0:00:16.666732
elapsed time: 1:07:09.047837
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 03:13:25.044431
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.96
 ---- batch: 020 ----
mean loss: 180.48
 ---- batch: 030 ----
mean loss: 193.81
 ---- batch: 040 ----
mean loss: 185.73
 ---- batch: 050 ----
mean loss: 184.10
 ---- batch: 060 ----
mean loss: 180.71
 ---- batch: 070 ----
mean loss: 185.82
 ---- batch: 080 ----
mean loss: 191.39
 ---- batch: 090 ----
mean loss: 184.73
train mean loss: 185.58
epoch train time: 0:00:16.591838
elapsed time: 1:07:25.641036
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 03:13:41.637457
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.65
 ---- batch: 020 ----
mean loss: 184.59
 ---- batch: 030 ----
mean loss: 186.08
 ---- batch: 040 ----
mean loss: 183.88
 ---- batch: 050 ----
mean loss: 187.04
 ---- batch: 060 ----
mean loss: 180.83
 ---- batch: 070 ----
mean loss: 184.49
 ---- batch: 080 ----
mean loss: 181.91
 ---- batch: 090 ----
mean loss: 185.79
train mean loss: 185.07
epoch train time: 0:00:16.614793
elapsed time: 1:07:42.257030
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 03:13:58.253424
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.03
 ---- batch: 020 ----
mean loss: 187.66
 ---- batch: 030 ----
mean loss: 182.44
 ---- batch: 040 ----
mean loss: 178.47
 ---- batch: 050 ----
mean loss: 187.26
 ---- batch: 060 ----
mean loss: 186.09
 ---- batch: 070 ----
mean loss: 181.44
 ---- batch: 080 ----
mean loss: 183.49
 ---- batch: 090 ----
mean loss: 187.79
train mean loss: 184.90
epoch train time: 0:00:16.603569
elapsed time: 1:07:58.861815
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 03:14:14.858172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.18
 ---- batch: 020 ----
mean loss: 187.99
 ---- batch: 030 ----
mean loss: 186.27
 ---- batch: 040 ----
mean loss: 177.98
 ---- batch: 050 ----
mean loss: 185.48
 ---- batch: 060 ----
mean loss: 184.47
 ---- batch: 070 ----
mean loss: 190.26
 ---- batch: 080 ----
mean loss: 183.82
 ---- batch: 090 ----
mean loss: 179.91
train mean loss: 185.18
epoch train time: 0:00:16.610521
elapsed time: 1:08:15.473443
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 03:14:31.469840
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.60
 ---- batch: 020 ----
mean loss: 181.68
 ---- batch: 030 ----
mean loss: 186.69
 ---- batch: 040 ----
mean loss: 181.06
 ---- batch: 050 ----
mean loss: 185.07
 ---- batch: 060 ----
mean loss: 177.72
 ---- batch: 070 ----
mean loss: 187.11
 ---- batch: 080 ----
mean loss: 181.75
 ---- batch: 090 ----
mean loss: 190.14
train mean loss: 185.11
epoch train time: 0:00:16.602359
elapsed time: 1:08:32.076995
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 03:14:48.073392
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.97
 ---- batch: 020 ----
mean loss: 181.81
 ---- batch: 030 ----
mean loss: 189.22
 ---- batch: 040 ----
mean loss: 186.82
 ---- batch: 050 ----
mean loss: 178.81
 ---- batch: 060 ----
mean loss: 183.58
 ---- batch: 070 ----
mean loss: 180.95
 ---- batch: 080 ----
mean loss: 183.20
 ---- batch: 090 ----
mean loss: 190.14
train mean loss: 184.87
epoch train time: 0:00:16.545891
elapsed time: 1:08:48.624137
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 03:15:04.620508
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.76
 ---- batch: 020 ----
mean loss: 185.69
 ---- batch: 030 ----
mean loss: 186.07
 ---- batch: 040 ----
mean loss: 181.58
 ---- batch: 050 ----
mean loss: 181.12
 ---- batch: 060 ----
mean loss: 186.53
 ---- batch: 070 ----
mean loss: 190.89
 ---- batch: 080 ----
mean loss: 179.02
 ---- batch: 090 ----
mean loss: 190.55
train mean loss: 185.10
epoch train time: 0:00:16.747232
elapsed time: 1:09:05.372549
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 03:15:21.369004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.51
 ---- batch: 020 ----
mean loss: 194.77
 ---- batch: 030 ----
mean loss: 184.77
 ---- batch: 040 ----
mean loss: 178.00
 ---- batch: 050 ----
mean loss: 185.46
 ---- batch: 060 ----
mean loss: 183.35
 ---- batch: 070 ----
mean loss: 186.33
 ---- batch: 080 ----
mean loss: 179.82
 ---- batch: 090 ----
mean loss: 186.00
train mean loss: 185.08
epoch train time: 0:00:16.538553
elapsed time: 1:09:21.912402
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 03:15:37.908779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.51
 ---- batch: 020 ----
mean loss: 186.90
 ---- batch: 030 ----
mean loss: 179.73
 ---- batch: 040 ----
mean loss: 183.44
 ---- batch: 050 ----
mean loss: 191.12
 ---- batch: 060 ----
mean loss: 187.69
 ---- batch: 070 ----
mean loss: 181.61
 ---- batch: 080 ----
mean loss: 185.94
 ---- batch: 090 ----
mean loss: 189.82
train mean loss: 184.47
epoch train time: 0:00:16.526183
elapsed time: 1:09:38.439791
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 03:15:54.436169
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.03
 ---- batch: 020 ----
mean loss: 184.76
 ---- batch: 030 ----
mean loss: 181.32
 ---- batch: 040 ----
mean loss: 189.66
 ---- batch: 050 ----
mean loss: 183.46
 ---- batch: 060 ----
mean loss: 179.26
 ---- batch: 070 ----
mean loss: 184.36
 ---- batch: 080 ----
mean loss: 180.32
 ---- batch: 090 ----
mean loss: 185.39
train mean loss: 184.79
epoch train time: 0:00:16.547813
elapsed time: 1:09:54.988888
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 03:16:10.985337
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.52
 ---- batch: 020 ----
mean loss: 183.82
 ---- batch: 030 ----
mean loss: 182.71
 ---- batch: 040 ----
mean loss: 189.97
 ---- batch: 050 ----
mean loss: 187.74
 ---- batch: 060 ----
mean loss: 189.04
 ---- batch: 070 ----
mean loss: 179.41
 ---- batch: 080 ----
mean loss: 176.68
 ---- batch: 090 ----
mean loss: 188.74
train mean loss: 184.47
epoch train time: 0:00:16.544514
elapsed time: 1:10:11.534652
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 03:16:27.531010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.50
 ---- batch: 020 ----
mean loss: 184.79
 ---- batch: 030 ----
mean loss: 181.28
 ---- batch: 040 ----
mean loss: 188.51
 ---- batch: 050 ----
mean loss: 180.11
 ---- batch: 060 ----
mean loss: 188.27
 ---- batch: 070 ----
mean loss: 197.74
 ---- batch: 080 ----
mean loss: 178.98
 ---- batch: 090 ----
mean loss: 183.00
train mean loss: 184.87
epoch train time: 0:00:16.572140
elapsed time: 1:10:28.107911
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 03:16:44.104216
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.18
 ---- batch: 020 ----
mean loss: 186.03
 ---- batch: 030 ----
mean loss: 183.04
 ---- batch: 040 ----
mean loss: 175.92
 ---- batch: 050 ----
mean loss: 191.82
 ---- batch: 060 ----
mean loss: 188.59
 ---- batch: 070 ----
mean loss: 182.42
 ---- batch: 080 ----
mean loss: 186.87
 ---- batch: 090 ----
mean loss: 186.97
train mean loss: 184.58
epoch train time: 0:00:16.606807
elapsed time: 1:10:44.725548
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_6/checkpoint.pth.tar
**** end time: 2019-09-25 03:17:00.721386 ****
