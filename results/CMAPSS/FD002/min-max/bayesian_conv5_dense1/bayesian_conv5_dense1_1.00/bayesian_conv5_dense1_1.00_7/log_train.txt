Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 7305
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 03:17:26.592193 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 03:17:26.609552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2629.73
 ---- batch: 020 ----
mean loss: 1904.15
 ---- batch: 030 ----
mean loss: 1588.72
 ---- batch: 040 ----
mean loss: 1408.07
 ---- batch: 050 ----
mean loss: 1300.23
 ---- batch: 060 ----
mean loss: 1232.44
 ---- batch: 070 ----
mean loss: 1218.34
 ---- batch: 080 ----
mean loss: 1203.63
 ---- batch: 090 ----
mean loss: 1170.15
train mean loss: 1493.44
epoch train time: 0:00:46.615485
elapsed time: 0:00:46.641994
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 03:18:13.234248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1153.53
 ---- batch: 020 ----
mean loss: 1132.90
 ---- batch: 030 ----
mean loss: 1126.79
 ---- batch: 040 ----
mean loss: 1114.38
 ---- batch: 050 ----
mean loss: 1102.89
 ---- batch: 060 ----
mean loss: 1097.47
 ---- batch: 070 ----
mean loss: 1118.85
 ---- batch: 080 ----
mean loss: 1094.78
 ---- batch: 090 ----
mean loss: 1125.53
train mean loss: 1115.19
epoch train time: 0:00:16.610762
elapsed time: 0:01:03.253528
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 03:18:29.846285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1083.29
 ---- batch: 020 ----
mean loss: 1072.48
 ---- batch: 030 ----
mean loss: 1088.31
 ---- batch: 040 ----
mean loss: 1108.81
 ---- batch: 050 ----
mean loss: 1110.02
 ---- batch: 060 ----
mean loss: 1068.71
 ---- batch: 070 ----
mean loss: 1082.02
 ---- batch: 080 ----
mean loss: 1051.25
 ---- batch: 090 ----
mean loss: 1061.32
train mean loss: 1080.33
epoch train time: 0:00:16.634573
elapsed time: 0:01:19.889470
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 03:18:46.482109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1092.22
 ---- batch: 020 ----
mean loss: 1062.69
 ---- batch: 030 ----
mean loss: 1058.88
 ---- batch: 040 ----
mean loss: 1070.69
 ---- batch: 050 ----
mean loss: 1063.02
 ---- batch: 060 ----
mean loss: 1034.62
 ---- batch: 070 ----
mean loss: 1075.08
 ---- batch: 080 ----
mean loss: 1071.36
 ---- batch: 090 ----
mean loss: 1045.40
train mean loss: 1064.09
epoch train time: 0:00:16.625470
elapsed time: 0:01:36.516054
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 03:19:03.108841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1068.18
 ---- batch: 020 ----
mean loss: 1025.31
 ---- batch: 030 ----
mean loss: 1042.85
 ---- batch: 040 ----
mean loss: 1030.87
 ---- batch: 050 ----
mean loss: 1041.36
 ---- batch: 060 ----
mean loss: 1031.30
 ---- batch: 070 ----
mean loss: 1053.88
 ---- batch: 080 ----
mean loss: 1030.40
 ---- batch: 090 ----
mean loss: 1035.63
train mean loss: 1038.90
epoch train time: 0:00:16.597775
elapsed time: 0:01:53.115018
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 03:19:19.707780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1043.40
 ---- batch: 020 ----
mean loss: 1036.23
 ---- batch: 030 ----
mean loss: 1047.39
 ---- batch: 040 ----
mean loss: 1048.11
 ---- batch: 050 ----
mean loss: 1051.57
 ---- batch: 060 ----
mean loss: 1000.52
 ---- batch: 070 ----
mean loss: 1020.32
 ---- batch: 080 ----
mean loss: 1025.59
 ---- batch: 090 ----
mean loss: 1026.53
train mean loss: 1031.43
epoch train time: 0:00:16.589450
elapsed time: 0:02:09.705723
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 03:19:36.298482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1016.54
 ---- batch: 020 ----
mean loss: 1008.72
 ---- batch: 030 ----
mean loss: 1033.33
 ---- batch: 040 ----
mean loss: 1040.03
 ---- batch: 050 ----
mean loss: 1031.99
 ---- batch: 060 ----
mean loss: 1022.35
 ---- batch: 070 ----
mean loss: 993.33
 ---- batch: 080 ----
mean loss: 1006.57
 ---- batch: 090 ----
mean loss: 1025.09
train mean loss: 1019.11
epoch train time: 0:00:16.624465
elapsed time: 0:02:26.331450
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 03:19:52.924207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 992.31
 ---- batch: 020 ----
mean loss: 1012.85
 ---- batch: 030 ----
mean loss: 1053.83
 ---- batch: 040 ----
mean loss: 995.87
 ---- batch: 050 ----
mean loss: 1013.35
 ---- batch: 060 ----
mean loss: 1009.04
 ---- batch: 070 ----
mean loss: 1015.25
 ---- batch: 080 ----
mean loss: 1009.50
 ---- batch: 090 ----
mean loss: 980.46
train mean loss: 1006.93
epoch train time: 0:00:16.632715
elapsed time: 0:02:42.965381
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 03:20:09.558152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 993.77
 ---- batch: 020 ----
mean loss: 995.61
 ---- batch: 030 ----
mean loss: 1003.28
 ---- batch: 040 ----
mean loss: 1019.24
 ---- batch: 050 ----
mean loss: 984.73
 ---- batch: 060 ----
mean loss: 989.64
 ---- batch: 070 ----
mean loss: 1005.96
 ---- batch: 080 ----
mean loss: 974.08
 ---- batch: 090 ----
mean loss: 1000.01
train mean loss: 995.80
epoch train time: 0:00:16.657840
elapsed time: 0:02:59.624468
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 03:20:26.217200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 985.28
 ---- batch: 020 ----
mean loss: 996.26
 ---- batch: 030 ----
mean loss: 963.42
 ---- batch: 040 ----
mean loss: 976.34
 ---- batch: 050 ----
mean loss: 997.79
 ---- batch: 060 ----
mean loss: 1006.72
 ---- batch: 070 ----
mean loss: 1005.40
 ---- batch: 080 ----
mean loss: 973.01
 ---- batch: 090 ----
mean loss: 978.68
train mean loss: 985.59
epoch train time: 0:00:16.617555
elapsed time: 0:03:16.243196
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 03:20:42.835928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 992.79
 ---- batch: 020 ----
mean loss: 995.99
 ---- batch: 030 ----
mean loss: 971.96
 ---- batch: 040 ----
mean loss: 984.64
 ---- batch: 050 ----
mean loss: 978.84
 ---- batch: 060 ----
mean loss: 985.99
 ---- batch: 070 ----
mean loss: 986.83
 ---- batch: 080 ----
mean loss: 951.69
 ---- batch: 090 ----
mean loss: 994.46
train mean loss: 979.50
epoch train time: 0:00:16.562573
elapsed time: 0:03:32.806935
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 03:20:59.399720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.61
 ---- batch: 020 ----
mean loss: 967.54
 ---- batch: 030 ----
mean loss: 994.35
 ---- batch: 040 ----
mean loss: 996.46
 ---- batch: 050 ----
mean loss: 981.31
 ---- batch: 060 ----
mean loss: 980.38
 ---- batch: 070 ----
mean loss: 998.06
 ---- batch: 080 ----
mean loss: 979.92
 ---- batch: 090 ----
mean loss: 970.23
train mean loss: 977.78
epoch train time: 0:00:16.539048
elapsed time: 0:03:49.347236
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 03:21:15.939974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.14
 ---- batch: 020 ----
mean loss: 979.36
 ---- batch: 030 ----
mean loss: 962.20
 ---- batch: 040 ----
mean loss: 951.52
 ---- batch: 050 ----
mean loss: 977.44
 ---- batch: 060 ----
mean loss: 999.73
 ---- batch: 070 ----
mean loss: 953.91
 ---- batch: 080 ----
mean loss: 959.41
 ---- batch: 090 ----
mean loss: 962.16
train mean loss: 969.30
epoch train time: 0:00:16.603576
elapsed time: 0:04:05.951930
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 03:21:32.544659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 971.99
 ---- batch: 020 ----
mean loss: 977.95
 ---- batch: 030 ----
mean loss: 967.83
 ---- batch: 040 ----
mean loss: 947.01
 ---- batch: 050 ----
mean loss: 967.97
 ---- batch: 060 ----
mean loss: 959.00
 ---- batch: 070 ----
mean loss: 979.38
 ---- batch: 080 ----
mean loss: 961.92
 ---- batch: 090 ----
mean loss: 966.68
train mean loss: 965.03
epoch train time: 0:00:16.565883
elapsed time: 0:04:22.518981
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 03:21:49.111724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.84
 ---- batch: 020 ----
mean loss: 958.52
 ---- batch: 030 ----
mean loss: 957.29
 ---- batch: 040 ----
mean loss: 934.51
 ---- batch: 050 ----
mean loss: 959.48
 ---- batch: 060 ----
mean loss: 935.75
 ---- batch: 070 ----
mean loss: 943.30
 ---- batch: 080 ----
mean loss: 970.60
 ---- batch: 090 ----
mean loss: 952.12
train mean loss: 955.33
epoch train time: 0:00:16.542312
elapsed time: 0:04:39.062428
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 03:22:05.655208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 945.72
 ---- batch: 020 ----
mean loss: 968.97
 ---- batch: 030 ----
mean loss: 956.32
 ---- batch: 040 ----
mean loss: 952.65
 ---- batch: 050 ----
mean loss: 971.62
 ---- batch: 060 ----
mean loss: 960.98
 ---- batch: 070 ----
mean loss: 935.22
 ---- batch: 080 ----
mean loss: 948.86
 ---- batch: 090 ----
mean loss: 953.76
train mean loss: 955.59
epoch train time: 0:00:16.610188
elapsed time: 0:04:55.673807
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 03:22:22.266550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 961.09
 ---- batch: 020 ----
mean loss: 943.94
 ---- batch: 030 ----
mean loss: 946.15
 ---- batch: 040 ----
mean loss: 947.40
 ---- batch: 050 ----
mean loss: 937.42
 ---- batch: 060 ----
mean loss: 952.56
 ---- batch: 070 ----
mean loss: 975.31
 ---- batch: 080 ----
mean loss: 990.75
 ---- batch: 090 ----
mean loss: 935.78
train mean loss: 953.98
epoch train time: 0:00:16.574568
elapsed time: 0:05:12.249588
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 03:22:38.842334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.19
 ---- batch: 020 ----
mean loss: 943.42
 ---- batch: 030 ----
mean loss: 962.23
 ---- batch: 040 ----
mean loss: 959.70
 ---- batch: 050 ----
mean loss: 929.41
 ---- batch: 060 ----
mean loss: 930.52
 ---- batch: 070 ----
mean loss: 943.24
 ---- batch: 080 ----
mean loss: 944.54
 ---- batch: 090 ----
mean loss: 927.86
train mean loss: 945.51
epoch train time: 0:00:16.549721
elapsed time: 0:05:28.800492
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 03:22:55.393255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.59
 ---- batch: 020 ----
mean loss: 951.56
 ---- batch: 030 ----
mean loss: 922.73
 ---- batch: 040 ----
mean loss: 949.88
 ---- batch: 050 ----
mean loss: 944.07
 ---- batch: 060 ----
mean loss: 936.91
 ---- batch: 070 ----
mean loss: 925.60
 ---- batch: 080 ----
mean loss: 947.17
 ---- batch: 090 ----
mean loss: 950.29
train mean loss: 940.67
epoch train time: 0:00:16.588519
elapsed time: 0:05:45.390201
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 03:23:11.983091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.95
 ---- batch: 020 ----
mean loss: 945.49
 ---- batch: 030 ----
mean loss: 939.87
 ---- batch: 040 ----
mean loss: 941.20
 ---- batch: 050 ----
mean loss: 933.16
 ---- batch: 060 ----
mean loss: 947.35
 ---- batch: 070 ----
mean loss: 942.46
 ---- batch: 080 ----
mean loss: 926.31
 ---- batch: 090 ----
mean loss: 932.90
train mean loss: 941.28
epoch train time: 0:00:16.651171
elapsed time: 0:06:02.042686
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 03:23:28.635451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 936.08
 ---- batch: 020 ----
mean loss: 917.03
 ---- batch: 030 ----
mean loss: 932.81
 ---- batch: 040 ----
mean loss: 930.30
 ---- batch: 050 ----
mean loss: 947.74
 ---- batch: 060 ----
mean loss: 939.11
 ---- batch: 070 ----
mean loss: 912.33
 ---- batch: 080 ----
mean loss: 951.23
 ---- batch: 090 ----
mean loss: 934.89
train mean loss: 932.68
epoch train time: 0:00:16.640597
elapsed time: 0:06:18.684474
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 03:23:45.277255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.82
 ---- batch: 020 ----
mean loss: 934.18
 ---- batch: 030 ----
mean loss: 919.60
 ---- batch: 040 ----
mean loss: 912.74
 ---- batch: 050 ----
mean loss: 927.55
 ---- batch: 060 ----
mean loss: 947.22
 ---- batch: 070 ----
mean loss: 942.99
 ---- batch: 080 ----
mean loss: 925.36
 ---- batch: 090 ----
mean loss: 932.97
train mean loss: 929.82
epoch train time: 0:00:16.610799
elapsed time: 0:06:35.296467
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 03:24:01.889113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.95
 ---- batch: 020 ----
mean loss: 914.58
 ---- batch: 030 ----
mean loss: 916.16
 ---- batch: 040 ----
mean loss: 926.86
 ---- batch: 050 ----
mean loss: 922.90
 ---- batch: 060 ----
mean loss: 922.87
 ---- batch: 070 ----
mean loss: 922.58
 ---- batch: 080 ----
mean loss: 935.76
 ---- batch: 090 ----
mean loss: 938.51
train mean loss: 926.34
epoch train time: 0:00:16.612372
elapsed time: 0:06:51.909953
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 03:24:18.502728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 938.90
 ---- batch: 020 ----
mean loss: 909.18
 ---- batch: 030 ----
mean loss: 919.37
 ---- batch: 040 ----
mean loss: 906.37
 ---- batch: 050 ----
mean loss: 918.42
 ---- batch: 060 ----
mean loss: 915.29
 ---- batch: 070 ----
mean loss: 922.02
 ---- batch: 080 ----
mean loss: 923.36
 ---- batch: 090 ----
mean loss: 913.90
train mean loss: 919.55
epoch train time: 0:00:16.567704
elapsed time: 0:07:08.478893
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 03:24:35.071639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.80
 ---- batch: 020 ----
mean loss: 932.51
 ---- batch: 030 ----
mean loss: 923.59
 ---- batch: 040 ----
mean loss: 912.36
 ---- batch: 050 ----
mean loss: 916.37
 ---- batch: 060 ----
mean loss: 904.57
 ---- batch: 070 ----
mean loss: 896.01
 ---- batch: 080 ----
mean loss: 895.14
 ---- batch: 090 ----
mean loss: 916.20
train mean loss: 908.48
epoch train time: 0:00:16.612869
elapsed time: 0:07:25.092934
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 03:24:51.685726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.25
 ---- batch: 020 ----
mean loss: 879.35
 ---- batch: 030 ----
mean loss: 893.30
 ---- batch: 040 ----
mean loss: 876.02
 ---- batch: 050 ----
mean loss: 875.90
 ---- batch: 060 ----
mean loss: 903.59
 ---- batch: 070 ----
mean loss: 879.79
 ---- batch: 080 ----
mean loss: 858.16
 ---- batch: 090 ----
mean loss: 857.76
train mean loss: 879.28
epoch train time: 0:00:16.596546
elapsed time: 0:07:41.690681
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 03:25:08.283498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.32
 ---- batch: 020 ----
mean loss: 839.73
 ---- batch: 030 ----
mean loss: 831.27
 ---- batch: 040 ----
mean loss: 822.84
 ---- batch: 050 ----
mean loss: 790.61
 ---- batch: 060 ----
mean loss: 773.28
 ---- batch: 070 ----
mean loss: 777.07
 ---- batch: 080 ----
mean loss: 768.71
 ---- batch: 090 ----
mean loss: 759.04
train mean loss: 796.59
epoch train time: 0:00:16.636942
elapsed time: 0:07:58.328895
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 03:25:24.921661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.34
 ---- batch: 020 ----
mean loss: 719.47
 ---- batch: 030 ----
mean loss: 721.82
 ---- batch: 040 ----
mean loss: 721.82
 ---- batch: 050 ----
mean loss: 708.00
 ---- batch: 060 ----
mean loss: 694.94
 ---- batch: 070 ----
mean loss: 683.62
 ---- batch: 080 ----
mean loss: 684.26
 ---- batch: 090 ----
mean loss: 666.89
train mean loss: 700.92
epoch train time: 0:00:16.662171
elapsed time: 0:08:14.992285
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 03:25:41.585031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 674.87
 ---- batch: 020 ----
mean loss: 669.47
 ---- batch: 030 ----
mean loss: 634.29
 ---- batch: 040 ----
mean loss: 639.99
 ---- batch: 050 ----
mean loss: 627.42
 ---- batch: 060 ----
mean loss: 625.33
 ---- batch: 070 ----
mean loss: 622.08
 ---- batch: 080 ----
mean loss: 598.20
 ---- batch: 090 ----
mean loss: 600.07
train mean loss: 629.84
epoch train time: 0:00:16.593127
elapsed time: 0:08:31.586589
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 03:25:58.179368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 604.31
 ---- batch: 020 ----
mean loss: 584.44
 ---- batch: 030 ----
mean loss: 567.69
 ---- batch: 040 ----
mean loss: 572.59
 ---- batch: 050 ----
mean loss: 596.06
 ---- batch: 060 ----
mean loss: 577.66
 ---- batch: 070 ----
mean loss: 567.88
 ---- batch: 080 ----
mean loss: 572.02
 ---- batch: 090 ----
mean loss: 537.76
train mean loss: 572.43
epoch train time: 0:00:16.591870
elapsed time: 0:08:48.179667
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 03:26:14.772474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 536.54
 ---- batch: 020 ----
mean loss: 529.76
 ---- batch: 030 ----
mean loss: 520.75
 ---- batch: 040 ----
mean loss: 527.99
 ---- batch: 050 ----
mean loss: 545.20
 ---- batch: 060 ----
mean loss: 516.05
 ---- batch: 070 ----
mean loss: 516.89
 ---- batch: 080 ----
mean loss: 515.79
 ---- batch: 090 ----
mean loss: 507.02
train mean loss: 523.33
epoch train time: 0:00:16.582713
elapsed time: 0:09:04.763615
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 03:26:31.356356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.30
 ---- batch: 020 ----
mean loss: 492.80
 ---- batch: 030 ----
mean loss: 505.09
 ---- batch: 040 ----
mean loss: 487.81
 ---- batch: 050 ----
mean loss: 475.16
 ---- batch: 060 ----
mean loss: 490.61
 ---- batch: 070 ----
mean loss: 484.12
 ---- batch: 080 ----
mean loss: 489.14
 ---- batch: 090 ----
mean loss: 486.03
train mean loss: 490.33
epoch train time: 0:00:16.574860
elapsed time: 0:09:21.339616
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 03:26:47.932377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.11
 ---- batch: 020 ----
mean loss: 481.07
 ---- batch: 030 ----
mean loss: 463.88
 ---- batch: 040 ----
mean loss: 467.10
 ---- batch: 050 ----
mean loss: 476.40
 ---- batch: 060 ----
mean loss: 461.14
 ---- batch: 070 ----
mean loss: 455.95
 ---- batch: 080 ----
mean loss: 461.15
 ---- batch: 090 ----
mean loss: 454.06
train mean loss: 466.71
epoch train time: 0:00:16.588550
elapsed time: 0:09:37.929430
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 03:27:04.522217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.95
 ---- batch: 020 ----
mean loss: 458.21
 ---- batch: 030 ----
mean loss: 447.48
 ---- batch: 040 ----
mean loss: 443.16
 ---- batch: 050 ----
mean loss: 433.26
 ---- batch: 060 ----
mean loss: 443.15
 ---- batch: 070 ----
mean loss: 442.06
 ---- batch: 080 ----
mean loss: 443.38
 ---- batch: 090 ----
mean loss: 425.66
train mean loss: 442.44
epoch train time: 0:00:16.587620
elapsed time: 0:09:54.518316
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 03:27:21.111078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.67
 ---- batch: 020 ----
mean loss: 440.59
 ---- batch: 030 ----
mean loss: 433.57
 ---- batch: 040 ----
mean loss: 421.83
 ---- batch: 050 ----
mean loss: 430.31
 ---- batch: 060 ----
mean loss: 419.50
 ---- batch: 070 ----
mean loss: 422.17
 ---- batch: 080 ----
mean loss: 412.32
 ---- batch: 090 ----
mean loss: 417.92
train mean loss: 425.34
epoch train time: 0:00:16.561800
elapsed time: 0:10:11.081322
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 03:27:37.674235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.25
 ---- batch: 020 ----
mean loss: 425.54
 ---- batch: 030 ----
mean loss: 422.14
 ---- batch: 040 ----
mean loss: 416.53
 ---- batch: 050 ----
mean loss: 408.55
 ---- batch: 060 ----
mean loss: 397.91
 ---- batch: 070 ----
mean loss: 408.46
 ---- batch: 080 ----
mean loss: 412.03
 ---- batch: 090 ----
mean loss: 406.76
train mean loss: 414.08
epoch train time: 0:00:16.558012
elapsed time: 0:10:27.640736
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 03:27:54.233502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.43
 ---- batch: 020 ----
mean loss: 407.36
 ---- batch: 030 ----
mean loss: 399.75
 ---- batch: 040 ----
mean loss: 391.75
 ---- batch: 050 ----
mean loss: 401.15
 ---- batch: 060 ----
mean loss: 397.92
 ---- batch: 070 ----
mean loss: 392.08
 ---- batch: 080 ----
mean loss: 396.08
 ---- batch: 090 ----
mean loss: 395.72
train mean loss: 399.23
epoch train time: 0:00:16.605077
elapsed time: 0:10:44.246979
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 03:28:10.839780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.39
 ---- batch: 020 ----
mean loss: 387.64
 ---- batch: 030 ----
mean loss: 378.27
 ---- batch: 040 ----
mean loss: 402.78
 ---- batch: 050 ----
mean loss: 382.43
 ---- batch: 060 ----
mean loss: 387.20
 ---- batch: 070 ----
mean loss: 382.28
 ---- batch: 080 ----
mean loss: 386.41
 ---- batch: 090 ----
mean loss: 375.22
train mean loss: 385.53
epoch train time: 0:00:16.558139
elapsed time: 0:11:00.806324
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 03:28:27.399076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.36
 ---- batch: 020 ----
mean loss: 375.77
 ---- batch: 030 ----
mean loss: 376.18
 ---- batch: 040 ----
mean loss: 383.75
 ---- batch: 050 ----
mean loss: 384.15
 ---- batch: 060 ----
mean loss: 378.14
 ---- batch: 070 ----
mean loss: 371.68
 ---- batch: 080 ----
mean loss: 365.46
 ---- batch: 090 ----
mean loss: 372.80
train mean loss: 375.00
epoch train time: 0:00:16.600315
elapsed time: 0:11:17.407878
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 03:28:44.000611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.01
 ---- batch: 020 ----
mean loss: 368.77
 ---- batch: 030 ----
mean loss: 360.64
 ---- batch: 040 ----
mean loss: 369.00
 ---- batch: 050 ----
mean loss: 373.53
 ---- batch: 060 ----
mean loss: 366.27
 ---- batch: 070 ----
mean loss: 367.88
 ---- batch: 080 ----
mean loss: 366.68
 ---- batch: 090 ----
mean loss: 359.77
train mean loss: 365.94
epoch train time: 0:00:16.558433
elapsed time: 0:11:33.967493
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 03:29:00.560241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.58
 ---- batch: 020 ----
mean loss: 362.83
 ---- batch: 030 ----
mean loss: 358.65
 ---- batch: 040 ----
mean loss: 352.43
 ---- batch: 050 ----
mean loss: 354.66
 ---- batch: 060 ----
mean loss: 358.55
 ---- batch: 070 ----
mean loss: 356.02
 ---- batch: 080 ----
mean loss: 359.43
 ---- batch: 090 ----
mean loss: 344.48
train mean loss: 356.15
epoch train time: 0:00:16.597301
elapsed time: 0:11:50.565950
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 03:29:17.158666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.26
 ---- batch: 020 ----
mean loss: 353.23
 ---- batch: 030 ----
mean loss: 355.82
 ---- batch: 040 ----
mean loss: 349.99
 ---- batch: 050 ----
mean loss: 351.91
 ---- batch: 060 ----
mean loss: 347.44
 ---- batch: 070 ----
mean loss: 348.05
 ---- batch: 080 ----
mean loss: 349.32
 ---- batch: 090 ----
mean loss: 348.28
train mean loss: 349.32
epoch train time: 0:00:16.551405
elapsed time: 0:12:07.118468
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 03:29:33.711209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.35
 ---- batch: 020 ----
mean loss: 336.05
 ---- batch: 030 ----
mean loss: 337.56
 ---- batch: 040 ----
mean loss: 334.64
 ---- batch: 050 ----
mean loss: 348.38
 ---- batch: 060 ----
mean loss: 353.84
 ---- batch: 070 ----
mean loss: 334.73
 ---- batch: 080 ----
mean loss: 348.48
 ---- batch: 090 ----
mean loss: 336.10
train mean loss: 341.36
epoch train time: 0:00:16.568360
elapsed time: 0:12:23.688045
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 03:29:50.280808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.71
 ---- batch: 020 ----
mean loss: 344.21
 ---- batch: 030 ----
mean loss: 328.78
 ---- batch: 040 ----
mean loss: 334.32
 ---- batch: 050 ----
mean loss: 328.66
 ---- batch: 060 ----
mean loss: 325.95
 ---- batch: 070 ----
mean loss: 329.60
 ---- batch: 080 ----
mean loss: 327.93
 ---- batch: 090 ----
mean loss: 331.21
train mean loss: 330.15
epoch train time: 0:00:16.597348
elapsed time: 0:12:40.286548
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 03:30:06.879152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.25
 ---- batch: 020 ----
mean loss: 307.99
 ---- batch: 030 ----
mean loss: 343.26
 ---- batch: 040 ----
mean loss: 317.27
 ---- batch: 050 ----
mean loss: 323.70
 ---- batch: 060 ----
mean loss: 332.34
 ---- batch: 070 ----
mean loss: 331.36
 ---- batch: 080 ----
mean loss: 324.78
 ---- batch: 090 ----
mean loss: 324.41
train mean loss: 326.72
epoch train time: 0:00:16.561595
elapsed time: 0:12:56.849163
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 03:30:23.441930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.88
 ---- batch: 020 ----
mean loss: 332.91
 ---- batch: 030 ----
mean loss: 316.36
 ---- batch: 040 ----
mean loss: 323.55
 ---- batch: 050 ----
mean loss: 318.07
 ---- batch: 060 ----
mean loss: 309.51
 ---- batch: 070 ----
mean loss: 323.87
 ---- batch: 080 ----
mean loss: 310.68
 ---- batch: 090 ----
mean loss: 325.13
train mean loss: 320.46
epoch train time: 0:00:16.630998
elapsed time: 0:13:13.481458
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 03:30:40.074241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.63
 ---- batch: 020 ----
mean loss: 327.53
 ---- batch: 030 ----
mean loss: 328.23
 ---- batch: 040 ----
mean loss: 309.50
 ---- batch: 050 ----
mean loss: 310.64
 ---- batch: 060 ----
mean loss: 323.18
 ---- batch: 070 ----
mean loss: 320.22
 ---- batch: 080 ----
mean loss: 307.82
 ---- batch: 090 ----
mean loss: 308.26
train mean loss: 318.46
epoch train time: 0:00:16.695621
elapsed time: 0:13:30.178247
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 03:30:56.770947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.35
 ---- batch: 020 ----
mean loss: 314.98
 ---- batch: 030 ----
mean loss: 313.45
 ---- batch: 040 ----
mean loss: 308.81
 ---- batch: 050 ----
mean loss: 309.66
 ---- batch: 060 ----
mean loss: 308.65
 ---- batch: 070 ----
mean loss: 309.35
 ---- batch: 080 ----
mean loss: 305.97
 ---- batch: 090 ----
mean loss: 304.68
train mean loss: 309.82
epoch train time: 0:00:16.567085
elapsed time: 0:13:46.746412
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 03:31:13.339126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.86
 ---- batch: 020 ----
mean loss: 301.23
 ---- batch: 030 ----
mean loss: 314.51
 ---- batch: 040 ----
mean loss: 305.80
 ---- batch: 050 ----
mean loss: 300.91
 ---- batch: 060 ----
mean loss: 309.72
 ---- batch: 070 ----
mean loss: 302.13
 ---- batch: 080 ----
mean loss: 313.65
 ---- batch: 090 ----
mean loss: 303.53
train mean loss: 305.84
epoch train time: 0:00:16.586912
elapsed time: 0:14:03.334619
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 03:31:29.927484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.00
 ---- batch: 020 ----
mean loss: 294.09
 ---- batch: 030 ----
mean loss: 311.71
 ---- batch: 040 ----
mean loss: 294.45
 ---- batch: 050 ----
mean loss: 302.91
 ---- batch: 060 ----
mean loss: 311.86
 ---- batch: 070 ----
mean loss: 299.77
 ---- batch: 080 ----
mean loss: 307.09
 ---- batch: 090 ----
mean loss: 297.99
train mean loss: 302.66
epoch train time: 0:00:16.594000
elapsed time: 0:14:19.929919
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 03:31:46.522623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.33
 ---- batch: 020 ----
mean loss: 299.65
 ---- batch: 030 ----
mean loss: 298.67
 ---- batch: 040 ----
mean loss: 301.51
 ---- batch: 050 ----
mean loss: 302.35
 ---- batch: 060 ----
mean loss: 302.31
 ---- batch: 070 ----
mean loss: 298.00
 ---- batch: 080 ----
mean loss: 291.79
 ---- batch: 090 ----
mean loss: 295.49
train mean loss: 298.76
epoch train time: 0:00:16.671789
elapsed time: 0:14:36.602875
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 03:32:03.195683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.47
 ---- batch: 020 ----
mean loss: 294.07
 ---- batch: 030 ----
mean loss: 296.49
 ---- batch: 040 ----
mean loss: 299.74
 ---- batch: 050 ----
mean loss: 285.03
 ---- batch: 060 ----
mean loss: 288.89
 ---- batch: 070 ----
mean loss: 286.25
 ---- batch: 080 ----
mean loss: 296.06
 ---- batch: 090 ----
mean loss: 295.35
train mean loss: 293.89
epoch train time: 0:00:16.638192
elapsed time: 0:14:53.242304
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 03:32:19.835057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.95
 ---- batch: 020 ----
mean loss: 290.13
 ---- batch: 030 ----
mean loss: 288.51
 ---- batch: 040 ----
mean loss: 290.68
 ---- batch: 050 ----
mean loss: 294.68
 ---- batch: 060 ----
mean loss: 296.73
 ---- batch: 070 ----
mean loss: 276.59
 ---- batch: 080 ----
mean loss: 289.18
 ---- batch: 090 ----
mean loss: 300.05
train mean loss: 291.47
epoch train time: 0:00:16.638257
elapsed time: 0:15:09.881857
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 03:32:36.474664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.36
 ---- batch: 020 ----
mean loss: 292.09
 ---- batch: 030 ----
mean loss: 301.49
 ---- batch: 040 ----
mean loss: 288.39
 ---- batch: 050 ----
mean loss: 285.73
 ---- batch: 060 ----
mean loss: 286.56
 ---- batch: 070 ----
mean loss: 284.28
 ---- batch: 080 ----
mean loss: 295.74
 ---- batch: 090 ----
mean loss: 280.76
train mean loss: 290.13
epoch train time: 0:00:16.618901
elapsed time: 0:15:26.501967
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 03:32:53.094704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.56
 ---- batch: 020 ----
mean loss: 280.56
 ---- batch: 030 ----
mean loss: 287.37
 ---- batch: 040 ----
mean loss: 290.04
 ---- batch: 050 ----
mean loss: 287.19
 ---- batch: 060 ----
mean loss: 289.12
 ---- batch: 070 ----
mean loss: 291.90
 ---- batch: 080 ----
mean loss: 274.16
 ---- batch: 090 ----
mean loss: 290.33
train mean loss: 286.29
epoch train time: 0:00:16.658555
elapsed time: 0:15:43.161668
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 03:33:09.754403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.32
 ---- batch: 020 ----
mean loss: 277.23
 ---- batch: 030 ----
mean loss: 281.05
 ---- batch: 040 ----
mean loss: 281.64
 ---- batch: 050 ----
mean loss: 275.26
 ---- batch: 060 ----
mean loss: 284.46
 ---- batch: 070 ----
mean loss: 292.90
 ---- batch: 080 ----
mean loss: 272.41
 ---- batch: 090 ----
mean loss: 273.74
train mean loss: 281.58
epoch train time: 0:00:16.668810
elapsed time: 0:15:59.831630
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 03:33:26.424410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.02
 ---- batch: 020 ----
mean loss: 275.79
 ---- batch: 030 ----
mean loss: 290.15
 ---- batch: 040 ----
mean loss: 282.72
 ---- batch: 050 ----
mean loss: 276.56
 ---- batch: 060 ----
mean loss: 275.13
 ---- batch: 070 ----
mean loss: 284.22
 ---- batch: 080 ----
mean loss: 272.16
 ---- batch: 090 ----
mean loss: 281.32
train mean loss: 279.74
epoch train time: 0:00:16.646650
elapsed time: 0:16:16.479470
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 03:33:43.072195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.38
 ---- batch: 020 ----
mean loss: 279.87
 ---- batch: 030 ----
mean loss: 276.30
 ---- batch: 040 ----
mean loss: 275.46
 ---- batch: 050 ----
mean loss: 283.48
 ---- batch: 060 ----
mean loss: 282.97
 ---- batch: 070 ----
mean loss: 271.24
 ---- batch: 080 ----
mean loss: 274.53
 ---- batch: 090 ----
mean loss: 273.01
train mean loss: 276.78
epoch train time: 0:00:16.672576
elapsed time: 0:16:33.153352
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 03:33:59.746121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.22
 ---- batch: 020 ----
mean loss: 279.65
 ---- batch: 030 ----
mean loss: 274.41
 ---- batch: 040 ----
mean loss: 279.66
 ---- batch: 050 ----
mean loss: 274.98
 ---- batch: 060 ----
mean loss: 277.02
 ---- batch: 070 ----
mean loss: 272.05
 ---- batch: 080 ----
mean loss: 273.39
 ---- batch: 090 ----
mean loss: 276.74
train mean loss: 275.30
epoch train time: 0:00:16.589828
elapsed time: 0:16:49.744490
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 03:34:16.337243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.80
 ---- batch: 020 ----
mean loss: 276.60
 ---- batch: 030 ----
mean loss: 265.46
 ---- batch: 040 ----
mean loss: 278.41
 ---- batch: 050 ----
mean loss: 276.96
 ---- batch: 060 ----
mean loss: 278.83
 ---- batch: 070 ----
mean loss: 269.54
 ---- batch: 080 ----
mean loss: 267.24
 ---- batch: 090 ----
mean loss: 272.28
train mean loss: 272.34
epoch train time: 0:00:16.646051
elapsed time: 0:17:06.391752
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 03:34:32.984550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.66
 ---- batch: 020 ----
mean loss: 267.44
 ---- batch: 030 ----
mean loss: 264.12
 ---- batch: 040 ----
mean loss: 273.40
 ---- batch: 050 ----
mean loss: 266.80
 ---- batch: 060 ----
mean loss: 262.01
 ---- batch: 070 ----
mean loss: 273.22
 ---- batch: 080 ----
mean loss: 277.58
 ---- batch: 090 ----
mean loss: 274.95
train mean loss: 270.12
epoch train time: 0:00:16.582931
elapsed time: 0:17:22.975972
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 03:34:49.568707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.53
 ---- batch: 020 ----
mean loss: 264.16
 ---- batch: 030 ----
mean loss: 263.37
 ---- batch: 040 ----
mean loss: 253.82
 ---- batch: 050 ----
mean loss: 263.71
 ---- batch: 060 ----
mean loss: 280.39
 ---- batch: 070 ----
mean loss: 267.26
 ---- batch: 080 ----
mean loss: 265.14
 ---- batch: 090 ----
mean loss: 277.98
train mean loss: 267.14
epoch train time: 0:00:16.605506
elapsed time: 0:17:39.582628
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 03:35:06.175406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.86
 ---- batch: 020 ----
mean loss: 259.75
 ---- batch: 030 ----
mean loss: 268.55
 ---- batch: 040 ----
mean loss: 255.16
 ---- batch: 050 ----
mean loss: 270.59
 ---- batch: 060 ----
mean loss: 272.88
 ---- batch: 070 ----
mean loss: 268.67
 ---- batch: 080 ----
mean loss: 269.19
 ---- batch: 090 ----
mean loss: 263.45
train mean loss: 265.74
epoch train time: 0:00:16.628061
elapsed time: 0:17:56.211962
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 03:35:22.804727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.62
 ---- batch: 020 ----
mean loss: 267.00
 ---- batch: 030 ----
mean loss: 264.15
 ---- batch: 040 ----
mean loss: 260.43
 ---- batch: 050 ----
mean loss: 272.53
 ---- batch: 060 ----
mean loss: 255.69
 ---- batch: 070 ----
mean loss: 254.44
 ---- batch: 080 ----
mean loss: 257.37
 ---- batch: 090 ----
mean loss: 256.86
train mean loss: 261.44
epoch train time: 0:00:16.682461
elapsed time: 0:18:12.895582
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 03:35:39.488321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.41
 ---- batch: 020 ----
mean loss: 255.32
 ---- batch: 030 ----
mean loss: 263.93
 ---- batch: 040 ----
mean loss: 267.93
 ---- batch: 050 ----
mean loss: 267.82
 ---- batch: 060 ----
mean loss: 259.97
 ---- batch: 070 ----
mean loss: 251.95
 ---- batch: 080 ----
mean loss: 255.72
 ---- batch: 090 ----
mean loss: 258.31
train mean loss: 260.09
epoch train time: 0:00:16.579334
elapsed time: 0:18:29.476197
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 03:35:56.068944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.09
 ---- batch: 020 ----
mean loss: 260.13
 ---- batch: 030 ----
mean loss: 263.40
 ---- batch: 040 ----
mean loss: 254.80
 ---- batch: 050 ----
mean loss: 261.97
 ---- batch: 060 ----
mean loss: 259.84
 ---- batch: 070 ----
mean loss: 255.69
 ---- batch: 080 ----
mean loss: 262.74
 ---- batch: 090 ----
mean loss: 253.06
train mean loss: 258.04
epoch train time: 0:00:16.586854
elapsed time: 0:18:46.064221
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 03:36:12.657032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.82
 ---- batch: 020 ----
mean loss: 253.19
 ---- batch: 030 ----
mean loss: 259.22
 ---- batch: 040 ----
mean loss: 261.99
 ---- batch: 050 ----
mean loss: 261.47
 ---- batch: 060 ----
mean loss: 265.68
 ---- batch: 070 ----
mean loss: 252.23
 ---- batch: 080 ----
mean loss: 251.78
 ---- batch: 090 ----
mean loss: 248.01
train mean loss: 256.38
epoch train time: 0:00:16.469601
elapsed time: 0:19:02.535093
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 03:36:29.127929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.49
 ---- batch: 020 ----
mean loss: 257.57
 ---- batch: 030 ----
mean loss: 245.14
 ---- batch: 040 ----
mean loss: 246.45
 ---- batch: 050 ----
mean loss: 255.69
 ---- batch: 060 ----
mean loss: 254.92
 ---- batch: 070 ----
mean loss: 257.97
 ---- batch: 080 ----
mean loss: 259.83
 ---- batch: 090 ----
mean loss: 248.99
train mean loss: 253.40
epoch train time: 0:00:16.467864
elapsed time: 0:19:19.004217
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 03:36:45.597025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.32
 ---- batch: 020 ----
mean loss: 258.84
 ---- batch: 030 ----
mean loss: 243.93
 ---- batch: 040 ----
mean loss: 249.95
 ---- batch: 050 ----
mean loss: 251.34
 ---- batch: 060 ----
mean loss: 250.25
 ---- batch: 070 ----
mean loss: 259.59
 ---- batch: 080 ----
mean loss: 247.82
 ---- batch: 090 ----
mean loss: 255.75
train mean loss: 252.38
epoch train time: 0:00:16.531287
elapsed time: 0:19:35.536717
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 03:37:02.129414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.65
 ---- batch: 020 ----
mean loss: 244.70
 ---- batch: 030 ----
mean loss: 245.21
 ---- batch: 040 ----
mean loss: 249.45
 ---- batch: 050 ----
mean loss: 253.62
 ---- batch: 060 ----
mean loss: 241.40
 ---- batch: 070 ----
mean loss: 240.44
 ---- batch: 080 ----
mean loss: 262.27
 ---- batch: 090 ----
mean loss: 254.93
train mean loss: 251.17
epoch train time: 0:00:16.517894
elapsed time: 0:19:52.055742
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 03:37:18.648531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.71
 ---- batch: 020 ----
mean loss: 244.55
 ---- batch: 030 ----
mean loss: 248.45
 ---- batch: 040 ----
mean loss: 258.25
 ---- batch: 050 ----
mean loss: 254.37
 ---- batch: 060 ----
mean loss: 250.93
 ---- batch: 070 ----
mean loss: 251.47
 ---- batch: 080 ----
mean loss: 246.56
 ---- batch: 090 ----
mean loss: 248.18
train mean loss: 249.51
epoch train time: 0:00:16.512970
elapsed time: 0:20:08.569972
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 03:37:35.162719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.70
 ---- batch: 020 ----
mean loss: 244.35
 ---- batch: 030 ----
mean loss: 254.20
 ---- batch: 040 ----
mean loss: 246.75
 ---- batch: 050 ----
mean loss: 251.56
 ---- batch: 060 ----
mean loss: 260.88
 ---- batch: 070 ----
mean loss: 241.00
 ---- batch: 080 ----
mean loss: 249.54
 ---- batch: 090 ----
mean loss: 250.43
train mean loss: 249.33
epoch train time: 0:00:16.489471
elapsed time: 0:20:25.060810
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 03:37:51.653652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.01
 ---- batch: 020 ----
mean loss: 230.40
 ---- batch: 030 ----
mean loss: 245.25
 ---- batch: 040 ----
mean loss: 247.75
 ---- batch: 050 ----
mean loss: 253.22
 ---- batch: 060 ----
mean loss: 245.11
 ---- batch: 070 ----
mean loss: 248.02
 ---- batch: 080 ----
mean loss: 253.07
 ---- batch: 090 ----
mean loss: 245.43
train mean loss: 245.23
epoch train time: 0:00:16.537924
elapsed time: 0:20:41.599987
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 03:38:08.192748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.46
 ---- batch: 020 ----
mean loss: 245.10
 ---- batch: 030 ----
mean loss: 252.35
 ---- batch: 040 ----
mean loss: 240.81
 ---- batch: 050 ----
mean loss: 236.38
 ---- batch: 060 ----
mean loss: 242.80
 ---- batch: 070 ----
mean loss: 243.79
 ---- batch: 080 ----
mean loss: 240.46
 ---- batch: 090 ----
mean loss: 242.15
train mean loss: 243.10
epoch train time: 0:00:16.539823
elapsed time: 0:20:58.141041
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 03:38:24.733819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.49
 ---- batch: 020 ----
mean loss: 242.91
 ---- batch: 030 ----
mean loss: 247.06
 ---- batch: 040 ----
mean loss: 249.42
 ---- batch: 050 ----
mean loss: 245.03
 ---- batch: 060 ----
mean loss: 244.79
 ---- batch: 070 ----
mean loss: 240.24
 ---- batch: 080 ----
mean loss: 240.46
 ---- batch: 090 ----
mean loss: 241.51
train mean loss: 243.86
epoch train time: 0:00:16.508123
elapsed time: 0:21:14.650323
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 03:38:41.243163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.47
 ---- batch: 020 ----
mean loss: 240.71
 ---- batch: 030 ----
mean loss: 245.69
 ---- batch: 040 ----
mean loss: 246.65
 ---- batch: 050 ----
mean loss: 236.91
 ---- batch: 060 ----
mean loss: 241.47
 ---- batch: 070 ----
mean loss: 242.08
 ---- batch: 080 ----
mean loss: 243.85
 ---- batch: 090 ----
mean loss: 242.38
train mean loss: 241.54
epoch train time: 0:00:16.521059
elapsed time: 0:21:31.172706
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 03:38:57.765459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.68
 ---- batch: 020 ----
mean loss: 242.77
 ---- batch: 030 ----
mean loss: 234.31
 ---- batch: 040 ----
mean loss: 241.71
 ---- batch: 050 ----
mean loss: 247.09
 ---- batch: 060 ----
mean loss: 244.54
 ---- batch: 070 ----
mean loss: 243.04
 ---- batch: 080 ----
mean loss: 243.05
 ---- batch: 090 ----
mean loss: 241.09
train mean loss: 242.45
epoch train time: 0:00:16.530960
elapsed time: 0:21:47.704902
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 03:39:14.297693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.83
 ---- batch: 020 ----
mean loss: 234.96
 ---- batch: 030 ----
mean loss: 234.50
 ---- batch: 040 ----
mean loss: 251.01
 ---- batch: 050 ----
mean loss: 236.10
 ---- batch: 060 ----
mean loss: 234.44
 ---- batch: 070 ----
mean loss: 238.44
 ---- batch: 080 ----
mean loss: 241.40
 ---- batch: 090 ----
mean loss: 235.98
train mean loss: 238.06
epoch train time: 0:00:16.526056
elapsed time: 0:22:04.232297
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 03:39:30.825060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.39
 ---- batch: 020 ----
mean loss: 234.19
 ---- batch: 030 ----
mean loss: 234.46
 ---- batch: 040 ----
mean loss: 247.75
 ---- batch: 050 ----
mean loss: 233.39
 ---- batch: 060 ----
mean loss: 231.16
 ---- batch: 070 ----
mean loss: 246.89
 ---- batch: 080 ----
mean loss: 240.31
 ---- batch: 090 ----
mean loss: 238.74
train mean loss: 237.08
epoch train time: 0:00:16.518056
elapsed time: 0:22:20.751548
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 03:39:47.344143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.90
 ---- batch: 020 ----
mean loss: 233.07
 ---- batch: 030 ----
mean loss: 235.69
 ---- batch: 040 ----
mean loss: 243.47
 ---- batch: 050 ----
mean loss: 234.87
 ---- batch: 060 ----
mean loss: 239.89
 ---- batch: 070 ----
mean loss: 224.95
 ---- batch: 080 ----
mean loss: 235.59
 ---- batch: 090 ----
mean loss: 235.56
train mean loss: 234.74
epoch train time: 0:00:16.515711
elapsed time: 0:22:37.268294
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 03:40:03.861043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.05
 ---- batch: 020 ----
mean loss: 227.78
 ---- batch: 030 ----
mean loss: 234.19
 ---- batch: 040 ----
mean loss: 229.52
 ---- batch: 050 ----
mean loss: 231.69
 ---- batch: 060 ----
mean loss: 235.61
 ---- batch: 070 ----
mean loss: 234.62
 ---- batch: 080 ----
mean loss: 234.90
 ---- batch: 090 ----
mean loss: 241.30
train mean loss: 233.73
epoch train time: 0:00:16.476245
elapsed time: 0:22:53.745705
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 03:40:20.338521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.61
 ---- batch: 020 ----
mean loss: 238.19
 ---- batch: 030 ----
mean loss: 241.29
 ---- batch: 040 ----
mean loss: 231.16
 ---- batch: 050 ----
mean loss: 240.41
 ---- batch: 060 ----
mean loss: 232.13
 ---- batch: 070 ----
mean loss: 233.98
 ---- batch: 080 ----
mean loss: 227.62
 ---- batch: 090 ----
mean loss: 231.31
train mean loss: 234.34
epoch train time: 0:00:16.503854
elapsed time: 0:23:10.251089
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 03:40:36.844051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.23
 ---- batch: 020 ----
mean loss: 230.73
 ---- batch: 030 ----
mean loss: 230.03
 ---- batch: 040 ----
mean loss: 231.19
 ---- batch: 050 ----
mean loss: 236.58
 ---- batch: 060 ----
mean loss: 240.35
 ---- batch: 070 ----
mean loss: 225.94
 ---- batch: 080 ----
mean loss: 238.52
 ---- batch: 090 ----
mean loss: 229.37
train mean loss: 232.67
epoch train time: 0:00:16.523616
elapsed time: 0:23:26.776051
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 03:40:53.368827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.72
 ---- batch: 020 ----
mean loss: 226.14
 ---- batch: 030 ----
mean loss: 234.38
 ---- batch: 040 ----
mean loss: 230.29
 ---- batch: 050 ----
mean loss: 229.02
 ---- batch: 060 ----
mean loss: 228.14
 ---- batch: 070 ----
mean loss: 227.71
 ---- batch: 080 ----
mean loss: 228.49
 ---- batch: 090 ----
mean loss: 231.16
train mean loss: 230.37
epoch train time: 0:00:16.545194
elapsed time: 0:23:43.322535
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 03:41:09.915359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.49
 ---- batch: 020 ----
mean loss: 223.86
 ---- batch: 030 ----
mean loss: 227.18
 ---- batch: 040 ----
mean loss: 229.89
 ---- batch: 050 ----
mean loss: 231.95
 ---- batch: 060 ----
mean loss: 228.98
 ---- batch: 070 ----
mean loss: 222.37
 ---- batch: 080 ----
mean loss: 225.53
 ---- batch: 090 ----
mean loss: 234.72
train mean loss: 228.84
epoch train time: 0:00:16.525157
elapsed time: 0:23:59.848907
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 03:41:26.441793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.42
 ---- batch: 020 ----
mean loss: 236.94
 ---- batch: 030 ----
mean loss: 228.41
 ---- batch: 040 ----
mean loss: 237.13
 ---- batch: 050 ----
mean loss: 228.35
 ---- batch: 060 ----
mean loss: 220.92
 ---- batch: 070 ----
mean loss: 218.28
 ---- batch: 080 ----
mean loss: 224.69
 ---- batch: 090 ----
mean loss: 233.74
train mean loss: 229.80
epoch train time: 0:00:16.504201
elapsed time: 0:24:16.354449
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 03:41:42.947175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.27
 ---- batch: 020 ----
mean loss: 228.29
 ---- batch: 030 ----
mean loss: 233.73
 ---- batch: 040 ----
mean loss: 229.64
 ---- batch: 050 ----
mean loss: 226.30
 ---- batch: 060 ----
mean loss: 229.40
 ---- batch: 070 ----
mean loss: 221.36
 ---- batch: 080 ----
mean loss: 225.97
 ---- batch: 090 ----
mean loss: 221.45
train mean loss: 227.07
epoch train time: 0:00:16.487963
elapsed time: 0:24:32.843546
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 03:41:59.436312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.77
 ---- batch: 020 ----
mean loss: 229.06
 ---- batch: 030 ----
mean loss: 225.09
 ---- batch: 040 ----
mean loss: 225.31
 ---- batch: 050 ----
mean loss: 213.37
 ---- batch: 060 ----
mean loss: 232.62
 ---- batch: 070 ----
mean loss: 232.18
 ---- batch: 080 ----
mean loss: 227.71
 ---- batch: 090 ----
mean loss: 226.62
train mean loss: 226.89
epoch train time: 0:00:16.484841
elapsed time: 0:24:49.329626
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 03:42:15.922435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.36
 ---- batch: 020 ----
mean loss: 227.78
 ---- batch: 030 ----
mean loss: 223.25
 ---- batch: 040 ----
mean loss: 225.25
 ---- batch: 050 ----
mean loss: 222.45
 ---- batch: 060 ----
mean loss: 233.19
 ---- batch: 070 ----
mean loss: 222.32
 ---- batch: 080 ----
mean loss: 224.90
 ---- batch: 090 ----
mean loss: 222.48
train mean loss: 226.00
epoch train time: 0:00:16.439993
elapsed time: 0:25:05.770840
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 03:42:32.363578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.78
 ---- batch: 020 ----
mean loss: 228.35
 ---- batch: 030 ----
mean loss: 227.86
 ---- batch: 040 ----
mean loss: 226.86
 ---- batch: 050 ----
mean loss: 228.05
 ---- batch: 060 ----
mean loss: 224.09
 ---- batch: 070 ----
mean loss: 224.56
 ---- batch: 080 ----
mean loss: 221.89
 ---- batch: 090 ----
mean loss: 229.24
train mean loss: 226.11
epoch train time: 0:00:16.469779
elapsed time: 0:25:22.242002
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 03:42:48.834787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.02
 ---- batch: 020 ----
mean loss: 223.67
 ---- batch: 030 ----
mean loss: 214.20
 ---- batch: 040 ----
mean loss: 221.80
 ---- batch: 050 ----
mean loss: 230.37
 ---- batch: 060 ----
mean loss: 232.01
 ---- batch: 070 ----
mean loss: 222.84
 ---- batch: 080 ----
mean loss: 226.92
 ---- batch: 090 ----
mean loss: 221.54
train mean loss: 224.22
epoch train time: 0:00:16.559690
elapsed time: 0:25:38.802888
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 03:43:05.395753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.19
 ---- batch: 020 ----
mean loss: 227.73
 ---- batch: 030 ----
mean loss: 218.96
 ---- batch: 040 ----
mean loss: 226.42
 ---- batch: 050 ----
mean loss: 227.53
 ---- batch: 060 ----
mean loss: 233.16
 ---- batch: 070 ----
mean loss: 228.21
 ---- batch: 080 ----
mean loss: 224.20
 ---- batch: 090 ----
mean loss: 214.52
train mean loss: 223.96
epoch train time: 0:00:16.546786
elapsed time: 0:25:55.350989
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 03:43:21.943752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.77
 ---- batch: 020 ----
mean loss: 229.54
 ---- batch: 030 ----
mean loss: 223.89
 ---- batch: 040 ----
mean loss: 223.66
 ---- batch: 050 ----
mean loss: 220.05
 ---- batch: 060 ----
mean loss: 222.99
 ---- batch: 070 ----
mean loss: 224.04
 ---- batch: 080 ----
mean loss: 225.93
 ---- batch: 090 ----
mean loss: 216.99
train mean loss: 223.27
epoch train time: 0:00:16.486537
elapsed time: 0:26:11.838696
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 03:43:38.431483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.02
 ---- batch: 020 ----
mean loss: 229.71
 ---- batch: 030 ----
mean loss: 221.24
 ---- batch: 040 ----
mean loss: 224.78
 ---- batch: 050 ----
mean loss: 218.48
 ---- batch: 060 ----
mean loss: 228.84
 ---- batch: 070 ----
mean loss: 227.10
 ---- batch: 080 ----
mean loss: 219.79
 ---- batch: 090 ----
mean loss: 213.25
train mean loss: 222.09
epoch train time: 0:00:16.539026
elapsed time: 0:26:28.378951
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 03:43:54.971708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.04
 ---- batch: 020 ----
mean loss: 218.99
 ---- batch: 030 ----
mean loss: 228.99
 ---- batch: 040 ----
mean loss: 214.06
 ---- batch: 050 ----
mean loss: 216.04
 ---- batch: 060 ----
mean loss: 232.84
 ---- batch: 070 ----
mean loss: 225.60
 ---- batch: 080 ----
mean loss: 221.49
 ---- batch: 090 ----
mean loss: 224.86
train mean loss: 222.01
epoch train time: 0:00:16.512964
elapsed time: 0:26:44.893121
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 03:44:11.485900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.31
 ---- batch: 020 ----
mean loss: 220.21
 ---- batch: 030 ----
mean loss: 222.31
 ---- batch: 040 ----
mean loss: 215.90
 ---- batch: 050 ----
mean loss: 219.71
 ---- batch: 060 ----
mean loss: 221.04
 ---- batch: 070 ----
mean loss: 214.81
 ---- batch: 080 ----
mean loss: 218.57
 ---- batch: 090 ----
mean loss: 217.61
train mean loss: 219.19
epoch train time: 0:00:16.564156
elapsed time: 0:27:01.458563
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 03:44:28.051372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.00
 ---- batch: 020 ----
mean loss: 218.66
 ---- batch: 030 ----
mean loss: 220.61
 ---- batch: 040 ----
mean loss: 221.97
 ---- batch: 050 ----
mean loss: 219.98
 ---- batch: 060 ----
mean loss: 212.40
 ---- batch: 070 ----
mean loss: 228.26
 ---- batch: 080 ----
mean loss: 214.30
 ---- batch: 090 ----
mean loss: 221.83
train mean loss: 221.01
epoch train time: 0:00:16.582099
elapsed time: 0:27:18.041856
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 03:44:44.634581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.62
 ---- batch: 020 ----
mean loss: 216.13
 ---- batch: 030 ----
mean loss: 215.83
 ---- batch: 040 ----
mean loss: 218.92
 ---- batch: 050 ----
mean loss: 218.14
 ---- batch: 060 ----
mean loss: 218.78
 ---- batch: 070 ----
mean loss: 225.43
 ---- batch: 080 ----
mean loss: 219.86
 ---- batch: 090 ----
mean loss: 224.62
train mean loss: 219.18
epoch train time: 0:00:16.500302
elapsed time: 0:27:34.543359
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 03:45:01.136139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.84
 ---- batch: 020 ----
mean loss: 220.53
 ---- batch: 030 ----
mean loss: 209.86
 ---- batch: 040 ----
mean loss: 214.60
 ---- batch: 050 ----
mean loss: 210.30
 ---- batch: 060 ----
mean loss: 222.15
 ---- batch: 070 ----
mean loss: 219.41
 ---- batch: 080 ----
mean loss: 221.75
 ---- batch: 090 ----
mean loss: 214.40
train mean loss: 216.61
epoch train time: 0:00:16.571329
elapsed time: 0:27:51.115853
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 03:45:17.708595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.29
 ---- batch: 020 ----
mean loss: 210.25
 ---- batch: 030 ----
mean loss: 216.06
 ---- batch: 040 ----
mean loss: 212.81
 ---- batch: 050 ----
mean loss: 220.80
 ---- batch: 060 ----
mean loss: 221.41
 ---- batch: 070 ----
mean loss: 216.78
 ---- batch: 080 ----
mean loss: 216.13
 ---- batch: 090 ----
mean loss: 214.62
train mean loss: 216.82
epoch train time: 0:00:16.629479
elapsed time: 0:28:07.746489
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 03:45:34.339250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.51
 ---- batch: 020 ----
mean loss: 213.56
 ---- batch: 030 ----
mean loss: 214.25
 ---- batch: 040 ----
mean loss: 220.92
 ---- batch: 050 ----
mean loss: 228.87
 ---- batch: 060 ----
mean loss: 210.81
 ---- batch: 070 ----
mean loss: 211.12
 ---- batch: 080 ----
mean loss: 218.38
 ---- batch: 090 ----
mean loss: 222.61
train mean loss: 217.73
epoch train time: 0:00:16.662905
elapsed time: 0:28:24.410595
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 03:45:51.003283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.38
 ---- batch: 020 ----
mean loss: 216.21
 ---- batch: 030 ----
mean loss: 215.39
 ---- batch: 040 ----
mean loss: 212.38
 ---- batch: 050 ----
mean loss: 222.19
 ---- batch: 060 ----
mean loss: 217.08
 ---- batch: 070 ----
mean loss: 211.99
 ---- batch: 080 ----
mean loss: 211.35
 ---- batch: 090 ----
mean loss: 217.51
train mean loss: 215.54
epoch train time: 0:00:16.556722
elapsed time: 0:28:40.968488
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 03:46:07.561256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.53
 ---- batch: 020 ----
mean loss: 211.29
 ---- batch: 030 ----
mean loss: 211.96
 ---- batch: 040 ----
mean loss: 212.02
 ---- batch: 050 ----
mean loss: 211.38
 ---- batch: 060 ----
mean loss: 216.54
 ---- batch: 070 ----
mean loss: 215.32
 ---- batch: 080 ----
mean loss: 214.65
 ---- batch: 090 ----
mean loss: 220.55
train mean loss: 215.39
epoch train time: 0:00:16.681266
elapsed time: 0:28:57.650976
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 03:46:24.243734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.53
 ---- batch: 020 ----
mean loss: 217.47
 ---- batch: 030 ----
mean loss: 215.56
 ---- batch: 040 ----
mean loss: 213.58
 ---- batch: 050 ----
mean loss: 215.22
 ---- batch: 060 ----
mean loss: 214.14
 ---- batch: 070 ----
mean loss: 218.29
 ---- batch: 080 ----
mean loss: 201.88
 ---- batch: 090 ----
mean loss: 214.23
train mean loss: 214.98
epoch train time: 0:00:16.602578
elapsed time: 0:29:14.254840
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 03:46:40.847652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.21
 ---- batch: 020 ----
mean loss: 218.72
 ---- batch: 030 ----
mean loss: 213.87
 ---- batch: 040 ----
mean loss: 220.08
 ---- batch: 050 ----
mean loss: 219.50
 ---- batch: 060 ----
mean loss: 217.42
 ---- batch: 070 ----
mean loss: 203.54
 ---- batch: 080 ----
mean loss: 210.26
 ---- batch: 090 ----
mean loss: 207.78
train mean loss: 214.66
epoch train time: 0:00:16.516045
elapsed time: 0:29:30.772213
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 03:46:57.365003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.36
 ---- batch: 020 ----
mean loss: 213.16
 ---- batch: 030 ----
mean loss: 221.57
 ---- batch: 040 ----
mean loss: 214.09
 ---- batch: 050 ----
mean loss: 210.05
 ---- batch: 060 ----
mean loss: 217.89
 ---- batch: 070 ----
mean loss: 212.63
 ---- batch: 080 ----
mean loss: 216.39
 ---- batch: 090 ----
mean loss: 214.91
train mean loss: 214.33
epoch train time: 0:00:16.536588
elapsed time: 0:29:47.310059
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 03:47:13.902893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.76
 ---- batch: 020 ----
mean loss: 219.44
 ---- batch: 030 ----
mean loss: 210.34
 ---- batch: 040 ----
mean loss: 205.56
 ---- batch: 050 ----
mean loss: 214.84
 ---- batch: 060 ----
mean loss: 218.58
 ---- batch: 070 ----
mean loss: 210.76
 ---- batch: 080 ----
mean loss: 216.53
 ---- batch: 090 ----
mean loss: 213.21
train mean loss: 213.24
epoch train time: 0:00:16.539350
elapsed time: 0:30:03.850836
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 03:47:30.443495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.18
 ---- batch: 020 ----
mean loss: 215.76
 ---- batch: 030 ----
mean loss: 209.68
 ---- batch: 040 ----
mean loss: 214.97
 ---- batch: 050 ----
mean loss: 208.21
 ---- batch: 060 ----
mean loss: 215.32
 ---- batch: 070 ----
mean loss: 210.95
 ---- batch: 080 ----
mean loss: 219.27
 ---- batch: 090 ----
mean loss: 213.03
train mean loss: 212.10
epoch train time: 0:00:16.496291
elapsed time: 0:30:20.348366
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 03:47:46.941137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.34
 ---- batch: 020 ----
mean loss: 216.22
 ---- batch: 030 ----
mean loss: 207.47
 ---- batch: 040 ----
mean loss: 210.40
 ---- batch: 050 ----
mean loss: 207.66
 ---- batch: 060 ----
mean loss: 220.39
 ---- batch: 070 ----
mean loss: 215.92
 ---- batch: 080 ----
mean loss: 213.37
 ---- batch: 090 ----
mean loss: 213.15
train mean loss: 213.65
epoch train time: 0:00:16.534978
elapsed time: 0:30:36.884519
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 03:48:03.477266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.88
 ---- batch: 020 ----
mean loss: 211.78
 ---- batch: 030 ----
mean loss: 212.31
 ---- batch: 040 ----
mean loss: 207.41
 ---- batch: 050 ----
mean loss: 208.04
 ---- batch: 060 ----
mean loss: 212.81
 ---- batch: 070 ----
mean loss: 214.91
 ---- batch: 080 ----
mean loss: 210.29
 ---- batch: 090 ----
mean loss: 210.36
train mean loss: 212.06
epoch train time: 0:00:16.498455
elapsed time: 0:30:53.384129
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 03:48:19.976882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.81
 ---- batch: 020 ----
mean loss: 211.06
 ---- batch: 030 ----
mean loss: 213.07
 ---- batch: 040 ----
mean loss: 210.29
 ---- batch: 050 ----
mean loss: 214.80
 ---- batch: 060 ----
mean loss: 216.48
 ---- batch: 070 ----
mean loss: 207.24
 ---- batch: 080 ----
mean loss: 205.40
 ---- batch: 090 ----
mean loss: 209.41
train mean loss: 212.00
epoch train time: 0:00:16.496755
elapsed time: 0:31:09.882097
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 03:48:36.474831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.11
 ---- batch: 020 ----
mean loss: 211.47
 ---- batch: 030 ----
mean loss: 216.73
 ---- batch: 040 ----
mean loss: 214.77
 ---- batch: 050 ----
mean loss: 212.82
 ---- batch: 060 ----
mean loss: 210.00
 ---- batch: 070 ----
mean loss: 215.26
 ---- batch: 080 ----
mean loss: 204.91
 ---- batch: 090 ----
mean loss: 207.63
train mean loss: 210.29
epoch train time: 0:00:16.458792
elapsed time: 0:31:26.341989
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 03:48:52.934747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.37
 ---- batch: 020 ----
mean loss: 210.56
 ---- batch: 030 ----
mean loss: 208.14
 ---- batch: 040 ----
mean loss: 205.35
 ---- batch: 050 ----
mean loss: 205.50
 ---- batch: 060 ----
mean loss: 214.77
 ---- batch: 070 ----
mean loss: 219.32
 ---- batch: 080 ----
mean loss: 211.45
 ---- batch: 090 ----
mean loss: 213.07
train mean loss: 210.47
epoch train time: 0:00:16.525244
elapsed time: 0:31:42.868423
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 03:49:09.461195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.96
 ---- batch: 020 ----
mean loss: 218.00
 ---- batch: 030 ----
mean loss: 209.15
 ---- batch: 040 ----
mean loss: 213.82
 ---- batch: 050 ----
mean loss: 211.49
 ---- batch: 060 ----
mean loss: 215.66
 ---- batch: 070 ----
mean loss: 202.51
 ---- batch: 080 ----
mean loss: 209.78
 ---- batch: 090 ----
mean loss: 209.16
train mean loss: 212.12
epoch train time: 0:00:16.533731
elapsed time: 0:31:59.403420
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 03:49:25.996176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.32
 ---- batch: 020 ----
mean loss: 213.93
 ---- batch: 030 ----
mean loss: 205.78
 ---- batch: 040 ----
mean loss: 207.64
 ---- batch: 050 ----
mean loss: 210.20
 ---- batch: 060 ----
mean loss: 210.16
 ---- batch: 070 ----
mean loss: 213.62
 ---- batch: 080 ----
mean loss: 199.30
 ---- batch: 090 ----
mean loss: 212.92
train mean loss: 209.83
epoch train time: 0:00:16.582303
elapsed time: 0:32:15.986876
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 03:49:42.579598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.41
 ---- batch: 020 ----
mean loss: 220.08
 ---- batch: 030 ----
mean loss: 213.46
 ---- batch: 040 ----
mean loss: 202.70
 ---- batch: 050 ----
mean loss: 216.45
 ---- batch: 060 ----
mean loss: 213.47
 ---- batch: 070 ----
mean loss: 212.49
 ---- batch: 080 ----
mean loss: 208.02
 ---- batch: 090 ----
mean loss: 206.40
train mean loss: 211.19
epoch train time: 0:00:16.569293
elapsed time: 0:32:32.557306
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 03:49:59.150092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.93
 ---- batch: 020 ----
mean loss: 205.93
 ---- batch: 030 ----
mean loss: 207.78
 ---- batch: 040 ----
mean loss: 202.73
 ---- batch: 050 ----
mean loss: 209.26
 ---- batch: 060 ----
mean loss: 215.72
 ---- batch: 070 ----
mean loss: 215.60
 ---- batch: 080 ----
mean loss: 214.63
 ---- batch: 090 ----
mean loss: 213.23
train mean loss: 208.99
epoch train time: 0:00:16.596028
elapsed time: 0:32:49.154585
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 03:50:15.747333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.80
 ---- batch: 020 ----
mean loss: 205.01
 ---- batch: 030 ----
mean loss: 210.76
 ---- batch: 040 ----
mean loss: 206.57
 ---- batch: 050 ----
mean loss: 206.16
 ---- batch: 060 ----
mean loss: 205.14
 ---- batch: 070 ----
mean loss: 217.13
 ---- batch: 080 ----
mean loss: 212.99
 ---- batch: 090 ----
mean loss: 220.99
train mean loss: 210.50
epoch train time: 0:00:16.606503
elapsed time: 0:33:05.762261
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 03:50:32.355005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.21
 ---- batch: 020 ----
mean loss: 211.12
 ---- batch: 030 ----
mean loss: 209.97
 ---- batch: 040 ----
mean loss: 213.73
 ---- batch: 050 ----
mean loss: 211.41
 ---- batch: 060 ----
mean loss: 207.00
 ---- batch: 070 ----
mean loss: 207.22
 ---- batch: 080 ----
mean loss: 208.64
 ---- batch: 090 ----
mean loss: 199.97
train mean loss: 207.93
epoch train time: 0:00:16.549934
elapsed time: 0:33:22.313339
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 03:50:48.906064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.53
 ---- batch: 020 ----
mean loss: 206.26
 ---- batch: 030 ----
mean loss: 202.86
 ---- batch: 040 ----
mean loss: 214.91
 ---- batch: 050 ----
mean loss: 204.02
 ---- batch: 060 ----
mean loss: 209.57
 ---- batch: 070 ----
mean loss: 205.96
 ---- batch: 080 ----
mean loss: 210.92
 ---- batch: 090 ----
mean loss: 204.90
train mean loss: 207.60
epoch train time: 0:00:16.584437
elapsed time: 0:33:38.898915
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 03:51:05.491673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.20
 ---- batch: 020 ----
mean loss: 207.79
 ---- batch: 030 ----
mean loss: 203.39
 ---- batch: 040 ----
mean loss: 213.49
 ---- batch: 050 ----
mean loss: 212.17
 ---- batch: 060 ----
mean loss: 204.97
 ---- batch: 070 ----
mean loss: 199.76
 ---- batch: 080 ----
mean loss: 211.84
 ---- batch: 090 ----
mean loss: 201.22
train mean loss: 207.00
epoch train time: 0:00:16.533469
elapsed time: 0:33:55.434009
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 03:51:22.026636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.22
 ---- batch: 020 ----
mean loss: 206.89
 ---- batch: 030 ----
mean loss: 213.06
 ---- batch: 040 ----
mean loss: 210.95
 ---- batch: 050 ----
mean loss: 216.38
 ---- batch: 060 ----
mean loss: 205.18
 ---- batch: 070 ----
mean loss: 210.97
 ---- batch: 080 ----
mean loss: 214.31
 ---- batch: 090 ----
mean loss: 203.96
train mean loss: 208.56
epoch train time: 0:00:16.670612
elapsed time: 0:34:12.105735
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 03:51:38.698580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.32
 ---- batch: 020 ----
mean loss: 208.28
 ---- batch: 030 ----
mean loss: 203.11
 ---- batch: 040 ----
mean loss: 211.30
 ---- batch: 050 ----
mean loss: 204.50
 ---- batch: 060 ----
mean loss: 205.91
 ---- batch: 070 ----
mean loss: 199.10
 ---- batch: 080 ----
mean loss: 200.96
 ---- batch: 090 ----
mean loss: 202.36
train mean loss: 205.16
epoch train time: 0:00:16.520161
elapsed time: 0:34:28.627150
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 03:51:55.219911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.99
 ---- batch: 020 ----
mean loss: 197.77
 ---- batch: 030 ----
mean loss: 201.95
 ---- batch: 040 ----
mean loss: 209.49
 ---- batch: 050 ----
mean loss: 208.72
 ---- batch: 060 ----
mean loss: 207.05
 ---- batch: 070 ----
mean loss: 208.85
 ---- batch: 080 ----
mean loss: 208.10
 ---- batch: 090 ----
mean loss: 206.40
train mean loss: 206.28
epoch train time: 0:00:16.592764
elapsed time: 0:34:45.221117
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 03:52:11.813912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.20
 ---- batch: 020 ----
mean loss: 203.03
 ---- batch: 030 ----
mean loss: 198.52
 ---- batch: 040 ----
mean loss: 206.59
 ---- batch: 050 ----
mean loss: 205.07
 ---- batch: 060 ----
mean loss: 209.45
 ---- batch: 070 ----
mean loss: 209.19
 ---- batch: 080 ----
mean loss: 206.62
 ---- batch: 090 ----
mean loss: 209.47
train mean loss: 205.36
epoch train time: 0:00:16.548840
elapsed time: 0:35:01.771290
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 03:52:28.364030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.88
 ---- batch: 020 ----
mean loss: 207.90
 ---- batch: 030 ----
mean loss: 200.83
 ---- batch: 040 ----
mean loss: 200.25
 ---- batch: 050 ----
mean loss: 206.11
 ---- batch: 060 ----
mean loss: 201.92
 ---- batch: 070 ----
mean loss: 204.22
 ---- batch: 080 ----
mean loss: 210.33
 ---- batch: 090 ----
mean loss: 209.83
train mean loss: 205.24
epoch train time: 0:00:16.494864
elapsed time: 0:35:18.267405
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 03:52:44.860266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.18
 ---- batch: 020 ----
mean loss: 205.85
 ---- batch: 030 ----
mean loss: 210.96
 ---- batch: 040 ----
mean loss: 206.38
 ---- batch: 050 ----
mean loss: 202.24
 ---- batch: 060 ----
mean loss: 203.28
 ---- batch: 070 ----
mean loss: 202.05
 ---- batch: 080 ----
mean loss: 205.24
 ---- batch: 090 ----
mean loss: 207.00
train mean loss: 204.32
epoch train time: 0:00:16.551577
elapsed time: 0:35:34.820570
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 03:53:01.413267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.23
 ---- batch: 020 ----
mean loss: 203.45
 ---- batch: 030 ----
mean loss: 203.42
 ---- batch: 040 ----
mean loss: 205.85
 ---- batch: 050 ----
mean loss: 203.72
 ---- batch: 060 ----
mean loss: 203.99
 ---- batch: 070 ----
mean loss: 198.77
 ---- batch: 080 ----
mean loss: 201.63
 ---- batch: 090 ----
mean loss: 215.52
train mean loss: 204.03
epoch train time: 0:00:16.536199
elapsed time: 0:35:51.357937
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 03:53:17.950709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.98
 ---- batch: 020 ----
mean loss: 195.72
 ---- batch: 030 ----
mean loss: 203.06
 ---- batch: 040 ----
mean loss: 204.31
 ---- batch: 050 ----
mean loss: 202.10
 ---- batch: 060 ----
mean loss: 202.00
 ---- batch: 070 ----
mean loss: 205.34
 ---- batch: 080 ----
mean loss: 209.24
 ---- batch: 090 ----
mean loss: 211.58
train mean loss: 204.56
epoch train time: 0:00:16.572194
elapsed time: 0:36:07.931427
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 03:53:34.524179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.08
 ---- batch: 020 ----
mean loss: 204.62
 ---- batch: 030 ----
mean loss: 202.44
 ---- batch: 040 ----
mean loss: 206.20
 ---- batch: 050 ----
mean loss: 201.90
 ---- batch: 060 ----
mean loss: 200.96
 ---- batch: 070 ----
mean loss: 205.36
 ---- batch: 080 ----
mean loss: 210.97
 ---- batch: 090 ----
mean loss: 206.88
train mean loss: 205.13
epoch train time: 0:00:16.560656
elapsed time: 0:36:24.493230
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 03:53:51.086038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.25
 ---- batch: 020 ----
mean loss: 200.69
 ---- batch: 030 ----
mean loss: 200.91
 ---- batch: 040 ----
mean loss: 206.52
 ---- batch: 050 ----
mean loss: 203.50
 ---- batch: 060 ----
mean loss: 201.61
 ---- batch: 070 ----
mean loss: 203.70
 ---- batch: 080 ----
mean loss: 201.00
 ---- batch: 090 ----
mean loss: 211.47
train mean loss: 203.47
epoch train time: 0:00:16.539367
elapsed time: 0:36:41.033874
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 03:54:07.626633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.99
 ---- batch: 020 ----
mean loss: 201.96
 ---- batch: 030 ----
mean loss: 211.33
 ---- batch: 040 ----
mean loss: 211.56
 ---- batch: 050 ----
mean loss: 208.37
 ---- batch: 060 ----
mean loss: 195.66
 ---- batch: 070 ----
mean loss: 206.11
 ---- batch: 080 ----
mean loss: 211.62
 ---- batch: 090 ----
mean loss: 206.10
train mean loss: 207.20
epoch train time: 0:00:16.547587
elapsed time: 0:36:57.582645
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 03:54:24.175414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.23
 ---- batch: 020 ----
mean loss: 205.49
 ---- batch: 030 ----
mean loss: 200.98
 ---- batch: 040 ----
mean loss: 199.52
 ---- batch: 050 ----
mean loss: 202.98
 ---- batch: 060 ----
mean loss: 204.03
 ---- batch: 070 ----
mean loss: 201.91
 ---- batch: 080 ----
mean loss: 209.22
 ---- batch: 090 ----
mean loss: 199.19
train mean loss: 203.35
epoch train time: 0:00:16.536097
elapsed time: 0:37:14.119917
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 03:54:40.712705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.49
 ---- batch: 020 ----
mean loss: 195.32
 ---- batch: 030 ----
mean loss: 197.54
 ---- batch: 040 ----
mean loss: 204.05
 ---- batch: 050 ----
mean loss: 204.57
 ---- batch: 060 ----
mean loss: 198.93
 ---- batch: 070 ----
mean loss: 201.58
 ---- batch: 080 ----
mean loss: 204.84
 ---- batch: 090 ----
mean loss: 207.24
train mean loss: 202.63
epoch train time: 0:00:16.468889
elapsed time: 0:37:30.590032
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 03:54:57.182796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.58
 ---- batch: 020 ----
mean loss: 203.08
 ---- batch: 030 ----
mean loss: 198.93
 ---- batch: 040 ----
mean loss: 202.65
 ---- batch: 050 ----
mean loss: 205.14
 ---- batch: 060 ----
mean loss: 212.24
 ---- batch: 070 ----
mean loss: 203.21
 ---- batch: 080 ----
mean loss: 200.17
 ---- batch: 090 ----
mean loss: 199.14
train mean loss: 203.07
epoch train time: 0:00:16.548308
elapsed time: 0:37:47.139506
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 03:55:13.732280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.79
 ---- batch: 020 ----
mean loss: 203.40
 ---- batch: 030 ----
mean loss: 200.02
 ---- batch: 040 ----
mean loss: 204.65
 ---- batch: 050 ----
mean loss: 195.73
 ---- batch: 060 ----
mean loss: 198.31
 ---- batch: 070 ----
mean loss: 197.90
 ---- batch: 080 ----
mean loss: 203.58
 ---- batch: 090 ----
mean loss: 208.78
train mean loss: 202.49
epoch train time: 0:00:16.516113
elapsed time: 0:38:03.656960
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 03:55:30.249764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.94
 ---- batch: 020 ----
mean loss: 198.34
 ---- batch: 030 ----
mean loss: 204.76
 ---- batch: 040 ----
mean loss: 201.77
 ---- batch: 050 ----
mean loss: 197.83
 ---- batch: 060 ----
mean loss: 204.04
 ---- batch: 070 ----
mean loss: 202.33
 ---- batch: 080 ----
mean loss: 200.17
 ---- batch: 090 ----
mean loss: 202.57
train mean loss: 201.79
epoch train time: 0:00:16.544019
elapsed time: 0:38:20.202256
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 03:55:46.795066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.68
 ---- batch: 020 ----
mean loss: 203.83
 ---- batch: 030 ----
mean loss: 198.47
 ---- batch: 040 ----
mean loss: 203.03
 ---- batch: 050 ----
mean loss: 197.32
 ---- batch: 060 ----
mean loss: 203.17
 ---- batch: 070 ----
mean loss: 208.20
 ---- batch: 080 ----
mean loss: 203.00
 ---- batch: 090 ----
mean loss: 196.85
train mean loss: 202.34
epoch train time: 0:00:16.628386
elapsed time: 0:38:36.831924
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 03:56:03.424705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.93
 ---- batch: 020 ----
mean loss: 206.53
 ---- batch: 030 ----
mean loss: 204.89
 ---- batch: 040 ----
mean loss: 201.03
 ---- batch: 050 ----
mean loss: 207.93
 ---- batch: 060 ----
mean loss: 199.95
 ---- batch: 070 ----
mean loss: 200.84
 ---- batch: 080 ----
mean loss: 202.37
 ---- batch: 090 ----
mean loss: 207.46
train mean loss: 204.05
epoch train time: 0:00:16.536650
elapsed time: 0:38:53.369818
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 03:56:19.962562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.21
 ---- batch: 020 ----
mean loss: 198.82
 ---- batch: 030 ----
mean loss: 204.02
 ---- batch: 040 ----
mean loss: 200.61
 ---- batch: 050 ----
mean loss: 196.66
 ---- batch: 060 ----
mean loss: 203.73
 ---- batch: 070 ----
mean loss: 202.90
 ---- batch: 080 ----
mean loss: 209.24
 ---- batch: 090 ----
mean loss: 196.13
train mean loss: 200.77
epoch train time: 0:00:16.583396
elapsed time: 0:39:09.954372
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 03:56:36.547098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.58
 ---- batch: 020 ----
mean loss: 201.54
 ---- batch: 030 ----
mean loss: 197.59
 ---- batch: 040 ----
mean loss: 201.74
 ---- batch: 050 ----
mean loss: 205.63
 ---- batch: 060 ----
mean loss: 201.28
 ---- batch: 070 ----
mean loss: 203.68
 ---- batch: 080 ----
mean loss: 201.00
 ---- batch: 090 ----
mean loss: 205.30
train mean loss: 201.46
epoch train time: 0:00:16.645893
elapsed time: 0:39:26.601403
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 03:56:53.194249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.08
 ---- batch: 020 ----
mean loss: 203.80
 ---- batch: 030 ----
mean loss: 208.76
 ---- batch: 040 ----
mean loss: 188.75
 ---- batch: 050 ----
mean loss: 205.63
 ---- batch: 060 ----
mean loss: 201.30
 ---- batch: 070 ----
mean loss: 203.21
 ---- batch: 080 ----
mean loss: 199.78
 ---- batch: 090 ----
mean loss: 204.79
train mean loss: 201.49
epoch train time: 0:00:16.522378
elapsed time: 0:39:43.125283
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 03:57:09.718138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.91
 ---- batch: 020 ----
mean loss: 198.07
 ---- batch: 030 ----
mean loss: 204.11
 ---- batch: 040 ----
mean loss: 206.68
 ---- batch: 050 ----
mean loss: 205.74
 ---- batch: 060 ----
mean loss: 202.62
 ---- batch: 070 ----
mean loss: 204.33
 ---- batch: 080 ----
mean loss: 211.60
 ---- batch: 090 ----
mean loss: 196.92
train mean loss: 203.32
epoch train time: 0:00:16.547352
elapsed time: 0:39:59.673894
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 03:57:26.266657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.91
 ---- batch: 020 ----
mean loss: 205.06
 ---- batch: 030 ----
mean loss: 192.75
 ---- batch: 040 ----
mean loss: 205.94
 ---- batch: 050 ----
mean loss: 195.83
 ---- batch: 060 ----
mean loss: 192.86
 ---- batch: 070 ----
mean loss: 202.66
 ---- batch: 080 ----
mean loss: 195.42
 ---- batch: 090 ----
mean loss: 196.15
train mean loss: 199.85
epoch train time: 0:00:16.505878
elapsed time: 0:40:16.180926
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 03:57:42.773701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.20
 ---- batch: 020 ----
mean loss: 207.35
 ---- batch: 030 ----
mean loss: 200.48
 ---- batch: 040 ----
mean loss: 201.62
 ---- batch: 050 ----
mean loss: 191.40
 ---- batch: 060 ----
mean loss: 209.32
 ---- batch: 070 ----
mean loss: 203.13
 ---- batch: 080 ----
mean loss: 198.37
 ---- batch: 090 ----
mean loss: 202.93
train mean loss: 201.52
epoch train time: 0:00:16.546121
elapsed time: 0:40:32.728314
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 03:57:59.321062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.18
 ---- batch: 020 ----
mean loss: 205.19
 ---- batch: 030 ----
mean loss: 197.18
 ---- batch: 040 ----
mean loss: 203.26
 ---- batch: 050 ----
mean loss: 203.63
 ---- batch: 060 ----
mean loss: 203.38
 ---- batch: 070 ----
mean loss: 195.16
 ---- batch: 080 ----
mean loss: 205.60
 ---- batch: 090 ----
mean loss: 199.26
train mean loss: 199.91
epoch train time: 0:00:16.534537
elapsed time: 0:40:49.264057
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 03:58:15.856785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.60
 ---- batch: 020 ----
mean loss: 201.48
 ---- batch: 030 ----
mean loss: 203.77
 ---- batch: 040 ----
mean loss: 202.39
 ---- batch: 050 ----
mean loss: 200.82
 ---- batch: 060 ----
mean loss: 200.60
 ---- batch: 070 ----
mean loss: 203.55
 ---- batch: 080 ----
mean loss: 192.53
 ---- batch: 090 ----
mean loss: 196.99
train mean loss: 200.12
epoch train time: 0:00:16.542737
elapsed time: 0:41:05.808015
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 03:58:32.400790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.26
 ---- batch: 020 ----
mean loss: 204.42
 ---- batch: 030 ----
mean loss: 203.01
 ---- batch: 040 ----
mean loss: 199.58
 ---- batch: 050 ----
mean loss: 202.34
 ---- batch: 060 ----
mean loss: 205.41
 ---- batch: 070 ----
mean loss: 194.22
 ---- batch: 080 ----
mean loss: 201.28
 ---- batch: 090 ----
mean loss: 202.32
train mean loss: 200.77
epoch train time: 0:00:16.560341
elapsed time: 0:41:22.369582
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 03:58:48.962452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.30
 ---- batch: 020 ----
mean loss: 198.77
 ---- batch: 030 ----
mean loss: 199.20
 ---- batch: 040 ----
mean loss: 201.88
 ---- batch: 050 ----
mean loss: 191.37
 ---- batch: 060 ----
mean loss: 194.82
 ---- batch: 070 ----
mean loss: 202.73
 ---- batch: 080 ----
mean loss: 203.70
 ---- batch: 090 ----
mean loss: 200.43
train mean loss: 199.45
epoch train time: 0:00:16.589712
elapsed time: 0:41:38.961310
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 03:59:05.553717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.56
 ---- batch: 020 ----
mean loss: 202.39
 ---- batch: 030 ----
mean loss: 201.32
 ---- batch: 040 ----
mean loss: 205.97
 ---- batch: 050 ----
mean loss: 199.18
 ---- batch: 060 ----
mean loss: 206.08
 ---- batch: 070 ----
mean loss: 201.34
 ---- batch: 080 ----
mean loss: 195.03
 ---- batch: 090 ----
mean loss: 198.76
train mean loss: 200.55
epoch train time: 0:00:16.569584
elapsed time: 0:41:55.531859
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 03:59:22.124608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.32
 ---- batch: 020 ----
mean loss: 206.46
 ---- batch: 030 ----
mean loss: 196.84
 ---- batch: 040 ----
mean loss: 205.62
 ---- batch: 050 ----
mean loss: 197.73
 ---- batch: 060 ----
mean loss: 203.85
 ---- batch: 070 ----
mean loss: 191.27
 ---- batch: 080 ----
mean loss: 206.64
 ---- batch: 090 ----
mean loss: 198.24
train mean loss: 201.47
epoch train time: 0:00:16.604510
elapsed time: 0:42:12.137615
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 03:59:38.730375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.95
 ---- batch: 020 ----
mean loss: 198.32
 ---- batch: 030 ----
mean loss: 196.59
 ---- batch: 040 ----
mean loss: 197.60
 ---- batch: 050 ----
mean loss: 199.34
 ---- batch: 060 ----
mean loss: 211.69
 ---- batch: 070 ----
mean loss: 199.84
 ---- batch: 080 ----
mean loss: 200.87
 ---- batch: 090 ----
mean loss: 204.98
train mean loss: 200.24
epoch train time: 0:00:16.584858
elapsed time: 0:42:28.723638
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 03:59:55.316401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.44
 ---- batch: 020 ----
mean loss: 202.14
 ---- batch: 030 ----
mean loss: 203.16
 ---- batch: 040 ----
mean loss: 194.65
 ---- batch: 050 ----
mean loss: 197.90
 ---- batch: 060 ----
mean loss: 199.62
 ---- batch: 070 ----
mean loss: 201.94
 ---- batch: 080 ----
mean loss: 195.22
 ---- batch: 090 ----
mean loss: 201.48
train mean loss: 199.19
epoch train time: 0:00:16.579460
elapsed time: 0:42:45.304353
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 04:00:11.897128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.15
 ---- batch: 020 ----
mean loss: 194.66
 ---- batch: 030 ----
mean loss: 206.64
 ---- batch: 040 ----
mean loss: 205.79
 ---- batch: 050 ----
mean loss: 199.84
 ---- batch: 060 ----
mean loss: 195.07
 ---- batch: 070 ----
mean loss: 193.75
 ---- batch: 080 ----
mean loss: 193.27
 ---- batch: 090 ----
mean loss: 196.78
train mean loss: 198.70
epoch train time: 0:00:16.578079
elapsed time: 0:43:01.883577
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 04:00:28.476309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.55
 ---- batch: 020 ----
mean loss: 198.67
 ---- batch: 030 ----
mean loss: 194.95
 ---- batch: 040 ----
mean loss: 200.11
 ---- batch: 050 ----
mean loss: 200.06
 ---- batch: 060 ----
mean loss: 207.04
 ---- batch: 070 ----
mean loss: 200.01
 ---- batch: 080 ----
mean loss: 200.06
 ---- batch: 090 ----
mean loss: 195.51
train mean loss: 199.04
epoch train time: 0:00:16.540010
elapsed time: 0:43:18.424774
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 04:00:45.017570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.59
 ---- batch: 020 ----
mean loss: 199.52
 ---- batch: 030 ----
mean loss: 200.16
 ---- batch: 040 ----
mean loss: 194.80
 ---- batch: 050 ----
mean loss: 192.03
 ---- batch: 060 ----
mean loss: 205.59
 ---- batch: 070 ----
mean loss: 197.86
 ---- batch: 080 ----
mean loss: 192.98
 ---- batch: 090 ----
mean loss: 195.79
train mean loss: 197.44
epoch train time: 0:00:16.550481
elapsed time: 0:43:34.976430
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 04:01:01.569244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.23
 ---- batch: 020 ----
mean loss: 197.70
 ---- batch: 030 ----
mean loss: 209.60
 ---- batch: 040 ----
mean loss: 202.92
 ---- batch: 050 ----
mean loss: 190.65
 ---- batch: 060 ----
mean loss: 197.80
 ---- batch: 070 ----
mean loss: 204.14
 ---- batch: 080 ----
mean loss: 218.17
 ---- batch: 090 ----
mean loss: 197.88
train mean loss: 201.79
epoch train time: 0:00:16.580069
elapsed time: 0:43:51.557731
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 04:01:18.150444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.23
 ---- batch: 020 ----
mean loss: 205.47
 ---- batch: 030 ----
mean loss: 200.52
 ---- batch: 040 ----
mean loss: 201.97
 ---- batch: 050 ----
mean loss: 202.98
 ---- batch: 060 ----
mean loss: 205.57
 ---- batch: 070 ----
mean loss: 196.98
 ---- batch: 080 ----
mean loss: 191.59
 ---- batch: 090 ----
mean loss: 192.52
train mean loss: 199.42
epoch train time: 0:00:16.573002
elapsed time: 0:44:08.131857
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 04:01:34.724619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.41
 ---- batch: 020 ----
mean loss: 195.27
 ---- batch: 030 ----
mean loss: 194.11
 ---- batch: 040 ----
mean loss: 194.13
 ---- batch: 050 ----
mean loss: 192.95
 ---- batch: 060 ----
mean loss: 192.18
 ---- batch: 070 ----
mean loss: 198.22
 ---- batch: 080 ----
mean loss: 202.46
 ---- batch: 090 ----
mean loss: 195.89
train mean loss: 197.13
epoch train time: 0:00:16.593371
elapsed time: 0:44:24.726408
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 04:01:51.319137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.60
 ---- batch: 020 ----
mean loss: 188.90
 ---- batch: 030 ----
mean loss: 201.28
 ---- batch: 040 ----
mean loss: 199.71
 ---- batch: 050 ----
mean loss: 202.37
 ---- batch: 060 ----
mean loss: 195.94
 ---- batch: 070 ----
mean loss: 201.32
 ---- batch: 080 ----
mean loss: 199.87
 ---- batch: 090 ----
mean loss: 197.49
train mean loss: 197.69
epoch train time: 0:00:16.622499
elapsed time: 0:44:41.350034
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 04:02:07.942748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.29
 ---- batch: 020 ----
mean loss: 197.05
 ---- batch: 030 ----
mean loss: 192.69
 ---- batch: 040 ----
mean loss: 201.99
 ---- batch: 050 ----
mean loss: 198.51
 ---- batch: 060 ----
mean loss: 196.94
 ---- batch: 070 ----
mean loss: 207.58
 ---- batch: 080 ----
mean loss: 195.84
 ---- batch: 090 ----
mean loss: 194.82
train mean loss: 197.79
epoch train time: 0:00:16.501086
elapsed time: 0:44:57.852283
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 04:02:24.445027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.24
 ---- batch: 020 ----
mean loss: 192.27
 ---- batch: 030 ----
mean loss: 201.08
 ---- batch: 040 ----
mean loss: 195.62
 ---- batch: 050 ----
mean loss: 198.42
 ---- batch: 060 ----
mean loss: 206.60
 ---- batch: 070 ----
mean loss: 204.51
 ---- batch: 080 ----
mean loss: 196.86
 ---- batch: 090 ----
mean loss: 197.76
train mean loss: 198.95
epoch train time: 0:00:16.502570
elapsed time: 0:45:14.356064
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 04:02:40.948828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.10
 ---- batch: 020 ----
mean loss: 204.14
 ---- batch: 030 ----
mean loss: 201.95
 ---- batch: 040 ----
mean loss: 205.15
 ---- batch: 050 ----
mean loss: 190.78
 ---- batch: 060 ----
mean loss: 201.04
 ---- batch: 070 ----
mean loss: 198.61
 ---- batch: 080 ----
mean loss: 199.73
 ---- batch: 090 ----
mean loss: 190.18
train mean loss: 197.74
epoch train time: 0:00:16.457289
elapsed time: 0:45:30.814490
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 04:02:57.407202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.96
 ---- batch: 020 ----
mean loss: 188.47
 ---- batch: 030 ----
mean loss: 194.72
 ---- batch: 040 ----
mean loss: 197.48
 ---- batch: 050 ----
mean loss: 197.24
 ---- batch: 060 ----
mean loss: 199.75
 ---- batch: 070 ----
mean loss: 200.35
 ---- batch: 080 ----
mean loss: 190.70
 ---- batch: 090 ----
mean loss: 199.10
train mean loss: 196.73
epoch train time: 0:00:16.557679
elapsed time: 0:45:47.373336
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 04:03:13.966112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.06
 ---- batch: 020 ----
mean loss: 192.85
 ---- batch: 030 ----
mean loss: 195.16
 ---- batch: 040 ----
mean loss: 190.68
 ---- batch: 050 ----
mean loss: 202.40
 ---- batch: 060 ----
mean loss: 196.72
 ---- batch: 070 ----
mean loss: 201.77
 ---- batch: 080 ----
mean loss: 196.23
 ---- batch: 090 ----
mean loss: 205.09
train mean loss: 197.34
epoch train time: 0:00:16.553614
elapsed time: 0:46:03.928292
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 04:03:30.521098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.83
 ---- batch: 020 ----
mean loss: 200.95
 ---- batch: 030 ----
mean loss: 192.72
 ---- batch: 040 ----
mean loss: 207.66
 ---- batch: 050 ----
mean loss: 196.79
 ---- batch: 060 ----
mean loss: 207.94
 ---- batch: 070 ----
mean loss: 195.92
 ---- batch: 080 ----
mean loss: 192.48
 ---- batch: 090 ----
mean loss: 199.24
train mean loss: 198.69
epoch train time: 0:00:16.483702
elapsed time: 0:46:20.413226
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 04:03:47.006118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.31
 ---- batch: 020 ----
mean loss: 195.49
 ---- batch: 030 ----
mean loss: 199.66
 ---- batch: 040 ----
mean loss: 197.26
 ---- batch: 050 ----
mean loss: 196.55
 ---- batch: 060 ----
mean loss: 193.71
 ---- batch: 070 ----
mean loss: 186.08
 ---- batch: 080 ----
mean loss: 200.41
 ---- batch: 090 ----
mean loss: 203.69
train mean loss: 197.23
epoch train time: 0:00:16.474668
elapsed time: 0:46:36.889145
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 04:04:03.481889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.26
 ---- batch: 020 ----
mean loss: 196.98
 ---- batch: 030 ----
mean loss: 200.92
 ---- batch: 040 ----
mean loss: 192.58
 ---- batch: 050 ----
mean loss: 196.23
 ---- batch: 060 ----
mean loss: 195.17
 ---- batch: 070 ----
mean loss: 200.66
 ---- batch: 080 ----
mean loss: 198.33
 ---- batch: 090 ----
mean loss: 194.93
train mean loss: 197.05
epoch train time: 0:00:16.503911
elapsed time: 0:46:53.394188
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 04:04:19.986910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.76
 ---- batch: 020 ----
mean loss: 200.42
 ---- batch: 030 ----
mean loss: 191.41
 ---- batch: 040 ----
mean loss: 200.69
 ---- batch: 050 ----
mean loss: 204.04
 ---- batch: 060 ----
mean loss: 202.37
 ---- batch: 070 ----
mean loss: 190.29
 ---- batch: 080 ----
mean loss: 199.00
 ---- batch: 090 ----
mean loss: 195.16
train mean loss: 198.64
epoch train time: 0:00:16.459096
elapsed time: 0:47:09.854414
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 04:04:36.447151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.70
 ---- batch: 020 ----
mean loss: 194.80
 ---- batch: 030 ----
mean loss: 192.05
 ---- batch: 040 ----
mean loss: 196.87
 ---- batch: 050 ----
mean loss: 198.06
 ---- batch: 060 ----
mean loss: 202.37
 ---- batch: 070 ----
mean loss: 200.00
 ---- batch: 080 ----
mean loss: 198.46
 ---- batch: 090 ----
mean loss: 193.93
train mean loss: 197.35
epoch train time: 0:00:16.490891
elapsed time: 0:47:26.346513
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 04:04:52.939227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.05
 ---- batch: 020 ----
mean loss: 196.80
 ---- batch: 030 ----
mean loss: 193.94
 ---- batch: 040 ----
mean loss: 195.60
 ---- batch: 050 ----
mean loss: 205.61
 ---- batch: 060 ----
mean loss: 196.56
 ---- batch: 070 ----
mean loss: 197.09
 ---- batch: 080 ----
mean loss: 186.54
 ---- batch: 090 ----
mean loss: 196.48
train mean loss: 196.89
epoch train time: 0:00:16.491773
elapsed time: 0:47:42.839391
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 04:05:09.432261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.27
 ---- batch: 020 ----
mean loss: 194.27
 ---- batch: 030 ----
mean loss: 191.11
 ---- batch: 040 ----
mean loss: 197.49
 ---- batch: 050 ----
mean loss: 193.60
 ---- batch: 060 ----
mean loss: 197.40
 ---- batch: 070 ----
mean loss: 202.63
 ---- batch: 080 ----
mean loss: 192.73
 ---- batch: 090 ----
mean loss: 202.18
train mean loss: 195.77
epoch train time: 0:00:16.516553
elapsed time: 0:47:59.357283
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 04:05:25.950298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.23
 ---- batch: 020 ----
mean loss: 192.83
 ---- batch: 030 ----
mean loss: 196.40
 ---- batch: 040 ----
mean loss: 196.83
 ---- batch: 050 ----
mean loss: 193.66
 ---- batch: 060 ----
mean loss: 193.84
 ---- batch: 070 ----
mean loss: 199.36
 ---- batch: 080 ----
mean loss: 195.91
 ---- batch: 090 ----
mean loss: 193.99
train mean loss: 196.28
epoch train time: 0:00:16.500355
elapsed time: 0:48:15.859020
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 04:05:42.452031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.86
 ---- batch: 020 ----
mean loss: 190.19
 ---- batch: 030 ----
mean loss: 198.29
 ---- batch: 040 ----
mean loss: 190.27
 ---- batch: 050 ----
mean loss: 192.86
 ---- batch: 060 ----
mean loss: 199.12
 ---- batch: 070 ----
mean loss: 198.73
 ---- batch: 080 ----
mean loss: 205.47
 ---- batch: 090 ----
mean loss: 191.98
train mean loss: 195.54
epoch train time: 0:00:16.521017
elapsed time: 0:48:32.381724
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 04:05:58.974136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.60
 ---- batch: 020 ----
mean loss: 201.60
 ---- batch: 030 ----
mean loss: 204.41
 ---- batch: 040 ----
mean loss: 188.14
 ---- batch: 050 ----
mean loss: 192.96
 ---- batch: 060 ----
mean loss: 201.85
 ---- batch: 070 ----
mean loss: 194.70
 ---- batch: 080 ----
mean loss: 197.12
 ---- batch: 090 ----
mean loss: 193.01
train mean loss: 196.28
epoch train time: 0:00:16.589822
elapsed time: 0:48:48.972602
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 04:06:15.565358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.40
 ---- batch: 020 ----
mean loss: 191.64
 ---- batch: 030 ----
mean loss: 194.91
 ---- batch: 040 ----
mean loss: 196.36
 ---- batch: 050 ----
mean loss: 196.34
 ---- batch: 060 ----
mean loss: 197.91
 ---- batch: 070 ----
mean loss: 198.03
 ---- batch: 080 ----
mean loss: 188.09
 ---- batch: 090 ----
mean loss: 195.67
train mean loss: 195.32
epoch train time: 0:00:16.543209
elapsed time: 0:49:05.516941
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 04:06:32.109723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.97
 ---- batch: 020 ----
mean loss: 198.86
 ---- batch: 030 ----
mean loss: 193.00
 ---- batch: 040 ----
mean loss: 198.54
 ---- batch: 050 ----
mean loss: 200.67
 ---- batch: 060 ----
mean loss: 194.85
 ---- batch: 070 ----
mean loss: 187.68
 ---- batch: 080 ----
mean loss: 188.33
 ---- batch: 090 ----
mean loss: 201.60
train mean loss: 194.94
epoch train time: 0:00:16.568067
elapsed time: 0:49:22.086208
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 04:06:48.678942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.10
 ---- batch: 020 ----
mean loss: 198.99
 ---- batch: 030 ----
mean loss: 197.16
 ---- batch: 040 ----
mean loss: 189.90
 ---- batch: 050 ----
mean loss: 190.04
 ---- batch: 060 ----
mean loss: 196.74
 ---- batch: 070 ----
mean loss: 200.97
 ---- batch: 080 ----
mean loss: 192.43
 ---- batch: 090 ----
mean loss: 192.83
train mean loss: 194.82
epoch train time: 0:00:16.569249
elapsed time: 0:49:38.656608
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 04:07:05.249369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.31
 ---- batch: 020 ----
mean loss: 197.14
 ---- batch: 030 ----
mean loss: 198.13
 ---- batch: 040 ----
mean loss: 196.20
 ---- batch: 050 ----
mean loss: 192.75
 ---- batch: 060 ----
mean loss: 196.52
 ---- batch: 070 ----
mean loss: 192.09
 ---- batch: 080 ----
mean loss: 185.22
 ---- batch: 090 ----
mean loss: 195.59
train mean loss: 194.58
epoch train time: 0:00:16.577524
elapsed time: 0:49:55.235336
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 04:07:21.828128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.51
 ---- batch: 020 ----
mean loss: 194.43
 ---- batch: 030 ----
mean loss: 196.27
 ---- batch: 040 ----
mean loss: 192.84
 ---- batch: 050 ----
mean loss: 198.67
 ---- batch: 060 ----
mean loss: 193.93
 ---- batch: 070 ----
mean loss: 191.42
 ---- batch: 080 ----
mean loss: 194.68
 ---- batch: 090 ----
mean loss: 196.34
train mean loss: 194.85
epoch train time: 0:00:16.538964
elapsed time: 0:50:11.775505
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 04:07:38.368276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.26
 ---- batch: 020 ----
mean loss: 194.81
 ---- batch: 030 ----
mean loss: 195.96
 ---- batch: 040 ----
mean loss: 198.41
 ---- batch: 050 ----
mean loss: 196.12
 ---- batch: 060 ----
mean loss: 187.02
 ---- batch: 070 ----
mean loss: 191.31
 ---- batch: 080 ----
mean loss: 192.63
 ---- batch: 090 ----
mean loss: 192.57
train mean loss: 194.36
epoch train time: 0:00:16.538110
elapsed time: 0:50:28.314847
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 04:07:54.907602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.98
 ---- batch: 020 ----
mean loss: 201.26
 ---- batch: 030 ----
mean loss: 194.78
 ---- batch: 040 ----
mean loss: 195.52
 ---- batch: 050 ----
mean loss: 191.45
 ---- batch: 060 ----
mean loss: 197.43
 ---- batch: 070 ----
mean loss: 197.65
 ---- batch: 080 ----
mean loss: 194.46
 ---- batch: 090 ----
mean loss: 193.09
train mean loss: 195.84
epoch train time: 0:00:16.556778
elapsed time: 0:50:44.872841
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 04:08:11.465629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.64
 ---- batch: 020 ----
mean loss: 192.33
 ---- batch: 030 ----
mean loss: 197.25
 ---- batch: 040 ----
mean loss: 197.96
 ---- batch: 050 ----
mean loss: 191.00
 ---- batch: 060 ----
mean loss: 198.83
 ---- batch: 070 ----
mean loss: 192.27
 ---- batch: 080 ----
mean loss: 196.61
 ---- batch: 090 ----
mean loss: 186.59
train mean loss: 194.36
epoch train time: 0:00:16.549003
elapsed time: 0:51:01.423111
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 04:08:28.015860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.86
 ---- batch: 020 ----
mean loss: 199.34
 ---- batch: 030 ----
mean loss: 195.20
 ---- batch: 040 ----
mean loss: 198.20
 ---- batch: 050 ----
mean loss: 200.70
 ---- batch: 060 ----
mean loss: 194.87
 ---- batch: 070 ----
mean loss: 192.88
 ---- batch: 080 ----
mean loss: 196.68
 ---- batch: 090 ----
mean loss: 190.16
train mean loss: 195.90
epoch train time: 0:00:16.580935
elapsed time: 0:51:18.005227
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 04:08:44.597997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.09
 ---- batch: 020 ----
mean loss: 195.32
 ---- batch: 030 ----
mean loss: 192.23
 ---- batch: 040 ----
mean loss: 194.75
 ---- batch: 050 ----
mean loss: 193.62
 ---- batch: 060 ----
mean loss: 198.94
 ---- batch: 070 ----
mean loss: 192.74
 ---- batch: 080 ----
mean loss: 190.87
 ---- batch: 090 ----
mean loss: 195.46
train mean loss: 194.68
epoch train time: 0:00:16.528758
elapsed time: 0:51:34.535150
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 04:09:01.127903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.47
 ---- batch: 020 ----
mean loss: 197.03
 ---- batch: 030 ----
mean loss: 192.14
 ---- batch: 040 ----
mean loss: 199.41
 ---- batch: 050 ----
mean loss: 205.38
 ---- batch: 060 ----
mean loss: 195.77
 ---- batch: 070 ----
mean loss: 195.08
 ---- batch: 080 ----
mean loss: 193.73
 ---- batch: 090 ----
mean loss: 197.94
train mean loss: 197.41
epoch train time: 0:00:16.528492
elapsed time: 0:51:51.064864
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 04:09:17.657635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.21
 ---- batch: 020 ----
mean loss: 188.83
 ---- batch: 030 ----
mean loss: 198.66
 ---- batch: 040 ----
mean loss: 196.13
 ---- batch: 050 ----
mean loss: 192.06
 ---- batch: 060 ----
mean loss: 198.39
 ---- batch: 070 ----
mean loss: 203.02
 ---- batch: 080 ----
mean loss: 195.64
 ---- batch: 090 ----
mean loss: 198.40
train mean loss: 195.88
epoch train time: 0:00:16.551396
elapsed time: 0:52:07.617472
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 04:09:34.210234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.35
 ---- batch: 020 ----
mean loss: 199.82
 ---- batch: 030 ----
mean loss: 192.46
 ---- batch: 040 ----
mean loss: 193.67
 ---- batch: 050 ----
mean loss: 196.28
 ---- batch: 060 ----
mean loss: 195.41
 ---- batch: 070 ----
mean loss: 192.95
 ---- batch: 080 ----
mean loss: 194.26
 ---- batch: 090 ----
mean loss: 194.12
train mean loss: 194.68
epoch train time: 0:00:16.561147
elapsed time: 0:52:24.179879
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 04:09:50.772488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.15
 ---- batch: 020 ----
mean loss: 201.63
 ---- batch: 030 ----
mean loss: 190.07
 ---- batch: 040 ----
mean loss: 190.61
 ---- batch: 050 ----
mean loss: 193.97
 ---- batch: 060 ----
mean loss: 194.19
 ---- batch: 070 ----
mean loss: 195.58
 ---- batch: 080 ----
mean loss: 194.82
 ---- batch: 090 ----
mean loss: 193.19
train mean loss: 194.68
epoch train time: 0:00:16.576190
elapsed time: 0:52:40.757076
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 04:10:07.349895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.39
 ---- batch: 020 ----
mean loss: 195.15
 ---- batch: 030 ----
mean loss: 206.14
 ---- batch: 040 ----
mean loss: 192.09
 ---- batch: 050 ----
mean loss: 192.64
 ---- batch: 060 ----
mean loss: 182.50
 ---- batch: 070 ----
mean loss: 191.25
 ---- batch: 080 ----
mean loss: 200.72
 ---- batch: 090 ----
mean loss: 202.65
train mean loss: 195.34
epoch train time: 0:00:16.552973
elapsed time: 0:52:57.311321
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 04:10:23.904078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.74
 ---- batch: 020 ----
mean loss: 190.73
 ---- batch: 030 ----
mean loss: 196.37
 ---- batch: 040 ----
mean loss: 207.40
 ---- batch: 050 ----
mean loss: 205.00
 ---- batch: 060 ----
mean loss: 196.12
 ---- batch: 070 ----
mean loss: 195.36
 ---- batch: 080 ----
mean loss: 198.18
 ---- batch: 090 ----
mean loss: 185.55
train mean loss: 195.95
epoch train time: 0:00:16.570409
elapsed time: 0:53:13.882966
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 04:10:40.475751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.08
 ---- batch: 020 ----
mean loss: 196.18
 ---- batch: 030 ----
mean loss: 197.90
 ---- batch: 040 ----
mean loss: 187.92
 ---- batch: 050 ----
mean loss: 193.27
 ---- batch: 060 ----
mean loss: 197.84
 ---- batch: 070 ----
mean loss: 192.31
 ---- batch: 080 ----
mean loss: 189.28
 ---- batch: 090 ----
mean loss: 194.93
train mean loss: 193.90
epoch train time: 0:00:16.547983
elapsed time: 0:53:30.432157
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 04:10:57.024976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.55
 ---- batch: 020 ----
mean loss: 195.94
 ---- batch: 030 ----
mean loss: 189.77
 ---- batch: 040 ----
mean loss: 192.62
 ---- batch: 050 ----
mean loss: 195.42
 ---- batch: 060 ----
mean loss: 197.31
 ---- batch: 070 ----
mean loss: 186.08
 ---- batch: 080 ----
mean loss: 188.01
 ---- batch: 090 ----
mean loss: 192.29
train mean loss: 192.45
epoch train time: 0:00:16.559427
elapsed time: 0:53:46.992796
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 04:11:13.585640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.93
 ---- batch: 020 ----
mean loss: 188.31
 ---- batch: 030 ----
mean loss: 193.37
 ---- batch: 040 ----
mean loss: 191.26
 ---- batch: 050 ----
mean loss: 188.88
 ---- batch: 060 ----
mean loss: 183.74
 ---- batch: 070 ----
mean loss: 201.56
 ---- batch: 080 ----
mean loss: 196.52
 ---- batch: 090 ----
mean loss: 199.96
train mean loss: 193.24
epoch train time: 0:00:16.566125
elapsed time: 0:54:03.560205
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 04:11:30.152926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.08
 ---- batch: 020 ----
mean loss: 193.86
 ---- batch: 030 ----
mean loss: 192.03
 ---- batch: 040 ----
mean loss: 193.86
 ---- batch: 050 ----
mean loss: 187.01
 ---- batch: 060 ----
mean loss: 196.44
 ---- batch: 070 ----
mean loss: 193.72
 ---- batch: 080 ----
mean loss: 193.87
 ---- batch: 090 ----
mean loss: 193.72
train mean loss: 193.55
epoch train time: 0:00:16.548465
elapsed time: 0:54:20.109917
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 04:11:46.702757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.28
 ---- batch: 020 ----
mean loss: 195.90
 ---- batch: 030 ----
mean loss: 198.66
 ---- batch: 040 ----
mean loss: 190.26
 ---- batch: 050 ----
mean loss: 190.71
 ---- batch: 060 ----
mean loss: 195.92
 ---- batch: 070 ----
mean loss: 196.70
 ---- batch: 080 ----
mean loss: 191.24
 ---- batch: 090 ----
mean loss: 189.65
train mean loss: 193.13
epoch train time: 0:00:16.588278
elapsed time: 0:54:36.699413
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 04:12:03.292211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.12
 ---- batch: 020 ----
mean loss: 196.62
 ---- batch: 030 ----
mean loss: 190.62
 ---- batch: 040 ----
mean loss: 191.55
 ---- batch: 050 ----
mean loss: 191.68
 ---- batch: 060 ----
mean loss: 190.75
 ---- batch: 070 ----
mean loss: 191.51
 ---- batch: 080 ----
mean loss: 187.82
 ---- batch: 090 ----
mean loss: 195.79
train mean loss: 192.82
epoch train time: 0:00:16.587203
elapsed time: 0:54:53.287865
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 04:12:19.880574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.77
 ---- batch: 020 ----
mean loss: 194.30
 ---- batch: 030 ----
mean loss: 193.91
 ---- batch: 040 ----
mean loss: 190.29
 ---- batch: 050 ----
mean loss: 193.19
 ---- batch: 060 ----
mean loss: 198.91
 ---- batch: 070 ----
mean loss: 186.30
 ---- batch: 080 ----
mean loss: 192.01
 ---- batch: 090 ----
mean loss: 198.92
train mean loss: 193.10
epoch train time: 0:00:16.501498
elapsed time: 0:55:09.790455
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 04:12:36.383169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.44
 ---- batch: 020 ----
mean loss: 187.14
 ---- batch: 030 ----
mean loss: 193.41
 ---- batch: 040 ----
mean loss: 200.86
 ---- batch: 050 ----
mean loss: 200.27
 ---- batch: 060 ----
mean loss: 193.89
 ---- batch: 070 ----
mean loss: 195.38
 ---- batch: 080 ----
mean loss: 196.98
 ---- batch: 090 ----
mean loss: 187.83
train mean loss: 193.65
epoch train time: 0:00:16.487521
elapsed time: 0:55:26.279098
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 04:12:52.871799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.82
 ---- batch: 020 ----
mean loss: 184.44
 ---- batch: 030 ----
mean loss: 195.94
 ---- batch: 040 ----
mean loss: 190.64
 ---- batch: 050 ----
mean loss: 192.00
 ---- batch: 060 ----
mean loss: 196.94
 ---- batch: 070 ----
mean loss: 192.22
 ---- batch: 080 ----
mean loss: 193.59
 ---- batch: 090 ----
mean loss: 189.57
train mean loss: 192.02
epoch train time: 0:00:16.521569
elapsed time: 0:55:42.801813
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 04:13:09.394630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.21
 ---- batch: 020 ----
mean loss: 193.28
 ---- batch: 030 ----
mean loss: 184.22
 ---- batch: 040 ----
mean loss: 192.88
 ---- batch: 050 ----
mean loss: 196.43
 ---- batch: 060 ----
mean loss: 195.42
 ---- batch: 070 ----
mean loss: 199.87
 ---- batch: 080 ----
mean loss: 197.18
 ---- batch: 090 ----
mean loss: 200.55
train mean loss: 194.58
epoch train time: 0:00:16.513375
elapsed time: 0:55:59.316459
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 04:13:25.909350
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.27
 ---- batch: 020 ----
mean loss: 190.49
 ---- batch: 030 ----
mean loss: 188.93
 ---- batch: 040 ----
mean loss: 197.07
 ---- batch: 050 ----
mean loss: 187.73
 ---- batch: 060 ----
mean loss: 184.95
 ---- batch: 070 ----
mean loss: 190.68
 ---- batch: 080 ----
mean loss: 193.02
 ---- batch: 090 ----
mean loss: 194.40
train mean loss: 192.26
epoch train time: 0:00:16.568108
elapsed time: 0:56:15.886587
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 04:13:42.478965
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.72
 ---- batch: 020 ----
mean loss: 186.09
 ---- batch: 030 ----
mean loss: 190.84
 ---- batch: 040 ----
mean loss: 183.14
 ---- batch: 050 ----
mean loss: 195.17
 ---- batch: 060 ----
mean loss: 190.78
 ---- batch: 070 ----
mean loss: 186.53
 ---- batch: 080 ----
mean loss: 193.61
 ---- batch: 090 ----
mean loss: 188.08
train mean loss: 190.19
epoch train time: 0:00:16.508885
elapsed time: 0:56:32.396286
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 04:13:58.989055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.97
 ---- batch: 020 ----
mean loss: 193.96
 ---- batch: 030 ----
mean loss: 185.35
 ---- batch: 040 ----
mean loss: 192.30
 ---- batch: 050 ----
mean loss: 193.01
 ---- batch: 060 ----
mean loss: 182.15
 ---- batch: 070 ----
mean loss: 195.36
 ---- batch: 080 ----
mean loss: 188.53
 ---- batch: 090 ----
mean loss: 188.44
train mean loss: 189.79
epoch train time: 0:00:16.555387
elapsed time: 0:56:48.952867
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 04:14:15.545627
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.76
 ---- batch: 020 ----
mean loss: 190.71
 ---- batch: 030 ----
mean loss: 196.01
 ---- batch: 040 ----
mean loss: 186.50
 ---- batch: 050 ----
mean loss: 184.77
 ---- batch: 060 ----
mean loss: 188.76
 ---- batch: 070 ----
mean loss: 188.96
 ---- batch: 080 ----
mean loss: 201.03
 ---- batch: 090 ----
mean loss: 186.87
train mean loss: 189.71
epoch train time: 0:00:16.529978
elapsed time: 0:57:05.484028
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 04:14:32.076820
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.79
 ---- batch: 020 ----
mean loss: 191.80
 ---- batch: 030 ----
mean loss: 197.86
 ---- batch: 040 ----
mean loss: 178.84
 ---- batch: 050 ----
mean loss: 192.60
 ---- batch: 060 ----
mean loss: 191.73
 ---- batch: 070 ----
mean loss: 184.75
 ---- batch: 080 ----
mean loss: 199.07
 ---- batch: 090 ----
mean loss: 185.85
train mean loss: 189.82
epoch train time: 0:00:16.551503
elapsed time: 0:57:22.036708
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 04:14:48.629459
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.53
 ---- batch: 020 ----
mean loss: 186.03
 ---- batch: 030 ----
mean loss: 186.62
 ---- batch: 040 ----
mean loss: 196.11
 ---- batch: 050 ----
mean loss: 188.84
 ---- batch: 060 ----
mean loss: 190.80
 ---- batch: 070 ----
mean loss: 197.28
 ---- batch: 080 ----
mean loss: 190.60
 ---- batch: 090 ----
mean loss: 190.83
train mean loss: 190.00
epoch train time: 0:00:16.598259
elapsed time: 0:57:38.636161
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 04:15:05.228913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.24
 ---- batch: 020 ----
mean loss: 184.51
 ---- batch: 030 ----
mean loss: 191.94
 ---- batch: 040 ----
mean loss: 194.71
 ---- batch: 050 ----
mean loss: 191.65
 ---- batch: 060 ----
mean loss: 186.30
 ---- batch: 070 ----
mean loss: 188.76
 ---- batch: 080 ----
mean loss: 186.70
 ---- batch: 090 ----
mean loss: 191.44
train mean loss: 190.19
epoch train time: 0:00:16.567409
elapsed time: 0:57:55.204843
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 04:15:21.797656
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.83
 ---- batch: 020 ----
mean loss: 190.99
 ---- batch: 030 ----
mean loss: 190.25
 ---- batch: 040 ----
mean loss: 198.20
 ---- batch: 050 ----
mean loss: 190.78
 ---- batch: 060 ----
mean loss: 186.72
 ---- batch: 070 ----
mean loss: 188.10
 ---- batch: 080 ----
mean loss: 193.34
 ---- batch: 090 ----
mean loss: 184.57
train mean loss: 189.82
epoch train time: 0:00:16.596940
elapsed time: 0:58:11.803049
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 04:15:38.395864
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.37
 ---- batch: 020 ----
mean loss: 196.14
 ---- batch: 030 ----
mean loss: 191.91
 ---- batch: 040 ----
mean loss: 180.87
 ---- batch: 050 ----
mean loss: 197.27
 ---- batch: 060 ----
mean loss: 195.54
 ---- batch: 070 ----
mean loss: 185.81
 ---- batch: 080 ----
mean loss: 188.09
 ---- batch: 090 ----
mean loss: 186.36
train mean loss: 189.92
epoch train time: 0:00:16.611405
elapsed time: 0:58:28.415855
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 04:15:55.008618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.78
 ---- batch: 020 ----
mean loss: 191.24
 ---- batch: 030 ----
mean loss: 192.19
 ---- batch: 040 ----
mean loss: 188.27
 ---- batch: 050 ----
mean loss: 185.86
 ---- batch: 060 ----
mean loss: 195.77
 ---- batch: 070 ----
mean loss: 189.22
 ---- batch: 080 ----
mean loss: 188.85
 ---- batch: 090 ----
mean loss: 187.34
train mean loss: 189.71
epoch train time: 0:00:16.597511
elapsed time: 0:58:45.014517
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 04:16:11.607366
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.94
 ---- batch: 020 ----
mean loss: 184.51
 ---- batch: 030 ----
mean loss: 191.28
 ---- batch: 040 ----
mean loss: 188.66
 ---- batch: 050 ----
mean loss: 187.54
 ---- batch: 060 ----
mean loss: 186.93
 ---- batch: 070 ----
mean loss: 195.84
 ---- batch: 080 ----
mean loss: 191.74
 ---- batch: 090 ----
mean loss: 193.29
train mean loss: 189.85
epoch train time: 0:00:16.650451
elapsed time: 0:59:01.666237
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 04:16:28.259142
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.94
 ---- batch: 020 ----
mean loss: 189.28
 ---- batch: 030 ----
mean loss: 188.88
 ---- batch: 040 ----
mean loss: 196.41
 ---- batch: 050 ----
mean loss: 187.18
 ---- batch: 060 ----
mean loss: 183.48
 ---- batch: 070 ----
mean loss: 191.81
 ---- batch: 080 ----
mean loss: 192.04
 ---- batch: 090 ----
mean loss: 188.12
train mean loss: 189.77
epoch train time: 0:00:16.579292
elapsed time: 0:59:18.246851
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 04:16:44.839591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.36
 ---- batch: 020 ----
mean loss: 193.59
 ---- batch: 030 ----
mean loss: 186.07
 ---- batch: 040 ----
mean loss: 192.56
 ---- batch: 050 ----
mean loss: 189.50
 ---- batch: 060 ----
mean loss: 194.68
 ---- batch: 070 ----
mean loss: 186.96
 ---- batch: 080 ----
mean loss: 188.32
 ---- batch: 090 ----
mean loss: 189.99
train mean loss: 189.73
epoch train time: 0:00:16.565043
elapsed time: 0:59:34.813070
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 04:17:01.405975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.58
 ---- batch: 020 ----
mean loss: 178.03
 ---- batch: 030 ----
mean loss: 186.67
 ---- batch: 040 ----
mean loss: 190.72
 ---- batch: 050 ----
mean loss: 188.89
 ---- batch: 060 ----
mean loss: 193.82
 ---- batch: 070 ----
mean loss: 196.00
 ---- batch: 080 ----
mean loss: 199.35
 ---- batch: 090 ----
mean loss: 190.15
train mean loss: 190.18
epoch train time: 0:00:16.613967
elapsed time: 0:59:51.428681
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 04:17:18.021442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.83
 ---- batch: 020 ----
mean loss: 191.61
 ---- batch: 030 ----
mean loss: 186.55
 ---- batch: 040 ----
mean loss: 187.78
 ---- batch: 050 ----
mean loss: 189.37
 ---- batch: 060 ----
mean loss: 190.65
 ---- batch: 070 ----
mean loss: 187.50
 ---- batch: 080 ----
mean loss: 191.96
 ---- batch: 090 ----
mean loss: 188.60
train mean loss: 190.00
epoch train time: 0:00:16.710845
elapsed time: 1:00:08.140717
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 04:17:34.733506
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.35
 ---- batch: 020 ----
mean loss: 186.33
 ---- batch: 030 ----
mean loss: 195.45
 ---- batch: 040 ----
mean loss: 189.78
 ---- batch: 050 ----
mean loss: 188.10
 ---- batch: 060 ----
mean loss: 190.57
 ---- batch: 070 ----
mean loss: 186.86
 ---- batch: 080 ----
mean loss: 192.89
 ---- batch: 090 ----
mean loss: 186.41
train mean loss: 189.80
epoch train time: 0:00:16.578053
elapsed time: 1:00:24.719985
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 04:17:51.312719
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.14
 ---- batch: 020 ----
mean loss: 188.96
 ---- batch: 030 ----
mean loss: 192.29
 ---- batch: 040 ----
mean loss: 189.29
 ---- batch: 050 ----
mean loss: 195.51
 ---- batch: 060 ----
mean loss: 187.68
 ---- batch: 070 ----
mean loss: 185.19
 ---- batch: 080 ----
mean loss: 194.91
 ---- batch: 090 ----
mean loss: 188.26
train mean loss: 190.03
epoch train time: 0:00:16.569989
elapsed time: 1:00:41.291133
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 04:18:07.883924
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.70
 ---- batch: 020 ----
mean loss: 192.11
 ---- batch: 030 ----
mean loss: 193.23
 ---- batch: 040 ----
mean loss: 190.95
 ---- batch: 050 ----
mean loss: 188.64
 ---- batch: 060 ----
mean loss: 191.72
 ---- batch: 070 ----
mean loss: 185.03
 ---- batch: 080 ----
mean loss: 192.67
 ---- batch: 090 ----
mean loss: 190.78
train mean loss: 189.80
epoch train time: 0:00:16.515716
elapsed time: 1:00:57.808024
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 04:18:24.400810
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.30
 ---- batch: 020 ----
mean loss: 189.69
 ---- batch: 030 ----
mean loss: 194.70
 ---- batch: 040 ----
mean loss: 189.85
 ---- batch: 050 ----
mean loss: 194.73
 ---- batch: 060 ----
mean loss: 184.90
 ---- batch: 070 ----
mean loss: 183.04
 ---- batch: 080 ----
mean loss: 194.95
 ---- batch: 090 ----
mean loss: 185.46
train mean loss: 189.35
epoch train time: 0:00:16.513051
elapsed time: 1:01:14.322262
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 04:18:40.915031
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.40
 ---- batch: 020 ----
mean loss: 185.16
 ---- batch: 030 ----
mean loss: 189.34
 ---- batch: 040 ----
mean loss: 189.97
 ---- batch: 050 ----
mean loss: 194.12
 ---- batch: 060 ----
mean loss: 187.64
 ---- batch: 070 ----
mean loss: 188.00
 ---- batch: 080 ----
mean loss: 192.52
 ---- batch: 090 ----
mean loss: 189.07
train mean loss: 189.55
epoch train time: 0:00:16.548021
elapsed time: 1:01:30.871470
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 04:18:57.464338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.22
 ---- batch: 020 ----
mean loss: 188.00
 ---- batch: 030 ----
mean loss: 186.53
 ---- batch: 040 ----
mean loss: 190.24
 ---- batch: 050 ----
mean loss: 188.94
 ---- batch: 060 ----
mean loss: 197.17
 ---- batch: 070 ----
mean loss: 187.72
 ---- batch: 080 ----
mean loss: 194.62
 ---- batch: 090 ----
mean loss: 184.27
train mean loss: 189.41
epoch train time: 0:00:16.590467
elapsed time: 1:01:47.463375
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 04:19:14.056105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.93
 ---- batch: 020 ----
mean loss: 194.98
 ---- batch: 030 ----
mean loss: 188.24
 ---- batch: 040 ----
mean loss: 186.21
 ---- batch: 050 ----
mean loss: 185.49
 ---- batch: 060 ----
mean loss: 191.04
 ---- batch: 070 ----
mean loss: 187.72
 ---- batch: 080 ----
mean loss: 195.14
 ---- batch: 090 ----
mean loss: 191.16
train mean loss: 189.60
epoch train time: 0:00:16.598600
elapsed time: 1:02:04.063166
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 04:19:30.655974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.90
 ---- batch: 020 ----
mean loss: 190.08
 ---- batch: 030 ----
mean loss: 188.64
 ---- batch: 040 ----
mean loss: 186.41
 ---- batch: 050 ----
mean loss: 191.80
 ---- batch: 060 ----
mean loss: 191.72
 ---- batch: 070 ----
mean loss: 189.76
 ---- batch: 080 ----
mean loss: 182.61
 ---- batch: 090 ----
mean loss: 195.99
train mean loss: 189.42
epoch train time: 0:00:16.566351
elapsed time: 1:02:20.630732
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 04:19:47.223513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.66
 ---- batch: 020 ----
mean loss: 186.28
 ---- batch: 030 ----
mean loss: 187.92
 ---- batch: 040 ----
mean loss: 186.02
 ---- batch: 050 ----
mean loss: 187.34
 ---- batch: 060 ----
mean loss: 186.60
 ---- batch: 070 ----
mean loss: 190.46
 ---- batch: 080 ----
mean loss: 203.99
 ---- batch: 090 ----
mean loss: 193.50
train mean loss: 190.10
epoch train time: 0:00:16.590594
elapsed time: 1:02:37.222568
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 04:20:03.815330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.72
 ---- batch: 020 ----
mean loss: 193.57
 ---- batch: 030 ----
mean loss: 187.56
 ---- batch: 040 ----
mean loss: 192.74
 ---- batch: 050 ----
mean loss: 188.72
 ---- batch: 060 ----
mean loss: 187.66
 ---- batch: 070 ----
mean loss: 184.59
 ---- batch: 080 ----
mean loss: 186.20
 ---- batch: 090 ----
mean loss: 192.21
train mean loss: 189.49
epoch train time: 0:00:16.534835
elapsed time: 1:02:53.758566
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 04:20:20.351284
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.13
 ---- batch: 020 ----
mean loss: 194.66
 ---- batch: 030 ----
mean loss: 190.98
 ---- batch: 040 ----
mean loss: 184.80
 ---- batch: 050 ----
mean loss: 185.58
 ---- batch: 060 ----
mean loss: 187.71
 ---- batch: 070 ----
mean loss: 183.83
 ---- batch: 080 ----
mean loss: 194.09
 ---- batch: 090 ----
mean loss: 196.65
train mean loss: 189.68
epoch train time: 0:00:16.570944
elapsed time: 1:03:10.330635
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 04:20:36.923292
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.66
 ---- batch: 020 ----
mean loss: 192.47
 ---- batch: 030 ----
mean loss: 188.25
 ---- batch: 040 ----
mean loss: 194.53
 ---- batch: 050 ----
mean loss: 196.38
 ---- batch: 060 ----
mean loss: 186.60
 ---- batch: 070 ----
mean loss: 189.03
 ---- batch: 080 ----
mean loss: 188.24
 ---- batch: 090 ----
mean loss: 187.09
train mean loss: 189.54
epoch train time: 0:00:16.567188
elapsed time: 1:03:26.898857
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 04:20:53.491616
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.36
 ---- batch: 020 ----
mean loss: 184.28
 ---- batch: 030 ----
mean loss: 191.87
 ---- batch: 040 ----
mean loss: 191.75
 ---- batch: 050 ----
mean loss: 185.17
 ---- batch: 060 ----
mean loss: 189.86
 ---- batch: 070 ----
mean loss: 192.92
 ---- batch: 080 ----
mean loss: 185.59
 ---- batch: 090 ----
mean loss: 190.80
train mean loss: 189.35
epoch train time: 0:00:16.580894
elapsed time: 1:03:43.480948
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 04:21:10.073786
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.54
 ---- batch: 020 ----
mean loss: 188.09
 ---- batch: 030 ----
mean loss: 188.80
 ---- batch: 040 ----
mean loss: 196.08
 ---- batch: 050 ----
mean loss: 192.88
 ---- batch: 060 ----
mean loss: 187.58
 ---- batch: 070 ----
mean loss: 189.28
 ---- batch: 080 ----
mean loss: 182.67
 ---- batch: 090 ----
mean loss: 193.53
train mean loss: 189.55
epoch train time: 0:00:16.603384
elapsed time: 1:04:00.085746
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 04:21:26.678527
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.40
 ---- batch: 020 ----
mean loss: 192.71
 ---- batch: 030 ----
mean loss: 192.12
 ---- batch: 040 ----
mean loss: 187.05
 ---- batch: 050 ----
mean loss: 188.81
 ---- batch: 060 ----
mean loss: 186.67
 ---- batch: 070 ----
mean loss: 189.06
 ---- batch: 080 ----
mean loss: 189.91
 ---- batch: 090 ----
mean loss: 188.42
train mean loss: 189.80
epoch train time: 0:00:16.639787
elapsed time: 1:04:16.726767
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 04:21:43.319533
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.12
 ---- batch: 020 ----
mean loss: 189.98
 ---- batch: 030 ----
mean loss: 190.22
 ---- batch: 040 ----
mean loss: 191.41
 ---- batch: 050 ----
mean loss: 187.71
 ---- batch: 060 ----
mean loss: 182.73
 ---- batch: 070 ----
mean loss: 187.32
 ---- batch: 080 ----
mean loss: 192.90
 ---- batch: 090 ----
mean loss: 192.68
train mean loss: 189.69
epoch train time: 0:00:16.577368
elapsed time: 1:04:33.305393
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 04:21:59.898158
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.33
 ---- batch: 020 ----
mean loss: 189.14
 ---- batch: 030 ----
mean loss: 191.34
 ---- batch: 040 ----
mean loss: 185.40
 ---- batch: 050 ----
mean loss: 184.69
 ---- batch: 060 ----
mean loss: 192.90
 ---- batch: 070 ----
mean loss: 191.02
 ---- batch: 080 ----
mean loss: 185.55
 ---- batch: 090 ----
mean loss: 191.27
train mean loss: 189.21
epoch train time: 0:00:16.528329
elapsed time: 1:04:49.834890
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 04:22:16.427822
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.36
 ---- batch: 020 ----
mean loss: 188.73
 ---- batch: 030 ----
mean loss: 186.69
 ---- batch: 040 ----
mean loss: 186.84
 ---- batch: 050 ----
mean loss: 191.85
 ---- batch: 060 ----
mean loss: 189.77
 ---- batch: 070 ----
mean loss: 184.13
 ---- batch: 080 ----
mean loss: 190.21
 ---- batch: 090 ----
mean loss: 189.64
train mean loss: 189.87
epoch train time: 0:00:16.591952
elapsed time: 1:05:06.429154
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 04:22:33.021434
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.65
 ---- batch: 020 ----
mean loss: 192.63
 ---- batch: 030 ----
mean loss: 186.10
 ---- batch: 040 ----
mean loss: 193.20
 ---- batch: 050 ----
mean loss: 191.35
 ---- batch: 060 ----
mean loss: 188.54
 ---- batch: 070 ----
mean loss: 192.79
 ---- batch: 080 ----
mean loss: 190.12
 ---- batch: 090 ----
mean loss: 186.41
train mean loss: 189.47
epoch train time: 0:00:16.610454
elapsed time: 1:05:23.040334
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 04:22:49.633107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.09
 ---- batch: 020 ----
mean loss: 190.44
 ---- batch: 030 ----
mean loss: 185.47
 ---- batch: 040 ----
mean loss: 185.12
 ---- batch: 050 ----
mean loss: 193.04
 ---- batch: 060 ----
mean loss: 193.10
 ---- batch: 070 ----
mean loss: 187.70
 ---- batch: 080 ----
mean loss: 191.00
 ---- batch: 090 ----
mean loss: 188.91
train mean loss: 189.51
epoch train time: 0:00:16.521509
elapsed time: 1:05:39.563041
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 04:23:06.155810
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.10
 ---- batch: 020 ----
mean loss: 189.27
 ---- batch: 030 ----
mean loss: 190.97
 ---- batch: 040 ----
mean loss: 192.83
 ---- batch: 050 ----
mean loss: 184.36
 ---- batch: 060 ----
mean loss: 183.49
 ---- batch: 070 ----
mean loss: 194.19
 ---- batch: 080 ----
mean loss: 191.14
 ---- batch: 090 ----
mean loss: 191.55
train mean loss: 189.31
epoch train time: 0:00:16.560682
elapsed time: 1:05:56.125039
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 04:23:22.717834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.13
 ---- batch: 020 ----
mean loss: 184.57
 ---- batch: 030 ----
mean loss: 198.06
 ---- batch: 040 ----
mean loss: 189.44
 ---- batch: 050 ----
mean loss: 189.52
 ---- batch: 060 ----
mean loss: 184.87
 ---- batch: 070 ----
mean loss: 189.86
 ---- batch: 080 ----
mean loss: 196.80
 ---- batch: 090 ----
mean loss: 188.78
train mean loss: 190.19
epoch train time: 0:00:16.592286
elapsed time: 1:06:12.718556
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 04:23:39.311309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.80
 ---- batch: 020 ----
mean loss: 188.38
 ---- batch: 030 ----
mean loss: 190.48
 ---- batch: 040 ----
mean loss: 188.31
 ---- batch: 050 ----
mean loss: 190.55
 ---- batch: 060 ----
mean loss: 185.56
 ---- batch: 070 ----
mean loss: 187.88
 ---- batch: 080 ----
mean loss: 187.14
 ---- batch: 090 ----
mean loss: 191.07
train mean loss: 189.27
epoch train time: 0:00:16.587011
elapsed time: 1:06:29.306747
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 04:23:55.899448
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.44
 ---- batch: 020 ----
mean loss: 191.83
 ---- batch: 030 ----
mean loss: 186.76
 ---- batch: 040 ----
mean loss: 182.46
 ---- batch: 050 ----
mean loss: 191.02
 ---- batch: 060 ----
mean loss: 190.56
 ---- batch: 070 ----
mean loss: 187.11
 ---- batch: 080 ----
mean loss: 187.71
 ---- batch: 090 ----
mean loss: 193.21
train mean loss: 189.29
epoch train time: 0:00:16.563095
elapsed time: 1:06:45.871053
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 04:24:12.463789
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.09
 ---- batch: 020 ----
mean loss: 192.18
 ---- batch: 030 ----
mean loss: 191.35
 ---- batch: 040 ----
mean loss: 182.40
 ---- batch: 050 ----
mean loss: 188.92
 ---- batch: 060 ----
mean loss: 186.80
 ---- batch: 070 ----
mean loss: 193.42
 ---- batch: 080 ----
mean loss: 189.49
 ---- batch: 090 ----
mean loss: 184.52
train mean loss: 189.43
epoch train time: 0:00:16.544883
elapsed time: 1:07:02.417086
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 04:24:29.009888
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.56
 ---- batch: 020 ----
mean loss: 185.50
 ---- batch: 030 ----
mean loss: 191.37
 ---- batch: 040 ----
mean loss: 186.32
 ---- batch: 050 ----
mean loss: 188.19
 ---- batch: 060 ----
mean loss: 182.82
 ---- batch: 070 ----
mean loss: 190.20
 ---- batch: 080 ----
mean loss: 184.63
 ---- batch: 090 ----
mean loss: 195.56
train mean loss: 189.41
epoch train time: 0:00:16.563824
elapsed time: 1:07:18.982140
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 04:24:45.574901
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.78
 ---- batch: 020 ----
mean loss: 185.72
 ---- batch: 030 ----
mean loss: 191.99
 ---- batch: 040 ----
mean loss: 191.25
 ---- batch: 050 ----
mean loss: 184.69
 ---- batch: 060 ----
mean loss: 186.91
 ---- batch: 070 ----
mean loss: 186.56
 ---- batch: 080 ----
mean loss: 187.76
 ---- batch: 090 ----
mean loss: 193.14
train mean loss: 189.20
epoch train time: 0:00:16.519608
elapsed time: 1:07:35.502914
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 04:25:02.095669
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.87
 ---- batch: 020 ----
mean loss: 190.74
 ---- batch: 030 ----
mean loss: 190.88
 ---- batch: 040 ----
mean loss: 186.05
 ---- batch: 050 ----
mean loss: 184.83
 ---- batch: 060 ----
mean loss: 192.46
 ---- batch: 070 ----
mean loss: 195.06
 ---- batch: 080 ----
mean loss: 182.30
 ---- batch: 090 ----
mean loss: 195.50
train mean loss: 189.64
epoch train time: 0:00:16.556991
elapsed time: 1:07:52.061118
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 04:25:18.653901
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.79
 ---- batch: 020 ----
mean loss: 199.22
 ---- batch: 030 ----
mean loss: 188.91
 ---- batch: 040 ----
mean loss: 182.59
 ---- batch: 050 ----
mean loss: 191.39
 ---- batch: 060 ----
mean loss: 187.21
 ---- batch: 070 ----
mean loss: 191.17
 ---- batch: 080 ----
mean loss: 183.07
 ---- batch: 090 ----
mean loss: 191.46
train mean loss: 189.56
epoch train time: 0:00:16.595390
elapsed time: 1:08:08.657711
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 04:25:35.250462
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.39
 ---- batch: 020 ----
mean loss: 190.48
 ---- batch: 030 ----
mean loss: 185.78
 ---- batch: 040 ----
mean loss: 188.91
 ---- batch: 050 ----
mean loss: 195.15
 ---- batch: 060 ----
mean loss: 190.76
 ---- batch: 070 ----
mean loss: 186.63
 ---- batch: 080 ----
mean loss: 191.96
 ---- batch: 090 ----
mean loss: 193.69
train mean loss: 188.90
epoch train time: 0:00:16.548432
elapsed time: 1:08:25.207337
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 04:25:51.800128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.96
 ---- batch: 020 ----
mean loss: 188.98
 ---- batch: 030 ----
mean loss: 185.51
 ---- batch: 040 ----
mean loss: 193.75
 ---- batch: 050 ----
mean loss: 186.81
 ---- batch: 060 ----
mean loss: 184.76
 ---- batch: 070 ----
mean loss: 189.25
 ---- batch: 080 ----
mean loss: 184.63
 ---- batch: 090 ----
mean loss: 190.87
train mean loss: 189.23
epoch train time: 0:00:16.545813
elapsed time: 1:08:41.754499
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 04:26:08.347280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.55
 ---- batch: 020 ----
mean loss: 188.45
 ---- batch: 030 ----
mean loss: 187.63
 ---- batch: 040 ----
mean loss: 194.32
 ---- batch: 050 ----
mean loss: 192.45
 ---- batch: 060 ----
mean loss: 193.02
 ---- batch: 070 ----
mean loss: 183.14
 ---- batch: 080 ----
mean loss: 181.39
 ---- batch: 090 ----
mean loss: 193.57
train mean loss: 188.81
epoch train time: 0:00:16.581020
elapsed time: 1:08:58.336716
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 04:26:24.929436
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.87
 ---- batch: 020 ----
mean loss: 188.07
 ---- batch: 030 ----
mean loss: 185.06
 ---- batch: 040 ----
mean loss: 193.21
 ---- batch: 050 ----
mean loss: 184.14
 ---- batch: 060 ----
mean loss: 192.75
 ---- batch: 070 ----
mean loss: 203.26
 ---- batch: 080 ----
mean loss: 183.33
 ---- batch: 090 ----
mean loss: 186.74
train mean loss: 189.15
epoch train time: 0:00:16.641210
elapsed time: 1:09:14.979030
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 04:26:41.571740
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.47
 ---- batch: 020 ----
mean loss: 189.60
 ---- batch: 030 ----
mean loss: 188.81
 ---- batch: 040 ----
mean loss: 180.17
 ---- batch: 050 ----
mean loss: 195.48
 ---- batch: 060 ----
mean loss: 193.66
 ---- batch: 070 ----
mean loss: 188.38
 ---- batch: 080 ----
mean loss: 190.19
 ---- batch: 090 ----
mean loss: 190.62
train mean loss: 189.08
epoch train time: 0:00:16.529119
elapsed time: 1:09:31.519840
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_7/checkpoint.pth.tar
**** end time: 2019-09-25 04:26:58.112052 ****
