Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_4', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 29537
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 03:58:38.879071 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 03:58:38.896641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3329.72
 ---- batch: 020 ----
mean loss: 1884.44
 ---- batch: 030 ----
mean loss: 1590.41
 ---- batch: 040 ----
mean loss: 1431.62
 ---- batch: 050 ----
mean loss: 1357.65
 ---- batch: 060 ----
mean loss: 1289.46
 ---- batch: 070 ----
mean loss: 1232.41
 ---- batch: 080 ----
mean loss: 1215.43
 ---- batch: 090 ----
mean loss: 1158.39
 ---- batch: 100 ----
mean loss: 1142.95
 ---- batch: 110 ----
mean loss: 1094.00
train mean loss: 1509.49
epoch train time: 0:00:45.266634
elapsed time: 0:00:45.292511
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 03:59:24.171627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1112.50
 ---- batch: 020 ----
mean loss: 1086.40
 ---- batch: 030 ----
mean loss: 1063.59
 ---- batch: 040 ----
mean loss: 1061.53
 ---- batch: 050 ----
mean loss: 1060.49
 ---- batch: 060 ----
mean loss: 1072.52
 ---- batch: 070 ----
mean loss: 1091.51
 ---- batch: 080 ----
mean loss: 1043.19
 ---- batch: 090 ----
mean loss: 1072.34
 ---- batch: 100 ----
mean loss: 1058.07
 ---- batch: 110 ----
mean loss: 1048.43
train mean loss: 1068.57
epoch train time: 0:00:15.402625
elapsed time: 0:01:00.695681
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 03:59:39.575253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1037.13
 ---- batch: 020 ----
mean loss: 1046.68
 ---- batch: 030 ----
mean loss: 1020.09
 ---- batch: 040 ----
mean loss: 1036.48
 ---- batch: 050 ----
mean loss: 1019.77
 ---- batch: 060 ----
mean loss: 1004.18
 ---- batch: 070 ----
mean loss: 1055.50
 ---- batch: 080 ----
mean loss: 1035.37
 ---- batch: 090 ----
mean loss: 1041.23
 ---- batch: 100 ----
mean loss: 997.94
 ---- batch: 110 ----
mean loss: 1051.53
train mean loss: 1031.10
epoch train time: 0:00:15.403569
elapsed time: 0:01:16.100185
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 03:59:54.979756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1040.24
 ---- batch: 020 ----
mean loss: 1023.27
 ---- batch: 030 ----
mean loss: 1038.51
 ---- batch: 040 ----
mean loss: 995.08
 ---- batch: 050 ----
mean loss: 1016.32
 ---- batch: 060 ----
mean loss: 1006.95
 ---- batch: 070 ----
mean loss: 1008.25
 ---- batch: 080 ----
mean loss: 996.76
 ---- batch: 090 ----
mean loss: 997.96
 ---- batch: 100 ----
mean loss: 1004.03
 ---- batch: 110 ----
mean loss: 1014.25
train mean loss: 1012.47
epoch train time: 0:00:15.177823
elapsed time: 0:01:31.278922
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 04:00:10.158499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 990.08
 ---- batch: 020 ----
mean loss: 1002.94
 ---- batch: 030 ----
mean loss: 1008.81
 ---- batch: 040 ----
mean loss: 996.25
 ---- batch: 050 ----
mean loss: 997.92
 ---- batch: 060 ----
mean loss: 971.50
 ---- batch: 070 ----
mean loss: 969.35
 ---- batch: 080 ----
mean loss: 976.73
 ---- batch: 090 ----
mean loss: 988.11
 ---- batch: 100 ----
mean loss: 995.84
 ---- batch: 110 ----
mean loss: 1004.49
train mean loss: 990.90
epoch train time: 0:00:15.224827
elapsed time: 0:01:46.504665
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 04:00:25.384229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.72
 ---- batch: 020 ----
mean loss: 986.33
 ---- batch: 030 ----
mean loss: 998.63
 ---- batch: 040 ----
mean loss: 970.68
 ---- batch: 050 ----
mean loss: 957.85
 ---- batch: 060 ----
mean loss: 983.46
 ---- batch: 070 ----
mean loss: 973.27
 ---- batch: 080 ----
mean loss: 997.87
 ---- batch: 090 ----
mean loss: 958.92
 ---- batch: 100 ----
mean loss: 967.96
 ---- batch: 110 ----
mean loss: 964.07
train mean loss: 976.65
epoch train time: 0:00:15.293325
elapsed time: 0:02:01.798935
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 04:00:40.678552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.92
 ---- batch: 020 ----
mean loss: 958.98
 ---- batch: 030 ----
mean loss: 956.04
 ---- batch: 040 ----
mean loss: 958.17
 ---- batch: 050 ----
mean loss: 927.80
 ---- batch: 060 ----
mean loss: 985.15
 ---- batch: 070 ----
mean loss: 954.72
 ---- batch: 080 ----
mean loss: 982.92
 ---- batch: 090 ----
mean loss: 970.57
 ---- batch: 100 ----
mean loss: 976.66
 ---- batch: 110 ----
mean loss: 954.63
train mean loss: 963.27
epoch train time: 0:00:15.299139
elapsed time: 0:02:17.099035
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 04:00:55.978594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 945.60
 ---- batch: 020 ----
mean loss: 939.11
 ---- batch: 030 ----
mean loss: 974.51
 ---- batch: 040 ----
mean loss: 945.88
 ---- batch: 050 ----
mean loss: 950.74
 ---- batch: 060 ----
mean loss: 950.89
 ---- batch: 070 ----
mean loss: 948.43
 ---- batch: 080 ----
mean loss: 946.67
 ---- batch: 090 ----
mean loss: 951.85
 ---- batch: 100 ----
mean loss: 944.47
 ---- batch: 110 ----
mean loss: 938.10
train mean loss: 948.33
epoch train time: 0:00:15.289376
elapsed time: 0:02:32.389304
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 04:01:11.268878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.56
 ---- batch: 020 ----
mean loss: 961.62
 ---- batch: 030 ----
mean loss: 926.23
 ---- batch: 040 ----
mean loss: 913.61
 ---- batch: 050 ----
mean loss: 928.44
 ---- batch: 060 ----
mean loss: 917.29
 ---- batch: 070 ----
mean loss: 956.72
 ---- batch: 080 ----
mean loss: 944.41
 ---- batch: 090 ----
mean loss: 935.42
 ---- batch: 100 ----
mean loss: 946.58
 ---- batch: 110 ----
mean loss: 934.07
train mean loss: 937.74
epoch train time: 0:00:15.201939
elapsed time: 0:02:47.592164
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 04:01:26.471726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.10
 ---- batch: 020 ----
mean loss: 929.77
 ---- batch: 030 ----
mean loss: 902.67
 ---- batch: 040 ----
mean loss: 946.82
 ---- batch: 050 ----
mean loss: 924.66
 ---- batch: 060 ----
mean loss: 930.49
 ---- batch: 070 ----
mean loss: 913.79
 ---- batch: 080 ----
mean loss: 944.63
 ---- batch: 090 ----
mean loss: 915.95
 ---- batch: 100 ----
mean loss: 920.88
 ---- batch: 110 ----
mean loss: 916.94
train mean loss: 925.76
epoch train time: 0:00:15.154507
elapsed time: 0:03:02.747594
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 04:01:41.627427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.97
 ---- batch: 020 ----
mean loss: 905.52
 ---- batch: 030 ----
mean loss: 922.19
 ---- batch: 040 ----
mean loss: 936.23
 ---- batch: 050 ----
mean loss: 911.39
 ---- batch: 060 ----
mean loss: 922.14
 ---- batch: 070 ----
mean loss: 913.36
 ---- batch: 080 ----
mean loss: 899.00
 ---- batch: 090 ----
mean loss: 929.44
 ---- batch: 100 ----
mean loss: 915.28
 ---- batch: 110 ----
mean loss: 903.10
train mean loss: 915.83
epoch train time: 0:00:15.165865
elapsed time: 0:03:17.914744
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 04:01:56.794345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.97
 ---- batch: 020 ----
mean loss: 883.99
 ---- batch: 030 ----
mean loss: 907.76
 ---- batch: 040 ----
mean loss: 915.32
 ---- batch: 050 ----
mean loss: 907.25
 ---- batch: 060 ----
mean loss: 892.23
 ---- batch: 070 ----
mean loss: 903.85
 ---- batch: 080 ----
mean loss: 883.01
 ---- batch: 090 ----
mean loss: 916.43
 ---- batch: 100 ----
mean loss: 901.78
 ---- batch: 110 ----
mean loss: 866.31
train mean loss: 901.30
epoch train time: 0:00:15.167197
elapsed time: 0:03:33.082772
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 04:02:11.962329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.32
 ---- batch: 020 ----
mean loss: 896.07
 ---- batch: 030 ----
mean loss: 886.78
 ---- batch: 040 ----
mean loss: 874.92
 ---- batch: 050 ----
mean loss: 876.87
 ---- batch: 060 ----
mean loss: 877.87
 ---- batch: 070 ----
mean loss: 900.17
 ---- batch: 080 ----
mean loss: 860.94
 ---- batch: 090 ----
mean loss: 873.12
 ---- batch: 100 ----
mean loss: 885.89
 ---- batch: 110 ----
mean loss: 845.16
train mean loss: 879.64
epoch train time: 0:00:15.154907
elapsed time: 0:03:48.238484
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 04:02:27.118229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.78
 ---- batch: 020 ----
mean loss: 872.01
 ---- batch: 030 ----
mean loss: 863.20
 ---- batch: 040 ----
mean loss: 838.13
 ---- batch: 050 ----
mean loss: 845.09
 ---- batch: 060 ----
mean loss: 879.59
 ---- batch: 070 ----
mean loss: 857.16
 ---- batch: 080 ----
mean loss: 869.47
 ---- batch: 090 ----
mean loss: 858.90
 ---- batch: 100 ----
mean loss: 871.08
 ---- batch: 110 ----
mean loss: 857.64
train mean loss: 862.57
epoch train time: 0:00:15.077182
elapsed time: 0:04:03.316766
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 04:02:42.196339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.17
 ---- batch: 020 ----
mean loss: 838.78
 ---- batch: 030 ----
mean loss: 825.88
 ---- batch: 040 ----
mean loss: 849.45
 ---- batch: 050 ----
mean loss: 839.70
 ---- batch: 060 ----
mean loss: 802.24
 ---- batch: 070 ----
mean loss: 811.44
 ---- batch: 080 ----
mean loss: 802.18
 ---- batch: 090 ----
mean loss: 805.86
 ---- batch: 100 ----
mean loss: 788.01
 ---- batch: 110 ----
mean loss: 798.95
train mean loss: 816.37
epoch train time: 0:00:15.067192
elapsed time: 0:04:18.384861
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 04:02:57.264563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 799.31
 ---- batch: 020 ----
mean loss: 772.23
 ---- batch: 030 ----
mean loss: 750.70
 ---- batch: 040 ----
mean loss: 742.11
 ---- batch: 050 ----
mean loss: 756.82
 ---- batch: 060 ----
mean loss: 761.74
 ---- batch: 070 ----
mean loss: 755.20
 ---- batch: 080 ----
mean loss: 731.80
 ---- batch: 090 ----
mean loss: 732.86
 ---- batch: 100 ----
mean loss: 751.23
 ---- batch: 110 ----
mean loss: 726.22
train mean loss: 752.88
epoch train time: 0:00:15.173880
elapsed time: 0:04:33.559764
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 04:03:12.439300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 709.54
 ---- batch: 020 ----
mean loss: 715.11
 ---- batch: 030 ----
mean loss: 722.62
 ---- batch: 040 ----
mean loss: 728.20
 ---- batch: 050 ----
mean loss: 727.64
 ---- batch: 060 ----
mean loss: 729.71
 ---- batch: 070 ----
mean loss: 710.91
 ---- batch: 080 ----
mean loss: 700.41
 ---- batch: 090 ----
mean loss: 678.78
 ---- batch: 100 ----
mean loss: 691.15
 ---- batch: 110 ----
mean loss: 683.75
train mean loss: 708.26
epoch train time: 0:00:15.105076
elapsed time: 0:04:48.665736
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 04:03:27.545298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 686.84
 ---- batch: 020 ----
mean loss: 684.77
 ---- batch: 030 ----
mean loss: 669.97
 ---- batch: 040 ----
mean loss: 677.61
 ---- batch: 050 ----
mean loss: 685.02
 ---- batch: 060 ----
mean loss: 662.53
 ---- batch: 070 ----
mean loss: 668.35
 ---- batch: 080 ----
mean loss: 676.99
 ---- batch: 090 ----
mean loss: 655.93
 ---- batch: 100 ----
mean loss: 653.80
 ---- batch: 110 ----
mean loss: 656.43
train mean loss: 669.74
epoch train time: 0:00:15.108732
elapsed time: 0:05:03.775404
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 04:03:42.655009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 646.58
 ---- batch: 020 ----
mean loss: 657.48
 ---- batch: 030 ----
mean loss: 653.59
 ---- batch: 040 ----
mean loss: 612.84
 ---- batch: 050 ----
mean loss: 633.00
 ---- batch: 060 ----
mean loss: 639.54
 ---- batch: 070 ----
mean loss: 642.03
 ---- batch: 080 ----
mean loss: 639.43
 ---- batch: 090 ----
mean loss: 614.98
 ---- batch: 100 ----
mean loss: 636.45
 ---- batch: 110 ----
mean loss: 626.45
train mean loss: 635.62
epoch train time: 0:00:15.157791
elapsed time: 0:05:18.934211
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 04:03:57.813821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 610.25
 ---- batch: 020 ----
mean loss: 624.90
 ---- batch: 030 ----
mean loss: 614.70
 ---- batch: 040 ----
mean loss: 605.46
 ---- batch: 050 ----
mean loss: 605.97
 ---- batch: 060 ----
mean loss: 612.90
 ---- batch: 070 ----
mean loss: 600.78
 ---- batch: 080 ----
mean loss: 595.12
 ---- batch: 090 ----
mean loss: 592.81
 ---- batch: 100 ----
mean loss: 591.74
 ---- batch: 110 ----
mean loss: 593.15
train mean loss: 603.40
epoch train time: 0:00:15.154748
elapsed time: 0:05:34.089790
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 04:04:12.969390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 571.31
 ---- batch: 020 ----
mean loss: 591.54
 ---- batch: 030 ----
mean loss: 596.62
 ---- batch: 040 ----
mean loss: 586.51
 ---- batch: 050 ----
mean loss: 587.23
 ---- batch: 060 ----
mean loss: 560.92
 ---- batch: 070 ----
mean loss: 574.22
 ---- batch: 080 ----
mean loss: 572.47
 ---- batch: 090 ----
mean loss: 553.83
 ---- batch: 100 ----
mean loss: 551.03
 ---- batch: 110 ----
mean loss: 553.97
train mean loss: 573.07
epoch train time: 0:00:15.111206
elapsed time: 0:05:49.201880
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 04:04:28.081440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 555.53
 ---- batch: 020 ----
mean loss: 555.42
 ---- batch: 030 ----
mean loss: 553.62
 ---- batch: 040 ----
mean loss: 556.87
 ---- batch: 050 ----
mean loss: 553.21
 ---- batch: 060 ----
mean loss: 546.96
 ---- batch: 070 ----
mean loss: 554.22
 ---- batch: 080 ----
mean loss: 544.74
 ---- batch: 090 ----
mean loss: 533.71
 ---- batch: 100 ----
mean loss: 541.54
 ---- batch: 110 ----
mean loss: 516.19
train mean loss: 546.15
epoch train time: 0:00:15.093206
elapsed time: 0:06:04.295940
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 04:04:43.175608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.12
 ---- batch: 020 ----
mean loss: 525.57
 ---- batch: 030 ----
mean loss: 514.90
 ---- batch: 040 ----
mean loss: 521.85
 ---- batch: 050 ----
mean loss: 524.60
 ---- batch: 060 ----
mean loss: 519.31
 ---- batch: 070 ----
mean loss: 531.64
 ---- batch: 080 ----
mean loss: 519.43
 ---- batch: 090 ----
mean loss: 500.57
 ---- batch: 100 ----
mean loss: 516.22
 ---- batch: 110 ----
mean loss: 506.74
train mean loss: 517.97
epoch train time: 0:00:15.298743
elapsed time: 0:06:19.595777
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 04:04:58.475525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.46
 ---- batch: 020 ----
mean loss: 492.34
 ---- batch: 030 ----
mean loss: 500.76
 ---- batch: 040 ----
mean loss: 501.66
 ---- batch: 050 ----
mean loss: 500.11
 ---- batch: 060 ----
mean loss: 505.45
 ---- batch: 070 ----
mean loss: 498.67
 ---- batch: 080 ----
mean loss: 484.69
 ---- batch: 090 ----
mean loss: 485.79
 ---- batch: 100 ----
mean loss: 485.65
 ---- batch: 110 ----
mean loss: 492.06
train mean loss: 493.81
epoch train time: 0:00:15.085250
elapsed time: 0:06:34.682106
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 04:05:13.561761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.97
 ---- batch: 020 ----
mean loss: 491.27
 ---- batch: 030 ----
mean loss: 474.10
 ---- batch: 040 ----
mean loss: 470.14
 ---- batch: 050 ----
mean loss: 475.33
 ---- batch: 060 ----
mean loss: 460.62
 ---- batch: 070 ----
mean loss: 465.29
 ---- batch: 080 ----
mean loss: 462.95
 ---- batch: 090 ----
mean loss: 462.11
 ---- batch: 100 ----
mean loss: 466.33
 ---- batch: 110 ----
mean loss: 476.45
train mean loss: 470.69
epoch train time: 0:00:15.097523
elapsed time: 0:06:49.780630
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 04:05:28.660274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.89
 ---- batch: 020 ----
mean loss: 465.80
 ---- batch: 030 ----
mean loss: 444.97
 ---- batch: 040 ----
mean loss: 467.28
 ---- batch: 050 ----
mean loss: 460.34
 ---- batch: 060 ----
mean loss: 448.79
 ---- batch: 070 ----
mean loss: 454.39
 ---- batch: 080 ----
mean loss: 454.85
 ---- batch: 090 ----
mean loss: 451.01
 ---- batch: 100 ----
mean loss: 443.66
 ---- batch: 110 ----
mean loss: 444.69
train mean loss: 453.72
epoch train time: 0:00:15.058692
elapsed time: 0:07:04.840314
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 04:05:43.719902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.60
 ---- batch: 020 ----
mean loss: 452.75
 ---- batch: 030 ----
mean loss: 442.69
 ---- batch: 040 ----
mean loss: 429.68
 ---- batch: 050 ----
mean loss: 428.34
 ---- batch: 060 ----
mean loss: 433.30
 ---- batch: 070 ----
mean loss: 421.37
 ---- batch: 080 ----
mean loss: 437.54
 ---- batch: 090 ----
mean loss: 447.28
 ---- batch: 100 ----
mean loss: 430.09
 ---- batch: 110 ----
mean loss: 440.26
train mean loss: 438.28
epoch train time: 0:00:15.082386
elapsed time: 0:07:19.923636
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 04:05:58.803234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.79
 ---- batch: 020 ----
mean loss: 411.05
 ---- batch: 030 ----
mean loss: 431.29
 ---- batch: 040 ----
mean loss: 429.80
 ---- batch: 050 ----
mean loss: 433.56
 ---- batch: 060 ----
mean loss: 419.95
 ---- batch: 070 ----
mean loss: 420.43
 ---- batch: 080 ----
mean loss: 415.14
 ---- batch: 090 ----
mean loss: 412.70
 ---- batch: 100 ----
mean loss: 418.77
 ---- batch: 110 ----
mean loss: 420.00
train mean loss: 421.93
epoch train time: 0:00:14.996516
elapsed time: 0:07:34.921040
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 04:06:13.800818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.73
 ---- batch: 020 ----
mean loss: 412.98
 ---- batch: 030 ----
mean loss: 403.56
 ---- batch: 040 ----
mean loss: 406.64
 ---- batch: 050 ----
mean loss: 412.28
 ---- batch: 060 ----
mean loss: 400.57
 ---- batch: 070 ----
mean loss: 410.90
 ---- batch: 080 ----
mean loss: 412.48
 ---- batch: 090 ----
mean loss: 402.48
 ---- batch: 100 ----
mean loss: 407.96
 ---- batch: 110 ----
mean loss: 406.98
train mean loss: 407.89
epoch train time: 0:00:15.095964
elapsed time: 0:07:50.018103
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 04:06:28.897705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.69
 ---- batch: 020 ----
mean loss: 403.30
 ---- batch: 030 ----
mean loss: 395.25
 ---- batch: 040 ----
mean loss: 408.44
 ---- batch: 050 ----
mean loss: 407.11
 ---- batch: 060 ----
mean loss: 395.66
 ---- batch: 070 ----
mean loss: 391.22
 ---- batch: 080 ----
mean loss: 393.81
 ---- batch: 090 ----
mean loss: 391.79
 ---- batch: 100 ----
mean loss: 397.78
 ---- batch: 110 ----
mean loss: 406.91
train mean loss: 399.34
epoch train time: 0:00:14.970587
elapsed time: 0:08:04.989629
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 04:06:43.869278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.17
 ---- batch: 020 ----
mean loss: 395.54
 ---- batch: 030 ----
mean loss: 393.89
 ---- batch: 040 ----
mean loss: 384.69
 ---- batch: 050 ----
mean loss: 381.49
 ---- batch: 060 ----
mean loss: 399.29
 ---- batch: 070 ----
mean loss: 374.86
 ---- batch: 080 ----
mean loss: 389.28
 ---- batch: 090 ----
mean loss: 382.00
 ---- batch: 100 ----
mean loss: 381.95
 ---- batch: 110 ----
mean loss: 386.75
train mean loss: 386.72
epoch train time: 0:00:14.991203
elapsed time: 0:08:19.981814
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 04:06:58.861382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.59
 ---- batch: 020 ----
mean loss: 378.58
 ---- batch: 030 ----
mean loss: 383.37
 ---- batch: 040 ----
mean loss: 381.89
 ---- batch: 050 ----
mean loss: 370.66
 ---- batch: 060 ----
mean loss: 384.25
 ---- batch: 070 ----
mean loss: 367.24
 ---- batch: 080 ----
mean loss: 378.70
 ---- batch: 090 ----
mean loss: 379.00
 ---- batch: 100 ----
mean loss: 376.93
 ---- batch: 110 ----
mean loss: 378.25
train mean loss: 378.69
epoch train time: 0:00:14.948330
elapsed time: 0:08:34.931162
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 04:07:13.810735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.74
 ---- batch: 020 ----
mean loss: 367.72
 ---- batch: 030 ----
mean loss: 365.82
 ---- batch: 040 ----
mean loss: 375.49
 ---- batch: 050 ----
mean loss: 369.81
 ---- batch: 060 ----
mean loss: 369.74
 ---- batch: 070 ----
mean loss: 348.03
 ---- batch: 080 ----
mean loss: 386.65
 ---- batch: 090 ----
mean loss: 366.38
 ---- batch: 100 ----
mean loss: 364.67
 ---- batch: 110 ----
mean loss: 366.96
train mean loss: 368.67
epoch train time: 0:00:15.001459
elapsed time: 0:08:49.933551
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 04:07:28.813167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.89
 ---- batch: 020 ----
mean loss: 365.76
 ---- batch: 030 ----
mean loss: 361.83
 ---- batch: 040 ----
mean loss: 357.21
 ---- batch: 050 ----
mean loss: 360.01
 ---- batch: 060 ----
mean loss: 346.81
 ---- batch: 070 ----
mean loss: 369.12
 ---- batch: 080 ----
mean loss: 354.87
 ---- batch: 090 ----
mean loss: 366.17
 ---- batch: 100 ----
mean loss: 357.73
 ---- batch: 110 ----
mean loss: 356.09
train mean loss: 359.51
epoch train time: 0:00:14.995489
elapsed time: 0:09:04.929994
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 04:07:43.809708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.44
 ---- batch: 020 ----
mean loss: 345.53
 ---- batch: 030 ----
mean loss: 354.05
 ---- batch: 040 ----
mean loss: 354.53
 ---- batch: 050 ----
mean loss: 348.81
 ---- batch: 060 ----
mean loss: 352.95
 ---- batch: 070 ----
mean loss: 342.31
 ---- batch: 080 ----
mean loss: 358.30
 ---- batch: 090 ----
mean loss: 355.75
 ---- batch: 100 ----
mean loss: 365.74
 ---- batch: 110 ----
mean loss: 368.20
train mean loss: 354.34
epoch train time: 0:00:15.020552
elapsed time: 0:09:19.951578
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 04:07:58.831141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.85
 ---- batch: 020 ----
mean loss: 352.84
 ---- batch: 030 ----
mean loss: 348.75
 ---- batch: 040 ----
mean loss: 335.02
 ---- batch: 050 ----
mean loss: 352.38
 ---- batch: 060 ----
mean loss: 351.28
 ---- batch: 070 ----
mean loss: 347.19
 ---- batch: 080 ----
mean loss: 351.56
 ---- batch: 090 ----
mean loss: 342.51
 ---- batch: 100 ----
mean loss: 355.66
 ---- batch: 110 ----
mean loss: 342.55
train mean loss: 349.12
epoch train time: 0:00:15.020289
elapsed time: 0:09:34.972785
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 04:08:13.852361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.80
 ---- batch: 020 ----
mean loss: 348.97
 ---- batch: 030 ----
mean loss: 349.12
 ---- batch: 040 ----
mean loss: 335.75
 ---- batch: 050 ----
mean loss: 342.52
 ---- batch: 060 ----
mean loss: 347.91
 ---- batch: 070 ----
mean loss: 348.65
 ---- batch: 080 ----
mean loss: 339.30
 ---- batch: 090 ----
mean loss: 337.74
 ---- batch: 100 ----
mean loss: 342.21
 ---- batch: 110 ----
mean loss: 339.66
train mean loss: 343.44
epoch train time: 0:00:14.991662
elapsed time: 0:09:49.965399
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 04:08:28.845029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.00
 ---- batch: 020 ----
mean loss: 335.89
 ---- batch: 030 ----
mean loss: 346.47
 ---- batch: 040 ----
mean loss: 342.54
 ---- batch: 050 ----
mean loss: 329.60
 ---- batch: 060 ----
mean loss: 335.46
 ---- batch: 070 ----
mean loss: 324.48
 ---- batch: 080 ----
mean loss: 332.87
 ---- batch: 090 ----
mean loss: 321.55
 ---- batch: 100 ----
mean loss: 334.87
 ---- batch: 110 ----
mean loss: 344.27
train mean loss: 333.96
epoch train time: 0:00:14.948253
elapsed time: 0:10:04.914643
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 04:08:43.794284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.50
 ---- batch: 020 ----
mean loss: 343.33
 ---- batch: 030 ----
mean loss: 340.23
 ---- batch: 040 ----
mean loss: 339.12
 ---- batch: 050 ----
mean loss: 325.00
 ---- batch: 060 ----
mean loss: 330.90
 ---- batch: 070 ----
mean loss: 318.62
 ---- batch: 080 ----
mean loss: 347.38
 ---- batch: 090 ----
mean loss: 325.12
 ---- batch: 100 ----
mean loss: 334.31
 ---- batch: 110 ----
mean loss: 319.45
train mean loss: 332.70
epoch train time: 0:00:15.073990
elapsed time: 0:10:19.989654
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 04:08:58.869309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.80
 ---- batch: 020 ----
mean loss: 327.51
 ---- batch: 030 ----
mean loss: 319.03
 ---- batch: 040 ----
mean loss: 325.67
 ---- batch: 050 ----
mean loss: 326.52
 ---- batch: 060 ----
mean loss: 318.98
 ---- batch: 070 ----
mean loss: 330.29
 ---- batch: 080 ----
mean loss: 325.34
 ---- batch: 090 ----
mean loss: 330.82
 ---- batch: 100 ----
mean loss: 331.11
 ---- batch: 110 ----
mean loss: 322.39
train mean loss: 325.58
epoch train time: 0:00:15.131211
elapsed time: 0:10:35.121888
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 04:09:14.001481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.05
 ---- batch: 020 ----
mean loss: 320.31
 ---- batch: 030 ----
mean loss: 323.59
 ---- batch: 040 ----
mean loss: 329.26
 ---- batch: 050 ----
mean loss: 323.22
 ---- batch: 060 ----
mean loss: 328.24
 ---- batch: 070 ----
mean loss: 313.55
 ---- batch: 080 ----
mean loss: 327.70
 ---- batch: 090 ----
mean loss: 322.53
 ---- batch: 100 ----
mean loss: 316.44
 ---- batch: 110 ----
mean loss: 322.88
train mean loss: 322.06
epoch train time: 0:00:15.115939
elapsed time: 0:10:50.238770
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 04:09:29.118326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.87
 ---- batch: 020 ----
mean loss: 311.45
 ---- batch: 030 ----
mean loss: 325.56
 ---- batch: 040 ----
mean loss: 323.36
 ---- batch: 050 ----
mean loss: 312.25
 ---- batch: 060 ----
mean loss: 306.92
 ---- batch: 070 ----
mean loss: 321.22
 ---- batch: 080 ----
mean loss: 323.00
 ---- batch: 090 ----
mean loss: 318.29
 ---- batch: 100 ----
mean loss: 305.48
 ---- batch: 110 ----
mean loss: 310.90
train mean loss: 315.44
epoch train time: 0:00:15.051335
elapsed time: 0:11:05.290912
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 04:09:44.170536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.54
 ---- batch: 020 ----
mean loss: 312.34
 ---- batch: 030 ----
mean loss: 313.20
 ---- batch: 040 ----
mean loss: 324.01
 ---- batch: 050 ----
mean loss: 312.55
 ---- batch: 060 ----
mean loss: 317.84
 ---- batch: 070 ----
mean loss: 317.20
 ---- batch: 080 ----
mean loss: 304.13
 ---- batch: 090 ----
mean loss: 306.48
 ---- batch: 100 ----
mean loss: 302.90
 ---- batch: 110 ----
mean loss: 320.02
train mean loss: 313.04
epoch train time: 0:00:15.149829
elapsed time: 0:11:20.441717
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 04:09:59.321304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.43
 ---- batch: 020 ----
mean loss: 306.51
 ---- batch: 030 ----
mean loss: 314.15
 ---- batch: 040 ----
mean loss: 315.66
 ---- batch: 050 ----
mean loss: 307.78
 ---- batch: 060 ----
mean loss: 305.77
 ---- batch: 070 ----
mean loss: 313.39
 ---- batch: 080 ----
mean loss: 313.56
 ---- batch: 090 ----
mean loss: 305.83
 ---- batch: 100 ----
mean loss: 313.27
 ---- batch: 110 ----
mean loss: 305.38
train mean loss: 310.03
epoch train time: 0:00:15.197693
elapsed time: 0:11:35.640245
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 04:10:14.519843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.23
 ---- batch: 020 ----
mean loss: 311.49
 ---- batch: 030 ----
mean loss: 300.59
 ---- batch: 040 ----
mean loss: 304.13
 ---- batch: 050 ----
mean loss: 309.16
 ---- batch: 060 ----
mean loss: 316.27
 ---- batch: 070 ----
mean loss: 296.24
 ---- batch: 080 ----
mean loss: 300.59
 ---- batch: 090 ----
mean loss: 314.53
 ---- batch: 100 ----
mean loss: 301.97
 ---- batch: 110 ----
mean loss: 293.34
train mean loss: 304.66
epoch train time: 0:00:15.372545
elapsed time: 0:11:51.013778
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 04:10:29.893525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.39
 ---- batch: 020 ----
mean loss: 307.45
 ---- batch: 030 ----
mean loss: 301.71
 ---- batch: 040 ----
mean loss: 302.87
 ---- batch: 050 ----
mean loss: 293.45
 ---- batch: 060 ----
mean loss: 292.26
 ---- batch: 070 ----
mean loss: 309.06
 ---- batch: 080 ----
mean loss: 307.15
 ---- batch: 090 ----
mean loss: 299.12
 ---- batch: 100 ----
mean loss: 299.52
 ---- batch: 110 ----
mean loss: 296.34
train mean loss: 301.95
epoch train time: 0:00:15.356283
elapsed time: 0:12:06.371146
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 04:10:45.250738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.66
 ---- batch: 020 ----
mean loss: 296.18
 ---- batch: 030 ----
mean loss: 294.90
 ---- batch: 040 ----
mean loss: 292.17
 ---- batch: 050 ----
mean loss: 305.07
 ---- batch: 060 ----
mean loss: 298.86
 ---- batch: 070 ----
mean loss: 306.02
 ---- batch: 080 ----
mean loss: 295.39
 ---- batch: 090 ----
mean loss: 290.68
 ---- batch: 100 ----
mean loss: 290.77
 ---- batch: 110 ----
mean loss: 296.56
train mean loss: 296.45
epoch train time: 0:00:15.428632
elapsed time: 0:12:21.800776
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 04:11:00.680405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.10
 ---- batch: 020 ----
mean loss: 306.34
 ---- batch: 030 ----
mean loss: 291.17
 ---- batch: 040 ----
mean loss: 300.43
 ---- batch: 050 ----
mean loss: 295.93
 ---- batch: 060 ----
mean loss: 301.08
 ---- batch: 070 ----
mean loss: 303.21
 ---- batch: 080 ----
mean loss: 301.11
 ---- batch: 090 ----
mean loss: 287.73
 ---- batch: 100 ----
mean loss: 272.41
 ---- batch: 110 ----
mean loss: 289.11
train mean loss: 294.06
epoch train time: 0:00:15.425775
elapsed time: 0:12:37.227583
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 04:11:16.107183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.92
 ---- batch: 020 ----
mean loss: 294.93
 ---- batch: 030 ----
mean loss: 289.78
 ---- batch: 040 ----
mean loss: 294.35
 ---- batch: 050 ----
mean loss: 286.63
 ---- batch: 060 ----
mean loss: 293.27
 ---- batch: 070 ----
mean loss: 292.34
 ---- batch: 080 ----
mean loss: 299.99
 ---- batch: 090 ----
mean loss: 290.24
 ---- batch: 100 ----
mean loss: 284.68
 ---- batch: 110 ----
mean loss: 288.12
train mean loss: 292.19
epoch train time: 0:00:15.248700
elapsed time: 0:12:52.477252
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 04:11:31.356959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.60
 ---- batch: 020 ----
mean loss: 291.77
 ---- batch: 030 ----
mean loss: 284.36
 ---- batch: 040 ----
mean loss: 297.82
 ---- batch: 050 ----
mean loss: 290.93
 ---- batch: 060 ----
mean loss: 287.51
 ---- batch: 070 ----
mean loss: 280.88
 ---- batch: 080 ----
mean loss: 283.45
 ---- batch: 090 ----
mean loss: 287.12
 ---- batch: 100 ----
mean loss: 285.81
 ---- batch: 110 ----
mean loss: 274.72
train mean loss: 287.53
epoch train time: 0:00:15.178273
elapsed time: 0:13:07.656592
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 04:11:46.536287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.41
 ---- batch: 020 ----
mean loss: 287.71
 ---- batch: 030 ----
mean loss: 281.31
 ---- batch: 040 ----
mean loss: 284.95
 ---- batch: 050 ----
mean loss: 289.51
 ---- batch: 060 ----
mean loss: 283.97
 ---- batch: 070 ----
mean loss: 282.62
 ---- batch: 080 ----
mean loss: 274.93
 ---- batch: 090 ----
mean loss: 293.01
 ---- batch: 100 ----
mean loss: 284.59
 ---- batch: 110 ----
mean loss: 296.73
train mean loss: 285.43
epoch train time: 0:00:15.179293
elapsed time: 0:13:22.836911
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 04:12:01.716471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.60
 ---- batch: 020 ----
mean loss: 285.42
 ---- batch: 030 ----
mean loss: 294.13
 ---- batch: 040 ----
mean loss: 279.07
 ---- batch: 050 ----
mean loss: 279.48
 ---- batch: 060 ----
mean loss: 279.44
 ---- batch: 070 ----
mean loss: 283.71
 ---- batch: 080 ----
mean loss: 285.83
 ---- batch: 090 ----
mean loss: 281.52
 ---- batch: 100 ----
mean loss: 282.73
 ---- batch: 110 ----
mean loss: 282.26
train mean loss: 282.79
epoch train time: 0:00:15.139608
elapsed time: 0:13:37.977444
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 04:12:16.857058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.45
 ---- batch: 020 ----
mean loss: 286.26
 ---- batch: 030 ----
mean loss: 283.46
 ---- batch: 040 ----
mean loss: 278.88
 ---- batch: 050 ----
mean loss: 275.85
 ---- batch: 060 ----
mean loss: 279.24
 ---- batch: 070 ----
mean loss: 276.86
 ---- batch: 080 ----
mean loss: 277.37
 ---- batch: 090 ----
mean loss: 273.75
 ---- batch: 100 ----
mean loss: 272.67
 ---- batch: 110 ----
mean loss: 285.44
train mean loss: 280.03
epoch train time: 0:00:15.191015
elapsed time: 0:13:53.169589
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 04:12:32.049162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.96
 ---- batch: 020 ----
mean loss: 277.91
 ---- batch: 030 ----
mean loss: 276.85
 ---- batch: 040 ----
mean loss: 274.36
 ---- batch: 050 ----
mean loss: 283.60
 ---- batch: 060 ----
mean loss: 280.59
 ---- batch: 070 ----
mean loss: 279.36
 ---- batch: 080 ----
mean loss: 274.58
 ---- batch: 090 ----
mean loss: 275.41
 ---- batch: 100 ----
mean loss: 277.17
 ---- batch: 110 ----
mean loss: 278.12
train mean loss: 277.88
epoch train time: 0:00:15.138001
elapsed time: 0:14:08.308483
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 04:12:47.188032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.10
 ---- batch: 020 ----
mean loss: 271.29
 ---- batch: 030 ----
mean loss: 280.55
 ---- batch: 040 ----
mean loss: 275.52
 ---- batch: 050 ----
mean loss: 279.41
 ---- batch: 060 ----
mean loss: 263.31
 ---- batch: 070 ----
mean loss: 262.97
 ---- batch: 080 ----
mean loss: 271.92
 ---- batch: 090 ----
mean loss: 272.66
 ---- batch: 100 ----
mean loss: 283.46
 ---- batch: 110 ----
mean loss: 281.48
train mean loss: 274.85
epoch train time: 0:00:15.164383
elapsed time: 0:14:23.473796
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 04:13:02.353434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.75
 ---- batch: 020 ----
mean loss: 265.48
 ---- batch: 030 ----
mean loss: 277.34
 ---- batch: 040 ----
mean loss: 274.20
 ---- batch: 050 ----
mean loss: 276.83
 ---- batch: 060 ----
mean loss: 268.10
 ---- batch: 070 ----
mean loss: 271.82
 ---- batch: 080 ----
mean loss: 265.04
 ---- batch: 090 ----
mean loss: 275.65
 ---- batch: 100 ----
mean loss: 269.02
 ---- batch: 110 ----
mean loss: 269.28
train mean loss: 271.73
epoch train time: 0:00:15.143575
elapsed time: 0:14:38.618357
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 04:13:17.497965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.28
 ---- batch: 020 ----
mean loss: 272.74
 ---- batch: 030 ----
mean loss: 281.42
 ---- batch: 040 ----
mean loss: 264.88
 ---- batch: 050 ----
mean loss: 280.92
 ---- batch: 060 ----
mean loss: 277.04
 ---- batch: 070 ----
mean loss: 262.72
 ---- batch: 080 ----
mean loss: 259.29
 ---- batch: 090 ----
mean loss: 266.49
 ---- batch: 100 ----
mean loss: 262.12
 ---- batch: 110 ----
mean loss: 273.88
train mean loss: 270.63
epoch train time: 0:00:15.123244
elapsed time: 0:14:53.742553
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 04:13:32.622180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.88
 ---- batch: 020 ----
mean loss: 268.24
 ---- batch: 030 ----
mean loss: 266.36
 ---- batch: 040 ----
mean loss: 261.41
 ---- batch: 050 ----
mean loss: 268.23
 ---- batch: 060 ----
mean loss: 268.93
 ---- batch: 070 ----
mean loss: 270.81
 ---- batch: 080 ----
mean loss: 265.20
 ---- batch: 090 ----
mean loss: 268.37
 ---- batch: 100 ----
mean loss: 265.39
 ---- batch: 110 ----
mean loss: 281.64
train mean loss: 269.39
epoch train time: 0:00:15.049827
elapsed time: 0:15:08.793397
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 04:13:47.672993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.69
 ---- batch: 020 ----
mean loss: 273.12
 ---- batch: 030 ----
mean loss: 267.83
 ---- batch: 040 ----
mean loss: 263.76
 ---- batch: 050 ----
mean loss: 263.31
 ---- batch: 060 ----
mean loss: 256.04
 ---- batch: 070 ----
mean loss: 267.94
 ---- batch: 080 ----
mean loss: 271.99
 ---- batch: 090 ----
mean loss: 261.15
 ---- batch: 100 ----
mean loss: 275.13
 ---- batch: 110 ----
mean loss: 262.98
train mean loss: 266.12
epoch train time: 0:00:15.179413
elapsed time: 0:15:23.973770
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 04:14:02.853345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.21
 ---- batch: 020 ----
mean loss: 260.16
 ---- batch: 030 ----
mean loss: 266.74
 ---- batch: 040 ----
mean loss: 263.24
 ---- batch: 050 ----
mean loss: 253.38
 ---- batch: 060 ----
mean loss: 262.69
 ---- batch: 070 ----
mean loss: 261.65
 ---- batch: 080 ----
mean loss: 269.81
 ---- batch: 090 ----
mean loss: 270.10
 ---- batch: 100 ----
mean loss: 264.90
 ---- batch: 110 ----
mean loss: 268.88
train mean loss: 264.12
epoch train time: 0:00:15.093691
elapsed time: 0:15:39.068401
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 04:14:17.947995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.29
 ---- batch: 020 ----
mean loss: 254.70
 ---- batch: 030 ----
mean loss: 262.51
 ---- batch: 040 ----
mean loss: 260.50
 ---- batch: 050 ----
mean loss: 261.28
 ---- batch: 060 ----
mean loss: 260.88
 ---- batch: 070 ----
mean loss: 255.96
 ---- batch: 080 ----
mean loss: 270.97
 ---- batch: 090 ----
mean loss: 264.27
 ---- batch: 100 ----
mean loss: 261.51
 ---- batch: 110 ----
mean loss: 260.69
train mean loss: 261.02
epoch train time: 0:00:15.112119
elapsed time: 0:15:54.181441
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 04:14:33.060992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.50
 ---- batch: 020 ----
mean loss: 266.90
 ---- batch: 030 ----
mean loss: 261.02
 ---- batch: 040 ----
mean loss: 243.19
 ---- batch: 050 ----
mean loss: 255.86
 ---- batch: 060 ----
mean loss: 257.07
 ---- batch: 070 ----
mean loss: 270.79
 ---- batch: 080 ----
mean loss: 259.28
 ---- batch: 090 ----
mean loss: 264.81
 ---- batch: 100 ----
mean loss: 248.99
 ---- batch: 110 ----
mean loss: 260.53
train mean loss: 258.17
epoch train time: 0:00:15.089158
elapsed time: 0:16:09.271470
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 04:14:48.151043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.15
 ---- batch: 020 ----
mean loss: 249.35
 ---- batch: 030 ----
mean loss: 252.47
 ---- batch: 040 ----
mean loss: 262.82
 ---- batch: 050 ----
mean loss: 255.56
 ---- batch: 060 ----
mean loss: 264.48
 ---- batch: 070 ----
mean loss: 249.31
 ---- batch: 080 ----
mean loss: 270.04
 ---- batch: 090 ----
mean loss: 258.51
 ---- batch: 100 ----
mean loss: 259.13
 ---- batch: 110 ----
mean loss: 263.80
train mean loss: 258.65
epoch train time: 0:00:15.180333
elapsed time: 0:16:24.452720
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 04:15:03.332437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.34
 ---- batch: 020 ----
mean loss: 249.26
 ---- batch: 030 ----
mean loss: 253.36
 ---- batch: 040 ----
mean loss: 257.25
 ---- batch: 050 ----
mean loss: 249.30
 ---- batch: 060 ----
mean loss: 254.14
 ---- batch: 070 ----
mean loss: 261.62
 ---- batch: 080 ----
mean loss: 256.98
 ---- batch: 090 ----
mean loss: 251.71
 ---- batch: 100 ----
mean loss: 250.71
 ---- batch: 110 ----
mean loss: 264.71
train mean loss: 254.58
epoch train time: 0:00:15.172261
elapsed time: 0:16:39.626097
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 04:15:18.505771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.44
 ---- batch: 020 ----
mean loss: 258.41
 ---- batch: 030 ----
mean loss: 252.39
 ---- batch: 040 ----
mean loss: 238.78
 ---- batch: 050 ----
mean loss: 257.01
 ---- batch: 060 ----
mean loss: 254.92
 ---- batch: 070 ----
mean loss: 256.46
 ---- batch: 080 ----
mean loss: 253.32
 ---- batch: 090 ----
mean loss: 255.35
 ---- batch: 100 ----
mean loss: 252.50
 ---- batch: 110 ----
mean loss: 244.89
train mean loss: 252.87
epoch train time: 0:00:15.117257
elapsed time: 0:16:54.744472
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 04:15:33.624107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.82
 ---- batch: 020 ----
mean loss: 244.72
 ---- batch: 030 ----
mean loss: 256.49
 ---- batch: 040 ----
mean loss: 255.78
 ---- batch: 050 ----
mean loss: 255.77
 ---- batch: 060 ----
mean loss: 253.31
 ---- batch: 070 ----
mean loss: 259.22
 ---- batch: 080 ----
mean loss: 249.64
 ---- batch: 090 ----
mean loss: 244.13
 ---- batch: 100 ----
mean loss: 245.12
 ---- batch: 110 ----
mean loss: 248.02
train mean loss: 251.77
epoch train time: 0:00:15.148054
elapsed time: 0:17:09.893544
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 04:15:48.773189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.63
 ---- batch: 020 ----
mean loss: 249.72
 ---- batch: 030 ----
mean loss: 249.25
 ---- batch: 040 ----
mean loss: 246.14
 ---- batch: 050 ----
mean loss: 244.59
 ---- batch: 060 ----
mean loss: 256.51
 ---- batch: 070 ----
mean loss: 245.91
 ---- batch: 080 ----
mean loss: 252.52
 ---- batch: 090 ----
mean loss: 248.31
 ---- batch: 100 ----
mean loss: 247.75
 ---- batch: 110 ----
mean loss: 247.19
train mean loss: 249.20
epoch train time: 0:00:15.143957
elapsed time: 0:17:25.038518
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 04:16:03.918173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.55
 ---- batch: 020 ----
mean loss: 241.72
 ---- batch: 030 ----
mean loss: 239.76
 ---- batch: 040 ----
mean loss: 252.89
 ---- batch: 050 ----
mean loss: 258.35
 ---- batch: 060 ----
mean loss: 236.76
 ---- batch: 070 ----
mean loss: 249.56
 ---- batch: 080 ----
mean loss: 242.98
 ---- batch: 090 ----
mean loss: 252.01
 ---- batch: 100 ----
mean loss: 246.66
 ---- batch: 110 ----
mean loss: 252.61
train mean loss: 247.28
epoch train time: 0:00:15.097283
elapsed time: 0:17:40.136841
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 04:16:19.016387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.52
 ---- batch: 020 ----
mean loss: 249.90
 ---- batch: 030 ----
mean loss: 239.49
 ---- batch: 040 ----
mean loss: 249.50
 ---- batch: 050 ----
mean loss: 247.76
 ---- batch: 060 ----
mean loss: 253.37
 ---- batch: 070 ----
mean loss: 245.80
 ---- batch: 080 ----
mean loss: 238.78
 ---- batch: 090 ----
mean loss: 251.22
 ---- batch: 100 ----
mean loss: 237.33
 ---- batch: 110 ----
mean loss: 248.77
train mean loss: 245.48
epoch train time: 0:00:15.151708
elapsed time: 0:17:55.289421
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 04:16:34.168986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.85
 ---- batch: 020 ----
mean loss: 246.69
 ---- batch: 030 ----
mean loss: 249.05
 ---- batch: 040 ----
mean loss: 238.66
 ---- batch: 050 ----
mean loss: 247.57
 ---- batch: 060 ----
mean loss: 253.65
 ---- batch: 070 ----
mean loss: 242.55
 ---- batch: 080 ----
mean loss: 243.67
 ---- batch: 090 ----
mean loss: 244.36
 ---- batch: 100 ----
mean loss: 239.03
 ---- batch: 110 ----
mean loss: 249.15
train mean loss: 246.13
epoch train time: 0:00:15.164951
elapsed time: 0:18:10.455392
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 04:16:49.335044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.83
 ---- batch: 020 ----
mean loss: 243.91
 ---- batch: 030 ----
mean loss: 248.71
 ---- batch: 040 ----
mean loss: 244.94
 ---- batch: 050 ----
mean loss: 236.48
 ---- batch: 060 ----
mean loss: 233.83
 ---- batch: 070 ----
mean loss: 243.74
 ---- batch: 080 ----
mean loss: 240.82
 ---- batch: 090 ----
mean loss: 246.06
 ---- batch: 100 ----
mean loss: 246.46
 ---- batch: 110 ----
mean loss: 243.20
train mean loss: 242.43
epoch train time: 0:00:15.138389
elapsed time: 0:18:25.594738
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 04:17:04.474266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.94
 ---- batch: 020 ----
mean loss: 235.49
 ---- batch: 030 ----
mean loss: 247.03
 ---- batch: 040 ----
mean loss: 239.67
 ---- batch: 050 ----
mean loss: 226.87
 ---- batch: 060 ----
mean loss: 240.67
 ---- batch: 070 ----
mean loss: 239.53
 ---- batch: 080 ----
mean loss: 247.56
 ---- batch: 090 ----
mean loss: 239.55
 ---- batch: 100 ----
mean loss: 239.43
 ---- batch: 110 ----
mean loss: 244.55
train mean loss: 240.38
epoch train time: 0:00:15.120275
elapsed time: 0:18:40.715869
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 04:17:19.595523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.29
 ---- batch: 020 ----
mean loss: 241.45
 ---- batch: 030 ----
mean loss: 236.06
 ---- batch: 040 ----
mean loss: 243.36
 ---- batch: 050 ----
mean loss: 242.32
 ---- batch: 060 ----
mean loss: 235.92
 ---- batch: 070 ----
mean loss: 230.89
 ---- batch: 080 ----
mean loss: 235.27
 ---- batch: 090 ----
mean loss: 235.52
 ---- batch: 100 ----
mean loss: 244.19
 ---- batch: 110 ----
mean loss: 237.51
train mean loss: 237.75
epoch train time: 0:00:15.138740
elapsed time: 0:18:55.855620
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 04:17:34.735236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.65
 ---- batch: 020 ----
mean loss: 243.12
 ---- batch: 030 ----
mean loss: 232.37
 ---- batch: 040 ----
mean loss: 233.55
 ---- batch: 050 ----
mean loss: 233.22
 ---- batch: 060 ----
mean loss: 241.42
 ---- batch: 070 ----
mean loss: 231.54
 ---- batch: 080 ----
mean loss: 245.56
 ---- batch: 090 ----
mean loss: 235.86
 ---- batch: 100 ----
mean loss: 238.65
 ---- batch: 110 ----
mean loss: 230.66
train mean loss: 236.92
epoch train time: 0:00:15.111481
elapsed time: 0:19:10.968068
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 04:17:49.847685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.83
 ---- batch: 020 ----
mean loss: 235.98
 ---- batch: 030 ----
mean loss: 232.77
 ---- batch: 040 ----
mean loss: 236.63
 ---- batch: 050 ----
mean loss: 227.66
 ---- batch: 060 ----
mean loss: 237.84
 ---- batch: 070 ----
mean loss: 239.85
 ---- batch: 080 ----
mean loss: 242.05
 ---- batch: 090 ----
mean loss: 237.03
 ---- batch: 100 ----
mean loss: 235.37
 ---- batch: 110 ----
mean loss: 234.68
train mean loss: 236.58
epoch train time: 0:00:15.142474
elapsed time: 0:19:26.111548
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 04:18:04.991105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.21
 ---- batch: 020 ----
mean loss: 231.48
 ---- batch: 030 ----
mean loss: 242.10
 ---- batch: 040 ----
mean loss: 234.84
 ---- batch: 050 ----
mean loss: 241.45
 ---- batch: 060 ----
mean loss: 231.41
 ---- batch: 070 ----
mean loss: 226.04
 ---- batch: 080 ----
mean loss: 230.72
 ---- batch: 090 ----
mean loss: 231.09
 ---- batch: 100 ----
mean loss: 230.68
 ---- batch: 110 ----
mean loss: 228.10
train mean loss: 232.84
epoch train time: 0:00:15.095968
elapsed time: 0:19:41.208278
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 04:18:20.087849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.85
 ---- batch: 020 ----
mean loss: 227.96
 ---- batch: 030 ----
mean loss: 235.14
 ---- batch: 040 ----
mean loss: 234.23
 ---- batch: 050 ----
mean loss: 234.26
 ---- batch: 060 ----
mean loss: 234.59
 ---- batch: 070 ----
mean loss: 234.44
 ---- batch: 080 ----
mean loss: 229.77
 ---- batch: 090 ----
mean loss: 224.78
 ---- batch: 100 ----
mean loss: 222.84
 ---- batch: 110 ----
mean loss: 234.56
train mean loss: 231.34
epoch train time: 0:00:15.111167
elapsed time: 0:19:56.320381
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 04:18:35.200018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.91
 ---- batch: 020 ----
mean loss: 239.29
 ---- batch: 030 ----
mean loss: 227.93
 ---- batch: 040 ----
mean loss: 227.86
 ---- batch: 050 ----
mean loss: 232.84
 ---- batch: 060 ----
mean loss: 236.30
 ---- batch: 070 ----
mean loss: 229.75
 ---- batch: 080 ----
mean loss: 232.12
 ---- batch: 090 ----
mean loss: 237.55
 ---- batch: 100 ----
mean loss: 234.71
 ---- batch: 110 ----
mean loss: 225.08
train mean loss: 232.20
epoch train time: 0:00:15.050472
elapsed time: 0:20:11.371848
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 04:18:50.251436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.63
 ---- batch: 020 ----
mean loss: 224.37
 ---- batch: 030 ----
mean loss: 221.96
 ---- batch: 040 ----
mean loss: 230.66
 ---- batch: 050 ----
mean loss: 236.19
 ---- batch: 060 ----
mean loss: 238.46
 ---- batch: 070 ----
mean loss: 235.65
 ---- batch: 080 ----
mean loss: 234.83
 ---- batch: 090 ----
mean loss: 228.37
 ---- batch: 100 ----
mean loss: 221.42
 ---- batch: 110 ----
mean loss: 229.51
train mean loss: 229.40
epoch train time: 0:00:15.078532
elapsed time: 0:20:26.451296
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 04:19:05.330849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.95
 ---- batch: 020 ----
mean loss: 229.49
 ---- batch: 030 ----
mean loss: 218.27
 ---- batch: 040 ----
mean loss: 231.63
 ---- batch: 050 ----
mean loss: 227.59
 ---- batch: 060 ----
mean loss: 230.71
 ---- batch: 070 ----
mean loss: 234.25
 ---- batch: 080 ----
mean loss: 219.18
 ---- batch: 090 ----
mean loss: 234.32
 ---- batch: 100 ----
mean loss: 217.21
 ---- batch: 110 ----
mean loss: 237.84
train mean loss: 227.36
epoch train time: 0:00:15.198780
elapsed time: 0:20:41.651013
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 04:19:20.530618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.54
 ---- batch: 020 ----
mean loss: 227.48
 ---- batch: 030 ----
mean loss: 220.71
 ---- batch: 040 ----
mean loss: 225.43
 ---- batch: 050 ----
mean loss: 229.37
 ---- batch: 060 ----
mean loss: 225.61
 ---- batch: 070 ----
mean loss: 234.60
 ---- batch: 080 ----
mean loss: 221.76
 ---- batch: 090 ----
mean loss: 225.83
 ---- batch: 100 ----
mean loss: 223.64
 ---- batch: 110 ----
mean loss: 222.86
train mean loss: 225.08
epoch train time: 0:00:15.145542
elapsed time: 0:20:56.797501
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 04:19:35.677074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.07
 ---- batch: 020 ----
mean loss: 225.19
 ---- batch: 030 ----
mean loss: 222.37
 ---- batch: 040 ----
mean loss: 227.44
 ---- batch: 050 ----
mean loss: 226.10
 ---- batch: 060 ----
mean loss: 223.16
 ---- batch: 070 ----
mean loss: 219.55
 ---- batch: 080 ----
mean loss: 239.63
 ---- batch: 090 ----
mean loss: 228.95
 ---- batch: 100 ----
mean loss: 216.76
 ---- batch: 110 ----
mean loss: 233.60
train mean loss: 225.63
epoch train time: 0:00:15.079042
elapsed time: 0:21:11.877550
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 04:19:50.757234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.36
 ---- batch: 020 ----
mean loss: 225.43
 ---- batch: 030 ----
mean loss: 229.93
 ---- batch: 040 ----
mean loss: 223.30
 ---- batch: 050 ----
mean loss: 221.55
 ---- batch: 060 ----
mean loss: 224.62
 ---- batch: 070 ----
mean loss: 224.43
 ---- batch: 080 ----
mean loss: 219.13
 ---- batch: 090 ----
mean loss: 222.14
 ---- batch: 100 ----
mean loss: 217.54
 ---- batch: 110 ----
mean loss: 219.26
train mean loss: 222.43
epoch train time: 0:00:15.066226
elapsed time: 0:21:26.944708
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 04:20:05.824309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.58
 ---- batch: 020 ----
mean loss: 223.26
 ---- batch: 030 ----
mean loss: 221.42
 ---- batch: 040 ----
mean loss: 212.71
 ---- batch: 050 ----
mean loss: 221.87
 ---- batch: 060 ----
mean loss: 224.00
 ---- batch: 070 ----
mean loss: 218.06
 ---- batch: 080 ----
mean loss: 225.34
 ---- batch: 090 ----
mean loss: 227.60
 ---- batch: 100 ----
mean loss: 228.07
 ---- batch: 110 ----
mean loss: 220.94
train mean loss: 222.09
epoch train time: 0:00:15.111475
elapsed time: 0:21:42.057119
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 04:20:20.936667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.47
 ---- batch: 020 ----
mean loss: 220.40
 ---- batch: 030 ----
mean loss: 220.47
 ---- batch: 040 ----
mean loss: 217.67
 ---- batch: 050 ----
mean loss: 213.51
 ---- batch: 060 ----
mean loss: 217.39
 ---- batch: 070 ----
mean loss: 227.00
 ---- batch: 080 ----
mean loss: 228.75
 ---- batch: 090 ----
mean loss: 222.68
 ---- batch: 100 ----
mean loss: 223.89
 ---- batch: 110 ----
mean loss: 215.38
train mean loss: 220.51
epoch train time: 0:00:15.039219
elapsed time: 0:21:57.097202
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 04:20:35.976860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.28
 ---- batch: 020 ----
mean loss: 223.52
 ---- batch: 030 ----
mean loss: 208.23
 ---- batch: 040 ----
mean loss: 226.07
 ---- batch: 050 ----
mean loss: 219.22
 ---- batch: 060 ----
mean loss: 218.64
 ---- batch: 070 ----
mean loss: 223.23
 ---- batch: 080 ----
mean loss: 224.67
 ---- batch: 090 ----
mean loss: 223.23
 ---- batch: 100 ----
mean loss: 215.54
 ---- batch: 110 ----
mean loss: 216.95
train mean loss: 218.84
epoch train time: 0:00:15.011676
elapsed time: 0:22:12.109939
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 04:20:50.989454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.78
 ---- batch: 020 ----
mean loss: 229.20
 ---- batch: 030 ----
mean loss: 212.36
 ---- batch: 040 ----
mean loss: 214.07
 ---- batch: 050 ----
mean loss: 217.27
 ---- batch: 060 ----
mean loss: 213.79
 ---- batch: 070 ----
mean loss: 208.52
 ---- batch: 080 ----
mean loss: 213.55
 ---- batch: 090 ----
mean loss: 222.30
 ---- batch: 100 ----
mean loss: 219.66
 ---- batch: 110 ----
mean loss: 222.46
train mean loss: 216.69
epoch train time: 0:00:15.053740
elapsed time: 0:22:27.164535
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 04:21:06.044240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.11
 ---- batch: 020 ----
mean loss: 213.64
 ---- batch: 030 ----
mean loss: 215.74
 ---- batch: 040 ----
mean loss: 210.71
 ---- batch: 050 ----
mean loss: 213.76
 ---- batch: 060 ----
mean loss: 219.61
 ---- batch: 070 ----
mean loss: 219.17
 ---- batch: 080 ----
mean loss: 229.50
 ---- batch: 090 ----
mean loss: 213.71
 ---- batch: 100 ----
mean loss: 208.03
 ---- batch: 110 ----
mean loss: 215.08
train mean loss: 216.06
epoch train time: 0:00:15.115158
elapsed time: 0:22:42.280772
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 04:21:21.160408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.55
 ---- batch: 020 ----
mean loss: 209.12
 ---- batch: 030 ----
mean loss: 222.19
 ---- batch: 040 ----
mean loss: 216.56
 ---- batch: 050 ----
mean loss: 216.96
 ---- batch: 060 ----
mean loss: 213.58
 ---- batch: 070 ----
mean loss: 212.71
 ---- batch: 080 ----
mean loss: 216.41
 ---- batch: 090 ----
mean loss: 221.54
 ---- batch: 100 ----
mean loss: 221.27
 ---- batch: 110 ----
mean loss: 216.74
train mean loss: 215.40
epoch train time: 0:00:15.062917
elapsed time: 0:22:57.344659
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 04:21:36.224222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.38
 ---- batch: 020 ----
mean loss: 221.31
 ---- batch: 030 ----
mean loss: 214.98
 ---- batch: 040 ----
mean loss: 221.83
 ---- batch: 050 ----
mean loss: 204.10
 ---- batch: 060 ----
mean loss: 207.61
 ---- batch: 070 ----
mean loss: 216.07
 ---- batch: 080 ----
mean loss: 214.34
 ---- batch: 090 ----
mean loss: 214.66
 ---- batch: 100 ----
mean loss: 210.95
 ---- batch: 110 ----
mean loss: 210.63
train mean loss: 213.85
epoch train time: 0:00:15.111093
elapsed time: 0:23:12.456568
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 04:21:51.336160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.39
 ---- batch: 020 ----
mean loss: 209.44
 ---- batch: 030 ----
mean loss: 204.28
 ---- batch: 040 ----
mean loss: 213.42
 ---- batch: 050 ----
mean loss: 205.90
 ---- batch: 060 ----
mean loss: 216.76
 ---- batch: 070 ----
mean loss: 221.72
 ---- batch: 080 ----
mean loss: 215.34
 ---- batch: 090 ----
mean loss: 206.66
 ---- batch: 100 ----
mean loss: 214.61
 ---- batch: 110 ----
mean loss: 214.68
train mean loss: 211.77
epoch train time: 0:00:15.105298
elapsed time: 0:23:27.562797
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 04:22:06.442417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.95
 ---- batch: 020 ----
mean loss: 218.04
 ---- batch: 030 ----
mean loss: 213.44
 ---- batch: 040 ----
mean loss: 209.85
 ---- batch: 050 ----
mean loss: 207.11
 ---- batch: 060 ----
mean loss: 216.65
 ---- batch: 070 ----
mean loss: 212.12
 ---- batch: 080 ----
mean loss: 205.16
 ---- batch: 090 ----
mean loss: 209.21
 ---- batch: 100 ----
mean loss: 206.73
 ---- batch: 110 ----
mean loss: 218.48
train mean loss: 211.96
epoch train time: 0:00:15.032591
elapsed time: 0:23:42.596275
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 04:22:21.475844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.47
 ---- batch: 020 ----
mean loss: 212.21
 ---- batch: 030 ----
mean loss: 206.40
 ---- batch: 040 ----
mean loss: 219.19
 ---- batch: 050 ----
mean loss: 201.90
 ---- batch: 060 ----
mean loss: 203.20
 ---- batch: 070 ----
mean loss: 213.53
 ---- batch: 080 ----
mean loss: 213.17
 ---- batch: 090 ----
mean loss: 209.18
 ---- batch: 100 ----
mean loss: 204.74
 ---- batch: 110 ----
mean loss: 210.90
train mean loss: 209.85
epoch train time: 0:00:15.067607
elapsed time: 0:23:57.664737
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 04:22:36.544300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.59
 ---- batch: 020 ----
mean loss: 220.88
 ---- batch: 030 ----
mean loss: 215.37
 ---- batch: 040 ----
mean loss: 217.73
 ---- batch: 050 ----
mean loss: 210.94
 ---- batch: 060 ----
mean loss: 199.65
 ---- batch: 070 ----
mean loss: 208.96
 ---- batch: 080 ----
mean loss: 198.05
 ---- batch: 090 ----
mean loss: 210.19
 ---- batch: 100 ----
mean loss: 206.51
 ---- batch: 110 ----
mean loss: 203.98
train mean loss: 209.06
epoch train time: 0:00:15.065290
elapsed time: 0:24:12.730806
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 04:22:51.610360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.38
 ---- batch: 020 ----
mean loss: 213.83
 ---- batch: 030 ----
mean loss: 211.75
 ---- batch: 040 ----
mean loss: 205.09
 ---- batch: 050 ----
mean loss: 200.00
 ---- batch: 060 ----
mean loss: 214.09
 ---- batch: 070 ----
mean loss: 217.43
 ---- batch: 080 ----
mean loss: 210.58
 ---- batch: 090 ----
mean loss: 203.54
 ---- batch: 100 ----
mean loss: 208.23
 ---- batch: 110 ----
mean loss: 202.21
train mean loss: 208.17
epoch train time: 0:00:15.053900
elapsed time: 0:24:27.785580
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 04:23:06.665158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.97
 ---- batch: 020 ----
mean loss: 210.28
 ---- batch: 030 ----
mean loss: 213.67
 ---- batch: 040 ----
mean loss: 200.12
 ---- batch: 050 ----
mean loss: 209.06
 ---- batch: 060 ----
mean loss: 202.35
 ---- batch: 070 ----
mean loss: 209.54
 ---- batch: 080 ----
mean loss: 205.41
 ---- batch: 090 ----
mean loss: 204.93
 ---- batch: 100 ----
mean loss: 206.34
 ---- batch: 110 ----
mean loss: 201.56
train mean loss: 206.00
epoch train time: 0:00:15.050769
elapsed time: 0:24:42.837262
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 04:23:21.716870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.71
 ---- batch: 020 ----
mean loss: 206.06
 ---- batch: 030 ----
mean loss: 206.75
 ---- batch: 040 ----
mean loss: 215.09
 ---- batch: 050 ----
mean loss: 211.05
 ---- batch: 060 ----
mean loss: 197.53
 ---- batch: 070 ----
mean loss: 196.44
 ---- batch: 080 ----
mean loss: 201.52
 ---- batch: 090 ----
mean loss: 209.95
 ---- batch: 100 ----
mean loss: 205.74
 ---- batch: 110 ----
mean loss: 205.25
train mean loss: 206.09
epoch train time: 0:00:15.070539
elapsed time: 0:24:57.908778
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 04:23:36.788340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.46
 ---- batch: 020 ----
mean loss: 200.13
 ---- batch: 030 ----
mean loss: 190.53
 ---- batch: 040 ----
mean loss: 212.74
 ---- batch: 050 ----
mean loss: 214.17
 ---- batch: 060 ----
mean loss: 210.95
 ---- batch: 070 ----
mean loss: 208.06
 ---- batch: 080 ----
mean loss: 204.91
 ---- batch: 090 ----
mean loss: 199.28
 ---- batch: 100 ----
mean loss: 203.80
 ---- batch: 110 ----
mean loss: 207.07
train mean loss: 204.71
epoch train time: 0:00:15.028413
elapsed time: 0:25:12.938065
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 04:23:51.817698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.64
 ---- batch: 020 ----
mean loss: 201.57
 ---- batch: 030 ----
mean loss: 203.77
 ---- batch: 040 ----
mean loss: 207.31
 ---- batch: 050 ----
mean loss: 215.17
 ---- batch: 060 ----
mean loss: 196.46
 ---- batch: 070 ----
mean loss: 202.84
 ---- batch: 080 ----
mean loss: 209.09
 ---- batch: 090 ----
mean loss: 205.75
 ---- batch: 100 ----
mean loss: 206.41
 ---- batch: 110 ----
mean loss: 203.17
train mean loss: 204.25
epoch train time: 0:00:15.050969
elapsed time: 0:25:27.990029
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 04:24:06.869587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.13
 ---- batch: 020 ----
mean loss: 205.35
 ---- batch: 030 ----
mean loss: 202.50
 ---- batch: 040 ----
mean loss: 199.43
 ---- batch: 050 ----
mean loss: 205.26
 ---- batch: 060 ----
mean loss: 211.03
 ---- batch: 070 ----
mean loss: 202.43
 ---- batch: 080 ----
mean loss: 208.33
 ---- batch: 090 ----
mean loss: 205.89
 ---- batch: 100 ----
mean loss: 207.83
 ---- batch: 110 ----
mean loss: 199.18
train mean loss: 204.27
epoch train time: 0:00:14.993707
elapsed time: 0:25:42.984719
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 04:24:21.864339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.93
 ---- batch: 020 ----
mean loss: 203.60
 ---- batch: 030 ----
mean loss: 191.14
 ---- batch: 040 ----
mean loss: 207.68
 ---- batch: 050 ----
mean loss: 199.25
 ---- batch: 060 ----
mean loss: 206.46
 ---- batch: 070 ----
mean loss: 196.71
 ---- batch: 080 ----
mean loss: 190.98
 ---- batch: 090 ----
mean loss: 202.87
 ---- batch: 100 ----
mean loss: 204.77
 ---- batch: 110 ----
mean loss: 208.24
train mean loss: 201.83
epoch train time: 0:00:15.125445
elapsed time: 0:25:58.111116
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 04:24:36.990693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.66
 ---- batch: 020 ----
mean loss: 198.56
 ---- batch: 030 ----
mean loss: 210.04
 ---- batch: 040 ----
mean loss: 198.42
 ---- batch: 050 ----
mean loss: 201.33
 ---- batch: 060 ----
mean loss: 203.20
 ---- batch: 070 ----
mean loss: 197.45
 ---- batch: 080 ----
mean loss: 199.51
 ---- batch: 090 ----
mean loss: 197.54
 ---- batch: 100 ----
mean loss: 203.22
 ---- batch: 110 ----
mean loss: 201.43
train mean loss: 200.09
epoch train time: 0:00:15.163955
elapsed time: 0:26:13.276114
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 04:24:52.155723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.54
 ---- batch: 020 ----
mean loss: 204.69
 ---- batch: 030 ----
mean loss: 205.84
 ---- batch: 040 ----
mean loss: 192.23
 ---- batch: 050 ----
mean loss: 202.51
 ---- batch: 060 ----
mean loss: 195.39
 ---- batch: 070 ----
mean loss: 197.24
 ---- batch: 080 ----
mean loss: 203.30
 ---- batch: 090 ----
mean loss: 197.47
 ---- batch: 100 ----
mean loss: 191.00
 ---- batch: 110 ----
mean loss: 204.69
train mean loss: 199.98
epoch train time: 0:00:15.103365
elapsed time: 0:26:28.380436
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 04:25:07.260084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.48
 ---- batch: 020 ----
mean loss: 202.39
 ---- batch: 030 ----
mean loss: 202.19
 ---- batch: 040 ----
mean loss: 196.91
 ---- batch: 050 ----
mean loss: 199.10
 ---- batch: 060 ----
mean loss: 205.62
 ---- batch: 070 ----
mean loss: 191.61
 ---- batch: 080 ----
mean loss: 202.36
 ---- batch: 090 ----
mean loss: 194.09
 ---- batch: 100 ----
mean loss: 197.31
 ---- batch: 110 ----
mean loss: 194.32
train mean loss: 199.10
epoch train time: 0:00:15.159578
elapsed time: 0:26:43.540926
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 04:25:22.420519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.66
 ---- batch: 020 ----
mean loss: 198.77
 ---- batch: 030 ----
mean loss: 205.72
 ---- batch: 040 ----
mean loss: 195.06
 ---- batch: 050 ----
mean loss: 191.93
 ---- batch: 060 ----
mean loss: 202.96
 ---- batch: 070 ----
mean loss: 197.15
 ---- batch: 080 ----
mean loss: 194.07
 ---- batch: 090 ----
mean loss: 193.85
 ---- batch: 100 ----
mean loss: 198.55
 ---- batch: 110 ----
mean loss: 204.01
train mean loss: 198.83
epoch train time: 0:00:15.147605
elapsed time: 0:26:58.689462
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 04:25:37.569020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.96
 ---- batch: 020 ----
mean loss: 200.62
 ---- batch: 030 ----
mean loss: 197.78
 ---- batch: 040 ----
mean loss: 196.02
 ---- batch: 050 ----
mean loss: 197.47
 ---- batch: 060 ----
mean loss: 195.51
 ---- batch: 070 ----
mean loss: 198.46
 ---- batch: 080 ----
mean loss: 192.15
 ---- batch: 090 ----
mean loss: 197.06
 ---- batch: 100 ----
mean loss: 206.54
 ---- batch: 110 ----
mean loss: 211.15
train mean loss: 197.92
epoch train time: 0:00:15.128358
elapsed time: 0:27:13.818711
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 04:25:52.698466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.08
 ---- batch: 020 ----
mean loss: 193.56
 ---- batch: 030 ----
mean loss: 203.01
 ---- batch: 040 ----
mean loss: 194.48
 ---- batch: 050 ----
mean loss: 190.07
 ---- batch: 060 ----
mean loss: 204.23
 ---- batch: 070 ----
mean loss: 197.10
 ---- batch: 080 ----
mean loss: 192.19
 ---- batch: 090 ----
mean loss: 195.54
 ---- batch: 100 ----
mean loss: 200.47
 ---- batch: 110 ----
mean loss: 194.47
train mean loss: 197.28
epoch train time: 0:00:15.195759
elapsed time: 0:27:29.015840
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 04:26:07.895420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.28
 ---- batch: 020 ----
mean loss: 210.88
 ---- batch: 030 ----
mean loss: 191.07
 ---- batch: 040 ----
mean loss: 200.63
 ---- batch: 050 ----
mean loss: 201.84
 ---- batch: 060 ----
mean loss: 198.84
 ---- batch: 070 ----
mean loss: 189.73
 ---- batch: 080 ----
mean loss: 206.36
 ---- batch: 090 ----
mean loss: 205.61
 ---- batch: 100 ----
mean loss: 190.69
 ---- batch: 110 ----
mean loss: 198.59
train mean loss: 198.79
epoch train time: 0:00:15.284637
elapsed time: 0:27:44.301426
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 04:26:23.181007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.20
 ---- batch: 020 ----
mean loss: 198.28
 ---- batch: 030 ----
mean loss: 198.66
 ---- batch: 040 ----
mean loss: 191.80
 ---- batch: 050 ----
mean loss: 202.36
 ---- batch: 060 ----
mean loss: 186.38
 ---- batch: 070 ----
mean loss: 200.11
 ---- batch: 080 ----
mean loss: 202.11
 ---- batch: 090 ----
mean loss: 197.69
 ---- batch: 100 ----
mean loss: 181.25
 ---- batch: 110 ----
mean loss: 196.41
train mean loss: 195.94
epoch train time: 0:00:15.282488
elapsed time: 0:27:59.584827
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 04:26:38.464413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.26
 ---- batch: 020 ----
mean loss: 200.28
 ---- batch: 030 ----
mean loss: 187.03
 ---- batch: 040 ----
mean loss: 190.75
 ---- batch: 050 ----
mean loss: 196.53
 ---- batch: 060 ----
mean loss: 194.86
 ---- batch: 070 ----
mean loss: 195.91
 ---- batch: 080 ----
mean loss: 198.69
 ---- batch: 090 ----
mean loss: 204.37
 ---- batch: 100 ----
mean loss: 199.07
 ---- batch: 110 ----
mean loss: 194.56
train mean loss: 195.85
epoch train time: 0:00:15.423818
elapsed time: 0:28:15.009689
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 04:26:53.889386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.02
 ---- batch: 020 ----
mean loss: 190.84
 ---- batch: 030 ----
mean loss: 196.95
 ---- batch: 040 ----
mean loss: 192.34
 ---- batch: 050 ----
mean loss: 190.89
 ---- batch: 060 ----
mean loss: 196.52
 ---- batch: 070 ----
mean loss: 203.56
 ---- batch: 080 ----
mean loss: 198.72
 ---- batch: 090 ----
mean loss: 183.38
 ---- batch: 100 ----
mean loss: 194.51
 ---- batch: 110 ----
mean loss: 195.59
train mean loss: 194.17
epoch train time: 0:00:15.599918
elapsed time: 0:28:30.610601
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 04:27:09.490161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.05
 ---- batch: 020 ----
mean loss: 194.26
 ---- batch: 030 ----
mean loss: 190.74
 ---- batch: 040 ----
mean loss: 182.21
 ---- batch: 050 ----
mean loss: 201.58
 ---- batch: 060 ----
mean loss: 190.35
 ---- batch: 070 ----
mean loss: 194.70
 ---- batch: 080 ----
mean loss: 200.04
 ---- batch: 090 ----
mean loss: 188.99
 ---- batch: 100 ----
mean loss: 188.31
 ---- batch: 110 ----
mean loss: 200.73
train mean loss: 193.72
epoch train time: 0:00:15.518857
elapsed time: 0:28:46.130458
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 04:27:25.010048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.69
 ---- batch: 020 ----
mean loss: 188.23
 ---- batch: 030 ----
mean loss: 191.19
 ---- batch: 040 ----
mean loss: 195.26
 ---- batch: 050 ----
mean loss: 188.10
 ---- batch: 060 ----
mean loss: 197.45
 ---- batch: 070 ----
mean loss: 197.24
 ---- batch: 080 ----
mean loss: 195.20
 ---- batch: 090 ----
mean loss: 198.39
 ---- batch: 100 ----
mean loss: 187.26
 ---- batch: 110 ----
mean loss: 193.71
train mean loss: 193.28
epoch train time: 0:00:15.575776
elapsed time: 0:29:01.707256
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 04:27:40.586937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.97
 ---- batch: 020 ----
mean loss: 186.06
 ---- batch: 030 ----
mean loss: 193.47
 ---- batch: 040 ----
mean loss: 184.24
 ---- batch: 050 ----
mean loss: 192.87
 ---- batch: 060 ----
mean loss: 194.44
 ---- batch: 070 ----
mean loss: 185.83
 ---- batch: 080 ----
mean loss: 197.19
 ---- batch: 090 ----
mean loss: 194.85
 ---- batch: 100 ----
mean loss: 193.73
 ---- batch: 110 ----
mean loss: 199.44
train mean loss: 192.29
epoch train time: 0:00:15.662627
elapsed time: 0:29:17.370966
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 04:27:56.250519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.17
 ---- batch: 020 ----
mean loss: 203.54
 ---- batch: 030 ----
mean loss: 182.88
 ---- batch: 040 ----
mean loss: 194.27
 ---- batch: 050 ----
mean loss: 199.26
 ---- batch: 060 ----
mean loss: 205.43
 ---- batch: 070 ----
mean loss: 190.07
 ---- batch: 080 ----
mean loss: 184.18
 ---- batch: 090 ----
mean loss: 188.20
 ---- batch: 100 ----
mean loss: 188.80
 ---- batch: 110 ----
mean loss: 187.47
train mean loss: 192.14
epoch train time: 0:00:15.645202
elapsed time: 0:29:33.017120
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 04:28:11.896792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.49
 ---- batch: 020 ----
mean loss: 199.92
 ---- batch: 030 ----
mean loss: 188.74
 ---- batch: 040 ----
mean loss: 186.19
 ---- batch: 050 ----
mean loss: 193.65
 ---- batch: 060 ----
mean loss: 188.22
 ---- batch: 070 ----
mean loss: 187.78
 ---- batch: 080 ----
mean loss: 203.67
 ---- batch: 090 ----
mean loss: 195.18
 ---- batch: 100 ----
mean loss: 178.60
 ---- batch: 110 ----
mean loss: 185.27
train mean loss: 191.46
epoch train time: 0:00:15.774203
elapsed time: 0:29:48.792426
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 04:28:27.672067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.70
 ---- batch: 020 ----
mean loss: 188.85
 ---- batch: 030 ----
mean loss: 189.38
 ---- batch: 040 ----
mean loss: 190.90
 ---- batch: 050 ----
mean loss: 186.65
 ---- batch: 060 ----
mean loss: 190.77
 ---- batch: 070 ----
mean loss: 207.35
 ---- batch: 080 ----
mean loss: 202.60
 ---- batch: 090 ----
mean loss: 188.12
 ---- batch: 100 ----
mean loss: 199.77
 ---- batch: 110 ----
mean loss: 187.61
train mean loss: 192.63
epoch train time: 0:00:15.746443
elapsed time: 0:30:04.539758
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 04:28:43.419333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.62
 ---- batch: 020 ----
mean loss: 191.89
 ---- batch: 030 ----
mean loss: 188.00
 ---- batch: 040 ----
mean loss: 186.96
 ---- batch: 050 ----
mean loss: 186.77
 ---- batch: 060 ----
mean loss: 195.99
 ---- batch: 070 ----
mean loss: 194.96
 ---- batch: 080 ----
mean loss: 198.75
 ---- batch: 090 ----
mean loss: 185.77
 ---- batch: 100 ----
mean loss: 190.78
 ---- batch: 110 ----
mean loss: 190.96
train mean loss: 191.36
epoch train time: 0:00:15.492129
elapsed time: 0:30:20.032824
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 04:28:58.912439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.53
 ---- batch: 020 ----
mean loss: 188.70
 ---- batch: 030 ----
mean loss: 187.81
 ---- batch: 040 ----
mean loss: 186.92
 ---- batch: 050 ----
mean loss: 190.31
 ---- batch: 060 ----
mean loss: 188.69
 ---- batch: 070 ----
mean loss: 188.59
 ---- batch: 080 ----
mean loss: 196.26
 ---- batch: 090 ----
mean loss: 196.69
 ---- batch: 100 ----
mean loss: 195.11
 ---- batch: 110 ----
mean loss: 190.60
train mean loss: 190.40
epoch train time: 0:00:15.682882
elapsed time: 0:30:35.716739
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 04:29:14.596314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.60
 ---- batch: 020 ----
mean loss: 189.33
 ---- batch: 030 ----
mean loss: 186.40
 ---- batch: 040 ----
mean loss: 188.04
 ---- batch: 050 ----
mean loss: 193.79
 ---- batch: 060 ----
mean loss: 198.33
 ---- batch: 070 ----
mean loss: 187.85
 ---- batch: 080 ----
mean loss: 189.29
 ---- batch: 090 ----
mean loss: 194.85
 ---- batch: 100 ----
mean loss: 188.14
 ---- batch: 110 ----
mean loss: 189.56
train mean loss: 189.62
epoch train time: 0:00:15.483978
elapsed time: 0:30:51.201784
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 04:29:30.081363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.32
 ---- batch: 020 ----
mean loss: 178.02
 ---- batch: 030 ----
mean loss: 195.80
 ---- batch: 040 ----
mean loss: 188.55
 ---- batch: 050 ----
mean loss: 184.75
 ---- batch: 060 ----
mean loss: 181.74
 ---- batch: 070 ----
mean loss: 195.85
 ---- batch: 080 ----
mean loss: 179.82
 ---- batch: 090 ----
mean loss: 194.55
 ---- batch: 100 ----
mean loss: 190.67
 ---- batch: 110 ----
mean loss: 192.31
train mean loss: 188.40
epoch train time: 0:00:15.551385
elapsed time: 0:31:06.754227
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 04:29:45.633993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.71
 ---- batch: 020 ----
mean loss: 189.05
 ---- batch: 030 ----
mean loss: 185.10
 ---- batch: 040 ----
mean loss: 188.88
 ---- batch: 050 ----
mean loss: 180.08
 ---- batch: 060 ----
mean loss: 186.69
 ---- batch: 070 ----
mean loss: 189.95
 ---- batch: 080 ----
mean loss: 189.17
 ---- batch: 090 ----
mean loss: 188.94
 ---- batch: 100 ----
mean loss: 191.38
 ---- batch: 110 ----
mean loss: 192.09
train mean loss: 188.05
epoch train time: 0:00:15.671189
elapsed time: 0:31:22.426631
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 04:30:01.306230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.66
 ---- batch: 020 ----
mean loss: 197.96
 ---- batch: 030 ----
mean loss: 183.30
 ---- batch: 040 ----
mean loss: 193.28
 ---- batch: 050 ----
mean loss: 192.13
 ---- batch: 060 ----
mean loss: 188.24
 ---- batch: 070 ----
mean loss: 191.57
 ---- batch: 080 ----
mean loss: 190.70
 ---- batch: 090 ----
mean loss: 180.65
 ---- batch: 100 ----
mean loss: 191.58
 ---- batch: 110 ----
mean loss: 181.19
train mean loss: 189.48
epoch train time: 0:00:15.661521
elapsed time: 0:31:38.089175
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 04:30:16.968801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.03
 ---- batch: 020 ----
mean loss: 187.65
 ---- batch: 030 ----
mean loss: 186.16
 ---- batch: 040 ----
mean loss: 188.42
 ---- batch: 050 ----
mean loss: 187.41
 ---- batch: 060 ----
mean loss: 186.29
 ---- batch: 070 ----
mean loss: 185.22
 ---- batch: 080 ----
mean loss: 189.34
 ---- batch: 090 ----
mean loss: 195.80
 ---- batch: 100 ----
mean loss: 191.70
 ---- batch: 110 ----
mean loss: 181.80
train mean loss: 188.60
epoch train time: 0:00:15.768530
elapsed time: 0:31:53.858632
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 04:30:32.738371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.52
 ---- batch: 020 ----
mean loss: 177.60
 ---- batch: 030 ----
mean loss: 186.05
 ---- batch: 040 ----
mean loss: 192.88
 ---- batch: 050 ----
mean loss: 194.65
 ---- batch: 060 ----
mean loss: 188.89
 ---- batch: 070 ----
mean loss: 192.91
 ---- batch: 080 ----
mean loss: 182.36
 ---- batch: 090 ----
mean loss: 188.93
 ---- batch: 100 ----
mean loss: 180.77
 ---- batch: 110 ----
mean loss: 187.05
train mean loss: 187.15
epoch train time: 0:00:15.456133
elapsed time: 0:32:09.315875
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 04:30:48.195472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.83
 ---- batch: 020 ----
mean loss: 180.13
 ---- batch: 030 ----
mean loss: 186.46
 ---- batch: 040 ----
mean loss: 194.41
 ---- batch: 050 ----
mean loss: 181.13
 ---- batch: 060 ----
mean loss: 189.92
 ---- batch: 070 ----
mean loss: 193.77
 ---- batch: 080 ----
mean loss: 194.46
 ---- batch: 090 ----
mean loss: 180.84
 ---- batch: 100 ----
mean loss: 183.53
 ---- batch: 110 ----
mean loss: 186.81
train mean loss: 187.21
epoch train time: 0:00:15.502057
elapsed time: 0:32:24.818932
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 04:31:03.698660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.87
 ---- batch: 020 ----
mean loss: 183.42
 ---- batch: 030 ----
mean loss: 185.59
 ---- batch: 040 ----
mean loss: 190.46
 ---- batch: 050 ----
mean loss: 187.35
 ---- batch: 060 ----
mean loss: 190.37
 ---- batch: 070 ----
mean loss: 183.97
 ---- batch: 080 ----
mean loss: 194.51
 ---- batch: 090 ----
mean loss: 186.96
 ---- batch: 100 ----
mean loss: 180.68
 ---- batch: 110 ----
mean loss: 185.51
train mean loss: 187.06
epoch train time: 0:00:15.436568
elapsed time: 0:32:40.256840
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 04:31:19.136377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.03
 ---- batch: 020 ----
mean loss: 191.14
 ---- batch: 030 ----
mean loss: 187.70
 ---- batch: 040 ----
mean loss: 171.17
 ---- batch: 050 ----
mean loss: 198.84
 ---- batch: 060 ----
mean loss: 183.48
 ---- batch: 070 ----
mean loss: 185.75
 ---- batch: 080 ----
mean loss: 184.53
 ---- batch: 090 ----
mean loss: 190.10
 ---- batch: 100 ----
mean loss: 181.61
 ---- batch: 110 ----
mean loss: 189.32
train mean loss: 186.70
epoch train time: 0:00:15.680992
elapsed time: 0:32:55.938769
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 04:31:34.818369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.75
 ---- batch: 020 ----
mean loss: 180.06
 ---- batch: 030 ----
mean loss: 185.58
 ---- batch: 040 ----
mean loss: 185.68
 ---- batch: 050 ----
mean loss: 191.18
 ---- batch: 060 ----
mean loss: 189.40
 ---- batch: 070 ----
mean loss: 187.12
 ---- batch: 080 ----
mean loss: 186.99
 ---- batch: 090 ----
mean loss: 176.34
 ---- batch: 100 ----
mean loss: 193.47
 ---- batch: 110 ----
mean loss: 178.87
train mean loss: 186.24
epoch train time: 0:00:16.154253
elapsed time: 0:33:12.093950
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 04:31:50.973627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.37
 ---- batch: 020 ----
mean loss: 193.58
 ---- batch: 030 ----
mean loss: 187.63
 ---- batch: 040 ----
mean loss: 184.82
 ---- batch: 050 ----
mean loss: 190.85
 ---- batch: 060 ----
mean loss: 190.43
 ---- batch: 070 ----
mean loss: 181.34
 ---- batch: 080 ----
mean loss: 182.07
 ---- batch: 090 ----
mean loss: 180.55
 ---- batch: 100 ----
mean loss: 182.14
 ---- batch: 110 ----
mean loss: 186.26
train mean loss: 185.73
epoch train time: 0:00:16.100194
elapsed time: 0:33:28.195139
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 04:32:07.074757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.34
 ---- batch: 020 ----
mean loss: 184.58
 ---- batch: 030 ----
mean loss: 184.81
 ---- batch: 040 ----
mean loss: 191.09
 ---- batch: 050 ----
mean loss: 177.28
 ---- batch: 060 ----
mean loss: 181.33
 ---- batch: 070 ----
mean loss: 191.28
 ---- batch: 080 ----
mean loss: 187.77
 ---- batch: 090 ----
mean loss: 185.63
 ---- batch: 100 ----
mean loss: 184.53
 ---- batch: 110 ----
mean loss: 180.83
train mean loss: 184.40
epoch train time: 0:00:16.167759
elapsed time: 0:33:44.363850
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 04:32:23.243458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.91
 ---- batch: 020 ----
mean loss: 183.62
 ---- batch: 030 ----
mean loss: 185.09
 ---- batch: 040 ----
mean loss: 186.77
 ---- batch: 050 ----
mean loss: 186.01
 ---- batch: 060 ----
mean loss: 180.77
 ---- batch: 070 ----
mean loss: 182.33
 ---- batch: 080 ----
mean loss: 179.38
 ---- batch: 090 ----
mean loss: 179.04
 ---- batch: 100 ----
mean loss: 189.86
 ---- batch: 110 ----
mean loss: 185.83
train mean loss: 184.10
epoch train time: 0:00:15.984481
elapsed time: 0:34:00.349412
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 04:32:39.229072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.71
 ---- batch: 020 ----
mean loss: 183.62
 ---- batch: 030 ----
mean loss: 186.02
 ---- batch: 040 ----
mean loss: 187.08
 ---- batch: 050 ----
mean loss: 183.23
 ---- batch: 060 ----
mean loss: 186.53
 ---- batch: 070 ----
mean loss: 192.54
 ---- batch: 080 ----
mean loss: 179.11
 ---- batch: 090 ----
mean loss: 177.85
 ---- batch: 100 ----
mean loss: 181.99
 ---- batch: 110 ----
mean loss: 177.53
train mean loss: 183.70
epoch train time: 0:00:16.052396
elapsed time: 0:34:16.402872
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 04:32:55.282477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.40
 ---- batch: 020 ----
mean loss: 189.65
 ---- batch: 030 ----
mean loss: 190.88
 ---- batch: 040 ----
mean loss: 185.97
 ---- batch: 050 ----
mean loss: 190.89
 ---- batch: 060 ----
mean loss: 178.46
 ---- batch: 070 ----
mean loss: 179.15
 ---- batch: 080 ----
mean loss: 179.37
 ---- batch: 090 ----
mean loss: 189.58
 ---- batch: 100 ----
mean loss: 177.54
 ---- batch: 110 ----
mean loss: 191.24
train mean loss: 187.18
epoch train time: 0:00:16.005702
elapsed time: 0:34:32.409487
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 04:33:11.289096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.62
 ---- batch: 020 ----
mean loss: 191.31
 ---- batch: 030 ----
mean loss: 192.25
 ---- batch: 040 ----
mean loss: 184.38
 ---- batch: 050 ----
mean loss: 185.86
 ---- batch: 060 ----
mean loss: 185.43
 ---- batch: 070 ----
mean loss: 181.37
 ---- batch: 080 ----
mean loss: 176.95
 ---- batch: 090 ----
mean loss: 181.35
 ---- batch: 100 ----
mean loss: 185.76
 ---- batch: 110 ----
mean loss: 181.27
train mean loss: 183.99
epoch train time: 0:00:15.998445
elapsed time: 0:34:48.409033
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 04:33:27.288636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.37
 ---- batch: 020 ----
mean loss: 192.96
 ---- batch: 030 ----
mean loss: 185.28
 ---- batch: 040 ----
mean loss: 184.74
 ---- batch: 050 ----
mean loss: 177.04
 ---- batch: 060 ----
mean loss: 186.87
 ---- batch: 070 ----
mean loss: 182.37
 ---- batch: 080 ----
mean loss: 178.87
 ---- batch: 090 ----
mean loss: 186.37
 ---- batch: 100 ----
mean loss: 176.84
 ---- batch: 110 ----
mean loss: 183.15
train mean loss: 183.16
epoch train time: 0:00:16.080122
elapsed time: 0:35:04.490209
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 04:33:43.369917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.41
 ---- batch: 020 ----
mean loss: 184.99
 ---- batch: 030 ----
mean loss: 185.79
 ---- batch: 040 ----
mean loss: 180.92
 ---- batch: 050 ----
mean loss: 175.66
 ---- batch: 060 ----
mean loss: 179.25
 ---- batch: 070 ----
mean loss: 186.97
 ---- batch: 080 ----
mean loss: 186.47
 ---- batch: 090 ----
mean loss: 186.77
 ---- batch: 100 ----
mean loss: 176.06
 ---- batch: 110 ----
mean loss: 183.84
train mean loss: 183.71
epoch train time: 0:00:15.999412
elapsed time: 0:35:20.490877
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 04:33:59.370470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.94
 ---- batch: 020 ----
mean loss: 185.79
 ---- batch: 030 ----
mean loss: 182.63
 ---- batch: 040 ----
mean loss: 183.80
 ---- batch: 050 ----
mean loss: 182.91
 ---- batch: 060 ----
mean loss: 186.37
 ---- batch: 070 ----
mean loss: 180.34
 ---- batch: 080 ----
mean loss: 182.39
 ---- batch: 090 ----
mean loss: 170.02
 ---- batch: 100 ----
mean loss: 188.88
 ---- batch: 110 ----
mean loss: 186.76
train mean loss: 181.93
epoch train time: 0:00:16.105043
elapsed time: 0:35:36.597060
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 04:34:15.476708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.18
 ---- batch: 020 ----
mean loss: 191.55
 ---- batch: 030 ----
mean loss: 172.62
 ---- batch: 040 ----
mean loss: 193.86
 ---- batch: 050 ----
mean loss: 173.47
 ---- batch: 060 ----
mean loss: 185.55
 ---- batch: 070 ----
mean loss: 172.61
 ---- batch: 080 ----
mean loss: 182.24
 ---- batch: 090 ----
mean loss: 173.43
 ---- batch: 100 ----
mean loss: 185.78
 ---- batch: 110 ----
mean loss: 178.86
train mean loss: 182.25
epoch train time: 0:00:16.048352
elapsed time: 0:35:52.646493
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 04:34:31.526118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.91
 ---- batch: 020 ----
mean loss: 184.67
 ---- batch: 030 ----
mean loss: 184.67
 ---- batch: 040 ----
mean loss: 187.38
 ---- batch: 050 ----
mean loss: 188.80
 ---- batch: 060 ----
mean loss: 193.01
 ---- batch: 070 ----
mean loss: 177.25
 ---- batch: 080 ----
mean loss: 171.55
 ---- batch: 090 ----
mean loss: 179.75
 ---- batch: 100 ----
mean loss: 169.99
 ---- batch: 110 ----
mean loss: 185.22
train mean loss: 181.90
epoch train time: 0:00:15.962436
elapsed time: 0:36:08.610069
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 04:34:47.489683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.59
 ---- batch: 020 ----
mean loss: 182.48
 ---- batch: 030 ----
mean loss: 187.33
 ---- batch: 040 ----
mean loss: 191.11
 ---- batch: 050 ----
mean loss: 185.95
 ---- batch: 060 ----
mean loss: 175.14
 ---- batch: 070 ----
mean loss: 182.87
 ---- batch: 080 ----
mean loss: 181.06
 ---- batch: 090 ----
mean loss: 187.99
 ---- batch: 100 ----
mean loss: 185.84
 ---- batch: 110 ----
mean loss: 181.67
train mean loss: 183.58
epoch train time: 0:00:15.758878
elapsed time: 0:36:24.369937
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 04:35:03.249574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.37
 ---- batch: 020 ----
mean loss: 184.79
 ---- batch: 030 ----
mean loss: 178.24
 ---- batch: 040 ----
mean loss: 179.65
 ---- batch: 050 ----
mean loss: 180.53
 ---- batch: 060 ----
mean loss: 180.15
 ---- batch: 070 ----
mean loss: 179.86
 ---- batch: 080 ----
mean loss: 182.10
 ---- batch: 090 ----
mean loss: 183.87
 ---- batch: 100 ----
mean loss: 177.40
 ---- batch: 110 ----
mean loss: 175.76
train mean loss: 181.21
epoch train time: 0:00:15.481616
elapsed time: 0:36:39.852638
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 04:35:18.732267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.51
 ---- batch: 020 ----
mean loss: 192.78
 ---- batch: 030 ----
mean loss: 179.82
 ---- batch: 040 ----
mean loss: 186.29
 ---- batch: 050 ----
mean loss: 181.33
 ---- batch: 060 ----
mean loss: 184.73
 ---- batch: 070 ----
mean loss: 179.99
 ---- batch: 080 ----
mean loss: 177.57
 ---- batch: 090 ----
mean loss: 168.87
 ---- batch: 100 ----
mean loss: 187.39
 ---- batch: 110 ----
mean loss: 192.55
train mean loss: 182.52
epoch train time: 0:00:15.521438
elapsed time: 0:36:55.375107
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 04:35:34.254685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.69
 ---- batch: 020 ----
mean loss: 176.63
 ---- batch: 030 ----
mean loss: 175.08
 ---- batch: 040 ----
mean loss: 180.83
 ---- batch: 050 ----
mean loss: 186.20
 ---- batch: 060 ----
mean loss: 174.02
 ---- batch: 070 ----
mean loss: 184.91
 ---- batch: 080 ----
mean loss: 188.69
 ---- batch: 090 ----
mean loss: 189.70
 ---- batch: 100 ----
mean loss: 184.33
 ---- batch: 110 ----
mean loss: 171.17
train mean loss: 181.64
epoch train time: 0:00:15.545738
elapsed time: 0:37:10.921669
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 04:35:49.801247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.74
 ---- batch: 020 ----
mean loss: 179.35
 ---- batch: 030 ----
mean loss: 181.38
 ---- batch: 040 ----
mean loss: 183.53
 ---- batch: 050 ----
mean loss: 180.49
 ---- batch: 060 ----
mean loss: 177.20
 ---- batch: 070 ----
mean loss: 177.98
 ---- batch: 080 ----
mean loss: 186.97
 ---- batch: 090 ----
mean loss: 181.63
 ---- batch: 100 ----
mean loss: 181.92
 ---- batch: 110 ----
mean loss: 179.33
train mean loss: 180.89
epoch train time: 0:00:15.512346
elapsed time: 0:37:26.434953
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 04:36:05.314507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.20
 ---- batch: 020 ----
mean loss: 186.00
 ---- batch: 030 ----
mean loss: 188.11
 ---- batch: 040 ----
mean loss: 178.61
 ---- batch: 050 ----
mean loss: 187.46
 ---- batch: 060 ----
mean loss: 183.19
 ---- batch: 070 ----
mean loss: 183.52
 ---- batch: 080 ----
mean loss: 179.73
 ---- batch: 090 ----
mean loss: 175.83
 ---- batch: 100 ----
mean loss: 177.26
 ---- batch: 110 ----
mean loss: 178.65
train mean loss: 180.87
epoch train time: 0:00:15.543341
elapsed time: 0:37:41.979262
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 04:36:20.858812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.45
 ---- batch: 020 ----
mean loss: 187.23
 ---- batch: 030 ----
mean loss: 176.21
 ---- batch: 040 ----
mean loss: 182.15
 ---- batch: 050 ----
mean loss: 176.37
 ---- batch: 060 ----
mean loss: 179.19
 ---- batch: 070 ----
mean loss: 183.89
 ---- batch: 080 ----
mean loss: 181.09
 ---- batch: 090 ----
mean loss: 180.71
 ---- batch: 100 ----
mean loss: 177.85
 ---- batch: 110 ----
mean loss: 185.62
train mean loss: 181.91
epoch train time: 0:00:15.562519
elapsed time: 0:37:57.542730
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 04:36:36.422325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.92
 ---- batch: 020 ----
mean loss: 185.80
 ---- batch: 030 ----
mean loss: 178.64
 ---- batch: 040 ----
mean loss: 175.71
 ---- batch: 050 ----
mean loss: 187.66
 ---- batch: 060 ----
mean loss: 176.94
 ---- batch: 070 ----
mean loss: 179.92
 ---- batch: 080 ----
mean loss: 183.37
 ---- batch: 090 ----
mean loss: 185.46
 ---- batch: 100 ----
mean loss: 189.03
 ---- batch: 110 ----
mean loss: 172.60
train mean loss: 181.11
epoch train time: 0:00:15.539040
elapsed time: 0:38:13.082768
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 04:36:51.962526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.52
 ---- batch: 020 ----
mean loss: 183.02
 ---- batch: 030 ----
mean loss: 183.55
 ---- batch: 040 ----
mean loss: 179.81
 ---- batch: 050 ----
mean loss: 178.82
 ---- batch: 060 ----
mean loss: 175.63
 ---- batch: 070 ----
mean loss: 179.36
 ---- batch: 080 ----
mean loss: 187.66
 ---- batch: 090 ----
mean loss: 179.55
 ---- batch: 100 ----
mean loss: 185.59
 ---- batch: 110 ----
mean loss: 174.71
train mean loss: 180.24
epoch train time: 0:00:15.584964
elapsed time: 0:38:28.669149
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 04:37:07.548763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.85
 ---- batch: 020 ----
mean loss: 178.76
 ---- batch: 030 ----
mean loss: 175.83
 ---- batch: 040 ----
mean loss: 170.33
 ---- batch: 050 ----
mean loss: 173.99
 ---- batch: 060 ----
mean loss: 182.44
 ---- batch: 070 ----
mean loss: 185.84
 ---- batch: 080 ----
mean loss: 183.58
 ---- batch: 090 ----
mean loss: 180.56
 ---- batch: 100 ----
mean loss: 184.43
 ---- batch: 110 ----
mean loss: 176.87
train mean loss: 180.15
epoch train time: 0:00:15.638181
elapsed time: 0:38:44.308393
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 04:37:23.188127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.86
 ---- batch: 020 ----
mean loss: 180.30
 ---- batch: 030 ----
mean loss: 184.87
 ---- batch: 040 ----
mean loss: 174.89
 ---- batch: 050 ----
mean loss: 178.42
 ---- batch: 060 ----
mean loss: 187.06
 ---- batch: 070 ----
mean loss: 176.94
 ---- batch: 080 ----
mean loss: 184.01
 ---- batch: 090 ----
mean loss: 180.33
 ---- batch: 100 ----
mean loss: 174.28
 ---- batch: 110 ----
mean loss: 179.16
train mean loss: 180.01
epoch train time: 0:00:15.607984
elapsed time: 0:38:59.917480
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 04:37:38.797115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.52
 ---- batch: 020 ----
mean loss: 176.45
 ---- batch: 030 ----
mean loss: 188.81
 ---- batch: 040 ----
mean loss: 186.24
 ---- batch: 050 ----
mean loss: 174.39
 ---- batch: 060 ----
mean loss: 179.25
 ---- batch: 070 ----
mean loss: 180.22
 ---- batch: 080 ----
mean loss: 179.53
 ---- batch: 090 ----
mean loss: 176.36
 ---- batch: 100 ----
mean loss: 179.95
 ---- batch: 110 ----
mean loss: 171.96
train mean loss: 179.91
epoch train time: 0:00:15.592180
elapsed time: 0:39:15.510720
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 04:37:54.390317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.95
 ---- batch: 020 ----
mean loss: 176.66
 ---- batch: 030 ----
mean loss: 176.30
 ---- batch: 040 ----
mean loss: 178.11
 ---- batch: 050 ----
mean loss: 185.69
 ---- batch: 060 ----
mean loss: 180.07
 ---- batch: 070 ----
mean loss: 173.95
 ---- batch: 080 ----
mean loss: 184.83
 ---- batch: 090 ----
mean loss: 180.85
 ---- batch: 100 ----
mean loss: 180.72
 ---- batch: 110 ----
mean loss: 174.13
train mean loss: 180.10
epoch train time: 0:00:15.579277
elapsed time: 0:39:31.090969
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 04:38:09.970534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.72
 ---- batch: 020 ----
mean loss: 176.46
 ---- batch: 030 ----
mean loss: 180.34
 ---- batch: 040 ----
mean loss: 178.75
 ---- batch: 050 ----
mean loss: 181.04
 ---- batch: 060 ----
mean loss: 183.80
 ---- batch: 070 ----
mean loss: 185.42
 ---- batch: 080 ----
mean loss: 175.58
 ---- batch: 090 ----
mean loss: 171.03
 ---- batch: 100 ----
mean loss: 178.27
 ---- batch: 110 ----
mean loss: 185.57
train mean loss: 180.48
epoch train time: 0:00:15.571742
elapsed time: 0:39:46.663553
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 04:38:25.543147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.70
 ---- batch: 020 ----
mean loss: 176.56
 ---- batch: 030 ----
mean loss: 179.38
 ---- batch: 040 ----
mean loss: 175.95
 ---- batch: 050 ----
mean loss: 182.90
 ---- batch: 060 ----
mean loss: 178.92
 ---- batch: 070 ----
mean loss: 179.38
 ---- batch: 080 ----
mean loss: 172.59
 ---- batch: 090 ----
mean loss: 180.44
 ---- batch: 100 ----
mean loss: 175.80
 ---- batch: 110 ----
mean loss: 185.38
train mean loss: 178.66
epoch train time: 0:00:15.597138
elapsed time: 0:40:02.261717
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 04:38:41.141353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.39
 ---- batch: 020 ----
mean loss: 175.32
 ---- batch: 030 ----
mean loss: 173.26
 ---- batch: 040 ----
mean loss: 174.00
 ---- batch: 050 ----
mean loss: 172.09
 ---- batch: 060 ----
mean loss: 177.60
 ---- batch: 070 ----
mean loss: 187.69
 ---- batch: 080 ----
mean loss: 174.36
 ---- batch: 090 ----
mean loss: 182.05
 ---- batch: 100 ----
mean loss: 175.44
 ---- batch: 110 ----
mean loss: 186.56
train mean loss: 178.32
epoch train time: 0:00:15.631910
elapsed time: 0:40:17.894605
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 04:38:56.774218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.08
 ---- batch: 020 ----
mean loss: 173.55
 ---- batch: 030 ----
mean loss: 190.58
 ---- batch: 040 ----
mean loss: 188.04
 ---- batch: 050 ----
mean loss: 175.28
 ---- batch: 060 ----
mean loss: 176.25
 ---- batch: 070 ----
mean loss: 173.32
 ---- batch: 080 ----
mean loss: 174.46
 ---- batch: 090 ----
mean loss: 174.57
 ---- batch: 100 ----
mean loss: 179.13
 ---- batch: 110 ----
mean loss: 175.83
train mean loss: 178.61
epoch train time: 0:00:15.630616
elapsed time: 0:40:33.526223
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 04:39:12.405847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.20
 ---- batch: 020 ----
mean loss: 174.94
 ---- batch: 030 ----
mean loss: 181.11
 ---- batch: 040 ----
mean loss: 177.87
 ---- batch: 050 ----
mean loss: 169.90
 ---- batch: 060 ----
mean loss: 185.57
 ---- batch: 070 ----
mean loss: 169.82
 ---- batch: 080 ----
mean loss: 175.71
 ---- batch: 090 ----
mean loss: 186.65
 ---- batch: 100 ----
mean loss: 184.72
 ---- batch: 110 ----
mean loss: 183.18
train mean loss: 178.61
epoch train time: 0:00:15.632745
elapsed time: 0:40:49.159935
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 04:39:28.039506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.97
 ---- batch: 020 ----
mean loss: 175.28
 ---- batch: 030 ----
mean loss: 179.15
 ---- batch: 040 ----
mean loss: 171.70
 ---- batch: 050 ----
mean loss: 176.95
 ---- batch: 060 ----
mean loss: 176.33
 ---- batch: 070 ----
mean loss: 188.05
 ---- batch: 080 ----
mean loss: 179.68
 ---- batch: 090 ----
mean loss: 169.63
 ---- batch: 100 ----
mean loss: 181.37
 ---- batch: 110 ----
mean loss: 184.51
train mean loss: 178.89
epoch train time: 0:00:15.634702
elapsed time: 0:41:04.795844
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 04:39:43.675964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.54
 ---- batch: 020 ----
mean loss: 176.24
 ---- batch: 030 ----
mean loss: 176.01
 ---- batch: 040 ----
mean loss: 187.08
 ---- batch: 050 ----
mean loss: 177.01
 ---- batch: 060 ----
mean loss: 172.13
 ---- batch: 070 ----
mean loss: 181.32
 ---- batch: 080 ----
mean loss: 176.16
 ---- batch: 090 ----
mean loss: 173.25
 ---- batch: 100 ----
mean loss: 184.27
 ---- batch: 110 ----
mean loss: 173.74
train mean loss: 178.28
epoch train time: 0:00:15.786303
elapsed time: 0:41:20.583709
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 04:39:59.463335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.68
 ---- batch: 020 ----
mean loss: 176.38
 ---- batch: 030 ----
mean loss: 170.13
 ---- batch: 040 ----
mean loss: 182.91
 ---- batch: 050 ----
mean loss: 194.58
 ---- batch: 060 ----
mean loss: 171.44
 ---- batch: 070 ----
mean loss: 171.00
 ---- batch: 080 ----
mean loss: 183.14
 ---- batch: 090 ----
mean loss: 180.96
 ---- batch: 100 ----
mean loss: 167.83
 ---- batch: 110 ----
mean loss: 171.82
train mean loss: 178.02
epoch train time: 0:00:16.017335
elapsed time: 0:41:36.602026
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 04:40:15.482034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.70
 ---- batch: 020 ----
mean loss: 182.18
 ---- batch: 030 ----
mean loss: 179.75
 ---- batch: 040 ----
mean loss: 184.52
 ---- batch: 050 ----
mean loss: 188.82
 ---- batch: 060 ----
mean loss: 176.17
 ---- batch: 070 ----
mean loss: 176.19
 ---- batch: 080 ----
mean loss: 172.08
 ---- batch: 090 ----
mean loss: 178.05
 ---- batch: 100 ----
mean loss: 187.93
 ---- batch: 110 ----
mean loss: 173.89
train mean loss: 178.96
epoch train time: 0:00:15.548863
elapsed time: 0:41:52.152280
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 04:40:31.031888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.77
 ---- batch: 020 ----
mean loss: 181.08
 ---- batch: 030 ----
mean loss: 180.79
 ---- batch: 040 ----
mean loss: 171.60
 ---- batch: 050 ----
mean loss: 188.06
 ---- batch: 060 ----
mean loss: 178.34
 ---- batch: 070 ----
mean loss: 175.49
 ---- batch: 080 ----
mean loss: 160.84
 ---- batch: 090 ----
mean loss: 175.68
 ---- batch: 100 ----
mean loss: 183.37
 ---- batch: 110 ----
mean loss: 182.39
train mean loss: 178.21
epoch train time: 0:00:15.436497
elapsed time: 0:42:07.589693
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 04:40:46.469249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.07
 ---- batch: 020 ----
mean loss: 181.62
 ---- batch: 030 ----
mean loss: 183.94
 ---- batch: 040 ----
mean loss: 182.53
 ---- batch: 050 ----
mean loss: 179.82
 ---- batch: 060 ----
mean loss: 174.51
 ---- batch: 070 ----
mean loss: 171.53
 ---- batch: 080 ----
mean loss: 177.58
 ---- batch: 090 ----
mean loss: 170.94
 ---- batch: 100 ----
mean loss: 168.24
 ---- batch: 110 ----
mean loss: 177.27
train mean loss: 177.55
epoch train time: 0:00:15.316810
elapsed time: 0:42:22.907444
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 04:41:01.787043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.76
 ---- batch: 020 ----
mean loss: 183.49
 ---- batch: 030 ----
mean loss: 176.87
 ---- batch: 040 ----
mean loss: 178.55
 ---- batch: 050 ----
mean loss: 182.98
 ---- batch: 060 ----
mean loss: 177.68
 ---- batch: 070 ----
mean loss: 192.93
 ---- batch: 080 ----
mean loss: 175.47
 ---- batch: 090 ----
mean loss: 165.89
 ---- batch: 100 ----
mean loss: 179.41
 ---- batch: 110 ----
mean loss: 176.07
train mean loss: 178.20
epoch train time: 0:00:15.435528
elapsed time: 0:42:38.343919
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 04:41:17.223501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.02
 ---- batch: 020 ----
mean loss: 169.43
 ---- batch: 030 ----
mean loss: 178.54
 ---- batch: 040 ----
mean loss: 178.44
 ---- batch: 050 ----
mean loss: 187.89
 ---- batch: 060 ----
mean loss: 172.58
 ---- batch: 070 ----
mean loss: 172.84
 ---- batch: 080 ----
mean loss: 177.27
 ---- batch: 090 ----
mean loss: 180.10
 ---- batch: 100 ----
mean loss: 171.03
 ---- batch: 110 ----
mean loss: 165.07
train mean loss: 176.55
epoch train time: 0:00:15.391967
elapsed time: 0:42:53.736701
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 04:41:32.616290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.06
 ---- batch: 020 ----
mean loss: 179.06
 ---- batch: 030 ----
mean loss: 176.81
 ---- batch: 040 ----
mean loss: 174.01
 ---- batch: 050 ----
mean loss: 169.30
 ---- batch: 060 ----
mean loss: 174.14
 ---- batch: 070 ----
mean loss: 161.36
 ---- batch: 080 ----
mean loss: 185.75
 ---- batch: 090 ----
mean loss: 182.12
 ---- batch: 100 ----
mean loss: 192.17
 ---- batch: 110 ----
mean loss: 171.37
train mean loss: 176.66
epoch train time: 0:00:15.347359
elapsed time: 0:43:09.085036
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 04:41:47.964655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.65
 ---- batch: 020 ----
mean loss: 178.56
 ---- batch: 030 ----
mean loss: 168.90
 ---- batch: 040 ----
mean loss: 170.50
 ---- batch: 050 ----
mean loss: 181.73
 ---- batch: 060 ----
mean loss: 175.76
 ---- batch: 070 ----
mean loss: 176.15
 ---- batch: 080 ----
mean loss: 168.62
 ---- batch: 090 ----
mean loss: 186.57
 ---- batch: 100 ----
mean loss: 179.11
 ---- batch: 110 ----
mean loss: 186.15
train mean loss: 177.26
epoch train time: 0:00:15.403047
elapsed time: 0:43:24.489009
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 04:42:03.368674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.80
 ---- batch: 020 ----
mean loss: 182.11
 ---- batch: 030 ----
mean loss: 175.93
 ---- batch: 040 ----
mean loss: 185.71
 ---- batch: 050 ----
mean loss: 185.57
 ---- batch: 060 ----
mean loss: 176.80
 ---- batch: 070 ----
mean loss: 167.00
 ---- batch: 080 ----
mean loss: 178.20
 ---- batch: 090 ----
mean loss: 171.45
 ---- batch: 100 ----
mean loss: 170.86
 ---- batch: 110 ----
mean loss: 173.95
train mean loss: 177.31
epoch train time: 0:00:15.358150
elapsed time: 0:43:39.848161
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 04:42:18.727753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.70
 ---- batch: 020 ----
mean loss: 179.07
 ---- batch: 030 ----
mean loss: 167.34
 ---- batch: 040 ----
mean loss: 184.75
 ---- batch: 050 ----
mean loss: 181.54
 ---- batch: 060 ----
mean loss: 182.06
 ---- batch: 070 ----
mean loss: 172.78
 ---- batch: 080 ----
mean loss: 179.65
 ---- batch: 090 ----
mean loss: 174.58
 ---- batch: 100 ----
mean loss: 175.14
 ---- batch: 110 ----
mean loss: 166.95
train mean loss: 176.21
epoch train time: 0:00:15.421269
elapsed time: 0:43:55.270451
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 04:42:34.149998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.62
 ---- batch: 020 ----
mean loss: 183.02
 ---- batch: 030 ----
mean loss: 178.65
 ---- batch: 040 ----
mean loss: 176.91
 ---- batch: 050 ----
mean loss: 176.25
 ---- batch: 060 ----
mean loss: 183.90
 ---- batch: 070 ----
mean loss: 170.27
 ---- batch: 080 ----
mean loss: 179.61
 ---- batch: 090 ----
mean loss: 178.12
 ---- batch: 100 ----
mean loss: 169.48
 ---- batch: 110 ----
mean loss: 167.94
train mean loss: 176.76
epoch train time: 0:00:15.394062
elapsed time: 0:44:10.665421
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 04:42:49.545069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.11
 ---- batch: 020 ----
mean loss: 188.82
 ---- batch: 030 ----
mean loss: 185.25
 ---- batch: 040 ----
mean loss: 177.42
 ---- batch: 050 ----
mean loss: 172.36
 ---- batch: 060 ----
mean loss: 168.97
 ---- batch: 070 ----
mean loss: 183.13
 ---- batch: 080 ----
mean loss: 175.48
 ---- batch: 090 ----
mean loss: 175.07
 ---- batch: 100 ----
mean loss: 182.26
 ---- batch: 110 ----
mean loss: 177.39
train mean loss: 177.44
epoch train time: 0:00:15.284595
elapsed time: 0:44:25.950928
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 04:43:04.830639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.54
 ---- batch: 020 ----
mean loss: 180.61
 ---- batch: 030 ----
mean loss: 182.40
 ---- batch: 040 ----
mean loss: 167.81
 ---- batch: 050 ----
mean loss: 171.43
 ---- batch: 060 ----
mean loss: 179.63
 ---- batch: 070 ----
mean loss: 172.16
 ---- batch: 080 ----
mean loss: 176.63
 ---- batch: 090 ----
mean loss: 180.64
 ---- batch: 100 ----
mean loss: 180.74
 ---- batch: 110 ----
mean loss: 177.57
train mean loss: 176.80
epoch train time: 0:00:15.333112
elapsed time: 0:44:41.285094
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 04:43:20.164787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.58
 ---- batch: 020 ----
mean loss: 175.20
 ---- batch: 030 ----
mean loss: 181.78
 ---- batch: 040 ----
mean loss: 176.92
 ---- batch: 050 ----
mean loss: 165.15
 ---- batch: 060 ----
mean loss: 176.64
 ---- batch: 070 ----
mean loss: 184.70
 ---- batch: 080 ----
mean loss: 176.49
 ---- batch: 090 ----
mean loss: 185.32
 ---- batch: 100 ----
mean loss: 173.24
 ---- batch: 110 ----
mean loss: 187.14
train mean loss: 177.80
epoch train time: 0:00:15.330177
elapsed time: 0:44:56.616985
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 04:43:35.496269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.60
 ---- batch: 020 ----
mean loss: 181.22
 ---- batch: 030 ----
mean loss: 176.54
 ---- batch: 040 ----
mean loss: 178.07
 ---- batch: 050 ----
mean loss: 184.78
 ---- batch: 060 ----
mean loss: 176.41
 ---- batch: 070 ----
mean loss: 177.12
 ---- batch: 080 ----
mean loss: 172.36
 ---- batch: 090 ----
mean loss: 175.63
 ---- batch: 100 ----
mean loss: 177.87
 ---- batch: 110 ----
mean loss: 179.84
train mean loss: 177.35
epoch train time: 0:00:15.325132
elapsed time: 0:45:11.942860
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 04:43:50.822557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.23
 ---- batch: 020 ----
mean loss: 172.95
 ---- batch: 030 ----
mean loss: 174.26
 ---- batch: 040 ----
mean loss: 177.44
 ---- batch: 050 ----
mean loss: 175.74
 ---- batch: 060 ----
mean loss: 174.86
 ---- batch: 070 ----
mean loss: 183.14
 ---- batch: 080 ----
mean loss: 179.45
 ---- batch: 090 ----
mean loss: 181.55
 ---- batch: 100 ----
mean loss: 177.62
 ---- batch: 110 ----
mean loss: 170.58
train mean loss: 177.42
epoch train time: 0:00:15.369847
elapsed time: 0:45:27.313828
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 04:44:06.193388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.44
 ---- batch: 020 ----
mean loss: 178.29
 ---- batch: 030 ----
mean loss: 167.37
 ---- batch: 040 ----
mean loss: 187.67
 ---- batch: 050 ----
mean loss: 177.88
 ---- batch: 060 ----
mean loss: 175.48
 ---- batch: 070 ----
mean loss: 175.92
 ---- batch: 080 ----
mean loss: 177.96
 ---- batch: 090 ----
mean loss: 170.97
 ---- batch: 100 ----
mean loss: 176.82
 ---- batch: 110 ----
mean loss: 185.06
train mean loss: 177.37
epoch train time: 0:00:15.348112
elapsed time: 0:45:42.662740
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 04:44:21.542324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.37
 ---- batch: 020 ----
mean loss: 182.14
 ---- batch: 030 ----
mean loss: 180.27
 ---- batch: 040 ----
mean loss: 169.74
 ---- batch: 050 ----
mean loss: 174.50
 ---- batch: 060 ----
mean loss: 172.56
 ---- batch: 070 ----
mean loss: 170.59
 ---- batch: 080 ----
mean loss: 175.16
 ---- batch: 090 ----
mean loss: 179.02
 ---- batch: 100 ----
mean loss: 179.73
 ---- batch: 110 ----
mean loss: 165.00
train mean loss: 175.48
epoch train time: 0:00:15.288448
elapsed time: 0:45:57.952245
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 04:44:36.831895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.43
 ---- batch: 020 ----
mean loss: 186.33
 ---- batch: 030 ----
mean loss: 183.47
 ---- batch: 040 ----
mean loss: 170.92
 ---- batch: 050 ----
mean loss: 167.45
 ---- batch: 060 ----
mean loss: 177.11
 ---- batch: 070 ----
mean loss: 170.90
 ---- batch: 080 ----
mean loss: 169.56
 ---- batch: 090 ----
mean loss: 179.31
 ---- batch: 100 ----
mean loss: 170.15
 ---- batch: 110 ----
mean loss: 179.40
train mean loss: 175.66
epoch train time: 0:00:15.270703
elapsed time: 0:46:13.223945
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 04:44:52.103648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.09
 ---- batch: 020 ----
mean loss: 169.93
 ---- batch: 030 ----
mean loss: 174.38
 ---- batch: 040 ----
mean loss: 178.46
 ---- batch: 050 ----
mean loss: 177.18
 ---- batch: 060 ----
mean loss: 172.89
 ---- batch: 070 ----
mean loss: 182.41
 ---- batch: 080 ----
mean loss: 175.72
 ---- batch: 090 ----
mean loss: 173.46
 ---- batch: 100 ----
mean loss: 172.82
 ---- batch: 110 ----
mean loss: 182.55
train mean loss: 175.17
epoch train time: 0:00:15.332955
elapsed time: 0:46:28.557976
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 04:45:07.437532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.82
 ---- batch: 020 ----
mean loss: 177.99
 ---- batch: 030 ----
mean loss: 171.01
 ---- batch: 040 ----
mean loss: 166.78
 ---- batch: 050 ----
mean loss: 184.40
 ---- batch: 060 ----
mean loss: 181.33
 ---- batch: 070 ----
mean loss: 181.25
 ---- batch: 080 ----
mean loss: 180.09
 ---- batch: 090 ----
mean loss: 172.90
 ---- batch: 100 ----
mean loss: 181.47
 ---- batch: 110 ----
mean loss: 178.38
train mean loss: 177.09
epoch train time: 0:00:15.350130
elapsed time: 0:46:43.909050
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 04:45:22.788635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.95
 ---- batch: 020 ----
mean loss: 175.28
 ---- batch: 030 ----
mean loss: 184.90
 ---- batch: 040 ----
mean loss: 171.19
 ---- batch: 050 ----
mean loss: 173.54
 ---- batch: 060 ----
mean loss: 165.43
 ---- batch: 070 ----
mean loss: 174.02
 ---- batch: 080 ----
mean loss: 175.90
 ---- batch: 090 ----
mean loss: 174.62
 ---- batch: 100 ----
mean loss: 178.13
 ---- batch: 110 ----
mean loss: 178.30
train mean loss: 174.69
epoch train time: 0:00:15.375468
elapsed time: 0:46:59.285471
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 04:45:38.165065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.45
 ---- batch: 020 ----
mean loss: 178.59
 ---- batch: 030 ----
mean loss: 172.04
 ---- batch: 040 ----
mean loss: 176.78
 ---- batch: 050 ----
mean loss: 178.32
 ---- batch: 060 ----
mean loss: 183.06
 ---- batch: 070 ----
mean loss: 171.19
 ---- batch: 080 ----
mean loss: 173.52
 ---- batch: 090 ----
mean loss: 173.83
 ---- batch: 100 ----
mean loss: 173.66
 ---- batch: 110 ----
mean loss: 166.97
train mean loss: 175.17
epoch train time: 0:00:15.377909
elapsed time: 0:47:14.664224
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 04:45:53.543840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.75
 ---- batch: 020 ----
mean loss: 176.54
 ---- batch: 030 ----
mean loss: 168.44
 ---- batch: 040 ----
mean loss: 174.51
 ---- batch: 050 ----
mean loss: 171.89
 ---- batch: 060 ----
mean loss: 170.42
 ---- batch: 070 ----
mean loss: 179.74
 ---- batch: 080 ----
mean loss: 177.59
 ---- batch: 090 ----
mean loss: 191.22
 ---- batch: 100 ----
mean loss: 175.00
 ---- batch: 110 ----
mean loss: 170.55
train mean loss: 175.41
epoch train time: 0:00:15.363697
elapsed time: 0:47:30.028883
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 04:46:08.908452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.62
 ---- batch: 020 ----
mean loss: 178.59
 ---- batch: 030 ----
mean loss: 176.25
 ---- batch: 040 ----
mean loss: 173.00
 ---- batch: 050 ----
mean loss: 179.91
 ---- batch: 060 ----
mean loss: 183.38
 ---- batch: 070 ----
mean loss: 178.31
 ---- batch: 080 ----
mean loss: 176.10
 ---- batch: 090 ----
mean loss: 164.35
 ---- batch: 100 ----
mean loss: 170.65
 ---- batch: 110 ----
mean loss: 180.36
train mean loss: 175.67
epoch train time: 0:00:15.306768
elapsed time: 0:47:45.336567
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 04:46:24.216183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.51
 ---- batch: 020 ----
mean loss: 170.05
 ---- batch: 030 ----
mean loss: 183.11
 ---- batch: 040 ----
mean loss: 173.73
 ---- batch: 050 ----
mean loss: 178.36
 ---- batch: 060 ----
mean loss: 174.72
 ---- batch: 070 ----
mean loss: 178.26
 ---- batch: 080 ----
mean loss: 165.06
 ---- batch: 090 ----
mean loss: 173.33
 ---- batch: 100 ----
mean loss: 173.88
 ---- batch: 110 ----
mean loss: 177.52
train mean loss: 174.84
epoch train time: 0:00:15.318144
elapsed time: 0:48:00.655778
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 04:46:39.535358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.97
 ---- batch: 020 ----
mean loss: 180.42
 ---- batch: 030 ----
mean loss: 181.91
 ---- batch: 040 ----
mean loss: 171.44
 ---- batch: 050 ----
mean loss: 171.17
 ---- batch: 060 ----
mean loss: 174.40
 ---- batch: 070 ----
mean loss: 173.11
 ---- batch: 080 ----
mean loss: 170.00
 ---- batch: 090 ----
mean loss: 167.98
 ---- batch: 100 ----
mean loss: 165.51
 ---- batch: 110 ----
mean loss: 172.88
train mean loss: 173.76
epoch train time: 0:00:15.347334
elapsed time: 0:48:16.003965
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 04:46:54.883548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.48
 ---- batch: 020 ----
mean loss: 182.23
 ---- batch: 030 ----
mean loss: 178.69
 ---- batch: 040 ----
mean loss: 178.36
 ---- batch: 050 ----
mean loss: 179.50
 ---- batch: 060 ----
mean loss: 178.52
 ---- batch: 070 ----
mean loss: 167.11
 ---- batch: 080 ----
mean loss: 171.41
 ---- batch: 090 ----
mean loss: 180.31
 ---- batch: 100 ----
mean loss: 177.42
 ---- batch: 110 ----
mean loss: 175.10
train mean loss: 176.21
epoch train time: 0:00:15.276439
elapsed time: 0:48:31.281325
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 04:47:10.160951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.82
 ---- batch: 020 ----
mean loss: 177.35
 ---- batch: 030 ----
mean loss: 184.02
 ---- batch: 040 ----
mean loss: 168.11
 ---- batch: 050 ----
mean loss: 175.50
 ---- batch: 060 ----
mean loss: 172.95
 ---- batch: 070 ----
mean loss: 173.20
 ---- batch: 080 ----
mean loss: 169.50
 ---- batch: 090 ----
mean loss: 178.31
 ---- batch: 100 ----
mean loss: 176.46
 ---- batch: 110 ----
mean loss: 169.70
train mean loss: 174.90
epoch train time: 0:00:15.310875
elapsed time: 0:48:46.593207
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 04:47:25.472772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.65
 ---- batch: 020 ----
mean loss: 177.41
 ---- batch: 030 ----
mean loss: 191.73
 ---- batch: 040 ----
mean loss: 185.81
 ---- batch: 050 ----
mean loss: 175.18
 ---- batch: 060 ----
mean loss: 171.96
 ---- batch: 070 ----
mean loss: 170.12
 ---- batch: 080 ----
mean loss: 183.25
 ---- batch: 090 ----
mean loss: 172.85
 ---- batch: 100 ----
mean loss: 164.20
 ---- batch: 110 ----
mean loss: 171.38
train mean loss: 176.19
epoch train time: 0:00:15.307914
elapsed time: 0:49:01.902050
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 04:47:40.781636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.62
 ---- batch: 020 ----
mean loss: 170.05
 ---- batch: 030 ----
mean loss: 178.16
 ---- batch: 040 ----
mean loss: 170.88
 ---- batch: 050 ----
mean loss: 173.80
 ---- batch: 060 ----
mean loss: 176.79
 ---- batch: 070 ----
mean loss: 181.27
 ---- batch: 080 ----
mean loss: 170.69
 ---- batch: 090 ----
mean loss: 169.99
 ---- batch: 100 ----
mean loss: 173.57
 ---- batch: 110 ----
mean loss: 181.12
train mean loss: 174.02
epoch train time: 0:00:15.366248
elapsed time: 0:49:17.269298
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 04:47:56.148866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.66
 ---- batch: 020 ----
mean loss: 167.79
 ---- batch: 030 ----
mean loss: 174.53
 ---- batch: 040 ----
mean loss: 176.14
 ---- batch: 050 ----
mean loss: 181.90
 ---- batch: 060 ----
mean loss: 177.47
 ---- batch: 070 ----
mean loss: 177.04
 ---- batch: 080 ----
mean loss: 175.30
 ---- batch: 090 ----
mean loss: 181.76
 ---- batch: 100 ----
mean loss: 178.94
 ---- batch: 110 ----
mean loss: 175.88
train mean loss: 176.64
epoch train time: 0:00:15.241905
elapsed time: 0:49:32.512093
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 04:48:11.391717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.87
 ---- batch: 020 ----
mean loss: 171.52
 ---- batch: 030 ----
mean loss: 173.50
 ---- batch: 040 ----
mean loss: 172.61
 ---- batch: 050 ----
mean loss: 177.56
 ---- batch: 060 ----
mean loss: 181.43
 ---- batch: 070 ----
mean loss: 167.74
 ---- batch: 080 ----
mean loss: 176.09
 ---- batch: 090 ----
mean loss: 166.57
 ---- batch: 100 ----
mean loss: 170.60
 ---- batch: 110 ----
mean loss: 176.70
train mean loss: 174.31
epoch train time: 0:00:15.234410
elapsed time: 0:49:47.747482
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 04:48:26.627049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.71
 ---- batch: 020 ----
mean loss: 180.67
 ---- batch: 030 ----
mean loss: 175.86
 ---- batch: 040 ----
mean loss: 179.79
 ---- batch: 050 ----
mean loss: 175.64
 ---- batch: 060 ----
mean loss: 164.43
 ---- batch: 070 ----
mean loss: 167.65
 ---- batch: 080 ----
mean loss: 172.19
 ---- batch: 090 ----
mean loss: 163.94
 ---- batch: 100 ----
mean loss: 181.12
 ---- batch: 110 ----
mean loss: 178.22
train mean loss: 174.13
epoch train time: 0:00:15.255378
elapsed time: 0:50:03.003844
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 04:48:41.883460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.04
 ---- batch: 020 ----
mean loss: 188.99
 ---- batch: 030 ----
mean loss: 178.20
 ---- batch: 040 ----
mean loss: 174.19
 ---- batch: 050 ----
mean loss: 169.18
 ---- batch: 060 ----
mean loss: 171.94
 ---- batch: 070 ----
mean loss: 180.82
 ---- batch: 080 ----
mean loss: 171.17
 ---- batch: 090 ----
mean loss: 178.17
 ---- batch: 100 ----
mean loss: 174.94
 ---- batch: 110 ----
mean loss: 163.19
train mean loss: 174.51
epoch train time: 0:00:15.211160
elapsed time: 0:50:18.215958
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 04:48:57.095582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.52
 ---- batch: 020 ----
mean loss: 178.94
 ---- batch: 030 ----
mean loss: 183.38
 ---- batch: 040 ----
mean loss: 182.26
 ---- batch: 050 ----
mean loss: 169.18
 ---- batch: 060 ----
mean loss: 170.00
 ---- batch: 070 ----
mean loss: 172.19
 ---- batch: 080 ----
mean loss: 171.88
 ---- batch: 090 ----
mean loss: 179.62
 ---- batch: 100 ----
mean loss: 167.61
 ---- batch: 110 ----
mean loss: 179.51
train mean loss: 174.63
epoch train time: 0:00:15.239381
elapsed time: 0:50:33.456319
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 04:49:12.335921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.26
 ---- batch: 020 ----
mean loss: 173.75
 ---- batch: 030 ----
mean loss: 173.14
 ---- batch: 040 ----
mean loss: 190.43
 ---- batch: 050 ----
mean loss: 169.53
 ---- batch: 060 ----
mean loss: 165.90
 ---- batch: 070 ----
mean loss: 174.37
 ---- batch: 080 ----
mean loss: 177.30
 ---- batch: 090 ----
mean loss: 171.82
 ---- batch: 100 ----
mean loss: 163.39
 ---- batch: 110 ----
mean loss: 179.23
train mean loss: 174.10
epoch train time: 0:00:15.247251
elapsed time: 0:50:48.704494
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 04:49:27.584048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.89
 ---- batch: 020 ----
mean loss: 179.03
 ---- batch: 030 ----
mean loss: 179.14
 ---- batch: 040 ----
mean loss: 164.20
 ---- batch: 050 ----
mean loss: 177.56
 ---- batch: 060 ----
mean loss: 165.24
 ---- batch: 070 ----
mean loss: 187.51
 ---- batch: 080 ----
mean loss: 179.46
 ---- batch: 090 ----
mean loss: 171.70
 ---- batch: 100 ----
mean loss: 173.74
 ---- batch: 110 ----
mean loss: 171.12
train mean loss: 174.62
epoch train time: 0:00:15.259283
elapsed time: 0:51:03.964684
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 04:49:42.844304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.03
 ---- batch: 020 ----
mean loss: 169.23
 ---- batch: 030 ----
mean loss: 170.77
 ---- batch: 040 ----
mean loss: 175.06
 ---- batch: 050 ----
mean loss: 172.94
 ---- batch: 060 ----
mean loss: 181.20
 ---- batch: 070 ----
mean loss: 168.04
 ---- batch: 080 ----
mean loss: 174.26
 ---- batch: 090 ----
mean loss: 169.42
 ---- batch: 100 ----
mean loss: 166.14
 ---- batch: 110 ----
mean loss: 177.08
train mean loss: 173.44
epoch train time: 0:00:15.244814
elapsed time: 0:51:19.210406
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 04:49:58.090046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.62
 ---- batch: 020 ----
mean loss: 164.97
 ---- batch: 030 ----
mean loss: 186.04
 ---- batch: 040 ----
mean loss: 164.96
 ---- batch: 050 ----
mean loss: 178.08
 ---- batch: 060 ----
mean loss: 188.05
 ---- batch: 070 ----
mean loss: 181.08
 ---- batch: 080 ----
mean loss: 172.65
 ---- batch: 090 ----
mean loss: 164.10
 ---- batch: 100 ----
mean loss: 172.81
 ---- batch: 110 ----
mean loss: 173.88
train mean loss: 174.70
epoch train time: 0:00:15.209106
elapsed time: 0:51:34.420468
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 04:50:13.300030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.17
 ---- batch: 020 ----
mean loss: 171.63
 ---- batch: 030 ----
mean loss: 173.64
 ---- batch: 040 ----
mean loss: 171.53
 ---- batch: 050 ----
mean loss: 172.65
 ---- batch: 060 ----
mean loss: 179.65
 ---- batch: 070 ----
mean loss: 173.04
 ---- batch: 080 ----
mean loss: 164.14
 ---- batch: 090 ----
mean loss: 170.91
 ---- batch: 100 ----
mean loss: 178.30
 ---- batch: 110 ----
mean loss: 185.48
train mean loss: 173.88
epoch train time: 0:00:15.289534
elapsed time: 0:51:49.710961
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 04:50:28.590700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.69
 ---- batch: 020 ----
mean loss: 170.80
 ---- batch: 030 ----
mean loss: 181.51
 ---- batch: 040 ----
mean loss: 168.83
 ---- batch: 050 ----
mean loss: 164.42
 ---- batch: 060 ----
mean loss: 171.56
 ---- batch: 070 ----
mean loss: 167.64
 ---- batch: 080 ----
mean loss: 174.39
 ---- batch: 090 ----
mean loss: 171.37
 ---- batch: 100 ----
mean loss: 171.27
 ---- batch: 110 ----
mean loss: 167.12
train mean loss: 170.71
epoch train time: 0:00:15.252144
elapsed time: 0:52:04.964988
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 04:50:43.844167
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.72
 ---- batch: 020 ----
mean loss: 163.86
 ---- batch: 030 ----
mean loss: 164.76
 ---- batch: 040 ----
mean loss: 173.62
 ---- batch: 050 ----
mean loss: 171.42
 ---- batch: 060 ----
mean loss: 172.04
 ---- batch: 070 ----
mean loss: 162.30
 ---- batch: 080 ----
mean loss: 180.71
 ---- batch: 090 ----
mean loss: 174.96
 ---- batch: 100 ----
mean loss: 172.11
 ---- batch: 110 ----
mean loss: 164.95
train mean loss: 170.51
epoch train time: 0:00:15.375722
elapsed time: 0:52:20.341110
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 04:50:59.220695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.90
 ---- batch: 020 ----
mean loss: 172.33
 ---- batch: 030 ----
mean loss: 176.70
 ---- batch: 040 ----
mean loss: 171.31
 ---- batch: 050 ----
mean loss: 171.06
 ---- batch: 060 ----
mean loss: 174.74
 ---- batch: 070 ----
mean loss: 164.83
 ---- batch: 080 ----
mean loss: 176.81
 ---- batch: 090 ----
mean loss: 167.74
 ---- batch: 100 ----
mean loss: 169.00
 ---- batch: 110 ----
mean loss: 167.45
train mean loss: 170.71
epoch train time: 0:00:15.300333
elapsed time: 0:52:35.642414
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 04:51:14.522030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.33
 ---- batch: 020 ----
mean loss: 166.57
 ---- batch: 030 ----
mean loss: 166.36
 ---- batch: 040 ----
mean loss: 168.54
 ---- batch: 050 ----
mean loss: 173.69
 ---- batch: 060 ----
mean loss: 171.56
 ---- batch: 070 ----
mean loss: 175.85
 ---- batch: 080 ----
mean loss: 175.43
 ---- batch: 090 ----
mean loss: 174.83
 ---- batch: 100 ----
mean loss: 162.79
 ---- batch: 110 ----
mean loss: 168.62
train mean loss: 170.53
epoch train time: 0:00:15.278439
elapsed time: 0:52:50.921870
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 04:51:29.801484
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.67
 ---- batch: 020 ----
mean loss: 168.08
 ---- batch: 030 ----
mean loss: 170.33
 ---- batch: 040 ----
mean loss: 168.61
 ---- batch: 050 ----
mean loss: 171.24
 ---- batch: 060 ----
mean loss: 172.91
 ---- batch: 070 ----
mean loss: 173.22
 ---- batch: 080 ----
mean loss: 176.75
 ---- batch: 090 ----
mean loss: 163.30
 ---- batch: 100 ----
mean loss: 163.99
 ---- batch: 110 ----
mean loss: 174.83
train mean loss: 170.64
epoch train time: 0:00:15.367652
elapsed time: 0:53:06.290391
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 04:51:45.170012
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.04
 ---- batch: 020 ----
mean loss: 167.15
 ---- batch: 030 ----
mean loss: 171.36
 ---- batch: 040 ----
mean loss: 172.41
 ---- batch: 050 ----
mean loss: 163.68
 ---- batch: 060 ----
mean loss: 175.78
 ---- batch: 070 ----
mean loss: 171.40
 ---- batch: 080 ----
mean loss: 176.07
 ---- batch: 090 ----
mean loss: 167.31
 ---- batch: 100 ----
mean loss: 163.06
 ---- batch: 110 ----
mean loss: 176.74
train mean loss: 170.70
epoch train time: 0:00:15.311428
elapsed time: 0:53:21.602695
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 04:52:00.482281
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.44
 ---- batch: 020 ----
mean loss: 174.59
 ---- batch: 030 ----
mean loss: 172.05
 ---- batch: 040 ----
mean loss: 182.04
 ---- batch: 050 ----
mean loss: 162.93
 ---- batch: 060 ----
mean loss: 170.75
 ---- batch: 070 ----
mean loss: 162.15
 ---- batch: 080 ----
mean loss: 177.53
 ---- batch: 090 ----
mean loss: 164.66
 ---- batch: 100 ----
mean loss: 177.18
 ---- batch: 110 ----
mean loss: 167.13
train mean loss: 170.52
epoch train time: 0:00:15.292741
elapsed time: 0:53:36.896497
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 04:52:15.776089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.53
 ---- batch: 020 ----
mean loss: 169.90
 ---- batch: 030 ----
mean loss: 166.41
 ---- batch: 040 ----
mean loss: 165.47
 ---- batch: 050 ----
mean loss: 171.63
 ---- batch: 060 ----
mean loss: 178.03
 ---- batch: 070 ----
mean loss: 175.66
 ---- batch: 080 ----
mean loss: 172.22
 ---- batch: 090 ----
mean loss: 170.14
 ---- batch: 100 ----
mean loss: 165.42
 ---- batch: 110 ----
mean loss: 175.12
train mean loss: 170.48
epoch train time: 0:00:15.349056
elapsed time: 0:53:52.246487
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 04:52:31.126142
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.30
 ---- batch: 020 ----
mean loss: 156.77
 ---- batch: 030 ----
mean loss: 177.12
 ---- batch: 040 ----
mean loss: 165.77
 ---- batch: 050 ----
mean loss: 166.73
 ---- batch: 060 ----
mean loss: 174.79
 ---- batch: 070 ----
mean loss: 170.98
 ---- batch: 080 ----
mean loss: 168.71
 ---- batch: 090 ----
mean loss: 173.54
 ---- batch: 100 ----
mean loss: 168.85
 ---- batch: 110 ----
mean loss: 181.14
train mean loss: 170.55
epoch train time: 0:00:15.276434
elapsed time: 0:54:07.524059
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 04:52:46.403661
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.62
 ---- batch: 020 ----
mean loss: 169.21
 ---- batch: 030 ----
mean loss: 166.33
 ---- batch: 040 ----
mean loss: 174.38
 ---- batch: 050 ----
mean loss: 175.04
 ---- batch: 060 ----
mean loss: 172.44
 ---- batch: 070 ----
mean loss: 163.42
 ---- batch: 080 ----
mean loss: 166.08
 ---- batch: 090 ----
mean loss: 166.74
 ---- batch: 100 ----
mean loss: 167.33
 ---- batch: 110 ----
mean loss: 166.11
train mean loss: 170.43
epoch train time: 0:00:15.407693
elapsed time: 0:54:22.932708
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 04:53:01.812292
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.32
 ---- batch: 020 ----
mean loss: 173.15
 ---- batch: 030 ----
mean loss: 172.41
 ---- batch: 040 ----
mean loss: 174.26
 ---- batch: 050 ----
mean loss: 171.55
 ---- batch: 060 ----
mean loss: 172.66
 ---- batch: 070 ----
mean loss: 164.28
 ---- batch: 080 ----
mean loss: 167.60
 ---- batch: 090 ----
mean loss: 174.46
 ---- batch: 100 ----
mean loss: 163.18
 ---- batch: 110 ----
mean loss: 171.66
train mean loss: 170.74
epoch train time: 0:00:15.400328
elapsed time: 0:54:38.333984
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 04:53:17.213558
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.11
 ---- batch: 020 ----
mean loss: 167.00
 ---- batch: 030 ----
mean loss: 169.60
 ---- batch: 040 ----
mean loss: 178.30
 ---- batch: 050 ----
mean loss: 173.30
 ---- batch: 060 ----
mean loss: 171.85
 ---- batch: 070 ----
mean loss: 173.37
 ---- batch: 080 ----
mean loss: 167.89
 ---- batch: 090 ----
mean loss: 169.05
 ---- batch: 100 ----
mean loss: 171.22
 ---- batch: 110 ----
mean loss: 166.84
train mean loss: 170.27
epoch train time: 0:00:15.305454
elapsed time: 0:54:53.640377
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 04:53:32.520024
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.24
 ---- batch: 020 ----
mean loss: 163.52
 ---- batch: 030 ----
mean loss: 160.87
 ---- batch: 040 ----
mean loss: 180.48
 ---- batch: 050 ----
mean loss: 174.19
 ---- batch: 060 ----
mean loss: 175.88
 ---- batch: 070 ----
mean loss: 164.24
 ---- batch: 080 ----
mean loss: 165.57
 ---- batch: 090 ----
mean loss: 161.75
 ---- batch: 100 ----
mean loss: 178.25
 ---- batch: 110 ----
mean loss: 176.51
train mean loss: 170.23
epoch train time: 0:00:15.312174
elapsed time: 0:55:08.953605
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 04:53:47.833217
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.95
 ---- batch: 020 ----
mean loss: 174.30
 ---- batch: 030 ----
mean loss: 166.80
 ---- batch: 040 ----
mean loss: 161.33
 ---- batch: 050 ----
mean loss: 175.18
 ---- batch: 060 ----
mean loss: 166.85
 ---- batch: 070 ----
mean loss: 180.67
 ---- batch: 080 ----
mean loss: 171.89
 ---- batch: 090 ----
mean loss: 166.84
 ---- batch: 100 ----
mean loss: 173.18
 ---- batch: 110 ----
mean loss: 174.52
train mean loss: 170.61
epoch train time: 0:00:15.389189
elapsed time: 0:55:24.343761
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 04:54:03.223371
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.65
 ---- batch: 020 ----
mean loss: 173.82
 ---- batch: 030 ----
mean loss: 174.53
 ---- batch: 040 ----
mean loss: 173.45
 ---- batch: 050 ----
mean loss: 165.96
 ---- batch: 060 ----
mean loss: 169.67
 ---- batch: 070 ----
mean loss: 176.28
 ---- batch: 080 ----
mean loss: 161.82
 ---- batch: 090 ----
mean loss: 160.51
 ---- batch: 100 ----
mean loss: 167.74
 ---- batch: 110 ----
mean loss: 172.09
train mean loss: 170.47
epoch train time: 0:00:15.315446
elapsed time: 0:55:39.660164
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 04:54:18.539737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.04
 ---- batch: 020 ----
mean loss: 172.11
 ---- batch: 030 ----
mean loss: 169.80
 ---- batch: 040 ----
mean loss: 170.70
 ---- batch: 050 ----
mean loss: 166.65
 ---- batch: 060 ----
mean loss: 167.06
 ---- batch: 070 ----
mean loss: 163.41
 ---- batch: 080 ----
mean loss: 172.22
 ---- batch: 090 ----
mean loss: 174.06
 ---- batch: 100 ----
mean loss: 172.46
 ---- batch: 110 ----
mean loss: 169.78
train mean loss: 170.44
epoch train time: 0:00:15.312344
elapsed time: 0:55:54.973450
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 04:54:33.853060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.76
 ---- batch: 020 ----
mean loss: 165.74
 ---- batch: 030 ----
mean loss: 172.93
 ---- batch: 040 ----
mean loss: 166.48
 ---- batch: 050 ----
mean loss: 171.22
 ---- batch: 060 ----
mean loss: 171.74
 ---- batch: 070 ----
mean loss: 172.19
 ---- batch: 080 ----
mean loss: 173.10
 ---- batch: 090 ----
mean loss: 173.38
 ---- batch: 100 ----
mean loss: 178.98
 ---- batch: 110 ----
mean loss: 165.71
train mean loss: 170.42
epoch train time: 0:00:15.373674
elapsed time: 0:56:10.348126
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 04:54:49.227684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.58
 ---- batch: 020 ----
mean loss: 175.53
 ---- batch: 030 ----
mean loss: 175.72
 ---- batch: 040 ----
mean loss: 171.28
 ---- batch: 050 ----
mean loss: 168.44
 ---- batch: 060 ----
mean loss: 172.96
 ---- batch: 070 ----
mean loss: 165.10
 ---- batch: 080 ----
mean loss: 161.65
 ---- batch: 090 ----
mean loss: 164.52
 ---- batch: 100 ----
mean loss: 174.28
 ---- batch: 110 ----
mean loss: 168.14
train mean loss: 170.20
epoch train time: 0:00:15.383254
elapsed time: 0:56:25.732305
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 04:55:04.611894
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.95
 ---- batch: 020 ----
mean loss: 170.04
 ---- batch: 030 ----
mean loss: 166.11
 ---- batch: 040 ----
mean loss: 168.92
 ---- batch: 050 ----
mean loss: 174.75
 ---- batch: 060 ----
mean loss: 164.07
 ---- batch: 070 ----
mean loss: 167.55
 ---- batch: 080 ----
mean loss: 180.87
 ---- batch: 090 ----
mean loss: 170.60
 ---- batch: 100 ----
mean loss: 170.13
 ---- batch: 110 ----
mean loss: 172.23
train mean loss: 170.39
epoch train time: 0:00:15.407630
elapsed time: 0:56:41.141119
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 04:55:20.020756
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.29
 ---- batch: 020 ----
mean loss: 172.15
 ---- batch: 030 ----
mean loss: 174.53
 ---- batch: 040 ----
mean loss: 172.27
 ---- batch: 050 ----
mean loss: 178.15
 ---- batch: 060 ----
mean loss: 166.67
 ---- batch: 070 ----
mean loss: 167.45
 ---- batch: 080 ----
mean loss: 168.20
 ---- batch: 090 ----
mean loss: 169.91
 ---- batch: 100 ----
mean loss: 173.74
 ---- batch: 110 ----
mean loss: 169.27
train mean loss: 170.34
epoch train time: 0:00:15.445845
elapsed time: 0:56:56.587951
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 04:55:35.467555
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.95
 ---- batch: 020 ----
mean loss: 167.17
 ---- batch: 030 ----
mean loss: 174.08
 ---- batch: 040 ----
mean loss: 176.91
 ---- batch: 050 ----
mean loss: 164.41
 ---- batch: 060 ----
mean loss: 169.58
 ---- batch: 070 ----
mean loss: 175.54
 ---- batch: 080 ----
mean loss: 177.44
 ---- batch: 090 ----
mean loss: 164.82
 ---- batch: 100 ----
mean loss: 166.37
 ---- batch: 110 ----
mean loss: 170.86
train mean loss: 170.24
epoch train time: 0:00:15.329599
elapsed time: 0:57:11.918595
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 04:55:50.798202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.34
 ---- batch: 020 ----
mean loss: 171.38
 ---- batch: 030 ----
mean loss: 179.97
 ---- batch: 040 ----
mean loss: 167.51
 ---- batch: 050 ----
mean loss: 170.71
 ---- batch: 060 ----
mean loss: 170.55
 ---- batch: 070 ----
mean loss: 167.33
 ---- batch: 080 ----
mean loss: 168.09
 ---- batch: 090 ----
mean loss: 167.28
 ---- batch: 100 ----
mean loss: 174.54
 ---- batch: 110 ----
mean loss: 165.70
train mean loss: 170.42
epoch train time: 0:00:15.302039
elapsed time: 0:57:27.221585
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 04:56:06.101234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.85
 ---- batch: 020 ----
mean loss: 167.58
 ---- batch: 030 ----
mean loss: 166.31
 ---- batch: 040 ----
mean loss: 168.00
 ---- batch: 050 ----
mean loss: 178.27
 ---- batch: 060 ----
mean loss: 169.71
 ---- batch: 070 ----
mean loss: 172.57
 ---- batch: 080 ----
mean loss: 169.13
 ---- batch: 090 ----
mean loss: 175.21
 ---- batch: 100 ----
mean loss: 172.16
 ---- batch: 110 ----
mean loss: 164.15
train mean loss: 170.28
epoch train time: 0:00:15.370328
elapsed time: 0:57:42.592879
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 04:56:21.472527
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.92
 ---- batch: 020 ----
mean loss: 173.97
 ---- batch: 030 ----
mean loss: 172.39
 ---- batch: 040 ----
mean loss: 168.83
 ---- batch: 050 ----
mean loss: 171.67
 ---- batch: 060 ----
mean loss: 176.45
 ---- batch: 070 ----
mean loss: 169.12
 ---- batch: 080 ----
mean loss: 174.55
 ---- batch: 090 ----
mean loss: 173.03
 ---- batch: 100 ----
mean loss: 160.66
 ---- batch: 110 ----
mean loss: 167.43
train mean loss: 170.48
epoch train time: 0:00:15.263293
elapsed time: 0:57:57.857084
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 04:56:36.736673
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.69
 ---- batch: 020 ----
mean loss: 164.93
 ---- batch: 030 ----
mean loss: 178.00
 ---- batch: 040 ----
mean loss: 176.56
 ---- batch: 050 ----
mean loss: 163.06
 ---- batch: 060 ----
mean loss: 176.83
 ---- batch: 070 ----
mean loss: 161.22
 ---- batch: 080 ----
mean loss: 163.59
 ---- batch: 090 ----
mean loss: 175.35
 ---- batch: 100 ----
mean loss: 175.12
 ---- batch: 110 ----
mean loss: 171.44
train mean loss: 170.54
epoch train time: 0:00:15.282586
elapsed time: 0:58:13.140634
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 04:56:52.020247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.07
 ---- batch: 020 ----
mean loss: 178.62
 ---- batch: 030 ----
mean loss: 171.76
 ---- batch: 040 ----
mean loss: 164.79
 ---- batch: 050 ----
mean loss: 171.37
 ---- batch: 060 ----
mean loss: 156.71
 ---- batch: 070 ----
mean loss: 179.10
 ---- batch: 080 ----
mean loss: 165.23
 ---- batch: 090 ----
mean loss: 175.52
 ---- batch: 100 ----
mean loss: 172.17
 ---- batch: 110 ----
mean loss: 171.55
train mean loss: 170.16
epoch train time: 0:00:15.358814
elapsed time: 0:58:28.500319
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 04:57:07.379976
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.79
 ---- batch: 020 ----
mean loss: 170.13
 ---- batch: 030 ----
mean loss: 172.80
 ---- batch: 040 ----
mean loss: 167.37
 ---- batch: 050 ----
mean loss: 169.01
 ---- batch: 060 ----
mean loss: 170.72
 ---- batch: 070 ----
mean loss: 180.87
 ---- batch: 080 ----
mean loss: 177.30
 ---- batch: 090 ----
mean loss: 168.85
 ---- batch: 100 ----
mean loss: 169.34
 ---- batch: 110 ----
mean loss: 165.09
train mean loss: 170.07
epoch train time: 0:00:15.339511
elapsed time: 0:58:43.840885
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 04:57:22.720457
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.59
 ---- batch: 020 ----
mean loss: 166.18
 ---- batch: 030 ----
mean loss: 165.71
 ---- batch: 040 ----
mean loss: 173.46
 ---- batch: 050 ----
mean loss: 172.86
 ---- batch: 060 ----
mean loss: 173.29
 ---- batch: 070 ----
mean loss: 173.99
 ---- batch: 080 ----
mean loss: 173.80
 ---- batch: 090 ----
mean loss: 166.68
 ---- batch: 100 ----
mean loss: 167.10
 ---- batch: 110 ----
mean loss: 169.63
train mean loss: 170.35
epoch train time: 0:00:15.320451
elapsed time: 0:58:59.162269
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 04:57:38.041882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 161.52
 ---- batch: 020 ----
mean loss: 175.72
 ---- batch: 030 ----
mean loss: 179.50
 ---- batch: 040 ----
mean loss: 175.37
 ---- batch: 050 ----
mean loss: 170.50
 ---- batch: 060 ----
mean loss: 172.71
 ---- batch: 070 ----
mean loss: 168.12
 ---- batch: 080 ----
mean loss: 160.68
 ---- batch: 090 ----
mean loss: 173.80
 ---- batch: 100 ----
mean loss: 169.44
 ---- batch: 110 ----
mean loss: 166.60
train mean loss: 170.31
epoch train time: 0:00:15.434182
elapsed time: 0:59:14.597340
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 04:57:53.477053
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.56
 ---- batch: 020 ----
mean loss: 162.07
 ---- batch: 030 ----
mean loss: 166.41
 ---- batch: 040 ----
mean loss: 181.28
 ---- batch: 050 ----
mean loss: 168.03
 ---- batch: 060 ----
mean loss: 171.27
 ---- batch: 070 ----
mean loss: 173.69
 ---- batch: 080 ----
mean loss: 167.67
 ---- batch: 090 ----
mean loss: 169.97
 ---- batch: 100 ----
mean loss: 162.61
 ---- batch: 110 ----
mean loss: 173.17
train mean loss: 170.10
epoch train time: 0:00:15.450062
elapsed time: 0:59:30.048485
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 04:58:08.928067
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.36
 ---- batch: 020 ----
mean loss: 160.35
 ---- batch: 030 ----
mean loss: 163.36
 ---- batch: 040 ----
mean loss: 179.25
 ---- batch: 050 ----
mean loss: 176.22
 ---- batch: 060 ----
mean loss: 159.90
 ---- batch: 070 ----
mean loss: 168.95
 ---- batch: 080 ----
mean loss: 177.60
 ---- batch: 090 ----
mean loss: 167.03
 ---- batch: 100 ----
mean loss: 176.84
 ---- batch: 110 ----
mean loss: 174.99
train mean loss: 170.04
epoch train time: 0:00:15.350324
elapsed time: 0:59:45.399705
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 04:58:24.279340
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.80
 ---- batch: 020 ----
mean loss: 170.53
 ---- batch: 030 ----
mean loss: 172.15
 ---- batch: 040 ----
mean loss: 175.12
 ---- batch: 050 ----
mean loss: 161.79
 ---- batch: 060 ----
mean loss: 177.41
 ---- batch: 070 ----
mean loss: 171.61
 ---- batch: 080 ----
mean loss: 172.94
 ---- batch: 090 ----
mean loss: 169.98
 ---- batch: 100 ----
mean loss: 165.67
 ---- batch: 110 ----
mean loss: 166.59
train mean loss: 170.24
epoch train time: 0:00:15.412197
elapsed time: 1:00:00.812943
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 04:58:39.692796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.75
 ---- batch: 020 ----
mean loss: 164.62
 ---- batch: 030 ----
mean loss: 172.72
 ---- batch: 040 ----
mean loss: 174.56
 ---- batch: 050 ----
mean loss: 165.48
 ---- batch: 060 ----
mean loss: 175.19
 ---- batch: 070 ----
mean loss: 173.32
 ---- batch: 080 ----
mean loss: 171.12
 ---- batch: 090 ----
mean loss: 168.01
 ---- batch: 100 ----
mean loss: 176.12
 ---- batch: 110 ----
mean loss: 167.04
train mean loss: 170.55
epoch train time: 0:00:15.329862
elapsed time: 1:00:16.144659
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 04:58:55.023952
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.64
 ---- batch: 020 ----
mean loss: 167.03
 ---- batch: 030 ----
mean loss: 160.81
 ---- batch: 040 ----
mean loss: 170.59
 ---- batch: 050 ----
mean loss: 167.45
 ---- batch: 060 ----
mean loss: 175.19
 ---- batch: 070 ----
mean loss: 167.37
 ---- batch: 080 ----
mean loss: 176.30
 ---- batch: 090 ----
mean loss: 175.45
 ---- batch: 100 ----
mean loss: 171.23
 ---- batch: 110 ----
mean loss: 172.00
train mean loss: 170.32
epoch train time: 0:00:15.370252
elapsed time: 1:00:31.515573
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 04:59:10.395235
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.75
 ---- batch: 020 ----
mean loss: 162.92
 ---- batch: 030 ----
mean loss: 164.85
 ---- batch: 040 ----
mean loss: 179.57
 ---- batch: 050 ----
mean loss: 167.78
 ---- batch: 060 ----
mean loss: 176.63
 ---- batch: 070 ----
mean loss: 172.37
 ---- batch: 080 ----
mean loss: 165.41
 ---- batch: 090 ----
mean loss: 167.32
 ---- batch: 100 ----
mean loss: 166.75
 ---- batch: 110 ----
mean loss: 168.16
train mean loss: 170.04
epoch train time: 0:00:15.443034
elapsed time: 1:00:46.959720
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 04:59:25.839393
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.44
 ---- batch: 020 ----
mean loss: 172.68
 ---- batch: 030 ----
mean loss: 166.88
 ---- batch: 040 ----
mean loss: 173.02
 ---- batch: 050 ----
mean loss: 170.99
 ---- batch: 060 ----
mean loss: 181.30
 ---- batch: 070 ----
mean loss: 169.86
 ---- batch: 080 ----
mean loss: 166.87
 ---- batch: 090 ----
mean loss: 165.09
 ---- batch: 100 ----
mean loss: 168.31
 ---- batch: 110 ----
mean loss: 165.81
train mean loss: 169.93
epoch train time: 0:00:15.362781
elapsed time: 1:01:02.323566
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 04:59:41.203155
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.36
 ---- batch: 020 ----
mean loss: 181.90
 ---- batch: 030 ----
mean loss: 167.97
 ---- batch: 040 ----
mean loss: 167.15
 ---- batch: 050 ----
mean loss: 171.86
 ---- batch: 060 ----
mean loss: 174.63
 ---- batch: 070 ----
mean loss: 163.20
 ---- batch: 080 ----
mean loss: 164.63
 ---- batch: 090 ----
mean loss: 174.12
 ---- batch: 100 ----
mean loss: 166.94
 ---- batch: 110 ----
mean loss: 162.84
train mean loss: 169.78
epoch train time: 0:00:15.390028
elapsed time: 1:01:17.714611
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 04:59:56.594200
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.39
 ---- batch: 020 ----
mean loss: 174.01
 ---- batch: 030 ----
mean loss: 169.54
 ---- batch: 040 ----
mean loss: 174.49
 ---- batch: 050 ----
mean loss: 169.18
 ---- batch: 060 ----
mean loss: 173.44
 ---- batch: 070 ----
mean loss: 182.68
 ---- batch: 080 ----
mean loss: 159.58
 ---- batch: 090 ----
mean loss: 159.98
 ---- batch: 100 ----
mean loss: 175.19
 ---- batch: 110 ----
mean loss: 163.74
train mean loss: 169.92
epoch train time: 0:00:15.402760
elapsed time: 1:01:33.118372
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 05:00:11.997983
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 162.39
 ---- batch: 020 ----
mean loss: 174.20
 ---- batch: 030 ----
mean loss: 174.08
 ---- batch: 040 ----
mean loss: 168.99
 ---- batch: 050 ----
mean loss: 167.01
 ---- batch: 060 ----
mean loss: 180.09
 ---- batch: 070 ----
mean loss: 173.14
 ---- batch: 080 ----
mean loss: 178.02
 ---- batch: 090 ----
mean loss: 163.96
 ---- batch: 100 ----
mean loss: 167.13
 ---- batch: 110 ----
mean loss: 158.49
train mean loss: 169.88
epoch train time: 0:00:15.369215
elapsed time: 1:01:48.488474
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 05:00:27.368068
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.27
 ---- batch: 020 ----
mean loss: 165.33
 ---- batch: 030 ----
mean loss: 172.77
 ---- batch: 040 ----
mean loss: 163.33
 ---- batch: 050 ----
mean loss: 168.03
 ---- batch: 060 ----
mean loss: 170.58
 ---- batch: 070 ----
mean loss: 177.30
 ---- batch: 080 ----
mean loss: 170.39
 ---- batch: 090 ----
mean loss: 182.64
 ---- batch: 100 ----
mean loss: 165.29
 ---- batch: 110 ----
mean loss: 172.44
train mean loss: 170.35
epoch train time: 0:00:15.310013
elapsed time: 1:02:03.799495
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 05:00:42.679094
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.66
 ---- batch: 020 ----
mean loss: 166.74
 ---- batch: 030 ----
mean loss: 168.17
 ---- batch: 040 ----
mean loss: 168.23
 ---- batch: 050 ----
mean loss: 176.50
 ---- batch: 060 ----
mean loss: 177.71
 ---- batch: 070 ----
mean loss: 171.98
 ---- batch: 080 ----
mean loss: 166.96
 ---- batch: 090 ----
mean loss: 168.69
 ---- batch: 100 ----
mean loss: 179.51
 ---- batch: 110 ----
mean loss: 163.03
train mean loss: 169.82
epoch train time: 0:00:15.367667
elapsed time: 1:02:19.168007
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 05:00:58.047591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.27
 ---- batch: 020 ----
mean loss: 181.08
 ---- batch: 030 ----
mean loss: 178.53
 ---- batch: 040 ----
mean loss: 164.40
 ---- batch: 050 ----
mean loss: 164.31
 ---- batch: 060 ----
mean loss: 170.03
 ---- batch: 070 ----
mean loss: 172.20
 ---- batch: 080 ----
mean loss: 154.81
 ---- batch: 090 ----
mean loss: 166.03
 ---- batch: 100 ----
mean loss: 173.60
 ---- batch: 110 ----
mean loss: 172.64
train mean loss: 169.88
epoch train time: 0:00:15.351761
elapsed time: 1:02:34.520699
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 05:01:13.400279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.71
 ---- batch: 020 ----
mean loss: 179.80
 ---- batch: 030 ----
mean loss: 164.31
 ---- batch: 040 ----
mean loss: 180.07
 ---- batch: 050 ----
mean loss: 175.95
 ---- batch: 060 ----
mean loss: 167.22
 ---- batch: 070 ----
mean loss: 176.69
 ---- batch: 080 ----
mean loss: 162.23
 ---- batch: 090 ----
mean loss: 161.50
 ---- batch: 100 ----
mean loss: 175.08
 ---- batch: 110 ----
mean loss: 165.96
train mean loss: 170.03
epoch train time: 0:00:15.358773
elapsed time: 1:02:49.880306
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 05:01:28.759867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.54
 ---- batch: 020 ----
mean loss: 169.80
 ---- batch: 030 ----
mean loss: 167.13
 ---- batch: 040 ----
mean loss: 167.56
 ---- batch: 050 ----
mean loss: 168.86
 ---- batch: 060 ----
mean loss: 167.75
 ---- batch: 070 ----
mean loss: 176.42
 ---- batch: 080 ----
mean loss: 176.61
 ---- batch: 090 ----
mean loss: 164.42
 ---- batch: 100 ----
mean loss: 172.66
 ---- batch: 110 ----
mean loss: 174.86
train mean loss: 169.67
epoch train time: 0:00:15.331846
elapsed time: 1:03:05.212969
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 05:01:44.092572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.32
 ---- batch: 020 ----
mean loss: 167.95
 ---- batch: 030 ----
mean loss: 173.34
 ---- batch: 040 ----
mean loss: 170.49
 ---- batch: 050 ----
mean loss: 163.34
 ---- batch: 060 ----
mean loss: 174.33
 ---- batch: 070 ----
mean loss: 170.16
 ---- batch: 080 ----
mean loss: 163.17
 ---- batch: 090 ----
mean loss: 169.16
 ---- batch: 100 ----
mean loss: 174.47
 ---- batch: 110 ----
mean loss: 174.86
train mean loss: 169.95
epoch train time: 0:00:15.366195
elapsed time: 1:03:20.580098
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 05:01:59.459661
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.76
 ---- batch: 020 ----
mean loss: 171.60
 ---- batch: 030 ----
mean loss: 168.98
 ---- batch: 040 ----
mean loss: 167.96
 ---- batch: 050 ----
mean loss: 170.58
 ---- batch: 060 ----
mean loss: 167.22
 ---- batch: 070 ----
mean loss: 171.65
 ---- batch: 080 ----
mean loss: 167.01
 ---- batch: 090 ----
mean loss: 168.60
 ---- batch: 100 ----
mean loss: 169.26
 ---- batch: 110 ----
mean loss: 171.52
train mean loss: 170.27
epoch train time: 0:00:15.274453
elapsed time: 1:03:35.855474
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 05:02:14.735033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 160.43
 ---- batch: 020 ----
mean loss: 174.00
 ---- batch: 030 ----
mean loss: 171.72
 ---- batch: 040 ----
mean loss: 166.75
 ---- batch: 050 ----
mean loss: 167.09
 ---- batch: 060 ----
mean loss: 171.29
 ---- batch: 070 ----
mean loss: 171.12
 ---- batch: 080 ----
mean loss: 165.22
 ---- batch: 090 ----
mean loss: 169.06
 ---- batch: 100 ----
mean loss: 174.89
 ---- batch: 110 ----
mean loss: 179.29
train mean loss: 169.78
epoch train time: 0:00:15.361338
elapsed time: 1:03:51.217736
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 05:02:30.097352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.20
 ---- batch: 020 ----
mean loss: 168.89
 ---- batch: 030 ----
mean loss: 165.29
 ---- batch: 040 ----
mean loss: 169.89
 ---- batch: 050 ----
mean loss: 171.05
 ---- batch: 060 ----
mean loss: 165.78
 ---- batch: 070 ----
mean loss: 164.57
 ---- batch: 080 ----
mean loss: 170.75
 ---- batch: 090 ----
mean loss: 175.86
 ---- batch: 100 ----
mean loss: 178.53
 ---- batch: 110 ----
mean loss: 169.94
train mean loss: 169.96
epoch train time: 0:00:15.341747
elapsed time: 1:04:06.560431
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 05:02:45.440155
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.54
 ---- batch: 020 ----
mean loss: 168.88
 ---- batch: 030 ----
mean loss: 163.73
 ---- batch: 040 ----
mean loss: 170.34
 ---- batch: 050 ----
mean loss: 164.91
 ---- batch: 060 ----
mean loss: 173.77
 ---- batch: 070 ----
mean loss: 168.42
 ---- batch: 080 ----
mean loss: 173.82
 ---- batch: 090 ----
mean loss: 172.90
 ---- batch: 100 ----
mean loss: 174.18
 ---- batch: 110 ----
mean loss: 170.93
train mean loss: 169.77
epoch train time: 0:00:15.295952
elapsed time: 1:04:21.866786
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_4/checkpoint.pth.tar
**** end time: 2019-09-26 05:03:00.745891 ****
