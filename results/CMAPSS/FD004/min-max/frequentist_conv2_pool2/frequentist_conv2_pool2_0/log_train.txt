Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_0', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 16959
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 01:25:27.866726 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 11, 11]             560
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
            Conv2d-4            [-1, 14, 4, 11]             224
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
            Linear-8                    [-1, 1]             308
================================================================
Total params: 1,092
Trainable params: 1,092
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 01:25:27.872345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4881.87
 ---- batch: 020 ----
mean loss: 4732.73
 ---- batch: 030 ----
mean loss: 4584.05
 ---- batch: 040 ----
mean loss: 4405.02
 ---- batch: 050 ----
mean loss: 4235.33
 ---- batch: 060 ----
mean loss: 4010.20
 ---- batch: 070 ----
mean loss: 3842.64
 ---- batch: 080 ----
mean loss: 3629.05
 ---- batch: 090 ----
mean loss: 3413.40
 ---- batch: 100 ----
mean loss: 3225.48
 ---- batch: 110 ----
mean loss: 3026.78
train mean loss: 3968.72
epoch train time: 0:00:33.384426
elapsed time: 0:00:33.391362
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 01:26:01.258130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2733.04
 ---- batch: 020 ----
mean loss: 2517.94
 ---- batch: 030 ----
mean loss: 2352.86
 ---- batch: 040 ----
mean loss: 2181.26
 ---- batch: 050 ----
mean loss: 2042.29
 ---- batch: 060 ----
mean loss: 1886.58
 ---- batch: 070 ----
mean loss: 1752.51
 ---- batch: 080 ----
mean loss: 1649.62
 ---- batch: 090 ----
mean loss: 1541.19
 ---- batch: 100 ----
mean loss: 1438.71
 ---- batch: 110 ----
mean loss: 1353.70
train mean loss: 1933.62
epoch train time: 0:00:00.722785
elapsed time: 0:00:34.114287
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 01:26:01.981081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1268.75
 ---- batch: 020 ----
mean loss: 1219.84
 ---- batch: 030 ----
mean loss: 1167.69
 ---- batch: 040 ----
mean loss: 1123.18
 ---- batch: 050 ----
mean loss: 1068.88
 ---- batch: 060 ----
mean loss: 1027.05
 ---- batch: 070 ----
mean loss: 1026.08
 ---- batch: 080 ----
mean loss: 987.17
 ---- batch: 090 ----
mean loss: 965.86
 ---- batch: 100 ----
mean loss: 944.72
 ---- batch: 110 ----
mean loss: 932.03
train mean loss: 1062.30
epoch train time: 0:00:00.723338
elapsed time: 0:00:34.837785
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 01:26:02.704561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.93
 ---- batch: 020 ----
mean loss: 906.40
 ---- batch: 030 ----
mean loss: 904.05
 ---- batch: 040 ----
mean loss: 880.36
 ---- batch: 050 ----
mean loss: 864.40
 ---- batch: 060 ----
mean loss: 867.04
 ---- batch: 070 ----
mean loss: 869.05
 ---- batch: 080 ----
mean loss: 842.43
 ---- batch: 090 ----
mean loss: 867.87
 ---- batch: 100 ----
mean loss: 870.35
 ---- batch: 110 ----
mean loss: 850.20
train mean loss: 876.08
epoch train time: 0:00:00.722409
elapsed time: 0:00:35.560338
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 01:26:03.427114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.26
 ---- batch: 020 ----
mean loss: 846.52
 ---- batch: 030 ----
mean loss: 853.26
 ---- batch: 040 ----
mean loss: 859.68
 ---- batch: 050 ----
mean loss: 852.87
 ---- batch: 060 ----
mean loss: 838.66
 ---- batch: 070 ----
mean loss: 842.07
 ---- batch: 080 ----
mean loss: 839.22
 ---- batch: 090 ----
mean loss: 840.19
 ---- batch: 100 ----
mean loss: 855.22
 ---- batch: 110 ----
mean loss: 853.53
train mean loss: 847.84
epoch train time: 0:00:00.720920
elapsed time: 0:00:36.281398
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 01:26:04.148172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.92
 ---- batch: 020 ----
mean loss: 842.56
 ---- batch: 030 ----
mean loss: 847.40
 ---- batch: 040 ----
mean loss: 822.04
 ---- batch: 050 ----
mean loss: 836.20
 ---- batch: 060 ----
mean loss: 857.36
 ---- batch: 070 ----
mean loss: 835.41
 ---- batch: 080 ----
mean loss: 856.70
 ---- batch: 090 ----
mean loss: 836.26
 ---- batch: 100 ----
mean loss: 844.14
 ---- batch: 110 ----
mean loss: 842.00
train mean loss: 841.60
epoch train time: 0:00:00.714061
elapsed time: 0:00:36.995632
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 01:26:04.862418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.04
 ---- batch: 020 ----
mean loss: 824.54
 ---- batch: 030 ----
mean loss: 827.70
 ---- batch: 040 ----
mean loss: 837.36
 ---- batch: 050 ----
mean loss: 810.34
 ---- batch: 060 ----
mean loss: 838.59
 ---- batch: 070 ----
mean loss: 829.19
 ---- batch: 080 ----
mean loss: 852.59
 ---- batch: 090 ----
mean loss: 848.03
 ---- batch: 100 ----
mean loss: 851.90
 ---- batch: 110 ----
mean loss: 839.26
train mean loss: 837.20
epoch train time: 0:00:00.707528
elapsed time: 0:00:37.703316
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 01:26:05.570092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 827.94
 ---- batch: 020 ----
mean loss: 819.14
 ---- batch: 030 ----
mean loss: 853.90
 ---- batch: 040 ----
mean loss: 834.89
 ---- batch: 050 ----
mean loss: 835.46
 ---- batch: 060 ----
mean loss: 840.72
 ---- batch: 070 ----
mean loss: 811.37
 ---- batch: 080 ----
mean loss: 831.12
 ---- batch: 090 ----
mean loss: 834.32
 ---- batch: 100 ----
mean loss: 841.46
 ---- batch: 110 ----
mean loss: 836.14
train mean loss: 832.77
epoch train time: 0:00:00.719649
elapsed time: 0:00:38.423203
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 01:26:06.290002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.72
 ---- batch: 020 ----
mean loss: 834.79
 ---- batch: 030 ----
mean loss: 815.83
 ---- batch: 040 ----
mean loss: 815.65
 ---- batch: 050 ----
mean loss: 832.50
 ---- batch: 060 ----
mean loss: 815.90
 ---- batch: 070 ----
mean loss: 837.70
 ---- batch: 080 ----
mean loss: 822.81
 ---- batch: 090 ----
mean loss: 821.62
 ---- batch: 100 ----
mean loss: 846.35
 ---- batch: 110 ----
mean loss: 834.32
train mean loss: 828.37
epoch train time: 0:00:00.717308
elapsed time: 0:00:39.140678
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 01:26:07.007484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.35
 ---- batch: 020 ----
mean loss: 833.40
 ---- batch: 030 ----
mean loss: 805.27
 ---- batch: 040 ----
mean loss: 837.66
 ---- batch: 050 ----
mean loss: 818.53
 ---- batch: 060 ----
mean loss: 825.40
 ---- batch: 070 ----
mean loss: 818.73
 ---- batch: 080 ----
mean loss: 852.90
 ---- batch: 090 ----
mean loss: 814.44
 ---- batch: 100 ----
mean loss: 808.33
 ---- batch: 110 ----
mean loss: 829.57
train mean loss: 823.44
epoch train time: 0:00:00.710411
elapsed time: 0:00:39.851293
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 01:26:07.718098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.36
 ---- batch: 020 ----
mean loss: 804.27
 ---- batch: 030 ----
mean loss: 820.54
 ---- batch: 040 ----
mean loss: 836.96
 ---- batch: 050 ----
mean loss: 811.12
 ---- batch: 060 ----
mean loss: 814.87
 ---- batch: 070 ----
mean loss: 819.91
 ---- batch: 080 ----
mean loss: 812.85
 ---- batch: 090 ----
mean loss: 827.90
 ---- batch: 100 ----
mean loss: 811.96
 ---- batch: 110 ----
mean loss: 814.00
train mean loss: 818.16
epoch train time: 0:00:00.716843
elapsed time: 0:00:40.568306
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 01:26:08.435079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.35
 ---- batch: 020 ----
mean loss: 794.69
 ---- batch: 030 ----
mean loss: 829.54
 ---- batch: 040 ----
mean loss: 822.86
 ---- batch: 050 ----
mean loss: 813.43
 ---- batch: 060 ----
mean loss: 806.29
 ---- batch: 070 ----
mean loss: 810.65
 ---- batch: 080 ----
mean loss: 803.28
 ---- batch: 090 ----
mean loss: 818.30
 ---- batch: 100 ----
mean loss: 807.79
 ---- batch: 110 ----
mean loss: 781.09
train mean loss: 812.32
epoch train time: 0:00:00.708205
elapsed time: 0:00:41.276647
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 01:26:09.143420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.87
 ---- batch: 020 ----
mean loss: 815.83
 ---- batch: 030 ----
mean loss: 813.09
 ---- batch: 040 ----
mean loss: 802.03
 ---- batch: 050 ----
mean loss: 804.93
 ---- batch: 060 ----
mean loss: 800.59
 ---- batch: 070 ----
mean loss: 816.13
 ---- batch: 080 ----
mean loss: 781.61
 ---- batch: 090 ----
mean loss: 802.85
 ---- batch: 100 ----
mean loss: 815.58
 ---- batch: 110 ----
mean loss: 788.20
train mean loss: 806.50
epoch train time: 0:00:00.706258
elapsed time: 0:00:41.983073
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 01:26:09.849848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 790.48
 ---- batch: 020 ----
mean loss: 804.62
 ---- batch: 030 ----
mean loss: 796.23
 ---- batch: 040 ----
mean loss: 785.81
 ---- batch: 050 ----
mean loss: 791.95
 ---- batch: 060 ----
mean loss: 806.20
 ---- batch: 070 ----
mean loss: 804.73
 ---- batch: 080 ----
mean loss: 806.00
 ---- batch: 090 ----
mean loss: 791.89
 ---- batch: 100 ----
mean loss: 808.49
 ---- batch: 110 ----
mean loss: 811.54
train mean loss: 800.53
epoch train time: 0:00:00.706521
elapsed time: 0:00:42.689733
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 01:26:10.556507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 799.44
 ---- batch: 020 ----
mean loss: 789.56
 ---- batch: 030 ----
mean loss: 805.54
 ---- batch: 040 ----
mean loss: 806.84
 ---- batch: 050 ----
mean loss: 798.54
 ---- batch: 060 ----
mean loss: 787.05
 ---- batch: 070 ----
mean loss: 791.54
 ---- batch: 080 ----
mean loss: 782.00
 ---- batch: 090 ----
mean loss: 791.08
 ---- batch: 100 ----
mean loss: 791.11
 ---- batch: 110 ----
mean loss: 809.56
train mean loss: 794.63
epoch train time: 0:00:00.698641
elapsed time: 0:00:43.388541
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 01:26:11.255332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 795.16
 ---- batch: 020 ----
mean loss: 797.16
 ---- batch: 030 ----
mean loss: 786.57
 ---- batch: 040 ----
mean loss: 774.78
 ---- batch: 050 ----
mean loss: 787.00
 ---- batch: 060 ----
mean loss: 797.48
 ---- batch: 070 ----
mean loss: 799.60
 ---- batch: 080 ----
mean loss: 776.32
 ---- batch: 090 ----
mean loss: 778.97
 ---- batch: 100 ----
mean loss: 799.85
 ---- batch: 110 ----
mean loss: 776.22
train mean loss: 789.01
epoch train time: 0:00:00.702423
elapsed time: 0:00:44.091137
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 01:26:11.957936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 780.36
 ---- batch: 020 ----
mean loss: 755.36
 ---- batch: 030 ----
mean loss: 781.92
 ---- batch: 040 ----
mean loss: 804.42
 ---- batch: 050 ----
mean loss: 801.99
 ---- batch: 060 ----
mean loss: 798.58
 ---- batch: 070 ----
mean loss: 795.12
 ---- batch: 080 ----
mean loss: 782.38
 ---- batch: 090 ----
mean loss: 768.79
 ---- batch: 100 ----
mean loss: 776.53
 ---- batch: 110 ----
mean loss: 774.12
train mean loss: 783.62
epoch train time: 0:00:00.710631
elapsed time: 0:00:44.801934
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 01:26:12.668707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.80
 ---- batch: 020 ----
mean loss: 786.08
 ---- batch: 030 ----
mean loss: 766.19
 ---- batch: 040 ----
mean loss: 784.83
 ---- batch: 050 ----
mean loss: 794.60
 ---- batch: 060 ----
mean loss: 762.09
 ---- batch: 070 ----
mean loss: 789.00
 ---- batch: 080 ----
mean loss: 773.22
 ---- batch: 090 ----
mean loss: 773.08
 ---- batch: 100 ----
mean loss: 789.95
 ---- batch: 110 ----
mean loss: 785.60
train mean loss: 778.23
epoch train time: 0:00:00.716847
elapsed time: 0:00:45.518955
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 01:26:13.385734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.34
 ---- batch: 020 ----
mean loss: 786.62
 ---- batch: 030 ----
mean loss: 774.34
 ---- batch: 040 ----
mean loss: 755.22
 ---- batch: 050 ----
mean loss: 763.95
 ---- batch: 060 ----
mean loss: 778.43
 ---- batch: 070 ----
mean loss: 772.03
 ---- batch: 080 ----
mean loss: 772.29
 ---- batch: 090 ----
mean loss: 777.40
 ---- batch: 100 ----
mean loss: 767.65
 ---- batch: 110 ----
mean loss: 790.05
train mean loss: 772.95
epoch train time: 0:00:00.715416
elapsed time: 0:00:46.234572
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 01:26:14.101345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 768.41
 ---- batch: 020 ----
mean loss: 775.81
 ---- batch: 030 ----
mean loss: 779.21
 ---- batch: 040 ----
mean loss: 763.07
 ---- batch: 050 ----
mean loss: 755.33
 ---- batch: 060 ----
mean loss: 775.60
 ---- batch: 070 ----
mean loss: 769.26
 ---- batch: 080 ----
mean loss: 771.61
 ---- batch: 090 ----
mean loss: 761.38
 ---- batch: 100 ----
mean loss: 767.01
 ---- batch: 110 ----
mean loss: 769.30
train mean loss: 767.78
epoch train time: 0:00:00.725622
elapsed time: 0:00:46.960347
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 01:26:14.827146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 731.63
 ---- batch: 020 ----
mean loss: 800.07
 ---- batch: 030 ----
mean loss: 765.53
 ---- batch: 040 ----
mean loss: 786.01
 ---- batch: 050 ----
mean loss: 768.53
 ---- batch: 060 ----
mean loss: 769.46
 ---- batch: 070 ----
mean loss: 759.83
 ---- batch: 080 ----
mean loss: 768.98
 ---- batch: 090 ----
mean loss: 750.39
 ---- batch: 100 ----
mean loss: 747.91
 ---- batch: 110 ----
mean loss: 742.22
train mean loss: 762.58
epoch train time: 0:00:00.714270
elapsed time: 0:00:47.674793
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 01:26:15.541566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 751.52
 ---- batch: 020 ----
mean loss: 773.57
 ---- batch: 030 ----
mean loss: 749.43
 ---- batch: 040 ----
mean loss: 774.29
 ---- batch: 050 ----
mean loss: 761.43
 ---- batch: 060 ----
mean loss: 754.40
 ---- batch: 070 ----
mean loss: 766.29
 ---- batch: 080 ----
mean loss: 752.02
 ---- batch: 090 ----
mean loss: 748.89
 ---- batch: 100 ----
mean loss: 746.35
 ---- batch: 110 ----
mean loss: 760.71
train mean loss: 757.58
epoch train time: 0:00:00.708071
elapsed time: 0:00:48.382998
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 01:26:16.249787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 738.72
 ---- batch: 020 ----
mean loss: 741.13
 ---- batch: 030 ----
mean loss: 732.78
 ---- batch: 040 ----
mean loss: 758.51
 ---- batch: 050 ----
mean loss: 771.73
 ---- batch: 060 ----
mean loss: 749.01
 ---- batch: 070 ----
mean loss: 771.94
 ---- batch: 080 ----
mean loss: 742.69
 ---- batch: 090 ----
mean loss: 747.05
 ---- batch: 100 ----
mean loss: 774.68
 ---- batch: 110 ----
mean loss: 745.28
train mean loss: 752.66
epoch train time: 0:00:00.713387
elapsed time: 0:00:49.096535
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 01:26:16.963307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 732.14
 ---- batch: 020 ----
mean loss: 749.04
 ---- batch: 030 ----
mean loss: 763.05
 ---- batch: 040 ----
mean loss: 744.50
 ---- batch: 050 ----
mean loss: 762.39
 ---- batch: 060 ----
mean loss: 741.59
 ---- batch: 070 ----
mean loss: 744.00
 ---- batch: 080 ----
mean loss: 757.63
 ---- batch: 090 ----
mean loss: 743.36
 ---- batch: 100 ----
mean loss: 753.08
 ---- batch: 110 ----
mean loss: 732.47
train mean loss: 747.72
epoch train time: 0:00:00.707842
elapsed time: 0:00:49.804539
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 01:26:17.671359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 742.63
 ---- batch: 020 ----
mean loss: 744.44
 ---- batch: 030 ----
mean loss: 731.28
 ---- batch: 040 ----
mean loss: 750.24
 ---- batch: 050 ----
mean loss: 743.46
 ---- batch: 060 ----
mean loss: 752.32
 ---- batch: 070 ----
mean loss: 725.70
 ---- batch: 080 ----
mean loss: 744.94
 ---- batch: 090 ----
mean loss: 751.44
 ---- batch: 100 ----
mean loss: 730.26
 ---- batch: 110 ----
mean loss: 750.67
train mean loss: 742.80
epoch train time: 0:00:00.711591
elapsed time: 0:00:50.516313
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 01:26:18.383085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 742.65
 ---- batch: 020 ----
mean loss: 738.86
 ---- batch: 030 ----
mean loss: 731.78
 ---- batch: 040 ----
mean loss: 738.59
 ---- batch: 050 ----
mean loss: 737.88
 ---- batch: 060 ----
mean loss: 745.01
 ---- batch: 070 ----
mean loss: 751.17
 ---- batch: 080 ----
mean loss: 724.04
 ---- batch: 090 ----
mean loss: 746.13
 ---- batch: 100 ----
mean loss: 731.90
 ---- batch: 110 ----
mean loss: 734.35
train mean loss: 737.73
epoch train time: 0:00:00.704315
elapsed time: 0:00:51.220762
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 01:26:19.087543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 738.73
 ---- batch: 020 ----
mean loss: 739.23
 ---- batch: 030 ----
mean loss: 744.78
 ---- batch: 040 ----
mean loss: 738.95
 ---- batch: 050 ----
mean loss: 728.15
 ---- batch: 060 ----
mean loss: 717.59
 ---- batch: 070 ----
mean loss: 716.41
 ---- batch: 080 ----
mean loss: 750.62
 ---- batch: 090 ----
mean loss: 745.12
 ---- batch: 100 ----
mean loss: 720.16
 ---- batch: 110 ----
mean loss: 724.69
train mean loss: 732.69
epoch train time: 0:00:00.711379
elapsed time: 0:00:51.932288
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 01:26:19.799063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 727.57
 ---- batch: 020 ----
mean loss: 718.85
 ---- batch: 030 ----
mean loss: 744.70
 ---- batch: 040 ----
mean loss: 745.09
 ---- batch: 050 ----
mean loss: 724.82
 ---- batch: 060 ----
mean loss: 724.87
 ---- batch: 070 ----
mean loss: 722.01
 ---- batch: 080 ----
mean loss: 724.34
 ---- batch: 090 ----
mean loss: 732.58
 ---- batch: 100 ----
mean loss: 717.72
 ---- batch: 110 ----
mean loss: 726.82
train mean loss: 727.30
epoch train time: 0:00:00.713297
elapsed time: 0:00:52.645724
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 01:26:20.512498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.12
 ---- batch: 020 ----
mean loss: 720.56
 ---- batch: 030 ----
mean loss: 733.02
 ---- batch: 040 ----
mean loss: 742.65
 ---- batch: 050 ----
mean loss: 741.97
 ---- batch: 060 ----
mean loss: 713.35
 ---- batch: 070 ----
mean loss: 722.66
 ---- batch: 080 ----
mean loss: 722.53
 ---- batch: 090 ----
mean loss: 707.48
 ---- batch: 100 ----
mean loss: 725.65
 ---- batch: 110 ----
mean loss: 718.10
train mean loss: 721.96
epoch train time: 0:00:00.721378
elapsed time: 0:00:53.367240
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 01:26:21.234035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 724.61
 ---- batch: 020 ----
mean loss: 702.22
 ---- batch: 030 ----
mean loss: 712.74
 ---- batch: 040 ----
mean loss: 712.13
 ---- batch: 050 ----
mean loss: 725.24
 ---- batch: 060 ----
mean loss: 713.60
 ---- batch: 070 ----
mean loss: 715.46
 ---- batch: 080 ----
mean loss: 726.03
 ---- batch: 090 ----
mean loss: 716.49
 ---- batch: 100 ----
mean loss: 722.88
 ---- batch: 110 ----
mean loss: 715.39
train mean loss: 716.32
epoch train time: 0:00:00.716368
elapsed time: 0:00:54.083770
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 01:26:21.950544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 712.26
 ---- batch: 020 ----
mean loss: 705.70
 ---- batch: 030 ----
mean loss: 716.03
 ---- batch: 040 ----
mean loss: 717.46
 ---- batch: 050 ----
mean loss: 713.36
 ---- batch: 060 ----
mean loss: 712.24
 ---- batch: 070 ----
mean loss: 678.65
 ---- batch: 080 ----
mean loss: 719.81
 ---- batch: 090 ----
mean loss: 711.34
 ---- batch: 100 ----
mean loss: 712.51
 ---- batch: 110 ----
mean loss: 716.51
train mean loss: 710.57
epoch train time: 0:00:00.713817
elapsed time: 0:00:54.797721
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 01:26:22.664491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.35
 ---- batch: 020 ----
mean loss: 692.14
 ---- batch: 030 ----
mean loss: 720.60
 ---- batch: 040 ----
mean loss: 724.98
 ---- batch: 050 ----
mean loss: 687.67
 ---- batch: 060 ----
mean loss: 708.34
 ---- batch: 070 ----
mean loss: 706.05
 ---- batch: 080 ----
mean loss: 705.20
 ---- batch: 090 ----
mean loss: 700.15
 ---- batch: 100 ----
mean loss: 704.62
 ---- batch: 110 ----
mean loss: 691.77
train mean loss: 704.67
epoch train time: 0:00:00.719545
elapsed time: 0:00:55.517401
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 01:26:23.384193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.12
 ---- batch: 020 ----
mean loss: 691.73
 ---- batch: 030 ----
mean loss: 710.08
 ---- batch: 040 ----
mean loss: 712.49
 ---- batch: 050 ----
mean loss: 701.61
 ---- batch: 060 ----
mean loss: 707.73
 ---- batch: 070 ----
mean loss: 674.22
 ---- batch: 080 ----
mean loss: 706.68
 ---- batch: 090 ----
mean loss: 695.14
 ---- batch: 100 ----
mean loss: 704.64
 ---- batch: 110 ----
mean loss: 687.79
train mean loss: 698.14
epoch train time: 0:00:00.721153
elapsed time: 0:00:56.238729
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 01:26:24.105506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 688.27
 ---- batch: 020 ----
mean loss: 696.61
 ---- batch: 030 ----
mean loss: 685.90
 ---- batch: 040 ----
mean loss: 693.23
 ---- batch: 050 ----
mean loss: 677.28
 ---- batch: 060 ----
mean loss: 685.26
 ---- batch: 070 ----
mean loss: 699.04
 ---- batch: 080 ----
mean loss: 699.28
 ---- batch: 090 ----
mean loss: 698.55
 ---- batch: 100 ----
mean loss: 697.02
 ---- batch: 110 ----
mean loss: 693.29
train mean loss: 691.55
epoch train time: 0:00:00.713220
elapsed time: 0:00:56.952093
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 01:26:24.818867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.13
 ---- batch: 020 ----
mean loss: 660.27
 ---- batch: 030 ----
mean loss: 681.09
 ---- batch: 040 ----
mean loss: 697.90
 ---- batch: 050 ----
mean loss: 685.43
 ---- batch: 060 ----
mean loss: 694.23
 ---- batch: 070 ----
mean loss: 669.91
 ---- batch: 080 ----
mean loss: 692.60
 ---- batch: 090 ----
mean loss: 691.96
 ---- batch: 100 ----
mean loss: 691.99
 ---- batch: 110 ----
mean loss: 687.18
train mean loss: 684.62
epoch train time: 0:00:00.716683
elapsed time: 0:00:57.668912
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 01:26:25.535706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 706.35
 ---- batch: 020 ----
mean loss: 689.56
 ---- batch: 030 ----
mean loss: 676.34
 ---- batch: 040 ----
mean loss: 675.67
 ---- batch: 050 ----
mean loss: 677.10
 ---- batch: 060 ----
mean loss: 670.23
 ---- batch: 070 ----
mean loss: 673.74
 ---- batch: 080 ----
mean loss: 676.27
 ---- batch: 090 ----
mean loss: 663.43
 ---- batch: 100 ----
mean loss: 674.67
 ---- batch: 110 ----
mean loss: 668.06
train mean loss: 677.33
epoch train time: 0:00:00.720586
elapsed time: 0:00:58.389659
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 01:26:26.256434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.05
 ---- batch: 020 ----
mean loss: 669.95
 ---- batch: 030 ----
mean loss: 678.62
 ---- batch: 040 ----
mean loss: 671.34
 ---- batch: 050 ----
mean loss: 661.65
 ---- batch: 060 ----
mean loss: 677.46
 ---- batch: 070 ----
mean loss: 662.05
 ---- batch: 080 ----
mean loss: 659.19
 ---- batch: 090 ----
mean loss: 678.99
 ---- batch: 100 ----
mean loss: 672.18
 ---- batch: 110 ----
mean loss: 650.81
train mean loss: 669.65
epoch train time: 0:00:00.712869
elapsed time: 0:00:59.102665
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 01:26:26.969458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 668.05
 ---- batch: 020 ----
mean loss: 671.81
 ---- batch: 030 ----
mean loss: 664.76
 ---- batch: 040 ----
mean loss: 662.88
 ---- batch: 050 ----
mean loss: 680.40
 ---- batch: 060 ----
mean loss: 647.81
 ---- batch: 070 ----
mean loss: 654.21
 ---- batch: 080 ----
mean loss: 653.93
 ---- batch: 090 ----
mean loss: 648.77
 ---- batch: 100 ----
mean loss: 658.70
 ---- batch: 110 ----
mean loss: 672.49
train mean loss: 661.83
epoch train time: 0:00:00.704759
elapsed time: 0:00:59.807585
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 01:26:27.674359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 665.55
 ---- batch: 020 ----
mean loss: 659.30
 ---- batch: 030 ----
mean loss: 661.52
 ---- batch: 040 ----
mean loss: 657.62
 ---- batch: 050 ----
mean loss: 635.06
 ---- batch: 060 ----
mean loss: 647.80
 ---- batch: 070 ----
mean loss: 652.09
 ---- batch: 080 ----
mean loss: 661.86
 ---- batch: 090 ----
mean loss: 646.44
 ---- batch: 100 ----
mean loss: 646.19
 ---- batch: 110 ----
mean loss: 654.40
train mean loss: 653.77
epoch train time: 0:00:00.718870
elapsed time: 0:01:00.526592
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 01:26:28.393366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 648.17
 ---- batch: 020 ----
mean loss: 651.67
 ---- batch: 030 ----
mean loss: 636.79
 ---- batch: 040 ----
mean loss: 646.83
 ---- batch: 050 ----
mean loss: 648.24
 ---- batch: 060 ----
mean loss: 640.02
 ---- batch: 070 ----
mean loss: 643.31
 ---- batch: 080 ----
mean loss: 652.13
 ---- batch: 090 ----
mean loss: 642.11
 ---- batch: 100 ----
mean loss: 650.64
 ---- batch: 110 ----
mean loss: 647.36
train mean loss: 645.27
epoch train time: 0:00:00.719817
elapsed time: 0:01:01.246543
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 01:26:29.113316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 633.67
 ---- batch: 020 ----
mean loss: 650.55
 ---- batch: 030 ----
mean loss: 628.99
 ---- batch: 040 ----
mean loss: 645.64
 ---- batch: 050 ----
mean loss: 642.95
 ---- batch: 060 ----
mean loss: 638.18
 ---- batch: 070 ----
mean loss: 638.03
 ---- batch: 080 ----
mean loss: 640.23
 ---- batch: 090 ----
mean loss: 622.88
 ---- batch: 100 ----
mean loss: 631.52
 ---- batch: 110 ----
mean loss: 635.78
train mean loss: 636.70
epoch train time: 0:00:00.714207
elapsed time: 0:01:01.960884
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 01:26:29.827657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 638.44
 ---- batch: 020 ----
mean loss: 621.48
 ---- batch: 030 ----
mean loss: 631.24
 ---- batch: 040 ----
mean loss: 630.66
 ---- batch: 050 ----
mean loss: 620.67
 ---- batch: 060 ----
mean loss: 620.15
 ---- batch: 070 ----
mean loss: 640.79
 ---- batch: 080 ----
mean loss: 642.12
 ---- batch: 090 ----
mean loss: 630.19
 ---- batch: 100 ----
mean loss: 625.16
 ---- batch: 110 ----
mean loss: 612.69
train mean loss: 627.72
epoch train time: 0:00:00.717333
elapsed time: 0:01:02.678355
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 01:26:30.545144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 623.35
 ---- batch: 020 ----
mean loss: 615.93
 ---- batch: 030 ----
mean loss: 622.83
 ---- batch: 040 ----
mean loss: 624.86
 ---- batch: 050 ----
mean loss: 626.17
 ---- batch: 060 ----
mean loss: 617.79
 ---- batch: 070 ----
mean loss: 623.08
 ---- batch: 080 ----
mean loss: 617.62
 ---- batch: 090 ----
mean loss: 614.69
 ---- batch: 100 ----
mean loss: 602.17
 ---- batch: 110 ----
mean loss: 616.68
train mean loss: 618.56
epoch train time: 0:00:00.719661
elapsed time: 0:01:03.398214
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 01:26:31.265004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 606.99
 ---- batch: 020 ----
mean loss: 604.86
 ---- batch: 030 ----
mean loss: 621.55
 ---- batch: 040 ----
mean loss: 611.62
 ---- batch: 050 ----
mean loss: 610.38
 ---- batch: 060 ----
mean loss: 608.92
 ---- batch: 070 ----
mean loss: 593.31
 ---- batch: 080 ----
mean loss: 614.99
 ---- batch: 090 ----
mean loss: 603.46
 ---- batch: 100 ----
mean loss: 610.58
 ---- batch: 110 ----
mean loss: 602.26
train mean loss: 608.50
epoch train time: 0:00:00.717087
elapsed time: 0:01:04.115489
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 01:26:31.982277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.56
 ---- batch: 020 ----
mean loss: 599.93
 ---- batch: 030 ----
mean loss: 594.76
 ---- batch: 040 ----
mean loss: 602.15
 ---- batch: 050 ----
mean loss: 594.23
 ---- batch: 060 ----
mean loss: 623.53
 ---- batch: 070 ----
mean loss: 601.95
 ---- batch: 080 ----
mean loss: 586.39
 ---- batch: 090 ----
mean loss: 585.50
 ---- batch: 100 ----
mean loss: 573.13
 ---- batch: 110 ----
mean loss: 587.99
train mean loss: 594.85
epoch train time: 0:00:00.712394
elapsed time: 0:01:04.828037
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 01:26:32.694811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 569.31
 ---- batch: 020 ----
mean loss: 598.58
 ---- batch: 030 ----
mean loss: 588.94
 ---- batch: 040 ----
mean loss: 582.34
 ---- batch: 050 ----
mean loss: 568.33
 ---- batch: 060 ----
mean loss: 565.89
 ---- batch: 070 ----
mean loss: 564.25
 ---- batch: 080 ----
mean loss: 566.67
 ---- batch: 090 ----
mean loss: 555.29
 ---- batch: 100 ----
mean loss: 566.44
 ---- batch: 110 ----
mean loss: 564.71
train mean loss: 571.23
epoch train time: 0:00:00.722536
elapsed time: 0:01:05.550713
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 01:26:33.417523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 552.31
 ---- batch: 020 ----
mean loss: 556.50
 ---- batch: 030 ----
mean loss: 554.71
 ---- batch: 040 ----
mean loss: 546.51
 ---- batch: 050 ----
mean loss: 545.30
 ---- batch: 060 ----
mean loss: 530.13
 ---- batch: 070 ----
mean loss: 555.75
 ---- batch: 080 ----
mean loss: 535.60
 ---- batch: 090 ----
mean loss: 536.01
 ---- batch: 100 ----
mean loss: 529.01
 ---- batch: 110 ----
mean loss: 539.08
train mean loss: 543.52
epoch train time: 0:00:00.724684
elapsed time: 0:01:06.275588
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 01:26:34.142394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 526.92
 ---- batch: 020 ----
mean loss: 534.34
 ---- batch: 030 ----
mean loss: 518.37
 ---- batch: 040 ----
mean loss: 510.49
 ---- batch: 050 ----
mean loss: 525.15
 ---- batch: 060 ----
mean loss: 513.22
 ---- batch: 070 ----
mean loss: 530.88
 ---- batch: 080 ----
mean loss: 510.78
 ---- batch: 090 ----
mean loss: 519.95
 ---- batch: 100 ----
mean loss: 494.09
 ---- batch: 110 ----
mean loss: 506.19
train mean loss: 516.90
epoch train time: 0:00:00.713469
elapsed time: 0:01:06.989229
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 01:26:34.856022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.36
 ---- batch: 020 ----
mean loss: 497.88
 ---- batch: 030 ----
mean loss: 505.03
 ---- batch: 040 ----
mean loss: 484.21
 ---- batch: 050 ----
mean loss: 488.13
 ---- batch: 060 ----
mean loss: 501.85
 ---- batch: 070 ----
mean loss: 498.50
 ---- batch: 080 ----
mean loss: 497.05
 ---- batch: 090 ----
mean loss: 485.70
 ---- batch: 100 ----
mean loss: 484.82
 ---- batch: 110 ----
mean loss: 479.66
train mean loss: 493.79
epoch train time: 0:00:00.726022
elapsed time: 0:01:07.715410
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 01:26:35.582184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.49
 ---- batch: 020 ----
mean loss: 482.73
 ---- batch: 030 ----
mean loss: 475.41
 ---- batch: 040 ----
mean loss: 484.02
 ---- batch: 050 ----
mean loss: 467.09
 ---- batch: 060 ----
mean loss: 469.58
 ---- batch: 070 ----
mean loss: 468.95
 ---- batch: 080 ----
mean loss: 475.94
 ---- batch: 090 ----
mean loss: 475.77
 ---- batch: 100 ----
mean loss: 460.53
 ---- batch: 110 ----
mean loss: 451.43
train mean loss: 472.94
epoch train time: 0:00:00.719724
elapsed time: 0:01:08.435275
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 01:26:36.302053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.85
 ---- batch: 020 ----
mean loss: 462.02
 ---- batch: 030 ----
mean loss: 451.87
 ---- batch: 040 ----
mean loss: 461.12
 ---- batch: 050 ----
mean loss: 463.08
 ---- batch: 060 ----
mean loss: 456.73
 ---- batch: 070 ----
mean loss: 454.85
 ---- batch: 080 ----
mean loss: 449.21
 ---- batch: 090 ----
mean loss: 450.31
 ---- batch: 100 ----
mean loss: 447.46
 ---- batch: 110 ----
mean loss: 449.16
train mean loss: 455.18
epoch train time: 0:00:00.722575
elapsed time: 0:01:09.158009
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 01:26:37.024781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.98
 ---- batch: 020 ----
mean loss: 449.99
 ---- batch: 030 ----
mean loss: 459.45
 ---- batch: 040 ----
mean loss: 437.70
 ---- batch: 050 ----
mean loss: 430.87
 ---- batch: 060 ----
mean loss: 442.91
 ---- batch: 070 ----
mean loss: 436.37
 ---- batch: 080 ----
mean loss: 435.02
 ---- batch: 090 ----
mean loss: 428.29
 ---- batch: 100 ----
mean loss: 441.22
 ---- batch: 110 ----
mean loss: 437.04
train mean loss: 439.44
epoch train time: 0:00:00.706619
elapsed time: 0:01:09.864763
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 01:26:37.731539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.83
 ---- batch: 020 ----
mean loss: 440.60
 ---- batch: 030 ----
mean loss: 428.14
 ---- batch: 040 ----
mean loss: 419.58
 ---- batch: 050 ----
mean loss: 415.33
 ---- batch: 060 ----
mean loss: 424.41
 ---- batch: 070 ----
mean loss: 429.05
 ---- batch: 080 ----
mean loss: 428.32
 ---- batch: 090 ----
mean loss: 413.12
 ---- batch: 100 ----
mean loss: 414.84
 ---- batch: 110 ----
mean loss: 427.38
train mean loss: 425.27
epoch train time: 0:00:00.718928
elapsed time: 0:01:10.583831
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 01:26:38.450605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.23
 ---- batch: 020 ----
mean loss: 418.30
 ---- batch: 030 ----
mean loss: 421.18
 ---- batch: 040 ----
mean loss: 411.19
 ---- batch: 050 ----
mean loss: 404.87
 ---- batch: 060 ----
mean loss: 417.47
 ---- batch: 070 ----
mean loss: 415.00
 ---- batch: 080 ----
mean loss: 411.35
 ---- batch: 090 ----
mean loss: 400.57
 ---- batch: 100 ----
mean loss: 409.35
 ---- batch: 110 ----
mean loss: 404.30
train mean loss: 411.99
epoch train time: 0:00:00.716591
elapsed time: 0:01:11.300556
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 01:26:39.167327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.56
 ---- batch: 020 ----
mean loss: 397.86
 ---- batch: 030 ----
mean loss: 415.15
 ---- batch: 040 ----
mean loss: 410.28
 ---- batch: 050 ----
mean loss: 405.44
 ---- batch: 060 ----
mean loss: 384.09
 ---- batch: 070 ----
mean loss: 390.62
 ---- batch: 080 ----
mean loss: 388.96
 ---- batch: 090 ----
mean loss: 393.76
 ---- batch: 100 ----
mean loss: 409.46
 ---- batch: 110 ----
mean loss: 394.53
train mean loss: 399.47
epoch train time: 0:00:00.715142
elapsed time: 0:01:12.015833
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 01:26:39.882627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.34
 ---- batch: 020 ----
mean loss: 394.07
 ---- batch: 030 ----
mean loss: 398.87
 ---- batch: 040 ----
mean loss: 385.18
 ---- batch: 050 ----
mean loss: 391.43
 ---- batch: 060 ----
mean loss: 379.81
 ---- batch: 070 ----
mean loss: 372.89
 ---- batch: 080 ----
mean loss: 387.20
 ---- batch: 090 ----
mean loss: 391.96
 ---- batch: 100 ----
mean loss: 374.57
 ---- batch: 110 ----
mean loss: 381.75
train mean loss: 387.70
epoch train time: 0:00:00.715278
elapsed time: 0:01:12.731272
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 01:26:40.598047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.24
 ---- batch: 020 ----
mean loss: 379.94
 ---- batch: 030 ----
mean loss: 385.60
 ---- batch: 040 ----
mean loss: 377.84
 ---- batch: 050 ----
mean loss: 387.59
 ---- batch: 060 ----
mean loss: 387.52
 ---- batch: 070 ----
mean loss: 362.11
 ---- batch: 080 ----
mean loss: 363.04
 ---- batch: 090 ----
mean loss: 372.02
 ---- batch: 100 ----
mean loss: 368.63
 ---- batch: 110 ----
mean loss: 374.07
train mean loss: 376.67
epoch train time: 0:00:00.717571
elapsed time: 0:01:13.448982
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 01:26:41.315757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.87
 ---- batch: 020 ----
mean loss: 373.17
 ---- batch: 030 ----
mean loss: 359.86
 ---- batch: 040 ----
mean loss: 360.98
 ---- batch: 050 ----
mean loss: 363.96
 ---- batch: 060 ----
mean loss: 364.62
 ---- batch: 070 ----
mean loss: 367.44
 ---- batch: 080 ----
mean loss: 354.61
 ---- batch: 090 ----
mean loss: 372.67
 ---- batch: 100 ----
mean loss: 364.53
 ---- batch: 110 ----
mean loss: 373.51
train mean loss: 366.63
epoch train time: 0:00:00.715370
elapsed time: 0:01:14.164494
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 01:26:42.031269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.16
 ---- batch: 020 ----
mean loss: 365.76
 ---- batch: 030 ----
mean loss: 366.96
 ---- batch: 040 ----
mean loss: 360.89
 ---- batch: 050 ----
mean loss: 353.22
 ---- batch: 060 ----
mean loss: 337.93
 ---- batch: 070 ----
mean loss: 352.93
 ---- batch: 080 ----
mean loss: 361.73
 ---- batch: 090 ----
mean loss: 352.60
 ---- batch: 100 ----
mean loss: 360.31
 ---- batch: 110 ----
mean loss: 356.95
train mean loss: 356.94
epoch train time: 0:00:00.720939
elapsed time: 0:01:14.885573
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 01:26:42.752345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.29
 ---- batch: 020 ----
mean loss: 351.44
 ---- batch: 030 ----
mean loss: 359.39
 ---- batch: 040 ----
mean loss: 346.97
 ---- batch: 050 ----
mean loss: 341.25
 ---- batch: 060 ----
mean loss: 346.34
 ---- batch: 070 ----
mean loss: 329.75
 ---- batch: 080 ----
mean loss: 345.18
 ---- batch: 090 ----
mean loss: 358.64
 ---- batch: 100 ----
mean loss: 347.28
 ---- batch: 110 ----
mean loss: 344.65
train mean loss: 347.86
epoch train time: 0:00:00.728360
elapsed time: 0:01:15.614086
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 01:26:43.480891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.89
 ---- batch: 020 ----
mean loss: 334.01
 ---- batch: 030 ----
mean loss: 348.45
 ---- batch: 040 ----
mean loss: 341.52
 ---- batch: 050 ----
mean loss: 350.61
 ---- batch: 060 ----
mean loss: 332.24
 ---- batch: 070 ----
mean loss: 331.62
 ---- batch: 080 ----
mean loss: 334.37
 ---- batch: 090 ----
mean loss: 334.32
 ---- batch: 100 ----
mean loss: 340.22
 ---- batch: 110 ----
mean loss: 335.11
train mean loss: 339.13
epoch train time: 0:00:00.722297
elapsed time: 0:01:16.336550
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 01:26:44.203357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.75
 ---- batch: 020 ----
mean loss: 336.44
 ---- batch: 030 ----
mean loss: 333.25
 ---- batch: 040 ----
mean loss: 322.27
 ---- batch: 050 ----
mean loss: 331.93
 ---- batch: 060 ----
mean loss: 335.11
 ---- batch: 070 ----
mean loss: 340.71
 ---- batch: 080 ----
mean loss: 326.56
 ---- batch: 090 ----
mean loss: 336.73
 ---- batch: 100 ----
mean loss: 323.47
 ---- batch: 110 ----
mean loss: 338.96
train mean loss: 331.17
epoch train time: 0:00:00.713035
elapsed time: 0:01:17.049756
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 01:26:44.916529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.09
 ---- batch: 020 ----
mean loss: 319.65
 ---- batch: 030 ----
mean loss: 320.12
 ---- batch: 040 ----
mean loss: 331.63
 ---- batch: 050 ----
mean loss: 324.95
 ---- batch: 060 ----
mean loss: 319.97
 ---- batch: 070 ----
mean loss: 309.52
 ---- batch: 080 ----
mean loss: 328.18
 ---- batch: 090 ----
mean loss: 326.55
 ---- batch: 100 ----
mean loss: 323.59
 ---- batch: 110 ----
mean loss: 325.22
train mean loss: 323.93
epoch train time: 0:00:00.711173
elapsed time: 0:01:17.761062
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 01:26:45.627835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.07
 ---- batch: 020 ----
mean loss: 313.88
 ---- batch: 030 ----
mean loss: 317.19
 ---- batch: 040 ----
mean loss: 318.83
 ---- batch: 050 ----
mean loss: 309.52
 ---- batch: 060 ----
mean loss: 314.34
 ---- batch: 070 ----
mean loss: 324.79
 ---- batch: 080 ----
mean loss: 312.46
 ---- batch: 090 ----
mean loss: 314.00
 ---- batch: 100 ----
mean loss: 318.70
 ---- batch: 110 ----
mean loss: 330.12
train mean loss: 317.04
epoch train time: 0:00:00.727897
elapsed time: 0:01:18.489099
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 01:26:46.355875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.28
 ---- batch: 020 ----
mean loss: 313.60
 ---- batch: 030 ----
mean loss: 311.41
 ---- batch: 040 ----
mean loss: 298.32
 ---- batch: 050 ----
mean loss: 310.49
 ---- batch: 060 ----
mean loss: 311.01
 ---- batch: 070 ----
mean loss: 309.22
 ---- batch: 080 ----
mean loss: 314.53
 ---- batch: 090 ----
mean loss: 321.97
 ---- batch: 100 ----
mean loss: 307.95
 ---- batch: 110 ----
mean loss: 303.80
train mean loss: 310.87
epoch train time: 0:00:00.718939
elapsed time: 0:01:19.208178
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 01:26:47.074952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.19
 ---- batch: 020 ----
mean loss: 300.57
 ---- batch: 030 ----
mean loss: 305.44
 ---- batch: 040 ----
mean loss: 309.47
 ---- batch: 050 ----
mean loss: 318.32
 ---- batch: 060 ----
mean loss: 300.42
 ---- batch: 070 ----
mean loss: 305.51
 ---- batch: 080 ----
mean loss: 303.01
 ---- batch: 090 ----
mean loss: 296.84
 ---- batch: 100 ----
mean loss: 294.30
 ---- batch: 110 ----
mean loss: 307.72
train mean loss: 305.05
epoch train time: 0:00:00.709217
elapsed time: 0:01:19.917549
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 01:26:47.784324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.18
 ---- batch: 020 ----
mean loss: 303.92
 ---- batch: 030 ----
mean loss: 303.26
 ---- batch: 040 ----
mean loss: 298.93
 ---- batch: 050 ----
mean loss: 295.53
 ---- batch: 060 ----
mean loss: 312.16
 ---- batch: 070 ----
mean loss: 298.83
 ---- batch: 080 ----
mean loss: 303.75
 ---- batch: 090 ----
mean loss: 293.42
 ---- batch: 100 ----
mean loss: 298.05
 ---- batch: 110 ----
mean loss: 289.80
train mean loss: 299.81
epoch train time: 0:00:00.707771
elapsed time: 0:01:20.625499
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 01:26:48.492273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.59
 ---- batch: 020 ----
mean loss: 297.37
 ---- batch: 030 ----
mean loss: 291.01
 ---- batch: 040 ----
mean loss: 298.55
 ---- batch: 050 ----
mean loss: 294.36
 ---- batch: 060 ----
mean loss: 288.84
 ---- batch: 070 ----
mean loss: 289.55
 ---- batch: 080 ----
mean loss: 293.05
 ---- batch: 090 ----
mean loss: 296.28
 ---- batch: 100 ----
mean loss: 290.75
 ---- batch: 110 ----
mean loss: 302.52
train mean loss: 294.89
epoch train time: 0:00:00.716411
elapsed time: 0:01:21.342046
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 01:26:49.208839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.07
 ---- batch: 020 ----
mean loss: 295.37
 ---- batch: 030 ----
mean loss: 285.46
 ---- batch: 040 ----
mean loss: 296.43
 ---- batch: 050 ----
mean loss: 293.15
 ---- batch: 060 ----
mean loss: 299.84
 ---- batch: 070 ----
mean loss: 293.70
 ---- batch: 080 ----
mean loss: 285.10
 ---- batch: 090 ----
mean loss: 284.92
 ---- batch: 100 ----
mean loss: 277.89
 ---- batch: 110 ----
mean loss: 294.77
train mean loss: 289.99
epoch train time: 0:00:00.715617
elapsed time: 0:01:22.057824
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 01:26:49.924599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.38
 ---- batch: 020 ----
mean loss: 283.28
 ---- batch: 030 ----
mean loss: 290.05
 ---- batch: 040 ----
mean loss: 276.71
 ---- batch: 050 ----
mean loss: 280.78
 ---- batch: 060 ----
mean loss: 293.50
 ---- batch: 070 ----
mean loss: 288.34
 ---- batch: 080 ----
mean loss: 279.94
 ---- batch: 090 ----
mean loss: 290.62
 ---- batch: 100 ----
mean loss: 277.43
 ---- batch: 110 ----
mean loss: 287.26
train mean loss: 285.52
epoch train time: 0:00:00.724011
elapsed time: 0:01:22.781973
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 01:26:50.648747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.52
 ---- batch: 020 ----
mean loss: 283.17
 ---- batch: 030 ----
mean loss: 285.15
 ---- batch: 040 ----
mean loss: 285.77
 ---- batch: 050 ----
mean loss: 276.32
 ---- batch: 060 ----
mean loss: 271.34
 ---- batch: 070 ----
mean loss: 284.67
 ---- batch: 080 ----
mean loss: 281.02
 ---- batch: 090 ----
mean loss: 283.52
 ---- batch: 100 ----
mean loss: 281.48
 ---- batch: 110 ----
mean loss: 279.68
train mean loss: 280.98
epoch train time: 0:00:00.732477
elapsed time: 0:01:23.514635
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 01:26:51.381432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.48
 ---- batch: 020 ----
mean loss: 276.23
 ---- batch: 030 ----
mean loss: 283.90
 ---- batch: 040 ----
mean loss: 275.98
 ---- batch: 050 ----
mean loss: 255.34
 ---- batch: 060 ----
mean loss: 277.50
 ---- batch: 070 ----
mean loss: 281.48
 ---- batch: 080 ----
mean loss: 287.95
 ---- batch: 090 ----
mean loss: 276.22
 ---- batch: 100 ----
mean loss: 279.32
 ---- batch: 110 ----
mean loss: 271.82
train mean loss: 276.74
epoch train time: 0:00:00.729633
elapsed time: 0:01:24.244430
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 01:26:52.111205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.83
 ---- batch: 020 ----
mean loss: 271.98
 ---- batch: 030 ----
mean loss: 272.94
 ---- batch: 040 ----
mean loss: 278.91
 ---- batch: 050 ----
mean loss: 279.28
 ---- batch: 060 ----
mean loss: 270.51
 ---- batch: 070 ----
mean loss: 268.84
 ---- batch: 080 ----
mean loss: 274.66
 ---- batch: 090 ----
mean loss: 276.14
 ---- batch: 100 ----
mean loss: 274.14
 ---- batch: 110 ----
mean loss: 271.65
train mean loss: 272.64
epoch train time: 0:00:00.716837
elapsed time: 0:01:24.961409
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 01:26:52.828184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.15
 ---- batch: 020 ----
mean loss: 272.55
 ---- batch: 030 ----
mean loss: 261.58
 ---- batch: 040 ----
mean loss: 267.13
 ---- batch: 050 ----
mean loss: 261.67
 ---- batch: 060 ----
mean loss: 274.55
 ---- batch: 070 ----
mean loss: 271.91
 ---- batch: 080 ----
mean loss: 275.70
 ---- batch: 090 ----
mean loss: 268.51
 ---- batch: 100 ----
mean loss: 267.98
 ---- batch: 110 ----
mean loss: 264.49
train mean loss: 269.03
epoch train time: 0:00:00.715732
elapsed time: 0:01:25.677283
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 01:26:53.544056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.59
 ---- batch: 020 ----
mean loss: 263.02
 ---- batch: 030 ----
mean loss: 261.47
 ---- batch: 040 ----
mean loss: 262.53
 ---- batch: 050 ----
mean loss: 260.16
 ---- batch: 060 ----
mean loss: 273.57
 ---- batch: 070 ----
mean loss: 265.84
 ---- batch: 080 ----
mean loss: 267.08
 ---- batch: 090 ----
mean loss: 263.83
 ---- batch: 100 ----
mean loss: 267.62
 ---- batch: 110 ----
mean loss: 264.41
train mean loss: 265.78
epoch train time: 0:00:00.708665
elapsed time: 0:01:26.386096
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 01:26:54.252932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.93
 ---- batch: 020 ----
mean loss: 264.44
 ---- batch: 030 ----
mean loss: 274.75
 ---- batch: 040 ----
mean loss: 266.58
 ---- batch: 050 ----
mean loss: 269.16
 ---- batch: 060 ----
mean loss: 256.37
 ---- batch: 070 ----
mean loss: 256.57
 ---- batch: 080 ----
mean loss: 257.73
 ---- batch: 090 ----
mean loss: 263.26
 ---- batch: 100 ----
mean loss: 259.41
 ---- batch: 110 ----
mean loss: 261.19
train mean loss: 262.64
epoch train time: 0:00:00.706182
elapsed time: 0:01:27.092474
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 01:26:54.959244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.02
 ---- batch: 020 ----
mean loss: 256.92
 ---- batch: 030 ----
mean loss: 261.78
 ---- batch: 040 ----
mean loss: 265.81
 ---- batch: 050 ----
mean loss: 259.72
 ---- batch: 060 ----
mean loss: 263.92
 ---- batch: 070 ----
mean loss: 257.90
 ---- batch: 080 ----
mean loss: 257.76
 ---- batch: 090 ----
mean loss: 260.22
 ---- batch: 100 ----
mean loss: 257.69
 ---- batch: 110 ----
mean loss: 262.34
train mean loss: 259.69
epoch train time: 0:00:00.701156
elapsed time: 0:01:27.793760
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 01:26:55.660546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.28
 ---- batch: 020 ----
mean loss: 261.93
 ---- batch: 030 ----
mean loss: 252.15
 ---- batch: 040 ----
mean loss: 260.70
 ---- batch: 050 ----
mean loss: 264.92
 ---- batch: 060 ----
mean loss: 258.53
 ---- batch: 070 ----
mean loss: 251.18
 ---- batch: 080 ----
mean loss: 254.96
 ---- batch: 090 ----
mean loss: 264.29
 ---- batch: 100 ----
mean loss: 256.12
 ---- batch: 110 ----
mean loss: 247.62
train mean loss: 257.41
epoch train time: 0:00:00.711676
elapsed time: 0:01:28.505602
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 01:26:56.372383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.06
 ---- batch: 020 ----
mean loss: 256.26
 ---- batch: 030 ----
mean loss: 251.03
 ---- batch: 040 ----
mean loss: 256.55
 ---- batch: 050 ----
mean loss: 257.95
 ---- batch: 060 ----
mean loss: 262.21
 ---- batch: 070 ----
mean loss: 257.50
 ---- batch: 080 ----
mean loss: 260.55
 ---- batch: 090 ----
mean loss: 248.18
 ---- batch: 100 ----
mean loss: 250.30
 ---- batch: 110 ----
mean loss: 252.98
train mean loss: 255.08
epoch train time: 0:00:00.712223
elapsed time: 0:01:29.217967
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 01:26:57.084740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.32
 ---- batch: 020 ----
mean loss: 251.84
 ---- batch: 030 ----
mean loss: 254.37
 ---- batch: 040 ----
mean loss: 259.84
 ---- batch: 050 ----
mean loss: 254.59
 ---- batch: 060 ----
mean loss: 253.11
 ---- batch: 070 ----
mean loss: 250.84
 ---- batch: 080 ----
mean loss: 249.98
 ---- batch: 090 ----
mean loss: 260.53
 ---- batch: 100 ----
mean loss: 237.08
 ---- batch: 110 ----
mean loss: 262.06
train mean loss: 253.09
epoch train time: 0:00:00.711228
elapsed time: 0:01:29.929332
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 01:26:57.796108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.55
 ---- batch: 020 ----
mean loss: 258.55
 ---- batch: 030 ----
mean loss: 247.12
 ---- batch: 040 ----
mean loss: 245.14
 ---- batch: 050 ----
mean loss: 255.33
 ---- batch: 060 ----
mean loss: 252.20
 ---- batch: 070 ----
mean loss: 258.63
 ---- batch: 080 ----
mean loss: 247.22
 ---- batch: 090 ----
mean loss: 252.55
 ---- batch: 100 ----
mean loss: 254.16
 ---- batch: 110 ----
mean loss: 242.45
train mean loss: 251.18
epoch train time: 0:00:00.712500
elapsed time: 0:01:30.641971
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 01:26:58.508760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.72
 ---- batch: 020 ----
mean loss: 252.53
 ---- batch: 030 ----
mean loss: 245.59
 ---- batch: 040 ----
mean loss: 248.09
 ---- batch: 050 ----
mean loss: 246.92
 ---- batch: 060 ----
mean loss: 249.83
 ---- batch: 070 ----
mean loss: 242.58
 ---- batch: 080 ----
mean loss: 260.46
 ---- batch: 090 ----
mean loss: 255.11
 ---- batch: 100 ----
mean loss: 238.07
 ---- batch: 110 ----
mean loss: 256.57
train mean loss: 249.38
epoch train time: 0:00:00.716402
elapsed time: 0:01:31.358523
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 01:26:59.225295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.42
 ---- batch: 020 ----
mean loss: 254.83
 ---- batch: 030 ----
mean loss: 254.65
 ---- batch: 040 ----
mean loss: 249.97
 ---- batch: 050 ----
mean loss: 246.45
 ---- batch: 060 ----
mean loss: 248.74
 ---- batch: 070 ----
mean loss: 248.86
 ---- batch: 080 ----
mean loss: 247.21
 ---- batch: 090 ----
mean loss: 242.59
 ---- batch: 100 ----
mean loss: 246.31
 ---- batch: 110 ----
mean loss: 248.71
train mean loss: 248.09
epoch train time: 0:00:00.714893
elapsed time: 0:01:32.073550
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 01:26:59.940359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.36
 ---- batch: 020 ----
mean loss: 252.62
 ---- batch: 030 ----
mean loss: 243.63
 ---- batch: 040 ----
mean loss: 243.60
 ---- batch: 050 ----
mean loss: 249.30
 ---- batch: 060 ----
mean loss: 245.31
 ---- batch: 070 ----
mean loss: 235.33
 ---- batch: 080 ----
mean loss: 244.61
 ---- batch: 090 ----
mean loss: 245.25
 ---- batch: 100 ----
mean loss: 250.29
 ---- batch: 110 ----
mean loss: 245.21
train mean loss: 246.24
epoch train time: 0:00:00.709842
elapsed time: 0:01:32.783565
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 01:27:00.650338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.42
 ---- batch: 020 ----
mean loss: 247.58
 ---- batch: 030 ----
mean loss: 249.37
 ---- batch: 040 ----
mean loss: 242.11
 ---- batch: 050 ----
mean loss: 235.60
 ---- batch: 060 ----
mean loss: 241.72
 ---- batch: 070 ----
mean loss: 249.35
 ---- batch: 080 ----
mean loss: 253.90
 ---- batch: 090 ----
mean loss: 243.25
 ---- batch: 100 ----
mean loss: 247.65
 ---- batch: 110 ----
mean loss: 240.22
train mean loss: 244.94
epoch train time: 0:00:00.715558
elapsed time: 0:01:33.499257
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 01:27:01.366029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.36
 ---- batch: 020 ----
mean loss: 252.13
 ---- batch: 030 ----
mean loss: 233.87
 ---- batch: 040 ----
mean loss: 248.52
 ---- batch: 050 ----
mean loss: 250.44
 ---- batch: 060 ----
mean loss: 241.00
 ---- batch: 070 ----
mean loss: 242.47
 ---- batch: 080 ----
mean loss: 249.89
 ---- batch: 090 ----
mean loss: 244.54
 ---- batch: 100 ----
mean loss: 242.92
 ---- batch: 110 ----
mean loss: 237.17
train mean loss: 243.44
epoch train time: 0:00:00.711065
elapsed time: 0:01:34.210454
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 01:27:02.077226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.32
 ---- batch: 020 ----
mean loss: 253.31
 ---- batch: 030 ----
mean loss: 238.87
 ---- batch: 040 ----
mean loss: 240.40
 ---- batch: 050 ----
mean loss: 245.68
 ---- batch: 060 ----
mean loss: 240.66
 ---- batch: 070 ----
mean loss: 234.89
 ---- batch: 080 ----
mean loss: 234.68
 ---- batch: 090 ----
mean loss: 245.85
 ---- batch: 100 ----
mean loss: 241.08
 ---- batch: 110 ----
mean loss: 242.69
train mean loss: 242.18
epoch train time: 0:00:00.703960
elapsed time: 0:01:34.914560
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 01:27:02.781333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.69
 ---- batch: 020 ----
mean loss: 243.95
 ---- batch: 030 ----
mean loss: 237.98
 ---- batch: 040 ----
mean loss: 232.80
 ---- batch: 050 ----
mean loss: 238.91
 ---- batch: 060 ----
mean loss: 240.36
 ---- batch: 070 ----
mean loss: 240.03
 ---- batch: 080 ----
mean loss: 249.17
 ---- batch: 090 ----
mean loss: 239.34
 ---- batch: 100 ----
mean loss: 235.71
 ---- batch: 110 ----
mean loss: 240.25
train mean loss: 240.91
epoch train time: 0:00:00.709608
elapsed time: 0:01:35.624304
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 01:27:03.491094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.01
 ---- batch: 020 ----
mean loss: 236.85
 ---- batch: 030 ----
mean loss: 246.22
 ---- batch: 040 ----
mean loss: 241.55
 ---- batch: 050 ----
mean loss: 245.54
 ---- batch: 060 ----
mean loss: 234.75
 ---- batch: 070 ----
mean loss: 239.02
 ---- batch: 080 ----
mean loss: 243.55
 ---- batch: 090 ----
mean loss: 241.48
 ---- batch: 100 ----
mean loss: 240.15
 ---- batch: 110 ----
mean loss: 238.31
train mean loss: 239.70
epoch train time: 0:00:00.714147
elapsed time: 0:01:36.338618
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 01:27:04.205432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.94
 ---- batch: 020 ----
mean loss: 243.16
 ---- batch: 030 ----
mean loss: 241.67
 ---- batch: 040 ----
mean loss: 242.88
 ---- batch: 050 ----
mean loss: 234.70
 ---- batch: 060 ----
mean loss: 235.93
 ---- batch: 070 ----
mean loss: 239.98
 ---- batch: 080 ----
mean loss: 235.32
 ---- batch: 090 ----
mean loss: 235.77
 ---- batch: 100 ----
mean loss: 238.22
 ---- batch: 110 ----
mean loss: 233.01
train mean loss: 238.78
epoch train time: 0:00:00.711113
elapsed time: 0:01:37.049922
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 01:27:04.916695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.11
 ---- batch: 020 ----
mean loss: 234.85
 ---- batch: 030 ----
mean loss: 234.14
 ---- batch: 040 ----
mean loss: 236.85
 ---- batch: 050 ----
mean loss: 233.09
 ---- batch: 060 ----
mean loss: 242.25
 ---- batch: 070 ----
mean loss: 243.94
 ---- batch: 080 ----
mean loss: 246.28
 ---- batch: 090 ----
mean loss: 234.32
 ---- batch: 100 ----
mean loss: 237.45
 ---- batch: 110 ----
mean loss: 237.91
train mean loss: 237.56
epoch train time: 0:00:00.706080
elapsed time: 0:01:37.756137
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 01:27:05.622909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.80
 ---- batch: 020 ----
mean loss: 245.87
 ---- batch: 030 ----
mean loss: 244.35
 ---- batch: 040 ----
mean loss: 232.08
 ---- batch: 050 ----
mean loss: 235.73
 ---- batch: 060 ----
mean loss: 240.01
 ---- batch: 070 ----
mean loss: 235.24
 ---- batch: 080 ----
mean loss: 233.30
 ---- batch: 090 ----
mean loss: 235.15
 ---- batch: 100 ----
mean loss: 226.39
 ---- batch: 110 ----
mean loss: 238.58
train mean loss: 236.65
epoch train time: 0:00:00.718362
elapsed time: 0:01:38.474638
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 01:27:06.341415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.70
 ---- batch: 020 ----
mean loss: 240.65
 ---- batch: 030 ----
mean loss: 232.20
 ---- batch: 040 ----
mean loss: 241.73
 ---- batch: 050 ----
mean loss: 228.38
 ---- batch: 060 ----
mean loss: 234.74
 ---- batch: 070 ----
mean loss: 239.37
 ---- batch: 080 ----
mean loss: 231.99
 ---- batch: 090 ----
mean loss: 230.08
 ---- batch: 100 ----
mean loss: 231.50
 ---- batch: 110 ----
mean loss: 240.91
train mean loss: 235.71
epoch train time: 0:00:00.721997
elapsed time: 0:01:39.196774
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 01:27:07.063550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.60
 ---- batch: 020 ----
mean loss: 238.31
 ---- batch: 030 ----
mean loss: 236.94
 ---- batch: 040 ----
mean loss: 240.33
 ---- batch: 050 ----
mean loss: 228.37
 ---- batch: 060 ----
mean loss: 228.01
 ---- batch: 070 ----
mean loss: 235.40
 ---- batch: 080 ----
mean loss: 229.13
 ---- batch: 090 ----
mean loss: 240.32
 ---- batch: 100 ----
mean loss: 234.37
 ---- batch: 110 ----
mean loss: 235.11
train mean loss: 234.69
epoch train time: 0:00:00.704252
elapsed time: 0:01:39.901187
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 01:27:07.768001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.03
 ---- batch: 020 ----
mean loss: 243.18
 ---- batch: 030 ----
mean loss: 227.55
 ---- batch: 040 ----
mean loss: 231.14
 ---- batch: 050 ----
mean loss: 231.47
 ---- batch: 060 ----
mean loss: 232.10
 ---- batch: 070 ----
mean loss: 238.99
 ---- batch: 080 ----
mean loss: 238.09
 ---- batch: 090 ----
mean loss: 235.78
 ---- batch: 100 ----
mean loss: 233.76
 ---- batch: 110 ----
mean loss: 230.44
train mean loss: 233.98
epoch train time: 0:00:00.710650
elapsed time: 0:01:40.612016
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 01:27:08.478790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.80
 ---- batch: 020 ----
mean loss: 237.34
 ---- batch: 030 ----
mean loss: 235.33
 ---- batch: 040 ----
mean loss: 230.91
 ---- batch: 050 ----
mean loss: 236.05
 ---- batch: 060 ----
mean loss: 235.01
 ---- batch: 070 ----
mean loss: 234.44
 ---- batch: 080 ----
mean loss: 233.00
 ---- batch: 090 ----
mean loss: 234.88
 ---- batch: 100 ----
mean loss: 230.38
 ---- batch: 110 ----
mean loss: 227.53
train mean loss: 233.00
epoch train time: 0:00:00.708982
elapsed time: 0:01:41.321138
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 01:27:09.187911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.06
 ---- batch: 020 ----
mean loss: 238.58
 ---- batch: 030 ----
mean loss: 234.96
 ---- batch: 040 ----
mean loss: 235.90
 ---- batch: 050 ----
mean loss: 238.99
 ---- batch: 060 ----
mean loss: 227.02
 ---- batch: 070 ----
mean loss: 226.18
 ---- batch: 080 ----
mean loss: 227.52
 ---- batch: 090 ----
mean loss: 231.69
 ---- batch: 100 ----
mean loss: 227.42
 ---- batch: 110 ----
mean loss: 231.56
train mean loss: 232.34
epoch train time: 0:00:00.711360
elapsed time: 0:01:42.032651
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 01:27:09.899433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.41
 ---- batch: 020 ----
mean loss: 223.70
 ---- batch: 030 ----
mean loss: 214.63
 ---- batch: 040 ----
mean loss: 237.26
 ---- batch: 050 ----
mean loss: 246.97
 ---- batch: 060 ----
mean loss: 234.83
 ---- batch: 070 ----
mean loss: 231.21
 ---- batch: 080 ----
mean loss: 227.23
 ---- batch: 090 ----
mean loss: 226.94
 ---- batch: 100 ----
mean loss: 233.09
 ---- batch: 110 ----
mean loss: 240.01
train mean loss: 231.37
epoch train time: 0:00:00.708588
elapsed time: 0:01:42.741413
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 01:27:10.608186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.28
 ---- batch: 020 ----
mean loss: 222.95
 ---- batch: 030 ----
mean loss: 229.84
 ---- batch: 040 ----
mean loss: 230.98
 ---- batch: 050 ----
mean loss: 240.93
 ---- batch: 060 ----
mean loss: 224.65
 ---- batch: 070 ----
mean loss: 230.55
 ---- batch: 080 ----
mean loss: 236.33
 ---- batch: 090 ----
mean loss: 233.50
 ---- batch: 100 ----
mean loss: 234.51
 ---- batch: 110 ----
mean loss: 229.99
train mean loss: 230.60
epoch train time: 0:00:00.708545
elapsed time: 0:01:43.450095
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 01:27:11.316869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.28
 ---- batch: 020 ----
mean loss: 228.38
 ---- batch: 030 ----
mean loss: 231.05
 ---- batch: 040 ----
mean loss: 221.53
 ---- batch: 050 ----
mean loss: 227.35
 ---- batch: 060 ----
mean loss: 235.74
 ---- batch: 070 ----
mean loss: 230.14
 ---- batch: 080 ----
mean loss: 243.71
 ---- batch: 090 ----
mean loss: 233.27
 ---- batch: 100 ----
mean loss: 226.90
 ---- batch: 110 ----
mean loss: 225.71
train mean loss: 229.68
epoch train time: 0:00:00.705096
elapsed time: 0:01:44.155330
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 01:27:12.022102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.61
 ---- batch: 020 ----
mean loss: 235.31
 ---- batch: 030 ----
mean loss: 223.38
 ---- batch: 040 ----
mean loss: 237.63
 ---- batch: 050 ----
mean loss: 220.71
 ---- batch: 060 ----
mean loss: 234.65
 ---- batch: 070 ----
mean loss: 220.52
 ---- batch: 080 ----
mean loss: 219.58
 ---- batch: 090 ----
mean loss: 225.80
 ---- batch: 100 ----
mean loss: 230.10
 ---- batch: 110 ----
mean loss: 238.98
train mean loss: 229.04
epoch train time: 0:00:00.709963
elapsed time: 0:01:44.865428
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 01:27:12.732217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.53
 ---- batch: 020 ----
mean loss: 229.40
 ---- batch: 030 ----
mean loss: 237.46
 ---- batch: 040 ----
mean loss: 225.63
 ---- batch: 050 ----
mean loss: 232.19
 ---- batch: 060 ----
mean loss: 227.75
 ---- batch: 070 ----
mean loss: 228.58
 ---- batch: 080 ----
mean loss: 224.57
 ---- batch: 090 ----
mean loss: 220.48
 ---- batch: 100 ----
mean loss: 234.64
 ---- batch: 110 ----
mean loss: 228.31
train mean loss: 228.27
epoch train time: 0:00:00.707826
elapsed time: 0:01:45.573420
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 01:27:13.440191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.87
 ---- batch: 020 ----
mean loss: 229.30
 ---- batch: 030 ----
mean loss: 230.16
 ---- batch: 040 ----
mean loss: 224.91
 ---- batch: 050 ----
mean loss: 231.85
 ---- batch: 060 ----
mean loss: 221.08
 ---- batch: 070 ----
mean loss: 227.46
 ---- batch: 080 ----
mean loss: 231.77
 ---- batch: 090 ----
mean loss: 225.87
 ---- batch: 100 ----
mean loss: 222.96
 ---- batch: 110 ----
mean loss: 226.23
train mean loss: 227.59
epoch train time: 0:00:00.718678
elapsed time: 0:01:46.292237
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 01:27:14.159011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.72
 ---- batch: 020 ----
mean loss: 226.58
 ---- batch: 030 ----
mean loss: 234.14
 ---- batch: 040 ----
mean loss: 228.92
 ---- batch: 050 ----
mean loss: 226.45
 ---- batch: 060 ----
mean loss: 232.22
 ---- batch: 070 ----
mean loss: 220.30
 ---- batch: 080 ----
mean loss: 231.21
 ---- batch: 090 ----
mean loss: 220.27
 ---- batch: 100 ----
mean loss: 224.80
 ---- batch: 110 ----
mean loss: 221.39
train mean loss: 226.83
epoch train time: 0:00:00.725027
elapsed time: 0:01:47.017408
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 01:27:14.884188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.66
 ---- batch: 020 ----
mean loss: 221.60
 ---- batch: 030 ----
mean loss: 225.14
 ---- batch: 040 ----
mean loss: 225.03
 ---- batch: 050 ----
mean loss: 224.11
 ---- batch: 060 ----
mean loss: 228.56
 ---- batch: 070 ----
mean loss: 222.08
 ---- batch: 080 ----
mean loss: 220.37
 ---- batch: 090 ----
mean loss: 223.55
 ---- batch: 100 ----
mean loss: 227.59
 ---- batch: 110 ----
mean loss: 234.75
train mean loss: 226.22
epoch train time: 0:00:00.711910
elapsed time: 0:01:47.729462
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 01:27:15.596234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.04
 ---- batch: 020 ----
mean loss: 228.47
 ---- batch: 030 ----
mean loss: 225.89
 ---- batch: 040 ----
mean loss: 222.30
 ---- batch: 050 ----
mean loss: 223.99
 ---- batch: 060 ----
mean loss: 219.00
 ---- batch: 070 ----
mean loss: 234.14
 ---- batch: 080 ----
mean loss: 218.59
 ---- batch: 090 ----
mean loss: 224.26
 ---- batch: 100 ----
mean loss: 229.00
 ---- batch: 110 ----
mean loss: 235.58
train mean loss: 225.54
epoch train time: 0:00:00.718466
elapsed time: 0:01:48.448063
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 01:27:16.314838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.70
 ---- batch: 020 ----
mean loss: 223.29
 ---- batch: 030 ----
mean loss: 224.71
 ---- batch: 040 ----
mean loss: 220.91
 ---- batch: 050 ----
mean loss: 219.24
 ---- batch: 060 ----
mean loss: 230.72
 ---- batch: 070 ----
mean loss: 226.19
 ---- batch: 080 ----
mean loss: 221.54
 ---- batch: 090 ----
mean loss: 224.22
 ---- batch: 100 ----
mean loss: 233.21
 ---- batch: 110 ----
mean loss: 218.48
train mean loss: 224.86
epoch train time: 0:00:00.720539
elapsed time: 0:01:49.168755
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 01:27:17.035540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.76
 ---- batch: 020 ----
mean loss: 233.61
 ---- batch: 030 ----
mean loss: 218.39
 ---- batch: 040 ----
mean loss: 224.48
 ---- batch: 050 ----
mean loss: 227.17
 ---- batch: 060 ----
mean loss: 221.71
 ---- batch: 070 ----
mean loss: 219.03
 ---- batch: 080 ----
mean loss: 229.03
 ---- batch: 090 ----
mean loss: 230.84
 ---- batch: 100 ----
mean loss: 214.28
 ---- batch: 110 ----
mean loss: 226.56
train mean loss: 224.32
epoch train time: 0:00:00.722971
elapsed time: 0:01:49.891885
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 01:27:17.758663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.07
 ---- batch: 020 ----
mean loss: 226.92
 ---- batch: 030 ----
mean loss: 227.75
 ---- batch: 040 ----
mean loss: 222.87
 ---- batch: 050 ----
mean loss: 231.41
 ---- batch: 060 ----
mean loss: 216.07
 ---- batch: 070 ----
mean loss: 232.91
 ---- batch: 080 ----
mean loss: 223.54
 ---- batch: 090 ----
mean loss: 226.90
 ---- batch: 100 ----
mean loss: 209.88
 ---- batch: 110 ----
mean loss: 217.03
train mean loss: 223.63
epoch train time: 0:00:00.724688
elapsed time: 0:01:50.616717
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 01:27:18.483490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.72
 ---- batch: 020 ----
mean loss: 228.24
 ---- batch: 030 ----
mean loss: 212.62
 ---- batch: 040 ----
mean loss: 219.42
 ---- batch: 050 ----
mean loss: 225.51
 ---- batch: 060 ----
mean loss: 218.62
 ---- batch: 070 ----
mean loss: 226.35
 ---- batch: 080 ----
mean loss: 222.20
 ---- batch: 090 ----
mean loss: 226.11
 ---- batch: 100 ----
mean loss: 229.40
 ---- batch: 110 ----
mean loss: 223.78
train mean loss: 223.12
epoch train time: 0:00:00.720618
elapsed time: 0:01:51.337472
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 01:27:19.204245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.12
 ---- batch: 020 ----
mean loss: 215.61
 ---- batch: 030 ----
mean loss: 226.76
 ---- batch: 040 ----
mean loss: 215.58
 ---- batch: 050 ----
mean loss: 223.17
 ---- batch: 060 ----
mean loss: 217.29
 ---- batch: 070 ----
mean loss: 229.96
 ---- batch: 080 ----
mean loss: 222.41
 ---- batch: 090 ----
mean loss: 216.87
 ---- batch: 100 ----
mean loss: 227.48
 ---- batch: 110 ----
mean loss: 229.12
train mean loss: 222.57
epoch train time: 0:00:00.713450
elapsed time: 0:01:52.051062
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 01:27:19.917835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.54
 ---- batch: 020 ----
mean loss: 221.94
 ---- batch: 030 ----
mean loss: 218.92
 ---- batch: 040 ----
mean loss: 212.32
 ---- batch: 050 ----
mean loss: 231.66
 ---- batch: 060 ----
mean loss: 222.77
 ---- batch: 070 ----
mean loss: 222.81
 ---- batch: 080 ----
mean loss: 233.42
 ---- batch: 090 ----
mean loss: 216.10
 ---- batch: 100 ----
mean loss: 213.58
 ---- batch: 110 ----
mean loss: 226.31
train mean loss: 222.12
epoch train time: 0:00:00.712994
elapsed time: 0:01:52.764195
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 01:27:20.630970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.05
 ---- batch: 020 ----
mean loss: 218.04
 ---- batch: 030 ----
mean loss: 223.63
 ---- batch: 040 ----
mean loss: 218.82
 ---- batch: 050 ----
mean loss: 212.00
 ---- batch: 060 ----
mean loss: 227.74
 ---- batch: 070 ----
mean loss: 223.38
 ---- batch: 080 ----
mean loss: 226.61
 ---- batch: 090 ----
mean loss: 230.71
 ---- batch: 100 ----
mean loss: 213.46
 ---- batch: 110 ----
mean loss: 219.26
train mean loss: 221.63
epoch train time: 0:00:00.723766
elapsed time: 0:01:53.488101
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 01:27:21.354876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.74
 ---- batch: 020 ----
mean loss: 212.11
 ---- batch: 030 ----
mean loss: 225.72
 ---- batch: 040 ----
mean loss: 208.36
 ---- batch: 050 ----
mean loss: 221.35
 ---- batch: 060 ----
mean loss: 222.64
 ---- batch: 070 ----
mean loss: 223.81
 ---- batch: 080 ----
mean loss: 226.79
 ---- batch: 090 ----
mean loss: 220.82
 ---- batch: 100 ----
mean loss: 221.63
 ---- batch: 110 ----
mean loss: 230.19
train mean loss: 221.07
epoch train time: 0:00:00.718978
elapsed time: 0:01:54.207224
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 01:27:22.074017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.38
 ---- batch: 020 ----
mean loss: 232.46
 ---- batch: 030 ----
mean loss: 214.33
 ---- batch: 040 ----
mean loss: 220.01
 ---- batch: 050 ----
mean loss: 230.21
 ---- batch: 060 ----
mean loss: 236.09
 ---- batch: 070 ----
mean loss: 220.85
 ---- batch: 080 ----
mean loss: 209.90
 ---- batch: 090 ----
mean loss: 217.87
 ---- batch: 100 ----
mean loss: 218.71
 ---- batch: 110 ----
mean loss: 217.27
train mean loss: 220.95
epoch train time: 0:00:00.716704
elapsed time: 0:01:54.924086
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 01:27:22.790860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.73
 ---- batch: 020 ----
mean loss: 226.67
 ---- batch: 030 ----
mean loss: 222.11
 ---- batch: 040 ----
mean loss: 212.97
 ---- batch: 050 ----
mean loss: 221.79
 ---- batch: 060 ----
mean loss: 218.35
 ---- batch: 070 ----
mean loss: 216.59
 ---- batch: 080 ----
mean loss: 225.95
 ---- batch: 090 ----
mean loss: 225.70
 ---- batch: 100 ----
mean loss: 210.72
 ---- batch: 110 ----
mean loss: 212.40
train mean loss: 220.22
epoch train time: 0:00:00.720415
elapsed time: 0:01:55.644638
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 01:27:23.511410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.64
 ---- batch: 020 ----
mean loss: 219.88
 ---- batch: 030 ----
mean loss: 214.73
 ---- batch: 040 ----
mean loss: 218.76
 ---- batch: 050 ----
mean loss: 218.08
 ---- batch: 060 ----
mean loss: 220.55
 ---- batch: 070 ----
mean loss: 225.30
 ---- batch: 080 ----
mean loss: 223.69
 ---- batch: 090 ----
mean loss: 219.08
 ---- batch: 100 ----
mean loss: 226.42
 ---- batch: 110 ----
mean loss: 217.44
train mean loss: 219.86
epoch train time: 0:00:00.725238
elapsed time: 0:01:56.370013
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 01:27:24.236788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.69
 ---- batch: 020 ----
mean loss: 224.49
 ---- batch: 030 ----
mean loss: 220.20
 ---- batch: 040 ----
mean loss: 218.83
 ---- batch: 050 ----
mean loss: 214.28
 ---- batch: 060 ----
mean loss: 217.16
 ---- batch: 070 ----
mean loss: 216.92
 ---- batch: 080 ----
mean loss: 227.95
 ---- batch: 090 ----
mean loss: 216.69
 ---- batch: 100 ----
mean loss: 220.29
 ---- batch: 110 ----
mean loss: 211.61
train mean loss: 219.32
epoch train time: 0:00:00.715771
elapsed time: 0:01:57.085921
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 01:27:24.952692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.27
 ---- batch: 020 ----
mean loss: 218.46
 ---- batch: 030 ----
mean loss: 222.65
 ---- batch: 040 ----
mean loss: 216.19
 ---- batch: 050 ----
mean loss: 220.78
 ---- batch: 060 ----
mean loss: 214.67
 ---- batch: 070 ----
mean loss: 216.96
 ---- batch: 080 ----
mean loss: 222.36
 ---- batch: 090 ----
mean loss: 225.54
 ---- batch: 100 ----
mean loss: 221.10
 ---- batch: 110 ----
mean loss: 215.88
train mean loss: 218.80
epoch train time: 0:00:00.703631
elapsed time: 0:01:57.789685
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 01:27:25.656458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.80
 ---- batch: 020 ----
mean loss: 209.20
 ---- batch: 030 ----
mean loss: 217.78
 ---- batch: 040 ----
mean loss: 220.51
 ---- batch: 050 ----
mean loss: 222.48
 ---- batch: 060 ----
mean loss: 229.30
 ---- batch: 070 ----
mean loss: 216.37
 ---- batch: 080 ----
mean loss: 217.73
 ---- batch: 090 ----
mean loss: 226.87
 ---- batch: 100 ----
mean loss: 215.16
 ---- batch: 110 ----
mean loss: 220.03
train mean loss: 218.66
epoch train time: 0:00:00.715371
elapsed time: 0:01:58.505198
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 01:27:26.371977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.41
 ---- batch: 020 ----
mean loss: 205.99
 ---- batch: 030 ----
mean loss: 219.76
 ---- batch: 040 ----
mean loss: 217.87
 ---- batch: 050 ----
mean loss: 215.45
 ---- batch: 060 ----
mean loss: 218.21
 ---- batch: 070 ----
mean loss: 222.65
 ---- batch: 080 ----
mean loss: 206.53
 ---- batch: 090 ----
mean loss: 226.87
 ---- batch: 100 ----
mean loss: 220.58
 ---- batch: 110 ----
mean loss: 223.67
train mean loss: 218.19
epoch train time: 0:00:00.720398
elapsed time: 0:01:59.225741
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 01:27:27.092534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.38
 ---- batch: 020 ----
mean loss: 215.67
 ---- batch: 030 ----
mean loss: 218.18
 ---- batch: 040 ----
mean loss: 221.09
 ---- batch: 050 ----
mean loss: 210.03
 ---- batch: 060 ----
mean loss: 215.57
 ---- batch: 070 ----
mean loss: 217.85
 ---- batch: 080 ----
mean loss: 217.35
 ---- batch: 090 ----
mean loss: 220.68
 ---- batch: 100 ----
mean loss: 225.58
 ---- batch: 110 ----
mean loss: 215.34
train mean loss: 217.73
epoch train time: 0:00:00.724115
elapsed time: 0:01:59.950013
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 01:27:27.816792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.74
 ---- batch: 020 ----
mean loss: 223.58
 ---- batch: 030 ----
mean loss: 211.73
 ---- batch: 040 ----
mean loss: 220.48
 ---- batch: 050 ----
mean loss: 225.78
 ---- batch: 060 ----
mean loss: 214.94
 ---- batch: 070 ----
mean loss: 221.55
 ---- batch: 080 ----
mean loss: 218.96
 ---- batch: 090 ----
mean loss: 203.56
 ---- batch: 100 ----
mean loss: 222.75
 ---- batch: 110 ----
mean loss: 208.68
train mean loss: 217.41
epoch train time: 0:00:00.710596
elapsed time: 0:02:00.660749
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 01:27:28.527522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.19
 ---- batch: 020 ----
mean loss: 207.34
 ---- batch: 030 ----
mean loss: 213.28
 ---- batch: 040 ----
mean loss: 219.72
 ---- batch: 050 ----
mean loss: 219.09
 ---- batch: 060 ----
mean loss: 218.19
 ---- batch: 070 ----
mean loss: 216.28
 ---- batch: 080 ----
mean loss: 220.17
 ---- batch: 090 ----
mean loss: 225.60
 ---- batch: 100 ----
mean loss: 214.88
 ---- batch: 110 ----
mean loss: 210.98
train mean loss: 216.92
epoch train time: 0:00:00.708745
elapsed time: 0:02:01.369630
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 01:27:29.236403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.79
 ---- batch: 020 ----
mean loss: 213.65
 ---- batch: 030 ----
mean loss: 216.49
 ---- batch: 040 ----
mean loss: 223.03
 ---- batch: 050 ----
mean loss: 225.27
 ---- batch: 060 ----
mean loss: 220.38
 ---- batch: 070 ----
mean loss: 218.84
 ---- batch: 080 ----
mean loss: 212.37
 ---- batch: 090 ----
mean loss: 217.96
 ---- batch: 100 ----
mean loss: 207.84
 ---- batch: 110 ----
mean loss: 211.25
train mean loss: 216.72
epoch train time: 0:00:00.712572
elapsed time: 0:02:02.082340
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 01:27:29.949115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.91
 ---- batch: 020 ----
mean loss: 211.27
 ---- batch: 030 ----
mean loss: 219.07
 ---- batch: 040 ----
mean loss: 224.66
 ---- batch: 050 ----
mean loss: 210.28
 ---- batch: 060 ----
mean loss: 217.39
 ---- batch: 070 ----
mean loss: 223.21
 ---- batch: 080 ----
mean loss: 220.12
 ---- batch: 090 ----
mean loss: 210.06
 ---- batch: 100 ----
mean loss: 208.21
 ---- batch: 110 ----
mean loss: 218.99
train mean loss: 216.34
epoch train time: 0:00:00.713010
elapsed time: 0:02:02.795491
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 01:27:30.662267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.81
 ---- batch: 020 ----
mean loss: 210.77
 ---- batch: 030 ----
mean loss: 211.55
 ---- batch: 040 ----
mean loss: 219.52
 ---- batch: 050 ----
mean loss: 215.70
 ---- batch: 060 ----
mean loss: 212.51
 ---- batch: 070 ----
mean loss: 221.05
 ---- batch: 080 ----
mean loss: 218.20
 ---- batch: 090 ----
mean loss: 214.97
 ---- batch: 100 ----
mean loss: 216.84
 ---- batch: 110 ----
mean loss: 217.37
train mean loss: 216.13
epoch train time: 0:00:00.712536
elapsed time: 0:02:03.508188
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 01:27:31.374978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.43
 ---- batch: 020 ----
mean loss: 214.81
 ---- batch: 030 ----
mean loss: 215.93
 ---- batch: 040 ----
mean loss: 198.90
 ---- batch: 050 ----
mean loss: 228.98
 ---- batch: 060 ----
mean loss: 217.00
 ---- batch: 070 ----
mean loss: 213.95
 ---- batch: 080 ----
mean loss: 217.03
 ---- batch: 090 ----
mean loss: 213.89
 ---- batch: 100 ----
mean loss: 213.20
 ---- batch: 110 ----
mean loss: 217.54
train mean loss: 215.74
epoch train time: 0:00:00.721255
elapsed time: 0:02:04.229598
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 01:27:32.096372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.55
 ---- batch: 020 ----
mean loss: 211.04
 ---- batch: 030 ----
mean loss: 215.07
 ---- batch: 040 ----
mean loss: 216.33
 ---- batch: 050 ----
mean loss: 221.95
 ---- batch: 060 ----
mean loss: 214.02
 ---- batch: 070 ----
mean loss: 217.55
 ---- batch: 080 ----
mean loss: 217.94
 ---- batch: 090 ----
mean loss: 208.28
 ---- batch: 100 ----
mean loss: 222.65
 ---- batch: 110 ----
mean loss: 205.49
train mean loss: 215.43
epoch train time: 0:00:00.710700
elapsed time: 0:02:04.940436
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 01:27:32.807227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.59
 ---- batch: 020 ----
mean loss: 223.08
 ---- batch: 030 ----
mean loss: 214.84
 ---- batch: 040 ----
mean loss: 214.03
 ---- batch: 050 ----
mean loss: 220.40
 ---- batch: 060 ----
mean loss: 217.88
 ---- batch: 070 ----
mean loss: 206.16
 ---- batch: 080 ----
mean loss: 213.99
 ---- batch: 090 ----
mean loss: 213.92
 ---- batch: 100 ----
mean loss: 213.96
 ---- batch: 110 ----
mean loss: 216.06
train mean loss: 215.18
epoch train time: 0:00:00.713410
elapsed time: 0:02:05.654001
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 01:27:33.520774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.35
 ---- batch: 020 ----
mean loss: 213.29
 ---- batch: 030 ----
mean loss: 216.43
 ---- batch: 040 ----
mean loss: 225.90
 ---- batch: 050 ----
mean loss: 206.02
 ---- batch: 060 ----
mean loss: 211.87
 ---- batch: 070 ----
mean loss: 224.19
 ---- batch: 080 ----
mean loss: 217.53
 ---- batch: 090 ----
mean loss: 217.84
 ---- batch: 100 ----
mean loss: 210.62
 ---- batch: 110 ----
mean loss: 210.26
train mean loss: 214.85
epoch train time: 0:00:00.717440
elapsed time: 0:02:06.371635
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 01:27:34.238422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.05
 ---- batch: 020 ----
mean loss: 214.25
 ---- batch: 030 ----
mean loss: 220.75
 ---- batch: 040 ----
mean loss: 216.06
 ---- batch: 050 ----
mean loss: 216.28
 ---- batch: 060 ----
mean loss: 214.28
 ---- batch: 070 ----
mean loss: 217.43
 ---- batch: 080 ----
mean loss: 211.99
 ---- batch: 090 ----
mean loss: 206.71
 ---- batch: 100 ----
mean loss: 217.70
 ---- batch: 110 ----
mean loss: 211.41
train mean loss: 214.84
epoch train time: 0:00:00.711538
elapsed time: 0:02:07.083336
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 01:27:34.950108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.78
 ---- batch: 020 ----
mean loss: 219.24
 ---- batch: 030 ----
mean loss: 216.73
 ---- batch: 040 ----
mean loss: 217.27
 ---- batch: 050 ----
mean loss: 211.14
 ---- batch: 060 ----
mean loss: 215.41
 ---- batch: 070 ----
mean loss: 222.32
 ---- batch: 080 ----
mean loss: 211.52
 ---- batch: 090 ----
mean loss: 204.57
 ---- batch: 100 ----
mean loss: 213.12
 ---- batch: 110 ----
mean loss: 209.23
train mean loss: 214.21
epoch train time: 0:00:00.703660
elapsed time: 0:02:07.787130
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 01:27:35.653902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.24
 ---- batch: 020 ----
mean loss: 211.43
 ---- batch: 030 ----
mean loss: 215.19
 ---- batch: 040 ----
mean loss: 207.81
 ---- batch: 050 ----
mean loss: 213.08
 ---- batch: 060 ----
mean loss: 205.76
 ---- batch: 070 ----
mean loss: 215.13
 ---- batch: 080 ----
mean loss: 212.97
 ---- batch: 090 ----
mean loss: 223.37
 ---- batch: 100 ----
mean loss: 202.84
 ---- batch: 110 ----
mean loss: 219.73
train mean loss: 214.02
epoch train time: 0:00:00.713933
elapsed time: 0:02:08.501209
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 01:27:36.368004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.76
 ---- batch: 020 ----
mean loss: 216.12
 ---- batch: 030 ----
mean loss: 223.71
 ---- batch: 040 ----
mean loss: 215.05
 ---- batch: 050 ----
mean loss: 212.65
 ---- batch: 060 ----
mean loss: 218.24
 ---- batch: 070 ----
mean loss: 213.15
 ---- batch: 080 ----
mean loss: 209.50
 ---- batch: 090 ----
mean loss: 210.29
 ---- batch: 100 ----
mean loss: 212.89
 ---- batch: 110 ----
mean loss: 207.46
train mean loss: 213.87
epoch train time: 0:00:00.713802
elapsed time: 0:02:09.215171
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 01:27:37.081945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.40
 ---- batch: 020 ----
mean loss: 223.56
 ---- batch: 030 ----
mean loss: 220.18
 ---- batch: 040 ----
mean loss: 213.68
 ---- batch: 050 ----
mean loss: 210.31
 ---- batch: 060 ----
mean loss: 214.08
 ---- batch: 070 ----
mean loss: 208.52
 ---- batch: 080 ----
mean loss: 210.28
 ---- batch: 090 ----
mean loss: 219.18
 ---- batch: 100 ----
mean loss: 207.74
 ---- batch: 110 ----
mean loss: 213.14
train mean loss: 213.62
epoch train time: 0:00:00.713194
elapsed time: 0:02:09.928516
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 01:27:37.795299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.16
 ---- batch: 020 ----
mean loss: 219.16
 ---- batch: 030 ----
mean loss: 212.20
 ---- batch: 040 ----
mean loss: 216.52
 ---- batch: 050 ----
mean loss: 207.08
 ---- batch: 060 ----
mean loss: 205.34
 ---- batch: 070 ----
mean loss: 215.48
 ---- batch: 080 ----
mean loss: 210.28
 ---- batch: 090 ----
mean loss: 217.43
 ---- batch: 100 ----
mean loss: 204.57
 ---- batch: 110 ----
mean loss: 209.93
train mean loss: 213.34
epoch train time: 0:00:00.716652
elapsed time: 0:02:10.645312
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 01:27:38.512084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.50
 ---- batch: 020 ----
mean loss: 217.93
 ---- batch: 030 ----
mean loss: 214.88
 ---- batch: 040 ----
mean loss: 214.20
 ---- batch: 050 ----
mean loss: 215.66
 ---- batch: 060 ----
mean loss: 218.08
 ---- batch: 070 ----
mean loss: 207.39
 ---- batch: 080 ----
mean loss: 214.80
 ---- batch: 090 ----
mean loss: 203.48
 ---- batch: 100 ----
mean loss: 219.42
 ---- batch: 110 ----
mean loss: 216.77
train mean loss: 212.97
epoch train time: 0:00:00.708951
elapsed time: 0:02:11.354395
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 01:27:39.221167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.18
 ---- batch: 020 ----
mean loss: 216.72
 ---- batch: 030 ----
mean loss: 207.88
 ---- batch: 040 ----
mean loss: 219.16
 ---- batch: 050 ----
mean loss: 204.21
 ---- batch: 060 ----
mean loss: 216.97
 ---- batch: 070 ----
mean loss: 203.89
 ---- batch: 080 ----
mean loss: 214.14
 ---- batch: 090 ----
mean loss: 208.81
 ---- batch: 100 ----
mean loss: 219.43
 ---- batch: 110 ----
mean loss: 207.24
train mean loss: 212.70
epoch train time: 0:00:00.709876
elapsed time: 0:02:12.064420
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 01:27:39.931192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.48
 ---- batch: 020 ----
mean loss: 222.19
 ---- batch: 030 ----
mean loss: 212.66
 ---- batch: 040 ----
mean loss: 217.21
 ---- batch: 050 ----
mean loss: 216.18
 ---- batch: 060 ----
mean loss: 216.53
 ---- batch: 070 ----
mean loss: 207.36
 ---- batch: 080 ----
mean loss: 207.46
 ---- batch: 090 ----
mean loss: 216.18
 ---- batch: 100 ----
mean loss: 200.86
 ---- batch: 110 ----
mean loss: 212.93
train mean loss: 212.56
epoch train time: 0:00:00.704118
elapsed time: 0:02:12.768674
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 01:27:40.635466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.69
 ---- batch: 020 ----
mean loss: 212.19
 ---- batch: 030 ----
mean loss: 221.53
 ---- batch: 040 ----
mean loss: 219.14
 ---- batch: 050 ----
mean loss: 216.61
 ---- batch: 060 ----
mean loss: 205.80
 ---- batch: 070 ----
mean loss: 215.73
 ---- batch: 080 ----
mean loss: 211.80
 ---- batch: 090 ----
mean loss: 219.77
 ---- batch: 100 ----
mean loss: 208.25
 ---- batch: 110 ----
mean loss: 204.89
train mean loss: 212.46
epoch train time: 0:00:00.715201
elapsed time: 0:02:13.484029
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 01:27:41.350802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.44
 ---- batch: 020 ----
mean loss: 222.26
 ---- batch: 030 ----
mean loss: 210.11
 ---- batch: 040 ----
mean loss: 208.96
 ---- batch: 050 ----
mean loss: 204.60
 ---- batch: 060 ----
mean loss: 217.06
 ---- batch: 070 ----
mean loss: 211.93
 ---- batch: 080 ----
mean loss: 214.87
 ---- batch: 090 ----
mean loss: 212.62
 ---- batch: 100 ----
mean loss: 207.82
 ---- batch: 110 ----
mean loss: 204.61
train mean loss: 212.11
epoch train time: 0:00:00.711310
elapsed time: 0:02:14.195476
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 01:27:42.062248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.45
 ---- batch: 020 ----
mean loss: 223.57
 ---- batch: 030 ----
mean loss: 208.66
 ---- batch: 040 ----
mean loss: 211.52
 ---- batch: 050 ----
mean loss: 209.75
 ---- batch: 060 ----
mean loss: 208.94
 ---- batch: 070 ----
mean loss: 207.94
 ---- batch: 080 ----
mean loss: 210.32
 ---- batch: 090 ----
mean loss: 202.31
 ---- batch: 100 ----
mean loss: 218.17
 ---- batch: 110 ----
mean loss: 221.10
train mean loss: 211.99
epoch train time: 0:00:00.709523
elapsed time: 0:02:14.905149
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 01:27:42.771928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.78
 ---- batch: 020 ----
mean loss: 206.01
 ---- batch: 030 ----
mean loss: 204.62
 ---- batch: 040 ----
mean loss: 208.31
 ---- batch: 050 ----
mean loss: 219.02
 ---- batch: 060 ----
mean loss: 204.73
 ---- batch: 070 ----
mean loss: 214.42
 ---- batch: 080 ----
mean loss: 223.18
 ---- batch: 090 ----
mean loss: 215.06
 ---- batch: 100 ----
mean loss: 215.64
 ---- batch: 110 ----
mean loss: 202.02
train mean loss: 211.70
epoch train time: 0:00:00.711949
elapsed time: 0:02:15.617247
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 01:27:43.484021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.05
 ---- batch: 020 ----
mean loss: 208.76
 ---- batch: 030 ----
mean loss: 213.22
 ---- batch: 040 ----
mean loss: 216.56
 ---- batch: 050 ----
mean loss: 209.25
 ---- batch: 060 ----
mean loss: 214.00
 ---- batch: 070 ----
mean loss: 207.93
 ---- batch: 080 ----
mean loss: 219.89
 ---- batch: 090 ----
mean loss: 208.50
 ---- batch: 100 ----
mean loss: 212.58
 ---- batch: 110 ----
mean loss: 208.20
train mean loss: 211.51
epoch train time: 0:00:00.715442
elapsed time: 0:02:16.332827
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 01:27:44.199600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.17
 ---- batch: 020 ----
mean loss: 212.74
 ---- batch: 030 ----
mean loss: 219.43
 ---- batch: 040 ----
mean loss: 208.36
 ---- batch: 050 ----
mean loss: 216.98
 ---- batch: 060 ----
mean loss: 215.43
 ---- batch: 070 ----
mean loss: 211.33
 ---- batch: 080 ----
mean loss: 210.16
 ---- batch: 090 ----
mean loss: 208.70
 ---- batch: 100 ----
mean loss: 210.82
 ---- batch: 110 ----
mean loss: 210.04
train mean loss: 211.26
epoch train time: 0:00:00.715664
elapsed time: 0:02:17.048628
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 01:27:44.915409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.99
 ---- batch: 020 ----
mean loss: 215.41
 ---- batch: 030 ----
mean loss: 206.32
 ---- batch: 040 ----
mean loss: 215.01
 ---- batch: 050 ----
mean loss: 207.94
 ---- batch: 060 ----
mean loss: 208.68
 ---- batch: 070 ----
mean loss: 215.75
 ---- batch: 080 ----
mean loss: 207.35
 ---- batch: 090 ----
mean loss: 213.45
 ---- batch: 100 ----
mean loss: 204.88
 ---- batch: 110 ----
mean loss: 210.01
train mean loss: 211.14
epoch train time: 0:00:00.708776
elapsed time: 0:02:17.757548
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 01:27:45.624345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.85
 ---- batch: 020 ----
mean loss: 215.82
 ---- batch: 030 ----
mean loss: 209.79
 ---- batch: 040 ----
mean loss: 209.35
 ---- batch: 050 ----
mean loss: 220.79
 ---- batch: 060 ----
mean loss: 205.87
 ---- batch: 070 ----
mean loss: 212.78
 ---- batch: 080 ----
mean loss: 208.39
 ---- batch: 090 ----
mean loss: 209.26
 ---- batch: 100 ----
mean loss: 217.21
 ---- batch: 110 ----
mean loss: 202.30
train mean loss: 210.78
epoch train time: 0:00:00.715384
elapsed time: 0:02:18.473091
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 01:27:46.339866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.09
 ---- batch: 020 ----
mean loss: 215.29
 ---- batch: 030 ----
mean loss: 216.50
 ---- batch: 040 ----
mean loss: 205.51
 ---- batch: 050 ----
mean loss: 212.14
 ---- batch: 060 ----
mean loss: 200.06
 ---- batch: 070 ----
mean loss: 207.57
 ---- batch: 080 ----
mean loss: 220.41
 ---- batch: 090 ----
mean loss: 215.96
 ---- batch: 100 ----
mean loss: 214.27
 ---- batch: 110 ----
mean loss: 204.83
train mean loss: 210.72
epoch train time: 0:00:00.713786
elapsed time: 0:02:19.187031
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 01:27:47.053794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.10
 ---- batch: 020 ----
mean loss: 207.25
 ---- batch: 030 ----
mean loss: 202.80
 ---- batch: 040 ----
mean loss: 196.77
 ---- batch: 050 ----
mean loss: 211.34
 ---- batch: 060 ----
mean loss: 213.66
 ---- batch: 070 ----
mean loss: 216.23
 ---- batch: 080 ----
mean loss: 213.83
 ---- batch: 090 ----
mean loss: 210.47
 ---- batch: 100 ----
mean loss: 214.77
 ---- batch: 110 ----
mean loss: 207.84
train mean loss: 210.44
epoch train time: 0:00:00.704927
elapsed time: 0:02:19.892084
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 01:27:47.758855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.22
 ---- batch: 020 ----
mean loss: 209.10
 ---- batch: 030 ----
mean loss: 215.28
 ---- batch: 040 ----
mean loss: 204.10
 ---- batch: 050 ----
mean loss: 211.24
 ---- batch: 060 ----
mean loss: 216.06
 ---- batch: 070 ----
mean loss: 204.26
 ---- batch: 080 ----
mean loss: 215.27
 ---- batch: 090 ----
mean loss: 212.70
 ---- batch: 100 ----
mean loss: 204.66
 ---- batch: 110 ----
mean loss: 209.21
train mean loss: 210.42
epoch train time: 0:00:00.711346
elapsed time: 0:02:20.603563
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 01:27:48.470334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.15
 ---- batch: 020 ----
mean loss: 206.59
 ---- batch: 030 ----
mean loss: 213.94
 ---- batch: 040 ----
mean loss: 211.29
 ---- batch: 050 ----
mean loss: 209.59
 ---- batch: 060 ----
mean loss: 212.07
 ---- batch: 070 ----
mean loss: 212.64
 ---- batch: 080 ----
mean loss: 207.86
 ---- batch: 090 ----
mean loss: 210.21
 ---- batch: 100 ----
mean loss: 214.30
 ---- batch: 110 ----
mean loss: 203.05
train mean loss: 210.25
epoch train time: 0:00:00.712233
elapsed time: 0:02:21.315957
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 01:27:49.182732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.31
 ---- batch: 020 ----
mean loss: 207.38
 ---- batch: 030 ----
mean loss: 205.47
 ---- batch: 040 ----
mean loss: 202.76
 ---- batch: 050 ----
mean loss: 212.20
 ---- batch: 060 ----
mean loss: 208.24
 ---- batch: 070 ----
mean loss: 204.73
 ---- batch: 080 ----
mean loss: 219.49
 ---- batch: 090 ----
mean loss: 215.39
 ---- batch: 100 ----
mean loss: 208.43
 ---- batch: 110 ----
mean loss: 204.15
train mean loss: 209.86
epoch train time: 0:00:00.718159
elapsed time: 0:02:22.034256
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 01:27:49.901058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.07
 ---- batch: 020 ----
mean loss: 199.39
 ---- batch: 030 ----
mean loss: 207.97
 ---- batch: 040 ----
mean loss: 208.85
 ---- batch: 050 ----
mean loss: 216.29
 ---- batch: 060 ----
mean loss: 212.12
 ---- batch: 070 ----
mean loss: 213.72
 ---- batch: 080 ----
mean loss: 207.45
 ---- batch: 090 ----
mean loss: 204.74
 ---- batch: 100 ----
mean loss: 209.07
 ---- batch: 110 ----
mean loss: 213.83
train mean loss: 209.68
epoch train time: 0:00:00.706178
elapsed time: 0:02:22.740600
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 01:27:50.607372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.36
 ---- batch: 020 ----
mean loss: 207.31
 ---- batch: 030 ----
mean loss: 211.46
 ---- batch: 040 ----
mean loss: 206.97
 ---- batch: 050 ----
mean loss: 209.66
 ---- batch: 060 ----
mean loss: 206.19
 ---- batch: 070 ----
mean loss: 211.25
 ---- batch: 080 ----
mean loss: 209.16
 ---- batch: 090 ----
mean loss: 212.32
 ---- batch: 100 ----
mean loss: 205.74
 ---- batch: 110 ----
mean loss: 212.79
train mean loss: 209.41
epoch train time: 0:00:00.720896
elapsed time: 0:02:23.461632
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 01:27:51.328407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.01
 ---- batch: 020 ----
mean loss: 206.03
 ---- batch: 030 ----
mean loss: 208.05
 ---- batch: 040 ----
mean loss: 210.87
 ---- batch: 050 ----
mean loss: 204.33
 ---- batch: 060 ----
mean loss: 206.74
 ---- batch: 070 ----
mean loss: 213.92
 ---- batch: 080 ----
mean loss: 205.39
 ---- batch: 090 ----
mean loss: 212.63
 ---- batch: 100 ----
mean loss: 210.68
 ---- batch: 110 ----
mean loss: 213.11
train mean loss: 209.42
epoch train time: 0:00:00.714465
elapsed time: 0:02:24.176236
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 01:27:52.043008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.93
 ---- batch: 020 ----
mean loss: 204.08
 ---- batch: 030 ----
mean loss: 218.12
 ---- batch: 040 ----
mean loss: 223.75
 ---- batch: 050 ----
mean loss: 205.25
 ---- batch: 060 ----
mean loss: 208.87
 ---- batch: 070 ----
mean loss: 208.53
 ---- batch: 080 ----
mean loss: 204.73
 ---- batch: 090 ----
mean loss: 201.95
 ---- batch: 100 ----
mean loss: 212.48
 ---- batch: 110 ----
mean loss: 205.34
train mean loss: 209.41
epoch train time: 0:00:00.713236
elapsed time: 0:02:24.889608
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 01:27:52.756382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.75
 ---- batch: 020 ----
mean loss: 207.88
 ---- batch: 030 ----
mean loss: 213.85
 ---- batch: 040 ----
mean loss: 204.50
 ---- batch: 050 ----
mean loss: 200.95
 ---- batch: 060 ----
mean loss: 214.05
 ---- batch: 070 ----
mean loss: 202.09
 ---- batch: 080 ----
mean loss: 210.65
 ---- batch: 090 ----
mean loss: 212.56
 ---- batch: 100 ----
mean loss: 211.34
 ---- batch: 110 ----
mean loss: 214.30
train mean loss: 209.04
epoch train time: 0:00:00.712714
elapsed time: 0:02:25.602458
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 01:27:53.469230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.38
 ---- batch: 020 ----
mean loss: 206.85
 ---- batch: 030 ----
mean loss: 212.80
 ---- batch: 040 ----
mean loss: 199.81
 ---- batch: 050 ----
mean loss: 206.38
 ---- batch: 060 ----
mean loss: 207.98
 ---- batch: 070 ----
mean loss: 209.14
 ---- batch: 080 ----
mean loss: 205.88
 ---- batch: 090 ----
mean loss: 200.78
 ---- batch: 100 ----
mean loss: 213.52
 ---- batch: 110 ----
mean loss: 218.69
train mean loss: 208.77
epoch train time: 0:00:00.718536
elapsed time: 0:02:26.321130
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 01:27:54.187911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.70
 ---- batch: 020 ----
mean loss: 205.32
 ---- batch: 030 ----
mean loss: 205.19
 ---- batch: 040 ----
mean loss: 210.93
 ---- batch: 050 ----
mean loss: 212.07
 ---- batch: 060 ----
mean loss: 203.11
 ---- batch: 070 ----
mean loss: 213.17
 ---- batch: 080 ----
mean loss: 209.07
 ---- batch: 090 ----
mean loss: 202.68
 ---- batch: 100 ----
mean loss: 220.85
 ---- batch: 110 ----
mean loss: 205.37
train mean loss: 208.84
epoch train time: 0:00:00.710428
elapsed time: 0:02:27.031708
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 01:27:54.898500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.51
 ---- batch: 020 ----
mean loss: 207.42
 ---- batch: 030 ----
mean loss: 204.97
 ---- batch: 040 ----
mean loss: 210.90
 ---- batch: 050 ----
mean loss: 225.93
 ---- batch: 060 ----
mean loss: 200.82
 ---- batch: 070 ----
mean loss: 204.08
 ---- batch: 080 ----
mean loss: 217.77
 ---- batch: 090 ----
mean loss: 212.43
 ---- batch: 100 ----
mean loss: 196.24
 ---- batch: 110 ----
mean loss: 202.73
train mean loss: 208.42
epoch train time: 0:00:00.703265
elapsed time: 0:02:27.735127
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 01:27:55.601898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.67
 ---- batch: 020 ----
mean loss: 212.03
 ---- batch: 030 ----
mean loss: 211.30
 ---- batch: 040 ----
mean loss: 209.38
 ---- batch: 050 ----
mean loss: 217.83
 ---- batch: 060 ----
mean loss: 208.18
 ---- batch: 070 ----
mean loss: 206.77
 ---- batch: 080 ----
mean loss: 207.64
 ---- batch: 090 ----
mean loss: 206.97
 ---- batch: 100 ----
mean loss: 214.54
 ---- batch: 110 ----
mean loss: 205.05
train mean loss: 208.36
epoch train time: 0:00:00.708043
elapsed time: 0:02:28.443305
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 01:27:56.310094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.28
 ---- batch: 020 ----
mean loss: 210.32
 ---- batch: 030 ----
mean loss: 210.84
 ---- batch: 040 ----
mean loss: 201.04
 ---- batch: 050 ----
mean loss: 221.16
 ---- batch: 060 ----
mean loss: 213.28
 ---- batch: 070 ----
mean loss: 202.50
 ---- batch: 080 ----
mean loss: 192.30
 ---- batch: 090 ----
mean loss: 204.05
 ---- batch: 100 ----
mean loss: 210.09
 ---- batch: 110 ----
mean loss: 213.66
train mean loss: 208.34
epoch train time: 0:00:00.710565
elapsed time: 0:02:29.154020
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 01:27:57.020803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.36
 ---- batch: 020 ----
mean loss: 209.23
 ---- batch: 030 ----
mean loss: 213.77
 ---- batch: 040 ----
mean loss: 217.58
 ---- batch: 050 ----
mean loss: 209.04
 ---- batch: 060 ----
mean loss: 205.75
 ---- batch: 070 ----
mean loss: 209.09
 ---- batch: 080 ----
mean loss: 206.40
 ---- batch: 090 ----
mean loss: 201.00
 ---- batch: 100 ----
mean loss: 195.14
 ---- batch: 110 ----
mean loss: 208.02
train mean loss: 207.99
epoch train time: 0:00:00.712403
elapsed time: 0:02:29.866567
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 01:27:57.733340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.33
 ---- batch: 020 ----
mean loss: 211.29
 ---- batch: 030 ----
mean loss: 205.64
 ---- batch: 040 ----
mean loss: 209.83
 ---- batch: 050 ----
mean loss: 214.88
 ---- batch: 060 ----
mean loss: 202.03
 ---- batch: 070 ----
mean loss: 223.68
 ---- batch: 080 ----
mean loss: 202.67
 ---- batch: 090 ----
mean loss: 198.50
 ---- batch: 100 ----
mean loss: 209.11
 ---- batch: 110 ----
mean loss: 204.72
train mean loss: 207.90
epoch train time: 0:00:00.718187
elapsed time: 0:02:30.584890
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 01:27:58.451663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.46
 ---- batch: 020 ----
mean loss: 202.02
 ---- batch: 030 ----
mean loss: 211.83
 ---- batch: 040 ----
mean loss: 208.07
 ---- batch: 050 ----
mean loss: 219.01
 ---- batch: 060 ----
mean loss: 205.10
 ---- batch: 070 ----
mean loss: 203.84
 ---- batch: 080 ----
mean loss: 210.54
 ---- batch: 090 ----
mean loss: 208.77
 ---- batch: 100 ----
mean loss: 204.40
 ---- batch: 110 ----
mean loss: 197.62
train mean loss: 207.85
epoch train time: 0:00:00.720549
elapsed time: 0:02:31.305575
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 01:27:59.172348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.97
 ---- batch: 020 ----
mean loss: 207.09
 ---- batch: 030 ----
mean loss: 206.25
 ---- batch: 040 ----
mean loss: 202.63
 ---- batch: 050 ----
mean loss: 199.32
 ---- batch: 060 ----
mean loss: 212.56
 ---- batch: 070 ----
mean loss: 191.72
 ---- batch: 080 ----
mean loss: 220.70
 ---- batch: 090 ----
mean loss: 212.88
 ---- batch: 100 ----
mean loss: 220.73
 ---- batch: 110 ----
mean loss: 202.15
train mean loss: 207.71
epoch train time: 0:00:00.721306
elapsed time: 0:02:32.027038
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 01:27:59.893830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.93
 ---- batch: 020 ----
mean loss: 207.32
 ---- batch: 030 ----
mean loss: 203.80
 ---- batch: 040 ----
mean loss: 200.34
 ---- batch: 050 ----
mean loss: 208.23
 ---- batch: 060 ----
mean loss: 205.00
 ---- batch: 070 ----
mean loss: 205.70
 ---- batch: 080 ----
mean loss: 202.30
 ---- batch: 090 ----
mean loss: 215.97
 ---- batch: 100 ----
mean loss: 212.14
 ---- batch: 110 ----
mean loss: 212.14
train mean loss: 207.38
epoch train time: 0:00:00.722942
elapsed time: 0:02:32.750134
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 01:28:00.616914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.38
 ---- batch: 020 ----
mean loss: 212.87
 ---- batch: 030 ----
mean loss: 207.77
 ---- batch: 040 ----
mean loss: 219.66
 ---- batch: 050 ----
mean loss: 217.71
 ---- batch: 060 ----
mean loss: 203.63
 ---- batch: 070 ----
mean loss: 194.13
 ---- batch: 080 ----
mean loss: 207.85
 ---- batch: 090 ----
mean loss: 201.18
 ---- batch: 100 ----
mean loss: 202.22
 ---- batch: 110 ----
mean loss: 200.02
train mean loss: 207.32
epoch train time: 0:00:00.728466
elapsed time: 0:02:33.478745
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 01:28:01.345521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.87
 ---- batch: 020 ----
mean loss: 207.94
 ---- batch: 030 ----
mean loss: 196.42
 ---- batch: 040 ----
mean loss: 218.10
 ---- batch: 050 ----
mean loss: 210.90
 ---- batch: 060 ----
mean loss: 211.91
 ---- batch: 070 ----
mean loss: 206.53
 ---- batch: 080 ----
mean loss: 212.86
 ---- batch: 090 ----
mean loss: 203.96
 ---- batch: 100 ----
mean loss: 208.09
 ---- batch: 110 ----
mean loss: 200.29
train mean loss: 207.28
epoch train time: 0:00:00.721110
elapsed time: 0:02:34.199997
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 01:28:02.066772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.92
 ---- batch: 020 ----
mean loss: 212.16
 ---- batch: 030 ----
mean loss: 213.49
 ---- batch: 040 ----
mean loss: 204.16
 ---- batch: 050 ----
mean loss: 201.60
 ---- batch: 060 ----
mean loss: 210.94
 ---- batch: 070 ----
mean loss: 203.32
 ---- batch: 080 ----
mean loss: 208.97
 ---- batch: 090 ----
mean loss: 212.85
 ---- batch: 100 ----
mean loss: 200.16
 ---- batch: 110 ----
mean loss: 199.54
train mean loss: 207.05
epoch train time: 0:00:00.723625
elapsed time: 0:02:34.923765
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 01:28:02.790538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.11
 ---- batch: 020 ----
mean loss: 217.26
 ---- batch: 030 ----
mean loss: 213.41
 ---- batch: 040 ----
mean loss: 203.15
 ---- batch: 050 ----
mean loss: 207.76
 ---- batch: 060 ----
mean loss: 199.13
 ---- batch: 070 ----
mean loss: 213.28
 ---- batch: 080 ----
mean loss: 206.23
 ---- batch: 090 ----
mean loss: 208.99
 ---- batch: 100 ----
mean loss: 204.10
 ---- batch: 110 ----
mean loss: 205.50
train mean loss: 206.83
epoch train time: 0:00:00.727058
elapsed time: 0:02:35.650984
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 01:28:03.517758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.10
 ---- batch: 020 ----
mean loss: 213.36
 ---- batch: 030 ----
mean loss: 208.24
 ---- batch: 040 ----
mean loss: 201.27
 ---- batch: 050 ----
mean loss: 204.40
 ---- batch: 060 ----
mean loss: 205.67
 ---- batch: 070 ----
mean loss: 196.65
 ---- batch: 080 ----
mean loss: 211.03
 ---- batch: 090 ----
mean loss: 211.27
 ---- batch: 100 ----
mean loss: 203.16
 ---- batch: 110 ----
mean loss: 211.08
train mean loss: 206.64
epoch train time: 0:00:00.723130
elapsed time: 0:02:36.374267
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 01:28:04.241060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.82
 ---- batch: 020 ----
mean loss: 205.78
 ---- batch: 030 ----
mean loss: 213.95
 ---- batch: 040 ----
mean loss: 207.15
 ---- batch: 050 ----
mean loss: 199.20
 ---- batch: 060 ----
mean loss: 204.32
 ---- batch: 070 ----
mean loss: 208.34
 ---- batch: 080 ----
mean loss: 202.94
 ---- batch: 090 ----
mean loss: 216.57
 ---- batch: 100 ----
mean loss: 195.81
 ---- batch: 110 ----
mean loss: 208.39
train mean loss: 206.56
epoch train time: 0:00:00.714099
elapsed time: 0:02:37.088539
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 01:28:04.955304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.50
 ---- batch: 020 ----
mean loss: 211.46
 ---- batch: 030 ----
mean loss: 208.98
 ---- batch: 040 ----
mean loss: 206.68
 ---- batch: 050 ----
mean loss: 208.94
 ---- batch: 060 ----
mean loss: 206.48
 ---- batch: 070 ----
mean loss: 202.98
 ---- batch: 080 ----
mean loss: 199.33
 ---- batch: 090 ----
mean loss: 204.19
 ---- batch: 100 ----
mean loss: 208.41
 ---- batch: 110 ----
mean loss: 206.22
train mean loss: 206.31
epoch train time: 0:00:00.724708
elapsed time: 0:02:37.813396
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 01:28:05.680187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.31
 ---- batch: 020 ----
mean loss: 205.16
 ---- batch: 030 ----
mean loss: 203.87
 ---- batch: 040 ----
mean loss: 205.80
 ---- batch: 050 ----
mean loss: 201.17
 ---- batch: 060 ----
mean loss: 207.51
 ---- batch: 070 ----
mean loss: 210.40
 ---- batch: 080 ----
mean loss: 202.58
 ---- batch: 090 ----
mean loss: 212.63
 ---- batch: 100 ----
mean loss: 205.86
 ---- batch: 110 ----
mean loss: 198.65
train mean loss: 206.30
epoch train time: 0:00:00.723643
elapsed time: 0:02:38.537193
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 01:28:06.403979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.61
 ---- batch: 020 ----
mean loss: 207.53
 ---- batch: 030 ----
mean loss: 194.14
 ---- batch: 040 ----
mean loss: 218.06
 ---- batch: 050 ----
mean loss: 210.45
 ---- batch: 060 ----
mean loss: 206.15
 ---- batch: 070 ----
mean loss: 209.88
 ---- batch: 080 ----
mean loss: 206.56
 ---- batch: 090 ----
mean loss: 200.75
 ---- batch: 100 ----
mean loss: 203.45
 ---- batch: 110 ----
mean loss: 202.20
train mean loss: 206.09
epoch train time: 0:00:00.735431
elapsed time: 0:02:39.272774
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 01:28:07.139547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.27
 ---- batch: 020 ----
mean loss: 216.86
 ---- batch: 030 ----
mean loss: 208.21
 ---- batch: 040 ----
mean loss: 199.34
 ---- batch: 050 ----
mean loss: 205.85
 ---- batch: 060 ----
mean loss: 206.42
 ---- batch: 070 ----
mean loss: 197.15
 ---- batch: 080 ----
mean loss: 208.24
 ---- batch: 090 ----
mean loss: 207.97
 ---- batch: 100 ----
mean loss: 204.68
 ---- batch: 110 ----
mean loss: 196.75
train mean loss: 205.98
epoch train time: 0:00:00.725008
elapsed time: 0:02:39.997934
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 01:28:07.864708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.70
 ---- batch: 020 ----
mean loss: 213.34
 ---- batch: 030 ----
mean loss: 207.23
 ---- batch: 040 ----
mean loss: 202.91
 ---- batch: 050 ----
mean loss: 203.23
 ---- batch: 060 ----
mean loss: 208.07
 ---- batch: 070 ----
mean loss: 205.23
 ---- batch: 080 ----
mean loss: 201.44
 ---- batch: 090 ----
mean loss: 208.33
 ---- batch: 100 ----
mean loss: 200.30
 ---- batch: 110 ----
mean loss: 210.34
train mean loss: 205.91
epoch train time: 0:00:00.725859
elapsed time: 0:02:40.723947
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 01:28:08.590720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.97
 ---- batch: 020 ----
mean loss: 198.28
 ---- batch: 030 ----
mean loss: 209.57
 ---- batch: 040 ----
mean loss: 211.22
 ---- batch: 050 ----
mean loss: 213.03
 ---- batch: 060 ----
mean loss: 198.54
 ---- batch: 070 ----
mean loss: 211.54
 ---- batch: 080 ----
mean loss: 203.61
 ---- batch: 090 ----
mean loss: 202.65
 ---- batch: 100 ----
mean loss: 200.33
 ---- batch: 110 ----
mean loss: 213.41
train mean loss: 205.92
epoch train time: 0:00:00.717784
elapsed time: 0:02:41.441869
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 01:28:09.308664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.64
 ---- batch: 020 ----
mean loss: 213.50
 ---- batch: 030 ----
mean loss: 205.00
 ---- batch: 040 ----
mean loss: 197.09
 ---- batch: 050 ----
mean loss: 212.98
 ---- batch: 060 ----
mean loss: 210.06
 ---- batch: 070 ----
mean loss: 205.51
 ---- batch: 080 ----
mean loss: 207.28
 ---- batch: 090 ----
mean loss: 198.58
 ---- batch: 100 ----
mean loss: 206.95
 ---- batch: 110 ----
mean loss: 205.75
train mean loss: 205.54
epoch train time: 0:00:00.722016
elapsed time: 0:02:42.164045
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 01:28:10.030818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.34
 ---- batch: 020 ----
mean loss: 203.63
 ---- batch: 030 ----
mean loss: 219.44
 ---- batch: 040 ----
mean loss: 194.68
 ---- batch: 050 ----
mean loss: 202.44
 ---- batch: 060 ----
mean loss: 196.83
 ---- batch: 070 ----
mean loss: 201.08
 ---- batch: 080 ----
mean loss: 207.93
 ---- batch: 090 ----
mean loss: 208.75
 ---- batch: 100 ----
mean loss: 210.09
 ---- batch: 110 ----
mean loss: 206.20
train mean loss: 205.51
epoch train time: 0:00:00.721694
elapsed time: 0:02:42.885876
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 01:28:10.752650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.77
 ---- batch: 020 ----
mean loss: 211.22
 ---- batch: 030 ----
mean loss: 205.72
 ---- batch: 040 ----
mean loss: 212.35
 ---- batch: 050 ----
mean loss: 202.06
 ---- batch: 060 ----
mean loss: 210.36
 ---- batch: 070 ----
mean loss: 202.70
 ---- batch: 080 ----
mean loss: 201.17
 ---- batch: 090 ----
mean loss: 205.32
 ---- batch: 100 ----
mean loss: 200.81
 ---- batch: 110 ----
mean loss: 197.36
train mean loss: 205.36
epoch train time: 0:00:00.719611
elapsed time: 0:02:43.605656
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 01:28:11.472445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.02
 ---- batch: 020 ----
mean loss: 207.70
 ---- batch: 030 ----
mean loss: 197.35
 ---- batch: 040 ----
mean loss: 210.46
 ---- batch: 050 ----
mean loss: 194.40
 ---- batch: 060 ----
mean loss: 203.60
 ---- batch: 070 ----
mean loss: 205.22
 ---- batch: 080 ----
mean loss: 206.31
 ---- batch: 090 ----
mean loss: 216.53
 ---- batch: 100 ----
mean loss: 207.50
 ---- batch: 110 ----
mean loss: 202.51
train mean loss: 205.09
epoch train time: 0:00:00.723889
elapsed time: 0:02:44.329698
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 01:28:12.196482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.24
 ---- batch: 020 ----
mean loss: 207.53
 ---- batch: 030 ----
mean loss: 209.83
 ---- batch: 040 ----
mean loss: 202.07
 ---- batch: 050 ----
mean loss: 208.37
 ---- batch: 060 ----
mean loss: 209.83
 ---- batch: 070 ----
mean loss: 202.89
 ---- batch: 080 ----
mean loss: 202.70
 ---- batch: 090 ----
mean loss: 197.39
 ---- batch: 100 ----
mean loss: 202.73
 ---- batch: 110 ----
mean loss: 212.77
train mean loss: 204.87
epoch train time: 0:00:00.726481
elapsed time: 0:02:45.056325
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 01:28:12.923097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.52
 ---- batch: 020 ----
mean loss: 194.95
 ---- batch: 030 ----
mean loss: 211.10
 ---- batch: 040 ----
mean loss: 204.35
 ---- batch: 050 ----
mean loss: 205.95
 ---- batch: 060 ----
mean loss: 204.48
 ---- batch: 070 ----
mean loss: 206.67
 ---- batch: 080 ----
mean loss: 200.44
 ---- batch: 090 ----
mean loss: 202.47
 ---- batch: 100 ----
mean loss: 212.09
 ---- batch: 110 ----
mean loss: 209.84
train mean loss: 205.00
epoch train time: 0:00:00.725676
elapsed time: 0:02:45.782164
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 01:28:13.648964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.66
 ---- batch: 020 ----
mean loss: 211.31
 ---- batch: 030 ----
mean loss: 210.07
 ---- batch: 040 ----
mean loss: 201.93
 ---- batch: 050 ----
mean loss: 205.85
 ---- batch: 060 ----
mean loss: 203.31
 ---- batch: 070 ----
mean loss: 207.41
 ---- batch: 080 ----
mean loss: 204.39
 ---- batch: 090 ----
mean loss: 196.39
 ---- batch: 100 ----
mean loss: 195.16
 ---- batch: 110 ----
mean loss: 205.86
train mean loss: 204.76
epoch train time: 0:00:00.719439
elapsed time: 0:02:46.501774
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 01:28:14.368547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.15
 ---- batch: 020 ----
mean loss: 199.61
 ---- batch: 030 ----
mean loss: 204.77
 ---- batch: 040 ----
mean loss: 208.66
 ---- batch: 050 ----
mean loss: 209.63
 ---- batch: 060 ----
mean loss: 213.47
 ---- batch: 070 ----
mean loss: 203.14
 ---- batch: 080 ----
mean loss: 197.47
 ---- batch: 090 ----
mean loss: 210.06
 ---- batch: 100 ----
mean loss: 204.87
 ---- batch: 110 ----
mean loss: 203.87
train mean loss: 204.72
epoch train time: 0:00:00.722883
elapsed time: 0:02:47.224796
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 01:28:15.091581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.71
 ---- batch: 020 ----
mean loss: 208.33
 ---- batch: 030 ----
mean loss: 213.81
 ---- batch: 040 ----
mean loss: 195.88
 ---- batch: 050 ----
mean loss: 205.41
 ---- batch: 060 ----
mean loss: 202.24
 ---- batch: 070 ----
mean loss: 208.04
 ---- batch: 080 ----
mean loss: 198.58
 ---- batch: 090 ----
mean loss: 204.52
 ---- batch: 100 ----
mean loss: 205.73
 ---- batch: 110 ----
mean loss: 200.43
train mean loss: 204.48
epoch train time: 0:00:00.727883
elapsed time: 0:02:47.952836
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 01:28:15.819619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.76
 ---- batch: 020 ----
mean loss: 208.20
 ---- batch: 030 ----
mean loss: 215.32
 ---- batch: 040 ----
mean loss: 207.41
 ---- batch: 050 ----
mean loss: 202.42
 ---- batch: 060 ----
mean loss: 201.78
 ---- batch: 070 ----
mean loss: 202.15
 ---- batch: 080 ----
mean loss: 211.41
 ---- batch: 090 ----
mean loss: 205.28
 ---- batch: 100 ----
mean loss: 195.59
 ---- batch: 110 ----
mean loss: 197.82
train mean loss: 204.41
epoch train time: 0:00:00.719811
elapsed time: 0:02:48.672809
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 01:28:16.539599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.96
 ---- batch: 020 ----
mean loss: 200.75
 ---- batch: 030 ----
mean loss: 213.24
 ---- batch: 040 ----
mean loss: 202.82
 ---- batch: 050 ----
mean loss: 197.69
 ---- batch: 060 ----
mean loss: 209.83
 ---- batch: 070 ----
mean loss: 212.50
 ---- batch: 080 ----
mean loss: 201.49
 ---- batch: 090 ----
mean loss: 197.65
 ---- batch: 100 ----
mean loss: 206.29
 ---- batch: 110 ----
mean loss: 211.53
train mean loss: 204.39
epoch train time: 0:00:00.716207
elapsed time: 0:02:49.389185
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 01:28:17.255971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.42
 ---- batch: 020 ----
mean loss: 197.86
 ---- batch: 030 ----
mean loss: 203.97
 ---- batch: 040 ----
mean loss: 205.04
 ---- batch: 050 ----
mean loss: 209.72
 ---- batch: 060 ----
mean loss: 209.46
 ---- batch: 070 ----
mean loss: 201.33
 ---- batch: 080 ----
mean loss: 196.95
 ---- batch: 090 ----
mean loss: 209.27
 ---- batch: 100 ----
mean loss: 205.90
 ---- batch: 110 ----
mean loss: 201.45
train mean loss: 204.19
epoch train time: 0:00:00.717030
elapsed time: 0:02:50.106370
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 01:28:17.973143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.52
 ---- batch: 020 ----
mean loss: 202.57
 ---- batch: 030 ----
mean loss: 199.21
 ---- batch: 040 ----
mean loss: 204.47
 ---- batch: 050 ----
mean loss: 209.25
 ---- batch: 060 ----
mean loss: 213.47
 ---- batch: 070 ----
mean loss: 199.17
 ---- batch: 080 ----
mean loss: 205.78
 ---- batch: 090 ----
mean loss: 200.83
 ---- batch: 100 ----
mean loss: 196.42
 ---- batch: 110 ----
mean loss: 201.80
train mean loss: 204.00
epoch train time: 0:00:00.718937
elapsed time: 0:02:50.825448
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 01:28:18.692238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.68
 ---- batch: 020 ----
mean loss: 204.53
 ---- batch: 030 ----
mean loss: 202.72
 ---- batch: 040 ----
mean loss: 207.87
 ---- batch: 050 ----
mean loss: 210.45
 ---- batch: 060 ----
mean loss: 197.86
 ---- batch: 070 ----
mean loss: 196.95
 ---- batch: 080 ----
mean loss: 204.27
 ---- batch: 090 ----
mean loss: 194.70
 ---- batch: 100 ----
mean loss: 209.62
 ---- batch: 110 ----
mean loss: 210.24
train mean loss: 203.85
epoch train time: 0:00:00.730496
elapsed time: 0:02:51.556103
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 01:28:19.422876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.44
 ---- batch: 020 ----
mean loss: 212.66
 ---- batch: 030 ----
mean loss: 209.05
 ---- batch: 040 ----
mean loss: 202.78
 ---- batch: 050 ----
mean loss: 195.49
 ---- batch: 060 ----
mean loss: 203.29
 ---- batch: 070 ----
mean loss: 213.29
 ---- batch: 080 ----
mean loss: 200.93
 ---- batch: 090 ----
mean loss: 205.20
 ---- batch: 100 ----
mean loss: 207.81
 ---- batch: 110 ----
mean loss: 193.74
train mean loss: 203.67
epoch train time: 0:00:00.723013
elapsed time: 0:02:52.279251
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 01:28:20.146023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.10
 ---- batch: 020 ----
mean loss: 207.80
 ---- batch: 030 ----
mean loss: 211.95
 ---- batch: 040 ----
mean loss: 208.17
 ---- batch: 050 ----
mean loss: 193.26
 ---- batch: 060 ----
mean loss: 199.09
 ---- batch: 070 ----
mean loss: 205.79
 ---- batch: 080 ----
mean loss: 200.47
 ---- batch: 090 ----
mean loss: 207.39
 ---- batch: 100 ----
mean loss: 202.40
 ---- batch: 110 ----
mean loss: 209.59
train mean loss: 203.63
epoch train time: 0:00:00.714446
elapsed time: 0:02:52.993855
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 01:28:20.860628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.09
 ---- batch: 020 ----
mean loss: 205.74
 ---- batch: 030 ----
mean loss: 198.06
 ---- batch: 040 ----
mean loss: 214.58
 ---- batch: 050 ----
mean loss: 200.23
 ---- batch: 060 ----
mean loss: 202.56
 ---- batch: 070 ----
mean loss: 201.97
 ---- batch: 080 ----
mean loss: 204.24
 ---- batch: 090 ----
mean loss: 200.98
 ---- batch: 100 ----
mean loss: 191.54
 ---- batch: 110 ----
mean loss: 210.40
train mean loss: 203.36
epoch train time: 0:00:00.712268
elapsed time: 0:02:53.706291
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 01:28:21.573066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.18
 ---- batch: 020 ----
mean loss: 202.93
 ---- batch: 030 ----
mean loss: 207.44
 ---- batch: 040 ----
mean loss: 199.02
 ---- batch: 050 ----
mean loss: 207.54
 ---- batch: 060 ----
mean loss: 201.17
 ---- batch: 070 ----
mean loss: 218.34
 ---- batch: 080 ----
mean loss: 206.75
 ---- batch: 090 ----
mean loss: 199.43
 ---- batch: 100 ----
mean loss: 198.38
 ---- batch: 110 ----
mean loss: 197.19
train mean loss: 203.45
epoch train time: 0:00:00.722915
elapsed time: 0:02:54.429345
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 01:28:22.296119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.48
 ---- batch: 020 ----
mean loss: 200.95
 ---- batch: 030 ----
mean loss: 201.96
 ---- batch: 040 ----
mean loss: 204.01
 ---- batch: 050 ----
mean loss: 201.55
 ---- batch: 060 ----
mean loss: 209.76
 ---- batch: 070 ----
mean loss: 198.06
 ---- batch: 080 ----
mean loss: 198.99
 ---- batch: 090 ----
mean loss: 201.33
 ---- batch: 100 ----
mean loss: 194.23
 ---- batch: 110 ----
mean loss: 209.87
train mean loss: 203.28
epoch train time: 0:00:00.721714
elapsed time: 0:02:55.151216
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 01:28:23.017991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.69
 ---- batch: 020 ----
mean loss: 196.72
 ---- batch: 030 ----
mean loss: 213.71
 ---- batch: 040 ----
mean loss: 193.91
 ---- batch: 050 ----
mean loss: 206.59
 ---- batch: 060 ----
mean loss: 214.88
 ---- batch: 070 ----
mean loss: 202.29
 ---- batch: 080 ----
mean loss: 206.29
 ---- batch: 090 ----
mean loss: 192.44
 ---- batch: 100 ----
mean loss: 199.71
 ---- batch: 110 ----
mean loss: 206.78
train mean loss: 203.09
epoch train time: 0:00:00.730182
elapsed time: 0:02:55.881560
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 01:28:23.748364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.59
 ---- batch: 020 ----
mean loss: 198.80
 ---- batch: 030 ----
mean loss: 202.62
 ---- batch: 040 ----
mean loss: 197.92
 ---- batch: 050 ----
mean loss: 206.43
 ---- batch: 060 ----
mean loss: 206.10
 ---- batch: 070 ----
mean loss: 201.36
 ---- batch: 080 ----
mean loss: 196.83
 ---- batch: 090 ----
mean loss: 202.22
 ---- batch: 100 ----
mean loss: 203.52
 ---- batch: 110 ----
mean loss: 214.83
train mean loss: 203.12
epoch train time: 0:00:00.721132
elapsed time: 0:02:56.602905
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 01:28:24.469687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.76
 ---- batch: 020 ----
mean loss: 200.36
 ---- batch: 030 ----
mean loss: 212.50
 ---- batch: 040 ----
mean loss: 201.88
 ---- batch: 050 ----
mean loss: 200.08
 ---- batch: 060 ----
mean loss: 202.56
 ---- batch: 070 ----
mean loss: 198.46
 ---- batch: 080 ----
mean loss: 204.63
 ---- batch: 090 ----
mean loss: 203.87
 ---- batch: 100 ----
mean loss: 196.50
 ---- batch: 110 ----
mean loss: 201.15
train mean loss: 202.53
epoch train time: 0:00:00.714031
elapsed time: 0:02:57.317094
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 01:28:25.183857
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.30
 ---- batch: 020 ----
mean loss: 198.50
 ---- batch: 030 ----
mean loss: 194.06
 ---- batch: 040 ----
mean loss: 203.83
 ---- batch: 050 ----
mean loss: 204.74
 ---- batch: 060 ----
mean loss: 202.38
 ---- batch: 070 ----
mean loss: 197.92
 ---- batch: 080 ----
mean loss: 211.38
 ---- batch: 090 ----
mean loss: 208.11
 ---- batch: 100 ----
mean loss: 203.00
 ---- batch: 110 ----
mean loss: 195.72
train mean loss: 202.54
epoch train time: 0:00:00.719928
elapsed time: 0:02:58.037149
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 01:28:25.903946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.60
 ---- batch: 020 ----
mean loss: 206.20
 ---- batch: 030 ----
mean loss: 206.43
 ---- batch: 040 ----
mean loss: 203.55
 ---- batch: 050 ----
mean loss: 206.06
 ---- batch: 060 ----
mean loss: 201.71
 ---- batch: 070 ----
mean loss: 196.20
 ---- batch: 080 ----
mean loss: 211.36
 ---- batch: 090 ----
mean loss: 202.16
 ---- batch: 100 ----
mean loss: 198.49
 ---- batch: 110 ----
mean loss: 196.23
train mean loss: 202.46
epoch train time: 0:00:00.713938
elapsed time: 0:02:58.751248
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 01:28:26.618023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.88
 ---- batch: 020 ----
mean loss: 205.21
 ---- batch: 030 ----
mean loss: 197.19
 ---- batch: 040 ----
mean loss: 200.32
 ---- batch: 050 ----
mean loss: 207.32
 ---- batch: 060 ----
mean loss: 202.76
 ---- batch: 070 ----
mean loss: 205.62
 ---- batch: 080 ----
mean loss: 208.51
 ---- batch: 090 ----
mean loss: 203.25
 ---- batch: 100 ----
mean loss: 195.56
 ---- batch: 110 ----
mean loss: 197.79
train mean loss: 202.44
epoch train time: 0:00:00.723947
elapsed time: 0:02:59.475335
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 01:28:27.342107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.85
 ---- batch: 020 ----
mean loss: 196.80
 ---- batch: 030 ----
mean loss: 201.21
 ---- batch: 040 ----
mean loss: 198.66
 ---- batch: 050 ----
mean loss: 205.23
 ---- batch: 060 ----
mean loss: 208.27
 ---- batch: 070 ----
mean loss: 204.72
 ---- batch: 080 ----
mean loss: 208.67
 ---- batch: 090 ----
mean loss: 199.62
 ---- batch: 100 ----
mean loss: 194.16
 ---- batch: 110 ----
mean loss: 211.53
train mean loss: 202.47
epoch train time: 0:00:00.721284
elapsed time: 0:03:00.196759
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 01:28:28.063552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.72
 ---- batch: 020 ----
mean loss: 199.86
 ---- batch: 030 ----
mean loss: 202.54
 ---- batch: 040 ----
mean loss: 205.98
 ---- batch: 050 ----
mean loss: 194.34
 ---- batch: 060 ----
mean loss: 204.58
 ---- batch: 070 ----
mean loss: 203.45
 ---- batch: 080 ----
mean loss: 208.20
 ---- batch: 090 ----
mean loss: 200.46
 ---- batch: 100 ----
mean loss: 192.79
 ---- batch: 110 ----
mean loss: 208.34
train mean loss: 202.52
epoch train time: 0:00:00.708861
elapsed time: 0:03:00.905820
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 01:28:28.772592
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.43
 ---- batch: 020 ----
mean loss: 204.54
 ---- batch: 030 ----
mean loss: 205.36
 ---- batch: 040 ----
mean loss: 210.03
 ---- batch: 050 ----
mean loss: 192.15
 ---- batch: 060 ----
mean loss: 203.45
 ---- batch: 070 ----
mean loss: 194.01
 ---- batch: 080 ----
mean loss: 204.74
 ---- batch: 090 ----
mean loss: 196.81
 ---- batch: 100 ----
mean loss: 216.59
 ---- batch: 110 ----
mean loss: 195.96
train mean loss: 202.53
epoch train time: 0:00:00.725308
elapsed time: 0:03:01.631334
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 01:28:29.498111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.28
 ---- batch: 020 ----
mean loss: 199.89
 ---- batch: 030 ----
mean loss: 196.08
 ---- batch: 040 ----
mean loss: 198.63
 ---- batch: 050 ----
mean loss: 203.77
 ---- batch: 060 ----
mean loss: 212.21
 ---- batch: 070 ----
mean loss: 209.96
 ---- batch: 080 ----
mean loss: 209.11
 ---- batch: 090 ----
mean loss: 201.00
 ---- batch: 100 ----
mean loss: 196.35
 ---- batch: 110 ----
mean loss: 206.39
train mean loss: 202.52
epoch train time: 0:00:00.720154
elapsed time: 0:03:02.351658
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 01:28:30.218429
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.31
 ---- batch: 020 ----
mean loss: 184.05
 ---- batch: 030 ----
mean loss: 206.25
 ---- batch: 040 ----
mean loss: 198.24
 ---- batch: 050 ----
mean loss: 201.92
 ---- batch: 060 ----
mean loss: 207.68
 ---- batch: 070 ----
mean loss: 202.12
 ---- batch: 080 ----
mean loss: 201.11
 ---- batch: 090 ----
mean loss: 204.13
 ---- batch: 100 ----
mean loss: 203.67
 ---- batch: 110 ----
mean loss: 215.75
train mean loss: 202.40
epoch train time: 0:00:00.712257
elapsed time: 0:03:03.064054
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 01:28:30.930825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.57
 ---- batch: 020 ----
mean loss: 197.33
 ---- batch: 030 ----
mean loss: 205.29
 ---- batch: 040 ----
mean loss: 208.18
 ---- batch: 050 ----
mean loss: 205.83
 ---- batch: 060 ----
mean loss: 207.41
 ---- batch: 070 ----
mean loss: 197.81
 ---- batch: 080 ----
mean loss: 193.64
 ---- batch: 090 ----
mean loss: 190.93
 ---- batch: 100 ----
mean loss: 197.02
 ---- batch: 110 ----
mean loss: 204.85
train mean loss: 202.40
epoch train time: 0:00:00.716484
elapsed time: 0:03:03.780673
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 01:28:31.647445
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.43
 ---- batch: 020 ----
mean loss: 208.81
 ---- batch: 030 ----
mean loss: 200.81
 ---- batch: 040 ----
mean loss: 206.27
 ---- batch: 050 ----
mean loss: 211.85
 ---- batch: 060 ----
mean loss: 204.45
 ---- batch: 070 ----
mean loss: 194.81
 ---- batch: 080 ----
mean loss: 195.49
 ---- batch: 090 ----
mean loss: 208.33
 ---- batch: 100 ----
mean loss: 192.25
 ---- batch: 110 ----
mean loss: 199.38
train mean loss: 202.40
epoch train time: 0:00:00.720309
elapsed time: 0:03:04.501118
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 01:28:32.367898
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.99
 ---- batch: 020 ----
mean loss: 200.32
 ---- batch: 030 ----
mean loss: 202.37
 ---- batch: 040 ----
mean loss: 204.38
 ---- batch: 050 ----
mean loss: 207.48
 ---- batch: 060 ----
mean loss: 205.33
 ---- batch: 070 ----
mean loss: 207.10
 ---- batch: 080 ----
mean loss: 199.62
 ---- batch: 090 ----
mean loss: 202.90
 ---- batch: 100 ----
mean loss: 199.64
 ---- batch: 110 ----
mean loss: 200.22
train mean loss: 202.34
epoch train time: 0:00:00.713156
elapsed time: 0:03:05.214419
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 01:28:33.081209
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.55
 ---- batch: 020 ----
mean loss: 201.18
 ---- batch: 030 ----
mean loss: 191.66
 ---- batch: 040 ----
mean loss: 208.52
 ---- batch: 050 ----
mean loss: 206.68
 ---- batch: 060 ----
mean loss: 205.81
 ---- batch: 070 ----
mean loss: 196.63
 ---- batch: 080 ----
mean loss: 201.27
 ---- batch: 090 ----
mean loss: 195.44
 ---- batch: 100 ----
mean loss: 208.44
 ---- batch: 110 ----
mean loss: 207.25
train mean loss: 202.31
epoch train time: 0:00:00.714530
elapsed time: 0:03:05.929101
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 01:28:33.795913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.11
 ---- batch: 020 ----
mean loss: 205.11
 ---- batch: 030 ----
mean loss: 204.99
 ---- batch: 040 ----
mean loss: 190.49
 ---- batch: 050 ----
mean loss: 205.84
 ---- batch: 060 ----
mean loss: 200.81
 ---- batch: 070 ----
mean loss: 211.38
 ---- batch: 080 ----
mean loss: 204.86
 ---- batch: 090 ----
mean loss: 198.21
 ---- batch: 100 ----
mean loss: 202.43
 ---- batch: 110 ----
mean loss: 204.00
train mean loss: 202.41
epoch train time: 0:00:00.712067
elapsed time: 0:03:06.641356
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 01:28:34.508128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.93
 ---- batch: 020 ----
mean loss: 209.06
 ---- batch: 030 ----
mean loss: 201.83
 ---- batch: 040 ----
mean loss: 209.63
 ---- batch: 050 ----
mean loss: 198.34
 ---- batch: 060 ----
mean loss: 194.59
 ---- batch: 070 ----
mean loss: 211.62
 ---- batch: 080 ----
mean loss: 196.15
 ---- batch: 090 ----
mean loss: 189.86
 ---- batch: 100 ----
mean loss: 198.90
 ---- batch: 110 ----
mean loss: 203.93
train mean loss: 202.36
epoch train time: 0:00:00.713724
elapsed time: 0:03:07.355258
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 01:28:35.222030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.69
 ---- batch: 020 ----
mean loss: 200.31
 ---- batch: 030 ----
mean loss: 201.78
 ---- batch: 040 ----
mean loss: 204.92
 ---- batch: 050 ----
mean loss: 200.29
 ---- batch: 060 ----
mean loss: 202.00
 ---- batch: 070 ----
mean loss: 196.75
 ---- batch: 080 ----
mean loss: 207.65
 ---- batch: 090 ----
mean loss: 203.93
 ---- batch: 100 ----
mean loss: 204.15
 ---- batch: 110 ----
mean loss: 201.19
train mean loss: 202.37
epoch train time: 0:00:00.712266
elapsed time: 0:03:08.067700
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 01:28:35.934473
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.40
 ---- batch: 020 ----
mean loss: 199.65
 ---- batch: 030 ----
mean loss: 202.34
 ---- batch: 040 ----
mean loss: 201.86
 ---- batch: 050 ----
mean loss: 197.91
 ---- batch: 060 ----
mean loss: 206.38
 ---- batch: 070 ----
mean loss: 200.85
 ---- batch: 080 ----
mean loss: 207.39
 ---- batch: 090 ----
mean loss: 200.10
 ---- batch: 100 ----
mean loss: 207.63
 ---- batch: 110 ----
mean loss: 206.87
train mean loss: 202.37
epoch train time: 0:00:00.716027
elapsed time: 0:03:08.783880
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 01:28:36.650653
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.51
 ---- batch: 020 ----
mean loss: 208.15
 ---- batch: 030 ----
mean loss: 206.59
 ---- batch: 040 ----
mean loss: 206.88
 ---- batch: 050 ----
mean loss: 200.48
 ---- batch: 060 ----
mean loss: 204.87
 ---- batch: 070 ----
mean loss: 205.20
 ---- batch: 080 ----
mean loss: 192.36
 ---- batch: 090 ----
mean loss: 201.08
 ---- batch: 100 ----
mean loss: 198.17
 ---- batch: 110 ----
mean loss: 200.93
train mean loss: 202.29
epoch train time: 0:00:00.722756
elapsed time: 0:03:09.506803
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 01:28:37.373646
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.10
 ---- batch: 020 ----
mean loss: 205.97
 ---- batch: 030 ----
mean loss: 198.94
 ---- batch: 040 ----
mean loss: 198.91
 ---- batch: 050 ----
mean loss: 205.67
 ---- batch: 060 ----
mean loss: 197.53
 ---- batch: 070 ----
mean loss: 199.38
 ---- batch: 080 ----
mean loss: 208.34
 ---- batch: 090 ----
mean loss: 203.53
 ---- batch: 100 ----
mean loss: 201.69
 ---- batch: 110 ----
mean loss: 204.55
train mean loss: 202.33
epoch train time: 0:00:00.723840
elapsed time: 0:03:10.230868
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 01:28:38.097667
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.72
 ---- batch: 020 ----
mean loss: 206.03
 ---- batch: 030 ----
mean loss: 202.51
 ---- batch: 040 ----
mean loss: 201.33
 ---- batch: 050 ----
mean loss: 210.18
 ---- batch: 060 ----
mean loss: 197.30
 ---- batch: 070 ----
mean loss: 202.12
 ---- batch: 080 ----
mean loss: 203.15
 ---- batch: 090 ----
mean loss: 202.68
 ---- batch: 100 ----
mean loss: 208.62
 ---- batch: 110 ----
mean loss: 199.17
train mean loss: 202.27
epoch train time: 0:00:00.722418
elapsed time: 0:03:10.953447
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 01:28:38.820233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.24
 ---- batch: 020 ----
mean loss: 195.84
 ---- batch: 030 ----
mean loss: 205.54
 ---- batch: 040 ----
mean loss: 214.06
 ---- batch: 050 ----
mean loss: 195.44
 ---- batch: 060 ----
mean loss: 199.06
 ---- batch: 070 ----
mean loss: 206.72
 ---- batch: 080 ----
mean loss: 206.35
 ---- batch: 090 ----
mean loss: 199.01
 ---- batch: 100 ----
mean loss: 199.63
 ---- batch: 110 ----
mean loss: 207.24
train mean loss: 202.23
epoch train time: 0:00:00.720477
elapsed time: 0:03:11.674081
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 01:28:39.540868
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.14
 ---- batch: 020 ----
mean loss: 208.53
 ---- batch: 030 ----
mean loss: 215.88
 ---- batch: 040 ----
mean loss: 195.54
 ---- batch: 050 ----
mean loss: 201.67
 ---- batch: 060 ----
mean loss: 199.72
 ---- batch: 070 ----
mean loss: 200.30
 ---- batch: 080 ----
mean loss: 197.90
 ---- batch: 090 ----
mean loss: 196.23
 ---- batch: 100 ----
mean loss: 204.76
 ---- batch: 110 ----
mean loss: 197.21
train mean loss: 202.31
epoch train time: 0:00:00.723113
elapsed time: 0:03:12.397344
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 01:28:40.264132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.04
 ---- batch: 020 ----
mean loss: 201.10
 ---- batch: 030 ----
mean loss: 197.63
 ---- batch: 040 ----
mean loss: 208.26
 ---- batch: 050 ----
mean loss: 210.58
 ---- batch: 060 ----
mean loss: 200.06
 ---- batch: 070 ----
mean loss: 200.20
 ---- batch: 080 ----
mean loss: 197.36
 ---- batch: 090 ----
mean loss: 203.37
 ---- batch: 100 ----
mean loss: 204.11
 ---- batch: 110 ----
mean loss: 200.17
train mean loss: 202.26
epoch train time: 0:00:00.724324
elapsed time: 0:03:13.121817
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 01:28:40.988588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.80
 ---- batch: 020 ----
mean loss: 203.24
 ---- batch: 030 ----
mean loss: 200.88
 ---- batch: 040 ----
mean loss: 205.23
 ---- batch: 050 ----
mean loss: 199.55
 ---- batch: 060 ----
mean loss: 211.08
 ---- batch: 070 ----
mean loss: 201.09
 ---- batch: 080 ----
mean loss: 206.70
 ---- batch: 090 ----
mean loss: 207.25
 ---- batch: 100 ----
mean loss: 189.26
 ---- batch: 110 ----
mean loss: 199.44
train mean loss: 202.26
epoch train time: 0:00:00.706870
elapsed time: 0:03:13.828827
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 01:28:41.695605
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.84
 ---- batch: 020 ----
mean loss: 198.51
 ---- batch: 030 ----
mean loss: 206.08
 ---- batch: 040 ----
mean loss: 206.85
 ---- batch: 050 ----
mean loss: 194.41
 ---- batch: 060 ----
mean loss: 203.23
 ---- batch: 070 ----
mean loss: 197.56
 ---- batch: 080 ----
mean loss: 195.87
 ---- batch: 090 ----
mean loss: 206.31
 ---- batch: 100 ----
mean loss: 208.84
 ---- batch: 110 ----
mean loss: 200.07
train mean loss: 202.28
epoch train time: 0:00:00.716568
elapsed time: 0:03:14.545535
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 01:28:42.412329
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.95
 ---- batch: 020 ----
mean loss: 211.01
 ---- batch: 030 ----
mean loss: 204.46
 ---- batch: 040 ----
mean loss: 198.11
 ---- batch: 050 ----
mean loss: 202.62
 ---- batch: 060 ----
mean loss: 188.34
 ---- batch: 070 ----
mean loss: 209.03
 ---- batch: 080 ----
mean loss: 198.33
 ---- batch: 090 ----
mean loss: 210.11
 ---- batch: 100 ----
mean loss: 205.52
 ---- batch: 110 ----
mean loss: 202.77
train mean loss: 202.18
epoch train time: 0:00:00.712706
elapsed time: 0:03:15.258404
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 01:28:43.125206
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.07
 ---- batch: 020 ----
mean loss: 202.51
 ---- batch: 030 ----
mean loss: 202.47
 ---- batch: 040 ----
mean loss: 197.77
 ---- batch: 050 ----
mean loss: 199.61
 ---- batch: 060 ----
mean loss: 207.21
 ---- batch: 070 ----
mean loss: 209.91
 ---- batch: 080 ----
mean loss: 209.83
 ---- batch: 090 ----
mean loss: 203.93
 ---- batch: 100 ----
mean loss: 202.29
 ---- batch: 110 ----
mean loss: 195.22
train mean loss: 202.18
epoch train time: 0:00:00.712364
elapsed time: 0:03:15.970965
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 01:28:43.837743
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.23
 ---- batch: 020 ----
mean loss: 198.28
 ---- batch: 030 ----
mean loss: 197.12
 ---- batch: 040 ----
mean loss: 203.47
 ---- batch: 050 ----
mean loss: 201.91
 ---- batch: 060 ----
mean loss: 205.93
 ---- batch: 070 ----
mean loss: 209.72
 ---- batch: 080 ----
mean loss: 203.66
 ---- batch: 090 ----
mean loss: 197.14
 ---- batch: 100 ----
mean loss: 199.73
 ---- batch: 110 ----
mean loss: 200.99
train mean loss: 202.23
epoch train time: 0:00:00.729534
elapsed time: 0:03:16.700644
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 01:28:44.567440
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.04
 ---- batch: 020 ----
mean loss: 206.80
 ---- batch: 030 ----
mean loss: 213.46
 ---- batch: 040 ----
mean loss: 205.34
 ---- batch: 050 ----
mean loss: 201.44
 ---- batch: 060 ----
mean loss: 198.41
 ---- batch: 070 ----
mean loss: 199.68
 ---- batch: 080 ----
mean loss: 190.52
 ---- batch: 090 ----
mean loss: 206.82
 ---- batch: 100 ----
mean loss: 204.51
 ---- batch: 110 ----
mean loss: 203.40
train mean loss: 202.15
epoch train time: 0:00:00.718881
elapsed time: 0:03:17.419688
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 01:28:45.286471
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.98
 ---- batch: 020 ----
mean loss: 199.04
 ---- batch: 030 ----
mean loss: 195.06
 ---- batch: 040 ----
mean loss: 215.38
 ---- batch: 050 ----
mean loss: 196.42
 ---- batch: 060 ----
mean loss: 200.95
 ---- batch: 070 ----
mean loss: 204.63
 ---- batch: 080 ----
mean loss: 204.23
 ---- batch: 090 ----
mean loss: 200.72
 ---- batch: 100 ----
mean loss: 195.68
 ---- batch: 110 ----
mean loss: 203.43
train mean loss: 202.26
epoch train time: 0:00:00.719754
elapsed time: 0:03:18.139590
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 01:28:46.006364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.34
 ---- batch: 020 ----
mean loss: 194.25
 ---- batch: 030 ----
mean loss: 193.92
 ---- batch: 040 ----
mean loss: 207.48
 ---- batch: 050 ----
mean loss: 205.33
 ---- batch: 060 ----
mean loss: 194.27
 ---- batch: 070 ----
mean loss: 206.69
 ---- batch: 080 ----
mean loss: 212.36
 ---- batch: 090 ----
mean loss: 200.25
 ---- batch: 100 ----
mean loss: 208.29
 ---- batch: 110 ----
mean loss: 202.63
train mean loss: 202.14
epoch train time: 0:00:00.717388
elapsed time: 0:03:18.857111
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 01:28:46.723905
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.83
 ---- batch: 020 ----
mean loss: 198.54
 ---- batch: 030 ----
mean loss: 209.09
 ---- batch: 040 ----
mean loss: 208.06
 ---- batch: 050 ----
mean loss: 189.49
 ---- batch: 060 ----
mean loss: 205.90
 ---- batch: 070 ----
mean loss: 203.49
 ---- batch: 080 ----
mean loss: 206.81
 ---- batch: 090 ----
mean loss: 201.12
 ---- batch: 100 ----
mean loss: 201.15
 ---- batch: 110 ----
mean loss: 201.05
train mean loss: 202.08
epoch train time: 0:00:00.710049
elapsed time: 0:03:19.567326
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 01:28:47.434102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.86
 ---- batch: 020 ----
mean loss: 195.90
 ---- batch: 030 ----
mean loss: 199.82
 ---- batch: 040 ----
mean loss: 203.12
 ---- batch: 050 ----
mean loss: 201.90
 ---- batch: 060 ----
mean loss: 212.59
 ---- batch: 070 ----
mean loss: 205.35
 ---- batch: 080 ----
mean loss: 201.91
 ---- batch: 090 ----
mean loss: 198.05
 ---- batch: 100 ----
mean loss: 205.82
 ---- batch: 110 ----
mean loss: 201.33
train mean loss: 202.10
epoch train time: 0:00:00.715165
elapsed time: 0:03:20.282645
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 01:28:48.149410
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.05
 ---- batch: 020 ----
mean loss: 199.42
 ---- batch: 030 ----
mean loss: 191.72
 ---- batch: 040 ----
mean loss: 204.76
 ---- batch: 050 ----
mean loss: 197.48
 ---- batch: 060 ----
mean loss: 202.23
 ---- batch: 070 ----
mean loss: 198.18
 ---- batch: 080 ----
mean loss: 212.06
 ---- batch: 090 ----
mean loss: 210.64
 ---- batch: 100 ----
mean loss: 204.02
 ---- batch: 110 ----
mean loss: 200.02
train mean loss: 202.09
epoch train time: 0:00:00.710539
elapsed time: 0:03:20.993343
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 01:28:48.860115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.90
 ---- batch: 020 ----
mean loss: 195.74
 ---- batch: 030 ----
mean loss: 199.66
 ---- batch: 040 ----
mean loss: 210.43
 ---- batch: 050 ----
mean loss: 196.58
 ---- batch: 060 ----
mean loss: 209.23
 ---- batch: 070 ----
mean loss: 204.09
 ---- batch: 080 ----
mean loss: 195.90
 ---- batch: 090 ----
mean loss: 199.40
 ---- batch: 100 ----
mean loss: 206.29
 ---- batch: 110 ----
mean loss: 198.65
train mean loss: 202.10
epoch train time: 0:00:00.709344
elapsed time: 0:03:21.702823
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 01:28:49.569597
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.82
 ---- batch: 020 ----
mean loss: 207.16
 ---- batch: 030 ----
mean loss: 197.33
 ---- batch: 040 ----
mean loss: 206.70
 ---- batch: 050 ----
mean loss: 202.51
 ---- batch: 060 ----
mean loss: 214.51
 ---- batch: 070 ----
mean loss: 203.20
 ---- batch: 080 ----
mean loss: 194.80
 ---- batch: 090 ----
mean loss: 200.99
 ---- batch: 100 ----
mean loss: 198.64
 ---- batch: 110 ----
mean loss: 197.66
train mean loss: 202.10
epoch train time: 0:00:00.720296
elapsed time: 0:03:22.423261
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 01:28:50.290034
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.78
 ---- batch: 020 ----
mean loss: 212.32
 ---- batch: 030 ----
mean loss: 201.19
 ---- batch: 040 ----
mean loss: 194.73
 ---- batch: 050 ----
mean loss: 203.56
 ---- batch: 060 ----
mean loss: 206.27
 ---- batch: 070 ----
mean loss: 193.68
 ---- batch: 080 ----
mean loss: 198.32
 ---- batch: 090 ----
mean loss: 205.82
 ---- batch: 100 ----
mean loss: 206.37
 ---- batch: 110 ----
mean loss: 194.37
train mean loss: 202.07
epoch train time: 0:00:00.726912
elapsed time: 0:03:23.150324
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 01:28:51.017096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.35
 ---- batch: 020 ----
mean loss: 206.90
 ---- batch: 030 ----
mean loss: 200.17
 ---- batch: 040 ----
mean loss: 203.32
 ---- batch: 050 ----
mean loss: 199.14
 ---- batch: 060 ----
mean loss: 206.09
 ---- batch: 070 ----
mean loss: 212.81
 ---- batch: 080 ----
mean loss: 197.84
 ---- batch: 090 ----
mean loss: 192.85
 ---- batch: 100 ----
mean loss: 211.06
 ---- batch: 110 ----
mean loss: 196.98
train mean loss: 202.03
epoch train time: 0:00:00.705583
elapsed time: 0:03:23.856045
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 01:28:51.722837
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.25
 ---- batch: 020 ----
mean loss: 202.98
 ---- batch: 030 ----
mean loss: 203.93
 ---- batch: 040 ----
mean loss: 210.82
 ---- batch: 050 ----
mean loss: 197.51
 ---- batch: 060 ----
mean loss: 208.63
 ---- batch: 070 ----
mean loss: 202.00
 ---- batch: 080 ----
mean loss: 211.68
 ---- batch: 090 ----
mean loss: 194.93
 ---- batch: 100 ----
mean loss: 201.31
 ---- batch: 110 ----
mean loss: 195.29
train mean loss: 202.05
epoch train time: 0:00:00.726199
elapsed time: 0:03:24.582457
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 01:28:52.449231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.52
 ---- batch: 020 ----
mean loss: 197.20
 ---- batch: 030 ----
mean loss: 204.76
 ---- batch: 040 ----
mean loss: 192.65
 ---- batch: 050 ----
mean loss: 202.67
 ---- batch: 060 ----
mean loss: 200.57
 ---- batch: 070 ----
mean loss: 204.44
 ---- batch: 080 ----
mean loss: 203.41
 ---- batch: 090 ----
mean loss: 213.54
 ---- batch: 100 ----
mean loss: 202.86
 ---- batch: 110 ----
mean loss: 204.50
train mean loss: 202.06
epoch train time: 0:00:00.742329
elapsed time: 0:03:25.324924
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 01:28:53.191699
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.81
 ---- batch: 020 ----
mean loss: 202.60
 ---- batch: 030 ----
mean loss: 200.02
 ---- batch: 040 ----
mean loss: 202.44
 ---- batch: 050 ----
mean loss: 206.47
 ---- batch: 060 ----
mean loss: 206.54
 ---- batch: 070 ----
mean loss: 205.13
 ---- batch: 080 ----
mean loss: 201.76
 ---- batch: 090 ----
mean loss: 204.36
 ---- batch: 100 ----
mean loss: 209.30
 ---- batch: 110 ----
mean loss: 192.95
train mean loss: 202.01
epoch train time: 0:00:00.736952
elapsed time: 0:03:26.062015
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 01:28:53.928791
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.75
 ---- batch: 020 ----
mean loss: 218.39
 ---- batch: 030 ----
mean loss: 210.50
 ---- batch: 040 ----
mean loss: 196.17
 ---- batch: 050 ----
mean loss: 197.40
 ---- batch: 060 ----
mean loss: 202.49
 ---- batch: 070 ----
mean loss: 205.42
 ---- batch: 080 ----
mean loss: 184.64
 ---- batch: 090 ----
mean loss: 200.65
 ---- batch: 100 ----
mean loss: 206.64
 ---- batch: 110 ----
mean loss: 198.14
train mean loss: 201.97
epoch train time: 0:00:00.739281
elapsed time: 0:03:26.801436
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 01:28:54.668209
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.84
 ---- batch: 020 ----
mean loss: 209.39
 ---- batch: 030 ----
mean loss: 198.71
 ---- batch: 040 ----
mean loss: 210.77
 ---- batch: 050 ----
mean loss: 207.42
 ---- batch: 060 ----
mean loss: 200.77
 ---- batch: 070 ----
mean loss: 208.22
 ---- batch: 080 ----
mean loss: 195.60
 ---- batch: 090 ----
mean loss: 192.14
 ---- batch: 100 ----
mean loss: 205.97
 ---- batch: 110 ----
mean loss: 197.46
train mean loss: 202.00
epoch train time: 0:00:00.729593
elapsed time: 0:03:27.531166
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 01:28:55.397947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.52
 ---- batch: 020 ----
mean loss: 202.26
 ---- batch: 030 ----
mean loss: 202.96
 ---- batch: 040 ----
mean loss: 201.50
 ---- batch: 050 ----
mean loss: 201.74
 ---- batch: 060 ----
mean loss: 194.80
 ---- batch: 070 ----
mean loss: 206.17
 ---- batch: 080 ----
mean loss: 208.63
 ---- batch: 090 ----
mean loss: 200.95
 ---- batch: 100 ----
mean loss: 202.90
 ---- batch: 110 ----
mean loss: 206.87
train mean loss: 201.96
epoch train time: 0:00:00.736017
elapsed time: 0:03:28.267327
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 01:28:56.134117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.75
 ---- batch: 020 ----
mean loss: 198.11
 ---- batch: 030 ----
mean loss: 203.43
 ---- batch: 040 ----
mean loss: 201.10
 ---- batch: 050 ----
mean loss: 199.85
 ---- batch: 060 ----
mean loss: 205.23
 ---- batch: 070 ----
mean loss: 207.04
 ---- batch: 080 ----
mean loss: 196.43
 ---- batch: 090 ----
mean loss: 200.86
 ---- batch: 100 ----
mean loss: 206.36
 ---- batch: 110 ----
mean loss: 208.43
train mean loss: 201.98
epoch train time: 0:00:00.739099
elapsed time: 0:03:29.006595
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 01:28:56.873369
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.80
 ---- batch: 020 ----
mean loss: 200.09
 ---- batch: 030 ----
mean loss: 199.56
 ---- batch: 040 ----
mean loss: 199.45
 ---- batch: 050 ----
mean loss: 203.67
 ---- batch: 060 ----
mean loss: 200.26
 ---- batch: 070 ----
mean loss: 203.10
 ---- batch: 080 ----
mean loss: 199.85
 ---- batch: 090 ----
mean loss: 199.22
 ---- batch: 100 ----
mean loss: 201.75
 ---- batch: 110 ----
mean loss: 204.01
train mean loss: 202.00
epoch train time: 0:00:00.714277
elapsed time: 0:03:29.721008
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 01:28:57.587781
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.98
 ---- batch: 020 ----
mean loss: 208.78
 ---- batch: 030 ----
mean loss: 203.70
 ---- batch: 040 ----
mean loss: 202.04
 ---- batch: 050 ----
mean loss: 199.29
 ---- batch: 060 ----
mean loss: 202.61
 ---- batch: 070 ----
mean loss: 205.05
 ---- batch: 080 ----
mean loss: 197.50
 ---- batch: 090 ----
mean loss: 203.24
 ---- batch: 100 ----
mean loss: 207.19
 ---- batch: 110 ----
mean loss: 207.83
train mean loss: 201.93
epoch train time: 0:00:00.712515
elapsed time: 0:03:30.433657
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 01:28:58.300444
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.57
 ---- batch: 020 ----
mean loss: 202.00
 ---- batch: 030 ----
mean loss: 202.71
 ---- batch: 040 ----
mean loss: 200.69
 ---- batch: 050 ----
mean loss: 196.58
 ---- batch: 060 ----
mean loss: 198.79
 ---- batch: 070 ----
mean loss: 197.67
 ---- batch: 080 ----
mean loss: 201.58
 ---- batch: 090 ----
mean loss: 205.69
 ---- batch: 100 ----
mean loss: 209.77
 ---- batch: 110 ----
mean loss: 202.99
train mean loss: 201.86
epoch train time: 0:00:00.712540
elapsed time: 0:03:31.146346
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 01:28:59.013117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.23
 ---- batch: 020 ----
mean loss: 204.79
 ---- batch: 030 ----
mean loss: 198.21
 ---- batch: 040 ----
mean loss: 202.55
 ---- batch: 050 ----
mean loss: 200.05
 ---- batch: 060 ----
mean loss: 209.30
 ---- batch: 070 ----
mean loss: 201.14
 ---- batch: 080 ----
mean loss: 203.12
 ---- batch: 090 ----
mean loss: 205.21
 ---- batch: 100 ----
mean loss: 203.43
 ---- batch: 110 ----
mean loss: 199.59
train mean loss: 201.87
epoch train time: 0:00:00.711733
elapsed time: 0:03:31.860335
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_0/checkpoint.pth.tar
**** end time: 2019-09-27 01:28:59.727077 ****
