Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_6', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 12537
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 10:25:19.296227 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 10:25:19.315526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2709.59
 ---- batch: 020 ----
mean loss: 1416.13
 ---- batch: 030 ----
mean loss: 1228.94
 ---- batch: 040 ----
mean loss: 1171.75
train mean loss: 1546.24
epoch train time: 0:00:24.226007
elapsed time: 0:00:24.254461
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 10:25:43.550731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1071.86
 ---- batch: 020 ----
mean loss: 1071.30
 ---- batch: 030 ----
mean loss: 1056.36
 ---- batch: 040 ----
mean loss: 1024.79
train mean loss: 1051.31
epoch train time: 0:00:08.654615
elapsed time: 0:00:32.910066
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 10:25:52.206562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1009.31
 ---- batch: 020 ----
mean loss: 1012.72
 ---- batch: 030 ----
mean loss: 1022.92
 ---- batch: 040 ----
mean loss: 997.39
train mean loss: 1004.37
epoch train time: 0:00:08.642525
elapsed time: 0:00:41.553900
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 10:26:00.850446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 989.02
 ---- batch: 020 ----
mean loss: 990.92
 ---- batch: 030 ----
mean loss: 947.22
 ---- batch: 040 ----
mean loss: 976.57
train mean loss: 974.96
epoch train time: 0:00:08.667331
elapsed time: 0:00:50.222597
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 10:26:09.519160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 979.25
 ---- batch: 020 ----
mean loss: 955.90
 ---- batch: 030 ----
mean loss: 958.05
 ---- batch: 040 ----
mean loss: 952.05
train mean loss: 957.24
epoch train time: 0:00:08.651427
elapsed time: 0:00:58.875461
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 10:26:18.171984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.46
 ---- batch: 020 ----
mean loss: 951.40
 ---- batch: 030 ----
mean loss: 944.30
 ---- batch: 040 ----
mean loss: 928.48
train mean loss: 948.80
epoch train time: 0:00:08.686405
elapsed time: 0:01:07.563286
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 10:26:26.859892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.72
 ---- batch: 020 ----
mean loss: 932.88
 ---- batch: 030 ----
mean loss: 927.82
 ---- batch: 040 ----
mean loss: 917.59
train mean loss: 934.47
epoch train time: 0:00:08.653957
elapsed time: 0:01:16.218613
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 10:26:35.515187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.15
 ---- batch: 020 ----
mean loss: 938.96
 ---- batch: 030 ----
mean loss: 930.66
 ---- batch: 040 ----
mean loss: 947.02
train mean loss: 939.74
epoch train time: 0:00:08.503226
elapsed time: 0:01:24.723191
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 10:26:44.019733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.49
 ---- batch: 020 ----
mean loss: 945.11
 ---- batch: 030 ----
mean loss: 913.18
 ---- batch: 040 ----
mean loss: 940.19
train mean loss: 935.23
epoch train time: 0:00:08.378571
elapsed time: 0:01:33.103211
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 10:26:52.399766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.11
 ---- batch: 020 ----
mean loss: 937.97
 ---- batch: 030 ----
mean loss: 914.25
 ---- batch: 040 ----
mean loss: 936.48
train mean loss: 930.46
epoch train time: 0:00:08.407197
elapsed time: 0:01:41.511725
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 10:27:00.808317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 933.92
 ---- batch: 020 ----
mean loss: 927.11
 ---- batch: 030 ----
mean loss: 926.17
 ---- batch: 040 ----
mean loss: 916.31
train mean loss: 924.86
epoch train time: 0:00:08.505956
elapsed time: 0:01:50.019016
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 10:27:09.315549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.91
 ---- batch: 020 ----
mean loss: 927.61
 ---- batch: 030 ----
mean loss: 917.22
 ---- batch: 040 ----
mean loss: 918.08
train mean loss: 919.62
epoch train time: 0:00:08.480761
elapsed time: 0:01:58.501004
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 10:27:17.797505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.69
 ---- batch: 020 ----
mean loss: 904.81
 ---- batch: 030 ----
mean loss: 907.65
 ---- batch: 040 ----
mean loss: 911.26
train mean loss: 917.49
epoch train time: 0:00:08.366407
elapsed time: 0:02:06.868816
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 10:27:26.165351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.97
 ---- batch: 020 ----
mean loss: 906.20
 ---- batch: 030 ----
mean loss: 922.25
 ---- batch: 040 ----
mean loss: 912.40
train mean loss: 916.11
epoch train time: 0:00:08.505644
elapsed time: 0:02:15.375709
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 10:27:34.672232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.08
 ---- batch: 020 ----
mean loss: 918.73
 ---- batch: 030 ----
mean loss: 911.47
 ---- batch: 040 ----
mean loss: 905.00
train mean loss: 913.67
epoch train time: 0:00:08.564239
elapsed time: 0:02:23.941254
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 10:27:43.237809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.75
 ---- batch: 020 ----
mean loss: 903.58
 ---- batch: 030 ----
mean loss: 925.90
 ---- batch: 040 ----
mean loss: 905.17
train mean loss: 910.47
epoch train time: 0:00:08.490269
elapsed time: 0:02:32.432875
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 10:27:51.729419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.60
 ---- batch: 020 ----
mean loss: 920.10
 ---- batch: 030 ----
mean loss: 908.35
 ---- batch: 040 ----
mean loss: 915.01
train mean loss: 905.29
epoch train time: 0:00:08.638681
elapsed time: 0:02:41.072826
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 10:28:00.369286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.31
 ---- batch: 020 ----
mean loss: 902.48
 ---- batch: 030 ----
mean loss: 907.20
 ---- batch: 040 ----
mean loss: 903.34
train mean loss: 902.89
epoch train time: 0:00:08.635674
elapsed time: 0:02:49.709716
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 10:28:09.006322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.41
 ---- batch: 020 ----
mean loss: 907.68
 ---- batch: 030 ----
mean loss: 892.86
 ---- batch: 040 ----
mean loss: 895.77
train mean loss: 897.60
epoch train time: 0:00:08.722880
elapsed time: 0:02:58.433992
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 10:28:17.730529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.24
 ---- batch: 020 ----
mean loss: 886.62
 ---- batch: 030 ----
mean loss: 900.10
 ---- batch: 040 ----
mean loss: 925.01
train mean loss: 896.42
epoch train time: 0:00:08.653830
elapsed time: 0:03:07.089110
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 10:28:26.385678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.81
 ---- batch: 020 ----
mean loss: 894.88
 ---- batch: 030 ----
mean loss: 886.99
 ---- batch: 040 ----
mean loss: 898.84
train mean loss: 890.21
epoch train time: 0:00:08.496783
elapsed time: 0:03:15.587210
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 10:28:34.883806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.06
 ---- batch: 020 ----
mean loss: 881.34
 ---- batch: 030 ----
mean loss: 877.03
 ---- batch: 040 ----
mean loss: 903.41
train mean loss: 888.61
epoch train time: 0:00:08.475486
elapsed time: 0:03:24.064138
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 10:28:43.360609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.17
 ---- batch: 020 ----
mean loss: 878.98
 ---- batch: 030 ----
mean loss: 880.65
 ---- batch: 040 ----
mean loss: 890.61
train mean loss: 881.71
epoch train time: 0:00:08.518641
elapsed time: 0:03:32.583994
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 10:28:51.880543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.12
 ---- batch: 020 ----
mean loss: 881.58
 ---- batch: 030 ----
mean loss: 856.35
 ---- batch: 040 ----
mean loss: 884.41
train mean loss: 879.06
epoch train time: 0:00:08.724365
elapsed time: 0:03:41.309665
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 10:29:00.606211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.67
 ---- batch: 020 ----
mean loss: 860.68
 ---- batch: 030 ----
mean loss: 863.82
 ---- batch: 040 ----
mean loss: 883.42
train mean loss: 871.67
epoch train time: 0:00:08.723221
elapsed time: 0:03:50.034191
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 10:29:09.330737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.67
 ---- batch: 020 ----
mean loss: 874.21
 ---- batch: 030 ----
mean loss: 857.52
 ---- batch: 040 ----
mean loss: 863.79
train mean loss: 861.35
epoch train time: 0:00:08.674840
elapsed time: 0:03:58.710519
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 10:29:18.007080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.94
 ---- batch: 020 ----
mean loss: 832.83
 ---- batch: 030 ----
mean loss: 833.49
 ---- batch: 040 ----
mean loss: 814.29
train mean loss: 827.29
epoch train time: 0:00:08.660462
elapsed time: 0:04:07.372331
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 10:29:26.668854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 784.18
 ---- batch: 020 ----
mean loss: 752.10
 ---- batch: 030 ----
mean loss: 727.55
 ---- batch: 040 ----
mean loss: 711.86
train mean loss: 740.27
epoch train time: 0:00:08.715752
elapsed time: 0:04:16.089397
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 10:29:35.385930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.38
 ---- batch: 020 ----
mean loss: 703.52
 ---- batch: 030 ----
mean loss: 678.00
 ---- batch: 040 ----
mean loss: 676.31
train mean loss: 688.84
epoch train time: 0:00:08.634194
elapsed time: 0:04:24.724948
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 10:29:44.021499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.00
 ---- batch: 020 ----
mean loss: 660.00
 ---- batch: 030 ----
mean loss: 665.54
 ---- batch: 040 ----
mean loss: 640.00
train mean loss: 656.85
epoch train time: 0:00:08.587627
elapsed time: 0:04:33.313975
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 10:29:52.610553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 624.67
 ---- batch: 020 ----
mean loss: 638.48
 ---- batch: 030 ----
mean loss: 641.09
 ---- batch: 040 ----
mean loss: 628.40
train mean loss: 634.33
epoch train time: 0:00:08.607141
elapsed time: 0:04:41.922441
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 10:30:01.218987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 623.00
 ---- batch: 020 ----
mean loss: 601.98
 ---- batch: 030 ----
mean loss: 606.60
 ---- batch: 040 ----
mean loss: 606.20
train mean loss: 608.15
epoch train time: 0:00:08.640194
elapsed time: 0:04:50.563946
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 10:30:09.860501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 591.83
 ---- batch: 020 ----
mean loss: 576.76
 ---- batch: 030 ----
mean loss: 582.82
 ---- batch: 040 ----
mean loss: 580.93
train mean loss: 582.42
epoch train time: 0:00:08.640400
elapsed time: 0:04:59.205713
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 10:30:18.502286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.55
 ---- batch: 020 ----
mean loss: 580.93
 ---- batch: 030 ----
mean loss: 552.58
 ---- batch: 040 ----
mean loss: 548.67
train mean loss: 558.77
epoch train time: 0:00:08.588628
elapsed time: 0:05:07.795683
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 10:30:27.092239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 554.57
 ---- batch: 020 ----
mean loss: 541.88
 ---- batch: 030 ----
mean loss: 524.45
 ---- batch: 040 ----
mean loss: 537.99
train mean loss: 538.37
epoch train time: 0:00:08.519517
elapsed time: 0:05:16.316497
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 10:30:35.613034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.64
 ---- batch: 020 ----
mean loss: 522.77
 ---- batch: 030 ----
mean loss: 521.09
 ---- batch: 040 ----
mean loss: 510.74
train mean loss: 514.97
epoch train time: 0:00:08.552348
elapsed time: 0:05:24.870176
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 10:30:44.166708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.90
 ---- batch: 020 ----
mean loss: 494.07
 ---- batch: 030 ----
mean loss: 496.44
 ---- batch: 040 ----
mean loss: 499.82
train mean loss: 497.17
epoch train time: 0:00:08.512871
elapsed time: 0:05:33.384255
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 10:30:52.680786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.18
 ---- batch: 020 ----
mean loss: 479.81
 ---- batch: 030 ----
mean loss: 477.79
 ---- batch: 040 ----
mean loss: 473.71
train mean loss: 479.43
epoch train time: 0:00:08.610960
elapsed time: 0:05:41.996585
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 10:31:01.293135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.03
 ---- batch: 020 ----
mean loss: 455.35
 ---- batch: 030 ----
mean loss: 466.70
 ---- batch: 040 ----
mean loss: 467.35
train mean loss: 467.11
epoch train time: 0:00:08.604646
elapsed time: 0:05:50.602629
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 10:31:09.899183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.55
 ---- batch: 020 ----
mean loss: 457.24
 ---- batch: 030 ----
mean loss: 459.03
 ---- batch: 040 ----
mean loss: 460.93
train mean loss: 458.59
epoch train time: 0:00:08.477115
elapsed time: 0:05:59.081030
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 10:31:18.377568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.84
 ---- batch: 020 ----
mean loss: 440.75
 ---- batch: 030 ----
mean loss: 435.53
 ---- batch: 040 ----
mean loss: 442.95
train mean loss: 442.41
epoch train time: 0:00:08.640220
elapsed time: 0:06:07.722529
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 10:31:27.019071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.83
 ---- batch: 020 ----
mean loss: 438.86
 ---- batch: 030 ----
mean loss: 440.60
 ---- batch: 040 ----
mean loss: 426.65
train mean loss: 435.25
epoch train time: 0:00:08.638101
elapsed time: 0:06:16.361972
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 10:31:35.658526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.66
 ---- batch: 020 ----
mean loss: 418.75
 ---- batch: 030 ----
mean loss: 435.44
 ---- batch: 040 ----
mean loss: 437.49
train mean loss: 429.32
epoch train time: 0:00:08.511378
elapsed time: 0:06:24.874707
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 10:31:44.171270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.54
 ---- batch: 020 ----
mean loss: 417.00
 ---- batch: 030 ----
mean loss: 403.98
 ---- batch: 040 ----
mean loss: 425.69
train mean loss: 415.85
epoch train time: 0:00:08.526390
elapsed time: 0:06:33.402583
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 10:31:52.699029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.43
 ---- batch: 020 ----
mean loss: 421.65
 ---- batch: 030 ----
mean loss: 414.04
 ---- batch: 040 ----
mean loss: 417.36
train mean loss: 416.29
epoch train time: 0:00:08.552255
elapsed time: 0:06:41.956004
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 10:32:01.252561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.03
 ---- batch: 020 ----
mean loss: 413.36
 ---- batch: 030 ----
mean loss: 409.48
 ---- batch: 040 ----
mean loss: 394.87
train mean loss: 406.06
epoch train time: 0:00:08.509549
elapsed time: 0:06:50.466885
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 10:32:09.763434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.35
 ---- batch: 020 ----
mean loss: 391.71
 ---- batch: 030 ----
mean loss: 400.10
 ---- batch: 040 ----
mean loss: 396.19
train mean loss: 395.95
epoch train time: 0:00:08.551960
elapsed time: 0:06:59.020200
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 10:32:18.316735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.65
 ---- batch: 020 ----
mean loss: 390.02
 ---- batch: 030 ----
mean loss: 395.22
 ---- batch: 040 ----
mean loss: 391.68
train mean loss: 390.31
epoch train time: 0:00:08.601833
elapsed time: 0:07:07.623384
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 10:32:26.919994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.87
 ---- batch: 020 ----
mean loss: 390.60
 ---- batch: 030 ----
mean loss: 381.30
 ---- batch: 040 ----
mean loss: 384.22
train mean loss: 386.40
epoch train time: 0:00:08.603248
elapsed time: 0:07:16.228041
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 10:32:35.524597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.14
 ---- batch: 020 ----
mean loss: 384.73
 ---- batch: 030 ----
mean loss: 392.50
 ---- batch: 040 ----
mean loss: 377.75
train mean loss: 379.40
epoch train time: 0:00:08.610315
elapsed time: 0:07:24.839719
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 10:32:44.136247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.98
 ---- batch: 020 ----
mean loss: 373.83
 ---- batch: 030 ----
mean loss: 385.68
 ---- batch: 040 ----
mean loss: 367.11
train mean loss: 376.08
epoch train time: 0:00:08.627075
elapsed time: 0:07:33.468118
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 10:32:52.764660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.37
 ---- batch: 020 ----
mean loss: 377.19
 ---- batch: 030 ----
mean loss: 367.59
 ---- batch: 040 ----
mean loss: 382.01
train mean loss: 372.67
epoch train time: 0:00:08.578200
elapsed time: 0:07:42.047615
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 10:33:01.344145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.80
 ---- batch: 020 ----
mean loss: 371.96
 ---- batch: 030 ----
mean loss: 367.89
 ---- batch: 040 ----
mean loss: 372.53
train mean loss: 368.09
epoch train time: 0:00:08.576878
elapsed time: 0:07:50.625810
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 10:33:09.922356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.70
 ---- batch: 020 ----
mean loss: 360.01
 ---- batch: 030 ----
mean loss: 364.81
 ---- batch: 040 ----
mean loss: 369.32
train mean loss: 364.46
epoch train time: 0:00:08.661207
elapsed time: 0:07:59.288325
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 10:33:18.584868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.84
 ---- batch: 020 ----
mean loss: 363.11
 ---- batch: 030 ----
mean loss: 348.42
 ---- batch: 040 ----
mean loss: 357.12
train mean loss: 357.45
epoch train time: 0:00:08.632067
elapsed time: 0:08:07.921689
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 10:33:27.218224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.33
 ---- batch: 020 ----
mean loss: 346.70
 ---- batch: 030 ----
mean loss: 352.31
 ---- batch: 040 ----
mean loss: 366.86
train mean loss: 355.98
epoch train time: 0:00:08.572614
elapsed time: 0:08:16.495697
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 10:33:35.792242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.78
 ---- batch: 020 ----
mean loss: 355.24
 ---- batch: 030 ----
mean loss: 345.45
 ---- batch: 040 ----
mean loss: 349.31
train mean loss: 349.13
epoch train time: 0:00:08.653947
elapsed time: 0:08:25.150928
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 10:33:44.447492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.90
 ---- batch: 020 ----
mean loss: 344.05
 ---- batch: 030 ----
mean loss: 345.08
 ---- batch: 040 ----
mean loss: 346.79
train mean loss: 347.89
epoch train time: 0:00:08.616638
elapsed time: 0:08:33.768908
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 10:33:53.065448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.43
 ---- batch: 020 ----
mean loss: 339.84
 ---- batch: 030 ----
mean loss: 348.13
 ---- batch: 040 ----
mean loss: 342.20
train mean loss: 346.23
epoch train time: 0:00:08.633772
elapsed time: 0:08:42.404009
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 10:34:01.700560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.35
 ---- batch: 020 ----
mean loss: 338.55
 ---- batch: 030 ----
mean loss: 340.35
 ---- batch: 040 ----
mean loss: 333.75
train mean loss: 337.52
epoch train time: 0:00:08.693285
elapsed time: 0:08:51.098708
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 10:34:10.395366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.85
 ---- batch: 020 ----
mean loss: 345.06
 ---- batch: 030 ----
mean loss: 325.86
 ---- batch: 040 ----
mean loss: 338.84
train mean loss: 333.90
epoch train time: 0:00:08.708989
elapsed time: 0:08:59.809275
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 10:34:19.105921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.98
 ---- batch: 020 ----
mean loss: 329.99
 ---- batch: 030 ----
mean loss: 342.12
 ---- batch: 040 ----
mean loss: 325.66
train mean loss: 332.78
epoch train time: 0:00:08.740441
elapsed time: 0:09:08.551181
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 10:34:27.847754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.61
 ---- batch: 020 ----
mean loss: 328.62
 ---- batch: 030 ----
mean loss: 333.36
 ---- batch: 040 ----
mean loss: 322.28
train mean loss: 328.16
epoch train time: 0:00:08.655607
elapsed time: 0:09:17.208269
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 10:34:36.504939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.42
 ---- batch: 020 ----
mean loss: 320.21
 ---- batch: 030 ----
mean loss: 336.70
 ---- batch: 040 ----
mean loss: 337.76
train mean loss: 327.87
epoch train time: 0:00:08.648689
elapsed time: 0:09:25.858453
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 10:34:45.155040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.25
 ---- batch: 020 ----
mean loss: 322.16
 ---- batch: 030 ----
mean loss: 315.93
 ---- batch: 040 ----
mean loss: 310.55
train mean loss: 323.41
epoch train time: 0:00:08.649142
elapsed time: 0:09:34.508898
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 10:34:53.805435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.76
 ---- batch: 020 ----
mean loss: 319.90
 ---- batch: 030 ----
mean loss: 317.43
 ---- batch: 040 ----
mean loss: 321.78
train mean loss: 319.79
epoch train time: 0:00:08.606608
elapsed time: 0:09:43.116773
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 10:35:02.413315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.53
 ---- batch: 020 ----
mean loss: 322.36
 ---- batch: 030 ----
mean loss: 305.87
 ---- batch: 040 ----
mean loss: 324.62
train mean loss: 318.13
epoch train time: 0:00:08.645413
elapsed time: 0:09:51.763503
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 10:35:11.060043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.21
 ---- batch: 020 ----
mean loss: 314.31
 ---- batch: 030 ----
mean loss: 322.98
 ---- batch: 040 ----
mean loss: 316.37
train mean loss: 314.33
epoch train time: 0:00:08.676765
elapsed time: 0:10:00.441554
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 10:35:19.738131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.13
 ---- batch: 020 ----
mean loss: 314.22
 ---- batch: 030 ----
mean loss: 311.96
 ---- batch: 040 ----
mean loss: 311.20
train mean loss: 310.19
epoch train time: 0:00:08.638306
elapsed time: 0:10:09.081179
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 10:35:28.377773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.20
 ---- batch: 020 ----
mean loss: 317.33
 ---- batch: 030 ----
mean loss: 318.64
 ---- batch: 040 ----
mean loss: 323.34
train mean loss: 313.26
epoch train time: 0:00:08.750239
elapsed time: 0:10:17.833033
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 10:35:37.129638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.75
 ---- batch: 020 ----
mean loss: 311.31
 ---- batch: 030 ----
mean loss: 299.54
 ---- batch: 040 ----
mean loss: 311.40
train mean loss: 308.51
epoch train time: 0:00:08.694990
elapsed time: 0:10:26.529368
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 10:35:45.825919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.69
 ---- batch: 020 ----
mean loss: 298.05
 ---- batch: 030 ----
mean loss: 313.43
 ---- batch: 040 ----
mean loss: 306.36
train mean loss: 303.01
epoch train time: 0:00:08.675334
elapsed time: 0:10:35.206132
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 10:35:54.502697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.79
 ---- batch: 020 ----
mean loss: 300.47
 ---- batch: 030 ----
mean loss: 302.91
 ---- batch: 040 ----
mean loss: 293.95
train mean loss: 301.81
epoch train time: 0:00:08.672504
elapsed time: 0:10:43.879977
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 10:36:03.176539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.43
 ---- batch: 020 ----
mean loss: 300.52
 ---- batch: 030 ----
mean loss: 301.59
 ---- batch: 040 ----
mean loss: 294.41
train mean loss: 299.30
epoch train time: 0:00:08.611412
elapsed time: 0:10:52.492767
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 10:36:11.789307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.28
 ---- batch: 020 ----
mean loss: 296.92
 ---- batch: 030 ----
mean loss: 295.01
 ---- batch: 040 ----
mean loss: 300.28
train mean loss: 296.51
epoch train time: 0:00:08.622899
elapsed time: 0:11:01.116938
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 10:36:20.413461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.79
 ---- batch: 020 ----
mean loss: 293.66
 ---- batch: 030 ----
mean loss: 296.91
 ---- batch: 040 ----
mean loss: 288.31
train mean loss: 294.14
epoch train time: 0:00:08.564457
elapsed time: 0:11:09.682858
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 10:36:28.979450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.12
 ---- batch: 020 ----
mean loss: 288.43
 ---- batch: 030 ----
mean loss: 297.50
 ---- batch: 040 ----
mean loss: 295.20
train mean loss: 295.94
epoch train time: 0:00:08.578334
elapsed time: 0:11:18.262600
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 10:36:37.559143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.41
 ---- batch: 020 ----
mean loss: 299.70
 ---- batch: 030 ----
mean loss: 302.04
 ---- batch: 040 ----
mean loss: 287.61
train mean loss: 294.93
epoch train time: 0:00:08.617013
elapsed time: 0:11:26.880851
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 10:36:46.177395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.22
 ---- batch: 020 ----
mean loss: 291.82
 ---- batch: 030 ----
mean loss: 295.69
 ---- batch: 040 ----
mean loss: 280.28
train mean loss: 288.96
epoch train time: 0:00:08.606597
elapsed time: 0:11:35.488713
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 10:36:54.785243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.37
 ---- batch: 020 ----
mean loss: 282.11
 ---- batch: 030 ----
mean loss: 299.93
 ---- batch: 040 ----
mean loss: 291.39
train mean loss: 290.82
epoch train time: 0:00:08.562512
elapsed time: 0:11:44.052531
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 10:37:03.349098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.55
 ---- batch: 020 ----
mean loss: 283.18
 ---- batch: 030 ----
mean loss: 287.29
 ---- batch: 040 ----
mean loss: 277.91
train mean loss: 286.78
epoch train time: 0:00:08.586896
elapsed time: 0:11:52.640843
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 10:37:11.937415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.06
 ---- batch: 020 ----
mean loss: 283.81
 ---- batch: 030 ----
mean loss: 276.69
 ---- batch: 040 ----
mean loss: 292.71
train mean loss: 284.44
epoch train time: 0:00:08.556042
elapsed time: 0:12:01.198240
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 10:37:20.494799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.78
 ---- batch: 020 ----
mean loss: 293.18
 ---- batch: 030 ----
mean loss: 286.76
 ---- batch: 040 ----
mean loss: 282.95
train mean loss: 286.72
epoch train time: 0:00:08.612586
elapsed time: 0:12:09.812128
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 10:37:29.108688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.53
 ---- batch: 020 ----
mean loss: 287.58
 ---- batch: 030 ----
mean loss: 300.82
 ---- batch: 040 ----
mean loss: 277.22
train mean loss: 288.76
epoch train time: 0:00:08.550886
elapsed time: 0:12:18.364325
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 10:37:37.660886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.78
 ---- batch: 020 ----
mean loss: 279.83
 ---- batch: 030 ----
mean loss: 282.25
 ---- batch: 040 ----
mean loss: 281.82
train mean loss: 279.78
epoch train time: 0:00:08.562028
elapsed time: 0:12:26.927623
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 10:37:46.224174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.84
 ---- batch: 020 ----
mean loss: 280.71
 ---- batch: 030 ----
mean loss: 288.50
 ---- batch: 040 ----
mean loss: 280.57
train mean loss: 281.45
epoch train time: 0:00:08.560195
elapsed time: 0:12:35.489194
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 10:37:54.785715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.77
 ---- batch: 020 ----
mean loss: 282.12
 ---- batch: 030 ----
mean loss: 285.64
 ---- batch: 040 ----
mean loss: 272.58
train mean loss: 279.81
epoch train time: 0:00:08.606475
elapsed time: 0:12:44.096922
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 10:38:03.393484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.42
 ---- batch: 020 ----
mean loss: 281.45
 ---- batch: 030 ----
mean loss: 270.61
 ---- batch: 040 ----
mean loss: 269.55
train mean loss: 275.75
epoch train time: 0:00:08.515040
elapsed time: 0:12:52.613270
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 10:38:11.909838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.56
 ---- batch: 020 ----
mean loss: 282.24
 ---- batch: 030 ----
mean loss: 277.26
 ---- batch: 040 ----
mean loss: 269.51
train mean loss: 277.76
epoch train time: 0:00:08.459949
elapsed time: 0:13:01.074587
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 10:38:20.371137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.77
 ---- batch: 020 ----
mean loss: 269.43
 ---- batch: 030 ----
mean loss: 275.66
 ---- batch: 040 ----
mean loss: 274.65
train mean loss: 275.74
epoch train time: 0:00:08.476002
elapsed time: 0:13:09.551890
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 10:38:28.848438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.69
 ---- batch: 020 ----
mean loss: 274.95
 ---- batch: 030 ----
mean loss: 265.10
 ---- batch: 040 ----
mean loss: 277.00
train mean loss: 274.07
epoch train time: 0:00:08.486310
elapsed time: 0:13:18.039499
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 10:38:37.336165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.08
 ---- batch: 020 ----
mean loss: 277.19
 ---- batch: 030 ----
mean loss: 273.95
 ---- batch: 040 ----
mean loss: 274.57
train mean loss: 272.19
epoch train time: 0:00:08.540223
elapsed time: 0:13:26.581149
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 10:38:45.877737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.41
 ---- batch: 020 ----
mean loss: 268.18
 ---- batch: 030 ----
mean loss: 281.06
 ---- batch: 040 ----
mean loss: 274.28
train mean loss: 273.89
epoch train time: 0:00:08.504080
elapsed time: 0:13:35.086576
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 10:38:54.383170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.77
 ---- batch: 020 ----
mean loss: 275.66
 ---- batch: 030 ----
mean loss: 269.71
 ---- batch: 040 ----
mean loss: 267.79
train mean loss: 269.89
epoch train time: 0:00:08.506454
elapsed time: 0:13:43.594378
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 10:39:02.890935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.14
 ---- batch: 020 ----
mean loss: 274.43
 ---- batch: 030 ----
mean loss: 259.93
 ---- batch: 040 ----
mean loss: 262.99
train mean loss: 267.11
epoch train time: 0:00:08.545149
elapsed time: 0:13:52.140779
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 10:39:11.437319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.81
 ---- batch: 020 ----
mean loss: 259.21
 ---- batch: 030 ----
mean loss: 265.64
 ---- batch: 040 ----
mean loss: 262.01
train mean loss: 267.44
epoch train time: 0:00:08.494243
elapsed time: 0:14:00.636325
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 10:39:19.932854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.54
 ---- batch: 020 ----
mean loss: 266.84
 ---- batch: 030 ----
mean loss: 264.44
 ---- batch: 040 ----
mean loss: 262.69
train mean loss: 267.07
epoch train time: 0:00:08.514311
elapsed time: 0:14:09.152072
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 10:39:28.448665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.04
 ---- batch: 020 ----
mean loss: 269.85
 ---- batch: 030 ----
mean loss: 263.00
 ---- batch: 040 ----
mean loss: 267.43
train mean loss: 266.76
epoch train time: 0:00:08.444725
elapsed time: 0:14:17.598179
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 10:39:36.894708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.73
 ---- batch: 020 ----
mean loss: 264.44
 ---- batch: 030 ----
mean loss: 265.46
 ---- batch: 040 ----
mean loss: 260.78
train mean loss: 264.03
epoch train time: 0:00:08.416588
elapsed time: 0:14:26.016046
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 10:39:45.312602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.11
 ---- batch: 020 ----
mean loss: 264.81
 ---- batch: 030 ----
mean loss: 276.15
 ---- batch: 040 ----
mean loss: 269.61
train mean loss: 264.94
epoch train time: 0:00:08.427658
elapsed time: 0:14:34.445001
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 10:39:53.741565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.02
 ---- batch: 020 ----
mean loss: 263.13
 ---- batch: 030 ----
mean loss: 257.38
 ---- batch: 040 ----
mean loss: 266.53
train mean loss: 263.14
epoch train time: 0:00:08.394303
elapsed time: 0:14:42.840552
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 10:40:02.137087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.59
 ---- batch: 020 ----
mean loss: 274.41
 ---- batch: 030 ----
mean loss: 259.28
 ---- batch: 040 ----
mean loss: 253.11
train mean loss: 261.84
epoch train time: 0:00:08.528951
elapsed time: 0:14:51.370776
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 10:40:10.667299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.79
 ---- batch: 020 ----
mean loss: 265.21
 ---- batch: 030 ----
mean loss: 264.76
 ---- batch: 040 ----
mean loss: 263.46
train mean loss: 263.47
epoch train time: 0:00:08.398025
elapsed time: 0:14:59.770174
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 10:40:19.066764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.42
 ---- batch: 020 ----
mean loss: 260.65
 ---- batch: 030 ----
mean loss: 256.90
 ---- batch: 040 ----
mean loss: 260.05
train mean loss: 259.30
epoch train time: 0:00:08.424266
elapsed time: 0:15:08.195751
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 10:40:27.492269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.12
 ---- batch: 020 ----
mean loss: 264.21
 ---- batch: 030 ----
mean loss: 267.96
 ---- batch: 040 ----
mean loss: 262.07
train mean loss: 260.12
epoch train time: 0:00:08.384977
elapsed time: 0:15:16.581957
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 10:40:35.878500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.71
 ---- batch: 020 ----
mean loss: 265.24
 ---- batch: 030 ----
mean loss: 264.00
 ---- batch: 040 ----
mean loss: 260.10
train mean loss: 262.09
epoch train time: 0:00:08.459917
elapsed time: 0:15:25.043139
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 10:40:44.339677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.38
 ---- batch: 020 ----
mean loss: 263.51
 ---- batch: 030 ----
mean loss: 258.65
 ---- batch: 040 ----
mean loss: 253.03
train mean loss: 258.45
epoch train time: 0:00:08.434398
elapsed time: 0:15:33.479018
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 10:40:52.775381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.32
 ---- batch: 020 ----
mean loss: 252.18
 ---- batch: 030 ----
mean loss: 261.94
 ---- batch: 040 ----
mean loss: 251.68
train mean loss: 256.27
epoch train time: 0:00:08.415504
elapsed time: 0:15:41.895626
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 10:41:01.192175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.52
 ---- batch: 020 ----
mean loss: 251.67
 ---- batch: 030 ----
mean loss: 255.59
 ---- batch: 040 ----
mean loss: 255.98
train mean loss: 254.75
epoch train time: 0:00:08.413458
elapsed time: 0:15:50.310351
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 10:41:09.606894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.06
 ---- batch: 020 ----
mean loss: 252.40
 ---- batch: 030 ----
mean loss: 246.74
 ---- batch: 040 ----
mean loss: 253.54
train mean loss: 254.46
epoch train time: 0:00:08.413607
elapsed time: 0:15:58.725217
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 10:41:18.021766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.79
 ---- batch: 020 ----
mean loss: 254.32
 ---- batch: 030 ----
mean loss: 258.13
 ---- batch: 040 ----
mean loss: 252.66
train mean loss: 254.07
epoch train time: 0:00:08.442674
elapsed time: 0:16:07.169160
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 10:41:26.465707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.13
 ---- batch: 020 ----
mean loss: 251.74
 ---- batch: 030 ----
mean loss: 257.22
 ---- batch: 040 ----
mean loss: 249.73
train mean loss: 253.58
epoch train time: 0:00:08.441221
elapsed time: 0:16:15.611660
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 10:41:34.908203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.61
 ---- batch: 020 ----
mean loss: 256.41
 ---- batch: 030 ----
mean loss: 255.52
 ---- batch: 040 ----
mean loss: 253.54
train mean loss: 255.49
epoch train time: 0:00:08.412286
elapsed time: 0:16:24.025248
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 10:41:43.321817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.75
 ---- batch: 020 ----
mean loss: 248.12
 ---- batch: 030 ----
mean loss: 250.78
 ---- batch: 040 ----
mean loss: 259.90
train mean loss: 252.97
epoch train time: 0:00:08.441031
elapsed time: 0:16:32.467576
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 10:41:51.764107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.15
 ---- batch: 020 ----
mean loss: 255.59
 ---- batch: 030 ----
mean loss: 248.48
 ---- batch: 040 ----
mean loss: 247.69
train mean loss: 250.01
epoch train time: 0:00:08.418173
elapsed time: 0:16:40.887007
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 10:42:00.183552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.96
 ---- batch: 020 ----
mean loss: 259.00
 ---- batch: 030 ----
mean loss: 260.47
 ---- batch: 040 ----
mean loss: 255.17
train mean loss: 257.21
epoch train time: 0:00:08.414998
elapsed time: 0:16:49.303270
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 10:42:08.599815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.41
 ---- batch: 020 ----
mean loss: 246.08
 ---- batch: 030 ----
mean loss: 246.70
 ---- batch: 040 ----
mean loss: 256.98
train mean loss: 251.89
epoch train time: 0:00:08.419402
elapsed time: 0:16:57.724000
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 10:42:17.020567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.07
 ---- batch: 020 ----
mean loss: 244.03
 ---- batch: 030 ----
mean loss: 249.17
 ---- batch: 040 ----
mean loss: 254.15
train mean loss: 250.94
epoch train time: 0:00:08.380588
elapsed time: 0:17:06.105953
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 10:42:25.402492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.73
 ---- batch: 020 ----
mean loss: 254.66
 ---- batch: 030 ----
mean loss: 249.10
 ---- batch: 040 ----
mean loss: 238.01
train mean loss: 247.27
epoch train time: 0:00:08.380381
elapsed time: 0:17:14.487688
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 10:42:33.784251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.56
 ---- batch: 020 ----
mean loss: 248.98
 ---- batch: 030 ----
mean loss: 249.29
 ---- batch: 040 ----
mean loss: 256.67
train mean loss: 249.27
epoch train time: 0:00:08.428009
elapsed time: 0:17:22.917015
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 10:42:42.213545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.30
 ---- batch: 020 ----
mean loss: 243.78
 ---- batch: 030 ----
mean loss: 253.03
 ---- batch: 040 ----
mean loss: 253.61
train mean loss: 249.54
epoch train time: 0:00:08.417233
elapsed time: 0:17:31.335625
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 10:42:50.632180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.65
 ---- batch: 020 ----
mean loss: 240.68
 ---- batch: 030 ----
mean loss: 249.82
 ---- batch: 040 ----
mean loss: 244.92
train mean loss: 246.37
epoch train time: 0:00:08.416151
elapsed time: 0:17:39.753074
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 10:42:59.049625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.66
 ---- batch: 020 ----
mean loss: 243.90
 ---- batch: 030 ----
mean loss: 247.86
 ---- batch: 040 ----
mean loss: 238.82
train mean loss: 245.24
epoch train time: 0:00:08.442781
elapsed time: 0:17:48.197149
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 10:43:07.493701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.39
 ---- batch: 020 ----
mean loss: 241.98
 ---- batch: 030 ----
mean loss: 246.40
 ---- batch: 040 ----
mean loss: 248.49
train mean loss: 245.05
epoch train time: 0:00:08.516668
elapsed time: 0:17:56.715109
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 10:43:16.011687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.09
 ---- batch: 020 ----
mean loss: 244.69
 ---- batch: 030 ----
mean loss: 240.05
 ---- batch: 040 ----
mean loss: 252.22
train mean loss: 244.72
epoch train time: 0:00:08.440255
elapsed time: 0:18:05.156727
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 10:43:24.453261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.42
 ---- batch: 020 ----
mean loss: 237.52
 ---- batch: 030 ----
mean loss: 240.23
 ---- batch: 040 ----
mean loss: 247.40
train mean loss: 242.85
epoch train time: 0:00:08.579941
elapsed time: 0:18:13.737952
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 10:43:33.034510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.19
 ---- batch: 020 ----
mean loss: 246.74
 ---- batch: 030 ----
mean loss: 251.71
 ---- batch: 040 ----
mean loss: 233.61
train mean loss: 243.84
epoch train time: 0:00:08.584108
elapsed time: 0:18:22.323664
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 10:43:41.619992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.25
 ---- batch: 020 ----
mean loss: 257.93
 ---- batch: 030 ----
mean loss: 239.83
 ---- batch: 040 ----
mean loss: 247.35
train mean loss: 247.14
epoch train time: 0:00:08.619665
elapsed time: 0:18:30.944569
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 10:43:50.241025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.45
 ---- batch: 020 ----
mean loss: 242.46
 ---- batch: 030 ----
mean loss: 242.11
 ---- batch: 040 ----
mean loss: 245.40
train mean loss: 243.58
epoch train time: 0:00:08.578149
elapsed time: 0:18:39.523902
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 10:43:58.820483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.68
 ---- batch: 020 ----
mean loss: 242.66
 ---- batch: 030 ----
mean loss: 239.95
 ---- batch: 040 ----
mean loss: 240.33
train mean loss: 242.15
epoch train time: 0:00:08.579396
elapsed time: 0:18:48.104799
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 10:44:07.401334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.72
 ---- batch: 020 ----
mean loss: 244.16
 ---- batch: 030 ----
mean loss: 240.96
 ---- batch: 040 ----
mean loss: 239.68
train mean loss: 242.71
epoch train time: 0:00:08.596216
elapsed time: 0:18:56.702256
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 10:44:15.998814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.12
 ---- batch: 020 ----
mean loss: 242.25
 ---- batch: 030 ----
mean loss: 246.57
 ---- batch: 040 ----
mean loss: 235.37
train mean loss: 242.00
epoch train time: 0:00:08.608606
elapsed time: 0:19:05.312163
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 10:44:24.608719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.99
 ---- batch: 020 ----
mean loss: 244.30
 ---- batch: 030 ----
mean loss: 237.96
 ---- batch: 040 ----
mean loss: 245.51
train mean loss: 239.24
epoch train time: 0:00:08.561919
elapsed time: 0:19:13.875375
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 10:44:33.171918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.38
 ---- batch: 020 ----
mean loss: 235.25
 ---- batch: 030 ----
mean loss: 236.79
 ---- batch: 040 ----
mean loss: 239.04
train mean loss: 238.71
epoch train time: 0:00:08.609644
elapsed time: 0:19:22.486338
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 10:44:41.782863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.17
 ---- batch: 020 ----
mean loss: 240.55
 ---- batch: 030 ----
mean loss: 237.25
 ---- batch: 040 ----
mean loss: 240.77
train mean loss: 239.86
epoch train time: 0:00:08.596573
elapsed time: 0:19:31.084344
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 10:44:50.380919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.79
 ---- batch: 020 ----
mean loss: 236.07
 ---- batch: 030 ----
mean loss: 245.43
 ---- batch: 040 ----
mean loss: 239.35
train mean loss: 237.74
epoch train time: 0:00:08.519104
elapsed time: 0:19:39.604861
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 10:44:58.901466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.27
 ---- batch: 020 ----
mean loss: 238.61
 ---- batch: 030 ----
mean loss: 231.96
 ---- batch: 040 ----
mean loss: 234.63
train mean loss: 237.23
epoch train time: 0:00:08.567980
elapsed time: 0:19:48.174208
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 10:45:07.470760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.96
 ---- batch: 020 ----
mean loss: 248.33
 ---- batch: 030 ----
mean loss: 242.44
 ---- batch: 040 ----
mean loss: 235.82
train mean loss: 240.29
epoch train time: 0:00:08.563480
elapsed time: 0:19:56.738946
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 10:45:16.035491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.50
 ---- batch: 020 ----
mean loss: 238.78
 ---- batch: 030 ----
mean loss: 244.32
 ---- batch: 040 ----
mean loss: 237.13
train mean loss: 238.74
epoch train time: 0:00:08.562867
elapsed time: 0:20:05.303109
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 10:45:24.599644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.06
 ---- batch: 020 ----
mean loss: 237.96
 ---- batch: 030 ----
mean loss: 232.98
 ---- batch: 040 ----
mean loss: 243.35
train mean loss: 236.72
epoch train time: 0:00:08.580041
elapsed time: 0:20:13.884445
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 10:45:33.180989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.48
 ---- batch: 020 ----
mean loss: 234.72
 ---- batch: 030 ----
mean loss: 231.05
 ---- batch: 040 ----
mean loss: 242.04
train mean loss: 236.63
epoch train time: 0:00:08.633406
elapsed time: 0:20:22.519210
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 10:45:41.815766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.40
 ---- batch: 020 ----
mean loss: 231.39
 ---- batch: 030 ----
mean loss: 231.00
 ---- batch: 040 ----
mean loss: 239.12
train mean loss: 234.81
epoch train time: 0:00:08.607701
elapsed time: 0:20:31.128225
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 10:45:50.424765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.60
 ---- batch: 020 ----
mean loss: 238.70
 ---- batch: 030 ----
mean loss: 233.11
 ---- batch: 040 ----
mean loss: 237.56
train mean loss: 237.31
epoch train time: 0:00:08.641861
elapsed time: 0:20:39.771370
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 10:45:59.067916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.71
 ---- batch: 020 ----
mean loss: 228.38
 ---- batch: 030 ----
mean loss: 238.48
 ---- batch: 040 ----
mean loss: 235.86
train mean loss: 234.24
epoch train time: 0:00:08.563576
elapsed time: 0:20:48.336224
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 10:46:07.632775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.56
 ---- batch: 020 ----
mean loss: 232.72
 ---- batch: 030 ----
mean loss: 228.71
 ---- batch: 040 ----
mean loss: 243.31
train mean loss: 234.86
epoch train time: 0:00:08.567690
elapsed time: 0:20:56.905264
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 10:46:16.201816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.69
 ---- batch: 020 ----
mean loss: 226.23
 ---- batch: 030 ----
mean loss: 242.65
 ---- batch: 040 ----
mean loss: 228.73
train mean loss: 235.22
epoch train time: 0:00:08.543564
elapsed time: 0:21:05.450231
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 10:46:24.746852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.28
 ---- batch: 020 ----
mean loss: 238.04
 ---- batch: 030 ----
mean loss: 236.78
 ---- batch: 040 ----
mean loss: 226.94
train mean loss: 233.97
epoch train time: 0:00:08.587447
elapsed time: 0:21:14.039059
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 10:46:33.335600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.93
 ---- batch: 020 ----
mean loss: 234.01
 ---- batch: 030 ----
mean loss: 231.33
 ---- batch: 040 ----
mean loss: 240.69
train mean loss: 234.04
epoch train time: 0:00:08.479773
elapsed time: 0:21:22.520159
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 10:46:41.816751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.30
 ---- batch: 020 ----
mean loss: 225.70
 ---- batch: 030 ----
mean loss: 233.35
 ---- batch: 040 ----
mean loss: 233.11
train mean loss: 232.52
epoch train time: 0:00:08.517366
elapsed time: 0:21:31.039178
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 10:46:50.335481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.04
 ---- batch: 020 ----
mean loss: 233.15
 ---- batch: 030 ----
mean loss: 227.53
 ---- batch: 040 ----
mean loss: 234.84
train mean loss: 231.96
epoch train time: 0:00:08.497788
elapsed time: 0:21:39.538026
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 10:46:58.834703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.41
 ---- batch: 020 ----
mean loss: 237.66
 ---- batch: 030 ----
mean loss: 233.58
 ---- batch: 040 ----
mean loss: 228.78
train mean loss: 231.99
epoch train time: 0:00:08.486025
elapsed time: 0:21:48.025517
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 10:47:07.322074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.98
 ---- batch: 020 ----
mean loss: 232.09
 ---- batch: 030 ----
mean loss: 227.14
 ---- batch: 040 ----
mean loss: 229.10
train mean loss: 231.75
epoch train time: 0:00:08.520611
elapsed time: 0:21:56.547405
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 10:47:15.843938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.90
 ---- batch: 020 ----
mean loss: 232.53
 ---- batch: 030 ----
mean loss: 230.47
 ---- batch: 040 ----
mean loss: 234.53
train mean loss: 231.57
epoch train time: 0:00:08.497470
elapsed time: 0:22:05.046218
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 10:47:24.342818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.16
 ---- batch: 020 ----
mean loss: 233.63
 ---- batch: 030 ----
mean loss: 233.95
 ---- batch: 040 ----
mean loss: 225.07
train mean loss: 232.32
epoch train time: 0:00:08.525327
elapsed time: 0:22:13.572857
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 10:47:32.869393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.87
 ---- batch: 020 ----
mean loss: 240.30
 ---- batch: 030 ----
mean loss: 229.40
 ---- batch: 040 ----
mean loss: 222.55
train mean loss: 231.51
epoch train time: 0:00:08.549338
elapsed time: 0:22:22.123530
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 10:47:41.420122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.51
 ---- batch: 020 ----
mean loss: 227.16
 ---- batch: 030 ----
mean loss: 224.60
 ---- batch: 040 ----
mean loss: 233.19
train mean loss: 227.90
epoch train time: 0:00:08.577033
elapsed time: 0:22:30.701945
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 10:47:49.998480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.56
 ---- batch: 020 ----
mean loss: 226.99
 ---- batch: 030 ----
mean loss: 226.68
 ---- batch: 040 ----
mean loss: 222.67
train mean loss: 227.10
epoch train time: 0:00:08.557300
elapsed time: 0:22:39.260498
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 10:47:58.557089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.53
 ---- batch: 020 ----
mean loss: 223.52
 ---- batch: 030 ----
mean loss: 227.37
 ---- batch: 040 ----
mean loss: 232.52
train mean loss: 229.36
epoch train time: 0:00:08.570538
elapsed time: 0:22:47.832340
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 10:48:07.128885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.08
 ---- batch: 020 ----
mean loss: 222.77
 ---- batch: 030 ----
mean loss: 225.54
 ---- batch: 040 ----
mean loss: 232.97
train mean loss: 226.87
epoch train time: 0:00:08.564372
elapsed time: 0:22:56.398068
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 10:48:15.694619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.21
 ---- batch: 020 ----
mean loss: 228.13
 ---- batch: 030 ----
mean loss: 229.81
 ---- batch: 040 ----
mean loss: 217.92
train mean loss: 226.97
epoch train time: 0:00:08.473949
elapsed time: 0:23:04.873271
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 10:48:24.169855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.12
 ---- batch: 020 ----
mean loss: 227.86
 ---- batch: 030 ----
mean loss: 226.80
 ---- batch: 040 ----
mean loss: 224.94
train mean loss: 226.63
epoch train time: 0:00:08.400988
elapsed time: 0:23:13.275581
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 10:48:32.572171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.44
 ---- batch: 020 ----
mean loss: 226.16
 ---- batch: 030 ----
mean loss: 225.65
 ---- batch: 040 ----
mean loss: 227.76
train mean loss: 224.87
epoch train time: 0:00:08.404341
elapsed time: 0:23:21.681195
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 10:48:40.977741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.36
 ---- batch: 020 ----
mean loss: 217.02
 ---- batch: 030 ----
mean loss: 229.29
 ---- batch: 040 ----
mean loss: 228.53
train mean loss: 224.24
epoch train time: 0:00:08.439108
elapsed time: 0:23:30.121557
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 10:48:49.418107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.29
 ---- batch: 020 ----
mean loss: 223.99
 ---- batch: 030 ----
mean loss: 228.24
 ---- batch: 040 ----
mean loss: 229.53
train mean loss: 226.03
epoch train time: 0:00:08.392351
elapsed time: 0:23:38.515147
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 10:48:57.811683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.16
 ---- batch: 020 ----
mean loss: 229.52
 ---- batch: 030 ----
mean loss: 220.13
 ---- batch: 040 ----
mean loss: 228.91
train mean loss: 226.01
epoch train time: 0:00:08.418936
elapsed time: 0:23:46.935408
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 10:49:06.232033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.35
 ---- batch: 020 ----
mean loss: 221.33
 ---- batch: 030 ----
mean loss: 220.90
 ---- batch: 040 ----
mean loss: 230.78
train mean loss: 224.39
epoch train time: 0:00:08.391674
elapsed time: 0:23:55.328369
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 10:49:14.624968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.70
 ---- batch: 020 ----
mean loss: 226.70
 ---- batch: 030 ----
mean loss: 223.91
 ---- batch: 040 ----
mean loss: 219.90
train mean loss: 224.00
epoch train time: 0:00:08.393047
elapsed time: 0:24:03.722726
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 10:49:23.019319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.20
 ---- batch: 020 ----
mean loss: 224.82
 ---- batch: 030 ----
mean loss: 217.56
 ---- batch: 040 ----
mean loss: 224.70
train mean loss: 224.03
epoch train time: 0:00:08.375800
elapsed time: 0:24:12.099831
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 10:49:31.396361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.79
 ---- batch: 020 ----
mean loss: 228.58
 ---- batch: 030 ----
mean loss: 231.09
 ---- batch: 040 ----
mean loss: 219.13
train mean loss: 226.60
epoch train time: 0:00:08.344309
elapsed time: 0:24:20.445580
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 10:49:39.742198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.32
 ---- batch: 020 ----
mean loss: 214.37
 ---- batch: 030 ----
mean loss: 225.49
 ---- batch: 040 ----
mean loss: 227.80
train mean loss: 223.38
epoch train time: 0:00:08.359335
elapsed time: 0:24:28.806282
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 10:49:48.102830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.21
 ---- batch: 020 ----
mean loss: 222.71
 ---- batch: 030 ----
mean loss: 212.58
 ---- batch: 040 ----
mean loss: 231.20
train mean loss: 223.72
epoch train time: 0:00:08.364282
elapsed time: 0:24:37.171791
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 10:49:56.468348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.87
 ---- batch: 020 ----
mean loss: 220.77
 ---- batch: 030 ----
mean loss: 222.86
 ---- batch: 040 ----
mean loss: 229.16
train mean loss: 223.86
epoch train time: 0:00:08.376563
elapsed time: 0:24:45.549619
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 10:50:04.846145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.25
 ---- batch: 020 ----
mean loss: 218.06
 ---- batch: 030 ----
mean loss: 224.06
 ---- batch: 040 ----
mean loss: 223.96
train mean loss: 222.68
epoch train time: 0:00:08.341661
elapsed time: 0:24:53.892496
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 10:50:13.189038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.23
 ---- batch: 020 ----
mean loss: 224.13
 ---- batch: 030 ----
mean loss: 219.86
 ---- batch: 040 ----
mean loss: 218.80
train mean loss: 223.04
epoch train time: 0:00:08.355783
elapsed time: 0:25:02.249854
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 10:50:21.546140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.53
 ---- batch: 020 ----
mean loss: 226.44
 ---- batch: 030 ----
mean loss: 217.65
 ---- batch: 040 ----
mean loss: 227.11
train mean loss: 222.57
epoch train time: 0:00:08.334568
elapsed time: 0:25:10.585558
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 10:50:29.882144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.05
 ---- batch: 020 ----
mean loss: 215.72
 ---- batch: 030 ----
mean loss: 226.06
 ---- batch: 040 ----
mean loss: 229.37
train mean loss: 221.13
epoch train time: 0:00:08.374513
elapsed time: 0:25:18.961401
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 10:50:38.257979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.79
 ---- batch: 020 ----
mean loss: 224.67
 ---- batch: 030 ----
mean loss: 217.82
 ---- batch: 040 ----
mean loss: 221.86
train mean loss: 222.31
epoch train time: 0:00:08.357510
elapsed time: 0:25:27.320154
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 10:50:46.616821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.33
 ---- batch: 020 ----
mean loss: 217.88
 ---- batch: 030 ----
mean loss: 214.58
 ---- batch: 040 ----
mean loss: 224.87
train mean loss: 221.27
epoch train time: 0:00:08.367426
elapsed time: 0:25:35.688943
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 10:50:54.985473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.77
 ---- batch: 020 ----
mean loss: 217.05
 ---- batch: 030 ----
mean loss: 221.87
 ---- batch: 040 ----
mean loss: 218.08
train mean loss: 220.30
epoch train time: 0:00:08.393516
elapsed time: 0:25:44.083683
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 10:51:03.380217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.84
 ---- batch: 020 ----
mean loss: 214.64
 ---- batch: 030 ----
mean loss: 221.05
 ---- batch: 040 ----
mean loss: 226.10
train mean loss: 220.00
epoch train time: 0:00:08.426015
elapsed time: 0:25:52.511031
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 10:51:11.807655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.93
 ---- batch: 020 ----
mean loss: 223.95
 ---- batch: 030 ----
mean loss: 225.03
 ---- batch: 040 ----
mean loss: 211.72
train mean loss: 219.70
epoch train time: 0:00:08.382359
elapsed time: 0:26:00.894718
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 10:51:20.191168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.14
 ---- batch: 020 ----
mean loss: 214.69
 ---- batch: 030 ----
mean loss: 219.49
 ---- batch: 040 ----
mean loss: 220.25
train mean loss: 219.46
epoch train time: 0:00:08.357771
elapsed time: 0:26:09.253719
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 10:51:28.550262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.51
 ---- batch: 020 ----
mean loss: 217.55
 ---- batch: 030 ----
mean loss: 215.59
 ---- batch: 040 ----
mean loss: 219.13
train mean loss: 218.74
epoch train time: 0:00:08.359636
elapsed time: 0:26:17.614637
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 10:51:36.911174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.82
 ---- batch: 020 ----
mean loss: 221.93
 ---- batch: 030 ----
mean loss: 221.26
 ---- batch: 040 ----
mean loss: 214.04
train mean loss: 217.86
epoch train time: 0:00:08.380306
elapsed time: 0:26:25.996242
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 10:51:45.292784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.19
 ---- batch: 020 ----
mean loss: 223.29
 ---- batch: 030 ----
mean loss: 219.14
 ---- batch: 040 ----
mean loss: 215.57
train mean loss: 217.99
epoch train time: 0:00:08.354729
elapsed time: 0:26:34.352155
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 10:51:53.648719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.17
 ---- batch: 020 ----
mean loss: 220.75
 ---- batch: 030 ----
mean loss: 220.46
 ---- batch: 040 ----
mean loss: 217.64
train mean loss: 220.59
epoch train time: 0:00:08.372970
elapsed time: 0:26:42.726384
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 10:52:02.022929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.93
 ---- batch: 020 ----
mean loss: 231.91
 ---- batch: 030 ----
mean loss: 222.22
 ---- batch: 040 ----
mean loss: 224.54
train mean loss: 222.94
epoch train time: 0:00:08.376052
elapsed time: 0:26:51.103643
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 10:52:10.400174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.01
 ---- batch: 020 ----
mean loss: 217.91
 ---- batch: 030 ----
mean loss: 220.84
 ---- batch: 040 ----
mean loss: 213.69
train mean loss: 217.22
epoch train time: 0:00:08.368316
elapsed time: 0:26:59.473413
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 10:52:18.770044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.72
 ---- batch: 020 ----
mean loss: 219.35
 ---- batch: 030 ----
mean loss: 214.87
 ---- batch: 040 ----
mean loss: 216.66
train mean loss: 217.15
epoch train time: 0:00:08.337570
elapsed time: 0:27:07.812322
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 10:52:27.108874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.56
 ---- batch: 020 ----
mean loss: 221.20
 ---- batch: 030 ----
mean loss: 221.80
 ---- batch: 040 ----
mean loss: 216.89
train mean loss: 216.50
epoch train time: 0:00:08.333200
elapsed time: 0:27:16.146757
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 10:52:35.443318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.54
 ---- batch: 020 ----
mean loss: 220.64
 ---- batch: 030 ----
mean loss: 211.56
 ---- batch: 040 ----
mean loss: 213.29
train mean loss: 216.86
epoch train time: 0:00:08.331880
elapsed time: 0:27:24.479921
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 10:52:43.776471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.42
 ---- batch: 020 ----
mean loss: 220.53
 ---- batch: 030 ----
mean loss: 218.07
 ---- batch: 040 ----
mean loss: 212.52
train mean loss: 215.66
epoch train time: 0:00:08.438191
elapsed time: 0:27:32.919443
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 10:52:52.215992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.96
 ---- batch: 020 ----
mean loss: 218.44
 ---- batch: 030 ----
mean loss: 218.13
 ---- batch: 040 ----
mean loss: 213.51
train mean loss: 215.66
epoch train time: 0:00:08.391130
elapsed time: 0:27:41.311921
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 10:53:00.608453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.69
 ---- batch: 020 ----
mean loss: 212.64
 ---- batch: 030 ----
mean loss: 214.86
 ---- batch: 040 ----
mean loss: 217.19
train mean loss: 217.53
epoch train time: 0:00:08.483228
elapsed time: 0:27:49.796470
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 10:53:09.093005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.13
 ---- batch: 020 ----
mean loss: 213.11
 ---- batch: 030 ----
mean loss: 218.06
 ---- batch: 040 ----
mean loss: 212.60
train mean loss: 215.00
epoch train time: 0:00:08.473930
elapsed time: 0:27:58.271899
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 10:53:17.568464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.66
 ---- batch: 020 ----
mean loss: 210.87
 ---- batch: 030 ----
mean loss: 211.78
 ---- batch: 040 ----
mean loss: 222.24
train mean loss: 215.80
epoch train time: 0:00:08.484599
elapsed time: 0:28:06.757916
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 10:53:26.054471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.17
 ---- batch: 020 ----
mean loss: 217.86
 ---- batch: 030 ----
mean loss: 211.26
 ---- batch: 040 ----
mean loss: 213.07
train mean loss: 215.14
epoch train time: 0:00:08.485420
elapsed time: 0:28:15.244567
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 10:53:34.541099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.30
 ---- batch: 020 ----
mean loss: 216.94
 ---- batch: 030 ----
mean loss: 212.64
 ---- batch: 040 ----
mean loss: 223.13
train mean loss: 215.30
epoch train time: 0:00:08.479719
elapsed time: 0:28:23.725563
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 10:53:43.022094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.23
 ---- batch: 020 ----
mean loss: 221.81
 ---- batch: 030 ----
mean loss: 213.11
 ---- batch: 040 ----
mean loss: 214.10
train mean loss: 214.59
epoch train time: 0:00:08.552681
elapsed time: 0:28:32.279565
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 10:53:51.576185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.81
 ---- batch: 020 ----
mean loss: 212.91
 ---- batch: 030 ----
mean loss: 217.72
 ---- batch: 040 ----
mean loss: 213.30
train mean loss: 213.88
epoch train time: 0:00:08.531099
elapsed time: 0:28:40.812007
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 10:54:00.108574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.22
 ---- batch: 020 ----
mean loss: 220.50
 ---- batch: 030 ----
mean loss: 212.84
 ---- batch: 040 ----
mean loss: 218.63
train mean loss: 214.23
epoch train time: 0:00:08.475901
elapsed time: 0:28:49.289335
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 10:54:08.585947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.24
 ---- batch: 020 ----
mean loss: 213.79
 ---- batch: 030 ----
mean loss: 212.41
 ---- batch: 040 ----
mean loss: 217.03
train mean loss: 211.79
epoch train time: 0:00:08.505863
elapsed time: 0:28:57.796786
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 10:54:17.093072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.22
 ---- batch: 020 ----
mean loss: 201.63
 ---- batch: 030 ----
mean loss: 214.25
 ---- batch: 040 ----
mean loss: 210.84
train mean loss: 210.81
epoch train time: 0:00:08.552230
elapsed time: 0:29:06.350066
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 10:54:25.646644
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.27
 ---- batch: 020 ----
mean loss: 207.56
 ---- batch: 030 ----
mean loss: 216.48
 ---- batch: 040 ----
mean loss: 203.28
train mean loss: 212.15
epoch train time: 0:00:08.487885
elapsed time: 0:29:14.839310
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 10:54:34.135850
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.26
 ---- batch: 020 ----
mean loss: 209.56
 ---- batch: 030 ----
mean loss: 217.02
 ---- batch: 040 ----
mean loss: 215.32
train mean loss: 212.01
epoch train time: 0:00:08.489788
elapsed time: 0:29:23.330302
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 10:54:42.626848
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.92
 ---- batch: 020 ----
mean loss: 212.82
 ---- batch: 030 ----
mean loss: 205.20
 ---- batch: 040 ----
mean loss: 212.61
train mean loss: 211.33
epoch train time: 0:00:08.479541
elapsed time: 0:29:31.811081
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 10:54:51.107680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.88
 ---- batch: 020 ----
mean loss: 208.43
 ---- batch: 030 ----
mean loss: 210.38
 ---- batch: 040 ----
mean loss: 209.82
train mean loss: 211.18
epoch train time: 0:00:08.459536
elapsed time: 0:29:40.271914
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 10:54:59.568442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.16
 ---- batch: 020 ----
mean loss: 206.65
 ---- batch: 030 ----
mean loss: 208.26
 ---- batch: 040 ----
mean loss: 210.44
train mean loss: 210.39
epoch train time: 0:00:08.457820
elapsed time: 0:29:48.730993
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 10:55:08.027560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.37
 ---- batch: 020 ----
mean loss: 214.90
 ---- batch: 030 ----
mean loss: 211.42
 ---- batch: 040 ----
mean loss: 209.91
train mean loss: 210.93
epoch train time: 0:00:08.503142
elapsed time: 0:29:57.235468
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 10:55:16.532037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.22
 ---- batch: 020 ----
mean loss: 213.75
 ---- batch: 030 ----
mean loss: 206.49
 ---- batch: 040 ----
mean loss: 215.53
train mean loss: 210.77
epoch train time: 0:00:08.454158
elapsed time: 0:30:05.690975
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 10:55:24.987515
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.89
 ---- batch: 020 ----
mean loss: 215.12
 ---- batch: 030 ----
mean loss: 212.82
 ---- batch: 040 ----
mean loss: 208.70
train mean loss: 211.47
epoch train time: 0:00:08.478702
elapsed time: 0:30:14.170883
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 10:55:33.467416
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.15
 ---- batch: 020 ----
mean loss: 215.86
 ---- batch: 030 ----
mean loss: 215.13
 ---- batch: 040 ----
mean loss: 203.77
train mean loss: 211.65
epoch train time: 0:00:08.452200
elapsed time: 0:30:22.624341
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 10:55:41.920954
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.19
 ---- batch: 020 ----
mean loss: 204.89
 ---- batch: 030 ----
mean loss: 210.02
 ---- batch: 040 ----
mean loss: 209.55
train mean loss: 211.54
epoch train time: 0:00:08.463765
elapsed time: 0:30:31.089525
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 10:55:50.386121
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.11
 ---- batch: 020 ----
mean loss: 213.22
 ---- batch: 030 ----
mean loss: 208.67
 ---- batch: 040 ----
mean loss: 207.93
train mean loss: 211.25
epoch train time: 0:00:08.569885
elapsed time: 0:30:39.660761
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 10:55:58.957310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.51
 ---- batch: 020 ----
mean loss: 213.44
 ---- batch: 030 ----
mean loss: 206.94
 ---- batch: 040 ----
mean loss: 215.84
train mean loss: 211.28
epoch train time: 0:00:08.457918
elapsed time: 0:30:48.119993
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 10:56:07.416550
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.27
 ---- batch: 020 ----
mean loss: 205.78
 ---- batch: 030 ----
mean loss: 212.63
 ---- batch: 040 ----
mean loss: 212.11
train mean loss: 211.09
epoch train time: 0:00:08.469605
elapsed time: 0:30:56.590827
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 10:56:15.887367
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.61
 ---- batch: 020 ----
mean loss: 205.92
 ---- batch: 030 ----
mean loss: 209.94
 ---- batch: 040 ----
mean loss: 209.41
train mean loss: 210.26
epoch train time: 0:00:08.412979
elapsed time: 0:31:05.005046
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 10:56:24.301548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.32
 ---- batch: 020 ----
mean loss: 206.45
 ---- batch: 030 ----
mean loss: 213.11
 ---- batch: 040 ----
mean loss: 214.88
train mean loss: 210.91
epoch train time: 0:00:08.346224
elapsed time: 0:31:13.352446
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 10:56:32.649010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.42
 ---- batch: 020 ----
mean loss: 212.70
 ---- batch: 030 ----
mean loss: 214.58
 ---- batch: 040 ----
mean loss: 208.02
train mean loss: 211.25
epoch train time: 0:00:08.358464
elapsed time: 0:31:21.712231
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 10:56:41.008771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.46
 ---- batch: 020 ----
mean loss: 217.60
 ---- batch: 030 ----
mean loss: 214.63
 ---- batch: 040 ----
mean loss: 211.23
train mean loss: 211.41
epoch train time: 0:00:08.369913
elapsed time: 0:31:30.083394
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 10:56:49.379947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.54
 ---- batch: 020 ----
mean loss: 210.99
 ---- batch: 030 ----
mean loss: 207.83
 ---- batch: 040 ----
mean loss: 217.52
train mean loss: 211.13
epoch train time: 0:00:08.342265
elapsed time: 0:31:38.426930
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 10:56:57.723488
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.46
 ---- batch: 020 ----
mean loss: 207.09
 ---- batch: 030 ----
mean loss: 209.49
 ---- batch: 040 ----
mean loss: 211.39
train mean loss: 210.92
epoch train time: 0:00:08.368572
elapsed time: 0:31:46.796770
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 10:57:06.093309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.33
 ---- batch: 020 ----
mean loss: 208.02
 ---- batch: 030 ----
mean loss: 205.36
 ---- batch: 040 ----
mean loss: 216.27
train mean loss: 211.10
epoch train time: 0:00:08.383014
elapsed time: 0:31:55.181073
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 10:57:14.477588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.81
 ---- batch: 020 ----
mean loss: 210.75
 ---- batch: 030 ----
mean loss: 213.36
 ---- batch: 040 ----
mean loss: 210.18
train mean loss: 211.58
epoch train time: 0:00:08.382868
elapsed time: 0:32:03.565148
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 10:57:22.861800
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.88
 ---- batch: 020 ----
mean loss: 211.36
 ---- batch: 030 ----
mean loss: 212.35
 ---- batch: 040 ----
mean loss: 210.58
train mean loss: 212.01
epoch train time: 0:00:08.366863
elapsed time: 0:32:11.933284
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 10:57:31.229827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.80
 ---- batch: 020 ----
mean loss: 217.41
 ---- batch: 030 ----
mean loss: 210.62
 ---- batch: 040 ----
mean loss: 201.51
train mean loss: 210.52
epoch train time: 0:00:08.368677
elapsed time: 0:32:20.303362
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 10:57:39.600034
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.85
 ---- batch: 020 ----
mean loss: 208.95
 ---- batch: 030 ----
mean loss: 215.36
 ---- batch: 040 ----
mean loss: 213.48
train mean loss: 211.06
epoch train time: 0:00:08.395010
elapsed time: 0:32:28.699766
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 10:57:47.996302
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.88
 ---- batch: 020 ----
mean loss: 210.27
 ---- batch: 030 ----
mean loss: 209.52
 ---- batch: 040 ----
mean loss: 211.42
train mean loss: 210.50
epoch train time: 0:00:08.378840
elapsed time: 0:32:37.079924
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 10:57:56.376453
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.66
 ---- batch: 020 ----
mean loss: 215.75
 ---- batch: 030 ----
mean loss: 213.41
 ---- batch: 040 ----
mean loss: 207.94
train mean loss: 211.24
epoch train time: 0:00:08.401230
elapsed time: 0:32:45.482428
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 10:58:04.778995
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.45
 ---- batch: 020 ----
mean loss: 208.70
 ---- batch: 030 ----
mean loss: 221.43
 ---- batch: 040 ----
mean loss: 207.21
train mean loss: 210.50
epoch train time: 0:00:08.374757
elapsed time: 0:32:53.858417
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 10:58:13.155005
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.24
 ---- batch: 020 ----
mean loss: 207.60
 ---- batch: 030 ----
mean loss: 214.42
 ---- batch: 040 ----
mean loss: 211.30
train mean loss: 211.37
epoch train time: 0:00:08.342682
elapsed time: 0:33:02.202362
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 10:58:21.498916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.13
 ---- batch: 020 ----
mean loss: 209.08
 ---- batch: 030 ----
mean loss: 207.53
 ---- batch: 040 ----
mean loss: 210.95
train mean loss: 211.52
epoch train time: 0:00:08.368135
elapsed time: 0:33:10.571725
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 10:58:29.868262
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.53
 ---- batch: 020 ----
mean loss: 215.50
 ---- batch: 030 ----
mean loss: 208.27
 ---- batch: 040 ----
mean loss: 214.64
train mean loss: 211.41
epoch train time: 0:00:08.375828
elapsed time: 0:33:18.948784
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 10:58:38.245328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.83
 ---- batch: 020 ----
mean loss: 208.85
 ---- batch: 030 ----
mean loss: 210.31
 ---- batch: 040 ----
mean loss: 208.13
train mean loss: 210.50
epoch train time: 0:00:08.371346
elapsed time: 0:33:27.321810
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 10:58:46.618099
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.95
 ---- batch: 020 ----
mean loss: 215.88
 ---- batch: 030 ----
mean loss: 209.58
 ---- batch: 040 ----
mean loss: 212.63
train mean loss: 210.45
epoch train time: 0:00:08.387754
elapsed time: 0:33:35.710633
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 10:58:55.007222
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.33
 ---- batch: 020 ----
mean loss: 213.13
 ---- batch: 030 ----
mean loss: 211.25
 ---- batch: 040 ----
mean loss: 205.71
train mean loss: 210.71
epoch train time: 0:00:08.396789
elapsed time: 0:33:44.108804
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 10:59:03.405375
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.56
 ---- batch: 020 ----
mean loss: 206.43
 ---- batch: 030 ----
mean loss: 213.71
 ---- batch: 040 ----
mean loss: 207.05
train mean loss: 210.77
epoch train time: 0:00:08.390297
elapsed time: 0:33:52.500486
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 10:59:11.797027
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.13
 ---- batch: 020 ----
mean loss: 218.07
 ---- batch: 030 ----
mean loss: 208.19
 ---- batch: 040 ----
mean loss: 202.28
train mean loss: 210.85
epoch train time: 0:00:08.384317
elapsed time: 0:34:00.886028
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 10:59:20.182596
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.41
 ---- batch: 020 ----
mean loss: 212.41
 ---- batch: 030 ----
mean loss: 209.37
 ---- batch: 040 ----
mean loss: 214.83
train mean loss: 210.64
epoch train time: 0:00:08.385700
elapsed time: 0:34:09.273070
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 10:59:28.569600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.79
 ---- batch: 020 ----
mean loss: 213.75
 ---- batch: 030 ----
mean loss: 208.32
 ---- batch: 040 ----
mean loss: 213.60
train mean loss: 210.43
epoch train time: 0:00:08.365413
elapsed time: 0:34:17.640040
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 10:59:36.936598
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.30
 ---- batch: 020 ----
mean loss: 214.20
 ---- batch: 030 ----
mean loss: 207.95
 ---- batch: 040 ----
mean loss: 204.21
train mean loss: 210.24
epoch train time: 0:00:08.355218
elapsed time: 0:34:25.996523
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 10:59:45.293097
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.73
 ---- batch: 020 ----
mean loss: 212.97
 ---- batch: 030 ----
mean loss: 211.32
 ---- batch: 040 ----
mean loss: 205.44
train mean loss: 210.46
epoch train time: 0:00:08.358650
elapsed time: 0:34:34.356462
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 10:59:53.653009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.11
 ---- batch: 020 ----
mean loss: 217.81
 ---- batch: 030 ----
mean loss: 211.40
 ---- batch: 040 ----
mean loss: 208.19
train mean loss: 210.72
epoch train time: 0:00:08.371450
elapsed time: 0:34:42.729153
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 11:00:02.025783
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.48
 ---- batch: 020 ----
mean loss: 209.56
 ---- batch: 030 ----
mean loss: 206.38
 ---- batch: 040 ----
mean loss: 219.29
train mean loss: 210.25
epoch train time: 0:00:08.365858
elapsed time: 0:34:51.096311
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 11:00:10.392856
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.52
 ---- batch: 020 ----
mean loss: 213.97
 ---- batch: 030 ----
mean loss: 209.01
 ---- batch: 040 ----
mean loss: 217.23
train mean loss: 210.56
epoch train time: 0:00:08.342871
elapsed time: 0:34:59.440387
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 11:00:18.736941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.74
 ---- batch: 020 ----
mean loss: 207.34
 ---- batch: 030 ----
mean loss: 207.68
 ---- batch: 040 ----
mean loss: 208.88
train mean loss: 209.86
epoch train time: 0:00:08.346691
elapsed time: 0:35:07.788323
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 11:00:27.084845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.69
 ---- batch: 020 ----
mean loss: 212.06
 ---- batch: 030 ----
mean loss: 210.92
 ---- batch: 040 ----
mean loss: 211.10
train mean loss: 210.27
epoch train time: 0:00:08.359237
elapsed time: 0:35:16.148857
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 11:00:35.445459
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.98
 ---- batch: 020 ----
mean loss: 207.26
 ---- batch: 030 ----
mean loss: 206.68
 ---- batch: 040 ----
mean loss: 210.85
train mean loss: 210.97
epoch train time: 0:00:08.348023
elapsed time: 0:35:24.498181
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 11:00:43.794797
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.05
 ---- batch: 020 ----
mean loss: 208.40
 ---- batch: 030 ----
mean loss: 208.68
 ---- batch: 040 ----
mean loss: 210.36
train mean loss: 210.07
epoch train time: 0:00:08.357519
elapsed time: 0:35:32.857032
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 11:00:52.153684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.88
 ---- batch: 020 ----
mean loss: 209.87
 ---- batch: 030 ----
mean loss: 206.01
 ---- batch: 040 ----
mean loss: 206.33
train mean loss: 209.42
epoch train time: 0:00:08.341174
elapsed time: 0:35:41.209618
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_6/checkpoint.pth.tar
**** end time: 2019-09-25 11:01:00.505865 ****
