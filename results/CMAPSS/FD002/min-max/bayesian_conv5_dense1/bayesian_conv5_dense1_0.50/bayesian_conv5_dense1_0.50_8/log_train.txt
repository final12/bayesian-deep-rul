Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 13424
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 11:37:24.293935 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 11:37:24.312150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3890.80
 ---- batch: 020 ----
mean loss: 1673.38
 ---- batch: 030 ----
mean loss: 1431.85
 ---- batch: 040 ----
mean loss: 1242.21
train mean loss: 1906.39
epoch train time: 0:00:24.006477
elapsed time: 0:00:24.033672
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 11:37:48.327649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1114.08
 ---- batch: 020 ----
mean loss: 1074.91
 ---- batch: 030 ----
mean loss: 1106.08
 ---- batch: 040 ----
mean loss: 1064.00
train mean loss: 1081.88
epoch train time: 0:00:08.541261
elapsed time: 0:00:32.575896
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 11:37:56.870151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1045.01
 ---- batch: 020 ----
mean loss: 1034.78
 ---- batch: 030 ----
mean loss: 1022.15
 ---- batch: 040 ----
mean loss: 1028.53
train mean loss: 1025.42
epoch train time: 0:00:08.513826
elapsed time: 0:00:41.090925
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 11:38:05.385161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 998.07
 ---- batch: 020 ----
mean loss: 997.75
 ---- batch: 030 ----
mean loss: 981.42
 ---- batch: 040 ----
mean loss: 1005.86
train mean loss: 994.81
epoch train time: 0:00:08.513363
elapsed time: 0:00:49.605453
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 11:38:13.899701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 982.61
 ---- batch: 020 ----
mean loss: 975.12
 ---- batch: 030 ----
mean loss: 977.53
 ---- batch: 040 ----
mean loss: 958.15
train mean loss: 970.87
epoch train time: 0:00:08.481182
elapsed time: 0:00:58.087925
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 11:38:22.382181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 957.61
 ---- batch: 020 ----
mean loss: 957.37
 ---- batch: 030 ----
mean loss: 955.46
 ---- batch: 040 ----
mean loss: 945.71
train mean loss: 954.59
epoch train time: 0:00:08.476816
elapsed time: 0:01:06.566102
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 11:38:30.860354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 957.59
 ---- batch: 020 ----
mean loss: 956.13
 ---- batch: 030 ----
mean loss: 948.03
 ---- batch: 040 ----
mean loss: 937.55
train mean loss: 949.87
epoch train time: 0:00:08.494081
elapsed time: 0:01:15.061483
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 11:38:39.355744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 959.62
 ---- batch: 020 ----
mean loss: 942.41
 ---- batch: 030 ----
mean loss: 953.44
 ---- batch: 040 ----
mean loss: 978.30
train mean loss: 959.61
epoch train time: 0:00:08.492716
elapsed time: 0:01:23.555412
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 11:38:47.849687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.76
 ---- batch: 020 ----
mean loss: 946.05
 ---- batch: 030 ----
mean loss: 923.20
 ---- batch: 040 ----
mean loss: 932.78
train mean loss: 938.84
epoch train time: 0:00:08.505336
elapsed time: 0:01:32.061994
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 11:38:56.356241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.63
 ---- batch: 020 ----
mean loss: 943.27
 ---- batch: 030 ----
mean loss: 917.35
 ---- batch: 040 ----
mean loss: 945.93
train mean loss: 935.73
epoch train time: 0:00:08.503632
elapsed time: 0:01:40.566797
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 11:39:04.861073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.11
 ---- batch: 020 ----
mean loss: 924.39
 ---- batch: 030 ----
mean loss: 930.35
 ---- batch: 040 ----
mean loss: 933.80
train mean loss: 928.32
epoch train time: 0:00:08.493722
elapsed time: 0:01:49.061775
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 11:39:13.356059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.83
 ---- batch: 020 ----
mean loss: 932.74
 ---- batch: 030 ----
mean loss: 936.33
 ---- batch: 040 ----
mean loss: 928.33
train mean loss: 928.17
epoch train time: 0:00:08.516660
elapsed time: 0:01:57.579834
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 11:39:21.874051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.93
 ---- batch: 020 ----
mean loss: 908.66
 ---- batch: 030 ----
mean loss: 903.36
 ---- batch: 040 ----
mean loss: 919.55
train mean loss: 919.34
epoch train time: 0:00:08.515243
elapsed time: 0:02:06.096231
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 11:39:30.390480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.86
 ---- batch: 020 ----
mean loss: 914.56
 ---- batch: 030 ----
mean loss: 928.30
 ---- batch: 040 ----
mean loss: 906.37
train mean loss: 916.07
epoch train time: 0:00:08.518518
elapsed time: 0:02:14.616086
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 11:39:38.910413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.50
 ---- batch: 020 ----
mean loss: 926.22
 ---- batch: 030 ----
mean loss: 912.74
 ---- batch: 040 ----
mean loss: 900.34
train mean loss: 910.38
epoch train time: 0:00:08.539802
elapsed time: 0:02:23.157231
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 11:39:47.451467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 897.69
 ---- batch: 020 ----
mean loss: 897.39
 ---- batch: 030 ----
mean loss: 919.32
 ---- batch: 040 ----
mean loss: 929.09
train mean loss: 909.80
epoch train time: 0:00:08.509349
elapsed time: 0:02:31.667826
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 11:39:55.962081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.12
 ---- batch: 020 ----
mean loss: 929.75
 ---- batch: 030 ----
mean loss: 903.25
 ---- batch: 040 ----
mean loss: 915.98
train mean loss: 908.43
epoch train time: 0:00:08.500991
elapsed time: 0:02:40.170270
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 11:40:04.464603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.62
 ---- batch: 020 ----
mean loss: 901.09
 ---- batch: 030 ----
mean loss: 893.76
 ---- batch: 040 ----
mean loss: 891.66
train mean loss: 899.67
epoch train time: 0:00:08.492001
elapsed time: 0:02:48.663555
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 11:40:12.957837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.39
 ---- batch: 020 ----
mean loss: 897.17
 ---- batch: 030 ----
mean loss: 893.08
 ---- batch: 040 ----
mean loss: 897.97
train mean loss: 891.62
epoch train time: 0:00:08.488112
elapsed time: 0:02:57.152977
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 11:40:21.447249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.51
 ---- batch: 020 ----
mean loss: 878.62
 ---- batch: 030 ----
mean loss: 891.10
 ---- batch: 040 ----
mean loss: 912.55
train mean loss: 887.78
epoch train time: 0:00:08.496985
elapsed time: 0:03:05.651166
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 11:40:29.945391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.62
 ---- batch: 020 ----
mean loss: 893.55
 ---- batch: 030 ----
mean loss: 872.39
 ---- batch: 040 ----
mean loss: 883.55
train mean loss: 877.74
epoch train time: 0:00:08.528244
elapsed time: 0:03:14.180650
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 11:40:38.474920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.66
 ---- batch: 020 ----
mean loss: 851.85
 ---- batch: 030 ----
mean loss: 848.25
 ---- batch: 040 ----
mean loss: 862.55
train mean loss: 852.06
epoch train time: 0:00:08.515373
elapsed time: 0:03:22.697282
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 11:40:46.991451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.61
 ---- batch: 020 ----
mean loss: 814.58
 ---- batch: 030 ----
mean loss: 801.68
 ---- batch: 040 ----
mean loss: 801.22
train mean loss: 804.54
epoch train time: 0:00:08.501199
elapsed time: 0:03:31.199626
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 11:40:55.493894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 759.80
 ---- batch: 020 ----
mean loss: 747.84
 ---- batch: 030 ----
mean loss: 711.36
 ---- batch: 040 ----
mean loss: 725.25
train mean loss: 739.17
epoch train time: 0:00:08.478442
elapsed time: 0:03:39.679394
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 11:41:03.973705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 702.53
 ---- batch: 020 ----
mean loss: 683.48
 ---- batch: 030 ----
mean loss: 708.69
 ---- batch: 040 ----
mean loss: 696.88
train mean loss: 698.34
epoch train time: 0:00:08.494126
elapsed time: 0:03:48.174800
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 11:41:12.469037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.24
 ---- batch: 020 ----
mean loss: 679.64
 ---- batch: 030 ----
mean loss: 666.11
 ---- batch: 040 ----
mean loss: 655.54
train mean loss: 673.75
epoch train time: 0:00:08.519431
elapsed time: 0:03:56.695433
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 11:41:20.989746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 651.00
 ---- batch: 020 ----
mean loss: 652.71
 ---- batch: 030 ----
mean loss: 651.91
 ---- batch: 040 ----
mean loss: 640.09
train mean loss: 647.89
epoch train time: 0:00:08.481928
elapsed time: 0:04:05.178614
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 11:41:29.472845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 631.75
 ---- batch: 020 ----
mean loss: 620.27
 ---- batch: 030 ----
mean loss: 615.20
 ---- batch: 040 ----
mean loss: 606.11
train mean loss: 617.99
epoch train time: 0:00:08.492123
elapsed time: 0:04:13.671922
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 11:41:37.966193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 594.02
 ---- batch: 020 ----
mean loss: 595.74
 ---- batch: 030 ----
mean loss: 580.13
 ---- batch: 040 ----
mean loss: 581.23
train mean loss: 585.51
epoch train time: 0:00:08.879398
elapsed time: 0:04:22.552593
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 11:41:46.846842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 590.52
 ---- batch: 020 ----
mean loss: 574.33
 ---- batch: 030 ----
mean loss: 562.45
 ---- batch: 040 ----
mean loss: 574.30
train mean loss: 569.53
epoch train time: 0:00:08.502514
elapsed time: 0:04:31.056375
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 11:41:55.350631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.77
 ---- batch: 020 ----
mean loss: 555.23
 ---- batch: 030 ----
mean loss: 554.43
 ---- batch: 040 ----
mean loss: 542.25
train mean loss: 547.06
epoch train time: 0:00:08.507827
elapsed time: 0:04:39.565422
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 11:42:03.859684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.56
 ---- batch: 020 ----
mean loss: 521.28
 ---- batch: 030 ----
mean loss: 519.19
 ---- batch: 040 ----
mean loss: 520.01
train mean loss: 522.33
epoch train time: 0:00:08.502229
elapsed time: 0:04:48.068866
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 11:42:12.363149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.67
 ---- batch: 020 ----
mean loss: 499.42
 ---- batch: 030 ----
mean loss: 509.24
 ---- batch: 040 ----
mean loss: 499.28
train mean loss: 502.55
epoch train time: 0:00:08.497312
elapsed time: 0:04:56.567598
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 11:42:20.861909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.97
 ---- batch: 020 ----
mean loss: 509.09
 ---- batch: 030 ----
mean loss: 478.14
 ---- batch: 040 ----
mean loss: 487.13
train mean loss: 489.84
epoch train time: 0:00:08.491849
elapsed time: 0:05:05.060773
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 11:42:29.355088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.03
 ---- batch: 020 ----
mean loss: 468.81
 ---- batch: 030 ----
mean loss: 460.80
 ---- batch: 040 ----
mean loss: 476.42
train mean loss: 470.49
epoch train time: 0:00:08.516844
elapsed time: 0:05:13.578877
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 11:42:37.873184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.84
 ---- batch: 020 ----
mean loss: 460.76
 ---- batch: 030 ----
mean loss: 471.70
 ---- batch: 040 ----
mean loss: 464.51
train mean loss: 462.12
epoch train time: 0:00:08.482371
elapsed time: 0:05:22.062670
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 11:42:46.356946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.62
 ---- batch: 020 ----
mean loss: 449.84
 ---- batch: 030 ----
mean loss: 442.97
 ---- batch: 040 ----
mean loss: 462.69
train mean loss: 452.09
epoch train time: 0:00:08.576777
elapsed time: 0:05:30.640843
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 11:42:54.935095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.27
 ---- batch: 020 ----
mean loss: 436.57
 ---- batch: 030 ----
mean loss: 452.89
 ---- batch: 040 ----
mean loss: 439.08
train mean loss: 443.89
epoch train time: 0:00:08.526280
elapsed time: 0:05:39.168481
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 11:43:03.462727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.10
 ---- batch: 020 ----
mean loss: 434.77
 ---- batch: 030 ----
mean loss: 428.29
 ---- batch: 040 ----
mean loss: 427.07
train mean loss: 433.82
epoch train time: 0:00:08.489314
elapsed time: 0:05:47.659070
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 11:43:11.953306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.96
 ---- batch: 020 ----
mean loss: 423.18
 ---- batch: 030 ----
mean loss: 428.94
 ---- batch: 040 ----
mean loss: 415.72
train mean loss: 424.57
epoch train time: 0:00:08.489902
elapsed time: 0:05:56.150176
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 11:43:20.444499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.14
 ---- batch: 020 ----
mean loss: 422.80
 ---- batch: 030 ----
mean loss: 415.37
 ---- batch: 040 ----
mean loss: 414.02
train mean loss: 419.40
epoch train time: 0:00:08.481272
elapsed time: 0:06:04.632742
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 11:43:28.927073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.18
 ---- batch: 020 ----
mean loss: 418.06
 ---- batch: 030 ----
mean loss: 407.27
 ---- batch: 040 ----
mean loss: 403.08
train mean loss: 409.64
epoch train time: 0:00:08.480577
elapsed time: 0:06:13.114769
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 11:43:37.409017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.03
 ---- batch: 020 ----
mean loss: 403.97
 ---- batch: 030 ----
mean loss: 407.24
 ---- batch: 040 ----
mean loss: 415.10
train mean loss: 408.89
epoch train time: 0:00:08.454815
elapsed time: 0:06:21.570791
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 11:43:45.865042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.13
 ---- batch: 020 ----
mean loss: 410.19
 ---- batch: 030 ----
mean loss: 391.54
 ---- batch: 040 ----
mean loss: 408.69
train mean loss: 404.36
epoch train time: 0:00:08.525478
elapsed time: 0:06:30.097505
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 11:43:54.391666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.42
 ---- batch: 020 ----
mean loss: 391.75
 ---- batch: 030 ----
mean loss: 392.97
 ---- batch: 040 ----
mean loss: 397.96
train mean loss: 399.74
epoch train time: 0:00:08.485249
elapsed time: 0:06:38.584018
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 11:44:02.878099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.17
 ---- batch: 020 ----
mean loss: 403.27
 ---- batch: 030 ----
mean loss: 389.51
 ---- batch: 040 ----
mean loss: 392.58
train mean loss: 392.99
epoch train time: 0:00:08.477132
elapsed time: 0:06:47.062261
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 11:44:11.356572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.24
 ---- batch: 020 ----
mean loss: 382.83
 ---- batch: 030 ----
mean loss: 385.68
 ---- batch: 040 ----
mean loss: 396.73
train mean loss: 385.89
epoch train time: 0:00:08.471736
elapsed time: 0:06:55.535403
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 11:44:19.829706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.98
 ---- batch: 020 ----
mean loss: 375.85
 ---- batch: 030 ----
mean loss: 387.96
 ---- batch: 040 ----
mean loss: 388.44
train mean loss: 381.12
epoch train time: 0:00:08.472857
elapsed time: 0:07:04.009523
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 11:44:28.303757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.39
 ---- batch: 020 ----
mean loss: 372.73
 ---- batch: 030 ----
mean loss: 369.62
 ---- batch: 040 ----
mean loss: 369.90
train mean loss: 373.56
epoch train time: 0:00:08.460201
elapsed time: 0:07:12.470947
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 11:44:36.765204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.85
 ---- batch: 020 ----
mean loss: 374.12
 ---- batch: 030 ----
mean loss: 386.36
 ---- batch: 040 ----
mean loss: 372.39
train mean loss: 374.58
epoch train time: 0:00:08.465914
elapsed time: 0:07:20.938242
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 11:44:45.232500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.62
 ---- batch: 020 ----
mean loss: 370.60
 ---- batch: 030 ----
mean loss: 371.57
 ---- batch: 040 ----
mean loss: 366.14
train mean loss: 368.25
epoch train time: 0:00:08.495490
elapsed time: 0:07:29.434929
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 11:44:53.729175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.68
 ---- batch: 020 ----
mean loss: 370.48
 ---- batch: 030 ----
mean loss: 355.71
 ---- batch: 040 ----
mean loss: 372.50
train mean loss: 363.64
epoch train time: 0:00:08.450347
elapsed time: 0:07:37.886520
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 11:45:02.180776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.17
 ---- batch: 020 ----
mean loss: 362.58
 ---- batch: 030 ----
mean loss: 362.81
 ---- batch: 040 ----
mean loss: 358.90
train mean loss: 359.60
epoch train time: 0:00:08.475907
elapsed time: 0:07:46.363817
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 11:45:10.658097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.30
 ---- batch: 020 ----
mean loss: 348.91
 ---- batch: 030 ----
mean loss: 355.37
 ---- batch: 040 ----
mean loss: 343.85
train mean loss: 352.99
epoch train time: 0:00:08.487721
elapsed time: 0:07:54.853049
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 11:45:19.147356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.98
 ---- batch: 020 ----
mean loss: 359.30
 ---- batch: 030 ----
mean loss: 349.22
 ---- batch: 040 ----
mean loss: 347.25
train mean loss: 350.59
epoch train time: 0:00:08.475176
elapsed time: 0:08:03.329498
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 11:45:27.623721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.34
 ---- batch: 020 ----
mean loss: 345.64
 ---- batch: 030 ----
mean loss: 345.33
 ---- batch: 040 ----
mean loss: 348.50
train mean loss: 347.55
epoch train time: 0:00:08.461028
elapsed time: 0:08:11.791741
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 11:45:36.085998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.51
 ---- batch: 020 ----
mean loss: 341.62
 ---- batch: 030 ----
mean loss: 344.69
 ---- batch: 040 ----
mean loss: 337.99
train mean loss: 339.10
epoch train time: 0:00:08.463578
elapsed time: 0:08:20.256601
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 11:45:44.550849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.72
 ---- batch: 020 ----
mean loss: 339.28
 ---- batch: 030 ----
mean loss: 333.42
 ---- batch: 040 ----
mean loss: 339.40
train mean loss: 338.13
epoch train time: 0:00:08.491762
elapsed time: 0:08:28.749601
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 11:45:53.043850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.50
 ---- batch: 020 ----
mean loss: 330.93
 ---- batch: 030 ----
mean loss: 333.03
 ---- batch: 040 ----
mean loss: 333.71
train mean loss: 336.13
epoch train time: 0:00:08.494321
elapsed time: 0:08:37.245327
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 11:46:01.539644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.70
 ---- batch: 020 ----
mean loss: 341.85
 ---- batch: 030 ----
mean loss: 340.69
 ---- batch: 040 ----
mean loss: 332.72
train mean loss: 336.78
epoch train time: 0:00:08.487090
elapsed time: 0:08:45.733689
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 11:46:10.027944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.33
 ---- batch: 020 ----
mean loss: 333.87
 ---- batch: 030 ----
mean loss: 321.28
 ---- batch: 040 ----
mean loss: 337.60
train mean loss: 328.46
epoch train time: 0:00:08.484156
elapsed time: 0:08:54.219072
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 11:46:18.513347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.43
 ---- batch: 020 ----
mean loss: 326.90
 ---- batch: 030 ----
mean loss: 337.31
 ---- batch: 040 ----
mean loss: 315.07
train mean loss: 327.60
epoch train time: 0:00:08.499644
elapsed time: 0:09:02.720058
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 11:46:27.014322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.47
 ---- batch: 020 ----
mean loss: 321.65
 ---- batch: 030 ----
mean loss: 325.45
 ---- batch: 040 ----
mean loss: 318.59
train mean loss: 323.27
epoch train time: 0:00:08.504870
elapsed time: 0:09:11.226107
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 11:46:35.520344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.32
 ---- batch: 020 ----
mean loss: 314.96
 ---- batch: 030 ----
mean loss: 324.18
 ---- batch: 040 ----
mean loss: 328.45
train mean loss: 320.74
epoch train time: 0:00:08.490478
elapsed time: 0:09:19.717791
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 11:46:44.012042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.35
 ---- batch: 020 ----
mean loss: 320.13
 ---- batch: 030 ----
mean loss: 312.28
 ---- batch: 040 ----
mean loss: 314.85
train mean loss: 320.11
epoch train time: 0:00:08.511461
elapsed time: 0:09:28.230653
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 11:46:52.524928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.64
 ---- batch: 020 ----
mean loss: 315.32
 ---- batch: 030 ----
mean loss: 312.65
 ---- batch: 040 ----
mean loss: 315.84
train mean loss: 316.01
epoch train time: 0:00:08.498822
elapsed time: 0:09:36.730718
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 11:47:01.024971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.35
 ---- batch: 020 ----
mean loss: 324.69
 ---- batch: 030 ----
mean loss: 305.87
 ---- batch: 040 ----
mean loss: 323.87
train mean loss: 315.73
epoch train time: 0:00:08.516762
elapsed time: 0:09:45.248666
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 11:47:09.542934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.72
 ---- batch: 020 ----
mean loss: 317.26
 ---- batch: 030 ----
mean loss: 313.92
 ---- batch: 040 ----
mean loss: 307.15
train mean loss: 311.61
epoch train time: 0:00:08.507018
elapsed time: 0:09:53.756943
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 11:47:18.051197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.28
 ---- batch: 020 ----
mean loss: 316.23
 ---- batch: 030 ----
mean loss: 300.87
 ---- batch: 040 ----
mean loss: 303.06
train mean loss: 305.01
epoch train time: 0:00:08.515110
elapsed time: 0:10:02.273280
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 11:47:26.567520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.14
 ---- batch: 020 ----
mean loss: 305.13
 ---- batch: 030 ----
mean loss: 311.08
 ---- batch: 040 ----
mean loss: 314.51
train mean loss: 307.25
epoch train time: 0:00:08.519541
elapsed time: 0:10:10.794167
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 11:47:35.088403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.55
 ---- batch: 020 ----
mean loss: 304.27
 ---- batch: 030 ----
mean loss: 297.04
 ---- batch: 040 ----
mean loss: 303.66
train mean loss: 302.25
epoch train time: 0:00:08.480107
elapsed time: 0:10:19.275486
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 11:47:43.569765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.79
 ---- batch: 020 ----
mean loss: 295.56
 ---- batch: 030 ----
mean loss: 306.00
 ---- batch: 040 ----
mean loss: 296.74
train mean loss: 299.32
epoch train time: 0:00:08.506204
elapsed time: 0:10:27.782964
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 11:47:52.077236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.44
 ---- batch: 020 ----
mean loss: 303.18
 ---- batch: 030 ----
mean loss: 305.51
 ---- batch: 040 ----
mean loss: 301.13
train mean loss: 305.27
epoch train time: 0:00:08.510000
elapsed time: 0:10:36.294223
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 11:48:00.588489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.53
 ---- batch: 020 ----
mean loss: 288.60
 ---- batch: 030 ----
mean loss: 294.92
 ---- batch: 040 ----
mean loss: 291.97
train mean loss: 292.39
epoch train time: 0:00:08.588891
elapsed time: 0:10:44.884382
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 11:48:09.178639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.22
 ---- batch: 020 ----
mean loss: 295.75
 ---- batch: 030 ----
mean loss: 295.92
 ---- batch: 040 ----
mean loss: 297.01
train mean loss: 295.38
epoch train time: 0:00:08.501542
elapsed time: 0:10:53.387161
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 11:48:17.681419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.46
 ---- batch: 020 ----
mean loss: 284.12
 ---- batch: 030 ----
mean loss: 295.19
 ---- batch: 040 ----
mean loss: 290.40
train mean loss: 291.83
epoch train time: 0:00:08.488761
elapsed time: 0:11:01.877166
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 11:48:26.171409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.46
 ---- batch: 020 ----
mean loss: 285.35
 ---- batch: 030 ----
mean loss: 291.24
 ---- batch: 040 ----
mean loss: 294.34
train mean loss: 291.74
epoch train time: 0:00:08.477331
elapsed time: 0:11:10.355731
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 11:48:34.650026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.49
 ---- batch: 020 ----
mean loss: 290.70
 ---- batch: 030 ----
mean loss: 294.89
 ---- batch: 040 ----
mean loss: 279.90
train mean loss: 287.54
epoch train time: 0:00:08.488236
elapsed time: 0:11:18.845267
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 11:48:43.139504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.59
 ---- batch: 020 ----
mean loss: 285.85
 ---- batch: 030 ----
mean loss: 296.37
 ---- batch: 040 ----
mean loss: 275.16
train mean loss: 285.41
epoch train time: 0:00:08.480760
elapsed time: 0:11:27.327211
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 11:48:51.621450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.66
 ---- batch: 020 ----
mean loss: 283.30
 ---- batch: 030 ----
mean loss: 287.37
 ---- batch: 040 ----
mean loss: 283.42
train mean loss: 285.47
epoch train time: 0:00:08.513092
elapsed time: 0:11:35.841570
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 11:49:00.135864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.57
 ---- batch: 020 ----
mean loss: 279.11
 ---- batch: 030 ----
mean loss: 285.38
 ---- batch: 040 ----
mean loss: 275.56
train mean loss: 281.85
epoch train time: 0:00:08.505553
elapsed time: 0:11:44.348498
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 11:49:08.642732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.16
 ---- batch: 020 ----
mean loss: 283.32
 ---- batch: 030 ----
mean loss: 275.14
 ---- batch: 040 ----
mean loss: 284.08
train mean loss: 283.73
epoch train time: 0:00:08.483375
elapsed time: 0:11:52.833238
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 11:49:17.127552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.34
 ---- batch: 020 ----
mean loss: 277.09
 ---- batch: 030 ----
mean loss: 282.78
 ---- batch: 040 ----
mean loss: 277.92
train mean loss: 280.00
epoch train time: 0:00:08.492220
elapsed time: 0:12:01.326791
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 11:49:25.621088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.37
 ---- batch: 020 ----
mean loss: 277.00
 ---- batch: 030 ----
mean loss: 282.14
 ---- batch: 040 ----
mean loss: 269.46
train mean loss: 278.98
epoch train time: 0:00:08.498889
elapsed time: 0:12:09.826927
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 11:49:34.121174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.90
 ---- batch: 020 ----
mean loss: 276.32
 ---- batch: 030 ----
mean loss: 277.94
 ---- batch: 040 ----
mean loss: 278.94
train mean loss: 276.14
epoch train time: 0:00:08.468278
elapsed time: 0:12:18.296471
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 11:49:42.590712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.58
 ---- batch: 020 ----
mean loss: 273.98
 ---- batch: 030 ----
mean loss: 275.27
 ---- batch: 040 ----
mean loss: 274.21
train mean loss: 275.53
epoch train time: 0:00:08.496012
elapsed time: 0:12:26.793746
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 11:49:51.087926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.47
 ---- batch: 020 ----
mean loss: 277.03
 ---- batch: 030 ----
mean loss: 281.75
 ---- batch: 040 ----
mean loss: 266.56
train mean loss: 275.00
epoch train time: 0:00:08.557831
elapsed time: 0:12:35.352912
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 11:49:59.647671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.93
 ---- batch: 020 ----
mean loss: 272.69
 ---- batch: 030 ----
mean loss: 268.97
 ---- batch: 040 ----
mean loss: 264.04
train mean loss: 271.31
epoch train time: 0:00:08.511327
elapsed time: 0:12:43.865968
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 11:50:08.160277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.02
 ---- batch: 020 ----
mean loss: 270.44
 ---- batch: 030 ----
mean loss: 268.87
 ---- batch: 040 ----
mean loss: 262.70
train mean loss: 269.44
epoch train time: 0:00:08.483141
elapsed time: 0:12:52.350463
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 11:50:16.644766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.90
 ---- batch: 020 ----
mean loss: 261.97
 ---- batch: 030 ----
mean loss: 275.11
 ---- batch: 040 ----
mean loss: 266.30
train mean loss: 269.37
epoch train time: 0:00:08.477780
elapsed time: 0:13:00.829538
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 11:50:25.123797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.62
 ---- batch: 020 ----
mean loss: 267.29
 ---- batch: 030 ----
mean loss: 261.81
 ---- batch: 040 ----
mean loss: 266.00
train mean loss: 267.13
epoch train time: 0:00:08.495817
elapsed time: 0:13:09.326561
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 11:50:33.620816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.07
 ---- batch: 020 ----
mean loss: 274.33
 ---- batch: 030 ----
mean loss: 272.21
 ---- batch: 040 ----
mean loss: 268.73
train mean loss: 268.17
epoch train time: 0:00:08.510505
elapsed time: 0:13:17.838492
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 11:50:42.132727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.96
 ---- batch: 020 ----
mean loss: 264.28
 ---- batch: 030 ----
mean loss: 273.89
 ---- batch: 040 ----
mean loss: 260.71
train mean loss: 266.70
epoch train time: 0:00:08.506114
elapsed time: 0:13:26.345812
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 11:50:50.640214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.92
 ---- batch: 020 ----
mean loss: 269.68
 ---- batch: 030 ----
mean loss: 265.74
 ---- batch: 040 ----
mean loss: 264.15
train mean loss: 266.39
epoch train time: 0:00:08.484026
elapsed time: 0:13:34.831254
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 11:50:59.125533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.02
 ---- batch: 020 ----
mean loss: 269.16
 ---- batch: 030 ----
mean loss: 250.71
 ---- batch: 040 ----
mean loss: 262.27
train mean loss: 262.43
epoch train time: 0:00:08.495566
elapsed time: 0:13:43.328130
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 11:51:07.622403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.63
 ---- batch: 020 ----
mean loss: 257.67
 ---- batch: 030 ----
mean loss: 261.97
 ---- batch: 040 ----
mean loss: 258.01
train mean loss: 262.02
epoch train time: 0:00:08.505074
elapsed time: 0:13:51.834473
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 11:51:16.128765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.41
 ---- batch: 020 ----
mean loss: 257.89
 ---- batch: 030 ----
mean loss: 265.34
 ---- batch: 040 ----
mean loss: 261.77
train mean loss: 264.18
epoch train time: 0:00:08.559358
elapsed time: 0:14:00.395068
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 11:51:24.689308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.13
 ---- batch: 020 ----
mean loss: 263.23
 ---- batch: 030 ----
mean loss: 260.24
 ---- batch: 040 ----
mean loss: 262.26
train mean loss: 262.46
epoch train time: 0:00:08.504120
elapsed time: 0:14:08.900372
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 11:51:33.194611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.28
 ---- batch: 020 ----
mean loss: 260.56
 ---- batch: 030 ----
mean loss: 263.45
 ---- batch: 040 ----
mean loss: 255.61
train mean loss: 258.83
epoch train time: 0:00:08.485100
elapsed time: 0:14:17.386825
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 11:51:41.681097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.78
 ---- batch: 020 ----
mean loss: 259.22
 ---- batch: 030 ----
mean loss: 267.91
 ---- batch: 040 ----
mean loss: 263.76
train mean loss: 258.01
epoch train time: 0:00:08.518677
elapsed time: 0:14:25.906693
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 11:51:50.200940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.34
 ---- batch: 020 ----
mean loss: 261.33
 ---- batch: 030 ----
mean loss: 256.74
 ---- batch: 040 ----
mean loss: 266.65
train mean loss: 259.91
epoch train time: 0:00:08.499155
elapsed time: 0:14:34.407037
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 11:51:58.701289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.54
 ---- batch: 020 ----
mean loss: 265.82
 ---- batch: 030 ----
mean loss: 254.76
 ---- batch: 040 ----
mean loss: 245.36
train mean loss: 255.77
epoch train time: 0:00:08.507722
elapsed time: 0:14:42.916004
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 11:52:07.210258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.11
 ---- batch: 020 ----
mean loss: 259.70
 ---- batch: 030 ----
mean loss: 254.82
 ---- batch: 040 ----
mean loss: 252.44
train mean loss: 255.79
epoch train time: 0:00:08.494390
elapsed time: 0:14:51.411758
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 11:52:15.706003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.64
 ---- batch: 020 ----
mean loss: 253.66
 ---- batch: 030 ----
mean loss: 250.82
 ---- batch: 040 ----
mean loss: 254.74
train mean loss: 253.36
epoch train time: 0:00:08.486949
elapsed time: 0:14:59.899887
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 11:52:24.194156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.16
 ---- batch: 020 ----
mean loss: 256.50
 ---- batch: 030 ----
mean loss: 256.49
 ---- batch: 040 ----
mean loss: 251.77
train mean loss: 252.56
epoch train time: 0:00:08.483185
elapsed time: 0:15:08.384398
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 11:52:32.678662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.16
 ---- batch: 020 ----
mean loss: 257.64
 ---- batch: 030 ----
mean loss: 256.47
 ---- batch: 040 ----
mean loss: 246.30
train mean loss: 255.18
epoch train time: 0:00:08.470024
elapsed time: 0:15:16.855727
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 11:52:41.150018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.88
 ---- batch: 020 ----
mean loss: 261.91
 ---- batch: 030 ----
mean loss: 251.44
 ---- batch: 040 ----
mean loss: 246.45
train mean loss: 252.59
epoch train time: 0:00:08.464259
elapsed time: 0:15:25.321479
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 11:52:49.615573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.40
 ---- batch: 020 ----
mean loss: 249.06
 ---- batch: 030 ----
mean loss: 253.03
 ---- batch: 040 ----
mean loss: 246.31
train mean loss: 249.78
epoch train time: 0:00:08.476347
elapsed time: 0:15:33.798918
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 11:52:58.093176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.75
 ---- batch: 020 ----
mean loss: 249.36
 ---- batch: 030 ----
mean loss: 254.03
 ---- batch: 040 ----
mean loss: 252.77
train mean loss: 252.31
epoch train time: 0:00:08.469965
elapsed time: 0:15:42.270099
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 11:53:06.564350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.86
 ---- batch: 020 ----
mean loss: 247.27
 ---- batch: 030 ----
mean loss: 239.65
 ---- batch: 040 ----
mean loss: 250.21
train mean loss: 247.55
epoch train time: 0:00:08.547932
elapsed time: 0:15:50.819321
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 11:53:15.113583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.45
 ---- batch: 020 ----
mean loss: 253.32
 ---- batch: 030 ----
mean loss: 251.37
 ---- batch: 040 ----
mean loss: 252.98
train mean loss: 250.31
epoch train time: 0:00:08.509779
elapsed time: 0:15:59.330296
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 11:53:23.624548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.19
 ---- batch: 020 ----
mean loss: 243.03
 ---- batch: 030 ----
mean loss: 247.65
 ---- batch: 040 ----
mean loss: 245.96
train mean loss: 246.66
epoch train time: 0:00:08.474684
elapsed time: 0:16:07.806324
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 11:53:32.100555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.17
 ---- batch: 020 ----
mean loss: 245.98
 ---- batch: 030 ----
mean loss: 247.71
 ---- batch: 040 ----
mean loss: 248.64
train mean loss: 248.97
epoch train time: 0:00:08.470170
elapsed time: 0:16:16.277677
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 11:53:40.571972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.41
 ---- batch: 020 ----
mean loss: 236.31
 ---- batch: 030 ----
mean loss: 242.74
 ---- batch: 040 ----
mean loss: 254.99
train mean loss: 244.75
epoch train time: 0:00:08.510368
elapsed time: 0:16:24.789361
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 11:53:49.083628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.73
 ---- batch: 020 ----
mean loss: 248.11
 ---- batch: 030 ----
mean loss: 240.95
 ---- batch: 040 ----
mean loss: 244.12
train mean loss: 244.36
epoch train time: 0:00:08.494897
elapsed time: 0:16:33.285463
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 11:53:57.579731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.29
 ---- batch: 020 ----
mean loss: 254.11
 ---- batch: 030 ----
mean loss: 245.95
 ---- batch: 040 ----
mean loss: 252.25
train mean loss: 250.28
epoch train time: 0:00:08.495021
elapsed time: 0:16:41.781801
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 11:54:06.076077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.11
 ---- batch: 020 ----
mean loss: 239.52
 ---- batch: 030 ----
mean loss: 239.88
 ---- batch: 040 ----
mean loss: 252.65
train mean loss: 246.42
epoch train time: 0:00:08.494476
elapsed time: 0:16:50.277504
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 11:54:14.571767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.35
 ---- batch: 020 ----
mean loss: 241.64
 ---- batch: 030 ----
mean loss: 247.33
 ---- batch: 040 ----
mean loss: 246.45
train mean loss: 246.30
epoch train time: 0:00:08.479901
elapsed time: 0:16:58.758741
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 11:54:23.053018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.32
 ---- batch: 020 ----
mean loss: 246.83
 ---- batch: 030 ----
mean loss: 252.18
 ---- batch: 040 ----
mean loss: 236.63
train mean loss: 244.00
epoch train time: 0:00:08.467973
elapsed time: 0:17:07.227970
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 11:54:31.522219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.82
 ---- batch: 020 ----
mean loss: 241.19
 ---- batch: 030 ----
mean loss: 242.07
 ---- batch: 040 ----
mean loss: 249.02
train mean loss: 241.95
epoch train time: 0:00:08.465342
elapsed time: 0:17:15.694492
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 11:54:39.988745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.42
 ---- batch: 020 ----
mean loss: 246.36
 ---- batch: 030 ----
mean loss: 248.11
 ---- batch: 040 ----
mean loss: 247.46
train mean loss: 244.38
epoch train time: 0:00:08.467390
elapsed time: 0:17:24.163121
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 11:54:48.457382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.69
 ---- batch: 020 ----
mean loss: 236.21
 ---- batch: 030 ----
mean loss: 243.38
 ---- batch: 040 ----
mean loss: 239.66
train mean loss: 239.88
epoch train time: 0:00:08.484957
elapsed time: 0:17:32.649452
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 11:54:56.943709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.24
 ---- batch: 020 ----
mean loss: 240.83
 ---- batch: 030 ----
mean loss: 242.29
 ---- batch: 040 ----
mean loss: 232.17
train mean loss: 238.39
epoch train time: 0:00:08.512630
elapsed time: 0:17:41.163327
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 11:55:05.457656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.20
 ---- batch: 020 ----
mean loss: 235.91
 ---- batch: 030 ----
mean loss: 239.08
 ---- batch: 040 ----
mean loss: 241.12
train mean loss: 238.29
epoch train time: 0:00:08.493030
elapsed time: 0:17:49.657631
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 11:55:13.951923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.70
 ---- batch: 020 ----
mean loss: 239.71
 ---- batch: 030 ----
mean loss: 234.96
 ---- batch: 040 ----
mean loss: 243.46
train mean loss: 237.46
epoch train time: 0:00:08.479713
elapsed time: 0:17:58.138552
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 11:55:22.432840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.73
 ---- batch: 020 ----
mean loss: 233.14
 ---- batch: 030 ----
mean loss: 239.35
 ---- batch: 040 ----
mean loss: 241.56
train mean loss: 237.75
epoch train time: 0:00:08.459561
elapsed time: 0:18:06.599458
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 11:55:30.893738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.72
 ---- batch: 020 ----
mean loss: 240.12
 ---- batch: 030 ----
mean loss: 245.11
 ---- batch: 040 ----
mean loss: 227.85
train mean loss: 237.50
epoch train time: 0:00:08.444133
elapsed time: 0:18:15.045071
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 11:55:39.339157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.64
 ---- batch: 020 ----
mean loss: 246.53
 ---- batch: 030 ----
mean loss: 229.63
 ---- batch: 040 ----
mean loss: 238.08
train mean loss: 238.06
epoch train time: 0:00:08.463389
elapsed time: 0:18:23.509644
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 11:55:47.803838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.73
 ---- batch: 020 ----
mean loss: 237.25
 ---- batch: 030 ----
mean loss: 230.56
 ---- batch: 040 ----
mean loss: 234.93
train mean loss: 235.87
epoch train time: 0:00:08.479352
elapsed time: 0:18:31.990201
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 11:55:56.284446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.94
 ---- batch: 020 ----
mean loss: 235.82
 ---- batch: 030 ----
mean loss: 230.50
 ---- batch: 040 ----
mean loss: 235.83
train mean loss: 233.87
epoch train time: 0:00:08.490467
elapsed time: 0:18:40.481863
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 11:56:04.776149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.82
 ---- batch: 020 ----
mean loss: 233.42
 ---- batch: 030 ----
mean loss: 228.40
 ---- batch: 040 ----
mean loss: 232.57
train mean loss: 234.32
epoch train time: 0:00:08.475091
elapsed time: 0:18:48.958231
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 11:56:13.252503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.57
 ---- batch: 020 ----
mean loss: 234.92
 ---- batch: 030 ----
mean loss: 237.57
 ---- batch: 040 ----
mean loss: 227.45
train mean loss: 234.77
epoch train time: 0:00:08.448656
elapsed time: 0:18:57.408174
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 11:56:21.702418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.66
 ---- batch: 020 ----
mean loss: 235.33
 ---- batch: 030 ----
mean loss: 229.98
 ---- batch: 040 ----
mean loss: 239.90
train mean loss: 233.00
epoch train time: 0:00:08.460297
elapsed time: 0:19:05.869727
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 11:56:30.164001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.83
 ---- batch: 020 ----
mean loss: 227.89
 ---- batch: 030 ----
mean loss: 229.66
 ---- batch: 040 ----
mean loss: 224.43
train mean loss: 230.55
epoch train time: 0:00:08.465249
elapsed time: 0:19:14.336233
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 11:56:38.630517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.66
 ---- batch: 020 ----
mean loss: 231.25
 ---- batch: 030 ----
mean loss: 230.79
 ---- batch: 040 ----
mean loss: 232.56
train mean loss: 232.55
epoch train time: 0:00:08.477023
elapsed time: 0:19:22.814504
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 11:56:47.108798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.44
 ---- batch: 020 ----
mean loss: 232.32
 ---- batch: 030 ----
mean loss: 239.53
 ---- batch: 040 ----
mean loss: 235.03
train mean loss: 232.14
epoch train time: 0:00:08.460353
elapsed time: 0:19:31.276165
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 11:56:55.570455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.73
 ---- batch: 020 ----
mean loss: 231.16
 ---- batch: 030 ----
mean loss: 226.36
 ---- batch: 040 ----
mean loss: 230.12
train mean loss: 229.73
epoch train time: 0:00:08.446454
elapsed time: 0:19:39.723977
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 11:57:04.018255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.02
 ---- batch: 020 ----
mean loss: 238.83
 ---- batch: 030 ----
mean loss: 229.43
 ---- batch: 040 ----
mean loss: 226.51
train mean loss: 231.06
epoch train time: 0:00:08.512070
elapsed time: 0:19:48.237388
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 11:57:12.531664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.58
 ---- batch: 020 ----
mean loss: 229.74
 ---- batch: 030 ----
mean loss: 235.58
 ---- batch: 040 ----
mean loss: 231.52
train mean loss: 231.56
epoch train time: 0:00:08.409592
elapsed time: 0:19:56.648282
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 11:57:20.942680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.08
 ---- batch: 020 ----
mean loss: 229.66
 ---- batch: 030 ----
mean loss: 225.36
 ---- batch: 040 ----
mean loss: 237.33
train mean loss: 229.31
epoch train time: 0:00:08.374698
elapsed time: 0:20:05.024359
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 11:57:29.318628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.31
 ---- batch: 020 ----
mean loss: 229.88
 ---- batch: 030 ----
mean loss: 228.17
 ---- batch: 040 ----
mean loss: 235.41
train mean loss: 230.64
epoch train time: 0:00:08.382437
elapsed time: 0:20:13.408049
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 11:57:37.702350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.05
 ---- batch: 020 ----
mean loss: 222.42
 ---- batch: 030 ----
mean loss: 223.66
 ---- batch: 040 ----
mean loss: 228.20
train mean loss: 226.07
epoch train time: 0:00:08.378006
elapsed time: 0:20:21.787353
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 11:57:46.081712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.13
 ---- batch: 020 ----
mean loss: 229.51
 ---- batch: 030 ----
mean loss: 228.15
 ---- batch: 040 ----
mean loss: 230.77
train mean loss: 228.64
epoch train time: 0:00:08.393355
elapsed time: 0:20:30.182149
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 11:57:54.476447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.57
 ---- batch: 020 ----
mean loss: 221.47
 ---- batch: 030 ----
mean loss: 229.97
 ---- batch: 040 ----
mean loss: 226.50
train mean loss: 225.81
epoch train time: 0:00:08.392021
elapsed time: 0:20:38.575408
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 11:58:02.869707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.38
 ---- batch: 020 ----
mean loss: 223.93
 ---- batch: 030 ----
mean loss: 221.88
 ---- batch: 040 ----
mean loss: 234.55
train mean loss: 227.01
epoch train time: 0:00:08.367286
elapsed time: 0:20:46.943937
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 11:58:11.238201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.32
 ---- batch: 020 ----
mean loss: 222.00
 ---- batch: 030 ----
mean loss: 231.78
 ---- batch: 040 ----
mean loss: 224.99
train mean loss: 228.14
epoch train time: 0:00:08.321764
elapsed time: 0:20:55.266930
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 11:58:19.561187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.80
 ---- batch: 020 ----
mean loss: 233.44
 ---- batch: 030 ----
mean loss: 229.21
 ---- batch: 040 ----
mean loss: 218.39
train mean loss: 226.60
epoch train time: 0:00:08.392695
elapsed time: 0:21:03.660841
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 11:58:27.955126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.34
 ---- batch: 020 ----
mean loss: 228.61
 ---- batch: 030 ----
mean loss: 230.88
 ---- batch: 040 ----
mean loss: 234.18
train mean loss: 227.94
epoch train time: 0:00:08.342987
elapsed time: 0:21:12.005053
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 11:58:36.299316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.96
 ---- batch: 020 ----
mean loss: 216.66
 ---- batch: 030 ----
mean loss: 227.93
 ---- batch: 040 ----
mean loss: 225.36
train mean loss: 223.87
epoch train time: 0:00:08.352976
elapsed time: 0:21:20.359482
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 11:58:44.653510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.12
 ---- batch: 020 ----
mean loss: 222.06
 ---- batch: 030 ----
mean loss: 219.31
 ---- batch: 040 ----
mean loss: 223.46
train mean loss: 222.38
epoch train time: 0:00:08.501286
elapsed time: 0:21:28.862250
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 11:58:53.156893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.49
 ---- batch: 020 ----
mean loss: 228.70
 ---- batch: 030 ----
mean loss: 227.43
 ---- batch: 040 ----
mean loss: 223.80
train mean loss: 225.61
epoch train time: 0:00:08.427111
elapsed time: 0:21:37.291021
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 11:59:01.585295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.12
 ---- batch: 020 ----
mean loss: 220.57
 ---- batch: 030 ----
mean loss: 222.62
 ---- batch: 040 ----
mean loss: 223.51
train mean loss: 224.89
epoch train time: 0:00:08.375245
elapsed time: 0:21:45.667493
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 11:59:09.961770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.05
 ---- batch: 020 ----
mean loss: 224.82
 ---- batch: 030 ----
mean loss: 219.64
 ---- batch: 040 ----
mean loss: 224.01
train mean loss: 222.10
epoch train time: 0:00:08.393150
elapsed time: 0:21:54.061891
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 11:59:18.356140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.05
 ---- batch: 020 ----
mean loss: 226.18
 ---- batch: 030 ----
mean loss: 226.26
 ---- batch: 040 ----
mean loss: 218.10
train mean loss: 224.54
epoch train time: 0:00:08.411364
elapsed time: 0:22:02.474547
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 11:59:26.768797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.60
 ---- batch: 020 ----
mean loss: 234.06
 ---- batch: 030 ----
mean loss: 225.21
 ---- batch: 040 ----
mean loss: 217.09
train mean loss: 225.11
epoch train time: 0:00:08.496147
elapsed time: 0:22:10.971906
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 11:59:35.266142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.90
 ---- batch: 020 ----
mean loss: 217.44
 ---- batch: 030 ----
mean loss: 219.32
 ---- batch: 040 ----
mean loss: 224.92
train mean loss: 220.60
epoch train time: 0:00:08.417034
elapsed time: 0:22:19.390193
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 11:59:43.684447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.93
 ---- batch: 020 ----
mean loss: 219.65
 ---- batch: 030 ----
mean loss: 220.13
 ---- batch: 040 ----
mean loss: 218.21
train mean loss: 220.95
epoch train time: 0:00:08.466294
elapsed time: 0:22:27.857681
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 11:59:52.151908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.19
 ---- batch: 020 ----
mean loss: 217.52
 ---- batch: 030 ----
mean loss: 221.04
 ---- batch: 040 ----
mean loss: 229.22
train mean loss: 224.03
epoch train time: 0:00:08.416809
elapsed time: 0:22:36.275623
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 12:00:00.569885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.69
 ---- batch: 020 ----
mean loss: 216.15
 ---- batch: 030 ----
mean loss: 214.24
 ---- batch: 040 ----
mean loss: 227.91
train mean loss: 219.93
epoch train time: 0:00:08.445045
elapsed time: 0:22:44.722053
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 12:00:09.016321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.82
 ---- batch: 020 ----
mean loss: 221.84
 ---- batch: 030 ----
mean loss: 221.02
 ---- batch: 040 ----
mean loss: 211.16
train mean loss: 218.49
epoch train time: 0:00:08.386822
elapsed time: 0:22:53.110089
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 12:00:17.404331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.66
 ---- batch: 020 ----
mean loss: 222.48
 ---- batch: 030 ----
mean loss: 221.13
 ---- batch: 040 ----
mean loss: 216.05
train mean loss: 221.17
epoch train time: 0:00:08.417768
elapsed time: 0:23:01.529041
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 12:00:25.823289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.48
 ---- batch: 020 ----
mean loss: 218.11
 ---- batch: 030 ----
mean loss: 218.83
 ---- batch: 040 ----
mean loss: 224.24
train mean loss: 218.77
epoch train time: 0:00:08.447901
elapsed time: 0:23:09.978192
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 12:00:34.272458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.32
 ---- batch: 020 ----
mean loss: 209.97
 ---- batch: 030 ----
mean loss: 222.95
 ---- batch: 040 ----
mean loss: 222.31
train mean loss: 218.11
epoch train time: 0:00:08.445776
elapsed time: 0:23:18.425206
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 12:00:42.719455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.42
 ---- batch: 020 ----
mean loss: 218.38
 ---- batch: 030 ----
mean loss: 223.51
 ---- batch: 040 ----
mean loss: 225.22
train mean loss: 222.61
epoch train time: 0:00:08.471809
elapsed time: 0:23:26.898259
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 12:00:51.192502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.50
 ---- batch: 020 ----
mean loss: 223.82
 ---- batch: 030 ----
mean loss: 214.33
 ---- batch: 040 ----
mean loss: 220.41
train mean loss: 219.86
epoch train time: 0:00:08.446494
elapsed time: 0:23:35.346008
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 12:00:59.640262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.30
 ---- batch: 020 ----
mean loss: 211.61
 ---- batch: 030 ----
mean loss: 215.62
 ---- batch: 040 ----
mean loss: 225.15
train mean loss: 217.40
epoch train time: 0:00:08.447987
elapsed time: 0:23:43.795211
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 12:01:08.089440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.14
 ---- batch: 020 ----
mean loss: 223.26
 ---- batch: 030 ----
mean loss: 220.51
 ---- batch: 040 ----
mean loss: 214.06
train mean loss: 219.55
epoch train time: 0:00:08.437766
elapsed time: 0:23:52.234183
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 12:01:16.528431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.91
 ---- batch: 020 ----
mean loss: 218.38
 ---- batch: 030 ----
mean loss: 210.37
 ---- batch: 040 ----
mean loss: 217.13
train mean loss: 217.37
epoch train time: 0:00:08.488741
elapsed time: 0:24:00.724369
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 12:01:25.018694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.02
 ---- batch: 020 ----
mean loss: 219.74
 ---- batch: 030 ----
mean loss: 223.27
 ---- batch: 040 ----
mean loss: 212.03
train mean loss: 219.31
epoch train time: 0:00:08.446778
elapsed time: 0:24:09.172649
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 12:01:33.466893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.95
 ---- batch: 020 ----
mean loss: 211.01
 ---- batch: 030 ----
mean loss: 220.27
 ---- batch: 040 ----
mean loss: 222.84
train mean loss: 217.14
epoch train time: 0:00:08.433990
elapsed time: 0:24:17.607910
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 12:01:41.902107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.32
 ---- batch: 020 ----
mean loss: 218.54
 ---- batch: 030 ----
mean loss: 205.35
 ---- batch: 040 ----
mean loss: 227.95
train mean loss: 218.54
epoch train time: 0:00:08.433619
elapsed time: 0:24:26.042806
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 12:01:50.337123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.63
 ---- batch: 020 ----
mean loss: 212.40
 ---- batch: 030 ----
mean loss: 215.76
 ---- batch: 040 ----
mean loss: 222.33
train mean loss: 215.76
epoch train time: 0:00:08.447745
elapsed time: 0:24:34.491848
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 12:01:58.786079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.22
 ---- batch: 020 ----
mean loss: 213.84
 ---- batch: 030 ----
mean loss: 218.90
 ---- batch: 040 ----
mean loss: 218.15
train mean loss: 215.73
epoch train time: 0:00:08.409352
elapsed time: 0:24:42.902621
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 12:02:07.196896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.50
 ---- batch: 020 ----
mean loss: 219.70
 ---- batch: 030 ----
mean loss: 213.26
 ---- batch: 040 ----
mean loss: 214.63
train mean loss: 217.22
epoch train time: 0:00:08.508770
elapsed time: 0:24:51.412887
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 12:02:15.706891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.79
 ---- batch: 020 ----
mean loss: 218.86
 ---- batch: 030 ----
mean loss: 213.35
 ---- batch: 040 ----
mean loss: 219.19
train mean loss: 215.96
epoch train time: 0:00:08.459171
elapsed time: 0:24:59.872986
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 12:02:24.167220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.13
 ---- batch: 020 ----
mean loss: 209.79
 ---- batch: 030 ----
mean loss: 217.99
 ---- batch: 040 ----
mean loss: 221.01
train mean loss: 214.61
epoch train time: 0:00:08.455286
elapsed time: 0:25:08.329515
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 12:02:32.623758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.58
 ---- batch: 020 ----
mean loss: 219.22
 ---- batch: 030 ----
mean loss: 213.39
 ---- batch: 040 ----
mean loss: 215.50
train mean loss: 216.03
epoch train time: 0:00:08.447810
elapsed time: 0:25:16.778531
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 12:02:41.072770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.89
 ---- batch: 020 ----
mean loss: 210.62
 ---- batch: 030 ----
mean loss: 207.46
 ---- batch: 040 ----
mean loss: 217.60
train mean loss: 213.37
epoch train time: 0:00:08.482446
elapsed time: 0:25:25.262217
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 12:02:49.556462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.11
 ---- batch: 020 ----
mean loss: 210.09
 ---- batch: 030 ----
mean loss: 214.32
 ---- batch: 040 ----
mean loss: 210.24
train mean loss: 214.54
epoch train time: 0:00:08.433691
elapsed time: 0:25:33.697096
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 12:02:57.991356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.30
 ---- batch: 020 ----
mean loss: 210.04
 ---- batch: 030 ----
mean loss: 216.30
 ---- batch: 040 ----
mean loss: 220.47
train mean loss: 214.27
epoch train time: 0:00:08.394871
elapsed time: 0:25:42.093269
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 12:03:06.387572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.88
 ---- batch: 020 ----
mean loss: 216.31
 ---- batch: 030 ----
mean loss: 217.87
 ---- batch: 040 ----
mean loss: 205.80
train mean loss: 212.62
epoch train time: 0:00:08.449828
elapsed time: 0:25:50.544464
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 12:03:14.838622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.92
 ---- batch: 020 ----
mean loss: 208.75
 ---- batch: 030 ----
mean loss: 210.17
 ---- batch: 040 ----
mean loss: 214.93
train mean loss: 211.63
epoch train time: 0:00:08.474457
elapsed time: 0:25:59.020092
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 12:03:23.314334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.80
 ---- batch: 020 ----
mean loss: 209.25
 ---- batch: 030 ----
mean loss: 210.83
 ---- batch: 040 ----
mean loss: 213.04
train mean loss: 212.46
epoch train time: 0:00:08.414524
elapsed time: 0:26:07.435883
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 12:03:31.730245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.45
 ---- batch: 020 ----
mean loss: 214.88
 ---- batch: 030 ----
mean loss: 216.29
 ---- batch: 040 ----
mean loss: 207.36
train mean loss: 211.13
epoch train time: 0:00:08.505114
elapsed time: 0:26:15.942400
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 12:03:40.236673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.72
 ---- batch: 020 ----
mean loss: 218.10
 ---- batch: 030 ----
mean loss: 211.74
 ---- batch: 040 ----
mean loss: 210.41
train mean loss: 212.72
epoch train time: 0:00:08.430090
elapsed time: 0:26:24.373704
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 12:03:48.667980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.94
 ---- batch: 020 ----
mean loss: 217.86
 ---- batch: 030 ----
mean loss: 214.19
 ---- batch: 040 ----
mean loss: 212.09
train mean loss: 215.10
epoch train time: 0:00:08.441078
elapsed time: 0:26:32.816017
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 12:03:57.110298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.53
 ---- batch: 020 ----
mean loss: 226.24
 ---- batch: 030 ----
mean loss: 213.34
 ---- batch: 040 ----
mean loss: 217.13
train mean loss: 215.84
epoch train time: 0:00:08.374197
elapsed time: 0:26:41.191435
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 12:04:05.485736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.48
 ---- batch: 020 ----
mean loss: 211.31
 ---- batch: 030 ----
mean loss: 216.55
 ---- batch: 040 ----
mean loss: 204.65
train mean loss: 210.75
epoch train time: 0:00:08.414391
elapsed time: 0:26:49.607143
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 12:04:13.901369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.21
 ---- batch: 020 ----
mean loss: 214.46
 ---- batch: 030 ----
mean loss: 207.28
 ---- batch: 040 ----
mean loss: 209.25
train mean loss: 210.89
epoch train time: 0:00:08.419134
elapsed time: 0:26:58.027450
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 12:04:22.321770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.51
 ---- batch: 020 ----
mean loss: 217.19
 ---- batch: 030 ----
mean loss: 214.44
 ---- batch: 040 ----
mean loss: 212.60
train mean loss: 210.59
epoch train time: 0:00:08.351322
elapsed time: 0:27:06.380154
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 12:04:30.674425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.62
 ---- batch: 020 ----
mean loss: 213.63
 ---- batch: 030 ----
mean loss: 204.38
 ---- batch: 040 ----
mean loss: 208.26
train mean loss: 210.28
epoch train time: 0:00:08.451653
elapsed time: 0:27:14.833058
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 12:04:39.127306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.62
 ---- batch: 020 ----
mean loss: 213.99
 ---- batch: 030 ----
mean loss: 211.01
 ---- batch: 040 ----
mean loss: 207.89
train mean loss: 210.15
epoch train time: 0:00:08.419092
elapsed time: 0:27:23.253312
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 12:04:47.547545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.30
 ---- batch: 020 ----
mean loss: 210.87
 ---- batch: 030 ----
mean loss: 213.65
 ---- batch: 040 ----
mean loss: 206.55
train mean loss: 209.72
epoch train time: 0:00:08.436919
elapsed time: 0:27:31.691463
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 12:04:55.985766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.40
 ---- batch: 020 ----
mean loss: 206.90
 ---- batch: 030 ----
mean loss: 208.83
 ---- batch: 040 ----
mean loss: 211.78
train mean loss: 212.05
epoch train time: 0:00:08.442379
elapsed time: 0:27:40.135184
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 12:05:04.429478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.06
 ---- batch: 020 ----
mean loss: 211.19
 ---- batch: 030 ----
mean loss: 211.03
 ---- batch: 040 ----
mean loss: 208.60
train mean loss: 210.21
epoch train time: 0:00:08.515422
elapsed time: 0:27:48.652118
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 12:05:12.946351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.55
 ---- batch: 020 ----
mean loss: 203.11
 ---- batch: 030 ----
mean loss: 209.05
 ---- batch: 040 ----
mean loss: 215.22
train mean loss: 208.94
epoch train time: 0:00:08.475776
elapsed time: 0:27:57.129169
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 12:05:21.423434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.40
 ---- batch: 020 ----
mean loss: 210.04
 ---- batch: 030 ----
mean loss: 206.40
 ---- batch: 040 ----
mean loss: 206.89
train mean loss: 209.01
epoch train time: 0:00:08.401502
elapsed time: 0:28:05.531936
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 12:05:29.826250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.37
 ---- batch: 020 ----
mean loss: 211.35
 ---- batch: 030 ----
mean loss: 208.27
 ---- batch: 040 ----
mean loss: 214.43
train mean loss: 209.67
epoch train time: 0:00:08.487101
elapsed time: 0:28:14.020445
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 12:05:38.314647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.05
 ---- batch: 020 ----
mean loss: 217.66
 ---- batch: 030 ----
mean loss: 204.57
 ---- batch: 040 ----
mean loss: 208.34
train mean loss: 208.07
epoch train time: 0:00:08.461185
elapsed time: 0:28:22.482878
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 12:05:46.777129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.87
 ---- batch: 020 ----
mean loss: 207.44
 ---- batch: 030 ----
mean loss: 211.70
 ---- batch: 040 ----
mean loss: 210.67
train mean loss: 209.21
epoch train time: 0:00:08.424414
elapsed time: 0:28:30.908522
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 12:05:55.202777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.80
 ---- batch: 020 ----
mean loss: 216.85
 ---- batch: 030 ----
mean loss: 208.25
 ---- batch: 040 ----
mean loss: 214.88
train mean loss: 210.49
epoch train time: 0:00:08.390791
elapsed time: 0:28:39.300562
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 12:06:03.594865
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.39
 ---- batch: 020 ----
mean loss: 207.81
 ---- batch: 030 ----
mean loss: 206.31
 ---- batch: 040 ----
mean loss: 210.15
train mean loss: 205.94
epoch train time: 0:00:08.426187
elapsed time: 0:28:47.728343
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 12:06:12.022340
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.99
 ---- batch: 020 ----
mean loss: 197.63
 ---- batch: 030 ----
mean loss: 206.74
 ---- batch: 040 ----
mean loss: 205.87
train mean loss: 205.02
epoch train time: 0:00:08.464402
elapsed time: 0:28:56.193674
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 12:06:20.487896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.75
 ---- batch: 020 ----
mean loss: 203.48
 ---- batch: 030 ----
mean loss: 208.10
 ---- batch: 040 ----
mean loss: 196.35
train mean loss: 205.05
epoch train time: 0:00:08.375066
elapsed time: 0:29:04.570158
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 12:06:28.864461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.96
 ---- batch: 020 ----
mean loss: 205.49
 ---- batch: 030 ----
mean loss: 210.69
 ---- batch: 040 ----
mean loss: 209.76
train mean loss: 206.30
epoch train time: 0:00:08.540779
elapsed time: 0:29:13.112416
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 12:06:37.406697
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.60
 ---- batch: 020 ----
mean loss: 205.46
 ---- batch: 030 ----
mean loss: 200.46
 ---- batch: 040 ----
mean loss: 206.10
train mean loss: 205.37
epoch train time: 0:00:08.472631
elapsed time: 0:29:21.586390
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 12:06:45.880633
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.92
 ---- batch: 020 ----
mean loss: 203.26
 ---- batch: 030 ----
mean loss: 205.06
 ---- batch: 040 ----
mean loss: 204.44
train mean loss: 204.97
epoch train time: 0:00:08.411820
elapsed time: 0:29:29.999473
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 12:06:54.293732
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.51
 ---- batch: 020 ----
mean loss: 200.78
 ---- batch: 030 ----
mean loss: 202.67
 ---- batch: 040 ----
mean loss: 206.70
train mean loss: 205.33
epoch train time: 0:00:08.407920
elapsed time: 0:29:38.408669
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 12:07:02.702942
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.80
 ---- batch: 020 ----
mean loss: 210.48
 ---- batch: 030 ----
mean loss: 203.96
 ---- batch: 040 ----
mean loss: 203.87
train mean loss: 205.18
epoch train time: 0:00:08.438183
elapsed time: 0:29:46.848142
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 12:07:11.142425
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.45
 ---- batch: 020 ----
mean loss: 206.53
 ---- batch: 030 ----
mean loss: 203.73
 ---- batch: 040 ----
mean loss: 210.01
train mean loss: 205.77
epoch train time: 0:00:08.486438
elapsed time: 0:29:55.335857
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 12:07:19.630086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.26
 ---- batch: 020 ----
mean loss: 213.89
 ---- batch: 030 ----
mean loss: 206.07
 ---- batch: 040 ----
mean loss: 198.68
train mean loss: 205.39
epoch train time: 0:00:08.419400
elapsed time: 0:30:03.756555
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 12:07:28.050795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.25
 ---- batch: 020 ----
mean loss: 210.04
 ---- batch: 030 ----
mean loss: 208.17
 ---- batch: 040 ----
mean loss: 196.90
train mean loss: 205.38
epoch train time: 0:00:08.521034
elapsed time: 0:30:12.278818
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 12:07:36.573058
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.60
 ---- batch: 020 ----
mean loss: 198.79
 ---- batch: 030 ----
mean loss: 205.25
 ---- batch: 040 ----
mean loss: 204.01
train mean loss: 206.09
epoch train time: 0:00:08.401376
elapsed time: 0:30:20.681395
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 12:07:44.975627
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.76
 ---- batch: 020 ----
mean loss: 205.43
 ---- batch: 030 ----
mean loss: 204.20
 ---- batch: 040 ----
mean loss: 202.22
train mean loss: 204.78
epoch train time: 0:00:08.340630
elapsed time: 0:30:29.023294
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 12:07:53.317569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.89
 ---- batch: 020 ----
mean loss: 206.85
 ---- batch: 030 ----
mean loss: 200.15
 ---- batch: 040 ----
mean loss: 207.76
train mean loss: 205.34
epoch train time: 0:00:08.477520
elapsed time: 0:30:37.502166
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 12:08:01.796453
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.57
 ---- batch: 020 ----
mean loss: 199.96
 ---- batch: 030 ----
mean loss: 206.02
 ---- batch: 040 ----
mean loss: 207.44
train mean loss: 205.21
epoch train time: 0:00:08.426528
elapsed time: 0:30:45.929883
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 12:08:10.224173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.68
 ---- batch: 020 ----
mean loss: 201.36
 ---- batch: 030 ----
mean loss: 203.26
 ---- batch: 040 ----
mean loss: 205.35
train mean loss: 205.28
epoch train time: 0:00:08.362851
elapsed time: 0:30:54.294169
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 12:08:18.588410
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.13
 ---- batch: 020 ----
mean loss: 199.06
 ---- batch: 030 ----
mean loss: 205.65
 ---- batch: 040 ----
mean loss: 209.68
train mean loss: 204.88
epoch train time: 0:00:08.334065
elapsed time: 0:31:02.629430
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 12:08:26.923661
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.27
 ---- batch: 020 ----
mean loss: 205.12
 ---- batch: 030 ----
mean loss: 207.39
 ---- batch: 040 ----
mean loss: 203.10
train mean loss: 204.89
epoch train time: 0:00:08.379368
elapsed time: 0:31:11.010056
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 12:08:35.304364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.71
 ---- batch: 020 ----
mean loss: 211.09
 ---- batch: 030 ----
mean loss: 208.24
 ---- batch: 040 ----
mean loss: 204.79
train mean loss: 205.33
epoch train time: 0:00:08.405128
elapsed time: 0:31:19.416564
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 12:08:43.710912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.23
 ---- batch: 020 ----
mean loss: 206.62
 ---- batch: 030 ----
mean loss: 201.04
 ---- batch: 040 ----
mean loss: 211.37
train mean loss: 205.21
epoch train time: 0:00:08.417356
elapsed time: 0:31:27.835199
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 12:08:52.129456
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.87
 ---- batch: 020 ----
mean loss: 202.71
 ---- batch: 030 ----
mean loss: 202.91
 ---- batch: 040 ----
mean loss: 202.31
train mean loss: 205.04
epoch train time: 0:00:08.416683
elapsed time: 0:31:36.253105
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 12:09:00.547353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.25
 ---- batch: 020 ----
mean loss: 201.60
 ---- batch: 030 ----
mean loss: 197.77
 ---- batch: 040 ----
mean loss: 209.54
train mean loss: 204.90
epoch train time: 0:00:08.417014
elapsed time: 0:31:44.671319
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 12:09:08.965547
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.15
 ---- batch: 020 ----
mean loss: 205.96
 ---- batch: 030 ----
mean loss: 204.42
 ---- batch: 040 ----
mean loss: 202.57
train mean loss: 204.86
epoch train time: 0:00:08.356245
elapsed time: 0:31:53.028851
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 12:09:17.323081
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.42
 ---- batch: 020 ----
mean loss: 205.94
 ---- batch: 030 ----
mean loss: 204.71
 ---- batch: 040 ----
mean loss: 200.33
train mean loss: 204.38
epoch train time: 0:00:08.418504
elapsed time: 0:32:01.448572
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 12:09:25.742825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.75
 ---- batch: 020 ----
mean loss: 212.31
 ---- batch: 030 ----
mean loss: 203.75
 ---- batch: 040 ----
mean loss: 196.43
train mean loss: 205.04
epoch train time: 0:00:08.470894
elapsed time: 0:32:09.920664
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 12:09:34.214999
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.51
 ---- batch: 020 ----
mean loss: 202.50
 ---- batch: 030 ----
mean loss: 208.02
 ---- batch: 040 ----
mean loss: 208.12
train mean loss: 204.73
epoch train time: 0:00:08.374559
elapsed time: 0:32:18.296629
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 12:09:42.591040
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.34
 ---- batch: 020 ----
mean loss: 206.70
 ---- batch: 030 ----
mean loss: 202.83
 ---- batch: 040 ----
mean loss: 205.44
train mean loss: 204.85
epoch train time: 0:00:08.345146
elapsed time: 0:32:26.643256
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 12:09:50.937543
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.25
 ---- batch: 020 ----
mean loss: 209.45
 ---- batch: 030 ----
mean loss: 207.37
 ---- batch: 040 ----
mean loss: 201.74
train mean loss: 205.47
epoch train time: 0:00:08.398479
elapsed time: 0:32:35.043024
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 12:09:59.337258
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.58
 ---- batch: 020 ----
mean loss: 203.97
 ---- batch: 030 ----
mean loss: 215.82
 ---- batch: 040 ----
mean loss: 201.73
train mean loss: 205.02
epoch train time: 0:00:08.407535
elapsed time: 0:32:43.451899
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 12:10:07.746220
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.85
 ---- batch: 020 ----
mean loss: 199.81
 ---- batch: 030 ----
mean loss: 206.31
 ---- batch: 040 ----
mean loss: 205.23
train mean loss: 203.85
epoch train time: 0:00:08.365222
elapsed time: 0:32:51.818364
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 12:10:16.112605
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.61
 ---- batch: 020 ----
mean loss: 201.39
 ---- batch: 030 ----
mean loss: 200.70
 ---- batch: 040 ----
mean loss: 202.63
train mean loss: 204.40
epoch train time: 0:00:08.374297
elapsed time: 0:33:00.193797
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 12:10:24.488046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.87
 ---- batch: 020 ----
mean loss: 209.01
 ---- batch: 030 ----
mean loss: 201.60
 ---- batch: 040 ----
mean loss: 209.80
train mean loss: 204.84
epoch train time: 0:00:08.371436
elapsed time: 0:33:08.566510
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 12:10:32.860780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.76
 ---- batch: 020 ----
mean loss: 203.44
 ---- batch: 030 ----
mean loss: 205.30
 ---- batch: 040 ----
mean loss: 199.79
train mean loss: 204.09
epoch train time: 0:00:08.291999
elapsed time: 0:33:16.859976
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 12:10:41.153962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.18
 ---- batch: 020 ----
mean loss: 211.65
 ---- batch: 030 ----
mean loss: 203.69
 ---- batch: 040 ----
mean loss: 206.82
train mean loss: 205.16
epoch train time: 0:00:08.343516
elapsed time: 0:33:25.204601
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 12:10:49.498885
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.57
 ---- batch: 020 ----
mean loss: 204.49
 ---- batch: 030 ----
mean loss: 204.41
 ---- batch: 040 ----
mean loss: 198.54
train mean loss: 204.49
epoch train time: 0:00:08.347932
elapsed time: 0:33:33.553769
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 12:10:57.848021
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.03
 ---- batch: 020 ----
mean loss: 198.52
 ---- batch: 030 ----
mean loss: 206.33
 ---- batch: 040 ----
mean loss: 200.11
train mean loss: 203.61
epoch train time: 0:00:08.327921
elapsed time: 0:33:41.882905
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 12:11:06.177159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.22
 ---- batch: 020 ----
mean loss: 210.93
 ---- batch: 030 ----
mean loss: 200.86
 ---- batch: 040 ----
mean loss: 196.34
train mean loss: 204.44
epoch train time: 0:00:08.417563
elapsed time: 0:33:50.301667
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 12:11:14.595919
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.93
 ---- batch: 020 ----
mean loss: 207.12
 ---- batch: 030 ----
mean loss: 202.80
 ---- batch: 040 ----
mean loss: 207.69
train mean loss: 204.25
epoch train time: 0:00:08.358452
elapsed time: 0:33:58.661405
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 12:11:22.955648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.84
 ---- batch: 020 ----
mean loss: 208.12
 ---- batch: 030 ----
mean loss: 204.59
 ---- batch: 040 ----
mean loss: 206.90
train mean loss: 205.07
epoch train time: 0:00:08.391471
elapsed time: 0:34:07.054063
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 12:11:31.348288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.26
 ---- batch: 020 ----
mean loss: 207.31
 ---- batch: 030 ----
mean loss: 204.05
 ---- batch: 040 ----
mean loss: 197.40
train mean loss: 204.12
epoch train time: 0:00:08.381649
elapsed time: 0:34:15.436871
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 12:11:39.731103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.73
 ---- batch: 020 ----
mean loss: 206.74
 ---- batch: 030 ----
mean loss: 204.29
 ---- batch: 040 ----
mean loss: 200.04
train mean loss: 203.36
epoch train time: 0:00:08.414258
elapsed time: 0:34:23.852329
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 12:11:48.146605
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.77
 ---- batch: 020 ----
mean loss: 210.96
 ---- batch: 030 ----
mean loss: 203.70
 ---- batch: 040 ----
mean loss: 202.32
train mean loss: 204.32
epoch train time: 0:00:08.406505
elapsed time: 0:34:32.260011
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 12:11:56.554247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.07
 ---- batch: 020 ----
mean loss: 202.62
 ---- batch: 030 ----
mean loss: 202.22
 ---- batch: 040 ----
mean loss: 210.85
train mean loss: 204.01
epoch train time: 0:00:08.331958
elapsed time: 0:34:40.593289
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 12:12:04.887569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.49
 ---- batch: 020 ----
mean loss: 207.66
 ---- batch: 030 ----
mean loss: 203.41
 ---- batch: 040 ----
mean loss: 211.03
train mean loss: 204.01
epoch train time: 0:00:08.355241
elapsed time: 0:34:48.949788
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 12:12:13.244030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.99
 ---- batch: 020 ----
mean loss: 201.04
 ---- batch: 030 ----
mean loss: 202.01
 ---- batch: 040 ----
mean loss: 203.11
train mean loss: 203.64
epoch train time: 0:00:08.370778
elapsed time: 0:34:57.321763
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 12:12:21.616075
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.46
 ---- batch: 020 ----
mean loss: 205.82
 ---- batch: 030 ----
mean loss: 204.10
 ---- batch: 040 ----
mean loss: 204.46
train mean loss: 204.19
epoch train time: 0:00:08.396477
elapsed time: 0:35:05.719635
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 12:12:30.013947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.57
 ---- batch: 020 ----
mean loss: 201.89
 ---- batch: 030 ----
mean loss: 201.79
 ---- batch: 040 ----
mean loss: 204.35
train mean loss: 204.66
epoch train time: 0:00:08.323985
elapsed time: 0:35:14.044892
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 12:12:38.339164
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.59
 ---- batch: 020 ----
mean loss: 203.35
 ---- batch: 030 ----
mean loss: 200.66
 ---- batch: 040 ----
mean loss: 207.76
train mean loss: 204.21
epoch train time: 0:00:08.442630
elapsed time: 0:35:22.488783
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 12:12:46.783132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.36
 ---- batch: 020 ----
mean loss: 205.79
 ---- batch: 030 ----
mean loss: 199.58
 ---- batch: 040 ----
mean loss: 202.91
train mean loss: 204.15
epoch train time: 0:00:08.421280
elapsed time: 0:35:30.920957
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_8/checkpoint.pth.tar
**** end time: 2019-09-25 12:12:55.214909 ****
