Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_0', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24044
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistDense3...
Done.
**** start time: 2019-09-25 23:01:58.651958 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
            Linear-2                  [-1, 100]          48,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 68,100
Trainable params: 68,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 23:01:58.655331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4305.87
 ---- batch: 020 ----
mean loss: 4139.69
 ---- batch: 030 ----
mean loss: 4122.81
 ---- batch: 040 ----
mean loss: 4007.50
 ---- batch: 050 ----
mean loss: 3870.32
 ---- batch: 060 ----
mean loss: 3904.04
 ---- batch: 070 ----
mean loss: 3768.87
 ---- batch: 080 ----
mean loss: 3759.20
 ---- batch: 090 ----
mean loss: 3697.71
train mean loss: 3933.00
epoch train time: 0:00:33.764117
elapsed time: 0:00:33.770026
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 23:02:32.422047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3529.64
 ---- batch: 020 ----
mean loss: 3562.25
 ---- batch: 030 ----
mean loss: 3489.46
 ---- batch: 040 ----
mean loss: 3439.93
 ---- batch: 050 ----
mean loss: 3318.04
 ---- batch: 060 ----
mean loss: 3320.03
 ---- batch: 070 ----
mean loss: 3257.49
 ---- batch: 080 ----
mean loss: 3171.84
 ---- batch: 090 ----
mean loss: 3142.00
train mean loss: 3342.72
epoch train time: 0:00:00.515679
elapsed time: 0:00:34.285873
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 23:02:32.937902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3016.84
 ---- batch: 020 ----
mean loss: 2955.48
 ---- batch: 030 ----
mean loss: 2941.12
 ---- batch: 040 ----
mean loss: 2893.12
 ---- batch: 050 ----
mean loss: 2855.80
 ---- batch: 060 ----
mean loss: 2798.89
 ---- batch: 070 ----
mean loss: 2778.57
 ---- batch: 080 ----
mean loss: 2700.58
 ---- batch: 090 ----
mean loss: 2622.73
train mean loss: 2828.78
epoch train time: 0:00:00.497465
elapsed time: 0:00:34.783519
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 23:02:33.435543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2576.02
 ---- batch: 020 ----
mean loss: 2499.53
 ---- batch: 030 ----
mean loss: 2450.03
 ---- batch: 040 ----
mean loss: 2465.97
 ---- batch: 050 ----
mean loss: 2401.54
 ---- batch: 060 ----
mean loss: 2381.86
 ---- batch: 070 ----
mean loss: 2295.82
 ---- batch: 080 ----
mean loss: 2274.93
 ---- batch: 090 ----
mean loss: 2266.48
train mean loss: 2389.69
epoch train time: 0:00:00.503209
elapsed time: 0:00:35.286923
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 23:02:33.938940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2168.49
 ---- batch: 020 ----
mean loss: 2155.88
 ---- batch: 030 ----
mean loss: 2100.21
 ---- batch: 040 ----
mean loss: 2066.32
 ---- batch: 050 ----
mean loss: 2079.61
 ---- batch: 060 ----
mean loss: 1995.21
 ---- batch: 070 ----
mean loss: 1987.08
 ---- batch: 080 ----
mean loss: 1987.44
 ---- batch: 090 ----
mean loss: 1930.75
train mean loss: 2042.28
epoch train time: 0:00:00.499497
elapsed time: 0:00:35.786585
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 23:02:34.438596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1856.39
 ---- batch: 020 ----
mean loss: 1825.21
 ---- batch: 030 ----
mean loss: 1834.08
 ---- batch: 040 ----
mean loss: 1787.94
 ---- batch: 050 ----
mean loss: 1820.65
 ---- batch: 060 ----
mean loss: 1715.69
 ---- batch: 070 ----
mean loss: 1737.17
 ---- batch: 080 ----
mean loss: 1739.78
 ---- batch: 090 ----
mean loss: 1661.38
train mean loss: 1766.83
epoch train time: 0:00:00.514649
elapsed time: 0:00:36.301386
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 23:02:34.953425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1639.50
 ---- batch: 020 ----
mean loss: 1632.53
 ---- batch: 030 ----
mean loss: 1553.10
 ---- batch: 040 ----
mean loss: 1583.48
 ---- batch: 050 ----
mean loss: 1553.71
 ---- batch: 060 ----
mean loss: 1547.37
 ---- batch: 070 ----
mean loss: 1512.79
 ---- batch: 080 ----
mean loss: 1501.13
 ---- batch: 090 ----
mean loss: 1485.36
train mean loss: 1548.75
epoch train time: 0:00:00.507011
elapsed time: 0:00:36.808583
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 23:02:35.460582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1428.30
 ---- batch: 020 ----
mean loss: 1435.41
 ---- batch: 030 ----
mean loss: 1436.61
 ---- batch: 040 ----
mean loss: 1363.15
 ---- batch: 050 ----
mean loss: 1398.46
 ---- batch: 060 ----
mean loss: 1376.91
 ---- batch: 070 ----
mean loss: 1332.08
 ---- batch: 080 ----
mean loss: 1350.22
 ---- batch: 090 ----
mean loss: 1309.60
train mean loss: 1375.39
epoch train time: 0:00:00.508308
elapsed time: 0:00:37.317026
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 23:02:35.969036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1301.95
 ---- batch: 020 ----
mean loss: 1261.68
 ---- batch: 030 ----
mean loss: 1260.89
 ---- batch: 040 ----
mean loss: 1246.98
 ---- batch: 050 ----
mean loss: 1267.53
 ---- batch: 060 ----
mean loss: 1216.25
 ---- batch: 070 ----
mean loss: 1231.42
 ---- batch: 080 ----
mean loss: 1189.30
 ---- batch: 090 ----
mean loss: 1217.09
train mean loss: 1241.14
epoch train time: 0:00:00.504984
elapsed time: 0:00:37.822157
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 23:02:36.474162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1188.02
 ---- batch: 020 ----
mean loss: 1161.11
 ---- batch: 030 ----
mean loss: 1164.36
 ---- batch: 040 ----
mean loss: 1120.50
 ---- batch: 050 ----
mean loss: 1147.17
 ---- batch: 060 ----
mean loss: 1128.67
 ---- batch: 070 ----
mean loss: 1137.21
 ---- batch: 080 ----
mean loss: 1093.84
 ---- batch: 090 ----
mean loss: 1106.73
train mean loss: 1134.59
epoch train time: 0:00:00.505877
elapsed time: 0:00:38.328174
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 23:02:36.980182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1064.15
 ---- batch: 020 ----
mean loss: 1081.92
 ---- batch: 030 ----
mean loss: 1076.00
 ---- batch: 040 ----
mean loss: 1046.69
 ---- batch: 050 ----
mean loss: 1044.65
 ---- batch: 060 ----
mean loss: 1057.03
 ---- batch: 070 ----
mean loss: 1041.50
 ---- batch: 080 ----
mean loss: 1012.60
 ---- batch: 090 ----
mean loss: 1035.22
train mean loss: 1049.14
epoch train time: 0:00:00.503730
elapsed time: 0:00:38.832054
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 23:02:37.484062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 996.48
 ---- batch: 020 ----
mean loss: 1008.76
 ---- batch: 030 ----
mean loss: 1013.38
 ---- batch: 040 ----
mean loss: 1004.42
 ---- batch: 050 ----
mean loss: 998.52
 ---- batch: 060 ----
mean loss: 992.45
 ---- batch: 070 ----
mean loss: 993.85
 ---- batch: 080 ----
mean loss: 966.34
 ---- batch: 090 ----
mean loss: 956.94
train mean loss: 989.37
epoch train time: 0:00:00.508874
elapsed time: 0:00:39.341084
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 23:02:37.993088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 956.37
 ---- batch: 020 ----
mean loss: 951.22
 ---- batch: 030 ----
mean loss: 964.51
 ---- batch: 040 ----
mean loss: 942.85
 ---- batch: 050 ----
mean loss: 970.42
 ---- batch: 060 ----
mean loss: 947.98
 ---- batch: 070 ----
mean loss: 931.25
 ---- batch: 080 ----
mean loss: 941.75
 ---- batch: 090 ----
mean loss: 942.60
train mean loss: 949.47
epoch train time: 0:00:00.495052
elapsed time: 0:00:39.836277
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 23:02:38.488339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 936.72
 ---- batch: 020 ----
mean loss: 928.44
 ---- batch: 030 ----
mean loss: 923.76
 ---- batch: 040 ----
mean loss: 904.14
 ---- batch: 050 ----
mean loss: 923.09
 ---- batch: 060 ----
mean loss: 917.18
 ---- batch: 070 ----
mean loss: 932.49
 ---- batch: 080 ----
mean loss: 916.91
 ---- batch: 090 ----
mean loss: 915.80
train mean loss: 921.02
epoch train time: 0:00:00.500494
elapsed time: 0:00:40.336968
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 23:02:38.988998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 920.64
 ---- batch: 020 ----
mean loss: 910.88
 ---- batch: 030 ----
mean loss: 904.93
 ---- batch: 040 ----
mean loss: 891.00
 ---- batch: 050 ----
mean loss: 897.55
 ---- batch: 060 ----
mean loss: 891.82
 ---- batch: 070 ----
mean loss: 894.44
 ---- batch: 080 ----
mean loss: 909.64
 ---- batch: 090 ----
mean loss: 901.42
train mean loss: 902.48
epoch train time: 0:00:00.495864
elapsed time: 0:00:40.833018
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 23:02:39.485025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.14
 ---- batch: 020 ----
mean loss: 893.78
 ---- batch: 030 ----
mean loss: 891.18
 ---- batch: 040 ----
mean loss: 901.20
 ---- batch: 050 ----
mean loss: 894.86
 ---- batch: 060 ----
mean loss: 883.19
 ---- batch: 070 ----
mean loss: 868.21
 ---- batch: 080 ----
mean loss: 888.77
 ---- batch: 090 ----
mean loss: 890.50
train mean loss: 890.42
epoch train time: 0:00:00.507327
elapsed time: 0:00:41.340487
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 23:02:39.992495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.66
 ---- batch: 020 ----
mean loss: 865.70
 ---- batch: 030 ----
mean loss: 881.91
 ---- batch: 040 ----
mean loss: 889.62
 ---- batch: 050 ----
mean loss: 865.85
 ---- batch: 060 ----
mean loss: 891.53
 ---- batch: 070 ----
mean loss: 897.47
 ---- batch: 080 ----
mean loss: 899.04
 ---- batch: 090 ----
mean loss: 869.26
train mean loss: 884.01
epoch train time: 0:00:00.497327
elapsed time: 0:00:41.837970
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 23:02:40.489975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.23
 ---- batch: 020 ----
mean loss: 875.58
 ---- batch: 030 ----
mean loss: 897.27
 ---- batch: 040 ----
mean loss: 887.91
 ---- batch: 050 ----
mean loss: 869.82
 ---- batch: 060 ----
mean loss: 864.14
 ---- batch: 070 ----
mean loss: 878.12
 ---- batch: 080 ----
mean loss: 874.95
 ---- batch: 090 ----
mean loss: 872.65
train mean loss: 879.62
epoch train time: 0:00:00.502086
elapsed time: 0:00:42.340198
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 23:02:40.992217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.41
 ---- batch: 020 ----
mean loss: 886.29
 ---- batch: 030 ----
mean loss: 864.14
 ---- batch: 040 ----
mean loss: 883.03
 ---- batch: 050 ----
mean loss: 880.34
 ---- batch: 060 ----
mean loss: 874.24
 ---- batch: 070 ----
mean loss: 859.69
 ---- batch: 080 ----
mean loss: 890.69
 ---- batch: 090 ----
mean loss: 885.07
train mean loss: 877.61
epoch train time: 0:00:00.501305
elapsed time: 0:00:42.841725
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 23:02:41.493749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.25
 ---- batch: 020 ----
mean loss: 888.92
 ---- batch: 030 ----
mean loss: 876.25
 ---- batch: 040 ----
mean loss: 883.81
 ---- batch: 050 ----
mean loss: 866.82
 ---- batch: 060 ----
mean loss: 878.19
 ---- batch: 070 ----
mean loss: 885.99
 ---- batch: 080 ----
mean loss: 863.86
 ---- batch: 090 ----
mean loss: 872.03
train mean loss: 876.64
epoch train time: 0:00:00.505769
elapsed time: 0:00:43.347655
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 23:02:41.999664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.67
 ---- batch: 020 ----
mean loss: 868.60
 ---- batch: 030 ----
mean loss: 867.83
 ---- batch: 040 ----
mean loss: 875.77
 ---- batch: 050 ----
mean loss: 894.11
 ---- batch: 060 ----
mean loss: 873.60
 ---- batch: 070 ----
mean loss: 857.68
 ---- batch: 080 ----
mean loss: 889.54
 ---- batch: 090 ----
mean loss: 879.23
train mean loss: 875.16
epoch train time: 0:00:00.500250
elapsed time: 0:00:43.848059
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 23:02:42.500081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.37
 ---- batch: 020 ----
mean loss: 888.01
 ---- batch: 030 ----
mean loss: 867.03
 ---- batch: 040 ----
mean loss: 856.56
 ---- batch: 050 ----
mean loss: 868.07
 ---- batch: 060 ----
mean loss: 891.09
 ---- batch: 070 ----
mean loss: 882.12
 ---- batch: 080 ----
mean loss: 871.20
 ---- batch: 090 ----
mean loss: 876.55
train mean loss: 875.89
epoch train time: 0:00:00.506709
elapsed time: 0:00:44.354955
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 23:02:43.006967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.99
 ---- batch: 020 ----
mean loss: 870.55
 ---- batch: 030 ----
mean loss: 861.02
 ---- batch: 040 ----
mean loss: 868.37
 ---- batch: 050 ----
mean loss: 878.78
 ---- batch: 060 ----
mean loss: 879.38
 ---- batch: 070 ----
mean loss: 875.79
 ---- batch: 080 ----
mean loss: 878.39
 ---- batch: 090 ----
mean loss: 885.01
train mean loss: 875.08
epoch train time: 0:00:00.502461
elapsed time: 0:00:44.857558
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 23:02:43.509563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.59
 ---- batch: 020 ----
mean loss: 871.06
 ---- batch: 030 ----
mean loss: 877.13
 ---- batch: 040 ----
mean loss: 857.95
 ---- batch: 050 ----
mean loss: 872.98
 ---- batch: 060 ----
mean loss: 865.57
 ---- batch: 070 ----
mean loss: 881.43
 ---- batch: 080 ----
mean loss: 881.54
 ---- batch: 090 ----
mean loss: 873.41
train mean loss: 876.24
epoch train time: 0:00:00.505497
elapsed time: 0:00:45.363252
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 23:02:44.015263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.75
 ---- batch: 020 ----
mean loss: 888.99
 ---- batch: 030 ----
mean loss: 883.39
 ---- batch: 040 ----
mean loss: 876.09
 ---- batch: 050 ----
mean loss: 881.02
 ---- batch: 060 ----
mean loss: 877.85
 ---- batch: 070 ----
mean loss: 873.89
 ---- batch: 080 ----
mean loss: 865.58
 ---- batch: 090 ----
mean loss: 883.90
train mean loss: 875.20
epoch train time: 0:00:00.495935
elapsed time: 0:00:45.859344
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 23:02:44.511354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.50
 ---- batch: 020 ----
mean loss: 869.74
 ---- batch: 030 ----
mean loss: 863.27
 ---- batch: 040 ----
mean loss: 866.22
 ---- batch: 050 ----
mean loss: 864.51
 ---- batch: 060 ----
mean loss: 902.36
 ---- batch: 070 ----
mean loss: 883.60
 ---- batch: 080 ----
mean loss: 878.39
 ---- batch: 090 ----
mean loss: 869.10
train mean loss: 874.90
epoch train time: 0:00:00.508220
elapsed time: 0:00:46.367711
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 23:02:45.019720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.26
 ---- batch: 020 ----
mean loss: 873.49
 ---- batch: 030 ----
mean loss: 871.79
 ---- batch: 040 ----
mean loss: 872.87
 ---- batch: 050 ----
mean loss: 864.73
 ---- batch: 060 ----
mean loss: 869.85
 ---- batch: 070 ----
mean loss: 879.62
 ---- batch: 080 ----
mean loss: 887.26
 ---- batch: 090 ----
mean loss: 888.05
train mean loss: 875.11
epoch train time: 0:00:00.500967
elapsed time: 0:00:46.868822
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 23:02:45.520829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.93
 ---- batch: 020 ----
mean loss: 869.18
 ---- batch: 030 ----
mean loss: 878.11
 ---- batch: 040 ----
mean loss: 885.38
 ---- batch: 050 ----
mean loss: 873.31
 ---- batch: 060 ----
mean loss: 871.07
 ---- batch: 070 ----
mean loss: 862.61
 ---- batch: 080 ----
mean loss: 892.12
 ---- batch: 090 ----
mean loss: 862.39
train mean loss: 874.89
epoch train time: 0:00:00.505446
elapsed time: 0:00:47.374412
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 23:02:46.026435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.86
 ---- batch: 020 ----
mean loss: 874.97
 ---- batch: 030 ----
mean loss: 861.88
 ---- batch: 040 ----
mean loss: 874.73
 ---- batch: 050 ----
mean loss: 883.70
 ---- batch: 060 ----
mean loss: 883.32
 ---- batch: 070 ----
mean loss: 889.98
 ---- batch: 080 ----
mean loss: 863.96
 ---- batch: 090 ----
mean loss: 859.31
train mean loss: 875.83
epoch train time: 0:00:00.497274
elapsed time: 0:00:47.871854
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 23:02:46.523854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.98
 ---- batch: 020 ----
mean loss: 886.46
 ---- batch: 030 ----
mean loss: 870.55
 ---- batch: 040 ----
mean loss: 876.61
 ---- batch: 050 ----
mean loss: 875.43
 ---- batch: 060 ----
mean loss: 878.10
 ---- batch: 070 ----
mean loss: 872.73
 ---- batch: 080 ----
mean loss: 870.15
 ---- batch: 090 ----
mean loss: 857.33
train mean loss: 874.97
epoch train time: 0:00:00.513692
elapsed time: 0:00:48.385707
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 23:02:47.037720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.88
 ---- batch: 020 ----
mean loss: 871.09
 ---- batch: 030 ----
mean loss: 868.37
 ---- batch: 040 ----
mean loss: 878.46
 ---- batch: 050 ----
mean loss: 892.61
 ---- batch: 060 ----
mean loss: 866.61
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 868.92
 ---- batch: 090 ----
mean loss: 861.11
train mean loss: 875.07
epoch train time: 0:00:00.505301
elapsed time: 0:00:48.891169
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 23:02:47.543170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.11
 ---- batch: 020 ----
mean loss: 879.14
 ---- batch: 030 ----
mean loss: 874.04
 ---- batch: 040 ----
mean loss: 868.01
 ---- batch: 050 ----
mean loss: 876.37
 ---- batch: 060 ----
mean loss: 886.70
 ---- batch: 070 ----
mean loss: 861.69
 ---- batch: 080 ----
mean loss: 882.77
 ---- batch: 090 ----
mean loss: 870.74
train mean loss: 874.91
epoch train time: 0:00:00.502141
elapsed time: 0:00:49.393479
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 23:02:48.045502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.46
 ---- batch: 020 ----
mean loss: 876.58
 ---- batch: 030 ----
mean loss: 877.96
 ---- batch: 040 ----
mean loss: 870.24
 ---- batch: 050 ----
mean loss: 869.36
 ---- batch: 060 ----
mean loss: 868.26
 ---- batch: 070 ----
mean loss: 868.20
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 877.78
train mean loss: 876.12
epoch train time: 0:00:00.495209
elapsed time: 0:00:49.888846
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 23:02:48.540853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.88
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 866.29
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 889.38
 ---- batch: 060 ----
mean loss: 879.60
 ---- batch: 070 ----
mean loss: 884.78
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 873.48
train mean loss: 875.11
epoch train time: 0:00:00.508805
elapsed time: 0:00:50.397794
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 23:02:49.049800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.81
 ---- batch: 020 ----
mean loss: 872.41
 ---- batch: 030 ----
mean loss: 858.52
 ---- batch: 040 ----
mean loss: 878.56
 ---- batch: 050 ----
mean loss: 870.22
 ---- batch: 060 ----
mean loss: 889.59
 ---- batch: 070 ----
mean loss: 878.83
 ---- batch: 080 ----
mean loss: 875.94
 ---- batch: 090 ----
mean loss: 887.80
train mean loss: 875.70
epoch train time: 0:00:00.498668
elapsed time: 0:00:50.896610
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 23:02:49.548618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 870.46
 ---- batch: 030 ----
mean loss: 875.57
 ---- batch: 040 ----
mean loss: 886.46
 ---- batch: 050 ----
mean loss: 888.80
 ---- batch: 060 ----
mean loss: 856.43
 ---- batch: 070 ----
mean loss: 861.54
 ---- batch: 080 ----
mean loss: 865.08
 ---- batch: 090 ----
mean loss: 908.21
train mean loss: 875.33
epoch train time: 0:00:00.511240
elapsed time: 0:00:51.408008
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 23:02:50.060034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.66
 ---- batch: 020 ----
mean loss: 867.37
 ---- batch: 030 ----
mean loss: 881.86
 ---- batch: 040 ----
mean loss: 896.13
 ---- batch: 050 ----
mean loss: 895.66
 ---- batch: 060 ----
mean loss: 872.61
 ---- batch: 070 ----
mean loss: 871.27
 ---- batch: 080 ----
mean loss: 850.89
 ---- batch: 090 ----
mean loss: 868.18
train mean loss: 874.80
epoch train time: 0:00:00.496157
elapsed time: 0:00:51.904325
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 23:02:50.556333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 879.71
 ---- batch: 040 ----
mean loss: 880.95
 ---- batch: 050 ----
mean loss: 865.29
 ---- batch: 060 ----
mean loss: 882.77
 ---- batch: 070 ----
mean loss: 869.99
 ---- batch: 080 ----
mean loss: 882.00
 ---- batch: 090 ----
mean loss: 857.67
train mean loss: 875.42
epoch train time: 0:00:00.502980
elapsed time: 0:00:52.407444
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 23:02:51.059452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 874.52
 ---- batch: 030 ----
mean loss: 869.88
 ---- batch: 040 ----
mean loss: 872.46
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 884.13
 ---- batch: 070 ----
mean loss: 882.33
 ---- batch: 080 ----
mean loss: 868.00
 ---- batch: 090 ----
mean loss: 884.71
train mean loss: 875.94
epoch train time: 0:00:00.498220
elapsed time: 0:00:52.905808
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 23:02:51.557815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.36
 ---- batch: 020 ----
mean loss: 863.17
 ---- batch: 030 ----
mean loss: 868.58
 ---- batch: 040 ----
mean loss: 872.91
 ---- batch: 050 ----
mean loss: 875.23
 ---- batch: 060 ----
mean loss: 874.13
 ---- batch: 070 ----
mean loss: 875.79
 ---- batch: 080 ----
mean loss: 872.87
 ---- batch: 090 ----
mean loss: 880.58
train mean loss: 875.20
epoch train time: 0:00:00.510814
elapsed time: 0:00:53.416764
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 23:02:52.068773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
 ---- batch: 020 ----
mean loss: 888.19
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 864.90
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 877.49
 ---- batch: 070 ----
mean loss: 873.63
 ---- batch: 080 ----
mean loss: 861.95
 ---- batch: 090 ----
mean loss: 867.30
train mean loss: 874.55
epoch train time: 0:00:00.500510
elapsed time: 0:00:53.917472
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 23:02:52.569483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.19
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 878.84
 ---- batch: 040 ----
mean loss: 868.87
 ---- batch: 050 ----
mean loss: 860.44
 ---- batch: 060 ----
mean loss: 885.18
 ---- batch: 070 ----
mean loss: 876.98
 ---- batch: 080 ----
mean loss: 875.13
 ---- batch: 090 ----
mean loss: 883.68
train mean loss: 875.45
epoch train time: 0:00:00.505529
elapsed time: 0:00:54.423168
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 23:02:53.075210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.89
 ---- batch: 020 ----
mean loss: 862.14
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 852.48
 ---- batch: 060 ----
mean loss: 873.50
 ---- batch: 070 ----
mean loss: 889.67
 ---- batch: 080 ----
mean loss: 893.94
 ---- batch: 090 ----
mean loss: 900.18
train mean loss: 875.66
epoch train time: 0:00:00.507342
elapsed time: 0:00:54.930704
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 23:02:53.582729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.25
 ---- batch: 020 ----
mean loss: 859.41
 ---- batch: 030 ----
mean loss: 880.51
 ---- batch: 040 ----
mean loss: 889.92
 ---- batch: 050 ----
mean loss: 869.13
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 886.11
 ---- batch: 090 ----
mean loss: 885.30
train mean loss: 875.24
epoch train time: 0:00:00.507939
elapsed time: 0:00:55.438804
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 23:02:54.090813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.92
 ---- batch: 020 ----
mean loss: 896.00
 ---- batch: 030 ----
mean loss: 895.89
 ---- batch: 040 ----
mean loss: 875.86
 ---- batch: 050 ----
mean loss: 854.47
 ---- batch: 060 ----
mean loss: 885.08
 ---- batch: 070 ----
mean loss: 872.57
 ---- batch: 080 ----
mean loss: 874.54
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 875.66
epoch train time: 0:00:00.505763
elapsed time: 0:00:55.944736
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 23:02:54.596744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.48
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 875.70
 ---- batch: 040 ----
mean loss: 877.23
 ---- batch: 050 ----
mean loss: 878.31
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.63
 ---- batch: 080 ----
mean loss: 876.95
 ---- batch: 090 ----
mean loss: 864.58
train mean loss: 874.88
epoch train time: 0:00:00.507323
elapsed time: 0:00:56.452199
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 23:02:55.104205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 874.57
 ---- batch: 030 ----
mean loss: 885.12
 ---- batch: 040 ----
mean loss: 870.30
 ---- batch: 050 ----
mean loss: 885.77
 ---- batch: 060 ----
mean loss: 877.45
 ---- batch: 070 ----
mean loss: 893.99
 ---- batch: 080 ----
mean loss: 865.68
 ---- batch: 090 ----
mean loss: 871.94
train mean loss: 875.10
epoch train time: 0:00:00.501235
elapsed time: 0:00:56.953582
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 23:02:55.605580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.71
 ---- batch: 020 ----
mean loss: 867.95
 ---- batch: 030 ----
mean loss: 868.70
 ---- batch: 040 ----
mean loss: 880.39
 ---- batch: 050 ----
mean loss: 874.64
 ---- batch: 060 ----
mean loss: 876.82
 ---- batch: 070 ----
mean loss: 871.95
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 896.57
train mean loss: 874.86
epoch train time: 0:00:00.516924
elapsed time: 0:00:57.470667
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 23:02:56.122672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.54
 ---- batch: 020 ----
mean loss: 874.88
 ---- batch: 030 ----
mean loss: 842.41
 ---- batch: 040 ----
mean loss: 870.90
 ---- batch: 050 ----
mean loss: 879.94
 ---- batch: 060 ----
mean loss: 870.33
 ---- batch: 070 ----
mean loss: 893.30
 ---- batch: 080 ----
mean loss: 874.30
 ---- batch: 090 ----
mean loss: 901.59
train mean loss: 874.94
epoch train time: 0:00:00.512120
elapsed time: 0:00:57.982942
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 23:02:56.634943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.79
 ---- batch: 020 ----
mean loss: 885.95
 ---- batch: 030 ----
mean loss: 869.68
 ---- batch: 040 ----
mean loss: 881.87
 ---- batch: 050 ----
mean loss: 866.17
 ---- batch: 060 ----
mean loss: 869.62
 ---- batch: 070 ----
mean loss: 865.49
 ---- batch: 080 ----
mean loss: 885.13
 ---- batch: 090 ----
mean loss: 878.95
train mean loss: 874.96
epoch train time: 0:00:00.511707
elapsed time: 0:00:58.494796
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 23:02:57.146794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.61
 ---- batch: 020 ----
mean loss: 893.23
 ---- batch: 030 ----
mean loss: 872.99
 ---- batch: 040 ----
mean loss: 888.75
 ---- batch: 050 ----
mean loss: 867.76
 ---- batch: 060 ----
mean loss: 869.11
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 858.03
 ---- batch: 090 ----
mean loss: 875.46
train mean loss: 875.16
epoch train time: 0:00:00.494306
elapsed time: 0:00:58.989248
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 23:02:57.641302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.40
 ---- batch: 020 ----
mean loss: 873.36
 ---- batch: 030 ----
mean loss: 870.54
 ---- batch: 040 ----
mean loss: 862.73
 ---- batch: 050 ----
mean loss: 871.31
 ---- batch: 060 ----
mean loss: 875.63
 ---- batch: 070 ----
mean loss: 898.83
 ---- batch: 080 ----
mean loss: 874.46
 ---- batch: 090 ----
mean loss: 887.67
train mean loss: 875.70
epoch train time: 0:00:00.500822
elapsed time: 0:00:59.490308
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 23:02:58.142333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.21
 ---- batch: 020 ----
mean loss: 875.67
 ---- batch: 030 ----
mean loss: 868.29
 ---- batch: 040 ----
mean loss: 876.40
 ---- batch: 050 ----
mean loss: 869.83
 ---- batch: 060 ----
mean loss: 874.43
 ---- batch: 070 ----
mean loss: 877.12
 ---- batch: 080 ----
mean loss: 890.28
 ---- batch: 090 ----
mean loss: 870.38
train mean loss: 875.10
epoch train time: 0:00:00.492350
elapsed time: 0:00:59.982820
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 23:02:58.634828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.40
 ---- batch: 020 ----
mean loss: 863.19
 ---- batch: 030 ----
mean loss: 884.34
 ---- batch: 040 ----
mean loss: 878.62
 ---- batch: 050 ----
mean loss: 875.68
 ---- batch: 060 ----
mean loss: 870.51
 ---- batch: 070 ----
mean loss: 875.02
 ---- batch: 080 ----
mean loss: 873.33
 ---- batch: 090 ----
mean loss: 881.99
train mean loss: 874.52
epoch train time: 0:00:00.512531
elapsed time: 0:01:00.495503
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 23:02:59.147504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.20
 ---- batch: 020 ----
mean loss: 820.22
 ---- batch: 030 ----
mean loss: 808.61
 ---- batch: 040 ----
mean loss: 785.01
 ---- batch: 050 ----
mean loss: 765.26
 ---- batch: 060 ----
mean loss: 733.70
 ---- batch: 070 ----
mean loss: 725.71
 ---- batch: 080 ----
mean loss: 690.73
 ---- batch: 090 ----
mean loss: 650.92
train mean loss: 753.39
epoch train time: 0:00:00.495529
elapsed time: 0:01:00.991168
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 23:02:59.643176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 575.32
 ---- batch: 020 ----
mean loss: 499.35
 ---- batch: 030 ----
mean loss: 454.94
 ---- batch: 040 ----
mean loss: 423.09
 ---- batch: 050 ----
mean loss: 405.55
 ---- batch: 060 ----
mean loss: 398.81
 ---- batch: 070 ----
mean loss: 401.83
 ---- batch: 080 ----
mean loss: 371.25
 ---- batch: 090 ----
mean loss: 354.23
train mean loss: 427.95
epoch train time: 0:00:00.508448
elapsed time: 0:01:01.499758
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 23:03:00.151777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.14
 ---- batch: 020 ----
mean loss: 337.34
 ---- batch: 030 ----
mean loss: 339.22
 ---- batch: 040 ----
mean loss: 334.82
 ---- batch: 050 ----
mean loss: 324.21
 ---- batch: 060 ----
mean loss: 312.47
 ---- batch: 070 ----
mean loss: 309.59
 ---- batch: 080 ----
mean loss: 305.77
 ---- batch: 090 ----
mean loss: 297.12
train mean loss: 321.74
epoch train time: 0:00:00.498168
elapsed time: 0:01:01.998086
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 23:03:00.650095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.44
 ---- batch: 020 ----
mean loss: 290.10
 ---- batch: 030 ----
mean loss: 285.53
 ---- batch: 040 ----
mean loss: 282.60
 ---- batch: 050 ----
mean loss: 286.21
 ---- batch: 060 ----
mean loss: 285.25
 ---- batch: 070 ----
mean loss: 270.72
 ---- batch: 080 ----
mean loss: 270.51
 ---- batch: 090 ----
mean loss: 270.78
train mean loss: 280.48
epoch train time: 0:00:00.501636
elapsed time: 0:01:02.499876
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 23:03:01.151901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.41
 ---- batch: 020 ----
mean loss: 258.00
 ---- batch: 030 ----
mean loss: 264.49
 ---- batch: 040 ----
mean loss: 262.62
 ---- batch: 050 ----
mean loss: 260.56
 ---- batch: 060 ----
mean loss: 258.64
 ---- batch: 070 ----
mean loss: 257.11
 ---- batch: 080 ----
mean loss: 257.37
 ---- batch: 090 ----
mean loss: 264.01
train mean loss: 260.92
epoch train time: 0:00:00.503407
elapsed time: 0:01:03.003450
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 23:03:01.655506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.68
 ---- batch: 020 ----
mean loss: 257.78
 ---- batch: 030 ----
mean loss: 244.92
 ---- batch: 040 ----
mean loss: 245.09
 ---- batch: 050 ----
mean loss: 249.11
 ---- batch: 060 ----
mean loss: 251.74
 ---- batch: 070 ----
mean loss: 247.60
 ---- batch: 080 ----
mean loss: 239.95
 ---- batch: 090 ----
mean loss: 246.98
train mean loss: 248.34
epoch train time: 0:00:00.510390
elapsed time: 0:01:03.514045
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 23:03:02.166046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.06
 ---- batch: 020 ----
mean loss: 240.70
 ---- batch: 030 ----
mean loss: 237.88
 ---- batch: 040 ----
mean loss: 239.72
 ---- batch: 050 ----
mean loss: 234.12
 ---- batch: 060 ----
mean loss: 233.29
 ---- batch: 070 ----
mean loss: 242.40
 ---- batch: 080 ----
mean loss: 241.83
 ---- batch: 090 ----
mean loss: 237.91
train mean loss: 238.28
epoch train time: 0:00:00.503658
elapsed time: 0:01:04.017859
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 23:03:02.669867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.34
 ---- batch: 020 ----
mean loss: 230.75
 ---- batch: 030 ----
mean loss: 227.26
 ---- batch: 040 ----
mean loss: 226.70
 ---- batch: 050 ----
mean loss: 233.75
 ---- batch: 060 ----
mean loss: 241.07
 ---- batch: 070 ----
mean loss: 228.48
 ---- batch: 080 ----
mean loss: 226.34
 ---- batch: 090 ----
mean loss: 243.69
train mean loss: 232.97
epoch train time: 0:00:00.507273
elapsed time: 0:01:04.525283
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 23:03:03.177298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.94
 ---- batch: 020 ----
mean loss: 222.39
 ---- batch: 030 ----
mean loss: 232.70
 ---- batch: 040 ----
mean loss: 219.92
 ---- batch: 050 ----
mean loss: 228.70
 ---- batch: 060 ----
mean loss: 232.06
 ---- batch: 070 ----
mean loss: 233.64
 ---- batch: 080 ----
mean loss: 231.23
 ---- batch: 090 ----
mean loss: 227.24
train mean loss: 228.04
epoch train time: 0:00:00.501201
elapsed time: 0:01:05.026640
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 23:03:03.678658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.76
 ---- batch: 020 ----
mean loss: 228.76
 ---- batch: 030 ----
mean loss: 222.09
 ---- batch: 040 ----
mean loss: 226.16
 ---- batch: 050 ----
mean loss: 238.43
 ---- batch: 060 ----
mean loss: 218.14
 ---- batch: 070 ----
mean loss: 218.64
 ---- batch: 080 ----
mean loss: 219.77
 ---- batch: 090 ----
mean loss: 216.39
train mean loss: 223.43
epoch train time: 0:00:00.521948
elapsed time: 0:01:05.548746
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 23:03:04.200756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.58
 ---- batch: 020 ----
mean loss: 218.03
 ---- batch: 030 ----
mean loss: 229.11
 ---- batch: 040 ----
mean loss: 223.22
 ---- batch: 050 ----
mean loss: 226.41
 ---- batch: 060 ----
mean loss: 224.30
 ---- batch: 070 ----
mean loss: 214.09
 ---- batch: 080 ----
mean loss: 214.96
 ---- batch: 090 ----
mean loss: 218.44
train mean loss: 221.25
epoch train time: 0:00:00.508200
elapsed time: 0:01:06.057090
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 23:03:04.709119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.12
 ---- batch: 020 ----
mean loss: 216.13
 ---- batch: 030 ----
mean loss: 217.51
 ---- batch: 040 ----
mean loss: 214.68
 ---- batch: 050 ----
mean loss: 217.92
 ---- batch: 060 ----
mean loss: 221.46
 ---- batch: 070 ----
mean loss: 216.18
 ---- batch: 080 ----
mean loss: 226.63
 ---- batch: 090 ----
mean loss: 214.10
train mean loss: 217.64
epoch train time: 0:00:00.505742
elapsed time: 0:01:06.563016
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 23:03:05.215040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.67
 ---- batch: 020 ----
mean loss: 208.50
 ---- batch: 030 ----
mean loss: 226.68
 ---- batch: 040 ----
mean loss: 222.53
 ---- batch: 050 ----
mean loss: 217.16
 ---- batch: 060 ----
mean loss: 224.53
 ---- batch: 070 ----
mean loss: 209.48
 ---- batch: 080 ----
mean loss: 210.21
 ---- batch: 090 ----
mean loss: 207.85
train mean loss: 214.60
epoch train time: 0:00:00.505054
elapsed time: 0:01:07.068228
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 23:03:05.720235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.91
 ---- batch: 020 ----
mean loss: 213.85
 ---- batch: 030 ----
mean loss: 206.53
 ---- batch: 040 ----
mean loss: 204.75
 ---- batch: 050 ----
mean loss: 207.66
 ---- batch: 060 ----
mean loss: 211.17
 ---- batch: 070 ----
mean loss: 211.71
 ---- batch: 080 ----
mean loss: 213.36
 ---- batch: 090 ----
mean loss: 205.63
train mean loss: 209.80
epoch train time: 0:00:00.503060
elapsed time: 0:01:07.571430
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 23:03:06.223438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.17
 ---- batch: 020 ----
mean loss: 213.53
 ---- batch: 030 ----
mean loss: 200.76
 ---- batch: 040 ----
mean loss: 205.97
 ---- batch: 050 ----
mean loss: 203.10
 ---- batch: 060 ----
mean loss: 207.96
 ---- batch: 070 ----
mean loss: 216.07
 ---- batch: 080 ----
mean loss: 208.26
 ---- batch: 090 ----
mean loss: 210.82
train mean loss: 208.32
epoch train time: 0:00:00.496571
elapsed time: 0:01:08.068145
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 23:03:06.720241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.40
 ---- batch: 020 ----
mean loss: 200.52
 ---- batch: 030 ----
mean loss: 201.21
 ---- batch: 040 ----
mean loss: 203.53
 ---- batch: 050 ----
mean loss: 206.25
 ---- batch: 060 ----
mean loss: 200.20
 ---- batch: 070 ----
mean loss: 196.50
 ---- batch: 080 ----
mean loss: 211.60
 ---- batch: 090 ----
mean loss: 211.77
train mean loss: 205.68
epoch train time: 0:00:00.509316
elapsed time: 0:01:08.577728
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 23:03:07.229738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.06
 ---- batch: 020 ----
mean loss: 195.31
 ---- batch: 030 ----
mean loss: 203.31
 ---- batch: 040 ----
mean loss: 210.31
 ---- batch: 050 ----
mean loss: 202.87
 ---- batch: 060 ----
mean loss: 201.90
 ---- batch: 070 ----
mean loss: 202.39
 ---- batch: 080 ----
mean loss: 204.45
 ---- batch: 090 ----
mean loss: 204.89
train mean loss: 202.38
epoch train time: 0:00:00.501474
elapsed time: 0:01:09.079361
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 23:03:07.731372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.42
 ---- batch: 020 ----
mean loss: 194.21
 ---- batch: 030 ----
mean loss: 201.94
 ---- batch: 040 ----
mean loss: 201.55
 ---- batch: 050 ----
mean loss: 201.65
 ---- batch: 060 ----
mean loss: 205.70
 ---- batch: 070 ----
mean loss: 197.73
 ---- batch: 080 ----
mean loss: 205.86
 ---- batch: 090 ----
mean loss: 209.57
train mean loss: 201.71
epoch train time: 0:00:00.504588
elapsed time: 0:01:09.584105
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 23:03:08.236114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.80
 ---- batch: 020 ----
mean loss: 184.51
 ---- batch: 030 ----
mean loss: 202.02
 ---- batch: 040 ----
mean loss: 196.27
 ---- batch: 050 ----
mean loss: 207.74
 ---- batch: 060 ----
mean loss: 200.21
 ---- batch: 070 ----
mean loss: 204.75
 ---- batch: 080 ----
mean loss: 205.40
 ---- batch: 090 ----
mean loss: 201.69
train mean loss: 198.79
epoch train time: 0:00:00.507298
elapsed time: 0:01:10.091555
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 23:03:08.743566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.61
 ---- batch: 020 ----
mean loss: 196.09
 ---- batch: 030 ----
mean loss: 205.20
 ---- batch: 040 ----
mean loss: 195.98
 ---- batch: 050 ----
mean loss: 192.70
 ---- batch: 060 ----
mean loss: 200.71
 ---- batch: 070 ----
mean loss: 194.51
 ---- batch: 080 ----
mean loss: 204.48
 ---- batch: 090 ----
mean loss: 198.51
train mean loss: 197.77
epoch train time: 0:00:00.513838
elapsed time: 0:01:10.605555
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 23:03:09.257585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.04
 ---- batch: 020 ----
mean loss: 194.40
 ---- batch: 030 ----
mean loss: 194.76
 ---- batch: 040 ----
mean loss: 199.77
 ---- batch: 050 ----
mean loss: 194.90
 ---- batch: 060 ----
mean loss: 200.16
 ---- batch: 070 ----
mean loss: 191.24
 ---- batch: 080 ----
mean loss: 195.18
 ---- batch: 090 ----
mean loss: 199.09
train mean loss: 196.09
epoch train time: 0:00:00.526761
elapsed time: 0:01:11.132492
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 23:03:09.784510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.31
 ---- batch: 020 ----
mean loss: 195.04
 ---- batch: 030 ----
mean loss: 194.58
 ---- batch: 040 ----
mean loss: 196.71
 ---- batch: 050 ----
mean loss: 191.29
 ---- batch: 060 ----
mean loss: 191.73
 ---- batch: 070 ----
mean loss: 200.40
 ---- batch: 080 ----
mean loss: 197.14
 ---- batch: 090 ----
mean loss: 196.68
train mean loss: 194.33
epoch train time: 0:00:00.515626
elapsed time: 0:01:11.648272
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 23:03:10.300280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.65
 ---- batch: 020 ----
mean loss: 191.83
 ---- batch: 030 ----
mean loss: 182.64
 ---- batch: 040 ----
mean loss: 189.22
 ---- batch: 050 ----
mean loss: 198.58
 ---- batch: 060 ----
mean loss: 191.02
 ---- batch: 070 ----
mean loss: 201.37
 ---- batch: 080 ----
mean loss: 198.20
 ---- batch: 090 ----
mean loss: 192.79
train mean loss: 192.94
epoch train time: 0:00:00.503928
elapsed time: 0:01:12.152341
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 23:03:10.804378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.88
 ---- batch: 020 ----
mean loss: 190.86
 ---- batch: 030 ----
mean loss: 192.44
 ---- batch: 040 ----
mean loss: 195.37
 ---- batch: 050 ----
mean loss: 180.58
 ---- batch: 060 ----
mean loss: 191.12
 ---- batch: 070 ----
mean loss: 191.37
 ---- batch: 080 ----
mean loss: 195.08
 ---- batch: 090 ----
mean loss: 190.94
train mean loss: 190.33
epoch train time: 0:00:00.500872
elapsed time: 0:01:12.653383
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 23:03:11.305394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.74
 ---- batch: 020 ----
mean loss: 186.64
 ---- batch: 030 ----
mean loss: 184.38
 ---- batch: 040 ----
mean loss: 198.54
 ---- batch: 050 ----
mean loss: 190.26
 ---- batch: 060 ----
mean loss: 185.74
 ---- batch: 070 ----
mean loss: 202.11
 ---- batch: 080 ----
mean loss: 192.79
 ---- batch: 090 ----
mean loss: 193.54
train mean loss: 190.08
epoch train time: 0:00:00.502599
elapsed time: 0:01:13.156138
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 23:03:11.808156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.95
 ---- batch: 020 ----
mean loss: 186.48
 ---- batch: 030 ----
mean loss: 181.93
 ---- batch: 040 ----
mean loss: 197.66
 ---- batch: 050 ----
mean loss: 191.83
 ---- batch: 060 ----
mean loss: 192.29
 ---- batch: 070 ----
mean loss: 183.50
 ---- batch: 080 ----
mean loss: 193.17
 ---- batch: 090 ----
mean loss: 190.95
train mean loss: 188.73
epoch train time: 0:00:00.539510
elapsed time: 0:01:13.695802
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 23:03:12.347812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.49
 ---- batch: 020 ----
mean loss: 181.37
 ---- batch: 030 ----
mean loss: 181.84
 ---- batch: 040 ----
mean loss: 185.50
 ---- batch: 050 ----
mean loss: 191.64
 ---- batch: 060 ----
mean loss: 186.94
 ---- batch: 070 ----
mean loss: 186.83
 ---- batch: 080 ----
mean loss: 189.80
 ---- batch: 090 ----
mean loss: 192.35
train mean loss: 187.20
epoch train time: 0:00:00.510429
elapsed time: 0:01:14.206397
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 23:03:12.858414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.60
 ---- batch: 020 ----
mean loss: 184.08
 ---- batch: 030 ----
mean loss: 193.37
 ---- batch: 040 ----
mean loss: 185.25
 ---- batch: 050 ----
mean loss: 187.49
 ---- batch: 060 ----
mean loss: 182.96
 ---- batch: 070 ----
mean loss: 185.38
 ---- batch: 080 ----
mean loss: 183.89
 ---- batch: 090 ----
mean loss: 185.60
train mean loss: 185.27
epoch train time: 0:00:00.498342
elapsed time: 0:01:14.704920
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 23:03:13.356926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.91
 ---- batch: 020 ----
mean loss: 178.72
 ---- batch: 030 ----
mean loss: 181.24
 ---- batch: 040 ----
mean loss: 184.48
 ---- batch: 050 ----
mean loss: 190.06
 ---- batch: 060 ----
mean loss: 186.99
 ---- batch: 070 ----
mean loss: 179.74
 ---- batch: 080 ----
mean loss: 193.32
 ---- batch: 090 ----
mean loss: 184.53
train mean loss: 184.81
epoch train time: 0:00:00.509822
elapsed time: 0:01:15.214884
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 23:03:13.866891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.75
 ---- batch: 020 ----
mean loss: 176.16
 ---- batch: 030 ----
mean loss: 185.48
 ---- batch: 040 ----
mean loss: 178.32
 ---- batch: 050 ----
mean loss: 187.64
 ---- batch: 060 ----
mean loss: 191.29
 ---- batch: 070 ----
mean loss: 181.79
 ---- batch: 080 ----
mean loss: 183.38
 ---- batch: 090 ----
mean loss: 187.98
train mean loss: 184.09
epoch train time: 0:00:00.502302
elapsed time: 0:01:15.717330
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 23:03:14.369336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.83
 ---- batch: 020 ----
mean loss: 178.59
 ---- batch: 030 ----
mean loss: 179.80
 ---- batch: 040 ----
mean loss: 181.02
 ---- batch: 050 ----
mean loss: 182.90
 ---- batch: 060 ----
mean loss: 191.13
 ---- batch: 070 ----
mean loss: 176.76
 ---- batch: 080 ----
mean loss: 183.56
 ---- batch: 090 ----
mean loss: 192.24
train mean loss: 183.54
epoch train time: 0:00:00.506221
elapsed time: 0:01:16.223762
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 23:03:14.875821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.69
 ---- batch: 020 ----
mean loss: 187.01
 ---- batch: 030 ----
mean loss: 176.46
 ---- batch: 040 ----
mean loss: 185.30
 ---- batch: 050 ----
mean loss: 183.81
 ---- batch: 060 ----
mean loss: 179.08
 ---- batch: 070 ----
mean loss: 174.80
 ---- batch: 080 ----
mean loss: 187.79
 ---- batch: 090 ----
mean loss: 187.96
train mean loss: 183.29
epoch train time: 0:00:00.504693
elapsed time: 0:01:16.728651
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 23:03:15.380660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.78
 ---- batch: 020 ----
mean loss: 178.69
 ---- batch: 030 ----
mean loss: 182.82
 ---- batch: 040 ----
mean loss: 185.98
 ---- batch: 050 ----
mean loss: 178.96
 ---- batch: 060 ----
mean loss: 181.28
 ---- batch: 070 ----
mean loss: 175.67
 ---- batch: 080 ----
mean loss: 184.71
 ---- batch: 090 ----
mean loss: 174.28
train mean loss: 180.15
epoch train time: 0:00:00.512980
elapsed time: 0:01:17.241778
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 23:03:15.893785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.73
 ---- batch: 020 ----
mean loss: 176.38
 ---- batch: 030 ----
mean loss: 178.88
 ---- batch: 040 ----
mean loss: 180.27
 ---- batch: 050 ----
mean loss: 166.72
 ---- batch: 060 ----
mean loss: 180.63
 ---- batch: 070 ----
mean loss: 183.81
 ---- batch: 080 ----
mean loss: 179.69
 ---- batch: 090 ----
mean loss: 184.98
train mean loss: 179.25
epoch train time: 0:00:00.498781
elapsed time: 0:01:17.740701
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 23:03:16.392708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.48
 ---- batch: 020 ----
mean loss: 177.16
 ---- batch: 030 ----
mean loss: 175.51
 ---- batch: 040 ----
mean loss: 180.54
 ---- batch: 050 ----
mean loss: 174.68
 ---- batch: 060 ----
mean loss: 184.65
 ---- batch: 070 ----
mean loss: 178.73
 ---- batch: 080 ----
mean loss: 179.04
 ---- batch: 090 ----
mean loss: 180.69
train mean loss: 179.48
epoch train time: 0:00:00.514483
elapsed time: 0:01:18.255335
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 23:03:16.907362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.68
 ---- batch: 020 ----
mean loss: 180.30
 ---- batch: 030 ----
mean loss: 179.10
 ---- batch: 040 ----
mean loss: 179.64
 ---- batch: 050 ----
mean loss: 181.38
 ---- batch: 060 ----
mean loss: 178.09
 ---- batch: 070 ----
mean loss: 182.09
 ---- batch: 080 ----
mean loss: 177.26
 ---- batch: 090 ----
mean loss: 179.84
train mean loss: 178.46
epoch train time: 0:00:00.508005
elapsed time: 0:01:18.763520
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 23:03:17.415530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.45
 ---- batch: 020 ----
mean loss: 173.37
 ---- batch: 030 ----
mean loss: 169.05
 ---- batch: 040 ----
mean loss: 174.84
 ---- batch: 050 ----
mean loss: 180.94
 ---- batch: 060 ----
mean loss: 190.07
 ---- batch: 070 ----
mean loss: 175.37
 ---- batch: 080 ----
mean loss: 186.63
 ---- batch: 090 ----
mean loss: 173.99
train mean loss: 177.24
epoch train time: 0:00:00.509836
elapsed time: 0:01:19.273503
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 23:03:17.925511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.15
 ---- batch: 020 ----
mean loss: 180.56
 ---- batch: 030 ----
mean loss: 169.53
 ---- batch: 040 ----
mean loss: 171.33
 ---- batch: 050 ----
mean loss: 180.74
 ---- batch: 060 ----
mean loss: 180.55
 ---- batch: 070 ----
mean loss: 180.20
 ---- batch: 080 ----
mean loss: 181.64
 ---- batch: 090 ----
mean loss: 173.20
train mean loss: 176.81
epoch train time: 0:00:00.500033
elapsed time: 0:01:19.773722
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 23:03:18.425734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.00
 ---- batch: 020 ----
mean loss: 177.26
 ---- batch: 030 ----
mean loss: 176.51
 ---- batch: 040 ----
mean loss: 171.28
 ---- batch: 050 ----
mean loss: 169.08
 ---- batch: 060 ----
mean loss: 181.52
 ---- batch: 070 ----
mean loss: 171.68
 ---- batch: 080 ----
mean loss: 181.31
 ---- batch: 090 ----
mean loss: 175.59
train mean loss: 175.43
epoch train time: 0:00:00.502438
elapsed time: 0:01:20.276310
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 23:03:18.928319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.50
 ---- batch: 020 ----
mean loss: 175.83
 ---- batch: 030 ----
mean loss: 172.17
 ---- batch: 040 ----
mean loss: 177.69
 ---- batch: 050 ----
mean loss: 176.31
 ---- batch: 060 ----
mean loss: 181.44
 ---- batch: 070 ----
mean loss: 178.54
 ---- batch: 080 ----
mean loss: 175.86
 ---- batch: 090 ----
mean loss: 171.43
train mean loss: 174.48
epoch train time: 0:00:00.501879
elapsed time: 0:01:20.778333
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 23:03:19.430340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.55
 ---- batch: 020 ----
mean loss: 164.66
 ---- batch: 030 ----
mean loss: 179.00
 ---- batch: 040 ----
mean loss: 166.36
 ---- batch: 050 ----
mean loss: 169.18
 ---- batch: 060 ----
mean loss: 182.98
 ---- batch: 070 ----
mean loss: 174.77
 ---- batch: 080 ----
mean loss: 179.21
 ---- batch: 090 ----
mean loss: 181.41
train mean loss: 174.16
epoch train time: 0:00:00.509453
elapsed time: 0:01:21.287933
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 23:03:19.939941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.24
 ---- batch: 020 ----
mean loss: 167.68
 ---- batch: 030 ----
mean loss: 167.18
 ---- batch: 040 ----
mean loss: 168.86
 ---- batch: 050 ----
mean loss: 179.12
 ---- batch: 060 ----
mean loss: 179.69
 ---- batch: 070 ----
mean loss: 173.41
 ---- batch: 080 ----
mean loss: 175.63
 ---- batch: 090 ----
mean loss: 178.56
train mean loss: 173.54
epoch train time: 0:00:00.504828
elapsed time: 0:01:21.792903
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 23:03:20.444934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.08
 ---- batch: 020 ----
mean loss: 160.39
 ---- batch: 030 ----
mean loss: 169.12
 ---- batch: 040 ----
mean loss: 177.33
 ---- batch: 050 ----
mean loss: 173.04
 ---- batch: 060 ----
mean loss: 175.30
 ---- batch: 070 ----
mean loss: 180.36
 ---- batch: 080 ----
mean loss: 172.14
 ---- batch: 090 ----
mean loss: 169.94
train mean loss: 172.05
epoch train time: 0:00:00.506926
elapsed time: 0:01:22.299996
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 23:03:20.952005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.16
 ---- batch: 020 ----
mean loss: 168.09
 ---- batch: 030 ----
mean loss: 169.51
 ---- batch: 040 ----
mean loss: 172.30
 ---- batch: 050 ----
mean loss: 174.21
 ---- batch: 060 ----
mean loss: 168.88
 ---- batch: 070 ----
mean loss: 173.22
 ---- batch: 080 ----
mean loss: 175.98
 ---- batch: 090 ----
mean loss: 176.63
train mean loss: 171.91
epoch train time: 0:00:00.497995
elapsed time: 0:01:22.798140
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 23:03:21.450147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.73
 ---- batch: 020 ----
mean loss: 166.99
 ---- batch: 030 ----
mean loss: 167.07
 ---- batch: 040 ----
mean loss: 170.82
 ---- batch: 050 ----
mean loss: 171.27
 ---- batch: 060 ----
mean loss: 173.08
 ---- batch: 070 ----
mean loss: 176.57
 ---- batch: 080 ----
mean loss: 174.52
 ---- batch: 090 ----
mean loss: 167.99
train mean loss: 171.08
epoch train time: 0:00:00.516570
elapsed time: 0:01:23.314856
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 23:03:21.966864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.81
 ---- batch: 020 ----
mean loss: 164.20
 ---- batch: 030 ----
mean loss: 165.52
 ---- batch: 040 ----
mean loss: 164.27
 ---- batch: 050 ----
mean loss: 175.61
 ---- batch: 060 ----
mean loss: 171.61
 ---- batch: 070 ----
mean loss: 169.34
 ---- batch: 080 ----
mean loss: 177.53
 ---- batch: 090 ----
mean loss: 171.23
train mean loss: 170.20
epoch train time: 0:00:00.502214
elapsed time: 0:01:23.817217
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 23:03:22.469228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.30
 ---- batch: 020 ----
mean loss: 164.04
 ---- batch: 030 ----
mean loss: 164.59
 ---- batch: 040 ----
mean loss: 170.19
 ---- batch: 050 ----
mean loss: 173.39
 ---- batch: 060 ----
mean loss: 164.72
 ---- batch: 070 ----
mean loss: 169.43
 ---- batch: 080 ----
mean loss: 170.66
 ---- batch: 090 ----
mean loss: 174.65
train mean loss: 169.10
epoch train time: 0:00:00.516710
elapsed time: 0:01:24.334078
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 23:03:22.986086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.36
 ---- batch: 020 ----
mean loss: 164.59
 ---- batch: 030 ----
mean loss: 168.12
 ---- batch: 040 ----
mean loss: 171.06
 ---- batch: 050 ----
mean loss: 176.89
 ---- batch: 060 ----
mean loss: 171.04
 ---- batch: 070 ----
mean loss: 167.95
 ---- batch: 080 ----
mean loss: 170.88
 ---- batch: 090 ----
mean loss: 167.40
train mean loss: 170.11
epoch train time: 0:00:00.502714
elapsed time: 0:01:24.836936
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 23:03:23.488954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.99
 ---- batch: 020 ----
mean loss: 165.45
 ---- batch: 030 ----
mean loss: 168.47
 ---- batch: 040 ----
mean loss: 169.36
 ---- batch: 050 ----
mean loss: 167.78
 ---- batch: 060 ----
mean loss: 171.04
 ---- batch: 070 ----
mean loss: 166.80
 ---- batch: 080 ----
mean loss: 168.88
 ---- batch: 090 ----
mean loss: 169.51
train mean loss: 169.15
epoch train time: 0:00:00.511363
elapsed time: 0:01:25.348452
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 23:03:24.000459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.27
 ---- batch: 020 ----
mean loss: 163.23
 ---- batch: 030 ----
mean loss: 164.33
 ---- batch: 040 ----
mean loss: 164.17
 ---- batch: 050 ----
mean loss: 169.37
 ---- batch: 060 ----
mean loss: 163.65
 ---- batch: 070 ----
mean loss: 175.68
 ---- batch: 080 ----
mean loss: 157.65
 ---- batch: 090 ----
mean loss: 169.19
train mean loss: 166.68
epoch train time: 0:00:00.503109
elapsed time: 0:01:25.851705
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 23:03:24.503730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.55
 ---- batch: 020 ----
mean loss: 166.74
 ---- batch: 030 ----
mean loss: 161.98
 ---- batch: 040 ----
mean loss: 170.39
 ---- batch: 050 ----
mean loss: 169.08
 ---- batch: 060 ----
mean loss: 172.20
 ---- batch: 070 ----
mean loss: 162.49
 ---- batch: 080 ----
mean loss: 160.34
 ---- batch: 090 ----
mean loss: 165.55
train mean loss: 166.82
epoch train time: 0:00:00.507569
elapsed time: 0:01:26.359439
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 23:03:25.011447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.69
 ---- batch: 020 ----
mean loss: 157.24
 ---- batch: 030 ----
mean loss: 168.52
 ---- batch: 040 ----
mean loss: 164.24
 ---- batch: 050 ----
mean loss: 161.24
 ---- batch: 060 ----
mean loss: 166.56
 ---- batch: 070 ----
mean loss: 167.56
 ---- batch: 080 ----
mean loss: 170.12
 ---- batch: 090 ----
mean loss: 170.91
train mean loss: 165.74
epoch train time: 0:00:00.503306
elapsed time: 0:01:26.862902
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 23:03:25.514921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.53
 ---- batch: 020 ----
mean loss: 167.33
 ---- batch: 030 ----
mean loss: 157.55
 ---- batch: 040 ----
mean loss: 162.05
 ---- batch: 050 ----
mean loss: 168.53
 ---- batch: 060 ----
mean loss: 168.07
 ---- batch: 070 ----
mean loss: 170.71
 ---- batch: 080 ----
mean loss: 166.72
 ---- batch: 090 ----
mean loss: 166.53
train mean loss: 165.58
epoch train time: 0:00:00.505643
elapsed time: 0:01:27.368718
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 23:03:26.020714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.44
 ---- batch: 020 ----
mean loss: 168.17
 ---- batch: 030 ----
mean loss: 162.75
 ---- batch: 040 ----
mean loss: 167.54
 ---- batch: 050 ----
mean loss: 162.95
 ---- batch: 060 ----
mean loss: 165.01
 ---- batch: 070 ----
mean loss: 167.01
 ---- batch: 080 ----
mean loss: 165.39
 ---- batch: 090 ----
mean loss: 168.53
train mean loss: 164.76
epoch train time: 0:00:00.505752
elapsed time: 0:01:27.874620
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 23:03:26.526629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.54
 ---- batch: 020 ----
mean loss: 169.87
 ---- batch: 030 ----
mean loss: 160.39
 ---- batch: 040 ----
mean loss: 165.30
 ---- batch: 050 ----
mean loss: 159.77
 ---- batch: 060 ----
mean loss: 167.62
 ---- batch: 070 ----
mean loss: 166.68
 ---- batch: 080 ----
mean loss: 166.65
 ---- batch: 090 ----
mean loss: 158.03
train mean loss: 164.41
epoch train time: 0:00:00.506149
elapsed time: 0:01:28.380931
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 23:03:27.032947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.16
 ---- batch: 020 ----
mean loss: 161.01
 ---- batch: 030 ----
mean loss: 165.47
 ---- batch: 040 ----
mean loss: 158.64
 ---- batch: 050 ----
mean loss: 165.57
 ---- batch: 060 ----
mean loss: 168.74
 ---- batch: 070 ----
mean loss: 168.18
 ---- batch: 080 ----
mean loss: 164.80
 ---- batch: 090 ----
mean loss: 159.03
train mean loss: 164.25
epoch train time: 0:00:00.505047
elapsed time: 0:01:28.886136
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 23:03:27.538146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.11
 ---- batch: 020 ----
mean loss: 160.77
 ---- batch: 030 ----
mean loss: 159.79
 ---- batch: 040 ----
mean loss: 160.53
 ---- batch: 050 ----
mean loss: 161.40
 ---- batch: 060 ----
mean loss: 175.05
 ---- batch: 070 ----
mean loss: 164.26
 ---- batch: 080 ----
mean loss: 162.31
 ---- batch: 090 ----
mean loss: 166.13
train mean loss: 164.80
epoch train time: 0:00:00.513600
elapsed time: 0:01:29.399886
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 23:03:28.051920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.64
 ---- batch: 020 ----
mean loss: 159.29
 ---- batch: 030 ----
mean loss: 167.50
 ---- batch: 040 ----
mean loss: 168.24
 ---- batch: 050 ----
mean loss: 162.78
 ---- batch: 060 ----
mean loss: 163.67
 ---- batch: 070 ----
mean loss: 167.23
 ---- batch: 080 ----
mean loss: 158.20
 ---- batch: 090 ----
mean loss: 162.07
train mean loss: 162.53
epoch train time: 0:00:00.503432
elapsed time: 0:01:29.903491
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 23:03:28.555517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.24
 ---- batch: 020 ----
mean loss: 166.05
 ---- batch: 030 ----
mean loss: 165.71
 ---- batch: 040 ----
mean loss: 156.23
 ---- batch: 050 ----
mean loss: 161.29
 ---- batch: 060 ----
mean loss: 164.21
 ---- batch: 070 ----
mean loss: 168.66
 ---- batch: 080 ----
mean loss: 161.00
 ---- batch: 090 ----
mean loss: 162.77
train mean loss: 162.81
epoch train time: 0:00:00.508026
elapsed time: 0:01:30.411696
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 23:03:29.063705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.00
 ---- batch: 020 ----
mean loss: 158.97
 ---- batch: 030 ----
mean loss: 159.90
 ---- batch: 040 ----
mean loss: 166.64
 ---- batch: 050 ----
mean loss: 165.44
 ---- batch: 060 ----
mean loss: 168.30
 ---- batch: 070 ----
mean loss: 154.78
 ---- batch: 080 ----
mean loss: 163.54
 ---- batch: 090 ----
mean loss: 163.58
train mean loss: 162.68
epoch train time: 0:00:00.504245
elapsed time: 0:01:30.916101
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 23:03:29.568112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.71
 ---- batch: 020 ----
mean loss: 157.42
 ---- batch: 030 ----
mean loss: 159.41
 ---- batch: 040 ----
mean loss: 161.37
 ---- batch: 050 ----
mean loss: 164.14
 ---- batch: 060 ----
mean loss: 169.36
 ---- batch: 070 ----
mean loss: 168.41
 ---- batch: 080 ----
mean loss: 150.50
 ---- batch: 090 ----
mean loss: 168.99
train mean loss: 162.59
epoch train time: 0:00:00.515510
elapsed time: 0:01:31.431758
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 23:03:30.083766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.28
 ---- batch: 020 ----
mean loss: 159.43
 ---- batch: 030 ----
mean loss: 162.63
 ---- batch: 040 ----
mean loss: 151.91
 ---- batch: 050 ----
mean loss: 164.84
 ---- batch: 060 ----
mean loss: 163.16
 ---- batch: 070 ----
mean loss: 164.16
 ---- batch: 080 ----
mean loss: 157.55
 ---- batch: 090 ----
mean loss: 165.68
train mean loss: 161.23
epoch train time: 0:00:00.500888
elapsed time: 0:01:31.932834
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 23:03:30.584841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.66
 ---- batch: 020 ----
mean loss: 150.85
 ---- batch: 030 ----
mean loss: 157.82
 ---- batch: 040 ----
mean loss: 160.99
 ---- batch: 050 ----
mean loss: 165.13
 ---- batch: 060 ----
mean loss: 165.27
 ---- batch: 070 ----
mean loss: 163.90
 ---- batch: 080 ----
mean loss: 164.10
 ---- batch: 090 ----
mean loss: 166.50
train mean loss: 160.03
epoch train time: 0:00:00.511564
elapsed time: 0:01:32.444544
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 23:03:31.096554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.06
 ---- batch: 020 ----
mean loss: 150.77
 ---- batch: 030 ----
mean loss: 155.99
 ---- batch: 040 ----
mean loss: 162.57
 ---- batch: 050 ----
mean loss: 158.41
 ---- batch: 060 ----
mean loss: 161.65
 ---- batch: 070 ----
mean loss: 159.55
 ---- batch: 080 ----
mean loss: 161.40
 ---- batch: 090 ----
mean loss: 168.24
train mean loss: 159.94
epoch train time: 0:00:00.506158
elapsed time: 0:01:32.950849
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 23:03:31.602856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.46
 ---- batch: 020 ----
mean loss: 156.65
 ---- batch: 030 ----
mean loss: 163.56
 ---- batch: 040 ----
mean loss: 162.15
 ---- batch: 050 ----
mean loss: 163.33
 ---- batch: 060 ----
mean loss: 159.40
 ---- batch: 070 ----
mean loss: 159.90
 ---- batch: 080 ----
mean loss: 158.33
 ---- batch: 090 ----
mean loss: 152.51
train mean loss: 159.47
epoch train time: 0:00:00.509968
elapsed time: 0:01:33.460979
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 23:03:32.113024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.07
 ---- batch: 020 ----
mean loss: 157.17
 ---- batch: 030 ----
mean loss: 155.02
 ---- batch: 040 ----
mean loss: 156.54
 ---- batch: 050 ----
mean loss: 159.26
 ---- batch: 060 ----
mean loss: 165.00
 ---- batch: 070 ----
mean loss: 158.17
 ---- batch: 080 ----
mean loss: 162.08
 ---- batch: 090 ----
mean loss: 161.26
train mean loss: 159.42
epoch train time: 0:00:00.505801
elapsed time: 0:01:33.966961
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 23:03:32.618969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.55
 ---- batch: 020 ----
mean loss: 157.74
 ---- batch: 030 ----
mean loss: 152.34
 ---- batch: 040 ----
mean loss: 159.31
 ---- batch: 050 ----
mean loss: 166.85
 ---- batch: 060 ----
mean loss: 159.64
 ---- batch: 070 ----
mean loss: 152.94
 ---- batch: 080 ----
mean loss: 163.99
 ---- batch: 090 ----
mean loss: 158.93
train mean loss: 158.70
epoch train time: 0:00:00.506163
elapsed time: 0:01:34.473269
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 23:03:33.125277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.43
 ---- batch: 020 ----
mean loss: 151.83
 ---- batch: 030 ----
mean loss: 152.70
 ---- batch: 040 ----
mean loss: 159.60
 ---- batch: 050 ----
mean loss: 160.64
 ---- batch: 060 ----
mean loss: 161.24
 ---- batch: 070 ----
mean loss: 159.15
 ---- batch: 080 ----
mean loss: 163.61
 ---- batch: 090 ----
mean loss: 156.29
train mean loss: 157.89
epoch train time: 0:00:00.504774
elapsed time: 0:01:34.978189
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 23:03:33.630201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.51
 ---- batch: 020 ----
mean loss: 161.42
 ---- batch: 030 ----
mean loss: 154.00
 ---- batch: 040 ----
mean loss: 159.31
 ---- batch: 050 ----
mean loss: 158.35
 ---- batch: 060 ----
mean loss: 158.54
 ---- batch: 070 ----
mean loss: 153.62
 ---- batch: 080 ----
mean loss: 156.33
 ---- batch: 090 ----
mean loss: 158.31
train mean loss: 157.40
epoch train time: 0:00:00.519219
elapsed time: 0:01:35.497554
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 23:03:34.149561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.00
 ---- batch: 020 ----
mean loss: 148.84
 ---- batch: 030 ----
mean loss: 149.78
 ---- batch: 040 ----
mean loss: 158.47
 ---- batch: 050 ----
mean loss: 161.31
 ---- batch: 060 ----
mean loss: 157.08
 ---- batch: 070 ----
mean loss: 160.57
 ---- batch: 080 ----
mean loss: 160.82
 ---- batch: 090 ----
mean loss: 158.15
train mean loss: 156.70
epoch train time: 0:00:00.502482
elapsed time: 0:01:36.000174
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 23:03:34.652181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.15
 ---- batch: 020 ----
mean loss: 152.53
 ---- batch: 030 ----
mean loss: 150.64
 ---- batch: 040 ----
mean loss: 157.61
 ---- batch: 050 ----
mean loss: 156.89
 ---- batch: 060 ----
mean loss: 166.30
 ---- batch: 070 ----
mean loss: 157.66
 ---- batch: 080 ----
mean loss: 156.24
 ---- batch: 090 ----
mean loss: 156.63
train mean loss: 156.41
epoch train time: 0:00:00.514002
elapsed time: 0:01:36.514326
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 23:03:35.166336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.09
 ---- batch: 020 ----
mean loss: 156.91
 ---- batch: 030 ----
mean loss: 153.27
 ---- batch: 040 ----
mean loss: 152.93
 ---- batch: 050 ----
mean loss: 156.62
 ---- batch: 060 ----
mean loss: 155.79
 ---- batch: 070 ----
mean loss: 158.92
 ---- batch: 080 ----
mean loss: 156.63
 ---- batch: 090 ----
mean loss: 164.58
train mean loss: 156.64
epoch train time: 0:00:00.513047
elapsed time: 0:01:37.027538
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 23:03:35.679590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.82
 ---- batch: 020 ----
mean loss: 154.16
 ---- batch: 030 ----
mean loss: 151.80
 ---- batch: 040 ----
mean loss: 158.35
 ---- batch: 050 ----
mean loss: 156.94
 ---- batch: 060 ----
mean loss: 159.49
 ---- batch: 070 ----
mean loss: 156.18
 ---- batch: 080 ----
mean loss: 160.51
 ---- batch: 090 ----
mean loss: 163.62
train mean loss: 156.06
epoch train time: 0:00:00.508645
elapsed time: 0:01:37.536383
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 23:03:36.188380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.67
 ---- batch: 020 ----
mean loss: 154.17
 ---- batch: 030 ----
mean loss: 153.26
 ---- batch: 040 ----
mean loss: 154.10
 ---- batch: 050 ----
mean loss: 160.60
 ---- batch: 060 ----
mean loss: 159.36
 ---- batch: 070 ----
mean loss: 154.57
 ---- batch: 080 ----
mean loss: 153.69
 ---- batch: 090 ----
mean loss: 165.85
train mean loss: 156.08
epoch train time: 0:00:00.503344
elapsed time: 0:01:38.039859
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 23:03:36.691867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.61
 ---- batch: 020 ----
mean loss: 146.29
 ---- batch: 030 ----
mean loss: 152.23
 ---- batch: 040 ----
mean loss: 155.21
 ---- batch: 050 ----
mean loss: 154.41
 ---- batch: 060 ----
mean loss: 150.90
 ---- batch: 070 ----
mean loss: 156.95
 ---- batch: 080 ----
mean loss: 161.13
 ---- batch: 090 ----
mean loss: 163.26
train mean loss: 154.93
epoch train time: 0:00:00.515559
elapsed time: 0:01:38.555561
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 23:03:37.207570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.89
 ---- batch: 020 ----
mean loss: 149.43
 ---- batch: 030 ----
mean loss: 149.93
 ---- batch: 040 ----
mean loss: 152.73
 ---- batch: 050 ----
mean loss: 152.46
 ---- batch: 060 ----
mean loss: 150.70
 ---- batch: 070 ----
mean loss: 153.54
 ---- batch: 080 ----
mean loss: 159.46
 ---- batch: 090 ----
mean loss: 164.30
train mean loss: 154.17
epoch train time: 0:00:00.503958
elapsed time: 0:01:39.059666
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 23:03:37.711677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.80
 ---- batch: 020 ----
mean loss: 152.04
 ---- batch: 030 ----
mean loss: 151.78
 ---- batch: 040 ----
mean loss: 156.50
 ---- batch: 050 ----
mean loss: 154.68
 ---- batch: 060 ----
mean loss: 149.44
 ---- batch: 070 ----
mean loss: 158.07
 ---- batch: 080 ----
mean loss: 154.76
 ---- batch: 090 ----
mean loss: 162.21
train mean loss: 154.18
epoch train time: 0:00:00.499809
elapsed time: 0:01:39.559625
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 23:03:38.211633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.80
 ---- batch: 020 ----
mean loss: 150.14
 ---- batch: 030 ----
mean loss: 159.50
 ---- batch: 040 ----
mean loss: 151.05
 ---- batch: 050 ----
mean loss: 154.01
 ---- batch: 060 ----
mean loss: 153.82
 ---- batch: 070 ----
mean loss: 158.25
 ---- batch: 080 ----
mean loss: 163.11
 ---- batch: 090 ----
mean loss: 152.29
train mean loss: 155.40
epoch train time: 0:00:00.499582
elapsed time: 0:01:40.059354
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 23:03:38.711362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.61
 ---- batch: 020 ----
mean loss: 149.64
 ---- batch: 030 ----
mean loss: 152.58
 ---- batch: 040 ----
mean loss: 149.26
 ---- batch: 050 ----
mean loss: 147.51
 ---- batch: 060 ----
mean loss: 153.95
 ---- batch: 070 ----
mean loss: 154.04
 ---- batch: 080 ----
mean loss: 158.99
 ---- batch: 090 ----
mean loss: 153.05
train mean loss: 152.99
epoch train time: 0:00:00.511428
elapsed time: 0:01:40.570963
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 23:03:39.222973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.60
 ---- batch: 020 ----
mean loss: 145.18
 ---- batch: 030 ----
mean loss: 152.35
 ---- batch: 040 ----
mean loss: 151.07
 ---- batch: 050 ----
mean loss: 152.39
 ---- batch: 060 ----
mean loss: 146.90
 ---- batch: 070 ----
mean loss: 154.52
 ---- batch: 080 ----
mean loss: 158.31
 ---- batch: 090 ----
mean loss: 161.12
train mean loss: 152.98
epoch train time: 0:00:00.512779
elapsed time: 0:01:41.083891
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 23:03:39.735927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.23
 ---- batch: 020 ----
mean loss: 150.45
 ---- batch: 030 ----
mean loss: 149.13
 ---- batch: 040 ----
mean loss: 155.23
 ---- batch: 050 ----
mean loss: 146.87
 ---- batch: 060 ----
mean loss: 158.12
 ---- batch: 070 ----
mean loss: 156.65
 ---- batch: 080 ----
mean loss: 149.91
 ---- batch: 090 ----
mean loss: 148.18
train mean loss: 151.79
epoch train time: 0:00:00.499571
elapsed time: 0:01:41.583633
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 23:03:40.235643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.24
 ---- batch: 020 ----
mean loss: 147.67
 ---- batch: 030 ----
mean loss: 149.36
 ---- batch: 040 ----
mean loss: 149.57
 ---- batch: 050 ----
mean loss: 143.19
 ---- batch: 060 ----
mean loss: 150.75
 ---- batch: 070 ----
mean loss: 152.18
 ---- batch: 080 ----
mean loss: 159.52
 ---- batch: 090 ----
mean loss: 158.45
train mean loss: 152.23
epoch train time: 0:00:00.502042
elapsed time: 0:01:42.085824
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 23:03:40.737832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.26
 ---- batch: 020 ----
mean loss: 144.95
 ---- batch: 030 ----
mean loss: 147.15
 ---- batch: 040 ----
mean loss: 150.19
 ---- batch: 050 ----
mean loss: 152.50
 ---- batch: 060 ----
mean loss: 155.97
 ---- batch: 070 ----
mean loss: 152.36
 ---- batch: 080 ----
mean loss: 155.34
 ---- batch: 090 ----
mean loss: 155.97
train mean loss: 151.77
epoch train time: 0:00:00.512756
elapsed time: 0:01:42.598779
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 23:03:41.250793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.68
 ---- batch: 020 ----
mean loss: 151.62
 ---- batch: 030 ----
mean loss: 150.52
 ---- batch: 040 ----
mean loss: 155.18
 ---- batch: 050 ----
mean loss: 146.86
 ---- batch: 060 ----
mean loss: 157.96
 ---- batch: 070 ----
mean loss: 148.80
 ---- batch: 080 ----
mean loss: 153.28
 ---- batch: 090 ----
mean loss: 152.17
train mean loss: 151.55
epoch train time: 0:00:00.506704
elapsed time: 0:01:43.105631
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 23:03:41.757672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.23
 ---- batch: 020 ----
mean loss: 146.00
 ---- batch: 030 ----
mean loss: 150.73
 ---- batch: 040 ----
mean loss: 150.19
 ---- batch: 050 ----
mean loss: 152.19
 ---- batch: 060 ----
mean loss: 148.96
 ---- batch: 070 ----
mean loss: 149.51
 ---- batch: 080 ----
mean loss: 154.69
 ---- batch: 090 ----
mean loss: 163.92
train mean loss: 150.98
epoch train time: 0:00:00.511418
elapsed time: 0:01:43.617235
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 23:03:42.269247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.89
 ---- batch: 020 ----
mean loss: 147.80
 ---- batch: 030 ----
mean loss: 152.25
 ---- batch: 040 ----
mean loss: 150.43
 ---- batch: 050 ----
mean loss: 144.61
 ---- batch: 060 ----
mean loss: 154.37
 ---- batch: 070 ----
mean loss: 154.95
 ---- batch: 080 ----
mean loss: 156.48
 ---- batch: 090 ----
mean loss: 152.15
train mean loss: 150.91
epoch train time: 0:00:00.510598
elapsed time: 0:01:44.127986
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 23:03:42.779998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.85
 ---- batch: 020 ----
mean loss: 151.99
 ---- batch: 030 ----
mean loss: 148.36
 ---- batch: 040 ----
mean loss: 150.56
 ---- batch: 050 ----
mean loss: 153.58
 ---- batch: 060 ----
mean loss: 150.80
 ---- batch: 070 ----
mean loss: 146.68
 ---- batch: 080 ----
mean loss: 154.25
 ---- batch: 090 ----
mean loss: 150.29
train mean loss: 150.41
epoch train time: 0:00:00.507420
elapsed time: 0:01:44.635614
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 23:03:43.287672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.90
 ---- batch: 020 ----
mean loss: 146.42
 ---- batch: 030 ----
mean loss: 154.98
 ---- batch: 040 ----
mean loss: 139.43
 ---- batch: 050 ----
mean loss: 155.03
 ---- batch: 060 ----
mean loss: 145.85
 ---- batch: 070 ----
mean loss: 156.48
 ---- batch: 080 ----
mean loss: 149.66
 ---- batch: 090 ----
mean loss: 154.58
train mean loss: 149.92
epoch train time: 0:00:00.506020
elapsed time: 0:01:45.141833
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 23:03:43.793843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.47
 ---- batch: 020 ----
mean loss: 148.00
 ---- batch: 030 ----
mean loss: 144.68
 ---- batch: 040 ----
mean loss: 153.14
 ---- batch: 050 ----
mean loss: 153.32
 ---- batch: 060 ----
mean loss: 154.29
 ---- batch: 070 ----
mean loss: 151.34
 ---- batch: 080 ----
mean loss: 152.46
 ---- batch: 090 ----
mean loss: 148.72
train mean loss: 150.57
epoch train time: 0:00:00.509413
elapsed time: 0:01:45.651421
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 23:03:44.303430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.24
 ---- batch: 020 ----
mean loss: 148.94
 ---- batch: 030 ----
mean loss: 144.81
 ---- batch: 040 ----
mean loss: 155.02
 ---- batch: 050 ----
mean loss: 144.87
 ---- batch: 060 ----
mean loss: 145.69
 ---- batch: 070 ----
mean loss: 149.91
 ---- batch: 080 ----
mean loss: 146.08
 ---- batch: 090 ----
mean loss: 150.29
train mean loss: 149.04
epoch train time: 0:00:00.505514
elapsed time: 0:01:46.157093
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 23:03:44.809116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.46
 ---- batch: 020 ----
mean loss: 152.37
 ---- batch: 030 ----
mean loss: 149.72
 ---- batch: 040 ----
mean loss: 150.03
 ---- batch: 050 ----
mean loss: 142.77
 ---- batch: 060 ----
mean loss: 154.24
 ---- batch: 070 ----
mean loss: 148.04
 ---- batch: 080 ----
mean loss: 149.24
 ---- batch: 090 ----
mean loss: 147.97
train mean loss: 148.34
epoch train time: 0:00:00.522001
elapsed time: 0:01:46.679286
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 23:03:45.331297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.90
 ---- batch: 020 ----
mean loss: 148.55
 ---- batch: 030 ----
mean loss: 145.27
 ---- batch: 040 ----
mean loss: 148.42
 ---- batch: 050 ----
mean loss: 150.17
 ---- batch: 060 ----
mean loss: 149.98
 ---- batch: 070 ----
mean loss: 150.49
 ---- batch: 080 ----
mean loss: 152.66
 ---- batch: 090 ----
mean loss: 157.88
train mean loss: 149.26
epoch train time: 0:00:00.528404
elapsed time: 0:01:47.207852
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 23:03:45.859895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.46
 ---- batch: 020 ----
mean loss: 146.92
 ---- batch: 030 ----
mean loss: 149.25
 ---- batch: 040 ----
mean loss: 151.56
 ---- batch: 050 ----
mean loss: 151.50
 ---- batch: 060 ----
mean loss: 148.42
 ---- batch: 070 ----
mean loss: 145.25
 ---- batch: 080 ----
mean loss: 148.62
 ---- batch: 090 ----
mean loss: 148.94
train mean loss: 148.85
epoch train time: 0:00:00.503654
elapsed time: 0:01:47.711700
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 23:03:46.363708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.56
 ---- batch: 020 ----
mean loss: 145.21
 ---- batch: 030 ----
mean loss: 140.41
 ---- batch: 040 ----
mean loss: 146.88
 ---- batch: 050 ----
mean loss: 147.09
 ---- batch: 060 ----
mean loss: 152.90
 ---- batch: 070 ----
mean loss: 144.48
 ---- batch: 080 ----
mean loss: 157.53
 ---- batch: 090 ----
mean loss: 149.37
train mean loss: 147.54
epoch train time: 0:00:00.498899
elapsed time: 0:01:48.210767
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 23:03:46.862778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.19
 ---- batch: 020 ----
mean loss: 146.61
 ---- batch: 030 ----
mean loss: 139.97
 ---- batch: 040 ----
mean loss: 149.14
 ---- batch: 050 ----
mean loss: 139.65
 ---- batch: 060 ----
mean loss: 144.28
 ---- batch: 070 ----
mean loss: 150.32
 ---- batch: 080 ----
mean loss: 152.40
 ---- batch: 090 ----
mean loss: 151.55
train mean loss: 147.60
epoch train time: 0:00:00.500852
elapsed time: 0:01:48.711787
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 23:03:47.363801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.34
 ---- batch: 020 ----
mean loss: 141.18
 ---- batch: 030 ----
mean loss: 143.16
 ---- batch: 040 ----
mean loss: 145.79
 ---- batch: 050 ----
mean loss: 151.37
 ---- batch: 060 ----
mean loss: 152.93
 ---- batch: 070 ----
mean loss: 152.73
 ---- batch: 080 ----
mean loss: 142.22
 ---- batch: 090 ----
mean loss: 153.44
train mean loss: 147.74
epoch train time: 0:00:00.504543
elapsed time: 0:01:49.216479
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 23:03:47.868486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.72
 ---- batch: 020 ----
mean loss: 147.67
 ---- batch: 030 ----
mean loss: 141.45
 ---- batch: 040 ----
mean loss: 153.09
 ---- batch: 050 ----
mean loss: 143.33
 ---- batch: 060 ----
mean loss: 148.24
 ---- batch: 070 ----
mean loss: 140.76
 ---- batch: 080 ----
mean loss: 150.60
 ---- batch: 090 ----
mean loss: 149.75
train mean loss: 146.79
epoch train time: 0:00:00.501644
elapsed time: 0:01:49.718322
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 23:03:48.370331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.36
 ---- batch: 020 ----
mean loss: 145.36
 ---- batch: 030 ----
mean loss: 142.32
 ---- batch: 040 ----
mean loss: 144.51
 ---- batch: 050 ----
mean loss: 143.60
 ---- batch: 060 ----
mean loss: 149.83
 ---- batch: 070 ----
mean loss: 143.33
 ---- batch: 080 ----
mean loss: 148.95
 ---- batch: 090 ----
mean loss: 156.57
train mean loss: 146.62
epoch train time: 0:00:00.504894
elapsed time: 0:01:50.223363
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 23:03:48.875373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.57
 ---- batch: 020 ----
mean loss: 146.95
 ---- batch: 030 ----
mean loss: 150.17
 ---- batch: 040 ----
mean loss: 147.56
 ---- batch: 050 ----
mean loss: 142.37
 ---- batch: 060 ----
mean loss: 142.65
 ---- batch: 070 ----
mean loss: 149.93
 ---- batch: 080 ----
mean loss: 146.87
 ---- batch: 090 ----
mean loss: 152.95
train mean loss: 146.32
epoch train time: 0:00:00.500922
elapsed time: 0:01:50.724430
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 23:03:49.376463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.56
 ---- batch: 020 ----
mean loss: 141.42
 ---- batch: 030 ----
mean loss: 145.82
 ---- batch: 040 ----
mean loss: 148.25
 ---- batch: 050 ----
mean loss: 142.44
 ---- batch: 060 ----
mean loss: 140.87
 ---- batch: 070 ----
mean loss: 145.99
 ---- batch: 080 ----
mean loss: 143.04
 ---- batch: 090 ----
mean loss: 150.65
train mean loss: 145.38
epoch train time: 0:00:00.509517
elapsed time: 0:01:51.234122
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 23:03:49.886133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.97
 ---- batch: 020 ----
mean loss: 147.30
 ---- batch: 030 ----
mean loss: 136.77
 ---- batch: 040 ----
mean loss: 143.64
 ---- batch: 050 ----
mean loss: 150.74
 ---- batch: 060 ----
mean loss: 147.19
 ---- batch: 070 ----
mean loss: 149.33
 ---- batch: 080 ----
mean loss: 149.93
 ---- batch: 090 ----
mean loss: 147.76
train mean loss: 146.30
epoch train time: 0:00:00.507015
elapsed time: 0:01:51.741286
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 23:03:50.393294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.73
 ---- batch: 020 ----
mean loss: 145.89
 ---- batch: 030 ----
mean loss: 141.80
 ---- batch: 040 ----
mean loss: 139.80
 ---- batch: 050 ----
mean loss: 145.63
 ---- batch: 060 ----
mean loss: 149.66
 ---- batch: 070 ----
mean loss: 148.18
 ---- batch: 080 ----
mean loss: 146.94
 ---- batch: 090 ----
mean loss: 146.50
train mean loss: 145.29
epoch train time: 0:00:00.508539
elapsed time: 0:01:52.249987
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 23:03:50.901999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.27
 ---- batch: 020 ----
mean loss: 140.62
 ---- batch: 030 ----
mean loss: 147.26
 ---- batch: 040 ----
mean loss: 142.95
 ---- batch: 050 ----
mean loss: 138.57
 ---- batch: 060 ----
mean loss: 151.75
 ---- batch: 070 ----
mean loss: 149.32
 ---- batch: 080 ----
mean loss: 152.73
 ---- batch: 090 ----
mean loss: 146.93
train mean loss: 145.15
epoch train time: 0:00:00.505855
elapsed time: 0:01:52.755991
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 23:03:51.408022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.40
 ---- batch: 020 ----
mean loss: 146.15
 ---- batch: 030 ----
mean loss: 146.18
 ---- batch: 040 ----
mean loss: 145.84
 ---- batch: 050 ----
mean loss: 148.84
 ---- batch: 060 ----
mean loss: 145.46
 ---- batch: 070 ----
mean loss: 142.40
 ---- batch: 080 ----
mean loss: 143.71
 ---- batch: 090 ----
mean loss: 144.07
train mean loss: 145.29
epoch train time: 0:00:00.510156
elapsed time: 0:01:53.266325
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 23:03:51.918335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.73
 ---- batch: 020 ----
mean loss: 139.92
 ---- batch: 030 ----
mean loss: 148.50
 ---- batch: 040 ----
mean loss: 140.96
 ---- batch: 050 ----
mean loss: 139.36
 ---- batch: 060 ----
mean loss: 137.54
 ---- batch: 070 ----
mean loss: 145.53
 ---- batch: 080 ----
mean loss: 149.37
 ---- batch: 090 ----
mean loss: 144.29
train mean loss: 143.94
epoch train time: 0:00:00.505866
elapsed time: 0:01:53.772338
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 23:03:52.424347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.45
 ---- batch: 020 ----
mean loss: 140.62
 ---- batch: 030 ----
mean loss: 142.82
 ---- batch: 040 ----
mean loss: 148.88
 ---- batch: 050 ----
mean loss: 150.91
 ---- batch: 060 ----
mean loss: 144.51
 ---- batch: 070 ----
mean loss: 145.71
 ---- batch: 080 ----
mean loss: 147.02
 ---- batch: 090 ----
mean loss: 145.32
train mean loss: 144.30
epoch train time: 0:00:00.508144
elapsed time: 0:01:54.280629
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 23:03:52.932638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.74
 ---- batch: 020 ----
mean loss: 141.85
 ---- batch: 030 ----
mean loss: 140.07
 ---- batch: 040 ----
mean loss: 142.68
 ---- batch: 050 ----
mean loss: 145.67
 ---- batch: 060 ----
mean loss: 142.87
 ---- batch: 070 ----
mean loss: 148.07
 ---- batch: 080 ----
mean loss: 149.30
 ---- batch: 090 ----
mean loss: 142.05
train mean loss: 143.59
epoch train time: 0:00:00.501220
elapsed time: 0:01:54.782007
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 23:03:53.434016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.69
 ---- batch: 020 ----
mean loss: 136.76
 ---- batch: 030 ----
mean loss: 138.21
 ---- batch: 040 ----
mean loss: 142.04
 ---- batch: 050 ----
mean loss: 136.24
 ---- batch: 060 ----
mean loss: 143.82
 ---- batch: 070 ----
mean loss: 154.44
 ---- batch: 080 ----
mean loss: 146.44
 ---- batch: 090 ----
mean loss: 142.85
train mean loss: 143.32
epoch train time: 0:00:00.516852
elapsed time: 0:01:55.299011
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 23:03:53.951029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.04
 ---- batch: 020 ----
mean loss: 146.25
 ---- batch: 030 ----
mean loss: 149.21
 ---- batch: 040 ----
mean loss: 148.36
 ---- batch: 050 ----
mean loss: 137.11
 ---- batch: 060 ----
mean loss: 148.65
 ---- batch: 070 ----
mean loss: 144.50
 ---- batch: 080 ----
mean loss: 144.40
 ---- batch: 090 ----
mean loss: 139.34
train mean loss: 143.72
epoch train time: 0:00:00.509566
elapsed time: 0:01:55.808733
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 23:03:54.460759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.95
 ---- batch: 020 ----
mean loss: 137.06
 ---- batch: 030 ----
mean loss: 142.50
 ---- batch: 040 ----
mean loss: 144.47
 ---- batch: 050 ----
mean loss: 142.97
 ---- batch: 060 ----
mean loss: 153.04
 ---- batch: 070 ----
mean loss: 146.63
 ---- batch: 080 ----
mean loss: 143.37
 ---- batch: 090 ----
mean loss: 147.66
train mean loss: 144.34
epoch train time: 0:00:00.508891
elapsed time: 0:01:56.317790
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 23:03:54.969821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.62
 ---- batch: 020 ----
mean loss: 139.60
 ---- batch: 030 ----
mean loss: 141.93
 ---- batch: 040 ----
mean loss: 138.00
 ---- batch: 050 ----
mean loss: 144.79
 ---- batch: 060 ----
mean loss: 144.96
 ---- batch: 070 ----
mean loss: 147.64
 ---- batch: 080 ----
mean loss: 138.31
 ---- batch: 090 ----
mean loss: 145.07
train mean loss: 142.86
epoch train time: 0:00:00.498352
elapsed time: 0:01:56.816310
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 23:03:55.468318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.21
 ---- batch: 020 ----
mean loss: 141.27
 ---- batch: 030 ----
mean loss: 138.30
 ---- batch: 040 ----
mean loss: 149.06
 ---- batch: 050 ----
mean loss: 145.33
 ---- batch: 060 ----
mean loss: 146.09
 ---- batch: 070 ----
mean loss: 143.50
 ---- batch: 080 ----
mean loss: 135.81
 ---- batch: 090 ----
mean loss: 143.18
train mean loss: 142.43
epoch train time: 0:00:00.510744
elapsed time: 0:01:57.327202
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 23:03:55.979212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.66
 ---- batch: 020 ----
mean loss: 138.29
 ---- batch: 030 ----
mean loss: 147.74
 ---- batch: 040 ----
mean loss: 142.57
 ---- batch: 050 ----
mean loss: 140.77
 ---- batch: 060 ----
mean loss: 142.77
 ---- batch: 070 ----
mean loss: 139.04
 ---- batch: 080 ----
mean loss: 139.52
 ---- batch: 090 ----
mean loss: 145.97
train mean loss: 142.61
epoch train time: 0:00:00.509177
elapsed time: 0:01:57.836541
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 23:03:56.488548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.52
 ---- batch: 020 ----
mean loss: 138.36
 ---- batch: 030 ----
mean loss: 147.11
 ---- batch: 040 ----
mean loss: 135.55
 ---- batch: 050 ----
mean loss: 144.68
 ---- batch: 060 ----
mean loss: 135.96
 ---- batch: 070 ----
mean loss: 145.33
 ---- batch: 080 ----
mean loss: 140.48
 ---- batch: 090 ----
mean loss: 144.61
train mean loss: 142.24
epoch train time: 0:00:00.513624
elapsed time: 0:01:58.350313
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 23:03:57.002322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.91
 ---- batch: 020 ----
mean loss: 144.78
 ---- batch: 030 ----
mean loss: 134.64
 ---- batch: 040 ----
mean loss: 137.99
 ---- batch: 050 ----
mean loss: 141.28
 ---- batch: 060 ----
mean loss: 151.88
 ---- batch: 070 ----
mean loss: 139.27
 ---- batch: 080 ----
mean loss: 145.40
 ---- batch: 090 ----
mean loss: 138.98
train mean loss: 141.66
epoch train time: 0:00:00.503415
elapsed time: 0:01:58.853877
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 23:03:57.505889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.78
 ---- batch: 020 ----
mean loss: 137.34
 ---- batch: 030 ----
mean loss: 135.65
 ---- batch: 040 ----
mean loss: 148.71
 ---- batch: 050 ----
mean loss: 140.32
 ---- batch: 060 ----
mean loss: 145.97
 ---- batch: 070 ----
mean loss: 141.56
 ---- batch: 080 ----
mean loss: 141.94
 ---- batch: 090 ----
mean loss: 141.98
train mean loss: 141.78
epoch train time: 0:00:00.508980
elapsed time: 0:01:59.363007
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 23:03:58.015016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.75
 ---- batch: 020 ----
mean loss: 142.82
 ---- batch: 030 ----
mean loss: 140.76
 ---- batch: 040 ----
mean loss: 142.62
 ---- batch: 050 ----
mean loss: 145.85
 ---- batch: 060 ----
mean loss: 137.83
 ---- batch: 070 ----
mean loss: 145.17
 ---- batch: 080 ----
mean loss: 135.92
 ---- batch: 090 ----
mean loss: 138.37
train mean loss: 141.02
epoch train time: 0:00:00.495273
elapsed time: 0:01:59.858423
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 23:03:58.510462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.24
 ---- batch: 020 ----
mean loss: 135.92
 ---- batch: 030 ----
mean loss: 139.71
 ---- batch: 040 ----
mean loss: 139.39
 ---- batch: 050 ----
mean loss: 141.64
 ---- batch: 060 ----
mean loss: 140.63
 ---- batch: 070 ----
mean loss: 145.49
 ---- batch: 080 ----
mean loss: 141.28
 ---- batch: 090 ----
mean loss: 143.74
train mean loss: 140.84
epoch train time: 0:00:00.509226
elapsed time: 0:02:00.367827
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 23:03:59.019862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.67
 ---- batch: 020 ----
mean loss: 143.09
 ---- batch: 030 ----
mean loss: 140.96
 ---- batch: 040 ----
mean loss: 141.06
 ---- batch: 050 ----
mean loss: 138.62
 ---- batch: 060 ----
mean loss: 138.79
 ---- batch: 070 ----
mean loss: 141.67
 ---- batch: 080 ----
mean loss: 141.45
 ---- batch: 090 ----
mean loss: 143.39
train mean loss: 141.40
epoch train time: 0:00:00.503119
elapsed time: 0:02:00.871118
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 23:03:59.523132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.11
 ---- batch: 020 ----
mean loss: 136.56
 ---- batch: 030 ----
mean loss: 143.57
 ---- batch: 040 ----
mean loss: 139.81
 ---- batch: 050 ----
mean loss: 136.47
 ---- batch: 060 ----
mean loss: 145.53
 ---- batch: 070 ----
mean loss: 145.15
 ---- batch: 080 ----
mean loss: 139.93
 ---- batch: 090 ----
mean loss: 139.55
train mean loss: 139.92
epoch train time: 0:00:00.507585
elapsed time: 0:02:01.378870
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 23:04:00.030866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.92
 ---- batch: 020 ----
mean loss: 140.82
 ---- batch: 030 ----
mean loss: 146.59
 ---- batch: 040 ----
mean loss: 135.04
 ---- batch: 050 ----
mean loss: 137.66
 ---- batch: 060 ----
mean loss: 142.40
 ---- batch: 070 ----
mean loss: 141.61
 ---- batch: 080 ----
mean loss: 139.43
 ---- batch: 090 ----
mean loss: 142.34
train mean loss: 140.64
epoch train time: 0:00:00.501339
elapsed time: 0:02:01.880342
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 23:04:00.532352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.09
 ---- batch: 020 ----
mean loss: 137.82
 ---- batch: 030 ----
mean loss: 138.27
 ---- batch: 040 ----
mean loss: 141.78
 ---- batch: 050 ----
mean loss: 134.63
 ---- batch: 060 ----
mean loss: 146.59
 ---- batch: 070 ----
mean loss: 143.56
 ---- batch: 080 ----
mean loss: 141.63
 ---- batch: 090 ----
mean loss: 138.09
train mean loss: 139.67
epoch train time: 0:00:00.507526
elapsed time: 0:02:02.388022
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 23:04:01.040033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.80
 ---- batch: 020 ----
mean loss: 134.11
 ---- batch: 030 ----
mean loss: 134.55
 ---- batch: 040 ----
mean loss: 141.93
 ---- batch: 050 ----
mean loss: 143.27
 ---- batch: 060 ----
mean loss: 145.61
 ---- batch: 070 ----
mean loss: 136.00
 ---- batch: 080 ----
mean loss: 135.99
 ---- batch: 090 ----
mean loss: 147.18
train mean loss: 139.38
epoch train time: 0:00:00.502213
elapsed time: 0:02:02.890386
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 23:04:01.542402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.92
 ---- batch: 020 ----
mean loss: 133.59
 ---- batch: 030 ----
mean loss: 138.52
 ---- batch: 040 ----
mean loss: 136.86
 ---- batch: 050 ----
mean loss: 136.99
 ---- batch: 060 ----
mean loss: 140.83
 ---- batch: 070 ----
mean loss: 141.75
 ---- batch: 080 ----
mean loss: 138.66
 ---- batch: 090 ----
mean loss: 136.17
train mean loss: 138.32
epoch train time: 0:00:00.507107
elapsed time: 0:02:03.397672
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 23:04:02.049701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.60
 ---- batch: 020 ----
mean loss: 138.99
 ---- batch: 030 ----
mean loss: 137.76
 ---- batch: 040 ----
mean loss: 141.22
 ---- batch: 050 ----
mean loss: 141.99
 ---- batch: 060 ----
mean loss: 142.45
 ---- batch: 070 ----
mean loss: 136.30
 ---- batch: 080 ----
mean loss: 136.38
 ---- batch: 090 ----
mean loss: 139.81
train mean loss: 138.55
epoch train time: 0:00:00.509430
elapsed time: 0:02:03.907266
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 23:04:02.559294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.26
 ---- batch: 020 ----
mean loss: 131.85
 ---- batch: 030 ----
mean loss: 137.68
 ---- batch: 040 ----
mean loss: 135.77
 ---- batch: 050 ----
mean loss: 139.63
 ---- batch: 060 ----
mean loss: 139.59
 ---- batch: 070 ----
mean loss: 136.83
 ---- batch: 080 ----
mean loss: 139.92
 ---- batch: 090 ----
mean loss: 143.71
train mean loss: 138.17
epoch train time: 0:00:00.504327
elapsed time: 0:02:04.411755
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 23:04:03.063761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.77
 ---- batch: 020 ----
mean loss: 138.83
 ---- batch: 030 ----
mean loss: 134.95
 ---- batch: 040 ----
mean loss: 134.22
 ---- batch: 050 ----
mean loss: 140.37
 ---- batch: 060 ----
mean loss: 134.23
 ---- batch: 070 ----
mean loss: 137.47
 ---- batch: 080 ----
mean loss: 141.85
 ---- batch: 090 ----
mean loss: 142.21
train mean loss: 138.39
epoch train time: 0:00:00.496121
elapsed time: 0:02:04.908024
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 23:04:03.560033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.36
 ---- batch: 020 ----
mean loss: 133.61
 ---- batch: 030 ----
mean loss: 135.49
 ---- batch: 040 ----
mean loss: 139.24
 ---- batch: 050 ----
mean loss: 135.84
 ---- batch: 060 ----
mean loss: 142.11
 ---- batch: 070 ----
mean loss: 136.56
 ---- batch: 080 ----
mean loss: 140.64
 ---- batch: 090 ----
mean loss: 139.28
train mean loss: 137.57
epoch train time: 0:00:00.507552
elapsed time: 0:02:05.415718
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 23:04:04.067757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.36
 ---- batch: 020 ----
mean loss: 134.82
 ---- batch: 030 ----
mean loss: 137.31
 ---- batch: 040 ----
mean loss: 140.77
 ---- batch: 050 ----
mean loss: 138.94
 ---- batch: 060 ----
mean loss: 140.30
 ---- batch: 070 ----
mean loss: 135.17
 ---- batch: 080 ----
mean loss: 138.11
 ---- batch: 090 ----
mean loss: 140.19
train mean loss: 137.87
epoch train time: 0:00:00.502461
elapsed time: 0:02:05.918355
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 23:04:04.570366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.27
 ---- batch: 020 ----
mean loss: 134.69
 ---- batch: 030 ----
mean loss: 132.31
 ---- batch: 040 ----
mean loss: 138.28
 ---- batch: 050 ----
mean loss: 138.60
 ---- batch: 060 ----
mean loss: 140.64
 ---- batch: 070 ----
mean loss: 139.25
 ---- batch: 080 ----
mean loss: 137.63
 ---- batch: 090 ----
mean loss: 133.60
train mean loss: 137.49
epoch train time: 0:00:00.506124
elapsed time: 0:02:06.424626
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 23:04:05.076636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.52
 ---- batch: 020 ----
mean loss: 138.48
 ---- batch: 030 ----
mean loss: 130.36
 ---- batch: 040 ----
mean loss: 142.99
 ---- batch: 050 ----
mean loss: 140.71
 ---- batch: 060 ----
mean loss: 143.53
 ---- batch: 070 ----
mean loss: 134.52
 ---- batch: 080 ----
mean loss: 138.78
 ---- batch: 090 ----
mean loss: 138.65
train mean loss: 138.08
epoch train time: 0:00:00.499897
elapsed time: 0:02:06.924669
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 23:04:05.576676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.25
 ---- batch: 020 ----
mean loss: 133.03
 ---- batch: 030 ----
mean loss: 131.68
 ---- batch: 040 ----
mean loss: 143.42
 ---- batch: 050 ----
mean loss: 143.00
 ---- batch: 060 ----
mean loss: 138.68
 ---- batch: 070 ----
mean loss: 138.52
 ---- batch: 080 ----
mean loss: 136.63
 ---- batch: 090 ----
mean loss: 137.43
train mean loss: 137.30
epoch train time: 0:00:00.507659
elapsed time: 0:02:07.432490
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 23:04:06.084523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.19
 ---- batch: 020 ----
mean loss: 128.20
 ---- batch: 030 ----
mean loss: 138.31
 ---- batch: 040 ----
mean loss: 137.66
 ---- batch: 050 ----
mean loss: 134.96
 ---- batch: 060 ----
mean loss: 141.87
 ---- batch: 070 ----
mean loss: 148.86
 ---- batch: 080 ----
mean loss: 131.86
 ---- batch: 090 ----
mean loss: 144.18
train mean loss: 137.87
epoch train time: 0:00:00.507232
elapsed time: 0:02:07.939892
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 23:04:06.591929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.16
 ---- batch: 020 ----
mean loss: 138.61
 ---- batch: 030 ----
mean loss: 138.12
 ---- batch: 040 ----
mean loss: 135.92
 ---- batch: 050 ----
mean loss: 138.06
 ---- batch: 060 ----
mean loss: 137.67
 ---- batch: 070 ----
mean loss: 137.01
 ---- batch: 080 ----
mean loss: 138.38
 ---- batch: 090 ----
mean loss: 142.05
train mean loss: 137.22
epoch train time: 0:00:00.521070
elapsed time: 0:02:08.461133
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 23:04:07.113152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.31
 ---- batch: 020 ----
mean loss: 137.31
 ---- batch: 030 ----
mean loss: 130.99
 ---- batch: 040 ----
mean loss: 134.89
 ---- batch: 050 ----
mean loss: 137.74
 ---- batch: 060 ----
mean loss: 139.71
 ---- batch: 070 ----
mean loss: 138.01
 ---- batch: 080 ----
mean loss: 145.10
 ---- batch: 090 ----
mean loss: 139.24
train mean loss: 136.93
epoch train time: 0:00:00.508080
elapsed time: 0:02:08.969372
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 23:04:07.621385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.62
 ---- batch: 020 ----
mean loss: 134.56
 ---- batch: 030 ----
mean loss: 141.17
 ---- batch: 040 ----
mean loss: 130.77
 ---- batch: 050 ----
mean loss: 134.98
 ---- batch: 060 ----
mean loss: 136.43
 ---- batch: 070 ----
mean loss: 136.35
 ---- batch: 080 ----
mean loss: 140.17
 ---- batch: 090 ----
mean loss: 141.90
train mean loss: 136.69
epoch train time: 0:00:00.508866
elapsed time: 0:02:09.478390
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 23:04:08.130413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.59
 ---- batch: 020 ----
mean loss: 133.70
 ---- batch: 030 ----
mean loss: 137.32
 ---- batch: 040 ----
mean loss: 136.38
 ---- batch: 050 ----
mean loss: 139.31
 ---- batch: 060 ----
mean loss: 140.70
 ---- batch: 070 ----
mean loss: 136.50
 ---- batch: 080 ----
mean loss: 137.64
 ---- batch: 090 ----
mean loss: 129.62
train mean loss: 136.08
epoch train time: 0:00:00.498430
elapsed time: 0:02:09.976976
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 23:04:08.628984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.52
 ---- batch: 020 ----
mean loss: 133.20
 ---- batch: 030 ----
mean loss: 132.99
 ---- batch: 040 ----
mean loss: 133.32
 ---- batch: 050 ----
mean loss: 134.71
 ---- batch: 060 ----
mean loss: 140.34
 ---- batch: 070 ----
mean loss: 139.72
 ---- batch: 080 ----
mean loss: 139.93
 ---- batch: 090 ----
mean loss: 139.58
train mean loss: 135.87
epoch train time: 0:00:00.518557
elapsed time: 0:02:10.495683
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 23:04:09.147692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.11
 ---- batch: 020 ----
mean loss: 132.87
 ---- batch: 030 ----
mean loss: 135.04
 ---- batch: 040 ----
mean loss: 131.52
 ---- batch: 050 ----
mean loss: 135.47
 ---- batch: 060 ----
mean loss: 140.69
 ---- batch: 070 ----
mean loss: 133.24
 ---- batch: 080 ----
mean loss: 130.65
 ---- batch: 090 ----
mean loss: 140.95
train mean loss: 134.49
epoch train time: 0:00:00.505223
elapsed time: 0:02:11.001054
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 23:04:09.653068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.27
 ---- batch: 020 ----
mean loss: 131.53
 ---- batch: 030 ----
mean loss: 134.18
 ---- batch: 040 ----
mean loss: 137.54
 ---- batch: 050 ----
mean loss: 134.15
 ---- batch: 060 ----
mean loss: 131.35
 ---- batch: 070 ----
mean loss: 137.05
 ---- batch: 080 ----
mean loss: 140.40
 ---- batch: 090 ----
mean loss: 144.46
train mean loss: 135.27
epoch train time: 0:00:00.509663
elapsed time: 0:02:11.510914
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 23:04:10.162952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.05
 ---- batch: 020 ----
mean loss: 133.71
 ---- batch: 030 ----
mean loss: 130.65
 ---- batch: 040 ----
mean loss: 134.34
 ---- batch: 050 ----
mean loss: 135.91
 ---- batch: 060 ----
mean loss: 138.96
 ---- batch: 070 ----
mean loss: 137.67
 ---- batch: 080 ----
mean loss: 138.28
 ---- batch: 090 ----
mean loss: 137.14
train mean loss: 135.33
epoch train time: 0:00:00.497330
elapsed time: 0:02:12.008432
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 23:04:10.660439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.74
 ---- batch: 020 ----
mean loss: 134.69
 ---- batch: 030 ----
mean loss: 137.21
 ---- batch: 040 ----
mean loss: 132.04
 ---- batch: 050 ----
mean loss: 133.29
 ---- batch: 060 ----
mean loss: 137.52
 ---- batch: 070 ----
mean loss: 140.22
 ---- batch: 080 ----
mean loss: 134.43
 ---- batch: 090 ----
mean loss: 135.93
train mean loss: 134.30
epoch train time: 0:00:00.504896
elapsed time: 0:02:12.513481
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 23:04:11.165507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.15
 ---- batch: 020 ----
mean loss: 133.53
 ---- batch: 030 ----
mean loss: 135.75
 ---- batch: 040 ----
mean loss: 138.22
 ---- batch: 050 ----
mean loss: 132.62
 ---- batch: 060 ----
mean loss: 138.23
 ---- batch: 070 ----
mean loss: 135.11
 ---- batch: 080 ----
mean loss: 125.43
 ---- batch: 090 ----
mean loss: 141.54
train mean loss: 134.84
epoch train time: 0:00:00.507739
elapsed time: 0:02:13.021398
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 23:04:11.673409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.73
 ---- batch: 020 ----
mean loss: 133.69
 ---- batch: 030 ----
mean loss: 128.03
 ---- batch: 040 ----
mean loss: 131.91
 ---- batch: 050 ----
mean loss: 129.91
 ---- batch: 060 ----
mean loss: 135.60
 ---- batch: 070 ----
mean loss: 132.46
 ---- batch: 080 ----
mean loss: 136.12
 ---- batch: 090 ----
mean loss: 144.51
train mean loss: 134.53
epoch train time: 0:00:00.509177
elapsed time: 0:02:13.530724
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 23:04:12.182732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.60
 ---- batch: 020 ----
mean loss: 128.67
 ---- batch: 030 ----
mean loss: 131.49
 ---- batch: 040 ----
mean loss: 138.35
 ---- batch: 050 ----
mean loss: 143.92
 ---- batch: 060 ----
mean loss: 141.40
 ---- batch: 070 ----
mean loss: 133.60
 ---- batch: 080 ----
mean loss: 140.28
 ---- batch: 090 ----
mean loss: 136.59
train mean loss: 135.62
epoch train time: 0:00:00.505530
elapsed time: 0:02:14.036398
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 23:04:12.688416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.05
 ---- batch: 020 ----
mean loss: 131.51
 ---- batch: 030 ----
mean loss: 129.47
 ---- batch: 040 ----
mean loss: 135.81
 ---- batch: 050 ----
mean loss: 130.89
 ---- batch: 060 ----
mean loss: 141.16
 ---- batch: 070 ----
mean loss: 134.29
 ---- batch: 080 ----
mean loss: 139.68
 ---- batch: 090 ----
mean loss: 134.95
train mean loss: 134.77
epoch train time: 0:00:00.499838
elapsed time: 0:02:14.536402
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 23:04:13.188408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.22
 ---- batch: 020 ----
mean loss: 136.68
 ---- batch: 030 ----
mean loss: 129.01
 ---- batch: 040 ----
mean loss: 135.10
 ---- batch: 050 ----
mean loss: 131.70
 ---- batch: 060 ----
mean loss: 133.36
 ---- batch: 070 ----
mean loss: 135.73
 ---- batch: 080 ----
mean loss: 139.16
 ---- batch: 090 ----
mean loss: 137.14
train mean loss: 134.07
epoch train time: 0:00:00.506194
elapsed time: 0:02:15.042790
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 23:04:13.694815
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.61
 ---- batch: 020 ----
mean loss: 132.51
 ---- batch: 030 ----
mean loss: 124.40
 ---- batch: 040 ----
mean loss: 128.50
 ---- batch: 050 ----
mean loss: 122.62
 ---- batch: 060 ----
mean loss: 124.58
 ---- batch: 070 ----
mean loss: 128.59
 ---- batch: 080 ----
mean loss: 130.82
 ---- batch: 090 ----
mean loss: 125.89
train mean loss: 128.54
epoch train time: 0:00:00.516214
elapsed time: 0:02:15.559223
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 23:04:14.211238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.12
 ---- batch: 020 ----
mean loss: 125.06
 ---- batch: 030 ----
mean loss: 128.43
 ---- batch: 040 ----
mean loss: 121.03
 ---- batch: 050 ----
mean loss: 127.86
 ---- batch: 060 ----
mean loss: 130.37
 ---- batch: 070 ----
mean loss: 124.14
 ---- batch: 080 ----
mean loss: 129.23
 ---- batch: 090 ----
mean loss: 121.84
train mean loss: 126.55
epoch train time: 0:00:00.515419
elapsed time: 0:02:16.074801
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 23:04:14.726812
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.57
 ---- batch: 020 ----
mean loss: 123.68
 ---- batch: 030 ----
mean loss: 123.93
 ---- batch: 040 ----
mean loss: 129.00
 ---- batch: 050 ----
mean loss: 131.89
 ---- batch: 060 ----
mean loss: 123.96
 ---- batch: 070 ----
mean loss: 125.55
 ---- batch: 080 ----
mean loss: 125.60
 ---- batch: 090 ----
mean loss: 123.75
train mean loss: 125.94
epoch train time: 0:00:00.528638
elapsed time: 0:02:16.603595
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 23:04:15.255604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.54
 ---- batch: 020 ----
mean loss: 122.34
 ---- batch: 030 ----
mean loss: 131.26
 ---- batch: 040 ----
mean loss: 118.85
 ---- batch: 050 ----
mean loss: 127.58
 ---- batch: 060 ----
mean loss: 124.99
 ---- batch: 070 ----
mean loss: 124.58
 ---- batch: 080 ----
mean loss: 133.65
 ---- batch: 090 ----
mean loss: 124.95
train mean loss: 125.76
epoch train time: 0:00:00.533876
elapsed time: 0:02:17.137619
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 23:04:15.789652
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.38
 ---- batch: 020 ----
mean loss: 127.85
 ---- batch: 030 ----
mean loss: 130.96
 ---- batch: 040 ----
mean loss: 116.99
 ---- batch: 050 ----
mean loss: 128.17
 ---- batch: 060 ----
mean loss: 126.78
 ---- batch: 070 ----
mean loss: 121.88
 ---- batch: 080 ----
mean loss: 132.62
 ---- batch: 090 ----
mean loss: 123.67
train mean loss: 126.03
epoch train time: 0:00:00.513603
elapsed time: 0:02:17.651399
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 23:04:16.303414
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.12
 ---- batch: 020 ----
mean loss: 120.53
 ---- batch: 030 ----
mean loss: 126.34
 ---- batch: 040 ----
mean loss: 128.49
 ---- batch: 050 ----
mean loss: 125.54
 ---- batch: 060 ----
mean loss: 124.65
 ---- batch: 070 ----
mean loss: 127.90
 ---- batch: 080 ----
mean loss: 126.10
 ---- batch: 090 ----
mean loss: 128.34
train mean loss: 125.74
epoch train time: 0:00:00.516610
elapsed time: 0:02:18.168161
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 23:04:16.820188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.01
 ---- batch: 020 ----
mean loss: 120.79
 ---- batch: 030 ----
mean loss: 127.83
 ---- batch: 040 ----
mean loss: 126.92
 ---- batch: 050 ----
mean loss: 127.29
 ---- batch: 060 ----
mean loss: 119.88
 ---- batch: 070 ----
mean loss: 123.68
 ---- batch: 080 ----
mean loss: 126.11
 ---- batch: 090 ----
mean loss: 126.74
train mean loss: 125.77
epoch train time: 0:00:00.523829
elapsed time: 0:02:18.692154
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 23:04:17.344164
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.52
 ---- batch: 020 ----
mean loss: 127.50
 ---- batch: 030 ----
mean loss: 126.90
 ---- batch: 040 ----
mean loss: 126.90
 ---- batch: 050 ----
mean loss: 126.84
 ---- batch: 060 ----
mean loss: 120.49
 ---- batch: 070 ----
mean loss: 125.42
 ---- batch: 080 ----
mean loss: 128.34
 ---- batch: 090 ----
mean loss: 122.72
train mean loss: 125.34
epoch train time: 0:00:00.522402
elapsed time: 0:02:19.214703
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 23:04:17.866728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.68
 ---- batch: 020 ----
mean loss: 126.99
 ---- batch: 030 ----
mean loss: 124.22
 ---- batch: 040 ----
mean loss: 121.64
 ---- batch: 050 ----
mean loss: 128.16
 ---- batch: 060 ----
mean loss: 129.41
 ---- batch: 070 ----
mean loss: 124.21
 ---- batch: 080 ----
mean loss: 124.60
 ---- batch: 090 ----
mean loss: 124.18
train mean loss: 125.35
epoch train time: 0:00:00.513792
elapsed time: 0:02:19.728673
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 23:04:18.380680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.14
 ---- batch: 020 ----
mean loss: 124.11
 ---- batch: 030 ----
mean loss: 125.24
 ---- batch: 040 ----
mean loss: 126.19
 ---- batch: 050 ----
mean loss: 121.65
 ---- batch: 060 ----
mean loss: 129.02
 ---- batch: 070 ----
mean loss: 125.90
 ---- batch: 080 ----
mean loss: 131.88
 ---- batch: 090 ----
mean loss: 123.75
train mean loss: 125.39
epoch train time: 0:00:00.517883
elapsed time: 0:02:20.246701
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 23:04:18.898709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.95
 ---- batch: 020 ----
mean loss: 119.00
 ---- batch: 030 ----
mean loss: 127.99
 ---- batch: 040 ----
mean loss: 125.51
 ---- batch: 050 ----
mean loss: 122.07
 ---- batch: 060 ----
mean loss: 125.98
 ---- batch: 070 ----
mean loss: 129.26
 ---- batch: 080 ----
mean loss: 123.53
 ---- batch: 090 ----
mean loss: 127.74
train mean loss: 125.46
epoch train time: 0:00:00.500522
elapsed time: 0:02:20.747385
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 23:04:19.399396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.86
 ---- batch: 020 ----
mean loss: 123.34
 ---- batch: 030 ----
mean loss: 123.24
 ---- batch: 040 ----
mean loss: 132.13
 ---- batch: 050 ----
mean loss: 124.59
 ---- batch: 060 ----
mean loss: 118.05
 ---- batch: 070 ----
mean loss: 125.07
 ---- batch: 080 ----
mean loss: 129.20
 ---- batch: 090 ----
mean loss: 123.76
train mean loss: 125.50
epoch train time: 0:00:00.499379
elapsed time: 0:02:21.246931
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 23:04:19.898953
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.35
 ---- batch: 020 ----
mean loss: 121.74
 ---- batch: 030 ----
mean loss: 120.91
 ---- batch: 040 ----
mean loss: 126.18
 ---- batch: 050 ----
mean loss: 129.57
 ---- batch: 060 ----
mean loss: 127.30
 ---- batch: 070 ----
mean loss: 125.94
 ---- batch: 080 ----
mean loss: 125.82
 ---- batch: 090 ----
mean loss: 126.48
train mean loss: 125.40
epoch train time: 0:00:00.497394
elapsed time: 0:02:21.744491
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 23:04:20.396517
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.88
 ---- batch: 020 ----
mean loss: 116.89
 ---- batch: 030 ----
mean loss: 124.71
 ---- batch: 040 ----
mean loss: 123.66
 ---- batch: 050 ----
mean loss: 126.20
 ---- batch: 060 ----
mean loss: 126.64
 ---- batch: 070 ----
mean loss: 125.10
 ---- batch: 080 ----
mean loss: 132.10
 ---- batch: 090 ----
mean loss: 128.82
train mean loss: 125.36
epoch train time: 0:00:00.510835
elapsed time: 0:02:22.255488
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 23:04:20.907496
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.89
 ---- batch: 020 ----
mean loss: 126.72
 ---- batch: 030 ----
mean loss: 121.29
 ---- batch: 040 ----
mean loss: 120.68
 ---- batch: 050 ----
mean loss: 127.35
 ---- batch: 060 ----
mean loss: 124.39
 ---- batch: 070 ----
mean loss: 126.36
 ---- batch: 080 ----
mean loss: 132.31
 ---- batch: 090 ----
mean loss: 125.17
train mean loss: 125.32
epoch train time: 0:00:00.503442
elapsed time: 0:02:22.759073
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 23:04:21.411096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.84
 ---- batch: 020 ----
mean loss: 124.41
 ---- batch: 030 ----
mean loss: 123.73
 ---- batch: 040 ----
mean loss: 125.40
 ---- batch: 050 ----
mean loss: 126.37
 ---- batch: 060 ----
mean loss: 124.46
 ---- batch: 070 ----
mean loss: 126.93
 ---- batch: 080 ----
mean loss: 122.84
 ---- batch: 090 ----
mean loss: 127.09
train mean loss: 125.09
epoch train time: 0:00:00.500610
elapsed time: 0:02:23.259886
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 23:04:21.911911
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.97
 ---- batch: 020 ----
mean loss: 126.83
 ---- batch: 030 ----
mean loss: 126.59
 ---- batch: 040 ----
mean loss: 121.10
 ---- batch: 050 ----
mean loss: 123.52
 ---- batch: 060 ----
mean loss: 121.41
 ---- batch: 070 ----
mean loss: 126.57
 ---- batch: 080 ----
mean loss: 130.92
 ---- batch: 090 ----
mean loss: 125.96
train mean loss: 125.19
epoch train time: 0:00:00.498684
elapsed time: 0:02:23.758765
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 23:04:22.410767
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.30
 ---- batch: 020 ----
mean loss: 124.52
 ---- batch: 030 ----
mean loss: 127.44
 ---- batch: 040 ----
mean loss: 124.96
 ---- batch: 050 ----
mean loss: 124.84
 ---- batch: 060 ----
mean loss: 123.79
 ---- batch: 070 ----
mean loss: 122.23
 ---- batch: 080 ----
mean loss: 128.07
 ---- batch: 090 ----
mean loss: 132.91
train mean loss: 125.01
epoch train time: 0:00:00.501913
elapsed time: 0:02:24.260878
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 23:04:22.912886
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.72
 ---- batch: 020 ----
mean loss: 125.66
 ---- batch: 030 ----
mean loss: 128.15
 ---- batch: 040 ----
mean loss: 126.91
 ---- batch: 050 ----
mean loss: 126.49
 ---- batch: 060 ----
mean loss: 120.01
 ---- batch: 070 ----
mean loss: 120.24
 ---- batch: 080 ----
mean loss: 130.19
 ---- batch: 090 ----
mean loss: 124.95
train mean loss: 125.05
epoch train time: 0:00:00.503998
elapsed time: 0:02:24.765030
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 23:04:23.417040
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.68
 ---- batch: 020 ----
mean loss: 122.72
 ---- batch: 030 ----
mean loss: 122.18
 ---- batch: 040 ----
mean loss: 124.31
 ---- batch: 050 ----
mean loss: 124.59
 ---- batch: 060 ----
mean loss: 126.18
 ---- batch: 070 ----
mean loss: 124.49
 ---- batch: 080 ----
mean loss: 127.73
 ---- batch: 090 ----
mean loss: 126.73
train mean loss: 124.90
epoch train time: 0:00:00.514148
elapsed time: 0:02:25.279336
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 23:04:23.931354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.74
 ---- batch: 020 ----
mean loss: 123.41
 ---- batch: 030 ----
mean loss: 123.43
 ---- batch: 040 ----
mean loss: 123.05
 ---- batch: 050 ----
mean loss: 127.65
 ---- batch: 060 ----
mean loss: 131.40
 ---- batch: 070 ----
mean loss: 122.24
 ---- batch: 080 ----
mean loss: 129.02
 ---- batch: 090 ----
mean loss: 119.47
train mean loss: 124.82
epoch train time: 0:00:00.501226
elapsed time: 0:02:25.780719
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 23:04:24.432728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.25
 ---- batch: 020 ----
mean loss: 125.24
 ---- batch: 030 ----
mean loss: 124.57
 ---- batch: 040 ----
mean loss: 123.63
 ---- batch: 050 ----
mean loss: 123.18
 ---- batch: 060 ----
mean loss: 126.68
 ---- batch: 070 ----
mean loss: 124.15
 ---- batch: 080 ----
mean loss: 131.35
 ---- batch: 090 ----
mean loss: 125.47
train mean loss: 125.13
epoch train time: 0:00:00.501949
elapsed time: 0:02:26.282819
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 23:04:24.934828
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.74
 ---- batch: 020 ----
mean loss: 124.45
 ---- batch: 030 ----
mean loss: 123.03
 ---- batch: 040 ----
mean loss: 123.92
 ---- batch: 050 ----
mean loss: 124.56
 ---- batch: 060 ----
mean loss: 123.42
 ---- batch: 070 ----
mean loss: 127.31
 ---- batch: 080 ----
mean loss: 124.71
 ---- batch: 090 ----
mean loss: 129.61
train mean loss: 124.84
epoch train time: 0:00:00.504965
elapsed time: 0:02:26.787927
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 23:04:25.439958
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.43
 ---- batch: 020 ----
mean loss: 115.62
 ---- batch: 030 ----
mean loss: 126.60
 ---- batch: 040 ----
mean loss: 123.76
 ---- batch: 050 ----
mean loss: 126.29
 ---- batch: 060 ----
mean loss: 123.60
 ---- batch: 070 ----
mean loss: 125.87
 ---- batch: 080 ----
mean loss: 133.72
 ---- batch: 090 ----
mean loss: 126.19
train mean loss: 124.92
epoch train time: 0:00:00.506240
elapsed time: 0:02:27.294334
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 23:04:25.946340
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.58
 ---- batch: 020 ----
mean loss: 134.48
 ---- batch: 030 ----
mean loss: 123.46
 ---- batch: 040 ----
mean loss: 123.97
 ---- batch: 050 ----
mean loss: 123.19
 ---- batch: 060 ----
mean loss: 125.15
 ---- batch: 070 ----
mean loss: 119.15
 ---- batch: 080 ----
mean loss: 119.62
 ---- batch: 090 ----
mean loss: 127.15
train mean loss: 124.85
epoch train time: 0:00:00.498559
elapsed time: 0:02:27.793043
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 23:04:26.445054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.95
 ---- batch: 020 ----
mean loss: 130.85
 ---- batch: 030 ----
mean loss: 124.33
 ---- batch: 040 ----
mean loss: 124.53
 ---- batch: 050 ----
mean loss: 123.11
 ---- batch: 060 ----
mean loss: 125.09
 ---- batch: 070 ----
mean loss: 118.41
 ---- batch: 080 ----
mean loss: 124.07
 ---- batch: 090 ----
mean loss: 129.20
train mean loss: 124.80
epoch train time: 0:00:00.509934
elapsed time: 0:02:28.303123
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 23:04:26.955131
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.77
 ---- batch: 020 ----
mean loss: 131.99
 ---- batch: 030 ----
mean loss: 124.51
 ---- batch: 040 ----
mean loss: 128.30
 ---- batch: 050 ----
mean loss: 128.41
 ---- batch: 060 ----
mean loss: 122.50
 ---- batch: 070 ----
mean loss: 123.13
 ---- batch: 080 ----
mean loss: 127.24
 ---- batch: 090 ----
mean loss: 117.26
train mean loss: 124.84
epoch train time: 0:00:00.503694
elapsed time: 0:02:28.806964
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 23:04:27.458973
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.83
 ---- batch: 020 ----
mean loss: 120.62
 ---- batch: 030 ----
mean loss: 127.67
 ---- batch: 040 ----
mean loss: 125.70
 ---- batch: 050 ----
mean loss: 126.34
 ---- batch: 060 ----
mean loss: 126.93
 ---- batch: 070 ----
mean loss: 124.16
 ---- batch: 080 ----
mean loss: 123.00
 ---- batch: 090 ----
mean loss: 127.33
train mean loss: 124.63
epoch train time: 0:00:00.505200
elapsed time: 0:02:29.312307
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 23:04:27.964315
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.01
 ---- batch: 020 ----
mean loss: 127.97
 ---- batch: 030 ----
mean loss: 120.66
 ---- batch: 040 ----
mean loss: 129.47
 ---- batch: 050 ----
mean loss: 124.20
 ---- batch: 060 ----
mean loss: 127.29
 ---- batch: 070 ----
mean loss: 122.37
 ---- batch: 080 ----
mean loss: 119.49
 ---- batch: 090 ----
mean loss: 130.30
train mean loss: 124.67
epoch train time: 0:00:00.502512
elapsed time: 0:02:29.814999
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 23:04:28.467007
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.81
 ---- batch: 020 ----
mean loss: 122.56
 ---- batch: 030 ----
mean loss: 128.50
 ---- batch: 040 ----
mean loss: 120.29
 ---- batch: 050 ----
mean loss: 123.44
 ---- batch: 060 ----
mean loss: 124.12
 ---- batch: 070 ----
mean loss: 126.98
 ---- batch: 080 ----
mean loss: 130.06
 ---- batch: 090 ----
mean loss: 124.07
train mean loss: 124.71
epoch train time: 0:00:00.505520
elapsed time: 0:02:30.320664
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 23:04:28.972671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.10
 ---- batch: 020 ----
mean loss: 124.73
 ---- batch: 030 ----
mean loss: 124.36
 ---- batch: 040 ----
mean loss: 123.39
 ---- batch: 050 ----
mean loss: 121.55
 ---- batch: 060 ----
mean loss: 122.65
 ---- batch: 070 ----
mean loss: 124.57
 ---- batch: 080 ----
mean loss: 127.44
 ---- batch: 090 ----
mean loss: 124.11
train mean loss: 124.58
epoch train time: 0:00:00.493165
elapsed time: 0:02:30.813982
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 23:04:29.466008
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.76
 ---- batch: 020 ----
mean loss: 123.33
 ---- batch: 030 ----
mean loss: 121.39
 ---- batch: 040 ----
mean loss: 122.72
 ---- batch: 050 ----
mean loss: 125.95
 ---- batch: 060 ----
mean loss: 126.99
 ---- batch: 070 ----
mean loss: 128.68
 ---- batch: 080 ----
mean loss: 122.72
 ---- batch: 090 ----
mean loss: 123.49
train mean loss: 124.46
epoch train time: 0:00:00.500102
elapsed time: 0:02:31.314250
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 23:04:29.966259
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.39
 ---- batch: 020 ----
mean loss: 125.27
 ---- batch: 030 ----
mean loss: 119.71
 ---- batch: 040 ----
mean loss: 123.25
 ---- batch: 050 ----
mean loss: 125.01
 ---- batch: 060 ----
mean loss: 124.52
 ---- batch: 070 ----
mean loss: 120.86
 ---- batch: 080 ----
mean loss: 131.86
 ---- batch: 090 ----
mean loss: 124.48
train mean loss: 124.88
epoch train time: 0:00:00.496327
elapsed time: 0:02:31.810782
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 23:04:30.462778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.87
 ---- batch: 020 ----
mean loss: 128.58
 ---- batch: 030 ----
mean loss: 123.13
 ---- batch: 040 ----
mean loss: 124.58
 ---- batch: 050 ----
mean loss: 125.71
 ---- batch: 060 ----
mean loss: 124.34
 ---- batch: 070 ----
mean loss: 124.70
 ---- batch: 080 ----
mean loss: 122.35
 ---- batch: 090 ----
mean loss: 126.06
train mean loss: 124.63
epoch train time: 0:00:00.505258
elapsed time: 0:02:32.316174
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 23:04:30.968180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.41
 ---- batch: 020 ----
mean loss: 125.54
 ---- batch: 030 ----
mean loss: 123.78
 ---- batch: 040 ----
mean loss: 120.40
 ---- batch: 050 ----
mean loss: 128.79
 ---- batch: 060 ----
mean loss: 126.06
 ---- batch: 070 ----
mean loss: 126.76
 ---- batch: 080 ----
mean loss: 126.17
 ---- batch: 090 ----
mean loss: 122.21
train mean loss: 124.53
epoch train time: 0:00:00.495291
elapsed time: 0:02:32.811610
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 23:04:31.463616
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.15
 ---- batch: 020 ----
mean loss: 124.72
 ---- batch: 030 ----
mean loss: 126.76
 ---- batch: 040 ----
mean loss: 126.53
 ---- batch: 050 ----
mean loss: 123.22
 ---- batch: 060 ----
mean loss: 120.07
 ---- batch: 070 ----
mean loss: 124.34
 ---- batch: 080 ----
mean loss: 124.83
 ---- batch: 090 ----
mean loss: 123.45
train mean loss: 124.44
epoch train time: 0:00:00.507216
elapsed time: 0:02:33.318968
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 23:04:31.970975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.56
 ---- batch: 020 ----
mean loss: 124.33
 ---- batch: 030 ----
mean loss: 134.75
 ---- batch: 040 ----
mean loss: 119.16
 ---- batch: 050 ----
mean loss: 124.84
 ---- batch: 060 ----
mean loss: 120.99
 ---- batch: 070 ----
mean loss: 125.35
 ---- batch: 080 ----
mean loss: 129.65
 ---- batch: 090 ----
mean loss: 122.57
train mean loss: 124.60
epoch train time: 0:00:00.505702
elapsed time: 0:02:33.824814
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 23:04:32.476822
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.36
 ---- batch: 020 ----
mean loss: 127.81
 ---- batch: 030 ----
mean loss: 125.06
 ---- batch: 040 ----
mean loss: 118.63
 ---- batch: 050 ----
mean loss: 128.10
 ---- batch: 060 ----
mean loss: 124.14
 ---- batch: 070 ----
mean loss: 126.48
 ---- batch: 080 ----
mean loss: 121.61
 ---- batch: 090 ----
mean loss: 123.42
train mean loss: 124.36
epoch train time: 0:00:00.511276
elapsed time: 0:02:34.336233
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 23:04:32.988258
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.82
 ---- batch: 020 ----
mean loss: 124.74
 ---- batch: 030 ----
mean loss: 123.88
 ---- batch: 040 ----
mean loss: 119.02
 ---- batch: 050 ----
mean loss: 132.48
 ---- batch: 060 ----
mean loss: 125.12
 ---- batch: 070 ----
mean loss: 117.31
 ---- batch: 080 ----
mean loss: 122.04
 ---- batch: 090 ----
mean loss: 129.61
train mean loss: 124.49
epoch train time: 0:00:00.500913
elapsed time: 0:02:34.837327
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 23:04:33.489353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.35
 ---- batch: 020 ----
mean loss: 126.86
 ---- batch: 030 ----
mean loss: 118.82
 ---- batch: 040 ----
mean loss: 116.41
 ---- batch: 050 ----
mean loss: 126.92
 ---- batch: 060 ----
mean loss: 124.96
 ---- batch: 070 ----
mean loss: 125.09
 ---- batch: 080 ----
mean loss: 126.79
 ---- batch: 090 ----
mean loss: 119.76
train mean loss: 124.39
epoch train time: 0:00:00.502964
elapsed time: 0:02:35.340463
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 23:04:33.992475
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.36
 ---- batch: 020 ----
mean loss: 118.58
 ---- batch: 030 ----
mean loss: 124.79
 ---- batch: 040 ----
mean loss: 121.06
 ---- batch: 050 ----
mean loss: 125.09
 ---- batch: 060 ----
mean loss: 121.96
 ---- batch: 070 ----
mean loss: 125.10
 ---- batch: 080 ----
mean loss: 126.10
 ---- batch: 090 ----
mean loss: 126.61
train mean loss: 124.37
epoch train time: 0:00:00.498941
elapsed time: 0:02:35.839569
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 23:04:34.491576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.94
 ---- batch: 020 ----
mean loss: 124.25
 ---- batch: 030 ----
mean loss: 124.52
 ---- batch: 040 ----
mean loss: 123.40
 ---- batch: 050 ----
mean loss: 114.10
 ---- batch: 060 ----
mean loss: 125.43
 ---- batch: 070 ----
mean loss: 123.29
 ---- batch: 080 ----
mean loss: 124.98
 ---- batch: 090 ----
mean loss: 127.55
train mean loss: 124.35
epoch train time: 0:00:00.505449
elapsed time: 0:02:36.345172
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 23:04:34.997182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.59
 ---- batch: 020 ----
mean loss: 125.25
 ---- batch: 030 ----
mean loss: 124.00
 ---- batch: 040 ----
mean loss: 122.82
 ---- batch: 050 ----
mean loss: 120.58
 ---- batch: 060 ----
mean loss: 124.30
 ---- batch: 070 ----
mean loss: 128.49
 ---- batch: 080 ----
mean loss: 118.90
 ---- batch: 090 ----
mean loss: 129.51
train mean loss: 124.34
epoch train time: 0:00:00.493732
elapsed time: 0:02:36.839068
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 23:04:35.491069
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.55
 ---- batch: 020 ----
mean loss: 126.78
 ---- batch: 030 ----
mean loss: 120.75
 ---- batch: 040 ----
mean loss: 123.63
 ---- batch: 050 ----
mean loss: 123.84
 ---- batch: 060 ----
mean loss: 124.50
 ---- batch: 070 ----
mean loss: 129.34
 ---- batch: 080 ----
mean loss: 121.38
 ---- batch: 090 ----
mean loss: 126.21
train mean loss: 124.24
epoch train time: 0:00:00.498706
elapsed time: 0:02:37.337923
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 23:04:35.989948
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.37
 ---- batch: 020 ----
mean loss: 125.12
 ---- batch: 030 ----
mean loss: 119.77
 ---- batch: 040 ----
mean loss: 122.94
 ---- batch: 050 ----
mean loss: 130.94
 ---- batch: 060 ----
mean loss: 122.12
 ---- batch: 070 ----
mean loss: 122.94
 ---- batch: 080 ----
mean loss: 130.34
 ---- batch: 090 ----
mean loss: 125.76
train mean loss: 124.09
epoch train time: 0:00:00.500213
elapsed time: 0:02:37.838315
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 23:04:36.490322
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.37
 ---- batch: 020 ----
mean loss: 122.08
 ---- batch: 030 ----
mean loss: 119.96
 ---- batch: 040 ----
mean loss: 125.13
 ---- batch: 050 ----
mean loss: 123.56
 ---- batch: 060 ----
mean loss: 124.56
 ---- batch: 070 ----
mean loss: 125.88
 ---- batch: 080 ----
mean loss: 120.33
 ---- batch: 090 ----
mean loss: 127.35
train mean loss: 124.17
epoch train time: 0:00:00.520858
elapsed time: 0:02:38.359335
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 23:04:37.011362
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.20
 ---- batch: 020 ----
mean loss: 123.70
 ---- batch: 030 ----
mean loss: 120.72
 ---- batch: 040 ----
mean loss: 126.21
 ---- batch: 050 ----
mean loss: 121.39
 ---- batch: 060 ----
mean loss: 127.26
 ---- batch: 070 ----
mean loss: 121.65
 ---- batch: 080 ----
mean loss: 124.47
 ---- batch: 090 ----
mean loss: 127.32
train mean loss: 124.19
epoch train time: 0:00:00.504438
elapsed time: 0:02:38.863946
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 23:04:37.515956
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.26
 ---- batch: 020 ----
mean loss: 119.92
 ---- batch: 030 ----
mean loss: 118.28
 ---- batch: 040 ----
mean loss: 123.20
 ---- batch: 050 ----
mean loss: 128.60
 ---- batch: 060 ----
mean loss: 128.92
 ---- batch: 070 ----
mean loss: 133.08
 ---- batch: 080 ----
mean loss: 122.12
 ---- batch: 090 ----
mean loss: 121.94
train mean loss: 124.15
epoch train time: 0:00:00.506393
elapsed time: 0:02:39.370486
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 23:04:38.022501
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.47
 ---- batch: 020 ----
mean loss: 126.40
 ---- batch: 030 ----
mean loss: 122.52
 ---- batch: 040 ----
mean loss: 117.17
 ---- batch: 050 ----
mean loss: 125.30
 ---- batch: 060 ----
mean loss: 122.09
 ---- batch: 070 ----
mean loss: 128.42
 ---- batch: 080 ----
mean loss: 125.69
 ---- batch: 090 ----
mean loss: 129.75
train mean loss: 124.09
epoch train time: 0:00:00.494099
elapsed time: 0:02:39.868089
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_0/checkpoint.pth.tar
**** end time: 2019-09-25 23:04:38.520064 ****
