Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 27673
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-20 10:44:18.826476 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 31, 14]             200
           Sigmoid-2           [-1, 10, 31, 14]               0
    BayesianConv2d-3           [-1, 10, 30, 14]           2,000
           Sigmoid-4           [-1, 10, 30, 14]               0
    BayesianConv2d-5           [-1, 10, 31, 14]           2,000
           Sigmoid-6           [-1, 10, 31, 14]               0
    BayesianConv2d-7           [-1, 10, 30, 14]           2,000
           Sigmoid-8           [-1, 10, 30, 14]               0
    BayesianConv2d-9            [-1, 1, 30, 14]              60
         Softplus-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
   BayesianLinear-12                  [-1, 100]          84,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 90,460
Trainable params: 90,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 10:44:18.844369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1870.72
 ---- batch: 020 ----
mean loss: 1401.91
 ---- batch: 030 ----
mean loss: 1244.55
 ---- batch: 040 ----
mean loss: 1200.66
train mean loss: 1407.94
epoch train time: 0:00:20.492548
elapsed time: 0:00:20.517858
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 10:44:39.344372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1079.60
 ---- batch: 020 ----
mean loss: 1080.83
 ---- batch: 030 ----
mean loss: 1085.05
 ---- batch: 040 ----
mean loss: 1015.85
train mean loss: 1064.70
epoch train time: 0:00:08.177137
elapsed time: 0:00:28.695342
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 10:44:47.521996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1019.65
 ---- batch: 020 ----
mean loss: 1003.99
 ---- batch: 030 ----
mean loss: 977.59
 ---- batch: 040 ----
mean loss: 1002.43
train mean loss: 1001.17
epoch train time: 0:00:08.200000
elapsed time: 0:00:36.895940
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 10:44:55.722657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 989.03
 ---- batch: 020 ----
mean loss: 936.16
 ---- batch: 030 ----
mean loss: 965.83
 ---- batch: 040 ----
mean loss: 925.12
train mean loss: 953.17
epoch train time: 0:00:08.104098
elapsed time: 0:00:45.000564
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 10:45:03.827209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.97
 ---- batch: 020 ----
mean loss: 859.18
 ---- batch: 030 ----
mean loss: 854.81
 ---- batch: 040 ----
mean loss: 806.79
train mean loss: 851.78
epoch train time: 0:00:08.121320
elapsed time: 0:00:53.122362
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 10:45:11.949012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 729.63
 ---- batch: 020 ----
mean loss: 653.83
 ---- batch: 030 ----
mean loss: 582.48
 ---- batch: 040 ----
mean loss: 518.37
train mean loss: 609.72
epoch train time: 0:00:07.951839
elapsed time: 0:01:01.074630
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 10:45:19.901274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.89
 ---- batch: 020 ----
mean loss: 460.12
 ---- batch: 030 ----
mean loss: 441.10
 ---- batch: 040 ----
mean loss: 428.64
train mean loss: 447.43
epoch train time: 0:00:07.944554
elapsed time: 0:01:09.019592
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 10:45:27.846215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.99
 ---- batch: 020 ----
mean loss: 417.83
 ---- batch: 030 ----
mean loss: 395.07
 ---- batch: 040 ----
mean loss: 376.58
train mean loss: 398.32
epoch train time: 0:00:07.891215
elapsed time: 0:01:16.911256
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 10:45:35.737918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.46
 ---- batch: 020 ----
mean loss: 369.09
 ---- batch: 030 ----
mean loss: 355.31
 ---- batch: 040 ----
mean loss: 365.76
train mean loss: 366.18
epoch train time: 0:00:07.865562
elapsed time: 0:01:24.777259
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 10:45:43.603908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.35
 ---- batch: 020 ----
mean loss: 350.69
 ---- batch: 030 ----
mean loss: 348.01
 ---- batch: 040 ----
mean loss: 342.38
train mean loss: 348.81
epoch train time: 0:00:07.831336
elapsed time: 0:01:32.609086
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 10:45:51.435727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.63
 ---- batch: 020 ----
mean loss: 339.20
 ---- batch: 030 ----
mean loss: 322.05
 ---- batch: 040 ----
mean loss: 324.77
train mean loss: 330.42
epoch train time: 0:00:07.806356
elapsed time: 0:01:40.415882
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 10:45:59.242552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.25
 ---- batch: 020 ----
mean loss: 326.11
 ---- batch: 030 ----
mean loss: 307.36
 ---- batch: 040 ----
mean loss: 319.50
train mean loss: 318.82
epoch train time: 0:00:07.854385
elapsed time: 0:01:48.270769
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 10:46:07.097405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.57
 ---- batch: 020 ----
mean loss: 305.81
 ---- batch: 030 ----
mean loss: 302.87
 ---- batch: 040 ----
mean loss: 298.20
train mean loss: 305.88
epoch train time: 0:00:07.795003
elapsed time: 0:01:56.066334
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 10:46:14.892974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.93
 ---- batch: 020 ----
mean loss: 297.29
 ---- batch: 030 ----
mean loss: 289.23
 ---- batch: 040 ----
mean loss: 297.72
train mean loss: 299.64
epoch train time: 0:00:07.795246
elapsed time: 0:02:03.862100
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 10:46:22.688761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.60
 ---- batch: 020 ----
mean loss: 296.64
 ---- batch: 030 ----
mean loss: 292.00
 ---- batch: 040 ----
mean loss: 281.92
train mean loss: 292.52
epoch train time: 0:00:07.885398
elapsed time: 0:02:11.748012
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 10:46:30.574672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.61
 ---- batch: 020 ----
mean loss: 281.78
 ---- batch: 030 ----
mean loss: 279.91
 ---- batch: 040 ----
mean loss: 284.94
train mean loss: 283.13
epoch train time: 0:00:07.952057
elapsed time: 0:02:19.700545
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 10:46:38.527203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.71
 ---- batch: 020 ----
mean loss: 273.25
 ---- batch: 030 ----
mean loss: 274.19
 ---- batch: 040 ----
mean loss: 271.66
train mean loss: 274.37
epoch train time: 0:00:07.912881
elapsed time: 0:02:27.613945
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 10:46:46.440633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.85
 ---- batch: 020 ----
mean loss: 269.77
 ---- batch: 030 ----
mean loss: 267.70
 ---- batch: 040 ----
mean loss: 266.26
train mean loss: 266.97
epoch train time: 0:00:07.848097
elapsed time: 0:02:35.462515
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 10:46:54.289178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.32
 ---- batch: 020 ----
mean loss: 272.78
 ---- batch: 030 ----
mean loss: 260.39
 ---- batch: 040 ----
mean loss: 257.90
train mean loss: 262.85
epoch train time: 0:00:07.876136
elapsed time: 0:02:43.339099
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 10:47:02.165780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.07
 ---- batch: 020 ----
mean loss: 258.19
 ---- batch: 030 ----
mean loss: 254.55
 ---- batch: 040 ----
mean loss: 254.70
train mean loss: 256.89
epoch train time: 0:00:07.861224
elapsed time: 0:02:51.200789
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 10:47:10.027444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.18
 ---- batch: 020 ----
mean loss: 252.83
 ---- batch: 030 ----
mean loss: 246.18
 ---- batch: 040 ----
mean loss: 248.35
train mean loss: 249.20
epoch train time: 0:00:07.913410
elapsed time: 0:02:59.114620
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 10:47:17.941246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.04
 ---- batch: 020 ----
mean loss: 248.01
 ---- batch: 030 ----
mean loss: 246.90
 ---- batch: 040 ----
mean loss: 236.66
train mean loss: 244.78
epoch train time: 0:00:08.032147
elapsed time: 0:03:07.147176
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 10:47:25.973773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.93
 ---- batch: 020 ----
mean loss: 238.04
 ---- batch: 030 ----
mean loss: 231.86
 ---- batch: 040 ----
mean loss: 235.73
train mean loss: 236.29
epoch train time: 0:00:08.038677
elapsed time: 0:03:15.186256
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 10:47:34.012896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.35
 ---- batch: 020 ----
mean loss: 233.33
 ---- batch: 030 ----
mean loss: 230.74
 ---- batch: 040 ----
mean loss: 232.07
train mean loss: 231.17
epoch train time: 0:00:08.017695
elapsed time: 0:03:23.204409
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 10:47:42.031050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.35
 ---- batch: 020 ----
mean loss: 219.20
 ---- batch: 030 ----
mean loss: 228.61
 ---- batch: 040 ----
mean loss: 224.11
train mean loss: 223.70
epoch train time: 0:00:08.035593
elapsed time: 0:03:31.240481
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 10:47:50.067130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.01
 ---- batch: 020 ----
mean loss: 215.70
 ---- batch: 030 ----
mean loss: 216.18
 ---- batch: 040 ----
mean loss: 210.29
train mean loss: 216.93
epoch train time: 0:00:08.071976
elapsed time: 0:03:39.312937
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 10:47:58.139588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.80
 ---- batch: 020 ----
mean loss: 213.23
 ---- batch: 030 ----
mean loss: 216.72
 ---- batch: 040 ----
mean loss: 206.98
train mean loss: 212.69
epoch train time: 0:00:08.060761
elapsed time: 0:03:47.374224
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 10:48:06.200914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.97
 ---- batch: 020 ----
mean loss: 215.13
 ---- batch: 030 ----
mean loss: 207.39
 ---- batch: 040 ----
mean loss: 201.76
train mean loss: 206.99
epoch train time: 0:00:08.042276
elapsed time: 0:03:55.417000
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 10:48:14.243650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.98
 ---- batch: 020 ----
mean loss: 199.47
 ---- batch: 030 ----
mean loss: 200.91
 ---- batch: 040 ----
mean loss: 208.58
train mean loss: 201.74
epoch train time: 0:00:08.013267
elapsed time: 0:04:03.430694
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 10:48:22.257327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.69
 ---- batch: 020 ----
mean loss: 204.27
 ---- batch: 030 ----
mean loss: 198.83
 ---- batch: 040 ----
mean loss: 192.66
train mean loss: 197.86
epoch train time: 0:00:08.042403
elapsed time: 0:04:11.473525
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 10:48:30.300190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.91
 ---- batch: 020 ----
mean loss: 192.99
 ---- batch: 030 ----
mean loss: 194.42
 ---- batch: 040 ----
mean loss: 196.06
train mean loss: 195.60
epoch train time: 0:00:08.059704
elapsed time: 0:04:19.533698
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 10:48:38.360331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.44
 ---- batch: 020 ----
mean loss: 189.67
 ---- batch: 030 ----
mean loss: 192.12
 ---- batch: 040 ----
mean loss: 190.57
train mean loss: 192.59
epoch train time: 0:00:08.016698
elapsed time: 0:04:27.550797
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 10:48:46.377394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.69
 ---- batch: 020 ----
mean loss: 184.57
 ---- batch: 030 ----
mean loss: 190.13
 ---- batch: 040 ----
mean loss: 191.97
train mean loss: 190.92
epoch train time: 0:00:08.022360
elapsed time: 0:04:35.573523
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 10:48:54.400184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.87
 ---- batch: 020 ----
mean loss: 188.35
 ---- batch: 030 ----
mean loss: 181.65
 ---- batch: 040 ----
mean loss: 181.81
train mean loss: 184.46
epoch train time: 0:00:08.071479
elapsed time: 0:04:43.645493
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 10:49:02.472157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.30
 ---- batch: 020 ----
mean loss: 185.03
 ---- batch: 030 ----
mean loss: 174.38
 ---- batch: 040 ----
mean loss: 180.53
train mean loss: 179.77
epoch train time: 0:00:08.069991
elapsed time: 0:04:51.716025
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 10:49:10.542612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.96
 ---- batch: 020 ----
mean loss: 179.12
 ---- batch: 030 ----
mean loss: 175.46
 ---- batch: 040 ----
mean loss: 176.63
train mean loss: 177.85
epoch train time: 0:00:08.046259
elapsed time: 0:04:59.762679
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 10:49:18.589321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.06
 ---- batch: 020 ----
mean loss: 173.75
 ---- batch: 030 ----
mean loss: 176.86
 ---- batch: 040 ----
mean loss: 179.82
train mean loss: 175.28
epoch train time: 0:00:08.021616
elapsed time: 0:05:07.784717
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 10:49:26.611342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.87
 ---- batch: 020 ----
mean loss: 168.88
 ---- batch: 030 ----
mean loss: 174.70
 ---- batch: 040 ----
mean loss: 174.47
train mean loss: 174.77
epoch train time: 0:00:08.014858
elapsed time: 0:05:15.799994
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 10:49:34.626654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.03
 ---- batch: 020 ----
mean loss: 163.72
 ---- batch: 030 ----
mean loss: 164.58
 ---- batch: 040 ----
mean loss: 168.04
train mean loss: 168.26
epoch train time: 0:00:08.009615
elapsed time: 0:05:23.810087
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 10:49:42.636757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.41
 ---- batch: 020 ----
mean loss: 172.30
 ---- batch: 030 ----
mean loss: 167.29
 ---- batch: 040 ----
mean loss: 165.44
train mean loss: 168.77
epoch train time: 0:00:08.037033
elapsed time: 0:05:31.847599
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 10:49:50.674223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.21
 ---- batch: 020 ----
mean loss: 165.90
 ---- batch: 030 ----
mean loss: 171.54
 ---- batch: 040 ----
mean loss: 163.22
train mean loss: 166.36
epoch train time: 0:00:08.001207
elapsed time: 0:05:39.849304
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 10:49:58.675975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.75
 ---- batch: 020 ----
mean loss: 164.46
 ---- batch: 030 ----
mean loss: 171.29
 ---- batch: 040 ----
mean loss: 157.66
train mean loss: 163.59
epoch train time: 0:00:07.876514
elapsed time: 0:05:47.726312
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 10:50:06.552954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.43
 ---- batch: 020 ----
mean loss: 157.08
 ---- batch: 030 ----
mean loss: 160.88
 ---- batch: 040 ----
mean loss: 161.80
train mean loss: 160.47
epoch train time: 0:00:07.775884
elapsed time: 0:05:55.502604
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 10:50:14.329224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.76
 ---- batch: 020 ----
mean loss: 161.20
 ---- batch: 030 ----
mean loss: 155.69
 ---- batch: 040 ----
mean loss: 156.15
train mean loss: 160.22
epoch train time: 0:00:07.745128
elapsed time: 0:06:03.248213
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 10:50:22.074807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.63
 ---- batch: 020 ----
mean loss: 159.36
 ---- batch: 030 ----
mean loss: 156.91
 ---- batch: 040 ----
mean loss: 154.47
train mean loss: 156.30
epoch train time: 0:00:07.785801
elapsed time: 0:06:11.034459
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 10:50:29.861099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.98
 ---- batch: 020 ----
mean loss: 162.52
 ---- batch: 030 ----
mean loss: 155.58
 ---- batch: 040 ----
mean loss: 155.15
train mean loss: 157.34
epoch train time: 0:00:07.804742
elapsed time: 0:06:18.839686
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 10:50:37.666376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.41
 ---- batch: 020 ----
mean loss: 151.99
 ---- batch: 030 ----
mean loss: 161.45
 ---- batch: 040 ----
mean loss: 155.80
train mean loss: 154.42
epoch train time: 0:00:07.768212
elapsed time: 0:06:26.608442
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 10:50:45.435138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.92
 ---- batch: 020 ----
mean loss: 151.57
 ---- batch: 030 ----
mean loss: 161.96
 ---- batch: 040 ----
mean loss: 150.37
train mean loss: 153.40
epoch train time: 0:00:07.810649
elapsed time: 0:06:34.419622
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 10:50:53.246265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.83
 ---- batch: 020 ----
mean loss: 153.71
 ---- batch: 030 ----
mean loss: 150.11
 ---- batch: 040 ----
mean loss: 160.40
train mean loss: 154.09
epoch train time: 0:00:07.856381
elapsed time: 0:06:42.276553
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 10:51:01.103208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.93
 ---- batch: 020 ----
mean loss: 148.90
 ---- batch: 030 ----
mean loss: 145.61
 ---- batch: 040 ----
mean loss: 149.07
train mean loss: 150.00
epoch train time: 0:00:07.876932
elapsed time: 0:06:50.153945
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 10:51:08.980570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.36
 ---- batch: 020 ----
mean loss: 151.50
 ---- batch: 030 ----
mean loss: 148.09
 ---- batch: 040 ----
mean loss: 148.04
train mean loss: 148.20
epoch train time: 0:00:07.899289
elapsed time: 0:06:58.053784
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 10:51:16.880461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.86
 ---- batch: 020 ----
mean loss: 147.48
 ---- batch: 030 ----
mean loss: 151.33
 ---- batch: 040 ----
mean loss: 142.63
train mean loss: 146.12
epoch train time: 0:00:08.021005
elapsed time: 0:07:06.075286
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 10:51:24.901927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.73
 ---- batch: 020 ----
mean loss: 144.50
 ---- batch: 030 ----
mean loss: 145.56
 ---- batch: 040 ----
mean loss: 138.38
train mean loss: 145.36
epoch train time: 0:00:08.012648
elapsed time: 0:07:14.088557
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 10:51:32.915204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.76
 ---- batch: 020 ----
mean loss: 139.51
 ---- batch: 030 ----
mean loss: 147.35
 ---- batch: 040 ----
mean loss: 146.77
train mean loss: 144.20
epoch train time: 0:00:08.038125
elapsed time: 0:07:22.127139
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 10:51:40.953775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.21
 ---- batch: 020 ----
mean loss: 145.59
 ---- batch: 030 ----
mean loss: 147.91
 ---- batch: 040 ----
mean loss: 144.35
train mean loss: 145.27
epoch train time: 0:00:08.049000
elapsed time: 0:07:30.176583
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 10:51:49.003228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.26
 ---- batch: 020 ----
mean loss: 144.03
 ---- batch: 030 ----
mean loss: 143.02
 ---- batch: 040 ----
mean loss: 138.27
train mean loss: 142.63
epoch train time: 0:00:08.067976
elapsed time: 0:07:38.245015
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 10:51:57.071642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.62
 ---- batch: 020 ----
mean loss: 142.48
 ---- batch: 030 ----
mean loss: 140.03
 ---- batch: 040 ----
mean loss: 144.25
train mean loss: 142.85
epoch train time: 0:00:08.111246
elapsed time: 0:07:46.356722
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 10:52:05.183365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.35
 ---- batch: 020 ----
mean loss: 140.92
 ---- batch: 030 ----
mean loss: 137.53
 ---- batch: 040 ----
mean loss: 143.94
train mean loss: 140.42
epoch train time: 0:00:08.049847
elapsed time: 0:07:54.407065
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 10:52:13.233712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.71
 ---- batch: 020 ----
mean loss: 135.96
 ---- batch: 030 ----
mean loss: 140.76
 ---- batch: 040 ----
mean loss: 144.32
train mean loss: 139.74
epoch train time: 0:00:07.995030
elapsed time: 0:08:02.402528
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 10:52:21.229164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.02
 ---- batch: 020 ----
mean loss: 144.69
 ---- batch: 030 ----
mean loss: 132.33
 ---- batch: 040 ----
mean loss: 134.97
train mean loss: 137.70
epoch train time: 0:00:07.938598
elapsed time: 0:08:10.341611
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 10:52:29.168269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.21
 ---- batch: 020 ----
mean loss: 131.14
 ---- batch: 030 ----
mean loss: 138.81
 ---- batch: 040 ----
mean loss: 141.30
train mean loss: 137.03
epoch train time: 0:00:07.899359
elapsed time: 0:08:18.241525
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 10:52:37.068160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.63
 ---- batch: 020 ----
mean loss: 138.89
 ---- batch: 030 ----
mean loss: 132.59
 ---- batch: 040 ----
mean loss: 139.27
train mean loss: 136.56
epoch train time: 0:00:07.949293
elapsed time: 0:08:26.191311
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 10:52:45.017976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.39
 ---- batch: 020 ----
mean loss: 135.54
 ---- batch: 030 ----
mean loss: 134.14
 ---- batch: 040 ----
mean loss: 139.39
train mean loss: 135.12
epoch train time: 0:00:07.954931
elapsed time: 0:08:34.146693
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 10:52:52.973346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.49
 ---- batch: 020 ----
mean loss: 134.43
 ---- batch: 030 ----
mean loss: 133.49
 ---- batch: 040 ----
mean loss: 133.54
train mean loss: 133.81
epoch train time: 0:00:07.841857
elapsed time: 0:08:41.989086
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 10:53:00.815751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.66
 ---- batch: 020 ----
mean loss: 131.40
 ---- batch: 030 ----
mean loss: 134.50
 ---- batch: 040 ----
mean loss: 129.86
train mean loss: 132.52
epoch train time: 0:00:07.736306
elapsed time: 0:08:49.725820
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 10:53:08.552469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.10
 ---- batch: 020 ----
mean loss: 133.34
 ---- batch: 030 ----
mean loss: 130.88
 ---- batch: 040 ----
mean loss: 137.18
train mean loss: 132.48
epoch train time: 0:00:07.638296
elapsed time: 0:08:57.364548
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 10:53:16.191184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.58
 ---- batch: 020 ----
mean loss: 125.09
 ---- batch: 030 ----
mean loss: 129.92
 ---- batch: 040 ----
mean loss: 137.31
train mean loss: 130.70
epoch train time: 0:00:07.761027
elapsed time: 0:09:05.126046
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 10:53:23.952711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.36
 ---- batch: 020 ----
mean loss: 128.65
 ---- batch: 030 ----
mean loss: 131.94
 ---- batch: 040 ----
mean loss: 126.51
train mean loss: 129.84
epoch train time: 0:00:07.738572
elapsed time: 0:09:12.865157
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 10:53:31.691848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.32
 ---- batch: 020 ----
mean loss: 126.68
 ---- batch: 030 ----
mean loss: 128.21
 ---- batch: 040 ----
mean loss: 130.44
train mean loss: 129.24
epoch train time: 0:00:07.744940
elapsed time: 0:09:20.610601
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 10:53:39.437281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.88
 ---- batch: 020 ----
mean loss: 127.69
 ---- batch: 030 ----
mean loss: 127.15
 ---- batch: 040 ----
mean loss: 129.90
train mean loss: 128.81
epoch train time: 0:00:07.777911
elapsed time: 0:09:28.389062
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 10:53:47.215732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.59
 ---- batch: 020 ----
mean loss: 126.82
 ---- batch: 030 ----
mean loss: 130.57
 ---- batch: 040 ----
mean loss: 125.37
train mean loss: 127.46
epoch train time: 0:00:07.771857
elapsed time: 0:09:36.161440
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 10:53:54.988095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.78
 ---- batch: 020 ----
mean loss: 127.46
 ---- batch: 030 ----
mean loss: 128.50
 ---- batch: 040 ----
mean loss: 124.58
train mean loss: 126.45
epoch train time: 0:00:07.714865
elapsed time: 0:09:43.876764
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 10:54:02.703411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.68
 ---- batch: 020 ----
mean loss: 133.06
 ---- batch: 030 ----
mean loss: 123.67
 ---- batch: 040 ----
mean loss: 124.76
train mean loss: 125.26
epoch train time: 0:00:07.856924
elapsed time: 0:09:51.734137
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 10:54:10.560783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.42
 ---- batch: 020 ----
mean loss: 121.63
 ---- batch: 030 ----
mean loss: 124.93
 ---- batch: 040 ----
mean loss: 125.20
train mean loss: 124.41
epoch train time: 0:00:07.776666
elapsed time: 0:09:59.511258
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 10:54:18.337905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.30
 ---- batch: 020 ----
mean loss: 123.07
 ---- batch: 030 ----
mean loss: 123.80
 ---- batch: 040 ----
mean loss: 124.50
train mean loss: 124.66
epoch train time: 0:00:07.761467
elapsed time: 0:10:07.273167
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 10:54:26.099838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.52
 ---- batch: 020 ----
mean loss: 122.76
 ---- batch: 030 ----
mean loss: 121.60
 ---- batch: 040 ----
mean loss: 121.12
train mean loss: 121.39
epoch train time: 0:00:07.790289
elapsed time: 0:10:15.063916
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 10:54:33.890560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.72
 ---- batch: 020 ----
mean loss: 118.57
 ---- batch: 030 ----
mean loss: 122.68
 ---- batch: 040 ----
mean loss: 122.17
train mean loss: 120.81
epoch train time: 0:00:07.817128
elapsed time: 0:10:22.881475
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 10:54:41.708116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.08
 ---- batch: 020 ----
mean loss: 124.97
 ---- batch: 030 ----
mean loss: 122.53
 ---- batch: 040 ----
mean loss: 119.65
train mean loss: 121.72
epoch train time: 0:00:07.581568
elapsed time: 0:10:30.463465
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 10:54:49.290095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.19
 ---- batch: 020 ----
mean loss: 119.14
 ---- batch: 030 ----
mean loss: 120.45
 ---- batch: 040 ----
mean loss: 122.02
train mean loss: 121.32
epoch train time: 0:00:07.811640
elapsed time: 0:10:38.275531
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 10:54:57.102200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.34
 ---- batch: 020 ----
mean loss: 119.73
 ---- batch: 030 ----
mean loss: 121.50
 ---- batch: 040 ----
mean loss: 122.01
train mean loss: 120.52
epoch train time: 0:00:07.918882
elapsed time: 0:10:46.194886
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 10:55:05.021570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.60
 ---- batch: 020 ----
mean loss: 119.60
 ---- batch: 030 ----
mean loss: 118.89
 ---- batch: 040 ----
mean loss: 120.41
train mean loss: 120.24
epoch train time: 0:00:07.806436
elapsed time: 0:10:54.001829
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 10:55:12.828496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.17
 ---- batch: 020 ----
mean loss: 119.70
 ---- batch: 030 ----
mean loss: 118.68
 ---- batch: 040 ----
mean loss: 118.19
train mean loss: 117.54
epoch train time: 0:00:07.775479
elapsed time: 0:11:01.777823
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 10:55:20.604489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.43
 ---- batch: 020 ----
mean loss: 117.60
 ---- batch: 030 ----
mean loss: 115.29
 ---- batch: 040 ----
mean loss: 116.74
train mean loss: 116.40
epoch train time: 0:00:07.705614
elapsed time: 0:11:09.483962
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 10:55:28.310625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.72
 ---- batch: 020 ----
mean loss: 115.86
 ---- batch: 030 ----
mean loss: 115.46
 ---- batch: 040 ----
mean loss: 117.91
train mean loss: 115.70
epoch train time: 0:00:07.648137
elapsed time: 0:11:17.132606
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 10:55:35.959291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.82
 ---- batch: 020 ----
mean loss: 115.93
 ---- batch: 030 ----
mean loss: 114.63
 ---- batch: 040 ----
mean loss: 115.08
train mean loss: 115.50
epoch train time: 0:00:07.841293
elapsed time: 0:11:24.974406
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 10:55:43.800995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.32
 ---- batch: 020 ----
mean loss: 114.49
 ---- batch: 030 ----
mean loss: 114.19
 ---- batch: 040 ----
mean loss: 118.14
train mean loss: 114.79
epoch train time: 0:00:08.033207
elapsed time: 0:11:33.008022
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 10:55:51.834708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.90
 ---- batch: 020 ----
mean loss: 113.21
 ---- batch: 030 ----
mean loss: 115.06
 ---- batch: 040 ----
mean loss: 115.06
train mean loss: 113.96
epoch train time: 0:00:08.038759
elapsed time: 0:11:41.047271
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 10:55:59.873918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.98
 ---- batch: 020 ----
mean loss: 112.11
 ---- batch: 030 ----
mean loss: 116.20
 ---- batch: 040 ----
mean loss: 109.53
train mean loss: 113.42
epoch train time: 0:00:08.043240
elapsed time: 0:11:49.091003
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 10:56:07.917690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.04
 ---- batch: 020 ----
mean loss: 115.04
 ---- batch: 030 ----
mean loss: 109.61
 ---- batch: 040 ----
mean loss: 107.51
train mean loss: 112.32
epoch train time: 0:00:08.042541
elapsed time: 0:11:57.134064
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 10:56:15.960711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.35
 ---- batch: 020 ----
mean loss: 110.99
 ---- batch: 030 ----
mean loss: 113.06
 ---- batch: 040 ----
mean loss: 114.51
train mean loss: 112.70
epoch train time: 0:00:08.025859
elapsed time: 0:12:05.160370
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 10:56:23.987012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.03
 ---- batch: 020 ----
mean loss: 116.86
 ---- batch: 030 ----
mean loss: 106.68
 ---- batch: 040 ----
mean loss: 110.00
train mean loss: 110.49
epoch train time: 0:00:07.989549
elapsed time: 0:12:13.150397
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 10:56:31.977036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.26
 ---- batch: 020 ----
mean loss: 111.96
 ---- batch: 030 ----
mean loss: 105.12
 ---- batch: 040 ----
mean loss: 109.75
train mean loss: 109.69
epoch train time: 0:00:08.006373
elapsed time: 0:12:21.157259
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 10:56:39.983938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.54
 ---- batch: 020 ----
mean loss: 107.99
 ---- batch: 030 ----
mean loss: 110.01
 ---- batch: 040 ----
mean loss: 109.62
train mean loss: 109.62
epoch train time: 0:00:08.104213
elapsed time: 0:12:29.262056
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 10:56:48.088774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.63
 ---- batch: 020 ----
mean loss: 106.60
 ---- batch: 030 ----
mean loss: 109.13
 ---- batch: 040 ----
mean loss: 107.03
train mean loss: 108.69
epoch train time: 0:00:08.023934
elapsed time: 0:12:37.286493
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 10:56:56.113119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.84
 ---- batch: 020 ----
mean loss: 109.20
 ---- batch: 030 ----
mean loss: 103.71
 ---- batch: 040 ----
mean loss: 109.96
train mean loss: 107.21
epoch train time: 0:00:07.822475
elapsed time: 0:12:45.109379
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 10:57:03.936052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.66
 ---- batch: 020 ----
mean loss: 109.57
 ---- batch: 030 ----
mean loss: 104.17
 ---- batch: 040 ----
mean loss: 104.51
train mean loss: 106.51
epoch train time: 0:00:07.840694
elapsed time: 0:12:52.950595
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 10:57:11.777224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.95
 ---- batch: 020 ----
mean loss: 105.39
 ---- batch: 030 ----
mean loss: 101.20
 ---- batch: 040 ----
mean loss: 108.23
train mean loss: 105.99
epoch train time: 0:00:07.868615
elapsed time: 0:13:00.819663
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 10:57:19.646310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.61
 ---- batch: 020 ----
mean loss: 100.61
 ---- batch: 030 ----
mean loss: 106.92
 ---- batch: 040 ----
mean loss: 104.85
train mean loss: 105.13
epoch train time: 0:00:07.986470
elapsed time: 0:13:08.806553
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 10:57:27.633187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.00
 ---- batch: 020 ----
mean loss: 105.83
 ---- batch: 030 ----
mean loss: 105.08
 ---- batch: 040 ----
mean loss: 108.15
train mean loss: 104.63
epoch train time: 0:00:07.801538
elapsed time: 0:13:16.608518
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 10:57:35.435137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.37
 ---- batch: 020 ----
mean loss: 106.91
 ---- batch: 030 ----
mean loss: 103.98
 ---- batch: 040 ----
mean loss: 102.72
train mean loss: 104.24
epoch train time: 0:00:07.790351
elapsed time: 0:13:24.399288
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 10:57:43.225917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.01
 ---- batch: 020 ----
mean loss: 105.27
 ---- batch: 030 ----
mean loss: 103.54
 ---- batch: 040 ----
mean loss: 106.75
train mean loss: 103.98
epoch train time: 0:00:07.720231
elapsed time: 0:13:32.119923
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 10:57:50.946543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.58
 ---- batch: 020 ----
mean loss: 103.68
 ---- batch: 030 ----
mean loss: 103.00
 ---- batch: 040 ----
mean loss: 102.98
train mean loss: 103.85
epoch train time: 0:00:07.755436
elapsed time: 0:13:39.875834
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 10:57:58.702549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.65
 ---- batch: 020 ----
mean loss: 98.99
 ---- batch: 030 ----
mean loss: 103.84
 ---- batch: 040 ----
mean loss: 103.39
train mean loss: 101.65
epoch train time: 0:00:07.710841
elapsed time: 0:13:47.587252
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 10:58:06.413965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.05
 ---- batch: 020 ----
mean loss: 99.60
 ---- batch: 030 ----
mean loss: 102.71
 ---- batch: 040 ----
mean loss: 99.30
train mean loss: 100.70
epoch train time: 0:00:07.915859
elapsed time: 0:13:55.503691
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 10:58:14.330335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.57
 ---- batch: 020 ----
mean loss: 105.48
 ---- batch: 030 ----
mean loss: 96.49
 ---- batch: 040 ----
mean loss: 104.84
train mean loss: 101.26
epoch train time: 0:00:07.945693
elapsed time: 0:14:03.449841
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 10:58:22.276504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.27
 ---- batch: 020 ----
mean loss: 98.33
 ---- batch: 030 ----
mean loss: 98.94
 ---- batch: 040 ----
mean loss: 103.33
train mean loss: 100.99
epoch train time: 0:00:07.858615
elapsed time: 0:14:11.308895
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 10:58:30.135518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.34
 ---- batch: 020 ----
mean loss: 100.23
 ---- batch: 030 ----
mean loss: 99.68
 ---- batch: 040 ----
mean loss: 99.07
train mean loss: 99.83
epoch train time: 0:00:07.775209
elapsed time: 0:14:19.084661
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 10:58:37.911281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.48
 ---- batch: 020 ----
mean loss: 99.59
 ---- batch: 030 ----
mean loss: 98.44
 ---- batch: 040 ----
mean loss: 100.24
train mean loss: 99.86
epoch train time: 0:00:07.761936
elapsed time: 0:14:26.847054
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 10:58:45.673698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.99
 ---- batch: 020 ----
mean loss: 99.66
 ---- batch: 030 ----
mean loss: 97.28
 ---- batch: 040 ----
mean loss: 103.15
train mean loss: 98.58
epoch train time: 0:00:07.790321
elapsed time: 0:14:34.637786
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 10:58:53.464448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.59
 ---- batch: 020 ----
mean loss: 98.26
 ---- batch: 030 ----
mean loss: 102.13
 ---- batch: 040 ----
mean loss: 92.68
train mean loss: 99.51
epoch train time: 0:00:07.965771
elapsed time: 0:14:42.604008
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 10:59:01.430668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.54
 ---- batch: 020 ----
mean loss: 97.37
 ---- batch: 030 ----
mean loss: 101.89
 ---- batch: 040 ----
mean loss: 95.49
train mean loss: 97.96
epoch train time: 0:00:07.922944
elapsed time: 0:14:50.527393
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 10:59:09.354107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.46
 ---- batch: 020 ----
mean loss: 97.48
 ---- batch: 030 ----
mean loss: 97.48
 ---- batch: 040 ----
mean loss: 98.09
train mean loss: 97.34
epoch train time: 0:00:07.793539
elapsed time: 0:14:58.321422
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 10:59:17.148068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.13
 ---- batch: 020 ----
mean loss: 98.34
 ---- batch: 030 ----
mean loss: 97.90
 ---- batch: 040 ----
mean loss: 92.84
train mean loss: 97.47
epoch train time: 0:00:07.774143
elapsed time: 0:15:06.096039
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 10:59:24.922664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.11
 ---- batch: 020 ----
mean loss: 95.10
 ---- batch: 030 ----
mean loss: 99.03
 ---- batch: 040 ----
mean loss: 96.89
train mean loss: 97.18
epoch train time: 0:00:07.678028
elapsed time: 0:15:13.774511
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 10:59:32.601191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.88
 ---- batch: 020 ----
mean loss: 93.86
 ---- batch: 030 ----
mean loss: 98.57
 ---- batch: 040 ----
mean loss: 97.77
train mean loss: 97.12
epoch train time: 0:00:07.672162
elapsed time: 0:15:21.447110
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 10:59:40.273733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.62
 ---- batch: 020 ----
mean loss: 94.57
 ---- batch: 030 ----
mean loss: 97.91
 ---- batch: 040 ----
mean loss: 92.85
train mean loss: 95.60
epoch train time: 0:00:07.918091
elapsed time: 0:15:29.365840
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 10:59:48.192496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.24
 ---- batch: 020 ----
mean loss: 93.85
 ---- batch: 030 ----
mean loss: 95.34
 ---- batch: 040 ----
mean loss: 99.69
train mean loss: 96.42
epoch train time: 0:00:07.948117
elapsed time: 0:15:37.314452
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 10:59:56.141104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.56
 ---- batch: 020 ----
mean loss: 95.34
 ---- batch: 030 ----
mean loss: 92.39
 ---- batch: 040 ----
mean loss: 94.56
train mean loss: 94.80
epoch train time: 0:00:07.924746
elapsed time: 0:15:45.239685
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 11:00:04.066340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.55
 ---- batch: 020 ----
mean loss: 95.45
 ---- batch: 030 ----
mean loss: 95.50
 ---- batch: 040 ----
mean loss: 91.15
train mean loss: 93.39
epoch train time: 0:00:07.894341
elapsed time: 0:15:53.134462
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 11:00:11.961122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.74
 ---- batch: 020 ----
mean loss: 93.81
 ---- batch: 030 ----
mean loss: 96.74
 ---- batch: 040 ----
mean loss: 94.10
train mean loss: 94.36
epoch train time: 0:00:07.904885
elapsed time: 0:16:01.039854
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 11:00:19.866494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.13
 ---- batch: 020 ----
mean loss: 95.13
 ---- batch: 030 ----
mean loss: 96.72
 ---- batch: 040 ----
mean loss: 92.93
train mean loss: 93.87
epoch train time: 0:00:07.924711
elapsed time: 0:16:08.965019
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 11:00:27.791657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.91
 ---- batch: 020 ----
mean loss: 96.14
 ---- batch: 030 ----
mean loss: 93.20
 ---- batch: 040 ----
mean loss: 95.18
train mean loss: 93.86
epoch train time: 0:00:07.970419
elapsed time: 0:16:16.935857
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 11:00:35.762533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.49
 ---- batch: 020 ----
mean loss: 92.95
 ---- batch: 030 ----
mean loss: 93.23
 ---- batch: 040 ----
mean loss: 93.70
train mean loss: 93.16
epoch train time: 0:00:08.021161
elapsed time: 0:16:24.957532
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 11:00:43.784261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.08
 ---- batch: 020 ----
mean loss: 93.21
 ---- batch: 030 ----
mean loss: 92.24
 ---- batch: 040 ----
mean loss: 91.25
train mean loss: 91.88
epoch train time: 0:00:08.030761
elapsed time: 0:16:32.988935
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 11:00:51.815625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.87
 ---- batch: 020 ----
mean loss: 91.73
 ---- batch: 030 ----
mean loss: 93.05
 ---- batch: 040 ----
mean loss: 92.99
train mean loss: 92.95
epoch train time: 0:00:07.983139
elapsed time: 0:16:40.972579
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 11:00:59.799208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.17
 ---- batch: 020 ----
mean loss: 89.88
 ---- batch: 030 ----
mean loss: 98.27
 ---- batch: 040 ----
mean loss: 94.10
train mean loss: 92.46
epoch train time: 0:00:07.848916
elapsed time: 0:16:48.821963
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 11:01:07.648609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.96
 ---- batch: 020 ----
mean loss: 94.68
 ---- batch: 030 ----
mean loss: 92.78
 ---- batch: 040 ----
mean loss: 90.64
train mean loss: 92.12
epoch train time: 0:00:07.720362
elapsed time: 0:16:56.542860
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 11:01:15.369385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.89
 ---- batch: 020 ----
mean loss: 88.77
 ---- batch: 030 ----
mean loss: 90.21
 ---- batch: 040 ----
mean loss: 90.53
train mean loss: 91.57
epoch train time: 0:00:07.764588
elapsed time: 0:17:04.307864
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 11:01:23.134514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.33
 ---- batch: 020 ----
mean loss: 91.53
 ---- batch: 030 ----
mean loss: 93.20
 ---- batch: 040 ----
mean loss: 90.23
train mean loss: 91.12
epoch train time: 0:00:07.973202
elapsed time: 0:17:12.281569
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 11:01:31.108246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.57
 ---- batch: 020 ----
mean loss: 90.84
 ---- batch: 030 ----
mean loss: 91.03
 ---- batch: 040 ----
mean loss: 93.07
train mean loss: 91.56
epoch train time: 0:00:07.826656
elapsed time: 0:17:20.108765
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 11:01:38.935426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.77
 ---- batch: 020 ----
mean loss: 91.99
 ---- batch: 030 ----
mean loss: 89.26
 ---- batch: 040 ----
mean loss: 88.67
train mean loss: 90.51
epoch train time: 0:00:07.792476
elapsed time: 0:17:27.901741
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 11:01:46.728412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.09
 ---- batch: 020 ----
mean loss: 88.18
 ---- batch: 030 ----
mean loss: 87.57
 ---- batch: 040 ----
mean loss: 91.85
train mean loss: 90.35
epoch train time: 0:00:07.793055
elapsed time: 0:17:35.695336
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 11:01:54.522008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.01
 ---- batch: 020 ----
mean loss: 89.28
 ---- batch: 030 ----
mean loss: 88.15
 ---- batch: 040 ----
mean loss: 85.86
train mean loss: 89.10
epoch train time: 0:00:07.774280
elapsed time: 0:17:43.470205
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 11:02:02.296877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.83
 ---- batch: 020 ----
mean loss: 89.54
 ---- batch: 030 ----
mean loss: 92.37
 ---- batch: 040 ----
mean loss: 89.13
train mean loss: 89.02
epoch train time: 0:00:07.791435
elapsed time: 0:17:51.262180
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 11:02:10.088834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.73
 ---- batch: 020 ----
mean loss: 92.09
 ---- batch: 030 ----
mean loss: 88.83
 ---- batch: 040 ----
mean loss: 87.99
train mean loss: 88.90
epoch train time: 0:00:07.939079
elapsed time: 0:17:59.201781
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 11:02:18.028466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.63
 ---- batch: 020 ----
mean loss: 86.57
 ---- batch: 030 ----
mean loss: 86.06
 ---- batch: 040 ----
mean loss: 91.54
train mean loss: 88.23
epoch train time: 0:00:07.916379
elapsed time: 0:18:07.118612
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 11:02:25.945231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.63
 ---- batch: 020 ----
mean loss: 90.11
 ---- batch: 030 ----
mean loss: 84.79
 ---- batch: 040 ----
mean loss: 85.05
train mean loss: 87.31
epoch train time: 0:00:07.880074
elapsed time: 0:18:14.999184
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 11:02:33.825934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.37
 ---- batch: 020 ----
mean loss: 87.64
 ---- batch: 030 ----
mean loss: 87.72
 ---- batch: 040 ----
mean loss: 87.46
train mean loss: 87.93
epoch train time: 0:00:07.903829
elapsed time: 0:18:22.903631
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 11:02:41.730271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.47
 ---- batch: 020 ----
mean loss: 94.67
 ---- batch: 030 ----
mean loss: 87.77
 ---- batch: 040 ----
mean loss: 85.00
train mean loss: 88.29
epoch train time: 0:00:07.865031
elapsed time: 0:18:30.769075
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 11:02:49.595699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.06
 ---- batch: 020 ----
mean loss: 85.81
 ---- batch: 030 ----
mean loss: 87.58
 ---- batch: 040 ----
mean loss: 87.11
train mean loss: 86.94
epoch train time: 0:00:07.934697
elapsed time: 0:18:38.704172
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 11:02:57.530809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.67
 ---- batch: 020 ----
mean loss: 81.82
 ---- batch: 030 ----
mean loss: 86.95
 ---- batch: 040 ----
mean loss: 87.51
train mean loss: 85.87
epoch train time: 0:00:08.068441
elapsed time: 0:18:46.773082
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 11:03:05.599728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.50
 ---- batch: 020 ----
mean loss: 86.25
 ---- batch: 030 ----
mean loss: 85.75
 ---- batch: 040 ----
mean loss: 88.32
train mean loss: 85.96
epoch train time: 0:00:08.077284
elapsed time: 0:18:54.850895
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 11:03:13.677555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.28
 ---- batch: 020 ----
mean loss: 88.64
 ---- batch: 030 ----
mean loss: 87.81
 ---- batch: 040 ----
mean loss: 83.52
train mean loss: 85.99
epoch train time: 0:00:07.940550
elapsed time: 0:19:02.791894
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 11:03:21.618549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.94
 ---- batch: 020 ----
mean loss: 86.02
 ---- batch: 030 ----
mean loss: 86.47
 ---- batch: 040 ----
mean loss: 86.16
train mean loss: 86.76
epoch train time: 0:00:07.940475
elapsed time: 0:19:10.732851
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 11:03:29.559503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.68
 ---- batch: 020 ----
mean loss: 84.67
 ---- batch: 030 ----
mean loss: 87.40
 ---- batch: 040 ----
mean loss: 82.52
train mean loss: 85.22
epoch train time: 0:00:08.009445
elapsed time: 0:19:18.742816
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 11:03:37.569517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.67
 ---- batch: 020 ----
mean loss: 82.36
 ---- batch: 030 ----
mean loss: 85.07
 ---- batch: 040 ----
mean loss: 85.62
train mean loss: 84.89
epoch train time: 0:00:07.952692
elapsed time: 0:19:26.696068
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 11:03:45.522727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.39
 ---- batch: 020 ----
mean loss: 85.26
 ---- batch: 030 ----
mean loss: 83.32
 ---- batch: 040 ----
mean loss: 87.48
train mean loss: 84.11
epoch train time: 0:00:07.963512
elapsed time: 0:19:34.660030
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 11:03:53.486709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.89
 ---- batch: 020 ----
mean loss: 81.47
 ---- batch: 030 ----
mean loss: 86.26
 ---- batch: 040 ----
mean loss: 83.40
train mean loss: 84.19
epoch train time: 0:00:08.024211
elapsed time: 0:19:42.684745
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 11:04:01.511413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.78
 ---- batch: 020 ----
mean loss: 86.22
 ---- batch: 030 ----
mean loss: 83.01
 ---- batch: 040 ----
mean loss: 80.94
train mean loss: 82.75
epoch train time: 0:00:08.082488
elapsed time: 0:19:50.767779
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 11:04:09.594364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.91
 ---- batch: 020 ----
mean loss: 81.14
 ---- batch: 030 ----
mean loss: 83.70
 ---- batch: 040 ----
mean loss: 80.72
train mean loss: 83.05
epoch train time: 0:00:08.066488
elapsed time: 0:19:58.834638
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 11:04:17.661296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.61
 ---- batch: 020 ----
mean loss: 85.93
 ---- batch: 030 ----
mean loss: 84.74
 ---- batch: 040 ----
mean loss: 83.73
train mean loss: 83.86
epoch train time: 0:00:08.016811
elapsed time: 0:20:06.851885
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 11:04:25.678524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.11
 ---- batch: 020 ----
mean loss: 82.59
 ---- batch: 030 ----
mean loss: 84.15
 ---- batch: 040 ----
mean loss: 83.40
train mean loss: 83.30
epoch train time: 0:00:08.057520
elapsed time: 0:20:14.909859
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 11:04:33.736517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.67
 ---- batch: 020 ----
mean loss: 81.65
 ---- batch: 030 ----
mean loss: 83.07
 ---- batch: 040 ----
mean loss: 84.43
train mean loss: 83.07
epoch train time: 0:00:08.096743
elapsed time: 0:20:23.007078
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 11:04:41.833705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.26
 ---- batch: 020 ----
mean loss: 82.40
 ---- batch: 030 ----
mean loss: 78.40
 ---- batch: 040 ----
mean loss: 81.64
train mean loss: 81.46
epoch train time: 0:00:07.923247
elapsed time: 0:20:30.930816
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 11:04:49.757454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.39
 ---- batch: 020 ----
mean loss: 83.69
 ---- batch: 030 ----
mean loss: 79.97
 ---- batch: 040 ----
mean loss: 85.78
train mean loss: 82.30
epoch train time: 0:00:08.103984
elapsed time: 0:20:39.035269
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 11:04:57.861930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.90
 ---- batch: 020 ----
mean loss: 83.96
 ---- batch: 030 ----
mean loss: 86.17
 ---- batch: 040 ----
mean loss: 81.08
train mean loss: 82.46
epoch train time: 0:00:08.092367
elapsed time: 0:20:47.128106
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 11:05:05.954749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.34
 ---- batch: 020 ----
mean loss: 84.22
 ---- batch: 030 ----
mean loss: 79.24
 ---- batch: 040 ----
mean loss: 84.47
train mean loss: 81.56
epoch train time: 0:00:07.943727
elapsed time: 0:20:55.072238
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 11:05:13.898873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.83
 ---- batch: 020 ----
mean loss: 82.19
 ---- batch: 030 ----
mean loss: 82.59
 ---- batch: 040 ----
mean loss: 79.78
train mean loss: 81.40
epoch train time: 0:00:07.942343
elapsed time: 0:21:03.015007
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 11:05:21.841632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.90
 ---- batch: 020 ----
mean loss: 80.24
 ---- batch: 030 ----
mean loss: 81.81
 ---- batch: 040 ----
mean loss: 79.87
train mean loss: 81.02
epoch train time: 0:00:07.928993
elapsed time: 0:21:10.944613
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 11:05:29.771291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.92
 ---- batch: 020 ----
mean loss: 80.06
 ---- batch: 030 ----
mean loss: 78.27
 ---- batch: 040 ----
mean loss: 85.48
train mean loss: 81.02
epoch train time: 0:00:07.959263
elapsed time: 0:21:18.904315
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 11:05:37.730936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.81
 ---- batch: 020 ----
mean loss: 80.49
 ---- batch: 030 ----
mean loss: 79.97
 ---- batch: 040 ----
mean loss: 83.23
train mean loss: 80.33
epoch train time: 0:00:08.095768
elapsed time: 0:21:27.000516
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 11:05:45.827166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.25
 ---- batch: 020 ----
mean loss: 80.64
 ---- batch: 030 ----
mean loss: 79.36
 ---- batch: 040 ----
mean loss: 79.92
train mean loss: 80.02
epoch train time: 0:00:07.940255
elapsed time: 0:21:34.941232
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 11:05:53.767884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.25
 ---- batch: 020 ----
mean loss: 80.14
 ---- batch: 030 ----
mean loss: 77.68
 ---- batch: 040 ----
mean loss: 81.03
train mean loss: 79.32
epoch train time: 0:00:07.944960
elapsed time: 0:21:42.886708
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 11:06:01.713418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.86
 ---- batch: 020 ----
mean loss: 78.31
 ---- batch: 030 ----
mean loss: 81.66
 ---- batch: 040 ----
mean loss: 78.54
train mean loss: 79.22
epoch train time: 0:00:07.980426
elapsed time: 0:21:50.867737
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 11:06:09.694404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.40
 ---- batch: 020 ----
mean loss: 79.44
 ---- batch: 030 ----
mean loss: 80.61
 ---- batch: 040 ----
mean loss: 81.82
train mean loss: 79.05
epoch train time: 0:00:07.967393
elapsed time: 0:21:58.835621
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 11:06:17.662244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.97
 ---- batch: 020 ----
mean loss: 76.16
 ---- batch: 030 ----
mean loss: 77.96
 ---- batch: 040 ----
mean loss: 78.33
train mean loss: 78.98
epoch train time: 0:00:07.950752
elapsed time: 0:22:06.786792
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 11:06:25.613439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.64
 ---- batch: 020 ----
mean loss: 79.52
 ---- batch: 030 ----
mean loss: 77.94
 ---- batch: 040 ----
mean loss: 79.39
train mean loss: 78.74
epoch train time: 0:00:07.990656
elapsed time: 0:22:14.777853
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 11:06:33.604504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.45
 ---- batch: 020 ----
mean loss: 77.32
 ---- batch: 030 ----
mean loss: 78.74
 ---- batch: 040 ----
mean loss: 79.50
train mean loss: 78.23
epoch train time: 0:00:07.869584
elapsed time: 0:22:22.647860
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 11:06:41.474505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.08
 ---- batch: 020 ----
mean loss: 79.11
 ---- batch: 030 ----
mean loss: 85.18
 ---- batch: 040 ----
mean loss: 77.79
train mean loss: 80.28
epoch train time: 0:00:07.728704
elapsed time: 0:22:30.377026
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 11:06:49.203666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.07
 ---- batch: 020 ----
mean loss: 77.19
 ---- batch: 030 ----
mean loss: 83.60
 ---- batch: 040 ----
mean loss: 79.76
train mean loss: 79.23
epoch train time: 0:00:07.699770
elapsed time: 0:22:38.077288
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 11:06:56.903941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.59
 ---- batch: 020 ----
mean loss: 75.34
 ---- batch: 030 ----
mean loss: 77.06
 ---- batch: 040 ----
mean loss: 77.42
train mean loss: 77.19
epoch train time: 0:00:07.703936
elapsed time: 0:22:45.781639
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 11:07:04.608353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.56
 ---- batch: 020 ----
mean loss: 73.85
 ---- batch: 030 ----
mean loss: 83.08
 ---- batch: 040 ----
mean loss: 75.14
train mean loss: 77.27
epoch train time: 0:00:07.856710
elapsed time: 0:22:53.638868
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 11:07:12.465504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.19
 ---- batch: 020 ----
mean loss: 81.01
 ---- batch: 030 ----
mean loss: 78.09
 ---- batch: 040 ----
mean loss: 82.33
train mean loss: 79.86
epoch train time: 0:00:07.858944
elapsed time: 0:23:01.498268
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 11:07:20.324933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.20
 ---- batch: 020 ----
mean loss: 79.62
 ---- batch: 030 ----
mean loss: 79.61
 ---- batch: 040 ----
mean loss: 76.66
train mean loss: 77.69
epoch train time: 0:00:07.866043
elapsed time: 0:23:09.364846
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 11:07:28.191421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.28
 ---- batch: 020 ----
mean loss: 76.99
 ---- batch: 030 ----
mean loss: 74.87
 ---- batch: 040 ----
mean loss: 73.33
train mean loss: 75.77
epoch train time: 0:00:07.864101
elapsed time: 0:23:17.229342
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 11:07:36.055986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.28
 ---- batch: 020 ----
mean loss: 76.61
 ---- batch: 030 ----
mean loss: 76.31
 ---- batch: 040 ----
mean loss: 75.49
train mean loss: 75.96
epoch train time: 0:00:07.860216
elapsed time: 0:23:25.090001
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 11:07:43.916634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.37
 ---- batch: 020 ----
mean loss: 75.11
 ---- batch: 030 ----
mean loss: 75.16
 ---- batch: 040 ----
mean loss: 78.34
train mean loss: 75.30
epoch train time: 0:00:07.851142
elapsed time: 0:23:32.941625
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 11:07:51.768613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.55
 ---- batch: 020 ----
mean loss: 71.61
 ---- batch: 030 ----
mean loss: 74.23
 ---- batch: 040 ----
mean loss: 76.41
train mean loss: 75.06
epoch train time: 0:00:07.968998
elapsed time: 0:23:40.911446
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 11:07:59.738092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.61
 ---- batch: 020 ----
mean loss: 74.49
 ---- batch: 030 ----
mean loss: 73.58
 ---- batch: 040 ----
mean loss: 75.18
train mean loss: 75.79
epoch train time: 0:00:08.066685
elapsed time: 0:23:48.978665
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 11:08:07.805403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.73
 ---- batch: 020 ----
mean loss: 79.74
 ---- batch: 030 ----
mean loss: 76.56
 ---- batch: 040 ----
mean loss: 73.45
train mean loss: 77.02
epoch train time: 0:00:08.064254
elapsed time: 0:23:57.043444
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 11:08:15.870027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.02
 ---- batch: 020 ----
mean loss: 78.53
 ---- batch: 030 ----
mean loss: 73.04
 ---- batch: 040 ----
mean loss: 75.12
train mean loss: 75.22
epoch train time: 0:00:08.031532
elapsed time: 0:24:05.075370
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 11:08:23.902004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.56
 ---- batch: 020 ----
mean loss: 74.32
 ---- batch: 030 ----
mean loss: 72.50
 ---- batch: 040 ----
mean loss: 74.74
train mean loss: 73.79
epoch train time: 0:00:08.029767
elapsed time: 0:24:13.105562
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 11:08:31.932189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.00
 ---- batch: 020 ----
mean loss: 77.61
 ---- batch: 030 ----
mean loss: 71.71
 ---- batch: 040 ----
mean loss: 75.78
train mean loss: 74.02
epoch train time: 0:00:08.047788
elapsed time: 0:24:21.153752
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 11:08:39.980428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.86
 ---- batch: 020 ----
mean loss: 75.21
 ---- batch: 030 ----
mean loss: 75.37
 ---- batch: 040 ----
mean loss: 73.80
train mean loss: 74.37
epoch train time: 0:00:08.014078
elapsed time: 0:24:29.168339
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 11:08:47.995022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.32
 ---- batch: 020 ----
mean loss: 75.40
 ---- batch: 030 ----
mean loss: 68.63
 ---- batch: 040 ----
mean loss: 75.45
train mean loss: 73.15
epoch train time: 0:00:07.975584
elapsed time: 0:24:37.144374
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 11:08:55.971030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.88
 ---- batch: 020 ----
mean loss: 69.41
 ---- batch: 030 ----
mean loss: 75.50
 ---- batch: 040 ----
mean loss: 73.70
train mean loss: 73.63
epoch train time: 0:00:08.042074
elapsed time: 0:24:45.186902
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 11:09:04.013586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.71
 ---- batch: 020 ----
mean loss: 75.42
 ---- batch: 030 ----
mean loss: 72.97
 ---- batch: 040 ----
mean loss: 68.62
train mean loss: 72.53
epoch train time: 0:00:08.065647
elapsed time: 0:24:53.253059
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 11:09:12.079736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.78
 ---- batch: 020 ----
mean loss: 70.29
 ---- batch: 030 ----
mean loss: 73.59
 ---- batch: 040 ----
mean loss: 73.72
train mean loss: 72.36
epoch train time: 0:00:08.090060
elapsed time: 0:25:01.343658
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 11:09:20.170303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.30
 ---- batch: 020 ----
mean loss: 73.03
 ---- batch: 030 ----
mean loss: 74.33
 ---- batch: 040 ----
mean loss: 71.98
train mean loss: 72.12
epoch train time: 0:00:08.030827
elapsed time: 0:25:09.374921
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 11:09:28.201558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.52
 ---- batch: 020 ----
mean loss: 73.67
 ---- batch: 030 ----
mean loss: 72.67
 ---- batch: 040 ----
mean loss: 77.51
train mean loss: 73.57
epoch train time: 0:00:07.937655
elapsed time: 0:25:17.312983
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 11:09:36.139617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.48
 ---- batch: 020 ----
mean loss: 72.56
 ---- batch: 030 ----
mean loss: 71.39
 ---- batch: 040 ----
mean loss: 71.39
train mean loss: 72.08
epoch train time: 0:00:07.920235
elapsed time: 0:25:25.233752
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 11:09:44.060484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.06
 ---- batch: 020 ----
mean loss: 70.20
 ---- batch: 030 ----
mean loss: 74.09
 ---- batch: 040 ----
mean loss: 70.69
train mean loss: 71.23
epoch train time: 0:00:07.898632
elapsed time: 0:25:33.132940
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 11:09:51.959589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.91
 ---- batch: 020 ----
mean loss: 71.13
 ---- batch: 030 ----
mean loss: 74.08
 ---- batch: 040 ----
mean loss: 71.19
train mean loss: 71.71
epoch train time: 0:00:07.916348
elapsed time: 0:25:41.049726
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 11:09:59.876400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.89
 ---- batch: 020 ----
mean loss: 72.18
 ---- batch: 030 ----
mean loss: 73.40
 ---- batch: 040 ----
mean loss: 73.48
train mean loss: 71.86
epoch train time: 0:00:07.893358
elapsed time: 0:25:48.943552
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 11:10:07.770203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.44
 ---- batch: 020 ----
mean loss: 73.19
 ---- batch: 030 ----
mean loss: 70.29
 ---- batch: 040 ----
mean loss: 69.68
train mean loss: 71.07
epoch train time: 0:00:07.770342
elapsed time: 0:25:56.714295
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 11:10:15.540949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.70
 ---- batch: 020 ----
mean loss: 73.09
 ---- batch: 030 ----
mean loss: 75.66
 ---- batch: 040 ----
mean loss: 72.08
train mean loss: 72.64
epoch train time: 0:00:07.791744
elapsed time: 0:26:04.506463
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 11:10:23.333106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.31
 ---- batch: 020 ----
mean loss: 73.66
 ---- batch: 030 ----
mean loss: 68.79
 ---- batch: 040 ----
mean loss: 71.01
train mean loss: 71.06
epoch train time: 0:00:07.692032
elapsed time: 0:26:12.198913
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 11:10:31.025582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.17
 ---- batch: 020 ----
mean loss: 68.56
 ---- batch: 030 ----
mean loss: 70.39
 ---- batch: 040 ----
mean loss: 69.03
train mean loss: 70.03
epoch train time: 0:00:07.688769
elapsed time: 0:26:19.888145
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 11:10:38.714781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.11
 ---- batch: 020 ----
mean loss: 69.14
 ---- batch: 030 ----
mean loss: 71.11
 ---- batch: 040 ----
mean loss: 72.00
train mean loss: 70.55
epoch train time: 0:00:07.704357
elapsed time: 0:26:27.592950
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 11:10:46.419593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.52
 ---- batch: 020 ----
mean loss: 72.69
 ---- batch: 030 ----
mean loss: 72.45
 ---- batch: 040 ----
mean loss: 70.69
train mean loss: 70.88
epoch train time: 0:00:07.942383
elapsed time: 0:26:35.535797
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 11:10:54.362486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.09
 ---- batch: 020 ----
mean loss: 69.48
 ---- batch: 030 ----
mean loss: 69.65
 ---- batch: 040 ----
mean loss: 69.83
train mean loss: 69.56
epoch train time: 0:00:07.839328
elapsed time: 0:26:43.375689
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 11:11:02.202354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.48
 ---- batch: 020 ----
mean loss: 67.64
 ---- batch: 030 ----
mean loss: 69.19
 ---- batch: 040 ----
mean loss: 70.29
train mean loss: 68.36
epoch train time: 0:00:07.798428
elapsed time: 0:26:51.174729
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 11:11:10.001265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.29
 ---- batch: 020 ----
mean loss: 73.02
 ---- batch: 030 ----
mean loss: 67.69
 ---- batch: 040 ----
mean loss: 68.11
train mean loss: 69.10
epoch train time: 0:00:07.824843
elapsed time: 0:26:58.999967
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 11:11:17.826628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.62
 ---- batch: 020 ----
mean loss: 70.68
 ---- batch: 030 ----
mean loss: 66.71
 ---- batch: 040 ----
mean loss: 67.87
train mean loss: 68.22
epoch train time: 0:00:07.812697
elapsed time: 0:27:06.813173
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 11:11:25.639821
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.71
 ---- batch: 020 ----
mean loss: 67.73
 ---- batch: 030 ----
mean loss: 68.57
 ---- batch: 040 ----
mean loss: 71.80
train mean loss: 68.72
epoch train time: 0:00:07.835699
elapsed time: 0:27:14.649326
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 11:11:33.475951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.86
 ---- batch: 020 ----
mean loss: 68.99
 ---- batch: 030 ----
mean loss: 68.37
 ---- batch: 040 ----
mean loss: 68.45
train mean loss: 68.72
epoch train time: 0:00:07.972601
elapsed time: 0:27:22.622430
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 11:11:41.449064
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.24
 ---- batch: 020 ----
mean loss: 68.56
 ---- batch: 030 ----
mean loss: 68.40
 ---- batch: 040 ----
mean loss: 66.27
train mean loss: 68.18
epoch train time: 0:00:07.992470
elapsed time: 0:27:30.615434
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 11:11:49.442123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.89
 ---- batch: 020 ----
mean loss: 67.17
 ---- batch: 030 ----
mean loss: 67.97
 ---- batch: 040 ----
mean loss: 67.14
train mean loss: 68.70
epoch train time: 0:00:07.992916
elapsed time: 0:27:38.608862
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 11:11:57.435510
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.28
 ---- batch: 020 ----
mean loss: 70.92
 ---- batch: 030 ----
mean loss: 66.16
 ---- batch: 040 ----
mean loss: 70.06
train mean loss: 68.67
epoch train time: 0:00:07.972392
elapsed time: 0:27:46.581669
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 11:12:05.408307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.41
 ---- batch: 020 ----
mean loss: 67.92
 ---- batch: 030 ----
mean loss: 65.45
 ---- batch: 040 ----
mean loss: 71.13
train mean loss: 68.12
epoch train time: 0:00:07.847209
elapsed time: 0:27:54.429291
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 11:12:13.255926
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.85
 ---- batch: 020 ----
mean loss: 72.29
 ---- batch: 030 ----
mean loss: 67.72
 ---- batch: 040 ----
mean loss: 64.46
train mean loss: 68.18
epoch train time: 0:00:07.789993
elapsed time: 0:28:02.219719
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 11:12:21.046375
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.49
 ---- batch: 020 ----
mean loss: 67.77
 ---- batch: 030 ----
mean loss: 64.92
 ---- batch: 040 ----
mean loss: 68.87
train mean loss: 68.39
epoch train time: 0:00:07.922891
elapsed time: 0:28:10.143071
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 11:12:28.969707
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.65
 ---- batch: 020 ----
mean loss: 68.10
 ---- batch: 030 ----
mean loss: 65.56
 ---- batch: 040 ----
mean loss: 69.51
train mean loss: 67.79
epoch train time: 0:00:07.950044
elapsed time: 0:28:18.093554
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 11:12:36.920194
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.59
 ---- batch: 020 ----
mean loss: 64.28
 ---- batch: 030 ----
mean loss: 66.56
 ---- batch: 040 ----
mean loss: 72.85
train mean loss: 68.27
epoch train time: 0:00:07.835318
elapsed time: 0:28:25.929316
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 11:12:44.755952
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.30
 ---- batch: 020 ----
mean loss: 68.94
 ---- batch: 030 ----
mean loss: 68.15
 ---- batch: 040 ----
mean loss: 67.52
train mean loss: 68.53
epoch train time: 0:00:07.821592
elapsed time: 0:28:33.751326
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 11:12:52.577963
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.36
 ---- batch: 020 ----
mean loss: 68.37
 ---- batch: 030 ----
mean loss: 68.85
 ---- batch: 040 ----
mean loss: 68.05
train mean loss: 68.09
epoch train time: 0:00:07.840748
elapsed time: 0:28:41.592572
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 11:13:00.419252
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.21
 ---- batch: 020 ----
mean loss: 67.71
 ---- batch: 030 ----
mean loss: 68.21
 ---- batch: 040 ----
mean loss: 64.26
train mean loss: 67.88
epoch train time: 0:00:07.900588
elapsed time: 0:28:49.493737
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 11:13:08.320431
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.34
 ---- batch: 020 ----
mean loss: 67.08
 ---- batch: 030 ----
mean loss: 70.60
 ---- batch: 040 ----
mean loss: 69.47
train mean loss: 68.43
epoch train time: 0:00:08.050613
elapsed time: 0:28:57.544885
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 11:13:16.371548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.73
 ---- batch: 020 ----
mean loss: 70.07
 ---- batch: 030 ----
mean loss: 68.94
 ---- batch: 040 ----
mean loss: 68.04
train mean loss: 68.21
epoch train time: 0:00:07.987827
elapsed time: 0:29:05.533186
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 11:13:24.359911
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.44
 ---- batch: 020 ----
mean loss: 68.60
 ---- batch: 030 ----
mean loss: 67.80
 ---- batch: 040 ----
mean loss: 67.97
train mean loss: 68.34
epoch train time: 0:00:08.020462
elapsed time: 0:29:13.554170
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 11:13:32.380804
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.67
 ---- batch: 020 ----
mean loss: 70.44
 ---- batch: 030 ----
mean loss: 69.64
 ---- batch: 040 ----
mean loss: 68.34
train mean loss: 68.69
epoch train time: 0:00:07.756938
elapsed time: 0:29:21.311529
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 11:13:40.138161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.46
 ---- batch: 020 ----
mean loss: 67.61
 ---- batch: 030 ----
mean loss: 66.55
 ---- batch: 040 ----
mean loss: 70.60
train mean loss: 67.96
epoch train time: 0:00:07.751929
elapsed time: 0:29:29.063849
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 11:13:47.890469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.07
 ---- batch: 020 ----
mean loss: 69.86
 ---- batch: 030 ----
mean loss: 69.36
 ---- batch: 040 ----
mean loss: 66.72
train mean loss: 68.00
epoch train time: 0:00:07.850067
elapsed time: 0:29:36.914321
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 11:13:55.740958
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.58
 ---- batch: 020 ----
mean loss: 67.60
 ---- batch: 030 ----
mean loss: 69.06
 ---- batch: 040 ----
mean loss: 65.74
train mean loss: 67.75
epoch train time: 0:00:07.956412
elapsed time: 0:29:44.871148
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 11:14:03.697798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.25
 ---- batch: 020 ----
mean loss: 68.97
 ---- batch: 030 ----
mean loss: 65.93
 ---- batch: 040 ----
mean loss: 68.69
train mean loss: 68.02
epoch train time: 0:00:07.816360
elapsed time: 0:29:52.687969
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 11:14:11.514620
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.28
 ---- batch: 020 ----
mean loss: 66.04
 ---- batch: 030 ----
mean loss: 69.17
 ---- batch: 040 ----
mean loss: 69.83
train mean loss: 68.25
epoch train time: 0:00:07.801783
elapsed time: 0:30:00.490235
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 11:14:19.316874
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.97
 ---- batch: 020 ----
mean loss: 68.15
 ---- batch: 030 ----
mean loss: 66.68
 ---- batch: 040 ----
mean loss: 68.57
train mean loss: 67.82
epoch train time: 0:00:07.828801
elapsed time: 0:30:08.319556
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 11:14:27.146197
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.26
 ---- batch: 020 ----
mean loss: 67.28
 ---- batch: 030 ----
mean loss: 71.97
 ---- batch: 040 ----
mean loss: 65.44
train mean loss: 68.07
epoch train time: 0:00:07.854059
elapsed time: 0:30:16.174128
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 11:14:35.000808
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.31
 ---- batch: 020 ----
mean loss: 65.60
 ---- batch: 030 ----
mean loss: 71.28
 ---- batch: 040 ----
mean loss: 67.55
train mean loss: 68.22
epoch train time: 0:00:07.924890
elapsed time: 0:30:24.099523
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 11:14:42.926161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.66
 ---- batch: 020 ----
mean loss: 68.01
 ---- batch: 030 ----
mean loss: 66.85
 ---- batch: 040 ----
mean loss: 68.96
train mean loss: 67.52
epoch train time: 0:00:07.918864
elapsed time: 0:30:32.018857
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 11:14:50.845498
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.52
 ---- batch: 020 ----
mean loss: 69.37
 ---- batch: 030 ----
mean loss: 68.52
 ---- batch: 040 ----
mean loss: 64.10
train mean loss: 68.13
epoch train time: 0:00:07.948592
elapsed time: 0:30:39.967905
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 11:14:58.794542
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.79
 ---- batch: 020 ----
mean loss: 65.08
 ---- batch: 030 ----
mean loss: 67.36
 ---- batch: 040 ----
mean loss: 67.10
train mean loss: 67.62
epoch train time: 0:00:07.949764
elapsed time: 0:30:47.918118
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 11:15:06.744768
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.59
 ---- batch: 020 ----
mean loss: 67.83
 ---- batch: 030 ----
mean loss: 69.49
 ---- batch: 040 ----
mean loss: 67.69
train mean loss: 67.62
epoch train time: 0:00:07.980214
elapsed time: 0:30:55.898782
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 11:15:14.725427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.61
 ---- batch: 020 ----
mean loss: 68.17
 ---- batch: 030 ----
mean loss: 67.82
 ---- batch: 040 ----
mean loss: 67.26
train mean loss: 68.00
epoch train time: 0:00:08.110691
elapsed time: 0:31:04.010198
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 11:15:22.836725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.58
 ---- batch: 020 ----
mean loss: 68.49
 ---- batch: 030 ----
mean loss: 69.09
 ---- batch: 040 ----
mean loss: 68.38
train mean loss: 68.47
epoch train time: 0:00:08.050407
elapsed time: 0:31:12.060944
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 11:15:30.887573
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.02
 ---- batch: 020 ----
mean loss: 67.29
 ---- batch: 030 ----
mean loss: 66.04
 ---- batch: 040 ----
mean loss: 71.05
train mean loss: 68.05
epoch train time: 0:00:08.081302
elapsed time: 0:31:20.142656
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 11:15:38.969288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.54
 ---- batch: 020 ----
mean loss: 69.35
 ---- batch: 030 ----
mean loss: 68.49
 ---- batch: 040 ----
mean loss: 66.55
train mean loss: 67.80
epoch train time: 0:00:08.076617
elapsed time: 0:31:28.219779
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 11:15:47.046441
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.75
 ---- batch: 020 ----
mean loss: 64.87
 ---- batch: 030 ----
mean loss: 68.64
 ---- batch: 040 ----
mean loss: 67.54
train mean loss: 67.60
epoch train time: 0:00:08.076691
elapsed time: 0:31:36.297010
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 11:15:55.123660
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.82
 ---- batch: 020 ----
mean loss: 66.72
 ---- batch: 030 ----
mean loss: 68.38
 ---- batch: 040 ----
mean loss: 67.66
train mean loss: 67.61
epoch train time: 0:00:08.052303
elapsed time: 0:31:44.349765
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 11:16:03.176432
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.74
 ---- batch: 020 ----
mean loss: 67.06
 ---- batch: 030 ----
mean loss: 70.04
 ---- batch: 040 ----
mean loss: 68.17
train mean loss: 67.53
epoch train time: 0:00:07.861671
elapsed time: 0:31:52.211919
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 11:16:11.038577
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.16
 ---- batch: 020 ----
mean loss: 68.81
 ---- batch: 030 ----
mean loss: 64.19
 ---- batch: 040 ----
mean loss: 68.80
train mean loss: 67.95
epoch train time: 0:00:07.895366
elapsed time: 0:32:00.107776
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 11:16:18.934441
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.92
 ---- batch: 020 ----
mean loss: 67.42
 ---- batch: 030 ----
mean loss: 67.38
 ---- batch: 040 ----
mean loss: 67.00
train mean loss: 67.59
epoch train time: 0:00:07.900823
elapsed time: 0:32:08.009071
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 11:16:26.835744
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.70
 ---- batch: 020 ----
mean loss: 67.01
 ---- batch: 030 ----
mean loss: 67.55
 ---- batch: 040 ----
mean loss: 67.99
train mean loss: 67.42
epoch train time: 0:00:07.805080
elapsed time: 0:32:15.814682
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 11:16:34.641317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.88
 ---- batch: 020 ----
mean loss: 68.05
 ---- batch: 030 ----
mean loss: 67.19
 ---- batch: 040 ----
mean loss: 69.21
train mean loss: 67.73
epoch train time: 0:00:07.786726
elapsed time: 0:32:23.601850
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 11:16:42.428536
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.71
 ---- batch: 020 ----
mean loss: 69.56
 ---- batch: 030 ----
mean loss: 67.43
 ---- batch: 040 ----
mean loss: 66.30
train mean loss: 67.08
epoch train time: 0:00:07.766133
elapsed time: 0:32:31.368463
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 11:16:50.195097
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.31
 ---- batch: 020 ----
mean loss: 70.89
 ---- batch: 030 ----
mean loss: 64.91
 ---- batch: 040 ----
mean loss: 68.57
train mean loss: 67.31
epoch train time: 0:00:07.785841
elapsed time: 0:32:39.154704
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 11:16:57.981339
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.89
 ---- batch: 020 ----
mean loss: 66.58
 ---- batch: 030 ----
mean loss: 69.85
 ---- batch: 040 ----
mean loss: 67.55
train mean loss: 67.26
epoch train time: 0:00:07.656649
elapsed time: 0:32:46.811832
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 11:17:05.638519
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.81
 ---- batch: 020 ----
mean loss: 67.89
 ---- batch: 030 ----
mean loss: 65.72
 ---- batch: 040 ----
mean loss: 67.25
train mean loss: 67.59
epoch train time: 0:00:07.925821
elapsed time: 0:32:54.738187
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 11:17:13.564853
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.36
 ---- batch: 020 ----
mean loss: 65.75
 ---- batch: 030 ----
mean loss: 70.67
 ---- batch: 040 ----
mean loss: 64.55
train mean loss: 67.23
epoch train time: 0:00:08.002012
elapsed time: 0:33:02.740689
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 11:17:21.567358
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.28
 ---- batch: 020 ----
mean loss: 67.65
 ---- batch: 030 ----
mean loss: 66.15
 ---- batch: 040 ----
mean loss: 66.09
train mean loss: 67.41
epoch train time: 0:00:07.949847
elapsed time: 0:33:10.699553
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0/checkpoint.pth.tar
**** end time: 2019-09-20 11:17:29.526045 ****
