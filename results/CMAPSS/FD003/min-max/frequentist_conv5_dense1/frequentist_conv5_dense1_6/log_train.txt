Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_6', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 7270
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-20 22:53:13.055100 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 31, 14]             100
              Tanh-2           [-1, 10, 31, 14]               0
            Conv2d-3           [-1, 10, 30, 14]           1,000
              Tanh-4           [-1, 10, 30, 14]               0
            Conv2d-5           [-1, 10, 31, 14]           1,000
              Tanh-6           [-1, 10, 31, 14]               0
            Conv2d-7           [-1, 10, 30, 14]           1,000
              Tanh-8           [-1, 10, 30, 14]               0
            Conv2d-9            [-1, 1, 30, 14]              30
             Tanh-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
          Dropout-12                  [-1, 420]               0
           Linear-13                  [-1, 100]          42,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 45,230
Trainable params: 45,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 22:53:13.062852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3816.66
 ---- batch: 020 ----
mean loss: 1527.09
 ---- batch: 030 ----
mean loss: 468.85
 ---- batch: 040 ----
mean loss: 423.96
train mean loss: 1479.22
epoch train time: 0:00:15.539795
elapsed time: 0:00:15.550905
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 22:53:28.606040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.25
 ---- batch: 020 ----
mean loss: 348.74
 ---- batch: 030 ----
mean loss: 324.21
 ---- batch: 040 ----
mean loss: 316.20
train mean loss: 337.90
epoch train time: 0:00:01.840498
elapsed time: 0:00:17.391562
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 22:53:30.446719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.23
 ---- batch: 020 ----
mean loss: 305.31
 ---- batch: 030 ----
mean loss: 296.11
 ---- batch: 040 ----
mean loss: 294.34
train mean loss: 299.40
epoch train time: 0:00:01.689486
elapsed time: 0:00:19.081214
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 22:53:32.136411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.78
 ---- batch: 020 ----
mean loss: 266.41
 ---- batch: 030 ----
mean loss: 256.33
 ---- batch: 040 ----
mean loss: 245.66
train mean loss: 259.41
epoch train time: 0:00:01.772656
elapsed time: 0:00:20.854084
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 22:53:33.909234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.40
 ---- batch: 020 ----
mean loss: 235.15
 ---- batch: 030 ----
mean loss: 226.20
 ---- batch: 040 ----
mean loss: 217.75
train mean loss: 229.82
epoch train time: 0:00:01.669286
elapsed time: 0:00:22.523535
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 22:53:35.578688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.44
 ---- batch: 020 ----
mean loss: 212.37
 ---- batch: 030 ----
mean loss: 223.63
 ---- batch: 040 ----
mean loss: 216.68
train mean loss: 217.80
epoch train time: 0:00:01.767820
elapsed time: 0:00:24.291492
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 22:53:37.346638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.49
 ---- batch: 020 ----
mean loss: 213.55
 ---- batch: 030 ----
mean loss: 211.76
 ---- batch: 040 ----
mean loss: 207.19
train mean loss: 210.09
epoch train time: 0:00:01.669139
elapsed time: 0:00:25.960782
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 22:53:39.015936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.27
 ---- batch: 020 ----
mean loss: 213.34
 ---- batch: 030 ----
mean loss: 201.92
 ---- batch: 040 ----
mean loss: 200.53
train mean loss: 207.95
epoch train time: 0:00:01.663012
elapsed time: 0:00:27.623932
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 22:53:40.679099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.07
 ---- batch: 020 ----
mean loss: 207.26
 ---- batch: 030 ----
mean loss: 195.20
 ---- batch: 040 ----
mean loss: 208.98
train mean loss: 204.33
epoch train time: 0:00:01.771887
elapsed time: 0:00:29.395986
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 22:53:42.451131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.91
 ---- batch: 020 ----
mean loss: 201.73
 ---- batch: 030 ----
mean loss: 205.11
 ---- batch: 040 ----
mean loss: 207.52
train mean loss: 204.83
epoch train time: 0:00:01.668882
elapsed time: 0:00:31.065011
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 22:53:44.120162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.13
 ---- batch: 020 ----
mean loss: 203.86
 ---- batch: 030 ----
mean loss: 200.83
 ---- batch: 040 ----
mean loss: 198.20
train mean loss: 202.51
epoch train time: 0:00:01.784677
elapsed time: 0:00:32.849859
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 22:53:45.905008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.89
 ---- batch: 020 ----
mean loss: 201.81
 ---- batch: 030 ----
mean loss: 195.44
 ---- batch: 040 ----
mean loss: 205.89
train mean loss: 200.10
epoch train time: 0:00:01.669141
elapsed time: 0:00:34.519148
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 22:53:47.574310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.34
 ---- batch: 020 ----
mean loss: 197.22
 ---- batch: 030 ----
mean loss: 193.32
 ---- batch: 040 ----
mean loss: 198.55
train mean loss: 198.74
epoch train time: 0:00:01.771842
elapsed time: 0:00:36.291134
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 22:53:49.346280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.27
 ---- batch: 020 ----
mean loss: 193.99
 ---- batch: 030 ----
mean loss: 193.57
 ---- batch: 040 ----
mean loss: 198.36
train mean loss: 197.18
epoch train time: 0:00:01.671368
elapsed time: 0:00:37.962669
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 22:53:51.017822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.30
 ---- batch: 020 ----
mean loss: 195.70
 ---- batch: 030 ----
mean loss: 189.58
 ---- batch: 040 ----
mean loss: 198.48
train mean loss: 195.84
epoch train time: 0:00:01.673663
elapsed time: 0:00:39.636481
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 22:53:52.691629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.50
 ---- batch: 020 ----
mean loss: 194.55
 ---- batch: 030 ----
mean loss: 194.76
 ---- batch: 040 ----
mean loss: 198.58
train mean loss: 194.71
epoch train time: 0:00:01.785709
elapsed time: 0:00:41.422334
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 22:53:54.477487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.08
 ---- batch: 020 ----
mean loss: 200.71
 ---- batch: 030 ----
mean loss: 197.47
 ---- batch: 040 ----
mean loss: 193.90
train mean loss: 197.29
epoch train time: 0:00:01.677816
elapsed time: 0:00:43.100327
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 22:53:56.155476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.91
 ---- batch: 020 ----
mean loss: 196.87
 ---- batch: 030 ----
mean loss: 199.45
 ---- batch: 040 ----
mean loss: 193.92
train mean loss: 195.12
epoch train time: 0:00:01.784222
elapsed time: 0:00:44.884723
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 22:53:57.939872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.83
 ---- batch: 020 ----
mean loss: 199.23
 ---- batch: 030 ----
mean loss: 200.74
 ---- batch: 040 ----
mean loss: 188.37
train mean loss: 194.51
epoch train time: 0:00:01.672721
elapsed time: 0:00:46.557600
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 22:53:59.612755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.17
 ---- batch: 020 ----
mean loss: 196.42
 ---- batch: 030 ----
mean loss: 198.40
 ---- batch: 040 ----
mean loss: 194.07
train mean loss: 192.88
epoch train time: 0:00:01.723152
elapsed time: 0:00:48.280921
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 22:54:01.336084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.60
 ---- batch: 020 ----
mean loss: 189.18
 ---- batch: 030 ----
mean loss: 195.92
 ---- batch: 040 ----
mean loss: 187.81
train mean loss: 190.76
epoch train time: 0:00:01.738177
elapsed time: 0:00:50.019267
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 22:54:03.074417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.95
 ---- batch: 020 ----
mean loss: 199.04
 ---- batch: 030 ----
mean loss: 181.43
 ---- batch: 040 ----
mean loss: 192.70
train mean loss: 192.05
epoch train time: 0:00:01.677100
elapsed time: 0:00:51.696546
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 22:54:04.751696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.60
 ---- batch: 020 ----
mean loss: 194.53
 ---- batch: 030 ----
mean loss: 188.82
 ---- batch: 040 ----
mean loss: 189.91
train mean loss: 193.22
epoch train time: 0:00:01.790667
elapsed time: 0:00:53.487370
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 22:54:06.542520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.03
 ---- batch: 020 ----
mean loss: 191.29
 ---- batch: 030 ----
mean loss: 191.61
 ---- batch: 040 ----
mean loss: 192.43
train mean loss: 191.10
epoch train time: 0:00:01.683338
elapsed time: 0:00:55.170868
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 22:54:08.226015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.01
 ---- batch: 020 ----
mean loss: 196.45
 ---- batch: 030 ----
mean loss: 193.83
 ---- batch: 040 ----
mean loss: 192.67
train mean loss: 192.46
epoch train time: 0:00:01.779760
elapsed time: 0:00:56.950772
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 22:54:10.005926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.04
 ---- batch: 020 ----
mean loss: 195.68
 ---- batch: 030 ----
mean loss: 186.49
 ---- batch: 040 ----
mean loss: 194.11
train mean loss: 191.35
epoch train time: 0:00:01.674925
elapsed time: 0:00:58.625870
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 22:54:11.681049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.98
 ---- batch: 020 ----
mean loss: 187.18
 ---- batch: 030 ----
mean loss: 195.21
 ---- batch: 040 ----
mean loss: 187.15
train mean loss: 188.95
epoch train time: 0:00:01.790898
elapsed time: 0:01:00.416969
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 22:54:13.472110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.83
 ---- batch: 020 ----
mean loss: 192.65
 ---- batch: 030 ----
mean loss: 192.57
 ---- batch: 040 ----
mean loss: 187.84
train mean loss: 191.03
epoch train time: 0:00:01.686606
elapsed time: 0:01:02.103718
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 22:54:15.158886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.12
 ---- batch: 020 ----
mean loss: 185.94
 ---- batch: 030 ----
mean loss: 189.47
 ---- batch: 040 ----
mean loss: 203.83
train mean loss: 192.88
epoch train time: 0:00:01.778172
elapsed time: 0:01:03.882083
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 22:54:16.937234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.40
 ---- batch: 020 ----
mean loss: 203.73
 ---- batch: 030 ----
mean loss: 179.64
 ---- batch: 040 ----
mean loss: 186.22
train mean loss: 192.47
epoch train time: 0:00:01.682868
elapsed time: 0:01:05.565098
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 22:54:18.620262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.24
 ---- batch: 020 ----
mean loss: 192.13
 ---- batch: 030 ----
mean loss: 193.79
 ---- batch: 040 ----
mean loss: 183.32
train mean loss: 191.03
epoch train time: 0:00:01.699962
elapsed time: 0:01:07.265213
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 22:54:20.320358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.51
 ---- batch: 020 ----
mean loss: 191.88
 ---- batch: 030 ----
mean loss: 191.71
 ---- batch: 040 ----
mean loss: 189.17
train mean loss: 191.85
epoch train time: 0:00:01.765189
elapsed time: 0:01:09.030544
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 22:54:22.085707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.88
 ---- batch: 020 ----
mean loss: 188.80
 ---- batch: 030 ----
mean loss: 188.24
 ---- batch: 040 ----
mean loss: 187.28
train mean loss: 190.91
epoch train time: 0:00:01.674121
elapsed time: 0:01:10.704833
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 22:54:23.759979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.64
 ---- batch: 020 ----
mean loss: 192.92
 ---- batch: 030 ----
mean loss: 184.30
 ---- batch: 040 ----
mean loss: 184.61
train mean loss: 188.24
epoch train time: 0:00:01.791375
elapsed time: 0:01:12.496338
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 22:54:25.551484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.09
 ---- batch: 020 ----
mean loss: 194.14
 ---- batch: 030 ----
mean loss: 186.46
 ---- batch: 040 ----
mean loss: 186.99
train mean loss: 188.43
epoch train time: 0:00:01.677333
elapsed time: 0:01:14.173818
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 22:54:27.228970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.44
 ---- batch: 020 ----
mean loss: 185.25
 ---- batch: 030 ----
mean loss: 186.20
 ---- batch: 040 ----
mean loss: 188.55
train mean loss: 187.21
epoch train time: 0:00:01.796578
elapsed time: 0:01:15.970532
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 22:54:29.025677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.24
 ---- batch: 020 ----
mean loss: 191.45
 ---- batch: 030 ----
mean loss: 185.57
 ---- batch: 040 ----
mean loss: 186.80
train mean loss: 185.87
epoch train time: 0:00:01.681679
elapsed time: 0:01:17.652354
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 22:54:30.707523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.17
 ---- batch: 020 ----
mean loss: 180.24
 ---- batch: 030 ----
mean loss: 186.23
 ---- batch: 040 ----
mean loss: 187.10
train mean loss: 186.41
epoch train time: 0:00:01.786874
elapsed time: 0:01:19.439384
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 22:54:32.494529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.79
 ---- batch: 020 ----
mean loss: 175.79
 ---- batch: 030 ----
mean loss: 187.30
 ---- batch: 040 ----
mean loss: 189.92
train mean loss: 186.82
epoch train time: 0:00:01.676219
elapsed time: 0:01:21.115755
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 22:54:34.170906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.36
 ---- batch: 020 ----
mean loss: 187.12
 ---- batch: 030 ----
mean loss: 183.83
 ---- batch: 040 ----
mean loss: 181.30
train mean loss: 185.22
epoch train time: 0:00:01.781354
elapsed time: 0:01:22.897242
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 22:54:35.952406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.84
 ---- batch: 020 ----
mean loss: 181.55
 ---- batch: 030 ----
mean loss: 192.87
 ---- batch: 040 ----
mean loss: 186.45
train mean loss: 185.58
epoch train time: 0:00:01.686878
elapsed time: 0:01:24.584327
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 22:54:37.639475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.16
 ---- batch: 020 ----
mean loss: 180.27
 ---- batch: 030 ----
mean loss: 190.40
 ---- batch: 040 ----
mean loss: 180.91
train mean loss: 183.68
epoch train time: 0:00:01.774532
elapsed time: 0:01:26.358984
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 22:54:39.414126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.19
 ---- batch: 020 ----
mean loss: 180.36
 ---- batch: 030 ----
mean loss: 189.16
 ---- batch: 040 ----
mean loss: 178.85
train mean loss: 182.86
epoch train time: 0:00:01.681678
elapsed time: 0:01:28.040806
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 22:54:41.095958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.10
 ---- batch: 020 ----
mean loss: 187.85
 ---- batch: 030 ----
mean loss: 178.56
 ---- batch: 040 ----
mean loss: 181.76
train mean loss: 184.62
epoch train time: 0:00:01.674205
elapsed time: 0:01:29.715164
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 22:54:42.770328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.85
 ---- batch: 020 ----
mean loss: 188.73
 ---- batch: 030 ----
mean loss: 179.82
 ---- batch: 040 ----
mean loss: 180.43
train mean loss: 183.58
epoch train time: 0:00:01.799142
elapsed time: 0:01:31.514452
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 22:54:44.569597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.09
 ---- batch: 020 ----
mean loss: 183.30
 ---- batch: 030 ----
mean loss: 179.42
 ---- batch: 040 ----
mean loss: 183.00
train mean loss: 183.00
epoch train time: 0:00:01.679921
elapsed time: 0:01:33.194514
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 22:54:46.249663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.38
 ---- batch: 020 ----
mean loss: 181.03
 ---- batch: 030 ----
mean loss: 183.76
 ---- batch: 040 ----
mean loss: 179.43
train mean loss: 182.23
epoch train time: 0:00:01.784524
elapsed time: 0:01:34.979228
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 22:54:48.034380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.41
 ---- batch: 020 ----
mean loss: 177.41
 ---- batch: 030 ----
mean loss: 185.25
 ---- batch: 040 ----
mean loss: 183.03
train mean loss: 181.55
epoch train time: 0:00:01.680510
elapsed time: 0:01:36.659902
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 22:54:49.715055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.06
 ---- batch: 020 ----
mean loss: 187.45
 ---- batch: 030 ----
mean loss: 177.04
 ---- batch: 040 ----
mean loss: 187.75
train mean loss: 184.00
epoch train time: 0:00:01.787848
elapsed time: 0:01:38.447899
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 22:54:51.503052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.97
 ---- batch: 020 ----
mean loss: 177.53
 ---- batch: 030 ----
mean loss: 172.69
 ---- batch: 040 ----
mean loss: 177.67
train mean loss: 178.09
epoch train time: 0:00:01.687570
elapsed time: 0:01:40.135611
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 22:54:53.190769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.65
 ---- batch: 020 ----
mean loss: 179.04
 ---- batch: 030 ----
mean loss: 174.12
 ---- batch: 040 ----
mean loss: 181.31
train mean loss: 179.42
epoch train time: 0:00:01.788577
elapsed time: 0:01:41.924358
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 22:54:54.979505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.26
 ---- batch: 020 ----
mean loss: 178.97
 ---- batch: 030 ----
mean loss: 185.05
 ---- batch: 040 ----
mean loss: 174.48
train mean loss: 180.26
epoch train time: 0:00:01.686847
elapsed time: 0:01:43.611343
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 22:54:56.666491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.15
 ---- batch: 020 ----
mean loss: 171.99
 ---- batch: 030 ----
mean loss: 171.47
 ---- batch: 040 ----
mean loss: 172.22
train mean loss: 175.90
epoch train time: 0:00:01.790270
elapsed time: 0:01:45.401781
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 22:54:58.456928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.18
 ---- batch: 020 ----
mean loss: 174.34
 ---- batch: 030 ----
mean loss: 173.40
 ---- batch: 040 ----
mean loss: 174.29
train mean loss: 174.12
epoch train time: 0:00:01.686357
elapsed time: 0:01:47.088282
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 22:55:00.143432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.39
 ---- batch: 020 ----
mean loss: 180.47
 ---- batch: 030 ----
mean loss: 182.34
 ---- batch: 040 ----
mean loss: 171.48
train mean loss: 176.96
epoch train time: 0:00:01.781353
elapsed time: 0:01:48.869776
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 22:55:01.924928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.77
 ---- batch: 020 ----
mean loss: 185.00
 ---- batch: 030 ----
mean loss: 181.15
 ---- batch: 040 ----
mean loss: 171.34
train mean loss: 180.01
epoch train time: 0:00:01.687183
elapsed time: 0:01:50.557137
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 22:55:03.612299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.13
 ---- batch: 020 ----
mean loss: 178.10
 ---- batch: 030 ----
mean loss: 168.80
 ---- batch: 040 ----
mean loss: 171.93
train mean loss: 174.41
epoch train time: 0:00:01.677745
elapsed time: 0:01:52.235049
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 22:55:05.290197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.43
 ---- batch: 020 ----
mean loss: 171.79
 ---- batch: 030 ----
mean loss: 183.70
 ---- batch: 040 ----
mean loss: 182.54
train mean loss: 176.76
epoch train time: 0:00:01.795441
elapsed time: 0:01:54.030647
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 22:55:07.085814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.10
 ---- batch: 020 ----
mean loss: 173.24
 ---- batch: 030 ----
mean loss: 168.69
 ---- batch: 040 ----
mean loss: 177.91
train mean loss: 172.86
epoch train time: 0:00:01.676197
elapsed time: 0:01:55.707018
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 22:55:08.762168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.15
 ---- batch: 020 ----
mean loss: 174.70
 ---- batch: 030 ----
mean loss: 166.65
 ---- batch: 040 ----
mean loss: 167.33
train mean loss: 168.15
epoch train time: 0:00:01.795265
elapsed time: 0:01:57.502422
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 22:55:10.557570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.99
 ---- batch: 020 ----
mean loss: 170.68
 ---- batch: 030 ----
mean loss: 174.26
 ---- batch: 040 ----
mean loss: 175.16
train mean loss: 170.92
epoch train time: 0:00:01.677329
elapsed time: 0:01:59.179902
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 22:55:12.235053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.39
 ---- batch: 020 ----
mean loss: 173.32
 ---- batch: 030 ----
mean loss: 165.39
 ---- batch: 040 ----
mean loss: 174.62
train mean loss: 171.48
epoch train time: 0:00:01.789268
elapsed time: 0:02:00.969324
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 22:55:14.024473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.95
 ---- batch: 020 ----
mean loss: 166.19
 ---- batch: 030 ----
mean loss: 167.57
 ---- batch: 040 ----
mean loss: 172.29
train mean loss: 167.68
epoch train time: 0:00:01.687362
elapsed time: 0:02:02.656828
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 22:55:15.711977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.93
 ---- batch: 020 ----
mean loss: 169.15
 ---- batch: 030 ----
mean loss: 172.63
 ---- batch: 040 ----
mean loss: 163.34
train mean loss: 169.08
epoch train time: 0:00:01.787612
elapsed time: 0:02:04.444569
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 22:55:17.499715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.66
 ---- batch: 020 ----
mean loss: 165.42
 ---- batch: 030 ----
mean loss: 171.47
 ---- batch: 040 ----
mean loss: 163.50
train mean loss: 169.30
epoch train time: 0:00:01.689491
elapsed time: 0:02:06.134195
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 22:55:19.189341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.21
 ---- batch: 020 ----
mean loss: 167.66
 ---- batch: 030 ----
mean loss: 157.45
 ---- batch: 040 ----
mean loss: 167.98
train mean loss: 164.88
epoch train time: 0:00:01.775697
elapsed time: 0:02:07.910021
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 22:55:20.965167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.57
 ---- batch: 020 ----
mean loss: 159.11
 ---- batch: 030 ----
mean loss: 168.15
 ---- batch: 040 ----
mean loss: 169.39
train mean loss: 165.39
epoch train time: 0:00:01.682168
elapsed time: 0:02:09.592355
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 22:55:22.647505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.26
 ---- batch: 020 ----
mean loss: 163.43
 ---- batch: 030 ----
mean loss: 162.80
 ---- batch: 040 ----
mean loss: 160.46
train mean loss: 162.37
epoch train time: 0:00:01.775470
elapsed time: 0:02:11.367964
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 22:55:24.423120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.18
 ---- batch: 020 ----
mean loss: 158.41
 ---- batch: 030 ----
mean loss: 161.00
 ---- batch: 040 ----
mean loss: 164.08
train mean loss: 162.55
epoch train time: 0:00:01.683240
elapsed time: 0:02:13.051352
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 22:55:26.106516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.84
 ---- batch: 020 ----
mean loss: 156.12
 ---- batch: 030 ----
mean loss: 155.14
 ---- batch: 040 ----
mean loss: 162.54
train mean loss: 160.76
epoch train time: 0:00:01.674882
elapsed time: 0:02:14.726433
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 22:55:27.781611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.60
 ---- batch: 020 ----
mean loss: 163.56
 ---- batch: 030 ----
mean loss: 165.91
 ---- batch: 040 ----
mean loss: 161.35
train mean loss: 164.59
epoch train time: 0:00:01.788112
elapsed time: 0:02:16.514713
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 22:55:29.569858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.06
 ---- batch: 020 ----
mean loss: 159.35
 ---- batch: 030 ----
mean loss: 157.82
 ---- batch: 040 ----
mean loss: 154.85
train mean loss: 158.06
epoch train time: 0:00:01.678342
elapsed time: 0:02:18.193184
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 22:55:31.248330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.60
 ---- batch: 020 ----
mean loss: 158.93
 ---- batch: 030 ----
mean loss: 153.49
 ---- batch: 040 ----
mean loss: 158.04
train mean loss: 157.95
epoch train time: 0:00:01.788201
elapsed time: 0:02:19.981557
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 22:55:33.036707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.12
 ---- batch: 020 ----
mean loss: 160.01
 ---- batch: 030 ----
mean loss: 159.80
 ---- batch: 040 ----
mean loss: 156.42
train mean loss: 161.37
epoch train time: 0:00:01.677385
elapsed time: 0:02:21.659129
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 22:55:34.714306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.33
 ---- batch: 020 ----
mean loss: 160.00
 ---- batch: 030 ----
mean loss: 156.53
 ---- batch: 040 ----
mean loss: 160.93
train mean loss: 158.21
epoch train time: 0:00:01.787207
elapsed time: 0:02:23.446495
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 22:55:36.501654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.60
 ---- batch: 020 ----
mean loss: 168.23
 ---- batch: 030 ----
mean loss: 165.28
 ---- batch: 040 ----
mean loss: 158.80
train mean loss: 161.40
epoch train time: 0:00:01.672902
elapsed time: 0:02:25.119542
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 22:55:38.174690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.45
 ---- batch: 020 ----
mean loss: 155.55
 ---- batch: 030 ----
mean loss: 152.07
 ---- batch: 040 ----
mean loss: 152.47
train mean loss: 153.98
epoch train time: 0:00:01.794909
elapsed time: 0:02:26.914622
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 22:55:39.969810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.57
 ---- batch: 020 ----
mean loss: 158.55
 ---- batch: 030 ----
mean loss: 154.40
 ---- batch: 040 ----
mean loss: 146.09
train mean loss: 152.66
epoch train time: 0:00:01.687905
elapsed time: 0:02:28.602723
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 22:55:41.657885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.34
 ---- batch: 020 ----
mean loss: 158.87
 ---- batch: 030 ----
mean loss: 158.17
 ---- batch: 040 ----
mean loss: 150.85
train mean loss: 158.22
epoch train time: 0:00:01.779870
elapsed time: 0:02:30.382752
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 22:55:43.437909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.22
 ---- batch: 020 ----
mean loss: 145.99
 ---- batch: 030 ----
mean loss: 149.27
 ---- batch: 040 ----
mean loss: 147.30
train mean loss: 148.05
epoch train time: 0:00:01.683764
elapsed time: 0:02:32.066679
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 22:55:45.121849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.08
 ---- batch: 020 ----
mean loss: 158.31
 ---- batch: 030 ----
mean loss: 159.67
 ---- batch: 040 ----
mean loss: 157.01
train mean loss: 157.13
epoch train time: 0:00:01.678554
elapsed time: 0:02:33.745412
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 22:55:46.800579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.34
 ---- batch: 020 ----
mean loss: 152.67
 ---- batch: 030 ----
mean loss: 149.27
 ---- batch: 040 ----
mean loss: 149.93
train mean loss: 150.53
epoch train time: 0:00:01.714462
elapsed time: 0:02:35.460080
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 22:55:48.515247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.66
 ---- batch: 020 ----
mean loss: 152.50
 ---- batch: 030 ----
mean loss: 151.45
 ---- batch: 040 ----
mean loss: 144.05
train mean loss: 149.31
epoch train time: 0:00:01.680685
elapsed time: 0:02:37.140914
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 22:55:50.196080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.06
 ---- batch: 020 ----
mean loss: 149.26
 ---- batch: 030 ----
mean loss: 146.14
 ---- batch: 040 ----
mean loss: 154.27
train mean loss: 148.25
epoch train time: 0:00:01.777663
elapsed time: 0:02:38.918727
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 22:55:51.973873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.87
 ---- batch: 020 ----
mean loss: 150.52
 ---- batch: 030 ----
mean loss: 147.49
 ---- batch: 040 ----
mean loss: 148.16
train mean loss: 148.30
epoch train time: 0:00:01.676790
elapsed time: 0:02:40.595667
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 22:55:53.650851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.77
 ---- batch: 020 ----
mean loss: 147.21
 ---- batch: 030 ----
mean loss: 142.85
 ---- batch: 040 ----
mean loss: 147.56
train mean loss: 145.03
epoch train time: 0:00:01.787342
elapsed time: 0:02:42.383193
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 22:55:55.438341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.38
 ---- batch: 020 ----
mean loss: 149.17
 ---- batch: 030 ----
mean loss: 149.18
 ---- batch: 040 ----
mean loss: 145.73
train mean loss: 148.63
epoch train time: 0:00:01.681036
elapsed time: 0:02:44.064368
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 22:55:57.119515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.33
 ---- batch: 020 ----
mean loss: 149.42
 ---- batch: 030 ----
mean loss: 149.99
 ---- batch: 040 ----
mean loss: 146.44
train mean loss: 147.96
epoch train time: 0:00:01.784060
elapsed time: 0:02:45.848570
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 22:55:58.903723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.50
 ---- batch: 020 ----
mean loss: 148.11
 ---- batch: 030 ----
mean loss: 139.77
 ---- batch: 040 ----
mean loss: 142.68
train mean loss: 143.51
epoch train time: 0:00:01.692925
elapsed time: 0:02:47.541653
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 22:56:00.596795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.69
 ---- batch: 020 ----
mean loss: 138.76
 ---- batch: 030 ----
mean loss: 141.38
 ---- batch: 040 ----
mean loss: 143.67
train mean loss: 141.70
epoch train time: 0:00:01.672304
elapsed time: 0:02:49.214086
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 22:56:02.269253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.12
 ---- batch: 020 ----
mean loss: 142.34
 ---- batch: 030 ----
mean loss: 136.53
 ---- batch: 040 ----
mean loss: 141.76
train mean loss: 139.41
epoch train time: 0:00:01.791159
elapsed time: 0:02:51.005403
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 22:56:04.060551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.10
 ---- batch: 020 ----
mean loss: 143.59
 ---- batch: 030 ----
mean loss: 148.59
 ---- batch: 040 ----
mean loss: 143.21
train mean loss: 144.16
epoch train time: 0:00:01.677461
elapsed time: 0:02:52.683016
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 22:56:05.738216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.73
 ---- batch: 020 ----
mean loss: 137.39
 ---- batch: 030 ----
mean loss: 144.76
 ---- batch: 040 ----
mean loss: 145.26
train mean loss: 142.02
epoch train time: 0:00:01.783693
elapsed time: 0:02:54.466942
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 22:56:07.522097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.79
 ---- batch: 020 ----
mean loss: 145.50
 ---- batch: 030 ----
mean loss: 142.32
 ---- batch: 040 ----
mean loss: 148.53
train mean loss: 145.77
epoch train time: 0:00:01.693127
elapsed time: 0:02:56.160223
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 22:56:09.215387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.81
 ---- batch: 020 ----
mean loss: 139.18
 ---- batch: 030 ----
mean loss: 139.24
 ---- batch: 040 ----
mean loss: 142.31
train mean loss: 139.92
epoch train time: 0:00:01.768388
elapsed time: 0:02:57.928792
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 22:56:10.983940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.71
 ---- batch: 020 ----
mean loss: 140.44
 ---- batch: 030 ----
mean loss: 138.90
 ---- batch: 040 ----
mean loss: 137.89
train mean loss: 139.60
epoch train time: 0:00:01.704981
elapsed time: 0:02:59.633928
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 22:56:12.689080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.99
 ---- batch: 020 ----
mean loss: 141.14
 ---- batch: 030 ----
mean loss: 137.09
 ---- batch: 040 ----
mean loss: 142.46
train mean loss: 141.49
epoch train time: 0:00:01.673137
elapsed time: 0:03:01.307219
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 22:56:14.362372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.91
 ---- batch: 020 ----
mean loss: 133.62
 ---- batch: 030 ----
mean loss: 140.28
 ---- batch: 040 ----
mean loss: 145.22
train mean loss: 140.41
epoch train time: 0:00:01.791823
elapsed time: 0:03:03.099180
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 22:56:16.154317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.02
 ---- batch: 020 ----
mean loss: 145.65
 ---- batch: 030 ----
mean loss: 146.17
 ---- batch: 040 ----
mean loss: 147.11
train mean loss: 144.31
epoch train time: 0:00:01.679106
elapsed time: 0:03:04.778436
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 22:56:17.833595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.96
 ---- batch: 020 ----
mean loss: 140.10
 ---- batch: 030 ----
mean loss: 141.49
 ---- batch: 040 ----
mean loss: 136.85
train mean loss: 139.00
epoch train time: 0:00:01.778050
elapsed time: 0:03:06.556630
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 22:56:19.611790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.22
 ---- batch: 020 ----
mean loss: 140.79
 ---- batch: 030 ----
mean loss: 136.10
 ---- batch: 040 ----
mean loss: 140.19
train mean loss: 138.52
epoch train time: 0:00:01.673207
elapsed time: 0:03:08.230032
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 22:56:21.285181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.04
 ---- batch: 020 ----
mean loss: 141.05
 ---- batch: 030 ----
mean loss: 139.02
 ---- batch: 040 ----
mean loss: 136.98
train mean loss: 140.71
epoch train time: 0:00:01.675571
elapsed time: 0:03:09.905750
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 22:56:22.960971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.38
 ---- batch: 020 ----
mean loss: 136.19
 ---- batch: 030 ----
mean loss: 135.87
 ---- batch: 040 ----
mean loss: 137.28
train mean loss: 135.84
epoch train time: 0:00:01.782607
elapsed time: 0:03:11.688564
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 22:56:24.743726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.92
 ---- batch: 020 ----
mean loss: 132.54
 ---- batch: 030 ----
mean loss: 131.73
 ---- batch: 040 ----
mean loss: 131.53
train mean loss: 132.59
epoch train time: 0:00:01.678500
elapsed time: 0:03:13.367249
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 22:56:26.422413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.22
 ---- batch: 020 ----
mean loss: 139.23
 ---- batch: 030 ----
mean loss: 141.08
 ---- batch: 040 ----
mean loss: 145.38
train mean loss: 140.86
epoch train time: 0:00:01.787752
elapsed time: 0:03:15.155157
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 22:56:28.210304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.77
 ---- batch: 020 ----
mean loss: 136.83
 ---- batch: 030 ----
mean loss: 133.45
 ---- batch: 040 ----
mean loss: 134.42
train mean loss: 136.11
epoch train time: 0:00:01.671469
elapsed time: 0:03:16.826763
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 22:56:29.881913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.48
 ---- batch: 020 ----
mean loss: 130.99
 ---- batch: 030 ----
mean loss: 135.94
 ---- batch: 040 ----
mean loss: 130.88
train mean loss: 134.38
epoch train time: 0:00:01.772505
elapsed time: 0:03:18.599477
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 22:56:31.654633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.66
 ---- batch: 020 ----
mean loss: 138.25
 ---- batch: 030 ----
mean loss: 134.40
 ---- batch: 040 ----
mean loss: 138.75
train mean loss: 136.59
epoch train time: 0:00:01.678513
elapsed time: 0:03:20.278139
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 22:56:33.333304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.46
 ---- batch: 020 ----
mean loss: 131.72
 ---- batch: 030 ----
mean loss: 128.76
 ---- batch: 040 ----
mean loss: 140.58
train mean loss: 133.68
epoch train time: 0:00:01.668139
elapsed time: 0:03:21.946446
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 22:56:35.001595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.28
 ---- batch: 020 ----
mean loss: 128.77
 ---- batch: 030 ----
mean loss: 134.12
 ---- batch: 040 ----
mean loss: 125.52
train mean loss: 132.10
epoch train time: 0:00:01.802084
elapsed time: 0:03:23.748684
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 22:56:36.803832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.27
 ---- batch: 020 ----
mean loss: 129.14
 ---- batch: 030 ----
mean loss: 140.63
 ---- batch: 040 ----
mean loss: 132.09
train mean loss: 132.83
epoch train time: 0:00:01.683173
elapsed time: 0:03:25.432011
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 22:56:38.487164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.36
 ---- batch: 020 ----
mean loss: 132.55
 ---- batch: 030 ----
mean loss: 131.46
 ---- batch: 040 ----
mean loss: 128.60
train mean loss: 132.51
epoch train time: 0:00:01.789005
elapsed time: 0:03:27.221158
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 22:56:40.276336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.24
 ---- batch: 020 ----
mean loss: 126.91
 ---- batch: 030 ----
mean loss: 128.62
 ---- batch: 040 ----
mean loss: 128.58
train mean loss: 129.75
epoch train time: 0:00:01.681772
elapsed time: 0:03:28.903101
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 22:56:41.958249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.46
 ---- batch: 020 ----
mean loss: 127.58
 ---- batch: 030 ----
mean loss: 135.56
 ---- batch: 040 ----
mean loss: 135.44
train mean loss: 132.38
epoch train time: 0:00:01.668209
elapsed time: 0:03:30.571466
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 22:56:43.626617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.92
 ---- batch: 020 ----
mean loss: 127.10
 ---- batch: 030 ----
mean loss: 132.01
 ---- batch: 040 ----
mean loss: 136.99
train mean loss: 130.99
epoch train time: 0:00:01.781860
elapsed time: 0:03:32.353473
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 22:56:45.408616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.99
 ---- batch: 020 ----
mean loss: 130.27
 ---- batch: 030 ----
mean loss: 132.94
 ---- batch: 040 ----
mean loss: 130.27
train mean loss: 130.88
epoch train time: 0:00:01.682464
elapsed time: 0:03:34.036077
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 22:56:47.091246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.93
 ---- batch: 020 ----
mean loss: 130.83
 ---- batch: 030 ----
mean loss: 129.83
 ---- batch: 040 ----
mean loss: 128.21
train mean loss: 128.20
epoch train time: 0:00:01.781873
elapsed time: 0:03:35.818108
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 22:56:48.873266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.61
 ---- batch: 020 ----
mean loss: 128.57
 ---- batch: 030 ----
mean loss: 129.74
 ---- batch: 040 ----
mean loss: 130.07
train mean loss: 128.77
epoch train time: 0:00:01.679176
elapsed time: 0:03:37.497433
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 22:56:50.552582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.80
 ---- batch: 020 ----
mean loss: 127.27
 ---- batch: 030 ----
mean loss: 126.00
 ---- batch: 040 ----
mean loss: 127.49
train mean loss: 126.21
epoch train time: 0:00:01.753987
elapsed time: 0:03:39.251554
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 22:56:52.306699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.18
 ---- batch: 020 ----
mean loss: 123.44
 ---- batch: 030 ----
mean loss: 128.96
 ---- batch: 040 ----
mean loss: 131.48
train mean loss: 129.74
epoch train time: 0:00:01.712516
elapsed time: 0:03:40.964206
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 22:56:54.019354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.75
 ---- batch: 020 ----
mean loss: 127.73
 ---- batch: 030 ----
mean loss: 131.49
 ---- batch: 040 ----
mean loss: 128.22
train mean loss: 128.18
epoch train time: 0:00:01.675034
elapsed time: 0:03:42.639397
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 22:56:55.694548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.77
 ---- batch: 020 ----
mean loss: 131.04
 ---- batch: 030 ----
mean loss: 121.33
 ---- batch: 040 ----
mean loss: 137.32
train mean loss: 129.31
epoch train time: 0:00:01.786022
elapsed time: 0:03:44.425585
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 22:56:57.480733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.53
 ---- batch: 020 ----
mean loss: 135.90
 ---- batch: 030 ----
mean loss: 136.09
 ---- batch: 040 ----
mean loss: 126.79
train mean loss: 132.27
epoch train time: 0:00:01.680565
elapsed time: 0:03:46.106285
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 22:56:59.161431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.17
 ---- batch: 020 ----
mean loss: 129.80
 ---- batch: 030 ----
mean loss: 134.72
 ---- batch: 040 ----
mean loss: 134.27
train mean loss: 131.90
epoch train time: 0:00:01.785360
elapsed time: 0:03:47.891780
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 22:57:00.946931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.31
 ---- batch: 020 ----
mean loss: 128.14
 ---- batch: 030 ----
mean loss: 122.44
 ---- batch: 040 ----
mean loss: 124.48
train mean loss: 125.12
epoch train time: 0:00:01.679063
elapsed time: 0:03:49.570991
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 22:57:02.626140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.36
 ---- batch: 020 ----
mean loss: 126.50
 ---- batch: 030 ----
mean loss: 128.36
 ---- batch: 040 ----
mean loss: 126.81
train mean loss: 127.55
epoch train time: 0:00:01.671821
elapsed time: 0:03:51.242943
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 22:57:04.298105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.66
 ---- batch: 020 ----
mean loss: 129.30
 ---- batch: 030 ----
mean loss: 138.32
 ---- batch: 040 ----
mean loss: 139.26
train mean loss: 133.08
epoch train time: 0:00:01.790547
elapsed time: 0:03:53.033688
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 22:57:06.088832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.29
 ---- batch: 020 ----
mean loss: 129.31
 ---- batch: 030 ----
mean loss: 126.21
 ---- batch: 040 ----
mean loss: 132.37
train mean loss: 130.81
epoch train time: 0:00:01.678605
elapsed time: 0:03:54.712457
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 22:57:07.767622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.83
 ---- batch: 020 ----
mean loss: 137.12
 ---- batch: 030 ----
mean loss: 129.85
 ---- batch: 040 ----
mean loss: 130.24
train mean loss: 130.58
epoch train time: 0:00:01.785333
elapsed time: 0:03:56.497942
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 22:57:09.553088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.73
 ---- batch: 020 ----
mean loss: 127.80
 ---- batch: 030 ----
mean loss: 128.04
 ---- batch: 040 ----
mean loss: 128.14
train mean loss: 127.15
epoch train time: 0:00:01.682589
elapsed time: 0:03:58.180668
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 22:57:11.235815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.95
 ---- batch: 020 ----
mean loss: 125.10
 ---- batch: 030 ----
mean loss: 124.96
 ---- batch: 040 ----
mean loss: 124.51
train mean loss: 126.63
epoch train time: 0:00:01.770029
elapsed time: 0:03:59.950842
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 22:57:13.006017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.63
 ---- batch: 020 ----
mean loss: 127.63
 ---- batch: 030 ----
mean loss: 121.18
 ---- batch: 040 ----
mean loss: 121.96
train mean loss: 124.54
epoch train time: 0:00:01.693101
elapsed time: 0:04:01.644113
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 22:57:14.699277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.69
 ---- batch: 020 ----
mean loss: 122.18
 ---- batch: 030 ----
mean loss: 125.76
 ---- batch: 040 ----
mean loss: 129.86
train mean loss: 125.85
epoch train time: 0:00:01.673659
elapsed time: 0:04:03.317929
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 22:57:16.373081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.45
 ---- batch: 020 ----
mean loss: 122.89
 ---- batch: 030 ----
mean loss: 126.05
 ---- batch: 040 ----
mean loss: 122.46
train mean loss: 123.64
epoch train time: 0:00:01.792980
elapsed time: 0:04:05.111043
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 22:57:18.166187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.80
 ---- batch: 020 ----
mean loss: 125.96
 ---- batch: 030 ----
mean loss: 128.87
 ---- batch: 040 ----
mean loss: 126.62
train mean loss: 126.57
epoch train time: 0:00:01.685996
elapsed time: 0:04:06.797183
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 22:57:19.852345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.91
 ---- batch: 020 ----
mean loss: 123.10
 ---- batch: 030 ----
mean loss: 123.76
 ---- batch: 040 ----
mean loss: 122.35
train mean loss: 124.31
epoch train time: 0:00:01.773349
elapsed time: 0:04:08.570683
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 22:57:21.625832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.16
 ---- batch: 020 ----
mean loss: 128.43
 ---- batch: 030 ----
mean loss: 120.91
 ---- batch: 040 ----
mean loss: 122.45
train mean loss: 124.87
epoch train time: 0:00:01.683895
elapsed time: 0:04:10.254720
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 22:57:23.309867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.06
 ---- batch: 020 ----
mean loss: 124.08
 ---- batch: 030 ----
mean loss: 137.53
 ---- batch: 040 ----
mean loss: 128.81
train mean loss: 128.58
epoch train time: 0:00:01.669229
elapsed time: 0:04:11.924076
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 22:57:24.979219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.45
 ---- batch: 020 ----
mean loss: 127.91
 ---- batch: 030 ----
mean loss: 118.42
 ---- batch: 040 ----
mean loss: 124.12
train mean loss: 124.51
epoch train time: 0:00:01.776207
elapsed time: 0:04:13.700406
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 22:57:26.755553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.63
 ---- batch: 020 ----
mean loss: 137.71
 ---- batch: 030 ----
mean loss: 127.79
 ---- batch: 040 ----
mean loss: 124.35
train mean loss: 129.63
epoch train time: 0:00:01.675843
elapsed time: 0:04:15.376399
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 22:57:28.431578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.97
 ---- batch: 020 ----
mean loss: 123.68
 ---- batch: 030 ----
mean loss: 124.68
 ---- batch: 040 ----
mean loss: 124.92
train mean loss: 125.42
epoch train time: 0:00:01.791181
elapsed time: 0:04:17.167760
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 22:57:30.222905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.42
 ---- batch: 020 ----
mean loss: 130.59
 ---- batch: 030 ----
mean loss: 123.75
 ---- batch: 040 ----
mean loss: 126.15
train mean loss: 125.55
epoch train time: 0:00:01.674803
elapsed time: 0:04:18.842891
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 22:57:31.898049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.75
 ---- batch: 020 ----
mean loss: 124.12
 ---- batch: 030 ----
mean loss: 125.43
 ---- batch: 040 ----
mean loss: 115.84
train mean loss: 121.23
epoch train time: 0:00:01.782266
elapsed time: 0:04:20.625328
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 22:57:33.680477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.22
 ---- batch: 020 ----
mean loss: 123.11
 ---- batch: 030 ----
mean loss: 124.86
 ---- batch: 040 ----
mean loss: 130.27
train mean loss: 124.03
epoch train time: 0:00:01.680056
elapsed time: 0:04:22.305534
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 22:57:35.360733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.98
 ---- batch: 020 ----
mean loss: 124.14
 ---- batch: 030 ----
mean loss: 126.46
 ---- batch: 040 ----
mean loss: 119.36
train mean loss: 123.44
epoch train time: 0:00:01.761524
elapsed time: 0:04:24.067291
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 22:57:37.122446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.85
 ---- batch: 020 ----
mean loss: 121.19
 ---- batch: 030 ----
mean loss: 119.35
 ---- batch: 040 ----
mean loss: 122.25
train mean loss: 122.29
epoch train time: 0:00:01.701768
elapsed time: 0:04:25.769220
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 22:57:38.824370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.21
 ---- batch: 020 ----
mean loss: 123.75
 ---- batch: 030 ----
mean loss: 120.76
 ---- batch: 040 ----
mean loss: 123.36
train mean loss: 121.42
epoch train time: 0:00:01.672750
elapsed time: 0:04:27.442109
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 22:57:40.497258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.45
 ---- batch: 020 ----
mean loss: 120.58
 ---- batch: 030 ----
mean loss: 126.65
 ---- batch: 040 ----
mean loss: 116.78
train mean loss: 123.98
epoch train time: 0:00:01.790585
elapsed time: 0:04:29.232856
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 22:57:42.288005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.39
 ---- batch: 020 ----
mean loss: 127.02
 ---- batch: 030 ----
mean loss: 121.90
 ---- batch: 040 ----
mean loss: 121.34
train mean loss: 124.18
epoch train time: 0:00:01.671814
elapsed time: 0:04:30.904826
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 22:57:43.959997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.64
 ---- batch: 020 ----
mean loss: 127.53
 ---- batch: 030 ----
mean loss: 123.86
 ---- batch: 040 ----
mean loss: 124.14
train mean loss: 125.56
epoch train time: 0:00:01.780658
elapsed time: 0:04:32.685653
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 22:57:45.740805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.12
 ---- batch: 020 ----
mean loss: 123.44
 ---- batch: 030 ----
mean loss: 126.75
 ---- batch: 040 ----
mean loss: 122.37
train mean loss: 124.32
epoch train time: 0:00:01.678281
elapsed time: 0:04:34.364077
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 22:57:47.419225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.43
 ---- batch: 020 ----
mean loss: 120.16
 ---- batch: 030 ----
mean loss: 125.90
 ---- batch: 040 ----
mean loss: 126.42
train mean loss: 124.84
epoch train time: 0:00:01.803693
elapsed time: 0:04:36.167900
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 22:57:49.223065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.66
 ---- batch: 020 ----
mean loss: 121.28
 ---- batch: 030 ----
mean loss: 124.55
 ---- batch: 040 ----
mean loss: 132.01
train mean loss: 125.88
epoch train time: 0:00:01.680489
elapsed time: 0:04:37.848577
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 22:57:50.903726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.22
 ---- batch: 020 ----
mean loss: 122.68
 ---- batch: 030 ----
mean loss: 117.70
 ---- batch: 040 ----
mean loss: 125.01
train mean loss: 122.91
epoch train time: 0:00:01.783966
elapsed time: 0:04:39.632687
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 22:57:52.687830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.12
 ---- batch: 020 ----
mean loss: 127.54
 ---- batch: 030 ----
mean loss: 117.15
 ---- batch: 040 ----
mean loss: 125.31
train mean loss: 124.78
epoch train time: 0:00:01.686593
elapsed time: 0:04:41.319414
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 22:57:54.374564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.06
 ---- batch: 020 ----
mean loss: 124.16
 ---- batch: 030 ----
mean loss: 129.50
 ---- batch: 040 ----
mean loss: 120.82
train mean loss: 124.28
epoch train time: 0:00:01.773816
elapsed time: 0:04:43.093393
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 22:57:56.148573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.33
 ---- batch: 020 ----
mean loss: 120.71
 ---- batch: 030 ----
mean loss: 116.50
 ---- batch: 040 ----
mean loss: 131.82
train mean loss: 123.32
epoch train time: 0:00:01.684139
elapsed time: 0:04:44.777728
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 22:57:57.832882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.17
 ---- batch: 020 ----
mean loss: 121.62
 ---- batch: 030 ----
mean loss: 119.91
 ---- batch: 040 ----
mean loss: 122.35
train mean loss: 121.13
epoch train time: 0:00:01.672200
elapsed time: 0:04:46.450060
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 22:57:59.505203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.46
 ---- batch: 020 ----
mean loss: 123.49
 ---- batch: 030 ----
mean loss: 132.85
 ---- batch: 040 ----
mean loss: 131.56
train mean loss: 128.07
epoch train time: 0:00:01.788164
elapsed time: 0:04:48.238370
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 22:58:01.293520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.51
 ---- batch: 020 ----
mean loss: 123.31
 ---- batch: 030 ----
mean loss: 122.93
 ---- batch: 040 ----
mean loss: 122.85
train mean loss: 123.12
epoch train time: 0:00:01.666018
elapsed time: 0:04:49.904533
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 22:58:02.959684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.91
 ---- batch: 020 ----
mean loss: 118.85
 ---- batch: 030 ----
mean loss: 121.09
 ---- batch: 040 ----
mean loss: 126.33
train mean loss: 122.22
epoch train time: 0:00:01.792484
elapsed time: 0:04:51.697157
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 22:58:04.752331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.03
 ---- batch: 020 ----
mean loss: 119.59
 ---- batch: 030 ----
mean loss: 122.48
 ---- batch: 040 ----
mean loss: 124.98
train mean loss: 123.01
epoch train time: 0:00:01.675602
elapsed time: 0:04:53.372947
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 22:58:06.428096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.40
 ---- batch: 020 ----
mean loss: 118.17
 ---- batch: 030 ----
mean loss: 119.99
 ---- batch: 040 ----
mean loss: 124.09
train mean loss: 121.65
epoch train time: 0:00:01.782033
elapsed time: 0:04:55.155129
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 22:58:08.210279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.13
 ---- batch: 020 ----
mean loss: 118.36
 ---- batch: 030 ----
mean loss: 122.85
 ---- batch: 040 ----
mean loss: 118.59
train mean loss: 121.16
epoch train time: 0:00:01.673463
elapsed time: 0:04:56.828746
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 22:58:09.883912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.27
 ---- batch: 020 ----
mean loss: 128.04
 ---- batch: 030 ----
mean loss: 124.66
 ---- batch: 040 ----
mean loss: 119.04
train mean loss: 125.08
epoch train time: 0:00:01.785808
elapsed time: 0:04:58.614722
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 22:58:11.669886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.29
 ---- batch: 020 ----
mean loss: 121.39
 ---- batch: 030 ----
mean loss: 119.80
 ---- batch: 040 ----
mean loss: 120.10
train mean loss: 123.19
epoch train time: 0:00:01.678211
elapsed time: 0:05:00.293104
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 22:58:13.348255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.91
 ---- batch: 020 ----
mean loss: 118.95
 ---- batch: 030 ----
mean loss: 121.06
 ---- batch: 040 ----
mean loss: 113.03
train mean loss: 117.97
epoch train time: 0:00:01.766688
elapsed time: 0:05:02.059960
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 22:58:15.115115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.22
 ---- batch: 020 ----
mean loss: 124.61
 ---- batch: 030 ----
mean loss: 123.56
 ---- batch: 040 ----
mean loss: 121.47
train mean loss: 122.82
epoch train time: 0:00:01.669331
elapsed time: 0:05:03.729463
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 22:58:16.784613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.57
 ---- batch: 020 ----
mean loss: 126.91
 ---- batch: 030 ----
mean loss: 124.60
 ---- batch: 040 ----
mean loss: 118.82
train mean loss: 125.93
epoch train time: 0:00:01.662446
elapsed time: 0:05:05.392062
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 22:58:18.447215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.78
 ---- batch: 020 ----
mean loss: 122.05
 ---- batch: 030 ----
mean loss: 119.87
 ---- batch: 040 ----
mean loss: 119.88
train mean loss: 121.05
epoch train time: 0:00:01.794202
elapsed time: 0:05:07.186421
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 22:58:20.241571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.39
 ---- batch: 020 ----
mean loss: 123.84
 ---- batch: 030 ----
mean loss: 127.67
 ---- batch: 040 ----
mean loss: 126.03
train mean loss: 125.82
epoch train time: 0:00:01.670987
elapsed time: 0:05:08.857565
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 22:58:21.912720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.26
 ---- batch: 020 ----
mean loss: 116.51
 ---- batch: 030 ----
mean loss: 123.04
 ---- batch: 040 ----
mean loss: 118.70
train mean loss: 119.69
epoch train time: 0:00:01.779684
elapsed time: 0:05:10.637399
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 22:58:23.692552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.68
 ---- batch: 020 ----
mean loss: 117.83
 ---- batch: 030 ----
mean loss: 120.90
 ---- batch: 040 ----
mean loss: 119.17
train mean loss: 120.47
epoch train time: 0:00:01.672185
elapsed time: 0:05:12.309749
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 22:58:25.364903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.94
 ---- batch: 020 ----
mean loss: 125.11
 ---- batch: 030 ----
mean loss: 124.62
 ---- batch: 040 ----
mean loss: 126.22
train mean loss: 123.65
epoch train time: 0:00:01.774577
elapsed time: 0:05:14.084489
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 22:58:27.139627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.09
 ---- batch: 020 ----
mean loss: 121.71
 ---- batch: 030 ----
mean loss: 122.95
 ---- batch: 040 ----
mean loss: 121.71
train mean loss: 122.23
epoch train time: 0:00:01.665247
elapsed time: 0:05:15.749886
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 22:58:28.805055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.45
 ---- batch: 020 ----
mean loss: 121.84
 ---- batch: 030 ----
mean loss: 119.82
 ---- batch: 040 ----
mean loss: 122.39
train mean loss: 120.54
epoch train time: 0:00:01.767617
elapsed time: 0:05:17.517668
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 22:58:30.572817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.14
 ---- batch: 020 ----
mean loss: 121.85
 ---- batch: 030 ----
mean loss: 124.58
 ---- batch: 040 ----
mean loss: 118.26
train mean loss: 120.47
epoch train time: 0:00:01.673262
elapsed time: 0:05:19.191096
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 22:58:32.246257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.95
 ---- batch: 020 ----
mean loss: 119.60
 ---- batch: 030 ----
mean loss: 119.40
 ---- batch: 040 ----
mean loss: 120.13
train mean loss: 121.10
epoch train time: 0:00:01.771812
elapsed time: 0:05:20.963083
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 22:58:34.018234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.55
 ---- batch: 020 ----
mean loss: 118.20
 ---- batch: 030 ----
mean loss: 123.76
 ---- batch: 040 ----
mean loss: 122.70
train mean loss: 121.71
epoch train time: 0:00:01.674369
elapsed time: 0:05:22.637674
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 22:58:35.692862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.51
 ---- batch: 020 ----
mean loss: 123.98
 ---- batch: 030 ----
mean loss: 122.50
 ---- batch: 040 ----
mean loss: 120.87
train mean loss: 121.44
epoch train time: 0:00:01.770547
elapsed time: 0:05:24.408410
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 22:58:37.463560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.16
 ---- batch: 020 ----
mean loss: 125.97
 ---- batch: 030 ----
mean loss: 121.39
 ---- batch: 040 ----
mean loss: 128.99
train mean loss: 124.78
epoch train time: 0:00:01.676035
elapsed time: 0:05:26.084598
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 22:58:39.139751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.12
 ---- batch: 020 ----
mean loss: 119.81
 ---- batch: 030 ----
mean loss: 121.11
 ---- batch: 040 ----
mean loss: 122.02
train mean loss: 120.51
epoch train time: 0:00:01.671612
elapsed time: 0:05:27.756364
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 22:58:40.811514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.25
 ---- batch: 020 ----
mean loss: 137.43
 ---- batch: 030 ----
mean loss: 124.83
 ---- batch: 040 ----
mean loss: 127.54
train mean loss: 127.28
epoch train time: 0:00:01.793390
elapsed time: 0:05:29.549908
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 22:58:42.605058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.90
 ---- batch: 020 ----
mean loss: 122.42
 ---- batch: 030 ----
mean loss: 118.66
 ---- batch: 040 ----
mean loss: 121.58
train mean loss: 122.37
epoch train time: 0:00:01.669772
elapsed time: 0:05:31.219836
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 22:58:44.274989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.37
 ---- batch: 020 ----
mean loss: 123.01
 ---- batch: 030 ----
mean loss: 115.55
 ---- batch: 040 ----
mean loss: 122.16
train mean loss: 120.85
epoch train time: 0:00:01.775408
elapsed time: 0:05:32.995407
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 22:58:46.050558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.18
 ---- batch: 020 ----
mean loss: 122.02
 ---- batch: 030 ----
mean loss: 119.96
 ---- batch: 040 ----
mean loss: 122.90
train mean loss: 120.10
epoch train time: 0:00:01.666613
elapsed time: 0:05:34.662178
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 22:58:47.717329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.64
 ---- batch: 020 ----
mean loss: 118.50
 ---- batch: 030 ----
mean loss: 119.36
 ---- batch: 040 ----
mean loss: 122.38
train mean loss: 119.93
epoch train time: 0:00:01.789928
elapsed time: 0:05:36.452260
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 22:58:49.507427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.33
 ---- batch: 020 ----
mean loss: 116.72
 ---- batch: 030 ----
mean loss: 115.16
 ---- batch: 040 ----
mean loss: 121.16
train mean loss: 119.23
epoch train time: 0:00:01.671496
elapsed time: 0:05:38.123924
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 22:58:51.179071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.47
 ---- batch: 020 ----
mean loss: 120.24
 ---- batch: 030 ----
mean loss: 120.44
 ---- batch: 040 ----
mean loss: 115.12
train mean loss: 119.02
epoch train time: 0:00:01.770917
elapsed time: 0:05:39.894992
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 22:58:52.950142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.85
 ---- batch: 020 ----
mean loss: 120.56
 ---- batch: 030 ----
mean loss: 115.56
 ---- batch: 040 ----
mean loss: 118.28
train mean loss: 118.05
epoch train time: 0:00:01.676703
elapsed time: 0:05:41.571871
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 22:58:54.627029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.44
 ---- batch: 020 ----
mean loss: 121.92
 ---- batch: 030 ----
mean loss: 117.55
 ---- batch: 040 ----
mean loss: 121.96
train mean loss: 120.10
epoch train time: 0:00:01.764873
elapsed time: 0:05:43.336904
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 22:58:56.392052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.63
 ---- batch: 020 ----
mean loss: 119.96
 ---- batch: 030 ----
mean loss: 122.81
 ---- batch: 040 ----
mean loss: 124.20
train mean loss: 121.90
epoch train time: 0:00:01.693754
elapsed time: 0:05:45.030815
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 22:58:58.085970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.77
 ---- batch: 020 ----
mean loss: 118.25
 ---- batch: 030 ----
mean loss: 120.73
 ---- batch: 040 ----
mean loss: 117.62
train mean loss: 118.88
epoch train time: 0:00:01.665647
elapsed time: 0:05:46.696612
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 22:58:59.751764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.81
 ---- batch: 020 ----
mean loss: 121.81
 ---- batch: 030 ----
mean loss: 115.96
 ---- batch: 040 ----
mean loss: 117.83
train mean loss: 118.56
epoch train time: 0:00:01.778540
elapsed time: 0:05:48.475301
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 22:59:01.530451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.96
 ---- batch: 020 ----
mean loss: 123.46
 ---- batch: 030 ----
mean loss: 123.01
 ---- batch: 040 ----
mean loss: 120.50
train mean loss: 120.94
epoch train time: 0:00:01.662720
elapsed time: 0:05:50.138182
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 22:59:03.193335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.76
 ---- batch: 020 ----
mean loss: 119.92
 ---- batch: 030 ----
mean loss: 117.75
 ---- batch: 040 ----
mean loss: 116.70
train mean loss: 118.34
epoch train time: 0:00:01.782424
elapsed time: 0:05:51.920759
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 22:59:04.975911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.54
 ---- batch: 020 ----
mean loss: 119.59
 ---- batch: 030 ----
mean loss: 117.02
 ---- batch: 040 ----
mean loss: 125.59
train mean loss: 120.28
epoch train time: 0:00:01.666695
elapsed time: 0:05:53.587615
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 22:59:06.642776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.15
 ---- batch: 020 ----
mean loss: 117.44
 ---- batch: 030 ----
mean loss: 116.43
 ---- batch: 040 ----
mean loss: 120.19
train mean loss: 118.65
epoch train time: 0:00:01.778396
elapsed time: 0:05:55.366206
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 22:59:08.421370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.60
 ---- batch: 020 ----
mean loss: 118.35
 ---- batch: 030 ----
mean loss: 119.06
 ---- batch: 040 ----
mean loss: 116.00
train mean loss: 117.46
epoch train time: 0:00:01.676731
elapsed time: 0:05:57.043097
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 22:59:10.098266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.56
 ---- batch: 020 ----
mean loss: 128.74
 ---- batch: 030 ----
mean loss: 130.00
 ---- batch: 040 ----
mean loss: 118.20
train mean loss: 123.57
epoch train time: 0:00:01.782044
elapsed time: 0:05:58.825318
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 22:59:11.880471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.52
 ---- batch: 020 ----
mean loss: 118.64
 ---- batch: 030 ----
mean loss: 119.98
 ---- batch: 040 ----
mean loss: 116.19
train mean loss: 118.53
epoch train time: 0:00:01.689482
elapsed time: 0:06:00.514958
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 22:59:13.570111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.79
 ---- batch: 020 ----
mean loss: 115.36
 ---- batch: 030 ----
mean loss: 112.64
 ---- batch: 040 ----
mean loss: 116.27
train mean loss: 115.65
epoch train time: 0:00:01.770656
elapsed time: 0:06:02.285776
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 22:59:15.340929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.46
 ---- batch: 020 ----
mean loss: 115.99
 ---- batch: 030 ----
mean loss: 116.10
 ---- batch: 040 ----
mean loss: 112.36
train mean loss: 115.02
epoch train time: 0:00:01.684471
elapsed time: 0:06:03.970408
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 22:59:17.025557
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.77
 ---- batch: 020 ----
mean loss: 115.68
 ---- batch: 030 ----
mean loss: 112.99
 ---- batch: 040 ----
mean loss: 112.59
train mean loss: 114.19
epoch train time: 0:00:01.666296
elapsed time: 0:06:05.636846
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 22:59:18.692010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.86
 ---- batch: 020 ----
mean loss: 116.69
 ---- batch: 030 ----
mean loss: 114.36
 ---- batch: 040 ----
mean loss: 117.48
train mean loss: 115.12
epoch train time: 0:00:01.786394
elapsed time: 0:06:07.423413
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 22:59:20.478567
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.03
 ---- batch: 020 ----
mean loss: 115.43
 ---- batch: 030 ----
mean loss: 114.81
 ---- batch: 040 ----
mean loss: 118.00
train mean loss: 116.11
epoch train time: 0:00:01.669197
elapsed time: 0:06:09.092795
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 22:59:22.147948
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.67
 ---- batch: 020 ----
mean loss: 118.19
 ---- batch: 030 ----
mean loss: 116.45
 ---- batch: 040 ----
mean loss: 115.51
train mean loss: 116.27
epoch train time: 0:00:01.791098
elapsed time: 0:06:10.884062
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 22:59:23.939219
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.20
 ---- batch: 020 ----
mean loss: 112.62
 ---- batch: 030 ----
mean loss: 112.31
 ---- batch: 040 ----
mean loss: 117.33
train mean loss: 115.62
epoch train time: 0:00:01.675250
elapsed time: 0:06:12.559480
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 22:59:25.614634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.59
 ---- batch: 020 ----
mean loss: 114.95
 ---- batch: 030 ----
mean loss: 110.57
 ---- batch: 040 ----
mean loss: 114.45
train mean loss: 113.32
epoch train time: 0:00:01.777952
elapsed time: 0:06:14.337611
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 22:59:27.392772
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.25
 ---- batch: 020 ----
mean loss: 119.46
 ---- batch: 030 ----
mean loss: 114.78
 ---- batch: 040 ----
mean loss: 119.66
train mean loss: 116.22
epoch train time: 0:00:01.667522
elapsed time: 0:06:16.005324
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 22:59:29.060474
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.10
 ---- batch: 020 ----
mean loss: 115.83
 ---- batch: 030 ----
mean loss: 114.52
 ---- batch: 040 ----
mean loss: 111.92
train mean loss: 114.98
epoch train time: 0:00:01.780818
elapsed time: 0:06:17.786279
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 22:59:30.841441
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.36
 ---- batch: 020 ----
mean loss: 116.85
 ---- batch: 030 ----
mean loss: 115.73
 ---- batch: 040 ----
mean loss: 116.31
train mean loss: 115.49
epoch train time: 0:00:01.678795
elapsed time: 0:06:19.465232
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 22:59:32.520402
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.48
 ---- batch: 020 ----
mean loss: 114.13
 ---- batch: 030 ----
mean loss: 113.74
 ---- batch: 040 ----
mean loss: 115.71
train mean loss: 115.81
epoch train time: 0:00:01.777893
elapsed time: 0:06:21.243286
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 22:59:34.298436
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.99
 ---- batch: 020 ----
mean loss: 111.26
 ---- batch: 030 ----
mean loss: 120.47
 ---- batch: 040 ----
mean loss: 115.74
train mean loss: 115.44
epoch train time: 0:00:01.679961
elapsed time: 0:06:22.923420
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 22:59:35.978585
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.79
 ---- batch: 020 ----
mean loss: 112.96
 ---- batch: 030 ----
mean loss: 116.18
 ---- batch: 040 ----
mean loss: 114.91
train mean loss: 114.22
epoch train time: 0:00:01.673389
elapsed time: 0:06:24.596964
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 22:59:37.652144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.89
 ---- batch: 020 ----
mean loss: 114.63
 ---- batch: 030 ----
mean loss: 113.47
 ---- batch: 040 ----
mean loss: 114.69
train mean loss: 113.93
epoch train time: 0:00:01.758724
elapsed time: 0:06:26.355873
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 22:59:39.411023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.59
 ---- batch: 020 ----
mean loss: 115.73
 ---- batch: 030 ----
mean loss: 114.23
 ---- batch: 040 ----
mean loss: 113.63
train mean loss: 115.66
epoch train time: 0:00:01.671781
elapsed time: 0:06:28.027791
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 22:59:41.082940
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.24
 ---- batch: 020 ----
mean loss: 112.92
 ---- batch: 030 ----
mean loss: 116.04
 ---- batch: 040 ----
mean loss: 114.44
train mean loss: 114.58
epoch train time: 0:00:01.779186
elapsed time: 0:06:29.807117
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 22:59:42.862266
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.58
 ---- batch: 020 ----
mean loss: 117.73
 ---- batch: 030 ----
mean loss: 115.54
 ---- batch: 040 ----
mean loss: 115.52
train mean loss: 115.29
epoch train time: 0:00:01.672045
elapsed time: 0:06:31.479348
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 22:59:44.534495
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.89
 ---- batch: 020 ----
mean loss: 111.02
 ---- batch: 030 ----
mean loss: 113.97
 ---- batch: 040 ----
mean loss: 116.22
train mean loss: 114.34
epoch train time: 0:00:01.782315
elapsed time: 0:06:33.261827
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 22:59:46.316977
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.36
 ---- batch: 020 ----
mean loss: 113.97
 ---- batch: 030 ----
mean loss: 116.34
 ---- batch: 040 ----
mean loss: 114.19
train mean loss: 114.54
epoch train time: 0:00:01.682021
elapsed time: 0:06:34.944008
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 22:59:47.999156
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.26
 ---- batch: 020 ----
mean loss: 114.17
 ---- batch: 030 ----
mean loss: 111.59
 ---- batch: 040 ----
mean loss: 116.54
train mean loss: 114.28
epoch train time: 0:00:01.780339
elapsed time: 0:06:36.724483
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 22:59:49.779628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.73
 ---- batch: 020 ----
mean loss: 117.52
 ---- batch: 030 ----
mean loss: 114.30
 ---- batch: 040 ----
mean loss: 110.93
train mean loss: 115.13
epoch train time: 0:00:01.675050
elapsed time: 0:06:38.399678
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 22:59:51.454843
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.96
 ---- batch: 020 ----
mean loss: 118.39
 ---- batch: 030 ----
mean loss: 116.89
 ---- batch: 040 ----
mean loss: 111.30
train mean loss: 115.54
epoch train time: 0:00:01.772557
elapsed time: 0:06:40.172401
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 22:59:53.227564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.36
 ---- batch: 020 ----
mean loss: 118.07
 ---- batch: 030 ----
mean loss: 111.91
 ---- batch: 040 ----
mean loss: 116.30
train mean loss: 115.67
epoch train time: 0:00:01.683138
elapsed time: 0:06:41.855697
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 22:59:54.910860
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.81
 ---- batch: 020 ----
mean loss: 113.59
 ---- batch: 030 ----
mean loss: 120.13
 ---- batch: 040 ----
mean loss: 116.49
train mean loss: 116.29
epoch train time: 0:00:01.673653
elapsed time: 0:06:43.529509
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 22:59:56.584654
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.12
 ---- batch: 020 ----
mean loss: 112.61
 ---- batch: 030 ----
mean loss: 112.34
 ---- batch: 040 ----
mean loss: 115.55
train mean loss: 115.26
epoch train time: 0:00:01.786847
elapsed time: 0:06:45.316546
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 22:59:58.371695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.30
 ---- batch: 020 ----
mean loss: 116.76
 ---- batch: 030 ----
mean loss: 112.70
 ---- batch: 040 ----
mean loss: 112.86
train mean loss: 114.33
epoch train time: 0:00:01.679632
elapsed time: 0:06:46.996360
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:00:00.051508
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.37
 ---- batch: 020 ----
mean loss: 113.85
 ---- batch: 030 ----
mean loss: 114.91
 ---- batch: 040 ----
mean loss: 116.01
train mean loss: 115.12
epoch train time: 0:00:01.793347
elapsed time: 0:06:48.789850
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:00:01.845035
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.87
 ---- batch: 020 ----
mean loss: 115.21
 ---- batch: 030 ----
mean loss: 113.13
 ---- batch: 040 ----
mean loss: 115.89
train mean loss: 114.45
epoch train time: 0:00:01.676492
elapsed time: 0:06:50.466543
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:00:03.521695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.61
 ---- batch: 020 ----
mean loss: 118.39
 ---- batch: 030 ----
mean loss: 116.91
 ---- batch: 040 ----
mean loss: 113.19
train mean loss: 114.95
epoch train time: 0:00:01.785206
elapsed time: 0:06:52.251905
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:00:05.307055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.32
 ---- batch: 020 ----
mean loss: 113.77
 ---- batch: 030 ----
mean loss: 113.50
 ---- batch: 040 ----
mean loss: 114.45
train mean loss: 114.21
epoch train time: 0:00:01.681756
elapsed time: 0:06:53.933822
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:00:06.988974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.71
 ---- batch: 020 ----
mean loss: 119.48
 ---- batch: 030 ----
mean loss: 117.38
 ---- batch: 040 ----
mean loss: 111.46
train mean loss: 115.41
epoch train time: 0:00:01.792472
elapsed time: 0:06:55.726438
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:00:08.781589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.08
 ---- batch: 020 ----
mean loss: 114.57
 ---- batch: 030 ----
mean loss: 114.81
 ---- batch: 040 ----
mean loss: 111.66
train mean loss: 113.78
epoch train time: 0:00:01.682509
elapsed time: 0:06:57.409114
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:00:10.464252
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.26
 ---- batch: 020 ----
mean loss: 117.94
 ---- batch: 030 ----
mean loss: 116.98
 ---- batch: 040 ----
mean loss: 113.44
train mean loss: 116.04
epoch train time: 0:00:01.752767
elapsed time: 0:06:59.162006
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:00:12.217170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.83
 ---- batch: 020 ----
mean loss: 119.92
 ---- batch: 030 ----
mean loss: 114.53
 ---- batch: 040 ----
mean loss: 117.72
train mean loss: 117.31
epoch train time: 0:00:01.703745
elapsed time: 0:07:00.865941
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:00:13.921104
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.78
 ---- batch: 020 ----
mean loss: 109.70
 ---- batch: 030 ----
mean loss: 113.58
 ---- batch: 040 ----
mean loss: 114.12
train mean loss: 114.73
epoch train time: 0:00:01.669395
elapsed time: 0:07:02.535505
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:00:15.590651
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.58
 ---- batch: 020 ----
mean loss: 113.23
 ---- batch: 030 ----
mean loss: 115.80
 ---- batch: 040 ----
mean loss: 111.79
train mean loss: 114.58
epoch train time: 0:00:01.779767
elapsed time: 0:07:04.315410
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:00:17.370561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.84
 ---- batch: 020 ----
mean loss: 110.05
 ---- batch: 030 ----
mean loss: 115.60
 ---- batch: 040 ----
mean loss: 113.07
train mean loss: 113.49
epoch train time: 0:00:01.682327
elapsed time: 0:07:05.997894
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:00:19.053046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.52
 ---- batch: 020 ----
mean loss: 112.62
 ---- batch: 030 ----
mean loss: 114.81
 ---- batch: 040 ----
mean loss: 113.92
train mean loss: 114.20
epoch train time: 0:00:01.779300
elapsed time: 0:07:07.777360
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:00:20.832539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.64
 ---- batch: 020 ----
mean loss: 116.46
 ---- batch: 030 ----
mean loss: 111.17
 ---- batch: 040 ----
mean loss: 117.13
train mean loss: 115.44
epoch train time: 0:00:01.684192
elapsed time: 0:07:09.461744
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:00:22.516908
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.60
 ---- batch: 020 ----
mean loss: 116.11
 ---- batch: 030 ----
mean loss: 112.80
 ---- batch: 040 ----
mean loss: 116.60
train mean loss: 114.39
epoch train time: 0:00:01.670621
elapsed time: 0:07:11.132514
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:00:24.187660
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.83
 ---- batch: 020 ----
mean loss: 115.25
 ---- batch: 030 ----
mean loss: 115.44
 ---- batch: 040 ----
mean loss: 114.46
train mean loss: 114.15
epoch train time: 0:00:01.780952
elapsed time: 0:07:12.913596
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:00:25.968742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.08
 ---- batch: 020 ----
mean loss: 113.54
 ---- batch: 030 ----
mean loss: 114.23
 ---- batch: 040 ----
mean loss: 115.87
train mean loss: 114.78
epoch train time: 0:00:01.678078
elapsed time: 0:07:14.591840
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:00:27.646992
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.34
 ---- batch: 020 ----
mean loss: 117.05
 ---- batch: 030 ----
mean loss: 117.10
 ---- batch: 040 ----
mean loss: 112.11
train mean loss: 115.78
epoch train time: 0:00:01.786754
elapsed time: 0:07:16.378745
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:00:29.433890
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.80
 ---- batch: 020 ----
mean loss: 118.34
 ---- batch: 030 ----
mean loss: 112.90
 ---- batch: 040 ----
mean loss: 111.79
train mean loss: 114.54
epoch train time: 0:00:01.678751
elapsed time: 0:07:18.057650
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:00:31.112796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.37
 ---- batch: 020 ----
mean loss: 115.78
 ---- batch: 030 ----
mean loss: 116.35
 ---- batch: 040 ----
mean loss: 114.68
train mean loss: 115.33
epoch train time: 0:00:01.785717
elapsed time: 0:07:19.843500
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:00:32.898678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.21
 ---- batch: 020 ----
mean loss: 115.18
 ---- batch: 030 ----
mean loss: 111.62
 ---- batch: 040 ----
mean loss: 114.65
train mean loss: 113.76
epoch train time: 0:00:01.673871
elapsed time: 0:07:21.517544
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:00:34.572694
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.32
 ---- batch: 020 ----
mean loss: 117.07
 ---- batch: 030 ----
mean loss: 115.74
 ---- batch: 040 ----
mean loss: 111.20
train mean loss: 114.42
epoch train time: 0:00:01.781191
elapsed time: 0:07:23.298869
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:00:36.354013
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.89
 ---- batch: 020 ----
mean loss: 111.78
 ---- batch: 030 ----
mean loss: 117.49
 ---- batch: 040 ----
mean loss: 113.89
train mean loss: 115.10
epoch train time: 0:00:01.681061
elapsed time: 0:07:24.983564
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_6/checkpoint.pth.tar
**** end time: 2019-09-20 23:00:38.038683 ****
