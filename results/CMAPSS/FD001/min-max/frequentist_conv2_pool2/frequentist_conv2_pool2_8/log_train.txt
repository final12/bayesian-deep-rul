Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_8', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31892
use_cuda: True
Dataset: CMAPSS/FD001
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 16:49:32.941068 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 16:49:32.946497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4122.23
 ---- batch: 020 ----
mean loss: 3956.93
 ---- batch: 030 ----
mean loss: 4003.58
train mean loss: 4021.69
epoch train time: 0:00:12.475789
elapsed time: 0:00:12.483151
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 16:49:45.424257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3937.36
 ---- batch: 020 ----
mean loss: 3848.26
 ---- batch: 030 ----
mean loss: 3815.05
train mean loss: 3858.10
epoch train time: 0:00:00.176048
elapsed time: 0:00:12.659327
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 16:49:45.600479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3710.35
 ---- batch: 020 ----
mean loss: 3623.70
 ---- batch: 030 ----
mean loss: 3563.11
train mean loss: 3608.74
epoch train time: 0:00:00.171281
elapsed time: 0:00:12.830780
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 16:49:45.771896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3412.51
 ---- batch: 020 ----
mean loss: 3315.98
 ---- batch: 030 ----
mean loss: 3281.45
train mean loss: 3327.16
epoch train time: 0:00:00.176422
elapsed time: 0:00:13.007362
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 16:49:45.948482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3110.89
 ---- batch: 020 ----
mean loss: 3036.44
 ---- batch: 030 ----
mean loss: 3087.07
train mean loss: 3062.21
epoch train time: 0:00:00.171109
elapsed time: 0:00:13.178610
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 16:49:46.119725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2907.21
 ---- batch: 020 ----
mean loss: 2828.36
 ---- batch: 030 ----
mean loss: 2801.46
train mean loss: 2832.39
epoch train time: 0:00:00.169667
elapsed time: 0:00:13.348420
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 16:49:46.289537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2717.35
 ---- batch: 020 ----
mean loss: 2632.67
 ---- batch: 030 ----
mean loss: 2572.57
train mean loss: 2627.83
epoch train time: 0:00:00.169351
elapsed time: 0:00:13.517932
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 16:49:46.459048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2504.97
 ---- batch: 020 ----
mean loss: 2476.38
 ---- batch: 030 ----
mean loss: 2381.96
train mean loss: 2443.31
epoch train time: 0:00:00.188803
elapsed time: 0:00:13.706892
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 16:49:46.648038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2331.08
 ---- batch: 020 ----
mean loss: 2287.69
 ---- batch: 030 ----
mean loss: 2250.79
train mean loss: 2279.04
epoch train time: 0:00:00.173248
elapsed time: 0:00:13.880334
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 16:49:46.821456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2194.95
 ---- batch: 020 ----
mean loss: 2131.02
 ---- batch: 030 ----
mean loss: 2101.31
train mean loss: 2128.84
epoch train time: 0:00:00.174898
elapsed time: 0:00:14.055377
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 16:49:46.996516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2027.07
 ---- batch: 020 ----
mean loss: 2015.51
 ---- batch: 030 ----
mean loss: 1957.03
train mean loss: 1989.41
epoch train time: 0:00:00.173485
elapsed time: 0:00:14.229025
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 16:49:47.170142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1900.86
 ---- batch: 020 ----
mean loss: 1883.98
 ---- batch: 030 ----
mean loss: 1815.94
train mean loss: 1864.74
epoch train time: 0:00:00.173338
elapsed time: 0:00:14.402508
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 16:49:47.343635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1795.78
 ---- batch: 020 ----
mean loss: 1742.70
 ---- batch: 030 ----
mean loss: 1741.66
train mean loss: 1746.53
epoch train time: 0:00:00.174410
elapsed time: 0:00:14.577070
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 16:49:47.518202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1705.52
 ---- batch: 020 ----
mean loss: 1653.50
 ---- batch: 030 ----
mean loss: 1596.34
train mean loss: 1638.12
epoch train time: 0:00:00.171897
elapsed time: 0:00:14.749136
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 16:49:47.690253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1568.63
 ---- batch: 020 ----
mean loss: 1548.62
 ---- batch: 030 ----
mean loss: 1520.10
train mean loss: 1539.28
epoch train time: 0:00:00.177668
elapsed time: 0:00:14.926969
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 16:49:47.868105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1471.50
 ---- batch: 020 ----
mean loss: 1446.56
 ---- batch: 030 ----
mean loss: 1435.41
train mean loss: 1446.02
epoch train time: 0:00:00.178466
elapsed time: 0:00:15.105615
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 16:49:48.046735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1394.05
 ---- batch: 020 ----
mean loss: 1359.22
 ---- batch: 030 ----
mean loss: 1342.22
train mean loss: 1359.64
epoch train time: 0:00:00.172524
elapsed time: 0:00:15.278294
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 16:49:48.219453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1310.60
 ---- batch: 020 ----
mean loss: 1288.64
 ---- batch: 030 ----
mean loss: 1246.36
train mean loss: 1281.20
epoch train time: 0:00:00.174199
elapsed time: 0:00:15.452678
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 16:49:48.393797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1237.82
 ---- batch: 020 ----
mean loss: 1216.79
 ---- batch: 030 ----
mean loss: 1184.70
train mean loss: 1207.33
epoch train time: 0:00:00.173997
elapsed time: 0:00:15.626843
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 16:49:48.567963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1167.48
 ---- batch: 020 ----
mean loss: 1140.18
 ---- batch: 030 ----
mean loss: 1122.16
train mean loss: 1139.03
epoch train time: 0:00:00.175914
elapsed time: 0:00:15.802936
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 16:49:48.744119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1087.00
 ---- batch: 020 ----
mean loss: 1080.20
 ---- batch: 030 ----
mean loss: 1062.41
train mean loss: 1077.18
epoch train time: 0:00:00.174909
elapsed time: 0:00:15.978069
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 16:49:48.919203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1031.74
 ---- batch: 020 ----
mean loss: 1042.80
 ---- batch: 030 ----
mean loss: 1007.90
train mean loss: 1018.95
epoch train time: 0:00:00.177675
elapsed time: 0:00:16.155924
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 16:49:49.097043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 992.80
 ---- batch: 020 ----
mean loss: 968.18
 ---- batch: 030 ----
mean loss: 958.08
train mean loss: 965.48
epoch train time: 0:00:00.175172
elapsed time: 0:00:16.331237
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 16:49:49.272364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.47
 ---- batch: 020 ----
mean loss: 932.54
 ---- batch: 030 ----
mean loss: 916.01
train mean loss: 915.66
epoch train time: 0:00:00.178798
elapsed time: 0:00:16.510201
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 16:49:49.451384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.79
 ---- batch: 020 ----
mean loss: 886.44
 ---- batch: 030 ----
mean loss: 853.91
train mean loss: 870.50
epoch train time: 0:00:00.174911
elapsed time: 0:00:16.685318
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 16:49:49.626509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.21
 ---- batch: 020 ----
mean loss: 846.81
 ---- batch: 030 ----
mean loss: 825.19
train mean loss: 829.64
epoch train time: 0:00:00.179031
elapsed time: 0:00:16.864597
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 16:49:49.805726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.16
 ---- batch: 020 ----
mean loss: 797.78
 ---- batch: 030 ----
mean loss: 768.63
train mean loss: 791.86
epoch train time: 0:00:00.182338
elapsed time: 0:00:17.047089
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 16:49:49.988206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 771.68
 ---- batch: 020 ----
mean loss: 758.82
 ---- batch: 030 ----
mean loss: 741.02
train mean loss: 755.56
epoch train time: 0:00:00.174336
elapsed time: 0:00:17.221566
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 16:49:50.162683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 727.75
 ---- batch: 020 ----
mean loss: 739.97
 ---- batch: 030 ----
mean loss: 715.56
train mean loss: 722.70
epoch train time: 0:00:00.170949
elapsed time: 0:00:17.392655
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 16:49:50.333772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.71
 ---- batch: 020 ----
mean loss: 692.74
 ---- batch: 030 ----
mean loss: 687.24
train mean loss: 693.75
epoch train time: 0:00:00.176940
elapsed time: 0:00:17.569735
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 16:49:50.510853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.80
 ---- batch: 020 ----
mean loss: 659.82
 ---- batch: 030 ----
mean loss: 674.80
train mean loss: 666.77
epoch train time: 0:00:00.172185
elapsed time: 0:00:17.742061
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 16:49:50.683177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 651.49
 ---- batch: 020 ----
mean loss: 647.62
 ---- batch: 030 ----
mean loss: 632.22
train mean loss: 640.74
epoch train time: 0:00:00.175164
elapsed time: 0:00:17.917374
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 16:49:50.858519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 629.56
 ---- batch: 020 ----
mean loss: 616.97
 ---- batch: 030 ----
mean loss: 604.60
train mean loss: 618.40
epoch train time: 0:00:00.173446
elapsed time: 0:00:18.090987
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 16:49:51.032104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 605.13
 ---- batch: 020 ----
mean loss: 606.03
 ---- batch: 030 ----
mean loss: 587.58
train mean loss: 597.66
epoch train time: 0:00:00.170779
elapsed time: 0:00:18.261903
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 16:49:51.203020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 579.68
 ---- batch: 020 ----
mean loss: 586.70
 ---- batch: 030 ----
mean loss: 572.09
train mean loss: 578.41
epoch train time: 0:00:00.170915
elapsed time: 0:00:18.432960
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 16:49:51.374077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 563.23
 ---- batch: 020 ----
mean loss: 558.47
 ---- batch: 030 ----
mean loss: 567.18
train mean loss: 561.38
epoch train time: 0:00:00.172832
elapsed time: 0:00:18.605931
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 16:49:51.547047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 550.91
 ---- batch: 020 ----
mean loss: 547.84
 ---- batch: 030 ----
mean loss: 541.70
train mean loss: 544.41
epoch train time: 0:00:00.176944
elapsed time: 0:00:18.783042
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 16:49:51.724187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 536.58
 ---- batch: 020 ----
mean loss: 526.53
 ---- batch: 030 ----
mean loss: 530.91
train mean loss: 529.98
epoch train time: 0:00:00.180292
elapsed time: 0:00:18.963504
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 16:49:51.904624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.27
 ---- batch: 020 ----
mean loss: 517.77
 ---- batch: 030 ----
mean loss: 511.39
train mean loss: 516.89
epoch train time: 0:00:00.176197
elapsed time: 0:00:19.139853
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 16:49:52.080974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.55
 ---- batch: 020 ----
mean loss: 503.98
 ---- batch: 030 ----
mean loss: 504.65
train mean loss: 504.40
epoch train time: 0:00:00.172227
elapsed time: 0:00:19.312219
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 16:49:52.253384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.41
 ---- batch: 020 ----
mean loss: 490.44
 ---- batch: 030 ----
mean loss: 491.52
train mean loss: 493.03
epoch train time: 0:00:00.172204
elapsed time: 0:00:19.484674
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 16:49:52.425792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.59
 ---- batch: 020 ----
mean loss: 476.20
 ---- batch: 030 ----
mean loss: 483.98
train mean loss: 482.43
epoch train time: 0:00:00.173609
elapsed time: 0:00:19.658436
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 16:49:52.599568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.75
 ---- batch: 020 ----
mean loss: 471.18
 ---- batch: 030 ----
mean loss: 468.78
train mean loss: 472.70
epoch train time: 0:00:00.173800
elapsed time: 0:00:19.832389
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 16:49:52.773505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.21
 ---- batch: 020 ----
mean loss: 472.44
 ---- batch: 030 ----
mean loss: 462.96
train mean loss: 464.25
epoch train time: 0:00:00.175381
elapsed time: 0:00:20.007924
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 16:49:52.949040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.12
 ---- batch: 020 ----
mean loss: 451.63
 ---- batch: 030 ----
mean loss: 458.92
train mean loss: 455.89
epoch train time: 0:00:00.172801
elapsed time: 0:00:20.180866
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 16:49:53.121983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.66
 ---- batch: 020 ----
mean loss: 446.83
 ---- batch: 030 ----
mean loss: 444.88
train mean loss: 448.68
epoch train time: 0:00:00.174274
elapsed time: 0:00:20.355278
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 16:49:53.296395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.38
 ---- batch: 020 ----
mean loss: 442.10
 ---- batch: 030 ----
mean loss: 440.82
train mean loss: 441.10
epoch train time: 0:00:00.172097
elapsed time: 0:00:20.527620
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 16:49:53.468751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.95
 ---- batch: 020 ----
mean loss: 439.93
 ---- batch: 030 ----
mean loss: 431.93
train mean loss: 434.71
epoch train time: 0:00:00.172770
elapsed time: 0:00:20.700562
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 16:49:53.641682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.51
 ---- batch: 020 ----
mean loss: 428.79
 ---- batch: 030 ----
mean loss: 422.82
train mean loss: 428.29
epoch train time: 0:00:00.172820
elapsed time: 0:00:20.873542
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 16:49:53.814660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.12
 ---- batch: 020 ----
mean loss: 422.80
 ---- batch: 030 ----
mean loss: 419.47
train mean loss: 423.16
epoch train time: 0:00:00.174765
elapsed time: 0:00:21.048461
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 16:49:53.989587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.62
 ---- batch: 020 ----
mean loss: 416.33
 ---- batch: 030 ----
mean loss: 419.74
train mean loss: 417.65
epoch train time: 0:00:00.173454
elapsed time: 0:00:21.222067
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 16:49:54.163233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.99
 ---- batch: 020 ----
mean loss: 405.10
 ---- batch: 030 ----
mean loss: 417.10
train mean loss: 413.02
epoch train time: 0:00:00.172730
elapsed time: 0:00:21.394983
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 16:49:54.336099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.32
 ---- batch: 020 ----
mean loss: 412.30
 ---- batch: 030 ----
mean loss: 400.59
train mean loss: 408.30
epoch train time: 0:00:00.176850
elapsed time: 0:00:21.571973
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 16:49:54.513089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.84
 ---- batch: 020 ----
mean loss: 402.17
 ---- batch: 030 ----
mean loss: 406.80
train mean loss: 404.49
epoch train time: 0:00:00.171903
elapsed time: 0:00:21.744033
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 16:49:54.685149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.99
 ---- batch: 020 ----
mean loss: 404.01
 ---- batch: 030 ----
mean loss: 394.54
train mean loss: 400.87
epoch train time: 0:00:00.177239
elapsed time: 0:00:21.921421
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 16:49:54.862547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.62
 ---- batch: 020 ----
mean loss: 396.54
 ---- batch: 030 ----
mean loss: 399.86
train mean loss: 397.60
epoch train time: 0:00:00.173728
elapsed time: 0:00:22.095295
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 16:49:55.036420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.30
 ---- batch: 020 ----
mean loss: 402.24
 ---- batch: 030 ----
mean loss: 391.59
train mean loss: 395.09
epoch train time: 0:00:00.177386
elapsed time: 0:00:22.272834
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 16:49:55.213953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.08
 ---- batch: 020 ----
mean loss: 392.17
 ---- batch: 030 ----
mean loss: 392.71
train mean loss: 392.16
epoch train time: 0:00:00.174412
elapsed time: 0:00:22.447386
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 16:49:55.388504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.84
 ---- batch: 020 ----
mean loss: 399.29
 ---- batch: 030 ----
mean loss: 390.13
train mean loss: 389.83
epoch train time: 0:00:00.177169
elapsed time: 0:00:22.624698
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 16:49:55.565815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.92
 ---- batch: 020 ----
mean loss: 390.69
 ---- batch: 030 ----
mean loss: 382.62
train mean loss: 387.87
epoch train time: 0:00:00.173532
elapsed time: 0:00:22.798379
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 16:49:55.739499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.70
 ---- batch: 020 ----
mean loss: 379.33
 ---- batch: 030 ----
mean loss: 382.91
train mean loss: 386.50
epoch train time: 0:00:00.174758
elapsed time: 0:00:22.973277
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 16:49:55.914394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.65
 ---- batch: 020 ----
mean loss: 383.12
 ---- batch: 030 ----
mean loss: 390.68
train mean loss: 384.53
epoch train time: 0:00:00.172020
elapsed time: 0:00:23.145434
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 16:49:56.086882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.42
 ---- batch: 020 ----
mean loss: 386.40
 ---- batch: 030 ----
mean loss: 379.35
train mean loss: 383.07
epoch train time: 0:00:00.169067
elapsed time: 0:00:23.314973
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 16:49:56.256096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.17
 ---- batch: 020 ----
mean loss: 383.41
 ---- batch: 030 ----
mean loss: 380.37
train mean loss: 381.40
epoch train time: 0:00:00.168520
elapsed time: 0:00:23.483635
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 16:49:56.424768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.91
 ---- batch: 020 ----
mean loss: 376.70
 ---- batch: 030 ----
mean loss: 379.80
train mean loss: 380.50
epoch train time: 0:00:00.169338
elapsed time: 0:00:23.653136
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 16:49:56.594255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.74
 ---- batch: 020 ----
mean loss: 374.90
 ---- batch: 030 ----
mean loss: 377.81
train mean loss: 379.34
epoch train time: 0:00:00.176483
elapsed time: 0:00:23.829758
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 16:49:56.770875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.62
 ---- batch: 020 ----
mean loss: 375.15
 ---- batch: 030 ----
mean loss: 375.22
train mean loss: 378.06
epoch train time: 0:00:00.180669
elapsed time: 0:00:24.010577
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 16:49:56.951697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.12
 ---- batch: 020 ----
mean loss: 373.84
 ---- batch: 030 ----
mean loss: 378.49
train mean loss: 377.41
epoch train time: 0:00:00.174070
elapsed time: 0:00:24.184808
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 16:49:57.125931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.21
 ---- batch: 020 ----
mean loss: 379.64
 ---- batch: 030 ----
mean loss: 375.95
train mean loss: 376.06
epoch train time: 0:00:00.170297
elapsed time: 0:00:24.355257
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 16:49:57.296374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.56
 ---- batch: 020 ----
mean loss: 375.67
 ---- batch: 030 ----
mean loss: 375.27
train mean loss: 375.63
epoch train time: 0:00:00.172209
elapsed time: 0:00:24.527604
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 16:49:57.468720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.78
 ---- batch: 020 ----
mean loss: 366.57
 ---- batch: 030 ----
mean loss: 379.76
train mean loss: 374.53
epoch train time: 0:00:00.171744
elapsed time: 0:00:24.699492
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 16:49:57.640609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.39
 ---- batch: 020 ----
mean loss: 372.57
 ---- batch: 030 ----
mean loss: 368.21
train mean loss: 374.24
epoch train time: 0:00:00.179375
elapsed time: 0:00:24.879015
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 16:49:57.820156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.10
 ---- batch: 020 ----
mean loss: 374.57
 ---- batch: 030 ----
mean loss: 370.05
train mean loss: 373.25
epoch train time: 0:00:00.179471
elapsed time: 0:00:25.058655
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 16:49:57.999771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.70
 ---- batch: 020 ----
mean loss: 380.65
 ---- batch: 030 ----
mean loss: 369.19
train mean loss: 372.68
epoch train time: 0:00:00.173625
elapsed time: 0:00:25.232417
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 16:49:58.173533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.71
 ---- batch: 020 ----
mean loss: 368.52
 ---- batch: 030 ----
mean loss: 375.13
train mean loss: 372.46
epoch train time: 0:00:00.170381
elapsed time: 0:00:25.402936
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 16:49:58.344052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.43
 ---- batch: 020 ----
mean loss: 375.77
 ---- batch: 030 ----
mean loss: 372.70
train mean loss: 371.95
epoch train time: 0:00:00.168857
elapsed time: 0:00:25.571945
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 16:49:58.513062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.84
 ---- batch: 020 ----
mean loss: 366.92
 ---- batch: 030 ----
mean loss: 371.86
train mean loss: 371.68
epoch train time: 0:00:00.171539
elapsed time: 0:00:25.743656
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 16:49:58.684773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.44
 ---- batch: 020 ----
mean loss: 374.21
 ---- batch: 030 ----
mean loss: 374.35
train mean loss: 370.90
epoch train time: 0:00:00.173602
elapsed time: 0:00:25.917427
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 16:49:58.858579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.25
 ---- batch: 020 ----
mean loss: 364.69
 ---- batch: 030 ----
mean loss: 366.71
train mean loss: 370.94
epoch train time: 0:00:00.173472
elapsed time: 0:00:26.091082
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 16:49:59.032199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.25
 ---- batch: 020 ----
mean loss: 370.45
 ---- batch: 030 ----
mean loss: 373.44
train mean loss: 370.55
epoch train time: 0:00:00.174417
elapsed time: 0:00:26.265639
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 16:49:59.206754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.56
 ---- batch: 020 ----
mean loss: 369.01
 ---- batch: 030 ----
mean loss: 381.14
train mean loss: 369.66
epoch train time: 0:00:00.167664
elapsed time: 0:00:26.433463
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 16:49:59.374582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.55
 ---- batch: 020 ----
mean loss: 371.51
 ---- batch: 030 ----
mean loss: 369.30
train mean loss: 369.58
epoch train time: 0:00:00.169253
elapsed time: 0:00:26.602857
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 16:49:59.543974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.70
 ---- batch: 020 ----
mean loss: 363.54
 ---- batch: 030 ----
mean loss: 370.68
train mean loss: 369.40
epoch train time: 0:00:00.176743
elapsed time: 0:00:26.779742
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 16:49:59.720877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.23
 ---- batch: 020 ----
mean loss: 372.36
 ---- batch: 030 ----
mean loss: 367.07
train mean loss: 368.89
epoch train time: 0:00:00.181245
elapsed time: 0:00:26.961167
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 16:49:59.902300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.90
 ---- batch: 020 ----
mean loss: 364.56
 ---- batch: 030 ----
mean loss: 368.80
train mean loss: 368.93
epoch train time: 0:00:00.173407
elapsed time: 0:00:27.134776
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 16:50:00.075924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.13
 ---- batch: 020 ----
mean loss: 373.87
 ---- batch: 030 ----
mean loss: 367.14
train mean loss: 368.23
epoch train time: 0:00:00.175168
elapsed time: 0:00:27.310131
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 16:50:00.251248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.48
 ---- batch: 020 ----
mean loss: 369.06
 ---- batch: 030 ----
mean loss: 364.78
train mean loss: 368.01
epoch train time: 0:00:00.172690
elapsed time: 0:00:27.482961
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 16:50:00.424077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.83
 ---- batch: 020 ----
mean loss: 378.07
 ---- batch: 030 ----
mean loss: 359.72
train mean loss: 367.73
epoch train time: 0:00:00.177274
elapsed time: 0:00:27.660372
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 16:50:00.601491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.23
 ---- batch: 020 ----
mean loss: 363.33
 ---- batch: 030 ----
mean loss: 362.67
train mean loss: 367.66
epoch train time: 0:00:00.176072
elapsed time: 0:00:27.836612
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 16:50:00.777734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.96
 ---- batch: 020 ----
mean loss: 363.74
 ---- batch: 030 ----
mean loss: 371.15
train mean loss: 367.12
epoch train time: 0:00:00.184139
elapsed time: 0:00:28.020896
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 16:50:00.962014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.86
 ---- batch: 020 ----
mean loss: 369.72
 ---- batch: 030 ----
mean loss: 370.32
train mean loss: 367.11
epoch train time: 0:00:00.173680
elapsed time: 0:00:28.194715
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 16:50:01.135852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.51
 ---- batch: 020 ----
mean loss: 366.57
 ---- batch: 030 ----
mean loss: 369.70
train mean loss: 366.53
epoch train time: 0:00:00.172819
elapsed time: 0:00:28.367694
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 16:50:01.308812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.69
 ---- batch: 020 ----
mean loss: 366.87
 ---- batch: 030 ----
mean loss: 363.40
train mean loss: 366.49
epoch train time: 0:00:00.175668
elapsed time: 0:00:28.543500
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 16:50:01.484617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.18
 ---- batch: 020 ----
mean loss: 364.81
 ---- batch: 030 ----
mean loss: 367.30
train mean loss: 366.38
epoch train time: 0:00:00.172163
elapsed time: 0:00:28.715803
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 16:50:01.656933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.85
 ---- batch: 020 ----
mean loss: 360.30
 ---- batch: 030 ----
mean loss: 373.79
train mean loss: 366.40
epoch train time: 0:00:00.173180
elapsed time: 0:00:28.889137
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 16:50:01.830255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.94
 ---- batch: 020 ----
mean loss: 363.20
 ---- batch: 030 ----
mean loss: 371.20
train mean loss: 365.62
epoch train time: 0:00:00.172402
elapsed time: 0:00:29.061678
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 16:50:02.002797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.46
 ---- batch: 020 ----
mean loss: 365.02
 ---- batch: 030 ----
mean loss: 370.17
train mean loss: 365.55
epoch train time: 0:00:00.172056
elapsed time: 0:00:29.233878
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 16:50:02.174996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.75
 ---- batch: 020 ----
mean loss: 363.15
 ---- batch: 030 ----
mean loss: 361.94
train mean loss: 365.53
epoch train time: 0:00:00.172252
elapsed time: 0:00:29.406271
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 16:50:02.347390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.41
 ---- batch: 020 ----
mean loss: 363.99
 ---- batch: 030 ----
mean loss: 362.52
train mean loss: 365.56
epoch train time: 0:00:00.172615
elapsed time: 0:00:29.579026
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 16:50:02.520146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.10
 ---- batch: 020 ----
mean loss: 361.53
 ---- batch: 030 ----
mean loss: 367.05
train mean loss: 364.84
epoch train time: 0:00:00.173954
elapsed time: 0:00:29.753127
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 16:50:02.694246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.56
 ---- batch: 020 ----
mean loss: 367.70
 ---- batch: 030 ----
mean loss: 359.45
train mean loss: 365.21
epoch train time: 0:00:00.196699
elapsed time: 0:00:29.949970
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 16:50:02.891090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.76
 ---- batch: 020 ----
mean loss: 369.55
 ---- batch: 030 ----
mean loss: 365.03
train mean loss: 364.70
epoch train time: 0:00:00.176153
elapsed time: 0:00:30.126281
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 16:50:03.067436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.82
 ---- batch: 020 ----
mean loss: 365.21
 ---- batch: 030 ----
mean loss: 364.90
train mean loss: 364.14
epoch train time: 0:00:00.174372
elapsed time: 0:00:30.300855
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 16:50:03.241973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.30
 ---- batch: 020 ----
mean loss: 369.94
 ---- batch: 030 ----
mean loss: 364.85
train mean loss: 364.39
epoch train time: 0:00:00.172407
elapsed time: 0:00:30.473416
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 16:50:03.414540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.38
 ---- batch: 020 ----
mean loss: 370.54
 ---- batch: 030 ----
mean loss: 366.11
train mean loss: 363.94
epoch train time: 0:00:00.170882
elapsed time: 0:00:30.644443
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 16:50:03.585577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.95
 ---- batch: 020 ----
mean loss: 364.46
 ---- batch: 030 ----
mean loss: 369.95
train mean loss: 363.93
epoch train time: 0:00:00.172743
elapsed time: 0:00:30.817338
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 16:50:03.758455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.40
 ---- batch: 020 ----
mean loss: 366.39
 ---- batch: 030 ----
mean loss: 365.85
train mean loss: 363.76
epoch train time: 0:00:00.177918
elapsed time: 0:00:30.995406
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 16:50:03.936514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.75
 ---- batch: 020 ----
mean loss: 356.02
 ---- batch: 030 ----
mean loss: 367.44
train mean loss: 363.53
epoch train time: 0:00:00.171965
elapsed time: 0:00:31.167499
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 16:50:04.108615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.37
 ---- batch: 020 ----
mean loss: 370.35
 ---- batch: 030 ----
mean loss: 358.73
train mean loss: 363.60
epoch train time: 0:00:00.172574
elapsed time: 0:00:31.340211
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 16:50:04.281327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.41
 ---- batch: 020 ----
mean loss: 357.34
 ---- batch: 030 ----
mean loss: 365.54
train mean loss: 363.39
epoch train time: 0:00:00.174071
elapsed time: 0:00:31.514436
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 16:50:04.455553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.57
 ---- batch: 020 ----
mean loss: 362.35
 ---- batch: 030 ----
mean loss: 363.79
train mean loss: 362.63
epoch train time: 0:00:00.171170
elapsed time: 0:00:31.685741
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 16:50:04.626857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.24
 ---- batch: 020 ----
mean loss: 361.93
 ---- batch: 030 ----
mean loss: 365.61
train mean loss: 362.68
epoch train time: 0:00:00.175170
elapsed time: 0:00:31.861051
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 16:50:04.802172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.73
 ---- batch: 020 ----
mean loss: 359.58
 ---- batch: 030 ----
mean loss: 361.21
train mean loss: 363.15
epoch train time: 0:00:00.173867
elapsed time: 0:00:32.035060
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 16:50:04.976201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.86
 ---- batch: 020 ----
mean loss: 361.88
 ---- batch: 030 ----
mean loss: 366.15
train mean loss: 362.34
epoch train time: 0:00:00.170443
elapsed time: 0:00:32.205680
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 16:50:05.146795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.00
 ---- batch: 020 ----
mean loss: 364.99
 ---- batch: 030 ----
mean loss: 370.31
train mean loss: 362.30
epoch train time: 0:00:00.171495
elapsed time: 0:00:32.377311
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 16:50:05.318427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.11
 ---- batch: 020 ----
mean loss: 363.23
 ---- batch: 030 ----
mean loss: 360.78
train mean loss: 362.60
epoch train time: 0:00:00.171481
elapsed time: 0:00:32.549619
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 16:50:05.490750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.73
 ---- batch: 020 ----
mean loss: 362.99
 ---- batch: 030 ----
mean loss: 363.56
train mean loss: 361.73
epoch train time: 0:00:00.171824
elapsed time: 0:00:32.721592
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 16:50:05.662708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.44
 ---- batch: 020 ----
mean loss: 364.65
 ---- batch: 030 ----
mean loss: 360.25
train mean loss: 361.26
epoch train time: 0:00:00.175163
elapsed time: 0:00:32.896911
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 16:50:05.838064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.74
 ---- batch: 020 ----
mean loss: 365.29
 ---- batch: 030 ----
mean loss: 360.73
train mean loss: 361.75
epoch train time: 0:00:00.174396
elapsed time: 0:00:33.071482
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 16:50:06.012599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.83
 ---- batch: 020 ----
mean loss: 358.87
 ---- batch: 030 ----
mean loss: 360.05
train mean loss: 361.46
epoch train time: 0:00:00.172913
elapsed time: 0:00:33.244533
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 16:50:06.185673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.90
 ---- batch: 020 ----
mean loss: 358.14
 ---- batch: 030 ----
mean loss: 359.05
train mean loss: 361.41
epoch train time: 0:00:00.172687
elapsed time: 0:00:33.417382
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 16:50:06.358514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.86
 ---- batch: 020 ----
mean loss: 358.94
 ---- batch: 030 ----
mean loss: 368.01
train mean loss: 360.94
epoch train time: 0:00:00.171415
elapsed time: 0:00:33.588959
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 16:50:06.530080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.43
 ---- batch: 020 ----
mean loss: 359.27
 ---- batch: 030 ----
mean loss: 364.84
train mean loss: 361.07
epoch train time: 0:00:00.177376
elapsed time: 0:00:33.766493
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 16:50:06.707619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.20
 ---- batch: 020 ----
mean loss: 362.71
 ---- batch: 030 ----
mean loss: 359.45
train mean loss: 360.65
epoch train time: 0:00:00.189237
elapsed time: 0:00:33.955907
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 16:50:06.897026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.81
 ---- batch: 020 ----
mean loss: 355.12
 ---- batch: 030 ----
mean loss: 363.60
train mean loss: 360.37
epoch train time: 0:00:00.171552
elapsed time: 0:00:34.127599
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 16:50:07.068716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.23
 ---- batch: 020 ----
mean loss: 358.43
 ---- batch: 030 ----
mean loss: 352.19
train mean loss: 360.76
epoch train time: 0:00:00.171826
elapsed time: 0:00:34.299562
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 16:50:07.240680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.27
 ---- batch: 020 ----
mean loss: 366.31
 ---- batch: 030 ----
mean loss: 360.96
train mean loss: 360.23
epoch train time: 0:00:00.170757
elapsed time: 0:00:34.470471
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 16:50:07.411577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.28
 ---- batch: 020 ----
mean loss: 359.09
 ---- batch: 030 ----
mean loss: 364.69
train mean loss: 360.17
epoch train time: 0:00:00.174264
elapsed time: 0:00:34.644874
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 16:50:07.585993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.18
 ---- batch: 020 ----
mean loss: 362.95
 ---- batch: 030 ----
mean loss: 361.18
train mean loss: 359.77
epoch train time: 0:00:00.172450
elapsed time: 0:00:34.817461
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 16:50:07.758579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.46
 ---- batch: 020 ----
mean loss: 361.49
 ---- batch: 030 ----
mean loss: 361.78
train mean loss: 359.62
epoch train time: 0:00:00.174800
elapsed time: 0:00:34.992410
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 16:50:07.933545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.45
 ---- batch: 020 ----
mean loss: 359.47
 ---- batch: 030 ----
mean loss: 370.13
train mean loss: 359.94
epoch train time: 0:00:00.173206
elapsed time: 0:00:35.165787
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 16:50:08.106915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.80
 ---- batch: 020 ----
mean loss: 371.95
 ---- batch: 030 ----
mean loss: 352.75
train mean loss: 359.53
epoch train time: 0:00:00.177690
elapsed time: 0:00:35.343626
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 16:50:08.284763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.92
 ---- batch: 020 ----
mean loss: 363.66
 ---- batch: 030 ----
mean loss: 366.94
train mean loss: 359.40
epoch train time: 0:00:00.174083
elapsed time: 0:00:35.517916
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 16:50:08.459034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.42
 ---- batch: 020 ----
mean loss: 357.35
 ---- batch: 030 ----
mean loss: 352.61
train mean loss: 359.19
epoch train time: 0:00:00.172634
elapsed time: 0:00:35.690716
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 16:50:08.631833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.11
 ---- batch: 020 ----
mean loss: 355.95
 ---- batch: 030 ----
mean loss: 365.52
train mean loss: 358.89
epoch train time: 0:00:00.185558
elapsed time: 0:00:35.876415
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 16:50:08.817534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.19
 ---- batch: 020 ----
mean loss: 361.31
 ---- batch: 030 ----
mean loss: 366.40
train mean loss: 359.07
epoch train time: 0:00:00.170613
elapsed time: 0:00:36.047167
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 16:50:08.988298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.47
 ---- batch: 020 ----
mean loss: 354.71
 ---- batch: 030 ----
mean loss: 354.70
train mean loss: 358.90
epoch train time: 0:00:00.173936
elapsed time: 0:00:36.221297
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 16:50:09.162423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.94
 ---- batch: 020 ----
mean loss: 357.20
 ---- batch: 030 ----
mean loss: 360.46
train mean loss: 358.67
epoch train time: 0:00:00.170448
elapsed time: 0:00:36.391889
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 16:50:09.333005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.85
 ---- batch: 020 ----
mean loss: 355.36
 ---- batch: 030 ----
mean loss: 361.69
train mean loss: 358.14
epoch train time: 0:00:00.170324
elapsed time: 0:00:36.562364
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 16:50:09.503478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.33
 ---- batch: 020 ----
mean loss: 355.13
 ---- batch: 030 ----
mean loss: 351.43
train mean loss: 358.43
epoch train time: 0:00:00.169956
elapsed time: 0:00:36.732467
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 16:50:09.673593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.62
 ---- batch: 020 ----
mean loss: 360.50
 ---- batch: 030 ----
mean loss: 353.28
train mean loss: 358.46
epoch train time: 0:00:00.176411
elapsed time: 0:00:36.909035
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 16:50:09.850155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.29
 ---- batch: 020 ----
mean loss: 356.71
 ---- batch: 030 ----
mean loss: 357.63
train mean loss: 358.32
epoch train time: 0:00:00.181238
elapsed time: 0:00:37.090415
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 16:50:10.031546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.71
 ---- batch: 020 ----
mean loss: 362.92
 ---- batch: 030 ----
mean loss: 357.36
train mean loss: 357.91
epoch train time: 0:00:00.172352
elapsed time: 0:00:37.262921
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 16:50:10.204040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.22
 ---- batch: 020 ----
mean loss: 364.94
 ---- batch: 030 ----
mean loss: 353.60
train mean loss: 357.67
epoch train time: 0:00:00.171735
elapsed time: 0:00:37.434805
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 16:50:10.375932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.45
 ---- batch: 020 ----
mean loss: 357.60
 ---- batch: 030 ----
mean loss: 357.47
train mean loss: 357.42
epoch train time: 0:00:00.174398
elapsed time: 0:00:37.609361
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 16:50:10.550494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.94
 ---- batch: 020 ----
mean loss: 359.19
 ---- batch: 030 ----
mean loss: 355.10
train mean loss: 357.47
epoch train time: 0:00:00.172449
elapsed time: 0:00:37.781964
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 16:50:10.723080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.34
 ---- batch: 020 ----
mean loss: 358.22
 ---- batch: 030 ----
mean loss: 358.44
train mean loss: 357.14
epoch train time: 0:00:00.181405
elapsed time: 0:00:37.963505
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 16:50:10.904637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.18
 ---- batch: 020 ----
mean loss: 352.82
 ---- batch: 030 ----
mean loss: 357.00
train mean loss: 356.89
epoch train time: 0:00:00.172790
elapsed time: 0:00:38.136462
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 16:50:11.077579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.12
 ---- batch: 020 ----
mean loss: 353.67
 ---- batch: 030 ----
mean loss: 360.98
train mean loss: 356.85
epoch train time: 0:00:00.172991
elapsed time: 0:00:38.309606
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 16:50:11.250713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.99
 ---- batch: 020 ----
mean loss: 357.11
 ---- batch: 030 ----
mean loss: 363.42
train mean loss: 356.68
epoch train time: 0:00:00.169568
elapsed time: 0:00:38.479303
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 16:50:11.420420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.15
 ---- batch: 020 ----
mean loss: 361.92
 ---- batch: 030 ----
mean loss: 350.81
train mean loss: 356.61
epoch train time: 0:00:00.173285
elapsed time: 0:00:38.652728
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 16:50:11.593844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.34
 ---- batch: 020 ----
mean loss: 363.37
 ---- batch: 030 ----
mean loss: 353.00
train mean loss: 356.48
epoch train time: 0:00:00.173592
elapsed time: 0:00:38.826459
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 16:50:11.767575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.81
 ---- batch: 020 ----
mean loss: 354.77
 ---- batch: 030 ----
mean loss: 353.56
train mean loss: 356.74
epoch train time: 0:00:00.177047
elapsed time: 0:00:39.003642
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 16:50:11.944758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.87
 ---- batch: 020 ----
mean loss: 355.14
 ---- batch: 030 ----
mean loss: 360.71
train mean loss: 356.13
epoch train time: 0:00:00.170756
elapsed time: 0:00:39.174555
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 16:50:12.115681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.67
 ---- batch: 020 ----
mean loss: 358.55
 ---- batch: 030 ----
mean loss: 359.93
train mean loss: 355.94
epoch train time: 0:00:00.172671
elapsed time: 0:00:39.347394
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 16:50:12.288512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.95
 ---- batch: 020 ----
mean loss: 353.31
 ---- batch: 030 ----
mean loss: 353.63
train mean loss: 356.20
epoch train time: 0:00:00.170818
elapsed time: 0:00:39.518351
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 16:50:12.459467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.18
 ---- batch: 020 ----
mean loss: 356.30
 ---- batch: 030 ----
mean loss: 351.95
train mean loss: 355.73
epoch train time: 0:00:00.183698
elapsed time: 0:00:39.702187
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 16:50:12.643323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.96
 ---- batch: 020 ----
mean loss: 360.17
 ---- batch: 030 ----
mean loss: 357.58
train mean loss: 355.73
epoch train time: 0:00:00.177598
elapsed time: 0:00:39.879955
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 16:50:12.821073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.22
 ---- batch: 020 ----
mean loss: 350.96
 ---- batch: 030 ----
mean loss: 355.85
train mean loss: 355.39
epoch train time: 0:00:00.173159
elapsed time: 0:00:40.053250
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 16:50:12.994385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.09
 ---- batch: 020 ----
mean loss: 358.67
 ---- batch: 030 ----
mean loss: 352.05
train mean loss: 355.45
epoch train time: 0:00:00.169405
elapsed time: 0:00:40.222826
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 16:50:13.163941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.18
 ---- batch: 020 ----
mean loss: 365.40
 ---- batch: 030 ----
mean loss: 348.10
train mean loss: 354.68
epoch train time: 0:00:00.172646
elapsed time: 0:00:40.395621
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 16:50:13.336736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.36
 ---- batch: 020 ----
mean loss: 351.48
 ---- batch: 030 ----
mean loss: 359.56
train mean loss: 354.88
epoch train time: 0:00:00.169575
elapsed time: 0:00:40.565345
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 16:50:13.506503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.71
 ---- batch: 020 ----
mean loss: 358.25
 ---- batch: 030 ----
mean loss: 358.23
train mean loss: 355.03
epoch train time: 0:00:00.169432
elapsed time: 0:00:40.734953
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 16:50:13.676068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.57
 ---- batch: 020 ----
mean loss: 357.13
 ---- batch: 030 ----
mean loss: 354.22
train mean loss: 354.89
epoch train time: 0:00:00.170052
elapsed time: 0:00:40.905143
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 16:50:13.846259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.05
 ---- batch: 020 ----
mean loss: 360.96
 ---- batch: 030 ----
mean loss: 352.56
train mean loss: 354.54
epoch train time: 0:00:00.172683
elapsed time: 0:00:41.077964
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 16:50:14.019080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.85
 ---- batch: 020 ----
mean loss: 360.09
 ---- batch: 030 ----
mean loss: 349.09
train mean loss: 353.99
epoch train time: 0:00:00.170600
elapsed time: 0:00:41.248704
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 16:50:14.189845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.08
 ---- batch: 020 ----
mean loss: 353.78
 ---- batch: 030 ----
mean loss: 355.21
train mean loss: 354.18
epoch train time: 0:00:00.171174
elapsed time: 0:00:41.420050
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 16:50:14.361178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.90
 ---- batch: 020 ----
mean loss: 346.90
 ---- batch: 030 ----
mean loss: 362.72
train mean loss: 354.54
epoch train time: 0:00:00.175253
elapsed time: 0:00:41.595449
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 16:50:14.536578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.78
 ---- batch: 020 ----
mean loss: 352.41
 ---- batch: 030 ----
mean loss: 354.37
train mean loss: 354.10
epoch train time: 0:00:00.169479
elapsed time: 0:00:41.765078
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 16:50:14.706193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.81
 ---- batch: 020 ----
mean loss: 357.10
 ---- batch: 030 ----
mean loss: 350.91
train mean loss: 353.82
epoch train time: 0:00:00.168293
elapsed time: 0:00:41.933535
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 16:50:14.874680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.15
 ---- batch: 020 ----
mean loss: 353.35
 ---- batch: 030 ----
mean loss: 351.88
train mean loss: 353.60
epoch train time: 0:00:00.170765
elapsed time: 0:00:42.104467
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 16:50:15.045583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.39
 ---- batch: 020 ----
mean loss: 360.18
 ---- batch: 030 ----
mean loss: 360.82
train mean loss: 353.44
epoch train time: 0:00:00.171300
elapsed time: 0:00:42.275903
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 16:50:15.217018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.81
 ---- batch: 020 ----
mean loss: 348.31
 ---- batch: 030 ----
mean loss: 355.25
train mean loss: 353.83
epoch train time: 0:00:00.166795
elapsed time: 0:00:42.442830
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 16:50:15.383966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.47
 ---- batch: 020 ----
mean loss: 356.69
 ---- batch: 030 ----
mean loss: 349.95
train mean loss: 353.44
epoch train time: 0:00:00.171121
elapsed time: 0:00:42.614133
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 16:50:15.555286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.37
 ---- batch: 020 ----
mean loss: 356.40
 ---- batch: 030 ----
mean loss: 354.05
train mean loss: 353.05
epoch train time: 0:00:00.173053
elapsed time: 0:00:42.787359
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 16:50:15.728476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.45
 ---- batch: 020 ----
mean loss: 355.34
 ---- batch: 030 ----
mean loss: 350.50
train mean loss: 352.95
epoch train time: 0:00:00.182069
elapsed time: 0:00:42.969611
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 16:50:15.910761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.12
 ---- batch: 020 ----
mean loss: 347.64
 ---- batch: 030 ----
mean loss: 358.89
train mean loss: 352.71
epoch train time: 0:00:00.172288
elapsed time: 0:00:43.142080
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 16:50:16.083198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.91
 ---- batch: 020 ----
mean loss: 351.32
 ---- batch: 030 ----
mean loss: 346.53
train mean loss: 352.52
epoch train time: 0:00:00.171165
elapsed time: 0:00:43.313394
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 16:50:16.254512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.23
 ---- batch: 020 ----
mean loss: 358.97
 ---- batch: 030 ----
mean loss: 346.32
train mean loss: 352.73
epoch train time: 0:00:00.168367
elapsed time: 0:00:43.481950
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 16:50:16.423065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.97
 ---- batch: 020 ----
mean loss: 352.23
 ---- batch: 030 ----
mean loss: 356.21
train mean loss: 352.24
epoch train time: 0:00:00.172831
elapsed time: 0:00:43.654915
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 16:50:16.596032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.87
 ---- batch: 020 ----
mean loss: 356.80
 ---- batch: 030 ----
mean loss: 346.83
train mean loss: 352.53
epoch train time: 0:00:00.172467
elapsed time: 0:00:43.827520
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 16:50:16.768636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.56
 ---- batch: 020 ----
mean loss: 350.44
 ---- batch: 030 ----
mean loss: 356.73
train mean loss: 351.95
epoch train time: 0:00:00.177265
elapsed time: 0:00:44.004926
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 16:50:16.946051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.89
 ---- batch: 020 ----
mean loss: 349.09
 ---- batch: 030 ----
mean loss: 352.77
train mean loss: 352.11
epoch train time: 0:00:00.173259
elapsed time: 0:00:44.178330
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 16:50:17.119446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.91
 ---- batch: 020 ----
mean loss: 351.84
 ---- batch: 030 ----
mean loss: 353.85
train mean loss: 351.98
epoch train time: 0:00:00.172288
elapsed time: 0:00:44.350754
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 16:50:17.291886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.75
 ---- batch: 020 ----
mean loss: 346.97
 ---- batch: 030 ----
mean loss: 351.30
train mean loss: 351.76
epoch train time: 0:00:00.172799
elapsed time: 0:00:44.523716
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 16:50:17.464832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.54
 ---- batch: 020 ----
mean loss: 357.38
 ---- batch: 030 ----
mean loss: 342.67
train mean loss: 351.75
epoch train time: 0:00:00.169573
elapsed time: 0:00:44.693452
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 16:50:17.634585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.55
 ---- batch: 020 ----
mean loss: 356.08
 ---- batch: 030 ----
mean loss: 347.44
train mean loss: 351.38
epoch train time: 0:00:00.174530
elapsed time: 0:00:44.868161
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 16:50:17.809288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.06
 ---- batch: 020 ----
mean loss: 358.00
 ---- batch: 030 ----
mean loss: 351.01
train mean loss: 351.34
epoch train time: 0:00:00.180563
elapsed time: 0:00:45.048877
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 16:50:17.989993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.71
 ---- batch: 020 ----
mean loss: 345.01
 ---- batch: 030 ----
mean loss: 356.51
train mean loss: 351.00
epoch train time: 0:00:00.171880
elapsed time: 0:00:45.220908
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 16:50:18.162043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.39
 ---- batch: 020 ----
mean loss: 354.07
 ---- batch: 030 ----
mean loss: 354.90
train mean loss: 350.71
epoch train time: 0:00:00.172520
elapsed time: 0:00:45.393586
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 16:50:18.334702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.50
 ---- batch: 020 ----
mean loss: 354.35
 ---- batch: 030 ----
mean loss: 351.06
train mean loss: 350.77
epoch train time: 0:00:00.172775
elapsed time: 0:00:45.566542
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 16:50:18.507658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.47
 ---- batch: 020 ----
mean loss: 356.21
 ---- batch: 030 ----
mean loss: 347.52
train mean loss: 350.37
epoch train time: 0:00:00.170440
elapsed time: 0:00:45.737121
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 16:50:18.678237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.76
 ---- batch: 020 ----
mean loss: 354.55
 ---- batch: 030 ----
mean loss: 345.35
train mean loss: 350.26
epoch train time: 0:00:00.175106
elapsed time: 0:00:45.912373
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 16:50:18.853490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.17
 ---- batch: 020 ----
mean loss: 349.78
 ---- batch: 030 ----
mean loss: 348.24
train mean loss: 350.32
epoch train time: 0:00:00.176084
elapsed time: 0:00:46.088636
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 16:50:19.029770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.54
 ---- batch: 020 ----
mean loss: 353.39
 ---- batch: 030 ----
mean loss: 349.25
train mean loss: 350.11
epoch train time: 0:00:00.171782
elapsed time: 0:00:46.260595
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 16:50:19.201717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.60
 ---- batch: 020 ----
mean loss: 344.98
 ---- batch: 030 ----
mean loss: 349.59
train mean loss: 349.82
epoch train time: 0:00:00.169777
elapsed time: 0:00:46.430517
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 16:50:19.371633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.16
 ---- batch: 020 ----
mean loss: 349.61
 ---- batch: 030 ----
mean loss: 346.82
train mean loss: 349.42
epoch train time: 0:00:00.173007
elapsed time: 0:00:46.603671
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 16:50:19.544793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.62
 ---- batch: 020 ----
mean loss: 353.64
 ---- batch: 030 ----
mean loss: 355.61
train mean loss: 349.55
epoch train time: 0:00:00.171517
elapsed time: 0:00:46.775333
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 16:50:19.716451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.45
 ---- batch: 020 ----
mean loss: 348.18
 ---- batch: 030 ----
mean loss: 348.85
train mean loss: 349.43
epoch train time: 0:00:00.170979
elapsed time: 0:00:46.946451
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 16:50:19.887567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.02
 ---- batch: 020 ----
mean loss: 349.92
 ---- batch: 030 ----
mean loss: 351.65
train mean loss: 348.78
epoch train time: 0:00:00.174301
elapsed time: 0:00:47.120905
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 16:50:20.062054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.55
 ---- batch: 020 ----
mean loss: 357.24
 ---- batch: 030 ----
mean loss: 345.12
train mean loss: 349.29
epoch train time: 0:00:00.169488
elapsed time: 0:00:47.290569
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 16:50:20.231687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.71
 ---- batch: 020 ----
mean loss: 345.17
 ---- batch: 030 ----
mean loss: 351.44
train mean loss: 348.99
epoch train time: 0:00:00.168919
elapsed time: 0:00:47.459644
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 16:50:20.400749
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.78
 ---- batch: 020 ----
mean loss: 350.25
 ---- batch: 030 ----
mean loss: 354.05
train mean loss: 349.05
epoch train time: 0:00:00.172410
elapsed time: 0:00:47.632180
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 16:50:20.573328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.37
 ---- batch: 020 ----
mean loss: 351.77
 ---- batch: 030 ----
mean loss: 342.37
train mean loss: 349.14
epoch train time: 0:00:00.174141
elapsed time: 0:00:47.806491
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 16:50:20.747608
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.26
 ---- batch: 020 ----
mean loss: 344.87
 ---- batch: 030 ----
mean loss: 356.12
train mean loss: 348.96
epoch train time: 0:00:00.170574
elapsed time: 0:00:47.977206
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 16:50:20.918324
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.57
 ---- batch: 020 ----
mean loss: 348.19
 ---- batch: 030 ----
mean loss: 352.18
train mean loss: 348.65
epoch train time: 0:00:00.174285
elapsed time: 0:00:48.151635
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 16:50:21.092744
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.99
 ---- batch: 020 ----
mean loss: 347.74
 ---- batch: 030 ----
mean loss: 346.49
train mean loss: 348.95
epoch train time: 0:00:00.169747
elapsed time: 0:00:48.321527
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 16:50:21.262643
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.98
 ---- batch: 020 ----
mean loss: 349.73
 ---- batch: 030 ----
mean loss: 355.10
train mean loss: 348.72
epoch train time: 0:00:00.168504
elapsed time: 0:00:48.490192
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 16:50:21.431308
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.23
 ---- batch: 020 ----
mean loss: 342.47
 ---- batch: 030 ----
mean loss: 350.17
train mean loss: 349.08
epoch train time: 0:00:00.170083
elapsed time: 0:00:48.660423
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 16:50:21.601541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.38
 ---- batch: 020 ----
mean loss: 356.19
 ---- batch: 030 ----
mean loss: 350.26
train mean loss: 348.65
epoch train time: 0:00:00.175880
elapsed time: 0:00:48.836445
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 16:50:21.777563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.18
 ---- batch: 020 ----
mean loss: 346.41
 ---- batch: 030 ----
mean loss: 352.17
train mean loss: 349.40
epoch train time: 0:00:00.173268
elapsed time: 0:00:49.009856
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 16:50:21.950991
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.56
 ---- batch: 020 ----
mean loss: 348.31
 ---- batch: 030 ----
mean loss: 355.34
train mean loss: 348.52
epoch train time: 0:00:00.176341
elapsed time: 0:00:49.186355
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 16:50:22.127473
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.59
 ---- batch: 020 ----
mean loss: 355.84
 ---- batch: 030 ----
mean loss: 345.90
train mean loss: 348.42
epoch train time: 0:00:00.174463
elapsed time: 0:00:49.360974
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 16:50:22.302123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.10
 ---- batch: 020 ----
mean loss: 346.70
 ---- batch: 030 ----
mean loss: 344.90
train mean loss: 348.31
epoch train time: 0:00:00.176443
elapsed time: 0:00:49.537591
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 16:50:22.478706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.44
 ---- batch: 020 ----
mean loss: 345.19
 ---- batch: 030 ----
mean loss: 350.04
train mean loss: 349.17
epoch train time: 0:00:00.176963
elapsed time: 0:00:49.714705
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 16:50:22.655832
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.77
 ---- batch: 020 ----
mean loss: 344.33
 ---- batch: 030 ----
mean loss: 349.52
train mean loss: 348.61
epoch train time: 0:00:00.179626
elapsed time: 0:00:49.894489
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 16:50:22.835608
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.61
 ---- batch: 020 ----
mean loss: 350.10
 ---- batch: 030 ----
mean loss: 346.15
train mean loss: 348.35
epoch train time: 0:00:00.174521
elapsed time: 0:00:50.069154
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 16:50:23.010273
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.71
 ---- batch: 020 ----
mean loss: 346.16
 ---- batch: 030 ----
mean loss: 351.39
train mean loss: 349.04
epoch train time: 0:00:00.178206
elapsed time: 0:00:50.247504
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 16:50:23.188622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.25
 ---- batch: 020 ----
mean loss: 348.57
 ---- batch: 030 ----
mean loss: 345.45
train mean loss: 348.86
epoch train time: 0:00:00.177112
elapsed time: 0:00:50.424760
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 16:50:23.365876
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.58
 ---- batch: 020 ----
mean loss: 351.31
 ---- batch: 030 ----
mean loss: 343.60
train mean loss: 348.61
epoch train time: 0:00:00.176593
elapsed time: 0:00:50.601493
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 16:50:23.542611
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.10
 ---- batch: 020 ----
mean loss: 344.48
 ---- batch: 030 ----
mean loss: 347.13
train mean loss: 348.50
epoch train time: 0:00:00.180645
elapsed time: 0:00:50.782281
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 16:50:23.723410
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.94
 ---- batch: 020 ----
mean loss: 347.00
 ---- batch: 030 ----
mean loss: 345.27
train mean loss: 348.62
epoch train time: 0:00:00.174066
elapsed time: 0:00:50.956500
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 16:50:23.897640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.90
 ---- batch: 020 ----
mean loss: 335.79
 ---- batch: 030 ----
mean loss: 351.83
train mean loss: 348.37
epoch train time: 0:00:00.176498
elapsed time: 0:00:51.133173
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 16:50:24.074290
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.32
 ---- batch: 020 ----
mean loss: 346.36
 ---- batch: 030 ----
mean loss: 349.44
train mean loss: 348.46
epoch train time: 0:00:00.173877
elapsed time: 0:00:51.307187
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 16:50:24.248302
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.92
 ---- batch: 020 ----
mean loss: 349.06
 ---- batch: 030 ----
mean loss: 350.90
train mean loss: 348.66
epoch train time: 0:00:00.173292
elapsed time: 0:00:51.480637
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 16:50:24.421754
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.28
 ---- batch: 020 ----
mean loss: 347.35
 ---- batch: 030 ----
mean loss: 344.98
train mean loss: 348.60
epoch train time: 0:00:00.169362
elapsed time: 0:00:51.650135
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 16:50:24.591265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.19
 ---- batch: 020 ----
mean loss: 343.98
 ---- batch: 030 ----
mean loss: 346.49
train mean loss: 348.31
epoch train time: 0:00:00.188294
elapsed time: 0:00:51.838602
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 16:50:24.779756
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.27
 ---- batch: 020 ----
mean loss: 342.12
 ---- batch: 030 ----
mean loss: 351.98
train mean loss: 348.32
epoch train time: 0:00:00.175855
elapsed time: 0:00:52.014636
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 16:50:24.955752
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.38
 ---- batch: 020 ----
mean loss: 357.63
 ---- batch: 030 ----
mean loss: 349.81
train mean loss: 348.31
epoch train time: 0:00:00.178685
elapsed time: 0:00:52.193458
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 16:50:25.134575
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 344.63
 ---- batch: 020 ----
mean loss: 350.88
 ---- batch: 030 ----
mean loss: 348.78
train mean loss: 348.98
epoch train time: 0:00:00.173445
elapsed time: 0:00:52.367042
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 16:50:25.308159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.27
 ---- batch: 020 ----
mean loss: 355.57
 ---- batch: 030 ----
mean loss: 340.46
train mean loss: 347.97
epoch train time: 0:00:00.173872
elapsed time: 0:00:52.541056
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 16:50:25.482174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.47
 ---- batch: 020 ----
mean loss: 339.00
 ---- batch: 030 ----
mean loss: 356.18
train mean loss: 348.23
epoch train time: 0:00:00.176728
elapsed time: 0:00:52.717924
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 16:50:25.659043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.79
 ---- batch: 020 ----
mean loss: 342.29
 ---- batch: 030 ----
mean loss: 355.90
train mean loss: 348.48
epoch train time: 0:00:00.175869
elapsed time: 0:00:52.893935
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 16:50:25.835054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.84
 ---- batch: 020 ----
mean loss: 352.00
 ---- batch: 030 ----
mean loss: 351.44
train mean loss: 348.23
epoch train time: 0:00:00.174452
elapsed time: 0:00:53.068563
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 16:50:26.009690
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.05
 ---- batch: 020 ----
mean loss: 348.12
 ---- batch: 030 ----
mean loss: 350.09
train mean loss: 348.57
epoch train time: 0:00:00.173577
elapsed time: 0:00:53.242290
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 16:50:26.183406
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.16
 ---- batch: 020 ----
mean loss: 347.35
 ---- batch: 030 ----
mean loss: 352.49
train mean loss: 348.06
epoch train time: 0:00:00.172103
elapsed time: 0:00:53.414530
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 16:50:26.355645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.72
 ---- batch: 020 ----
mean loss: 351.01
 ---- batch: 030 ----
mean loss: 342.86
train mean loss: 348.42
epoch train time: 0:00:00.172505
elapsed time: 0:00:53.587171
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 16:50:26.528288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.27
 ---- batch: 020 ----
mean loss: 353.89
 ---- batch: 030 ----
mean loss: 346.99
train mean loss: 348.30
epoch train time: 0:00:00.173214
elapsed time: 0:00:53.760527
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 16:50:26.701729
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.59
 ---- batch: 020 ----
mean loss: 354.70
 ---- batch: 030 ----
mean loss: 337.29
train mean loss: 348.35
epoch train time: 0:00:00.176238
elapsed time: 0:00:53.937027
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 16:50:26.878144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.22
 ---- batch: 020 ----
mean loss: 353.71
 ---- batch: 030 ----
mean loss: 346.61
train mean loss: 348.31
epoch train time: 0:00:00.175814
elapsed time: 0:00:54.112993
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 16:50:27.054180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.03
 ---- batch: 020 ----
mean loss: 343.11
 ---- batch: 030 ----
mean loss: 346.43
train mean loss: 347.80
epoch train time: 0:00:00.179700
elapsed time: 0:00:54.292908
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 16:50:27.234026
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.59
 ---- batch: 020 ----
mean loss: 347.71
 ---- batch: 030 ----
mean loss: 347.29
train mean loss: 348.06
epoch train time: 0:00:00.180045
elapsed time: 0:00:54.473098
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 16:50:27.414216
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.63
 ---- batch: 020 ----
mean loss: 344.06
 ---- batch: 030 ----
mean loss: 348.37
train mean loss: 348.53
epoch train time: 0:00:00.176746
elapsed time: 0:00:54.649996
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 16:50:27.591114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.70
 ---- batch: 020 ----
mean loss: 353.78
 ---- batch: 030 ----
mean loss: 346.21
train mean loss: 347.88
epoch train time: 0:00:00.175138
elapsed time: 0:00:54.825274
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 16:50:27.766393
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.47
 ---- batch: 020 ----
mean loss: 350.44
 ---- batch: 030 ----
mean loss: 341.05
train mean loss: 348.32
epoch train time: 0:00:00.178741
elapsed time: 0:00:55.004172
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 16:50:27.945290
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.75
 ---- batch: 020 ----
mean loss: 344.80
 ---- batch: 030 ----
mean loss: 348.26
train mean loss: 348.25
epoch train time: 0:00:00.176152
elapsed time: 0:00:55.180467
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 16:50:28.121597
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.24
 ---- batch: 020 ----
mean loss: 359.42
 ---- batch: 030 ----
mean loss: 343.39
train mean loss: 348.29
epoch train time: 0:00:00.179238
elapsed time: 0:00:55.359866
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 16:50:28.300982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.48
 ---- batch: 020 ----
mean loss: 349.70
 ---- batch: 030 ----
mean loss: 342.45
train mean loss: 348.30
epoch train time: 0:00:00.178465
elapsed time: 0:00:55.538470
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 16:50:28.479601
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.59
 ---- batch: 020 ----
mean loss: 346.25
 ---- batch: 030 ----
mean loss: 353.88
train mean loss: 348.10
epoch train time: 0:00:00.178928
elapsed time: 0:00:55.717667
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 16:50:28.658796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.44
 ---- batch: 020 ----
mean loss: 342.91
 ---- batch: 030 ----
mean loss: 349.80
train mean loss: 347.87
epoch train time: 0:00:00.186369
elapsed time: 0:00:55.906344
checkpoint saved in file: log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_8/checkpoint.pth.tar
**** end time: 2019-09-27 16:50:28.847429 ****
