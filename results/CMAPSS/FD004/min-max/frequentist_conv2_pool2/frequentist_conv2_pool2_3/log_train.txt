Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_3', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17196
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 01:37:00.757715 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 11, 11]             560
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
            Conv2d-4            [-1, 14, 4, 11]             224
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
            Linear-8                    [-1, 1]             308
================================================================
Total params: 1,092
Trainable params: 1,092
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 01:37:00.763373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4926.31
 ---- batch: 020 ----
mean loss: 4771.20
 ---- batch: 030 ----
mean loss: 4618.77
 ---- batch: 040 ----
mean loss: 4436.07
 ---- batch: 050 ----
mean loss: 4262.47
 ---- batch: 060 ----
mean loss: 4033.14
 ---- batch: 070 ----
mean loss: 3861.57
 ---- batch: 080 ----
mean loss: 3644.72
 ---- batch: 090 ----
mean loss: 3426.81
 ---- batch: 100 ----
mean loss: 3238.38
 ---- batch: 110 ----
mean loss: 3040.58
train mean loss: 3993.31
epoch train time: 0:00:34.163742
elapsed time: 0:00:34.170760
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 01:37:34.928517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2748.55
 ---- batch: 020 ----
mean loss: 2536.10
 ---- batch: 030 ----
mean loss: 2373.37
 ---- batch: 040 ----
mean loss: 2204.36
 ---- batch: 050 ----
mean loss: 2067.92
 ---- batch: 060 ----
mean loss: 1913.36
 ---- batch: 070 ----
mean loss: 1780.26
 ---- batch: 080 ----
mean loss: 1678.75
 ---- batch: 090 ----
mean loss: 1569.99
 ---- batch: 100 ----
mean loss: 1466.61
 ---- batch: 110 ----
mean loss: 1380.61
train mean loss: 1958.27
epoch train time: 0:00:00.724530
elapsed time: 0:00:34.895444
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 01:37:35.653208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1295.50
 ---- batch: 020 ----
mean loss: 1244.88
 ---- batch: 030 ----
mean loss: 1191.97
 ---- batch: 040 ----
mean loss: 1145.66
 ---- batch: 050 ----
mean loss: 1089.81
 ---- batch: 060 ----
mean loss: 1046.26
 ---- batch: 070 ----
mean loss: 1043.30
 ---- batch: 080 ----
mean loss: 1002.49
 ---- batch: 090 ----
mean loss: 979.34
 ---- batch: 100 ----
mean loss: 957.65
 ---- batch: 110 ----
mean loss: 941.99
train mean loss: 1080.97
epoch train time: 0:00:00.723853
elapsed time: 0:00:35.619434
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 01:37:36.377214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.94
 ---- batch: 020 ----
mean loss: 914.04
 ---- batch: 030 ----
mean loss: 911.54
 ---- batch: 040 ----
mean loss: 887.13
 ---- batch: 050 ----
mean loss: 870.02
 ---- batch: 060 ----
mean loss: 871.83
 ---- batch: 070 ----
mean loss: 873.28
 ---- batch: 080 ----
mean loss: 846.68
 ---- batch: 090 ----
mean loss: 870.65
 ---- batch: 100 ----
mean loss: 872.77
 ---- batch: 110 ----
mean loss: 852.44
train mean loss: 881.18
epoch train time: 0:00:00.727100
elapsed time: 0:00:36.346692
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 01:37:37.104459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.58
 ---- batch: 020 ----
mean loss: 848.14
 ---- batch: 030 ----
mean loss: 854.49
 ---- batch: 040 ----
mean loss: 860.45
 ---- batch: 050 ----
mean loss: 853.75
 ---- batch: 060 ----
mean loss: 839.47
 ---- batch: 070 ----
mean loss: 842.82
 ---- batch: 080 ----
mean loss: 839.62
 ---- batch: 090 ----
mean loss: 840.56
 ---- batch: 100 ----
mean loss: 855.69
 ---- batch: 110 ----
mean loss: 853.95
train mean loss: 848.74
epoch train time: 0:00:00.724931
elapsed time: 0:00:37.071793
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 01:37:37.829564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.15
 ---- batch: 020 ----
mean loss: 842.81
 ---- batch: 030 ----
mean loss: 847.89
 ---- batch: 040 ----
mean loss: 822.64
 ---- batch: 050 ----
mean loss: 836.77
 ---- batch: 060 ----
mean loss: 857.98
 ---- batch: 070 ----
mean loss: 836.07
 ---- batch: 080 ----
mean loss: 857.19
 ---- batch: 090 ----
mean loss: 836.99
 ---- batch: 100 ----
mean loss: 844.32
 ---- batch: 110 ----
mean loss: 842.41
train mean loss: 842.08
epoch train time: 0:00:00.719110
elapsed time: 0:00:37.791077
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 01:37:38.548867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.50
 ---- batch: 020 ----
mean loss: 825.03
 ---- batch: 030 ----
mean loss: 828.39
 ---- batch: 040 ----
mean loss: 838.18
 ---- batch: 050 ----
mean loss: 811.07
 ---- batch: 060 ----
mean loss: 839.37
 ---- batch: 070 ----
mean loss: 829.82
 ---- batch: 080 ----
mean loss: 853.37
 ---- batch: 090 ----
mean loss: 848.79
 ---- batch: 100 ----
mean loss: 852.85
 ---- batch: 110 ----
mean loss: 840.36
train mean loss: 837.95
epoch train time: 0:00:00.715743
elapsed time: 0:00:38.506997
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 01:37:39.264764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.06
 ---- batch: 020 ----
mean loss: 820.29
 ---- batch: 030 ----
mean loss: 854.75
 ---- batch: 040 ----
mean loss: 835.82
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 841.59
 ---- batch: 070 ----
mean loss: 812.40
 ---- batch: 080 ----
mean loss: 831.83
 ---- batch: 090 ----
mean loss: 835.07
 ---- batch: 100 ----
mean loss: 842.30
 ---- batch: 110 ----
mean loss: 837.32
train mean loss: 833.71
epoch train time: 0:00:00.725394
elapsed time: 0:00:39.232535
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 01:37:39.990301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.78
 ---- batch: 020 ----
mean loss: 835.56
 ---- batch: 030 ----
mean loss: 816.81
 ---- batch: 040 ----
mean loss: 816.32
 ---- batch: 050 ----
mean loss: 833.27
 ---- batch: 060 ----
mean loss: 816.71
 ---- batch: 070 ----
mean loss: 838.72
 ---- batch: 080 ----
mean loss: 823.61
 ---- batch: 090 ----
mean loss: 822.33
 ---- batch: 100 ----
mean loss: 847.12
 ---- batch: 110 ----
mean loss: 835.35
train mean loss: 829.22
epoch train time: 0:00:00.722335
elapsed time: 0:00:39.955011
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 01:37:40.712776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.87
 ---- batch: 020 ----
mean loss: 834.13
 ---- batch: 030 ----
mean loss: 805.89
 ---- batch: 040 ----
mean loss: 838.24
 ---- batch: 050 ----
mean loss: 819.26
 ---- batch: 060 ----
mean loss: 826.03
 ---- batch: 070 ----
mean loss: 819.20
 ---- batch: 080 ----
mean loss: 853.58
 ---- batch: 090 ----
mean loss: 814.84
 ---- batch: 100 ----
mean loss: 808.75
 ---- batch: 110 ----
mean loss: 830.15
train mean loss: 824.03
epoch train time: 0:00:00.715382
elapsed time: 0:00:40.670530
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 01:37:41.428293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 825.00
 ---- batch: 020 ----
mean loss: 804.95
 ---- batch: 030 ----
mean loss: 821.09
 ---- batch: 040 ----
mean loss: 837.61
 ---- batch: 050 ----
mean loss: 811.75
 ---- batch: 060 ----
mean loss: 815.71
 ---- batch: 070 ----
mean loss: 820.62
 ---- batch: 080 ----
mean loss: 813.52
 ---- batch: 090 ----
mean loss: 828.79
 ---- batch: 100 ----
mean loss: 812.74
 ---- batch: 110 ----
mean loss: 815.18
train mean loss: 818.90
epoch train time: 0:00:00.720827
elapsed time: 0:00:41.391508
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 01:37:42.149304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.23
 ---- batch: 020 ----
mean loss: 795.93
 ---- batch: 030 ----
mean loss: 830.77
 ---- batch: 040 ----
mean loss: 824.00
 ---- batch: 050 ----
mean loss: 814.57
 ---- batch: 060 ----
mean loss: 807.53
 ---- batch: 070 ----
mean loss: 812.17
 ---- batch: 080 ----
mean loss: 804.82
 ---- batch: 090 ----
mean loss: 820.18
 ---- batch: 100 ----
mean loss: 809.57
 ---- batch: 110 ----
mean loss: 782.63
train mean loss: 813.71
epoch train time: 0:00:00.722028
elapsed time: 0:00:42.113712
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 01:37:42.871491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.59
 ---- batch: 020 ----
mean loss: 818.01
 ---- batch: 030 ----
mean loss: 815.42
 ---- batch: 040 ----
mean loss: 804.01
 ---- batch: 050 ----
mean loss: 807.05
 ---- batch: 060 ----
mean loss: 802.51
 ---- batch: 070 ----
mean loss: 818.57
 ---- batch: 080 ----
mean loss: 784.14
 ---- batch: 090 ----
mean loss: 805.37
 ---- batch: 100 ----
mean loss: 818.34
 ---- batch: 110 ----
mean loss: 790.45
train mean loss: 808.78
epoch train time: 0:00:00.722003
elapsed time: 0:00:42.835865
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 01:37:43.593648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 793.22
 ---- batch: 020 ----
mean loss: 807.78
 ---- batch: 030 ----
mean loss: 799.42
 ---- batch: 040 ----
mean loss: 788.68
 ---- batch: 050 ----
mean loss: 795.08
 ---- batch: 060 ----
mean loss: 809.89
 ---- batch: 070 ----
mean loss: 807.68
 ---- batch: 080 ----
mean loss: 810.29
 ---- batch: 090 ----
mean loss: 795.49
 ---- batch: 100 ----
mean loss: 812.27
 ---- batch: 110 ----
mean loss: 814.31
train mean loss: 803.84
epoch train time: 0:00:00.719164
elapsed time: 0:00:43.555185
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 01:37:44.312952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.44
 ---- batch: 020 ----
mean loss: 794.23
 ---- batch: 030 ----
mean loss: 810.10
 ---- batch: 040 ----
mean loss: 810.82
 ---- batch: 050 ----
mean loss: 802.59
 ---- batch: 060 ----
mean loss: 791.20
 ---- batch: 070 ----
mean loss: 795.24
 ---- batch: 080 ----
mean loss: 786.37
 ---- batch: 090 ----
mean loss: 795.52
 ---- batch: 100 ----
mean loss: 795.24
 ---- batch: 110 ----
mean loss: 814.37
train mean loss: 798.91
epoch train time: 0:00:00.719110
elapsed time: 0:00:44.274439
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 01:37:45.032219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 800.07
 ---- batch: 020 ----
mean loss: 802.29
 ---- batch: 030 ----
mean loss: 791.61
 ---- batch: 040 ----
mean loss: 779.51
 ---- batch: 050 ----
mean loss: 791.74
 ---- batch: 060 ----
mean loss: 802.19
 ---- batch: 070 ----
mean loss: 805.18
 ---- batch: 080 ----
mean loss: 781.37
 ---- batch: 090 ----
mean loss: 784.31
 ---- batch: 100 ----
mean loss: 804.84
 ---- batch: 110 ----
mean loss: 781.17
train mean loss: 794.04
epoch train time: 0:00:00.715270
elapsed time: 0:00:44.989882
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 01:37:45.747649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 785.69
 ---- batch: 020 ----
mean loss: 760.78
 ---- batch: 030 ----
mean loss: 787.09
 ---- batch: 040 ----
mean loss: 809.85
 ---- batch: 050 ----
mean loss: 807.84
 ---- batch: 060 ----
mean loss: 804.13
 ---- batch: 070 ----
mean loss: 800.49
 ---- batch: 080 ----
mean loss: 788.01
 ---- batch: 090 ----
mean loss: 773.84
 ---- batch: 100 ----
mean loss: 782.17
 ---- batch: 110 ----
mean loss: 779.27
train mean loss: 789.05
epoch train time: 0:00:00.715402
elapsed time: 0:00:45.705444
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 01:37:46.463207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 766.33
 ---- batch: 020 ----
mean loss: 791.33
 ---- batch: 030 ----
mean loss: 771.92
 ---- batch: 040 ----
mean loss: 789.68
 ---- batch: 050 ----
mean loss: 800.38
 ---- batch: 060 ----
mean loss: 767.03
 ---- batch: 070 ----
mean loss: 794.84
 ---- batch: 080 ----
mean loss: 778.63
 ---- batch: 090 ----
mean loss: 778.79
 ---- batch: 100 ----
mean loss: 795.84
 ---- batch: 110 ----
mean loss: 791.69
train mean loss: 783.77
epoch train time: 0:00:00.730940
elapsed time: 0:00:46.436525
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 01:37:47.194290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 780.57
 ---- batch: 020 ----
mean loss: 792.38
 ---- batch: 030 ----
mean loss: 779.57
 ---- batch: 040 ----
mean loss: 760.51
 ---- batch: 050 ----
mean loss: 769.34
 ---- batch: 060 ----
mean loss: 784.75
 ---- batch: 070 ----
mean loss: 777.47
 ---- batch: 080 ----
mean loss: 777.61
 ---- batch: 090 ----
mean loss: 783.34
 ---- batch: 100 ----
mean loss: 772.97
 ---- batch: 110 ----
mean loss: 795.68
train mean loss: 778.57
epoch train time: 0:00:00.727614
elapsed time: 0:00:47.164282
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 01:37:47.922071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.24
 ---- batch: 020 ----
mean loss: 782.15
 ---- batch: 030 ----
mean loss: 785.36
 ---- batch: 040 ----
mean loss: 768.48
 ---- batch: 050 ----
mean loss: 760.58
 ---- batch: 060 ----
mean loss: 781.15
 ---- batch: 070 ----
mean loss: 774.99
 ---- batch: 080 ----
mean loss: 777.19
 ---- batch: 090 ----
mean loss: 767.23
 ---- batch: 100 ----
mean loss: 772.96
 ---- batch: 110 ----
mean loss: 775.15
train mean loss: 773.52
epoch train time: 0:00:00.714099
elapsed time: 0:00:47.878545
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 01:37:48.636311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.49
 ---- batch: 020 ----
mean loss: 805.95
 ---- batch: 030 ----
mean loss: 771.11
 ---- batch: 040 ----
mean loss: 791.77
 ---- batch: 050 ----
mean loss: 774.74
 ---- batch: 060 ----
mean loss: 776.18
 ---- batch: 070 ----
mean loss: 765.42
 ---- batch: 080 ----
mean loss: 775.54
 ---- batch: 090 ----
mean loss: 756.59
 ---- batch: 100 ----
mean loss: 752.89
 ---- batch: 110 ----
mean loss: 748.45
train mean loss: 768.50
epoch train time: 0:00:00.721861
elapsed time: 0:00:48.600552
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 01:37:49.358321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 757.82
 ---- batch: 020 ----
mean loss: 778.98
 ---- batch: 030 ----
mean loss: 754.94
 ---- batch: 040 ----
mean loss: 780.33
 ---- batch: 050 ----
mean loss: 767.41
 ---- batch: 060 ----
mean loss: 759.90
 ---- batch: 070 ----
mean loss: 772.02
 ---- batch: 080 ----
mean loss: 758.39
 ---- batch: 090 ----
mean loss: 754.54
 ---- batch: 100 ----
mean loss: 752.36
 ---- batch: 110 ----
mean loss: 767.10
train mean loss: 763.47
epoch train time: 0:00:00.716611
elapsed time: 0:00:49.317308
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 01:37:50.075071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 744.70
 ---- batch: 020 ----
mean loss: 746.82
 ---- batch: 030 ----
mean loss: 738.70
 ---- batch: 040 ----
mean loss: 763.52
 ---- batch: 050 ----
mean loss: 778.02
 ---- batch: 060 ----
mean loss: 754.44
 ---- batch: 070 ----
mean loss: 778.02
 ---- batch: 080 ----
mean loss: 747.83
 ---- batch: 090 ----
mean loss: 752.59
 ---- batch: 100 ----
mean loss: 779.94
 ---- batch: 110 ----
mean loss: 750.78
train mean loss: 758.26
epoch train time: 0:00:00.717361
elapsed time: 0:00:50.034840
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 01:37:50.792605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.51
 ---- batch: 020 ----
mean loss: 754.34
 ---- batch: 030 ----
mean loss: 768.49
 ---- batch: 040 ----
mean loss: 749.80
 ---- batch: 050 ----
mean loss: 767.50
 ---- batch: 060 ----
mean loss: 746.40
 ---- batch: 070 ----
mean loss: 748.74
 ---- batch: 080 ----
mean loss: 762.72
 ---- batch: 090 ----
mean loss: 748.52
 ---- batch: 100 ----
mean loss: 758.01
 ---- batch: 110 ----
mean loss: 737.24
train mean loss: 752.80
epoch train time: 0:00:00.717733
elapsed time: 0:00:50.752723
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 01:37:51.510490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 747.50
 ---- batch: 020 ----
mean loss: 748.64
 ---- batch: 030 ----
mean loss: 735.29
 ---- batch: 040 ----
mean loss: 754.69
 ---- batch: 050 ----
mean loss: 748.62
 ---- batch: 060 ----
mean loss: 756.74
 ---- batch: 070 ----
mean loss: 729.19
 ---- batch: 080 ----
mean loss: 748.81
 ---- batch: 090 ----
mean loss: 755.70
 ---- batch: 100 ----
mean loss: 734.78
 ---- batch: 110 ----
mean loss: 754.91
train mean loss: 747.10
epoch train time: 0:00:00.721037
elapsed time: 0:00:51.473902
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 01:37:52.231685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 746.53
 ---- batch: 020 ----
mean loss: 742.74
 ---- batch: 030 ----
mean loss: 735.65
 ---- batch: 040 ----
mean loss: 742.00
 ---- batch: 050 ----
mean loss: 740.92
 ---- batch: 060 ----
mean loss: 748.59
 ---- batch: 070 ----
mean loss: 754.27
 ---- batch: 080 ----
mean loss: 726.91
 ---- batch: 090 ----
mean loss: 748.80
 ---- batch: 100 ----
mean loss: 734.78
 ---- batch: 110 ----
mean loss: 737.23
train mean loss: 740.98
epoch train time: 0:00:00.721150
elapsed time: 0:00:52.195219
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 01:37:52.953001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 741.19
 ---- batch: 020 ----
mean loss: 741.95
 ---- batch: 030 ----
mean loss: 747.35
 ---- batch: 040 ----
mean loss: 740.71
 ---- batch: 050 ----
mean loss: 730.96
 ---- batch: 060 ----
mean loss: 719.87
 ---- batch: 070 ----
mean loss: 717.98
 ---- batch: 080 ----
mean loss: 752.38
 ---- batch: 090 ----
mean loss: 747.18
 ---- batch: 100 ----
mean loss: 721.87
 ---- batch: 110 ----
mean loss: 726.28
train mean loss: 734.77
epoch train time: 0:00:00.711633
elapsed time: 0:00:52.907009
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 01:37:53.664805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 729.07
 ---- batch: 020 ----
mean loss: 720.26
 ---- batch: 030 ----
mean loss: 745.83
 ---- batch: 040 ----
mean loss: 746.49
 ---- batch: 050 ----
mean loss: 725.38
 ---- batch: 060 ----
mean loss: 725.77
 ---- batch: 070 ----
mean loss: 722.68
 ---- batch: 080 ----
mean loss: 725.19
 ---- batch: 090 ----
mean loss: 733.75
 ---- batch: 100 ----
mean loss: 718.40
 ---- batch: 110 ----
mean loss: 727.09
train mean loss: 728.24
epoch train time: 0:00:00.730258
elapsed time: 0:00:53.637457
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 01:37:54.395230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.39
 ---- batch: 020 ----
mean loss: 721.00
 ---- batch: 030 ----
mean loss: 733.10
 ---- batch: 040 ----
mean loss: 742.39
 ---- batch: 050 ----
mean loss: 741.69
 ---- batch: 060 ----
mean loss: 713.58
 ---- batch: 070 ----
mean loss: 722.55
 ---- batch: 080 ----
mean loss: 721.98
 ---- batch: 090 ----
mean loss: 706.74
 ---- batch: 100 ----
mean loss: 725.55
 ---- batch: 110 ----
mean loss: 716.80
train mean loss: 721.74
epoch train time: 0:00:00.730423
elapsed time: 0:00:54.368026
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 01:37:55.125790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 724.14
 ---- batch: 020 ----
mean loss: 701.21
 ---- batch: 030 ----
mean loss: 711.26
 ---- batch: 040 ----
mean loss: 711.32
 ---- batch: 050 ----
mean loss: 723.44
 ---- batch: 060 ----
mean loss: 712.47
 ---- batch: 070 ----
mean loss: 714.31
 ---- batch: 080 ----
mean loss: 724.33
 ---- batch: 090 ----
mean loss: 714.79
 ---- batch: 100 ----
mean loss: 720.74
 ---- batch: 110 ----
mean loss: 713.70
train mean loss: 714.93
epoch train time: 0:00:00.720021
elapsed time: 0:00:55.088190
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 01:37:55.845969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 710.50
 ---- batch: 020 ----
mean loss: 703.30
 ---- batch: 030 ----
mean loss: 713.72
 ---- batch: 040 ----
mean loss: 714.76
 ---- batch: 050 ----
mean loss: 710.31
 ---- batch: 060 ----
mean loss: 709.64
 ---- batch: 070 ----
mean loss: 675.89
 ---- batch: 080 ----
mean loss: 717.82
 ---- batch: 090 ----
mean loss: 708.51
 ---- batch: 100 ----
mean loss: 710.32
 ---- batch: 110 ----
mean loss: 713.30
train mean loss: 708.02
epoch train time: 0:00:00.715045
elapsed time: 0:00:55.803403
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 01:37:56.561182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.20
 ---- batch: 020 ----
mean loss: 689.75
 ---- batch: 030 ----
mean loss: 716.94
 ---- batch: 040 ----
mean loss: 721.08
 ---- batch: 050 ----
mean loss: 683.97
 ---- batch: 060 ----
mean loss: 704.22
 ---- batch: 070 ----
mean loss: 702.41
 ---- batch: 080 ----
mean loss: 701.25
 ---- batch: 090 ----
mean loss: 696.01
 ---- batch: 100 ----
mean loss: 700.00
 ---- batch: 110 ----
mean loss: 687.78
train mean loss: 700.93
epoch train time: 0:00:00.718922
elapsed time: 0:00:56.522489
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 01:37:57.280314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 685.69
 ---- batch: 020 ----
mean loss: 687.31
 ---- batch: 030 ----
mean loss: 705.11
 ---- batch: 040 ----
mean loss: 707.50
 ---- batch: 050 ----
mean loss: 697.01
 ---- batch: 060 ----
mean loss: 702.46
 ---- batch: 070 ----
mean loss: 669.95
 ---- batch: 080 ----
mean loss: 701.12
 ---- batch: 090 ----
mean loss: 690.15
 ---- batch: 100 ----
mean loss: 698.79
 ---- batch: 110 ----
mean loss: 683.20
train mean loss: 693.24
epoch train time: 0:00:00.724404
elapsed time: 0:00:57.247122
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 01:37:58.004888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.22
 ---- batch: 020 ----
mean loss: 690.35
 ---- batch: 030 ----
mean loss: 680.52
 ---- batch: 040 ----
mean loss: 687.13
 ---- batch: 050 ----
mean loss: 671.19
 ---- batch: 060 ----
mean loss: 679.18
 ---- batch: 070 ----
mean loss: 692.61
 ---- batch: 080 ----
mean loss: 693.64
 ---- batch: 090 ----
mean loss: 691.97
 ---- batch: 100 ----
mean loss: 691.02
 ---- batch: 110 ----
mean loss: 687.30
train mean loss: 685.50
epoch train time: 0:00:00.717857
elapsed time: 0:00:57.965143
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 01:37:58.722911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 678.12
 ---- batch: 020 ----
mean loss: 653.74
 ---- batch: 030 ----
mean loss: 673.75
 ---- batch: 040 ----
mean loss: 691.42
 ---- batch: 050 ----
mean loss: 678.66
 ---- batch: 060 ----
mean loss: 686.87
 ---- batch: 070 ----
mean loss: 662.07
 ---- batch: 080 ----
mean loss: 685.30
 ---- batch: 090 ----
mean loss: 684.46
 ---- batch: 100 ----
mean loss: 684.53
 ---- batch: 110 ----
mean loss: 679.11
train mean loss: 677.45
epoch train time: 0:00:00.732037
elapsed time: 0:00:58.697325
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 01:37:59.455094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 698.64
 ---- batch: 020 ----
mean loss: 682.51
 ---- batch: 030 ----
mean loss: 667.96
 ---- batch: 040 ----
mean loss: 668.16
 ---- batch: 050 ----
mean loss: 669.18
 ---- batch: 060 ----
mean loss: 662.64
 ---- batch: 070 ----
mean loss: 665.22
 ---- batch: 080 ----
mean loss: 668.17
 ---- batch: 090 ----
mean loss: 653.98
 ---- batch: 100 ----
mean loss: 666.29
 ---- batch: 110 ----
mean loss: 658.39
train mean loss: 669.10
epoch train time: 0:00:00.720045
elapsed time: 0:00:59.417520
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 01:38:00.175285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.15
 ---- batch: 020 ----
mean loss: 661.42
 ---- batch: 030 ----
mean loss: 668.99
 ---- batch: 040 ----
mean loss: 662.03
 ---- batch: 050 ----
mean loss: 652.49
 ---- batch: 060 ----
mean loss: 668.47
 ---- batch: 070 ----
mean loss: 652.61
 ---- batch: 080 ----
mean loss: 649.69
 ---- batch: 090 ----
mean loss: 669.02
 ---- batch: 100 ----
mean loss: 662.49
 ---- batch: 110 ----
mean loss: 641.78
train mean loss: 660.33
epoch train time: 0:00:00.717530
elapsed time: 0:01:00.135207
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 01:38:00.892990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 658.39
 ---- batch: 020 ----
mean loss: 661.30
 ---- batch: 030 ----
mean loss: 655.67
 ---- batch: 040 ----
mean loss: 652.33
 ---- batch: 050 ----
mean loss: 669.45
 ---- batch: 060 ----
mean loss: 638.05
 ---- batch: 070 ----
mean loss: 643.46
 ---- batch: 080 ----
mean loss: 641.87
 ---- batch: 090 ----
mean loss: 637.47
 ---- batch: 100 ----
mean loss: 649.54
 ---- batch: 110 ----
mean loss: 660.83
train mean loss: 651.35
epoch train time: 0:00:00.716317
elapsed time: 0:01:00.851680
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 01:38:01.609446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 653.42
 ---- batch: 020 ----
mean loss: 647.24
 ---- batch: 030 ----
mean loss: 650.64
 ---- batch: 040 ----
mean loss: 646.49
 ---- batch: 050 ----
mean loss: 623.85
 ---- batch: 060 ----
mean loss: 635.83
 ---- batch: 070 ----
mean loss: 640.18
 ---- batch: 080 ----
mean loss: 650.36
 ---- batch: 090 ----
mean loss: 634.01
 ---- batch: 100 ----
mean loss: 634.25
 ---- batch: 110 ----
mean loss: 642.44
train mean loss: 641.98
epoch train time: 0:00:00.737420
elapsed time: 0:01:01.589241
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 01:38:02.347024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 636.04
 ---- batch: 020 ----
mean loss: 637.68
 ---- batch: 030 ----
mean loss: 624.73
 ---- batch: 040 ----
mean loss: 633.99
 ---- batch: 050 ----
mean loss: 634.45
 ---- batch: 060 ----
mean loss: 626.31
 ---- batch: 070 ----
mean loss: 630.28
 ---- batch: 080 ----
mean loss: 640.21
 ---- batch: 090 ----
mean loss: 629.71
 ---- batch: 100 ----
mean loss: 636.48
 ---- batch: 110 ----
mean loss: 632.87
train mean loss: 632.10
epoch train time: 0:00:00.717568
elapsed time: 0:01:02.306981
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 01:38:03.064744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 619.68
 ---- batch: 020 ----
mean loss: 635.59
 ---- batch: 030 ----
mean loss: 613.66
 ---- batch: 040 ----
mean loss: 631.66
 ---- batch: 050 ----
mean loss: 628.33
 ---- batch: 060 ----
mean loss: 624.35
 ---- batch: 070 ----
mean loss: 621.64
 ---- batch: 080 ----
mean loss: 624.79
 ---- batch: 090 ----
mean loss: 608.39
 ---- batch: 100 ----
mean loss: 617.42
 ---- batch: 110 ----
mean loss: 621.44
train mean loss: 622.04
epoch train time: 0:00:00.716192
elapsed time: 0:01:03.023324
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 01:38:03.781118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 622.76
 ---- batch: 020 ----
mean loss: 605.83
 ---- batch: 030 ----
mean loss: 615.98
 ---- batch: 040 ----
mean loss: 614.93
 ---- batch: 050 ----
mean loss: 604.40
 ---- batch: 060 ----
mean loss: 604.63
 ---- batch: 070 ----
mean loss: 623.73
 ---- batch: 080 ----
mean loss: 625.99
 ---- batch: 090 ----
mean loss: 613.37
 ---- batch: 100 ----
mean loss: 607.97
 ---- batch: 110 ----
mean loss: 596.58
train mean loss: 611.55
epoch train time: 0:00:00.721013
elapsed time: 0:01:03.744509
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 01:38:04.502278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 605.98
 ---- batch: 020 ----
mean loss: 599.89
 ---- batch: 030 ----
mean loss: 605.22
 ---- batch: 040 ----
mean loss: 607.22
 ---- batch: 050 ----
mean loss: 607.71
 ---- batch: 060 ----
mean loss: 602.09
 ---- batch: 070 ----
mean loss: 603.28
 ---- batch: 080 ----
mean loss: 599.87
 ---- batch: 090 ----
mean loss: 596.89
 ---- batch: 100 ----
mean loss: 585.00
 ---- batch: 110 ----
mean loss: 597.55
train mean loss: 600.99
epoch train time: 0:00:00.718072
elapsed time: 0:01:04.462778
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 01:38:05.220547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.36
 ---- batch: 020 ----
mean loss: 587.90
 ---- batch: 030 ----
mean loss: 603.91
 ---- batch: 040 ----
mean loss: 593.52
 ---- batch: 050 ----
mean loss: 591.52
 ---- batch: 060 ----
mean loss: 590.01
 ---- batch: 070 ----
mean loss: 575.70
 ---- batch: 080 ----
mean loss: 596.89
 ---- batch: 090 ----
mean loss: 582.15
 ---- batch: 100 ----
mean loss: 590.94
 ---- batch: 110 ----
mean loss: 584.45
train mean loss: 590.07
epoch train time: 0:00:00.721254
elapsed time: 0:01:05.184181
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 01:38:05.941947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.01
 ---- batch: 020 ----
mean loss: 580.64
 ---- batch: 030 ----
mean loss: 576.84
 ---- batch: 040 ----
mean loss: 585.87
 ---- batch: 050 ----
mean loss: 579.18
 ---- batch: 060 ----
mean loss: 606.08
 ---- batch: 070 ----
mean loss: 583.83
 ---- batch: 080 ----
mean loss: 570.38
 ---- batch: 090 ----
mean loss: 572.50
 ---- batch: 100 ----
mean loss: 560.58
 ---- batch: 110 ----
mean loss: 574.30
train mean loss: 578.98
epoch train time: 0:00:00.727414
elapsed time: 0:01:05.911765
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 01:38:06.669552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.59
 ---- batch: 020 ----
mean loss: 589.92
 ---- batch: 030 ----
mean loss: 578.82
 ---- batch: 040 ----
mean loss: 576.06
 ---- batch: 050 ----
mean loss: 561.66
 ---- batch: 060 ----
mean loss: 563.28
 ---- batch: 070 ----
mean loss: 561.25
 ---- batch: 080 ----
mean loss: 566.59
 ---- batch: 090 ----
mean loss: 556.87
 ---- batch: 100 ----
mean loss: 568.48
 ---- batch: 110 ----
mean loss: 567.33
train mean loss: 567.64
epoch train time: 0:00:00.717329
elapsed time: 0:01:06.629261
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 01:38:07.387026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.31
 ---- batch: 020 ----
mean loss: 560.54
 ---- batch: 030 ----
mean loss: 564.09
 ---- batch: 040 ----
mean loss: 555.81
 ---- batch: 050 ----
mean loss: 556.23
 ---- batch: 060 ----
mean loss: 541.88
 ---- batch: 070 ----
mean loss: 569.69
 ---- batch: 080 ----
mean loss: 551.43
 ---- batch: 090 ----
mean loss: 556.85
 ---- batch: 100 ----
mean loss: 546.40
 ---- batch: 110 ----
mean loss: 558.56
train mean loss: 556.42
epoch train time: 0:00:00.721167
elapsed time: 0:01:07.350566
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 01:38:08.108330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 547.34
 ---- batch: 020 ----
mean loss: 555.41
 ---- batch: 030 ----
mean loss: 542.32
 ---- batch: 040 ----
mean loss: 534.41
 ---- batch: 050 ----
mean loss: 552.79
 ---- batch: 060 ----
mean loss: 541.78
 ---- batch: 070 ----
mean loss: 558.77
 ---- batch: 080 ----
mean loss: 542.35
 ---- batch: 090 ----
mean loss: 551.79
 ---- batch: 100 ----
mean loss: 525.83
 ---- batch: 110 ----
mean loss: 539.90
train mean loss: 544.72
epoch train time: 0:00:00.719815
elapsed time: 0:01:08.070524
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 01:38:08.828301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 539.44
 ---- batch: 020 ----
mean loss: 536.78
 ---- batch: 030 ----
mean loss: 539.24
 ---- batch: 040 ----
mean loss: 525.93
 ---- batch: 050 ----
mean loss: 526.02
 ---- batch: 060 ----
mean loss: 541.19
 ---- batch: 070 ----
mean loss: 542.68
 ---- batch: 080 ----
mean loss: 533.25
 ---- batch: 090 ----
mean loss: 527.41
 ---- batch: 100 ----
mean loss: 529.20
 ---- batch: 110 ----
mean loss: 520.21
train mean loss: 533.13
epoch train time: 0:00:00.733026
elapsed time: 0:01:08.803705
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 01:38:09.561476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 528.50
 ---- batch: 020 ----
mean loss: 531.85
 ---- batch: 030 ----
mean loss: 525.98
 ---- batch: 040 ----
mean loss: 524.72
 ---- batch: 050 ----
mean loss: 514.46
 ---- batch: 060 ----
mean loss: 519.59
 ---- batch: 070 ----
mean loss: 514.02
 ---- batch: 080 ----
mean loss: 521.75
 ---- batch: 090 ----
mean loss: 524.93
 ---- batch: 100 ----
mean loss: 511.69
 ---- batch: 110 ----
mean loss: 511.16
train mean loss: 521.15
epoch train time: 0:00:00.731527
elapsed time: 0:01:09.535380
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 01:38:10.293147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.34
 ---- batch: 020 ----
mean loss: 514.21
 ---- batch: 030 ----
mean loss: 511.68
 ---- batch: 040 ----
mean loss: 514.93
 ---- batch: 050 ----
mean loss: 511.71
 ---- batch: 060 ----
mean loss: 508.90
 ---- batch: 070 ----
mean loss: 514.63
 ---- batch: 080 ----
mean loss: 509.62
 ---- batch: 090 ----
mean loss: 501.18
 ---- batch: 100 ----
mean loss: 497.74
 ---- batch: 110 ----
mean loss: 501.87
train mean loss: 509.30
epoch train time: 0:00:00.737852
elapsed time: 0:01:10.273375
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 01:38:11.031142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.73
 ---- batch: 020 ----
mean loss: 508.61
 ---- batch: 030 ----
mean loss: 514.70
 ---- batch: 040 ----
mean loss: 494.25
 ---- batch: 050 ----
mean loss: 490.17
 ---- batch: 060 ----
mean loss: 501.53
 ---- batch: 070 ----
mean loss: 494.90
 ---- batch: 080 ----
mean loss: 485.58
 ---- batch: 090 ----
mean loss: 487.17
 ---- batch: 100 ----
mean loss: 498.35
 ---- batch: 110 ----
mean loss: 493.99
train mean loss: 497.28
epoch train time: 0:00:00.723415
elapsed time: 0:01:10.996932
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 01:38:11.754720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.53
 ---- batch: 020 ----
mean loss: 497.43
 ---- batch: 030 ----
mean loss: 484.00
 ---- batch: 040 ----
mean loss: 479.27
 ---- batch: 050 ----
mean loss: 473.54
 ---- batch: 060 ----
mean loss: 490.64
 ---- batch: 070 ----
mean loss: 483.84
 ---- batch: 080 ----
mean loss: 486.23
 ---- batch: 090 ----
mean loss: 481.59
 ---- batch: 100 ----
mean loss: 475.63
 ---- batch: 110 ----
mean loss: 490.57
train mean loss: 485.39
epoch train time: 0:00:00.715008
elapsed time: 0:01:11.712115
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 01:38:12.469882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.10
 ---- batch: 020 ----
mean loss: 482.59
 ---- batch: 030 ----
mean loss: 481.28
 ---- batch: 040 ----
mean loss: 471.29
 ---- batch: 050 ----
mean loss: 463.48
 ---- batch: 060 ----
mean loss: 477.43
 ---- batch: 070 ----
mean loss: 476.96
 ---- batch: 080 ----
mean loss: 475.77
 ---- batch: 090 ----
mean loss: 459.70
 ---- batch: 100 ----
mean loss: 473.86
 ---- batch: 110 ----
mean loss: 467.81
train mean loss: 473.74
epoch train time: 0:00:00.721444
elapsed time: 0:01:12.433700
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 01:38:13.191465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.57
 ---- batch: 020 ----
mean loss: 466.75
 ---- batch: 030 ----
mean loss: 471.39
 ---- batch: 040 ----
mean loss: 468.70
 ---- batch: 050 ----
mean loss: 470.35
 ---- batch: 060 ----
mean loss: 445.24
 ---- batch: 070 ----
mean loss: 454.76
 ---- batch: 080 ----
mean loss: 455.28
 ---- batch: 090 ----
mean loss: 450.88
 ---- batch: 100 ----
mean loss: 469.65
 ---- batch: 110 ----
mean loss: 460.43
train mean loss: 462.13
epoch train time: 0:00:00.724271
elapsed time: 0:01:13.158124
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 01:38:13.915912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.20
 ---- batch: 020 ----
mean loss: 455.15
 ---- batch: 030 ----
mean loss: 466.37
 ---- batch: 040 ----
mean loss: 448.57
 ---- batch: 050 ----
mean loss: 447.49
 ---- batch: 060 ----
mean loss: 441.11
 ---- batch: 070 ----
mean loss: 443.78
 ---- batch: 080 ----
mean loss: 445.57
 ---- batch: 090 ----
mean loss: 453.62
 ---- batch: 100 ----
mean loss: 440.30
 ---- batch: 110 ----
mean loss: 448.42
train mean loss: 450.88
epoch train time: 0:00:00.726221
elapsed time: 0:01:13.884510
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 01:38:14.642291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.12
 ---- batch: 020 ----
mean loss: 445.67
 ---- batch: 030 ----
mean loss: 445.45
 ---- batch: 040 ----
mean loss: 441.85
 ---- batch: 050 ----
mean loss: 448.40
 ---- batch: 060 ----
mean loss: 452.91
 ---- batch: 070 ----
mean loss: 424.17
 ---- batch: 080 ----
mean loss: 427.80
 ---- batch: 090 ----
mean loss: 432.20
 ---- batch: 100 ----
mean loss: 431.05
 ---- batch: 110 ----
mean loss: 434.85
train mean loss: 439.93
epoch train time: 0:00:00.717340
elapsed time: 0:01:14.602020
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 01:38:15.359785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.78
 ---- batch: 020 ----
mean loss: 438.31
 ---- batch: 030 ----
mean loss: 423.12
 ---- batch: 040 ----
mean loss: 426.49
 ---- batch: 050 ----
mean loss: 426.92
 ---- batch: 060 ----
mean loss: 425.45
 ---- batch: 070 ----
mean loss: 427.87
 ---- batch: 080 ----
mean loss: 410.39
 ---- batch: 090 ----
mean loss: 436.05
 ---- batch: 100 ----
mean loss: 432.13
 ---- batch: 110 ----
mean loss: 433.41
train mean loss: 429.05
epoch train time: 0:00:00.731632
elapsed time: 0:01:15.333824
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 01:38:16.091627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.60
 ---- batch: 020 ----
mean loss: 423.86
 ---- batch: 030 ----
mean loss: 428.03
 ---- batch: 040 ----
mean loss: 422.80
 ---- batch: 050 ----
mean loss: 419.02
 ---- batch: 060 ----
mean loss: 402.18
 ---- batch: 070 ----
mean loss: 417.32
 ---- batch: 080 ----
mean loss: 419.21
 ---- batch: 090 ----
mean loss: 413.00
 ---- batch: 100 ----
mean loss: 416.97
 ---- batch: 110 ----
mean loss: 416.36
train mean loss: 417.66
epoch train time: 0:00:00.719737
elapsed time: 0:01:16.053741
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 01:38:16.811506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.49
 ---- batch: 020 ----
mean loss: 412.67
 ---- batch: 030 ----
mean loss: 416.88
 ---- batch: 040 ----
mean loss: 401.77
 ---- batch: 050 ----
mean loss: 400.66
 ---- batch: 060 ----
mean loss: 401.64
 ---- batch: 070 ----
mean loss: 387.82
 ---- batch: 080 ----
mean loss: 396.92
 ---- batch: 090 ----
mean loss: 408.59
 ---- batch: 100 ----
mean loss: 399.39
 ---- batch: 110 ----
mean loss: 395.88
train mean loss: 403.57
epoch train time: 0:00:00.712894
elapsed time: 0:01:16.766782
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 01:38:17.524564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.62
 ---- batch: 020 ----
mean loss: 386.79
 ---- batch: 030 ----
mean loss: 395.13
 ---- batch: 040 ----
mean loss: 385.79
 ---- batch: 050 ----
mean loss: 391.49
 ---- batch: 060 ----
mean loss: 370.43
 ---- batch: 070 ----
mean loss: 369.37
 ---- batch: 080 ----
mean loss: 378.96
 ---- batch: 090 ----
mean loss: 370.66
 ---- batch: 100 ----
mean loss: 375.29
 ---- batch: 110 ----
mean loss: 370.04
train mean loss: 380.93
epoch train time: 0:00:00.715056
elapsed time: 0:01:17.481994
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 01:38:18.239759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.71
 ---- batch: 020 ----
mean loss: 365.40
 ---- batch: 030 ----
mean loss: 360.90
 ---- batch: 040 ----
mean loss: 354.52
 ---- batch: 050 ----
mean loss: 358.42
 ---- batch: 060 ----
mean loss: 356.89
 ---- batch: 070 ----
mean loss: 360.21
 ---- batch: 080 ----
mean loss: 346.17
 ---- batch: 090 ----
mean loss: 353.72
 ---- batch: 100 ----
mean loss: 340.37
 ---- batch: 110 ----
mean loss: 353.20
train mean loss: 354.36
epoch train time: 0:00:00.720877
elapsed time: 0:01:18.203013
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 01:38:18.960779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.13
 ---- batch: 020 ----
mean loss: 333.76
 ---- batch: 030 ----
mean loss: 334.15
 ---- batch: 040 ----
mean loss: 344.36
 ---- batch: 050 ----
mean loss: 337.16
 ---- batch: 060 ----
mean loss: 331.39
 ---- batch: 070 ----
mean loss: 319.20
 ---- batch: 080 ----
mean loss: 333.50
 ---- batch: 090 ----
mean loss: 336.55
 ---- batch: 100 ----
mean loss: 331.47
 ---- batch: 110 ----
mean loss: 330.73
train mean loss: 334.24
epoch train time: 0:00:00.717751
elapsed time: 0:01:18.920906
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 01:38:19.678674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.16
 ---- batch: 020 ----
mean loss: 320.42
 ---- batch: 030 ----
mean loss: 322.16
 ---- batch: 040 ----
mean loss: 324.67
 ---- batch: 050 ----
mean loss: 314.35
 ---- batch: 060 ----
mean loss: 314.14
 ---- batch: 070 ----
mean loss: 328.57
 ---- batch: 080 ----
mean loss: 312.98
 ---- batch: 090 ----
mean loss: 312.97
 ---- batch: 100 ----
mean loss: 321.90
 ---- batch: 110 ----
mean loss: 332.82
train mean loss: 320.16
epoch train time: 0:00:00.717654
elapsed time: 0:01:19.638719
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 01:38:20.396484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.87
 ---- batch: 020 ----
mean loss: 313.90
 ---- batch: 030 ----
mean loss: 311.71
 ---- batch: 040 ----
mean loss: 293.53
 ---- batch: 050 ----
mean loss: 305.51
 ---- batch: 060 ----
mean loss: 307.85
 ---- batch: 070 ----
mean loss: 311.49
 ---- batch: 080 ----
mean loss: 310.73
 ---- batch: 090 ----
mean loss: 318.25
 ---- batch: 100 ----
mean loss: 303.98
 ---- batch: 110 ----
mean loss: 304.42
train mean loss: 308.96
epoch train time: 0:00:00.719985
elapsed time: 0:01:20.358851
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 01:38:21.116636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.23
 ---- batch: 020 ----
mean loss: 295.33
 ---- batch: 030 ----
mean loss: 300.67
 ---- batch: 040 ----
mean loss: 304.47
 ---- batch: 050 ----
mean loss: 310.55
 ---- batch: 060 ----
mean loss: 294.70
 ---- batch: 070 ----
mean loss: 297.39
 ---- batch: 080 ----
mean loss: 293.20
 ---- batch: 090 ----
mean loss: 294.08
 ---- batch: 100 ----
mean loss: 286.64
 ---- batch: 110 ----
mean loss: 299.41
train mean loss: 299.11
epoch train time: 0:00:00.715583
elapsed time: 0:01:21.074622
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 01:38:21.832398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.66
 ---- batch: 020 ----
mean loss: 295.71
 ---- batch: 030 ----
mean loss: 289.39
 ---- batch: 040 ----
mean loss: 289.87
 ---- batch: 050 ----
mean loss: 280.52
 ---- batch: 060 ----
mean loss: 304.11
 ---- batch: 070 ----
mean loss: 288.70
 ---- batch: 080 ----
mean loss: 297.78
 ---- batch: 090 ----
mean loss: 286.09
 ---- batch: 100 ----
mean loss: 289.75
 ---- batch: 110 ----
mean loss: 278.55
train mean loss: 290.21
epoch train time: 0:00:00.714153
elapsed time: 0:01:21.788926
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 01:38:22.546692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.31
 ---- batch: 020 ----
mean loss: 288.19
 ---- batch: 030 ----
mean loss: 275.44
 ---- batch: 040 ----
mean loss: 283.67
 ---- batch: 050 ----
mean loss: 284.33
 ---- batch: 060 ----
mean loss: 273.70
 ---- batch: 070 ----
mean loss: 273.67
 ---- batch: 080 ----
mean loss: 279.12
 ---- batch: 090 ----
mean loss: 278.82
 ---- batch: 100 ----
mean loss: 277.84
 ---- batch: 110 ----
mean loss: 287.04
train mean loss: 280.98
epoch train time: 0:00:00.723308
elapsed time: 0:01:22.512436
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 01:38:23.270239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.81
 ---- batch: 020 ----
mean loss: 280.83
 ---- batch: 030 ----
mean loss: 266.64
 ---- batch: 040 ----
mean loss: 277.21
 ---- batch: 050 ----
mean loss: 275.22
 ---- batch: 060 ----
mean loss: 282.43
 ---- batch: 070 ----
mean loss: 272.96
 ---- batch: 080 ----
mean loss: 263.42
 ---- batch: 090 ----
mean loss: 264.90
 ---- batch: 100 ----
mean loss: 258.13
 ---- batch: 110 ----
mean loss: 275.01
train mean loss: 271.59
epoch train time: 0:00:00.728069
elapsed time: 0:01:23.240684
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 01:38:23.998449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.42
 ---- batch: 020 ----
mean loss: 261.60
 ---- batch: 030 ----
mean loss: 267.97
 ---- batch: 040 ----
mean loss: 250.37
 ---- batch: 050 ----
mean loss: 262.66
 ---- batch: 060 ----
mean loss: 270.38
 ---- batch: 070 ----
mean loss: 261.00
 ---- batch: 080 ----
mean loss: 256.77
 ---- batch: 090 ----
mean loss: 261.62
 ---- batch: 100 ----
mean loss: 252.22
 ---- batch: 110 ----
mean loss: 261.71
train mean loss: 261.89
epoch train time: 0:00:00.715000
elapsed time: 0:01:23.955852
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 01:38:24.713637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.95
 ---- batch: 020 ----
mean loss: 255.93
 ---- batch: 030 ----
mean loss: 260.31
 ---- batch: 040 ----
mean loss: 259.97
 ---- batch: 050 ----
mean loss: 246.57
 ---- batch: 060 ----
mean loss: 247.29
 ---- batch: 070 ----
mean loss: 256.35
 ---- batch: 080 ----
mean loss: 251.33
 ---- batch: 090 ----
mean loss: 259.49
 ---- batch: 100 ----
mean loss: 254.16
 ---- batch: 110 ----
mean loss: 255.14
train mean loss: 254.48
epoch train time: 0:00:00.718824
elapsed time: 0:01:24.674885
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 01:38:25.432679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.49
 ---- batch: 020 ----
mean loss: 249.99
 ---- batch: 030 ----
mean loss: 255.36
 ---- batch: 040 ----
mean loss: 249.74
 ---- batch: 050 ----
mean loss: 231.02
 ---- batch: 060 ----
mean loss: 254.14
 ---- batch: 070 ----
mean loss: 255.38
 ---- batch: 080 ----
mean loss: 258.69
 ---- batch: 090 ----
mean loss: 246.72
 ---- batch: 100 ----
mean loss: 254.53
 ---- batch: 110 ----
mean loss: 247.28
train mean loss: 250.37
epoch train time: 0:00:00.720050
elapsed time: 0:01:25.395133
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 01:38:26.152899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.76
 ---- batch: 020 ----
mean loss: 244.31
 ---- batch: 030 ----
mean loss: 250.30
 ---- batch: 040 ----
mean loss: 255.63
 ---- batch: 050 ----
mean loss: 256.37
 ---- batch: 060 ----
mean loss: 244.09
 ---- batch: 070 ----
mean loss: 246.02
 ---- batch: 080 ----
mean loss: 245.29
 ---- batch: 090 ----
mean loss: 249.46
 ---- batch: 100 ----
mean loss: 249.47
 ---- batch: 110 ----
mean loss: 245.64
train mean loss: 247.35
epoch train time: 0:00:00.712278
elapsed time: 0:01:26.107552
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 01:38:26.865318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.12
 ---- batch: 020 ----
mean loss: 249.03
 ---- batch: 030 ----
mean loss: 241.71
 ---- batch: 040 ----
mean loss: 244.65
 ---- batch: 050 ----
mean loss: 238.56
 ---- batch: 060 ----
mean loss: 247.53
 ---- batch: 070 ----
mean loss: 244.62
 ---- batch: 080 ----
mean loss: 251.34
 ---- batch: 090 ----
mean loss: 244.68
 ---- batch: 100 ----
mean loss: 243.09
 ---- batch: 110 ----
mean loss: 242.20
train mean loss: 245.00
epoch train time: 0:00:00.722455
elapsed time: 0:01:26.830147
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 01:38:27.587912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.70
 ---- batch: 020 ----
mean loss: 240.10
 ---- batch: 030 ----
mean loss: 238.68
 ---- batch: 040 ----
mean loss: 241.28
 ---- batch: 050 ----
mean loss: 238.59
 ---- batch: 060 ----
mean loss: 249.92
 ---- batch: 070 ----
mean loss: 244.41
 ---- batch: 080 ----
mean loss: 244.32
 ---- batch: 090 ----
mean loss: 238.81
 ---- batch: 100 ----
mean loss: 245.91
 ---- batch: 110 ----
mean loss: 238.92
train mean loss: 242.99
epoch train time: 0:00:00.723690
elapsed time: 0:01:27.553976
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 01:38:28.311741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.62
 ---- batch: 020 ----
mean loss: 245.31
 ---- batch: 030 ----
mean loss: 251.11
 ---- batch: 040 ----
mean loss: 243.81
 ---- batch: 050 ----
mean loss: 245.93
 ---- batch: 060 ----
mean loss: 235.01
 ---- batch: 070 ----
mean loss: 236.35
 ---- batch: 080 ----
mean loss: 237.27
 ---- batch: 090 ----
mean loss: 241.52
 ---- batch: 100 ----
mean loss: 234.44
 ---- batch: 110 ----
mean loss: 240.34
train mean loss: 240.81
epoch train time: 0:00:00.718798
elapsed time: 0:01:28.272915
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 01:38:29.030679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.60
 ---- batch: 020 ----
mean loss: 237.84
 ---- batch: 030 ----
mean loss: 243.27
 ---- batch: 040 ----
mean loss: 242.18
 ---- batch: 050 ----
mean loss: 242.79
 ---- batch: 060 ----
mean loss: 237.63
 ---- batch: 070 ----
mean loss: 234.52
 ---- batch: 080 ----
mean loss: 238.29
 ---- batch: 090 ----
mean loss: 240.02
 ---- batch: 100 ----
mean loss: 235.89
 ---- batch: 110 ----
mean loss: 239.68
train mean loss: 238.96
epoch train time: 0:00:00.728554
elapsed time: 0:01:29.001625
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 01:38:29.759394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.07
 ---- batch: 020 ----
mean loss: 242.35
 ---- batch: 030 ----
mean loss: 231.70
 ---- batch: 040 ----
mean loss: 241.79
 ---- batch: 050 ----
mean loss: 245.94
 ---- batch: 060 ----
mean loss: 238.46
 ---- batch: 070 ----
mean loss: 231.45
 ---- batch: 080 ----
mean loss: 232.88
 ---- batch: 090 ----
mean loss: 245.45
 ---- batch: 100 ----
mean loss: 235.25
 ---- batch: 110 ----
mean loss: 229.35
train mean loss: 237.49
epoch train time: 0:00:00.722706
elapsed time: 0:01:29.724480
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 01:38:30.482265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.04
 ---- batch: 020 ----
mean loss: 236.76
 ---- batch: 030 ----
mean loss: 230.51
 ---- batch: 040 ----
mean loss: 239.58
 ---- batch: 050 ----
mean loss: 237.45
 ---- batch: 060 ----
mean loss: 241.57
 ---- batch: 070 ----
mean loss: 237.88
 ---- batch: 080 ----
mean loss: 243.55
 ---- batch: 090 ----
mean loss: 233.01
 ---- batch: 100 ----
mean loss: 229.63
 ---- batch: 110 ----
mean loss: 234.23
train mean loss: 236.01
epoch train time: 0:00:00.725032
elapsed time: 0:01:30.449694
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 01:38:31.207478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.03
 ---- batch: 020 ----
mean loss: 236.41
 ---- batch: 030 ----
mean loss: 231.72
 ---- batch: 040 ----
mean loss: 240.69
 ---- batch: 050 ----
mean loss: 236.96
 ---- batch: 060 ----
mean loss: 233.36
 ---- batch: 070 ----
mean loss: 236.15
 ---- batch: 080 ----
mean loss: 229.41
 ---- batch: 090 ----
mean loss: 243.96
 ---- batch: 100 ----
mean loss: 222.34
 ---- batch: 110 ----
mean loss: 241.71
train mean loss: 234.76
epoch train time: 0:00:00.722953
elapsed time: 0:01:31.172825
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 01:38:31.930598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.77
 ---- batch: 020 ----
mean loss: 239.46
 ---- batch: 030 ----
mean loss: 226.82
 ---- batch: 040 ----
mean loss: 230.63
 ---- batch: 050 ----
mean loss: 237.17
 ---- batch: 060 ----
mean loss: 235.28
 ---- batch: 070 ----
mean loss: 244.39
 ---- batch: 080 ----
mean loss: 228.08
 ---- batch: 090 ----
mean loss: 234.56
 ---- batch: 100 ----
mean loss: 233.83
 ---- batch: 110 ----
mean loss: 227.93
train mean loss: 233.59
epoch train time: 0:00:00.725769
elapsed time: 0:01:31.898746
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 01:38:32.656513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.61
 ---- batch: 020 ----
mean loss: 236.77
 ---- batch: 030 ----
mean loss: 228.74
 ---- batch: 040 ----
mean loss: 232.43
 ---- batch: 050 ----
mean loss: 230.22
 ---- batch: 060 ----
mean loss: 229.52
 ---- batch: 070 ----
mean loss: 230.81
 ---- batch: 080 ----
mean loss: 242.49
 ---- batch: 090 ----
mean loss: 234.21
 ---- batch: 100 ----
mean loss: 221.34
 ---- batch: 110 ----
mean loss: 241.62
train mean loss: 232.48
epoch train time: 0:00:00.724320
elapsed time: 0:01:32.623204
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 01:38:33.380975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.33
 ---- batch: 020 ----
mean loss: 237.13
 ---- batch: 030 ----
mean loss: 238.13
 ---- batch: 040 ----
mean loss: 240.84
 ---- batch: 050 ----
mean loss: 229.40
 ---- batch: 060 ----
mean loss: 232.56
 ---- batch: 070 ----
mean loss: 232.63
 ---- batch: 080 ----
mean loss: 231.15
 ---- batch: 090 ----
mean loss: 225.08
 ---- batch: 100 ----
mean loss: 229.67
 ---- batch: 110 ----
mean loss: 229.31
train mean loss: 231.75
epoch train time: 0:00:00.725794
elapsed time: 0:01:33.349145
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 01:38:34.106939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.40
 ---- batch: 020 ----
mean loss: 239.71
 ---- batch: 030 ----
mean loss: 225.49
 ---- batch: 040 ----
mean loss: 228.33
 ---- batch: 050 ----
mean loss: 232.30
 ---- batch: 060 ----
mean loss: 228.36
 ---- batch: 070 ----
mean loss: 220.08
 ---- batch: 080 ----
mean loss: 229.73
 ---- batch: 090 ----
mean loss: 229.41
 ---- batch: 100 ----
mean loss: 235.24
 ---- batch: 110 ----
mean loss: 230.38
train mean loss: 230.49
epoch train time: 0:00:00.727228
elapsed time: 0:01:34.076544
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 01:38:34.834313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.81
 ---- batch: 020 ----
mean loss: 231.49
 ---- batch: 030 ----
mean loss: 229.73
 ---- batch: 040 ----
mean loss: 224.19
 ---- batch: 050 ----
mean loss: 223.99
 ---- batch: 060 ----
mean loss: 229.72
 ---- batch: 070 ----
mean loss: 238.82
 ---- batch: 080 ----
mean loss: 239.07
 ---- batch: 090 ----
mean loss: 229.40
 ---- batch: 100 ----
mean loss: 230.41
 ---- batch: 110 ----
mean loss: 225.27
train mean loss: 229.66
epoch train time: 0:00:00.720529
elapsed time: 0:01:34.797218
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 01:38:35.555000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.78
 ---- batch: 020 ----
mean loss: 235.14
 ---- batch: 030 ----
mean loss: 221.10
 ---- batch: 040 ----
mean loss: 235.00
 ---- batch: 050 ----
mean loss: 230.35
 ---- batch: 060 ----
mean loss: 226.83
 ---- batch: 070 ----
mean loss: 227.88
 ---- batch: 080 ----
mean loss: 234.81
 ---- batch: 090 ----
mean loss: 229.16
 ---- batch: 100 ----
mean loss: 227.55
 ---- batch: 110 ----
mean loss: 225.75
train mean loss: 228.69
epoch train time: 0:00:00.724146
elapsed time: 0:01:35.521521
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 01:38:36.279285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.41
 ---- batch: 020 ----
mean loss: 240.97
 ---- batch: 030 ----
mean loss: 224.03
 ---- batch: 040 ----
mean loss: 224.98
 ---- batch: 050 ----
mean loss: 232.01
 ---- batch: 060 ----
mean loss: 223.63
 ---- batch: 070 ----
mean loss: 222.34
 ---- batch: 080 ----
mean loss: 221.70
 ---- batch: 090 ----
mean loss: 228.42
 ---- batch: 100 ----
mean loss: 228.38
 ---- batch: 110 ----
mean loss: 227.20
train mean loss: 227.86
epoch train time: 0:00:00.731397
elapsed time: 0:01:36.253060
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 01:38:37.010825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.57
 ---- batch: 020 ----
mean loss: 232.01
 ---- batch: 030 ----
mean loss: 223.71
 ---- batch: 040 ----
mean loss: 219.47
 ---- batch: 050 ----
mean loss: 225.15
 ---- batch: 060 ----
mean loss: 223.22
 ---- batch: 070 ----
mean loss: 228.55
 ---- batch: 080 ----
mean loss: 237.98
 ---- batch: 090 ----
mean loss: 226.69
 ---- batch: 100 ----
mean loss: 220.45
 ---- batch: 110 ----
mean loss: 226.46
train mean loss: 227.07
epoch train time: 0:00:00.719467
elapsed time: 0:01:36.972669
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 01:38:37.730445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.91
 ---- batch: 020 ----
mean loss: 224.06
 ---- batch: 030 ----
mean loss: 231.99
 ---- batch: 040 ----
mean loss: 228.10
 ---- batch: 050 ----
mean loss: 232.53
 ---- batch: 060 ----
mean loss: 220.66
 ---- batch: 070 ----
mean loss: 225.00
 ---- batch: 080 ----
mean loss: 229.35
 ---- batch: 090 ----
mean loss: 230.16
 ---- batch: 100 ----
mean loss: 227.81
 ---- batch: 110 ----
mean loss: 225.08
train mean loss: 226.17
epoch train time: 0:00:00.715776
elapsed time: 0:01:37.688602
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 01:38:38.446369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.65
 ---- batch: 020 ----
mean loss: 228.11
 ---- batch: 030 ----
mean loss: 228.23
 ---- batch: 040 ----
mean loss: 230.41
 ---- batch: 050 ----
mean loss: 220.66
 ---- batch: 060 ----
mean loss: 225.14
 ---- batch: 070 ----
mean loss: 225.65
 ---- batch: 080 ----
mean loss: 226.42
 ---- batch: 090 ----
mean loss: 222.65
 ---- batch: 100 ----
mean loss: 222.35
 ---- batch: 110 ----
mean loss: 222.21
train mean loss: 225.66
epoch train time: 0:00:00.717585
elapsed time: 0:01:38.406327
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 01:38:39.164105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.99
 ---- batch: 020 ----
mean loss: 222.18
 ---- batch: 030 ----
mean loss: 219.72
 ---- batch: 040 ----
mean loss: 225.31
 ---- batch: 050 ----
mean loss: 220.09
 ---- batch: 060 ----
mean loss: 230.59
 ---- batch: 070 ----
mean loss: 233.62
 ---- batch: 080 ----
mean loss: 228.76
 ---- batch: 090 ----
mean loss: 221.14
 ---- batch: 100 ----
mean loss: 225.06
 ---- batch: 110 ----
mean loss: 225.43
train mean loss: 224.70
epoch train time: 0:00:00.724953
elapsed time: 0:01:39.131434
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 01:38:39.889218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.05
 ---- batch: 020 ----
mean loss: 233.66
 ---- batch: 030 ----
mean loss: 232.04
 ---- batch: 040 ----
mean loss: 221.78
 ---- batch: 050 ----
mean loss: 226.41
 ---- batch: 060 ----
mean loss: 226.09
 ---- batch: 070 ----
mean loss: 220.38
 ---- batch: 080 ----
mean loss: 218.14
 ---- batch: 090 ----
mean loss: 222.47
 ---- batch: 100 ----
mean loss: 212.71
 ---- batch: 110 ----
mean loss: 227.50
train mean loss: 224.02
epoch train time: 0:00:00.718334
elapsed time: 0:01:39.849927
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 01:38:40.607695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.09
 ---- batch: 020 ----
mean loss: 229.20
 ---- batch: 030 ----
mean loss: 220.71
 ---- batch: 040 ----
mean loss: 228.72
 ---- batch: 050 ----
mean loss: 215.57
 ---- batch: 060 ----
mean loss: 221.38
 ---- batch: 070 ----
mean loss: 226.69
 ---- batch: 080 ----
mean loss: 219.80
 ---- batch: 090 ----
mean loss: 219.53
 ---- batch: 100 ----
mean loss: 219.95
 ---- batch: 110 ----
mean loss: 226.70
train mean loss: 223.29
epoch train time: 0:00:00.723334
elapsed time: 0:01:40.573403
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 01:38:41.331165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.39
 ---- batch: 020 ----
mean loss: 228.50
 ---- batch: 030 ----
mean loss: 222.55
 ---- batch: 040 ----
mean loss: 230.35
 ---- batch: 050 ----
mean loss: 219.31
 ---- batch: 060 ----
mean loss: 212.98
 ---- batch: 070 ----
mean loss: 222.60
 ---- batch: 080 ----
mean loss: 212.65
 ---- batch: 090 ----
mean loss: 229.28
 ---- batch: 100 ----
mean loss: 222.81
 ---- batch: 110 ----
mean loss: 224.70
train mean loss: 222.54
epoch train time: 0:00:00.717154
elapsed time: 0:01:41.290693
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 01:38:42.048468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.86
 ---- batch: 020 ----
mean loss: 230.94
 ---- batch: 030 ----
mean loss: 214.95
 ---- batch: 040 ----
mean loss: 216.69
 ---- batch: 050 ----
mean loss: 218.57
 ---- batch: 060 ----
mean loss: 219.51
 ---- batch: 070 ----
mean loss: 227.23
 ---- batch: 080 ----
mean loss: 230.91
 ---- batch: 090 ----
mean loss: 223.92
 ---- batch: 100 ----
mean loss: 221.91
 ---- batch: 110 ----
mean loss: 218.30
train mean loss: 221.96
epoch train time: 0:00:00.715428
elapsed time: 0:01:42.006284
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 01:38:42.764047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.67
 ---- batch: 020 ----
mean loss: 223.74
 ---- batch: 030 ----
mean loss: 224.16
 ---- batch: 040 ----
mean loss: 220.29
 ---- batch: 050 ----
mean loss: 223.90
 ---- batch: 060 ----
mean loss: 221.39
 ---- batch: 070 ----
mean loss: 223.49
 ---- batch: 080 ----
mean loss: 222.66
 ---- batch: 090 ----
mean loss: 223.39
 ---- batch: 100 ----
mean loss: 217.85
 ---- batch: 110 ----
mean loss: 214.42
train mean loss: 221.07
epoch train time: 0:00:00.720833
elapsed time: 0:01:42.727277
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 01:38:43.485046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.01
 ---- batch: 020 ----
mean loss: 223.76
 ---- batch: 030 ----
mean loss: 223.54
 ---- batch: 040 ----
mean loss: 220.90
 ---- batch: 050 ----
mean loss: 226.81
 ---- batch: 060 ----
mean loss: 216.41
 ---- batch: 070 ----
mean loss: 211.66
 ---- batch: 080 ----
mean loss: 214.45
 ---- batch: 090 ----
mean loss: 223.01
 ---- batch: 100 ----
mean loss: 218.79
 ---- batch: 110 ----
mean loss: 219.69
train mean loss: 220.54
epoch train time: 0:00:00.720899
elapsed time: 0:01:43.448325
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 01:38:44.206092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.74
 ---- batch: 020 ----
mean loss: 212.04
 ---- batch: 030 ----
mean loss: 202.41
 ---- batch: 040 ----
mean loss: 224.02
 ---- batch: 050 ----
mean loss: 236.03
 ---- batch: 060 ----
mean loss: 225.60
 ---- batch: 070 ----
mean loss: 219.02
 ---- batch: 080 ----
mean loss: 220.83
 ---- batch: 090 ----
mean loss: 216.09
 ---- batch: 100 ----
mean loss: 221.59
 ---- batch: 110 ----
mean loss: 224.39
train mean loss: 219.71
epoch train time: 0:00:00.731009
elapsed time: 0:01:44.179479
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 01:38:44.937268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.49
 ---- batch: 020 ----
mean loss: 211.37
 ---- batch: 030 ----
mean loss: 219.20
 ---- batch: 040 ----
mean loss: 221.35
 ---- batch: 050 ----
mean loss: 231.13
 ---- batch: 060 ----
mean loss: 214.97
 ---- batch: 070 ----
mean loss: 215.71
 ---- batch: 080 ----
mean loss: 226.37
 ---- batch: 090 ----
mean loss: 218.93
 ---- batch: 100 ----
mean loss: 222.81
 ---- batch: 110 ----
mean loss: 218.94
train mean loss: 219.13
epoch train time: 0:00:00.722593
elapsed time: 0:01:44.902239
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 01:38:45.660006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.30
 ---- batch: 020 ----
mean loss: 220.54
 ---- batch: 030 ----
mean loss: 217.99
 ---- batch: 040 ----
mean loss: 208.84
 ---- batch: 050 ----
mean loss: 217.73
 ---- batch: 060 ----
mean loss: 222.94
 ---- batch: 070 ----
mean loss: 217.94
 ---- batch: 080 ----
mean loss: 232.34
 ---- batch: 090 ----
mean loss: 223.34
 ---- batch: 100 ----
mean loss: 217.23
 ---- batch: 110 ----
mean loss: 211.76
train mean loss: 218.30
epoch train time: 0:00:00.723144
elapsed time: 0:01:45.625533
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 01:38:46.383299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.42
 ---- batch: 020 ----
mean loss: 224.93
 ---- batch: 030 ----
mean loss: 211.62
 ---- batch: 040 ----
mean loss: 225.28
 ---- batch: 050 ----
mean loss: 208.43
 ---- batch: 060 ----
mean loss: 220.86
 ---- batch: 070 ----
mean loss: 208.08
 ---- batch: 080 ----
mean loss: 209.96
 ---- batch: 090 ----
mean loss: 214.76
 ---- batch: 100 ----
mean loss: 220.07
 ---- batch: 110 ----
mean loss: 226.40
train mean loss: 217.65
epoch train time: 0:00:00.722578
elapsed time: 0:01:46.348268
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 01:38:47.106036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.45
 ---- batch: 020 ----
mean loss: 217.32
 ---- batch: 030 ----
mean loss: 224.71
 ---- batch: 040 ----
mean loss: 213.22
 ---- batch: 050 ----
mean loss: 223.22
 ---- batch: 060 ----
mean loss: 218.95
 ---- batch: 070 ----
mean loss: 217.24
 ---- batch: 080 ----
mean loss: 215.27
 ---- batch: 090 ----
mean loss: 209.30
 ---- batch: 100 ----
mean loss: 221.47
 ---- batch: 110 ----
mean loss: 216.56
train mean loss: 217.02
epoch train time: 0:00:00.724119
elapsed time: 0:01:47.072533
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 01:38:47.830297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.53
 ---- batch: 020 ----
mean loss: 214.84
 ---- batch: 030 ----
mean loss: 214.64
 ---- batch: 040 ----
mean loss: 213.41
 ---- batch: 050 ----
mean loss: 221.15
 ---- batch: 060 ----
mean loss: 212.15
 ---- batch: 070 ----
mean loss: 217.30
 ---- batch: 080 ----
mean loss: 222.45
 ---- batch: 090 ----
mean loss: 216.67
 ---- batch: 100 ----
mean loss: 207.83
 ---- batch: 110 ----
mean loss: 218.87
train mean loss: 216.50
epoch train time: 0:00:00.711532
elapsed time: 0:01:47.784206
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 01:38:48.541978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.93
 ---- batch: 020 ----
mean loss: 215.56
 ---- batch: 030 ----
mean loss: 222.96
 ---- batch: 040 ----
mean loss: 219.68
 ---- batch: 050 ----
mean loss: 213.57
 ---- batch: 060 ----
mean loss: 217.67
 ---- batch: 070 ----
mean loss: 209.74
 ---- batch: 080 ----
mean loss: 219.59
 ---- batch: 090 ----
mean loss: 209.81
 ---- batch: 100 ----
mean loss: 214.21
 ---- batch: 110 ----
mean loss: 211.81
train mean loss: 215.80
epoch train time: 0:00:00.716943
elapsed time: 0:01:48.501296
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 01:38:49.259079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.16
 ---- batch: 020 ----
mean loss: 210.82
 ---- batch: 030 ----
mean loss: 216.87
 ---- batch: 040 ----
mean loss: 212.47
 ---- batch: 050 ----
mean loss: 205.75
 ---- batch: 060 ----
mean loss: 219.17
 ---- batch: 070 ----
mean loss: 211.75
 ---- batch: 080 ----
mean loss: 211.78
 ---- batch: 090 ----
mean loss: 210.63
 ---- batch: 100 ----
mean loss: 218.49
 ---- batch: 110 ----
mean loss: 224.07
train mean loss: 215.13
epoch train time: 0:00:00.716980
elapsed time: 0:01:49.218435
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 01:38:49.976218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.56
 ---- batch: 020 ----
mean loss: 218.08
 ---- batch: 030 ----
mean loss: 212.92
 ---- batch: 040 ----
mean loss: 211.25
 ---- batch: 050 ----
mean loss: 212.74
 ---- batch: 060 ----
mean loss: 209.11
 ---- batch: 070 ----
mean loss: 219.75
 ---- batch: 080 ----
mean loss: 210.64
 ---- batch: 090 ----
mean loss: 214.49
 ---- batch: 100 ----
mean loss: 219.26
 ---- batch: 110 ----
mean loss: 224.69
train mean loss: 214.59
epoch train time: 0:00:00.714354
elapsed time: 0:01:49.932949
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 01:38:50.690715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.38
 ---- batch: 020 ----
mean loss: 212.52
 ---- batch: 030 ----
mean loss: 213.05
 ---- batch: 040 ----
mean loss: 209.02
 ---- batch: 050 ----
mean loss: 207.73
 ---- batch: 060 ----
mean loss: 217.13
 ---- batch: 070 ----
mean loss: 214.96
 ---- batch: 080 ----
mean loss: 211.37
 ---- batch: 090 ----
mean loss: 215.17
 ---- batch: 100 ----
mean loss: 222.31
 ---- batch: 110 ----
mean loss: 208.39
train mean loss: 214.01
epoch train time: 0:00:00.717276
elapsed time: 0:01:50.650379
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 01:38:51.408136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.74
 ---- batch: 020 ----
mean loss: 222.92
 ---- batch: 030 ----
mean loss: 206.64
 ---- batch: 040 ----
mean loss: 212.21
 ---- batch: 050 ----
mean loss: 216.03
 ---- batch: 060 ----
mean loss: 210.88
 ---- batch: 070 ----
mean loss: 211.08
 ---- batch: 080 ----
mean loss: 216.82
 ---- batch: 090 ----
mean loss: 219.36
 ---- batch: 100 ----
mean loss: 205.28
 ---- batch: 110 ----
mean loss: 216.46
train mean loss: 213.53
epoch train time: 0:00:00.724328
elapsed time: 0:01:51.374839
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 01:38:52.132604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.14
 ---- batch: 020 ----
mean loss: 215.14
 ---- batch: 030 ----
mean loss: 217.39
 ---- batch: 040 ----
mean loss: 210.80
 ---- batch: 050 ----
mean loss: 218.75
 ---- batch: 060 ----
mean loss: 205.63
 ---- batch: 070 ----
mean loss: 220.76
 ---- batch: 080 ----
mean loss: 212.56
 ---- batch: 090 ----
mean loss: 219.19
 ---- batch: 100 ----
mean loss: 198.43
 ---- batch: 110 ----
mean loss: 208.01
train mean loss: 212.87
epoch train time: 0:00:00.721709
elapsed time: 0:01:52.096689
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 01:38:52.854477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.30
 ---- batch: 020 ----
mean loss: 217.10
 ---- batch: 030 ----
mean loss: 199.84
 ---- batch: 040 ----
mean loss: 210.98
 ---- batch: 050 ----
mean loss: 211.90
 ---- batch: 060 ----
mean loss: 212.23
 ---- batch: 070 ----
mean loss: 212.77
 ---- batch: 080 ----
mean loss: 210.02
 ---- batch: 090 ----
mean loss: 216.41
 ---- batch: 100 ----
mean loss: 217.54
 ---- batch: 110 ----
mean loss: 215.53
train mean loss: 212.40
epoch train time: 0:00:00.720512
elapsed time: 0:01:52.817364
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 01:38:53.575138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.10
 ---- batch: 020 ----
mean loss: 206.54
 ---- batch: 030 ----
mean loss: 218.36
 ---- batch: 040 ----
mean loss: 204.45
 ---- batch: 050 ----
mean loss: 211.86
 ---- batch: 060 ----
mean loss: 208.33
 ---- batch: 070 ----
mean loss: 220.54
 ---- batch: 080 ----
mean loss: 212.02
 ---- batch: 090 ----
mean loss: 206.45
 ---- batch: 100 ----
mean loss: 214.33
 ---- batch: 110 ----
mean loss: 216.88
train mean loss: 211.91
epoch train time: 0:00:00.720770
elapsed time: 0:01:53.538290
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 01:38:54.296074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.06
 ---- batch: 020 ----
mean loss: 212.03
 ---- batch: 030 ----
mean loss: 209.83
 ---- batch: 040 ----
mean loss: 200.22
 ---- batch: 050 ----
mean loss: 219.32
 ---- batch: 060 ----
mean loss: 207.56
 ---- batch: 070 ----
mean loss: 213.65
 ---- batch: 080 ----
mean loss: 220.74
 ---- batch: 090 ----
mean loss: 208.86
 ---- batch: 100 ----
mean loss: 205.41
 ---- batch: 110 ----
mean loss: 217.61
train mean loss: 211.49
epoch train time: 0:00:00.715256
elapsed time: 0:01:54.253709
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 01:38:55.011474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.56
 ---- batch: 020 ----
mean loss: 205.36
 ---- batch: 030 ----
mean loss: 216.45
 ---- batch: 040 ----
mean loss: 209.62
 ---- batch: 050 ----
mean loss: 203.94
 ---- batch: 060 ----
mean loss: 215.51
 ---- batch: 070 ----
mean loss: 211.11
 ---- batch: 080 ----
mean loss: 217.44
 ---- batch: 090 ----
mean loss: 216.77
 ---- batch: 100 ----
mean loss: 199.00
 ---- batch: 110 ----
mean loss: 210.95
train mean loss: 210.97
epoch train time: 0:00:00.713503
elapsed time: 0:01:54.967352
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 01:38:55.725115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.28
 ---- batch: 020 ----
mean loss: 201.85
 ---- batch: 030 ----
mean loss: 212.45
 ---- batch: 040 ----
mean loss: 198.21
 ---- batch: 050 ----
mean loss: 211.99
 ---- batch: 060 ----
mean loss: 214.48
 ---- batch: 070 ----
mean loss: 210.30
 ---- batch: 080 ----
mean loss: 216.85
 ---- batch: 090 ----
mean loss: 210.90
 ---- batch: 100 ----
mean loss: 212.38
 ---- batch: 110 ----
mean loss: 215.36
train mean loss: 210.44
epoch train time: 0:00:00.724709
elapsed time: 0:01:55.692203
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 01:38:56.449985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.01
 ---- batch: 020 ----
mean loss: 220.08
 ---- batch: 030 ----
mean loss: 204.27
 ---- batch: 040 ----
mean loss: 209.95
 ---- batch: 050 ----
mean loss: 219.38
 ---- batch: 060 ----
mean loss: 224.74
 ---- batch: 070 ----
mean loss: 210.41
 ---- batch: 080 ----
mean loss: 199.52
 ---- batch: 090 ----
mean loss: 204.88
 ---- batch: 100 ----
mean loss: 206.12
 ---- batch: 110 ----
mean loss: 206.17
train mean loss: 210.26
epoch train time: 0:00:00.720162
elapsed time: 0:01:56.412526
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 01:38:57.170283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.46
 ---- batch: 020 ----
mean loss: 218.18
 ---- batch: 030 ----
mean loss: 210.04
 ---- batch: 040 ----
mean loss: 202.95
 ---- batch: 050 ----
mean loss: 209.70
 ---- batch: 060 ----
mean loss: 207.16
 ---- batch: 070 ----
mean loss: 205.37
 ---- batch: 080 ----
mean loss: 215.68
 ---- batch: 090 ----
mean loss: 213.47
 ---- batch: 100 ----
mean loss: 199.96
 ---- batch: 110 ----
mean loss: 204.86
train mean loss: 209.59
epoch train time: 0:00:00.721551
elapsed time: 0:01:57.134233
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 01:38:57.892000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.22
 ---- batch: 020 ----
mean loss: 210.59
 ---- batch: 030 ----
mean loss: 207.98
 ---- batch: 040 ----
mean loss: 207.44
 ---- batch: 050 ----
mean loss: 205.45
 ---- batch: 060 ----
mean loss: 208.27
 ---- batch: 070 ----
mean loss: 218.94
 ---- batch: 080 ----
mean loss: 211.26
 ---- batch: 090 ----
mean loss: 208.40
 ---- batch: 100 ----
mean loss: 212.79
 ---- batch: 110 ----
mean loss: 204.38
train mean loss: 209.24
epoch train time: 0:00:00.715774
elapsed time: 0:01:57.850147
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 01:38:58.607928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.86
 ---- batch: 020 ----
mean loss: 212.08
 ---- batch: 030 ----
mean loss: 210.71
 ---- batch: 040 ----
mean loss: 207.26
 ---- batch: 050 ----
mean loss: 205.79
 ---- batch: 060 ----
mean loss: 206.13
 ---- batch: 070 ----
mean loss: 207.72
 ---- batch: 080 ----
mean loss: 216.78
 ---- batch: 090 ----
mean loss: 207.54
 ---- batch: 100 ----
mean loss: 209.19
 ---- batch: 110 ----
mean loss: 200.63
train mean loss: 208.68
epoch train time: 0:00:00.720306
elapsed time: 0:01:58.570608
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 01:38:59.328374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.88
 ---- batch: 020 ----
mean loss: 207.48
 ---- batch: 030 ----
mean loss: 209.47
 ---- batch: 040 ----
mean loss: 205.92
 ---- batch: 050 ----
mean loss: 207.52
 ---- batch: 060 ----
mean loss: 201.91
 ---- batch: 070 ----
mean loss: 203.50
 ---- batch: 080 ----
mean loss: 213.06
 ---- batch: 090 ----
mean loss: 214.19
 ---- batch: 100 ----
mean loss: 210.47
 ---- batch: 110 ----
mean loss: 209.49
train mean loss: 208.17
epoch train time: 0:00:00.723909
elapsed time: 0:01:59.294660
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 01:39:00.052435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.10
 ---- batch: 020 ----
mean loss: 202.37
 ---- batch: 030 ----
mean loss: 205.31
 ---- batch: 040 ----
mean loss: 210.68
 ---- batch: 050 ----
mean loss: 210.16
 ---- batch: 060 ----
mean loss: 217.07
 ---- batch: 070 ----
mean loss: 204.93
 ---- batch: 080 ----
mean loss: 207.26
 ---- batch: 090 ----
mean loss: 214.65
 ---- batch: 100 ----
mean loss: 206.11
 ---- batch: 110 ----
mean loss: 207.33
train mean loss: 208.01
epoch train time: 0:00:00.716950
elapsed time: 0:02:00.011760
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 01:39:00.769527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.74
 ---- batch: 020 ----
mean loss: 195.70
 ---- batch: 030 ----
mean loss: 208.86
 ---- batch: 040 ----
mean loss: 208.95
 ---- batch: 050 ----
mean loss: 204.33
 ---- batch: 060 ----
mean loss: 205.86
 ---- batch: 070 ----
mean loss: 211.49
 ---- batch: 080 ----
mean loss: 197.87
 ---- batch: 090 ----
mean loss: 214.50
 ---- batch: 100 ----
mean loss: 208.58
 ---- batch: 110 ----
mean loss: 211.51
train mean loss: 207.51
epoch train time: 0:00:00.718572
elapsed time: 0:02:00.730472
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 01:39:01.488253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.88
 ---- batch: 020 ----
mean loss: 204.22
 ---- batch: 030 ----
mean loss: 206.06
 ---- batch: 040 ----
mean loss: 211.44
 ---- batch: 050 ----
mean loss: 201.83
 ---- batch: 060 ----
mean loss: 206.91
 ---- batch: 070 ----
mean loss: 206.51
 ---- batch: 080 ----
mean loss: 207.93
 ---- batch: 090 ----
mean loss: 209.23
 ---- batch: 100 ----
mean loss: 211.93
 ---- batch: 110 ----
mean loss: 204.82
train mean loss: 207.03
epoch train time: 0:00:00.726131
elapsed time: 0:02:01.456784
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 01:39:02.214565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.27
 ---- batch: 020 ----
mean loss: 213.45
 ---- batch: 030 ----
mean loss: 200.83
 ---- batch: 040 ----
mean loss: 207.83
 ---- batch: 050 ----
mean loss: 216.30
 ---- batch: 060 ----
mean loss: 206.37
 ---- batch: 070 ----
mean loss: 211.31
 ---- batch: 080 ----
mean loss: 208.92
 ---- batch: 090 ----
mean loss: 193.96
 ---- batch: 100 ----
mean loss: 209.88
 ---- batch: 110 ----
mean loss: 199.01
train mean loss: 206.77
epoch train time: 0:00:00.720224
elapsed time: 0:02:02.177166
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 01:39:02.934940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.78
 ---- batch: 020 ----
mean loss: 198.58
 ---- batch: 030 ----
mean loss: 204.24
 ---- batch: 040 ----
mean loss: 208.75
 ---- batch: 050 ----
mean loss: 207.33
 ---- batch: 060 ----
mean loss: 207.65
 ---- batch: 070 ----
mean loss: 207.41
 ---- batch: 080 ----
mean loss: 209.36
 ---- batch: 090 ----
mean loss: 214.93
 ---- batch: 100 ----
mean loss: 203.65
 ---- batch: 110 ----
mean loss: 199.21
train mean loss: 206.14
epoch train time: 0:00:00.745149
elapsed time: 0:02:02.922481
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 01:39:03.680265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.19
 ---- batch: 020 ----
mean loss: 201.64
 ---- batch: 030 ----
mean loss: 206.50
 ---- batch: 040 ----
mean loss: 212.36
 ---- batch: 050 ----
mean loss: 213.56
 ---- batch: 060 ----
mean loss: 207.93
 ---- batch: 070 ----
mean loss: 210.26
 ---- batch: 080 ----
mean loss: 200.03
 ---- batch: 090 ----
mean loss: 206.90
 ---- batch: 100 ----
mean loss: 196.33
 ---- batch: 110 ----
mean loss: 203.52
train mean loss: 205.93
epoch train time: 0:00:00.735349
elapsed time: 0:02:03.657992
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 01:39:04.415758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.13
 ---- batch: 020 ----
mean loss: 200.60
 ---- batch: 030 ----
mean loss: 209.25
 ---- batch: 040 ----
mean loss: 214.86
 ---- batch: 050 ----
mean loss: 197.24
 ---- batch: 060 ----
mean loss: 206.02
 ---- batch: 070 ----
mean loss: 212.68
 ---- batch: 080 ----
mean loss: 211.02
 ---- batch: 090 ----
mean loss: 198.78
 ---- batch: 100 ----
mean loss: 198.74
 ---- batch: 110 ----
mean loss: 206.86
train mean loss: 205.58
epoch train time: 0:00:00.740373
elapsed time: 0:02:04.398524
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 01:39:05.156294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.51
 ---- batch: 020 ----
mean loss: 201.30
 ---- batch: 030 ----
mean loss: 202.21
 ---- batch: 040 ----
mean loss: 208.30
 ---- batch: 050 ----
mean loss: 206.18
 ---- batch: 060 ----
mean loss: 203.71
 ---- batch: 070 ----
mean loss: 206.71
 ---- batch: 080 ----
mean loss: 208.96
 ---- batch: 090 ----
mean loss: 204.12
 ---- batch: 100 ----
mean loss: 205.51
 ---- batch: 110 ----
mean loss: 202.18
train mean loss: 205.31
epoch train time: 0:00:00.743140
elapsed time: 0:02:05.141836
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 01:39:05.899592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.60
 ---- batch: 020 ----
mean loss: 201.12
 ---- batch: 030 ----
mean loss: 207.57
 ---- batch: 040 ----
mean loss: 189.37
 ---- batch: 050 ----
mean loss: 217.78
 ---- batch: 060 ----
mean loss: 208.45
 ---- batch: 070 ----
mean loss: 203.56
 ---- batch: 080 ----
mean loss: 204.37
 ---- batch: 090 ----
mean loss: 204.96
 ---- batch: 100 ----
mean loss: 203.84
 ---- batch: 110 ----
mean loss: 206.32
train mean loss: 204.87
epoch train time: 0:00:00.735440
elapsed time: 0:02:05.877409
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 01:39:06.635217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.92
 ---- batch: 020 ----
mean loss: 197.65
 ---- batch: 030 ----
mean loss: 203.29
 ---- batch: 040 ----
mean loss: 203.54
 ---- batch: 050 ----
mean loss: 213.33
 ---- batch: 060 ----
mean loss: 204.39
 ---- batch: 070 ----
mean loss: 207.80
 ---- batch: 080 ----
mean loss: 203.86
 ---- batch: 090 ----
mean loss: 198.52
 ---- batch: 100 ----
mean loss: 212.33
 ---- batch: 110 ----
mean loss: 193.51
train mean loss: 204.65
epoch train time: 0:00:00.737247
elapsed time: 0:02:06.614878
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 01:39:07.372665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.39
 ---- batch: 020 ----
mean loss: 213.13
 ---- batch: 030 ----
mean loss: 203.08
 ---- batch: 040 ----
mean loss: 203.91
 ---- batch: 050 ----
mean loss: 208.58
 ---- batch: 060 ----
mean loss: 207.56
 ---- batch: 070 ----
mean loss: 193.23
 ---- batch: 080 ----
mean loss: 204.08
 ---- batch: 090 ----
mean loss: 203.08
 ---- batch: 100 ----
mean loss: 201.91
 ---- batch: 110 ----
mean loss: 208.21
train mean loss: 204.25
epoch train time: 0:00:00.735769
elapsed time: 0:02:07.350807
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 01:39:08.108572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.00
 ---- batch: 020 ----
mean loss: 203.29
 ---- batch: 030 ----
mean loss: 202.70
 ---- batch: 040 ----
mean loss: 211.16
 ---- batch: 050 ----
mean loss: 197.72
 ---- batch: 060 ----
mean loss: 201.50
 ---- batch: 070 ----
mean loss: 210.07
 ---- batch: 080 ----
mean loss: 207.50
 ---- batch: 090 ----
mean loss: 207.33
 ---- batch: 100 ----
mean loss: 201.82
 ---- batch: 110 ----
mean loss: 199.34
train mean loss: 203.86
epoch train time: 0:00:00.734422
elapsed time: 0:02:08.085373
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 01:39:08.843172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.96
 ---- batch: 020 ----
mean loss: 201.03
 ---- batch: 030 ----
mean loss: 210.19
 ---- batch: 040 ----
mean loss: 204.94
 ---- batch: 050 ----
mean loss: 204.50
 ---- batch: 060 ----
mean loss: 201.89
 ---- batch: 070 ----
mean loss: 204.81
 ---- batch: 080 ----
mean loss: 203.38
 ---- batch: 090 ----
mean loss: 197.86
 ---- batch: 100 ----
mean loss: 207.82
 ---- batch: 110 ----
mean loss: 203.06
train mean loss: 203.80
epoch train time: 0:00:00.721848
elapsed time: 0:02:08.807414
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 01:39:09.565458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.40
 ---- batch: 020 ----
mean loss: 208.59
 ---- batch: 030 ----
mean loss: 208.69
 ---- batch: 040 ----
mean loss: 208.32
 ---- batch: 050 ----
mean loss: 200.52
 ---- batch: 060 ----
mean loss: 204.31
 ---- batch: 070 ----
mean loss: 207.21
 ---- batch: 080 ----
mean loss: 198.16
 ---- batch: 090 ----
mean loss: 194.02
 ---- batch: 100 ----
mean loss: 202.70
 ---- batch: 110 ----
mean loss: 198.84
train mean loss: 203.17
epoch train time: 0:00:00.717889
elapsed time: 0:02:09.525720
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 01:39:10.283484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.03
 ---- batch: 020 ----
mean loss: 199.86
 ---- batch: 030 ----
mean loss: 206.19
 ---- batch: 040 ----
mean loss: 195.16
 ---- batch: 050 ----
mean loss: 200.33
 ---- batch: 060 ----
mean loss: 196.68
 ---- batch: 070 ----
mean loss: 201.44
 ---- batch: 080 ----
mean loss: 201.68
 ---- batch: 090 ----
mean loss: 211.81
 ---- batch: 100 ----
mean loss: 193.97
 ---- batch: 110 ----
mean loss: 210.03
train mean loss: 202.93
epoch train time: 0:00:00.716415
elapsed time: 0:02:10.242293
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 01:39:11.000061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.92
 ---- batch: 020 ----
mean loss: 205.51
 ---- batch: 030 ----
mean loss: 211.02
 ---- batch: 040 ----
mean loss: 205.24
 ---- batch: 050 ----
mean loss: 203.94
 ---- batch: 060 ----
mean loss: 202.55
 ---- batch: 070 ----
mean loss: 198.22
 ---- batch: 080 ----
mean loss: 198.35
 ---- batch: 090 ----
mean loss: 197.61
 ---- batch: 100 ----
mean loss: 206.85
 ---- batch: 110 ----
mean loss: 197.88
train mean loss: 202.79
epoch train time: 0:00:00.715141
elapsed time: 0:02:10.957583
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 01:39:11.715366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.31
 ---- batch: 020 ----
mean loss: 210.68
 ---- batch: 030 ----
mean loss: 207.37
 ---- batch: 040 ----
mean loss: 203.12
 ---- batch: 050 ----
mean loss: 200.16
 ---- batch: 060 ----
mean loss: 203.30
 ---- batch: 070 ----
mean loss: 197.90
 ---- batch: 080 ----
mean loss: 201.84
 ---- batch: 090 ----
mean loss: 206.77
 ---- batch: 100 ----
mean loss: 196.96
 ---- batch: 110 ----
mean loss: 200.48
train mean loss: 202.49
epoch train time: 0:00:00.730757
elapsed time: 0:02:11.688517
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 01:39:12.446282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.93
 ---- batch: 020 ----
mean loss: 207.55
 ---- batch: 030 ----
mean loss: 204.14
 ---- batch: 040 ----
mean loss: 201.19
 ---- batch: 050 ----
mean loss: 197.40
 ---- batch: 060 ----
mean loss: 198.18
 ---- batch: 070 ----
mean loss: 205.69
 ---- batch: 080 ----
mean loss: 200.12
 ---- batch: 090 ----
mean loss: 204.56
 ---- batch: 100 ----
mean loss: 195.95
 ---- batch: 110 ----
mean loss: 195.09
train mean loss: 202.11
epoch train time: 0:00:00.717722
elapsed time: 0:02:12.406376
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 01:39:13.164163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.90
 ---- batch: 020 ----
mean loss: 204.71
 ---- batch: 030 ----
mean loss: 200.91
 ---- batch: 040 ----
mean loss: 203.28
 ---- batch: 050 ----
mean loss: 206.18
 ---- batch: 060 ----
mean loss: 205.35
 ---- batch: 070 ----
mean loss: 195.71
 ---- batch: 080 ----
mean loss: 205.41
 ---- batch: 090 ----
mean loss: 191.96
 ---- batch: 100 ----
mean loss: 209.18
 ---- batch: 110 ----
mean loss: 206.03
train mean loss: 201.76
epoch train time: 0:00:00.724290
elapsed time: 0:02:13.130853
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 01:39:13.888644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.47
 ---- batch: 020 ----
mean loss: 209.10
 ---- batch: 030 ----
mean loss: 196.39
 ---- batch: 040 ----
mean loss: 207.02
 ---- batch: 050 ----
mean loss: 192.00
 ---- batch: 060 ----
mean loss: 207.01
 ---- batch: 070 ----
mean loss: 191.92
 ---- batch: 080 ----
mean loss: 201.68
 ---- batch: 090 ----
mean loss: 195.00
 ---- batch: 100 ----
mean loss: 207.04
 ---- batch: 110 ----
mean loss: 195.51
train mean loss: 201.47
epoch train time: 0:00:00.713262
elapsed time: 0:02:13.844287
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 01:39:14.602052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.45
 ---- batch: 020 ----
mean loss: 210.74
 ---- batch: 030 ----
mean loss: 201.48
 ---- batch: 040 ----
mean loss: 208.62
 ---- batch: 050 ----
mean loss: 204.05
 ---- batch: 060 ----
mean loss: 206.14
 ---- batch: 070 ----
mean loss: 195.51
 ---- batch: 080 ----
mean loss: 194.42
 ---- batch: 090 ----
mean loss: 200.39
 ---- batch: 100 ----
mean loss: 189.53
 ---- batch: 110 ----
mean loss: 205.59
train mean loss: 201.23
epoch train time: 0:00:00.718132
elapsed time: 0:02:14.562557
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 01:39:15.320321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.12
 ---- batch: 020 ----
mean loss: 201.84
 ---- batch: 030 ----
mean loss: 211.57
 ---- batch: 040 ----
mean loss: 207.31
 ---- batch: 050 ----
mean loss: 207.22
 ---- batch: 060 ----
mean loss: 195.14
 ---- batch: 070 ----
mean loss: 199.97
 ---- batch: 080 ----
mean loss: 200.84
 ---- batch: 090 ----
mean loss: 208.44
 ---- batch: 100 ----
mean loss: 196.93
 ---- batch: 110 ----
mean loss: 193.50
train mean loss: 201.12
epoch train time: 0:00:00.721603
elapsed time: 0:02:15.284303
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 01:39:16.042068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.58
 ---- batch: 020 ----
mean loss: 207.32
 ---- batch: 030 ----
mean loss: 199.76
 ---- batch: 040 ----
mean loss: 198.03
 ---- batch: 050 ----
mean loss: 197.16
 ---- batch: 060 ----
mean loss: 203.53
 ---- batch: 070 ----
mean loss: 199.87
 ---- batch: 080 ----
mean loss: 205.12
 ---- batch: 090 ----
mean loss: 200.03
 ---- batch: 100 ----
mean loss: 197.09
 ---- batch: 110 ----
mean loss: 194.79
train mean loss: 200.73
epoch train time: 0:00:00.725247
elapsed time: 0:02:16.009695
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 01:39:16.767460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.99
 ---- batch: 020 ----
mean loss: 211.52
 ---- batch: 030 ----
mean loss: 194.74
 ---- batch: 040 ----
mean loss: 198.36
 ---- batch: 050 ----
mean loss: 197.84
 ---- batch: 060 ----
mean loss: 198.20
 ---- batch: 070 ----
mean loss: 197.43
 ---- batch: 080 ----
mean loss: 198.31
 ---- batch: 090 ----
mean loss: 191.06
 ---- batch: 100 ----
mean loss: 206.74
 ---- batch: 110 ----
mean loss: 211.67
train mean loss: 200.63
epoch train time: 0:00:00.717483
elapsed time: 0:02:16.727319
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 01:39:17.485083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.09
 ---- batch: 020 ----
mean loss: 195.69
 ---- batch: 030 ----
mean loss: 192.42
 ---- batch: 040 ----
mean loss: 198.94
 ---- batch: 050 ----
mean loss: 208.18
 ---- batch: 060 ----
mean loss: 193.10
 ---- batch: 070 ----
mean loss: 203.34
 ---- batch: 080 ----
mean loss: 209.29
 ---- batch: 090 ----
mean loss: 205.28
 ---- batch: 100 ----
mean loss: 203.17
 ---- batch: 110 ----
mean loss: 192.31
train mean loss: 200.27
epoch train time: 0:00:00.719369
elapsed time: 0:02:17.446833
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 01:39:18.204623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.94
 ---- batch: 020 ----
mean loss: 199.03
 ---- batch: 030 ----
mean loss: 203.83
 ---- batch: 040 ----
mean loss: 204.98
 ---- batch: 050 ----
mean loss: 197.54
 ---- batch: 060 ----
mean loss: 201.26
 ---- batch: 070 ----
mean loss: 194.31
 ---- batch: 080 ----
mean loss: 204.64
 ---- batch: 090 ----
mean loss: 198.60
 ---- batch: 100 ----
mean loss: 201.73
 ---- batch: 110 ----
mean loss: 199.25
train mean loss: 199.99
epoch train time: 0:00:00.722544
elapsed time: 0:02:18.169557
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 01:39:18.927321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.75
 ---- batch: 020 ----
mean loss: 203.58
 ---- batch: 030 ----
mean loss: 206.18
 ---- batch: 040 ----
mean loss: 194.85
 ---- batch: 050 ----
mean loss: 206.53
 ---- batch: 060 ----
mean loss: 204.33
 ---- batch: 070 ----
mean loss: 198.48
 ---- batch: 080 ----
mean loss: 199.60
 ---- batch: 090 ----
mean loss: 197.69
 ---- batch: 100 ----
mean loss: 197.08
 ---- batch: 110 ----
mean loss: 201.02
train mean loss: 199.74
epoch train time: 0:00:00.712252
elapsed time: 0:02:18.881946
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 01:39:19.639710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.09
 ---- batch: 020 ----
mean loss: 205.12
 ---- batch: 030 ----
mean loss: 192.24
 ---- batch: 040 ----
mean loss: 201.21
 ---- batch: 050 ----
mean loss: 194.19
 ---- batch: 060 ----
mean loss: 198.93
 ---- batch: 070 ----
mean loss: 203.56
 ---- batch: 080 ----
mean loss: 195.16
 ---- batch: 090 ----
mean loss: 202.25
 ---- batch: 100 ----
mean loss: 193.66
 ---- batch: 110 ----
mean loss: 203.23
train mean loss: 199.62
epoch train time: 0:00:00.715552
elapsed time: 0:02:19.597636
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 01:39:20.355402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.00
 ---- batch: 020 ----
mean loss: 201.30
 ---- batch: 030 ----
mean loss: 197.31
 ---- batch: 040 ----
mean loss: 196.22
 ---- batch: 050 ----
mean loss: 208.28
 ---- batch: 060 ----
mean loss: 195.71
 ---- batch: 070 ----
mean loss: 198.31
 ---- batch: 080 ----
mean loss: 200.61
 ---- batch: 090 ----
mean loss: 199.34
 ---- batch: 100 ----
mean loss: 205.89
 ---- batch: 110 ----
mean loss: 189.84
train mean loss: 199.16
epoch train time: 0:00:00.721302
elapsed time: 0:02:20.319083
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 01:39:21.076885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.76
 ---- batch: 020 ----
mean loss: 201.03
 ---- batch: 030 ----
mean loss: 204.16
 ---- batch: 040 ----
mean loss: 195.13
 ---- batch: 050 ----
mean loss: 201.57
 ---- batch: 060 ----
mean loss: 189.82
 ---- batch: 070 ----
mean loss: 195.26
 ---- batch: 080 ----
mean loss: 205.35
 ---- batch: 090 ----
mean loss: 203.39
 ---- batch: 100 ----
mean loss: 203.14
 ---- batch: 110 ----
mean loss: 194.90
train mean loss: 199.08
epoch train time: 0:00:00.735729
elapsed time: 0:02:21.055033
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 01:39:21.812804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.32
 ---- batch: 020 ----
mean loss: 197.95
 ---- batch: 030 ----
mean loss: 189.06
 ---- batch: 040 ----
mean loss: 188.12
 ---- batch: 050 ----
mean loss: 197.78
 ---- batch: 060 ----
mean loss: 198.50
 ---- batch: 070 ----
mean loss: 202.68
 ---- batch: 080 ----
mean loss: 203.99
 ---- batch: 090 ----
mean loss: 200.75
 ---- batch: 100 ----
mean loss: 202.12
 ---- batch: 110 ----
mean loss: 196.59
train mean loss: 198.77
epoch train time: 0:00:00.712841
elapsed time: 0:02:21.768020
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 01:39:22.525801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.56
 ---- batch: 020 ----
mean loss: 196.92
 ---- batch: 030 ----
mean loss: 201.77
 ---- batch: 040 ----
mean loss: 195.18
 ---- batch: 050 ----
mean loss: 197.24
 ---- batch: 060 ----
mean loss: 203.24
 ---- batch: 070 ----
mean loss: 194.54
 ---- batch: 080 ----
mean loss: 204.00
 ---- batch: 090 ----
mean loss: 201.58
 ---- batch: 100 ----
mean loss: 194.42
 ---- batch: 110 ----
mean loss: 197.76
train mean loss: 198.77
epoch train time: 0:00:00.720021
elapsed time: 0:02:22.488224
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 01:39:23.246008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.30
 ---- batch: 020 ----
mean loss: 192.34
 ---- batch: 030 ----
mean loss: 201.58
 ---- batch: 040 ----
mean loss: 200.71
 ---- batch: 050 ----
mean loss: 197.21
 ---- batch: 060 ----
mean loss: 201.69
 ---- batch: 070 ----
mean loss: 199.51
 ---- batch: 080 ----
mean loss: 195.46
 ---- batch: 090 ----
mean loss: 197.47
 ---- batch: 100 ----
mean loss: 204.76
 ---- batch: 110 ----
mean loss: 191.35
train mean loss: 198.41
epoch train time: 0:00:00.725710
elapsed time: 0:02:23.214096
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 01:39:23.971861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.54
 ---- batch: 020 ----
mean loss: 193.51
 ---- batch: 030 ----
mean loss: 196.99
 ---- batch: 040 ----
mean loss: 191.65
 ---- batch: 050 ----
mean loss: 197.93
 ---- batch: 060 ----
mean loss: 194.78
 ---- batch: 070 ----
mean loss: 191.28
 ---- batch: 080 ----
mean loss: 204.68
 ---- batch: 090 ----
mean loss: 203.91
 ---- batch: 100 ----
mean loss: 197.30
 ---- batch: 110 ----
mean loss: 194.20
train mean loss: 198.11
epoch train time: 0:00:00.718045
elapsed time: 0:02:23.932285
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 01:39:24.690050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.49
 ---- batch: 020 ----
mean loss: 189.16
 ---- batch: 030 ----
mean loss: 195.92
 ---- batch: 040 ----
mean loss: 198.20
 ---- batch: 050 ----
mean loss: 203.95
 ---- batch: 060 ----
mean loss: 201.50
 ---- batch: 070 ----
mean loss: 199.81
 ---- batch: 080 ----
mean loss: 195.70
 ---- batch: 090 ----
mean loss: 193.48
 ---- batch: 100 ----
mean loss: 197.70
 ---- batch: 110 ----
mean loss: 199.34
train mean loss: 197.91
epoch train time: 0:00:00.718945
elapsed time: 0:02:24.651376
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 01:39:25.409146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.24
 ---- batch: 020 ----
mean loss: 197.40
 ---- batch: 030 ----
mean loss: 196.66
 ---- batch: 040 ----
mean loss: 196.20
 ---- batch: 050 ----
mean loss: 198.81
 ---- batch: 060 ----
mean loss: 192.22
 ---- batch: 070 ----
mean loss: 199.19
 ---- batch: 080 ----
mean loss: 196.78
 ---- batch: 090 ----
mean loss: 201.36
 ---- batch: 100 ----
mean loss: 195.24
 ---- batch: 110 ----
mean loss: 201.98
train mean loss: 197.57
epoch train time: 0:00:00.721452
elapsed time: 0:02:25.372993
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 01:39:26.130760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.25
 ---- batch: 020 ----
mean loss: 196.51
 ---- batch: 030 ----
mean loss: 194.38
 ---- batch: 040 ----
mean loss: 199.01
 ---- batch: 050 ----
mean loss: 192.76
 ---- batch: 060 ----
mean loss: 192.83
 ---- batch: 070 ----
mean loss: 203.77
 ---- batch: 080 ----
mean loss: 195.20
 ---- batch: 090 ----
mean loss: 201.54
 ---- batch: 100 ----
mean loss: 196.01
 ---- batch: 110 ----
mean loss: 201.10
train mean loss: 197.52
epoch train time: 0:00:00.724211
elapsed time: 0:02:26.097347
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 01:39:26.855111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.30
 ---- batch: 020 ----
mean loss: 194.41
 ---- batch: 030 ----
mean loss: 204.77
 ---- batch: 040 ----
mean loss: 210.93
 ---- batch: 050 ----
mean loss: 194.07
 ---- batch: 060 ----
mean loss: 196.65
 ---- batch: 070 ----
mean loss: 197.80
 ---- batch: 080 ----
mean loss: 191.51
 ---- batch: 090 ----
mean loss: 189.19
 ---- batch: 100 ----
mean loss: 199.84
 ---- batch: 110 ----
mean loss: 193.21
train mean loss: 197.37
epoch train time: 0:00:00.711688
elapsed time: 0:02:26.809191
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 01:39:27.566987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.40
 ---- batch: 020 ----
mean loss: 196.40
 ---- batch: 030 ----
mean loss: 201.15
 ---- batch: 040 ----
mean loss: 195.78
 ---- batch: 050 ----
mean loss: 188.31
 ---- batch: 060 ----
mean loss: 205.03
 ---- batch: 070 ----
mean loss: 192.93
 ---- batch: 080 ----
mean loss: 196.69
 ---- batch: 090 ----
mean loss: 200.61
 ---- batch: 100 ----
mean loss: 198.44
 ---- batch: 110 ----
mean loss: 203.16
train mean loss: 197.19
epoch train time: 0:00:00.718753
elapsed time: 0:02:27.528134
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 01:39:28.285912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.26
 ---- batch: 020 ----
mean loss: 195.97
 ---- batch: 030 ----
mean loss: 198.27
 ---- batch: 040 ----
mean loss: 190.53
 ---- batch: 050 ----
mean loss: 195.56
 ---- batch: 060 ----
mean loss: 193.84
 ---- batch: 070 ----
mean loss: 197.87
 ---- batch: 080 ----
mean loss: 193.92
 ---- batch: 090 ----
mean loss: 188.23
 ---- batch: 100 ----
mean loss: 202.10
 ---- batch: 110 ----
mean loss: 206.05
train mean loss: 196.82
epoch train time: 0:00:00.722140
elapsed time: 0:02:28.250426
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 01:39:29.008189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.02
 ---- batch: 020 ----
mean loss: 193.01
 ---- batch: 030 ----
mean loss: 193.04
 ---- batch: 040 ----
mean loss: 199.45
 ---- batch: 050 ----
mean loss: 199.13
 ---- batch: 060 ----
mean loss: 191.64
 ---- batch: 070 ----
mean loss: 200.46
 ---- batch: 080 ----
mean loss: 195.42
 ---- batch: 090 ----
mean loss: 190.83
 ---- batch: 100 ----
mean loss: 206.74
 ---- batch: 110 ----
mean loss: 194.65
train mean loss: 196.83
epoch train time: 0:00:00.711720
elapsed time: 0:02:28.962298
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 01:39:29.720063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.62
 ---- batch: 020 ----
mean loss: 193.83
 ---- batch: 030 ----
mean loss: 191.79
 ---- batch: 040 ----
mean loss: 199.32
 ---- batch: 050 ----
mean loss: 212.62
 ---- batch: 060 ----
mean loss: 192.07
 ---- batch: 070 ----
mean loss: 191.33
 ---- batch: 080 ----
mean loss: 204.34
 ---- batch: 090 ----
mean loss: 201.24
 ---- batch: 100 ----
mean loss: 187.00
 ---- batch: 110 ----
mean loss: 191.92
train mean loss: 196.43
epoch train time: 0:00:00.714639
elapsed time: 0:02:29.677077
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 01:39:30.434841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.60
 ---- batch: 020 ----
mean loss: 197.96
 ---- batch: 030 ----
mean loss: 195.93
 ---- batch: 040 ----
mean loss: 200.32
 ---- batch: 050 ----
mean loss: 205.17
 ---- batch: 060 ----
mean loss: 193.13
 ---- batch: 070 ----
mean loss: 193.88
 ---- batch: 080 ----
mean loss: 194.07
 ---- batch: 090 ----
mean loss: 195.97
 ---- batch: 100 ----
mean loss: 202.26
 ---- batch: 110 ----
mean loss: 197.22
train mean loss: 196.31
epoch train time: 0:00:00.725210
elapsed time: 0:02:30.402427
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 01:39:31.160194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.07
 ---- batch: 020 ----
mean loss: 200.90
 ---- batch: 030 ----
mean loss: 201.25
 ---- batch: 040 ----
mean loss: 190.08
 ---- batch: 050 ----
mean loss: 206.83
 ---- batch: 060 ----
mean loss: 199.47
 ---- batch: 070 ----
mean loss: 189.98
 ---- batch: 080 ----
mean loss: 178.03
 ---- batch: 090 ----
mean loss: 190.90
 ---- batch: 100 ----
mean loss: 199.66
 ---- batch: 110 ----
mean loss: 202.12
train mean loss: 196.33
epoch train time: 0:00:00.722099
elapsed time: 0:02:31.124704
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 01:39:31.882504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.15
 ---- batch: 020 ----
mean loss: 196.24
 ---- batch: 030 ----
mean loss: 201.55
 ---- batch: 040 ----
mean loss: 206.12
 ---- batch: 050 ----
mean loss: 200.27
 ---- batch: 060 ----
mean loss: 192.17
 ---- batch: 070 ----
mean loss: 195.67
 ---- batch: 080 ----
mean loss: 195.04
 ---- batch: 090 ----
mean loss: 187.34
 ---- batch: 100 ----
mean loss: 186.70
 ---- batch: 110 ----
mean loss: 196.24
train mean loss: 195.89
epoch train time: 0:00:00.719816
elapsed time: 0:02:31.844707
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 01:39:32.602474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.11
 ---- batch: 020 ----
mean loss: 199.93
 ---- batch: 030 ----
mean loss: 193.63
 ---- batch: 040 ----
mean loss: 198.24
 ---- batch: 050 ----
mean loss: 199.80
 ---- batch: 060 ----
mean loss: 191.65
 ---- batch: 070 ----
mean loss: 209.02
 ---- batch: 080 ----
mean loss: 192.60
 ---- batch: 090 ----
mean loss: 186.09
 ---- batch: 100 ----
mean loss: 193.67
 ---- batch: 110 ----
mean loss: 194.59
train mean loss: 195.77
epoch train time: 0:00:00.715915
elapsed time: 0:02:32.560764
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 01:39:33.318529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.44
 ---- batch: 020 ----
mean loss: 188.39
 ---- batch: 030 ----
mean loss: 198.50
 ---- batch: 040 ----
mean loss: 195.62
 ---- batch: 050 ----
mean loss: 208.89
 ---- batch: 060 ----
mean loss: 193.90
 ---- batch: 070 ----
mean loss: 192.61
 ---- batch: 080 ----
mean loss: 196.69
 ---- batch: 090 ----
mean loss: 197.33
 ---- batch: 100 ----
mean loss: 193.17
 ---- batch: 110 ----
mean loss: 186.15
train mean loss: 195.65
epoch train time: 0:00:00.718832
elapsed time: 0:02:33.279738
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 01:39:34.037515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.10
 ---- batch: 020 ----
mean loss: 196.26
 ---- batch: 030 ----
mean loss: 190.60
 ---- batch: 040 ----
mean loss: 190.74
 ---- batch: 050 ----
mean loss: 187.56
 ---- batch: 060 ----
mean loss: 197.42
 ---- batch: 070 ----
mean loss: 183.50
 ---- batch: 080 ----
mean loss: 209.20
 ---- batch: 090 ----
mean loss: 198.26
 ---- batch: 100 ----
mean loss: 210.82
 ---- batch: 110 ----
mean loss: 188.91
train mean loss: 195.53
epoch train time: 0:00:00.722063
elapsed time: 0:02:34.001954
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 01:39:34.759720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.05
 ---- batch: 020 ----
mean loss: 194.32
 ---- batch: 030 ----
mean loss: 192.20
 ---- batch: 040 ----
mean loss: 188.41
 ---- batch: 050 ----
mean loss: 196.60
 ---- batch: 060 ----
mean loss: 192.36
 ---- batch: 070 ----
mean loss: 193.10
 ---- batch: 080 ----
mean loss: 191.60
 ---- batch: 090 ----
mean loss: 202.25
 ---- batch: 100 ----
mean loss: 200.38
 ---- batch: 110 ----
mean loss: 199.51
train mean loss: 195.15
epoch train time: 0:00:00.714521
elapsed time: 0:02:34.716619
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 01:39:35.474387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.32
 ---- batch: 020 ----
mean loss: 202.03
 ---- batch: 030 ----
mean loss: 197.48
 ---- batch: 040 ----
mean loss: 203.36
 ---- batch: 050 ----
mean loss: 203.69
 ---- batch: 060 ----
mean loss: 190.58
 ---- batch: 070 ----
mean loss: 185.56
 ---- batch: 080 ----
mean loss: 194.09
 ---- batch: 090 ----
mean loss: 191.21
 ---- batch: 100 ----
mean loss: 189.60
 ---- batch: 110 ----
mean loss: 192.37
train mean loss: 195.11
epoch train time: 0:00:00.713530
elapsed time: 0:02:35.430322
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 01:39:36.188087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.09
 ---- batch: 020 ----
mean loss: 195.31
 ---- batch: 030 ----
mean loss: 186.53
 ---- batch: 040 ----
mean loss: 204.70
 ---- batch: 050 ----
mean loss: 199.95
 ---- batch: 060 ----
mean loss: 198.19
 ---- batch: 070 ----
mean loss: 195.04
 ---- batch: 080 ----
mean loss: 201.33
 ---- batch: 090 ----
mean loss: 188.42
 ---- batch: 100 ----
mean loss: 197.01
 ---- batch: 110 ----
mean loss: 188.73
train mean loss: 195.06
epoch train time: 0:00:00.720736
elapsed time: 0:02:36.151205
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 01:39:36.908994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.67
 ---- batch: 020 ----
mean loss: 200.99
 ---- batch: 030 ----
mean loss: 199.10
 ---- batch: 040 ----
mean loss: 194.93
 ---- batch: 050 ----
mean loss: 190.42
 ---- batch: 060 ----
mean loss: 198.28
 ---- batch: 070 ----
mean loss: 191.55
 ---- batch: 080 ----
mean loss: 194.88
 ---- batch: 090 ----
mean loss: 198.77
 ---- batch: 100 ----
mean loss: 186.44
 ---- batch: 110 ----
mean loss: 188.20
train mean loss: 194.82
epoch train time: 0:00:00.722621
elapsed time: 0:02:36.873992
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 01:39:37.631757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.48
 ---- batch: 020 ----
mean loss: 206.49
 ---- batch: 030 ----
mean loss: 199.67
 ---- batch: 040 ----
mean loss: 191.85
 ---- batch: 050 ----
mean loss: 192.99
 ---- batch: 060 ----
mean loss: 186.16
 ---- batch: 070 ----
mean loss: 201.13
 ---- batch: 080 ----
mean loss: 192.02
 ---- batch: 090 ----
mean loss: 197.10
 ---- batch: 100 ----
mean loss: 193.82
 ---- batch: 110 ----
mean loss: 194.20
train mean loss: 194.66
epoch train time: 0:00:00.715698
elapsed time: 0:02:37.589838
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 01:39:38.347605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.18
 ---- batch: 020 ----
mean loss: 202.58
 ---- batch: 030 ----
mean loss: 193.01
 ---- batch: 040 ----
mean loss: 185.91
 ---- batch: 050 ----
mean loss: 193.91
 ---- batch: 060 ----
mean loss: 194.15
 ---- batch: 070 ----
mean loss: 186.46
 ---- batch: 080 ----
mean loss: 198.21
 ---- batch: 090 ----
mean loss: 202.92
 ---- batch: 100 ----
mean loss: 191.80
 ---- batch: 110 ----
mean loss: 197.11
train mean loss: 194.42
epoch train time: 0:00:00.721901
elapsed time: 0:02:38.311883
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 01:39:39.069705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.77
 ---- batch: 020 ----
mean loss: 193.40
 ---- batch: 030 ----
mean loss: 199.81
 ---- batch: 040 ----
mean loss: 196.32
 ---- batch: 050 ----
mean loss: 187.62
 ---- batch: 060 ----
mean loss: 193.47
 ---- batch: 070 ----
mean loss: 196.26
 ---- batch: 080 ----
mean loss: 189.47
 ---- batch: 090 ----
mean loss: 199.76
 ---- batch: 100 ----
mean loss: 186.45
 ---- batch: 110 ----
mean loss: 200.17
train mean loss: 194.26
epoch train time: 0:00:00.718486
elapsed time: 0:02:39.030585
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 01:39:39.788342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.36
 ---- batch: 020 ----
mean loss: 200.47
 ---- batch: 030 ----
mean loss: 194.84
 ---- batch: 040 ----
mean loss: 192.83
 ---- batch: 050 ----
mean loss: 200.18
 ---- batch: 060 ----
mean loss: 191.67
 ---- batch: 070 ----
mean loss: 192.59
 ---- batch: 080 ----
mean loss: 188.77
 ---- batch: 090 ----
mean loss: 192.37
 ---- batch: 100 ----
mean loss: 193.84
 ---- batch: 110 ----
mean loss: 194.23
train mean loss: 193.98
epoch train time: 0:00:00.732822
elapsed time: 0:02:39.763539
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 01:39:40.521311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.58
 ---- batch: 020 ----
mean loss: 193.70
 ---- batch: 030 ----
mean loss: 190.17
 ---- batch: 040 ----
mean loss: 190.75
 ---- batch: 050 ----
mean loss: 188.68
 ---- batch: 060 ----
mean loss: 193.05
 ---- batch: 070 ----
mean loss: 197.17
 ---- batch: 080 ----
mean loss: 190.74
 ---- batch: 090 ----
mean loss: 201.67
 ---- batch: 100 ----
mean loss: 196.44
 ---- batch: 110 ----
mean loss: 190.11
train mean loss: 194.00
epoch train time: 0:00:00.726980
elapsed time: 0:02:40.490668
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 01:39:41.248436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.86
 ---- batch: 020 ----
mean loss: 193.83
 ---- batch: 030 ----
mean loss: 184.73
 ---- batch: 040 ----
mean loss: 205.51
 ---- batch: 050 ----
mean loss: 199.22
 ---- batch: 060 ----
mean loss: 194.89
 ---- batch: 070 ----
mean loss: 193.63
 ---- batch: 080 ----
mean loss: 195.47
 ---- batch: 090 ----
mean loss: 188.60
 ---- batch: 100 ----
mean loss: 191.99
 ---- batch: 110 ----
mean loss: 189.68
train mean loss: 193.74
epoch train time: 0:00:00.723437
elapsed time: 0:02:41.214256
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 01:39:41.972046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.27
 ---- batch: 020 ----
mean loss: 203.34
 ---- batch: 030 ----
mean loss: 198.09
 ---- batch: 040 ----
mean loss: 185.21
 ---- batch: 050 ----
mean loss: 192.69
 ---- batch: 060 ----
mean loss: 192.48
 ---- batch: 070 ----
mean loss: 184.25
 ---- batch: 080 ----
mean loss: 194.49
 ---- batch: 090 ----
mean loss: 197.70
 ---- batch: 100 ----
mean loss: 193.95
 ---- batch: 110 ----
mean loss: 185.99
train mean loss: 193.65
epoch train time: 0:00:00.716954
elapsed time: 0:02:41.931375
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 01:39:42.689143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.04
 ---- batch: 020 ----
mean loss: 199.74
 ---- batch: 030 ----
mean loss: 195.27
 ---- batch: 040 ----
mean loss: 189.41
 ---- batch: 050 ----
mean loss: 188.91
 ---- batch: 060 ----
mean loss: 196.68
 ---- batch: 070 ----
mean loss: 193.54
 ---- batch: 080 ----
mean loss: 188.37
 ---- batch: 090 ----
mean loss: 196.93
 ---- batch: 100 ----
mean loss: 190.52
 ---- batch: 110 ----
mean loss: 197.83
train mean loss: 193.52
epoch train time: 0:00:00.721114
elapsed time: 0:02:42.652631
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 01:39:43.410395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.09
 ---- batch: 020 ----
mean loss: 189.13
 ---- batch: 030 ----
mean loss: 194.86
 ---- batch: 040 ----
mean loss: 197.42
 ---- batch: 050 ----
mean loss: 197.53
 ---- batch: 060 ----
mean loss: 186.46
 ---- batch: 070 ----
mean loss: 200.19
 ---- batch: 080 ----
mean loss: 194.39
 ---- batch: 090 ----
mean loss: 191.20
 ---- batch: 100 ----
mean loss: 187.99
 ---- batch: 110 ----
mean loss: 201.45
train mean loss: 193.50
epoch train time: 0:00:00.719715
elapsed time: 0:02:43.372489
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 01:39:44.130274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.44
 ---- batch: 020 ----
mean loss: 198.93
 ---- batch: 030 ----
mean loss: 188.79
 ---- batch: 040 ----
mean loss: 185.99
 ---- batch: 050 ----
mean loss: 200.07
 ---- batch: 060 ----
mean loss: 198.04
 ---- batch: 070 ----
mean loss: 195.05
 ---- batch: 080 ----
mean loss: 194.01
 ---- batch: 090 ----
mean loss: 187.58
 ---- batch: 100 ----
mean loss: 194.09
 ---- batch: 110 ----
mean loss: 197.23
train mean loss: 193.21
epoch train time: 0:00:00.716457
elapsed time: 0:02:44.089105
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 01:39:44.846868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.70
 ---- batch: 020 ----
mean loss: 193.15
 ---- batch: 030 ----
mean loss: 204.50
 ---- batch: 040 ----
mean loss: 183.79
 ---- batch: 050 ----
mean loss: 191.81
 ---- batch: 060 ----
mean loss: 183.89
 ---- batch: 070 ----
mean loss: 187.49
 ---- batch: 080 ----
mean loss: 193.58
 ---- batch: 090 ----
mean loss: 197.01
 ---- batch: 100 ----
mean loss: 197.81
 ---- batch: 110 ----
mean loss: 197.96
train mean loss: 193.13
epoch train time: 0:00:00.720272
elapsed time: 0:02:44.809532
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 01:39:45.567300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.26
 ---- batch: 020 ----
mean loss: 199.65
 ---- batch: 030 ----
mean loss: 193.92
 ---- batch: 040 ----
mean loss: 198.65
 ---- batch: 050 ----
mean loss: 191.94
 ---- batch: 060 ----
mean loss: 196.83
 ---- batch: 070 ----
mean loss: 188.64
 ---- batch: 080 ----
mean loss: 190.24
 ---- batch: 090 ----
mean loss: 194.58
 ---- batch: 100 ----
mean loss: 189.03
 ---- batch: 110 ----
mean loss: 183.38
train mean loss: 192.95
epoch train time: 0:00:00.716723
elapsed time: 0:02:45.526398
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 01:39:46.284182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.26
 ---- batch: 020 ----
mean loss: 195.50
 ---- batch: 030 ----
mean loss: 185.31
 ---- batch: 040 ----
mean loss: 196.73
 ---- batch: 050 ----
mean loss: 181.29
 ---- batch: 060 ----
mean loss: 193.87
 ---- batch: 070 ----
mean loss: 191.70
 ---- batch: 080 ----
mean loss: 195.56
 ---- batch: 090 ----
mean loss: 202.34
 ---- batch: 100 ----
mean loss: 195.55
 ---- batch: 110 ----
mean loss: 189.61
train mean loss: 192.67
epoch train time: 0:00:00.719090
elapsed time: 0:02:46.245662
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 01:39:47.003426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.10
 ---- batch: 020 ----
mean loss: 195.39
 ---- batch: 030 ----
mean loss: 197.56
 ---- batch: 040 ----
mean loss: 189.39
 ---- batch: 050 ----
mean loss: 196.26
 ---- batch: 060 ----
mean loss: 197.93
 ---- batch: 070 ----
mean loss: 194.06
 ---- batch: 080 ----
mean loss: 192.95
 ---- batch: 090 ----
mean loss: 182.57
 ---- batch: 100 ----
mean loss: 189.36
 ---- batch: 110 ----
mean loss: 196.68
train mean loss: 192.48
epoch train time: 0:00:00.716260
elapsed time: 0:02:46.962069
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 01:39:47.719842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.29
 ---- batch: 020 ----
mean loss: 181.39
 ---- batch: 030 ----
mean loss: 200.57
 ---- batch: 040 ----
mean loss: 192.69
 ---- batch: 050 ----
mean loss: 195.98
 ---- batch: 060 ----
mean loss: 192.75
 ---- batch: 070 ----
mean loss: 194.46
 ---- batch: 080 ----
mean loss: 186.48
 ---- batch: 090 ----
mean loss: 190.07
 ---- batch: 100 ----
mean loss: 196.97
 ---- batch: 110 ----
mean loss: 195.80
train mean loss: 192.68
epoch train time: 0:00:00.733428
elapsed time: 0:02:47.695664
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 01:39:48.453432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.78
 ---- batch: 020 ----
mean loss: 204.89
 ---- batch: 030 ----
mean loss: 195.01
 ---- batch: 040 ----
mean loss: 189.19
 ---- batch: 050 ----
mean loss: 190.82
 ---- batch: 060 ----
mean loss: 192.81
 ---- batch: 070 ----
mean loss: 194.16
 ---- batch: 080 ----
mean loss: 190.15
 ---- batch: 090 ----
mean loss: 184.55
 ---- batch: 100 ----
mean loss: 186.18
 ---- batch: 110 ----
mean loss: 190.72
train mean loss: 192.29
epoch train time: 0:00:00.733913
elapsed time: 0:02:48.429728
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 01:39:49.187494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.97
 ---- batch: 020 ----
mean loss: 188.67
 ---- batch: 030 ----
mean loss: 193.51
 ---- batch: 040 ----
mean loss: 196.42
 ---- batch: 050 ----
mean loss: 194.91
 ---- batch: 060 ----
mean loss: 199.47
 ---- batch: 070 ----
mean loss: 189.13
 ---- batch: 080 ----
mean loss: 186.35
 ---- batch: 090 ----
mean loss: 198.40
 ---- batch: 100 ----
mean loss: 191.73
 ---- batch: 110 ----
mean loss: 191.06
train mean loss: 192.27
epoch train time: 0:00:00.723744
elapsed time: 0:02:49.153614
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 01:39:49.911379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.52
 ---- batch: 020 ----
mean loss: 194.02
 ---- batch: 030 ----
mean loss: 202.66
 ---- batch: 040 ----
mean loss: 182.69
 ---- batch: 050 ----
mean loss: 192.60
 ---- batch: 060 ----
mean loss: 189.36
 ---- batch: 070 ----
mean loss: 194.53
 ---- batch: 080 ----
mean loss: 189.41
 ---- batch: 090 ----
mean loss: 192.01
 ---- batch: 100 ----
mean loss: 190.36
 ---- batch: 110 ----
mean loss: 189.78
train mean loss: 192.00
epoch train time: 0:00:00.724971
elapsed time: 0:02:49.878729
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 01:39:50.636500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.95
 ---- batch: 020 ----
mean loss: 193.42
 ---- batch: 030 ----
mean loss: 201.25
 ---- batch: 040 ----
mean loss: 192.31
 ---- batch: 050 ----
mean loss: 192.22
 ---- batch: 060 ----
mean loss: 191.46
 ---- batch: 070 ----
mean loss: 186.23
 ---- batch: 080 ----
mean loss: 198.81
 ---- batch: 090 ----
mean loss: 192.31
 ---- batch: 100 ----
mean loss: 183.51
 ---- batch: 110 ----
mean loss: 187.40
train mean loss: 191.98
epoch train time: 0:00:00.721812
elapsed time: 0:02:50.600692
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 01:39:51.358484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.54
 ---- batch: 020 ----
mean loss: 189.44
 ---- batch: 030 ----
mean loss: 199.12
 ---- batch: 040 ----
mean loss: 188.27
 ---- batch: 050 ----
mean loss: 185.72
 ---- batch: 060 ----
mean loss: 198.54
 ---- batch: 070 ----
mean loss: 199.80
 ---- batch: 080 ----
mean loss: 187.96
 ---- batch: 090 ----
mean loss: 187.40
 ---- batch: 100 ----
mean loss: 193.31
 ---- batch: 110 ----
mean loss: 198.92
train mean loss: 191.96
epoch train time: 0:00:00.717109
elapsed time: 0:02:51.317966
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 01:39:52.075729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.79
 ---- batch: 020 ----
mean loss: 183.89
 ---- batch: 030 ----
mean loss: 192.86
 ---- batch: 040 ----
mean loss: 193.44
 ---- batch: 050 ----
mean loss: 199.31
 ---- batch: 060 ----
mean loss: 197.11
 ---- batch: 070 ----
mean loss: 187.40
 ---- batch: 080 ----
mean loss: 183.57
 ---- batch: 090 ----
mean loss: 195.60
 ---- batch: 100 ----
mean loss: 193.90
 ---- batch: 110 ----
mean loss: 188.04
train mean loss: 191.63
epoch train time: 0:00:00.715138
elapsed time: 0:02:52.033238
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 01:39:52.790999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.62
 ---- batch: 020 ----
mean loss: 187.59
 ---- batch: 030 ----
mean loss: 188.56
 ---- batch: 040 ----
mean loss: 192.15
 ---- batch: 050 ----
mean loss: 197.00
 ---- batch: 060 ----
mean loss: 200.04
 ---- batch: 070 ----
mean loss: 186.77
 ---- batch: 080 ----
mean loss: 190.27
 ---- batch: 090 ----
mean loss: 185.02
 ---- batch: 100 ----
mean loss: 187.28
 ---- batch: 110 ----
mean loss: 192.24
train mean loss: 191.56
epoch train time: 0:00:00.716041
elapsed time: 0:02:52.749433
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 01:39:53.507198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.35
 ---- batch: 020 ----
mean loss: 193.02
 ---- batch: 030 ----
mean loss: 189.54
 ---- batch: 040 ----
mean loss: 193.55
 ---- batch: 050 ----
mean loss: 197.14
 ---- batch: 060 ----
mean loss: 184.01
 ---- batch: 070 ----
mean loss: 186.48
 ---- batch: 080 ----
mean loss: 188.35
 ---- batch: 090 ----
mean loss: 184.15
 ---- batch: 100 ----
mean loss: 197.84
 ---- batch: 110 ----
mean loss: 198.98
train mean loss: 191.33
epoch train time: 0:00:00.718605
elapsed time: 0:02:53.468205
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 01:39:54.225977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.86
 ---- batch: 020 ----
mean loss: 203.52
 ---- batch: 030 ----
mean loss: 192.59
 ---- batch: 040 ----
mean loss: 188.40
 ---- batch: 050 ----
mean loss: 183.85
 ---- batch: 060 ----
mean loss: 187.56
 ---- batch: 070 ----
mean loss: 200.53
 ---- batch: 080 ----
mean loss: 187.78
 ---- batch: 090 ----
mean loss: 192.72
 ---- batch: 100 ----
mean loss: 195.55
 ---- batch: 110 ----
mean loss: 183.76
train mean loss: 191.22
epoch train time: 0:00:00.719086
elapsed time: 0:02:54.187437
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 01:39:54.945217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.49
 ---- batch: 020 ----
mean loss: 199.61
 ---- batch: 030 ----
mean loss: 199.33
 ---- batch: 040 ----
mean loss: 190.47
 ---- batch: 050 ----
mean loss: 182.03
 ---- batch: 060 ----
mean loss: 183.82
 ---- batch: 070 ----
mean loss: 192.53
 ---- batch: 080 ----
mean loss: 187.95
 ---- batch: 090 ----
mean loss: 195.69
 ---- batch: 100 ----
mean loss: 189.82
 ---- batch: 110 ----
mean loss: 197.71
train mean loss: 191.17
epoch train time: 0:00:00.713741
elapsed time: 0:02:54.901333
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 01:39:55.659118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.36
 ---- batch: 020 ----
mean loss: 192.91
 ---- batch: 030 ----
mean loss: 187.42
 ---- batch: 040 ----
mean loss: 203.64
 ---- batch: 050 ----
mean loss: 185.60
 ---- batch: 060 ----
mean loss: 189.02
 ---- batch: 070 ----
mean loss: 188.96
 ---- batch: 080 ----
mean loss: 190.69
 ---- batch: 090 ----
mean loss: 188.79
 ---- batch: 100 ----
mean loss: 179.38
 ---- batch: 110 ----
mean loss: 198.62
train mean loss: 190.89
epoch train time: 0:00:00.725249
elapsed time: 0:02:55.626751
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 01:39:56.384519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.21
 ---- batch: 020 ----
mean loss: 190.32
 ---- batch: 030 ----
mean loss: 194.15
 ---- batch: 040 ----
mean loss: 183.51
 ---- batch: 050 ----
mean loss: 194.54
 ---- batch: 060 ----
mean loss: 187.31
 ---- batch: 070 ----
mean loss: 203.49
 ---- batch: 080 ----
mean loss: 197.08
 ---- batch: 090 ----
mean loss: 188.82
 ---- batch: 100 ----
mean loss: 188.70
 ---- batch: 110 ----
mean loss: 183.98
train mean loss: 190.89
epoch train time: 0:00:00.725527
elapsed time: 0:02:56.352451
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 01:39:57.110230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.88
 ---- batch: 020 ----
mean loss: 188.06
 ---- batch: 030 ----
mean loss: 190.06
 ---- batch: 040 ----
mean loss: 191.10
 ---- batch: 050 ----
mean loss: 187.99
 ---- batch: 060 ----
mean loss: 198.86
 ---- batch: 070 ----
mean loss: 186.38
 ---- batch: 080 ----
mean loss: 186.16
 ---- batch: 090 ----
mean loss: 190.05
 ---- batch: 100 ----
mean loss: 181.68
 ---- batch: 110 ----
mean loss: 197.23
train mean loss: 190.79
epoch train time: 0:00:00.718829
elapsed time: 0:02:57.071462
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 01:39:57.829224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.67
 ---- batch: 020 ----
mean loss: 183.20
 ---- batch: 030 ----
mean loss: 202.28
 ---- batch: 040 ----
mean loss: 183.42
 ---- batch: 050 ----
mean loss: 191.54
 ---- batch: 060 ----
mean loss: 200.75
 ---- batch: 070 ----
mean loss: 191.33
 ---- batch: 080 ----
mean loss: 191.62
 ---- batch: 090 ----
mean loss: 181.95
 ---- batch: 100 ----
mean loss: 185.36
 ---- batch: 110 ----
mean loss: 194.85
train mean loss: 190.64
epoch train time: 0:00:00.721530
elapsed time: 0:02:57.793130
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 01:39:58.550894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.17
 ---- batch: 020 ----
mean loss: 187.34
 ---- batch: 030 ----
mean loss: 190.10
 ---- batch: 040 ----
mean loss: 185.37
 ---- batch: 050 ----
mean loss: 192.20
 ---- batch: 060 ----
mean loss: 192.45
 ---- batch: 070 ----
mean loss: 190.90
 ---- batch: 080 ----
mean loss: 181.37
 ---- batch: 090 ----
mean loss: 190.84
 ---- batch: 100 ----
mean loss: 190.73
 ---- batch: 110 ----
mean loss: 202.25
train mean loss: 190.52
epoch train time: 0:00:00.730257
elapsed time: 0:02:58.523559
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 01:39:59.281340
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.90
 ---- batch: 020 ----
mean loss: 186.49
 ---- batch: 030 ----
mean loss: 199.40
 ---- batch: 040 ----
mean loss: 191.23
 ---- batch: 050 ----
mean loss: 185.99
 ---- batch: 060 ----
mean loss: 191.62
 ---- batch: 070 ----
mean loss: 185.89
 ---- batch: 080 ----
mean loss: 191.72
 ---- batch: 090 ----
mean loss: 191.53
 ---- batch: 100 ----
mean loss: 186.43
 ---- batch: 110 ----
mean loss: 186.75
train mean loss: 190.03
epoch train time: 0:00:00.724984
elapsed time: 0:02:59.248714
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 01:40:00.006471
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.00
 ---- batch: 020 ----
mean loss: 186.93
 ---- batch: 030 ----
mean loss: 181.38
 ---- batch: 040 ----
mean loss: 192.54
 ---- batch: 050 ----
mean loss: 189.18
 ---- batch: 060 ----
mean loss: 192.74
 ---- batch: 070 ----
mean loss: 186.23
 ---- batch: 080 ----
mean loss: 197.37
 ---- batch: 090 ----
mean loss: 195.52
 ---- batch: 100 ----
mean loss: 185.51
 ---- batch: 110 ----
mean loss: 184.55
train mean loss: 190.01
epoch train time: 0:00:00.724780
elapsed time: 0:02:59.973644
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 01:40:00.731429
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.20
 ---- batch: 020 ----
mean loss: 193.28
 ---- batch: 030 ----
mean loss: 192.62
 ---- batch: 040 ----
mean loss: 192.61
 ---- batch: 050 ----
mean loss: 193.67
 ---- batch: 060 ----
mean loss: 189.19
 ---- batch: 070 ----
mean loss: 183.92
 ---- batch: 080 ----
mean loss: 195.85
 ---- batch: 090 ----
mean loss: 190.27
 ---- batch: 100 ----
mean loss: 186.58
 ---- batch: 110 ----
mean loss: 183.43
train mean loss: 189.92
epoch train time: 0:00:00.739130
elapsed time: 0:03:00.712941
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 01:40:01.470707
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.97
 ---- batch: 020 ----
mean loss: 189.90
 ---- batch: 030 ----
mean loss: 186.61
 ---- batch: 040 ----
mean loss: 190.25
 ---- batch: 050 ----
mean loss: 193.53
 ---- batch: 060 ----
mean loss: 188.91
 ---- batch: 070 ----
mean loss: 195.87
 ---- batch: 080 ----
mean loss: 192.36
 ---- batch: 090 ----
mean loss: 190.49
 ---- batch: 100 ----
mean loss: 183.58
 ---- batch: 110 ----
mean loss: 188.60
train mean loss: 189.91
epoch train time: 0:00:00.733733
elapsed time: 0:03:01.446818
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 01:40:02.204595
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.78
 ---- batch: 020 ----
mean loss: 185.85
 ---- batch: 030 ----
mean loss: 190.60
 ---- batch: 040 ----
mean loss: 185.82
 ---- batch: 050 ----
mean loss: 192.77
 ---- batch: 060 ----
mean loss: 191.37
 ---- batch: 070 ----
mean loss: 193.26
 ---- batch: 080 ----
mean loss: 194.05
 ---- batch: 090 ----
mean loss: 183.81
 ---- batch: 100 ----
mean loss: 182.47
 ---- batch: 110 ----
mean loss: 198.85
train mean loss: 189.93
epoch train time: 0:00:00.729673
elapsed time: 0:03:02.176644
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 01:40:02.934409
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.56
 ---- batch: 020 ----
mean loss: 189.35
 ---- batch: 030 ----
mean loss: 189.76
 ---- batch: 040 ----
mean loss: 191.97
 ---- batch: 050 ----
mean loss: 184.31
 ---- batch: 060 ----
mean loss: 193.10
 ---- batch: 070 ----
mean loss: 190.58
 ---- batch: 080 ----
mean loss: 193.66
 ---- batch: 090 ----
mean loss: 189.10
 ---- batch: 100 ----
mean loss: 180.68
 ---- batch: 110 ----
mean loss: 196.83
train mean loss: 189.97
epoch train time: 0:00:00.715344
elapsed time: 0:03:02.892175
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 01:40:03.649950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.41
 ---- batch: 020 ----
mean loss: 190.97
 ---- batch: 030 ----
mean loss: 189.87
 ---- batch: 040 ----
mean loss: 203.09
 ---- batch: 050 ----
mean loss: 178.29
 ---- batch: 060 ----
mean loss: 192.75
 ---- batch: 070 ----
mean loss: 180.15
 ---- batch: 080 ----
mean loss: 192.67
 ---- batch: 090 ----
mean loss: 186.60
 ---- batch: 100 ----
mean loss: 200.20
 ---- batch: 110 ----
mean loss: 187.58
train mean loss: 189.95
epoch train time: 0:00:00.716640
elapsed time: 0:03:03.609028
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 01:40:04.366795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.63
 ---- batch: 020 ----
mean loss: 188.47
 ---- batch: 030 ----
mean loss: 184.88
 ---- batch: 040 ----
mean loss: 187.13
 ---- batch: 050 ----
mean loss: 191.85
 ---- batch: 060 ----
mean loss: 197.43
 ---- batch: 070 ----
mean loss: 196.02
 ---- batch: 080 ----
mean loss: 195.81
 ---- batch: 090 ----
mean loss: 187.22
 ---- batch: 100 ----
mean loss: 184.02
 ---- batch: 110 ----
mean loss: 192.99
train mean loss: 189.96
epoch train time: 0:00:00.723239
elapsed time: 0:03:04.332438
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 01:40:05.090227
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.97
 ---- batch: 020 ----
mean loss: 174.39
 ---- batch: 030 ----
mean loss: 193.16
 ---- batch: 040 ----
mean loss: 185.93
 ---- batch: 050 ----
mean loss: 186.84
 ---- batch: 060 ----
mean loss: 195.57
 ---- batch: 070 ----
mean loss: 189.71
 ---- batch: 080 ----
mean loss: 186.12
 ---- batch: 090 ----
mean loss: 192.11
 ---- batch: 100 ----
mean loss: 190.78
 ---- batch: 110 ----
mean loss: 203.82
train mean loss: 189.88
epoch train time: 0:00:00.732399
elapsed time: 0:03:05.065014
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 01:40:05.822779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.09
 ---- batch: 020 ----
mean loss: 188.39
 ---- batch: 030 ----
mean loss: 189.85
 ---- batch: 040 ----
mean loss: 193.41
 ---- batch: 050 ----
mean loss: 192.38
 ---- batch: 060 ----
mean loss: 194.84
 ---- batch: 070 ----
mean loss: 186.26
 ---- batch: 080 ----
mean loss: 183.34
 ---- batch: 090 ----
mean loss: 180.55
 ---- batch: 100 ----
mean loss: 183.53
 ---- batch: 110 ----
mean loss: 191.75
train mean loss: 189.88
epoch train time: 0:00:00.730394
elapsed time: 0:03:05.795551
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 01:40:06.553320
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.12
 ---- batch: 020 ----
mean loss: 196.42
 ---- batch: 030 ----
mean loss: 192.68
 ---- batch: 040 ----
mean loss: 193.22
 ---- batch: 050 ----
mean loss: 196.09
 ---- batch: 060 ----
mean loss: 190.54
 ---- batch: 070 ----
mean loss: 181.80
 ---- batch: 080 ----
mean loss: 183.19
 ---- batch: 090 ----
mean loss: 196.46
 ---- batch: 100 ----
mean loss: 179.39
 ---- batch: 110 ----
mean loss: 189.08
train mean loss: 189.86
epoch train time: 0:00:00.727222
elapsed time: 0:03:06.522971
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 01:40:07.280747
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.51
 ---- batch: 020 ----
mean loss: 185.79
 ---- batch: 030 ----
mean loss: 189.88
 ---- batch: 040 ----
mean loss: 190.85
 ---- batch: 050 ----
mean loss: 193.04
 ---- batch: 060 ----
mean loss: 192.12
 ---- batch: 070 ----
mean loss: 195.60
 ---- batch: 080 ----
mean loss: 186.65
 ---- batch: 090 ----
mean loss: 188.79
 ---- batch: 100 ----
mean loss: 191.61
 ---- batch: 110 ----
mean loss: 189.50
train mean loss: 189.81
epoch train time: 0:00:00.719545
elapsed time: 0:03:07.242668
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 01:40:08.000433
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.83
 ---- batch: 020 ----
mean loss: 187.83
 ---- batch: 030 ----
mean loss: 180.98
 ---- batch: 040 ----
mean loss: 196.67
 ---- batch: 050 ----
mean loss: 192.44
 ---- batch: 060 ----
mean loss: 197.99
 ---- batch: 070 ----
mean loss: 183.10
 ---- batch: 080 ----
mean loss: 186.73
 ---- batch: 090 ----
mean loss: 179.65
 ---- batch: 100 ----
mean loss: 196.52
 ---- batch: 110 ----
mean loss: 196.37
train mean loss: 189.78
epoch train time: 0:00:00.718038
elapsed time: 0:03:07.960863
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 01:40:08.718659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.03
 ---- batch: 020 ----
mean loss: 189.73
 ---- batch: 030 ----
mean loss: 190.71
 ---- batch: 040 ----
mean loss: 179.11
 ---- batch: 050 ----
mean loss: 194.83
 ---- batch: 060 ----
mean loss: 185.09
 ---- batch: 070 ----
mean loss: 197.24
 ---- batch: 080 ----
mean loss: 191.89
 ---- batch: 090 ----
mean loss: 186.69
 ---- batch: 100 ----
mean loss: 191.34
 ---- batch: 110 ----
mean loss: 192.68
train mean loss: 189.87
epoch train time: 0:00:00.714843
elapsed time: 0:03:08.675878
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 01:40:09.433664
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.81
 ---- batch: 020 ----
mean loss: 193.73
 ---- batch: 030 ----
mean loss: 191.72
 ---- batch: 040 ----
mean loss: 193.57
 ---- batch: 050 ----
mean loss: 186.42
 ---- batch: 060 ----
mean loss: 184.73
 ---- batch: 070 ----
mean loss: 198.68
 ---- batch: 080 ----
mean loss: 181.96
 ---- batch: 090 ----
mean loss: 177.38
 ---- batch: 100 ----
mean loss: 188.15
 ---- batch: 110 ----
mean loss: 191.87
train mean loss: 189.81
epoch train time: 0:00:00.717199
elapsed time: 0:03:09.393237
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 01:40:10.151018
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.77
 ---- batch: 020 ----
mean loss: 190.60
 ---- batch: 030 ----
mean loss: 191.95
 ---- batch: 040 ----
mean loss: 186.92
 ---- batch: 050 ----
mean loss: 186.10
 ---- batch: 060 ----
mean loss: 186.00
 ---- batch: 070 ----
mean loss: 183.81
 ---- batch: 080 ----
mean loss: 195.35
 ---- batch: 090 ----
mean loss: 189.67
 ---- batch: 100 ----
mean loss: 195.86
 ---- batch: 110 ----
mean loss: 189.20
train mean loss: 189.82
epoch train time: 0:00:00.728387
elapsed time: 0:03:10.121781
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 01:40:10.879563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.44
 ---- batch: 020 ----
mean loss: 187.97
 ---- batch: 030 ----
mean loss: 190.74
 ---- batch: 040 ----
mean loss: 192.97
 ---- batch: 050 ----
mean loss: 186.42
 ---- batch: 060 ----
mean loss: 191.22
 ---- batch: 070 ----
mean loss: 187.59
 ---- batch: 080 ----
mean loss: 192.74
 ---- batch: 090 ----
mean loss: 188.48
 ---- batch: 100 ----
mean loss: 197.44
 ---- batch: 110 ----
mean loss: 189.97
train mean loss: 189.83
epoch train time: 0:00:00.719646
elapsed time: 0:03:10.841584
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 01:40:11.599349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.12
 ---- batch: 020 ----
mean loss: 194.72
 ---- batch: 030 ----
mean loss: 196.89
 ---- batch: 040 ----
mean loss: 192.62
 ---- batch: 050 ----
mean loss: 188.25
 ---- batch: 060 ----
mean loss: 190.31
 ---- batch: 070 ----
mean loss: 189.16
 ---- batch: 080 ----
mean loss: 179.11
 ---- batch: 090 ----
mean loss: 187.49
 ---- batch: 100 ----
mean loss: 191.04
 ---- batch: 110 ----
mean loss: 188.19
train mean loss: 189.76
epoch train time: 0:00:00.717594
elapsed time: 0:03:11.559320
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 01:40:12.317085
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.50
 ---- batch: 020 ----
mean loss: 191.82
 ---- batch: 030 ----
mean loss: 185.61
 ---- batch: 040 ----
mean loss: 189.68
 ---- batch: 050 ----
mean loss: 191.27
 ---- batch: 060 ----
mean loss: 182.64
 ---- batch: 070 ----
mean loss: 187.48
 ---- batch: 080 ----
mean loss: 194.53
 ---- batch: 090 ----
mean loss: 193.32
 ---- batch: 100 ----
mean loss: 191.18
 ---- batch: 110 ----
mean loss: 192.73
train mean loss: 189.77
epoch train time: 0:00:00.720644
elapsed time: 0:03:12.280136
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 01:40:13.037920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.56
 ---- batch: 020 ----
mean loss: 191.04
 ---- batch: 030 ----
mean loss: 192.49
 ---- batch: 040 ----
mean loss: 190.53
 ---- batch: 050 ----
mean loss: 199.01
 ---- batch: 060 ----
mean loss: 184.64
 ---- batch: 070 ----
mean loss: 187.10
 ---- batch: 080 ----
mean loss: 188.86
 ---- batch: 090 ----
mean loss: 191.43
 ---- batch: 100 ----
mean loss: 194.18
 ---- batch: 110 ----
mean loss: 188.68
train mean loss: 189.73
epoch train time: 0:00:00.729320
elapsed time: 0:03:13.009619
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 01:40:13.767386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.00
 ---- batch: 020 ----
mean loss: 184.98
 ---- batch: 030 ----
mean loss: 192.08
 ---- batch: 040 ----
mean loss: 197.58
 ---- batch: 050 ----
mean loss: 183.02
 ---- batch: 060 ----
mean loss: 186.69
 ---- batch: 070 ----
mean loss: 193.33
 ---- batch: 080 ----
mean loss: 195.99
 ---- batch: 090 ----
mean loss: 188.17
 ---- batch: 100 ----
mean loss: 189.28
 ---- batch: 110 ----
mean loss: 193.98
train mean loss: 189.70
epoch train time: 0:00:00.717520
elapsed time: 0:03:13.727290
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 01:40:14.485057
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.05
 ---- batch: 020 ----
mean loss: 194.43
 ---- batch: 030 ----
mean loss: 200.29
 ---- batch: 040 ----
mean loss: 185.93
 ---- batch: 050 ----
mean loss: 187.37
 ---- batch: 060 ----
mean loss: 186.91
 ---- batch: 070 ----
mean loss: 190.45
 ---- batch: 080 ----
mean loss: 184.48
 ---- batch: 090 ----
mean loss: 184.64
 ---- batch: 100 ----
mean loss: 193.59
 ---- batch: 110 ----
mean loss: 185.53
train mean loss: 189.75
epoch train time: 0:00:00.719537
elapsed time: 0:03:14.446970
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 01:40:15.204754
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.73
 ---- batch: 020 ----
mean loss: 191.62
 ---- batch: 030 ----
mean loss: 183.25
 ---- batch: 040 ----
mean loss: 191.31
 ---- batch: 050 ----
mean loss: 195.82
 ---- batch: 060 ----
mean loss: 187.08
 ---- batch: 070 ----
mean loss: 186.89
 ---- batch: 080 ----
mean loss: 189.40
 ---- batch: 090 ----
mean loss: 193.66
 ---- batch: 100 ----
mean loss: 191.00
 ---- batch: 110 ----
mean loss: 186.40
train mean loss: 189.73
epoch train time: 0:00:00.719411
elapsed time: 0:03:15.166536
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 01:40:15.924301
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.66
 ---- batch: 020 ----
mean loss: 194.41
 ---- batch: 030 ----
mean loss: 190.07
 ---- batch: 040 ----
mean loss: 191.82
 ---- batch: 050 ----
mean loss: 186.23
 ---- batch: 060 ----
mean loss: 199.78
 ---- batch: 070 ----
mean loss: 190.36
 ---- batch: 080 ----
mean loss: 193.07
 ---- batch: 090 ----
mean loss: 190.98
 ---- batch: 100 ----
mean loss: 176.29
 ---- batch: 110 ----
mean loss: 187.66
train mean loss: 189.72
epoch train time: 0:00:00.726622
elapsed time: 0:03:15.893298
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 01:40:16.651064
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.32
 ---- batch: 020 ----
mean loss: 185.99
 ---- batch: 030 ----
mean loss: 195.42
 ---- batch: 040 ----
mean loss: 197.12
 ---- batch: 050 ----
mean loss: 180.36
 ---- batch: 060 ----
mean loss: 189.81
 ---- batch: 070 ----
mean loss: 184.87
 ---- batch: 080 ----
mean loss: 185.41
 ---- batch: 090 ----
mean loss: 194.13
 ---- batch: 100 ----
mean loss: 194.31
 ---- batch: 110 ----
mean loss: 186.76
train mean loss: 189.74
epoch train time: 0:00:00.724870
elapsed time: 0:03:16.618312
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 01:40:17.376079
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.56
 ---- batch: 020 ----
mean loss: 197.27
 ---- batch: 030 ----
mean loss: 193.80
 ---- batch: 040 ----
mean loss: 185.88
 ---- batch: 050 ----
mean loss: 189.20
 ---- batch: 060 ----
mean loss: 177.45
 ---- batch: 070 ----
mean loss: 195.80
 ---- batch: 080 ----
mean loss: 182.51
 ---- batch: 090 ----
mean loss: 195.44
 ---- batch: 100 ----
mean loss: 196.22
 ---- batch: 110 ----
mean loss: 191.00
train mean loss: 189.63
epoch train time: 0:00:00.728848
elapsed time: 0:03:17.347306
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 01:40:18.105073
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.48
 ---- batch: 020 ----
mean loss: 190.98
 ---- batch: 030 ----
mean loss: 190.13
 ---- batch: 040 ----
mean loss: 184.52
 ---- batch: 050 ----
mean loss: 186.15
 ---- batch: 060 ----
mean loss: 192.77
 ---- batch: 070 ----
mean loss: 197.85
 ---- batch: 080 ----
mean loss: 197.33
 ---- batch: 090 ----
mean loss: 192.00
 ---- batch: 100 ----
mean loss: 191.95
 ---- batch: 110 ----
mean loss: 185.50
train mean loss: 189.66
epoch train time: 0:00:00.723308
elapsed time: 0:03:18.070789
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 01:40:18.828564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.65
 ---- batch: 020 ----
mean loss: 186.61
 ---- batch: 030 ----
mean loss: 185.01
 ---- batch: 040 ----
mean loss: 193.09
 ---- batch: 050 ----
mean loss: 192.77
 ---- batch: 060 ----
mean loss: 191.72
 ---- batch: 070 ----
mean loss: 196.81
 ---- batch: 080 ----
mean loss: 188.84
 ---- batch: 090 ----
mean loss: 182.85
 ---- batch: 100 ----
mean loss: 189.53
 ---- batch: 110 ----
mean loss: 188.52
train mean loss: 189.69
epoch train time: 0:00:00.725431
elapsed time: 0:03:18.796373
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 01:40:19.554159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.18
 ---- batch: 020 ----
mean loss: 194.70
 ---- batch: 030 ----
mean loss: 194.17
 ---- batch: 040 ----
mean loss: 191.63
 ---- batch: 050 ----
mean loss: 187.80
 ---- batch: 060 ----
mean loss: 189.90
 ---- batch: 070 ----
mean loss: 187.02
 ---- batch: 080 ----
mean loss: 180.60
 ---- batch: 090 ----
mean loss: 193.99
 ---- batch: 100 ----
mean loss: 194.40
 ---- batch: 110 ----
mean loss: 189.19
train mean loss: 189.61
epoch train time: 0:00:00.721903
elapsed time: 0:03:19.518448
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 01:40:20.276248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.87
 ---- batch: 020 ----
mean loss: 183.61
 ---- batch: 030 ----
mean loss: 184.26
 ---- batch: 040 ----
mean loss: 205.56
 ---- batch: 050 ----
mean loss: 183.29
 ---- batch: 060 ----
mean loss: 190.05
 ---- batch: 070 ----
mean loss: 189.98
 ---- batch: 080 ----
mean loss: 192.33
 ---- batch: 090 ----
mean loss: 184.33
 ---- batch: 100 ----
mean loss: 183.65
 ---- batch: 110 ----
mean loss: 194.17
train mean loss: 189.71
epoch train time: 0:00:00.726625
elapsed time: 0:03:20.245264
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 01:40:21.003031
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.94
 ---- batch: 020 ----
mean loss: 180.12
 ---- batch: 030 ----
mean loss: 182.24
 ---- batch: 040 ----
mean loss: 198.03
 ---- batch: 050 ----
mean loss: 193.04
 ---- batch: 060 ----
mean loss: 181.69
 ---- batch: 070 ----
mean loss: 190.82
 ---- batch: 080 ----
mean loss: 198.56
 ---- batch: 090 ----
mean loss: 188.97
 ---- batch: 100 ----
mean loss: 196.03
 ---- batch: 110 ----
mean loss: 192.88
train mean loss: 189.59
epoch train time: 0:00:00.737955
elapsed time: 0:03:20.983372
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 01:40:21.741148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.29
 ---- batch: 020 ----
mean loss: 185.31
 ---- batch: 030 ----
mean loss: 194.74
 ---- batch: 040 ----
mean loss: 194.51
 ---- batch: 050 ----
mean loss: 178.45
 ---- batch: 060 ----
mean loss: 194.36
 ---- batch: 070 ----
mean loss: 193.41
 ---- batch: 080 ----
mean loss: 193.69
 ---- batch: 090 ----
mean loss: 190.44
 ---- batch: 100 ----
mean loss: 185.08
 ---- batch: 110 ----
mean loss: 190.41
train mean loss: 189.55
epoch train time: 0:00:00.727527
elapsed time: 0:03:21.711054
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 01:40:22.468853
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.38
 ---- batch: 020 ----
mean loss: 181.73
 ---- batch: 030 ----
mean loss: 186.47
 ---- batch: 040 ----
mean loss: 191.97
 ---- batch: 050 ----
mean loss: 191.87
 ---- batch: 060 ----
mean loss: 198.33
 ---- batch: 070 ----
mean loss: 191.38
 ---- batch: 080 ----
mean loss: 191.53
 ---- batch: 090 ----
mean loss: 187.14
 ---- batch: 100 ----
mean loss: 191.55
 ---- batch: 110 ----
mean loss: 186.71
train mean loss: 189.57
epoch train time: 0:00:00.728806
elapsed time: 0:03:22.440046
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 01:40:23.197805
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.29
 ---- batch: 020 ----
mean loss: 186.52
 ---- batch: 030 ----
mean loss: 179.37
 ---- batch: 040 ----
mean loss: 191.91
 ---- batch: 050 ----
mean loss: 186.66
 ---- batch: 060 ----
mean loss: 191.56
 ---- batch: 070 ----
mean loss: 188.85
 ---- batch: 080 ----
mean loss: 197.63
 ---- batch: 090 ----
mean loss: 196.70
 ---- batch: 100 ----
mean loss: 190.53
 ---- batch: 110 ----
mean loss: 186.73
train mean loss: 189.57
epoch train time: 0:00:00.728364
elapsed time: 0:03:23.168549
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 01:40:23.926317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.21
 ---- batch: 020 ----
mean loss: 183.89
 ---- batch: 030 ----
mean loss: 185.01
 ---- batch: 040 ----
mean loss: 198.99
 ---- batch: 050 ----
mean loss: 185.52
 ---- batch: 060 ----
mean loss: 196.82
 ---- batch: 070 ----
mean loss: 193.65
 ---- batch: 080 ----
mean loss: 180.97
 ---- batch: 090 ----
mean loss: 186.95
 ---- batch: 100 ----
mean loss: 192.22
 ---- batch: 110 ----
mean loss: 185.09
train mean loss: 189.55
epoch train time: 0:00:00.720274
elapsed time: 0:03:23.888969
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 01:40:24.646773
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.25
 ---- batch: 020 ----
mean loss: 191.56
 ---- batch: 030 ----
mean loss: 185.82
 ---- batch: 040 ----
mean loss: 193.51
 ---- batch: 050 ----
mean loss: 190.93
 ---- batch: 060 ----
mean loss: 199.98
 ---- batch: 070 ----
mean loss: 192.28
 ---- batch: 080 ----
mean loss: 184.91
 ---- batch: 090 ----
mean loss: 183.71
 ---- batch: 100 ----
mean loss: 187.43
 ---- batch: 110 ----
mean loss: 186.54
train mean loss: 189.56
epoch train time: 0:00:00.719284
elapsed time: 0:03:24.608454
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 01:40:25.366223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.79
 ---- batch: 020 ----
mean loss: 198.66
 ---- batch: 030 ----
mean loss: 187.71
 ---- batch: 040 ----
mean loss: 188.04
 ---- batch: 050 ----
mean loss: 187.93
 ---- batch: 060 ----
mean loss: 192.74
 ---- batch: 070 ----
mean loss: 180.66
 ---- batch: 080 ----
mean loss: 187.31
 ---- batch: 090 ----
mean loss: 194.29
 ---- batch: 100 ----
mean loss: 191.16
 ---- batch: 110 ----
mean loss: 182.50
train mean loss: 189.52
epoch train time: 0:00:00.719588
elapsed time: 0:03:25.328197
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 01:40:26.085972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.73
 ---- batch: 020 ----
mean loss: 194.02
 ---- batch: 030 ----
mean loss: 189.41
 ---- batch: 040 ----
mean loss: 189.18
 ---- batch: 050 ----
mean loss: 187.28
 ---- batch: 060 ----
mean loss: 190.81
 ---- batch: 070 ----
mean loss: 201.88
 ---- batch: 080 ----
mean loss: 182.84
 ---- batch: 090 ----
mean loss: 180.30
 ---- batch: 100 ----
mean loss: 197.45
 ---- batch: 110 ----
mean loss: 185.52
train mean loss: 189.49
epoch train time: 0:00:00.717467
elapsed time: 0:03:26.045824
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 01:40:26.803593
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.45
 ---- batch: 020 ----
mean loss: 190.60
 ---- batch: 030 ----
mean loss: 190.58
 ---- batch: 040 ----
mean loss: 194.97
 ---- batch: 050 ----
mean loss: 184.75
 ---- batch: 060 ----
mean loss: 198.04
 ---- batch: 070 ----
mean loss: 192.42
 ---- batch: 080 ----
mean loss: 195.19
 ---- batch: 090 ----
mean loss: 182.01
 ---- batch: 100 ----
mean loss: 189.77
 ---- batch: 110 ----
mean loss: 183.79
train mean loss: 189.53
epoch train time: 0:00:00.720744
elapsed time: 0:03:26.766720
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 01:40:27.524488
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.28
 ---- batch: 020 ----
mean loss: 186.29
 ---- batch: 030 ----
mean loss: 193.48
 ---- batch: 040 ----
mean loss: 179.70
 ---- batch: 050 ----
mean loss: 185.86
 ---- batch: 060 ----
mean loss: 190.74
 ---- batch: 070 ----
mean loss: 191.18
 ---- batch: 080 ----
mean loss: 188.60
 ---- batch: 090 ----
mean loss: 203.45
 ---- batch: 100 ----
mean loss: 187.76
 ---- batch: 110 ----
mean loss: 192.82
train mean loss: 189.54
epoch train time: 0:00:00.725798
elapsed time: 0:03:27.492670
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 01:40:28.250439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.53
 ---- batch: 020 ----
mean loss: 188.97
 ---- batch: 030 ----
mean loss: 186.30
 ---- batch: 040 ----
mean loss: 186.65
 ---- batch: 050 ----
mean loss: 191.57
 ---- batch: 060 ----
mean loss: 195.85
 ---- batch: 070 ----
mean loss: 191.33
 ---- batch: 080 ----
mean loss: 189.97
 ---- batch: 090 ----
mean loss: 192.71
 ---- batch: 100 ----
mean loss: 199.25
 ---- batch: 110 ----
mean loss: 182.97
train mean loss: 189.48
epoch train time: 0:00:00.725459
elapsed time: 0:03:28.218275
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 01:40:28.976062
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.89
 ---- batch: 020 ----
mean loss: 205.12
 ---- batch: 030 ----
mean loss: 196.73
 ---- batch: 040 ----
mean loss: 180.91
 ---- batch: 050 ----
mean loss: 186.68
 ---- batch: 060 ----
mean loss: 191.55
 ---- batch: 070 ----
mean loss: 190.78
 ---- batch: 080 ----
mean loss: 172.73
 ---- batch: 090 ----
mean loss: 187.35
 ---- batch: 100 ----
mean loss: 192.48
 ---- batch: 110 ----
mean loss: 190.44
train mean loss: 189.43
epoch train time: 0:00:00.716852
elapsed time: 0:03:28.935305
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 01:40:29.693086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.93
 ---- batch: 020 ----
mean loss: 198.14
 ---- batch: 030 ----
mean loss: 184.69
 ---- batch: 040 ----
mean loss: 199.39
 ---- batch: 050 ----
mean loss: 195.22
 ---- batch: 060 ----
mean loss: 184.87
 ---- batch: 070 ----
mean loss: 195.52
 ---- batch: 080 ----
mean loss: 181.54
 ---- batch: 090 ----
mean loss: 182.73
 ---- batch: 100 ----
mean loss: 193.94
 ---- batch: 110 ----
mean loss: 183.95
train mean loss: 189.46
epoch train time: 0:00:00.714695
elapsed time: 0:03:29.650154
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 01:40:30.407934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.28
 ---- batch: 020 ----
mean loss: 190.10
 ---- batch: 030 ----
mean loss: 188.16
 ---- batch: 040 ----
mean loss: 186.85
 ---- batch: 050 ----
mean loss: 189.83
 ---- batch: 060 ----
mean loss: 183.90
 ---- batch: 070 ----
mean loss: 194.76
 ---- batch: 080 ----
mean loss: 196.72
 ---- batch: 090 ----
mean loss: 188.39
 ---- batch: 100 ----
mean loss: 191.67
 ---- batch: 110 ----
mean loss: 193.15
train mean loss: 189.43
epoch train time: 0:00:00.716238
elapsed time: 0:03:30.366545
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 01:40:31.124322
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.13
 ---- batch: 020 ----
mean loss: 186.49
 ---- batch: 030 ----
mean loss: 191.26
 ---- batch: 040 ----
mean loss: 188.11
 ---- batch: 050 ----
mean loss: 187.12
 ---- batch: 060 ----
mean loss: 192.44
 ---- batch: 070 ----
mean loss: 194.38
 ---- batch: 080 ----
mean loss: 184.94
 ---- batch: 090 ----
mean loss: 186.93
 ---- batch: 100 ----
mean loss: 194.93
 ---- batch: 110 ----
mean loss: 194.46
train mean loss: 189.43
epoch train time: 0:00:00.718595
elapsed time: 0:03:31.085328
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 01:40:31.843116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.61
 ---- batch: 020 ----
mean loss: 188.09
 ---- batch: 030 ----
mean loss: 186.21
 ---- batch: 040 ----
mean loss: 187.26
 ---- batch: 050 ----
mean loss: 192.12
 ---- batch: 060 ----
mean loss: 191.13
 ---- batch: 070 ----
mean loss: 189.85
 ---- batch: 080 ----
mean loss: 187.32
 ---- batch: 090 ----
mean loss: 185.32
 ---- batch: 100 ----
mean loss: 189.46
 ---- batch: 110 ----
mean loss: 190.43
train mean loss: 189.46
epoch train time: 0:00:00.717944
elapsed time: 0:03:31.803448
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 01:40:32.561213
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.68
 ---- batch: 020 ----
mean loss: 196.61
 ---- batch: 030 ----
mean loss: 191.82
 ---- batch: 040 ----
mean loss: 191.35
 ---- batch: 050 ----
mean loss: 187.69
 ---- batch: 060 ----
mean loss: 190.41
 ---- batch: 070 ----
mean loss: 189.97
 ---- batch: 080 ----
mean loss: 185.56
 ---- batch: 090 ----
mean loss: 189.29
 ---- batch: 100 ----
mean loss: 193.34
 ---- batch: 110 ----
mean loss: 195.93
train mean loss: 189.39
epoch train time: 0:00:00.716860
elapsed time: 0:03:32.520447
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 01:40:33.278210
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.63
 ---- batch: 020 ----
mean loss: 190.35
 ---- batch: 030 ----
mean loss: 188.70
 ---- batch: 040 ----
mean loss: 189.06
 ---- batch: 050 ----
mean loss: 186.73
 ---- batch: 060 ----
mean loss: 187.28
 ---- batch: 070 ----
mean loss: 185.31
 ---- batch: 080 ----
mean loss: 187.55
 ---- batch: 090 ----
mean loss: 190.94
 ---- batch: 100 ----
mean loss: 196.52
 ---- batch: 110 ----
mean loss: 192.84
train mean loss: 189.32
epoch train time: 0:00:00.714216
elapsed time: 0:03:33.234802
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 01:40:33.992588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.12
 ---- batch: 020 ----
mean loss: 189.19
 ---- batch: 030 ----
mean loss: 188.24
 ---- batch: 040 ----
mean loss: 189.09
 ---- batch: 050 ----
mean loss: 184.49
 ---- batch: 060 ----
mean loss: 196.10
 ---- batch: 070 ----
mean loss: 188.90
 ---- batch: 080 ----
mean loss: 191.13
 ---- batch: 090 ----
mean loss: 190.87
 ---- batch: 100 ----
mean loss: 194.52
 ---- batch: 110 ----
mean loss: 188.76
train mean loss: 189.35
epoch train time: 0:00:00.720652
elapsed time: 0:03:33.957806
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_3/checkpoint.pth.tar
**** end time: 2019-09-27 01:40:34.715536 ****
