Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 20871
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianDense3...
Done.
**** start time: 2019-09-25 20:16:57.935422 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
    BayesianLinear-2                  [-1, 100]          96,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 136,200
Trainable params: 136,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 20:16:57.944214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4072.65
 ---- batch: 020 ----
mean loss: 3849.98
 ---- batch: 030 ----
mean loss: 3739.56
 ---- batch: 040 ----
mean loss: 3520.54
 ---- batch: 050 ----
mean loss: 3275.75
 ---- batch: 060 ----
mean loss: 3212.00
 ---- batch: 070 ----
mean loss: 3017.65
 ---- batch: 080 ----
mean loss: 2940.39
 ---- batch: 090 ----
mean loss: 2820.75
train mean loss: 3341.90
epoch train time: 0:00:34.364040
elapsed time: 0:00:34.379648
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 20:17:32.315119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2604.79
 ---- batch: 020 ----
mean loss: 2615.71
 ---- batch: 030 ----
mean loss: 2521.28
 ---- batch: 040 ----
mean loss: 2447.15
 ---- batch: 050 ----
mean loss: 2335.47
 ---- batch: 060 ----
mean loss: 2312.71
 ---- batch: 070 ----
mean loss: 2263.54
 ---- batch: 080 ----
mean loss: 2199.91
 ---- batch: 090 ----
mean loss: 2179.23
train mean loss: 2370.78
epoch train time: 0:00:01.621603
elapsed time: 0:00:36.001678
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 20:17:33.937397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2082.24
 ---- batch: 020 ----
mean loss: 2026.04
 ---- batch: 030 ----
mean loss: 2024.20
 ---- batch: 040 ----
mean loss: 2001.74
 ---- batch: 050 ----
mean loss: 1978.92
 ---- batch: 060 ----
mean loss: 1921.46
 ---- batch: 070 ----
mean loss: 1920.44
 ---- batch: 080 ----
mean loss: 1872.64
 ---- batch: 090 ----
mean loss: 1824.96
train mean loss: 1953.84
epoch train time: 0:00:01.635286
elapsed time: 0:00:37.637538
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 20:17:35.573264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1797.47
 ---- batch: 020 ----
mean loss: 1753.69
 ---- batch: 030 ----
mean loss: 1715.13
 ---- batch: 040 ----
mean loss: 1748.69
 ---- batch: 050 ----
mean loss: 1693.46
 ---- batch: 060 ----
mean loss: 1678.75
 ---- batch: 070 ----
mean loss: 1621.65
 ---- batch: 080 ----
mean loss: 1617.22
 ---- batch: 090 ----
mean loss: 1603.89
train mean loss: 1685.65
epoch train time: 0:00:01.586500
elapsed time: 0:00:39.224689
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 20:17:37.160363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1560.13
 ---- batch: 020 ----
mean loss: 1549.37
 ---- batch: 030 ----
mean loss: 1523.58
 ---- batch: 040 ----
mean loss: 1486.98
 ---- batch: 050 ----
mean loss: 1506.95
 ---- batch: 060 ----
mean loss: 1463.59
 ---- batch: 070 ----
mean loss: 1470.13
 ---- batch: 080 ----
mean loss: 1454.10
 ---- batch: 090 ----
mean loss: 1427.57
train mean loss: 1487.72
epoch train time: 0:00:01.597156
elapsed time: 0:00:40.822487
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 20:17:38.758243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1385.30
 ---- batch: 020 ----
mean loss: 1359.68
 ---- batch: 030 ----
mean loss: 1377.89
 ---- batch: 040 ----
mean loss: 1324.77
 ---- batch: 050 ----
mean loss: 1362.19
 ---- batch: 060 ----
mean loss: 1296.52
 ---- batch: 070 ----
mean loss: 1326.09
 ---- batch: 080 ----
mean loss: 1316.14
 ---- batch: 090 ----
mean loss: 1283.65
train mean loss: 1332.95
epoch train time: 0:00:01.598448
elapsed time: 0:00:42.421561
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 20:17:40.357281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1257.48
 ---- batch: 020 ----
mean loss: 1274.75
 ---- batch: 030 ----
mean loss: 1213.81
 ---- batch: 040 ----
mean loss: 1245.69
 ---- batch: 050 ----
mean loss: 1234.48
 ---- batch: 060 ----
mean loss: 1212.57
 ---- batch: 070 ----
mean loss: 1206.59
 ---- batch: 080 ----
mean loss: 1190.95
 ---- batch: 090 ----
mean loss: 1185.98
train mean loss: 1221.15
epoch train time: 0:00:01.597625
elapsed time: 0:00:44.019905
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 20:17:41.955728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1159.14
 ---- batch: 020 ----
mean loss: 1167.20
 ---- batch: 030 ----
mean loss: 1175.01
 ---- batch: 040 ----
mean loss: 1122.06
 ---- batch: 050 ----
mean loss: 1137.67
 ---- batch: 060 ----
mean loss: 1135.63
 ---- batch: 070 ----
mean loss: 1118.87
 ---- batch: 080 ----
mean loss: 1117.39
 ---- batch: 090 ----
mean loss: 1097.80
train mean loss: 1133.25
epoch train time: 0:00:01.596828
elapsed time: 0:00:45.617519
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 20:17:43.553352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1096.25
 ---- batch: 020 ----
mean loss: 1074.88
 ---- batch: 030 ----
mean loss: 1078.28
 ---- batch: 040 ----
mean loss: 1078.59
 ---- batch: 050 ----
mean loss: 1084.49
 ---- batch: 060 ----
mean loss: 1051.51
 ---- batch: 070 ----
mean loss: 1067.43
 ---- batch: 080 ----
mean loss: 1030.08
 ---- batch: 090 ----
mean loss: 1066.30
train mean loss: 1068.39
epoch train time: 0:00:01.606850
elapsed time: 0:00:47.225097
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 20:17:45.160812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1047.92
 ---- batch: 020 ----
mean loss: 1022.30
 ---- batch: 030 ----
mean loss: 1028.04
 ---- batch: 040 ----
mean loss: 1007.03
 ---- batch: 050 ----
mean loss: 1031.14
 ---- batch: 060 ----
mean loss: 1025.77
 ---- batch: 070 ----
mean loss: 1022.94
 ---- batch: 080 ----
mean loss: 1001.70
 ---- batch: 090 ----
mean loss: 1012.81
train mean loss: 1019.42
epoch train time: 0:00:01.620949
elapsed time: 0:00:48.846704
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 20:17:46.782433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 987.20
 ---- batch: 020 ----
mean loss: 1003.63
 ---- batch: 030 ----
mean loss: 998.23
 ---- batch: 040 ----
mean loss: 974.30
 ---- batch: 050 ----
mean loss: 980.99
 ---- batch: 060 ----
mean loss: 991.85
 ---- batch: 070 ----
mean loss: 982.27
 ---- batch: 080 ----
mean loss: 965.46
 ---- batch: 090 ----
mean loss: 987.57
train mean loss: 983.90
epoch train time: 0:00:01.603222
elapsed time: 0:00:50.450535
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 20:17:48.386283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.03
 ---- batch: 020 ----
mean loss: 964.04
 ---- batch: 030 ----
mean loss: 969.78
 ---- batch: 040 ----
mean loss: 972.18
 ---- batch: 050 ----
mean loss: 974.63
 ---- batch: 060 ----
mean loss: 961.14
 ---- batch: 070 ----
mean loss: 967.98
 ---- batch: 080 ----
mean loss: 938.68
 ---- batch: 090 ----
mean loss: 937.03
train mean loss: 956.88
epoch train time: 0:00:01.612597
elapsed time: 0:00:52.063731
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 20:17:49.999366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.54
 ---- batch: 020 ----
mean loss: 937.74
 ---- batch: 030 ----
mean loss: 948.12
 ---- batch: 040 ----
mean loss: 928.76
 ---- batch: 050 ----
mean loss: 951.70
 ---- batch: 060 ----
mean loss: 944.13
 ---- batch: 070 ----
mean loss: 914.67
 ---- batch: 080 ----
mean loss: 942.45
 ---- batch: 090 ----
mean loss: 946.15
train mean loss: 939.16
epoch train time: 0:00:01.572110
elapsed time: 0:00:53.636381
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 20:17:51.572211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.38
 ---- batch: 020 ----
mean loss: 928.71
 ---- batch: 030 ----
mean loss: 928.52
 ---- batch: 040 ----
mean loss: 897.51
 ---- batch: 050 ----
mean loss: 927.12
 ---- batch: 060 ----
mean loss: 924.36
 ---- batch: 070 ----
mean loss: 940.65
 ---- batch: 080 ----
mean loss: 916.67
 ---- batch: 090 ----
mean loss: 924.36
train mean loss: 924.65
epoch train time: 0:00:01.598200
elapsed time: 0:00:55.235335
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 20:17:53.171124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.20
 ---- batch: 020 ----
mean loss: 936.55
 ---- batch: 030 ----
mean loss: 917.78
 ---- batch: 040 ----
mean loss: 902.62
 ---- batch: 050 ----
mean loss: 905.18
 ---- batch: 060 ----
mean loss: 908.80
 ---- batch: 070 ----
mean loss: 915.72
 ---- batch: 080 ----
mean loss: 921.47
 ---- batch: 090 ----
mean loss: 918.56
train mean loss: 917.22
epoch train time: 0:00:01.611176
elapsed time: 0:00:56.847314
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 20:17:54.783079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.04
 ---- batch: 020 ----
mean loss: 908.95
 ---- batch: 030 ----
mean loss: 911.73
 ---- batch: 040 ----
mean loss: 923.05
 ---- batch: 050 ----
mean loss: 909.28
 ---- batch: 060 ----
mean loss: 894.96
 ---- batch: 070 ----
mean loss: 878.65
 ---- batch: 080 ----
mean loss: 915.47
 ---- batch: 090 ----
mean loss: 914.14
train mean loss: 907.64
epoch train time: 0:00:01.592824
elapsed time: 0:00:58.440779
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 20:17:56.376524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 910.64
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 901.57
 ---- batch: 040 ----
mean loss: 911.11
 ---- batch: 050 ----
mean loss: 889.51
 ---- batch: 060 ----
mean loss: 911.40
 ---- batch: 070 ----
mean loss: 916.48
 ---- batch: 080 ----
mean loss: 919.72
 ---- batch: 090 ----
mean loss: 888.31
train mean loss: 904.21
epoch train time: 0:00:01.608952
elapsed time: 0:01:00.050384
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 20:17:57.986137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.46
 ---- batch: 020 ----
mean loss: 901.40
 ---- batch: 030 ----
mean loss: 915.32
 ---- batch: 040 ----
mean loss: 913.20
 ---- batch: 050 ----
mean loss: 898.17
 ---- batch: 060 ----
mean loss: 880.81
 ---- batch: 070 ----
mean loss: 901.45
 ---- batch: 080 ----
mean loss: 899.62
 ---- batch: 090 ----
mean loss: 886.83
train mean loss: 902.22
epoch train time: 0:00:01.610713
elapsed time: 0:01:01.661748
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 20:17:59.597538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.03
 ---- batch: 020 ----
mean loss: 907.91
 ---- batch: 030 ----
mean loss: 880.96
 ---- batch: 040 ----
mean loss: 911.14
 ---- batch: 050 ----
mean loss: 895.08
 ---- batch: 060 ----
mean loss: 895.63
 ---- batch: 070 ----
mean loss: 880.75
 ---- batch: 080 ----
mean loss: 907.91
 ---- batch: 090 ----
mean loss: 904.29
train mean loss: 897.90
epoch train time: 0:00:01.607695
elapsed time: 0:01:03.270162
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 20:18:01.205933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.14
 ---- batch: 020 ----
mean loss: 914.66
 ---- batch: 030 ----
mean loss: 898.99
 ---- batch: 040 ----
mean loss: 901.02
 ---- batch: 050 ----
mean loss: 888.42
 ---- batch: 060 ----
mean loss: 899.07
 ---- batch: 070 ----
mean loss: 905.45
 ---- batch: 080 ----
mean loss: 879.23
 ---- batch: 090 ----
mean loss: 892.39
train mean loss: 897.58
epoch train time: 0:00:01.585388
elapsed time: 0:01:04.856206
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 20:18:02.791914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.33
 ---- batch: 020 ----
mean loss: 890.05
 ---- batch: 030 ----
mean loss: 890.41
 ---- batch: 040 ----
mean loss: 896.27
 ---- batch: 050 ----
mean loss: 905.56
 ---- batch: 060 ----
mean loss: 892.02
 ---- batch: 070 ----
mean loss: 879.25
 ---- batch: 080 ----
mean loss: 911.30
 ---- batch: 090 ----
mean loss: 896.46
train mean loss: 894.74
epoch train time: 0:00:01.573103
elapsed time: 0:01:06.429902
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 20:18:04.365665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.20
 ---- batch: 020 ----
mean loss: 905.66
 ---- batch: 030 ----
mean loss: 884.97
 ---- batch: 040 ----
mean loss: 875.62
 ---- batch: 050 ----
mean loss: 884.55
 ---- batch: 060 ----
mean loss: 911.48
 ---- batch: 070 ----
mean loss: 899.47
 ---- batch: 080 ----
mean loss: 889.42
 ---- batch: 090 ----
mean loss: 895.62
train mean loss: 893.97
epoch train time: 0:00:01.604101
elapsed time: 0:01:08.034664
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 20:18:05.970214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.12
 ---- batch: 020 ----
mean loss: 895.40
 ---- batch: 030 ----
mean loss: 878.72
 ---- batch: 040 ----
mean loss: 888.34
 ---- batch: 050 ----
mean loss: 897.56
 ---- batch: 060 ----
mean loss: 892.52
 ---- batch: 070 ----
mean loss: 893.28
 ---- batch: 080 ----
mean loss: 898.36
 ---- batch: 090 ----
mean loss: 895.11
train mean loss: 893.61
epoch train time: 0:00:01.614488
elapsed time: 0:01:09.649551
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 20:18:07.585287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.21
 ---- batch: 020 ----
mean loss: 879.26
 ---- batch: 030 ----
mean loss: 898.32
 ---- batch: 040 ----
mean loss: 878.33
 ---- batch: 050 ----
mean loss: 887.24
 ---- batch: 060 ----
mean loss: 883.32
 ---- batch: 070 ----
mean loss: 894.68
 ---- batch: 080 ----
mean loss: 897.31
 ---- batch: 090 ----
mean loss: 889.61
train mean loss: 891.48
epoch train time: 0:00:01.603327
elapsed time: 0:01:11.253483
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 20:18:09.189200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.32
 ---- batch: 020 ----
mean loss: 911.14
 ---- batch: 030 ----
mean loss: 894.53
 ---- batch: 040 ----
mean loss: 893.56
 ---- batch: 050 ----
mean loss: 893.48
 ---- batch: 060 ----
mean loss: 895.33
 ---- batch: 070 ----
mean loss: 886.88
 ---- batch: 080 ----
mean loss: 881.34
 ---- batch: 090 ----
mean loss: 894.71
train mean loss: 890.18
epoch train time: 0:00:01.615486
elapsed time: 0:01:12.869575
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 20:18:10.805301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.66
 ---- batch: 020 ----
mean loss: 882.21
 ---- batch: 030 ----
mean loss: 882.89
 ---- batch: 040 ----
mean loss: 878.69
 ---- batch: 050 ----
mean loss: 883.70
 ---- batch: 060 ----
mean loss: 918.17
 ---- batch: 070 ----
mean loss: 889.47
 ---- batch: 080 ----
mean loss: 890.52
 ---- batch: 090 ----
mean loss: 881.85
train mean loss: 888.61
epoch train time: 0:00:01.594813
elapsed time: 0:01:14.464980
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 20:18:12.400736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.30
 ---- batch: 020 ----
mean loss: 890.13
 ---- batch: 030 ----
mean loss: 884.11
 ---- batch: 040 ----
mean loss: 885.96
 ---- batch: 050 ----
mean loss: 875.99
 ---- batch: 060 ----
mean loss: 882.54
 ---- batch: 070 ----
mean loss: 895.71
 ---- batch: 080 ----
mean loss: 904.54
 ---- batch: 090 ----
mean loss: 908.03
train mean loss: 891.10
epoch train time: 0:00:01.604445
elapsed time: 0:01:16.070127
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 20:18:14.005902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 897.97
 ---- batch: 020 ----
mean loss: 882.94
 ---- batch: 030 ----
mean loss: 891.47
 ---- batch: 040 ----
mean loss: 902.43
 ---- batch: 050 ----
mean loss: 888.92
 ---- batch: 060 ----
mean loss: 881.89
 ---- batch: 070 ----
mean loss: 878.99
 ---- batch: 080 ----
mean loss: 908.77
 ---- batch: 090 ----
mean loss: 872.16
train mean loss: 888.62
epoch train time: 0:00:01.617167
elapsed time: 0:01:17.687992
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 20:18:15.623776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.82
 ---- batch: 020 ----
mean loss: 888.16
 ---- batch: 030 ----
mean loss: 877.85
 ---- batch: 040 ----
mean loss: 887.85
 ---- batch: 050 ----
mean loss: 899.44
 ---- batch: 060 ----
mean loss: 893.26
 ---- batch: 070 ----
mean loss: 903.64
 ---- batch: 080 ----
mean loss: 882.72
 ---- batch: 090 ----
mean loss: 871.92
train mean loss: 889.94
epoch train time: 0:00:01.614792
elapsed time: 0:01:19.303530
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 20:18:17.239268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.85
 ---- batch: 020 ----
mean loss: 900.24
 ---- batch: 030 ----
mean loss: 884.51
 ---- batch: 040 ----
mean loss: 888.96
 ---- batch: 050 ----
mean loss: 883.98
 ---- batch: 060 ----
mean loss: 892.26
 ---- batch: 070 ----
mean loss: 882.16
 ---- batch: 080 ----
mean loss: 883.29
 ---- batch: 090 ----
mean loss: 866.20
train mean loss: 887.32
epoch train time: 0:00:01.605361
elapsed time: 0:01:20.909531
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 20:18:18.845307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.85
 ---- batch: 020 ----
mean loss: 885.14
 ---- batch: 030 ----
mean loss: 878.64
 ---- batch: 040 ----
mean loss: 890.69
 ---- batch: 050 ----
mean loss: 900.71
 ---- batch: 060 ----
mean loss: 877.73
 ---- batch: 070 ----
mean loss: 906.56
 ---- batch: 080 ----
mean loss: 881.43
 ---- batch: 090 ----
mean loss: 869.16
train mean loss: 886.99
epoch train time: 0:00:01.619568
elapsed time: 0:01:22.529834
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 20:18:20.465554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.27
 ---- batch: 020 ----
mean loss: 888.81
 ---- batch: 030 ----
mean loss: 884.77
 ---- batch: 040 ----
mean loss: 877.23
 ---- batch: 050 ----
mean loss: 886.68
 ---- batch: 060 ----
mean loss: 891.62
 ---- batch: 070 ----
mean loss: 870.30
 ---- batch: 080 ----
mean loss: 895.56
 ---- batch: 090 ----
mean loss: 883.21
train mean loss: 885.81
epoch train time: 0:00:01.588282
elapsed time: 0:01:24.118721
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 20:18:22.054471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.11
 ---- batch: 020 ----
mean loss: 887.40
 ---- batch: 030 ----
mean loss: 887.38
 ---- batch: 040 ----
mean loss: 877.85
 ---- batch: 050 ----
mean loss: 879.42
 ---- batch: 060 ----
mean loss: 879.95
 ---- batch: 070 ----
mean loss: 879.32
 ---- batch: 080 ----
mean loss: 887.51
 ---- batch: 090 ----
mean loss: 885.89
train mean loss: 885.89
epoch train time: 0:00:01.585350
elapsed time: 0:01:25.704728
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 20:18:23.640490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.73
 ---- batch: 020 ----
mean loss: 893.88
 ---- batch: 030 ----
mean loss: 876.13
 ---- batch: 040 ----
mean loss: 890.79
 ---- batch: 050 ----
mean loss: 896.27
 ---- batch: 060 ----
mean loss: 890.72
 ---- batch: 070 ----
mean loss: 894.59
 ---- batch: 080 ----
mean loss: 867.39
 ---- batch: 090 ----
mean loss: 886.66
train mean loss: 885.81
epoch train time: 0:00:01.602479
elapsed time: 0:01:27.307850
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 20:18:25.243576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.67
 ---- batch: 020 ----
mean loss: 879.69
 ---- batch: 030 ----
mean loss: 867.20
 ---- batch: 040 ----
mean loss: 888.89
 ---- batch: 050 ----
mean loss: 881.62
 ---- batch: 060 ----
mean loss: 898.64
 ---- batch: 070 ----
mean loss: 882.69
 ---- batch: 080 ----
mean loss: 882.17
 ---- batch: 090 ----
mean loss: 896.39
train mean loss: 884.23
epoch train time: 0:00:01.621410
elapsed time: 0:01:28.929868
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 20:18:26.865655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.73
 ---- batch: 020 ----
mean loss: 880.83
 ---- batch: 030 ----
mean loss: 884.10
 ---- batch: 040 ----
mean loss: 895.10
 ---- batch: 050 ----
mean loss: 898.57
 ---- batch: 060 ----
mean loss: 863.55
 ---- batch: 070 ----
mean loss: 867.02
 ---- batch: 080 ----
mean loss: 879.45
 ---- batch: 090 ----
mean loss: 920.18
train mean loss: 884.95
epoch train time: 0:00:01.584625
elapsed time: 0:01:30.515215
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 20:18:28.450935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.85
 ---- batch: 020 ----
mean loss: 877.02
 ---- batch: 030 ----
mean loss: 890.07
 ---- batch: 040 ----
mean loss: 903.66
 ---- batch: 050 ----
mean loss: 900.96
 ---- batch: 060 ----
mean loss: 880.75
 ---- batch: 070 ----
mean loss: 879.31
 ---- batch: 080 ----
mean loss: 859.76
 ---- batch: 090 ----
mean loss: 873.37
train mean loss: 882.88
epoch train time: 0:00:01.583948
elapsed time: 0:01:32.099804
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 20:18:30.035573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.14
 ---- batch: 020 ----
mean loss: 888.57
 ---- batch: 030 ----
mean loss: 889.81
 ---- batch: 040 ----
mean loss: 889.45
 ---- batch: 050 ----
mean loss: 872.28
 ---- batch: 060 ----
mean loss: 889.54
 ---- batch: 070 ----
mean loss: 875.79
 ---- batch: 080 ----
mean loss: 888.95
 ---- batch: 090 ----
mean loss: 866.90
train mean loss: 883.17
epoch train time: 0:00:01.602435
elapsed time: 0:01:33.702955
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 20:18:31.638711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.09
 ---- batch: 020 ----
mean loss: 882.53
 ---- batch: 030 ----
mean loss: 880.76
 ---- batch: 040 ----
mean loss: 880.94
 ---- batch: 050 ----
mean loss: 874.87
 ---- batch: 060 ----
mean loss: 890.74
 ---- batch: 070 ----
mean loss: 888.09
 ---- batch: 080 ----
mean loss: 874.66
 ---- batch: 090 ----
mean loss: 889.47
train mean loss: 884.20
epoch train time: 0:00:01.615925
elapsed time: 0:01:35.319552
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 20:18:33.255332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.69
 ---- batch: 020 ----
mean loss: 877.67
 ---- batch: 030 ----
mean loss: 875.90
 ---- batch: 040 ----
mean loss: 879.54
 ---- batch: 050 ----
mean loss: 882.67
 ---- batch: 060 ----
mean loss: 880.51
 ---- batch: 070 ----
mean loss: 880.67
 ---- batch: 080 ----
mean loss: 876.91
 ---- batch: 090 ----
mean loss: 887.74
train mean loss: 882.74
epoch train time: 0:00:01.587234
elapsed time: 0:01:36.907549
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 20:18:34.843284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.74
 ---- batch: 020 ----
mean loss: 895.22
 ---- batch: 030 ----
mean loss: 882.93
 ---- batch: 040 ----
mean loss: 869.03
 ---- batch: 050 ----
mean loss: 893.67
 ---- batch: 060 ----
mean loss: 885.56
 ---- batch: 070 ----
mean loss: 882.64
 ---- batch: 080 ----
mean loss: 869.88
 ---- batch: 090 ----
mean loss: 874.78
train mean loss: 882.17
epoch train time: 0:00:01.595586
elapsed time: 0:01:38.503827
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 20:18:36.439581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.57
 ---- batch: 020 ----
mean loss: 887.23
 ---- batch: 030 ----
mean loss: 887.05
 ---- batch: 040 ----
mean loss: 874.93
 ---- batch: 050 ----
mean loss: 869.07
 ---- batch: 060 ----
mean loss: 892.17
 ---- batch: 070 ----
mean loss: 886.12
 ---- batch: 080 ----
mean loss: 884.97
 ---- batch: 090 ----
mean loss: 888.24
train mean loss: 882.71
epoch train time: 0:00:01.644214
elapsed time: 0:01:40.148699
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 20:18:38.084420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.35
 ---- batch: 020 ----
mean loss: 867.69
 ---- batch: 030 ----
mean loss: 885.72
 ---- batch: 040 ----
mean loss: 857.44
 ---- batch: 050 ----
mean loss: 859.41
 ---- batch: 060 ----
mean loss: 878.94
 ---- batch: 070 ----
mean loss: 897.04
 ---- batch: 080 ----
mean loss: 901.19
 ---- batch: 090 ----
mean loss: 908.33
train mean loss: 882.83
epoch train time: 0:00:01.595503
elapsed time: 0:01:41.744883
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 20:18:39.680619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.23
 ---- batch: 020 ----
mean loss: 864.56
 ---- batch: 030 ----
mean loss: 885.23
 ---- batch: 040 ----
mean loss: 898.21
 ---- batch: 050 ----
mean loss: 876.98
 ---- batch: 060 ----
mean loss: 886.24
 ---- batch: 070 ----
mean loss: 876.23
 ---- batch: 080 ----
mean loss: 894.13
 ---- batch: 090 ----
mean loss: 894.20
train mean loss: 881.92
epoch train time: 0:00:01.629866
elapsed time: 0:01:43.375355
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 20:18:41.310904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.40
 ---- batch: 020 ----
mean loss: 901.90
 ---- batch: 030 ----
mean loss: 900.05
 ---- batch: 040 ----
mean loss: 881.26
 ---- batch: 050 ----
mean loss: 861.01
 ---- batch: 060 ----
mean loss: 890.61
 ---- batch: 070 ----
mean loss: 877.50
 ---- batch: 080 ----
mean loss: 879.01
 ---- batch: 090 ----
mean loss: 874.43
train mean loss: 880.78
epoch train time: 0:00:01.602044
elapsed time: 0:01:44.977791
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 20:18:42.913506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.22
 ---- batch: 020 ----
mean loss: 872.90
 ---- batch: 030 ----
mean loss: 874.31
 ---- batch: 040 ----
mean loss: 872.27
 ---- batch: 050 ----
mean loss: 866.83
 ---- batch: 060 ----
mean loss: 843.89
 ---- batch: 070 ----
mean loss: 829.76
 ---- batch: 080 ----
mean loss: 771.70
 ---- batch: 090 ----
mean loss: 693.63
train mean loss: 822.35
epoch train time: 0:00:01.606038
elapsed time: 0:01:46.584504
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 20:18:44.520271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 581.08
 ---- batch: 020 ----
mean loss: 539.06
 ---- batch: 030 ----
mean loss: 506.96
 ---- batch: 040 ----
mean loss: 483.13
 ---- batch: 050 ----
mean loss: 474.07
 ---- batch: 060 ----
mean loss: 461.50
 ---- batch: 070 ----
mean loss: 439.77
 ---- batch: 080 ----
mean loss: 428.62
 ---- batch: 090 ----
mean loss: 429.95
train mean loss: 478.65
epoch train time: 0:00:01.594522
elapsed time: 0:01:48.179751
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 20:18:46.115500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.08
 ---- batch: 020 ----
mean loss: 403.52
 ---- batch: 030 ----
mean loss: 391.74
 ---- batch: 040 ----
mean loss: 385.81
 ---- batch: 050 ----
mean loss: 411.63
 ---- batch: 060 ----
mean loss: 379.96
 ---- batch: 070 ----
mean loss: 376.96
 ---- batch: 080 ----
mean loss: 362.18
 ---- batch: 090 ----
mean loss: 365.93
train mean loss: 386.50
epoch train time: 0:00:01.571830
elapsed time: 0:01:49.752185
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 20:18:47.687962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.91
 ---- batch: 020 ----
mean loss: 349.88
 ---- batch: 030 ----
mean loss: 355.58
 ---- batch: 040 ----
mean loss: 340.95
 ---- batch: 050 ----
mean loss: 344.58
 ---- batch: 060 ----
mean loss: 343.93
 ---- batch: 070 ----
mean loss: 359.88
 ---- batch: 080 ----
mean loss: 349.25
 ---- batch: 090 ----
mean loss: 332.51
train mean loss: 347.09
epoch train time: 0:00:01.619029
elapsed time: 0:01:51.371901
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 20:18:49.307648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.74
 ---- batch: 020 ----
mean loss: 316.71
 ---- batch: 030 ----
mean loss: 337.42
 ---- batch: 040 ----
mean loss: 326.62
 ---- batch: 050 ----
mean loss: 324.74
 ---- batch: 060 ----
mean loss: 333.56
 ---- batch: 070 ----
mean loss: 319.73
 ---- batch: 080 ----
mean loss: 329.28
 ---- batch: 090 ----
mean loss: 316.95
train mean loss: 325.99
epoch train time: 0:00:01.615583
elapsed time: 0:01:52.988210
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 20:18:50.923948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.07
 ---- batch: 020 ----
mean loss: 313.63
 ---- batch: 030 ----
mean loss: 327.72
 ---- batch: 040 ----
mean loss: 307.85
 ---- batch: 050 ----
mean loss: 314.02
 ---- batch: 060 ----
mean loss: 299.67
 ---- batch: 070 ----
mean loss: 313.37
 ---- batch: 080 ----
mean loss: 298.85
 ---- batch: 090 ----
mean loss: 302.08
train mean loss: 310.39
epoch train time: 0:00:01.580695
elapsed time: 0:01:54.569557
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 20:18:52.505282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.58
 ---- batch: 020 ----
mean loss: 296.16
 ---- batch: 030 ----
mean loss: 305.18
 ---- batch: 040 ----
mean loss: 312.66
 ---- batch: 050 ----
mean loss: 296.05
 ---- batch: 060 ----
mean loss: 295.31
 ---- batch: 070 ----
mean loss: 293.81
 ---- batch: 080 ----
mean loss: 293.53
 ---- batch: 090 ----
mean loss: 299.65
train mean loss: 299.30
epoch train time: 0:00:01.599362
elapsed time: 0:01:56.169583
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 20:18:54.105344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.12
 ---- batch: 020 ----
mean loss: 289.82
 ---- batch: 030 ----
mean loss: 292.11
 ---- batch: 040 ----
mean loss: 284.42
 ---- batch: 050 ----
mean loss: 295.64
 ---- batch: 060 ----
mean loss: 294.91
 ---- batch: 070 ----
mean loss: 280.09
 ---- batch: 080 ----
mean loss: 286.36
 ---- batch: 090 ----
mean loss: 298.94
train mean loss: 290.71
epoch train time: 0:00:01.633468
elapsed time: 0:01:57.803680
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 20:18:55.739568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.05
 ---- batch: 020 ----
mean loss: 283.48
 ---- batch: 030 ----
mean loss: 293.43
 ---- batch: 040 ----
mean loss: 282.71
 ---- batch: 050 ----
mean loss: 280.63
 ---- batch: 060 ----
mean loss: 286.19
 ---- batch: 070 ----
mean loss: 274.05
 ---- batch: 080 ----
mean loss: 291.78
 ---- batch: 090 ----
mean loss: 278.01
train mean loss: 283.44
epoch train time: 0:00:01.627401
elapsed time: 0:01:59.431866
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 20:18:57.367597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.37
 ---- batch: 020 ----
mean loss: 284.52
 ---- batch: 030 ----
mean loss: 287.65
 ---- batch: 040 ----
mean loss: 281.50
 ---- batch: 050 ----
mean loss: 270.32
 ---- batch: 060 ----
mean loss: 277.96
 ---- batch: 070 ----
mean loss: 278.86
 ---- batch: 080 ----
mean loss: 260.34
 ---- batch: 090 ----
mean loss: 269.58
train mean loss: 276.15
epoch train time: 0:00:01.634991
elapsed time: 0:02:01.067452
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 20:18:59.003264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.58
 ---- batch: 020 ----
mean loss: 269.39
 ---- batch: 030 ----
mean loss: 272.06
 ---- batch: 040 ----
mean loss: 272.08
 ---- batch: 050 ----
mean loss: 272.27
 ---- batch: 060 ----
mean loss: 278.83
 ---- batch: 070 ----
mean loss: 276.83
 ---- batch: 080 ----
mean loss: 260.59
 ---- batch: 090 ----
mean loss: 267.09
train mean loss: 271.73
epoch train time: 0:00:01.615553
elapsed time: 0:02:02.683689
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 20:19:00.619420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.85
 ---- batch: 020 ----
mean loss: 256.39
 ---- batch: 030 ----
mean loss: 268.32
 ---- batch: 040 ----
mean loss: 271.10
 ---- batch: 050 ----
mean loss: 262.46
 ---- batch: 060 ----
mean loss: 258.14
 ---- batch: 070 ----
mean loss: 265.49
 ---- batch: 080 ----
mean loss: 257.63
 ---- batch: 090 ----
mean loss: 270.13
train mean loss: 263.05
epoch train time: 0:00:01.606746
elapsed time: 0:02:04.291036
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 20:19:02.226765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.64
 ---- batch: 020 ----
mean loss: 261.98
 ---- batch: 030 ----
mean loss: 266.09
 ---- batch: 040 ----
mean loss: 257.05
 ---- batch: 050 ----
mean loss: 266.60
 ---- batch: 060 ----
mean loss: 263.22
 ---- batch: 070 ----
mean loss: 249.86
 ---- batch: 080 ----
mean loss: 261.09
 ---- batch: 090 ----
mean loss: 255.44
train mean loss: 259.05
epoch train time: 0:00:01.624223
elapsed time: 0:02:05.915938
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 20:19:03.851671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.09
 ---- batch: 020 ----
mean loss: 252.21
 ---- batch: 030 ----
mean loss: 253.76
 ---- batch: 040 ----
mean loss: 250.88
 ---- batch: 050 ----
mean loss: 251.75
 ---- batch: 060 ----
mean loss: 248.88
 ---- batch: 070 ----
mean loss: 248.39
 ---- batch: 080 ----
mean loss: 257.00
 ---- batch: 090 ----
mean loss: 260.00
train mean loss: 254.43
epoch train time: 0:00:01.568297
elapsed time: 0:02:07.484839
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 20:19:05.420606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.66
 ---- batch: 020 ----
mean loss: 258.26
 ---- batch: 030 ----
mean loss: 246.56
 ---- batch: 040 ----
mean loss: 247.00
 ---- batch: 050 ----
mean loss: 251.42
 ---- batch: 060 ----
mean loss: 255.98
 ---- batch: 070 ----
mean loss: 256.58
 ---- batch: 080 ----
mean loss: 248.80
 ---- batch: 090 ----
mean loss: 248.28
train mean loss: 251.25
epoch train time: 0:00:01.627809
elapsed time: 0:02:09.113301
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 20:19:07.049022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.04
 ---- batch: 020 ----
mean loss: 244.07
 ---- batch: 030 ----
mean loss: 243.47
 ---- batch: 040 ----
mean loss: 248.39
 ---- batch: 050 ----
mean loss: 243.16
 ---- batch: 060 ----
mean loss: 246.10
 ---- batch: 070 ----
mean loss: 255.20
 ---- batch: 080 ----
mean loss: 255.93
 ---- batch: 090 ----
mean loss: 249.90
train mean loss: 247.69
epoch train time: 0:00:01.597228
elapsed time: 0:02:10.711170
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 20:19:08.646909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.42
 ---- batch: 020 ----
mean loss: 240.93
 ---- batch: 030 ----
mean loss: 240.80
 ---- batch: 040 ----
mean loss: 239.76
 ---- batch: 050 ----
mean loss: 244.85
 ---- batch: 060 ----
mean loss: 253.27
 ---- batch: 070 ----
mean loss: 241.79
 ---- batch: 080 ----
mean loss: 240.25
 ---- batch: 090 ----
mean loss: 257.03
train mean loss: 244.99
epoch train time: 0:00:01.617015
elapsed time: 0:02:12.328794
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 20:19:10.264515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.70
 ---- batch: 020 ----
mean loss: 234.58
 ---- batch: 030 ----
mean loss: 246.54
 ---- batch: 040 ----
mean loss: 235.74
 ---- batch: 050 ----
mean loss: 239.53
 ---- batch: 060 ----
mean loss: 246.13
 ---- batch: 070 ----
mean loss: 243.94
 ---- batch: 080 ----
mean loss: 238.38
 ---- batch: 090 ----
mean loss: 239.84
train mean loss: 240.29
epoch train time: 0:00:01.602977
elapsed time: 0:02:13.932397
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 20:19:11.868107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.65
 ---- batch: 020 ----
mean loss: 240.24
 ---- batch: 030 ----
mean loss: 231.17
 ---- batch: 040 ----
mean loss: 234.26
 ---- batch: 050 ----
mean loss: 248.81
 ---- batch: 060 ----
mean loss: 231.03
 ---- batch: 070 ----
mean loss: 238.88
 ---- batch: 080 ----
mean loss: 235.90
 ---- batch: 090 ----
mean loss: 235.09
train mean loss: 236.76
epoch train time: 0:00:01.626353
elapsed time: 0:02:15.559355
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 20:19:13.495083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.33
 ---- batch: 020 ----
mean loss: 230.81
 ---- batch: 030 ----
mean loss: 235.91
 ---- batch: 040 ----
mean loss: 238.78
 ---- batch: 050 ----
mean loss: 240.10
 ---- batch: 060 ----
mean loss: 233.17
 ---- batch: 070 ----
mean loss: 224.70
 ---- batch: 080 ----
mean loss: 227.37
 ---- batch: 090 ----
mean loss: 232.68
train mean loss: 234.17
epoch train time: 0:00:01.615594
elapsed time: 0:02:17.175582
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 20:19:15.111292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.16
 ---- batch: 020 ----
mean loss: 233.05
 ---- batch: 030 ----
mean loss: 228.05
 ---- batch: 040 ----
mean loss: 227.89
 ---- batch: 050 ----
mean loss: 231.88
 ---- batch: 060 ----
mean loss: 234.64
 ---- batch: 070 ----
mean loss: 232.62
 ---- batch: 080 ----
mean loss: 238.08
 ---- batch: 090 ----
mean loss: 229.42
train mean loss: 231.33
epoch train time: 0:00:01.633705
elapsed time: 0:02:18.809902
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 20:19:16.745663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.24
 ---- batch: 020 ----
mean loss: 220.82
 ---- batch: 030 ----
mean loss: 242.11
 ---- batch: 040 ----
mean loss: 239.00
 ---- batch: 050 ----
mean loss: 231.58
 ---- batch: 060 ----
mean loss: 231.25
 ---- batch: 070 ----
mean loss: 228.80
 ---- batch: 080 ----
mean loss: 222.24
 ---- batch: 090 ----
mean loss: 221.03
train mean loss: 228.78
epoch train time: 0:00:01.608314
elapsed time: 0:02:20.418840
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 20:19:18.354581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.31
 ---- batch: 020 ----
mean loss: 232.76
 ---- batch: 030 ----
mean loss: 220.37
 ---- batch: 040 ----
mean loss: 219.19
 ---- batch: 050 ----
mean loss: 222.36
 ---- batch: 060 ----
mean loss: 221.89
 ---- batch: 070 ----
mean loss: 232.22
 ---- batch: 080 ----
mean loss: 233.24
 ---- batch: 090 ----
mean loss: 227.31
train mean loss: 226.82
epoch train time: 0:00:01.606619
elapsed time: 0:02:22.026149
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 20:19:19.961919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.91
 ---- batch: 020 ----
mean loss: 228.62
 ---- batch: 030 ----
mean loss: 213.92
 ---- batch: 040 ----
mean loss: 224.44
 ---- batch: 050 ----
mean loss: 216.03
 ---- batch: 060 ----
mean loss: 218.82
 ---- batch: 070 ----
mean loss: 227.03
 ---- batch: 080 ----
mean loss: 223.07
 ---- batch: 090 ----
mean loss: 230.07
train mean loss: 223.11
epoch train time: 0:00:01.601495
elapsed time: 0:02:23.628347
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 20:19:21.564080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.79
 ---- batch: 020 ----
mean loss: 217.42
 ---- batch: 030 ----
mean loss: 216.63
 ---- batch: 040 ----
mean loss: 216.35
 ---- batch: 050 ----
mean loss: 220.85
 ---- batch: 060 ----
mean loss: 214.38
 ---- batch: 070 ----
mean loss: 207.70
 ---- batch: 080 ----
mean loss: 227.79
 ---- batch: 090 ----
mean loss: 226.48
train mean loss: 219.80
epoch train time: 0:00:01.607073
elapsed time: 0:02:25.236034
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 20:19:23.171749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.62
 ---- batch: 020 ----
mean loss: 210.19
 ---- batch: 030 ----
mean loss: 216.78
 ---- batch: 040 ----
mean loss: 224.86
 ---- batch: 050 ----
mean loss: 221.69
 ---- batch: 060 ----
mean loss: 215.56
 ---- batch: 070 ----
mean loss: 218.69
 ---- batch: 080 ----
mean loss: 213.89
 ---- batch: 090 ----
mean loss: 216.73
train mean loss: 216.90
epoch train time: 0:00:01.600805
elapsed time: 0:02:26.837442
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 20:19:24.773277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.88
 ---- batch: 020 ----
mean loss: 218.86
 ---- batch: 030 ----
mean loss: 215.82
 ---- batch: 040 ----
mean loss: 217.94
 ---- batch: 050 ----
mean loss: 220.99
 ---- batch: 060 ----
mean loss: 222.52
 ---- batch: 070 ----
mean loss: 212.42
 ---- batch: 080 ----
mean loss: 213.50
 ---- batch: 090 ----
mean loss: 217.85
train mean loss: 217.28
epoch train time: 0:00:01.647605
elapsed time: 0:02:28.485757
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 20:19:26.421474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.88
 ---- batch: 020 ----
mean loss: 199.11
 ---- batch: 030 ----
mean loss: 214.30
 ---- batch: 040 ----
mean loss: 210.18
 ---- batch: 050 ----
mean loss: 223.39
 ---- batch: 060 ----
mean loss: 212.67
 ---- batch: 070 ----
mean loss: 214.14
 ---- batch: 080 ----
mean loss: 223.95
 ---- batch: 090 ----
mean loss: 212.83
train mean loss: 211.75
epoch train time: 0:00:01.660180
elapsed time: 0:02:30.146650
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 20:19:28.082403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.85
 ---- batch: 020 ----
mean loss: 213.69
 ---- batch: 030 ----
mean loss: 214.94
 ---- batch: 040 ----
mean loss: 206.85
 ---- batch: 050 ----
mean loss: 213.10
 ---- batch: 060 ----
mean loss: 212.93
 ---- batch: 070 ----
mean loss: 211.93
 ---- batch: 080 ----
mean loss: 210.69
 ---- batch: 090 ----
mean loss: 212.55
train mean loss: 210.49
epoch train time: 0:00:01.609715
elapsed time: 0:02:31.757117
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 20:19:29.692899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.74
 ---- batch: 020 ----
mean loss: 208.06
 ---- batch: 030 ----
mean loss: 207.04
 ---- batch: 040 ----
mean loss: 215.22
 ---- batch: 050 ----
mean loss: 208.14
 ---- batch: 060 ----
mean loss: 207.88
 ---- batch: 070 ----
mean loss: 210.53
 ---- batch: 080 ----
mean loss: 207.74
 ---- batch: 090 ----
mean loss: 209.18
train mean loss: 209.27
epoch train time: 0:00:01.626466
elapsed time: 0:02:33.384317
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 20:19:31.320055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.97
 ---- batch: 020 ----
mean loss: 201.97
 ---- batch: 030 ----
mean loss: 205.40
 ---- batch: 040 ----
mean loss: 212.39
 ---- batch: 050 ----
mean loss: 204.22
 ---- batch: 060 ----
mean loss: 203.90
 ---- batch: 070 ----
mean loss: 210.89
 ---- batch: 080 ----
mean loss: 208.55
 ---- batch: 090 ----
mean loss: 205.23
train mean loss: 206.02
epoch train time: 0:00:01.593436
elapsed time: 0:02:34.978557
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 20:19:32.914312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.29
 ---- batch: 020 ----
mean loss: 199.78
 ---- batch: 030 ----
mean loss: 191.90
 ---- batch: 040 ----
mean loss: 203.13
 ---- batch: 050 ----
mean loss: 205.86
 ---- batch: 060 ----
mean loss: 203.62
 ---- batch: 070 ----
mean loss: 206.00
 ---- batch: 080 ----
mean loss: 212.89
 ---- batch: 090 ----
mean loss: 207.26
train mean loss: 203.92
epoch train time: 0:00:01.592784
elapsed time: 0:02:36.571967
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 20:19:34.507694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.54
 ---- batch: 020 ----
mean loss: 202.51
 ---- batch: 030 ----
mean loss: 197.46
 ---- batch: 040 ----
mean loss: 203.46
 ---- batch: 050 ----
mean loss: 196.58
 ---- batch: 060 ----
mean loss: 199.21
 ---- batch: 070 ----
mean loss: 206.04
 ---- batch: 080 ----
mean loss: 205.32
 ---- batch: 090 ----
mean loss: 204.73
train mean loss: 201.62
epoch train time: 0:00:01.601355
elapsed time: 0:02:38.173936
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 20:19:36.109693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.68
 ---- batch: 020 ----
mean loss: 193.92
 ---- batch: 030 ----
mean loss: 201.11
 ---- batch: 040 ----
mean loss: 210.49
 ---- batch: 050 ----
mean loss: 198.72
 ---- batch: 060 ----
mean loss: 195.44
 ---- batch: 070 ----
mean loss: 212.03
 ---- batch: 080 ----
mean loss: 200.04
 ---- batch: 090 ----
mean loss: 199.65
train mean loss: 199.98
epoch train time: 0:00:01.609394
elapsed time: 0:02:39.783988
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 20:19:37.719744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.72
 ---- batch: 020 ----
mean loss: 194.76
 ---- batch: 030 ----
mean loss: 190.85
 ---- batch: 040 ----
mean loss: 207.48
 ---- batch: 050 ----
mean loss: 200.37
 ---- batch: 060 ----
mean loss: 203.05
 ---- batch: 070 ----
mean loss: 194.31
 ---- batch: 080 ----
mean loss: 200.93
 ---- batch: 090 ----
mean loss: 200.59
train mean loss: 197.99
epoch train time: 0:00:01.613681
elapsed time: 0:02:41.398380
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 20:19:39.334139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.89
 ---- batch: 020 ----
mean loss: 189.84
 ---- batch: 030 ----
mean loss: 193.44
 ---- batch: 040 ----
mean loss: 195.19
 ---- batch: 050 ----
mean loss: 200.09
 ---- batch: 060 ----
mean loss: 196.69
 ---- batch: 070 ----
mean loss: 199.67
 ---- batch: 080 ----
mean loss: 198.38
 ---- batch: 090 ----
mean loss: 205.21
train mean loss: 196.95
epoch train time: 0:00:01.614977
elapsed time: 0:02:43.014056
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 20:19:40.949807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.28
 ---- batch: 020 ----
mean loss: 198.73
 ---- batch: 030 ----
mean loss: 202.79
 ---- batch: 040 ----
mean loss: 195.08
 ---- batch: 050 ----
mean loss: 198.78
 ---- batch: 060 ----
mean loss: 190.52
 ---- batch: 070 ----
mean loss: 191.79
 ---- batch: 080 ----
mean loss: 188.07
 ---- batch: 090 ----
mean loss: 192.58
train mean loss: 194.20
epoch train time: 0:00:01.617154
elapsed time: 0:02:44.631862
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 20:19:42.567602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.62
 ---- batch: 020 ----
mean loss: 190.17
 ---- batch: 030 ----
mean loss: 189.42
 ---- batch: 040 ----
mean loss: 189.66
 ---- batch: 050 ----
mean loss: 198.77
 ---- batch: 060 ----
mean loss: 196.40
 ---- batch: 070 ----
mean loss: 188.07
 ---- batch: 080 ----
mean loss: 199.62
 ---- batch: 090 ----
mean loss: 189.92
train mean loss: 192.64
epoch train time: 0:00:01.601088
elapsed time: 0:02:46.233571
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 20:19:44.169305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.44
 ---- batch: 020 ----
mean loss: 180.05
 ---- batch: 030 ----
mean loss: 188.81
 ---- batch: 040 ----
mean loss: 191.42
 ---- batch: 050 ----
mean loss: 198.07
 ---- batch: 060 ----
mean loss: 197.50
 ---- batch: 070 ----
mean loss: 191.02
 ---- batch: 080 ----
mean loss: 190.53
 ---- batch: 090 ----
mean loss: 196.50
train mean loss: 191.16
epoch train time: 0:00:01.594110
elapsed time: 0:02:47.828278
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 20:19:45.763990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.08
 ---- batch: 020 ----
mean loss: 187.30
 ---- batch: 030 ----
mean loss: 189.85
 ---- batch: 040 ----
mean loss: 183.77
 ---- batch: 050 ----
mean loss: 195.48
 ---- batch: 060 ----
mean loss: 193.88
 ---- batch: 070 ----
mean loss: 175.81
 ---- batch: 080 ----
mean loss: 188.05
 ---- batch: 090 ----
mean loss: 197.90
train mean loss: 189.97
epoch train time: 0:00:01.618173
elapsed time: 0:02:49.447014
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 20:19:47.382734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.99
 ---- batch: 020 ----
mean loss: 194.24
 ---- batch: 030 ----
mean loss: 185.14
 ---- batch: 040 ----
mean loss: 196.73
 ---- batch: 050 ----
mean loss: 188.53
 ---- batch: 060 ----
mean loss: 180.22
 ---- batch: 070 ----
mean loss: 179.06
 ---- batch: 080 ----
mean loss: 191.28
 ---- batch: 090 ----
mean loss: 188.69
train mean loss: 188.27
epoch train time: 0:00:01.632020
elapsed time: 0:02:51.079634
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 20:19:49.015199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.40
 ---- batch: 020 ----
mean loss: 181.64
 ---- batch: 030 ----
mean loss: 191.38
 ---- batch: 040 ----
mean loss: 190.92
 ---- batch: 050 ----
mean loss: 183.24
 ---- batch: 060 ----
mean loss: 184.38
 ---- batch: 070 ----
mean loss: 183.88
 ---- batch: 080 ----
mean loss: 189.21
 ---- batch: 090 ----
mean loss: 180.38
train mean loss: 186.03
epoch train time: 0:00:01.621176
elapsed time: 0:02:52.701283
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 20:19:50.637023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.91
 ---- batch: 020 ----
mean loss: 182.60
 ---- batch: 030 ----
mean loss: 181.22
 ---- batch: 040 ----
mean loss: 184.18
 ---- batch: 050 ----
mean loss: 176.29
 ---- batch: 060 ----
mean loss: 183.61
 ---- batch: 070 ----
mean loss: 188.69
 ---- batch: 080 ----
mean loss: 189.82
 ---- batch: 090 ----
mean loss: 193.47
train mean loss: 185.62
epoch train time: 0:00:01.595022
elapsed time: 0:02:54.296981
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 20:19:52.232696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.36
 ---- batch: 020 ----
mean loss: 180.92
 ---- batch: 030 ----
mean loss: 179.49
 ---- batch: 040 ----
mean loss: 186.57
 ---- batch: 050 ----
mean loss: 177.23
 ---- batch: 060 ----
mean loss: 187.05
 ---- batch: 070 ----
mean loss: 185.79
 ---- batch: 080 ----
mean loss: 182.55
 ---- batch: 090 ----
mean loss: 187.40
train mean loss: 183.58
epoch train time: 0:00:01.589075
elapsed time: 0:02:55.886722
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 20:19:53.822484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.75
 ---- batch: 020 ----
mean loss: 180.15
 ---- batch: 030 ----
mean loss: 179.82
 ---- batch: 040 ----
mean loss: 178.54
 ---- batch: 050 ----
mean loss: 183.69
 ---- batch: 060 ----
mean loss: 181.03
 ---- batch: 070 ----
mean loss: 186.52
 ---- batch: 080 ----
mean loss: 185.14
 ---- batch: 090 ----
mean loss: 183.88
train mean loss: 181.59
epoch train time: 0:00:01.586511
elapsed time: 0:02:57.473885
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 20:19:55.409639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.85
 ---- batch: 020 ----
mean loss: 178.29
 ---- batch: 030 ----
mean loss: 170.15
 ---- batch: 040 ----
mean loss: 178.12
 ---- batch: 050 ----
mean loss: 183.89
 ---- batch: 060 ----
mean loss: 189.48
 ---- batch: 070 ----
mean loss: 177.05
 ---- batch: 080 ----
mean loss: 187.56
 ---- batch: 090 ----
mean loss: 178.66
train mean loss: 180.21
epoch train time: 0:00:01.598121
elapsed time: 0:02:59.072700
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 20:19:57.008426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.00
 ---- batch: 020 ----
mean loss: 182.44
 ---- batch: 030 ----
mean loss: 177.01
 ---- batch: 040 ----
mean loss: 172.49
 ---- batch: 050 ----
mean loss: 182.16
 ---- batch: 060 ----
mean loss: 181.23
 ---- batch: 070 ----
mean loss: 179.78
 ---- batch: 080 ----
mean loss: 186.81
 ---- batch: 090 ----
mean loss: 175.92
train mean loss: 179.38
epoch train time: 0:00:01.571450
elapsed time: 0:03:00.644828
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 20:19:58.580557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.62
 ---- batch: 020 ----
mean loss: 180.50
 ---- batch: 030 ----
mean loss: 176.57
 ---- batch: 040 ----
mean loss: 180.71
 ---- batch: 050 ----
mean loss: 168.76
 ---- batch: 060 ----
mean loss: 181.37
 ---- batch: 070 ----
mean loss: 177.46
 ---- batch: 080 ----
mean loss: 185.82
 ---- batch: 090 ----
mean loss: 177.06
train mean loss: 178.45
epoch train time: 0:00:01.595994
elapsed time: 0:03:02.241390
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 20:20:00.177106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.39
 ---- batch: 020 ----
mean loss: 177.57
 ---- batch: 030 ----
mean loss: 172.87
 ---- batch: 040 ----
mean loss: 180.61
 ---- batch: 050 ----
mean loss: 173.09
 ---- batch: 060 ----
mean loss: 184.73
 ---- batch: 070 ----
mean loss: 185.20
 ---- batch: 080 ----
mean loss: 177.71
 ---- batch: 090 ----
mean loss: 172.84
train mean loss: 176.69
epoch train time: 0:00:01.607412
elapsed time: 0:03:03.849476
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 20:20:01.785269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.21
 ---- batch: 020 ----
mean loss: 166.45
 ---- batch: 030 ----
mean loss: 181.21
 ---- batch: 040 ----
mean loss: 172.05
 ---- batch: 050 ----
mean loss: 169.21
 ---- batch: 060 ----
mean loss: 183.16
 ---- batch: 070 ----
mean loss: 178.60
 ---- batch: 080 ----
mean loss: 177.23
 ---- batch: 090 ----
mean loss: 184.83
train mean loss: 175.71
epoch train time: 0:00:01.610542
elapsed time: 0:03:05.460673
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 20:20:03.396403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.73
 ---- batch: 020 ----
mean loss: 171.80
 ---- batch: 030 ----
mean loss: 170.59
 ---- batch: 040 ----
mean loss: 176.42
 ---- batch: 050 ----
mean loss: 175.58
 ---- batch: 060 ----
mean loss: 173.62
 ---- batch: 070 ----
mean loss: 169.74
 ---- batch: 080 ----
mean loss: 178.13
 ---- batch: 090 ----
mean loss: 174.28
train mean loss: 174.28
epoch train time: 0:00:01.616479
elapsed time: 0:03:07.077754
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 20:20:05.013478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.20
 ---- batch: 020 ----
mean loss: 166.18
 ---- batch: 030 ----
mean loss: 169.47
 ---- batch: 040 ----
mean loss: 174.95
 ---- batch: 050 ----
mean loss: 167.30
 ---- batch: 060 ----
mean loss: 170.72
 ---- batch: 070 ----
mean loss: 179.88
 ---- batch: 080 ----
mean loss: 178.67
 ---- batch: 090 ----
mean loss: 170.12
train mean loss: 172.00
epoch train time: 0:00:01.593637
elapsed time: 0:03:08.672047
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 20:20:06.607871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.51
 ---- batch: 020 ----
mean loss: 168.20
 ---- batch: 030 ----
mean loss: 168.44
 ---- batch: 040 ----
mean loss: 174.58
 ---- batch: 050 ----
mean loss: 168.36
 ---- batch: 060 ----
mean loss: 168.77
 ---- batch: 070 ----
mean loss: 177.80
 ---- batch: 080 ----
mean loss: 175.46
 ---- batch: 090 ----
mean loss: 170.65
train mean loss: 171.13
epoch train time: 0:00:01.607210
elapsed time: 0:03:10.280038
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 20:20:08.215792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.86
 ---- batch: 020 ----
mean loss: 171.68
 ---- batch: 030 ----
mean loss: 167.52
 ---- batch: 040 ----
mean loss: 172.13
 ---- batch: 050 ----
mean loss: 170.76
 ---- batch: 060 ----
mean loss: 175.07
 ---- batch: 070 ----
mean loss: 168.32
 ---- batch: 080 ----
mean loss: 171.90
 ---- batch: 090 ----
mean loss: 168.50
train mean loss: 170.18
epoch train time: 0:00:01.599685
elapsed time: 0:03:11.880392
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 20:20:09.816137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.44
 ---- batch: 020 ----
mean loss: 160.12
 ---- batch: 030 ----
mean loss: 168.46
 ---- batch: 040 ----
mean loss: 163.09
 ---- batch: 050 ----
mean loss: 172.86
 ---- batch: 060 ----
mean loss: 171.92
 ---- batch: 070 ----
mean loss: 170.68
 ---- batch: 080 ----
mean loss: 169.43
 ---- batch: 090 ----
mean loss: 170.82
train mean loss: 169.40
epoch train time: 0:00:01.592692
elapsed time: 0:03:13.473679
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 20:20:11.409403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.98
 ---- batch: 020 ----
mean loss: 160.63
 ---- batch: 030 ----
mean loss: 161.24
 ---- batch: 040 ----
mean loss: 166.38
 ---- batch: 050 ----
mean loss: 170.55
 ---- batch: 060 ----
mean loss: 161.80
 ---- batch: 070 ----
mean loss: 169.37
 ---- batch: 080 ----
mean loss: 174.13
 ---- batch: 090 ----
mean loss: 174.92
train mean loss: 167.89
epoch train time: 0:00:01.612073
elapsed time: 0:03:15.086378
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 20:20:13.022101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.85
 ---- batch: 020 ----
mean loss: 160.05
 ---- batch: 030 ----
mean loss: 166.48
 ---- batch: 040 ----
mean loss: 165.73
 ---- batch: 050 ----
mean loss: 168.47
 ---- batch: 060 ----
mean loss: 174.07
 ---- batch: 070 ----
mean loss: 167.97
 ---- batch: 080 ----
mean loss: 167.28
 ---- batch: 090 ----
mean loss: 171.22
train mean loss: 167.69
epoch train time: 0:00:01.586473
elapsed time: 0:03:16.673476
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 20:20:14.609245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.89
 ---- batch: 020 ----
mean loss: 162.57
 ---- batch: 030 ----
mean loss: 168.25
 ---- batch: 040 ----
mean loss: 165.56
 ---- batch: 050 ----
mean loss: 170.85
 ---- batch: 060 ----
mean loss: 167.30
 ---- batch: 070 ----
mean loss: 169.78
 ---- batch: 080 ----
mean loss: 164.62
 ---- batch: 090 ----
mean loss: 168.95
train mean loss: 167.02
epoch train time: 0:00:01.613597
elapsed time: 0:03:18.287758
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 20:20:16.223490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.64
 ---- batch: 020 ----
mean loss: 166.01
 ---- batch: 030 ----
mean loss: 157.36
 ---- batch: 040 ----
mean loss: 161.62
 ---- batch: 050 ----
mean loss: 166.05
 ---- batch: 060 ----
mean loss: 164.55
 ---- batch: 070 ----
mean loss: 171.43
 ---- batch: 080 ----
mean loss: 157.23
 ---- batch: 090 ----
mean loss: 168.68
train mean loss: 164.64
epoch train time: 0:00:01.614838
elapsed time: 0:03:19.903267
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 20:20:17.838991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.37
 ---- batch: 020 ----
mean loss: 173.05
 ---- batch: 030 ----
mean loss: 157.88
 ---- batch: 040 ----
mean loss: 167.33
 ---- batch: 050 ----
mean loss: 161.69
 ---- batch: 060 ----
mean loss: 168.22
 ---- batch: 070 ----
mean loss: 158.18
 ---- batch: 080 ----
mean loss: 163.23
 ---- batch: 090 ----
mean loss: 166.91
train mean loss: 164.52
epoch train time: 0:00:01.586135
elapsed time: 0:03:21.490045
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 20:20:19.425791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.63
 ---- batch: 020 ----
mean loss: 152.05
 ---- batch: 030 ----
mean loss: 166.97
 ---- batch: 040 ----
mean loss: 160.03
 ---- batch: 050 ----
mean loss: 158.02
 ---- batch: 060 ----
mean loss: 165.09
 ---- batch: 070 ----
mean loss: 168.43
 ---- batch: 080 ----
mean loss: 165.24
 ---- batch: 090 ----
mean loss: 169.91
train mean loss: 162.95
epoch train time: 0:00:01.615658
elapsed time: 0:03:23.106390
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 20:20:21.042179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.73
 ---- batch: 020 ----
mean loss: 164.73
 ---- batch: 030 ----
mean loss: 161.15
 ---- batch: 040 ----
mean loss: 156.34
 ---- batch: 050 ----
mean loss: 164.52
 ---- batch: 060 ----
mean loss: 162.05
 ---- batch: 070 ----
mean loss: 162.43
 ---- batch: 080 ----
mean loss: 162.38
 ---- batch: 090 ----
mean loss: 163.50
train mean loss: 162.37
epoch train time: 0:00:01.620902
elapsed time: 0:03:24.728185
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 20:20:22.663750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.58
 ---- batch: 020 ----
mean loss: 163.63
 ---- batch: 030 ----
mean loss: 156.45
 ---- batch: 040 ----
mean loss: 164.14
 ---- batch: 050 ----
mean loss: 163.19
 ---- batch: 060 ----
mean loss: 162.45
 ---- batch: 070 ----
mean loss: 167.91
 ---- batch: 080 ----
mean loss: 163.84
 ---- batch: 090 ----
mean loss: 161.96
train mean loss: 161.63
epoch train time: 0:00:01.600830
elapsed time: 0:03:26.329447
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 20:20:24.265202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.39
 ---- batch: 020 ----
mean loss: 160.57
 ---- batch: 030 ----
mean loss: 150.84
 ---- batch: 040 ----
mean loss: 164.18
 ---- batch: 050 ----
mean loss: 160.92
 ---- batch: 060 ----
mean loss: 160.89
 ---- batch: 070 ----
mean loss: 163.33
 ---- batch: 080 ----
mean loss: 161.97
 ---- batch: 090 ----
mean loss: 160.61
train mean loss: 159.41
epoch train time: 0:00:01.653124
elapsed time: 0:03:27.983327
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 20:20:25.919053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.49
 ---- batch: 020 ----
mean loss: 159.76
 ---- batch: 030 ----
mean loss: 157.77
 ---- batch: 040 ----
mean loss: 152.55
 ---- batch: 050 ----
mean loss: 155.79
 ---- batch: 060 ----
mean loss: 163.92
 ---- batch: 070 ----
mean loss: 160.24
 ---- batch: 080 ----
mean loss: 162.69
 ---- batch: 090 ----
mean loss: 159.53
train mean loss: 159.74
epoch train time: 0:00:01.635528
elapsed time: 0:03:29.619488
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 20:20:27.555268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.86
 ---- batch: 020 ----
mean loss: 155.28
 ---- batch: 030 ----
mean loss: 156.37
 ---- batch: 040 ----
mean loss: 152.21
 ---- batch: 050 ----
mean loss: 157.87
 ---- batch: 060 ----
mean loss: 158.99
 ---- batch: 070 ----
mean loss: 162.37
 ---- batch: 080 ----
mean loss: 157.55
 ---- batch: 090 ----
mean loss: 164.40
train mean loss: 158.50
epoch train time: 0:00:01.604304
elapsed time: 0:03:31.224421
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 20:20:29.160176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.74
 ---- batch: 020 ----
mean loss: 150.18
 ---- batch: 030 ----
mean loss: 161.54
 ---- batch: 040 ----
mean loss: 156.70
 ---- batch: 050 ----
mean loss: 158.10
 ---- batch: 060 ----
mean loss: 161.43
 ---- batch: 070 ----
mean loss: 161.05
 ---- batch: 080 ----
mean loss: 157.60
 ---- batch: 090 ----
mean loss: 158.83
train mean loss: 157.24
epoch train time: 0:00:01.618542
elapsed time: 0:03:32.843583
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 20:20:30.779318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.40
 ---- batch: 020 ----
mean loss: 155.57
 ---- batch: 030 ----
mean loss: 154.23
 ---- batch: 040 ----
mean loss: 152.26
 ---- batch: 050 ----
mean loss: 156.47
 ---- batch: 060 ----
mean loss: 158.30
 ---- batch: 070 ----
mean loss: 163.18
 ---- batch: 080 ----
mean loss: 157.21
 ---- batch: 090 ----
mean loss: 153.90
train mean loss: 156.60
epoch train time: 0:00:01.600949
elapsed time: 0:03:34.445132
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 20:20:32.380953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.59
 ---- batch: 020 ----
mean loss: 151.56
 ---- batch: 030 ----
mean loss: 149.69
 ---- batch: 040 ----
mean loss: 158.20
 ---- batch: 050 ----
mean loss: 156.24
 ---- batch: 060 ----
mean loss: 163.68
 ---- batch: 070 ----
mean loss: 151.33
 ---- batch: 080 ----
mean loss: 155.59
 ---- batch: 090 ----
mean loss: 155.09
train mean loss: 155.98
epoch train time: 0:00:01.639807
elapsed time: 0:03:36.085731
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 20:20:34.021522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.12
 ---- batch: 020 ----
mean loss: 152.62
 ---- batch: 030 ----
mean loss: 151.46
 ---- batch: 040 ----
mean loss: 155.41
 ---- batch: 050 ----
mean loss: 155.50
 ---- batch: 060 ----
mean loss: 157.81
 ---- batch: 070 ----
mean loss: 160.19
 ---- batch: 080 ----
mean loss: 145.91
 ---- batch: 090 ----
mean loss: 161.29
train mean loss: 154.73
epoch train time: 0:00:01.602360
elapsed time: 0:03:37.688772
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 20:20:35.624507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.98
 ---- batch: 020 ----
mean loss: 153.06
 ---- batch: 030 ----
mean loss: 151.61
 ---- batch: 040 ----
mean loss: 148.01
 ---- batch: 050 ----
mean loss: 163.67
 ---- batch: 060 ----
mean loss: 154.14
 ---- batch: 070 ----
mean loss: 154.58
 ---- batch: 080 ----
mean loss: 153.76
 ---- batch: 090 ----
mean loss: 159.98
train mean loss: 154.10
epoch train time: 0:00:01.627601
elapsed time: 0:03:39.316973
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 20:20:37.252729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.23
 ---- batch: 020 ----
mean loss: 141.53
 ---- batch: 030 ----
mean loss: 153.12
 ---- batch: 040 ----
mean loss: 151.44
 ---- batch: 050 ----
mean loss: 153.14
 ---- batch: 060 ----
mean loss: 157.31
 ---- batch: 070 ----
mean loss: 152.72
 ---- batch: 080 ----
mean loss: 165.36
 ---- batch: 090 ----
mean loss: 162.17
train mean loss: 152.99
epoch train time: 0:00:01.639221
elapsed time: 0:03:40.956892
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 20:20:38.892612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.07
 ---- batch: 020 ----
mean loss: 140.44
 ---- batch: 030 ----
mean loss: 155.51
 ---- batch: 040 ----
mean loss: 150.80
 ---- batch: 050 ----
mean loss: 152.06
 ---- batch: 060 ----
mean loss: 152.04
 ---- batch: 070 ----
mean loss: 156.68
 ---- batch: 080 ----
mean loss: 154.72
 ---- batch: 090 ----
mean loss: 165.55
train mean loss: 153.23
epoch train time: 0:00:01.607435
elapsed time: 0:03:42.564956
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 20:20:40.500672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.18
 ---- batch: 020 ----
mean loss: 150.52
 ---- batch: 030 ----
mean loss: 152.30
 ---- batch: 040 ----
mean loss: 152.07
 ---- batch: 050 ----
mean loss: 157.90
 ---- batch: 060 ----
mean loss: 153.32
 ---- batch: 070 ----
mean loss: 154.85
 ---- batch: 080 ----
mean loss: 152.14
 ---- batch: 090 ----
mean loss: 149.66
train mean loss: 151.98
epoch train time: 0:00:01.607241
elapsed time: 0:03:44.172916
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 20:20:42.108722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.16
 ---- batch: 020 ----
mean loss: 149.07
 ---- batch: 030 ----
mean loss: 149.01
 ---- batch: 040 ----
mean loss: 147.38
 ---- batch: 050 ----
mean loss: 154.45
 ---- batch: 060 ----
mean loss: 151.76
 ---- batch: 070 ----
mean loss: 154.69
 ---- batch: 080 ----
mean loss: 156.23
 ---- batch: 090 ----
mean loss: 146.53
train mean loss: 151.06
epoch train time: 0:00:01.634159
elapsed time: 0:03:45.807793
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 20:20:43.743557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.19
 ---- batch: 020 ----
mean loss: 144.84
 ---- batch: 030 ----
mean loss: 143.10
 ---- batch: 040 ----
mean loss: 147.60
 ---- batch: 050 ----
mean loss: 156.23
 ---- batch: 060 ----
mean loss: 151.04
 ---- batch: 070 ----
mean loss: 148.62
 ---- batch: 080 ----
mean loss: 155.35
 ---- batch: 090 ----
mean loss: 155.55
train mean loss: 149.76
epoch train time: 0:00:01.641279
elapsed time: 0:03:47.449740
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 20:20:45.385483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.87
 ---- batch: 020 ----
mean loss: 147.92
 ---- batch: 030 ----
mean loss: 150.32
 ---- batch: 040 ----
mean loss: 148.63
 ---- batch: 050 ----
mean loss: 148.63
 ---- batch: 060 ----
mean loss: 147.20
 ---- batch: 070 ----
mean loss: 153.20
 ---- batch: 080 ----
mean loss: 157.63
 ---- batch: 090 ----
mean loss: 153.11
train mean loss: 149.53
epoch train time: 0:00:01.599847
elapsed time: 0:03:49.050199
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 20:20:46.985923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.28
 ---- batch: 020 ----
mean loss: 146.98
 ---- batch: 030 ----
mean loss: 146.19
 ---- batch: 040 ----
mean loss: 151.29
 ---- batch: 050 ----
mean loss: 152.95
 ---- batch: 060 ----
mean loss: 148.70
 ---- batch: 070 ----
mean loss: 149.70
 ---- batch: 080 ----
mean loss: 152.04
 ---- batch: 090 ----
mean loss: 142.64
train mean loss: 148.92
epoch train time: 0:00:01.648608
elapsed time: 0:03:50.699423
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 20:20:48.635195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.04
 ---- batch: 020 ----
mean loss: 140.64
 ---- batch: 030 ----
mean loss: 143.39
 ---- batch: 040 ----
mean loss: 147.69
 ---- batch: 050 ----
mean loss: 151.01
 ---- batch: 060 ----
mean loss: 149.73
 ---- batch: 070 ----
mean loss: 150.40
 ---- batch: 080 ----
mean loss: 154.52
 ---- batch: 090 ----
mean loss: 152.63
train mean loss: 147.71
epoch train time: 0:00:01.649701
elapsed time: 0:03:52.349813
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 20:20:50.285588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.36
 ---- batch: 020 ----
mean loss: 147.57
 ---- batch: 030 ----
mean loss: 141.13
 ---- batch: 040 ----
mean loss: 150.67
 ---- batch: 050 ----
mean loss: 145.49
 ---- batch: 060 ----
mean loss: 154.45
 ---- batch: 070 ----
mean loss: 150.37
 ---- batch: 080 ----
mean loss: 145.55
 ---- batch: 090 ----
mean loss: 143.73
train mean loss: 147.30
epoch train time: 0:00:01.612793
elapsed time: 0:03:53.963394
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 20:20:51.899161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.21
 ---- batch: 020 ----
mean loss: 140.12
 ---- batch: 030 ----
mean loss: 142.74
 ---- batch: 040 ----
mean loss: 145.39
 ---- batch: 050 ----
mean loss: 146.04
 ---- batch: 060 ----
mean loss: 149.20
 ---- batch: 070 ----
mean loss: 146.88
 ---- batch: 080 ----
mean loss: 149.35
 ---- batch: 090 ----
mean loss: 155.98
train mean loss: 146.26
epoch train time: 0:00:01.620292
elapsed time: 0:03:55.584371
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 20:20:53.520109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.11
 ---- batch: 020 ----
mean loss: 142.71
 ---- batch: 030 ----
mean loss: 143.97
 ---- batch: 040 ----
mean loss: 146.46
 ---- batch: 050 ----
mean loss: 145.02
 ---- batch: 060 ----
mean loss: 148.71
 ---- batch: 070 ----
mean loss: 148.56
 ---- batch: 080 ----
mean loss: 149.90
 ---- batch: 090 ----
mean loss: 153.68
train mean loss: 146.07
epoch train time: 0:00:01.624042
elapsed time: 0:03:57.209189
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 20:20:55.144744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.22
 ---- batch: 020 ----
mean loss: 141.13
 ---- batch: 030 ----
mean loss: 143.36
 ---- batch: 040 ----
mean loss: 142.46
 ---- batch: 050 ----
mean loss: 147.81
 ---- batch: 060 ----
mean loss: 148.73
 ---- batch: 070 ----
mean loss: 143.44
 ---- batch: 080 ----
mean loss: 145.29
 ---- batch: 090 ----
mean loss: 154.30
train mean loss: 145.35
epoch train time: 0:00:01.586343
elapsed time: 0:03:58.796010
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 20:20:56.731762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.20
 ---- batch: 020 ----
mean loss: 135.13
 ---- batch: 030 ----
mean loss: 137.94
 ---- batch: 040 ----
mean loss: 143.75
 ---- batch: 050 ----
mean loss: 142.00
 ---- batch: 060 ----
mean loss: 147.94
 ---- batch: 070 ----
mean loss: 151.01
 ---- batch: 080 ----
mean loss: 152.21
 ---- batch: 090 ----
mean loss: 151.88
train mean loss: 143.86
epoch train time: 0:00:01.587011
elapsed time: 0:04:00.383624
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 20:20:58.319369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.60
 ---- batch: 020 ----
mean loss: 141.48
 ---- batch: 030 ----
mean loss: 141.33
 ---- batch: 040 ----
mean loss: 138.68
 ---- batch: 050 ----
mean loss: 139.22
 ---- batch: 060 ----
mean loss: 144.34
 ---- batch: 070 ----
mean loss: 147.76
 ---- batch: 080 ----
mean loss: 154.01
 ---- batch: 090 ----
mean loss: 147.68
train mean loss: 144.57
epoch train time: 0:00:01.600133
elapsed time: 0:04:01.984385
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 20:20:59.920154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.93
 ---- batch: 020 ----
mean loss: 145.51
 ---- batch: 030 ----
mean loss: 137.31
 ---- batch: 040 ----
mean loss: 141.72
 ---- batch: 050 ----
mean loss: 142.43
 ---- batch: 060 ----
mean loss: 140.34
 ---- batch: 070 ----
mean loss: 146.56
 ---- batch: 080 ----
mean loss: 142.34
 ---- batch: 090 ----
mean loss: 152.91
train mean loss: 143.17
epoch train time: 0:00:01.584429
elapsed time: 0:04:03.569452
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 20:21:01.505169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.99
 ---- batch: 020 ----
mean loss: 136.46
 ---- batch: 030 ----
mean loss: 144.13
 ---- batch: 040 ----
mean loss: 142.69
 ---- batch: 050 ----
mean loss: 145.49
 ---- batch: 060 ----
mean loss: 135.86
 ---- batch: 070 ----
mean loss: 148.09
 ---- batch: 080 ----
mean loss: 147.78
 ---- batch: 090 ----
mean loss: 140.77
train mean loss: 142.66
epoch train time: 0:00:01.592324
elapsed time: 0:04:05.162376
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 20:21:03.098096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.29
 ---- batch: 020 ----
mean loss: 135.49
 ---- batch: 030 ----
mean loss: 141.82
 ---- batch: 040 ----
mean loss: 138.84
 ---- batch: 050 ----
mean loss: 138.98
 ---- batch: 060 ----
mean loss: 147.07
 ---- batch: 070 ----
mean loss: 142.40
 ---- batch: 080 ----
mean loss: 145.82
 ---- batch: 090 ----
mean loss: 142.12
train mean loss: 141.62
epoch train time: 0:00:01.616627
elapsed time: 0:04:06.779753
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 20:21:04.715541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.40
 ---- batch: 020 ----
mean loss: 130.63
 ---- batch: 030 ----
mean loss: 138.65
 ---- batch: 040 ----
mean loss: 143.15
 ---- batch: 050 ----
mean loss: 141.29
 ---- batch: 060 ----
mean loss: 139.07
 ---- batch: 070 ----
mean loss: 145.40
 ---- batch: 080 ----
mean loss: 148.90
 ---- batch: 090 ----
mean loss: 147.58
train mean loss: 141.40
epoch train time: 0:00:01.583596
elapsed time: 0:04:08.364006
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 20:21:06.299760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.78
 ---- batch: 020 ----
mean loss: 139.71
 ---- batch: 030 ----
mean loss: 137.46
 ---- batch: 040 ----
mean loss: 138.54
 ---- batch: 050 ----
mean loss: 140.24
 ---- batch: 060 ----
mean loss: 145.83
 ---- batch: 070 ----
mean loss: 143.93
 ---- batch: 080 ----
mean loss: 139.55
 ---- batch: 090 ----
mean loss: 135.33
train mean loss: 140.24
epoch train time: 0:00:01.608128
elapsed time: 0:04:09.972843
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 20:21:07.908614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.14
 ---- batch: 020 ----
mean loss: 136.08
 ---- batch: 030 ----
mean loss: 137.50
 ---- batch: 040 ----
mean loss: 141.15
 ---- batch: 050 ----
mean loss: 132.83
 ---- batch: 060 ----
mean loss: 136.42
 ---- batch: 070 ----
mean loss: 140.37
 ---- batch: 080 ----
mean loss: 143.64
 ---- batch: 090 ----
mean loss: 145.13
train mean loss: 139.92
epoch train time: 0:00:01.603207
elapsed time: 0:04:11.576706
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 20:21:09.512452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.30
 ---- batch: 020 ----
mean loss: 134.47
 ---- batch: 030 ----
mean loss: 135.50
 ---- batch: 040 ----
mean loss: 138.21
 ---- batch: 050 ----
mean loss: 142.91
 ---- batch: 060 ----
mean loss: 139.83
 ---- batch: 070 ----
mean loss: 137.01
 ---- batch: 080 ----
mean loss: 137.78
 ---- batch: 090 ----
mean loss: 142.04
train mean loss: 138.85
epoch train time: 0:00:01.622102
elapsed time: 0:04:13.199453
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 20:21:11.135273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.93
 ---- batch: 020 ----
mean loss: 134.98
 ---- batch: 030 ----
mean loss: 138.77
 ---- batch: 040 ----
mean loss: 140.88
 ---- batch: 050 ----
mean loss: 137.38
 ---- batch: 060 ----
mean loss: 142.06
 ---- batch: 070 ----
mean loss: 138.67
 ---- batch: 080 ----
mean loss: 140.31
 ---- batch: 090 ----
mean loss: 141.06
train mean loss: 138.79
epoch train time: 0:00:01.607991
elapsed time: 0:04:14.808160
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 20:21:12.743909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.49
 ---- batch: 020 ----
mean loss: 136.03
 ---- batch: 030 ----
mean loss: 137.20
 ---- batch: 040 ----
mean loss: 133.98
 ---- batch: 050 ----
mean loss: 140.71
 ---- batch: 060 ----
mean loss: 140.04
 ---- batch: 070 ----
mean loss: 133.63
 ---- batch: 080 ----
mean loss: 141.75
 ---- batch: 090 ----
mean loss: 147.52
train mean loss: 138.37
epoch train time: 0:00:01.605322
elapsed time: 0:04:16.414098
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 20:21:14.349854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.58
 ---- batch: 020 ----
mean loss: 133.26
 ---- batch: 030 ----
mean loss: 139.68
 ---- batch: 040 ----
mean loss: 134.23
 ---- batch: 050 ----
mean loss: 135.23
 ---- batch: 060 ----
mean loss: 143.01
 ---- batch: 070 ----
mean loss: 139.17
 ---- batch: 080 ----
mean loss: 142.83
 ---- batch: 090 ----
mean loss: 139.08
train mean loss: 137.35
epoch train time: 0:00:01.601241
elapsed time: 0:04:18.016047
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 20:21:15.951786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.63
 ---- batch: 020 ----
mean loss: 137.54
 ---- batch: 030 ----
mean loss: 134.26
 ---- batch: 040 ----
mean loss: 137.17
 ---- batch: 050 ----
mean loss: 137.16
 ---- batch: 060 ----
mean loss: 137.44
 ---- batch: 070 ----
mean loss: 129.80
 ---- batch: 080 ----
mean loss: 146.19
 ---- batch: 090 ----
mean loss: 138.14
train mean loss: 136.52
epoch train time: 0:00:01.620487
elapsed time: 0:04:19.637113
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 20:21:17.572841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.04
 ---- batch: 020 ----
mean loss: 132.95
 ---- batch: 030 ----
mean loss: 133.97
 ---- batch: 040 ----
mean loss: 128.76
 ---- batch: 050 ----
mean loss: 142.10
 ---- batch: 060 ----
mean loss: 134.06
 ---- batch: 070 ----
mean loss: 145.09
 ---- batch: 080 ----
mean loss: 138.31
 ---- batch: 090 ----
mean loss: 144.04
train mean loss: 136.68
epoch train time: 0:00:01.624672
elapsed time: 0:04:21.262434
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 20:21:19.198148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.14
 ---- batch: 020 ----
mean loss: 131.05
 ---- batch: 030 ----
mean loss: 132.71
 ---- batch: 040 ----
mean loss: 132.84
 ---- batch: 050 ----
mean loss: 138.30
 ---- batch: 060 ----
mean loss: 140.97
 ---- batch: 070 ----
mean loss: 138.01
 ---- batch: 080 ----
mean loss: 139.66
 ---- batch: 090 ----
mean loss: 137.05
train mean loss: 136.08
epoch train time: 0:00:01.606788
elapsed time: 0:04:22.869845
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 20:21:20.805570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.15
 ---- batch: 020 ----
mean loss: 135.82
 ---- batch: 030 ----
mean loss: 132.33
 ---- batch: 040 ----
mean loss: 138.42
 ---- batch: 050 ----
mean loss: 134.26
 ---- batch: 060 ----
mean loss: 134.73
 ---- batch: 070 ----
mean loss: 138.53
 ---- batch: 080 ----
mean loss: 131.16
 ---- batch: 090 ----
mean loss: 136.33
train mean loss: 135.89
epoch train time: 0:00:01.610543
elapsed time: 0:04:24.481025
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 20:21:22.416817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.45
 ---- batch: 020 ----
mean loss: 133.06
 ---- batch: 030 ----
mean loss: 137.59
 ---- batch: 040 ----
mean loss: 138.31
 ---- batch: 050 ----
mean loss: 134.27
 ---- batch: 060 ----
mean loss: 140.15
 ---- batch: 070 ----
mean loss: 135.12
 ---- batch: 080 ----
mean loss: 133.13
 ---- batch: 090 ----
mean loss: 134.28
train mean loss: 134.56
epoch train time: 0:00:01.622484
elapsed time: 0:04:26.104247
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 20:21:24.040003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.67
 ---- batch: 020 ----
mean loss: 130.80
 ---- batch: 030 ----
mean loss: 129.01
 ---- batch: 040 ----
mean loss: 130.23
 ---- batch: 050 ----
mean loss: 141.76
 ---- batch: 060 ----
mean loss: 138.09
 ---- batch: 070 ----
mean loss: 131.25
 ---- batch: 080 ----
mean loss: 139.95
 ---- batch: 090 ----
mean loss: 137.97
train mean loss: 134.34
epoch train time: 0:00:01.579155
elapsed time: 0:04:27.684025
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 20:21:25.619748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.67
 ---- batch: 020 ----
mean loss: 124.90
 ---- batch: 030 ----
mean loss: 136.88
 ---- batch: 040 ----
mean loss: 133.54
 ---- batch: 050 ----
mean loss: 133.40
 ---- batch: 060 ----
mean loss: 134.58
 ---- batch: 070 ----
mean loss: 133.76
 ---- batch: 080 ----
mean loss: 134.51
 ---- batch: 090 ----
mean loss: 134.36
train mean loss: 133.44
epoch train time: 0:00:01.625074
elapsed time: 0:04:29.309801
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 20:21:27.245541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.18
 ---- batch: 020 ----
mean loss: 133.03
 ---- batch: 030 ----
mean loss: 132.29
 ---- batch: 040 ----
mean loss: 132.77
 ---- batch: 050 ----
mean loss: 129.02
 ---- batch: 060 ----
mean loss: 135.63
 ---- batch: 070 ----
mean loss: 134.00
 ---- batch: 080 ----
mean loss: 137.41
 ---- batch: 090 ----
mean loss: 133.89
train mean loss: 132.43
epoch train time: 0:00:01.607562
elapsed time: 0:04:30.917980
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 20:21:28.853707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.21
 ---- batch: 020 ----
mean loss: 132.35
 ---- batch: 030 ----
mean loss: 130.14
 ---- batch: 040 ----
mean loss: 133.65
 ---- batch: 050 ----
mean loss: 129.82
 ---- batch: 060 ----
mean loss: 133.64
 ---- batch: 070 ----
mean loss: 136.08
 ---- batch: 080 ----
mean loss: 134.77
 ---- batch: 090 ----
mean loss: 137.12
train mean loss: 133.14
epoch train time: 0:00:01.601741
elapsed time: 0:04:32.520629
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 20:21:30.456124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.38
 ---- batch: 020 ----
mean loss: 129.57
 ---- batch: 030 ----
mean loss: 129.36
 ---- batch: 040 ----
mean loss: 130.66
 ---- batch: 050 ----
mean loss: 131.73
 ---- batch: 060 ----
mean loss: 135.54
 ---- batch: 070 ----
mean loss: 135.95
 ---- batch: 080 ----
mean loss: 129.83
 ---- batch: 090 ----
mean loss: 138.63
train mean loss: 132.10
epoch train time: 0:00:01.611858
elapsed time: 0:04:34.132816
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 20:21:32.068524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.02
 ---- batch: 020 ----
mean loss: 132.36
 ---- batch: 030 ----
mean loss: 127.38
 ---- batch: 040 ----
mean loss: 134.72
 ---- batch: 050 ----
mean loss: 129.09
 ---- batch: 060 ----
mean loss: 132.94
 ---- batch: 070 ----
mean loss: 124.04
 ---- batch: 080 ----
mean loss: 135.48
 ---- batch: 090 ----
mean loss: 130.59
train mean loss: 131.13
epoch train time: 0:00:01.606753
elapsed time: 0:04:35.740196
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 20:21:33.675951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.79
 ---- batch: 020 ----
mean loss: 127.47
 ---- batch: 030 ----
mean loss: 127.62
 ---- batch: 040 ----
mean loss: 131.26
 ---- batch: 050 ----
mean loss: 131.78
 ---- batch: 060 ----
mean loss: 132.54
 ---- batch: 070 ----
mean loss: 129.01
 ---- batch: 080 ----
mean loss: 135.45
 ---- batch: 090 ----
mean loss: 133.63
train mean loss: 131.05
epoch train time: 0:00:01.613026
elapsed time: 0:04:37.353865
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 20:21:35.289663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.18
 ---- batch: 020 ----
mean loss: 128.88
 ---- batch: 030 ----
mean loss: 134.32
 ---- batch: 040 ----
mean loss: 128.94
 ---- batch: 050 ----
mean loss: 127.09
 ---- batch: 060 ----
mean loss: 127.70
 ---- batch: 070 ----
mean loss: 129.81
 ---- batch: 080 ----
mean loss: 135.71
 ---- batch: 090 ----
mean loss: 137.85
train mean loss: 130.49
epoch train time: 0:00:01.595652
elapsed time: 0:04:38.950312
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 20:21:36.886057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.46
 ---- batch: 020 ----
mean loss: 125.99
 ---- batch: 030 ----
mean loss: 131.96
 ---- batch: 040 ----
mean loss: 129.78
 ---- batch: 050 ----
mean loss: 129.16
 ---- batch: 060 ----
mean loss: 127.13
 ---- batch: 070 ----
mean loss: 130.36
 ---- batch: 080 ----
mean loss: 128.22
 ---- batch: 090 ----
mean loss: 134.21
train mean loss: 129.94
epoch train time: 0:00:01.607288
elapsed time: 0:04:40.558304
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 20:21:38.494047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.26
 ---- batch: 020 ----
mean loss: 129.42
 ---- batch: 030 ----
mean loss: 124.67
 ---- batch: 040 ----
mean loss: 131.13
 ---- batch: 050 ----
mean loss: 130.24
 ---- batch: 060 ----
mean loss: 130.21
 ---- batch: 070 ----
mean loss: 132.78
 ---- batch: 080 ----
mean loss: 136.34
 ---- batch: 090 ----
mean loss: 128.86
train mean loss: 129.39
epoch train time: 0:00:01.616344
elapsed time: 0:04:42.175274
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 20:21:40.111006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.36
 ---- batch: 020 ----
mean loss: 127.08
 ---- batch: 030 ----
mean loss: 126.30
 ---- batch: 040 ----
mean loss: 127.15
 ---- batch: 050 ----
mean loss: 129.91
 ---- batch: 060 ----
mean loss: 134.02
 ---- batch: 070 ----
mean loss: 130.01
 ---- batch: 080 ----
mean loss: 128.56
 ---- batch: 090 ----
mean loss: 132.40
train mean loss: 129.01
epoch train time: 0:00:01.659519
elapsed time: 0:04:43.835534
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 20:21:41.771300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.42
 ---- batch: 020 ----
mean loss: 127.61
 ---- batch: 030 ----
mean loss: 132.27
 ---- batch: 040 ----
mean loss: 128.45
 ---- batch: 050 ----
mean loss: 125.37
 ---- batch: 060 ----
mean loss: 126.75
 ---- batch: 070 ----
mean loss: 135.74
 ---- batch: 080 ----
mean loss: 133.10
 ---- batch: 090 ----
mean loss: 127.54
train mean loss: 128.65
epoch train time: 0:00:01.629782
elapsed time: 0:04:45.465994
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 20:21:43.401744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.24
 ---- batch: 020 ----
mean loss: 125.76
 ---- batch: 030 ----
mean loss: 122.72
 ---- batch: 040 ----
mean loss: 128.69
 ---- batch: 050 ----
mean loss: 128.32
 ---- batch: 060 ----
mean loss: 127.89
 ---- batch: 070 ----
mean loss: 130.93
 ---- batch: 080 ----
mean loss: 130.51
 ---- batch: 090 ----
mean loss: 132.10
train mean loss: 128.10
epoch train time: 0:00:01.650392
elapsed time: 0:04:47.117044
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 20:21:45.052827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.89
 ---- batch: 020 ----
mean loss: 122.63
 ---- batch: 030 ----
mean loss: 128.15
 ---- batch: 040 ----
mean loss: 128.67
 ---- batch: 050 ----
mean loss: 128.74
 ---- batch: 060 ----
mean loss: 120.66
 ---- batch: 070 ----
mean loss: 126.79
 ---- batch: 080 ----
mean loss: 130.93
 ---- batch: 090 ----
mean loss: 130.27
train mean loss: 127.95
epoch train time: 0:00:01.625173
elapsed time: 0:04:48.742976
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 20:21:46.678719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.18
 ---- batch: 020 ----
mean loss: 121.03
 ---- batch: 030 ----
mean loss: 121.56
 ---- batch: 040 ----
mean loss: 133.97
 ---- batch: 050 ----
mean loss: 130.75
 ---- batch: 060 ----
mean loss: 127.39
 ---- batch: 070 ----
mean loss: 130.59
 ---- batch: 080 ----
mean loss: 123.78
 ---- batch: 090 ----
mean loss: 131.71
train mean loss: 126.84
epoch train time: 0:00:01.617587
elapsed time: 0:04:50.361152
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 20:21:48.296874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.71
 ---- batch: 020 ----
mean loss: 125.05
 ---- batch: 030 ----
mean loss: 123.36
 ---- batch: 040 ----
mean loss: 125.85
 ---- batch: 050 ----
mean loss: 126.02
 ---- batch: 060 ----
mean loss: 130.78
 ---- batch: 070 ----
mean loss: 131.49
 ---- batch: 080 ----
mean loss: 129.03
 ---- batch: 090 ----
mean loss: 125.95
train mean loss: 126.31
epoch train time: 0:00:01.620006
elapsed time: 0:04:51.981885
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 20:21:49.917645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.92
 ---- batch: 020 ----
mean loss: 124.23
 ---- batch: 030 ----
mean loss: 124.57
 ---- batch: 040 ----
mean loss: 125.98
 ---- batch: 050 ----
mean loss: 119.05
 ---- batch: 060 ----
mean loss: 125.38
 ---- batch: 070 ----
mean loss: 133.81
 ---- batch: 080 ----
mean loss: 126.59
 ---- batch: 090 ----
mean loss: 126.13
train mean loss: 125.54
epoch train time: 0:00:01.582124
elapsed time: 0:04:53.564657
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 20:21:51.500369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.19
 ---- batch: 020 ----
mean loss: 127.15
 ---- batch: 030 ----
mean loss: 129.15
 ---- batch: 040 ----
mean loss: 127.95
 ---- batch: 050 ----
mean loss: 123.16
 ---- batch: 060 ----
mean loss: 126.14
 ---- batch: 070 ----
mean loss: 126.82
 ---- batch: 080 ----
mean loss: 126.71
 ---- batch: 090 ----
mean loss: 121.96
train mean loss: 125.33
epoch train time: 0:00:01.584966
elapsed time: 0:04:55.150172
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 20:21:53.085977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.38
 ---- batch: 020 ----
mean loss: 117.07
 ---- batch: 030 ----
mean loss: 122.26
 ---- batch: 040 ----
mean loss: 127.02
 ---- batch: 050 ----
mean loss: 122.89
 ---- batch: 060 ----
mean loss: 131.08
 ---- batch: 070 ----
mean loss: 127.21
 ---- batch: 080 ----
mean loss: 122.78
 ---- batch: 090 ----
mean loss: 134.70
train mean loss: 125.33
epoch train time: 0:00:01.601170
elapsed time: 0:04:56.752238
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 20:21:54.687973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.49
 ---- batch: 020 ----
mean loss: 123.11
 ---- batch: 030 ----
mean loss: 121.76
 ---- batch: 040 ----
mean loss: 115.88
 ---- batch: 050 ----
mean loss: 126.42
 ---- batch: 060 ----
mean loss: 126.76
 ---- batch: 070 ----
mean loss: 127.12
 ---- batch: 080 ----
mean loss: 123.12
 ---- batch: 090 ----
mean loss: 127.46
train mean loss: 124.76
epoch train time: 0:00:01.603771
elapsed time: 0:04:58.356645
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 20:21:56.292404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.50
 ---- batch: 020 ----
mean loss: 121.97
 ---- batch: 030 ----
mean loss: 121.28
 ---- batch: 040 ----
mean loss: 125.15
 ---- batch: 050 ----
mean loss: 128.93
 ---- batch: 060 ----
mean loss: 124.49
 ---- batch: 070 ----
mean loss: 126.69
 ---- batch: 080 ----
mean loss: 126.65
 ---- batch: 090 ----
mean loss: 121.92
train mean loss: 123.98
epoch train time: 0:00:01.609744
elapsed time: 0:04:59.967089
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 20:21:57.902832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.69
 ---- batch: 020 ----
mean loss: 125.29
 ---- batch: 030 ----
mean loss: 127.71
 ---- batch: 040 ----
mean loss: 124.20
 ---- batch: 050 ----
mean loss: 116.59
 ---- batch: 060 ----
mean loss: 117.04
 ---- batch: 070 ----
mean loss: 119.75
 ---- batch: 080 ----
mean loss: 125.95
 ---- batch: 090 ----
mean loss: 125.68
train mean loss: 123.49
epoch train time: 0:00:01.642749
elapsed time: 0:05:01.610565
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 20:21:59.546333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.15
 ---- batch: 020 ----
mean loss: 117.77
 ---- batch: 030 ----
mean loss: 126.07
 ---- batch: 040 ----
mean loss: 116.00
 ---- batch: 050 ----
mean loss: 122.10
 ---- batch: 060 ----
mean loss: 122.39
 ---- batch: 070 ----
mean loss: 135.45
 ---- batch: 080 ----
mean loss: 125.10
 ---- batch: 090 ----
mean loss: 125.88
train mean loss: 123.44
epoch train time: 0:00:01.615396
elapsed time: 0:05:03.226603
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 20:22:01.162361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.22
 ---- batch: 020 ----
mean loss: 122.76
 ---- batch: 030 ----
mean loss: 119.84
 ---- batch: 040 ----
mean loss: 116.07
 ---- batch: 050 ----
mean loss: 123.93
 ---- batch: 060 ----
mean loss: 128.78
 ---- batch: 070 ----
mean loss: 124.11
 ---- batch: 080 ----
mean loss: 128.68
 ---- batch: 090 ----
mean loss: 125.64
train mean loss: 123.81
epoch train time: 0:00:01.632717
elapsed time: 0:05:04.859988
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 20:22:02.795719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.86
 ---- batch: 020 ----
mean loss: 118.68
 ---- batch: 030 ----
mean loss: 118.34
 ---- batch: 040 ----
mean loss: 124.99
 ---- batch: 050 ----
mean loss: 116.87
 ---- batch: 060 ----
mean loss: 123.22
 ---- batch: 070 ----
mean loss: 127.46
 ---- batch: 080 ----
mean loss: 123.50
 ---- batch: 090 ----
mean loss: 125.38
train mean loss: 122.19
epoch train time: 0:00:01.578430
elapsed time: 0:05:06.439063
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 20:22:04.374841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.34
 ---- batch: 020 ----
mean loss: 122.06
 ---- batch: 030 ----
mean loss: 119.60
 ---- batch: 040 ----
mean loss: 121.85
 ---- batch: 050 ----
mean loss: 124.30
 ---- batch: 060 ----
mean loss: 120.86
 ---- batch: 070 ----
mean loss: 126.00
 ---- batch: 080 ----
mean loss: 123.51
 ---- batch: 090 ----
mean loss: 120.98
train mean loss: 121.74
epoch train time: 0:00:01.598039
elapsed time: 0:05:08.037734
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 20:22:05.973530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.01
 ---- batch: 020 ----
mean loss: 115.94
 ---- batch: 030 ----
mean loss: 121.66
 ---- batch: 040 ----
mean loss: 118.81
 ---- batch: 050 ----
mean loss: 120.20
 ---- batch: 060 ----
mean loss: 118.68
 ---- batch: 070 ----
mean loss: 125.21
 ---- batch: 080 ----
mean loss: 120.50
 ---- batch: 090 ----
mean loss: 131.26
train mean loss: 121.62
epoch train time: 0:00:01.612373
elapsed time: 0:05:09.650773
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 20:22:07.586480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.89
 ---- batch: 020 ----
mean loss: 119.69
 ---- batch: 030 ----
mean loss: 116.30
 ---- batch: 040 ----
mean loss: 119.44
 ---- batch: 050 ----
mean loss: 115.92
 ---- batch: 060 ----
mean loss: 117.50
 ---- batch: 070 ----
mean loss: 124.21
 ---- batch: 080 ----
mean loss: 127.37
 ---- batch: 090 ----
mean loss: 124.13
train mean loss: 120.78
epoch train time: 0:00:01.591746
elapsed time: 0:05:11.243124
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 20:22:09.178838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.80
 ---- batch: 020 ----
mean loss: 116.62
 ---- batch: 030 ----
mean loss: 118.02
 ---- batch: 040 ----
mean loss: 118.54
 ---- batch: 050 ----
mean loss: 119.21
 ---- batch: 060 ----
mean loss: 130.14
 ---- batch: 070 ----
mean loss: 120.90
 ---- batch: 080 ----
mean loss: 120.50
 ---- batch: 090 ----
mean loss: 122.87
train mean loss: 120.20
epoch train time: 0:00:01.593492
elapsed time: 0:05:12.837437
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 20:22:10.772946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.70
 ---- batch: 020 ----
mean loss: 122.32
 ---- batch: 030 ----
mean loss: 117.42
 ---- batch: 040 ----
mean loss: 117.82
 ---- batch: 050 ----
mean loss: 117.64
 ---- batch: 060 ----
mean loss: 120.53
 ---- batch: 070 ----
mean loss: 120.90
 ---- batch: 080 ----
mean loss: 122.14
 ---- batch: 090 ----
mean loss: 124.48
train mean loss: 120.70
epoch train time: 0:00:01.628138
elapsed time: 0:05:14.465916
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 20:22:12.401665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.34
 ---- batch: 020 ----
mean loss: 116.92
 ---- batch: 030 ----
mean loss: 120.41
 ---- batch: 040 ----
mean loss: 127.15
 ---- batch: 050 ----
mean loss: 125.61
 ---- batch: 060 ----
mean loss: 122.64
 ---- batch: 070 ----
mean loss: 122.46
 ---- batch: 080 ----
mean loss: 123.70
 ---- batch: 090 ----
mean loss: 116.85
train mean loss: 121.22
epoch train time: 0:00:01.603724
elapsed time: 0:05:16.070309
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 20:22:14.006050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.59
 ---- batch: 020 ----
mean loss: 120.40
 ---- batch: 030 ----
mean loss: 118.14
 ---- batch: 040 ----
mean loss: 120.13
 ---- batch: 050 ----
mean loss: 119.12
 ---- batch: 060 ----
mean loss: 119.62
 ---- batch: 070 ----
mean loss: 119.76
 ---- batch: 080 ----
mean loss: 117.37
 ---- batch: 090 ----
mean loss: 126.67
train mean loss: 119.97
epoch train time: 0:00:01.624258
elapsed time: 0:05:17.695259
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 20:22:15.630992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.72
 ---- batch: 020 ----
mean loss: 114.40
 ---- batch: 030 ----
mean loss: 122.44
 ---- batch: 040 ----
mean loss: 113.59
 ---- batch: 050 ----
mean loss: 114.27
 ---- batch: 060 ----
mean loss: 120.62
 ---- batch: 070 ----
mean loss: 124.28
 ---- batch: 080 ----
mean loss: 118.70
 ---- batch: 090 ----
mean loss: 118.30
train mean loss: 118.24
epoch train time: 0:00:01.609530
elapsed time: 0:05:19.305434
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 20:22:17.241166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.38
 ---- batch: 020 ----
mean loss: 115.19
 ---- batch: 030 ----
mean loss: 113.37
 ---- batch: 040 ----
mean loss: 118.83
 ---- batch: 050 ----
mean loss: 122.37
 ---- batch: 060 ----
mean loss: 121.70
 ---- batch: 070 ----
mean loss: 118.53
 ---- batch: 080 ----
mean loss: 118.76
 ---- batch: 090 ----
mean loss: 124.12
train mean loss: 118.68
epoch train time: 0:00:01.610808
elapsed time: 0:05:20.916876
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 20:22:18.852613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.84
 ---- batch: 020 ----
mean loss: 114.92
 ---- batch: 030 ----
mean loss: 117.59
 ---- batch: 040 ----
mean loss: 114.29
 ---- batch: 050 ----
mean loss: 120.66
 ---- batch: 060 ----
mean loss: 116.45
 ---- batch: 070 ----
mean loss: 121.15
 ---- batch: 080 ----
mean loss: 121.65
 ---- batch: 090 ----
mean loss: 120.09
train mean loss: 117.68
epoch train time: 0:00:01.608534
elapsed time: 0:05:22.526018
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 20:22:20.461750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.20
 ---- batch: 020 ----
mean loss: 116.71
 ---- batch: 030 ----
mean loss: 115.88
 ---- batch: 040 ----
mean loss: 116.70
 ---- batch: 050 ----
mean loss: 117.71
 ---- batch: 060 ----
mean loss: 111.72
 ---- batch: 070 ----
mean loss: 117.17
 ---- batch: 080 ----
mean loss: 116.54
 ---- batch: 090 ----
mean loss: 121.89
train mean loss: 117.16
epoch train time: 0:00:01.589501
elapsed time: 0:05:24.116181
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 20:22:22.051921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.91
 ---- batch: 020 ----
mean loss: 111.71
 ---- batch: 030 ----
mean loss: 114.37
 ---- batch: 040 ----
mean loss: 118.96
 ---- batch: 050 ----
mean loss: 115.53
 ---- batch: 060 ----
mean loss: 114.97
 ---- batch: 070 ----
mean loss: 115.81
 ---- batch: 080 ----
mean loss: 119.76
 ---- batch: 090 ----
mean loss: 120.59
train mean loss: 116.47
epoch train time: 0:00:01.597101
elapsed time: 0:05:25.713939
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 20:22:23.649675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.76
 ---- batch: 020 ----
mean loss: 117.38
 ---- batch: 030 ----
mean loss: 114.99
 ---- batch: 040 ----
mean loss: 113.04
 ---- batch: 050 ----
mean loss: 118.02
 ---- batch: 060 ----
mean loss: 117.70
 ---- batch: 070 ----
mean loss: 113.71
 ---- batch: 080 ----
mean loss: 118.17
 ---- batch: 090 ----
mean loss: 117.14
train mean loss: 116.46
epoch train time: 0:00:01.610014
elapsed time: 0:05:27.324576
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 20:22:25.260312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.15
 ---- batch: 020 ----
mean loss: 113.78
 ---- batch: 030 ----
mean loss: 111.47
 ---- batch: 040 ----
mean loss: 112.30
 ---- batch: 050 ----
mean loss: 116.65
 ---- batch: 060 ----
mean loss: 120.22
 ---- batch: 070 ----
mean loss: 117.69
 ---- batch: 080 ----
mean loss: 119.72
 ---- batch: 090 ----
mean loss: 115.76
train mean loss: 116.22
epoch train time: 0:00:01.625642
elapsed time: 0:05:28.950933
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 20:22:26.886630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.15
 ---- batch: 020 ----
mean loss: 111.58
 ---- batch: 030 ----
mean loss: 110.74
 ---- batch: 040 ----
mean loss: 116.99
 ---- batch: 050 ----
mean loss: 116.30
 ---- batch: 060 ----
mean loss: 118.41
 ---- batch: 070 ----
mean loss: 112.53
 ---- batch: 080 ----
mean loss: 120.96
 ---- batch: 090 ----
mean loss: 115.47
train mean loss: 115.45
epoch train time: 0:00:01.591563
elapsed time: 0:05:30.543153
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 20:22:28.478921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.97
 ---- batch: 020 ----
mean loss: 112.38
 ---- batch: 030 ----
mean loss: 112.06
 ---- batch: 040 ----
mean loss: 116.46
 ---- batch: 050 ----
mean loss: 122.80
 ---- batch: 060 ----
mean loss: 119.93
 ---- batch: 070 ----
mean loss: 118.46
 ---- batch: 080 ----
mean loss: 112.44
 ---- batch: 090 ----
mean loss: 118.23
train mean loss: 115.69
epoch train time: 0:00:01.633651
elapsed time: 0:05:32.177435
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 20:22:30.113126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.59
 ---- batch: 020 ----
mean loss: 107.99
 ---- batch: 030 ----
mean loss: 113.77
 ---- batch: 040 ----
mean loss: 114.12
 ---- batch: 050 ----
mean loss: 117.07
 ---- batch: 060 ----
mean loss: 114.74
 ---- batch: 070 ----
mean loss: 119.20
 ---- batch: 080 ----
mean loss: 115.64
 ---- batch: 090 ----
mean loss: 117.11
train mean loss: 115.00
epoch train time: 0:00:01.599482
elapsed time: 0:05:33.777461
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 20:22:31.713179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.35
 ---- batch: 020 ----
mean loss: 112.04
 ---- batch: 030 ----
mean loss: 110.17
 ---- batch: 040 ----
mean loss: 112.81
 ---- batch: 050 ----
mean loss: 116.80
 ---- batch: 060 ----
mean loss: 119.39
 ---- batch: 070 ----
mean loss: 114.88
 ---- batch: 080 ----
mean loss: 115.20
 ---- batch: 090 ----
mean loss: 116.08
train mean loss: 114.56
epoch train time: 0:00:01.593423
elapsed time: 0:05:35.371466
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 20:22:33.307210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.02
 ---- batch: 020 ----
mean loss: 112.15
 ---- batch: 030 ----
mean loss: 110.94
 ---- batch: 040 ----
mean loss: 111.38
 ---- batch: 050 ----
mean loss: 115.04
 ---- batch: 060 ----
mean loss: 116.45
 ---- batch: 070 ----
mean loss: 114.41
 ---- batch: 080 ----
mean loss: 117.97
 ---- batch: 090 ----
mean loss: 117.40
train mean loss: 114.14
epoch train time: 0:00:01.625175
elapsed time: 0:05:36.997266
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 20:22:34.933009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.50
 ---- batch: 020 ----
mean loss: 109.42
 ---- batch: 030 ----
mean loss: 112.49
 ---- batch: 040 ----
mean loss: 117.90
 ---- batch: 050 ----
mean loss: 113.25
 ---- batch: 060 ----
mean loss: 111.77
 ---- batch: 070 ----
mean loss: 114.57
 ---- batch: 080 ----
mean loss: 118.38
 ---- batch: 090 ----
mean loss: 120.02
train mean loss: 113.64
epoch train time: 0:00:01.601597
elapsed time: 0:05:38.599452
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 20:22:36.535182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.62
 ---- batch: 020 ----
mean loss: 111.49
 ---- batch: 030 ----
mean loss: 110.02
 ---- batch: 040 ----
mean loss: 113.82
 ---- batch: 050 ----
mean loss: 115.67
 ---- batch: 060 ----
mean loss: 117.27
 ---- batch: 070 ----
mean loss: 114.66
 ---- batch: 080 ----
mean loss: 114.15
 ---- batch: 090 ----
mean loss: 111.00
train mean loss: 113.22
epoch train time: 0:00:01.598305
elapsed time: 0:05:40.198359
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 20:22:38.134138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.36
 ---- batch: 020 ----
mean loss: 106.30
 ---- batch: 030 ----
mean loss: 114.64
 ---- batch: 040 ----
mean loss: 111.85
 ---- batch: 050 ----
mean loss: 115.37
 ---- batch: 060 ----
mean loss: 116.65
 ---- batch: 070 ----
mean loss: 118.02
 ---- batch: 080 ----
mean loss: 113.87
 ---- batch: 090 ----
mean loss: 108.64
train mean loss: 113.08
epoch train time: 0:00:01.610263
elapsed time: 0:05:41.809338
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 20:22:39.745126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.22
 ---- batch: 020 ----
mean loss: 114.29
 ---- batch: 030 ----
mean loss: 113.38
 ---- batch: 040 ----
mean loss: 114.18
 ---- batch: 050 ----
mean loss: 116.05
 ---- batch: 060 ----
mean loss: 119.63
 ---- batch: 070 ----
mean loss: 108.30
 ---- batch: 080 ----
mean loss: 109.44
 ---- batch: 090 ----
mean loss: 114.43
train mean loss: 112.93
epoch train time: 0:00:01.606380
elapsed time: 0:05:43.416361
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 20:22:41.352044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.26
 ---- batch: 020 ----
mean loss: 108.87
 ---- batch: 030 ----
mean loss: 112.63
 ---- batch: 040 ----
mean loss: 109.53
 ---- batch: 050 ----
mean loss: 113.05
 ---- batch: 060 ----
mean loss: 110.82
 ---- batch: 070 ----
mean loss: 109.39
 ---- batch: 080 ----
mean loss: 119.44
 ---- batch: 090 ----
mean loss: 120.65
train mean loss: 112.45
epoch train time: 0:00:01.615659
elapsed time: 0:05:45.032629
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 20:22:42.968367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.32
 ---- batch: 020 ----
mean loss: 109.07
 ---- batch: 030 ----
mean loss: 103.57
 ---- batch: 040 ----
mean loss: 111.36
 ---- batch: 050 ----
mean loss: 113.66
 ---- batch: 060 ----
mean loss: 114.15
 ---- batch: 070 ----
mean loss: 111.53
 ---- batch: 080 ----
mean loss: 117.18
 ---- batch: 090 ----
mean loss: 112.92
train mean loss: 112.07
epoch train time: 0:00:01.596253
elapsed time: 0:05:46.629544
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 20:22:44.565310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.69
 ---- batch: 020 ----
mean loss: 110.19
 ---- batch: 030 ----
mean loss: 114.09
 ---- batch: 040 ----
mean loss: 110.64
 ---- batch: 050 ----
mean loss: 109.49
 ---- batch: 060 ----
mean loss: 107.38
 ---- batch: 070 ----
mean loss: 120.32
 ---- batch: 080 ----
mean loss: 117.05
 ---- batch: 090 ----
mean loss: 110.03
train mean loss: 111.53
epoch train time: 0:00:01.623695
elapsed time: 0:05:48.253978
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 20:22:46.189818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.59
 ---- batch: 020 ----
mean loss: 107.26
 ---- batch: 030 ----
mean loss: 109.21
 ---- batch: 040 ----
mean loss: 108.84
 ---- batch: 050 ----
mean loss: 105.44
 ---- batch: 060 ----
mean loss: 115.88
 ---- batch: 070 ----
mean loss: 111.97
 ---- batch: 080 ----
mean loss: 104.71
 ---- batch: 090 ----
mean loss: 116.67
train mean loss: 110.96
epoch train time: 0:00:01.623244
elapsed time: 0:05:49.878022
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 20:22:47.813847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.27
 ---- batch: 020 ----
mean loss: 107.49
 ---- batch: 030 ----
mean loss: 108.80
 ---- batch: 040 ----
mean loss: 104.36
 ---- batch: 050 ----
mean loss: 108.80
 ---- batch: 060 ----
mean loss: 114.50
 ---- batch: 070 ----
mean loss: 110.17
 ---- batch: 080 ----
mean loss: 113.91
 ---- batch: 090 ----
mean loss: 122.95
train mean loss: 111.33
epoch train time: 0:00:01.610659
elapsed time: 0:05:51.489421
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 20:22:49.425180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.29
 ---- batch: 020 ----
mean loss: 106.73
 ---- batch: 030 ----
mean loss: 112.80
 ---- batch: 040 ----
mean loss: 111.88
 ---- batch: 050 ----
mean loss: 112.68
 ---- batch: 060 ----
mean loss: 115.19
 ---- batch: 070 ----
mean loss: 109.85
 ---- batch: 080 ----
mean loss: 114.14
 ---- batch: 090 ----
mean loss: 114.20
train mean loss: 111.06
epoch train time: 0:00:01.627944
elapsed time: 0:05:53.118090
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 20:22:51.053834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.34
 ---- batch: 020 ----
mean loss: 107.28
 ---- batch: 030 ----
mean loss: 107.99
 ---- batch: 040 ----
mean loss: 108.08
 ---- batch: 050 ----
mean loss: 111.90
 ---- batch: 060 ----
mean loss: 113.14
 ---- batch: 070 ----
mean loss: 112.44
 ---- batch: 080 ----
mean loss: 115.76
 ---- batch: 090 ----
mean loss: 111.78
train mean loss: 110.49
epoch train time: 0:00:01.611105
elapsed time: 0:05:54.729851
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 20:22:52.665658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.33
 ---- batch: 020 ----
mean loss: 110.67
 ---- batch: 030 ----
mean loss: 104.06
 ---- batch: 040 ----
mean loss: 108.70
 ---- batch: 050 ----
mean loss: 102.64
 ---- batch: 060 ----
mean loss: 109.03
 ---- batch: 070 ----
mean loss: 117.58
 ---- batch: 080 ----
mean loss: 114.89
 ---- batch: 090 ----
mean loss: 112.87
train mean loss: 109.96
epoch train time: 0:00:01.599408
elapsed time: 0:05:56.329989
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 20:22:54.265761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.67
 ---- batch: 020 ----
mean loss: 108.75
 ---- batch: 030 ----
mean loss: 101.59
 ---- batch: 040 ----
mean loss: 108.85
 ---- batch: 050 ----
mean loss: 101.48
 ---- batch: 060 ----
mean loss: 103.72
 ---- batch: 070 ----
mean loss: 103.11
 ---- batch: 080 ----
mean loss: 104.62
 ---- batch: 090 ----
mean loss: 102.71
train mean loss: 104.45
epoch train time: 0:00:01.588938
elapsed time: 0:05:57.919936
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 20:22:55.855423
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.67
 ---- batch: 020 ----
mean loss: 98.73
 ---- batch: 030 ----
mean loss: 106.03
 ---- batch: 040 ----
mean loss: 100.23
 ---- batch: 050 ----
mean loss: 105.31
 ---- batch: 060 ----
mean loss: 111.64
 ---- batch: 070 ----
mean loss: 101.72
 ---- batch: 080 ----
mean loss: 102.61
 ---- batch: 090 ----
mean loss: 101.69
train mean loss: 103.57
epoch train time: 0:00:01.599371
elapsed time: 0:05:59.519649
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 20:22:57.455411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.04
 ---- batch: 020 ----
mean loss: 102.08
 ---- batch: 030 ----
mean loss: 99.99
 ---- batch: 040 ----
mean loss: 105.37
 ---- batch: 050 ----
mean loss: 104.75
 ---- batch: 060 ----
mean loss: 99.57
 ---- batch: 070 ----
mean loss: 103.47
 ---- batch: 080 ----
mean loss: 106.70
 ---- batch: 090 ----
mean loss: 103.84
train mean loss: 103.38
epoch train time: 0:00:01.615877
elapsed time: 0:06:01.136177
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 20:22:59.071914
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.97
 ---- batch: 020 ----
mean loss: 102.12
 ---- batch: 030 ----
mean loss: 107.15
 ---- batch: 040 ----
mean loss: 100.05
 ---- batch: 050 ----
mean loss: 101.78
 ---- batch: 060 ----
mean loss: 104.03
 ---- batch: 070 ----
mean loss: 100.31
 ---- batch: 080 ----
mean loss: 109.47
 ---- batch: 090 ----
mean loss: 101.71
train mean loss: 103.18
epoch train time: 0:00:01.590287
elapsed time: 0:06:02.727084
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 20:23:00.662794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.98
 ---- batch: 020 ----
mean loss: 103.92
 ---- batch: 030 ----
mean loss: 103.56
 ---- batch: 040 ----
mean loss: 98.92
 ---- batch: 050 ----
mean loss: 107.33
 ---- batch: 060 ----
mean loss: 103.45
 ---- batch: 070 ----
mean loss: 101.83
 ---- batch: 080 ----
mean loss: 108.89
 ---- batch: 090 ----
mean loss: 99.88
train mean loss: 103.12
epoch train time: 0:00:01.612457
elapsed time: 0:06:04.340136
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 20:23:02.275897
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.94
 ---- batch: 020 ----
mean loss: 102.45
 ---- batch: 030 ----
mean loss: 108.58
 ---- batch: 040 ----
mean loss: 104.40
 ---- batch: 050 ----
mean loss: 104.90
 ---- batch: 060 ----
mean loss: 102.74
 ---- batch: 070 ----
mean loss: 108.23
 ---- batch: 080 ----
mean loss: 99.73
 ---- batch: 090 ----
mean loss: 101.63
train mean loss: 103.03
epoch train time: 0:00:01.661969
elapsed time: 0:06:06.002815
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 20:23:03.938540
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.81
 ---- batch: 020 ----
mean loss: 101.01
 ---- batch: 030 ----
mean loss: 103.68
 ---- batch: 040 ----
mean loss: 107.83
 ---- batch: 050 ----
mean loss: 102.82
 ---- batch: 060 ----
mean loss: 101.11
 ---- batch: 070 ----
mean loss: 96.95
 ---- batch: 080 ----
mean loss: 107.39
 ---- batch: 090 ----
mean loss: 101.79
train mean loss: 103.07
epoch train time: 0:00:01.606178
elapsed time: 0:06:07.609650
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 20:23:05.545449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.05
 ---- batch: 020 ----
mean loss: 103.54
 ---- batch: 030 ----
mean loss: 103.90
 ---- batch: 040 ----
mean loss: 105.56
 ---- batch: 050 ----
mean loss: 102.98
 ---- batch: 060 ----
mean loss: 98.91
 ---- batch: 070 ----
mean loss: 98.34
 ---- batch: 080 ----
mean loss: 106.82
 ---- batch: 090 ----
mean loss: 99.73
train mean loss: 102.84
epoch train time: 0:00:01.621722
elapsed time: 0:06:09.232050
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 20:23:07.167757
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.02
 ---- batch: 020 ----
mean loss: 100.49
 ---- batch: 030 ----
mean loss: 104.58
 ---- batch: 040 ----
mean loss: 105.23
 ---- batch: 050 ----
mean loss: 101.94
 ---- batch: 060 ----
mean loss: 106.05
 ---- batch: 070 ----
mean loss: 98.47
 ---- batch: 080 ----
mean loss: 102.50
 ---- batch: 090 ----
mean loss: 102.68
train mean loss: 102.85
epoch train time: 0:00:01.596772
elapsed time: 0:06:10.829419
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 20:23:08.765147
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.59
 ---- batch: 020 ----
mean loss: 104.05
 ---- batch: 030 ----
mean loss: 103.67
 ---- batch: 040 ----
mean loss: 103.28
 ---- batch: 050 ----
mean loss: 103.48
 ---- batch: 060 ----
mean loss: 103.86
 ---- batch: 070 ----
mean loss: 99.74
 ---- batch: 080 ----
mean loss: 106.83
 ---- batch: 090 ----
mean loss: 103.86
train mean loss: 102.75
epoch train time: 0:00:01.598055
elapsed time: 0:06:12.428085
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 20:23:10.363821
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.06
 ---- batch: 020 ----
mean loss: 99.20
 ---- batch: 030 ----
mean loss: 102.59
 ---- batch: 040 ----
mean loss: 107.63
 ---- batch: 050 ----
mean loss: 105.85
 ---- batch: 060 ----
mean loss: 103.53
 ---- batch: 070 ----
mean loss: 103.11
 ---- batch: 080 ----
mean loss: 99.94
 ---- batch: 090 ----
mean loss: 102.14
train mean loss: 102.66
epoch train time: 0:00:01.618776
elapsed time: 0:06:14.047502
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 20:23:11.983296
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.42
 ---- batch: 020 ----
mean loss: 97.61
 ---- batch: 030 ----
mean loss: 101.54
 ---- batch: 040 ----
mean loss: 108.49
 ---- batch: 050 ----
mean loss: 106.57
 ---- batch: 060 ----
mean loss: 99.40
 ---- batch: 070 ----
mean loss: 104.64
 ---- batch: 080 ----
mean loss: 101.90
 ---- batch: 090 ----
mean loss: 101.31
train mean loss: 102.82
epoch train time: 0:00:01.591418
elapsed time: 0:06:15.639634
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 20:23:13.575397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.49
 ---- batch: 020 ----
mean loss: 101.49
 ---- batch: 030 ----
mean loss: 98.97
 ---- batch: 040 ----
mean loss: 105.68
 ---- batch: 050 ----
mean loss: 103.63
 ---- batch: 060 ----
mean loss: 102.85
 ---- batch: 070 ----
mean loss: 104.08
 ---- batch: 080 ----
mean loss: 102.82
 ---- batch: 090 ----
mean loss: 96.82
train mean loss: 102.72
epoch train time: 0:00:01.615037
elapsed time: 0:06:17.255350
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 20:23:15.191072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.16
 ---- batch: 020 ----
mean loss: 92.77
 ---- batch: 030 ----
mean loss: 101.83
 ---- batch: 040 ----
mean loss: 99.13
 ---- batch: 050 ----
mean loss: 103.87
 ---- batch: 060 ----
mean loss: 105.95
 ---- batch: 070 ----
mean loss: 107.97
 ---- batch: 080 ----
mean loss: 106.56
 ---- batch: 090 ----
mean loss: 104.61
train mean loss: 102.69
epoch train time: 0:00:01.602326
elapsed time: 0:06:18.858360
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 20:23:16.794098
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.34
 ---- batch: 020 ----
mean loss: 103.54
 ---- batch: 030 ----
mean loss: 100.20
 ---- batch: 040 ----
mean loss: 102.34
 ---- batch: 050 ----
mean loss: 100.69
 ---- batch: 060 ----
mean loss: 102.67
 ---- batch: 070 ----
mean loss: 100.86
 ---- batch: 080 ----
mean loss: 104.72
 ---- batch: 090 ----
mean loss: 102.78
train mean loss: 102.46
epoch train time: 0:00:01.638673
elapsed time: 0:06:20.497685
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 20:23:18.433390
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.61
 ---- batch: 020 ----
mean loss: 101.13
 ---- batch: 030 ----
mean loss: 101.21
 ---- batch: 040 ----
mean loss: 100.18
 ---- batch: 050 ----
mean loss: 103.38
 ---- batch: 060 ----
mean loss: 100.12
 ---- batch: 070 ----
mean loss: 102.71
 ---- batch: 080 ----
mean loss: 103.33
 ---- batch: 090 ----
mean loss: 104.05
train mean loss: 102.53
epoch train time: 0:00:01.626153
elapsed time: 0:06:22.124446
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 20:23:20.060187
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.08
 ---- batch: 020 ----
mean loss: 100.19
 ---- batch: 030 ----
mean loss: 103.11
 ---- batch: 040 ----
mean loss: 98.44
 ---- batch: 050 ----
mean loss: 104.66
 ---- batch: 060 ----
mean loss: 103.31
 ---- batch: 070 ----
mean loss: 108.18
 ---- batch: 080 ----
mean loss: 103.47
 ---- batch: 090 ----
mean loss: 102.53
train mean loss: 102.29
epoch train time: 0:00:01.627134
elapsed time: 0:06:23.752237
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 20:23:21.687999
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.03
 ---- batch: 020 ----
mean loss: 99.35
 ---- batch: 030 ----
mean loss: 102.42
 ---- batch: 040 ----
mean loss: 101.25
 ---- batch: 050 ----
mean loss: 101.88
 ---- batch: 060 ----
mean loss: 105.95
 ---- batch: 070 ----
mean loss: 100.93
 ---- batch: 080 ----
mean loss: 105.26
 ---- batch: 090 ----
mean loss: 109.38
train mean loss: 102.35
epoch train time: 0:00:01.605123
elapsed time: 0:06:25.358069
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 20:23:23.293838
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.00
 ---- batch: 020 ----
mean loss: 103.42
 ---- batch: 030 ----
mean loss: 101.39
 ---- batch: 040 ----
mean loss: 108.52
 ---- batch: 050 ----
mean loss: 104.86
 ---- batch: 060 ----
mean loss: 98.06
 ---- batch: 070 ----
mean loss: 99.22
 ---- batch: 080 ----
mean loss: 105.40
 ---- batch: 090 ----
mean loss: 100.80
train mean loss: 102.53
epoch train time: 0:00:01.619074
elapsed time: 0:06:26.977912
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 20:23:24.913674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.10
 ---- batch: 020 ----
mean loss: 99.78
 ---- batch: 030 ----
mean loss: 97.39
 ---- batch: 040 ----
mean loss: 100.29
 ---- batch: 050 ----
mean loss: 106.63
 ---- batch: 060 ----
mean loss: 107.88
 ---- batch: 070 ----
mean loss: 101.86
 ---- batch: 080 ----
mean loss: 105.83
 ---- batch: 090 ----
mean loss: 100.84
train mean loss: 102.20
epoch train time: 0:00:01.637646
elapsed time: 0:06:28.616328
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 20:23:26.552076
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.20
 ---- batch: 020 ----
mean loss: 99.43
 ---- batch: 030 ----
mean loss: 100.73
 ---- batch: 040 ----
mean loss: 105.38
 ---- batch: 050 ----
mean loss: 101.67
 ---- batch: 060 ----
mean loss: 106.10
 ---- batch: 070 ----
mean loss: 98.51
 ---- batch: 080 ----
mean loss: 106.22
 ---- batch: 090 ----
mean loss: 98.61
train mean loss: 102.24
epoch train time: 0:00:01.619333
elapsed time: 0:06:30.236331
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 20:23:28.172037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.02
 ---- batch: 020 ----
mean loss: 103.50
 ---- batch: 030 ----
mean loss: 105.16
 ---- batch: 040 ----
mean loss: 96.14
 ---- batch: 050 ----
mean loss: 95.08
 ---- batch: 060 ----
mean loss: 103.43
 ---- batch: 070 ----
mean loss: 103.87
 ---- batch: 080 ----
mean loss: 107.95
 ---- batch: 090 ----
mean loss: 101.79
train mean loss: 102.36
epoch train time: 0:00:01.646370
elapsed time: 0:06:31.883392
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 20:23:29.818976
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.08
 ---- batch: 020 ----
mean loss: 102.88
 ---- batch: 030 ----
mean loss: 106.77
 ---- batch: 040 ----
mean loss: 104.00
 ---- batch: 050 ----
mean loss: 98.74
 ---- batch: 060 ----
mean loss: 99.13
 ---- batch: 070 ----
mean loss: 102.51
 ---- batch: 080 ----
mean loss: 99.18
 ---- batch: 090 ----
mean loss: 108.06
train mean loss: 102.18
epoch train time: 0:00:01.598603
elapsed time: 0:06:33.482477
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 20:23:31.418214
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.92
 ---- batch: 020 ----
mean loss: 93.64
 ---- batch: 030 ----
mean loss: 106.48
 ---- batch: 040 ----
mean loss: 100.27
 ---- batch: 050 ----
mean loss: 103.80
 ---- batch: 060 ----
mean loss: 98.18
 ---- batch: 070 ----
mean loss: 100.07
 ---- batch: 080 ----
mean loss: 108.85
 ---- batch: 090 ----
mean loss: 105.68
train mean loss: 102.16
epoch train time: 0:00:01.624547
elapsed time: 0:06:35.107650
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 20:23:33.043375
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.46
 ---- batch: 020 ----
mean loss: 106.24
 ---- batch: 030 ----
mean loss: 100.29
 ---- batch: 040 ----
mean loss: 98.93
 ---- batch: 050 ----
mean loss: 106.05
 ---- batch: 060 ----
mean loss: 99.69
 ---- batch: 070 ----
mean loss: 100.98
 ---- batch: 080 ----
mean loss: 102.17
 ---- batch: 090 ----
mean loss: 99.08
train mean loss: 102.13
epoch train time: 0:00:01.596517
elapsed time: 0:06:36.704793
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 20:23:34.640506
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.70
 ---- batch: 020 ----
mean loss: 103.35
 ---- batch: 030 ----
mean loss: 98.48
 ---- batch: 040 ----
mean loss: 100.98
 ---- batch: 050 ----
mean loss: 100.45
 ---- batch: 060 ----
mean loss: 103.14
 ---- batch: 070 ----
mean loss: 101.42
 ---- batch: 080 ----
mean loss: 103.81
 ---- batch: 090 ----
mean loss: 103.42
train mean loss: 102.03
epoch train time: 0:00:01.662601
elapsed time: 0:06:38.368035
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 20:23:36.303794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.15
 ---- batch: 020 ----
mean loss: 105.21
 ---- batch: 030 ----
mean loss: 101.97
 ---- batch: 040 ----
mean loss: 104.40
 ---- batch: 050 ----
mean loss: 107.10
 ---- batch: 060 ----
mean loss: 100.71
 ---- batch: 070 ----
mean loss: 105.30
 ---- batch: 080 ----
mean loss: 102.84
 ---- batch: 090 ----
mean loss: 95.39
train mean loss: 101.84
epoch train time: 0:00:01.638810
elapsed time: 0:06:40.007571
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 20:23:37.943297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.84
 ---- batch: 020 ----
mean loss: 100.53
 ---- batch: 030 ----
mean loss: 105.71
 ---- batch: 040 ----
mean loss: 104.71
 ---- batch: 050 ----
mean loss: 101.68
 ---- batch: 060 ----
mean loss: 100.28
 ---- batch: 070 ----
mean loss: 104.34
 ---- batch: 080 ----
mean loss: 99.50
 ---- batch: 090 ----
mean loss: 103.24
train mean loss: 101.94
epoch train time: 0:00:01.602724
elapsed time: 0:06:41.610947
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 20:23:39.546670
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.30
 ---- batch: 020 ----
mean loss: 101.27
 ---- batch: 030 ----
mean loss: 103.00
 ---- batch: 040 ----
mean loss: 102.39
 ---- batch: 050 ----
mean loss: 106.86
 ---- batch: 060 ----
mean loss: 103.96
 ---- batch: 070 ----
mean loss: 103.59
 ---- batch: 080 ----
mean loss: 97.46
 ---- batch: 090 ----
mean loss: 103.47
train mean loss: 101.90
epoch train time: 0:00:01.627477
elapsed time: 0:06:43.239055
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 20:23:41.174813
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.95
 ---- batch: 020 ----
mean loss: 101.84
 ---- batch: 030 ----
mean loss: 104.22
 ---- batch: 040 ----
mean loss: 98.16
 ---- batch: 050 ----
mean loss: 99.52
 ---- batch: 060 ----
mean loss: 103.74
 ---- batch: 070 ----
mean loss: 103.89
 ---- batch: 080 ----
mean loss: 106.33
 ---- batch: 090 ----
mean loss: 93.43
train mean loss: 102.03
epoch train time: 0:00:01.637147
elapsed time: 0:06:44.876964
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 20:23:42.812678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.07
 ---- batch: 020 ----
mean loss: 100.67
 ---- batch: 030 ----
mean loss: 108.59
 ---- batch: 040 ----
mean loss: 98.40
 ---- batch: 050 ----
mean loss: 101.30
 ---- batch: 060 ----
mean loss: 99.18
 ---- batch: 070 ----
mean loss: 102.88
 ---- batch: 080 ----
mean loss: 103.05
 ---- batch: 090 ----
mean loss: 100.27
train mean loss: 101.86
epoch train time: 0:00:01.649485
elapsed time: 0:06:46.527118
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 20:23:44.462845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.38
 ---- batch: 020 ----
mean loss: 100.80
 ---- batch: 030 ----
mean loss: 100.97
 ---- batch: 040 ----
mean loss: 96.49
 ---- batch: 050 ----
mean loss: 103.23
 ---- batch: 060 ----
mean loss: 99.72
 ---- batch: 070 ----
mean loss: 106.81
 ---- batch: 080 ----
mean loss: 103.67
 ---- batch: 090 ----
mean loss: 100.19
train mean loss: 101.86
epoch train time: 0:00:01.639043
elapsed time: 0:06:48.166784
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 20:23:46.102549
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.93
 ---- batch: 020 ----
mean loss: 99.36
 ---- batch: 030 ----
mean loss: 102.64
 ---- batch: 040 ----
mean loss: 99.83
 ---- batch: 050 ----
mean loss: 103.57
 ---- batch: 060 ----
mean loss: 97.99
 ---- batch: 070 ----
mean loss: 101.22
 ---- batch: 080 ----
mean loss: 105.59
 ---- batch: 090 ----
mean loss: 104.19
train mean loss: 101.72
epoch train time: 0:00:01.595941
elapsed time: 0:06:49.763931
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 20:23:47.699427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 93.98
 ---- batch: 020 ----
mean loss: 103.02
 ---- batch: 030 ----
mean loss: 100.96
 ---- batch: 040 ----
mean loss: 105.07
 ---- batch: 050 ----
mean loss: 102.75
 ---- batch: 060 ----
mean loss: 100.48
 ---- batch: 070 ----
mean loss: 102.38
 ---- batch: 080 ----
mean loss: 102.34
 ---- batch: 090 ----
mean loss: 103.31
train mean loss: 101.61
epoch train time: 0:00:01.617602
elapsed time: 0:06:51.381997
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 20:23:49.317818
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.94
 ---- batch: 020 ----
mean loss: 103.36
 ---- batch: 030 ----
mean loss: 96.64
 ---- batch: 040 ----
mean loss: 101.68
 ---- batch: 050 ----
mean loss: 105.24
 ---- batch: 060 ----
mean loss: 101.60
 ---- batch: 070 ----
mean loss: 102.43
 ---- batch: 080 ----
mean loss: 99.29
 ---- batch: 090 ----
mean loss: 103.68
train mean loss: 101.82
epoch train time: 0:00:01.613269
elapsed time: 0:06:52.996032
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 20:23:50.931811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.36
 ---- batch: 020 ----
mean loss: 99.70
 ---- batch: 030 ----
mean loss: 107.48
 ---- batch: 040 ----
mean loss: 101.56
 ---- batch: 050 ----
mean loss: 101.33
 ---- batch: 060 ----
mean loss: 98.72
 ---- batch: 070 ----
mean loss: 100.35
 ---- batch: 080 ----
mean loss: 104.14
 ---- batch: 090 ----
mean loss: 100.82
train mean loss: 101.46
epoch train time: 0:00:01.618651
elapsed time: 0:06:54.615367
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 20:23:52.551088
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.77
 ---- batch: 020 ----
mean loss: 97.81
 ---- batch: 030 ----
mean loss: 109.10
 ---- batch: 040 ----
mean loss: 99.49
 ---- batch: 050 ----
mean loss: 100.51
 ---- batch: 060 ----
mean loss: 99.45
 ---- batch: 070 ----
mean loss: 103.12
 ---- batch: 080 ----
mean loss: 100.19
 ---- batch: 090 ----
mean loss: 104.21
train mean loss: 101.57
epoch train time: 0:00:01.635690
elapsed time: 0:06:56.251816
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 20:23:54.187551
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.21
 ---- batch: 020 ----
mean loss: 100.93
 ---- batch: 030 ----
mean loss: 96.55
 ---- batch: 040 ----
mean loss: 100.97
 ---- batch: 050 ----
mean loss: 106.95
 ---- batch: 060 ----
mean loss: 102.68
 ---- batch: 070 ----
mean loss: 103.28
 ---- batch: 080 ----
mean loss: 99.77
 ---- batch: 090 ----
mean loss: 98.93
train mean loss: 101.43
epoch train time: 0:00:01.638424
elapsed time: 0:06:57.890962
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 20:23:55.826729
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.08
 ---- batch: 020 ----
mean loss: 100.14
 ---- batch: 030 ----
mean loss: 102.67
 ---- batch: 040 ----
mean loss: 97.68
 ---- batch: 050 ----
mean loss: 105.43
 ---- batch: 060 ----
mean loss: 97.35
 ---- batch: 070 ----
mean loss: 97.15
 ---- batch: 080 ----
mean loss: 105.59
 ---- batch: 090 ----
mean loss: 103.98
train mean loss: 101.58
epoch train time: 0:00:01.625564
elapsed time: 0:06:59.517239
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 20:23:57.452989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.62
 ---- batch: 020 ----
mean loss: 104.20
 ---- batch: 030 ----
mean loss: 99.41
 ---- batch: 040 ----
mean loss: 96.28
 ---- batch: 050 ----
mean loss: 101.11
 ---- batch: 060 ----
mean loss: 101.82
 ---- batch: 070 ----
mean loss: 104.61
 ---- batch: 080 ----
mean loss: 100.64
 ---- batch: 090 ----
mean loss: 101.28
train mean loss: 101.35
epoch train time: 0:00:01.631236
elapsed time: 0:07:01.149241
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 20:23:59.085045
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.38
 ---- batch: 020 ----
mean loss: 94.44
 ---- batch: 030 ----
mean loss: 105.31
 ---- batch: 040 ----
mean loss: 103.12
 ---- batch: 050 ----
mean loss: 100.74
 ---- batch: 060 ----
mean loss: 99.14
 ---- batch: 070 ----
mean loss: 103.92
 ---- batch: 080 ----
mean loss: 102.75
 ---- batch: 090 ----
mean loss: 96.97
train mean loss: 101.40
epoch train time: 0:00:01.631807
elapsed time: 0:07:02.781828
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 20:24:00.717583
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.67
 ---- batch: 020 ----
mean loss: 97.13
 ---- batch: 030 ----
mean loss: 99.41
 ---- batch: 040 ----
mean loss: 100.65
 ---- batch: 050 ----
mean loss: 96.04
 ---- batch: 060 ----
mean loss: 104.69
 ---- batch: 070 ----
mean loss: 102.77
 ---- batch: 080 ----
mean loss: 104.00
 ---- batch: 090 ----
mean loss: 102.35
train mean loss: 101.33
epoch train time: 0:00:01.623816
elapsed time: 0:07:04.406394
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 20:24:02.342160
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.26
 ---- batch: 020 ----
mean loss: 103.84
 ---- batch: 030 ----
mean loss: 94.51
 ---- batch: 040 ----
mean loss: 100.23
 ---- batch: 050 ----
mean loss: 98.90
 ---- batch: 060 ----
mean loss: 104.05
 ---- batch: 070 ----
mean loss: 103.87
 ---- batch: 080 ----
mean loss: 99.26
 ---- batch: 090 ----
mean loss: 105.18
train mean loss: 101.21
epoch train time: 0:00:01.654372
elapsed time: 0:07:06.061441
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 20:24:03.997187
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.21
 ---- batch: 020 ----
mean loss: 103.02
 ---- batch: 030 ----
mean loss: 98.81
 ---- batch: 040 ----
mean loss: 97.77
 ---- batch: 050 ----
mean loss: 99.97
 ---- batch: 060 ----
mean loss: 99.23
 ---- batch: 070 ----
mean loss: 104.80
 ---- batch: 080 ----
mean loss: 100.80
 ---- batch: 090 ----
mean loss: 104.22
train mean loss: 101.29
epoch train time: 0:00:01.606529
elapsed time: 0:07:07.668612
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 20:24:05.604343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.43
 ---- batch: 020 ----
mean loss: 101.13
 ---- batch: 030 ----
mean loss: 99.66
 ---- batch: 040 ----
mean loss: 100.88
 ---- batch: 050 ----
mean loss: 102.99
 ---- batch: 060 ----
mean loss: 103.60
 ---- batch: 070 ----
mean loss: 105.34
 ---- batch: 080 ----
mean loss: 102.47
 ---- batch: 090 ----
mean loss: 101.98
train mean loss: 101.07
epoch train time: 0:00:01.635590
elapsed time: 0:07:09.304844
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 20:24:07.240604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.11
 ---- batch: 020 ----
mean loss: 97.18
 ---- batch: 030 ----
mean loss: 101.08
 ---- batch: 040 ----
mean loss: 104.06
 ---- batch: 050 ----
mean loss: 100.02
 ---- batch: 060 ----
mean loss: 101.26
 ---- batch: 070 ----
mean loss: 99.46
 ---- batch: 080 ----
mean loss: 95.97
 ---- batch: 090 ----
mean loss: 107.96
train mean loss: 101.24
epoch train time: 0:00:01.618683
elapsed time: 0:07:10.924187
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 20:24:08.859990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.19
 ---- batch: 020 ----
mean loss: 102.45
 ---- batch: 030 ----
mean loss: 97.13
 ---- batch: 040 ----
mean loss: 109.61
 ---- batch: 050 ----
mean loss: 103.81
 ---- batch: 060 ----
mean loss: 100.18
 ---- batch: 070 ----
mean loss: 96.49
 ---- batch: 080 ----
mean loss: 102.63
 ---- batch: 090 ----
mean loss: 100.65
train mean loss: 101.26
epoch train time: 0:00:01.590234
elapsed time: 0:07:12.515144
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 20:24:10.450855
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.45
 ---- batch: 020 ----
mean loss: 95.91
 ---- batch: 030 ----
mean loss: 96.29
 ---- batch: 040 ----
mean loss: 99.64
 ---- batch: 050 ----
mean loss: 105.56
 ---- batch: 060 ----
mean loss: 104.83
 ---- batch: 070 ----
mean loss: 103.67
 ---- batch: 080 ----
mean loss: 101.22
 ---- batch: 090 ----
mean loss: 101.12
train mean loss: 101.00
epoch train time: 0:00:01.618914
elapsed time: 0:07:14.134670
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 20:24:12.070421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.91
 ---- batch: 020 ----
mean loss: 105.97
 ---- batch: 030 ----
mean loss: 100.70
 ---- batch: 040 ----
mean loss: 94.75
 ---- batch: 050 ----
mean loss: 97.44
 ---- batch: 060 ----
mean loss: 100.03
 ---- batch: 070 ----
mean loss: 101.08
 ---- batch: 080 ----
mean loss: 107.53
 ---- batch: 090 ----
mean loss: 102.83
train mean loss: 101.00
epoch train time: 0:00:01.587967
elapsed time: 0:07:15.731105
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_7/checkpoint.pth.tar
**** end time: 2019-09-25 20:24:13.666544 ****
