Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_0', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 7843
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-20 23:23:56.367694 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:23:56.372844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4763.09
 ---- batch: 020 ----
mean loss: 4725.27
 ---- batch: 030 ----
mean loss: 4748.69
 ---- batch: 040 ----
mean loss: 4639.42
train mean loss: 4708.56
epoch train time: 0:00:15.242525
elapsed time: 0:00:15.248962
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:24:11.616693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4603.37
 ---- batch: 020 ----
mean loss: 4475.21
 ---- batch: 030 ----
mean loss: 4384.13
 ---- batch: 040 ----
mean loss: 4407.18
train mean loss: 4453.74
epoch train time: 0:00:00.219133
elapsed time: 0:00:15.468246
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:24:11.835998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4198.88
 ---- batch: 020 ----
mean loss: 4149.05
 ---- batch: 030 ----
mean loss: 4126.29
 ---- batch: 040 ----
mean loss: 3976.58
train mean loss: 4093.90
epoch train time: 0:00:00.214665
elapsed time: 0:00:15.683065
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:24:12.050833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3833.51
 ---- batch: 020 ----
mean loss: 3865.99
 ---- batch: 030 ----
mean loss: 3587.69
 ---- batch: 040 ----
mean loss: 3645.19
train mean loss: 3730.64
epoch train time: 0:00:00.210987
elapsed time: 0:00:15.894238
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:24:12.262007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3547.65
 ---- batch: 020 ----
mean loss: 3435.39
 ---- batch: 030 ----
mean loss: 3401.79
 ---- batch: 040 ----
mean loss: 3305.66
train mean loss: 3412.26
epoch train time: 0:00:00.209654
elapsed time: 0:00:16.104054
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:24:12.471792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3212.31
 ---- batch: 020 ----
mean loss: 3217.30
 ---- batch: 030 ----
mean loss: 3109.35
 ---- batch: 040 ----
mean loss: 3035.42
train mean loss: 3137.51
epoch train time: 0:00:00.203701
elapsed time: 0:00:16.307876
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:24:12.675614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2958.87
 ---- batch: 020 ----
mean loss: 2973.47
 ---- batch: 030 ----
mean loss: 2893.27
 ---- batch: 040 ----
mean loss: 2765.34
train mean loss: 2891.59
epoch train time: 0:00:00.213370
elapsed time: 0:00:16.521367
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:24:12.889102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2766.64
 ---- batch: 020 ----
mean loss: 2655.47
 ---- batch: 030 ----
mean loss: 2682.22
 ---- batch: 040 ----
mean loss: 2597.84
train mean loss: 2670.91
epoch train time: 0:00:00.204302
elapsed time: 0:00:16.725782
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:24:13.093534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2602.70
 ---- batch: 020 ----
mean loss: 2477.18
 ---- batch: 030 ----
mean loss: 2439.47
 ---- batch: 040 ----
mean loss: 2388.93
train mean loss: 2471.04
epoch train time: 0:00:00.204566
elapsed time: 0:00:16.930491
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:24:13.298231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2348.19
 ---- batch: 020 ----
mean loss: 2299.66
 ---- batch: 030 ----
mean loss: 2274.16
 ---- batch: 040 ----
mean loss: 2266.98
train mean loss: 2289.72
epoch train time: 0:00:00.203566
elapsed time: 0:00:17.134179
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:24:13.501914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2175.34
 ---- batch: 020 ----
mean loss: 2154.18
 ---- batch: 030 ----
mean loss: 2126.21
 ---- batch: 040 ----
mean loss: 2042.24
train mean loss: 2125.47
epoch train time: 0:00:00.205200
elapsed time: 0:00:17.339512
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:24:13.707260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2064.63
 ---- batch: 020 ----
mean loss: 1973.24
 ---- batch: 030 ----
mean loss: 1943.96
 ---- batch: 040 ----
mean loss: 1930.37
train mean loss: 1974.77
epoch train time: 0:00:00.203763
elapsed time: 0:00:17.543406
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:24:13.911142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1876.90
 ---- batch: 020 ----
mean loss: 1859.86
 ---- batch: 030 ----
mean loss: 1813.87
 ---- batch: 040 ----
mean loss: 1814.33
train mean loss: 1837.15
epoch train time: 0:00:00.208114
elapsed time: 0:00:17.751639
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:24:14.119375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1752.13
 ---- batch: 020 ----
mean loss: 1729.15
 ---- batch: 030 ----
mean loss: 1696.87
 ---- batch: 040 ----
mean loss: 1683.43
train mean loss: 1713.82
epoch train time: 0:00:00.200481
elapsed time: 0:00:17.952242
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:24:14.319977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1651.71
 ---- batch: 020 ----
mean loss: 1607.38
 ---- batch: 030 ----
mean loss: 1580.45
 ---- batch: 040 ----
mean loss: 1578.31
train mean loss: 1598.90
epoch train time: 0:00:00.209430
elapsed time: 0:00:18.161793
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:24:14.529547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1514.55
 ---- batch: 020 ----
mean loss: 1526.07
 ---- batch: 030 ----
mean loss: 1495.71
 ---- batch: 040 ----
mean loss: 1461.60
train mean loss: 1495.95
epoch train time: 0:00:00.214351
elapsed time: 0:00:18.376283
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:24:14.744021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1420.17
 ---- batch: 020 ----
mean loss: 1409.73
 ---- batch: 030 ----
mean loss: 1399.93
 ---- batch: 040 ----
mean loss: 1384.46
train mean loss: 1403.43
epoch train time: 0:00:00.214214
elapsed time: 0:00:18.590621
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:24:14.958358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1340.69
 ---- batch: 020 ----
mean loss: 1330.67
 ---- batch: 030 ----
mean loss: 1311.39
 ---- batch: 040 ----
mean loss: 1295.95
train mean loss: 1318.11
epoch train time: 0:00:00.210191
elapsed time: 0:00:18.800939
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:24:15.168693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1283.02
 ---- batch: 020 ----
mean loss: 1228.71
 ---- batch: 030 ----
mean loss: 1249.33
 ---- batch: 040 ----
mean loss: 1218.09
train mean loss: 1241.33
epoch train time: 0:00:00.210499
elapsed time: 0:00:19.011579
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:24:15.379316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1184.92
 ---- batch: 020 ----
mean loss: 1188.15
 ---- batch: 030 ----
mean loss: 1183.88
 ---- batch: 040 ----
mean loss: 1139.19
train mean loss: 1174.04
epoch train time: 0:00:00.212948
elapsed time: 0:00:19.224669
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:24:15.592435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1130.01
 ---- batch: 020 ----
mean loss: 1122.97
 ---- batch: 030 ----
mean loss: 1106.62
 ---- batch: 040 ----
mean loss: 1093.06
train mean loss: 1111.52
epoch train time: 0:00:00.214958
elapsed time: 0:00:19.439782
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:24:15.807523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1080.54
 ---- batch: 020 ----
mean loss: 1052.11
 ---- batch: 030 ----
mean loss: 1050.27
 ---- batch: 040 ----
mean loss: 1048.45
train mean loss: 1055.61
epoch train time: 0:00:00.209689
elapsed time: 0:00:19.649645
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:24:16.017396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1027.88
 ---- batch: 020 ----
mean loss: 1019.01
 ---- batch: 030 ----
mean loss: 1000.89
 ---- batch: 040 ----
mean loss: 981.43
train mean loss: 1006.33
epoch train time: 0:00:00.213634
elapsed time: 0:00:19.863451
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:24:16.231202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 989.84
 ---- batch: 020 ----
mean loss: 976.72
 ---- batch: 030 ----
mean loss: 955.84
 ---- batch: 040 ----
mean loss: 939.46
train mean loss: 962.23
epoch train time: 0:00:00.206678
elapsed time: 0:00:20.070278
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:24:16.438035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.92
 ---- batch: 020 ----
mean loss: 929.06
 ---- batch: 030 ----
mean loss: 924.10
 ---- batch: 040 ----
mean loss: 896.87
train mean loss: 923.34
epoch train time: 0:00:00.203809
elapsed time: 0:00:20.274295
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:24:16.642040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.75
 ---- batch: 020 ----
mean loss: 907.06
 ---- batch: 030 ----
mean loss: 885.33
 ---- batch: 040 ----
mean loss: 873.36
train mean loss: 888.15
epoch train time: 0:00:00.207238
elapsed time: 0:00:20.481665
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:24:16.849418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.95
 ---- batch: 020 ----
mean loss: 849.78
 ---- batch: 030 ----
mean loss: 854.79
 ---- batch: 040 ----
mean loss: 852.84
train mean loss: 857.51
epoch train time: 0:00:00.204598
elapsed time: 0:00:20.686400
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:24:17.054136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.47
 ---- batch: 020 ----
mean loss: 836.26
 ---- batch: 030 ----
mean loss: 827.39
 ---- batch: 040 ----
mean loss: 826.73
train mean loss: 830.89
epoch train time: 0:00:00.200978
elapsed time: 0:00:20.887496
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:24:17.255242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.60
 ---- batch: 020 ----
mean loss: 798.91
 ---- batch: 030 ----
mean loss: 803.88
 ---- batch: 040 ----
mean loss: 800.64
train mean loss: 807.37
epoch train time: 0:00:00.200762
elapsed time: 0:00:21.088389
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:24:17.456126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 794.24
 ---- batch: 020 ----
mean loss: 775.80
 ---- batch: 030 ----
mean loss: 797.49
 ---- batch: 040 ----
mean loss: 778.19
train mean loss: 787.15
epoch train time: 0:00:00.203786
elapsed time: 0:00:21.292289
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:24:17.660022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 773.69
 ---- batch: 020 ----
mean loss: 765.50
 ---- batch: 030 ----
mean loss: 773.57
 ---- batch: 040 ----
mean loss: 766.00
train mean loss: 768.62
epoch train time: 0:00:00.206440
elapsed time: 0:00:21.498861
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:24:17.866597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 770.70
 ---- batch: 020 ----
mean loss: 745.86
 ---- batch: 030 ----
mean loss: 760.67
 ---- batch: 040 ----
mean loss: 741.38
train mean loss: 753.01
epoch train time: 0:00:00.201470
elapsed time: 0:00:21.700446
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:24:18.068179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 765.54
 ---- batch: 020 ----
mean loss: 725.49
 ---- batch: 030 ----
mean loss: 726.64
 ---- batch: 040 ----
mean loss: 749.47
train mean loss: 739.59
epoch train time: 0:00:00.202789
elapsed time: 0:00:21.903352
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:24:18.271089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 730.36
 ---- batch: 020 ----
mean loss: 729.79
 ---- batch: 030 ----
mean loss: 733.39
 ---- batch: 040 ----
mean loss: 722.87
train mean loss: 727.94
epoch train time: 0:00:00.208763
elapsed time: 0:00:22.112237
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:24:18.479973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 718.85
 ---- batch: 020 ----
mean loss: 720.67
 ---- batch: 030 ----
mean loss: 706.36
 ---- batch: 040 ----
mean loss: 720.51
train mean loss: 718.49
epoch train time: 0:00:00.211291
elapsed time: 0:00:22.323659
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:24:18.691396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.53
 ---- batch: 020 ----
mean loss: 700.93
 ---- batch: 030 ----
mean loss: 703.74
 ---- batch: 040 ----
mean loss: 725.40
train mean loss: 709.25
epoch train time: 0:00:00.211989
elapsed time: 0:00:22.535774
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:24:18.903505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 703.16
 ---- batch: 020 ----
mean loss: 709.81
 ---- batch: 030 ----
mean loss: 703.83
 ---- batch: 040 ----
mean loss: 704.74
train mean loss: 702.50
epoch train time: 0:00:00.207454
elapsed time: 0:00:22.743353
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:24:19.111087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.52
 ---- batch: 020 ----
mean loss: 688.76
 ---- batch: 030 ----
mean loss: 687.80
 ---- batch: 040 ----
mean loss: 713.95
train mean loss: 696.49
epoch train time: 0:00:00.207216
elapsed time: 0:00:22.950684
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:24:19.318436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.57
 ---- batch: 020 ----
mean loss: 679.07
 ---- batch: 030 ----
mean loss: 710.03
 ---- batch: 040 ----
mean loss: 687.96
train mean loss: 691.00
epoch train time: 0:00:00.208455
elapsed time: 0:00:23.159277
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:24:19.527014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.24
 ---- batch: 020 ----
mean loss: 688.35
 ---- batch: 030 ----
mean loss: 685.92
 ---- batch: 040 ----
mean loss: 678.68
train mean loss: 686.26
epoch train time: 0:00:00.208584
elapsed time: 0:00:23.367982
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:24:19.735718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.02
 ---- batch: 020 ----
mean loss: 699.70
 ---- batch: 030 ----
mean loss: 685.44
 ---- batch: 040 ----
mean loss: 676.06
train mean loss: 682.97
epoch train time: 0:00:00.208688
elapsed time: 0:00:23.576795
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:24:19.944555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 696.77
 ---- batch: 020 ----
mean loss: 681.08
 ---- batch: 030 ----
mean loss: 674.12
 ---- batch: 040 ----
mean loss: 667.00
train mean loss: 680.03
epoch train time: 0:00:00.205515
elapsed time: 0:00:23.782449
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:24:20.150183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 677.73
 ---- batch: 020 ----
mean loss: 678.10
 ---- batch: 030 ----
mean loss: 680.78
 ---- batch: 040 ----
mean loss: 679.34
train mean loss: 677.47
epoch train time: 0:00:00.200102
elapsed time: 0:00:23.982695
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:24:20.350446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 692.54
 ---- batch: 020 ----
mean loss: 665.03
 ---- batch: 030 ----
mean loss: 675.75
 ---- batch: 040 ----
mean loss: 669.10
train mean loss: 675.94
epoch train time: 0:00:00.200432
elapsed time: 0:00:24.183271
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:24:20.551015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 676.18
 ---- batch: 020 ----
mean loss: 648.69
 ---- batch: 030 ----
mean loss: 678.06
 ---- batch: 040 ----
mean loss: 688.39
train mean loss: 673.58
epoch train time: 0:00:00.214155
elapsed time: 0:00:24.397559
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:24:20.765297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 685.42
 ---- batch: 020 ----
mean loss: 655.21
 ---- batch: 030 ----
mean loss: 672.26
 ---- batch: 040 ----
mean loss: 677.87
train mean loss: 672.42
epoch train time: 0:00:00.207957
elapsed time: 0:00:24.605632
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:24:20.973366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.44
 ---- batch: 020 ----
mean loss: 680.22
 ---- batch: 030 ----
mean loss: 661.40
 ---- batch: 040 ----
mean loss: 686.53
train mean loss: 671.03
epoch train time: 0:00:00.203701
elapsed time: 0:00:24.809448
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:24:21.177182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.73
 ---- batch: 020 ----
mean loss: 681.21
 ---- batch: 030 ----
mean loss: 666.97
 ---- batch: 040 ----
mean loss: 675.08
train mean loss: 670.16
epoch train time: 0:00:00.201823
elapsed time: 0:00:25.011401
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:24:21.379151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.71
 ---- batch: 020 ----
mean loss: 680.17
 ---- batch: 030 ----
mean loss: 657.88
 ---- batch: 040 ----
mean loss: 675.37
train mean loss: 669.23
epoch train time: 0:00:00.200961
elapsed time: 0:00:25.212516
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:24:21.580276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 657.92
 ---- batch: 020 ----
mean loss: 661.92
 ---- batch: 030 ----
mean loss: 682.66
 ---- batch: 040 ----
mean loss: 678.66
train mean loss: 669.12
epoch train time: 0:00:00.206248
elapsed time: 0:00:25.418929
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:24:21.786681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.75
 ---- batch: 020 ----
mean loss: 680.07
 ---- batch: 030 ----
mean loss: 659.99
 ---- batch: 040 ----
mean loss: 666.78
train mean loss: 668.71
epoch train time: 0:00:00.206927
elapsed time: 0:00:25.626009
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:24:21.993746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 651.70
 ---- batch: 020 ----
mean loss: 658.29
 ---- batch: 030 ----
mean loss: 674.93
 ---- batch: 040 ----
mean loss: 675.55
train mean loss: 667.03
epoch train time: 0:00:00.211843
elapsed time: 0:00:25.837976
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:24:22.205715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.11
 ---- batch: 020 ----
mean loss: 684.95
 ---- batch: 030 ----
mean loss: 643.62
 ---- batch: 040 ----
mean loss: 676.57
train mean loss: 668.11
epoch train time: 0:00:00.213135
elapsed time: 0:00:26.051239
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:24:22.418977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 670.38
 ---- batch: 020 ----
mean loss: 658.07
 ---- batch: 030 ----
mean loss: 662.41
 ---- batch: 040 ----
mean loss: 675.30
train mean loss: 666.70
epoch train time: 0:00:00.214827
elapsed time: 0:00:26.266194
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:24:22.633932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.75
 ---- batch: 020 ----
mean loss: 658.66
 ---- batch: 030 ----
mean loss: 674.85
 ---- batch: 040 ----
mean loss: 663.56
train mean loss: 667.05
epoch train time: 0:00:00.216101
elapsed time: 0:00:26.482432
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:24:22.850183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 678.76
 ---- batch: 020 ----
mean loss: 649.37
 ---- batch: 030 ----
mean loss: 653.22
 ---- batch: 040 ----
mean loss: 680.53
train mean loss: 664.80
epoch train time: 0:00:00.209030
elapsed time: 0:00:26.691600
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:24:23.059337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 680.96
 ---- batch: 020 ----
mean loss: 662.67
 ---- batch: 030 ----
mean loss: 663.80
 ---- batch: 040 ----
mean loss: 637.90
train mean loss: 657.91
epoch train time: 0:00:00.209417
elapsed time: 0:00:26.901137
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:24:23.268873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 637.45
 ---- batch: 020 ----
mean loss: 562.13
 ---- batch: 030 ----
mean loss: 487.62
 ---- batch: 040 ----
mean loss: 441.59
train mean loss: 525.38
epoch train time: 0:00:00.213768
elapsed time: 0:00:27.115023
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:24:23.482771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.72
 ---- batch: 020 ----
mean loss: 428.40
 ---- batch: 030 ----
mean loss: 421.69
 ---- batch: 040 ----
mean loss: 416.98
train mean loss: 421.21
epoch train time: 0:00:00.212635
elapsed time: 0:00:27.327791
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:24:23.695527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.68
 ---- batch: 020 ----
mean loss: 414.33
 ---- batch: 030 ----
mean loss: 406.94
 ---- batch: 040 ----
mean loss: 404.23
train mean loss: 406.52
epoch train time: 0:00:00.213137
elapsed time: 0:00:27.541074
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:24:23.908809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.11
 ---- batch: 020 ----
mean loss: 397.12
 ---- batch: 030 ----
mean loss: 400.35
 ---- batch: 040 ----
mean loss: 388.27
train mean loss: 396.34
epoch train time: 0:00:00.206430
elapsed time: 0:00:27.747626
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:24:24.115362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.32
 ---- batch: 020 ----
mean loss: 394.10
 ---- batch: 030 ----
mean loss: 383.35
 ---- batch: 040 ----
mean loss: 389.76
train mean loss: 387.81
epoch train time: 0:00:00.202105
elapsed time: 0:00:27.949878
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:24:24.317646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.19
 ---- batch: 020 ----
mean loss: 382.08
 ---- batch: 030 ----
mean loss: 377.58
 ---- batch: 040 ----
mean loss: 379.32
train mean loss: 380.25
epoch train time: 0:00:00.214092
elapsed time: 0:00:28.164133
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:24:24.531867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.21
 ---- batch: 020 ----
mean loss: 376.93
 ---- batch: 030 ----
mean loss: 377.10
 ---- batch: 040 ----
mean loss: 372.31
train mean loss: 373.91
epoch train time: 0:00:00.203967
elapsed time: 0:00:28.368242
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:24:24.735982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.92
 ---- batch: 020 ----
mean loss: 370.66
 ---- batch: 030 ----
mean loss: 371.99
 ---- batch: 040 ----
mean loss: 355.16
train mean loss: 368.18
epoch train time: 0:00:00.209196
elapsed time: 0:00:28.577586
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:24:24.945324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.18
 ---- batch: 020 ----
mean loss: 364.93
 ---- batch: 030 ----
mean loss: 364.06
 ---- batch: 040 ----
mean loss: 359.24
train mean loss: 363.65
epoch train time: 0:00:00.205568
elapsed time: 0:00:28.783274
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:24:25.151009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.78
 ---- batch: 020 ----
mean loss: 343.51
 ---- batch: 030 ----
mean loss: 362.82
 ---- batch: 040 ----
mean loss: 361.12
train mean loss: 359.11
epoch train time: 0:00:00.200618
elapsed time: 0:00:28.984008
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:24:25.351783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.69
 ---- batch: 020 ----
mean loss: 356.07
 ---- batch: 030 ----
mean loss: 351.34
 ---- batch: 040 ----
mean loss: 355.35
train mean loss: 355.10
epoch train time: 0:00:00.201022
elapsed time: 0:00:29.185185
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:24:25.552919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.45
 ---- batch: 020 ----
mean loss: 340.09
 ---- batch: 030 ----
mean loss: 355.81
 ---- batch: 040 ----
mean loss: 355.42
train mean loss: 351.74
epoch train time: 0:00:00.205148
elapsed time: 0:00:29.390450
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:24:25.758186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.85
 ---- batch: 020 ----
mean loss: 347.87
 ---- batch: 030 ----
mean loss: 339.73
 ---- batch: 040 ----
mean loss: 344.62
train mean loss: 348.83
epoch train time: 0:00:00.210526
elapsed time: 0:00:29.601097
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:24:25.968833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.47
 ---- batch: 020 ----
mean loss: 348.97
 ---- batch: 030 ----
mean loss: 350.74
 ---- batch: 040 ----
mean loss: 338.33
train mean loss: 345.88
epoch train time: 0:00:00.208774
elapsed time: 0:00:29.809991
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:24:26.177760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.51
 ---- batch: 020 ----
mean loss: 347.09
 ---- batch: 030 ----
mean loss: 345.62
 ---- batch: 040 ----
mean loss: 340.74
train mean loss: 343.71
epoch train time: 0:00:00.207801
elapsed time: 0:00:30.017957
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:24:26.385709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.45
 ---- batch: 020 ----
mean loss: 344.07
 ---- batch: 030 ----
mean loss: 338.64
 ---- batch: 040 ----
mean loss: 344.19
train mean loss: 341.26
epoch train time: 0:00:00.207861
elapsed time: 0:00:30.225957
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:24:26.593693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.26
 ---- batch: 020 ----
mean loss: 331.19
 ---- batch: 030 ----
mean loss: 339.59
 ---- batch: 040 ----
mean loss: 339.48
train mean loss: 339.54
epoch train time: 0:00:00.213725
elapsed time: 0:00:30.439806
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:24:26.807590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.49
 ---- batch: 020 ----
mean loss: 337.11
 ---- batch: 030 ----
mean loss: 326.88
 ---- batch: 040 ----
mean loss: 344.16
train mean loss: 338.52
epoch train time: 0:00:00.219771
elapsed time: 0:00:30.659749
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:24:27.027486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.15
 ---- batch: 020 ----
mean loss: 337.48
 ---- batch: 030 ----
mean loss: 332.52
 ---- batch: 040 ----
mean loss: 345.36
train mean loss: 336.20
epoch train time: 0:00:00.209913
elapsed time: 0:00:30.869783
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:24:27.237518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.13
 ---- batch: 020 ----
mean loss: 334.83
 ---- batch: 030 ----
mean loss: 334.86
 ---- batch: 040 ----
mean loss: 336.81
train mean loss: 335.24
epoch train time: 0:00:00.208845
elapsed time: 0:00:31.078743
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:24:27.446494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.61
 ---- batch: 020 ----
mean loss: 339.24
 ---- batch: 030 ----
mean loss: 327.94
 ---- batch: 040 ----
mean loss: 334.87
train mean loss: 333.99
epoch train time: 0:00:00.207244
elapsed time: 0:00:31.286129
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:24:27.653881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.05
 ---- batch: 020 ----
mean loss: 333.70
 ---- batch: 030 ----
mean loss: 328.80
 ---- batch: 040 ----
mean loss: 334.78
train mean loss: 332.82
epoch train time: 0:00:00.210831
elapsed time: 0:00:31.497107
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:24:27.864841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.88
 ---- batch: 020 ----
mean loss: 329.72
 ---- batch: 030 ----
mean loss: 340.09
 ---- batch: 040 ----
mean loss: 323.29
train mean loss: 331.93
epoch train time: 0:00:00.206478
elapsed time: 0:00:31.703705
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:24:28.071443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.98
 ---- batch: 020 ----
mean loss: 332.95
 ---- batch: 030 ----
mean loss: 339.01
 ---- batch: 040 ----
mean loss: 319.77
train mean loss: 331.01
epoch train time: 0:00:00.204500
elapsed time: 0:00:31.908330
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:24:28.276068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.37
 ---- batch: 020 ----
mean loss: 327.01
 ---- batch: 030 ----
mean loss: 326.72
 ---- batch: 040 ----
mean loss: 331.86
train mean loss: 329.82
epoch train time: 0:00:00.201760
elapsed time: 0:00:32.110243
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:24:28.477978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.89
 ---- batch: 020 ----
mean loss: 338.00
 ---- batch: 030 ----
mean loss: 325.39
 ---- batch: 040 ----
mean loss: 333.54
train mean loss: 329.64
epoch train time: 0:00:00.202568
elapsed time: 0:00:32.312930
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:24:28.680665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.18
 ---- batch: 020 ----
mean loss: 332.21
 ---- batch: 030 ----
mean loss: 319.82
 ---- batch: 040 ----
mean loss: 335.79
train mean loss: 328.51
epoch train time: 0:00:00.206867
elapsed time: 0:00:32.519921
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:24:28.887660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.23
 ---- batch: 020 ----
mean loss: 329.52
 ---- batch: 030 ----
mean loss: 324.96
 ---- batch: 040 ----
mean loss: 330.42
train mean loss: 328.05
epoch train time: 0:00:00.201413
elapsed time: 0:00:32.721450
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:24:29.089183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.36
 ---- batch: 020 ----
mean loss: 327.37
 ---- batch: 030 ----
mean loss: 331.22
 ---- batch: 040 ----
mean loss: 333.59
train mean loss: 327.62
epoch train time: 0:00:00.198824
elapsed time: 0:00:32.920400
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:24:29.288161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.88
 ---- batch: 020 ----
mean loss: 330.73
 ---- batch: 030 ----
mean loss: 318.76
 ---- batch: 040 ----
mean loss: 329.15
train mean loss: 327.33
epoch train time: 0:00:00.195071
elapsed time: 0:00:33.115612
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:24:29.483347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.51
 ---- batch: 020 ----
mean loss: 318.27
 ---- batch: 030 ----
mean loss: 335.72
 ---- batch: 040 ----
mean loss: 326.83
train mean loss: 326.69
epoch train time: 0:00:00.210052
elapsed time: 0:00:33.325791
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:24:29.693530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.69
 ---- batch: 020 ----
mean loss: 329.11
 ---- batch: 030 ----
mean loss: 329.58
 ---- batch: 040 ----
mean loss: 322.25
train mean loss: 325.75
epoch train time: 0:00:00.215414
elapsed time: 0:00:33.541334
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:24:29.909070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.85
 ---- batch: 020 ----
mean loss: 320.94
 ---- batch: 030 ----
mean loss: 326.48
 ---- batch: 040 ----
mean loss: 326.56
train mean loss: 325.45
epoch train time: 0:00:00.212063
elapsed time: 0:00:33.753514
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:24:30.121260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.70
 ---- batch: 020 ----
mean loss: 323.46
 ---- batch: 030 ----
mean loss: 324.72
 ---- batch: 040 ----
mean loss: 327.85
train mean loss: 324.82
epoch train time: 0:00:00.206778
elapsed time: 0:00:33.960447
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:24:30.328184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.21
 ---- batch: 020 ----
mean loss: 329.28
 ---- batch: 030 ----
mean loss: 324.67
 ---- batch: 040 ----
mean loss: 325.52
train mean loss: 324.48
epoch train time: 0:00:00.206436
elapsed time: 0:00:34.167004
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:24:30.534740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.74
 ---- batch: 020 ----
mean loss: 330.02
 ---- batch: 030 ----
mean loss: 323.22
 ---- batch: 040 ----
mean loss: 322.76
train mean loss: 324.50
epoch train time: 0:00:00.209247
elapsed time: 0:00:34.376856
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:24:30.744605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.43
 ---- batch: 020 ----
mean loss: 323.26
 ---- batch: 030 ----
mean loss: 313.11
 ---- batch: 040 ----
mean loss: 330.79
train mean loss: 323.71
epoch train time: 0:00:00.212191
elapsed time: 0:00:34.589186
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:24:30.956925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.99
 ---- batch: 020 ----
mean loss: 329.16
 ---- batch: 030 ----
mean loss: 328.37
 ---- batch: 040 ----
mean loss: 318.39
train mean loss: 323.32
epoch train time: 0:00:00.211021
elapsed time: 0:00:34.800333
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:24:31.168070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.19
 ---- batch: 020 ----
mean loss: 325.50
 ---- batch: 030 ----
mean loss: 312.44
 ---- batch: 040 ----
mean loss: 323.97
train mean loss: 323.09
epoch train time: 0:00:00.208934
elapsed time: 0:00:35.009435
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:24:31.377179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.83
 ---- batch: 020 ----
mean loss: 322.00
 ---- batch: 030 ----
mean loss: 318.54
 ---- batch: 040 ----
mean loss: 330.52
train mean loss: 322.46
epoch train time: 0:00:00.206988
elapsed time: 0:00:35.216550
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:24:31.584285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.07
 ---- batch: 020 ----
mean loss: 322.64
 ---- batch: 030 ----
mean loss: 324.51
 ---- batch: 040 ----
mean loss: 322.51
train mean loss: 322.00
epoch train time: 0:00:00.206153
elapsed time: 0:00:35.422822
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:24:31.790573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.92
 ---- batch: 020 ----
mean loss: 322.45
 ---- batch: 030 ----
mean loss: 327.81
 ---- batch: 040 ----
mean loss: 317.38
train mean loss: 321.76
epoch train time: 0:00:00.204138
elapsed time: 0:00:35.627095
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:24:31.994829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.63
 ---- batch: 020 ----
mean loss: 323.52
 ---- batch: 030 ----
mean loss: 316.63
 ---- batch: 040 ----
mean loss: 328.48
train mean loss: 321.39
epoch train time: 0:00:00.200577
elapsed time: 0:00:35.827783
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:24:32.195515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.41
 ---- batch: 020 ----
mean loss: 323.72
 ---- batch: 030 ----
mean loss: 327.54
 ---- batch: 040 ----
mean loss: 317.87
train mean loss: 321.52
epoch train time: 0:00:00.199082
elapsed time: 0:00:36.027007
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:24:32.394752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.00
 ---- batch: 020 ----
mean loss: 322.81
 ---- batch: 030 ----
mean loss: 323.10
 ---- batch: 040 ----
mean loss: 314.66
train mean loss: 321.21
epoch train time: 0:00:00.210043
elapsed time: 0:00:36.237183
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:24:32.604919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.15
 ---- batch: 020 ----
mean loss: 329.19
 ---- batch: 030 ----
mean loss: 326.33
 ---- batch: 040 ----
mean loss: 309.10
train mean loss: 320.46
epoch train time: 0:00:00.214836
elapsed time: 0:00:36.452208
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:24:32.819941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.63
 ---- batch: 020 ----
mean loss: 326.92
 ---- batch: 030 ----
mean loss: 318.09
 ---- batch: 040 ----
mean loss: 317.43
train mean loss: 320.61
epoch train time: 0:00:00.207407
elapsed time: 0:00:36.659728
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:24:33.027462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.40
 ---- batch: 020 ----
mean loss: 321.77
 ---- batch: 030 ----
mean loss: 310.12
 ---- batch: 040 ----
mean loss: 318.85
train mean loss: 320.37
epoch train time: 0:00:00.206737
elapsed time: 0:00:36.866583
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:24:33.234316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.43
 ---- batch: 020 ----
mean loss: 325.26
 ---- batch: 030 ----
mean loss: 319.15
 ---- batch: 040 ----
mean loss: 316.78
train mean loss: 319.99
epoch train time: 0:00:00.208958
elapsed time: 0:00:37.075670
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:24:33.443420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.35
 ---- batch: 020 ----
mean loss: 324.03
 ---- batch: 030 ----
mean loss: 312.79
 ---- batch: 040 ----
mean loss: 317.52
train mean loss: 319.44
epoch train time: 0:00:00.213443
elapsed time: 0:00:37.289262
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:24:33.656991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.88
 ---- batch: 020 ----
mean loss: 332.63
 ---- batch: 030 ----
mean loss: 316.61
 ---- batch: 040 ----
mean loss: 311.10
train mean loss: 319.05
epoch train time: 0:00:00.215248
elapsed time: 0:00:37.504633
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:24:33.872372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.75
 ---- batch: 020 ----
mean loss: 325.78
 ---- batch: 030 ----
mean loss: 311.98
 ---- batch: 040 ----
mean loss: 313.07
train mean loss: 319.16
epoch train time: 0:00:00.217088
elapsed time: 0:00:37.721865
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:24:34.089603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.43
 ---- batch: 020 ----
mean loss: 325.53
 ---- batch: 030 ----
mean loss: 323.20
 ---- batch: 040 ----
mean loss: 313.23
train mean loss: 319.20
epoch train time: 0:00:00.222172
elapsed time: 0:00:37.944159
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:24:34.311910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.39
 ---- batch: 020 ----
mean loss: 304.70
 ---- batch: 030 ----
mean loss: 326.80
 ---- batch: 040 ----
mean loss: 326.96
train mean loss: 318.14
epoch train time: 0:00:00.214259
elapsed time: 0:00:38.158561
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:24:34.526299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.62
 ---- batch: 020 ----
mean loss: 311.25
 ---- batch: 030 ----
mean loss: 319.52
 ---- batch: 040 ----
mean loss: 319.74
train mean loss: 318.14
epoch train time: 0:00:00.215563
elapsed time: 0:00:38.374260
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:24:34.742045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.67
 ---- batch: 020 ----
mean loss: 313.37
 ---- batch: 030 ----
mean loss: 318.40
 ---- batch: 040 ----
mean loss: 322.56
train mean loss: 318.11
epoch train time: 0:00:00.216717
elapsed time: 0:00:38.591146
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:24:34.958904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.28
 ---- batch: 020 ----
mean loss: 326.13
 ---- batch: 030 ----
mean loss: 319.08
 ---- batch: 040 ----
mean loss: 313.72
train mean loss: 317.60
epoch train time: 0:00:00.214293
elapsed time: 0:00:38.805631
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:24:35.173366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.25
 ---- batch: 020 ----
mean loss: 317.97
 ---- batch: 030 ----
mean loss: 313.72
 ---- batch: 040 ----
mean loss: 313.38
train mean loss: 317.90
epoch train time: 0:00:00.210138
elapsed time: 0:00:39.015913
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:24:35.383678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.93
 ---- batch: 020 ----
mean loss: 314.73
 ---- batch: 030 ----
mean loss: 320.24
 ---- batch: 040 ----
mean loss: 316.17
train mean loss: 317.35
epoch train time: 0:00:00.205553
elapsed time: 0:00:39.221621
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:24:35.589355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.11
 ---- batch: 020 ----
mean loss: 317.67
 ---- batch: 030 ----
mean loss: 312.14
 ---- batch: 040 ----
mean loss: 313.46
train mean loss: 317.27
epoch train time: 0:00:00.212306
elapsed time: 0:00:39.434050
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:24:35.801787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.47
 ---- batch: 020 ----
mean loss: 325.69
 ---- batch: 030 ----
mean loss: 316.90
 ---- batch: 040 ----
mean loss: 313.29
train mean loss: 316.76
epoch train time: 0:00:00.208924
elapsed time: 0:00:39.643099
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:24:36.010834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.81
 ---- batch: 020 ----
mean loss: 309.88
 ---- batch: 030 ----
mean loss: 315.89
 ---- batch: 040 ----
mean loss: 316.45
train mean loss: 317.00
epoch train time: 0:00:00.210187
elapsed time: 0:00:39.853399
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:24:36.221134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.63
 ---- batch: 020 ----
mean loss: 316.27
 ---- batch: 030 ----
mean loss: 313.71
 ---- batch: 040 ----
mean loss: 320.45
train mean loss: 316.32
epoch train time: 0:00:00.210008
elapsed time: 0:00:40.063525
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:24:36.431259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.61
 ---- batch: 020 ----
mean loss: 316.97
 ---- batch: 030 ----
mean loss: 315.61
 ---- batch: 040 ----
mean loss: 314.18
train mean loss: 316.28
epoch train time: 0:00:00.209223
elapsed time: 0:00:40.272864
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:24:36.640598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.17
 ---- batch: 020 ----
mean loss: 324.20
 ---- batch: 030 ----
mean loss: 315.46
 ---- batch: 040 ----
mean loss: 312.33
train mean loss: 315.91
epoch train time: 0:00:00.219908
elapsed time: 0:00:40.492892
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:24:36.860660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.91
 ---- batch: 020 ----
mean loss: 312.34
 ---- batch: 030 ----
mean loss: 317.65
 ---- batch: 040 ----
mean loss: 324.88
train mean loss: 315.71
epoch train time: 0:00:00.208817
elapsed time: 0:00:40.701870
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:24:37.069607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.25
 ---- batch: 020 ----
mean loss: 311.18
 ---- batch: 030 ----
mean loss: 313.95
 ---- batch: 040 ----
mean loss: 316.61
train mean loss: 315.64
epoch train time: 0:00:00.214859
elapsed time: 0:00:40.916865
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:24:37.284632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.37
 ---- batch: 020 ----
mean loss: 312.92
 ---- batch: 030 ----
mean loss: 315.10
 ---- batch: 040 ----
mean loss: 313.21
train mean loss: 315.47
epoch train time: 0:00:00.214773
elapsed time: 0:00:41.131790
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:24:37.499526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.84
 ---- batch: 020 ----
mean loss: 320.52
 ---- batch: 030 ----
mean loss: 312.89
 ---- batch: 040 ----
mean loss: 330.40
train mean loss: 315.29
epoch train time: 0:00:00.215350
elapsed time: 0:00:41.347261
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:24:37.714999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.50
 ---- batch: 020 ----
mean loss: 309.67
 ---- batch: 030 ----
mean loss: 320.84
 ---- batch: 040 ----
mean loss: 308.94
train mean loss: 315.05
epoch train time: 0:00:00.216044
elapsed time: 0:00:41.563437
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:24:37.931166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.20
 ---- batch: 020 ----
mean loss: 308.01
 ---- batch: 030 ----
mean loss: 319.01
 ---- batch: 040 ----
mean loss: 316.57
train mean loss: 315.43
epoch train time: 0:00:00.215452
elapsed time: 0:00:41.779000
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:24:38.146734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.17
 ---- batch: 020 ----
mean loss: 310.93
 ---- batch: 030 ----
mean loss: 307.38
 ---- batch: 040 ----
mean loss: 316.85
train mean loss: 314.64
epoch train time: 0:00:00.215275
elapsed time: 0:00:41.994400
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:24:38.362138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.68
 ---- batch: 020 ----
mean loss: 315.79
 ---- batch: 030 ----
mean loss: 322.45
 ---- batch: 040 ----
mean loss: 316.96
train mean loss: 314.44
epoch train time: 0:00:00.215398
elapsed time: 0:00:42.209922
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:24:38.577657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.87
 ---- batch: 020 ----
mean loss: 308.72
 ---- batch: 030 ----
mean loss: 317.19
 ---- batch: 040 ----
mean loss: 310.05
train mean loss: 314.43
epoch train time: 0:00:00.214335
elapsed time: 0:00:42.424376
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:24:38.792127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.79
 ---- batch: 020 ----
mean loss: 314.25
 ---- batch: 030 ----
mean loss: 309.21
 ---- batch: 040 ----
mean loss: 316.86
train mean loss: 314.03
epoch train time: 0:00:00.210235
elapsed time: 0:00:42.634742
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:24:39.002475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.06
 ---- batch: 020 ----
mean loss: 313.36
 ---- batch: 030 ----
mean loss: 320.48
 ---- batch: 040 ----
mean loss: 318.66
train mean loss: 314.14
epoch train time: 0:00:00.212030
elapsed time: 0:00:42.846887
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:24:39.214623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.44
 ---- batch: 020 ----
mean loss: 312.68
 ---- batch: 030 ----
mean loss: 320.04
 ---- batch: 040 ----
mean loss: 306.10
train mean loss: 313.81
epoch train time: 0:00:00.209380
elapsed time: 0:00:43.056382
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:24:39.424130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.96
 ---- batch: 020 ----
mean loss: 317.34
 ---- batch: 030 ----
mean loss: 309.71
 ---- batch: 040 ----
mean loss: 315.06
train mean loss: 313.71
epoch train time: 0:00:00.204891
elapsed time: 0:00:43.261404
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:24:39.629144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.55
 ---- batch: 020 ----
mean loss: 314.38
 ---- batch: 030 ----
mean loss: 310.52
 ---- batch: 040 ----
mean loss: 317.83
train mean loss: 313.62
epoch train time: 0:00:00.218021
elapsed time: 0:00:43.479562
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:24:39.847304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.97
 ---- batch: 020 ----
mean loss: 310.09
 ---- batch: 030 ----
mean loss: 313.22
 ---- batch: 040 ----
mean loss: 315.06
train mean loss: 313.25
epoch train time: 0:00:00.213187
elapsed time: 0:00:43.692874
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:24:40.060609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.90
 ---- batch: 020 ----
mean loss: 319.28
 ---- batch: 030 ----
mean loss: 314.24
 ---- batch: 040 ----
mean loss: 308.77
train mean loss: 313.32
epoch train time: 0:00:00.208185
elapsed time: 0:00:43.901176
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:24:40.268909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.84
 ---- batch: 020 ----
mean loss: 309.84
 ---- batch: 030 ----
mean loss: 317.28
 ---- batch: 040 ----
mean loss: 307.64
train mean loss: 312.73
epoch train time: 0:00:00.208533
elapsed time: 0:00:44.109825
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:24:40.477560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.90
 ---- batch: 020 ----
mean loss: 308.42
 ---- batch: 030 ----
mean loss: 314.85
 ---- batch: 040 ----
mean loss: 310.57
train mean loss: 312.67
epoch train time: 0:00:00.213203
elapsed time: 0:00:44.323153
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:24:40.690896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.67
 ---- batch: 020 ----
mean loss: 313.28
 ---- batch: 030 ----
mean loss: 310.47
 ---- batch: 040 ----
mean loss: 319.29
train mean loss: 312.36
epoch train time: 0:00:00.220014
elapsed time: 0:00:44.543322
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:24:40.911060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.16
 ---- batch: 020 ----
mean loss: 311.26
 ---- batch: 030 ----
mean loss: 313.98
 ---- batch: 040 ----
mean loss: 315.99
train mean loss: 312.65
epoch train time: 0:00:00.217218
elapsed time: 0:00:44.760671
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:24:41.128430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.93
 ---- batch: 020 ----
mean loss: 315.35
 ---- batch: 030 ----
mean loss: 306.87
 ---- batch: 040 ----
mean loss: 306.93
train mean loss: 312.34
epoch train time: 0:00:00.214138
elapsed time: 0:00:44.974967
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:24:41.342706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.09
 ---- batch: 020 ----
mean loss: 308.10
 ---- batch: 030 ----
mean loss: 312.73
 ---- batch: 040 ----
mean loss: 316.94
train mean loss: 312.29
epoch train time: 0:00:00.212276
elapsed time: 0:00:45.187403
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:24:41.555150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.71
 ---- batch: 020 ----
mean loss: 312.51
 ---- batch: 030 ----
mean loss: 311.72
 ---- batch: 040 ----
mean loss: 310.21
train mean loss: 311.80
epoch train time: 0:00:00.214857
elapsed time: 0:00:45.402392
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:24:41.770144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.70
 ---- batch: 020 ----
mean loss: 319.84
 ---- batch: 030 ----
mean loss: 310.56
 ---- batch: 040 ----
mean loss: 306.64
train mean loss: 311.78
epoch train time: 0:00:00.217850
elapsed time: 0:00:45.620411
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:24:41.988171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.55
 ---- batch: 020 ----
mean loss: 317.59
 ---- batch: 030 ----
mean loss: 308.73
 ---- batch: 040 ----
mean loss: 314.77
train mean loss: 311.54
epoch train time: 0:00:00.217968
elapsed time: 0:00:45.838527
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:24:42.206299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.96
 ---- batch: 020 ----
mean loss: 312.06
 ---- batch: 030 ----
mean loss: 311.41
 ---- batch: 040 ----
mean loss: 309.34
train mean loss: 312.01
epoch train time: 0:00:00.217404
elapsed time: 0:00:46.056090
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:24:42.423828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.64
 ---- batch: 020 ----
mean loss: 298.19
 ---- batch: 030 ----
mean loss: 318.04
 ---- batch: 040 ----
mean loss: 312.45
train mean loss: 311.39
epoch train time: 0:00:00.209750
elapsed time: 0:00:46.265964
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:24:42.633689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.78
 ---- batch: 020 ----
mean loss: 308.97
 ---- batch: 030 ----
mean loss: 313.46
 ---- batch: 040 ----
mean loss: 310.49
train mean loss: 311.36
epoch train time: 0:00:00.220319
elapsed time: 0:00:46.486395
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:24:42.854135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.20
 ---- batch: 020 ----
mean loss: 307.54
 ---- batch: 030 ----
mean loss: 319.61
 ---- batch: 040 ----
mean loss: 308.47
train mean loss: 311.10
epoch train time: 0:00:00.207464
elapsed time: 0:00:46.694002
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:24:43.061769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.45
 ---- batch: 020 ----
mean loss: 302.28
 ---- batch: 030 ----
mean loss: 319.20
 ---- batch: 040 ----
mean loss: 311.06
train mean loss: 310.76
epoch train time: 0:00:00.208084
elapsed time: 0:00:46.902248
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:24:43.269983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.11
 ---- batch: 020 ----
mean loss: 307.87
 ---- batch: 030 ----
mean loss: 307.32
 ---- batch: 040 ----
mean loss: 309.98
train mean loss: 310.95
epoch train time: 0:00:00.205202
elapsed time: 0:00:47.107585
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:24:43.475321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.42
 ---- batch: 020 ----
mean loss: 311.10
 ---- batch: 030 ----
mean loss: 303.47
 ---- batch: 040 ----
mean loss: 315.09
train mean loss: 310.79
epoch train time: 0:00:00.212284
elapsed time: 0:00:47.319999
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:24:43.687773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.99
 ---- batch: 020 ----
mean loss: 313.78
 ---- batch: 030 ----
mean loss: 304.78
 ---- batch: 040 ----
mean loss: 318.58
train mean loss: 310.38
epoch train time: 0:00:00.218998
elapsed time: 0:00:47.539162
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:24:43.906905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.49
 ---- batch: 020 ----
mean loss: 306.61
 ---- batch: 030 ----
mean loss: 317.44
 ---- batch: 040 ----
mean loss: 309.76
train mean loss: 310.16
epoch train time: 0:00:00.207777
elapsed time: 0:00:47.747083
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:24:44.114820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.05
 ---- batch: 020 ----
mean loss: 319.84
 ---- batch: 030 ----
mean loss: 308.05
 ---- batch: 040 ----
mean loss: 309.21
train mean loss: 309.58
epoch train time: 0:00:00.203134
elapsed time: 0:00:47.950453
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:24:44.318189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.06
 ---- batch: 020 ----
mean loss: 313.50
 ---- batch: 030 ----
mean loss: 312.84
 ---- batch: 040 ----
mean loss: 298.08
train mean loss: 310.15
epoch train time: 0:00:00.208331
elapsed time: 0:00:48.158935
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:24:44.526686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.07
 ---- batch: 020 ----
mean loss: 313.55
 ---- batch: 030 ----
mean loss: 307.04
 ---- batch: 040 ----
mean loss: 308.17
train mean loss: 309.62
epoch train time: 0:00:00.212901
elapsed time: 0:00:48.372001
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:24:44.739739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.68
 ---- batch: 020 ----
mean loss: 298.81
 ---- batch: 030 ----
mean loss: 307.01
 ---- batch: 040 ----
mean loss: 324.75
train mean loss: 309.56
epoch train time: 0:00:00.218369
elapsed time: 0:00:48.590499
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:24:44.958239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.04
 ---- batch: 020 ----
mean loss: 312.04
 ---- batch: 030 ----
mean loss: 306.28
 ---- batch: 040 ----
mean loss: 307.18
train mean loss: 309.07
epoch train time: 0:00:00.214843
elapsed time: 0:00:48.805472
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:24:45.173210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.21
 ---- batch: 020 ----
mean loss: 308.38
 ---- batch: 030 ----
mean loss: 304.13
 ---- batch: 040 ----
mean loss: 307.45
train mean loss: 308.98
epoch train time: 0:00:00.214652
elapsed time: 0:00:49.020255
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:24:45.387990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.25
 ---- batch: 020 ----
mean loss: 300.77
 ---- batch: 030 ----
mean loss: 310.71
 ---- batch: 040 ----
mean loss: 309.93
train mean loss: 309.18
epoch train time: 0:00:00.216168
elapsed time: 0:00:49.236546
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:24:45.604284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.64
 ---- batch: 020 ----
mean loss: 309.24
 ---- batch: 030 ----
mean loss: 308.76
 ---- batch: 040 ----
mean loss: 311.52
train mean loss: 308.55
epoch train time: 0:00:00.212230
elapsed time: 0:00:49.448905
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:24:45.816644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.32
 ---- batch: 020 ----
mean loss: 310.87
 ---- batch: 030 ----
mean loss: 315.81
 ---- batch: 040 ----
mean loss: 308.65
train mean loss: 308.56
epoch train time: 0:00:00.208953
elapsed time: 0:00:49.657980
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:24:46.025714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.01
 ---- batch: 020 ----
mean loss: 308.58
 ---- batch: 030 ----
mean loss: 305.19
 ---- batch: 040 ----
mean loss: 313.70
train mean loss: 308.82
epoch train time: 0:00:00.207631
elapsed time: 0:00:49.865727
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:24:46.233462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.44
 ---- batch: 020 ----
mean loss: 301.72
 ---- batch: 030 ----
mean loss: 316.80
 ---- batch: 040 ----
mean loss: 309.77
train mean loss: 308.49
epoch train time: 0:00:00.204028
elapsed time: 0:00:50.069869
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:24:46.437603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.81
 ---- batch: 020 ----
mean loss: 313.45
 ---- batch: 030 ----
mean loss: 313.11
 ---- batch: 040 ----
mean loss: 305.22
train mean loss: 307.52
epoch train time: 0:00:00.203405
elapsed time: 0:00:50.273387
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:24:46.641119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.36
 ---- batch: 020 ----
mean loss: 307.68
 ---- batch: 030 ----
mean loss: 307.59
 ---- batch: 040 ----
mean loss: 312.51
train mean loss: 308.10
epoch train time: 0:00:00.211678
elapsed time: 0:00:50.485177
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:24:46.852910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.35
 ---- batch: 020 ----
mean loss: 311.19
 ---- batch: 030 ----
mean loss: 313.02
 ---- batch: 040 ----
mean loss: 298.28
train mean loss: 308.28
epoch train time: 0:00:00.204996
elapsed time: 0:00:50.690290
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:24:47.058043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.33
 ---- batch: 020 ----
mean loss: 305.56
 ---- batch: 030 ----
mean loss: 308.23
 ---- batch: 040 ----
mean loss: 303.17
train mean loss: 308.22
epoch train time: 0:00:00.205367
elapsed time: 0:00:50.895788
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:24:47.263521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.14
 ---- batch: 020 ----
mean loss: 303.63
 ---- batch: 030 ----
mean loss: 309.77
 ---- batch: 040 ----
mean loss: 309.62
train mean loss: 307.84
epoch train time: 0:00:00.204833
elapsed time: 0:00:51.100762
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:24:47.468507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.76
 ---- batch: 020 ----
mean loss: 307.25
 ---- batch: 030 ----
mean loss: 308.79
 ---- batch: 040 ----
mean loss: 313.79
train mean loss: 307.12
epoch train time: 0:00:00.201676
elapsed time: 0:00:51.302560
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:24:47.670293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.70
 ---- batch: 020 ----
mean loss: 308.57
 ---- batch: 030 ----
mean loss: 304.91
 ---- batch: 040 ----
mean loss: 306.63
train mean loss: 307.45
epoch train time: 0:00:00.206845
elapsed time: 0:00:51.509526
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:24:47.877267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.66
 ---- batch: 020 ----
mean loss: 309.97
 ---- batch: 030 ----
mean loss: 311.30
 ---- batch: 040 ----
mean loss: 296.20
train mean loss: 307.34
epoch train time: 0:00:00.204569
elapsed time: 0:00:51.714222
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:24:48.081974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.13
 ---- batch: 020 ----
mean loss: 297.02
 ---- batch: 030 ----
mean loss: 312.43
 ---- batch: 040 ----
mean loss: 317.70
train mean loss: 307.15
epoch train time: 0:00:00.211141
elapsed time: 0:00:51.925503
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:24:48.293242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.05
 ---- batch: 020 ----
mean loss: 312.55
 ---- batch: 030 ----
mean loss: 306.55
 ---- batch: 040 ----
mean loss: 300.96
train mean loss: 307.17
epoch train time: 0:00:00.211259
elapsed time: 0:00:52.136888
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:24:48.504624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.84
 ---- batch: 020 ----
mean loss: 297.35
 ---- batch: 030 ----
mean loss: 306.98
 ---- batch: 040 ----
mean loss: 312.06
train mean loss: 306.70
epoch train time: 0:00:00.212101
elapsed time: 0:00:52.349139
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:24:48.716879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.62
 ---- batch: 020 ----
mean loss: 309.91
 ---- batch: 030 ----
mean loss: 302.45
 ---- batch: 040 ----
mean loss: 304.45
train mean loss: 306.71
epoch train time: 0:00:00.213112
elapsed time: 0:00:52.562379
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:24:48.930134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.18
 ---- batch: 020 ----
mean loss: 299.03
 ---- batch: 030 ----
mean loss: 303.83
 ---- batch: 040 ----
mean loss: 313.27
train mean loss: 305.82
epoch train time: 0:00:00.213880
elapsed time: 0:00:52.776403
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:24:49.144142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.80
 ---- batch: 020 ----
mean loss: 303.25
 ---- batch: 030 ----
mean loss: 307.72
 ---- batch: 040 ----
mean loss: 310.76
train mean loss: 305.76
epoch train time: 0:00:00.217700
elapsed time: 0:00:52.994224
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:24:49.361977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.48
 ---- batch: 020 ----
mean loss: 307.65
 ---- batch: 030 ----
mean loss: 300.65
 ---- batch: 040 ----
mean loss: 307.54
train mean loss: 305.93
epoch train time: 0:00:00.211157
elapsed time: 0:00:53.205518
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:24:49.573270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.76
 ---- batch: 020 ----
mean loss: 312.12
 ---- batch: 030 ----
mean loss: 300.90
 ---- batch: 040 ----
mean loss: 303.73
train mean loss: 305.87
epoch train time: 0:00:00.216222
elapsed time: 0:00:53.421877
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:24:49.789613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.44
 ---- batch: 020 ----
mean loss: 311.57
 ---- batch: 030 ----
mean loss: 299.44
 ---- batch: 040 ----
mean loss: 305.00
train mean loss: 305.63
epoch train time: 0:00:00.208396
elapsed time: 0:00:53.630392
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:24:49.998129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.80
 ---- batch: 020 ----
mean loss: 300.28
 ---- batch: 030 ----
mean loss: 307.93
 ---- batch: 040 ----
mean loss: 310.83
train mean loss: 305.46
epoch train time: 0:00:00.204361
elapsed time: 0:00:53.834871
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:24:50.202605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.39
 ---- batch: 020 ----
mean loss: 303.49
 ---- batch: 030 ----
mean loss: 297.60
 ---- batch: 040 ----
mean loss: 316.17
train mean loss: 305.31
epoch train time: 0:00:00.198681
elapsed time: 0:00:54.033673
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:24:50.401410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.82
 ---- batch: 020 ----
mean loss: 310.06
 ---- batch: 030 ----
mean loss: 307.93
 ---- batch: 040 ----
mean loss: 301.34
train mean loss: 305.40
epoch train time: 0:00:00.204437
elapsed time: 0:00:54.238244
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:24:50.605983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.22
 ---- batch: 020 ----
mean loss: 298.88
 ---- batch: 030 ----
mean loss: 310.27
 ---- batch: 040 ----
mean loss: 308.80
train mean loss: 305.34
epoch train time: 0:00:00.217746
elapsed time: 0:00:54.456125
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:24:50.823866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.45
 ---- batch: 020 ----
mean loss: 304.95
 ---- batch: 030 ----
mean loss: 306.52
 ---- batch: 040 ----
mean loss: 299.76
train mean loss: 304.81
epoch train time: 0:00:00.208592
elapsed time: 0:00:54.664853
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:24:51.032595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.35
 ---- batch: 020 ----
mean loss: 315.36
 ---- batch: 030 ----
mean loss: 297.24
 ---- batch: 040 ----
mean loss: 309.69
train mean loss: 305.15
epoch train time: 0:00:00.209947
elapsed time: 0:00:54.874926
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:24:51.242678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.34
 ---- batch: 020 ----
mean loss: 308.94
 ---- batch: 030 ----
mean loss: 308.71
 ---- batch: 040 ----
mean loss: 293.80
train mean loss: 304.39
epoch train time: 0:00:00.204335
elapsed time: 0:00:55.079426
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:24:51.447161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.32
 ---- batch: 020 ----
mean loss: 303.06
 ---- batch: 030 ----
mean loss: 303.62
 ---- batch: 040 ----
mean loss: 308.71
train mean loss: 304.25
epoch train time: 0:00:00.206262
elapsed time: 0:00:55.285800
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:24:51.653532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.67
 ---- batch: 020 ----
mean loss: 301.88
 ---- batch: 030 ----
mean loss: 299.11
 ---- batch: 040 ----
mean loss: 313.14
train mean loss: 304.52
epoch train time: 0:00:00.213383
elapsed time: 0:00:55.499311
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:24:51.867061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.27
 ---- batch: 020 ----
mean loss: 309.53
 ---- batch: 030 ----
mean loss: 305.48
 ---- batch: 040 ----
mean loss: 309.55
train mean loss: 304.40
epoch train time: 0:00:00.217531
elapsed time: 0:00:55.716983
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:24:52.084749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.06
 ---- batch: 020 ----
mean loss: 316.39
 ---- batch: 030 ----
mean loss: 302.24
 ---- batch: 040 ----
mean loss: 299.51
train mean loss: 303.65
epoch train time: 0:00:00.226990
elapsed time: 0:00:55.944132
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:24:52.311875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.24
 ---- batch: 020 ----
mean loss: 304.12
 ---- batch: 030 ----
mean loss: 303.25
 ---- batch: 040 ----
mean loss: 305.33
train mean loss: 303.57
epoch train time: 0:00:00.217871
elapsed time: 0:00:56.162151
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:24:52.529903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.58
 ---- batch: 020 ----
mean loss: 302.99
 ---- batch: 030 ----
mean loss: 299.44
 ---- batch: 040 ----
mean loss: 306.58
train mean loss: 303.52
epoch train time: 0:00:00.213657
elapsed time: 0:00:56.375947
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:24:52.743684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.10
 ---- batch: 020 ----
mean loss: 304.93
 ---- batch: 030 ----
mean loss: 308.56
 ---- batch: 040 ----
mean loss: 302.50
train mean loss: 303.45
epoch train time: 0:00:00.216090
elapsed time: 0:00:56.592158
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:24:52.959893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.73
 ---- batch: 020 ----
mean loss: 300.59
 ---- batch: 030 ----
mean loss: 299.40
 ---- batch: 040 ----
mean loss: 301.09
train mean loss: 303.09
epoch train time: 0:00:00.210689
elapsed time: 0:00:56.802996
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:24:53.170741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.06
 ---- batch: 020 ----
mean loss: 313.73
 ---- batch: 030 ----
mean loss: 297.82
 ---- batch: 040 ----
mean loss: 307.60
train mean loss: 302.82
epoch train time: 0:00:00.211112
elapsed time: 0:00:57.014235
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:24:53.382020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.83
 ---- batch: 020 ----
mean loss: 299.56
 ---- batch: 030 ----
mean loss: 300.52
 ---- batch: 040 ----
mean loss: 306.16
train mean loss: 303.00
epoch train time: 0:00:00.210015
elapsed time: 0:00:57.224424
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:24:53.592159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.01
 ---- batch: 020 ----
mean loss: 305.60
 ---- batch: 030 ----
mean loss: 300.81
 ---- batch: 040 ----
mean loss: 303.73
train mean loss: 302.93
epoch train time: 0:00:00.211076
elapsed time: 0:00:57.435672
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:24:53.803417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.40
 ---- batch: 020 ----
mean loss: 308.17
 ---- batch: 030 ----
mean loss: 301.57
 ---- batch: 040 ----
mean loss: 304.28
train mean loss: 302.52
epoch train time: 0:00:00.207027
elapsed time: 0:00:57.642833
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:24:54.010581
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.08
 ---- batch: 020 ----
mean loss: 311.39
 ---- batch: 030 ----
mean loss: 306.52
 ---- batch: 040 ----
mean loss: 299.98
train mean loss: 302.72
epoch train time: 0:00:00.205950
elapsed time: 0:00:57.848949
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:24:54.216700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.65
 ---- batch: 020 ----
mean loss: 304.22
 ---- batch: 030 ----
mean loss: 295.81
 ---- batch: 040 ----
mean loss: 311.07
train mean loss: 302.92
epoch train time: 0:00:00.202707
elapsed time: 0:00:58.051787
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:24:54.419522
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.45
 ---- batch: 020 ----
mean loss: 305.75
 ---- batch: 030 ----
mean loss: 304.40
 ---- batch: 040 ----
mean loss: 295.19
train mean loss: 302.60
epoch train time: 0:00:00.205489
elapsed time: 0:00:58.257428
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:24:54.625192
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.86
 ---- batch: 020 ----
mean loss: 300.75
 ---- batch: 030 ----
mean loss: 309.28
 ---- batch: 040 ----
mean loss: 301.06
train mean loss: 302.49
epoch train time: 0:00:00.212200
elapsed time: 0:00:58.469777
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:24:54.837515
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.90
 ---- batch: 020 ----
mean loss: 299.57
 ---- batch: 030 ----
mean loss: 307.86
 ---- batch: 040 ----
mean loss: 297.83
train mean loss: 302.95
epoch train time: 0:00:00.208431
elapsed time: 0:00:58.678332
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:24:55.046070
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.21
 ---- batch: 020 ----
mean loss: 310.40
 ---- batch: 030 ----
mean loss: 298.34
 ---- batch: 040 ----
mean loss: 303.73
train mean loss: 302.50
epoch train time: 0:00:00.222792
elapsed time: 0:00:58.901245
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:24:55.268980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.78
 ---- batch: 020 ----
mean loss: 307.85
 ---- batch: 030 ----
mean loss: 299.64
 ---- batch: 040 ----
mean loss: 306.37
train mean loss: 302.81
epoch train time: 0:00:00.202510
elapsed time: 0:00:59.103884
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:24:55.471618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.91
 ---- batch: 020 ----
mean loss: 300.73
 ---- batch: 030 ----
mean loss: 297.54
 ---- batch: 040 ----
mean loss: 311.08
train mean loss: 302.64
epoch train time: 0:00:00.206261
elapsed time: 0:00:59.310261
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:24:55.678011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.60
 ---- batch: 020 ----
mean loss: 302.15
 ---- batch: 030 ----
mean loss: 303.32
 ---- batch: 040 ----
mean loss: 307.45
train mean loss: 302.50
epoch train time: 0:00:00.215138
elapsed time: 0:00:59.525532
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:24:55.893270
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.63
 ---- batch: 020 ----
mean loss: 300.37
 ---- batch: 030 ----
mean loss: 300.18
 ---- batch: 040 ----
mean loss: 307.22
train mean loss: 302.96
epoch train time: 0:00:00.210848
elapsed time: 0:00:59.736502
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:24:56.104255
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.00
 ---- batch: 020 ----
mean loss: 304.46
 ---- batch: 030 ----
mean loss: 301.71
 ---- batch: 040 ----
mean loss: 302.30
train mean loss: 302.47
epoch train time: 0:00:00.206869
elapsed time: 0:00:59.943508
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:24:56.311245
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.36
 ---- batch: 020 ----
mean loss: 296.57
 ---- batch: 030 ----
mean loss: 302.80
 ---- batch: 040 ----
mean loss: 308.51
train mean loss: 302.65
epoch train time: 0:00:00.207913
elapsed time: 0:01:00.151540
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:24:56.519277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.34
 ---- batch: 020 ----
mean loss: 309.91
 ---- batch: 030 ----
mean loss: 296.13
 ---- batch: 040 ----
mean loss: 300.24
train mean loss: 302.66
epoch train time: 0:00:00.209042
elapsed time: 0:01:00.360731
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:24:56.728473
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.90
 ---- batch: 020 ----
mean loss: 297.79
 ---- batch: 030 ----
mean loss: 304.14
 ---- batch: 040 ----
mean loss: 299.98
train mean loss: 302.51
epoch train time: 0:00:00.210841
elapsed time: 0:01:00.571700
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:24:56.939454
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.31
 ---- batch: 020 ----
mean loss: 303.10
 ---- batch: 030 ----
mean loss: 296.11
 ---- batch: 040 ----
mean loss: 306.51
train mean loss: 302.81
epoch train time: 0:00:00.212354
elapsed time: 0:01:00.784192
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:24:57.151929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.31
 ---- batch: 020 ----
mean loss: 308.98
 ---- batch: 030 ----
mean loss: 301.35
 ---- batch: 040 ----
mean loss: 296.11
train mean loss: 302.40
epoch train time: 0:00:00.210519
elapsed time: 0:01:00.994834
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:24:57.362573
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.78
 ---- batch: 020 ----
mean loss: 298.09
 ---- batch: 030 ----
mean loss: 296.74
 ---- batch: 040 ----
mean loss: 302.83
train mean loss: 302.59
epoch train time: 0:00:00.208365
elapsed time: 0:01:01.203321
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:24:57.571086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.43
 ---- batch: 020 ----
mean loss: 303.31
 ---- batch: 030 ----
mean loss: 302.05
 ---- batch: 040 ----
mean loss: 305.28
train mean loss: 302.45
epoch train time: 0:00:00.201435
elapsed time: 0:01:01.404903
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:24:57.772651
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.91
 ---- batch: 020 ----
mean loss: 308.71
 ---- batch: 030 ----
mean loss: 297.22
 ---- batch: 040 ----
mean loss: 289.12
train mean loss: 303.04
epoch train time: 0:00:00.203668
elapsed time: 0:01:01.608753
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:24:57.976493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.08
 ---- batch: 020 ----
mean loss: 301.65
 ---- batch: 030 ----
mean loss: 307.55
 ---- batch: 040 ----
mean loss: 301.55
train mean loss: 302.74
epoch train time: 0:00:00.198644
elapsed time: 0:01:01.807540
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:24:58.175288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.61
 ---- batch: 020 ----
mean loss: 306.00
 ---- batch: 030 ----
mean loss: 295.33
 ---- batch: 040 ----
mean loss: 302.52
train mean loss: 302.44
epoch train time: 0:00:00.197400
elapsed time: 0:01:02.005101
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:24:58.372879
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.86
 ---- batch: 020 ----
mean loss: 304.16
 ---- batch: 030 ----
mean loss: 303.15
 ---- batch: 040 ----
mean loss: 299.82
train mean loss: 302.58
epoch train time: 0:00:00.201856
elapsed time: 0:01:02.207144
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:24:58.574888
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.01
 ---- batch: 020 ----
mean loss: 290.96
 ---- batch: 030 ----
mean loss: 303.30
 ---- batch: 040 ----
mean loss: 307.26
train mean loss: 302.48
epoch train time: 0:00:00.200134
elapsed time: 0:01:02.407405
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:24:58.775140
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.92
 ---- batch: 020 ----
mean loss: 297.90
 ---- batch: 030 ----
mean loss: 306.84
 ---- batch: 040 ----
mean loss: 300.29
train mean loss: 302.19
epoch train time: 0:00:00.205778
elapsed time: 0:01:02.613299
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:24:58.981033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.42
 ---- batch: 020 ----
mean loss: 303.15
 ---- batch: 030 ----
mean loss: 299.59
 ---- batch: 040 ----
mean loss: 299.16
train mean loss: 302.23
epoch train time: 0:00:00.202123
elapsed time: 0:01:02.815545
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:24:59.183280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.59
 ---- batch: 020 ----
mean loss: 297.45
 ---- batch: 030 ----
mean loss: 305.00
 ---- batch: 040 ----
mean loss: 303.99
train mean loss: 302.26
epoch train time: 0:00:00.204365
elapsed time: 0:01:03.020028
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:24:59.387787
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.13
 ---- batch: 020 ----
mean loss: 308.84
 ---- batch: 030 ----
mean loss: 294.93
 ---- batch: 040 ----
mean loss: 303.04
train mean loss: 302.22
epoch train time: 0:00:00.207924
elapsed time: 0:01:03.228096
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:24:59.595833
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.47
 ---- batch: 020 ----
mean loss: 305.50
 ---- batch: 030 ----
mean loss: 305.72
 ---- batch: 040 ----
mean loss: 307.69
train mean loss: 301.81
epoch train time: 0:00:00.216062
elapsed time: 0:01:03.444297
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:24:59.812037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.72
 ---- batch: 020 ----
mean loss: 298.08
 ---- batch: 030 ----
mean loss: 307.99
 ---- batch: 040 ----
mean loss: 303.66
train mean loss: 302.20
epoch train time: 0:00:00.209829
elapsed time: 0:01:03.654257
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:25:00.022012
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.01
 ---- batch: 020 ----
mean loss: 303.13
 ---- batch: 030 ----
mean loss: 306.64
 ---- batch: 040 ----
mean loss: 305.29
train mean loss: 302.34
epoch train time: 0:00:00.210483
elapsed time: 0:01:03.864883
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:25:00.232621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.87
 ---- batch: 020 ----
mean loss: 308.61
 ---- batch: 030 ----
mean loss: 301.49
 ---- batch: 040 ----
mean loss: 300.40
train mean loss: 302.29
epoch train time: 0:00:00.207791
elapsed time: 0:01:04.072824
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:25:00.440566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.36
 ---- batch: 020 ----
mean loss: 307.54
 ---- batch: 030 ----
mean loss: 306.06
 ---- batch: 040 ----
mean loss: 294.86
train mean loss: 302.19
epoch train time: 0:00:00.208403
elapsed time: 0:01:04.281352
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:25:00.649087
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.57
 ---- batch: 020 ----
mean loss: 304.22
 ---- batch: 030 ----
mean loss: 301.52
 ---- batch: 040 ----
mean loss: 301.57
train mean loss: 301.89
epoch train time: 0:00:00.210536
elapsed time: 0:01:04.492024
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:25:00.859762
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 307.95
 ---- batch: 020 ----
mean loss: 294.58
 ---- batch: 030 ----
mean loss: 305.34
 ---- batch: 040 ----
mean loss: 295.81
train mean loss: 302.29
epoch train time: 0:00:00.213716
elapsed time: 0:01:04.705873
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:25:01.073619
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 311.31
 ---- batch: 020 ----
mean loss: 297.65
 ---- batch: 030 ----
mean loss: 303.42
 ---- batch: 040 ----
mean loss: 299.93
train mean loss: 301.92
epoch train time: 0:00:00.212443
elapsed time: 0:01:04.918449
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:25:01.286186
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.78
 ---- batch: 020 ----
mean loss: 298.68
 ---- batch: 030 ----
mean loss: 299.56
 ---- batch: 040 ----
mean loss: 297.19
train mean loss: 302.20
epoch train time: 0:00:00.200645
elapsed time: 0:01:05.119205
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:25:01.486937
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.62
 ---- batch: 020 ----
mean loss: 302.79
 ---- batch: 030 ----
mean loss: 298.99
 ---- batch: 040 ----
mean loss: 308.67
train mean loss: 301.90
epoch train time: 0:00:00.199023
elapsed time: 0:01:05.318356
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:25:01.686102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.12
 ---- batch: 020 ----
mean loss: 306.08
 ---- batch: 030 ----
mean loss: 303.57
 ---- batch: 040 ----
mean loss: 304.18
train mean loss: 301.87
epoch train time: 0:00:00.214414
elapsed time: 0:01:05.532906
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:25:01.900659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.69
 ---- batch: 020 ----
mean loss: 304.99
 ---- batch: 030 ----
mean loss: 305.94
 ---- batch: 040 ----
mean loss: 300.21
train mean loss: 302.30
epoch train time: 0:00:00.213622
elapsed time: 0:01:05.746665
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:25:02.114412
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.58
 ---- batch: 020 ----
mean loss: 305.28
 ---- batch: 030 ----
mean loss: 305.68
 ---- batch: 040 ----
mean loss: 296.76
train mean loss: 302.13
epoch train time: 0:00:00.207390
elapsed time: 0:01:05.954186
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:25:02.321923
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.44
 ---- batch: 020 ----
mean loss: 301.36
 ---- batch: 030 ----
mean loss: 299.99
 ---- batch: 040 ----
mean loss: 305.46
train mean loss: 301.93
epoch train time: 0:00:00.207448
elapsed time: 0:01:06.161769
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:25:02.529521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.46
 ---- batch: 020 ----
mean loss: 303.95
 ---- batch: 030 ----
mean loss: 305.89
 ---- batch: 040 ----
mean loss: 304.12
train mean loss: 301.89
epoch train time: 0:00:00.206470
elapsed time: 0:01:06.368382
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:25:02.736120
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.89
 ---- batch: 020 ----
mean loss: 301.90
 ---- batch: 030 ----
mean loss: 301.15
 ---- batch: 040 ----
mean loss: 303.07
train mean loss: 301.82
epoch train time: 0:00:00.208662
elapsed time: 0:01:06.577167
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:25:02.944925
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.36
 ---- batch: 020 ----
mean loss: 302.21
 ---- batch: 030 ----
mean loss: 307.61
 ---- batch: 040 ----
mean loss: 295.63
train mean loss: 302.32
epoch train time: 0:00:00.207352
elapsed time: 0:01:06.784736
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:25:03.152476
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.26
 ---- batch: 020 ----
mean loss: 307.51
 ---- batch: 030 ----
mean loss: 300.58
 ---- batch: 040 ----
mean loss: 297.59
train mean loss: 302.53
epoch train time: 0:00:00.211133
elapsed time: 0:01:06.995998
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:25:03.363759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.30
 ---- batch: 020 ----
mean loss: 295.87
 ---- batch: 030 ----
mean loss: 310.65
 ---- batch: 040 ----
mean loss: 295.50
train mean loss: 301.89
epoch train time: 0:00:00.213608
elapsed time: 0:01:07.209750
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:25:03.577505
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.95
 ---- batch: 020 ----
mean loss: 308.41
 ---- batch: 030 ----
mean loss: 300.35
 ---- batch: 040 ----
mean loss: 296.56
train mean loss: 301.87
epoch train time: 0:00:00.211766
elapsed time: 0:01:07.423711
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_0/checkpoint.pth.tar
**** end time: 2019-09-20 23:25:03.791423 ****
