Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_6', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 20738
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianDense3...
Done.
**** start time: 2019-09-25 20:09:12.209546 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
    BayesianLinear-2                  [-1, 100]          96,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 136,200
Trainable params: 136,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 20:09:12.220017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4198.62
 ---- batch: 020 ----
mean loss: 3941.71
 ---- batch: 030 ----
mean loss: 3779.38
 ---- batch: 040 ----
mean loss: 3521.79
 ---- batch: 050 ----
mean loss: 3249.41
 ---- batch: 060 ----
mean loss: 3169.44
 ---- batch: 070 ----
mean loss: 2961.03
 ---- batch: 080 ----
mean loss: 2886.41
 ---- batch: 090 ----
mean loss: 2770.84
train mean loss: 3342.08
epoch train time: 0:00:35.137883
elapsed time: 0:00:35.155317
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 20:09:47.364905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2565.30
 ---- batch: 020 ----
mean loss: 2554.77
 ---- batch: 030 ----
mean loss: 2468.26
 ---- batch: 040 ----
mean loss: 2414.08
 ---- batch: 050 ----
mean loss: 2296.22
 ---- batch: 060 ----
mean loss: 2285.24
 ---- batch: 070 ----
mean loss: 2228.77
 ---- batch: 080 ----
mean loss: 2170.78
 ---- batch: 090 ----
mean loss: 2145.13
train mean loss: 2332.39
epoch train time: 0:00:01.647747
elapsed time: 0:00:36.803507
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 20:09:49.013378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2048.60
 ---- batch: 020 ----
mean loss: 2000.99
 ---- batch: 030 ----
mean loss: 1994.51
 ---- batch: 040 ----
mean loss: 1964.26
 ---- batch: 050 ----
mean loss: 1953.35
 ---- batch: 060 ----
mean loss: 1909.93
 ---- batch: 070 ----
mean loss: 1904.56
 ---- batch: 080 ----
mean loss: 1849.38
 ---- batch: 090 ----
mean loss: 1806.77
train mean loss: 1930.18
epoch train time: 0:00:01.629926
elapsed time: 0:00:38.434146
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 20:09:50.643943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1785.21
 ---- batch: 020 ----
mean loss: 1736.14
 ---- batch: 030 ----
mean loss: 1712.27
 ---- batch: 040 ----
mean loss: 1730.67
 ---- batch: 050 ----
mean loss: 1687.85
 ---- batch: 060 ----
mean loss: 1679.59
 ---- batch: 070 ----
mean loss: 1636.50
 ---- batch: 080 ----
mean loss: 1623.33
 ---- batch: 090 ----
mean loss: 1628.13
train mean loss: 1685.04
epoch train time: 0:00:01.642184
elapsed time: 0:00:40.076914
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 20:09:52.286779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1574.17
 ---- batch: 020 ----
mean loss: 1561.64
 ---- batch: 030 ----
mean loss: 1530.70
 ---- batch: 040 ----
mean loss: 1514.26
 ---- batch: 050 ----
mean loss: 1526.00
 ---- batch: 060 ----
mean loss: 1478.57
 ---- batch: 070 ----
mean loss: 1485.22
 ---- batch: 080 ----
mean loss: 1485.33
 ---- batch: 090 ----
mean loss: 1448.26
train mean loss: 1505.32
epoch train time: 0:00:01.660409
elapsed time: 0:00:41.738066
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 20:09:53.947915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1403.69
 ---- batch: 020 ----
mean loss: 1388.61
 ---- batch: 030 ----
mean loss: 1399.55
 ---- batch: 040 ----
mean loss: 1371.77
 ---- batch: 050 ----
mean loss: 1401.23
 ---- batch: 060 ----
mean loss: 1326.40
 ---- batch: 070 ----
mean loss: 1357.34
 ---- batch: 080 ----
mean loss: 1358.57
 ---- batch: 090 ----
mean loss: 1303.24
train mean loss: 1363.39
epoch train time: 0:00:01.658604
elapsed time: 0:00:43.397306
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 20:09:55.607162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1294.82
 ---- batch: 020 ----
mean loss: 1301.98
 ---- batch: 030 ----
mean loss: 1249.37
 ---- batch: 040 ----
mean loss: 1279.65
 ---- batch: 050 ----
mean loss: 1258.57
 ---- batch: 060 ----
mean loss: 1252.59
 ---- batch: 070 ----
mean loss: 1229.49
 ---- batch: 080 ----
mean loss: 1228.55
 ---- batch: 090 ----
mean loss: 1215.04
train mean loss: 1251.76
epoch train time: 0:00:01.704759
elapsed time: 0:00:45.102807
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 20:09:57.312721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1179.98
 ---- batch: 020 ----
mean loss: 1190.93
 ---- batch: 030 ----
mean loss: 1206.41
 ---- batch: 040 ----
mean loss: 1144.88
 ---- batch: 050 ----
mean loss: 1175.90
 ---- batch: 060 ----
mean loss: 1165.51
 ---- batch: 070 ----
mean loss: 1135.06
 ---- batch: 080 ----
mean loss: 1147.41
 ---- batch: 090 ----
mean loss: 1119.84
train mean loss: 1159.42
epoch train time: 0:00:01.633039
elapsed time: 0:00:46.736603
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 20:09:58.946460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1126.00
 ---- batch: 020 ----
mean loss: 1099.86
 ---- batch: 030 ----
mean loss: 1096.54
 ---- batch: 040 ----
mean loss: 1098.63
 ---- batch: 050 ----
mean loss: 1102.06
 ---- batch: 060 ----
mean loss: 1074.59
 ---- batch: 070 ----
mean loss: 1088.59
 ---- batch: 080 ----
mean loss: 1056.05
 ---- batch: 090 ----
mean loss: 1081.93
train mean loss: 1090.43
epoch train time: 0:00:01.655924
elapsed time: 0:00:48.393177
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 20:10:00.603037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1064.74
 ---- batch: 020 ----
mean loss: 1048.15
 ---- batch: 030 ----
mean loss: 1047.58
 ---- batch: 040 ----
mean loss: 1020.05
 ---- batch: 050 ----
mean loss: 1049.63
 ---- batch: 060 ----
mean loss: 1034.85
 ---- batch: 070 ----
mean loss: 1044.98
 ---- batch: 080 ----
mean loss: 1010.65
 ---- batch: 090 ----
mean loss: 1024.11
train mean loss: 1035.79
epoch train time: 0:00:01.665363
elapsed time: 0:00:50.059249
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 20:10:02.269117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 998.28
 ---- batch: 020 ----
mean loss: 1014.96
 ---- batch: 030 ----
mean loss: 1011.80
 ---- batch: 040 ----
mean loss: 986.37
 ---- batch: 050 ----
mean loss: 989.27
 ---- batch: 060 ----
mean loss: 1002.32
 ---- batch: 070 ----
mean loss: 990.16
 ---- batch: 080 ----
mean loss: 960.98
 ---- batch: 090 ----
mean loss: 988.43
train mean loss: 992.91
epoch train time: 0:00:01.658178
elapsed time: 0:00:51.718090
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 20:10:03.927942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 954.59
 ---- batch: 020 ----
mean loss: 971.75
 ---- batch: 030 ----
mean loss: 977.55
 ---- batch: 040 ----
mean loss: 977.48
 ---- batch: 050 ----
mean loss: 970.95
 ---- batch: 060 ----
mean loss: 963.02
 ---- batch: 070 ----
mean loss: 966.38
 ---- batch: 080 ----
mean loss: 941.31
 ---- batch: 090 ----
mean loss: 932.27
train mean loss: 959.46
epoch train time: 0:00:01.643799
elapsed time: 0:00:53.362510
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 20:10:05.572238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 938.27
 ---- batch: 020 ----
mean loss: 932.63
 ---- batch: 030 ----
mean loss: 945.31
 ---- batch: 040 ----
mean loss: 929.72
 ---- batch: 050 ----
mean loss: 955.49
 ---- batch: 060 ----
mean loss: 937.41
 ---- batch: 070 ----
mean loss: 917.83
 ---- batch: 080 ----
mean loss: 930.94
 ---- batch: 090 ----
mean loss: 931.88
train mean loss: 935.26
epoch train time: 0:00:01.640679
elapsed time: 0:00:55.003740
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 20:10:07.213575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.93
 ---- batch: 020 ----
mean loss: 918.81
 ---- batch: 030 ----
mean loss: 920.86
 ---- batch: 040 ----
mean loss: 897.25
 ---- batch: 050 ----
mean loss: 915.58
 ---- batch: 060 ----
mean loss: 915.97
 ---- batch: 070 ----
mean loss: 939.25
 ---- batch: 080 ----
mean loss: 914.51
 ---- batch: 090 ----
mean loss: 916.26
train mean loss: 918.37
epoch train time: 0:00:01.644471
elapsed time: 0:00:56.648938
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 20:10:08.858822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.69
 ---- batch: 020 ----
mean loss: 918.74
 ---- batch: 030 ----
mean loss: 909.44
 ---- batch: 040 ----
mean loss: 896.31
 ---- batch: 050 ----
mean loss: 901.02
 ---- batch: 060 ----
mean loss: 901.75
 ---- batch: 070 ----
mean loss: 901.68
 ---- batch: 080 ----
mean loss: 917.16
 ---- batch: 090 ----
mean loss: 906.08
train mean loss: 907.80
epoch train time: 0:00:01.643596
elapsed time: 0:00:58.293230
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 20:10:10.503059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.38
 ---- batch: 020 ----
mean loss: 898.94
 ---- batch: 030 ----
mean loss: 898.86
 ---- batch: 040 ----
mean loss: 910.35
 ---- batch: 050 ----
mean loss: 904.28
 ---- batch: 060 ----
mean loss: 889.98
 ---- batch: 070 ----
mean loss: 876.65
 ---- batch: 080 ----
mean loss: 895.10
 ---- batch: 090 ----
mean loss: 898.98
train mean loss: 898.11
epoch train time: 0:00:01.688302
elapsed time: 0:00:59.982149
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 20:10:12.192019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.64
 ---- batch: 020 ----
mean loss: 875.61
 ---- batch: 030 ----
mean loss: 891.03
 ---- batch: 040 ----
mean loss: 898.62
 ---- batch: 050 ----
mean loss: 875.21
 ---- batch: 060 ----
mean loss: 902.16
 ---- batch: 070 ----
mean loss: 906.30
 ---- batch: 080 ----
mean loss: 910.85
 ---- batch: 090 ----
mean loss: 874.72
train mean loss: 893.40
epoch train time: 0:00:01.658120
elapsed time: 0:01:01.641060
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 20:10:13.850931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.42
 ---- batch: 020 ----
mean loss: 883.63
 ---- batch: 030 ----
mean loss: 900.08
 ---- batch: 040 ----
mean loss: 893.22
 ---- batch: 050 ----
mean loss: 878.73
 ---- batch: 060 ----
mean loss: 873.92
 ---- batch: 070 ----
mean loss: 888.55
 ---- batch: 080 ----
mean loss: 881.99
 ---- batch: 090 ----
mean loss: 880.90
train mean loss: 887.52
epoch train time: 0:00:01.652013
elapsed time: 0:01:03.293765
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 20:10:15.503655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.52
 ---- batch: 020 ----
mean loss: 890.78
 ---- batch: 030 ----
mean loss: 874.29
 ---- batch: 040 ----
mean loss: 891.34
 ---- batch: 050 ----
mean loss: 888.34
 ---- batch: 060 ----
mean loss: 879.21
 ---- batch: 070 ----
mean loss: 862.98
 ---- batch: 080 ----
mean loss: 899.43
 ---- batch: 090 ----
mean loss: 890.63
train mean loss: 884.95
epoch train time: 0:00:01.653735
elapsed time: 0:01:04.948199
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 20:10:17.158065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.81
 ---- batch: 020 ----
mean loss: 893.62
 ---- batch: 030 ----
mean loss: 881.05
 ---- batch: 040 ----
mean loss: 888.33
 ---- batch: 050 ----
mean loss: 873.87
 ---- batch: 060 ----
mean loss: 885.96
 ---- batch: 070 ----
mean loss: 895.78
 ---- batch: 080 ----
mean loss: 869.96
 ---- batch: 090 ----
mean loss: 878.88
train mean loss: 882.67
epoch train time: 0:00:01.669472
elapsed time: 0:01:06.618357
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 20:10:18.828204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.73
 ---- batch: 020 ----
mean loss: 873.14
 ---- batch: 030 ----
mean loss: 875.78
 ---- batch: 040 ----
mean loss: 883.22
 ---- batch: 050 ----
mean loss: 904.03
 ---- batch: 060 ----
mean loss: 881.82
 ---- batch: 070 ----
mean loss: 863.93
 ---- batch: 080 ----
mean loss: 893.63
 ---- batch: 090 ----
mean loss: 888.28
train mean loss: 882.38
epoch train time: 0:00:01.662989
elapsed time: 0:01:08.281956
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 20:10:20.491797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.23
 ---- batch: 020 ----
mean loss: 895.74
 ---- batch: 030 ----
mean loss: 873.13
 ---- batch: 040 ----
mean loss: 861.12
 ---- batch: 050 ----
mean loss: 876.63
 ---- batch: 060 ----
mean loss: 900.21
 ---- batch: 070 ----
mean loss: 887.03
 ---- batch: 080 ----
mean loss: 875.83
 ---- batch: 090 ----
mean loss: 880.61
train mean loss: 881.49
epoch train time: 0:00:01.645439
elapsed time: 0:01:09.928108
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 20:10:22.137785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.61
 ---- batch: 020 ----
mean loss: 879.13
 ---- batch: 030 ----
mean loss: 865.33
 ---- batch: 040 ----
mean loss: 875.43
 ---- batch: 050 ----
mean loss: 883.71
 ---- batch: 060 ----
mean loss: 882.82
 ---- batch: 070 ----
mean loss: 882.93
 ---- batch: 080 ----
mean loss: 884.73
 ---- batch: 090 ----
mean loss: 886.85
train mean loss: 880.48
epoch train time: 0:00:01.694236
elapsed time: 0:01:11.622951
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 20:10:23.832903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.64
 ---- batch: 020 ----
mean loss: 876.09
 ---- batch: 030 ----
mean loss: 883.09
 ---- batch: 040 ----
mean loss: 862.75
 ---- batch: 050 ----
mean loss: 879.90
 ---- batch: 060 ----
mean loss: 873.64
 ---- batch: 070 ----
mean loss: 884.05
 ---- batch: 080 ----
mean loss: 888.73
 ---- batch: 090 ----
mean loss: 878.84
train mean loss: 882.32
epoch train time: 0:00:01.685951
elapsed time: 0:01:13.309663
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 20:10:25.519502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.55
 ---- batch: 020 ----
mean loss: 895.15
 ---- batch: 030 ----
mean loss: 886.93
 ---- batch: 040 ----
mean loss: 878.49
 ---- batch: 050 ----
mean loss: 882.25
 ---- batch: 060 ----
mean loss: 881.93
 ---- batch: 070 ----
mean loss: 877.06
 ---- batch: 080 ----
mean loss: 867.18
 ---- batch: 090 ----
mean loss: 890.19
train mean loss: 878.72
epoch train time: 0:00:01.681059
elapsed time: 0:01:14.991415
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 20:10:27.201349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.96
 ---- batch: 020 ----
mean loss: 873.72
 ---- batch: 030 ----
mean loss: 868.24
 ---- batch: 040 ----
mean loss: 869.60
 ---- batch: 050 ----
mean loss: 868.78
 ---- batch: 060 ----
mean loss: 905.97
 ---- batch: 070 ----
mean loss: 890.63
 ---- batch: 080 ----
mean loss: 880.85
 ---- batch: 090 ----
mean loss: 871.33
train mean loss: 878.50
epoch train time: 0:00:01.677057
elapsed time: 0:01:16.669343
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 20:10:28.879421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.15
 ---- batch: 020 ----
mean loss: 875.78
 ---- batch: 030 ----
mean loss: 877.84
 ---- batch: 040 ----
mean loss: 876.90
 ---- batch: 050 ----
mean loss: 870.03
 ---- batch: 060 ----
mean loss: 874.57
 ---- batch: 070 ----
mean loss: 889.71
 ---- batch: 080 ----
mean loss: 888.75
 ---- batch: 090 ----
mean loss: 890.28
train mean loss: 879.53
epoch train time: 0:00:01.710706
elapsed time: 0:01:18.381019
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 20:10:30.590916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.23
 ---- batch: 020 ----
mean loss: 873.55
 ---- batch: 030 ----
mean loss: 882.83
 ---- batch: 040 ----
mean loss: 885.08
 ---- batch: 050 ----
mean loss: 878.35
 ---- batch: 060 ----
mean loss: 872.37
 ---- batch: 070 ----
mean loss: 866.46
 ---- batch: 080 ----
mean loss: 894.33
 ---- batch: 090 ----
mean loss: 867.18
train mean loss: 878.14
epoch train time: 0:00:01.700013
elapsed time: 0:01:20.081749
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 20:10:32.291601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.96
 ---- batch: 020 ----
mean loss: 877.32
 ---- batch: 030 ----
mean loss: 867.83
 ---- batch: 040 ----
mean loss: 879.20
 ---- batch: 050 ----
mean loss: 887.07
 ---- batch: 060 ----
mean loss: 888.51
 ---- batch: 070 ----
mean loss: 890.47
 ---- batch: 080 ----
mean loss: 867.37
 ---- batch: 090 ----
mean loss: 864.78
train mean loss: 879.80
epoch train time: 0:00:01.642029
elapsed time: 0:01:21.724424
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 20:10:33.934246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.99
 ---- batch: 020 ----
mean loss: 891.02
 ---- batch: 030 ----
mean loss: 873.27
 ---- batch: 040 ----
mean loss: 880.13
 ---- batch: 050 ----
mean loss: 877.56
 ---- batch: 060 ----
mean loss: 879.39
 ---- batch: 070 ----
mean loss: 876.91
 ---- batch: 080 ----
mean loss: 873.34
 ---- batch: 090 ----
mean loss: 862.46
train mean loss: 878.32
epoch train time: 0:00:01.642888
elapsed time: 0:01:23.367923
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 20:10:35.577829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.89
 ---- batch: 020 ----
mean loss: 873.96
 ---- batch: 030 ----
mean loss: 874.86
 ---- batch: 040 ----
mean loss: 883.27
 ---- batch: 050 ----
mean loss: 893.32
 ---- batch: 060 ----
mean loss: 869.05
 ---- batch: 070 ----
mean loss: 894.33
 ---- batch: 080 ----
mean loss: 871.46
 ---- batch: 090 ----
mean loss: 863.49
train mean loss: 878.17
epoch train time: 0:00:01.643725
elapsed time: 0:01:25.012366
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 20:10:37.222288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.70
 ---- batch: 020 ----
mean loss: 882.27
 ---- batch: 030 ----
mean loss: 876.72
 ---- batch: 040 ----
mean loss: 872.24
 ---- batch: 050 ----
mean loss: 877.23
 ---- batch: 060 ----
mean loss: 891.03
 ---- batch: 070 ----
mean loss: 865.21
 ---- batch: 080 ----
mean loss: 885.15
 ---- batch: 090 ----
mean loss: 874.45
train mean loss: 877.95
epoch train time: 0:00:01.663905
elapsed time: 0:01:26.676994
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 20:10:38.886831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.54
 ---- batch: 020 ----
mean loss: 879.31
 ---- batch: 030 ----
mean loss: 882.67
 ---- batch: 040 ----
mean loss: 875.15
 ---- batch: 050 ----
mean loss: 870.83
 ---- batch: 060 ----
mean loss: 871.92
 ---- batch: 070 ----
mean loss: 870.65
 ---- batch: 080 ----
mean loss: 879.75
 ---- batch: 090 ----
mean loss: 879.17
train mean loss: 879.41
epoch train time: 0:00:01.670667
elapsed time: 0:01:28.348456
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 20:10:40.558344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.50
 ---- batch: 020 ----
mean loss: 885.39
 ---- batch: 030 ----
mean loss: 870.13
 ---- batch: 040 ----
mean loss: 877.61
 ---- batch: 050 ----
mean loss: 892.08
 ---- batch: 060 ----
mean loss: 883.16
 ---- batch: 070 ----
mean loss: 888.37
 ---- batch: 080 ----
mean loss: 860.35
 ---- batch: 090 ----
mean loss: 877.71
train mean loss: 877.83
epoch train time: 0:00:01.668899
elapsed time: 0:01:30.018177
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 20:10:42.228085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.91
 ---- batch: 020 ----
mean loss: 875.24
 ---- batch: 030 ----
mean loss: 862.83
 ---- batch: 040 ----
mean loss: 880.07
 ---- batch: 050 ----
mean loss: 873.01
 ---- batch: 060 ----
mean loss: 888.24
 ---- batch: 070 ----
mean loss: 879.93
 ---- batch: 080 ----
mean loss: 880.11
 ---- batch: 090 ----
mean loss: 890.85
train mean loss: 878.21
epoch train time: 0:00:01.674166
elapsed time: 0:01:31.693096
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 20:10:43.902950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.82
 ---- batch: 020 ----
mean loss: 871.60
 ---- batch: 030 ----
mean loss: 877.45
 ---- batch: 040 ----
mean loss: 889.22
 ---- batch: 050 ----
mean loss: 891.56
 ---- batch: 060 ----
mean loss: 861.02
 ---- batch: 070 ----
mean loss: 864.08
 ---- batch: 080 ----
mean loss: 868.54
 ---- batch: 090 ----
mean loss: 910.13
train mean loss: 878.22
epoch train time: 0:00:01.659021
elapsed time: 0:01:33.352893
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 20:10:45.562757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.80
 ---- batch: 020 ----
mean loss: 870.39
 ---- batch: 030 ----
mean loss: 883.09
 ---- batch: 040 ----
mean loss: 900.36
 ---- batch: 050 ----
mean loss: 900.59
 ---- batch: 060 ----
mean loss: 874.69
 ---- batch: 070 ----
mean loss: 871.96
 ---- batch: 080 ----
mean loss: 853.01
 ---- batch: 090 ----
mean loss: 874.03
train mean loss: 878.06
epoch train time: 0:00:01.694041
elapsed time: 0:01:35.047608
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 20:10:47.257510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.05
 ---- batch: 020 ----
mean loss: 888.60
 ---- batch: 030 ----
mean loss: 881.64
 ---- batch: 040 ----
mean loss: 882.35
 ---- batch: 050 ----
mean loss: 868.14
 ---- batch: 060 ----
mean loss: 884.46
 ---- batch: 070 ----
mean loss: 871.88
 ---- batch: 080 ----
mean loss: 885.23
 ---- batch: 090 ----
mean loss: 860.73
train mean loss: 878.04
epoch train time: 0:00:01.681038
elapsed time: 0:01:36.729408
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 20:10:48.939260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.29
 ---- batch: 020 ----
mean loss: 876.25
 ---- batch: 030 ----
mean loss: 872.27
 ---- batch: 040 ----
mean loss: 877.46
 ---- batch: 050 ----
mean loss: 863.51
 ---- batch: 060 ----
mean loss: 888.02
 ---- batch: 070 ----
mean loss: 885.77
 ---- batch: 080 ----
mean loss: 869.97
 ---- batch: 090 ----
mean loss: 885.45
train mean loss: 878.45
epoch train time: 0:00:01.624658
elapsed time: 0:01:38.354688
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 20:10:50.564529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.09
 ---- batch: 020 ----
mean loss: 866.60
 ---- batch: 030 ----
mean loss: 871.08
 ---- batch: 040 ----
mean loss: 875.67
 ---- batch: 050 ----
mean loss: 877.81
 ---- batch: 060 ----
mean loss: 875.39
 ---- batch: 070 ----
mean loss: 877.87
 ---- batch: 080 ----
mean loss: 875.29
 ---- batch: 090 ----
mean loss: 881.08
train mean loss: 877.58
epoch train time: 0:00:01.672697
elapsed time: 0:01:40.028035
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 20:10:52.237916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.58
 ---- batch: 020 ----
mean loss: 889.71
 ---- batch: 030 ----
mean loss: 877.24
 ---- batch: 040 ----
mean loss: 867.67
 ---- batch: 050 ----
mean loss: 891.36
 ---- batch: 060 ----
mean loss: 880.19
 ---- batch: 070 ----
mean loss: 874.61
 ---- batch: 080 ----
mean loss: 864.04
 ---- batch: 090 ----
mean loss: 869.21
train mean loss: 876.54
epoch train time: 0:00:01.643039
elapsed time: 0:01:41.671774
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 20:10:53.881639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.26
 ---- batch: 020 ----
mean loss: 880.05
 ---- batch: 030 ----
mean loss: 877.97
 ---- batch: 040 ----
mean loss: 869.92
 ---- batch: 050 ----
mean loss: 861.07
 ---- batch: 060 ----
mean loss: 884.70
 ---- batch: 070 ----
mean loss: 878.70
 ---- batch: 080 ----
mean loss: 878.78
 ---- batch: 090 ----
mean loss: 887.28
train mean loss: 876.56
epoch train time: 0:00:01.626366
elapsed time: 0:01:43.298845
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 20:10:55.508719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.71
 ---- batch: 020 ----
mean loss: 862.67
 ---- batch: 030 ----
mean loss: 880.79
 ---- batch: 040 ----
mean loss: 851.49
 ---- batch: 050 ----
mean loss: 856.20
 ---- batch: 060 ----
mean loss: 876.24
 ---- batch: 070 ----
mean loss: 891.65
 ---- batch: 080 ----
mean loss: 894.14
 ---- batch: 090 ----
mean loss: 901.22
train mean loss: 877.41
epoch train time: 0:00:01.698439
elapsed time: 0:01:44.997913
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 20:10:57.207800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.37
 ---- batch: 020 ----
mean loss: 861.14
 ---- batch: 030 ----
mean loss: 881.50
 ---- batch: 040 ----
mean loss: 891.44
 ---- batch: 050 ----
mean loss: 873.08
 ---- batch: 060 ----
mean loss: 878.81
 ---- batch: 070 ----
mean loss: 873.95
 ---- batch: 080 ----
mean loss: 887.40
 ---- batch: 090 ----
mean loss: 884.99
train mean loss: 876.65
epoch train time: 0:00:01.659295
elapsed time: 0:01:46.657894
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 20:10:58.867562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.84
 ---- batch: 020 ----
mean loss: 896.11
 ---- batch: 030 ----
mean loss: 898.65
 ---- batch: 040 ----
mean loss: 876.81
 ---- batch: 050 ----
mean loss: 855.82
 ---- batch: 060 ----
mean loss: 888.17
 ---- batch: 070 ----
mean loss: 874.60
 ---- batch: 080 ----
mean loss: 874.50
 ---- batch: 090 ----
mean loss: 872.36
train mean loss: 877.20
epoch train time: 0:00:01.651503
elapsed time: 0:01:48.309867
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 20:11:00.519719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.65
 ---- batch: 020 ----
mean loss: 872.98
 ---- batch: 030 ----
mean loss: 877.21
 ---- batch: 040 ----
mean loss: 878.63
 ---- batch: 050 ----
mean loss: 879.97
 ---- batch: 060 ----
mean loss: 868.13
 ---- batch: 070 ----
mean loss: 880.41
 ---- batch: 080 ----
mean loss: 876.98
 ---- batch: 090 ----
mean loss: 864.73
train mean loss: 876.29
epoch train time: 0:00:01.696971
elapsed time: 0:01:50.007488
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 20:11:02.217346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.24
 ---- batch: 020 ----
mean loss: 876.41
 ---- batch: 030 ----
mean loss: 887.73
 ---- batch: 040 ----
mean loss: 872.84
 ---- batch: 050 ----
mean loss: 887.41
 ---- batch: 060 ----
mean loss: 880.28
 ---- batch: 070 ----
mean loss: 893.25
 ---- batch: 080 ----
mean loss: 869.53
 ---- batch: 090 ----
mean loss: 873.73
train mean loss: 877.20
epoch train time: 0:00:01.678549
elapsed time: 0:01:51.686760
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 20:11:03.896645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.73
 ---- batch: 020 ----
mean loss: 869.49
 ---- batch: 030 ----
mean loss: 870.62
 ---- batch: 040 ----
mean loss: 883.01
 ---- batch: 050 ----
mean loss: 875.40
 ---- batch: 060 ----
mean loss: 876.80
 ---- batch: 070 ----
mean loss: 874.74
 ---- batch: 080 ----
mean loss: 878.51
 ---- batch: 090 ----
mean loss: 897.08
train mean loss: 876.18
epoch train time: 0:00:01.660142
elapsed time: 0:01:53.347659
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 20:11:05.557644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.48
 ---- batch: 020 ----
mean loss: 876.61
 ---- batch: 030 ----
mean loss: 843.11
 ---- batch: 040 ----
mean loss: 872.53
 ---- batch: 050 ----
mean loss: 880.46
 ---- batch: 060 ----
mean loss: 870.93
 ---- batch: 070 ----
mean loss: 895.16
 ---- batch: 080 ----
mean loss: 875.07
 ---- batch: 090 ----
mean loss: 901.86
train mean loss: 875.84
epoch train time: 0:00:01.632344
elapsed time: 0:01:54.980779
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 20:11:07.190612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.06
 ---- batch: 020 ----
mean loss: 886.87
 ---- batch: 030 ----
mean loss: 870.98
 ---- batch: 040 ----
mean loss: 883.76
 ---- batch: 050 ----
mean loss: 868.81
 ---- batch: 060 ----
mean loss: 870.00
 ---- batch: 070 ----
mean loss: 866.36
 ---- batch: 080 ----
mean loss: 887.37
 ---- batch: 090 ----
mean loss: 881.02
train mean loss: 876.40
epoch train time: 0:00:01.622636
elapsed time: 0:01:56.604146
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 20:11:08.814007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.20
 ---- batch: 020 ----
mean loss: 895.59
 ---- batch: 030 ----
mean loss: 876.12
 ---- batch: 040 ----
mean loss: 888.49
 ---- batch: 050 ----
mean loss: 869.07
 ---- batch: 060 ----
mean loss: 870.72
 ---- batch: 070 ----
mean loss: 874.51
 ---- batch: 080 ----
mean loss: 859.59
 ---- batch: 090 ----
mean loss: 877.34
train mean loss: 876.74
epoch train time: 0:00:01.674712
elapsed time: 0:01:58.279485
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 20:11:10.489430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.71
 ---- batch: 020 ----
mean loss: 874.93
 ---- batch: 030 ----
mean loss: 872.28
 ---- batch: 040 ----
mean loss: 863.57
 ---- batch: 050 ----
mean loss: 870.83
 ---- batch: 060 ----
mean loss: 877.49
 ---- batch: 070 ----
mean loss: 899.48
 ---- batch: 080 ----
mean loss: 874.93
 ---- batch: 090 ----
mean loss: 887.06
train mean loss: 876.32
epoch train time: 0:00:01.663134
elapsed time: 0:01:59.943396
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 20:11:12.153314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.75
 ---- batch: 020 ----
mean loss: 876.11
 ---- batch: 030 ----
mean loss: 866.66
 ---- batch: 040 ----
mean loss: 871.26
 ---- batch: 050 ----
mean loss: 861.83
 ---- batch: 060 ----
mean loss: 857.97
 ---- batch: 070 ----
mean loss: 847.27
 ---- batch: 080 ----
mean loss: 829.76
 ---- batch: 090 ----
mean loss: 771.39
train mean loss: 843.64
epoch train time: 0:00:01.656849
elapsed time: 0:02:01.600968
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 20:11:13.810804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.69
 ---- batch: 020 ----
mean loss: 619.16
 ---- batch: 030 ----
mean loss: 578.86
 ---- batch: 040 ----
mean loss: 533.22
 ---- batch: 050 ----
mean loss: 508.00
 ---- batch: 060 ----
mean loss: 480.79
 ---- batch: 070 ----
mean loss: 484.19
 ---- batch: 080 ----
mean loss: 469.85
 ---- batch: 090 ----
mean loss: 435.92
train mean loss: 526.90
epoch train time: 0:00:01.692677
elapsed time: 0:02:03.294227
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 20:11:15.504097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.75
 ---- batch: 020 ----
mean loss: 422.29
 ---- batch: 030 ----
mean loss: 427.80
 ---- batch: 040 ----
mean loss: 407.68
 ---- batch: 050 ----
mean loss: 389.80
 ---- batch: 060 ----
mean loss: 406.88
 ---- batch: 070 ----
mean loss: 405.81
 ---- batch: 080 ----
mean loss: 381.29
 ---- batch: 090 ----
mean loss: 390.95
train mean loss: 407.04
epoch train time: 0:00:01.650690
elapsed time: 0:02:04.945646
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 20:11:17.155503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.35
 ---- batch: 020 ----
mean loss: 365.46
 ---- batch: 030 ----
mean loss: 371.17
 ---- batch: 040 ----
mean loss: 372.55
 ---- batch: 050 ----
mean loss: 370.98
 ---- batch: 060 ----
mean loss: 360.05
 ---- batch: 070 ----
mean loss: 373.90
 ---- batch: 080 ----
mean loss: 352.92
 ---- batch: 090 ----
mean loss: 342.81
train mean loss: 365.01
epoch train time: 0:00:01.694383
elapsed time: 0:02:06.640736
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 20:11:18.850603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.14
 ---- batch: 020 ----
mean loss: 347.53
 ---- batch: 030 ----
mean loss: 352.96
 ---- batch: 040 ----
mean loss: 346.67
 ---- batch: 050 ----
mean loss: 341.76
 ---- batch: 060 ----
mean loss: 331.77
 ---- batch: 070 ----
mean loss: 332.35
 ---- batch: 080 ----
mean loss: 329.97
 ---- batch: 090 ----
mean loss: 333.85
train mean loss: 339.22
epoch train time: 0:00:01.667278
elapsed time: 0:02:08.308741
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 20:11:20.518594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.37
 ---- batch: 020 ----
mean loss: 332.29
 ---- batch: 030 ----
mean loss: 335.41
 ---- batch: 040 ----
mean loss: 319.95
 ---- batch: 050 ----
mean loss: 337.59
 ---- batch: 060 ----
mean loss: 320.05
 ---- batch: 070 ----
mean loss: 316.86
 ---- batch: 080 ----
mean loss: 314.43
 ---- batch: 090 ----
mean loss: 317.62
train mean loss: 323.68
epoch train time: 0:00:01.664421
elapsed time: 0:02:09.973810
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 20:11:22.183697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.87
 ---- batch: 020 ----
mean loss: 307.47
 ---- batch: 030 ----
mean loss: 309.99
 ---- batch: 040 ----
mean loss: 306.85
 ---- batch: 050 ----
mean loss: 305.78
 ---- batch: 060 ----
mean loss: 305.90
 ---- batch: 070 ----
mean loss: 309.13
 ---- batch: 080 ----
mean loss: 314.00
 ---- batch: 090 ----
mean loss: 312.52
train mean loss: 308.18
epoch train time: 0:00:01.635259
elapsed time: 0:02:11.609823
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 20:11:23.819690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.78
 ---- batch: 020 ----
mean loss: 316.42
 ---- batch: 030 ----
mean loss: 297.82
 ---- batch: 040 ----
mean loss: 297.63
 ---- batch: 050 ----
mean loss: 298.07
 ---- batch: 060 ----
mean loss: 305.62
 ---- batch: 070 ----
mean loss: 295.44
 ---- batch: 080 ----
mean loss: 289.81
 ---- batch: 090 ----
mean loss: 297.02
train mean loss: 299.12
epoch train time: 0:00:01.653795
elapsed time: 0:02:13.264513
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 20:11:25.474393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.00
 ---- batch: 020 ----
mean loss: 289.08
 ---- batch: 030 ----
mean loss: 289.70
 ---- batch: 040 ----
mean loss: 291.46
 ---- batch: 050 ----
mean loss: 282.03
 ---- batch: 060 ----
mean loss: 281.50
 ---- batch: 070 ----
mean loss: 295.90
 ---- batch: 080 ----
mean loss: 296.32
 ---- batch: 090 ----
mean loss: 288.96
train mean loss: 289.05
epoch train time: 0:00:01.635546
elapsed time: 0:02:14.900760
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 20:11:27.110623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.22
 ---- batch: 020 ----
mean loss: 283.85
 ---- batch: 030 ----
mean loss: 285.87
 ---- batch: 040 ----
mean loss: 281.22
 ---- batch: 050 ----
mean loss: 290.79
 ---- batch: 060 ----
mean loss: 287.72
 ---- batch: 070 ----
mean loss: 269.55
 ---- batch: 080 ----
mean loss: 278.82
 ---- batch: 090 ----
mean loss: 293.11
train mean loss: 284.31
epoch train time: 0:00:01.688502
elapsed time: 0:02:16.589903
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 20:11:28.799761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.40
 ---- batch: 020 ----
mean loss: 278.98
 ---- batch: 030 ----
mean loss: 281.49
 ---- batch: 040 ----
mean loss: 261.42
 ---- batch: 050 ----
mean loss: 280.54
 ---- batch: 060 ----
mean loss: 278.09
 ---- batch: 070 ----
mean loss: 272.93
 ---- batch: 080 ----
mean loss: 282.19
 ---- batch: 090 ----
mean loss: 276.77
train mean loss: 277.35
epoch train time: 0:00:01.678319
elapsed time: 0:02:18.268890
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 20:11:30.478800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.21
 ---- batch: 020 ----
mean loss: 278.48
 ---- batch: 030 ----
mean loss: 270.84
 ---- batch: 040 ----
mean loss: 272.60
 ---- batch: 050 ----
mean loss: 287.38
 ---- batch: 060 ----
mean loss: 261.56
 ---- batch: 070 ----
mean loss: 271.11
 ---- batch: 080 ----
mean loss: 270.96
 ---- batch: 090 ----
mean loss: 270.51
train mean loss: 273.10
epoch train time: 0:00:01.641125
elapsed time: 0:02:19.910676
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 20:11:32.120519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.98
 ---- batch: 020 ----
mean loss: 269.41
 ---- batch: 030 ----
mean loss: 271.80
 ---- batch: 040 ----
mean loss: 277.03
 ---- batch: 050 ----
mean loss: 272.00
 ---- batch: 060 ----
mean loss: 267.99
 ---- batch: 070 ----
mean loss: 256.49
 ---- batch: 080 ----
mean loss: 257.38
 ---- batch: 090 ----
mean loss: 268.00
train mean loss: 268.48
epoch train time: 0:00:01.689071
elapsed time: 0:02:21.600507
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 20:11:33.810390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.13
 ---- batch: 020 ----
mean loss: 263.26
 ---- batch: 030 ----
mean loss: 260.90
 ---- batch: 040 ----
mean loss: 261.36
 ---- batch: 050 ----
mean loss: 263.68
 ---- batch: 060 ----
mean loss: 262.66
 ---- batch: 070 ----
mean loss: 260.28
 ---- batch: 080 ----
mean loss: 269.99
 ---- batch: 090 ----
mean loss: 252.36
train mean loss: 261.07
epoch train time: 0:00:01.681158
elapsed time: 0:02:23.282345
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 20:11:35.492229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.49
 ---- batch: 020 ----
mean loss: 248.73
 ---- batch: 030 ----
mean loss: 270.83
 ---- batch: 040 ----
mean loss: 267.00
 ---- batch: 050 ----
mean loss: 259.54
 ---- batch: 060 ----
mean loss: 265.36
 ---- batch: 070 ----
mean loss: 251.21
 ---- batch: 080 ----
mean loss: 254.90
 ---- batch: 090 ----
mean loss: 251.15
train mean loss: 257.59
epoch train time: 0:00:01.692004
elapsed time: 0:02:24.975031
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 20:11:37.184942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.91
 ---- batch: 020 ----
mean loss: 258.48
 ---- batch: 030 ----
mean loss: 250.51
 ---- batch: 040 ----
mean loss: 249.96
 ---- batch: 050 ----
mean loss: 255.98
 ---- batch: 060 ----
mean loss: 248.93
 ---- batch: 070 ----
mean loss: 254.13
 ---- batch: 080 ----
mean loss: 262.53
 ---- batch: 090 ----
mean loss: 242.38
train mean loss: 253.81
epoch train time: 0:00:01.672627
elapsed time: 0:02:26.648410
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 20:11:38.858279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.64
 ---- batch: 020 ----
mean loss: 260.44
 ---- batch: 030 ----
mean loss: 243.27
 ---- batch: 040 ----
mean loss: 248.72
 ---- batch: 050 ----
mean loss: 247.29
 ---- batch: 060 ----
mean loss: 250.77
 ---- batch: 070 ----
mean loss: 259.32
 ---- batch: 080 ----
mean loss: 251.91
 ---- batch: 090 ----
mean loss: 252.94
train mean loss: 251.59
epoch train time: 0:00:01.673109
elapsed time: 0:02:28.322174
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 20:11:40.532013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.76
 ---- batch: 020 ----
mean loss: 238.63
 ---- batch: 030 ----
mean loss: 243.60
 ---- batch: 040 ----
mean loss: 240.50
 ---- batch: 050 ----
mean loss: 244.18
 ---- batch: 060 ----
mean loss: 238.41
 ---- batch: 070 ----
mean loss: 235.75
 ---- batch: 080 ----
mean loss: 255.88
 ---- batch: 090 ----
mean loss: 249.73
train mean loss: 244.86
epoch train time: 0:00:01.634614
elapsed time: 0:02:29.957411
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 20:11:42.167253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.26
 ---- batch: 020 ----
mean loss: 241.41
 ---- batch: 030 ----
mean loss: 239.98
 ---- batch: 040 ----
mean loss: 253.15
 ---- batch: 050 ----
mean loss: 250.39
 ---- batch: 060 ----
mean loss: 233.43
 ---- batch: 070 ----
mean loss: 249.12
 ---- batch: 080 ----
mean loss: 238.93
 ---- batch: 090 ----
mean loss: 241.39
train mean loss: 242.98
epoch train time: 0:00:01.653514
elapsed time: 0:02:31.611637
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 20:11:43.821708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.23
 ---- batch: 020 ----
mean loss: 236.10
 ---- batch: 030 ----
mean loss: 239.99
 ---- batch: 040 ----
mean loss: 244.31
 ---- batch: 050 ----
mean loss: 241.75
 ---- batch: 060 ----
mean loss: 248.15
 ---- batch: 070 ----
mean loss: 236.32
 ---- batch: 080 ----
mean loss: 237.91
 ---- batch: 090 ----
mean loss: 242.40
train mean loss: 240.76
epoch train time: 0:00:01.674274
elapsed time: 0:02:33.286763
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 20:11:45.496631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.80
 ---- batch: 020 ----
mean loss: 220.39
 ---- batch: 030 ----
mean loss: 241.56
 ---- batch: 040 ----
mean loss: 237.26
 ---- batch: 050 ----
mean loss: 246.70
 ---- batch: 060 ----
mean loss: 235.70
 ---- batch: 070 ----
mean loss: 240.04
 ---- batch: 080 ----
mean loss: 242.40
 ---- batch: 090 ----
mean loss: 240.42
train mean loss: 236.44
epoch train time: 0:00:01.675822
elapsed time: 0:02:34.963346
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 20:11:47.173273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.05
 ---- batch: 020 ----
mean loss: 236.46
 ---- batch: 030 ----
mean loss: 238.16
 ---- batch: 040 ----
mean loss: 231.89
 ---- batch: 050 ----
mean loss: 228.87
 ---- batch: 060 ----
mean loss: 230.67
 ---- batch: 070 ----
mean loss: 234.11
 ---- batch: 080 ----
mean loss: 233.45
 ---- batch: 090 ----
mean loss: 236.06
train mean loss: 233.05
epoch train time: 0:00:01.684621
elapsed time: 0:02:36.648719
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 20:11:48.858570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.64
 ---- batch: 020 ----
mean loss: 227.07
 ---- batch: 030 ----
mean loss: 231.96
 ---- batch: 040 ----
mean loss: 234.36
 ---- batch: 050 ----
mean loss: 230.69
 ---- batch: 060 ----
mean loss: 236.55
 ---- batch: 070 ----
mean loss: 228.42
 ---- batch: 080 ----
mean loss: 230.29
 ---- batch: 090 ----
mean loss: 230.80
train mean loss: 230.98
epoch train time: 0:00:01.657127
elapsed time: 0:02:38.306556
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 20:11:50.516415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.41
 ---- batch: 020 ----
mean loss: 228.12
 ---- batch: 030 ----
mean loss: 232.59
 ---- batch: 040 ----
mean loss: 233.38
 ---- batch: 050 ----
mean loss: 223.72
 ---- batch: 060 ----
mean loss: 227.43
 ---- batch: 070 ----
mean loss: 234.78
 ---- batch: 080 ----
mean loss: 230.52
 ---- batch: 090 ----
mean loss: 228.86
train mean loss: 229.55
epoch train time: 0:00:01.653162
elapsed time: 0:02:39.960322
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 20:11:52.170152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.79
 ---- batch: 020 ----
mean loss: 223.29
 ---- batch: 030 ----
mean loss: 217.97
 ---- batch: 040 ----
mean loss: 222.74
 ---- batch: 050 ----
mean loss: 230.75
 ---- batch: 060 ----
mean loss: 227.77
 ---- batch: 070 ----
mean loss: 227.97
 ---- batch: 080 ----
mean loss: 225.29
 ---- batch: 090 ----
mean loss: 224.11
train mean loss: 225.48
epoch train time: 0:00:01.650711
elapsed time: 0:02:41.611689
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 20:11:53.821535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.14
 ---- batch: 020 ----
mean loss: 225.41
 ---- batch: 030 ----
mean loss: 218.02
 ---- batch: 040 ----
mean loss: 226.10
 ---- batch: 050 ----
mean loss: 215.17
 ---- batch: 060 ----
mean loss: 219.14
 ---- batch: 070 ----
mean loss: 228.86
 ---- batch: 080 ----
mean loss: 227.41
 ---- batch: 090 ----
mean loss: 220.90
train mean loss: 222.63
epoch train time: 0:00:01.657484
elapsed time: 0:02:43.269802
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 20:11:55.479688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.70
 ---- batch: 020 ----
mean loss: 217.28
 ---- batch: 030 ----
mean loss: 220.15
 ---- batch: 040 ----
mean loss: 235.61
 ---- batch: 050 ----
mean loss: 221.45
 ---- batch: 060 ----
mean loss: 217.32
 ---- batch: 070 ----
mean loss: 227.37
 ---- batch: 080 ----
mean loss: 223.10
 ---- batch: 090 ----
mean loss: 221.05
train mean loss: 220.79
epoch train time: 0:00:01.658887
elapsed time: 0:02:44.929356
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 20:11:57.139206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.55
 ---- batch: 020 ----
mean loss: 215.55
 ---- batch: 030 ----
mean loss: 218.02
 ---- batch: 040 ----
mean loss: 230.58
 ---- batch: 050 ----
mean loss: 215.37
 ---- batch: 060 ----
mean loss: 224.09
 ---- batch: 070 ----
mean loss: 218.50
 ---- batch: 080 ----
mean loss: 218.72
 ---- batch: 090 ----
mean loss: 217.43
train mean loss: 218.56
epoch train time: 0:00:01.663786
elapsed time: 0:02:46.593808
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 20:11:58.803705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.73
 ---- batch: 020 ----
mean loss: 210.06
 ---- batch: 030 ----
mean loss: 208.78
 ---- batch: 040 ----
mean loss: 213.68
 ---- batch: 050 ----
mean loss: 217.85
 ---- batch: 060 ----
mean loss: 212.57
 ---- batch: 070 ----
mean loss: 214.35
 ---- batch: 080 ----
mean loss: 216.25
 ---- batch: 090 ----
mean loss: 222.95
train mean loss: 215.52
epoch train time: 0:00:01.646887
elapsed time: 0:02:48.241348
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 20:12:00.451180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.60
 ---- batch: 020 ----
mean loss: 216.36
 ---- batch: 030 ----
mean loss: 218.70
 ---- batch: 040 ----
mean loss: 212.46
 ---- batch: 050 ----
mean loss: 219.38
 ---- batch: 060 ----
mean loss: 208.89
 ---- batch: 070 ----
mean loss: 207.01
 ---- batch: 080 ----
mean loss: 208.15
 ---- batch: 090 ----
mean loss: 214.37
train mean loss: 212.34
epoch train time: 0:00:01.638300
elapsed time: 0:02:49.880324
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 20:12:02.090222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.36
 ---- batch: 020 ----
mean loss: 207.05
 ---- batch: 030 ----
mean loss: 203.92
 ---- batch: 040 ----
mean loss: 210.62
 ---- batch: 050 ----
mean loss: 219.05
 ---- batch: 060 ----
mean loss: 214.47
 ---- batch: 070 ----
mean loss: 208.93
 ---- batch: 080 ----
mean loss: 216.90
 ---- batch: 090 ----
mean loss: 208.71
train mean loss: 211.36
epoch train time: 0:00:01.653920
elapsed time: 0:02:51.534979
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 20:12:03.744824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.84
 ---- batch: 020 ----
mean loss: 197.71
 ---- batch: 030 ----
mean loss: 203.51
 ---- batch: 040 ----
mean loss: 205.98
 ---- batch: 050 ----
mean loss: 210.12
 ---- batch: 060 ----
mean loss: 212.91
 ---- batch: 070 ----
mean loss: 209.48
 ---- batch: 080 ----
mean loss: 208.34
 ---- batch: 090 ----
mean loss: 213.58
train mean loss: 207.93
epoch train time: 0:00:01.663221
elapsed time: 0:02:53.198826
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 20:12:05.408694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.90
 ---- batch: 020 ----
mean loss: 200.38
 ---- batch: 030 ----
mean loss: 202.88
 ---- batch: 040 ----
mean loss: 205.00
 ---- batch: 050 ----
mean loss: 208.79
 ---- batch: 060 ----
mean loss: 208.10
 ---- batch: 070 ----
mean loss: 199.32
 ---- batch: 080 ----
mean loss: 205.51
 ---- batch: 090 ----
mean loss: 218.74
train mean loss: 206.39
epoch train time: 0:00:01.627804
elapsed time: 0:02:54.827328
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 20:12:07.037207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.45
 ---- batch: 020 ----
mean loss: 208.58
 ---- batch: 030 ----
mean loss: 204.44
 ---- batch: 040 ----
mean loss: 214.37
 ---- batch: 050 ----
mean loss: 205.70
 ---- batch: 060 ----
mean loss: 195.40
 ---- batch: 070 ----
mean loss: 190.79
 ---- batch: 080 ----
mean loss: 206.30
 ---- batch: 090 ----
mean loss: 208.30
train mean loss: 205.11
epoch train time: 0:00:01.658533
elapsed time: 0:02:56.486531
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 20:12:08.696227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.66
 ---- batch: 020 ----
mean loss: 199.00
 ---- batch: 030 ----
mean loss: 207.27
 ---- batch: 040 ----
mean loss: 208.66
 ---- batch: 050 ----
mean loss: 196.98
 ---- batch: 060 ----
mean loss: 203.99
 ---- batch: 070 ----
mean loss: 202.36
 ---- batch: 080 ----
mean loss: 208.14
 ---- batch: 090 ----
mean loss: 197.99
train mean loss: 203.07
epoch train time: 0:00:01.687954
elapsed time: 0:02:58.175067
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 20:12:10.384994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.63
 ---- batch: 020 ----
mean loss: 197.05
 ---- batch: 030 ----
mean loss: 199.89
 ---- batch: 040 ----
mean loss: 199.93
 ---- batch: 050 ----
mean loss: 190.51
 ---- batch: 060 ----
mean loss: 203.88
 ---- batch: 070 ----
mean loss: 200.17
 ---- batch: 080 ----
mean loss: 203.34
 ---- batch: 090 ----
mean loss: 209.53
train mean loss: 200.92
epoch train time: 0:00:01.655156
elapsed time: 0:02:59.830980
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 20:12:12.040828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.91
 ---- batch: 020 ----
mean loss: 196.28
 ---- batch: 030 ----
mean loss: 193.85
 ---- batch: 040 ----
mean loss: 197.92
 ---- batch: 050 ----
mean loss: 193.37
 ---- batch: 060 ----
mean loss: 204.29
 ---- batch: 070 ----
mean loss: 197.95
 ---- batch: 080 ----
mean loss: 200.38
 ---- batch: 090 ----
mean loss: 203.49
train mean loss: 199.03
epoch train time: 0:00:01.654366
elapsed time: 0:03:01.486016
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 20:12:13.695876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.76
 ---- batch: 020 ----
mean loss: 197.56
 ---- batch: 030 ----
mean loss: 195.63
 ---- batch: 040 ----
mean loss: 195.08
 ---- batch: 050 ----
mean loss: 199.44
 ---- batch: 060 ----
mean loss: 191.52
 ---- batch: 070 ----
mean loss: 198.12
 ---- batch: 080 ----
mean loss: 200.27
 ---- batch: 090 ----
mean loss: 198.51
train mean loss: 196.69
epoch train time: 0:00:01.653414
elapsed time: 0:03:03.140102
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 20:12:15.349969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.11
 ---- batch: 020 ----
mean loss: 191.51
 ---- batch: 030 ----
mean loss: 186.51
 ---- batch: 040 ----
mean loss: 191.89
 ---- batch: 050 ----
mean loss: 199.43
 ---- batch: 060 ----
mean loss: 205.91
 ---- batch: 070 ----
mean loss: 190.28
 ---- batch: 080 ----
mean loss: 201.17
 ---- batch: 090 ----
mean loss: 195.04
train mean loss: 194.74
epoch train time: 0:00:01.643404
elapsed time: 0:03:04.784252
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 20:12:16.994099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.13
 ---- batch: 020 ----
mean loss: 197.04
 ---- batch: 030 ----
mean loss: 189.26
 ---- batch: 040 ----
mean loss: 190.51
 ---- batch: 050 ----
mean loss: 194.39
 ---- batch: 060 ----
mean loss: 199.06
 ---- batch: 070 ----
mean loss: 192.13
 ---- batch: 080 ----
mean loss: 200.85
 ---- batch: 090 ----
mean loss: 191.00
train mean loss: 193.83
epoch train time: 0:00:01.627652
elapsed time: 0:03:06.412535
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 20:12:18.622394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.29
 ---- batch: 020 ----
mean loss: 195.33
 ---- batch: 030 ----
mean loss: 192.43
 ---- batch: 040 ----
mean loss: 194.45
 ---- batch: 050 ----
mean loss: 182.65
 ---- batch: 060 ----
mean loss: 196.15
 ---- batch: 070 ----
mean loss: 189.21
 ---- batch: 080 ----
mean loss: 195.58
 ---- batch: 090 ----
mean loss: 192.48
train mean loss: 191.88
epoch train time: 0:00:01.678977
elapsed time: 0:03:08.092172
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 20:12:20.302114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.11
 ---- batch: 020 ----
mean loss: 192.41
 ---- batch: 030 ----
mean loss: 192.85
 ---- batch: 040 ----
mean loss: 190.47
 ---- batch: 050 ----
mean loss: 192.03
 ---- batch: 060 ----
mean loss: 194.48
 ---- batch: 070 ----
mean loss: 197.66
 ---- batch: 080 ----
mean loss: 188.35
 ---- batch: 090 ----
mean loss: 181.78
train mean loss: 190.07
epoch train time: 0:00:01.677334
elapsed time: 0:03:09.770218
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 20:12:21.980062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.24
 ---- batch: 020 ----
mean loss: 174.45
 ---- batch: 030 ----
mean loss: 200.00
 ---- batch: 040 ----
mean loss: 182.31
 ---- batch: 050 ----
mean loss: 184.91
 ---- batch: 060 ----
mean loss: 196.07
 ---- batch: 070 ----
mean loss: 186.75
 ---- batch: 080 ----
mean loss: 188.14
 ---- batch: 090 ----
mean loss: 199.33
train mean loss: 188.24
epoch train time: 0:00:01.661016
elapsed time: 0:03:11.432012
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 20:12:23.641881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.33
 ---- batch: 020 ----
mean loss: 183.32
 ---- batch: 030 ----
mean loss: 183.77
 ---- batch: 040 ----
mean loss: 183.78
 ---- batch: 050 ----
mean loss: 192.10
 ---- batch: 060 ----
mean loss: 188.21
 ---- batch: 070 ----
mean loss: 177.93
 ---- batch: 080 ----
mean loss: 185.79
 ---- batch: 090 ----
mean loss: 189.32
train mean loss: 186.47
epoch train time: 0:00:01.637964
elapsed time: 0:03:13.070636
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 20:12:25.280485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.12
 ---- batch: 020 ----
mean loss: 174.31
 ---- batch: 030 ----
mean loss: 178.55
 ---- batch: 040 ----
mean loss: 186.36
 ---- batch: 050 ----
mean loss: 180.31
 ---- batch: 060 ----
mean loss: 190.96
 ---- batch: 070 ----
mean loss: 190.85
 ---- batch: 080 ----
mean loss: 190.64
 ---- batch: 090 ----
mean loss: 185.05
train mean loss: 184.79
epoch train time: 0:00:01.645500
elapsed time: 0:03:14.716805
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 20:12:26.926659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.79
 ---- batch: 020 ----
mean loss: 179.11
 ---- batch: 030 ----
mean loss: 180.55
 ---- batch: 040 ----
mean loss: 186.24
 ---- batch: 050 ----
mean loss: 182.52
 ---- batch: 060 ----
mean loss: 182.72
 ---- batch: 070 ----
mean loss: 189.98
 ---- batch: 080 ----
mean loss: 182.52
 ---- batch: 090 ----
mean loss: 185.67
train mean loss: 183.85
epoch train time: 0:00:01.654255
elapsed time: 0:03:16.371813
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 20:12:28.581715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.54
 ---- batch: 020 ----
mean loss: 182.01
 ---- batch: 030 ----
mean loss: 175.58
 ---- batch: 040 ----
mean loss: 178.30
 ---- batch: 050 ----
mean loss: 182.18
 ---- batch: 060 ----
mean loss: 187.15
 ---- batch: 070 ----
mean loss: 183.95
 ---- batch: 080 ----
mean loss: 183.24
 ---- batch: 090 ----
mean loss: 184.43
train mean loss: 182.08
epoch train time: 0:00:01.659568
elapsed time: 0:03:18.032166
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 20:12:30.242078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.91
 ---- batch: 020 ----
mean loss: 167.99
 ---- batch: 030 ----
mean loss: 175.63
 ---- batch: 040 ----
mean loss: 176.34
 ---- batch: 050 ----
mean loss: 187.51
 ---- batch: 060 ----
mean loss: 185.31
 ---- batch: 070 ----
mean loss: 174.27
 ---- batch: 080 ----
mean loss: 185.32
 ---- batch: 090 ----
mean loss: 186.17
train mean loss: 179.97
epoch train time: 0:00:01.656124
elapsed time: 0:03:19.689020
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 20:12:31.898875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.49
 ---- batch: 020 ----
mean loss: 175.05
 ---- batch: 030 ----
mean loss: 171.46
 ---- batch: 040 ----
mean loss: 179.12
 ---- batch: 050 ----
mean loss: 184.40
 ---- batch: 060 ----
mean loss: 176.30
 ---- batch: 070 ----
mean loss: 176.45
 ---- batch: 080 ----
mean loss: 184.31
 ---- batch: 090 ----
mean loss: 176.83
train mean loss: 178.77
epoch train time: 0:00:01.653948
elapsed time: 0:03:21.343607
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 20:12:33.553484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.24
 ---- batch: 020 ----
mean loss: 177.45
 ---- batch: 030 ----
mean loss: 176.33
 ---- batch: 040 ----
mean loss: 178.49
 ---- batch: 050 ----
mean loss: 186.44
 ---- batch: 060 ----
mean loss: 181.81
 ---- batch: 070 ----
mean loss: 174.50
 ---- batch: 080 ----
mean loss: 177.83
 ---- batch: 090 ----
mean loss: 176.43
train mean loss: 178.71
epoch train time: 0:00:01.651393
elapsed time: 0:03:22.995663
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 20:12:35.205537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.30
 ---- batch: 020 ----
mean loss: 174.31
 ---- batch: 030 ----
mean loss: 173.75
 ---- batch: 040 ----
mean loss: 174.05
 ---- batch: 050 ----
mean loss: 175.37
 ---- batch: 060 ----
mean loss: 179.90
 ---- batch: 070 ----
mean loss: 175.26
 ---- batch: 080 ----
mean loss: 174.56
 ---- batch: 090 ----
mean loss: 181.31
train mean loss: 176.22
epoch train time: 0:00:01.647697
elapsed time: 0:03:24.644097
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 20:12:36.853951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.96
 ---- batch: 020 ----
mean loss: 173.64
 ---- batch: 030 ----
mean loss: 166.23
 ---- batch: 040 ----
mean loss: 170.68
 ---- batch: 050 ----
mean loss: 174.54
 ---- batch: 060 ----
mean loss: 175.39
 ---- batch: 070 ----
mean loss: 177.68
 ---- batch: 080 ----
mean loss: 173.47
 ---- batch: 090 ----
mean loss: 178.53
train mean loss: 174.94
epoch train time: 0:00:01.664637
elapsed time: 0:03:26.309534
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 20:12:38.519385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.81
 ---- batch: 020 ----
mean loss: 177.35
 ---- batch: 030 ----
mean loss: 169.23
 ---- batch: 040 ----
mean loss: 175.84
 ---- batch: 050 ----
mean loss: 174.24
 ---- batch: 060 ----
mean loss: 178.60
 ---- batch: 070 ----
mean loss: 168.68
 ---- batch: 080 ----
mean loss: 167.74
 ---- batch: 090 ----
mean loss: 178.40
train mean loss: 174.25
epoch train time: 0:00:01.662910
elapsed time: 0:03:27.973070
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 20:12:40.182954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.64
 ---- batch: 020 ----
mean loss: 166.99
 ---- batch: 030 ----
mean loss: 179.69
 ---- batch: 040 ----
mean loss: 168.43
 ---- batch: 050 ----
mean loss: 166.99
 ---- batch: 060 ----
mean loss: 180.86
 ---- batch: 070 ----
mean loss: 172.85
 ---- batch: 080 ----
mean loss: 170.50
 ---- batch: 090 ----
mean loss: 171.46
train mean loss: 172.24
epoch train time: 0:00:01.651203
elapsed time: 0:03:29.625034
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 20:12:41.834928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.63
 ---- batch: 020 ----
mean loss: 173.07
 ---- batch: 030 ----
mean loss: 167.14
 ---- batch: 040 ----
mean loss: 170.26
 ---- batch: 050 ----
mean loss: 173.20
 ---- batch: 060 ----
mean loss: 175.54
 ---- batch: 070 ----
mean loss: 171.11
 ---- batch: 080 ----
mean loss: 168.93
 ---- batch: 090 ----
mean loss: 169.63
train mean loss: 170.68
epoch train time: 0:00:01.647203
elapsed time: 0:03:31.273121
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 20:12:43.482807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.25
 ---- batch: 020 ----
mean loss: 170.32
 ---- batch: 030 ----
mean loss: 163.71
 ---- batch: 040 ----
mean loss: 168.43
 ---- batch: 050 ----
mean loss: 167.80
 ---- batch: 060 ----
mean loss: 168.18
 ---- batch: 070 ----
mean loss: 175.97
 ---- batch: 080 ----
mean loss: 172.36
 ---- batch: 090 ----
mean loss: 173.34
train mean loss: 168.86
epoch train time: 0:00:01.654505
elapsed time: 0:03:32.928136
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 20:12:45.138038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.70
 ---- batch: 020 ----
mean loss: 171.55
 ---- batch: 030 ----
mean loss: 161.63
 ---- batch: 040 ----
mean loss: 168.98
 ---- batch: 050 ----
mean loss: 166.67
 ---- batch: 060 ----
mean loss: 168.72
 ---- batch: 070 ----
mean loss: 167.81
 ---- batch: 080 ----
mean loss: 170.09
 ---- batch: 090 ----
mean loss: 166.88
train mean loss: 168.22
epoch train time: 0:00:01.641296
elapsed time: 0:03:34.570143
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 20:12:46.780020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.88
 ---- batch: 020 ----
mean loss: 164.51
 ---- batch: 030 ----
mean loss: 167.91
 ---- batch: 040 ----
mean loss: 162.32
 ---- batch: 050 ----
mean loss: 163.37
 ---- batch: 060 ----
mean loss: 171.14
 ---- batch: 070 ----
mean loss: 169.60
 ---- batch: 080 ----
mean loss: 170.97
 ---- batch: 090 ----
mean loss: 170.42
train mean loss: 168.16
epoch train time: 0:00:01.652390
elapsed time: 0:03:36.223180
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 20:12:48.433024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.21
 ---- batch: 020 ----
mean loss: 167.88
 ---- batch: 030 ----
mean loss: 161.28
 ---- batch: 040 ----
mean loss: 163.02
 ---- batch: 050 ----
mean loss: 169.58
 ---- batch: 060 ----
mean loss: 167.32
 ---- batch: 070 ----
mean loss: 169.88
 ---- batch: 080 ----
mean loss: 166.57
 ---- batch: 090 ----
mean loss: 170.35
train mean loss: 167.28
epoch train time: 0:00:01.643661
elapsed time: 0:03:37.867465
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 20:12:50.077312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.29
 ---- batch: 020 ----
mean loss: 154.70
 ---- batch: 030 ----
mean loss: 165.15
 ---- batch: 040 ----
mean loss: 169.58
 ---- batch: 050 ----
mean loss: 168.27
 ---- batch: 060 ----
mean loss: 165.21
 ---- batch: 070 ----
mean loss: 170.60
 ---- batch: 080 ----
mean loss: 165.36
 ---- batch: 090 ----
mean loss: 166.03
train mean loss: 164.79
epoch train time: 0:00:01.638225
elapsed time: 0:03:39.506384
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 20:12:51.716250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.55
 ---- batch: 020 ----
mean loss: 163.63
 ---- batch: 030 ----
mean loss: 161.07
 ---- batch: 040 ----
mean loss: 162.61
 ---- batch: 050 ----
mean loss: 166.27
 ---- batch: 060 ----
mean loss: 165.51
 ---- batch: 070 ----
mean loss: 170.88
 ---- batch: 080 ----
mean loss: 162.45
 ---- batch: 090 ----
mean loss: 164.95
train mean loss: 164.49
epoch train time: 0:00:01.646706
elapsed time: 0:03:41.153681
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 20:12:53.363546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.00
 ---- batch: 020 ----
mean loss: 156.90
 ---- batch: 030 ----
mean loss: 162.66
 ---- batch: 040 ----
mean loss: 159.91
 ---- batch: 050 ----
mean loss: 166.12
 ---- batch: 060 ----
mean loss: 170.76
 ---- batch: 070 ----
mean loss: 157.67
 ---- batch: 080 ----
mean loss: 163.62
 ---- batch: 090 ----
mean loss: 163.53
train mean loss: 162.99
epoch train time: 0:00:01.640374
elapsed time: 0:03:42.794769
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 20:12:55.004622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.23
 ---- batch: 020 ----
mean loss: 156.66
 ---- batch: 030 ----
mean loss: 158.72
 ---- batch: 040 ----
mean loss: 160.06
 ---- batch: 050 ----
mean loss: 158.46
 ---- batch: 060 ----
mean loss: 170.19
 ---- batch: 070 ----
mean loss: 169.18
 ---- batch: 080 ----
mean loss: 158.16
 ---- batch: 090 ----
mean loss: 164.27
train mean loss: 162.21
epoch train time: 0:00:01.647633
elapsed time: 0:03:44.443029
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 20:12:56.652854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.99
 ---- batch: 020 ----
mean loss: 160.77
 ---- batch: 030 ----
mean loss: 159.41
 ---- batch: 040 ----
mean loss: 154.45
 ---- batch: 050 ----
mean loss: 163.85
 ---- batch: 060 ----
mean loss: 159.82
 ---- batch: 070 ----
mean loss: 165.78
 ---- batch: 080 ----
mean loss: 162.52
 ---- batch: 090 ----
mean loss: 171.09
train mean loss: 161.24
epoch train time: 0:00:01.654703
elapsed time: 0:03:46.098286
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 20:12:58.308188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.45
 ---- batch: 020 ----
mean loss: 148.07
 ---- batch: 030 ----
mean loss: 158.98
 ---- batch: 040 ----
mean loss: 157.55
 ---- batch: 050 ----
mean loss: 159.69
 ---- batch: 060 ----
mean loss: 162.20
 ---- batch: 070 ----
mean loss: 165.61
 ---- batch: 080 ----
mean loss: 167.32
 ---- batch: 090 ----
mean loss: 168.40
train mean loss: 159.31
epoch train time: 0:00:01.631104
elapsed time: 0:03:47.730089
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 20:12:59.939941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.13
 ---- batch: 020 ----
mean loss: 151.33
 ---- batch: 030 ----
mean loss: 160.04
 ---- batch: 040 ----
mean loss: 159.54
 ---- batch: 050 ----
mean loss: 162.53
 ---- batch: 060 ----
mean loss: 154.24
 ---- batch: 070 ----
mean loss: 165.18
 ---- batch: 080 ----
mean loss: 163.24
 ---- batch: 090 ----
mean loss: 166.47
train mean loss: 159.71
epoch train time: 0:00:01.656505
elapsed time: 0:03:49.387204
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 20:13:01.597061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.18
 ---- batch: 020 ----
mean loss: 159.04
 ---- batch: 030 ----
mean loss: 161.64
 ---- batch: 040 ----
mean loss: 164.21
 ---- batch: 050 ----
mean loss: 156.31
 ---- batch: 060 ----
mean loss: 161.58
 ---- batch: 070 ----
mean loss: 155.94
 ---- batch: 080 ----
mean loss: 159.99
 ---- batch: 090 ----
mean loss: 153.82
train mean loss: 158.10
epoch train time: 0:00:01.649996
elapsed time: 0:03:51.037791
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 20:13:03.247650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.85
 ---- batch: 020 ----
mean loss: 155.09
 ---- batch: 030 ----
mean loss: 154.99
 ---- batch: 040 ----
mean loss: 157.55
 ---- batch: 050 ----
mean loss: 155.57
 ---- batch: 060 ----
mean loss: 162.20
 ---- batch: 070 ----
mean loss: 156.41
 ---- batch: 080 ----
mean loss: 163.08
 ---- batch: 090 ----
mean loss: 154.20
train mean loss: 157.24
epoch train time: 0:00:01.666267
elapsed time: 0:03:52.704787
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 20:13:04.914632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.78
 ---- batch: 020 ----
mean loss: 150.54
 ---- batch: 030 ----
mean loss: 153.35
 ---- batch: 040 ----
mean loss: 159.58
 ---- batch: 050 ----
mean loss: 159.71
 ---- batch: 060 ----
mean loss: 155.58
 ---- batch: 070 ----
mean loss: 153.03
 ---- batch: 080 ----
mean loss: 162.94
 ---- batch: 090 ----
mean loss: 157.95
train mean loss: 155.87
epoch train time: 0:00:01.632088
elapsed time: 0:03:54.337543
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 20:13:06.547381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.72
 ---- batch: 020 ----
mean loss: 151.99
 ---- batch: 030 ----
mean loss: 151.81
 ---- batch: 040 ----
mean loss: 153.52
 ---- batch: 050 ----
mean loss: 160.79
 ---- batch: 060 ----
mean loss: 156.65
 ---- batch: 070 ----
mean loss: 156.71
 ---- batch: 080 ----
mean loss: 158.67
 ---- batch: 090 ----
mean loss: 154.33
train mean loss: 154.64
epoch train time: 0:00:01.666464
elapsed time: 0:03:56.004709
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 20:13:08.214561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.77
 ---- batch: 020 ----
mean loss: 153.12
 ---- batch: 030 ----
mean loss: 151.47
 ---- batch: 040 ----
mean loss: 156.52
 ---- batch: 050 ----
mean loss: 159.34
 ---- batch: 060 ----
mean loss: 153.95
 ---- batch: 070 ----
mean loss: 154.28
 ---- batch: 080 ----
mean loss: 156.84
 ---- batch: 090 ----
mean loss: 155.34
train mean loss: 155.09
epoch train time: 0:00:01.650862
elapsed time: 0:03:57.656257
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 20:13:09.866102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.37
 ---- batch: 020 ----
mean loss: 147.87
 ---- batch: 030 ----
mean loss: 150.01
 ---- batch: 040 ----
mean loss: 159.87
 ---- batch: 050 ----
mean loss: 156.51
 ---- batch: 060 ----
mean loss: 155.23
 ---- batch: 070 ----
mean loss: 158.51
 ---- batch: 080 ----
mean loss: 158.63
 ---- batch: 090 ----
mean loss: 155.33
train mean loss: 154.54
epoch train time: 0:00:01.620256
elapsed time: 0:03:59.277200
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 20:13:11.487149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.12
 ---- batch: 020 ----
mean loss: 153.03
 ---- batch: 030 ----
mean loss: 147.10
 ---- batch: 040 ----
mean loss: 151.98
 ---- batch: 050 ----
mean loss: 147.12
 ---- batch: 060 ----
mean loss: 162.68
 ---- batch: 070 ----
mean loss: 155.73
 ---- batch: 080 ----
mean loss: 152.04
 ---- batch: 090 ----
mean loss: 151.74
train mean loss: 153.09
epoch train time: 0:00:01.643255
elapsed time: 0:04:00.921192
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 20:13:13.131046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.43
 ---- batch: 020 ----
mean loss: 146.03
 ---- batch: 030 ----
mean loss: 146.77
 ---- batch: 040 ----
mean loss: 146.66
 ---- batch: 050 ----
mean loss: 157.71
 ---- batch: 060 ----
mean loss: 148.81
 ---- batch: 070 ----
mean loss: 153.92
 ---- batch: 080 ----
mean loss: 151.02
 ---- batch: 090 ----
mean loss: 163.36
train mean loss: 151.18
epoch train time: 0:00:01.662096
elapsed time: 0:04:02.583937
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 20:13:14.793809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.95
 ---- batch: 020 ----
mean loss: 147.82
 ---- batch: 030 ----
mean loss: 148.83
 ---- batch: 040 ----
mean loss: 154.09
 ---- batch: 050 ----
mean loss: 147.24
 ---- batch: 060 ----
mean loss: 155.00
 ---- batch: 070 ----
mean loss: 151.89
 ---- batch: 080 ----
mean loss: 149.41
 ---- batch: 090 ----
mean loss: 158.15
train mean loss: 150.38
epoch train time: 0:00:01.636932
elapsed time: 0:04:04.221689
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 20:13:16.431362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.56
 ---- batch: 020 ----
mean loss: 150.10
 ---- batch: 030 ----
mean loss: 141.83
 ---- batch: 040 ----
mean loss: 145.98
 ---- batch: 050 ----
mean loss: 158.35
 ---- batch: 060 ----
mean loss: 153.33
 ---- batch: 070 ----
mean loss: 148.62
 ---- batch: 080 ----
mean loss: 149.81
 ---- batch: 090 ----
mean loss: 161.94
train mean loss: 150.29
epoch train time: 0:00:01.669480
elapsed time: 0:04:05.891618
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 20:13:18.101486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.89
 ---- batch: 020 ----
mean loss: 142.32
 ---- batch: 030 ----
mean loss: 140.77
 ---- batch: 040 ----
mean loss: 147.18
 ---- batch: 050 ----
mean loss: 150.52
 ---- batch: 060 ----
mean loss: 149.68
 ---- batch: 070 ----
mean loss: 156.54
 ---- batch: 080 ----
mean loss: 159.17
 ---- batch: 090 ----
mean loss: 155.17
train mean loss: 149.75
epoch train time: 0:00:01.642972
elapsed time: 0:04:07.535257
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 20:13:19.745118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.32
 ---- batch: 020 ----
mean loss: 146.22
 ---- batch: 030 ----
mean loss: 144.75
 ---- batch: 040 ----
mean loss: 143.57
 ---- batch: 050 ----
mean loss: 148.00
 ---- batch: 060 ----
mean loss: 146.29
 ---- batch: 070 ----
mean loss: 145.67
 ---- batch: 080 ----
mean loss: 156.88
 ---- batch: 090 ----
mean loss: 154.71
train mean loss: 148.46
epoch train time: 0:00:01.668126
elapsed time: 0:04:09.203993
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 20:13:21.413849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.75
 ---- batch: 020 ----
mean loss: 150.70
 ---- batch: 030 ----
mean loss: 141.67
 ---- batch: 040 ----
mean loss: 142.48
 ---- batch: 050 ----
mean loss: 143.92
 ---- batch: 060 ----
mean loss: 145.01
 ---- batch: 070 ----
mean loss: 153.03
 ---- batch: 080 ----
mean loss: 146.98
 ---- batch: 090 ----
mean loss: 156.12
train mean loss: 147.35
epoch train time: 0:00:01.704868
elapsed time: 0:04:10.909523
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 20:13:23.119430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.55
 ---- batch: 020 ----
mean loss: 142.45
 ---- batch: 030 ----
mean loss: 150.62
 ---- batch: 040 ----
mean loss: 141.62
 ---- batch: 050 ----
mean loss: 146.81
 ---- batch: 060 ----
mean loss: 145.88
 ---- batch: 070 ----
mean loss: 146.63
 ---- batch: 080 ----
mean loss: 149.87
 ---- batch: 090 ----
mean loss: 149.89
train mean loss: 146.61
epoch train time: 0:00:01.654434
elapsed time: 0:04:12.564660
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 20:13:24.774531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.93
 ---- batch: 020 ----
mean loss: 141.11
 ---- batch: 030 ----
mean loss: 144.42
 ---- batch: 040 ----
mean loss: 143.39
 ---- batch: 050 ----
mean loss: 141.88
 ---- batch: 060 ----
mean loss: 152.79
 ---- batch: 070 ----
mean loss: 145.69
 ---- batch: 080 ----
mean loss: 148.36
 ---- batch: 090 ----
mean loss: 146.72
train mean loss: 145.85
epoch train time: 0:00:01.678847
elapsed time: 0:04:14.244150
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 20:13:26.454009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.47
 ---- batch: 020 ----
mean loss: 134.55
 ---- batch: 030 ----
mean loss: 141.16
 ---- batch: 040 ----
mean loss: 140.43
 ---- batch: 050 ----
mean loss: 146.13
 ---- batch: 060 ----
mean loss: 148.73
 ---- batch: 070 ----
mean loss: 157.48
 ---- batch: 080 ----
mean loss: 150.17
 ---- batch: 090 ----
mean loss: 150.00
train mean loss: 145.63
epoch train time: 0:00:01.677519
elapsed time: 0:04:15.922270
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 20:13:28.132174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.75
 ---- batch: 020 ----
mean loss: 142.21
 ---- batch: 030 ----
mean loss: 142.17
 ---- batch: 040 ----
mean loss: 137.71
 ---- batch: 050 ----
mean loss: 146.47
 ---- batch: 060 ----
mean loss: 150.74
 ---- batch: 070 ----
mean loss: 149.58
 ---- batch: 080 ----
mean loss: 147.40
 ---- batch: 090 ----
mean loss: 142.64
train mean loss: 144.52
epoch train time: 0:00:01.622868
elapsed time: 0:04:17.545863
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 20:13:29.755706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.03
 ---- batch: 020 ----
mean loss: 134.35
 ---- batch: 030 ----
mean loss: 142.03
 ---- batch: 040 ----
mean loss: 145.15
 ---- batch: 050 ----
mean loss: 139.96
 ---- batch: 060 ----
mean loss: 141.77
 ---- batch: 070 ----
mean loss: 145.49
 ---- batch: 080 ----
mean loss: 149.68
 ---- batch: 090 ----
mean loss: 150.40
train mean loss: 143.74
epoch train time: 0:00:01.635199
elapsed time: 0:04:19.181691
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 20:13:31.391564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.68
 ---- batch: 020 ----
mean loss: 139.39
 ---- batch: 030 ----
mean loss: 138.24
 ---- batch: 040 ----
mean loss: 142.83
 ---- batch: 050 ----
mean loss: 143.04
 ---- batch: 060 ----
mean loss: 141.23
 ---- batch: 070 ----
mean loss: 147.90
 ---- batch: 080 ----
mean loss: 144.45
 ---- batch: 090 ----
mean loss: 144.68
train mean loss: 142.40
epoch train time: 0:00:01.667812
elapsed time: 0:04:20.850229
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 20:13:33.060100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.64
 ---- batch: 020 ----
mean loss: 137.50
 ---- batch: 030 ----
mean loss: 134.56
 ---- batch: 040 ----
mean loss: 145.04
 ---- batch: 050 ----
mean loss: 136.99
 ---- batch: 060 ----
mean loss: 148.39
 ---- batch: 070 ----
mean loss: 144.88
 ---- batch: 080 ----
mean loss: 147.96
 ---- batch: 090 ----
mean loss: 144.08
train mean loss: 142.09
epoch train time: 0:00:01.637634
elapsed time: 0:04:22.488565
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 20:13:34.698440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.66
 ---- batch: 020 ----
mean loss: 138.36
 ---- batch: 030 ----
mean loss: 138.94
 ---- batch: 040 ----
mean loss: 140.97
 ---- batch: 050 ----
mean loss: 141.36
 ---- batch: 060 ----
mean loss: 138.07
 ---- batch: 070 ----
mean loss: 137.31
 ---- batch: 080 ----
mean loss: 146.76
 ---- batch: 090 ----
mean loss: 152.90
train mean loss: 141.39
epoch train time: 0:00:01.675114
elapsed time: 0:04:24.164327
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 20:13:36.374208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.53
 ---- batch: 020 ----
mean loss: 133.54
 ---- batch: 030 ----
mean loss: 141.37
 ---- batch: 040 ----
mean loss: 137.72
 ---- batch: 050 ----
mean loss: 136.87
 ---- batch: 060 ----
mean loss: 145.84
 ---- batch: 070 ----
mean loss: 141.99
 ---- batch: 080 ----
mean loss: 150.01
 ---- batch: 090 ----
mean loss: 142.61
train mean loss: 140.36
epoch train time: 0:00:01.661235
elapsed time: 0:04:25.826287
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 20:13:38.036141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.08
 ---- batch: 020 ----
mean loss: 138.90
 ---- batch: 030 ----
mean loss: 138.11
 ---- batch: 040 ----
mean loss: 134.80
 ---- batch: 050 ----
mean loss: 140.77
 ---- batch: 060 ----
mean loss: 142.79
 ---- batch: 070 ----
mean loss: 141.67
 ---- batch: 080 ----
mean loss: 143.81
 ---- batch: 090 ----
mean loss: 140.90
train mean loss: 139.87
epoch train time: 0:00:01.666592
elapsed time: 0:04:27.493620
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 20:13:39.703565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.15
 ---- batch: 020 ----
mean loss: 135.59
 ---- batch: 030 ----
mean loss: 141.56
 ---- batch: 040 ----
mean loss: 133.36
 ---- batch: 050 ----
mean loss: 145.75
 ---- batch: 060 ----
mean loss: 137.91
 ---- batch: 070 ----
mean loss: 139.51
 ---- batch: 080 ----
mean loss: 138.68
 ---- batch: 090 ----
mean loss: 147.65
train mean loss: 139.75
epoch train time: 0:00:01.654962
elapsed time: 0:04:29.149390
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 20:13:41.359236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.87
 ---- batch: 020 ----
mean loss: 134.60
 ---- batch: 030 ----
mean loss: 132.14
 ---- batch: 040 ----
mean loss: 137.19
 ---- batch: 050 ----
mean loss: 142.14
 ---- batch: 060 ----
mean loss: 140.99
 ---- batch: 070 ----
mean loss: 143.15
 ---- batch: 080 ----
mean loss: 142.70
 ---- batch: 090 ----
mean loss: 135.85
train mean loss: 138.57
epoch train time: 0:00:01.644397
elapsed time: 0:04:30.794449
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 20:13:43.004305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.20
 ---- batch: 020 ----
mean loss: 139.65
 ---- batch: 030 ----
mean loss: 129.76
 ---- batch: 040 ----
mean loss: 139.27
 ---- batch: 050 ----
mean loss: 135.12
 ---- batch: 060 ----
mean loss: 139.28
 ---- batch: 070 ----
mean loss: 139.03
 ---- batch: 080 ----
mean loss: 133.20
 ---- batch: 090 ----
mean loss: 135.68
train mean loss: 137.47
epoch train time: 0:00:01.652642
elapsed time: 0:04:32.447721
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 20:13:44.657573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.52
 ---- batch: 020 ----
mean loss: 136.60
 ---- batch: 030 ----
mean loss: 135.05
 ---- batch: 040 ----
mean loss: 139.48
 ---- batch: 050 ----
mean loss: 130.45
 ---- batch: 060 ----
mean loss: 140.55
 ---- batch: 070 ----
mean loss: 143.76
 ---- batch: 080 ----
mean loss: 137.14
 ---- batch: 090 ----
mean loss: 141.15
train mean loss: 136.99
epoch train time: 0:00:01.654170
elapsed time: 0:04:34.102670
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 20:13:46.312564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.81
 ---- batch: 020 ----
mean loss: 134.43
 ---- batch: 030 ----
mean loss: 132.69
 ---- batch: 040 ----
mean loss: 136.12
 ---- batch: 050 ----
mean loss: 141.76
 ---- batch: 060 ----
mean loss: 138.44
 ---- batch: 070 ----
mean loss: 132.78
 ---- batch: 080 ----
mean loss: 143.28
 ---- batch: 090 ----
mean loss: 140.56
train mean loss: 136.54
epoch train time: 0:00:01.678167
elapsed time: 0:04:35.781497
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 20:13:47.991371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.70
 ---- batch: 020 ----
mean loss: 131.05
 ---- batch: 030 ----
mean loss: 137.17
 ---- batch: 040 ----
mean loss: 136.41
 ---- batch: 050 ----
mean loss: 134.17
 ---- batch: 060 ----
mean loss: 135.60
 ---- batch: 070 ----
mean loss: 136.15
 ---- batch: 080 ----
mean loss: 135.14
 ---- batch: 090 ----
mean loss: 140.04
train mean loss: 136.38
epoch train time: 0:00:01.666103
elapsed time: 0:04:37.448248
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 20:13:49.658146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.07
 ---- batch: 020 ----
mean loss: 133.86
 ---- batch: 030 ----
mean loss: 135.33
 ---- batch: 040 ----
mean loss: 135.73
 ---- batch: 050 ----
mean loss: 130.66
 ---- batch: 060 ----
mean loss: 137.64
 ---- batch: 070 ----
mean loss: 131.13
 ---- batch: 080 ----
mean loss: 143.28
 ---- batch: 090 ----
mean loss: 138.52
train mean loss: 135.25
epoch train time: 0:00:01.656964
elapsed time: 0:04:39.105875
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 20:13:51.315724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.58
 ---- batch: 020 ----
mean loss: 128.73
 ---- batch: 030 ----
mean loss: 135.04
 ---- batch: 040 ----
mean loss: 135.57
 ---- batch: 050 ----
mean loss: 129.68
 ---- batch: 060 ----
mean loss: 130.72
 ---- batch: 070 ----
mean loss: 140.56
 ---- batch: 080 ----
mean loss: 143.36
 ---- batch: 090 ----
mean loss: 138.85
train mean loss: 135.04
epoch train time: 0:00:01.672175
elapsed time: 0:04:40.778889
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 20:13:52.988562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.13
 ---- batch: 020 ----
mean loss: 130.54
 ---- batch: 030 ----
mean loss: 132.87
 ---- batch: 040 ----
mean loss: 135.25
 ---- batch: 050 ----
mean loss: 134.25
 ---- batch: 060 ----
mean loss: 136.69
 ---- batch: 070 ----
mean loss: 142.61
 ---- batch: 080 ----
mean loss: 132.95
 ---- batch: 090 ----
mean loss: 137.50
train mean loss: 134.85
epoch train time: 0:00:01.645701
elapsed time: 0:04:42.425057
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 20:13:54.634912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.57
 ---- batch: 020 ----
mean loss: 136.94
 ---- batch: 030 ----
mean loss: 130.80
 ---- batch: 040 ----
mean loss: 137.24
 ---- batch: 050 ----
mean loss: 129.78
 ---- batch: 060 ----
mean loss: 134.86
 ---- batch: 070 ----
mean loss: 128.68
 ---- batch: 080 ----
mean loss: 137.51
 ---- batch: 090 ----
mean loss: 137.00
train mean loss: 134.01
epoch train time: 0:00:01.623281
elapsed time: 0:04:44.049039
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 20:13:56.258921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.19
 ---- batch: 020 ----
mean loss: 132.53
 ---- batch: 030 ----
mean loss: 129.76
 ---- batch: 040 ----
mean loss: 132.71
 ---- batch: 050 ----
mean loss: 133.26
 ---- batch: 060 ----
mean loss: 135.20
 ---- batch: 070 ----
mean loss: 132.50
 ---- batch: 080 ----
mean loss: 130.89
 ---- batch: 090 ----
mean loss: 137.34
train mean loss: 133.29
epoch train time: 0:00:01.650950
elapsed time: 0:04:45.700731
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 20:13:57.910611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.88
 ---- batch: 020 ----
mean loss: 132.69
 ---- batch: 030 ----
mean loss: 135.17
 ---- batch: 040 ----
mean loss: 131.73
 ---- batch: 050 ----
mean loss: 128.15
 ---- batch: 060 ----
mean loss: 133.70
 ---- batch: 070 ----
mean loss: 137.44
 ---- batch: 080 ----
mean loss: 134.02
 ---- batch: 090 ----
mean loss: 138.99
train mean loss: 132.86
epoch train time: 0:00:01.630143
elapsed time: 0:04:47.331672
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 20:13:59.541645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.48
 ---- batch: 020 ----
mean loss: 124.99
 ---- batch: 030 ----
mean loss: 130.54
 ---- batch: 040 ----
mean loss: 134.82
 ---- batch: 050 ----
mean loss: 131.44
 ---- batch: 060 ----
mean loss: 132.36
 ---- batch: 070 ----
mean loss: 131.05
 ---- batch: 080 ----
mean loss: 131.58
 ---- batch: 090 ----
mean loss: 133.43
train mean loss: 131.74
epoch train time: 0:00:01.655316
elapsed time: 0:04:48.987837
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 20:14:01.197829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.52
 ---- batch: 020 ----
mean loss: 132.94
 ---- batch: 030 ----
mean loss: 124.91
 ---- batch: 040 ----
mean loss: 130.32
 ---- batch: 050 ----
mean loss: 131.48
 ---- batch: 060 ----
mean loss: 132.69
 ---- batch: 070 ----
mean loss: 130.12
 ---- batch: 080 ----
mean loss: 134.14
 ---- batch: 090 ----
mean loss: 136.26
train mean loss: 130.88
epoch train time: 0:00:01.666464
elapsed time: 0:04:50.655122
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 20:14:02.864966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.29
 ---- batch: 020 ----
mean loss: 130.76
 ---- batch: 030 ----
mean loss: 124.09
 ---- batch: 040 ----
mean loss: 128.89
 ---- batch: 050 ----
mean loss: 125.13
 ---- batch: 060 ----
mean loss: 134.92
 ---- batch: 070 ----
mean loss: 133.77
 ---- batch: 080 ----
mean loss: 130.75
 ---- batch: 090 ----
mean loss: 133.67
train mean loss: 130.94
epoch train time: 0:00:01.671007
elapsed time: 0:04:52.326761
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 20:14:04.536614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.64
 ---- batch: 020 ----
mean loss: 125.79
 ---- batch: 030 ----
mean loss: 132.96
 ---- batch: 040 ----
mean loss: 130.86
 ---- batch: 050 ----
mean loss: 124.41
 ---- batch: 060 ----
mean loss: 127.98
 ---- batch: 070 ----
mean loss: 136.96
 ---- batch: 080 ----
mean loss: 135.51
 ---- batch: 090 ----
mean loss: 131.35
train mean loss: 130.51
epoch train time: 0:00:01.679870
elapsed time: 0:04:54.007314
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 20:14:06.217182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.48
 ---- batch: 020 ----
mean loss: 131.44
 ---- batch: 030 ----
mean loss: 124.81
 ---- batch: 040 ----
mean loss: 131.25
 ---- batch: 050 ----
mean loss: 132.82
 ---- batch: 060 ----
mean loss: 128.33
 ---- batch: 070 ----
mean loss: 126.35
 ---- batch: 080 ----
mean loss: 126.48
 ---- batch: 090 ----
mean loss: 129.79
train mean loss: 129.45
epoch train time: 0:00:01.657930
elapsed time: 0:04:55.666069
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 20:14:07.875967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.68
 ---- batch: 020 ----
mean loss: 123.02
 ---- batch: 030 ----
mean loss: 128.60
 ---- batch: 040 ----
mean loss: 128.75
 ---- batch: 050 ----
mean loss: 126.89
 ---- batch: 060 ----
mean loss: 124.96
 ---- batch: 070 ----
mean loss: 131.49
 ---- batch: 080 ----
mean loss: 133.43
 ---- batch: 090 ----
mean loss: 132.45
train mean loss: 129.57
epoch train time: 0:00:01.687694
elapsed time: 0:04:57.354465
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 20:14:09.564265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.81
 ---- batch: 020 ----
mean loss: 122.56
 ---- batch: 030 ----
mean loss: 121.16
 ---- batch: 040 ----
mean loss: 133.27
 ---- batch: 050 ----
mean loss: 130.93
 ---- batch: 060 ----
mean loss: 128.36
 ---- batch: 070 ----
mean loss: 130.86
 ---- batch: 080 ----
mean loss: 130.18
 ---- batch: 090 ----
mean loss: 131.23
train mean loss: 127.85
epoch train time: 0:00:01.666739
elapsed time: 0:04:59.021851
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 20:14:11.231641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.84
 ---- batch: 020 ----
mean loss: 123.95
 ---- batch: 030 ----
mean loss: 122.01
 ---- batch: 040 ----
mean loss: 125.15
 ---- batch: 050 ----
mean loss: 125.09
 ---- batch: 060 ----
mean loss: 132.10
 ---- batch: 070 ----
mean loss: 133.45
 ---- batch: 080 ----
mean loss: 130.97
 ---- batch: 090 ----
mean loss: 129.52
train mean loss: 127.53
epoch train time: 0:00:01.643646
elapsed time: 0:05:00.666113
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 20:14:12.876000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.74
 ---- batch: 020 ----
mean loss: 125.17
 ---- batch: 030 ----
mean loss: 127.18
 ---- batch: 040 ----
mean loss: 124.76
 ---- batch: 050 ----
mean loss: 119.58
 ---- batch: 060 ----
mean loss: 124.69
 ---- batch: 070 ----
mean loss: 130.98
 ---- batch: 080 ----
mean loss: 128.80
 ---- batch: 090 ----
mean loss: 133.43
train mean loss: 127.38
epoch train time: 0:00:01.649348
elapsed time: 0:05:02.316168
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 20:14:14.526032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.49
 ---- batch: 020 ----
mean loss: 129.64
 ---- batch: 030 ----
mean loss: 130.38
 ---- batch: 040 ----
mean loss: 128.83
 ---- batch: 050 ----
mean loss: 123.35
 ---- batch: 060 ----
mean loss: 128.59
 ---- batch: 070 ----
mean loss: 131.70
 ---- batch: 080 ----
mean loss: 130.83
 ---- batch: 090 ----
mean loss: 126.97
train mean loss: 127.69
epoch train time: 0:00:01.662601
elapsed time: 0:05:03.979532
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 20:14:16.189373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.00
 ---- batch: 020 ----
mean loss: 123.50
 ---- batch: 030 ----
mean loss: 126.95
 ---- batch: 040 ----
mean loss: 125.89
 ---- batch: 050 ----
mean loss: 125.90
 ---- batch: 060 ----
mean loss: 130.78
 ---- batch: 070 ----
mean loss: 129.73
 ---- batch: 080 ----
mean loss: 127.40
 ---- batch: 090 ----
mean loss: 132.40
train mean loss: 127.10
epoch train time: 0:00:01.711474
elapsed time: 0:05:05.691650
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 20:14:17.901557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.74
 ---- batch: 020 ----
mean loss: 120.89
 ---- batch: 030 ----
mean loss: 120.31
 ---- batch: 040 ----
mean loss: 121.55
 ---- batch: 050 ----
mean loss: 127.42
 ---- batch: 060 ----
mean loss: 127.63
 ---- batch: 070 ----
mean loss: 127.65
 ---- batch: 080 ----
mean loss: 125.88
 ---- batch: 090 ----
mean loss: 129.04
train mean loss: 125.38
epoch train time: 0:00:01.661120
elapsed time: 0:05:07.353497
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 20:14:19.563369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.83
 ---- batch: 020 ----
mean loss: 120.97
 ---- batch: 030 ----
mean loss: 123.60
 ---- batch: 040 ----
mean loss: 125.78
 ---- batch: 050 ----
mean loss: 128.58
 ---- batch: 060 ----
mean loss: 127.04
 ---- batch: 070 ----
mean loss: 125.13
 ---- batch: 080 ----
mean loss: 123.32
 ---- batch: 090 ----
mean loss: 123.46
train mean loss: 124.73
epoch train time: 0:00:01.681888
elapsed time: 0:05:09.036105
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 20:14:21.245954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.11
 ---- batch: 020 ----
mean loss: 125.78
 ---- batch: 030 ----
mean loss: 121.19
 ---- batch: 040 ----
mean loss: 120.81
 ---- batch: 050 ----
mean loss: 121.89
 ---- batch: 060 ----
mean loss: 125.55
 ---- batch: 070 ----
mean loss: 122.34
 ---- batch: 080 ----
mean loss: 125.34
 ---- batch: 090 ----
mean loss: 129.68
train mean loss: 124.37
epoch train time: 0:00:01.675007
elapsed time: 0:05:10.711761
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 20:14:22.921594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.08
 ---- batch: 020 ----
mean loss: 119.85
 ---- batch: 030 ----
mean loss: 122.63
 ---- batch: 040 ----
mean loss: 115.71
 ---- batch: 050 ----
mean loss: 122.73
 ---- batch: 060 ----
mean loss: 124.40
 ---- batch: 070 ----
mean loss: 132.64
 ---- batch: 080 ----
mean loss: 125.78
 ---- batch: 090 ----
mean loss: 128.22
train mean loss: 123.89
epoch train time: 0:00:01.645520
elapsed time: 0:05:12.357900
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 20:14:24.567746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.48
 ---- batch: 020 ----
mean loss: 123.59
 ---- batch: 030 ----
mean loss: 116.04
 ---- batch: 040 ----
mean loss: 121.42
 ---- batch: 050 ----
mean loss: 124.08
 ---- batch: 060 ----
mean loss: 126.66
 ---- batch: 070 ----
mean loss: 123.80
 ---- batch: 080 ----
mean loss: 128.74
 ---- batch: 090 ----
mean loss: 124.41
train mean loss: 123.38
epoch train time: 0:00:01.667959
elapsed time: 0:05:14.026500
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 20:14:26.236386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.75
 ---- batch: 020 ----
mean loss: 120.83
 ---- batch: 030 ----
mean loss: 122.64
 ---- batch: 040 ----
mean loss: 123.00
 ---- batch: 050 ----
mean loss: 119.77
 ---- batch: 060 ----
mean loss: 123.90
 ---- batch: 070 ----
mean loss: 121.30
 ---- batch: 080 ----
mean loss: 125.63
 ---- batch: 090 ----
mean loss: 121.30
train mean loss: 122.68
epoch train time: 0:00:01.635105
elapsed time: 0:05:15.662266
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 20:14:27.872161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.30
 ---- batch: 020 ----
mean loss: 119.36
 ---- batch: 030 ----
mean loss: 120.33
 ---- batch: 040 ----
mean loss: 123.58
 ---- batch: 050 ----
mean loss: 129.13
 ---- batch: 060 ----
mean loss: 123.06
 ---- batch: 070 ----
mean loss: 124.46
 ---- batch: 080 ----
mean loss: 126.21
 ---- batch: 090 ----
mean loss: 121.53
train mean loss: 122.65
epoch train time: 0:00:01.674326
elapsed time: 0:05:17.337354
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 20:14:29.547262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.94
 ---- batch: 020 ----
mean loss: 123.35
 ---- batch: 030 ----
mean loss: 123.74
 ---- batch: 040 ----
mean loss: 118.89
 ---- batch: 050 ----
mean loss: 120.37
 ---- batch: 060 ----
mean loss: 123.74
 ---- batch: 070 ----
mean loss: 122.89
 ---- batch: 080 ----
mean loss: 118.16
 ---- batch: 090 ----
mean loss: 125.16
train mean loss: 122.10
epoch train time: 0:00:01.670183
elapsed time: 0:05:19.008198
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 20:14:31.218083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.66
 ---- batch: 020 ----
mean loss: 119.64
 ---- batch: 030 ----
mean loss: 120.33
 ---- batch: 040 ----
mean loss: 125.06
 ---- batch: 050 ----
mean loss: 120.32
 ---- batch: 060 ----
mean loss: 118.39
 ---- batch: 070 ----
mean loss: 121.70
 ---- batch: 080 ----
mean loss: 124.68
 ---- batch: 090 ----
mean loss: 120.10
train mean loss: 121.67
epoch train time: 0:00:01.694395
elapsed time: 0:05:20.703222
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 20:14:32.913054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.78
 ---- batch: 020 ----
mean loss: 116.24
 ---- batch: 030 ----
mean loss: 121.88
 ---- batch: 040 ----
mean loss: 120.92
 ---- batch: 050 ----
mean loss: 115.27
 ---- batch: 060 ----
mean loss: 128.39
 ---- batch: 070 ----
mean loss: 123.08
 ---- batch: 080 ----
mean loss: 122.57
 ---- batch: 090 ----
mean loss: 121.33
train mean loss: 121.08
epoch train time: 0:00:01.645325
elapsed time: 0:05:22.349515
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 20:14:34.559110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.01
 ---- batch: 020 ----
mean loss: 121.10
 ---- batch: 030 ----
mean loss: 121.88
 ---- batch: 040 ----
mean loss: 117.56
 ---- batch: 050 ----
mean loss: 118.72
 ---- batch: 060 ----
mean loss: 124.67
 ---- batch: 070 ----
mean loss: 118.80
 ---- batch: 080 ----
mean loss: 117.30
 ---- batch: 090 ----
mean loss: 122.23
train mean loss: 120.32
epoch train time: 0:00:01.645503
elapsed time: 0:05:23.995363
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 20:14:36.205222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.94
 ---- batch: 020 ----
mean loss: 115.67
 ---- batch: 030 ----
mean loss: 120.89
 ---- batch: 040 ----
mean loss: 123.92
 ---- batch: 050 ----
mean loss: 121.35
 ---- batch: 060 ----
mean loss: 123.14
 ---- batch: 070 ----
mean loss: 123.01
 ---- batch: 080 ----
mean loss: 124.32
 ---- batch: 090 ----
mean loss: 116.85
train mean loss: 120.15
epoch train time: 0:00:01.617172
elapsed time: 0:05:25.613216
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 20:14:37.823090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.05
 ---- batch: 020 ----
mean loss: 118.28
 ---- batch: 030 ----
mean loss: 114.86
 ---- batch: 040 ----
mean loss: 117.49
 ---- batch: 050 ----
mean loss: 121.18
 ---- batch: 060 ----
mean loss: 122.87
 ---- batch: 070 ----
mean loss: 117.17
 ---- batch: 080 ----
mean loss: 123.37
 ---- batch: 090 ----
mean loss: 121.45
train mean loss: 119.27
epoch train time: 0:00:01.639022
elapsed time: 0:05:27.252887
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 20:14:39.462734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.28
 ---- batch: 020 ----
mean loss: 112.54
 ---- batch: 030 ----
mean loss: 117.74
 ---- batch: 040 ----
mean loss: 116.54
 ---- batch: 050 ----
mean loss: 115.57
 ---- batch: 060 ----
mean loss: 118.06
 ---- batch: 070 ----
mean loss: 124.72
 ---- batch: 080 ----
mean loss: 119.24
 ---- batch: 090 ----
mean loss: 119.95
train mean loss: 118.55
epoch train time: 0:00:01.626234
elapsed time: 0:05:28.879753
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 20:14:41.089630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.74
 ---- batch: 020 ----
mean loss: 116.29
 ---- batch: 030 ----
mean loss: 114.32
 ---- batch: 040 ----
mean loss: 116.03
 ---- batch: 050 ----
mean loss: 119.35
 ---- batch: 060 ----
mean loss: 120.63
 ---- batch: 070 ----
mean loss: 116.68
 ---- batch: 080 ----
mean loss: 120.69
 ---- batch: 090 ----
mean loss: 126.23
train mean loss: 118.21
epoch train time: 0:00:01.640412
elapsed time: 0:05:30.520888
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 20:14:42.730764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.64
 ---- batch: 020 ----
mean loss: 112.80
 ---- batch: 030 ----
mean loss: 116.48
 ---- batch: 040 ----
mean loss: 119.95
 ---- batch: 050 ----
mean loss: 121.90
 ---- batch: 060 ----
mean loss: 123.68
 ---- batch: 070 ----
mean loss: 118.18
 ---- batch: 080 ----
mean loss: 120.93
 ---- batch: 090 ----
mean loss: 118.75
train mean loss: 118.75
epoch train time: 0:00:01.683137
elapsed time: 0:05:32.204702
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 20:14:44.414563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.26
 ---- batch: 020 ----
mean loss: 113.26
 ---- batch: 030 ----
mean loss: 117.04
 ---- batch: 040 ----
mean loss: 116.72
 ---- batch: 050 ----
mean loss: 120.20
 ---- batch: 060 ----
mean loss: 112.65
 ---- batch: 070 ----
mean loss: 116.72
 ---- batch: 080 ----
mean loss: 118.52
 ---- batch: 090 ----
mean loss: 118.46
train mean loss: 117.63
epoch train time: 0:00:01.664901
elapsed time: 0:05:33.870250
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 20:14:46.080105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.65
 ---- batch: 020 ----
mean loss: 111.20
 ---- batch: 030 ----
mean loss: 115.13
 ---- batch: 040 ----
mean loss: 117.83
 ---- batch: 050 ----
mean loss: 117.55
 ---- batch: 060 ----
mean loss: 123.43
 ---- batch: 070 ----
mean loss: 117.38
 ---- batch: 080 ----
mean loss: 120.63
 ---- batch: 090 ----
mean loss: 119.41
train mean loss: 117.31
epoch train time: 0:00:01.689043
elapsed time: 0:05:35.560016
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 20:14:47.769888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.58
 ---- batch: 020 ----
mean loss: 115.56
 ---- batch: 030 ----
mean loss: 116.89
 ---- batch: 040 ----
mean loss: 116.18
 ---- batch: 050 ----
mean loss: 114.87
 ---- batch: 060 ----
mean loss: 116.62
 ---- batch: 070 ----
mean loss: 115.97
 ---- batch: 080 ----
mean loss: 118.97
 ---- batch: 090 ----
mean loss: 116.90
train mean loss: 116.56
epoch train time: 0:00:01.665817
elapsed time: 0:05:37.226605
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 20:14:49.436509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.52
 ---- batch: 020 ----
mean loss: 114.70
 ---- batch: 030 ----
mean loss: 115.04
 ---- batch: 040 ----
mean loss: 118.85
 ---- batch: 050 ----
mean loss: 114.73
 ---- batch: 060 ----
mean loss: 117.32
 ---- batch: 070 ----
mean loss: 114.75
 ---- batch: 080 ----
mean loss: 117.10
 ---- batch: 090 ----
mean loss: 111.25
train mean loss: 116.08
epoch train time: 0:00:01.666671
elapsed time: 0:05:38.894016
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 20:14:51.103887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.86
 ---- batch: 020 ----
mean loss: 116.80
 ---- batch: 030 ----
mean loss: 113.73
 ---- batch: 040 ----
mean loss: 116.31
 ---- batch: 050 ----
mean loss: 113.45
 ---- batch: 060 ----
mean loss: 121.30
 ---- batch: 070 ----
mean loss: 114.58
 ---- batch: 080 ----
mean loss: 118.61
 ---- batch: 090 ----
mean loss: 116.11
train mean loss: 116.21
epoch train time: 0:00:01.673328
elapsed time: 0:05:40.568023
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 20:14:52.777890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.61
 ---- batch: 020 ----
mean loss: 114.21
 ---- batch: 030 ----
mean loss: 109.86
 ---- batch: 040 ----
mean loss: 116.05
 ---- batch: 050 ----
mean loss: 115.83
 ---- batch: 060 ----
mean loss: 116.48
 ---- batch: 070 ----
mean loss: 117.65
 ---- batch: 080 ----
mean loss: 115.62
 ---- batch: 090 ----
mean loss: 120.87
train mean loss: 116.11
epoch train time: 0:00:01.643916
elapsed time: 0:05:42.212606
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 20:14:54.422473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.66
 ---- batch: 020 ----
mean loss: 109.60
 ---- batch: 030 ----
mean loss: 116.74
 ---- batch: 040 ----
mean loss: 114.06
 ---- batch: 050 ----
mean loss: 114.39
 ---- batch: 060 ----
mean loss: 119.05
 ---- batch: 070 ----
mean loss: 119.58
 ---- batch: 080 ----
mean loss: 113.89
 ---- batch: 090 ----
mean loss: 118.78
train mean loss: 115.50
epoch train time: 0:00:01.693382
elapsed time: 0:05:43.906770
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 20:14:56.116634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.03
 ---- batch: 020 ----
mean loss: 116.90
 ---- batch: 030 ----
mean loss: 113.31
 ---- batch: 040 ----
mean loss: 116.56
 ---- batch: 050 ----
mean loss: 112.49
 ---- batch: 060 ----
mean loss: 113.18
 ---- batch: 070 ----
mean loss: 112.12
 ---- batch: 080 ----
mean loss: 114.65
 ---- batch: 090 ----
mean loss: 117.51
train mean loss: 114.19
epoch train time: 0:00:01.645652
elapsed time: 0:05:45.553183
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 20:14:57.763110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.37
 ---- batch: 020 ----
mean loss: 111.23
 ---- batch: 030 ----
mean loss: 110.93
 ---- batch: 040 ----
mean loss: 111.19
 ---- batch: 050 ----
mean loss: 117.17
 ---- batch: 060 ----
mean loss: 118.67
 ---- batch: 070 ----
mean loss: 118.63
 ---- batch: 080 ----
mean loss: 115.30
 ---- batch: 090 ----
mean loss: 117.76
train mean loss: 114.59
epoch train time: 0:00:01.673626
elapsed time: 0:05:47.227496
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 20:14:59.437394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.39
 ---- batch: 020 ----
mean loss: 107.17
 ---- batch: 030 ----
mean loss: 115.35
 ---- batch: 040 ----
mean loss: 113.29
 ---- batch: 050 ----
mean loss: 113.51
 ---- batch: 060 ----
mean loss: 109.60
 ---- batch: 070 ----
mean loss: 112.59
 ---- batch: 080 ----
mean loss: 121.32
 ---- batch: 090 ----
mean loss: 118.01
train mean loss: 113.41
epoch train time: 0:00:01.660928
elapsed time: 0:05:48.889073
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 20:15:01.098966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.47
 ---- batch: 020 ----
mean loss: 114.09
 ---- batch: 030 ----
mean loss: 110.66
 ---- batch: 040 ----
mean loss: 114.57
 ---- batch: 050 ----
mean loss: 119.42
 ---- batch: 060 ----
mean loss: 115.41
 ---- batch: 070 ----
mean loss: 113.08
 ---- batch: 080 ----
mean loss: 117.54
 ---- batch: 090 ----
mean loss: 112.82
train mean loss: 113.62
epoch train time: 0:00:01.661285
elapsed time: 0:05:50.551106
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 20:15:02.760966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.30
 ---- batch: 020 ----
mean loss: 110.16
 ---- batch: 030 ----
mean loss: 115.85
 ---- batch: 040 ----
mean loss: 107.38
 ---- batch: 050 ----
mean loss: 113.49
 ---- batch: 060 ----
mean loss: 110.95
 ---- batch: 070 ----
mean loss: 123.57
 ---- batch: 080 ----
mean loss: 116.45
 ---- batch: 090 ----
mean loss: 109.88
train mean loss: 112.48
epoch train time: 0:00:01.671589
elapsed time: 0:05:52.223403
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 20:15:04.433275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.96
 ---- batch: 020 ----
mean loss: 112.03
 ---- batch: 030 ----
mean loss: 110.01
 ---- batch: 040 ----
mean loss: 114.03
 ---- batch: 050 ----
mean loss: 108.48
 ---- batch: 060 ----
mean loss: 116.02
 ---- batch: 070 ----
mean loss: 110.37
 ---- batch: 080 ----
mean loss: 111.17
 ---- batch: 090 ----
mean loss: 112.35
train mean loss: 111.84
epoch train time: 0:00:01.665679
elapsed time: 0:05:53.889715
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 20:15:06.099583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.34
 ---- batch: 020 ----
mean loss: 105.44
 ---- batch: 030 ----
mean loss: 110.23
 ---- batch: 040 ----
mean loss: 111.50
 ---- batch: 050 ----
mean loss: 111.24
 ---- batch: 060 ----
mean loss: 108.07
 ---- batch: 070 ----
mean loss: 111.59
 ---- batch: 080 ----
mean loss: 113.82
 ---- batch: 090 ----
mean loss: 122.08
train mean loss: 112.05
epoch train time: 0:00:01.628394
elapsed time: 0:05:55.518745
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 20:15:07.728612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.92
 ---- batch: 020 ----
mean loss: 108.97
 ---- batch: 030 ----
mean loss: 105.67
 ---- batch: 040 ----
mean loss: 111.78
 ---- batch: 050 ----
mean loss: 110.93
 ---- batch: 060 ----
mean loss: 112.01
 ---- batch: 070 ----
mean loss: 107.70
 ---- batch: 080 ----
mean loss: 114.32
 ---- batch: 090 ----
mean loss: 115.05
train mean loss: 111.21
epoch train time: 0:00:01.648059
elapsed time: 0:05:57.167510
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 20:15:09.377410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.39
 ---- batch: 020 ----
mean loss: 113.24
 ---- batch: 030 ----
mean loss: 107.36
 ---- batch: 040 ----
mean loss: 108.22
 ---- batch: 050 ----
mean loss: 113.14
 ---- batch: 060 ----
mean loss: 109.79
 ---- batch: 070 ----
mean loss: 114.51
 ---- batch: 080 ----
mean loss: 115.72
 ---- batch: 090 ----
mean loss: 110.41
train mean loss: 111.30
epoch train time: 0:00:01.658078
elapsed time: 0:05:58.826284
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 20:15:11.036147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.83
 ---- batch: 020 ----
mean loss: 108.55
 ---- batch: 030 ----
mean loss: 114.43
 ---- batch: 040 ----
mean loss: 109.87
 ---- batch: 050 ----
mean loss: 105.12
 ---- batch: 060 ----
mean loss: 114.01
 ---- batch: 070 ----
mean loss: 108.59
 ---- batch: 080 ----
mean loss: 109.10
 ---- batch: 090 ----
mean loss: 118.18
train mean loss: 110.97
epoch train time: 0:00:01.651178
elapsed time: 0:06:00.478211
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 20:15:12.688117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.50
 ---- batch: 020 ----
mean loss: 107.25
 ---- batch: 030 ----
mean loss: 108.31
 ---- batch: 040 ----
mean loss: 103.15
 ---- batch: 050 ----
mean loss: 108.99
 ---- batch: 060 ----
mean loss: 117.75
 ---- batch: 070 ----
mean loss: 112.30
 ---- batch: 080 ----
mean loss: 114.68
 ---- batch: 090 ----
mean loss: 118.10
train mean loss: 111.00
epoch train time: 0:00:01.669958
elapsed time: 0:06:02.148876
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 20:15:14.358746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.00
 ---- batch: 020 ----
mean loss: 106.78
 ---- batch: 030 ----
mean loss: 107.32
 ---- batch: 040 ----
mean loss: 109.16
 ---- batch: 050 ----
mean loss: 110.14
 ---- batch: 060 ----
mean loss: 109.18
 ---- batch: 070 ----
mean loss: 108.72
 ---- batch: 080 ----
mean loss: 117.98
 ---- batch: 090 ----
mean loss: 114.25
train mean loss: 109.89
epoch train time: 0:00:01.671225
elapsed time: 0:06:03.820688
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 20:15:16.030550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.36
 ---- batch: 020 ----
mean loss: 105.20
 ---- batch: 030 ----
mean loss: 104.35
 ---- batch: 040 ----
mean loss: 112.53
 ---- batch: 050 ----
mean loss: 107.72
 ---- batch: 060 ----
mean loss: 112.62
 ---- batch: 070 ----
mean loss: 110.89
 ---- batch: 080 ----
mean loss: 114.68
 ---- batch: 090 ----
mean loss: 111.97
train mean loss: 109.73
epoch train time: 0:00:01.662775
elapsed time: 0:06:05.484240
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 20:15:17.694081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.39
 ---- batch: 020 ----
mean loss: 109.33
 ---- batch: 030 ----
mean loss: 101.82
 ---- batch: 040 ----
mean loss: 107.62
 ---- batch: 050 ----
mean loss: 103.88
 ---- batch: 060 ----
mean loss: 113.71
 ---- batch: 070 ----
mean loss: 115.70
 ---- batch: 080 ----
mean loss: 114.95
 ---- batch: 090 ----
mean loss: 111.55
train mean loss: 109.60
epoch train time: 0:00:01.646403
elapsed time: 0:06:07.131321
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 20:15:19.341211
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.07
 ---- batch: 020 ----
mean loss: 103.93
 ---- batch: 030 ----
mean loss: 101.41
 ---- batch: 040 ----
mean loss: 109.26
 ---- batch: 050 ----
mean loss: 101.82
 ---- batch: 060 ----
mean loss: 98.63
 ---- batch: 070 ----
mean loss: 99.93
 ---- batch: 080 ----
mean loss: 106.87
 ---- batch: 090 ----
mean loss: 101.55
train mean loss: 104.04
epoch train time: 0:00:01.623357
elapsed time: 0:06:08.755637
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 20:15:20.965240
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.61
 ---- batch: 020 ----
mean loss: 100.51
 ---- batch: 030 ----
mean loss: 103.82
 ---- batch: 040 ----
mean loss: 100.75
 ---- batch: 050 ----
mean loss: 103.91
 ---- batch: 060 ----
mean loss: 107.96
 ---- batch: 070 ----
mean loss: 99.51
 ---- batch: 080 ----
mean loss: 103.30
 ---- batch: 090 ----
mean loss: 99.45
train mean loss: 103.20
epoch train time: 0:00:01.663068
elapsed time: 0:06:10.419146
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 20:15:22.628997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.29
 ---- batch: 020 ----
mean loss: 100.46
 ---- batch: 030 ----
mean loss: 97.83
 ---- batch: 040 ----
mean loss: 103.72
 ---- batch: 050 ----
mean loss: 106.37
 ---- batch: 060 ----
mean loss: 99.58
 ---- batch: 070 ----
mean loss: 106.14
 ---- batch: 080 ----
mean loss: 104.40
 ---- batch: 090 ----
mean loss: 104.93
train mean loss: 103.16
epoch train time: 0:00:01.649890
elapsed time: 0:06:12.069806
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 20:15:24.279658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.42
 ---- batch: 020 ----
mean loss: 98.88
 ---- batch: 030 ----
mean loss: 104.70
 ---- batch: 040 ----
mean loss: 98.99
 ---- batch: 050 ----
mean loss: 101.27
 ---- batch: 060 ----
mean loss: 106.30
 ---- batch: 070 ----
mean loss: 103.86
 ---- batch: 080 ----
mean loss: 106.93
 ---- batch: 090 ----
mean loss: 98.80
train mean loss: 102.81
epoch train time: 0:00:01.649044
elapsed time: 0:06:13.719609
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 20:15:25.929563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.40
 ---- batch: 020 ----
mean loss: 104.70
 ---- batch: 030 ----
mean loss: 101.23
 ---- batch: 040 ----
mean loss: 99.83
 ---- batch: 050 ----
mean loss: 106.04
 ---- batch: 060 ----
mean loss: 105.12
 ---- batch: 070 ----
mean loss: 102.33
 ---- batch: 080 ----
mean loss: 105.31
 ---- batch: 090 ----
mean loss: 101.40
train mean loss: 103.01
epoch train time: 0:00:01.671843
elapsed time: 0:06:15.392222
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 20:15:27.602112
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.98
 ---- batch: 020 ----
mean loss: 99.01
 ---- batch: 030 ----
mean loss: 103.10
 ---- batch: 040 ----
mean loss: 103.12
 ---- batch: 050 ----
mean loss: 102.63
 ---- batch: 060 ----
mean loss: 103.53
 ---- batch: 070 ----
mean loss: 104.65
 ---- batch: 080 ----
mean loss: 100.42
 ---- batch: 090 ----
mean loss: 104.48
train mean loss: 102.60
epoch train time: 0:00:01.657247
elapsed time: 0:06:17.050164
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 20:15:29.260012
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.66
 ---- batch: 020 ----
mean loss: 103.72
 ---- batch: 030 ----
mean loss: 103.76
 ---- batch: 040 ----
mean loss: 104.81
 ---- batch: 050 ----
mean loss: 102.91
 ---- batch: 060 ----
mean loss: 100.32
 ---- batch: 070 ----
mean loss: 99.59
 ---- batch: 080 ----
mean loss: 100.41
 ---- batch: 090 ----
mean loss: 106.46
train mean loss: 102.63
epoch train time: 0:00:01.625467
elapsed time: 0:06:18.676344
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 20:15:30.886241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.74
 ---- batch: 020 ----
mean loss: 108.76
 ---- batch: 030 ----
mean loss: 105.69
 ---- batch: 040 ----
mean loss: 101.15
 ---- batch: 050 ----
mean loss: 100.50
 ---- batch: 060 ----
mean loss: 99.11
 ---- batch: 070 ----
mean loss: 99.05
 ---- batch: 080 ----
mean loss: 105.66
 ---- batch: 090 ----
mean loss: 98.08
train mean loss: 102.39
epoch train time: 0:00:01.660027
elapsed time: 0:06:20.337059
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 20:15:32.546917
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.96
 ---- batch: 020 ----
mean loss: 105.25
 ---- batch: 030 ----
mean loss: 99.06
 ---- batch: 040 ----
mean loss: 100.22
 ---- batch: 050 ----
mean loss: 103.02
 ---- batch: 060 ----
mean loss: 104.89
 ---- batch: 070 ----
mean loss: 101.66
 ---- batch: 080 ----
mean loss: 103.58
 ---- batch: 090 ----
mean loss: 102.98
train mean loss: 102.66
epoch train time: 0:00:01.736460
elapsed time: 0:06:22.074172
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 20:15:34.284096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.96
 ---- batch: 020 ----
mean loss: 98.89
 ---- batch: 030 ----
mean loss: 105.25
 ---- batch: 040 ----
mean loss: 106.70
 ---- batch: 050 ----
mean loss: 100.57
 ---- batch: 060 ----
mean loss: 106.82
 ---- batch: 070 ----
mean loss: 102.18
 ---- batch: 080 ----
mean loss: 102.93
 ---- batch: 090 ----
mean loss: 101.02
train mean loss: 102.44
epoch train time: 0:00:01.688136
elapsed time: 0:06:23.763121
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 20:15:35.972991
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.33
 ---- batch: 020 ----
mean loss: 98.48
 ---- batch: 030 ----
mean loss: 103.76
 ---- batch: 040 ----
mean loss: 104.63
 ---- batch: 050 ----
mean loss: 99.51
 ---- batch: 060 ----
mean loss: 106.40
 ---- batch: 070 ----
mean loss: 106.65
 ---- batch: 080 ----
mean loss: 97.06
 ---- batch: 090 ----
mean loss: 100.81
train mean loss: 102.25
epoch train time: 0:00:01.694455
elapsed time: 0:06:25.458230
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 20:15:37.668065
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.63
 ---- batch: 020 ----
mean loss: 95.25
 ---- batch: 030 ----
mean loss: 103.81
 ---- batch: 040 ----
mean loss: 105.10
 ---- batch: 050 ----
mean loss: 103.71
 ---- batch: 060 ----
mean loss: 103.64
 ---- batch: 070 ----
mean loss: 98.85
 ---- batch: 080 ----
mean loss: 101.99
 ---- batch: 090 ----
mean loss: 106.12
train mean loss: 102.35
epoch train time: 0:00:01.684172
elapsed time: 0:06:27.143019
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 20:15:39.352820
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.54
 ---- batch: 020 ----
mean loss: 99.44
 ---- batch: 030 ----
mean loss: 97.96
 ---- batch: 040 ----
mean loss: 108.28
 ---- batch: 050 ----
mean loss: 107.76
 ---- batch: 060 ----
mean loss: 102.70
 ---- batch: 070 ----
mean loss: 100.22
 ---- batch: 080 ----
mean loss: 102.66
 ---- batch: 090 ----
mean loss: 98.80
train mean loss: 102.42
epoch train time: 0:00:01.695739
elapsed time: 0:06:28.839408
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 20:15:41.049309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.39
 ---- batch: 020 ----
mean loss: 95.19
 ---- batch: 030 ----
mean loss: 100.70
 ---- batch: 040 ----
mean loss: 99.90
 ---- batch: 050 ----
mean loss: 104.36
 ---- batch: 060 ----
mean loss: 99.28
 ---- batch: 070 ----
mean loss: 104.33
 ---- batch: 080 ----
mean loss: 108.93
 ---- batch: 090 ----
mean loss: 103.36
train mean loss: 102.25
epoch train time: 0:00:01.692825
elapsed time: 0:06:30.532966
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 20:15:42.742813
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.01
 ---- batch: 020 ----
mean loss: 103.95
 ---- batch: 030 ----
mean loss: 96.80
 ---- batch: 040 ----
mean loss: 100.21
 ---- batch: 050 ----
mean loss: 98.93
 ---- batch: 060 ----
mean loss: 99.68
 ---- batch: 070 ----
mean loss: 101.93
 ---- batch: 080 ----
mean loss: 106.23
 ---- batch: 090 ----
mean loss: 108.10
train mean loss: 102.18
epoch train time: 0:00:01.663658
elapsed time: 0:06:32.197333
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 20:15:44.407180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.04
 ---- batch: 020 ----
mean loss: 99.83
 ---- batch: 030 ----
mean loss: 102.75
 ---- batch: 040 ----
mean loss: 104.07
 ---- batch: 050 ----
mean loss: 97.40
 ---- batch: 060 ----
mean loss: 101.62
 ---- batch: 070 ----
mean loss: 102.39
 ---- batch: 080 ----
mean loss: 103.89
 ---- batch: 090 ----
mean loss: 105.24
train mean loss: 102.06
epoch train time: 0:00:01.641037
elapsed time: 0:06:33.839014
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 20:15:46.048867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.10
 ---- batch: 020 ----
mean loss: 99.71
 ---- batch: 030 ----
mean loss: 105.20
 ---- batch: 040 ----
mean loss: 99.51
 ---- batch: 050 ----
mean loss: 104.25
 ---- batch: 060 ----
mean loss: 98.82
 ---- batch: 070 ----
mean loss: 100.88
 ---- batch: 080 ----
mean loss: 106.22
 ---- batch: 090 ----
mean loss: 106.24
train mean loss: 102.06
epoch train time: 0:00:01.697734
elapsed time: 0:06:35.537490
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 20:15:47.747333
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.60
 ---- batch: 020 ----
mean loss: 100.22
 ---- batch: 030 ----
mean loss: 104.57
 ---- batch: 040 ----
mean loss: 99.84
 ---- batch: 050 ----
mean loss: 103.10
 ---- batch: 060 ----
mean loss: 100.05
 ---- batch: 070 ----
mean loss: 101.29
 ---- batch: 080 ----
mean loss: 104.28
 ---- batch: 090 ----
mean loss: 107.09
train mean loss: 102.23
epoch train time: 0:00:01.690678
elapsed time: 0:06:37.228832
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 20:15:49.438686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.17
 ---- batch: 020 ----
mean loss: 102.21
 ---- batch: 030 ----
mean loss: 106.18
 ---- batch: 040 ----
mean loss: 106.07
 ---- batch: 050 ----
mean loss: 103.04
 ---- batch: 060 ----
mean loss: 97.19
 ---- batch: 070 ----
mean loss: 99.86
 ---- batch: 080 ----
mean loss: 101.76
 ---- batch: 090 ----
mean loss: 99.18
train mean loss: 101.97
epoch train time: 0:00:01.623775
elapsed time: 0:06:38.853258
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 20:15:51.063106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.69
 ---- batch: 020 ----
mean loss: 103.50
 ---- batch: 030 ----
mean loss: 98.33
 ---- batch: 040 ----
mean loss: 102.22
 ---- batch: 050 ----
mean loss: 104.40
 ---- batch: 060 ----
mean loss: 101.72
 ---- batch: 070 ----
mean loss: 101.85
 ---- batch: 080 ----
mean loss: 103.82
 ---- batch: 090 ----
mean loss: 99.06
train mean loss: 101.83
epoch train time: 0:00:01.660913
elapsed time: 0:06:40.514971
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 20:15:52.724854
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.13
 ---- batch: 020 ----
mean loss: 100.04
 ---- batch: 030 ----
mean loss: 99.33
 ---- batch: 040 ----
mean loss: 101.12
 ---- batch: 050 ----
mean loss: 103.51
 ---- batch: 060 ----
mean loss: 105.43
 ---- batch: 070 ----
mean loss: 99.15
 ---- batch: 080 ----
mean loss: 107.29
 ---- batch: 090 ----
mean loss: 99.45
train mean loss: 101.85
epoch train time: 0:00:01.675492
elapsed time: 0:06:42.191132
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 20:15:54.400993
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.96
 ---- batch: 020 ----
mean loss: 100.59
 ---- batch: 030 ----
mean loss: 100.25
 ---- batch: 040 ----
mean loss: 96.92
 ---- batch: 050 ----
mean loss: 99.12
 ---- batch: 060 ----
mean loss: 103.11
 ---- batch: 070 ----
mean loss: 103.05
 ---- batch: 080 ----
mean loss: 108.68
 ---- batch: 090 ----
mean loss: 103.82
train mean loss: 102.10
epoch train time: 0:00:01.696915
elapsed time: 0:06:43.888792
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 20:15:56.098476
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.99
 ---- batch: 020 ----
mean loss: 104.58
 ---- batch: 030 ----
mean loss: 100.88
 ---- batch: 040 ----
mean loss: 104.44
 ---- batch: 050 ----
mean loss: 102.63
 ---- batch: 060 ----
mean loss: 100.22
 ---- batch: 070 ----
mean loss: 98.89
 ---- batch: 080 ----
mean loss: 99.69
 ---- batch: 090 ----
mean loss: 106.64
train mean loss: 101.64
epoch train time: 0:00:01.662208
elapsed time: 0:06:45.551526
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 20:15:57.761439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.08
 ---- batch: 020 ----
mean loss: 94.66
 ---- batch: 030 ----
mean loss: 104.06
 ---- batch: 040 ----
mean loss: 100.17
 ---- batch: 050 ----
mean loss: 101.47
 ---- batch: 060 ----
mean loss: 98.13
 ---- batch: 070 ----
mean loss: 100.52
 ---- batch: 080 ----
mean loss: 106.59
 ---- batch: 090 ----
mean loss: 108.41
train mean loss: 101.71
epoch train time: 0:00:01.655763
elapsed time: 0:06:47.208023
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 20:15:59.417895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.03
 ---- batch: 020 ----
mean loss: 105.59
 ---- batch: 030 ----
mean loss: 100.61
 ---- batch: 040 ----
mean loss: 100.95
 ---- batch: 050 ----
mean loss: 104.21
 ---- batch: 060 ----
mean loss: 101.16
 ---- batch: 070 ----
mean loss: 98.03
 ---- batch: 080 ----
mean loss: 101.27
 ---- batch: 090 ----
mean loss: 102.38
train mean loss: 101.72
epoch train time: 0:00:01.671753
elapsed time: 0:06:48.880431
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 20:16:01.090280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.71
 ---- batch: 020 ----
mean loss: 104.27
 ---- batch: 030 ----
mean loss: 102.27
 ---- batch: 040 ----
mean loss: 100.94
 ---- batch: 050 ----
mean loss: 97.29
 ---- batch: 060 ----
mean loss: 101.88
 ---- batch: 070 ----
mean loss: 96.55
 ---- batch: 080 ----
mean loss: 105.37
 ---- batch: 090 ----
mean loss: 103.61
train mean loss: 101.66
epoch train time: 0:00:01.641327
elapsed time: 0:06:50.522406
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 20:16:02.732245
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.92
 ---- batch: 020 ----
mean loss: 103.94
 ---- batch: 030 ----
mean loss: 104.37
 ---- batch: 040 ----
mean loss: 103.60
 ---- batch: 050 ----
mean loss: 102.95
 ---- batch: 060 ----
mean loss: 103.32
 ---- batch: 070 ----
mean loss: 100.63
 ---- batch: 080 ----
mean loss: 99.88
 ---- batch: 090 ----
mean loss: 98.63
train mean loss: 101.65
epoch train time: 0:00:01.632152
elapsed time: 0:06:52.155192
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 20:16:04.365029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.29
 ---- batch: 020 ----
mean loss: 98.75
 ---- batch: 030 ----
mean loss: 104.80
 ---- batch: 040 ----
mean loss: 110.41
 ---- batch: 050 ----
mean loss: 100.77
 ---- batch: 060 ----
mean loss: 101.76
 ---- batch: 070 ----
mean loss: 103.03
 ---- batch: 080 ----
mean loss: 98.19
 ---- batch: 090 ----
mean loss: 101.14
train mean loss: 101.56
epoch train time: 0:00:01.636734
elapsed time: 0:06:53.792633
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 20:16:06.002526
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.10
 ---- batch: 020 ----
mean loss: 98.51
 ---- batch: 030 ----
mean loss: 99.94
 ---- batch: 040 ----
mean loss: 106.00
 ---- batch: 050 ----
mean loss: 103.96
 ---- batch: 060 ----
mean loss: 102.56
 ---- batch: 070 ----
mean loss: 103.43
 ---- batch: 080 ----
mean loss: 96.75
 ---- batch: 090 ----
mean loss: 104.78
train mean loss: 101.48
epoch train time: 0:00:01.649515
elapsed time: 0:06:55.442860
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 20:16:07.652719
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.62
 ---- batch: 020 ----
mean loss: 100.48
 ---- batch: 030 ----
mean loss: 100.96
 ---- batch: 040 ----
mean loss: 97.63
 ---- batch: 050 ----
mean loss: 102.28
 ---- batch: 060 ----
mean loss: 100.26
 ---- batch: 070 ----
mean loss: 101.97
 ---- batch: 080 ----
mean loss: 106.04
 ---- batch: 090 ----
mean loss: 97.96
train mean loss: 101.45
epoch train time: 0:00:01.643997
elapsed time: 0:06:57.087571
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 20:16:09.297443
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.87
 ---- batch: 020 ----
mean loss: 103.64
 ---- batch: 030 ----
mean loss: 103.80
 ---- batch: 040 ----
mean loss: 99.85
 ---- batch: 050 ----
mean loss: 98.23
 ---- batch: 060 ----
mean loss: 98.02
 ---- batch: 070 ----
mean loss: 99.15
 ---- batch: 080 ----
mean loss: 103.22
 ---- batch: 090 ----
mean loss: 101.62
train mean loss: 101.43
epoch train time: 0:00:01.646359
elapsed time: 0:06:58.734661
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 20:16:10.944497
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.79
 ---- batch: 020 ----
mean loss: 97.14
 ---- batch: 030 ----
mean loss: 102.92
 ---- batch: 040 ----
mean loss: 103.05
 ---- batch: 050 ----
mean loss: 100.53
 ---- batch: 060 ----
mean loss: 102.47
 ---- batch: 070 ----
mean loss: 105.49
 ---- batch: 080 ----
mean loss: 101.49
 ---- batch: 090 ----
mean loss: 99.57
train mean loss: 101.20
epoch train time: 0:00:01.657004
elapsed time: 0:07:00.392321
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 20:16:12.602168
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.49
 ---- batch: 020 ----
mean loss: 96.49
 ---- batch: 030 ----
mean loss: 100.90
 ---- batch: 040 ----
mean loss: 96.58
 ---- batch: 050 ----
mean loss: 102.33
 ---- batch: 060 ----
mean loss: 98.87
 ---- batch: 070 ----
mean loss: 96.82
 ---- batch: 080 ----
mean loss: 108.12
 ---- batch: 090 ----
mean loss: 106.80
train mean loss: 101.60
epoch train time: 0:00:01.696812
elapsed time: 0:07:02.090139
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 20:16:14.299731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.85
 ---- batch: 020 ----
mean loss: 104.04
 ---- batch: 030 ----
mean loss: 98.03
 ---- batch: 040 ----
mean loss: 105.18
 ---- batch: 050 ----
mean loss: 103.03
 ---- batch: 060 ----
mean loss: 101.18
 ---- batch: 070 ----
mean loss: 98.02
 ---- batch: 080 ----
mean loss: 103.76
 ---- batch: 090 ----
mean loss: 102.98
train mean loss: 101.26
epoch train time: 0:00:01.655427
elapsed time: 0:07:03.745979
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 20:16:15.955828
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.98
 ---- batch: 020 ----
mean loss: 106.84
 ---- batch: 030 ----
mean loss: 99.83
 ---- batch: 040 ----
mean loss: 96.89
 ---- batch: 050 ----
mean loss: 102.50
 ---- batch: 060 ----
mean loss: 100.52
 ---- batch: 070 ----
mean loss: 102.50
 ---- batch: 080 ----
mean loss: 98.76
 ---- batch: 090 ----
mean loss: 101.16
train mean loss: 101.16
epoch train time: 0:00:01.656226
elapsed time: 0:07:05.402828
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 20:16:17.612682
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.41
 ---- batch: 020 ----
mean loss: 101.26
 ---- batch: 030 ----
mean loss: 102.43
 ---- batch: 040 ----
mean loss: 106.66
 ---- batch: 050 ----
mean loss: 102.14
 ---- batch: 060 ----
mean loss: 99.15
 ---- batch: 070 ----
mean loss: 97.41
 ---- batch: 080 ----
mean loss: 98.66
 ---- batch: 090 ----
mean loss: 102.00
train mean loss: 101.27
epoch train time: 0:00:01.656172
elapsed time: 0:07:07.059667
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 20:16:19.269538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.45
 ---- batch: 020 ----
mean loss: 100.31
 ---- batch: 030 ----
mean loss: 107.72
 ---- batch: 040 ----
mean loss: 100.37
 ---- batch: 050 ----
mean loss: 99.65
 ---- batch: 060 ----
mean loss: 97.44
 ---- batch: 070 ----
mean loss: 97.85
 ---- batch: 080 ----
mean loss: 106.27
 ---- batch: 090 ----
mean loss: 101.26
train mean loss: 101.39
epoch train time: 0:00:01.660719
elapsed time: 0:07:08.721059
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 20:16:20.930941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.20
 ---- batch: 020 ----
mean loss: 101.04
 ---- batch: 030 ----
mean loss: 103.41
 ---- batch: 040 ----
mean loss: 96.22
 ---- batch: 050 ----
mean loss: 101.53
 ---- batch: 060 ----
mean loss: 99.20
 ---- batch: 070 ----
mean loss: 102.83
 ---- batch: 080 ----
mean loss: 99.36
 ---- batch: 090 ----
mean loss: 101.16
train mean loss: 100.99
epoch train time: 0:00:01.618119
elapsed time: 0:07:10.339880
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 20:16:22.549798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.53
 ---- batch: 020 ----
mean loss: 100.33
 ---- batch: 030 ----
mean loss: 101.96
 ---- batch: 040 ----
mean loss: 96.27
 ---- batch: 050 ----
mean loss: 103.20
 ---- batch: 060 ----
mean loss: 98.66
 ---- batch: 070 ----
mean loss: 97.82
 ---- batch: 080 ----
mean loss: 103.98
 ---- batch: 090 ----
mean loss: 105.57
train mean loss: 101.06
epoch train time: 0:00:01.632494
elapsed time: 0:07:11.973110
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 20:16:24.182951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.94
 ---- batch: 020 ----
mean loss: 106.70
 ---- batch: 030 ----
mean loss: 99.69
 ---- batch: 040 ----
mean loss: 95.77
 ---- batch: 050 ----
mean loss: 102.22
 ---- batch: 060 ----
mean loss: 99.08
 ---- batch: 070 ----
mean loss: 101.42
 ---- batch: 080 ----
mean loss: 100.64
 ---- batch: 090 ----
mean loss: 96.18
train mean loss: 101.05
epoch train time: 0:00:01.655278
elapsed time: 0:07:13.629067
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 20:16:25.838912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.70
 ---- batch: 020 ----
mean loss: 103.20
 ---- batch: 030 ----
mean loss: 102.59
 ---- batch: 040 ----
mean loss: 98.53
 ---- batch: 050 ----
mean loss: 103.24
 ---- batch: 060 ----
mean loss: 94.77
 ---- batch: 070 ----
mean loss: 100.94
 ---- batch: 080 ----
mean loss: 100.89
 ---- batch: 090 ----
mean loss: 99.86
train mean loss: 101.10
epoch train time: 0:00:01.645645
elapsed time: 0:07:15.275347
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 20:16:27.485257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.86
 ---- batch: 020 ----
mean loss: 95.57
 ---- batch: 030 ----
mean loss: 101.13
 ---- batch: 040 ----
mean loss: 102.25
 ---- batch: 050 ----
mean loss: 96.35
 ---- batch: 060 ----
mean loss: 102.66
 ---- batch: 070 ----
mean loss: 104.42
 ---- batch: 080 ----
mean loss: 99.38
 ---- batch: 090 ----
mean loss: 101.37
train mean loss: 100.84
epoch train time: 0:00:01.644052
elapsed time: 0:07:16.920165
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 20:16:29.130041
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.03
 ---- batch: 020 ----
mean loss: 100.81
 ---- batch: 030 ----
mean loss: 96.99
 ---- batch: 040 ----
mean loss: 95.51
 ---- batch: 050 ----
mean loss: 96.84
 ---- batch: 060 ----
mean loss: 106.38
 ---- batch: 070 ----
mean loss: 103.94
 ---- batch: 080 ----
mean loss: 101.07
 ---- batch: 090 ----
mean loss: 105.69
train mean loss: 100.89
epoch train time: 0:00:01.661989
elapsed time: 0:07:18.582819
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 20:16:30.792771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.38
 ---- batch: 020 ----
mean loss: 106.33
 ---- batch: 030 ----
mean loss: 98.01
 ---- batch: 040 ----
mean loss: 100.17
 ---- batch: 050 ----
mean loss: 100.16
 ---- batch: 060 ----
mean loss: 96.64
 ---- batch: 070 ----
mean loss: 103.62
 ---- batch: 080 ----
mean loss: 99.80
 ---- batch: 090 ----
mean loss: 104.25
train mean loss: 100.87
epoch train time: 0:00:01.631061
elapsed time: 0:07:20.214614
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 20:16:32.424449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.20
 ---- batch: 020 ----
mean loss: 100.90
 ---- batch: 030 ----
mean loss: 98.59
 ---- batch: 040 ----
mean loss: 100.00
 ---- batch: 050 ----
mean loss: 103.66
 ---- batch: 060 ----
mean loss: 102.86
 ---- batch: 070 ----
mean loss: 99.60
 ---- batch: 080 ----
mean loss: 104.56
 ---- batch: 090 ----
mean loss: 104.30
train mean loss: 100.79
epoch train time: 0:00:01.645399
elapsed time: 0:07:21.860628
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 20:16:34.070496
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.31
 ---- batch: 020 ----
mean loss: 99.42
 ---- batch: 030 ----
mean loss: 97.91
 ---- batch: 040 ----
mean loss: 105.46
 ---- batch: 050 ----
mean loss: 101.45
 ---- batch: 060 ----
mean loss: 100.28
 ---- batch: 070 ----
mean loss: 102.88
 ---- batch: 080 ----
mean loss: 96.57
 ---- batch: 090 ----
mean loss: 102.57
train mean loss: 100.70
epoch train time: 0:00:01.644480
elapsed time: 0:07:23.505791
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 20:16:35.715631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.46
 ---- batch: 020 ----
mean loss: 98.88
 ---- batch: 030 ----
mean loss: 98.50
 ---- batch: 040 ----
mean loss: 102.65
 ---- batch: 050 ----
mean loss: 104.03
 ---- batch: 060 ----
mean loss: 104.55
 ---- batch: 070 ----
mean loss: 95.11
 ---- batch: 080 ----
mean loss: 101.59
 ---- batch: 090 ----
mean loss: 99.66
train mean loss: 100.74
epoch train time: 0:00:01.671973
elapsed time: 0:07:25.178348
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 20:16:37.388207
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.95
 ---- batch: 020 ----
mean loss: 97.73
 ---- batch: 030 ----
mean loss: 100.80
 ---- batch: 040 ----
mean loss: 98.25
 ---- batch: 050 ----
mean loss: 103.19
 ---- batch: 060 ----
mean loss: 104.92
 ---- batch: 070 ----
mean loss: 103.95
 ---- batch: 080 ----
mean loss: 98.47
 ---- batch: 090 ----
mean loss: 97.98
train mean loss: 100.78
epoch train time: 0:00:01.654181
elapsed time: 0:07:26.833166
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 20:16:39.043032
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.67
 ---- batch: 020 ----
mean loss: 103.48
 ---- batch: 030 ----
mean loss: 100.45
 ---- batch: 040 ----
mean loss: 93.81
 ---- batch: 050 ----
mean loss: 98.27
 ---- batch: 060 ----
mean loss: 99.52
 ---- batch: 070 ----
mean loss: 99.59
 ---- batch: 080 ----
mean loss: 102.82
 ---- batch: 090 ----
mean loss: 106.06
train mean loss: 100.50
epoch train time: 0:00:01.636936
elapsed time: 0:07:28.479198
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_6/checkpoint.pth.tar
**** end time: 2019-09-25 20:16:40.688761 ****
