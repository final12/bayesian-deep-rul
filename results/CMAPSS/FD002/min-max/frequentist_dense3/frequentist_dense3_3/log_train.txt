Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_3', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24319
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistDense3...
Done.
**** start time: 2019-09-25 23:10:45.139117 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
            Linear-2                  [-1, 100]          48,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 68,100
Trainable params: 68,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 23:10:45.142595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4309.49
 ---- batch: 020 ----
mean loss: 4137.51
 ---- batch: 030 ----
mean loss: 4111.66
 ---- batch: 040 ----
mean loss: 3989.50
 ---- batch: 050 ----
mean loss: 3852.57
 ---- batch: 060 ----
mean loss: 3890.03
 ---- batch: 070 ----
mean loss: 3760.05
 ---- batch: 080 ----
mean loss: 3754.75
 ---- batch: 090 ----
mean loss: 3696.99
train mean loss: 3925.49
epoch train time: 0:00:34.030813
elapsed time: 0:00:34.036637
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 23:11:19.175793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3534.83
 ---- batch: 020 ----
mean loss: 3571.46
 ---- batch: 030 ----
mean loss: 3502.22
 ---- batch: 040 ----
mean loss: 3455.64
 ---- batch: 050 ----
mean loss: 3335.05
 ---- batch: 060 ----
mean loss: 3339.28
 ---- batch: 070 ----
mean loss: 3281.21
 ---- batch: 080 ----
mean loss: 3201.43
 ---- batch: 090 ----
mean loss: 3176.48
train mean loss: 3362.47
epoch train time: 0:00:00.517796
elapsed time: 0:00:34.554619
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 23:11:19.693790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3055.49
 ---- batch: 020 ----
mean loss: 2994.08
 ---- batch: 030 ----
mean loss: 2979.00
 ---- batch: 040 ----
mean loss: 2931.22
 ---- batch: 050 ----
mean loss: 2896.53
 ---- batch: 060 ----
mean loss: 2843.11
 ---- batch: 070 ----
mean loss: 2826.10
 ---- batch: 080 ----
mean loss: 2750.76
 ---- batch: 090 ----
mean loss: 2674.54
train mean loss: 2872.57
epoch train time: 0:00:00.519744
elapsed time: 0:00:35.074562
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 23:11:20.213739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2630.13
 ---- batch: 020 ----
mean loss: 2552.32
 ---- batch: 030 ----
mean loss: 2502.18
 ---- batch: 040 ----
mean loss: 2518.90
 ---- batch: 050 ----
mean loss: 2454.38
 ---- batch: 060 ----
mean loss: 2435.10
 ---- batch: 070 ----
mean loss: 2348.01
 ---- batch: 080 ----
mean loss: 2327.67
 ---- batch: 090 ----
mean loss: 2320.20
train mean loss: 2442.67
epoch train time: 0:00:00.508416
elapsed time: 0:00:35.583154
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 23:11:20.722316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2221.16
 ---- batch: 020 ----
mean loss: 2209.77
 ---- batch: 030 ----
mean loss: 2153.35
 ---- batch: 040 ----
mean loss: 2119.50
 ---- batch: 050 ----
mean loss: 2134.35
 ---- batch: 060 ----
mean loss: 2048.46
 ---- batch: 070 ----
mean loss: 2040.49
 ---- batch: 080 ----
mean loss: 2041.79
 ---- batch: 090 ----
mean loss: 1984.83
train mean loss: 2095.96
epoch train time: 0:00:00.509059
elapsed time: 0:00:36.092364
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 23:11:21.231524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1909.35
 ---- batch: 020 ----
mean loss: 1878.04
 ---- batch: 030 ----
mean loss: 1887.41
 ---- batch: 040 ----
mean loss: 1840.88
 ---- batch: 050 ----
mean loss: 1875.15
 ---- batch: 060 ----
mean loss: 1768.19
 ---- batch: 070 ----
mean loss: 1789.39
 ---- batch: 080 ----
mean loss: 1791.34
 ---- batch: 090 ----
mean loss: 1709.66
train mean loss: 1818.79
epoch train time: 0:00:00.504023
elapsed time: 0:00:36.596541
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 23:11:21.735723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1685.89
 ---- batch: 020 ----
mean loss: 1677.35
 ---- batch: 030 ----
mean loss: 1594.69
 ---- batch: 040 ----
mean loss: 1625.44
 ---- batch: 050 ----
mean loss: 1594.31
 ---- batch: 060 ----
mean loss: 1587.75
 ---- batch: 070 ----
mean loss: 1551.76
 ---- batch: 080 ----
mean loss: 1539.51
 ---- batch: 090 ----
mean loss: 1523.15
train mean loss: 1589.61
epoch train time: 0:00:00.510472
elapsed time: 0:00:37.107184
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 23:11:22.246345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1464.04
 ---- batch: 020 ----
mean loss: 1470.70
 ---- batch: 030 ----
mean loss: 1470.62
 ---- batch: 040 ----
mean loss: 1395.72
 ---- batch: 050 ----
mean loss: 1432.29
 ---- batch: 060 ----
mean loss: 1409.99
 ---- batch: 070 ----
mean loss: 1362.78
 ---- batch: 080 ----
mean loss: 1382.01
 ---- batch: 090 ----
mean loss: 1340.52
train mean loss: 1408.31
epoch train time: 0:00:00.515679
elapsed time: 0:00:37.623007
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 23:11:22.762178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1331.78
 ---- batch: 020 ----
mean loss: 1290.07
 ---- batch: 030 ----
mean loss: 1288.86
 ---- batch: 040 ----
mean loss: 1273.34
 ---- batch: 050 ----
mean loss: 1296.23
 ---- batch: 060 ----
mean loss: 1242.56
 ---- batch: 070 ----
mean loss: 1258.21
 ---- batch: 080 ----
mean loss: 1214.76
 ---- batch: 090 ----
mean loss: 1242.66
train mean loss: 1268.28
epoch train time: 0:00:00.513853
elapsed time: 0:00:38.137012
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 23:11:23.276182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1212.67
 ---- batch: 020 ----
mean loss: 1184.72
 ---- batch: 030 ----
mean loss: 1190.04
 ---- batch: 040 ----
mean loss: 1144.22
 ---- batch: 050 ----
mean loss: 1172.09
 ---- batch: 060 ----
mean loss: 1153.51
 ---- batch: 070 ----
mean loss: 1163.43
 ---- batch: 080 ----
mean loss: 1119.42
 ---- batch: 090 ----
mean loss: 1134.19
train mean loss: 1159.80
epoch train time: 0:00:00.521755
elapsed time: 0:00:38.658919
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 23:11:23.798086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1090.43
 ---- batch: 020 ----
mean loss: 1109.57
 ---- batch: 030 ----
mean loss: 1104.81
 ---- batch: 040 ----
mean loss: 1073.70
 ---- batch: 050 ----
mean loss: 1072.86
 ---- batch: 060 ----
mean loss: 1086.16
 ---- batch: 070 ----
mean loss: 1070.11
 ---- batch: 080 ----
mean loss: 1040.38
 ---- batch: 090 ----
mean loss: 1062.98
train mean loss: 1077.15
epoch train time: 0:00:00.513616
elapsed time: 0:00:39.172697
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 23:11:24.311929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1024.20
 ---- batch: 020 ----
mean loss: 1034.89
 ---- batch: 030 ----
mean loss: 1038.69
 ---- batch: 040 ----
mean loss: 1026.97
 ---- batch: 050 ----
mean loss: 1020.45
 ---- batch: 060 ----
mean loss: 1014.78
 ---- batch: 070 ----
mean loss: 1015.22
 ---- batch: 080 ----
mean loss: 986.43
 ---- batch: 090 ----
mean loss: 976.53
train mean loss: 1012.17
epoch train time: 0:00:00.513590
elapsed time: 0:00:39.686537
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 23:11:24.825708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 975.02
 ---- batch: 020 ----
mean loss: 968.27
 ---- batch: 030 ----
mean loss: 982.59
 ---- batch: 040 ----
mean loss: 959.92
 ---- batch: 050 ----
mean loss: 987.49
 ---- batch: 060 ----
mean loss: 961.89
 ---- batch: 070 ----
mean loss: 946.09
 ---- batch: 080 ----
mean loss: 955.48
 ---- batch: 090 ----
mean loss: 954.50
train mean loss: 965.08
epoch train time: 0:00:00.504330
elapsed time: 0:00:40.191018
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 23:11:25.330203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.17
 ---- batch: 020 ----
mean loss: 939.09
 ---- batch: 030 ----
mean loss: 934.16
 ---- batch: 040 ----
mean loss: 914.73
 ---- batch: 050 ----
mean loss: 932.53
 ---- batch: 060 ----
mean loss: 927.28
 ---- batch: 070 ----
mean loss: 941.44
 ---- batch: 080 ----
mean loss: 926.47
 ---- batch: 090 ----
mean loss: 925.03
train mean loss: 930.95
epoch train time: 0:00:00.517718
elapsed time: 0:00:40.708930
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 23:11:25.848110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.88
 ---- batch: 020 ----
mean loss: 919.52
 ---- batch: 030 ----
mean loss: 912.57
 ---- batch: 040 ----
mean loss: 897.38
 ---- batch: 050 ----
mean loss: 905.87
 ---- batch: 060 ----
mean loss: 899.92
 ---- batch: 070 ----
mean loss: 901.67
 ---- batch: 080 ----
mean loss: 916.32
 ---- batch: 090 ----
mean loss: 908.20
train mean loss: 909.91
epoch train time: 0:00:00.508000
elapsed time: 0:00:41.217088
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 23:11:26.356256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.58
 ---- batch: 020 ----
mean loss: 899.07
 ---- batch: 030 ----
mean loss: 897.23
 ---- batch: 040 ----
mean loss: 905.80
 ---- batch: 050 ----
mean loss: 899.45
 ---- batch: 060 ----
mean loss: 887.78
 ---- batch: 070 ----
mean loss: 872.40
 ---- batch: 080 ----
mean loss: 893.18
 ---- batch: 090 ----
mean loss: 893.63
train mean loss: 895.28
epoch train time: 0:00:00.512634
elapsed time: 0:00:41.729870
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 23:11:26.869040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.65
 ---- batch: 020 ----
mean loss: 869.51
 ---- batch: 030 ----
mean loss: 884.95
 ---- batch: 040 ----
mean loss: 892.29
 ---- batch: 050 ----
mean loss: 868.69
 ---- batch: 060 ----
mean loss: 893.89
 ---- batch: 070 ----
mean loss: 899.53
 ---- batch: 080 ----
mean loss: 900.73
 ---- batch: 090 ----
mean loss: 871.18
train mean loss: 886.54
epoch train time: 0:00:00.514045
elapsed time: 0:00:42.244066
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 23:11:27.383237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.83
 ---- batch: 020 ----
mean loss: 877.20
 ---- batch: 030 ----
mean loss: 898.90
 ---- batch: 040 ----
mean loss: 889.34
 ---- batch: 050 ----
mean loss: 871.24
 ---- batch: 060 ----
mean loss: 865.72
 ---- batch: 070 ----
mean loss: 878.96
 ---- batch: 080 ----
mean loss: 875.89
 ---- batch: 090 ----
mean loss: 873.53
train mean loss: 880.94
epoch train time: 0:00:00.535861
elapsed time: 0:00:42.780087
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 23:11:27.919265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.24
 ---- batch: 020 ----
mean loss: 886.78
 ---- batch: 030 ----
mean loss: 865.34
 ---- batch: 040 ----
mean loss: 883.65
 ---- batch: 050 ----
mean loss: 880.98
 ---- batch: 060 ----
mean loss: 874.75
 ---- batch: 070 ----
mean loss: 860.43
 ---- batch: 080 ----
mean loss: 891.21
 ---- batch: 090 ----
mean loss: 885.54
train mean loss: 878.27
epoch train time: 0:00:00.509906
elapsed time: 0:00:43.290153
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 23:11:28.429322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.65
 ---- batch: 020 ----
mean loss: 889.32
 ---- batch: 030 ----
mean loss: 876.55
 ---- batch: 040 ----
mean loss: 884.03
 ---- batch: 050 ----
mean loss: 867.23
 ---- batch: 060 ----
mean loss: 878.36
 ---- batch: 070 ----
mean loss: 886.29
 ---- batch: 080 ----
mean loss: 864.26
 ---- batch: 090 ----
mean loss: 872.34
train mean loss: 876.96
epoch train time: 0:00:00.518559
elapsed time: 0:00:43.808864
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 23:11:28.948049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.87
 ---- batch: 020 ----
mean loss: 868.75
 ---- batch: 030 ----
mean loss: 868.04
 ---- batch: 040 ----
mean loss: 875.99
 ---- batch: 050 ----
mean loss: 894.15
 ---- batch: 060 ----
mean loss: 873.75
 ---- batch: 070 ----
mean loss: 857.92
 ---- batch: 080 ----
mean loss: 889.58
 ---- batch: 090 ----
mean loss: 879.36
train mean loss: 875.32
epoch train time: 0:00:00.506460
elapsed time: 0:00:44.315487
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 23:11:29.454682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.40
 ---- batch: 020 ----
mean loss: 888.08
 ---- batch: 030 ----
mean loss: 867.21
 ---- batch: 040 ----
mean loss: 856.62
 ---- batch: 050 ----
mean loss: 868.13
 ---- batch: 060 ----
mean loss: 891.16
 ---- batch: 070 ----
mean loss: 882.15
 ---- batch: 080 ----
mean loss: 871.27
 ---- batch: 090 ----
mean loss: 876.60
train mean loss: 875.96
epoch train time: 0:00:00.530338
elapsed time: 0:00:44.846002
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 23:11:29.985168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.01
 ---- batch: 020 ----
mean loss: 870.63
 ---- batch: 030 ----
mean loss: 861.10
 ---- batch: 040 ----
mean loss: 868.36
 ---- batch: 050 ----
mean loss: 878.79
 ---- batch: 060 ----
mean loss: 879.42
 ---- batch: 070 ----
mean loss: 875.82
 ---- batch: 080 ----
mean loss: 878.42
 ---- batch: 090 ----
mean loss: 885.00
train mean loss: 875.11
epoch train time: 0:00:00.499225
elapsed time: 0:00:45.345378
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 23:11:30.484554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.59
 ---- batch: 020 ----
mean loss: 871.06
 ---- batch: 030 ----
mean loss: 877.14
 ---- batch: 040 ----
mean loss: 858.00
 ---- batch: 050 ----
mean loss: 872.98
 ---- batch: 060 ----
mean loss: 865.60
 ---- batch: 070 ----
mean loss: 881.44
 ---- batch: 080 ----
mean loss: 881.53
 ---- batch: 090 ----
mean loss: 873.41
train mean loss: 876.25
epoch train time: 0:00:00.517996
elapsed time: 0:00:45.863537
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 23:11:31.002709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.75
 ---- batch: 020 ----
mean loss: 889.00
 ---- batch: 030 ----
mean loss: 883.38
 ---- batch: 040 ----
mean loss: 876.09
 ---- batch: 050 ----
mean loss: 881.02
 ---- batch: 060 ----
mean loss: 877.85
 ---- batch: 070 ----
mean loss: 873.89
 ---- batch: 080 ----
mean loss: 865.59
 ---- batch: 090 ----
mean loss: 883.90
train mean loss: 875.20
epoch train time: 0:00:00.505639
elapsed time: 0:00:46.369327
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 23:11:31.508505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.50
 ---- batch: 020 ----
mean loss: 869.74
 ---- batch: 030 ----
mean loss: 863.27
 ---- batch: 040 ----
mean loss: 866.21
 ---- batch: 050 ----
mean loss: 864.49
 ---- batch: 060 ----
mean loss: 902.40
 ---- batch: 070 ----
mean loss: 883.60
 ---- batch: 080 ----
mean loss: 878.40
 ---- batch: 090 ----
mean loss: 869.09
train mean loss: 874.90
epoch train time: 0:00:00.509217
elapsed time: 0:00:46.878705
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 23:11:32.017875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.27
 ---- batch: 020 ----
mean loss: 873.49
 ---- batch: 030 ----
mean loss: 871.79
 ---- batch: 040 ----
mean loss: 872.86
 ---- batch: 050 ----
mean loss: 864.73
 ---- batch: 060 ----
mean loss: 869.84
 ---- batch: 070 ----
mean loss: 879.61
 ---- batch: 080 ----
mean loss: 887.27
 ---- batch: 090 ----
mean loss: 888.06
train mean loss: 875.11
epoch train time: 0:00:00.500687
elapsed time: 0:00:47.379545
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 23:11:32.518715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.94
 ---- batch: 020 ----
mean loss: 869.18
 ---- batch: 030 ----
mean loss: 878.10
 ---- batch: 040 ----
mean loss: 885.38
 ---- batch: 050 ----
mean loss: 873.31
 ---- batch: 060 ----
mean loss: 871.07
 ---- batch: 070 ----
mean loss: 862.61
 ---- batch: 080 ----
mean loss: 892.12
 ---- batch: 090 ----
mean loss: 862.39
train mean loss: 874.90
epoch train time: 0:00:00.505041
elapsed time: 0:00:47.884744
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 23:11:33.023929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.86
 ---- batch: 020 ----
mean loss: 874.97
 ---- batch: 030 ----
mean loss: 861.88
 ---- batch: 040 ----
mean loss: 874.73
 ---- batch: 050 ----
mean loss: 883.70
 ---- batch: 060 ----
mean loss: 883.33
 ---- batch: 070 ----
mean loss: 889.99
 ---- batch: 080 ----
mean loss: 863.96
 ---- batch: 090 ----
mean loss: 859.31
train mean loss: 875.83
epoch train time: 0:00:00.498385
elapsed time: 0:00:48.383290
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 23:11:33.522456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.98
 ---- batch: 020 ----
mean loss: 886.47
 ---- batch: 030 ----
mean loss: 870.55
 ---- batch: 040 ----
mean loss: 876.61
 ---- batch: 050 ----
mean loss: 875.43
 ---- batch: 060 ----
mean loss: 878.11
 ---- batch: 070 ----
mean loss: 872.73
 ---- batch: 080 ----
mean loss: 870.15
 ---- batch: 090 ----
mean loss: 857.33
train mean loss: 874.97
epoch train time: 0:00:00.515253
elapsed time: 0:00:48.898690
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 23:11:34.037860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.88
 ---- batch: 020 ----
mean loss: 871.10
 ---- batch: 030 ----
mean loss: 868.37
 ---- batch: 040 ----
mean loss: 878.46
 ---- batch: 050 ----
mean loss: 892.61
 ---- batch: 060 ----
mean loss: 866.61
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 868.92
 ---- batch: 090 ----
mean loss: 861.10
train mean loss: 875.07
epoch train time: 0:00:00.505855
elapsed time: 0:00:49.404692
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 23:11:34.543859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.11
 ---- batch: 020 ----
mean loss: 879.14
 ---- batch: 030 ----
mean loss: 874.04
 ---- batch: 040 ----
mean loss: 868.00
 ---- batch: 050 ----
mean loss: 876.37
 ---- batch: 060 ----
mean loss: 886.71
 ---- batch: 070 ----
mean loss: 861.68
 ---- batch: 080 ----
mean loss: 882.78
 ---- batch: 090 ----
mean loss: 870.74
train mean loss: 874.91
epoch train time: 0:00:00.512340
elapsed time: 0:00:49.917202
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 23:11:35.056362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.46
 ---- batch: 020 ----
mean loss: 876.58
 ---- batch: 030 ----
mean loss: 877.96
 ---- batch: 040 ----
mean loss: 870.24
 ---- batch: 050 ----
mean loss: 869.36
 ---- batch: 060 ----
mean loss: 868.26
 ---- batch: 070 ----
mean loss: 868.20
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 877.78
train mean loss: 876.12
epoch train time: 0:00:00.498633
elapsed time: 0:00:50.415978
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 23:11:35.555135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.88
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 866.29
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 889.38
 ---- batch: 060 ----
mean loss: 879.60
 ---- batch: 070 ----
mean loss: 884.78
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 873.48
train mean loss: 875.11
epoch train time: 0:00:00.512228
elapsed time: 0:00:50.928382
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 23:11:36.067552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.81
 ---- batch: 020 ----
mean loss: 872.41
 ---- batch: 030 ----
mean loss: 858.52
 ---- batch: 040 ----
mean loss: 878.56
 ---- batch: 050 ----
mean loss: 870.22
 ---- batch: 060 ----
mean loss: 889.60
 ---- batch: 070 ----
mean loss: 878.83
 ---- batch: 080 ----
mean loss: 875.94
 ---- batch: 090 ----
mean loss: 887.80
train mean loss: 875.71
epoch train time: 0:00:00.507736
elapsed time: 0:00:51.436275
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 23:11:36.575482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 870.46
 ---- batch: 030 ----
mean loss: 875.57
 ---- batch: 040 ----
mean loss: 886.46
 ---- batch: 050 ----
mean loss: 888.80
 ---- batch: 060 ----
mean loss: 856.43
 ---- batch: 070 ----
mean loss: 861.54
 ---- batch: 080 ----
mean loss: 865.08
 ---- batch: 090 ----
mean loss: 908.22
train mean loss: 875.33
epoch train time: 0:00:00.508959
elapsed time: 0:00:51.945479
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 23:11:37.084658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.66
 ---- batch: 020 ----
mean loss: 867.37
 ---- batch: 030 ----
mean loss: 881.86
 ---- batch: 040 ----
mean loss: 896.14
 ---- batch: 050 ----
mean loss: 895.66
 ---- batch: 060 ----
mean loss: 872.61
 ---- batch: 070 ----
mean loss: 871.27
 ---- batch: 080 ----
mean loss: 850.89
 ---- batch: 090 ----
mean loss: 868.18
train mean loss: 874.80
epoch train time: 0:00:00.498651
elapsed time: 0:00:52.444291
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 23:11:37.583451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 879.71
 ---- batch: 040 ----
mean loss: 880.95
 ---- batch: 050 ----
mean loss: 865.29
 ---- batch: 060 ----
mean loss: 882.77
 ---- batch: 070 ----
mean loss: 869.99
 ---- batch: 080 ----
mean loss: 882.00
 ---- batch: 090 ----
mean loss: 857.67
train mean loss: 875.43
epoch train time: 0:00:00.528104
elapsed time: 0:00:52.972572
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 23:11:38.111732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 874.52
 ---- batch: 030 ----
mean loss: 869.88
 ---- batch: 040 ----
mean loss: 872.46
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 884.13
 ---- batch: 070 ----
mean loss: 882.33
 ---- batch: 080 ----
mean loss: 868.00
 ---- batch: 090 ----
mean loss: 884.71
train mean loss: 875.94
epoch train time: 0:00:00.505330
elapsed time: 0:00:53.478039
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 23:11:38.617210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.36
 ---- batch: 020 ----
mean loss: 863.17
 ---- batch: 030 ----
mean loss: 868.58
 ---- batch: 040 ----
mean loss: 872.91
 ---- batch: 050 ----
mean loss: 875.23
 ---- batch: 060 ----
mean loss: 874.13
 ---- batch: 070 ----
mean loss: 875.79
 ---- batch: 080 ----
mean loss: 872.87
 ---- batch: 090 ----
mean loss: 880.58
train mean loss: 875.20
epoch train time: 0:00:00.517305
elapsed time: 0:00:53.995525
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 23:11:39.134686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
 ---- batch: 020 ----
mean loss: 888.19
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 864.90
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 877.49
 ---- batch: 070 ----
mean loss: 873.63
 ---- batch: 080 ----
mean loss: 861.95
 ---- batch: 090 ----
mean loss: 867.30
train mean loss: 874.55
epoch train time: 0:00:00.516020
elapsed time: 0:00:54.511696
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 23:11:39.650869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.19
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 878.84
 ---- batch: 040 ----
mean loss: 868.87
 ---- batch: 050 ----
mean loss: 860.44
 ---- batch: 060 ----
mean loss: 885.18
 ---- batch: 070 ----
mean loss: 876.98
 ---- batch: 080 ----
mean loss: 875.13
 ---- batch: 090 ----
mean loss: 883.68
train mean loss: 875.45
epoch train time: 0:00:00.520425
elapsed time: 0:00:55.032277
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 23:11:40.171457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.89
 ---- batch: 020 ----
mean loss: 862.14
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 852.48
 ---- batch: 060 ----
mean loss: 873.50
 ---- batch: 070 ----
mean loss: 889.67
 ---- batch: 080 ----
mean loss: 893.94
 ---- batch: 090 ----
mean loss: 900.18
train mean loss: 875.66
epoch train time: 0:00:00.505877
elapsed time: 0:00:55.538321
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 23:11:40.677508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.25
 ---- batch: 020 ----
mean loss: 859.42
 ---- batch: 030 ----
mean loss: 880.51
 ---- batch: 040 ----
mean loss: 889.92
 ---- batch: 050 ----
mean loss: 869.13
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 886.11
 ---- batch: 090 ----
mean loss: 885.30
train mean loss: 875.24
epoch train time: 0:00:00.507022
elapsed time: 0:00:56.045508
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 23:11:41.184678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.92
 ---- batch: 020 ----
mean loss: 896.00
 ---- batch: 030 ----
mean loss: 895.89
 ---- batch: 040 ----
mean loss: 875.86
 ---- batch: 050 ----
mean loss: 854.47
 ---- batch: 060 ----
mean loss: 885.08
 ---- batch: 070 ----
mean loss: 872.56
 ---- batch: 080 ----
mean loss: 874.54
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 875.66
epoch train time: 0:00:00.501369
elapsed time: 0:00:56.547029
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 23:11:41.686198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.48
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 875.70
 ---- batch: 040 ----
mean loss: 877.23
 ---- batch: 050 ----
mean loss: 878.31
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.63
 ---- batch: 080 ----
mean loss: 876.95
 ---- batch: 090 ----
mean loss: 864.58
train mean loss: 874.88
epoch train time: 0:00:00.512164
elapsed time: 0:00:57.059341
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 23:11:42.198508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 874.57
 ---- batch: 030 ----
mean loss: 885.12
 ---- batch: 040 ----
mean loss: 870.30
 ---- batch: 050 ----
mean loss: 885.77
 ---- batch: 060 ----
mean loss: 877.45
 ---- batch: 070 ----
mean loss: 893.99
 ---- batch: 080 ----
mean loss: 865.68
 ---- batch: 090 ----
mean loss: 871.94
train mean loss: 875.10
epoch train time: 0:00:00.508723
elapsed time: 0:00:57.568220
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 23:11:42.707388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.70
 ---- batch: 020 ----
mean loss: 867.95
 ---- batch: 030 ----
mean loss: 868.70
 ---- batch: 040 ----
mean loss: 880.39
 ---- batch: 050 ----
mean loss: 874.64
 ---- batch: 060 ----
mean loss: 876.81
 ---- batch: 070 ----
mean loss: 871.93
 ---- batch: 080 ----
mean loss: 877.42
 ---- batch: 090 ----
mean loss: 896.45
train mean loss: 874.83
epoch train time: 0:00:00.503766
elapsed time: 0:00:58.072133
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 23:11:43.211303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.02
 ---- batch: 020 ----
mean loss: 873.20
 ---- batch: 030 ----
mean loss: 838.26
 ---- batch: 040 ----
mean loss: 854.58
 ---- batch: 050 ----
mean loss: 819.70
 ---- batch: 060 ----
mean loss: 763.23
 ---- batch: 070 ----
mean loss: 728.92
 ---- batch: 080 ----
mean loss: 674.43
 ---- batch: 090 ----
mean loss: 620.60
train mean loss: 769.51
epoch train time: 0:00:00.503728
elapsed time: 0:00:58.576022
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 23:11:43.715212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.10
 ---- batch: 020 ----
mean loss: 466.48
 ---- batch: 030 ----
mean loss: 447.07
 ---- batch: 040 ----
mean loss: 422.95
 ---- batch: 050 ----
mean loss: 414.15
 ---- batch: 060 ----
mean loss: 414.57
 ---- batch: 070 ----
mean loss: 386.07
 ---- batch: 080 ----
mean loss: 394.31
 ---- batch: 090 ----
mean loss: 371.13
train mean loss: 422.29
epoch train time: 0:00:00.507160
elapsed time: 0:00:59.083350
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 23:11:44.222517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.44
 ---- batch: 020 ----
mean loss: 354.91
 ---- batch: 030 ----
mean loss: 348.56
 ---- batch: 040 ----
mean loss: 332.73
 ---- batch: 050 ----
mean loss: 326.70
 ---- batch: 060 ----
mean loss: 322.32
 ---- batch: 070 ----
mean loss: 327.87
 ---- batch: 080 ----
mean loss: 309.92
 ---- batch: 090 ----
mean loss: 313.00
train mean loss: 331.56
epoch train time: 0:00:00.498331
elapsed time: 0:00:59.581824
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 23:11:44.721036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.50
 ---- batch: 020 ----
mean loss: 293.92
 ---- batch: 030 ----
mean loss: 300.95
 ---- batch: 040 ----
mean loss: 298.60
 ---- batch: 050 ----
mean loss: 282.40
 ---- batch: 060 ----
mean loss: 285.74
 ---- batch: 070 ----
mean loss: 274.71
 ---- batch: 080 ----
mean loss: 280.35
 ---- batch: 090 ----
mean loss: 282.43
train mean loss: 287.64
epoch train time: 0:00:00.508027
elapsed time: 0:01:00.090057
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 23:11:45.229257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.02
 ---- batch: 020 ----
mean loss: 269.59
 ---- batch: 030 ----
mean loss: 266.16
 ---- batch: 040 ----
mean loss: 262.45
 ---- batch: 050 ----
mean loss: 270.40
 ---- batch: 060 ----
mean loss: 271.83
 ---- batch: 070 ----
mean loss: 253.60
 ---- batch: 080 ----
mean loss: 257.44
 ---- batch: 090 ----
mean loss: 263.83
train mean loss: 265.14
epoch train time: 0:00:00.521277
elapsed time: 0:01:00.611516
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 23:11:45.750685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.14
 ---- batch: 020 ----
mean loss: 253.66
 ---- batch: 030 ----
mean loss: 265.47
 ---- batch: 040 ----
mean loss: 249.00
 ---- batch: 050 ----
mean loss: 248.03
 ---- batch: 060 ----
mean loss: 248.21
 ---- batch: 070 ----
mean loss: 241.28
 ---- batch: 080 ----
mean loss: 259.23
 ---- batch: 090 ----
mean loss: 246.68
train mean loss: 251.54
epoch train time: 0:00:00.519035
elapsed time: 0:01:01.130708
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 23:11:46.269880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.02
 ---- batch: 020 ----
mean loss: 248.93
 ---- batch: 030 ----
mean loss: 245.54
 ---- batch: 040 ----
mean loss: 245.13
 ---- batch: 050 ----
mean loss: 243.00
 ---- batch: 060 ----
mean loss: 243.98
 ---- batch: 070 ----
mean loss: 252.08
 ---- batch: 080 ----
mean loss: 231.36
 ---- batch: 090 ----
mean loss: 239.90
train mean loss: 243.69
epoch train time: 0:00:00.516889
elapsed time: 0:01:01.647747
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 23:11:46.786915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.09
 ---- batch: 020 ----
mean loss: 235.85
 ---- batch: 030 ----
mean loss: 237.95
 ---- batch: 040 ----
mean loss: 239.52
 ---- batch: 050 ----
mean loss: 236.14
 ---- batch: 060 ----
mean loss: 238.95
 ---- batch: 070 ----
mean loss: 242.21
 ---- batch: 080 ----
mean loss: 225.98
 ---- batch: 090 ----
mean loss: 226.38
train mean loss: 236.26
epoch train time: 0:00:00.504848
elapsed time: 0:01:02.152742
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 23:11:47.291911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.08
 ---- batch: 020 ----
mean loss: 227.69
 ---- batch: 030 ----
mean loss: 233.29
 ---- batch: 040 ----
mean loss: 235.10
 ---- batch: 050 ----
mean loss: 230.92
 ---- batch: 060 ----
mean loss: 231.85
 ---- batch: 070 ----
mean loss: 231.86
 ---- batch: 080 ----
mean loss: 230.62
 ---- batch: 090 ----
mean loss: 233.82
train mean loss: 231.49
epoch train time: 0:00:00.503504
elapsed time: 0:01:02.656396
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 23:11:47.795588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.38
 ---- batch: 020 ----
mean loss: 229.73
 ---- batch: 030 ----
mean loss: 230.82
 ---- batch: 040 ----
mean loss: 225.26
 ---- batch: 050 ----
mean loss: 230.10
 ---- batch: 060 ----
mean loss: 230.02
 ---- batch: 070 ----
mean loss: 221.01
 ---- batch: 080 ----
mean loss: 225.31
 ---- batch: 090 ----
mean loss: 229.84
train mean loss: 227.42
epoch train time: 0:00:00.507066
elapsed time: 0:01:03.163663
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 23:11:48.302841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.47
 ---- batch: 020 ----
mean loss: 218.23
 ---- batch: 030 ----
mean loss: 224.26
 ---- batch: 040 ----
mean loss: 229.44
 ---- batch: 050 ----
mean loss: 222.75
 ---- batch: 060 ----
mean loss: 224.43
 ---- batch: 070 ----
mean loss: 221.75
 ---- batch: 080 ----
mean loss: 226.35
 ---- batch: 090 ----
mean loss: 231.41
train mean loss: 224.92
epoch train time: 0:00:00.506610
elapsed time: 0:01:03.670432
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 23:11:48.809600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.04
 ---- batch: 020 ----
mean loss: 231.07
 ---- batch: 030 ----
mean loss: 217.07
 ---- batch: 040 ----
mean loss: 218.64
 ---- batch: 050 ----
mean loss: 221.61
 ---- batch: 060 ----
mean loss: 224.92
 ---- batch: 070 ----
mean loss: 221.44
 ---- batch: 080 ----
mean loss: 218.92
 ---- batch: 090 ----
mean loss: 223.55
train mean loss: 222.32
epoch train time: 0:00:00.503831
elapsed time: 0:01:04.174418
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 23:11:49.313586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.39
 ---- batch: 020 ----
mean loss: 222.89
 ---- batch: 030 ----
mean loss: 218.03
 ---- batch: 040 ----
mean loss: 218.41
 ---- batch: 050 ----
mean loss: 212.99
 ---- batch: 060 ----
mean loss: 217.36
 ---- batch: 070 ----
mean loss: 221.94
 ---- batch: 080 ----
mean loss: 221.07
 ---- batch: 090 ----
mean loss: 219.95
train mean loss: 218.92
epoch train time: 0:00:00.503125
elapsed time: 0:01:04.677709
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 23:11:49.816875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.29
 ---- batch: 020 ----
mean loss: 214.19
 ---- batch: 030 ----
mean loss: 209.06
 ---- batch: 040 ----
mean loss: 213.58
 ---- batch: 050 ----
mean loss: 218.11
 ---- batch: 060 ----
mean loss: 224.42
 ---- batch: 070 ----
mean loss: 214.15
 ---- batch: 080 ----
mean loss: 208.62
 ---- batch: 090 ----
mean loss: 227.94
train mean loss: 216.57
epoch train time: 0:00:00.518893
elapsed time: 0:01:05.196776
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 23:11:50.335938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.71
 ---- batch: 020 ----
mean loss: 205.43
 ---- batch: 030 ----
mean loss: 216.55
 ---- batch: 040 ----
mean loss: 203.73
 ---- batch: 050 ----
mean loss: 215.83
 ---- batch: 060 ----
mean loss: 219.07
 ---- batch: 070 ----
mean loss: 220.90
 ---- batch: 080 ----
mean loss: 218.75
 ---- batch: 090 ----
mean loss: 212.39
train mean loss: 213.94
epoch train time: 0:00:00.523671
elapsed time: 0:01:05.720614
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 23:11:50.859776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.30
 ---- batch: 020 ----
mean loss: 215.44
 ---- batch: 030 ----
mean loss: 209.78
 ---- batch: 040 ----
mean loss: 212.19
 ---- batch: 050 ----
mean loss: 222.07
 ---- batch: 060 ----
mean loss: 208.02
 ---- batch: 070 ----
mean loss: 208.27
 ---- batch: 080 ----
mean loss: 209.00
 ---- batch: 090 ----
mean loss: 206.79
train mean loss: 211.29
epoch train time: 0:00:00.506343
elapsed time: 0:01:06.227113
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 23:11:51.366281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.10
 ---- batch: 020 ----
mean loss: 205.61
 ---- batch: 030 ----
mean loss: 216.39
 ---- batch: 040 ----
mean loss: 215.27
 ---- batch: 050 ----
mean loss: 214.92
 ---- batch: 060 ----
mean loss: 212.99
 ---- batch: 070 ----
mean loss: 205.68
 ---- batch: 080 ----
mean loss: 205.77
 ---- batch: 090 ----
mean loss: 205.50
train mean loss: 210.47
epoch train time: 0:00:00.502638
elapsed time: 0:01:06.729931
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 23:11:51.869091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.79
 ---- batch: 020 ----
mean loss: 205.67
 ---- batch: 030 ----
mean loss: 209.04
 ---- batch: 040 ----
mean loss: 204.91
 ---- batch: 050 ----
mean loss: 207.93
 ---- batch: 060 ----
mean loss: 209.96
 ---- batch: 070 ----
mean loss: 206.59
 ---- batch: 080 ----
mean loss: 214.22
 ---- batch: 090 ----
mean loss: 205.24
train mean loss: 207.41
epoch train time: 0:00:00.510429
elapsed time: 0:01:07.240506
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 23:11:52.379692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.03
 ---- batch: 020 ----
mean loss: 200.81
 ---- batch: 030 ----
mean loss: 213.81
 ---- batch: 040 ----
mean loss: 211.74
 ---- batch: 050 ----
mean loss: 207.42
 ---- batch: 060 ----
mean loss: 213.20
 ---- batch: 070 ----
mean loss: 200.22
 ---- batch: 080 ----
mean loss: 201.16
 ---- batch: 090 ----
mean loss: 197.85
train mean loss: 204.95
epoch train time: 0:00:00.516864
elapsed time: 0:01:07.757553
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 23:11:52.896723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.64
 ---- batch: 020 ----
mean loss: 205.63
 ---- batch: 030 ----
mean loss: 197.79
 ---- batch: 040 ----
mean loss: 198.03
 ---- batch: 050 ----
mean loss: 200.68
 ---- batch: 060 ----
mean loss: 199.38
 ---- batch: 070 ----
mean loss: 204.14
 ---- batch: 080 ----
mean loss: 205.04
 ---- batch: 090 ----
mean loss: 194.82
train mean loss: 201.55
epoch train time: 0:00:00.507098
elapsed time: 0:01:08.264804
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 23:11:53.403982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.07
 ---- batch: 020 ----
mean loss: 202.20
 ---- batch: 030 ----
mean loss: 193.01
 ---- batch: 040 ----
mean loss: 198.71
 ---- batch: 050 ----
mean loss: 197.49
 ---- batch: 060 ----
mean loss: 199.92
 ---- batch: 070 ----
mean loss: 209.58
 ---- batch: 080 ----
mean loss: 201.56
 ---- batch: 090 ----
mean loss: 203.91
train mean loss: 200.94
epoch train time: 0:00:00.522436
elapsed time: 0:01:08.787438
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 23:11:53.926640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.09
 ---- batch: 020 ----
mean loss: 194.83
 ---- batch: 030 ----
mean loss: 194.99
 ---- batch: 040 ----
mean loss: 196.34
 ---- batch: 050 ----
mean loss: 196.02
 ---- batch: 060 ----
mean loss: 196.57
 ---- batch: 070 ----
mean loss: 190.61
 ---- batch: 080 ----
mean loss: 203.74
 ---- batch: 090 ----
mean loss: 204.22
train mean loss: 199.07
epoch train time: 0:00:00.505369
elapsed time: 0:01:09.293011
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 23:11:54.432185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.85
 ---- batch: 020 ----
mean loss: 187.80
 ---- batch: 030 ----
mean loss: 195.18
 ---- batch: 040 ----
mean loss: 204.69
 ---- batch: 050 ----
mean loss: 197.66
 ---- batch: 060 ----
mean loss: 195.60
 ---- batch: 070 ----
mean loss: 196.98
 ---- batch: 080 ----
mean loss: 195.01
 ---- batch: 090 ----
mean loss: 197.77
train mean loss: 195.62
epoch train time: 0:00:00.510154
elapsed time: 0:01:09.803321
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 23:11:54.942491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.07
 ---- batch: 020 ----
mean loss: 195.07
 ---- batch: 030 ----
mean loss: 194.78
 ---- batch: 040 ----
mean loss: 197.88
 ---- batch: 050 ----
mean loss: 196.73
 ---- batch: 060 ----
mean loss: 199.08
 ---- batch: 070 ----
mean loss: 194.24
 ---- batch: 080 ----
mean loss: 200.30
 ---- batch: 090 ----
mean loss: 197.66
train mean loss: 196.34
epoch train time: 0:00:00.504951
elapsed time: 0:01:10.308418
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 23:11:55.447617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.19
 ---- batch: 020 ----
mean loss: 178.45
 ---- batch: 030 ----
mean loss: 197.88
 ---- batch: 040 ----
mean loss: 192.60
 ---- batch: 050 ----
mean loss: 203.47
 ---- batch: 060 ----
mean loss: 195.67
 ---- batch: 070 ----
mean loss: 195.56
 ---- batch: 080 ----
mean loss: 200.39
 ---- batch: 090 ----
mean loss: 197.43
train mean loss: 193.67
epoch train time: 0:00:00.516029
elapsed time: 0:01:10.824638
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 23:11:55.963821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.54
 ---- batch: 020 ----
mean loss: 192.48
 ---- batch: 030 ----
mean loss: 199.28
 ---- batch: 040 ----
mean loss: 189.94
 ---- batch: 050 ----
mean loss: 189.68
 ---- batch: 060 ----
mean loss: 195.03
 ---- batch: 070 ----
mean loss: 191.06
 ---- batch: 080 ----
mean loss: 196.73
 ---- batch: 090 ----
mean loss: 193.79
train mean loss: 192.87
epoch train time: 0:00:00.517027
elapsed time: 0:01:11.341843
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 23:11:56.481027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.72
 ---- batch: 020 ----
mean loss: 189.78
 ---- batch: 030 ----
mean loss: 188.16
 ---- batch: 040 ----
mean loss: 195.36
 ---- batch: 050 ----
mean loss: 189.22
 ---- batch: 060 ----
mean loss: 193.92
 ---- batch: 070 ----
mean loss: 189.70
 ---- batch: 080 ----
mean loss: 192.64
 ---- batch: 090 ----
mean loss: 194.29
train mean loss: 191.65
epoch train time: 0:00:00.508808
elapsed time: 0:01:11.850814
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 23:11:56.989981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.63
 ---- batch: 020 ----
mean loss: 191.62
 ---- batch: 030 ----
mean loss: 189.38
 ---- batch: 040 ----
mean loss: 194.55
 ---- batch: 050 ----
mean loss: 189.68
 ---- batch: 060 ----
mean loss: 186.94
 ---- batch: 070 ----
mean loss: 194.30
 ---- batch: 080 ----
mean loss: 191.08
 ---- batch: 090 ----
mean loss: 191.34
train mean loss: 190.55
epoch train time: 0:00:00.505891
elapsed time: 0:01:12.356860
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 23:11:57.496039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.25
 ---- batch: 020 ----
mean loss: 188.41
 ---- batch: 030 ----
mean loss: 178.44
 ---- batch: 040 ----
mean loss: 186.78
 ---- batch: 050 ----
mean loss: 195.76
 ---- batch: 060 ----
mean loss: 189.91
 ---- batch: 070 ----
mean loss: 197.24
 ---- batch: 080 ----
mean loss: 197.12
 ---- batch: 090 ----
mean loss: 187.37
train mean loss: 189.57
epoch train time: 0:00:00.515589
elapsed time: 0:01:12.872610
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 23:11:58.011778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.38
 ---- batch: 020 ----
mean loss: 190.36
 ---- batch: 030 ----
mean loss: 185.28
 ---- batch: 040 ----
mean loss: 194.26
 ---- batch: 050 ----
mean loss: 179.02
 ---- batch: 060 ----
mean loss: 186.62
 ---- batch: 070 ----
mean loss: 189.77
 ---- batch: 080 ----
mean loss: 191.10
 ---- batch: 090 ----
mean loss: 187.67
train mean loss: 187.09
epoch train time: 0:00:00.512181
elapsed time: 0:01:13.384935
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 23:11:58.524117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.05
 ---- batch: 020 ----
mean loss: 184.36
 ---- batch: 030 ----
mean loss: 181.66
 ---- batch: 040 ----
mean loss: 195.47
 ---- batch: 050 ----
mean loss: 186.95
 ---- batch: 060 ----
mean loss: 180.18
 ---- batch: 070 ----
mean loss: 197.20
 ---- batch: 080 ----
mean loss: 187.38
 ---- batch: 090 ----
mean loss: 190.62
train mean loss: 186.80
epoch train time: 0:00:00.516196
elapsed time: 0:01:13.901301
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 23:11:59.040472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.51
 ---- batch: 020 ----
mean loss: 183.52
 ---- batch: 030 ----
mean loss: 183.87
 ---- batch: 040 ----
mean loss: 194.53
 ---- batch: 050 ----
mean loss: 185.77
 ---- batch: 060 ----
mean loss: 189.69
 ---- batch: 070 ----
mean loss: 180.31
 ---- batch: 080 ----
mean loss: 188.60
 ---- batch: 090 ----
mean loss: 185.28
train mean loss: 185.40
epoch train time: 0:00:00.511814
elapsed time: 0:01:14.413291
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 23:11:59.552518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.27
 ---- batch: 020 ----
mean loss: 178.01
 ---- batch: 030 ----
mean loss: 179.71
 ---- batch: 040 ----
mean loss: 183.31
 ---- batch: 050 ----
mean loss: 188.88
 ---- batch: 060 ----
mean loss: 182.92
 ---- batch: 070 ----
mean loss: 184.17
 ---- batch: 080 ----
mean loss: 184.39
 ---- batch: 090 ----
mean loss: 190.33
train mean loss: 184.48
epoch train time: 0:00:00.519262
elapsed time: 0:01:14.932763
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 23:12:00.071946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.85
 ---- batch: 020 ----
mean loss: 182.39
 ---- batch: 030 ----
mean loss: 191.15
 ---- batch: 040 ----
mean loss: 184.42
 ---- batch: 050 ----
mean loss: 186.27
 ---- batch: 060 ----
mean loss: 179.60
 ---- batch: 070 ----
mean loss: 182.13
 ---- batch: 080 ----
mean loss: 179.08
 ---- batch: 090 ----
mean loss: 183.90
train mean loss: 182.70
epoch train time: 0:00:00.503387
elapsed time: 0:01:15.436312
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 23:12:00.575481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.45
 ---- batch: 020 ----
mean loss: 176.86
 ---- batch: 030 ----
mean loss: 175.93
 ---- batch: 040 ----
mean loss: 182.26
 ---- batch: 050 ----
mean loss: 192.99
 ---- batch: 060 ----
mean loss: 185.27
 ---- batch: 070 ----
mean loss: 176.88
 ---- batch: 080 ----
mean loss: 192.77
 ---- batch: 090 ----
mean loss: 182.37
train mean loss: 182.72
epoch train time: 0:00:00.505993
elapsed time: 0:01:15.942454
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 23:12:01.081651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.80
 ---- batch: 020 ----
mean loss: 172.14
 ---- batch: 030 ----
mean loss: 183.70
 ---- batch: 040 ----
mean loss: 178.72
 ---- batch: 050 ----
mean loss: 183.20
 ---- batch: 060 ----
mean loss: 189.49
 ---- batch: 070 ----
mean loss: 181.61
 ---- batch: 080 ----
mean loss: 183.78
 ---- batch: 090 ----
mean loss: 187.75
train mean loss: 182.31
epoch train time: 0:00:00.504541
elapsed time: 0:01:16.447169
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 23:12:01.586335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.04
 ---- batch: 020 ----
mean loss: 176.44
 ---- batch: 030 ----
mean loss: 180.26
 ---- batch: 040 ----
mean loss: 176.87
 ---- batch: 050 ----
mean loss: 182.77
 ---- batch: 060 ----
mean loss: 187.02
 ---- batch: 070 ----
mean loss: 176.86
 ---- batch: 080 ----
mean loss: 183.28
 ---- batch: 090 ----
mean loss: 190.29
train mean loss: 181.64
epoch train time: 0:00:00.514179
elapsed time: 0:01:16.961499
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 23:12:02.100659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.96
 ---- batch: 020 ----
mean loss: 187.59
 ---- batch: 030 ----
mean loss: 174.29
 ---- batch: 040 ----
mean loss: 181.43
 ---- batch: 050 ----
mean loss: 182.59
 ---- batch: 060 ----
mean loss: 177.34
 ---- batch: 070 ----
mean loss: 174.43
 ---- batch: 080 ----
mean loss: 181.40
 ---- batch: 090 ----
mean loss: 186.82
train mean loss: 181.21
epoch train time: 0:00:00.510246
elapsed time: 0:01:17.471904
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 23:12:02.611077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.18
 ---- batch: 020 ----
mean loss: 177.71
 ---- batch: 030 ----
mean loss: 183.05
 ---- batch: 040 ----
mean loss: 180.77
 ---- batch: 050 ----
mean loss: 179.00
 ---- batch: 060 ----
mean loss: 179.01
 ---- batch: 070 ----
mean loss: 176.43
 ---- batch: 080 ----
mean loss: 179.91
 ---- batch: 090 ----
mean loss: 169.05
train mean loss: 178.15
epoch train time: 0:00:00.512220
elapsed time: 0:01:17.984274
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 23:12:03.123443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.86
 ---- batch: 020 ----
mean loss: 176.12
 ---- batch: 030 ----
mean loss: 177.45
 ---- batch: 040 ----
mean loss: 179.24
 ---- batch: 050 ----
mean loss: 167.85
 ---- batch: 060 ----
mean loss: 179.01
 ---- batch: 070 ----
mean loss: 181.51
 ---- batch: 080 ----
mean loss: 182.26
 ---- batch: 090 ----
mean loss: 187.67
train mean loss: 178.76
epoch train time: 0:00:00.504260
elapsed time: 0:01:18.488711
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 23:12:03.627899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.07
 ---- batch: 020 ----
mean loss: 173.91
 ---- batch: 030 ----
mean loss: 175.13
 ---- batch: 040 ----
mean loss: 176.59
 ---- batch: 050 ----
mean loss: 175.28
 ---- batch: 060 ----
mean loss: 186.39
 ---- batch: 070 ----
mean loss: 176.35
 ---- batch: 080 ----
mean loss: 179.58
 ---- batch: 090 ----
mean loss: 176.59
train mean loss: 177.67
epoch train time: 0:00:00.520574
elapsed time: 0:01:19.009452
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 23:12:04.148620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.24
 ---- batch: 020 ----
mean loss: 179.43
 ---- batch: 030 ----
mean loss: 179.76
 ---- batch: 040 ----
mean loss: 177.49
 ---- batch: 050 ----
mean loss: 181.26
 ---- batch: 060 ----
mean loss: 178.02
 ---- batch: 070 ----
mean loss: 181.45
 ---- batch: 080 ----
mean loss: 178.81
 ---- batch: 090 ----
mean loss: 176.83
train mean loss: 177.89
epoch train time: 0:00:00.499567
elapsed time: 0:01:19.509198
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 23:12:04.648366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.95
 ---- batch: 020 ----
mean loss: 172.32
 ---- batch: 030 ----
mean loss: 167.27
 ---- batch: 040 ----
mean loss: 174.03
 ---- batch: 050 ----
mean loss: 180.06
 ---- batch: 060 ----
mean loss: 186.19
 ---- batch: 070 ----
mean loss: 173.77
 ---- batch: 080 ----
mean loss: 183.70
 ---- batch: 090 ----
mean loss: 174.64
train mean loss: 175.89
epoch train time: 0:00:00.519274
elapsed time: 0:01:20.028636
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 23:12:05.167823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.72
 ---- batch: 020 ----
mean loss: 181.34
 ---- batch: 030 ----
mean loss: 172.88
 ---- batch: 040 ----
mean loss: 168.74
 ---- batch: 050 ----
mean loss: 179.00
 ---- batch: 060 ----
mean loss: 178.18
 ---- batch: 070 ----
mean loss: 176.89
 ---- batch: 080 ----
mean loss: 180.85
 ---- batch: 090 ----
mean loss: 172.17
train mean loss: 176.17
epoch train time: 0:00:00.521596
elapsed time: 0:01:20.550399
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 23:12:05.689570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.51
 ---- batch: 020 ----
mean loss: 179.22
 ---- batch: 030 ----
mean loss: 176.59
 ---- batch: 040 ----
mean loss: 173.16
 ---- batch: 050 ----
mean loss: 166.53
 ---- batch: 060 ----
mean loss: 180.47
 ---- batch: 070 ----
mean loss: 174.04
 ---- batch: 080 ----
mean loss: 184.04
 ---- batch: 090 ----
mean loss: 172.94
train mean loss: 175.45
epoch train time: 0:00:00.508247
elapsed time: 0:01:21.058802
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 23:12:06.197971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.02
 ---- batch: 020 ----
mean loss: 175.91
 ---- batch: 030 ----
mean loss: 172.84
 ---- batch: 040 ----
mean loss: 176.21
 ---- batch: 050 ----
mean loss: 172.72
 ---- batch: 060 ----
mean loss: 183.22
 ---- batch: 070 ----
mean loss: 181.59
 ---- batch: 080 ----
mean loss: 176.19
 ---- batch: 090 ----
mean loss: 168.92
train mean loss: 174.28
epoch train time: 0:00:00.508033
elapsed time: 0:01:21.566981
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 23:12:06.706148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.27
 ---- batch: 020 ----
mean loss: 167.91
 ---- batch: 030 ----
mean loss: 178.23
 ---- batch: 040 ----
mean loss: 165.95
 ---- batch: 050 ----
mean loss: 169.07
 ---- batch: 060 ----
mean loss: 182.07
 ---- batch: 070 ----
mean loss: 175.91
 ---- batch: 080 ----
mean loss: 175.37
 ---- batch: 090 ----
mean loss: 179.14
train mean loss: 173.68
epoch train time: 0:00:00.505751
elapsed time: 0:01:22.072882
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 23:12:07.212049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.92
 ---- batch: 020 ----
mean loss: 168.71
 ---- batch: 030 ----
mean loss: 174.33
 ---- batch: 040 ----
mean loss: 171.83
 ---- batch: 050 ----
mean loss: 184.01
 ---- batch: 060 ----
mean loss: 178.65
 ---- batch: 070 ----
mean loss: 173.45
 ---- batch: 080 ----
mean loss: 175.00
 ---- batch: 090 ----
mean loss: 175.70
train mean loss: 174.88
epoch train time: 0:00:00.514128
elapsed time: 0:01:22.587162
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 23:12:07.726353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.88
 ---- batch: 020 ----
mean loss: 165.64
 ---- batch: 030 ----
mean loss: 172.44
 ---- batch: 040 ----
mean loss: 174.04
 ---- batch: 050 ----
mean loss: 170.32
 ---- batch: 060 ----
mean loss: 173.73
 ---- batch: 070 ----
mean loss: 182.28
 ---- batch: 080 ----
mean loss: 173.85
 ---- batch: 090 ----
mean loss: 169.47
train mean loss: 172.38
epoch train time: 0:00:00.504024
elapsed time: 0:01:23.091372
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 23:12:08.230556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.12
 ---- batch: 020 ----
mean loss: 170.82
 ---- batch: 030 ----
mean loss: 171.65
 ---- batch: 040 ----
mean loss: 172.04
 ---- batch: 050 ----
mean loss: 169.75
 ---- batch: 060 ----
mean loss: 171.05
 ---- batch: 070 ----
mean loss: 176.89
 ---- batch: 080 ----
mean loss: 176.00
 ---- batch: 090 ----
mean loss: 176.38
train mean loss: 172.05
epoch train time: 0:00:00.500119
elapsed time: 0:01:23.591654
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 23:12:08.730821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.18
 ---- batch: 020 ----
mean loss: 172.47
 ---- batch: 030 ----
mean loss: 167.40
 ---- batch: 040 ----
mean loss: 168.19
 ---- batch: 050 ----
mean loss: 171.72
 ---- batch: 060 ----
mean loss: 172.62
 ---- batch: 070 ----
mean loss: 174.08
 ---- batch: 080 ----
mean loss: 172.18
 ---- batch: 090 ----
mean loss: 167.06
train mean loss: 170.68
epoch train time: 0:00:00.509580
elapsed time: 0:01:24.101384
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 23:12:09.240562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.97
 ---- batch: 020 ----
mean loss: 163.96
 ---- batch: 030 ----
mean loss: 167.61
 ---- batch: 040 ----
mean loss: 168.46
 ---- batch: 050 ----
mean loss: 174.94
 ---- batch: 060 ----
mean loss: 170.65
 ---- batch: 070 ----
mean loss: 167.88
 ---- batch: 080 ----
mean loss: 174.38
 ---- batch: 090 ----
mean loss: 175.54
train mean loss: 171.00
epoch train time: 0:00:00.520348
elapsed time: 0:01:24.621909
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 23:12:09.761080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.58
 ---- batch: 020 ----
mean loss: 165.65
 ---- batch: 030 ----
mean loss: 165.95
 ---- batch: 040 ----
mean loss: 169.77
 ---- batch: 050 ----
mean loss: 172.73
 ---- batch: 060 ----
mean loss: 164.45
 ---- batch: 070 ----
mean loss: 167.58
 ---- batch: 080 ----
mean loss: 171.41
 ---- batch: 090 ----
mean loss: 177.98
train mean loss: 169.89
epoch train time: 0:00:00.508345
elapsed time: 0:01:25.130406
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 23:12:10.269575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.45
 ---- batch: 020 ----
mean loss: 167.99
 ---- batch: 030 ----
mean loss: 171.04
 ---- batch: 040 ----
mean loss: 173.27
 ---- batch: 050 ----
mean loss: 177.92
 ---- batch: 060 ----
mean loss: 175.07
 ---- batch: 070 ----
mean loss: 166.26
 ---- batch: 080 ----
mean loss: 165.59
 ---- batch: 090 ----
mean loss: 166.78
train mean loss: 171.02
epoch train time: 0:00:00.509413
elapsed time: 0:01:25.639966
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 23:12:10.779134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.42
 ---- batch: 020 ----
mean loss: 168.50
 ---- batch: 030 ----
mean loss: 166.28
 ---- batch: 040 ----
mean loss: 167.79
 ---- batch: 050 ----
mean loss: 169.80
 ---- batch: 060 ----
mean loss: 173.25
 ---- batch: 070 ----
mean loss: 168.60
 ---- batch: 080 ----
mean loss: 167.26
 ---- batch: 090 ----
mean loss: 170.67
train mean loss: 169.47
epoch train time: 0:00:00.508180
elapsed time: 0:01:26.148300
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 23:12:11.287467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.36
 ---- batch: 020 ----
mean loss: 169.90
 ---- batch: 030 ----
mean loss: 164.47
 ---- batch: 040 ----
mean loss: 168.55
 ---- batch: 050 ----
mean loss: 168.28
 ---- batch: 060 ----
mean loss: 165.92
 ---- batch: 070 ----
mean loss: 173.16
 ---- batch: 080 ----
mean loss: 159.52
 ---- batch: 090 ----
mean loss: 169.49
train mean loss: 167.77
epoch train time: 0:00:00.507907
elapsed time: 0:01:26.656379
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 23:12:11.795579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.45
 ---- batch: 020 ----
mean loss: 169.24
 ---- batch: 030 ----
mean loss: 163.44
 ---- batch: 040 ----
mean loss: 170.97
 ---- batch: 050 ----
mean loss: 170.47
 ---- batch: 060 ----
mean loss: 169.32
 ---- batch: 070 ----
mean loss: 163.86
 ---- batch: 080 ----
mean loss: 160.78
 ---- batch: 090 ----
mean loss: 167.68
train mean loss: 167.62
epoch train time: 0:00:00.507170
elapsed time: 0:01:27.163729
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 23:12:12.302898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.33
 ---- batch: 020 ----
mean loss: 163.45
 ---- batch: 030 ----
mean loss: 173.87
 ---- batch: 040 ----
mean loss: 163.04
 ---- batch: 050 ----
mean loss: 159.01
 ---- batch: 060 ----
mean loss: 171.40
 ---- batch: 070 ----
mean loss: 166.14
 ---- batch: 080 ----
mean loss: 172.37
 ---- batch: 090 ----
mean loss: 172.62
train mean loss: 167.11
epoch train time: 0:00:00.512038
elapsed time: 0:01:27.675915
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 23:12:12.815088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.84
 ---- batch: 020 ----
mean loss: 169.29
 ---- batch: 030 ----
mean loss: 162.79
 ---- batch: 040 ----
mean loss: 164.16
 ---- batch: 050 ----
mean loss: 169.41
 ---- batch: 060 ----
mean loss: 167.60
 ---- batch: 070 ----
mean loss: 166.91
 ---- batch: 080 ----
mean loss: 166.21
 ---- batch: 090 ----
mean loss: 168.74
train mean loss: 167.02
epoch train time: 0:00:00.504872
elapsed time: 0:01:28.180956
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 23:12:13.320143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.16
 ---- batch: 020 ----
mean loss: 168.28
 ---- batch: 030 ----
mean loss: 163.47
 ---- batch: 040 ----
mean loss: 166.81
 ---- batch: 050 ----
mean loss: 165.74
 ---- batch: 060 ----
mean loss: 166.97
 ---- batch: 070 ----
mean loss: 168.50
 ---- batch: 080 ----
mean loss: 169.61
 ---- batch: 090 ----
mean loss: 165.05
train mean loss: 165.87
epoch train time: 0:00:00.506218
elapsed time: 0:01:28.687345
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 23:12:13.826515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.09
 ---- batch: 020 ----
mean loss: 166.28
 ---- batch: 030 ----
mean loss: 158.89
 ---- batch: 040 ----
mean loss: 166.72
 ---- batch: 050 ----
mean loss: 162.92
 ---- batch: 060 ----
mean loss: 169.19
 ---- batch: 070 ----
mean loss: 166.06
 ---- batch: 080 ----
mean loss: 165.62
 ---- batch: 090 ----
mean loss: 162.22
train mean loss: 165.60
epoch train time: 0:00:00.502235
elapsed time: 0:01:29.189729
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 23:12:14.328904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.21
 ---- batch: 020 ----
mean loss: 161.47
 ---- batch: 030 ----
mean loss: 165.41
 ---- batch: 040 ----
mean loss: 161.86
 ---- batch: 050 ----
mean loss: 165.47
 ---- batch: 060 ----
mean loss: 166.51
 ---- batch: 070 ----
mean loss: 169.13
 ---- batch: 080 ----
mean loss: 165.91
 ---- batch: 090 ----
mean loss: 164.80
train mean loss: 165.61
epoch train time: 0:00:00.522192
elapsed time: 0:01:29.712088
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 23:12:14.851261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.65
 ---- batch: 020 ----
mean loss: 161.58
 ---- batch: 030 ----
mean loss: 164.13
 ---- batch: 040 ----
mean loss: 162.14
 ---- batch: 050 ----
mean loss: 165.83
 ---- batch: 060 ----
mean loss: 171.41
 ---- batch: 070 ----
mean loss: 165.28
 ---- batch: 080 ----
mean loss: 166.56
 ---- batch: 090 ----
mean loss: 169.73
train mean loss: 166.46
epoch train time: 0:00:00.512059
elapsed time: 0:01:30.224297
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 23:12:15.363485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.54
 ---- batch: 020 ----
mean loss: 156.98
 ---- batch: 030 ----
mean loss: 167.61
 ---- batch: 040 ----
mean loss: 167.54
 ---- batch: 050 ----
mean loss: 168.33
 ---- batch: 060 ----
mean loss: 165.69
 ---- batch: 070 ----
mean loss: 166.77
 ---- batch: 080 ----
mean loss: 158.19
 ---- batch: 090 ----
mean loss: 165.00
train mean loss: 163.57
epoch train time: 0:00:00.507561
elapsed time: 0:01:30.732025
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 23:12:15.871195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.77
 ---- batch: 020 ----
mean loss: 168.95
 ---- batch: 030 ----
mean loss: 164.38
 ---- batch: 040 ----
mean loss: 161.24
 ---- batch: 050 ----
mean loss: 156.94
 ---- batch: 060 ----
mean loss: 164.61
 ---- batch: 070 ----
mean loss: 168.43
 ---- batch: 080 ----
mean loss: 162.08
 ---- batch: 090 ----
mean loss: 168.23
train mean loss: 164.52
epoch train time: 0:00:00.496235
elapsed time: 0:01:31.228411
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 23:12:16.367579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.79
 ---- batch: 020 ----
mean loss: 159.70
 ---- batch: 030 ----
mean loss: 162.56
 ---- batch: 040 ----
mean loss: 165.33
 ---- batch: 050 ----
mean loss: 167.03
 ---- batch: 060 ----
mean loss: 167.98
 ---- batch: 070 ----
mean loss: 157.90
 ---- batch: 080 ----
mean loss: 165.56
 ---- batch: 090 ----
mean loss: 164.94
train mean loss: 164.08
epoch train time: 0:00:00.519287
elapsed time: 0:01:31.747871
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 23:12:16.887084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.20
 ---- batch: 020 ----
mean loss: 159.91
 ---- batch: 030 ----
mean loss: 159.76
 ---- batch: 040 ----
mean loss: 161.84
 ---- batch: 050 ----
mean loss: 163.34
 ---- batch: 060 ----
mean loss: 170.17
 ---- batch: 070 ----
mean loss: 171.27
 ---- batch: 080 ----
mean loss: 157.70
 ---- batch: 090 ----
mean loss: 172.15
train mean loss: 164.29
epoch train time: 0:00:00.500165
elapsed time: 0:01:32.248231
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 23:12:17.387415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.22
 ---- batch: 020 ----
mean loss: 159.91
 ---- batch: 030 ----
mean loss: 161.58
 ---- batch: 040 ----
mean loss: 156.29
 ---- batch: 050 ----
mean loss: 169.13
 ---- batch: 060 ----
mean loss: 165.41
 ---- batch: 070 ----
mean loss: 163.69
 ---- batch: 080 ----
mean loss: 157.93
 ---- batch: 090 ----
mean loss: 167.90
train mean loss: 162.96
epoch train time: 0:00:00.509978
elapsed time: 0:01:32.758375
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 23:12:17.897548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.34
 ---- batch: 020 ----
mean loss: 154.80
 ---- batch: 030 ----
mean loss: 159.36
 ---- batch: 040 ----
mean loss: 163.07
 ---- batch: 050 ----
mean loss: 166.45
 ---- batch: 060 ----
mean loss: 167.36
 ---- batch: 070 ----
mean loss: 163.83
 ---- batch: 080 ----
mean loss: 166.39
 ---- batch: 090 ----
mean loss: 171.84
train mean loss: 162.30
epoch train time: 0:00:00.504304
elapsed time: 0:01:33.262841
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 23:12:18.402012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.46
 ---- batch: 020 ----
mean loss: 154.20
 ---- batch: 030 ----
mean loss: 162.75
 ---- batch: 040 ----
mean loss: 162.12
 ---- batch: 050 ----
mean loss: 163.20
 ---- batch: 060 ----
mean loss: 161.47
 ---- batch: 070 ----
mean loss: 162.23
 ---- batch: 080 ----
mean loss: 164.81
 ---- batch: 090 ----
mean loss: 169.35
train mean loss: 162.28
epoch train time: 0:00:00.508386
elapsed time: 0:01:33.771383
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 23:12:18.910569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.12
 ---- batch: 020 ----
mean loss: 158.28
 ---- batch: 030 ----
mean loss: 165.30
 ---- batch: 040 ----
mean loss: 163.38
 ---- batch: 050 ----
mean loss: 166.67
 ---- batch: 060 ----
mean loss: 163.26
 ---- batch: 070 ----
mean loss: 162.79
 ---- batch: 080 ----
mean loss: 163.79
 ---- batch: 090 ----
mean loss: 156.10
train mean loss: 161.61
epoch train time: 0:00:00.496492
elapsed time: 0:01:34.268052
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 23:12:19.407251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.29
 ---- batch: 020 ----
mean loss: 159.17
 ---- batch: 030 ----
mean loss: 159.06
 ---- batch: 040 ----
mean loss: 157.75
 ---- batch: 050 ----
mean loss: 159.43
 ---- batch: 060 ----
mean loss: 171.60
 ---- batch: 070 ----
mean loss: 164.32
 ---- batch: 080 ----
mean loss: 165.32
 ---- batch: 090 ----
mean loss: 160.24
train mean loss: 162.14
epoch train time: 0:00:00.502916
elapsed time: 0:01:34.771149
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 23:12:19.910319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.37
 ---- batch: 020 ----
mean loss: 160.55
 ---- batch: 030 ----
mean loss: 154.26
 ---- batch: 040 ----
mean loss: 161.98
 ---- batch: 050 ----
mean loss: 164.62
 ---- batch: 060 ----
mean loss: 161.44
 ---- batch: 070 ----
mean loss: 153.80
 ---- batch: 080 ----
mean loss: 167.62
 ---- batch: 090 ----
mean loss: 159.70
train mean loss: 160.40
epoch train time: 0:00:00.506751
elapsed time: 0:01:35.278047
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 23:12:20.417222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.81
 ---- batch: 020 ----
mean loss: 155.37
 ---- batch: 030 ----
mean loss: 156.47
 ---- batch: 040 ----
mean loss: 161.12
 ---- batch: 050 ----
mean loss: 162.18
 ---- batch: 060 ----
mean loss: 160.85
 ---- batch: 070 ----
mean loss: 161.45
 ---- batch: 080 ----
mean loss: 168.76
 ---- batch: 090 ----
mean loss: 159.92
train mean loss: 160.09
epoch train time: 0:00:00.519412
elapsed time: 0:01:35.797613
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 23:12:20.936799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.56
 ---- batch: 020 ----
mean loss: 158.95
 ---- batch: 030 ----
mean loss: 155.80
 ---- batch: 040 ----
mean loss: 159.29
 ---- batch: 050 ----
mean loss: 165.06
 ---- batch: 060 ----
mean loss: 162.62
 ---- batch: 070 ----
mean loss: 160.20
 ---- batch: 080 ----
mean loss: 161.24
 ---- batch: 090 ----
mean loss: 158.94
train mean loss: 160.20
epoch train time: 0:00:00.514859
elapsed time: 0:01:36.312639
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 23:12:21.451808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.00
 ---- batch: 020 ----
mean loss: 154.93
 ---- batch: 030 ----
mean loss: 151.68
 ---- batch: 040 ----
mean loss: 160.05
 ---- batch: 050 ----
mean loss: 164.89
 ---- batch: 060 ----
mean loss: 154.40
 ---- batch: 070 ----
mean loss: 161.70
 ---- batch: 080 ----
mean loss: 162.81
 ---- batch: 090 ----
mean loss: 161.44
train mean loss: 158.77
epoch train time: 0:00:00.535183
elapsed time: 0:01:36.847966
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 23:12:21.987148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.78
 ---- batch: 020 ----
mean loss: 154.26
 ---- batch: 030 ----
mean loss: 151.34
 ---- batch: 040 ----
mean loss: 158.73
 ---- batch: 050 ----
mean loss: 155.92
 ---- batch: 060 ----
mean loss: 166.63
 ---- batch: 070 ----
mean loss: 162.17
 ---- batch: 080 ----
mean loss: 162.17
 ---- batch: 090 ----
mean loss: 162.91
train mean loss: 159.31
epoch train time: 0:00:00.500201
elapsed time: 0:01:37.348371
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 23:12:22.487562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.65
 ---- batch: 020 ----
mean loss: 159.81
 ---- batch: 030 ----
mean loss: 157.14
 ---- batch: 040 ----
mean loss: 155.13
 ---- batch: 050 ----
mean loss: 155.35
 ---- batch: 060 ----
mean loss: 158.88
 ---- batch: 070 ----
mean loss: 161.45
 ---- batch: 080 ----
mean loss: 162.83
 ---- batch: 090 ----
mean loss: 163.27
train mean loss: 158.59
epoch train time: 0:00:00.511881
elapsed time: 0:01:37.860422
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 23:12:22.999615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.83
 ---- batch: 020 ----
mean loss: 154.15
 ---- batch: 030 ----
mean loss: 154.08
 ---- batch: 040 ----
mean loss: 159.49
 ---- batch: 050 ----
mean loss: 158.53
 ---- batch: 060 ----
mean loss: 162.31
 ---- batch: 070 ----
mean loss: 157.86
 ---- batch: 080 ----
mean loss: 163.47
 ---- batch: 090 ----
mean loss: 170.47
train mean loss: 158.70
epoch train time: 0:00:00.499458
elapsed time: 0:01:38.360087
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 23:12:23.499272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.81
 ---- batch: 020 ----
mean loss: 157.45
 ---- batch: 030 ----
mean loss: 155.72
 ---- batch: 040 ----
mean loss: 160.08
 ---- batch: 050 ----
mean loss: 162.16
 ---- batch: 060 ----
mean loss: 158.04
 ---- batch: 070 ----
mean loss: 156.60
 ---- batch: 080 ----
mean loss: 155.07
 ---- batch: 090 ----
mean loss: 169.11
train mean loss: 158.53
epoch train time: 0:00:00.510439
elapsed time: 0:01:38.870693
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 23:12:24.009865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.20
 ---- batch: 020 ----
mean loss: 151.32
 ---- batch: 030 ----
mean loss: 155.73
 ---- batch: 040 ----
mean loss: 158.51
 ---- batch: 050 ----
mean loss: 159.26
 ---- batch: 060 ----
mean loss: 155.76
 ---- batch: 070 ----
mean loss: 161.04
 ---- batch: 080 ----
mean loss: 162.54
 ---- batch: 090 ----
mean loss: 160.80
train mean loss: 157.28
epoch train time: 0:00:00.500977
elapsed time: 0:01:39.371819
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 23:12:24.511019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.90
 ---- batch: 020 ----
mean loss: 155.41
 ---- batch: 030 ----
mean loss: 151.99
 ---- batch: 040 ----
mean loss: 155.63
 ---- batch: 050 ----
mean loss: 153.72
 ---- batch: 060 ----
mean loss: 150.89
 ---- batch: 070 ----
mean loss: 158.63
 ---- batch: 080 ----
mean loss: 165.69
 ---- batch: 090 ----
mean loss: 165.43
train mean loss: 157.72
epoch train time: 0:00:00.516841
elapsed time: 0:01:39.888865
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 23:12:25.028044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.84
 ---- batch: 020 ----
mean loss: 154.91
 ---- batch: 030 ----
mean loss: 156.94
 ---- batch: 040 ----
mean loss: 157.88
 ---- batch: 050 ----
mean loss: 156.53
 ---- batch: 060 ----
mean loss: 152.39
 ---- batch: 070 ----
mean loss: 157.94
 ---- batch: 080 ----
mean loss: 157.27
 ---- batch: 090 ----
mean loss: 161.99
train mean loss: 156.77
epoch train time: 0:00:00.522758
elapsed time: 0:01:40.411779
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 23:12:25.550960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.82
 ---- batch: 020 ----
mean loss: 155.59
 ---- batch: 030 ----
mean loss: 157.69
 ---- batch: 040 ----
mean loss: 157.07
 ---- batch: 050 ----
mean loss: 155.60
 ---- batch: 060 ----
mean loss: 155.41
 ---- batch: 070 ----
mean loss: 158.58
 ---- batch: 080 ----
mean loss: 165.24
 ---- batch: 090 ----
mean loss: 155.95
train mean loss: 157.84
epoch train time: 0:00:00.512629
elapsed time: 0:01:40.924715
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 23:12:26.063897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.21
 ---- batch: 020 ----
mean loss: 155.33
 ---- batch: 030 ----
mean loss: 159.80
 ---- batch: 040 ----
mean loss: 152.11
 ---- batch: 050 ----
mean loss: 146.76
 ---- batch: 060 ----
mean loss: 158.90
 ---- batch: 070 ----
mean loss: 158.12
 ---- batch: 080 ----
mean loss: 158.69
 ---- batch: 090 ----
mean loss: 157.11
train mean loss: 156.41
epoch train time: 0:00:00.509189
elapsed time: 0:01:41.434067
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 23:12:26.573238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.29
 ---- batch: 020 ----
mean loss: 149.27
 ---- batch: 030 ----
mean loss: 152.53
 ---- batch: 040 ----
mean loss: 156.48
 ---- batch: 050 ----
mean loss: 154.46
 ---- batch: 060 ----
mean loss: 151.62
 ---- batch: 070 ----
mean loss: 159.48
 ---- batch: 080 ----
mean loss: 162.07
 ---- batch: 090 ----
mean loss: 163.65
train mean loss: 156.61
epoch train time: 0:00:00.524829
elapsed time: 0:01:41.959046
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 23:12:27.098232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.92
 ---- batch: 020 ----
mean loss: 158.03
 ---- batch: 030 ----
mean loss: 149.92
 ---- batch: 040 ----
mean loss: 153.29
 ---- batch: 050 ----
mean loss: 154.03
 ---- batch: 060 ----
mean loss: 160.21
 ---- batch: 070 ----
mean loss: 164.44
 ---- batch: 080 ----
mean loss: 153.98
 ---- batch: 090 ----
mean loss: 150.79
train mean loss: 155.12
epoch train time: 0:00:00.504121
elapsed time: 0:01:42.463378
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 23:12:27.602563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.62
 ---- batch: 020 ----
mean loss: 151.09
 ---- batch: 030 ----
mean loss: 149.15
 ---- batch: 040 ----
mean loss: 154.71
 ---- batch: 050 ----
mean loss: 150.31
 ---- batch: 060 ----
mean loss: 153.97
 ---- batch: 070 ----
mean loss: 155.05
 ---- batch: 080 ----
mean loss: 158.58
 ---- batch: 090 ----
mean loss: 158.94
train mean loss: 155.07
epoch train time: 0:00:00.519009
elapsed time: 0:01:42.982570
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 23:12:28.121742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.53
 ---- batch: 020 ----
mean loss: 152.35
 ---- batch: 030 ----
mean loss: 150.85
 ---- batch: 040 ----
mean loss: 153.54
 ---- batch: 050 ----
mean loss: 156.37
 ---- batch: 060 ----
mean loss: 159.08
 ---- batch: 070 ----
mean loss: 152.40
 ---- batch: 080 ----
mean loss: 158.41
 ---- batch: 090 ----
mean loss: 156.40
train mean loss: 154.85
epoch train time: 0:00:00.499064
elapsed time: 0:01:43.481799
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 23:12:28.620994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.20
 ---- batch: 020 ----
mean loss: 153.34
 ---- batch: 030 ----
mean loss: 155.15
 ---- batch: 040 ----
mean loss: 156.71
 ---- batch: 050 ----
mean loss: 153.15
 ---- batch: 060 ----
mean loss: 161.81
 ---- batch: 070 ----
mean loss: 152.05
 ---- batch: 080 ----
mean loss: 153.37
 ---- batch: 090 ----
mean loss: 156.77
train mean loss: 154.46
epoch train time: 0:00:00.510637
elapsed time: 0:01:43.992618
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 23:12:29.131780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.20
 ---- batch: 020 ----
mean loss: 150.08
 ---- batch: 030 ----
mean loss: 153.50
 ---- batch: 040 ----
mean loss: 153.25
 ---- batch: 050 ----
mean loss: 156.23
 ---- batch: 060 ----
mean loss: 151.32
 ---- batch: 070 ----
mean loss: 155.60
 ---- batch: 080 ----
mean loss: 158.99
 ---- batch: 090 ----
mean loss: 167.66
train mean loss: 154.94
epoch train time: 0:00:00.502318
elapsed time: 0:01:44.495093
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 23:12:29.634260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.06
 ---- batch: 020 ----
mean loss: 151.75
 ---- batch: 030 ----
mean loss: 158.12
 ---- batch: 040 ----
mean loss: 150.76
 ---- batch: 050 ----
mean loss: 149.82
 ---- batch: 060 ----
mean loss: 158.74
 ---- batch: 070 ----
mean loss: 155.76
 ---- batch: 080 ----
mean loss: 158.37
 ---- batch: 090 ----
mean loss: 157.89
train mean loss: 154.54
epoch train time: 0:00:00.506416
elapsed time: 0:01:45.001661
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 23:12:30.140830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.62
 ---- batch: 020 ----
mean loss: 156.98
 ---- batch: 030 ----
mean loss: 150.85
 ---- batch: 040 ----
mean loss: 152.36
 ---- batch: 050 ----
mean loss: 153.88
 ---- batch: 060 ----
mean loss: 155.10
 ---- batch: 070 ----
mean loss: 154.26
 ---- batch: 080 ----
mean loss: 155.64
 ---- batch: 090 ----
mean loss: 155.20
train mean loss: 153.78
epoch train time: 0:00:00.506615
elapsed time: 0:01:45.508436
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 23:12:30.647631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.21
 ---- batch: 020 ----
mean loss: 149.26
 ---- batch: 030 ----
mean loss: 163.78
 ---- batch: 040 ----
mean loss: 143.22
 ---- batch: 050 ----
mean loss: 155.70
 ---- batch: 060 ----
mean loss: 152.38
 ---- batch: 070 ----
mean loss: 160.38
 ---- batch: 080 ----
mean loss: 151.16
 ---- batch: 090 ----
mean loss: 158.81
train mean loss: 153.52
epoch train time: 0:00:00.509037
elapsed time: 0:01:46.017659
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 23:12:31.156829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.72
 ---- batch: 020 ----
mean loss: 149.04
 ---- batch: 030 ----
mean loss: 146.56
 ---- batch: 040 ----
mean loss: 153.08
 ---- batch: 050 ----
mean loss: 159.13
 ---- batch: 060 ----
mean loss: 155.22
 ---- batch: 070 ----
mean loss: 156.48
 ---- batch: 080 ----
mean loss: 156.41
 ---- batch: 090 ----
mean loss: 152.45
train mean loss: 153.53
epoch train time: 0:00:00.508138
elapsed time: 0:01:46.525946
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 23:12:31.665113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.66
 ---- batch: 020 ----
mean loss: 154.80
 ---- batch: 030 ----
mean loss: 145.42
 ---- batch: 040 ----
mean loss: 156.15
 ---- batch: 050 ----
mean loss: 150.05
 ---- batch: 060 ----
mean loss: 147.92
 ---- batch: 070 ----
mean loss: 153.89
 ---- batch: 080 ----
mean loss: 149.43
 ---- batch: 090 ----
mean loss: 152.53
train mean loss: 152.46
epoch train time: 0:00:00.517755
elapsed time: 0:01:47.043856
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 23:12:32.183025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.72
 ---- batch: 020 ----
mean loss: 155.57
 ---- batch: 030 ----
mean loss: 155.63
 ---- batch: 040 ----
mean loss: 154.65
 ---- batch: 050 ----
mean loss: 147.87
 ---- batch: 060 ----
mean loss: 156.62
 ---- batch: 070 ----
mean loss: 154.63
 ---- batch: 080 ----
mean loss: 151.03
 ---- batch: 090 ----
mean loss: 150.71
train mean loss: 152.48
epoch train time: 0:00:00.512416
elapsed time: 0:01:47.556422
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 23:12:32.695590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.59
 ---- batch: 020 ----
mean loss: 156.78
 ---- batch: 030 ----
mean loss: 147.29
 ---- batch: 040 ----
mean loss: 155.80
 ---- batch: 050 ----
mean loss: 157.88
 ---- batch: 060 ----
mean loss: 156.24
 ---- batch: 070 ----
mean loss: 148.12
 ---- batch: 080 ----
mean loss: 155.71
 ---- batch: 090 ----
mean loss: 152.87
train mean loss: 152.81
epoch train time: 0:00:00.504714
elapsed time: 0:01:48.061286
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 23:12:33.200454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.12
 ---- batch: 020 ----
mean loss: 149.24
 ---- batch: 030 ----
mean loss: 155.07
 ---- batch: 040 ----
mean loss: 156.68
 ---- batch: 050 ----
mean loss: 155.12
 ---- batch: 060 ----
mean loss: 154.87
 ---- batch: 070 ----
mean loss: 151.17
 ---- batch: 080 ----
mean loss: 152.42
 ---- batch: 090 ----
mean loss: 151.32
train mean loss: 152.86
epoch train time: 0:00:00.513307
elapsed time: 0:01:48.574742
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 23:12:33.713910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.52
 ---- batch: 020 ----
mean loss: 148.34
 ---- batch: 030 ----
mean loss: 148.70
 ---- batch: 040 ----
mean loss: 151.31
 ---- batch: 050 ----
mean loss: 150.57
 ---- batch: 060 ----
mean loss: 151.58
 ---- batch: 070 ----
mean loss: 149.59
 ---- batch: 080 ----
mean loss: 158.81
 ---- batch: 090 ----
mean loss: 156.26
train mean loss: 151.15
epoch train time: 0:00:00.506445
elapsed time: 0:01:49.081354
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 23:12:34.220534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.59
 ---- batch: 020 ----
mean loss: 149.39
 ---- batch: 030 ----
mean loss: 147.45
 ---- batch: 040 ----
mean loss: 155.08
 ---- batch: 050 ----
mean loss: 145.12
 ---- batch: 060 ----
mean loss: 148.12
 ---- batch: 070 ----
mean loss: 152.30
 ---- batch: 080 ----
mean loss: 151.65
 ---- batch: 090 ----
mean loss: 153.64
train mean loss: 151.11
epoch train time: 0:00:00.508815
elapsed time: 0:01:49.590370
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 23:12:34.729565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.71
 ---- batch: 020 ----
mean loss: 144.82
 ---- batch: 030 ----
mean loss: 149.39
 ---- batch: 040 ----
mean loss: 152.37
 ---- batch: 050 ----
mean loss: 154.36
 ---- batch: 060 ----
mean loss: 157.00
 ---- batch: 070 ----
mean loss: 154.77
 ---- batch: 080 ----
mean loss: 148.40
 ---- batch: 090 ----
mean loss: 154.94
train mean loss: 151.77
epoch train time: 0:00:00.499331
elapsed time: 0:01:50.089874
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 23:12:35.229039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.10
 ---- batch: 020 ----
mean loss: 155.01
 ---- batch: 030 ----
mean loss: 147.58
 ---- batch: 040 ----
mean loss: 155.31
 ---- batch: 050 ----
mean loss: 144.82
 ---- batch: 060 ----
mean loss: 153.09
 ---- batch: 070 ----
mean loss: 144.91
 ---- batch: 080 ----
mean loss: 156.49
 ---- batch: 090 ----
mean loss: 153.74
train mean loss: 150.92
epoch train time: 0:00:00.500833
elapsed time: 0:01:50.590856
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 23:12:35.730023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.77
 ---- batch: 020 ----
mean loss: 150.24
 ---- batch: 030 ----
mean loss: 143.45
 ---- batch: 040 ----
mean loss: 147.04
 ---- batch: 050 ----
mean loss: 147.15
 ---- batch: 060 ----
mean loss: 153.41
 ---- batch: 070 ----
mean loss: 151.28
 ---- batch: 080 ----
mean loss: 151.29
 ---- batch: 090 ----
mean loss: 159.10
train mean loss: 150.46
epoch train time: 0:00:00.513997
elapsed time: 0:01:51.105015
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 23:12:36.244183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.00
 ---- batch: 020 ----
mean loss: 148.75
 ---- batch: 030 ----
mean loss: 157.24
 ---- batch: 040 ----
mean loss: 150.78
 ---- batch: 050 ----
mean loss: 150.77
 ---- batch: 060 ----
mean loss: 146.89
 ---- batch: 070 ----
mean loss: 151.16
 ---- batch: 080 ----
mean loss: 151.95
 ---- batch: 090 ----
mean loss: 154.30
train mean loss: 149.91
epoch train time: 0:00:00.516255
elapsed time: 0:01:51.621423
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 23:12:36.760593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.52
 ---- batch: 020 ----
mean loss: 146.60
 ---- batch: 030 ----
mean loss: 151.11
 ---- batch: 040 ----
mean loss: 153.14
 ---- batch: 050 ----
mean loss: 148.13
 ---- batch: 060 ----
mean loss: 145.82
 ---- batch: 070 ----
mean loss: 149.78
 ---- batch: 080 ----
mean loss: 145.96
 ---- batch: 090 ----
mean loss: 152.89
train mean loss: 149.94
epoch train time: 0:00:00.508627
elapsed time: 0:01:52.130214
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 23:12:37.269381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.30
 ---- batch: 020 ----
mean loss: 150.52
 ---- batch: 030 ----
mean loss: 144.17
 ---- batch: 040 ----
mean loss: 149.50
 ---- batch: 050 ----
mean loss: 153.70
 ---- batch: 060 ----
mean loss: 157.07
 ---- batch: 070 ----
mean loss: 153.73
 ---- batch: 080 ----
mean loss: 154.87
 ---- batch: 090 ----
mean loss: 151.97
train mean loss: 151.31
epoch train time: 0:00:00.510922
elapsed time: 0:01:52.641324
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 23:12:37.780498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.57
 ---- batch: 020 ----
mean loss: 150.58
 ---- batch: 030 ----
mean loss: 153.28
 ---- batch: 040 ----
mean loss: 143.86
 ---- batch: 050 ----
mean loss: 146.21
 ---- batch: 060 ----
mean loss: 155.49
 ---- batch: 070 ----
mean loss: 147.66
 ---- batch: 080 ----
mean loss: 146.21
 ---- batch: 090 ----
mean loss: 148.45
train mean loss: 149.47
epoch train time: 0:00:00.504969
elapsed time: 0:01:53.146448
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 23:12:38.285663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.09
 ---- batch: 020 ----
mean loss: 145.45
 ---- batch: 030 ----
mean loss: 155.23
 ---- batch: 040 ----
mean loss: 150.19
 ---- batch: 050 ----
mean loss: 142.42
 ---- batch: 060 ----
mean loss: 151.55
 ---- batch: 070 ----
mean loss: 149.56
 ---- batch: 080 ----
mean loss: 158.04
 ---- batch: 090 ----
mean loss: 149.71
train mean loss: 149.19
epoch train time: 0:00:00.506580
elapsed time: 0:01:53.653222
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 23:12:38.792391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.15
 ---- batch: 020 ----
mean loss: 151.38
 ---- batch: 030 ----
mean loss: 149.52
 ---- batch: 040 ----
mean loss: 151.34
 ---- batch: 050 ----
mean loss: 156.43
 ---- batch: 060 ----
mean loss: 148.07
 ---- batch: 070 ----
mean loss: 142.13
 ---- batch: 080 ----
mean loss: 148.25
 ---- batch: 090 ----
mean loss: 149.80
train mean loss: 149.42
epoch train time: 0:00:00.503040
elapsed time: 0:01:54.156423
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 23:12:39.295590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.90
 ---- batch: 020 ----
mean loss: 142.27
 ---- batch: 030 ----
mean loss: 150.00
 ---- batch: 040 ----
mean loss: 145.73
 ---- batch: 050 ----
mean loss: 149.42
 ---- batch: 060 ----
mean loss: 142.66
 ---- batch: 070 ----
mean loss: 147.83
 ---- batch: 080 ----
mean loss: 154.73
 ---- batch: 090 ----
mean loss: 151.74
train mean loss: 148.46
epoch train time: 0:00:00.507489
elapsed time: 0:01:54.664057
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 23:12:39.803225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.98
 ---- batch: 020 ----
mean loss: 142.94
 ---- batch: 030 ----
mean loss: 144.59
 ---- batch: 040 ----
mean loss: 153.03
 ---- batch: 050 ----
mean loss: 154.64
 ---- batch: 060 ----
mean loss: 147.02
 ---- batch: 070 ----
mean loss: 153.27
 ---- batch: 080 ----
mean loss: 150.91
 ---- batch: 090 ----
mean loss: 151.69
train mean loss: 148.49
epoch train time: 0:00:00.497718
elapsed time: 0:01:55.161932
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 23:12:40.301101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.41
 ---- batch: 020 ----
mean loss: 148.04
 ---- batch: 030 ----
mean loss: 144.95
 ---- batch: 040 ----
mean loss: 148.94
 ---- batch: 050 ----
mean loss: 143.68
 ---- batch: 060 ----
mean loss: 150.85
 ---- batch: 070 ----
mean loss: 153.03
 ---- batch: 080 ----
mean loss: 153.80
 ---- batch: 090 ----
mean loss: 145.95
train mean loss: 147.84
epoch train time: 0:00:00.497857
elapsed time: 0:01:55.659933
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 23:12:40.799100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.07
 ---- batch: 020 ----
mean loss: 143.59
 ---- batch: 030 ----
mean loss: 149.17
 ---- batch: 040 ----
mean loss: 147.24
 ---- batch: 050 ----
mean loss: 143.19
 ---- batch: 060 ----
mean loss: 143.54
 ---- batch: 070 ----
mean loss: 153.58
 ---- batch: 080 ----
mean loss: 151.06
 ---- batch: 090 ----
mean loss: 148.72
train mean loss: 147.31
epoch train time: 0:00:00.510791
elapsed time: 0:01:56.170890
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 23:12:41.310058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.07
 ---- batch: 020 ----
mean loss: 149.43
 ---- batch: 030 ----
mean loss: 154.40
 ---- batch: 040 ----
mean loss: 153.52
 ---- batch: 050 ----
mean loss: 142.16
 ---- batch: 060 ----
mean loss: 149.66
 ---- batch: 070 ----
mean loss: 153.10
 ---- batch: 080 ----
mean loss: 149.79
 ---- batch: 090 ----
mean loss: 138.72
train mean loss: 147.25
epoch train time: 0:00:00.512459
elapsed time: 0:01:56.683499
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 23:12:41.822686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.24
 ---- batch: 020 ----
mean loss: 141.73
 ---- batch: 030 ----
mean loss: 145.36
 ---- batch: 040 ----
mean loss: 147.15
 ---- batch: 050 ----
mean loss: 145.56
 ---- batch: 060 ----
mean loss: 156.45
 ---- batch: 070 ----
mean loss: 148.53
 ---- batch: 080 ----
mean loss: 143.22
 ---- batch: 090 ----
mean loss: 151.27
train mean loss: 147.70
epoch train time: 0:00:00.498345
elapsed time: 0:01:57.182009
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 23:12:42.321195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.14
 ---- batch: 020 ----
mean loss: 143.86
 ---- batch: 030 ----
mean loss: 143.77
 ---- batch: 040 ----
mean loss: 142.76
 ---- batch: 050 ----
mean loss: 147.73
 ---- batch: 060 ----
mean loss: 146.41
 ---- batch: 070 ----
mean loss: 151.57
 ---- batch: 080 ----
mean loss: 143.52
 ---- batch: 090 ----
mean loss: 151.70
train mean loss: 147.26
epoch train time: 0:00:00.499955
elapsed time: 0:01:57.682130
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 23:12:42.821298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.96
 ---- batch: 020 ----
mean loss: 147.87
 ---- batch: 030 ----
mean loss: 144.40
 ---- batch: 040 ----
mean loss: 150.06
 ---- batch: 050 ----
mean loss: 146.42
 ---- batch: 060 ----
mean loss: 150.95
 ---- batch: 070 ----
mean loss: 149.23
 ---- batch: 080 ----
mean loss: 143.98
 ---- batch: 090 ----
mean loss: 148.07
train mean loss: 146.92
epoch train time: 0:00:00.505838
elapsed time: 0:01:58.188116
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 23:12:43.327286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.35
 ---- batch: 020 ----
mean loss: 144.35
 ---- batch: 030 ----
mean loss: 148.31
 ---- batch: 040 ----
mean loss: 143.86
 ---- batch: 050 ----
mean loss: 148.21
 ---- batch: 060 ----
mean loss: 145.05
 ---- batch: 070 ----
mean loss: 140.38
 ---- batch: 080 ----
mean loss: 148.99
 ---- batch: 090 ----
mean loss: 150.91
train mean loss: 146.72
epoch train time: 0:00:00.507384
elapsed time: 0:01:58.695650
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 23:12:43.834818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.16
 ---- batch: 020 ----
mean loss: 142.31
 ---- batch: 030 ----
mean loss: 151.21
 ---- batch: 040 ----
mean loss: 136.20
 ---- batch: 050 ----
mean loss: 146.51
 ---- batch: 060 ----
mean loss: 142.49
 ---- batch: 070 ----
mean loss: 147.35
 ---- batch: 080 ----
mean loss: 148.04
 ---- batch: 090 ----
mean loss: 149.98
train mean loss: 145.95
epoch train time: 0:00:00.510975
elapsed time: 0:01:59.206771
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 23:12:44.345953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.44
 ---- batch: 020 ----
mean loss: 149.04
 ---- batch: 030 ----
mean loss: 142.73
 ---- batch: 040 ----
mean loss: 147.22
 ---- batch: 050 ----
mean loss: 149.25
 ---- batch: 060 ----
mean loss: 148.74
 ---- batch: 070 ----
mean loss: 147.98
 ---- batch: 080 ----
mean loss: 146.99
 ---- batch: 090 ----
mean loss: 145.38
train mean loss: 146.55
epoch train time: 0:00:00.514085
elapsed time: 0:01:59.721017
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 23:12:44.860185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.75
 ---- batch: 020 ----
mean loss: 142.78
 ---- batch: 030 ----
mean loss: 142.05
 ---- batch: 040 ----
mean loss: 148.82
 ---- batch: 050 ----
mean loss: 143.88
 ---- batch: 060 ----
mean loss: 148.24
 ---- batch: 070 ----
mean loss: 144.06
 ---- batch: 080 ----
mean loss: 147.78
 ---- batch: 090 ----
mean loss: 147.88
train mean loss: 145.71
epoch train time: 0:00:00.514257
elapsed time: 0:02:00.235423
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 23:12:45.374591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.59
 ---- batch: 020 ----
mean loss: 146.75
 ---- batch: 030 ----
mean loss: 141.10
 ---- batch: 040 ----
mean loss: 146.67
 ---- batch: 050 ----
mean loss: 148.74
 ---- batch: 060 ----
mean loss: 142.21
 ---- batch: 070 ----
mean loss: 154.15
 ---- batch: 080 ----
mean loss: 140.86
 ---- batch: 090 ----
mean loss: 146.88
train mean loss: 145.39
epoch train time: 0:00:00.512285
elapsed time: 0:02:00.747871
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 23:12:45.887057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.09
 ---- batch: 020 ----
mean loss: 141.98
 ---- batch: 030 ----
mean loss: 143.17
 ---- batch: 040 ----
mean loss: 144.55
 ---- batch: 050 ----
mean loss: 142.76
 ---- batch: 060 ----
mean loss: 146.27
 ---- batch: 070 ----
mean loss: 149.04
 ---- batch: 080 ----
mean loss: 144.83
 ---- batch: 090 ----
mean loss: 146.81
train mean loss: 144.80
epoch train time: 0:00:00.506027
elapsed time: 0:02:01.254113
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 23:12:46.393281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.54
 ---- batch: 020 ----
mean loss: 140.71
 ---- batch: 030 ----
mean loss: 145.52
 ---- batch: 040 ----
mean loss: 146.48
 ---- batch: 050 ----
mean loss: 143.13
 ---- batch: 060 ----
mean loss: 140.85
 ---- batch: 070 ----
mean loss: 145.51
 ---- batch: 080 ----
mean loss: 149.96
 ---- batch: 090 ----
mean loss: 147.76
train mean loss: 145.37
epoch train time: 0:00:00.517439
elapsed time: 0:02:01.771704
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 23:12:46.910875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.91
 ---- batch: 020 ----
mean loss: 140.57
 ---- batch: 030 ----
mean loss: 150.87
 ---- batch: 040 ----
mean loss: 138.17
 ---- batch: 050 ----
mean loss: 144.56
 ---- batch: 060 ----
mean loss: 144.04
 ---- batch: 070 ----
mean loss: 146.89
 ---- batch: 080 ----
mean loss: 144.67
 ---- batch: 090 ----
mean loss: 147.99
train mean loss: 144.19
epoch train time: 0:00:00.509827
elapsed time: 0:02:02.281694
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 23:12:47.420882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.10
 ---- batch: 020 ----
mean loss: 144.66
 ---- batch: 030 ----
mean loss: 146.75
 ---- batch: 040 ----
mean loss: 141.93
 ---- batch: 050 ----
mean loss: 142.16
 ---- batch: 060 ----
mean loss: 146.37
 ---- batch: 070 ----
mean loss: 148.14
 ---- batch: 080 ----
mean loss: 145.86
 ---- batch: 090 ----
mean loss: 142.28
train mean loss: 144.37
epoch train time: 0:00:00.524288
elapsed time: 0:02:02.806149
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 23:12:47.945318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.88
 ---- batch: 020 ----
mean loss: 137.90
 ---- batch: 030 ----
mean loss: 146.68
 ---- batch: 040 ----
mean loss: 144.31
 ---- batch: 050 ----
mean loss: 143.13
 ---- batch: 060 ----
mean loss: 150.07
 ---- batch: 070 ----
mean loss: 151.86
 ---- batch: 080 ----
mean loss: 141.20
 ---- batch: 090 ----
mean loss: 140.92
train mean loss: 144.37
epoch train time: 0:00:00.502571
elapsed time: 0:02:03.308867
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 23:12:48.448038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.98
 ---- batch: 020 ----
mean loss: 140.80
 ---- batch: 030 ----
mean loss: 141.21
 ---- batch: 040 ----
mean loss: 146.46
 ---- batch: 050 ----
mean loss: 146.16
 ---- batch: 060 ----
mean loss: 147.79
 ---- batch: 070 ----
mean loss: 142.41
 ---- batch: 080 ----
mean loss: 140.82
 ---- batch: 090 ----
mean loss: 148.19
train mean loss: 143.73
epoch train time: 0:00:00.515067
elapsed time: 0:02:03.824088
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 23:12:48.963257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.64
 ---- batch: 020 ----
mean loss: 134.45
 ---- batch: 030 ----
mean loss: 142.93
 ---- batch: 040 ----
mean loss: 136.27
 ---- batch: 050 ----
mean loss: 139.69
 ---- batch: 060 ----
mean loss: 150.86
 ---- batch: 070 ----
mean loss: 152.47
 ---- batch: 080 ----
mean loss: 143.37
 ---- batch: 090 ----
mean loss: 141.79
train mean loss: 143.21
epoch train time: 0:00:00.506043
elapsed time: 0:02:04.330281
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 23:12:49.469465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.46
 ---- batch: 020 ----
mean loss: 143.56
 ---- batch: 030 ----
mean loss: 141.45
 ---- batch: 040 ----
mean loss: 143.91
 ---- batch: 050 ----
mean loss: 146.67
 ---- batch: 060 ----
mean loss: 149.21
 ---- batch: 070 ----
mean loss: 136.28
 ---- batch: 080 ----
mean loss: 138.27
 ---- batch: 090 ----
mean loss: 143.57
train mean loss: 143.11
epoch train time: 0:00:00.511394
elapsed time: 0:02:04.841844
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 23:12:49.981052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.46
 ---- batch: 020 ----
mean loss: 136.84
 ---- batch: 030 ----
mean loss: 147.71
 ---- batch: 040 ----
mean loss: 145.45
 ---- batch: 050 ----
mean loss: 143.72
 ---- batch: 060 ----
mean loss: 144.74
 ---- batch: 070 ----
mean loss: 145.34
 ---- batch: 080 ----
mean loss: 143.33
 ---- batch: 090 ----
mean loss: 150.67
train mean loss: 143.97
epoch train time: 0:00:00.502778
elapsed time: 0:02:05.344820
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 23:12:50.483993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.82
 ---- batch: 020 ----
mean loss: 142.63
 ---- batch: 030 ----
mean loss: 144.82
 ---- batch: 040 ----
mean loss: 146.52
 ---- batch: 050 ----
mean loss: 148.49
 ---- batch: 060 ----
mean loss: 138.38
 ---- batch: 070 ----
mean loss: 143.89
 ---- batch: 080 ----
mean loss: 140.51
 ---- batch: 090 ----
mean loss: 143.24
train mean loss: 143.10
epoch train time: 0:00:00.509028
elapsed time: 0:02:05.854035
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 23:12:50.993236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.83
 ---- batch: 020 ----
mean loss: 141.79
 ---- batch: 030 ----
mean loss: 137.97
 ---- batch: 040 ----
mean loss: 143.88
 ---- batch: 050 ----
mean loss: 138.89
 ---- batch: 060 ----
mean loss: 149.80
 ---- batch: 070 ----
mean loss: 147.31
 ---- batch: 080 ----
mean loss: 141.74
 ---- batch: 090 ----
mean loss: 143.01
train mean loss: 142.51
epoch train time: 0:00:00.501999
elapsed time: 0:02:06.356227
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 23:12:51.495410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.25
 ---- batch: 020 ----
mean loss: 142.51
 ---- batch: 030 ----
mean loss: 142.98
 ---- batch: 040 ----
mean loss: 140.66
 ---- batch: 050 ----
mean loss: 141.07
 ---- batch: 060 ----
mean loss: 144.08
 ---- batch: 070 ----
mean loss: 138.58
 ---- batch: 080 ----
mean loss: 142.26
 ---- batch: 090 ----
mean loss: 139.85
train mean loss: 142.19
epoch train time: 0:00:00.511616
elapsed time: 0:02:06.868003
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 23:12:52.007171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.50
 ---- batch: 020 ----
mean loss: 139.14
 ---- batch: 030 ----
mean loss: 138.09
 ---- batch: 040 ----
mean loss: 140.06
 ---- batch: 050 ----
mean loss: 144.70
 ---- batch: 060 ----
mean loss: 144.98
 ---- batch: 070 ----
mean loss: 141.56
 ---- batch: 080 ----
mean loss: 142.12
 ---- batch: 090 ----
mean loss: 141.37
train mean loss: 142.44
epoch train time: 0:00:00.498667
elapsed time: 0:02:07.366814
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 23:12:52.505980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.97
 ---- batch: 020 ----
mean loss: 142.79
 ---- batch: 030 ----
mean loss: 138.44
 ---- batch: 040 ----
mean loss: 144.72
 ---- batch: 050 ----
mean loss: 147.42
 ---- batch: 060 ----
mean loss: 146.00
 ---- batch: 070 ----
mean loss: 140.67
 ---- batch: 080 ----
mean loss: 138.13
 ---- batch: 090 ----
mean loss: 140.45
train mean loss: 142.12
epoch train time: 0:00:00.508070
elapsed time: 0:02:07.875031
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 23:12:53.014204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.42
 ---- batch: 020 ----
mean loss: 136.29
 ---- batch: 030 ----
mean loss: 135.47
 ---- batch: 040 ----
mean loss: 149.30
 ---- batch: 050 ----
mean loss: 147.82
 ---- batch: 060 ----
mean loss: 142.27
 ---- batch: 070 ----
mean loss: 141.27
 ---- batch: 080 ----
mean loss: 141.66
 ---- batch: 090 ----
mean loss: 143.67
train mean loss: 142.03
epoch train time: 0:00:00.502170
elapsed time: 0:02:08.377361
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 23:12:53.516532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.44
 ---- batch: 020 ----
mean loss: 133.56
 ---- batch: 030 ----
mean loss: 147.95
 ---- batch: 040 ----
mean loss: 144.11
 ---- batch: 050 ----
mean loss: 139.15
 ---- batch: 060 ----
mean loss: 146.16
 ---- batch: 070 ----
mean loss: 150.16
 ---- batch: 080 ----
mean loss: 134.90
 ---- batch: 090 ----
mean loss: 149.03
train mean loss: 142.75
epoch train time: 0:00:00.517041
elapsed time: 0:02:08.894583
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 23:12:54.033770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.44
 ---- batch: 020 ----
mean loss: 142.15
 ---- batch: 030 ----
mean loss: 141.25
 ---- batch: 040 ----
mean loss: 141.68
 ---- batch: 050 ----
mean loss: 142.87
 ---- batch: 060 ----
mean loss: 141.08
 ---- batch: 070 ----
mean loss: 139.63
 ---- batch: 080 ----
mean loss: 144.15
 ---- batch: 090 ----
mean loss: 144.85
train mean loss: 142.46
epoch train time: 0:00:00.505222
elapsed time: 0:02:09.399971
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 23:12:54.539155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.62
 ---- batch: 020 ----
mean loss: 141.21
 ---- batch: 030 ----
mean loss: 137.50
 ---- batch: 040 ----
mean loss: 137.99
 ---- batch: 050 ----
mean loss: 142.58
 ---- batch: 060 ----
mean loss: 144.30
 ---- batch: 070 ----
mean loss: 141.03
 ---- batch: 080 ----
mean loss: 146.91
 ---- batch: 090 ----
mean loss: 142.81
train mean loss: 141.44
epoch train time: 0:00:00.522113
elapsed time: 0:02:09.922255
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 23:12:55.061426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.33
 ---- batch: 020 ----
mean loss: 134.07
 ---- batch: 030 ----
mean loss: 142.56
 ---- batch: 040 ----
mean loss: 136.74
 ---- batch: 050 ----
mean loss: 137.31
 ---- batch: 060 ----
mean loss: 134.61
 ---- batch: 070 ----
mean loss: 140.63
 ---- batch: 080 ----
mean loss: 150.97
 ---- batch: 090 ----
mean loss: 151.40
train mean loss: 140.45
epoch train time: 0:00:00.501748
elapsed time: 0:02:10.424164
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 23:12:55.563332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.00
 ---- batch: 020 ----
mean loss: 138.28
 ---- batch: 030 ----
mean loss: 142.06
 ---- batch: 040 ----
mean loss: 141.42
 ---- batch: 050 ----
mean loss: 144.28
 ---- batch: 060 ----
mean loss: 141.68
 ---- batch: 070 ----
mean loss: 139.81
 ---- batch: 080 ----
mean loss: 147.24
 ---- batch: 090 ----
mean loss: 137.40
train mean loss: 140.98
epoch train time: 0:00:00.520647
elapsed time: 0:02:10.944973
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 23:12:56.084175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.08
 ---- batch: 020 ----
mean loss: 139.81
 ---- batch: 030 ----
mean loss: 136.78
 ---- batch: 040 ----
mean loss: 141.36
 ---- batch: 050 ----
mean loss: 137.13
 ---- batch: 060 ----
mean loss: 138.77
 ---- batch: 070 ----
mean loss: 146.99
 ---- batch: 080 ----
mean loss: 146.64
 ---- batch: 090 ----
mean loss: 138.05
train mean loss: 140.05
epoch train time: 0:00:00.512865
elapsed time: 0:02:11.458022
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 23:12:56.597191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.91
 ---- batch: 020 ----
mean loss: 141.82
 ---- batch: 030 ----
mean loss: 141.20
 ---- batch: 040 ----
mean loss: 137.25
 ---- batch: 050 ----
mean loss: 139.43
 ---- batch: 060 ----
mean loss: 145.55
 ---- batch: 070 ----
mean loss: 133.74
 ---- batch: 080 ----
mean loss: 137.61
 ---- batch: 090 ----
mean loss: 144.14
train mean loss: 139.60
epoch train time: 0:00:00.515914
elapsed time: 0:02:11.974078
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 23:12:57.113245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.41
 ---- batch: 020 ----
mean loss: 134.74
 ---- batch: 030 ----
mean loss: 139.15
 ---- batch: 040 ----
mean loss: 140.21
 ---- batch: 050 ----
mean loss: 139.75
 ---- batch: 060 ----
mean loss: 135.95
 ---- batch: 070 ----
mean loss: 139.08
 ---- batch: 080 ----
mean loss: 142.04
 ---- batch: 090 ----
mean loss: 151.04
train mean loss: 139.74
epoch train time: 0:00:00.499199
elapsed time: 0:02:12.473437
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 23:12:57.612627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.08
 ---- batch: 020 ----
mean loss: 136.25
 ---- batch: 030 ----
mean loss: 139.02
 ---- batch: 040 ----
mean loss: 144.01
 ---- batch: 050 ----
mean loss: 139.57
 ---- batch: 060 ----
mean loss: 143.97
 ---- batch: 070 ----
mean loss: 135.05
 ---- batch: 080 ----
mean loss: 140.10
 ---- batch: 090 ----
mean loss: 139.04
train mean loss: 140.09
epoch train time: 0:00:00.520123
elapsed time: 0:02:12.993727
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 23:12:58.132897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.75
 ---- batch: 020 ----
mean loss: 143.24
 ---- batch: 030 ----
mean loss: 143.77
 ---- batch: 040 ----
mean loss: 132.90
 ---- batch: 050 ----
mean loss: 135.00
 ---- batch: 060 ----
mean loss: 138.24
 ---- batch: 070 ----
mean loss: 143.44
 ---- batch: 080 ----
mean loss: 143.94
 ---- batch: 090 ----
mean loss: 142.12
train mean loss: 139.75
epoch train time: 0:00:00.520156
elapsed time: 0:02:13.514033
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 23:12:58.653201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.88
 ---- batch: 020 ----
mean loss: 139.19
 ---- batch: 030 ----
mean loss: 137.76
 ---- batch: 040 ----
mean loss: 138.93
 ---- batch: 050 ----
mean loss: 133.07
 ---- batch: 060 ----
mean loss: 146.43
 ---- batch: 070 ----
mean loss: 140.83
 ---- batch: 080 ----
mean loss: 134.09
 ---- batch: 090 ----
mean loss: 147.52
train mean loss: 140.06
epoch train time: 0:00:00.522392
elapsed time: 0:02:14.036586
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 23:12:59.175770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.09
 ---- batch: 020 ----
mean loss: 135.22
 ---- batch: 030 ----
mean loss: 133.81
 ---- batch: 040 ----
mean loss: 135.14
 ---- batch: 050 ----
mean loss: 138.37
 ---- batch: 060 ----
mean loss: 140.68
 ---- batch: 070 ----
mean loss: 139.93
 ---- batch: 080 ----
mean loss: 143.46
 ---- batch: 090 ----
mean loss: 147.98
train mean loss: 139.34
epoch train time: 0:00:00.507904
elapsed time: 0:02:14.544648
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 23:12:59.683814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.31
 ---- batch: 020 ----
mean loss: 133.31
 ---- batch: 030 ----
mean loss: 138.21
 ---- batch: 040 ----
mean loss: 141.66
 ---- batch: 050 ----
mean loss: 143.51
 ---- batch: 060 ----
mean loss: 141.86
 ---- batch: 070 ----
mean loss: 138.17
 ---- batch: 080 ----
mean loss: 143.18
 ---- batch: 090 ----
mean loss: 138.43
train mean loss: 138.50
epoch train time: 0:00:00.510899
elapsed time: 0:02:15.055693
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 23:13:00.194863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.75
 ---- batch: 020 ----
mean loss: 136.49
 ---- batch: 030 ----
mean loss: 135.72
 ---- batch: 040 ----
mean loss: 137.22
 ---- batch: 050 ----
mean loss: 136.69
 ---- batch: 060 ----
mean loss: 147.27
 ---- batch: 070 ----
mean loss: 141.10
 ---- batch: 080 ----
mean loss: 141.47
 ---- batch: 090 ----
mean loss: 140.67
train mean loss: 139.48
epoch train time: 0:00:00.502370
elapsed time: 0:02:15.558243
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 23:13:00.697425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.65
 ---- batch: 020 ----
mean loss: 141.72
 ---- batch: 030 ----
mean loss: 132.61
 ---- batch: 040 ----
mean loss: 138.93
 ---- batch: 050 ----
mean loss: 137.45
 ---- batch: 060 ----
mean loss: 141.76
 ---- batch: 070 ----
mean loss: 143.36
 ---- batch: 080 ----
mean loss: 142.18
 ---- batch: 090 ----
mean loss: 143.13
train mean loss: 139.45
epoch train time: 0:00:00.512059
elapsed time: 0:02:16.070463
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 23:13:01.209658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.08
 ---- batch: 020 ----
mean loss: 131.20
 ---- batch: 030 ----
mean loss: 130.22
 ---- batch: 040 ----
mean loss: 140.01
 ---- batch: 050 ----
mean loss: 128.81
 ---- batch: 060 ----
mean loss: 127.90
 ---- batch: 070 ----
mean loss: 134.65
 ---- batch: 080 ----
mean loss: 136.95
 ---- batch: 090 ----
mean loss: 132.72
train mean loss: 133.16
epoch train time: 0:00:00.515590
elapsed time: 0:02:16.586258
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 23:13:01.725448
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.58
 ---- batch: 020 ----
mean loss: 128.25
 ---- batch: 030 ----
mean loss: 131.33
 ---- batch: 040 ----
mean loss: 126.43
 ---- batch: 050 ----
mean loss: 133.74
 ---- batch: 060 ----
mean loss: 134.55
 ---- batch: 070 ----
mean loss: 131.66
 ---- batch: 080 ----
mean loss: 132.87
 ---- batch: 090 ----
mean loss: 128.14
train mean loss: 131.72
epoch train time: 0:00:00.510715
elapsed time: 0:02:17.097144
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 23:13:02.236313
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.23
 ---- batch: 020 ----
mean loss: 130.82
 ---- batch: 030 ----
mean loss: 128.49
 ---- batch: 040 ----
mean loss: 132.49
 ---- batch: 050 ----
mean loss: 134.03
 ---- batch: 060 ----
mean loss: 127.63
 ---- batch: 070 ----
mean loss: 132.88
 ---- batch: 080 ----
mean loss: 132.37
 ---- batch: 090 ----
mean loss: 131.40
train mean loss: 131.21
epoch train time: 0:00:00.512418
elapsed time: 0:02:17.609756
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 23:13:02.748922
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.13
 ---- batch: 020 ----
mean loss: 130.61
 ---- batch: 030 ----
mean loss: 135.47
 ---- batch: 040 ----
mean loss: 128.78
 ---- batch: 050 ----
mean loss: 130.30
 ---- batch: 060 ----
mean loss: 130.43
 ---- batch: 070 ----
mean loss: 126.14
 ---- batch: 080 ----
mean loss: 137.42
 ---- batch: 090 ----
mean loss: 129.70
train mean loss: 131.05
epoch train time: 0:00:00.516180
elapsed time: 0:02:18.126100
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 23:13:03.265271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.04
 ---- batch: 020 ----
mean loss: 130.89
 ---- batch: 030 ----
mean loss: 132.72
 ---- batch: 040 ----
mean loss: 124.66
 ---- batch: 050 ----
mean loss: 136.08
 ---- batch: 060 ----
mean loss: 135.80
 ---- batch: 070 ----
mean loss: 129.79
 ---- batch: 080 ----
mean loss: 135.90
 ---- batch: 090 ----
mean loss: 129.27
train mean loss: 131.03
epoch train time: 0:00:00.510615
elapsed time: 0:02:18.636867
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 23:13:03.776034
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.40
 ---- batch: 020 ----
mean loss: 128.78
 ---- batch: 030 ----
mean loss: 131.01
 ---- batch: 040 ----
mean loss: 134.07
 ---- batch: 050 ----
mean loss: 135.94
 ---- batch: 060 ----
mean loss: 129.05
 ---- batch: 070 ----
mean loss: 130.28
 ---- batch: 080 ----
mean loss: 132.07
 ---- batch: 090 ----
mean loss: 133.04
train mean loss: 131.18
epoch train time: 0:00:00.508544
elapsed time: 0:02:19.145557
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 23:13:04.284726
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.19
 ---- batch: 020 ----
mean loss: 125.96
 ---- batch: 030 ----
mean loss: 131.22
 ---- batch: 040 ----
mean loss: 133.75
 ---- batch: 050 ----
mean loss: 136.59
 ---- batch: 060 ----
mean loss: 120.03
 ---- batch: 070 ----
mean loss: 131.94
 ---- batch: 080 ----
mean loss: 132.11
 ---- batch: 090 ----
mean loss: 132.52
train mean loss: 131.17
epoch train time: 0:00:00.505709
elapsed time: 0:02:19.651416
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 23:13:04.790584
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.86
 ---- batch: 020 ----
mean loss: 132.75
 ---- batch: 030 ----
mean loss: 134.06
 ---- batch: 040 ----
mean loss: 134.78
 ---- batch: 050 ----
mean loss: 131.06
 ---- batch: 060 ----
mean loss: 125.00
 ---- batch: 070 ----
mean loss: 130.81
 ---- batch: 080 ----
mean loss: 133.46
 ---- batch: 090 ----
mean loss: 125.35
train mean loss: 130.71
epoch train time: 0:00:00.510382
elapsed time: 0:02:20.161948
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 23:13:05.301136
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.92
 ---- batch: 020 ----
mean loss: 130.85
 ---- batch: 030 ----
mean loss: 129.73
 ---- batch: 040 ----
mean loss: 126.78
 ---- batch: 050 ----
mean loss: 134.81
 ---- batch: 060 ----
mean loss: 135.00
 ---- batch: 070 ----
mean loss: 127.16
 ---- batch: 080 ----
mean loss: 130.20
 ---- batch: 090 ----
mean loss: 134.40
train mean loss: 130.82
epoch train time: 0:00:00.521858
elapsed time: 0:02:20.683984
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 23:13:05.823162
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.48
 ---- batch: 020 ----
mean loss: 131.48
 ---- batch: 030 ----
mean loss: 134.54
 ---- batch: 040 ----
mean loss: 133.28
 ---- batch: 050 ----
mean loss: 127.34
 ---- batch: 060 ----
mean loss: 131.74
 ---- batch: 070 ----
mean loss: 127.50
 ---- batch: 080 ----
mean loss: 136.77
 ---- batch: 090 ----
mean loss: 127.94
train mean loss: 130.69
epoch train time: 0:00:00.502935
elapsed time: 0:02:21.187122
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 23:13:06.326290
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.89
 ---- batch: 020 ----
mean loss: 123.30
 ---- batch: 030 ----
mean loss: 135.63
 ---- batch: 040 ----
mean loss: 131.31
 ---- batch: 050 ----
mean loss: 132.48
 ---- batch: 060 ----
mean loss: 130.08
 ---- batch: 070 ----
mean loss: 135.33
 ---- batch: 080 ----
mean loss: 130.22
 ---- batch: 090 ----
mean loss: 130.29
train mean loss: 130.74
epoch train time: 0:00:00.513273
elapsed time: 0:02:21.700548
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 23:13:06.839723
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.18
 ---- batch: 020 ----
mean loss: 125.17
 ---- batch: 030 ----
mean loss: 131.61
 ---- batch: 040 ----
mean loss: 138.56
 ---- batch: 050 ----
mean loss: 131.27
 ---- batch: 060 ----
mean loss: 125.55
 ---- batch: 070 ----
mean loss: 129.49
 ---- batch: 080 ----
mean loss: 132.90
 ---- batch: 090 ----
mean loss: 130.77
train mean loss: 130.70
epoch train time: 0:00:00.519693
elapsed time: 0:02:22.220413
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 23:13:07.359584
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.00
 ---- batch: 020 ----
mean loss: 131.70
 ---- batch: 030 ----
mean loss: 126.14
 ---- batch: 040 ----
mean loss: 136.23
 ---- batch: 050 ----
mean loss: 137.88
 ---- batch: 060 ----
mean loss: 127.70
 ---- batch: 070 ----
mean loss: 130.97
 ---- batch: 080 ----
mean loss: 130.32
 ---- batch: 090 ----
mean loss: 129.15
train mean loss: 130.57
epoch train time: 0:00:00.514064
elapsed time: 0:02:22.734636
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 23:13:07.873798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.02
 ---- batch: 020 ----
mean loss: 120.30
 ---- batch: 030 ----
mean loss: 129.32
 ---- batch: 040 ----
mean loss: 131.52
 ---- batch: 050 ----
mean loss: 129.65
 ---- batch: 060 ----
mean loss: 130.58
 ---- batch: 070 ----
mean loss: 130.58
 ---- batch: 080 ----
mean loss: 139.27
 ---- batch: 090 ----
mean loss: 134.08
train mean loss: 130.70
epoch train time: 0:00:00.507563
elapsed time: 0:02:23.242353
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 23:13:08.381522
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.24
 ---- batch: 020 ----
mean loss: 127.69
 ---- batch: 030 ----
mean loss: 127.95
 ---- batch: 040 ----
mean loss: 129.20
 ---- batch: 050 ----
mean loss: 130.58
 ---- batch: 060 ----
mean loss: 133.43
 ---- batch: 070 ----
mean loss: 129.10
 ---- batch: 080 ----
mean loss: 135.64
 ---- batch: 090 ----
mean loss: 128.91
train mean loss: 130.75
epoch train time: 0:00:00.513404
elapsed time: 0:02:23.755904
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 23:13:08.895080
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.44
 ---- batch: 020 ----
mean loss: 131.84
 ---- batch: 030 ----
mean loss: 134.07
 ---- batch: 040 ----
mean loss: 130.18
 ---- batch: 050 ----
mean loss: 129.05
 ---- batch: 060 ----
mean loss: 127.50
 ---- batch: 070 ----
mean loss: 129.44
 ---- batch: 080 ----
mean loss: 133.74
 ---- batch: 090 ----
mean loss: 132.79
train mean loss: 130.41
epoch train time: 0:00:00.506107
elapsed time: 0:02:24.262182
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 23:13:09.401385
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.28
 ---- batch: 020 ----
mean loss: 133.48
 ---- batch: 030 ----
mean loss: 131.91
 ---- batch: 040 ----
mean loss: 127.16
 ---- batch: 050 ----
mean loss: 130.72
 ---- batch: 060 ----
mean loss: 129.74
 ---- batch: 070 ----
mean loss: 130.49
 ---- batch: 080 ----
mean loss: 137.14
 ---- batch: 090 ----
mean loss: 129.62
train mean loss: 130.60
epoch train time: 0:00:00.507694
elapsed time: 0:02:24.770069
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 23:13:09.909240
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.94
 ---- batch: 020 ----
mean loss: 130.15
 ---- batch: 030 ----
mean loss: 132.98
 ---- batch: 040 ----
mean loss: 128.85
 ---- batch: 050 ----
mean loss: 127.07
 ---- batch: 060 ----
mean loss: 131.97
 ---- batch: 070 ----
mean loss: 128.11
 ---- batch: 080 ----
mean loss: 134.09
 ---- batch: 090 ----
mean loss: 136.14
train mean loss: 130.47
epoch train time: 0:00:00.510047
elapsed time: 0:02:25.280275
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 23:13:10.419436
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.05
 ---- batch: 020 ----
mean loss: 127.58
 ---- batch: 030 ----
mean loss: 133.64
 ---- batch: 040 ----
mean loss: 136.11
 ---- batch: 050 ----
mean loss: 134.74
 ---- batch: 060 ----
mean loss: 127.96
 ---- batch: 070 ----
mean loss: 124.93
 ---- batch: 080 ----
mean loss: 131.79
 ---- batch: 090 ----
mean loss: 128.73
train mean loss: 130.37
epoch train time: 0:00:00.520551
elapsed time: 0:02:25.800980
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 23:13:10.940194
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.30
 ---- batch: 020 ----
mean loss: 126.21
 ---- batch: 030 ----
mean loss: 125.00
 ---- batch: 040 ----
mean loss: 130.80
 ---- batch: 050 ----
mean loss: 134.03
 ---- batch: 060 ----
mean loss: 131.28
 ---- batch: 070 ----
mean loss: 131.14
 ---- batch: 080 ----
mean loss: 135.08
 ---- batch: 090 ----
mean loss: 131.53
train mean loss: 130.51
epoch train time: 0:00:00.498808
elapsed time: 0:02:26.299978
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 23:13:11.439161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.23
 ---- batch: 020 ----
mean loss: 122.90
 ---- batch: 030 ----
mean loss: 130.98
 ---- batch: 040 ----
mean loss: 131.52
 ---- batch: 050 ----
mean loss: 131.56
 ---- batch: 060 ----
mean loss: 137.64
 ---- batch: 070 ----
mean loss: 129.58
 ---- batch: 080 ----
mean loss: 130.38
 ---- batch: 090 ----
mean loss: 128.63
train mean loss: 130.25
epoch train time: 0:00:00.512940
elapsed time: 0:02:26.813113
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 23:13:11.952283
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.35
 ---- batch: 020 ----
mean loss: 130.22
 ---- batch: 030 ----
mean loss: 133.25
 ---- batch: 040 ----
mean loss: 125.70
 ---- batch: 050 ----
mean loss: 123.43
 ---- batch: 060 ----
mean loss: 132.54
 ---- batch: 070 ----
mean loss: 126.55
 ---- batch: 080 ----
mean loss: 134.92
 ---- batch: 090 ----
mean loss: 134.31
train mean loss: 130.33
epoch train time: 0:00:00.508327
elapsed time: 0:02:27.321586
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 23:13:12.460753
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.57
 ---- batch: 020 ----
mean loss: 131.31
 ---- batch: 030 ----
mean loss: 132.30
 ---- batch: 040 ----
mean loss: 132.46
 ---- batch: 050 ----
mean loss: 126.47
 ---- batch: 060 ----
mean loss: 131.43
 ---- batch: 070 ----
mean loss: 128.10
 ---- batch: 080 ----
mean loss: 131.06
 ---- batch: 090 ----
mean loss: 135.08
train mean loss: 130.31
epoch train time: 0:00:00.525241
elapsed time: 0:02:27.846996
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 23:13:12.986188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.30
 ---- batch: 020 ----
mean loss: 124.25
 ---- batch: 030 ----
mean loss: 130.84
 ---- batch: 040 ----
mean loss: 128.17
 ---- batch: 050 ----
mean loss: 130.31
 ---- batch: 060 ----
mean loss: 127.10
 ---- batch: 070 ----
mean loss: 135.08
 ---- batch: 080 ----
mean loss: 139.72
 ---- batch: 090 ----
mean loss: 131.91
train mean loss: 130.46
epoch train time: 0:00:00.506865
elapsed time: 0:02:28.354044
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 23:13:13.493212
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.91
 ---- batch: 020 ----
mean loss: 137.94
 ---- batch: 030 ----
mean loss: 127.99
 ---- batch: 040 ----
mean loss: 131.38
 ---- batch: 050 ----
mean loss: 130.36
 ---- batch: 060 ----
mean loss: 124.99
 ---- batch: 070 ----
mean loss: 126.91
 ---- batch: 080 ----
mean loss: 130.02
 ---- batch: 090 ----
mean loss: 132.11
train mean loss: 130.27
epoch train time: 0:00:00.509395
elapsed time: 0:02:28.863622
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 23:13:14.002789
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.43
 ---- batch: 020 ----
mean loss: 137.46
 ---- batch: 030 ----
mean loss: 132.14
 ---- batch: 040 ----
mean loss: 130.31
 ---- batch: 050 ----
mean loss: 130.40
 ---- batch: 060 ----
mean loss: 127.80
 ---- batch: 070 ----
mean loss: 125.03
 ---- batch: 080 ----
mean loss: 129.72
 ---- batch: 090 ----
mean loss: 134.52
train mean loss: 130.39
epoch train time: 0:00:00.504512
elapsed time: 0:02:29.368295
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 23:13:14.507468
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.78
 ---- batch: 020 ----
mean loss: 137.46
 ---- batch: 030 ----
mean loss: 129.67
 ---- batch: 040 ----
mean loss: 135.41
 ---- batch: 050 ----
mean loss: 133.77
 ---- batch: 060 ----
mean loss: 127.68
 ---- batch: 070 ----
mean loss: 125.48
 ---- batch: 080 ----
mean loss: 134.65
 ---- batch: 090 ----
mean loss: 124.45
train mean loss: 130.10
epoch train time: 0:00:00.524717
elapsed time: 0:02:29.893191
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 23:13:15.032361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.82
 ---- batch: 020 ----
mean loss: 126.64
 ---- batch: 030 ----
mean loss: 133.29
 ---- batch: 040 ----
mean loss: 133.04
 ---- batch: 050 ----
mean loss: 127.86
 ---- batch: 060 ----
mean loss: 128.24
 ---- batch: 070 ----
mean loss: 133.30
 ---- batch: 080 ----
mean loss: 130.28
 ---- batch: 090 ----
mean loss: 129.79
train mean loss: 130.12
epoch train time: 0:00:00.501911
elapsed time: 0:02:30.395253
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 23:13:15.534422
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.07
 ---- batch: 020 ----
mean loss: 129.52
 ---- batch: 030 ----
mean loss: 127.81
 ---- batch: 040 ----
mean loss: 129.89
 ---- batch: 050 ----
mean loss: 135.98
 ---- batch: 060 ----
mean loss: 133.59
 ---- batch: 070 ----
mean loss: 128.49
 ---- batch: 080 ----
mean loss: 124.81
 ---- batch: 090 ----
mean loss: 134.01
train mean loss: 130.19
epoch train time: 0:00:00.516417
elapsed time: 0:02:30.911818
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 23:13:16.051015
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.39
 ---- batch: 020 ----
mean loss: 132.83
 ---- batch: 030 ----
mean loss: 130.64
 ---- batch: 040 ----
mean loss: 127.71
 ---- batch: 050 ----
mean loss: 127.20
 ---- batch: 060 ----
mean loss: 131.23
 ---- batch: 070 ----
mean loss: 128.63
 ---- batch: 080 ----
mean loss: 132.94
 ---- batch: 090 ----
mean loss: 124.76
train mean loss: 130.16
epoch train time: 0:00:00.497628
elapsed time: 0:02:31.409619
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 23:13:16.548787
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.93
 ---- batch: 020 ----
mean loss: 127.14
 ---- batch: 030 ----
mean loss: 131.76
 ---- batch: 040 ----
mean loss: 133.14
 ---- batch: 050 ----
mean loss: 125.29
 ---- batch: 060 ----
mean loss: 126.38
 ---- batch: 070 ----
mean loss: 133.15
 ---- batch: 080 ----
mean loss: 132.00
 ---- batch: 090 ----
mean loss: 129.60
train mean loss: 129.99
epoch train time: 0:00:00.521750
elapsed time: 0:02:31.931515
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 23:13:17.070701
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.57
 ---- batch: 020 ----
mean loss: 127.70
 ---- batch: 030 ----
mean loss: 128.25
 ---- batch: 040 ----
mean loss: 127.57
 ---- batch: 050 ----
mean loss: 128.17
 ---- batch: 060 ----
mean loss: 131.47
 ---- batch: 070 ----
mean loss: 132.84
 ---- batch: 080 ----
mean loss: 129.77
 ---- batch: 090 ----
mean loss: 130.76
train mean loss: 129.75
epoch train time: 0:00:00.501143
elapsed time: 0:02:32.432828
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 23:13:17.571999
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.10
 ---- batch: 020 ----
mean loss: 131.15
 ---- batch: 030 ----
mean loss: 131.38
 ---- batch: 040 ----
mean loss: 129.94
 ---- batch: 050 ----
mean loss: 124.17
 ---- batch: 060 ----
mean loss: 131.05
 ---- batch: 070 ----
mean loss: 126.08
 ---- batch: 080 ----
mean loss: 131.63
 ---- batch: 090 ----
mean loss: 127.19
train mean loss: 130.08
epoch train time: 0:00:00.513394
elapsed time: 0:02:32.946386
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 23:13:18.085545
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.38
 ---- batch: 020 ----
mean loss: 134.47
 ---- batch: 030 ----
mean loss: 127.30
 ---- batch: 040 ----
mean loss: 129.93
 ---- batch: 050 ----
mean loss: 130.08
 ---- batch: 060 ----
mean loss: 132.03
 ---- batch: 070 ----
mean loss: 131.44
 ---- batch: 080 ----
mean loss: 127.26
 ---- batch: 090 ----
mean loss: 131.10
train mean loss: 129.99
epoch train time: 0:00:00.507994
elapsed time: 0:02:33.454535
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 23:13:18.593704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.23
 ---- batch: 020 ----
mean loss: 131.98
 ---- batch: 030 ----
mean loss: 125.61
 ---- batch: 040 ----
mean loss: 125.21
 ---- batch: 050 ----
mean loss: 133.26
 ---- batch: 060 ----
mean loss: 133.73
 ---- batch: 070 ----
mean loss: 131.89
 ---- batch: 080 ----
mean loss: 131.26
 ---- batch: 090 ----
mean loss: 127.98
train mean loss: 129.77
epoch train time: 0:00:00.506823
elapsed time: 0:02:33.961538
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 23:13:19.100708
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.38
 ---- batch: 020 ----
mean loss: 130.05
 ---- batch: 030 ----
mean loss: 130.62
 ---- batch: 040 ----
mean loss: 135.24
 ---- batch: 050 ----
mean loss: 131.18
 ---- batch: 060 ----
mean loss: 123.07
 ---- batch: 070 ----
mean loss: 127.81
 ---- batch: 080 ----
mean loss: 131.24
 ---- batch: 090 ----
mean loss: 125.90
train mean loss: 129.88
epoch train time: 0:00:00.505997
elapsed time: 0:02:34.467700
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 23:13:19.606867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.70
 ---- batch: 020 ----
mean loss: 126.04
 ---- batch: 030 ----
mean loss: 137.14
 ---- batch: 040 ----
mean loss: 127.03
 ---- batch: 050 ----
mean loss: 130.16
 ---- batch: 060 ----
mean loss: 127.88
 ---- batch: 070 ----
mean loss: 134.38
 ---- batch: 080 ----
mean loss: 131.12
 ---- batch: 090 ----
mean loss: 130.91
train mean loss: 129.90
epoch train time: 0:00:00.512174
elapsed time: 0:02:34.980017
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 23:13:20.119199
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.56
 ---- batch: 020 ----
mean loss: 130.06
 ---- batch: 030 ----
mean loss: 129.81
 ---- batch: 040 ----
mean loss: 127.53
 ---- batch: 050 ----
mean loss: 132.27
 ---- batch: 060 ----
mean loss: 125.54
 ---- batch: 070 ----
mean loss: 131.67
 ---- batch: 080 ----
mean loss: 130.58
 ---- batch: 090 ----
mean loss: 127.28
train mean loss: 129.82
epoch train time: 0:00:00.508485
elapsed time: 0:02:35.488662
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 23:13:20.627855
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.01
 ---- batch: 020 ----
mean loss: 134.17
 ---- batch: 030 ----
mean loss: 128.77
 ---- batch: 040 ----
mean loss: 121.27
 ---- batch: 050 ----
mean loss: 133.35
 ---- batch: 060 ----
mean loss: 131.73
 ---- batch: 070 ----
mean loss: 125.33
 ---- batch: 080 ----
mean loss: 132.85
 ---- batch: 090 ----
mean loss: 130.66
train mean loss: 130.02
epoch train time: 0:00:00.514929
elapsed time: 0:02:36.003765
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 23:13:21.142934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.60
 ---- batch: 020 ----
mean loss: 133.63
 ---- batch: 030 ----
mean loss: 125.33
 ---- batch: 040 ----
mean loss: 122.12
 ---- batch: 050 ----
mean loss: 128.31
 ---- batch: 060 ----
mean loss: 131.04
 ---- batch: 070 ----
mean loss: 133.87
 ---- batch: 080 ----
mean loss: 131.37
 ---- batch: 090 ----
mean loss: 122.88
train mean loss: 129.80
epoch train time: 0:00:00.513509
elapsed time: 0:02:36.517472
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 23:13:21.656647
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.99
 ---- batch: 020 ----
mean loss: 126.85
 ---- batch: 030 ----
mean loss: 134.74
 ---- batch: 040 ----
mean loss: 128.26
 ---- batch: 050 ----
mean loss: 126.81
 ---- batch: 060 ----
mean loss: 124.85
 ---- batch: 070 ----
mean loss: 130.23
 ---- batch: 080 ----
mean loss: 129.25
 ---- batch: 090 ----
mean loss: 129.96
train mean loss: 129.75
epoch train time: 0:00:00.510990
elapsed time: 0:02:37.028615
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 23:13:22.167784
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.96
 ---- batch: 020 ----
mean loss: 124.31
 ---- batch: 030 ----
mean loss: 130.34
 ---- batch: 040 ----
mean loss: 129.79
 ---- batch: 050 ----
mean loss: 124.56
 ---- batch: 060 ----
mean loss: 130.92
 ---- batch: 070 ----
mean loss: 130.69
 ---- batch: 080 ----
mean loss: 130.70
 ---- batch: 090 ----
mean loss: 131.79
train mean loss: 129.64
epoch train time: 0:00:00.499222
elapsed time: 0:02:37.527993
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 23:13:22.667169
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.52
 ---- batch: 020 ----
mean loss: 131.58
 ---- batch: 030 ----
mean loss: 130.14
 ---- batch: 040 ----
mean loss: 127.05
 ---- batch: 050 ----
mean loss: 127.55
 ---- batch: 060 ----
mean loss: 135.99
 ---- batch: 070 ----
mean loss: 131.86
 ---- batch: 080 ----
mean loss: 123.50
 ---- batch: 090 ----
mean loss: 132.44
train mean loss: 129.86
epoch train time: 0:00:00.525651
elapsed time: 0:02:38.053800
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 23:13:23.192970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.16
 ---- batch: 020 ----
mean loss: 135.89
 ---- batch: 030 ----
mean loss: 126.96
 ---- batch: 040 ----
mean loss: 125.22
 ---- batch: 050 ----
mean loss: 127.14
 ---- batch: 060 ----
mean loss: 129.90
 ---- batch: 070 ----
mean loss: 129.85
 ---- batch: 080 ----
mean loss: 123.78
 ---- batch: 090 ----
mean loss: 138.31
train mean loss: 129.66
epoch train time: 0:00:00.514175
elapsed time: 0:02:38.568135
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 23:13:23.707312
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.66
 ---- batch: 020 ----
mean loss: 131.86
 ---- batch: 030 ----
mean loss: 124.13
 ---- batch: 040 ----
mean loss: 129.96
 ---- batch: 050 ----
mean loss: 133.53
 ---- batch: 060 ----
mean loss: 131.05
 ---- batch: 070 ----
mean loss: 130.66
 ---- batch: 080 ----
mean loss: 133.41
 ---- batch: 090 ----
mean loss: 129.60
train mean loss: 129.67
epoch train time: 0:00:00.520053
elapsed time: 0:02:39.088385
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 23:13:24.227561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.80
 ---- batch: 020 ----
mean loss: 128.59
 ---- batch: 030 ----
mean loss: 125.87
 ---- batch: 040 ----
mean loss: 129.91
 ---- batch: 050 ----
mean loss: 130.32
 ---- batch: 060 ----
mean loss: 130.08
 ---- batch: 070 ----
mean loss: 129.87
 ---- batch: 080 ----
mean loss: 127.55
 ---- batch: 090 ----
mean loss: 134.14
train mean loss: 129.59
epoch train time: 0:00:00.515394
elapsed time: 0:02:39.603942
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 23:13:24.743143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.57
 ---- batch: 020 ----
mean loss: 129.76
 ---- batch: 030 ----
mean loss: 129.74
 ---- batch: 040 ----
mean loss: 133.00
 ---- batch: 050 ----
mean loss: 130.80
 ---- batch: 060 ----
mean loss: 129.57
 ---- batch: 070 ----
mean loss: 124.91
 ---- batch: 080 ----
mean loss: 130.31
 ---- batch: 090 ----
mean loss: 131.54
train mean loss: 129.52
epoch train time: 0:00:00.513840
elapsed time: 0:02:40.117968
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 23:13:25.257137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.69
 ---- batch: 020 ----
mean loss: 125.39
 ---- batch: 030 ----
mean loss: 125.63
 ---- batch: 040 ----
mean loss: 130.63
 ---- batch: 050 ----
mean loss: 133.13
 ---- batch: 060 ----
mean loss: 135.59
 ---- batch: 070 ----
mean loss: 137.63
 ---- batch: 080 ----
mean loss: 127.04
 ---- batch: 090 ----
mean loss: 125.35
train mean loss: 129.46
epoch train time: 0:00:00.503886
elapsed time: 0:02:40.622003
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 23:13:25.761173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.63
 ---- batch: 020 ----
mean loss: 130.18
 ---- batch: 030 ----
mean loss: 128.24
 ---- batch: 040 ----
mean loss: 122.29
 ---- batch: 050 ----
mean loss: 132.48
 ---- batch: 060 ----
mean loss: 127.60
 ---- batch: 070 ----
mean loss: 130.59
 ---- batch: 080 ----
mean loss: 136.16
 ---- batch: 090 ----
mean loss: 130.87
train mean loss: 129.42
epoch train time: 0:00:00.507556
elapsed time: 0:02:41.133267
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_3/checkpoint.pth.tar
**** end time: 2019-09-25 23:13:26.272400 ****
