Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_1', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17853
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistDense3...
Done.
**** start time: 2019-09-27 02:07:01.444218 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
            Linear-2                  [-1, 100]          36,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 56,100
Trainable params: 56,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:07:01.447773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4958.06
 ---- batch: 020 ----
mean loss: 4842.82
 ---- batch: 030 ----
mean loss: 4741.35
 ---- batch: 040 ----
mean loss: 4640.17
 ---- batch: 050 ----
mean loss: 4575.40
 ---- batch: 060 ----
mean loss: 4470.90
 ---- batch: 070 ----
mean loss: 4438.24
 ---- batch: 080 ----
mean loss: 4366.95
 ---- batch: 090 ----
mean loss: 4298.26
 ---- batch: 100 ----
mean loss: 4266.41
 ---- batch: 110 ----
mean loss: 4225.42
train mean loss: 4519.39
epoch train time: 0:00:33.245962
elapsed time: 0:00:33.251770
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:07:34.696034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4099.13
 ---- batch: 020 ----
mean loss: 4003.12
 ---- batch: 030 ----
mean loss: 3971.32
 ---- batch: 040 ----
mean loss: 3900.43
 ---- batch: 050 ----
mean loss: 3876.37
 ---- batch: 060 ----
mean loss: 3785.08
 ---- batch: 070 ----
mean loss: 3692.43
 ---- batch: 080 ----
mean loss: 3678.02
 ---- batch: 090 ----
mean loss: 3587.01
 ---- batch: 100 ----
mean loss: 3496.04
 ---- batch: 110 ----
mean loss: 3413.39
train mean loss: 3765.12
epoch train time: 0:00:00.555325
elapsed time: 0:00:33.807234
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:07:35.251498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3393.19
 ---- batch: 020 ----
mean loss: 3337.94
 ---- batch: 030 ----
mean loss: 3312.72
 ---- batch: 040 ----
mean loss: 3255.50
 ---- batch: 050 ----
mean loss: 3195.16
 ---- batch: 060 ----
mean loss: 3133.72
 ---- batch: 070 ----
mean loss: 3107.95
 ---- batch: 080 ----
mean loss: 3023.03
 ---- batch: 090 ----
mean loss: 2961.37
 ---- batch: 100 ----
mean loss: 2954.94
 ---- batch: 110 ----
mean loss: 2826.51
train mean loss: 3130.36
epoch train time: 0:00:00.553278
elapsed time: 0:00:34.360640
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:07:35.804905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2809.34
 ---- batch: 020 ----
mean loss: 2722.64
 ---- batch: 030 ----
mean loss: 2748.84
 ---- batch: 040 ----
mean loss: 2704.89
 ---- batch: 050 ----
mean loss: 2616.66
 ---- batch: 060 ----
mean loss: 2599.89
 ---- batch: 070 ----
mean loss: 2546.06
 ---- batch: 080 ----
mean loss: 2537.56
 ---- batch: 090 ----
mean loss: 2437.73
 ---- batch: 100 ----
mean loss: 2416.01
 ---- batch: 110 ----
mean loss: 2371.71
train mean loss: 2585.30
epoch train time: 0:00:00.567813
elapsed time: 0:00:34.928595
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:07:36.372887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2353.90
 ---- batch: 020 ----
mean loss: 2287.49
 ---- batch: 030 ----
mean loss: 2234.14
 ---- batch: 040 ----
mean loss: 2193.22
 ---- batch: 050 ----
mean loss: 2171.27
 ---- batch: 060 ----
mean loss: 2123.56
 ---- batch: 070 ----
mean loss: 2071.16
 ---- batch: 080 ----
mean loss: 2034.91
 ---- batch: 090 ----
mean loss: 2026.63
 ---- batch: 100 ----
mean loss: 2009.83
 ---- batch: 110 ----
mean loss: 1988.04
train mean loss: 2130.37
epoch train time: 0:00:00.592082
elapsed time: 0:00:35.520836
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:07:36.965130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1943.96
 ---- batch: 020 ----
mean loss: 1909.87
 ---- batch: 030 ----
mean loss: 1874.81
 ---- batch: 040 ----
mean loss: 1877.76
 ---- batch: 050 ----
mean loss: 1801.62
 ---- batch: 060 ----
mean loss: 1776.60
 ---- batch: 070 ----
mean loss: 1744.20
 ---- batch: 080 ----
mean loss: 1716.37
 ---- batch: 090 ----
mean loss: 1707.13
 ---- batch: 100 ----
mean loss: 1701.96
 ---- batch: 110 ----
mean loss: 1663.81
train mean loss: 1789.29
epoch train time: 0:00:00.551592
elapsed time: 0:00:36.072594
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:07:37.516881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1642.58
 ---- batch: 020 ----
mean loss: 1611.92
 ---- batch: 030 ----
mean loss: 1581.60
 ---- batch: 040 ----
mean loss: 1572.63
 ---- batch: 050 ----
mean loss: 1586.34
 ---- batch: 060 ----
mean loss: 1503.30
 ---- batch: 070 ----
mean loss: 1497.18
 ---- batch: 080 ----
mean loss: 1475.14
 ---- batch: 090 ----
mean loss: 1470.40
 ---- batch: 100 ----
mean loss: 1434.29
 ---- batch: 110 ----
mean loss: 1451.23
train mean loss: 1526.68
epoch train time: 0:00:00.560483
elapsed time: 0:00:36.633241
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:07:38.077524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1388.20
 ---- batch: 020 ----
mean loss: 1397.49
 ---- batch: 030 ----
mean loss: 1356.66
 ---- batch: 040 ----
mean loss: 1338.17
 ---- batch: 050 ----
mean loss: 1336.87
 ---- batch: 060 ----
mean loss: 1321.76
 ---- batch: 070 ----
mean loss: 1303.71
 ---- batch: 080 ----
mean loss: 1292.59
 ---- batch: 090 ----
mean loss: 1296.74
 ---- batch: 100 ----
mean loss: 1282.72
 ---- batch: 110 ----
mean loss: 1228.73
train mean loss: 1320.46
epoch train time: 0:00:00.549388
elapsed time: 0:00:37.182816
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:07:38.627084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1219.47
 ---- batch: 020 ----
mean loss: 1226.60
 ---- batch: 030 ----
mean loss: 1195.73
 ---- batch: 040 ----
mean loss: 1177.24
 ---- batch: 050 ----
mean loss: 1180.76
 ---- batch: 060 ----
mean loss: 1176.90
 ---- batch: 070 ----
mean loss: 1163.23
 ---- batch: 080 ----
mean loss: 1145.29
 ---- batch: 090 ----
mean loss: 1130.67
 ---- batch: 100 ----
mean loss: 1131.91
 ---- batch: 110 ----
mean loss: 1124.38
train mean loss: 1167.77
epoch train time: 0:00:00.570448
elapsed time: 0:00:37.753424
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:07:39.197710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1099.87
 ---- batch: 020 ----
mean loss: 1098.38
 ---- batch: 030 ----
mean loss: 1070.45
 ---- batch: 040 ----
mean loss: 1063.68
 ---- batch: 050 ----
mean loss: 1057.71
 ---- batch: 060 ----
mean loss: 1058.11
 ---- batch: 070 ----
mean loss: 1052.73
 ---- batch: 080 ----
mean loss: 1043.86
 ---- batch: 090 ----
mean loss: 1034.24
 ---- batch: 100 ----
mean loss: 1011.37
 ---- batch: 110 ----
mean loss: 1027.52
train mean loss: 1055.14
epoch train time: 0:00:00.563333
elapsed time: 0:00:38.316929
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:07:39.761221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1006.12
 ---- batch: 020 ----
mean loss: 996.15
 ---- batch: 030 ----
mean loss: 997.80
 ---- batch: 040 ----
mean loss: 994.83
 ---- batch: 050 ----
mean loss: 984.27
 ---- batch: 060 ----
mean loss: 978.66
 ---- batch: 070 ----
mean loss: 963.62
 ---- batch: 080 ----
mean loss: 965.58
 ---- batch: 090 ----
mean loss: 970.28
 ---- batch: 100 ----
mean loss: 965.23
 ---- batch: 110 ----
mean loss: 948.94
train mean loss: 978.52
epoch train time: 0:00:00.566603
elapsed time: 0:00:38.883715
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:07:40.327984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 959.17
 ---- batch: 020 ----
mean loss: 938.66
 ---- batch: 030 ----
mean loss: 940.33
 ---- batch: 040 ----
mean loss: 930.04
 ---- batch: 050 ----
mean loss: 919.27
 ---- batch: 060 ----
mean loss: 924.99
 ---- batch: 070 ----
mean loss: 928.30
 ---- batch: 080 ----
mean loss: 918.34
 ---- batch: 090 ----
mean loss: 922.20
 ---- batch: 100 ----
mean loss: 916.36
 ---- batch: 110 ----
mean loss: 900.58
train mean loss: 927.14
epoch train time: 0:00:00.551803
elapsed time: 0:00:39.435653
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:07:40.879919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.12
 ---- batch: 020 ----
mean loss: 912.55
 ---- batch: 030 ----
mean loss: 906.59
 ---- batch: 040 ----
mean loss: 896.80
 ---- batch: 050 ----
mean loss: 891.11
 ---- batch: 060 ----
mean loss: 877.38
 ---- batch: 070 ----
mean loss: 901.92
 ---- batch: 080 ----
mean loss: 875.69
 ---- batch: 090 ----
mean loss: 884.25
 ---- batch: 100 ----
mean loss: 895.89
 ---- batch: 110 ----
mean loss: 871.07
train mean loss: 893.64
epoch train time: 0:00:00.547318
elapsed time: 0:00:39.983108
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:07:41.427376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.16
 ---- batch: 020 ----
mean loss: 874.44
 ---- batch: 030 ----
mean loss: 874.06
 ---- batch: 040 ----
mean loss: 866.36
 ---- batch: 050 ----
mean loss: 869.11
 ---- batch: 060 ----
mean loss: 881.50
 ---- batch: 070 ----
mean loss: 873.97
 ---- batch: 080 ----
mean loss: 875.52
 ---- batch: 090 ----
mean loss: 863.74
 ---- batch: 100 ----
mean loss: 872.99
 ---- batch: 110 ----
mean loss: 872.64
train mean loss: 872.64
epoch train time: 0:00:00.565601
elapsed time: 0:00:40.548872
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:07:41.993139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.76
 ---- batch: 020 ----
mean loss: 859.99
 ---- batch: 030 ----
mean loss: 876.31
 ---- batch: 040 ----
mean loss: 873.62
 ---- batch: 050 ----
mean loss: 860.10
 ---- batch: 060 ----
mean loss: 851.83
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 847.26
 ---- batch: 090 ----
mean loss: 856.09
 ---- batch: 100 ----
mean loss: 848.74
 ---- batch: 110 ----
mean loss: 868.99
train mean loss: 859.18
epoch train time: 0:00:00.557621
elapsed time: 0:00:41.106632
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:07:42.550901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.34
 ---- batch: 020 ----
mean loss: 859.20
 ---- batch: 030 ----
mean loss: 849.89
 ---- batch: 040 ----
mean loss: 838.16
 ---- batch: 050 ----
mean loss: 848.68
 ---- batch: 060 ----
mean loss: 855.30
 ---- batch: 070 ----
mean loss: 865.00
 ---- batch: 080 ----
mean loss: 840.29
 ---- batch: 090 ----
mean loss: 844.33
 ---- batch: 100 ----
mean loss: 858.68
 ---- batch: 110 ----
mean loss: 838.38
train mean loss: 851.65
epoch train time: 0:00:00.562769
elapsed time: 0:00:41.669546
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:07:43.113816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.48
 ---- batch: 020 ----
mean loss: 821.22
 ---- batch: 030 ----
mean loss: 846.92
 ---- batch: 040 ----
mean loss: 866.14
 ---- batch: 050 ----
mean loss: 866.73
 ---- batch: 060 ----
mean loss: 862.12
 ---- batch: 070 ----
mean loss: 856.91
 ---- batch: 080 ----
mean loss: 847.95
 ---- batch: 090 ----
mean loss: 833.87
 ---- batch: 100 ----
mean loss: 839.20
 ---- batch: 110 ----
mean loss: 839.76
train mean loss: 847.71
epoch train time: 0:00:00.559666
elapsed time: 0:00:42.229357
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:07:43.673668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 825.92
 ---- batch: 020 ----
mean loss: 849.98
 ---- batch: 030 ----
mean loss: 833.72
 ---- batch: 040 ----
mean loss: 846.74
 ---- batch: 050 ----
mean loss: 861.09
 ---- batch: 060 ----
mean loss: 825.52
 ---- batch: 070 ----
mean loss: 859.58
 ---- batch: 080 ----
mean loss: 841.81
 ---- batch: 090 ----
mean loss: 842.23
 ---- batch: 100 ----
mean loss: 860.77
 ---- batch: 110 ----
mean loss: 856.64
train mean loss: 845.57
epoch train time: 0:00:00.558885
elapsed time: 0:00:42.788422
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:07:44.232690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.85
 ---- batch: 020 ----
mean loss: 856.10
 ---- batch: 030 ----
mean loss: 843.40
 ---- batch: 040 ----
mean loss: 822.97
 ---- batch: 050 ----
mean loss: 835.65
 ---- batch: 060 ----
mean loss: 852.11
 ---- batch: 070 ----
mean loss: 841.58
 ---- batch: 080 ----
mean loss: 842.08
 ---- batch: 090 ----
mean loss: 852.60
 ---- batch: 100 ----
mean loss: 840.38
 ---- batch: 110 ----
mean loss: 863.08
train mean loss: 844.55
epoch train time: 0:00:00.561860
elapsed time: 0:00:43.350423
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:07:44.794688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.03
 ---- batch: 020 ----
mean loss: 852.86
 ---- batch: 030 ----
mean loss: 854.75
 ---- batch: 040 ----
mean loss: 838.73
 ---- batch: 050 ----
mean loss: 828.43
 ---- batch: 060 ----
mean loss: 851.64
 ---- batch: 070 ----
mean loss: 843.04
 ---- batch: 080 ----
mean loss: 848.95
 ---- batch: 090 ----
mean loss: 838.05
 ---- batch: 100 ----
mean loss: 846.11
 ---- batch: 110 ----
mean loss: 851.24
train mean loss: 844.19
epoch train time: 0:00:00.565417
elapsed time: 0:00:43.915991
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:07:45.360260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.85
 ---- batch: 020 ----
mean loss: 880.91
 ---- batch: 030 ----
mean loss: 843.28
 ---- batch: 040 ----
mean loss: 869.77
 ---- batch: 050 ----
mean loss: 847.84
 ---- batch: 060 ----
mean loss: 859.65
 ---- batch: 070 ----
mean loss: 840.66
 ---- batch: 080 ----
mean loss: 857.19
 ---- batch: 090 ----
mean loss: 830.11
 ---- batch: 100 ----
mean loss: 826.10
 ---- batch: 110 ----
mean loss: 827.71
train mean loss: 844.07
epoch train time: 0:00:00.584893
elapsed time: 0:00:44.501036
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:07:45.945307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.67
 ---- batch: 020 ----
mean loss: 856.33
 ---- batch: 030 ----
mean loss: 828.47
 ---- batch: 040 ----
mean loss: 859.02
 ---- batch: 050 ----
mean loss: 850.50
 ---- batch: 060 ----
mean loss: 841.68
 ---- batch: 070 ----
mean loss: 853.77
 ---- batch: 080 ----
mean loss: 837.56
 ---- batch: 090 ----
mean loss: 835.49
 ---- batch: 100 ----
mean loss: 831.83
 ---- batch: 110 ----
mean loss: 853.07
train mean loss: 844.00
epoch train time: 0:00:00.592234
elapsed time: 0:00:45.093451
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:07:46.537723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.45
 ---- batch: 020 ----
mean loss: 834.57
 ---- batch: 030 ----
mean loss: 818.95
 ---- batch: 040 ----
mean loss: 844.21
 ---- batch: 050 ----
mean loss: 871.48
 ---- batch: 060 ----
mean loss: 835.61
 ---- batch: 070 ----
mean loss: 864.78
 ---- batch: 080 ----
mean loss: 832.26
 ---- batch: 090 ----
mean loss: 846.15
 ---- batch: 100 ----
mean loss: 862.06
 ---- batch: 110 ----
mean loss: 844.17
train mean loss: 844.03
epoch train time: 0:00:00.575685
elapsed time: 0:00:45.669290
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:07:47.113626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.39
 ---- batch: 020 ----
mean loss: 841.60
 ---- batch: 030 ----
mean loss: 856.90
 ---- batch: 040 ----
mean loss: 844.26
 ---- batch: 050 ----
mean loss: 860.70
 ---- batch: 060 ----
mean loss: 834.28
 ---- batch: 070 ----
mean loss: 833.50
 ---- batch: 080 ----
mean loss: 858.25
 ---- batch: 090 ----
mean loss: 841.83
 ---- batch: 100 ----
mean loss: 851.80
 ---- batch: 110 ----
mean loss: 829.23
train mean loss: 843.99
epoch train time: 0:00:00.547760
elapsed time: 0:00:46.217277
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:07:47.661543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.90
 ---- batch: 020 ----
mean loss: 842.31
 ---- batch: 030 ----
mean loss: 827.30
 ---- batch: 040 ----
mean loss: 849.85
 ---- batch: 050 ----
mean loss: 846.90
 ---- batch: 060 ----
mean loss: 858.68
 ---- batch: 070 ----
mean loss: 818.89
 ---- batch: 080 ----
mean loss: 846.67
 ---- batch: 090 ----
mean loss: 854.56
 ---- batch: 100 ----
mean loss: 835.48
 ---- batch: 110 ----
mean loss: 854.71
train mean loss: 844.00
epoch train time: 0:00:00.550425
elapsed time: 0:00:46.767834
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:07:48.212098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.76
 ---- batch: 020 ----
mean loss: 845.49
 ---- batch: 030 ----
mean loss: 843.53
 ---- batch: 040 ----
mean loss: 840.50
 ---- batch: 050 ----
mean loss: 834.65
 ---- batch: 060 ----
mean loss: 852.35
 ---- batch: 070 ----
mean loss: 864.89
 ---- batch: 080 ----
mean loss: 830.97
 ---- batch: 090 ----
mean loss: 848.37
 ---- batch: 100 ----
mean loss: 838.74
 ---- batch: 110 ----
mean loss: 838.40
train mean loss: 843.96
epoch train time: 0:00:00.575378
elapsed time: 0:00:47.343363
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:07:48.787629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.07
 ---- batch: 020 ----
mean loss: 852.37
 ---- batch: 030 ----
mean loss: 858.97
 ---- batch: 040 ----
mean loss: 845.35
 ---- batch: 050 ----
mean loss: 839.65
 ---- batch: 060 ----
mean loss: 819.39
 ---- batch: 070 ----
mean loss: 828.48
 ---- batch: 080 ----
mean loss: 860.89
 ---- batch: 090 ----
mean loss: 863.14
 ---- batch: 100 ----
mean loss: 836.78
 ---- batch: 110 ----
mean loss: 837.83
train mean loss: 843.92
epoch train time: 0:00:00.559044
elapsed time: 0:00:47.902545
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:07:49.346812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.02
 ---- batch: 020 ----
mean loss: 834.65
 ---- batch: 030 ----
mean loss: 854.13
 ---- batch: 040 ----
mean loss: 865.57
 ---- batch: 050 ----
mean loss: 838.80
 ---- batch: 060 ----
mean loss: 839.51
 ---- batch: 070 ----
mean loss: 829.95
 ---- batch: 080 ----
mean loss: 849.25
 ---- batch: 090 ----
mean loss: 852.49
 ---- batch: 100 ----
mean loss: 835.78
 ---- batch: 110 ----
mean loss: 847.17
train mean loss: 843.85
epoch train time: 0:00:00.555224
elapsed time: 0:00:48.457955
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:07:49.902223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.60
 ---- batch: 020 ----
mean loss: 839.61
 ---- batch: 030 ----
mean loss: 857.25
 ---- batch: 040 ----
mean loss: 861.74
 ---- batch: 050 ----
mean loss: 858.49
 ---- batch: 060 ----
mean loss: 838.93
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 844.46
 ---- batch: 090 ----
mean loss: 822.49
 ---- batch: 100 ----
mean loss: 855.87
 ---- batch: 110 ----
mean loss: 837.78
train mean loss: 843.94
epoch train time: 0:00:00.555541
elapsed time: 0:00:49.013635
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:07:50.457905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 823.18
 ---- batch: 030 ----
mean loss: 834.62
 ---- batch: 040 ----
mean loss: 843.74
 ---- batch: 050 ----
mean loss: 847.57
 ---- batch: 060 ----
mean loss: 842.47
 ---- batch: 070 ----
mean loss: 847.41
 ---- batch: 080 ----
mean loss: 859.33
 ---- batch: 090 ----
mean loss: 843.56
 ---- batch: 100 ----
mean loss: 848.51
 ---- batch: 110 ----
mean loss: 841.92
train mean loss: 843.91
epoch train time: 0:00:00.561884
elapsed time: 0:00:49.575654
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:07:51.019944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.91
 ---- batch: 020 ----
mean loss: 839.22
 ---- batch: 030 ----
mean loss: 847.48
 ---- batch: 040 ----
mean loss: 860.20
 ---- batch: 050 ----
mean loss: 844.45
 ---- batch: 060 ----
mean loss: 840.44
 ---- batch: 070 ----
mean loss: 807.10
 ---- batch: 080 ----
mean loss: 851.62
 ---- batch: 090 ----
mean loss: 844.55
 ---- batch: 100 ----
mean loss: 855.06
 ---- batch: 110 ----
mean loss: 856.51
train mean loss: 844.02
epoch train time: 0:00:00.559824
elapsed time: 0:00:50.135638
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:07:51.579916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.92
 ---- batch: 020 ----
mean loss: 833.89
 ---- batch: 030 ----
mean loss: 864.36
 ---- batch: 040 ----
mean loss: 867.12
 ---- batch: 050 ----
mean loss: 818.04
 ---- batch: 060 ----
mean loss: 833.13
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 846.87
 ---- batch: 090 ----
mean loss: 842.99
 ---- batch: 100 ----
mean loss: 845.87
 ---- batch: 110 ----
mean loss: 831.11
train mean loss: 844.04
epoch train time: 0:00:00.556137
elapsed time: 0:00:50.691921
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:07:52.136201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.77
 ---- batch: 020 ----
mean loss: 832.53
 ---- batch: 030 ----
mean loss: 855.60
 ---- batch: 040 ----
mean loss: 864.32
 ---- batch: 050 ----
mean loss: 850.39
 ---- batch: 060 ----
mean loss: 854.95
 ---- batch: 070 ----
mean loss: 825.56
 ---- batch: 080 ----
mean loss: 851.27
 ---- batch: 090 ----
mean loss: 841.01
 ---- batch: 100 ----
mean loss: 851.35
 ---- batch: 110 ----
mean loss: 831.01
train mean loss: 843.87
epoch train time: 0:00:00.553484
elapsed time: 0:00:51.245554
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:07:52.689821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.60
 ---- batch: 020 ----
mean loss: 848.83
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 843.44
 ---- batch: 050 ----
mean loss: 824.87
 ---- batch: 060 ----
mean loss: 836.67
 ---- batch: 070 ----
mean loss: 843.43
 ---- batch: 080 ----
mean loss: 858.18
 ---- batch: 090 ----
mean loss: 855.87
 ---- batch: 100 ----
mean loss: 850.60
 ---- batch: 110 ----
mean loss: 855.83
train mean loss: 843.97
epoch train time: 0:00:00.550922
elapsed time: 0:00:51.796653
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:07:53.240929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.51
 ---- batch: 020 ----
mean loss: 814.72
 ---- batch: 030 ----
mean loss: 838.89
 ---- batch: 040 ----
mean loss: 851.10
 ---- batch: 050 ----
mean loss: 848.75
 ---- batch: 060 ----
mean loss: 852.61
 ---- batch: 070 ----
mean loss: 834.73
 ---- batch: 080 ----
mean loss: 850.42
 ---- batch: 090 ----
mean loss: 854.91
 ---- batch: 100 ----
mean loss: 853.50
 ---- batch: 110 ----
mean loss: 840.83
train mean loss: 844.05
epoch train time: 0:00:00.547963
elapsed time: 0:00:52.344763
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:07:53.789046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.12
 ---- batch: 020 ----
mean loss: 861.63
 ---- batch: 030 ----
mean loss: 838.12
 ---- batch: 040 ----
mean loss: 845.02
 ---- batch: 050 ----
mean loss: 836.21
 ---- batch: 060 ----
mean loss: 835.64
 ---- batch: 070 ----
mean loss: 842.72
 ---- batch: 080 ----
mean loss: 851.19
 ---- batch: 090 ----
mean loss: 833.50
 ---- batch: 100 ----
mean loss: 834.01
 ---- batch: 110 ----
mean loss: 834.96
train mean loss: 844.04
epoch train time: 0:00:00.551931
elapsed time: 0:00:52.896888
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:07:54.341166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.26
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 855.51
 ---- batch: 040 ----
mean loss: 842.87
 ---- batch: 050 ----
mean loss: 826.74
 ---- batch: 060 ----
mean loss: 850.21
 ---- batch: 070 ----
mean loss: 835.43
 ---- batch: 080 ----
mean loss: 827.13
 ---- batch: 090 ----
mean loss: 864.26
 ---- batch: 100 ----
mean loss: 852.66
 ---- batch: 110 ----
mean loss: 827.14
train mean loss: 843.95
epoch train time: 0:00:00.558421
elapsed time: 0:00:53.455455
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:07:54.899729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.02
 ---- batch: 020 ----
mean loss: 855.72
 ---- batch: 030 ----
mean loss: 830.99
 ---- batch: 040 ----
mean loss: 838.39
 ---- batch: 050 ----
mean loss: 868.38
 ---- batch: 060 ----
mean loss: 833.24
 ---- batch: 070 ----
mean loss: 839.23
 ---- batch: 080 ----
mean loss: 829.76
 ---- batch: 090 ----
mean loss: 826.33
 ---- batch: 100 ----
mean loss: 855.08
 ---- batch: 110 ----
mean loss: 855.25
train mean loss: 843.94
epoch train time: 0:00:00.555631
elapsed time: 0:00:54.011245
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:07:55.455510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.59
 ---- batch: 020 ----
mean loss: 836.38
 ---- batch: 030 ----
mean loss: 854.91
 ---- batch: 040 ----
mean loss: 859.66
 ---- batch: 050 ----
mean loss: 817.84
 ---- batch: 060 ----
mean loss: 832.36
 ---- batch: 070 ----
mean loss: 858.03
 ---- batch: 080 ----
mean loss: 854.70
 ---- batch: 090 ----
mean loss: 824.22
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 844.56
train mean loss: 844.09
epoch train time: 0:00:00.552473
elapsed time: 0:00:54.563850
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:07:56.008117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.50
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 842.46
 ---- batch: 040 ----
mean loss: 845.67
 ---- batch: 050 ----
mean loss: 833.50
 ---- batch: 060 ----
mean loss: 835.28
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 852.60
 ---- batch: 090 ----
mean loss: 845.35
 ---- batch: 100 ----
mean loss: 850.67
 ---- batch: 110 ----
mean loss: 849.95
train mean loss: 844.01
epoch train time: 0:00:00.546951
elapsed time: 0:00:55.110952
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:07:56.555221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.99
 ---- batch: 020 ----
mean loss: 855.23
 ---- batch: 030 ----
mean loss: 833.33
 ---- batch: 040 ----
mean loss: 858.46
 ---- batch: 050 ----
mean loss: 855.47
 ---- batch: 060 ----
mean loss: 845.19
 ---- batch: 070 ----
mean loss: 832.66
 ---- batch: 080 ----
mean loss: 846.02
 ---- batch: 090 ----
mean loss: 835.69
 ---- batch: 100 ----
mean loss: 840.27
 ---- batch: 110 ----
mean loss: 845.14
train mean loss: 843.93
epoch train time: 0:00:00.571792
elapsed time: 0:00:55.682893
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:07:57.127161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.50
 ---- batch: 020 ----
mean loss: 825.58
 ---- batch: 030 ----
mean loss: 839.15
 ---- batch: 040 ----
mean loss: 848.79
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 850.23
 ---- batch: 070 ----
mean loss: 845.20
 ---- batch: 080 ----
mean loss: 866.49
 ---- batch: 090 ----
mean loss: 853.99
 ---- batch: 100 ----
mean loss: 844.03
 ---- batch: 110 ----
mean loss: 822.72
train mean loss: 843.96
epoch train time: 0:00:00.562355
elapsed time: 0:00:56.245437
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:07:57.689718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.68
 ---- batch: 020 ----
mean loss: 835.69
 ---- batch: 030 ----
mean loss: 846.89
 ---- batch: 040 ----
mean loss: 835.59
 ---- batch: 050 ----
mean loss: 868.30
 ---- batch: 060 ----
mean loss: 830.60
 ---- batch: 070 ----
mean loss: 843.45
 ---- batch: 080 ----
mean loss: 856.95
 ---- batch: 090 ----
mean loss: 841.46
 ---- batch: 100 ----
mean loss: 842.00
 ---- batch: 110 ----
mean loss: 840.71
train mean loss: 844.08
epoch train time: 0:00:00.548055
elapsed time: 0:00:56.793641
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:07:58.237925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.32
 ---- batch: 020 ----
mean loss: 848.50
 ---- batch: 030 ----
mean loss: 852.80
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 845.95
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 809.16
 ---- batch: 080 ----
mean loss: 856.45
 ---- batch: 090 ----
mean loss: 845.24
 ---- batch: 100 ----
mean loss: 849.24
 ---- batch: 110 ----
mean loss: 851.07
train mean loss: 844.06
epoch train time: 0:00:00.549345
elapsed time: 0:00:57.343145
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:07:58.787435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.24
 ---- batch: 020 ----
mean loss: 835.81
 ---- batch: 030 ----
mean loss: 850.60
 ---- batch: 040 ----
mean loss: 858.12
 ---- batch: 050 ----
mean loss: 833.74
 ---- batch: 060 ----
mean loss: 865.07
 ---- batch: 070 ----
mean loss: 852.63
 ---- batch: 080 ----
mean loss: 833.13
 ---- batch: 090 ----
mean loss: 822.31
 ---- batch: 100 ----
mean loss: 830.73
 ---- batch: 110 ----
mean loss: 843.90
train mean loss: 844.04
epoch train time: 0:00:00.552492
elapsed time: 0:00:57.895799
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:07:59.340065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.99
 ---- batch: 020 ----
mean loss: 869.91
 ---- batch: 030 ----
mean loss: 857.71
 ---- batch: 040 ----
mean loss: 850.94
 ---- batch: 050 ----
mean loss: 831.70
 ---- batch: 060 ----
mean loss: 842.32
 ---- batch: 070 ----
mean loss: 837.32
 ---- batch: 080 ----
mean loss: 844.26
 ---- batch: 090 ----
mean loss: 840.97
 ---- batch: 100 ----
mean loss: 843.34
 ---- batch: 110 ----
mean loss: 853.45
train mean loss: 843.94
epoch train time: 0:00:00.557902
elapsed time: 0:00:58.453837
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:07:59.898133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.24
 ---- batch: 020 ----
mean loss: 826.17
 ---- batch: 030 ----
mean loss: 857.23
 ---- batch: 040 ----
mean loss: 855.75
 ---- batch: 050 ----
mean loss: 829.66
 ---- batch: 060 ----
mean loss: 825.05
 ---- batch: 070 ----
mean loss: 862.09
 ---- batch: 080 ----
mean loss: 821.70
 ---- batch: 090 ----
mean loss: 868.27
 ---- batch: 100 ----
mean loss: 840.49
 ---- batch: 110 ----
mean loss: 854.33
train mean loss: 844.02
epoch train time: 0:00:00.555504
elapsed time: 0:00:59.009511
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:08:00.453779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.59
 ---- batch: 020 ----
mean loss: 839.91
 ---- batch: 030 ----
mean loss: 832.06
 ---- batch: 040 ----
mean loss: 833.44
 ---- batch: 050 ----
mean loss: 843.63
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 865.28
 ---- batch: 080 ----
mean loss: 850.58
 ---- batch: 090 ----
mean loss: 864.03
 ---- batch: 100 ----
mean loss: 829.88
 ---- batch: 110 ----
mean loss: 840.02
train mean loss: 844.02
epoch train time: 0:00:00.561491
elapsed time: 0:00:59.571144
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:08:01.015413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.87
 ---- batch: 020 ----
mean loss: 843.80
 ---- batch: 030 ----
mean loss: 851.04
 ---- batch: 040 ----
mean loss: 856.67
 ---- batch: 050 ----
mean loss: 832.84
 ---- batch: 060 ----
mean loss: 851.55
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 833.22
 ---- batch: 090 ----
mean loss: 833.16
 ---- batch: 100 ----
mean loss: 851.11
 ---- batch: 110 ----
mean loss: 824.81
train mean loss: 844.02
epoch train time: 0:00:00.550518
elapsed time: 0:01:00.121803
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:08:01.566070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.72
 ---- batch: 020 ----
mean loss: 849.35
 ---- batch: 030 ----
mean loss: 854.59
 ---- batch: 040 ----
mean loss: 833.74
 ---- batch: 050 ----
mean loss: 834.98
 ---- batch: 060 ----
mean loss: 847.40
 ---- batch: 070 ----
mean loss: 828.85
 ---- batch: 080 ----
mean loss: 845.63
 ---- batch: 090 ----
mean loss: 847.92
 ---- batch: 100 ----
mean loss: 839.45
 ---- batch: 110 ----
mean loss: 846.27
train mean loss: 844.00
epoch train time: 0:00:00.546051
elapsed time: 0:01:00.667988
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:08:02.112294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.21
 ---- batch: 020 ----
mean loss: 846.51
 ---- batch: 030 ----
mean loss: 862.86
 ---- batch: 040 ----
mean loss: 852.97
 ---- batch: 050 ----
mean loss: 837.18
 ---- batch: 060 ----
mean loss: 840.37
 ---- batch: 070 ----
mean loss: 860.78
 ---- batch: 080 ----
mean loss: 855.69
 ---- batch: 090 ----
mean loss: 833.01
 ---- batch: 100 ----
mean loss: 817.31
 ---- batch: 110 ----
mean loss: 839.15
train mean loss: 843.95
epoch train time: 0:00:00.546391
elapsed time: 0:01:01.214551
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:08:02.658816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.07
 ---- batch: 020 ----
mean loss: 842.82
 ---- batch: 030 ----
mean loss: 842.87
 ---- batch: 040 ----
mean loss: 838.15
 ---- batch: 050 ----
mean loss: 844.73
 ---- batch: 060 ----
mean loss: 863.77
 ---- batch: 070 ----
mean loss: 842.70
 ---- batch: 080 ----
mean loss: 808.24
 ---- batch: 090 ----
mean loss: 833.56
 ---- batch: 100 ----
mean loss: 859.02
 ---- batch: 110 ----
mean loss: 849.06
train mean loss: 844.05
epoch train time: 0:00:00.560840
elapsed time: 0:01:01.775542
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:08:03.219808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.95
 ---- batch: 020 ----
mean loss: 835.17
 ---- batch: 030 ----
mean loss: 837.94
 ---- batch: 040 ----
mean loss: 824.16
 ---- batch: 050 ----
mean loss: 839.35
 ---- batch: 060 ----
mean loss: 857.76
 ---- batch: 070 ----
mean loss: 822.82
 ---- batch: 080 ----
mean loss: 857.44
 ---- batch: 090 ----
mean loss: 835.61
 ---- batch: 100 ----
mean loss: 851.21
 ---- batch: 110 ----
mean loss: 863.64
train mean loss: 844.00
epoch train time: 0:00:00.555010
elapsed time: 0:01:02.330727
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:08:03.775011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.58
 ---- batch: 020 ----
mean loss: 858.18
 ---- batch: 030 ----
mean loss: 852.88
 ---- batch: 040 ----
mean loss: 839.77
 ---- batch: 050 ----
mean loss: 830.19
 ---- batch: 060 ----
mean loss: 841.44
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 849.65
 ---- batch: 090 ----
mean loss: 832.65
 ---- batch: 100 ----
mean loss: 836.72
 ---- batch: 110 ----
mean loss: 842.07
train mean loss: 843.93
epoch train time: 0:00:00.560901
elapsed time: 0:01:02.891800
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:08:04.336070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.38
 ---- batch: 020 ----
mean loss: 853.02
 ---- batch: 030 ----
mean loss: 847.40
 ---- batch: 040 ----
mean loss: 842.31
 ---- batch: 050 ----
mean loss: 854.32
 ---- batch: 060 ----
mean loss: 802.54
 ---- batch: 070 ----
mean loss: 822.20
 ---- batch: 080 ----
mean loss: 772.28
 ---- batch: 090 ----
mean loss: 773.55
 ---- batch: 100 ----
mean loss: 772.57
 ---- batch: 110 ----
mean loss: 773.02
train mean loss: 813.78
epoch train time: 0:00:00.557258
elapsed time: 0:01:03.449196
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:08:04.893485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 757.20
 ---- batch: 020 ----
mean loss: 727.35
 ---- batch: 030 ----
mean loss: 711.02
 ---- batch: 040 ----
mean loss: 670.23
 ---- batch: 050 ----
mean loss: 593.61
 ---- batch: 060 ----
mean loss: 495.57
 ---- batch: 070 ----
mean loss: 429.22
 ---- batch: 080 ----
mean loss: 406.83
 ---- batch: 090 ----
mean loss: 403.93
 ---- batch: 100 ----
mean loss: 376.32
 ---- batch: 110 ----
mean loss: 363.54
train mean loss: 534.77
epoch train time: 0:00:00.549850
elapsed time: 0:01:03.999256
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:08:05.443547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.99
 ---- batch: 020 ----
mean loss: 343.85
 ---- batch: 030 ----
mean loss: 344.26
 ---- batch: 040 ----
mean loss: 326.99
 ---- batch: 050 ----
mean loss: 324.59
 ---- batch: 060 ----
mean loss: 315.82
 ---- batch: 070 ----
mean loss: 307.10
 ---- batch: 080 ----
mean loss: 292.08
 ---- batch: 090 ----
mean loss: 297.85
 ---- batch: 100 ----
mean loss: 290.62
 ---- batch: 110 ----
mean loss: 292.27
train mean loss: 316.37
epoch train time: 0:00:00.559279
elapsed time: 0:01:04.558721
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:08:06.002983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.65
 ---- batch: 020 ----
mean loss: 272.48
 ---- batch: 030 ----
mean loss: 270.36
 ---- batch: 040 ----
mean loss: 265.83
 ---- batch: 050 ----
mean loss: 270.98
 ---- batch: 060 ----
mean loss: 268.68
 ---- batch: 070 ----
mean loss: 268.52
 ---- batch: 080 ----
mean loss: 261.33
 ---- batch: 090 ----
mean loss: 263.67
 ---- batch: 100 ----
mean loss: 257.01
 ---- batch: 110 ----
mean loss: 266.10
train mean loss: 268.41
epoch train time: 0:00:00.551522
elapsed time: 0:01:05.110404
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:08:06.554685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.25
 ---- batch: 020 ----
mean loss: 254.44
 ---- batch: 030 ----
mean loss: 255.37
 ---- batch: 040 ----
mean loss: 249.08
 ---- batch: 050 ----
mean loss: 240.73
 ---- batch: 060 ----
mean loss: 238.29
 ---- batch: 070 ----
mean loss: 242.46
 ---- batch: 080 ----
mean loss: 253.92
 ---- batch: 090 ----
mean loss: 241.86
 ---- batch: 100 ----
mean loss: 253.28
 ---- batch: 110 ----
mean loss: 238.33
train mean loss: 246.44
epoch train time: 0:00:00.560583
elapsed time: 0:01:05.671142
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:08:07.115411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.50
 ---- batch: 020 ----
mean loss: 233.63
 ---- batch: 030 ----
mean loss: 237.82
 ---- batch: 040 ----
mean loss: 238.57
 ---- batch: 050 ----
mean loss: 228.48
 ---- batch: 060 ----
mean loss: 228.40
 ---- batch: 070 ----
mean loss: 223.85
 ---- batch: 080 ----
mean loss: 235.97
 ---- batch: 090 ----
mean loss: 242.84
 ---- batch: 100 ----
mean loss: 237.57
 ---- batch: 110 ----
mean loss: 239.09
train mean loss: 234.38
epoch train time: 0:00:00.553317
elapsed time: 0:01:06.224602
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:08:07.668871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.15
 ---- batch: 020 ----
mean loss: 220.06
 ---- batch: 030 ----
mean loss: 233.05
 ---- batch: 040 ----
mean loss: 229.46
 ---- batch: 050 ----
mean loss: 230.42
 ---- batch: 060 ----
mean loss: 229.48
 ---- batch: 070 ----
mean loss: 223.65
 ---- batch: 080 ----
mean loss: 231.80
 ---- batch: 090 ----
mean loss: 233.15
 ---- batch: 100 ----
mean loss: 222.38
 ---- batch: 110 ----
mean loss: 219.46
train mean loss: 226.83
epoch train time: 0:00:00.553520
elapsed time: 0:01:06.778261
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:08:08.222527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.85
 ---- batch: 020 ----
mean loss: 230.22
 ---- batch: 030 ----
mean loss: 220.87
 ---- batch: 040 ----
mean loss: 207.50
 ---- batch: 050 ----
mean loss: 221.38
 ---- batch: 060 ----
mean loss: 215.81
 ---- batch: 070 ----
mean loss: 224.97
 ---- batch: 080 ----
mean loss: 212.18
 ---- batch: 090 ----
mean loss: 222.93
 ---- batch: 100 ----
mean loss: 211.67
 ---- batch: 110 ----
mean loss: 224.37
train mean loss: 218.40
epoch train time: 0:00:00.546807
elapsed time: 0:01:07.325200
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:08:08.769466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.56
 ---- batch: 020 ----
mean loss: 202.71
 ---- batch: 030 ----
mean loss: 209.60
 ---- batch: 040 ----
mean loss: 216.58
 ---- batch: 050 ----
mean loss: 201.76
 ---- batch: 060 ----
mean loss: 213.47
 ---- batch: 070 ----
mean loss: 200.31
 ---- batch: 080 ----
mean loss: 221.09
 ---- batch: 090 ----
mean loss: 208.15
 ---- batch: 100 ----
mean loss: 214.96
 ---- batch: 110 ----
mean loss: 211.33
train mean loss: 211.22
epoch train time: 0:00:00.557838
elapsed time: 0:01:07.883170
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:08:09.327463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.86
 ---- batch: 020 ----
mean loss: 204.11
 ---- batch: 030 ----
mean loss: 203.83
 ---- batch: 040 ----
mean loss: 209.67
 ---- batch: 050 ----
mean loss: 202.61
 ---- batch: 060 ----
mean loss: 203.21
 ---- batch: 070 ----
mean loss: 210.27
 ---- batch: 080 ----
mean loss: 202.58
 ---- batch: 090 ----
mean loss: 199.81
 ---- batch: 100 ----
mean loss: 202.45
 ---- batch: 110 ----
mean loss: 213.66
train mean loss: 205.21
epoch train time: 0:00:00.557883
elapsed time: 0:01:08.441235
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:08:09.885498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.94
 ---- batch: 020 ----
mean loss: 202.57
 ---- batch: 030 ----
mean loss: 194.57
 ---- batch: 040 ----
mean loss: 185.46
 ---- batch: 050 ----
mean loss: 199.51
 ---- batch: 060 ----
mean loss: 204.06
 ---- batch: 070 ----
mean loss: 203.36
 ---- batch: 080 ----
mean loss: 208.03
 ---- batch: 090 ----
mean loss: 210.42
 ---- batch: 100 ----
mean loss: 200.41
 ---- batch: 110 ----
mean loss: 195.52
train mean loss: 200.40
epoch train time: 0:00:00.565266
elapsed time: 0:01:09.006639
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:08:10.450911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.96
 ---- batch: 020 ----
mean loss: 189.78
 ---- batch: 030 ----
mean loss: 202.59
 ---- batch: 040 ----
mean loss: 200.78
 ---- batch: 050 ----
mean loss: 203.92
 ---- batch: 060 ----
mean loss: 198.24
 ---- batch: 070 ----
mean loss: 205.85
 ---- batch: 080 ----
mean loss: 194.45
 ---- batch: 090 ----
mean loss: 190.12
 ---- batch: 100 ----
mean loss: 193.32
 ---- batch: 110 ----
mean loss: 200.92
train mean loss: 198.44
epoch train time: 0:00:00.568501
elapsed time: 0:01:09.575284
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:08:11.019549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.90
 ---- batch: 020 ----
mean loss: 190.55
 ---- batch: 030 ----
mean loss: 191.05
 ---- batch: 040 ----
mean loss: 190.54
 ---- batch: 050 ----
mean loss: 183.58
 ---- batch: 060 ----
mean loss: 196.20
 ---- batch: 070 ----
mean loss: 192.09
 ---- batch: 080 ----
mean loss: 199.92
 ---- batch: 090 ----
mean loss: 191.99
 ---- batch: 100 ----
mean loss: 194.65
 ---- batch: 110 ----
mean loss: 196.47
train mean loss: 192.37
epoch train time: 0:00:00.561576
elapsed time: 0:01:10.136998
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:08:11.581267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.86
 ---- batch: 020 ----
mean loss: 184.21
 ---- batch: 030 ----
mean loss: 178.33
 ---- batch: 040 ----
mean loss: 192.04
 ---- batch: 050 ----
mean loss: 189.24
 ---- batch: 060 ----
mean loss: 186.85
 ---- batch: 070 ----
mean loss: 195.88
 ---- batch: 080 ----
mean loss: 189.99
 ---- batch: 090 ----
mean loss: 197.21
 ---- batch: 100 ----
mean loss: 193.00
 ---- batch: 110 ----
mean loss: 196.97
train mean loss: 190.61
epoch train time: 0:00:00.560927
elapsed time: 0:01:10.698067
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:08:12.142351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.34
 ---- batch: 020 ----
mean loss: 189.69
 ---- batch: 030 ----
mean loss: 180.01
 ---- batch: 040 ----
mean loss: 188.16
 ---- batch: 050 ----
mean loss: 191.33
 ---- batch: 060 ----
mean loss: 195.10
 ---- batch: 070 ----
mean loss: 193.58
 ---- batch: 080 ----
mean loss: 188.84
 ---- batch: 090 ----
mean loss: 195.50
 ---- batch: 100 ----
mean loss: 177.06
 ---- batch: 110 ----
mean loss: 192.85
train mean loss: 188.92
epoch train time: 0:00:00.566010
elapsed time: 0:01:11.264246
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:08:12.708547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.77
 ---- batch: 020 ----
mean loss: 188.38
 ---- batch: 030 ----
mean loss: 190.61
 ---- batch: 040 ----
mean loss: 179.66
 ---- batch: 050 ----
mean loss: 188.92
 ---- batch: 060 ----
mean loss: 190.35
 ---- batch: 070 ----
mean loss: 183.57
 ---- batch: 080 ----
mean loss: 186.24
 ---- batch: 090 ----
mean loss: 183.32
 ---- batch: 100 ----
mean loss: 182.25
 ---- batch: 110 ----
mean loss: 192.67
train mean loss: 186.97
epoch train time: 0:00:00.573181
elapsed time: 0:01:11.837623
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:08:13.281895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.23
 ---- batch: 020 ----
mean loss: 183.60
 ---- batch: 030 ----
mean loss: 194.19
 ---- batch: 040 ----
mean loss: 184.68
 ---- batch: 050 ----
mean loss: 183.25
 ---- batch: 060 ----
mean loss: 180.77
 ---- batch: 070 ----
mean loss: 187.13
 ---- batch: 080 ----
mean loss: 185.48
 ---- batch: 090 ----
mean loss: 190.10
 ---- batch: 100 ----
mean loss: 180.83
 ---- batch: 110 ----
mean loss: 186.19
train mean loss: 184.83
epoch train time: 0:00:00.556828
elapsed time: 0:01:12.394595
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:08:13.838861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.55
 ---- batch: 020 ----
mean loss: 172.85
 ---- batch: 030 ----
mean loss: 185.66
 ---- batch: 040 ----
mean loss: 184.36
 ---- batch: 050 ----
mean loss: 172.92
 ---- batch: 060 ----
mean loss: 183.86
 ---- batch: 070 ----
mean loss: 190.19
 ---- batch: 080 ----
mean loss: 194.41
 ---- batch: 090 ----
mean loss: 182.04
 ---- batch: 100 ----
mean loss: 183.64
 ---- batch: 110 ----
mean loss: 185.89
train mean loss: 183.34
epoch train time: 0:00:00.549354
elapsed time: 0:01:12.944111
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:08:14.388381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.07
 ---- batch: 020 ----
mean loss: 180.88
 ---- batch: 030 ----
mean loss: 185.27
 ---- batch: 040 ----
mean loss: 181.38
 ---- batch: 050 ----
mean loss: 185.82
 ---- batch: 060 ----
mean loss: 180.06
 ---- batch: 070 ----
mean loss: 183.08
 ---- batch: 080 ----
mean loss: 178.01
 ---- batch: 090 ----
mean loss: 177.78
 ---- batch: 100 ----
mean loss: 186.27
 ---- batch: 110 ----
mean loss: 178.71
train mean loss: 181.52
epoch train time: 0:00:00.571767
elapsed time: 0:01:13.516022
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:08:14.960289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.08
 ---- batch: 020 ----
mean loss: 183.79
 ---- batch: 030 ----
mean loss: 179.09
 ---- batch: 040 ----
mean loss: 175.72
 ---- batch: 050 ----
mean loss: 177.32
 ---- batch: 060 ----
mean loss: 186.37
 ---- batch: 070 ----
mean loss: 177.00
 ---- batch: 080 ----
mean loss: 186.64
 ---- batch: 090 ----
mean loss: 185.10
 ---- batch: 100 ----
mean loss: 185.47
 ---- batch: 110 ----
mean loss: 177.97
train mean loss: 181.14
epoch train time: 0:00:00.563109
elapsed time: 0:01:14.079338
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:08:15.523605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.47
 ---- batch: 020 ----
mean loss: 177.98
 ---- batch: 030 ----
mean loss: 175.65
 ---- batch: 040 ----
mean loss: 174.87
 ---- batch: 050 ----
mean loss: 180.35
 ---- batch: 060 ----
mean loss: 180.73
 ---- batch: 070 ----
mean loss: 183.07
 ---- batch: 080 ----
mean loss: 185.95
 ---- batch: 090 ----
mean loss: 176.72
 ---- batch: 100 ----
mean loss: 178.59
 ---- batch: 110 ----
mean loss: 180.04
train mean loss: 179.99
epoch train time: 0:00:00.569710
elapsed time: 0:01:14.649183
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:08:16.093449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.71
 ---- batch: 020 ----
mean loss: 174.02
 ---- batch: 030 ----
mean loss: 185.27
 ---- batch: 040 ----
mean loss: 182.51
 ---- batch: 050 ----
mean loss: 183.93
 ---- batch: 060 ----
mean loss: 172.12
 ---- batch: 070 ----
mean loss: 178.02
 ---- batch: 080 ----
mean loss: 179.55
 ---- batch: 090 ----
mean loss: 178.60
 ---- batch: 100 ----
mean loss: 173.19
 ---- batch: 110 ----
mean loss: 186.04
train mean loss: 178.83
epoch train time: 0:00:00.550069
elapsed time: 0:01:15.199391
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:08:16.643674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.25
 ---- batch: 020 ----
mean loss: 167.75
 ---- batch: 030 ----
mean loss: 181.13
 ---- batch: 040 ----
mean loss: 180.71
 ---- batch: 050 ----
mean loss: 182.69
 ---- batch: 060 ----
mean loss: 182.91
 ---- batch: 070 ----
mean loss: 183.74
 ---- batch: 080 ----
mean loss: 176.62
 ---- batch: 090 ----
mean loss: 179.03
 ---- batch: 100 ----
mean loss: 168.96
 ---- batch: 110 ----
mean loss: 183.25
train mean loss: 178.66
epoch train time: 0:00:00.553805
elapsed time: 0:01:15.753354
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:08:17.197662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.87
 ---- batch: 020 ----
mean loss: 181.70
 ---- batch: 030 ----
mean loss: 172.71
 ---- batch: 040 ----
mean loss: 175.03
 ---- batch: 050 ----
mean loss: 179.36
 ---- batch: 060 ----
mean loss: 177.64
 ---- batch: 070 ----
mean loss: 175.06
 ---- batch: 080 ----
mean loss: 177.04
 ---- batch: 090 ----
mean loss: 182.12
 ---- batch: 100 ----
mean loss: 182.74
 ---- batch: 110 ----
mean loss: 169.84
train mean loss: 177.70
epoch train time: 0:00:00.558805
elapsed time: 0:01:16.312356
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:08:17.756625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.20
 ---- batch: 020 ----
mean loss: 171.35
 ---- batch: 030 ----
mean loss: 167.13
 ---- batch: 040 ----
mean loss: 178.45
 ---- batch: 050 ----
mean loss: 177.39
 ---- batch: 060 ----
mean loss: 185.60
 ---- batch: 070 ----
mean loss: 174.91
 ---- batch: 080 ----
mean loss: 179.80
 ---- batch: 090 ----
mean loss: 176.99
 ---- batch: 100 ----
mean loss: 176.82
 ---- batch: 110 ----
mean loss: 177.22
train mean loss: 175.64
epoch train time: 0:00:00.549929
elapsed time: 0:01:16.862444
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:08:18.306723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.80
 ---- batch: 020 ----
mean loss: 176.22
 ---- batch: 030 ----
mean loss: 168.29
 ---- batch: 040 ----
mean loss: 179.78
 ---- batch: 050 ----
mean loss: 177.01
 ---- batch: 060 ----
mean loss: 180.19
 ---- batch: 070 ----
mean loss: 176.49
 ---- batch: 080 ----
mean loss: 172.99
 ---- batch: 090 ----
mean loss: 178.58
 ---- batch: 100 ----
mean loss: 168.75
 ---- batch: 110 ----
mean loss: 190.58
train mean loss: 175.66
epoch train time: 0:00:00.550464
elapsed time: 0:01:17.413063
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:08:18.857336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.03
 ---- batch: 020 ----
mean loss: 176.49
 ---- batch: 030 ----
mean loss: 169.51
 ---- batch: 040 ----
mean loss: 170.93
 ---- batch: 050 ----
mean loss: 173.71
 ---- batch: 060 ----
mean loss: 176.69
 ---- batch: 070 ----
mean loss: 182.48
 ---- batch: 080 ----
mean loss: 170.39
 ---- batch: 090 ----
mean loss: 178.28
 ---- batch: 100 ----
mean loss: 178.75
 ---- batch: 110 ----
mean loss: 173.23
train mean loss: 174.80
epoch train time: 0:00:00.553920
elapsed time: 0:01:17.967136
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:08:19.411402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.40
 ---- batch: 020 ----
mean loss: 171.30
 ---- batch: 030 ----
mean loss: 171.21
 ---- batch: 040 ----
mean loss: 173.71
 ---- batch: 050 ----
mean loss: 175.02
 ---- batch: 060 ----
mean loss: 169.49
 ---- batch: 070 ----
mean loss: 170.47
 ---- batch: 080 ----
mean loss: 187.68
 ---- batch: 090 ----
mean loss: 177.20
 ---- batch: 100 ----
mean loss: 164.14
 ---- batch: 110 ----
mean loss: 179.22
train mean loss: 173.58
epoch train time: 0:00:00.551478
elapsed time: 0:01:18.518749
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:08:19.963023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.66
 ---- batch: 020 ----
mean loss: 178.25
 ---- batch: 030 ----
mean loss: 176.17
 ---- batch: 040 ----
mean loss: 177.62
 ---- batch: 050 ----
mean loss: 163.40
 ---- batch: 060 ----
mean loss: 175.32
 ---- batch: 070 ----
mean loss: 182.76
 ---- batch: 080 ----
mean loss: 174.52
 ---- batch: 090 ----
mean loss: 177.15
 ---- batch: 100 ----
mean loss: 168.38
 ---- batch: 110 ----
mean loss: 173.26
train mean loss: 174.00
epoch train time: 0:00:00.551426
elapsed time: 0:01:19.070328
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:08:20.514605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.49
 ---- batch: 020 ----
mean loss: 171.78
 ---- batch: 030 ----
mean loss: 172.78
 ---- batch: 040 ----
mean loss: 165.91
 ---- batch: 050 ----
mean loss: 172.50
 ---- batch: 060 ----
mean loss: 172.43
 ---- batch: 070 ----
mean loss: 171.38
 ---- batch: 080 ----
mean loss: 173.12
 ---- batch: 090 ----
mean loss: 172.12
 ---- batch: 100 ----
mean loss: 174.94
 ---- batch: 110 ----
mean loss: 175.50
train mean loss: 172.56
epoch train time: 0:00:00.555752
elapsed time: 0:01:19.626265
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:08:21.070531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.54
 ---- batch: 020 ----
mean loss: 173.58
 ---- batch: 030 ----
mean loss: 171.21
 ---- batch: 040 ----
mean loss: 167.76
 ---- batch: 050 ----
mean loss: 165.06
 ---- batch: 060 ----
mean loss: 176.87
 ---- batch: 070 ----
mean loss: 181.90
 ---- batch: 080 ----
mean loss: 177.58
 ---- batch: 090 ----
mean loss: 169.87
 ---- batch: 100 ----
mean loss: 178.19
 ---- batch: 110 ----
mean loss: 171.48
train mean loss: 172.82
epoch train time: 0:00:00.559871
elapsed time: 0:01:20.186290
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:08:21.630572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.26
 ---- batch: 020 ----
mean loss: 178.55
 ---- batch: 030 ----
mean loss: 160.51
 ---- batch: 040 ----
mean loss: 171.63
 ---- batch: 050 ----
mean loss: 173.24
 ---- batch: 060 ----
mean loss: 171.71
 ---- batch: 070 ----
mean loss: 169.85
 ---- batch: 080 ----
mean loss: 178.07
 ---- batch: 090 ----
mean loss: 173.50
 ---- batch: 100 ----
mean loss: 175.90
 ---- batch: 110 ----
mean loss: 178.49
train mean loss: 172.12
epoch train time: 0:00:00.562475
elapsed time: 0:01:20.748912
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:08:22.193179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.15
 ---- batch: 020 ----
mean loss: 181.03
 ---- batch: 030 ----
mean loss: 161.56
 ---- batch: 040 ----
mean loss: 166.42
 ---- batch: 050 ----
mean loss: 174.55
 ---- batch: 060 ----
mean loss: 170.62
 ---- batch: 070 ----
mean loss: 168.88
 ---- batch: 080 ----
mean loss: 170.55
 ---- batch: 090 ----
mean loss: 172.01
 ---- batch: 100 ----
mean loss: 176.34
 ---- batch: 110 ----
mean loss: 174.76
train mean loss: 170.76
epoch train time: 0:00:00.554487
elapsed time: 0:01:21.303534
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:08:22.747808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.82
 ---- batch: 020 ----
mean loss: 170.99
 ---- batch: 030 ----
mean loss: 168.42
 ---- batch: 040 ----
mean loss: 159.97
 ---- batch: 050 ----
mean loss: 169.02
 ---- batch: 060 ----
mean loss: 168.49
 ---- batch: 070 ----
mean loss: 172.59
 ---- batch: 080 ----
mean loss: 177.99
 ---- batch: 090 ----
mean loss: 173.09
 ---- batch: 100 ----
mean loss: 163.63
 ---- batch: 110 ----
mean loss: 177.62
train mean loss: 170.13
epoch train time: 0:00:00.554550
elapsed time: 0:01:21.858232
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:08:23.302516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.37
 ---- batch: 020 ----
mean loss: 164.70
 ---- batch: 030 ----
mean loss: 171.09
 ---- batch: 040 ----
mean loss: 167.76
 ---- batch: 050 ----
mean loss: 171.73
 ---- batch: 060 ----
mean loss: 163.09
 ---- batch: 070 ----
mean loss: 175.64
 ---- batch: 080 ----
mean loss: 169.69
 ---- batch: 090 ----
mean loss: 170.90
 ---- batch: 100 ----
mean loss: 178.08
 ---- batch: 110 ----
mean loss: 168.91
train mean loss: 168.98
epoch train time: 0:00:00.547543
elapsed time: 0:01:22.405955
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:08:23.850242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.30
 ---- batch: 020 ----
mean loss: 165.79
 ---- batch: 030 ----
mean loss: 167.99
 ---- batch: 040 ----
mean loss: 175.89
 ---- batch: 050 ----
mean loss: 163.73
 ---- batch: 060 ----
mean loss: 167.78
 ---- batch: 070 ----
mean loss: 169.52
 ---- batch: 080 ----
mean loss: 174.61
 ---- batch: 090 ----
mean loss: 167.30
 ---- batch: 100 ----
mean loss: 169.88
 ---- batch: 110 ----
mean loss: 171.96
train mean loss: 168.98
epoch train time: 0:00:00.546466
elapsed time: 0:01:22.952585
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:08:24.396849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.26
 ---- batch: 020 ----
mean loss: 162.31
 ---- batch: 030 ----
mean loss: 158.69
 ---- batch: 040 ----
mean loss: 165.04
 ---- batch: 050 ----
mean loss: 162.09
 ---- batch: 060 ----
mean loss: 173.85
 ---- batch: 070 ----
mean loss: 179.83
 ---- batch: 080 ----
mean loss: 169.52
 ---- batch: 090 ----
mean loss: 164.98
 ---- batch: 100 ----
mean loss: 173.03
 ---- batch: 110 ----
mean loss: 173.58
train mean loss: 168.42
epoch train time: 0:00:00.554471
elapsed time: 0:01:23.507186
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:08:24.951451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.33
 ---- batch: 020 ----
mean loss: 167.55
 ---- batch: 030 ----
mean loss: 167.67
 ---- batch: 040 ----
mean loss: 162.05
 ---- batch: 050 ----
mean loss: 168.35
 ---- batch: 060 ----
mean loss: 171.53
 ---- batch: 070 ----
mean loss: 168.35
 ---- batch: 080 ----
mean loss: 165.39
 ---- batch: 090 ----
mean loss: 170.02
 ---- batch: 100 ----
mean loss: 167.90
 ---- batch: 110 ----
mean loss: 171.71
train mean loss: 167.69
epoch train time: 0:00:00.550067
elapsed time: 0:01:24.057382
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:08:25.501667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.36
 ---- batch: 020 ----
mean loss: 169.83
 ---- batch: 030 ----
mean loss: 157.57
 ---- batch: 040 ----
mean loss: 173.14
 ---- batch: 050 ----
mean loss: 157.06
 ---- batch: 060 ----
mean loss: 170.11
 ---- batch: 070 ----
mean loss: 172.31
 ---- batch: 080 ----
mean loss: 177.72
 ---- batch: 090 ----
mean loss: 165.94
 ---- batch: 100 ----
mean loss: 168.13
 ---- batch: 110 ----
mean loss: 169.99
train mean loss: 167.93
epoch train time: 0:00:00.548840
elapsed time: 0:01:24.606373
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:08:26.050638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.56
 ---- batch: 020 ----
mean loss: 168.51
 ---- batch: 030 ----
mean loss: 163.78
 ---- batch: 040 ----
mean loss: 168.74
 ---- batch: 050 ----
mean loss: 167.56
 ---- batch: 060 ----
mean loss: 160.05
 ---- batch: 070 ----
mean loss: 167.24
 ---- batch: 080 ----
mean loss: 160.60
 ---- batch: 090 ----
mean loss: 171.96
 ---- batch: 100 ----
mean loss: 169.84
 ---- batch: 110 ----
mean loss: 169.89
train mean loss: 166.58
epoch train time: 0:00:00.547717
elapsed time: 0:01:25.154241
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:08:26.598564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.12
 ---- batch: 020 ----
mean loss: 172.81
 ---- batch: 030 ----
mean loss: 168.14
 ---- batch: 040 ----
mean loss: 161.76
 ---- batch: 050 ----
mean loss: 159.80
 ---- batch: 060 ----
mean loss: 169.08
 ---- batch: 070 ----
mean loss: 172.21
 ---- batch: 080 ----
mean loss: 173.42
 ---- batch: 090 ----
mean loss: 166.03
 ---- batch: 100 ----
mean loss: 165.64
 ---- batch: 110 ----
mean loss: 164.07
train mean loss: 166.67
epoch train time: 0:00:00.551931
elapsed time: 0:01:25.706362
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:08:27.150626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.42
 ---- batch: 020 ----
mean loss: 165.50
 ---- batch: 030 ----
mean loss: 170.37
 ---- batch: 040 ----
mean loss: 164.79
 ---- batch: 050 ----
mean loss: 168.60
 ---- batch: 060 ----
mean loss: 166.73
 ---- batch: 070 ----
mean loss: 161.72
 ---- batch: 080 ----
mean loss: 165.86
 ---- batch: 090 ----
mean loss: 166.97
 ---- batch: 100 ----
mean loss: 165.19
 ---- batch: 110 ----
mean loss: 166.92
train mean loss: 166.32
epoch train time: 0:00:00.556604
elapsed time: 0:01:26.263115
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:08:27.707398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.83
 ---- batch: 020 ----
mean loss: 166.36
 ---- batch: 030 ----
mean loss: 163.51
 ---- batch: 040 ----
mean loss: 175.11
 ---- batch: 050 ----
mean loss: 172.46
 ---- batch: 060 ----
mean loss: 161.62
 ---- batch: 070 ----
mean loss: 160.57
 ---- batch: 080 ----
mean loss: 159.88
 ---- batch: 090 ----
mean loss: 168.77
 ---- batch: 100 ----
mean loss: 164.23
 ---- batch: 110 ----
mean loss: 166.84
train mean loss: 166.33
epoch train time: 0:00:00.556589
elapsed time: 0:01:26.819853
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:08:28.264116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.39
 ---- batch: 020 ----
mean loss: 157.00
 ---- batch: 030 ----
mean loss: 152.12
 ---- batch: 040 ----
mean loss: 165.00
 ---- batch: 050 ----
mean loss: 176.19
 ---- batch: 060 ----
mean loss: 169.55
 ---- batch: 070 ----
mean loss: 168.94
 ---- batch: 080 ----
mean loss: 166.65
 ---- batch: 090 ----
mean loss: 161.00
 ---- batch: 100 ----
mean loss: 171.03
 ---- batch: 110 ----
mean loss: 170.36
train mean loss: 165.76
epoch train time: 0:00:00.570023
elapsed time: 0:01:27.390016
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:08:28.834286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.80
 ---- batch: 020 ----
mean loss: 161.81
 ---- batch: 030 ----
mean loss: 160.42
 ---- batch: 040 ----
mean loss: 161.90
 ---- batch: 050 ----
mean loss: 171.77
 ---- batch: 060 ----
mean loss: 162.69
 ---- batch: 070 ----
mean loss: 161.47
 ---- batch: 080 ----
mean loss: 169.19
 ---- batch: 090 ----
mean loss: 173.09
 ---- batch: 100 ----
mean loss: 168.74
 ---- batch: 110 ----
mean loss: 160.27
train mean loss: 164.98
epoch train time: 0:00:00.555501
elapsed time: 0:01:27.945710
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:08:29.389974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.18
 ---- batch: 020 ----
mean loss: 162.48
 ---- batch: 030 ----
mean loss: 162.74
 ---- batch: 040 ----
mean loss: 162.74
 ---- batch: 050 ----
mean loss: 164.47
 ---- batch: 060 ----
mean loss: 168.84
 ---- batch: 070 ----
mean loss: 168.22
 ---- batch: 080 ----
mean loss: 177.65
 ---- batch: 090 ----
mean loss: 162.33
 ---- batch: 100 ----
mean loss: 160.97
 ---- batch: 110 ----
mean loss: 162.11
train mean loss: 164.44
epoch train time: 0:00:00.556148
elapsed time: 0:01:28.501990
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:08:29.946254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.23
 ---- batch: 020 ----
mean loss: 167.27
 ---- batch: 030 ----
mean loss: 155.69
 ---- batch: 040 ----
mean loss: 168.69
 ---- batch: 050 ----
mean loss: 160.87
 ---- batch: 060 ----
mean loss: 164.19
 ---- batch: 070 ----
mean loss: 154.57
 ---- batch: 080 ----
mean loss: 156.81
 ---- batch: 090 ----
mean loss: 166.15
 ---- batch: 100 ----
mean loss: 175.06
 ---- batch: 110 ----
mean loss: 172.55
train mean loss: 164.46
epoch train time: 0:00:00.550925
elapsed time: 0:01:29.053052
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:08:30.497319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.53
 ---- batch: 020 ----
mean loss: 159.72
 ---- batch: 030 ----
mean loss: 166.65
 ---- batch: 040 ----
mean loss: 158.64
 ---- batch: 050 ----
mean loss: 164.21
 ---- batch: 060 ----
mean loss: 168.29
 ---- batch: 070 ----
mean loss: 164.77
 ---- batch: 080 ----
mean loss: 162.99
 ---- batch: 090 ----
mean loss: 161.07
 ---- batch: 100 ----
mean loss: 169.76
 ---- batch: 110 ----
mean loss: 164.17
train mean loss: 163.18
epoch train time: 0:00:00.552706
elapsed time: 0:01:29.605915
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:08:31.050203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.13
 ---- batch: 020 ----
mean loss: 160.90
 ---- batch: 030 ----
mean loss: 162.53
 ---- batch: 040 ----
mean loss: 157.21
 ---- batch: 050 ----
mean loss: 169.21
 ---- batch: 060 ----
mean loss: 157.94
 ---- batch: 070 ----
mean loss: 165.41
 ---- batch: 080 ----
mean loss: 166.13
 ---- batch: 090 ----
mean loss: 161.82
 ---- batch: 100 ----
mean loss: 154.96
 ---- batch: 110 ----
mean loss: 167.30
train mean loss: 162.49
epoch train time: 0:00:00.561855
elapsed time: 0:01:30.167954
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:08:31.612219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.96
 ---- batch: 020 ----
mean loss: 160.42
 ---- batch: 030 ----
mean loss: 161.84
 ---- batch: 040 ----
mean loss: 168.22
 ---- batch: 050 ----
mean loss: 160.24
 ---- batch: 060 ----
mean loss: 166.08
 ---- batch: 070 ----
mean loss: 158.84
 ---- batch: 080 ----
mean loss: 169.05
 ---- batch: 090 ----
mean loss: 163.10
 ---- batch: 100 ----
mean loss: 164.10
 ---- batch: 110 ----
mean loss: 160.63
train mean loss: 163.13
epoch train time: 0:00:00.557035
elapsed time: 0:01:30.725123
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:08:32.169388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.38
 ---- batch: 020 ----
mean loss: 155.27
 ---- batch: 030 ----
mean loss: 160.42
 ---- batch: 040 ----
mean loss: 161.64
 ---- batch: 050 ----
mean loss: 154.58
 ---- batch: 060 ----
mean loss: 164.86
 ---- batch: 070 ----
mean loss: 159.72
 ---- batch: 080 ----
mean loss: 160.25
 ---- batch: 090 ----
mean loss: 159.10
 ---- batch: 100 ----
mean loss: 169.37
 ---- batch: 110 ----
mean loss: 167.70
train mean loss: 161.96
epoch train time: 0:00:00.556104
elapsed time: 0:01:31.281377
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:08:32.725722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.34
 ---- batch: 020 ----
mean loss: 161.22
 ---- batch: 030 ----
mean loss: 164.59
 ---- batch: 040 ----
mean loss: 156.05
 ---- batch: 050 ----
mean loss: 163.78
 ---- batch: 060 ----
mean loss: 163.53
 ---- batch: 070 ----
mean loss: 156.53
 ---- batch: 080 ----
mean loss: 157.00
 ---- batch: 090 ----
mean loss: 161.29
 ---- batch: 100 ----
mean loss: 169.80
 ---- batch: 110 ----
mean loss: 175.17
train mean loss: 161.72
epoch train time: 0:00:00.549656
elapsed time: 0:01:31.831294
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:08:33.275561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.72
 ---- batch: 020 ----
mean loss: 158.12
 ---- batch: 030 ----
mean loss: 162.00
 ---- batch: 040 ----
mean loss: 153.25
 ---- batch: 050 ----
mean loss: 155.07
 ---- batch: 060 ----
mean loss: 166.07
 ---- batch: 070 ----
mean loss: 163.69
 ---- batch: 080 ----
mean loss: 158.08
 ---- batch: 090 ----
mean loss: 160.02
 ---- batch: 100 ----
mean loss: 167.07
 ---- batch: 110 ----
mean loss: 162.24
train mean loss: 161.45
epoch train time: 0:00:00.545888
elapsed time: 0:01:32.377377
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:08:33.821655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.54
 ---- batch: 020 ----
mean loss: 167.01
 ---- batch: 030 ----
mean loss: 157.56
 ---- batch: 040 ----
mean loss: 160.10
 ---- batch: 050 ----
mean loss: 159.63
 ---- batch: 060 ----
mean loss: 159.54
 ---- batch: 070 ----
mean loss: 160.25
 ---- batch: 080 ----
mean loss: 163.06
 ---- batch: 090 ----
mean loss: 167.48
 ---- batch: 100 ----
mean loss: 157.17
 ---- batch: 110 ----
mean loss: 164.26
train mean loss: 161.23
epoch train time: 0:00:00.564338
elapsed time: 0:01:32.941871
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:08:34.386137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.96
 ---- batch: 020 ----
mean loss: 164.34
 ---- batch: 030 ----
mean loss: 160.31
 ---- batch: 040 ----
mean loss: 158.65
 ---- batch: 050 ----
mean loss: 165.21
 ---- batch: 060 ----
mean loss: 156.15
 ---- batch: 070 ----
mean loss: 170.16
 ---- batch: 080 ----
mean loss: 162.37
 ---- batch: 090 ----
mean loss: 160.79
 ---- batch: 100 ----
mean loss: 155.42
 ---- batch: 110 ----
mean loss: 159.95
train mean loss: 161.09
epoch train time: 0:00:00.547908
elapsed time: 0:01:33.489929
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:08:34.934213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.11
 ---- batch: 020 ----
mean loss: 160.76
 ---- batch: 030 ----
mean loss: 150.27
 ---- batch: 040 ----
mean loss: 157.22
 ---- batch: 050 ----
mean loss: 161.30
 ---- batch: 060 ----
mean loss: 157.13
 ---- batch: 070 ----
mean loss: 165.71
 ---- batch: 080 ----
mean loss: 164.27
 ---- batch: 090 ----
mean loss: 164.28
 ---- batch: 100 ----
mean loss: 167.41
 ---- batch: 110 ----
mean loss: 161.77
train mean loss: 160.54
epoch train time: 0:00:00.549396
elapsed time: 0:01:34.039475
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:08:35.483741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.52
 ---- batch: 020 ----
mean loss: 153.77
 ---- batch: 030 ----
mean loss: 163.11
 ---- batch: 040 ----
mean loss: 156.46
 ---- batch: 050 ----
mean loss: 161.06
 ---- batch: 060 ----
mean loss: 155.96
 ---- batch: 070 ----
mean loss: 168.63
 ---- batch: 080 ----
mean loss: 162.64
 ---- batch: 090 ----
mean loss: 156.11
 ---- batch: 100 ----
mean loss: 158.24
 ---- batch: 110 ----
mean loss: 165.50
train mean loss: 159.63
epoch train time: 0:00:00.559945
elapsed time: 0:01:34.599551
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:08:36.043832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.17
 ---- batch: 020 ----
mean loss: 160.53
 ---- batch: 030 ----
mean loss: 155.23
 ---- batch: 040 ----
mean loss: 151.98
 ---- batch: 050 ----
mean loss: 163.53
 ---- batch: 060 ----
mean loss: 158.61
 ---- batch: 070 ----
mean loss: 160.79
 ---- batch: 080 ----
mean loss: 162.33
 ---- batch: 090 ----
mean loss: 155.44
 ---- batch: 100 ----
mean loss: 155.66
 ---- batch: 110 ----
mean loss: 165.29
train mean loss: 159.26
epoch train time: 0:00:00.566150
elapsed time: 0:01:35.165864
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:08:36.610131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.63
 ---- batch: 020 ----
mean loss: 153.88
 ---- batch: 030 ----
mean loss: 156.70
 ---- batch: 040 ----
mean loss: 160.86
 ---- batch: 050 ----
mean loss: 153.95
 ---- batch: 060 ----
mean loss: 159.94
 ---- batch: 070 ----
mean loss: 161.36
 ---- batch: 080 ----
mean loss: 167.72
 ---- batch: 090 ----
mean loss: 165.57
 ---- batch: 100 ----
mean loss: 155.76
 ---- batch: 110 ----
mean loss: 162.05
train mean loss: 160.17
epoch train time: 0:00:00.549960
elapsed time: 0:01:35.715958
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:08:37.160224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.43
 ---- batch: 020 ----
mean loss: 151.84
 ---- batch: 030 ----
mean loss: 158.40
 ---- batch: 040 ----
mean loss: 150.10
 ---- batch: 050 ----
mean loss: 160.91
 ---- batch: 060 ----
mean loss: 157.38
 ---- batch: 070 ----
mean loss: 154.19
 ---- batch: 080 ----
mean loss: 165.27
 ---- batch: 090 ----
mean loss: 160.65
 ---- batch: 100 ----
mean loss: 160.45
 ---- batch: 110 ----
mean loss: 167.25
train mean loss: 158.76
epoch train time: 0:00:00.571242
elapsed time: 0:01:36.287335
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:08:37.731603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.65
 ---- batch: 020 ----
mean loss: 162.83
 ---- batch: 030 ----
mean loss: 155.70
 ---- batch: 040 ----
mean loss: 159.22
 ---- batch: 050 ----
mean loss: 161.53
 ---- batch: 060 ----
mean loss: 168.89
 ---- batch: 070 ----
mean loss: 152.45
 ---- batch: 080 ----
mean loss: 152.00
 ---- batch: 090 ----
mean loss: 158.00
 ---- batch: 100 ----
mean loss: 162.04
 ---- batch: 110 ----
mean loss: 158.34
train mean loss: 158.73
epoch train time: 0:00:00.559893
elapsed time: 0:01:36.847400
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:08:38.291702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.24
 ---- batch: 020 ----
mean loss: 162.28
 ---- batch: 030 ----
mean loss: 153.63
 ---- batch: 040 ----
mean loss: 151.37
 ---- batch: 050 ----
mean loss: 159.17
 ---- batch: 060 ----
mean loss: 158.93
 ---- batch: 070 ----
mean loss: 161.54
 ---- batch: 080 ----
mean loss: 162.18
 ---- batch: 090 ----
mean loss: 157.38
 ---- batch: 100 ----
mean loss: 150.42
 ---- batch: 110 ----
mean loss: 151.92
train mean loss: 157.32
epoch train time: 0:00:00.560956
elapsed time: 0:01:37.408549
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:08:38.852818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.12
 ---- batch: 020 ----
mean loss: 158.91
 ---- batch: 030 ----
mean loss: 156.85
 ---- batch: 040 ----
mean loss: 155.97
 ---- batch: 050 ----
mean loss: 158.74
 ---- batch: 060 ----
mean loss: 161.75
 ---- batch: 070 ----
mean loss: 167.30
 ---- batch: 080 ----
mean loss: 163.75
 ---- batch: 090 ----
mean loss: 157.15
 ---- batch: 100 ----
mean loss: 162.53
 ---- batch: 110 ----
mean loss: 152.75
train mean loss: 158.73
epoch train time: 0:00:00.570568
elapsed time: 0:01:37.979259
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:08:39.423531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.56
 ---- batch: 020 ----
mean loss: 160.18
 ---- batch: 030 ----
mean loss: 155.93
 ---- batch: 040 ----
mean loss: 152.70
 ---- batch: 050 ----
mean loss: 151.12
 ---- batch: 060 ----
mean loss: 153.16
 ---- batch: 070 ----
mean loss: 163.25
 ---- batch: 080 ----
mean loss: 161.70
 ---- batch: 090 ----
mean loss: 160.55
 ---- batch: 100 ----
mean loss: 158.70
 ---- batch: 110 ----
mean loss: 154.14
train mean loss: 157.84
epoch train time: 0:00:00.631402
elapsed time: 0:01:38.610823
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:08:40.055103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.69
 ---- batch: 020 ----
mean loss: 156.81
 ---- batch: 030 ----
mean loss: 154.31
 ---- batch: 040 ----
mean loss: 156.98
 ---- batch: 050 ----
mean loss: 159.86
 ---- batch: 060 ----
mean loss: 155.46
 ---- batch: 070 ----
mean loss: 155.40
 ---- batch: 080 ----
mean loss: 160.47
 ---- batch: 090 ----
mean loss: 160.72
 ---- batch: 100 ----
mean loss: 158.39
 ---- batch: 110 ----
mean loss: 159.80
train mean loss: 157.38
epoch train time: 0:00:00.597746
elapsed time: 0:01:39.208739
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:08:40.653007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.22
 ---- batch: 020 ----
mean loss: 152.57
 ---- batch: 030 ----
mean loss: 155.62
 ---- batch: 040 ----
mean loss: 157.74
 ---- batch: 050 ----
mean loss: 160.56
 ---- batch: 060 ----
mean loss: 157.86
 ---- batch: 070 ----
mean loss: 159.00
 ---- batch: 080 ----
mean loss: 155.80
 ---- batch: 090 ----
mean loss: 164.26
 ---- batch: 100 ----
mean loss: 159.98
 ---- batch: 110 ----
mean loss: 161.21
train mean loss: 158.05
epoch train time: 0:00:00.551929
elapsed time: 0:01:39.760802
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:08:41.205098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.65
 ---- batch: 020 ----
mean loss: 144.97
 ---- batch: 030 ----
mean loss: 162.29
 ---- batch: 040 ----
mean loss: 155.94
 ---- batch: 050 ----
mean loss: 154.06
 ---- batch: 060 ----
mean loss: 151.86
 ---- batch: 070 ----
mean loss: 162.80
 ---- batch: 080 ----
mean loss: 151.61
 ---- batch: 090 ----
mean loss: 162.18
 ---- batch: 100 ----
mean loss: 154.67
 ---- batch: 110 ----
mean loss: 158.35
train mean loss: 156.03
epoch train time: 0:00:00.549539
elapsed time: 0:01:40.310526
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:08:41.754810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.49
 ---- batch: 020 ----
mean loss: 148.78
 ---- batch: 030 ----
mean loss: 154.24
 ---- batch: 040 ----
mean loss: 151.19
 ---- batch: 050 ----
mean loss: 149.40
 ---- batch: 060 ----
mean loss: 152.28
 ---- batch: 070 ----
mean loss: 158.98
 ---- batch: 080 ----
mean loss: 161.29
 ---- batch: 090 ----
mean loss: 156.90
 ---- batch: 100 ----
mean loss: 161.98
 ---- batch: 110 ----
mean loss: 159.59
train mean loss: 155.79
epoch train time: 0:00:00.547881
elapsed time: 0:01:40.858577
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:08:42.302866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.45
 ---- batch: 020 ----
mean loss: 160.66
 ---- batch: 030 ----
mean loss: 152.13
 ---- batch: 040 ----
mean loss: 154.23
 ---- batch: 050 ----
mean loss: 159.33
 ---- batch: 060 ----
mean loss: 158.81
 ---- batch: 070 ----
mean loss: 162.85
 ---- batch: 080 ----
mean loss: 158.31
 ---- batch: 090 ----
mean loss: 154.39
 ---- batch: 100 ----
mean loss: 159.01
 ---- batch: 110 ----
mean loss: 152.64
train mean loss: 156.89
epoch train time: 0:00:00.558529
elapsed time: 0:01:41.417264
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:08:42.861530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.47
 ---- batch: 020 ----
mean loss: 153.00
 ---- batch: 030 ----
mean loss: 152.02
 ---- batch: 040 ----
mean loss: 153.95
 ---- batch: 050 ----
mean loss: 153.56
 ---- batch: 060 ----
mean loss: 153.44
 ---- batch: 070 ----
mean loss: 156.57
 ---- batch: 080 ----
mean loss: 161.06
 ---- batch: 090 ----
mean loss: 165.71
 ---- batch: 100 ----
mean loss: 160.89
 ---- batch: 110 ----
mean loss: 148.90
train mean loss: 155.75
epoch train time: 0:00:00.554203
elapsed time: 0:01:41.971601
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:08:43.415881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.01
 ---- batch: 020 ----
mean loss: 145.85
 ---- batch: 030 ----
mean loss: 154.66
 ---- batch: 040 ----
mean loss: 154.14
 ---- batch: 050 ----
mean loss: 160.70
 ---- batch: 060 ----
mean loss: 160.17
 ---- batch: 070 ----
mean loss: 163.18
 ---- batch: 080 ----
mean loss: 150.78
 ---- batch: 090 ----
mean loss: 157.04
 ---- batch: 100 ----
mean loss: 153.63
 ---- batch: 110 ----
mean loss: 151.53
train mean loss: 155.01
epoch train time: 0:00:00.559457
elapsed time: 0:01:42.531224
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:08:43.975498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.72
 ---- batch: 020 ----
mean loss: 145.61
 ---- batch: 030 ----
mean loss: 156.32
 ---- batch: 040 ----
mean loss: 158.71
 ---- batch: 050 ----
mean loss: 149.29
 ---- batch: 060 ----
mean loss: 156.22
 ---- batch: 070 ----
mean loss: 161.22
 ---- batch: 080 ----
mean loss: 158.72
 ---- batch: 090 ----
mean loss: 155.84
 ---- batch: 100 ----
mean loss: 154.57
 ---- batch: 110 ----
mean loss: 159.08
train mean loss: 155.81
epoch train time: 0:00:00.555830
elapsed time: 0:01:43.087194
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:08:44.531463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.86
 ---- batch: 020 ----
mean loss: 142.88
 ---- batch: 030 ----
mean loss: 155.43
 ---- batch: 040 ----
mean loss: 158.58
 ---- batch: 050 ----
mean loss: 154.67
 ---- batch: 060 ----
mean loss: 152.38
 ---- batch: 070 ----
mean loss: 152.32
 ---- batch: 080 ----
mean loss: 160.94
 ---- batch: 090 ----
mean loss: 159.42
 ---- batch: 100 ----
mean loss: 156.41
 ---- batch: 110 ----
mean loss: 155.75
train mean loss: 154.77
epoch train time: 0:00:00.558378
elapsed time: 0:01:43.645731
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:08:45.089989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.25
 ---- batch: 020 ----
mean loss: 150.77
 ---- batch: 030 ----
mean loss: 153.27
 ---- batch: 040 ----
mean loss: 136.96
 ---- batch: 050 ----
mean loss: 166.03
 ---- batch: 060 ----
mean loss: 158.52
 ---- batch: 070 ----
mean loss: 153.85
 ---- batch: 080 ----
mean loss: 156.50
 ---- batch: 090 ----
mean loss: 158.76
 ---- batch: 100 ----
mean loss: 154.16
 ---- batch: 110 ----
mean loss: 157.59
train mean loss: 154.78
epoch train time: 0:00:00.550546
elapsed time: 0:01:44.196410
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:08:45.640675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.55
 ---- batch: 020 ----
mean loss: 141.47
 ---- batch: 030 ----
mean loss: 153.93
 ---- batch: 040 ----
mean loss: 154.40
 ---- batch: 050 ----
mean loss: 152.23
 ---- batch: 060 ----
mean loss: 153.18
 ---- batch: 070 ----
mean loss: 157.71
 ---- batch: 080 ----
mean loss: 160.33
 ---- batch: 090 ----
mean loss: 154.30
 ---- batch: 100 ----
mean loss: 167.02
 ---- batch: 110 ----
mean loss: 145.76
train mean loss: 154.58
epoch train time: 0:00:00.548843
elapsed time: 0:01:44.745421
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:08:46.189697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.45
 ---- batch: 020 ----
mean loss: 159.72
 ---- batch: 030 ----
mean loss: 151.19
 ---- batch: 040 ----
mean loss: 151.20
 ---- batch: 050 ----
mean loss: 154.63
 ---- batch: 060 ----
mean loss: 154.64
 ---- batch: 070 ----
mean loss: 147.16
 ---- batch: 080 ----
mean loss: 154.43
 ---- batch: 090 ----
mean loss: 153.16
 ---- batch: 100 ----
mean loss: 154.03
 ---- batch: 110 ----
mean loss: 158.71
train mean loss: 153.95
epoch train time: 0:00:00.550474
elapsed time: 0:01:45.296043
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:08:46.740370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.57
 ---- batch: 020 ----
mean loss: 149.77
 ---- batch: 030 ----
mean loss: 154.45
 ---- batch: 040 ----
mean loss: 159.20
 ---- batch: 050 ----
mean loss: 147.37
 ---- batch: 060 ----
mean loss: 148.58
 ---- batch: 070 ----
mean loss: 159.38
 ---- batch: 080 ----
mean loss: 156.78
 ---- batch: 090 ----
mean loss: 160.12
 ---- batch: 100 ----
mean loss: 146.57
 ---- batch: 110 ----
mean loss: 157.71
train mean loss: 153.70
epoch train time: 0:00:00.547502
elapsed time: 0:01:45.843737
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:08:47.288000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.19
 ---- batch: 020 ----
mean loss: 152.04
 ---- batch: 030 ----
mean loss: 155.15
 ---- batch: 040 ----
mean loss: 152.47
 ---- batch: 050 ----
mean loss: 153.39
 ---- batch: 060 ----
mean loss: 152.61
 ---- batch: 070 ----
mean loss: 155.30
 ---- batch: 080 ----
mean loss: 150.10
 ---- batch: 090 ----
mean loss: 147.66
 ---- batch: 100 ----
mean loss: 156.45
 ---- batch: 110 ----
mean loss: 157.56
train mean loss: 153.12
epoch train time: 0:00:00.558161
elapsed time: 0:01:46.402039
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:08:47.846313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.38
 ---- batch: 020 ----
mean loss: 153.12
 ---- batch: 030 ----
mean loss: 152.91
 ---- batch: 040 ----
mean loss: 157.47
 ---- batch: 050 ----
mean loss: 150.94
 ---- batch: 060 ----
mean loss: 155.53
 ---- batch: 070 ----
mean loss: 159.79
 ---- batch: 080 ----
mean loss: 145.28
 ---- batch: 090 ----
mean loss: 146.79
 ---- batch: 100 ----
mean loss: 153.40
 ---- batch: 110 ----
mean loss: 151.06
train mean loss: 152.41
epoch train time: 0:00:00.552616
elapsed time: 0:01:46.954800
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:08:48.399163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.56
 ---- batch: 020 ----
mean loss: 149.01
 ---- batch: 030 ----
mean loss: 149.98
 ---- batch: 040 ----
mean loss: 148.57
 ---- batch: 050 ----
mean loss: 151.23
 ---- batch: 060 ----
mean loss: 145.53
 ---- batch: 070 ----
mean loss: 157.88
 ---- batch: 080 ----
mean loss: 152.71
 ---- batch: 090 ----
mean loss: 163.07
 ---- batch: 100 ----
mean loss: 149.94
 ---- batch: 110 ----
mean loss: 158.77
train mean loss: 153.90
epoch train time: 0:00:00.556863
elapsed time: 0:01:47.511896
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:08:48.956162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.37
 ---- batch: 020 ----
mean loss: 153.41
 ---- batch: 030 ----
mean loss: 158.05
 ---- batch: 040 ----
mean loss: 154.69
 ---- batch: 050 ----
mean loss: 158.48
 ---- batch: 060 ----
mean loss: 152.35
 ---- batch: 070 ----
mean loss: 151.79
 ---- batch: 080 ----
mean loss: 148.70
 ---- batch: 090 ----
mean loss: 148.35
 ---- batch: 100 ----
mean loss: 155.41
 ---- batch: 110 ----
mean loss: 152.86
train mean loss: 153.20
epoch train time: 0:00:00.556533
elapsed time: 0:01:48.068562
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:08:49.512850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.32
 ---- batch: 020 ----
mean loss: 154.46
 ---- batch: 030 ----
mean loss: 152.26
 ---- batch: 040 ----
mean loss: 157.69
 ---- batch: 050 ----
mean loss: 150.23
 ---- batch: 060 ----
mean loss: 156.29
 ---- batch: 070 ----
mean loss: 152.22
 ---- batch: 080 ----
mean loss: 152.21
 ---- batch: 090 ----
mean loss: 150.89
 ---- batch: 100 ----
mean loss: 147.86
 ---- batch: 110 ----
mean loss: 159.43
train mean loss: 153.24
epoch train time: 0:00:00.552260
elapsed time: 0:01:48.620987
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:08:50.065255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.59
 ---- batch: 020 ----
mean loss: 153.34
 ---- batch: 030 ----
mean loss: 152.77
 ---- batch: 040 ----
mean loss: 149.25
 ---- batch: 050 ----
mean loss: 148.35
 ---- batch: 060 ----
mean loss: 153.52
 ---- batch: 070 ----
mean loss: 155.48
 ---- batch: 080 ----
mean loss: 149.00
 ---- batch: 090 ----
mean loss: 153.46
 ---- batch: 100 ----
mean loss: 148.30
 ---- batch: 110 ----
mean loss: 149.54
train mean loss: 152.27
epoch train time: 0:00:00.540798
elapsed time: 0:01:49.161922
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:08:50.606190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.48
 ---- batch: 020 ----
mean loss: 151.96
 ---- batch: 030 ----
mean loss: 149.28
 ---- batch: 040 ----
mean loss: 153.46
 ---- batch: 050 ----
mean loss: 154.15
 ---- batch: 060 ----
mean loss: 154.97
 ---- batch: 070 ----
mean loss: 148.93
 ---- batch: 080 ----
mean loss: 153.24
 ---- batch: 090 ----
mean loss: 141.72
 ---- batch: 100 ----
mean loss: 159.69
 ---- batch: 110 ----
mean loss: 157.84
train mean loss: 151.26
epoch train time: 0:00:00.564257
elapsed time: 0:01:49.726337
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:08:51.170622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.64
 ---- batch: 020 ----
mean loss: 156.08
 ---- batch: 030 ----
mean loss: 145.40
 ---- batch: 040 ----
mean loss: 156.97
 ---- batch: 050 ----
mean loss: 144.84
 ---- batch: 060 ----
mean loss: 157.80
 ---- batch: 070 ----
mean loss: 150.27
 ---- batch: 080 ----
mean loss: 152.75
 ---- batch: 090 ----
mean loss: 148.42
 ---- batch: 100 ----
mean loss: 153.70
 ---- batch: 110 ----
mean loss: 149.96
train mean loss: 151.81
epoch train time: 0:00:00.558292
elapsed time: 0:01:50.284806
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:08:51.729090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.13
 ---- batch: 020 ----
mean loss: 151.07
 ---- batch: 030 ----
mean loss: 156.81
 ---- batch: 040 ----
mean loss: 158.38
 ---- batch: 050 ----
mean loss: 158.57
 ---- batch: 060 ----
mean loss: 153.72
 ---- batch: 070 ----
mean loss: 147.76
 ---- batch: 080 ----
mean loss: 145.96
 ---- batch: 090 ----
mean loss: 151.30
 ---- batch: 100 ----
mean loss: 140.43
 ---- batch: 110 ----
mean loss: 160.52
train mean loss: 151.82
epoch train time: 0:00:00.560109
elapsed time: 0:01:50.845069
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:08:52.289335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.61
 ---- batch: 020 ----
mean loss: 148.33
 ---- batch: 030 ----
mean loss: 150.62
 ---- batch: 040 ----
mean loss: 157.20
 ---- batch: 050 ----
mean loss: 155.51
 ---- batch: 060 ----
mean loss: 144.21
 ---- batch: 070 ----
mean loss: 149.40
 ---- batch: 080 ----
mean loss: 155.61
 ---- batch: 090 ----
mean loss: 163.49
 ---- batch: 100 ----
mean loss: 151.77
 ---- batch: 110 ----
mean loss: 147.43
train mean loss: 151.32
epoch train time: 0:00:00.567230
elapsed time: 0:01:51.412432
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:08:52.856697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.31
 ---- batch: 020 ----
mean loss: 149.10
 ---- batch: 030 ----
mean loss: 148.91
 ---- batch: 040 ----
mean loss: 144.96
 ---- batch: 050 ----
mean loss: 151.31
 ---- batch: 060 ----
mean loss: 150.84
 ---- batch: 070 ----
mean loss: 149.39
 ---- batch: 080 ----
mean loss: 154.01
 ---- batch: 090 ----
mean loss: 152.83
 ---- batch: 100 ----
mean loss: 149.48
 ---- batch: 110 ----
mean loss: 148.26
train mean loss: 150.65
epoch train time: 0:00:00.549388
elapsed time: 0:01:51.961958
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:08:53.406256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.99
 ---- batch: 020 ----
mean loss: 154.36
 ---- batch: 030 ----
mean loss: 147.87
 ---- batch: 040 ----
mean loss: 150.24
 ---- batch: 050 ----
mean loss: 143.57
 ---- batch: 060 ----
mean loss: 151.54
 ---- batch: 070 ----
mean loss: 148.23
 ---- batch: 080 ----
mean loss: 147.43
 ---- batch: 090 ----
mean loss: 146.93
 ---- batch: 100 ----
mean loss: 159.29
 ---- batch: 110 ----
mean loss: 159.40
train mean loss: 150.58
epoch train time: 0:00:00.553210
elapsed time: 0:01:52.515335
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:08:53.959601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.98
 ---- batch: 020 ----
mean loss: 139.70
 ---- batch: 030 ----
mean loss: 146.51
 ---- batch: 040 ----
mean loss: 151.64
 ---- batch: 050 ----
mean loss: 153.39
 ---- batch: 060 ----
mean loss: 141.87
 ---- batch: 070 ----
mean loss: 158.84
 ---- batch: 080 ----
mean loss: 156.99
 ---- batch: 090 ----
mean loss: 163.34
 ---- batch: 100 ----
mean loss: 151.61
 ---- batch: 110 ----
mean loss: 147.19
train mean loss: 150.56
epoch train time: 0:00:00.552458
elapsed time: 0:01:53.067925
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:08:54.512190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.42
 ---- batch: 020 ----
mean loss: 143.10
 ---- batch: 030 ----
mean loss: 159.20
 ---- batch: 040 ----
mean loss: 155.04
 ---- batch: 050 ----
mean loss: 144.87
 ---- batch: 060 ----
mean loss: 150.25
 ---- batch: 070 ----
mean loss: 141.34
 ---- batch: 080 ----
mean loss: 150.87
 ---- batch: 090 ----
mean loss: 150.38
 ---- batch: 100 ----
mean loss: 153.37
 ---- batch: 110 ----
mean loss: 153.06
train mean loss: 150.13
epoch train time: 0:00:00.589277
elapsed time: 0:01:53.657345
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:08:55.101644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.62
 ---- batch: 020 ----
mean loss: 150.47
 ---- batch: 030 ----
mean loss: 152.46
 ---- batch: 040 ----
mean loss: 150.70
 ---- batch: 050 ----
mean loss: 155.97
 ---- batch: 060 ----
mean loss: 154.11
 ---- batch: 070 ----
mean loss: 149.32
 ---- batch: 080 ----
mean loss: 153.96
 ---- batch: 090 ----
mean loss: 151.87
 ---- batch: 100 ----
mean loss: 144.58
 ---- batch: 110 ----
mean loss: 147.38
train mean loss: 149.91
epoch train time: 0:00:00.587377
elapsed time: 0:01:54.244913
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:08:55.689196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.48
 ---- batch: 020 ----
mean loss: 147.91
 ---- batch: 030 ----
mean loss: 147.27
 ---- batch: 040 ----
mean loss: 148.20
 ---- batch: 050 ----
mean loss: 145.45
 ---- batch: 060 ----
mean loss: 148.67
 ---- batch: 070 ----
mean loss: 152.43
 ---- batch: 080 ----
mean loss: 146.92
 ---- batch: 090 ----
mean loss: 146.98
 ---- batch: 100 ----
mean loss: 148.22
 ---- batch: 110 ----
mean loss: 150.56
train mean loss: 148.69
epoch train time: 0:00:00.577113
elapsed time: 0:01:54.822181
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:08:56.266445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.70
 ---- batch: 020 ----
mean loss: 149.15
 ---- batch: 030 ----
mean loss: 145.01
 ---- batch: 040 ----
mean loss: 145.56
 ---- batch: 050 ----
mean loss: 153.06
 ---- batch: 060 ----
mean loss: 146.22
 ---- batch: 070 ----
mean loss: 149.53
 ---- batch: 080 ----
mean loss: 150.26
 ---- batch: 090 ----
mean loss: 152.50
 ---- batch: 100 ----
mean loss: 156.38
 ---- batch: 110 ----
mean loss: 146.12
train mean loss: 149.20
epoch train time: 0:00:00.556824
elapsed time: 0:01:55.379155
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:08:56.823456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.57
 ---- batch: 020 ----
mean loss: 153.42
 ---- batch: 030 ----
mean loss: 148.51
 ---- batch: 040 ----
mean loss: 146.08
 ---- batch: 050 ----
mean loss: 146.23
 ---- batch: 060 ----
mean loss: 142.80
 ---- batch: 070 ----
mean loss: 149.32
 ---- batch: 080 ----
mean loss: 158.20
 ---- batch: 090 ----
mean loss: 152.10
 ---- batch: 100 ----
mean loss: 155.80
 ---- batch: 110 ----
mean loss: 148.78
train mean loss: 149.13
epoch train time: 0:00:00.553955
elapsed time: 0:01:55.933301
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:08:57.377559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.83
 ---- batch: 020 ----
mean loss: 148.65
 ---- batch: 030 ----
mean loss: 143.41
 ---- batch: 040 ----
mean loss: 138.32
 ---- batch: 050 ----
mean loss: 141.89
 ---- batch: 060 ----
mean loss: 148.51
 ---- batch: 070 ----
mean loss: 153.38
 ---- batch: 080 ----
mean loss: 153.84
 ---- batch: 090 ----
mean loss: 149.98
 ---- batch: 100 ----
mean loss: 153.64
 ---- batch: 110 ----
mean loss: 150.09
train mean loss: 149.04
epoch train time: 0:00:00.555774
elapsed time: 0:01:56.489215
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:08:57.933496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.96
 ---- batch: 020 ----
mean loss: 145.12
 ---- batch: 030 ----
mean loss: 151.17
 ---- batch: 040 ----
mean loss: 142.82
 ---- batch: 050 ----
mean loss: 150.45
 ---- batch: 060 ----
mean loss: 156.17
 ---- batch: 070 ----
mean loss: 144.03
 ---- batch: 080 ----
mean loss: 153.18
 ---- batch: 090 ----
mean loss: 154.24
 ---- batch: 100 ----
mean loss: 144.21
 ---- batch: 110 ----
mean loss: 144.48
train mean loss: 148.41
epoch train time: 0:00:00.550615
elapsed time: 0:01:57.040008
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:08:58.484287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.67
 ---- batch: 020 ----
mean loss: 141.58
 ---- batch: 030 ----
mean loss: 151.01
 ---- batch: 040 ----
mean loss: 147.53
 ---- batch: 050 ----
mean loss: 142.41
 ---- batch: 060 ----
mean loss: 155.68
 ---- batch: 070 ----
mean loss: 151.62
 ---- batch: 080 ----
mean loss: 148.84
 ---- batch: 090 ----
mean loss: 150.83
 ---- batch: 100 ----
mean loss: 158.75
 ---- batch: 110 ----
mean loss: 145.64
train mean loss: 149.12
epoch train time: 0:00:00.569748
elapsed time: 0:01:57.609911
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:08:59.054178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.75
 ---- batch: 020 ----
mean loss: 146.02
 ---- batch: 030 ----
mean loss: 144.68
 ---- batch: 040 ----
mean loss: 143.27
 ---- batch: 050 ----
mean loss: 148.01
 ---- batch: 060 ----
mean loss: 146.02
 ---- batch: 070 ----
mean loss: 146.84
 ---- batch: 080 ----
mean loss: 151.28
 ---- batch: 090 ----
mean loss: 147.98
 ---- batch: 100 ----
mean loss: 147.66
 ---- batch: 110 ----
mean loss: 147.77
train mean loss: 148.13
epoch train time: 0:00:00.585218
elapsed time: 0:01:58.195302
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:08:59.639579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.27
 ---- batch: 020 ----
mean loss: 135.94
 ---- batch: 030 ----
mean loss: 146.78
 ---- batch: 040 ----
mean loss: 147.59
 ---- batch: 050 ----
mean loss: 155.87
 ---- batch: 060 ----
mean loss: 152.94
 ---- batch: 070 ----
mean loss: 155.11
 ---- batch: 080 ----
mean loss: 148.34
 ---- batch: 090 ----
mean loss: 142.55
 ---- batch: 100 ----
mean loss: 150.95
 ---- batch: 110 ----
mean loss: 148.62
train mean loss: 148.77
epoch train time: 0:00:00.579499
elapsed time: 0:01:58.774949
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:09:00.219218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.02
 ---- batch: 020 ----
mean loss: 137.83
 ---- batch: 030 ----
mean loss: 147.70
 ---- batch: 040 ----
mean loss: 146.98
 ---- batch: 050 ----
mean loss: 147.82
 ---- batch: 060 ----
mean loss: 148.64
 ---- batch: 070 ----
mean loss: 145.44
 ---- batch: 080 ----
mean loss: 146.89
 ---- batch: 090 ----
mean loss: 153.18
 ---- batch: 100 ----
mean loss: 146.66
 ---- batch: 110 ----
mean loss: 154.25
train mean loss: 147.39
epoch train time: 0:00:00.584165
elapsed time: 0:01:59.359255
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:09:00.803571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.59
 ---- batch: 020 ----
mean loss: 143.61
 ---- batch: 030 ----
mean loss: 147.23
 ---- batch: 040 ----
mean loss: 146.37
 ---- batch: 050 ----
mean loss: 142.96
 ---- batch: 060 ----
mean loss: 143.50
 ---- batch: 070 ----
mean loss: 156.01
 ---- batch: 080 ----
mean loss: 146.19
 ---- batch: 090 ----
mean loss: 155.28
 ---- batch: 100 ----
mean loss: 145.44
 ---- batch: 110 ----
mean loss: 154.55
train mean loss: 147.94
epoch train time: 0:00:00.581673
elapsed time: 0:01:59.941117
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:09:01.385401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.21
 ---- batch: 020 ----
mean loss: 139.99
 ---- batch: 030 ----
mean loss: 153.07
 ---- batch: 040 ----
mean loss: 159.43
 ---- batch: 050 ----
mean loss: 150.78
 ---- batch: 060 ----
mean loss: 153.37
 ---- batch: 070 ----
mean loss: 149.13
 ---- batch: 080 ----
mean loss: 143.11
 ---- batch: 090 ----
mean loss: 142.67
 ---- batch: 100 ----
mean loss: 155.16
 ---- batch: 110 ----
mean loss: 147.96
train mean loss: 148.84
epoch train time: 0:00:00.585076
elapsed time: 0:02:00.526352
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:09:01.970620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.94
 ---- batch: 020 ----
mean loss: 142.94
 ---- batch: 030 ----
mean loss: 147.74
 ---- batch: 040 ----
mean loss: 145.73
 ---- batch: 050 ----
mean loss: 141.61
 ---- batch: 060 ----
mean loss: 156.46
 ---- batch: 070 ----
mean loss: 138.37
 ---- batch: 080 ----
mean loss: 147.46
 ---- batch: 090 ----
mean loss: 156.32
 ---- batch: 100 ----
mean loss: 145.88
 ---- batch: 110 ----
mean loss: 154.00
train mean loss: 147.04
epoch train time: 0:00:00.571925
elapsed time: 0:02:01.098420
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:09:02.542687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.76
 ---- batch: 020 ----
mean loss: 144.98
 ---- batch: 030 ----
mean loss: 146.54
 ---- batch: 040 ----
mean loss: 146.16
 ---- batch: 050 ----
mean loss: 148.02
 ---- batch: 060 ----
mean loss: 140.30
 ---- batch: 070 ----
mean loss: 147.86
 ---- batch: 080 ----
mean loss: 144.39
 ---- batch: 090 ----
mean loss: 134.90
 ---- batch: 100 ----
mean loss: 150.68
 ---- batch: 110 ----
mean loss: 155.87
train mean loss: 146.53
epoch train time: 0:00:00.573621
elapsed time: 0:02:01.672197
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:09:03.116468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.18
 ---- batch: 020 ----
mean loss: 149.53
 ---- batch: 030 ----
mean loss: 145.49
 ---- batch: 040 ----
mean loss: 154.25
 ---- batch: 050 ----
mean loss: 148.04
 ---- batch: 060 ----
mean loss: 138.73
 ---- batch: 070 ----
mean loss: 148.33
 ---- batch: 080 ----
mean loss: 145.18
 ---- batch: 090 ----
mean loss: 144.50
 ---- batch: 100 ----
mean loss: 156.49
 ---- batch: 110 ----
mean loss: 146.39
train mean loss: 147.69
epoch train time: 0:00:00.577058
elapsed time: 0:02:02.249440
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:09:03.693710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.96
 ---- batch: 020 ----
mean loss: 144.08
 ---- batch: 030 ----
mean loss: 141.61
 ---- batch: 040 ----
mean loss: 148.49
 ---- batch: 050 ----
mean loss: 155.59
 ---- batch: 060 ----
mean loss: 144.57
 ---- batch: 070 ----
mean loss: 140.09
 ---- batch: 080 ----
mean loss: 155.76
 ---- batch: 090 ----
mean loss: 154.58
 ---- batch: 100 ----
mean loss: 142.14
 ---- batch: 110 ----
mean loss: 146.47
train mean loss: 146.73
epoch train time: 0:00:00.612962
elapsed time: 0:02:02.862548
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:09:04.306863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.39
 ---- batch: 020 ----
mean loss: 145.19
 ---- batch: 030 ----
mean loss: 140.11
 ---- batch: 040 ----
mean loss: 144.23
 ---- batch: 050 ----
mean loss: 151.93
 ---- batch: 060 ----
mean loss: 148.87
 ---- batch: 070 ----
mean loss: 145.27
 ---- batch: 080 ----
mean loss: 145.10
 ---- batch: 090 ----
mean loss: 146.10
 ---- batch: 100 ----
mean loss: 154.39
 ---- batch: 110 ----
mean loss: 145.40
train mean loss: 146.02
epoch train time: 0:00:00.575195
elapsed time: 0:02:03.437931
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:09:04.882200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.59
 ---- batch: 020 ----
mean loss: 149.15
 ---- batch: 030 ----
mean loss: 143.50
 ---- batch: 040 ----
mean loss: 142.19
 ---- batch: 050 ----
mean loss: 152.76
 ---- batch: 060 ----
mean loss: 147.50
 ---- batch: 070 ----
mean loss: 144.34
 ---- batch: 080 ----
mean loss: 131.33
 ---- batch: 090 ----
mean loss: 137.47
 ---- batch: 100 ----
mean loss: 151.08
 ---- batch: 110 ----
mean loss: 153.42
train mean loss: 145.56
epoch train time: 0:00:00.562410
elapsed time: 0:02:04.000480
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:09:05.444747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.75
 ---- batch: 020 ----
mean loss: 140.85
 ---- batch: 030 ----
mean loss: 149.28
 ---- batch: 040 ----
mean loss: 148.38
 ---- batch: 050 ----
mean loss: 147.43
 ---- batch: 060 ----
mean loss: 145.33
 ---- batch: 070 ----
mean loss: 142.13
 ---- batch: 080 ----
mean loss: 146.63
 ---- batch: 090 ----
mean loss: 145.77
 ---- batch: 100 ----
mean loss: 140.34
 ---- batch: 110 ----
mean loss: 145.98
train mean loss: 145.52
epoch train time: 0:00:00.561723
elapsed time: 0:02:04.562342
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:09:06.006611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.27
 ---- batch: 020 ----
mean loss: 149.30
 ---- batch: 030 ----
mean loss: 147.73
 ---- batch: 040 ----
mean loss: 146.14
 ---- batch: 050 ----
mean loss: 144.76
 ---- batch: 060 ----
mean loss: 143.61
 ---- batch: 070 ----
mean loss: 153.36
 ---- batch: 080 ----
mean loss: 143.95
 ---- batch: 090 ----
mean loss: 135.13
 ---- batch: 100 ----
mean loss: 149.26
 ---- batch: 110 ----
mean loss: 145.18
train mean loss: 145.21
epoch train time: 0:00:00.584833
elapsed time: 0:02:05.147327
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:09:06.591598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.55
 ---- batch: 020 ----
mean loss: 140.54
 ---- batch: 030 ----
mean loss: 147.16
 ---- batch: 040 ----
mean loss: 147.50
 ---- batch: 050 ----
mean loss: 153.28
 ---- batch: 060 ----
mean loss: 141.70
 ---- batch: 070 ----
mean loss: 144.47
 ---- batch: 080 ----
mean loss: 148.84
 ---- batch: 090 ----
mean loss: 150.15
 ---- batch: 100 ----
mean loss: 143.81
 ---- batch: 110 ----
mean loss: 135.49
train mean loss: 145.36
epoch train time: 0:00:00.605666
elapsed time: 0:02:05.753137
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:09:07.197406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.68
 ---- batch: 020 ----
mean loss: 143.74
 ---- batch: 030 ----
mean loss: 141.99
 ---- batch: 040 ----
mean loss: 142.23
 ---- batch: 050 ----
mean loss: 141.94
 ---- batch: 060 ----
mean loss: 146.31
 ---- batch: 070 ----
mean loss: 133.40
 ---- batch: 080 ----
mean loss: 155.22
 ---- batch: 090 ----
mean loss: 148.01
 ---- batch: 100 ----
mean loss: 160.29
 ---- batch: 110 ----
mean loss: 140.59
train mean loss: 144.94
epoch train time: 0:00:00.571326
elapsed time: 0:02:06.324603
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:09:07.768872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.62
 ---- batch: 020 ----
mean loss: 140.21
 ---- batch: 030 ----
mean loss: 142.68
 ---- batch: 040 ----
mean loss: 138.82
 ---- batch: 050 ----
mean loss: 144.03
 ---- batch: 060 ----
mean loss: 141.90
 ---- batch: 070 ----
mean loss: 144.08
 ---- batch: 080 ----
mean loss: 138.34
 ---- batch: 090 ----
mean loss: 152.65
 ---- batch: 100 ----
mean loss: 152.64
 ---- batch: 110 ----
mean loss: 151.91
train mean loss: 144.34
epoch train time: 0:00:00.561206
elapsed time: 0:02:06.885948
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:09:08.330233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.04
 ---- batch: 020 ----
mean loss: 145.35
 ---- batch: 030 ----
mean loss: 138.09
 ---- batch: 040 ----
mean loss: 152.29
 ---- batch: 050 ----
mean loss: 151.94
 ---- batch: 060 ----
mean loss: 140.46
 ---- batch: 070 ----
mean loss: 136.74
 ---- batch: 080 ----
mean loss: 146.30
 ---- batch: 090 ----
mean loss: 146.74
 ---- batch: 100 ----
mean loss: 143.58
 ---- batch: 110 ----
mean loss: 145.54
train mean loss: 144.27
epoch train time: 0:00:00.556560
elapsed time: 0:02:07.442723
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:09:08.887156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.32
 ---- batch: 020 ----
mean loss: 144.39
 ---- batch: 030 ----
mean loss: 134.84
 ---- batch: 040 ----
mean loss: 149.15
 ---- batch: 050 ----
mean loss: 147.61
 ---- batch: 060 ----
mean loss: 145.79
 ---- batch: 070 ----
mean loss: 144.44
 ---- batch: 080 ----
mean loss: 148.51
 ---- batch: 090 ----
mean loss: 144.60
 ---- batch: 100 ----
mean loss: 152.66
 ---- batch: 110 ----
mean loss: 140.17
train mean loss: 144.17
epoch train time: 0:00:00.601232
elapsed time: 0:02:08.044276
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:09:09.488541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.19
 ---- batch: 020 ----
mean loss: 141.01
 ---- batch: 030 ----
mean loss: 144.13
 ---- batch: 040 ----
mean loss: 143.61
 ---- batch: 050 ----
mean loss: 146.05
 ---- batch: 060 ----
mean loss: 149.11
 ---- batch: 070 ----
mean loss: 140.66
 ---- batch: 080 ----
mean loss: 148.58
 ---- batch: 090 ----
mean loss: 152.03
 ---- batch: 100 ----
mean loss: 133.48
 ---- batch: 110 ----
mean loss: 141.70
train mean loss: 144.72
epoch train time: 0:00:00.602602
elapsed time: 0:02:08.647026
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:09:10.091295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.82
 ---- batch: 020 ----
mean loss: 147.71
 ---- batch: 030 ----
mean loss: 152.53
 ---- batch: 040 ----
mean loss: 143.99
 ---- batch: 050 ----
mean loss: 140.64
 ---- batch: 060 ----
mean loss: 141.31
 ---- batch: 070 ----
mean loss: 151.35
 ---- batch: 080 ----
mean loss: 142.17
 ---- batch: 090 ----
mean loss: 147.75
 ---- batch: 100 ----
mean loss: 145.80
 ---- batch: 110 ----
mean loss: 144.34
train mean loss: 144.82
epoch train time: 0:00:00.575206
elapsed time: 0:02:09.222369
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:09:10.666635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.22
 ---- batch: 020 ----
mean loss: 143.99
 ---- batch: 030 ----
mean loss: 143.16
 ---- batch: 040 ----
mean loss: 138.19
 ---- batch: 050 ----
mean loss: 142.93
 ---- batch: 060 ----
mean loss: 142.17
 ---- batch: 070 ----
mean loss: 134.68
 ---- batch: 080 ----
mean loss: 148.57
 ---- batch: 090 ----
mean loss: 150.83
 ---- batch: 100 ----
mean loss: 149.16
 ---- batch: 110 ----
mean loss: 150.28
train mean loss: 144.27
epoch train time: 0:00:00.556946
elapsed time: 0:02:09.779483
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:09:11.223764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.14
 ---- batch: 020 ----
mean loss: 142.22
 ---- batch: 030 ----
mean loss: 147.26
 ---- batch: 040 ----
mean loss: 142.73
 ---- batch: 050 ----
mean loss: 138.33
 ---- batch: 060 ----
mean loss: 140.42
 ---- batch: 070 ----
mean loss: 146.05
 ---- batch: 080 ----
mean loss: 136.45
 ---- batch: 090 ----
mean loss: 147.75
 ---- batch: 100 ----
mean loss: 136.64
 ---- batch: 110 ----
mean loss: 153.00
train mean loss: 143.71
epoch train time: 0:00:00.550747
elapsed time: 0:02:10.330396
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:09:11.774660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.02
 ---- batch: 020 ----
mean loss: 144.67
 ---- batch: 030 ----
mean loss: 141.20
 ---- batch: 040 ----
mean loss: 143.48
 ---- batch: 050 ----
mean loss: 147.21
 ---- batch: 060 ----
mean loss: 141.63
 ---- batch: 070 ----
mean loss: 144.11
 ---- batch: 080 ----
mean loss: 138.48
 ---- batch: 090 ----
mean loss: 147.60
 ---- batch: 100 ----
mean loss: 144.44
 ---- batch: 110 ----
mean loss: 149.76
train mean loss: 143.33
epoch train time: 0:00:00.587454
elapsed time: 0:02:10.918010
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:09:12.362286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.50
 ---- batch: 020 ----
mean loss: 136.44
 ---- batch: 030 ----
mean loss: 138.92
 ---- batch: 040 ----
mean loss: 142.69
 ---- batch: 050 ----
mean loss: 143.34
 ---- batch: 060 ----
mean loss: 143.59
 ---- batch: 070 ----
mean loss: 150.40
 ---- batch: 080 ----
mean loss: 142.55
 ---- batch: 090 ----
mean loss: 144.60
 ---- batch: 100 ----
mean loss: 140.81
 ---- batch: 110 ----
mean loss: 140.62
train mean loss: 143.03
epoch train time: 0:00:00.566722
elapsed time: 0:02:11.484879
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:09:12.929147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.05
 ---- batch: 020 ----
mean loss: 143.54
 ---- batch: 030 ----
mean loss: 133.76
 ---- batch: 040 ----
mean loss: 153.27
 ---- batch: 050 ----
mean loss: 147.50
 ---- batch: 060 ----
mean loss: 144.98
 ---- batch: 070 ----
mean loss: 140.12
 ---- batch: 080 ----
mean loss: 148.94
 ---- batch: 090 ----
mean loss: 139.83
 ---- batch: 100 ----
mean loss: 138.66
 ---- batch: 110 ----
mean loss: 139.50
train mean loss: 142.85
epoch train time: 0:00:00.563716
elapsed time: 0:02:12.048745
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:09:13.493013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.42
 ---- batch: 020 ----
mean loss: 147.62
 ---- batch: 030 ----
mean loss: 143.21
 ---- batch: 040 ----
mean loss: 137.46
 ---- batch: 050 ----
mean loss: 143.06
 ---- batch: 060 ----
mean loss: 146.69
 ---- batch: 070 ----
mean loss: 139.44
 ---- batch: 080 ----
mean loss: 143.49
 ---- batch: 090 ----
mean loss: 146.40
 ---- batch: 100 ----
mean loss: 148.25
 ---- batch: 110 ----
mean loss: 141.67
train mean loss: 143.70
epoch train time: 0:00:00.568278
elapsed time: 0:02:12.617164
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:09:14.061432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.75
 ---- batch: 020 ----
mean loss: 146.21
 ---- batch: 030 ----
mean loss: 144.28
 ---- batch: 040 ----
mean loss: 137.12
 ---- batch: 050 ----
mean loss: 142.46
 ---- batch: 060 ----
mean loss: 145.27
 ---- batch: 070 ----
mean loss: 138.75
 ---- batch: 080 ----
mean loss: 142.44
 ---- batch: 090 ----
mean loss: 148.17
 ---- batch: 100 ----
mean loss: 141.03
 ---- batch: 110 ----
mean loss: 148.07
train mean loss: 143.05
epoch train time: 0:00:00.572233
elapsed time: 0:02:13.189553
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:09:14.633819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.68
 ---- batch: 020 ----
mean loss: 137.80
 ---- batch: 030 ----
mean loss: 137.64
 ---- batch: 040 ----
mean loss: 147.56
 ---- batch: 050 ----
mean loss: 141.80
 ---- batch: 060 ----
mean loss: 139.66
 ---- batch: 070 ----
mean loss: 145.29
 ---- batch: 080 ----
mean loss: 146.01
 ---- batch: 090 ----
mean loss: 141.90
 ---- batch: 100 ----
mean loss: 144.51
 ---- batch: 110 ----
mean loss: 150.64
train mean loss: 142.83
epoch train time: 0:00:00.576926
elapsed time: 0:02:13.766619
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:09:15.210906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.84
 ---- batch: 020 ----
mean loss: 142.28
 ---- batch: 030 ----
mean loss: 133.09
 ---- batch: 040 ----
mean loss: 136.62
 ---- batch: 050 ----
mean loss: 147.05
 ---- batch: 060 ----
mean loss: 142.15
 ---- batch: 070 ----
mean loss: 146.93
 ---- batch: 080 ----
mean loss: 150.52
 ---- batch: 090 ----
mean loss: 140.38
 ---- batch: 100 ----
mean loss: 150.60
 ---- batch: 110 ----
mean loss: 142.09
train mean loss: 142.03
epoch train time: 0:00:00.565141
elapsed time: 0:02:14.331918
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:09:15.776207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.93
 ---- batch: 020 ----
mean loss: 141.04
 ---- batch: 030 ----
mean loss: 152.24
 ---- batch: 040 ----
mean loss: 130.42
 ---- batch: 050 ----
mean loss: 140.55
 ---- batch: 060 ----
mean loss: 132.65
 ---- batch: 070 ----
mean loss: 140.21
 ---- batch: 080 ----
mean loss: 143.15
 ---- batch: 090 ----
mean loss: 145.40
 ---- batch: 100 ----
mean loss: 146.38
 ---- batch: 110 ----
mean loss: 149.82
train mean loss: 141.84
epoch train time: 0:00:00.567270
elapsed time: 0:02:14.899348
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:09:16.343631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.77
 ---- batch: 020 ----
mean loss: 143.66
 ---- batch: 030 ----
mean loss: 147.09
 ---- batch: 040 ----
mean loss: 145.43
 ---- batch: 050 ----
mean loss: 145.26
 ---- batch: 060 ----
mean loss: 146.32
 ---- batch: 070 ----
mean loss: 142.41
 ---- batch: 080 ----
mean loss: 140.06
 ---- batch: 090 ----
mean loss: 139.70
 ---- batch: 100 ----
mean loss: 143.77
 ---- batch: 110 ----
mean loss: 140.33
train mean loss: 143.66
epoch train time: 0:00:00.566955
elapsed time: 0:02:15.466459
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:09:16.910730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.38
 ---- batch: 020 ----
mean loss: 140.93
 ---- batch: 030 ----
mean loss: 133.87
 ---- batch: 040 ----
mean loss: 144.06
 ---- batch: 050 ----
mean loss: 131.73
 ---- batch: 060 ----
mean loss: 137.79
 ---- batch: 070 ----
mean loss: 145.01
 ---- batch: 080 ----
mean loss: 146.37
 ---- batch: 090 ----
mean loss: 152.73
 ---- batch: 100 ----
mean loss: 142.59
 ---- batch: 110 ----
mean loss: 140.88
train mean loss: 141.66
epoch train time: 0:00:00.561919
elapsed time: 0:02:16.028534
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:09:17.472802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.89
 ---- batch: 020 ----
mean loss: 144.61
 ---- batch: 030 ----
mean loss: 143.05
 ---- batch: 040 ----
mean loss: 140.03
 ---- batch: 050 ----
mean loss: 146.08
 ---- batch: 060 ----
mean loss: 144.03
 ---- batch: 070 ----
mean loss: 141.26
 ---- batch: 080 ----
mean loss: 142.10
 ---- batch: 090 ----
mean loss: 133.55
 ---- batch: 100 ----
mean loss: 145.00
 ---- batch: 110 ----
mean loss: 147.77
train mean loss: 141.88
epoch train time: 0:00:00.614014
elapsed time: 0:02:16.642737
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:09:18.087056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.24
 ---- batch: 020 ----
mean loss: 134.44
 ---- batch: 030 ----
mean loss: 148.43
 ---- batch: 040 ----
mean loss: 142.91
 ---- batch: 050 ----
mean loss: 146.95
 ---- batch: 060 ----
mean loss: 144.02
 ---- batch: 070 ----
mean loss: 141.03
 ---- batch: 080 ----
mean loss: 135.16
 ---- batch: 090 ----
mean loss: 137.73
 ---- batch: 100 ----
mean loss: 142.66
 ---- batch: 110 ----
mean loss: 147.24
train mean loss: 142.02
epoch train time: 0:00:00.566265
elapsed time: 0:02:17.209205
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:09:18.653473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.51
 ---- batch: 020 ----
mean loss: 148.03
 ---- batch: 030 ----
mean loss: 148.12
 ---- batch: 040 ----
mean loss: 139.10
 ---- batch: 050 ----
mean loss: 140.12
 ---- batch: 060 ----
mean loss: 141.23
 ---- batch: 070 ----
mean loss: 143.65
 ---- batch: 080 ----
mean loss: 140.32
 ---- batch: 090 ----
mean loss: 137.70
 ---- batch: 100 ----
mean loss: 139.15
 ---- batch: 110 ----
mean loss: 139.54
train mean loss: 141.16
epoch train time: 0:00:00.560756
elapsed time: 0:02:17.770101
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:09:19.214370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.39
 ---- batch: 020 ----
mean loss: 138.95
 ---- batch: 030 ----
mean loss: 137.78
 ---- batch: 040 ----
mean loss: 144.80
 ---- batch: 050 ----
mean loss: 145.48
 ---- batch: 060 ----
mean loss: 147.11
 ---- batch: 070 ----
mean loss: 132.60
 ---- batch: 080 ----
mean loss: 139.24
 ---- batch: 090 ----
mean loss: 149.45
 ---- batch: 100 ----
mean loss: 144.23
 ---- batch: 110 ----
mean loss: 145.13
train mean loss: 141.54
epoch train time: 0:00:00.560390
elapsed time: 0:02:18.330638
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:09:19.774931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.02
 ---- batch: 020 ----
mean loss: 141.03
 ---- batch: 030 ----
mean loss: 148.75
 ---- batch: 040 ----
mean loss: 131.20
 ---- batch: 050 ----
mean loss: 146.12
 ---- batch: 060 ----
mean loss: 139.93
 ---- batch: 070 ----
mean loss: 143.46
 ---- batch: 080 ----
mean loss: 139.42
 ---- batch: 090 ----
mean loss: 146.36
 ---- batch: 100 ----
mean loss: 141.98
 ---- batch: 110 ----
mean loss: 136.75
train mean loss: 140.98
epoch train time: 0:00:00.579028
elapsed time: 0:02:18.909833
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:09:20.354104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.40
 ---- batch: 020 ----
mean loss: 140.82
 ---- batch: 030 ----
mean loss: 153.65
 ---- batch: 040 ----
mean loss: 137.60
 ---- batch: 050 ----
mean loss: 133.02
 ---- batch: 060 ----
mean loss: 142.36
 ---- batch: 070 ----
mean loss: 134.98
 ---- batch: 080 ----
mean loss: 147.84
 ---- batch: 090 ----
mean loss: 141.25
 ---- batch: 100 ----
mean loss: 136.12
 ---- batch: 110 ----
mean loss: 144.12
train mean loss: 140.68
epoch train time: 0:00:00.571400
elapsed time: 0:02:19.481376
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:09:20.925673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.17
 ---- batch: 020 ----
mean loss: 130.93
 ---- batch: 030 ----
mean loss: 138.92
 ---- batch: 040 ----
mean loss: 134.75
 ---- batch: 050 ----
mean loss: 140.23
 ---- batch: 060 ----
mean loss: 145.60
 ---- batch: 070 ----
mean loss: 149.67
 ---- batch: 080 ----
mean loss: 138.70
 ---- batch: 090 ----
mean loss: 138.77
 ---- batch: 100 ----
mean loss: 139.60
 ---- batch: 110 ----
mean loss: 149.11
train mean loss: 140.12
epoch train time: 0:00:00.594532
elapsed time: 0:02:20.076081
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:09:21.520350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.75
 ---- batch: 020 ----
mean loss: 129.84
 ---- batch: 030 ----
mean loss: 138.14
 ---- batch: 040 ----
mean loss: 139.54
 ---- batch: 050 ----
mean loss: 140.23
 ---- batch: 060 ----
mean loss: 145.10
 ---- batch: 070 ----
mean loss: 141.62
 ---- batch: 080 ----
mean loss: 136.38
 ---- batch: 090 ----
mean loss: 142.07
 ---- batch: 100 ----
mean loss: 143.99
 ---- batch: 110 ----
mean loss: 140.97
train mean loss: 139.65
epoch train time: 0:00:00.561798
elapsed time: 0:02:20.638037
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:09:22.082304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.88
 ---- batch: 020 ----
mean loss: 141.22
 ---- batch: 030 ----
mean loss: 146.53
 ---- batch: 040 ----
mean loss: 139.87
 ---- batch: 050 ----
mean loss: 142.73
 ---- batch: 060 ----
mean loss: 144.58
 ---- batch: 070 ----
mean loss: 132.76
 ---- batch: 080 ----
mean loss: 139.86
 ---- batch: 090 ----
mean loss: 136.66
 ---- batch: 100 ----
mean loss: 137.91
 ---- batch: 110 ----
mean loss: 144.43
train mean loss: 140.58
epoch train time: 0:00:00.557695
elapsed time: 0:02:21.195914
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:09:22.640183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.61
 ---- batch: 020 ----
mean loss: 140.77
 ---- batch: 030 ----
mean loss: 136.92
 ---- batch: 040 ----
mean loss: 140.32
 ---- batch: 050 ----
mean loss: 146.64
 ---- batch: 060 ----
mean loss: 138.74
 ---- batch: 070 ----
mean loss: 137.41
 ---- batch: 080 ----
mean loss: 139.11
 ---- batch: 090 ----
mean loss: 133.14
 ---- batch: 100 ----
mean loss: 150.12
 ---- batch: 110 ----
mean loss: 149.50
train mean loss: 141.05
epoch train time: 0:00:00.562706
elapsed time: 0:02:21.758763
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:09:23.203047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.69
 ---- batch: 020 ----
mean loss: 149.29
 ---- batch: 030 ----
mean loss: 141.30
 ---- batch: 040 ----
mean loss: 138.80
 ---- batch: 050 ----
mean loss: 133.80
 ---- batch: 060 ----
mean loss: 130.53
 ---- batch: 070 ----
mean loss: 146.98
 ---- batch: 080 ----
mean loss: 138.09
 ---- batch: 090 ----
mean loss: 139.42
 ---- batch: 100 ----
mean loss: 138.84
 ---- batch: 110 ----
mean loss: 137.00
train mean loss: 139.05
epoch train time: 0:00:00.565491
elapsed time: 0:02:22.324409
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:09:23.768676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.93
 ---- batch: 020 ----
mean loss: 144.49
 ---- batch: 030 ----
mean loss: 145.10
 ---- batch: 040 ----
mean loss: 142.51
 ---- batch: 050 ----
mean loss: 131.10
 ---- batch: 060 ----
mean loss: 132.66
 ---- batch: 070 ----
mean loss: 145.18
 ---- batch: 080 ----
mean loss: 135.93
 ---- batch: 090 ----
mean loss: 144.40
 ---- batch: 100 ----
mean loss: 139.81
 ---- batch: 110 ----
mean loss: 141.56
train mean loss: 139.58
epoch train time: 0:00:00.570519
elapsed time: 0:02:22.895079
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:09:24.339360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.81
 ---- batch: 020 ----
mean loss: 135.59
 ---- batch: 030 ----
mean loss: 135.92
 ---- batch: 040 ----
mean loss: 148.74
 ---- batch: 050 ----
mean loss: 130.69
 ---- batch: 060 ----
mean loss: 133.33
 ---- batch: 070 ----
mean loss: 146.04
 ---- batch: 080 ----
mean loss: 141.82
 ---- batch: 090 ----
mean loss: 140.94
 ---- batch: 100 ----
mean loss: 135.95
 ---- batch: 110 ----
mean loss: 144.08
train mean loss: 139.53
epoch train time: 0:00:00.562627
elapsed time: 0:02:23.457866
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:09:24.902136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.40
 ---- batch: 020 ----
mean loss: 137.69
 ---- batch: 030 ----
mean loss: 140.49
 ---- batch: 040 ----
mean loss: 134.57
 ---- batch: 050 ----
mean loss: 141.57
 ---- batch: 060 ----
mean loss: 132.87
 ---- batch: 070 ----
mean loss: 152.28
 ---- batch: 080 ----
mean loss: 143.94
 ---- batch: 090 ----
mean loss: 142.59
 ---- batch: 100 ----
mean loss: 142.28
 ---- batch: 110 ----
mean loss: 135.77
train mean loss: 139.78
epoch train time: 0:00:00.557489
elapsed time: 0:02:24.015497
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:09:25.459778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.11
 ---- batch: 020 ----
mean loss: 133.75
 ---- batch: 030 ----
mean loss: 137.01
 ---- batch: 040 ----
mean loss: 141.88
 ---- batch: 050 ----
mean loss: 136.61
 ---- batch: 060 ----
mean loss: 147.02
 ---- batch: 070 ----
mean loss: 132.11
 ---- batch: 080 ----
mean loss: 141.53
 ---- batch: 090 ----
mean loss: 133.96
 ---- batch: 100 ----
mean loss: 139.16
 ---- batch: 110 ----
mean loss: 145.20
train mean loss: 139.28
epoch train time: 0:00:00.571281
elapsed time: 0:02:24.586933
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:09:26.031200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.14
 ---- batch: 020 ----
mean loss: 135.25
 ---- batch: 030 ----
mean loss: 144.66
 ---- batch: 040 ----
mean loss: 128.83
 ---- batch: 050 ----
mean loss: 137.94
 ---- batch: 060 ----
mean loss: 147.01
 ---- batch: 070 ----
mean loss: 142.15
 ---- batch: 080 ----
mean loss: 141.63
 ---- batch: 090 ----
mean loss: 136.30
 ---- batch: 100 ----
mean loss: 135.76
 ---- batch: 110 ----
mean loss: 142.10
train mean loss: 139.57
epoch train time: 0:00:00.577097
elapsed time: 0:02:25.164184
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:09:26.608450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.51
 ---- batch: 020 ----
mean loss: 131.44
 ---- batch: 030 ----
mean loss: 135.00
 ---- batch: 040 ----
mean loss: 134.27
 ---- batch: 050 ----
mean loss: 138.28
 ---- batch: 060 ----
mean loss: 140.77
 ---- batch: 070 ----
mean loss: 140.06
 ---- batch: 080 ----
mean loss: 135.66
 ---- batch: 090 ----
mean loss: 141.50
 ---- batch: 100 ----
mean loss: 141.03
 ---- batch: 110 ----
mean loss: 145.70
train mean loss: 138.55
epoch train time: 0:00:00.563585
elapsed time: 0:02:25.727905
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:09:27.172195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.46
 ---- batch: 020 ----
mean loss: 133.36
 ---- batch: 030 ----
mean loss: 140.77
 ---- batch: 040 ----
mean loss: 131.07
 ---- batch: 050 ----
mean loss: 127.92
 ---- batch: 060 ----
mean loss: 132.52
 ---- batch: 070 ----
mean loss: 123.60
 ---- batch: 080 ----
mean loss: 135.11
 ---- batch: 090 ----
mean loss: 135.10
 ---- batch: 100 ----
mean loss: 136.31
 ---- batch: 110 ----
mean loss: 130.26
train mean loss: 132.66
epoch train time: 0:00:00.559552
elapsed time: 0:02:26.287638
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:09:27.731896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.87
 ---- batch: 020 ----
mean loss: 127.62
 ---- batch: 030 ----
mean loss: 127.53
 ---- batch: 040 ----
mean loss: 131.83
 ---- batch: 050 ----
mean loss: 132.01
 ---- batch: 060 ----
mean loss: 131.00
 ---- batch: 070 ----
mean loss: 125.65
 ---- batch: 080 ----
mean loss: 140.24
 ---- batch: 090 ----
mean loss: 136.69
 ---- batch: 100 ----
mean loss: 134.48
 ---- batch: 110 ----
mean loss: 127.66
train mean loss: 132.01
epoch train time: 0:00:00.619592
elapsed time: 0:02:26.907371
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:09:28.351640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.55
 ---- batch: 020 ----
mean loss: 132.66
 ---- batch: 030 ----
mean loss: 134.02
 ---- batch: 040 ----
mean loss: 125.67
 ---- batch: 050 ----
mean loss: 134.58
 ---- batch: 060 ----
mean loss: 132.92
 ---- batch: 070 ----
mean loss: 129.79
 ---- batch: 080 ----
mean loss: 140.18
 ---- batch: 090 ----
mean loss: 131.29
 ---- batch: 100 ----
mean loss: 131.64
 ---- batch: 110 ----
mean loss: 129.22
train mean loss: 131.73
epoch train time: 0:00:00.604343
elapsed time: 0:02:27.511858
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:09:28.956123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.89
 ---- batch: 020 ----
mean loss: 128.21
 ---- batch: 030 ----
mean loss: 128.59
 ---- batch: 040 ----
mean loss: 125.50
 ---- batch: 050 ----
mean loss: 136.02
 ---- batch: 060 ----
mean loss: 131.72
 ---- batch: 070 ----
mean loss: 137.74
 ---- batch: 080 ----
mean loss: 131.41
 ---- batch: 090 ----
mean loss: 138.34
 ---- batch: 100 ----
mean loss: 126.24
 ---- batch: 110 ----
mean loss: 130.70
train mean loss: 131.57
epoch train time: 0:00:00.563404
elapsed time: 0:02:28.075398
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:09:29.519665
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.48
 ---- batch: 020 ----
mean loss: 130.13
 ---- batch: 030 ----
mean loss: 133.09
 ---- batch: 040 ----
mean loss: 128.62
 ---- batch: 050 ----
mean loss: 126.01
 ---- batch: 060 ----
mean loss: 134.25
 ---- batch: 070 ----
mean loss: 137.57
 ---- batch: 080 ----
mean loss: 138.77
 ---- batch: 090 ----
mean loss: 123.14
 ---- batch: 100 ----
mean loss: 126.41
 ---- batch: 110 ----
mean loss: 135.18
train mean loss: 131.67
epoch train time: 0:00:00.581916
elapsed time: 0:02:28.657476
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:09:30.101746
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.80
 ---- batch: 020 ----
mean loss: 128.54
 ---- batch: 030 ----
mean loss: 129.11
 ---- batch: 040 ----
mean loss: 134.52
 ---- batch: 050 ----
mean loss: 123.81
 ---- batch: 060 ----
mean loss: 133.37
 ---- batch: 070 ----
mean loss: 133.93
 ---- batch: 080 ----
mean loss: 137.11
 ---- batch: 090 ----
mean loss: 134.44
 ---- batch: 100 ----
mean loss: 124.51
 ---- batch: 110 ----
mean loss: 134.69
train mean loss: 131.64
epoch train time: 0:00:00.577203
elapsed time: 0:02:29.234844
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:09:30.679133
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.01
 ---- batch: 020 ----
mean loss: 138.79
 ---- batch: 030 ----
mean loss: 132.93
 ---- batch: 040 ----
mean loss: 142.29
 ---- batch: 050 ----
mean loss: 124.54
 ---- batch: 060 ----
mean loss: 128.83
 ---- batch: 070 ----
mean loss: 124.92
 ---- batch: 080 ----
mean loss: 136.51
 ---- batch: 090 ----
mean loss: 131.78
 ---- batch: 100 ----
mean loss: 134.83
 ---- batch: 110 ----
mean loss: 126.39
train mean loss: 131.51
epoch train time: 0:00:00.570700
elapsed time: 0:02:29.805709
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:09:31.249977
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.89
 ---- batch: 020 ----
mean loss: 129.63
 ---- batch: 030 ----
mean loss: 130.78
 ---- batch: 040 ----
mean loss: 126.11
 ---- batch: 050 ----
mean loss: 132.88
 ---- batch: 060 ----
mean loss: 136.46
 ---- batch: 070 ----
mean loss: 136.24
 ---- batch: 080 ----
mean loss: 134.26
 ---- batch: 090 ----
mean loss: 130.89
 ---- batch: 100 ----
mean loss: 129.33
 ---- batch: 110 ----
mean loss: 132.31
train mean loss: 131.43
epoch train time: 0:00:00.570515
elapsed time: 0:02:30.376370
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:09:31.820640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.01
 ---- batch: 020 ----
mean loss: 123.49
 ---- batch: 030 ----
mean loss: 136.75
 ---- batch: 040 ----
mean loss: 131.62
 ---- batch: 050 ----
mean loss: 128.66
 ---- batch: 060 ----
mean loss: 133.43
 ---- batch: 070 ----
mean loss: 132.40
 ---- batch: 080 ----
mean loss: 121.75
 ---- batch: 090 ----
mean loss: 134.21
 ---- batch: 100 ----
mean loss: 131.51
 ---- batch: 110 ----
mean loss: 142.66
train mean loss: 131.38
epoch train time: 0:00:00.587310
elapsed time: 0:02:30.963836
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:09:32.408107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.90
 ---- batch: 020 ----
mean loss: 128.19
 ---- batch: 030 ----
mean loss: 133.14
 ---- batch: 040 ----
mean loss: 133.48
 ---- batch: 050 ----
mean loss: 132.60
 ---- batch: 060 ----
mean loss: 132.09
 ---- batch: 070 ----
mean loss: 129.19
 ---- batch: 080 ----
mean loss: 125.54
 ---- batch: 090 ----
mean loss: 127.51
 ---- batch: 100 ----
mean loss: 128.13
 ---- batch: 110 ----
mean loss: 130.53
train mean loss: 131.41
epoch train time: 0:00:00.618218
elapsed time: 0:02:31.582208
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:09:33.026477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.14
 ---- batch: 020 ----
mean loss: 138.48
 ---- batch: 030 ----
mean loss: 136.79
 ---- batch: 040 ----
mean loss: 132.77
 ---- batch: 050 ----
mean loss: 136.56
 ---- batch: 060 ----
mean loss: 130.09
 ---- batch: 070 ----
mean loss: 124.12
 ---- batch: 080 ----
mean loss: 127.88
 ---- batch: 090 ----
mean loss: 139.74
 ---- batch: 100 ----
mean loss: 118.20
 ---- batch: 110 ----
mean loss: 128.97
train mean loss: 131.44
epoch train time: 0:00:00.584794
elapsed time: 0:02:32.167140
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:09:33.611417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.22
 ---- batch: 020 ----
mean loss: 126.20
 ---- batch: 030 ----
mean loss: 134.76
 ---- batch: 040 ----
mean loss: 132.88
 ---- batch: 050 ----
mean loss: 131.91
 ---- batch: 060 ----
mean loss: 132.77
 ---- batch: 070 ----
mean loss: 130.43
 ---- batch: 080 ----
mean loss: 132.10
 ---- batch: 090 ----
mean loss: 132.74
 ---- batch: 100 ----
mean loss: 135.93
 ---- batch: 110 ----
mean loss: 131.44
train mean loss: 131.37
epoch train time: 0:00:00.554319
elapsed time: 0:02:32.721640
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:09:34.165908
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.18
 ---- batch: 020 ----
mean loss: 133.65
 ---- batch: 030 ----
mean loss: 119.94
 ---- batch: 040 ----
mean loss: 137.33
 ---- batch: 050 ----
mean loss: 135.64
 ---- batch: 060 ----
mean loss: 133.81
 ---- batch: 070 ----
mean loss: 122.85
 ---- batch: 080 ----
mean loss: 129.42
 ---- batch: 090 ----
mean loss: 122.14
 ---- batch: 100 ----
mean loss: 136.02
 ---- batch: 110 ----
mean loss: 138.23
train mean loss: 131.26
epoch train time: 0:00:00.557654
elapsed time: 0:02:33.279443
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:09:34.723741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.76
 ---- batch: 020 ----
mean loss: 126.21
 ---- batch: 030 ----
mean loss: 128.71
 ---- batch: 040 ----
mean loss: 127.65
 ---- batch: 050 ----
mean loss: 131.65
 ---- batch: 060 ----
mean loss: 128.76
 ---- batch: 070 ----
mean loss: 138.47
 ---- batch: 080 ----
mean loss: 134.49
 ---- batch: 090 ----
mean loss: 131.49
 ---- batch: 100 ----
mean loss: 134.58
 ---- batch: 110 ----
mean loss: 134.92
train mean loss: 131.37
epoch train time: 0:00:00.563998
elapsed time: 0:02:33.843631
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:09:35.287896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.72
 ---- batch: 020 ----
mean loss: 131.87
 ---- batch: 030 ----
mean loss: 135.38
 ---- batch: 040 ----
mean loss: 130.43
 ---- batch: 050 ----
mean loss: 127.15
 ---- batch: 060 ----
mean loss: 131.51
 ---- batch: 070 ----
mean loss: 134.30
 ---- batch: 080 ----
mean loss: 127.07
 ---- batch: 090 ----
mean loss: 120.44
 ---- batch: 100 ----
mean loss: 130.77
 ---- batch: 110 ----
mean loss: 133.37
train mean loss: 131.33
epoch train time: 0:00:00.572414
elapsed time: 0:02:34.416192
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:09:35.860458
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.27
 ---- batch: 020 ----
mean loss: 134.84
 ---- batch: 030 ----
mean loss: 130.73
 ---- batch: 040 ----
mean loss: 127.85
 ---- batch: 050 ----
mean loss: 126.46
 ---- batch: 060 ----
mean loss: 130.74
 ---- batch: 070 ----
mean loss: 129.95
 ---- batch: 080 ----
mean loss: 134.02
 ---- batch: 090 ----
mean loss: 132.75
 ---- batch: 100 ----
mean loss: 130.34
 ---- batch: 110 ----
mean loss: 130.65
train mean loss: 131.26
epoch train time: 0:00:00.560397
elapsed time: 0:02:34.976741
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:09:36.421007
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.44
 ---- batch: 020 ----
mean loss: 126.64
 ---- batch: 030 ----
mean loss: 132.23
 ---- batch: 040 ----
mean loss: 129.85
 ---- batch: 050 ----
mean loss: 130.69
 ---- batch: 060 ----
mean loss: 132.95
 ---- batch: 070 ----
mean loss: 133.11
 ---- batch: 080 ----
mean loss: 130.39
 ---- batch: 090 ----
mean loss: 135.34
 ---- batch: 100 ----
mean loss: 134.59
 ---- batch: 110 ----
mean loss: 131.11
train mean loss: 131.24
epoch train time: 0:00:00.561323
elapsed time: 0:02:35.538202
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:09:36.982467
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.79
 ---- batch: 020 ----
mean loss: 128.99
 ---- batch: 030 ----
mean loss: 135.51
 ---- batch: 040 ----
mean loss: 131.31
 ---- batch: 050 ----
mean loss: 130.19
 ---- batch: 060 ----
mean loss: 129.65
 ---- batch: 070 ----
mean loss: 129.58
 ---- batch: 080 ----
mean loss: 129.72
 ---- batch: 090 ----
mean loss: 130.61
 ---- batch: 100 ----
mean loss: 137.02
 ---- batch: 110 ----
mean loss: 128.15
train mean loss: 131.28
epoch train time: 0:00:00.563939
elapsed time: 0:02:36.102282
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:09:37.546550
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.53
 ---- batch: 020 ----
mean loss: 131.19
 ---- batch: 030 ----
mean loss: 128.15
 ---- batch: 040 ----
mean loss: 127.24
 ---- batch: 050 ----
mean loss: 131.55
 ---- batch: 060 ----
mean loss: 118.70
 ---- batch: 070 ----
mean loss: 132.13
 ---- batch: 080 ----
mean loss: 140.56
 ---- batch: 090 ----
mean loss: 133.07
 ---- batch: 100 ----
mean loss: 131.64
 ---- batch: 110 ----
mean loss: 137.57
train mean loss: 131.24
epoch train time: 0:00:00.573891
elapsed time: 0:02:36.676324
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:09:38.120589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.10
 ---- batch: 020 ----
mean loss: 132.25
 ---- batch: 030 ----
mean loss: 132.65
 ---- batch: 040 ----
mean loss: 133.35
 ---- batch: 050 ----
mean loss: 134.70
 ---- batch: 060 ----
mean loss: 127.81
 ---- batch: 070 ----
mean loss: 126.81
 ---- batch: 080 ----
mean loss: 127.88
 ---- batch: 090 ----
mean loss: 133.05
 ---- batch: 100 ----
mean loss: 137.41
 ---- batch: 110 ----
mean loss: 135.02
train mean loss: 131.15
epoch train time: 0:00:00.577105
elapsed time: 0:02:37.253586
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:09:38.697877
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.57
 ---- batch: 020 ----
mean loss: 128.66
 ---- batch: 030 ----
mean loss: 134.86
 ---- batch: 040 ----
mean loss: 133.70
 ---- batch: 050 ----
mean loss: 129.27
 ---- batch: 060 ----
mean loss: 127.99
 ---- batch: 070 ----
mean loss: 135.80
 ---- batch: 080 ----
mean loss: 139.15
 ---- batch: 090 ----
mean loss: 126.28
 ---- batch: 100 ----
mean loss: 128.08
 ---- batch: 110 ----
mean loss: 133.82
train mean loss: 131.15
epoch train time: 0:00:00.580900
elapsed time: 0:02:37.834651
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:09:39.278919
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.37
 ---- batch: 020 ----
mean loss: 132.60
 ---- batch: 030 ----
mean loss: 137.75
 ---- batch: 040 ----
mean loss: 127.89
 ---- batch: 050 ----
mean loss: 127.95
 ---- batch: 060 ----
mean loss: 128.43
 ---- batch: 070 ----
mean loss: 133.00
 ---- batch: 080 ----
mean loss: 128.35
 ---- batch: 090 ----
mean loss: 130.03
 ---- batch: 100 ----
mean loss: 130.70
 ---- batch: 110 ----
mean loss: 126.87
train mean loss: 131.12
epoch train time: 0:00:00.575690
elapsed time: 0:02:38.410486
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:09:39.854761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.79
 ---- batch: 020 ----
mean loss: 132.97
 ---- batch: 030 ----
mean loss: 129.34
 ---- batch: 040 ----
mean loss: 128.03
 ---- batch: 050 ----
mean loss: 135.67
 ---- batch: 060 ----
mean loss: 131.63
 ---- batch: 070 ----
mean loss: 132.16
 ---- batch: 080 ----
mean loss: 131.48
 ---- batch: 090 ----
mean loss: 134.83
 ---- batch: 100 ----
mean loss: 129.13
 ---- batch: 110 ----
mean loss: 128.08
train mean loss: 131.12
epoch train time: 0:00:00.573879
elapsed time: 0:02:38.984512
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:09:40.428796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.85
 ---- batch: 020 ----
mean loss: 127.69
 ---- batch: 030 ----
mean loss: 130.95
 ---- batch: 040 ----
mean loss: 125.96
 ---- batch: 050 ----
mean loss: 131.34
 ---- batch: 060 ----
mean loss: 136.13
 ---- batch: 070 ----
mean loss: 133.14
 ---- batch: 080 ----
mean loss: 141.02
 ---- batch: 090 ----
mean loss: 133.94
 ---- batch: 100 ----
mean loss: 123.56
 ---- batch: 110 ----
mean loss: 130.78
train mean loss: 131.09
epoch train time: 0:00:00.576509
elapsed time: 0:02:39.561178
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:09:41.005454
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.04
 ---- batch: 020 ----
mean loss: 124.87
 ---- batch: 030 ----
mean loss: 134.34
 ---- batch: 040 ----
mean loss: 134.76
 ---- batch: 050 ----
mean loss: 124.80
 ---- batch: 060 ----
mean loss: 137.46
 ---- batch: 070 ----
mean loss: 128.15
 ---- batch: 080 ----
mean loss: 123.46
 ---- batch: 090 ----
mean loss: 134.77
 ---- batch: 100 ----
mean loss: 134.61
 ---- batch: 110 ----
mean loss: 131.09
train mean loss: 131.09
epoch train time: 0:00:00.566595
elapsed time: 0:02:40.127921
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:09:41.572189
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.22
 ---- batch: 020 ----
mean loss: 139.89
 ---- batch: 030 ----
mean loss: 131.84
 ---- batch: 040 ----
mean loss: 125.92
 ---- batch: 050 ----
mean loss: 126.60
 ---- batch: 060 ----
mean loss: 122.98
 ---- batch: 070 ----
mean loss: 141.09
 ---- batch: 080 ----
mean loss: 126.11
 ---- batch: 090 ----
mean loss: 134.64
 ---- batch: 100 ----
mean loss: 133.84
 ---- batch: 110 ----
mean loss: 130.52
train mean loss: 130.90
epoch train time: 0:00:00.579532
elapsed time: 0:02:40.707587
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:09:42.151871
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.53
 ---- batch: 020 ----
mean loss: 131.41
 ---- batch: 030 ----
mean loss: 128.32
 ---- batch: 040 ----
mean loss: 123.82
 ---- batch: 050 ----
mean loss: 130.03
 ---- batch: 060 ----
mean loss: 135.85
 ---- batch: 070 ----
mean loss: 135.08
 ---- batch: 080 ----
mean loss: 139.52
 ---- batch: 090 ----
mean loss: 131.45
 ---- batch: 100 ----
mean loss: 134.21
 ---- batch: 110 ----
mean loss: 130.93
train mean loss: 130.98
epoch train time: 0:00:00.579648
elapsed time: 0:02:41.287405
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:09:42.731671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.79
 ---- batch: 020 ----
mean loss: 127.15
 ---- batch: 030 ----
mean loss: 130.03
 ---- batch: 040 ----
mean loss: 133.70
 ---- batch: 050 ----
mean loss: 130.85
 ---- batch: 060 ----
mean loss: 130.79
 ---- batch: 070 ----
mean loss: 136.76
 ---- batch: 080 ----
mean loss: 132.77
 ---- batch: 090 ----
mean loss: 130.54
 ---- batch: 100 ----
mean loss: 127.92
 ---- batch: 110 ----
mean loss: 128.85
train mean loss: 130.91
epoch train time: 0:00:00.564664
elapsed time: 0:02:41.852219
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:09:43.296487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.00
 ---- batch: 020 ----
mean loss: 133.94
 ---- batch: 030 ----
mean loss: 140.61
 ---- batch: 040 ----
mean loss: 137.55
 ---- batch: 050 ----
mean loss: 127.03
 ---- batch: 060 ----
mean loss: 133.47
 ---- batch: 070 ----
mean loss: 125.94
 ---- batch: 080 ----
mean loss: 118.87
 ---- batch: 090 ----
mean loss: 135.96
 ---- batch: 100 ----
mean loss: 132.87
 ---- batch: 110 ----
mean loss: 131.66
train mean loss: 130.92
epoch train time: 0:00:00.559888
elapsed time: 0:02:42.412253
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:09:43.856521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.80
 ---- batch: 020 ----
mean loss: 127.86
 ---- batch: 030 ----
mean loss: 124.98
 ---- batch: 040 ----
mean loss: 139.18
 ---- batch: 050 ----
mean loss: 129.37
 ---- batch: 060 ----
mean loss: 131.51
 ---- batch: 070 ----
mean loss: 128.42
 ---- batch: 080 ----
mean loss: 130.68
 ---- batch: 090 ----
mean loss: 129.74
 ---- batch: 100 ----
mean loss: 132.18
 ---- batch: 110 ----
mean loss: 134.49
train mean loss: 131.07
epoch train time: 0:00:00.551118
elapsed time: 0:02:42.963510
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:09:44.407776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.75
 ---- batch: 020 ----
mean loss: 124.66
 ---- batch: 030 ----
mean loss: 124.05
 ---- batch: 040 ----
mean loss: 139.43
 ---- batch: 050 ----
mean loss: 132.20
 ---- batch: 060 ----
mean loss: 123.21
 ---- batch: 070 ----
mean loss: 133.23
 ---- batch: 080 ----
mean loss: 135.93
 ---- batch: 090 ----
mean loss: 129.41
 ---- batch: 100 ----
mean loss: 139.40
 ---- batch: 110 ----
mean loss: 135.40
train mean loss: 130.94
epoch train time: 0:00:00.559653
elapsed time: 0:02:43.523344
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:09:44.967641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.32
 ---- batch: 020 ----
mean loss: 128.27
 ---- batch: 030 ----
mean loss: 134.42
 ---- batch: 040 ----
mean loss: 129.76
 ---- batch: 050 ----
mean loss: 125.06
 ---- batch: 060 ----
mean loss: 139.26
 ---- batch: 070 ----
mean loss: 133.02
 ---- batch: 080 ----
mean loss: 132.63
 ---- batch: 090 ----
mean loss: 131.38
 ---- batch: 100 ----
mean loss: 123.96
 ---- batch: 110 ----
mean loss: 129.47
train mean loss: 130.85
epoch train time: 0:00:00.547566
elapsed time: 0:02:44.071078
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:09:45.515349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.33
 ---- batch: 020 ----
mean loss: 121.40
 ---- batch: 030 ----
mean loss: 131.73
 ---- batch: 040 ----
mean loss: 134.10
 ---- batch: 050 ----
mean loss: 127.00
 ---- batch: 060 ----
mean loss: 136.70
 ---- batch: 070 ----
mean loss: 133.66
 ---- batch: 080 ----
mean loss: 132.39
 ---- batch: 090 ----
mean loss: 125.98
 ---- batch: 100 ----
mean loss: 136.24
 ---- batch: 110 ----
mean loss: 130.97
train mean loss: 130.86
epoch train time: 0:00:00.562804
elapsed time: 0:02:44.634050
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:09:46.078328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.19
 ---- batch: 020 ----
mean loss: 128.89
 ---- batch: 030 ----
mean loss: 121.67
 ---- batch: 040 ----
mean loss: 134.02
 ---- batch: 050 ----
mean loss: 129.22
 ---- batch: 060 ----
mean loss: 136.12
 ---- batch: 070 ----
mean loss: 133.71
 ---- batch: 080 ----
mean loss: 130.46
 ---- batch: 090 ----
mean loss: 137.78
 ---- batch: 100 ----
mean loss: 134.39
 ---- batch: 110 ----
mean loss: 128.24
train mean loss: 130.77
epoch train time: 0:00:00.556487
elapsed time: 0:02:45.190697
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:09:46.634969
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.52
 ---- batch: 020 ----
mean loss: 124.49
 ---- batch: 030 ----
mean loss: 125.73
 ---- batch: 040 ----
mean loss: 138.46
 ---- batch: 050 ----
mean loss: 129.18
 ---- batch: 060 ----
mean loss: 136.36
 ---- batch: 070 ----
mean loss: 135.36
 ---- batch: 080 ----
mean loss: 129.75
 ---- batch: 090 ----
mean loss: 131.93
 ---- batch: 100 ----
mean loss: 128.25
 ---- batch: 110 ----
mean loss: 126.04
train mean loss: 130.79
epoch train time: 0:00:00.569544
elapsed time: 0:02:45.760401
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:09:47.204687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.33
 ---- batch: 020 ----
mean loss: 133.19
 ---- batch: 030 ----
mean loss: 131.28
 ---- batch: 040 ----
mean loss: 134.44
 ---- batch: 050 ----
mean loss: 129.84
 ---- batch: 060 ----
mean loss: 143.36
 ---- batch: 070 ----
mean loss: 132.65
 ---- batch: 080 ----
mean loss: 125.73
 ---- batch: 090 ----
mean loss: 125.05
 ---- batch: 100 ----
mean loss: 129.51
 ---- batch: 110 ----
mean loss: 123.56
train mean loss: 130.80
epoch train time: 0:00:00.569679
elapsed time: 0:02:46.330246
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:09:47.774517
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.55
 ---- batch: 020 ----
mean loss: 139.33
 ---- batch: 030 ----
mean loss: 128.87
 ---- batch: 040 ----
mean loss: 131.09
 ---- batch: 050 ----
mean loss: 129.33
 ---- batch: 060 ----
mean loss: 132.76
 ---- batch: 070 ----
mean loss: 120.47
 ---- batch: 080 ----
mean loss: 127.14
 ---- batch: 090 ----
mean loss: 132.28
 ---- batch: 100 ----
mean loss: 132.90
 ---- batch: 110 ----
mean loss: 128.54
train mean loss: 130.68
epoch train time: 0:00:00.557904
elapsed time: 0:02:46.888288
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:09:48.332555
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.36
 ---- batch: 020 ----
mean loss: 132.40
 ---- batch: 030 ----
mean loss: 130.17
 ---- batch: 040 ----
mean loss: 133.13
 ---- batch: 050 ----
mean loss: 132.01
 ---- batch: 060 ----
mean loss: 130.79
 ---- batch: 070 ----
mean loss: 138.11
 ---- batch: 080 ----
mean loss: 121.31
 ---- batch: 090 ----
mean loss: 122.86
 ---- batch: 100 ----
mean loss: 133.74
 ---- batch: 110 ----
mean loss: 128.09
train mean loss: 130.70
epoch train time: 0:00:00.564975
elapsed time: 0:02:47.453428
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:09:48.897709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.07
 ---- batch: 020 ----
mean loss: 132.44
 ---- batch: 030 ----
mean loss: 132.98
 ---- batch: 040 ----
mean loss: 132.76
 ---- batch: 050 ----
mean loss: 129.29
 ---- batch: 060 ----
mean loss: 140.38
 ---- batch: 070 ----
mean loss: 136.92
 ---- batch: 080 ----
mean loss: 134.09
 ---- batch: 090 ----
mean loss: 125.29
 ---- batch: 100 ----
mean loss: 129.00
 ---- batch: 110 ----
mean loss: 124.26
train mean loss: 130.73
epoch train time: 0:00:00.558766
elapsed time: 0:02:48.012346
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:09:49.456637
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.76
 ---- batch: 020 ----
mean loss: 127.22
 ---- batch: 030 ----
mean loss: 131.84
 ---- batch: 040 ----
mean loss: 126.41
 ---- batch: 050 ----
mean loss: 126.82
 ---- batch: 060 ----
mean loss: 128.45
 ---- batch: 070 ----
mean loss: 129.74
 ---- batch: 080 ----
mean loss: 133.33
 ---- batch: 090 ----
mean loss: 146.88
 ---- batch: 100 ----
mean loss: 124.42
 ---- batch: 110 ----
mean loss: 135.20
train mean loss: 130.73
epoch train time: 0:00:00.559559
elapsed time: 0:02:48.572083
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:09:50.016348
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.00
 ---- batch: 020 ----
mean loss: 123.38
 ---- batch: 030 ----
mean loss: 128.01
 ---- batch: 040 ----
mean loss: 131.59
 ---- batch: 050 ----
mean loss: 134.87
 ---- batch: 060 ----
mean loss: 134.23
 ---- batch: 070 ----
mean loss: 133.84
 ---- batch: 080 ----
mean loss: 127.54
 ---- batch: 090 ----
mean loss: 130.03
 ---- batch: 100 ----
mean loss: 139.09
 ---- batch: 110 ----
mean loss: 126.58
train mean loss: 130.47
epoch train time: 0:00:00.559244
elapsed time: 0:02:49.131463
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:09:50.575732
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.67
 ---- batch: 020 ----
mean loss: 136.47
 ---- batch: 030 ----
mean loss: 136.36
 ---- batch: 040 ----
mean loss: 123.22
 ---- batch: 050 ----
mean loss: 128.93
 ---- batch: 060 ----
mean loss: 136.06
 ---- batch: 070 ----
mean loss: 133.22
 ---- batch: 080 ----
mean loss: 119.36
 ---- batch: 090 ----
mean loss: 126.27
 ---- batch: 100 ----
mean loss: 137.41
 ---- batch: 110 ----
mean loss: 133.48
train mean loss: 130.80
epoch train time: 0:00:00.553286
elapsed time: 0:02:49.684890
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:09:51.129171
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.25
 ---- batch: 020 ----
mean loss: 138.84
 ---- batch: 030 ----
mean loss: 124.30
 ---- batch: 040 ----
mean loss: 136.81
 ---- batch: 050 ----
mean loss: 134.70
 ---- batch: 060 ----
mean loss: 126.42
 ---- batch: 070 ----
mean loss: 136.32
 ---- batch: 080 ----
mean loss: 123.45
 ---- batch: 090 ----
mean loss: 124.82
 ---- batch: 100 ----
mean loss: 135.48
 ---- batch: 110 ----
mean loss: 127.94
train mean loss: 130.60
epoch train time: 0:00:00.557470
elapsed time: 0:02:50.242513
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:09:51.686780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.85
 ---- batch: 020 ----
mean loss: 123.51
 ---- batch: 030 ----
mean loss: 129.02
 ---- batch: 040 ----
mean loss: 135.51
 ---- batch: 050 ----
mean loss: 130.71
 ---- batch: 060 ----
mean loss: 130.16
 ---- batch: 070 ----
mean loss: 135.73
 ---- batch: 080 ----
mean loss: 135.58
 ---- batch: 090 ----
mean loss: 127.29
 ---- batch: 100 ----
mean loss: 129.79
 ---- batch: 110 ----
mean loss: 135.48
train mean loss: 130.59
epoch train time: 0:00:00.557044
elapsed time: 0:02:50.799690
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:09:52.243965
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.76
 ---- batch: 020 ----
mean loss: 126.59
 ---- batch: 030 ----
mean loss: 137.17
 ---- batch: 040 ----
mean loss: 131.57
 ---- batch: 050 ----
mean loss: 123.87
 ---- batch: 060 ----
mean loss: 131.84
 ---- batch: 070 ----
mean loss: 134.87
 ---- batch: 080 ----
mean loss: 126.07
 ---- batch: 090 ----
mean loss: 126.91
 ---- batch: 100 ----
mean loss: 135.91
 ---- batch: 110 ----
mean loss: 136.37
train mean loss: 130.48
epoch train time: 0:00:00.555234
elapsed time: 0:02:51.355082
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:09:52.799349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.48
 ---- batch: 020 ----
mean loss: 131.15
 ---- batch: 030 ----
mean loss: 124.27
 ---- batch: 040 ----
mean loss: 126.96
 ---- batch: 050 ----
mean loss: 131.25
 ---- batch: 060 ----
mean loss: 134.44
 ---- batch: 070 ----
mean loss: 132.65
 ---- batch: 080 ----
mean loss: 128.22
 ---- batch: 090 ----
mean loss: 131.05
 ---- batch: 100 ----
mean loss: 133.26
 ---- batch: 110 ----
mean loss: 125.00
train mean loss: 130.51
epoch train time: 0:00:00.550346
elapsed time: 0:02:51.905578
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:09:53.349862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.95
 ---- batch: 020 ----
mean loss: 132.54
 ---- batch: 030 ----
mean loss: 133.02
 ---- batch: 040 ----
mean loss: 127.67
 ---- batch: 050 ----
mean loss: 128.74
 ---- batch: 060 ----
mean loss: 133.13
 ---- batch: 070 ----
mean loss: 131.38
 ---- batch: 080 ----
mean loss: 128.43
 ---- batch: 090 ----
mean loss: 129.70
 ---- batch: 100 ----
mean loss: 134.96
 ---- batch: 110 ----
mean loss: 134.10
train mean loss: 130.36
epoch train time: 0:00:00.548577
elapsed time: 0:02:52.454312
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:09:53.898579
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.77
 ---- batch: 020 ----
mean loss: 126.84
 ---- batch: 030 ----
mean loss: 126.53
 ---- batch: 040 ----
mean loss: 131.81
 ---- batch: 050 ----
mean loss: 128.72
 ---- batch: 060 ----
mean loss: 128.64
 ---- batch: 070 ----
mean loss: 127.31
 ---- batch: 080 ----
mean loss: 130.35
 ---- batch: 090 ----
mean loss: 136.06
 ---- batch: 100 ----
mean loss: 134.73
 ---- batch: 110 ----
mean loss: 131.84
train mean loss: 130.44
epoch train time: 0:00:00.560234
elapsed time: 0:02:53.014679
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:09:54.458953
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.00
 ---- batch: 020 ----
mean loss: 130.30
 ---- batch: 030 ----
mean loss: 126.95
 ---- batch: 040 ----
mean loss: 128.10
 ---- batch: 050 ----
mean loss: 131.62
 ---- batch: 060 ----
mean loss: 136.91
 ---- batch: 070 ----
mean loss: 124.07
 ---- batch: 080 ----
mean loss: 138.32
 ---- batch: 090 ----
mean loss: 134.61
 ---- batch: 100 ----
mean loss: 131.18
 ---- batch: 110 ----
mean loss: 127.16
train mean loss: 130.46
epoch train time: 0:00:00.557285
elapsed time: 0:02:53.575306
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_1/checkpoint.pth.tar
**** end time: 2019-09-27 02:09:55.019542 ****
