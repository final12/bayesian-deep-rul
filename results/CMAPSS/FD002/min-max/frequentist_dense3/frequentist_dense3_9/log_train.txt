Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_9', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24827
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistDense3...
Done.
**** start time: 2019-09-25 23:28:23.552379 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
            Linear-2                  [-1, 100]          48,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 68,100
Trainable params: 68,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 23:28:23.555985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4335.07
 ---- batch: 020 ----
mean loss: 4170.71
 ---- batch: 030 ----
mean loss: 4149.01
 ---- batch: 040 ----
mean loss: 4026.24
 ---- batch: 050 ----
mean loss: 3887.91
 ---- batch: 060 ----
mean loss: 3927.06
 ---- batch: 070 ----
mean loss: 3800.15
 ---- batch: 080 ----
mean loss: 3800.07
 ---- batch: 090 ----
mean loss: 3746.50
train mean loss: 3964.20
epoch train time: 0:00:33.650330
elapsed time: 0:00:33.656247
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 23:28:57.208667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3588.65
 ---- batch: 020 ----
mean loss: 3626.82
 ---- batch: 030 ----
mean loss: 3554.40
 ---- batch: 040 ----
mean loss: 3503.37
 ---- batch: 050 ----
mean loss: 3378.78
 ---- batch: 060 ----
mean loss: 3381.22
 ---- batch: 070 ----
mean loss: 3321.96
 ---- batch: 080 ----
mean loss: 3241.13
 ---- batch: 090 ----
mean loss: 3216.13
train mean loss: 3408.19
epoch train time: 0:00:00.499967
elapsed time: 0:00:34.156352
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 23:28:57.708784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3095.14
 ---- batch: 020 ----
mean loss: 3033.95
 ---- batch: 030 ----
mean loss: 3019.90
 ---- batch: 040 ----
mean loss: 2971.96
 ---- batch: 050 ----
mean loss: 2936.77
 ---- batch: 060 ----
mean loss: 2884.17
 ---- batch: 070 ----
mean loss: 2868.01
 ---- batch: 080 ----
mean loss: 2793.12
 ---- batch: 090 ----
mean loss: 2716.75
train mean loss: 2913.73
epoch train time: 0:00:00.498775
elapsed time: 0:00:34.655276
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 23:28:58.207708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2674.40
 ---- batch: 020 ----
mean loss: 2600.59
 ---- batch: 030 ----
mean loss: 2554.56
 ---- batch: 040 ----
mean loss: 2575.65
 ---- batch: 050 ----
mean loss: 2514.03
 ---- batch: 060 ----
mean loss: 2497.65
 ---- batch: 070 ----
mean loss: 2411.31
 ---- batch: 080 ----
mean loss: 2393.28
 ---- batch: 090 ----
mean loss: 2388.41
train mean loss: 2501.22
epoch train time: 0:00:00.494486
elapsed time: 0:00:35.149912
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 23:28:58.702345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2289.84
 ---- batch: 020 ----
mean loss: 2280.91
 ---- batch: 030 ----
mean loss: 2224.32
 ---- batch: 040 ----
mean loss: 2191.23
 ---- batch: 050 ----
mean loss: 2208.76
 ---- batch: 060 ----
mean loss: 2121.42
 ---- batch: 070 ----
mean loss: 2114.14
 ---- batch: 080 ----
mean loss: 2117.07
 ---- batch: 090 ----
mean loss: 2059.99
train mean loss: 2168.79
epoch train time: 0:00:00.495388
elapsed time: 0:00:35.645462
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 23:28:59.197899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1981.80
 ---- batch: 020 ----
mean loss: 1948.91
 ---- batch: 030 ----
mean loss: 1957.62
 ---- batch: 040 ----
mean loss: 1909.31
 ---- batch: 050 ----
mean loss: 1943.66
 ---- batch: 060 ----
mean loss: 1831.87
 ---- batch: 070 ----
mean loss: 1851.35
 ---- batch: 080 ----
mean loss: 1853.80
 ---- batch: 090 ----
mean loss: 1770.11
train mean loss: 1884.90
epoch train time: 0:00:00.498498
elapsed time: 0:00:36.144128
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 23:28:59.696551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1746.45
 ---- batch: 020 ----
mean loss: 1737.20
 ---- batch: 030 ----
mean loss: 1651.41
 ---- batch: 040 ----
mean loss: 1683.59
 ---- batch: 050 ----
mean loss: 1651.44
 ---- batch: 060 ----
mean loss: 1645.30
 ---- batch: 070 ----
mean loss: 1608.00
 ---- batch: 080 ----
mean loss: 1595.52
 ---- batch: 090 ----
mean loss: 1578.83
train mean loss: 1646.86
epoch train time: 0:00:00.492430
elapsed time: 0:00:36.636702
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 23:29:00.189128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1517.45
 ---- batch: 020 ----
mean loss: 1523.83
 ---- batch: 030 ----
mean loss: 1522.18
 ---- batch: 040 ----
mean loss: 1445.36
 ---- batch: 050 ----
mean loss: 1483.56
 ---- batch: 060 ----
mean loss: 1458.30
 ---- batch: 070 ----
mean loss: 1405.43
 ---- batch: 080 ----
mean loss: 1423.89
 ---- batch: 090 ----
mean loss: 1379.29
train mean loss: 1455.43
epoch train time: 0:00:00.486416
elapsed time: 0:00:37.123255
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 23:29:00.675684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1366.36
 ---- batch: 020 ----
mean loss: 1320.93
 ---- batch: 030 ----
mean loss: 1316.69
 ---- batch: 040 ----
mean loss: 1297.20
 ---- batch: 050 ----
mean loss: 1318.85
 ---- batch: 060 ----
mean loss: 1260.24
 ---- batch: 070 ----
mean loss: 1273.33
 ---- batch: 080 ----
mean loss: 1226.54
 ---- batch: 090 ----
mean loss: 1251.76
train mean loss: 1288.84
epoch train time: 0:00:00.501004
elapsed time: 0:00:37.624406
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 23:29:01.176851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1216.74
 ---- batch: 020 ----
mean loss: 1185.93
 ---- batch: 030 ----
mean loss: 1188.34
 ---- batch: 040 ----
mean loss: 1139.49
 ---- batch: 050 ----
mean loss: 1163.77
 ---- batch: 060 ----
mean loss: 1142.23
 ---- batch: 070 ----
mean loss: 1148.76
 ---- batch: 080 ----
mean loss: 1102.81
 ---- batch: 090 ----
mean loss: 1114.58
train mean loss: 1151.13
epoch train time: 0:00:00.499939
elapsed time: 0:00:38.124513
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 23:29:01.676934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1069.46
 ---- batch: 020 ----
mean loss: 1085.93
 ---- batch: 030 ----
mean loss: 1078.45
 ---- batch: 040 ----
mean loss: 1047.53
 ---- batch: 050 ----
mean loss: 1044.21
 ---- batch: 060 ----
mean loss: 1055.34
 ---- batch: 070 ----
mean loss: 1038.75
 ---- batch: 080 ----
mean loss: 1008.95
 ---- batch: 090 ----
mean loss: 1030.62
train mean loss: 1048.74
epoch train time: 0:00:00.501262
elapsed time: 0:00:38.625919
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 23:29:02.178339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 990.40
 ---- batch: 020 ----
mean loss: 1002.20
 ---- batch: 030 ----
mean loss: 1006.27
 ---- batch: 040 ----
mean loss: 997.47
 ---- batch: 050 ----
mean loss: 991.21
 ---- batch: 060 ----
mean loss: 984.45
 ---- batch: 070 ----
mean loss: 985.70
 ---- batch: 080 ----
mean loss: 958.25
 ---- batch: 090 ----
mean loss: 948.65
train mean loss: 981.89
epoch train time: 0:00:00.496892
elapsed time: 0:00:39.122948
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 23:29:02.675379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.91
 ---- batch: 020 ----
mean loss: 943.20
 ---- batch: 030 ----
mean loss: 955.47
 ---- batch: 040 ----
mean loss: 933.76
 ---- batch: 050 ----
mean loss: 960.73
 ---- batch: 060 ----
mean loss: 939.72
 ---- batch: 070 ----
mean loss: 921.87
 ---- batch: 080 ----
mean loss: 932.64
 ---- batch: 090 ----
mean loss: 934.40
train mean loss: 940.66
epoch train time: 0:00:00.517773
elapsed time: 0:00:39.640880
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 23:29:03.193328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.27
 ---- batch: 020 ----
mean loss: 920.34
 ---- batch: 030 ----
mean loss: 915.68
 ---- batch: 040 ----
mean loss: 895.97
 ---- batch: 050 ----
mean loss: 916.11
 ---- batch: 060 ----
mean loss: 909.86
 ---- batch: 070 ----
mean loss: 926.27
 ---- batch: 080 ----
mean loss: 910.40
 ---- batch: 090 ----
mean loss: 909.69
train mean loss: 913.81
epoch train time: 0:00:00.496261
elapsed time: 0:00:40.137326
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 23:29:03.689772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.43
 ---- batch: 020 ----
mean loss: 905.48
 ---- batch: 030 ----
mean loss: 900.31
 ---- batch: 040 ----
mean loss: 887.29
 ---- batch: 050 ----
mean loss: 892.61
 ---- batch: 060 ----
mean loss: 887.10
 ---- batch: 070 ----
mean loss: 890.33
 ---- batch: 080 ----
mean loss: 905.92
 ---- batch: 090 ----
mean loss: 897.68
train mean loss: 898.11
epoch train time: 0:00:00.498896
elapsed time: 0:00:40.636400
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 23:29:04.188828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.04
 ---- batch: 020 ----
mean loss: 890.99
 ---- batch: 030 ----
mean loss: 887.92
 ---- batch: 040 ----
mean loss: 898.76
 ---- batch: 050 ----
mean loss: 892.35
 ---- batch: 060 ----
mean loss: 880.63
 ---- batch: 070 ----
mean loss: 865.83
 ---- batch: 080 ----
mean loss: 886.19
 ---- batch: 090 ----
mean loss: 888.70
train mean loss: 887.72
epoch train time: 0:00:00.505935
elapsed time: 0:00:41.142498
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 23:29:04.694939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.89
 ---- batch: 020 ----
mean loss: 863.32
 ---- batch: 030 ----
mean loss: 880.02
 ---- batch: 040 ----
mean loss: 887.94
 ---- batch: 050 ----
mean loss: 864.01
 ---- batch: 060 ----
mean loss: 890.00
 ---- batch: 070 ----
mean loss: 896.12
 ---- batch: 080 ----
mean loss: 897.94
 ---- batch: 090 ----
mean loss: 867.96
train mean loss: 882.40
epoch train time: 0:00:00.500201
elapsed time: 0:00:41.642885
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 23:29:05.195326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.14
 ---- batch: 020 ----
mean loss: 874.46
 ---- batch: 030 ----
mean loss: 896.12
 ---- batch: 040 ----
mean loss: 886.88
 ---- batch: 050 ----
mean loss: 868.78
 ---- batch: 060 ----
mean loss: 862.93
 ---- batch: 070 ----
mean loss: 877.53
 ---- batch: 080 ----
mean loss: 874.26
 ---- batch: 090 ----
mean loss: 871.98
train mean loss: 878.66
epoch train time: 0:00:00.498838
elapsed time: 0:00:42.141889
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 23:29:05.694322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.77
 ---- batch: 020 ----
mean loss: 885.94
 ---- batch: 030 ----
mean loss: 863.14
 ---- batch: 040 ----
mean loss: 882.55
 ---- batch: 050 ----
mean loss: 879.82
 ---- batch: 060 ----
mean loss: 873.83
 ---- batch: 070 ----
mean loss: 859.06
 ---- batch: 080 ----
mean loss: 890.25
 ---- batch: 090 ----
mean loss: 884.69
train mean loss: 877.07
epoch train time: 0:00:00.509308
elapsed time: 0:00:42.651349
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 23:29:06.203795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.92
 ---- batch: 020 ----
mean loss: 888.57
 ---- batch: 030 ----
mean loss: 875.99
 ---- batch: 040 ----
mean loss: 883.62
 ---- batch: 050 ----
mean loss: 866.44
 ---- batch: 060 ----
mean loss: 878.05
 ---- batch: 070 ----
mean loss: 885.71
 ---- batch: 080 ----
mean loss: 863.47
 ---- batch: 090 ----
mean loss: 871.73
train mean loss: 876.35
epoch train time: 0:00:00.498743
elapsed time: 0:00:43.150264
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 23:29:06.702711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.49
 ---- batch: 020 ----
mean loss: 868.46
 ---- batch: 030 ----
mean loss: 867.63
 ---- batch: 040 ----
mean loss: 875.54
 ---- batch: 050 ----
mean loss: 894.08
 ---- batch: 060 ----
mean loss: 873.44
 ---- batch: 070 ----
mean loss: 857.42
 ---- batch: 080 ----
mean loss: 889.52
 ---- batch: 090 ----
mean loss: 879.11
train mean loss: 875.00
epoch train time: 0:00:00.507235
elapsed time: 0:00:43.657669
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 23:29:07.210100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.36
 ---- batch: 020 ----
mean loss: 887.95
 ---- batch: 030 ----
mean loss: 866.82
 ---- batch: 040 ----
mean loss: 856.49
 ---- batch: 050 ----
mean loss: 868.00
 ---- batch: 060 ----
mean loss: 891.02
 ---- batch: 070 ----
mean loss: 882.10
 ---- batch: 080 ----
mean loss: 871.12
 ---- batch: 090 ----
mean loss: 876.50
train mean loss: 875.82
epoch train time: 0:00:00.503946
elapsed time: 0:00:44.161765
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 23:29:07.714196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.98
 ---- batch: 020 ----
mean loss: 870.45
 ---- batch: 030 ----
mean loss: 860.92
 ---- batch: 040 ----
mean loss: 868.39
 ---- batch: 050 ----
mean loss: 878.78
 ---- batch: 060 ----
mean loss: 879.31
 ---- batch: 070 ----
mean loss: 875.76
 ---- batch: 080 ----
mean loss: 878.35
 ---- batch: 090 ----
mean loss: 885.03
train mean loss: 875.05
epoch train time: 0:00:00.505405
elapsed time: 0:00:44.667371
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 23:29:08.219805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.59
 ---- batch: 020 ----
mean loss: 871.05
 ---- batch: 030 ----
mean loss: 877.11
 ---- batch: 040 ----
mean loss: 857.87
 ---- batch: 050 ----
mean loss: 872.97
 ---- batch: 060 ----
mean loss: 865.49
 ---- batch: 070 ----
mean loss: 881.41
 ---- batch: 080 ----
mean loss: 881.56
 ---- batch: 090 ----
mean loss: 873.43
train mean loss: 876.23
epoch train time: 0:00:00.495743
elapsed time: 0:00:45.163279
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 23:29:08.715717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.73
 ---- batch: 020 ----
mean loss: 888.97
 ---- batch: 030 ----
mean loss: 883.41
 ---- batch: 040 ----
mean loss: 876.11
 ---- batch: 050 ----
mean loss: 881.02
 ---- batch: 060 ----
mean loss: 877.85
 ---- batch: 070 ----
mean loss: 873.89
 ---- batch: 080 ----
mean loss: 865.54
 ---- batch: 090 ----
mean loss: 883.91
train mean loss: 875.19
epoch train time: 0:00:00.497162
elapsed time: 0:00:45.660590
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 23:29:09.213018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.50
 ---- batch: 020 ----
mean loss: 869.74
 ---- batch: 030 ----
mean loss: 863.25
 ---- batch: 040 ----
mean loss: 866.19
 ---- batch: 050 ----
mean loss: 864.48
 ---- batch: 060 ----
mean loss: 902.41
 ---- batch: 070 ----
mean loss: 883.60
 ---- batch: 080 ----
mean loss: 878.41
 ---- batch: 090 ----
mean loss: 869.08
train mean loss: 874.90
epoch train time: 0:00:00.500978
elapsed time: 0:00:46.161719
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 23:29:09.714149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.27
 ---- batch: 020 ----
mean loss: 873.49
 ---- batch: 030 ----
mean loss: 871.79
 ---- batch: 040 ----
mean loss: 872.86
 ---- batch: 050 ----
mean loss: 864.73
 ---- batch: 060 ----
mean loss: 869.84
 ---- batch: 070 ----
mean loss: 879.61
 ---- batch: 080 ----
mean loss: 887.26
 ---- batch: 090 ----
mean loss: 888.06
train mean loss: 875.11
epoch train time: 0:00:00.493071
elapsed time: 0:00:46.654938
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 23:29:10.207356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.94
 ---- batch: 020 ----
mean loss: 869.18
 ---- batch: 030 ----
mean loss: 878.10
 ---- batch: 040 ----
mean loss: 885.39
 ---- batch: 050 ----
mean loss: 873.31
 ---- batch: 060 ----
mean loss: 871.07
 ---- batch: 070 ----
mean loss: 862.61
 ---- batch: 080 ----
mean loss: 892.13
 ---- batch: 090 ----
mean loss: 862.39
train mean loss: 874.89
epoch train time: 0:00:00.490703
elapsed time: 0:00:47.145782
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 23:29:10.698232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.86
 ---- batch: 020 ----
mean loss: 874.97
 ---- batch: 030 ----
mean loss: 861.88
 ---- batch: 040 ----
mean loss: 874.73
 ---- batch: 050 ----
mean loss: 883.70
 ---- batch: 060 ----
mean loss: 883.33
 ---- batch: 070 ----
mean loss: 889.99
 ---- batch: 080 ----
mean loss: 863.96
 ---- batch: 090 ----
mean loss: 859.30
train mean loss: 875.83
epoch train time: 0:00:00.501969
elapsed time: 0:00:47.647915
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 23:29:11.200345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.98
 ---- batch: 020 ----
mean loss: 886.47
 ---- batch: 030 ----
mean loss: 870.55
 ---- batch: 040 ----
mean loss: 876.61
 ---- batch: 050 ----
mean loss: 875.43
 ---- batch: 060 ----
mean loss: 878.11
 ---- batch: 070 ----
mean loss: 872.73
 ---- batch: 080 ----
mean loss: 870.15
 ---- batch: 090 ----
mean loss: 857.33
train mean loss: 874.97
epoch train time: 0:00:00.501467
elapsed time: 0:00:48.149534
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 23:29:11.701967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.88
 ---- batch: 020 ----
mean loss: 871.10
 ---- batch: 030 ----
mean loss: 868.37
 ---- batch: 040 ----
mean loss: 878.46
 ---- batch: 050 ----
mean loss: 892.61
 ---- batch: 060 ----
mean loss: 866.61
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 868.92
 ---- batch: 090 ----
mean loss: 861.10
train mean loss: 875.07
epoch train time: 0:00:00.514835
elapsed time: 0:00:48.664512
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 23:29:12.216946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.11
 ---- batch: 020 ----
mean loss: 879.14
 ---- batch: 030 ----
mean loss: 874.04
 ---- batch: 040 ----
mean loss: 868.00
 ---- batch: 050 ----
mean loss: 876.37
 ---- batch: 060 ----
mean loss: 886.70
 ---- batch: 070 ----
mean loss: 861.68
 ---- batch: 080 ----
mean loss: 882.78
 ---- batch: 090 ----
mean loss: 870.74
train mean loss: 874.91
epoch train time: 0:00:00.504421
elapsed time: 0:00:49.169100
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 23:29:12.721558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.46
 ---- batch: 020 ----
mean loss: 876.58
 ---- batch: 030 ----
mean loss: 877.96
 ---- batch: 040 ----
mean loss: 870.24
 ---- batch: 050 ----
mean loss: 869.36
 ---- batch: 060 ----
mean loss: 868.26
 ---- batch: 070 ----
mean loss: 868.20
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 877.78
train mean loss: 876.12
epoch train time: 0:00:00.494705
elapsed time: 0:00:49.663978
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 23:29:13.216406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.88
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 866.29
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 889.38
 ---- batch: 060 ----
mean loss: 879.60
 ---- batch: 070 ----
mean loss: 884.78
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 873.48
train mean loss: 875.11
epoch train time: 0:00:00.492290
elapsed time: 0:00:50.156411
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 23:29:13.708842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.81
 ---- batch: 020 ----
mean loss: 872.41
 ---- batch: 030 ----
mean loss: 858.52
 ---- batch: 040 ----
mean loss: 878.56
 ---- batch: 050 ----
mean loss: 870.22
 ---- batch: 060 ----
mean loss: 889.60
 ---- batch: 070 ----
mean loss: 878.83
 ---- batch: 080 ----
mean loss: 875.94
 ---- batch: 090 ----
mean loss: 887.80
train mean loss: 875.70
epoch train time: 0:00:00.496453
elapsed time: 0:00:50.653021
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 23:29:14.205449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 870.46
 ---- batch: 030 ----
mean loss: 875.57
 ---- batch: 040 ----
mean loss: 886.46
 ---- batch: 050 ----
mean loss: 888.80
 ---- batch: 060 ----
mean loss: 856.43
 ---- batch: 070 ----
mean loss: 861.54
 ---- batch: 080 ----
mean loss: 865.08
 ---- batch: 090 ----
mean loss: 908.21
train mean loss: 875.33
epoch train time: 0:00:00.494297
elapsed time: 0:00:51.147465
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 23:29:14.699895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.66
 ---- batch: 020 ----
mean loss: 867.37
 ---- batch: 030 ----
mean loss: 881.86
 ---- batch: 040 ----
mean loss: 896.14
 ---- batch: 050 ----
mean loss: 895.66
 ---- batch: 060 ----
mean loss: 872.61
 ---- batch: 070 ----
mean loss: 871.27
 ---- batch: 080 ----
mean loss: 850.89
 ---- batch: 090 ----
mean loss: 868.18
train mean loss: 874.80
epoch train time: 0:00:00.510484
elapsed time: 0:00:51.658094
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 23:29:15.210538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 879.71
 ---- batch: 040 ----
mean loss: 880.95
 ---- batch: 050 ----
mean loss: 865.29
 ---- batch: 060 ----
mean loss: 882.77
 ---- batch: 070 ----
mean loss: 869.99
 ---- batch: 080 ----
mean loss: 882.00
 ---- batch: 090 ----
mean loss: 857.67
train mean loss: 875.43
epoch train time: 0:00:00.499313
elapsed time: 0:00:52.157606
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 23:29:15.710037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 874.52
 ---- batch: 030 ----
mean loss: 869.88
 ---- batch: 040 ----
mean loss: 872.46
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 884.13
 ---- batch: 070 ----
mean loss: 882.33
 ---- batch: 080 ----
mean loss: 868.00
 ---- batch: 090 ----
mean loss: 884.71
train mean loss: 875.94
epoch train time: 0:00:00.501252
elapsed time: 0:00:52.659005
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 23:29:16.211446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.36
 ---- batch: 020 ----
mean loss: 863.17
 ---- batch: 030 ----
mean loss: 868.58
 ---- batch: 040 ----
mean loss: 872.91
 ---- batch: 050 ----
mean loss: 875.23
 ---- batch: 060 ----
mean loss: 874.13
 ---- batch: 070 ----
mean loss: 875.79
 ---- batch: 080 ----
mean loss: 872.87
 ---- batch: 090 ----
mean loss: 880.58
train mean loss: 875.20
epoch train time: 0:00:00.493277
elapsed time: 0:00:53.152439
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 23:29:16.704868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
 ---- batch: 020 ----
mean loss: 888.19
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 864.90
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 877.49
 ---- batch: 070 ----
mean loss: 873.63
 ---- batch: 080 ----
mean loss: 861.95
 ---- batch: 090 ----
mean loss: 867.30
train mean loss: 874.55
epoch train time: 0:00:00.514991
elapsed time: 0:00:53.667574
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 23:29:17.220005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.19
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 878.84
 ---- batch: 040 ----
mean loss: 868.87
 ---- batch: 050 ----
mean loss: 860.44
 ---- batch: 060 ----
mean loss: 885.18
 ---- batch: 070 ----
mean loss: 876.98
 ---- batch: 080 ----
mean loss: 875.13
 ---- batch: 090 ----
mean loss: 883.68
train mean loss: 875.45
epoch train time: 0:00:00.510365
elapsed time: 0:00:54.178095
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 23:29:17.730537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.89
 ---- batch: 020 ----
mean loss: 862.14
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 852.48
 ---- batch: 060 ----
mean loss: 873.50
 ---- batch: 070 ----
mean loss: 889.67
 ---- batch: 080 ----
mean loss: 893.95
 ---- batch: 090 ----
mean loss: 900.18
train mean loss: 875.66
epoch train time: 0:00:00.503246
elapsed time: 0:00:54.681501
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 23:29:18.233955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.25
 ---- batch: 020 ----
mean loss: 859.42
 ---- batch: 030 ----
mean loss: 880.51
 ---- batch: 040 ----
mean loss: 889.92
 ---- batch: 050 ----
mean loss: 869.13
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 886.11
 ---- batch: 090 ----
mean loss: 885.30
train mean loss: 875.24
epoch train time: 0:00:00.509127
elapsed time: 0:00:55.190803
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 23:29:18.743291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.92
 ---- batch: 020 ----
mean loss: 896.00
 ---- batch: 030 ----
mean loss: 895.89
 ---- batch: 040 ----
mean loss: 875.86
 ---- batch: 050 ----
mean loss: 854.48
 ---- batch: 060 ----
mean loss: 885.08
 ---- batch: 070 ----
mean loss: 872.57
 ---- batch: 080 ----
mean loss: 874.54
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 875.66
epoch train time: 0:00:00.504842
elapsed time: 0:00:55.695849
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 23:29:19.248282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.48
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 875.70
 ---- batch: 040 ----
mean loss: 877.24
 ---- batch: 050 ----
mean loss: 878.31
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.63
 ---- batch: 080 ----
mean loss: 876.96
 ---- batch: 090 ----
mean loss: 864.58
train mean loss: 874.88
epoch train time: 0:00:00.505664
elapsed time: 0:00:56.201664
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 23:29:19.754097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 874.57
 ---- batch: 030 ----
mean loss: 885.12
 ---- batch: 040 ----
mean loss: 870.30
 ---- batch: 050 ----
mean loss: 885.77
 ---- batch: 060 ----
mean loss: 877.45
 ---- batch: 070 ----
mean loss: 893.99
 ---- batch: 080 ----
mean loss: 865.69
 ---- batch: 090 ----
mean loss: 871.94
train mean loss: 875.10
epoch train time: 0:00:00.504551
elapsed time: 0:00:56.706366
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 23:29:20.258809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.71
 ---- batch: 020 ----
mean loss: 867.95
 ---- batch: 030 ----
mean loss: 868.71
 ---- batch: 040 ----
mean loss: 880.39
 ---- batch: 050 ----
mean loss: 874.64
 ---- batch: 060 ----
mean loss: 876.82
 ---- batch: 070 ----
mean loss: 871.95
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 896.57
train mean loss: 874.86
epoch train time: 0:00:00.503666
elapsed time: 0:00:57.210191
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 23:29:20.762621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.54
 ---- batch: 020 ----
mean loss: 874.88
 ---- batch: 030 ----
mean loss: 842.41
 ---- batch: 040 ----
mean loss: 870.90
 ---- batch: 050 ----
mean loss: 879.94
 ---- batch: 060 ----
mean loss: 870.33
 ---- batch: 070 ----
mean loss: 893.30
 ---- batch: 080 ----
mean loss: 874.30
 ---- batch: 090 ----
mean loss: 901.59
train mean loss: 874.94
epoch train time: 0:00:00.494601
elapsed time: 0:00:57.704956
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 23:29:21.257388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.79
 ---- batch: 020 ----
mean loss: 885.95
 ---- batch: 030 ----
mean loss: 869.68
 ---- batch: 040 ----
mean loss: 881.87
 ---- batch: 050 ----
mean loss: 866.18
 ---- batch: 060 ----
mean loss: 869.62
 ---- batch: 070 ----
mean loss: 865.49
 ---- batch: 080 ----
mean loss: 885.13
 ---- batch: 090 ----
mean loss: 878.95
train mean loss: 874.97
epoch train time: 0:00:00.499095
elapsed time: 0:00:58.204245
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 23:29:21.756681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.61
 ---- batch: 020 ----
mean loss: 893.23
 ---- batch: 030 ----
mean loss: 872.99
 ---- batch: 040 ----
mean loss: 888.75
 ---- batch: 050 ----
mean loss: 867.76
 ---- batch: 060 ----
mean loss: 869.11
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 858.03
 ---- batch: 090 ----
mean loss: 875.46
train mean loss: 875.16
epoch train time: 0:00:00.494392
elapsed time: 0:00:58.698784
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 23:29:22.251209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.39
 ---- batch: 020 ----
mean loss: 873.36
 ---- batch: 030 ----
mean loss: 870.53
 ---- batch: 040 ----
mean loss: 862.72
 ---- batch: 050 ----
mean loss: 871.25
 ---- batch: 060 ----
mean loss: 875.47
 ---- batch: 070 ----
mean loss: 898.28
 ---- batch: 080 ----
mean loss: 873.97
 ---- batch: 090 ----
mean loss: 885.21
train mean loss: 874.91
epoch train time: 0:00:00.494975
elapsed time: 0:00:59.193906
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 23:29:22.746352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.25
 ---- batch: 020 ----
mean loss: 819.27
 ---- batch: 030 ----
mean loss: 786.25
 ---- batch: 040 ----
mean loss: 759.47
 ---- batch: 050 ----
mean loss: 725.86
 ---- batch: 060 ----
mean loss: 704.56
 ---- batch: 070 ----
mean loss: 664.11
 ---- batch: 080 ----
mean loss: 617.23
 ---- batch: 090 ----
mean loss: 541.73
train mean loss: 705.60
epoch train time: 0:00:00.498184
elapsed time: 0:00:59.692255
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 23:29:23.244684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.76
 ---- batch: 020 ----
mean loss: 427.25
 ---- batch: 030 ----
mean loss: 421.27
 ---- batch: 040 ----
mean loss: 396.18
 ---- batch: 050 ----
mean loss: 392.58
 ---- batch: 060 ----
mean loss: 359.26
 ---- batch: 070 ----
mean loss: 353.74
 ---- batch: 080 ----
mean loss: 357.42
 ---- batch: 090 ----
mean loss: 336.04
train mean loss: 385.86
epoch train time: 0:00:00.496859
elapsed time: 0:01:00.189305
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 23:29:23.741746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.37
 ---- batch: 020 ----
mean loss: 330.26
 ---- batch: 030 ----
mean loss: 317.29
 ---- batch: 040 ----
mean loss: 304.24
 ---- batch: 050 ----
mean loss: 299.39
 ---- batch: 060 ----
mean loss: 302.31
 ---- batch: 070 ----
mean loss: 307.17
 ---- batch: 080 ----
mean loss: 286.73
 ---- batch: 090 ----
mean loss: 297.49
train mean loss: 308.66
epoch train time: 0:00:00.494884
elapsed time: 0:01:00.684346
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 23:29:24.236789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.35
 ---- batch: 020 ----
mean loss: 272.36
 ---- batch: 030 ----
mean loss: 274.97
 ---- batch: 040 ----
mean loss: 273.81
 ---- batch: 050 ----
mean loss: 272.31
 ---- batch: 060 ----
mean loss: 272.45
 ---- batch: 070 ----
mean loss: 277.23
 ---- batch: 080 ----
mean loss: 254.70
 ---- batch: 090 ----
mean loss: 255.76
train mean loss: 271.25
epoch train time: 0:00:00.493144
elapsed time: 0:01:01.177653
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 23:29:24.730085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.89
 ---- batch: 020 ----
mean loss: 251.33
 ---- batch: 030 ----
mean loss: 256.03
 ---- batch: 040 ----
mean loss: 256.85
 ---- batch: 050 ----
mean loss: 251.55
 ---- batch: 060 ----
mean loss: 251.06
 ---- batch: 070 ----
mean loss: 254.53
 ---- batch: 080 ----
mean loss: 250.02
 ---- batch: 090 ----
mean loss: 252.32
train mean loss: 253.07
epoch train time: 0:00:00.494144
elapsed time: 0:01:01.671989
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 23:29:25.224417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.97
 ---- batch: 020 ----
mean loss: 244.32
 ---- batch: 030 ----
mean loss: 242.86
 ---- batch: 040 ----
mean loss: 240.90
 ---- batch: 050 ----
mean loss: 245.39
 ---- batch: 060 ----
mean loss: 246.17
 ---- batch: 070 ----
mean loss: 235.52
 ---- batch: 080 ----
mean loss: 236.03
 ---- batch: 090 ----
mean loss: 242.34
train mean loss: 241.43
epoch train time: 0:00:00.489159
elapsed time: 0:01:02.161313
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 23:29:25.713763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.15
 ---- batch: 020 ----
mean loss: 229.17
 ---- batch: 030 ----
mean loss: 234.05
 ---- batch: 040 ----
mean loss: 238.95
 ---- batch: 050 ----
mean loss: 232.57
 ---- batch: 060 ----
mean loss: 234.74
 ---- batch: 070 ----
mean loss: 232.39
 ---- batch: 080 ----
mean loss: 237.45
 ---- batch: 090 ----
mean loss: 238.28
train mean loss: 235.01
epoch train time: 0:00:00.495421
elapsed time: 0:01:02.656898
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 23:29:26.209325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.50
 ---- batch: 020 ----
mean loss: 236.90
 ---- batch: 030 ----
mean loss: 224.99
 ---- batch: 040 ----
mean loss: 228.32
 ---- batch: 050 ----
mean loss: 231.26
 ---- batch: 060 ----
mean loss: 232.54
 ---- batch: 070 ----
mean loss: 228.15
 ---- batch: 080 ----
mean loss: 225.42
 ---- batch: 090 ----
mean loss: 229.58
train mean loss: 229.48
epoch train time: 0:00:00.506275
elapsed time: 0:01:03.163317
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 23:29:26.715749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.15
 ---- batch: 020 ----
mean loss: 227.36
 ---- batch: 030 ----
mean loss: 224.09
 ---- batch: 040 ----
mean loss: 224.25
 ---- batch: 050 ----
mean loss: 215.90
 ---- batch: 060 ----
mean loss: 222.19
 ---- batch: 070 ----
mean loss: 228.10
 ---- batch: 080 ----
mean loss: 226.15
 ---- batch: 090 ----
mean loss: 225.11
train mean loss: 223.91
epoch train time: 0:00:00.500740
elapsed time: 0:01:03.664209
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 23:29:27.216645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.95
 ---- batch: 020 ----
mean loss: 217.99
 ---- batch: 030 ----
mean loss: 213.22
 ---- batch: 040 ----
mean loss: 215.39
 ---- batch: 050 ----
mean loss: 221.22
 ---- batch: 060 ----
mean loss: 229.12
 ---- batch: 070 ----
mean loss: 218.65
 ---- batch: 080 ----
mean loss: 215.71
 ---- batch: 090 ----
mean loss: 229.37
train mean loss: 220.34
epoch train time: 0:00:00.495975
elapsed time: 0:01:04.160355
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 23:29:27.712786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.68
 ---- batch: 020 ----
mean loss: 209.74
 ---- batch: 030 ----
mean loss: 218.71
 ---- batch: 040 ----
mean loss: 208.62
 ---- batch: 050 ----
mean loss: 217.11
 ---- batch: 060 ----
mean loss: 222.78
 ---- batch: 070 ----
mean loss: 223.49
 ---- batch: 080 ----
mean loss: 221.19
 ---- batch: 090 ----
mean loss: 217.41
train mean loss: 217.29
epoch train time: 0:00:00.503078
elapsed time: 0:01:04.663581
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 23:29:28.216011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.01
 ---- batch: 020 ----
mean loss: 215.48
 ---- batch: 030 ----
mean loss: 212.30
 ---- batch: 040 ----
mean loss: 214.90
 ---- batch: 050 ----
mean loss: 225.03
 ---- batch: 060 ----
mean loss: 212.01
 ---- batch: 070 ----
mean loss: 210.55
 ---- batch: 080 ----
mean loss: 212.46
 ---- batch: 090 ----
mean loss: 209.30
train mean loss: 213.58
epoch train time: 0:00:00.493147
elapsed time: 0:01:05.156872
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 23:29:28.709318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.11
 ---- batch: 020 ----
mean loss: 207.70
 ---- batch: 030 ----
mean loss: 216.76
 ---- batch: 040 ----
mean loss: 215.27
 ---- batch: 050 ----
mean loss: 217.40
 ---- batch: 060 ----
mean loss: 212.97
 ---- batch: 070 ----
mean loss: 205.09
 ---- batch: 080 ----
mean loss: 208.66
 ---- batch: 090 ----
mean loss: 208.63
train mean loss: 211.74
epoch train time: 0:00:00.500493
elapsed time: 0:01:05.657526
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 23:29:29.209988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.25
 ---- batch: 020 ----
mean loss: 205.61
 ---- batch: 030 ----
mean loss: 211.58
 ---- batch: 040 ----
mean loss: 204.79
 ---- batch: 050 ----
mean loss: 208.83
 ---- batch: 060 ----
mean loss: 211.79
 ---- batch: 070 ----
mean loss: 207.62
 ---- batch: 080 ----
mean loss: 215.95
 ---- batch: 090 ----
mean loss: 205.37
train mean loss: 208.20
epoch train time: 0:00:00.497756
elapsed time: 0:01:06.155473
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 23:29:29.707903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.61
 ---- batch: 020 ----
mean loss: 199.41
 ---- batch: 030 ----
mean loss: 214.21
 ---- batch: 040 ----
mean loss: 213.42
 ---- batch: 050 ----
mean loss: 206.28
 ---- batch: 060 ----
mean loss: 216.51
 ---- batch: 070 ----
mean loss: 202.41
 ---- batch: 080 ----
mean loss: 206.22
 ---- batch: 090 ----
mean loss: 199.01
train mean loss: 206.42
epoch train time: 0:00:00.509822
elapsed time: 0:01:06.665441
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 23:29:30.217869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.84
 ---- batch: 020 ----
mean loss: 204.28
 ---- batch: 030 ----
mean loss: 196.73
 ---- batch: 040 ----
mean loss: 197.51
 ---- batch: 050 ----
mean loss: 198.52
 ---- batch: 060 ----
mean loss: 198.48
 ---- batch: 070 ----
mean loss: 206.85
 ---- batch: 080 ----
mean loss: 206.56
 ---- batch: 090 ----
mean loss: 199.33
train mean loss: 201.95
epoch train time: 0:00:00.496641
elapsed time: 0:01:07.162227
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 23:29:30.714659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.92
 ---- batch: 020 ----
mean loss: 207.06
 ---- batch: 030 ----
mean loss: 191.43
 ---- batch: 040 ----
mean loss: 199.02
 ---- batch: 050 ----
mean loss: 195.63
 ---- batch: 060 ----
mean loss: 199.12
 ---- batch: 070 ----
mean loss: 209.08
 ---- batch: 080 ----
mean loss: 198.30
 ---- batch: 090 ----
mean loss: 203.30
train mean loss: 200.51
epoch train time: 0:00:00.501673
elapsed time: 0:01:07.664045
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 23:29:31.216473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.54
 ---- batch: 020 ----
mean loss: 191.51
 ---- batch: 030 ----
mean loss: 197.15
 ---- batch: 040 ----
mean loss: 197.50
 ---- batch: 050 ----
mean loss: 197.17
 ---- batch: 060 ----
mean loss: 192.46
 ---- batch: 070 ----
mean loss: 190.70
 ---- batch: 080 ----
mean loss: 205.73
 ---- batch: 090 ----
mean loss: 205.11
train mean loss: 198.62
epoch train time: 0:00:00.492719
elapsed time: 0:01:08.156909
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 23:29:31.709353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.19
 ---- batch: 020 ----
mean loss: 188.14
 ---- batch: 030 ----
mean loss: 195.75
 ---- batch: 040 ----
mean loss: 203.16
 ---- batch: 050 ----
mean loss: 198.57
 ---- batch: 060 ----
mean loss: 192.75
 ---- batch: 070 ----
mean loss: 197.85
 ---- batch: 080 ----
mean loss: 197.45
 ---- batch: 090 ----
mean loss: 198.90
train mean loss: 195.71
epoch train time: 0:00:00.514595
elapsed time: 0:01:08.671664
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 23:29:32.224093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.42
 ---- batch: 020 ----
mean loss: 189.64
 ---- batch: 030 ----
mean loss: 192.11
 ---- batch: 040 ----
mean loss: 197.00
 ---- batch: 050 ----
mean loss: 197.46
 ---- batch: 060 ----
mean loss: 199.00
 ---- batch: 070 ----
mean loss: 194.24
 ---- batch: 080 ----
mean loss: 199.82
 ---- batch: 090 ----
mean loss: 199.12
train mean loss: 195.25
epoch train time: 0:00:00.506649
elapsed time: 0:01:09.178484
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 23:29:32.730974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.68
 ---- batch: 020 ----
mean loss: 178.31
 ---- batch: 030 ----
mean loss: 195.34
 ---- batch: 040 ----
mean loss: 189.68
 ---- batch: 050 ----
mean loss: 199.45
 ---- batch: 060 ----
mean loss: 195.00
 ---- batch: 070 ----
mean loss: 195.11
 ---- batch: 080 ----
mean loss: 198.81
 ---- batch: 090 ----
mean loss: 195.46
train mean loss: 192.13
epoch train time: 0:00:00.498287
elapsed time: 0:01:09.676981
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 23:29:33.229437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.59
 ---- batch: 020 ----
mean loss: 189.88
 ---- batch: 030 ----
mean loss: 197.08
 ---- batch: 040 ----
mean loss: 187.37
 ---- batch: 050 ----
mean loss: 189.05
 ---- batch: 060 ----
mean loss: 195.54
 ---- batch: 070 ----
mean loss: 187.57
 ---- batch: 080 ----
mean loss: 197.15
 ---- batch: 090 ----
mean loss: 193.15
train mean loss: 191.28
epoch train time: 0:00:00.499702
elapsed time: 0:01:10.176887
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 23:29:33.729330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.88
 ---- batch: 020 ----
mean loss: 188.17
 ---- batch: 030 ----
mean loss: 187.10
 ---- batch: 040 ----
mean loss: 192.35
 ---- batch: 050 ----
mean loss: 189.89
 ---- batch: 060 ----
mean loss: 191.02
 ---- batch: 070 ----
mean loss: 185.31
 ---- batch: 080 ----
mean loss: 186.26
 ---- batch: 090 ----
mean loss: 194.03
train mean loss: 189.73
epoch train time: 0:00:00.506356
elapsed time: 0:01:10.683403
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 23:29:34.235834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.47
 ---- batch: 020 ----
mean loss: 188.68
 ---- batch: 030 ----
mean loss: 189.20
 ---- batch: 040 ----
mean loss: 193.34
 ---- batch: 050 ----
mean loss: 187.38
 ---- batch: 060 ----
mean loss: 185.16
 ---- batch: 070 ----
mean loss: 193.16
 ---- batch: 080 ----
mean loss: 190.85
 ---- batch: 090 ----
mean loss: 190.41
train mean loss: 188.78
epoch train time: 0:00:00.501544
elapsed time: 0:01:11.185099
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 23:29:34.737545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.16
 ---- batch: 020 ----
mean loss: 184.54
 ---- batch: 030 ----
mean loss: 176.28
 ---- batch: 040 ----
mean loss: 185.45
 ---- batch: 050 ----
mean loss: 192.81
 ---- batch: 060 ----
mean loss: 186.01
 ---- batch: 070 ----
mean loss: 191.64
 ---- batch: 080 ----
mean loss: 193.11
 ---- batch: 090 ----
mean loss: 189.10
train mean loss: 186.87
epoch train time: 0:00:00.507534
elapsed time: 0:01:11.692830
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 23:29:35.245268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.04
 ---- batch: 020 ----
mean loss: 184.78
 ---- batch: 030 ----
mean loss: 183.85
 ---- batch: 040 ----
mean loss: 193.50
 ---- batch: 050 ----
mean loss: 175.83
 ---- batch: 060 ----
mean loss: 184.03
 ---- batch: 070 ----
mean loss: 189.88
 ---- batch: 080 ----
mean loss: 185.59
 ---- batch: 090 ----
mean loss: 187.94
train mean loss: 184.60
epoch train time: 0:00:00.495065
elapsed time: 0:01:12.188056
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 23:29:35.740490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.58
 ---- batch: 020 ----
mean loss: 182.13
 ---- batch: 030 ----
mean loss: 176.72
 ---- batch: 040 ----
mean loss: 191.18
 ---- batch: 050 ----
mean loss: 185.29
 ---- batch: 060 ----
mean loss: 180.61
 ---- batch: 070 ----
mean loss: 195.66
 ---- batch: 080 ----
mean loss: 187.57
 ---- batch: 090 ----
mean loss: 186.16
train mean loss: 183.97
epoch train time: 0:00:00.503568
elapsed time: 0:01:12.691771
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 23:29:36.244200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.88
 ---- batch: 020 ----
mean loss: 178.63
 ---- batch: 030 ----
mean loss: 182.12
 ---- batch: 040 ----
mean loss: 192.11
 ---- batch: 050 ----
mean loss: 184.21
 ---- batch: 060 ----
mean loss: 189.35
 ---- batch: 070 ----
mean loss: 176.44
 ---- batch: 080 ----
mean loss: 188.58
 ---- batch: 090 ----
mean loss: 186.23
train mean loss: 183.34
epoch train time: 0:00:00.504707
elapsed time: 0:01:13.196623
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 23:29:36.749070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.11
 ---- batch: 020 ----
mean loss: 175.09
 ---- batch: 030 ----
mean loss: 177.60
 ---- batch: 040 ----
mean loss: 175.36
 ---- batch: 050 ----
mean loss: 186.11
 ---- batch: 060 ----
mean loss: 182.13
 ---- batch: 070 ----
mean loss: 184.57
 ---- batch: 080 ----
mean loss: 180.42
 ---- batch: 090 ----
mean loss: 188.61
train mean loss: 181.64
epoch train time: 0:00:00.506393
elapsed time: 0:01:13.703193
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 23:29:37.255623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.10
 ---- batch: 020 ----
mean loss: 180.41
 ---- batch: 030 ----
mean loss: 188.38
 ---- batch: 040 ----
mean loss: 180.74
 ---- batch: 050 ----
mean loss: 183.11
 ---- batch: 060 ----
mean loss: 174.79
 ---- batch: 070 ----
mean loss: 179.67
 ---- batch: 080 ----
mean loss: 178.70
 ---- batch: 090 ----
mean loss: 179.49
train mean loss: 179.75
epoch train time: 0:00:00.496719
elapsed time: 0:01:14.200082
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 23:29:37.752514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.07
 ---- batch: 020 ----
mean loss: 174.53
 ---- batch: 030 ----
mean loss: 173.37
 ---- batch: 040 ----
mean loss: 179.74
 ---- batch: 050 ----
mean loss: 189.21
 ---- batch: 060 ----
mean loss: 184.58
 ---- batch: 070 ----
mean loss: 174.85
 ---- batch: 080 ----
mean loss: 185.23
 ---- batch: 090 ----
mean loss: 176.45
train mean loss: 179.78
epoch train time: 0:00:00.501062
elapsed time: 0:01:14.701343
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 23:29:38.253781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.91
 ---- batch: 020 ----
mean loss: 169.66
 ---- batch: 030 ----
mean loss: 180.01
 ---- batch: 040 ----
mean loss: 173.95
 ---- batch: 050 ----
mean loss: 184.06
 ---- batch: 060 ----
mean loss: 183.36
 ---- batch: 070 ----
mean loss: 175.59
 ---- batch: 080 ----
mean loss: 180.33
 ---- batch: 090 ----
mean loss: 185.85
train mean loss: 178.88
epoch train time: 0:00:00.496627
elapsed time: 0:01:15.198130
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 23:29:38.750561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.24
 ---- batch: 020 ----
mean loss: 173.15
 ---- batch: 030 ----
mean loss: 177.82
 ---- batch: 040 ----
mean loss: 173.30
 ---- batch: 050 ----
mean loss: 181.01
 ---- batch: 060 ----
mean loss: 185.64
 ---- batch: 070 ----
mean loss: 170.98
 ---- batch: 080 ----
mean loss: 178.82
 ---- batch: 090 ----
mean loss: 186.58
train mean loss: 178.95
epoch train time: 0:00:00.495343
elapsed time: 0:01:15.693634
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 23:29:39.246073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.77
 ---- batch: 020 ----
mean loss: 180.54
 ---- batch: 030 ----
mean loss: 170.91
 ---- batch: 040 ----
mean loss: 178.16
 ---- batch: 050 ----
mean loss: 178.99
 ---- batch: 060 ----
mean loss: 171.88
 ---- batch: 070 ----
mean loss: 171.56
 ---- batch: 080 ----
mean loss: 178.03
 ---- batch: 090 ----
mean loss: 181.89
train mean loss: 176.63
epoch train time: 0:00:00.494582
elapsed time: 0:01:16.188372
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 23:29:39.740805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.45
 ---- batch: 020 ----
mean loss: 170.22
 ---- batch: 030 ----
mean loss: 177.77
 ---- batch: 040 ----
mean loss: 177.56
 ---- batch: 050 ----
mean loss: 171.47
 ---- batch: 060 ----
mean loss: 179.11
 ---- batch: 070 ----
mean loss: 172.70
 ---- batch: 080 ----
mean loss: 179.71
 ---- batch: 090 ----
mean loss: 169.44
train mean loss: 174.24
epoch train time: 0:00:00.506551
elapsed time: 0:01:16.695080
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 23:29:40.247511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.45
 ---- batch: 020 ----
mean loss: 174.10
 ---- batch: 030 ----
mean loss: 170.61
 ---- batch: 040 ----
mean loss: 173.52
 ---- batch: 050 ----
mean loss: 162.19
 ---- batch: 060 ----
mean loss: 175.96
 ---- batch: 070 ----
mean loss: 180.47
 ---- batch: 080 ----
mean loss: 180.26
 ---- batch: 090 ----
mean loss: 182.02
train mean loss: 175.43
epoch train time: 0:00:00.501877
elapsed time: 0:01:17.197103
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 23:29:40.749554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.09
 ---- batch: 020 ----
mean loss: 172.89
 ---- batch: 030 ----
mean loss: 170.39
 ---- batch: 040 ----
mean loss: 171.36
 ---- batch: 050 ----
mean loss: 171.85
 ---- batch: 060 ----
mean loss: 178.02
 ---- batch: 070 ----
mean loss: 173.15
 ---- batch: 080 ----
mean loss: 177.09
 ---- batch: 090 ----
mean loss: 171.65
train mean loss: 173.71
epoch train time: 0:00:00.497074
elapsed time: 0:01:17.694349
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 23:29:41.246779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.05
 ---- batch: 020 ----
mean loss: 172.33
 ---- batch: 030 ----
mean loss: 178.22
 ---- batch: 040 ----
mean loss: 176.41
 ---- batch: 050 ----
mean loss: 177.34
 ---- batch: 060 ----
mean loss: 173.31
 ---- batch: 070 ----
mean loss: 180.80
 ---- batch: 080 ----
mean loss: 175.82
 ---- batch: 090 ----
mean loss: 174.54
train mean loss: 174.24
epoch train time: 0:00:00.493708
elapsed time: 0:01:18.188207
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 23:29:41.740659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.28
 ---- batch: 020 ----
mean loss: 166.84
 ---- batch: 030 ----
mean loss: 163.49
 ---- batch: 040 ----
mean loss: 170.39
 ---- batch: 050 ----
mean loss: 177.66
 ---- batch: 060 ----
mean loss: 180.20
 ---- batch: 070 ----
mean loss: 169.95
 ---- batch: 080 ----
mean loss: 179.17
 ---- batch: 090 ----
mean loss: 171.42
train mean loss: 172.22
epoch train time: 0:00:00.513134
elapsed time: 0:01:18.701514
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 23:29:42.253954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.51
 ---- batch: 020 ----
mean loss: 175.52
 ---- batch: 030 ----
mean loss: 169.72
 ---- batch: 040 ----
mean loss: 167.06
 ---- batch: 050 ----
mean loss: 171.56
 ---- batch: 060 ----
mean loss: 175.22
 ---- batch: 070 ----
mean loss: 173.08
 ---- batch: 080 ----
mean loss: 174.50
 ---- batch: 090 ----
mean loss: 170.43
train mean loss: 171.95
epoch train time: 0:00:00.511786
elapsed time: 0:01:19.213455
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 23:29:42.765882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.97
 ---- batch: 020 ----
mean loss: 171.53
 ---- batch: 030 ----
mean loss: 173.67
 ---- batch: 040 ----
mean loss: 171.80
 ---- batch: 050 ----
mean loss: 164.29
 ---- batch: 060 ----
mean loss: 177.52
 ---- batch: 070 ----
mean loss: 169.65
 ---- batch: 080 ----
mean loss: 177.69
 ---- batch: 090 ----
mean loss: 169.67
train mean loss: 171.55
epoch train time: 0:00:00.510309
elapsed time: 0:01:19.723908
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 23:29:43.276338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.38
 ---- batch: 020 ----
mean loss: 174.17
 ---- batch: 030 ----
mean loss: 169.09
 ---- batch: 040 ----
mean loss: 168.48
 ---- batch: 050 ----
mean loss: 171.19
 ---- batch: 060 ----
mean loss: 176.31
 ---- batch: 070 ----
mean loss: 174.58
 ---- batch: 080 ----
mean loss: 169.45
 ---- batch: 090 ----
mean loss: 165.46
train mean loss: 169.31
epoch train time: 0:00:00.506502
elapsed time: 0:01:20.230560
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 23:29:43.782993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.18
 ---- batch: 020 ----
mean loss: 161.38
 ---- batch: 030 ----
mean loss: 172.94
 ---- batch: 040 ----
mean loss: 163.42
 ---- batch: 050 ----
mean loss: 164.26
 ---- batch: 060 ----
mean loss: 179.76
 ---- batch: 070 ----
mean loss: 171.01
 ---- batch: 080 ----
mean loss: 171.82
 ---- batch: 090 ----
mean loss: 175.42
train mean loss: 169.58
epoch train time: 0:00:00.531812
elapsed time: 0:01:20.762527
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 23:29:44.314957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.84
 ---- batch: 020 ----
mean loss: 163.00
 ---- batch: 030 ----
mean loss: 165.02
 ---- batch: 040 ----
mean loss: 166.98
 ---- batch: 050 ----
mean loss: 176.81
 ---- batch: 060 ----
mean loss: 173.65
 ---- batch: 070 ----
mean loss: 170.02
 ---- batch: 080 ----
mean loss: 172.23
 ---- batch: 090 ----
mean loss: 169.42
train mean loss: 169.35
epoch train time: 0:00:00.518724
elapsed time: 0:01:21.281444
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 23:29:44.833895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.39
 ---- batch: 020 ----
mean loss: 156.60
 ---- batch: 030 ----
mean loss: 165.11
 ---- batch: 040 ----
mean loss: 168.97
 ---- batch: 050 ----
mean loss: 167.79
 ---- batch: 060 ----
mean loss: 167.00
 ---- batch: 070 ----
mean loss: 177.14
 ---- batch: 080 ----
mean loss: 173.57
 ---- batch: 090 ----
mean loss: 168.29
train mean loss: 167.78
epoch train time: 0:00:00.518610
elapsed time: 0:01:21.800232
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 23:29:45.352694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.35
 ---- batch: 020 ----
mean loss: 160.16
 ---- batch: 030 ----
mean loss: 162.41
 ---- batch: 040 ----
mean loss: 167.81
 ---- batch: 050 ----
mean loss: 167.06
 ---- batch: 060 ----
mean loss: 164.64
 ---- batch: 070 ----
mean loss: 171.78
 ---- batch: 080 ----
mean loss: 173.23
 ---- batch: 090 ----
mean loss: 172.58
train mean loss: 166.93
epoch train time: 0:00:00.510624
elapsed time: 0:01:22.311030
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 23:29:45.863459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.58
 ---- batch: 020 ----
mean loss: 163.91
 ---- batch: 030 ----
mean loss: 160.83
 ---- batch: 040 ----
mean loss: 165.11
 ---- batch: 050 ----
mean loss: 170.96
 ---- batch: 060 ----
mean loss: 166.93
 ---- batch: 070 ----
mean loss: 169.45
 ---- batch: 080 ----
mean loss: 170.55
 ---- batch: 090 ----
mean loss: 161.07
train mean loss: 166.11
epoch train time: 0:00:00.513226
elapsed time: 0:01:22.824402
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 23:29:46.376832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.47
 ---- batch: 020 ----
mean loss: 157.37
 ---- batch: 030 ----
mean loss: 159.65
 ---- batch: 040 ----
mean loss: 159.95
 ---- batch: 050 ----
mean loss: 169.94
 ---- batch: 060 ----
mean loss: 167.06
 ---- batch: 070 ----
mean loss: 166.82
 ---- batch: 080 ----
mean loss: 172.57
 ---- batch: 090 ----
mean loss: 169.30
train mean loss: 165.76
epoch train time: 0:00:00.520742
elapsed time: 0:01:23.345307
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 23:29:46.897741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.84
 ---- batch: 020 ----
mean loss: 158.57
 ---- batch: 030 ----
mean loss: 160.72
 ---- batch: 040 ----
mean loss: 163.10
 ---- batch: 050 ----
mean loss: 169.04
 ---- batch: 060 ----
mean loss: 155.60
 ---- batch: 070 ----
mean loss: 165.45
 ---- batch: 080 ----
mean loss: 165.33
 ---- batch: 090 ----
mean loss: 172.13
train mean loss: 164.53
epoch train time: 0:00:00.499369
elapsed time: 0:01:23.844832
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 23:29:47.397261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.33
 ---- batch: 020 ----
mean loss: 162.28
 ---- batch: 030 ----
mean loss: 163.69
 ---- batch: 040 ----
mean loss: 163.72
 ---- batch: 050 ----
mean loss: 166.43
 ---- batch: 060 ----
mean loss: 171.49
 ---- batch: 070 ----
mean loss: 165.99
 ---- batch: 080 ----
mean loss: 160.86
 ---- batch: 090 ----
mean loss: 163.88
train mean loss: 165.46
epoch train time: 0:00:00.495199
elapsed time: 0:01:24.340184
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 23:29:47.892620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.40
 ---- batch: 020 ----
mean loss: 162.11
 ---- batch: 030 ----
mean loss: 160.99
 ---- batch: 040 ----
mean loss: 161.27
 ---- batch: 050 ----
mean loss: 163.32
 ---- batch: 060 ----
mean loss: 165.72
 ---- batch: 070 ----
mean loss: 164.64
 ---- batch: 080 ----
mean loss: 163.42
 ---- batch: 090 ----
mean loss: 165.07
train mean loss: 163.87
epoch train time: 0:00:00.500396
elapsed time: 0:01:24.840740
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 23:29:48.393185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.50
 ---- batch: 020 ----
mean loss: 162.53
 ---- batch: 030 ----
mean loss: 158.63
 ---- batch: 040 ----
mean loss: 161.22
 ---- batch: 050 ----
mean loss: 163.64
 ---- batch: 060 ----
mean loss: 159.33
 ---- batch: 070 ----
mean loss: 169.67
 ---- batch: 080 ----
mean loss: 152.38
 ---- batch: 090 ----
mean loss: 169.44
train mean loss: 162.51
epoch train time: 0:00:00.494671
elapsed time: 0:01:25.335571
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 23:29:48.888000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.30
 ---- batch: 020 ----
mean loss: 162.13
 ---- batch: 030 ----
mean loss: 157.76
 ---- batch: 040 ----
mean loss: 168.02
 ---- batch: 050 ----
mean loss: 166.40
 ---- batch: 060 ----
mean loss: 163.18
 ---- batch: 070 ----
mean loss: 156.89
 ---- batch: 080 ----
mean loss: 157.00
 ---- batch: 090 ----
mean loss: 161.17
train mean loss: 162.60
epoch train time: 0:00:00.490597
elapsed time: 0:01:25.826317
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 23:29:49.378747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.88
 ---- batch: 020 ----
mean loss: 154.94
 ---- batch: 030 ----
mean loss: 166.53
 ---- batch: 040 ----
mean loss: 156.01
 ---- batch: 050 ----
mean loss: 153.40
 ---- batch: 060 ----
mean loss: 168.09
 ---- batch: 070 ----
mean loss: 167.32
 ---- batch: 080 ----
mean loss: 165.17
 ---- batch: 090 ----
mean loss: 167.12
train mean loss: 161.43
epoch train time: 0:00:00.499623
elapsed time: 0:01:26.326090
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 23:29:49.878539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.56
 ---- batch: 020 ----
mean loss: 163.29
 ---- batch: 030 ----
mean loss: 156.01
 ---- batch: 040 ----
mean loss: 156.10
 ---- batch: 050 ----
mean loss: 166.94
 ---- batch: 060 ----
mean loss: 162.67
 ---- batch: 070 ----
mean loss: 162.40
 ---- batch: 080 ----
mean loss: 163.90
 ---- batch: 090 ----
mean loss: 161.24
train mean loss: 161.52
epoch train time: 0:00:00.504470
elapsed time: 0:01:26.830744
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 23:29:50.383173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.42
 ---- batch: 020 ----
mean loss: 160.76
 ---- batch: 030 ----
mean loss: 156.24
 ---- batch: 040 ----
mean loss: 163.08
 ---- batch: 050 ----
mean loss: 160.81
 ---- batch: 060 ----
mean loss: 161.22
 ---- batch: 070 ----
mean loss: 164.01
 ---- batch: 080 ----
mean loss: 163.89
 ---- batch: 090 ----
mean loss: 161.36
train mean loss: 160.36
epoch train time: 0:00:00.507455
elapsed time: 0:01:27.338353
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 23:29:50.890785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.41
 ---- batch: 020 ----
mean loss: 161.41
 ---- batch: 030 ----
mean loss: 156.75
 ---- batch: 040 ----
mean loss: 156.56
 ---- batch: 050 ----
mean loss: 154.44
 ---- batch: 060 ----
mean loss: 166.19
 ---- batch: 070 ----
mean loss: 161.51
 ---- batch: 080 ----
mean loss: 162.05
 ---- batch: 090 ----
mean loss: 159.60
train mean loss: 160.17
epoch train time: 0:00:00.505500
elapsed time: 0:01:27.844002
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 23:29:51.396433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.46
 ---- batch: 020 ----
mean loss: 158.35
 ---- batch: 030 ----
mean loss: 158.67
 ---- batch: 040 ----
mean loss: 156.57
 ---- batch: 050 ----
mean loss: 159.97
 ---- batch: 060 ----
mean loss: 163.43
 ---- batch: 070 ----
mean loss: 160.14
 ---- batch: 080 ----
mean loss: 163.45
 ---- batch: 090 ----
mean loss: 162.86
train mean loss: 160.70
epoch train time: 0:00:00.510444
elapsed time: 0:01:28.354597
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 23:29:51.907030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.38
 ---- batch: 020 ----
mean loss: 154.36
 ---- batch: 030 ----
mean loss: 156.90
 ---- batch: 040 ----
mean loss: 159.95
 ---- batch: 050 ----
mean loss: 159.10
 ---- batch: 060 ----
mean loss: 165.07
 ---- batch: 070 ----
mean loss: 164.59
 ---- batch: 080 ----
mean loss: 161.04
 ---- batch: 090 ----
mean loss: 164.10
train mean loss: 161.16
epoch train time: 0:00:00.495136
elapsed time: 0:01:28.849884
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 23:29:52.402332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.35
 ---- batch: 020 ----
mean loss: 155.92
 ---- batch: 030 ----
mean loss: 162.60
 ---- batch: 040 ----
mean loss: 163.41
 ---- batch: 050 ----
mean loss: 159.87
 ---- batch: 060 ----
mean loss: 158.72
 ---- batch: 070 ----
mean loss: 163.66
 ---- batch: 080 ----
mean loss: 155.12
 ---- batch: 090 ----
mean loss: 159.92
train mean loss: 158.84
epoch train time: 0:00:00.498317
elapsed time: 0:01:29.348366
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 23:29:52.900802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.23
 ---- batch: 020 ----
mean loss: 162.54
 ---- batch: 030 ----
mean loss: 158.41
 ---- batch: 040 ----
mean loss: 151.83
 ---- batch: 050 ----
mean loss: 153.82
 ---- batch: 060 ----
mean loss: 162.67
 ---- batch: 070 ----
mean loss: 163.11
 ---- batch: 080 ----
mean loss: 159.87
 ---- batch: 090 ----
mean loss: 159.03
train mean loss: 158.79
epoch train time: 0:00:00.501956
elapsed time: 0:01:29.850493
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 23:29:53.402932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.25
 ---- batch: 020 ----
mean loss: 159.51
 ---- batch: 030 ----
mean loss: 158.11
 ---- batch: 040 ----
mean loss: 160.20
 ---- batch: 050 ----
mean loss: 160.26
 ---- batch: 060 ----
mean loss: 164.12
 ---- batch: 070 ----
mean loss: 149.53
 ---- batch: 080 ----
mean loss: 158.88
 ---- batch: 090 ----
mean loss: 161.48
train mean loss: 158.72
epoch train time: 0:00:00.501225
elapsed time: 0:01:30.351875
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 23:29:53.904305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.63
 ---- batch: 020 ----
mean loss: 153.91
 ---- batch: 030 ----
mean loss: 159.99
 ---- batch: 040 ----
mean loss: 157.17
 ---- batch: 050 ----
mean loss: 155.80
 ---- batch: 060 ----
mean loss: 163.68
 ---- batch: 070 ----
mean loss: 162.79
 ---- batch: 080 ----
mean loss: 148.68
 ---- batch: 090 ----
mean loss: 167.50
train mean loss: 158.42
epoch train time: 0:00:00.490049
elapsed time: 0:01:30.842066
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 23:29:54.394493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.13
 ---- batch: 020 ----
mean loss: 154.38
 ---- batch: 030 ----
mean loss: 155.18
 ---- batch: 040 ----
mean loss: 149.38
 ---- batch: 050 ----
mean loss: 161.55
 ---- batch: 060 ----
mean loss: 158.64
 ---- batch: 070 ----
mean loss: 155.16
 ---- batch: 080 ----
mean loss: 158.13
 ---- batch: 090 ----
mean loss: 164.59
train mean loss: 157.06
epoch train time: 0:00:00.497613
elapsed time: 0:01:31.339822
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 23:29:54.892254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.02
 ---- batch: 020 ----
mean loss: 144.88
 ---- batch: 030 ----
mean loss: 152.66
 ---- batch: 040 ----
mean loss: 156.01
 ---- batch: 050 ----
mean loss: 160.81
 ---- batch: 060 ----
mean loss: 162.83
 ---- batch: 070 ----
mean loss: 158.99
 ---- batch: 080 ----
mean loss: 162.16
 ---- batch: 090 ----
mean loss: 164.22
train mean loss: 156.30
epoch train time: 0:00:00.498168
elapsed time: 0:01:31.838139
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 23:29:55.390571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.34
 ---- batch: 020 ----
mean loss: 146.21
 ---- batch: 030 ----
mean loss: 157.79
 ---- batch: 040 ----
mean loss: 155.35
 ---- batch: 050 ----
mean loss: 154.09
 ---- batch: 060 ----
mean loss: 154.13
 ---- batch: 070 ----
mean loss: 157.80
 ---- batch: 080 ----
mean loss: 161.15
 ---- batch: 090 ----
mean loss: 165.86
train mean loss: 156.93
epoch train time: 0:00:00.498321
elapsed time: 0:01:32.336652
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 23:29:55.889126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.01
 ---- batch: 020 ----
mean loss: 157.32
 ---- batch: 030 ----
mean loss: 156.01
 ---- batch: 040 ----
mean loss: 158.73
 ---- batch: 050 ----
mean loss: 157.35
 ---- batch: 060 ----
mean loss: 154.83
 ---- batch: 070 ----
mean loss: 153.61
 ---- batch: 080 ----
mean loss: 156.81
 ---- batch: 090 ----
mean loss: 154.37
train mean loss: 155.94
epoch train time: 0:00:00.495645
elapsed time: 0:01:32.832483
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 23:29:56.384912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.09
 ---- batch: 020 ----
mean loss: 154.21
 ---- batch: 030 ----
mean loss: 153.24
 ---- batch: 040 ----
mean loss: 151.92
 ---- batch: 050 ----
mean loss: 154.10
 ---- batch: 060 ----
mean loss: 157.37
 ---- batch: 070 ----
mean loss: 155.80
 ---- batch: 080 ----
mean loss: 158.28
 ---- batch: 090 ----
mean loss: 154.72
train mean loss: 156.09
epoch train time: 0:00:00.497275
elapsed time: 0:01:33.329905
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 23:29:56.882337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.13
 ---- batch: 020 ----
mean loss: 155.58
 ---- batch: 030 ----
mean loss: 151.79
 ---- batch: 040 ----
mean loss: 157.23
 ---- batch: 050 ----
mean loss: 157.95
 ---- batch: 060 ----
mean loss: 152.69
 ---- batch: 070 ----
mean loss: 149.11
 ---- batch: 080 ----
mean loss: 163.98
 ---- batch: 090 ----
mean loss: 156.23
train mean loss: 154.62
epoch train time: 0:00:00.501247
elapsed time: 0:01:33.831297
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 23:29:57.383727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.82
 ---- batch: 020 ----
mean loss: 152.64
 ---- batch: 030 ----
mean loss: 153.49
 ---- batch: 040 ----
mean loss: 156.65
 ---- batch: 050 ----
mean loss: 153.73
 ---- batch: 060 ----
mean loss: 152.69
 ---- batch: 070 ----
mean loss: 154.81
 ---- batch: 080 ----
mean loss: 162.71
 ---- batch: 090 ----
mean loss: 155.66
train mean loss: 154.55
epoch train time: 0:00:00.502952
elapsed time: 0:01:34.334393
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 23:29:57.886821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.61
 ---- batch: 020 ----
mean loss: 154.75
 ---- batch: 030 ----
mean loss: 150.76
 ---- batch: 040 ----
mean loss: 153.92
 ---- batch: 050 ----
mean loss: 156.40
 ---- batch: 060 ----
mean loss: 154.03
 ---- batch: 070 ----
mean loss: 153.20
 ---- batch: 080 ----
mean loss: 153.89
 ---- batch: 090 ----
mean loss: 153.33
train mean loss: 154.14
epoch train time: 0:00:00.492338
elapsed time: 0:01:34.826873
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 23:29:58.379302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.38
 ---- batch: 020 ----
mean loss: 144.31
 ---- batch: 030 ----
mean loss: 148.38
 ---- batch: 040 ----
mean loss: 155.88
 ---- batch: 050 ----
mean loss: 156.24
 ---- batch: 060 ----
mean loss: 152.43
 ---- batch: 070 ----
mean loss: 157.38
 ---- batch: 080 ----
mean loss: 157.78
 ---- batch: 090 ----
mean loss: 156.51
train mean loss: 153.18
epoch train time: 0:00:00.500486
elapsed time: 0:01:35.327535
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 23:29:58.879970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.16
 ---- batch: 020 ----
mean loss: 147.89
 ---- batch: 030 ----
mean loss: 150.28
 ---- batch: 040 ----
mean loss: 156.49
 ---- batch: 050 ----
mean loss: 149.84
 ---- batch: 060 ----
mean loss: 162.91
 ---- batch: 070 ----
mean loss: 153.64
 ---- batch: 080 ----
mean loss: 154.66
 ---- batch: 090 ----
mean loss: 154.31
train mean loss: 153.95
epoch train time: 0:00:00.496917
elapsed time: 0:01:35.824636
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 23:29:59.377067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.00
 ---- batch: 020 ----
mean loss: 152.89
 ---- batch: 030 ----
mean loss: 148.22
 ---- batch: 040 ----
mean loss: 152.19
 ---- batch: 050 ----
mean loss: 152.07
 ---- batch: 060 ----
mean loss: 158.14
 ---- batch: 070 ----
mean loss: 152.27
 ---- batch: 080 ----
mean loss: 154.84
 ---- batch: 090 ----
mean loss: 161.44
train mean loss: 153.39
epoch train time: 0:00:00.509997
elapsed time: 0:01:36.334786
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 23:29:59.887237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.79
 ---- batch: 020 ----
mean loss: 147.63
 ---- batch: 030 ----
mean loss: 149.44
 ---- batch: 040 ----
mean loss: 151.83
 ---- batch: 050 ----
mean loss: 150.08
 ---- batch: 060 ----
mean loss: 158.50
 ---- batch: 070 ----
mean loss: 154.87
 ---- batch: 080 ----
mean loss: 160.41
 ---- batch: 090 ----
mean loss: 165.27
train mean loss: 153.78
epoch train time: 0:00:00.499297
elapsed time: 0:01:36.834278
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 23:30:00.386696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.04
 ---- batch: 020 ----
mean loss: 150.47
 ---- batch: 030 ----
mean loss: 146.31
 ---- batch: 040 ----
mean loss: 154.35
 ---- batch: 050 ----
mean loss: 156.88
 ---- batch: 060 ----
mean loss: 153.83
 ---- batch: 070 ----
mean loss: 147.16
 ---- batch: 080 ----
mean loss: 150.10
 ---- batch: 090 ----
mean loss: 164.15
train mean loss: 152.55
epoch train time: 0:00:00.501120
elapsed time: 0:01:37.335554
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 23:30:00.887989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.90
 ---- batch: 020 ----
mean loss: 144.83
 ---- batch: 030 ----
mean loss: 148.81
 ---- batch: 040 ----
mean loss: 153.56
 ---- batch: 050 ----
mean loss: 150.50
 ---- batch: 060 ----
mean loss: 146.62
 ---- batch: 070 ----
mean loss: 157.19
 ---- batch: 080 ----
mean loss: 158.36
 ---- batch: 090 ----
mean loss: 153.50
train mean loss: 151.25
epoch train time: 0:00:00.506944
elapsed time: 0:01:37.842648
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 23:30:01.395080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.34
 ---- batch: 020 ----
mean loss: 148.27
 ---- batch: 030 ----
mean loss: 147.58
 ---- batch: 040 ----
mean loss: 149.69
 ---- batch: 050 ----
mean loss: 150.14
 ---- batch: 060 ----
mean loss: 144.34
 ---- batch: 070 ----
mean loss: 146.77
 ---- batch: 080 ----
mean loss: 159.95
 ---- batch: 090 ----
mean loss: 158.23
train mean loss: 150.99
epoch train time: 0:00:00.508222
elapsed time: 0:01:38.351018
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 23:30:01.903448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.53
 ---- batch: 020 ----
mean loss: 152.24
 ---- batch: 030 ----
mean loss: 149.45
 ---- batch: 040 ----
mean loss: 149.62
 ---- batch: 050 ----
mean loss: 148.96
 ---- batch: 060 ----
mean loss: 147.02
 ---- batch: 070 ----
mean loss: 149.12
 ---- batch: 080 ----
mean loss: 154.72
 ---- batch: 090 ----
mean loss: 157.74
train mean loss: 150.53
epoch train time: 0:00:00.489931
elapsed time: 0:01:38.841092
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 23:30:02.393541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.82
 ---- batch: 020 ----
mean loss: 144.43
 ---- batch: 030 ----
mean loss: 154.16
 ---- batch: 040 ----
mean loss: 148.65
 ---- batch: 050 ----
mean loss: 150.93
 ---- batch: 060 ----
mean loss: 153.09
 ---- batch: 070 ----
mean loss: 156.52
 ---- batch: 080 ----
mean loss: 161.33
 ---- batch: 090 ----
mean loss: 151.16
train mean loss: 151.71
epoch train time: 0:00:00.496747
elapsed time: 0:01:39.338004
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 23:30:02.890434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.59
 ---- batch: 020 ----
mean loss: 150.96
 ---- batch: 030 ----
mean loss: 147.47
 ---- batch: 040 ----
mean loss: 142.67
 ---- batch: 050 ----
mean loss: 142.33
 ---- batch: 060 ----
mean loss: 150.48
 ---- batch: 070 ----
mean loss: 149.68
 ---- batch: 080 ----
mean loss: 155.48
 ---- batch: 090 ----
mean loss: 151.47
train mean loss: 149.83
epoch train time: 0:00:00.491261
elapsed time: 0:01:39.829409
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 23:30:03.381845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.36
 ---- batch: 020 ----
mean loss: 139.13
 ---- batch: 030 ----
mean loss: 150.98
 ---- batch: 040 ----
mean loss: 145.79
 ---- batch: 050 ----
mean loss: 148.90
 ---- batch: 060 ----
mean loss: 148.25
 ---- batch: 070 ----
mean loss: 150.19
 ---- batch: 080 ----
mean loss: 158.03
 ---- batch: 090 ----
mean loss: 163.30
train mean loss: 150.49
epoch train time: 0:00:00.516467
elapsed time: 0:01:40.346071
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 23:30:03.898528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.05
 ---- batch: 020 ----
mean loss: 149.56
 ---- batch: 030 ----
mean loss: 147.91
 ---- batch: 040 ----
mean loss: 149.49
 ---- batch: 050 ----
mean loss: 147.13
 ---- batch: 060 ----
mean loss: 150.48
 ---- batch: 070 ----
mean loss: 150.03
 ---- batch: 080 ----
mean loss: 151.03
 ---- batch: 090 ----
mean loss: 145.89
train mean loss: 148.82
epoch train time: 0:00:00.490908
elapsed time: 0:01:40.837148
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 23:30:04.389586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.36
 ---- batch: 020 ----
mean loss: 144.33
 ---- batch: 030 ----
mean loss: 145.76
 ---- batch: 040 ----
mean loss: 150.56
 ---- batch: 050 ----
mean loss: 144.04
 ---- batch: 060 ----
mean loss: 145.23
 ---- batch: 070 ----
mean loss: 150.71
 ---- batch: 080 ----
mean loss: 151.43
 ---- batch: 090 ----
mean loss: 155.44
train mean loss: 149.07
epoch train time: 0:00:00.496807
elapsed time: 0:01:41.334107
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 23:30:04.886535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.79
 ---- batch: 020 ----
mean loss: 145.24
 ---- batch: 030 ----
mean loss: 143.46
 ---- batch: 040 ----
mean loss: 147.77
 ---- batch: 050 ----
mean loss: 148.85
 ---- batch: 060 ----
mean loss: 149.26
 ---- batch: 070 ----
mean loss: 152.13
 ---- batch: 080 ----
mean loss: 155.39
 ---- batch: 090 ----
mean loss: 151.31
train mean loss: 148.71
epoch train time: 0:00:00.489884
elapsed time: 0:01:41.824132
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 23:30:05.376570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.50
 ---- batch: 020 ----
mean loss: 144.85
 ---- batch: 030 ----
mean loss: 148.68
 ---- batch: 040 ----
mean loss: 152.10
 ---- batch: 050 ----
mean loss: 149.08
 ---- batch: 060 ----
mean loss: 155.78
 ---- batch: 070 ----
mean loss: 145.61
 ---- batch: 080 ----
mean loss: 148.10
 ---- batch: 090 ----
mean loss: 146.04
train mean loss: 148.48
epoch train time: 0:00:00.503888
elapsed time: 0:01:42.328192
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 23:30:05.880621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.84
 ---- batch: 020 ----
mean loss: 147.15
 ---- batch: 030 ----
mean loss: 144.41
 ---- batch: 040 ----
mean loss: 145.25
 ---- batch: 050 ----
mean loss: 150.29
 ---- batch: 060 ----
mean loss: 147.10
 ---- batch: 070 ----
mean loss: 145.92
 ---- batch: 080 ----
mean loss: 150.47
 ---- batch: 090 ----
mean loss: 158.05
train mean loss: 147.87
epoch train time: 0:00:00.493231
elapsed time: 0:01:42.821604
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 23:30:06.374036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.68
 ---- batch: 020 ----
mean loss: 142.57
 ---- batch: 030 ----
mean loss: 151.08
 ---- batch: 040 ----
mean loss: 148.58
 ---- batch: 050 ----
mean loss: 144.16
 ---- batch: 060 ----
mean loss: 147.92
 ---- batch: 070 ----
mean loss: 149.64
 ---- batch: 080 ----
mean loss: 157.81
 ---- batch: 090 ----
mean loss: 148.90
train mean loss: 147.92
epoch train time: 0:00:00.497914
elapsed time: 0:01:43.319689
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 23:30:06.872145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.46
 ---- batch: 020 ----
mean loss: 147.93
 ---- batch: 030 ----
mean loss: 146.97
 ---- batch: 040 ----
mean loss: 145.43
 ---- batch: 050 ----
mean loss: 150.00
 ---- batch: 060 ----
mean loss: 146.42
 ---- batch: 070 ----
mean loss: 144.65
 ---- batch: 080 ----
mean loss: 149.76
 ---- batch: 090 ----
mean loss: 152.94
train mean loss: 147.21
epoch train time: 0:00:00.507407
elapsed time: 0:01:43.827277
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 23:30:07.379726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.80
 ---- batch: 020 ----
mean loss: 145.97
 ---- batch: 030 ----
mean loss: 153.80
 ---- batch: 040 ----
mean loss: 136.46
 ---- batch: 050 ----
mean loss: 151.68
 ---- batch: 060 ----
mean loss: 144.58
 ---- batch: 070 ----
mean loss: 149.87
 ---- batch: 080 ----
mean loss: 145.84
 ---- batch: 090 ----
mean loss: 154.16
train mean loss: 147.04
epoch train time: 0:00:00.520113
elapsed time: 0:01:44.347590
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 23:30:07.900030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.19
 ---- batch: 020 ----
mean loss: 140.90
 ---- batch: 030 ----
mean loss: 139.90
 ---- batch: 040 ----
mean loss: 150.53
 ---- batch: 050 ----
mean loss: 151.17
 ---- batch: 060 ----
mean loss: 147.84
 ---- batch: 070 ----
mean loss: 148.08
 ---- batch: 080 ----
mean loss: 149.59
 ---- batch: 090 ----
mean loss: 150.45
train mean loss: 146.81
epoch train time: 0:00:00.496725
elapsed time: 0:01:44.844478
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 23:30:08.396907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.15
 ---- batch: 020 ----
mean loss: 148.78
 ---- batch: 030 ----
mean loss: 139.69
 ---- batch: 040 ----
mean loss: 149.52
 ---- batch: 050 ----
mean loss: 143.85
 ---- batch: 060 ----
mean loss: 140.91
 ---- batch: 070 ----
mean loss: 149.86
 ---- batch: 080 ----
mean loss: 145.45
 ---- batch: 090 ----
mean loss: 144.15
train mean loss: 146.39
epoch train time: 0:00:00.492113
elapsed time: 0:01:45.336732
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 23:30:08.889162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.16
 ---- batch: 020 ----
mean loss: 147.05
 ---- batch: 030 ----
mean loss: 150.20
 ---- batch: 040 ----
mean loss: 147.02
 ---- batch: 050 ----
mean loss: 145.99
 ---- batch: 060 ----
mean loss: 149.92
 ---- batch: 070 ----
mean loss: 153.74
 ---- batch: 080 ----
mean loss: 144.27
 ---- batch: 090 ----
mean loss: 143.81
train mean loss: 146.33
epoch train time: 0:00:00.487766
elapsed time: 0:01:45.824641
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 23:30:09.377071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.23
 ---- batch: 020 ----
mean loss: 145.63
 ---- batch: 030 ----
mean loss: 144.78
 ---- batch: 040 ----
mean loss: 147.56
 ---- batch: 050 ----
mean loss: 152.31
 ---- batch: 060 ----
mean loss: 149.75
 ---- batch: 070 ----
mean loss: 145.88
 ---- batch: 080 ----
mean loss: 151.42
 ---- batch: 090 ----
mean loss: 149.82
train mean loss: 146.76
epoch train time: 0:00:00.501957
elapsed time: 0:01:46.326739
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 23:30:09.879170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.44
 ---- batch: 020 ----
mean loss: 141.31
 ---- batch: 030 ----
mean loss: 146.10
 ---- batch: 040 ----
mean loss: 147.09
 ---- batch: 050 ----
mean loss: 148.94
 ---- batch: 060 ----
mean loss: 151.62
 ---- batch: 070 ----
mean loss: 138.99
 ---- batch: 080 ----
mean loss: 150.06
 ---- batch: 090 ----
mean loss: 143.00
train mean loss: 145.59
epoch train time: 0:00:00.492850
elapsed time: 0:01:46.819764
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 23:30:10.372207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.87
 ---- batch: 020 ----
mean loss: 145.80
 ---- batch: 030 ----
mean loss: 139.69
 ---- batch: 040 ----
mean loss: 143.60
 ---- batch: 050 ----
mean loss: 143.55
 ---- batch: 060 ----
mean loss: 146.08
 ---- batch: 070 ----
mean loss: 144.00
 ---- batch: 080 ----
mean loss: 152.08
 ---- batch: 090 ----
mean loss: 147.57
train mean loss: 144.60
epoch train time: 0:00:00.492861
elapsed time: 0:01:47.312801
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 23:30:10.865240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.95
 ---- batch: 020 ----
mean loss: 142.99
 ---- batch: 030 ----
mean loss: 138.48
 ---- batch: 040 ----
mean loss: 142.77
 ---- batch: 050 ----
mean loss: 140.92
 ---- batch: 060 ----
mean loss: 142.38
 ---- batch: 070 ----
mean loss: 149.34
 ---- batch: 080 ----
mean loss: 147.99
 ---- batch: 090 ----
mean loss: 147.71
train mean loss: 144.30
epoch train time: 0:00:00.497317
elapsed time: 0:01:47.810293
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 23:30:11.362711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.50
 ---- batch: 020 ----
mean loss: 137.34
 ---- batch: 030 ----
mean loss: 143.62
 ---- batch: 040 ----
mean loss: 147.35
 ---- batch: 050 ----
mean loss: 146.27
 ---- batch: 060 ----
mean loss: 145.13
 ---- batch: 070 ----
mean loss: 150.61
 ---- batch: 080 ----
mean loss: 143.40
 ---- batch: 090 ----
mean loss: 149.68
train mean loss: 144.94
epoch train time: 0:00:00.500866
elapsed time: 0:01:48.311292
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 23:30:11.863740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.29
 ---- batch: 020 ----
mean loss: 143.97
 ---- batch: 030 ----
mean loss: 140.61
 ---- batch: 040 ----
mean loss: 150.75
 ---- batch: 050 ----
mean loss: 142.59
 ---- batch: 060 ----
mean loss: 144.26
 ---- batch: 070 ----
mean loss: 138.35
 ---- batch: 080 ----
mean loss: 152.28
 ---- batch: 090 ----
mean loss: 142.72
train mean loss: 144.16
epoch train time: 0:00:00.495772
elapsed time: 0:01:48.807242
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 23:30:12.359671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.96
 ---- batch: 020 ----
mean loss: 140.48
 ---- batch: 030 ----
mean loss: 140.06
 ---- batch: 040 ----
mean loss: 142.40
 ---- batch: 050 ----
mean loss: 142.69
 ---- batch: 060 ----
mean loss: 144.36
 ---- batch: 070 ----
mean loss: 143.53
 ---- batch: 080 ----
mean loss: 144.55
 ---- batch: 090 ----
mean loss: 150.69
train mean loss: 143.59
epoch train time: 0:00:00.499367
elapsed time: 0:01:49.306765
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 23:30:12.859199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.73
 ---- batch: 020 ----
mean loss: 139.75
 ---- batch: 030 ----
mean loss: 149.65
 ---- batch: 040 ----
mean loss: 143.37
 ---- batch: 050 ----
mean loss: 142.74
 ---- batch: 060 ----
mean loss: 141.77
 ---- batch: 070 ----
mean loss: 142.34
 ---- batch: 080 ----
mean loss: 146.77
 ---- batch: 090 ----
mean loss: 147.41
train mean loss: 143.23
epoch train time: 0:00:00.504293
elapsed time: 0:01:49.811221
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 23:30:13.363650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.28
 ---- batch: 020 ----
mean loss: 141.38
 ---- batch: 030 ----
mean loss: 144.77
 ---- batch: 040 ----
mean loss: 146.01
 ---- batch: 050 ----
mean loss: 141.91
 ---- batch: 060 ----
mean loss: 142.06
 ---- batch: 070 ----
mean loss: 143.99
 ---- batch: 080 ----
mean loss: 141.56
 ---- batch: 090 ----
mean loss: 144.25
train mean loss: 143.41
epoch train time: 0:00:00.502649
elapsed time: 0:01:50.314021
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 23:30:13.866454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.13
 ---- batch: 020 ----
mean loss: 144.85
 ---- batch: 030 ----
mean loss: 135.83
 ---- batch: 040 ----
mean loss: 143.44
 ---- batch: 050 ----
mean loss: 147.09
 ---- batch: 060 ----
mean loss: 145.95
 ---- batch: 070 ----
mean loss: 142.32
 ---- batch: 080 ----
mean loss: 144.54
 ---- batch: 090 ----
mean loss: 143.00
train mean loss: 143.27
epoch train time: 0:00:00.494779
elapsed time: 0:01:50.808944
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 23:30:14.361374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.81
 ---- batch: 020 ----
mean loss: 140.43
 ---- batch: 030 ----
mean loss: 142.07
 ---- batch: 040 ----
mean loss: 140.93
 ---- batch: 050 ----
mean loss: 141.75
 ---- batch: 060 ----
mean loss: 148.07
 ---- batch: 070 ----
mean loss: 141.17
 ---- batch: 080 ----
mean loss: 142.26
 ---- batch: 090 ----
mean loss: 143.84
train mean loss: 143.14
epoch train time: 0:00:00.498410
elapsed time: 0:01:51.307500
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 23:30:14.859955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.10
 ---- batch: 020 ----
mean loss: 137.65
 ---- batch: 030 ----
mean loss: 146.77
 ---- batch: 040 ----
mean loss: 140.55
 ---- batch: 050 ----
mean loss: 139.17
 ---- batch: 060 ----
mean loss: 144.50
 ---- batch: 070 ----
mean loss: 143.15
 ---- batch: 080 ----
mean loss: 145.77
 ---- batch: 090 ----
mean loss: 145.56
train mean loss: 142.14
epoch train time: 0:00:00.520729
elapsed time: 0:01:51.828400
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 23:30:15.380832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.06
 ---- batch: 020 ----
mean loss: 139.60
 ---- batch: 030 ----
mean loss: 141.13
 ---- batch: 040 ----
mean loss: 141.74
 ---- batch: 050 ----
mean loss: 145.08
 ---- batch: 060 ----
mean loss: 144.60
 ---- batch: 070 ----
mean loss: 136.18
 ---- batch: 080 ----
mean loss: 144.13
 ---- batch: 090 ----
mean loss: 144.60
train mean loss: 142.04
epoch train time: 0:00:00.500335
elapsed time: 0:01:52.328884
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 23:30:15.881315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.21
 ---- batch: 020 ----
mean loss: 135.36
 ---- batch: 030 ----
mean loss: 142.77
 ---- batch: 040 ----
mean loss: 141.78
 ---- batch: 050 ----
mean loss: 140.70
 ---- batch: 060 ----
mean loss: 137.11
 ---- batch: 070 ----
mean loss: 141.97
 ---- batch: 080 ----
mean loss: 149.66
 ---- batch: 090 ----
mean loss: 140.06
train mean loss: 141.87
epoch train time: 0:00:00.496489
elapsed time: 0:01:52.825525
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 23:30:16.377953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.87
 ---- batch: 020 ----
mean loss: 137.94
 ---- batch: 030 ----
mean loss: 139.42
 ---- batch: 040 ----
mean loss: 142.79
 ---- batch: 050 ----
mean loss: 146.93
 ---- batch: 060 ----
mean loss: 142.34
 ---- batch: 070 ----
mean loss: 144.68
 ---- batch: 080 ----
mean loss: 139.47
 ---- batch: 090 ----
mean loss: 143.59
train mean loss: 141.45
epoch train time: 0:00:00.501195
elapsed time: 0:01:53.326874
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 23:30:16.879314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.38
 ---- batch: 020 ----
mean loss: 137.12
 ---- batch: 030 ----
mean loss: 135.90
 ---- batch: 040 ----
mean loss: 139.67
 ---- batch: 050 ----
mean loss: 144.79
 ---- batch: 060 ----
mean loss: 142.68
 ---- batch: 070 ----
mean loss: 144.97
 ---- batch: 080 ----
mean loss: 151.80
 ---- batch: 090 ----
mean loss: 142.28
train mean loss: 141.64
epoch train time: 0:00:00.501728
elapsed time: 0:01:53.828756
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 23:30:17.381192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.96
 ---- batch: 020 ----
mean loss: 134.60
 ---- batch: 030 ----
mean loss: 139.34
 ---- batch: 040 ----
mean loss: 141.48
 ---- batch: 050 ----
mean loss: 134.88
 ---- batch: 060 ----
mean loss: 144.03
 ---- batch: 070 ----
mean loss: 146.05
 ---- batch: 080 ----
mean loss: 143.77
 ---- batch: 090 ----
mean loss: 140.73
train mean loss: 140.69
epoch train time: 0:00:00.502399
elapsed time: 0:01:54.331305
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 23:30:17.883734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.76
 ---- batch: 020 ----
mean loss: 141.96
 ---- batch: 030 ----
mean loss: 147.66
 ---- batch: 040 ----
mean loss: 146.16
 ---- batch: 050 ----
mean loss: 134.21
 ---- batch: 060 ----
mean loss: 141.96
 ---- batch: 070 ----
mean loss: 142.94
 ---- batch: 080 ----
mean loss: 148.37
 ---- batch: 090 ----
mean loss: 137.94
train mean loss: 141.02
epoch train time: 0:00:00.492947
elapsed time: 0:01:54.824412
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 23:30:18.376880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.07
 ---- batch: 020 ----
mean loss: 136.47
 ---- batch: 030 ----
mean loss: 138.47
 ---- batch: 040 ----
mean loss: 140.63
 ---- batch: 050 ----
mean loss: 138.81
 ---- batch: 060 ----
mean loss: 144.05
 ---- batch: 070 ----
mean loss: 144.21
 ---- batch: 080 ----
mean loss: 135.92
 ---- batch: 090 ----
mean loss: 147.75
train mean loss: 140.72
epoch train time: 0:00:00.496508
elapsed time: 0:01:55.321100
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 23:30:18.873531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.97
 ---- batch: 020 ----
mean loss: 136.91
 ---- batch: 030 ----
mean loss: 139.40
 ---- batch: 040 ----
mean loss: 133.81
 ---- batch: 050 ----
mean loss: 144.72
 ---- batch: 060 ----
mean loss: 138.95
 ---- batch: 070 ----
mean loss: 144.44
 ---- batch: 080 ----
mean loss: 136.97
 ---- batch: 090 ----
mean loss: 140.30
train mean loss: 140.45
epoch train time: 0:00:00.493310
elapsed time: 0:01:55.814561
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 23:30:19.367002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.59
 ---- batch: 020 ----
mean loss: 139.21
 ---- batch: 030 ----
mean loss: 134.56
 ---- batch: 040 ----
mean loss: 143.13
 ---- batch: 050 ----
mean loss: 139.93
 ---- batch: 060 ----
mean loss: 144.96
 ---- batch: 070 ----
mean loss: 140.96
 ---- batch: 080 ----
mean loss: 136.89
 ---- batch: 090 ----
mean loss: 138.66
train mean loss: 139.37
epoch train time: 0:00:00.497857
elapsed time: 0:01:56.312584
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 23:30:19.865016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.12
 ---- batch: 020 ----
mean loss: 138.26
 ---- batch: 030 ----
mean loss: 141.47
 ---- batch: 040 ----
mean loss: 134.72
 ---- batch: 050 ----
mean loss: 140.95
 ---- batch: 060 ----
mean loss: 138.01
 ---- batch: 070 ----
mean loss: 134.60
 ---- batch: 080 ----
mean loss: 140.75
 ---- batch: 090 ----
mean loss: 142.97
train mean loss: 139.18
epoch train time: 0:00:00.494586
elapsed time: 0:01:56.807312
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 23:30:20.359739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.75
 ---- batch: 020 ----
mean loss: 137.55
 ---- batch: 030 ----
mean loss: 144.23
 ---- batch: 040 ----
mean loss: 131.70
 ---- batch: 050 ----
mean loss: 140.40
 ---- batch: 060 ----
mean loss: 133.77
 ---- batch: 070 ----
mean loss: 143.82
 ---- batch: 080 ----
mean loss: 138.61
 ---- batch: 090 ----
mean loss: 144.70
train mean loss: 139.33
epoch train time: 0:00:00.496522
elapsed time: 0:01:57.303982
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 23:30:20.856414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.25
 ---- batch: 020 ----
mean loss: 140.98
 ---- batch: 030 ----
mean loss: 134.79
 ---- batch: 040 ----
mean loss: 135.90
 ---- batch: 050 ----
mean loss: 142.21
 ---- batch: 060 ----
mean loss: 145.83
 ---- batch: 070 ----
mean loss: 140.44
 ---- batch: 080 ----
mean loss: 143.82
 ---- batch: 090 ----
mean loss: 137.19
train mean loss: 139.43
epoch train time: 0:00:00.491879
elapsed time: 0:01:57.796005
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 23:30:21.348432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.68
 ---- batch: 020 ----
mean loss: 132.67
 ---- batch: 030 ----
mean loss: 132.51
 ---- batch: 040 ----
mean loss: 141.62
 ---- batch: 050 ----
mean loss: 134.02
 ---- batch: 060 ----
mean loss: 142.95
 ---- batch: 070 ----
mean loss: 138.53
 ---- batch: 080 ----
mean loss: 142.53
 ---- batch: 090 ----
mean loss: 144.93
train mean loss: 138.69
epoch train time: 0:00:00.500275
elapsed time: 0:01:58.296424
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 23:30:21.848854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.97
 ---- batch: 020 ----
mean loss: 136.64
 ---- batch: 030 ----
mean loss: 132.75
 ---- batch: 040 ----
mean loss: 139.48
 ---- batch: 050 ----
mean loss: 141.96
 ---- batch: 060 ----
mean loss: 135.19
 ---- batch: 070 ----
mean loss: 142.90
 ---- batch: 080 ----
mean loss: 137.46
 ---- batch: 090 ----
mean loss: 141.19
train mean loss: 138.11
epoch train time: 0:00:00.499766
elapsed time: 0:01:58.796334
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 23:30:22.348784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.02
 ---- batch: 020 ----
mean loss: 134.73
 ---- batch: 030 ----
mean loss: 137.11
 ---- batch: 040 ----
mean loss: 136.11
 ---- batch: 050 ----
mean loss: 133.59
 ---- batch: 060 ----
mean loss: 138.07
 ---- batch: 070 ----
mean loss: 139.36
 ---- batch: 080 ----
mean loss: 137.82
 ---- batch: 090 ----
mean loss: 141.75
train mean loss: 137.48
epoch train time: 0:00:00.495152
elapsed time: 0:01:59.291661
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 23:30:22.844089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.68
 ---- batch: 020 ----
mean loss: 133.37
 ---- batch: 030 ----
mean loss: 142.34
 ---- batch: 040 ----
mean loss: 142.50
 ---- batch: 050 ----
mean loss: 137.09
 ---- batch: 060 ----
mean loss: 134.07
 ---- batch: 070 ----
mean loss: 139.62
 ---- batch: 080 ----
mean loss: 138.61
 ---- batch: 090 ----
mean loss: 139.20
train mean loss: 138.13
epoch train time: 0:00:00.490405
elapsed time: 0:01:59.782213
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 23:30:23.334644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.80
 ---- batch: 020 ----
mean loss: 133.30
 ---- batch: 030 ----
mean loss: 144.76
 ---- batch: 040 ----
mean loss: 133.37
 ---- batch: 050 ----
mean loss: 136.91
 ---- batch: 060 ----
mean loss: 139.48
 ---- batch: 070 ----
mean loss: 139.31
 ---- batch: 080 ----
mean loss: 138.33
 ---- batch: 090 ----
mean loss: 138.13
train mean loss: 137.66
epoch train time: 0:00:00.491918
elapsed time: 0:02:00.274314
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 23:30:23.826757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.07
 ---- batch: 020 ----
mean loss: 136.45
 ---- batch: 030 ----
mean loss: 138.28
 ---- batch: 040 ----
mean loss: 136.42
 ---- batch: 050 ----
mean loss: 136.41
 ---- batch: 060 ----
mean loss: 139.51
 ---- batch: 070 ----
mean loss: 138.97
 ---- batch: 080 ----
mean loss: 140.34
 ---- batch: 090 ----
mean loss: 136.42
train mean loss: 137.95
epoch train time: 0:00:00.499404
elapsed time: 0:02:00.773881
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 23:30:24.326313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.85
 ---- batch: 020 ----
mean loss: 135.19
 ---- batch: 030 ----
mean loss: 135.65
 ---- batch: 040 ----
mean loss: 139.03
 ---- batch: 050 ----
mean loss: 135.29
 ---- batch: 060 ----
mean loss: 143.76
 ---- batch: 070 ----
mean loss: 140.49
 ---- batch: 080 ----
mean loss: 134.72
 ---- batch: 090 ----
mean loss: 136.10
train mean loss: 137.15
epoch train time: 0:00:00.504558
elapsed time: 0:02:01.278611
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 23:30:24.831051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.33
 ---- batch: 020 ----
mean loss: 133.44
 ---- batch: 030 ----
mean loss: 133.57
 ---- batch: 040 ----
mean loss: 136.35
 ---- batch: 050 ----
mean loss: 141.98
 ---- batch: 060 ----
mean loss: 137.64
 ---- batch: 070 ----
mean loss: 135.29
 ---- batch: 080 ----
mean loss: 133.75
 ---- batch: 090 ----
mean loss: 145.10
train mean loss: 137.27
epoch train time: 0:00:00.503746
elapsed time: 0:02:01.782511
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 23:30:25.334950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.46
 ---- batch: 020 ----
mean loss: 128.94
 ---- batch: 030 ----
mean loss: 135.50
 ---- batch: 040 ----
mean loss: 131.13
 ---- batch: 050 ----
mean loss: 133.30
 ---- batch: 060 ----
mean loss: 140.31
 ---- batch: 070 ----
mean loss: 142.99
 ---- batch: 080 ----
mean loss: 137.51
 ---- batch: 090 ----
mean loss: 136.05
train mean loss: 135.86
epoch train time: 0:00:00.498985
elapsed time: 0:02:02.281663
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 23:30:25.834102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.04
 ---- batch: 020 ----
mean loss: 132.68
 ---- batch: 030 ----
mean loss: 137.75
 ---- batch: 040 ----
mean loss: 136.88
 ---- batch: 050 ----
mean loss: 139.99
 ---- batch: 060 ----
mean loss: 144.74
 ---- batch: 070 ----
mean loss: 135.92
 ---- batch: 080 ----
mean loss: 133.67
 ---- batch: 090 ----
mean loss: 134.79
train mean loss: 136.17
epoch train time: 0:00:00.500863
elapsed time: 0:02:02.782728
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 23:30:26.335192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.28
 ---- batch: 020 ----
mean loss: 131.23
 ---- batch: 030 ----
mean loss: 137.34
 ---- batch: 040 ----
mean loss: 134.13
 ---- batch: 050 ----
mean loss: 134.14
 ---- batch: 060 ----
mean loss: 135.33
 ---- batch: 070 ----
mean loss: 135.31
 ---- batch: 080 ----
mean loss: 136.33
 ---- batch: 090 ----
mean loss: 142.71
train mean loss: 135.55
epoch train time: 0:00:00.508235
elapsed time: 0:02:03.291257
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 23:30:26.843688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.91
 ---- batch: 020 ----
mean loss: 135.74
 ---- batch: 030 ----
mean loss: 135.50
 ---- batch: 040 ----
mean loss: 138.10
 ---- batch: 050 ----
mean loss: 141.29
 ---- batch: 060 ----
mean loss: 129.77
 ---- batch: 070 ----
mean loss: 135.23
 ---- batch: 080 ----
mean loss: 136.14
 ---- batch: 090 ----
mean loss: 136.50
train mean loss: 136.30
epoch train time: 0:00:00.497644
elapsed time: 0:02:03.789072
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 23:30:27.341503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.43
 ---- batch: 020 ----
mean loss: 129.37
 ---- batch: 030 ----
mean loss: 129.62
 ---- batch: 040 ----
mean loss: 136.10
 ---- batch: 050 ----
mean loss: 136.35
 ---- batch: 060 ----
mean loss: 138.12
 ---- batch: 070 ----
mean loss: 132.18
 ---- batch: 080 ----
mean loss: 138.77
 ---- batch: 090 ----
mean loss: 139.17
train mean loss: 134.75
epoch train time: 0:00:00.500052
elapsed time: 0:02:04.289309
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 23:30:27.841747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.20
 ---- batch: 020 ----
mean loss: 132.23
 ---- batch: 030 ----
mean loss: 131.57
 ---- batch: 040 ----
mean loss: 136.22
 ---- batch: 050 ----
mean loss: 138.74
 ---- batch: 060 ----
mean loss: 138.76
 ---- batch: 070 ----
mean loss: 132.99
 ---- batch: 080 ----
mean loss: 138.93
 ---- batch: 090 ----
mean loss: 135.61
train mean loss: 135.33
epoch train time: 0:00:00.493946
elapsed time: 0:02:04.783409
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 23:30:28.335847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.40
 ---- batch: 020 ----
mean loss: 135.37
 ---- batch: 030 ----
mean loss: 133.32
 ---- batch: 040 ----
mean loss: 133.27
 ---- batch: 050 ----
mean loss: 134.28
 ---- batch: 060 ----
mean loss: 136.90
 ---- batch: 070 ----
mean loss: 133.10
 ---- batch: 080 ----
mean loss: 136.20
 ---- batch: 090 ----
mean loss: 135.83
train mean loss: 135.70
epoch train time: 0:00:00.508792
elapsed time: 0:02:05.292362
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 23:30:28.844813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.65
 ---- batch: 020 ----
mean loss: 133.77
 ---- batch: 030 ----
mean loss: 133.61
 ---- batch: 040 ----
mean loss: 138.17
 ---- batch: 050 ----
mean loss: 136.09
 ---- batch: 060 ----
mean loss: 139.73
 ---- batch: 070 ----
mean loss: 128.82
 ---- batch: 080 ----
mean loss: 134.26
 ---- batch: 090 ----
mean loss: 138.19
train mean loss: 135.55
epoch train time: 0:00:00.508208
elapsed time: 0:02:05.800738
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 23:30:29.353170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.91
 ---- batch: 020 ----
mean loss: 128.31
 ---- batch: 030 ----
mean loss: 126.40
 ---- batch: 040 ----
mean loss: 141.38
 ---- batch: 050 ----
mean loss: 142.14
 ---- batch: 060 ----
mean loss: 134.37
 ---- batch: 070 ----
mean loss: 136.23
 ---- batch: 080 ----
mean loss: 135.55
 ---- batch: 090 ----
mean loss: 138.34
train mean loss: 135.20
epoch train time: 0:00:00.511884
elapsed time: 0:02:06.312770
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 23:30:29.865216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.24
 ---- batch: 020 ----
mean loss: 130.13
 ---- batch: 030 ----
mean loss: 130.70
 ---- batch: 040 ----
mean loss: 133.09
 ---- batch: 050 ----
mean loss: 132.84
 ---- batch: 060 ----
mean loss: 138.49
 ---- batch: 070 ----
mean loss: 144.00
 ---- batch: 080 ----
mean loss: 134.19
 ---- batch: 090 ----
mean loss: 141.25
train mean loss: 134.83
epoch train time: 0:00:00.499665
elapsed time: 0:02:06.812596
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 23:30:30.365039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.15
 ---- batch: 020 ----
mean loss: 135.68
 ---- batch: 030 ----
mean loss: 129.55
 ---- batch: 040 ----
mean loss: 132.60
 ---- batch: 050 ----
mean loss: 138.50
 ---- batch: 060 ----
mean loss: 140.23
 ---- batch: 070 ----
mean loss: 133.07
 ---- batch: 080 ----
mean loss: 136.31
 ---- batch: 090 ----
mean loss: 133.88
train mean loss: 135.00
epoch train time: 0:00:00.505683
elapsed time: 0:02:07.318457
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 23:30:30.870936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.20
 ---- batch: 020 ----
mean loss: 130.79
 ---- batch: 030 ----
mean loss: 131.13
 ---- batch: 040 ----
mean loss: 130.48
 ---- batch: 050 ----
mean loss: 136.01
 ---- batch: 060 ----
mean loss: 139.51
 ---- batch: 070 ----
mean loss: 134.48
 ---- batch: 080 ----
mean loss: 140.63
 ---- batch: 090 ----
mean loss: 133.99
train mean loss: 133.79
epoch train time: 0:00:00.510951
elapsed time: 0:02:07.829626
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 23:30:31.382059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.86
 ---- batch: 020 ----
mean loss: 127.54
 ---- batch: 030 ----
mean loss: 132.72
 ---- batch: 040 ----
mean loss: 129.82
 ---- batch: 050 ----
mean loss: 132.60
 ---- batch: 060 ----
mean loss: 127.07
 ---- batch: 070 ----
mean loss: 134.26
 ---- batch: 080 ----
mean loss: 137.97
 ---- batch: 090 ----
mean loss: 144.55
train mean loss: 133.65
epoch train time: 0:00:00.501196
elapsed time: 0:02:08.330987
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 23:30:31.883459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.43
 ---- batch: 020 ----
mean loss: 132.65
 ---- batch: 030 ----
mean loss: 130.74
 ---- batch: 040 ----
mean loss: 133.47
 ---- batch: 050 ----
mean loss: 133.83
 ---- batch: 060 ----
mean loss: 135.57
 ---- batch: 070 ----
mean loss: 133.90
 ---- batch: 080 ----
mean loss: 136.91
 ---- batch: 090 ----
mean loss: 130.01
train mean loss: 133.17
epoch train time: 0:00:00.501504
elapsed time: 0:02:08.832692
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 23:30:32.385122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.45
 ---- batch: 020 ----
mean loss: 129.14
 ---- batch: 030 ----
mean loss: 134.67
 ---- batch: 040 ----
mean loss: 132.96
 ---- batch: 050 ----
mean loss: 132.28
 ---- batch: 060 ----
mean loss: 133.32
 ---- batch: 070 ----
mean loss: 135.12
 ---- batch: 080 ----
mean loss: 134.49
 ---- batch: 090 ----
mean loss: 135.43
train mean loss: 132.79
epoch train time: 0:00:00.497381
elapsed time: 0:02:09.330221
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 23:30:32.882652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.35
 ---- batch: 020 ----
mean loss: 133.07
 ---- batch: 030 ----
mean loss: 134.54
 ---- batch: 040 ----
mean loss: 130.50
 ---- batch: 050 ----
mean loss: 134.84
 ---- batch: 060 ----
mean loss: 133.97
 ---- batch: 070 ----
mean loss: 132.46
 ---- batch: 080 ----
mean loss: 130.46
 ---- batch: 090 ----
mean loss: 137.73
train mean loss: 132.95
epoch train time: 0:00:00.501655
elapsed time: 0:02:09.832022
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 23:30:33.384462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.45
 ---- batch: 020 ----
mean loss: 128.42
 ---- batch: 030 ----
mean loss: 132.75
 ---- batch: 040 ----
mean loss: 128.76
 ---- batch: 050 ----
mean loss: 133.71
 ---- batch: 060 ----
mean loss: 129.71
 ---- batch: 070 ----
mean loss: 131.40
 ---- batch: 080 ----
mean loss: 137.91
 ---- batch: 090 ----
mean loss: 143.76
train mean loss: 132.64
epoch train time: 0:00:00.499038
elapsed time: 0:02:10.331211
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 23:30:33.883662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.43
 ---- batch: 020 ----
mean loss: 131.35
 ---- batch: 030 ----
mean loss: 131.25
 ---- batch: 040 ----
mean loss: 134.37
 ---- batch: 050 ----
mean loss: 136.73
 ---- batch: 060 ----
mean loss: 132.89
 ---- batch: 070 ----
mean loss: 132.11
 ---- batch: 080 ----
mean loss: 134.10
 ---- batch: 090 ----
mean loss: 134.58
train mean loss: 133.28
epoch train time: 0:00:00.493519
elapsed time: 0:02:10.824923
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 23:30:34.377353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.71
 ---- batch: 020 ----
mean loss: 133.05
 ---- batch: 030 ----
mean loss: 137.78
 ---- batch: 040 ----
mean loss: 129.16
 ---- batch: 050 ----
mean loss: 131.94
 ---- batch: 060 ----
mean loss: 131.03
 ---- batch: 070 ----
mean loss: 137.47
 ---- batch: 080 ----
mean loss: 138.36
 ---- batch: 090 ----
mean loss: 136.32
train mean loss: 133.59
epoch train time: 0:00:00.498922
elapsed time: 0:02:11.323990
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 23:30:34.876429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.60
 ---- batch: 020 ----
mean loss: 128.28
 ---- batch: 030 ----
mean loss: 130.03
 ---- batch: 040 ----
mean loss: 131.15
 ---- batch: 050 ----
mean loss: 126.03
 ---- batch: 060 ----
mean loss: 135.72
 ---- batch: 070 ----
mean loss: 134.54
 ---- batch: 080 ----
mean loss: 131.15
 ---- batch: 090 ----
mean loss: 138.37
train mean loss: 132.75
epoch train time: 0:00:00.501576
elapsed time: 0:02:11.825720
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 23:30:35.378149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.01
 ---- batch: 020 ----
mean loss: 130.59
 ---- batch: 030 ----
mean loss: 128.50
 ---- batch: 040 ----
mean loss: 128.45
 ---- batch: 050 ----
mean loss: 127.06
 ---- batch: 060 ----
mean loss: 135.58
 ---- batch: 070 ----
mean loss: 130.25
 ---- batch: 080 ----
mean loss: 134.87
 ---- batch: 090 ----
mean loss: 142.35
train mean loss: 131.94
epoch train time: 0:00:00.510064
elapsed time: 0:02:12.335924
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 23:30:35.888363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.66
 ---- batch: 020 ----
mean loss: 125.49
 ---- batch: 030 ----
mean loss: 127.78
 ---- batch: 040 ----
mean loss: 135.54
 ---- batch: 050 ----
mean loss: 131.38
 ---- batch: 060 ----
mean loss: 134.96
 ---- batch: 070 ----
mean loss: 133.90
 ---- batch: 080 ----
mean loss: 138.64
 ---- batch: 090 ----
mean loss: 131.73
train mean loss: 131.59
epoch train time: 0:00:00.498797
elapsed time: 0:02:12.834881
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 23:30:36.387363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.99
 ---- batch: 020 ----
mean loss: 124.54
 ---- batch: 030 ----
mean loss: 130.86
 ---- batch: 040 ----
mean loss: 131.82
 ---- batch: 050 ----
mean loss: 131.16
 ---- batch: 060 ----
mean loss: 137.88
 ---- batch: 070 ----
mean loss: 133.05
 ---- batch: 080 ----
mean loss: 136.28
 ---- batch: 090 ----
mean loss: 133.62
train mean loss: 132.24
epoch train time: 0:00:00.496295
elapsed time: 0:02:13.331399
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 23:30:36.883827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.30
 ---- batch: 020 ----
mean loss: 138.99
 ---- batch: 030 ----
mean loss: 123.92
 ---- batch: 040 ----
mean loss: 129.21
 ---- batch: 050 ----
mean loss: 126.44
 ---- batch: 060 ----
mean loss: 131.91
 ---- batch: 070 ----
mean loss: 137.05
 ---- batch: 080 ----
mean loss: 140.52
 ---- batch: 090 ----
mean loss: 136.90
train mean loss: 132.68
epoch train time: 0:00:00.493254
elapsed time: 0:02:13.824807
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 23:30:37.377251
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.81
 ---- batch: 020 ----
mean loss: 126.86
 ---- batch: 030 ----
mean loss: 119.84
 ---- batch: 040 ----
mean loss: 129.00
 ---- batch: 050 ----
mean loss: 124.80
 ---- batch: 060 ----
mean loss: 120.45
 ---- batch: 070 ----
mean loss: 126.86
 ---- batch: 080 ----
mean loss: 126.90
 ---- batch: 090 ----
mean loss: 124.21
train mean loss: 125.34
epoch train time: 0:00:00.494824
elapsed time: 0:02:14.319804
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 23:30:37.872221
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.00
 ---- batch: 020 ----
mean loss: 120.04
 ---- batch: 030 ----
mean loss: 122.51
 ---- batch: 040 ----
mean loss: 119.07
 ---- batch: 050 ----
mean loss: 125.17
 ---- batch: 060 ----
mean loss: 128.98
 ---- batch: 070 ----
mean loss: 121.82
 ---- batch: 080 ----
mean loss: 125.69
 ---- batch: 090 ----
mean loss: 120.88
train mean loss: 123.95
epoch train time: 0:00:00.491426
elapsed time: 0:02:14.811361
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 23:30:38.363792
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.57
 ---- batch: 020 ----
mean loss: 126.01
 ---- batch: 030 ----
mean loss: 120.94
 ---- batch: 040 ----
mean loss: 125.62
 ---- batch: 050 ----
mean loss: 123.73
 ---- batch: 060 ----
mean loss: 119.80
 ---- batch: 070 ----
mean loss: 122.38
 ---- batch: 080 ----
mean loss: 122.56
 ---- batch: 090 ----
mean loss: 126.06
train mean loss: 123.52
epoch train time: 0:00:00.498160
elapsed time: 0:02:15.309682
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 23:30:38.862122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.71
 ---- batch: 020 ----
mean loss: 120.00
 ---- batch: 030 ----
mean loss: 125.97
 ---- batch: 040 ----
mean loss: 120.15
 ---- batch: 050 ----
mean loss: 121.50
 ---- batch: 060 ----
mean loss: 126.45
 ---- batch: 070 ----
mean loss: 121.07
 ---- batch: 080 ----
mean loss: 131.75
 ---- batch: 090 ----
mean loss: 122.44
train mean loss: 123.26
epoch train time: 0:00:00.499818
elapsed time: 0:02:15.809664
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 23:30:39.362095
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.84
 ---- batch: 020 ----
mean loss: 126.42
 ---- batch: 030 ----
mean loss: 125.05
 ---- batch: 040 ----
mean loss: 116.27
 ---- batch: 050 ----
mean loss: 125.63
 ---- batch: 060 ----
mean loss: 126.51
 ---- batch: 070 ----
mean loss: 120.23
 ---- batch: 080 ----
mean loss: 128.59
 ---- batch: 090 ----
mean loss: 120.71
train mean loss: 123.32
epoch train time: 0:00:00.502488
elapsed time: 0:02:16.312297
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 23:30:39.864726
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.40
 ---- batch: 020 ----
mean loss: 121.15
 ---- batch: 030 ----
mean loss: 124.66
 ---- batch: 040 ----
mean loss: 126.53
 ---- batch: 050 ----
mean loss: 125.65
 ---- batch: 060 ----
mean loss: 121.95
 ---- batch: 070 ----
mean loss: 121.42
 ---- batch: 080 ----
mean loss: 123.76
 ---- batch: 090 ----
mean loss: 124.89
train mean loss: 123.19
epoch train time: 0:00:00.498490
elapsed time: 0:02:16.810930
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 23:30:40.363360
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.65
 ---- batch: 020 ----
mean loss: 121.97
 ---- batch: 030 ----
mean loss: 125.07
 ---- batch: 040 ----
mean loss: 125.07
 ---- batch: 050 ----
mean loss: 124.20
 ---- batch: 060 ----
mean loss: 117.43
 ---- batch: 070 ----
mean loss: 118.90
 ---- batch: 080 ----
mean loss: 123.86
 ---- batch: 090 ----
mean loss: 127.39
train mean loss: 123.12
epoch train time: 0:00:00.498202
elapsed time: 0:02:17.309332
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 23:30:40.861782
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.27
 ---- batch: 020 ----
mean loss: 126.41
 ---- batch: 030 ----
mean loss: 122.02
 ---- batch: 040 ----
mean loss: 125.41
 ---- batch: 050 ----
mean loss: 122.53
 ---- batch: 060 ----
mean loss: 121.03
 ---- batch: 070 ----
mean loss: 124.33
 ---- batch: 080 ----
mean loss: 122.13
 ---- batch: 090 ----
mean loss: 121.40
train mean loss: 122.97
epoch train time: 0:00:00.488631
elapsed time: 0:02:17.798130
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 23:30:41.350587
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.54
 ---- batch: 020 ----
mean loss: 121.98
 ---- batch: 030 ----
mean loss: 121.00
 ---- batch: 040 ----
mean loss: 120.47
 ---- batch: 050 ----
mean loss: 130.21
 ---- batch: 060 ----
mean loss: 122.06
 ---- batch: 070 ----
mean loss: 123.39
 ---- batch: 080 ----
mean loss: 120.82
 ---- batch: 090 ----
mean loss: 123.84
train mean loss: 123.01
epoch train time: 0:00:00.498021
elapsed time: 0:02:18.296325
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 23:30:41.848763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.69
 ---- batch: 020 ----
mean loss: 123.02
 ---- batch: 030 ----
mean loss: 125.54
 ---- batch: 040 ----
mean loss: 124.46
 ---- batch: 050 ----
mean loss: 119.01
 ---- batch: 060 ----
mean loss: 125.78
 ---- batch: 070 ----
mean loss: 121.53
 ---- batch: 080 ----
mean loss: 124.34
 ---- batch: 090 ----
mean loss: 119.54
train mean loss: 122.81
epoch train time: 0:00:00.492700
elapsed time: 0:02:18.789196
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 23:30:42.341651
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.98
 ---- batch: 020 ----
mean loss: 118.80
 ---- batch: 030 ----
mean loss: 125.81
 ---- batch: 040 ----
mean loss: 124.22
 ---- batch: 050 ----
mean loss: 122.58
 ---- batch: 060 ----
mean loss: 123.07
 ---- batch: 070 ----
mean loss: 122.52
 ---- batch: 080 ----
mean loss: 122.42
 ---- batch: 090 ----
mean loss: 125.76
train mean loss: 122.82
epoch train time: 0:00:00.495030
elapsed time: 0:02:19.284399
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 23:30:42.836829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.04
 ---- batch: 020 ----
mean loss: 120.12
 ---- batch: 030 ----
mean loss: 120.07
 ---- batch: 040 ----
mean loss: 127.23
 ---- batch: 050 ----
mean loss: 128.81
 ---- batch: 060 ----
mean loss: 118.02
 ---- batch: 070 ----
mean loss: 123.21
 ---- batch: 080 ----
mean loss: 125.16
 ---- batch: 090 ----
mean loss: 121.68
train mean loss: 122.89
epoch train time: 0:00:00.491728
elapsed time: 0:02:19.776286
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 23:30:43.328715
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.92
 ---- batch: 020 ----
mean loss: 119.70
 ---- batch: 030 ----
mean loss: 118.29
 ---- batch: 040 ----
mean loss: 125.60
 ---- batch: 050 ----
mean loss: 128.70
 ---- batch: 060 ----
mean loss: 128.06
 ---- batch: 070 ----
mean loss: 119.66
 ---- batch: 080 ----
mean loss: 124.25
 ---- batch: 090 ----
mean loss: 118.85
train mean loss: 122.87
epoch train time: 0:00:00.517982
elapsed time: 0:02:20.294425
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 23:30:43.846862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.40
 ---- batch: 020 ----
mean loss: 114.60
 ---- batch: 030 ----
mean loss: 119.04
 ---- batch: 040 ----
mean loss: 117.92
 ---- batch: 050 ----
mean loss: 122.48
 ---- batch: 060 ----
mean loss: 122.88
 ---- batch: 070 ----
mean loss: 126.90
 ---- batch: 080 ----
mean loss: 125.68
 ---- batch: 090 ----
mean loss: 129.58
train mean loss: 122.70
epoch train time: 0:00:00.501157
elapsed time: 0:02:20.795735
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 23:30:44.348165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.95
 ---- batch: 020 ----
mean loss: 123.65
 ---- batch: 030 ----
mean loss: 118.85
 ---- batch: 040 ----
mean loss: 120.29
 ---- batch: 050 ----
mean loss: 125.01
 ---- batch: 060 ----
mean loss: 124.29
 ---- batch: 070 ----
mean loss: 122.41
 ---- batch: 080 ----
mean loss: 127.06
 ---- batch: 090 ----
mean loss: 118.64
train mean loss: 122.85
epoch train time: 0:00:00.505393
elapsed time: 0:02:21.301293
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 23:30:44.853737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.89
 ---- batch: 020 ----
mean loss: 120.86
 ---- batch: 030 ----
mean loss: 124.77
 ---- batch: 040 ----
mean loss: 120.34
 ---- batch: 050 ----
mean loss: 125.08
 ---- batch: 060 ----
mean loss: 118.30
 ---- batch: 070 ----
mean loss: 122.06
 ---- batch: 080 ----
mean loss: 125.04
 ---- batch: 090 ----
mean loss: 122.36
train mean loss: 122.69
epoch train time: 0:00:00.509373
elapsed time: 0:02:21.810825
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 23:30:45.363254
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.77
 ---- batch: 020 ----
mean loss: 123.88
 ---- batch: 030 ----
mean loss: 127.00
 ---- batch: 040 ----
mean loss: 119.16
 ---- batch: 050 ----
mean loss: 119.41
 ---- batch: 060 ----
mean loss: 121.79
 ---- batch: 070 ----
mean loss: 122.65
 ---- batch: 080 ----
mean loss: 126.16
 ---- batch: 090 ----
mean loss: 124.06
train mean loss: 122.53
epoch train time: 0:00:00.503399
elapsed time: 0:02:22.314410
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 23:30:45.866846
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.92
 ---- batch: 020 ----
mean loss: 123.30
 ---- batch: 030 ----
mean loss: 124.46
 ---- batch: 040 ----
mean loss: 121.19
 ---- batch: 050 ----
mean loss: 120.71
 ---- batch: 060 ----
mean loss: 121.81
 ---- batch: 070 ----
mean loss: 119.88
 ---- batch: 080 ----
mean loss: 126.44
 ---- batch: 090 ----
mean loss: 130.97
train mean loss: 122.54
epoch train time: 0:00:00.497066
elapsed time: 0:02:22.811662
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 23:30:46.364106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.97
 ---- batch: 020 ----
mean loss: 121.05
 ---- batch: 030 ----
mean loss: 123.64
 ---- batch: 040 ----
mean loss: 126.38
 ---- batch: 050 ----
mean loss: 122.62
 ---- batch: 060 ----
mean loss: 116.52
 ---- batch: 070 ----
mean loss: 119.96
 ---- batch: 080 ----
mean loss: 128.05
 ---- batch: 090 ----
mean loss: 120.41
train mean loss: 122.58
epoch train time: 0:00:00.498895
elapsed time: 0:02:23.310762
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 23:30:46.863189
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.58
 ---- batch: 020 ----
mean loss: 119.04
 ---- batch: 030 ----
mean loss: 121.11
 ---- batch: 040 ----
mean loss: 124.13
 ---- batch: 050 ----
mean loss: 122.63
 ---- batch: 060 ----
mean loss: 125.92
 ---- batch: 070 ----
mean loss: 122.16
 ---- batch: 080 ----
mean loss: 123.27
 ---- batch: 090 ----
mean loss: 121.76
train mean loss: 122.48
epoch train time: 0:00:00.500704
elapsed time: 0:02:23.811619
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 23:30:47.364049
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.70
 ---- batch: 020 ----
mean loss: 118.83
 ---- batch: 030 ----
mean loss: 121.28
 ---- batch: 040 ----
mean loss: 121.79
 ---- batch: 050 ----
mean loss: 121.11
 ---- batch: 060 ----
mean loss: 125.15
 ---- batch: 070 ----
mean loss: 118.42
 ---- batch: 080 ----
mean loss: 129.38
 ---- batch: 090 ----
mean loss: 122.00
train mean loss: 122.26
epoch train time: 0:00:00.503002
elapsed time: 0:02:24.314767
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 23:30:47.867197
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.22
 ---- batch: 020 ----
mean loss: 124.26
 ---- batch: 030 ----
mean loss: 123.72
 ---- batch: 040 ----
mean loss: 119.14
 ---- batch: 050 ----
mean loss: 119.03
 ---- batch: 060 ----
mean loss: 124.76
 ---- batch: 070 ----
mean loss: 123.12
 ---- batch: 080 ----
mean loss: 126.44
 ---- batch: 090 ----
mean loss: 122.01
train mean loss: 122.49
epoch train time: 0:00:00.504891
elapsed time: 0:02:24.819808
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 23:30:48.372241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.29
 ---- batch: 020 ----
mean loss: 123.08
 ---- batch: 030 ----
mean loss: 126.47
 ---- batch: 040 ----
mean loss: 122.81
 ---- batch: 050 ----
mean loss: 117.69
 ---- batch: 060 ----
mean loss: 124.65
 ---- batch: 070 ----
mean loss: 121.86
 ---- batch: 080 ----
mean loss: 116.63
 ---- batch: 090 ----
mean loss: 127.83
train mean loss: 122.35
epoch train time: 0:00:00.515105
elapsed time: 0:02:25.335080
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 23:30:48.887573
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.73
 ---- batch: 020 ----
mean loss: 113.46
 ---- batch: 030 ----
mean loss: 120.46
 ---- batch: 040 ----
mean loss: 121.47
 ---- batch: 050 ----
mean loss: 121.04
 ---- batch: 060 ----
mean loss: 119.68
 ---- batch: 070 ----
mean loss: 128.04
 ---- batch: 080 ----
mean loss: 126.98
 ---- batch: 090 ----
mean loss: 131.56
train mean loss: 122.39
epoch train time: 0:00:00.499871
elapsed time: 0:02:25.835160
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 23:30:49.387593
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.61
 ---- batch: 020 ----
mean loss: 127.33
 ---- batch: 030 ----
mean loss: 118.76
 ---- batch: 040 ----
mean loss: 125.22
 ---- batch: 050 ----
mean loss: 120.41
 ---- batch: 060 ----
mean loss: 118.27
 ---- batch: 070 ----
mean loss: 120.01
 ---- batch: 080 ----
mean loss: 120.29
 ---- batch: 090 ----
mean loss: 123.03
train mean loss: 122.28
epoch train time: 0:00:00.500712
elapsed time: 0:02:26.336016
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 23:30:49.888458
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.19
 ---- batch: 020 ----
mean loss: 122.86
 ---- batch: 030 ----
mean loss: 121.16
 ---- batch: 040 ----
mean loss: 121.63
 ---- batch: 050 ----
mean loss: 122.21
 ---- batch: 060 ----
mean loss: 119.60
 ---- batch: 070 ----
mean loss: 121.66
 ---- batch: 080 ----
mean loss: 123.04
 ---- batch: 090 ----
mean loss: 127.88
train mean loss: 122.66
epoch train time: 0:00:00.491320
elapsed time: 0:02:26.827492
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 23:30:50.379922
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.33
 ---- batch: 020 ----
mean loss: 130.33
 ---- batch: 030 ----
mean loss: 122.89
 ---- batch: 040 ----
mean loss: 128.14
 ---- batch: 050 ----
mean loss: 124.85
 ---- batch: 060 ----
mean loss: 121.66
 ---- batch: 070 ----
mean loss: 116.27
 ---- batch: 080 ----
mean loss: 124.39
 ---- batch: 090 ----
mean loss: 114.63
train mean loss: 122.33
epoch train time: 0:00:00.493871
elapsed time: 0:02:27.321509
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 23:30:50.873938
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.71
 ---- batch: 020 ----
mean loss: 117.54
 ---- batch: 030 ----
mean loss: 125.66
 ---- batch: 040 ----
mean loss: 127.50
 ---- batch: 050 ----
mean loss: 123.09
 ---- batch: 060 ----
mean loss: 120.18
 ---- batch: 070 ----
mean loss: 125.71
 ---- batch: 080 ----
mean loss: 115.86
 ---- batch: 090 ----
mean loss: 125.22
train mean loss: 122.05
epoch train time: 0:00:00.488279
elapsed time: 0:02:27.809933
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 23:30:51.362362
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.15
 ---- batch: 020 ----
mean loss: 124.02
 ---- batch: 030 ----
mean loss: 122.37
 ---- batch: 040 ----
mean loss: 122.71
 ---- batch: 050 ----
mean loss: 123.97
 ---- batch: 060 ----
mean loss: 124.41
 ---- batch: 070 ----
mean loss: 119.07
 ---- batch: 080 ----
mean loss: 117.24
 ---- batch: 090 ----
mean loss: 126.68
train mean loss: 122.16
epoch train time: 0:00:00.504481
elapsed time: 0:02:28.314571
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 23:30:51.867003
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.48
 ---- batch: 020 ----
mean loss: 118.00
 ---- batch: 030 ----
mean loss: 125.27
 ---- batch: 040 ----
mean loss: 122.89
 ---- batch: 050 ----
mean loss: 119.24
 ---- batch: 060 ----
mean loss: 121.95
 ---- batch: 070 ----
mean loss: 125.05
 ---- batch: 080 ----
mean loss: 127.18
 ---- batch: 090 ----
mean loss: 118.28
train mean loss: 122.25
epoch train time: 0:00:00.499898
elapsed time: 0:02:28.814623
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 23:30:52.367062
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.49
 ---- batch: 020 ----
mean loss: 121.12
 ---- batch: 030 ----
mean loss: 122.92
 ---- batch: 040 ----
mean loss: 122.94
 ---- batch: 050 ----
mean loss: 118.14
 ---- batch: 060 ----
mean loss: 117.34
 ---- batch: 070 ----
mean loss: 123.11
 ---- batch: 080 ----
mean loss: 123.97
 ---- batch: 090 ----
mean loss: 120.77
train mean loss: 122.04
epoch train time: 0:00:00.502334
elapsed time: 0:02:29.317110
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 23:30:52.869541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.32
 ---- batch: 020 ----
mean loss: 119.21
 ---- batch: 030 ----
mean loss: 120.84
 ---- batch: 040 ----
mean loss: 117.42
 ---- batch: 050 ----
mean loss: 122.95
 ---- batch: 060 ----
mean loss: 124.57
 ---- batch: 070 ----
mean loss: 124.56
 ---- batch: 080 ----
mean loss: 121.79
 ---- batch: 090 ----
mean loss: 120.78
train mean loss: 121.83
epoch train time: 0:00:00.500050
elapsed time: 0:02:29.817344
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 23:30:53.369776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.83
 ---- batch: 020 ----
mean loss: 122.21
 ---- batch: 030 ----
mean loss: 121.82
 ---- batch: 040 ----
mean loss: 121.89
 ---- batch: 050 ----
mean loss: 120.56
 ---- batch: 060 ----
mean loss: 120.31
 ---- batch: 070 ----
mean loss: 115.27
 ---- batch: 080 ----
mean loss: 126.66
 ---- batch: 090 ----
mean loss: 123.21
train mean loss: 122.30
epoch train time: 0:00:00.501935
elapsed time: 0:02:30.319444
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 23:30:53.871872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.12
 ---- batch: 020 ----
mean loss: 123.98
 ---- batch: 030 ----
mean loss: 119.11
 ---- batch: 040 ----
mean loss: 122.60
 ---- batch: 050 ----
mean loss: 124.71
 ---- batch: 060 ----
mean loss: 119.32
 ---- batch: 070 ----
mean loss: 126.01
 ---- batch: 080 ----
mean loss: 122.78
 ---- batch: 090 ----
mean loss: 125.91
train mean loss: 122.11
epoch train time: 0:00:00.495772
elapsed time: 0:02:30.815359
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 23:30:54.367788
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.58
 ---- batch: 020 ----
mean loss: 123.33
 ---- batch: 030 ----
mean loss: 119.99
 ---- batch: 040 ----
mean loss: 124.65
 ---- batch: 050 ----
mean loss: 121.55
 ---- batch: 060 ----
mean loss: 124.89
 ---- batch: 070 ----
mean loss: 125.67
 ---- batch: 080 ----
mean loss: 117.74
 ---- batch: 090 ----
mean loss: 120.89
train mean loss: 121.92
epoch train time: 0:00:00.497802
elapsed time: 0:02:31.313325
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 23:30:54.865757
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.44
 ---- batch: 020 ----
mean loss: 121.04
 ---- batch: 030 ----
mean loss: 125.51
 ---- batch: 040 ----
mean loss: 123.07
 ---- batch: 050 ----
mean loss: 120.19
 ---- batch: 060 ----
mean loss: 119.59
 ---- batch: 070 ----
mean loss: 122.57
 ---- batch: 080 ----
mean loss: 125.53
 ---- batch: 090 ----
mean loss: 120.42
train mean loss: 121.81
epoch train time: 0:00:00.492264
elapsed time: 0:02:31.805740
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 23:30:55.358176
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.37
 ---- batch: 020 ----
mean loss: 117.87
 ---- batch: 030 ----
mean loss: 128.17
 ---- batch: 040 ----
mean loss: 117.06
 ---- batch: 050 ----
mean loss: 121.73
 ---- batch: 060 ----
mean loss: 119.41
 ---- batch: 070 ----
mean loss: 122.39
 ---- batch: 080 ----
mean loss: 124.90
 ---- batch: 090 ----
mean loss: 124.44
train mean loss: 122.15
epoch train time: 0:00:00.498090
elapsed time: 0:02:32.303980
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 23:30:55.856409
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.63
 ---- batch: 020 ----
mean loss: 122.53
 ---- batch: 030 ----
mean loss: 124.52
 ---- batch: 040 ----
mean loss: 118.08
 ---- batch: 050 ----
mean loss: 125.43
 ---- batch: 060 ----
mean loss: 120.06
 ---- batch: 070 ----
mean loss: 122.60
 ---- batch: 080 ----
mean loss: 123.26
 ---- batch: 090 ----
mean loss: 121.77
train mean loss: 121.93
epoch train time: 0:00:00.507955
elapsed time: 0:02:32.812086
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 23:30:56.364550
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.41
 ---- batch: 020 ----
mean loss: 123.89
 ---- batch: 030 ----
mean loss: 121.36
 ---- batch: 040 ----
mean loss: 119.73
 ---- batch: 050 ----
mean loss: 126.25
 ---- batch: 060 ----
mean loss: 118.69
 ---- batch: 070 ----
mean loss: 115.43
 ---- batch: 080 ----
mean loss: 129.18
 ---- batch: 090 ----
mean loss: 123.21
train mean loss: 122.14
epoch train time: 0:00:00.510721
elapsed time: 0:02:33.322991
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 23:30:56.875432
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.06
 ---- batch: 020 ----
mean loss: 123.96
 ---- batch: 030 ----
mean loss: 122.83
 ---- batch: 040 ----
mean loss: 116.73
 ---- batch: 050 ----
mean loss: 120.48
 ---- batch: 060 ----
mean loss: 121.51
 ---- batch: 070 ----
mean loss: 120.73
 ---- batch: 080 ----
mean loss: 123.87
 ---- batch: 090 ----
mean loss: 117.30
train mean loss: 121.67
epoch train time: 0:00:00.503175
elapsed time: 0:02:33.826341
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 23:30:57.378771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.36
 ---- batch: 020 ----
mean loss: 120.82
 ---- batch: 030 ----
mean loss: 120.51
 ---- batch: 040 ----
mean loss: 122.63
 ---- batch: 050 ----
mean loss: 119.42
 ---- batch: 060 ----
mean loss: 118.69
 ---- batch: 070 ----
mean loss: 124.57
 ---- batch: 080 ----
mean loss: 120.90
 ---- batch: 090 ----
mean loss: 122.03
train mean loss: 121.96
epoch train time: 0:00:00.504204
elapsed time: 0:02:34.330688
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 23:30:57.883117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.49
 ---- batch: 020 ----
mean loss: 118.79
 ---- batch: 030 ----
mean loss: 124.44
 ---- batch: 040 ----
mean loss: 123.68
 ---- batch: 050 ----
mean loss: 118.59
 ---- batch: 060 ----
mean loss: 123.00
 ---- batch: 070 ----
mean loss: 121.50
 ---- batch: 080 ----
mean loss: 119.75
 ---- batch: 090 ----
mean loss: 121.36
train mean loss: 121.88
epoch train time: 0:00:00.494544
elapsed time: 0:02:34.825377
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 23:30:58.377805
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.89
 ---- batch: 020 ----
mean loss: 122.74
 ---- batch: 030 ----
mean loss: 122.30
 ---- batch: 040 ----
mean loss: 118.22
 ---- batch: 050 ----
mean loss: 123.18
 ---- batch: 060 ----
mean loss: 125.77
 ---- batch: 070 ----
mean loss: 124.75
 ---- batch: 080 ----
mean loss: 115.40
 ---- batch: 090 ----
mean loss: 121.78
train mean loss: 121.87
epoch train time: 0:00:00.499179
elapsed time: 0:02:35.324724
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 23:30:58.877163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.98
 ---- batch: 020 ----
mean loss: 127.35
 ---- batch: 030 ----
mean loss: 116.63
 ---- batch: 040 ----
mean loss: 116.89
 ---- batch: 050 ----
mean loss: 120.89
 ---- batch: 060 ----
mean loss: 122.14
 ---- batch: 070 ----
mean loss: 119.92
 ---- batch: 080 ----
mean loss: 117.39
 ---- batch: 090 ----
mean loss: 131.66
train mean loss: 121.77
epoch train time: 0:00:00.497377
elapsed time: 0:02:35.822266
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 23:30:59.374688
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.04
 ---- batch: 020 ----
mean loss: 120.87
 ---- batch: 030 ----
mean loss: 114.45
 ---- batch: 040 ----
mean loss: 119.21
 ---- batch: 050 ----
mean loss: 124.33
 ---- batch: 060 ----
mean loss: 125.53
 ---- batch: 070 ----
mean loss: 120.54
 ---- batch: 080 ----
mean loss: 124.82
 ---- batch: 090 ----
mean loss: 126.38
train mean loss: 121.54
epoch train time: 0:00:00.508928
elapsed time: 0:02:36.331396
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 23:30:59.883826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.21
 ---- batch: 020 ----
mean loss: 120.25
 ---- batch: 030 ----
mean loss: 115.89
 ---- batch: 040 ----
mean loss: 125.06
 ---- batch: 050 ----
mean loss: 122.94
 ---- batch: 060 ----
mean loss: 121.36
 ---- batch: 070 ----
mean loss: 123.69
 ---- batch: 080 ----
mean loss: 119.96
 ---- batch: 090 ----
mean loss: 124.17
train mean loss: 121.54
epoch train time: 0:00:00.507286
elapsed time: 0:02:36.838832
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 23:31:00.391265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.31
 ---- batch: 020 ----
mean loss: 119.94
 ---- batch: 030 ----
mean loss: 120.56
 ---- batch: 040 ----
mean loss: 123.06
 ---- batch: 050 ----
mean loss: 122.06
 ---- batch: 060 ----
mean loss: 123.98
 ---- batch: 070 ----
mean loss: 121.18
 ---- batch: 080 ----
mean loss: 120.66
 ---- batch: 090 ----
mean loss: 123.68
train mean loss: 121.59
epoch train time: 0:00:00.516227
elapsed time: 0:02:37.355209
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 23:31:00.907640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.56
 ---- batch: 020 ----
mean loss: 120.38
 ---- batch: 030 ----
mean loss: 116.98
 ---- batch: 040 ----
mean loss: 121.83
 ---- batch: 050 ----
mean loss: 128.17
 ---- batch: 060 ----
mean loss: 125.63
 ---- batch: 070 ----
mean loss: 128.32
 ---- batch: 080 ----
mean loss: 116.53
 ---- batch: 090 ----
mean loss: 116.66
train mean loss: 121.41
epoch train time: 0:00:00.505497
elapsed time: 0:02:37.860867
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 23:31:01.413313
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.14
 ---- batch: 020 ----
mean loss: 126.94
 ---- batch: 030 ----
mean loss: 121.39
 ---- batch: 040 ----
mean loss: 114.55
 ---- batch: 050 ----
mean loss: 123.63
 ---- batch: 060 ----
mean loss: 117.50
 ---- batch: 070 ----
mean loss: 122.70
 ---- batch: 080 ----
mean loss: 123.49
 ---- batch: 090 ----
mean loss: 125.83
train mean loss: 121.43
epoch train time: 0:00:00.506422
elapsed time: 0:02:38.370942
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_9/checkpoint.pth.tar
**** end time: 2019-09-25 23:31:01.923338 ****
