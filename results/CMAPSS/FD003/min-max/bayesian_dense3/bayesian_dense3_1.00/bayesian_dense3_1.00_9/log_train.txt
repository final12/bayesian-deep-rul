Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 6335
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-09-20 22:03:47.204478 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 22:03:47.213449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4562.96
 ---- batch: 020 ----
mean loss: 4251.34
 ---- batch: 030 ----
mean loss: 3977.48
 ---- batch: 040 ----
mean loss: 3688.27
train mean loss: 4078.52
epoch train time: 0:00:15.110533
elapsed time: 0:00:15.125160
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 22:04:02.329677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3464.64
 ---- batch: 020 ----
mean loss: 3261.95
 ---- batch: 030 ----
mean loss: 3127.03
 ---- batch: 040 ----
mean loss: 3076.99
train mean loss: 3212.80
epoch train time: 0:00:00.657679
elapsed time: 0:00:15.783019
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 22:04:02.987603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2878.13
 ---- batch: 020 ----
mean loss: 2822.88
 ---- batch: 030 ----
mean loss: 2797.96
 ---- batch: 040 ----
mean loss: 2695.35
train mean loss: 2783.31
epoch train time: 0:00:00.643490
elapsed time: 0:00:16.426746
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 22:04:03.631293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2584.75
 ---- batch: 020 ----
mean loss: 2613.80
 ---- batch: 030 ----
mean loss: 2423.12
 ---- batch: 040 ----
mean loss: 2474.91
train mean loss: 2523.66
epoch train time: 0:00:00.680811
elapsed time: 0:00:17.107804
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 22:04:04.312363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2427.03
 ---- batch: 020 ----
mean loss: 2345.88
 ---- batch: 030 ----
mean loss: 2345.27
 ---- batch: 040 ----
mean loss: 2283.18
train mean loss: 2344.41
epoch train time: 0:00:00.691854
elapsed time: 0:00:17.799993
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 22:04:05.004555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2234.76
 ---- batch: 020 ----
mean loss: 2259.66
 ---- batch: 030 ----
mean loss: 2200.32
 ---- batch: 040 ----
mean loss: 2147.24
train mean loss: 2207.80
epoch train time: 0:00:00.690128
elapsed time: 0:00:18.490380
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 22:04:05.694935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2113.97
 ---- batch: 020 ----
mean loss: 2137.67
 ---- batch: 030 ----
mean loss: 2079.45
 ---- batch: 040 ----
mean loss: 2006.87
train mean loss: 2081.73
epoch train time: 0:00:00.665076
elapsed time: 0:00:19.155677
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 22:04:06.360226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2026.28
 ---- batch: 020 ----
mean loss: 1944.82
 ---- batch: 030 ----
mean loss: 1985.01
 ---- batch: 040 ----
mean loss: 1928.09
train mean loss: 1969.42
epoch train time: 0:00:00.653984
elapsed time: 0:00:19.809899
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 22:04:07.014488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1952.64
 ---- batch: 020 ----
mean loss: 1872.92
 ---- batch: 030 ----
mean loss: 1844.75
 ---- batch: 040 ----
mean loss: 1819.39
train mean loss: 1869.17
epoch train time: 0:00:00.663571
elapsed time: 0:00:20.473720
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 22:04:07.678269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1798.93
 ---- batch: 020 ----
mean loss: 1775.27
 ---- batch: 030 ----
mean loss: 1767.32
 ---- batch: 040 ----
mean loss: 1768.47
train mean loss: 1772.40
epoch train time: 0:00:00.669325
elapsed time: 0:00:21.143295
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 22:04:08.347895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1703.51
 ---- batch: 020 ----
mean loss: 1697.19
 ---- batch: 030 ----
mean loss: 1687.89
 ---- batch: 040 ----
mean loss: 1624.41
train mean loss: 1679.34
epoch train time: 0:00:00.682337
elapsed time: 0:00:21.825930
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 22:04:09.030487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1646.55
 ---- batch: 020 ----
mean loss: 1572.09
 ---- batch: 030 ----
mean loss: 1560.58
 ---- batch: 040 ----
mean loss: 1549.55
train mean loss: 1579.66
epoch train time: 0:00:00.672658
elapsed time: 0:00:22.498858
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 22:04:09.703413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1507.78
 ---- batch: 020 ----
mean loss: 1501.78
 ---- batch: 030 ----
mean loss: 1463.75
 ---- batch: 040 ----
mean loss: 1461.30
train mean loss: 1480.94
epoch train time: 0:00:00.682046
elapsed time: 0:00:23.181143
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 22:04:10.385723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1416.54
 ---- batch: 020 ----
mean loss: 1408.97
 ---- batch: 030 ----
mean loss: 1378.11
 ---- batch: 040 ----
mean loss: 1382.23
train mean loss: 1396.42
epoch train time: 0:00:00.658863
elapsed time: 0:00:23.840272
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 22:04:11.044824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1356.92
 ---- batch: 020 ----
mean loss: 1325.76
 ---- batch: 030 ----
mean loss: 1304.30
 ---- batch: 040 ----
mean loss: 1313.21
train mean loss: 1320.64
epoch train time: 0:00:00.647805
elapsed time: 0:00:24.488302
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 22:04:11.692855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1260.80
 ---- batch: 020 ----
mean loss: 1268.21
 ---- batch: 030 ----
mean loss: 1244.89
 ---- batch: 040 ----
mean loss: 1221.00
train mean loss: 1246.92
epoch train time: 0:00:00.662485
elapsed time: 0:00:25.151070
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 22:04:12.355662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1182.18
 ---- batch: 020 ----
mean loss: 1173.80
 ---- batch: 030 ----
mean loss: 1169.41
 ---- batch: 040 ----
mean loss: 1165.13
train mean loss: 1174.46
epoch train time: 0:00:00.713040
elapsed time: 0:00:25.864382
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 22:04:13.068931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1123.01
 ---- batch: 020 ----
mean loss: 1119.68
 ---- batch: 030 ----
mean loss: 1109.97
 ---- batch: 040 ----
mean loss: 1092.77
train mean loss: 1108.68
epoch train time: 0:00:00.687836
elapsed time: 0:00:26.552466
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 22:04:13.757024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1085.66
 ---- batch: 020 ----
mean loss: 1032.09
 ---- batch: 030 ----
mean loss: 1060.61
 ---- batch: 040 ----
mean loss: 1024.45
train mean loss: 1046.58
epoch train time: 0:00:00.679391
elapsed time: 0:00:27.232089
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 22:04:14.436628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 999.40
 ---- batch: 020 ----
mean loss: 1002.88
 ---- batch: 030 ----
mean loss: 995.59
 ---- batch: 040 ----
mean loss: 957.98
train mean loss: 988.93
epoch train time: 0:00:00.660035
elapsed time: 0:00:27.892365
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 22:04:15.096917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.17
 ---- batch: 020 ----
mean loss: 944.98
 ---- batch: 030 ----
mean loss: 933.53
 ---- batch: 040 ----
mean loss: 920.18
train mean loss: 933.06
epoch train time: 0:00:00.650823
elapsed time: 0:00:28.543551
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 22:04:15.748108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.65
 ---- batch: 020 ----
mean loss: 876.94
 ---- batch: 030 ----
mean loss: 883.30
 ---- batch: 040 ----
mean loss: 870.72
train mean loss: 878.85
epoch train time: 0:00:00.656021
elapsed time: 0:00:29.199831
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 22:04:16.404400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.17
 ---- batch: 020 ----
mean loss: 841.21
 ---- batch: 030 ----
mean loss: 831.67
 ---- batch: 040 ----
mean loss: 802.54
train mean loss: 829.13
epoch train time: 0:00:00.683616
elapsed time: 0:00:29.883697
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 22:04:17.088252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 800.28
 ---- batch: 020 ----
mean loss: 804.45
 ---- batch: 030 ----
mean loss: 775.13
 ---- batch: 040 ----
mean loss: 758.60
train mean loss: 781.12
epoch train time: 0:00:00.685311
elapsed time: 0:00:30.569327
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 22:04:17.773889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 766.83
 ---- batch: 020 ----
mean loss: 738.19
 ---- batch: 030 ----
mean loss: 730.85
 ---- batch: 040 ----
mean loss: 720.30
train mean loss: 735.89
epoch train time: 0:00:00.670583
elapsed time: 0:00:31.240141
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 22:04:18.444702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 716.16
 ---- batch: 020 ----
mean loss: 714.36
 ---- batch: 030 ----
mean loss: 686.22
 ---- batch: 040 ----
mean loss: 675.07
train mean loss: 694.00
epoch train time: 0:00:00.655164
elapsed time: 0:00:31.895542
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 22:04:19.100088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 681.12
 ---- batch: 020 ----
mean loss: 655.56
 ---- batch: 030 ----
mean loss: 636.79
 ---- batch: 040 ----
mean loss: 643.47
train mean loss: 652.80
epoch train time: 0:00:00.632698
elapsed time: 0:00:32.528508
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 22:04:19.733054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 638.35
 ---- batch: 020 ----
mean loss: 622.32
 ---- batch: 030 ----
mean loss: 622.73
 ---- batch: 040 ----
mean loss: 600.54
train mean loss: 617.92
epoch train time: 0:00:00.648697
elapsed time: 0:00:33.177426
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 22:04:20.381973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 598.90
 ---- batch: 020 ----
mean loss: 586.67
 ---- batch: 030 ----
mean loss: 579.56
 ---- batch: 040 ----
mean loss: 570.74
train mean loss: 582.14
epoch train time: 0:00:00.682173
elapsed time: 0:00:33.859869
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 22:04:21.064430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.71
 ---- batch: 020 ----
mean loss: 543.93
 ---- batch: 030 ----
mean loss: 557.77
 ---- batch: 040 ----
mean loss: 538.88
train mean loss: 549.90
epoch train time: 0:00:00.693738
elapsed time: 0:00:34.553851
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 22:04:21.758403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.43
 ---- batch: 020 ----
mean loss: 530.64
 ---- batch: 030 ----
mean loss: 520.94
 ---- batch: 040 ----
mean loss: 511.42
train mean loss: 517.64
epoch train time: 0:00:00.668177
elapsed time: 0:00:35.222271
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 22:04:22.426822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.97
 ---- batch: 020 ----
mean loss: 499.26
 ---- batch: 030 ----
mean loss: 487.03
 ---- batch: 040 ----
mean loss: 477.24
train mean loss: 489.59
epoch train time: 0:00:00.649625
elapsed time: 0:00:35.872153
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 22:04:23.076713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.61
 ---- batch: 020 ----
mean loss: 474.39
 ---- batch: 030 ----
mean loss: 455.97
 ---- batch: 040 ----
mean loss: 463.81
train mean loss: 462.21
epoch train time: 0:00:00.642956
elapsed time: 0:00:36.515338
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 22:04:23.719906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.57
 ---- batch: 020 ----
mean loss: 434.51
 ---- batch: 030 ----
mean loss: 430.37
 ---- batch: 040 ----
mean loss: 432.12
train mean loss: 435.32
epoch train time: 0:00:00.656669
elapsed time: 0:00:37.172245
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 22:04:24.376793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.41
 ---- batch: 020 ----
mean loss: 415.87
 ---- batch: 030 ----
mean loss: 401.20
 ---- batch: 040 ----
mean loss: 395.08
train mean loss: 408.43
epoch train time: 0:00:00.689106
elapsed time: 0:00:37.861634
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 22:04:25.066190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.41
 ---- batch: 020 ----
mean loss: 388.32
 ---- batch: 030 ----
mean loss: 385.19
 ---- batch: 040 ----
mean loss: 383.29
train mean loss: 385.77
epoch train time: 0:00:00.679824
elapsed time: 0:00:38.541687
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 22:04:25.746236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.53
 ---- batch: 020 ----
mean loss: 370.35
 ---- batch: 030 ----
mean loss: 364.73
 ---- batch: 040 ----
mean loss: 360.04
train mean loss: 365.11
epoch train time: 0:00:00.675892
elapsed time: 0:00:39.217829
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 22:04:26.422418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.37
 ---- batch: 020 ----
mean loss: 346.88
 ---- batch: 030 ----
mean loss: 342.31
 ---- batch: 040 ----
mean loss: 338.98
train mean loss: 344.92
epoch train time: 0:00:00.662036
elapsed time: 0:00:39.880118
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 22:04:27.084682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.26
 ---- batch: 020 ----
mean loss: 331.66
 ---- batch: 030 ----
mean loss: 323.53
 ---- batch: 040 ----
mean loss: 320.01
train mean loss: 327.17
epoch train time: 0:00:00.664152
elapsed time: 0:00:40.544526
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 22:04:27.749071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.89
 ---- batch: 020 ----
mean loss: 311.49
 ---- batch: 030 ----
mean loss: 309.12
 ---- batch: 040 ----
mean loss: 303.13
train mean loss: 309.93
epoch train time: 0:00:00.660637
elapsed time: 0:00:41.205366
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 22:04:28.409930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.96
 ---- batch: 020 ----
mean loss: 293.83
 ---- batch: 030 ----
mean loss: 289.91
 ---- batch: 040 ----
mean loss: 285.38
train mean loss: 291.84
epoch train time: 0:00:00.696529
elapsed time: 0:00:41.902183
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 22:04:29.106741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.08
 ---- batch: 020 ----
mean loss: 276.34
 ---- batch: 030 ----
mean loss: 280.36
 ---- batch: 040 ----
mean loss: 273.96
train mean loss: 277.15
epoch train time: 0:00:00.687572
elapsed time: 0:00:42.590042
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 22:04:29.794594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.24
 ---- batch: 020 ----
mean loss: 264.30
 ---- batch: 030 ----
mean loss: 258.76
 ---- batch: 040 ----
mean loss: 257.06
train mean loss: 261.02
epoch train time: 0:00:00.659569
elapsed time: 0:00:43.249869
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 22:04:30.454466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.17
 ---- batch: 020 ----
mean loss: 260.11
 ---- batch: 030 ----
mean loss: 242.71
 ---- batch: 040 ----
mean loss: 239.14
train mean loss: 248.49
epoch train time: 0:00:00.645333
elapsed time: 0:00:43.895465
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 22:04:31.100025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.95
 ---- batch: 020 ----
mean loss: 236.55
 ---- batch: 030 ----
mean loss: 231.91
 ---- batch: 040 ----
mean loss: 229.70
train mean loss: 234.14
epoch train time: 0:00:00.643794
elapsed time: 0:00:44.539487
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 22:04:31.744044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.36
 ---- batch: 020 ----
mean loss: 227.90
 ---- batch: 030 ----
mean loss: 222.41
 ---- batch: 040 ----
mean loss: 217.91
train mean loss: 223.49
epoch train time: 0:00:00.659261
elapsed time: 0:00:45.198975
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 22:04:32.403533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.02
 ---- batch: 020 ----
mean loss: 210.83
 ---- batch: 030 ----
mean loss: 215.94
 ---- batch: 040 ----
mean loss: 209.76
train mean loss: 211.59
epoch train time: 0:00:00.689349
elapsed time: 0:00:45.888588
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 22:04:33.093138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.69
 ---- batch: 020 ----
mean loss: 204.23
 ---- batch: 030 ----
mean loss: 197.16
 ---- batch: 040 ----
mean loss: 199.61
train mean loss: 201.02
epoch train time: 0:00:00.692358
elapsed time: 0:00:46.581213
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 22:04:33.785785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.32
 ---- batch: 020 ----
mean loss: 190.01
 ---- batch: 030 ----
mean loss: 188.87
 ---- batch: 040 ----
mean loss: 190.77
train mean loss: 190.49
epoch train time: 0:00:00.684372
elapsed time: 0:00:47.265834
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 22:04:34.470399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.10
 ---- batch: 020 ----
mean loss: 184.10
 ---- batch: 030 ----
mean loss: 177.61
 ---- batch: 040 ----
mean loss: 175.70
train mean loss: 181.56
epoch train time: 0:00:00.696157
elapsed time: 0:00:47.962233
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 22:04:35.166784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.97
 ---- batch: 020 ----
mean loss: 173.54
 ---- batch: 030 ----
mean loss: 173.25
 ---- batch: 040 ----
mean loss: 174.41
train mean loss: 174.74
epoch train time: 0:00:00.666343
elapsed time: 0:00:48.628790
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 22:04:35.833345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.50
 ---- batch: 020 ----
mean loss: 164.91
 ---- batch: 030 ----
mean loss: 165.33
 ---- batch: 040 ----
mean loss: 164.02
train mean loss: 166.53
epoch train time: 0:00:00.657533
elapsed time: 0:00:49.286535
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 22:04:36.491084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.49
 ---- batch: 020 ----
mean loss: 158.77
 ---- batch: 030 ----
mean loss: 159.31
 ---- batch: 040 ----
mean loss: 153.95
train mean loss: 158.42
epoch train time: 0:00:00.693697
elapsed time: 0:00:49.980512
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 22:04:37.185083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.82
 ---- batch: 020 ----
mean loss: 156.24
 ---- batch: 030 ----
mean loss: 153.26
 ---- batch: 040 ----
mean loss: 150.05
train mean loss: 152.62
epoch train time: 0:00:00.692962
elapsed time: 0:00:50.673723
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 22:04:37.878294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.59
 ---- batch: 020 ----
mean loss: 148.30
 ---- batch: 030 ----
mean loss: 150.88
 ---- batch: 040 ----
mean loss: 143.72
train mean loss: 147.91
epoch train time: 0:00:00.684384
elapsed time: 0:00:51.358360
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 22:04:38.562931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.79
 ---- batch: 020 ----
mean loss: 143.10
 ---- batch: 030 ----
mean loss: 141.15
 ---- batch: 040 ----
mean loss: 136.42
train mean loss: 141.97
epoch train time: 0:00:00.656662
elapsed time: 0:00:52.015283
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 22:04:39.219848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.77
 ---- batch: 020 ----
mean loss: 137.69
 ---- batch: 030 ----
mean loss: 132.67
 ---- batch: 040 ----
mean loss: 135.81
train mean loss: 136.83
epoch train time: 0:00:00.649762
elapsed time: 0:00:52.665300
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 22:04:39.869855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.75
 ---- batch: 020 ----
mean loss: 131.67
 ---- batch: 030 ----
mean loss: 131.55
 ---- batch: 040 ----
mean loss: 128.20
train mean loss: 131.06
epoch train time: 0:00:00.680324
elapsed time: 0:00:53.345842
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 22:04:40.550394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.91
 ---- batch: 020 ----
mean loss: 124.97
 ---- batch: 030 ----
mean loss: 126.06
 ---- batch: 040 ----
mean loss: 127.93
train mean loss: 126.64
epoch train time: 0:00:00.690961
elapsed time: 0:00:54.037052
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 22:04:41.241620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.25
 ---- batch: 020 ----
mean loss: 123.95
 ---- batch: 030 ----
mean loss: 121.16
 ---- batch: 040 ----
mean loss: 119.51
train mean loss: 122.94
epoch train time: 0:00:00.686984
elapsed time: 0:00:54.724298
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 22:04:41.928852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.47
 ---- batch: 020 ----
mean loss: 117.16
 ---- batch: 030 ----
mean loss: 120.55
 ---- batch: 040 ----
mean loss: 115.30
train mean loss: 117.81
epoch train time: 0:00:00.665670
elapsed time: 0:00:55.390228
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 22:04:42.594802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.90
 ---- batch: 020 ----
mean loss: 113.91
 ---- batch: 030 ----
mean loss: 109.91
 ---- batch: 040 ----
mean loss: 113.93
train mean loss: 113.77
epoch train time: 0:00:00.663397
elapsed time: 0:00:56.053868
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 22:04:43.258431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.74
 ---- batch: 020 ----
mean loss: 111.86
 ---- batch: 030 ----
mean loss: 110.50
 ---- batch: 040 ----
mean loss: 113.03
train mean loss: 110.66
epoch train time: 0:00:00.652427
elapsed time: 0:00:56.706528
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 22:04:43.911086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.35
 ---- batch: 020 ----
mean loss: 108.37
 ---- batch: 030 ----
mean loss: 109.37
 ---- batch: 040 ----
mean loss: 105.13
train mean loss: 107.64
epoch train time: 0:00:00.647045
elapsed time: 0:00:57.353785
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 22:04:44.558330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.67
 ---- batch: 020 ----
mean loss: 103.00
 ---- batch: 030 ----
mean loss: 105.93
 ---- batch: 040 ----
mean loss: 106.59
train mean loss: 105.58
epoch train time: 0:00:00.690639
elapsed time: 0:00:58.044710
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 22:04:45.249273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.36
 ---- batch: 020 ----
mean loss: 104.26
 ---- batch: 030 ----
mean loss: 100.32
 ---- batch: 040 ----
mean loss: 102.12
train mean loss: 102.52
epoch train time: 0:00:00.682915
elapsed time: 0:00:58.727883
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 22:04:45.932456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.74
 ---- batch: 020 ----
mean loss: 99.02
 ---- batch: 030 ----
mean loss: 98.77
 ---- batch: 040 ----
mean loss: 95.89
train mean loss: 98.52
epoch train time: 0:00:00.664469
elapsed time: 0:00:59.392627
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 22:04:46.597196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.42
 ---- batch: 020 ----
mean loss: 99.45
 ---- batch: 030 ----
mean loss: 99.55
 ---- batch: 040 ----
mean loss: 96.66
train mean loss: 98.36
epoch train time: 0:00:00.691681
elapsed time: 0:01:00.084602
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 22:04:47.289212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.48
 ---- batch: 020 ----
mean loss: 94.54
 ---- batch: 030 ----
mean loss: 94.15
 ---- batch: 040 ----
mean loss: 95.25
train mean loss: 95.45
epoch train time: 0:00:00.685758
elapsed time: 0:01:00.770655
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 22:04:47.975204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.81
 ---- batch: 020 ----
mean loss: 92.82
 ---- batch: 030 ----
mean loss: 92.42
 ---- batch: 040 ----
mean loss: 92.03
train mean loss: 92.84
epoch train time: 0:00:00.655597
elapsed time: 0:01:01.426510
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 22:04:48.631064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.78
 ---- batch: 020 ----
mean loss: 89.32
 ---- batch: 030 ----
mean loss: 88.11
 ---- batch: 040 ----
mean loss: 90.80
train mean loss: 90.29
epoch train time: 0:00:00.689634
elapsed time: 0:01:02.116404
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 22:04:49.320959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.96
 ---- batch: 020 ----
mean loss: 90.35
 ---- batch: 030 ----
mean loss: 90.16
 ---- batch: 040 ----
mean loss: 86.61
train mean loss: 89.27
epoch train time: 0:00:00.696010
elapsed time: 0:01:02.812694
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 22:04:50.017252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.63
 ---- batch: 020 ----
mean loss: 91.75
 ---- batch: 030 ----
mean loss: 87.89
 ---- batch: 040 ----
mean loss: 86.42
train mean loss: 87.72
epoch train time: 0:00:00.685581
elapsed time: 0:01:03.498565
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 22:04:50.703151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.46
 ---- batch: 020 ----
mean loss: 84.72
 ---- batch: 030 ----
mean loss: 86.02
 ---- batch: 040 ----
mean loss: 86.08
train mean loss: 86.08
epoch train time: 0:00:00.688500
elapsed time: 0:01:04.187333
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 22:04:51.391880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.00
 ---- batch: 020 ----
mean loss: 82.42
 ---- batch: 030 ----
mean loss: 86.16
 ---- batch: 040 ----
mean loss: 84.16
train mean loss: 84.33
epoch train time: 0:00:00.669942
elapsed time: 0:01:04.857477
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 22:04:52.062063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.68
 ---- batch: 020 ----
mean loss: 85.62
 ---- batch: 030 ----
mean loss: 83.65
 ---- batch: 040 ----
mean loss: 79.07
train mean loss: 83.38
epoch train time: 0:00:00.652582
elapsed time: 0:01:05.510315
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 22:04:52.714863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.19
 ---- batch: 020 ----
mean loss: 78.15
 ---- batch: 030 ----
mean loss: 84.18
 ---- batch: 040 ----
mean loss: 84.49
train mean loss: 82.47
epoch train time: 0:00:00.689477
elapsed time: 0:01:06.200032
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 22:04:53.404607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.63
 ---- batch: 020 ----
mean loss: 84.22
 ---- batch: 030 ----
mean loss: 79.61
 ---- batch: 040 ----
mean loss: 79.17
train mean loss: 80.67
epoch train time: 0:00:00.687794
elapsed time: 0:01:06.888078
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 22:04:54.092628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.67
 ---- batch: 020 ----
mean loss: 76.50
 ---- batch: 030 ----
mean loss: 81.66
 ---- batch: 040 ----
mean loss: 79.36
train mean loss: 79.30
epoch train time: 0:00:00.676846
elapsed time: 0:01:07.565158
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 22:04:54.769737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.37
 ---- batch: 020 ----
mean loss: 78.19
 ---- batch: 030 ----
mean loss: 79.60
 ---- batch: 040 ----
mean loss: 78.79
train mean loss: 78.83
epoch train time: 0:00:00.666390
elapsed time: 0:01:08.231778
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 22:04:55.436343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.46
 ---- batch: 020 ----
mean loss: 76.56
 ---- batch: 030 ----
mean loss: 76.55
 ---- batch: 040 ----
mean loss: 76.94
train mean loss: 76.92
epoch train time: 0:00:00.644255
elapsed time: 0:01:08.876287
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 22:04:56.080851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.28
 ---- batch: 020 ----
mean loss: 77.36
 ---- batch: 030 ----
mean loss: 76.74
 ---- batch: 040 ----
mean loss: 75.98
train mean loss: 76.43
epoch train time: 0:00:00.664041
elapsed time: 0:01:09.540551
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 22:04:56.745103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.02
 ---- batch: 020 ----
mean loss: 72.42
 ---- batch: 030 ----
mean loss: 73.32
 ---- batch: 040 ----
mean loss: 74.99
train mean loss: 74.79
epoch train time: 0:00:00.680632
elapsed time: 0:01:10.221431
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 22:04:57.425985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.89
 ---- batch: 020 ----
mean loss: 74.98
 ---- batch: 030 ----
mean loss: 73.43
 ---- batch: 040 ----
mean loss: 75.83
train mean loss: 74.12
epoch train time: 0:00:00.669994
elapsed time: 0:01:10.891645
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 22:04:58.096194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.13
 ---- batch: 020 ----
mean loss: 72.88
 ---- batch: 030 ----
mean loss: 75.64
 ---- batch: 040 ----
mean loss: 71.74
train mean loss: 73.79
epoch train time: 0:00:00.672297
elapsed time: 0:01:11.564216
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 22:04:58.768797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.55
 ---- batch: 020 ----
mean loss: 73.26
 ---- batch: 030 ----
mean loss: 73.53
 ---- batch: 040 ----
mean loss: 72.68
train mean loss: 73.51
epoch train time: 0:00:00.646569
elapsed time: 0:01:12.211023
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 22:04:59.415570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.29
 ---- batch: 020 ----
mean loss: 75.37
 ---- batch: 030 ----
mean loss: 72.54
 ---- batch: 040 ----
mean loss: 73.23
train mean loss: 73.54
epoch train time: 0:00:00.680137
elapsed time: 0:01:12.891371
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 22:05:00.095920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.35
 ---- batch: 020 ----
mean loss: 70.42
 ---- batch: 030 ----
mean loss: 72.58
 ---- batch: 040 ----
mean loss: 71.28
train mean loss: 71.75
epoch train time: 0:00:00.648011
elapsed time: 0:01:13.539641
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 22:05:00.744193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.76
 ---- batch: 020 ----
mean loss: 70.48
 ---- batch: 030 ----
mean loss: 67.77
 ---- batch: 040 ----
mean loss: 68.87
train mean loss: 70.52
epoch train time: 0:00:00.684405
elapsed time: 0:01:14.224321
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 22:05:01.428879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.35
 ---- batch: 020 ----
mean loss: 69.80
 ---- batch: 030 ----
mean loss: 70.43
 ---- batch: 040 ----
mean loss: 72.42
train mean loss: 70.56
epoch train time: 0:00:00.677982
elapsed time: 0:01:14.902577
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 22:05:02.107142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.45
 ---- batch: 020 ----
mean loss: 72.77
 ---- batch: 030 ----
mean loss: 67.13
 ---- batch: 040 ----
mean loss: 70.46
train mean loss: 69.47
epoch train time: 0:00:00.662061
elapsed time: 0:01:15.564884
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 22:05:02.769452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.66
 ---- batch: 020 ----
mean loss: 70.67
 ---- batch: 030 ----
mean loss: 67.99
 ---- batch: 040 ----
mean loss: 67.42
train mean loss: 69.24
epoch train time: 0:00:00.641759
elapsed time: 0:01:16.206892
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 22:05:03.411438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.89
 ---- batch: 020 ----
mean loss: 67.74
 ---- batch: 030 ----
mean loss: 68.59
 ---- batch: 040 ----
mean loss: 68.10
train mean loss: 67.99
epoch train time: 0:00:00.655797
elapsed time: 0:01:16.862932
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 22:05:04.067531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.56
 ---- batch: 020 ----
mean loss: 68.30
 ---- batch: 030 ----
mean loss: 67.54
 ---- batch: 040 ----
mean loss: 66.98
train mean loss: 67.43
epoch train time: 0:00:00.641969
elapsed time: 0:01:17.505189
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 22:05:04.709769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.65
 ---- batch: 020 ----
mean loss: 69.51
 ---- batch: 030 ----
mean loss: 67.21
 ---- batch: 040 ----
mean loss: 68.70
train mean loss: 67.53
epoch train time: 0:00:00.689267
elapsed time: 0:01:18.194748
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 22:05:05.399308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.26
 ---- batch: 020 ----
mean loss: 66.45
 ---- batch: 030 ----
mean loss: 66.78
 ---- batch: 040 ----
mean loss: 68.59
train mean loss: 67.86
epoch train time: 0:00:00.697930
elapsed time: 0:01:18.892916
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 22:05:06.097468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.73
 ---- batch: 020 ----
mean loss: 69.89
 ---- batch: 030 ----
mean loss: 65.10
 ---- batch: 040 ----
mean loss: 65.65
train mean loss: 67.64
epoch train time: 0:00:00.672681
elapsed time: 0:01:19.565829
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 22:05:06.770404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.13
 ---- batch: 020 ----
mean loss: 64.49
 ---- batch: 030 ----
mean loss: 64.91
 ---- batch: 040 ----
mean loss: 67.86
train mean loss: 66.27
epoch train time: 0:00:00.649376
elapsed time: 0:01:20.215456
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 22:05:07.419996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.53
 ---- batch: 020 ----
mean loss: 65.60
 ---- batch: 030 ----
mean loss: 64.76
 ---- batch: 040 ----
mean loss: 66.79
train mean loss: 65.44
epoch train time: 0:00:00.664536
elapsed time: 0:01:20.880215
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 22:05:08.084764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.75
 ---- batch: 020 ----
mean loss: 65.99
 ---- batch: 030 ----
mean loss: 64.45
 ---- batch: 040 ----
mean loss: 62.00
train mean loss: 64.77
epoch train time: 0:00:00.657401
elapsed time: 0:01:21.537836
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 22:05:08.742432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.11
 ---- batch: 020 ----
mean loss: 63.95
 ---- batch: 030 ----
mean loss: 65.55
 ---- batch: 040 ----
mean loss: 64.00
train mean loss: 64.16
epoch train time: 0:00:00.686195
elapsed time: 0:01:22.224318
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 22:05:09.428880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.47
 ---- batch: 020 ----
mean loss: 62.43
 ---- batch: 030 ----
mean loss: 62.32
 ---- batch: 040 ----
mean loss: 66.20
train mean loss: 64.49
epoch train time: 0:00:00.705962
elapsed time: 0:01:22.930533
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 22:05:10.135131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.67
 ---- batch: 020 ----
mean loss: 61.78
 ---- batch: 030 ----
mean loss: 64.67
 ---- batch: 040 ----
mean loss: 65.61
train mean loss: 63.88
epoch train time: 0:00:00.683503
elapsed time: 0:01:23.614325
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 22:05:10.818890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.08
 ---- batch: 020 ----
mean loss: 65.25
 ---- batch: 030 ----
mean loss: 64.01
 ---- batch: 040 ----
mean loss: 60.11
train mean loss: 63.23
epoch train time: 0:00:00.647693
elapsed time: 0:01:24.262233
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 22:05:11.466777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.58
 ---- batch: 020 ----
mean loss: 63.71
 ---- batch: 030 ----
mean loss: 61.56
 ---- batch: 040 ----
mean loss: 64.99
train mean loss: 63.03
epoch train time: 0:00:00.647819
elapsed time: 0:01:24.910247
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 22:05:12.114795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.09
 ---- batch: 020 ----
mean loss: 63.19
 ---- batch: 030 ----
mean loss: 61.74
 ---- batch: 040 ----
mean loss: 63.36
train mean loss: 62.46
epoch train time: 0:00:00.652361
elapsed time: 0:01:25.562812
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 22:05:12.767395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.12
 ---- batch: 020 ----
mean loss: 62.00
 ---- batch: 030 ----
mean loss: 62.57
 ---- batch: 040 ----
mean loss: 63.68
train mean loss: 62.99
epoch train time: 0:00:00.690178
elapsed time: 0:01:26.253332
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 22:05:13.457875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.30
 ---- batch: 020 ----
mean loss: 62.12
 ---- batch: 030 ----
mean loss: 62.26
 ---- batch: 040 ----
mean loss: 60.62
train mean loss: 61.98
epoch train time: 0:00:00.705356
elapsed time: 0:01:26.958917
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 22:05:14.163471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.97
 ---- batch: 020 ----
mean loss: 59.42
 ---- batch: 030 ----
mean loss: 61.25
 ---- batch: 040 ----
mean loss: 65.93
train mean loss: 62.02
epoch train time: 0:00:00.691156
elapsed time: 0:01:27.650358
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 22:05:14.854959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.53
 ---- batch: 020 ----
mean loss: 60.45
 ---- batch: 030 ----
mean loss: 64.88
 ---- batch: 040 ----
mean loss: 58.91
train mean loss: 62.56
epoch train time: 0:00:00.671480
elapsed time: 0:01:28.322125
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 22:05:15.526675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.81
 ---- batch: 020 ----
mean loss: 59.86
 ---- batch: 030 ----
mean loss: 62.52
 ---- batch: 040 ----
mean loss: 61.48
train mean loss: 61.34
epoch train time: 0:00:00.649870
elapsed time: 0:01:28.972210
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 22:05:16.176796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.33
 ---- batch: 020 ----
mean loss: 61.35
 ---- batch: 030 ----
mean loss: 60.85
 ---- batch: 040 ----
mean loss: 59.31
train mean loss: 60.43
epoch train time: 0:00:00.651902
elapsed time: 0:01:29.624357
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 22:05:16.828906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.15
 ---- batch: 020 ----
mean loss: 63.02
 ---- batch: 030 ----
mean loss: 62.17
 ---- batch: 040 ----
mean loss: 58.98
train mean loss: 61.04
epoch train time: 0:00:00.666092
elapsed time: 0:01:30.290712
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 22:05:17.495265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.55
 ---- batch: 020 ----
mean loss: 62.37
 ---- batch: 030 ----
mean loss: 60.98
 ---- batch: 040 ----
mean loss: 59.17
train mean loss: 60.70
epoch train time: 0:00:00.683517
elapsed time: 0:01:30.974460
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 22:05:18.179030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.34
 ---- batch: 020 ----
mean loss: 58.28
 ---- batch: 030 ----
mean loss: 63.40
 ---- batch: 040 ----
mean loss: 60.84
train mean loss: 60.87
epoch train time: 0:00:00.687467
elapsed time: 0:01:31.662196
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 22:05:18.866767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.90
 ---- batch: 020 ----
mean loss: 59.52
 ---- batch: 030 ----
mean loss: 59.28
 ---- batch: 040 ----
mean loss: 56.97
train mean loss: 60.46
epoch train time: 0:00:00.650329
elapsed time: 0:01:32.312760
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 22:05:19.517323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.12
 ---- batch: 020 ----
mean loss: 59.08
 ---- batch: 030 ----
mean loss: 58.72
 ---- batch: 040 ----
mean loss: 62.35
train mean loss: 59.07
epoch train time: 0:00:00.654185
elapsed time: 0:01:32.967165
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 22:05:20.171715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.97
 ---- batch: 020 ----
mean loss: 61.78
 ---- batch: 030 ----
mean loss: 56.60
 ---- batch: 040 ----
mean loss: 57.87
train mean loss: 59.54
epoch train time: 0:00:00.648697
elapsed time: 0:01:33.616196
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 22:05:20.820751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.17
 ---- batch: 020 ----
mean loss: 58.24
 ---- batch: 030 ----
mean loss: 59.12
 ---- batch: 040 ----
mean loss: 57.90
train mean loss: 58.24
epoch train time: 0:00:00.676529
elapsed time: 0:01:34.292975
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 22:05:21.497563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.88
 ---- batch: 020 ----
mean loss: 59.74
 ---- batch: 030 ----
mean loss: 60.39
 ---- batch: 040 ----
mean loss: 58.10
train mean loss: 58.97
epoch train time: 0:00:00.699579
elapsed time: 0:01:34.992951
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 22:05:22.197530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.97
 ---- batch: 020 ----
mean loss: 59.01
 ---- batch: 030 ----
mean loss: 59.25
 ---- batch: 040 ----
mean loss: 57.79
train mean loss: 58.06
epoch train time: 0:00:00.686675
elapsed time: 0:01:35.679929
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 22:05:22.884513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.42
 ---- batch: 020 ----
mean loss: 59.33
 ---- batch: 030 ----
mean loss: 54.51
 ---- batch: 040 ----
mean loss: 60.77
train mean loss: 58.00
epoch train time: 0:00:00.648797
elapsed time: 0:01:36.328968
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 22:05:23.533530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.75
 ---- batch: 020 ----
mean loss: 57.54
 ---- batch: 030 ----
mean loss: 58.43
 ---- batch: 040 ----
mean loss: 57.38
train mean loss: 58.04
epoch train time: 0:00:00.680779
elapsed time: 0:01:37.009993
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 22:05:24.214547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.03
 ---- batch: 020 ----
mean loss: 57.97
 ---- batch: 030 ----
mean loss: 58.10
 ---- batch: 040 ----
mean loss: 55.54
train mean loss: 57.54
epoch train time: 0:00:00.653269
elapsed time: 0:01:37.663490
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 22:05:24.868040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.97
 ---- batch: 020 ----
mean loss: 55.58
 ---- batch: 030 ----
mean loss: 55.46
 ---- batch: 040 ----
mean loss: 56.52
train mean loss: 56.54
epoch train time: 0:00:00.690342
elapsed time: 0:01:38.354093
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 22:05:25.558650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.54
 ---- batch: 020 ----
mean loss: 56.68
 ---- batch: 030 ----
mean loss: 60.00
 ---- batch: 040 ----
mean loss: 56.45
train mean loss: 56.76
epoch train time: 0:00:00.681309
elapsed time: 0:01:39.035640
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 22:05:26.240191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.77
 ---- batch: 020 ----
mean loss: 56.23
 ---- batch: 030 ----
mean loss: 56.88
 ---- batch: 040 ----
mean loss: 55.37
train mean loss: 56.14
epoch train time: 0:00:00.677896
elapsed time: 0:01:39.713824
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 22:05:26.918346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.94
 ---- batch: 020 ----
mean loss: 54.92
 ---- batch: 030 ----
mean loss: 56.83
 ---- batch: 040 ----
mean loss: 56.25
train mean loss: 56.64
epoch train time: 0:00:00.669114
elapsed time: 0:01:40.383114
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 22:05:27.587660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.70
 ---- batch: 020 ----
mean loss: 57.21
 ---- batch: 030 ----
mean loss: 55.64
 ---- batch: 040 ----
mean loss: 56.16
train mean loss: 55.87
epoch train time: 0:00:00.644267
elapsed time: 0:01:41.027574
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 22:05:28.232118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.71
 ---- batch: 020 ----
mean loss: 55.91
 ---- batch: 030 ----
mean loss: 56.03
 ---- batch: 040 ----
mean loss: 53.58
train mean loss: 56.02
epoch train time: 0:00:00.655091
elapsed time: 0:01:41.682864
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 22:05:28.887410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.45
 ---- batch: 020 ----
mean loss: 58.15
 ---- batch: 030 ----
mean loss: 53.77
 ---- batch: 040 ----
mean loss: 56.84
train mean loss: 56.26
epoch train time: 0:00:00.680218
elapsed time: 0:01:42.363324
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 22:05:29.567877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.73
 ---- batch: 020 ----
mean loss: 55.24
 ---- batch: 030 ----
mean loss: 53.26
 ---- batch: 040 ----
mean loss: 55.62
train mean loss: 55.58
epoch train time: 0:00:00.688427
elapsed time: 0:01:43.052048
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 22:05:30.256608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.73
 ---- batch: 020 ----
mean loss: 55.25
 ---- batch: 030 ----
mean loss: 53.44
 ---- batch: 040 ----
mean loss: 53.54
train mean loss: 55.11
epoch train time: 0:00:00.691854
elapsed time: 0:01:43.744144
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 22:05:30.948695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.73
 ---- batch: 020 ----
mean loss: 53.11
 ---- batch: 030 ----
mean loss: 57.05
 ---- batch: 040 ----
mean loss: 55.01
train mean loss: 54.74
epoch train time: 0:00:00.672410
elapsed time: 0:01:44.416798
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 22:05:31.621363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.52
 ---- batch: 020 ----
mean loss: 55.30
 ---- batch: 030 ----
mean loss: 54.72
 ---- batch: 040 ----
mean loss: 54.09
train mean loss: 54.51
epoch train time: 0:00:00.648451
elapsed time: 0:01:45.065470
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 22:05:32.270043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.61
 ---- batch: 020 ----
mean loss: 53.53
 ---- batch: 030 ----
mean loss: 54.10
 ---- batch: 040 ----
mean loss: 55.57
train mean loss: 54.52
epoch train time: 0:00:00.672448
elapsed time: 0:01:45.738168
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 22:05:32.942811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.71
 ---- batch: 020 ----
mean loss: 55.90
 ---- batch: 030 ----
mean loss: 53.27
 ---- batch: 040 ----
mean loss: 51.45
train mean loss: 53.98
epoch train time: 0:00:00.687092
elapsed time: 0:01:46.425655
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 22:05:33.630209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.96
 ---- batch: 020 ----
mean loss: 54.21
 ---- batch: 030 ----
mean loss: 54.02
 ---- batch: 040 ----
mean loss: 53.77
train mean loss: 53.89
epoch train time: 0:00:00.703216
elapsed time: 0:01:47.129107
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 22:05:34.333662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.54
 ---- batch: 020 ----
mean loss: 57.17
 ---- batch: 030 ----
mean loss: 52.00
 ---- batch: 040 ----
mean loss: 51.84
train mean loss: 53.22
epoch train time: 0:00:00.694158
elapsed time: 0:01:47.823522
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 22:05:35.028094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.66
 ---- batch: 020 ----
mean loss: 52.01
 ---- batch: 030 ----
mean loss: 53.66
 ---- batch: 040 ----
mean loss: 53.78
train mean loss: 53.42
epoch train time: 0:00:00.671956
elapsed time: 0:01:48.495726
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 22:05:35.700275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.91
 ---- batch: 020 ----
mean loss: 49.86
 ---- batch: 030 ----
mean loss: 53.04
 ---- batch: 040 ----
mean loss: 54.28
train mean loss: 52.67
epoch train time: 0:00:00.655247
elapsed time: 0:01:49.151241
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 22:05:36.355809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.88
 ---- batch: 020 ----
mean loss: 53.64
 ---- batch: 030 ----
mean loss: 54.94
 ---- batch: 040 ----
mean loss: 52.89
train mean loss: 52.88
epoch train time: 0:00:00.658940
elapsed time: 0:01:49.810432
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 22:05:37.014986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.79
 ---- batch: 020 ----
mean loss: 57.04
 ---- batch: 030 ----
mean loss: 53.66
 ---- batch: 040 ----
mean loss: 50.14
train mean loss: 52.56
epoch train time: 0:00:00.668559
elapsed time: 0:01:50.479263
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 22:05:37.683822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.45
 ---- batch: 020 ----
mean loss: 52.17
 ---- batch: 030 ----
mean loss: 52.97
 ---- batch: 040 ----
mean loss: 52.13
train mean loss: 51.70
epoch train time: 0:00:00.697811
elapsed time: 0:01:51.177330
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 22:05:38.381887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.65
 ---- batch: 020 ----
mean loss: 53.70
 ---- batch: 030 ----
mean loss: 52.67
 ---- batch: 040 ----
mean loss: 51.01
train mean loss: 52.32
epoch train time: 0:00:00.682881
elapsed time: 0:01:51.860446
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 22:05:39.064998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.38
 ---- batch: 020 ----
mean loss: 48.89
 ---- batch: 030 ----
mean loss: 51.26
 ---- batch: 040 ----
mean loss: 53.60
train mean loss: 51.98
epoch train time: 0:00:00.658076
elapsed time: 0:01:52.518748
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 22:05:39.723311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.10
 ---- batch: 020 ----
mean loss: 52.88
 ---- batch: 030 ----
mean loss: 49.49
 ---- batch: 040 ----
mean loss: 53.01
train mean loss: 51.34
epoch train time: 0:00:00.660607
elapsed time: 0:01:53.179572
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 22:05:40.384119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.67
 ---- batch: 020 ----
mean loss: 49.40
 ---- batch: 030 ----
mean loss: 51.31
 ---- batch: 040 ----
mean loss: 50.85
train mean loss: 51.31
epoch train time: 0:00:00.652029
elapsed time: 0:01:53.831806
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 22:05:41.036406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.04
 ---- batch: 020 ----
mean loss: 53.27
 ---- batch: 030 ----
mean loss: 50.44
 ---- batch: 040 ----
mean loss: 49.90
train mean loss: 50.59
epoch train time: 0:00:00.672351
elapsed time: 0:01:54.504517
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 22:05:41.709035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.85
 ---- batch: 020 ----
mean loss: 50.74
 ---- batch: 030 ----
mean loss: 51.43
 ---- batch: 040 ----
mean loss: 50.96
train mean loss: 50.96
epoch train time: 0:00:00.708769
elapsed time: 0:01:55.213499
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 22:05:42.418054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.28
 ---- batch: 020 ----
mean loss: 50.15
 ---- batch: 030 ----
mean loss: 51.53
 ---- batch: 040 ----
mean loss: 52.72
train mean loss: 50.54
epoch train time: 0:00:00.688516
elapsed time: 0:01:55.902286
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 22:05:43.106845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.91
 ---- batch: 020 ----
mean loss: 49.18
 ---- batch: 030 ----
mean loss: 48.84
 ---- batch: 040 ----
mean loss: 49.75
train mean loss: 49.58
epoch train time: 0:00:00.691818
elapsed time: 0:01:56.594392
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 22:05:43.798940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.50
 ---- batch: 020 ----
mean loss: 50.84
 ---- batch: 030 ----
mean loss: 49.29
 ---- batch: 040 ----
mean loss: 49.97
train mean loss: 50.41
epoch train time: 0:00:00.645173
elapsed time: 0:01:57.239766
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 22:05:44.444315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.61
 ---- batch: 020 ----
mean loss: 51.83
 ---- batch: 030 ----
mean loss: 47.21
 ---- batch: 040 ----
mean loss: 49.23
train mean loss: 49.66
epoch train time: 0:00:00.659175
elapsed time: 0:01:57.899162
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 22:05:45.103741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.64
 ---- batch: 020 ----
mean loss: 49.78
 ---- batch: 030 ----
mean loss: 48.77
 ---- batch: 040 ----
mean loss: 50.05
train mean loss: 49.24
epoch train time: 0:00:00.669696
elapsed time: 0:01:58.569134
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 22:05:45.773685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.66
 ---- batch: 020 ----
mean loss: 50.40
 ---- batch: 030 ----
mean loss: 50.88
 ---- batch: 040 ----
mean loss: 48.49
train mean loss: 48.93
epoch train time: 0:00:00.669426
elapsed time: 0:01:59.238874
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 22:05:46.443447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.02
 ---- batch: 020 ----
mean loss: 48.28
 ---- batch: 030 ----
mean loss: 48.26
 ---- batch: 040 ----
mean loss: 49.23
train mean loss: 48.23
epoch train time: 0:00:00.675624
elapsed time: 0:01:59.914800
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 22:05:47.119355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.76
 ---- batch: 020 ----
mean loss: 49.57
 ---- batch: 030 ----
mean loss: 48.74
 ---- batch: 040 ----
mean loss: 48.59
train mean loss: 48.21
epoch train time: 0:00:00.674260
elapsed time: 0:02:00.589273
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 22:05:47.793838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.34
 ---- batch: 020 ----
mean loss: 47.71
 ---- batch: 030 ----
mean loss: 48.48
 ---- batch: 040 ----
mean loss: 47.35
train mean loss: 47.78
epoch train time: 0:00:00.645850
elapsed time: 0:02:01.235356
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 22:05:48.439903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.55
 ---- batch: 020 ----
mean loss: 48.05
 ---- batch: 030 ----
mean loss: 47.44
 ---- batch: 040 ----
mean loss: 51.27
train mean loss: 48.58
epoch train time: 0:00:00.652072
elapsed time: 0:02:01.887629
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 22:05:49.092174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.27
 ---- batch: 020 ----
mean loss: 46.04
 ---- batch: 030 ----
mean loss: 46.81
 ---- batch: 040 ----
mean loss: 49.61
train mean loss: 47.33
epoch train time: 0:00:00.647830
elapsed time: 0:02:02.535693
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 22:05:49.740266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.44
 ---- batch: 020 ----
mean loss: 44.83
 ---- batch: 030 ----
mean loss: 48.65
 ---- batch: 040 ----
mean loss: 50.34
train mean loss: 47.60
epoch train time: 0:00:00.669689
elapsed time: 0:02:03.205670
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 22:05:50.410236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.49
 ---- batch: 020 ----
mean loss: 48.32
 ---- batch: 030 ----
mean loss: 45.64
 ---- batch: 040 ----
mean loss: 48.65
train mean loss: 47.56
epoch train time: 0:00:00.674644
elapsed time: 0:02:03.880607
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 22:05:51.085169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.85
 ---- batch: 020 ----
mean loss: 45.82
 ---- batch: 030 ----
mean loss: 47.80
 ---- batch: 040 ----
mean loss: 48.45
train mean loss: 46.80
epoch train time: 0:00:00.666473
elapsed time: 0:02:04.547342
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 22:05:51.751897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.57
 ---- batch: 020 ----
mean loss: 46.12
 ---- batch: 030 ----
mean loss: 47.61
 ---- batch: 040 ----
mean loss: 46.36
train mean loss: 46.69
epoch train time: 0:00:00.656345
elapsed time: 0:02:05.203933
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 22:05:52.408500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.42
 ---- batch: 020 ----
mean loss: 45.13
 ---- batch: 030 ----
mean loss: 46.43
 ---- batch: 040 ----
mean loss: 46.70
train mean loss: 46.18
epoch train time: 0:00:00.660150
elapsed time: 0:02:05.864311
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 22:05:53.068858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.18
 ---- batch: 020 ----
mean loss: 45.71
 ---- batch: 030 ----
mean loss: 47.34
 ---- batch: 040 ----
mean loss: 44.48
train mean loss: 46.34
epoch train time: 0:00:00.675210
elapsed time: 0:02:06.539767
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 22:05:53.744346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.71
 ---- batch: 020 ----
mean loss: 45.41
 ---- batch: 030 ----
mean loss: 45.34
 ---- batch: 040 ----
mean loss: 46.46
train mean loss: 46.16
epoch train time: 0:00:00.714361
elapsed time: 0:02:07.254421
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 22:05:54.458992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.06
 ---- batch: 020 ----
mean loss: 46.41
 ---- batch: 030 ----
mean loss: 46.68
 ---- batch: 040 ----
mean loss: 43.57
train mean loss: 45.61
epoch train time: 0:00:00.690390
elapsed time: 0:02:07.945108
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 22:05:55.149677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.43
 ---- batch: 020 ----
mean loss: 42.95
 ---- batch: 030 ----
mean loss: 47.54
 ---- batch: 040 ----
mean loss: 46.70
train mean loss: 45.34
epoch train time: 0:00:00.677394
elapsed time: 0:02:08.622732
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 22:05:55.827331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.76
 ---- batch: 020 ----
mean loss: 43.16
 ---- batch: 030 ----
mean loss: 45.39
 ---- batch: 040 ----
mean loss: 45.51
train mean loss: 44.86
epoch train time: 0:00:00.651479
elapsed time: 0:02:09.274505
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 22:05:56.479056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.43
 ---- batch: 020 ----
mean loss: 43.14
 ---- batch: 030 ----
mean loss: 46.92
 ---- batch: 040 ----
mean loss: 45.05
train mean loss: 45.12
epoch train time: 0:00:00.662238
elapsed time: 0:02:09.936980
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 22:05:57.141532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.85
 ---- batch: 020 ----
mean loss: 46.67
 ---- batch: 030 ----
mean loss: 45.34
 ---- batch: 040 ----
mean loss: 46.72
train mean loss: 45.45
epoch train time: 0:00:00.652513
elapsed time: 0:02:10.589760
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 22:05:57.794335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.70
 ---- batch: 020 ----
mean loss: 46.79
 ---- batch: 030 ----
mean loss: 45.59
 ---- batch: 040 ----
mean loss: 42.63
train mean loss: 44.65
epoch train time: 0:00:00.676864
elapsed time: 0:02:11.266921
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 22:05:58.471467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.20
 ---- batch: 020 ----
mean loss: 44.98
 ---- batch: 030 ----
mean loss: 44.23
 ---- batch: 040 ----
mean loss: 42.81
train mean loss: 43.83
epoch train time: 0:00:00.689216
elapsed time: 0:02:11.956388
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 22:05:59.160950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.20
 ---- batch: 020 ----
mean loss: 44.08
 ---- batch: 030 ----
mean loss: 46.02
 ---- batch: 040 ----
mean loss: 42.11
train mean loss: 43.96
epoch train time: 0:00:00.689203
elapsed time: 0:02:12.645827
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 22:05:59.850399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.42
 ---- batch: 020 ----
mean loss: 43.75
 ---- batch: 030 ----
mean loss: 43.59
 ---- batch: 040 ----
mean loss: 44.91
train mean loss: 43.24
epoch train time: 0:00:00.651172
elapsed time: 0:02:13.297228
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 22:06:00.501805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.09
 ---- batch: 020 ----
mean loss: 42.52
 ---- batch: 030 ----
mean loss: 40.84
 ---- batch: 040 ----
mean loss: 44.94
train mean loss: 42.88
epoch train time: 0:00:00.654293
elapsed time: 0:02:13.951766
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 22:06:01.156307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.81
 ---- batch: 020 ----
mean loss: 43.45
 ---- batch: 030 ----
mean loss: 41.47
 ---- batch: 040 ----
mean loss: 43.40
train mean loss: 43.35
epoch train time: 0:00:00.678305
elapsed time: 0:02:14.630298
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 22:06:01.834852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.17
 ---- batch: 020 ----
mean loss: 43.20
 ---- batch: 030 ----
mean loss: 43.24
 ---- batch: 040 ----
mean loss: 41.85
train mean loss: 42.46
epoch train time: 0:00:00.675850
elapsed time: 0:02:15.306398
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 22:06:02.510949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.35
 ---- batch: 020 ----
mean loss: 44.98
 ---- batch: 030 ----
mean loss: 41.20
 ---- batch: 040 ----
mean loss: 41.70
train mean loss: 42.84
epoch train time: 0:00:00.707093
elapsed time: 0:02:16.013712
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 22:06:03.218272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.58
 ---- batch: 020 ----
mean loss: 41.14
 ---- batch: 030 ----
mean loss: 41.11
 ---- batch: 040 ----
mean loss: 43.62
train mean loss: 42.47
epoch train time: 0:00:00.678888
elapsed time: 0:02:16.692817
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 22:06:03.897379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.17
 ---- batch: 020 ----
mean loss: 42.91
 ---- batch: 030 ----
mean loss: 41.22
 ---- batch: 040 ----
mean loss: 43.23
train mean loss: 42.67
epoch train time: 0:00:00.640974
elapsed time: 0:02:17.334006
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 22:06:04.538565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.54
 ---- batch: 020 ----
mean loss: 43.31
 ---- batch: 030 ----
mean loss: 41.22
 ---- batch: 040 ----
mean loss: 41.99
train mean loss: 42.03
epoch train time: 0:00:00.661722
elapsed time: 0:02:17.995995
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 22:06:05.200545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.73
 ---- batch: 020 ----
mean loss: 42.87
 ---- batch: 030 ----
mean loss: 39.80
 ---- batch: 040 ----
mean loss: 42.80
train mean loss: 41.23
epoch train time: 0:00:00.649237
elapsed time: 0:02:18.645504
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 22:06:05.850070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.66
 ---- batch: 020 ----
mean loss: 40.11
 ---- batch: 030 ----
mean loss: 42.92
 ---- batch: 040 ----
mean loss: 40.74
train mean loss: 41.26
epoch train time: 0:00:00.673902
elapsed time: 0:02:19.319662
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 22:06:06.524214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.86
 ---- batch: 020 ----
mean loss: 43.48
 ---- batch: 030 ----
mean loss: 42.19
 ---- batch: 040 ----
mean loss: 35.85
train mean loss: 41.29
epoch train time: 0:00:00.689945
elapsed time: 0:02:20.009848
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 22:06:07.214417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.85
 ---- batch: 020 ----
mean loss: 40.59
 ---- batch: 030 ----
mean loss: 39.95
 ---- batch: 040 ----
mean loss: 42.59
train mean loss: 41.00
epoch train time: 0:00:00.706664
elapsed time: 0:02:20.716755
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 22:06:07.921310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.07
 ---- batch: 020 ----
mean loss: 40.05
 ---- batch: 030 ----
mean loss: 41.79
 ---- batch: 040 ----
mean loss: 42.68
train mean loss: 40.37
epoch train time: 0:00:00.673029
elapsed time: 0:02:21.390002
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 22:06:08.594550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.45
 ---- batch: 020 ----
mean loss: 39.06
 ---- batch: 030 ----
mean loss: 42.53
 ---- batch: 040 ----
mean loss: 41.59
train mean loss: 40.61
epoch train time: 0:00:00.656848
elapsed time: 0:02:22.047073
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 22:06:09.251662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.99
 ---- batch: 020 ----
mean loss: 40.18
 ---- batch: 030 ----
mean loss: 38.57
 ---- batch: 040 ----
mean loss: 40.68
train mean loss: 39.68
epoch train time: 0:00:00.638832
elapsed time: 0:02:22.686207
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 22:06:09.890809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.28
 ---- batch: 020 ----
mean loss: 38.84
 ---- batch: 030 ----
mean loss: 41.29
 ---- batch: 040 ----
mean loss: 38.68
train mean loss: 39.62
epoch train time: 0:00:00.674474
elapsed time: 0:02:23.360977
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 22:06:10.565526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.77
 ---- batch: 020 ----
mean loss: 39.44
 ---- batch: 030 ----
mean loss: 39.07
 ---- batch: 040 ----
mean loss: 38.27
train mean loss: 39.43
epoch train time: 0:00:00.679245
elapsed time: 0:02:24.040487
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 22:06:11.245053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.57
 ---- batch: 020 ----
mean loss: 38.05
 ---- batch: 030 ----
mean loss: 40.39
 ---- batch: 040 ----
mean loss: 39.26
train mean loss: 39.43
epoch train time: 0:00:00.680569
elapsed time: 0:02:24.721336
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 22:06:11.925921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.84
 ---- batch: 020 ----
mean loss: 39.94
 ---- batch: 030 ----
mean loss: 40.19
 ---- batch: 040 ----
mean loss: 37.52
train mean loss: 39.40
epoch train time: 0:00:00.657655
elapsed time: 0:02:25.379235
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 22:06:12.583780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.58
 ---- batch: 020 ----
mean loss: 38.53
 ---- batch: 030 ----
mean loss: 39.24
 ---- batch: 040 ----
mean loss: 38.77
train mean loss: 39.18
epoch train time: 0:00:00.656041
elapsed time: 0:02:26.035501
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 22:06:13.240068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.21
 ---- batch: 020 ----
mean loss: 40.09
 ---- batch: 030 ----
mean loss: 38.58
 ---- batch: 040 ----
mean loss: 37.93
train mean loss: 38.58
epoch train time: 0:00:00.666222
elapsed time: 0:02:26.702002
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 22:06:13.906554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.50
 ---- batch: 020 ----
mean loss: 38.62
 ---- batch: 030 ----
mean loss: 38.12
 ---- batch: 040 ----
mean loss: 38.91
train mean loss: 38.55
epoch train time: 0:00:00.684476
elapsed time: 0:02:27.386700
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 22:06:14.591248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.74
 ---- batch: 020 ----
mean loss: 37.07
 ---- batch: 030 ----
mean loss: 38.48
 ---- batch: 040 ----
mean loss: 38.32
train mean loss: 37.92
epoch train time: 0:00:00.703845
elapsed time: 0:02:28.090795
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 22:06:15.295364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.20
 ---- batch: 020 ----
mean loss: 37.02
 ---- batch: 030 ----
mean loss: 38.51
 ---- batch: 040 ----
mean loss: 38.87
train mean loss: 37.72
epoch train time: 0:00:00.675350
elapsed time: 0:02:28.766407
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 22:06:15.970961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.81
 ---- batch: 020 ----
mean loss: 37.29
 ---- batch: 030 ----
mean loss: 38.36
 ---- batch: 040 ----
mean loss: 38.58
train mean loss: 37.93
epoch train time: 0:00:00.644833
elapsed time: 0:02:29.411458
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 22:06:16.616006
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.38
 ---- batch: 020 ----
mean loss: 36.40
 ---- batch: 030 ----
mean loss: 37.14
 ---- batch: 040 ----
mean loss: 37.90
train mean loss: 36.42
epoch train time: 0:00:00.645345
elapsed time: 0:02:30.057082
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 22:06:17.261596
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.98
 ---- batch: 020 ----
mean loss: 38.22
 ---- batch: 030 ----
mean loss: 36.49
 ---- batch: 040 ----
mean loss: 36.77
train mean loss: 36.67
epoch train time: 0:00:00.684148
elapsed time: 0:02:30.741470
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 22:06:17.946027
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.58
 ---- batch: 020 ----
mean loss: 36.62
 ---- batch: 030 ----
mean loss: 35.36
 ---- batch: 040 ----
mean loss: 35.16
train mean loss: 36.34
epoch train time: 0:00:00.690996
elapsed time: 0:02:31.432723
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 22:06:18.637276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.38
 ---- batch: 020 ----
mean loss: 35.34
 ---- batch: 030 ----
mean loss: 36.55
 ---- batch: 040 ----
mean loss: 36.79
train mean loss: 35.89
epoch train time: 0:00:00.687079
elapsed time: 0:02:32.120048
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 22:06:19.324634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.54
 ---- batch: 020 ----
mean loss: 35.05
 ---- batch: 030 ----
mean loss: 36.61
 ---- batch: 040 ----
mean loss: 36.20
train mean loss: 36.16
epoch train time: 0:00:00.680418
elapsed time: 0:02:32.800719
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 22:06:20.005295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.02
 ---- batch: 020 ----
mean loss: 34.55
 ---- batch: 030 ----
mean loss: 37.15
 ---- batch: 040 ----
mean loss: 35.38
train mean loss: 36.10
epoch train time: 0:00:00.652780
elapsed time: 0:02:33.453749
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 22:06:20.658334
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.01
 ---- batch: 020 ----
mean loss: 34.59
 ---- batch: 030 ----
mean loss: 35.51
 ---- batch: 040 ----
mean loss: 36.59
train mean loss: 36.20
epoch train time: 0:00:00.669272
elapsed time: 0:02:34.123276
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 22:06:21.327838
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.19
 ---- batch: 020 ----
mean loss: 35.34
 ---- batch: 030 ----
mean loss: 35.19
 ---- batch: 040 ----
mean loss: 35.89
train mean loss: 36.20
epoch train time: 0:00:00.658032
elapsed time: 0:02:34.781549
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 22:06:21.986101
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.48
 ---- batch: 020 ----
mean loss: 36.64
 ---- batch: 030 ----
mean loss: 34.76
 ---- batch: 040 ----
mean loss: 38.29
train mean loss: 36.21
epoch train time: 0:00:00.682196
elapsed time: 0:02:35.464067
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 22:06:22.668625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.94
 ---- batch: 020 ----
mean loss: 36.80
 ---- batch: 030 ----
mean loss: 37.06
 ---- batch: 040 ----
mean loss: 34.33
train mean loss: 36.27
epoch train time: 0:00:00.661567
elapsed time: 0:02:36.125940
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 22:06:23.330488
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.48
 ---- batch: 020 ----
mean loss: 35.85
 ---- batch: 030 ----
mean loss: 34.93
 ---- batch: 040 ----
mean loss: 35.78
train mean loss: 35.95
epoch train time: 0:00:00.664020
elapsed time: 0:02:36.790194
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 22:06:23.994742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.18
 ---- batch: 020 ----
mean loss: 34.75
 ---- batch: 030 ----
mean loss: 35.19
 ---- batch: 040 ----
mean loss: 36.65
train mean loss: 35.98
epoch train time: 0:00:00.656407
elapsed time: 0:02:37.446833
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 22:06:24.651382
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.04
 ---- batch: 020 ----
mean loss: 33.88
 ---- batch: 030 ----
mean loss: 34.66
 ---- batch: 040 ----
mean loss: 37.97
train mean loss: 36.15
epoch train time: 0:00:00.654701
elapsed time: 0:02:38.101761
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 22:06:25.306310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.47
 ---- batch: 020 ----
mean loss: 36.18
 ---- batch: 030 ----
mean loss: 35.86
 ---- batch: 040 ----
mean loss: 36.39
train mean loss: 36.07
epoch train time: 0:00:00.652856
elapsed time: 0:02:38.754853
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 22:06:25.959444
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.26
 ---- batch: 020 ----
mean loss: 34.82
 ---- batch: 030 ----
mean loss: 36.09
 ---- batch: 040 ----
mean loss: 36.18
train mean loss: 35.82
epoch train time: 0:00:00.669653
elapsed time: 0:02:39.424818
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 22:06:26.629378
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.31
 ---- batch: 020 ----
mean loss: 36.85
 ---- batch: 030 ----
mean loss: 34.89
 ---- batch: 040 ----
mean loss: 34.37
train mean loss: 36.07
epoch train time: 0:00:00.668659
elapsed time: 0:02:40.093734
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 22:06:27.298302
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.80
 ---- batch: 020 ----
mean loss: 35.05
 ---- batch: 030 ----
mean loss: 38.12
 ---- batch: 040 ----
mean loss: 35.66
train mean loss: 36.44
epoch train time: 0:00:00.664569
elapsed time: 0:02:40.758569
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 22:06:27.963133
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.28
 ---- batch: 020 ----
mean loss: 37.78
 ---- batch: 030 ----
mean loss: 35.76
 ---- batch: 040 ----
mean loss: 36.29
train mean loss: 35.89
epoch train time: 0:00:00.644478
elapsed time: 0:02:41.403328
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 22:06:28.607881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.19
 ---- batch: 020 ----
mean loss: 37.21
 ---- batch: 030 ----
mean loss: 36.07
 ---- batch: 040 ----
mean loss: 34.03
train mean loss: 35.96
epoch train time: 0:00:00.649922
elapsed time: 0:02:42.053465
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 22:06:29.258011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.13
 ---- batch: 020 ----
mean loss: 37.03
 ---- batch: 030 ----
mean loss: 36.94
 ---- batch: 040 ----
mean loss: 35.05
train mean loss: 35.76
epoch train time: 0:00:00.635615
elapsed time: 0:02:42.689287
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 22:06:29.893873
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.46
 ---- batch: 020 ----
mean loss: 35.28
 ---- batch: 030 ----
mean loss: 36.04
 ---- batch: 040 ----
mean loss: 36.58
train mean loss: 35.84
epoch train time: 0:00:00.678106
elapsed time: 0:02:43.367748
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 22:06:30.572368
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.42
 ---- batch: 020 ----
mean loss: 36.91
 ---- batch: 030 ----
mean loss: 34.56
 ---- batch: 040 ----
mean loss: 35.11
train mean loss: 35.61
epoch train time: 0:00:00.702274
elapsed time: 0:02:44.070328
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 22:06:31.274879
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.80
 ---- batch: 020 ----
mean loss: 34.73
 ---- batch: 030 ----
mean loss: 35.59
 ---- batch: 040 ----
mean loss: 36.63
train mean loss: 35.71
epoch train time: 0:00:00.680511
elapsed time: 0:02:44.751096
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 22:06:31.955658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.31
 ---- batch: 020 ----
mean loss: 36.27
 ---- batch: 030 ----
mean loss: 34.79
 ---- batch: 040 ----
mean loss: 35.19
train mean loss: 35.60
epoch train time: 0:00:00.648460
elapsed time: 0:02:45.399783
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 22:06:32.604435
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.26
 ---- batch: 020 ----
mean loss: 36.35
 ---- batch: 030 ----
mean loss: 36.25
 ---- batch: 040 ----
mean loss: 35.40
train mean loss: 35.88
epoch train time: 0:00:00.658781
elapsed time: 0:02:46.058893
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 22:06:33.263459
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.94
 ---- batch: 020 ----
mean loss: 36.19
 ---- batch: 030 ----
mean loss: 36.28
 ---- batch: 040 ----
mean loss: 36.01
train mean loss: 35.58
epoch train time: 0:00:00.671939
elapsed time: 0:02:46.731059
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 22:06:33.935607
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.96
 ---- batch: 020 ----
mean loss: 35.87
 ---- batch: 030 ----
mean loss: 36.54
 ---- batch: 040 ----
mean loss: 35.52
train mean loss: 35.56
epoch train time: 0:00:00.693435
elapsed time: 0:02:47.424764
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 22:06:34.629332
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.84
 ---- batch: 020 ----
mean loss: 33.37
 ---- batch: 030 ----
mean loss: 36.60
 ---- batch: 040 ----
mean loss: 35.27
train mean loss: 35.19
epoch train time: 0:00:00.697794
elapsed time: 0:02:48.122844
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 22:06:35.327417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.32
 ---- batch: 020 ----
mean loss: 36.44
 ---- batch: 030 ----
mean loss: 35.91
 ---- batch: 040 ----
mean loss: 36.35
train mean loss: 35.78
epoch train time: 0:00:00.689988
elapsed time: 0:02:48.813111
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 22:06:36.017712
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.06
 ---- batch: 020 ----
mean loss: 36.88
 ---- batch: 030 ----
mean loss: 35.39
 ---- batch: 040 ----
mean loss: 34.04
train mean loss: 35.56
epoch train time: 0:00:00.667334
elapsed time: 0:02:49.480802
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 22:06:36.685354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.97
 ---- batch: 020 ----
mean loss: 33.62
 ---- batch: 030 ----
mean loss: 35.68
 ---- batch: 040 ----
mean loss: 36.42
train mean loss: 35.72
epoch train time: 0:00:00.642803
elapsed time: 0:02:50.123809
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 22:06:37.328348
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.77
 ---- batch: 020 ----
mean loss: 36.83
 ---- batch: 030 ----
mean loss: 35.93
 ---- batch: 040 ----
mean loss: 33.94
train mean loss: 35.54
epoch train time: 0:00:00.658964
elapsed time: 0:02:50.782986
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 22:06:37.987552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.48
 ---- batch: 020 ----
mean loss: 37.24
 ---- batch: 030 ----
mean loss: 33.55
 ---- batch: 040 ----
mean loss: 34.29
train mean loss: 35.35
epoch train time: 0:00:00.672459
elapsed time: 0:02:51.455822
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 22:06:38.660345
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.09
 ---- batch: 020 ----
mean loss: 36.40
 ---- batch: 030 ----
mean loss: 35.80
 ---- batch: 040 ----
mean loss: 35.44
train mean loss: 35.59
epoch train time: 0:00:00.691526
elapsed time: 0:02:52.147549
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 22:06:39.352131
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.15
 ---- batch: 020 ----
mean loss: 35.52
 ---- batch: 030 ----
mean loss: 34.21
 ---- batch: 040 ----
mean loss: 36.20
train mean loss: 35.77
epoch train time: 0:00:00.683483
elapsed time: 0:02:52.831285
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 22:06:40.035834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.89
 ---- batch: 020 ----
mean loss: 35.42
 ---- batch: 030 ----
mean loss: 36.29
 ---- batch: 040 ----
mean loss: 34.67
train mean loss: 35.49
epoch train time: 0:00:00.661872
elapsed time: 0:02:53.493378
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 22:06:40.697928
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.23
 ---- batch: 020 ----
mean loss: 33.84
 ---- batch: 030 ----
mean loss: 35.67
 ---- batch: 040 ----
mean loss: 35.33
train mean loss: 35.35
epoch train time: 0:00:00.636593
elapsed time: 0:02:54.130205
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 22:06:41.334750
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.66
 ---- batch: 020 ----
mean loss: 35.35
 ---- batch: 030 ----
mean loss: 35.51
 ---- batch: 040 ----
mean loss: 35.44
train mean loss: 35.21
epoch train time: 0:00:00.643428
elapsed time: 0:02:54.773827
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 22:06:41.978371
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.14
 ---- batch: 020 ----
mean loss: 36.45
 ---- batch: 030 ----
mean loss: 36.38
 ---- batch: 040 ----
mean loss: 36.15
train mean loss: 35.29
epoch train time: 0:00:00.657568
elapsed time: 0:02:55.431626
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 22:06:42.636178
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.27
 ---- batch: 020 ----
mean loss: 35.43
 ---- batch: 030 ----
mean loss: 34.52
 ---- batch: 040 ----
mean loss: 35.49
train mean loss: 35.49
epoch train time: 0:00:00.698002
elapsed time: 0:02:56.129907
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 22:06:43.334451
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.03
 ---- batch: 020 ----
mean loss: 34.22
 ---- batch: 030 ----
mean loss: 34.19
 ---- batch: 040 ----
mean loss: 36.13
train mean loss: 35.16
epoch train time: 0:00:00.681829
elapsed time: 0:02:56.811999
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 22:06:44.016574
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.65
 ---- batch: 020 ----
mean loss: 35.29
 ---- batch: 030 ----
mean loss: 34.16
 ---- batch: 040 ----
mean loss: 34.44
train mean loss: 35.11
epoch train time: 0:00:00.670092
elapsed time: 0:02:57.482389
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 22:06:44.686950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.59
 ---- batch: 020 ----
mean loss: 33.52
 ---- batch: 030 ----
mean loss: 35.73
 ---- batch: 040 ----
mean loss: 34.77
train mean loss: 35.32
epoch train time: 0:00:00.658661
elapsed time: 0:02:58.141272
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 22:06:45.345820
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.63
 ---- batch: 020 ----
mean loss: 35.80
 ---- batch: 030 ----
mean loss: 36.64
 ---- batch: 040 ----
mean loss: 34.15
train mean loss: 35.11
epoch train time: 0:00:00.655131
elapsed time: 0:02:58.796656
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 22:06:46.001202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.08
 ---- batch: 020 ----
mean loss: 36.20
 ---- batch: 030 ----
mean loss: 35.99
 ---- batch: 040 ----
mean loss: 35.60
train mean loss: 35.15
epoch train time: 0:00:00.674473
elapsed time: 0:02:59.471403
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 22:06:46.675997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.11
 ---- batch: 020 ----
mean loss: 34.10
 ---- batch: 030 ----
mean loss: 35.28
 ---- batch: 040 ----
mean loss: 37.25
train mean loss: 34.99
epoch train time: 0:00:00.681235
elapsed time: 0:03:00.152976
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 22:06:47.357526
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.57
 ---- batch: 020 ----
mean loss: 36.13
 ---- batch: 030 ----
mean loss: 34.24
 ---- batch: 040 ----
mean loss: 36.21
train mean loss: 35.04
epoch train time: 0:00:00.681466
elapsed time: 0:03:00.834707
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 22:06:48.039265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.90
 ---- batch: 020 ----
mean loss: 34.53
 ---- batch: 030 ----
mean loss: 34.59
 ---- batch: 040 ----
mean loss: 34.34
train mean loss: 35.12
epoch train time: 0:00:00.679550
elapsed time: 0:03:01.514476
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 22:06:48.719023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.48
 ---- batch: 020 ----
mean loss: 36.41
 ---- batch: 030 ----
mean loss: 34.39
 ---- batch: 040 ----
mean loss: 34.23
train mean loss: 35.05
epoch train time: 0:00:00.652910
elapsed time: 0:03:02.174573
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_9/checkpoint.pth.tar
**** end time: 2019-09-20 22:06:49.379083 ****
