Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_8', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 22646
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-25 22:04:31.031165 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 21, 24]             100
              Tanh-2           [-1, 10, 21, 24]               0
            Conv2d-3           [-1, 10, 20, 24]           1,000
              Tanh-4           [-1, 10, 20, 24]               0
            Conv2d-5           [-1, 10, 21, 24]           1,000
              Tanh-6           [-1, 10, 21, 24]               0
            Conv2d-7           [-1, 10, 20, 24]           1,000
              Tanh-8           [-1, 10, 20, 24]               0
            Conv2d-9            [-1, 1, 20, 24]              30
             Tanh-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
          Dropout-12                  [-1, 480]               0
           Linear-13                  [-1, 100]          48,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 51,230
Trainable params: 51,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 22:04:31.039660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3850.63
 ---- batch: 020 ----
mean loss: 1722.79
 ---- batch: 030 ----
mean loss: 1247.18
 ---- batch: 040 ----
mean loss: 1085.08
 ---- batch: 050 ----
mean loss: 1022.49
 ---- batch: 060 ----
mean loss: 985.15
 ---- batch: 070 ----
mean loss: 947.17
 ---- batch: 080 ----
mean loss: 922.38
 ---- batch: 090 ----
mean loss: 912.12
train mean loss: 1377.96
epoch train time: 0:00:35.412330
elapsed time: 0:00:35.423617
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 22:05:06.454861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.79
 ---- batch: 020 ----
mean loss: 850.73
 ---- batch: 030 ----
mean loss: 837.53
 ---- batch: 040 ----
mean loss: 812.14
 ---- batch: 050 ----
mean loss: 803.05
 ---- batch: 060 ----
mean loss: 799.66
 ---- batch: 070 ----
mean loss: 770.70
 ---- batch: 080 ----
mean loss: 770.62
 ---- batch: 090 ----
mean loss: 779.83
train mean loss: 805.85
epoch train time: 0:00:02.440452
elapsed time: 0:00:37.864320
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 22:05:08.895557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.85
 ---- batch: 020 ----
mean loss: 709.94
 ---- batch: 030 ----
mean loss: 715.29
 ---- batch: 040 ----
mean loss: 709.99
 ---- batch: 050 ----
mean loss: 690.47
 ---- batch: 060 ----
mean loss: 670.28
 ---- batch: 070 ----
mean loss: 674.49
 ---- batch: 080 ----
mean loss: 673.92
 ---- batch: 090 ----
mean loss: 657.89
train mean loss: 688.75
epoch train time: 0:00:02.368419
elapsed time: 0:00:40.232935
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 22:05:11.264164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 657.69
 ---- batch: 020 ----
mean loss: 642.74
 ---- batch: 030 ----
mean loss: 632.02
 ---- batch: 040 ----
mean loss: 619.59
 ---- batch: 050 ----
mean loss: 612.76
 ---- batch: 060 ----
mean loss: 594.77
 ---- batch: 070 ----
mean loss: 590.75
 ---- batch: 080 ----
mean loss: 581.71
 ---- batch: 090 ----
mean loss: 575.09
train mean loss: 610.53
epoch train time: 0:00:02.359979
elapsed time: 0:00:42.593099
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 22:05:13.624344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 594.61
 ---- batch: 020 ----
mean loss: 580.91
 ---- batch: 030 ----
mean loss: 584.02
 ---- batch: 040 ----
mean loss: 543.26
 ---- batch: 050 ----
mean loss: 569.87
 ---- batch: 060 ----
mean loss: 553.36
 ---- batch: 070 ----
mean loss: 557.77
 ---- batch: 080 ----
mean loss: 557.69
 ---- batch: 090 ----
mean loss: 529.88
train mean loss: 562.07
epoch train time: 0:00:02.358571
elapsed time: 0:00:44.951880
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 22:05:15.983109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.99
 ---- batch: 020 ----
mean loss: 539.69
 ---- batch: 030 ----
mean loss: 529.71
 ---- batch: 040 ----
mean loss: 524.37
 ---- batch: 050 ----
mean loss: 520.36
 ---- batch: 060 ----
mean loss: 493.43
 ---- batch: 070 ----
mean loss: 500.33
 ---- batch: 080 ----
mean loss: 500.67
 ---- batch: 090 ----
mean loss: 484.09
train mean loss: 513.40
epoch train time: 0:00:02.371487
elapsed time: 0:00:47.323564
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 22:05:18.354804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.39
 ---- batch: 020 ----
mean loss: 468.20
 ---- batch: 030 ----
mean loss: 464.27
 ---- batch: 040 ----
mean loss: 477.90
 ---- batch: 050 ----
mean loss: 471.41
 ---- batch: 060 ----
mean loss: 485.29
 ---- batch: 070 ----
mean loss: 459.91
 ---- batch: 080 ----
mean loss: 456.19
 ---- batch: 090 ----
mean loss: 460.12
train mean loss: 467.48
epoch train time: 0:00:02.362072
elapsed time: 0:00:49.685839
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 22:05:20.717084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.91
 ---- batch: 020 ----
mean loss: 460.17
 ---- batch: 030 ----
mean loss: 444.52
 ---- batch: 040 ----
mean loss: 468.27
 ---- batch: 050 ----
mean loss: 452.08
 ---- batch: 060 ----
mean loss: 446.05
 ---- batch: 070 ----
mean loss: 456.46
 ---- batch: 080 ----
mean loss: 457.60
 ---- batch: 090 ----
mean loss: 444.66
train mean loss: 453.47
epoch train time: 0:00:02.360909
elapsed time: 0:00:52.046964
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 22:05:23.078187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.89
 ---- batch: 020 ----
mean loss: 428.37
 ---- batch: 030 ----
mean loss: 446.53
 ---- batch: 040 ----
mean loss: 452.36
 ---- batch: 050 ----
mean loss: 436.27
 ---- batch: 060 ----
mean loss: 448.32
 ---- batch: 070 ----
mean loss: 431.66
 ---- batch: 080 ----
mean loss: 456.19
 ---- batch: 090 ----
mean loss: 434.07
train mean loss: 441.77
epoch train time: 0:00:02.361789
elapsed time: 0:00:54.408935
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 22:05:25.440175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.14
 ---- batch: 020 ----
mean loss: 439.49
 ---- batch: 030 ----
mean loss: 435.35
 ---- batch: 040 ----
mean loss: 440.56
 ---- batch: 050 ----
mean loss: 431.41
 ---- batch: 060 ----
mean loss: 443.03
 ---- batch: 070 ----
mean loss: 437.61
 ---- batch: 080 ----
mean loss: 430.05
 ---- batch: 090 ----
mean loss: 428.40
train mean loss: 438.79
epoch train time: 0:00:02.375636
elapsed time: 0:00:56.784815
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 22:05:27.816076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.48
 ---- batch: 020 ----
mean loss: 432.94
 ---- batch: 030 ----
mean loss: 430.61
 ---- batch: 040 ----
mean loss: 454.75
 ---- batch: 050 ----
mean loss: 450.72
 ---- batch: 060 ----
mean loss: 443.18
 ---- batch: 070 ----
mean loss: 450.97
 ---- batch: 080 ----
mean loss: 449.12
 ---- batch: 090 ----
mean loss: 434.52
train mean loss: 442.94
epoch train time: 0:00:02.360251
elapsed time: 0:00:59.145280
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 22:05:30.176503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.62
 ---- batch: 020 ----
mean loss: 422.29
 ---- batch: 030 ----
mean loss: 420.70
 ---- batch: 040 ----
mean loss: 425.55
 ---- batch: 050 ----
mean loss: 428.68
 ---- batch: 060 ----
mean loss: 414.14
 ---- batch: 070 ----
mean loss: 431.50
 ---- batch: 080 ----
mean loss: 434.19
 ---- batch: 090 ----
mean loss: 428.68
train mean loss: 425.52
epoch train time: 0:00:02.360267
elapsed time: 0:01:01.505746
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 22:05:32.536973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.22
 ---- batch: 020 ----
mean loss: 414.82
 ---- batch: 030 ----
mean loss: 425.98
 ---- batch: 040 ----
mean loss: 425.13
 ---- batch: 050 ----
mean loss: 438.45
 ---- batch: 060 ----
mean loss: 441.59
 ---- batch: 070 ----
mean loss: 417.28
 ---- batch: 080 ----
mean loss: 433.99
 ---- batch: 090 ----
mean loss: 424.91
train mean loss: 424.71
epoch train time: 0:00:02.358280
elapsed time: 0:01:03.864217
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 22:05:34.895472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.93
 ---- batch: 020 ----
mean loss: 428.11
 ---- batch: 030 ----
mean loss: 417.98
 ---- batch: 040 ----
mean loss: 423.96
 ---- batch: 050 ----
mean loss: 421.95
 ---- batch: 060 ----
mean loss: 417.99
 ---- batch: 070 ----
mean loss: 423.36
 ---- batch: 080 ----
mean loss: 430.60
 ---- batch: 090 ----
mean loss: 423.17
train mean loss: 423.74
epoch train time: 0:00:02.355583
elapsed time: 0:01:06.220007
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 22:05:37.251236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.05
 ---- batch: 020 ----
mean loss: 419.54
 ---- batch: 030 ----
mean loss: 417.72
 ---- batch: 040 ----
mean loss: 424.36
 ---- batch: 050 ----
mean loss: 417.51
 ---- batch: 060 ----
mean loss: 434.74
 ---- batch: 070 ----
mean loss: 410.76
 ---- batch: 080 ----
mean loss: 426.18
 ---- batch: 090 ----
mean loss: 414.27
train mean loss: 419.02
epoch train time: 0:00:02.355289
elapsed time: 0:01:08.575492
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 22:05:39.606733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.74
 ---- batch: 020 ----
mean loss: 451.10
 ---- batch: 030 ----
mean loss: 437.32
 ---- batch: 040 ----
mean loss: 412.53
 ---- batch: 050 ----
mean loss: 414.28
 ---- batch: 060 ----
mean loss: 411.85
 ---- batch: 070 ----
mean loss: 411.83
 ---- batch: 080 ----
mean loss: 410.91
 ---- batch: 090 ----
mean loss: 423.06
train mean loss: 420.45
epoch train time: 0:00:02.338173
elapsed time: 0:01:10.913868
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 22:05:41.945097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.14
 ---- batch: 020 ----
mean loss: 428.28
 ---- batch: 030 ----
mean loss: 421.56
 ---- batch: 040 ----
mean loss: 418.18
 ---- batch: 050 ----
mean loss: 406.52
 ---- batch: 060 ----
mean loss: 422.68
 ---- batch: 070 ----
mean loss: 420.23
 ---- batch: 080 ----
mean loss: 415.42
 ---- batch: 090 ----
mean loss: 413.60
train mean loss: 419.04
epoch train time: 0:00:02.357161
elapsed time: 0:01:13.271235
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 22:05:44.302460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.25
 ---- batch: 020 ----
mean loss: 405.55
 ---- batch: 030 ----
mean loss: 413.91
 ---- batch: 040 ----
mean loss: 409.99
 ---- batch: 050 ----
mean loss: 412.17
 ---- batch: 060 ----
mean loss: 423.30
 ---- batch: 070 ----
mean loss: 418.74
 ---- batch: 080 ----
mean loss: 425.56
 ---- batch: 090 ----
mean loss: 417.40
train mean loss: 416.42
epoch train time: 0:00:02.349350
elapsed time: 0:01:15.620805
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 22:05:46.652078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.90
 ---- batch: 020 ----
mean loss: 408.67
 ---- batch: 030 ----
mean loss: 406.13
 ---- batch: 040 ----
mean loss: 425.72
 ---- batch: 050 ----
mean loss: 422.95
 ---- batch: 060 ----
mean loss: 415.95
 ---- batch: 070 ----
mean loss: 408.72
 ---- batch: 080 ----
mean loss: 412.51
 ---- batch: 090 ----
mean loss: 413.97
train mean loss: 415.39
epoch train time: 0:00:02.340810
elapsed time: 0:01:17.961845
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 22:05:48.993069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.21
 ---- batch: 020 ----
mean loss: 411.76
 ---- batch: 030 ----
mean loss: 405.32
 ---- batch: 040 ----
mean loss: 420.23
 ---- batch: 050 ----
mean loss: 409.45
 ---- batch: 060 ----
mean loss: 413.77
 ---- batch: 070 ----
mean loss: 411.63
 ---- batch: 080 ----
mean loss: 418.07
 ---- batch: 090 ----
mean loss: 430.98
train mean loss: 413.51
epoch train time: 0:00:02.342604
elapsed time: 0:01:20.304648
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 22:05:51.335860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.67
 ---- batch: 020 ----
mean loss: 419.34
 ---- batch: 030 ----
mean loss: 418.34
 ---- batch: 040 ----
mean loss: 428.35
 ---- batch: 050 ----
mean loss: 409.52
 ---- batch: 060 ----
mean loss: 448.69
 ---- batch: 070 ----
mean loss: 436.23
 ---- batch: 080 ----
mean loss: 435.76
 ---- batch: 090 ----
mean loss: 425.10
train mean loss: 426.48
epoch train time: 0:00:02.350924
elapsed time: 0:01:22.655770
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 22:05:53.686983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.34
 ---- batch: 020 ----
mean loss: 410.55
 ---- batch: 030 ----
mean loss: 423.37
 ---- batch: 040 ----
mean loss: 407.68
 ---- batch: 050 ----
mean loss: 395.48
 ---- batch: 060 ----
mean loss: 413.04
 ---- batch: 070 ----
mean loss: 416.35
 ---- batch: 080 ----
mean loss: 414.82
 ---- batch: 090 ----
mean loss: 430.27
train mean loss: 414.46
epoch train time: 0:00:02.344824
elapsed time: 0:01:25.000767
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 22:05:56.032014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.05
 ---- batch: 020 ----
mean loss: 414.49
 ---- batch: 030 ----
mean loss: 416.08
 ---- batch: 040 ----
mean loss: 402.72
 ---- batch: 050 ----
mean loss: 402.67
 ---- batch: 060 ----
mean loss: 414.34
 ---- batch: 070 ----
mean loss: 410.37
 ---- batch: 080 ----
mean loss: 405.28
 ---- batch: 090 ----
mean loss: 416.05
train mean loss: 412.17
epoch train time: 0:00:02.341763
elapsed time: 0:01:27.342736
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 22:05:58.373968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.23
 ---- batch: 020 ----
mean loss: 412.31
 ---- batch: 030 ----
mean loss: 401.89
 ---- batch: 040 ----
mean loss: 407.08
 ---- batch: 050 ----
mean loss: 418.61
 ---- batch: 060 ----
mean loss: 409.24
 ---- batch: 070 ----
mean loss: 406.40
 ---- batch: 080 ----
mean loss: 396.61
 ---- batch: 090 ----
mean loss: 411.22
train mean loss: 410.35
epoch train time: 0:00:02.337296
elapsed time: 0:01:29.680252
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 22:06:00.711478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.44
 ---- batch: 020 ----
mean loss: 409.69
 ---- batch: 030 ----
mean loss: 423.60
 ---- batch: 040 ----
mean loss: 408.99
 ---- batch: 050 ----
mean loss: 419.68
 ---- batch: 060 ----
mean loss: 400.39
 ---- batch: 070 ----
mean loss: 408.43
 ---- batch: 080 ----
mean loss: 399.92
 ---- batch: 090 ----
mean loss: 412.31
train mean loss: 410.08
epoch train time: 0:00:02.345497
elapsed time: 0:01:32.025972
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 22:06:03.057201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.20
 ---- batch: 020 ----
mean loss: 416.18
 ---- batch: 030 ----
mean loss: 403.65
 ---- batch: 040 ----
mean loss: 412.85
 ---- batch: 050 ----
mean loss: 405.20
 ---- batch: 060 ----
mean loss: 411.32
 ---- batch: 070 ----
mean loss: 396.18
 ---- batch: 080 ----
mean loss: 405.98
 ---- batch: 090 ----
mean loss: 400.65
train mean loss: 407.53
epoch train time: 0:00:02.344112
elapsed time: 0:01:34.370289
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 22:06:05.401544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.23
 ---- batch: 020 ----
mean loss: 439.64
 ---- batch: 030 ----
mean loss: 417.51
 ---- batch: 040 ----
mean loss: 418.30
 ---- batch: 050 ----
mean loss: 405.89
 ---- batch: 060 ----
mean loss: 409.51
 ---- batch: 070 ----
mean loss: 417.75
 ---- batch: 080 ----
mean loss: 411.40
 ---- batch: 090 ----
mean loss: 400.28
train mean loss: 416.35
epoch train time: 0:00:02.332270
elapsed time: 0:01:36.702785
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 22:06:07.734023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.05
 ---- batch: 020 ----
mean loss: 405.08
 ---- batch: 030 ----
mean loss: 417.37
 ---- batch: 040 ----
mean loss: 397.31
 ---- batch: 050 ----
mean loss: 418.28
 ---- batch: 060 ----
mean loss: 415.72
 ---- batch: 070 ----
mean loss: 396.61
 ---- batch: 080 ----
mean loss: 391.70
 ---- batch: 090 ----
mean loss: 396.50
train mean loss: 405.49
epoch train time: 0:00:02.348564
elapsed time: 0:01:39.051547
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 22:06:10.082770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.24
 ---- batch: 020 ----
mean loss: 405.17
 ---- batch: 030 ----
mean loss: 411.89
 ---- batch: 040 ----
mean loss: 418.20
 ---- batch: 050 ----
mean loss: 401.59
 ---- batch: 060 ----
mean loss: 401.19
 ---- batch: 070 ----
mean loss: 409.54
 ---- batch: 080 ----
mean loss: 400.44
 ---- batch: 090 ----
mean loss: 398.91
train mean loss: 406.46
epoch train time: 0:00:02.337000
elapsed time: 0:01:41.388733
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 22:06:12.419961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.37
 ---- batch: 020 ----
mean loss: 404.40
 ---- batch: 030 ----
mean loss: 397.73
 ---- batch: 040 ----
mean loss: 392.13
 ---- batch: 050 ----
mean loss: 421.66
 ---- batch: 060 ----
mean loss: 435.19
 ---- batch: 070 ----
mean loss: 431.30
 ---- batch: 080 ----
mean loss: 406.90
 ---- batch: 090 ----
mean loss: 402.38
train mean loss: 413.28
epoch train time: 0:00:02.340795
elapsed time: 0:01:43.729736
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 22:06:14.761001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.01
 ---- batch: 020 ----
mean loss: 411.26
 ---- batch: 030 ----
mean loss: 419.54
 ---- batch: 040 ----
mean loss: 413.04
 ---- batch: 050 ----
mean loss: 425.80
 ---- batch: 060 ----
mean loss: 396.07
 ---- batch: 070 ----
mean loss: 402.28
 ---- batch: 080 ----
mean loss: 396.95
 ---- batch: 090 ----
mean loss: 408.75
train mean loss: 408.99
epoch train time: 0:00:02.328729
elapsed time: 0:01:46.058709
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 22:06:17.089949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.05
 ---- batch: 020 ----
mean loss: 404.44
 ---- batch: 030 ----
mean loss: 404.71
 ---- batch: 040 ----
mean loss: 400.54
 ---- batch: 050 ----
mean loss: 393.46
 ---- batch: 060 ----
mean loss: 419.41
 ---- batch: 070 ----
mean loss: 403.04
 ---- batch: 080 ----
mean loss: 421.11
 ---- batch: 090 ----
mean loss: 424.86
train mean loss: 408.83
epoch train time: 0:00:02.335790
elapsed time: 0:01:48.394698
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 22:06:19.425924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.36
 ---- batch: 020 ----
mean loss: 419.11
 ---- batch: 030 ----
mean loss: 414.57
 ---- batch: 040 ----
mean loss: 406.27
 ---- batch: 050 ----
mean loss: 398.35
 ---- batch: 060 ----
mean loss: 400.87
 ---- batch: 070 ----
mean loss: 420.86
 ---- batch: 080 ----
mean loss: 408.49
 ---- batch: 090 ----
mean loss: 384.58
train mean loss: 405.46
epoch train time: 0:00:02.342375
elapsed time: 0:01:50.737277
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 22:06:21.768512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.25
 ---- batch: 020 ----
mean loss: 413.12
 ---- batch: 030 ----
mean loss: 403.54
 ---- batch: 040 ----
mean loss: 400.12
 ---- batch: 050 ----
mean loss: 391.77
 ---- batch: 060 ----
mean loss: 390.04
 ---- batch: 070 ----
mean loss: 402.45
 ---- batch: 080 ----
mean loss: 402.00
 ---- batch: 090 ----
mean loss: 418.54
train mean loss: 403.09
epoch train time: 0:00:02.333769
elapsed time: 0:01:53.071302
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 22:06:24.102531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.75
 ---- batch: 020 ----
mean loss: 412.77
 ---- batch: 030 ----
mean loss: 406.13
 ---- batch: 040 ----
mean loss: 407.61
 ---- batch: 050 ----
mean loss: 395.88
 ---- batch: 060 ----
mean loss: 394.22
 ---- batch: 070 ----
mean loss: 406.64
 ---- batch: 080 ----
mean loss: 400.05
 ---- batch: 090 ----
mean loss: 395.06
train mean loss: 405.32
epoch train time: 0:00:02.336442
elapsed time: 0:01:55.407987
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 22:06:26.439224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.27
 ---- batch: 020 ----
mean loss: 411.93
 ---- batch: 030 ----
mean loss: 415.15
 ---- batch: 040 ----
mean loss: 402.19
 ---- batch: 050 ----
mean loss: 399.56
 ---- batch: 060 ----
mean loss: 407.33
 ---- batch: 070 ----
mean loss: 398.52
 ---- batch: 080 ----
mean loss: 401.72
 ---- batch: 090 ----
mean loss: 398.73
train mean loss: 403.42
epoch train time: 0:00:02.341099
elapsed time: 0:01:57.749304
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 22:06:28.780532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.28
 ---- batch: 020 ----
mean loss: 411.73
 ---- batch: 030 ----
mean loss: 383.93
 ---- batch: 040 ----
mean loss: 409.69
 ---- batch: 050 ----
mean loss: 409.31
 ---- batch: 060 ----
mean loss: 405.19
 ---- batch: 070 ----
mean loss: 397.70
 ---- batch: 080 ----
mean loss: 395.01
 ---- batch: 090 ----
mean loss: 409.98
train mean loss: 405.57
epoch train time: 0:00:02.337004
elapsed time: 0:02:00.086510
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 22:06:31.117765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.85
 ---- batch: 020 ----
mean loss: 423.48
 ---- batch: 030 ----
mean loss: 401.58
 ---- batch: 040 ----
mean loss: 417.73
 ---- batch: 050 ----
mean loss: 399.84
 ---- batch: 060 ----
mean loss: 405.15
 ---- batch: 070 ----
mean loss: 403.54
 ---- batch: 080 ----
mean loss: 394.26
 ---- batch: 090 ----
mean loss: 399.07
train mean loss: 406.65
epoch train time: 0:00:02.331300
elapsed time: 0:02:02.418051
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 22:06:33.449280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.56
 ---- batch: 020 ----
mean loss: 418.45
 ---- batch: 030 ----
mean loss: 403.88
 ---- batch: 040 ----
mean loss: 405.64
 ---- batch: 050 ----
mean loss: 400.21
 ---- batch: 060 ----
mean loss: 407.90
 ---- batch: 070 ----
mean loss: 409.34
 ---- batch: 080 ----
mean loss: 413.77
 ---- batch: 090 ----
mean loss: 398.89
train mean loss: 405.36
epoch train time: 0:00:02.335052
elapsed time: 0:02:04.753301
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 22:06:35.784529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.12
 ---- batch: 020 ----
mean loss: 395.60
 ---- batch: 030 ----
mean loss: 407.78
 ---- batch: 040 ----
mean loss: 433.11
 ---- batch: 050 ----
mean loss: 415.98
 ---- batch: 060 ----
mean loss: 401.12
 ---- batch: 070 ----
mean loss: 395.53
 ---- batch: 080 ----
mean loss: 400.43
 ---- batch: 090 ----
mean loss: 407.98
train mean loss: 406.32
epoch train time: 0:00:02.329867
elapsed time: 0:02:07.083396
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 22:06:38.114626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.32
 ---- batch: 020 ----
mean loss: 430.45
 ---- batch: 030 ----
mean loss: 403.27
 ---- batch: 040 ----
mean loss: 407.54
 ---- batch: 050 ----
mean loss: 405.04
 ---- batch: 060 ----
mean loss: 407.56
 ---- batch: 070 ----
mean loss: 406.57
 ---- batch: 080 ----
mean loss: 401.21
 ---- batch: 090 ----
mean loss: 388.06
train mean loss: 407.73
epoch train time: 0:00:02.326669
elapsed time: 0:02:09.410253
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 22:06:40.441477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.10
 ---- batch: 020 ----
mean loss: 398.21
 ---- batch: 030 ----
mean loss: 404.44
 ---- batch: 040 ----
mean loss: 401.12
 ---- batch: 050 ----
mean loss: 418.62
 ---- batch: 060 ----
mean loss: 399.66
 ---- batch: 070 ----
mean loss: 400.06
 ---- batch: 080 ----
mean loss: 410.23
 ---- batch: 090 ----
mean loss: 408.00
train mean loss: 405.17
epoch train time: 0:00:02.336105
elapsed time: 0:02:11.746577
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 22:06:42.777804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.29
 ---- batch: 020 ----
mean loss: 418.25
 ---- batch: 030 ----
mean loss: 435.50
 ---- batch: 040 ----
mean loss: 400.01
 ---- batch: 050 ----
mean loss: 408.82
 ---- batch: 060 ----
mean loss: 419.94
 ---- batch: 070 ----
mean loss: 408.59
 ---- batch: 080 ----
mean loss: 424.86
 ---- batch: 090 ----
mean loss: 422.25
train mean loss: 416.22
epoch train time: 0:00:02.337471
elapsed time: 0:02:14.084227
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 22:06:45.115467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.48
 ---- batch: 020 ----
mean loss: 406.20
 ---- batch: 030 ----
mean loss: 411.30
 ---- batch: 040 ----
mean loss: 405.64
 ---- batch: 050 ----
mean loss: 402.18
 ---- batch: 060 ----
mean loss: 401.53
 ---- batch: 070 ----
mean loss: 400.15
 ---- batch: 080 ----
mean loss: 405.99
 ---- batch: 090 ----
mean loss: 410.19
train mean loss: 404.81
epoch train time: 0:00:02.329895
elapsed time: 0:02:16.414364
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 22:06:47.445629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.73
 ---- batch: 020 ----
mean loss: 405.57
 ---- batch: 030 ----
mean loss: 418.19
 ---- batch: 040 ----
mean loss: 390.00
 ---- batch: 050 ----
mean loss: 401.47
 ---- batch: 060 ----
mean loss: 404.51
 ---- batch: 070 ----
mean loss: 400.01
 ---- batch: 080 ----
mean loss: 402.37
 ---- batch: 090 ----
mean loss: 401.36
train mean loss: 404.77
epoch train time: 0:00:02.330891
elapsed time: 0:02:18.745503
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 22:06:49.776777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.50
 ---- batch: 020 ----
mean loss: 410.78
 ---- batch: 030 ----
mean loss: 404.79
 ---- batch: 040 ----
mean loss: 398.16
 ---- batch: 050 ----
mean loss: 407.93
 ---- batch: 060 ----
mean loss: 396.61
 ---- batch: 070 ----
mean loss: 405.34
 ---- batch: 080 ----
mean loss: 410.84
 ---- batch: 090 ----
mean loss: 405.80
train mean loss: 404.06
epoch train time: 0:00:02.337790
elapsed time: 0:02:21.083523
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 22:06:52.114748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.97
 ---- batch: 020 ----
mean loss: 413.22
 ---- batch: 030 ----
mean loss: 415.84
 ---- batch: 040 ----
mean loss: 401.12
 ---- batch: 050 ----
mean loss: 408.27
 ---- batch: 060 ----
mean loss: 398.25
 ---- batch: 070 ----
mean loss: 403.49
 ---- batch: 080 ----
mean loss: 401.24
 ---- batch: 090 ----
mean loss: 426.93
train mean loss: 408.63
epoch train time: 0:00:02.332589
elapsed time: 0:02:23.416361
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 22:06:54.447593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.99
 ---- batch: 020 ----
mean loss: 396.61
 ---- batch: 030 ----
mean loss: 397.53
 ---- batch: 040 ----
mean loss: 395.95
 ---- batch: 050 ----
mean loss: 408.78
 ---- batch: 060 ----
mean loss: 431.52
 ---- batch: 070 ----
mean loss: 417.50
 ---- batch: 080 ----
mean loss: 400.19
 ---- batch: 090 ----
mean loss: 395.12
train mean loss: 407.46
epoch train time: 0:00:02.333571
elapsed time: 0:02:25.750161
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 22:06:56.781395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.51
 ---- batch: 020 ----
mean loss: 393.50
 ---- batch: 030 ----
mean loss: 403.37
 ---- batch: 040 ----
mean loss: 399.94
 ---- batch: 050 ----
mean loss: 408.32
 ---- batch: 060 ----
mean loss: 401.10
 ---- batch: 070 ----
mean loss: 415.28
 ---- batch: 080 ----
mean loss: 419.16
 ---- batch: 090 ----
mean loss: 401.77
train mean loss: 405.06
epoch train time: 0:00:02.331999
elapsed time: 0:02:28.082358
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 22:06:59.113588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.65
 ---- batch: 020 ----
mean loss: 385.18
 ---- batch: 030 ----
mean loss: 408.45
 ---- batch: 040 ----
mean loss: 399.11
 ---- batch: 050 ----
mean loss: 397.95
 ---- batch: 060 ----
mean loss: 414.69
 ---- batch: 070 ----
mean loss: 389.34
 ---- batch: 080 ----
mean loss: 393.99
 ---- batch: 090 ----
mean loss: 398.42
train mean loss: 396.76
epoch train time: 0:00:02.327554
elapsed time: 0:02:30.410095
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 22:07:01.441319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.23
 ---- batch: 020 ----
mean loss: 401.80
 ---- batch: 030 ----
mean loss: 401.32
 ---- batch: 040 ----
mean loss: 399.87
 ---- batch: 050 ----
mean loss: 401.90
 ---- batch: 060 ----
mean loss: 404.52
 ---- batch: 070 ----
mean loss: 401.04
 ---- batch: 080 ----
mean loss: 398.95
 ---- batch: 090 ----
mean loss: 398.40
train mean loss: 399.55
epoch train time: 0:00:02.340319
elapsed time: 0:02:32.750654
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 22:07:03.781878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.12
 ---- batch: 020 ----
mean loss: 398.26
 ---- batch: 030 ----
mean loss: 395.00
 ---- batch: 040 ----
mean loss: 409.21
 ---- batch: 050 ----
mean loss: 393.01
 ---- batch: 060 ----
mean loss: 409.92
 ---- batch: 070 ----
mean loss: 402.93
 ---- batch: 080 ----
mean loss: 389.84
 ---- batch: 090 ----
mean loss: 403.13
train mean loss: 399.99
epoch train time: 0:00:02.335515
elapsed time: 0:02:35.086369
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 22:07:06.117629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.89
 ---- batch: 020 ----
mean loss: 405.40
 ---- batch: 030 ----
mean loss: 399.95
 ---- batch: 040 ----
mean loss: 401.16
 ---- batch: 050 ----
mean loss: 402.08
 ---- batch: 060 ----
mean loss: 410.55
 ---- batch: 070 ----
mean loss: 386.87
 ---- batch: 080 ----
mean loss: 397.48
 ---- batch: 090 ----
mean loss: 387.92
train mean loss: 398.39
epoch train time: 0:00:02.333370
elapsed time: 0:02:37.419954
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 22:07:08.451195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.92
 ---- batch: 020 ----
mean loss: 393.50
 ---- batch: 030 ----
mean loss: 407.83
 ---- batch: 040 ----
mean loss: 389.19
 ---- batch: 050 ----
mean loss: 398.27
 ---- batch: 060 ----
mean loss: 400.58
 ---- batch: 070 ----
mean loss: 390.20
 ---- batch: 080 ----
mean loss: 415.33
 ---- batch: 090 ----
mean loss: 383.62
train mean loss: 398.28
epoch train time: 0:00:02.334141
elapsed time: 0:02:39.754297
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 22:07:10.785532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.80
 ---- batch: 020 ----
mean loss: 402.51
 ---- batch: 030 ----
mean loss: 413.58
 ---- batch: 040 ----
mean loss: 388.76
 ---- batch: 050 ----
mean loss: 405.01
 ---- batch: 060 ----
mean loss: 396.62
 ---- batch: 070 ----
mean loss: 408.34
 ---- batch: 080 ----
mean loss: 387.53
 ---- batch: 090 ----
mean loss: 392.59
train mean loss: 401.06
epoch train time: 0:00:02.339638
elapsed time: 0:02:42.094123
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 22:07:13.125347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.08
 ---- batch: 020 ----
mean loss: 396.86
 ---- batch: 030 ----
mean loss: 395.71
 ---- batch: 040 ----
mean loss: 402.37
 ---- batch: 050 ----
mean loss: 392.09
 ---- batch: 060 ----
mean loss: 401.61
 ---- batch: 070 ----
mean loss: 408.84
 ---- batch: 080 ----
mean loss: 390.47
 ---- batch: 090 ----
mean loss: 396.96
train mean loss: 397.10
epoch train time: 0:00:02.331322
elapsed time: 0:02:44.425662
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 22:07:15.456898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.87
 ---- batch: 020 ----
mean loss: 389.97
 ---- batch: 030 ----
mean loss: 408.10
 ---- batch: 040 ----
mean loss: 405.22
 ---- batch: 050 ----
mean loss: 395.70
 ---- batch: 060 ----
mean loss: 385.47
 ---- batch: 070 ----
mean loss: 397.36
 ---- batch: 080 ----
mean loss: 395.47
 ---- batch: 090 ----
mean loss: 394.07
train mean loss: 397.35
epoch train time: 0:00:02.338375
elapsed time: 0:02:46.764232
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 22:07:17.795472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.92
 ---- batch: 020 ----
mean loss: 418.09
 ---- batch: 030 ----
mean loss: 405.59
 ---- batch: 040 ----
mean loss: 394.70
 ---- batch: 050 ----
mean loss: 400.26
 ---- batch: 060 ----
mean loss: 412.98
 ---- batch: 070 ----
mean loss: 381.51
 ---- batch: 080 ----
mean loss: 411.57
 ---- batch: 090 ----
mean loss: 412.51
train mean loss: 403.32
epoch train time: 0:00:02.341581
elapsed time: 0:02:49.106026
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 22:07:20.137254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.24
 ---- batch: 020 ----
mean loss: 400.50
 ---- batch: 030 ----
mean loss: 401.16
 ---- batch: 040 ----
mean loss: 389.10
 ---- batch: 050 ----
mean loss: 385.68
 ---- batch: 060 ----
mean loss: 390.03
 ---- batch: 070 ----
mean loss: 388.46
 ---- batch: 080 ----
mean loss: 406.14
 ---- batch: 090 ----
mean loss: 399.63
train mean loss: 395.53
epoch train time: 0:00:02.330272
elapsed time: 0:02:51.436482
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 22:07:22.467707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.29
 ---- batch: 020 ----
mean loss: 394.96
 ---- batch: 030 ----
mean loss: 398.54
 ---- batch: 040 ----
mean loss: 402.67
 ---- batch: 050 ----
mean loss: 407.49
 ---- batch: 060 ----
mean loss: 399.88
 ---- batch: 070 ----
mean loss: 388.84
 ---- batch: 080 ----
mean loss: 395.55
 ---- batch: 090 ----
mean loss: 397.46
train mean loss: 397.14
epoch train time: 0:00:02.332997
elapsed time: 0:02:53.769690
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 22:07:24.800934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.21
 ---- batch: 020 ----
mean loss: 393.58
 ---- batch: 030 ----
mean loss: 403.19
 ---- batch: 040 ----
mean loss: 400.68
 ---- batch: 050 ----
mean loss: 398.13
 ---- batch: 060 ----
mean loss: 400.72
 ---- batch: 070 ----
mean loss: 406.72
 ---- batch: 080 ----
mean loss: 401.09
 ---- batch: 090 ----
mean loss: 385.87
train mean loss: 398.22
epoch train time: 0:00:02.333724
elapsed time: 0:02:56.103659
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 22:07:27.134892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.43
 ---- batch: 020 ----
mean loss: 397.31
 ---- batch: 030 ----
mean loss: 406.37
 ---- batch: 040 ----
mean loss: 386.70
 ---- batch: 050 ----
mean loss: 398.70
 ---- batch: 060 ----
mean loss: 418.81
 ---- batch: 070 ----
mean loss: 395.46
 ---- batch: 080 ----
mean loss: 409.15
 ---- batch: 090 ----
mean loss: 408.33
train mean loss: 402.17
epoch train time: 0:00:02.328452
elapsed time: 0:02:58.432352
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 22:07:29.463577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.81
 ---- batch: 020 ----
mean loss: 400.90
 ---- batch: 030 ----
mean loss: 391.49
 ---- batch: 040 ----
mean loss: 400.41
 ---- batch: 050 ----
mean loss: 403.21
 ---- batch: 060 ----
mean loss: 394.64
 ---- batch: 070 ----
mean loss: 397.32
 ---- batch: 080 ----
mean loss: 415.59
 ---- batch: 090 ----
mean loss: 391.93
train mean loss: 400.03
epoch train time: 0:00:02.336655
elapsed time: 0:03:00.769214
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 22:07:31.800445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.79
 ---- batch: 020 ----
mean loss: 425.33
 ---- batch: 030 ----
mean loss: 401.84
 ---- batch: 040 ----
mean loss: 399.60
 ---- batch: 050 ----
mean loss: 422.31
 ---- batch: 060 ----
mean loss: 417.62
 ---- batch: 070 ----
mean loss: 405.63
 ---- batch: 080 ----
mean loss: 392.47
 ---- batch: 090 ----
mean loss: 389.11
train mean loss: 405.07
epoch train time: 0:00:02.336225
elapsed time: 0:03:03.105651
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 22:07:34.136886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.17
 ---- batch: 020 ----
mean loss: 404.67
 ---- batch: 030 ----
mean loss: 406.41
 ---- batch: 040 ----
mean loss: 394.12
 ---- batch: 050 ----
mean loss: 397.45
 ---- batch: 060 ----
mean loss: 404.68
 ---- batch: 070 ----
mean loss: 394.74
 ---- batch: 080 ----
mean loss: 404.39
 ---- batch: 090 ----
mean loss: 399.16
train mean loss: 403.17
epoch train time: 0:00:02.340273
elapsed time: 0:03:05.446113
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 22:07:36.477335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.22
 ---- batch: 020 ----
mean loss: 393.05
 ---- batch: 030 ----
mean loss: 394.74
 ---- batch: 040 ----
mean loss: 400.11
 ---- batch: 050 ----
mean loss: 392.82
 ---- batch: 060 ----
mean loss: 403.95
 ---- batch: 070 ----
mean loss: 390.40
 ---- batch: 080 ----
mean loss: 396.03
 ---- batch: 090 ----
mean loss: 389.74
train mean loss: 394.67
epoch train time: 0:00:02.344341
elapsed time: 0:03:07.790649
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 22:07:38.821881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.73
 ---- batch: 020 ----
mean loss: 386.20
 ---- batch: 030 ----
mean loss: 411.02
 ---- batch: 040 ----
mean loss: 408.01
 ---- batch: 050 ----
mean loss: 406.32
 ---- batch: 060 ----
mean loss: 396.62
 ---- batch: 070 ----
mean loss: 388.45
 ---- batch: 080 ----
mean loss: 387.18
 ---- batch: 090 ----
mean loss: 387.50
train mean loss: 395.95
epoch train time: 0:00:02.335852
elapsed time: 0:03:10.126685
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 22:07:41.157908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.25
 ---- batch: 020 ----
mean loss: 402.15
 ---- batch: 030 ----
mean loss: 404.15
 ---- batch: 040 ----
mean loss: 391.56
 ---- batch: 050 ----
mean loss: 390.86
 ---- batch: 060 ----
mean loss: 398.37
 ---- batch: 070 ----
mean loss: 399.25
 ---- batch: 080 ----
mean loss: 396.91
 ---- batch: 090 ----
mean loss: 385.94
train mean loss: 395.62
epoch train time: 0:00:02.326530
elapsed time: 0:03:12.453390
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 22:07:43.484633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.48
 ---- batch: 020 ----
mean loss: 396.66
 ---- batch: 030 ----
mean loss: 400.50
 ---- batch: 040 ----
mean loss: 394.93
 ---- batch: 050 ----
mean loss: 390.99
 ---- batch: 060 ----
mean loss: 395.92
 ---- batch: 070 ----
mean loss: 402.99
 ---- batch: 080 ----
mean loss: 389.30
 ---- batch: 090 ----
mean loss: 402.17
train mean loss: 396.33
epoch train time: 0:00:02.339154
elapsed time: 0:03:14.792750
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 22:07:45.823980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.66
 ---- batch: 020 ----
mean loss: 400.68
 ---- batch: 030 ----
mean loss: 419.40
 ---- batch: 040 ----
mean loss: 409.37
 ---- batch: 050 ----
mean loss: 391.11
 ---- batch: 060 ----
mean loss: 397.97
 ---- batch: 070 ----
mean loss: 390.38
 ---- batch: 080 ----
mean loss: 405.37
 ---- batch: 090 ----
mean loss: 418.48
train mean loss: 406.39
epoch train time: 0:00:02.333876
elapsed time: 0:03:17.126837
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 22:07:48.158079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.76
 ---- batch: 020 ----
mean loss: 398.15
 ---- batch: 030 ----
mean loss: 404.03
 ---- batch: 040 ----
mean loss: 398.86
 ---- batch: 050 ----
mean loss: 397.22
 ---- batch: 060 ----
mean loss: 390.39
 ---- batch: 070 ----
mean loss: 404.99
 ---- batch: 080 ----
mean loss: 405.26
 ---- batch: 090 ----
mean loss: 405.04
train mean loss: 398.72
epoch train time: 0:00:02.328476
elapsed time: 0:03:19.455536
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 22:07:50.486763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.61
 ---- batch: 020 ----
mean loss: 400.78
 ---- batch: 030 ----
mean loss: 409.52
 ---- batch: 040 ----
mean loss: 399.67
 ---- batch: 050 ----
mean loss: 398.67
 ---- batch: 060 ----
mean loss: 398.59
 ---- batch: 070 ----
mean loss: 387.88
 ---- batch: 080 ----
mean loss: 385.88
 ---- batch: 090 ----
mean loss: 391.19
train mean loss: 396.63
epoch train time: 0:00:02.331944
elapsed time: 0:03:21.787679
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 22:07:52.818904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.62
 ---- batch: 020 ----
mean loss: 388.78
 ---- batch: 030 ----
mean loss: 389.73
 ---- batch: 040 ----
mean loss: 394.53
 ---- batch: 050 ----
mean loss: 406.02
 ---- batch: 060 ----
mean loss: 384.84
 ---- batch: 070 ----
mean loss: 405.80
 ---- batch: 080 ----
mean loss: 411.01
 ---- batch: 090 ----
mean loss: 399.47
train mean loss: 396.60
epoch train time: 0:00:02.346096
elapsed time: 0:03:24.133969
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 22:07:55.165198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.86
 ---- batch: 020 ----
mean loss: 401.64
 ---- batch: 030 ----
mean loss: 399.79
 ---- batch: 040 ----
mean loss: 401.03
 ---- batch: 050 ----
mean loss: 395.97
 ---- batch: 060 ----
mean loss: 400.98
 ---- batch: 070 ----
mean loss: 398.21
 ---- batch: 080 ----
mean loss: 389.71
 ---- batch: 090 ----
mean loss: 392.27
train mean loss: 397.71
epoch train time: 0:00:02.332038
elapsed time: 0:03:26.466191
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 22:07:57.497416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.81
 ---- batch: 020 ----
mean loss: 393.79
 ---- batch: 030 ----
mean loss: 406.29
 ---- batch: 040 ----
mean loss: 400.49
 ---- batch: 050 ----
mean loss: 393.16
 ---- batch: 060 ----
mean loss: 397.45
 ---- batch: 070 ----
mean loss: 394.94
 ---- batch: 080 ----
mean loss: 396.55
 ---- batch: 090 ----
mean loss: 395.78
train mean loss: 396.63
epoch train time: 0:00:02.342032
elapsed time: 0:03:28.808402
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 22:07:59.839645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.39
 ---- batch: 020 ----
mean loss: 393.41
 ---- batch: 030 ----
mean loss: 398.27
 ---- batch: 040 ----
mean loss: 399.54
 ---- batch: 050 ----
mean loss: 389.38
 ---- batch: 060 ----
mean loss: 386.02
 ---- batch: 070 ----
mean loss: 396.60
 ---- batch: 080 ----
mean loss: 397.21
 ---- batch: 090 ----
mean loss: 413.61
train mean loss: 395.79
epoch train time: 0:00:02.336204
elapsed time: 0:03:31.144827
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 22:08:02.176065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.22
 ---- batch: 020 ----
mean loss: 396.60
 ---- batch: 030 ----
mean loss: 384.39
 ---- batch: 040 ----
mean loss: 387.65
 ---- batch: 050 ----
mean loss: 401.41
 ---- batch: 060 ----
mean loss: 404.87
 ---- batch: 070 ----
mean loss: 383.28
 ---- batch: 080 ----
mean loss: 397.96
 ---- batch: 090 ----
mean loss: 398.94
train mean loss: 395.62
epoch train time: 0:00:02.330912
elapsed time: 0:03:33.475930
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 22:08:04.507154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.21
 ---- batch: 020 ----
mean loss: 409.51
 ---- batch: 030 ----
mean loss: 412.60
 ---- batch: 040 ----
mean loss: 392.75
 ---- batch: 050 ----
mean loss: 406.30
 ---- batch: 060 ----
mean loss: 387.66
 ---- batch: 070 ----
mean loss: 389.85
 ---- batch: 080 ----
mean loss: 393.59
 ---- batch: 090 ----
mean loss: 399.83
train mean loss: 396.24
epoch train time: 0:00:02.332520
elapsed time: 0:03:35.808633
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 22:08:06.839858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.50
 ---- batch: 020 ----
mean loss: 390.87
 ---- batch: 030 ----
mean loss: 397.22
 ---- batch: 040 ----
mean loss: 399.99
 ---- batch: 050 ----
mean loss: 405.33
 ---- batch: 060 ----
mean loss: 389.46
 ---- batch: 070 ----
mean loss: 395.30
 ---- batch: 080 ----
mean loss: 391.32
 ---- batch: 090 ----
mean loss: 398.87
train mean loss: 395.94
epoch train time: 0:00:02.334782
elapsed time: 0:03:38.143636
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 22:08:09.174863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.40
 ---- batch: 020 ----
mean loss: 388.46
 ---- batch: 030 ----
mean loss: 392.00
 ---- batch: 040 ----
mean loss: 393.50
 ---- batch: 050 ----
mean loss: 397.71
 ---- batch: 060 ----
mean loss: 394.46
 ---- batch: 070 ----
mean loss: 380.90
 ---- batch: 080 ----
mean loss: 394.12
 ---- batch: 090 ----
mean loss: 407.53
train mean loss: 392.82
epoch train time: 0:00:02.331742
elapsed time: 0:03:40.475570
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 22:08:11.506795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.13
 ---- batch: 020 ----
mean loss: 385.91
 ---- batch: 030 ----
mean loss: 399.99
 ---- batch: 040 ----
mean loss: 402.64
 ---- batch: 050 ----
mean loss: 388.05
 ---- batch: 060 ----
mean loss: 401.28
 ---- batch: 070 ----
mean loss: 387.90
 ---- batch: 080 ----
mean loss: 398.68
 ---- batch: 090 ----
mean loss: 391.25
train mean loss: 393.67
epoch train time: 0:00:02.350221
elapsed time: 0:03:42.825974
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 22:08:13.857198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.86
 ---- batch: 020 ----
mean loss: 405.06
 ---- batch: 030 ----
mean loss: 400.85
 ---- batch: 040 ----
mean loss: 389.72
 ---- batch: 050 ----
mean loss: 401.42
 ---- batch: 060 ----
mean loss: 390.11
 ---- batch: 070 ----
mean loss: 396.99
 ---- batch: 080 ----
mean loss: 389.38
 ---- batch: 090 ----
mean loss: 400.40
train mean loss: 397.25
epoch train time: 0:00:02.335912
elapsed time: 0:03:45.162064
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 22:08:16.193288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.26
 ---- batch: 020 ----
mean loss: 399.42
 ---- batch: 030 ----
mean loss: 393.58
 ---- batch: 040 ----
mean loss: 394.21
 ---- batch: 050 ----
mean loss: 395.19
 ---- batch: 060 ----
mean loss: 396.09
 ---- batch: 070 ----
mean loss: 389.57
 ---- batch: 080 ----
mean loss: 397.98
 ---- batch: 090 ----
mean loss: 390.56
train mean loss: 395.30
epoch train time: 0:00:02.333252
elapsed time: 0:03:47.495485
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 22:08:18.526726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.38
 ---- batch: 020 ----
mean loss: 378.88
 ---- batch: 030 ----
mean loss: 384.60
 ---- batch: 040 ----
mean loss: 389.78
 ---- batch: 050 ----
mean loss: 400.92
 ---- batch: 060 ----
mean loss: 409.17
 ---- batch: 070 ----
mean loss: 402.92
 ---- batch: 080 ----
mean loss: 392.25
 ---- batch: 090 ----
mean loss: 393.84
train mean loss: 395.37
epoch train time: 0:00:02.338867
elapsed time: 0:03:49.834566
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 22:08:20.865795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.91
 ---- batch: 020 ----
mean loss: 393.16
 ---- batch: 030 ----
mean loss: 396.47
 ---- batch: 040 ----
mean loss: 390.57
 ---- batch: 050 ----
mean loss: 403.89
 ---- batch: 060 ----
mean loss: 408.01
 ---- batch: 070 ----
mean loss: 410.51
 ---- batch: 080 ----
mean loss: 386.78
 ---- batch: 090 ----
mean loss: 402.47
train mean loss: 397.18
epoch train time: 0:00:02.324407
elapsed time: 0:03:52.159160
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 22:08:23.190386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.41
 ---- batch: 020 ----
mean loss: 403.51
 ---- batch: 030 ----
mean loss: 388.89
 ---- batch: 040 ----
mean loss: 398.83
 ---- batch: 050 ----
mean loss: 389.99
 ---- batch: 060 ----
mean loss: 379.13
 ---- batch: 070 ----
mean loss: 389.01
 ---- batch: 080 ----
mean loss: 395.76
 ---- batch: 090 ----
mean loss: 395.19
train mean loss: 393.36
epoch train time: 0:00:02.332021
elapsed time: 0:03:54.491377
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 22:08:25.522602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.40
 ---- batch: 020 ----
mean loss: 381.88
 ---- batch: 030 ----
mean loss: 402.02
 ---- batch: 040 ----
mean loss: 392.69
 ---- batch: 050 ----
mean loss: 377.80
 ---- batch: 060 ----
mean loss: 393.61
 ---- batch: 070 ----
mean loss: 406.98
 ---- batch: 080 ----
mean loss: 390.20
 ---- batch: 090 ----
mean loss: 393.53
train mean loss: 392.96
epoch train time: 0:00:02.331036
elapsed time: 0:03:56.822593
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 22:08:27.853841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.95
 ---- batch: 020 ----
mean loss: 390.53
 ---- batch: 030 ----
mean loss: 399.98
 ---- batch: 040 ----
mean loss: 396.42
 ---- batch: 050 ----
mean loss: 394.68
 ---- batch: 060 ----
mean loss: 394.84
 ---- batch: 070 ----
mean loss: 379.12
 ---- batch: 080 ----
mean loss: 388.17
 ---- batch: 090 ----
mean loss: 392.45
train mean loss: 391.66
epoch train time: 0:00:02.333745
elapsed time: 0:03:59.156543
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 22:08:30.187769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.01
 ---- batch: 020 ----
mean loss: 391.88
 ---- batch: 030 ----
mean loss: 392.51
 ---- batch: 040 ----
mean loss: 389.17
 ---- batch: 050 ----
mean loss: 388.60
 ---- batch: 060 ----
mean loss: 399.80
 ---- batch: 070 ----
mean loss: 385.90
 ---- batch: 080 ----
mean loss: 389.39
 ---- batch: 090 ----
mean loss: 390.02
train mean loss: 392.87
epoch train time: 0:00:02.333146
elapsed time: 0:04:01.489874
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 22:08:32.521100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.10
 ---- batch: 020 ----
mean loss: 414.18
 ---- batch: 030 ----
mean loss: 388.14
 ---- batch: 040 ----
mean loss: 412.13
 ---- batch: 050 ----
mean loss: 387.72
 ---- batch: 060 ----
mean loss: 381.29
 ---- batch: 070 ----
mean loss: 384.86
 ---- batch: 080 ----
mean loss: 394.99
 ---- batch: 090 ----
mean loss: 393.37
train mean loss: 392.66
epoch train time: 0:00:02.337812
elapsed time: 0:04:03.827891
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 22:08:34.859170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.59
 ---- batch: 020 ----
mean loss: 392.06
 ---- batch: 030 ----
mean loss: 389.93
 ---- batch: 040 ----
mean loss: 415.71
 ---- batch: 050 ----
mean loss: 412.64
 ---- batch: 060 ----
mean loss: 397.08
 ---- batch: 070 ----
mean loss: 383.04
 ---- batch: 080 ----
mean loss: 395.48
 ---- batch: 090 ----
mean loss: 378.27
train mean loss: 395.57
epoch train time: 0:00:02.335648
elapsed time: 0:04:06.163784
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 22:08:37.195013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.58
 ---- batch: 020 ----
mean loss: 400.22
 ---- batch: 030 ----
mean loss: 408.20
 ---- batch: 040 ----
mean loss: 401.23
 ---- batch: 050 ----
mean loss: 401.25
 ---- batch: 060 ----
mean loss: 427.93
 ---- batch: 070 ----
mean loss: 401.91
 ---- batch: 080 ----
mean loss: 401.15
 ---- batch: 090 ----
mean loss: 389.43
train mean loss: 400.23
epoch train time: 0:00:02.333464
elapsed time: 0:04:08.497435
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 22:08:39.528660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.39
 ---- batch: 020 ----
mean loss: 387.98
 ---- batch: 030 ----
mean loss: 402.10
 ---- batch: 040 ----
mean loss: 405.77
 ---- batch: 050 ----
mean loss: 397.13
 ---- batch: 060 ----
mean loss: 398.11
 ---- batch: 070 ----
mean loss: 396.35
 ---- batch: 080 ----
mean loss: 384.92
 ---- batch: 090 ----
mean loss: 382.64
train mean loss: 392.91
epoch train time: 0:00:02.333432
elapsed time: 0:04:10.831110
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 22:08:41.862351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.68
 ---- batch: 020 ----
mean loss: 394.47
 ---- batch: 030 ----
mean loss: 396.77
 ---- batch: 040 ----
mean loss: 404.39
 ---- batch: 050 ----
mean loss: 398.02
 ---- batch: 060 ----
mean loss: 394.27
 ---- batch: 070 ----
mean loss: 403.46
 ---- batch: 080 ----
mean loss: 389.17
 ---- batch: 090 ----
mean loss: 385.48
train mean loss: 395.67
epoch train time: 0:00:02.337859
elapsed time: 0:04:13.169164
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 22:08:44.200388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.14
 ---- batch: 020 ----
mean loss: 394.57
 ---- batch: 030 ----
mean loss: 416.93
 ---- batch: 040 ----
mean loss: 386.89
 ---- batch: 050 ----
mean loss: 394.00
 ---- batch: 060 ----
mean loss: 416.69
 ---- batch: 070 ----
mean loss: 396.57
 ---- batch: 080 ----
mean loss: 406.79
 ---- batch: 090 ----
mean loss: 395.59
train mean loss: 398.64
epoch train time: 0:00:02.325082
elapsed time: 0:04:15.494427
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 22:08:46.525692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.71
 ---- batch: 020 ----
mean loss: 390.37
 ---- batch: 030 ----
mean loss: 395.65
 ---- batch: 040 ----
mean loss: 389.81
 ---- batch: 050 ----
mean loss: 393.08
 ---- batch: 060 ----
mean loss: 392.21
 ---- batch: 070 ----
mean loss: 390.95
 ---- batch: 080 ----
mean loss: 393.10
 ---- batch: 090 ----
mean loss: 398.10
train mean loss: 391.15
epoch train time: 0:00:02.339738
elapsed time: 0:04:17.834390
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 22:08:48.865630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.29
 ---- batch: 020 ----
mean loss: 390.09
 ---- batch: 030 ----
mean loss: 377.94
 ---- batch: 040 ----
mean loss: 385.62
 ---- batch: 050 ----
mean loss: 384.02
 ---- batch: 060 ----
mean loss: 374.86
 ---- batch: 070 ----
mean loss: 387.35
 ---- batch: 080 ----
mean loss: 384.42
 ---- batch: 090 ----
mean loss: 393.69
train mean loss: 388.02
epoch train time: 0:00:02.338104
elapsed time: 0:04:20.172687
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 22:08:51.203913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.28
 ---- batch: 020 ----
mean loss: 391.44
 ---- batch: 030 ----
mean loss: 372.94
 ---- batch: 040 ----
mean loss: 391.87
 ---- batch: 050 ----
mean loss: 391.52
 ---- batch: 060 ----
mean loss: 393.64
 ---- batch: 070 ----
mean loss: 400.47
 ---- batch: 080 ----
mean loss: 405.48
 ---- batch: 090 ----
mean loss: 407.15
train mean loss: 393.06
epoch train time: 0:00:02.335589
elapsed time: 0:04:22.508485
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 22:08:53.539745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.70
 ---- batch: 020 ----
mean loss: 400.81
 ---- batch: 030 ----
mean loss: 394.03
 ---- batch: 040 ----
mean loss: 391.35
 ---- batch: 050 ----
mean loss: 386.87
 ---- batch: 060 ----
mean loss: 388.86
 ---- batch: 070 ----
mean loss: 396.74
 ---- batch: 080 ----
mean loss: 400.95
 ---- batch: 090 ----
mean loss: 397.99
train mean loss: 394.95
epoch train time: 0:00:02.335436
elapsed time: 0:04:24.844146
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 22:08:55.875372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.69
 ---- batch: 020 ----
mean loss: 387.07
 ---- batch: 030 ----
mean loss: 387.22
 ---- batch: 040 ----
mean loss: 389.03
 ---- batch: 050 ----
mean loss: 387.33
 ---- batch: 060 ----
mean loss: 389.36
 ---- batch: 070 ----
mean loss: 386.06
 ---- batch: 080 ----
mean loss: 396.71
 ---- batch: 090 ----
mean loss: 395.07
train mean loss: 390.38
epoch train time: 0:00:02.338459
elapsed time: 0:04:27.182797
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 22:08:58.214040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.22
 ---- batch: 020 ----
mean loss: 384.45
 ---- batch: 030 ----
mean loss: 396.52
 ---- batch: 040 ----
mean loss: 391.08
 ---- batch: 050 ----
mean loss: 392.14
 ---- batch: 060 ----
mean loss: 385.08
 ---- batch: 070 ----
mean loss: 395.48
 ---- batch: 080 ----
mean loss: 393.10
 ---- batch: 090 ----
mean loss: 391.87
train mean loss: 392.76
epoch train time: 0:00:02.336748
elapsed time: 0:04:29.519767
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 22:09:00.550993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.96
 ---- batch: 020 ----
mean loss: 390.13
 ---- batch: 030 ----
mean loss: 381.88
 ---- batch: 040 ----
mean loss: 388.77
 ---- batch: 050 ----
mean loss: 390.40
 ---- batch: 060 ----
mean loss: 390.35
 ---- batch: 070 ----
mean loss: 388.68
 ---- batch: 080 ----
mean loss: 391.47
 ---- batch: 090 ----
mean loss: 390.33
train mean loss: 388.42
epoch train time: 0:00:02.338639
elapsed time: 0:04:31.858636
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 22:09:02.889866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.87
 ---- batch: 020 ----
mean loss: 377.64
 ---- batch: 030 ----
mean loss: 389.32
 ---- batch: 040 ----
mean loss: 392.22
 ---- batch: 050 ----
mean loss: 374.53
 ---- batch: 060 ----
mean loss: 399.16
 ---- batch: 070 ----
mean loss: 396.02
 ---- batch: 080 ----
mean loss: 381.10
 ---- batch: 090 ----
mean loss: 391.87
train mean loss: 387.02
epoch train time: 0:00:02.331934
elapsed time: 0:04:34.190787
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 22:09:05.222011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.59
 ---- batch: 020 ----
mean loss: 387.20
 ---- batch: 030 ----
mean loss: 392.69
 ---- batch: 040 ----
mean loss: 376.97
 ---- batch: 050 ----
mean loss: 391.02
 ---- batch: 060 ----
mean loss: 392.54
 ---- batch: 070 ----
mean loss: 393.57
 ---- batch: 080 ----
mean loss: 412.80
 ---- batch: 090 ----
mean loss: 401.06
train mean loss: 395.25
epoch train time: 0:00:02.332924
elapsed time: 0:04:36.523893
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 22:09:07.555164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.85
 ---- batch: 020 ----
mean loss: 385.36
 ---- batch: 030 ----
mean loss: 391.82
 ---- batch: 040 ----
mean loss: 384.54
 ---- batch: 050 ----
mean loss: 390.45
 ---- batch: 060 ----
mean loss: 394.72
 ---- batch: 070 ----
mean loss: 382.53
 ---- batch: 080 ----
mean loss: 388.11
 ---- batch: 090 ----
mean loss: 388.56
train mean loss: 389.71
epoch train time: 0:00:02.336055
elapsed time: 0:04:38.860187
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 22:09:09.891433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.75
 ---- batch: 020 ----
mean loss: 405.23
 ---- batch: 030 ----
mean loss: 399.70
 ---- batch: 040 ----
mean loss: 388.63
 ---- batch: 050 ----
mean loss: 376.32
 ---- batch: 060 ----
mean loss: 388.30
 ---- batch: 070 ----
mean loss: 387.62
 ---- batch: 080 ----
mean loss: 391.36
 ---- batch: 090 ----
mean loss: 402.84
train mean loss: 393.43
epoch train time: 0:00:02.327924
elapsed time: 0:04:41.188320
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 22:09:12.219548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.15
 ---- batch: 020 ----
mean loss: 398.94
 ---- batch: 030 ----
mean loss: 385.24
 ---- batch: 040 ----
mean loss: 391.34
 ---- batch: 050 ----
mean loss: 388.99
 ---- batch: 060 ----
mean loss: 394.96
 ---- batch: 070 ----
mean loss: 384.21
 ---- batch: 080 ----
mean loss: 393.18
 ---- batch: 090 ----
mean loss: 386.67
train mean loss: 389.05
epoch train time: 0:00:02.332507
elapsed time: 0:04:43.521031
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 22:09:14.552237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.85
 ---- batch: 020 ----
mean loss: 386.27
 ---- batch: 030 ----
mean loss: 398.47
 ---- batch: 040 ----
mean loss: 375.27
 ---- batch: 050 ----
mean loss: 387.10
 ---- batch: 060 ----
mean loss: 392.35
 ---- batch: 070 ----
mean loss: 404.88
 ---- batch: 080 ----
mean loss: 395.35
 ---- batch: 090 ----
mean loss: 389.03
train mean loss: 390.07
epoch train time: 0:00:02.344253
elapsed time: 0:04:45.865474
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 22:09:16.896706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.61
 ---- batch: 020 ----
mean loss: 403.34
 ---- batch: 030 ----
mean loss: 391.53
 ---- batch: 040 ----
mean loss: 385.96
 ---- batch: 050 ----
mean loss: 389.44
 ---- batch: 060 ----
mean loss: 401.45
 ---- batch: 070 ----
mean loss: 397.06
 ---- batch: 080 ----
mean loss: 377.83
 ---- batch: 090 ----
mean loss: 402.64
train mean loss: 394.81
epoch train time: 0:00:02.334891
elapsed time: 0:04:48.200547
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 22:09:19.231772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.12
 ---- batch: 020 ----
mean loss: 393.34
 ---- batch: 030 ----
mean loss: 399.87
 ---- batch: 040 ----
mean loss: 394.92
 ---- batch: 050 ----
mean loss: 376.59
 ---- batch: 060 ----
mean loss: 386.53
 ---- batch: 070 ----
mean loss: 387.50
 ---- batch: 080 ----
mean loss: 372.16
 ---- batch: 090 ----
mean loss: 380.86
train mean loss: 387.83
epoch train time: 0:00:02.334951
elapsed time: 0:04:50.535691
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 22:09:21.566914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.24
 ---- batch: 020 ----
mean loss: 383.30
 ---- batch: 030 ----
mean loss: 375.96
 ---- batch: 040 ----
mean loss: 400.23
 ---- batch: 050 ----
mean loss: 398.49
 ---- batch: 060 ----
mean loss: 397.63
 ---- batch: 070 ----
mean loss: 391.99
 ---- batch: 080 ----
mean loss: 385.90
 ---- batch: 090 ----
mean loss: 388.98
train mean loss: 390.27
epoch train time: 0:00:02.337044
elapsed time: 0:04:52.872933
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 22:09:23.904162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.26
 ---- batch: 020 ----
mean loss: 385.82
 ---- batch: 030 ----
mean loss: 399.14
 ---- batch: 040 ----
mean loss: 401.09
 ---- batch: 050 ----
mean loss: 390.37
 ---- batch: 060 ----
mean loss: 387.53
 ---- batch: 070 ----
mean loss: 386.94
 ---- batch: 080 ----
mean loss: 392.88
 ---- batch: 090 ----
mean loss: 379.69
train mean loss: 389.30
epoch train time: 0:00:02.332416
elapsed time: 0:04:55.205611
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 22:09:26.236848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.78
 ---- batch: 020 ----
mean loss: 390.64
 ---- batch: 030 ----
mean loss: 387.96
 ---- batch: 040 ----
mean loss: 386.71
 ---- batch: 050 ----
mean loss: 382.33
 ---- batch: 060 ----
mean loss: 393.63
 ---- batch: 070 ----
mean loss: 399.34
 ---- batch: 080 ----
mean loss: 378.20
 ---- batch: 090 ----
mean loss: 380.64
train mean loss: 387.36
epoch train time: 0:00:02.334787
elapsed time: 0:04:57.540628
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 22:09:28.571903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.12
 ---- batch: 020 ----
mean loss: 382.57
 ---- batch: 030 ----
mean loss: 383.47
 ---- batch: 040 ----
mean loss: 379.58
 ---- batch: 050 ----
mean loss: 378.64
 ---- batch: 060 ----
mean loss: 402.96
 ---- batch: 070 ----
mean loss: 385.89
 ---- batch: 080 ----
mean loss: 389.34
 ---- batch: 090 ----
mean loss: 387.58
train mean loss: 391.40
epoch train time: 0:00:02.342996
elapsed time: 0:04:59.883862
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 22:09:30.915098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.50
 ---- batch: 020 ----
mean loss: 379.85
 ---- batch: 030 ----
mean loss: 386.40
 ---- batch: 040 ----
mean loss: 392.52
 ---- batch: 050 ----
mean loss: 381.18
 ---- batch: 060 ----
mean loss: 381.99
 ---- batch: 070 ----
mean loss: 396.02
 ---- batch: 080 ----
mean loss: 380.53
 ---- batch: 090 ----
mean loss: 390.37
train mean loss: 386.24
epoch train time: 0:00:02.330496
elapsed time: 0:05:02.214595
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 22:09:33.245824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.17
 ---- batch: 020 ----
mean loss: 384.45
 ---- batch: 030 ----
mean loss: 387.17
 ---- batch: 040 ----
mean loss: 369.28
 ---- batch: 050 ----
mean loss: 400.03
 ---- batch: 060 ----
mean loss: 376.27
 ---- batch: 070 ----
mean loss: 389.86
 ---- batch: 080 ----
mean loss: 378.25
 ---- batch: 090 ----
mean loss: 390.48
train mean loss: 385.13
epoch train time: 0:00:02.332930
elapsed time: 0:05:04.547724
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 22:09:35.578951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.40
 ---- batch: 020 ----
mean loss: 378.55
 ---- batch: 030 ----
mean loss: 378.53
 ---- batch: 040 ----
mean loss: 371.89
 ---- batch: 050 ----
mean loss: 399.73
 ---- batch: 060 ----
mean loss: 404.41
 ---- batch: 070 ----
mean loss: 406.94
 ---- batch: 080 ----
mean loss: 382.68
 ---- batch: 090 ----
mean loss: 394.35
train mean loss: 386.10
epoch train time: 0:00:02.335639
elapsed time: 0:05:06.883574
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 22:09:37.914800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.85
 ---- batch: 020 ----
mean loss: 393.09
 ---- batch: 030 ----
mean loss: 382.95
 ---- batch: 040 ----
mean loss: 372.60
 ---- batch: 050 ----
mean loss: 390.82
 ---- batch: 060 ----
mean loss: 385.01
 ---- batch: 070 ----
mean loss: 393.46
 ---- batch: 080 ----
mean loss: 404.95
 ---- batch: 090 ----
mean loss: 399.43
train mean loss: 390.34
epoch train time: 0:00:02.332554
elapsed time: 0:05:09.216336
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 22:09:40.247558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.41
 ---- batch: 020 ----
mean loss: 383.90
 ---- batch: 030 ----
mean loss: 391.92
 ---- batch: 040 ----
mean loss: 380.77
 ---- batch: 050 ----
mean loss: 377.06
 ---- batch: 060 ----
mean loss: 381.41
 ---- batch: 070 ----
mean loss: 379.10
 ---- batch: 080 ----
mean loss: 391.11
 ---- batch: 090 ----
mean loss: 378.38
train mean loss: 384.91
epoch train time: 0:00:02.332119
elapsed time: 0:05:11.548622
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 22:09:42.579855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.85
 ---- batch: 020 ----
mean loss: 380.96
 ---- batch: 030 ----
mean loss: 384.61
 ---- batch: 040 ----
mean loss: 383.76
 ---- batch: 050 ----
mean loss: 372.42
 ---- batch: 060 ----
mean loss: 388.27
 ---- batch: 070 ----
mean loss: 386.91
 ---- batch: 080 ----
mean loss: 402.01
 ---- batch: 090 ----
mean loss: 382.58
train mean loss: 386.97
epoch train time: 0:00:02.337490
elapsed time: 0:05:13.886311
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 22:09:44.917559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.37
 ---- batch: 020 ----
mean loss: 389.53
 ---- batch: 030 ----
mean loss: 400.01
 ---- batch: 040 ----
mean loss: 393.14
 ---- batch: 050 ----
mean loss: 377.04
 ---- batch: 060 ----
mean loss: 387.18
 ---- batch: 070 ----
mean loss: 382.77
 ---- batch: 080 ----
mean loss: 403.26
 ---- batch: 090 ----
mean loss: 388.11
train mean loss: 387.54
epoch train time: 0:00:02.330121
elapsed time: 0:05:16.216628
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 22:09:47.247867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.44
 ---- batch: 020 ----
mean loss: 382.43
 ---- batch: 030 ----
mean loss: 378.09
 ---- batch: 040 ----
mean loss: 375.78
 ---- batch: 050 ----
mean loss: 405.46
 ---- batch: 060 ----
mean loss: 393.07
 ---- batch: 070 ----
mean loss: 393.52
 ---- batch: 080 ----
mean loss: 414.16
 ---- batch: 090 ----
mean loss: 414.69
train mean loss: 392.90
epoch train time: 0:00:02.324252
elapsed time: 0:05:18.541099
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 22:09:49.572351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.90
 ---- batch: 020 ----
mean loss: 386.58
 ---- batch: 030 ----
mean loss: 382.84
 ---- batch: 040 ----
mean loss: 380.69
 ---- batch: 050 ----
mean loss: 379.65
 ---- batch: 060 ----
mean loss: 384.24
 ---- batch: 070 ----
mean loss: 388.74
 ---- batch: 080 ----
mean loss: 387.74
 ---- batch: 090 ----
mean loss: 388.37
train mean loss: 388.04
epoch train time: 0:00:02.334223
elapsed time: 0:05:20.875555
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 22:09:51.906782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.59
 ---- batch: 020 ----
mean loss: 383.77
 ---- batch: 030 ----
mean loss: 386.27
 ---- batch: 040 ----
mean loss: 383.00
 ---- batch: 050 ----
mean loss: 382.95
 ---- batch: 060 ----
mean loss: 387.51
 ---- batch: 070 ----
mean loss: 385.63
 ---- batch: 080 ----
mean loss: 390.88
 ---- batch: 090 ----
mean loss: 369.19
train mean loss: 385.48
epoch train time: 0:00:02.340372
elapsed time: 0:05:23.216161
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 22:09:54.247398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.63
 ---- batch: 020 ----
mean loss: 386.73
 ---- batch: 030 ----
mean loss: 373.92
 ---- batch: 040 ----
mean loss: 387.57
 ---- batch: 050 ----
mean loss: 386.42
 ---- batch: 060 ----
mean loss: 381.69
 ---- batch: 070 ----
mean loss: 385.41
 ---- batch: 080 ----
mean loss: 387.37
 ---- batch: 090 ----
mean loss: 386.86
train mean loss: 384.79
epoch train time: 0:00:02.343623
elapsed time: 0:05:25.560020
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 22:09:56.591251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.00
 ---- batch: 020 ----
mean loss: 381.79
 ---- batch: 030 ----
mean loss: 381.40
 ---- batch: 040 ----
mean loss: 377.27
 ---- batch: 050 ----
mean loss: 390.70
 ---- batch: 060 ----
mean loss: 392.87
 ---- batch: 070 ----
mean loss: 398.22
 ---- batch: 080 ----
mean loss: 395.49
 ---- batch: 090 ----
mean loss: 389.57
train mean loss: 387.11
epoch train time: 0:00:02.336311
elapsed time: 0:05:27.896537
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 22:09:58.927784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.58
 ---- batch: 020 ----
mean loss: 385.67
 ---- batch: 030 ----
mean loss: 383.80
 ---- batch: 040 ----
mean loss: 385.12
 ---- batch: 050 ----
mean loss: 380.27
 ---- batch: 060 ----
mean loss: 394.87
 ---- batch: 070 ----
mean loss: 375.25
 ---- batch: 080 ----
mean loss: 384.14
 ---- batch: 090 ----
mean loss: 394.56
train mean loss: 385.24
epoch train time: 0:00:02.331402
elapsed time: 0:05:30.228164
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 22:10:01.259387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.22
 ---- batch: 020 ----
mean loss: 380.02
 ---- batch: 030 ----
mean loss: 375.86
 ---- batch: 040 ----
mean loss: 397.30
 ---- batch: 050 ----
mean loss: 398.89
 ---- batch: 060 ----
mean loss: 390.98
 ---- batch: 070 ----
mean loss: 379.85
 ---- batch: 080 ----
mean loss: 374.90
 ---- batch: 090 ----
mean loss: 395.63
train mean loss: 387.30
epoch train time: 0:00:02.332093
elapsed time: 0:05:32.560448
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 22:10:03.591727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.44
 ---- batch: 020 ----
mean loss: 378.34
 ---- batch: 030 ----
mean loss: 376.09
 ---- batch: 040 ----
mean loss: 388.18
 ---- batch: 050 ----
mean loss: 384.40
 ---- batch: 060 ----
mean loss: 394.38
 ---- batch: 070 ----
mean loss: 385.22
 ---- batch: 080 ----
mean loss: 381.30
 ---- batch: 090 ----
mean loss: 403.73
train mean loss: 386.81
epoch train time: 0:00:02.342045
elapsed time: 0:05:34.902725
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 22:10:05.933953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.78
 ---- batch: 020 ----
mean loss: 377.15
 ---- batch: 030 ----
mean loss: 378.22
 ---- batch: 040 ----
mean loss: 372.82
 ---- batch: 050 ----
mean loss: 378.13
 ---- batch: 060 ----
mean loss: 383.35
 ---- batch: 070 ----
mean loss: 382.27
 ---- batch: 080 ----
mean loss: 387.21
 ---- batch: 090 ----
mean loss: 394.71
train mean loss: 380.94
epoch train time: 0:00:02.331499
elapsed time: 0:05:37.234405
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 22:10:08.265651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.83
 ---- batch: 020 ----
mean loss: 389.19
 ---- batch: 030 ----
mean loss: 372.16
 ---- batch: 040 ----
mean loss: 369.04
 ---- batch: 050 ----
mean loss: 386.39
 ---- batch: 060 ----
mean loss: 387.58
 ---- batch: 070 ----
mean loss: 368.50
 ---- batch: 080 ----
mean loss: 375.98
 ---- batch: 090 ----
mean loss: 397.84
train mean loss: 382.53
epoch train time: 0:00:02.339023
elapsed time: 0:05:39.573629
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 22:10:10.604854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.55
 ---- batch: 020 ----
mean loss: 368.51
 ---- batch: 030 ----
mean loss: 377.65
 ---- batch: 040 ----
mean loss: 369.67
 ---- batch: 050 ----
mean loss: 383.75
 ---- batch: 060 ----
mean loss: 387.52
 ---- batch: 070 ----
mean loss: 393.68
 ---- batch: 080 ----
mean loss: 387.45
 ---- batch: 090 ----
mean loss: 391.02
train mean loss: 383.23
epoch train time: 0:00:02.337056
elapsed time: 0:05:41.910917
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 22:10:12.942155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.86
 ---- batch: 020 ----
mean loss: 372.49
 ---- batch: 030 ----
mean loss: 373.40
 ---- batch: 040 ----
mean loss: 382.50
 ---- batch: 050 ----
mean loss: 385.90
 ---- batch: 060 ----
mean loss: 382.41
 ---- batch: 070 ----
mean loss: 386.15
 ---- batch: 080 ----
mean loss: 394.32
 ---- batch: 090 ----
mean loss: 366.56
train mean loss: 381.16
epoch train time: 0:00:02.333911
elapsed time: 0:05:44.245024
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 22:10:15.276251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.30
 ---- batch: 020 ----
mean loss: 368.18
 ---- batch: 030 ----
mean loss: 382.71
 ---- batch: 040 ----
mean loss: 408.84
 ---- batch: 050 ----
mean loss: 379.49
 ---- batch: 060 ----
mean loss: 385.13
 ---- batch: 070 ----
mean loss: 373.74
 ---- batch: 080 ----
mean loss: 382.76
 ---- batch: 090 ----
mean loss: 394.80
train mean loss: 385.14
epoch train time: 0:00:02.335629
elapsed time: 0:05:46.580873
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 22:10:17.612097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.43
 ---- batch: 020 ----
mean loss: 388.30
 ---- batch: 030 ----
mean loss: 383.14
 ---- batch: 040 ----
mean loss: 386.47
 ---- batch: 050 ----
mean loss: 381.75
 ---- batch: 060 ----
mean loss: 383.72
 ---- batch: 070 ----
mean loss: 387.51
 ---- batch: 080 ----
mean loss: 379.77
 ---- batch: 090 ----
mean loss: 383.53
train mean loss: 384.05
epoch train time: 0:00:02.343736
elapsed time: 0:05:48.924830
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 22:10:19.956067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.05
 ---- batch: 020 ----
mean loss: 394.87
 ---- batch: 030 ----
mean loss: 387.12
 ---- batch: 040 ----
mean loss: 379.84
 ---- batch: 050 ----
mean loss: 368.67
 ---- batch: 060 ----
mean loss: 373.16
 ---- batch: 070 ----
mean loss: 380.51
 ---- batch: 080 ----
mean loss: 382.95
 ---- batch: 090 ----
mean loss: 387.29
train mean loss: 384.32
epoch train time: 0:00:02.330252
elapsed time: 0:05:51.255283
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 22:10:22.286510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.11
 ---- batch: 020 ----
mean loss: 375.34
 ---- batch: 030 ----
mean loss: 389.18
 ---- batch: 040 ----
mean loss: 374.80
 ---- batch: 050 ----
mean loss: 377.94
 ---- batch: 060 ----
mean loss: 384.84
 ---- batch: 070 ----
mean loss: 368.77
 ---- batch: 080 ----
mean loss: 371.31
 ---- batch: 090 ----
mean loss: 384.37
train mean loss: 381.33
epoch train time: 0:00:02.330304
elapsed time: 0:05:53.585766
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 22:10:24.616994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.62
 ---- batch: 020 ----
mean loss: 378.30
 ---- batch: 030 ----
mean loss: 375.29
 ---- batch: 040 ----
mean loss: 372.71
 ---- batch: 050 ----
mean loss: 371.82
 ---- batch: 060 ----
mean loss: 390.70
 ---- batch: 070 ----
mean loss: 370.89
 ---- batch: 080 ----
mean loss: 371.21
 ---- batch: 090 ----
mean loss: 372.05
train mean loss: 374.68
epoch train time: 0:00:02.328727
elapsed time: 0:05:55.914718
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 22:10:26.945943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.03
 ---- batch: 020 ----
mean loss: 364.98
 ---- batch: 030 ----
mean loss: 372.95
 ---- batch: 040 ----
mean loss: 372.62
 ---- batch: 050 ----
mean loss: 371.29
 ---- batch: 060 ----
mean loss: 375.86
 ---- batch: 070 ----
mean loss: 387.31
 ---- batch: 080 ----
mean loss: 380.02
 ---- batch: 090 ----
mean loss: 382.72
train mean loss: 374.49
epoch train time: 0:00:02.335513
elapsed time: 0:05:58.250403
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 22:10:29.281646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.55
 ---- batch: 020 ----
mean loss: 370.97
 ---- batch: 030 ----
mean loss: 395.13
 ---- batch: 040 ----
mean loss: 386.64
 ---- batch: 050 ----
mean loss: 379.38
 ---- batch: 060 ----
mean loss: 368.78
 ---- batch: 070 ----
mean loss: 393.08
 ---- batch: 080 ----
mean loss: 386.82
 ---- batch: 090 ----
mean loss: 373.69
train mean loss: 379.55
epoch train time: 0:00:02.333235
elapsed time: 0:06:00.583829
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 22:10:31.615071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.97
 ---- batch: 020 ----
mean loss: 372.25
 ---- batch: 030 ----
mean loss: 363.84
 ---- batch: 040 ----
mean loss: 377.50
 ---- batch: 050 ----
mean loss: 381.05
 ---- batch: 060 ----
mean loss: 363.99
 ---- batch: 070 ----
mean loss: 361.37
 ---- batch: 080 ----
mean loss: 370.17
 ---- batch: 090 ----
mean loss: 387.26
train mean loss: 372.08
epoch train time: 0:00:02.349882
elapsed time: 0:06:02.933899
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 22:10:33.965128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.89
 ---- batch: 020 ----
mean loss: 385.39
 ---- batch: 030 ----
mean loss: 377.42
 ---- batch: 040 ----
mean loss: 364.18
 ---- batch: 050 ----
mean loss: 395.70
 ---- batch: 060 ----
mean loss: 384.94
 ---- batch: 070 ----
mean loss: 381.20
 ---- batch: 080 ----
mean loss: 378.54
 ---- batch: 090 ----
mean loss: 377.43
train mean loss: 381.40
epoch train time: 0:00:02.336351
elapsed time: 0:06:05.270436
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 22:10:36.301683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.59
 ---- batch: 020 ----
mean loss: 362.32
 ---- batch: 030 ----
mean loss: 361.73
 ---- batch: 040 ----
mean loss: 385.99
 ---- batch: 050 ----
mean loss: 385.22
 ---- batch: 060 ----
mean loss: 374.79
 ---- batch: 070 ----
mean loss: 362.20
 ---- batch: 080 ----
mean loss: 379.21
 ---- batch: 090 ----
mean loss: 388.05
train mean loss: 375.17
epoch train time: 0:00:02.337700
elapsed time: 0:06:07.608333
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 22:10:38.639589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.24
 ---- batch: 020 ----
mean loss: 374.91
 ---- batch: 030 ----
mean loss: 363.50
 ---- batch: 040 ----
mean loss: 367.19
 ---- batch: 050 ----
mean loss: 372.80
 ---- batch: 060 ----
mean loss: 359.72
 ---- batch: 070 ----
mean loss: 385.56
 ---- batch: 080 ----
mean loss: 381.30
 ---- batch: 090 ----
mean loss: 385.93
train mean loss: 374.93
epoch train time: 0:00:02.343488
elapsed time: 0:06:09.952051
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 22:10:40.983292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.38
 ---- batch: 020 ----
mean loss: 380.85
 ---- batch: 030 ----
mean loss: 372.60
 ---- batch: 040 ----
mean loss: 378.35
 ---- batch: 050 ----
mean loss: 360.74
 ---- batch: 060 ----
mean loss: 377.30
 ---- batch: 070 ----
mean loss: 366.92
 ---- batch: 080 ----
mean loss: 368.09
 ---- batch: 090 ----
mean loss: 378.61
train mean loss: 371.88
epoch train time: 0:00:02.334016
elapsed time: 0:06:12.286254
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 22:10:43.317475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.53
 ---- batch: 020 ----
mean loss: 360.70
 ---- batch: 030 ----
mean loss: 362.07
 ---- batch: 040 ----
mean loss: 373.35
 ---- batch: 050 ----
mean loss: 371.10
 ---- batch: 060 ----
mean loss: 378.74
 ---- batch: 070 ----
mean loss: 364.64
 ---- batch: 080 ----
mean loss: 382.19
 ---- batch: 090 ----
mean loss: 371.38
train mean loss: 369.35
epoch train time: 0:00:02.332671
elapsed time: 0:06:14.619103
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 22:10:45.650329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.67
 ---- batch: 020 ----
mean loss: 373.05
 ---- batch: 030 ----
mean loss: 373.50
 ---- batch: 040 ----
mean loss: 369.87
 ---- batch: 050 ----
mean loss: 372.56
 ---- batch: 060 ----
mean loss: 363.73
 ---- batch: 070 ----
mean loss: 379.72
 ---- batch: 080 ----
mean loss: 389.77
 ---- batch: 090 ----
mean loss: 360.11
train mean loss: 374.74
epoch train time: 0:00:02.343913
elapsed time: 0:06:16.963223
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 22:10:47.994446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.70
 ---- batch: 020 ----
mean loss: 385.58
 ---- batch: 030 ----
mean loss: 361.20
 ---- batch: 040 ----
mean loss: 361.49
 ---- batch: 050 ----
mean loss: 361.70
 ---- batch: 060 ----
mean loss: 367.04
 ---- batch: 070 ----
mean loss: 361.31
 ---- batch: 080 ----
mean loss: 375.30
 ---- batch: 090 ----
mean loss: 371.78
train mean loss: 368.04
epoch train time: 0:00:02.331809
elapsed time: 0:06:19.295207
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 22:10:50.326435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.56
 ---- batch: 020 ----
mean loss: 359.85
 ---- batch: 030 ----
mean loss: 360.61
 ---- batch: 040 ----
mean loss: 369.85
 ---- batch: 050 ----
mean loss: 362.41
 ---- batch: 060 ----
mean loss: 368.36
 ---- batch: 070 ----
mean loss: 390.96
 ---- batch: 080 ----
mean loss: 405.14
 ---- batch: 090 ----
mean loss: 390.27
train mean loss: 374.66
epoch train time: 0:00:02.333014
elapsed time: 0:06:21.628434
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 22:10:52.659640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.84
 ---- batch: 020 ----
mean loss: 384.91
 ---- batch: 030 ----
mean loss: 381.66
 ---- batch: 040 ----
mean loss: 398.25
 ---- batch: 050 ----
mean loss: 377.37
 ---- batch: 060 ----
mean loss: 384.10
 ---- batch: 070 ----
mean loss: 370.98
 ---- batch: 080 ----
mean loss: 356.17
 ---- batch: 090 ----
mean loss: 363.99
train mean loss: 375.67
epoch train time: 0:00:02.341639
elapsed time: 0:06:23.970231
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 22:10:55.001475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.46
 ---- batch: 020 ----
mean loss: 364.86
 ---- batch: 030 ----
mean loss: 363.84
 ---- batch: 040 ----
mean loss: 378.36
 ---- batch: 050 ----
mean loss: 380.50
 ---- batch: 060 ----
mean loss: 378.13
 ---- batch: 070 ----
mean loss: 370.49
 ---- batch: 080 ----
mean loss: 378.23
 ---- batch: 090 ----
mean loss: 372.76
train mean loss: 374.77
epoch train time: 0:00:02.334548
elapsed time: 0:06:26.304989
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 22:10:57.336215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.87
 ---- batch: 020 ----
mean loss: 380.78
 ---- batch: 030 ----
mean loss: 371.65
 ---- batch: 040 ----
mean loss: 368.34
 ---- batch: 050 ----
mean loss: 361.51
 ---- batch: 060 ----
mean loss: 385.14
 ---- batch: 070 ----
mean loss: 391.38
 ---- batch: 080 ----
mean loss: 368.73
 ---- batch: 090 ----
mean loss: 370.50
train mean loss: 376.43
epoch train time: 0:00:02.340614
elapsed time: 0:06:28.645793
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 22:10:59.677019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.59
 ---- batch: 020 ----
mean loss: 362.32
 ---- batch: 030 ----
mean loss: 376.23
 ---- batch: 040 ----
mean loss: 365.82
 ---- batch: 050 ----
mean loss: 371.07
 ---- batch: 060 ----
mean loss: 377.00
 ---- batch: 070 ----
mean loss: 374.06
 ---- batch: 080 ----
mean loss: 360.08
 ---- batch: 090 ----
mean loss: 362.41
train mean loss: 369.63
epoch train time: 0:00:02.349894
elapsed time: 0:06:30.995868
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 22:11:02.027106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.75
 ---- batch: 020 ----
mean loss: 354.61
 ---- batch: 030 ----
mean loss: 365.09
 ---- batch: 040 ----
mean loss: 368.44
 ---- batch: 050 ----
mean loss: 371.01
 ---- batch: 060 ----
mean loss: 357.80
 ---- batch: 070 ----
mean loss: 360.37
 ---- batch: 080 ----
mean loss: 353.14
 ---- batch: 090 ----
mean loss: 363.07
train mean loss: 363.03
epoch train time: 0:00:02.335140
elapsed time: 0:06:33.331219
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 22:11:04.362461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.66
 ---- batch: 020 ----
mean loss: 357.20
 ---- batch: 030 ----
mean loss: 357.07
 ---- batch: 040 ----
mean loss: 368.21
 ---- batch: 050 ----
mean loss: 368.18
 ---- batch: 060 ----
mean loss: 384.06
 ---- batch: 070 ----
mean loss: 369.74
 ---- batch: 080 ----
mean loss: 368.83
 ---- batch: 090 ----
mean loss: 363.14
train mean loss: 367.33
epoch train time: 0:00:02.340102
elapsed time: 0:06:35.671555
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 22:11:06.702783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.56
 ---- batch: 020 ----
mean loss: 367.88
 ---- batch: 030 ----
mean loss: 368.98
 ---- batch: 040 ----
mean loss: 367.49
 ---- batch: 050 ----
mean loss: 376.55
 ---- batch: 060 ----
mean loss: 375.43
 ---- batch: 070 ----
mean loss: 365.85
 ---- batch: 080 ----
mean loss: 354.53
 ---- batch: 090 ----
mean loss: 362.10
train mean loss: 368.11
epoch train time: 0:00:02.341684
elapsed time: 0:06:38.013433
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 22:11:09.044698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.76
 ---- batch: 020 ----
mean loss: 369.31
 ---- batch: 030 ----
mean loss: 361.36
 ---- batch: 040 ----
mean loss: 377.98
 ---- batch: 050 ----
mean loss: 362.73
 ---- batch: 060 ----
mean loss: 369.06
 ---- batch: 070 ----
mean loss: 383.75
 ---- batch: 080 ----
mean loss: 387.15
 ---- batch: 090 ----
mean loss: 364.46
train mean loss: 371.98
epoch train time: 0:00:02.336811
elapsed time: 0:06:40.350513
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 22:11:11.381756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.51
 ---- batch: 020 ----
mean loss: 363.45
 ---- batch: 030 ----
mean loss: 353.97
 ---- batch: 040 ----
mean loss: 353.92
 ---- batch: 050 ----
mean loss: 372.23
 ---- batch: 060 ----
mean loss: 359.85
 ---- batch: 070 ----
mean loss: 365.82
 ---- batch: 080 ----
mean loss: 359.93
 ---- batch: 090 ----
mean loss: 363.71
train mean loss: 362.62
epoch train time: 0:00:02.340711
elapsed time: 0:06:42.691447
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 22:11:13.722747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.59
 ---- batch: 020 ----
mean loss: 380.97
 ---- batch: 030 ----
mean loss: 377.13
 ---- batch: 040 ----
mean loss: 361.57
 ---- batch: 050 ----
mean loss: 364.43
 ---- batch: 060 ----
mean loss: 355.80
 ---- batch: 070 ----
mean loss: 367.29
 ---- batch: 080 ----
mean loss: 371.86
 ---- batch: 090 ----
mean loss: 370.26
train mean loss: 370.76
epoch train time: 0:00:02.333739
elapsed time: 0:06:45.025468
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 22:11:16.056710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.96
 ---- batch: 020 ----
mean loss: 362.59
 ---- batch: 030 ----
mean loss: 365.35
 ---- batch: 040 ----
mean loss: 366.47
 ---- batch: 050 ----
mean loss: 380.36
 ---- batch: 060 ----
mean loss: 368.30
 ---- batch: 070 ----
mean loss: 363.36
 ---- batch: 080 ----
mean loss: 364.85
 ---- batch: 090 ----
mean loss: 360.15
train mean loss: 368.40
epoch train time: 0:00:02.336816
elapsed time: 0:06:47.362514
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 22:11:18.393751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.69
 ---- batch: 020 ----
mean loss: 362.46
 ---- batch: 030 ----
mean loss: 356.58
 ---- batch: 040 ----
mean loss: 376.73
 ---- batch: 050 ----
mean loss: 366.55
 ---- batch: 060 ----
mean loss: 366.18
 ---- batch: 070 ----
mean loss: 364.79
 ---- batch: 080 ----
mean loss: 356.63
 ---- batch: 090 ----
mean loss: 362.41
train mean loss: 363.81
epoch train time: 0:00:02.342432
elapsed time: 0:06:49.705133
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 22:11:20.736356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.66
 ---- batch: 020 ----
mean loss: 365.22
 ---- batch: 030 ----
mean loss: 366.53
 ---- batch: 040 ----
mean loss: 364.24
 ---- batch: 050 ----
mean loss: 369.74
 ---- batch: 060 ----
mean loss: 370.15
 ---- batch: 070 ----
mean loss: 352.89
 ---- batch: 080 ----
mean loss: 352.93
 ---- batch: 090 ----
mean loss: 355.60
train mean loss: 363.41
epoch train time: 0:00:02.334831
elapsed time: 0:06:52.040144
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 22:11:23.071369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.18
 ---- batch: 020 ----
mean loss: 366.65
 ---- batch: 030 ----
mean loss: 361.36
 ---- batch: 040 ----
mean loss: 360.03
 ---- batch: 050 ----
mean loss: 368.14
 ---- batch: 060 ----
mean loss: 354.90
 ---- batch: 070 ----
mean loss: 365.53
 ---- batch: 080 ----
mean loss: 369.67
 ---- batch: 090 ----
mean loss: 356.61
train mean loss: 362.43
epoch train time: 0:00:02.337924
elapsed time: 0:06:54.378245
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 22:11:25.409471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.24
 ---- batch: 020 ----
mean loss: 367.15
 ---- batch: 030 ----
mean loss: 396.27
 ---- batch: 040 ----
mean loss: 378.34
 ---- batch: 050 ----
mean loss: 362.14
 ---- batch: 060 ----
mean loss: 369.46
 ---- batch: 070 ----
mean loss: 362.91
 ---- batch: 080 ----
mean loss: 369.05
 ---- batch: 090 ----
mean loss: 374.16
train mean loss: 370.53
epoch train time: 0:00:02.340099
elapsed time: 0:06:56.718598
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 22:11:27.749823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.66
 ---- batch: 020 ----
mean loss: 365.66
 ---- batch: 030 ----
mean loss: 364.18
 ---- batch: 040 ----
mean loss: 350.60
 ---- batch: 050 ----
mean loss: 376.18
 ---- batch: 060 ----
mean loss: 360.81
 ---- batch: 070 ----
mean loss: 357.51
 ---- batch: 080 ----
mean loss: 364.94
 ---- batch: 090 ----
mean loss: 359.12
train mean loss: 363.85
epoch train time: 0:00:02.335035
elapsed time: 0:06:59.053872
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 22:11:30.085127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.45
 ---- batch: 020 ----
mean loss: 360.63
 ---- batch: 030 ----
mean loss: 353.11
 ---- batch: 040 ----
mean loss: 370.03
 ---- batch: 050 ----
mean loss: 361.00
 ---- batch: 060 ----
mean loss: 376.93
 ---- batch: 070 ----
mean loss: 357.76
 ---- batch: 080 ----
mean loss: 365.12
 ---- batch: 090 ----
mean loss: 363.32
train mean loss: 363.78
epoch train time: 0:00:02.338081
elapsed time: 0:07:01.392215
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 22:11:32.423468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.22
 ---- batch: 020 ----
mean loss: 358.14
 ---- batch: 030 ----
mean loss: 358.53
 ---- batch: 040 ----
mean loss: 354.95
 ---- batch: 050 ----
mean loss: 375.79
 ---- batch: 060 ----
mean loss: 363.32
 ---- batch: 070 ----
mean loss: 370.73
 ---- batch: 080 ----
mean loss: 366.89
 ---- batch: 090 ----
mean loss: 381.62
train mean loss: 364.81
epoch train time: 0:00:02.343280
elapsed time: 0:07:03.735704
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 22:11:34.766943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.41
 ---- batch: 020 ----
mean loss: 359.98
 ---- batch: 030 ----
mean loss: 359.07
 ---- batch: 040 ----
mean loss: 345.92
 ---- batch: 050 ----
mean loss: 357.58
 ---- batch: 060 ----
mean loss: 357.16
 ---- batch: 070 ----
mean loss: 365.36
 ---- batch: 080 ----
mean loss: 374.04
 ---- batch: 090 ----
mean loss: 369.60
train mean loss: 361.12
epoch train time: 0:00:02.338367
elapsed time: 0:07:06.074271
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 22:11:37.105504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.21
 ---- batch: 020 ----
mean loss: 361.63
 ---- batch: 030 ----
mean loss: 365.47
 ---- batch: 040 ----
mean loss: 351.85
 ---- batch: 050 ----
mean loss: 364.28
 ---- batch: 060 ----
mean loss: 375.12
 ---- batch: 070 ----
mean loss: 354.78
 ---- batch: 080 ----
mean loss: 367.81
 ---- batch: 090 ----
mean loss: 362.86
train mean loss: 363.78
epoch train time: 0:00:02.336520
elapsed time: 0:07:08.411073
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 22:11:39.442331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.42
 ---- batch: 020 ----
mean loss: 352.97
 ---- batch: 030 ----
mean loss: 368.37
 ---- batch: 040 ----
mean loss: 358.04
 ---- batch: 050 ----
mean loss: 369.33
 ---- batch: 060 ----
mean loss: 374.24
 ---- batch: 070 ----
mean loss: 359.58
 ---- batch: 080 ----
mean loss: 363.73
 ---- batch: 090 ----
mean loss: 367.35
train mean loss: 364.65
epoch train time: 0:00:02.340117
elapsed time: 0:07:10.751449
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 22:11:41.782708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.13
 ---- batch: 020 ----
mean loss: 377.23
 ---- batch: 030 ----
mean loss: 364.88
 ---- batch: 040 ----
mean loss: 359.78
 ---- batch: 050 ----
mean loss: 360.68
 ---- batch: 060 ----
mean loss: 352.14
 ---- batch: 070 ----
mean loss: 357.23
 ---- batch: 080 ----
mean loss: 343.20
 ---- batch: 090 ----
mean loss: 362.65
train mean loss: 363.60
epoch train time: 0:00:02.337826
elapsed time: 0:07:13.089511
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 22:11:44.120735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.21
 ---- batch: 020 ----
mean loss: 367.85
 ---- batch: 030 ----
mean loss: 378.55
 ---- batch: 040 ----
mean loss: 379.80
 ---- batch: 050 ----
mean loss: 366.83
 ---- batch: 060 ----
mean loss: 381.10
 ---- batch: 070 ----
mean loss: 370.35
 ---- batch: 080 ----
mean loss: 354.14
 ---- batch: 090 ----
mean loss: 367.38
train mean loss: 369.87
epoch train time: 0:00:02.342662
elapsed time: 0:07:15.432348
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 22:11:46.463576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.25
 ---- batch: 020 ----
mean loss: 348.98
 ---- batch: 030 ----
mean loss: 350.53
 ---- batch: 040 ----
mean loss: 365.96
 ---- batch: 050 ----
mean loss: 360.84
 ---- batch: 060 ----
mean loss: 354.49
 ---- batch: 070 ----
mean loss: 364.35
 ---- batch: 080 ----
mean loss: 351.59
 ---- batch: 090 ----
mean loss: 351.71
train mean loss: 358.24
epoch train time: 0:00:02.334736
elapsed time: 0:07:17.767275
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 22:11:48.798528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.39
 ---- batch: 020 ----
mean loss: 350.01
 ---- batch: 030 ----
mean loss: 362.46
 ---- batch: 040 ----
mean loss: 355.40
 ---- batch: 050 ----
mean loss: 365.61
 ---- batch: 060 ----
mean loss: 363.69
 ---- batch: 070 ----
mean loss: 352.36
 ---- batch: 080 ----
mean loss: 384.02
 ---- batch: 090 ----
mean loss: 364.73
train mean loss: 361.45
epoch train time: 0:00:02.332693
elapsed time: 0:07:20.100227
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 22:11:51.131437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.33
 ---- batch: 020 ----
mean loss: 357.16
 ---- batch: 030 ----
mean loss: 373.14
 ---- batch: 040 ----
mean loss: 355.81
 ---- batch: 050 ----
mean loss: 359.43
 ---- batch: 060 ----
mean loss: 367.59
 ---- batch: 070 ----
mean loss: 351.52
 ---- batch: 080 ----
mean loss: 361.00
 ---- batch: 090 ----
mean loss: 349.63
train mean loss: 360.43
epoch train time: 0:00:02.337607
elapsed time: 0:07:22.437992
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 22:11:53.469215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.44
 ---- batch: 020 ----
mean loss: 350.34
 ---- batch: 030 ----
mean loss: 348.00
 ---- batch: 040 ----
mean loss: 367.94
 ---- batch: 050 ----
mean loss: 349.28
 ---- batch: 060 ----
mean loss: 356.73
 ---- batch: 070 ----
mean loss: 359.72
 ---- batch: 080 ----
mean loss: 349.26
 ---- batch: 090 ----
mean loss: 347.73
train mean loss: 354.31
epoch train time: 0:00:02.342252
elapsed time: 0:07:24.780450
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 22:11:55.811686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.65
 ---- batch: 020 ----
mean loss: 358.33
 ---- batch: 030 ----
mean loss: 363.58
 ---- batch: 040 ----
mean loss: 366.47
 ---- batch: 050 ----
mean loss: 350.82
 ---- batch: 060 ----
mean loss: 360.35
 ---- batch: 070 ----
mean loss: 349.73
 ---- batch: 080 ----
mean loss: 347.47
 ---- batch: 090 ----
mean loss: 357.66
train mean loss: 356.73
epoch train time: 0:00:02.335341
elapsed time: 0:07:27.115982
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 22:11:58.147210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.99
 ---- batch: 020 ----
mean loss: 377.66
 ---- batch: 030 ----
mean loss: 352.94
 ---- batch: 040 ----
mean loss: 353.31
 ---- batch: 050 ----
mean loss: 349.07
 ---- batch: 060 ----
mean loss: 345.81
 ---- batch: 070 ----
mean loss: 368.03
 ---- batch: 080 ----
mean loss: 348.11
 ---- batch: 090 ----
mean loss: 350.21
train mean loss: 356.13
epoch train time: 0:00:02.333046
elapsed time: 0:07:29.449209
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 22:12:00.480433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.84
 ---- batch: 020 ----
mean loss: 349.27
 ---- batch: 030 ----
mean loss: 357.06
 ---- batch: 040 ----
mean loss: 351.61
 ---- batch: 050 ----
mean loss: 348.86
 ---- batch: 060 ----
mean loss: 344.25
 ---- batch: 070 ----
mean loss: 354.12
 ---- batch: 080 ----
mean loss: 352.21
 ---- batch: 090 ----
mean loss: 359.90
train mean loss: 354.62
epoch train time: 0:00:02.342296
elapsed time: 0:07:31.791681
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 22:12:02.822918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.21
 ---- batch: 020 ----
mean loss: 359.02
 ---- batch: 030 ----
mean loss: 367.90
 ---- batch: 040 ----
mean loss: 349.54
 ---- batch: 050 ----
mean loss: 360.24
 ---- batch: 060 ----
mean loss: 358.26
 ---- batch: 070 ----
mean loss: 352.15
 ---- batch: 080 ----
mean loss: 362.35
 ---- batch: 090 ----
mean loss: 357.83
train mean loss: 359.35
epoch train time: 0:00:02.335473
elapsed time: 0:07:34.127353
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 22:12:05.158579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.90
 ---- batch: 020 ----
mean loss: 344.95
 ---- batch: 030 ----
mean loss: 347.37
 ---- batch: 040 ----
mean loss: 358.17
 ---- batch: 050 ----
mean loss: 355.06
 ---- batch: 060 ----
mean loss: 353.08
 ---- batch: 070 ----
mean loss: 355.55
 ---- batch: 080 ----
mean loss: 357.28
 ---- batch: 090 ----
mean loss: 349.12
train mean loss: 353.22
epoch train time: 0:00:02.338950
elapsed time: 0:07:36.466500
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 22:12:07.497748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.47
 ---- batch: 020 ----
mean loss: 346.83
 ---- batch: 030 ----
mean loss: 354.44
 ---- batch: 040 ----
mean loss: 349.76
 ---- batch: 050 ----
mean loss: 341.27
 ---- batch: 060 ----
mean loss: 346.82
 ---- batch: 070 ----
mean loss: 349.88
 ---- batch: 080 ----
mean loss: 350.47
 ---- batch: 090 ----
mean loss: 352.46
train mean loss: 350.29
epoch train time: 0:00:02.340800
elapsed time: 0:07:38.807508
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 22:12:09.838733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.30
 ---- batch: 020 ----
mean loss: 357.45
 ---- batch: 030 ----
mean loss: 370.38
 ---- batch: 040 ----
mean loss: 354.44
 ---- batch: 050 ----
mean loss: 352.19
 ---- batch: 060 ----
mean loss: 371.89
 ---- batch: 070 ----
mean loss: 353.51
 ---- batch: 080 ----
mean loss: 358.35
 ---- batch: 090 ----
mean loss: 358.76
train mean loss: 357.81
epoch train time: 0:00:02.337031
elapsed time: 0:07:41.144724
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 22:12:12.175951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.46
 ---- batch: 020 ----
mean loss: 357.75
 ---- batch: 030 ----
mean loss: 342.49
 ---- batch: 040 ----
mean loss: 359.44
 ---- batch: 050 ----
mean loss: 366.78
 ---- batch: 060 ----
mean loss: 357.40
 ---- batch: 070 ----
mean loss: 362.97
 ---- batch: 080 ----
mean loss: 357.24
 ---- batch: 090 ----
mean loss: 351.28
train mean loss: 356.44
epoch train time: 0:00:02.344431
elapsed time: 0:07:43.489361
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 22:12:14.520586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.91
 ---- batch: 020 ----
mean loss: 355.99
 ---- batch: 030 ----
mean loss: 352.11
 ---- batch: 040 ----
mean loss: 358.83
 ---- batch: 050 ----
mean loss: 360.35
 ---- batch: 060 ----
mean loss: 349.27
 ---- batch: 070 ----
mean loss: 338.41
 ---- batch: 080 ----
mean loss: 346.20
 ---- batch: 090 ----
mean loss: 356.01
train mean loss: 353.40
epoch train time: 0:00:02.332270
elapsed time: 0:07:45.821824
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 22:12:16.853052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.85
 ---- batch: 020 ----
mean loss: 338.05
 ---- batch: 030 ----
mean loss: 352.03
 ---- batch: 040 ----
mean loss: 353.43
 ---- batch: 050 ----
mean loss: 358.06
 ---- batch: 060 ----
mean loss: 338.95
 ---- batch: 070 ----
mean loss: 353.79
 ---- batch: 080 ----
mean loss: 352.51
 ---- batch: 090 ----
mean loss: 368.12
train mean loss: 351.82
epoch train time: 0:00:02.340141
elapsed time: 0:07:48.162155
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 22:12:19.193379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.26
 ---- batch: 020 ----
mean loss: 349.43
 ---- batch: 030 ----
mean loss: 344.66
 ---- batch: 040 ----
mean loss: 344.90
 ---- batch: 050 ----
mean loss: 333.18
 ---- batch: 060 ----
mean loss: 350.89
 ---- batch: 070 ----
mean loss: 354.45
 ---- batch: 080 ----
mean loss: 351.42
 ---- batch: 090 ----
mean loss: 364.09
train mean loss: 348.65
epoch train time: 0:00:02.343530
elapsed time: 0:07:50.505878
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 22:12:21.537105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.75
 ---- batch: 020 ----
mean loss: 359.61
 ---- batch: 030 ----
mean loss: 342.14
 ---- batch: 040 ----
mean loss: 347.49
 ---- batch: 050 ----
mean loss: 372.25
 ---- batch: 060 ----
mean loss: 352.86
 ---- batch: 070 ----
mean loss: 343.83
 ---- batch: 080 ----
mean loss: 351.08
 ---- batch: 090 ----
mean loss: 351.60
train mean loss: 352.60
epoch train time: 0:00:02.339163
elapsed time: 0:07:52.845240
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 22:12:23.876497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.61
 ---- batch: 020 ----
mean loss: 373.18
 ---- batch: 030 ----
mean loss: 366.87
 ---- batch: 040 ----
mean loss: 348.82
 ---- batch: 050 ----
mean loss: 353.15
 ---- batch: 060 ----
mean loss: 350.98
 ---- batch: 070 ----
mean loss: 343.89
 ---- batch: 080 ----
mean loss: 343.94
 ---- batch: 090 ----
mean loss: 345.86
train mean loss: 352.23
epoch train time: 0:00:02.336901
elapsed time: 0:07:55.182360
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 22:12:26.213599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.70
 ---- batch: 020 ----
mean loss: 344.31
 ---- batch: 030 ----
mean loss: 359.75
 ---- batch: 040 ----
mean loss: 336.36
 ---- batch: 050 ----
mean loss: 352.89
 ---- batch: 060 ----
mean loss: 349.27
 ---- batch: 070 ----
mean loss: 364.71
 ---- batch: 080 ----
mean loss: 352.91
 ---- batch: 090 ----
mean loss: 360.76
train mean loss: 353.14
epoch train time: 0:00:02.345814
elapsed time: 0:07:57.528406
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 22:12:28.559635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.53
 ---- batch: 020 ----
mean loss: 348.26
 ---- batch: 030 ----
mean loss: 361.37
 ---- batch: 040 ----
mean loss: 353.23
 ---- batch: 050 ----
mean loss: 363.34
 ---- batch: 060 ----
mean loss: 348.80
 ---- batch: 070 ----
mean loss: 342.59
 ---- batch: 080 ----
mean loss: 351.55
 ---- batch: 090 ----
mean loss: 350.77
train mean loss: 350.31
epoch train time: 0:00:02.342110
elapsed time: 0:07:59.870716
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 22:12:30.901948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.84
 ---- batch: 020 ----
mean loss: 362.13
 ---- batch: 030 ----
mean loss: 355.44
 ---- batch: 040 ----
mean loss: 350.38
 ---- batch: 050 ----
mean loss: 351.63
 ---- batch: 060 ----
mean loss: 351.46
 ---- batch: 070 ----
mean loss: 354.50
 ---- batch: 080 ----
mean loss: 341.25
 ---- batch: 090 ----
mean loss: 362.99
train mean loss: 353.32
epoch train time: 0:00:02.340186
elapsed time: 0:08:02.211087
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 22:12:33.242314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.76
 ---- batch: 020 ----
mean loss: 337.94
 ---- batch: 030 ----
mean loss: 339.00
 ---- batch: 040 ----
mean loss: 347.99
 ---- batch: 050 ----
mean loss: 343.15
 ---- batch: 060 ----
mean loss: 349.96
 ---- batch: 070 ----
mean loss: 348.73
 ---- batch: 080 ----
mean loss: 355.30
 ---- batch: 090 ----
mean loss: 349.43
train mean loss: 348.05
epoch train time: 0:00:02.342865
elapsed time: 0:08:04.554140
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 22:12:35.585364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.10
 ---- batch: 020 ----
mean loss: 353.24
 ---- batch: 030 ----
mean loss: 361.37
 ---- batch: 040 ----
mean loss: 356.20
 ---- batch: 050 ----
mean loss: 348.41
 ---- batch: 060 ----
mean loss: 337.90
 ---- batch: 070 ----
mean loss: 345.69
 ---- batch: 080 ----
mean loss: 365.82
 ---- batch: 090 ----
mean loss: 357.77
train mean loss: 352.53
epoch train time: 0:00:02.335147
elapsed time: 0:08:06.889471
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 22:12:37.920712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.90
 ---- batch: 020 ----
mean loss: 337.38
 ---- batch: 030 ----
mean loss: 343.43
 ---- batch: 040 ----
mean loss: 348.87
 ---- batch: 050 ----
mean loss: 342.76
 ---- batch: 060 ----
mean loss: 348.59
 ---- batch: 070 ----
mean loss: 353.09
 ---- batch: 080 ----
mean loss: 345.53
 ---- batch: 090 ----
mean loss: 340.88
train mean loss: 344.87
epoch train time: 0:00:02.340486
elapsed time: 0:08:09.230153
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 22:12:40.261393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.31
 ---- batch: 020 ----
mean loss: 341.23
 ---- batch: 030 ----
mean loss: 358.37
 ---- batch: 040 ----
mean loss: 342.68
 ---- batch: 050 ----
mean loss: 349.36
 ---- batch: 060 ----
mean loss: 354.82
 ---- batch: 070 ----
mean loss: 363.80
 ---- batch: 080 ----
mean loss: 361.26
 ---- batch: 090 ----
mean loss: 361.29
train mean loss: 354.81
epoch train time: 0:00:02.336284
elapsed time: 0:08:11.566635
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 22:12:42.597899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.07
 ---- batch: 020 ----
mean loss: 355.77
 ---- batch: 030 ----
mean loss: 356.18
 ---- batch: 040 ----
mean loss: 348.28
 ---- batch: 050 ----
mean loss: 355.67
 ---- batch: 060 ----
mean loss: 350.66
 ---- batch: 070 ----
mean loss: 353.27
 ---- batch: 080 ----
mean loss: 351.04
 ---- batch: 090 ----
mean loss: 362.36
train mean loss: 353.33
epoch train time: 0:00:02.331287
elapsed time: 0:08:13.898152
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 22:12:44.929378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.27
 ---- batch: 020 ----
mean loss: 348.56
 ---- batch: 030 ----
mean loss: 349.24
 ---- batch: 040 ----
mean loss: 340.76
 ---- batch: 050 ----
mean loss: 343.05
 ---- batch: 060 ----
mean loss: 352.47
 ---- batch: 070 ----
mean loss: 350.99
 ---- batch: 080 ----
mean loss: 346.86
 ---- batch: 090 ----
mean loss: 351.64
train mean loss: 347.75
epoch train time: 0:00:02.349922
elapsed time: 0:08:16.248286
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 22:12:47.279510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.67
 ---- batch: 020 ----
mean loss: 343.58
 ---- batch: 030 ----
mean loss: 344.67
 ---- batch: 040 ----
mean loss: 347.99
 ---- batch: 050 ----
mean loss: 351.40
 ---- batch: 060 ----
mean loss: 337.41
 ---- batch: 070 ----
mean loss: 348.45
 ---- batch: 080 ----
mean loss: 367.71
 ---- batch: 090 ----
mean loss: 340.46
train mean loss: 347.40
epoch train time: 0:00:02.336507
elapsed time: 0:08:18.584968
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 22:12:49.616194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.56
 ---- batch: 020 ----
mean loss: 332.42
 ---- batch: 030 ----
mean loss: 346.93
 ---- batch: 040 ----
mean loss: 340.48
 ---- batch: 050 ----
mean loss: 341.65
 ---- batch: 060 ----
mean loss: 341.45
 ---- batch: 070 ----
mean loss: 348.36
 ---- batch: 080 ----
mean loss: 350.02
 ---- batch: 090 ----
mean loss: 351.69
train mean loss: 345.31
epoch train time: 0:00:02.339187
elapsed time: 0:08:20.924340
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 22:12:51.955573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.05
 ---- batch: 020 ----
mean loss: 350.71
 ---- batch: 030 ----
mean loss: 352.23
 ---- batch: 040 ----
mean loss: 341.10
 ---- batch: 050 ----
mean loss: 332.64
 ---- batch: 060 ----
mean loss: 336.05
 ---- batch: 070 ----
mean loss: 347.98
 ---- batch: 080 ----
mean loss: 351.70
 ---- batch: 090 ----
mean loss: 364.45
train mean loss: 349.20
epoch train time: 0:00:02.347032
elapsed time: 0:08:23.271563
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 22:12:54.302791
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 335.64
 ---- batch: 020 ----
mean loss: 337.56
 ---- batch: 030 ----
mean loss: 334.65
 ---- batch: 040 ----
mean loss: 349.07
 ---- batch: 050 ----
mean loss: 343.52
 ---- batch: 060 ----
mean loss: 321.29
 ---- batch: 070 ----
mean loss: 338.18
 ---- batch: 080 ----
mean loss: 342.97
 ---- batch: 090 ----
mean loss: 344.32
train mean loss: 338.87
epoch train time: 0:00:02.335107
elapsed time: 0:08:25.606879
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 22:12:56.638088
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 335.74
 ---- batch: 020 ----
mean loss: 343.02
 ---- batch: 030 ----
mean loss: 333.78
 ---- batch: 040 ----
mean loss: 339.46
 ---- batch: 050 ----
mean loss: 341.78
 ---- batch: 060 ----
mean loss: 330.68
 ---- batch: 070 ----
mean loss: 334.45
 ---- batch: 080 ----
mean loss: 342.89
 ---- batch: 090 ----
mean loss: 342.83
train mean loss: 339.41
epoch train time: 0:00:02.336541
elapsed time: 0:08:27.943595
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 22:12:58.974842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 335.14
 ---- batch: 020 ----
mean loss: 336.64
 ---- batch: 030 ----
mean loss: 338.98
 ---- batch: 040 ----
mean loss: 330.94
 ---- batch: 050 ----
mean loss: 340.86
 ---- batch: 060 ----
mean loss: 340.04
 ---- batch: 070 ----
mean loss: 342.48
 ---- batch: 080 ----
mean loss: 344.38
 ---- batch: 090 ----
mean loss: 341.63
train mean loss: 337.90
epoch train time: 0:00:02.335678
elapsed time: 0:08:30.279463
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 22:13:01.310683
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 334.88
 ---- batch: 020 ----
mean loss: 337.31
 ---- batch: 030 ----
mean loss: 337.49
 ---- batch: 040 ----
mean loss: 333.35
 ---- batch: 050 ----
mean loss: 331.98
 ---- batch: 060 ----
mean loss: 342.10
 ---- batch: 070 ----
mean loss: 328.82
 ---- batch: 080 ----
mean loss: 347.13
 ---- batch: 090 ----
mean loss: 342.46
train mean loss: 337.06
epoch train time: 0:00:02.340046
elapsed time: 0:08:32.619686
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 22:13:03.650917
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 336.21
 ---- batch: 020 ----
mean loss: 342.50
 ---- batch: 030 ----
mean loss: 344.54
 ---- batch: 040 ----
mean loss: 323.38
 ---- batch: 050 ----
mean loss: 341.80
 ---- batch: 060 ----
mean loss: 332.78
 ---- batch: 070 ----
mean loss: 335.57
 ---- batch: 080 ----
mean loss: 351.00
 ---- batch: 090 ----
mean loss: 334.24
train mean loss: 337.38
epoch train time: 0:00:02.342899
elapsed time: 0:08:34.962790
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 22:13:05.994032
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 334.75
 ---- batch: 020 ----
mean loss: 331.16
 ---- batch: 030 ----
mean loss: 332.67
 ---- batch: 040 ----
mean loss: 342.52
 ---- batch: 050 ----
mean loss: 348.04
 ---- batch: 060 ----
mean loss: 341.85
 ---- batch: 070 ----
mean loss: 338.90
 ---- batch: 080 ----
mean loss: 345.85
 ---- batch: 090 ----
mean loss: 321.27
train mean loss: 337.80
epoch train time: 0:00:02.339814
elapsed time: 0:08:37.302805
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 22:13:08.334033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 338.20
 ---- batch: 020 ----
mean loss: 332.48
 ---- batch: 030 ----
mean loss: 332.49
 ---- batch: 040 ----
mean loss: 337.52
 ---- batch: 050 ----
mean loss: 333.20
 ---- batch: 060 ----
mean loss: 321.67
 ---- batch: 070 ----
mean loss: 340.99
 ---- batch: 080 ----
mean loss: 338.54
 ---- batch: 090 ----
mean loss: 336.37
train mean loss: 335.07
epoch train time: 0:00:02.340410
elapsed time: 0:08:39.643397
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 22:13:10.674620
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 328.23
 ---- batch: 020 ----
mean loss: 340.61
 ---- batch: 030 ----
mean loss: 336.94
 ---- batch: 040 ----
mean loss: 348.97
 ---- batch: 050 ----
mean loss: 338.00
 ---- batch: 060 ----
mean loss: 334.67
 ---- batch: 070 ----
mean loss: 336.14
 ---- batch: 080 ----
mean loss: 338.32
 ---- batch: 090 ----
mean loss: 330.44
train mean loss: 336.98
epoch train time: 0:00:02.340264
elapsed time: 0:08:41.983840
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 22:13:13.015067
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.16
 ---- batch: 020 ----
mean loss: 336.54
 ---- batch: 030 ----
mean loss: 343.24
 ---- batch: 040 ----
mean loss: 327.39
 ---- batch: 050 ----
mean loss: 333.94
 ---- batch: 060 ----
mean loss: 343.93
 ---- batch: 070 ----
mean loss: 332.17
 ---- batch: 080 ----
mean loss: 339.84
 ---- batch: 090 ----
mean loss: 345.29
train mean loss: 335.94
epoch train time: 0:00:02.338031
elapsed time: 0:08:44.322053
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 22:13:15.353281
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.37
 ---- batch: 020 ----
mean loss: 336.38
 ---- batch: 030 ----
mean loss: 339.01
 ---- batch: 040 ----
mean loss: 330.71
 ---- batch: 050 ----
mean loss: 321.90
 ---- batch: 060 ----
mean loss: 334.16
 ---- batch: 070 ----
mean loss: 331.42
 ---- batch: 080 ----
mean loss: 334.38
 ---- batch: 090 ----
mean loss: 331.73
train mean loss: 333.71
epoch train time: 0:00:02.339985
elapsed time: 0:08:46.662252
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 22:13:17.693507
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.73
 ---- batch: 020 ----
mean loss: 336.99
 ---- batch: 030 ----
mean loss: 329.21
 ---- batch: 040 ----
mean loss: 333.18
 ---- batch: 050 ----
mean loss: 331.32
 ---- batch: 060 ----
mean loss: 333.21
 ---- batch: 070 ----
mean loss: 331.55
 ---- batch: 080 ----
mean loss: 330.04
 ---- batch: 090 ----
mean loss: 337.89
train mean loss: 334.11
epoch train time: 0:00:02.351402
elapsed time: 0:08:49.013875
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 22:13:20.045103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 329.75
 ---- batch: 020 ----
mean loss: 336.62
 ---- batch: 030 ----
mean loss: 333.34
 ---- batch: 040 ----
mean loss: 345.75
 ---- batch: 050 ----
mean loss: 337.11
 ---- batch: 060 ----
mean loss: 339.08
 ---- batch: 070 ----
mean loss: 342.12
 ---- batch: 080 ----
mean loss: 330.83
 ---- batch: 090 ----
mean loss: 327.49
train mean loss: 335.96
epoch train time: 0:00:02.337353
elapsed time: 0:08:51.351415
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 22:13:22.382664
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 335.24
 ---- batch: 020 ----
mean loss: 335.52
 ---- batch: 030 ----
mean loss: 337.84
 ---- batch: 040 ----
mean loss: 341.23
 ---- batch: 050 ----
mean loss: 338.34
 ---- batch: 060 ----
mean loss: 338.56
 ---- batch: 070 ----
mean loss: 326.23
 ---- batch: 080 ----
mean loss: 339.63
 ---- batch: 090 ----
mean loss: 337.77
train mean loss: 335.94
epoch train time: 0:00:02.342363
elapsed time: 0:08:53.693985
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 22:13:24.725213
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 335.72
 ---- batch: 020 ----
mean loss: 324.74
 ---- batch: 030 ----
mean loss: 338.86
 ---- batch: 040 ----
mean loss: 339.04
 ---- batch: 050 ----
mean loss: 336.80
 ---- batch: 060 ----
mean loss: 331.57
 ---- batch: 070 ----
mean loss: 340.07
 ---- batch: 080 ----
mean loss: 339.23
 ---- batch: 090 ----
mean loss: 331.59
train mean loss: 336.00
epoch train time: 0:00:02.346089
elapsed time: 0:08:56.040268
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 22:13:27.071496
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 334.38
 ---- batch: 020 ----
mean loss: 334.77
 ---- batch: 030 ----
mean loss: 338.56
 ---- batch: 040 ----
mean loss: 337.06
 ---- batch: 050 ----
mean loss: 333.72
 ---- batch: 060 ----
mean loss: 334.57
 ---- batch: 070 ----
mean loss: 331.91
 ---- batch: 080 ----
mean loss: 332.10
 ---- batch: 090 ----
mean loss: 335.09
train mean loss: 335.36
epoch train time: 0:00:02.336178
elapsed time: 0:08:58.376632
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 22:13:29.407858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.97
 ---- batch: 020 ----
mean loss: 326.64
 ---- batch: 030 ----
mean loss: 337.86
 ---- batch: 040 ----
mean loss: 349.26
 ---- batch: 050 ----
mean loss: 341.17
 ---- batch: 060 ----
mean loss: 335.92
 ---- batch: 070 ----
mean loss: 329.43
 ---- batch: 080 ----
mean loss: 337.59
 ---- batch: 090 ----
mean loss: 334.16
train mean loss: 336.56
epoch train time: 0:00:02.338708
elapsed time: 0:09:00.715528
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 22:13:31.746752
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 331.00
 ---- batch: 020 ----
mean loss: 344.66
 ---- batch: 030 ----
mean loss: 324.81
 ---- batch: 040 ----
mean loss: 331.05
 ---- batch: 050 ----
mean loss: 342.86
 ---- batch: 060 ----
mean loss: 325.00
 ---- batch: 070 ----
mean loss: 328.68
 ---- batch: 080 ----
mean loss: 351.94
 ---- batch: 090 ----
mean loss: 333.82
train mean loss: 334.80
epoch train time: 0:00:02.342518
elapsed time: 0:09:03.058279
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 22:13:34.089527
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 336.42
 ---- batch: 020 ----
mean loss: 335.16
 ---- batch: 030 ----
mean loss: 334.63
 ---- batch: 040 ----
mean loss: 337.72
 ---- batch: 050 ----
mean loss: 333.72
 ---- batch: 060 ----
mean loss: 334.93
 ---- batch: 070 ----
mean loss: 332.22
 ---- batch: 080 ----
mean loss: 332.55
 ---- batch: 090 ----
mean loss: 324.30
train mean loss: 333.39
epoch train time: 0:00:02.333432
elapsed time: 0:09:05.391909
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 22:13:36.423134
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 335.46
 ---- batch: 020 ----
mean loss: 331.30
 ---- batch: 030 ----
mean loss: 332.43
 ---- batch: 040 ----
mean loss: 343.23
 ---- batch: 050 ----
mean loss: 338.16
 ---- batch: 060 ----
mean loss: 335.65
 ---- batch: 070 ----
mean loss: 333.31
 ---- batch: 080 ----
mean loss: 346.15
 ---- batch: 090 ----
mean loss: 330.14
train mean loss: 335.64
epoch train time: 0:00:02.329544
elapsed time: 0:09:07.721637
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 22:13:38.752862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 329.23
 ---- batch: 020 ----
mean loss: 322.95
 ---- batch: 030 ----
mean loss: 340.52
 ---- batch: 040 ----
mean loss: 334.41
 ---- batch: 050 ----
mean loss: 334.21
 ---- batch: 060 ----
mean loss: 329.40
 ---- batch: 070 ----
mean loss: 338.92
 ---- batch: 080 ----
mean loss: 348.19
 ---- batch: 090 ----
mean loss: 343.44
train mean loss: 336.32
epoch train time: 0:00:02.347333
elapsed time: 0:09:10.069144
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 22:13:41.100369
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 332.51
 ---- batch: 020 ----
mean loss: 335.69
 ---- batch: 030 ----
mean loss: 340.13
 ---- batch: 040 ----
mean loss: 330.62
 ---- batch: 050 ----
mean loss: 343.29
 ---- batch: 060 ----
mean loss: 350.24
 ---- batch: 070 ----
mean loss: 331.30
 ---- batch: 080 ----
mean loss: 332.81
 ---- batch: 090 ----
mean loss: 329.09
train mean loss: 335.58
epoch train time: 0:00:02.338792
elapsed time: 0:09:12.408111
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 22:13:43.439340
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 332.12
 ---- batch: 020 ----
mean loss: 333.62
 ---- batch: 030 ----
mean loss: 336.97
 ---- batch: 040 ----
mean loss: 328.55
 ---- batch: 050 ----
mean loss: 329.55
 ---- batch: 060 ----
mean loss: 337.11
 ---- batch: 070 ----
mean loss: 334.97
 ---- batch: 080 ----
mean loss: 345.86
 ---- batch: 090 ----
mean loss: 341.93
train mean loss: 335.74
epoch train time: 0:00:02.339808
elapsed time: 0:09:14.748110
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 22:13:45.779338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 337.62
 ---- batch: 020 ----
mean loss: 335.90
 ---- batch: 030 ----
mean loss: 333.50
 ---- batch: 040 ----
mean loss: 336.02
 ---- batch: 050 ----
mean loss: 333.55
 ---- batch: 060 ----
mean loss: 333.75
 ---- batch: 070 ----
mean loss: 337.17
 ---- batch: 080 ----
mean loss: 322.19
 ---- batch: 090 ----
mean loss: 346.40
train mean loss: 334.59
epoch train time: 0:00:02.335270
elapsed time: 0:09:17.083564
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 22:13:48.114790
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 335.35
 ---- batch: 020 ----
mean loss: 330.09
 ---- batch: 030 ----
mean loss: 338.18
 ---- batch: 040 ----
mean loss: 335.34
 ---- batch: 050 ----
mean loss: 332.02
 ---- batch: 060 ----
mean loss: 332.88
 ---- batch: 070 ----
mean loss: 335.24
 ---- batch: 080 ----
mean loss: 335.79
 ---- batch: 090 ----
mean loss: 332.91
train mean loss: 334.09
epoch train time: 0:00:02.332272
elapsed time: 0:09:19.416015
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 22:13:50.447240
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 334.98
 ---- batch: 020 ----
mean loss: 325.92
 ---- batch: 030 ----
mean loss: 330.99
 ---- batch: 040 ----
mean loss: 338.56
 ---- batch: 050 ----
mean loss: 338.72
 ---- batch: 060 ----
mean loss: 324.46
 ---- batch: 070 ----
mean loss: 328.25
 ---- batch: 080 ----
mean loss: 328.44
 ---- batch: 090 ----
mean loss: 343.58
train mean loss: 333.58
epoch train time: 0:00:02.337646
elapsed time: 0:09:21.753845
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 22:13:52.785091
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 330.31
 ---- batch: 020 ----
mean loss: 336.16
 ---- batch: 030 ----
mean loss: 333.22
 ---- batch: 040 ----
mean loss: 326.97
 ---- batch: 050 ----
mean loss: 338.09
 ---- batch: 060 ----
mean loss: 334.86
 ---- batch: 070 ----
mean loss: 330.09
 ---- batch: 080 ----
mean loss: 344.92
 ---- batch: 090 ----
mean loss: 348.84
train mean loss: 335.27
epoch train time: 0:00:02.338116
elapsed time: 0:09:24.092169
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 22:13:55.123398
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 328.93
 ---- batch: 020 ----
mean loss: 332.37
 ---- batch: 030 ----
mean loss: 336.33
 ---- batch: 040 ----
mean loss: 334.00
 ---- batch: 050 ----
mean loss: 338.05
 ---- batch: 060 ----
mean loss: 325.77
 ---- batch: 070 ----
mean loss: 332.02
 ---- batch: 080 ----
mean loss: 343.23
 ---- batch: 090 ----
mean loss: 338.72
train mean loss: 334.55
epoch train time: 0:00:02.343542
elapsed time: 0:09:26.435891
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 22:13:57.467115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.16
 ---- batch: 020 ----
mean loss: 323.27
 ---- batch: 030 ----
mean loss: 330.66
 ---- batch: 040 ----
mean loss: 338.18
 ---- batch: 050 ----
mean loss: 325.34
 ---- batch: 060 ----
mean loss: 333.78
 ---- batch: 070 ----
mean loss: 341.58
 ---- batch: 080 ----
mean loss: 318.00
 ---- batch: 090 ----
mean loss: 327.38
train mean loss: 330.66
epoch train time: 0:00:02.337326
elapsed time: 0:09:28.773401
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 22:13:59.804630
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 326.84
 ---- batch: 020 ----
mean loss: 340.35
 ---- batch: 030 ----
mean loss: 327.95
 ---- batch: 040 ----
mean loss: 347.53
 ---- batch: 050 ----
mean loss: 347.15
 ---- batch: 060 ----
mean loss: 326.82
 ---- batch: 070 ----
mean loss: 335.94
 ---- batch: 080 ----
mean loss: 324.47
 ---- batch: 090 ----
mean loss: 336.96
train mean loss: 334.65
epoch train time: 0:00:02.345829
elapsed time: 0:09:31.119430
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 22:14:02.150658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 333.54
 ---- batch: 020 ----
mean loss: 342.87
 ---- batch: 030 ----
mean loss: 336.45
 ---- batch: 040 ----
mean loss: 333.84
 ---- batch: 050 ----
mean loss: 324.40
 ---- batch: 060 ----
mean loss: 326.35
 ---- batch: 070 ----
mean loss: 329.73
 ---- batch: 080 ----
mean loss: 340.42
 ---- batch: 090 ----
mean loss: 343.34
train mean loss: 334.44
epoch train time: 0:00:02.349748
elapsed time: 0:09:33.469389
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 22:14:04.500628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 333.64
 ---- batch: 020 ----
mean loss: 328.81
 ---- batch: 030 ----
mean loss: 335.48
 ---- batch: 040 ----
mean loss: 327.10
 ---- batch: 050 ----
mean loss: 340.58
 ---- batch: 060 ----
mean loss: 331.75
 ---- batch: 070 ----
mean loss: 327.77
 ---- batch: 080 ----
mean loss: 336.94
 ---- batch: 090 ----
mean loss: 337.65
train mean loss: 333.19
epoch train time: 0:00:02.343873
elapsed time: 0:09:35.813461
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 22:14:06.844686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 335.97
 ---- batch: 020 ----
mean loss: 337.53
 ---- batch: 030 ----
mean loss: 330.89
 ---- batch: 040 ----
mean loss: 323.09
 ---- batch: 050 ----
mean loss: 333.98
 ---- batch: 060 ----
mean loss: 339.21
 ---- batch: 070 ----
mean loss: 328.27
 ---- batch: 080 ----
mean loss: 335.24
 ---- batch: 090 ----
mean loss: 336.80
train mean loss: 332.06
epoch train time: 0:00:02.340407
elapsed time: 0:09:38.154067
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 22:14:09.185315
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 331.70
 ---- batch: 020 ----
mean loss: 334.23
 ---- batch: 030 ----
mean loss: 342.10
 ---- batch: 040 ----
mean loss: 325.29
 ---- batch: 050 ----
mean loss: 331.18
 ---- batch: 060 ----
mean loss: 339.12
 ---- batch: 070 ----
mean loss: 334.28
 ---- batch: 080 ----
mean loss: 345.29
 ---- batch: 090 ----
mean loss: 328.36
train mean loss: 334.92
epoch train time: 0:00:02.340182
elapsed time: 0:09:40.494510
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 22:14:11.525736
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 325.04
 ---- batch: 020 ----
mean loss: 335.76
 ---- batch: 030 ----
mean loss: 329.00
 ---- batch: 040 ----
mean loss: 335.51
 ---- batch: 050 ----
mean loss: 336.70
 ---- batch: 060 ----
mean loss: 330.36
 ---- batch: 070 ----
mean loss: 329.93
 ---- batch: 080 ----
mean loss: 331.26
 ---- batch: 090 ----
mean loss: 336.16
train mean loss: 332.97
epoch train time: 0:00:02.336955
elapsed time: 0:09:42.831653
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 22:14:13.862878
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.37
 ---- batch: 020 ----
mean loss: 337.10
 ---- batch: 030 ----
mean loss: 333.02
 ---- batch: 040 ----
mean loss: 335.95
 ---- batch: 050 ----
mean loss: 340.11
 ---- batch: 060 ----
mean loss: 340.24
 ---- batch: 070 ----
mean loss: 331.68
 ---- batch: 080 ----
mean loss: 346.18
 ---- batch: 090 ----
mean loss: 329.43
train mean loss: 336.18
epoch train time: 0:00:02.342814
elapsed time: 0:09:45.174679
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 22:14:16.205905
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.51
 ---- batch: 020 ----
mean loss: 332.89
 ---- batch: 030 ----
mean loss: 331.83
 ---- batch: 040 ----
mean loss: 327.25
 ---- batch: 050 ----
mean loss: 327.16
 ---- batch: 060 ----
mean loss: 319.39
 ---- batch: 070 ----
mean loss: 345.32
 ---- batch: 080 ----
mean loss: 330.14
 ---- batch: 090 ----
mean loss: 328.99
train mean loss: 330.82
epoch train time: 0:00:02.332166
elapsed time: 0:09:47.507021
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 22:14:18.538245
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 331.39
 ---- batch: 020 ----
mean loss: 324.03
 ---- batch: 030 ----
mean loss: 338.94
 ---- batch: 040 ----
mean loss: 337.65
 ---- batch: 050 ----
mean loss: 327.77
 ---- batch: 060 ----
mean loss: 323.39
 ---- batch: 070 ----
mean loss: 329.26
 ---- batch: 080 ----
mean loss: 340.26
 ---- batch: 090 ----
mean loss: 337.78
train mean loss: 333.06
epoch train time: 0:00:02.334042
elapsed time: 0:09:49.841284
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 22:14:20.872548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.69
 ---- batch: 020 ----
mean loss: 330.11
 ---- batch: 030 ----
mean loss: 337.26
 ---- batch: 040 ----
mean loss: 322.98
 ---- batch: 050 ----
mean loss: 332.35
 ---- batch: 060 ----
mean loss: 332.19
 ---- batch: 070 ----
mean loss: 332.79
 ---- batch: 080 ----
mean loss: 333.03
 ---- batch: 090 ----
mean loss: 342.00
train mean loss: 333.25
epoch train time: 0:00:02.327323
elapsed time: 0:09:52.168879
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 22:14:23.200117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 337.70
 ---- batch: 020 ----
mean loss: 332.80
 ---- batch: 030 ----
mean loss: 325.11
 ---- batch: 040 ----
mean loss: 332.39
 ---- batch: 050 ----
mean loss: 323.16
 ---- batch: 060 ----
mean loss: 335.11
 ---- batch: 070 ----
mean loss: 322.98
 ---- batch: 080 ----
mean loss: 328.74
 ---- batch: 090 ----
mean loss: 329.32
train mean loss: 330.08
epoch train time: 0:00:02.330789
elapsed time: 0:09:54.499859
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 22:14:25.531086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 336.13
 ---- batch: 020 ----
mean loss: 338.52
 ---- batch: 030 ----
mean loss: 333.78
 ---- batch: 040 ----
mean loss: 323.84
 ---- batch: 050 ----
mean loss: 325.34
 ---- batch: 060 ----
mean loss: 343.30
 ---- batch: 070 ----
mean loss: 350.86
 ---- batch: 080 ----
mean loss: 333.83
 ---- batch: 090 ----
mean loss: 317.38
train mean loss: 332.94
epoch train time: 0:00:02.331612
elapsed time: 0:09:56.831661
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 22:14:27.862903
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 331.93
 ---- batch: 020 ----
mean loss: 327.78
 ---- batch: 030 ----
mean loss: 334.70
 ---- batch: 040 ----
mean loss: 323.87
 ---- batch: 050 ----
mean loss: 323.68
 ---- batch: 060 ----
mean loss: 331.86
 ---- batch: 070 ----
mean loss: 327.52
 ---- batch: 080 ----
mean loss: 329.36
 ---- batch: 090 ----
mean loss: 330.58
train mean loss: 328.80
epoch train time: 0:00:02.335550
elapsed time: 0:09:59.167410
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 22:14:30.198641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 338.04
 ---- batch: 020 ----
mean loss: 325.96
 ---- batch: 030 ----
mean loss: 342.38
 ---- batch: 040 ----
mean loss: 329.16
 ---- batch: 050 ----
mean loss: 332.02
 ---- batch: 060 ----
mean loss: 329.62
 ---- batch: 070 ----
mean loss: 328.52
 ---- batch: 080 ----
mean loss: 322.91
 ---- batch: 090 ----
mean loss: 334.15
train mean loss: 331.15
epoch train time: 0:00:02.336818
elapsed time: 0:10:01.504431
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 22:14:32.535659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 330.10
 ---- batch: 020 ----
mean loss: 332.12
 ---- batch: 030 ----
mean loss: 331.23
 ---- batch: 040 ----
mean loss: 327.97
 ---- batch: 050 ----
mean loss: 322.83
 ---- batch: 060 ----
mean loss: 335.88
 ---- batch: 070 ----
mean loss: 335.38
 ---- batch: 080 ----
mean loss: 329.39
 ---- batch: 090 ----
mean loss: 332.75
train mean loss: 331.09
epoch train time: 0:00:02.340102
elapsed time: 0:10:03.844717
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 22:14:34.875946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 331.80
 ---- batch: 020 ----
mean loss: 333.19
 ---- batch: 030 ----
mean loss: 325.51
 ---- batch: 040 ----
mean loss: 327.48
 ---- batch: 050 ----
mean loss: 341.85
 ---- batch: 060 ----
mean loss: 327.61
 ---- batch: 070 ----
mean loss: 327.46
 ---- batch: 080 ----
mean loss: 326.90
 ---- batch: 090 ----
mean loss: 331.36
train mean loss: 330.95
epoch train time: 0:00:02.332818
elapsed time: 0:10:06.177787
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 22:14:37.209042
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 322.33
 ---- batch: 020 ----
mean loss: 326.77
 ---- batch: 030 ----
mean loss: 327.78
 ---- batch: 040 ----
mean loss: 339.14
 ---- batch: 050 ----
mean loss: 336.95
 ---- batch: 060 ----
mean loss: 331.69
 ---- batch: 070 ----
mean loss: 337.36
 ---- batch: 080 ----
mean loss: 333.94
 ---- batch: 090 ----
mean loss: 341.96
train mean loss: 332.82
epoch train time: 0:00:02.337182
elapsed time: 0:10:08.515184
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 22:14:39.546411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 331.37
 ---- batch: 020 ----
mean loss: 340.10
 ---- batch: 030 ----
mean loss: 327.95
 ---- batch: 040 ----
mean loss: 331.26
 ---- batch: 050 ----
mean loss: 337.15
 ---- batch: 060 ----
mean loss: 323.32
 ---- batch: 070 ----
mean loss: 326.99
 ---- batch: 080 ----
mean loss: 316.97
 ---- batch: 090 ----
mean loss: 336.31
train mean loss: 330.76
epoch train time: 0:00:02.335793
elapsed time: 0:10:10.851159
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 22:14:41.882386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 336.48
 ---- batch: 020 ----
mean loss: 327.16
 ---- batch: 030 ----
mean loss: 337.59
 ---- batch: 040 ----
mean loss: 345.12
 ---- batch: 050 ----
mean loss: 332.65
 ---- batch: 060 ----
mean loss: 328.38
 ---- batch: 070 ----
mean loss: 326.19
 ---- batch: 080 ----
mean loss: 330.17
 ---- batch: 090 ----
mean loss: 337.19
train mean loss: 332.70
epoch train time: 0:00:02.330713
elapsed time: 0:10:13.182055
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 22:14:44.213316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 335.65
 ---- batch: 020 ----
mean loss: 334.04
 ---- batch: 030 ----
mean loss: 336.10
 ---- batch: 040 ----
mean loss: 330.68
 ---- batch: 050 ----
mean loss: 329.03
 ---- batch: 060 ----
mean loss: 326.76
 ---- batch: 070 ----
mean loss: 334.34
 ---- batch: 080 ----
mean loss: 336.67
 ---- batch: 090 ----
mean loss: 324.66
train mean loss: 332.41
epoch train time: 0:00:02.346761
elapsed time: 0:10:15.529050
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 22:14:46.560279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 324.10
 ---- batch: 020 ----
mean loss: 340.77
 ---- batch: 030 ----
mean loss: 323.80
 ---- batch: 040 ----
mean loss: 325.63
 ---- batch: 050 ----
mean loss: 343.92
 ---- batch: 060 ----
mean loss: 335.16
 ---- batch: 070 ----
mean loss: 329.37
 ---- batch: 080 ----
mean loss: 335.54
 ---- batch: 090 ----
mean loss: 336.44
train mean loss: 331.68
epoch train time: 0:00:02.341735
elapsed time: 0:10:17.874498
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_8/checkpoint.pth.tar
**** end time: 2019-09-25 22:14:48.905685 ****
