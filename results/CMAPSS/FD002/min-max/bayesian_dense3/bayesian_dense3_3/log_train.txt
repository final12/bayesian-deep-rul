Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_3', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 20353
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianDense3...
Done.
**** start time: 2019-09-25 19:45:52.241980 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
    BayesianLinear-2                  [-1, 100]          96,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 136,200
Trainable params: 136,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 19:45:52.251563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3846.87
 ---- batch: 020 ----
mean loss: 3565.70
 ---- batch: 030 ----
mean loss: 3420.51
 ---- batch: 040 ----
mean loss: 3184.80
 ---- batch: 050 ----
mean loss: 2952.35
 ---- batch: 060 ----
mean loss: 2853.40
 ---- batch: 070 ----
mean loss: 2653.10
 ---- batch: 080 ----
mean loss: 2567.30
 ---- batch: 090 ----
mean loss: 2436.46
train mean loss: 3009.57
epoch train time: 0:00:35.161588
elapsed time: 0:00:35.178243
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 19:46:27.420276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2237.57
 ---- batch: 020 ----
mean loss: 2206.95
 ---- batch: 030 ----
mean loss: 2123.88
 ---- batch: 040 ----
mean loss: 2056.77
 ---- batch: 050 ----
mean loss: 1953.28
 ---- batch: 060 ----
mean loss: 1927.72
 ---- batch: 070 ----
mean loss: 1878.99
 ---- batch: 080 ----
mean loss: 1826.08
 ---- batch: 090 ----
mean loss: 1803.75
train mean loss: 1986.69
epoch train time: 0:00:01.694786
elapsed time: 0:00:36.873481
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 19:46:29.115706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1712.85
 ---- batch: 020 ----
mean loss: 1672.48
 ---- batch: 030 ----
mean loss: 1670.04
 ---- batch: 040 ----
mean loss: 1648.76
 ---- batch: 050 ----
mean loss: 1623.94
 ---- batch: 060 ----
mean loss: 1586.20
 ---- batch: 070 ----
mean loss: 1581.93
 ---- batch: 080 ----
mean loss: 1539.86
 ---- batch: 090 ----
mean loss: 1505.45
train mean loss: 1610.16
epoch train time: 0:00:01.654406
elapsed time: 0:00:38.528530
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 19:46:30.770811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1497.32
 ---- batch: 020 ----
mean loss: 1444.52
 ---- batch: 030 ----
mean loss: 1427.68
 ---- batch: 040 ----
mean loss: 1438.24
 ---- batch: 050 ----
mean loss: 1404.04
 ---- batch: 060 ----
mean loss: 1403.47
 ---- batch: 070 ----
mean loss: 1358.94
 ---- batch: 080 ----
mean loss: 1350.48
 ---- batch: 090 ----
mean loss: 1346.41
train mean loss: 1402.44
epoch train time: 0:00:01.686284
elapsed time: 0:00:40.215431
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 19:46:32.457754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1313.16
 ---- batch: 020 ----
mean loss: 1292.50
 ---- batch: 030 ----
mean loss: 1280.97
 ---- batch: 040 ----
mean loss: 1267.34
 ---- batch: 050 ----
mean loss: 1271.41
 ---- batch: 060 ----
mean loss: 1231.30
 ---- batch: 070 ----
mean loss: 1239.65
 ---- batch: 080 ----
mean loss: 1238.44
 ---- batch: 090 ----
mean loss: 1201.40
train mean loss: 1253.96
epoch train time: 0:00:01.689344
elapsed time: 0:00:41.905445
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 19:46:34.147752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1175.25
 ---- batch: 020 ----
mean loss: 1160.21
 ---- batch: 030 ----
mean loss: 1169.18
 ---- batch: 040 ----
mean loss: 1154.87
 ---- batch: 050 ----
mean loss: 1166.03
 ---- batch: 060 ----
mean loss: 1108.32
 ---- batch: 070 ----
mean loss: 1141.18
 ---- batch: 080 ----
mean loss: 1137.04
 ---- batch: 090 ----
mean loss: 1100.28
train mean loss: 1142.38
epoch train time: 0:00:01.675210
elapsed time: 0:00:43.581355
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 19:46:35.823669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1088.07
 ---- batch: 020 ----
mean loss: 1096.92
 ---- batch: 030 ----
mean loss: 1062.18
 ---- batch: 040 ----
mean loss: 1083.60
 ---- batch: 050 ----
mean loss: 1067.13
 ---- batch: 060 ----
mean loss: 1058.00
 ---- batch: 070 ----
mean loss: 1042.65
 ---- batch: 080 ----
mean loss: 1039.35
 ---- batch: 090 ----
mean loss: 1033.39
train mean loss: 1060.31
epoch train time: 0:00:01.662952
elapsed time: 0:00:45.244941
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 19:46:37.487216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1013.62
 ---- batch: 020 ----
mean loss: 1020.09
 ---- batch: 030 ----
mean loss: 1043.81
 ---- batch: 040 ----
mean loss: 995.29
 ---- batch: 050 ----
mean loss: 1004.51
 ---- batch: 060 ----
mean loss: 997.97
 ---- batch: 070 ----
mean loss: 993.29
 ---- batch: 080 ----
mean loss: 992.33
 ---- batch: 090 ----
mean loss: 966.89
train mean loss: 999.98
epoch train time: 0:00:01.658778
elapsed time: 0:00:46.904425
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 19:46:39.146813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 983.44
 ---- batch: 020 ----
mean loss: 964.69
 ---- batch: 030 ----
mean loss: 962.38
 ---- batch: 040 ----
mean loss: 975.99
 ---- batch: 050 ----
mean loss: 960.78
 ---- batch: 060 ----
mean loss: 948.38
 ---- batch: 070 ----
mean loss: 954.86
 ---- batch: 080 ----
mean loss: 934.32
 ---- batch: 090 ----
mean loss: 959.03
train mean loss: 960.19
epoch train time: 0:00:01.710213
elapsed time: 0:00:48.615489
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 19:46:40.857816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.75
 ---- batch: 020 ----
mean loss: 940.22
 ---- batch: 030 ----
mean loss: 927.55
 ---- batch: 040 ----
mean loss: 925.16
 ---- batch: 050 ----
mean loss: 948.80
 ---- batch: 060 ----
mean loss: 938.19
 ---- batch: 070 ----
mean loss: 940.13
 ---- batch: 080 ----
mean loss: 909.78
 ---- batch: 090 ----
mean loss: 921.71
train mean loss: 932.05
epoch train time: 0:00:01.656643
elapsed time: 0:00:50.272888
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 19:46:42.515193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.27
 ---- batch: 020 ----
mean loss: 930.08
 ---- batch: 030 ----
mean loss: 917.72
 ---- batch: 040 ----
mean loss: 911.60
 ---- batch: 050 ----
mean loss: 904.08
 ---- batch: 060 ----
mean loss: 918.71
 ---- batch: 070 ----
mean loss: 919.06
 ---- batch: 080 ----
mean loss: 892.69
 ---- batch: 090 ----
mean loss: 918.54
train mean loss: 912.13
epoch train time: 0:00:01.641304
elapsed time: 0:00:51.914917
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 19:46:44.157211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.31
 ---- batch: 020 ----
mean loss: 894.15
 ---- batch: 030 ----
mean loss: 906.24
 ---- batch: 040 ----
mean loss: 919.26
 ---- batch: 050 ----
mean loss: 918.19
 ---- batch: 060 ----
mean loss: 908.64
 ---- batch: 070 ----
mean loss: 915.79
 ---- batch: 080 ----
mean loss: 892.59
 ---- batch: 090 ----
mean loss: 887.64
train mean loss: 900.27
epoch train time: 0:00:01.681539
elapsed time: 0:00:53.597153
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 19:46:45.839315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.84
 ---- batch: 020 ----
mean loss: 894.04
 ---- batch: 030 ----
mean loss: 897.52
 ---- batch: 040 ----
mean loss: 882.57
 ---- batch: 050 ----
mean loss: 901.26
 ---- batch: 060 ----
mean loss: 900.83
 ---- batch: 070 ----
mean loss: 878.10
 ---- batch: 080 ----
mean loss: 888.71
 ---- batch: 090 ----
mean loss: 908.74
train mean loss: 893.27
epoch train time: 0:00:01.645899
elapsed time: 0:00:55.243666
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 19:46:47.485968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.41
 ---- batch: 020 ----
mean loss: 889.13
 ---- batch: 030 ----
mean loss: 891.49
 ---- batch: 040 ----
mean loss: 870.93
 ---- batch: 050 ----
mean loss: 893.12
 ---- batch: 060 ----
mean loss: 883.62
 ---- batch: 070 ----
mean loss: 908.28
 ---- batch: 080 ----
mean loss: 892.48
 ---- batch: 090 ----
mean loss: 886.09
train mean loss: 889.31
epoch train time: 0:00:01.664467
elapsed time: 0:00:56.908883
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 19:46:49.151210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.57
 ---- batch: 020 ----
mean loss: 888.29
 ---- batch: 030 ----
mean loss: 885.74
 ---- batch: 040 ----
mean loss: 878.73
 ---- batch: 050 ----
mean loss: 876.99
 ---- batch: 060 ----
mean loss: 870.59
 ---- batch: 070 ----
mean loss: 876.93
 ---- batch: 080 ----
mean loss: 900.44
 ---- batch: 090 ----
mean loss: 892.26
train mean loss: 885.86
epoch train time: 0:00:01.680514
elapsed time: 0:00:58.590075
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 19:46:50.832348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.86
 ---- batch: 020 ----
mean loss: 891.22
 ---- batch: 030 ----
mean loss: 882.62
 ---- batch: 040 ----
mean loss: 897.40
 ---- batch: 050 ----
mean loss: 888.92
 ---- batch: 060 ----
mean loss: 876.96
 ---- batch: 070 ----
mean loss: 860.20
 ---- batch: 080 ----
mean loss: 883.70
 ---- batch: 090 ----
mean loss: 889.74
train mean loss: 883.98
epoch train time: 0:00:01.696095
elapsed time: 0:01:00.286816
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 19:46:52.529102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.67
 ---- batch: 020 ----
mean loss: 860.71
 ---- batch: 030 ----
mean loss: 883.24
 ---- batch: 040 ----
mean loss: 890.57
 ---- batch: 050 ----
mean loss: 864.44
 ---- batch: 060 ----
mean loss: 893.78
 ---- batch: 070 ----
mean loss: 900.07
 ---- batch: 080 ----
mean loss: 901.25
 ---- batch: 090 ----
mean loss: 867.16
train mean loss: 884.61
epoch train time: 0:00:01.658491
elapsed time: 0:01:01.945960
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 19:46:54.188242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.76
 ---- batch: 020 ----
mean loss: 878.48
 ---- batch: 030 ----
mean loss: 898.62
 ---- batch: 040 ----
mean loss: 890.44
 ---- batch: 050 ----
mean loss: 872.29
 ---- batch: 060 ----
mean loss: 864.55
 ---- batch: 070 ----
mean loss: 883.59
 ---- batch: 080 ----
mean loss: 879.23
 ---- batch: 090 ----
mean loss: 879.10
train mean loss: 882.89
epoch train time: 0:00:01.683003
elapsed time: 0:01:03.629651
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 19:46:55.871975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.32
 ---- batch: 020 ----
mean loss: 891.17
 ---- batch: 030 ----
mean loss: 868.93
 ---- batch: 040 ----
mean loss: 885.10
 ---- batch: 050 ----
mean loss: 887.12
 ---- batch: 060 ----
mean loss: 877.73
 ---- batch: 070 ----
mean loss: 863.97
 ---- batch: 080 ----
mean loss: 897.19
 ---- batch: 090 ----
mean loss: 888.23
train mean loss: 881.95
epoch train time: 0:00:01.681746
elapsed time: 0:01:05.312044
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 19:46:57.554386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.33
 ---- batch: 020 ----
mean loss: 893.39
 ---- batch: 030 ----
mean loss: 883.21
 ---- batch: 040 ----
mean loss: 891.92
 ---- batch: 050 ----
mean loss: 868.12
 ---- batch: 060 ----
mean loss: 885.39
 ---- batch: 070 ----
mean loss: 889.99
 ---- batch: 080 ----
mean loss: 867.22
 ---- batch: 090 ----
mean loss: 877.90
train mean loss: 881.80
epoch train time: 0:00:01.676500
elapsed time: 0:01:06.989271
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 19:46:59.231566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.96
 ---- batch: 020 ----
mean loss: 874.82
 ---- batch: 030 ----
mean loss: 871.79
 ---- batch: 040 ----
mean loss: 881.90
 ---- batch: 050 ----
mean loss: 900.69
 ---- batch: 060 ----
mean loss: 882.09
 ---- batch: 070 ----
mean loss: 863.77
 ---- batch: 080 ----
mean loss: 893.04
 ---- batch: 090 ----
mean loss: 885.72
train mean loss: 880.76
epoch train time: 0:00:01.660657
elapsed time: 0:01:08.650626
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 19:47:00.892912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.74
 ---- batch: 020 ----
mean loss: 890.90
 ---- batch: 030 ----
mean loss: 872.52
 ---- batch: 040 ----
mean loss: 863.82
 ---- batch: 050 ----
mean loss: 872.50
 ---- batch: 060 ----
mean loss: 897.18
 ---- batch: 070 ----
mean loss: 888.87
 ---- batch: 080 ----
mean loss: 877.36
 ---- batch: 090 ----
mean loss: 878.14
train mean loss: 881.54
epoch train time: 0:00:01.701263
elapsed time: 0:01:10.352560
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 19:47:02.594668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.69
 ---- batch: 020 ----
mean loss: 871.55
 ---- batch: 030 ----
mean loss: 862.32
 ---- batch: 040 ----
mean loss: 871.34
 ---- batch: 050 ----
mean loss: 886.40
 ---- batch: 060 ----
mean loss: 881.66
 ---- batch: 070 ----
mean loss: 883.26
 ---- batch: 080 ----
mean loss: 881.78
 ---- batch: 090 ----
mean loss: 891.27
train mean loss: 879.25
epoch train time: 0:00:01.709270
elapsed time: 0:01:12.062334
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 19:47:04.304615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.61
 ---- batch: 020 ----
mean loss: 878.41
 ---- batch: 030 ----
mean loss: 883.17
 ---- batch: 040 ----
mean loss: 864.95
 ---- batch: 050 ----
mean loss: 879.32
 ---- batch: 060 ----
mean loss: 869.13
 ---- batch: 070 ----
mean loss: 887.98
 ---- batch: 080 ----
mean loss: 885.04
 ---- batch: 090 ----
mean loss: 877.26
train mean loss: 881.61
epoch train time: 0:00:01.661725
elapsed time: 0:01:13.724778
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 19:47:05.967068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.34
 ---- batch: 020 ----
mean loss: 894.79
 ---- batch: 030 ----
mean loss: 888.19
 ---- batch: 040 ----
mean loss: 881.00
 ---- batch: 050 ----
mean loss: 884.63
 ---- batch: 060 ----
mean loss: 878.81
 ---- batch: 070 ----
mean loss: 879.14
 ---- batch: 080 ----
mean loss: 869.70
 ---- batch: 090 ----
mean loss: 887.64
train mean loss: 879.43
epoch train time: 0:00:01.703840
elapsed time: 0:01:15.429269
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 19:47:07.671565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.12
 ---- batch: 020 ----
mean loss: 871.04
 ---- batch: 030 ----
mean loss: 868.26
 ---- batch: 040 ----
mean loss: 873.32
 ---- batch: 050 ----
mean loss: 867.18
 ---- batch: 060 ----
mean loss: 906.13
 ---- batch: 070 ----
mean loss: 889.46
 ---- batch: 080 ----
mean loss: 882.95
 ---- batch: 090 ----
mean loss: 873.90
train mean loss: 878.94
epoch train time: 0:00:01.696719
elapsed time: 0:01:17.126722
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 19:47:09.369038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.45
 ---- batch: 020 ----
mean loss: 877.75
 ---- batch: 030 ----
mean loss: 876.12
 ---- batch: 040 ----
mean loss: 876.38
 ---- batch: 050 ----
mean loss: 870.30
 ---- batch: 060 ----
mean loss: 876.48
 ---- batch: 070 ----
mean loss: 881.80
 ---- batch: 080 ----
mean loss: 890.99
 ---- batch: 090 ----
mean loss: 893.32
train mean loss: 879.45
epoch train time: 0:00:01.685537
elapsed time: 0:01:18.812938
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 19:47:11.055289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.18
 ---- batch: 020 ----
mean loss: 874.15
 ---- batch: 030 ----
mean loss: 882.18
 ---- batch: 040 ----
mean loss: 888.97
 ---- batch: 050 ----
mean loss: 875.94
 ---- batch: 060 ----
mean loss: 875.57
 ---- batch: 070 ----
mean loss: 866.77
 ---- batch: 080 ----
mean loss: 895.82
 ---- batch: 090 ----
mean loss: 867.66
train mean loss: 879.05
epoch train time: 0:00:01.670620
elapsed time: 0:01:20.484414
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 19:47:12.726723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.50
 ---- batch: 020 ----
mean loss: 875.72
 ---- batch: 030 ----
mean loss: 867.47
 ---- batch: 040 ----
mean loss: 878.56
 ---- batch: 050 ----
mean loss: 888.70
 ---- batch: 060 ----
mean loss: 886.77
 ---- batch: 070 ----
mean loss: 893.75
 ---- batch: 080 ----
mean loss: 867.48
 ---- batch: 090 ----
mean loss: 863.12
train mean loss: 879.59
epoch train time: 0:00:01.679588
elapsed time: 0:01:22.164684
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 19:47:14.406994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.07
 ---- batch: 020 ----
mean loss: 892.73
 ---- batch: 030 ----
mean loss: 876.00
 ---- batch: 040 ----
mean loss: 877.94
 ---- batch: 050 ----
mean loss: 877.27
 ---- batch: 060 ----
mean loss: 880.48
 ---- batch: 070 ----
mean loss: 874.04
 ---- batch: 080 ----
mean loss: 874.16
 ---- batch: 090 ----
mean loss: 862.27
train mean loss: 878.41
epoch train time: 0:00:01.672263
elapsed time: 0:01:23.837634
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 19:47:16.079928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.63
 ---- batch: 020 ----
mean loss: 874.87
 ---- batch: 030 ----
mean loss: 869.06
 ---- batch: 040 ----
mean loss: 881.44
 ---- batch: 050 ----
mean loss: 894.82
 ---- batch: 060 ----
mean loss: 872.11
 ---- batch: 070 ----
mean loss: 894.73
 ---- batch: 080 ----
mean loss: 873.91
 ---- batch: 090 ----
mean loss: 864.18
train mean loss: 878.20
epoch train time: 0:00:01.651137
elapsed time: 0:01:25.489430
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 19:47:17.731712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.49
 ---- batch: 020 ----
mean loss: 877.86
 ---- batch: 030 ----
mean loss: 876.13
 ---- batch: 040 ----
mean loss: 870.27
 ---- batch: 050 ----
mean loss: 879.33
 ---- batch: 060 ----
mean loss: 887.50
 ---- batch: 070 ----
mean loss: 863.08
 ---- batch: 080 ----
mean loss: 886.26
 ---- batch: 090 ----
mean loss: 874.68
train mean loss: 876.92
epoch train time: 0:00:01.701602
elapsed time: 0:01:27.191711
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 19:47:19.434059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.74
 ---- batch: 020 ----
mean loss: 881.39
 ---- batch: 030 ----
mean loss: 879.34
 ---- batch: 040 ----
mean loss: 874.79
 ---- batch: 050 ----
mean loss: 873.71
 ---- batch: 060 ----
mean loss: 872.53
 ---- batch: 070 ----
mean loss: 872.18
 ---- batch: 080 ----
mean loss: 880.55
 ---- batch: 090 ----
mean loss: 881.00
train mean loss: 879.59
epoch train time: 0:00:01.660447
elapsed time: 0:01:28.852822
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 19:47:21.095117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.10
 ---- batch: 020 ----
mean loss: 886.17
 ---- batch: 030 ----
mean loss: 868.54
 ---- batch: 040 ----
mean loss: 878.78
 ---- batch: 050 ----
mean loss: 893.80
 ---- batch: 060 ----
mean loss: 883.84
 ---- batch: 070 ----
mean loss: 889.67
 ---- batch: 080 ----
mean loss: 859.61
 ---- batch: 090 ----
mean loss: 877.69
train mean loss: 878.25
epoch train time: 0:00:01.698291
elapsed time: 0:01:30.551929
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 19:47:22.794249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.43
 ---- batch: 020 ----
mean loss: 874.83
 ---- batch: 030 ----
mean loss: 863.56
 ---- batch: 040 ----
mean loss: 882.55
 ---- batch: 050 ----
mean loss: 872.77
 ---- batch: 060 ----
mean loss: 888.46
 ---- batch: 070 ----
mean loss: 883.18
 ---- batch: 080 ----
mean loss: 878.24
 ---- batch: 090 ----
mean loss: 892.82
train mean loss: 879.03
epoch train time: 0:00:01.715492
elapsed time: 0:01:32.268218
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 19:47:24.510558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.08
 ---- batch: 020 ----
mean loss: 875.51
 ---- batch: 030 ----
mean loss: 878.65
 ---- batch: 040 ----
mean loss: 890.71
 ---- batch: 050 ----
mean loss: 894.04
 ---- batch: 060 ----
mean loss: 860.09
 ---- batch: 070 ----
mean loss: 864.73
 ---- batch: 080 ----
mean loss: 868.85
 ---- batch: 090 ----
mean loss: 912.50
train mean loss: 879.17
epoch train time: 0:00:01.732051
elapsed time: 0:01:34.001001
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 19:47:26.243301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.16
 ---- batch: 020 ----
mean loss: 870.29
 ---- batch: 030 ----
mean loss: 883.86
 ---- batch: 040 ----
mean loss: 899.27
 ---- batch: 050 ----
mean loss: 898.12
 ---- batch: 060 ----
mean loss: 874.54
 ---- batch: 070 ----
mean loss: 871.86
 ---- batch: 080 ----
mean loss: 854.05
 ---- batch: 090 ----
mean loss: 869.01
train mean loss: 876.70
epoch train time: 0:00:01.691900
elapsed time: 0:01:35.693606
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 19:47:27.935905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.35
 ---- batch: 020 ----
mean loss: 886.42
 ---- batch: 030 ----
mean loss: 882.89
 ---- batch: 040 ----
mean loss: 882.30
 ---- batch: 050 ----
mean loss: 868.01
 ---- batch: 060 ----
mean loss: 882.73
 ---- batch: 070 ----
mean loss: 871.91
 ---- batch: 080 ----
mean loss: 884.61
 ---- batch: 090 ----
mean loss: 860.64
train mean loss: 877.29
epoch train time: 0:00:01.666494
elapsed time: 0:01:37.360784
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 19:47:29.603077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.90
 ---- batch: 020 ----
mean loss: 877.16
 ---- batch: 030 ----
mean loss: 871.51
 ---- batch: 040 ----
mean loss: 878.03
 ---- batch: 050 ----
mean loss: 862.82
 ---- batch: 060 ----
mean loss: 889.72
 ---- batch: 070 ----
mean loss: 885.44
 ---- batch: 080 ----
mean loss: 871.43
 ---- batch: 090 ----
mean loss: 885.56
train mean loss: 878.86
epoch train time: 0:00:01.650177
elapsed time: 0:01:39.011687
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 19:47:31.253983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.04
 ---- batch: 020 ----
mean loss: 866.00
 ---- batch: 030 ----
mean loss: 870.11
 ---- batch: 040 ----
mean loss: 872.71
 ---- batch: 050 ----
mean loss: 876.41
 ---- batch: 060 ----
mean loss: 877.10
 ---- batch: 070 ----
mean loss: 878.66
 ---- batch: 080 ----
mean loss: 873.57
 ---- batch: 090 ----
mean loss: 883.87
train mean loss: 877.35
epoch train time: 0:00:01.686429
elapsed time: 0:01:40.698827
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 19:47:32.941172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.26
 ---- batch: 020 ----
mean loss: 889.18
 ---- batch: 030 ----
mean loss: 878.20
 ---- batch: 040 ----
mean loss: 864.97
 ---- batch: 050 ----
mean loss: 888.81
 ---- batch: 060 ----
mean loss: 878.62
 ---- batch: 070 ----
mean loss: 875.64
 ---- batch: 080 ----
mean loss: 865.09
 ---- batch: 090 ----
mean loss: 869.88
train mean loss: 876.60
epoch train time: 0:00:01.688469
elapsed time: 0:01:42.388003
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 19:47:34.630344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.94
 ---- batch: 020 ----
mean loss: 882.40
 ---- batch: 030 ----
mean loss: 881.15
 ---- batch: 040 ----
mean loss: 869.06
 ---- batch: 050 ----
mean loss: 862.93
 ---- batch: 060 ----
mean loss: 885.58
 ---- batch: 070 ----
mean loss: 877.75
 ---- batch: 080 ----
mean loss: 876.44
 ---- batch: 090 ----
mean loss: 885.58
train mean loss: 877.13
epoch train time: 0:00:01.671139
elapsed time: 0:01:44.059906
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 19:47:36.302218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.33
 ---- batch: 020 ----
mean loss: 864.45
 ---- batch: 030 ----
mean loss: 880.82
 ---- batch: 040 ----
mean loss: 852.04
 ---- batch: 050 ----
mean loss: 852.84
 ---- batch: 060 ----
mean loss: 875.72
 ---- batch: 070 ----
mean loss: 891.44
 ---- batch: 080 ----
mean loss: 896.12
 ---- batch: 090 ----
mean loss: 901.78
train mean loss: 877.43
epoch train time: 0:00:01.648846
elapsed time: 0:01:45.709436
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 19:47:37.951704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.84
 ---- batch: 020 ----
mean loss: 861.33
 ---- batch: 030 ----
mean loss: 881.52
 ---- batch: 040 ----
mean loss: 890.11
 ---- batch: 050 ----
mean loss: 870.77
 ---- batch: 060 ----
mean loss: 880.32
 ---- batch: 070 ----
mean loss: 875.55
 ---- batch: 080 ----
mean loss: 889.55
 ---- batch: 090 ----
mean loss: 887.00
train mean loss: 877.15
epoch train time: 0:00:01.666513
elapsed time: 0:01:47.376608
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 19:47:39.618761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.22
 ---- batch: 020 ----
mean loss: 898.10
 ---- batch: 030 ----
mean loss: 896.80
 ---- batch: 040 ----
mean loss: 877.05
 ---- batch: 050 ----
mean loss: 854.70
 ---- batch: 060 ----
mean loss: 885.02
 ---- batch: 070 ----
mean loss: 874.02
 ---- batch: 080 ----
mean loss: 876.62
 ---- batch: 090 ----
mean loss: 871.05
train mean loss: 876.81
epoch train time: 0:00:01.677564
elapsed time: 0:01:49.054710
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 19:47:41.297000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.45
 ---- batch: 020 ----
mean loss: 871.34
 ---- batch: 030 ----
mean loss: 874.88
 ---- batch: 040 ----
mean loss: 876.92
 ---- batch: 050 ----
mean loss: 879.87
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.71
 ---- batch: 080 ----
mean loss: 876.59
 ---- batch: 090 ----
mean loss: 864.22
train mean loss: 874.98
epoch train time: 0:00:01.680364
elapsed time: 0:01:50.735754
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 19:47:42.978057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.43
 ---- batch: 020 ----
mean loss: 868.43
 ---- batch: 030 ----
mean loss: 876.86
 ---- batch: 040 ----
mean loss: 860.28
 ---- batch: 050 ----
mean loss: 864.95
 ---- batch: 060 ----
mean loss: 843.90
 ---- batch: 070 ----
mean loss: 833.84
 ---- batch: 080 ----
mean loss: 773.02
 ---- batch: 090 ----
mean loss: 726.88
train mean loss: 824.39
epoch train time: 0:00:01.669700
elapsed time: 0:01:52.406227
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 19:47:44.648519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 636.77
 ---- batch: 020 ----
mean loss: 587.48
 ---- batch: 030 ----
mean loss: 546.19
 ---- batch: 040 ----
mean loss: 511.99
 ---- batch: 050 ----
mean loss: 519.15
 ---- batch: 060 ----
mean loss: 469.32
 ---- batch: 070 ----
mean loss: 474.16
 ---- batch: 080 ----
mean loss: 449.02
 ---- batch: 090 ----
mean loss: 446.23
train mean loss: 511.92
epoch train time: 0:00:01.677205
elapsed time: 0:01:54.084131
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 19:47:46.326418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.05
 ---- batch: 020 ----
mean loss: 421.91
 ---- batch: 030 ----
mean loss: 425.69
 ---- batch: 040 ----
mean loss: 403.74
 ---- batch: 050 ----
mean loss: 408.48
 ---- batch: 060 ----
mean loss: 402.75
 ---- batch: 070 ----
mean loss: 408.02
 ---- batch: 080 ----
mean loss: 402.97
 ---- batch: 090 ----
mean loss: 388.43
train mean loss: 410.31
epoch train time: 0:00:01.697310
elapsed time: 0:01:55.782255
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 19:47:48.024577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.64
 ---- batch: 020 ----
mean loss: 367.72
 ---- batch: 030 ----
mean loss: 378.06
 ---- batch: 040 ----
mean loss: 362.81
 ---- batch: 050 ----
mean loss: 363.76
 ---- batch: 060 ----
mean loss: 382.67
 ---- batch: 070 ----
mean loss: 358.83
 ---- batch: 080 ----
mean loss: 366.42
 ---- batch: 090 ----
mean loss: 349.58
train mean loss: 367.27
epoch train time: 0:00:01.663442
elapsed time: 0:01:57.446504
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 19:47:49.688795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.99
 ---- batch: 020 ----
mean loss: 348.68
 ---- batch: 030 ----
mean loss: 358.07
 ---- batch: 040 ----
mean loss: 348.79
 ---- batch: 050 ----
mean loss: 339.85
 ---- batch: 060 ----
mean loss: 342.46
 ---- batch: 070 ----
mean loss: 347.17
 ---- batch: 080 ----
mean loss: 329.36
 ---- batch: 090 ----
mean loss: 338.94
train mean loss: 345.63
epoch train time: 0:00:01.674157
elapsed time: 0:01:59.121473
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 19:47:51.363819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.63
 ---- batch: 020 ----
mean loss: 331.10
 ---- batch: 030 ----
mean loss: 330.53
 ---- batch: 040 ----
mean loss: 347.31
 ---- batch: 050 ----
mean loss: 331.84
 ---- batch: 060 ----
mean loss: 322.02
 ---- batch: 070 ----
mean loss: 327.47
 ---- batch: 080 ----
mean loss: 329.42
 ---- batch: 090 ----
mean loss: 334.23
train mean loss: 330.99
epoch train time: 0:00:01.694245
elapsed time: 0:02:00.816551
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 19:47:53.058837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.82
 ---- batch: 020 ----
mean loss: 317.71
 ---- batch: 030 ----
mean loss: 309.84
 ---- batch: 040 ----
mean loss: 316.90
 ---- batch: 050 ----
mean loss: 320.30
 ---- batch: 060 ----
mean loss: 320.36
 ---- batch: 070 ----
mean loss: 312.58
 ---- batch: 080 ----
mean loss: 316.09
 ---- batch: 090 ----
mean loss: 319.04
train mean loss: 317.34
epoch train time: 0:00:01.688795
elapsed time: 0:02:02.505945
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 19:47:54.748238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.36
 ---- batch: 020 ----
mean loss: 308.41
 ---- batch: 030 ----
mean loss: 326.03
 ---- batch: 040 ----
mean loss: 308.84
 ---- batch: 050 ----
mean loss: 311.38
 ---- batch: 060 ----
mean loss: 301.97
 ---- batch: 070 ----
mean loss: 301.42
 ---- batch: 080 ----
mean loss: 311.32
 ---- batch: 090 ----
mean loss: 294.99
train mean loss: 306.88
epoch train time: 0:00:01.677561
elapsed time: 0:02:04.184115
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 19:47:56.426373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.09
 ---- batch: 020 ----
mean loss: 300.51
 ---- batch: 030 ----
mean loss: 305.99
 ---- batch: 040 ----
mean loss: 296.24
 ---- batch: 050 ----
mean loss: 298.83
 ---- batch: 060 ----
mean loss: 304.86
 ---- batch: 070 ----
mean loss: 305.99
 ---- batch: 080 ----
mean loss: 292.53
 ---- batch: 090 ----
mean loss: 289.40
train mean loss: 299.84
epoch train time: 0:00:01.646403
elapsed time: 0:02:05.831144
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 19:47:58.073411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.91
 ---- batch: 020 ----
mean loss: 292.26
 ---- batch: 030 ----
mean loss: 292.27
 ---- batch: 040 ----
mean loss: 290.29
 ---- batch: 050 ----
mean loss: 285.17
 ---- batch: 060 ----
mean loss: 294.58
 ---- batch: 070 ----
mean loss: 300.71
 ---- batch: 080 ----
mean loss: 280.76
 ---- batch: 090 ----
mean loss: 281.97
train mean loss: 291.32
epoch train time: 0:00:01.635565
elapsed time: 0:02:07.467362
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 19:47:59.709686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.42
 ---- batch: 020 ----
mean loss: 284.25
 ---- batch: 030 ----
mean loss: 293.16
 ---- batch: 040 ----
mean loss: 289.01
 ---- batch: 050 ----
mean loss: 284.12
 ---- batch: 060 ----
mean loss: 277.63
 ---- batch: 070 ----
mean loss: 282.19
 ---- batch: 080 ----
mean loss: 281.53
 ---- batch: 090 ----
mean loss: 285.08
train mean loss: 284.29
epoch train time: 0:00:01.656474
elapsed time: 0:02:09.124475
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 19:48:01.366752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.39
 ---- batch: 020 ----
mean loss: 277.48
 ---- batch: 030 ----
mean loss: 281.51
 ---- batch: 040 ----
mean loss: 276.29
 ---- batch: 050 ----
mean loss: 288.26
 ---- batch: 060 ----
mean loss: 288.37
 ---- batch: 070 ----
mean loss: 274.83
 ---- batch: 080 ----
mean loss: 279.86
 ---- batch: 090 ----
mean loss: 270.18
train mean loss: 279.22
epoch train time: 0:00:01.659031
elapsed time: 0:02:10.784165
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 19:48:03.026476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.39
 ---- batch: 020 ----
mean loss: 261.40
 ---- batch: 030 ----
mean loss: 283.43
 ---- batch: 040 ----
mean loss: 280.22
 ---- batch: 050 ----
mean loss: 278.72
 ---- batch: 060 ----
mean loss: 265.00
 ---- batch: 070 ----
mean loss: 270.28
 ---- batch: 080 ----
mean loss: 282.27
 ---- batch: 090 ----
mean loss: 279.71
train mean loss: 275.76
epoch train time: 0:00:01.642531
elapsed time: 0:02:12.427372
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 19:48:04.669734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.14
 ---- batch: 020 ----
mean loss: 280.98
 ---- batch: 030 ----
mean loss: 261.17
 ---- batch: 040 ----
mean loss: 271.39
 ---- batch: 050 ----
mean loss: 273.65
 ---- batch: 060 ----
mean loss: 278.19
 ---- batch: 070 ----
mean loss: 271.89
 ---- batch: 080 ----
mean loss: 266.58
 ---- batch: 090 ----
mean loss: 269.01
train mean loss: 271.54
epoch train time: 0:00:01.672980
elapsed time: 0:02:14.101137
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 19:48:06.343413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.34
 ---- batch: 020 ----
mean loss: 261.78
 ---- batch: 030 ----
mean loss: 263.83
 ---- batch: 040 ----
mean loss: 263.46
 ---- batch: 050 ----
mean loss: 261.77
 ---- batch: 060 ----
mean loss: 259.64
 ---- batch: 070 ----
mean loss: 275.43
 ---- batch: 080 ----
mean loss: 269.95
 ---- batch: 090 ----
mean loss: 274.87
train mean loss: 266.77
epoch train time: 0:00:01.679353
elapsed time: 0:02:15.781147
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 19:48:08.023449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.46
 ---- batch: 020 ----
mean loss: 259.32
 ---- batch: 030 ----
mean loss: 258.27
 ---- batch: 040 ----
mean loss: 259.65
 ---- batch: 050 ----
mean loss: 262.73
 ---- batch: 060 ----
mean loss: 271.86
 ---- batch: 070 ----
mean loss: 256.72
 ---- batch: 080 ----
mean loss: 262.86
 ---- batch: 090 ----
mean loss: 273.66
train mean loss: 263.09
epoch train time: 0:00:01.690430
elapsed time: 0:02:17.472254
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 19:48:09.714544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.47
 ---- batch: 020 ----
mean loss: 253.24
 ---- batch: 030 ----
mean loss: 262.42
 ---- batch: 040 ----
mean loss: 251.65
 ---- batch: 050 ----
mean loss: 262.48
 ---- batch: 060 ----
mean loss: 263.15
 ---- batch: 070 ----
mean loss: 262.08
 ---- batch: 080 ----
mean loss: 263.21
 ---- batch: 090 ----
mean loss: 257.33
train mean loss: 258.76
epoch train time: 0:00:01.681902
elapsed time: 0:02:19.154766
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 19:48:11.397046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.39
 ---- batch: 020 ----
mean loss: 255.93
 ---- batch: 030 ----
mean loss: 250.49
 ---- batch: 040 ----
mean loss: 260.37
 ---- batch: 050 ----
mean loss: 263.11
 ---- batch: 060 ----
mean loss: 250.35
 ---- batch: 070 ----
mean loss: 251.72
 ---- batch: 080 ----
mean loss: 255.87
 ---- batch: 090 ----
mean loss: 247.21
train mean loss: 253.89
epoch train time: 0:00:01.660803
elapsed time: 0:02:20.816221
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 19:48:13.058520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.13
 ---- batch: 020 ----
mean loss: 249.27
 ---- batch: 030 ----
mean loss: 259.74
 ---- batch: 040 ----
mean loss: 255.18
 ---- batch: 050 ----
mean loss: 259.31
 ---- batch: 060 ----
mean loss: 253.84
 ---- batch: 070 ----
mean loss: 243.67
 ---- batch: 080 ----
mean loss: 245.48
 ---- batch: 090 ----
mean loss: 249.80
train mean loss: 253.24
epoch train time: 0:00:01.682525
elapsed time: 0:02:22.499480
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 19:48:14.741754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.49
 ---- batch: 020 ----
mean loss: 249.51
 ---- batch: 030 ----
mean loss: 248.41
 ---- batch: 040 ----
mean loss: 246.98
 ---- batch: 050 ----
mean loss: 247.53
 ---- batch: 060 ----
mean loss: 255.73
 ---- batch: 070 ----
mean loss: 240.06
 ---- batch: 080 ----
mean loss: 258.64
 ---- batch: 090 ----
mean loss: 245.07
train mean loss: 247.81
epoch train time: 0:00:01.675532
elapsed time: 0:02:24.175773
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 19:48:16.418046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.56
 ---- batch: 020 ----
mean loss: 236.20
 ---- batch: 030 ----
mean loss: 254.73
 ---- batch: 040 ----
mean loss: 251.29
 ---- batch: 050 ----
mean loss: 246.86
 ---- batch: 060 ----
mean loss: 253.71
 ---- batch: 070 ----
mean loss: 241.42
 ---- batch: 080 ----
mean loss: 241.37
 ---- batch: 090 ----
mean loss: 237.36
train mean loss: 244.42
epoch train time: 0:00:01.692699
elapsed time: 0:02:25.869071
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 19:48:18.111349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.97
 ---- batch: 020 ----
mean loss: 248.43
 ---- batch: 030 ----
mean loss: 240.13
 ---- batch: 040 ----
mean loss: 237.36
 ---- batch: 050 ----
mean loss: 238.34
 ---- batch: 060 ----
mean loss: 238.30
 ---- batch: 070 ----
mean loss: 244.34
 ---- batch: 080 ----
mean loss: 249.77
 ---- batch: 090 ----
mean loss: 239.20
train mean loss: 242.68
epoch train time: 0:00:01.659036
elapsed time: 0:02:27.528760
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 19:48:19.771069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.41
 ---- batch: 020 ----
mean loss: 248.42
 ---- batch: 030 ----
mean loss: 230.84
 ---- batch: 040 ----
mean loss: 245.53
 ---- batch: 050 ----
mean loss: 236.65
 ---- batch: 060 ----
mean loss: 237.00
 ---- batch: 070 ----
mean loss: 247.84
 ---- batch: 080 ----
mean loss: 239.95
 ---- batch: 090 ----
mean loss: 244.22
train mean loss: 240.91
epoch train time: 0:00:01.648949
elapsed time: 0:02:29.178319
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 19:48:21.420599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.81
 ---- batch: 020 ----
mean loss: 233.37
 ---- batch: 030 ----
mean loss: 233.57
 ---- batch: 040 ----
mean loss: 236.07
 ---- batch: 050 ----
mean loss: 235.31
 ---- batch: 060 ----
mean loss: 228.31
 ---- batch: 070 ----
mean loss: 224.73
 ---- batch: 080 ----
mean loss: 239.60
 ---- batch: 090 ----
mean loss: 243.47
train mean loss: 236.11
epoch train time: 0:00:01.672239
elapsed time: 0:02:30.851189
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 19:48:23.093483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.96
 ---- batch: 020 ----
mean loss: 230.02
 ---- batch: 030 ----
mean loss: 234.99
 ---- batch: 040 ----
mean loss: 242.61
 ---- batch: 050 ----
mean loss: 235.54
 ---- batch: 060 ----
mean loss: 233.97
 ---- batch: 070 ----
mean loss: 234.68
 ---- batch: 080 ----
mean loss: 233.33
 ---- batch: 090 ----
mean loss: 236.30
train mean loss: 234.63
epoch train time: 0:00:01.674035
elapsed time: 0:02:32.525868
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 19:48:24.768160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.66
 ---- batch: 020 ----
mean loss: 227.90
 ---- batch: 030 ----
mean loss: 233.31
 ---- batch: 040 ----
mean loss: 231.77
 ---- batch: 050 ----
mean loss: 230.35
 ---- batch: 060 ----
mean loss: 236.31
 ---- batch: 070 ----
mean loss: 226.89
 ---- batch: 080 ----
mean loss: 230.39
 ---- batch: 090 ----
mean loss: 236.33
train mean loss: 231.97
epoch train time: 0:00:01.663400
elapsed time: 0:02:34.189903
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 19:48:26.432191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.52
 ---- batch: 020 ----
mean loss: 216.42
 ---- batch: 030 ----
mean loss: 236.99
 ---- batch: 040 ----
mean loss: 225.85
 ---- batch: 050 ----
mean loss: 235.41
 ---- batch: 060 ----
mean loss: 231.08
 ---- batch: 070 ----
mean loss: 231.31
 ---- batch: 080 ----
mean loss: 235.37
 ---- batch: 090 ----
mean loss: 236.92
train mean loss: 229.45
epoch train time: 0:00:01.667282
elapsed time: 0:02:35.857860
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 19:48:28.100211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.63
 ---- batch: 020 ----
mean loss: 227.70
 ---- batch: 030 ----
mean loss: 236.46
 ---- batch: 040 ----
mean loss: 231.76
 ---- batch: 050 ----
mean loss: 222.65
 ---- batch: 060 ----
mean loss: 227.45
 ---- batch: 070 ----
mean loss: 225.80
 ---- batch: 080 ----
mean loss: 226.72
 ---- batch: 090 ----
mean loss: 225.19
train mean loss: 226.93
epoch train time: 0:00:01.661601
elapsed time: 0:02:37.520371
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 19:48:29.762656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.99
 ---- batch: 020 ----
mean loss: 230.66
 ---- batch: 030 ----
mean loss: 222.81
 ---- batch: 040 ----
mean loss: 231.50
 ---- batch: 050 ----
mean loss: 226.55
 ---- batch: 060 ----
mean loss: 232.32
 ---- batch: 070 ----
mean loss: 223.43
 ---- batch: 080 ----
mean loss: 219.89
 ---- batch: 090 ----
mean loss: 226.54
train mean loss: 225.96
epoch train time: 0:00:01.690707
elapsed time: 0:02:39.211745
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 19:48:31.454089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.46
 ---- batch: 020 ----
mean loss: 222.99
 ---- batch: 030 ----
mean loss: 223.46
 ---- batch: 040 ----
mean loss: 225.12
 ---- batch: 050 ----
mean loss: 211.63
 ---- batch: 060 ----
mean loss: 222.78
 ---- batch: 070 ----
mean loss: 224.65
 ---- batch: 080 ----
mean loss: 224.93
 ---- batch: 090 ----
mean loss: 221.34
train mean loss: 222.07
epoch train time: 0:00:01.685005
elapsed time: 0:02:40.897416
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 19:48:33.139701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.23
 ---- batch: 020 ----
mean loss: 218.32
 ---- batch: 030 ----
mean loss: 212.51
 ---- batch: 040 ----
mean loss: 220.01
 ---- batch: 050 ----
mean loss: 226.76
 ---- batch: 060 ----
mean loss: 222.45
 ---- batch: 070 ----
mean loss: 225.06
 ---- batch: 080 ----
mean loss: 224.08
 ---- batch: 090 ----
mean loss: 220.30
train mean loss: 221.43
epoch train time: 0:00:01.643940
elapsed time: 0:02:42.542056
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 19:48:34.784329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.00
 ---- batch: 020 ----
mean loss: 216.13
 ---- batch: 030 ----
mean loss: 213.62
 ---- batch: 040 ----
mean loss: 225.13
 ---- batch: 050 ----
mean loss: 211.13
 ---- batch: 060 ----
mean loss: 217.41
 ---- batch: 070 ----
mean loss: 222.25
 ---- batch: 080 ----
mean loss: 224.44
 ---- batch: 090 ----
mean loss: 218.46
train mean loss: 218.58
epoch train time: 0:00:01.708880
elapsed time: 0:02:44.251646
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 19:48:36.493943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.30
 ---- batch: 020 ----
mean loss: 211.64
 ---- batch: 030 ----
mean loss: 214.68
 ---- batch: 040 ----
mean loss: 227.20
 ---- batch: 050 ----
mean loss: 218.71
 ---- batch: 060 ----
mean loss: 211.55
 ---- batch: 070 ----
mean loss: 226.74
 ---- batch: 080 ----
mean loss: 214.48
 ---- batch: 090 ----
mean loss: 217.11
train mean loss: 215.89
epoch train time: 0:00:01.702708
elapsed time: 0:02:45.954957
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 19:48:38.197257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.84
 ---- batch: 020 ----
mean loss: 207.34
 ---- batch: 030 ----
mean loss: 209.96
 ---- batch: 040 ----
mean loss: 222.18
 ---- batch: 050 ----
mean loss: 213.73
 ---- batch: 060 ----
mean loss: 219.61
 ---- batch: 070 ----
mean loss: 207.13
 ---- batch: 080 ----
mean loss: 220.70
 ---- batch: 090 ----
mean loss: 222.08
train mean loss: 214.57
epoch train time: 0:00:01.643562
elapsed time: 0:02:47.599291
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 19:48:39.841696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.97
 ---- batch: 020 ----
mean loss: 206.46
 ---- batch: 030 ----
mean loss: 206.15
 ---- batch: 040 ----
mean loss: 213.16
 ---- batch: 050 ----
mean loss: 214.38
 ---- batch: 060 ----
mean loss: 213.03
 ---- batch: 070 ----
mean loss: 214.08
 ---- batch: 080 ----
mean loss: 214.31
 ---- batch: 090 ----
mean loss: 219.49
train mean loss: 212.64
epoch train time: 0:00:01.674914
elapsed time: 0:02:49.274980
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 19:48:41.517251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.39
 ---- batch: 020 ----
mean loss: 214.26
 ---- batch: 030 ----
mean loss: 215.35
 ---- batch: 040 ----
mean loss: 206.01
 ---- batch: 050 ----
mean loss: 217.33
 ---- batch: 060 ----
mean loss: 208.56
 ---- batch: 070 ----
mean loss: 209.71
 ---- batch: 080 ----
mean loss: 208.44
 ---- batch: 090 ----
mean loss: 208.33
train mean loss: 210.26
epoch train time: 0:00:01.688449
elapsed time: 0:02:50.964032
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 19:48:43.206325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.96
 ---- batch: 020 ----
mean loss: 205.05
 ---- batch: 030 ----
mean loss: 201.43
 ---- batch: 040 ----
mean loss: 205.92
 ---- batch: 050 ----
mean loss: 213.45
 ---- batch: 060 ----
mean loss: 213.54
 ---- batch: 070 ----
mean loss: 203.19
 ---- batch: 080 ----
mean loss: 216.13
 ---- batch: 090 ----
mean loss: 205.03
train mean loss: 207.55
epoch train time: 0:00:01.655863
elapsed time: 0:02:52.620552
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 19:48:44.862966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.24
 ---- batch: 020 ----
mean loss: 196.34
 ---- batch: 030 ----
mean loss: 201.80
 ---- batch: 040 ----
mean loss: 202.62
 ---- batch: 050 ----
mean loss: 211.40
 ---- batch: 060 ----
mean loss: 209.78
 ---- batch: 070 ----
mean loss: 201.00
 ---- batch: 080 ----
mean loss: 207.77
 ---- batch: 090 ----
mean loss: 215.52
train mean loss: 206.47
epoch train time: 0:00:01.673136
elapsed time: 0:02:54.294464
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 19:48:46.536748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.74
 ---- batch: 020 ----
mean loss: 198.93
 ---- batch: 030 ----
mean loss: 202.18
 ---- batch: 040 ----
mean loss: 199.08
 ---- batch: 050 ----
mean loss: 212.47
 ---- batch: 060 ----
mean loss: 206.89
 ---- batch: 070 ----
mean loss: 198.77
 ---- batch: 080 ----
mean loss: 208.12
 ---- batch: 090 ----
mean loss: 215.59
train mean loss: 205.20
epoch train time: 0:00:01.666038
elapsed time: 0:02:55.961132
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 19:48:48.203423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.03
 ---- batch: 020 ----
mean loss: 207.00
 ---- batch: 030 ----
mean loss: 202.87
 ---- batch: 040 ----
mean loss: 207.47
 ---- batch: 050 ----
mean loss: 203.74
 ---- batch: 060 ----
mean loss: 198.89
 ---- batch: 070 ----
mean loss: 192.77
 ---- batch: 080 ----
mean loss: 205.53
 ---- batch: 090 ----
mean loss: 207.15
train mean loss: 203.55
epoch train time: 0:00:01.704025
elapsed time: 0:02:57.665829
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 19:48:49.907961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.84
 ---- batch: 020 ----
mean loss: 193.55
 ---- batch: 030 ----
mean loss: 204.03
 ---- batch: 040 ----
mean loss: 205.50
 ---- batch: 050 ----
mean loss: 197.02
 ---- batch: 060 ----
mean loss: 203.44
 ---- batch: 070 ----
mean loss: 198.50
 ---- batch: 080 ----
mean loss: 200.94
 ---- batch: 090 ----
mean loss: 195.57
train mean loss: 200.22
epoch train time: 0:00:01.702732
elapsed time: 0:02:59.369095
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 19:48:51.611381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.44
 ---- batch: 020 ----
mean loss: 195.94
 ---- batch: 030 ----
mean loss: 197.48
 ---- batch: 040 ----
mean loss: 197.56
 ---- batch: 050 ----
mean loss: 184.71
 ---- batch: 060 ----
mean loss: 201.11
 ---- batch: 070 ----
mean loss: 202.86
 ---- batch: 080 ----
mean loss: 197.54
 ---- batch: 090 ----
mean loss: 205.34
train mean loss: 198.87
epoch train time: 0:00:01.736538
elapsed time: 0:03:01.106342
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 19:48:53.348627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.90
 ---- batch: 020 ----
mean loss: 198.67
 ---- batch: 030 ----
mean loss: 196.15
 ---- batch: 040 ----
mean loss: 194.98
 ---- batch: 050 ----
mean loss: 192.91
 ---- batch: 060 ----
mean loss: 200.16
 ---- batch: 070 ----
mean loss: 196.72
 ---- batch: 080 ----
mean loss: 200.67
 ---- batch: 090 ----
mean loss: 200.98
train mean loss: 197.50
epoch train time: 0:00:01.695288
elapsed time: 0:03:02.802306
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 19:48:55.044668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.66
 ---- batch: 020 ----
mean loss: 197.87
 ---- batch: 030 ----
mean loss: 194.28
 ---- batch: 040 ----
mean loss: 196.52
 ---- batch: 050 ----
mean loss: 199.51
 ---- batch: 060 ----
mean loss: 192.07
 ---- batch: 070 ----
mean loss: 195.59
 ---- batch: 080 ----
mean loss: 193.76
 ---- batch: 090 ----
mean loss: 199.11
train mean loss: 195.16
epoch train time: 0:00:01.642728
elapsed time: 0:03:04.445810
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 19:48:56.688110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.21
 ---- batch: 020 ----
mean loss: 198.36
 ---- batch: 030 ----
mean loss: 186.85
 ---- batch: 040 ----
mean loss: 191.05
 ---- batch: 050 ----
mean loss: 196.36
 ---- batch: 060 ----
mean loss: 204.45
 ---- batch: 070 ----
mean loss: 190.23
 ---- batch: 080 ----
mean loss: 198.90
 ---- batch: 090 ----
mean loss: 194.21
train mean loss: 194.45
epoch train time: 0:00:01.678455
elapsed time: 0:03:06.124921
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 19:48:58.367236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.70
 ---- batch: 020 ----
mean loss: 199.27
 ---- batch: 030 ----
mean loss: 191.92
 ---- batch: 040 ----
mean loss: 190.47
 ---- batch: 050 ----
mean loss: 197.66
 ---- batch: 060 ----
mean loss: 196.58
 ---- batch: 070 ----
mean loss: 195.12
 ---- batch: 080 ----
mean loss: 198.44
 ---- batch: 090 ----
mean loss: 183.00
train mean loss: 193.25
epoch train time: 0:00:01.674742
elapsed time: 0:03:07.800362
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 19:49:00.042648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.28
 ---- batch: 020 ----
mean loss: 192.53
 ---- batch: 030 ----
mean loss: 187.81
 ---- batch: 040 ----
mean loss: 191.60
 ---- batch: 050 ----
mean loss: 183.22
 ---- batch: 060 ----
mean loss: 197.05
 ---- batch: 070 ----
mean loss: 186.24
 ---- batch: 080 ----
mean loss: 196.20
 ---- batch: 090 ----
mean loss: 190.16
train mean loss: 190.28
epoch train time: 0:00:01.655476
elapsed time: 0:03:09.456517
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 19:49:01.698813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.65
 ---- batch: 020 ----
mean loss: 191.27
 ---- batch: 030 ----
mean loss: 187.74
 ---- batch: 040 ----
mean loss: 191.65
 ---- batch: 050 ----
mean loss: 188.73
 ---- batch: 060 ----
mean loss: 199.64
 ---- batch: 070 ----
mean loss: 196.80
 ---- batch: 080 ----
mean loss: 186.93
 ---- batch: 090 ----
mean loss: 182.46
train mean loss: 189.49
epoch train time: 0:00:01.667618
elapsed time: 0:03:11.124832
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 19:49:03.367160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.04
 ---- batch: 020 ----
mean loss: 183.57
 ---- batch: 030 ----
mean loss: 195.70
 ---- batch: 040 ----
mean loss: 177.62
 ---- batch: 050 ----
mean loss: 184.38
 ---- batch: 060 ----
mean loss: 195.45
 ---- batch: 070 ----
mean loss: 187.45
 ---- batch: 080 ----
mean loss: 182.30
 ---- batch: 090 ----
mean loss: 200.34
train mean loss: 188.33
epoch train time: 0:00:01.676238
elapsed time: 0:03:12.801789
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 19:49:05.044201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.13
 ---- batch: 020 ----
mean loss: 187.55
 ---- batch: 030 ----
mean loss: 185.46
 ---- batch: 040 ----
mean loss: 184.75
 ---- batch: 050 ----
mean loss: 187.55
 ---- batch: 060 ----
mean loss: 190.50
 ---- batch: 070 ----
mean loss: 180.38
 ---- batch: 080 ----
mean loss: 184.81
 ---- batch: 090 ----
mean loss: 187.55
train mean loss: 186.07
epoch train time: 0:00:01.682370
elapsed time: 0:03:14.485085
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 19:49:06.727319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.88
 ---- batch: 020 ----
mean loss: 176.33
 ---- batch: 030 ----
mean loss: 183.76
 ---- batch: 040 ----
mean loss: 185.89
 ---- batch: 050 ----
mean loss: 184.36
 ---- batch: 060 ----
mean loss: 185.29
 ---- batch: 070 ----
mean loss: 192.47
 ---- batch: 080 ----
mean loss: 192.35
 ---- batch: 090 ----
mean loss: 185.72
train mean loss: 184.94
epoch train time: 0:00:01.658712
elapsed time: 0:03:16.144388
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 19:49:08.386694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.33
 ---- batch: 020 ----
mean loss: 174.80
 ---- batch: 030 ----
mean loss: 183.29
 ---- batch: 040 ----
mean loss: 181.55
 ---- batch: 050 ----
mean loss: 185.36
 ---- batch: 060 ----
mean loss: 184.00
 ---- batch: 070 ----
mean loss: 189.33
 ---- batch: 080 ----
mean loss: 186.87
 ---- batch: 090 ----
mean loss: 185.33
train mean loss: 183.21
epoch train time: 0:00:01.648662
elapsed time: 0:03:17.793718
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 19:49:10.035999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.07
 ---- batch: 020 ----
mean loss: 187.54
 ---- batch: 030 ----
mean loss: 181.11
 ---- batch: 040 ----
mean loss: 177.20
 ---- batch: 050 ----
mean loss: 182.60
 ---- batch: 060 ----
mean loss: 189.94
 ---- batch: 070 ----
mean loss: 186.54
 ---- batch: 080 ----
mean loss: 184.39
 ---- batch: 090 ----
mean loss: 181.26
train mean loss: 183.15
epoch train time: 0:00:01.687354
elapsed time: 0:03:19.481901
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 19:49:11.724214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.94
 ---- batch: 020 ----
mean loss: 170.34
 ---- batch: 030 ----
mean loss: 179.19
 ---- batch: 040 ----
mean loss: 173.42
 ---- batch: 050 ----
mean loss: 186.41
 ---- batch: 060 ----
mean loss: 184.00
 ---- batch: 070 ----
mean loss: 186.04
 ---- batch: 080 ----
mean loss: 186.47
 ---- batch: 090 ----
mean loss: 185.00
train mean loss: 181.71
epoch train time: 0:00:01.649792
elapsed time: 0:03:21.132421
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 19:49:13.374720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.12
 ---- batch: 020 ----
mean loss: 172.05
 ---- batch: 030 ----
mean loss: 179.02
 ---- batch: 040 ----
mean loss: 172.75
 ---- batch: 050 ----
mean loss: 183.89
 ---- batch: 060 ----
mean loss: 173.12
 ---- batch: 070 ----
mean loss: 179.69
 ---- batch: 080 ----
mean loss: 186.97
 ---- batch: 090 ----
mean loss: 183.96
train mean loss: 180.17
epoch train time: 0:00:01.665976
elapsed time: 0:03:22.799088
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 19:49:15.041381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.23
 ---- batch: 020 ----
mean loss: 176.52
 ---- batch: 030 ----
mean loss: 175.03
 ---- batch: 040 ----
mean loss: 177.81
 ---- batch: 050 ----
mean loss: 181.03
 ---- batch: 060 ----
mean loss: 187.63
 ---- batch: 070 ----
mean loss: 179.00
 ---- batch: 080 ----
mean loss: 175.36
 ---- batch: 090 ----
mean loss: 176.06
train mean loss: 178.70
epoch train time: 0:00:01.651768
elapsed time: 0:03:24.451548
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 19:49:16.693889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.35
 ---- batch: 020 ----
mean loss: 175.59
 ---- batch: 030 ----
mean loss: 177.69
 ---- batch: 040 ----
mean loss: 177.39
 ---- batch: 050 ----
mean loss: 178.94
 ---- batch: 060 ----
mean loss: 180.27
 ---- batch: 070 ----
mean loss: 179.37
 ---- batch: 080 ----
mean loss: 176.85
 ---- batch: 090 ----
mean loss: 179.60
train mean loss: 178.10
epoch train time: 0:00:01.645794
elapsed time: 0:03:26.098093
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 19:49:18.340402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.91
 ---- batch: 020 ----
mean loss: 176.55
 ---- batch: 030 ----
mean loss: 170.08
 ---- batch: 040 ----
mean loss: 175.67
 ---- batch: 050 ----
mean loss: 171.35
 ---- batch: 060 ----
mean loss: 177.16
 ---- batch: 070 ----
mean loss: 178.67
 ---- batch: 080 ----
mean loss: 166.62
 ---- batch: 090 ----
mean loss: 177.17
train mean loss: 174.96
epoch train time: 0:00:01.656134
elapsed time: 0:03:27.754920
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 19:49:19.997199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.48
 ---- batch: 020 ----
mean loss: 180.56
 ---- batch: 030 ----
mean loss: 168.87
 ---- batch: 040 ----
mean loss: 178.10
 ---- batch: 050 ----
mean loss: 173.59
 ---- batch: 060 ----
mean loss: 176.34
 ---- batch: 070 ----
mean loss: 173.78
 ---- batch: 080 ----
mean loss: 172.94
 ---- batch: 090 ----
mean loss: 174.24
train mean loss: 175.44
epoch train time: 0:00:01.689410
elapsed time: 0:03:29.445028
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 19:49:21.687309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.27
 ---- batch: 020 ----
mean loss: 165.51
 ---- batch: 030 ----
mean loss: 178.03
 ---- batch: 040 ----
mean loss: 167.95
 ---- batch: 050 ----
mean loss: 169.98
 ---- batch: 060 ----
mean loss: 175.74
 ---- batch: 070 ----
mean loss: 175.21
 ---- batch: 080 ----
mean loss: 177.34
 ---- batch: 090 ----
mean loss: 178.62
train mean loss: 173.93
epoch train time: 0:00:01.662496
elapsed time: 0:03:31.108280
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 19:49:23.350586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.96
 ---- batch: 020 ----
mean loss: 175.31
 ---- batch: 030 ----
mean loss: 164.33
 ---- batch: 040 ----
mean loss: 169.85
 ---- batch: 050 ----
mean loss: 175.08
 ---- batch: 060 ----
mean loss: 175.46
 ---- batch: 070 ----
mean loss: 173.56
 ---- batch: 080 ----
mean loss: 173.14
 ---- batch: 090 ----
mean loss: 173.46
train mean loss: 172.45
epoch train time: 0:00:01.669321
elapsed time: 0:03:32.778428
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 19:49:25.020560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.93
 ---- batch: 020 ----
mean loss: 168.53
 ---- batch: 030 ----
mean loss: 170.50
 ---- batch: 040 ----
mean loss: 175.62
 ---- batch: 050 ----
mean loss: 172.41
 ---- batch: 060 ----
mean loss: 171.01
 ---- batch: 070 ----
mean loss: 175.54
 ---- batch: 080 ----
mean loss: 175.01
 ---- batch: 090 ----
mean loss: 174.27
train mean loss: 171.87
epoch train time: 0:00:01.652268
elapsed time: 0:03:34.431239
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 19:49:26.673538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.84
 ---- batch: 020 ----
mean loss: 171.73
 ---- batch: 030 ----
mean loss: 163.53
 ---- batch: 040 ----
mean loss: 171.61
 ---- batch: 050 ----
mean loss: 167.41
 ---- batch: 060 ----
mean loss: 177.95
 ---- batch: 070 ----
mean loss: 172.18
 ---- batch: 080 ----
mean loss: 172.18
 ---- batch: 090 ----
mean loss: 166.44
train mean loss: 170.29
epoch train time: 0:00:01.653846
elapsed time: 0:03:36.085776
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 19:49:28.328075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.41
 ---- batch: 020 ----
mean loss: 166.83
 ---- batch: 030 ----
mean loss: 166.63
 ---- batch: 040 ----
mean loss: 159.80
 ---- batch: 050 ----
mean loss: 165.28
 ---- batch: 060 ----
mean loss: 172.32
 ---- batch: 070 ----
mean loss: 172.69
 ---- batch: 080 ----
mean loss: 174.04
 ---- batch: 090 ----
mean loss: 171.77
train mean loss: 169.54
epoch train time: 0:00:01.657813
elapsed time: 0:03:37.744310
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 19:49:29.986591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.22
 ---- batch: 020 ----
mean loss: 166.29
 ---- batch: 030 ----
mean loss: 167.62
 ---- batch: 040 ----
mean loss: 168.32
 ---- batch: 050 ----
mean loss: 166.86
 ---- batch: 060 ----
mean loss: 169.55
 ---- batch: 070 ----
mean loss: 172.49
 ---- batch: 080 ----
mean loss: 167.66
 ---- batch: 090 ----
mean loss: 171.08
train mean loss: 168.82
epoch train time: 0:00:01.668125
elapsed time: 0:03:39.413115
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 19:49:31.655408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.29
 ---- batch: 020 ----
mean loss: 159.72
 ---- batch: 030 ----
mean loss: 177.69
 ---- batch: 040 ----
mean loss: 172.23
 ---- batch: 050 ----
mean loss: 171.21
 ---- batch: 060 ----
mean loss: 167.13
 ---- batch: 070 ----
mean loss: 167.59
 ---- batch: 080 ----
mean loss: 165.74
 ---- batch: 090 ----
mean loss: 168.78
train mean loss: 167.10
epoch train time: 0:00:01.705971
elapsed time: 0:03:41.119773
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 19:49:33.362075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.63
 ---- batch: 020 ----
mean loss: 170.93
 ---- batch: 030 ----
mean loss: 163.38
 ---- batch: 040 ----
mean loss: 167.55
 ---- batch: 050 ----
mean loss: 163.45
 ---- batch: 060 ----
mean loss: 167.84
 ---- batch: 070 ----
mean loss: 171.81
 ---- batch: 080 ----
mean loss: 166.87
 ---- batch: 090 ----
mean loss: 159.66
train mean loss: 166.44
epoch train time: 0:00:01.654802
elapsed time: 0:03:42.775269
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 19:49:35.017543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.42
 ---- batch: 020 ----
mean loss: 161.61
 ---- batch: 030 ----
mean loss: 161.16
 ---- batch: 040 ----
mean loss: 164.23
 ---- batch: 050 ----
mean loss: 168.61
 ---- batch: 060 ----
mean loss: 170.23
 ---- batch: 070 ----
mean loss: 161.44
 ---- batch: 080 ----
mean loss: 169.40
 ---- batch: 090 ----
mean loss: 166.67
train mean loss: 165.60
epoch train time: 0:00:01.673520
elapsed time: 0:03:44.449467
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 19:49:36.691760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.52
 ---- batch: 020 ----
mean loss: 163.97
 ---- batch: 030 ----
mean loss: 157.11
 ---- batch: 040 ----
mean loss: 163.42
 ---- batch: 050 ----
mean loss: 160.83
 ---- batch: 060 ----
mean loss: 171.31
 ---- batch: 070 ----
mean loss: 170.98
 ---- batch: 080 ----
mean loss: 157.36
 ---- batch: 090 ----
mean loss: 171.41
train mean loss: 164.25
epoch train time: 0:00:01.667110
elapsed time: 0:03:46.117163
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 19:49:38.359460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.19
 ---- batch: 020 ----
mean loss: 164.16
 ---- batch: 030 ----
mean loss: 160.63
 ---- batch: 040 ----
mean loss: 155.07
 ---- batch: 050 ----
mean loss: 169.64
 ---- batch: 060 ----
mean loss: 165.15
 ---- batch: 070 ----
mean loss: 167.77
 ---- batch: 080 ----
mean loss: 165.39
 ---- batch: 090 ----
mean loss: 168.25
train mean loss: 162.91
epoch train time: 0:00:01.684565
elapsed time: 0:03:47.802388
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 19:49:40.044690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.41
 ---- batch: 020 ----
mean loss: 152.41
 ---- batch: 030 ----
mean loss: 161.30
 ---- batch: 040 ----
mean loss: 161.87
 ---- batch: 050 ----
mean loss: 165.87
 ---- batch: 060 ----
mean loss: 166.84
 ---- batch: 070 ----
mean loss: 168.34
 ---- batch: 080 ----
mean loss: 170.16
 ---- batch: 090 ----
mean loss: 172.09
train mean loss: 162.61
epoch train time: 0:00:01.717967
elapsed time: 0:03:49.521120
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 19:49:41.763434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.67
 ---- batch: 020 ----
mean loss: 154.59
 ---- batch: 030 ----
mean loss: 160.67
 ---- batch: 040 ----
mean loss: 161.34
 ---- batch: 050 ----
mean loss: 162.30
 ---- batch: 060 ----
mean loss: 157.49
 ---- batch: 070 ----
mean loss: 165.48
 ---- batch: 080 ----
mean loss: 167.82
 ---- batch: 090 ----
mean loss: 171.51
train mean loss: 162.16
epoch train time: 0:00:01.726641
elapsed time: 0:03:51.248476
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 19:49:43.490779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.19
 ---- batch: 020 ----
mean loss: 159.55
 ---- batch: 030 ----
mean loss: 158.93
 ---- batch: 040 ----
mean loss: 166.91
 ---- batch: 050 ----
mean loss: 162.42
 ---- batch: 060 ----
mean loss: 163.08
 ---- batch: 070 ----
mean loss: 161.04
 ---- batch: 080 ----
mean loss: 162.98
 ---- batch: 090 ----
mean loss: 158.83
train mean loss: 160.98
epoch train time: 0:00:01.716774
elapsed time: 0:03:52.965960
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 19:49:45.208350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.33
 ---- batch: 020 ----
mean loss: 155.46
 ---- batch: 030 ----
mean loss: 157.19
 ---- batch: 040 ----
mean loss: 160.75
 ---- batch: 050 ----
mean loss: 164.98
 ---- batch: 060 ----
mean loss: 160.39
 ---- batch: 070 ----
mean loss: 158.37
 ---- batch: 080 ----
mean loss: 166.85
 ---- batch: 090 ----
mean loss: 162.30
train mean loss: 160.45
epoch train time: 0:00:01.674652
elapsed time: 0:03:54.641448
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 19:49:46.883717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.37
 ---- batch: 020 ----
mean loss: 153.99
 ---- batch: 030 ----
mean loss: 156.74
 ---- batch: 040 ----
mean loss: 159.80
 ---- batch: 050 ----
mean loss: 162.03
 ---- batch: 060 ----
mean loss: 160.69
 ---- batch: 070 ----
mean loss: 151.53
 ---- batch: 080 ----
mean loss: 160.57
 ---- batch: 090 ----
mean loss: 162.42
train mean loss: 158.56
epoch train time: 0:00:01.691743
elapsed time: 0:03:56.333823
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 19:49:48.576106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.48
 ---- batch: 020 ----
mean loss: 154.78
 ---- batch: 030 ----
mean loss: 161.69
 ---- batch: 040 ----
mean loss: 158.81
 ---- batch: 050 ----
mean loss: 159.57
 ---- batch: 060 ----
mean loss: 156.72
 ---- batch: 070 ----
mean loss: 161.21
 ---- batch: 080 ----
mean loss: 165.54
 ---- batch: 090 ----
mean loss: 156.63
train mean loss: 158.40
epoch train time: 0:00:01.667543
elapsed time: 0:03:58.002052
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 19:49:50.244379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.16
 ---- batch: 020 ----
mean loss: 154.88
 ---- batch: 030 ----
mean loss: 153.25
 ---- batch: 040 ----
mean loss: 156.31
 ---- batch: 050 ----
mean loss: 161.28
 ---- batch: 060 ----
mean loss: 156.44
 ---- batch: 070 ----
mean loss: 156.25
 ---- batch: 080 ----
mean loss: 159.76
 ---- batch: 090 ----
mean loss: 158.08
train mean loss: 157.34
epoch train time: 0:00:01.703254
elapsed time: 0:03:59.706049
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 19:49:51.948384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.06
 ---- batch: 020 ----
mean loss: 151.48
 ---- batch: 030 ----
mean loss: 145.79
 ---- batch: 040 ----
mean loss: 161.79
 ---- batch: 050 ----
mean loss: 168.34
 ---- batch: 060 ----
mean loss: 158.28
 ---- batch: 070 ----
mean loss: 152.60
 ---- batch: 080 ----
mean loss: 159.82
 ---- batch: 090 ----
mean loss: 158.83
train mean loss: 156.52
epoch train time: 0:00:01.690465
elapsed time: 0:04:01.397196
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 19:49:53.639485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.26
 ---- batch: 020 ----
mean loss: 156.26
 ---- batch: 030 ----
mean loss: 152.15
 ---- batch: 040 ----
mean loss: 154.65
 ---- batch: 050 ----
mean loss: 155.90
 ---- batch: 060 ----
mean loss: 164.87
 ---- batch: 070 ----
mean loss: 153.69
 ---- batch: 080 ----
mean loss: 156.18
 ---- batch: 090 ----
mean loss: 153.73
train mean loss: 155.74
epoch train time: 0:00:01.665481
elapsed time: 0:04:03.063315
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 19:49:55.305593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.45
 ---- batch: 020 ----
mean loss: 157.04
 ---- batch: 030 ----
mean loss: 153.37
 ---- batch: 040 ----
mean loss: 147.36
 ---- batch: 050 ----
mean loss: 157.75
 ---- batch: 060 ----
mean loss: 155.58
 ---- batch: 070 ----
mean loss: 152.72
 ---- batch: 080 ----
mean loss: 153.15
 ---- batch: 090 ----
mean loss: 161.62
train mean loss: 153.97
epoch train time: 0:00:01.685712
elapsed time: 0:04:04.749668
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 19:49:56.991933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.67
 ---- batch: 020 ----
mean loss: 152.60
 ---- batch: 030 ----
mean loss: 152.69
 ---- batch: 040 ----
mean loss: 156.34
 ---- batch: 050 ----
mean loss: 146.56
 ---- batch: 060 ----
mean loss: 155.37
 ---- batch: 070 ----
mean loss: 158.18
 ---- batch: 080 ----
mean loss: 157.23
 ---- batch: 090 ----
mean loss: 163.01
train mean loss: 153.82
epoch train time: 0:00:01.660329
elapsed time: 0:04:06.410801
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 19:49:58.652915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.07
 ---- batch: 020 ----
mean loss: 148.71
 ---- batch: 030 ----
mean loss: 148.26
 ---- batch: 040 ----
mean loss: 153.92
 ---- batch: 050 ----
mean loss: 155.31
 ---- batch: 060 ----
mean loss: 155.94
 ---- batch: 070 ----
mean loss: 152.71
 ---- batch: 080 ----
mean loss: 150.31
 ---- batch: 090 ----
mean loss: 166.66
train mean loss: 153.09
epoch train time: 0:00:01.688709
elapsed time: 0:04:08.100165
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 19:50:00.342462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.84
 ---- batch: 020 ----
mean loss: 147.43
 ---- batch: 030 ----
mean loss: 148.25
 ---- batch: 040 ----
mean loss: 150.38
 ---- batch: 050 ----
mean loss: 154.06
 ---- batch: 060 ----
mean loss: 149.20
 ---- batch: 070 ----
mean loss: 160.81
 ---- batch: 080 ----
mean loss: 159.22
 ---- batch: 090 ----
mean loss: 153.17
train mean loss: 152.14
epoch train time: 0:00:01.668045
elapsed time: 0:04:09.768937
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 19:50:02.011253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.65
 ---- batch: 020 ----
mean loss: 150.67
 ---- batch: 030 ----
mean loss: 145.81
 ---- batch: 040 ----
mean loss: 149.32
 ---- batch: 050 ----
mean loss: 150.69
 ---- batch: 060 ----
mean loss: 144.85
 ---- batch: 070 ----
mean loss: 152.84
 ---- batch: 080 ----
mean loss: 158.17
 ---- batch: 090 ----
mean loss: 158.02
train mean loss: 151.71
epoch train time: 0:00:01.638021
elapsed time: 0:04:11.407722
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 19:50:03.650039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.21
 ---- batch: 020 ----
mean loss: 152.70
 ---- batch: 030 ----
mean loss: 150.07
 ---- batch: 040 ----
mean loss: 144.95
 ---- batch: 050 ----
mean loss: 150.30
 ---- batch: 060 ----
mean loss: 148.07
 ---- batch: 070 ----
mean loss: 154.07
 ---- batch: 080 ----
mean loss: 151.15
 ---- batch: 090 ----
mean loss: 161.52
train mean loss: 150.71
epoch train time: 0:00:01.653745
elapsed time: 0:04:13.062172
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 19:50:05.304495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.76
 ---- batch: 020 ----
mean loss: 144.20
 ---- batch: 030 ----
mean loss: 153.41
 ---- batch: 040 ----
mean loss: 146.98
 ---- batch: 050 ----
mean loss: 149.75
 ---- batch: 060 ----
mean loss: 150.39
 ---- batch: 070 ----
mean loss: 152.64
 ---- batch: 080 ----
mean loss: 160.59
 ---- batch: 090 ----
mean loss: 147.16
train mean loss: 150.75
epoch train time: 0:00:01.690825
elapsed time: 0:04:14.753769
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 19:50:06.996073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.69
 ---- batch: 020 ----
mean loss: 142.51
 ---- batch: 030 ----
mean loss: 148.67
 ---- batch: 040 ----
mean loss: 144.67
 ---- batch: 050 ----
mean loss: 148.11
 ---- batch: 060 ----
mean loss: 153.31
 ---- batch: 070 ----
mean loss: 154.05
 ---- batch: 080 ----
mean loss: 151.45
 ---- batch: 090 ----
mean loss: 151.60
train mean loss: 149.56
epoch train time: 0:00:01.685888
elapsed time: 0:04:16.440442
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 19:50:08.682739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.99
 ---- batch: 020 ----
mean loss: 138.65
 ---- batch: 030 ----
mean loss: 148.52
 ---- batch: 040 ----
mean loss: 148.55
 ---- batch: 050 ----
mean loss: 147.19
 ---- batch: 060 ----
mean loss: 146.97
 ---- batch: 070 ----
mean loss: 154.63
 ---- batch: 080 ----
mean loss: 153.95
 ---- batch: 090 ----
mean loss: 150.79
train mean loss: 148.63
epoch train time: 0:00:01.680994
elapsed time: 0:04:18.122139
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 19:50:10.364430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.73
 ---- batch: 020 ----
mean loss: 145.67
 ---- batch: 030 ----
mean loss: 147.12
 ---- batch: 040 ----
mean loss: 150.08
 ---- batch: 050 ----
mean loss: 148.06
 ---- batch: 060 ----
mean loss: 154.47
 ---- batch: 070 ----
mean loss: 154.93
 ---- batch: 080 ----
mean loss: 146.78
 ---- batch: 090 ----
mean loss: 142.45
train mean loss: 148.58
epoch train time: 0:00:01.695704
elapsed time: 0:04:19.818540
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 19:50:12.060807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.49
 ---- batch: 020 ----
mean loss: 140.47
 ---- batch: 030 ----
mean loss: 143.93
 ---- batch: 040 ----
mean loss: 143.83
 ---- batch: 050 ----
mean loss: 144.00
 ---- batch: 060 ----
mean loss: 145.09
 ---- batch: 070 ----
mean loss: 149.44
 ---- batch: 080 ----
mean loss: 152.24
 ---- batch: 090 ----
mean loss: 156.15
train mean loss: 147.33
epoch train time: 0:00:01.664110
elapsed time: 0:04:21.483317
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 19:50:13.725664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.09
 ---- batch: 020 ----
mean loss: 140.77
 ---- batch: 030 ----
mean loss: 142.50
 ---- batch: 040 ----
mean loss: 148.75
 ---- batch: 050 ----
mean loss: 145.30
 ---- batch: 060 ----
mean loss: 147.42
 ---- batch: 070 ----
mean loss: 147.80
 ---- batch: 080 ----
mean loss: 149.58
 ---- batch: 090 ----
mean loss: 151.44
train mean loss: 146.79
epoch train time: 0:00:01.682653
elapsed time: 0:04:23.166643
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 19:50:15.408946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.86
 ---- batch: 020 ----
mean loss: 145.09
 ---- batch: 030 ----
mean loss: 138.95
 ---- batch: 040 ----
mean loss: 149.81
 ---- batch: 050 ----
mean loss: 141.93
 ---- batch: 060 ----
mean loss: 153.81
 ---- batch: 070 ----
mean loss: 146.13
 ---- batch: 080 ----
mean loss: 146.78
 ---- batch: 090 ----
mean loss: 149.59
train mean loss: 146.30
epoch train time: 0:00:01.658170
elapsed time: 0:04:24.825480
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 19:50:17.067762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.61
 ---- batch: 020 ----
mean loss: 139.76
 ---- batch: 030 ----
mean loss: 149.35
 ---- batch: 040 ----
mean loss: 145.03
 ---- batch: 050 ----
mean loss: 148.40
 ---- batch: 060 ----
mean loss: 141.77
 ---- batch: 070 ----
mean loss: 139.30
 ---- batch: 080 ----
mean loss: 153.42
 ---- batch: 090 ----
mean loss: 156.76
train mean loss: 145.51
epoch train time: 0:00:01.648898
elapsed time: 0:04:26.475071
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 19:50:18.717456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.88
 ---- batch: 020 ----
mean loss: 135.65
 ---- batch: 030 ----
mean loss: 149.98
 ---- batch: 040 ----
mean loss: 145.57
 ---- batch: 050 ----
mean loss: 143.48
 ---- batch: 060 ----
mean loss: 150.91
 ---- batch: 070 ----
mean loss: 145.85
 ---- batch: 080 ----
mean loss: 151.09
 ---- batch: 090 ----
mean loss: 144.49
train mean loss: 144.83
epoch train time: 0:00:01.699627
elapsed time: 0:04:28.175497
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 19:50:20.417810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.75
 ---- batch: 020 ----
mean loss: 142.94
 ---- batch: 030 ----
mean loss: 141.40
 ---- batch: 040 ----
mean loss: 142.29
 ---- batch: 050 ----
mean loss: 145.48
 ---- batch: 060 ----
mean loss: 146.02
 ---- batch: 070 ----
mean loss: 142.74
 ---- batch: 080 ----
mean loss: 146.14
 ---- batch: 090 ----
mean loss: 148.10
train mean loss: 143.96
epoch train time: 0:00:01.666334
elapsed time: 0:04:29.842512
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 19:50:22.084835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.59
 ---- batch: 020 ----
mean loss: 143.25
 ---- batch: 030 ----
mean loss: 145.59
 ---- batch: 040 ----
mean loss: 134.37
 ---- batch: 050 ----
mean loss: 144.75
 ---- batch: 060 ----
mean loss: 142.75
 ---- batch: 070 ----
mean loss: 147.47
 ---- batch: 080 ----
mean loss: 139.58
 ---- batch: 090 ----
mean loss: 152.99
train mean loss: 143.44
epoch train time: 0:00:01.638929
elapsed time: 0:04:31.482295
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 19:50:23.724575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.90
 ---- batch: 020 ----
mean loss: 140.40
 ---- batch: 030 ----
mean loss: 138.49
 ---- batch: 040 ----
mean loss: 141.73
 ---- batch: 050 ----
mean loss: 147.30
 ---- batch: 060 ----
mean loss: 145.44
 ---- batch: 070 ----
mean loss: 140.55
 ---- batch: 080 ----
mean loss: 146.55
 ---- batch: 090 ----
mean loss: 144.43
train mean loss: 142.74
epoch train time: 0:00:01.676370
elapsed time: 0:04:33.159320
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 19:50:25.401651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.15
 ---- batch: 020 ----
mean loss: 146.97
 ---- batch: 030 ----
mean loss: 135.89
 ---- batch: 040 ----
mean loss: 144.81
 ---- batch: 050 ----
mean loss: 139.92
 ---- batch: 060 ----
mean loss: 139.73
 ---- batch: 070 ----
mean loss: 145.31
 ---- batch: 080 ----
mean loss: 140.71
 ---- batch: 090 ----
mean loss: 142.48
train mean loss: 142.33
epoch train time: 0:00:01.667137
elapsed time: 0:04:34.827223
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 19:50:27.069545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.45
 ---- batch: 020 ----
mean loss: 141.20
 ---- batch: 030 ----
mean loss: 142.32
 ---- batch: 040 ----
mean loss: 146.11
 ---- batch: 050 ----
mean loss: 137.93
 ---- batch: 060 ----
mean loss: 144.43
 ---- batch: 070 ----
mean loss: 143.98
 ---- batch: 080 ----
mean loss: 137.83
 ---- batch: 090 ----
mean loss: 146.44
train mean loss: 141.39
epoch train time: 0:00:01.679549
elapsed time: 0:04:36.507495
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 19:50:28.749830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.06
 ---- batch: 020 ----
mean loss: 139.73
 ---- batch: 030 ----
mean loss: 132.34
 ---- batch: 040 ----
mean loss: 141.11
 ---- batch: 050 ----
mean loss: 149.68
 ---- batch: 060 ----
mean loss: 143.71
 ---- batch: 070 ----
mean loss: 136.46
 ---- batch: 080 ----
mean loss: 148.99
 ---- batch: 090 ----
mean loss: 142.60
train mean loss: 140.86
epoch train time: 0:00:01.693024
elapsed time: 0:04:38.201237
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 19:50:30.443553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.13
 ---- batch: 020 ----
mean loss: 135.66
 ---- batch: 030 ----
mean loss: 142.31
 ---- batch: 040 ----
mean loss: 139.65
 ---- batch: 050 ----
mean loss: 144.74
 ---- batch: 060 ----
mean loss: 142.76
 ---- batch: 070 ----
mean loss: 139.65
 ---- batch: 080 ----
mean loss: 140.71
 ---- batch: 090 ----
mean loss: 140.54
train mean loss: 140.93
epoch train time: 0:00:01.674029
elapsed time: 0:04:39.875942
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 19:50:32.118270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.42
 ---- batch: 020 ----
mean loss: 143.21
 ---- batch: 030 ----
mean loss: 134.01
 ---- batch: 040 ----
mean loss: 135.94
 ---- batch: 050 ----
mean loss: 140.66
 ---- batch: 060 ----
mean loss: 138.72
 ---- batch: 070 ----
mean loss: 136.28
 ---- batch: 080 ----
mean loss: 145.48
 ---- batch: 090 ----
mean loss: 147.39
train mean loss: 139.58
epoch train time: 0:00:01.652984
elapsed time: 0:04:41.529715
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 19:50:33.772068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.21
 ---- batch: 020 ----
mean loss: 139.79
 ---- batch: 030 ----
mean loss: 137.35
 ---- batch: 040 ----
mean loss: 142.72
 ---- batch: 050 ----
mean loss: 133.70
 ---- batch: 060 ----
mean loss: 134.28
 ---- batch: 070 ----
mean loss: 144.34
 ---- batch: 080 ----
mean loss: 144.47
 ---- batch: 090 ----
mean loss: 141.00
train mean loss: 139.51
epoch train time: 0:00:01.652475
elapsed time: 0:04:43.183101
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 19:50:35.425207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.21
 ---- batch: 020 ----
mean loss: 133.72
 ---- batch: 030 ----
mean loss: 137.75
 ---- batch: 040 ----
mean loss: 139.09
 ---- batch: 050 ----
mean loss: 139.04
 ---- batch: 060 ----
mean loss: 141.28
 ---- batch: 070 ----
mean loss: 148.65
 ---- batch: 080 ----
mean loss: 137.96
 ---- batch: 090 ----
mean loss: 142.81
train mean loss: 138.88
epoch train time: 0:00:01.657222
elapsed time: 0:04:44.840886
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 19:50:37.083232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.85
 ---- batch: 020 ----
mean loss: 139.33
 ---- batch: 030 ----
mean loss: 133.99
 ---- batch: 040 ----
mean loss: 141.47
 ---- batch: 050 ----
mean loss: 133.97
 ---- batch: 060 ----
mean loss: 139.24
 ---- batch: 070 ----
mean loss: 130.07
 ---- batch: 080 ----
mean loss: 143.37
 ---- batch: 090 ----
mean loss: 141.90
train mean loss: 138.03
epoch train time: 0:00:01.642195
elapsed time: 0:04:46.483824
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 19:50:38.726175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.59
 ---- batch: 020 ----
mean loss: 135.55
 ---- batch: 030 ----
mean loss: 135.45
 ---- batch: 040 ----
mean loss: 137.64
 ---- batch: 050 ----
mean loss: 138.44
 ---- batch: 060 ----
mean loss: 145.01
 ---- batch: 070 ----
mean loss: 136.94
 ---- batch: 080 ----
mean loss: 138.16
 ---- batch: 090 ----
mean loss: 139.01
train mean loss: 138.30
epoch train time: 0:00:01.676445
elapsed time: 0:04:48.161003
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 19:50:40.403367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.88
 ---- batch: 020 ----
mean loss: 137.68
 ---- batch: 030 ----
mean loss: 141.78
 ---- batch: 040 ----
mean loss: 136.78
 ---- batch: 050 ----
mean loss: 135.85
 ---- batch: 060 ----
mean loss: 139.66
 ---- batch: 070 ----
mean loss: 135.81
 ---- batch: 080 ----
mean loss: 139.26
 ---- batch: 090 ----
mean loss: 143.24
train mean loss: 137.64
epoch train time: 0:00:01.665611
elapsed time: 0:04:49.827360
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 19:50:42.069745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.11
 ---- batch: 020 ----
mean loss: 130.33
 ---- batch: 030 ----
mean loss: 139.23
 ---- batch: 040 ----
mean loss: 140.05
 ---- batch: 050 ----
mean loss: 135.35
 ---- batch: 060 ----
mean loss: 131.98
 ---- batch: 070 ----
mean loss: 139.78
 ---- batch: 080 ----
mean loss: 134.25
 ---- batch: 090 ----
mean loss: 137.35
train mean loss: 136.00
epoch train time: 0:00:01.688341
elapsed time: 0:04:51.516442
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 19:50:43.758775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.38
 ---- batch: 020 ----
mean loss: 135.15
 ---- batch: 030 ----
mean loss: 132.00
 ---- batch: 040 ----
mean loss: 134.60
 ---- batch: 050 ----
mean loss: 139.33
 ---- batch: 060 ----
mean loss: 137.96
 ---- batch: 070 ----
mean loss: 137.27
 ---- batch: 080 ----
mean loss: 138.62
 ---- batch: 090 ----
mean loss: 133.85
train mean loss: 135.45
epoch train time: 0:00:01.692547
elapsed time: 0:04:53.209686
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 19:50:45.451972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.00
 ---- batch: 020 ----
mean loss: 132.34
 ---- batch: 030 ----
mean loss: 133.30
 ---- batch: 040 ----
mean loss: 134.16
 ---- batch: 050 ----
mean loss: 135.88
 ---- batch: 060 ----
mean loss: 136.90
 ---- batch: 070 ----
mean loss: 136.10
 ---- batch: 080 ----
mean loss: 138.29
 ---- batch: 090 ----
mean loss: 139.06
train mean loss: 135.37
epoch train time: 0:00:01.668101
elapsed time: 0:04:54.878510
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 19:50:47.120792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.22
 ---- batch: 020 ----
mean loss: 128.83
 ---- batch: 030 ----
mean loss: 140.60
 ---- batch: 040 ----
mean loss: 134.87
 ---- batch: 050 ----
mean loss: 129.35
 ---- batch: 060 ----
mean loss: 136.81
 ---- batch: 070 ----
mean loss: 141.12
 ---- batch: 080 ----
mean loss: 142.81
 ---- batch: 090 ----
mean loss: 135.99
train mean loss: 135.20
epoch train time: 0:00:01.639280
elapsed time: 0:04:56.518471
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 19:50:48.760770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.27
 ---- batch: 020 ----
mean loss: 136.05
 ---- batch: 030 ----
mean loss: 130.64
 ---- batch: 040 ----
mean loss: 136.40
 ---- batch: 050 ----
mean loss: 134.52
 ---- batch: 060 ----
mean loss: 137.58
 ---- batch: 070 ----
mean loss: 129.85
 ---- batch: 080 ----
mean loss: 136.15
 ---- batch: 090 ----
mean loss: 138.82
train mean loss: 134.30
epoch train time: 0:00:01.692041
elapsed time: 0:04:58.211224
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 19:50:50.453511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.95
 ---- batch: 020 ----
mean loss: 127.38
 ---- batch: 030 ----
mean loss: 138.12
 ---- batch: 040 ----
mean loss: 131.30
 ---- batch: 050 ----
mean loss: 134.05
 ---- batch: 060 ----
mean loss: 129.21
 ---- batch: 070 ----
mean loss: 136.26
 ---- batch: 080 ----
mean loss: 137.60
 ---- batch: 090 ----
mean loss: 132.90
train mean loss: 134.02
epoch train time: 0:00:01.715217
elapsed time: 0:04:59.927089
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 19:50:52.169391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.57
 ---- batch: 020 ----
mean loss: 130.93
 ---- batch: 030 ----
mean loss: 132.44
 ---- batch: 040 ----
mean loss: 138.97
 ---- batch: 050 ----
mean loss: 136.59
 ---- batch: 060 ----
mean loss: 136.63
 ---- batch: 070 ----
mean loss: 136.62
 ---- batch: 080 ----
mean loss: 130.60
 ---- batch: 090 ----
mean loss: 136.42
train mean loss: 133.28
epoch train time: 0:00:01.691784
elapsed time: 0:05:01.619678
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 19:50:53.862021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.91
 ---- batch: 020 ----
mean loss: 133.00
 ---- batch: 030 ----
mean loss: 129.15
 ---- batch: 040 ----
mean loss: 133.83
 ---- batch: 050 ----
mean loss: 132.14
 ---- batch: 060 ----
mean loss: 139.18
 ---- batch: 070 ----
mean loss: 136.53
 ---- batch: 080 ----
mean loss: 136.66
 ---- batch: 090 ----
mean loss: 128.40
train mean loss: 132.83
epoch train time: 0:00:01.686699
elapsed time: 0:05:03.307097
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 19:50:55.549391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.77
 ---- batch: 020 ----
mean loss: 127.97
 ---- batch: 030 ----
mean loss: 134.70
 ---- batch: 040 ----
mean loss: 126.82
 ---- batch: 050 ----
mean loss: 124.67
 ---- batch: 060 ----
mean loss: 131.25
 ---- batch: 070 ----
mean loss: 140.24
 ---- batch: 080 ----
mean loss: 132.51
 ---- batch: 090 ----
mean loss: 136.98
train mean loss: 131.98
epoch train time: 0:00:01.669747
elapsed time: 0:05:04.977579
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 19:50:57.219898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.00
 ---- batch: 020 ----
mean loss: 134.45
 ---- batch: 030 ----
mean loss: 136.77
 ---- batch: 040 ----
mean loss: 140.43
 ---- batch: 050 ----
mean loss: 128.55
 ---- batch: 060 ----
mean loss: 136.43
 ---- batch: 070 ----
mean loss: 134.01
 ---- batch: 080 ----
mean loss: 133.93
 ---- batch: 090 ----
mean loss: 126.48
train mean loss: 132.32
epoch train time: 0:00:01.726085
elapsed time: 0:05:06.704320
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 19:50:58.946631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.79
 ---- batch: 020 ----
mean loss: 128.50
 ---- batch: 030 ----
mean loss: 129.53
 ---- batch: 040 ----
mean loss: 129.84
 ---- batch: 050 ----
mean loss: 128.88
 ---- batch: 060 ----
mean loss: 134.44
 ---- batch: 070 ----
mean loss: 132.61
 ---- batch: 080 ----
mean loss: 132.18
 ---- batch: 090 ----
mean loss: 138.00
train mean loss: 131.47
epoch train time: 0:00:01.674089
elapsed time: 0:05:08.379053
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 19:51:00.621351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.88
 ---- batch: 020 ----
mean loss: 126.59
 ---- batch: 030 ----
mean loss: 129.68
 ---- batch: 040 ----
mean loss: 125.25
 ---- batch: 050 ----
mean loss: 132.29
 ---- batch: 060 ----
mean loss: 131.70
 ---- batch: 070 ----
mean loss: 133.58
 ---- batch: 080 ----
mean loss: 130.02
 ---- batch: 090 ----
mean loss: 134.28
train mean loss: 130.90
epoch train time: 0:00:01.665815
elapsed time: 0:05:10.045464
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 19:51:02.287737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.29
 ---- batch: 020 ----
mean loss: 128.48
 ---- batch: 030 ----
mean loss: 126.23
 ---- batch: 040 ----
mean loss: 136.52
 ---- batch: 050 ----
mean loss: 129.29
 ---- batch: 060 ----
mean loss: 134.41
 ---- batch: 070 ----
mean loss: 132.91
 ---- batch: 080 ----
mean loss: 129.80
 ---- batch: 090 ----
mean loss: 135.77
train mean loss: 131.01
epoch train time: 0:00:01.658358
elapsed time: 0:05:11.704618
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 19:51:03.946888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.24
 ---- batch: 020 ----
mean loss: 128.70
 ---- batch: 030 ----
mean loss: 131.85
 ---- batch: 040 ----
mean loss: 128.82
 ---- batch: 050 ----
mean loss: 128.32
 ---- batch: 060 ----
mean loss: 127.41
 ---- batch: 070 ----
mean loss: 122.83
 ---- batch: 080 ----
mean loss: 131.55
 ---- batch: 090 ----
mean loss: 137.42
train mean loss: 130.03
epoch train time: 0:00:01.669254
elapsed time: 0:05:13.374484
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 19:51:05.616794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.12
 ---- batch: 020 ----
mean loss: 123.30
 ---- batch: 030 ----
mean loss: 135.38
 ---- batch: 040 ----
mean loss: 121.69
 ---- batch: 050 ----
mean loss: 128.88
 ---- batch: 060 ----
mean loss: 127.21
 ---- batch: 070 ----
mean loss: 138.41
 ---- batch: 080 ----
mean loss: 130.91
 ---- batch: 090 ----
mean loss: 134.99
train mean loss: 130.17
epoch train time: 0:00:01.662336
elapsed time: 0:05:15.037424
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 19:51:07.279709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.54
 ---- batch: 020 ----
mean loss: 133.36
 ---- batch: 030 ----
mean loss: 126.14
 ---- batch: 040 ----
mean loss: 121.22
 ---- batch: 050 ----
mean loss: 126.09
 ---- batch: 060 ----
mean loss: 132.28
 ---- batch: 070 ----
mean loss: 132.07
 ---- batch: 080 ----
mean loss: 133.04
 ---- batch: 090 ----
mean loss: 130.89
train mean loss: 129.61
epoch train time: 0:00:01.665972
elapsed time: 0:05:16.704165
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 19:51:08.946496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.92
 ---- batch: 020 ----
mean loss: 127.15
 ---- batch: 030 ----
mean loss: 128.74
 ---- batch: 040 ----
mean loss: 131.53
 ---- batch: 050 ----
mean loss: 125.47
 ---- batch: 060 ----
mean loss: 132.55
 ---- batch: 070 ----
mean loss: 128.87
 ---- batch: 080 ----
mean loss: 129.76
 ---- batch: 090 ----
mean loss: 131.44
train mean loss: 128.82
epoch train time: 0:00:01.673153
elapsed time: 0:05:18.377974
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 19:51:10.620267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.08
 ---- batch: 020 ----
mean loss: 130.01
 ---- batch: 030 ----
mean loss: 124.30
 ---- batch: 040 ----
mean loss: 123.89
 ---- batch: 050 ----
mean loss: 137.64
 ---- batch: 060 ----
mean loss: 125.54
 ---- batch: 070 ----
mean loss: 130.82
 ---- batch: 080 ----
mean loss: 126.98
 ---- batch: 090 ----
mean loss: 131.62
train mean loss: 128.25
epoch train time: 0:00:01.684509
elapsed time: 0:05:20.063094
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 19:51:12.305363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.16
 ---- batch: 020 ----
mean loss: 125.95
 ---- batch: 030 ----
mean loss: 127.50
 ---- batch: 040 ----
mean loss: 124.38
 ---- batch: 050 ----
mean loss: 126.65
 ---- batch: 060 ----
mean loss: 128.53
 ---- batch: 070 ----
mean loss: 128.06
 ---- batch: 080 ----
mean loss: 122.61
 ---- batch: 090 ----
mean loss: 138.70
train mean loss: 127.84
epoch train time: 0:00:01.666183
elapsed time: 0:05:21.729939
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 19:51:13.972235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.91
 ---- batch: 020 ----
mean loss: 122.11
 ---- batch: 030 ----
mean loss: 127.80
 ---- batch: 040 ----
mean loss: 130.51
 ---- batch: 050 ----
mean loss: 127.27
 ---- batch: 060 ----
mean loss: 125.49
 ---- batch: 070 ----
mean loss: 126.34
 ---- batch: 080 ----
mean loss: 132.17
 ---- batch: 090 ----
mean loss: 127.27
train mean loss: 127.65
epoch train time: 0:00:01.709778
elapsed time: 0:05:23.440455
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 19:51:15.682743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.86
 ---- batch: 020 ----
mean loss: 121.63
 ---- batch: 030 ----
mean loss: 131.21
 ---- batch: 040 ----
mean loss: 122.46
 ---- batch: 050 ----
mean loss: 126.77
 ---- batch: 060 ----
mean loss: 130.22
 ---- batch: 070 ----
mean loss: 129.80
 ---- batch: 080 ----
mean loss: 126.80
 ---- batch: 090 ----
mean loss: 131.40
train mean loss: 126.80
epoch train time: 0:00:01.672925
elapsed time: 0:05:25.114260
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 19:51:17.356356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.60
 ---- batch: 020 ----
mean loss: 127.61
 ---- batch: 030 ----
mean loss: 126.18
 ---- batch: 040 ----
mean loss: 123.86
 ---- batch: 050 ----
mean loss: 126.56
 ---- batch: 060 ----
mean loss: 128.62
 ---- batch: 070 ----
mean loss: 125.26
 ---- batch: 080 ----
mean loss: 126.68
 ---- batch: 090 ----
mean loss: 128.96
train mean loss: 126.87
epoch train time: 0:00:01.662751
elapsed time: 0:05:26.777488
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 19:51:19.019759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.22
 ---- batch: 020 ----
mean loss: 121.08
 ---- batch: 030 ----
mean loss: 126.99
 ---- batch: 040 ----
mean loss: 126.88
 ---- batch: 050 ----
mean loss: 124.70
 ---- batch: 060 ----
mean loss: 129.82
 ---- batch: 070 ----
mean loss: 130.08
 ---- batch: 080 ----
mean loss: 131.91
 ---- batch: 090 ----
mean loss: 124.80
train mean loss: 126.26
epoch train time: 0:00:01.632169
elapsed time: 0:05:28.410341
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 19:51:20.652644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.98
 ---- batch: 020 ----
mean loss: 125.12
 ---- batch: 030 ----
mean loss: 120.70
 ---- batch: 040 ----
mean loss: 128.47
 ---- batch: 050 ----
mean loss: 128.49
 ---- batch: 060 ----
mean loss: 124.15
 ---- batch: 070 ----
mean loss: 123.45
 ---- batch: 080 ----
mean loss: 122.66
 ---- batch: 090 ----
mean loss: 129.72
train mean loss: 126.13
epoch train time: 0:00:01.672889
elapsed time: 0:05:30.083902
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 19:51:22.326199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.96
 ---- batch: 020 ----
mean loss: 119.24
 ---- batch: 030 ----
mean loss: 126.63
 ---- batch: 040 ----
mean loss: 120.65
 ---- batch: 050 ----
mean loss: 120.97
 ---- batch: 060 ----
mean loss: 129.19
 ---- batch: 070 ----
mean loss: 128.22
 ---- batch: 080 ----
mean loss: 129.72
 ---- batch: 090 ----
mean loss: 123.02
train mean loss: 125.09
epoch train time: 0:00:01.659983
elapsed time: 0:05:31.744525
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 19:51:23.986818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.23
 ---- batch: 020 ----
mean loss: 126.30
 ---- batch: 030 ----
mean loss: 123.17
 ---- batch: 040 ----
mean loss: 123.94
 ---- batch: 050 ----
mean loss: 128.09
 ---- batch: 060 ----
mean loss: 127.08
 ---- batch: 070 ----
mean loss: 130.62
 ---- batch: 080 ----
mean loss: 119.44
 ---- batch: 090 ----
mean loss: 127.17
train mean loss: 124.71
epoch train time: 0:00:01.673703
elapsed time: 0:05:33.418930
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 19:51:25.661211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.54
 ---- batch: 020 ----
mean loss: 119.48
 ---- batch: 030 ----
mean loss: 124.06
 ---- batch: 040 ----
mean loss: 127.38
 ---- batch: 050 ----
mean loss: 122.22
 ---- batch: 060 ----
mean loss: 126.50
 ---- batch: 070 ----
mean loss: 128.15
 ---- batch: 080 ----
mean loss: 125.65
 ---- batch: 090 ----
mean loss: 130.58
train mean loss: 124.96
epoch train time: 0:00:01.666637
elapsed time: 0:05:35.086222
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 19:51:27.328505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.65
 ---- batch: 020 ----
mean loss: 119.92
 ---- batch: 030 ----
mean loss: 123.31
 ---- batch: 040 ----
mean loss: 125.31
 ---- batch: 050 ----
mean loss: 125.27
 ---- batch: 060 ----
mean loss: 119.77
 ---- batch: 070 ----
mean loss: 124.46
 ---- batch: 080 ----
mean loss: 126.55
 ---- batch: 090 ----
mean loss: 128.96
train mean loss: 124.37
epoch train time: 0:00:01.666643
elapsed time: 0:05:36.753522
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 19:51:28.995792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.04
 ---- batch: 020 ----
mean loss: 121.35
 ---- batch: 030 ----
mean loss: 125.16
 ---- batch: 040 ----
mean loss: 123.33
 ---- batch: 050 ----
mean loss: 122.02
 ---- batch: 060 ----
mean loss: 124.23
 ---- batch: 070 ----
mean loss: 121.80
 ---- batch: 080 ----
mean loss: 126.33
 ---- batch: 090 ----
mean loss: 128.03
train mean loss: 123.94
epoch train time: 0:00:01.661517
elapsed time: 0:05:38.415686
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 19:51:30.658059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.53
 ---- batch: 020 ----
mean loss: 119.17
 ---- batch: 030 ----
mean loss: 127.22
 ---- batch: 040 ----
mean loss: 126.00
 ---- batch: 050 ----
mean loss: 123.01
 ---- batch: 060 ----
mean loss: 124.42
 ---- batch: 070 ----
mean loss: 121.16
 ---- batch: 080 ----
mean loss: 122.26
 ---- batch: 090 ----
mean loss: 119.54
train mean loss: 123.07
epoch train time: 0:00:01.653862
elapsed time: 0:05:40.070219
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 19:51:32.312527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.27
 ---- batch: 020 ----
mean loss: 122.60
 ---- batch: 030 ----
mean loss: 121.48
 ---- batch: 040 ----
mean loss: 123.88
 ---- batch: 050 ----
mean loss: 122.77
 ---- batch: 060 ----
mean loss: 124.90
 ---- batch: 070 ----
mean loss: 121.37
 ---- batch: 080 ----
mean loss: 120.71
 ---- batch: 090 ----
mean loss: 122.91
train mean loss: 122.68
epoch train time: 0:00:01.692298
elapsed time: 0:05:41.763176
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 19:51:34.005442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.04
 ---- batch: 020 ----
mean loss: 125.18
 ---- batch: 030 ----
mean loss: 115.93
 ---- batch: 040 ----
mean loss: 123.72
 ---- batch: 050 ----
mean loss: 122.34
 ---- batch: 060 ----
mean loss: 129.77
 ---- batch: 070 ----
mean loss: 121.10
 ---- batch: 080 ----
mean loss: 121.16
 ---- batch: 090 ----
mean loss: 123.85
train mean loss: 122.75
epoch train time: 0:00:01.647114
elapsed time: 0:05:43.410929
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 19:51:35.653210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.11
 ---- batch: 020 ----
mean loss: 116.88
 ---- batch: 030 ----
mean loss: 116.66
 ---- batch: 040 ----
mean loss: 122.02
 ---- batch: 050 ----
mean loss: 130.02
 ---- batch: 060 ----
mean loss: 127.87
 ---- batch: 070 ----
mean loss: 123.67
 ---- batch: 080 ----
mean loss: 123.77
 ---- batch: 090 ----
mean loss: 127.67
train mean loss: 122.47
epoch train time: 0:00:01.657914
elapsed time: 0:05:45.069476
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 19:51:37.311752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.19
 ---- batch: 020 ----
mean loss: 117.02
 ---- batch: 030 ----
mean loss: 120.11
 ---- batch: 040 ----
mean loss: 123.84
 ---- batch: 050 ----
mean loss: 121.22
 ---- batch: 060 ----
mean loss: 121.59
 ---- batch: 070 ----
mean loss: 129.28
 ---- batch: 080 ----
mean loss: 115.32
 ---- batch: 090 ----
mean loss: 128.72
train mean loss: 121.83
epoch train time: 0:00:01.683670
elapsed time: 0:05:46.753784
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 19:51:38.996068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.99
 ---- batch: 020 ----
mean loss: 120.79
 ---- batch: 030 ----
mean loss: 121.94
 ---- batch: 040 ----
mean loss: 118.61
 ---- batch: 050 ----
mean loss: 118.77
 ---- batch: 060 ----
mean loss: 125.27
 ---- batch: 070 ----
mean loss: 119.90
 ---- batch: 080 ----
mean loss: 122.03
 ---- batch: 090 ----
mean loss: 121.75
train mean loss: 121.62
epoch train time: 0:00:01.692263
elapsed time: 0:05:48.446739
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 19:51:40.689030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.41
 ---- batch: 020 ----
mean loss: 118.85
 ---- batch: 030 ----
mean loss: 117.51
 ---- batch: 040 ----
mean loss: 118.82
 ---- batch: 050 ----
mean loss: 122.29
 ---- batch: 060 ----
mean loss: 121.00
 ---- batch: 070 ----
mean loss: 121.46
 ---- batch: 080 ----
mean loss: 125.74
 ---- batch: 090 ----
mean loss: 122.01
train mean loss: 120.91
epoch train time: 0:00:01.679060
elapsed time: 0:05:50.126446
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 19:51:42.368720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.75
 ---- batch: 020 ----
mean loss: 113.29
 ---- batch: 030 ----
mean loss: 125.11
 ---- batch: 040 ----
mean loss: 116.67
 ---- batch: 050 ----
mean loss: 121.65
 ---- batch: 060 ----
mean loss: 121.11
 ---- batch: 070 ----
mean loss: 119.50
 ---- batch: 080 ----
mean loss: 127.88
 ---- batch: 090 ----
mean loss: 126.46
train mean loss: 120.66
epoch train time: 0:00:01.644797
elapsed time: 0:05:51.771873
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 19:51:44.014224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.41
 ---- batch: 020 ----
mean loss: 119.62
 ---- batch: 030 ----
mean loss: 120.06
 ---- batch: 040 ----
mean loss: 117.44
 ---- batch: 050 ----
mean loss: 120.95
 ---- batch: 060 ----
mean loss: 124.76
 ---- batch: 070 ----
mean loss: 127.28
 ---- batch: 080 ----
mean loss: 125.51
 ---- batch: 090 ----
mean loss: 115.46
train mean loss: 120.61
epoch train time: 0:00:01.620534
elapsed time: 0:05:53.393173
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 19:51:45.635564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.29
 ---- batch: 020 ----
mean loss: 116.25
 ---- batch: 030 ----
mean loss: 124.12
 ---- batch: 040 ----
mean loss: 118.84
 ---- batch: 050 ----
mean loss: 122.26
 ---- batch: 060 ----
mean loss: 118.82
 ---- batch: 070 ----
mean loss: 122.37
 ---- batch: 080 ----
mean loss: 125.95
 ---- batch: 090 ----
mean loss: 119.29
train mean loss: 120.54
epoch train time: 0:00:01.659353
elapsed time: 0:05:55.053267
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 19:51:47.295581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.10
 ---- batch: 020 ----
mean loss: 119.26
 ---- batch: 030 ----
mean loss: 115.68
 ---- batch: 040 ----
mean loss: 120.87
 ---- batch: 050 ----
mean loss: 120.90
 ---- batch: 060 ----
mean loss: 122.10
 ---- batch: 070 ----
mean loss: 121.91
 ---- batch: 080 ----
mean loss: 114.41
 ---- batch: 090 ----
mean loss: 121.78
train mean loss: 119.38
epoch train time: 0:00:01.678534
elapsed time: 0:05:56.732548
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 19:51:48.974836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.31
 ---- batch: 020 ----
mean loss: 113.24
 ---- batch: 030 ----
mean loss: 116.72
 ---- batch: 040 ----
mean loss: 117.19
 ---- batch: 050 ----
mean loss: 121.83
 ---- batch: 060 ----
mean loss: 115.86
 ---- batch: 070 ----
mean loss: 123.47
 ---- batch: 080 ----
mean loss: 119.34
 ---- batch: 090 ----
mean loss: 126.87
train mean loss: 119.04
epoch train time: 0:00:01.652986
elapsed time: 0:05:58.386155
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 19:51:50.628430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.96
 ---- batch: 020 ----
mean loss: 116.98
 ---- batch: 030 ----
mean loss: 115.95
 ---- batch: 040 ----
mean loss: 119.86
 ---- batch: 050 ----
mean loss: 114.90
 ---- batch: 060 ----
mean loss: 117.64
 ---- batch: 070 ----
mean loss: 117.90
 ---- batch: 080 ----
mean loss: 122.38
 ---- batch: 090 ----
mean loss: 121.37
train mean loss: 118.79
epoch train time: 0:00:01.669292
elapsed time: 0:06:00.056110
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 19:51:52.298529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.76
 ---- batch: 020 ----
mean loss: 119.84
 ---- batch: 030 ----
mean loss: 117.40
 ---- batch: 040 ----
mean loss: 114.92
 ---- batch: 050 ----
mean loss: 117.60
 ---- batch: 060 ----
mean loss: 120.00
 ---- batch: 070 ----
mean loss: 121.21
 ---- batch: 080 ----
mean loss: 122.98
 ---- batch: 090 ----
mean loss: 123.51
train mean loss: 118.99
epoch train time: 0:00:01.673560
elapsed time: 0:06:01.730558
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 19:51:53.972916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.72
 ---- batch: 020 ----
mean loss: 111.37
 ---- batch: 030 ----
mean loss: 118.34
 ---- batch: 040 ----
mean loss: 117.72
 ---- batch: 050 ----
mean loss: 116.76
 ---- batch: 060 ----
mean loss: 120.18
 ---- batch: 070 ----
mean loss: 122.13
 ---- batch: 080 ----
mean loss: 114.21
 ---- batch: 090 ----
mean loss: 122.89
train mean loss: 118.25
epoch train time: 0:00:01.670191
elapsed time: 0:06:03.401474
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 19:51:55.643762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.57
 ---- batch: 020 ----
mean loss: 113.33
 ---- batch: 030 ----
mean loss: 118.83
 ---- batch: 040 ----
mean loss: 114.98
 ---- batch: 050 ----
mean loss: 121.41
 ---- batch: 060 ----
mean loss: 118.85
 ---- batch: 070 ----
mean loss: 117.02
 ---- batch: 080 ----
mean loss: 118.10
 ---- batch: 090 ----
mean loss: 124.64
train mean loss: 118.18
epoch train time: 0:00:01.664415
elapsed time: 0:06:05.066487
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 19:51:57.308772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.54
 ---- batch: 020 ----
mean loss: 109.30
 ---- batch: 030 ----
mean loss: 118.85
 ---- batch: 040 ----
mean loss: 119.57
 ---- batch: 050 ----
mean loss: 118.61
 ---- batch: 060 ----
mean loss: 118.68
 ---- batch: 070 ----
mean loss: 120.63
 ---- batch: 080 ----
mean loss: 125.30
 ---- batch: 090 ----
mean loss: 120.24
train mean loss: 117.93
epoch train time: 0:00:01.667753
elapsed time: 0:06:06.734927
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 19:51:58.977226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.57
 ---- batch: 020 ----
mean loss: 114.84
 ---- batch: 030 ----
mean loss: 114.74
 ---- batch: 040 ----
mean loss: 118.21
 ---- batch: 050 ----
mean loss: 121.38
 ---- batch: 060 ----
mean loss: 119.76
 ---- batch: 070 ----
mean loss: 115.36
 ---- batch: 080 ----
mean loss: 119.05
 ---- batch: 090 ----
mean loss: 118.83
train mean loss: 117.65
epoch train time: 0:00:01.683889
elapsed time: 0:06:08.419542
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 19:52:00.661903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.21
 ---- batch: 020 ----
mean loss: 121.24
 ---- batch: 030 ----
mean loss: 112.90
 ---- batch: 040 ----
mean loss: 116.43
 ---- batch: 050 ----
mean loss: 114.65
 ---- batch: 060 ----
mean loss: 113.30
 ---- batch: 070 ----
mean loss: 122.74
 ---- batch: 080 ----
mean loss: 124.99
 ---- batch: 090 ----
mean loss: 120.66
train mean loss: 117.78
epoch train time: 0:00:01.668872
elapsed time: 0:06:10.089302
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 19:52:02.331593
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.92
 ---- batch: 020 ----
mean loss: 109.60
 ---- batch: 030 ----
mean loss: 109.56
 ---- batch: 040 ----
mean loss: 115.61
 ---- batch: 050 ----
mean loss: 112.69
 ---- batch: 060 ----
mean loss: 106.71
 ---- batch: 070 ----
mean loss: 111.10
 ---- batch: 080 ----
mean loss: 114.14
 ---- batch: 090 ----
mean loss: 111.33
train mean loss: 111.89
epoch train time: 0:00:01.670944
elapsed time: 0:06:11.761194
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 19:52:04.003237
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.96
 ---- batch: 020 ----
mean loss: 105.69
 ---- batch: 030 ----
mean loss: 111.79
 ---- batch: 040 ----
mean loss: 109.30
 ---- batch: 050 ----
mean loss: 111.78
 ---- batch: 060 ----
mean loss: 116.11
 ---- batch: 070 ----
mean loss: 108.81
 ---- batch: 080 ----
mean loss: 110.81
 ---- batch: 090 ----
mean loss: 107.02
train mean loss: 110.85
epoch train time: 0:00:01.652683
elapsed time: 0:06:13.414287
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 19:52:05.656645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.18
 ---- batch: 020 ----
mean loss: 112.59
 ---- batch: 030 ----
mean loss: 108.59
 ---- batch: 040 ----
mean loss: 110.91
 ---- batch: 050 ----
mean loss: 109.52
 ---- batch: 060 ----
mean loss: 103.59
 ---- batch: 070 ----
mean loss: 107.46
 ---- batch: 080 ----
mean loss: 113.97
 ---- batch: 090 ----
mean loss: 114.55
train mean loss: 110.49
epoch train time: 0:00:01.650674
elapsed time: 0:06:15.065696
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 19:52:07.307978
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.32
 ---- batch: 020 ----
mean loss: 107.47
 ---- batch: 030 ----
mean loss: 111.58
 ---- batch: 040 ----
mean loss: 109.03
 ---- batch: 050 ----
mean loss: 110.94
 ---- batch: 060 ----
mean loss: 111.34
 ---- batch: 070 ----
mean loss: 108.27
 ---- batch: 080 ----
mean loss: 114.96
 ---- batch: 090 ----
mean loss: 108.04
train mean loss: 110.27
epoch train time: 0:00:01.669932
elapsed time: 0:06:16.736258
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 19:52:08.978561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.43
 ---- batch: 020 ----
mean loss: 111.71
 ---- batch: 030 ----
mean loss: 110.56
 ---- batch: 040 ----
mean loss: 104.03
 ---- batch: 050 ----
mean loss: 111.28
 ---- batch: 060 ----
mean loss: 111.96
 ---- batch: 070 ----
mean loss: 108.82
 ---- batch: 080 ----
mean loss: 113.33
 ---- batch: 090 ----
mean loss: 113.69
train mean loss: 110.39
epoch train time: 0:00:01.673181
elapsed time: 0:06:18.410166
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 19:52:10.652504
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.32
 ---- batch: 020 ----
mean loss: 103.98
 ---- batch: 030 ----
mean loss: 112.40
 ---- batch: 040 ----
mean loss: 111.34
 ---- batch: 050 ----
mean loss: 110.59
 ---- batch: 060 ----
mean loss: 111.11
 ---- batch: 070 ----
mean loss: 112.80
 ---- batch: 080 ----
mean loss: 108.92
 ---- batch: 090 ----
mean loss: 110.42
train mean loss: 110.18
epoch train time: 0:00:01.675298
elapsed time: 0:06:20.086197
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 19:52:12.328483
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.70
 ---- batch: 020 ----
mean loss: 109.06
 ---- batch: 030 ----
mean loss: 109.42
 ---- batch: 040 ----
mean loss: 112.01
 ---- batch: 050 ----
mean loss: 112.06
 ---- batch: 060 ----
mean loss: 108.69
 ---- batch: 070 ----
mean loss: 107.45
 ---- batch: 080 ----
mean loss: 109.91
 ---- batch: 090 ----
mean loss: 107.51
train mean loss: 110.11
epoch train time: 0:00:01.659679
elapsed time: 0:06:21.746568
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 19:52:13.988863
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.09
 ---- batch: 020 ----
mean loss: 113.41
 ---- batch: 030 ----
mean loss: 109.79
 ---- batch: 040 ----
mean loss: 108.84
 ---- batch: 050 ----
mean loss: 111.07
 ---- batch: 060 ----
mean loss: 104.31
 ---- batch: 070 ----
mean loss: 110.79
 ---- batch: 080 ----
mean loss: 112.78
 ---- batch: 090 ----
mean loss: 107.00
train mean loss: 109.89
epoch train time: 0:00:01.639818
elapsed time: 0:06:23.387087
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 19:52:15.629359
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.74
 ---- batch: 020 ----
mean loss: 111.78
 ---- batch: 030 ----
mean loss: 107.34
 ---- batch: 040 ----
mean loss: 108.07
 ---- batch: 050 ----
mean loss: 111.75
 ---- batch: 060 ----
mean loss: 111.00
 ---- batch: 070 ----
mean loss: 109.31
 ---- batch: 080 ----
mean loss: 106.94
 ---- batch: 090 ----
mean loss: 109.94
train mean loss: 109.95
epoch train time: 0:00:01.676543
elapsed time: 0:06:25.064281
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 19:52:17.306570
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.28
 ---- batch: 020 ----
mean loss: 107.36
 ---- batch: 030 ----
mean loss: 110.85
 ---- batch: 040 ----
mean loss: 115.67
 ---- batch: 050 ----
mean loss: 108.74
 ---- batch: 060 ----
mean loss: 112.47
 ---- batch: 070 ----
mean loss: 108.63
 ---- batch: 080 ----
mean loss: 112.55
 ---- batch: 090 ----
mean loss: 106.30
train mean loss: 109.84
epoch train time: 0:00:01.636201
elapsed time: 0:06:26.701199
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 19:52:18.943516
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.47
 ---- batch: 020 ----
mean loss: 107.56
 ---- batch: 030 ----
mean loss: 109.94
 ---- batch: 040 ----
mean loss: 113.76
 ---- batch: 050 ----
mean loss: 110.62
 ---- batch: 060 ----
mean loss: 109.21
 ---- batch: 070 ----
mean loss: 109.89
 ---- batch: 080 ----
mean loss: 107.31
 ---- batch: 090 ----
mean loss: 109.84
train mean loss: 109.77
epoch train time: 0:00:01.647362
elapsed time: 0:06:28.349212
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 19:52:20.591503
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.36
 ---- batch: 020 ----
mean loss: 108.65
 ---- batch: 030 ----
mean loss: 107.57
 ---- batch: 040 ----
mean loss: 111.27
 ---- batch: 050 ----
mean loss: 110.28
 ---- batch: 060 ----
mean loss: 105.97
 ---- batch: 070 ----
mean loss: 111.27
 ---- batch: 080 ----
mean loss: 109.23
 ---- batch: 090 ----
mean loss: 109.25
train mean loss: 109.85
epoch train time: 0:00:01.655110
elapsed time: 0:06:30.004940
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 19:52:22.247226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.77
 ---- batch: 020 ----
mean loss: 106.19
 ---- batch: 030 ----
mean loss: 106.67
 ---- batch: 040 ----
mean loss: 106.86
 ---- batch: 050 ----
mean loss: 119.05
 ---- batch: 060 ----
mean loss: 112.41
 ---- batch: 070 ----
mean loss: 108.08
 ---- batch: 080 ----
mean loss: 111.72
 ---- batch: 090 ----
mean loss: 109.00
train mean loss: 109.62
epoch train time: 0:00:01.671124
elapsed time: 0:06:31.676723
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 19:52:23.919000
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.76
 ---- batch: 020 ----
mean loss: 100.93
 ---- batch: 030 ----
mean loss: 107.29
 ---- batch: 040 ----
mean loss: 110.13
 ---- batch: 050 ----
mean loss: 106.25
 ---- batch: 060 ----
mean loss: 113.69
 ---- batch: 070 ----
mean loss: 111.17
 ---- batch: 080 ----
mean loss: 114.97
 ---- batch: 090 ----
mean loss: 112.89
train mean loss: 109.89
epoch train time: 0:00:01.637263
elapsed time: 0:06:33.314658
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 19:52:25.556962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.01
 ---- batch: 020 ----
mean loss: 111.79
 ---- batch: 030 ----
mean loss: 106.79
 ---- batch: 040 ----
mean loss: 106.96
 ---- batch: 050 ----
mean loss: 109.18
 ---- batch: 060 ----
mean loss: 106.41
 ---- batch: 070 ----
mean loss: 106.08
 ---- batch: 080 ----
mean loss: 115.37
 ---- batch: 090 ----
mean loss: 109.03
train mean loss: 109.50
epoch train time: 0:00:01.631226
elapsed time: 0:06:34.946536
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 19:52:27.188869
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.98
 ---- batch: 020 ----
mean loss: 109.40
 ---- batch: 030 ----
mean loss: 111.28
 ---- batch: 040 ----
mean loss: 113.15
 ---- batch: 050 ----
mean loss: 108.63
 ---- batch: 060 ----
mean loss: 103.90
 ---- batch: 070 ----
mean loss: 111.80
 ---- batch: 080 ----
mean loss: 111.08
 ---- batch: 090 ----
mean loss: 105.04
train mean loss: 109.42
epoch train time: 0:00:01.647208
elapsed time: 0:06:36.594499
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 19:52:28.836825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.31
 ---- batch: 020 ----
mean loss: 106.34
 ---- batch: 030 ----
mean loss: 111.71
 ---- batch: 040 ----
mean loss: 106.75
 ---- batch: 050 ----
mean loss: 109.76
 ---- batch: 060 ----
mean loss: 110.81
 ---- batch: 070 ----
mean loss: 108.78
 ---- batch: 080 ----
mean loss: 113.32
 ---- batch: 090 ----
mean loss: 113.64
train mean loss: 109.45
epoch train time: 0:00:01.657773
elapsed time: 0:06:38.252992
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 19:52:30.495341
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.98
 ---- batch: 020 ----
mean loss: 107.96
 ---- batch: 030 ----
mean loss: 109.66
 ---- batch: 040 ----
mean loss: 107.77
 ---- batch: 050 ----
mean loss: 109.25
 ---- batch: 060 ----
mean loss: 111.33
 ---- batch: 070 ----
mean loss: 102.57
 ---- batch: 080 ----
mean loss: 114.17
 ---- batch: 090 ----
mean loss: 118.15
train mean loss: 109.38
epoch train time: 0:00:01.692973
elapsed time: 0:06:39.946617
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 19:52:32.188886
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.24
 ---- batch: 020 ----
mean loss: 106.98
 ---- batch: 030 ----
mean loss: 117.04
 ---- batch: 040 ----
mean loss: 115.69
 ---- batch: 050 ----
mean loss: 110.67
 ---- batch: 060 ----
mean loss: 109.44
 ---- batch: 070 ----
mean loss: 107.80
 ---- batch: 080 ----
mean loss: 109.51
 ---- batch: 090 ----
mean loss: 105.57
train mean loss: 109.54
epoch train time: 0:00:01.669218
elapsed time: 0:06:41.616487
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 19:52:33.858762
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.94
 ---- batch: 020 ----
mean loss: 107.24
 ---- batch: 030 ----
mean loss: 107.03
 ---- batch: 040 ----
mean loss: 111.51
 ---- batch: 050 ----
mean loss: 109.44
 ---- batch: 060 ----
mean loss: 114.75
 ---- batch: 070 ----
mean loss: 105.19
 ---- batch: 080 ----
mean loss: 114.59
 ---- batch: 090 ----
mean loss: 105.84
train mean loss: 109.20
epoch train time: 0:00:01.659652
elapsed time: 0:06:43.276833
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 19:52:35.519156
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.05
 ---- batch: 020 ----
mean loss: 104.99
 ---- batch: 030 ----
mean loss: 111.76
 ---- batch: 040 ----
mean loss: 111.51
 ---- batch: 050 ----
mean loss: 114.44
 ---- batch: 060 ----
mean loss: 109.70
 ---- batch: 070 ----
mean loss: 104.84
 ---- batch: 080 ----
mean loss: 112.81
 ---- batch: 090 ----
mean loss: 108.07
train mean loss: 109.45
epoch train time: 0:00:01.677043
elapsed time: 0:06:44.954539
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 19:52:37.196842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.28
 ---- batch: 020 ----
mean loss: 107.37
 ---- batch: 030 ----
mean loss: 108.91
 ---- batch: 040 ----
mean loss: 106.65
 ---- batch: 050 ----
mean loss: 106.28
 ---- batch: 060 ----
mean loss: 115.44
 ---- batch: 070 ----
mean loss: 112.47
 ---- batch: 080 ----
mean loss: 113.69
 ---- batch: 090 ----
mean loss: 110.97
train mean loss: 109.39
epoch train time: 0:00:01.648096
elapsed time: 0:06:46.603316
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 19:52:38.845421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.01
 ---- batch: 020 ----
mean loss: 109.03
 ---- batch: 030 ----
mean loss: 116.58
 ---- batch: 040 ----
mean loss: 106.49
 ---- batch: 050 ----
mean loss: 107.58
 ---- batch: 060 ----
mean loss: 106.51
 ---- batch: 070 ----
mean loss: 112.04
 ---- batch: 080 ----
mean loss: 107.69
 ---- batch: 090 ----
mean loss: 111.88
train mean loss: 109.28
epoch train time: 0:00:01.632846
elapsed time: 0:06:48.236604
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 19:52:40.478878
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.35
 ---- batch: 020 ----
mean loss: 98.42
 ---- batch: 030 ----
mean loss: 108.13
 ---- batch: 040 ----
mean loss: 107.61
 ---- batch: 050 ----
mean loss: 105.38
 ---- batch: 060 ----
mean loss: 109.22
 ---- batch: 070 ----
mean loss: 108.88
 ---- batch: 080 ----
mean loss: 116.29
 ---- batch: 090 ----
mean loss: 116.82
train mean loss: 109.15
epoch train time: 0:00:01.640448
elapsed time: 0:06:49.877654
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 19:52:42.119923
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.54
 ---- batch: 020 ----
mean loss: 115.40
 ---- batch: 030 ----
mean loss: 104.87
 ---- batch: 040 ----
mean loss: 110.50
 ---- batch: 050 ----
mean loss: 109.59
 ---- batch: 060 ----
mean loss: 110.19
 ---- batch: 070 ----
mean loss: 106.49
 ---- batch: 080 ----
mean loss: 105.41
 ---- batch: 090 ----
mean loss: 108.09
train mean loss: 109.20
epoch train time: 0:00:01.635312
elapsed time: 0:06:51.513573
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 19:52:43.755846
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.00
 ---- batch: 020 ----
mean loss: 110.85
 ---- batch: 030 ----
mean loss: 110.90
 ---- batch: 040 ----
mean loss: 107.10
 ---- batch: 050 ----
mean loss: 106.89
 ---- batch: 060 ----
mean loss: 111.63
 ---- batch: 070 ----
mean loss: 106.08
 ---- batch: 080 ----
mean loss: 112.50
 ---- batch: 090 ----
mean loss: 108.00
train mean loss: 108.86
epoch train time: 0:00:01.679067
elapsed time: 0:06:53.193288
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 19:52:45.435576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.65
 ---- batch: 020 ----
mean loss: 117.18
 ---- batch: 030 ----
mean loss: 111.03
 ---- batch: 040 ----
mean loss: 109.67
 ---- batch: 050 ----
mean loss: 112.31
 ---- batch: 060 ----
mean loss: 109.10
 ---- batch: 070 ----
mean loss: 105.69
 ---- batch: 080 ----
mean loss: 111.25
 ---- batch: 090 ----
mean loss: 101.73
train mean loss: 109.06
epoch train time: 0:00:01.663986
elapsed time: 0:06:54.857869
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 19:52:47.100173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.84
 ---- batch: 020 ----
mean loss: 108.73
 ---- batch: 030 ----
mean loss: 112.09
 ---- batch: 040 ----
mean loss: 109.54
 ---- batch: 050 ----
mean loss: 112.41
 ---- batch: 060 ----
mean loss: 108.75
 ---- batch: 070 ----
mean loss: 113.43
 ---- batch: 080 ----
mean loss: 105.71
 ---- batch: 090 ----
mean loss: 107.48
train mean loss: 108.96
epoch train time: 0:00:01.655189
elapsed time: 0:06:56.513734
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 19:52:48.756040
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.24
 ---- batch: 020 ----
mean loss: 109.45
 ---- batch: 030 ----
mean loss: 109.16
 ---- batch: 040 ----
mean loss: 109.12
 ---- batch: 050 ----
mean loss: 113.77
 ---- batch: 060 ----
mean loss: 107.87
 ---- batch: 070 ----
mean loss: 107.83
 ---- batch: 080 ----
mean loss: 104.42
 ---- batch: 090 ----
mean loss: 112.39
train mean loss: 108.90
epoch train time: 0:00:01.647260
elapsed time: 0:06:58.161683
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 19:52:50.403987
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.78
 ---- batch: 020 ----
mean loss: 110.50
 ---- batch: 030 ----
mean loss: 110.26
 ---- batch: 040 ----
mean loss: 105.92
 ---- batch: 050 ----
mean loss: 104.02
 ---- batch: 060 ----
mean loss: 109.28
 ---- batch: 070 ----
mean loss: 112.31
 ---- batch: 080 ----
mean loss: 110.67
 ---- batch: 090 ----
mean loss: 103.21
train mean loss: 108.98
epoch train time: 0:00:01.640047
elapsed time: 0:06:59.802423
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 19:52:52.044701
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.13
 ---- batch: 020 ----
mean loss: 114.09
 ---- batch: 030 ----
mean loss: 111.13
 ---- batch: 040 ----
mean loss: 110.46
 ---- batch: 050 ----
mean loss: 109.46
 ---- batch: 060 ----
mean loss: 104.06
 ---- batch: 070 ----
mean loss: 105.70
 ---- batch: 080 ----
mean loss: 106.94
 ---- batch: 090 ----
mean loss: 106.49
train mean loss: 108.74
epoch train time: 0:00:01.660784
elapsed time: 0:07:01.463933
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 19:52:53.706280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.67
 ---- batch: 020 ----
mean loss: 102.49
 ---- batch: 030 ----
mean loss: 108.28
 ---- batch: 040 ----
mean loss: 110.30
 ---- batch: 050 ----
mean loss: 110.61
 ---- batch: 060 ----
mean loss: 109.34
 ---- batch: 070 ----
mean loss: 114.19
 ---- batch: 080 ----
mean loss: 110.37
 ---- batch: 090 ----
mean loss: 108.17
train mean loss: 108.72
epoch train time: 0:00:01.664772
elapsed time: 0:07:03.129406
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 19:52:55.371725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.19
 ---- batch: 020 ----
mean loss: 105.37
 ---- batch: 030 ----
mean loss: 105.66
 ---- batch: 040 ----
mean loss: 113.30
 ---- batch: 050 ----
mean loss: 109.24
 ---- batch: 060 ----
mean loss: 106.48
 ---- batch: 070 ----
mean loss: 104.39
 ---- batch: 080 ----
mean loss: 113.20
 ---- batch: 090 ----
mean loss: 109.01
train mean loss: 108.97
epoch train time: 0:00:01.630509
elapsed time: 0:07:04.760886
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 19:52:57.002930
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.50
 ---- batch: 020 ----
mean loss: 108.47
 ---- batch: 030 ----
mean loss: 108.93
 ---- batch: 040 ----
mean loss: 110.43
 ---- batch: 050 ----
mean loss: 109.85
 ---- batch: 060 ----
mean loss: 109.16
 ---- batch: 070 ----
mean loss: 109.23
 ---- batch: 080 ----
mean loss: 109.01
 ---- batch: 090 ----
mean loss: 111.59
train mean loss: 108.74
epoch train time: 0:00:01.626100
elapsed time: 0:07:06.387374
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 19:52:58.629668
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.80
 ---- batch: 020 ----
mean loss: 110.06
 ---- batch: 030 ----
mean loss: 104.32
 ---- batch: 040 ----
mean loss: 104.96
 ---- batch: 050 ----
mean loss: 109.70
 ---- batch: 060 ----
mean loss: 108.49
 ---- batch: 070 ----
mean loss: 109.12
 ---- batch: 080 ----
mean loss: 109.61
 ---- batch: 090 ----
mean loss: 110.26
train mean loss: 108.51
epoch train time: 0:00:01.657879
elapsed time: 0:07:08.045845
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 19:53:00.288162
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.51
 ---- batch: 020 ----
mean loss: 110.61
 ---- batch: 030 ----
mean loss: 112.27
 ---- batch: 040 ----
mean loss: 109.06
 ---- batch: 050 ----
mean loss: 108.79
 ---- batch: 060 ----
mean loss: 103.32
 ---- batch: 070 ----
mean loss: 106.86
 ---- batch: 080 ----
mean loss: 104.22
 ---- batch: 090 ----
mean loss: 109.43
train mean loss: 108.54
epoch train time: 0:00:01.666905
elapsed time: 0:07:09.713587
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 19:53:01.955946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.62
 ---- batch: 020 ----
mean loss: 107.75
 ---- batch: 030 ----
mean loss: 118.75
 ---- batch: 040 ----
mean loss: 105.68
 ---- batch: 050 ----
mean loss: 108.42
 ---- batch: 060 ----
mean loss: 104.48
 ---- batch: 070 ----
mean loss: 107.89
 ---- batch: 080 ----
mean loss: 108.95
 ---- batch: 090 ----
mean loss: 107.08
train mean loss: 108.69
epoch train time: 0:00:01.668017
elapsed time: 0:07:11.382299
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 19:53:03.624583
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.66
 ---- batch: 020 ----
mean loss: 110.32
 ---- batch: 030 ----
mean loss: 104.35
 ---- batch: 040 ----
mean loss: 104.46
 ---- batch: 050 ----
mean loss: 108.38
 ---- batch: 060 ----
mean loss: 109.55
 ---- batch: 070 ----
mean loss: 112.43
 ---- batch: 080 ----
mean loss: 111.01
 ---- batch: 090 ----
mean loss: 106.50
train mean loss: 108.55
epoch train time: 0:00:01.659328
elapsed time: 0:07:13.042375
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 19:53:05.284691
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.61
 ---- batch: 020 ----
mean loss: 111.05
 ---- batch: 030 ----
mean loss: 107.96
 ---- batch: 040 ----
mean loss: 102.73
 ---- batch: 050 ----
mean loss: 108.37
 ---- batch: 060 ----
mean loss: 106.18
 ---- batch: 070 ----
mean loss: 107.66
 ---- batch: 080 ----
mean loss: 111.06
 ---- batch: 090 ----
mean loss: 112.04
train mean loss: 108.68
epoch train time: 0:00:01.673982
elapsed time: 0:07:14.717120
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 19:53:06.959387
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.21
 ---- batch: 020 ----
mean loss: 110.51
 ---- batch: 030 ----
mean loss: 106.25
 ---- batch: 040 ----
mean loss: 103.66
 ---- batch: 050 ----
mean loss: 103.07
 ---- batch: 060 ----
mean loss: 107.88
 ---- batch: 070 ----
mean loss: 110.46
 ---- batch: 080 ----
mean loss: 107.63
 ---- batch: 090 ----
mean loss: 109.57
train mean loss: 108.33
epoch train time: 0:00:01.645927
elapsed time: 0:07:16.363668
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 19:53:08.605946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.46
 ---- batch: 020 ----
mean loss: 107.85
 ---- batch: 030 ----
mean loss: 114.49
 ---- batch: 040 ----
mean loss: 108.00
 ---- batch: 050 ----
mean loss: 106.92
 ---- batch: 060 ----
mean loss: 107.19
 ---- batch: 070 ----
mean loss: 106.61
 ---- batch: 080 ----
mean loss: 106.39
 ---- batch: 090 ----
mean loss: 104.63
train mean loss: 108.42
epoch train time: 0:00:01.631907
elapsed time: 0:07:17.996164
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 19:53:10.238449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.47
 ---- batch: 020 ----
mean loss: 104.27
 ---- batch: 030 ----
mean loss: 111.57
 ---- batch: 040 ----
mean loss: 111.20
 ---- batch: 050 ----
mean loss: 103.58
 ---- batch: 060 ----
mean loss: 104.95
 ---- batch: 070 ----
mean loss: 108.63
 ---- batch: 080 ----
mean loss: 111.58
 ---- batch: 090 ----
mean loss: 108.67
train mean loss: 108.29
epoch train time: 0:00:01.673141
elapsed time: 0:07:19.670046
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 19:53:11.912324
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.92
 ---- batch: 020 ----
mean loss: 105.86
 ---- batch: 030 ----
mean loss: 103.46
 ---- batch: 040 ----
mean loss: 111.65
 ---- batch: 050 ----
mean loss: 106.69
 ---- batch: 060 ----
mean loss: 108.95
 ---- batch: 070 ----
mean loss: 114.03
 ---- batch: 080 ----
mean loss: 103.58
 ---- batch: 090 ----
mean loss: 113.65
train mean loss: 108.32
epoch train time: 0:00:01.636874
elapsed time: 0:07:21.307639
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 19:53:13.549921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.14
 ---- batch: 020 ----
mean loss: 111.49
 ---- batch: 030 ----
mean loss: 101.60
 ---- batch: 040 ----
mean loss: 105.93
 ---- batch: 050 ----
mean loss: 109.39
 ---- batch: 060 ----
mean loss: 103.73
 ---- batch: 070 ----
mean loss: 108.45
 ---- batch: 080 ----
mean loss: 112.36
 ---- batch: 090 ----
mean loss: 112.80
train mean loss: 108.42
epoch train time: 0:00:01.641288
elapsed time: 0:07:22.949557
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 19:53:15.191844
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.81
 ---- batch: 020 ----
mean loss: 108.44
 ---- batch: 030 ----
mean loss: 105.12
 ---- batch: 040 ----
mean loss: 107.82
 ---- batch: 050 ----
mean loss: 112.62
 ---- batch: 060 ----
mean loss: 107.63
 ---- batch: 070 ----
mean loss: 108.40
 ---- batch: 080 ----
mean loss: 110.29
 ---- batch: 090 ----
mean loss: 112.34
train mean loss: 108.12
epoch train time: 0:00:01.677798
elapsed time: 0:07:24.628013
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 19:53:16.870314
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.92
 ---- batch: 020 ----
mean loss: 104.11
 ---- batch: 030 ----
mean loss: 103.18
 ---- batch: 040 ----
mean loss: 110.91
 ---- batch: 050 ----
mean loss: 103.68
 ---- batch: 060 ----
mean loss: 111.42
 ---- batch: 070 ----
mean loss: 108.30
 ---- batch: 080 ----
mean loss: 103.02
 ---- batch: 090 ----
mean loss: 113.44
train mean loss: 108.15
epoch train time: 0:00:01.644387
elapsed time: 0:07:26.273045
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 19:53:18.515339
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.84
 ---- batch: 020 ----
mean loss: 105.85
 ---- batch: 030 ----
mean loss: 104.36
 ---- batch: 040 ----
mean loss: 110.26
 ---- batch: 050 ----
mean loss: 113.18
 ---- batch: 060 ----
mean loss: 111.23
 ---- batch: 070 ----
mean loss: 102.89
 ---- batch: 080 ----
mean loss: 111.63
 ---- batch: 090 ----
mean loss: 108.18
train mean loss: 108.17
epoch train time: 0:00:01.642387
elapsed time: 0:07:27.916028
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 19:53:20.158356
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.64
 ---- batch: 020 ----
mean loss: 101.91
 ---- batch: 030 ----
mean loss: 105.72
 ---- batch: 040 ----
mean loss: 107.20
 ---- batch: 050 ----
mean loss: 113.09
 ---- batch: 060 ----
mean loss: 115.39
 ---- batch: 070 ----
mean loss: 113.57
 ---- batch: 080 ----
mean loss: 105.88
 ---- batch: 090 ----
mean loss: 107.19
train mean loss: 108.05
epoch train time: 0:00:01.652759
elapsed time: 0:07:29.569465
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 19:53:21.811756
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.82
 ---- batch: 020 ----
mean loss: 112.23
 ---- batch: 030 ----
mean loss: 106.94
 ---- batch: 040 ----
mean loss: 101.45
 ---- batch: 050 ----
mean loss: 109.22
 ---- batch: 060 ----
mean loss: 107.79
 ---- batch: 070 ----
mean loss: 107.43
 ---- batch: 080 ----
mean loss: 112.77
 ---- batch: 090 ----
mean loss: 112.03
train mean loss: 108.03
epoch train time: 0:00:01.655189
elapsed time: 0:07:31.233414
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_3/checkpoint.pth.tar
**** end time: 2019-09-25 19:53:23.475412 ****
