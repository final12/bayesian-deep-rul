Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.125/bayesian_conv5_dense1_0.125_4', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.125, resume=False, step_size=200, visualize_step=50)
pid: 11401
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 19:32:50.945152 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 19:32:50.962797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2122.20
train mean loss: 1960.94
epoch train time: 0:00:05.796662
elapsed time: 0:00:05.822130
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 19:32:56.767332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1422.77
train mean loss: 1437.24
epoch train time: 0:00:02.029204
elapsed time: 0:00:07.851985
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 19:32:58.797300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1216.97
train mean loss: 1210.16
epoch train time: 0:00:02.011766
elapsed time: 0:00:09.864493
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 19:33:00.809789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1176.52
train mean loss: 1181.71
epoch train time: 0:00:02.005598
elapsed time: 0:00:11.870816
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 19:33:02.816110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1088.43
train mean loss: 1090.55
epoch train time: 0:00:02.012816
elapsed time: 0:00:13.884333
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 19:33:04.829661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1038.19
train mean loss: 1074.96
epoch train time: 0:00:02.010232
elapsed time: 0:00:15.895291
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 19:33:06.840591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1025.91
train mean loss: 1034.40
epoch train time: 0:00:01.981559
elapsed time: 0:00:17.877568
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 19:33:08.822865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1015.55
train mean loss: 1025.16
epoch train time: 0:00:02.015905
elapsed time: 0:00:19.894173
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 19:33:10.839509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1006.90
train mean loss: 980.06
epoch train time: 0:00:02.009271
elapsed time: 0:00:21.904197
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 19:33:12.849501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1009.42
train mean loss: 997.55
epoch train time: 0:00:02.022032
elapsed time: 0:00:23.927030
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 19:33:14.872333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 988.14
train mean loss: 989.13
epoch train time: 0:00:02.015486
elapsed time: 0:00:25.943234
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 19:33:16.888543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 969.29
train mean loss: 987.30
epoch train time: 0:00:02.037800
elapsed time: 0:00:27.981772
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 19:33:18.927073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 972.28
train mean loss: 969.59
epoch train time: 0:00:02.035016
elapsed time: 0:00:30.017555
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 19:33:20.962845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 973.20
train mean loss: 972.89
epoch train time: 0:00:02.022020
elapsed time: 0:00:32.041317
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 19:33:22.986625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.93
train mean loss: 957.54
epoch train time: 0:00:02.029643
elapsed time: 0:00:34.071685
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 19:33:25.016984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.15
train mean loss: 953.23
epoch train time: 0:00:02.026051
elapsed time: 0:00:36.098538
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 19:33:27.043829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.19
train mean loss: 936.98
epoch train time: 0:00:02.010740
elapsed time: 0:00:38.109960
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 19:33:29.055294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.70
train mean loss: 941.76
epoch train time: 0:00:02.012239
elapsed time: 0:00:40.122920
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 19:33:31.068220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.39
train mean loss: 938.81
epoch train time: 0:00:02.014744
elapsed time: 0:00:42.138453
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 19:33:33.083748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.67
train mean loss: 924.08
epoch train time: 0:00:02.007131
elapsed time: 0:00:44.146275
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 19:33:35.091572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.87
train mean loss: 924.05
epoch train time: 0:00:02.008155
elapsed time: 0:00:46.155130
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 19:33:37.100427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.69
train mean loss: 946.66
epoch train time: 0:00:02.019446
elapsed time: 0:00:48.175308
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 19:33:39.120599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.59
train mean loss: 909.92
epoch train time: 0:00:02.043982
elapsed time: 0:00:50.219975
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 19:33:41.165281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.71
train mean loss: 951.95
epoch train time: 0:00:02.048119
elapsed time: 0:00:52.268846
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 19:33:43.214165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.84
train mean loss: 918.46
epoch train time: 0:00:02.010310
elapsed time: 0:00:54.279907
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 19:33:45.225198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.29
train mean loss: 931.69
epoch train time: 0:00:02.046723
elapsed time: 0:00:56.327382
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 19:33:47.272704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.88
train mean loss: 919.63
epoch train time: 0:00:02.041000
elapsed time: 0:00:58.369103
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 19:33:49.314402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.03
train mean loss: 919.67
epoch train time: 0:00:02.050411
elapsed time: 0:01:00.420282
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 19:33:51.365587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.99
train mean loss: 931.37
epoch train time: 0:00:02.046958
elapsed time: 0:01:02.467937
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 19:33:53.413259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.07
train mean loss: 939.55
epoch train time: 0:00:02.002456
elapsed time: 0:01:04.471132
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 19:33:55.416424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 920.33
train mean loss: 935.43
epoch train time: 0:00:02.008264
elapsed time: 0:01:06.480151
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 19:33:57.425453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.43
train mean loss: 930.24
epoch train time: 0:00:02.003120
elapsed time: 0:01:08.484086
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 19:33:59.429404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.38
train mean loss: 906.37
epoch train time: 0:00:01.998142
elapsed time: 0:01:10.482941
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 19:34:01.428228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.51
train mean loss: 909.82
epoch train time: 0:00:02.020615
elapsed time: 0:01:12.504251
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 19:34:03.449535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.79
train mean loss: 909.89
epoch train time: 0:00:02.039084
elapsed time: 0:01:14.544010
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 19:34:05.489303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.09
train mean loss: 913.03
epoch train time: 0:00:02.032444
elapsed time: 0:01:16.577213
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 19:34:07.522507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 904.77
train mean loss: 931.86
epoch train time: 0:00:02.027839
elapsed time: 0:01:18.605833
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 19:34:09.551060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.03
train mean loss: 921.82
epoch train time: 0:00:02.022932
elapsed time: 0:01:20.629428
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 19:34:11.574726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 918.90
train mean loss: 895.96
epoch train time: 0:00:02.014769
elapsed time: 0:01:22.644900
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 19:34:13.590202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.30
train mean loss: 916.52
epoch train time: 0:00:02.015513
elapsed time: 0:01:24.661141
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 19:34:15.606435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 918.76
train mean loss: 913.91
epoch train time: 0:00:02.030047
elapsed time: 0:01:26.691979
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 19:34:17.637280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.53
train mean loss: 899.43
epoch train time: 0:00:02.043877
elapsed time: 0:01:28.736627
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 19:34:19.681922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.93
train mean loss: 918.51
epoch train time: 0:00:02.021948
elapsed time: 0:01:30.759327
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 19:34:21.704648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.33
train mean loss: 914.02
epoch train time: 0:00:02.007401
elapsed time: 0:01:32.767443
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 19:34:23.712732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.12
train mean loss: 909.86
epoch train time: 0:00:02.012681
elapsed time: 0:01:34.780822
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 19:34:25.726117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.04
train mean loss: 907.85
epoch train time: 0:00:01.999582
elapsed time: 0:01:36.781099
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 19:34:27.726418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.86
train mean loss: 897.02
epoch train time: 0:00:02.015455
elapsed time: 0:01:38.797321
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 19:34:29.742659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.29
train mean loss: 903.13
epoch train time: 0:00:02.013060
elapsed time: 0:01:40.811133
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 19:34:31.756433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.04
train mean loss: 915.42
epoch train time: 0:00:02.019140
elapsed time: 0:01:42.830941
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 19:34:33.776236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.70
train mean loss: 912.63
epoch train time: 0:00:02.005351
elapsed time: 0:01:44.837029
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 19:34:35.782315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.41
train mean loss: 904.94
epoch train time: 0:00:02.020235
elapsed time: 0:01:46.858067
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 19:34:37.803404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.92
train mean loss: 928.65
epoch train time: 0:00:02.025536
elapsed time: 0:01:48.885179
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 19:34:39.830570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.60
train mean loss: 909.23
epoch train time: 0:00:02.044744
elapsed time: 0:01:50.930791
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 19:34:41.876103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.38
train mean loss: 903.34
epoch train time: 0:00:02.050171
elapsed time: 0:01:52.981735
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 19:34:43.927050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.32
train mean loss: 887.22
epoch train time: 0:00:02.034350
elapsed time: 0:01:55.016934
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 19:34:45.962259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.67
train mean loss: 905.75
epoch train time: 0:00:02.038471
elapsed time: 0:01:57.056292
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 19:34:48.001634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.43
train mean loss: 895.17
epoch train time: 0:00:02.052075
elapsed time: 0:01:59.109128
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 19:34:50.054430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.33
train mean loss: 891.40
epoch train time: 0:00:02.048942
elapsed time: 0:02:01.158854
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 19:34:52.104155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.89
train mean loss: 890.71
epoch train time: 0:00:02.047988
elapsed time: 0:02:03.207578
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 19:34:54.152878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.85
train mean loss: 893.65
epoch train time: 0:00:02.050698
elapsed time: 0:02:05.259108
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 19:34:56.204398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.20
train mean loss: 890.82
epoch train time: 0:00:02.057915
elapsed time: 0:02:07.317762
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 19:34:58.263060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.86
train mean loss: 895.96
epoch train time: 0:00:02.050654
elapsed time: 0:02:09.369142
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 19:35:00.314435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.43
train mean loss: 890.86
epoch train time: 0:00:02.053105
elapsed time: 0:02:11.422958
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 19:35:02.368249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.18
train mean loss: 897.05
epoch train time: 0:00:02.050579
elapsed time: 0:02:13.474264
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 19:35:04.419560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.60
train mean loss: 905.23
epoch train time: 0:00:02.047251
elapsed time: 0:02:15.522250
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 19:35:06.467538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.92
train mean loss: 900.95
epoch train time: 0:00:02.044432
elapsed time: 0:02:17.567443
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 19:35:08.512765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.17
train mean loss: 891.84
epoch train time: 0:00:02.030183
elapsed time: 0:02:19.598454
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 19:35:10.543757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.88
train mean loss: 887.81
epoch train time: 0:00:02.029921
elapsed time: 0:02:21.629157
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 19:35:12.574473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.98
train mean loss: 897.37
epoch train time: 0:00:02.026016
elapsed time: 0:02:23.655951
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 19:35:14.601239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.39
train mean loss: 912.59
epoch train time: 0:00:02.004862
elapsed time: 0:02:25.661542
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 19:35:16.606842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.57
train mean loss: 886.95
epoch train time: 0:00:02.024761
elapsed time: 0:02:27.687119
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 19:35:18.632469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.54
train mean loss: 894.37
epoch train time: 0:00:02.027912
elapsed time: 0:02:29.715769
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 19:35:20.661078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.03
train mean loss: 889.03
epoch train time: 0:00:02.031370
elapsed time: 0:02:31.747939
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 19:35:22.693231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 904.90
train mean loss: 896.02
epoch train time: 0:00:02.033662
elapsed time: 0:02:33.782324
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 19:35:24.727619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.72
train mean loss: 881.77
epoch train time: 0:00:02.055866
elapsed time: 0:02:35.838925
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 19:35:26.784228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.51
train mean loss: 890.52
epoch train time: 0:00:01.996645
elapsed time: 0:02:37.836256
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 19:35:28.781599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.25
train mean loss: 897.09
epoch train time: 0:00:02.031740
elapsed time: 0:02:39.868802
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 19:35:30.814098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.69
train mean loss: 901.19
epoch train time: 0:00:02.044186
elapsed time: 0:02:41.913683
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 19:35:32.858993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.42
train mean loss: 892.66
epoch train time: 0:00:02.033615
elapsed time: 0:02:43.948134
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 19:35:34.893426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.55
train mean loss: 909.97
epoch train time: 0:00:02.004219
elapsed time: 0:02:45.953067
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 19:35:36.898356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.06
train mean loss: 884.00
epoch train time: 0:00:02.026841
elapsed time: 0:02:47.980636
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 19:35:38.925940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.86
train mean loss: 878.07
epoch train time: 0:00:02.041029
elapsed time: 0:02:50.022416
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 19:35:40.967705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.10
train mean loss: 883.05
epoch train time: 0:00:02.013794
elapsed time: 0:02:52.036886
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 19:35:42.982186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.77
train mean loss: 875.74
epoch train time: 0:00:02.012263
elapsed time: 0:02:54.049893
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 19:35:44.995240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.74
train mean loss: 886.76
epoch train time: 0:00:01.997451
elapsed time: 0:02:56.048190
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 19:35:46.993560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.91
train mean loss: 869.12
epoch train time: 0:00:01.986797
elapsed time: 0:02:58.035717
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 19:35:48.981021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.00
train mean loss: 882.56
epoch train time: 0:00:02.016776
elapsed time: 0:03:00.053202
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 19:35:50.998522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.21
train mean loss: 883.36
epoch train time: 0:00:02.001081
elapsed time: 0:03:02.054997
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 19:35:53.000292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.02
train mean loss: 874.64
epoch train time: 0:00:01.998652
elapsed time: 0:03:04.054543
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 19:35:54.999894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.28
train mean loss: 891.52
epoch train time: 0:00:02.031441
elapsed time: 0:03:06.087489
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 19:35:57.032854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.60
train mean loss: 872.43
epoch train time: 0:00:02.029512
elapsed time: 0:03:08.117790
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 19:35:59.063092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.63
train mean loss: 876.79
epoch train time: 0:00:02.015733
elapsed time: 0:03:10.134211
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 19:36:01.079515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.01
train mean loss: 883.28
epoch train time: 0:00:02.002568
elapsed time: 0:03:12.137463
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 19:36:03.082764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.55
train mean loss: 882.38
epoch train time: 0:00:01.989332
elapsed time: 0:03:14.127561
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 19:36:05.072849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.88
train mean loss: 888.32
epoch train time: 0:00:01.983787
elapsed time: 0:03:16.112052
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 19:36:07.057343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.48
train mean loss: 890.25
epoch train time: 0:00:01.987365
elapsed time: 0:03:18.100093
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 19:36:09.045386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.44
train mean loss: 880.41
epoch train time: 0:00:02.000884
elapsed time: 0:03:20.101696
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 19:36:11.046992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.12
train mean loss: 881.33
epoch train time: 0:00:02.052983
elapsed time: 0:03:22.155430
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 19:36:13.100750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.77
train mean loss: 890.12
epoch train time: 0:00:02.060881
elapsed time: 0:03:24.217069
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 19:36:15.162362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.95
train mean loss: 899.35
epoch train time: 0:00:02.045248
elapsed time: 0:03:26.263029
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 19:36:17.208326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.01
train mean loss: 875.76
epoch train time: 0:00:02.024211
elapsed time: 0:03:28.287951
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 19:36:19.233260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.34
train mean loss: 873.35
epoch train time: 0:00:02.048413
elapsed time: 0:03:30.337130
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 19:36:21.282488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.79
train mean loss: 881.30
epoch train time: 0:00:02.022324
elapsed time: 0:03:32.360287
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 19:36:23.305579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.74
train mean loss: 876.26
epoch train time: 0:00:02.020988
elapsed time: 0:03:34.381994
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 19:36:25.327317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.83
train mean loss: 890.90
epoch train time: 0:00:02.018281
elapsed time: 0:03:36.401129
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 19:36:27.346423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.80
train mean loss: 882.61
epoch train time: 0:00:02.022757
elapsed time: 0:03:38.424636
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 19:36:29.369954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.12
train mean loss: 871.85
epoch train time: 0:00:02.024919
elapsed time: 0:03:40.450369
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 19:36:31.395578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.49
train mean loss: 864.09
epoch train time: 0:00:02.033103
elapsed time: 0:03:42.484075
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 19:36:33.429363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.11
train mean loss: 880.80
epoch train time: 0:00:02.032043
elapsed time: 0:03:44.516865
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 19:36:35.462155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.11
train mean loss: 860.53
epoch train time: 0:00:02.037828
elapsed time: 0:03:46.555388
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 19:36:37.500675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.20
train mean loss: 867.92
epoch train time: 0:00:02.029753
elapsed time: 0:03:48.585834
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 19:36:39.531115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.06
train mean loss: 881.12
epoch train time: 0:00:02.026670
elapsed time: 0:03:50.613227
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 19:36:41.558528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.17
train mean loss: 861.86
epoch train time: 0:00:02.022061
elapsed time: 0:03:52.635986
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 19:36:43.581294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.19
train mean loss: 869.75
epoch train time: 0:00:02.040773
elapsed time: 0:03:54.677456
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 19:36:45.622747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.53
train mean loss: 891.58
epoch train time: 0:00:02.039561
elapsed time: 0:03:56.717910
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 19:36:47.663210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.94
train mean loss: 870.73
epoch train time: 0:00:02.041121
elapsed time: 0:03:58.759860
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 19:36:49.705152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.80
train mean loss: 870.66
epoch train time: 0:00:02.019848
elapsed time: 0:04:00.780568
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 19:36:51.725874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
train mean loss: 878.16
epoch train time: 0:00:02.002104
elapsed time: 0:04:02.783407
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 19:36:53.728695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.59
train mean loss: 860.83
epoch train time: 0:00:02.008631
elapsed time: 0:04:04.792807
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 19:36:55.738104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.58
train mean loss: 867.45
epoch train time: 0:00:02.046252
elapsed time: 0:04:06.839848
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 19:36:57.785341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.57
train mean loss: 880.31
epoch train time: 0:00:02.029693
elapsed time: 0:04:08.870493
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 19:36:59.815810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.81
train mean loss: 864.83
epoch train time: 0:00:02.023624
elapsed time: 0:04:10.894862
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 19:37:01.840156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.16
train mean loss: 868.69
epoch train time: 0:00:02.012352
elapsed time: 0:04:12.907979
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 19:37:03.853325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.13
train mean loss: 865.36
epoch train time: 0:00:02.006623
elapsed time: 0:04:14.915374
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 19:37:05.860667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.90
train mean loss: 856.93
epoch train time: 0:00:02.049523
elapsed time: 0:04:16.965606
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 19:37:07.910900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.76
train mean loss: 885.22
epoch train time: 0:00:02.015959
elapsed time: 0:04:18.982311
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 19:37:09.927614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.57
train mean loss: 869.12
epoch train time: 0:00:02.019259
elapsed time: 0:04:21.002558
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 19:37:11.947775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.16
train mean loss: 873.99
epoch train time: 0:00:02.024953
elapsed time: 0:04:23.028226
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 19:37:13.973550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.36
train mean loss: 866.54
epoch train time: 0:00:02.014595
elapsed time: 0:04:25.043666
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 19:37:15.988972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.32
train mean loss: 872.60
epoch train time: 0:00:02.016719
elapsed time: 0:04:27.061105
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 19:37:18.006409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.15
train mean loss: 874.28
epoch train time: 0:00:02.025184
elapsed time: 0:04:29.086990
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 19:37:20.032289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.94
train mean loss: 867.53
epoch train time: 0:00:02.038765
elapsed time: 0:04:31.126447
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 19:37:22.071738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.11
train mean loss: 857.08
epoch train time: 0:00:02.061744
elapsed time: 0:04:33.189005
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 19:37:24.134453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.20
train mean loss: 852.25
epoch train time: 0:00:02.057758
elapsed time: 0:04:35.247640
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 19:37:26.192950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.72
train mean loss: 855.52
epoch train time: 0:00:02.030180
elapsed time: 0:04:37.278590
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 19:37:28.223882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.40
train mean loss: 849.84
epoch train time: 0:00:02.034173
elapsed time: 0:04:39.313513
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 19:37:30.258815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.41
train mean loss: 854.43
epoch train time: 0:00:02.029893
elapsed time: 0:04:41.344089
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 19:37:32.289371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.92
train mean loss: 858.46
epoch train time: 0:00:02.013677
elapsed time: 0:04:43.358500
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 19:37:34.303800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.61
train mean loss: 849.99
epoch train time: 0:00:02.045009
elapsed time: 0:04:45.404239
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 19:37:36.349560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.34
train mean loss: 852.07
epoch train time: 0:00:02.059372
elapsed time: 0:04:47.464502
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 19:37:38.409802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.80
train mean loss: 847.13
epoch train time: 0:00:02.057724
elapsed time: 0:04:49.522990
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 19:37:40.468297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.54
train mean loss: 835.60
epoch train time: 0:00:02.029380
elapsed time: 0:04:51.553098
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 19:37:42.498390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.96
train mean loss: 853.44
epoch train time: 0:00:02.037010
elapsed time: 0:04:53.590882
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 19:37:44.536098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.83
train mean loss: 841.09
epoch train time: 0:00:02.026265
elapsed time: 0:04:55.617805
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 19:37:46.563157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.02
train mean loss: 846.37
epoch train time: 0:00:02.032242
elapsed time: 0:04:57.650976
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 19:37:48.596290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.97
train mean loss: 840.31
epoch train time: 0:00:02.059693
elapsed time: 0:04:59.711415
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 19:37:50.656705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.53
train mean loss: 836.69
epoch train time: 0:00:02.033566
elapsed time: 0:05:01.745793
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 19:37:52.691099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.74
train mean loss: 836.24
epoch train time: 0:00:02.031870
elapsed time: 0:05:03.778387
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 19:37:54.723676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.71
train mean loss: 828.89
epoch train time: 0:00:02.015635
elapsed time: 0:05:05.794814
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 19:37:56.740031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.96
train mean loss: 836.45
epoch train time: 0:00:02.032040
elapsed time: 0:05:07.827498
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 19:37:58.772809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.45
train mean loss: 832.22
epoch train time: 0:00:02.028492
elapsed time: 0:05:09.856730
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 19:38:00.802020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.79
train mean loss: 834.39
epoch train time: 0:00:02.041710
elapsed time: 0:05:11.899091
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 19:38:02.844374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.79
train mean loss: 835.64
epoch train time: 0:00:02.030467
elapsed time: 0:05:13.930285
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 19:38:04.875586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.06
train mean loss: 822.26
epoch train time: 0:00:02.035181
elapsed time: 0:05:15.966168
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 19:38:06.911460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 822.43
train mean loss: 829.26
epoch train time: 0:00:02.034070
elapsed time: 0:05:18.000951
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 19:38:08.946246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 822.96
train mean loss: 824.63
epoch train time: 0:00:02.022693
elapsed time: 0:05:20.024344
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 19:38:10.969696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 813.57
train mean loss: 820.91
epoch train time: 0:00:02.035700
elapsed time: 0:05:22.060797
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 19:38:13.006119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 819.88
train mean loss: 813.10
epoch train time: 0:00:02.037519
elapsed time: 0:05:24.099042
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 19:38:15.044357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 806.86
train mean loss: 817.06
epoch train time: 0:00:02.029406
elapsed time: 0:05:26.129136
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 19:38:17.074473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.24
train mean loss: 808.51
epoch train time: 0:00:02.008952
elapsed time: 0:05:28.138941
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 19:38:19.084312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 802.01
train mean loss: 815.63
epoch train time: 0:00:01.990094
elapsed time: 0:05:30.129944
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 19:38:21.075285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.42
train mean loss: 803.49
epoch train time: 0:00:01.996059
elapsed time: 0:05:32.126750
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 19:38:23.072052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 795.89
train mean loss: 803.22
epoch train time: 0:00:01.997409
elapsed time: 0:05:34.124880
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 19:38:25.070178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 800.86
train mean loss: 794.57
epoch train time: 0:00:01.988727
elapsed time: 0:05:36.114303
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 19:38:27.059595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 794.91
train mean loss: 781.49
epoch train time: 0:00:02.002434
elapsed time: 0:05:38.117397
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 19:38:29.062690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 783.73
train mean loss: 796.30
epoch train time: 0:00:02.008836
elapsed time: 0:05:40.127593
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 19:38:31.072945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 776.41
train mean loss: 776.52
epoch train time: 0:00:01.987695
elapsed time: 0:05:42.116027
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 19:38:33.061346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 770.66
train mean loss: 756.76
epoch train time: 0:00:02.022947
elapsed time: 0:05:44.139658
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 19:38:35.084983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 775.73
train mean loss: 766.21
epoch train time: 0:00:02.020060
elapsed time: 0:05:46.160503
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 19:38:37.105821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 759.78
train mean loss: 758.74
epoch train time: 0:00:02.034347
elapsed time: 0:05:48.195565
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 19:38:39.140862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 758.57
train mean loss: 750.37
epoch train time: 0:00:02.024701
elapsed time: 0:05:50.220977
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 19:38:41.166279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.47
train mean loss: 738.01
epoch train time: 0:00:02.029801
elapsed time: 0:05:52.251488
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 19:38:43.196789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 731.52
train mean loss: 725.16
epoch train time: 0:00:01.995164
elapsed time: 0:05:54.247320
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 19:38:45.192611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 719.42
train mean loss: 718.11
epoch train time: 0:00:01.979348
elapsed time: 0:05:56.227456
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 19:38:47.172662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 704.20
train mean loss: 699.51
epoch train time: 0:00:01.988022
elapsed time: 0:05:58.216070
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 19:38:49.161382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.55
train mean loss: 707.86
epoch train time: 0:00:01.984817
elapsed time: 0:06:00.201606
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 19:38:51.146918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 693.76
train mean loss: 684.61
epoch train time: 0:00:01.989632
elapsed time: 0:06:02.191931
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 19:38:53.137220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 688.80
train mean loss: 682.58
epoch train time: 0:00:01.999403
elapsed time: 0:06:04.192072
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 19:38:55.137364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 686.60
train mean loss: 674.56
epoch train time: 0:00:01.989729
elapsed time: 0:06:06.182479
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 19:38:57.127806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.15
train mean loss: 670.60
epoch train time: 0:00:01.994852
elapsed time: 0:06:08.178104
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 19:38:59.123394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.52
train mean loss: 658.26
epoch train time: 0:00:01.992242
elapsed time: 0:06:10.171018
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 19:39:01.116306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 665.85
train mean loss: 647.02
epoch train time: 0:00:02.007329
elapsed time: 0:06:12.179068
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 19:39:03.124375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 645.63
train mean loss: 641.68
epoch train time: 0:00:02.002413
elapsed time: 0:06:14.182189
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 19:39:05.127472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 637.91
train mean loss: 638.23
epoch train time: 0:00:01.992530
elapsed time: 0:06:16.175387
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 19:39:07.120685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 642.28
train mean loss: 626.17
epoch train time: 0:00:01.989079
elapsed time: 0:06:18.165171
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 19:39:09.110488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 632.04
train mean loss: 633.58
epoch train time: 0:00:01.996829
elapsed time: 0:06:20.162772
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 19:39:11.108075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 634.96
train mean loss: 623.21
epoch train time: 0:00:01.994235
elapsed time: 0:06:22.157691
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 19:39:13.102999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.30
train mean loss: 623.26
epoch train time: 0:00:02.006240
elapsed time: 0:06:24.164624
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 19:39:15.109950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 610.03
train mean loss: 611.97
epoch train time: 0:00:02.000491
elapsed time: 0:06:26.165822
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 19:39:17.111125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 599.42
train mean loss: 597.79
epoch train time: 0:00:01.996149
elapsed time: 0:06:28.162717
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 19:39:19.108026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 598.38
train mean loss: 594.06
epoch train time: 0:00:01.999307
elapsed time: 0:06:30.162722
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 19:39:21.108022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 600.75
train mean loss: 591.65
epoch train time: 0:00:01.990928
elapsed time: 0:06:32.154330
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 19:39:23.099615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 590.51
train mean loss: 579.59
epoch train time: 0:00:01.999984
elapsed time: 0:06:34.154986
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 19:39:25.100294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 594.45
train mean loss: 583.69
epoch train time: 0:00:01.990592
elapsed time: 0:06:36.146249
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 19:39:27.091600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.50
train mean loss: 602.77
epoch train time: 0:00:01.994805
elapsed time: 0:06:38.141786
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 19:39:29.087092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 566.23
train mean loss: 586.57
epoch train time: 0:00:02.009461
elapsed time: 0:06:40.151910
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 19:39:31.097204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 562.04
train mean loss: 568.02
epoch train time: 0:00:01.994452
elapsed time: 0:06:42.147012
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 19:39:33.092299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 577.89
train mean loss: 563.45
epoch train time: 0:00:02.000748
elapsed time: 0:06:44.148410
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 19:39:35.093720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 571.50
train mean loss: 568.16
epoch train time: 0:00:01.993691
elapsed time: 0:06:46.142769
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 19:39:37.088073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.40
train mean loss: 559.15
epoch train time: 0:00:02.004430
elapsed time: 0:06:48.147953
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 19:39:39.093242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 543.02
train mean loss: 549.14
epoch train time: 0:00:01.991948
elapsed time: 0:06:50.140599
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 19:39:41.085915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 540.38
train mean loss: 544.71
epoch train time: 0:00:02.013004
elapsed time: 0:06:52.154435
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 19:39:43.099639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 549.68
train mean loss: 550.65
epoch train time: 0:00:01.999001
elapsed time: 0:06:54.154045
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 19:39:45.099350
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 542.51
train mean loss: 549.66
epoch train time: 0:00:01.985238
elapsed time: 0:06:56.140017
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 19:39:47.085308
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 541.69
train mean loss: 548.73
epoch train time: 0:00:01.985682
elapsed time: 0:06:58.126364
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 19:39:49.071662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 548.18
train mean loss: 543.00
epoch train time: 0:00:01.995202
elapsed time: 0:07:00.122242
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 19:39:51.067545
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 535.59
train mean loss: 541.18
epoch train time: 0:00:02.001976
elapsed time: 0:07:02.125000
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 19:39:53.070289
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 551.16
train mean loss: 548.61
epoch train time: 0:00:01.991223
elapsed time: 0:07:04.117026
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 19:39:55.062322
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 551.83
train mean loss: 540.96
epoch train time: 0:00:01.988212
elapsed time: 0:07:06.105979
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 19:39:57.051264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 538.35
train mean loss: 545.65
epoch train time: 0:00:01.990389
elapsed time: 0:07:08.097057
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 19:39:59.042346
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 541.56
train mean loss: 552.92
epoch train time: 0:00:02.000286
elapsed time: 0:07:10.097997
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 19:40:01.043287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 546.02
train mean loss: 547.01
epoch train time: 0:00:01.999030
elapsed time: 0:07:12.097700
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 19:40:03.043000
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 537.75
train mean loss: 540.94
epoch train time: 0:00:02.006595
elapsed time: 0:07:14.105133
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 19:40:05.050481
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 532.03
train mean loss: 543.53
epoch train time: 0:00:02.000525
elapsed time: 0:07:16.106469
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 19:40:07.051778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 546.56
train mean loss: 545.07
epoch train time: 0:00:01.989484
elapsed time: 0:07:18.096688
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 19:40:09.041996
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 538.34
train mean loss: 534.49
epoch train time: 0:00:01.994580
elapsed time: 0:07:20.092027
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 19:40:11.037325
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 535.74
train mean loss: 535.67
epoch train time: 0:00:02.003277
elapsed time: 0:07:22.095986
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 19:40:13.041280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 538.25
train mean loss: 544.38
epoch train time: 0:00:02.004413
elapsed time: 0:07:24.101139
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 19:40:15.046440
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 545.25
train mean loss: 529.34
epoch train time: 0:00:02.023607
elapsed time: 0:07:26.125485
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 19:40:17.070776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 546.52
train mean loss: 537.19
epoch train time: 0:00:02.016424
elapsed time: 0:07:28.142586
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 19:40:19.087875
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 531.95
train mean loss: 545.82
epoch train time: 0:00:02.008022
elapsed time: 0:07:30.151291
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 19:40:21.096577
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 530.36
train mean loss: 529.05
epoch train time: 0:00:01.989791
elapsed time: 0:07:32.141771
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 19:40:23.087073
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 529.93
train mean loss: 525.35
epoch train time: 0:00:01.983686
elapsed time: 0:07:34.126159
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 19:40:25.071453
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 550.54
train mean loss: 551.68
epoch train time: 0:00:01.992898
elapsed time: 0:07:36.119753
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 19:40:27.065050
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 533.71
train mean loss: 540.74
epoch train time: 0:00:02.019913
elapsed time: 0:07:38.140359
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 19:40:29.085674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 535.91
train mean loss: 537.97
epoch train time: 0:00:02.018414
elapsed time: 0:07:40.159546
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 19:40:31.104832
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 549.25
train mean loss: 536.88
epoch train time: 0:00:02.017071
elapsed time: 0:07:42.177340
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 19:40:33.122632
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 540.19
train mean loss: 536.11
epoch train time: 0:00:02.021568
elapsed time: 0:07:44.199695
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 19:40:35.144994
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 534.23
train mean loss: 539.02
epoch train time: 0:00:02.010353
elapsed time: 0:07:46.210772
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 19:40:37.156069
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 538.94
train mean loss: 538.41
epoch train time: 0:00:01.997290
elapsed time: 0:07:48.208786
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 19:40:39.154089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 530.66
train mean loss: 537.46
epoch train time: 0:00:02.010372
elapsed time: 0:07:50.219916
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 19:40:41.165244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 530.93
train mean loss: 540.98
epoch train time: 0:00:02.014526
elapsed time: 0:07:52.235189
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 19:40:43.180489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 539.82
train mean loss: 535.67
epoch train time: 0:00:02.015930
elapsed time: 0:07:54.251812
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 19:40:45.197118
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 546.71
train mean loss: 536.75
epoch train time: 0:00:02.012983
elapsed time: 0:07:56.265602
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 19:40:47.210801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 529.86
train mean loss: 541.35
epoch train time: 0:00:02.007909
elapsed time: 0:07:58.274126
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 19:40:49.219416
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 535.71
train mean loss: 527.46
epoch train time: 0:00:02.007458
elapsed time: 0:08:00.282261
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 19:40:51.227563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 543.75
train mean loss: 525.17
epoch train time: 0:00:01.995983
elapsed time: 0:08:02.278971
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 19:40:53.224277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 535.21
train mean loss: 535.71
epoch train time: 0:00:02.006299
elapsed time: 0:08:04.285980
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 19:40:55.231270
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 539.48
train mean loss: 540.98
epoch train time: 0:00:01.973276
elapsed time: 0:08:06.259934
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 19:40:57.205227
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 530.19
train mean loss: 519.50
epoch train time: 0:00:01.987492
elapsed time: 0:08:08.248105
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 19:40:59.193395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 538.41
train mean loss: 538.23
epoch train time: 0:00:02.017278
elapsed time: 0:08:10.266106
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 19:41:01.211413
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 530.43
train mean loss: 540.94
epoch train time: 0:00:02.010647
elapsed time: 0:08:12.277595
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 19:41:03.222899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 538.93
train mean loss: 527.77
epoch train time: 0:00:02.022295
elapsed time: 0:08:14.301459
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 19:41:05.246829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 531.50
train mean loss: 529.88
epoch train time: 0:00:02.004290
elapsed time: 0:08:16.306581
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 19:41:07.251937
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 535.31
train mean loss: 543.27
epoch train time: 0:00:02.013166
elapsed time: 0:08:18.320563
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 19:41:09.265887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 526.78
train mean loss: 539.66
epoch train time: 0:00:02.014901
elapsed time: 0:08:20.336162
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 19:41:11.281460
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 525.72
train mean loss: 521.95
epoch train time: 0:00:02.021624
elapsed time: 0:08:22.358548
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 19:41:13.303862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 532.64
train mean loss: 526.41
epoch train time: 0:00:02.021560
elapsed time: 0:08:24.380853
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 19:41:15.326160
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 523.38
train mean loss: 527.80
epoch train time: 0:00:02.015478
elapsed time: 0:08:26.397011
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 19:41:17.342324
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 531.36
train mean loss: 526.46
epoch train time: 0:00:02.010007
elapsed time: 0:08:28.416095
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.125/bayesian_conv5_dense1_0.125_4/checkpoint.pth.tar
**** end time: 2019-09-26 19:41:19.361270 ****
