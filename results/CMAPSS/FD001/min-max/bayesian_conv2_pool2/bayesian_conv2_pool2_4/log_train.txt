Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_4', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 29076
use_cuda: True
Dataset: CMAPSS/FD001
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-27 14:49:45.687712 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1             [-1, 8, 26, 1]           1,120
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
    BayesianConv2d-4            [-1, 14, 12, 1]             448
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
    BayesianLinear-8                    [-1, 1]             168
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 1,736
Trainable params: 1,736
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 14:49:45.698390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3660.10
 ---- batch: 020 ----
mean loss: 3470.64
 ---- batch: 030 ----
mean loss: 3466.68
train mean loss: 3520.00
epoch train time: 0:00:12.731278
elapsed time: 0:00:12.744827
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 14:49:58.432576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3373.62
 ---- batch: 020 ----
mean loss: 3269.98
 ---- batch: 030 ----
mean loss: 3214.05
train mean loss: 3273.55
epoch train time: 0:00:00.509736
elapsed time: 0:00:13.254839
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 14:49:58.942667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3122.16
 ---- batch: 020 ----
mean loss: 3004.06
 ---- batch: 030 ----
mean loss: 2962.58
train mean loss: 3009.98
epoch train time: 0:00:00.500980
elapsed time: 0:00:13.756116
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 14:49:59.443919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2823.69
 ---- batch: 020 ----
mean loss: 2743.05
 ---- batch: 030 ----
mean loss: 2716.00
train mean loss: 2754.04
epoch train time: 0:00:00.506288
elapsed time: 0:00:14.262684
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 14:49:59.950488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2567.31
 ---- batch: 020 ----
mean loss: 2495.76
 ---- batch: 030 ----
mean loss: 2549.87
train mean loss: 2528.38
epoch train time: 0:00:00.508674
elapsed time: 0:00:14.771630
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 14:50:00.459462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2384.63
 ---- batch: 020 ----
mean loss: 2339.38
 ---- batch: 030 ----
mean loss: 2329.10
train mean loss: 2339.38
epoch train time: 0:00:00.512317
elapsed time: 0:00:15.284265
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 14:50:00.972070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2234.66
 ---- batch: 020 ----
mean loss: 2174.72
 ---- batch: 030 ----
mean loss: 2114.53
train mean loss: 2164.70
epoch train time: 0:00:00.519783
elapsed time: 0:00:15.804350
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 14:50:01.492204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2075.47
 ---- batch: 020 ----
mean loss: 2061.88
 ---- batch: 030 ----
mean loss: 1969.00
train mean loss: 2028.31
epoch train time: 0:00:00.522136
elapsed time: 0:00:16.326839
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 14:50:02.014627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1930.05
 ---- batch: 020 ----
mean loss: 1903.55
 ---- batch: 030 ----
mean loss: 1886.84
train mean loss: 1894.73
epoch train time: 0:00:00.492028
elapsed time: 0:00:16.819149
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 14:50:02.506951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1826.20
 ---- batch: 020 ----
mean loss: 1772.92
 ---- batch: 030 ----
mean loss: 1737.20
train mean loss: 1770.17
epoch train time: 0:00:00.502904
elapsed time: 0:00:17.322337
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 14:50:03.010146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1704.93
 ---- batch: 020 ----
mean loss: 1703.13
 ---- batch: 030 ----
mean loss: 1652.52
train mean loss: 1679.00
epoch train time: 0:00:00.505537
elapsed time: 0:00:17.828154
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 14:50:03.515959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1596.22
 ---- batch: 020 ----
mean loss: 1596.46
 ---- batch: 030 ----
mean loss: 1546.53
train mean loss: 1578.16
epoch train time: 0:00:00.515315
elapsed time: 0:00:18.343758
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 14:50:04.031564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1510.81
 ---- batch: 020 ----
mean loss: 1485.73
 ---- batch: 030 ----
mean loss: 1497.79
train mean loss: 1484.60
epoch train time: 0:00:00.506057
elapsed time: 0:00:18.850089
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 14:50:04.537887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1467.08
 ---- batch: 020 ----
mean loss: 1413.70
 ---- batch: 030 ----
mean loss: 1375.23
train mean loss: 1405.87
epoch train time: 0:00:00.505923
elapsed time: 0:00:19.356311
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 14:50:05.044118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1348.39
 ---- batch: 020 ----
mean loss: 1347.73
 ---- batch: 030 ----
mean loss: 1316.40
train mean loss: 1332.03
epoch train time: 0:00:00.512153
elapsed time: 0:00:19.868753
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 14:50:05.556559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1286.28
 ---- batch: 020 ----
mean loss: 1259.38
 ---- batch: 030 ----
mean loss: 1268.30
train mean loss: 1266.07
epoch train time: 0:00:00.512230
elapsed time: 0:00:20.381266
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 14:50:06.069071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1240.38
 ---- batch: 020 ----
mean loss: 1196.77
 ---- batch: 030 ----
mean loss: 1185.89
train mean loss: 1200.87
epoch train time: 0:00:00.508525
elapsed time: 0:00:20.890100
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 14:50:06.577894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1150.12
 ---- batch: 020 ----
mean loss: 1136.18
 ---- batch: 030 ----
mean loss: 1107.17
train mean loss: 1131.05
epoch train time: 0:00:00.502227
elapsed time: 0:00:21.392589
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 14:50:07.080392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1093.48
 ---- batch: 020 ----
mean loss: 1071.54
 ---- batch: 030 ----
mean loss: 1057.55
train mean loss: 1070.47
epoch train time: 0:00:00.506146
elapsed time: 0:00:21.899028
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 14:50:07.586847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1030.33
 ---- batch: 020 ----
mean loss: 1010.04
 ---- batch: 030 ----
mean loss: 990.04
train mean loss: 1005.68
epoch train time: 0:00:00.509515
elapsed time: 0:00:22.408853
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 14:50:08.096658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 962.19
 ---- batch: 020 ----
mean loss: 959.08
 ---- batch: 030 ----
mean loss: 946.09
train mean loss: 955.01
epoch train time: 0:00:00.510243
elapsed time: 0:00:22.919386
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 14:50:08.607190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.84
 ---- batch: 020 ----
mean loss: 914.97
 ---- batch: 030 ----
mean loss: 901.92
train mean loss: 907.35
epoch train time: 0:00:00.508168
elapsed time: 0:00:23.427832
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 14:50:09.115720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.07
 ---- batch: 020 ----
mean loss: 855.35
 ---- batch: 030 ----
mean loss: 856.78
train mean loss: 858.74
epoch train time: 0:00:00.508016
elapsed time: 0:00:23.936224
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 14:50:09.624026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.36
 ---- batch: 020 ----
mean loss: 833.74
 ---- batch: 030 ----
mean loss: 817.63
train mean loss: 818.65
epoch train time: 0:00:00.529686
elapsed time: 0:00:24.466218
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 14:50:10.154026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 806.51
 ---- batch: 020 ----
mean loss: 790.69
 ---- batch: 030 ----
mean loss: 764.30
train mean loss: 781.65
epoch train time: 0:00:00.505973
elapsed time: 0:00:24.972478
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 14:50:10.660285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 744.34
 ---- batch: 020 ----
mean loss: 765.44
 ---- batch: 030 ----
mean loss: 740.04
train mean loss: 746.34
epoch train time: 0:00:00.520833
elapsed time: 0:00:25.493669
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 14:50:11.181441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 732.35
 ---- batch: 020 ----
mean loss: 728.08
 ---- batch: 030 ----
mean loss: 703.08
train mean loss: 721.53
epoch train time: 0:00:00.505667
elapsed time: 0:00:25.999580
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 14:50:11.687414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 707.56
 ---- batch: 020 ----
mean loss: 689.89
 ---- batch: 030 ----
mean loss: 681.59
train mean loss: 691.21
epoch train time: 0:00:00.504077
elapsed time: 0:00:26.503963
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 14:50:12.191767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.86
 ---- batch: 020 ----
mean loss: 693.37
 ---- batch: 030 ----
mean loss: 665.12
train mean loss: 668.24
epoch train time: 0:00:00.511679
elapsed time: 0:00:27.015948
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 14:50:12.703758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.10
 ---- batch: 020 ----
mean loss: 643.68
 ---- batch: 030 ----
mean loss: 647.93
train mean loss: 648.87
epoch train time: 0:00:00.503580
elapsed time: 0:00:27.519810
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 14:50:13.207614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 631.39
 ---- batch: 020 ----
mean loss: 623.59
 ---- batch: 030 ----
mean loss: 630.73
train mean loss: 628.22
epoch train time: 0:00:00.502019
elapsed time: 0:00:28.022128
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 14:50:13.709926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 611.85
 ---- batch: 020 ----
mean loss: 617.79
 ---- batch: 030 ----
mean loss: 601.27
train mean loss: 609.15
epoch train time: 0:00:00.504114
elapsed time: 0:00:28.526540
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 14:50:14.214312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.34
 ---- batch: 020 ----
mean loss: 587.66
 ---- batch: 030 ----
mean loss: 580.28
train mean loss: 590.90
epoch train time: 0:00:00.501604
elapsed time: 0:00:29.028392
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 14:50:14.716196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 581.90
 ---- batch: 020 ----
mean loss: 578.24
 ---- batch: 030 ----
mean loss: 569.28
train mean loss: 574.92
epoch train time: 0:00:00.500515
elapsed time: 0:00:29.529194
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 14:50:15.216997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 559.94
 ---- batch: 020 ----
mean loss: 566.29
 ---- batch: 030 ----
mean loss: 556.30
train mean loss: 561.34
epoch train time: 0:00:00.503477
elapsed time: 0:00:30.032954
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 14:50:15.720772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 544.79
 ---- batch: 020 ----
mean loss: 544.41
 ---- batch: 030 ----
mean loss: 552.20
train mean loss: 546.70
epoch train time: 0:00:00.499181
elapsed time: 0:00:30.532420
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 14:50:16.220236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 544.87
 ---- batch: 020 ----
mean loss: 541.53
 ---- batch: 030 ----
mean loss: 542.04
train mean loss: 539.36
epoch train time: 0:00:00.493762
elapsed time: 0:00:31.026465
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 14:50:16.714266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 535.45
 ---- batch: 020 ----
mean loss: 522.41
 ---- batch: 030 ----
mean loss: 526.90
train mean loss: 526.78
epoch train time: 0:00:00.510873
elapsed time: 0:00:31.537624
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 14:50:17.225428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.84
 ---- batch: 020 ----
mean loss: 517.88
 ---- batch: 030 ----
mean loss: 509.96
train mean loss: 516.21
epoch train time: 0:00:00.519505
elapsed time: 0:00:32.057412
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 14:50:17.745214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.40
 ---- batch: 020 ----
mean loss: 518.36
 ---- batch: 030 ----
mean loss: 502.54
train mean loss: 506.64
epoch train time: 0:00:00.506755
elapsed time: 0:00:32.564446
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 14:50:18.252246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.20
 ---- batch: 020 ----
mean loss: 501.84
 ---- batch: 030 ----
mean loss: 494.96
train mean loss: 498.81
epoch train time: 0:00:00.510666
elapsed time: 0:00:33.075381
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 14:50:18.763183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.95
 ---- batch: 020 ----
mean loss: 485.87
 ---- batch: 030 ----
mean loss: 492.64
train mean loss: 492.58
epoch train time: 0:00:00.503306
elapsed time: 0:00:33.578952
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 14:50:19.266752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.34
 ---- batch: 020 ----
mean loss: 483.40
 ---- batch: 030 ----
mean loss: 484.19
train mean loss: 486.45
epoch train time: 0:00:00.522796
elapsed time: 0:00:34.102053
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 14:50:19.789863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.72
 ---- batch: 020 ----
mean loss: 485.91
 ---- batch: 030 ----
mean loss: 478.62
train mean loss: 477.91
epoch train time: 0:00:00.508856
elapsed time: 0:00:34.611209
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 14:50:20.299015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.71
 ---- batch: 020 ----
mean loss: 464.98
 ---- batch: 030 ----
mean loss: 478.36
train mean loss: 471.05
epoch train time: 0:00:00.507020
elapsed time: 0:00:35.118530
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 14:50:20.806334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.03
 ---- batch: 020 ----
mean loss: 471.84
 ---- batch: 030 ----
mean loss: 459.49
train mean loss: 468.51
epoch train time: 0:00:00.491299
elapsed time: 0:00:35.610095
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 14:50:21.297910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.24
 ---- batch: 020 ----
mean loss: 463.31
 ---- batch: 030 ----
mean loss: 459.33
train mean loss: 460.56
epoch train time: 0:00:00.507839
elapsed time: 0:00:36.118223
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 14:50:21.806028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.84
 ---- batch: 020 ----
mean loss: 464.48
 ---- batch: 030 ----
mean loss: 450.09
train mean loss: 454.19
epoch train time: 0:00:00.510435
elapsed time: 0:00:36.628940
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 14:50:22.316744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.98
 ---- batch: 020 ----
mean loss: 450.03
 ---- batch: 030 ----
mean loss: 441.75
train mean loss: 449.65
epoch train time: 0:00:00.516243
elapsed time: 0:00:37.145518
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 14:50:22.833350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.43
 ---- batch: 020 ----
mean loss: 441.03
 ---- batch: 030 ----
mean loss: 438.13
train mean loss: 442.81
epoch train time: 0:00:00.496992
elapsed time: 0:00:37.642805
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 14:50:23.330613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.66
 ---- batch: 020 ----
mean loss: 436.93
 ---- batch: 030 ----
mean loss: 443.30
train mean loss: 438.68
epoch train time: 0:00:00.501695
elapsed time: 0:00:38.144774
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 14:50:23.832577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.01
 ---- batch: 020 ----
mean loss: 426.45
 ---- batch: 030 ----
mean loss: 439.98
train mean loss: 435.89
epoch train time: 0:00:00.505465
elapsed time: 0:00:38.650512
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 14:50:24.338316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.76
 ---- batch: 020 ----
mean loss: 437.68
 ---- batch: 030 ----
mean loss: 416.05
train mean loss: 430.36
epoch train time: 0:00:00.524947
elapsed time: 0:00:39.175757
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 14:50:24.863565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.44
 ---- batch: 020 ----
mean loss: 427.98
 ---- batch: 030 ----
mean loss: 431.87
train mean loss: 426.47
epoch train time: 0:00:00.506290
elapsed time: 0:00:39.682340
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 14:50:25.370142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.66
 ---- batch: 020 ----
mean loss: 423.22
 ---- batch: 030 ----
mean loss: 421.51
train mean loss: 421.44
epoch train time: 0:00:00.524374
elapsed time: 0:00:40.207011
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 14:50:25.894808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.58
 ---- batch: 020 ----
mean loss: 421.01
 ---- batch: 030 ----
mean loss: 418.93
train mean loss: 417.56
epoch train time: 0:00:00.500842
elapsed time: 0:00:40.708148
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 14:50:26.395953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.95
 ---- batch: 020 ----
mean loss: 418.48
 ---- batch: 030 ----
mean loss: 412.26
train mean loss: 413.30
epoch train time: 0:00:00.505262
elapsed time: 0:00:41.213702
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 14:50:26.901506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.02
 ---- batch: 020 ----
mean loss: 409.32
 ---- batch: 030 ----
mean loss: 407.78
train mean loss: 409.97
epoch train time: 0:00:00.516532
elapsed time: 0:00:41.730532
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 14:50:27.418335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.84
 ---- batch: 020 ----
mean loss: 413.98
 ---- batch: 030 ----
mean loss: 410.59
train mean loss: 405.77
epoch train time: 0:00:00.517667
elapsed time: 0:00:42.248479
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 14:50:27.936284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.57
 ---- batch: 020 ----
mean loss: 404.31
 ---- batch: 030 ----
mean loss: 393.59
train mean loss: 400.20
epoch train time: 0:00:00.504370
elapsed time: 0:00:42.753126
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 14:50:28.440927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.62
 ---- batch: 020 ----
mean loss: 391.58
 ---- batch: 030 ----
mean loss: 395.54
train mean loss: 400.13
epoch train time: 0:00:00.504739
elapsed time: 0:00:43.258157
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 14:50:28.945959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.57
 ---- batch: 020 ----
mean loss: 396.04
 ---- batch: 030 ----
mean loss: 403.03
train mean loss: 395.95
epoch train time: 0:00:00.507721
elapsed time: 0:00:43.766175
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 14:50:29.453998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.58
 ---- batch: 020 ----
mean loss: 396.76
 ---- batch: 030 ----
mean loss: 391.82
train mean loss: 394.75
epoch train time: 0:00:00.505303
elapsed time: 0:00:44.271766
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 14:50:29.959564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.50
 ---- batch: 020 ----
mean loss: 395.23
 ---- batch: 030 ----
mean loss: 386.68
train mean loss: 388.79
epoch train time: 0:00:00.494495
elapsed time: 0:00:44.766518
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 14:50:30.454317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.98
 ---- batch: 020 ----
mean loss: 383.67
 ---- batch: 030 ----
mean loss: 389.74
train mean loss: 386.57
epoch train time: 0:00:00.506262
elapsed time: 0:00:45.273075
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 14:50:30.960878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.58
 ---- batch: 020 ----
mean loss: 378.91
 ---- batch: 030 ----
mean loss: 385.55
train mean loss: 386.30
epoch train time: 0:00:00.492379
elapsed time: 0:00:45.765723
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 14:50:31.453558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.92
 ---- batch: 020 ----
mean loss: 381.58
 ---- batch: 030 ----
mean loss: 385.84
train mean loss: 386.36
epoch train time: 0:00:00.496713
elapsed time: 0:00:46.262757
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 14:50:31.950591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.06
 ---- batch: 020 ----
mean loss: 379.66
 ---- batch: 030 ----
mean loss: 383.27
train mean loss: 381.82
epoch train time: 0:00:00.499672
elapsed time: 0:00:46.762738
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 14:50:32.450554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.25
 ---- batch: 020 ----
mean loss: 381.55
 ---- batch: 030 ----
mean loss: 377.80
train mean loss: 379.87
epoch train time: 0:00:00.518319
elapsed time: 0:00:47.281364
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 14:50:32.969171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.21
 ---- batch: 020 ----
mean loss: 381.45
 ---- batch: 030 ----
mean loss: 377.31
train mean loss: 378.88
epoch train time: 0:00:00.490483
elapsed time: 0:00:47.772112
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 14:50:33.459916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.47
 ---- batch: 020 ----
mean loss: 366.39
 ---- batch: 030 ----
mean loss: 376.92
train mean loss: 374.10
epoch train time: 0:00:00.518362
elapsed time: 0:00:48.290774
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 14:50:33.978581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.58
 ---- batch: 020 ----
mean loss: 375.76
 ---- batch: 030 ----
mean loss: 370.28
train mean loss: 375.37
epoch train time: 0:00:00.498015
elapsed time: 0:00:48.789061
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 14:50:34.476863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.21
 ---- batch: 020 ----
mean loss: 375.34
 ---- batch: 030 ----
mean loss: 374.76
train mean loss: 375.04
epoch train time: 0:00:00.502854
elapsed time: 0:00:49.292227
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 14:50:34.980033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.68
 ---- batch: 020 ----
mean loss: 376.15
 ---- batch: 030 ----
mean loss: 369.97
train mean loss: 371.74
epoch train time: 0:00:00.505715
elapsed time: 0:00:49.798263
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 14:50:35.486113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.72
 ---- batch: 020 ----
mean loss: 365.78
 ---- batch: 030 ----
mean loss: 372.83
train mean loss: 370.81
epoch train time: 0:00:00.501147
elapsed time: 0:00:50.299728
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 14:50:35.987533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.64
 ---- batch: 020 ----
mean loss: 373.36
 ---- batch: 030 ----
mean loss: 366.54
train mean loss: 368.38
epoch train time: 0:00:00.503287
elapsed time: 0:00:50.803306
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 14:50:36.491130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.28
 ---- batch: 020 ----
mean loss: 361.03
 ---- batch: 030 ----
mean loss: 364.95
train mean loss: 366.10
epoch train time: 0:00:00.506395
elapsed time: 0:00:51.310065
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 14:50:36.997923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.89
 ---- batch: 020 ----
mean loss: 363.22
 ---- batch: 030 ----
mean loss: 368.96
train mean loss: 363.31
epoch train time: 0:00:00.507845
elapsed time: 0:00:51.818255
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 14:50:37.506064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.52
 ---- batch: 020 ----
mean loss: 360.01
 ---- batch: 030 ----
mean loss: 357.89
train mean loss: 365.36
epoch train time: 0:00:00.521436
elapsed time: 0:00:52.339955
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 14:50:38.027767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.29
 ---- batch: 020 ----
mean loss: 360.42
 ---- batch: 030 ----
mean loss: 365.96
train mean loss: 362.60
epoch train time: 0:00:00.493669
elapsed time: 0:00:52.833934
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 14:50:38.521736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.48
 ---- batch: 020 ----
mean loss: 360.26
 ---- batch: 030 ----
mean loss: 371.95
train mean loss: 361.06
epoch train time: 0:00:00.520468
elapsed time: 0:00:53.354675
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 14:50:39.042480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.13
 ---- batch: 020 ----
mean loss: 363.54
 ---- batch: 030 ----
mean loss: 362.26
train mean loss: 361.76
epoch train time: 0:00:00.521647
elapsed time: 0:00:53.876666
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 14:50:39.564477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.14
 ---- batch: 020 ----
mean loss: 353.13
 ---- batch: 030 ----
mean loss: 360.49
train mean loss: 358.31
epoch train time: 0:00:00.509286
elapsed time: 0:00:54.386260
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 14:50:40.074062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.74
 ---- batch: 020 ----
mean loss: 359.61
 ---- batch: 030 ----
mean loss: 360.31
train mean loss: 358.85
epoch train time: 0:00:00.506518
elapsed time: 0:00:54.893041
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 14:50:40.580842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.31
 ---- batch: 020 ----
mean loss: 351.70
 ---- batch: 030 ----
mean loss: 356.56
train mean loss: 357.09
epoch train time: 0:00:00.499797
elapsed time: 0:00:55.393112
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 14:50:41.080908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.92
 ---- batch: 020 ----
mean loss: 357.98
 ---- batch: 030 ----
mean loss: 351.42
train mean loss: 354.42
epoch train time: 0:00:00.498389
elapsed time: 0:00:55.891758
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 14:50:41.579564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.24
 ---- batch: 020 ----
mean loss: 352.69
 ---- batch: 030 ----
mean loss: 349.49
train mean loss: 351.40
epoch train time: 0:00:00.521793
elapsed time: 0:00:56.413843
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 14:50:42.101667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.89
 ---- batch: 020 ----
mean loss: 363.49
 ---- batch: 030 ----
mean loss: 344.68
train mean loss: 353.01
epoch train time: 0:00:00.497146
elapsed time: 0:00:56.911304
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 14:50:42.599109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.19
 ---- batch: 020 ----
mean loss: 349.82
 ---- batch: 030 ----
mean loss: 349.27
train mean loss: 353.30
epoch train time: 0:00:00.500360
elapsed time: 0:00:57.411948
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 14:50:43.099781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.59
 ---- batch: 020 ----
mean loss: 348.83
 ---- batch: 030 ----
mean loss: 353.05
train mean loss: 350.17
epoch train time: 0:00:00.493715
elapsed time: 0:00:57.906015
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 14:50:43.593821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.82
 ---- batch: 020 ----
mean loss: 353.77
 ---- batch: 030 ----
mean loss: 353.19
train mean loss: 349.83
epoch train time: 0:00:00.513329
elapsed time: 0:00:58.419618
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 14:50:44.107416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.66
 ---- batch: 020 ----
mean loss: 346.39
 ---- batch: 030 ----
mean loss: 352.77
train mean loss: 347.58
epoch train time: 0:00:00.506561
elapsed time: 0:00:58.926496
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 14:50:44.614300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.98
 ---- batch: 020 ----
mean loss: 350.63
 ---- batch: 030 ----
mean loss: 342.90
train mean loss: 348.31
epoch train time: 0:00:00.501914
elapsed time: 0:00:59.428678
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 14:50:45.116484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.67
 ---- batch: 020 ----
mean loss: 343.91
 ---- batch: 030 ----
mean loss: 346.17
train mean loss: 347.55
epoch train time: 0:00:00.490699
elapsed time: 0:00:59.919638
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 14:50:45.607437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.60
 ---- batch: 020 ----
mean loss: 342.07
 ---- batch: 030 ----
mean loss: 354.64
train mean loss: 346.22
epoch train time: 0:00:00.519327
elapsed time: 0:01:00.439235
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 14:50:46.127040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.55
 ---- batch: 020 ----
mean loss: 340.66
 ---- batch: 030 ----
mean loss: 348.70
train mean loss: 343.82
epoch train time: 0:00:00.525178
elapsed time: 0:01:00.964678
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 14:50:46.652477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.54
 ---- batch: 020 ----
mean loss: 342.69
 ---- batch: 030 ----
mean loss: 350.26
train mean loss: 343.50
epoch train time: 0:00:00.502838
elapsed time: 0:01:01.467866
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 14:50:47.155711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.92
 ---- batch: 020 ----
mean loss: 339.70
 ---- batch: 030 ----
mean loss: 340.72
train mean loss: 342.79
epoch train time: 0:00:00.502472
elapsed time: 0:01:01.970660
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 14:50:47.658506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.78
 ---- batch: 020 ----
mean loss: 334.33
 ---- batch: 030 ----
mean loss: 337.33
train mean loss: 339.32
epoch train time: 0:00:00.503999
elapsed time: 0:01:02.475023
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 14:50:48.162889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.71
 ---- batch: 020 ----
mean loss: 335.54
 ---- batch: 030 ----
mean loss: 345.33
train mean loss: 339.60
epoch train time: 0:00:00.509755
elapsed time: 0:01:02.985160
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 14:50:48.672964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.32
 ---- batch: 020 ----
mean loss: 340.86
 ---- batch: 030 ----
mean loss: 333.12
train mean loss: 339.61
epoch train time: 0:00:00.515305
elapsed time: 0:01:03.500738
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 14:50:49.188543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.15
 ---- batch: 020 ----
mean loss: 340.81
 ---- batch: 030 ----
mean loss: 335.68
train mean loss: 336.60
epoch train time: 0:00:00.508297
elapsed time: 0:01:04.009324
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 14:50:49.697128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.80
 ---- batch: 020 ----
mean loss: 336.13
 ---- batch: 030 ----
mean loss: 335.71
train mean loss: 334.91
epoch train time: 0:00:00.510210
elapsed time: 0:01:04.519796
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 14:50:50.207604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.15
 ---- batch: 020 ----
mean loss: 339.73
 ---- batch: 030 ----
mean loss: 336.98
train mean loss: 336.10
epoch train time: 0:00:00.506598
elapsed time: 0:01:05.026749
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 14:50:50.714591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.00
 ---- batch: 020 ----
mean loss: 337.97
 ---- batch: 030 ----
mean loss: 334.82
train mean loss: 333.32
epoch train time: 0:00:00.500219
elapsed time: 0:01:05.527298
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 14:50:51.215098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.28
 ---- batch: 020 ----
mean loss: 335.93
 ---- batch: 030 ----
mean loss: 341.17
train mean loss: 334.25
epoch train time: 0:00:00.505304
elapsed time: 0:01:06.032885
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 14:50:51.720695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.83
 ---- batch: 020 ----
mean loss: 336.39
 ---- batch: 030 ----
mean loss: 333.40
train mean loss: 332.05
epoch train time: 0:00:00.505717
elapsed time: 0:01:06.538939
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 14:50:52.226714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.63
 ---- batch: 020 ----
mean loss: 324.74
 ---- batch: 030 ----
mean loss: 336.25
train mean loss: 332.87
epoch train time: 0:00:00.503810
elapsed time: 0:01:07.043040
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 14:50:52.730845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.24
 ---- batch: 020 ----
mean loss: 338.51
 ---- batch: 030 ----
mean loss: 326.14
train mean loss: 332.27
epoch train time: 0:00:00.511914
elapsed time: 0:01:07.555239
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 14:50:53.243046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.73
 ---- batch: 020 ----
mean loss: 326.08
 ---- batch: 030 ----
mean loss: 333.95
train mean loss: 331.62
epoch train time: 0:00:00.517778
elapsed time: 0:01:08.073316
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 14:50:53.761137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.40
 ---- batch: 020 ----
mean loss: 329.33
 ---- batch: 030 ----
mean loss: 330.08
train mean loss: 328.86
epoch train time: 0:00:00.519056
elapsed time: 0:01:08.592676
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 14:50:54.280483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.07
 ---- batch: 020 ----
mean loss: 324.97
 ---- batch: 030 ----
mean loss: 331.47
train mean loss: 328.19
epoch train time: 0:00:00.523348
elapsed time: 0:01:09.116319
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 14:50:54.804129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.38
 ---- batch: 020 ----
mean loss: 326.24
 ---- batch: 030 ----
mean loss: 325.99
train mean loss: 328.72
epoch train time: 0:00:00.507604
elapsed time: 0:01:09.624207
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 14:50:55.312013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.51
 ---- batch: 020 ----
mean loss: 324.90
 ---- batch: 030 ----
mean loss: 328.58
train mean loss: 325.48
epoch train time: 0:00:00.512442
elapsed time: 0:01:10.136939
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 14:50:55.824749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.58
 ---- batch: 020 ----
mean loss: 328.54
 ---- batch: 030 ----
mean loss: 332.71
train mean loss: 325.72
epoch train time: 0:00:00.512095
elapsed time: 0:01:10.649321
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 14:50:56.337123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.18
 ---- batch: 020 ----
mean loss: 328.13
 ---- batch: 030 ----
mean loss: 324.16
train mean loss: 325.66
epoch train time: 0:00:00.515274
elapsed time: 0:01:11.164862
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 14:50:56.852678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.73
 ---- batch: 020 ----
mean loss: 326.24
 ---- batch: 030 ----
mean loss: 326.47
train mean loss: 323.25
epoch train time: 0:00:00.496638
elapsed time: 0:01:11.661801
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 14:50:57.349635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.92
 ---- batch: 020 ----
mean loss: 329.97
 ---- batch: 030 ----
mean loss: 319.61
train mean loss: 322.33
epoch train time: 0:00:00.505333
elapsed time: 0:01:12.167486
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 14:50:57.855312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.25
 ---- batch: 020 ----
mean loss: 326.92
 ---- batch: 030 ----
mean loss: 322.64
train mean loss: 323.58
epoch train time: 0:00:00.505582
elapsed time: 0:01:12.673373
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 14:50:58.361175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.22
 ---- batch: 020 ----
mean loss: 318.33
 ---- batch: 030 ----
mean loss: 321.14
train mean loss: 321.33
epoch train time: 0:00:00.514408
elapsed time: 0:01:13.188060
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 14:50:58.875893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.89
 ---- batch: 020 ----
mean loss: 316.58
 ---- batch: 030 ----
mean loss: 317.25
train mean loss: 318.66
epoch train time: 0:00:00.501282
elapsed time: 0:01:13.689638
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 14:50:59.377441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.39
 ---- batch: 020 ----
mean loss: 317.45
 ---- batch: 030 ----
mean loss: 324.82
train mean loss: 319.30
epoch train time: 0:00:00.504380
elapsed time: 0:01:14.194296
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 14:50:59.882101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.33
 ---- batch: 020 ----
mean loss: 317.87
 ---- batch: 030 ----
mean loss: 323.20
train mean loss: 319.39
epoch train time: 0:00:00.499372
elapsed time: 0:01:14.693973
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 14:51:00.381811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.19
 ---- batch: 020 ----
mean loss: 318.49
 ---- batch: 030 ----
mean loss: 317.46
train mean loss: 317.89
epoch train time: 0:00:00.512051
elapsed time: 0:01:15.206374
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 14:51:00.894182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.87
 ---- batch: 020 ----
mean loss: 314.95
 ---- batch: 030 ----
mean loss: 319.16
train mean loss: 318.84
epoch train time: 0:00:00.514924
elapsed time: 0:01:15.721606
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 14:51:01.409412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.21
 ---- batch: 020 ----
mean loss: 313.36
 ---- batch: 030 ----
mean loss: 309.77
train mean loss: 317.17
epoch train time: 0:00:00.517477
elapsed time: 0:01:16.239361
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 14:51:01.927168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.04
 ---- batch: 020 ----
mean loss: 323.42
 ---- batch: 030 ----
mean loss: 317.04
train mean loss: 317.67
epoch train time: 0:00:00.499301
elapsed time: 0:01:16.738981
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 14:51:02.426747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.65
 ---- batch: 020 ----
mean loss: 315.62
 ---- batch: 030 ----
mean loss: 317.37
train mean loss: 314.70
epoch train time: 0:00:00.501678
elapsed time: 0:01:17.240899
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 14:51:02.928703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.38
 ---- batch: 020 ----
mean loss: 316.12
 ---- batch: 030 ----
mean loss: 313.21
train mean loss: 313.77
epoch train time: 0:00:00.515423
elapsed time: 0:01:17.756706
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 14:51:03.444559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.03
 ---- batch: 020 ----
mean loss: 313.95
 ---- batch: 030 ----
mean loss: 313.98
train mean loss: 312.95
epoch train time: 0:00:00.524616
elapsed time: 0:01:18.281659
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 14:51:03.969475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.24
 ---- batch: 020 ----
mean loss: 312.62
 ---- batch: 030 ----
mean loss: 322.20
train mean loss: 312.99
epoch train time: 0:00:00.515825
elapsed time: 0:01:18.797774
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 14:51:04.485578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.00
 ---- batch: 020 ----
mean loss: 320.02
 ---- batch: 030 ----
mean loss: 307.93
train mean loss: 311.92
epoch train time: 0:00:00.500901
elapsed time: 0:01:19.299017
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 14:51:04.986835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.71
 ---- batch: 020 ----
mean loss: 314.06
 ---- batch: 030 ----
mean loss: 315.19
train mean loss: 310.81
epoch train time: 0:00:00.499523
elapsed time: 0:01:19.798830
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 14:51:05.486634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.30
 ---- batch: 020 ----
mean loss: 305.69
 ---- batch: 030 ----
mean loss: 303.50
train mean loss: 309.44
epoch train time: 0:00:00.526329
elapsed time: 0:01:20.325450
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 14:51:06.013248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.70
 ---- batch: 020 ----
mean loss: 305.73
 ---- batch: 030 ----
mean loss: 313.80
train mean loss: 309.47
epoch train time: 0:00:00.509496
elapsed time: 0:01:20.835225
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 14:51:06.523028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.60
 ---- batch: 020 ----
mean loss: 310.12
 ---- batch: 030 ----
mean loss: 314.85
train mean loss: 308.99
epoch train time: 0:00:00.515504
elapsed time: 0:01:21.351005
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 14:51:07.038828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.89
 ---- batch: 020 ----
mean loss: 306.28
 ---- batch: 030 ----
mean loss: 303.95
train mean loss: 308.82
epoch train time: 0:00:00.506769
elapsed time: 0:01:21.858090
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 14:51:07.545888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.81
 ---- batch: 020 ----
mean loss: 307.06
 ---- batch: 030 ----
mean loss: 309.11
train mean loss: 307.07
epoch train time: 0:00:00.510736
elapsed time: 0:01:22.369111
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 14:51:08.056912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.28
 ---- batch: 020 ----
mean loss: 306.42
 ---- batch: 030 ----
mean loss: 307.84
train mean loss: 307.22
epoch train time: 0:00:00.512410
elapsed time: 0:01:22.881791
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 14:51:08.569595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.18
 ---- batch: 020 ----
mean loss: 305.70
 ---- batch: 030 ----
mean loss: 300.22
train mean loss: 306.95
epoch train time: 0:00:00.520190
elapsed time: 0:01:23.402264
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 14:51:09.090067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.95
 ---- batch: 020 ----
mean loss: 309.29
 ---- batch: 030 ----
mean loss: 298.75
train mean loss: 304.79
epoch train time: 0:00:00.503863
elapsed time: 0:01:23.906390
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 14:51:09.594191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.65
 ---- batch: 020 ----
mean loss: 302.55
 ---- batch: 030 ----
mean loss: 303.82
train mean loss: 305.63
epoch train time: 0:00:00.511054
elapsed time: 0:01:24.417738
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 14:51:10.105569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.60
 ---- batch: 020 ----
mean loss: 306.21
 ---- batch: 030 ----
mean loss: 304.81
train mean loss: 303.81
epoch train time: 0:00:00.516877
elapsed time: 0:01:24.934998
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 14:51:10.622839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.63
 ---- batch: 020 ----
mean loss: 308.64
 ---- batch: 030 ----
mean loss: 303.32
train mean loss: 303.54
epoch train time: 0:00:00.523373
elapsed time: 0:01:25.458674
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 14:51:11.146473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.16
 ---- batch: 020 ----
mean loss: 300.90
 ---- batch: 030 ----
mean loss: 304.63
train mean loss: 302.02
epoch train time: 0:00:00.496415
elapsed time: 0:01:25.955418
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 14:51:11.643311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.84
 ---- batch: 020 ----
mean loss: 306.31
 ---- batch: 030 ----
mean loss: 298.97
train mean loss: 302.26
epoch train time: 0:00:00.532996
elapsed time: 0:01:26.488781
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 14:51:12.176585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.35
 ---- batch: 020 ----
mean loss: 302.40
 ---- batch: 030 ----
mean loss: 299.60
train mean loss: 300.94
epoch train time: 0:00:00.511398
elapsed time: 0:01:27.000461
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 14:51:12.688265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.15
 ---- batch: 020 ----
mean loss: 299.20
 ---- batch: 030 ----
mean loss: 299.13
train mean loss: 300.05
epoch train time: 0:00:00.522843
elapsed time: 0:01:27.523599
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 14:51:13.211405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.69
 ---- batch: 020 ----
mean loss: 297.29
 ---- batch: 030 ----
mean loss: 303.32
train mean loss: 299.59
epoch train time: 0:00:00.505346
elapsed time: 0:01:28.029285
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 14:51:13.717043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.52
 ---- batch: 020 ----
mean loss: 295.48
 ---- batch: 030 ----
mean loss: 305.75
train mean loss: 298.00
epoch train time: 0:00:00.518576
elapsed time: 0:01:28.548094
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 14:51:14.235897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.24
 ---- batch: 020 ----
mean loss: 304.60
 ---- batch: 030 ----
mean loss: 292.61
train mean loss: 297.84
epoch train time: 0:00:00.507696
elapsed time: 0:01:29.056066
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 14:51:14.743873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.09
 ---- batch: 020 ----
mean loss: 301.92
 ---- batch: 030 ----
mean loss: 294.04
train mean loss: 296.95
epoch train time: 0:00:00.512894
elapsed time: 0:01:29.569260
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 14:51:15.257069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.08
 ---- batch: 020 ----
mean loss: 296.12
 ---- batch: 030 ----
mean loss: 291.22
train mean loss: 296.02
epoch train time: 0:00:00.511756
elapsed time: 0:01:30.081311
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 14:51:15.769112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.93
 ---- batch: 020 ----
mean loss: 295.36
 ---- batch: 030 ----
mean loss: 300.82
train mean loss: 296.49
epoch train time: 0:00:00.506298
elapsed time: 0:01:30.587905
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 14:51:16.275720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.53
 ---- batch: 020 ----
mean loss: 296.73
 ---- batch: 030 ----
mean loss: 298.51
train mean loss: 295.36
epoch train time: 0:00:00.507511
elapsed time: 0:01:31.095699
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 14:51:16.783533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.98
 ---- batch: 020 ----
mean loss: 289.79
 ---- batch: 030 ----
mean loss: 291.08
train mean loss: 293.64
epoch train time: 0:00:00.519944
elapsed time: 0:01:31.615961
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 14:51:17.303769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.67
 ---- batch: 020 ----
mean loss: 296.48
 ---- batch: 030 ----
mean loss: 290.03
train mean loss: 294.83
epoch train time: 0:00:00.540345
elapsed time: 0:01:32.156639
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 14:51:17.844452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.30
 ---- batch: 020 ----
mean loss: 298.58
 ---- batch: 030 ----
mean loss: 293.11
train mean loss: 293.87
epoch train time: 0:00:00.511467
elapsed time: 0:01:32.668408
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 14:51:18.356215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.96
 ---- batch: 020 ----
mean loss: 288.55
 ---- batch: 030 ----
mean loss: 294.23
train mean loss: 292.32
epoch train time: 0:00:00.505866
elapsed time: 0:01:33.174559
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 14:51:18.862395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.08
 ---- batch: 020 ----
mean loss: 292.48
 ---- batch: 030 ----
mean loss: 291.86
train mean loss: 291.81
epoch train time: 0:00:00.503621
elapsed time: 0:01:33.678486
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 14:51:19.366290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.57
 ---- batch: 020 ----
mean loss: 296.45
 ---- batch: 030 ----
mean loss: 287.47
train mean loss: 289.91
epoch train time: 0:00:00.523017
elapsed time: 0:01:34.201784
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 14:51:19.889591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.14
 ---- batch: 020 ----
mean loss: 289.80
 ---- batch: 030 ----
mean loss: 295.71
train mean loss: 291.82
epoch train time: 0:00:00.522471
elapsed time: 0:01:34.724551
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 14:51:20.412361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.21
 ---- batch: 020 ----
mean loss: 290.57
 ---- batch: 030 ----
mean loss: 294.06
train mean loss: 289.69
epoch train time: 0:00:00.524675
elapsed time: 0:01:35.249524
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 14:51:20.937325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.60
 ---- batch: 020 ----
mean loss: 293.13
 ---- batch: 030 ----
mean loss: 286.88
train mean loss: 288.31
epoch train time: 0:00:00.496979
elapsed time: 0:01:35.746760
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 14:51:21.434558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.90
 ---- batch: 020 ----
mean loss: 292.77
 ---- batch: 030 ----
mean loss: 287.72
train mean loss: 288.03
epoch train time: 0:00:00.508963
elapsed time: 0:01:36.256008
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 14:51:21.943812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.52
 ---- batch: 020 ----
mean loss: 288.86
 ---- batch: 030 ----
mean loss: 284.64
train mean loss: 287.03
epoch train time: 0:00:00.499727
elapsed time: 0:01:36.756009
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 14:51:22.443826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.43
 ---- batch: 020 ----
mean loss: 289.16
 ---- batch: 030 ----
mean loss: 287.65
train mean loss: 287.09
epoch train time: 0:00:00.513859
elapsed time: 0:01:37.270180
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 14:51:22.957995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.17
 ---- batch: 020 ----
mean loss: 280.84
 ---- batch: 030 ----
mean loss: 292.29
train mean loss: 286.37
epoch train time: 0:00:00.506784
elapsed time: 0:01:37.777255
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 14:51:23.465055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.72
 ---- batch: 020 ----
mean loss: 283.46
 ---- batch: 030 ----
mean loss: 285.99
train mean loss: 285.34
epoch train time: 0:00:00.509151
elapsed time: 0:01:38.286674
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 14:51:23.974471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.35
 ---- batch: 020 ----
mean loss: 286.43
 ---- batch: 030 ----
mean loss: 280.96
train mean loss: 284.68
epoch train time: 0:00:00.497333
elapsed time: 0:01:38.784274
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 14:51:24.472095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.99
 ---- batch: 020 ----
mean loss: 284.06
 ---- batch: 030 ----
mean loss: 283.33
train mean loss: 284.56
epoch train time: 0:00:00.513347
elapsed time: 0:01:39.297930
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 14:51:24.985748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.76
 ---- batch: 020 ----
mean loss: 287.85
 ---- batch: 030 ----
mean loss: 289.34
train mean loss: 283.78
epoch train time: 0:00:00.507259
elapsed time: 0:01:39.805504
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 14:51:25.493338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.15
 ---- batch: 020 ----
mean loss: 277.21
 ---- batch: 030 ----
mean loss: 285.22
train mean loss: 283.13
epoch train time: 0:00:00.505780
elapsed time: 0:01:40.311589
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 14:51:25.999403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.83
 ---- batch: 020 ----
mean loss: 286.51
 ---- batch: 030 ----
mean loss: 276.25
train mean loss: 281.82
epoch train time: 0:00:00.497567
elapsed time: 0:01:40.809517
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 14:51:26.497287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.59
 ---- batch: 020 ----
mean loss: 285.24
 ---- batch: 030 ----
mean loss: 283.87
train mean loss: 281.79
epoch train time: 0:00:00.503803
elapsed time: 0:01:41.313582
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 14:51:27.001417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.06
 ---- batch: 020 ----
mean loss: 282.09
 ---- batch: 030 ----
mean loss: 283.22
train mean loss: 281.68
epoch train time: 0:00:00.509603
elapsed time: 0:01:41.823522
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 14:51:27.511329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.42
 ---- batch: 020 ----
mean loss: 274.54
 ---- batch: 030 ----
mean loss: 288.41
train mean loss: 280.52
epoch train time: 0:00:00.521454
elapsed time: 0:01:42.345313
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 14:51:28.033128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.50
 ---- batch: 020 ----
mean loss: 276.59
 ---- batch: 030 ----
mean loss: 279.09
train mean loss: 280.24
epoch train time: 0:00:00.495768
elapsed time: 0:01:42.841355
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 14:51:28.529185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.45
 ---- batch: 020 ----
mean loss: 286.62
 ---- batch: 030 ----
mean loss: 272.61
train mean loss: 279.04
epoch train time: 0:00:00.517495
elapsed time: 0:01:43.359153
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 14:51:29.046954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.77
 ---- batch: 020 ----
mean loss: 277.83
 ---- batch: 030 ----
mean loss: 282.64
train mean loss: 278.38
epoch train time: 0:00:00.500244
elapsed time: 0:01:43.859694
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 14:51:29.547546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.46
 ---- batch: 020 ----
mean loss: 280.46
 ---- batch: 030 ----
mean loss: 274.08
train mean loss: 277.82
epoch train time: 0:00:00.511147
elapsed time: 0:01:44.371173
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 14:51:30.058978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.83
 ---- batch: 020 ----
mean loss: 276.46
 ---- batch: 030 ----
mean loss: 277.86
train mean loss: 276.89
epoch train time: 0:00:00.519175
elapsed time: 0:01:44.890617
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 14:51:30.578425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.93
 ---- batch: 020 ----
mean loss: 275.83
 ---- batch: 030 ----
mean loss: 276.40
train mean loss: 276.67
epoch train time: 0:00:00.515791
elapsed time: 0:01:45.406708
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 14:51:31.094517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.87
 ---- batch: 020 ----
mean loss: 273.93
 ---- batch: 030 ----
mean loss: 276.33
train mean loss: 276.11
epoch train time: 0:00:00.520503
elapsed time: 0:01:45.927537
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 14:51:31.615355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.13
 ---- batch: 020 ----
mean loss: 270.42
 ---- batch: 030 ----
mean loss: 274.73
train mean loss: 275.65
epoch train time: 0:00:00.518177
elapsed time: 0:01:46.446025
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 14:51:32.133847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.35
 ---- batch: 020 ----
mean loss: 283.73
 ---- batch: 030 ----
mean loss: 265.69
train mean loss: 274.86
epoch train time: 0:00:00.502018
elapsed time: 0:01:46.948342
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 14:51:32.636148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.09
 ---- batch: 020 ----
mean loss: 279.69
 ---- batch: 030 ----
mean loss: 270.98
train mean loss: 274.95
epoch train time: 0:00:00.516501
elapsed time: 0:01:47.465166
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 14:51:33.152971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.71
 ---- batch: 020 ----
mean loss: 281.69
 ---- batch: 030 ----
mean loss: 274.08
train mean loss: 273.69
epoch train time: 0:00:00.503943
elapsed time: 0:01:47.969423
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 14:51:33.657231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.68
 ---- batch: 020 ----
mean loss: 267.62
 ---- batch: 030 ----
mean loss: 278.05
train mean loss: 272.61
epoch train time: 0:00:00.517309
elapsed time: 0:01:48.487027
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 14:51:34.174838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.47
 ---- batch: 020 ----
mean loss: 276.33
 ---- batch: 030 ----
mean loss: 275.82
train mean loss: 272.13
epoch train time: 0:00:00.529061
elapsed time: 0:01:49.016401
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 14:51:34.704215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.09
 ---- batch: 020 ----
mean loss: 276.13
 ---- batch: 030 ----
mean loss: 269.67
train mean loss: 271.22
epoch train time: 0:00:00.532697
elapsed time: 0:01:49.549389
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 14:51:35.237203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.33
 ---- batch: 020 ----
mean loss: 271.94
 ---- batch: 030 ----
mean loss: 271.80
train mean loss: 270.84
epoch train time: 0:00:00.506851
elapsed time: 0:01:50.056548
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 14:51:35.744351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.58
 ---- batch: 020 ----
mean loss: 276.23
 ---- batch: 030 ----
mean loss: 265.16
train mean loss: 271.15
epoch train time: 0:00:00.511163
elapsed time: 0:01:50.567990
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 14:51:36.255810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.65
 ---- batch: 020 ----
mean loss: 271.95
 ---- batch: 030 ----
mean loss: 268.21
train mean loss: 269.60
epoch train time: 0:00:00.515807
elapsed time: 0:01:51.084106
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 14:51:36.771911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.88
 ---- batch: 020 ----
mean loss: 272.83
 ---- batch: 030 ----
mean loss: 268.94
train mean loss: 269.35
epoch train time: 0:00:00.520095
elapsed time: 0:01:51.604502
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 14:51:37.292340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.24
 ---- batch: 020 ----
mean loss: 266.04
 ---- batch: 030 ----
mean loss: 269.22
train mean loss: 268.59
epoch train time: 0:00:00.523747
elapsed time: 0:01:52.128563
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 14:51:37.816367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.63
 ---- batch: 020 ----
mean loss: 267.17
 ---- batch: 030 ----
mean loss: 265.60
train mean loss: 267.82
epoch train time: 0:00:00.504052
elapsed time: 0:01:52.632887
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 14:51:38.320711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.08
 ---- batch: 020 ----
mean loss: 276.88
 ---- batch: 030 ----
mean loss: 268.83
train mean loss: 268.99
epoch train time: 0:00:00.512225
elapsed time: 0:01:53.145426
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 14:51:38.833231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.42
 ---- batch: 020 ----
mean loss: 267.04
 ---- batch: 030 ----
mean loss: 266.44
train mean loss: 267.01
epoch train time: 0:00:00.517590
elapsed time: 0:01:53.663308
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 14:51:39.351112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.51
 ---- batch: 020 ----
mean loss: 266.40
 ---- batch: 030 ----
mean loss: 270.06
train mean loss: 266.49
epoch train time: 0:00:00.510423
elapsed time: 0:01:54.174571
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 14:51:39.862378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.59
 ---- batch: 020 ----
mean loss: 269.03
 ---- batch: 030 ----
mean loss: 266.07
train mean loss: 266.40
epoch train time: 0:00:00.495461
elapsed time: 0:01:54.670306
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 14:51:40.358110
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 267.43
 ---- batch: 020 ----
mean loss: 263.70
 ---- batch: 030 ----
mean loss: 267.22
train mean loss: 265.40
epoch train time: 0:00:00.500074
elapsed time: 0:01:55.170721
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 14:51:40.858478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.56
 ---- batch: 020 ----
mean loss: 268.25
 ---- batch: 030 ----
mean loss: 268.36
train mean loss: 264.69
epoch train time: 0:00:00.521913
elapsed time: 0:01:55.692869
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 14:51:41.380670
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 271.16
 ---- batch: 020 ----
mean loss: 269.16
 ---- batch: 030 ----
mean loss: 257.08
train mean loss: 265.41
epoch train time: 0:00:00.534529
elapsed time: 0:01:56.227708
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 14:51:41.915555
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.58
 ---- batch: 020 ----
mean loss: 260.58
 ---- batch: 030 ----
mean loss: 270.95
train mean loss: 265.25
epoch train time: 0:00:00.509980
elapsed time: 0:01:56.738038
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 14:51:42.425849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 267.46
 ---- batch: 020 ----
mean loss: 264.68
 ---- batch: 030 ----
mean loss: 265.89
train mean loss: 265.54
epoch train time: 0:00:00.506791
elapsed time: 0:01:57.245123
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 14:51:42.932932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.12
 ---- batch: 020 ----
mean loss: 265.35
 ---- batch: 030 ----
mean loss: 259.63
train mean loss: 265.04
epoch train time: 0:00:00.498934
elapsed time: 0:01:57.744363
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 14:51:43.432163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.80
 ---- batch: 020 ----
mean loss: 263.24
 ---- batch: 030 ----
mean loss: 269.79
train mean loss: 264.92
epoch train time: 0:00:00.505024
elapsed time: 0:01:58.249670
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 14:51:43.937489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.33
 ---- batch: 020 ----
mean loss: 260.82
 ---- batch: 030 ----
mean loss: 265.53
train mean loss: 264.50
epoch train time: 0:00:00.513572
elapsed time: 0:01:58.763619
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 14:51:44.451449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.38
 ---- batch: 020 ----
mean loss: 271.34
 ---- batch: 030 ----
mean loss: 264.69
train mean loss: 264.45
epoch train time: 0:00:00.511744
elapsed time: 0:01:59.275718
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 14:51:44.963553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.29
 ---- batch: 020 ----
mean loss: 264.00
 ---- batch: 030 ----
mean loss: 270.41
train mean loss: 265.59
epoch train time: 0:00:00.493490
elapsed time: 0:01:59.769526
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 14:51:45.457330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.28
 ---- batch: 020 ----
mean loss: 265.46
 ---- batch: 030 ----
mean loss: 270.79
train mean loss: 264.51
epoch train time: 0:00:00.498098
elapsed time: 0:02:00.267946
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 14:51:45.955749
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.59
 ---- batch: 020 ----
mean loss: 270.85
 ---- batch: 030 ----
mean loss: 264.37
train mean loss: 264.41
epoch train time: 0:00:00.504415
elapsed time: 0:02:00.772653
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 14:51:46.460489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 274.52
 ---- batch: 020 ----
mean loss: 260.57
 ---- batch: 030 ----
mean loss: 259.63
train mean loss: 264.98
epoch train time: 0:00:00.507275
elapsed time: 0:02:01.280246
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 14:51:46.968060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.08
 ---- batch: 020 ----
mean loss: 264.76
 ---- batch: 030 ----
mean loss: 261.25
train mean loss: 265.58
epoch train time: 0:00:00.499128
elapsed time: 0:02:01.779683
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 14:51:47.467499
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.65
 ---- batch: 020 ----
mean loss: 261.08
 ---- batch: 030 ----
mean loss: 262.58
train mean loss: 263.54
epoch train time: 0:00:00.507589
elapsed time: 0:02:02.287566
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 14:51:47.975371
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.52
 ---- batch: 020 ----
mean loss: 264.90
 ---- batch: 030 ----
mean loss: 261.88
train mean loss: 263.75
epoch train time: 0:00:00.507037
elapsed time: 0:02:02.794893
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 14:51:48.482718
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.41
 ---- batch: 020 ----
mean loss: 260.93
 ---- batch: 030 ----
mean loss: 268.39
train mean loss: 264.90
epoch train time: 0:00:00.519133
elapsed time: 0:02:03.314333
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 14:51:49.002141
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.57
 ---- batch: 020 ----
mean loss: 260.48
 ---- batch: 030 ----
mean loss: 263.31
train mean loss: 264.33
epoch train time: 0:00:00.520720
elapsed time: 0:02:03.835393
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 14:51:49.523203
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.26
 ---- batch: 020 ----
mean loss: 265.66
 ---- batch: 030 ----
mean loss: 263.72
train mean loss: 264.23
epoch train time: 0:00:00.515898
elapsed time: 0:02:04.351581
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 14:51:50.039389
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.75
 ---- batch: 020 ----
mean loss: 264.97
 ---- batch: 030 ----
mean loss: 259.70
train mean loss: 264.08
epoch train time: 0:00:00.515783
elapsed time: 0:02:04.867652
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 14:51:50.555459
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 263.20
 ---- batch: 020 ----
mean loss: 264.65
 ---- batch: 030 ----
mean loss: 264.28
train mean loss: 264.26
epoch train time: 0:00:00.513995
elapsed time: 0:02:05.381990
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 14:51:51.069808
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 269.86
 ---- batch: 020 ----
mean loss: 254.73
 ---- batch: 030 ----
mean loss: 266.09
train mean loss: 264.00
epoch train time: 0:00:00.500823
elapsed time: 0:02:05.883101
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 14:51:51.570904
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.07
 ---- batch: 020 ----
mean loss: 265.65
 ---- batch: 030 ----
mean loss: 264.03
train mean loss: 264.79
epoch train time: 0:00:00.521135
elapsed time: 0:02:06.404503
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 14:51:52.092323
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 263.31
 ---- batch: 020 ----
mean loss: 264.59
 ---- batch: 030 ----
mean loss: 266.40
train mean loss: 263.84
epoch train time: 0:00:00.506983
elapsed time: 0:02:06.911769
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 14:51:52.599572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.00
 ---- batch: 020 ----
mean loss: 265.85
 ---- batch: 030 ----
mean loss: 261.61
train mean loss: 263.93
epoch train time: 0:00:00.517690
elapsed time: 0:02:07.429741
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 14:51:53.117548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.31
 ---- batch: 020 ----
mean loss: 262.85
 ---- batch: 030 ----
mean loss: 260.14
train mean loss: 263.09
epoch train time: 0:00:00.517695
elapsed time: 0:02:07.947721
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 14:51:53.635558
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.39
 ---- batch: 020 ----
mean loss: 263.83
 ---- batch: 030 ----
mean loss: 262.14
train mean loss: 263.52
epoch train time: 0:00:00.522801
elapsed time: 0:02:08.470876
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 14:51:54.158684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.38
 ---- batch: 020 ----
mean loss: 268.37
 ---- batch: 030 ----
mean loss: 263.77
train mean loss: 263.63
epoch train time: 0:00:00.504537
elapsed time: 0:02:08.975734
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 14:51:54.663552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.62
 ---- batch: 020 ----
mean loss: 263.86
 ---- batch: 030 ----
mean loss: 262.71
train mean loss: 264.10
epoch train time: 0:00:00.503053
elapsed time: 0:02:09.479073
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 14:51:55.166877
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 263.50
 ---- batch: 020 ----
mean loss: 270.63
 ---- batch: 030 ----
mean loss: 258.40
train mean loss: 263.55
epoch train time: 0:00:00.504532
elapsed time: 0:02:09.983926
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 14:51:55.671732
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.79
 ---- batch: 020 ----
mean loss: 257.18
 ---- batch: 030 ----
mean loss: 267.32
train mean loss: 263.18
epoch train time: 0:00:00.511178
elapsed time: 0:02:10.495395
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 14:51:56.183196
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 263.18
 ---- batch: 020 ----
mean loss: 259.98
 ---- batch: 030 ----
mean loss: 267.68
train mean loss: 263.24
epoch train time: 0:00:00.509678
elapsed time: 0:02:11.005359
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 14:51:56.693177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.34
 ---- batch: 020 ----
mean loss: 267.77
 ---- batch: 030 ----
mean loss: 264.46
train mean loss: 262.98
epoch train time: 0:00:00.523868
elapsed time: 0:02:11.529565
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 14:51:57.217319
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.14
 ---- batch: 020 ----
mean loss: 260.83
 ---- batch: 030 ----
mean loss: 263.50
train mean loss: 263.26
epoch train time: 0:00:00.500518
elapsed time: 0:02:12.030346
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 14:51:57.718152
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.51
 ---- batch: 020 ----
mean loss: 260.46
 ---- batch: 030 ----
mean loss: 264.56
train mean loss: 262.38
epoch train time: 0:00:00.512117
elapsed time: 0:02:12.542770
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 14:51:58.230579
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 263.61
 ---- batch: 020 ----
mean loss: 265.25
 ---- batch: 030 ----
mean loss: 260.53
train mean loss: 263.74
epoch train time: 0:00:00.514691
elapsed time: 0:02:13.057779
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 14:51:58.745621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.38
 ---- batch: 020 ----
mean loss: 268.00
 ---- batch: 030 ----
mean loss: 263.39
train mean loss: 263.43
epoch train time: 0:00:00.513235
elapsed time: 0:02:13.571400
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 14:51:59.259291
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.77
 ---- batch: 020 ----
mean loss: 264.52
 ---- batch: 030 ----
mean loss: 254.89
train mean loss: 262.98
epoch train time: 0:00:00.502766
elapsed time: 0:02:14.074518
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 14:51:59.762319
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.88
 ---- batch: 020 ----
mean loss: 271.08
 ---- batch: 030 ----
mean loss: 261.03
train mean loss: 263.54
epoch train time: 0:00:00.517910
elapsed time: 0:02:14.592715
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 14:52:00.280526
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.67
 ---- batch: 020 ----
mean loss: 259.02
 ---- batch: 030 ----
mean loss: 263.45
train mean loss: 262.10
epoch train time: 0:00:00.510564
elapsed time: 0:02:15.103571
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 14:52:00.791380
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 270.72
 ---- batch: 020 ----
mean loss: 261.67
 ---- batch: 030 ----
mean loss: 261.61
train mean loss: 263.74
epoch train time: 0:00:00.519646
elapsed time: 0:02:15.623609
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 14:52:01.311411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.20
 ---- batch: 020 ----
mean loss: 258.45
 ---- batch: 030 ----
mean loss: 264.88
train mean loss: 262.86
epoch train time: 0:00:00.528808
elapsed time: 0:02:16.152696
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 14:52:01.840498
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.07
 ---- batch: 020 ----
mean loss: 268.07
 ---- batch: 030 ----
mean loss: 260.80
train mean loss: 263.06
epoch train time: 0:00:00.499941
elapsed time: 0:02:16.652932
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 14:52:02.340733
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 265.97
 ---- batch: 020 ----
mean loss: 265.58
 ---- batch: 030 ----
mean loss: 255.90
train mean loss: 263.12
epoch train time: 0:00:00.520100
elapsed time: 0:02:17.173344
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 14:52:02.861146
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 267.88
 ---- batch: 020 ----
mean loss: 259.73
 ---- batch: 030 ----
mean loss: 262.19
train mean loss: 262.93
epoch train time: 0:00:00.502285
elapsed time: 0:02:17.675898
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 14:52:03.363710
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.36
 ---- batch: 020 ----
mean loss: 269.09
 ---- batch: 030 ----
mean loss: 257.23
train mean loss: 261.92
epoch train time: 0:00:00.509219
elapsed time: 0:02:18.185429
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 14:52:03.873244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.92
 ---- batch: 020 ----
mean loss: 264.12
 ---- batch: 030 ----
mean loss: 256.20
train mean loss: 262.37
epoch train time: 0:00:00.519995
elapsed time: 0:02:18.705744
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 14:52:04.393566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 264.97
 ---- batch: 020 ----
mean loss: 259.00
 ---- batch: 030 ----
mean loss: 265.02
train mean loss: 261.97
epoch train time: 0:00:00.523780
elapsed time: 0:02:19.229831
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 14:52:04.917662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.90
 ---- batch: 020 ----
mean loss: 260.41
 ---- batch: 030 ----
mean loss: 264.70
train mean loss: 262.76
epoch train time: 0:00:00.512042
elapsed time: 0:02:19.745810
checkpoint saved in file: log/CMAPSS/FD001/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_4/checkpoint.pth.tar
**** end time: 2019-09-27 14:52:05.433538 ****
