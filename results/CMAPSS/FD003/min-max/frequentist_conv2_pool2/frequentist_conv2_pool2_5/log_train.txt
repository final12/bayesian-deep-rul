Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_5', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8121
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-20 23:30:47.193422 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:30:47.198382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4775.52
 ---- batch: 020 ----
mean loss: 4740.38
 ---- batch: 030 ----
mean loss: 4767.08
 ---- batch: 040 ----
mean loss: 4661.26
train mean loss: 4726.00
epoch train time: 0:00:14.996179
elapsed time: 0:00:15.002391
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:31:02.195857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4630.15
 ---- batch: 020 ----
mean loss: 4505.00
 ---- batch: 030 ----
mean loss: 4416.38
 ---- batch: 040 ----
mean loss: 4442.23
train mean loss: 4485.03
epoch train time: 0:00:00.208168
elapsed time: 0:00:15.210681
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:31:02.404159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4234.95
 ---- batch: 020 ----
mean loss: 4185.71
 ---- batch: 030 ----
mean loss: 4162.89
 ---- batch: 040 ----
mean loss: 4011.08
train mean loss: 4129.63
epoch train time: 0:00:00.203607
elapsed time: 0:00:15.414419
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:31:02.607885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3864.32
 ---- batch: 020 ----
mean loss: 3893.70
 ---- batch: 030 ----
mean loss: 3610.11
 ---- batch: 040 ----
mean loss: 3664.28
train mean loss: 3755.08
epoch train time: 0:00:00.207565
elapsed time: 0:00:15.622107
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:31:02.815574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3562.05
 ---- batch: 020 ----
mean loss: 3446.72
 ---- batch: 030 ----
mean loss: 3410.49
 ---- batch: 040 ----
mean loss: 3312.01
train mean loss: 3422.09
epoch train time: 0:00:00.199298
elapsed time: 0:00:15.821530
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:31:03.014993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3216.07
 ---- batch: 020 ----
mean loss: 3219.39
 ---- batch: 030 ----
mean loss: 3109.85
 ---- batch: 040 ----
mean loss: 3034.53
train mean loss: 3138.65
epoch train time: 0:00:00.194726
elapsed time: 0:00:16.016416
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:31:03.209880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2956.40
 ---- batch: 020 ----
mean loss: 2969.87
 ---- batch: 030 ----
mean loss: 2888.68
 ---- batch: 040 ----
mean loss: 2759.96
train mean loss: 2887.44
epoch train time: 0:00:00.196626
elapsed time: 0:00:16.213169
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:31:03.406633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2760.10
 ---- batch: 020 ----
mean loss: 2648.33
 ---- batch: 030 ----
mean loss: 2674.29
 ---- batch: 040 ----
mean loss: 2589.38
train mean loss: 2663.30
epoch train time: 0:00:00.210107
elapsed time: 0:00:16.423393
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:31:03.616873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2593.36
 ---- batch: 020 ----
mean loss: 2467.62
 ---- batch: 030 ----
mean loss: 2429.38
 ---- batch: 040 ----
mean loss: 2378.52
train mean loss: 2461.13
epoch train time: 0:00:00.199446
elapsed time: 0:00:16.622995
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:31:03.816457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2337.32
 ---- batch: 020 ----
mean loss: 2288.46
 ---- batch: 030 ----
mean loss: 2262.64
 ---- batch: 040 ----
mean loss: 2255.08
train mean loss: 2278.31
epoch train time: 0:00:00.198227
elapsed time: 0:00:16.821338
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:31:04.014801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2163.30
 ---- batch: 020 ----
mean loss: 2141.97
 ---- batch: 030 ----
mean loss: 2113.76
 ---- batch: 040 ----
mean loss: 2029.89
train mean loss: 2113.13
epoch train time: 0:00:00.200915
elapsed time: 0:00:17.022373
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:31:04.215849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2051.68
 ---- batch: 020 ----
mean loss: 1960.51
 ---- batch: 030 ----
mean loss: 1931.26
 ---- batch: 040 ----
mean loss: 1917.49
train mean loss: 1961.93
epoch train time: 0:00:00.200568
elapsed time: 0:00:17.223096
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:31:04.416559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1863.94
 ---- batch: 020 ----
mean loss: 1846.72
 ---- batch: 030 ----
mean loss: 1800.94
 ---- batch: 040 ----
mean loss: 1801.22
train mean loss: 1824.11
epoch train time: 0:00:00.198915
elapsed time: 0:00:17.422124
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:31:04.615599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1739.12
 ---- batch: 020 ----
mean loss: 1716.18
 ---- batch: 030 ----
mean loss: 1683.98
 ---- batch: 040 ----
mean loss: 1670.35
train mean loss: 1700.81
epoch train time: 0:00:00.210161
elapsed time: 0:00:17.632459
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:31:04.826020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1638.77
 ---- batch: 020 ----
mean loss: 1594.65
 ---- batch: 030 ----
mean loss: 1567.76
 ---- batch: 040 ----
mean loss: 1565.44
train mean loss: 1586.13
epoch train time: 0:00:00.210215
elapsed time: 0:00:17.842897
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:31:05.036360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1502.08
 ---- batch: 020 ----
mean loss: 1513.45
 ---- batch: 030 ----
mean loss: 1483.24
 ---- batch: 040 ----
mean loss: 1449.36
train mean loss: 1483.53
epoch train time: 0:00:00.207254
elapsed time: 0:00:18.050279
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:31:05.243749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1408.23
 ---- batch: 020 ----
mean loss: 1397.73
 ---- batch: 030 ----
mean loss: 1388.02
 ---- batch: 040 ----
mean loss: 1372.44
train mean loss: 1391.45
epoch train time: 0:00:00.207328
elapsed time: 0:00:18.257740
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:31:05.451207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1329.07
 ---- batch: 020 ----
mean loss: 1319.09
 ---- batch: 030 ----
mean loss: 1299.90
 ---- batch: 040 ----
mean loss: 1284.74
train mean loss: 1306.67
epoch train time: 0:00:00.210839
elapsed time: 0:00:18.468706
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:31:05.662172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1271.83
 ---- batch: 020 ----
mean loss: 1217.97
 ---- batch: 030 ----
mean loss: 1238.30
 ---- batch: 040 ----
mean loss: 1207.46
train mean loss: 1230.47
epoch train time: 0:00:00.207754
elapsed time: 0:00:18.676673
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:31:05.870175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1174.55
 ---- batch: 020 ----
mean loss: 1177.73
 ---- batch: 030 ----
mean loss: 1173.50
 ---- batch: 040 ----
mean loss: 1129.37
train mean loss: 1163.79
epoch train time: 0:00:00.205612
elapsed time: 0:00:18.882451
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:31:06.075920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1120.36
 ---- batch: 020 ----
mean loss: 1113.16
 ---- batch: 030 ----
mean loss: 1097.08
 ---- batch: 040 ----
mean loss: 1083.57
train mean loss: 1101.91
epoch train time: 0:00:00.214477
elapsed time: 0:00:19.097062
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:31:06.290530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1071.35
 ---- batch: 020 ----
mean loss: 1043.29
 ---- batch: 030 ----
mean loss: 1041.28
 ---- batch: 040 ----
mean loss: 1039.51
train mean loss: 1046.64
epoch train time: 0:00:00.219247
elapsed time: 0:00:19.316453
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:31:06.509934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1019.23
 ---- batch: 020 ----
mean loss: 1010.66
 ---- batch: 030 ----
mean loss: 992.58
 ---- batch: 040 ----
mean loss: 973.44
train mean loss: 998.02
epoch train time: 0:00:00.206643
elapsed time: 0:00:19.523229
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:31:06.716690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 981.97
 ---- batch: 020 ----
mean loss: 968.68
 ---- batch: 030 ----
mean loss: 948.26
 ---- batch: 040 ----
mean loss: 932.08
train mean loss: 954.56
epoch train time: 0:00:00.204998
elapsed time: 0:00:19.728349
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:31:06.921821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.46
 ---- batch: 020 ----
mean loss: 922.05
 ---- batch: 030 ----
mean loss: 917.18
 ---- batch: 040 ----
mean loss: 889.88
train mean loss: 916.29
epoch train time: 0:00:00.205062
elapsed time: 0:00:19.933534
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:31:07.127035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.97
 ---- batch: 020 ----
mean loss: 900.35
 ---- batch: 030 ----
mean loss: 878.93
 ---- batch: 040 ----
mean loss: 867.21
train mean loss: 881.68
epoch train time: 0:00:00.202269
elapsed time: 0:00:20.135956
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:31:07.329419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.75
 ---- batch: 020 ----
mean loss: 843.60
 ---- batch: 030 ----
mean loss: 849.25
 ---- batch: 040 ----
mean loss: 847.13
train mean loss: 851.62
epoch train time: 0:00:00.203141
elapsed time: 0:00:20.339216
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:31:07.532677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 833.79
 ---- batch: 020 ----
mean loss: 830.98
 ---- batch: 030 ----
mean loss: 821.90
 ---- batch: 040 ----
mean loss: 821.65
train mean loss: 825.55
epoch train time: 0:00:00.201727
elapsed time: 0:00:20.541056
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:31:07.734531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 823.54
 ---- batch: 020 ----
mean loss: 793.89
 ---- batch: 030 ----
mean loss: 799.14
 ---- batch: 040 ----
mean loss: 795.97
train mean loss: 802.53
epoch train time: 0:00:00.206809
elapsed time: 0:00:20.747999
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:31:07.941495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 789.74
 ---- batch: 020 ----
mean loss: 771.46
 ---- batch: 030 ----
mean loss: 793.04
 ---- batch: 040 ----
mean loss: 773.92
train mean loss: 782.79
epoch train time: 0:00:00.201951
elapsed time: 0:00:20.950102
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:31:08.143568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 769.88
 ---- batch: 020 ----
mean loss: 761.30
 ---- batch: 030 ----
mean loss: 769.62
 ---- batch: 040 ----
mean loss: 762.15
train mean loss: 764.71
epoch train time: 0:00:00.204274
elapsed time: 0:00:21.154514
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:31:08.347980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 767.13
 ---- batch: 020 ----
mean loss: 742.11
 ---- batch: 030 ----
mean loss: 757.28
 ---- batch: 040 ----
mean loss: 737.97
train mean loss: 749.52
epoch train time: 0:00:00.204056
elapsed time: 0:00:21.358701
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:31:08.552169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 762.59
 ---- batch: 020 ----
mean loss: 722.06
 ---- batch: 030 ----
mean loss: 723.53
 ---- batch: 040 ----
mean loss: 746.36
train mean loss: 736.47
epoch train time: 0:00:00.216743
elapsed time: 0:00:21.575575
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:31:08.769041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 727.37
 ---- batch: 020 ----
mean loss: 727.02
 ---- batch: 030 ----
mean loss: 730.72
 ---- batch: 040 ----
mean loss: 720.18
train mean loss: 725.16
epoch train time: 0:00:00.212791
elapsed time: 0:00:21.788491
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:31:08.982016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 716.10
 ---- batch: 020 ----
mean loss: 718.18
 ---- batch: 030 ----
mean loss: 703.88
 ---- batch: 040 ----
mean loss: 718.28
train mean loss: 716.03
epoch train time: 0:00:00.213063
elapsed time: 0:00:22.001735
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:31:09.195200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 712.30
 ---- batch: 020 ----
mean loss: 698.71
 ---- batch: 030 ----
mean loss: 701.58
 ---- batch: 040 ----
mean loss: 723.27
train mean loss: 707.06
epoch train time: 0:00:00.211647
elapsed time: 0:00:22.213511
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:31:09.406973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.16
 ---- batch: 020 ----
mean loss: 707.77
 ---- batch: 030 ----
mean loss: 701.91
 ---- batch: 040 ----
mean loss: 702.95
train mean loss: 700.58
epoch train time: 0:00:00.216103
elapsed time: 0:00:22.429779
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:31:09.623266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 693.71
 ---- batch: 020 ----
mean loss: 686.94
 ---- batch: 030 ----
mean loss: 686.11
 ---- batch: 040 ----
mean loss: 712.38
train mean loss: 694.79
epoch train time: 0:00:00.218260
elapsed time: 0:00:22.648182
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:31:09.841661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 694.02
 ---- batch: 020 ----
mean loss: 677.41
 ---- batch: 030 ----
mean loss: 708.73
 ---- batch: 040 ----
mean loss: 686.48
train mean loss: 689.49
epoch train time: 0:00:00.209278
elapsed time: 0:00:22.857623
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:31:10.051087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 698.81
 ---- batch: 020 ----
mean loss: 686.98
 ---- batch: 030 ----
mean loss: 684.64
 ---- batch: 040 ----
mean loss: 677.43
train mean loss: 684.92
epoch train time: 0:00:00.204276
elapsed time: 0:00:23.062016
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:31:10.255479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.66
 ---- batch: 020 ----
mean loss: 698.65
 ---- batch: 030 ----
mean loss: 684.25
 ---- batch: 040 ----
mean loss: 674.92
train mean loss: 681.79
epoch train time: 0:00:00.199570
elapsed time: 0:00:23.261699
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:31:10.455161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.79
 ---- batch: 020 ----
mean loss: 680.17
 ---- batch: 030 ----
mean loss: 672.93
 ---- batch: 040 ----
mean loss: 665.86
train mean loss: 678.97
epoch train time: 0:00:00.199018
elapsed time: 0:00:23.460847
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:31:10.654310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 676.79
 ---- batch: 020 ----
mean loss: 677.04
 ---- batch: 030 ----
mean loss: 679.89
 ---- batch: 040 ----
mean loss: 678.47
train mean loss: 676.52
epoch train time: 0:00:00.205590
elapsed time: 0:00:23.666566
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:31:10.860028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 691.77
 ---- batch: 020 ----
mean loss: 663.93
 ---- batch: 030 ----
mean loss: 674.93
 ---- batch: 040 ----
mean loss: 668.30
train mean loss: 675.08
epoch train time: 0:00:00.196794
elapsed time: 0:00:23.863474
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:31:11.056935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.33
 ---- batch: 020 ----
mean loss: 647.75
 ---- batch: 030 ----
mean loss: 677.35
 ---- batch: 040 ----
mean loss: 687.67
train mean loss: 672.79
epoch train time: 0:00:00.195407
elapsed time: 0:00:24.059014
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:31:11.252483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.75
 ---- batch: 020 ----
mean loss: 654.42
 ---- batch: 030 ----
mean loss: 671.51
 ---- batch: 040 ----
mean loss: 677.16
train mean loss: 671.68
epoch train time: 0:00:00.198517
elapsed time: 0:00:24.257653
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:31:11.451117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.80
 ---- batch: 020 ----
mean loss: 679.54
 ---- batch: 030 ----
mean loss: 660.52
 ---- batch: 040 ----
mean loss: 685.90
train mean loss: 670.33
epoch train time: 0:00:00.198258
elapsed time: 0:00:24.456034
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:31:11.649499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.03
 ---- batch: 020 ----
mean loss: 680.44
 ---- batch: 030 ----
mean loss: 666.35
 ---- batch: 040 ----
mean loss: 674.34
train mean loss: 669.45
epoch train time: 0:00:00.210858
elapsed time: 0:00:24.667086
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:31:11.860558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 670.99
 ---- batch: 020 ----
mean loss: 679.45
 ---- batch: 030 ----
mean loss: 657.14
 ---- batch: 040 ----
mean loss: 674.52
train mean loss: 668.46
epoch train time: 0:00:00.213583
elapsed time: 0:00:24.880834
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:31:12.074328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 657.07
 ---- batch: 020 ----
mean loss: 661.07
 ---- batch: 030 ----
mean loss: 681.64
 ---- batch: 040 ----
mean loss: 677.70
train mean loss: 668.19
epoch train time: 0:00:00.204329
elapsed time: 0:00:25.085340
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:31:12.278820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.69
 ---- batch: 020 ----
mean loss: 678.86
 ---- batch: 030 ----
mean loss: 658.64
 ---- batch: 040 ----
mean loss: 665.35
train mean loss: 667.42
epoch train time: 0:00:00.211485
elapsed time: 0:00:25.296980
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:31:12.490446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 650.03
 ---- batch: 020 ----
mean loss: 656.26
 ---- batch: 030 ----
mean loss: 672.63
 ---- batch: 040 ----
mean loss: 672.53
train mean loss: 664.67
epoch train time: 0:00:00.215218
elapsed time: 0:00:25.512329
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:31:12.705796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.09
 ---- batch: 020 ----
mean loss: 679.35
 ---- batch: 030 ----
mean loss: 636.32
 ---- batch: 040 ----
mean loss: 662.25
train mean loss: 659.24
epoch train time: 0:00:00.209789
elapsed time: 0:00:25.722258
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:31:12.915729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 625.10
 ---- batch: 020 ----
mean loss: 570.26
 ---- batch: 030 ----
mean loss: 503.68
 ---- batch: 040 ----
mean loss: 453.00
train mean loss: 531.21
epoch train time: 0:00:00.204322
elapsed time: 0:00:25.926711
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:31:13.120177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.03
 ---- batch: 020 ----
mean loss: 428.71
 ---- batch: 030 ----
mean loss: 424.91
 ---- batch: 040 ----
mean loss: 417.82
train mean loss: 426.22
epoch train time: 0:00:00.205361
elapsed time: 0:00:26.132199
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:31:13.325693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.91
 ---- batch: 020 ----
mean loss: 406.16
 ---- batch: 030 ----
mean loss: 411.23
 ---- batch: 040 ----
mean loss: 406.03
train mean loss: 411.05
epoch train time: 0:00:00.205750
elapsed time: 0:00:26.338099
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:31:13.531563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.05
 ---- batch: 020 ----
mean loss: 395.93
 ---- batch: 030 ----
mean loss: 399.06
 ---- batch: 040 ----
mean loss: 402.21
train mean loss: 400.60
epoch train time: 0:00:00.207918
elapsed time: 0:00:26.546138
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:31:13.739602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.36
 ---- batch: 020 ----
mean loss: 389.96
 ---- batch: 030 ----
mean loss: 391.28
 ---- batch: 040 ----
mean loss: 386.99
train mean loss: 391.73
epoch train time: 0:00:00.205180
elapsed time: 0:00:26.751442
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:31:13.944907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.12
 ---- batch: 020 ----
mean loss: 390.12
 ---- batch: 030 ----
mean loss: 384.72
 ---- batch: 040 ----
mean loss: 383.21
train mean loss: 384.23
epoch train time: 0:00:00.203335
elapsed time: 0:00:26.954897
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:31:14.148359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.14
 ---- batch: 020 ----
mean loss: 382.87
 ---- batch: 030 ----
mean loss: 378.23
 ---- batch: 040 ----
mean loss: 377.04
train mean loss: 377.36
epoch train time: 0:00:00.206604
elapsed time: 0:00:27.161615
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:31:14.355075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.92
 ---- batch: 020 ----
mean loss: 371.16
 ---- batch: 030 ----
mean loss: 376.63
 ---- batch: 040 ----
mean loss: 365.23
train mean loss: 371.80
epoch train time: 0:00:00.201934
elapsed time: 0:00:27.363663
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:31:14.557140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.58
 ---- batch: 020 ----
mean loss: 371.96
 ---- batch: 030 ----
mean loss: 362.31
 ---- batch: 040 ----
mean loss: 369.93
train mean loss: 366.71
epoch train time: 0:00:00.202706
elapsed time: 0:00:27.566516
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:31:14.759979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.19
 ---- batch: 020 ----
mean loss: 363.99
 ---- batch: 030 ----
mean loss: 359.54
 ---- batch: 040 ----
mean loss: 360.59
train mean loss: 361.90
epoch train time: 0:00:00.214408
elapsed time: 0:00:27.781043
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:31:14.974507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.18
 ---- batch: 020 ----
mean loss: 361.25
 ---- batch: 030 ----
mean loss: 361.48
 ---- batch: 040 ----
mean loss: 356.20
train mean loss: 357.74
epoch train time: 0:00:00.212838
elapsed time: 0:00:27.994003
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:31:15.187467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.14
 ---- batch: 020 ----
mean loss: 356.19
 ---- batch: 030 ----
mean loss: 357.85
 ---- batch: 040 ----
mean loss: 341.39
train mean loss: 353.95
epoch train time: 0:00:00.210076
elapsed time: 0:00:28.204201
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:31:15.397665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.18
 ---- batch: 020 ----
mean loss: 352.26
 ---- batch: 030 ----
mean loss: 351.99
 ---- batch: 040 ----
mean loss: 347.45
train mean loss: 351.13
epoch train time: 0:00:00.211594
elapsed time: 0:00:28.415914
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:31:15.609379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.06
 ---- batch: 020 ----
mean loss: 332.53
 ---- batch: 030 ----
mean loss: 351.92
 ---- batch: 040 ----
mean loss: 350.54
train mean loss: 348.04
epoch train time: 0:00:00.207534
elapsed time: 0:00:28.623567
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:31:15.817043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.85
 ---- batch: 020 ----
mean loss: 346.46
 ---- batch: 030 ----
mean loss: 341.49
 ---- batch: 040 ----
mean loss: 346.06
train mean loss: 345.38
epoch train time: 0:00:00.209451
elapsed time: 0:00:28.833153
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:31:16.026619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.13
 ---- batch: 020 ----
mean loss: 331.56
 ---- batch: 030 ----
mean loss: 346.57
 ---- batch: 040 ----
mean loss: 348.37
train mean loss: 343.18
epoch train time: 0:00:00.204638
elapsed time: 0:00:29.037916
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:31:16.231382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.57
 ---- batch: 020 ----
mean loss: 339.90
 ---- batch: 030 ----
mean loss: 332.27
 ---- batch: 040 ----
mean loss: 337.40
train mean loss: 341.32
epoch train time: 0:00:00.211905
elapsed time: 0:00:29.249951
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:31:16.443430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.94
 ---- batch: 020 ----
mean loss: 342.48
 ---- batch: 030 ----
mean loss: 344.10
 ---- batch: 040 ----
mean loss: 332.44
train mean loss: 339.33
epoch train time: 0:00:00.209222
elapsed time: 0:00:29.459333
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:31:16.652814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.49
 ---- batch: 020 ----
mean loss: 341.53
 ---- batch: 030 ----
mean loss: 339.89
 ---- batch: 040 ----
mean loss: 334.90
train mean loss: 337.93
epoch train time: 0:00:00.210898
elapsed time: 0:00:29.670370
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:31:16.863845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.69
 ---- batch: 020 ----
mean loss: 339.32
 ---- batch: 030 ----
mean loss: 334.22
 ---- batch: 040 ----
mean loss: 338.90
train mean loss: 336.14
epoch train time: 0:00:00.205391
elapsed time: 0:00:29.875921
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:31:17.069405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.81
 ---- batch: 020 ----
mean loss: 326.54
 ---- batch: 030 ----
mean loss: 334.93
 ---- batch: 040 ----
mean loss: 335.39
train mean loss: 335.01
epoch train time: 0:00:00.208888
elapsed time: 0:00:30.084951
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:31:17.278417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.88
 ---- batch: 020 ----
mean loss: 333.30
 ---- batch: 030 ----
mean loss: 322.87
 ---- batch: 040 ----
mean loss: 340.35
train mean loss: 334.49
epoch train time: 0:00:00.206672
elapsed time: 0:00:30.291753
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:31:17.485222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.82
 ---- batch: 020 ----
mean loss: 334.13
 ---- batch: 030 ----
mean loss: 328.45
 ---- batch: 040 ----
mean loss: 341.81
train mean loss: 332.59
epoch train time: 0:00:00.207913
elapsed time: 0:00:30.499795
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:31:17.693261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.29
 ---- batch: 020 ----
mean loss: 331.72
 ---- batch: 030 ----
mean loss: 331.82
 ---- batch: 040 ----
mean loss: 333.67
train mean loss: 332.01
epoch train time: 0:00:00.208086
elapsed time: 0:00:30.708013
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:31:17.901481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.83
 ---- batch: 020 ----
mean loss: 336.14
 ---- batch: 030 ----
mean loss: 325.15
 ---- batch: 040 ----
mean loss: 332.03
train mean loss: 331.06
epoch train time: 0:00:00.208668
elapsed time: 0:00:30.916810
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:31:18.110277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.16
 ---- batch: 020 ----
mean loss: 330.83
 ---- batch: 030 ----
mean loss: 326.41
 ---- batch: 040 ----
mean loss: 332.33
train mean loss: 330.17
epoch train time: 0:00:00.201280
elapsed time: 0:00:31.118233
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:31:18.311712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.43
 ---- batch: 020 ----
mean loss: 327.27
 ---- batch: 030 ----
mean loss: 337.66
 ---- batch: 040 ----
mean loss: 320.74
train mean loss: 329.52
epoch train time: 0:00:00.204497
elapsed time: 0:00:31.322865
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:31:18.516329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.62
 ---- batch: 020 ----
mean loss: 330.66
 ---- batch: 030 ----
mean loss: 336.88
 ---- batch: 040 ----
mean loss: 317.62
train mean loss: 328.80
epoch train time: 0:00:00.208314
elapsed time: 0:00:31.531306
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:31:18.724774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.42
 ---- batch: 020 ----
mean loss: 324.46
 ---- batch: 030 ----
mean loss: 324.69
 ---- batch: 040 ----
mean loss: 330.19
train mean loss: 327.78
epoch train time: 0:00:00.209044
elapsed time: 0:00:31.740475
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:31:18.933940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.99
 ---- batch: 020 ----
mean loss: 335.96
 ---- batch: 030 ----
mean loss: 323.35
 ---- batch: 040 ----
mean loss: 331.81
train mean loss: 327.75
epoch train time: 0:00:00.208169
elapsed time: 0:00:31.948766
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:31:19.142231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.59
 ---- batch: 020 ----
mean loss: 330.55
 ---- batch: 030 ----
mean loss: 318.03
 ---- batch: 040 ----
mean loss: 333.90
train mean loss: 326.76
epoch train time: 0:00:00.209183
elapsed time: 0:00:32.158083
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:31:19.351545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.43
 ---- batch: 020 ----
mean loss: 327.93
 ---- batch: 030 ----
mean loss: 323.43
 ---- batch: 040 ----
mean loss: 328.81
train mean loss: 326.41
epoch train time: 0:00:00.211573
elapsed time: 0:00:32.369791
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:31:19.563271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.80
 ---- batch: 020 ----
mean loss: 325.87
 ---- batch: 030 ----
mean loss: 329.71
 ---- batch: 040 ----
mean loss: 332.14
train mean loss: 326.10
epoch train time: 0:00:00.209604
elapsed time: 0:00:32.579536
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:31:19.773001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.35
 ---- batch: 020 ----
mean loss: 329.39
 ---- batch: 030 ----
mean loss: 317.42
 ---- batch: 040 ----
mean loss: 327.71
train mean loss: 325.89
epoch train time: 0:00:00.212585
elapsed time: 0:00:32.792258
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:31:19.985729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.19
 ---- batch: 020 ----
mean loss: 316.83
 ---- batch: 030 ----
mean loss: 334.41
 ---- batch: 040 ----
mean loss: 325.49
train mean loss: 325.34
epoch train time: 0:00:00.218287
elapsed time: 0:00:33.010678
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:31:20.204144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.42
 ---- batch: 020 ----
mean loss: 327.84
 ---- batch: 030 ----
mean loss: 328.33
 ---- batch: 040 ----
mean loss: 320.89
train mean loss: 324.47
epoch train time: 0:00:00.204446
elapsed time: 0:00:33.215265
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:31:20.408743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.57
 ---- batch: 020 ----
mean loss: 319.67
 ---- batch: 030 ----
mean loss: 325.26
 ---- batch: 040 ----
mean loss: 325.49
train mean loss: 324.24
epoch train time: 0:00:00.207153
elapsed time: 0:00:33.422559
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:31:20.616064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.60
 ---- batch: 020 ----
mean loss: 322.14
 ---- batch: 030 ----
mean loss: 323.57
 ---- batch: 040 ----
mean loss: 326.75
train mean loss: 323.67
epoch train time: 0:00:00.208990
elapsed time: 0:00:33.631735
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:31:20.825201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.07
 ---- batch: 020 ----
mean loss: 328.19
 ---- batch: 030 ----
mean loss: 323.53
 ---- batch: 040 ----
mean loss: 324.46
train mean loss: 323.37
epoch train time: 0:00:00.204786
elapsed time: 0:00:33.836645
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:31:21.030110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.64
 ---- batch: 020 ----
mean loss: 328.92
 ---- batch: 030 ----
mean loss: 322.22
 ---- batch: 040 ----
mean loss: 321.71
train mean loss: 323.44
epoch train time: 0:00:00.205001
elapsed time: 0:00:34.041771
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:31:21.235236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.40
 ---- batch: 020 ----
mean loss: 322.28
 ---- batch: 030 ----
mean loss: 312.17
 ---- batch: 040 ----
mean loss: 329.71
train mean loss: 322.70
epoch train time: 0:00:00.202440
elapsed time: 0:00:34.244334
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:31:21.437799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.97
 ---- batch: 020 ----
mean loss: 328.18
 ---- batch: 030 ----
mean loss: 327.42
 ---- batch: 040 ----
mean loss: 317.39
train mean loss: 322.35
epoch train time: 0:00:00.205251
elapsed time: 0:00:34.449713
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:31:21.643179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.32
 ---- batch: 020 ----
mean loss: 324.50
 ---- batch: 030 ----
mean loss: 311.54
 ---- batch: 040 ----
mean loss: 322.97
train mean loss: 322.14
epoch train time: 0:00:00.213904
elapsed time: 0:00:34.663752
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:31:21.857226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.84
 ---- batch: 020 ----
mean loss: 321.13
 ---- batch: 030 ----
mean loss: 317.67
 ---- batch: 040 ----
mean loss: 329.64
train mean loss: 321.56
epoch train time: 0:00:00.214642
elapsed time: 0:00:34.878529
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:31:22.072018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.19
 ---- batch: 020 ----
mean loss: 321.69
 ---- batch: 030 ----
mean loss: 323.66
 ---- batch: 040 ----
mean loss: 321.69
train mean loss: 321.13
epoch train time: 0:00:00.212174
elapsed time: 0:00:35.090850
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:31:22.284315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.05
 ---- batch: 020 ----
mean loss: 321.52
 ---- batch: 030 ----
mean loss: 327.09
 ---- batch: 040 ----
mean loss: 316.55
train mean loss: 320.92
epoch train time: 0:00:00.211770
elapsed time: 0:00:35.302777
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:31:22.496253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.85
 ---- batch: 020 ----
mean loss: 322.69
 ---- batch: 030 ----
mean loss: 315.79
 ---- batch: 040 ----
mean loss: 327.67
train mean loss: 320.57
epoch train time: 0:00:00.212720
elapsed time: 0:00:35.515632
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:31:22.709098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.55
 ---- batch: 020 ----
mean loss: 322.97
 ---- batch: 030 ----
mean loss: 326.69
 ---- batch: 040 ----
mean loss: 317.12
train mean loss: 320.73
epoch train time: 0:00:00.215563
elapsed time: 0:00:35.731320
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:31:22.924802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.26
 ---- batch: 020 ----
mean loss: 321.99
 ---- batch: 030 ----
mean loss: 322.37
 ---- batch: 040 ----
mean loss: 313.88
train mean loss: 320.44
epoch train time: 0:00:00.210731
elapsed time: 0:00:35.942189
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:31:23.135668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.37
 ---- batch: 020 ----
mean loss: 328.46
 ---- batch: 030 ----
mean loss: 325.58
 ---- batch: 040 ----
mean loss: 308.34
train mean loss: 319.71
epoch train time: 0:00:00.209433
elapsed time: 0:00:36.151755
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:31:23.345219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.92
 ---- batch: 020 ----
mean loss: 326.28
 ---- batch: 030 ----
mean loss: 317.26
 ---- batch: 040 ----
mean loss: 316.70
train mean loss: 319.88
epoch train time: 0:00:00.207143
elapsed time: 0:00:36.359045
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:31:23.552513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.70
 ---- batch: 020 ----
mean loss: 321.02
 ---- batch: 030 ----
mean loss: 309.43
 ---- batch: 040 ----
mean loss: 318.16
train mean loss: 319.66
epoch train time: 0:00:00.205365
elapsed time: 0:00:36.564534
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:31:23.757999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.67
 ---- batch: 020 ----
mean loss: 324.54
 ---- batch: 030 ----
mean loss: 318.49
 ---- batch: 040 ----
mean loss: 316.15
train mean loss: 319.30
epoch train time: 0:00:00.207103
elapsed time: 0:00:36.771774
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:31:23.965241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.67
 ---- batch: 020 ----
mean loss: 323.29
 ---- batch: 030 ----
mean loss: 312.14
 ---- batch: 040 ----
mean loss: 316.94
train mean loss: 318.77
epoch train time: 0:00:00.205310
elapsed time: 0:00:36.977228
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:31:24.170687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.19
 ---- batch: 020 ----
mean loss: 332.04
 ---- batch: 030 ----
mean loss: 315.94
 ---- batch: 040 ----
mean loss: 310.46
train mean loss: 318.39
epoch train time: 0:00:00.203593
elapsed time: 0:00:37.180940
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:31:24.374503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.11
 ---- batch: 020 ----
mean loss: 325.14
 ---- batch: 030 ----
mean loss: 311.33
 ---- batch: 040 ----
mean loss: 312.41
train mean loss: 318.52
epoch train time: 0:00:00.205671
elapsed time: 0:00:37.386834
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:31:24.580315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.83
 ---- batch: 020 ----
mean loss: 324.96
 ---- batch: 030 ----
mean loss: 322.49
 ---- batch: 040 ----
mean loss: 312.62
train mean loss: 318.57
epoch train time: 0:00:00.210126
elapsed time: 0:00:37.597113
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:31:24.790580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.73
 ---- batch: 020 ----
mean loss: 304.11
 ---- batch: 030 ----
mean loss: 326.19
 ---- batch: 040 ----
mean loss: 326.38
train mean loss: 317.53
epoch train time: 0:00:00.210385
elapsed time: 0:00:37.807624
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:31:25.001091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.00
 ---- batch: 020 ----
mean loss: 310.68
 ---- batch: 030 ----
mean loss: 318.91
 ---- batch: 040 ----
mean loss: 319.14
train mean loss: 317.54
epoch train time: 0:00:00.205501
elapsed time: 0:00:38.013248
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:31:25.206714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.09
 ---- batch: 020 ----
mean loss: 312.80
 ---- batch: 030 ----
mean loss: 317.81
 ---- batch: 040 ----
mean loss: 321.98
train mean loss: 317.52
epoch train time: 0:00:00.205954
elapsed time: 0:00:38.219361
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:31:25.412827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.70
 ---- batch: 020 ----
mean loss: 325.55
 ---- batch: 030 ----
mean loss: 318.50
 ---- batch: 040 ----
mean loss: 313.19
train mean loss: 317.03
epoch train time: 0:00:00.210926
elapsed time: 0:00:38.430441
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:31:25.623905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.64
 ---- batch: 020 ----
mean loss: 317.45
 ---- batch: 030 ----
mean loss: 313.13
 ---- batch: 040 ----
mean loss: 312.82
train mean loss: 317.34
epoch train time: 0:00:00.211851
elapsed time: 0:00:38.642414
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:31:25.835877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.41
 ---- batch: 020 ----
mean loss: 314.18
 ---- batch: 030 ----
mean loss: 319.61
 ---- batch: 040 ----
mean loss: 315.64
train mean loss: 316.80
epoch train time: 0:00:00.206782
elapsed time: 0:00:38.849314
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:31:26.042778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.56
 ---- batch: 020 ----
mean loss: 317.15
 ---- batch: 030 ----
mean loss: 311.59
 ---- batch: 040 ----
mean loss: 312.92
train mean loss: 316.73
epoch train time: 0:00:00.208795
elapsed time: 0:00:39.058226
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:31:26.251688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.97
 ---- batch: 020 ----
mean loss: 325.19
 ---- batch: 030 ----
mean loss: 316.34
 ---- batch: 040 ----
mean loss: 312.76
train mean loss: 316.23
epoch train time: 0:00:00.207930
elapsed time: 0:00:39.266287
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:31:26.459751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.32
 ---- batch: 020 ----
mean loss: 309.33
 ---- batch: 030 ----
mean loss: 315.38
 ---- batch: 040 ----
mean loss: 315.94
train mean loss: 316.48
epoch train time: 0:00:00.207852
elapsed time: 0:00:39.474259
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:31:26.667724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.10
 ---- batch: 020 ----
mean loss: 315.77
 ---- batch: 030 ----
mean loss: 313.18
 ---- batch: 040 ----
mean loss: 319.96
train mean loss: 315.81
epoch train time: 0:00:00.209897
elapsed time: 0:00:39.684283
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:31:26.877768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.09
 ---- batch: 020 ----
mean loss: 316.46
 ---- batch: 030 ----
mean loss: 315.10
 ---- batch: 040 ----
mean loss: 313.71
train mean loss: 315.78
epoch train time: 0:00:00.213016
elapsed time: 0:00:39.897459
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:31:27.090925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.73
 ---- batch: 020 ----
mean loss: 323.68
 ---- batch: 030 ----
mean loss: 314.95
 ---- batch: 040 ----
mean loss: 311.83
train mean loss: 315.42
epoch train time: 0:00:00.209058
elapsed time: 0:00:40.106645
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:31:27.300113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.46
 ---- batch: 020 ----
mean loss: 311.85
 ---- batch: 030 ----
mean loss: 317.13
 ---- batch: 040 ----
mean loss: 324.39
train mean loss: 315.22
epoch train time: 0:00:00.209039
elapsed time: 0:00:40.315811
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:31:27.509279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.77
 ---- batch: 020 ----
mean loss: 310.72
 ---- batch: 030 ----
mean loss: 313.42
 ---- batch: 040 ----
mean loss: 316.15
train mean loss: 315.17
epoch train time: 0:00:00.219213
elapsed time: 0:00:40.535164
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:31:27.728635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.89
 ---- batch: 020 ----
mean loss: 312.46
 ---- batch: 030 ----
mean loss: 314.60
 ---- batch: 040 ----
mean loss: 312.74
train mean loss: 315.00
epoch train time: 0:00:00.205444
elapsed time: 0:00:40.740738
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:31:27.934208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.35
 ---- batch: 020 ----
mean loss: 320.00
 ---- batch: 030 ----
mean loss: 312.47
 ---- batch: 040 ----
mean loss: 329.95
train mean loss: 314.83
epoch train time: 0:00:00.203346
elapsed time: 0:00:40.944212
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:31:28.137680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.04
 ---- batch: 020 ----
mean loss: 309.25
 ---- batch: 030 ----
mean loss: 320.35
 ---- batch: 040 ----
mean loss: 308.50
train mean loss: 314.59
epoch train time: 0:00:00.203106
elapsed time: 0:00:41.147461
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:31:28.340919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.73
 ---- batch: 020 ----
mean loss: 307.59
 ---- batch: 030 ----
mean loss: 318.54
 ---- batch: 040 ----
mean loss: 316.15
train mean loss: 314.98
epoch train time: 0:00:00.204070
elapsed time: 0:00:41.351650
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:31:28.545115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.71
 ---- batch: 020 ----
mean loss: 310.45
 ---- batch: 030 ----
mean loss: 306.94
 ---- batch: 040 ----
mean loss: 316.43
train mean loss: 314.19
epoch train time: 0:00:00.205305
elapsed time: 0:00:41.557078
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:31:28.750545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.28
 ---- batch: 020 ----
mean loss: 315.35
 ---- batch: 030 ----
mean loss: 321.97
 ---- batch: 040 ----
mean loss: 316.52
train mean loss: 314.00
epoch train time: 0:00:00.204756
elapsed time: 0:00:41.761960
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:31:28.955426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.47
 ---- batch: 020 ----
mean loss: 308.36
 ---- batch: 030 ----
mean loss: 316.72
 ---- batch: 040 ----
mean loss: 309.59
train mean loss: 314.00
epoch train time: 0:00:00.207367
elapsed time: 0:00:41.969450
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:31:29.162929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.33
 ---- batch: 020 ----
mean loss: 313.81
 ---- batch: 030 ----
mean loss: 308.82
 ---- batch: 040 ----
mean loss: 316.45
train mean loss: 313.60
epoch train time: 0:00:00.208255
elapsed time: 0:00:42.177855
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:31:29.371334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.65
 ---- batch: 020 ----
mean loss: 312.97
 ---- batch: 030 ----
mean loss: 320.03
 ---- batch: 040 ----
mean loss: 318.22
train mean loss: 313.72
epoch train time: 0:00:00.206829
elapsed time: 0:00:42.384816
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:31:29.578289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.00
 ---- batch: 020 ----
mean loss: 312.29
 ---- batch: 030 ----
mean loss: 319.61
 ---- batch: 040 ----
mean loss: 305.72
train mean loss: 313.39
epoch train time: 0:00:00.203951
elapsed time: 0:00:42.588896
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:31:29.782360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.59
 ---- batch: 020 ----
mean loss: 316.94
 ---- batch: 030 ----
mean loss: 309.29
 ---- batch: 040 ----
mean loss: 314.61
train mean loss: 313.30
epoch train time: 0:00:00.207816
elapsed time: 0:00:42.796853
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:31:29.990328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.17
 ---- batch: 020 ----
mean loss: 313.95
 ---- batch: 030 ----
mean loss: 310.12
 ---- batch: 040 ----
mean loss: 317.39
train mean loss: 313.22
epoch train time: 0:00:00.204605
elapsed time: 0:00:43.001604
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:31:30.195081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.57
 ---- batch: 020 ----
mean loss: 309.68
 ---- batch: 030 ----
mean loss: 312.88
 ---- batch: 040 ----
mean loss: 314.61
train mean loss: 312.84
epoch train time: 0:00:00.206558
elapsed time: 0:00:43.208296
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:31:30.401761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.52
 ---- batch: 020 ----
mean loss: 318.85
 ---- batch: 030 ----
mean loss: 313.78
 ---- batch: 040 ----
mean loss: 308.42
train mean loss: 312.93
epoch train time: 0:00:00.205304
elapsed time: 0:00:43.413720
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:31:30.607184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.43
 ---- batch: 020 ----
mean loss: 309.44
 ---- batch: 030 ----
mean loss: 316.94
 ---- batch: 040 ----
mean loss: 307.23
train mean loss: 312.34
epoch train time: 0:00:00.208568
elapsed time: 0:00:43.622410
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:31:30.815875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.50
 ---- batch: 020 ----
mean loss: 308.03
 ---- batch: 030 ----
mean loss: 314.48
 ---- batch: 040 ----
mean loss: 310.19
train mean loss: 312.29
epoch train time: 0:00:00.207399
elapsed time: 0:00:43.829933
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:31:31.023397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.30
 ---- batch: 020 ----
mean loss: 312.91
 ---- batch: 030 ----
mean loss: 310.10
 ---- batch: 040 ----
mean loss: 318.91
train mean loss: 311.98
epoch train time: 0:00:00.206992
elapsed time: 0:00:44.037046
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:31:31.230512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.81
 ---- batch: 020 ----
mean loss: 310.93
 ---- batch: 030 ----
mean loss: 313.54
 ---- batch: 040 ----
mean loss: 315.58
train mean loss: 312.27
epoch train time: 0:00:00.206779
elapsed time: 0:00:44.243950
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:31:31.437417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.59
 ---- batch: 020 ----
mean loss: 314.93
 ---- batch: 030 ----
mean loss: 306.46
 ---- batch: 040 ----
mean loss: 306.60
train mean loss: 311.97
epoch train time: 0:00:00.206314
elapsed time: 0:00:44.450391
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:31:31.643858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.72
 ---- batch: 020 ----
mean loss: 307.72
 ---- batch: 030 ----
mean loss: 312.34
 ---- batch: 040 ----
mean loss: 316.60
train mean loss: 311.93
epoch train time: 0:00:00.200275
elapsed time: 0:00:44.650792
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:31:31.844272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.36
 ---- batch: 020 ----
mean loss: 312.09
 ---- batch: 030 ----
mean loss: 311.38
 ---- batch: 040 ----
mean loss: 309.87
train mean loss: 311.44
epoch train time: 0:00:00.198799
elapsed time: 0:00:44.849733
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:31:32.043199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.35
 ---- batch: 020 ----
mean loss: 319.49
 ---- batch: 030 ----
mean loss: 310.15
 ---- batch: 040 ----
mean loss: 306.33
train mean loss: 311.41
epoch train time: 0:00:00.202558
elapsed time: 0:00:45.052422
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:31:32.245889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.21
 ---- batch: 020 ----
mean loss: 317.24
 ---- batch: 030 ----
mean loss: 308.37
 ---- batch: 040 ----
mean loss: 314.41
train mean loss: 311.18
epoch train time: 0:00:00.202716
elapsed time: 0:00:45.255282
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:31:32.448762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.63
 ---- batch: 020 ----
mean loss: 311.77
 ---- batch: 030 ----
mean loss: 311.02
 ---- batch: 040 ----
mean loss: 308.94
train mean loss: 311.65
epoch train time: 0:00:00.201969
elapsed time: 0:00:45.457398
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:31:32.650869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.26
 ---- batch: 020 ----
mean loss: 297.84
 ---- batch: 030 ----
mean loss: 317.70
 ---- batch: 040 ----
mean loss: 312.13
train mean loss: 311.04
epoch train time: 0:00:00.208352
elapsed time: 0:00:45.665892
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:31:32.859353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.42
 ---- batch: 020 ----
mean loss: 308.58
 ---- batch: 030 ----
mean loss: 313.16
 ---- batch: 040 ----
mean loss: 310.15
train mean loss: 311.02
epoch train time: 0:00:00.212000
elapsed time: 0:00:45.878010
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:31:33.071468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.84
 ---- batch: 020 ----
mean loss: 307.22
 ---- batch: 030 ----
mean loss: 319.26
 ---- batch: 040 ----
mean loss: 308.10
train mean loss: 310.76
epoch train time: 0:00:00.211780
elapsed time: 0:00:46.089933
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:31:33.283397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.12
 ---- batch: 020 ----
mean loss: 301.93
 ---- batch: 030 ----
mean loss: 318.86
 ---- batch: 040 ----
mean loss: 310.69
train mean loss: 310.42
epoch train time: 0:00:00.210672
elapsed time: 0:00:46.300725
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:31:33.494190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.75
 ---- batch: 020 ----
mean loss: 307.51
 ---- batch: 030 ----
mean loss: 307.03
 ---- batch: 040 ----
mean loss: 309.63
train mean loss: 310.61
epoch train time: 0:00:00.210275
elapsed time: 0:00:46.511122
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:31:33.704585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.10
 ---- batch: 020 ----
mean loss: 310.78
 ---- batch: 030 ----
mean loss: 303.11
 ---- batch: 040 ----
mean loss: 314.75
train mean loss: 310.46
epoch train time: 0:00:00.210057
elapsed time: 0:00:46.721300
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:31:33.914765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.66
 ---- batch: 020 ----
mean loss: 313.49
 ---- batch: 030 ----
mean loss: 304.43
 ---- batch: 040 ----
mean loss: 318.22
train mean loss: 310.06
epoch train time: 0:00:00.212473
elapsed time: 0:00:46.933906
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:31:34.127380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.17
 ---- batch: 020 ----
mean loss: 306.25
 ---- batch: 030 ----
mean loss: 317.10
 ---- batch: 040 ----
mean loss: 309.47
train mean loss: 309.84
epoch train time: 0:00:00.209496
elapsed time: 0:00:47.143546
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:31:34.337058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.77
 ---- batch: 020 ----
mean loss: 319.48
 ---- batch: 030 ----
mean loss: 307.72
 ---- batch: 040 ----
mean loss: 308.90
train mean loss: 309.26
epoch train time: 0:00:00.204025
elapsed time: 0:00:47.347752
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:31:34.541214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.74
 ---- batch: 020 ----
mean loss: 313.21
 ---- batch: 030 ----
mean loss: 312.52
 ---- batch: 040 ----
mean loss: 297.76
train mean loss: 309.82
epoch train time: 0:00:00.208324
elapsed time: 0:00:47.556196
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:31:34.749661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.84
 ---- batch: 020 ----
mean loss: 313.23
 ---- batch: 030 ----
mean loss: 306.69
 ---- batch: 040 ----
mean loss: 307.81
train mean loss: 309.31
epoch train time: 0:00:00.207547
elapsed time: 0:00:47.763878
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:31:34.957345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.37
 ---- batch: 020 ----
mean loss: 298.52
 ---- batch: 030 ----
mean loss: 306.61
 ---- batch: 040 ----
mean loss: 324.48
train mean loss: 309.24
epoch train time: 0:00:00.209862
elapsed time: 0:00:47.973870
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:31:35.167335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.71
 ---- batch: 020 ----
mean loss: 311.73
 ---- batch: 030 ----
mean loss: 305.92
 ---- batch: 040 ----
mean loss: 306.91
train mean loss: 308.76
epoch train time: 0:00:00.208742
elapsed time: 0:00:48.182739
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:31:35.376205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.85
 ---- batch: 020 ----
mean loss: 308.06
 ---- batch: 030 ----
mean loss: 303.88
 ---- batch: 040 ----
mean loss: 307.16
train mean loss: 308.67
epoch train time: 0:00:00.207160
elapsed time: 0:00:48.390023
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:31:35.583489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.91
 ---- batch: 020 ----
mean loss: 300.50
 ---- batch: 030 ----
mean loss: 310.41
 ---- batch: 040 ----
mean loss: 309.60
train mean loss: 308.87
epoch train time: 0:00:00.204795
elapsed time: 0:00:48.594950
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:31:35.788440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.35
 ---- batch: 020 ----
mean loss: 308.95
 ---- batch: 030 ----
mean loss: 308.46
 ---- batch: 040 ----
mean loss: 311.20
train mean loss: 308.25
epoch train time: 0:00:00.207908
elapsed time: 0:00:48.803029
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:31:35.996513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.01
 ---- batch: 020 ----
mean loss: 310.61
 ---- batch: 030 ----
mean loss: 315.46
 ---- batch: 040 ----
mean loss: 308.32
train mean loss: 308.26
epoch train time: 0:00:00.204688
elapsed time: 0:00:49.007858
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:31:36.201325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.71
 ---- batch: 020 ----
mean loss: 308.26
 ---- batch: 030 ----
mean loss: 304.94
 ---- batch: 040 ----
mean loss: 313.37
train mean loss: 308.53
epoch train time: 0:00:00.202650
elapsed time: 0:00:49.210637
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:31:36.404103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.18
 ---- batch: 020 ----
mean loss: 301.48
 ---- batch: 030 ----
mean loss: 316.47
 ---- batch: 040 ----
mean loss: 309.44
train mean loss: 308.20
epoch train time: 0:00:00.206956
elapsed time: 0:00:49.417717
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:31:36.611182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.46
 ---- batch: 020 ----
mean loss: 313.19
 ---- batch: 030 ----
mean loss: 312.84
 ---- batch: 040 ----
mean loss: 304.93
train mean loss: 307.23
epoch train time: 0:00:00.213669
elapsed time: 0:00:49.631514
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:31:36.825010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.06
 ---- batch: 020 ----
mean loss: 307.38
 ---- batch: 030 ----
mean loss: 307.32
 ---- batch: 040 ----
mean loss: 312.23
train mean loss: 307.81
epoch train time: 0:00:00.214670
elapsed time: 0:00:49.846339
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:31:37.039822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.14
 ---- batch: 020 ----
mean loss: 310.85
 ---- batch: 030 ----
mean loss: 312.74
 ---- batch: 040 ----
mean loss: 297.98
train mean loss: 307.99
epoch train time: 0:00:00.212838
elapsed time: 0:00:50.059318
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:31:37.252782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.99
 ---- batch: 020 ----
mean loss: 305.31
 ---- batch: 030 ----
mean loss: 307.95
 ---- batch: 040 ----
mean loss: 302.88
train mean loss: 307.94
epoch train time: 0:00:00.213607
elapsed time: 0:00:50.273047
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:31:37.466513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.92
 ---- batch: 020 ----
mean loss: 303.31
 ---- batch: 030 ----
mean loss: 309.44
 ---- batch: 040 ----
mean loss: 309.35
train mean loss: 307.56
epoch train time: 0:00:00.211168
elapsed time: 0:00:50.484347
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:31:37.677819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.47
 ---- batch: 020 ----
mean loss: 306.98
 ---- batch: 030 ----
mean loss: 308.48
 ---- batch: 040 ----
mean loss: 313.53
train mean loss: 306.84
epoch train time: 0:00:00.209968
elapsed time: 0:00:50.694441
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:31:37.887906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.42
 ---- batch: 020 ----
mean loss: 308.38
 ---- batch: 030 ----
mean loss: 304.63
 ---- batch: 040 ----
mean loss: 306.27
train mean loss: 307.17
epoch train time: 0:00:00.211687
elapsed time: 0:00:50.906259
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:31:38.099715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.41
 ---- batch: 020 ----
mean loss: 309.70
 ---- batch: 030 ----
mean loss: 311.02
 ---- batch: 040 ----
mean loss: 295.89
train mean loss: 307.07
epoch train time: 0:00:00.210659
elapsed time: 0:00:51.117031
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:31:38.310497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.89
 ---- batch: 020 ----
mean loss: 296.72
 ---- batch: 030 ----
mean loss: 312.15
 ---- batch: 040 ----
mean loss: 317.40
train mean loss: 306.88
epoch train time: 0:00:00.208387
elapsed time: 0:00:51.325544
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:31:38.519011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.83
 ---- batch: 020 ----
mean loss: 312.24
 ---- batch: 030 ----
mean loss: 306.25
 ---- batch: 040 ----
mean loss: 300.71
train mean loss: 306.90
epoch train time: 0:00:00.208226
elapsed time: 0:00:51.533900
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:31:38.727367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.55
 ---- batch: 020 ----
mean loss: 297.08
 ---- batch: 030 ----
mean loss: 306.71
 ---- batch: 040 ----
mean loss: 311.82
train mean loss: 306.43
epoch train time: 0:00:00.211704
elapsed time: 0:00:51.745733
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:31:38.939199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.41
 ---- batch: 020 ----
mean loss: 309.70
 ---- batch: 030 ----
mean loss: 302.16
 ---- batch: 040 ----
mean loss: 304.11
train mean loss: 306.44
epoch train time: 0:00:00.209163
elapsed time: 0:00:51.955049
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:31:39.148517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.94
 ---- batch: 020 ----
mean loss: 298.78
 ---- batch: 030 ----
mean loss: 303.54
 ---- batch: 040 ----
mean loss: 312.96
train mean loss: 305.56
epoch train time: 0:00:00.203363
elapsed time: 0:00:52.158538
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:31:39.352016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.58
 ---- batch: 020 ----
mean loss: 302.94
 ---- batch: 030 ----
mean loss: 307.44
 ---- batch: 040 ----
mean loss: 310.49
train mean loss: 305.50
epoch train time: 0:00:00.204985
elapsed time: 0:00:52.363665
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:31:39.557131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.23
 ---- batch: 020 ----
mean loss: 307.38
 ---- batch: 030 ----
mean loss: 300.35
 ---- batch: 040 ----
mean loss: 307.28
train mean loss: 305.67
epoch train time: 0:00:00.207050
elapsed time: 0:00:52.570837
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:31:39.764302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.51
 ---- batch: 020 ----
mean loss: 311.86
 ---- batch: 030 ----
mean loss: 300.56
 ---- batch: 040 ----
mean loss: 303.51
train mean loss: 305.61
epoch train time: 0:00:00.203436
elapsed time: 0:00:52.774421
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:31:39.967887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.24
 ---- batch: 020 ----
mean loss: 311.27
 ---- batch: 030 ----
mean loss: 299.12
 ---- batch: 040 ----
mean loss: 304.79
train mean loss: 305.38
epoch train time: 0:00:00.202681
elapsed time: 0:00:52.977226
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:31:40.170692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.55
 ---- batch: 020 ----
mean loss: 300.04
 ---- batch: 030 ----
mean loss: 307.63
 ---- batch: 040 ----
mean loss: 310.64
train mean loss: 305.21
epoch train time: 0:00:00.205365
elapsed time: 0:00:53.182713
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:31:40.376177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.19
 ---- batch: 020 ----
mean loss: 303.24
 ---- batch: 030 ----
mean loss: 297.37
 ---- batch: 040 ----
mean loss: 315.84
train mean loss: 305.06
epoch train time: 0:00:00.207452
elapsed time: 0:00:53.390287
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:31:40.583753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.57
 ---- batch: 020 ----
mean loss: 309.83
 ---- batch: 030 ----
mean loss: 307.68
 ---- batch: 040 ----
mean loss: 301.07
train mean loss: 305.15
epoch train time: 0:00:00.208186
elapsed time: 0:00:53.598597
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:31:40.792062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.03
 ---- batch: 020 ----
mean loss: 298.62
 ---- batch: 030 ----
mean loss: 309.99
 ---- batch: 040 ----
mean loss: 308.52
train mean loss: 305.09
epoch train time: 0:00:00.208501
elapsed time: 0:00:53.807224
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:31:41.000704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.19
 ---- batch: 020 ----
mean loss: 304.71
 ---- batch: 030 ----
mean loss: 306.27
 ---- batch: 040 ----
mean loss: 299.55
train mean loss: 304.56
epoch train time: 0:00:00.206225
elapsed time: 0:00:54.013619
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:31:41.207084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.12
 ---- batch: 020 ----
mean loss: 315.03
 ---- batch: 030 ----
mean loss: 297.06
 ---- batch: 040 ----
mean loss: 309.48
train mean loss: 304.91
epoch train time: 0:00:00.207409
elapsed time: 0:00:54.221150
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:31:41.414619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.03
 ---- batch: 020 ----
mean loss: 308.72
 ---- batch: 030 ----
mean loss: 308.49
 ---- batch: 040 ----
mean loss: 293.57
train mean loss: 304.14
epoch train time: 0:00:00.205937
elapsed time: 0:00:54.427214
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:31:41.620679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.10
 ---- batch: 020 ----
mean loss: 302.88
 ---- batch: 030 ----
mean loss: 303.32
 ---- batch: 040 ----
mean loss: 308.40
train mean loss: 304.01
epoch train time: 0:00:00.207169
elapsed time: 0:00:54.634506
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:31:41.827990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.43
 ---- batch: 020 ----
mean loss: 301.63
 ---- batch: 030 ----
mean loss: 298.90
 ---- batch: 040 ----
mean loss: 312.84
train mean loss: 304.28
epoch train time: 0:00:00.204311
elapsed time: 0:00:54.838957
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:31:42.032453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.05
 ---- batch: 020 ----
mean loss: 309.21
 ---- batch: 030 ----
mean loss: 305.27
 ---- batch: 040 ----
mean loss: 309.34
train mean loss: 304.17
epoch train time: 0:00:00.201651
elapsed time: 0:00:55.040763
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:31:42.234228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.74
 ---- batch: 020 ----
mean loss: 316.14
 ---- batch: 030 ----
mean loss: 302.01
 ---- batch: 040 ----
mean loss: 299.31
train mean loss: 303.42
epoch train time: 0:00:00.206449
elapsed time: 0:00:55.247343
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:31:42.440818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.96
 ---- batch: 020 ----
mean loss: 303.84
 ---- batch: 030 ----
mean loss: 302.99
 ---- batch: 040 ----
mean loss: 305.19
train mean loss: 303.33
epoch train time: 0:00:00.209880
elapsed time: 0:00:55.457359
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:31:42.650827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.36
 ---- batch: 020 ----
mean loss: 302.69
 ---- batch: 030 ----
mean loss: 299.25
 ---- batch: 040 ----
mean loss: 306.34
train mean loss: 303.29
epoch train time: 0:00:00.210969
elapsed time: 0:00:55.668472
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:31:42.861933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.91
 ---- batch: 020 ----
mean loss: 304.69
 ---- batch: 030 ----
mean loss: 308.34
 ---- batch: 040 ----
mean loss: 302.21
train mean loss: 303.22
epoch train time: 0:00:00.205439
elapsed time: 0:00:55.874033
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:31:43.067502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.59
 ---- batch: 020 ----
mean loss: 300.37
 ---- batch: 030 ----
mean loss: 299.19
 ---- batch: 040 ----
mean loss: 300.77
train mean loss: 302.86
epoch train time: 0:00:00.202857
elapsed time: 0:00:56.077018
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:31:43.270482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.86
 ---- batch: 020 ----
mean loss: 313.53
 ---- batch: 030 ----
mean loss: 297.54
 ---- batch: 040 ----
mean loss: 307.37
train mean loss: 302.59
epoch train time: 0:00:00.213845
elapsed time: 0:00:56.291011
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:31:43.484482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.58
 ---- batch: 020 ----
mean loss: 299.38
 ---- batch: 030 ----
mean loss: 300.23
 ---- batch: 040 ----
mean loss: 305.97
train mean loss: 302.78
epoch train time: 0:00:00.206653
elapsed time: 0:00:56.497814
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:31:43.691292
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.78
 ---- batch: 020 ----
mean loss: 305.34
 ---- batch: 030 ----
mean loss: 300.63
 ---- batch: 040 ----
mean loss: 303.48
train mean loss: 302.70
epoch train time: 0:00:00.209940
elapsed time: 0:00:56.707905
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:31:43.901363
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.13
 ---- batch: 020 ----
mean loss: 307.95
 ---- batch: 030 ----
mean loss: 301.30
 ---- batch: 040 ----
mean loss: 304.16
train mean loss: 302.29
epoch train time: 0:00:00.209403
elapsed time: 0:00:56.917424
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:31:44.110889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.81
 ---- batch: 020 ----
mean loss: 311.18
 ---- batch: 030 ----
mean loss: 306.24
 ---- batch: 040 ----
mean loss: 299.82
train mean loss: 302.49
epoch train time: 0:00:00.207727
elapsed time: 0:00:57.125273
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:31:44.318751
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.47
 ---- batch: 020 ----
mean loss: 303.91
 ---- batch: 030 ----
mean loss: 295.55
 ---- batch: 040 ----
mean loss: 310.88
train mean loss: 302.70
epoch train time: 0:00:00.208143
elapsed time: 0:00:57.333587
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:31:44.527051
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.19
 ---- batch: 020 ----
mean loss: 305.48
 ---- batch: 030 ----
mean loss: 304.30
 ---- batch: 040 ----
mean loss: 294.96
train mean loss: 302.37
epoch train time: 0:00:00.209857
elapsed time: 0:00:57.543567
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:31:44.737031
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.66
 ---- batch: 020 ----
mean loss: 300.48
 ---- batch: 030 ----
mean loss: 309.04
 ---- batch: 040 ----
mean loss: 300.88
train mean loss: 302.26
epoch train time: 0:00:00.212026
elapsed time: 0:00:57.755725
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:31:44.949190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.76
 ---- batch: 020 ----
mean loss: 299.32
 ---- batch: 030 ----
mean loss: 307.67
 ---- batch: 040 ----
mean loss: 297.49
train mean loss: 302.73
epoch train time: 0:00:00.204795
elapsed time: 0:00:57.960641
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:31:45.154104
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.89
 ---- batch: 020 ----
mean loss: 310.16
 ---- batch: 030 ----
mean loss: 298.21
 ---- batch: 040 ----
mean loss: 303.50
train mean loss: 302.28
epoch train time: 0:00:00.204819
elapsed time: 0:00:58.165601
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:31:45.359079
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.60
 ---- batch: 020 ----
mean loss: 307.65
 ---- batch: 030 ----
mean loss: 299.37
 ---- batch: 040 ----
mean loss: 306.09
train mean loss: 302.59
epoch train time: 0:00:00.206255
elapsed time: 0:00:58.371986
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:31:45.565446
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.65
 ---- batch: 020 ----
mean loss: 300.40
 ---- batch: 030 ----
mean loss: 297.38
 ---- batch: 040 ----
mean loss: 310.95
train mean loss: 302.41
epoch train time: 0:00:00.207390
elapsed time: 0:00:58.579491
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:31:45.772953
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.41
 ---- batch: 020 ----
mean loss: 301.94
 ---- batch: 030 ----
mean loss: 303.13
 ---- batch: 040 ----
mean loss: 307.13
train mean loss: 302.27
epoch train time: 0:00:00.211259
elapsed time: 0:00:58.790874
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:31:45.984342
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.48
 ---- batch: 020 ----
mean loss: 300.07
 ---- batch: 030 ----
mean loss: 299.93
 ---- batch: 040 ----
mean loss: 307.02
train mean loss: 302.74
epoch train time: 0:00:00.210303
elapsed time: 0:00:59.001307
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:31:46.194773
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.77
 ---- batch: 020 ----
mean loss: 304.35
 ---- batch: 030 ----
mean loss: 301.45
 ---- batch: 040 ----
mean loss: 302.03
train mean loss: 302.25
epoch train time: 0:00:00.206893
elapsed time: 0:00:59.208327
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:31:46.401795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.17
 ---- batch: 020 ----
mean loss: 296.31
 ---- batch: 030 ----
mean loss: 302.58
 ---- batch: 040 ----
mean loss: 308.30
train mean loss: 302.42
epoch train time: 0:00:00.208409
elapsed time: 0:00:59.416880
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:31:46.610347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.19
 ---- batch: 020 ----
mean loss: 309.71
 ---- batch: 030 ----
mean loss: 295.82
 ---- batch: 040 ----
mean loss: 300.00
train mean loss: 302.44
epoch train time: 0:00:00.203782
elapsed time: 0:00:59.620789
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:31:46.814344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.66
 ---- batch: 020 ----
mean loss: 297.60
 ---- batch: 030 ----
mean loss: 303.93
 ---- batch: 040 ----
mean loss: 299.72
train mean loss: 302.29
epoch train time: 0:00:00.204105
elapsed time: 0:00:59.825111
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:31:47.018578
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.20
 ---- batch: 020 ----
mean loss: 302.82
 ---- batch: 030 ----
mean loss: 295.85
 ---- batch: 040 ----
mean loss: 306.27
train mean loss: 302.58
epoch train time: 0:00:00.201064
elapsed time: 0:01:00.026303
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:31:47.219777
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.98
 ---- batch: 020 ----
mean loss: 308.88
 ---- batch: 030 ----
mean loss: 301.11
 ---- batch: 040 ----
mean loss: 295.90
train mean loss: 302.17
epoch train time: 0:00:00.201013
elapsed time: 0:01:00.227451
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:31:47.420929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.56
 ---- batch: 020 ----
mean loss: 297.91
 ---- batch: 030 ----
mean loss: 296.47
 ---- batch: 040 ----
mean loss: 302.57
train mean loss: 302.38
epoch train time: 0:00:00.199768
elapsed time: 0:01:00.427357
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:31:47.620821
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.24
 ---- batch: 020 ----
mean loss: 303.09
 ---- batch: 030 ----
mean loss: 301.85
 ---- batch: 040 ----
mean loss: 305.00
train mean loss: 302.23
epoch train time: 0:00:00.205193
elapsed time: 0:01:00.632673
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:31:47.826168
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.62
 ---- batch: 020 ----
mean loss: 308.50
 ---- batch: 030 ----
mean loss: 297.09
 ---- batch: 040 ----
mean loss: 288.85
train mean loss: 302.82
epoch train time: 0:00:00.211702
elapsed time: 0:01:00.844542
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:31:48.038023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.91
 ---- batch: 020 ----
mean loss: 301.41
 ---- batch: 030 ----
mean loss: 307.30
 ---- batch: 040 ----
mean loss: 301.31
train mean loss: 302.52
epoch train time: 0:00:00.211294
elapsed time: 0:01:01.055974
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:31:48.249440
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.38
 ---- batch: 020 ----
mean loss: 305.72
 ---- batch: 030 ----
mean loss: 295.08
 ---- batch: 040 ----
mean loss: 302.39
train mean loss: 302.22
epoch train time: 0:00:00.210473
elapsed time: 0:01:01.266584
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:31:48.460047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.66
 ---- batch: 020 ----
mean loss: 303.89
 ---- batch: 030 ----
mean loss: 302.98
 ---- batch: 040 ----
mean loss: 299.57
train mean loss: 302.36
epoch train time: 0:00:00.214776
elapsed time: 0:01:01.481498
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:31:48.674962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.76
 ---- batch: 020 ----
mean loss: 290.70
 ---- batch: 030 ----
mean loss: 303.09
 ---- batch: 040 ----
mean loss: 307.01
train mean loss: 302.26
epoch train time: 0:00:00.213202
elapsed time: 0:01:01.694823
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:31:48.888288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.64
 ---- batch: 020 ----
mean loss: 297.72
 ---- batch: 030 ----
mean loss: 306.64
 ---- batch: 040 ----
mean loss: 300.08
train mean loss: 301.97
epoch train time: 0:00:00.212649
elapsed time: 0:01:01.907596
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:31:49.101075
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.11
 ---- batch: 020 ----
mean loss: 303.03
 ---- batch: 030 ----
mean loss: 299.26
 ---- batch: 040 ----
mean loss: 298.99
train mean loss: 302.01
epoch train time: 0:00:00.214833
elapsed time: 0:01:02.122567
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:31:49.316048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.29
 ---- batch: 020 ----
mean loss: 297.32
 ---- batch: 030 ----
mean loss: 304.74
 ---- batch: 040 ----
mean loss: 303.80
train mean loss: 302.04
epoch train time: 0:00:00.215797
elapsed time: 0:01:02.338503
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:31:49.531969
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.95
 ---- batch: 020 ----
mean loss: 308.62
 ---- batch: 030 ----
mean loss: 294.70
 ---- batch: 040 ----
mean loss: 302.78
train mean loss: 302.00
epoch train time: 0:00:00.212662
elapsed time: 0:01:02.551296
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:31:49.744772
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.24
 ---- batch: 020 ----
mean loss: 305.28
 ---- batch: 030 ----
mean loss: 305.53
 ---- batch: 040 ----
mean loss: 307.46
train mean loss: 301.59
epoch train time: 0:00:00.212793
elapsed time: 0:01:02.764226
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:31:49.957692
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.61
 ---- batch: 020 ----
mean loss: 297.83
 ---- batch: 030 ----
mean loss: 307.70
 ---- batch: 040 ----
mean loss: 303.40
train mean loss: 301.98
epoch train time: 0:00:00.212747
elapsed time: 0:01:02.977100
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:31:50.170566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.74
 ---- batch: 020 ----
mean loss: 302.90
 ---- batch: 030 ----
mean loss: 306.43
 ---- batch: 040 ----
mean loss: 305.11
train mean loss: 302.12
epoch train time: 0:00:00.212581
elapsed time: 0:01:03.189816
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:31:50.383286
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.71
 ---- batch: 020 ----
mean loss: 308.31
 ---- batch: 030 ----
mean loss: 301.25
 ---- batch: 040 ----
mean loss: 300.22
train mean loss: 302.07
epoch train time: 0:00:00.212468
elapsed time: 0:01:03.402428
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:31:50.595903
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.15
 ---- batch: 020 ----
mean loss: 307.37
 ---- batch: 030 ----
mean loss: 305.81
 ---- batch: 040 ----
mean loss: 294.60
train mean loss: 301.97
epoch train time: 0:00:00.210628
elapsed time: 0:01:03.613192
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:31:50.806659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.38
 ---- batch: 020 ----
mean loss: 304.01
 ---- batch: 030 ----
mean loss: 301.29
 ---- batch: 040 ----
mean loss: 301.32
train mean loss: 301.67
epoch train time: 0:00:00.209671
elapsed time: 0:01:03.823017
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:31:51.016489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 307.74
 ---- batch: 020 ----
mean loss: 294.33
 ---- batch: 030 ----
mean loss: 305.14
 ---- batch: 040 ----
mean loss: 295.59
train mean loss: 302.07
epoch train time: 0:00:00.207319
elapsed time: 0:01:04.030470
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:31:51.223936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 311.00
 ---- batch: 020 ----
mean loss: 297.47
 ---- batch: 030 ----
mean loss: 303.22
 ---- batch: 040 ----
mean loss: 299.74
train mean loss: 301.71
epoch train time: 0:00:00.206471
elapsed time: 0:01:04.237110
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:31:51.430574
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.52
 ---- batch: 020 ----
mean loss: 298.45
 ---- batch: 030 ----
mean loss: 299.33
 ---- batch: 040 ----
mean loss: 296.97
train mean loss: 301.98
epoch train time: 0:00:00.213921
elapsed time: 0:01:04.451169
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:31:51.644635
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.35
 ---- batch: 020 ----
mean loss: 302.53
 ---- batch: 030 ----
mean loss: 298.85
 ---- batch: 040 ----
mean loss: 308.49
train mean loss: 301.69
epoch train time: 0:00:00.211479
elapsed time: 0:01:04.662773
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:31:51.856250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.91
 ---- batch: 020 ----
mean loss: 305.91
 ---- batch: 030 ----
mean loss: 303.38
 ---- batch: 040 ----
mean loss: 303.90
train mean loss: 301.65
epoch train time: 0:00:00.205976
elapsed time: 0:01:04.868929
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:31:52.062396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.42
 ---- batch: 020 ----
mean loss: 304.84
 ---- batch: 030 ----
mean loss: 305.71
 ---- batch: 040 ----
mean loss: 299.99
train mean loss: 302.09
epoch train time: 0:00:00.206494
elapsed time: 0:01:05.075547
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:31:52.269013
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.40
 ---- batch: 020 ----
mean loss: 304.98
 ---- batch: 030 ----
mean loss: 305.44
 ---- batch: 040 ----
mean loss: 296.60
train mean loss: 301.92
epoch train time: 0:00:00.206573
elapsed time: 0:01:05.282254
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:31:52.475719
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.30
 ---- batch: 020 ----
mean loss: 301.09
 ---- batch: 030 ----
mean loss: 299.71
 ---- batch: 040 ----
mean loss: 305.26
train mean loss: 301.71
epoch train time: 0:00:00.205151
elapsed time: 0:01:05.487526
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:31:52.680990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.26
 ---- batch: 020 ----
mean loss: 303.68
 ---- batch: 030 ----
mean loss: 305.65
 ---- batch: 040 ----
mean loss: 303.93
train mean loss: 301.67
epoch train time: 0:00:00.209699
elapsed time: 0:01:05.697360
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:31:52.890841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.65
 ---- batch: 020 ----
mean loss: 301.70
 ---- batch: 030 ----
mean loss: 300.90
 ---- batch: 040 ----
mean loss: 302.86
train mean loss: 301.60
epoch train time: 0:00:00.210901
elapsed time: 0:01:05.908398
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:31:53.101878
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.14
 ---- batch: 020 ----
mean loss: 301.96
 ---- batch: 030 ----
mean loss: 307.43
 ---- batch: 040 ----
mean loss: 295.41
train mean loss: 302.10
epoch train time: 0:00:00.207242
elapsed time: 0:01:06.115792
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:31:53.309282
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.04
 ---- batch: 020 ----
mean loss: 307.27
 ---- batch: 030 ----
mean loss: 300.35
 ---- batch: 040 ----
mean loss: 297.31
train mean loss: 302.32
epoch train time: 0:00:00.207732
elapsed time: 0:01:06.323676
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:31:53.517143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.05
 ---- batch: 020 ----
mean loss: 295.65
 ---- batch: 030 ----
mean loss: 310.45
 ---- batch: 040 ----
mean loss: 295.32
train mean loss: 301.67
epoch train time: 0:00:00.207851
elapsed time: 0:01:06.531656
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:31:53.725124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.72
 ---- batch: 020 ----
mean loss: 308.26
 ---- batch: 030 ----
mean loss: 300.11
 ---- batch: 040 ----
mean loss: 296.34
train mean loss: 301.65
epoch train time: 0:00:00.204412
elapsed time: 0:01:06.738108
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_5/checkpoint.pth.tar
**** end time: 2019-09-20 23:31:53.931547 ****
