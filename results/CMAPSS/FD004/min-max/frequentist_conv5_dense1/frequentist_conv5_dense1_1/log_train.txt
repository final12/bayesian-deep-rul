Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_1', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 15586
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-26 23:57:23.440720 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 16, 24]             100
              Tanh-2           [-1, 10, 16, 24]               0
            Conv2d-3           [-1, 10, 15, 24]           1,000
              Tanh-4           [-1, 10, 15, 24]               0
            Conv2d-5           [-1, 10, 16, 24]           1,000
              Tanh-6           [-1, 10, 16, 24]               0
            Conv2d-7           [-1, 10, 15, 24]           1,000
              Tanh-8           [-1, 10, 15, 24]               0
            Conv2d-9            [-1, 1, 15, 24]              30
             Tanh-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
          Dropout-12                  [-1, 360]               0
           Linear-13                  [-1, 100]          36,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 39,230
Trainable params: 39,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 23:57:23.449713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4616.24
 ---- batch: 020 ----
mean loss: 2575.33
 ---- batch: 030 ----
mean loss: 1386.79
 ---- batch: 040 ----
mean loss: 1326.68
 ---- batch: 050 ----
mean loss: 1171.47
 ---- batch: 060 ----
mean loss: 1099.96
 ---- batch: 070 ----
mean loss: 1058.37
 ---- batch: 080 ----
mean loss: 1024.55
 ---- batch: 090 ----
mean loss: 971.73
 ---- batch: 100 ----
mean loss: 949.73
 ---- batch: 110 ----
mean loss: 922.99
train mean loss: 1537.76
epoch train time: 0:00:34.142893
elapsed time: 0:00:34.154293
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 23:57:57.595055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.64
 ---- batch: 020 ----
mean loss: 874.04
 ---- batch: 030 ----
mean loss: 823.79
 ---- batch: 040 ----
mean loss: 829.42
 ---- batch: 050 ----
mean loss: 797.37
 ---- batch: 060 ----
mean loss: 784.40
 ---- batch: 070 ----
mean loss: 800.28
 ---- batch: 080 ----
mean loss: 780.93
 ---- batch: 090 ----
mean loss: 770.70
 ---- batch: 100 ----
mean loss: 767.37
 ---- batch: 110 ----
mean loss: 795.60
train mean loss: 807.98
epoch train time: 0:00:02.221964
elapsed time: 0:00:36.376402
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 23:57:59.817200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 730.22
 ---- batch: 020 ----
mean loss: 740.47
 ---- batch: 030 ----
mean loss: 704.15
 ---- batch: 040 ----
mean loss: 717.82
 ---- batch: 050 ----
mean loss: 717.03
 ---- batch: 060 ----
mean loss: 693.13
 ---- batch: 070 ----
mean loss: 705.18
 ---- batch: 080 ----
mean loss: 707.74
 ---- batch: 090 ----
mean loss: 688.41
 ---- batch: 100 ----
mean loss: 680.69
 ---- batch: 110 ----
mean loss: 681.68
train mean loss: 704.42
epoch train time: 0:00:02.131350
elapsed time: 0:00:38.507931
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 23:58:01.948703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.79
 ---- batch: 020 ----
mean loss: 667.83
 ---- batch: 030 ----
mean loss: 662.53
 ---- batch: 040 ----
mean loss: 649.66
 ---- batch: 050 ----
mean loss: 647.05
 ---- batch: 060 ----
mean loss: 644.73
 ---- batch: 070 ----
mean loss: 641.04
 ---- batch: 080 ----
mean loss: 625.52
 ---- batch: 090 ----
mean loss: 640.86
 ---- batch: 100 ----
mean loss: 629.36
 ---- batch: 110 ----
mean loss: 631.63
train mean loss: 646.25
epoch train time: 0:00:02.129617
elapsed time: 0:00:40.637699
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 23:58:04.078474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 613.45
 ---- batch: 020 ----
mean loss: 617.80
 ---- batch: 030 ----
mean loss: 627.10
 ---- batch: 040 ----
mean loss: 636.42
 ---- batch: 050 ----
mean loss: 612.42
 ---- batch: 060 ----
mean loss: 594.64
 ---- batch: 070 ----
mean loss: 613.80
 ---- batch: 080 ----
mean loss: 608.37
 ---- batch: 090 ----
mean loss: 593.83
 ---- batch: 100 ----
mean loss: 605.90
 ---- batch: 110 ----
mean loss: 610.38
train mean loss: 610.94
epoch train time: 0:00:02.131481
elapsed time: 0:00:42.769343
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 23:58:06.210115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.72
 ---- batch: 020 ----
mean loss: 586.36
 ---- batch: 030 ----
mean loss: 594.68
 ---- batch: 040 ----
mean loss: 579.05
 ---- batch: 050 ----
mean loss: 574.92
 ---- batch: 060 ----
mean loss: 571.84
 ---- batch: 070 ----
mean loss: 579.64
 ---- batch: 080 ----
mean loss: 593.67
 ---- batch: 090 ----
mean loss: 577.72
 ---- batch: 100 ----
mean loss: 562.62
 ---- batch: 110 ----
mean loss: 563.61
train mean loss: 579.20
epoch train time: 0:00:02.131353
elapsed time: 0:00:44.900841
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 23:58:08.341628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 574.96
 ---- batch: 020 ----
mean loss: 572.21
 ---- batch: 030 ----
mean loss: 569.92
 ---- batch: 040 ----
mean loss: 563.15
 ---- batch: 050 ----
mean loss: 553.47
 ---- batch: 060 ----
mean loss: 555.75
 ---- batch: 070 ----
mean loss: 561.83
 ---- batch: 080 ----
mean loss: 558.50
 ---- batch: 090 ----
mean loss: 547.90
 ---- batch: 100 ----
mean loss: 557.73
 ---- batch: 110 ----
mean loss: 549.58
train mean loss: 560.31
epoch train time: 0:00:02.134615
elapsed time: 0:00:47.035615
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 23:58:10.476387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.62
 ---- batch: 020 ----
mean loss: 544.34
 ---- batch: 030 ----
mean loss: 561.30
 ---- batch: 040 ----
mean loss: 553.84
 ---- batch: 050 ----
mean loss: 544.30
 ---- batch: 060 ----
mean loss: 559.12
 ---- batch: 070 ----
mean loss: 534.06
 ---- batch: 080 ----
mean loss: 537.92
 ---- batch: 090 ----
mean loss: 540.39
 ---- batch: 100 ----
mean loss: 540.75
 ---- batch: 110 ----
mean loss: 545.45
train mean loss: 545.59
epoch train time: 0:00:02.139019
elapsed time: 0:00:49.174782
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 23:58:12.615577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.43
 ---- batch: 020 ----
mean loss: 530.81
 ---- batch: 030 ----
mean loss: 539.99
 ---- batch: 040 ----
mean loss: 532.75
 ---- batch: 050 ----
mean loss: 532.60
 ---- batch: 060 ----
mean loss: 531.59
 ---- batch: 070 ----
mean loss: 521.93
 ---- batch: 080 ----
mean loss: 531.42
 ---- batch: 090 ----
mean loss: 522.60
 ---- batch: 100 ----
mean loss: 536.91
 ---- batch: 110 ----
mean loss: 541.28
train mean loss: 534.21
epoch train time: 0:00:02.140356
elapsed time: 0:00:51.315311
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 23:58:14.756106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.90
 ---- batch: 020 ----
mean loss: 544.68
 ---- batch: 030 ----
mean loss: 516.38
 ---- batch: 040 ----
mean loss: 538.91
 ---- batch: 050 ----
mean loss: 522.09
 ---- batch: 060 ----
mean loss: 532.73
 ---- batch: 070 ----
mean loss: 527.59
 ---- batch: 080 ----
mean loss: 548.30
 ---- batch: 090 ----
mean loss: 509.00
 ---- batch: 100 ----
mean loss: 511.35
 ---- batch: 110 ----
mean loss: 528.26
train mean loss: 527.11
epoch train time: 0:00:02.141348
elapsed time: 0:00:53.456835
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 23:58:16.897663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.09
 ---- batch: 020 ----
mean loss: 517.21
 ---- batch: 030 ----
mean loss: 520.06
 ---- batch: 040 ----
mean loss: 530.28
 ---- batch: 050 ----
mean loss: 523.51
 ---- batch: 060 ----
mean loss: 527.63
 ---- batch: 070 ----
mean loss: 513.97
 ---- batch: 080 ----
mean loss: 523.09
 ---- batch: 090 ----
mean loss: 523.50
 ---- batch: 100 ----
mean loss: 530.65
 ---- batch: 110 ----
mean loss: 536.57
train mean loss: 527.07
epoch train time: 0:00:02.137642
elapsed time: 0:00:55.594714
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 23:58:19.035485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 542.69
 ---- batch: 020 ----
mean loss: 544.10
 ---- batch: 030 ----
mean loss: 534.21
 ---- batch: 040 ----
mean loss: 521.38
 ---- batch: 050 ----
mean loss: 518.80
 ---- batch: 060 ----
mean loss: 510.67
 ---- batch: 070 ----
mean loss: 524.72
 ---- batch: 080 ----
mean loss: 536.69
 ---- batch: 090 ----
mean loss: 536.29
 ---- batch: 100 ----
mean loss: 512.43
 ---- batch: 110 ----
mean loss: 527.67
train mean loss: 528.32
epoch train time: 0:00:02.137420
elapsed time: 0:00:57.732286
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 23:58:21.173059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 531.31
 ---- batch: 020 ----
mean loss: 518.15
 ---- batch: 030 ----
mean loss: 528.33
 ---- batch: 040 ----
mean loss: 525.85
 ---- batch: 050 ----
mean loss: 509.95
 ---- batch: 060 ----
mean loss: 518.30
 ---- batch: 070 ----
mean loss: 521.31
 ---- batch: 080 ----
mean loss: 505.53
 ---- batch: 090 ----
mean loss: 524.20
 ---- batch: 100 ----
mean loss: 529.78
 ---- batch: 110 ----
mean loss: 514.12
train mean loss: 520.40
epoch train time: 0:00:02.135752
elapsed time: 0:00:59.868185
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 23:58:23.308958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.17
 ---- batch: 020 ----
mean loss: 520.49
 ---- batch: 030 ----
mean loss: 514.10
 ---- batch: 040 ----
mean loss: 517.04
 ---- batch: 050 ----
mean loss: 506.96
 ---- batch: 060 ----
mean loss: 529.73
 ---- batch: 070 ----
mean loss: 529.53
 ---- batch: 080 ----
mean loss: 519.60
 ---- batch: 090 ----
mean loss: 518.22
 ---- batch: 100 ----
mean loss: 516.24
 ---- batch: 110 ----
mean loss: 532.16
train mean loss: 519.24
epoch train time: 0:00:02.135268
elapsed time: 0:01:02.003596
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 23:58:25.444367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.99
 ---- batch: 020 ----
mean loss: 514.61
 ---- batch: 030 ----
mean loss: 528.06
 ---- batch: 040 ----
mean loss: 523.74
 ---- batch: 050 ----
mean loss: 526.51
 ---- batch: 060 ----
mean loss: 508.01
 ---- batch: 070 ----
mean loss: 522.85
 ---- batch: 080 ----
mean loss: 508.47
 ---- batch: 090 ----
mean loss: 510.65
 ---- batch: 100 ----
mean loss: 501.92
 ---- batch: 110 ----
mean loss: 538.70
train mean loss: 517.24
epoch train time: 0:00:02.136767
elapsed time: 0:01:04.140510
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 23:58:27.581300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.63
 ---- batch: 020 ----
mean loss: 513.56
 ---- batch: 030 ----
mean loss: 510.50
 ---- batch: 040 ----
mean loss: 497.31
 ---- batch: 050 ----
mean loss: 515.90
 ---- batch: 060 ----
mean loss: 521.83
 ---- batch: 070 ----
mean loss: 508.76
 ---- batch: 080 ----
mean loss: 509.72
 ---- batch: 090 ----
mean loss: 515.07
 ---- batch: 100 ----
mean loss: 506.59
 ---- batch: 110 ----
mean loss: 510.63
train mean loss: 511.40
epoch train time: 0:00:02.140871
elapsed time: 0:01:06.281558
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 23:58:29.722345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.72
 ---- batch: 020 ----
mean loss: 485.15
 ---- batch: 030 ----
mean loss: 497.35
 ---- batch: 040 ----
mean loss: 527.51
 ---- batch: 050 ----
mean loss: 523.34
 ---- batch: 060 ----
mean loss: 522.66
 ---- batch: 070 ----
mean loss: 506.90
 ---- batch: 080 ----
mean loss: 512.34
 ---- batch: 090 ----
mean loss: 505.41
 ---- batch: 100 ----
mean loss: 527.99
 ---- batch: 110 ----
mean loss: 515.98
train mean loss: 511.07
epoch train time: 0:00:02.136446
elapsed time: 0:01:08.418173
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 23:58:31.858948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.43
 ---- batch: 020 ----
mean loss: 528.29
 ---- batch: 030 ----
mean loss: 509.65
 ---- batch: 040 ----
mean loss: 521.12
 ---- batch: 050 ----
mean loss: 512.18
 ---- batch: 060 ----
mean loss: 516.51
 ---- batch: 070 ----
mean loss: 530.02
 ---- batch: 080 ----
mean loss: 511.19
 ---- batch: 090 ----
mean loss: 498.70
 ---- batch: 100 ----
mean loss: 527.89
 ---- batch: 110 ----
mean loss: 513.76
train mean loss: 515.00
epoch train time: 0:00:02.136980
elapsed time: 0:01:10.555305
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 23:58:33.996082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.01
 ---- batch: 020 ----
mean loss: 522.06
 ---- batch: 030 ----
mean loss: 502.59
 ---- batch: 040 ----
mean loss: 497.85
 ---- batch: 050 ----
mean loss: 518.32
 ---- batch: 060 ----
mean loss: 514.79
 ---- batch: 070 ----
mean loss: 526.97
 ---- batch: 080 ----
mean loss: 523.32
 ---- batch: 090 ----
mean loss: 514.84
 ---- batch: 100 ----
mean loss: 510.52
 ---- batch: 110 ----
mean loss: 512.58
train mean loss: 512.83
epoch train time: 0:00:02.134805
elapsed time: 0:01:12.690295
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 23:58:36.131078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.59
 ---- batch: 020 ----
mean loss: 506.59
 ---- batch: 030 ----
mean loss: 517.57
 ---- batch: 040 ----
mean loss: 509.92
 ---- batch: 050 ----
mean loss: 499.99
 ---- batch: 060 ----
mean loss: 530.43
 ---- batch: 070 ----
mean loss: 511.51
 ---- batch: 080 ----
mean loss: 508.11
 ---- batch: 090 ----
mean loss: 515.17
 ---- batch: 100 ----
mean loss: 512.50
 ---- batch: 110 ----
mean loss: 514.39
train mean loss: 512.68
epoch train time: 0:00:02.135709
elapsed time: 0:01:14.826168
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 23:58:38.266951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.28
 ---- batch: 020 ----
mean loss: 512.36
 ---- batch: 030 ----
mean loss: 521.09
 ---- batch: 040 ----
mean loss: 520.97
 ---- batch: 050 ----
mean loss: 507.73
 ---- batch: 060 ----
mean loss: 511.70
 ---- batch: 070 ----
mean loss: 523.89
 ---- batch: 080 ----
mean loss: 511.59
 ---- batch: 090 ----
mean loss: 499.24
 ---- batch: 100 ----
mean loss: 506.12
 ---- batch: 110 ----
mean loss: 495.23
train mean loss: 510.17
epoch train time: 0:00:02.138774
elapsed time: 0:01:16.965099
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 23:58:40.405875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.36
 ---- batch: 020 ----
mean loss: 516.65
 ---- batch: 030 ----
mean loss: 504.58
 ---- batch: 040 ----
mean loss: 516.89
 ---- batch: 050 ----
mean loss: 525.59
 ---- batch: 060 ----
mean loss: 520.38
 ---- batch: 070 ----
mean loss: 516.29
 ---- batch: 080 ----
mean loss: 504.72
 ---- batch: 090 ----
mean loss: 506.65
 ---- batch: 100 ----
mean loss: 504.53
 ---- batch: 110 ----
mean loss: 498.44
train mean loss: 510.70
epoch train time: 0:00:02.134906
elapsed time: 0:01:19.100154
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 23:58:42.540944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.91
 ---- batch: 020 ----
mean loss: 492.06
 ---- batch: 030 ----
mean loss: 507.76
 ---- batch: 040 ----
mean loss: 499.86
 ---- batch: 050 ----
mean loss: 495.31
 ---- batch: 060 ----
mean loss: 508.82
 ---- batch: 070 ----
mean loss: 503.17
 ---- batch: 080 ----
mean loss: 509.34
 ---- batch: 090 ----
mean loss: 508.22
 ---- batch: 100 ----
mean loss: 519.70
 ---- batch: 110 ----
mean loss: 510.94
train mean loss: 505.05
epoch train time: 0:00:02.134222
elapsed time: 0:01:21.234548
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 23:58:44.675322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.91
 ---- batch: 020 ----
mean loss: 511.13
 ---- batch: 030 ----
mean loss: 514.79
 ---- batch: 040 ----
mean loss: 505.67
 ---- batch: 050 ----
mean loss: 508.09
 ---- batch: 060 ----
mean loss: 504.88
 ---- batch: 070 ----
mean loss: 506.95
 ---- batch: 080 ----
mean loss: 499.44
 ---- batch: 090 ----
mean loss: 493.04
 ---- batch: 100 ----
mean loss: 511.97
 ---- batch: 110 ----
mean loss: 498.66
train mean loss: 506.32
epoch train time: 0:00:02.133105
elapsed time: 0:01:23.367807
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 23:58:46.808580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.67
 ---- batch: 020 ----
mean loss: 512.66
 ---- batch: 030 ----
mean loss: 500.98
 ---- batch: 040 ----
mean loss: 507.69
 ---- batch: 050 ----
mean loss: 493.33
 ---- batch: 060 ----
mean loss: 517.20
 ---- batch: 070 ----
mean loss: 510.84
 ---- batch: 080 ----
mean loss: 500.11
 ---- batch: 090 ----
mean loss: 502.70
 ---- batch: 100 ----
mean loss: 511.10
 ---- batch: 110 ----
mean loss: 509.21
train mean loss: 505.40
epoch train time: 0:00:02.132458
elapsed time: 0:01:25.500412
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 23:58:48.941184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.59
 ---- batch: 020 ----
mean loss: 520.48
 ---- batch: 030 ----
mean loss: 495.69
 ---- batch: 040 ----
mean loss: 500.56
 ---- batch: 050 ----
mean loss: 505.20
 ---- batch: 060 ----
mean loss: 513.18
 ---- batch: 070 ----
mean loss: 495.65
 ---- batch: 080 ----
mean loss: 512.49
 ---- batch: 090 ----
mean loss: 479.71
 ---- batch: 100 ----
mean loss: 494.16
 ---- batch: 110 ----
mean loss: 533.52
train mean loss: 505.11
epoch train time: 0:00:02.134346
elapsed time: 0:01:27.634913
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 23:58:51.075689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.42
 ---- batch: 020 ----
mean loss: 502.21
 ---- batch: 030 ----
mean loss: 503.22
 ---- batch: 040 ----
mean loss: 505.07
 ---- batch: 050 ----
mean loss: 487.44
 ---- batch: 060 ----
mean loss: 486.27
 ---- batch: 070 ----
mean loss: 480.29
 ---- batch: 080 ----
mean loss: 521.71
 ---- batch: 090 ----
mean loss: 513.87
 ---- batch: 100 ----
mean loss: 486.45
 ---- batch: 110 ----
mean loss: 505.77
train mean loss: 500.67
epoch train time: 0:00:02.137277
elapsed time: 0:01:29.772353
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 23:58:53.213122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.02
 ---- batch: 020 ----
mean loss: 508.94
 ---- batch: 030 ----
mean loss: 510.53
 ---- batch: 040 ----
mean loss: 511.70
 ---- batch: 050 ----
mean loss: 498.10
 ---- batch: 060 ----
mean loss: 507.11
 ---- batch: 070 ----
mean loss: 494.68
 ---- batch: 080 ----
mean loss: 508.50
 ---- batch: 090 ----
mean loss: 503.97
 ---- batch: 100 ----
mean loss: 505.21
 ---- batch: 110 ----
mean loss: 487.19
train mean loss: 503.12
epoch train time: 0:00:02.138608
elapsed time: 0:01:31.911109
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 23:58:55.351884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.34
 ---- batch: 020 ----
mean loss: 511.15
 ---- batch: 030 ----
mean loss: 484.76
 ---- batch: 040 ----
mean loss: 495.60
 ---- batch: 050 ----
mean loss: 517.30
 ---- batch: 060 ----
mean loss: 488.81
 ---- batch: 070 ----
mean loss: 504.71
 ---- batch: 080 ----
mean loss: 502.75
 ---- batch: 090 ----
mean loss: 490.08
 ---- batch: 100 ----
mean loss: 497.57
 ---- batch: 110 ----
mean loss: 495.53
train mean loss: 497.61
epoch train time: 0:00:02.134120
elapsed time: 0:01:34.045403
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 23:58:57.486195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.72
 ---- batch: 020 ----
mean loss: 493.86
 ---- batch: 030 ----
mean loss: 491.97
 ---- batch: 040 ----
mean loss: 502.18
 ---- batch: 050 ----
mean loss: 503.96
 ---- batch: 060 ----
mean loss: 490.80
 ---- batch: 070 ----
mean loss: 498.22
 ---- batch: 080 ----
mean loss: 495.12
 ---- batch: 090 ----
mean loss: 483.76
 ---- batch: 100 ----
mean loss: 511.63
 ---- batch: 110 ----
mean loss: 512.04
train mean loss: 498.93
epoch train time: 0:00:02.130522
elapsed time: 0:01:36.176088
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 23:58:59.616856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.57
 ---- batch: 020 ----
mean loss: 493.60
 ---- batch: 030 ----
mean loss: 492.30
 ---- batch: 040 ----
mean loss: 491.95
 ---- batch: 050 ----
mean loss: 497.67
 ---- batch: 060 ----
mean loss: 495.73
 ---- batch: 070 ----
mean loss: 489.62
 ---- batch: 080 ----
mean loss: 513.35
 ---- batch: 090 ----
mean loss: 511.01
 ---- batch: 100 ----
mean loss: 515.40
 ---- batch: 110 ----
mean loss: 509.51
train mean loss: 499.07
epoch train time: 0:00:02.131306
elapsed time: 0:01:38.307528
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 23:59:01.748299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.10
 ---- batch: 020 ----
mean loss: 527.77
 ---- batch: 030 ----
mean loss: 503.32
 ---- batch: 040 ----
mean loss: 503.79
 ---- batch: 050 ----
mean loss: 489.05
 ---- batch: 060 ----
mean loss: 497.01
 ---- batch: 070 ----
mean loss: 491.85
 ---- batch: 080 ----
mean loss: 490.52
 ---- batch: 090 ----
mean loss: 494.87
 ---- batch: 100 ----
mean loss: 504.38
 ---- batch: 110 ----
mean loss: 511.08
train mean loss: 502.05
epoch train time: 0:00:02.137084
elapsed time: 0:01:40.444776
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 23:59:03.885559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.48
 ---- batch: 020 ----
mean loss: 492.04
 ---- batch: 030 ----
mean loss: 494.29
 ---- batch: 040 ----
mean loss: 479.80
 ---- batch: 050 ----
mean loss: 490.63
 ---- batch: 060 ----
mean loss: 498.23
 ---- batch: 070 ----
mean loss: 491.94
 ---- batch: 080 ----
mean loss: 498.15
 ---- batch: 090 ----
mean loss: 504.88
 ---- batch: 100 ----
mean loss: 495.96
 ---- batch: 110 ----
mean loss: 502.91
train mean loss: 494.49
epoch train time: 0:00:02.134093
elapsed time: 0:01:42.579032
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 23:59:06.019807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.51
 ---- batch: 020 ----
mean loss: 497.96
 ---- batch: 030 ----
mean loss: 486.24
 ---- batch: 040 ----
mean loss: 483.61
 ---- batch: 050 ----
mean loss: 489.43
 ---- batch: 060 ----
mean loss: 475.83
 ---- batch: 070 ----
mean loss: 522.23
 ---- batch: 080 ----
mean loss: 489.10
 ---- batch: 090 ----
mean loss: 496.23
 ---- batch: 100 ----
mean loss: 494.61
 ---- batch: 110 ----
mean loss: 496.04
train mean loss: 493.50
epoch train time: 0:00:02.133840
elapsed time: 0:01:44.713021
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 23:59:08.153793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.20
 ---- batch: 020 ----
mean loss: 484.40
 ---- batch: 030 ----
mean loss: 490.48
 ---- batch: 040 ----
mean loss: 498.16
 ---- batch: 050 ----
mean loss: 500.94
 ---- batch: 060 ----
mean loss: 504.05
 ---- batch: 070 ----
mean loss: 486.85
 ---- batch: 080 ----
mean loss: 497.89
 ---- batch: 090 ----
mean loss: 499.35
 ---- batch: 100 ----
mean loss: 490.59
 ---- batch: 110 ----
mean loss: 499.49
train mean loss: 494.56
epoch train time: 0:00:02.133078
elapsed time: 0:01:46.846309
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 23:59:10.287118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.76
 ---- batch: 020 ----
mean loss: 502.02
 ---- batch: 030 ----
mean loss: 493.97
 ---- batch: 040 ----
mean loss: 487.85
 ---- batch: 050 ----
mean loss: 486.48
 ---- batch: 060 ----
mean loss: 497.45
 ---- batch: 070 ----
mean loss: 498.27
 ---- batch: 080 ----
mean loss: 489.33
 ---- batch: 090 ----
mean loss: 498.77
 ---- batch: 100 ----
mean loss: 510.09
 ---- batch: 110 ----
mean loss: 480.43
train mean loss: 496.27
epoch train time: 0:00:02.132196
elapsed time: 0:01:48.978696
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 23:59:12.419470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.15
 ---- batch: 020 ----
mean loss: 496.09
 ---- batch: 030 ----
mean loss: 508.38
 ---- batch: 040 ----
mean loss: 487.21
 ---- batch: 050 ----
mean loss: 491.77
 ---- batch: 060 ----
mean loss: 520.79
 ---- batch: 070 ----
mean loss: 497.36
 ---- batch: 080 ----
mean loss: 502.47
 ---- batch: 090 ----
mean loss: 494.60
 ---- batch: 100 ----
mean loss: 491.56
 ---- batch: 110 ----
mean loss: 506.01
train mean loss: 499.04
epoch train time: 0:00:02.133667
elapsed time: 0:01:51.112513
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 23:59:14.553285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.96
 ---- batch: 020 ----
mean loss: 487.23
 ---- batch: 030 ----
mean loss: 486.54
 ---- batch: 040 ----
mean loss: 492.18
 ---- batch: 050 ----
mean loss: 508.76
 ---- batch: 060 ----
mean loss: 489.53
 ---- batch: 070 ----
mean loss: 479.47
 ---- batch: 080 ----
mean loss: 480.28
 ---- batch: 090 ----
mean loss: 477.49
 ---- batch: 100 ----
mean loss: 492.81
 ---- batch: 110 ----
mean loss: 497.23
train mean loss: 489.77
epoch train time: 0:00:02.136854
elapsed time: 0:01:53.249519
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 23:59:16.690294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.14
 ---- batch: 020 ----
mean loss: 500.28
 ---- batch: 030 ----
mean loss: 495.56
 ---- batch: 040 ----
mean loss: 490.79
 ---- batch: 050 ----
mean loss: 475.29
 ---- batch: 060 ----
mean loss: 482.75
 ---- batch: 070 ----
mean loss: 482.31
 ---- batch: 080 ----
mean loss: 500.13
 ---- batch: 090 ----
mean loss: 479.12
 ---- batch: 100 ----
mean loss: 480.77
 ---- batch: 110 ----
mean loss: 501.41
train mean loss: 489.32
epoch train time: 0:00:02.139669
elapsed time: 0:01:55.389362
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 23:59:18.830138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.43
 ---- batch: 020 ----
mean loss: 495.20
 ---- batch: 030 ----
mean loss: 493.38
 ---- batch: 040 ----
mean loss: 491.47
 ---- batch: 050 ----
mean loss: 484.78
 ---- batch: 060 ----
mean loss: 486.52
 ---- batch: 070 ----
mean loss: 491.09
 ---- batch: 080 ----
mean loss: 493.32
 ---- batch: 090 ----
mean loss: 516.39
 ---- batch: 100 ----
mean loss: 508.63
 ---- batch: 110 ----
mean loss: 500.16
train mean loss: 495.85
epoch train time: 0:00:02.133607
elapsed time: 0:01:57.523120
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 23:59:20.963897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.32
 ---- batch: 020 ----
mean loss: 489.93
 ---- batch: 030 ----
mean loss: 488.04
 ---- batch: 040 ----
mean loss: 495.48
 ---- batch: 050 ----
mean loss: 488.82
 ---- batch: 060 ----
mean loss: 483.93
 ---- batch: 070 ----
mean loss: 504.83
 ---- batch: 080 ----
mean loss: 474.07
 ---- batch: 090 ----
mean loss: 491.29
 ---- batch: 100 ----
mean loss: 495.49
 ---- batch: 110 ----
mean loss: 501.40
train mean loss: 490.62
epoch train time: 0:00:02.133265
elapsed time: 0:01:59.656549
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 23:59:23.097327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.03
 ---- batch: 020 ----
mean loss: 491.92
 ---- batch: 030 ----
mean loss: 500.24
 ---- batch: 040 ----
mean loss: 485.14
 ---- batch: 050 ----
mean loss: 483.72
 ---- batch: 060 ----
mean loss: 489.63
 ---- batch: 070 ----
mean loss: 490.71
 ---- batch: 080 ----
mean loss: 503.31
 ---- batch: 090 ----
mean loss: 491.77
 ---- batch: 100 ----
mean loss: 494.77
 ---- batch: 110 ----
mean loss: 490.85
train mean loss: 491.41
epoch train time: 0:00:02.137968
elapsed time: 0:02:01.794675
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 23:59:25.235471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.96
 ---- batch: 020 ----
mean loss: 492.93
 ---- batch: 030 ----
mean loss: 495.43
 ---- batch: 040 ----
mean loss: 498.05
 ---- batch: 050 ----
mean loss: 485.55
 ---- batch: 060 ----
mean loss: 480.68
 ---- batch: 070 ----
mean loss: 484.54
 ---- batch: 080 ----
mean loss: 479.91
 ---- batch: 090 ----
mean loss: 482.06
 ---- batch: 100 ----
mean loss: 494.03
 ---- batch: 110 ----
mean loss: 501.05
train mean loss: 490.11
epoch train time: 0:00:02.135507
elapsed time: 0:02:03.930355
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 23:59:27.371177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.85
 ---- batch: 020 ----
mean loss: 483.36
 ---- batch: 030 ----
mean loss: 501.60
 ---- batch: 040 ----
mean loss: 504.69
 ---- batch: 050 ----
mean loss: 482.14
 ---- batch: 060 ----
mean loss: 485.06
 ---- batch: 070 ----
mean loss: 486.41
 ---- batch: 080 ----
mean loss: 492.41
 ---- batch: 090 ----
mean loss: 480.53
 ---- batch: 100 ----
mean loss: 506.49
 ---- batch: 110 ----
mean loss: 502.79
train mean loss: 494.18
epoch train time: 0:00:02.134283
elapsed time: 0:02:06.064843
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 23:59:29.505652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.41
 ---- batch: 020 ----
mean loss: 497.42
 ---- batch: 030 ----
mean loss: 483.57
 ---- batch: 040 ----
mean loss: 481.68
 ---- batch: 050 ----
mean loss: 503.51
 ---- batch: 060 ----
mean loss: 513.16
 ---- batch: 070 ----
mean loss: 487.56
 ---- batch: 080 ----
mean loss: 486.76
 ---- batch: 090 ----
mean loss: 483.53
 ---- batch: 100 ----
mean loss: 482.73
 ---- batch: 110 ----
mean loss: 478.41
train mean loss: 489.95
epoch train time: 0:00:02.134319
elapsed time: 0:02:08.199348
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 23:59:31.640123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.90
 ---- batch: 020 ----
mean loss: 493.41
 ---- batch: 030 ----
mean loss: 492.97
 ---- batch: 040 ----
mean loss: 489.84
 ---- batch: 050 ----
mean loss: 494.44
 ---- batch: 060 ----
mean loss: 477.92
 ---- batch: 070 ----
mean loss: 477.98
 ---- batch: 080 ----
mean loss: 495.00
 ---- batch: 090 ----
mean loss: 473.35
 ---- batch: 100 ----
mean loss: 481.93
 ---- batch: 110 ----
mean loss: 494.05
train mean loss: 487.97
epoch train time: 0:00:02.134245
elapsed time: 0:02:10.333751
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 23:59:33.774527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.36
 ---- batch: 020 ----
mean loss: 489.88
 ---- batch: 030 ----
mean loss: 492.85
 ---- batch: 040 ----
mean loss: 492.36
 ---- batch: 050 ----
mean loss: 472.51
 ---- batch: 060 ----
mean loss: 480.59
 ---- batch: 070 ----
mean loss: 498.01
 ---- batch: 080 ----
mean loss: 492.20
 ---- batch: 090 ----
mean loss: 496.87
 ---- batch: 100 ----
mean loss: 479.19
 ---- batch: 110 ----
mean loss: 491.55
train mean loss: 488.61
epoch train time: 0:00:02.133186
elapsed time: 0:02:12.467131
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 23:59:35.907926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.60
 ---- batch: 020 ----
mean loss: 510.96
 ---- batch: 030 ----
mean loss: 488.23
 ---- batch: 040 ----
mean loss: 492.42
 ---- batch: 050 ----
mean loss: 478.35
 ---- batch: 060 ----
mean loss: 505.98
 ---- batch: 070 ----
mean loss: 503.76
 ---- batch: 080 ----
mean loss: 490.00
 ---- batch: 090 ----
mean loss: 498.03
 ---- batch: 100 ----
mean loss: 483.75
 ---- batch: 110 ----
mean loss: 482.03
train mean loss: 492.90
epoch train time: 0:00:02.136313
elapsed time: 0:02:14.603642
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 23:59:38.044439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.21
 ---- batch: 020 ----
mean loss: 507.74
 ---- batch: 030 ----
mean loss: 496.31
 ---- batch: 040 ----
mean loss: 478.00
 ---- batch: 050 ----
mean loss: 499.46
 ---- batch: 060 ----
mean loss: 491.05
 ---- batch: 070 ----
mean loss: 513.02
 ---- batch: 080 ----
mean loss: 489.70
 ---- batch: 090 ----
mean loss: 485.25
 ---- batch: 100 ----
mean loss: 478.52
 ---- batch: 110 ----
mean loss: 486.80
train mean loss: 494.29
epoch train time: 0:00:02.136324
elapsed time: 0:02:16.740145
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 23:59:40.180921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.84
 ---- batch: 020 ----
mean loss: 492.05
 ---- batch: 030 ----
mean loss: 480.23
 ---- batch: 040 ----
mean loss: 514.12
 ---- batch: 050 ----
mean loss: 494.85
 ---- batch: 060 ----
mean loss: 487.61
 ---- batch: 070 ----
mean loss: 475.94
 ---- batch: 080 ----
mean loss: 507.56
 ---- batch: 090 ----
mean loss: 505.29
 ---- batch: 100 ----
mean loss: 493.23
 ---- batch: 110 ----
mean loss: 482.70
train mean loss: 492.71
epoch train time: 0:00:02.138082
elapsed time: 0:02:18.878381
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 23:59:42.319155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.59
 ---- batch: 020 ----
mean loss: 499.85
 ---- batch: 030 ----
mean loss: 474.29
 ---- batch: 040 ----
mean loss: 482.81
 ---- batch: 050 ----
mean loss: 472.86
 ---- batch: 060 ----
mean loss: 487.96
 ---- batch: 070 ----
mean loss: 485.96
 ---- batch: 080 ----
mean loss: 485.66
 ---- batch: 090 ----
mean loss: 495.58
 ---- batch: 100 ----
mean loss: 493.25
 ---- batch: 110 ----
mean loss: 500.69
train mean loss: 488.42
epoch train time: 0:00:02.137675
elapsed time: 0:02:21.016222
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 23:59:44.457004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.13
 ---- batch: 020 ----
mean loss: 493.49
 ---- batch: 030 ----
mean loss: 503.11
 ---- batch: 040 ----
mean loss: 493.45
 ---- batch: 050 ----
mean loss: 481.34
 ---- batch: 060 ----
mean loss: 497.86
 ---- batch: 070 ----
mean loss: 494.12
 ---- batch: 080 ----
mean loss: 475.14
 ---- batch: 090 ----
mean loss: 488.62
 ---- batch: 100 ----
mean loss: 493.83
 ---- batch: 110 ----
mean loss: 498.22
train mean loss: 491.06
epoch train time: 0:00:02.139264
elapsed time: 0:02:23.155643
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 23:59:46.596424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.16
 ---- batch: 020 ----
mean loss: 483.01
 ---- batch: 030 ----
mean loss: 493.36
 ---- batch: 040 ----
mean loss: 495.76
 ---- batch: 050 ----
mean loss: 489.13
 ---- batch: 060 ----
mean loss: 496.09
 ---- batch: 070 ----
mean loss: 478.72
 ---- batch: 080 ----
mean loss: 490.71
 ---- batch: 090 ----
mean loss: 491.58
 ---- batch: 100 ----
mean loss: 465.62
 ---- batch: 110 ----
mean loss: 495.14
train mean loss: 489.78
epoch train time: 0:00:02.129335
elapsed time: 0:02:25.285138
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 23:59:48.725922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.26
 ---- batch: 020 ----
mean loss: 480.04
 ---- batch: 030 ----
mean loss: 497.55
 ---- batch: 040 ----
mean loss: 483.50
 ---- batch: 050 ----
mean loss: 479.01
 ---- batch: 060 ----
mean loss: 489.87
 ---- batch: 070 ----
mean loss: 487.74
 ---- batch: 080 ----
mean loss: 485.40
 ---- batch: 090 ----
mean loss: 486.49
 ---- batch: 100 ----
mean loss: 476.48
 ---- batch: 110 ----
mean loss: 487.89
train mean loss: 484.81
epoch train time: 0:00:02.135101
elapsed time: 0:02:27.420412
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 23:59:50.861187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.34
 ---- batch: 020 ----
mean loss: 489.93
 ---- batch: 030 ----
mean loss: 495.78
 ---- batch: 040 ----
mean loss: 481.68
 ---- batch: 050 ----
mean loss: 484.61
 ---- batch: 060 ----
mean loss: 474.54
 ---- batch: 070 ----
mean loss: 488.25
 ---- batch: 080 ----
mean loss: 483.80
 ---- batch: 090 ----
mean loss: 467.25
 ---- batch: 100 ----
mean loss: 479.66
 ---- batch: 110 ----
mean loss: 502.99
train mean loss: 485.03
epoch train time: 0:00:02.135245
elapsed time: 0:02:29.555825
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 23:59:52.996648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.77
 ---- batch: 020 ----
mean loss: 488.80
 ---- batch: 030 ----
mean loss: 500.79
 ---- batch: 040 ----
mean loss: 490.93
 ---- batch: 050 ----
mean loss: 485.43
 ---- batch: 060 ----
mean loss: 470.34
 ---- batch: 070 ----
mean loss: 488.13
 ---- batch: 080 ----
mean loss: 475.32
 ---- batch: 090 ----
mean loss: 478.33
 ---- batch: 100 ----
mean loss: 500.65
 ---- batch: 110 ----
mean loss: 509.93
train mean loss: 489.37
epoch train time: 0:00:02.132556
elapsed time: 0:02:31.688616
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 23:59:55.129423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.35
 ---- batch: 020 ----
mean loss: 497.87
 ---- batch: 030 ----
mean loss: 494.14
 ---- batch: 040 ----
mean loss: 485.29
 ---- batch: 050 ----
mean loss: 500.26
 ---- batch: 060 ----
mean loss: 487.29
 ---- batch: 070 ----
mean loss: 488.17
 ---- batch: 080 ----
mean loss: 474.92
 ---- batch: 090 ----
mean loss: 485.38
 ---- batch: 100 ----
mean loss: 483.94
 ---- batch: 110 ----
mean loss: 485.46
train mean loss: 489.39
epoch train time: 0:00:02.136221
elapsed time: 0:02:33.825074
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 23:59:57.265903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.13
 ---- batch: 020 ----
mean loss: 475.10
 ---- batch: 030 ----
mean loss: 484.74
 ---- batch: 040 ----
mean loss: 483.51
 ---- batch: 050 ----
mean loss: 500.73
 ---- batch: 060 ----
mean loss: 481.79
 ---- batch: 070 ----
mean loss: 487.43
 ---- batch: 080 ----
mean loss: 461.71
 ---- batch: 090 ----
mean loss: 500.23
 ---- batch: 100 ----
mean loss: 481.67
 ---- batch: 110 ----
mean loss: 486.19
train mean loss: 484.15
epoch train time: 0:00:02.131107
elapsed time: 0:02:35.956389
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 23:59:59.397161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.93
 ---- batch: 020 ----
mean loss: 489.71
 ---- batch: 030 ----
mean loss: 496.63
 ---- batch: 040 ----
mean loss: 484.07
 ---- batch: 050 ----
mean loss: 481.81
 ---- batch: 060 ----
mean loss: 486.80
 ---- batch: 070 ----
mean loss: 479.80
 ---- batch: 080 ----
mean loss: 490.04
 ---- batch: 090 ----
mean loss: 494.09
 ---- batch: 100 ----
mean loss: 480.34
 ---- batch: 110 ----
mean loss: 482.53
train mean loss: 486.76
epoch train time: 0:00:02.136969
elapsed time: 0:02:38.093508
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 00:00:01.534290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.02
 ---- batch: 020 ----
mean loss: 478.27
 ---- batch: 030 ----
mean loss: 485.16
 ---- batch: 040 ----
mean loss: 473.16
 ---- batch: 050 ----
mean loss: 465.01
 ---- batch: 060 ----
mean loss: 479.14
 ---- batch: 070 ----
mean loss: 488.07
 ---- batch: 080 ----
mean loss: 486.15
 ---- batch: 090 ----
mean loss: 490.87
 ---- batch: 100 ----
mean loss: 485.39
 ---- batch: 110 ----
mean loss: 485.90
train mean loss: 482.43
epoch train time: 0:00:02.129229
elapsed time: 0:02:40.222893
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 00:00:03.663664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.04
 ---- batch: 020 ----
mean loss: 477.73
 ---- batch: 030 ----
mean loss: 491.61
 ---- batch: 040 ----
mean loss: 487.92
 ---- batch: 050 ----
mean loss: 498.57
 ---- batch: 060 ----
mean loss: 489.10
 ---- batch: 070 ----
mean loss: 466.25
 ---- batch: 080 ----
mean loss: 501.59
 ---- batch: 090 ----
mean loss: 476.40
 ---- batch: 100 ----
mean loss: 480.67
 ---- batch: 110 ----
mean loss: 469.60
train mean loss: 484.74
epoch train time: 0:00:02.138079
elapsed time: 0:02:42.361122
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 00:00:05.801936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.24
 ---- batch: 020 ----
mean loss: 488.65
 ---- batch: 030 ----
mean loss: 481.91
 ---- batch: 040 ----
mean loss: 451.36
 ---- batch: 050 ----
mean loss: 482.28
 ---- batch: 060 ----
mean loss: 477.83
 ---- batch: 070 ----
mean loss: 496.39
 ---- batch: 080 ----
mean loss: 480.35
 ---- batch: 090 ----
mean loss: 488.90
 ---- batch: 100 ----
mean loss: 471.85
 ---- batch: 110 ----
mean loss: 477.15
train mean loss: 479.41
epoch train time: 0:00:02.139026
elapsed time: 0:02:44.500359
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 00:00:07.941130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.16
 ---- batch: 020 ----
mean loss: 484.50
 ---- batch: 030 ----
mean loss: 482.27
 ---- batch: 040 ----
mean loss: 485.42
 ---- batch: 050 ----
mean loss: 491.88
 ---- batch: 060 ----
mean loss: 476.53
 ---- batch: 070 ----
mean loss: 461.59
 ---- batch: 080 ----
mean loss: 494.72
 ---- batch: 090 ----
mean loss: 481.12
 ---- batch: 100 ----
mean loss: 487.73
 ---- batch: 110 ----
mean loss: 496.99
train mean loss: 484.31
epoch train time: 0:00:02.132934
elapsed time: 0:02:46.633441
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 00:00:10.074213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.10
 ---- batch: 020 ----
mean loss: 471.70
 ---- batch: 030 ----
mean loss: 464.35
 ---- batch: 040 ----
mean loss: 483.66
 ---- batch: 050 ----
mean loss: 458.41
 ---- batch: 060 ----
mean loss: 484.08
 ---- batch: 070 ----
mean loss: 494.93
 ---- batch: 080 ----
mean loss: 483.94
 ---- batch: 090 ----
mean loss: 471.92
 ---- batch: 100 ----
mean loss: 485.61
 ---- batch: 110 ----
mean loss: 484.18
train mean loss: 478.87
epoch train time: 0:00:02.133526
elapsed time: 0:02:48.767124
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 00:00:12.207900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.11
 ---- batch: 020 ----
mean loss: 479.08
 ---- batch: 030 ----
mean loss: 486.31
 ---- batch: 040 ----
mean loss: 481.42
 ---- batch: 050 ----
mean loss: 478.64
 ---- batch: 060 ----
mean loss: 475.96
 ---- batch: 070 ----
mean loss: 486.94
 ---- batch: 080 ----
mean loss: 488.95
 ---- batch: 090 ----
mean loss: 483.22
 ---- batch: 100 ----
mean loss: 485.58
 ---- batch: 110 ----
mean loss: 480.32
train mean loss: 484.76
epoch train time: 0:00:02.138572
elapsed time: 0:02:50.905846
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 00:00:14.346618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.73
 ---- batch: 020 ----
mean loss: 480.20
 ---- batch: 030 ----
mean loss: 475.30
 ---- batch: 040 ----
mean loss: 483.71
 ---- batch: 050 ----
mean loss: 500.38
 ---- batch: 060 ----
mean loss: 466.43
 ---- batch: 070 ----
mean loss: 478.43
 ---- batch: 080 ----
mean loss: 484.47
 ---- batch: 090 ----
mean loss: 478.74
 ---- batch: 100 ----
mean loss: 469.63
 ---- batch: 110 ----
mean loss: 478.33
train mean loss: 480.82
epoch train time: 0:00:02.133950
elapsed time: 0:02:53.039954
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 00:00:16.480756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.05
 ---- batch: 020 ----
mean loss: 493.59
 ---- batch: 030 ----
mean loss: 489.58
 ---- batch: 040 ----
mean loss: 485.93
 ---- batch: 050 ----
mean loss: 473.59
 ---- batch: 060 ----
mean loss: 475.42
 ---- batch: 070 ----
mean loss: 484.80
 ---- batch: 080 ----
mean loss: 491.83
 ---- batch: 090 ----
mean loss: 485.44
 ---- batch: 100 ----
mean loss: 469.61
 ---- batch: 110 ----
mean loss: 482.15
train mean loss: 482.92
epoch train time: 0:00:02.133884
elapsed time: 0:02:55.174032
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 00:00:18.614808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.66
 ---- batch: 020 ----
mean loss: 481.82
 ---- batch: 030 ----
mean loss: 474.15
 ---- batch: 040 ----
mean loss: 478.47
 ---- batch: 050 ----
mean loss: 495.67
 ---- batch: 060 ----
mean loss: 479.51
 ---- batch: 070 ----
mean loss: 488.36
 ---- batch: 080 ----
mean loss: 495.36
 ---- batch: 090 ----
mean loss: 491.50
 ---- batch: 100 ----
mean loss: 482.56
 ---- batch: 110 ----
mean loss: 495.37
train mean loss: 487.45
epoch train time: 0:00:02.133403
elapsed time: 0:02:57.307587
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 00:00:20.748375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.75
 ---- batch: 020 ----
mean loss: 482.15
 ---- batch: 030 ----
mean loss: 480.95
 ---- batch: 040 ----
mean loss: 481.94
 ---- batch: 050 ----
mean loss: 478.64
 ---- batch: 060 ----
mean loss: 484.32
 ---- batch: 070 ----
mean loss: 497.53
 ---- batch: 080 ----
mean loss: 486.77
 ---- batch: 090 ----
mean loss: 490.81
 ---- batch: 100 ----
mean loss: 477.00
 ---- batch: 110 ----
mean loss: 486.05
train mean loss: 483.20
epoch train time: 0:00:02.134341
elapsed time: 0:02:59.442089
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 00:00:22.882863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.49
 ---- batch: 020 ----
mean loss: 472.71
 ---- batch: 030 ----
mean loss: 479.91
 ---- batch: 040 ----
mean loss: 466.29
 ---- batch: 050 ----
mean loss: 488.60
 ---- batch: 060 ----
mean loss: 498.63
 ---- batch: 070 ----
mean loss: 475.93
 ---- batch: 080 ----
mean loss: 491.89
 ---- batch: 090 ----
mean loss: 489.90
 ---- batch: 100 ----
mean loss: 476.18
 ---- batch: 110 ----
mean loss: 486.57
train mean loss: 483.35
epoch train time: 0:00:02.133761
elapsed time: 0:03:01.576002
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 00:00:25.016790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.22
 ---- batch: 020 ----
mean loss: 486.18
 ---- batch: 030 ----
mean loss: 473.68
 ---- batch: 040 ----
mean loss: 479.44
 ---- batch: 050 ----
mean loss: 464.16
 ---- batch: 060 ----
mean loss: 473.20
 ---- batch: 070 ----
mean loss: 488.57
 ---- batch: 080 ----
mean loss: 458.98
 ---- batch: 090 ----
mean loss: 484.52
 ---- batch: 100 ----
mean loss: 497.06
 ---- batch: 110 ----
mean loss: 483.56
train mean loss: 479.14
epoch train time: 0:00:02.134223
elapsed time: 0:03:03.710390
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 00:00:27.151164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.63
 ---- batch: 020 ----
mean loss: 484.36
 ---- batch: 030 ----
mean loss: 492.28
 ---- batch: 040 ----
mean loss: 471.92
 ---- batch: 050 ----
mean loss: 474.02
 ---- batch: 060 ----
mean loss: 488.81
 ---- batch: 070 ----
mean loss: 471.76
 ---- batch: 080 ----
mean loss: 501.02
 ---- batch: 090 ----
mean loss: 479.43
 ---- batch: 100 ----
mean loss: 484.69
 ---- batch: 110 ----
mean loss: 477.33
train mean loss: 481.97
epoch train time: 0:00:02.135832
elapsed time: 0:03:05.846377
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 00:00:29.287161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.29
 ---- batch: 020 ----
mean loss: 482.39
 ---- batch: 030 ----
mean loss: 473.89
 ---- batch: 040 ----
mean loss: 483.71
 ---- batch: 050 ----
mean loss: 484.10
 ---- batch: 060 ----
mean loss: 485.79
 ---- batch: 070 ----
mean loss: 474.09
 ---- batch: 080 ----
mean loss: 494.42
 ---- batch: 090 ----
mean loss: 482.13
 ---- batch: 100 ----
mean loss: 487.49
 ---- batch: 110 ----
mean loss: 472.10
train mean loss: 480.19
epoch train time: 0:00:02.133781
elapsed time: 0:03:07.980322
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 00:00:31.421096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.78
 ---- batch: 020 ----
mean loss: 488.70
 ---- batch: 030 ----
mean loss: 477.89
 ---- batch: 040 ----
mean loss: 459.74
 ---- batch: 050 ----
mean loss: 479.25
 ---- batch: 060 ----
mean loss: 493.86
 ---- batch: 070 ----
mean loss: 472.30
 ---- batch: 080 ----
mean loss: 501.30
 ---- batch: 090 ----
mean loss: 482.11
 ---- batch: 100 ----
mean loss: 487.48
 ---- batch: 110 ----
mean loss: 487.55
train mean loss: 483.90
epoch train time: 0:00:02.134394
elapsed time: 0:03:10.114866
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 00:00:33.555650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.43
 ---- batch: 020 ----
mean loss: 481.74
 ---- batch: 030 ----
mean loss: 468.07
 ---- batch: 040 ----
mean loss: 474.03
 ---- batch: 050 ----
mean loss: 477.57
 ---- batch: 060 ----
mean loss: 484.54
 ---- batch: 070 ----
mean loss: 464.41
 ---- batch: 080 ----
mean loss: 491.71
 ---- batch: 090 ----
mean loss: 490.58
 ---- batch: 100 ----
mean loss: 478.20
 ---- batch: 110 ----
mean loss: 487.82
train mean loss: 479.74
epoch train time: 0:00:02.134261
elapsed time: 0:03:12.249303
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 00:00:35.690095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.12
 ---- batch: 020 ----
mean loss: 479.26
 ---- batch: 030 ----
mean loss: 485.55
 ---- batch: 040 ----
mean loss: 479.99
 ---- batch: 050 ----
mean loss: 486.44
 ---- batch: 060 ----
mean loss: 468.73
 ---- batch: 070 ----
mean loss: 471.95
 ---- batch: 080 ----
mean loss: 478.69
 ---- batch: 090 ----
mean loss: 479.95
 ---- batch: 100 ----
mean loss: 483.34
 ---- batch: 110 ----
mean loss: 480.39
train mean loss: 480.62
epoch train time: 0:00:02.137468
elapsed time: 0:03:14.386950
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 00:00:37.827731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.65
 ---- batch: 020 ----
mean loss: 480.24
 ---- batch: 030 ----
mean loss: 471.52
 ---- batch: 040 ----
mean loss: 473.52
 ---- batch: 050 ----
mean loss: 473.24
 ---- batch: 060 ----
mean loss: 478.33
 ---- batch: 070 ----
mean loss: 480.87
 ---- batch: 080 ----
mean loss: 472.21
 ---- batch: 090 ----
mean loss: 465.00
 ---- batch: 100 ----
mean loss: 473.06
 ---- batch: 110 ----
mean loss: 471.13
train mean loss: 473.86
epoch train time: 0:00:02.133064
elapsed time: 0:03:16.520166
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 00:00:39.960959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.81
 ---- batch: 020 ----
mean loss: 472.52
 ---- batch: 030 ----
mean loss: 474.88
 ---- batch: 040 ----
mean loss: 479.71
 ---- batch: 050 ----
mean loss: 493.79
 ---- batch: 060 ----
mean loss: 480.62
 ---- batch: 070 ----
mean loss: 476.58
 ---- batch: 080 ----
mean loss: 474.99
 ---- batch: 090 ----
mean loss: 490.67
 ---- batch: 100 ----
mean loss: 492.85
 ---- batch: 110 ----
mean loss: 476.50
train mean loss: 481.67
epoch train time: 0:00:02.134489
elapsed time: 0:03:18.654828
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 00:00:42.095602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.00
 ---- batch: 020 ----
mean loss: 477.74
 ---- batch: 030 ----
mean loss: 474.30
 ---- batch: 040 ----
mean loss: 482.73
 ---- batch: 050 ----
mean loss: 471.20
 ---- batch: 060 ----
mean loss: 477.66
 ---- batch: 070 ----
mean loss: 494.99
 ---- batch: 080 ----
mean loss: 485.28
 ---- batch: 090 ----
mean loss: 458.42
 ---- batch: 100 ----
mean loss: 485.47
 ---- batch: 110 ----
mean loss: 472.48
train mean loss: 477.48
epoch train time: 0:00:02.131407
elapsed time: 0:03:20.786385
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 00:00:44.227153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.91
 ---- batch: 020 ----
mean loss: 472.90
 ---- batch: 030 ----
mean loss: 463.37
 ---- batch: 040 ----
mean loss: 478.84
 ---- batch: 050 ----
mean loss: 483.57
 ---- batch: 060 ----
mean loss: 476.07
 ---- batch: 070 ----
mean loss: 467.17
 ---- batch: 080 ----
mean loss: 459.92
 ---- batch: 090 ----
mean loss: 489.58
 ---- batch: 100 ----
mean loss: 472.60
 ---- batch: 110 ----
mean loss: 483.39
train mean loss: 475.98
epoch train time: 0:00:02.137631
elapsed time: 0:03:22.924179
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 00:00:46.364968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.07
 ---- batch: 020 ----
mean loss: 479.17
 ---- batch: 030 ----
mean loss: 466.18
 ---- batch: 040 ----
mean loss: 475.56
 ---- batch: 050 ----
mean loss: 482.50
 ---- batch: 060 ----
mean loss: 481.44
 ---- batch: 070 ----
mean loss: 484.84
 ---- batch: 080 ----
mean loss: 486.79
 ---- batch: 090 ----
mean loss: 474.31
 ---- batch: 100 ----
mean loss: 486.57
 ---- batch: 110 ----
mean loss: 474.48
train mean loss: 478.08
epoch train time: 0:00:02.136808
elapsed time: 0:03:25.061190
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 00:00:48.502013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.27
 ---- batch: 020 ----
mean loss: 484.35
 ---- batch: 030 ----
mean loss: 479.58
 ---- batch: 040 ----
mean loss: 475.43
 ---- batch: 050 ----
mean loss: 492.22
 ---- batch: 060 ----
mean loss: 476.41
 ---- batch: 070 ----
mean loss: 477.89
 ---- batch: 080 ----
mean loss: 486.00
 ---- batch: 090 ----
mean loss: 496.19
 ---- batch: 100 ----
mean loss: 471.66
 ---- batch: 110 ----
mean loss: 478.02
train mean loss: 480.92
epoch train time: 0:00:02.134333
elapsed time: 0:03:27.195720
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 00:00:50.636491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.45
 ---- batch: 020 ----
mean loss: 471.86
 ---- batch: 030 ----
mean loss: 484.56
 ---- batch: 040 ----
mean loss: 482.27
 ---- batch: 050 ----
mean loss: 467.27
 ---- batch: 060 ----
mean loss: 474.87
 ---- batch: 070 ----
mean loss: 477.41
 ---- batch: 080 ----
mean loss: 477.21
 ---- batch: 090 ----
mean loss: 455.78
 ---- batch: 100 ----
mean loss: 485.02
 ---- batch: 110 ----
mean loss: 461.14
train mean loss: 474.40
epoch train time: 0:00:02.133933
elapsed time: 0:03:29.329800
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 00:00:52.770572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.71
 ---- batch: 020 ----
mean loss: 481.64
 ---- batch: 030 ----
mean loss: 468.02
 ---- batch: 040 ----
mean loss: 464.83
 ---- batch: 050 ----
mean loss: 465.31
 ---- batch: 060 ----
mean loss: 481.88
 ---- batch: 070 ----
mean loss: 485.16
 ---- batch: 080 ----
mean loss: 472.01
 ---- batch: 090 ----
mean loss: 472.94
 ---- batch: 100 ----
mean loss: 478.98
 ---- batch: 110 ----
mean loss: 476.38
train mean loss: 475.57
epoch train time: 0:00:02.135947
elapsed time: 0:03:31.465917
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 00:00:54.906689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.73
 ---- batch: 020 ----
mean loss: 468.51
 ---- batch: 030 ----
mean loss: 477.93
 ---- batch: 040 ----
mean loss: 464.58
 ---- batch: 050 ----
mean loss: 466.88
 ---- batch: 060 ----
mean loss: 477.83
 ---- batch: 070 ----
mean loss: 475.82
 ---- batch: 080 ----
mean loss: 484.60
 ---- batch: 090 ----
mean loss: 470.69
 ---- batch: 100 ----
mean loss: 465.28
 ---- batch: 110 ----
mean loss: 467.62
train mean loss: 472.59
epoch train time: 0:00:02.135483
elapsed time: 0:03:33.601548
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 00:00:57.042326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.47
 ---- batch: 020 ----
mean loss: 487.21
 ---- batch: 030 ----
mean loss: 478.23
 ---- batch: 040 ----
mean loss: 485.67
 ---- batch: 050 ----
mean loss: 485.77
 ---- batch: 060 ----
mean loss: 473.57
 ---- batch: 070 ----
mean loss: 472.30
 ---- batch: 080 ----
mean loss: 469.03
 ---- batch: 090 ----
mean loss: 466.60
 ---- batch: 100 ----
mean loss: 482.33
 ---- batch: 110 ----
mean loss: 481.23
train mean loss: 475.89
epoch train time: 0:00:02.137379
elapsed time: 0:03:35.739082
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 00:00:59.179870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.70
 ---- batch: 020 ----
mean loss: 496.13
 ---- batch: 030 ----
mean loss: 480.13
 ---- batch: 040 ----
mean loss: 474.51
 ---- batch: 050 ----
mean loss: 477.23
 ---- batch: 060 ----
mean loss: 467.92
 ---- batch: 070 ----
mean loss: 465.11
 ---- batch: 080 ----
mean loss: 453.74
 ---- batch: 090 ----
mean loss: 492.30
 ---- batch: 100 ----
mean loss: 466.12
 ---- batch: 110 ----
mean loss: 470.56
train mean loss: 473.24
epoch train time: 0:00:02.136094
elapsed time: 0:03:37.875340
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 00:01:01.316115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.67
 ---- batch: 020 ----
mean loss: 487.36
 ---- batch: 030 ----
mean loss: 482.16
 ---- batch: 040 ----
mean loss: 460.09
 ---- batch: 050 ----
mean loss: 476.32
 ---- batch: 060 ----
mean loss: 484.63
 ---- batch: 070 ----
mean loss: 468.13
 ---- batch: 080 ----
mean loss: 487.33
 ---- batch: 090 ----
mean loss: 473.61
 ---- batch: 100 ----
mean loss: 458.88
 ---- batch: 110 ----
mean loss: 466.37
train mean loss: 473.89
epoch train time: 0:00:02.136595
elapsed time: 0:03:40.012086
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 00:01:03.452877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.36
 ---- batch: 020 ----
mean loss: 487.58
 ---- batch: 030 ----
mean loss: 453.37
 ---- batch: 040 ----
mean loss: 469.10
 ---- batch: 050 ----
mean loss: 472.56
 ---- batch: 060 ----
mean loss: 471.38
 ---- batch: 070 ----
mean loss: 466.25
 ---- batch: 080 ----
mean loss: 487.13
 ---- batch: 090 ----
mean loss: 475.11
 ---- batch: 100 ----
mean loss: 489.97
 ---- batch: 110 ----
mean loss: 486.00
train mean loss: 475.35
epoch train time: 0:00:02.133152
elapsed time: 0:03:42.145409
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 00:01:05.586231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.07
 ---- batch: 020 ----
mean loss: 479.25
 ---- batch: 030 ----
mean loss: 457.65
 ---- batch: 040 ----
mean loss: 469.02
 ---- batch: 050 ----
mean loss: 473.47
 ---- batch: 060 ----
mean loss: 468.60
 ---- batch: 070 ----
mean loss: 472.59
 ---- batch: 080 ----
mean loss: 483.15
 ---- batch: 090 ----
mean loss: 470.03
 ---- batch: 100 ----
mean loss: 472.47
 ---- batch: 110 ----
mean loss: 474.19
train mean loss: 472.27
epoch train time: 0:00:02.130985
elapsed time: 0:03:44.276590
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 00:01:07.717364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.65
 ---- batch: 020 ----
mean loss: 464.90
 ---- batch: 030 ----
mean loss: 464.22
 ---- batch: 040 ----
mean loss: 476.19
 ---- batch: 050 ----
mean loss: 474.29
 ---- batch: 060 ----
mean loss: 487.38
 ---- batch: 070 ----
mean loss: 455.65
 ---- batch: 080 ----
mean loss: 478.11
 ---- batch: 090 ----
mean loss: 479.26
 ---- batch: 100 ----
mean loss: 477.34
 ---- batch: 110 ----
mean loss: 489.74
train mean loss: 473.19
epoch train time: 0:00:02.136800
elapsed time: 0:03:46.413540
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 00:01:09.854310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.17
 ---- batch: 020 ----
mean loss: 482.49
 ---- batch: 030 ----
mean loss: 491.62
 ---- batch: 040 ----
mean loss: 463.81
 ---- batch: 050 ----
mean loss: 462.81
 ---- batch: 060 ----
mean loss: 478.23
 ---- batch: 070 ----
mean loss: 477.34
 ---- batch: 080 ----
mean loss: 464.04
 ---- batch: 090 ----
mean loss: 480.19
 ---- batch: 100 ----
mean loss: 467.86
 ---- batch: 110 ----
mean loss: 492.74
train mean loss: 475.73
epoch train time: 0:00:02.134860
elapsed time: 0:03:48.548545
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 00:01:11.989318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.17
 ---- batch: 020 ----
mean loss: 479.45
 ---- batch: 030 ----
mean loss: 483.22
 ---- batch: 040 ----
mean loss: 474.04
 ---- batch: 050 ----
mean loss: 474.86
 ---- batch: 060 ----
mean loss: 471.90
 ---- batch: 070 ----
mean loss: 467.18
 ---- batch: 080 ----
mean loss: 478.76
 ---- batch: 090 ----
mean loss: 464.76
 ---- batch: 100 ----
mean loss: 454.55
 ---- batch: 110 ----
mean loss: 473.18
train mean loss: 472.80
epoch train time: 0:00:02.137409
elapsed time: 0:03:50.686110
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 00:01:14.126881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.40
 ---- batch: 020 ----
mean loss: 486.97
 ---- batch: 030 ----
mean loss: 477.73
 ---- batch: 040 ----
mean loss: 467.39
 ---- batch: 050 ----
mean loss: 467.53
 ---- batch: 060 ----
mean loss: 462.12
 ---- batch: 070 ----
mean loss: 465.81
 ---- batch: 080 ----
mean loss: 473.79
 ---- batch: 090 ----
mean loss: 460.40
 ---- batch: 100 ----
mean loss: 459.32
 ---- batch: 110 ----
mean loss: 467.78
train mean loss: 469.84
epoch train time: 0:00:02.134609
elapsed time: 0:03:52.820867
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 00:01:16.261707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.47
 ---- batch: 020 ----
mean loss: 474.40
 ---- batch: 030 ----
mean loss: 471.99
 ---- batch: 040 ----
mean loss: 463.39
 ---- batch: 050 ----
mean loss: 476.62
 ---- batch: 060 ----
mean loss: 461.98
 ---- batch: 070 ----
mean loss: 464.11
 ---- batch: 080 ----
mean loss: 457.71
 ---- batch: 090 ----
mean loss: 468.21
 ---- batch: 100 ----
mean loss: 467.76
 ---- batch: 110 ----
mean loss: 470.69
train mean loss: 468.02
epoch train time: 0:00:02.136990
elapsed time: 0:03:54.958073
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 00:01:18.398851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.85
 ---- batch: 020 ----
mean loss: 469.88
 ---- batch: 030 ----
mean loss: 470.79
 ---- batch: 040 ----
mean loss: 477.13
 ---- batch: 050 ----
mean loss: 453.12
 ---- batch: 060 ----
mean loss: 470.11
 ---- batch: 070 ----
mean loss: 481.97
 ---- batch: 080 ----
mean loss: 465.41
 ---- batch: 090 ----
mean loss: 479.18
 ---- batch: 100 ----
mean loss: 471.96
 ---- batch: 110 ----
mean loss: 467.17
train mean loss: 470.04
epoch train time: 0:00:02.136612
elapsed time: 0:03:57.094850
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 00:01:20.535633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.03
 ---- batch: 020 ----
mean loss: 490.59
 ---- batch: 030 ----
mean loss: 471.19
 ---- batch: 040 ----
mean loss: 457.36
 ---- batch: 050 ----
mean loss: 456.92
 ---- batch: 060 ----
mean loss: 472.01
 ---- batch: 070 ----
mean loss: 466.25
 ---- batch: 080 ----
mean loss: 466.04
 ---- batch: 090 ----
mean loss: 464.07
 ---- batch: 100 ----
mean loss: 456.50
 ---- batch: 110 ----
mean loss: 487.87
train mean loss: 469.02
epoch train time: 0:00:02.136968
elapsed time: 0:03:59.231978
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 00:01:22.672752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.56
 ---- batch: 020 ----
mean loss: 464.74
 ---- batch: 030 ----
mean loss: 460.23
 ---- batch: 040 ----
mean loss: 474.63
 ---- batch: 050 ----
mean loss: 478.86
 ---- batch: 060 ----
mean loss: 463.92
 ---- batch: 070 ----
mean loss: 464.43
 ---- batch: 080 ----
mean loss: 460.95
 ---- batch: 090 ----
mean loss: 472.42
 ---- batch: 100 ----
mean loss: 455.68
 ---- batch: 110 ----
mean loss: 466.05
train mean loss: 466.46
epoch train time: 0:00:02.135917
elapsed time: 0:04:01.368044
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 00:01:24.808817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.26
 ---- batch: 020 ----
mean loss: 470.12
 ---- batch: 030 ----
mean loss: 479.44
 ---- batch: 040 ----
mean loss: 471.65
 ---- batch: 050 ----
mean loss: 483.49
 ---- batch: 060 ----
mean loss: 464.62
 ---- batch: 070 ----
mean loss: 467.85
 ---- batch: 080 ----
mean loss: 466.13
 ---- batch: 090 ----
mean loss: 472.32
 ---- batch: 100 ----
mean loss: 475.52
 ---- batch: 110 ----
mean loss: 468.06
train mean loss: 470.02
epoch train time: 0:00:02.143614
elapsed time: 0:04:03.511808
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 00:01:26.952583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.01
 ---- batch: 020 ----
mean loss: 463.80
 ---- batch: 030 ----
mean loss: 454.43
 ---- batch: 040 ----
mean loss: 464.68
 ---- batch: 050 ----
mean loss: 469.89
 ---- batch: 060 ----
mean loss: 466.54
 ---- batch: 070 ----
mean loss: 457.14
 ---- batch: 080 ----
mean loss: 466.76
 ---- batch: 090 ----
mean loss: 465.65
 ---- batch: 100 ----
mean loss: 459.60
 ---- batch: 110 ----
mean loss: 469.20
train mean loss: 462.90
epoch train time: 0:00:02.134769
elapsed time: 0:04:05.646728
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 00:01:29.087533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.16
 ---- batch: 020 ----
mean loss: 471.58
 ---- batch: 030 ----
mean loss: 469.26
 ---- batch: 040 ----
mean loss: 467.79
 ---- batch: 050 ----
mean loss: 456.03
 ---- batch: 060 ----
mean loss: 468.28
 ---- batch: 070 ----
mean loss: 475.14
 ---- batch: 080 ----
mean loss: 454.70
 ---- batch: 090 ----
mean loss: 463.36
 ---- batch: 100 ----
mean loss: 470.73
 ---- batch: 110 ----
mean loss: 488.82
train mean loss: 468.02
epoch train time: 0:00:02.134720
elapsed time: 0:04:07.781629
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 00:01:31.222434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.99
 ---- batch: 020 ----
mean loss: 463.45
 ---- batch: 030 ----
mean loss: 472.94
 ---- batch: 040 ----
mean loss: 467.17
 ---- batch: 050 ----
mean loss: 469.00
 ---- batch: 060 ----
mean loss: 461.75
 ---- batch: 070 ----
mean loss: 469.85
 ---- batch: 080 ----
mean loss: 478.17
 ---- batch: 090 ----
mean loss: 447.10
 ---- batch: 100 ----
mean loss: 468.60
 ---- batch: 110 ----
mean loss: 483.43
train mean loss: 469.16
epoch train time: 0:00:02.136569
elapsed time: 0:04:09.918419
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 00:01:33.359192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.41
 ---- batch: 020 ----
mean loss: 486.73
 ---- batch: 030 ----
mean loss: 479.83
 ---- batch: 040 ----
mean loss: 464.50
 ---- batch: 050 ----
mean loss: 465.56
 ---- batch: 060 ----
mean loss: 436.64
 ---- batch: 070 ----
mean loss: 467.56
 ---- batch: 080 ----
mean loss: 476.85
 ---- batch: 090 ----
mean loss: 467.21
 ---- batch: 100 ----
mean loss: 470.47
 ---- batch: 110 ----
mean loss: 461.65
train mean loss: 467.63
epoch train time: 0:00:02.133125
elapsed time: 0:04:12.051697
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 00:01:35.492467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.20
 ---- batch: 020 ----
mean loss: 450.78
 ---- batch: 030 ----
mean loss: 470.35
 ---- batch: 040 ----
mean loss: 451.28
 ---- batch: 050 ----
mean loss: 456.98
 ---- batch: 060 ----
mean loss: 458.40
 ---- batch: 070 ----
mean loss: 454.10
 ---- batch: 080 ----
mean loss: 478.16
 ---- batch: 090 ----
mean loss: 470.65
 ---- batch: 100 ----
mean loss: 465.66
 ---- batch: 110 ----
mean loss: 463.73
train mean loss: 460.62
epoch train time: 0:00:02.139601
elapsed time: 0:04:14.191451
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 00:01:37.632225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.91
 ---- batch: 020 ----
mean loss: 448.95
 ---- batch: 030 ----
mean loss: 443.43
 ---- batch: 040 ----
mean loss: 465.91
 ---- batch: 050 ----
mean loss: 463.79
 ---- batch: 060 ----
mean loss: 470.96
 ---- batch: 070 ----
mean loss: 464.84
 ---- batch: 080 ----
mean loss: 464.35
 ---- batch: 090 ----
mean loss: 462.03
 ---- batch: 100 ----
mean loss: 455.42
 ---- batch: 110 ----
mean loss: 461.10
train mean loss: 461.04
epoch train time: 0:00:02.138942
elapsed time: 0:04:16.330574
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 00:01:39.771347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.32
 ---- batch: 020 ----
mean loss: 467.13
 ---- batch: 030 ----
mean loss: 457.39
 ---- batch: 040 ----
mean loss: 465.14
 ---- batch: 050 ----
mean loss: 483.54
 ---- batch: 060 ----
mean loss: 451.27
 ---- batch: 070 ----
mean loss: 466.07
 ---- batch: 080 ----
mean loss: 448.42
 ---- batch: 090 ----
mean loss: 465.88
 ---- batch: 100 ----
mean loss: 466.87
 ---- batch: 110 ----
mean loss: 471.25
train mean loss: 464.49
epoch train time: 0:00:02.141635
elapsed time: 0:04:18.472358
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 00:01:41.913134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.24
 ---- batch: 020 ----
mean loss: 460.89
 ---- batch: 030 ----
mean loss: 454.38
 ---- batch: 040 ----
mean loss: 446.97
 ---- batch: 050 ----
mean loss: 454.75
 ---- batch: 060 ----
mean loss: 467.17
 ---- batch: 070 ----
mean loss: 461.93
 ---- batch: 080 ----
mean loss: 461.01
 ---- batch: 090 ----
mean loss: 469.50
 ---- batch: 100 ----
mean loss: 466.94
 ---- batch: 110 ----
mean loss: 446.26
train mean loss: 459.93
epoch train time: 0:00:02.135789
elapsed time: 0:04:20.608418
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 00:01:44.049214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.69
 ---- batch: 020 ----
mean loss: 468.97
 ---- batch: 030 ----
mean loss: 458.37
 ---- batch: 040 ----
mean loss: 463.52
 ---- batch: 050 ----
mean loss: 469.37
 ---- batch: 060 ----
mean loss: 451.80
 ---- batch: 070 ----
mean loss: 455.68
 ---- batch: 080 ----
mean loss: 463.02
 ---- batch: 090 ----
mean loss: 468.06
 ---- batch: 100 ----
mean loss: 437.76
 ---- batch: 110 ----
mean loss: 458.99
train mean loss: 460.18
epoch train time: 0:00:02.137817
elapsed time: 0:04:22.746407
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 00:01:46.187177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.63
 ---- batch: 020 ----
mean loss: 455.62
 ---- batch: 030 ----
mean loss: 464.43
 ---- batch: 040 ----
mean loss: 450.31
 ---- batch: 050 ----
mean loss: 463.58
 ---- batch: 060 ----
mean loss: 483.99
 ---- batch: 070 ----
mean loss: 460.40
 ---- batch: 080 ----
mean loss: 462.34
 ---- batch: 090 ----
mean loss: 471.49
 ---- batch: 100 ----
mean loss: 466.21
 ---- batch: 110 ----
mean loss: 450.27
train mean loss: 462.47
epoch train time: 0:00:02.135508
elapsed time: 0:04:24.882061
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 00:01:48.322833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.94
 ---- batch: 020 ----
mean loss: 469.29
 ---- batch: 030 ----
mean loss: 466.70
 ---- batch: 040 ----
mean loss: 450.32
 ---- batch: 050 ----
mean loss: 460.78
 ---- batch: 060 ----
mean loss: 451.45
 ---- batch: 070 ----
mean loss: 463.45
 ---- batch: 080 ----
mean loss: 455.48
 ---- batch: 090 ----
mean loss: 465.35
 ---- batch: 100 ----
mean loss: 473.21
 ---- batch: 110 ----
mean loss: 461.54
train mean loss: 460.69
epoch train time: 0:00:02.135281
elapsed time: 0:04:27.017501
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 00:01:50.458289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.43
 ---- batch: 020 ----
mean loss: 450.10
 ---- batch: 030 ----
mean loss: 469.34
 ---- batch: 040 ----
mean loss: 457.57
 ---- batch: 050 ----
mean loss: 458.34
 ---- batch: 060 ----
mean loss: 444.74
 ---- batch: 070 ----
mean loss: 457.51
 ---- batch: 080 ----
mean loss: 464.79
 ---- batch: 090 ----
mean loss: 458.19
 ---- batch: 100 ----
mean loss: 470.03
 ---- batch: 110 ----
mean loss: 475.67
train mean loss: 459.79
epoch train time: 0:00:02.135013
elapsed time: 0:04:29.152677
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 00:01:52.593457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.11
 ---- batch: 020 ----
mean loss: 454.77
 ---- batch: 030 ----
mean loss: 447.23
 ---- batch: 040 ----
mean loss: 446.90
 ---- batch: 050 ----
mean loss: 468.51
 ---- batch: 060 ----
mean loss: 459.90
 ---- batch: 070 ----
mean loss: 454.28
 ---- batch: 080 ----
mean loss: 465.97
 ---- batch: 090 ----
mean loss: 447.38
 ---- batch: 100 ----
mean loss: 448.90
 ---- batch: 110 ----
mean loss: 468.92
train mean loss: 456.84
epoch train time: 0:00:02.135679
elapsed time: 0:04:31.288540
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 00:01:54.729314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.90
 ---- batch: 020 ----
mean loss: 467.33
 ---- batch: 030 ----
mean loss: 456.99
 ---- batch: 040 ----
mean loss: 455.27
 ---- batch: 050 ----
mean loss: 448.26
 ---- batch: 060 ----
mean loss: 464.19
 ---- batch: 070 ----
mean loss: 456.80
 ---- batch: 080 ----
mean loss: 465.90
 ---- batch: 090 ----
mean loss: 475.67
 ---- batch: 100 ----
mean loss: 486.10
 ---- batch: 110 ----
mean loss: 464.65
train mean loss: 463.44
epoch train time: 0:00:02.137270
elapsed time: 0:04:33.425974
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 00:01:56.866769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.59
 ---- batch: 020 ----
mean loss: 455.01
 ---- batch: 030 ----
mean loss: 470.32
 ---- batch: 040 ----
mean loss: 461.47
 ---- batch: 050 ----
mean loss: 468.03
 ---- batch: 060 ----
mean loss: 459.56
 ---- batch: 070 ----
mean loss: 465.32
 ---- batch: 080 ----
mean loss: 459.07
 ---- batch: 090 ----
mean loss: 454.41
 ---- batch: 100 ----
mean loss: 446.58
 ---- batch: 110 ----
mean loss: 456.93
train mean loss: 459.25
epoch train time: 0:00:02.137346
elapsed time: 0:04:35.563495
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 00:01:59.004289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.07
 ---- batch: 020 ----
mean loss: 452.91
 ---- batch: 030 ----
mean loss: 450.77
 ---- batch: 040 ----
mean loss: 452.30
 ---- batch: 050 ----
mean loss: 465.43
 ---- batch: 060 ----
mean loss: 463.62
 ---- batch: 070 ----
mean loss: 458.70
 ---- batch: 080 ----
mean loss: 465.11
 ---- batch: 090 ----
mean loss: 451.73
 ---- batch: 100 ----
mean loss: 455.83
 ---- batch: 110 ----
mean loss: 456.27
train mean loss: 457.17
epoch train time: 0:00:02.136907
elapsed time: 0:04:37.700571
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 00:02:01.141341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.67
 ---- batch: 020 ----
mean loss: 454.33
 ---- batch: 030 ----
mean loss: 461.89
 ---- batch: 040 ----
mean loss: 448.92
 ---- batch: 050 ----
mean loss: 452.33
 ---- batch: 060 ----
mean loss: 441.03
 ---- batch: 070 ----
mean loss: 444.34
 ---- batch: 080 ----
mean loss: 467.33
 ---- batch: 090 ----
mean loss: 464.37
 ---- batch: 100 ----
mean loss: 447.39
 ---- batch: 110 ----
mean loss: 455.85
train mean loss: 454.99
epoch train time: 0:00:02.133462
elapsed time: 0:04:39.834186
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 00:02:03.275004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.40
 ---- batch: 020 ----
mean loss: 433.05
 ---- batch: 030 ----
mean loss: 452.34
 ---- batch: 040 ----
mean loss: 463.57
 ---- batch: 050 ----
mean loss: 454.99
 ---- batch: 060 ----
mean loss: 463.39
 ---- batch: 070 ----
mean loss: 455.09
 ---- batch: 080 ----
mean loss: 468.01
 ---- batch: 090 ----
mean loss: 447.18
 ---- batch: 100 ----
mean loss: 469.58
 ---- batch: 110 ----
mean loss: 451.24
train mean loss: 455.39
epoch train time: 0:00:02.133196
elapsed time: 0:04:41.967574
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 00:02:05.408345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.65
 ---- batch: 020 ----
mean loss: 457.14
 ---- batch: 030 ----
mean loss: 455.54
 ---- batch: 040 ----
mean loss: 450.63
 ---- batch: 050 ----
mean loss: 451.35
 ---- batch: 060 ----
mean loss: 453.01
 ---- batch: 070 ----
mean loss: 442.44
 ---- batch: 080 ----
mean loss: 472.04
 ---- batch: 090 ----
mean loss: 457.99
 ---- batch: 100 ----
mean loss: 456.36
 ---- batch: 110 ----
mean loss: 441.32
train mean loss: 454.23
epoch train time: 0:00:02.136590
elapsed time: 0:04:44.104321
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 00:02:07.545099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.09
 ---- batch: 020 ----
mean loss: 462.45
 ---- batch: 030 ----
mean loss: 454.54
 ---- batch: 040 ----
mean loss: 444.13
 ---- batch: 050 ----
mean loss: 461.14
 ---- batch: 060 ----
mean loss: 459.43
 ---- batch: 070 ----
mean loss: 451.71
 ---- batch: 080 ----
mean loss: 453.60
 ---- batch: 090 ----
mean loss: 463.90
 ---- batch: 100 ----
mean loss: 451.96
 ---- batch: 110 ----
mean loss: 452.50
train mean loss: 454.60
epoch train time: 0:00:02.135797
elapsed time: 0:04:46.240277
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 00:02:09.681052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.48
 ---- batch: 020 ----
mean loss: 438.47
 ---- batch: 030 ----
mean loss: 449.59
 ---- batch: 040 ----
mean loss: 434.58
 ---- batch: 050 ----
mean loss: 458.63
 ---- batch: 060 ----
mean loss: 452.48
 ---- batch: 070 ----
mean loss: 461.98
 ---- batch: 080 ----
mean loss: 449.97
 ---- batch: 090 ----
mean loss: 459.99
 ---- batch: 100 ----
mean loss: 437.69
 ---- batch: 110 ----
mean loss: 459.75
train mean loss: 449.81
epoch train time: 0:00:02.133846
elapsed time: 0:04:48.374286
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 00:02:11.815065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.39
 ---- batch: 020 ----
mean loss: 436.73
 ---- batch: 030 ----
mean loss: 448.69
 ---- batch: 040 ----
mean loss: 456.16
 ---- batch: 050 ----
mean loss: 453.34
 ---- batch: 060 ----
mean loss: 454.76
 ---- batch: 070 ----
mean loss: 450.71
 ---- batch: 080 ----
mean loss: 445.40
 ---- batch: 090 ----
mean loss: 455.87
 ---- batch: 100 ----
mean loss: 461.58
 ---- batch: 110 ----
mean loss: 450.73
train mean loss: 450.30
epoch train time: 0:00:02.134239
elapsed time: 0:04:50.508675
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 00:02:13.949487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.74
 ---- batch: 020 ----
mean loss: 451.90
 ---- batch: 030 ----
mean loss: 450.91
 ---- batch: 040 ----
mean loss: 450.69
 ---- batch: 050 ----
mean loss: 452.14
 ---- batch: 060 ----
mean loss: 457.30
 ---- batch: 070 ----
mean loss: 444.22
 ---- batch: 080 ----
mean loss: 444.70
 ---- batch: 090 ----
mean loss: 438.89
 ---- batch: 100 ----
mean loss: 450.99
 ---- batch: 110 ----
mean loss: 446.96
train mean loss: 449.03
epoch train time: 0:00:02.142758
elapsed time: 0:04:52.651630
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 00:02:16.092402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.65
 ---- batch: 020 ----
mean loss: 459.84
 ---- batch: 030 ----
mean loss: 440.58
 ---- batch: 040 ----
mean loss: 439.54
 ---- batch: 050 ----
mean loss: 467.34
 ---- batch: 060 ----
mean loss: 457.98
 ---- batch: 070 ----
mean loss: 452.81
 ---- batch: 080 ----
mean loss: 456.25
 ---- batch: 090 ----
mean loss: 425.74
 ---- batch: 100 ----
mean loss: 445.61
 ---- batch: 110 ----
mean loss: 434.04
train mean loss: 447.75
epoch train time: 0:00:02.141314
elapsed time: 0:04:54.793097
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 00:02:18.233872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.36
 ---- batch: 020 ----
mean loss: 451.76
 ---- batch: 030 ----
mean loss: 433.25
 ---- batch: 040 ----
mean loss: 448.02
 ---- batch: 050 ----
mean loss: 440.35
 ---- batch: 060 ----
mean loss: 444.48
 ---- batch: 070 ----
mean loss: 444.83
 ---- batch: 080 ----
mean loss: 447.20
 ---- batch: 090 ----
mean loss: 439.35
 ---- batch: 100 ----
mean loss: 438.43
 ---- batch: 110 ----
mean loss: 434.82
train mean loss: 443.76
epoch train time: 0:00:02.141426
elapsed time: 0:04:56.934735
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 00:02:20.375508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.96
 ---- batch: 020 ----
mean loss: 436.00
 ---- batch: 030 ----
mean loss: 452.74
 ---- batch: 040 ----
mean loss: 450.46
 ---- batch: 050 ----
mean loss: 461.53
 ---- batch: 060 ----
mean loss: 435.70
 ---- batch: 070 ----
mean loss: 428.26
 ---- batch: 080 ----
mean loss: 418.59
 ---- batch: 090 ----
mean loss: 437.04
 ---- batch: 100 ----
mean loss: 435.91
 ---- batch: 110 ----
mean loss: 442.04
train mean loss: 439.37
epoch train time: 0:00:02.136466
elapsed time: 0:04:59.071357
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 00:02:22.512135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.88
 ---- batch: 020 ----
mean loss: 431.89
 ---- batch: 030 ----
mean loss: 452.55
 ---- batch: 040 ----
mean loss: 441.23
 ---- batch: 050 ----
mean loss: 451.02
 ---- batch: 060 ----
mean loss: 421.80
 ---- batch: 070 ----
mean loss: 423.98
 ---- batch: 080 ----
mean loss: 449.10
 ---- batch: 090 ----
mean loss: 429.04
 ---- batch: 100 ----
mean loss: 441.03
 ---- batch: 110 ----
mean loss: 435.35
train mean loss: 437.25
epoch train time: 0:00:02.135015
elapsed time: 0:05:01.206524
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 00:02:24.647326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.79
 ---- batch: 020 ----
mean loss: 443.90
 ---- batch: 030 ----
mean loss: 429.27
 ---- batch: 040 ----
mean loss: 427.12
 ---- batch: 050 ----
mean loss: 419.39
 ---- batch: 060 ----
mean loss: 420.97
 ---- batch: 070 ----
mean loss: 432.10
 ---- batch: 080 ----
mean loss: 431.98
 ---- batch: 090 ----
mean loss: 431.22
 ---- batch: 100 ----
mean loss: 433.10
 ---- batch: 110 ----
mean loss: 422.14
train mean loss: 430.08
epoch train time: 0:00:02.133207
elapsed time: 0:05:03.339932
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 00:02:26.780722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.67
 ---- batch: 020 ----
mean loss: 432.07
 ---- batch: 030 ----
mean loss: 423.64
 ---- batch: 040 ----
mean loss: 416.55
 ---- batch: 050 ----
mean loss: 441.50
 ---- batch: 060 ----
mean loss: 431.16
 ---- batch: 070 ----
mean loss: 434.50
 ---- batch: 080 ----
mean loss: 425.18
 ---- batch: 090 ----
mean loss: 409.75
 ---- batch: 100 ----
mean loss: 427.30
 ---- batch: 110 ----
mean loss: 428.34
train mean loss: 427.32
epoch train time: 0:00:02.137452
elapsed time: 0:05:05.477547
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 00:02:28.918315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.53
 ---- batch: 020 ----
mean loss: 430.05
 ---- batch: 030 ----
mean loss: 432.93
 ---- batch: 040 ----
mean loss: 419.37
 ---- batch: 050 ----
mean loss: 428.95
 ---- batch: 060 ----
mean loss: 434.74
 ---- batch: 070 ----
mean loss: 426.41
 ---- batch: 080 ----
mean loss: 425.78
 ---- batch: 090 ----
mean loss: 416.07
 ---- batch: 100 ----
mean loss: 429.45
 ---- batch: 110 ----
mean loss: 416.60
train mean loss: 426.15
epoch train time: 0:00:02.133400
elapsed time: 0:05:07.611103
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 00:02:31.051919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.51
 ---- batch: 020 ----
mean loss: 423.96
 ---- batch: 030 ----
mean loss: 423.04
 ---- batch: 040 ----
mean loss: 429.54
 ---- batch: 050 ----
mean loss: 426.84
 ---- batch: 060 ----
mean loss: 426.13
 ---- batch: 070 ----
mean loss: 400.00
 ---- batch: 080 ----
mean loss: 413.91
 ---- batch: 090 ----
mean loss: 421.20
 ---- batch: 100 ----
mean loss: 415.90
 ---- batch: 110 ----
mean loss: 425.31
train mean loss: 420.87
epoch train time: 0:00:02.136619
elapsed time: 0:05:09.747926
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 00:02:33.188704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.96
 ---- batch: 020 ----
mean loss: 405.40
 ---- batch: 030 ----
mean loss: 423.98
 ---- batch: 040 ----
mean loss: 416.07
 ---- batch: 050 ----
mean loss: 399.99
 ---- batch: 060 ----
mean loss: 415.60
 ---- batch: 070 ----
mean loss: 417.18
 ---- batch: 080 ----
mean loss: 419.60
 ---- batch: 090 ----
mean loss: 413.67
 ---- batch: 100 ----
mean loss: 407.62
 ---- batch: 110 ----
mean loss: 413.04
train mean loss: 412.00
epoch train time: 0:00:02.137650
elapsed time: 0:05:11.885743
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 00:02:35.326523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.98
 ---- batch: 020 ----
mean loss: 411.11
 ---- batch: 030 ----
mean loss: 415.72
 ---- batch: 040 ----
mean loss: 412.58
 ---- batch: 050 ----
mean loss: 413.29
 ---- batch: 060 ----
mean loss: 410.10
 ---- batch: 070 ----
mean loss: 407.74
 ---- batch: 080 ----
mean loss: 406.41
 ---- batch: 090 ----
mean loss: 414.73
 ---- batch: 100 ----
mean loss: 406.54
 ---- batch: 110 ----
mean loss: 412.32
train mean loss: 412.42
epoch train time: 0:00:02.137582
elapsed time: 0:05:14.023486
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 00:02:37.464261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.12
 ---- batch: 020 ----
mean loss: 417.16
 ---- batch: 030 ----
mean loss: 397.11
 ---- batch: 040 ----
mean loss: 400.78
 ---- batch: 050 ----
mean loss: 404.79
 ---- batch: 060 ----
mean loss: 411.65
 ---- batch: 070 ----
mean loss: 405.46
 ---- batch: 080 ----
mean loss: 415.43
 ---- batch: 090 ----
mean loss: 394.75
 ---- batch: 100 ----
mean loss: 391.75
 ---- batch: 110 ----
mean loss: 421.26
train mean loss: 407.33
epoch train time: 0:00:02.142755
elapsed time: 0:05:16.166447
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 00:02:39.607256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.02
 ---- batch: 020 ----
mean loss: 408.29
 ---- batch: 030 ----
mean loss: 408.61
 ---- batch: 040 ----
mean loss: 415.82
 ---- batch: 050 ----
mean loss: 401.82
 ---- batch: 060 ----
mean loss: 399.47
 ---- batch: 070 ----
mean loss: 403.96
 ---- batch: 080 ----
mean loss: 403.79
 ---- batch: 090 ----
mean loss: 406.18
 ---- batch: 100 ----
mean loss: 394.97
 ---- batch: 110 ----
mean loss: 410.17
train mean loss: 406.79
epoch train time: 0:00:02.145820
elapsed time: 0:05:18.312458
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 00:02:41.753248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.27
 ---- batch: 020 ----
mean loss: 403.63
 ---- batch: 030 ----
mean loss: 417.24
 ---- batch: 040 ----
mean loss: 406.75
 ---- batch: 050 ----
mean loss: 410.64
 ---- batch: 060 ----
mean loss: 414.03
 ---- batch: 070 ----
mean loss: 398.37
 ---- batch: 080 ----
mean loss: 392.57
 ---- batch: 090 ----
mean loss: 399.39
 ---- batch: 100 ----
mean loss: 396.69
 ---- batch: 110 ----
mean loss: 388.52
train mean loss: 402.61
epoch train time: 0:00:02.144669
elapsed time: 0:05:20.457301
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 00:02:43.898113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.06
 ---- batch: 020 ----
mean loss: 414.97
 ---- batch: 030 ----
mean loss: 411.86
 ---- batch: 040 ----
mean loss: 404.08
 ---- batch: 050 ----
mean loss: 397.19
 ---- batch: 060 ----
mean loss: 403.60
 ---- batch: 070 ----
mean loss: 393.97
 ---- batch: 080 ----
mean loss: 392.04
 ---- batch: 090 ----
mean loss: 408.99
 ---- batch: 100 ----
mean loss: 397.46
 ---- batch: 110 ----
mean loss: 399.90
train mean loss: 401.04
epoch train time: 0:00:02.141029
elapsed time: 0:05:22.598521
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 00:02:46.039297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.00
 ---- batch: 020 ----
mean loss: 408.82
 ---- batch: 030 ----
mean loss: 394.11
 ---- batch: 040 ----
mean loss: 395.35
 ---- batch: 050 ----
mean loss: 397.41
 ---- batch: 060 ----
mean loss: 396.09
 ---- batch: 070 ----
mean loss: 411.03
 ---- batch: 080 ----
mean loss: 406.69
 ---- batch: 090 ----
mean loss: 399.17
 ---- batch: 100 ----
mean loss: 381.60
 ---- batch: 110 ----
mean loss: 398.90
train mean loss: 400.57
epoch train time: 0:00:02.143661
elapsed time: 0:05:24.742341
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 00:02:48.183114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.60
 ---- batch: 020 ----
mean loss: 404.12
 ---- batch: 030 ----
mean loss: 386.54
 ---- batch: 040 ----
mean loss: 399.64
 ---- batch: 050 ----
mean loss: 402.41
 ---- batch: 060 ----
mean loss: 393.70
 ---- batch: 070 ----
mean loss: 394.70
 ---- batch: 080 ----
mean loss: 405.17
 ---- batch: 090 ----
mean loss: 380.55
 ---- batch: 100 ----
mean loss: 395.07
 ---- batch: 110 ----
mean loss: 385.05
train mean loss: 394.41
epoch train time: 0:00:02.139643
elapsed time: 0:05:26.882133
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 00:02:50.322903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.33
 ---- batch: 020 ----
mean loss: 408.92
 ---- batch: 030 ----
mean loss: 378.65
 ---- batch: 040 ----
mean loss: 383.76
 ---- batch: 050 ----
mean loss: 386.05
 ---- batch: 060 ----
mean loss: 399.37
 ---- batch: 070 ----
mean loss: 375.95
 ---- batch: 080 ----
mean loss: 407.55
 ---- batch: 090 ----
mean loss: 380.61
 ---- batch: 100 ----
mean loss: 398.25
 ---- batch: 110 ----
mean loss: 376.00
train mean loss: 391.36
epoch train time: 0:00:02.135011
elapsed time: 0:05:29.017295
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 00:02:52.458067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.31
 ---- batch: 020 ----
mean loss: 402.14
 ---- batch: 030 ----
mean loss: 393.64
 ---- batch: 040 ----
mean loss: 405.22
 ---- batch: 050 ----
mean loss: 400.31
 ---- batch: 060 ----
mean loss: 388.21
 ---- batch: 070 ----
mean loss: 382.85
 ---- batch: 080 ----
mean loss: 386.63
 ---- batch: 090 ----
mean loss: 396.94
 ---- batch: 100 ----
mean loss: 376.24
 ---- batch: 110 ----
mean loss: 394.81
train mean loss: 392.39
epoch train time: 0:00:02.133878
elapsed time: 0:05:31.151316
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 00:02:54.592103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.64
 ---- batch: 020 ----
mean loss: 388.76
 ---- batch: 030 ----
mean loss: 400.56
 ---- batch: 040 ----
mean loss: 394.61
 ---- batch: 050 ----
mean loss: 399.15
 ---- batch: 060 ----
mean loss: 383.73
 ---- batch: 070 ----
mean loss: 397.16
 ---- batch: 080 ----
mean loss: 385.58
 ---- batch: 090 ----
mean loss: 391.75
 ---- batch: 100 ----
mean loss: 391.53
 ---- batch: 110 ----
mean loss: 390.32
train mean loss: 392.30
epoch train time: 0:00:02.133846
elapsed time: 0:05:33.285332
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 00:02:56.726104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.01
 ---- batch: 020 ----
mean loss: 385.99
 ---- batch: 030 ----
mean loss: 385.23
 ---- batch: 040 ----
mean loss: 384.40
 ---- batch: 050 ----
mean loss: 386.06
 ---- batch: 060 ----
mean loss: 385.82
 ---- batch: 070 ----
mean loss: 394.87
 ---- batch: 080 ----
mean loss: 377.96
 ---- batch: 090 ----
mean loss: 393.06
 ---- batch: 100 ----
mean loss: 390.28
 ---- batch: 110 ----
mean loss: 369.82
train mean loss: 385.99
epoch train time: 0:00:02.137396
elapsed time: 0:05:35.422876
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 00:02:58.863664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.49
 ---- batch: 020 ----
mean loss: 394.09
 ---- batch: 030 ----
mean loss: 384.89
 ---- batch: 040 ----
mean loss: 388.55
 ---- batch: 050 ----
mean loss: 376.80
 ---- batch: 060 ----
mean loss: 384.35
 ---- batch: 070 ----
mean loss: 385.85
 ---- batch: 080 ----
mean loss: 372.45
 ---- batch: 090 ----
mean loss: 376.72
 ---- batch: 100 ----
mean loss: 382.51
 ---- batch: 110 ----
mean loss: 402.02
train mean loss: 384.77
epoch train time: 0:00:02.134280
elapsed time: 0:05:37.557326
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 00:03:00.998130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.51
 ---- batch: 020 ----
mean loss: 380.17
 ---- batch: 030 ----
mean loss: 380.53
 ---- batch: 040 ----
mean loss: 377.37
 ---- batch: 050 ----
mean loss: 387.88
 ---- batch: 060 ----
mean loss: 381.08
 ---- batch: 070 ----
mean loss: 382.97
 ---- batch: 080 ----
mean loss: 397.83
 ---- batch: 090 ----
mean loss: 387.42
 ---- batch: 100 ----
mean loss: 382.52
 ---- batch: 110 ----
mean loss: 373.49
train mean loss: 383.61
epoch train time: 0:00:02.138070
elapsed time: 0:05:39.695574
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 00:03:03.136356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.34
 ---- batch: 020 ----
mean loss: 380.45
 ---- batch: 030 ----
mean loss: 373.90
 ---- batch: 040 ----
mean loss: 391.96
 ---- batch: 050 ----
mean loss: 380.36
 ---- batch: 060 ----
mean loss: 374.39
 ---- batch: 070 ----
mean loss: 384.21
 ---- batch: 080 ----
mean loss: 397.76
 ---- batch: 090 ----
mean loss: 374.72
 ---- batch: 100 ----
mean loss: 376.85
 ---- batch: 110 ----
mean loss: 381.00
train mean loss: 382.85
epoch train time: 0:00:02.135723
elapsed time: 0:05:41.831451
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 00:03:05.272221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.87
 ---- batch: 020 ----
mean loss: 383.76
 ---- batch: 030 ----
mean loss: 389.24
 ---- batch: 040 ----
mean loss: 383.84
 ---- batch: 050 ----
mean loss: 382.34
 ---- batch: 060 ----
mean loss: 372.11
 ---- batch: 070 ----
mean loss: 382.82
 ---- batch: 080 ----
mean loss: 369.63
 ---- batch: 090 ----
mean loss: 394.05
 ---- batch: 100 ----
mean loss: 375.70
 ---- batch: 110 ----
mean loss: 367.10
train mean loss: 380.25
epoch train time: 0:00:02.135007
elapsed time: 0:05:43.966633
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 00:03:07.407421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.65
 ---- batch: 020 ----
mean loss: 376.28
 ---- batch: 030 ----
mean loss: 378.67
 ---- batch: 040 ----
mean loss: 378.53
 ---- batch: 050 ----
mean loss: 367.98
 ---- batch: 060 ----
mean loss: 370.94
 ---- batch: 070 ----
mean loss: 381.89
 ---- batch: 080 ----
mean loss: 374.69
 ---- batch: 090 ----
mean loss: 374.72
 ---- batch: 100 ----
mean loss: 372.95
 ---- batch: 110 ----
mean loss: 374.87
train mean loss: 376.33
epoch train time: 0:00:02.134895
elapsed time: 0:05:46.101687
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 00:03:09.542491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.05
 ---- batch: 020 ----
mean loss: 378.03
 ---- batch: 030 ----
mean loss: 377.93
 ---- batch: 040 ----
mean loss: 373.69
 ---- batch: 050 ----
mean loss: 387.89
 ---- batch: 060 ----
mean loss: 372.73
 ---- batch: 070 ----
mean loss: 374.23
 ---- batch: 080 ----
mean loss: 385.78
 ---- batch: 090 ----
mean loss: 384.43
 ---- batch: 100 ----
mean loss: 387.24
 ---- batch: 110 ----
mean loss: 364.48
train mean loss: 377.74
epoch train time: 0:00:02.135213
elapsed time: 0:05:48.237093
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 00:03:11.677865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.90
 ---- batch: 020 ----
mean loss: 372.14
 ---- batch: 030 ----
mean loss: 375.36
 ---- batch: 040 ----
mean loss: 370.94
 ---- batch: 050 ----
mean loss: 382.54
 ---- batch: 060 ----
mean loss: 358.78
 ---- batch: 070 ----
mean loss: 375.30
 ---- batch: 080 ----
mean loss: 372.38
 ---- batch: 090 ----
mean loss: 376.42
 ---- batch: 100 ----
mean loss: 378.45
 ---- batch: 110 ----
mean loss: 366.80
train mean loss: 372.77
epoch train time: 0:00:02.132603
elapsed time: 0:05:50.369864
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 00:03:13.810627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.57
 ---- batch: 020 ----
mean loss: 375.84
 ---- batch: 030 ----
mean loss: 360.17
 ---- batch: 040 ----
mean loss: 362.30
 ---- batch: 050 ----
mean loss: 359.27
 ---- batch: 060 ----
mean loss: 370.80
 ---- batch: 070 ----
mean loss: 394.74
 ---- batch: 080 ----
mean loss: 377.69
 ---- batch: 090 ----
mean loss: 377.40
 ---- batch: 100 ----
mean loss: 374.88
 ---- batch: 110 ----
mean loss: 372.10
train mean loss: 372.05
epoch train time: 0:00:02.136511
elapsed time: 0:05:52.506513
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 00:03:15.947285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.76
 ---- batch: 020 ----
mean loss: 373.03
 ---- batch: 030 ----
mean loss: 368.32
 ---- batch: 040 ----
mean loss: 371.37
 ---- batch: 050 ----
mean loss: 372.96
 ---- batch: 060 ----
mean loss: 369.63
 ---- batch: 070 ----
mean loss: 361.30
 ---- batch: 080 ----
mean loss: 374.89
 ---- batch: 090 ----
mean loss: 363.35
 ---- batch: 100 ----
mean loss: 360.50
 ---- batch: 110 ----
mean loss: 365.52
train mean loss: 368.40
epoch train time: 0:00:02.135639
elapsed time: 0:05:54.642300
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 00:03:18.083090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.48
 ---- batch: 020 ----
mean loss: 366.83
 ---- batch: 030 ----
mean loss: 379.48
 ---- batch: 040 ----
mean loss: 374.48
 ---- batch: 050 ----
mean loss: 361.55
 ---- batch: 060 ----
mean loss: 364.65
 ---- batch: 070 ----
mean loss: 359.62
 ---- batch: 080 ----
mean loss: 372.40
 ---- batch: 090 ----
mean loss: 375.29
 ---- batch: 100 ----
mean loss: 375.38
 ---- batch: 110 ----
mean loss: 361.20
train mean loss: 368.91
epoch train time: 0:00:02.133378
elapsed time: 0:05:56.775866
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 00:03:20.216641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.91
 ---- batch: 020 ----
mean loss: 360.49
 ---- batch: 030 ----
mean loss: 365.72
 ---- batch: 040 ----
mean loss: 366.63
 ---- batch: 050 ----
mean loss: 366.98
 ---- batch: 060 ----
mean loss: 368.94
 ---- batch: 070 ----
mean loss: 364.22
 ---- batch: 080 ----
mean loss: 362.31
 ---- batch: 090 ----
mean loss: 361.17
 ---- batch: 100 ----
mean loss: 369.70
 ---- batch: 110 ----
mean loss: 348.93
train mean loss: 364.88
epoch train time: 0:00:02.136458
elapsed time: 0:05:58.912471
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 00:03:22.353260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.80
 ---- batch: 020 ----
mean loss: 355.50
 ---- batch: 030 ----
mean loss: 368.74
 ---- batch: 040 ----
mean loss: 360.18
 ---- batch: 050 ----
mean loss: 367.97
 ---- batch: 060 ----
mean loss: 366.77
 ---- batch: 070 ----
mean loss: 369.26
 ---- batch: 080 ----
mean loss: 365.60
 ---- batch: 090 ----
mean loss: 365.96
 ---- batch: 100 ----
mean loss: 351.63
 ---- batch: 110 ----
mean loss: 369.53
train mean loss: 363.93
epoch train time: 0:00:02.135863
elapsed time: 0:06:01.048513
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 00:03:24.489299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.79
 ---- batch: 020 ----
mean loss: 364.96
 ---- batch: 030 ----
mean loss: 364.34
 ---- batch: 040 ----
mean loss: 357.98
 ---- batch: 050 ----
mean loss: 365.70
 ---- batch: 060 ----
mean loss: 356.12
 ---- batch: 070 ----
mean loss: 355.50
 ---- batch: 080 ----
mean loss: 360.89
 ---- batch: 090 ----
mean loss: 359.74
 ---- batch: 100 ----
mean loss: 357.27
 ---- batch: 110 ----
mean loss: 361.08
train mean loss: 360.63
epoch train time: 0:00:02.134727
elapsed time: 0:06:03.183400
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 00:03:26.624172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.77
 ---- batch: 020 ----
mean loss: 354.32
 ---- batch: 030 ----
mean loss: 363.45
 ---- batch: 040 ----
mean loss: 362.83
 ---- batch: 050 ----
mean loss: 362.29
 ---- batch: 060 ----
mean loss: 362.11
 ---- batch: 070 ----
mean loss: 372.54
 ---- batch: 080 ----
mean loss: 360.12
 ---- batch: 090 ----
mean loss: 367.06
 ---- batch: 100 ----
mean loss: 366.16
 ---- batch: 110 ----
mean loss: 371.50
train mean loss: 363.50
epoch train time: 0:00:02.137830
elapsed time: 0:06:05.321417
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 00:03:28.762211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.91
 ---- batch: 020 ----
mean loss: 348.92
 ---- batch: 030 ----
mean loss: 368.49
 ---- batch: 040 ----
mean loss: 371.97
 ---- batch: 050 ----
mean loss: 354.01
 ---- batch: 060 ----
mean loss: 354.13
 ---- batch: 070 ----
mean loss: 350.91
 ---- batch: 080 ----
mean loss: 357.41
 ---- batch: 090 ----
mean loss: 353.58
 ---- batch: 100 ----
mean loss: 357.16
 ---- batch: 110 ----
mean loss: 345.24
train mean loss: 356.63
epoch train time: 0:00:02.135870
elapsed time: 0:06:07.457459
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 00:03:30.898231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.29
 ---- batch: 020 ----
mean loss: 357.58
 ---- batch: 030 ----
mean loss: 357.13
 ---- batch: 040 ----
mean loss: 362.97
 ---- batch: 050 ----
mean loss: 354.71
 ---- batch: 060 ----
mean loss: 358.59
 ---- batch: 070 ----
mean loss: 341.54
 ---- batch: 080 ----
mean loss: 357.41
 ---- batch: 090 ----
mean loss: 355.90
 ---- batch: 100 ----
mean loss: 367.40
 ---- batch: 110 ----
mean loss: 356.67
train mean loss: 356.45
epoch train time: 0:00:02.136133
elapsed time: 0:06:09.593747
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 00:03:33.034523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.36
 ---- batch: 020 ----
mean loss: 353.77
 ---- batch: 030 ----
mean loss: 356.63
 ---- batch: 040 ----
mean loss: 350.10
 ---- batch: 050 ----
mean loss: 373.94
 ---- batch: 060 ----
mean loss: 363.11
 ---- batch: 070 ----
mean loss: 348.97
 ---- batch: 080 ----
mean loss: 350.62
 ---- batch: 090 ----
mean loss: 348.47
 ---- batch: 100 ----
mean loss: 362.26
 ---- batch: 110 ----
mean loss: 358.46
train mean loss: 355.96
epoch train time: 0:00:02.133461
elapsed time: 0:06:11.727361
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 00:03:35.168133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.46
 ---- batch: 020 ----
mean loss: 356.22
 ---- batch: 030 ----
mean loss: 349.36
 ---- batch: 040 ----
mean loss: 361.92
 ---- batch: 050 ----
mean loss: 359.53
 ---- batch: 060 ----
mean loss: 341.12
 ---- batch: 070 ----
mean loss: 354.85
 ---- batch: 080 ----
mean loss: 346.69
 ---- batch: 090 ----
mean loss: 340.08
 ---- batch: 100 ----
mean loss: 367.23
 ---- batch: 110 ----
mean loss: 350.14
train mean loss: 353.56
epoch train time: 0:00:02.134662
elapsed time: 0:06:13.862185
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 00:03:37.302982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.65
 ---- batch: 020 ----
mean loss: 341.88
 ---- batch: 030 ----
mean loss: 342.21
 ---- batch: 040 ----
mean loss: 353.37
 ---- batch: 050 ----
mean loss: 366.06
 ---- batch: 060 ----
mean loss: 356.47
 ---- batch: 070 ----
mean loss: 343.93
 ---- batch: 080 ----
mean loss: 366.28
 ---- batch: 090 ----
mean loss: 360.41
 ---- batch: 100 ----
mean loss: 335.44
 ---- batch: 110 ----
mean loss: 339.78
train mean loss: 350.95
epoch train time: 0:00:02.132525
elapsed time: 0:06:15.994886
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 00:03:39.435658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.82
 ---- batch: 020 ----
mean loss: 348.39
 ---- batch: 030 ----
mean loss: 348.61
 ---- batch: 040 ----
mean loss: 355.03
 ---- batch: 050 ----
mean loss: 359.43
 ---- batch: 060 ----
mean loss: 353.88
 ---- batch: 070 ----
mean loss: 340.34
 ---- batch: 080 ----
mean loss: 345.72
 ---- batch: 090 ----
mean loss: 356.02
 ---- batch: 100 ----
mean loss: 364.47
 ---- batch: 110 ----
mean loss: 360.42
train mean loss: 352.05
epoch train time: 0:00:02.134874
elapsed time: 0:06:18.129906
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 00:03:41.570679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.92
 ---- batch: 020 ----
mean loss: 358.75
 ---- batch: 030 ----
mean loss: 355.58
 ---- batch: 040 ----
mean loss: 337.51
 ---- batch: 050 ----
mean loss: 361.16
 ---- batch: 060 ----
mean loss: 349.48
 ---- batch: 070 ----
mean loss: 344.81
 ---- batch: 080 ----
mean loss: 331.87
 ---- batch: 090 ----
mean loss: 359.64
 ---- batch: 100 ----
mean loss: 347.56
 ---- batch: 110 ----
mean loss: 356.05
train mean loss: 351.33
epoch train time: 0:00:02.139225
elapsed time: 0:06:20.269281
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 00:03:43.710055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.51
 ---- batch: 020 ----
mean loss: 355.23
 ---- batch: 030 ----
mean loss: 357.55
 ---- batch: 040 ----
mean loss: 346.44
 ---- batch: 050 ----
mean loss: 350.02
 ---- batch: 060 ----
mean loss: 347.06
 ---- batch: 070 ----
mean loss: 340.58
 ---- batch: 080 ----
mean loss: 354.62
 ---- batch: 090 ----
mean loss: 345.44
 ---- batch: 100 ----
mean loss: 338.65
 ---- batch: 110 ----
mean loss: 344.33
train mean loss: 349.36
epoch train time: 0:00:02.137979
elapsed time: 0:06:22.407418
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 00:03:45.848192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.69
 ---- batch: 020 ----
mean loss: 348.56
 ---- batch: 030 ----
mean loss: 346.48
 ---- batch: 040 ----
mean loss: 348.10
 ---- batch: 050 ----
mean loss: 350.84
 ---- batch: 060 ----
mean loss: 349.38
 ---- batch: 070 ----
mean loss: 351.89
 ---- batch: 080 ----
mean loss: 345.04
 ---- batch: 090 ----
mean loss: 332.21
 ---- batch: 100 ----
mean loss: 349.41
 ---- batch: 110 ----
mean loss: 343.27
train mean loss: 345.65
epoch train time: 0:00:02.135336
elapsed time: 0:06:24.542904
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 00:03:47.983679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.37
 ---- batch: 020 ----
mean loss: 343.41
 ---- batch: 030 ----
mean loss: 346.86
 ---- batch: 040 ----
mean loss: 345.69
 ---- batch: 050 ----
mean loss: 359.60
 ---- batch: 060 ----
mean loss: 344.69
 ---- batch: 070 ----
mean loss: 339.40
 ---- batch: 080 ----
mean loss: 340.89
 ---- batch: 090 ----
mean loss: 345.25
 ---- batch: 100 ----
mean loss: 340.28
 ---- batch: 110 ----
mean loss: 339.47
train mean loss: 345.54
epoch train time: 0:00:02.140098
elapsed time: 0:06:26.683152
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 00:03:50.123939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.66
 ---- batch: 020 ----
mean loss: 342.73
 ---- batch: 030 ----
mean loss: 338.39
 ---- batch: 040 ----
mean loss: 343.62
 ---- batch: 050 ----
mean loss: 334.01
 ---- batch: 060 ----
mean loss: 348.73
 ---- batch: 070 ----
mean loss: 337.14
 ---- batch: 080 ----
mean loss: 352.75
 ---- batch: 090 ----
mean loss: 344.66
 ---- batch: 100 ----
mean loss: 352.16
 ---- batch: 110 ----
mean loss: 332.00
train mean loss: 343.20
epoch train time: 0:00:02.136458
elapsed time: 0:06:28.819787
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 00:03:52.260654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.45
 ---- batch: 020 ----
mean loss: 341.96
 ---- batch: 030 ----
mean loss: 335.82
 ---- batch: 040 ----
mean loss: 323.96
 ---- batch: 050 ----
mean loss: 333.34
 ---- batch: 060 ----
mean loss: 339.76
 ---- batch: 070 ----
mean loss: 345.63
 ---- batch: 080 ----
mean loss: 338.25
 ---- batch: 090 ----
mean loss: 341.60
 ---- batch: 100 ----
mean loss: 338.74
 ---- batch: 110 ----
mean loss: 332.29
train mean loss: 337.38
epoch train time: 0:00:02.139113
elapsed time: 0:06:30.959141
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 00:03:54.399940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.19
 ---- batch: 020 ----
mean loss: 348.11
 ---- batch: 030 ----
mean loss: 329.95
 ---- batch: 040 ----
mean loss: 347.14
 ---- batch: 050 ----
mean loss: 346.16
 ---- batch: 060 ----
mean loss: 342.69
 ---- batch: 070 ----
mean loss: 334.16
 ---- batch: 080 ----
mean loss: 338.73
 ---- batch: 090 ----
mean loss: 338.57
 ---- batch: 100 ----
mean loss: 335.72
 ---- batch: 110 ----
mean loss: 336.53
train mean loss: 338.85
epoch train time: 0:00:02.135081
elapsed time: 0:06:33.094438
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 00:03:56.535214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.62
 ---- batch: 020 ----
mean loss: 343.08
 ---- batch: 030 ----
mean loss: 333.82
 ---- batch: 040 ----
mean loss: 340.70
 ---- batch: 050 ----
mean loss: 346.72
 ---- batch: 060 ----
mean loss: 338.82
 ---- batch: 070 ----
mean loss: 344.89
 ---- batch: 080 ----
mean loss: 346.78
 ---- batch: 090 ----
mean loss: 345.28
 ---- batch: 100 ----
mean loss: 339.32
 ---- batch: 110 ----
mean loss: 333.03
train mean loss: 340.80
epoch train time: 0:00:02.139461
elapsed time: 0:06:35.234048
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 00:03:58.674827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.43
 ---- batch: 020 ----
mean loss: 344.42
 ---- batch: 030 ----
mean loss: 346.12
 ---- batch: 040 ----
mean loss: 338.65
 ---- batch: 050 ----
mean loss: 327.69
 ---- batch: 060 ----
mean loss: 344.63
 ---- batch: 070 ----
mean loss: 329.51
 ---- batch: 080 ----
mean loss: 340.82
 ---- batch: 090 ----
mean loss: 340.23
 ---- batch: 100 ----
mean loss: 330.88
 ---- batch: 110 ----
mean loss: 324.42
train mean loss: 336.12
epoch train time: 0:00:02.134155
elapsed time: 0:06:37.368377
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 00:04:00.809152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.88
 ---- batch: 020 ----
mean loss: 347.84
 ---- batch: 030 ----
mean loss: 339.44
 ---- batch: 040 ----
mean loss: 334.74
 ---- batch: 050 ----
mean loss: 334.58
 ---- batch: 060 ----
mean loss: 331.46
 ---- batch: 070 ----
mean loss: 340.24
 ---- batch: 080 ----
mean loss: 349.74
 ---- batch: 090 ----
mean loss: 339.23
 ---- batch: 100 ----
mean loss: 320.71
 ---- batch: 110 ----
mean loss: 329.25
train mean loss: 335.66
epoch train time: 0:00:02.141634
elapsed time: 0:06:39.510190
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 00:04:02.950964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.69
 ---- batch: 020 ----
mean loss: 340.45
 ---- batch: 030 ----
mean loss: 339.59
 ---- batch: 040 ----
mean loss: 332.29
 ---- batch: 050 ----
mean loss: 335.64
 ---- batch: 060 ----
mean loss: 334.79
 ---- batch: 070 ----
mean loss: 326.46
 ---- batch: 080 ----
mean loss: 349.36
 ---- batch: 090 ----
mean loss: 334.47
 ---- batch: 100 ----
mean loss: 330.38
 ---- batch: 110 ----
mean loss: 335.50
train mean loss: 335.62
epoch train time: 0:00:02.134494
elapsed time: 0:06:41.644908
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 00:04:05.085719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.82
 ---- batch: 020 ----
mean loss: 331.59
 ---- batch: 030 ----
mean loss: 338.42
 ---- batch: 040 ----
mean loss: 328.26
 ---- batch: 050 ----
mean loss: 315.81
 ---- batch: 060 ----
mean loss: 331.16
 ---- batch: 070 ----
mean loss: 337.59
 ---- batch: 080 ----
mean loss: 334.38
 ---- batch: 090 ----
mean loss: 344.76
 ---- batch: 100 ----
mean loss: 323.83
 ---- batch: 110 ----
mean loss: 340.42
train mean loss: 332.80
epoch train time: 0:00:02.137009
elapsed time: 0:06:43.782132
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 00:04:07.222891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.67
 ---- batch: 020 ----
mean loss: 338.79
 ---- batch: 030 ----
mean loss: 332.83
 ---- batch: 040 ----
mean loss: 324.43
 ---- batch: 050 ----
mean loss: 346.39
 ---- batch: 060 ----
mean loss: 338.55
 ---- batch: 070 ----
mean loss: 327.99
 ---- batch: 080 ----
mean loss: 321.40
 ---- batch: 090 ----
mean loss: 334.75
 ---- batch: 100 ----
mean loss: 330.98
 ---- batch: 110 ----
mean loss: 323.20
train mean loss: 332.56
epoch train time: 0:00:02.142252
elapsed time: 0:06:45.924520
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 00:04:09.365296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.25
 ---- batch: 020 ----
mean loss: 323.48
 ---- batch: 030 ----
mean loss: 333.34
 ---- batch: 040 ----
mean loss: 335.56
 ---- batch: 050 ----
mean loss: 332.30
 ---- batch: 060 ----
mean loss: 322.75
 ---- batch: 070 ----
mean loss: 335.68
 ---- batch: 080 ----
mean loss: 329.56
 ---- batch: 090 ----
mean loss: 331.40
 ---- batch: 100 ----
mean loss: 331.24
 ---- batch: 110 ----
mean loss: 328.11
train mean loss: 331.77
epoch train time: 0:00:02.138721
elapsed time: 0:06:48.063398
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 00:04:11.504183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.57
 ---- batch: 020 ----
mean loss: 331.97
 ---- batch: 030 ----
mean loss: 326.71
 ---- batch: 040 ----
mean loss: 337.25
 ---- batch: 050 ----
mean loss: 325.48
 ---- batch: 060 ----
mean loss: 324.06
 ---- batch: 070 ----
mean loss: 329.51
 ---- batch: 080 ----
mean loss: 330.38
 ---- batch: 090 ----
mean loss: 318.08
 ---- batch: 100 ----
mean loss: 331.88
 ---- batch: 110 ----
mean loss: 345.20
train mean loss: 329.20
epoch train time: 0:00:02.133141
elapsed time: 0:06:50.196697
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 00:04:13.637471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.82
 ---- batch: 020 ----
mean loss: 341.97
 ---- batch: 030 ----
mean loss: 330.64
 ---- batch: 040 ----
mean loss: 323.65
 ---- batch: 050 ----
mean loss: 327.62
 ---- batch: 060 ----
mean loss: 332.24
 ---- batch: 070 ----
mean loss: 324.37
 ---- batch: 080 ----
mean loss: 319.42
 ---- batch: 090 ----
mean loss: 337.38
 ---- batch: 100 ----
mean loss: 333.24
 ---- batch: 110 ----
mean loss: 325.38
train mean loss: 331.20
epoch train time: 0:00:02.139402
elapsed time: 0:06:52.336245
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 00:04:15.777016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.06
 ---- batch: 020 ----
mean loss: 337.65
 ---- batch: 030 ----
mean loss: 325.63
 ---- batch: 040 ----
mean loss: 316.15
 ---- batch: 050 ----
mean loss: 322.29
 ---- batch: 060 ----
mean loss: 329.79
 ---- batch: 070 ----
mean loss: 331.64
 ---- batch: 080 ----
mean loss: 324.02
 ---- batch: 090 ----
mean loss: 338.80
 ---- batch: 100 ----
mean loss: 327.67
 ---- batch: 110 ----
mean loss: 345.05
train mean loss: 330.83
epoch train time: 0:00:02.133442
elapsed time: 0:06:54.469836
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 00:04:17.910610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.94
 ---- batch: 020 ----
mean loss: 319.83
 ---- batch: 030 ----
mean loss: 328.76
 ---- batch: 040 ----
mean loss: 329.01
 ---- batch: 050 ----
mean loss: 325.49
 ---- batch: 060 ----
mean loss: 317.27
 ---- batch: 070 ----
mean loss: 338.36
 ---- batch: 080 ----
mean loss: 332.92
 ---- batch: 090 ----
mean loss: 327.53
 ---- batch: 100 ----
mean loss: 318.95
 ---- batch: 110 ----
mean loss: 328.41
train mean loss: 326.04
epoch train time: 0:00:02.135552
elapsed time: 0:06:56.605539
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 00:04:20.046328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.33
 ---- batch: 020 ----
mean loss: 337.73
 ---- batch: 030 ----
mean loss: 324.36
 ---- batch: 040 ----
mean loss: 322.16
 ---- batch: 050 ----
mean loss: 328.40
 ---- batch: 060 ----
mean loss: 327.13
 ---- batch: 070 ----
mean loss: 329.53
 ---- batch: 080 ----
mean loss: 330.79
 ---- batch: 090 ----
mean loss: 327.16
 ---- batch: 100 ----
mean loss: 329.62
 ---- batch: 110 ----
mean loss: 329.15
train mean loss: 328.25
epoch train time: 0:00:02.138304
elapsed time: 0:06:58.744027
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 00:04:22.184834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.81
 ---- batch: 020 ----
mean loss: 327.13
 ---- batch: 030 ----
mean loss: 326.67
 ---- batch: 040 ----
mean loss: 314.46
 ---- batch: 050 ----
mean loss: 325.59
 ---- batch: 060 ----
mean loss: 316.90
 ---- batch: 070 ----
mean loss: 319.40
 ---- batch: 080 ----
mean loss: 325.65
 ---- batch: 090 ----
mean loss: 322.43
 ---- batch: 100 ----
mean loss: 334.24
 ---- batch: 110 ----
mean loss: 327.88
train mean loss: 323.37
epoch train time: 0:00:02.133721
elapsed time: 0:07:00.877931
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 00:04:24.318703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.15
 ---- batch: 020 ----
mean loss: 326.99
 ---- batch: 030 ----
mean loss: 325.69
 ---- batch: 040 ----
mean loss: 321.38
 ---- batch: 050 ----
mean loss: 309.81
 ---- batch: 060 ----
mean loss: 337.39
 ---- batch: 070 ----
mean loss: 319.49
 ---- batch: 080 ----
mean loss: 316.95
 ---- batch: 090 ----
mean loss: 317.27
 ---- batch: 100 ----
mean loss: 319.32
 ---- batch: 110 ----
mean loss: 307.34
train mean loss: 320.39
epoch train time: 0:00:02.136043
elapsed time: 0:07:03.014118
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 00:04:26.454901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.76
 ---- batch: 020 ----
mean loss: 320.49
 ---- batch: 030 ----
mean loss: 308.99
 ---- batch: 040 ----
mean loss: 325.16
 ---- batch: 050 ----
mean loss: 315.94
 ---- batch: 060 ----
mean loss: 325.72
 ---- batch: 070 ----
mean loss: 324.00
 ---- batch: 080 ----
mean loss: 321.52
 ---- batch: 090 ----
mean loss: 333.19
 ---- batch: 100 ----
mean loss: 327.35
 ---- batch: 110 ----
mean loss: 322.89
train mean loss: 321.64
epoch train time: 0:00:02.133823
elapsed time: 0:07:05.148098
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 00:04:28.588871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.23
 ---- batch: 020 ----
mean loss: 319.78
 ---- batch: 030 ----
mean loss: 322.09
 ---- batch: 040 ----
mean loss: 319.94
 ---- batch: 050 ----
mean loss: 324.98
 ---- batch: 060 ----
mean loss: 329.76
 ---- batch: 070 ----
mean loss: 315.53
 ---- batch: 080 ----
mean loss: 324.74
 ---- batch: 090 ----
mean loss: 317.39
 ---- batch: 100 ----
mean loss: 318.45
 ---- batch: 110 ----
mean loss: 336.24
train mean loss: 322.88
epoch train time: 0:00:02.140304
elapsed time: 0:07:07.288551
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 00:04:30.729322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.43
 ---- batch: 020 ----
mean loss: 318.41
 ---- batch: 030 ----
mean loss: 329.95
 ---- batch: 040 ----
mean loss: 319.21
 ---- batch: 050 ----
mean loss: 323.65
 ---- batch: 060 ----
mean loss: 321.06
 ---- batch: 070 ----
mean loss: 327.59
 ---- batch: 080 ----
mean loss: 316.46
 ---- batch: 090 ----
mean loss: 315.18
 ---- batch: 100 ----
mean loss: 324.53
 ---- batch: 110 ----
mean loss: 319.92
train mean loss: 322.31
epoch train time: 0:00:02.136410
elapsed time: 0:07:09.425154
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 00:04:32.865948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.93
 ---- batch: 020 ----
mean loss: 324.95
 ---- batch: 030 ----
mean loss: 328.60
 ---- batch: 040 ----
mean loss: 309.35
 ---- batch: 050 ----
mean loss: 317.87
 ---- batch: 060 ----
mean loss: 310.76
 ---- batch: 070 ----
mean loss: 309.24
 ---- batch: 080 ----
mean loss: 311.43
 ---- batch: 090 ----
mean loss: 313.76
 ---- batch: 100 ----
mean loss: 324.62
 ---- batch: 110 ----
mean loss: 328.72
train mean loss: 319.36
epoch train time: 0:00:02.144449
elapsed time: 0:07:11.569775
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 00:04:35.010551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.56
 ---- batch: 020 ----
mean loss: 316.47
 ---- batch: 030 ----
mean loss: 332.17
 ---- batch: 040 ----
mean loss: 312.91
 ---- batch: 050 ----
mean loss: 328.71
 ---- batch: 060 ----
mean loss: 329.48
 ---- batch: 070 ----
mean loss: 310.90
 ---- batch: 080 ----
mean loss: 312.45
 ---- batch: 090 ----
mean loss: 326.48
 ---- batch: 100 ----
mean loss: 309.12
 ---- batch: 110 ----
mean loss: 315.89
train mean loss: 319.70
epoch train time: 0:00:02.137184
elapsed time: 0:07:13.707136
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 00:04:37.147905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.86
 ---- batch: 020 ----
mean loss: 324.95
 ---- batch: 030 ----
mean loss: 317.86
 ---- batch: 040 ----
mean loss: 301.91
 ---- batch: 050 ----
mean loss: 317.22
 ---- batch: 060 ----
mean loss: 309.29
 ---- batch: 070 ----
mean loss: 317.69
 ---- batch: 080 ----
mean loss: 312.04
 ---- batch: 090 ----
mean loss: 321.74
 ---- batch: 100 ----
mean loss: 318.19
 ---- batch: 110 ----
mean loss: 312.12
train mean loss: 315.38
epoch train time: 0:00:02.134466
elapsed time: 0:07:15.841772
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 00:04:39.282544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.94
 ---- batch: 020 ----
mean loss: 327.70
 ---- batch: 030 ----
mean loss: 324.08
 ---- batch: 040 ----
mean loss: 316.66
 ---- batch: 050 ----
mean loss: 322.09
 ---- batch: 060 ----
mean loss: 311.48
 ---- batch: 070 ----
mean loss: 304.87
 ---- batch: 080 ----
mean loss: 317.02
 ---- batch: 090 ----
mean loss: 317.76
 ---- batch: 100 ----
mean loss: 305.29
 ---- batch: 110 ----
mean loss: 302.83
train mean loss: 315.39
epoch train time: 0:00:02.136865
elapsed time: 0:07:17.978817
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 00:04:41.419606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.59
 ---- batch: 020 ----
mean loss: 309.76
 ---- batch: 030 ----
mean loss: 322.91
 ---- batch: 040 ----
mean loss: 313.43
 ---- batch: 050 ----
mean loss: 309.70
 ---- batch: 060 ----
mean loss: 322.48
 ---- batch: 070 ----
mean loss: 322.64
 ---- batch: 080 ----
mean loss: 317.88
 ---- batch: 090 ----
mean loss: 324.15
 ---- batch: 100 ----
mean loss: 313.71
 ---- batch: 110 ----
mean loss: 330.71
train mean loss: 316.32
epoch train time: 0:00:02.133362
elapsed time: 0:07:20.112342
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 00:04:43.553114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.03
 ---- batch: 020 ----
mean loss: 303.89
 ---- batch: 030 ----
mean loss: 301.82
 ---- batch: 040 ----
mean loss: 317.29
 ---- batch: 050 ----
mean loss: 321.80
 ---- batch: 060 ----
mean loss: 322.40
 ---- batch: 070 ----
mean loss: 322.77
 ---- batch: 080 ----
mean loss: 315.87
 ---- batch: 090 ----
mean loss: 319.27
 ---- batch: 100 ----
mean loss: 321.40
 ---- batch: 110 ----
mean loss: 315.24
train mean loss: 316.33
epoch train time: 0:00:02.137021
elapsed time: 0:07:22.249518
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 00:04:45.690292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.00
 ---- batch: 020 ----
mean loss: 311.46
 ---- batch: 030 ----
mean loss: 317.17
 ---- batch: 040 ----
mean loss: 316.73
 ---- batch: 050 ----
mean loss: 322.12
 ---- batch: 060 ----
mean loss: 330.55
 ---- batch: 070 ----
mean loss: 301.94
 ---- batch: 080 ----
mean loss: 319.72
 ---- batch: 090 ----
mean loss: 311.81
 ---- batch: 100 ----
mean loss: 308.41
 ---- batch: 110 ----
mean loss: 316.37
train mean loss: 315.58
epoch train time: 0:00:02.142826
elapsed time: 0:07:24.392492
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 00:04:47.833279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.92
 ---- batch: 020 ----
mean loss: 309.19
 ---- batch: 030 ----
mean loss: 315.81
 ---- batch: 040 ----
mean loss: 318.50
 ---- batch: 050 ----
mean loss: 324.62
 ---- batch: 060 ----
mean loss: 294.93
 ---- batch: 070 ----
mean loss: 308.40
 ---- batch: 080 ----
mean loss: 326.07
 ---- batch: 090 ----
mean loss: 297.74
 ---- batch: 100 ----
mean loss: 322.82
 ---- batch: 110 ----
mean loss: 326.16
train mean loss: 314.67
epoch train time: 0:00:02.136317
elapsed time: 0:07:26.528988
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 00:04:49.969763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.30
 ---- batch: 020 ----
mean loss: 325.86
 ---- batch: 030 ----
mean loss: 320.57
 ---- batch: 040 ----
mean loss: 315.23
 ---- batch: 050 ----
mean loss: 309.61
 ---- batch: 060 ----
mean loss: 316.84
 ---- batch: 070 ----
mean loss: 328.85
 ---- batch: 080 ----
mean loss: 312.18
 ---- batch: 090 ----
mean loss: 313.29
 ---- batch: 100 ----
mean loss: 312.63
 ---- batch: 110 ----
mean loss: 313.47
train mean loss: 316.21
epoch train time: 0:00:02.136989
elapsed time: 0:07:28.666137
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 00:04:52.106908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.29
 ---- batch: 020 ----
mean loss: 319.11
 ---- batch: 030 ----
mean loss: 318.21
 ---- batch: 040 ----
mean loss: 311.55
 ---- batch: 050 ----
mean loss: 306.59
 ---- batch: 060 ----
mean loss: 315.49
 ---- batch: 070 ----
mean loss: 311.19
 ---- batch: 080 ----
mean loss: 313.77
 ---- batch: 090 ----
mean loss: 315.42
 ---- batch: 100 ----
mean loss: 305.48
 ---- batch: 110 ----
mean loss: 318.35
train mean loss: 313.52
epoch train time: 0:00:02.137305
elapsed time: 0:07:30.803588
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 00:04:54.244362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.83
 ---- batch: 020 ----
mean loss: 316.27
 ---- batch: 030 ----
mean loss: 301.68
 ---- batch: 040 ----
mean loss: 318.06
 ---- batch: 050 ----
mean loss: 313.86
 ---- batch: 060 ----
mean loss: 308.91
 ---- batch: 070 ----
mean loss: 327.70
 ---- batch: 080 ----
mean loss: 313.13
 ---- batch: 090 ----
mean loss: 311.39
 ---- batch: 100 ----
mean loss: 304.92
 ---- batch: 110 ----
mean loss: 320.60
train mean loss: 313.33
epoch train time: 0:00:02.130293
elapsed time: 0:07:32.934030
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 00:04:56.374817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.94
 ---- batch: 020 ----
mean loss: 317.61
 ---- batch: 030 ----
mean loss: 318.64
 ---- batch: 040 ----
mean loss: 309.91
 ---- batch: 050 ----
mean loss: 316.86
 ---- batch: 060 ----
mean loss: 302.09
 ---- batch: 070 ----
mean loss: 319.79
 ---- batch: 080 ----
mean loss: 317.11
 ---- batch: 090 ----
mean loss: 303.08
 ---- batch: 100 ----
mean loss: 311.60
 ---- batch: 110 ----
mean loss: 305.48
train mean loss: 311.99
epoch train time: 0:00:02.136407
elapsed time: 0:07:35.070596
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 00:04:58.511370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.27
 ---- batch: 020 ----
mean loss: 315.54
 ---- batch: 030 ----
mean loss: 304.21
 ---- batch: 040 ----
mean loss: 314.02
 ---- batch: 050 ----
mean loss: 300.89
 ---- batch: 060 ----
mean loss: 315.23
 ---- batch: 070 ----
mean loss: 302.73
 ---- batch: 080 ----
mean loss: 313.06
 ---- batch: 090 ----
mean loss: 306.38
 ---- batch: 100 ----
mean loss: 300.80
 ---- batch: 110 ----
mean loss: 311.25
train mean loss: 309.16
epoch train time: 0:00:02.132635
elapsed time: 0:07:37.203378
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 00:05:00.644174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.31
 ---- batch: 020 ----
mean loss: 302.01
 ---- batch: 030 ----
mean loss: 326.11
 ---- batch: 040 ----
mean loss: 296.82
 ---- batch: 050 ----
mean loss: 311.52
 ---- batch: 060 ----
mean loss: 322.82
 ---- batch: 070 ----
mean loss: 314.94
 ---- batch: 080 ----
mean loss: 308.71
 ---- batch: 090 ----
mean loss: 313.12
 ---- batch: 100 ----
mean loss: 306.33
 ---- batch: 110 ----
mean loss: 310.96
train mean loss: 310.98
epoch train time: 0:00:02.133258
elapsed time: 0:07:39.336808
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 00:05:02.777579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.50
 ---- batch: 020 ----
mean loss: 297.79
 ---- batch: 030 ----
mean loss: 301.83
 ---- batch: 040 ----
mean loss: 306.80
 ---- batch: 050 ----
mean loss: 316.92
 ---- batch: 060 ----
mean loss: 317.08
 ---- batch: 070 ----
mean loss: 308.57
 ---- batch: 080 ----
mean loss: 301.55
 ---- batch: 090 ----
mean loss: 309.89
 ---- batch: 100 ----
mean loss: 304.12
 ---- batch: 110 ----
mean loss: 318.71
train mean loss: 309.30
epoch train time: 0:00:02.132164
elapsed time: 0:07:41.469142
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 00:05:04.909927
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.89
 ---- batch: 020 ----
mean loss: 296.30
 ---- batch: 030 ----
mean loss: 307.79
 ---- batch: 040 ----
mean loss: 297.19
 ---- batch: 050 ----
mean loss: 295.70
 ---- batch: 060 ----
mean loss: 309.56
 ---- batch: 070 ----
mean loss: 295.20
 ---- batch: 080 ----
mean loss: 301.09
 ---- batch: 090 ----
mean loss: 302.92
 ---- batch: 100 ----
mean loss: 307.51
 ---- batch: 110 ----
mean loss: 301.70
train mean loss: 301.62
epoch train time: 0:00:02.137429
elapsed time: 0:07:43.606748
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 00:05:07.047516
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.45
 ---- batch: 020 ----
mean loss: 298.48
 ---- batch: 030 ----
mean loss: 290.20
 ---- batch: 040 ----
mean loss: 303.57
 ---- batch: 050 ----
mean loss: 293.27
 ---- batch: 060 ----
mean loss: 307.32
 ---- batch: 070 ----
mean loss: 305.88
 ---- batch: 080 ----
mean loss: 313.45
 ---- batch: 090 ----
mean loss: 312.86
 ---- batch: 100 ----
mean loss: 296.22
 ---- batch: 110 ----
mean loss: 300.79
train mean loss: 302.47
epoch train time: 0:00:02.135087
elapsed time: 0:07:45.741996
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 00:05:09.182770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.63
 ---- batch: 020 ----
mean loss: 295.65
 ---- batch: 030 ----
mean loss: 302.00
 ---- batch: 040 ----
mean loss: 304.12
 ---- batch: 050 ----
mean loss: 299.15
 ---- batch: 060 ----
mean loss: 304.65
 ---- batch: 070 ----
mean loss: 290.28
 ---- batch: 080 ----
mean loss: 304.52
 ---- batch: 090 ----
mean loss: 299.22
 ---- batch: 100 ----
mean loss: 298.42
 ---- batch: 110 ----
mean loss: 303.20
train mean loss: 299.81
epoch train time: 0:00:02.139628
elapsed time: 0:07:47.881774
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 00:05:11.322545
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.15
 ---- batch: 020 ----
mean loss: 295.86
 ---- batch: 030 ----
mean loss: 298.53
 ---- batch: 040 ----
mean loss: 295.88
 ---- batch: 050 ----
mean loss: 299.73
 ---- batch: 060 ----
mean loss: 301.01
 ---- batch: 070 ----
mean loss: 299.03
 ---- batch: 080 ----
mean loss: 302.83
 ---- batch: 090 ----
mean loss: 306.75
 ---- batch: 100 ----
mean loss: 298.90
 ---- batch: 110 ----
mean loss: 294.42
train mean loss: 299.24
epoch train time: 0:00:02.135008
elapsed time: 0:07:50.016947
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 00:05:13.457723
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.28
 ---- batch: 020 ----
mean loss: 299.64
 ---- batch: 030 ----
mean loss: 299.49
 ---- batch: 040 ----
mean loss: 307.45
 ---- batch: 050 ----
mean loss: 312.05
 ---- batch: 060 ----
mean loss: 298.10
 ---- batch: 070 ----
mean loss: 301.46
 ---- batch: 080 ----
mean loss: 305.10
 ---- batch: 090 ----
mean loss: 301.77
 ---- batch: 100 ----
mean loss: 296.45
 ---- batch: 110 ----
mean loss: 297.04
train mean loss: 302.39
epoch train time: 0:00:02.139084
elapsed time: 0:07:52.156184
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 00:05:15.596993
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.83
 ---- batch: 020 ----
mean loss: 296.87
 ---- batch: 030 ----
mean loss: 290.48
 ---- batch: 040 ----
mean loss: 296.86
 ---- batch: 050 ----
mean loss: 295.34
 ---- batch: 060 ----
mean loss: 301.67
 ---- batch: 070 ----
mean loss: 303.03
 ---- batch: 080 ----
mean loss: 307.19
 ---- batch: 090 ----
mean loss: 300.35
 ---- batch: 100 ----
mean loss: 296.62
 ---- batch: 110 ----
mean loss: 308.03
train mean loss: 299.69
epoch train time: 0:00:02.139016
elapsed time: 0:07:54.295385
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 00:05:17.736157
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.66
 ---- batch: 020 ----
mean loss: 306.07
 ---- batch: 030 ----
mean loss: 294.97
 ---- batch: 040 ----
mean loss: 303.90
 ---- batch: 050 ----
mean loss: 293.80
 ---- batch: 060 ----
mean loss: 305.12
 ---- batch: 070 ----
mean loss: 290.55
 ---- batch: 080 ----
mean loss: 306.46
 ---- batch: 090 ----
mean loss: 300.72
 ---- batch: 100 ----
mean loss: 318.31
 ---- batch: 110 ----
mean loss: 302.01
train mean loss: 301.48
epoch train time: 0:00:02.135398
elapsed time: 0:07:56.430954
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 00:05:19.871770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 294.91
 ---- batch: 020 ----
mean loss: 296.10
 ---- batch: 030 ----
mean loss: 298.82
 ---- batch: 040 ----
mean loss: 300.28
 ---- batch: 050 ----
mean loss: 301.68
 ---- batch: 060 ----
mean loss: 312.16
 ---- batch: 070 ----
mean loss: 300.50
 ---- batch: 080 ----
mean loss: 309.09
 ---- batch: 090 ----
mean loss: 288.57
 ---- batch: 100 ----
mean loss: 300.03
 ---- batch: 110 ----
mean loss: 301.97
train mean loss: 300.03
epoch train time: 0:00:02.133269
elapsed time: 0:07:58.564441
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 00:05:22.005223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.30
 ---- batch: 020 ----
mean loss: 289.55
 ---- batch: 030 ----
mean loss: 304.39
 ---- batch: 040 ----
mean loss: 296.89
 ---- batch: 050 ----
mean loss: 299.38
 ---- batch: 060 ----
mean loss: 304.43
 ---- batch: 070 ----
mean loss: 305.42
 ---- batch: 080 ----
mean loss: 295.13
 ---- batch: 090 ----
mean loss: 302.09
 ---- batch: 100 ----
mean loss: 300.58
 ---- batch: 110 ----
mean loss: 315.41
train mean loss: 301.41
epoch train time: 0:00:02.135324
elapsed time: 0:08:00.699923
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 00:05:24.140696
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.32
 ---- batch: 020 ----
mean loss: 306.80
 ---- batch: 030 ----
mean loss: 301.64
 ---- batch: 040 ----
mean loss: 299.04
 ---- batch: 050 ----
mean loss: 312.64
 ---- batch: 060 ----
mean loss: 305.92
 ---- batch: 070 ----
mean loss: 296.36
 ---- batch: 080 ----
mean loss: 293.25
 ---- batch: 090 ----
mean loss: 295.43
 ---- batch: 100 ----
mean loss: 296.17
 ---- batch: 110 ----
mean loss: 303.92
train mean loss: 301.37
epoch train time: 0:00:02.137950
elapsed time: 0:08:02.838027
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 00:05:26.278799
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.07
 ---- batch: 020 ----
mean loss: 302.43
 ---- batch: 030 ----
mean loss: 301.71
 ---- batch: 040 ----
mean loss: 302.81
 ---- batch: 050 ----
mean loss: 307.15
 ---- batch: 060 ----
mean loss: 301.36
 ---- batch: 070 ----
mean loss: 290.03
 ---- batch: 080 ----
mean loss: 293.66
 ---- batch: 090 ----
mean loss: 309.11
 ---- batch: 100 ----
mean loss: 296.26
 ---- batch: 110 ----
mean loss: 299.93
train mean loss: 300.40
epoch train time: 0:00:02.133630
elapsed time: 0:08:04.971806
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 00:05:28.412577
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.35
 ---- batch: 020 ----
mean loss: 298.04
 ---- batch: 030 ----
mean loss: 301.08
 ---- batch: 040 ----
mean loss: 301.47
 ---- batch: 050 ----
mean loss: 312.06
 ---- batch: 060 ----
mean loss: 302.54
 ---- batch: 070 ----
mean loss: 298.48
 ---- batch: 080 ----
mean loss: 296.05
 ---- batch: 090 ----
mean loss: 299.79
 ---- batch: 100 ----
mean loss: 297.50
 ---- batch: 110 ----
mean loss: 306.00
train mean loss: 300.93
epoch train time: 0:00:02.136862
elapsed time: 0:08:07.108817
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 00:05:30.549637
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 293.73
 ---- batch: 020 ----
mean loss: 300.50
 ---- batch: 030 ----
mean loss: 297.65
 ---- batch: 040 ----
mean loss: 304.55
 ---- batch: 050 ----
mean loss: 292.74
 ---- batch: 060 ----
mean loss: 302.82
 ---- batch: 070 ----
mean loss: 291.56
 ---- batch: 080 ----
mean loss: 297.40
 ---- batch: 090 ----
mean loss: 294.96
 ---- batch: 100 ----
mean loss: 307.30
 ---- batch: 110 ----
mean loss: 310.46
train mean loss: 299.44
epoch train time: 0:00:02.141582
elapsed time: 0:08:09.250597
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 00:05:32.691368
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.81
 ---- batch: 020 ----
mean loss: 311.48
 ---- batch: 030 ----
mean loss: 300.70
 ---- batch: 040 ----
mean loss: 290.46
 ---- batch: 050 ----
mean loss: 295.33
 ---- batch: 060 ----
mean loss: 296.64
 ---- batch: 070 ----
mean loss: 317.78
 ---- batch: 080 ----
mean loss: 301.38
 ---- batch: 090 ----
mean loss: 299.53
 ---- batch: 100 ----
mean loss: 303.77
 ---- batch: 110 ----
mean loss: 302.97
train mean loss: 301.17
epoch train time: 0:00:02.140772
elapsed time: 0:08:11.391521
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 00:05:34.832299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 292.83
 ---- batch: 020 ----
mean loss: 303.10
 ---- batch: 030 ----
mean loss: 308.45
 ---- batch: 040 ----
mean loss: 303.12
 ---- batch: 050 ----
mean loss: 293.90
 ---- batch: 060 ----
mean loss: 294.74
 ---- batch: 070 ----
mean loss: 304.34
 ---- batch: 080 ----
mean loss: 296.69
 ---- batch: 090 ----
mean loss: 291.37
 ---- batch: 100 ----
mean loss: 298.93
 ---- batch: 110 ----
mean loss: 294.84
train mean loss: 298.93
epoch train time: 0:00:02.137638
elapsed time: 0:08:13.529317
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 00:05:36.970091
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.48
 ---- batch: 020 ----
mean loss: 295.89
 ---- batch: 030 ----
mean loss: 297.84
 ---- batch: 040 ----
mean loss: 304.06
 ---- batch: 050 ----
mean loss: 295.35
 ---- batch: 060 ----
mean loss: 296.30
 ---- batch: 070 ----
mean loss: 294.61
 ---- batch: 080 ----
mean loss: 302.77
 ---- batch: 090 ----
mean loss: 302.41
 ---- batch: 100 ----
mean loss: 307.57
 ---- batch: 110 ----
mean loss: 301.09
train mean loss: 299.82
epoch train time: 0:00:02.139846
elapsed time: 0:08:15.669312
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 00:05:39.110089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.66
 ---- batch: 020 ----
mean loss: 282.88
 ---- batch: 030 ----
mean loss: 306.53
 ---- batch: 040 ----
mean loss: 309.47
 ---- batch: 050 ----
mean loss: 302.30
 ---- batch: 060 ----
mean loss: 306.51
 ---- batch: 070 ----
mean loss: 298.41
 ---- batch: 080 ----
mean loss: 294.23
 ---- batch: 090 ----
mean loss: 294.54
 ---- batch: 100 ----
mean loss: 311.95
 ---- batch: 110 ----
mean loss: 305.62
train mean loss: 300.33
epoch train time: 0:00:02.131092
elapsed time: 0:08:17.800570
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 00:05:41.241375
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.72
 ---- batch: 020 ----
mean loss: 302.16
 ---- batch: 030 ----
mean loss: 302.77
 ---- batch: 040 ----
mean loss: 311.84
 ---- batch: 050 ----
mean loss: 306.91
 ---- batch: 060 ----
mean loss: 312.26
 ---- batch: 070 ----
mean loss: 303.63
 ---- batch: 080 ----
mean loss: 294.44
 ---- batch: 090 ----
mean loss: 288.28
 ---- batch: 100 ----
mean loss: 295.94
 ---- batch: 110 ----
mean loss: 296.61
train mean loss: 300.90
epoch train time: 0:00:02.138874
elapsed time: 0:08:19.939644
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 00:05:43.380446
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 294.80
 ---- batch: 020 ----
mean loss: 302.36
 ---- batch: 030 ----
mean loss: 296.02
 ---- batch: 040 ----
mean loss: 304.07
 ---- batch: 050 ----
mean loss: 311.62
 ---- batch: 060 ----
mean loss: 291.17
 ---- batch: 070 ----
mean loss: 300.05
 ---- batch: 080 ----
mean loss: 308.45
 ---- batch: 090 ----
mean loss: 300.04
 ---- batch: 100 ----
mean loss: 299.88
 ---- batch: 110 ----
mean loss: 297.94
train mean loss: 300.09
epoch train time: 0:00:02.136897
elapsed time: 0:08:22.076752
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 00:05:45.517538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.43
 ---- batch: 020 ----
mean loss: 304.11
 ---- batch: 030 ----
mean loss: 293.32
 ---- batch: 040 ----
mean loss: 298.29
 ---- batch: 050 ----
mean loss: 305.13
 ---- batch: 060 ----
mean loss: 299.90
 ---- batch: 070 ----
mean loss: 304.83
 ---- batch: 080 ----
mean loss: 299.19
 ---- batch: 090 ----
mean loss: 307.64
 ---- batch: 100 ----
mean loss: 295.19
 ---- batch: 110 ----
mean loss: 302.13
train mean loss: 299.90
epoch train time: 0:00:02.132316
elapsed time: 0:08:24.209248
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 00:05:47.650054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.69
 ---- batch: 020 ----
mean loss: 294.76
 ---- batch: 030 ----
mean loss: 310.20
 ---- batch: 040 ----
mean loss: 296.60
 ---- batch: 050 ----
mean loss: 291.04
 ---- batch: 060 ----
mean loss: 284.96
 ---- batch: 070 ----
mean loss: 302.46
 ---- batch: 080 ----
mean loss: 304.34
 ---- batch: 090 ----
mean loss: 296.18
 ---- batch: 100 ----
mean loss: 296.72
 ---- batch: 110 ----
mean loss: 305.90
train mean loss: 298.58
epoch train time: 0:00:02.140075
elapsed time: 0:08:26.349507
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 00:05:49.790285
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.65
 ---- batch: 020 ----
mean loss: 299.87
 ---- batch: 030 ----
mean loss: 309.07
 ---- batch: 040 ----
mean loss: 297.05
 ---- batch: 050 ----
mean loss: 302.15
 ---- batch: 060 ----
mean loss: 299.99
 ---- batch: 070 ----
mean loss: 293.71
 ---- batch: 080 ----
mean loss: 294.99
 ---- batch: 090 ----
mean loss: 299.62
 ---- batch: 100 ----
mean loss: 302.65
 ---- batch: 110 ----
mean loss: 295.34
train mean loss: 300.09
epoch train time: 0:00:02.135645
elapsed time: 0:08:28.485309
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 00:05:51.926086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.29
 ---- batch: 020 ----
mean loss: 291.43
 ---- batch: 030 ----
mean loss: 298.79
 ---- batch: 040 ----
mean loss: 303.69
 ---- batch: 050 ----
mean loss: 303.77
 ---- batch: 060 ----
mean loss: 302.31
 ---- batch: 070 ----
mean loss: 306.03
 ---- batch: 080 ----
mean loss: 303.05
 ---- batch: 090 ----
mean loss: 297.18
 ---- batch: 100 ----
mean loss: 306.40
 ---- batch: 110 ----
mean loss: 296.35
train mean loss: 300.89
epoch train time: 0:00:02.135439
elapsed time: 0:08:30.620924
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 00:05:54.061708
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.13
 ---- batch: 020 ----
mean loss: 304.64
 ---- batch: 030 ----
mean loss: 297.12
 ---- batch: 040 ----
mean loss: 295.95
 ---- batch: 050 ----
mean loss: 296.77
 ---- batch: 060 ----
mean loss: 313.01
 ---- batch: 070 ----
mean loss: 298.49
 ---- batch: 080 ----
mean loss: 296.59
 ---- batch: 090 ----
mean loss: 300.92
 ---- batch: 100 ----
mean loss: 287.23
 ---- batch: 110 ----
mean loss: 284.25
train mean loss: 297.38
epoch train time: 0:00:02.133333
elapsed time: 0:08:32.754416
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 00:05:56.195203
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.83
 ---- batch: 020 ----
mean loss: 286.60
 ---- batch: 030 ----
mean loss: 300.72
 ---- batch: 040 ----
mean loss: 311.03
 ---- batch: 050 ----
mean loss: 282.62
 ---- batch: 060 ----
mean loss: 303.68
 ---- batch: 070 ----
mean loss: 295.46
 ---- batch: 080 ----
mean loss: 289.78
 ---- batch: 090 ----
mean loss: 292.25
 ---- batch: 100 ----
mean loss: 304.02
 ---- batch: 110 ----
mean loss: 299.28
train mean loss: 297.93
epoch train time: 0:00:02.142322
elapsed time: 0:08:34.896946
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 00:05:58.337750
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.56
 ---- batch: 020 ----
mean loss: 304.92
 ---- batch: 030 ----
mean loss: 295.65
 ---- batch: 040 ----
mean loss: 290.01
 ---- batch: 050 ----
mean loss: 300.11
 ---- batch: 060 ----
mean loss: 286.19
 ---- batch: 070 ----
mean loss: 307.24
 ---- batch: 080 ----
mean loss: 301.44
 ---- batch: 090 ----
mean loss: 300.01
 ---- batch: 100 ----
mean loss: 300.73
 ---- batch: 110 ----
mean loss: 298.67
train mean loss: 298.07
epoch train time: 0:00:02.140925
elapsed time: 0:08:37.038063
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 00:06:00.478866
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.93
 ---- batch: 020 ----
mean loss: 306.18
 ---- batch: 030 ----
mean loss: 297.30
 ---- batch: 040 ----
mean loss: 296.39
 ---- batch: 050 ----
mean loss: 298.45
 ---- batch: 060 ----
mean loss: 300.18
 ---- batch: 070 ----
mean loss: 303.48
 ---- batch: 080 ----
mean loss: 302.54
 ---- batch: 090 ----
mean loss: 298.79
 ---- batch: 100 ----
mean loss: 301.36
 ---- batch: 110 ----
mean loss: 292.79
train mean loss: 298.30
epoch train time: 0:00:02.135840
elapsed time: 0:08:39.174083
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 00:06:02.614872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.01
 ---- batch: 020 ----
mean loss: 298.67
 ---- batch: 030 ----
mean loss: 293.66
 ---- batch: 040 ----
mean loss: 296.52
 ---- batch: 050 ----
mean loss: 308.20
 ---- batch: 060 ----
mean loss: 303.78
 ---- batch: 070 ----
mean loss: 306.97
 ---- batch: 080 ----
mean loss: 304.47
 ---- batch: 090 ----
mean loss: 297.93
 ---- batch: 100 ----
mean loss: 294.02
 ---- batch: 110 ----
mean loss: 297.69
train mean loss: 299.73
epoch train time: 0:00:02.138182
elapsed time: 0:08:41.312448
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 00:06:04.753231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.20
 ---- batch: 020 ----
mean loss: 304.08
 ---- batch: 030 ----
mean loss: 305.83
 ---- batch: 040 ----
mean loss: 309.60
 ---- batch: 050 ----
mean loss: 299.96
 ---- batch: 060 ----
mean loss: 294.99
 ---- batch: 070 ----
mean loss: 298.87
 ---- batch: 080 ----
mean loss: 288.68
 ---- batch: 090 ----
mean loss: 300.11
 ---- batch: 100 ----
mean loss: 306.16
 ---- batch: 110 ----
mean loss: 294.62
train mean loss: 300.05
epoch train time: 0:00:02.136578
elapsed time: 0:08:43.449207
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 00:06:06.889980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.97
 ---- batch: 020 ----
mean loss: 295.86
 ---- batch: 030 ----
mean loss: 297.44
 ---- batch: 040 ----
mean loss: 313.65
 ---- batch: 050 ----
mean loss: 290.78
 ---- batch: 060 ----
mean loss: 290.93
 ---- batch: 070 ----
mean loss: 301.46
 ---- batch: 080 ----
mean loss: 292.32
 ---- batch: 090 ----
mean loss: 294.23
 ---- batch: 100 ----
mean loss: 285.54
 ---- batch: 110 ----
mean loss: 302.64
train mean loss: 297.12
epoch train time: 0:00:02.137462
elapsed time: 0:08:45.586819
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 00:06:09.027592
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 293.53
 ---- batch: 020 ----
mean loss: 290.30
 ---- batch: 030 ----
mean loss: 294.18
 ---- batch: 040 ----
mean loss: 307.48
 ---- batch: 050 ----
mean loss: 305.23
 ---- batch: 060 ----
mean loss: 295.45
 ---- batch: 070 ----
mean loss: 305.35
 ---- batch: 080 ----
mean loss: 306.71
 ---- batch: 090 ----
mean loss: 301.99
 ---- batch: 100 ----
mean loss: 312.04
 ---- batch: 110 ----
mean loss: 299.54
train mean loss: 300.89
epoch train time: 0:00:02.137390
elapsed time: 0:08:47.724354
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 00:06:11.165144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.46
 ---- batch: 020 ----
mean loss: 294.58
 ---- batch: 030 ----
mean loss: 300.68
 ---- batch: 040 ----
mean loss: 299.69
 ---- batch: 050 ----
mean loss: 286.06
 ---- batch: 060 ----
mean loss: 303.28
 ---- batch: 070 ----
mean loss: 297.24
 ---- batch: 080 ----
mean loss: 300.12
 ---- batch: 090 ----
mean loss: 296.71
 ---- batch: 100 ----
mean loss: 295.12
 ---- batch: 110 ----
mean loss: 299.26
train mean loss: 296.98
epoch train time: 0:00:02.134934
elapsed time: 0:08:49.859472
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 00:06:13.300248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.50
 ---- batch: 020 ----
mean loss: 285.70
 ---- batch: 030 ----
mean loss: 297.86
 ---- batch: 040 ----
mean loss: 300.03
 ---- batch: 050 ----
mean loss: 292.48
 ---- batch: 060 ----
mean loss: 302.54
 ---- batch: 070 ----
mean loss: 306.00
 ---- batch: 080 ----
mean loss: 294.34
 ---- batch: 090 ----
mean loss: 291.66
 ---- batch: 100 ----
mean loss: 299.48
 ---- batch: 110 ----
mean loss: 306.29
train mean loss: 297.09
epoch train time: 0:00:02.133694
elapsed time: 0:08:51.993335
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 00:06:15.434096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.03
 ---- batch: 020 ----
mean loss: 288.38
 ---- batch: 030 ----
mean loss: 286.38
 ---- batch: 040 ----
mean loss: 288.33
 ---- batch: 050 ----
mean loss: 302.94
 ---- batch: 060 ----
mean loss: 303.06
 ---- batch: 070 ----
mean loss: 303.10
 ---- batch: 080 ----
mean loss: 303.86
 ---- batch: 090 ----
mean loss: 305.17
 ---- batch: 100 ----
mean loss: 301.92
 ---- batch: 110 ----
mean loss: 293.76
train mean loss: 297.99
epoch train time: 0:00:02.139360
elapsed time: 0:08:54.132859
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 00:06:17.573677
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.10
 ---- batch: 020 ----
mean loss: 295.90
 ---- batch: 030 ----
mean loss: 300.25
 ---- batch: 040 ----
mean loss: 304.61
 ---- batch: 050 ----
mean loss: 295.65
 ---- batch: 060 ----
mean loss: 303.87
 ---- batch: 070 ----
mean loss: 297.98
 ---- batch: 080 ----
mean loss: 283.79
 ---- batch: 090 ----
mean loss: 299.90
 ---- batch: 100 ----
mean loss: 296.52
 ---- batch: 110 ----
mean loss: 289.84
train mean loss: 297.29
epoch train time: 0:00:02.140873
elapsed time: 0:08:56.273927
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 00:06:19.714701
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.26
 ---- batch: 020 ----
mean loss: 303.00
 ---- batch: 030 ----
mean loss: 295.48
 ---- batch: 040 ----
mean loss: 296.32
 ---- batch: 050 ----
mean loss: 303.16
 ---- batch: 060 ----
mean loss: 303.81
 ---- batch: 070 ----
mean loss: 299.63
 ---- batch: 080 ----
mean loss: 289.30
 ---- batch: 090 ----
mean loss: 299.22
 ---- batch: 100 ----
mean loss: 291.48
 ---- batch: 110 ----
mean loss: 295.28
train mean loss: 297.85
epoch train time: 0:00:02.136177
elapsed time: 0:08:58.410261
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 00:06:21.851033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.25
 ---- batch: 020 ----
mean loss: 302.44
 ---- batch: 030 ----
mean loss: 290.05
 ---- batch: 040 ----
mean loss: 297.99
 ---- batch: 050 ----
mean loss: 302.35
 ---- batch: 060 ----
mean loss: 297.77
 ---- batch: 070 ----
mean loss: 292.89
 ---- batch: 080 ----
mean loss: 297.59
 ---- batch: 090 ----
mean loss: 307.30
 ---- batch: 100 ----
mean loss: 302.06
 ---- batch: 110 ----
mean loss: 292.54
train mean loss: 297.96
epoch train time: 0:00:02.137595
elapsed time: 0:09:00.548007
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 00:06:23.988780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.74
 ---- batch: 020 ----
mean loss: 300.78
 ---- batch: 030 ----
mean loss: 300.02
 ---- batch: 040 ----
mean loss: 300.29
 ---- batch: 050 ----
mean loss: 305.23
 ---- batch: 060 ----
mean loss: 300.99
 ---- batch: 070 ----
mean loss: 311.10
 ---- batch: 080 ----
mean loss: 290.49
 ---- batch: 090 ----
mean loss: 286.06
 ---- batch: 100 ----
mean loss: 306.65
 ---- batch: 110 ----
mean loss: 283.83
train mean loss: 298.42
epoch train time: 0:00:02.136993
elapsed time: 0:09:02.685210
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 00:06:26.126034
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 293.10
 ---- batch: 020 ----
mean loss: 299.09
 ---- batch: 030 ----
mean loss: 292.58
 ---- batch: 040 ----
mean loss: 286.74
 ---- batch: 050 ----
mean loss: 291.51
 ---- batch: 060 ----
mean loss: 310.07
 ---- batch: 070 ----
mean loss: 302.79
 ---- batch: 080 ----
mean loss: 315.02
 ---- batch: 090 ----
mean loss: 294.72
 ---- batch: 100 ----
mean loss: 301.83
 ---- batch: 110 ----
mean loss: 295.67
train mean loss: 298.71
epoch train time: 0:00:02.137981
elapsed time: 0:09:04.823392
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 00:06:28.264165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.27
 ---- batch: 020 ----
mean loss: 299.37
 ---- batch: 030 ----
mean loss: 293.52
 ---- batch: 040 ----
mean loss: 289.99
 ---- batch: 050 ----
mean loss: 296.32
 ---- batch: 060 ----
mean loss: 293.70
 ---- batch: 070 ----
mean loss: 300.41
 ---- batch: 080 ----
mean loss: 297.67
 ---- batch: 090 ----
mean loss: 318.85
 ---- batch: 100 ----
mean loss: 290.39
 ---- batch: 110 ----
mean loss: 307.50
train mean loss: 298.78
epoch train time: 0:00:02.132997
elapsed time: 0:09:06.956552
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 00:06:30.397339
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.11
 ---- batch: 020 ----
mean loss: 291.23
 ---- batch: 030 ----
mean loss: 290.50
 ---- batch: 040 ----
mean loss: 288.68
 ---- batch: 050 ----
mean loss: 304.24
 ---- batch: 060 ----
mean loss: 299.92
 ---- batch: 070 ----
mean loss: 299.57
 ---- batch: 080 ----
mean loss: 296.20
 ---- batch: 090 ----
mean loss: 301.20
 ---- batch: 100 ----
mean loss: 305.42
 ---- batch: 110 ----
mean loss: 292.42
train mean loss: 296.89
epoch train time: 0:00:02.135171
elapsed time: 0:09:09.091885
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 00:06:32.532656
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.08
 ---- batch: 020 ----
mean loss: 310.90
 ---- batch: 030 ----
mean loss: 306.02
 ---- batch: 040 ----
mean loss: 291.93
 ---- batch: 050 ----
mean loss: 292.06
 ---- batch: 060 ----
mean loss: 294.51
 ---- batch: 070 ----
mean loss: 300.16
 ---- batch: 080 ----
mean loss: 289.64
 ---- batch: 090 ----
mean loss: 300.06
 ---- batch: 100 ----
mean loss: 295.59
 ---- batch: 110 ----
mean loss: 294.05
train mean loss: 297.61
epoch train time: 0:00:02.138335
elapsed time: 0:09:11.230368
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 00:06:34.671141
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.47
 ---- batch: 020 ----
mean loss: 309.26
 ---- batch: 030 ----
mean loss: 289.75
 ---- batch: 040 ----
mean loss: 308.89
 ---- batch: 050 ----
mean loss: 294.14
 ---- batch: 060 ----
mean loss: 297.47
 ---- batch: 070 ----
mean loss: 298.56
 ---- batch: 080 ----
mean loss: 293.34
 ---- batch: 090 ----
mean loss: 289.33
 ---- batch: 100 ----
mean loss: 298.18
 ---- batch: 110 ----
mean loss: 296.66
train mean loss: 296.73
epoch train time: 0:00:02.139279
elapsed time: 0:09:13.369822
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 00:06:36.810597
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.47
 ---- batch: 020 ----
mean loss: 301.49
 ---- batch: 030 ----
mean loss: 287.59
 ---- batch: 040 ----
mean loss: 294.55
 ---- batch: 050 ----
mean loss: 301.09
 ---- batch: 060 ----
mean loss: 297.97
 ---- batch: 070 ----
mean loss: 308.38
 ---- batch: 080 ----
mean loss: 302.00
 ---- batch: 090 ----
mean loss: 290.33
 ---- batch: 100 ----
mean loss: 300.78
 ---- batch: 110 ----
mean loss: 301.99
train mean loss: 297.17
epoch train time: 0:00:02.135649
elapsed time: 0:09:15.505624
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 00:06:38.946420
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.03
 ---- batch: 020 ----
mean loss: 291.83
 ---- batch: 030 ----
mean loss: 297.55
 ---- batch: 040 ----
mean loss: 299.01
 ---- batch: 050 ----
mean loss: 288.05
 ---- batch: 060 ----
mean loss: 304.45
 ---- batch: 070 ----
mean loss: 291.57
 ---- batch: 080 ----
mean loss: 291.92
 ---- batch: 090 ----
mean loss: 304.98
 ---- batch: 100 ----
mean loss: 304.03
 ---- batch: 110 ----
mean loss: 301.79
train mean loss: 296.46
epoch train time: 0:00:02.135926
elapsed time: 0:09:17.641728
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 00:06:41.082513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.28
 ---- batch: 020 ----
mean loss: 299.17
 ---- batch: 030 ----
mean loss: 293.40
 ---- batch: 040 ----
mean loss: 302.45
 ---- batch: 050 ----
mean loss: 296.95
 ---- batch: 060 ----
mean loss: 293.54
 ---- batch: 070 ----
mean loss: 301.61
 ---- batch: 080 ----
mean loss: 289.79
 ---- batch: 090 ----
mean loss: 296.06
 ---- batch: 100 ----
mean loss: 297.15
 ---- batch: 110 ----
mean loss: 303.56
train mean loss: 298.06
epoch train time: 0:00:02.142215
elapsed time: 0:09:19.784119
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 00:06:43.224896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.81
 ---- batch: 020 ----
mean loss: 302.07
 ---- batch: 030 ----
mean loss: 301.47
 ---- batch: 040 ----
mean loss: 295.86
 ---- batch: 050 ----
mean loss: 298.72
 ---- batch: 060 ----
mean loss: 297.83
 ---- batch: 070 ----
mean loss: 305.73
 ---- batch: 080 ----
mean loss: 294.30
 ---- batch: 090 ----
mean loss: 292.57
 ---- batch: 100 ----
mean loss: 302.81
 ---- batch: 110 ----
mean loss: 305.61
train mean loss: 297.88
epoch train time: 0:00:02.136838
elapsed time: 0:09:21.921125
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 00:06:45.361898
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.88
 ---- batch: 020 ----
mean loss: 298.01
 ---- batch: 030 ----
mean loss: 291.63
 ---- batch: 040 ----
mean loss: 293.48
 ---- batch: 050 ----
mean loss: 300.20
 ---- batch: 060 ----
mean loss: 298.57
 ---- batch: 070 ----
mean loss: 292.60
 ---- batch: 080 ----
mean loss: 301.42
 ---- batch: 090 ----
mean loss: 303.68
 ---- batch: 100 ----
mean loss: 300.00
 ---- batch: 110 ----
mean loss: 298.05
train mean loss: 297.66
epoch train time: 0:00:02.135046
elapsed time: 0:09:24.056335
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 00:06:47.497121
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.37
 ---- batch: 020 ----
mean loss: 292.13
 ---- batch: 030 ----
mean loss: 294.98
 ---- batch: 040 ----
mean loss: 296.30
 ---- batch: 050 ----
mean loss: 292.79
 ---- batch: 060 ----
mean loss: 307.15
 ---- batch: 070 ----
mean loss: 293.21
 ---- batch: 080 ----
mean loss: 294.58
 ---- batch: 090 ----
mean loss: 298.93
 ---- batch: 100 ----
mean loss: 304.54
 ---- batch: 110 ----
mean loss: 288.76
train mean loss: 295.95
epoch train time: 0:00:02.138510
elapsed time: 0:09:26.198668
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_1/checkpoint.pth.tar
**** end time: 2019-09-27 00:06:49.639404 ****
