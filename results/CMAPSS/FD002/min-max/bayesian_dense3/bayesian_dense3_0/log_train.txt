Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_0', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 19969
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianDense3...
Done.
**** start time: 2019-09-25 19:22:36.098852 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
    BayesianLinear-2                  [-1, 100]          96,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 136,200
Trainable params: 136,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 19:22:36.107862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4019.70
 ---- batch: 020 ----
mean loss: 3740.85
 ---- batch: 030 ----
mean loss: 3606.63
 ---- batch: 040 ----
mean loss: 3376.16
 ---- batch: 050 ----
mean loss: 3152.67
 ---- batch: 060 ----
mean loss: 3082.68
 ---- batch: 070 ----
mean loss: 2896.73
 ---- batch: 080 ----
mean loss: 2816.95
 ---- batch: 090 ----
mean loss: 2718.73
train mean loss: 3228.19
epoch train time: 0:00:35.388056
elapsed time: 0:00:35.404356
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 19:23:11.503252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2512.97
 ---- batch: 020 ----
mean loss: 2501.28
 ---- batch: 030 ----
mean loss: 2413.01
 ---- batch: 040 ----
mean loss: 2352.18
 ---- batch: 050 ----
mean loss: 2235.71
 ---- batch: 060 ----
mean loss: 2222.61
 ---- batch: 070 ----
mean loss: 2172.25
 ---- batch: 080 ----
mean loss: 2107.88
 ---- batch: 090 ----
mean loss: 2088.48
train mean loss: 2275.04
epoch train time: 0:00:01.730401
elapsed time: 0:00:37.135175
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 19:23:13.234345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1995.31
 ---- batch: 020 ----
mean loss: 1954.36
 ---- batch: 030 ----
mean loss: 1946.27
 ---- batch: 040 ----
mean loss: 1916.35
 ---- batch: 050 ----
mean loss: 1904.47
 ---- batch: 060 ----
mean loss: 1858.49
 ---- batch: 070 ----
mean loss: 1857.72
 ---- batch: 080 ----
mean loss: 1801.62
 ---- batch: 090 ----
mean loss: 1763.17
train mean loss: 1882.40
epoch train time: 0:00:01.710896
elapsed time: 0:00:38.846800
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 19:23:14.946113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1743.23
 ---- batch: 020 ----
mean loss: 1694.29
 ---- batch: 030 ----
mean loss: 1663.98
 ---- batch: 040 ----
mean loss: 1690.29
 ---- batch: 050 ----
mean loss: 1646.20
 ---- batch: 060 ----
mean loss: 1636.71
 ---- batch: 070 ----
mean loss: 1590.66
 ---- batch: 080 ----
mean loss: 1577.77
 ---- batch: 090 ----
mean loss: 1579.55
train mean loss: 1641.10
epoch train time: 0:00:01.690223
elapsed time: 0:00:40.537773
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 19:23:16.636911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1527.04
 ---- batch: 020 ----
mean loss: 1520.26
 ---- batch: 030 ----
mean loss: 1485.12
 ---- batch: 040 ----
mean loss: 1472.67
 ---- batch: 050 ----
mean loss: 1480.98
 ---- batch: 060 ----
mean loss: 1428.41
 ---- batch: 070 ----
mean loss: 1436.07
 ---- batch: 080 ----
mean loss: 1438.04
 ---- batch: 090 ----
mean loss: 1397.72
train mean loss: 1459.69
epoch train time: 0:00:01.699142
elapsed time: 0:00:42.237487
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 19:23:18.336636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1362.31
 ---- batch: 020 ----
mean loss: 1345.27
 ---- batch: 030 ----
mean loss: 1362.61
 ---- batch: 040 ----
mean loss: 1333.64
 ---- batch: 050 ----
mean loss: 1354.92
 ---- batch: 060 ----
mean loss: 1280.62
 ---- batch: 070 ----
mean loss: 1306.13
 ---- batch: 080 ----
mean loss: 1312.67
 ---- batch: 090 ----
mean loss: 1256.67
train mean loss: 1319.54
epoch train time: 0:00:01.726751
elapsed time: 0:00:43.964957
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 19:23:20.064118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1242.59
 ---- batch: 020 ----
mean loss: 1255.17
 ---- batch: 030 ----
mean loss: 1197.85
 ---- batch: 040 ----
mean loss: 1231.09
 ---- batch: 050 ----
mean loss: 1212.70
 ---- batch: 060 ----
mean loss: 1212.31
 ---- batch: 070 ----
mean loss: 1190.45
 ---- batch: 080 ----
mean loss: 1184.71
 ---- batch: 090 ----
mean loss: 1172.71
train mean loss: 1206.99
epoch train time: 0:00:01.738209
elapsed time: 0:00:45.703843
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 19:23:21.803054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1134.59
 ---- batch: 020 ----
mean loss: 1148.29
 ---- batch: 030 ----
mean loss: 1161.94
 ---- batch: 040 ----
mean loss: 1108.71
 ---- batch: 050 ----
mean loss: 1131.34
 ---- batch: 060 ----
mean loss: 1113.95
 ---- batch: 070 ----
mean loss: 1096.90
 ---- batch: 080 ----
mean loss: 1105.89
 ---- batch: 090 ----
mean loss: 1071.88
train mean loss: 1116.02
epoch train time: 0:00:01.753793
elapsed time: 0:00:47.458304
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 19:23:23.557459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1082.09
 ---- batch: 020 ----
mean loss: 1053.81
 ---- batch: 030 ----
mean loss: 1060.79
 ---- batch: 040 ----
mean loss: 1059.80
 ---- batch: 050 ----
mean loss: 1063.67
 ---- batch: 060 ----
mean loss: 1036.41
 ---- batch: 070 ----
mean loss: 1045.43
 ---- batch: 080 ----
mean loss: 1018.47
 ---- batch: 090 ----
mean loss: 1045.72
train mean loss: 1050.68
epoch train time: 0:00:01.775484
elapsed time: 0:00:49.234453
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 19:23:25.333635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1024.37
 ---- batch: 020 ----
mean loss: 1008.93
 ---- batch: 030 ----
mean loss: 1003.77
 ---- batch: 040 ----
mean loss: 984.53
 ---- batch: 050 ----
mean loss: 1016.33
 ---- batch: 060 ----
mean loss: 999.25
 ---- batch: 070 ----
mean loss: 1006.90
 ---- batch: 080 ----
mean loss: 980.05
 ---- batch: 090 ----
mean loss: 992.61
train mean loss: 1000.26
epoch train time: 0:00:01.717343
elapsed time: 0:00:50.952466
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 19:23:27.051626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 968.78
 ---- batch: 020 ----
mean loss: 984.79
 ---- batch: 030 ----
mean loss: 979.06
 ---- batch: 040 ----
mean loss: 961.01
 ---- batch: 050 ----
mean loss: 964.84
 ---- batch: 060 ----
mean loss: 976.16
 ---- batch: 070 ----
mean loss: 961.40
 ---- batch: 080 ----
mean loss: 939.96
 ---- batch: 090 ----
mean loss: 958.72
train mean loss: 964.59
epoch train time: 0:00:01.723416
elapsed time: 0:00:52.676552
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 19:23:28.775744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.33
 ---- batch: 020 ----
mean loss: 942.22
 ---- batch: 030 ----
mean loss: 950.45
 ---- batch: 040 ----
mean loss: 955.23
 ---- batch: 050 ----
mean loss: 944.85
 ---- batch: 060 ----
mean loss: 947.75
 ---- batch: 070 ----
mean loss: 952.41
 ---- batch: 080 ----
mean loss: 922.38
 ---- batch: 090 ----
mean loss: 922.11
train mean loss: 938.22
epoch train time: 0:00:01.726064
elapsed time: 0:00:54.403300
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 19:23:30.502345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.18
 ---- batch: 020 ----
mean loss: 920.03
 ---- batch: 030 ----
mean loss: 928.66
 ---- batch: 040 ----
mean loss: 907.26
 ---- batch: 050 ----
mean loss: 933.97
 ---- batch: 060 ----
mean loss: 924.86
 ---- batch: 070 ----
mean loss: 902.59
 ---- batch: 080 ----
mean loss: 915.20
 ---- batch: 090 ----
mean loss: 920.77
train mean loss: 918.71
epoch train time: 0:00:01.708858
elapsed time: 0:00:56.112734
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 19:23:32.211881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.86
 ---- batch: 020 ----
mean loss: 906.34
 ---- batch: 030 ----
mean loss: 904.50
 ---- batch: 040 ----
mean loss: 887.46
 ---- batch: 050 ----
mean loss: 908.18
 ---- batch: 060 ----
mean loss: 904.44
 ---- batch: 070 ----
mean loss: 922.90
 ---- batch: 080 ----
mean loss: 902.70
 ---- batch: 090 ----
mean loss: 906.93
train mean loss: 905.68
epoch train time: 0:00:01.699689
elapsed time: 0:00:57.813108
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 19:23:33.912276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.11
 ---- batch: 020 ----
mean loss: 901.71
 ---- batch: 030 ----
mean loss: 898.03
 ---- batch: 040 ----
mean loss: 888.81
 ---- batch: 050 ----
mean loss: 889.86
 ---- batch: 060 ----
mean loss: 887.86
 ---- batch: 070 ----
mean loss: 885.80
 ---- batch: 080 ----
mean loss: 904.52
 ---- batch: 090 ----
mean loss: 900.32
train mean loss: 896.70
epoch train time: 0:00:01.700083
elapsed time: 0:00:59.513846
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 19:23:35.612994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.28
 ---- batch: 020 ----
mean loss: 899.30
 ---- batch: 030 ----
mean loss: 894.27
 ---- batch: 040 ----
mean loss: 900.55
 ---- batch: 050 ----
mean loss: 896.44
 ---- batch: 060 ----
mean loss: 883.66
 ---- batch: 070 ----
mean loss: 866.77
 ---- batch: 080 ----
mean loss: 892.37
 ---- batch: 090 ----
mean loss: 896.38
train mean loss: 891.56
epoch train time: 0:00:01.685779
elapsed time: 0:01:01.200299
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 19:23:37.299479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.50
 ---- batch: 020 ----
mean loss: 865.24
 ---- batch: 030 ----
mean loss: 883.90
 ---- batch: 040 ----
mean loss: 894.64
 ---- batch: 050 ----
mean loss: 872.25
 ---- batch: 060 ----
mean loss: 896.46
 ---- batch: 070 ----
mean loss: 900.58
 ---- batch: 080 ----
mean loss: 907.01
 ---- batch: 090 ----
mean loss: 870.58
train mean loss: 888.06
epoch train time: 0:00:01.709770
elapsed time: 0:01:02.910780
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 19:23:39.009990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.71
 ---- batch: 020 ----
mean loss: 880.72
 ---- batch: 030 ----
mean loss: 901.76
 ---- batch: 040 ----
mean loss: 895.80
 ---- batch: 050 ----
mean loss: 876.25
 ---- batch: 060 ----
mean loss: 866.80
 ---- batch: 070 ----
mean loss: 883.42
 ---- batch: 080 ----
mean loss: 880.17
 ---- batch: 090 ----
mean loss: 878.02
train mean loss: 884.85
epoch train time: 0:00:01.700306
elapsed time: 0:01:04.611871
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 19:23:40.711063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.00
 ---- batch: 020 ----
mean loss: 897.68
 ---- batch: 030 ----
mean loss: 873.65
 ---- batch: 040 ----
mean loss: 887.33
 ---- batch: 050 ----
mean loss: 886.29
 ---- batch: 060 ----
mean loss: 881.27
 ---- batch: 070 ----
mean loss: 863.81
 ---- batch: 080 ----
mean loss: 896.25
 ---- batch: 090 ----
mean loss: 889.27
train mean loss: 884.81
epoch train time: 0:00:01.707973
elapsed time: 0:01:06.320510
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 19:23:42.419681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.76
 ---- batch: 020 ----
mean loss: 893.93
 ---- batch: 030 ----
mean loss: 882.36
 ---- batch: 040 ----
mean loss: 885.13
 ---- batch: 050 ----
mean loss: 873.95
 ---- batch: 060 ----
mean loss: 885.01
 ---- batch: 070 ----
mean loss: 893.65
 ---- batch: 080 ----
mean loss: 870.38
 ---- batch: 090 ----
mean loss: 879.07
train mean loss: 882.45
epoch train time: 0:00:01.734120
elapsed time: 0:01:08.055278
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 19:23:44.154438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.34
 ---- batch: 020 ----
mean loss: 875.45
 ---- batch: 030 ----
mean loss: 875.68
 ---- batch: 040 ----
mean loss: 884.63
 ---- batch: 050 ----
mean loss: 900.63
 ---- batch: 060 ----
mean loss: 879.13
 ---- batch: 070 ----
mean loss: 864.92
 ---- batch: 080 ----
mean loss: 893.79
 ---- batch: 090 ----
mean loss: 888.90
train mean loss: 882.36
epoch train time: 0:00:01.732989
elapsed time: 0:01:09.788943
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 19:23:45.888210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.51
 ---- batch: 020 ----
mean loss: 894.70
 ---- batch: 030 ----
mean loss: 874.58
 ---- batch: 040 ----
mean loss: 860.27
 ---- batch: 050 ----
mean loss: 872.97
 ---- batch: 060 ----
mean loss: 895.09
 ---- batch: 070 ----
mean loss: 885.06
 ---- batch: 080 ----
mean loss: 882.59
 ---- batch: 090 ----
mean loss: 881.05
train mean loss: 881.33
epoch train time: 0:00:01.737879
elapsed time: 0:01:11.527543
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 19:23:47.626516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.93
 ---- batch: 020 ----
mean loss: 874.30
 ---- batch: 030 ----
mean loss: 869.61
 ---- batch: 040 ----
mean loss: 875.68
 ---- batch: 050 ----
mean loss: 882.03
 ---- batch: 060 ----
mean loss: 886.90
 ---- batch: 070 ----
mean loss: 883.33
 ---- batch: 080 ----
mean loss: 882.83
 ---- batch: 090 ----
mean loss: 890.98
train mean loss: 880.75
epoch train time: 0:00:01.742582
elapsed time: 0:01:13.270670
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 19:23:49.369879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.25
 ---- batch: 020 ----
mean loss: 876.71
 ---- batch: 030 ----
mean loss: 879.99
 ---- batch: 040 ----
mean loss: 865.69
 ---- batch: 050 ----
mean loss: 879.27
 ---- batch: 060 ----
mean loss: 872.48
 ---- batch: 070 ----
mean loss: 888.02
 ---- batch: 080 ----
mean loss: 886.06
 ---- batch: 090 ----
mean loss: 877.29
train mean loss: 881.18
epoch train time: 0:00:01.748863
elapsed time: 0:01:15.020253
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 19:23:51.119412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.05
 ---- batch: 020 ----
mean loss: 891.79
 ---- batch: 030 ----
mean loss: 887.72
 ---- batch: 040 ----
mean loss: 882.80
 ---- batch: 050 ----
mean loss: 886.73
 ---- batch: 060 ----
mean loss: 881.36
 ---- batch: 070 ----
mean loss: 882.11
 ---- batch: 080 ----
mean loss: 870.42
 ---- batch: 090 ----
mean loss: 890.04
train mean loss: 880.78
epoch train time: 0:00:01.725659
elapsed time: 0:01:16.746683
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 19:23:52.845870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.56
 ---- batch: 020 ----
mean loss: 875.76
 ---- batch: 030 ----
mean loss: 867.65
 ---- batch: 040 ----
mean loss: 875.05
 ---- batch: 050 ----
mean loss: 869.31
 ---- batch: 060 ----
mean loss: 904.94
 ---- batch: 070 ----
mean loss: 888.14
 ---- batch: 080 ----
mean loss: 880.73
 ---- batch: 090 ----
mean loss: 875.82
train mean loss: 880.09
epoch train time: 0:00:01.738898
elapsed time: 0:01:18.486307
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 19:23:54.585515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.63
 ---- batch: 020 ----
mean loss: 875.94
 ---- batch: 030 ----
mean loss: 874.34
 ---- batch: 040 ----
mean loss: 876.31
 ---- batch: 050 ----
mean loss: 868.27
 ---- batch: 060 ----
mean loss: 875.31
 ---- batch: 070 ----
mean loss: 885.08
 ---- batch: 080 ----
mean loss: 893.99
 ---- batch: 090 ----
mean loss: 892.51
train mean loss: 879.65
epoch train time: 0:00:01.698727
elapsed time: 0:01:20.185745
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 19:23:56.284903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.34
 ---- batch: 020 ----
mean loss: 871.31
 ---- batch: 030 ----
mean loss: 881.64
 ---- batch: 040 ----
mean loss: 889.45
 ---- batch: 050 ----
mean loss: 876.51
 ---- batch: 060 ----
mean loss: 874.53
 ---- batch: 070 ----
mean loss: 864.73
 ---- batch: 080 ----
mean loss: 899.49
 ---- batch: 090 ----
mean loss: 865.99
train mean loss: 878.62
epoch train time: 0:00:01.743223
elapsed time: 0:01:21.929592
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 19:23:58.028752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.16
 ---- batch: 020 ----
mean loss: 878.95
 ---- batch: 030 ----
mean loss: 866.55
 ---- batch: 040 ----
mean loss: 878.29
 ---- batch: 050 ----
mean loss: 886.86
 ---- batch: 060 ----
mean loss: 883.70
 ---- batch: 070 ----
mean loss: 895.75
 ---- batch: 080 ----
mean loss: 867.77
 ---- batch: 090 ----
mean loss: 861.99
train mean loss: 879.41
epoch train time: 0:00:01.741359
elapsed time: 0:01:23.671612
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 19:23:59.770790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.88
 ---- batch: 020 ----
mean loss: 889.70
 ---- batch: 030 ----
mean loss: 874.26
 ---- batch: 040 ----
mean loss: 881.24
 ---- batch: 050 ----
mean loss: 880.31
 ---- batch: 060 ----
mean loss: 880.36
 ---- batch: 070 ----
mean loss: 877.61
 ---- batch: 080 ----
mean loss: 876.39
 ---- batch: 090 ----
mean loss: 858.79
train mean loss: 878.71
epoch train time: 0:00:01.728569
elapsed time: 0:01:25.400895
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 19:24:01.500060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.72
 ---- batch: 020 ----
mean loss: 874.49
 ---- batch: 030 ----
mean loss: 871.03
 ---- batch: 040 ----
mean loss: 881.44
 ---- batch: 050 ----
mean loss: 897.76
 ---- batch: 060 ----
mean loss: 870.24
 ---- batch: 070 ----
mean loss: 895.96
 ---- batch: 080 ----
mean loss: 872.61
 ---- batch: 090 ----
mean loss: 866.66
train mean loss: 879.39
epoch train time: 0:00:01.719017
elapsed time: 0:01:27.120529
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 19:24:03.219708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.02
 ---- batch: 020 ----
mean loss: 883.37
 ---- batch: 030 ----
mean loss: 878.32
 ---- batch: 040 ----
mean loss: 870.08
 ---- batch: 050 ----
mean loss: 877.41
 ---- batch: 060 ----
mean loss: 891.34
 ---- batch: 070 ----
mean loss: 864.77
 ---- batch: 080 ----
mean loss: 887.37
 ---- batch: 090 ----
mean loss: 872.56
train mean loss: 878.00
epoch train time: 0:00:01.734709
elapsed time: 0:01:28.855980
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 19:24:04.955150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.14
 ---- batch: 020 ----
mean loss: 882.27
 ---- batch: 030 ----
mean loss: 880.85
 ---- batch: 040 ----
mean loss: 873.73
 ---- batch: 050 ----
mean loss: 871.97
 ---- batch: 060 ----
mean loss: 872.51
 ---- batch: 070 ----
mean loss: 871.79
 ---- batch: 080 ----
mean loss: 878.56
 ---- batch: 090 ----
mean loss: 879.08
train mean loss: 879.13
epoch train time: 0:00:01.712892
elapsed time: 0:01:30.569567
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 19:24:06.668746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.09
 ---- batch: 020 ----
mean loss: 885.57
 ---- batch: 030 ----
mean loss: 870.24
 ---- batch: 040 ----
mean loss: 882.11
 ---- batch: 050 ----
mean loss: 892.35
 ---- batch: 060 ----
mean loss: 885.34
 ---- batch: 070 ----
mean loss: 885.72
 ---- batch: 080 ----
mean loss: 859.20
 ---- batch: 090 ----
mean loss: 877.74
train mean loss: 878.47
epoch train time: 0:00:01.722955
elapsed time: 0:01:32.293145
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 19:24:08.392311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.53
 ---- batch: 020 ----
mean loss: 874.72
 ---- batch: 030 ----
mean loss: 862.23
 ---- batch: 040 ----
mean loss: 881.84
 ---- batch: 050 ----
mean loss: 874.41
 ---- batch: 060 ----
mean loss: 891.59
 ---- batch: 070 ----
mean loss: 881.46
 ---- batch: 080 ----
mean loss: 877.40
 ---- batch: 090 ----
mean loss: 890.50
train mean loss: 878.52
epoch train time: 0:00:01.728960
elapsed time: 0:01:34.022783
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 19:24:10.121985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.58
 ---- batch: 020 ----
mean loss: 872.58
 ---- batch: 030 ----
mean loss: 877.87
 ---- batch: 040 ----
mean loss: 888.90
 ---- batch: 050 ----
mean loss: 891.07
 ---- batch: 060 ----
mean loss: 856.96
 ---- batch: 070 ----
mean loss: 861.19
 ---- batch: 080 ----
mean loss: 867.57
 ---- batch: 090 ----
mean loss: 911.47
train mean loss: 877.87
epoch train time: 0:00:01.696408
elapsed time: 0:01:35.719874
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 19:24:11.819034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.26
 ---- batch: 020 ----
mean loss: 869.93
 ---- batch: 030 ----
mean loss: 885.65
 ---- batch: 040 ----
mean loss: 898.98
 ---- batch: 050 ----
mean loss: 895.44
 ---- batch: 060 ----
mean loss: 874.03
 ---- batch: 070 ----
mean loss: 875.20
 ---- batch: 080 ----
mean loss: 854.20
 ---- batch: 090 ----
mean loss: 869.87
train mean loss: 877.08
epoch train time: 0:00:01.721803
elapsed time: 0:01:37.442321
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 19:24:13.541537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.77
 ---- batch: 020 ----
mean loss: 886.54
 ---- batch: 030 ----
mean loss: 882.31
 ---- batch: 040 ----
mean loss: 883.23
 ---- batch: 050 ----
mean loss: 867.03
 ---- batch: 060 ----
mean loss: 886.17
 ---- batch: 070 ----
mean loss: 873.35
 ---- batch: 080 ----
mean loss: 885.30
 ---- batch: 090 ----
mean loss: 860.49
train mean loss: 878.00
epoch train time: 0:00:01.697010
elapsed time: 0:01:39.140109
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 19:24:15.239273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.03
 ---- batch: 020 ----
mean loss: 878.13
 ---- batch: 030 ----
mean loss: 870.95
 ---- batch: 040 ----
mean loss: 878.26
 ---- batch: 050 ----
mean loss: 862.07
 ---- batch: 060 ----
mean loss: 889.39
 ---- batch: 070 ----
mean loss: 884.81
 ---- batch: 080 ----
mean loss: 868.35
 ---- batch: 090 ----
mean loss: 887.21
train mean loss: 878.81
epoch train time: 0:00:01.716674
elapsed time: 0:01:40.857475
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 19:24:16.956673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.58
 ---- batch: 020 ----
mean loss: 866.77
 ---- batch: 030 ----
mean loss: 872.58
 ---- batch: 040 ----
mean loss: 874.54
 ---- batch: 050 ----
mean loss: 877.18
 ---- batch: 060 ----
mean loss: 875.54
 ---- batch: 070 ----
mean loss: 879.96
 ---- batch: 080 ----
mean loss: 874.74
 ---- batch: 090 ----
mean loss: 881.85
train mean loss: 877.57
epoch train time: 0:00:01.696707
elapsed time: 0:01:42.554884
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 19:24:18.654060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.23
 ---- batch: 020 ----
mean loss: 891.02
 ---- batch: 030 ----
mean loss: 877.84
 ---- batch: 040 ----
mean loss: 867.24
 ---- batch: 050 ----
mean loss: 889.09
 ---- batch: 060 ----
mean loss: 882.94
 ---- batch: 070 ----
mean loss: 874.32
 ---- batch: 080 ----
mean loss: 863.87
 ---- batch: 090 ----
mean loss: 869.07
train mean loss: 877.17
epoch train time: 0:00:01.692378
elapsed time: 0:01:44.248030
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 19:24:20.347217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.61
 ---- batch: 020 ----
mean loss: 882.37
 ---- batch: 030 ----
mean loss: 880.28
 ---- batch: 040 ----
mean loss: 870.64
 ---- batch: 050 ----
mean loss: 861.36
 ---- batch: 060 ----
mean loss: 886.22
 ---- batch: 070 ----
mean loss: 878.81
 ---- batch: 080 ----
mean loss: 878.25
 ---- batch: 090 ----
mean loss: 885.50
train mean loss: 877.32
epoch train time: 0:00:01.703472
elapsed time: 0:01:45.952154
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 19:24:22.051306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.59
 ---- batch: 020 ----
mean loss: 865.99
 ---- batch: 030 ----
mean loss: 883.27
 ---- batch: 040 ----
mean loss: 851.05
 ---- batch: 050 ----
mean loss: 854.68
 ---- batch: 060 ----
mean loss: 874.89
 ---- batch: 070 ----
mean loss: 892.25
 ---- batch: 080 ----
mean loss: 897.84
 ---- batch: 090 ----
mean loss: 900.24
train mean loss: 877.84
epoch train time: 0:00:01.675774
elapsed time: 0:01:47.628671
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 19:24:23.727846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.64
 ---- batch: 020 ----
mean loss: 862.14
 ---- batch: 030 ----
mean loss: 882.52
 ---- batch: 040 ----
mean loss: 891.33
 ---- batch: 050 ----
mean loss: 871.50
 ---- batch: 060 ----
mean loss: 880.17
 ---- batch: 070 ----
mean loss: 871.45
 ---- batch: 080 ----
mean loss: 888.10
 ---- batch: 090 ----
mean loss: 888.54
train mean loss: 877.12
epoch train time: 0:00:01.694270
elapsed time: 0:01:49.323620
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 19:24:25.422594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.53
 ---- batch: 020 ----
mean loss: 897.86
 ---- batch: 030 ----
mean loss: 898.87
 ---- batch: 040 ----
mean loss: 877.54
 ---- batch: 050 ----
mean loss: 855.70
 ---- batch: 060 ----
mean loss: 886.22
 ---- batch: 070 ----
mean loss: 873.74
 ---- batch: 080 ----
mean loss: 876.20
 ---- batch: 090 ----
mean loss: 871.21
train mean loss: 877.11
epoch train time: 0:00:01.686123
elapsed time: 0:01:51.010223
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 19:24:27.109418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.52
 ---- batch: 020 ----
mean loss: 873.90
 ---- batch: 030 ----
mean loss: 879.45
 ---- batch: 040 ----
mean loss: 876.61
 ---- batch: 050 ----
mean loss: 879.55
 ---- batch: 060 ----
mean loss: 871.58
 ---- batch: 070 ----
mean loss: 882.35
 ---- batch: 080 ----
mean loss: 879.03
 ---- batch: 090 ----
mean loss: 866.63
train mean loss: 876.95
epoch train time: 0:00:01.715158
elapsed time: 0:01:52.726088
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 19:24:28.825247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.86
 ---- batch: 020 ----
mean loss: 876.06
 ---- batch: 030 ----
mean loss: 886.63
 ---- batch: 040 ----
mean loss: 872.72
 ---- batch: 050 ----
mean loss: 886.91
 ---- batch: 060 ----
mean loss: 879.18
 ---- batch: 070 ----
mean loss: 897.39
 ---- batch: 080 ----
mean loss: 868.64
 ---- batch: 090 ----
mean loss: 873.75
train mean loss: 876.83
epoch train time: 0:00:01.693784
elapsed time: 0:01:54.420545
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 19:24:30.519764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.06
 ---- batch: 020 ----
mean loss: 869.66
 ---- batch: 030 ----
mean loss: 869.55
 ---- batch: 040 ----
mean loss: 883.98
 ---- batch: 050 ----
mean loss: 876.63
 ---- batch: 060 ----
mean loss: 877.21
 ---- batch: 070 ----
mean loss: 874.17
 ---- batch: 080 ----
mean loss: 876.89
 ---- batch: 090 ----
mean loss: 899.76
train mean loss: 876.59
epoch train time: 0:00:01.694503
elapsed time: 0:01:56.115753
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 19:24:32.214926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.05
 ---- batch: 020 ----
mean loss: 876.47
 ---- batch: 030 ----
mean loss: 843.63
 ---- batch: 040 ----
mean loss: 872.04
 ---- batch: 050 ----
mean loss: 882.09
 ---- batch: 060 ----
mean loss: 871.37
 ---- batch: 070 ----
mean loss: 895.33
 ---- batch: 080 ----
mean loss: 873.74
 ---- batch: 090 ----
mean loss: 902.26
train mean loss: 876.25
epoch train time: 0:00:01.688802
elapsed time: 0:01:57.805234
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 19:24:33.904387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.37
 ---- batch: 020 ----
mean loss: 889.22
 ---- batch: 030 ----
mean loss: 868.96
 ---- batch: 040 ----
mean loss: 882.80
 ---- batch: 050 ----
mean loss: 866.60
 ---- batch: 060 ----
mean loss: 869.02
 ---- batch: 070 ----
mean loss: 863.44
 ---- batch: 080 ----
mean loss: 884.24
 ---- batch: 090 ----
mean loss: 877.53
train mean loss: 875.00
epoch train time: 0:00:01.722983
elapsed time: 0:01:59.528894
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 19:24:35.628063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.33
 ---- batch: 020 ----
mean loss: 886.50
 ---- batch: 030 ----
mean loss: 862.94
 ---- batch: 040 ----
mean loss: 866.11
 ---- batch: 050 ----
mean loss: 826.01
 ---- batch: 060 ----
mean loss: 802.05
 ---- batch: 070 ----
mean loss: 748.16
 ---- batch: 080 ----
mean loss: 680.21
 ---- batch: 090 ----
mean loss: 624.74
train mean loss: 782.93
epoch train time: 0:00:01.735777
elapsed time: 0:02:01.265305
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 19:24:37.364540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 541.03
 ---- batch: 020 ----
mean loss: 491.89
 ---- batch: 030 ----
mean loss: 486.97
 ---- batch: 040 ----
mean loss: 463.64
 ---- batch: 050 ----
mean loss: 440.90
 ---- batch: 060 ----
mean loss: 426.53
 ---- batch: 070 ----
mean loss: 421.95
 ---- batch: 080 ----
mean loss: 426.15
 ---- batch: 090 ----
mean loss: 406.25
train mean loss: 452.40
epoch train time: 0:00:01.713008
elapsed time: 0:02:02.979070
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 19:24:39.078260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.31
 ---- batch: 020 ----
mean loss: 387.16
 ---- batch: 030 ----
mean loss: 378.03
 ---- batch: 040 ----
mean loss: 370.83
 ---- batch: 050 ----
mean loss: 365.01
 ---- batch: 060 ----
mean loss: 376.58
 ---- batch: 070 ----
mean loss: 356.34
 ---- batch: 080 ----
mean loss: 357.80
 ---- batch: 090 ----
mean loss: 359.08
train mean loss: 369.32
epoch train time: 0:00:01.712964
elapsed time: 0:02:04.692735
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 19:24:40.791927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.62
 ---- batch: 020 ----
mean loss: 338.87
 ---- batch: 030 ----
mean loss: 359.48
 ---- batch: 040 ----
mean loss: 343.19
 ---- batch: 050 ----
mean loss: 338.75
 ---- batch: 060 ----
mean loss: 328.71
 ---- batch: 070 ----
mean loss: 317.13
 ---- batch: 080 ----
mean loss: 336.38
 ---- batch: 090 ----
mean loss: 318.42
train mean loss: 335.47
epoch train time: 0:00:01.743079
elapsed time: 0:02:06.436494
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 19:24:42.535701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.90
 ---- batch: 020 ----
mean loss: 320.97
 ---- batch: 030 ----
mean loss: 320.32
 ---- batch: 040 ----
mean loss: 315.77
 ---- batch: 050 ----
mean loss: 311.39
 ---- batch: 060 ----
mean loss: 323.68
 ---- batch: 070 ----
mean loss: 318.76
 ---- batch: 080 ----
mean loss: 302.56
 ---- batch: 090 ----
mean loss: 302.53
train mean loss: 314.44
epoch train time: 0:00:01.772281
elapsed time: 0:02:08.209498
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 19:24:44.308687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.47
 ---- batch: 020 ----
mean loss: 302.83
 ---- batch: 030 ----
mean loss: 303.16
 ---- batch: 040 ----
mean loss: 298.79
 ---- batch: 050 ----
mean loss: 302.42
 ---- batch: 060 ----
mean loss: 302.68
 ---- batch: 070 ----
mean loss: 309.15
 ---- batch: 080 ----
mean loss: 289.33
 ---- batch: 090 ----
mean loss: 287.30
train mean loss: 301.18
epoch train time: 0:00:01.721696
elapsed time: 0:02:09.931871
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 19:24:46.031158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.22
 ---- batch: 020 ----
mean loss: 284.26
 ---- batch: 030 ----
mean loss: 301.90
 ---- batch: 040 ----
mean loss: 293.07
 ---- batch: 050 ----
mean loss: 287.72
 ---- batch: 060 ----
mean loss: 289.10
 ---- batch: 070 ----
mean loss: 289.79
 ---- batch: 080 ----
mean loss: 282.15
 ---- batch: 090 ----
mean loss: 289.64
train mean loss: 289.61
epoch train time: 0:00:01.728027
elapsed time: 0:02:11.660681
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 19:24:47.759865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.92
 ---- batch: 020 ----
mean loss: 278.18
 ---- batch: 030 ----
mean loss: 292.73
 ---- batch: 040 ----
mean loss: 272.44
 ---- batch: 050 ----
mean loss: 284.80
 ---- batch: 060 ----
mean loss: 289.88
 ---- batch: 070 ----
mean loss: 269.39
 ---- batch: 080 ----
mean loss: 281.51
 ---- batch: 090 ----
mean loss: 280.42
train mean loss: 280.92
epoch train time: 0:00:01.746863
elapsed time: 0:02:13.408214
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 19:24:49.507368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.40
 ---- batch: 020 ----
mean loss: 273.96
 ---- batch: 030 ----
mean loss: 274.30
 ---- batch: 040 ----
mean loss: 275.39
 ---- batch: 050 ----
mean loss: 275.36
 ---- batch: 060 ----
mean loss: 277.43
 ---- batch: 070 ----
mean loss: 272.85
 ---- batch: 080 ----
mean loss: 276.77
 ---- batch: 090 ----
mean loss: 276.28
train mean loss: 275.92
epoch train time: 0:00:01.703027
elapsed time: 0:02:15.111856
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 19:24:51.211086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.68
 ---- batch: 020 ----
mean loss: 279.55
 ---- batch: 030 ----
mean loss: 264.15
 ---- batch: 040 ----
mean loss: 272.57
 ---- batch: 050 ----
mean loss: 265.72
 ---- batch: 060 ----
mean loss: 272.54
 ---- batch: 070 ----
mean loss: 270.44
 ---- batch: 080 ----
mean loss: 267.05
 ---- batch: 090 ----
mean loss: 269.76
train mean loss: 270.34
epoch train time: 0:00:01.692719
elapsed time: 0:02:16.805287
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 19:24:52.904529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.60
 ---- batch: 020 ----
mean loss: 264.91
 ---- batch: 030 ----
mean loss: 257.36
 ---- batch: 040 ----
mean loss: 264.27
 ---- batch: 050 ----
mean loss: 257.58
 ---- batch: 060 ----
mean loss: 255.30
 ---- batch: 070 ----
mean loss: 266.34
 ---- batch: 080 ----
mean loss: 269.19
 ---- batch: 090 ----
mean loss: 263.58
train mean loss: 262.14
epoch train time: 0:00:01.705227
elapsed time: 0:02:18.511321
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 19:24:54.610510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.28
 ---- batch: 020 ----
mean loss: 252.80
 ---- batch: 030 ----
mean loss: 254.73
 ---- batch: 040 ----
mean loss: 257.81
 ---- batch: 050 ----
mean loss: 259.73
 ---- batch: 060 ----
mean loss: 265.69
 ---- batch: 070 ----
mean loss: 258.92
 ---- batch: 080 ----
mean loss: 255.21
 ---- batch: 090 ----
mean loss: 268.84
train mean loss: 259.72
epoch train time: 0:00:01.717631
elapsed time: 0:02:20.229628
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 19:24:56.328828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.54
 ---- batch: 020 ----
mean loss: 250.65
 ---- batch: 030 ----
mean loss: 259.34
 ---- batch: 040 ----
mean loss: 249.19
 ---- batch: 050 ----
mean loss: 253.67
 ---- batch: 060 ----
mean loss: 253.87
 ---- batch: 070 ----
mean loss: 256.31
 ---- batch: 080 ----
mean loss: 260.31
 ---- batch: 090 ----
mean loss: 258.73
train mean loss: 254.70
epoch train time: 0:00:01.735225
elapsed time: 0:02:21.965635
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 19:24:58.064797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.49
 ---- batch: 020 ----
mean loss: 251.14
 ---- batch: 030 ----
mean loss: 244.13
 ---- batch: 040 ----
mean loss: 248.01
 ---- batch: 050 ----
mean loss: 259.38
 ---- batch: 060 ----
mean loss: 243.33
 ---- batch: 070 ----
mean loss: 240.27
 ---- batch: 080 ----
mean loss: 245.14
 ---- batch: 090 ----
mean loss: 245.22
train mean loss: 247.81
epoch train time: 0:00:01.702705
elapsed time: 0:02:23.669013
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 19:24:59.768210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.21
 ---- batch: 020 ----
mean loss: 243.51
 ---- batch: 030 ----
mean loss: 252.85
 ---- batch: 040 ----
mean loss: 249.58
 ---- batch: 050 ----
mean loss: 251.02
 ---- batch: 060 ----
mean loss: 248.51
 ---- batch: 070 ----
mean loss: 240.25
 ---- batch: 080 ----
mean loss: 238.89
 ---- batch: 090 ----
mean loss: 244.78
train mean loss: 247.15
epoch train time: 0:00:01.729843
elapsed time: 0:02:25.399554
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 19:25:01.498705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.29
 ---- batch: 020 ----
mean loss: 241.01
 ---- batch: 030 ----
mean loss: 240.16
 ---- batch: 040 ----
mean loss: 244.25
 ---- batch: 050 ----
mean loss: 250.10
 ---- batch: 060 ----
mean loss: 251.83
 ---- batch: 070 ----
mean loss: 241.61
 ---- batch: 080 ----
mean loss: 249.89
 ---- batch: 090 ----
mean loss: 238.18
train mean loss: 242.98
epoch train time: 0:00:01.722585
elapsed time: 0:02:27.122740
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 19:25:03.221982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.55
 ---- batch: 020 ----
mean loss: 234.85
 ---- batch: 030 ----
mean loss: 251.31
 ---- batch: 040 ----
mean loss: 243.26
 ---- batch: 050 ----
mean loss: 242.16
 ---- batch: 060 ----
mean loss: 245.36
 ---- batch: 070 ----
mean loss: 236.68
 ---- batch: 080 ----
mean loss: 232.37
 ---- batch: 090 ----
mean loss: 234.89
train mean loss: 238.98
epoch train time: 0:00:01.718463
elapsed time: 0:02:28.841944
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 19:25:04.941083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.75
 ---- batch: 020 ----
mean loss: 241.95
 ---- batch: 030 ----
mean loss: 230.40
 ---- batch: 040 ----
mean loss: 232.52
 ---- batch: 050 ----
mean loss: 231.81
 ---- batch: 060 ----
mean loss: 234.49
 ---- batch: 070 ----
mean loss: 233.21
 ---- batch: 080 ----
mean loss: 236.69
 ---- batch: 090 ----
mean loss: 233.65
train mean loss: 234.91
epoch train time: 0:00:01.732993
elapsed time: 0:02:30.575545
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 19:25:06.674706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.79
 ---- batch: 020 ----
mean loss: 238.17
 ---- batch: 030 ----
mean loss: 224.34
 ---- batch: 040 ----
mean loss: 233.66
 ---- batch: 050 ----
mean loss: 227.29
 ---- batch: 060 ----
mean loss: 230.35
 ---- batch: 070 ----
mean loss: 238.15
 ---- batch: 080 ----
mean loss: 226.04
 ---- batch: 090 ----
mean loss: 238.50
train mean loss: 233.42
epoch train time: 0:00:01.720652
elapsed time: 0:02:32.296808
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 19:25:08.395978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.09
 ---- batch: 020 ----
mean loss: 224.61
 ---- batch: 030 ----
mean loss: 231.17
 ---- batch: 040 ----
mean loss: 233.28
 ---- batch: 050 ----
mean loss: 227.14
 ---- batch: 060 ----
mean loss: 225.94
 ---- batch: 070 ----
mean loss: 215.54
 ---- batch: 080 ----
mean loss: 237.56
 ---- batch: 090 ----
mean loss: 237.11
train mean loss: 230.17
epoch train time: 0:00:01.713755
elapsed time: 0:02:34.011214
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 19:25:10.110367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.49
 ---- batch: 020 ----
mean loss: 219.83
 ---- batch: 030 ----
mean loss: 227.52
 ---- batch: 040 ----
mean loss: 235.17
 ---- batch: 050 ----
mean loss: 228.17
 ---- batch: 060 ----
mean loss: 218.48
 ---- batch: 070 ----
mean loss: 231.87
 ---- batch: 080 ----
mean loss: 220.17
 ---- batch: 090 ----
mean loss: 227.49
train mean loss: 225.44
epoch train time: 0:00:01.695732
elapsed time: 0:02:35.707652
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 19:25:11.806829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.46
 ---- batch: 020 ----
mean loss: 221.75
 ---- batch: 030 ----
mean loss: 225.53
 ---- batch: 040 ----
mean loss: 227.63
 ---- batch: 050 ----
mean loss: 224.76
 ---- batch: 060 ----
mean loss: 235.68
 ---- batch: 070 ----
mean loss: 216.48
 ---- batch: 080 ----
mean loss: 225.44
 ---- batch: 090 ----
mean loss: 229.37
train mean loss: 224.98
epoch train time: 0:00:01.701370
elapsed time: 0:02:37.409671
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 19:25:13.508816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.21
 ---- batch: 020 ----
mean loss: 205.22
 ---- batch: 030 ----
mean loss: 223.48
 ---- batch: 040 ----
mean loss: 217.85
 ---- batch: 050 ----
mean loss: 232.19
 ---- batch: 060 ----
mean loss: 220.08
 ---- batch: 070 ----
mean loss: 226.75
 ---- batch: 080 ----
mean loss: 234.31
 ---- batch: 090 ----
mean loss: 225.38
train mean loss: 221.06
epoch train time: 0:00:01.691490
elapsed time: 0:02:39.101769
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 19:25:15.200916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.03
 ---- batch: 020 ----
mean loss: 220.50
 ---- batch: 030 ----
mean loss: 222.01
 ---- batch: 040 ----
mean loss: 216.43
 ---- batch: 050 ----
mean loss: 216.69
 ---- batch: 060 ----
mean loss: 226.82
 ---- batch: 070 ----
mean loss: 210.92
 ---- batch: 080 ----
mean loss: 220.25
 ---- batch: 090 ----
mean loss: 219.40
train mean loss: 218.39
epoch train time: 0:00:01.690321
elapsed time: 0:02:40.792740
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 19:25:16.891926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.76
 ---- batch: 020 ----
mean loss: 217.23
 ---- batch: 030 ----
mean loss: 216.31
 ---- batch: 040 ----
mean loss: 217.40
 ---- batch: 050 ----
mean loss: 218.46
 ---- batch: 060 ----
mean loss: 214.33
 ---- batch: 070 ----
mean loss: 213.71
 ---- batch: 080 ----
mean loss: 218.40
 ---- batch: 090 ----
mean loss: 214.13
train mean loss: 215.78
epoch train time: 0:00:01.731333
elapsed time: 0:02:42.524750
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 19:25:18.623916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.35
 ---- batch: 020 ----
mean loss: 213.99
 ---- batch: 030 ----
mean loss: 214.20
 ---- batch: 040 ----
mean loss: 219.60
 ---- batch: 050 ----
mean loss: 208.82
 ---- batch: 060 ----
mean loss: 210.58
 ---- batch: 070 ----
mean loss: 215.50
 ---- batch: 080 ----
mean loss: 220.22
 ---- batch: 090 ----
mean loss: 217.23
train mean loss: 214.68
epoch train time: 0:00:01.730330
elapsed time: 0:02:44.255727
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 19:25:20.354896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.99
 ---- batch: 020 ----
mean loss: 209.80
 ---- batch: 030 ----
mean loss: 203.71
 ---- batch: 040 ----
mean loss: 210.29
 ---- batch: 050 ----
mean loss: 217.69
 ---- batch: 060 ----
mean loss: 208.15
 ---- batch: 070 ----
mean loss: 217.33
 ---- batch: 080 ----
mean loss: 214.84
 ---- batch: 090 ----
mean loss: 212.79
train mean loss: 212.12
epoch train time: 0:00:01.721326
elapsed time: 0:02:45.977700
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 19:25:22.076871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.83
 ---- batch: 020 ----
mean loss: 207.75
 ---- batch: 030 ----
mean loss: 210.18
 ---- batch: 040 ----
mean loss: 215.88
 ---- batch: 050 ----
mean loss: 203.26
 ---- batch: 060 ----
mean loss: 208.96
 ---- batch: 070 ----
mean loss: 215.97
 ---- batch: 080 ----
mean loss: 215.94
 ---- batch: 090 ----
mean loss: 212.39
train mean loss: 210.12
epoch train time: 0:00:01.760899
elapsed time: 0:02:47.739284
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 19:25:23.838456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.01
 ---- batch: 020 ----
mean loss: 206.29
 ---- batch: 030 ----
mean loss: 206.42
 ---- batch: 040 ----
mean loss: 219.70
 ---- batch: 050 ----
mean loss: 206.37
 ---- batch: 060 ----
mean loss: 205.42
 ---- batch: 070 ----
mean loss: 214.21
 ---- batch: 080 ----
mean loss: 208.62
 ---- batch: 090 ----
mean loss: 209.18
train mean loss: 208.67
epoch train time: 0:00:01.721521
elapsed time: 0:02:49.461437
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 19:25:25.560658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.34
 ---- batch: 020 ----
mean loss: 199.67
 ---- batch: 030 ----
mean loss: 199.55
 ---- batch: 040 ----
mean loss: 214.79
 ---- batch: 050 ----
mean loss: 203.81
 ---- batch: 060 ----
mean loss: 212.06
 ---- batch: 070 ----
mean loss: 199.75
 ---- batch: 080 ----
mean loss: 208.81
 ---- batch: 090 ----
mean loss: 209.71
train mean loss: 205.24
epoch train time: 0:00:01.700677
elapsed time: 0:02:51.162917
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 19:25:27.262160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.06
 ---- batch: 020 ----
mean loss: 195.01
 ---- batch: 030 ----
mean loss: 201.77
 ---- batch: 040 ----
mean loss: 202.22
 ---- batch: 050 ----
mean loss: 203.21
 ---- batch: 060 ----
mean loss: 205.41
 ---- batch: 070 ----
mean loss: 210.18
 ---- batch: 080 ----
mean loss: 201.93
 ---- batch: 090 ----
mean loss: 213.81
train mean loss: 203.90
epoch train time: 0:00:01.740437
elapsed time: 0:02:52.904068
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 19:25:29.003234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.03
 ---- batch: 020 ----
mean loss: 208.24
 ---- batch: 030 ----
mean loss: 206.65
 ---- batch: 040 ----
mean loss: 201.19
 ---- batch: 050 ----
mean loss: 206.87
 ---- batch: 060 ----
mean loss: 197.40
 ---- batch: 070 ----
mean loss: 195.92
 ---- batch: 080 ----
mean loss: 195.48
 ---- batch: 090 ----
mean loss: 199.25
train mean loss: 200.73
epoch train time: 0:00:01.713710
elapsed time: 0:02:54.618448
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 19:25:30.717646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.73
 ---- batch: 020 ----
mean loss: 199.20
 ---- batch: 030 ----
mean loss: 192.44
 ---- batch: 040 ----
mean loss: 194.59
 ---- batch: 050 ----
mean loss: 204.10
 ---- batch: 060 ----
mean loss: 202.98
 ---- batch: 070 ----
mean loss: 195.19
 ---- batch: 080 ----
mean loss: 205.50
 ---- batch: 090 ----
mean loss: 198.90
train mean loss: 199.07
epoch train time: 0:00:01.721413
elapsed time: 0:02:56.340543
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 19:25:32.439718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.12
 ---- batch: 020 ----
mean loss: 190.30
 ---- batch: 030 ----
mean loss: 195.59
 ---- batch: 040 ----
mean loss: 196.66
 ---- batch: 050 ----
mean loss: 204.19
 ---- batch: 060 ----
mean loss: 200.02
 ---- batch: 070 ----
mean loss: 192.31
 ---- batch: 080 ----
mean loss: 200.08
 ---- batch: 090 ----
mean loss: 200.42
train mean loss: 197.66
epoch train time: 0:00:01.728061
elapsed time: 0:02:58.069273
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 19:25:34.168446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.73
 ---- batch: 020 ----
mean loss: 194.62
 ---- batch: 030 ----
mean loss: 192.75
 ---- batch: 040 ----
mean loss: 195.78
 ---- batch: 050 ----
mean loss: 198.40
 ---- batch: 060 ----
mean loss: 202.29
 ---- batch: 070 ----
mean loss: 186.54
 ---- batch: 080 ----
mean loss: 195.77
 ---- batch: 090 ----
mean loss: 203.46
train mean loss: 196.45
epoch train time: 0:00:01.701283
elapsed time: 0:02:59.771255
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 19:25:35.870433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.26
 ---- batch: 020 ----
mean loss: 204.82
 ---- batch: 030 ----
mean loss: 192.88
 ---- batch: 040 ----
mean loss: 199.20
 ---- batch: 050 ----
mean loss: 196.15
 ---- batch: 060 ----
mean loss: 187.38
 ---- batch: 070 ----
mean loss: 185.32
 ---- batch: 080 ----
mean loss: 196.45
 ---- batch: 090 ----
mean loss: 199.76
train mean loss: 195.65
epoch train time: 0:00:01.753426
elapsed time: 0:03:01.525343
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 19:25:37.624357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.27
 ---- batch: 020 ----
mean loss: 187.38
 ---- batch: 030 ----
mean loss: 195.80
 ---- batch: 040 ----
mean loss: 196.29
 ---- batch: 050 ----
mean loss: 194.45
 ---- batch: 060 ----
mean loss: 193.42
 ---- batch: 070 ----
mean loss: 187.95
 ---- batch: 080 ----
mean loss: 189.38
 ---- batch: 090 ----
mean loss: 185.69
train mean loss: 192.00
epoch train time: 0:00:01.725722
elapsed time: 0:03:03.251544
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 19:25:39.350704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.24
 ---- batch: 020 ----
mean loss: 185.66
 ---- batch: 030 ----
mean loss: 192.24
 ---- batch: 040 ----
mean loss: 193.95
 ---- batch: 050 ----
mean loss: 178.96
 ---- batch: 060 ----
mean loss: 195.20
 ---- batch: 070 ----
mean loss: 192.45
 ---- batch: 080 ----
mean loss: 195.22
 ---- batch: 090 ----
mean loss: 193.84
train mean loss: 191.05
epoch train time: 0:00:01.734988
elapsed time: 0:03:04.987257
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 19:25:41.086539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.21
 ---- batch: 020 ----
mean loss: 188.17
 ---- batch: 030 ----
mean loss: 184.50
 ---- batch: 040 ----
mean loss: 195.44
 ---- batch: 050 ----
mean loss: 179.93
 ---- batch: 060 ----
mean loss: 192.77
 ---- batch: 070 ----
mean loss: 185.23
 ---- batch: 080 ----
mean loss: 190.81
 ---- batch: 090 ----
mean loss: 189.89
train mean loss: 189.45
epoch train time: 0:00:01.739485
elapsed time: 0:03:06.727521
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 19:25:42.826726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.19
 ---- batch: 020 ----
mean loss: 188.32
 ---- batch: 030 ----
mean loss: 187.94
 ---- batch: 040 ----
mean loss: 183.50
 ---- batch: 050 ----
mean loss: 184.77
 ---- batch: 060 ----
mean loss: 188.72
 ---- batch: 070 ----
mean loss: 190.09
 ---- batch: 080 ----
mean loss: 189.46
 ---- batch: 090 ----
mean loss: 191.52
train mean loss: 186.73
epoch train time: 0:00:01.698305
elapsed time: 0:03:08.426524
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 19:25:44.525710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.33
 ---- batch: 020 ----
mean loss: 184.14
 ---- batch: 030 ----
mean loss: 178.09
 ---- batch: 040 ----
mean loss: 183.44
 ---- batch: 050 ----
mean loss: 196.26
 ---- batch: 060 ----
mean loss: 196.87
 ---- batch: 070 ----
mean loss: 185.03
 ---- batch: 080 ----
mean loss: 192.83
 ---- batch: 090 ----
mean loss: 186.29
train mean loss: 186.93
epoch train time: 0:00:01.701193
elapsed time: 0:03:10.128382
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 19:25:46.227550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.94
 ---- batch: 020 ----
mean loss: 193.27
 ---- batch: 030 ----
mean loss: 181.18
 ---- batch: 040 ----
mean loss: 179.68
 ---- batch: 050 ----
mean loss: 186.24
 ---- batch: 060 ----
mean loss: 187.27
 ---- batch: 070 ----
mean loss: 181.90
 ---- batch: 080 ----
mean loss: 190.02
 ---- batch: 090 ----
mean loss: 180.17
train mean loss: 184.39
epoch train time: 0:00:01.696159
elapsed time: 0:03:11.825259
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 19:25:47.924538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.24
 ---- batch: 020 ----
mean loss: 187.50
 ---- batch: 030 ----
mean loss: 181.99
 ---- batch: 040 ----
mean loss: 180.40
 ---- batch: 050 ----
mean loss: 176.22
 ---- batch: 060 ----
mean loss: 189.80
 ---- batch: 070 ----
mean loss: 179.14
 ---- batch: 080 ----
mean loss: 187.73
 ---- batch: 090 ----
mean loss: 183.95
train mean loss: 183.44
epoch train time: 0:00:01.725629
elapsed time: 0:03:13.551683
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 19:25:49.650842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.20
 ---- batch: 020 ----
mean loss: 186.92
 ---- batch: 030 ----
mean loss: 181.99
 ---- batch: 040 ----
mean loss: 185.44
 ---- batch: 050 ----
mean loss: 180.95
 ---- batch: 060 ----
mean loss: 188.96
 ---- batch: 070 ----
mean loss: 188.06
 ---- batch: 080 ----
mean loss: 188.10
 ---- batch: 090 ----
mean loss: 179.15
train mean loss: 182.99
epoch train time: 0:00:01.727566
elapsed time: 0:03:15.279870
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 19:25:51.379056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.26
 ---- batch: 020 ----
mean loss: 170.59
 ---- batch: 030 ----
mean loss: 185.71
 ---- batch: 040 ----
mean loss: 175.15
 ---- batch: 050 ----
mean loss: 177.76
 ---- batch: 060 ----
mean loss: 186.78
 ---- batch: 070 ----
mean loss: 180.65
 ---- batch: 080 ----
mean loss: 184.31
 ---- batch: 090 ----
mean loss: 187.15
train mean loss: 180.52
epoch train time: 0:00:01.727702
elapsed time: 0:03:17.008240
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 19:25:53.107415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.61
 ---- batch: 020 ----
mean loss: 175.78
 ---- batch: 030 ----
mean loss: 175.68
 ---- batch: 040 ----
mean loss: 175.53
 ---- batch: 050 ----
mean loss: 181.86
 ---- batch: 060 ----
mean loss: 185.68
 ---- batch: 070 ----
mean loss: 177.50
 ---- batch: 080 ----
mean loss: 179.63
 ---- batch: 090 ----
mean loss: 181.56
train mean loss: 179.40
epoch train time: 0:00:01.727606
elapsed time: 0:03:18.736520
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 19:25:54.835707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.66
 ---- batch: 020 ----
mean loss: 169.79
 ---- batch: 030 ----
mean loss: 176.51
 ---- batch: 040 ----
mean loss: 178.28
 ---- batch: 050 ----
mean loss: 178.44
 ---- batch: 060 ----
mean loss: 181.22
 ---- batch: 070 ----
mean loss: 186.33
 ---- batch: 080 ----
mean loss: 181.13
 ---- batch: 090 ----
mean loss: 174.55
train mean loss: 178.13
epoch train time: 0:00:01.702761
elapsed time: 0:03:20.439955
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 19:25:56.539117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.76
 ---- batch: 020 ----
mean loss: 174.32
 ---- batch: 030 ----
mean loss: 176.70
 ---- batch: 040 ----
mean loss: 175.21
 ---- batch: 050 ----
mean loss: 174.54
 ---- batch: 060 ----
mean loss: 178.85
 ---- batch: 070 ----
mean loss: 182.49
 ---- batch: 080 ----
mean loss: 177.23
 ---- batch: 090 ----
mean loss: 182.43
train mean loss: 176.87
epoch train time: 0:00:01.710960
elapsed time: 0:03:22.151506
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 19:25:58.250670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.21
 ---- batch: 020 ----
mean loss: 171.99
 ---- batch: 030 ----
mean loss: 169.67
 ---- batch: 040 ----
mean loss: 170.78
 ---- batch: 050 ----
mean loss: 177.22
 ---- batch: 060 ----
mean loss: 178.86
 ---- batch: 070 ----
mean loss: 179.41
 ---- batch: 080 ----
mean loss: 181.39
 ---- batch: 090 ----
mean loss: 176.74
train mean loss: 175.10
epoch train time: 0:00:01.694885
elapsed time: 0:03:23.847081
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 19:25:59.946246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.42
 ---- batch: 020 ----
mean loss: 165.68
 ---- batch: 030 ----
mean loss: 168.99
 ---- batch: 040 ----
mean loss: 164.27
 ---- batch: 050 ----
mean loss: 179.28
 ---- batch: 060 ----
mean loss: 178.11
 ---- batch: 070 ----
mean loss: 171.27
 ---- batch: 080 ----
mean loss: 175.61
 ---- batch: 090 ----
mean loss: 179.47
train mean loss: 173.49
epoch train time: 0:00:01.660765
elapsed time: 0:03:25.508519
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 19:26:01.607685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.79
 ---- batch: 020 ----
mean loss: 168.74
 ---- batch: 030 ----
mean loss: 169.22
 ---- batch: 040 ----
mean loss: 174.56
 ---- batch: 050 ----
mean loss: 176.83
 ---- batch: 060 ----
mean loss: 167.91
 ---- batch: 070 ----
mean loss: 174.40
 ---- batch: 080 ----
mean loss: 179.26
 ---- batch: 090 ----
mean loss: 174.37
train mean loss: 173.62
epoch train time: 0:00:01.695034
elapsed time: 0:03:27.204211
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 19:26:03.303391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.60
 ---- batch: 020 ----
mean loss: 168.63
 ---- batch: 030 ----
mean loss: 170.85
 ---- batch: 040 ----
mean loss: 171.55
 ---- batch: 050 ----
mean loss: 176.02
 ---- batch: 060 ----
mean loss: 177.29
 ---- batch: 070 ----
mean loss: 170.85
 ---- batch: 080 ----
mean loss: 166.07
 ---- batch: 090 ----
mean loss: 173.23
train mean loss: 172.12
epoch train time: 0:00:01.703854
elapsed time: 0:03:28.908723
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 19:26:05.007878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.98
 ---- batch: 020 ----
mean loss: 169.58
 ---- batch: 030 ----
mean loss: 171.71
 ---- batch: 040 ----
mean loss: 175.39
 ---- batch: 050 ----
mean loss: 169.72
 ---- batch: 060 ----
mean loss: 170.95
 ---- batch: 070 ----
mean loss: 171.63
 ---- batch: 080 ----
mean loss: 167.85
 ---- batch: 090 ----
mean loss: 174.66
train mean loss: 171.69
epoch train time: 0:00:01.673495
elapsed time: 0:03:30.582883
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 19:26:06.682066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.80
 ---- batch: 020 ----
mean loss: 166.40
 ---- batch: 030 ----
mean loss: 167.01
 ---- batch: 040 ----
mean loss: 165.85
 ---- batch: 050 ----
mean loss: 164.64
 ---- batch: 060 ----
mean loss: 167.64
 ---- batch: 070 ----
mean loss: 178.49
 ---- batch: 080 ----
mean loss: 169.96
 ---- batch: 090 ----
mean loss: 172.50
train mean loss: 170.05
epoch train time: 0:00:01.709247
elapsed time: 0:03:32.292788
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 19:26:08.391947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.72
 ---- batch: 020 ----
mean loss: 170.98
 ---- batch: 030 ----
mean loss: 161.32
 ---- batch: 040 ----
mean loss: 168.85
 ---- batch: 050 ----
mean loss: 166.34
 ---- batch: 060 ----
mean loss: 171.81
 ---- batch: 070 ----
mean loss: 164.10
 ---- batch: 080 ----
mean loss: 165.41
 ---- batch: 090 ----
mean loss: 168.32
train mean loss: 168.03
epoch train time: 0:00:01.737078
elapsed time: 0:03:34.030473
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 19:26:10.129640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.40
 ---- batch: 020 ----
mean loss: 159.16
 ---- batch: 030 ----
mean loss: 170.18
 ---- batch: 040 ----
mean loss: 166.57
 ---- batch: 050 ----
mean loss: 162.55
 ---- batch: 060 ----
mean loss: 168.76
 ---- batch: 070 ----
mean loss: 173.71
 ---- batch: 080 ----
mean loss: 170.39
 ---- batch: 090 ----
mean loss: 172.88
train mean loss: 167.81
epoch train time: 0:00:01.698202
elapsed time: 0:03:35.729371
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 19:26:11.828525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.74
 ---- batch: 020 ----
mean loss: 166.17
 ---- batch: 030 ----
mean loss: 159.55
 ---- batch: 040 ----
mean loss: 167.37
 ---- batch: 050 ----
mean loss: 169.25
 ---- batch: 060 ----
mean loss: 166.34
 ---- batch: 070 ----
mean loss: 172.08
 ---- batch: 080 ----
mean loss: 166.81
 ---- batch: 090 ----
mean loss: 165.69
train mean loss: 166.25
epoch train time: 0:00:01.721718
elapsed time: 0:03:37.451850
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 19:26:13.550844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.60
 ---- batch: 020 ----
mean loss: 166.42
 ---- batch: 030 ----
mean loss: 163.03
 ---- batch: 040 ----
mean loss: 168.81
 ---- batch: 050 ----
mean loss: 169.91
 ---- batch: 060 ----
mean loss: 165.08
 ---- batch: 070 ----
mean loss: 169.33
 ---- batch: 080 ----
mean loss: 168.73
 ---- batch: 090 ----
mean loss: 169.79
train mean loss: 166.46
epoch train time: 0:00:01.717262
elapsed time: 0:03:39.169544
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 19:26:15.268697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.67
 ---- batch: 020 ----
mean loss: 173.26
 ---- batch: 030 ----
mean loss: 158.18
 ---- batch: 040 ----
mean loss: 166.55
 ---- batch: 050 ----
mean loss: 160.22
 ---- batch: 060 ----
mean loss: 168.93
 ---- batch: 070 ----
mean loss: 162.43
 ---- batch: 080 ----
mean loss: 164.27
 ---- batch: 090 ----
mean loss: 160.19
train mean loss: 164.05
epoch train time: 0:00:01.702267
elapsed time: 0:03:40.872492
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 19:26:16.971704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.04
 ---- batch: 020 ----
mean loss: 159.73
 ---- batch: 030 ----
mean loss: 158.76
 ---- batch: 040 ----
mean loss: 160.37
 ---- batch: 050 ----
mean loss: 155.75
 ---- batch: 060 ----
mean loss: 170.97
 ---- batch: 070 ----
mean loss: 165.66
 ---- batch: 080 ----
mean loss: 166.65
 ---- batch: 090 ----
mean loss: 165.38
train mean loss: 163.54
epoch train time: 0:00:01.733573
elapsed time: 0:03:42.606874
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 19:26:18.706067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.76
 ---- batch: 020 ----
mean loss: 161.34
 ---- batch: 030 ----
mean loss: 160.24
 ---- batch: 040 ----
mean loss: 161.69
 ---- batch: 050 ----
mean loss: 160.19
 ---- batch: 060 ----
mean loss: 164.98
 ---- batch: 070 ----
mean loss: 167.20
 ---- batch: 080 ----
mean loss: 160.71
 ---- batch: 090 ----
mean loss: 166.99
train mean loss: 163.34
epoch train time: 0:00:01.709876
elapsed time: 0:03:44.317394
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 19:26:20.416530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.96
 ---- batch: 020 ----
mean loss: 154.83
 ---- batch: 030 ----
mean loss: 161.90
 ---- batch: 040 ----
mean loss: 165.52
 ---- batch: 050 ----
mean loss: 165.52
 ---- batch: 060 ----
mean loss: 165.38
 ---- batch: 070 ----
mean loss: 166.29
 ---- batch: 080 ----
mean loss: 158.38
 ---- batch: 090 ----
mean loss: 159.54
train mean loss: 161.26
epoch train time: 0:00:01.730636
elapsed time: 0:03:46.048622
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 19:26:22.147790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.79
 ---- batch: 020 ----
mean loss: 160.81
 ---- batch: 030 ----
mean loss: 159.30
 ---- batch: 040 ----
mean loss: 156.24
 ---- batch: 050 ----
mean loss: 156.64
 ---- batch: 060 ----
mean loss: 166.82
 ---- batch: 070 ----
mean loss: 164.81
 ---- batch: 080 ----
mean loss: 163.91
 ---- batch: 090 ----
mean loss: 161.36
train mean loss: 160.86
epoch train time: 0:00:01.732693
elapsed time: 0:03:47.782023
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 19:26:23.881212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.15
 ---- batch: 020 ----
mean loss: 154.93
 ---- batch: 030 ----
mean loss: 153.88
 ---- batch: 040 ----
mean loss: 158.65
 ---- batch: 050 ----
mean loss: 158.00
 ---- batch: 060 ----
mean loss: 163.11
 ---- batch: 070 ----
mean loss: 157.79
 ---- batch: 080 ----
mean loss: 163.82
 ---- batch: 090 ----
mean loss: 161.46
train mean loss: 159.87
epoch train time: 0:00:01.713219
elapsed time: 0:03:49.495981
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 19:26:25.595220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.25
 ---- batch: 020 ----
mean loss: 159.02
 ---- batch: 030 ----
mean loss: 158.20
 ---- batch: 040 ----
mean loss: 159.81
 ---- batch: 050 ----
mean loss: 157.59
 ---- batch: 060 ----
mean loss: 166.04
 ---- batch: 070 ----
mean loss: 164.53
 ---- batch: 080 ----
mean loss: 151.52
 ---- batch: 090 ----
mean loss: 163.57
train mean loss: 159.83
epoch train time: 0:00:01.702601
elapsed time: 0:03:51.199345
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 19:26:27.298490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.62
 ---- batch: 020 ----
mean loss: 156.67
 ---- batch: 030 ----
mean loss: 156.81
 ---- batch: 040 ----
mean loss: 148.97
 ---- batch: 050 ----
mean loss: 164.30
 ---- batch: 060 ----
mean loss: 159.63
 ---- batch: 070 ----
mean loss: 160.43
 ---- batch: 080 ----
mean loss: 160.30
 ---- batch: 090 ----
mean loss: 163.71
train mean loss: 158.33
epoch train time: 0:00:01.709771
elapsed time: 0:03:52.909744
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 19:26:29.008964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.84
 ---- batch: 020 ----
mean loss: 143.66
 ---- batch: 030 ----
mean loss: 159.14
 ---- batch: 040 ----
mean loss: 156.67
 ---- batch: 050 ----
mean loss: 159.90
 ---- batch: 060 ----
mean loss: 160.31
 ---- batch: 070 ----
mean loss: 159.54
 ---- batch: 080 ----
mean loss: 164.64
 ---- batch: 090 ----
mean loss: 164.00
train mean loss: 156.90
epoch train time: 0:00:01.724618
elapsed time: 0:03:54.635076
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 19:26:30.734318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.44
 ---- batch: 020 ----
mean loss: 146.13
 ---- batch: 030 ----
mean loss: 156.67
 ---- batch: 040 ----
mean loss: 157.03
 ---- batch: 050 ----
mean loss: 155.95
 ---- batch: 060 ----
mean loss: 155.50
 ---- batch: 070 ----
mean loss: 154.01
 ---- batch: 080 ----
mean loss: 161.79
 ---- batch: 090 ----
mean loss: 167.34
train mean loss: 156.73
epoch train time: 0:00:01.717608
elapsed time: 0:03:56.353405
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 19:26:32.452619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.08
 ---- batch: 020 ----
mean loss: 151.88
 ---- batch: 030 ----
mean loss: 156.26
 ---- batch: 040 ----
mean loss: 157.70
 ---- batch: 050 ----
mean loss: 158.03
 ---- batch: 060 ----
mean loss: 157.33
 ---- batch: 070 ----
mean loss: 158.51
 ---- batch: 080 ----
mean loss: 161.01
 ---- batch: 090 ----
mean loss: 150.62
train mean loss: 155.30
epoch train time: 0:00:01.695387
elapsed time: 0:03:58.049554
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 19:26:34.148739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.83
 ---- batch: 020 ----
mean loss: 152.33
 ---- batch: 030 ----
mean loss: 156.74
 ---- batch: 040 ----
mean loss: 152.29
 ---- batch: 050 ----
mean loss: 155.01
 ---- batch: 060 ----
mean loss: 156.77
 ---- batch: 070 ----
mean loss: 161.51
 ---- batch: 080 ----
mean loss: 158.50
 ---- batch: 090 ----
mean loss: 152.80
train mean loss: 155.26
epoch train time: 0:00:01.710520
elapsed time: 0:03:59.760749
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 19:26:35.859920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.97
 ---- batch: 020 ----
mean loss: 149.39
 ---- batch: 030 ----
mean loss: 149.44
 ---- batch: 040 ----
mean loss: 153.66
 ---- batch: 050 ----
mean loss: 161.68
 ---- batch: 060 ----
mean loss: 150.96
 ---- batch: 070 ----
mean loss: 151.33
 ---- batch: 080 ----
mean loss: 160.97
 ---- batch: 090 ----
mean loss: 160.11
train mean loss: 153.47
epoch train time: 0:00:01.699719
elapsed time: 0:04:01.461148
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 19:26:37.560296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.13
 ---- batch: 020 ----
mean loss: 151.21
 ---- batch: 030 ----
mean loss: 149.64
 ---- batch: 040 ----
mean loss: 150.04
 ---- batch: 050 ----
mean loss: 157.26
 ---- batch: 060 ----
mean loss: 151.16
 ---- batch: 070 ----
mean loss: 157.58
 ---- batch: 080 ----
mean loss: 159.52
 ---- batch: 090 ----
mean loss: 154.34
train mean loss: 152.88
epoch train time: 0:00:01.731565
elapsed time: 0:04:03.193378
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 19:26:39.292567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.55
 ---- batch: 020 ----
mean loss: 153.37
 ---- batch: 030 ----
mean loss: 153.29
 ---- batch: 040 ----
mean loss: 153.52
 ---- batch: 050 ----
mean loss: 154.00
 ---- batch: 060 ----
mean loss: 151.53
 ---- batch: 070 ----
mean loss: 154.24
 ---- batch: 080 ----
mean loss: 150.46
 ---- batch: 090 ----
mean loss: 151.75
train mean loss: 152.83
epoch train time: 0:00:01.697974
elapsed time: 0:04:04.892045
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 19:26:40.991200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.25
 ---- batch: 020 ----
mean loss: 145.06
 ---- batch: 030 ----
mean loss: 151.08
 ---- batch: 040 ----
mean loss: 153.03
 ---- batch: 050 ----
mean loss: 159.29
 ---- batch: 060 ----
mean loss: 151.21
 ---- batch: 070 ----
mean loss: 153.23
 ---- batch: 080 ----
mean loss: 154.88
 ---- batch: 090 ----
mean loss: 153.51
train mean loss: 151.59
epoch train time: 0:00:01.684609
elapsed time: 0:04:06.577303
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 19:26:42.676559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.09
 ---- batch: 020 ----
mean loss: 147.00
 ---- batch: 030 ----
mean loss: 144.21
 ---- batch: 040 ----
mean loss: 152.50
 ---- batch: 050 ----
mean loss: 151.14
 ---- batch: 060 ----
mean loss: 155.33
 ---- batch: 070 ----
mean loss: 151.19
 ---- batch: 080 ----
mean loss: 151.73
 ---- batch: 090 ----
mean loss: 153.13
train mean loss: 150.69
epoch train time: 0:00:01.719301
elapsed time: 0:04:08.297368
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 19:26:44.396519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.53
 ---- batch: 020 ----
mean loss: 149.87
 ---- batch: 030 ----
mean loss: 144.07
 ---- batch: 040 ----
mean loss: 149.94
 ---- batch: 050 ----
mean loss: 154.37
 ---- batch: 060 ----
mean loss: 152.40
 ---- batch: 070 ----
mean loss: 147.93
 ---- batch: 080 ----
mean loss: 149.49
 ---- batch: 090 ----
mean loss: 161.89
train mean loss: 149.61
epoch train time: 0:00:01.737002
elapsed time: 0:04:10.035043
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 19:26:46.134198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.25
 ---- batch: 020 ----
mean loss: 149.53
 ---- batch: 030 ----
mean loss: 148.59
 ---- batch: 040 ----
mean loss: 149.50
 ---- batch: 050 ----
mean loss: 147.68
 ---- batch: 060 ----
mean loss: 152.70
 ---- batch: 070 ----
mean loss: 148.44
 ---- batch: 080 ----
mean loss: 152.53
 ---- batch: 090 ----
mean loss: 154.72
train mean loss: 148.82
epoch train time: 0:00:01.730993
elapsed time: 0:04:11.766878
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 19:26:47.865859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.33
 ---- batch: 020 ----
mean loss: 148.66
 ---- batch: 030 ----
mean loss: 144.45
 ---- batch: 040 ----
mean loss: 144.67
 ---- batch: 050 ----
mean loss: 148.02
 ---- batch: 060 ----
mean loss: 150.17
 ---- batch: 070 ----
mean loss: 149.22
 ---- batch: 080 ----
mean loss: 148.59
 ---- batch: 090 ----
mean loss: 161.00
train mean loss: 148.73
epoch train time: 0:00:01.731033
elapsed time: 0:04:13.498400
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 19:26:49.597585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.20
 ---- batch: 020 ----
mean loss: 141.99
 ---- batch: 030 ----
mean loss: 144.48
 ---- batch: 040 ----
mean loss: 144.52
 ---- batch: 050 ----
mean loss: 149.33
 ---- batch: 060 ----
mean loss: 144.77
 ---- batch: 070 ----
mean loss: 156.98
 ---- batch: 080 ----
mean loss: 153.93
 ---- batch: 090 ----
mean loss: 153.42
train mean loss: 148.04
epoch train time: 0:00:01.720736
elapsed time: 0:04:15.219781
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 19:26:51.318938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.29
 ---- batch: 020 ----
mean loss: 142.77
 ---- batch: 030 ----
mean loss: 142.66
 ---- batch: 040 ----
mean loss: 144.36
 ---- batch: 050 ----
mean loss: 144.73
 ---- batch: 060 ----
mean loss: 146.63
 ---- batch: 070 ----
mean loss: 148.95
 ---- batch: 080 ----
mean loss: 157.65
 ---- batch: 090 ----
mean loss: 150.45
train mean loss: 147.35
epoch train time: 0:00:01.706345
elapsed time: 0:04:16.926785
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 19:26:53.026018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.71
 ---- batch: 020 ----
mean loss: 149.67
 ---- batch: 030 ----
mean loss: 144.90
 ---- batch: 040 ----
mean loss: 141.83
 ---- batch: 050 ----
mean loss: 143.97
 ---- batch: 060 ----
mean loss: 140.17
 ---- batch: 070 ----
mean loss: 144.64
 ---- batch: 080 ----
mean loss: 150.69
 ---- batch: 090 ----
mean loss: 160.88
train mean loss: 146.40
epoch train time: 0:00:01.704789
elapsed time: 0:04:18.632293
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 19:26:54.731480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.26
 ---- batch: 020 ----
mean loss: 136.99
 ---- batch: 030 ----
mean loss: 150.38
 ---- batch: 040 ----
mean loss: 144.41
 ---- batch: 050 ----
mean loss: 148.47
 ---- batch: 060 ----
mean loss: 143.39
 ---- batch: 070 ----
mean loss: 153.15
 ---- batch: 080 ----
mean loss: 151.53
 ---- batch: 090 ----
mean loss: 144.07
train mean loss: 146.59
epoch train time: 0:00:01.706271
elapsed time: 0:04:20.339217
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 19:26:56.438373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.14
 ---- batch: 020 ----
mean loss: 143.17
 ---- batch: 030 ----
mean loss: 144.00
 ---- batch: 040 ----
mean loss: 143.59
 ---- batch: 050 ----
mean loss: 142.15
 ---- batch: 060 ----
mean loss: 145.16
 ---- batch: 070 ----
mean loss: 140.33
 ---- batch: 080 ----
mean loss: 148.02
 ---- batch: 090 ----
mean loss: 147.13
train mean loss: 144.76
epoch train time: 0:00:01.692630
elapsed time: 0:04:22.032529
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 19:26:58.131772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.41
 ---- batch: 020 ----
mean loss: 137.83
 ---- batch: 030 ----
mean loss: 140.72
 ---- batch: 040 ----
mean loss: 143.38
 ---- batch: 050 ----
mean loss: 144.25
 ---- batch: 060 ----
mean loss: 140.64
 ---- batch: 070 ----
mean loss: 153.66
 ---- batch: 080 ----
mean loss: 144.37
 ---- batch: 090 ----
mean loss: 151.29
train mean loss: 144.10
epoch train time: 0:00:01.705900
elapsed time: 0:04:23.739217
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 19:26:59.838367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.67
 ---- batch: 020 ----
mean loss: 141.65
 ---- batch: 030 ----
mean loss: 141.86
 ---- batch: 040 ----
mean loss: 140.87
 ---- batch: 050 ----
mean loss: 141.78
 ---- batch: 060 ----
mean loss: 147.38
 ---- batch: 070 ----
mean loss: 150.57
 ---- batch: 080 ----
mean loss: 145.04
 ---- batch: 090 ----
mean loss: 141.44
train mean loss: 143.63
epoch train time: 0:00:01.720348
elapsed time: 0:04:25.460237
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 19:27:01.559398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.42
 ---- batch: 020 ----
mean loss: 138.57
 ---- batch: 030 ----
mean loss: 141.00
 ---- batch: 040 ----
mean loss: 146.01
 ---- batch: 050 ----
mean loss: 141.55
 ---- batch: 060 ----
mean loss: 141.01
 ---- batch: 070 ----
mean loss: 142.08
 ---- batch: 080 ----
mean loss: 145.70
 ---- batch: 090 ----
mean loss: 145.51
train mean loss: 143.56
epoch train time: 0:00:01.744904
elapsed time: 0:04:27.205758
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 19:27:03.304970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.58
 ---- batch: 020 ----
mean loss: 137.36
 ---- batch: 030 ----
mean loss: 138.15
 ---- batch: 040 ----
mean loss: 138.58
 ---- batch: 050 ----
mean loss: 142.45
 ---- batch: 060 ----
mean loss: 141.93
 ---- batch: 070 ----
mean loss: 144.26
 ---- batch: 080 ----
mean loss: 147.05
 ---- batch: 090 ----
mean loss: 144.62
train mean loss: 141.99
epoch train time: 0:00:01.702441
elapsed time: 0:04:28.908916
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 19:27:05.008088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.74
 ---- batch: 020 ----
mean loss: 137.41
 ---- batch: 030 ----
mean loss: 135.17
 ---- batch: 040 ----
mean loss: 143.39
 ---- batch: 050 ----
mean loss: 141.88
 ---- batch: 060 ----
mean loss: 148.29
 ---- batch: 070 ----
mean loss: 140.37
 ---- batch: 080 ----
mean loss: 143.03
 ---- batch: 090 ----
mean loss: 143.09
train mean loss: 141.72
epoch train time: 0:00:01.687908
elapsed time: 0:04:30.597472
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 19:27:06.696619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.08
 ---- batch: 020 ----
mean loss: 138.08
 ---- batch: 030 ----
mean loss: 139.35
 ---- batch: 040 ----
mean loss: 144.07
 ---- batch: 050 ----
mean loss: 145.18
 ---- batch: 060 ----
mean loss: 139.85
 ---- batch: 070 ----
mean loss: 134.12
 ---- batch: 080 ----
mean loss: 144.74
 ---- batch: 090 ----
mean loss: 148.34
train mean loss: 141.22
epoch train time: 0:00:01.729630
elapsed time: 0:04:32.327767
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 19:27:08.426949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.96
 ---- batch: 020 ----
mean loss: 131.70
 ---- batch: 030 ----
mean loss: 144.85
 ---- batch: 040 ----
mean loss: 134.95
 ---- batch: 050 ----
mean loss: 138.88
 ---- batch: 060 ----
mean loss: 144.28
 ---- batch: 070 ----
mean loss: 146.00
 ---- batch: 080 ----
mean loss: 148.81
 ---- batch: 090 ----
mean loss: 141.69
train mean loss: 140.16
epoch train time: 0:00:01.728773
elapsed time: 0:04:34.057187
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 19:27:10.156364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.03
 ---- batch: 020 ----
mean loss: 141.05
 ---- batch: 030 ----
mean loss: 136.24
 ---- batch: 040 ----
mean loss: 139.28
 ---- batch: 050 ----
mean loss: 139.18
 ---- batch: 060 ----
mean loss: 141.68
 ---- batch: 070 ----
mean loss: 137.10
 ---- batch: 080 ----
mean loss: 144.77
 ---- batch: 090 ----
mean loss: 142.01
train mean loss: 139.96
epoch train time: 0:00:01.718894
elapsed time: 0:04:35.776764
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 19:27:11.875948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.50
 ---- batch: 020 ----
mean loss: 134.89
 ---- batch: 030 ----
mean loss: 139.95
 ---- batch: 040 ----
mean loss: 130.06
 ---- batch: 050 ----
mean loss: 140.96
 ---- batch: 060 ----
mean loss: 138.11
 ---- batch: 070 ----
mean loss: 146.92
 ---- batch: 080 ----
mean loss: 138.27
 ---- batch: 090 ----
mean loss: 149.24
train mean loss: 139.75
epoch train time: 0:00:01.687062
elapsed time: 0:04:37.464558
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 19:27:13.563790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.10
 ---- batch: 020 ----
mean loss: 137.31
 ---- batch: 030 ----
mean loss: 137.05
 ---- batch: 040 ----
mean loss: 138.87
 ---- batch: 050 ----
mean loss: 136.83
 ---- batch: 060 ----
mean loss: 143.29
 ---- batch: 070 ----
mean loss: 140.64
 ---- batch: 080 ----
mean loss: 138.64
 ---- batch: 090 ----
mean loss: 138.50
train mean loss: 138.92
epoch train time: 0:00:01.719039
elapsed time: 0:04:39.184327
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 19:27:15.283519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.89
 ---- batch: 020 ----
mean loss: 138.04
 ---- batch: 030 ----
mean loss: 134.39
 ---- batch: 040 ----
mean loss: 143.51
 ---- batch: 050 ----
mean loss: 130.86
 ---- batch: 060 ----
mean loss: 136.94
 ---- batch: 070 ----
mean loss: 140.05
 ---- batch: 080 ----
mean loss: 132.59
 ---- batch: 090 ----
mean loss: 138.33
train mean loss: 137.84
epoch train time: 0:00:01.691983
elapsed time: 0:04:40.877102
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 19:27:16.976290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.22
 ---- batch: 020 ----
mean loss: 138.50
 ---- batch: 030 ----
mean loss: 133.96
 ---- batch: 040 ----
mean loss: 137.67
 ---- batch: 050 ----
mean loss: 132.45
 ---- batch: 060 ----
mean loss: 144.82
 ---- batch: 070 ----
mean loss: 141.92
 ---- batch: 080 ----
mean loss: 131.80
 ---- batch: 090 ----
mean loss: 139.79
train mean loss: 137.22
epoch train time: 0:00:01.732673
elapsed time: 0:04:42.610450
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 19:27:18.709600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.83
 ---- batch: 020 ----
mean loss: 131.30
 ---- batch: 030 ----
mean loss: 130.53
 ---- batch: 040 ----
mean loss: 136.35
 ---- batch: 050 ----
mean loss: 145.06
 ---- batch: 060 ----
mean loss: 140.56
 ---- batch: 070 ----
mean loss: 135.79
 ---- batch: 080 ----
mean loss: 144.75
 ---- batch: 090 ----
mean loss: 136.36
train mean loss: 136.80
epoch train time: 0:00:01.714970
elapsed time: 0:04:44.326028
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 19:27:20.425227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.47
 ---- batch: 020 ----
mean loss: 130.72
 ---- batch: 030 ----
mean loss: 135.87
 ---- batch: 040 ----
mean loss: 134.21
 ---- batch: 050 ----
mean loss: 137.87
 ---- batch: 060 ----
mean loss: 137.95
 ---- batch: 070 ----
mean loss: 135.59
 ---- batch: 080 ----
mean loss: 137.22
 ---- batch: 090 ----
mean loss: 138.51
train mean loss: 136.04
epoch train time: 0:00:01.715434
elapsed time: 0:04:46.042151
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 19:27:22.141386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.81
 ---- batch: 020 ----
mean loss: 134.17
 ---- batch: 030 ----
mean loss: 133.08
 ---- batch: 040 ----
mean loss: 130.72
 ---- batch: 050 ----
mean loss: 132.98
 ---- batch: 060 ----
mean loss: 139.85
 ---- batch: 070 ----
mean loss: 136.11
 ---- batch: 080 ----
mean loss: 140.87
 ---- batch: 090 ----
mean loss: 136.99
train mean loss: 135.33
epoch train time: 0:00:01.743087
elapsed time: 0:04:47.785997
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 19:27:23.885175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.78
 ---- batch: 020 ----
mean loss: 133.29
 ---- batch: 030 ----
mean loss: 131.82
 ---- batch: 040 ----
mean loss: 132.82
 ---- batch: 050 ----
mean loss: 131.83
 ---- batch: 060 ----
mean loss: 130.01
 ---- batch: 070 ----
mean loss: 140.34
 ---- batch: 080 ----
mean loss: 138.92
 ---- batch: 090 ----
mean loss: 138.28
train mean loss: 134.92
epoch train time: 0:00:01.742600
elapsed time: 0:04:49.529463
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 19:27:25.628517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.69
 ---- batch: 020 ----
mean loss: 127.93
 ---- batch: 030 ----
mean loss: 130.73
 ---- batch: 040 ----
mean loss: 136.33
 ---- batch: 050 ----
mean loss: 136.27
 ---- batch: 060 ----
mean loss: 139.27
 ---- batch: 070 ----
mean loss: 140.42
 ---- batch: 080 ----
mean loss: 130.42
 ---- batch: 090 ----
mean loss: 140.68
train mean loss: 134.47
epoch train time: 0:00:01.704209
elapsed time: 0:04:51.234193
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 19:27:27.333357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.16
 ---- batch: 020 ----
mean loss: 136.64
 ---- batch: 030 ----
mean loss: 128.02
 ---- batch: 040 ----
mean loss: 134.96
 ---- batch: 050 ----
mean loss: 128.28
 ---- batch: 060 ----
mean loss: 137.77
 ---- batch: 070 ----
mean loss: 126.20
 ---- batch: 080 ----
mean loss: 140.81
 ---- batch: 090 ----
mean loss: 133.90
train mean loss: 133.40
epoch train time: 0:00:01.706872
elapsed time: 0:04:52.941710
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 19:27:29.040889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.02
 ---- batch: 020 ----
mean loss: 130.17
 ---- batch: 030 ----
mean loss: 126.02
 ---- batch: 040 ----
mean loss: 128.84
 ---- batch: 050 ----
mean loss: 136.33
 ---- batch: 060 ----
mean loss: 138.34
 ---- batch: 070 ----
mean loss: 131.45
 ---- batch: 080 ----
mean loss: 136.98
 ---- batch: 090 ----
mean loss: 136.76
train mean loss: 132.84
epoch train time: 0:00:01.727912
elapsed time: 0:04:54.670297
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 19:27:30.769476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.99
 ---- batch: 020 ----
mean loss: 133.49
 ---- batch: 030 ----
mean loss: 131.18
 ---- batch: 040 ----
mean loss: 133.06
 ---- batch: 050 ----
mean loss: 128.11
 ---- batch: 060 ----
mean loss: 129.39
 ---- batch: 070 ----
mean loss: 129.52
 ---- batch: 080 ----
mean loss: 141.29
 ---- batch: 090 ----
mean loss: 139.16
train mean loss: 132.30
epoch train time: 0:00:01.682013
elapsed time: 0:04:56.352963
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 19:27:32.452125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.39
 ---- batch: 020 ----
mean loss: 124.86
 ---- batch: 030 ----
mean loss: 134.06
 ---- batch: 040 ----
mean loss: 135.21
 ---- batch: 050 ----
mean loss: 136.99
 ---- batch: 060 ----
mean loss: 131.28
 ---- batch: 070 ----
mean loss: 133.81
 ---- batch: 080 ----
mean loss: 126.59
 ---- batch: 090 ----
mean loss: 132.73
train mean loss: 132.19
epoch train time: 0:00:01.723387
elapsed time: 0:04:58.076999
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 19:27:34.176224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.31
 ---- batch: 020 ----
mean loss: 128.98
 ---- batch: 030 ----
mean loss: 123.67
 ---- batch: 040 ----
mean loss: 131.33
 ---- batch: 050 ----
mean loss: 137.71
 ---- batch: 060 ----
mean loss: 135.07
 ---- batch: 070 ----
mean loss: 134.54
 ---- batch: 080 ----
mean loss: 132.53
 ---- batch: 090 ----
mean loss: 134.16
train mean loss: 132.02
epoch train time: 0:00:01.700740
elapsed time: 0:04:59.778455
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 19:27:35.877644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.03
 ---- batch: 020 ----
mean loss: 129.02
 ---- batch: 030 ----
mean loss: 129.37
 ---- batch: 040 ----
mean loss: 128.97
 ---- batch: 050 ----
mean loss: 128.06
 ---- batch: 060 ----
mean loss: 133.82
 ---- batch: 070 ----
mean loss: 131.77
 ---- batch: 080 ----
mean loss: 132.44
 ---- batch: 090 ----
mean loss: 133.00
train mean loss: 130.81
epoch train time: 0:00:01.717896
elapsed time: 0:05:01.497019
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 19:27:37.596195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.89
 ---- batch: 020 ----
mean loss: 123.62
 ---- batch: 030 ----
mean loss: 135.12
 ---- batch: 040 ----
mean loss: 131.92
 ---- batch: 050 ----
mean loss: 126.95
 ---- batch: 060 ----
mean loss: 129.45
 ---- batch: 070 ----
mean loss: 134.31
 ---- batch: 080 ----
mean loss: 141.77
 ---- batch: 090 ----
mean loss: 131.92
train mean loss: 130.98
epoch train time: 0:00:01.710701
elapsed time: 0:05:03.208436
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 19:27:39.307616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.09
 ---- batch: 020 ----
mean loss: 133.38
 ---- batch: 030 ----
mean loss: 125.55
 ---- batch: 040 ----
mean loss: 130.96
 ---- batch: 050 ----
mean loss: 130.83
 ---- batch: 060 ----
mean loss: 133.53
 ---- batch: 070 ----
mean loss: 125.48
 ---- batch: 080 ----
mean loss: 131.29
 ---- batch: 090 ----
mean loss: 130.88
train mean loss: 129.79
epoch train time: 0:00:01.686629
elapsed time: 0:05:04.895724
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 19:27:40.994882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.34
 ---- batch: 020 ----
mean loss: 127.42
 ---- batch: 030 ----
mean loss: 128.99
 ---- batch: 040 ----
mean loss: 127.19
 ---- batch: 050 ----
mean loss: 128.73
 ---- batch: 060 ----
mean loss: 126.68
 ---- batch: 070 ----
mean loss: 131.73
 ---- batch: 080 ----
mean loss: 132.22
 ---- batch: 090 ----
mean loss: 129.54
train mean loss: 129.69
epoch train time: 0:00:01.718078
elapsed time: 0:05:06.614416
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 19:27:42.713566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.64
 ---- batch: 020 ----
mean loss: 119.76
 ---- batch: 030 ----
mean loss: 122.43
 ---- batch: 040 ----
mean loss: 136.88
 ---- batch: 050 ----
mean loss: 137.75
 ---- batch: 060 ----
mean loss: 129.64
 ---- batch: 070 ----
mean loss: 128.92
 ---- batch: 080 ----
mean loss: 130.05
 ---- batch: 090 ----
mean loss: 131.07
train mean loss: 128.74
epoch train time: 0:00:01.762113
elapsed time: 0:05:08.377128
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 19:27:44.476284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.80
 ---- batch: 020 ----
mean loss: 123.08
 ---- batch: 030 ----
mean loss: 124.29
 ---- batch: 040 ----
mean loss: 130.06
 ---- batch: 050 ----
mean loss: 125.59
 ---- batch: 060 ----
mean loss: 132.12
 ---- batch: 070 ----
mean loss: 132.76
 ---- batch: 080 ----
mean loss: 132.77
 ---- batch: 090 ----
mean loss: 128.15
train mean loss: 128.10
epoch train time: 0:00:01.762960
elapsed time: 0:05:10.140704
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 19:27:46.239875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.69
 ---- batch: 020 ----
mean loss: 123.46
 ---- batch: 030 ----
mean loss: 125.82
 ---- batch: 040 ----
mean loss: 124.71
 ---- batch: 050 ----
mean loss: 121.39
 ---- batch: 060 ----
mean loss: 124.36
 ---- batch: 070 ----
mean loss: 133.60
 ---- batch: 080 ----
mean loss: 129.82
 ---- batch: 090 ----
mean loss: 131.11
train mean loss: 127.29
epoch train time: 0:00:01.728878
elapsed time: 0:05:11.870281
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 19:27:47.969474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.31
 ---- batch: 020 ----
mean loss: 131.02
 ---- batch: 030 ----
mean loss: 130.98
 ---- batch: 040 ----
mean loss: 128.15
 ---- batch: 050 ----
mean loss: 119.30
 ---- batch: 060 ----
mean loss: 129.18
 ---- batch: 070 ----
mean loss: 133.04
 ---- batch: 080 ----
mean loss: 130.54
 ---- batch: 090 ----
mean loss: 126.32
train mean loss: 127.49
epoch train time: 0:00:01.709199
elapsed time: 0:05:13.580118
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 19:27:49.679280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.11
 ---- batch: 020 ----
mean loss: 121.20
 ---- batch: 030 ----
mean loss: 123.54
 ---- batch: 040 ----
mean loss: 128.70
 ---- batch: 050 ----
mean loss: 126.57
 ---- batch: 060 ----
mean loss: 132.13
 ---- batch: 070 ----
mean loss: 130.51
 ---- batch: 080 ----
mean loss: 123.06
 ---- batch: 090 ----
mean loss: 130.70
train mean loss: 127.03
epoch train time: 0:00:01.708632
elapsed time: 0:05:15.289371
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 19:27:51.388514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.04
 ---- batch: 020 ----
mean loss: 122.50
 ---- batch: 030 ----
mean loss: 123.78
 ---- batch: 040 ----
mean loss: 117.73
 ---- batch: 050 ----
mean loss: 128.85
 ---- batch: 060 ----
mean loss: 130.53
 ---- batch: 070 ----
mean loss: 130.00
 ---- batch: 080 ----
mean loss: 124.02
 ---- batch: 090 ----
mean loss: 129.79
train mean loss: 126.32
epoch train time: 0:00:01.723162
elapsed time: 0:05:17.013170
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 19:27:53.112325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.70
 ---- batch: 020 ----
mean loss: 123.22
 ---- batch: 030 ----
mean loss: 127.00
 ---- batch: 040 ----
mean loss: 126.68
 ---- batch: 050 ----
mean loss: 127.26
 ---- batch: 060 ----
mean loss: 125.07
 ---- batch: 070 ----
mean loss: 127.27
 ---- batch: 080 ----
mean loss: 126.34
 ---- batch: 090 ----
mean loss: 127.47
train mean loss: 126.12
epoch train time: 0:00:01.750778
elapsed time: 0:05:18.764562
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 19:27:54.863756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.93
 ---- batch: 020 ----
mean loss: 122.92
 ---- batch: 030 ----
mean loss: 123.76
 ---- batch: 040 ----
mean loss: 123.96
 ---- batch: 050 ----
mean loss: 123.26
 ---- batch: 060 ----
mean loss: 123.39
 ---- batch: 070 ----
mean loss: 124.60
 ---- batch: 080 ----
mean loss: 129.87
 ---- batch: 090 ----
mean loss: 131.31
train mean loss: 125.54
epoch train time: 0:00:01.707162
elapsed time: 0:05:20.472400
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 19:27:56.571556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.07
 ---- batch: 020 ----
mean loss: 123.89
 ---- batch: 030 ----
mean loss: 128.65
 ---- batch: 040 ----
mean loss: 120.23
 ---- batch: 050 ----
mean loss: 124.60
 ---- batch: 060 ----
mean loss: 121.55
 ---- batch: 070 ----
mean loss: 127.49
 ---- batch: 080 ----
mean loss: 125.32
 ---- batch: 090 ----
mean loss: 125.34
train mean loss: 124.52
epoch train time: 0:00:01.708666
elapsed time: 0:05:22.181689
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 19:27:58.280839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.46
 ---- batch: 020 ----
mean loss: 125.50
 ---- batch: 030 ----
mean loss: 118.44
 ---- batch: 040 ----
mean loss: 121.73
 ---- batch: 050 ----
mean loss: 124.47
 ---- batch: 060 ----
mean loss: 128.47
 ---- batch: 070 ----
mean loss: 124.89
 ---- batch: 080 ----
mean loss: 130.39
 ---- batch: 090 ----
mean loss: 128.84
train mean loss: 124.58
epoch train time: 0:00:01.714535
elapsed time: 0:05:23.896839
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 19:27:59.996011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.44
 ---- batch: 020 ----
mean loss: 118.15
 ---- batch: 030 ----
mean loss: 120.39
 ---- batch: 040 ----
mean loss: 124.26
 ---- batch: 050 ----
mean loss: 117.71
 ---- batch: 060 ----
mean loss: 130.86
 ---- batch: 070 ----
mean loss: 125.21
 ---- batch: 080 ----
mean loss: 125.86
 ---- batch: 090 ----
mean loss: 127.90
train mean loss: 123.67
epoch train time: 0:00:01.715058
elapsed time: 0:05:25.612578
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 19:28:01.711729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.96
 ---- batch: 020 ----
mean loss: 122.83
 ---- batch: 030 ----
mean loss: 121.35
 ---- batch: 040 ----
mean loss: 120.97
 ---- batch: 050 ----
mean loss: 127.80
 ---- batch: 060 ----
mean loss: 125.09
 ---- batch: 070 ----
mean loss: 126.37
 ---- batch: 080 ----
mean loss: 123.24
 ---- batch: 090 ----
mean loss: 126.18
train mean loss: 123.59
epoch train time: 0:00:01.687546
elapsed time: 0:05:27.300745
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 19:28:03.399889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.59
 ---- batch: 020 ----
mean loss: 119.95
 ---- batch: 030 ----
mean loss: 120.48
 ---- batch: 040 ----
mean loss: 122.02
 ---- batch: 050 ----
mean loss: 118.39
 ---- batch: 060 ----
mean loss: 124.43
 ---- batch: 070 ----
mean loss: 124.57
 ---- batch: 080 ----
mean loss: 124.76
 ---- batch: 090 ----
mean loss: 127.61
train mean loss: 122.64
epoch train time: 0:00:01.728301
elapsed time: 0:05:29.029683
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 19:28:05.128837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.49
 ---- batch: 020 ----
mean loss: 120.99
 ---- batch: 030 ----
mean loss: 125.53
 ---- batch: 040 ----
mean loss: 121.78
 ---- batch: 050 ----
mean loss: 119.71
 ---- batch: 060 ----
mean loss: 119.02
 ---- batch: 070 ----
mean loss: 125.58
 ---- batch: 080 ----
mean loss: 122.90
 ---- batch: 090 ----
mean loss: 121.62
train mean loss: 122.84
epoch train time: 0:00:01.698074
elapsed time: 0:05:30.728523
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 19:28:06.827693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.44
 ---- batch: 020 ----
mean loss: 115.20
 ---- batch: 030 ----
mean loss: 120.23
 ---- batch: 040 ----
mean loss: 118.47
 ---- batch: 050 ----
mean loss: 119.03
 ---- batch: 060 ----
mean loss: 132.20
 ---- batch: 070 ----
mean loss: 120.72
 ---- batch: 080 ----
mean loss: 123.44
 ---- batch: 090 ----
mean loss: 127.64
train mean loss: 121.89
epoch train time: 0:00:01.711269
elapsed time: 0:05:32.440734
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 19:28:08.539647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.56
 ---- batch: 020 ----
mean loss: 124.35
 ---- batch: 030 ----
mean loss: 125.75
 ---- batch: 040 ----
mean loss: 116.62
 ---- batch: 050 ----
mean loss: 120.83
 ---- batch: 060 ----
mean loss: 121.20
 ---- batch: 070 ----
mean loss: 118.36
 ---- batch: 080 ----
mean loss: 120.20
 ---- batch: 090 ----
mean loss: 123.50
train mean loss: 121.09
epoch train time: 0:00:01.711538
elapsed time: 0:05:34.152635
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 19:28:10.251853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.89
 ---- batch: 020 ----
mean loss: 111.02
 ---- batch: 030 ----
mean loss: 123.85
 ---- batch: 040 ----
mean loss: 125.11
 ---- batch: 050 ----
mean loss: 125.00
 ---- batch: 060 ----
mean loss: 125.00
 ---- batch: 070 ----
mean loss: 123.46
 ---- batch: 080 ----
mean loss: 124.85
 ---- batch: 090 ----
mean loss: 116.82
train mean loss: 121.19
epoch train time: 0:00:01.725899
elapsed time: 0:05:35.879288
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 19:28:11.978437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.17
 ---- batch: 020 ----
mean loss: 119.37
 ---- batch: 030 ----
mean loss: 119.67
 ---- batch: 040 ----
mean loss: 121.00
 ---- batch: 050 ----
mean loss: 116.86
 ---- batch: 060 ----
mean loss: 122.14
 ---- batch: 070 ----
mean loss: 123.45
 ---- batch: 080 ----
mean loss: 119.08
 ---- batch: 090 ----
mean loss: 124.12
train mean loss: 120.66
epoch train time: 0:00:01.688758
elapsed time: 0:05:37.568712
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 19:28:13.667915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.37
 ---- batch: 020 ----
mean loss: 111.37
 ---- batch: 030 ----
mean loss: 119.56
 ---- batch: 040 ----
mean loss: 115.91
 ---- batch: 050 ----
mean loss: 117.47
 ---- batch: 060 ----
mean loss: 120.06
 ---- batch: 070 ----
mean loss: 128.29
 ---- batch: 080 ----
mean loss: 116.84
 ---- batch: 090 ----
mean loss: 118.21
train mean loss: 119.32
epoch train time: 0:00:01.711797
elapsed time: 0:05:39.281134
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 19:28:15.380278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.65
 ---- batch: 020 ----
mean loss: 116.43
 ---- batch: 030 ----
mean loss: 114.42
 ---- batch: 040 ----
mean loss: 116.56
 ---- batch: 050 ----
mean loss: 127.26
 ---- batch: 060 ----
mean loss: 123.94
 ---- batch: 070 ----
mean loss: 119.12
 ---- batch: 080 ----
mean loss: 120.30
 ---- batch: 090 ----
mean loss: 126.27
train mean loss: 119.80
epoch train time: 0:00:01.706834
elapsed time: 0:05:40.988534
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 19:28:17.087690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.47
 ---- batch: 020 ----
mean loss: 116.34
 ---- batch: 030 ----
mean loss: 117.62
 ---- batch: 040 ----
mean loss: 118.39
 ---- batch: 050 ----
mean loss: 119.98
 ---- batch: 060 ----
mean loss: 118.87
 ---- batch: 070 ----
mean loss: 118.46
 ---- batch: 080 ----
mean loss: 122.75
 ---- batch: 090 ----
mean loss: 121.44
train mean loss: 118.97
epoch train time: 0:00:01.746429
elapsed time: 0:05:42.735645
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 19:28:18.834806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.36
 ---- batch: 020 ----
mean loss: 119.76
 ---- batch: 030 ----
mean loss: 108.62
 ---- batch: 040 ----
mean loss: 117.68
 ---- batch: 050 ----
mean loss: 120.64
 ---- batch: 060 ----
mean loss: 116.38
 ---- batch: 070 ----
mean loss: 117.94
 ---- batch: 080 ----
mean loss: 122.38
 ---- batch: 090 ----
mean loss: 124.96
train mean loss: 118.49
epoch train time: 0:00:01.724984
elapsed time: 0:05:44.461298
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 19:28:20.560457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.48
 ---- batch: 020 ----
mean loss: 115.60
 ---- batch: 030 ----
mean loss: 113.33
 ---- batch: 040 ----
mean loss: 118.79
 ---- batch: 050 ----
mean loss: 115.87
 ---- batch: 060 ----
mean loss: 120.60
 ---- batch: 070 ----
mean loss: 119.50
 ---- batch: 080 ----
mean loss: 121.90
 ---- batch: 090 ----
mean loss: 118.81
train mean loss: 118.22
epoch train time: 0:00:01.742616
elapsed time: 0:05:46.204540
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 19:28:22.303697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.64
 ---- batch: 020 ----
mean loss: 114.40
 ---- batch: 030 ----
mean loss: 115.91
 ---- batch: 040 ----
mean loss: 116.01
 ---- batch: 050 ----
mean loss: 118.93
 ---- batch: 060 ----
mean loss: 114.81
 ---- batch: 070 ----
mean loss: 116.09
 ---- batch: 080 ----
mean loss: 122.52
 ---- batch: 090 ----
mean loss: 119.14
train mean loss: 117.72
epoch train time: 0:00:01.724383
elapsed time: 0:05:47.929535
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 19:28:24.028683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.12
 ---- batch: 020 ----
mean loss: 114.01
 ---- batch: 030 ----
mean loss: 116.46
 ---- batch: 040 ----
mean loss: 119.30
 ---- batch: 050 ----
mean loss: 118.87
 ---- batch: 060 ----
mean loss: 119.38
 ---- batch: 070 ----
mean loss: 118.17
 ---- batch: 080 ----
mean loss: 117.29
 ---- batch: 090 ----
mean loss: 115.47
train mean loss: 117.71
epoch train time: 0:00:01.690794
elapsed time: 0:05:49.620976
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 19:28:25.720233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.75
 ---- batch: 020 ----
mean loss: 118.08
 ---- batch: 030 ----
mean loss: 110.18
 ---- batch: 040 ----
mean loss: 118.06
 ---- batch: 050 ----
mean loss: 116.69
 ---- batch: 060 ----
mean loss: 123.67
 ---- batch: 070 ----
mean loss: 114.72
 ---- batch: 080 ----
mean loss: 114.77
 ---- batch: 090 ----
mean loss: 119.28
train mean loss: 117.03
epoch train time: 0:00:01.695327
elapsed time: 0:05:51.317047
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 19:28:27.416208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.86
 ---- batch: 020 ----
mean loss: 112.50
 ---- batch: 030 ----
mean loss: 111.44
 ---- batch: 040 ----
mean loss: 115.78
 ---- batch: 050 ----
mean loss: 121.83
 ---- batch: 060 ----
mean loss: 117.91
 ---- batch: 070 ----
mean loss: 120.75
 ---- batch: 080 ----
mean loss: 112.88
 ---- batch: 090 ----
mean loss: 120.71
train mean loss: 116.61
epoch train time: 0:00:01.736823
elapsed time: 0:05:53.054516
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 19:28:29.153707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.59
 ---- batch: 020 ----
mean loss: 109.30
 ---- batch: 030 ----
mean loss: 115.22
 ---- batch: 040 ----
mean loss: 119.04
 ---- batch: 050 ----
mean loss: 113.31
 ---- batch: 060 ----
mean loss: 120.76
 ---- batch: 070 ----
mean loss: 122.96
 ---- batch: 080 ----
mean loss: 110.98
 ---- batch: 090 ----
mean loss: 118.52
train mean loss: 116.04
epoch train time: 0:00:01.753909
elapsed time: 0:05:54.809084
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 19:28:30.908248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.70
 ---- batch: 020 ----
mean loss: 113.84
 ---- batch: 030 ----
mean loss: 114.38
 ---- batch: 040 ----
mean loss: 115.28
 ---- batch: 050 ----
mean loss: 112.44
 ---- batch: 060 ----
mean loss: 118.70
 ---- batch: 070 ----
mean loss: 112.39
 ---- batch: 080 ----
mean loss: 118.53
 ---- batch: 090 ----
mean loss: 119.95
train mean loss: 115.57
epoch train time: 0:00:01.713909
elapsed time: 0:05:56.523723
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 19:28:32.622875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.23
 ---- batch: 020 ----
mean loss: 111.11
 ---- batch: 030 ----
mean loss: 108.40
 ---- batch: 040 ----
mean loss: 113.76
 ---- batch: 050 ----
mean loss: 121.05
 ---- batch: 060 ----
mean loss: 115.14
 ---- batch: 070 ----
mean loss: 118.57
 ---- batch: 080 ----
mean loss: 123.10
 ---- batch: 090 ----
mean loss: 114.68
train mean loss: 115.38
epoch train time: 0:00:01.736586
elapsed time: 0:05:58.261066
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 19:28:34.360307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.25
 ---- batch: 020 ----
mean loss: 109.85
 ---- batch: 030 ----
mean loss: 116.09
 ---- batch: 040 ----
mean loss: 113.26
 ---- batch: 050 ----
mean loss: 112.85
 ---- batch: 060 ----
mean loss: 111.76
 ---- batch: 070 ----
mean loss: 116.92
 ---- batch: 080 ----
mean loss: 118.39
 ---- batch: 090 ----
mean loss: 123.31
train mean loss: 115.02
epoch train time: 0:00:01.700340
elapsed time: 0:05:59.962117
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 19:28:36.061278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.39
 ---- batch: 020 ----
mean loss: 114.33
 ---- batch: 030 ----
mean loss: 113.90
 ---- batch: 040 ----
mean loss: 114.94
 ---- batch: 050 ----
mean loss: 117.76
 ---- batch: 060 ----
mean loss: 113.52
 ---- batch: 070 ----
mean loss: 115.27
 ---- batch: 080 ----
mean loss: 117.87
 ---- batch: 090 ----
mean loss: 114.89
train mean loss: 114.39
epoch train time: 0:00:01.701183
elapsed time: 0:06:01.664033
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 19:28:37.763276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.69
 ---- batch: 020 ----
mean loss: 110.61
 ---- batch: 030 ----
mean loss: 114.61
 ---- batch: 040 ----
mean loss: 113.72
 ---- batch: 050 ----
mean loss: 111.02
 ---- batch: 060 ----
mean loss: 116.31
 ---- batch: 070 ----
mean loss: 119.94
 ---- batch: 080 ----
mean loss: 114.19
 ---- batch: 090 ----
mean loss: 115.39
train mean loss: 113.81
epoch train time: 0:00:01.732283
elapsed time: 0:06:03.397047
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 19:28:39.496230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.81
 ---- batch: 020 ----
mean loss: 112.94
 ---- batch: 030 ----
mean loss: 112.86
 ---- batch: 040 ----
mean loss: 113.74
 ---- batch: 050 ----
mean loss: 114.35
 ---- batch: 060 ----
mean loss: 119.69
 ---- batch: 070 ----
mean loss: 110.94
 ---- batch: 080 ----
mean loss: 110.70
 ---- batch: 090 ----
mean loss: 114.35
train mean loss: 113.38
epoch train time: 0:00:01.719333
elapsed time: 0:06:05.117049
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 19:28:41.216210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.34
 ---- batch: 020 ----
mean loss: 110.89
 ---- batch: 030 ----
mean loss: 107.46
 ---- batch: 040 ----
mean loss: 111.72
 ---- batch: 050 ----
mean loss: 113.65
 ---- batch: 060 ----
mean loss: 108.62
 ---- batch: 070 ----
mean loss: 114.31
 ---- batch: 080 ----
mean loss: 116.43
 ---- batch: 090 ----
mean loss: 124.12
train mean loss: 113.63
epoch train time: 0:00:01.666445
elapsed time: 0:06:06.784211
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 19:28:42.883356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.35
 ---- batch: 020 ----
mean loss: 113.34
 ---- batch: 030 ----
mean loss: 111.20
 ---- batch: 040 ----
mean loss: 113.92
 ---- batch: 050 ----
mean loss: 111.30
 ---- batch: 060 ----
mean loss: 114.31
 ---- batch: 070 ----
mean loss: 113.56
 ---- batch: 080 ----
mean loss: 118.17
 ---- batch: 090 ----
mean loss: 112.89
train mean loss: 113.55
epoch train time: 0:00:01.677298
elapsed time: 0:06:08.462133
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 19:28:44.561282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.10
 ---- batch: 020 ----
mean loss: 110.64
 ---- batch: 030 ----
mean loss: 115.26
 ---- batch: 040 ----
mean loss: 111.45
 ---- batch: 050 ----
mean loss: 112.61
 ---- batch: 060 ----
mean loss: 111.57
 ---- batch: 070 ----
mean loss: 114.14
 ---- batch: 080 ----
mean loss: 114.69
 ---- batch: 090 ----
mean loss: 115.44
train mean loss: 112.58
epoch train time: 0:00:01.708134
elapsed time: 0:06:10.170933
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 19:28:46.270093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.85
 ---- batch: 020 ----
mean loss: 107.20
 ---- batch: 030 ----
mean loss: 110.79
 ---- batch: 040 ----
mean loss: 110.92
 ---- batch: 050 ----
mean loss: 108.50
 ---- batch: 060 ----
mean loss: 113.37
 ---- batch: 070 ----
mean loss: 114.45
 ---- batch: 080 ----
mean loss: 109.37
 ---- batch: 090 ----
mean loss: 119.79
train mean loss: 112.54
epoch train time: 0:00:01.691363
elapsed time: 0:06:11.862941
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 19:28:47.962092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.79
 ---- batch: 020 ----
mean loss: 109.37
 ---- batch: 030 ----
mean loss: 111.63
 ---- batch: 040 ----
mean loss: 110.81
 ---- batch: 050 ----
mean loss: 110.25
 ---- batch: 060 ----
mean loss: 110.66
 ---- batch: 070 ----
mean loss: 114.52
 ---- batch: 080 ----
mean loss: 116.94
 ---- batch: 090 ----
mean loss: 113.32
train mean loss: 112.25
epoch train time: 0:00:01.706127
elapsed time: 0:06:13.569719
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 19:28:49.668881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.12
 ---- batch: 020 ----
mean loss: 106.30
 ---- batch: 030 ----
mean loss: 112.06
 ---- batch: 040 ----
mean loss: 111.24
 ---- batch: 050 ----
mean loss: 114.61
 ---- batch: 060 ----
mean loss: 112.68
 ---- batch: 070 ----
mean loss: 109.20
 ---- batch: 080 ----
mean loss: 115.86
 ---- batch: 090 ----
mean loss: 118.46
train mean loss: 111.19
epoch train time: 0:00:01.727220
elapsed time: 0:06:15.297546
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 19:28:51.396740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.06
 ---- batch: 020 ----
mean loss: 103.94
 ---- batch: 030 ----
mean loss: 106.40
 ---- batch: 040 ----
mean loss: 109.75
 ---- batch: 050 ----
mean loss: 115.22
 ---- batch: 060 ----
mean loss: 112.34
 ---- batch: 070 ----
mean loss: 109.58
 ---- batch: 080 ----
mean loss: 118.02
 ---- batch: 090 ----
mean loss: 114.77
train mean loss: 111.66
epoch train time: 0:00:01.711381
elapsed time: 0:06:17.009602
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 19:28:53.108766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.46
 ---- batch: 020 ----
mean loss: 109.61
 ---- batch: 030 ----
mean loss: 104.19
 ---- batch: 040 ----
mean loss: 110.52
 ---- batch: 050 ----
mean loss: 109.74
 ---- batch: 060 ----
mean loss: 109.21
 ---- batch: 070 ----
mean loss: 117.62
 ---- batch: 080 ----
mean loss: 113.67
 ---- batch: 090 ----
mean loss: 117.85
train mean loss: 110.74
epoch train time: 0:00:01.737942
elapsed time: 0:06:18.748320
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 19:28:54.847486
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.20
 ---- batch: 020 ----
mean loss: 108.83
 ---- batch: 030 ----
mean loss: 101.81
 ---- batch: 040 ----
mean loss: 107.96
 ---- batch: 050 ----
mean loss: 102.83
 ---- batch: 060 ----
mean loss: 102.85
 ---- batch: 070 ----
mean loss: 103.03
 ---- batch: 080 ----
mean loss: 105.80
 ---- batch: 090 ----
mean loss: 105.57
train mean loss: 105.05
epoch train time: 0:00:01.703443
elapsed time: 0:06:20.452691
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 19:28:56.551607
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.59
 ---- batch: 020 ----
mean loss: 99.31
 ---- batch: 030 ----
mean loss: 107.15
 ---- batch: 040 ----
mean loss: 99.41
 ---- batch: 050 ----
mean loss: 100.74
 ---- batch: 060 ----
mean loss: 108.03
 ---- batch: 070 ----
mean loss: 100.59
 ---- batch: 080 ----
mean loss: 108.86
 ---- batch: 090 ----
mean loss: 105.23
train mean loss: 104.43
epoch train time: 0:00:01.713591
elapsed time: 0:06:22.166872
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 19:28:58.266081
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.11
 ---- batch: 020 ----
mean loss: 104.74
 ---- batch: 030 ----
mean loss: 101.74
 ---- batch: 040 ----
mean loss: 109.44
 ---- batch: 050 ----
mean loss: 103.62
 ---- batch: 060 ----
mean loss: 100.37
 ---- batch: 070 ----
mean loss: 103.49
 ---- batch: 080 ----
mean loss: 105.45
 ---- batch: 090 ----
mean loss: 103.15
train mean loss: 104.18
epoch train time: 0:00:01.697303
elapsed time: 0:06:23.864899
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 19:28:59.964061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.92
 ---- batch: 020 ----
mean loss: 103.65
 ---- batch: 030 ----
mean loss: 106.53
 ---- batch: 040 ----
mean loss: 99.69
 ---- batch: 050 ----
mean loss: 103.77
 ---- batch: 060 ----
mean loss: 103.60
 ---- batch: 070 ----
mean loss: 100.15
 ---- batch: 080 ----
mean loss: 108.58
 ---- batch: 090 ----
mean loss: 102.87
train mean loss: 104.05
epoch train time: 0:00:01.680120
elapsed time: 0:06:25.545723
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 19:29:01.644878
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.82
 ---- batch: 020 ----
mean loss: 106.56
 ---- batch: 030 ----
mean loss: 100.15
 ---- batch: 040 ----
mean loss: 96.13
 ---- batch: 050 ----
mean loss: 108.06
 ---- batch: 060 ----
mean loss: 104.72
 ---- batch: 070 ----
mean loss: 103.84
 ---- batch: 080 ----
mean loss: 110.72
 ---- batch: 090 ----
mean loss: 102.33
train mean loss: 104.01
epoch train time: 0:00:01.687668
elapsed time: 0:06:27.234058
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 19:29:03.333208
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.19
 ---- batch: 020 ----
mean loss: 100.20
 ---- batch: 030 ----
mean loss: 103.48
 ---- batch: 040 ----
mean loss: 103.36
 ---- batch: 050 ----
mean loss: 103.80
 ---- batch: 060 ----
mean loss: 106.82
 ---- batch: 070 ----
mean loss: 106.86
 ---- batch: 080 ----
mean loss: 103.66
 ---- batch: 090 ----
mean loss: 105.78
train mean loss: 103.87
epoch train time: 0:00:01.717008
elapsed time: 0:06:28.951727
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 19:29:05.050997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.97
 ---- batch: 020 ----
mean loss: 102.82
 ---- batch: 030 ----
mean loss: 101.23
 ---- batch: 040 ----
mean loss: 107.41
 ---- batch: 050 ----
mean loss: 102.96
 ---- batch: 060 ----
mean loss: 101.11
 ---- batch: 070 ----
mean loss: 104.30
 ---- batch: 080 ----
mean loss: 103.41
 ---- batch: 090 ----
mean loss: 107.22
train mean loss: 103.70
epoch train time: 0:00:01.754146
elapsed time: 0:06:30.706735
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 19:29:06.806002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.63
 ---- batch: 020 ----
mean loss: 108.60
 ---- batch: 030 ----
mean loss: 105.60
 ---- batch: 040 ----
mean loss: 104.61
 ---- batch: 050 ----
mean loss: 103.94
 ---- batch: 060 ----
mean loss: 98.90
 ---- batch: 070 ----
mean loss: 101.75
 ---- batch: 080 ----
mean loss: 104.66
 ---- batch: 090 ----
mean loss: 99.10
train mean loss: 103.68
epoch train time: 0:00:01.722041
elapsed time: 0:06:32.429588
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 19:29:08.528764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.63
 ---- batch: 020 ----
mean loss: 103.65
 ---- batch: 030 ----
mean loss: 103.81
 ---- batch: 040 ----
mean loss: 101.01
 ---- batch: 050 ----
mean loss: 108.11
 ---- batch: 060 ----
mean loss: 102.07
 ---- batch: 070 ----
mean loss: 102.82
 ---- batch: 080 ----
mean loss: 103.52
 ---- batch: 090 ----
mean loss: 106.31
train mean loss: 103.81
epoch train time: 0:00:01.711047
elapsed time: 0:06:34.141333
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 19:29:10.240487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.56
 ---- batch: 020 ----
mean loss: 103.11
 ---- batch: 030 ----
mean loss: 108.14
 ---- batch: 040 ----
mean loss: 104.65
 ---- batch: 050 ----
mean loss: 103.44
 ---- batch: 060 ----
mean loss: 104.13
 ---- batch: 070 ----
mean loss: 101.09
 ---- batch: 080 ----
mean loss: 106.91
 ---- batch: 090 ----
mean loss: 101.99
train mean loss: 103.60
epoch train time: 0:00:01.729243
elapsed time: 0:06:35.871254
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 19:29:11.970425
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.91
 ---- batch: 020 ----
mean loss: 101.43
 ---- batch: 030 ----
mean loss: 104.01
 ---- batch: 040 ----
mean loss: 103.86
 ---- batch: 050 ----
mean loss: 104.96
 ---- batch: 060 ----
mean loss: 99.99
 ---- batch: 070 ----
mean loss: 101.59
 ---- batch: 080 ----
mean loss: 104.97
 ---- batch: 090 ----
mean loss: 105.49
train mean loss: 103.41
epoch train time: 0:00:01.746274
elapsed time: 0:06:37.618222
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 19:29:13.717382
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.98
 ---- batch: 020 ----
mean loss: 96.71
 ---- batch: 030 ----
mean loss: 104.34
 ---- batch: 040 ----
mean loss: 110.06
 ---- batch: 050 ----
mean loss: 103.11
 ---- batch: 060 ----
mean loss: 100.46
 ---- batch: 070 ----
mean loss: 103.51
 ---- batch: 080 ----
mean loss: 106.33
 ---- batch: 090 ----
mean loss: 101.66
train mean loss: 103.58
epoch train time: 0:00:01.691862
elapsed time: 0:06:39.310826
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 19:29:15.410002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.10
 ---- batch: 020 ----
mean loss: 103.28
 ---- batch: 030 ----
mean loss: 98.50
 ---- batch: 040 ----
mean loss: 106.64
 ---- batch: 050 ----
mean loss: 110.22
 ---- batch: 060 ----
mean loss: 106.93
 ---- batch: 070 ----
mean loss: 102.00
 ---- batch: 080 ----
mean loss: 100.83
 ---- batch: 090 ----
mean loss: 99.06
train mean loss: 103.54
epoch train time: 0:00:01.706806
elapsed time: 0:06:41.018303
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 19:29:17.117485
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.19
 ---- batch: 020 ----
mean loss: 96.34
 ---- batch: 030 ----
mean loss: 102.46
 ---- batch: 040 ----
mean loss: 103.59
 ---- batch: 050 ----
mean loss: 105.27
 ---- batch: 060 ----
mean loss: 102.15
 ---- batch: 070 ----
mean loss: 108.28
 ---- batch: 080 ----
mean loss: 105.18
 ---- batch: 090 ----
mean loss: 108.49
train mean loss: 103.53
epoch train time: 0:00:01.674189
elapsed time: 0:06:42.693203
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 19:29:18.792357
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.44
 ---- batch: 020 ----
mean loss: 101.37
 ---- batch: 030 ----
mean loss: 100.14
 ---- batch: 040 ----
mean loss: 101.91
 ---- batch: 050 ----
mean loss: 99.94
 ---- batch: 060 ----
mean loss: 103.59
 ---- batch: 070 ----
mean loss: 106.52
 ---- batch: 080 ----
mean loss: 106.96
 ---- batch: 090 ----
mean loss: 102.88
train mean loss: 103.47
epoch train time: 0:00:01.712737
elapsed time: 0:06:44.406666
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 19:29:20.505839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.63
 ---- batch: 020 ----
mean loss: 102.06
 ---- batch: 030 ----
mean loss: 105.24
 ---- batch: 040 ----
mean loss: 103.42
 ---- batch: 050 ----
mean loss: 102.44
 ---- batch: 060 ----
mean loss: 100.70
 ---- batch: 070 ----
mean loss: 101.91
 ---- batch: 080 ----
mean loss: 104.59
 ---- batch: 090 ----
mean loss: 102.01
train mean loss: 103.32
epoch train time: 0:00:01.720763
elapsed time: 0:06:46.128115
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 19:29:22.227281
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.07
 ---- batch: 020 ----
mean loss: 107.77
 ---- batch: 030 ----
mean loss: 105.32
 ---- batch: 040 ----
mean loss: 99.98
 ---- batch: 050 ----
mean loss: 102.81
 ---- batch: 060 ----
mean loss: 102.13
 ---- batch: 070 ----
mean loss: 100.64
 ---- batch: 080 ----
mean loss: 105.50
 ---- batch: 090 ----
mean loss: 105.01
train mean loss: 103.23
epoch train time: 0:00:01.713125
elapsed time: 0:06:47.841965
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 19:29:23.941117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.05
 ---- batch: 020 ----
mean loss: 99.26
 ---- batch: 030 ----
mean loss: 102.99
 ---- batch: 040 ----
mean loss: 98.53
 ---- batch: 050 ----
mean loss: 100.80
 ---- batch: 060 ----
mean loss: 106.92
 ---- batch: 070 ----
mean loss: 101.67
 ---- batch: 080 ----
mean loss: 108.27
 ---- batch: 090 ----
mean loss: 113.31
train mean loss: 103.28
epoch train time: 0:00:01.728044
elapsed time: 0:06:49.570644
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 19:29:25.669832
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.27
 ---- batch: 020 ----
mean loss: 101.15
 ---- batch: 030 ----
mean loss: 102.71
 ---- batch: 040 ----
mean loss: 107.65
 ---- batch: 050 ----
mean loss: 106.98
 ---- batch: 060 ----
mean loss: 97.70
 ---- batch: 070 ----
mean loss: 99.25
 ---- batch: 080 ----
mean loss: 105.55
 ---- batch: 090 ----
mean loss: 106.70
train mean loss: 103.24
epoch train time: 0:00:01.764801
elapsed time: 0:06:51.336100
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 19:29:27.435292
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.15
 ---- batch: 020 ----
mean loss: 101.17
 ---- batch: 030 ----
mean loss: 101.10
 ---- batch: 040 ----
mean loss: 104.58
 ---- batch: 050 ----
mean loss: 103.33
 ---- batch: 060 ----
mean loss: 103.01
 ---- batch: 070 ----
mean loss: 99.76
 ---- batch: 080 ----
mean loss: 111.77
 ---- batch: 090 ----
mean loss: 101.13
train mean loss: 102.97
epoch train time: 0:00:01.697845
elapsed time: 0:06:53.034612
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 19:29:29.133784
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.27
 ---- batch: 020 ----
mean loss: 100.90
 ---- batch: 030 ----
mean loss: 104.30
 ---- batch: 040 ----
mean loss: 101.62
 ---- batch: 050 ----
mean loss: 106.21
 ---- batch: 060 ----
mean loss: 106.80
 ---- batch: 070 ----
mean loss: 100.96
 ---- batch: 080 ----
mean loss: 105.99
 ---- batch: 090 ----
mean loss: 101.88
train mean loss: 103.12
epoch train time: 0:00:01.736068
elapsed time: 0:06:54.771317
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 19:29:30.870506
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.35
 ---- batch: 020 ----
mean loss: 103.55
 ---- batch: 030 ----
mean loss: 102.73
 ---- batch: 040 ----
mean loss: 99.66
 ---- batch: 050 ----
mean loss: 96.39
 ---- batch: 060 ----
mean loss: 106.55
 ---- batch: 070 ----
mean loss: 104.14
 ---- batch: 080 ----
mean loss: 108.46
 ---- batch: 090 ----
mean loss: 102.86
train mean loss: 103.23
epoch train time: 0:00:01.692347
elapsed time: 0:06:56.464305
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 19:29:32.563280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.39
 ---- batch: 020 ----
mean loss: 107.74
 ---- batch: 030 ----
mean loss: 105.29
 ---- batch: 040 ----
mean loss: 104.66
 ---- batch: 050 ----
mean loss: 102.68
 ---- batch: 060 ----
mean loss: 99.81
 ---- batch: 070 ----
mean loss: 103.55
 ---- batch: 080 ----
mean loss: 98.15
 ---- batch: 090 ----
mean loss: 105.57
train mean loss: 103.11
epoch train time: 0:00:01.683321
elapsed time: 0:06:58.148098
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 19:29:34.247260
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.77
 ---- batch: 020 ----
mean loss: 95.76
 ---- batch: 030 ----
mean loss: 103.21
 ---- batch: 040 ----
mean loss: 101.53
 ---- batch: 050 ----
mean loss: 100.89
 ---- batch: 060 ----
mean loss: 101.24
 ---- batch: 070 ----
mean loss: 102.73
 ---- batch: 080 ----
mean loss: 112.25
 ---- batch: 090 ----
mean loss: 107.90
train mean loss: 103.00
epoch train time: 0:00:01.666417
elapsed time: 0:06:59.815224
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 19:29:35.914387
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.91
 ---- batch: 020 ----
mean loss: 109.81
 ---- batch: 030 ----
mean loss: 100.95
 ---- batch: 040 ----
mean loss: 104.09
 ---- batch: 050 ----
mean loss: 102.49
 ---- batch: 060 ----
mean loss: 96.98
 ---- batch: 070 ----
mean loss: 99.97
 ---- batch: 080 ----
mean loss: 102.63
 ---- batch: 090 ----
mean loss: 105.28
train mean loss: 103.01
epoch train time: 0:00:01.704651
elapsed time: 0:07:01.520529
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 19:29:37.619780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.97
 ---- batch: 020 ----
mean loss: 104.78
 ---- batch: 030 ----
mean loss: 99.86
 ---- batch: 040 ----
mean loss: 100.46
 ---- batch: 050 ----
mean loss: 100.91
 ---- batch: 060 ----
mean loss: 105.76
 ---- batch: 070 ----
mean loss: 98.98
 ---- batch: 080 ----
mean loss: 102.79
 ---- batch: 090 ----
mean loss: 107.49
train mean loss: 102.96
epoch train time: 0:00:01.739343
elapsed time: 0:07:03.260641
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 19:29:39.359888
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.46
 ---- batch: 020 ----
mean loss: 106.00
 ---- batch: 030 ----
mean loss: 104.16
 ---- batch: 040 ----
mean loss: 104.17
 ---- batch: 050 ----
mean loss: 105.44
 ---- batch: 060 ----
mean loss: 102.53
 ---- batch: 070 ----
mean loss: 102.54
 ---- batch: 080 ----
mean loss: 103.22
 ---- batch: 090 ----
mean loss: 97.22
train mean loss: 102.74
epoch train time: 0:00:01.683276
elapsed time: 0:07:04.944656
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 19:29:41.043816
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.21
 ---- batch: 020 ----
mean loss: 103.91
 ---- batch: 030 ----
mean loss: 110.77
 ---- batch: 040 ----
mean loss: 103.44
 ---- batch: 050 ----
mean loss: 101.88
 ---- batch: 060 ----
mean loss: 99.94
 ---- batch: 070 ----
mean loss: 105.28
 ---- batch: 080 ----
mean loss: 97.28
 ---- batch: 090 ----
mean loss: 106.16
train mean loss: 102.65
epoch train time: 0:00:01.698142
elapsed time: 0:07:06.643493
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 19:29:42.742682
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.21
 ---- batch: 020 ----
mean loss: 104.80
 ---- batch: 030 ----
mean loss: 98.19
 ---- batch: 040 ----
mean loss: 104.26
 ---- batch: 050 ----
mean loss: 104.88
 ---- batch: 060 ----
mean loss: 103.99
 ---- batch: 070 ----
mean loss: 104.35
 ---- batch: 080 ----
mean loss: 95.53
 ---- batch: 090 ----
mean loss: 105.85
train mean loss: 102.65
epoch train time: 0:00:01.711387
elapsed time: 0:07:08.355538
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 19:29:44.454753
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.12
 ---- batch: 020 ----
mean loss: 105.43
 ---- batch: 030 ----
mean loss: 104.29
 ---- batch: 040 ----
mean loss: 97.05
 ---- batch: 050 ----
mean loss: 101.50
 ---- batch: 060 ----
mean loss: 100.84
 ---- batch: 070 ----
mean loss: 102.61
 ---- batch: 080 ----
mean loss: 108.11
 ---- batch: 090 ----
mean loss: 98.28
train mean loss: 102.62
epoch train time: 0:00:01.695109
elapsed time: 0:07:10.051363
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 19:29:46.150537
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.29
 ---- batch: 020 ----
mean loss: 103.14
 ---- batch: 030 ----
mean loss: 106.44
 ---- batch: 040 ----
mean loss: 102.26
 ---- batch: 050 ----
mean loss: 99.35
 ---- batch: 060 ----
mean loss: 98.73
 ---- batch: 070 ----
mean loss: 96.39
 ---- batch: 080 ----
mean loss: 102.65
 ---- batch: 090 ----
mean loss: 103.36
train mean loss: 102.61
epoch train time: 0:00:01.722026
elapsed time: 0:07:11.774066
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 19:29:47.873277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.35
 ---- batch: 020 ----
mean loss: 99.27
 ---- batch: 030 ----
mean loss: 102.81
 ---- batch: 040 ----
mean loss: 103.35
 ---- batch: 050 ----
mean loss: 104.03
 ---- batch: 060 ----
mean loss: 104.03
 ---- batch: 070 ----
mean loss: 106.61
 ---- batch: 080 ----
mean loss: 99.82
 ---- batch: 090 ----
mean loss: 99.18
train mean loss: 102.43
epoch train time: 0:00:01.707337
elapsed time: 0:07:13.482208
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 19:29:49.581361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.37
 ---- batch: 020 ----
mean loss: 105.10
 ---- batch: 030 ----
mean loss: 100.51
 ---- batch: 040 ----
mean loss: 102.18
 ---- batch: 050 ----
mean loss: 99.26
 ---- batch: 060 ----
mean loss: 102.62
 ---- batch: 070 ----
mean loss: 98.36
 ---- batch: 080 ----
mean loss: 109.90
 ---- batch: 090 ----
mean loss: 104.36
train mean loss: 102.87
epoch train time: 0:00:01.685144
elapsed time: 0:07:15.168302
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 19:29:51.267205
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.50
 ---- batch: 020 ----
mean loss: 103.79
 ---- batch: 030 ----
mean loss: 99.51
 ---- batch: 040 ----
mean loss: 104.75
 ---- batch: 050 ----
mean loss: 102.51
 ---- batch: 060 ----
mean loss: 100.11
 ---- batch: 070 ----
mean loss: 103.93
 ---- batch: 080 ----
mean loss: 105.63
 ---- batch: 090 ----
mean loss: 104.58
train mean loss: 102.44
epoch train time: 0:00:01.651158
elapsed time: 0:07:16.819894
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 19:29:52.919052
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.27
 ---- batch: 020 ----
mean loss: 104.41
 ---- batch: 030 ----
mean loss: 96.79
 ---- batch: 040 ----
mean loss: 100.01
 ---- batch: 050 ----
mean loss: 106.07
 ---- batch: 060 ----
mean loss: 104.06
 ---- batch: 070 ----
mean loss: 102.66
 ---- batch: 080 ----
mean loss: 105.93
 ---- batch: 090 ----
mean loss: 102.38
train mean loss: 102.25
epoch train time: 0:00:01.668254
elapsed time: 0:07:18.488773
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 19:29:54.587915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.08
 ---- batch: 020 ----
mean loss: 99.97
 ---- batch: 030 ----
mean loss: 104.08
 ---- batch: 040 ----
mean loss: 102.75
 ---- batch: 050 ----
mean loss: 102.31
 ---- batch: 060 ----
mean loss: 100.90
 ---- batch: 070 ----
mean loss: 100.56
 ---- batch: 080 ----
mean loss: 104.45
 ---- batch: 090 ----
mean loss: 103.80
train mean loss: 102.35
epoch train time: 0:00:01.712765
elapsed time: 0:07:20.202155
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 19:29:56.301325
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.61
 ---- batch: 020 ----
mean loss: 98.27
 ---- batch: 030 ----
mean loss: 112.43
 ---- batch: 040 ----
mean loss: 99.07
 ---- batch: 050 ----
mean loss: 99.37
 ---- batch: 060 ----
mean loss: 96.89
 ---- batch: 070 ----
mean loss: 104.01
 ---- batch: 080 ----
mean loss: 105.23
 ---- batch: 090 ----
mean loss: 105.64
train mean loss: 102.51
epoch train time: 0:00:01.733283
elapsed time: 0:07:21.936129
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 19:29:58.035308
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.86
 ---- batch: 020 ----
mean loss: 105.67
 ---- batch: 030 ----
mean loss: 101.07
 ---- batch: 040 ----
mean loss: 97.28
 ---- batch: 050 ----
mean loss: 102.97
 ---- batch: 060 ----
mean loss: 103.47
 ---- batch: 070 ----
mean loss: 104.10
 ---- batch: 080 ----
mean loss: 99.89
 ---- batch: 090 ----
mean loss: 101.10
train mean loss: 102.40
epoch train time: 0:00:01.692518
elapsed time: 0:07:23.629313
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 19:29:59.728453
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.19
 ---- batch: 020 ----
mean loss: 104.24
 ---- batch: 030 ----
mean loss: 103.25
 ---- batch: 040 ----
mean loss: 95.13
 ---- batch: 050 ----
mean loss: 103.74
 ---- batch: 060 ----
mean loss: 100.60
 ---- batch: 070 ----
mean loss: 101.28
 ---- batch: 080 ----
mean loss: 104.80
 ---- batch: 090 ----
mean loss: 104.37
train mean loss: 102.41
epoch train time: 0:00:01.720458
elapsed time: 0:07:25.350392
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 19:30:01.449545
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.09
 ---- batch: 020 ----
mean loss: 104.61
 ---- batch: 030 ----
mean loss: 99.44
 ---- batch: 040 ----
mean loss: 95.14
 ---- batch: 050 ----
mean loss: 102.28
 ---- batch: 060 ----
mean loss: 98.64
 ---- batch: 070 ----
mean loss: 105.19
 ---- batch: 080 ----
mean loss: 103.75
 ---- batch: 090 ----
mean loss: 100.61
train mean loss: 102.38
epoch train time: 0:00:01.739486
elapsed time: 0:07:27.090505
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 19:30:03.189728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.99
 ---- batch: 020 ----
mean loss: 99.92
 ---- batch: 030 ----
mean loss: 107.78
 ---- batch: 040 ----
mean loss: 98.19
 ---- batch: 050 ----
mean loss: 103.44
 ---- batch: 060 ----
mean loss: 100.08
 ---- batch: 070 ----
mean loss: 105.47
 ---- batch: 080 ----
mean loss: 103.40
 ---- batch: 090 ----
mean loss: 97.36
train mean loss: 102.31
epoch train time: 0:00:01.691762
elapsed time: 0:07:28.783009
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 19:30:04.882197
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.30
 ---- batch: 020 ----
mean loss: 99.44
 ---- batch: 030 ----
mean loss: 103.32
 ---- batch: 040 ----
mean loss: 99.25
 ---- batch: 050 ----
mean loss: 97.51
 ---- batch: 060 ----
mean loss: 101.73
 ---- batch: 070 ----
mean loss: 103.84
 ---- batch: 080 ----
mean loss: 103.21
 ---- batch: 090 ----
mean loss: 105.69
train mean loss: 101.97
epoch train time: 0:00:01.708315
elapsed time: 0:07:30.492042
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 19:30:06.591188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.95
 ---- batch: 020 ----
mean loss: 98.62
 ---- batch: 030 ----
mean loss: 102.08
 ---- batch: 040 ----
mean loss: 102.96
 ---- batch: 050 ----
mean loss: 99.35
 ---- batch: 060 ----
mean loss: 104.25
 ---- batch: 070 ----
mean loss: 103.89
 ---- batch: 080 ----
mean loss: 96.60
 ---- batch: 090 ----
mean loss: 107.03
train mean loss: 102.22
epoch train time: 0:00:01.715477
elapsed time: 0:07:32.208139
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 19:30:08.307317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.49
 ---- batch: 020 ----
mean loss: 106.34
 ---- batch: 030 ----
mean loss: 99.41
 ---- batch: 040 ----
mean loss: 97.97
 ---- batch: 050 ----
mean loss: 102.03
 ---- batch: 060 ----
mean loss: 100.33
 ---- batch: 070 ----
mean loss: 104.36
 ---- batch: 080 ----
mean loss: 98.42
 ---- batch: 090 ----
mean loss: 105.93
train mean loss: 102.13
epoch train time: 0:00:01.644392
elapsed time: 0:07:33.853203
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 19:30:09.952400
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.96
 ---- batch: 020 ----
mean loss: 101.82
 ---- batch: 030 ----
mean loss: 96.38
 ---- batch: 040 ----
mean loss: 102.32
 ---- batch: 050 ----
mean loss: 104.39
 ---- batch: 060 ----
mean loss: 103.42
 ---- batch: 070 ----
mean loss: 102.37
 ---- batch: 080 ----
mean loss: 104.69
 ---- batch: 090 ----
mean loss: 104.75
train mean loss: 101.99
epoch train time: 0:00:01.693054
elapsed time: 0:07:35.547000
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 19:30:11.646275
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.17
 ---- batch: 020 ----
mean loss: 97.29
 ---- batch: 030 ----
mean loss: 101.05
 ---- batch: 040 ----
mean loss: 103.47
 ---- batch: 050 ----
mean loss: 98.88
 ---- batch: 060 ----
mean loss: 103.66
 ---- batch: 070 ----
mean loss: 101.18
 ---- batch: 080 ----
mean loss: 98.81
 ---- batch: 090 ----
mean loss: 106.53
train mean loss: 101.99
epoch train time: 0:00:01.692972
elapsed time: 0:07:37.240706
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 19:30:13.339860
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.00
 ---- batch: 020 ----
mean loss: 101.88
 ---- batch: 030 ----
mean loss: 100.85
 ---- batch: 040 ----
mean loss: 104.29
 ---- batch: 050 ----
mean loss: 104.25
 ---- batch: 060 ----
mean loss: 102.90
 ---- batch: 070 ----
mean loss: 97.59
 ---- batch: 080 ----
mean loss: 101.10
 ---- batch: 090 ----
mean loss: 101.56
train mean loss: 101.98
epoch train time: 0:00:01.681192
elapsed time: 0:07:38.922541
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 19:30:15.021738
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.97
 ---- batch: 020 ----
mean loss: 99.84
 ---- batch: 030 ----
mean loss: 99.66
 ---- batch: 040 ----
mean loss: 99.73
 ---- batch: 050 ----
mean loss: 106.31
 ---- batch: 060 ----
mean loss: 105.59
 ---- batch: 070 ----
mean loss: 104.03
 ---- batch: 080 ----
mean loss: 98.50
 ---- batch: 090 ----
mean loss: 102.51
train mean loss: 101.88
epoch train time: 0:00:01.663783
elapsed time: 0:07:40.587151
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 19:30:16.686316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.13
 ---- batch: 020 ----
mean loss: 103.07
 ---- batch: 030 ----
mean loss: 102.01
 ---- batch: 040 ----
mean loss: 94.24
 ---- batch: 050 ----
mean loss: 100.26
 ---- batch: 060 ----
mean loss: 101.26
 ---- batch: 070 ----
mean loss: 102.48
 ---- batch: 080 ----
mean loss: 108.40
 ---- batch: 090 ----
mean loss: 105.22
train mean loss: 101.72
epoch train time: 0:00:01.706837
elapsed time: 0:07:42.303057
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_0/checkpoint.pth.tar
**** end time: 2019-09-25 19:30:18.401927 ****
