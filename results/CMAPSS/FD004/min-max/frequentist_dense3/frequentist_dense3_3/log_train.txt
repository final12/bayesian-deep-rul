Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_3', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18027
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistDense3...
Done.
**** start time: 2019-09-27 02:13:19.914204 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
            Linear-2                  [-1, 100]          36,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 56,100
Trainable params: 56,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:13:19.917757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4961.02
 ---- batch: 020 ----
mean loss: 4840.13
 ---- batch: 030 ----
mean loss: 4730.67
 ---- batch: 040 ----
mean loss: 4622.22
 ---- batch: 050 ----
mean loss: 4555.51
 ---- batch: 060 ----
mean loss: 4454.04
 ---- batch: 070 ----
mean loss: 4426.82
 ---- batch: 080 ----
mean loss: 4362.38
 ---- batch: 090 ----
mean loss: 4298.99
 ---- batch: 100 ----
mean loss: 4270.98
 ---- batch: 110 ----
mean loss: 4236.19
train mean loss: 4514.04
epoch train time: 0:00:32.578784
elapsed time: 0:00:32.584643
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:13:52.498892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4119.57
 ---- batch: 020 ----
mean loss: 4030.83
 ---- batch: 030 ----
mean loss: 4006.77
 ---- batch: 040 ----
mean loss: 3943.87
 ---- batch: 050 ----
mean loss: 3926.42
 ---- batch: 060 ----
mean loss: 3840.34
 ---- batch: 070 ----
mean loss: 3753.05
 ---- batch: 080 ----
mean loss: 3747.30
 ---- batch: 090 ----
mean loss: 3664.28
 ---- batch: 100 ----
mean loss: 3580.21
 ---- batch: 110 ----
mean loss: 3502.08
train mean loss: 3821.79
epoch train time: 0:00:00.564520
elapsed time: 0:00:33.149295
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:13:53.063547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3488.63
 ---- batch: 020 ----
mean loss: 3437.06
 ---- batch: 030 ----
mean loss: 3416.57
 ---- batch: 040 ----
mean loss: 3362.81
 ---- batch: 050 ----
mean loss: 3305.70
 ---- batch: 060 ----
mean loss: 3245.49
 ---- batch: 070 ----
mean loss: 3220.79
 ---- batch: 080 ----
mean loss: 3137.32
 ---- batch: 090 ----
mean loss: 3077.60
 ---- batch: 100 ----
mean loss: 3075.95
 ---- batch: 110 ----
mean loss: 2946.95
train mean loss: 3241.03
epoch train time: 0:00:00.549472
elapsed time: 0:00:33.698898
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:13:53.613148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2931.85
 ---- batch: 020 ----
mean loss: 2844.33
 ---- batch: 030 ----
mean loss: 2874.61
 ---- batch: 040 ----
mean loss: 2835.03
 ---- batch: 050 ----
mean loss: 2750.25
 ---- batch: 060 ----
mean loss: 2740.86
 ---- batch: 070 ----
mean loss: 2692.29
 ---- batch: 080 ----
mean loss: 2691.99
 ---- batch: 090 ----
mean loss: 2592.62
 ---- batch: 100 ----
mean loss: 2575.73
 ---- batch: 110 ----
mean loss: 2535.80
train mean loss: 2727.24
epoch train time: 0:00:00.545014
elapsed time: 0:00:34.244045
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:13:54.158298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2523.29
 ---- batch: 020 ----
mean loss: 2455.85
 ---- batch: 030 ----
mean loss: 2401.36
 ---- batch: 040 ----
mean loss: 2359.29
 ---- batch: 050 ----
mean loss: 2335.24
 ---- batch: 060 ----
mean loss: 2283.45
 ---- batch: 070 ----
mean loss: 2226.05
 ---- batch: 080 ----
mean loss: 2186.82
 ---- batch: 090 ----
mean loss: 2177.25
 ---- batch: 100 ----
mean loss: 2157.98
 ---- batch: 110 ----
mean loss: 2134.65
train mean loss: 2288.78
epoch train time: 0:00:00.546748
elapsed time: 0:00:34.790931
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:13:54.705179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2088.16
 ---- batch: 020 ----
mean loss: 2049.47
 ---- batch: 030 ----
mean loss: 2009.96
 ---- batch: 040 ----
mean loss: 2013.41
 ---- batch: 050 ----
mean loss: 1930.05
 ---- batch: 060 ----
mean loss: 1900.99
 ---- batch: 070 ----
mean loss: 1867.05
 ---- batch: 080 ----
mean loss: 1835.32
 ---- batch: 090 ----
mean loss: 1826.20
 ---- batch: 100 ----
mean loss: 1819.95
 ---- batch: 110 ----
mean loss: 1778.76
train mean loss: 1916.34
epoch train time: 0:00:00.557207
elapsed time: 0:00:35.348269
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:13:55.262533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1755.72
 ---- batch: 020 ----
mean loss: 1723.82
 ---- batch: 030 ----
mean loss: 1690.71
 ---- batch: 040 ----
mean loss: 1680.19
 ---- batch: 050 ----
mean loss: 1696.70
 ---- batch: 060 ----
mean loss: 1605.50
 ---- batch: 070 ----
mean loss: 1600.12
 ---- batch: 080 ----
mean loss: 1576.14
 ---- batch: 090 ----
mean loss: 1572.78
 ---- batch: 100 ----
mean loss: 1534.25
 ---- batch: 110 ----
mean loss: 1554.73
train mean loss: 1632.32
epoch train time: 0:00:00.548246
elapsed time: 0:00:35.896664
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:13:55.810916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1488.36
 ---- batch: 020 ----
mean loss: 1500.32
 ---- batch: 030 ----
mean loss: 1453.36
 ---- batch: 040 ----
mean loss: 1435.71
 ---- batch: 050 ----
mean loss: 1434.45
 ---- batch: 060 ----
mean loss: 1417.94
 ---- batch: 070 ----
mean loss: 1401.08
 ---- batch: 080 ----
mean loss: 1387.92
 ---- batch: 090 ----
mean loss: 1392.09
 ---- batch: 100 ----
mean loss: 1376.35
 ---- batch: 110 ----
mean loss: 1317.12
train mean loss: 1416.88
epoch train time: 0:00:00.554388
elapsed time: 0:00:36.451186
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:13:56.365463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1307.21
 ---- batch: 020 ----
mean loss: 1315.11
 ---- batch: 030 ----
mean loss: 1283.26
 ---- batch: 040 ----
mean loss: 1263.25
 ---- batch: 050 ----
mean loss: 1265.99
 ---- batch: 060 ----
mean loss: 1264.79
 ---- batch: 070 ----
mean loss: 1247.04
 ---- batch: 080 ----
mean loss: 1230.32
 ---- batch: 090 ----
mean loss: 1214.82
 ---- batch: 100 ----
mean loss: 1213.13
 ---- batch: 110 ----
mean loss: 1206.59
train mean loss: 1252.98
epoch train time: 0:00:00.551909
elapsed time: 0:00:37.003277
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:13:56.917563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1181.76
 ---- batch: 020 ----
mean loss: 1178.02
 ---- batch: 030 ----
mean loss: 1150.87
 ---- batch: 040 ----
mean loss: 1137.96
 ---- batch: 050 ----
mean loss: 1134.38
 ---- batch: 060 ----
mean loss: 1133.55
 ---- batch: 070 ----
mean loss: 1128.58
 ---- batch: 080 ----
mean loss: 1111.93
 ---- batch: 090 ----
mean loss: 1108.03
 ---- batch: 100 ----
mean loss: 1082.29
 ---- batch: 110 ----
mean loss: 1096.99
train mean loss: 1130.23
epoch train time: 0:00:00.552274
elapsed time: 0:00:37.555729
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:13:57.469982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1072.54
 ---- batch: 020 ----
mean loss: 1064.20
 ---- batch: 030 ----
mean loss: 1063.82
 ---- batch: 040 ----
mean loss: 1056.72
 ---- batch: 050 ----
mean loss: 1048.52
 ---- batch: 060 ----
mean loss: 1040.03
 ---- batch: 070 ----
mean loss: 1020.74
 ---- batch: 080 ----
mean loss: 1025.44
 ---- batch: 090 ----
mean loss: 1027.00
 ---- batch: 100 ----
mean loss: 1024.49
 ---- batch: 110 ----
mean loss: 1002.54
train mean loss: 1039.69
epoch train time: 0:00:00.552165
elapsed time: 0:00:38.108033
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:13:58.022285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1007.95
 ---- batch: 020 ----
mean loss: 994.00
 ---- batch: 030 ----
mean loss: 987.41
 ---- batch: 040 ----
mean loss: 976.49
 ---- batch: 050 ----
mean loss: 965.03
 ---- batch: 060 ----
mean loss: 973.38
 ---- batch: 070 ----
mean loss: 976.10
 ---- batch: 080 ----
mean loss: 964.88
 ---- batch: 090 ----
mean loss: 965.07
 ---- batch: 100 ----
mean loss: 960.07
 ---- batch: 110 ----
mean loss: 947.32
train mean loss: 974.27
epoch train time: 0:00:00.569074
elapsed time: 0:00:38.677246
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:13:58.591496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.77
 ---- batch: 020 ----
mean loss: 950.44
 ---- batch: 030 ----
mean loss: 942.45
 ---- batch: 040 ----
mean loss: 933.21
 ---- batch: 050 ----
mean loss: 923.96
 ---- batch: 060 ----
mean loss: 907.73
 ---- batch: 070 ----
mean loss: 932.87
 ---- batch: 080 ----
mean loss: 908.53
 ---- batch: 090 ----
mean loss: 913.02
 ---- batch: 100 ----
mean loss: 923.39
 ---- batch: 110 ----
mean loss: 898.89
train mean loss: 926.04
epoch train time: 0:00:00.551709
elapsed time: 0:00:39.229088
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:13:59.143338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.48
 ---- batch: 020 ----
mean loss: 896.25
 ---- batch: 030 ----
mean loss: 898.05
 ---- batch: 040 ----
mean loss: 891.11
 ---- batch: 050 ----
mean loss: 892.41
 ---- batch: 060 ----
mean loss: 902.10
 ---- batch: 070 ----
mean loss: 894.56
 ---- batch: 080 ----
mean loss: 893.26
 ---- batch: 090 ----
mean loss: 882.27
 ---- batch: 100 ----
mean loss: 889.72
 ---- batch: 110 ----
mean loss: 889.38
train mean loss: 893.49
epoch train time: 0:00:00.550047
elapsed time: 0:00:39.779288
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:13:59.693556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.41
 ---- batch: 020 ----
mean loss: 876.31
 ---- batch: 030 ----
mean loss: 892.01
 ---- batch: 040 ----
mean loss: 889.26
 ---- batch: 050 ----
mean loss: 873.49
 ---- batch: 060 ----
mean loss: 866.38
 ---- batch: 070 ----
mean loss: 864.95
 ---- batch: 080 ----
mean loss: 860.65
 ---- batch: 090 ----
mean loss: 869.18
 ---- batch: 100 ----
mean loss: 860.33
 ---- batch: 110 ----
mean loss: 878.69
train mean loss: 872.99
epoch train time: 0:00:00.549955
elapsed time: 0:00:40.329428
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:14:00.243677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.93
 ---- batch: 020 ----
mean loss: 868.24
 ---- batch: 030 ----
mean loss: 859.60
 ---- batch: 040 ----
mean loss: 848.65
 ---- batch: 050 ----
mean loss: 858.55
 ---- batch: 060 ----
mean loss: 863.22
 ---- batch: 070 ----
mean loss: 873.22
 ---- batch: 080 ----
mean loss: 849.96
 ---- batch: 090 ----
mean loss: 852.86
 ---- batch: 100 ----
mean loss: 865.18
 ---- batch: 110 ----
mean loss: 846.36
train mean loss: 860.55
epoch train time: 0:00:00.562682
elapsed time: 0:00:40.892265
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:14:00.806560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.36
 ---- batch: 020 ----
mean loss: 829.48
 ---- batch: 030 ----
mean loss: 853.94
 ---- batch: 040 ----
mean loss: 871.18
 ---- batch: 050 ----
mean loss: 870.41
 ---- batch: 060 ----
mean loss: 867.15
 ---- batch: 070 ----
mean loss: 860.97
 ---- batch: 080 ----
mean loss: 852.87
 ---- batch: 090 ----
mean loss: 839.46
 ---- batch: 100 ----
mean loss: 843.00
 ---- batch: 110 ----
mean loss: 844.76
train mean loss: 853.06
epoch train time: 0:00:00.553647
elapsed time: 0:00:41.446091
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:14:01.360342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.74
 ---- batch: 020 ----
mean loss: 853.46
 ---- batch: 030 ----
mean loss: 837.31
 ---- batch: 040 ----
mean loss: 849.56
 ---- batch: 050 ----
mean loss: 862.78
 ---- batch: 060 ----
mean loss: 829.46
 ---- batch: 070 ----
mean loss: 861.83
 ---- batch: 080 ----
mean loss: 844.47
 ---- batch: 090 ----
mean loss: 844.84
 ---- batch: 100 ----
mean loss: 862.51
 ---- batch: 110 ----
mean loss: 857.93
train mean loss: 848.39
epoch train time: 0:00:00.557246
elapsed time: 0:00:42.003512
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:14:01.917769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.32
 ---- batch: 020 ----
mean loss: 857.70
 ---- batch: 030 ----
mean loss: 844.91
 ---- batch: 040 ----
mean loss: 825.71
 ---- batch: 050 ----
mean loss: 837.16
 ---- batch: 060 ----
mean loss: 852.87
 ---- batch: 070 ----
mean loss: 842.70
 ---- batch: 080 ----
mean loss: 843.76
 ---- batch: 090 ----
mean loss: 853.38
 ---- batch: 100 ----
mean loss: 841.45
 ---- batch: 110 ----
mean loss: 863.68
train mean loss: 845.92
epoch train time: 0:00:00.555632
elapsed time: 0:00:42.559300
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:14:02.473550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.85
 ---- batch: 020 ----
mean loss: 853.51
 ---- batch: 030 ----
mean loss: 855.23
 ---- batch: 040 ----
mean loss: 839.73
 ---- batch: 050 ----
mean loss: 829.22
 ---- batch: 060 ----
mean loss: 852.02
 ---- batch: 070 ----
mean loss: 843.60
 ---- batch: 080 ----
mean loss: 849.10
 ---- batch: 090 ----
mean loss: 838.86
 ---- batch: 100 ----
mean loss: 846.81
 ---- batch: 110 ----
mean loss: 851.81
train mean loss: 844.83
epoch train time: 0:00:00.558963
elapsed time: 0:00:43.118392
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:14:03.032641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.72
 ---- batch: 020 ----
mean loss: 880.56
 ---- batch: 030 ----
mean loss: 843.58
 ---- batch: 040 ----
mean loss: 869.54
 ---- batch: 050 ----
mean loss: 847.97
 ---- batch: 060 ----
mean loss: 859.81
 ---- batch: 070 ----
mean loss: 840.74
 ---- batch: 080 ----
mean loss: 857.50
 ---- batch: 090 ----
mean loss: 830.81
 ---- batch: 100 ----
mean loss: 826.55
 ---- batch: 110 ----
mean loss: 828.17
train mean loss: 844.34
epoch train time: 0:00:00.542813
elapsed time: 0:00:43.661330
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:14:03.575611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.07
 ---- batch: 020 ----
mean loss: 856.47
 ---- batch: 030 ----
mean loss: 828.46
 ---- batch: 040 ----
mean loss: 858.94
 ---- batch: 050 ----
mean loss: 850.62
 ---- batch: 060 ----
mean loss: 841.74
 ---- batch: 070 ----
mean loss: 853.95
 ---- batch: 080 ----
mean loss: 837.77
 ---- batch: 090 ----
mean loss: 835.48
 ---- batch: 100 ----
mean loss: 831.93
 ---- batch: 110 ----
mean loss: 853.11
train mean loss: 844.11
epoch train time: 0:00:00.554590
elapsed time: 0:00:44.216115
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:14:04.130396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.56
 ---- batch: 020 ----
mean loss: 834.80
 ---- batch: 030 ----
mean loss: 819.20
 ---- batch: 040 ----
mean loss: 844.24
 ---- batch: 050 ----
mean loss: 871.36
 ---- batch: 060 ----
mean loss: 835.72
 ---- batch: 070 ----
mean loss: 864.72
 ---- batch: 080 ----
mean loss: 832.30
 ---- batch: 090 ----
mean loss: 846.11
 ---- batch: 100 ----
mean loss: 861.97
 ---- batch: 110 ----
mean loss: 844.25
train mean loss: 844.07
epoch train time: 0:00:00.549261
elapsed time: 0:00:44.765537
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:14:04.679788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.49
 ---- batch: 020 ----
mean loss: 841.67
 ---- batch: 030 ----
mean loss: 856.83
 ---- batch: 040 ----
mean loss: 844.19
 ---- batch: 050 ----
mean loss: 860.67
 ---- batch: 060 ----
mean loss: 834.28
 ---- batch: 070 ----
mean loss: 833.54
 ---- batch: 080 ----
mean loss: 858.24
 ---- batch: 090 ----
mean loss: 841.88
 ---- batch: 100 ----
mean loss: 851.78
 ---- batch: 110 ----
mean loss: 829.31
train mean loss: 844.01
epoch train time: 0:00:00.550368
elapsed time: 0:00:45.316058
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:14:05.230308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.90
 ---- batch: 020 ----
mean loss: 842.30
 ---- batch: 030 ----
mean loss: 827.37
 ---- batch: 040 ----
mean loss: 849.84
 ---- batch: 050 ----
mean loss: 846.93
 ---- batch: 060 ----
mean loss: 858.64
 ---- batch: 070 ----
mean loss: 818.94
 ---- batch: 080 ----
mean loss: 846.63
 ---- batch: 090 ----
mean loss: 854.55
 ---- batch: 100 ----
mean loss: 835.55
 ---- batch: 110 ----
mean loss: 854.67
train mean loss: 844.00
epoch train time: 0:00:00.549428
elapsed time: 0:00:45.865649
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:14:05.779921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.79
 ---- batch: 020 ----
mean loss: 845.52
 ---- batch: 030 ----
mean loss: 843.54
 ---- batch: 040 ----
mean loss: 840.50
 ---- batch: 050 ----
mean loss: 834.65
 ---- batch: 060 ----
mean loss: 852.34
 ---- batch: 070 ----
mean loss: 864.86
 ---- batch: 080 ----
mean loss: 830.97
 ---- batch: 090 ----
mean loss: 848.37
 ---- batch: 100 ----
mean loss: 838.75
 ---- batch: 110 ----
mean loss: 838.40
train mean loss: 843.96
epoch train time: 0:00:00.558842
elapsed time: 0:00:46.424668
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:14:06.338927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.06
 ---- batch: 020 ----
mean loss: 852.37
 ---- batch: 030 ----
mean loss: 858.97
 ---- batch: 040 ----
mean loss: 845.34
 ---- batch: 050 ----
mean loss: 839.65
 ---- batch: 060 ----
mean loss: 819.41
 ---- batch: 070 ----
mean loss: 828.50
 ---- batch: 080 ----
mean loss: 860.86
 ---- batch: 090 ----
mean loss: 863.12
 ---- batch: 100 ----
mean loss: 836.78
 ---- batch: 110 ----
mean loss: 837.83
train mean loss: 843.92
epoch train time: 0:00:00.553499
elapsed time: 0:00:46.978311
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:14:06.892565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.02
 ---- batch: 020 ----
mean loss: 834.66
 ---- batch: 030 ----
mean loss: 854.13
 ---- batch: 040 ----
mean loss: 865.56
 ---- batch: 050 ----
mean loss: 838.79
 ---- batch: 060 ----
mean loss: 839.51
 ---- batch: 070 ----
mean loss: 829.95
 ---- batch: 080 ----
mean loss: 849.24
 ---- batch: 090 ----
mean loss: 852.49
 ---- batch: 100 ----
mean loss: 835.78
 ---- batch: 110 ----
mean loss: 847.17
train mean loss: 843.84
epoch train time: 0:00:00.552171
elapsed time: 0:00:47.530664
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:14:07.444960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.61
 ---- batch: 020 ----
mean loss: 839.61
 ---- batch: 030 ----
mean loss: 857.25
 ---- batch: 040 ----
mean loss: 861.72
 ---- batch: 050 ----
mean loss: 858.48
 ---- batch: 060 ----
mean loss: 838.94
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 844.45
 ---- batch: 090 ----
mean loss: 822.49
 ---- batch: 100 ----
mean loss: 855.87
 ---- batch: 110 ----
mean loss: 837.78
train mean loss: 843.94
epoch train time: 0:00:00.551158
elapsed time: 0:00:48.082001
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:14:07.996250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 823.18
 ---- batch: 030 ----
mean loss: 834.62
 ---- batch: 040 ----
mean loss: 843.74
 ---- batch: 050 ----
mean loss: 847.57
 ---- batch: 060 ----
mean loss: 842.48
 ---- batch: 070 ----
mean loss: 847.41
 ---- batch: 080 ----
mean loss: 859.33
 ---- batch: 090 ----
mean loss: 843.56
 ---- batch: 100 ----
mean loss: 848.51
 ---- batch: 110 ----
mean loss: 841.92
train mean loss: 843.91
epoch train time: 0:00:00.554190
elapsed time: 0:00:48.636340
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:14:08.550591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.91
 ---- batch: 020 ----
mean loss: 839.22
 ---- batch: 030 ----
mean loss: 847.48
 ---- batch: 040 ----
mean loss: 860.20
 ---- batch: 050 ----
mean loss: 844.45
 ---- batch: 060 ----
mean loss: 840.43
 ---- batch: 070 ----
mean loss: 807.10
 ---- batch: 080 ----
mean loss: 851.61
 ---- batch: 090 ----
mean loss: 844.55
 ---- batch: 100 ----
mean loss: 855.06
 ---- batch: 110 ----
mean loss: 856.51
train mean loss: 844.02
epoch train time: 0:00:00.551824
elapsed time: 0:00:49.188350
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:14:09.102602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.92
 ---- batch: 020 ----
mean loss: 833.89
 ---- batch: 030 ----
mean loss: 864.36
 ---- batch: 040 ----
mean loss: 867.12
 ---- batch: 050 ----
mean loss: 818.03
 ---- batch: 060 ----
mean loss: 833.12
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 846.87
 ---- batch: 090 ----
mean loss: 842.99
 ---- batch: 100 ----
mean loss: 845.87
 ---- batch: 110 ----
mean loss: 831.11
train mean loss: 844.04
epoch train time: 0:00:00.552842
elapsed time: 0:00:49.741345
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:14:09.655597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.77
 ---- batch: 020 ----
mean loss: 832.53
 ---- batch: 030 ----
mean loss: 855.60
 ---- batch: 040 ----
mean loss: 864.32
 ---- batch: 050 ----
mean loss: 850.39
 ---- batch: 060 ----
mean loss: 854.95
 ---- batch: 070 ----
mean loss: 825.55
 ---- batch: 080 ----
mean loss: 851.27
 ---- batch: 090 ----
mean loss: 841.01
 ---- batch: 100 ----
mean loss: 851.35
 ---- batch: 110 ----
mean loss: 831.00
train mean loss: 843.87
epoch train time: 0:00:00.551106
elapsed time: 0:00:50.292585
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:14:10.206834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.60
 ---- batch: 020 ----
mean loss: 848.83
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 843.44
 ---- batch: 050 ----
mean loss: 824.87
 ---- batch: 060 ----
mean loss: 836.67
 ---- batch: 070 ----
mean loss: 843.42
 ---- batch: 080 ----
mean loss: 858.18
 ---- batch: 090 ----
mean loss: 855.87
 ---- batch: 100 ----
mean loss: 850.60
 ---- batch: 110 ----
mean loss: 855.83
train mean loss: 843.97
epoch train time: 0:00:00.544326
elapsed time: 0:00:50.837043
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:14:10.751315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.51
 ---- batch: 020 ----
mean loss: 814.73
 ---- batch: 030 ----
mean loss: 838.89
 ---- batch: 040 ----
mean loss: 851.10
 ---- batch: 050 ----
mean loss: 848.75
 ---- batch: 060 ----
mean loss: 852.61
 ---- batch: 070 ----
mean loss: 834.73
 ---- batch: 080 ----
mean loss: 850.42
 ---- batch: 090 ----
mean loss: 854.91
 ---- batch: 100 ----
mean loss: 853.50
 ---- batch: 110 ----
mean loss: 840.83
train mean loss: 844.05
epoch train time: 0:00:00.566809
elapsed time: 0:00:51.404022
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:14:11.318277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.12
 ---- batch: 020 ----
mean loss: 861.63
 ---- batch: 030 ----
mean loss: 838.12
 ---- batch: 040 ----
mean loss: 845.01
 ---- batch: 050 ----
mean loss: 836.21
 ---- batch: 060 ----
mean loss: 835.64
 ---- batch: 070 ----
mean loss: 842.72
 ---- batch: 080 ----
mean loss: 851.19
 ---- batch: 090 ----
mean loss: 833.50
 ---- batch: 100 ----
mean loss: 834.01
 ---- batch: 110 ----
mean loss: 834.96
train mean loss: 844.04
epoch train time: 0:00:00.575901
elapsed time: 0:00:51.980067
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:14:11.894322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.26
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 855.51
 ---- batch: 040 ----
mean loss: 842.87
 ---- batch: 050 ----
mean loss: 826.74
 ---- batch: 060 ----
mean loss: 850.21
 ---- batch: 070 ----
mean loss: 835.43
 ---- batch: 080 ----
mean loss: 827.14
 ---- batch: 090 ----
mean loss: 864.26
 ---- batch: 100 ----
mean loss: 852.65
 ---- batch: 110 ----
mean loss: 827.14
train mean loss: 843.95
epoch train time: 0:00:00.563877
elapsed time: 0:00:52.544104
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:14:12.458356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.02
 ---- batch: 020 ----
mean loss: 855.72
 ---- batch: 030 ----
mean loss: 830.98
 ---- batch: 040 ----
mean loss: 838.38
 ---- batch: 050 ----
mean loss: 868.38
 ---- batch: 060 ----
mean loss: 833.24
 ---- batch: 070 ----
mean loss: 839.23
 ---- batch: 080 ----
mean loss: 829.76
 ---- batch: 090 ----
mean loss: 826.33
 ---- batch: 100 ----
mean loss: 855.07
 ---- batch: 110 ----
mean loss: 855.24
train mean loss: 843.94
epoch train time: 0:00:00.564543
elapsed time: 0:00:53.108785
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:14:13.023036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.59
 ---- batch: 020 ----
mean loss: 836.38
 ---- batch: 030 ----
mean loss: 854.91
 ---- batch: 040 ----
mean loss: 859.66
 ---- batch: 050 ----
mean loss: 817.83
 ---- batch: 060 ----
mean loss: 832.36
 ---- batch: 070 ----
mean loss: 858.04
 ---- batch: 080 ----
mean loss: 854.70
 ---- batch: 090 ----
mean loss: 824.21
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 844.56
train mean loss: 844.09
epoch train time: 0:00:00.565792
elapsed time: 0:00:53.674711
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:14:13.588963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.50
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 842.46
 ---- batch: 040 ----
mean loss: 845.66
 ---- batch: 050 ----
mean loss: 833.50
 ---- batch: 060 ----
mean loss: 835.28
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 852.60
 ---- batch: 090 ----
mean loss: 845.35
 ---- batch: 100 ----
mean loss: 850.67
 ---- batch: 110 ----
mean loss: 849.95
train mean loss: 844.01
epoch train time: 0:00:00.570862
elapsed time: 0:00:54.245715
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:14:14.159969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.99
 ---- batch: 020 ----
mean loss: 855.23
 ---- batch: 030 ----
mean loss: 833.32
 ---- batch: 040 ----
mean loss: 858.46
 ---- batch: 050 ----
mean loss: 855.47
 ---- batch: 060 ----
mean loss: 845.19
 ---- batch: 070 ----
mean loss: 832.65
 ---- batch: 080 ----
mean loss: 846.02
 ---- batch: 090 ----
mean loss: 835.69
 ---- batch: 100 ----
mean loss: 840.27
 ---- batch: 110 ----
mean loss: 845.14
train mean loss: 843.93
epoch train time: 0:00:00.556592
elapsed time: 0:00:54.802475
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:14:14.716760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.49
 ---- batch: 020 ----
mean loss: 825.58
 ---- batch: 030 ----
mean loss: 839.14
 ---- batch: 040 ----
mean loss: 848.79
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 850.23
 ---- batch: 070 ----
mean loss: 845.20
 ---- batch: 080 ----
mean loss: 866.50
 ---- batch: 090 ----
mean loss: 853.99
 ---- batch: 100 ----
mean loss: 844.02
 ---- batch: 110 ----
mean loss: 822.71
train mean loss: 843.96
epoch train time: 0:00:00.574763
elapsed time: 0:00:55.377410
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:14:15.291670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.68
 ---- batch: 020 ----
mean loss: 835.69
 ---- batch: 030 ----
mean loss: 846.90
 ---- batch: 040 ----
mean loss: 835.58
 ---- batch: 050 ----
mean loss: 868.28
 ---- batch: 060 ----
mean loss: 830.60
 ---- batch: 070 ----
mean loss: 843.45
 ---- batch: 080 ----
mean loss: 856.95
 ---- batch: 090 ----
mean loss: 841.46
 ---- batch: 100 ----
mean loss: 841.99
 ---- batch: 110 ----
mean loss: 840.71
train mean loss: 844.08
epoch train time: 0:00:00.573557
elapsed time: 0:00:55.951138
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:14:15.865405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.32
 ---- batch: 020 ----
mean loss: 848.50
 ---- batch: 030 ----
mean loss: 852.79
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 845.94
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 809.15
 ---- batch: 080 ----
mean loss: 856.45
 ---- batch: 090 ----
mean loss: 845.25
 ---- batch: 100 ----
mean loss: 849.24
 ---- batch: 110 ----
mean loss: 851.07
train mean loss: 844.06
epoch train time: 0:00:00.555576
elapsed time: 0:00:56.506866
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:14:16.421135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.24
 ---- batch: 020 ----
mean loss: 835.81
 ---- batch: 030 ----
mean loss: 850.60
 ---- batch: 040 ----
mean loss: 858.13
 ---- batch: 050 ----
mean loss: 833.72
 ---- batch: 060 ----
mean loss: 865.09
 ---- batch: 070 ----
mean loss: 852.62
 ---- batch: 080 ----
mean loss: 833.10
 ---- batch: 090 ----
mean loss: 822.30
 ---- batch: 100 ----
mean loss: 830.74
 ---- batch: 110 ----
mean loss: 843.89
train mean loss: 844.04
epoch train time: 0:00:00.560680
elapsed time: 0:00:57.067704
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:14:16.981958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.00
 ---- batch: 020 ----
mean loss: 869.89
 ---- batch: 030 ----
mean loss: 857.71
 ---- batch: 040 ----
mean loss: 850.94
 ---- batch: 050 ----
mean loss: 831.70
 ---- batch: 060 ----
mean loss: 842.32
 ---- batch: 070 ----
mean loss: 837.32
 ---- batch: 080 ----
mean loss: 844.26
 ---- batch: 090 ----
mean loss: 840.97
 ---- batch: 100 ----
mean loss: 843.34
 ---- batch: 110 ----
mean loss: 853.45
train mean loss: 843.94
epoch train time: 0:00:00.565410
elapsed time: 0:00:57.633268
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:14:17.547558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.24
 ---- batch: 020 ----
mean loss: 826.17
 ---- batch: 030 ----
mean loss: 857.23
 ---- batch: 040 ----
mean loss: 855.75
 ---- batch: 050 ----
mean loss: 829.66
 ---- batch: 060 ----
mean loss: 825.05
 ---- batch: 070 ----
mean loss: 862.07
 ---- batch: 080 ----
mean loss: 821.71
 ---- batch: 090 ----
mean loss: 868.25
 ---- batch: 100 ----
mean loss: 840.49
 ---- batch: 110 ----
mean loss: 854.33
train mean loss: 844.02
epoch train time: 0:00:00.566546
elapsed time: 0:00:58.199991
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:14:18.114261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.59
 ---- batch: 020 ----
mean loss: 839.91
 ---- batch: 030 ----
mean loss: 832.06
 ---- batch: 040 ----
mean loss: 833.44
 ---- batch: 050 ----
mean loss: 843.63
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 865.28
 ---- batch: 080 ----
mean loss: 850.58
 ---- batch: 090 ----
mean loss: 864.03
 ---- batch: 100 ----
mean loss: 829.87
 ---- batch: 110 ----
mean loss: 840.02
train mean loss: 844.02
epoch train time: 0:00:00.565044
elapsed time: 0:00:58.765202
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:14:18.679460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.87
 ---- batch: 020 ----
mean loss: 843.81
 ---- batch: 030 ----
mean loss: 851.02
 ---- batch: 040 ----
mean loss: 856.66
 ---- batch: 050 ----
mean loss: 832.84
 ---- batch: 060 ----
mean loss: 851.55
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 833.21
 ---- batch: 090 ----
mean loss: 833.15
 ---- batch: 100 ----
mean loss: 851.12
 ---- batch: 110 ----
mean loss: 824.81
train mean loss: 844.02
epoch train time: 0:00:00.562493
elapsed time: 0:00:59.327836
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:14:19.242087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.72
 ---- batch: 020 ----
mean loss: 849.35
 ---- batch: 030 ----
mean loss: 854.58
 ---- batch: 040 ----
mean loss: 833.74
 ---- batch: 050 ----
mean loss: 834.99
 ---- batch: 060 ----
mean loss: 847.39
 ---- batch: 070 ----
mean loss: 828.86
 ---- batch: 080 ----
mean loss: 845.62
 ---- batch: 090 ----
mean loss: 847.91
 ---- batch: 100 ----
mean loss: 839.45
 ---- batch: 110 ----
mean loss: 846.27
train mean loss: 844.00
epoch train time: 0:00:00.565965
elapsed time: 0:00:59.893934
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:14:19.808205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.21
 ---- batch: 020 ----
mean loss: 846.50
 ---- batch: 030 ----
mean loss: 862.86
 ---- batch: 040 ----
mean loss: 852.97
 ---- batch: 050 ----
mean loss: 837.18
 ---- batch: 060 ----
mean loss: 840.36
 ---- batch: 070 ----
mean loss: 860.77
 ---- batch: 080 ----
mean loss: 855.69
 ---- batch: 090 ----
mean loss: 833.00
 ---- batch: 100 ----
mean loss: 817.31
 ---- batch: 110 ----
mean loss: 839.15
train mean loss: 843.95
epoch train time: 0:00:00.567348
elapsed time: 0:01:00.461441
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:14:20.375692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.07
 ---- batch: 020 ----
mean loss: 842.82
 ---- batch: 030 ----
mean loss: 842.87
 ---- batch: 040 ----
mean loss: 838.16
 ---- batch: 050 ----
mean loss: 844.73
 ---- batch: 060 ----
mean loss: 863.76
 ---- batch: 070 ----
mean loss: 842.70
 ---- batch: 080 ----
mean loss: 808.24
 ---- batch: 090 ----
mean loss: 833.55
 ---- batch: 100 ----
mean loss: 859.01
 ---- batch: 110 ----
mean loss: 849.06
train mean loss: 844.05
epoch train time: 0:00:00.562395
elapsed time: 0:01:01.023971
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:14:20.938221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.97
 ---- batch: 020 ----
mean loss: 835.15
 ---- batch: 030 ----
mean loss: 837.94
 ---- batch: 040 ----
mean loss: 824.13
 ---- batch: 050 ----
mean loss: 839.35
 ---- batch: 060 ----
mean loss: 857.77
 ---- batch: 070 ----
mean loss: 822.83
 ---- batch: 080 ----
mean loss: 857.42
 ---- batch: 090 ----
mean loss: 835.62
 ---- batch: 100 ----
mean loss: 851.20
 ---- batch: 110 ----
mean loss: 863.65
train mean loss: 844.00
epoch train time: 0:00:00.546473
elapsed time: 0:01:01.570579
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:14:21.484830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.57
 ---- batch: 020 ----
mean loss: 858.19
 ---- batch: 030 ----
mean loss: 852.89
 ---- batch: 040 ----
mean loss: 839.76
 ---- batch: 050 ----
mean loss: 830.17
 ---- batch: 060 ----
mean loss: 841.44
 ---- batch: 070 ----
mean loss: 855.69
 ---- batch: 080 ----
mean loss: 849.65
 ---- batch: 090 ----
mean loss: 832.65
 ---- batch: 100 ----
mean loss: 836.73
 ---- batch: 110 ----
mean loss: 842.08
train mean loss: 843.93
epoch train time: 0:00:00.556106
elapsed time: 0:01:02.126820
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:14:22.041092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.38
 ---- batch: 020 ----
mean loss: 853.11
 ---- batch: 030 ----
mean loss: 847.68
 ---- batch: 040 ----
mean loss: 843.46
 ---- batch: 050 ----
mean loss: 859.03
 ---- batch: 060 ----
mean loss: 816.25
 ---- batch: 070 ----
mean loss: 860.16
 ---- batch: 080 ----
mean loss: 813.18
 ---- batch: 090 ----
mean loss: 834.16
 ---- batch: 100 ----
mean loss: 842.72
 ---- batch: 110 ----
mean loss: 857.17
train mean loss: 843.89
epoch train time: 0:00:00.544659
elapsed time: 0:01:02.671631
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:14:22.585880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.12
 ---- batch: 020 ----
mean loss: 837.04
 ---- batch: 030 ----
mean loss: 843.16
 ---- batch: 040 ----
mean loss: 852.32
 ---- batch: 050 ----
mean loss: 844.03
 ---- batch: 060 ----
mean loss: 827.03
 ---- batch: 070 ----
mean loss: 815.07
 ---- batch: 080 ----
mean loss: 787.00
 ---- batch: 090 ----
mean loss: 788.95
 ---- batch: 100 ----
mean loss: 775.02
 ---- batch: 110 ----
mean loss: 767.73
train mean loss: 815.74
epoch train time: 0:00:00.546086
elapsed time: 0:01:03.217850
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:14:23.132101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 754.30
 ---- batch: 020 ----
mean loss: 727.91
 ---- batch: 030 ----
mean loss: 697.50
 ---- batch: 040 ----
mean loss: 652.98
 ---- batch: 050 ----
mean loss: 608.22
 ---- batch: 060 ----
mean loss: 550.34
 ---- batch: 070 ----
mean loss: 468.30
 ---- batch: 080 ----
mean loss: 411.80
 ---- batch: 090 ----
mean loss: 413.87
 ---- batch: 100 ----
mean loss: 385.29
 ---- batch: 110 ----
mean loss: 375.45
train mean loss: 544.90
epoch train time: 0:00:00.556866
elapsed time: 0:01:03.774868
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:14:23.689119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.48
 ---- batch: 020 ----
mean loss: 350.81
 ---- batch: 030 ----
mean loss: 333.43
 ---- batch: 040 ----
mean loss: 328.19
 ---- batch: 050 ----
mean loss: 331.79
 ---- batch: 060 ----
mean loss: 322.39
 ---- batch: 070 ----
mean loss: 315.62
 ---- batch: 080 ----
mean loss: 309.89
 ---- batch: 090 ----
mean loss: 306.06
 ---- batch: 100 ----
mean loss: 298.76
 ---- batch: 110 ----
mean loss: 299.17
train mean loss: 323.14
epoch train time: 0:00:00.555398
elapsed time: 0:01:04.330396
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:14:24.244679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.71
 ---- batch: 020 ----
mean loss: 288.79
 ---- batch: 030 ----
mean loss: 284.27
 ---- batch: 040 ----
mean loss: 276.75
 ---- batch: 050 ----
mean loss: 267.83
 ---- batch: 060 ----
mean loss: 261.29
 ---- batch: 070 ----
mean loss: 262.85
 ---- batch: 080 ----
mean loss: 273.71
 ---- batch: 090 ----
mean loss: 260.53
 ---- batch: 100 ----
mean loss: 270.55
 ---- batch: 110 ----
mean loss: 260.10
train mean loss: 270.95
epoch train time: 0:00:00.545064
elapsed time: 0:01:04.875627
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:14:24.789882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.85
 ---- batch: 020 ----
mean loss: 249.23
 ---- batch: 030 ----
mean loss: 254.44
 ---- batch: 040 ----
mean loss: 252.92
 ---- batch: 050 ----
mean loss: 238.56
 ---- batch: 060 ----
mean loss: 240.61
 ---- batch: 070 ----
mean loss: 238.85
 ---- batch: 080 ----
mean loss: 247.21
 ---- batch: 090 ----
mean loss: 257.13
 ---- batch: 100 ----
mean loss: 254.23
 ---- batch: 110 ----
mean loss: 246.98
train mean loss: 248.38
epoch train time: 0:00:00.546956
elapsed time: 0:01:05.422735
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:14:25.337005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.07
 ---- batch: 020 ----
mean loss: 231.19
 ---- batch: 030 ----
mean loss: 242.29
 ---- batch: 040 ----
mean loss: 239.63
 ---- batch: 050 ----
mean loss: 241.49
 ---- batch: 060 ----
mean loss: 238.93
 ---- batch: 070 ----
mean loss: 231.05
 ---- batch: 080 ----
mean loss: 238.74
 ---- batch: 090 ----
mean loss: 239.07
 ---- batch: 100 ----
mean loss: 232.46
 ---- batch: 110 ----
mean loss: 230.52
train mean loss: 236.51
epoch train time: 0:00:00.555048
elapsed time: 0:01:05.977949
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:14:25.892197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.69
 ---- batch: 020 ----
mean loss: 236.99
 ---- batch: 030 ----
mean loss: 230.33
 ---- batch: 040 ----
mean loss: 218.28
 ---- batch: 050 ----
mean loss: 230.49
 ---- batch: 060 ----
mean loss: 226.40
 ---- batch: 070 ----
mean loss: 231.68
 ---- batch: 080 ----
mean loss: 223.50
 ---- batch: 090 ----
mean loss: 231.39
 ---- batch: 100 ----
mean loss: 217.62
 ---- batch: 110 ----
mean loss: 229.02
train mean loss: 226.92
epoch train time: 0:00:00.560689
elapsed time: 0:01:06.538787
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:14:26.453056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.77
 ---- batch: 020 ----
mean loss: 211.53
 ---- batch: 030 ----
mean loss: 218.79
 ---- batch: 040 ----
mean loss: 224.73
 ---- batch: 050 ----
mean loss: 210.21
 ---- batch: 060 ----
mean loss: 219.13
 ---- batch: 070 ----
mean loss: 204.52
 ---- batch: 080 ----
mean loss: 226.82
 ---- batch: 090 ----
mean loss: 216.91
 ---- batch: 100 ----
mean loss: 218.74
 ---- batch: 110 ----
mean loss: 217.00
train mean loss: 218.00
epoch train time: 0:00:00.556707
elapsed time: 0:01:07.095649
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:14:27.009901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.39
 ---- batch: 020 ----
mean loss: 210.45
 ---- batch: 030 ----
mean loss: 210.36
 ---- batch: 040 ----
mean loss: 215.80
 ---- batch: 050 ----
mean loss: 207.48
 ---- batch: 060 ----
mean loss: 207.31
 ---- batch: 070 ----
mean loss: 215.83
 ---- batch: 080 ----
mean loss: 208.22
 ---- batch: 090 ----
mean loss: 206.59
 ---- batch: 100 ----
mean loss: 209.19
 ---- batch: 110 ----
mean loss: 220.29
train mean loss: 211.10
epoch train time: 0:00:00.556938
elapsed time: 0:01:07.652732
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:14:27.566985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.30
 ---- batch: 020 ----
mean loss: 208.57
 ---- batch: 030 ----
mean loss: 201.88
 ---- batch: 040 ----
mean loss: 191.41
 ---- batch: 050 ----
mean loss: 205.72
 ---- batch: 060 ----
mean loss: 211.06
 ---- batch: 070 ----
mean loss: 209.77
 ---- batch: 080 ----
mean loss: 210.33
 ---- batch: 090 ----
mean loss: 215.45
 ---- batch: 100 ----
mean loss: 205.28
 ---- batch: 110 ----
mean loss: 200.24
train mean loss: 206.15
epoch train time: 0:00:00.551129
elapsed time: 0:01:08.203998
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:14:28.118249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.59
 ---- batch: 020 ----
mean loss: 195.89
 ---- batch: 030 ----
mean loss: 210.08
 ---- batch: 040 ----
mean loss: 205.01
 ---- batch: 050 ----
mean loss: 211.19
 ---- batch: 060 ----
mean loss: 206.77
 ---- batch: 070 ----
mean loss: 212.79
 ---- batch: 080 ----
mean loss: 202.59
 ---- batch: 090 ----
mean loss: 194.22
 ---- batch: 100 ----
mean loss: 196.87
 ---- batch: 110 ----
mean loss: 206.49
train mean loss: 204.68
epoch train time: 0:00:00.555624
elapsed time: 0:01:08.759764
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:14:28.674015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.20
 ---- batch: 020 ----
mean loss: 195.92
 ---- batch: 030 ----
mean loss: 196.43
 ---- batch: 040 ----
mean loss: 195.05
 ---- batch: 050 ----
mean loss: 188.25
 ---- batch: 060 ----
mean loss: 204.34
 ---- batch: 070 ----
mean loss: 197.23
 ---- batch: 080 ----
mean loss: 202.12
 ---- batch: 090 ----
mean loss: 197.92
 ---- batch: 100 ----
mean loss: 198.90
 ---- batch: 110 ----
mean loss: 199.89
train mean loss: 197.81
epoch train time: 0:00:00.552625
elapsed time: 0:01:09.312527
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:14:29.226800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.58
 ---- batch: 020 ----
mean loss: 191.80
 ---- batch: 030 ----
mean loss: 183.21
 ---- batch: 040 ----
mean loss: 196.30
 ---- batch: 050 ----
mean loss: 193.34
 ---- batch: 060 ----
mean loss: 189.43
 ---- batch: 070 ----
mean loss: 198.14
 ---- batch: 080 ----
mean loss: 198.13
 ---- batch: 090 ----
mean loss: 207.55
 ---- batch: 100 ----
mean loss: 199.90
 ---- batch: 110 ----
mean loss: 197.26
train mean loss: 195.55
epoch train time: 0:00:00.559128
elapsed time: 0:01:09.871812
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:14:29.786063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.69
 ---- batch: 020 ----
mean loss: 195.12
 ---- batch: 030 ----
mean loss: 184.82
 ---- batch: 040 ----
mean loss: 190.18
 ---- batch: 050 ----
mean loss: 197.97
 ---- batch: 060 ----
mean loss: 201.78
 ---- batch: 070 ----
mean loss: 195.64
 ---- batch: 080 ----
mean loss: 192.30
 ---- batch: 090 ----
mean loss: 199.01
 ---- batch: 100 ----
mean loss: 183.61
 ---- batch: 110 ----
mean loss: 198.63
train mean loss: 193.66
epoch train time: 0:00:00.555834
elapsed time: 0:01:10.427784
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:14:30.342043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.59
 ---- batch: 020 ----
mean loss: 194.61
 ---- batch: 030 ----
mean loss: 194.65
 ---- batch: 040 ----
mean loss: 183.76
 ---- batch: 050 ----
mean loss: 193.45
 ---- batch: 060 ----
mean loss: 194.02
 ---- batch: 070 ----
mean loss: 188.06
 ---- batch: 080 ----
mean loss: 186.18
 ---- batch: 090 ----
mean loss: 187.95
 ---- batch: 100 ----
mean loss: 185.86
 ---- batch: 110 ----
mean loss: 195.98
train mean loss: 190.88
epoch train time: 0:00:00.562556
elapsed time: 0:01:10.990486
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:14:30.904738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.48
 ---- batch: 020 ----
mean loss: 190.14
 ---- batch: 030 ----
mean loss: 198.41
 ---- batch: 040 ----
mean loss: 188.81
 ---- batch: 050 ----
mean loss: 187.23
 ---- batch: 060 ----
mean loss: 182.60
 ---- batch: 070 ----
mean loss: 190.84
 ---- batch: 080 ----
mean loss: 187.92
 ---- batch: 090 ----
mean loss: 195.17
 ---- batch: 100 ----
mean loss: 187.09
 ---- batch: 110 ----
mean loss: 191.52
train mean loss: 188.97
epoch train time: 0:00:00.558406
elapsed time: 0:01:11.549026
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:14:31.463276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.14
 ---- batch: 020 ----
mean loss: 181.28
 ---- batch: 030 ----
mean loss: 190.90
 ---- batch: 040 ----
mean loss: 188.27
 ---- batch: 050 ----
mean loss: 175.96
 ---- batch: 060 ----
mean loss: 189.12
 ---- batch: 070 ----
mean loss: 191.94
 ---- batch: 080 ----
mean loss: 196.36
 ---- batch: 090 ----
mean loss: 185.19
 ---- batch: 100 ----
mean loss: 187.37
 ---- batch: 110 ----
mean loss: 187.60
train mean loss: 187.26
epoch train time: 0:00:00.560033
elapsed time: 0:01:12.109194
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:14:32.023455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.22
 ---- batch: 020 ----
mean loss: 182.74
 ---- batch: 030 ----
mean loss: 187.04
 ---- batch: 040 ----
mean loss: 187.70
 ---- batch: 050 ----
mean loss: 190.76
 ---- batch: 060 ----
mean loss: 183.66
 ---- batch: 070 ----
mean loss: 187.01
 ---- batch: 080 ----
mean loss: 182.20
 ---- batch: 090 ----
mean loss: 184.50
 ---- batch: 100 ----
mean loss: 190.44
 ---- batch: 110 ----
mean loss: 182.57
train mean loss: 185.64
epoch train time: 0:00:00.552818
elapsed time: 0:01:12.662158
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:14:32.576417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.83
 ---- batch: 020 ----
mean loss: 186.31
 ---- batch: 030 ----
mean loss: 183.76
 ---- batch: 040 ----
mean loss: 179.45
 ---- batch: 050 ----
mean loss: 181.30
 ---- batch: 060 ----
mean loss: 191.11
 ---- batch: 070 ----
mean loss: 182.80
 ---- batch: 080 ----
mean loss: 189.30
 ---- batch: 090 ----
mean loss: 187.99
 ---- batch: 100 ----
mean loss: 188.84
 ---- batch: 110 ----
mean loss: 181.16
train mean loss: 185.03
epoch train time: 0:00:00.552875
elapsed time: 0:01:13.215193
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:14:33.129463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.77
 ---- batch: 020 ----
mean loss: 182.91
 ---- batch: 030 ----
mean loss: 181.13
 ---- batch: 040 ----
mean loss: 178.54
 ---- batch: 050 ----
mean loss: 183.71
 ---- batch: 060 ----
mean loss: 185.92
 ---- batch: 070 ----
mean loss: 186.47
 ---- batch: 080 ----
mean loss: 188.36
 ---- batch: 090 ----
mean loss: 181.14
 ---- batch: 100 ----
mean loss: 180.83
 ---- batch: 110 ----
mean loss: 184.03
train mean loss: 184.03
epoch train time: 0:00:00.553807
elapsed time: 0:01:13.769153
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:14:33.683404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.47
 ---- batch: 020 ----
mean loss: 180.00
 ---- batch: 030 ----
mean loss: 191.14
 ---- batch: 040 ----
mean loss: 187.02
 ---- batch: 050 ----
mean loss: 191.75
 ---- batch: 060 ----
mean loss: 176.00
 ---- batch: 070 ----
mean loss: 178.28
 ---- batch: 080 ----
mean loss: 183.48
 ---- batch: 090 ----
mean loss: 184.79
 ---- batch: 100 ----
mean loss: 178.58
 ---- batch: 110 ----
mean loss: 187.43
train mean loss: 183.28
epoch train time: 0:00:00.564800
elapsed time: 0:01:14.334089
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:14:34.248341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.16
 ---- batch: 020 ----
mean loss: 175.36
 ---- batch: 030 ----
mean loss: 185.13
 ---- batch: 040 ----
mean loss: 185.61
 ---- batch: 050 ----
mean loss: 187.69
 ---- batch: 060 ----
mean loss: 188.77
 ---- batch: 070 ----
mean loss: 186.84
 ---- batch: 080 ----
mean loss: 180.22
 ---- batch: 090 ----
mean loss: 181.42
 ---- batch: 100 ----
mean loss: 170.96
 ---- batch: 110 ----
mean loss: 185.80
train mean loss: 182.77
epoch train time: 0:00:00.552955
elapsed time: 0:01:14.887186
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:14:34.801488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.75
 ---- batch: 020 ----
mean loss: 184.40
 ---- batch: 030 ----
mean loss: 176.28
 ---- batch: 040 ----
mean loss: 178.50
 ---- batch: 050 ----
mean loss: 184.30
 ---- batch: 060 ----
mean loss: 180.73
 ---- batch: 070 ----
mean loss: 180.39
 ---- batch: 080 ----
mean loss: 178.56
 ---- batch: 090 ----
mean loss: 182.76
 ---- batch: 100 ----
mean loss: 185.87
 ---- batch: 110 ----
mean loss: 177.94
train mean loss: 181.11
epoch train time: 0:00:00.556692
elapsed time: 0:01:15.444068
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:14:35.358320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.06
 ---- batch: 020 ----
mean loss: 174.32
 ---- batch: 030 ----
mean loss: 170.56
 ---- batch: 040 ----
mean loss: 182.22
 ---- batch: 050 ----
mean loss: 182.12
 ---- batch: 060 ----
mean loss: 187.51
 ---- batch: 070 ----
mean loss: 181.61
 ---- batch: 080 ----
mean loss: 184.24
 ---- batch: 090 ----
mean loss: 180.08
 ---- batch: 100 ----
mean loss: 176.53
 ---- batch: 110 ----
mean loss: 181.33
train mean loss: 179.22
epoch train time: 0:00:00.565263
elapsed time: 0:01:16.009466
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:14:35.923718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.74
 ---- batch: 020 ----
mean loss: 178.91
 ---- batch: 030 ----
mean loss: 171.42
 ---- batch: 040 ----
mean loss: 185.99
 ---- batch: 050 ----
mean loss: 179.91
 ---- batch: 060 ----
mean loss: 181.26
 ---- batch: 070 ----
mean loss: 178.36
 ---- batch: 080 ----
mean loss: 174.25
 ---- batch: 090 ----
mean loss: 181.78
 ---- batch: 100 ----
mean loss: 172.80
 ---- batch: 110 ----
mean loss: 195.69
train mean loss: 179.03
epoch train time: 0:00:00.554415
elapsed time: 0:01:16.564019
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:14:36.478289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.34
 ---- batch: 020 ----
mean loss: 180.77
 ---- batch: 030 ----
mean loss: 172.61
 ---- batch: 040 ----
mean loss: 175.78
 ---- batch: 050 ----
mean loss: 177.75
 ---- batch: 060 ----
mean loss: 183.64
 ---- batch: 070 ----
mean loss: 184.68
 ---- batch: 080 ----
mean loss: 174.35
 ---- batch: 090 ----
mean loss: 181.53
 ---- batch: 100 ----
mean loss: 180.27
 ---- batch: 110 ----
mean loss: 178.29
train mean loss: 178.59
epoch train time: 0:00:00.559209
elapsed time: 0:01:17.123382
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:14:37.037665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.78
 ---- batch: 020 ----
mean loss: 173.71
 ---- batch: 030 ----
mean loss: 176.80
 ---- batch: 040 ----
mean loss: 176.58
 ---- batch: 050 ----
mean loss: 178.89
 ---- batch: 060 ----
mean loss: 172.68
 ---- batch: 070 ----
mean loss: 174.56
 ---- batch: 080 ----
mean loss: 193.53
 ---- batch: 090 ----
mean loss: 178.52
 ---- batch: 100 ----
mean loss: 165.98
 ---- batch: 110 ----
mean loss: 184.98
train mean loss: 177.09
epoch train time: 0:00:00.552081
elapsed time: 0:01:17.675628
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:14:37.589879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.02
 ---- batch: 020 ----
mean loss: 181.80
 ---- batch: 030 ----
mean loss: 178.14
 ---- batch: 040 ----
mean loss: 181.78
 ---- batch: 050 ----
mean loss: 168.31
 ---- batch: 060 ----
mean loss: 180.63
 ---- batch: 070 ----
mean loss: 185.58
 ---- batch: 080 ----
mean loss: 177.92
 ---- batch: 090 ----
mean loss: 181.06
 ---- batch: 100 ----
mean loss: 175.29
 ---- batch: 110 ----
mean loss: 175.73
train mean loss: 177.85
epoch train time: 0:00:00.556907
elapsed time: 0:01:18.232691
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:14:38.146943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.45
 ---- batch: 020 ----
mean loss: 176.40
 ---- batch: 030 ----
mean loss: 176.18
 ---- batch: 040 ----
mean loss: 172.86
 ---- batch: 050 ----
mean loss: 177.66
 ---- batch: 060 ----
mean loss: 175.87
 ---- batch: 070 ----
mean loss: 174.41
 ---- batch: 080 ----
mean loss: 175.99
 ---- batch: 090 ----
mean loss: 176.27
 ---- batch: 100 ----
mean loss: 175.80
 ---- batch: 110 ----
mean loss: 174.78
train mean loss: 175.76
epoch train time: 0:00:00.561289
elapsed time: 0:01:18.794127
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:14:38.708395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.02
 ---- batch: 020 ----
mean loss: 178.89
 ---- batch: 030 ----
mean loss: 174.16
 ---- batch: 040 ----
mean loss: 169.77
 ---- batch: 050 ----
mean loss: 166.66
 ---- batch: 060 ----
mean loss: 177.47
 ---- batch: 070 ----
mean loss: 186.97
 ---- batch: 080 ----
mean loss: 180.46
 ---- batch: 090 ----
mean loss: 175.38
 ---- batch: 100 ----
mean loss: 181.15
 ---- batch: 110 ----
mean loss: 174.48
train mean loss: 176.29
epoch train time: 0:00:00.566821
elapsed time: 0:01:19.361118
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:14:39.275371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.12
 ---- batch: 020 ----
mean loss: 179.01
 ---- batch: 030 ----
mean loss: 162.64
 ---- batch: 040 ----
mean loss: 174.96
 ---- batch: 050 ----
mean loss: 178.03
 ---- batch: 060 ----
mean loss: 175.92
 ---- batch: 070 ----
mean loss: 174.81
 ---- batch: 080 ----
mean loss: 181.43
 ---- batch: 090 ----
mean loss: 176.94
 ---- batch: 100 ----
mean loss: 179.15
 ---- batch: 110 ----
mean loss: 179.66
train mean loss: 175.21
epoch train time: 0:00:00.561149
elapsed time: 0:01:19.922410
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:14:39.836663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.23
 ---- batch: 020 ----
mean loss: 183.80
 ---- batch: 030 ----
mean loss: 165.52
 ---- batch: 040 ----
mean loss: 170.63
 ---- batch: 050 ----
mean loss: 179.89
 ---- batch: 060 ----
mean loss: 172.94
 ---- batch: 070 ----
mean loss: 172.15
 ---- batch: 080 ----
mean loss: 170.24
 ---- batch: 090 ----
mean loss: 177.71
 ---- batch: 100 ----
mean loss: 180.93
 ---- batch: 110 ----
mean loss: 181.65
train mean loss: 174.30
epoch train time: 0:00:00.555550
elapsed time: 0:01:20.478097
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:14:40.392375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.48
 ---- batch: 020 ----
mean loss: 177.57
 ---- batch: 030 ----
mean loss: 173.77
 ---- batch: 040 ----
mean loss: 164.84
 ---- batch: 050 ----
mean loss: 171.76
 ---- batch: 060 ----
mean loss: 171.11
 ---- batch: 070 ----
mean loss: 173.45
 ---- batch: 080 ----
mean loss: 183.64
 ---- batch: 090 ----
mean loss: 175.80
 ---- batch: 100 ----
mean loss: 166.13
 ---- batch: 110 ----
mean loss: 180.59
train mean loss: 173.59
epoch train time: 0:00:00.555625
elapsed time: 0:01:21.033885
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:14:40.948138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.19
 ---- batch: 020 ----
mean loss: 166.67
 ---- batch: 030 ----
mean loss: 176.87
 ---- batch: 040 ----
mean loss: 172.53
 ---- batch: 050 ----
mean loss: 175.19
 ---- batch: 060 ----
mean loss: 166.95
 ---- batch: 070 ----
mean loss: 178.95
 ---- batch: 080 ----
mean loss: 174.94
 ---- batch: 090 ----
mean loss: 176.37
 ---- batch: 100 ----
mean loss: 177.43
 ---- batch: 110 ----
mean loss: 169.41
train mean loss: 172.46
epoch train time: 0:00:00.560318
elapsed time: 0:01:21.594344
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:14:41.508594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.69
 ---- batch: 020 ----
mean loss: 171.50
 ---- batch: 030 ----
mean loss: 171.33
 ---- batch: 040 ----
mean loss: 175.92
 ---- batch: 050 ----
mean loss: 164.48
 ---- batch: 060 ----
mean loss: 170.65
 ---- batch: 070 ----
mean loss: 173.14
 ---- batch: 080 ----
mean loss: 178.84
 ---- batch: 090 ----
mean loss: 169.80
 ---- batch: 100 ----
mean loss: 173.12
 ---- batch: 110 ----
mean loss: 172.26
train mean loss: 171.73
epoch train time: 0:00:00.555974
elapsed time: 0:01:22.150485
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:14:42.064740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.74
 ---- batch: 020 ----
mean loss: 167.77
 ---- batch: 030 ----
mean loss: 162.83
 ---- batch: 040 ----
mean loss: 167.42
 ---- batch: 050 ----
mean loss: 164.41
 ---- batch: 060 ----
mean loss: 179.12
 ---- batch: 070 ----
mean loss: 181.73
 ---- batch: 080 ----
mean loss: 172.84
 ---- batch: 090 ----
mean loss: 166.73
 ---- batch: 100 ----
mean loss: 176.10
 ---- batch: 110 ----
mean loss: 172.75
train mean loss: 171.56
epoch train time: 0:00:00.555743
elapsed time: 0:01:22.706397
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:14:42.620646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.39
 ---- batch: 020 ----
mean loss: 173.16
 ---- batch: 030 ----
mean loss: 171.63
 ---- batch: 040 ----
mean loss: 164.79
 ---- batch: 050 ----
mean loss: 167.20
 ---- batch: 060 ----
mean loss: 177.16
 ---- batch: 070 ----
mean loss: 170.92
 ---- batch: 080 ----
mean loss: 170.20
 ---- batch: 090 ----
mean loss: 172.67
 ---- batch: 100 ----
mean loss: 174.33
 ---- batch: 110 ----
mean loss: 175.65
train mean loss: 171.24
epoch train time: 0:00:00.588293
elapsed time: 0:01:23.294827
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:14:43.209081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.88
 ---- batch: 020 ----
mean loss: 175.44
 ---- batch: 030 ----
mean loss: 162.60
 ---- batch: 040 ----
mean loss: 174.61
 ---- batch: 050 ----
mean loss: 165.42
 ---- batch: 060 ----
mean loss: 173.21
 ---- batch: 070 ----
mean loss: 176.95
 ---- batch: 080 ----
mean loss: 180.73
 ---- batch: 090 ----
mean loss: 170.81
 ---- batch: 100 ----
mean loss: 170.00
 ---- batch: 110 ----
mean loss: 173.85
train mean loss: 172.03
epoch train time: 0:00:00.576171
elapsed time: 0:01:23.871135
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:14:43.785403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.32
 ---- batch: 020 ----
mean loss: 176.12
 ---- batch: 030 ----
mean loss: 167.28
 ---- batch: 040 ----
mean loss: 172.18
 ---- batch: 050 ----
mean loss: 170.05
 ---- batch: 060 ----
mean loss: 159.30
 ---- batch: 070 ----
mean loss: 174.14
 ---- batch: 080 ----
mean loss: 162.02
 ---- batch: 090 ----
mean loss: 174.77
 ---- batch: 100 ----
mean loss: 171.91
 ---- batch: 110 ----
mean loss: 171.49
train mean loss: 169.90
epoch train time: 0:00:00.573570
elapsed time: 0:01:24.444863
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:14:44.359113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.65
 ---- batch: 020 ----
mean loss: 176.72
 ---- batch: 030 ----
mean loss: 171.15
 ---- batch: 040 ----
mean loss: 163.23
 ---- batch: 050 ----
mean loss: 164.52
 ---- batch: 060 ----
mean loss: 172.63
 ---- batch: 070 ----
mean loss: 176.96
 ---- batch: 080 ----
mean loss: 175.76
 ---- batch: 090 ----
mean loss: 165.85
 ---- batch: 100 ----
mean loss: 171.96
 ---- batch: 110 ----
mean loss: 165.36
train mean loss: 169.93
epoch train time: 0:00:00.568647
elapsed time: 0:01:25.013640
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:14:44.927889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.99
 ---- batch: 020 ----
mean loss: 168.15
 ---- batch: 030 ----
mean loss: 174.25
 ---- batch: 040 ----
mean loss: 165.05
 ---- batch: 050 ----
mean loss: 174.51
 ---- batch: 060 ----
mean loss: 167.20
 ---- batch: 070 ----
mean loss: 168.18
 ---- batch: 080 ----
mean loss: 170.21
 ---- batch: 090 ----
mean loss: 169.07
 ---- batch: 100 ----
mean loss: 167.61
 ---- batch: 110 ----
mean loss: 168.38
train mean loss: 169.18
epoch train time: 0:00:00.560996
elapsed time: 0:01:25.574764
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:14:45.489031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.83
 ---- batch: 020 ----
mean loss: 171.56
 ---- batch: 030 ----
mean loss: 164.49
 ---- batch: 040 ----
mean loss: 174.62
 ---- batch: 050 ----
mean loss: 176.37
 ---- batch: 060 ----
mean loss: 164.81
 ---- batch: 070 ----
mean loss: 164.71
 ---- batch: 080 ----
mean loss: 162.97
 ---- batch: 090 ----
mean loss: 171.70
 ---- batch: 100 ----
mean loss: 170.01
 ---- batch: 110 ----
mean loss: 169.78
train mean loss: 169.22
epoch train time: 0:00:00.575523
elapsed time: 0:01:26.150440
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:14:46.064708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.48
 ---- batch: 020 ----
mean loss: 158.66
 ---- batch: 030 ----
mean loss: 154.92
 ---- batch: 040 ----
mean loss: 170.90
 ---- batch: 050 ----
mean loss: 177.06
 ---- batch: 060 ----
mean loss: 173.64
 ---- batch: 070 ----
mean loss: 170.83
 ---- batch: 080 ----
mean loss: 171.70
 ---- batch: 090 ----
mean loss: 166.72
 ---- batch: 100 ----
mean loss: 172.10
 ---- batch: 110 ----
mean loss: 172.67
train mean loss: 168.93
epoch train time: 0:00:00.575624
elapsed time: 0:01:26.726214
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:14:46.640467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.41
 ---- batch: 020 ----
mean loss: 163.65
 ---- batch: 030 ----
mean loss: 167.60
 ---- batch: 040 ----
mean loss: 171.05
 ---- batch: 050 ----
mean loss: 178.68
 ---- batch: 060 ----
mean loss: 165.94
 ---- batch: 070 ----
mean loss: 161.30
 ---- batch: 080 ----
mean loss: 172.40
 ---- batch: 090 ----
mean loss: 175.75
 ---- batch: 100 ----
mean loss: 170.87
 ---- batch: 110 ----
mean loss: 165.22
train mean loss: 168.52
epoch train time: 0:00:00.580850
elapsed time: 0:01:27.307195
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:14:47.221454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.47
 ---- batch: 020 ----
mean loss: 164.32
 ---- batch: 030 ----
mean loss: 165.27
 ---- batch: 040 ----
mean loss: 165.19
 ---- batch: 050 ----
mean loss: 171.29
 ---- batch: 060 ----
mean loss: 171.35
 ---- batch: 070 ----
mean loss: 169.90
 ---- batch: 080 ----
mean loss: 177.62
 ---- batch: 090 ----
mean loss: 165.17
 ---- batch: 100 ----
mean loss: 166.79
 ---- batch: 110 ----
mean loss: 163.63
train mean loss: 167.16
epoch train time: 0:00:00.568551
elapsed time: 0:01:27.875889
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:14:47.790158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.31
 ---- batch: 020 ----
mean loss: 170.00
 ---- batch: 030 ----
mean loss: 156.84
 ---- batch: 040 ----
mean loss: 171.63
 ---- batch: 050 ----
mean loss: 163.78
 ---- batch: 060 ----
mean loss: 166.57
 ---- batch: 070 ----
mean loss: 159.29
 ---- batch: 080 ----
mean loss: 157.83
 ---- batch: 090 ----
mean loss: 166.29
 ---- batch: 100 ----
mean loss: 172.06
 ---- batch: 110 ----
mean loss: 177.52
train mean loss: 166.67
epoch train time: 0:00:00.590356
elapsed time: 0:01:28.466394
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:14:48.380644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.87
 ---- batch: 020 ----
mean loss: 164.05
 ---- batch: 030 ----
mean loss: 170.17
 ---- batch: 040 ----
mean loss: 161.27
 ---- batch: 050 ----
mean loss: 165.79
 ---- batch: 060 ----
mean loss: 172.73
 ---- batch: 070 ----
mean loss: 166.51
 ---- batch: 080 ----
mean loss: 165.57
 ---- batch: 090 ----
mean loss: 162.75
 ---- batch: 100 ----
mean loss: 172.07
 ---- batch: 110 ----
mean loss: 165.29
train mean loss: 166.01
epoch train time: 0:00:00.577157
elapsed time: 0:01:29.043685
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:14:48.957943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.07
 ---- batch: 020 ----
mean loss: 162.98
 ---- batch: 030 ----
mean loss: 165.22
 ---- batch: 040 ----
mean loss: 159.42
 ---- batch: 050 ----
mean loss: 173.43
 ---- batch: 060 ----
mean loss: 159.30
 ---- batch: 070 ----
mean loss: 164.25
 ---- batch: 080 ----
mean loss: 170.13
 ---- batch: 090 ----
mean loss: 165.86
 ---- batch: 100 ----
mean loss: 158.29
 ---- batch: 110 ----
mean loss: 170.27
train mean loss: 165.60
epoch train time: 0:00:00.558248
elapsed time: 0:01:29.602074
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:14:49.516324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.51
 ---- batch: 020 ----
mean loss: 164.97
 ---- batch: 030 ----
mean loss: 166.05
 ---- batch: 040 ----
mean loss: 167.09
 ---- batch: 050 ----
mean loss: 163.49
 ---- batch: 060 ----
mean loss: 170.72
 ---- batch: 070 ----
mean loss: 165.54
 ---- batch: 080 ----
mean loss: 172.30
 ---- batch: 090 ----
mean loss: 167.88
 ---- batch: 100 ----
mean loss: 167.21
 ---- batch: 110 ----
mean loss: 162.67
train mean loss: 166.95
epoch train time: 0:00:00.551525
elapsed time: 0:01:30.153780
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:14:50.068028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.79
 ---- batch: 020 ----
mean loss: 160.62
 ---- batch: 030 ----
mean loss: 164.20
 ---- batch: 040 ----
mean loss: 166.63
 ---- batch: 050 ----
mean loss: 159.10
 ---- batch: 060 ----
mean loss: 170.56
 ---- batch: 070 ----
mean loss: 160.57
 ---- batch: 080 ----
mean loss: 162.13
 ---- batch: 090 ----
mean loss: 160.42
 ---- batch: 100 ----
mean loss: 168.64
 ---- batch: 110 ----
mean loss: 171.32
train mean loss: 165.33
epoch train time: 0:00:00.547989
elapsed time: 0:01:30.701898
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:14:50.616149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.98
 ---- batch: 020 ----
mean loss: 162.73
 ---- batch: 030 ----
mean loss: 166.50
 ---- batch: 040 ----
mean loss: 165.59
 ---- batch: 050 ----
mean loss: 163.91
 ---- batch: 060 ----
mean loss: 164.25
 ---- batch: 070 ----
mean loss: 162.24
 ---- batch: 080 ----
mean loss: 161.34
 ---- batch: 090 ----
mean loss: 166.03
 ---- batch: 100 ----
mean loss: 169.70
 ---- batch: 110 ----
mean loss: 175.87
train mean loss: 164.70
epoch train time: 0:00:00.549346
elapsed time: 0:01:31.251439
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:14:51.165714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.62
 ---- batch: 020 ----
mean loss: 162.78
 ---- batch: 030 ----
mean loss: 164.73
 ---- batch: 040 ----
mean loss: 156.90
 ---- batch: 050 ----
mean loss: 159.71
 ---- batch: 060 ----
mean loss: 167.97
 ---- batch: 070 ----
mean loss: 166.91
 ---- batch: 080 ----
mean loss: 166.08
 ---- batch: 090 ----
mean loss: 167.78
 ---- batch: 100 ----
mean loss: 170.45
 ---- batch: 110 ----
mean loss: 161.15
train mean loss: 164.80
epoch train time: 0:00:00.555882
elapsed time: 0:01:31.807548
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:14:51.721794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.69
 ---- batch: 020 ----
mean loss: 171.77
 ---- batch: 030 ----
mean loss: 165.08
 ---- batch: 040 ----
mean loss: 163.83
 ---- batch: 050 ----
mean loss: 164.82
 ---- batch: 060 ----
mean loss: 161.10
 ---- batch: 070 ----
mean loss: 161.14
 ---- batch: 080 ----
mean loss: 165.58
 ---- batch: 090 ----
mean loss: 167.84
 ---- batch: 100 ----
mean loss: 158.54
 ---- batch: 110 ----
mean loss: 166.12
train mean loss: 163.89
epoch train time: 0:00:00.571964
elapsed time: 0:01:32.379644
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:14:52.293898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.90
 ---- batch: 020 ----
mean loss: 165.82
 ---- batch: 030 ----
mean loss: 165.84
 ---- batch: 040 ----
mean loss: 161.63
 ---- batch: 050 ----
mean loss: 168.98
 ---- batch: 060 ----
mean loss: 158.19
 ---- batch: 070 ----
mean loss: 171.12
 ---- batch: 080 ----
mean loss: 166.87
 ---- batch: 090 ----
mean loss: 165.77
 ---- batch: 100 ----
mean loss: 156.08
 ---- batch: 110 ----
mean loss: 166.99
train mean loss: 164.18
epoch train time: 0:00:00.567009
elapsed time: 0:01:32.946810
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:14:52.861078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.73
 ---- batch: 020 ----
mean loss: 166.08
 ---- batch: 030 ----
mean loss: 150.64
 ---- batch: 040 ----
mean loss: 160.81
 ---- batch: 050 ----
mean loss: 167.75
 ---- batch: 060 ----
mean loss: 158.18
 ---- batch: 070 ----
mean loss: 166.98
 ---- batch: 080 ----
mean loss: 165.85
 ---- batch: 090 ----
mean loss: 166.30
 ---- batch: 100 ----
mean loss: 172.21
 ---- batch: 110 ----
mean loss: 166.88
train mean loss: 163.78
epoch train time: 0:00:00.547658
elapsed time: 0:01:33.494616
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:14:53.408866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.05
 ---- batch: 020 ----
mean loss: 156.68
 ---- batch: 030 ----
mean loss: 163.45
 ---- batch: 040 ----
mean loss: 159.50
 ---- batch: 050 ----
mean loss: 162.50
 ---- batch: 060 ----
mean loss: 160.73
 ---- batch: 070 ----
mean loss: 170.57
 ---- batch: 080 ----
mean loss: 166.57
 ---- batch: 090 ----
mean loss: 160.20
 ---- batch: 100 ----
mean loss: 162.23
 ---- batch: 110 ----
mean loss: 164.74
train mean loss: 162.21
epoch train time: 0:00:00.555327
elapsed time: 0:01:34.050074
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:14:53.964323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.10
 ---- batch: 020 ----
mean loss: 159.76
 ---- batch: 030 ----
mean loss: 157.22
 ---- batch: 040 ----
mean loss: 152.00
 ---- batch: 050 ----
mean loss: 168.48
 ---- batch: 060 ----
mean loss: 162.79
 ---- batch: 070 ----
mean loss: 165.73
 ---- batch: 080 ----
mean loss: 164.15
 ---- batch: 090 ----
mean loss: 158.98
 ---- batch: 100 ----
mean loss: 160.22
 ---- batch: 110 ----
mean loss: 166.17
train mean loss: 161.89
epoch train time: 0:00:00.544836
elapsed time: 0:01:34.595038
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:14:54.509286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.71
 ---- batch: 020 ----
mean loss: 157.02
 ---- batch: 030 ----
mean loss: 161.71
 ---- batch: 040 ----
mean loss: 162.62
 ---- batch: 050 ----
mean loss: 158.03
 ---- batch: 060 ----
mean loss: 162.65
 ---- batch: 070 ----
mean loss: 163.95
 ---- batch: 080 ----
mean loss: 170.15
 ---- batch: 090 ----
mean loss: 168.75
 ---- batch: 100 ----
mean loss: 156.06
 ---- batch: 110 ----
mean loss: 164.17
train mean loss: 162.75
epoch train time: 0:00:00.567164
elapsed time: 0:01:35.162366
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:14:55.076638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.45
 ---- batch: 020 ----
mean loss: 153.96
 ---- batch: 030 ----
mean loss: 161.37
 ---- batch: 040 ----
mean loss: 153.86
 ---- batch: 050 ----
mean loss: 161.99
 ---- batch: 060 ----
mean loss: 161.08
 ---- batch: 070 ----
mean loss: 157.52
 ---- batch: 080 ----
mean loss: 167.05
 ---- batch: 090 ----
mean loss: 166.36
 ---- batch: 100 ----
mean loss: 164.24
 ---- batch: 110 ----
mean loss: 169.94
train mean loss: 162.09
epoch train time: 0:00:00.556558
elapsed time: 0:01:35.719083
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:14:55.633339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.04
 ---- batch: 020 ----
mean loss: 170.86
 ---- batch: 030 ----
mean loss: 157.90
 ---- batch: 040 ----
mean loss: 159.23
 ---- batch: 050 ----
mean loss: 164.72
 ---- batch: 060 ----
mean loss: 173.60
 ---- batch: 070 ----
mean loss: 156.17
 ---- batch: 080 ----
mean loss: 154.41
 ---- batch: 090 ----
mean loss: 160.70
 ---- batch: 100 ----
mean loss: 162.98
 ---- batch: 110 ----
mean loss: 162.50
train mean loss: 162.03
epoch train time: 0:00:00.560483
elapsed time: 0:01:36.279716
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:14:56.193969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.97
 ---- batch: 020 ----
mean loss: 166.00
 ---- batch: 030 ----
mean loss: 158.46
 ---- batch: 040 ----
mean loss: 154.65
 ---- batch: 050 ----
mean loss: 163.24
 ---- batch: 060 ----
mean loss: 159.70
 ---- batch: 070 ----
mean loss: 161.26
 ---- batch: 080 ----
mean loss: 165.70
 ---- batch: 090 ----
mean loss: 159.19
 ---- batch: 100 ----
mean loss: 151.75
 ---- batch: 110 ----
mean loss: 152.81
train mean loss: 160.00
epoch train time: 0:00:00.552228
elapsed time: 0:01:36.832082
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:14:56.746334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.19
 ---- batch: 020 ----
mean loss: 161.29
 ---- batch: 030 ----
mean loss: 159.61
 ---- batch: 040 ----
mean loss: 161.50
 ---- batch: 050 ----
mean loss: 164.08
 ---- batch: 060 ----
mean loss: 164.22
 ---- batch: 070 ----
mean loss: 170.67
 ---- batch: 080 ----
mean loss: 168.64
 ---- batch: 090 ----
mean loss: 158.48
 ---- batch: 100 ----
mean loss: 165.10
 ---- batch: 110 ----
mean loss: 153.91
train mean loss: 162.04
epoch train time: 0:00:00.555976
elapsed time: 0:01:37.388208
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:14:57.302507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.10
 ---- batch: 020 ----
mean loss: 161.17
 ---- batch: 030 ----
mean loss: 160.46
 ---- batch: 040 ----
mean loss: 155.73
 ---- batch: 050 ----
mean loss: 157.51
 ---- batch: 060 ----
mean loss: 159.38
 ---- batch: 070 ----
mean loss: 163.77
 ---- batch: 080 ----
mean loss: 161.65
 ---- batch: 090 ----
mean loss: 160.23
 ---- batch: 100 ----
mean loss: 164.91
 ---- batch: 110 ----
mean loss: 156.20
train mean loss: 160.79
epoch train time: 0:00:00.559174
elapsed time: 0:01:37.947587
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:14:57.861842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.12
 ---- batch: 020 ----
mean loss: 157.40
 ---- batch: 030 ----
mean loss: 157.91
 ---- batch: 040 ----
mean loss: 157.05
 ---- batch: 050 ----
mean loss: 163.54
 ---- batch: 060 ----
mean loss: 156.58
 ---- batch: 070 ----
mean loss: 159.93
 ---- batch: 080 ----
mean loss: 164.24
 ---- batch: 090 ----
mean loss: 163.66
 ---- batch: 100 ----
mean loss: 162.49
 ---- batch: 110 ----
mean loss: 159.80
train mean loss: 159.99
epoch train time: 0:00:00.558396
elapsed time: 0:01:38.506155
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:14:58.420407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.58
 ---- batch: 020 ----
mean loss: 156.76
 ---- batch: 030 ----
mean loss: 154.95
 ---- batch: 040 ----
mean loss: 159.95
 ---- batch: 050 ----
mean loss: 163.41
 ---- batch: 060 ----
mean loss: 162.67
 ---- batch: 070 ----
mean loss: 161.88
 ---- batch: 080 ----
mean loss: 159.59
 ---- batch: 090 ----
mean loss: 165.88
 ---- batch: 100 ----
mean loss: 160.74
 ---- batch: 110 ----
mean loss: 163.72
train mean loss: 160.56
epoch train time: 0:00:00.570093
elapsed time: 0:01:39.076430
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:14:58.990701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.37
 ---- batch: 020 ----
mean loss: 148.74
 ---- batch: 030 ----
mean loss: 164.61
 ---- batch: 040 ----
mean loss: 161.34
 ---- batch: 050 ----
mean loss: 156.56
 ---- batch: 060 ----
mean loss: 153.19
 ---- batch: 070 ----
mean loss: 167.81
 ---- batch: 080 ----
mean loss: 155.47
 ---- batch: 090 ----
mean loss: 167.73
 ---- batch: 100 ----
mean loss: 159.74
 ---- batch: 110 ----
mean loss: 157.77
train mean loss: 159.47
epoch train time: 0:00:00.549923
elapsed time: 0:01:39.626522
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:14:59.540772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.53
 ---- batch: 020 ----
mean loss: 151.49
 ---- batch: 030 ----
mean loss: 157.66
 ---- batch: 040 ----
mean loss: 157.96
 ---- batch: 050 ----
mean loss: 148.25
 ---- batch: 060 ----
mean loss: 155.71
 ---- batch: 070 ----
mean loss: 161.05
 ---- batch: 080 ----
mean loss: 162.75
 ---- batch: 090 ----
mean loss: 162.23
 ---- batch: 100 ----
mean loss: 168.12
 ---- batch: 110 ----
mean loss: 164.28
train mean loss: 158.83
epoch train time: 0:00:00.559163
elapsed time: 0:01:40.185820
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:15:00.100072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.67
 ---- batch: 020 ----
mean loss: 166.34
 ---- batch: 030 ----
mean loss: 153.84
 ---- batch: 040 ----
mean loss: 156.62
 ---- batch: 050 ----
mean loss: 162.73
 ---- batch: 060 ----
mean loss: 162.52
 ---- batch: 070 ----
mean loss: 166.38
 ---- batch: 080 ----
mean loss: 161.56
 ---- batch: 090 ----
mean loss: 156.31
 ---- batch: 100 ----
mean loss: 159.13
 ---- batch: 110 ----
mean loss: 153.72
train mean loss: 159.94
epoch train time: 0:00:00.564358
elapsed time: 0:01:40.750323
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:15:00.664584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.49
 ---- batch: 020 ----
mean loss: 150.66
 ---- batch: 030 ----
mean loss: 155.21
 ---- batch: 040 ----
mean loss: 162.17
 ---- batch: 050 ----
mean loss: 156.67
 ---- batch: 060 ----
mean loss: 156.93
 ---- batch: 070 ----
mean loss: 157.62
 ---- batch: 080 ----
mean loss: 160.17
 ---- batch: 090 ----
mean loss: 166.49
 ---- batch: 100 ----
mean loss: 164.02
 ---- batch: 110 ----
mean loss: 152.86
train mean loss: 157.86
epoch train time: 0:00:00.560168
elapsed time: 0:01:41.310635
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:15:01.224901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.50
 ---- batch: 020 ----
mean loss: 147.00
 ---- batch: 030 ----
mean loss: 159.19
 ---- batch: 040 ----
mean loss: 158.29
 ---- batch: 050 ----
mean loss: 164.03
 ---- batch: 060 ----
mean loss: 166.74
 ---- batch: 070 ----
mean loss: 169.21
 ---- batch: 080 ----
mean loss: 151.80
 ---- batch: 090 ----
mean loss: 158.79
 ---- batch: 100 ----
mean loss: 158.79
 ---- batch: 110 ----
mean loss: 155.82
train mean loss: 158.72
epoch train time: 0:00:00.553192
elapsed time: 0:01:41.863991
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:15:01.778241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.23
 ---- batch: 020 ----
mean loss: 151.83
 ---- batch: 030 ----
mean loss: 161.72
 ---- batch: 040 ----
mean loss: 160.87
 ---- batch: 050 ----
mean loss: 154.31
 ---- batch: 060 ----
mean loss: 160.05
 ---- batch: 070 ----
mean loss: 167.36
 ---- batch: 080 ----
mean loss: 161.90
 ---- batch: 090 ----
mean loss: 154.81
 ---- batch: 100 ----
mean loss: 157.27
 ---- batch: 110 ----
mean loss: 161.10
train mean loss: 158.97
epoch train time: 0:00:00.551715
elapsed time: 0:01:42.415840
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:15:02.330110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.99
 ---- batch: 020 ----
mean loss: 149.64
 ---- batch: 030 ----
mean loss: 156.58
 ---- batch: 040 ----
mean loss: 162.48
 ---- batch: 050 ----
mean loss: 156.98
 ---- batch: 060 ----
mean loss: 155.38
 ---- batch: 070 ----
mean loss: 154.81
 ---- batch: 080 ----
mean loss: 165.04
 ---- batch: 090 ----
mean loss: 158.25
 ---- batch: 100 ----
mean loss: 156.80
 ---- batch: 110 ----
mean loss: 158.01
train mean loss: 157.33
epoch train time: 0:00:00.552279
elapsed time: 0:01:42.968305
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:15:02.882549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.08
 ---- batch: 020 ----
mean loss: 153.98
 ---- batch: 030 ----
mean loss: 157.62
 ---- batch: 040 ----
mean loss: 140.80
 ---- batch: 050 ----
mean loss: 171.51
 ---- batch: 060 ----
mean loss: 158.14
 ---- batch: 070 ----
mean loss: 155.54
 ---- batch: 080 ----
mean loss: 157.97
 ---- batch: 090 ----
mean loss: 162.41
 ---- batch: 100 ----
mean loss: 156.53
 ---- batch: 110 ----
mean loss: 160.52
train mean loss: 157.76
epoch train time: 0:00:00.555696
elapsed time: 0:01:43.524153
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:15:03.438522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.44
 ---- batch: 020 ----
mean loss: 144.88
 ---- batch: 030 ----
mean loss: 156.83
 ---- batch: 040 ----
mean loss: 157.08
 ---- batch: 050 ----
mean loss: 161.55
 ---- batch: 060 ----
mean loss: 156.89
 ---- batch: 070 ----
mean loss: 159.28
 ---- batch: 080 ----
mean loss: 164.80
 ---- batch: 090 ----
mean loss: 151.92
 ---- batch: 100 ----
mean loss: 166.32
 ---- batch: 110 ----
mean loss: 148.53
train mean loss: 157.60
epoch train time: 0:00:00.558143
elapsed time: 0:01:44.082566
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:15:03.996818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.23
 ---- batch: 020 ----
mean loss: 163.48
 ---- batch: 030 ----
mean loss: 151.97
 ---- batch: 040 ----
mean loss: 156.27
 ---- batch: 050 ----
mean loss: 155.95
 ---- batch: 060 ----
mean loss: 159.54
 ---- batch: 070 ----
mean loss: 150.57
 ---- batch: 080 ----
mean loss: 157.59
 ---- batch: 090 ----
mean loss: 155.99
 ---- batch: 100 ----
mean loss: 157.32
 ---- batch: 110 ----
mean loss: 159.59
train mean loss: 156.90
epoch train time: 0:00:00.547828
elapsed time: 0:01:44.630527
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:15:04.544776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.25
 ---- batch: 020 ----
mean loss: 152.85
 ---- batch: 030 ----
mean loss: 155.69
 ---- batch: 040 ----
mean loss: 160.00
 ---- batch: 050 ----
mean loss: 145.15
 ---- batch: 060 ----
mean loss: 154.27
 ---- batch: 070 ----
mean loss: 163.58
 ---- batch: 080 ----
mean loss: 161.34
 ---- batch: 090 ----
mean loss: 161.96
 ---- batch: 100 ----
mean loss: 154.69
 ---- batch: 110 ----
mean loss: 156.91
train mean loss: 156.47
epoch train time: 0:00:00.561237
elapsed time: 0:01:45.191900
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:15:05.106153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.13
 ---- batch: 020 ----
mean loss: 155.89
 ---- batch: 030 ----
mean loss: 157.85
 ---- batch: 040 ----
mean loss: 159.41
 ---- batch: 050 ----
mean loss: 154.02
 ---- batch: 060 ----
mean loss: 156.18
 ---- batch: 070 ----
mean loss: 161.62
 ---- batch: 080 ----
mean loss: 154.77
 ---- batch: 090 ----
mean loss: 153.24
 ---- batch: 100 ----
mean loss: 157.66
 ---- batch: 110 ----
mean loss: 161.29
train mean loss: 156.85
epoch train time: 0:00:00.556469
elapsed time: 0:01:45.748510
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:15:05.662761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.97
 ---- batch: 020 ----
mean loss: 158.43
 ---- batch: 030 ----
mean loss: 152.16
 ---- batch: 040 ----
mean loss: 157.91
 ---- batch: 050 ----
mean loss: 154.39
 ---- batch: 060 ----
mean loss: 159.82
 ---- batch: 070 ----
mean loss: 160.34
 ---- batch: 080 ----
mean loss: 148.35
 ---- batch: 090 ----
mean loss: 147.87
 ---- batch: 100 ----
mean loss: 154.28
 ---- batch: 110 ----
mean loss: 154.72
train mean loss: 155.19
epoch train time: 0:00:00.565647
elapsed time: 0:01:46.314291
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:15:06.228574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.38
 ---- batch: 020 ----
mean loss: 151.69
 ---- batch: 030 ----
mean loss: 153.09
 ---- batch: 040 ----
mean loss: 150.41
 ---- batch: 050 ----
mean loss: 156.18
 ---- batch: 060 ----
mean loss: 149.94
 ---- batch: 070 ----
mean loss: 154.77
 ---- batch: 080 ----
mean loss: 156.11
 ---- batch: 090 ----
mean loss: 168.22
 ---- batch: 100 ----
mean loss: 149.10
 ---- batch: 110 ----
mean loss: 164.53
train mean loss: 156.51
epoch train time: 0:00:00.553421
elapsed time: 0:01:46.867879
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:15:06.782129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.00
 ---- batch: 020 ----
mean loss: 160.59
 ---- batch: 030 ----
mean loss: 165.46
 ---- batch: 040 ----
mean loss: 157.21
 ---- batch: 050 ----
mean loss: 158.98
 ---- batch: 060 ----
mean loss: 154.83
 ---- batch: 070 ----
mean loss: 155.59
 ---- batch: 080 ----
mean loss: 149.94
 ---- batch: 090 ----
mean loss: 151.40
 ---- batch: 100 ----
mean loss: 159.65
 ---- batch: 110 ----
mean loss: 153.63
train mean loss: 156.32
epoch train time: 0:00:00.558573
elapsed time: 0:01:47.426590
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:15:07.340890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.88
 ---- batch: 020 ----
mean loss: 159.62
 ---- batch: 030 ----
mean loss: 155.84
 ---- batch: 040 ----
mean loss: 160.85
 ---- batch: 050 ----
mean loss: 150.55
 ---- batch: 060 ----
mean loss: 159.22
 ---- batch: 070 ----
mean loss: 156.31
 ---- batch: 080 ----
mean loss: 150.79
 ---- batch: 090 ----
mean loss: 154.18
 ---- batch: 100 ----
mean loss: 152.05
 ---- batch: 110 ----
mean loss: 157.78
train mean loss: 155.34
epoch train time: 0:00:00.569604
elapsed time: 0:01:47.996378
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:15:07.910630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.10
 ---- batch: 020 ----
mean loss: 157.71
 ---- batch: 030 ----
mean loss: 157.18
 ---- batch: 040 ----
mean loss: 148.02
 ---- batch: 050 ----
mean loss: 152.83
 ---- batch: 060 ----
mean loss: 154.06
 ---- batch: 070 ----
mean loss: 158.39
 ---- batch: 080 ----
mean loss: 154.25
 ---- batch: 090 ----
mean loss: 156.75
 ---- batch: 100 ----
mean loss: 150.78
 ---- batch: 110 ----
mean loss: 153.73
train mean loss: 155.12
epoch train time: 0:00:00.558105
elapsed time: 0:01:48.554616
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:15:08.468866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.19
 ---- batch: 020 ----
mean loss: 153.03
 ---- batch: 030 ----
mean loss: 150.56
 ---- batch: 040 ----
mean loss: 155.29
 ---- batch: 050 ----
mean loss: 154.66
 ---- batch: 060 ----
mean loss: 155.77
 ---- batch: 070 ----
mean loss: 150.53
 ---- batch: 080 ----
mean loss: 156.02
 ---- batch: 090 ----
mean loss: 145.76
 ---- batch: 100 ----
mean loss: 161.52
 ---- batch: 110 ----
mean loss: 165.03
train mean loss: 153.56
epoch train time: 0:00:00.554201
elapsed time: 0:01:49.108976
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:15:09.023239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.90
 ---- batch: 020 ----
mean loss: 159.39
 ---- batch: 030 ----
mean loss: 143.75
 ---- batch: 040 ----
mean loss: 162.34
 ---- batch: 050 ----
mean loss: 149.70
 ---- batch: 060 ----
mean loss: 157.50
 ---- batch: 070 ----
mean loss: 146.91
 ---- batch: 080 ----
mean loss: 158.24
 ---- batch: 090 ----
mean loss: 149.31
 ---- batch: 100 ----
mean loss: 158.57
 ---- batch: 110 ----
mean loss: 152.13
train mean loss: 154.43
epoch train time: 0:00:00.554830
elapsed time: 0:01:49.663967
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:15:09.578235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.81
 ---- batch: 020 ----
mean loss: 155.89
 ---- batch: 030 ----
mean loss: 156.76
 ---- batch: 040 ----
mean loss: 163.02
 ---- batch: 050 ----
mean loss: 158.66
 ---- batch: 060 ----
mean loss: 160.33
 ---- batch: 070 ----
mean loss: 150.19
 ---- batch: 080 ----
mean loss: 147.46
 ---- batch: 090 ----
mean loss: 154.84
 ---- batch: 100 ----
mean loss: 145.57
 ---- batch: 110 ----
mean loss: 157.57
train mean loss: 154.07
epoch train time: 0:00:00.554739
elapsed time: 0:01:50.218857
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:15:10.133107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.60
 ---- batch: 020 ----
mean loss: 150.04
 ---- batch: 030 ----
mean loss: 156.27
 ---- batch: 040 ----
mean loss: 158.09
 ---- batch: 050 ----
mean loss: 154.99
 ---- batch: 060 ----
mean loss: 147.59
 ---- batch: 070 ----
mean loss: 154.89
 ---- batch: 080 ----
mean loss: 158.24
 ---- batch: 090 ----
mean loss: 161.95
 ---- batch: 100 ----
mean loss: 153.74
 ---- batch: 110 ----
mean loss: 150.76
train mean loss: 153.87
epoch train time: 0:00:00.554936
elapsed time: 0:01:50.773927
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:15:10.688178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.90
 ---- batch: 020 ----
mean loss: 156.20
 ---- batch: 030 ----
mean loss: 153.60
 ---- batch: 040 ----
mean loss: 148.39
 ---- batch: 050 ----
mean loss: 156.03
 ---- batch: 060 ----
mean loss: 150.49
 ---- batch: 070 ----
mean loss: 154.96
 ---- batch: 080 ----
mean loss: 152.93
 ---- batch: 090 ----
mean loss: 153.88
 ---- batch: 100 ----
mean loss: 149.98
 ---- batch: 110 ----
mean loss: 148.50
train mean loss: 153.24
epoch train time: 0:00:00.557250
elapsed time: 0:01:51.331321
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:15:11.245573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.60
 ---- batch: 020 ----
mean loss: 159.03
 ---- batch: 030 ----
mean loss: 147.92
 ---- batch: 040 ----
mean loss: 152.19
 ---- batch: 050 ----
mean loss: 149.87
 ---- batch: 060 ----
mean loss: 151.22
 ---- batch: 070 ----
mean loss: 149.41
 ---- batch: 080 ----
mean loss: 158.62
 ---- batch: 090 ----
mean loss: 148.09
 ---- batch: 100 ----
mean loss: 160.23
 ---- batch: 110 ----
mean loss: 163.32
train mean loss: 153.59
epoch train time: 0:00:00.553484
elapsed time: 0:01:51.884958
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:15:11.799209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.64
 ---- batch: 020 ----
mean loss: 144.75
 ---- batch: 030 ----
mean loss: 147.57
 ---- batch: 040 ----
mean loss: 154.09
 ---- batch: 050 ----
mean loss: 156.72
 ---- batch: 060 ----
mean loss: 146.47
 ---- batch: 070 ----
mean loss: 158.49
 ---- batch: 080 ----
mean loss: 162.96
 ---- batch: 090 ----
mean loss: 162.26
 ---- batch: 100 ----
mean loss: 158.27
 ---- batch: 110 ----
mean loss: 145.16
train mean loss: 153.35
epoch train time: 0:00:00.553105
elapsed time: 0:01:52.438223
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:15:12.352489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.75
 ---- batch: 020 ----
mean loss: 149.67
 ---- batch: 030 ----
mean loss: 154.71
 ---- batch: 040 ----
mean loss: 156.18
 ---- batch: 050 ----
mean loss: 150.02
 ---- batch: 060 ----
mean loss: 153.79
 ---- batch: 070 ----
mean loss: 147.03
 ---- batch: 080 ----
mean loss: 152.73
 ---- batch: 090 ----
mean loss: 152.69
 ---- batch: 100 ----
mean loss: 154.71
 ---- batch: 110 ----
mean loss: 156.97
train mean loss: 152.83
epoch train time: 0:00:00.564196
elapsed time: 0:01:53.002584
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:15:12.916836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.39
 ---- batch: 020 ----
mean loss: 157.52
 ---- batch: 030 ----
mean loss: 154.16
 ---- batch: 040 ----
mean loss: 145.88
 ---- batch: 050 ----
mean loss: 158.85
 ---- batch: 060 ----
mean loss: 156.82
 ---- batch: 070 ----
mean loss: 152.03
 ---- batch: 080 ----
mean loss: 154.66
 ---- batch: 090 ----
mean loss: 154.10
 ---- batch: 100 ----
mean loss: 147.18
 ---- batch: 110 ----
mean loss: 151.14
train mean loss: 152.51
epoch train time: 0:00:00.547976
elapsed time: 0:01:53.550739
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:15:13.465009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.03
 ---- batch: 020 ----
mean loss: 151.24
 ---- batch: 030 ----
mean loss: 146.49
 ---- batch: 040 ----
mean loss: 150.09
 ---- batch: 050 ----
mean loss: 150.31
 ---- batch: 060 ----
mean loss: 153.32
 ---- batch: 070 ----
mean loss: 156.60
 ---- batch: 080 ----
mean loss: 148.93
 ---- batch: 090 ----
mean loss: 150.69
 ---- batch: 100 ----
mean loss: 152.65
 ---- batch: 110 ----
mean loss: 155.65
train mean loss: 151.79
epoch train time: 0:00:00.550942
elapsed time: 0:01:54.101840
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:15:14.016091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.80
 ---- batch: 020 ----
mean loss: 151.90
 ---- batch: 030 ----
mean loss: 149.45
 ---- batch: 040 ----
mean loss: 149.63
 ---- batch: 050 ----
mean loss: 154.35
 ---- batch: 060 ----
mean loss: 151.56
 ---- batch: 070 ----
mean loss: 153.55
 ---- batch: 080 ----
mean loss: 149.23
 ---- batch: 090 ----
mean loss: 156.90
 ---- batch: 100 ----
mean loss: 160.33
 ---- batch: 110 ----
mean loss: 149.66
train mean loss: 152.26
epoch train time: 0:00:00.558496
elapsed time: 0:01:54.660470
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:15:14.574722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.69
 ---- batch: 020 ----
mean loss: 156.63
 ---- batch: 030 ----
mean loss: 157.50
 ---- batch: 040 ----
mean loss: 147.93
 ---- batch: 050 ----
mean loss: 149.38
 ---- batch: 060 ----
mean loss: 148.19
 ---- batch: 070 ----
mean loss: 155.01
 ---- batch: 080 ----
mean loss: 156.68
 ---- batch: 090 ----
mean loss: 151.29
 ---- batch: 100 ----
mean loss: 154.07
 ---- batch: 110 ----
mean loss: 149.51
train mean loss: 151.68
epoch train time: 0:00:00.553205
elapsed time: 0:01:55.213836
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:15:15.128077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.53
 ---- batch: 020 ----
mean loss: 148.93
 ---- batch: 030 ----
mean loss: 146.34
 ---- batch: 040 ----
mean loss: 144.14
 ---- batch: 050 ----
mean loss: 144.45
 ---- batch: 060 ----
mean loss: 153.07
 ---- batch: 070 ----
mean loss: 152.12
 ---- batch: 080 ----
mean loss: 154.86
 ---- batch: 090 ----
mean loss: 153.09
 ---- batch: 100 ----
mean loss: 158.03
 ---- batch: 110 ----
mean loss: 153.63
train mean loss: 151.61
epoch train time: 0:00:00.561786
elapsed time: 0:01:55.775746
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:15:15.690011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.70
 ---- batch: 020 ----
mean loss: 143.02
 ---- batch: 030 ----
mean loss: 154.75
 ---- batch: 040 ----
mean loss: 148.48
 ---- batch: 050 ----
mean loss: 152.17
 ---- batch: 060 ----
mean loss: 157.54
 ---- batch: 070 ----
mean loss: 147.84
 ---- batch: 080 ----
mean loss: 157.85
 ---- batch: 090 ----
mean loss: 157.71
 ---- batch: 100 ----
mean loss: 147.36
 ---- batch: 110 ----
mean loss: 148.85
train mean loss: 151.26
epoch train time: 0:00:00.559416
elapsed time: 0:01:56.335319
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:15:16.249591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.48
 ---- batch: 020 ----
mean loss: 144.81
 ---- batch: 030 ----
mean loss: 154.09
 ---- batch: 040 ----
mean loss: 147.66
 ---- batch: 050 ----
mean loss: 147.36
 ---- batch: 060 ----
mean loss: 155.10
 ---- batch: 070 ----
mean loss: 157.51
 ---- batch: 080 ----
mean loss: 150.34
 ---- batch: 090 ----
mean loss: 152.73
 ---- batch: 100 ----
mean loss: 157.98
 ---- batch: 110 ----
mean loss: 147.45
train mean loss: 151.53
epoch train time: 0:00:00.563063
elapsed time: 0:01:56.898540
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:15:16.812790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.78
 ---- batch: 020 ----
mean loss: 147.42
 ---- batch: 030 ----
mean loss: 148.31
 ---- batch: 040 ----
mean loss: 144.44
 ---- batch: 050 ----
mean loss: 150.38
 ---- batch: 060 ----
mean loss: 147.73
 ---- batch: 070 ----
mean loss: 146.46
 ---- batch: 080 ----
mean loss: 157.99
 ---- batch: 090 ----
mean loss: 155.73
 ---- batch: 100 ----
mean loss: 152.33
 ---- batch: 110 ----
mean loss: 148.48
train mean loss: 150.76
epoch train time: 0:00:00.551403
elapsed time: 0:01:57.450072
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:15:17.364340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.87
 ---- batch: 020 ----
mean loss: 138.25
 ---- batch: 030 ----
mean loss: 148.74
 ---- batch: 040 ----
mean loss: 151.06
 ---- batch: 050 ----
mean loss: 155.81
 ---- batch: 060 ----
mean loss: 154.35
 ---- batch: 070 ----
mean loss: 156.69
 ---- batch: 080 ----
mean loss: 151.22
 ---- batch: 090 ----
mean loss: 143.93
 ---- batch: 100 ----
mean loss: 148.28
 ---- batch: 110 ----
mean loss: 153.28
train mean loss: 150.59
epoch train time: 0:00:00.554114
elapsed time: 0:01:58.004384
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:15:17.918634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.84
 ---- batch: 020 ----
mean loss: 143.83
 ---- batch: 030 ----
mean loss: 147.41
 ---- batch: 040 ----
mean loss: 151.60
 ---- batch: 050 ----
mean loss: 147.55
 ---- batch: 060 ----
mean loss: 153.00
 ---- batch: 070 ----
mean loss: 148.77
 ---- batch: 080 ----
mean loss: 147.16
 ---- batch: 090 ----
mean loss: 154.71
 ---- batch: 100 ----
mean loss: 148.66
 ---- batch: 110 ----
mean loss: 155.88
train mean loss: 149.50
epoch train time: 0:00:00.548688
elapsed time: 0:01:58.553240
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:15:18.467500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.98
 ---- batch: 020 ----
mean loss: 146.62
 ---- batch: 030 ----
mean loss: 152.01
 ---- batch: 040 ----
mean loss: 147.58
 ---- batch: 050 ----
mean loss: 147.63
 ---- batch: 060 ----
mean loss: 149.68
 ---- batch: 070 ----
mean loss: 159.44
 ---- batch: 080 ----
mean loss: 150.55
 ---- batch: 090 ----
mean loss: 149.78
 ---- batch: 100 ----
mean loss: 151.35
 ---- batch: 110 ----
mean loss: 158.75
train mean loss: 150.99
epoch train time: 0:00:00.564870
elapsed time: 0:01:59.118255
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:15:19.032503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.21
 ---- batch: 020 ----
mean loss: 147.13
 ---- batch: 030 ----
mean loss: 156.14
 ---- batch: 040 ----
mean loss: 161.22
 ---- batch: 050 ----
mean loss: 151.57
 ---- batch: 060 ----
mean loss: 153.16
 ---- batch: 070 ----
mean loss: 150.73
 ---- batch: 080 ----
mean loss: 148.08
 ---- batch: 090 ----
mean loss: 145.81
 ---- batch: 100 ----
mean loss: 152.96
 ---- batch: 110 ----
mean loss: 150.68
train mean loss: 151.10
epoch train time: 0:00:00.558023
elapsed time: 0:01:59.676411
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:15:19.590680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.73
 ---- batch: 020 ----
mean loss: 146.83
 ---- batch: 030 ----
mean loss: 152.64
 ---- batch: 040 ----
mean loss: 149.03
 ---- batch: 050 ----
mean loss: 138.80
 ---- batch: 060 ----
mean loss: 156.54
 ---- batch: 070 ----
mean loss: 141.99
 ---- batch: 080 ----
mean loss: 146.36
 ---- batch: 090 ----
mean loss: 160.35
 ---- batch: 100 ----
mean loss: 146.83
 ---- batch: 110 ----
mean loss: 156.07
train mean loss: 149.11
epoch train time: 0:00:00.560616
elapsed time: 0:02:00.237178
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:15:20.151436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.49
 ---- batch: 020 ----
mean loss: 144.78
 ---- batch: 030 ----
mean loss: 152.79
 ---- batch: 040 ----
mean loss: 144.26
 ---- batch: 050 ----
mean loss: 148.30
 ---- batch: 060 ----
mean loss: 141.58
 ---- batch: 070 ----
mean loss: 151.27
 ---- batch: 080 ----
mean loss: 145.61
 ---- batch: 090 ----
mean loss: 139.60
 ---- batch: 100 ----
mean loss: 156.95
 ---- batch: 110 ----
mean loss: 154.16
train mean loss: 148.71
epoch train time: 0:00:00.557873
elapsed time: 0:02:00.795196
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:15:20.709462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.41
 ---- batch: 020 ----
mean loss: 147.80
 ---- batch: 030 ----
mean loss: 141.16
 ---- batch: 040 ----
mean loss: 153.85
 ---- batch: 050 ----
mean loss: 148.93
 ---- batch: 060 ----
mean loss: 143.24
 ---- batch: 070 ----
mean loss: 153.41
 ---- batch: 080 ----
mean loss: 151.61
 ---- batch: 090 ----
mean loss: 146.94
 ---- batch: 100 ----
mean loss: 155.63
 ---- batch: 110 ----
mean loss: 145.84
train mean loss: 149.33
epoch train time: 0:00:00.548710
elapsed time: 0:02:01.344058
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:15:21.258325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.46
 ---- batch: 020 ----
mean loss: 144.19
 ---- batch: 030 ----
mean loss: 144.38
 ---- batch: 040 ----
mean loss: 156.55
 ---- batch: 050 ----
mean loss: 161.62
 ---- batch: 060 ----
mean loss: 145.22
 ---- batch: 070 ----
mean loss: 142.45
 ---- batch: 080 ----
mean loss: 160.37
 ---- batch: 090 ----
mean loss: 154.01
 ---- batch: 100 ----
mean loss: 138.45
 ---- batch: 110 ----
mean loss: 145.40
train mean loss: 148.75
epoch train time: 0:00:00.554143
elapsed time: 0:02:01.898351
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:15:21.812602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.46
 ---- batch: 020 ----
mean loss: 152.20
 ---- batch: 030 ----
mean loss: 147.47
 ---- batch: 040 ----
mean loss: 153.61
 ---- batch: 050 ----
mean loss: 156.92
 ---- batch: 060 ----
mean loss: 146.60
 ---- batch: 070 ----
mean loss: 146.93
 ---- batch: 080 ----
mean loss: 147.30
 ---- batch: 090 ----
mean loss: 145.93
 ---- batch: 100 ----
mean loss: 153.23
 ---- batch: 110 ----
mean loss: 147.79
train mean loss: 148.73
epoch train time: 0:00:00.553057
elapsed time: 0:02:02.451550
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:15:22.365800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.15
 ---- batch: 020 ----
mean loss: 153.86
 ---- batch: 030 ----
mean loss: 151.06
 ---- batch: 040 ----
mean loss: 141.82
 ---- batch: 050 ----
mean loss: 156.26
 ---- batch: 060 ----
mean loss: 149.12
 ---- batch: 070 ----
mean loss: 144.33
 ---- batch: 080 ----
mean loss: 133.90
 ---- batch: 090 ----
mean loss: 144.86
 ---- batch: 100 ----
mean loss: 151.98
 ---- batch: 110 ----
mean loss: 153.16
train mean loss: 148.26
epoch train time: 0:00:00.553755
elapsed time: 0:02:03.005472
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:15:22.919723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.95
 ---- batch: 020 ----
mean loss: 148.10
 ---- batch: 030 ----
mean loss: 154.01
 ---- batch: 040 ----
mean loss: 151.09
 ---- batch: 050 ----
mean loss: 146.09
 ---- batch: 060 ----
mean loss: 147.33
 ---- batch: 070 ----
mean loss: 143.85
 ---- batch: 080 ----
mean loss: 147.81
 ---- batch: 090 ----
mean loss: 142.83
 ---- batch: 100 ----
mean loss: 145.04
 ---- batch: 110 ----
mean loss: 152.53
train mean loss: 148.13
epoch train time: 0:00:00.555809
elapsed time: 0:02:03.561418
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:15:23.475669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.88
 ---- batch: 020 ----
mean loss: 153.09
 ---- batch: 030 ----
mean loss: 149.26
 ---- batch: 040 ----
mean loss: 147.14
 ---- batch: 050 ----
mean loss: 149.48
 ---- batch: 060 ----
mean loss: 144.47
 ---- batch: 070 ----
mean loss: 157.12
 ---- batch: 080 ----
mean loss: 145.83
 ---- batch: 090 ----
mean loss: 138.91
 ---- batch: 100 ----
mean loss: 150.48
 ---- batch: 110 ----
mean loss: 145.67
train mean loss: 147.48
epoch train time: 0:00:00.556875
elapsed time: 0:02:04.118456
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:15:24.032706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.80
 ---- batch: 020 ----
mean loss: 142.18
 ---- batch: 030 ----
mean loss: 147.61
 ---- batch: 040 ----
mean loss: 146.69
 ---- batch: 050 ----
mean loss: 159.87
 ---- batch: 060 ----
mean loss: 147.16
 ---- batch: 070 ----
mean loss: 149.08
 ---- batch: 080 ----
mean loss: 146.23
 ---- batch: 090 ----
mean loss: 152.08
 ---- batch: 100 ----
mean loss: 146.16
 ---- batch: 110 ----
mean loss: 139.53
train mean loss: 148.09
epoch train time: 0:00:00.554566
elapsed time: 0:02:04.673158
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:15:24.587428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.43
 ---- batch: 020 ----
mean loss: 149.86
 ---- batch: 030 ----
mean loss: 143.17
 ---- batch: 040 ----
mean loss: 146.23
 ---- batch: 050 ----
mean loss: 141.82
 ---- batch: 060 ----
mean loss: 147.33
 ---- batch: 070 ----
mean loss: 135.69
 ---- batch: 080 ----
mean loss: 158.29
 ---- batch: 090 ----
mean loss: 149.90
 ---- batch: 100 ----
mean loss: 161.37
 ---- batch: 110 ----
mean loss: 143.40
train mean loss: 147.20
epoch train time: 0:00:00.553662
elapsed time: 0:02:05.226978
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:15:25.141234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.16
 ---- batch: 020 ----
mean loss: 145.08
 ---- batch: 030 ----
mean loss: 145.86
 ---- batch: 040 ----
mean loss: 141.97
 ---- batch: 050 ----
mean loss: 149.60
 ---- batch: 060 ----
mean loss: 142.13
 ---- batch: 070 ----
mean loss: 144.18
 ---- batch: 080 ----
mean loss: 140.23
 ---- batch: 090 ----
mean loss: 154.57
 ---- batch: 100 ----
mean loss: 156.11
 ---- batch: 110 ----
mean loss: 151.45
train mean loss: 147.21
epoch train time: 0:00:00.549376
elapsed time: 0:02:05.776495
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:15:25.690745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.40
 ---- batch: 020 ----
mean loss: 147.51
 ---- batch: 030 ----
mean loss: 141.72
 ---- batch: 040 ----
mean loss: 158.51
 ---- batch: 050 ----
mean loss: 158.91
 ---- batch: 060 ----
mean loss: 136.65
 ---- batch: 070 ----
mean loss: 141.09
 ---- batch: 080 ----
mean loss: 148.13
 ---- batch: 090 ----
mean loss: 148.61
 ---- batch: 100 ----
mean loss: 145.08
 ---- batch: 110 ----
mean loss: 149.34
train mean loss: 146.98
epoch train time: 0:00:00.551804
elapsed time: 0:02:06.328479
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:15:26.242732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.15
 ---- batch: 020 ----
mean loss: 144.86
 ---- batch: 030 ----
mean loss: 135.91
 ---- batch: 040 ----
mean loss: 153.86
 ---- batch: 050 ----
mean loss: 151.36
 ---- batch: 060 ----
mean loss: 149.84
 ---- batch: 070 ----
mean loss: 148.43
 ---- batch: 080 ----
mean loss: 153.69
 ---- batch: 090 ----
mean loss: 147.35
 ---- batch: 100 ----
mean loss: 151.40
 ---- batch: 110 ----
mean loss: 141.59
train mean loss: 147.01
epoch train time: 0:00:00.557856
elapsed time: 0:02:06.886489
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:15:26.800742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.17
 ---- batch: 020 ----
mean loss: 144.61
 ---- batch: 030 ----
mean loss: 145.97
 ---- batch: 040 ----
mean loss: 144.49
 ---- batch: 050 ----
mean loss: 148.73
 ---- batch: 060 ----
mean loss: 149.32
 ---- batch: 070 ----
mean loss: 142.86
 ---- batch: 080 ----
mean loss: 148.51
 ---- batch: 090 ----
mean loss: 154.98
 ---- batch: 100 ----
mean loss: 142.78
 ---- batch: 110 ----
mean loss: 142.98
train mean loss: 146.89
epoch train time: 0:00:00.556449
elapsed time: 0:02:07.443073
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:15:27.357325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.17
 ---- batch: 020 ----
mean loss: 150.80
 ---- batch: 030 ----
mean loss: 154.56
 ---- batch: 040 ----
mean loss: 144.75
 ---- batch: 050 ----
mean loss: 142.38
 ---- batch: 060 ----
mean loss: 141.81
 ---- batch: 070 ----
mean loss: 153.99
 ---- batch: 080 ----
mean loss: 143.34
 ---- batch: 090 ----
mean loss: 151.48
 ---- batch: 100 ----
mean loss: 148.18
 ---- batch: 110 ----
mean loss: 142.92
train mean loss: 146.43
epoch train time: 0:00:00.555423
elapsed time: 0:02:07.998629
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:15:27.912895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.80
 ---- batch: 020 ----
mean loss: 151.51
 ---- batch: 030 ----
mean loss: 144.45
 ---- batch: 040 ----
mean loss: 140.41
 ---- batch: 050 ----
mean loss: 146.22
 ---- batch: 060 ----
mean loss: 143.61
 ---- batch: 070 ----
mean loss: 141.51
 ---- batch: 080 ----
mean loss: 152.37
 ---- batch: 090 ----
mean loss: 149.79
 ---- batch: 100 ----
mean loss: 148.43
 ---- batch: 110 ----
mean loss: 154.61
train mean loss: 146.72
epoch train time: 0:00:00.541749
elapsed time: 0:02:08.540522
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:15:28.454771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.43
 ---- batch: 020 ----
mean loss: 144.02
 ---- batch: 030 ----
mean loss: 149.55
 ---- batch: 040 ----
mean loss: 146.69
 ---- batch: 050 ----
mean loss: 138.43
 ---- batch: 060 ----
mean loss: 147.86
 ---- batch: 070 ----
mean loss: 147.64
 ---- batch: 080 ----
mean loss: 139.65
 ---- batch: 090 ----
mean loss: 151.40
 ---- batch: 100 ----
mean loss: 140.67
 ---- batch: 110 ----
mean loss: 153.74
train mean loss: 146.00
epoch train time: 0:00:00.550861
elapsed time: 0:02:09.091544
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:15:29.005802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.52
 ---- batch: 020 ----
mean loss: 148.04
 ---- batch: 030 ----
mean loss: 141.90
 ---- batch: 040 ----
mean loss: 149.09
 ---- batch: 050 ----
mean loss: 151.14
 ---- batch: 060 ----
mean loss: 141.47
 ---- batch: 070 ----
mean loss: 143.90
 ---- batch: 080 ----
mean loss: 146.06
 ---- batch: 090 ----
mean loss: 149.42
 ---- batch: 100 ----
mean loss: 149.00
 ---- batch: 110 ----
mean loss: 149.34
train mean loss: 146.06
epoch train time: 0:00:00.549672
elapsed time: 0:02:09.641377
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:15:29.555630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.23
 ---- batch: 020 ----
mean loss: 142.51
 ---- batch: 030 ----
mean loss: 142.27
 ---- batch: 040 ----
mean loss: 143.78
 ---- batch: 050 ----
mean loss: 141.89
 ---- batch: 060 ----
mean loss: 145.76
 ---- batch: 070 ----
mean loss: 151.80
 ---- batch: 080 ----
mean loss: 145.45
 ---- batch: 090 ----
mean loss: 145.14
 ---- batch: 100 ----
mean loss: 144.79
 ---- batch: 110 ----
mean loss: 146.36
train mean loss: 145.64
epoch train time: 0:00:00.553187
elapsed time: 0:02:10.194699
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:15:30.108955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.83
 ---- batch: 020 ----
mean loss: 148.80
 ---- batch: 030 ----
mean loss: 138.84
 ---- batch: 040 ----
mean loss: 156.09
 ---- batch: 050 ----
mean loss: 147.93
 ---- batch: 060 ----
mean loss: 143.34
 ---- batch: 070 ----
mean loss: 142.81
 ---- batch: 080 ----
mean loss: 148.72
 ---- batch: 090 ----
mean loss: 137.74
 ---- batch: 100 ----
mean loss: 142.68
 ---- batch: 110 ----
mean loss: 145.91
train mean loss: 145.23
epoch train time: 0:00:00.550442
elapsed time: 0:02:10.745283
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:15:30.659534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.55
 ---- batch: 020 ----
mean loss: 147.97
 ---- batch: 030 ----
mean loss: 147.55
 ---- batch: 040 ----
mean loss: 138.84
 ---- batch: 050 ----
mean loss: 146.49
 ---- batch: 060 ----
mean loss: 150.60
 ---- batch: 070 ----
mean loss: 139.69
 ---- batch: 080 ----
mean loss: 143.25
 ---- batch: 090 ----
mean loss: 148.15
 ---- batch: 100 ----
mean loss: 149.55
 ---- batch: 110 ----
mean loss: 140.92
train mean loss: 145.28
epoch train time: 0:00:00.552343
elapsed time: 0:02:11.297760
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:15:31.212018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.77
 ---- batch: 020 ----
mean loss: 150.32
 ---- batch: 030 ----
mean loss: 147.44
 ---- batch: 040 ----
mean loss: 137.54
 ---- batch: 050 ----
mean loss: 140.51
 ---- batch: 060 ----
mean loss: 150.48
 ---- batch: 070 ----
mean loss: 139.97
 ---- batch: 080 ----
mean loss: 141.79
 ---- batch: 090 ----
mean loss: 148.26
 ---- batch: 100 ----
mean loss: 141.19
 ---- batch: 110 ----
mean loss: 148.20
train mean loss: 144.57
epoch train time: 0:00:00.547700
elapsed time: 0:02:11.845610
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:15:31.759885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.20
 ---- batch: 020 ----
mean loss: 137.97
 ---- batch: 030 ----
mean loss: 140.88
 ---- batch: 040 ----
mean loss: 152.10
 ---- batch: 050 ----
mean loss: 146.57
 ---- batch: 060 ----
mean loss: 140.96
 ---- batch: 070 ----
mean loss: 148.19
 ---- batch: 080 ----
mean loss: 145.44
 ---- batch: 090 ----
mean loss: 143.24
 ---- batch: 100 ----
mean loss: 147.26
 ---- batch: 110 ----
mean loss: 152.73
train mean loss: 144.85
epoch train time: 0:00:00.558950
elapsed time: 0:02:12.404717
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:15:32.318970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.15
 ---- batch: 020 ----
mean loss: 146.80
 ---- batch: 030 ----
mean loss: 137.13
 ---- batch: 040 ----
mean loss: 138.21
 ---- batch: 050 ----
mean loss: 154.12
 ---- batch: 060 ----
mean loss: 146.23
 ---- batch: 070 ----
mean loss: 148.77
 ---- batch: 080 ----
mean loss: 148.58
 ---- batch: 090 ----
mean loss: 141.55
 ---- batch: 100 ----
mean loss: 147.49
 ---- batch: 110 ----
mean loss: 145.60
train mean loss: 144.62
epoch train time: 0:00:00.552723
elapsed time: 0:02:12.957578
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:15:32.871829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.09
 ---- batch: 020 ----
mean loss: 142.97
 ---- batch: 030 ----
mean loss: 149.19
 ---- batch: 040 ----
mean loss: 134.71
 ---- batch: 050 ----
mean loss: 144.97
 ---- batch: 060 ----
mean loss: 137.18
 ---- batch: 070 ----
mean loss: 142.67
 ---- batch: 080 ----
mean loss: 149.07
 ---- batch: 090 ----
mean loss: 148.65
 ---- batch: 100 ----
mean loss: 152.13
 ---- batch: 110 ----
mean loss: 151.82
train mean loss: 144.77
epoch train time: 0:00:00.542957
elapsed time: 0:02:13.500680
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:15:33.414930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.85
 ---- batch: 020 ----
mean loss: 149.14
 ---- batch: 030 ----
mean loss: 148.72
 ---- batch: 040 ----
mean loss: 148.90
 ---- batch: 050 ----
mean loss: 145.13
 ---- batch: 060 ----
mean loss: 149.85
 ---- batch: 070 ----
mean loss: 141.03
 ---- batch: 080 ----
mean loss: 142.52
 ---- batch: 090 ----
mean loss: 145.22
 ---- batch: 100 ----
mean loss: 147.60
 ---- batch: 110 ----
mean loss: 140.65
train mean loss: 146.16
epoch train time: 0:00:00.564737
elapsed time: 0:02:14.065561
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:15:33.979826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.47
 ---- batch: 020 ----
mean loss: 145.85
 ---- batch: 030 ----
mean loss: 136.07
 ---- batch: 040 ----
mean loss: 143.28
 ---- batch: 050 ----
mean loss: 135.56
 ---- batch: 060 ----
mean loss: 139.91
 ---- batch: 070 ----
mean loss: 148.38
 ---- batch: 080 ----
mean loss: 145.71
 ---- batch: 090 ----
mean loss: 157.74
 ---- batch: 100 ----
mean loss: 147.21
 ---- batch: 110 ----
mean loss: 143.96
train mean loss: 144.21
epoch train time: 0:00:00.559546
elapsed time: 0:02:14.625269
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:15:34.539520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.10
 ---- batch: 020 ----
mean loss: 146.19
 ---- batch: 030 ----
mean loss: 142.54
 ---- batch: 040 ----
mean loss: 141.33
 ---- batch: 050 ----
mean loss: 145.75
 ---- batch: 060 ----
mean loss: 145.46
 ---- batch: 070 ----
mean loss: 146.77
 ---- batch: 080 ----
mean loss: 143.14
 ---- batch: 090 ----
mean loss: 135.59
 ---- batch: 100 ----
mean loss: 148.51
 ---- batch: 110 ----
mean loss: 152.92
train mean loss: 144.00
epoch train time: 0:00:00.560818
elapsed time: 0:02:15.186221
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:15:35.100490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.96
 ---- batch: 020 ----
mean loss: 138.40
 ---- batch: 030 ----
mean loss: 149.14
 ---- batch: 040 ----
mean loss: 145.00
 ---- batch: 050 ----
mean loss: 148.40
 ---- batch: 060 ----
mean loss: 148.45
 ---- batch: 070 ----
mean loss: 143.89
 ---- batch: 080 ----
mean loss: 139.39
 ---- batch: 090 ----
mean loss: 143.38
 ---- batch: 100 ----
mean loss: 143.11
 ---- batch: 110 ----
mean loss: 151.81
train mean loss: 144.69
epoch train time: 0:00:00.566686
elapsed time: 0:02:15.753179
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:15:35.667431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.87
 ---- batch: 020 ----
mean loss: 147.21
 ---- batch: 030 ----
mean loss: 151.62
 ---- batch: 040 ----
mean loss: 142.35
 ---- batch: 050 ----
mean loss: 142.61
 ---- batch: 060 ----
mean loss: 144.96
 ---- batch: 070 ----
mean loss: 146.87
 ---- batch: 080 ----
mean loss: 143.71
 ---- batch: 090 ----
mean loss: 144.01
 ---- batch: 100 ----
mean loss: 138.26
 ---- batch: 110 ----
mean loss: 144.11
train mean loss: 144.04
epoch train time: 0:00:00.557860
elapsed time: 0:02:16.311173
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:15:36.225423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.01
 ---- batch: 020 ----
mean loss: 142.28
 ---- batch: 030 ----
mean loss: 137.77
 ---- batch: 040 ----
mean loss: 152.42
 ---- batch: 050 ----
mean loss: 150.26
 ---- batch: 060 ----
mean loss: 153.26
 ---- batch: 070 ----
mean loss: 133.42
 ---- batch: 080 ----
mean loss: 139.37
 ---- batch: 090 ----
mean loss: 148.57
 ---- batch: 100 ----
mean loss: 145.72
 ---- batch: 110 ----
mean loss: 148.22
train mean loss: 144.41
epoch train time: 0:00:00.544807
elapsed time: 0:02:16.856114
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:15:36.770394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.73
 ---- batch: 020 ----
mean loss: 146.66
 ---- batch: 030 ----
mean loss: 148.35
 ---- batch: 040 ----
mean loss: 134.58
 ---- batch: 050 ----
mean loss: 146.45
 ---- batch: 060 ----
mean loss: 139.36
 ---- batch: 070 ----
mean loss: 144.96
 ---- batch: 080 ----
mean loss: 140.30
 ---- batch: 090 ----
mean loss: 150.13
 ---- batch: 100 ----
mean loss: 147.95
 ---- batch: 110 ----
mean loss: 137.10
train mean loss: 143.12
epoch train time: 0:00:00.558857
elapsed time: 0:02:17.415134
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:15:37.329388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.39
 ---- batch: 020 ----
mean loss: 146.40
 ---- batch: 030 ----
mean loss: 155.15
 ---- batch: 040 ----
mean loss: 143.98
 ---- batch: 050 ----
mean loss: 144.06
 ---- batch: 060 ----
mean loss: 142.61
 ---- batch: 070 ----
mean loss: 134.81
 ---- batch: 080 ----
mean loss: 144.60
 ---- batch: 090 ----
mean loss: 143.02
 ---- batch: 100 ----
mean loss: 140.47
 ---- batch: 110 ----
mean loss: 144.25
train mean loss: 143.46
epoch train time: 0:00:00.552038
elapsed time: 0:02:17.967322
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:15:37.881573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.66
 ---- batch: 020 ----
mean loss: 134.19
 ---- batch: 030 ----
mean loss: 142.78
 ---- batch: 040 ----
mean loss: 138.57
 ---- batch: 050 ----
mean loss: 139.56
 ---- batch: 060 ----
mean loss: 152.46
 ---- batch: 070 ----
mean loss: 150.36
 ---- batch: 080 ----
mean loss: 140.18
 ---- batch: 090 ----
mean loss: 141.59
 ---- batch: 100 ----
mean loss: 146.83
 ---- batch: 110 ----
mean loss: 152.01
train mean loss: 143.31
epoch train time: 0:00:00.559920
elapsed time: 0:02:18.527376
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:15:38.441651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.69
 ---- batch: 020 ----
mean loss: 132.74
 ---- batch: 030 ----
mean loss: 141.69
 ---- batch: 040 ----
mean loss: 143.35
 ---- batch: 050 ----
mean loss: 144.47
 ---- batch: 060 ----
mean loss: 148.85
 ---- batch: 070 ----
mean loss: 141.46
 ---- batch: 080 ----
mean loss: 141.18
 ---- batch: 090 ----
mean loss: 143.04
 ---- batch: 100 ----
mean loss: 145.48
 ---- batch: 110 ----
mean loss: 141.66
train mean loss: 142.39
epoch train time: 0:00:00.561279
elapsed time: 0:02:19.088835
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:15:39.003108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.86
 ---- batch: 020 ----
mean loss: 142.45
 ---- batch: 030 ----
mean loss: 144.50
 ---- batch: 040 ----
mean loss: 141.09
 ---- batch: 050 ----
mean loss: 144.97
 ---- batch: 060 ----
mean loss: 146.62
 ---- batch: 070 ----
mean loss: 135.81
 ---- batch: 080 ----
mean loss: 141.97
 ---- batch: 090 ----
mean loss: 136.05
 ---- batch: 100 ----
mean loss: 139.89
 ---- batch: 110 ----
mean loss: 145.00
train mean loss: 142.23
epoch train time: 0:00:00.555377
elapsed time: 0:02:19.644405
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:15:39.558658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.80
 ---- batch: 020 ----
mean loss: 144.15
 ---- batch: 030 ----
mean loss: 139.86
 ---- batch: 040 ----
mean loss: 145.78
 ---- batch: 050 ----
mean loss: 144.58
 ---- batch: 060 ----
mean loss: 138.78
 ---- batch: 070 ----
mean loss: 138.77
 ---- batch: 080 ----
mean loss: 137.22
 ---- batch: 090 ----
mean loss: 134.55
 ---- batch: 100 ----
mean loss: 153.96
 ---- batch: 110 ----
mean loss: 152.88
train mean loss: 143.13
epoch train time: 0:00:00.558008
elapsed time: 0:02:20.202553
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:15:40.116804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.60
 ---- batch: 020 ----
mean loss: 151.48
 ---- batch: 030 ----
mean loss: 143.30
 ---- batch: 040 ----
mean loss: 141.23
 ---- batch: 050 ----
mean loss: 138.09
 ---- batch: 060 ----
mean loss: 132.80
 ---- batch: 070 ----
mean loss: 150.58
 ---- batch: 080 ----
mean loss: 139.56
 ---- batch: 090 ----
mean loss: 144.73
 ---- batch: 100 ----
mean loss: 143.27
 ---- batch: 110 ----
mean loss: 138.81
train mean loss: 141.77
epoch train time: 0:00:00.556470
elapsed time: 0:02:20.759169
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:15:40.673432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.87
 ---- batch: 020 ----
mean loss: 149.57
 ---- batch: 030 ----
mean loss: 148.71
 ---- batch: 040 ----
mean loss: 145.28
 ---- batch: 050 ----
mean loss: 130.98
 ---- batch: 060 ----
mean loss: 136.77
 ---- batch: 070 ----
mean loss: 144.80
 ---- batch: 080 ----
mean loss: 138.97
 ---- batch: 090 ----
mean loss: 144.65
 ---- batch: 100 ----
mean loss: 140.96
 ---- batch: 110 ----
mean loss: 145.73
train mean loss: 141.95
epoch train time: 0:00:00.565360
elapsed time: 0:02:21.324676
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:15:41.238926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.44
 ---- batch: 020 ----
mean loss: 139.32
 ---- batch: 030 ----
mean loss: 141.48
 ---- batch: 040 ----
mean loss: 153.74
 ---- batch: 050 ----
mean loss: 135.40
 ---- batch: 060 ----
mean loss: 136.23
 ---- batch: 070 ----
mean loss: 143.34
 ---- batch: 080 ----
mean loss: 143.41
 ---- batch: 090 ----
mean loss: 142.17
 ---- batch: 100 ----
mean loss: 136.96
 ---- batch: 110 ----
mean loss: 150.10
train mean loss: 142.28
epoch train time: 0:00:00.572662
elapsed time: 0:02:21.897476
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:15:41.811727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.03
 ---- batch: 020 ----
mean loss: 137.57
 ---- batch: 030 ----
mean loss: 142.94
 ---- batch: 040 ----
mean loss: 131.98
 ---- batch: 050 ----
mean loss: 144.53
 ---- batch: 060 ----
mean loss: 133.89
 ---- batch: 070 ----
mean loss: 155.36
 ---- batch: 080 ----
mean loss: 149.18
 ---- batch: 090 ----
mean loss: 141.40
 ---- batch: 100 ----
mean loss: 142.28
 ---- batch: 110 ----
mean loss: 137.74
train mean loss: 141.36
epoch train time: 0:00:00.556417
elapsed time: 0:02:22.454027
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:15:42.368280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.91
 ---- batch: 020 ----
mean loss: 134.91
 ---- batch: 030 ----
mean loss: 142.75
 ---- batch: 040 ----
mean loss: 144.92
 ---- batch: 050 ----
mean loss: 138.30
 ---- batch: 060 ----
mean loss: 149.35
 ---- batch: 070 ----
mean loss: 134.46
 ---- batch: 080 ----
mean loss: 141.75
 ---- batch: 090 ----
mean loss: 139.77
 ---- batch: 100 ----
mean loss: 138.38
 ---- batch: 110 ----
mean loss: 146.54
train mean loss: 141.43
epoch train time: 0:00:00.555293
elapsed time: 0:02:23.009453
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:15:42.923722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.95
 ---- batch: 020 ----
mean loss: 139.16
 ---- batch: 030 ----
mean loss: 146.06
 ---- batch: 040 ----
mean loss: 133.15
 ---- batch: 050 ----
mean loss: 138.58
 ---- batch: 060 ----
mean loss: 151.48
 ---- batch: 070 ----
mean loss: 142.84
 ---- batch: 080 ----
mean loss: 142.58
 ---- batch: 090 ----
mean loss: 137.25
 ---- batch: 100 ----
mean loss: 139.51
 ---- batch: 110 ----
mean loss: 145.23
train mean loss: 141.99
epoch train time: 0:00:00.553185
elapsed time: 0:02:23.562819
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:15:43.477069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.98
 ---- batch: 020 ----
mean loss: 134.92
 ---- batch: 030 ----
mean loss: 139.29
 ---- batch: 040 ----
mean loss: 140.22
 ---- batch: 050 ----
mean loss: 140.80
 ---- batch: 060 ----
mean loss: 142.75
 ---- batch: 070 ----
mean loss: 143.57
 ---- batch: 080 ----
mean loss: 136.62
 ---- batch: 090 ----
mean loss: 147.84
 ---- batch: 100 ----
mean loss: 143.75
 ---- batch: 110 ----
mean loss: 151.47
train mean loss: 141.76
epoch train time: 0:00:00.562681
elapsed time: 0:02:24.125633
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:15:44.039883
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.10
 ---- batch: 020 ----
mean loss: 132.42
 ---- batch: 030 ----
mean loss: 141.60
 ---- batch: 040 ----
mean loss: 131.86
 ---- batch: 050 ----
mean loss: 133.28
 ---- batch: 060 ----
mean loss: 136.92
 ---- batch: 070 ----
mean loss: 132.48
 ---- batch: 080 ----
mean loss: 139.69
 ---- batch: 090 ----
mean loss: 135.19
 ---- batch: 100 ----
mean loss: 139.56
 ---- batch: 110 ----
mean loss: 133.19
train mean loss: 135.73
epoch train time: 0:00:00.548483
elapsed time: 0:02:24.674277
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:15:44.588526
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 143.30
 ---- batch: 020 ----
mean loss: 132.68
 ---- batch: 030 ----
mean loss: 127.66
 ---- batch: 040 ----
mean loss: 136.62
 ---- batch: 050 ----
mean loss: 132.70
 ---- batch: 060 ----
mean loss: 137.67
 ---- batch: 070 ----
mean loss: 129.03
 ---- batch: 080 ----
mean loss: 142.89
 ---- batch: 090 ----
mean loss: 136.05
 ---- batch: 100 ----
mean loss: 133.74
 ---- batch: 110 ----
mean loss: 131.63
train mean loss: 134.85
epoch train time: 0:00:00.551185
elapsed time: 0:02:25.225593
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:15:45.139842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.66
 ---- batch: 020 ----
mean loss: 135.12
 ---- batch: 030 ----
mean loss: 139.46
 ---- batch: 040 ----
mean loss: 134.08
 ---- batch: 050 ----
mean loss: 134.41
 ---- batch: 060 ----
mean loss: 136.84
 ---- batch: 070 ----
mean loss: 130.99
 ---- batch: 080 ----
mean loss: 140.42
 ---- batch: 090 ----
mean loss: 134.69
 ---- batch: 100 ----
mean loss: 132.26
 ---- batch: 110 ----
mean loss: 131.26
train mean loss: 134.57
epoch train time: 0:00:00.580724
elapsed time: 0:02:25.806508
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:15:45.720761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.66
 ---- batch: 020 ----
mean loss: 126.38
 ---- batch: 030 ----
mean loss: 129.91
 ---- batch: 040 ----
mean loss: 128.93
 ---- batch: 050 ----
mean loss: 140.68
 ---- batch: 060 ----
mean loss: 136.04
 ---- batch: 070 ----
mean loss: 141.59
 ---- batch: 080 ----
mean loss: 134.97
 ---- batch: 090 ----
mean loss: 138.81
 ---- batch: 100 ----
mean loss: 129.06
 ---- batch: 110 ----
mean loss: 136.71
train mean loss: 134.42
epoch train time: 0:00:00.659389
elapsed time: 0:02:26.466057
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:15:46.380330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 139.87
 ---- batch: 020 ----
mean loss: 135.45
 ---- batch: 030 ----
mean loss: 132.97
 ---- batch: 040 ----
mean loss: 133.12
 ---- batch: 050 ----
mean loss: 128.55
 ---- batch: 060 ----
mean loss: 134.97
 ---- batch: 070 ----
mean loss: 136.64
 ---- batch: 080 ----
mean loss: 140.94
 ---- batch: 090 ----
mean loss: 128.04
 ---- batch: 100 ----
mean loss: 130.66
 ---- batch: 110 ----
mean loss: 140.39
train mean loss: 134.50
epoch train time: 0:00:00.559762
elapsed time: 0:02:27.025970
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:15:46.940217
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.55
 ---- batch: 020 ----
mean loss: 133.06
 ---- batch: 030 ----
mean loss: 131.93
 ---- batch: 040 ----
mean loss: 135.31
 ---- batch: 050 ----
mean loss: 124.87
 ---- batch: 060 ----
mean loss: 140.23
 ---- batch: 070 ----
mean loss: 139.54
 ---- batch: 080 ----
mean loss: 133.91
 ---- batch: 090 ----
mean loss: 136.59
 ---- batch: 100 ----
mean loss: 128.64
 ---- batch: 110 ----
mean loss: 134.68
train mean loss: 134.34
epoch train time: 0:00:00.546801
elapsed time: 0:02:27.572918
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:15:47.487169
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.47
 ---- batch: 020 ----
mean loss: 136.83
 ---- batch: 030 ----
mean loss: 129.80
 ---- batch: 040 ----
mean loss: 140.57
 ---- batch: 050 ----
mean loss: 133.63
 ---- batch: 060 ----
mean loss: 134.98
 ---- batch: 070 ----
mean loss: 128.31
 ---- batch: 080 ----
mean loss: 139.93
 ---- batch: 090 ----
mean loss: 136.93
 ---- batch: 100 ----
mean loss: 140.08
 ---- batch: 110 ----
mean loss: 130.10
train mean loss: 134.27
epoch train time: 0:00:00.552331
elapsed time: 0:02:28.125412
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:15:48.039662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.92
 ---- batch: 020 ----
mean loss: 128.92
 ---- batch: 030 ----
mean loss: 132.34
 ---- batch: 040 ----
mean loss: 132.23
 ---- batch: 050 ----
mean loss: 138.38
 ---- batch: 060 ----
mean loss: 142.11
 ---- batch: 070 ----
mean loss: 138.66
 ---- batch: 080 ----
mean loss: 134.96
 ---- batch: 090 ----
mean loss: 133.34
 ---- batch: 100 ----
mean loss: 132.33
 ---- batch: 110 ----
mean loss: 135.19
train mean loss: 134.25
epoch train time: 0:00:00.558128
elapsed time: 0:02:28.683676
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:15:48.597936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.75
 ---- batch: 020 ----
mean loss: 127.42
 ---- batch: 030 ----
mean loss: 141.34
 ---- batch: 040 ----
mean loss: 134.48
 ---- batch: 050 ----
mean loss: 128.81
 ---- batch: 060 ----
mean loss: 136.77
 ---- batch: 070 ----
mean loss: 137.89
 ---- batch: 080 ----
mean loss: 129.27
 ---- batch: 090 ----
mean loss: 134.54
 ---- batch: 100 ----
mean loss: 131.55
 ---- batch: 110 ----
mean loss: 140.96
train mean loss: 134.21
epoch train time: 0:00:00.551540
elapsed time: 0:02:29.235360
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:15:49.149640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.11
 ---- batch: 020 ----
mean loss: 127.92
 ---- batch: 030 ----
mean loss: 135.85
 ---- batch: 040 ----
mean loss: 139.51
 ---- batch: 050 ----
mean loss: 134.87
 ---- batch: 060 ----
mean loss: 133.78
 ---- batch: 070 ----
mean loss: 131.14
 ---- batch: 080 ----
mean loss: 129.88
 ---- batch: 090 ----
mean loss: 131.52
 ---- batch: 100 ----
mean loss: 134.70
 ---- batch: 110 ----
mean loss: 132.75
train mean loss: 134.19
epoch train time: 0:00:00.561916
elapsed time: 0:02:29.797443
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:15:49.711696
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.09
 ---- batch: 020 ----
mean loss: 141.24
 ---- batch: 030 ----
mean loss: 134.60
 ---- batch: 040 ----
mean loss: 133.31
 ---- batch: 050 ----
mean loss: 139.71
 ---- batch: 060 ----
mean loss: 132.03
 ---- batch: 070 ----
mean loss: 129.80
 ---- batch: 080 ----
mean loss: 131.44
 ---- batch: 090 ----
mean loss: 138.92
 ---- batch: 100 ----
mean loss: 128.26
 ---- batch: 110 ----
mean loss: 131.71
train mean loss: 134.27
epoch train time: 0:00:00.550734
elapsed time: 0:02:30.348347
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:15:50.262613
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.55
 ---- batch: 020 ----
mean loss: 129.00
 ---- batch: 030 ----
mean loss: 135.43
 ---- batch: 040 ----
mean loss: 135.08
 ---- batch: 050 ----
mean loss: 137.44
 ---- batch: 060 ----
mean loss: 132.59
 ---- batch: 070 ----
mean loss: 139.57
 ---- batch: 080 ----
mean loss: 134.87
 ---- batch: 090 ----
mean loss: 129.02
 ---- batch: 100 ----
mean loss: 140.52
 ---- batch: 110 ----
mean loss: 133.34
train mean loss: 134.06
epoch train time: 0:00:00.552598
elapsed time: 0:02:30.901115
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:15:50.815401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.31
 ---- batch: 020 ----
mean loss: 131.67
 ---- batch: 030 ----
mean loss: 123.98
 ---- batch: 040 ----
mean loss: 145.58
 ---- batch: 050 ----
mean loss: 133.25
 ---- batch: 060 ----
mean loss: 140.72
 ---- batch: 070 ----
mean loss: 127.96
 ---- batch: 080 ----
mean loss: 132.00
 ---- batch: 090 ----
mean loss: 126.35
 ---- batch: 100 ----
mean loss: 136.70
 ---- batch: 110 ----
mean loss: 141.09
train mean loss: 134.16
epoch train time: 0:00:00.556987
elapsed time: 0:02:31.458271
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:15:51.372520
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.18
 ---- batch: 020 ----
mean loss: 130.19
 ---- batch: 030 ----
mean loss: 129.01
 ---- batch: 040 ----
mean loss: 130.61
 ---- batch: 050 ----
mean loss: 136.03
 ---- batch: 060 ----
mean loss: 131.87
 ---- batch: 070 ----
mean loss: 141.96
 ---- batch: 080 ----
mean loss: 136.27
 ---- batch: 090 ----
mean loss: 132.53
 ---- batch: 100 ----
mean loss: 138.90
 ---- batch: 110 ----
mean loss: 140.01
train mean loss: 134.12
epoch train time: 0:00:00.563100
elapsed time: 0:02:32.021506
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:15:51.935758
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 140.74
 ---- batch: 020 ----
mean loss: 133.82
 ---- batch: 030 ----
mean loss: 141.61
 ---- batch: 040 ----
mean loss: 134.21
 ---- batch: 050 ----
mean loss: 131.16
 ---- batch: 060 ----
mean loss: 126.10
 ---- batch: 070 ----
mean loss: 139.01
 ---- batch: 080 ----
mean loss: 131.44
 ---- batch: 090 ----
mean loss: 125.24
 ---- batch: 100 ----
mean loss: 133.99
 ---- batch: 110 ----
mean loss: 134.35
train mean loss: 134.05
epoch train time: 0:00:00.558834
elapsed time: 0:02:32.580481
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:15:52.494735
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 141.25
 ---- batch: 020 ----
mean loss: 133.27
 ---- batch: 030 ----
mean loss: 130.73
 ---- batch: 040 ----
mean loss: 132.72
 ---- batch: 050 ----
mean loss: 131.52
 ---- batch: 060 ----
mean loss: 133.97
 ---- batch: 070 ----
mean loss: 130.39
 ---- batch: 080 ----
mean loss: 136.70
 ---- batch: 090 ----
mean loss: 135.70
 ---- batch: 100 ----
mean loss: 134.10
 ---- batch: 110 ----
mean loss: 135.49
train mean loss: 134.05
epoch train time: 0:00:00.558509
elapsed time: 0:02:33.139144
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:15:53.053396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.85
 ---- batch: 020 ----
mean loss: 132.99
 ---- batch: 030 ----
mean loss: 133.96
 ---- batch: 040 ----
mean loss: 129.23
 ---- batch: 050 ----
mean loss: 129.48
 ---- batch: 060 ----
mean loss: 135.70
 ---- batch: 070 ----
mean loss: 134.89
 ---- batch: 080 ----
mean loss: 137.18
 ---- batch: 090 ----
mean loss: 137.15
 ---- batch: 100 ----
mean loss: 140.18
 ---- batch: 110 ----
mean loss: 133.76
train mean loss: 134.04
epoch train time: 0:00:00.551853
elapsed time: 0:02:33.691135
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:15:53.605413
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.85
 ---- batch: 020 ----
mean loss: 135.05
 ---- batch: 030 ----
mean loss: 140.42
 ---- batch: 040 ----
mean loss: 134.59
 ---- batch: 050 ----
mean loss: 130.57
 ---- batch: 060 ----
mean loss: 134.38
 ---- batch: 070 ----
mean loss: 132.57
 ---- batch: 080 ----
mean loss: 129.34
 ---- batch: 090 ----
mean loss: 131.74
 ---- batch: 100 ----
mean loss: 141.81
 ---- batch: 110 ----
mean loss: 131.58
train mean loss: 134.09
epoch train time: 0:00:00.561190
elapsed time: 0:02:34.252491
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:15:54.166759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.20
 ---- batch: 020 ----
mean loss: 133.04
 ---- batch: 030 ----
mean loss: 131.48
 ---- batch: 040 ----
mean loss: 130.98
 ---- batch: 050 ----
mean loss: 133.50
 ---- batch: 060 ----
mean loss: 129.19
 ---- batch: 070 ----
mean loss: 131.70
 ---- batch: 080 ----
mean loss: 142.57
 ---- batch: 090 ----
mean loss: 133.25
 ---- batch: 100 ----
mean loss: 135.22
 ---- batch: 110 ----
mean loss: 138.22
train mean loss: 134.02
epoch train time: 0:00:00.571280
elapsed time: 0:02:34.823982
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:15:54.738261
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.24
 ---- batch: 020 ----
mean loss: 137.03
 ---- batch: 030 ----
mean loss: 137.77
 ---- batch: 040 ----
mean loss: 136.85
 ---- batch: 050 ----
mean loss: 137.83
 ---- batch: 060 ----
mean loss: 127.03
 ---- batch: 070 ----
mean loss: 131.91
 ---- batch: 080 ----
mean loss: 134.04
 ---- batch: 090 ----
mean loss: 134.99
 ---- batch: 100 ----
mean loss: 137.09
 ---- batch: 110 ----
mean loss: 135.22
train mean loss: 133.88
epoch train time: 0:00:00.555112
elapsed time: 0:02:35.379273
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:15:55.293531
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.21
 ---- batch: 020 ----
mean loss: 131.17
 ---- batch: 030 ----
mean loss: 136.92
 ---- batch: 040 ----
mean loss: 136.47
 ---- batch: 050 ----
mean loss: 127.86
 ---- batch: 060 ----
mean loss: 129.65
 ---- batch: 070 ----
mean loss: 140.18
 ---- batch: 080 ----
mean loss: 143.70
 ---- batch: 090 ----
mean loss: 129.44
 ---- batch: 100 ----
mean loss: 133.01
 ---- batch: 110 ----
mean loss: 134.49
train mean loss: 133.92
epoch train time: 0:00:00.551933
elapsed time: 0:02:35.931352
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:15:55.845600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.08
 ---- batch: 020 ----
mean loss: 134.71
 ---- batch: 030 ----
mean loss: 142.40
 ---- batch: 040 ----
mean loss: 131.54
 ---- batch: 050 ----
mean loss: 133.96
 ---- batch: 060 ----
mean loss: 130.69
 ---- batch: 070 ----
mean loss: 133.95
 ---- batch: 080 ----
mean loss: 130.40
 ---- batch: 090 ----
mean loss: 131.33
 ---- batch: 100 ----
mean loss: 133.77
 ---- batch: 110 ----
mean loss: 130.92
train mean loss: 133.95
epoch train time: 0:00:00.553925
elapsed time: 0:02:36.485424
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:15:56.399674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.54
 ---- batch: 020 ----
mean loss: 132.23
 ---- batch: 030 ----
mean loss: 127.57
 ---- batch: 040 ----
mean loss: 133.96
 ---- batch: 050 ----
mean loss: 139.03
 ---- batch: 060 ----
mean loss: 135.75
 ---- batch: 070 ----
mean loss: 132.43
 ---- batch: 080 ----
mean loss: 136.49
 ---- batch: 090 ----
mean loss: 136.80
 ---- batch: 100 ----
mean loss: 136.59
 ---- batch: 110 ----
mean loss: 130.21
train mean loss: 133.90
epoch train time: 0:00:00.572830
elapsed time: 0:02:37.058419
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:15:56.972684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.42
 ---- batch: 020 ----
mean loss: 136.59
 ---- batch: 030 ----
mean loss: 134.09
 ---- batch: 040 ----
mean loss: 131.28
 ---- batch: 050 ----
mean loss: 135.61
 ---- batch: 060 ----
mean loss: 138.04
 ---- batch: 070 ----
mean loss: 135.56
 ---- batch: 080 ----
mean loss: 140.84
 ---- batch: 090 ----
mean loss: 134.42
 ---- batch: 100 ----
mean loss: 123.45
 ---- batch: 110 ----
mean loss: 130.43
train mean loss: 133.91
epoch train time: 0:00:00.559256
elapsed time: 0:02:37.617825
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:15:57.532096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.88
 ---- batch: 020 ----
mean loss: 130.67
 ---- batch: 030 ----
mean loss: 134.56
 ---- batch: 040 ----
mean loss: 136.05
 ---- batch: 050 ----
mean loss: 127.08
 ---- batch: 060 ----
mean loss: 139.37
 ---- batch: 070 ----
mean loss: 131.67
 ---- batch: 080 ----
mean loss: 127.37
 ---- batch: 090 ----
mean loss: 136.80
 ---- batch: 100 ----
mean loss: 142.21
 ---- batch: 110 ----
mean loss: 134.09
train mean loss: 133.89
epoch train time: 0:00:00.563848
elapsed time: 0:02:38.181829
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:15:58.096082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.43
 ---- batch: 020 ----
mean loss: 144.23
 ---- batch: 030 ----
mean loss: 133.47
 ---- batch: 040 ----
mean loss: 129.54
 ---- batch: 050 ----
mean loss: 132.71
 ---- batch: 060 ----
mean loss: 122.48
 ---- batch: 070 ----
mean loss: 145.23
 ---- batch: 080 ----
mean loss: 126.51
 ---- batch: 090 ----
mean loss: 140.20
 ---- batch: 100 ----
mean loss: 133.58
 ---- batch: 110 ----
mean loss: 135.29
train mean loss: 133.65
epoch train time: 0:00:00.557364
elapsed time: 0:02:38.739330
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:15:58.653682
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.56
 ---- batch: 020 ----
mean loss: 132.59
 ---- batch: 030 ----
mean loss: 133.87
 ---- batch: 040 ----
mean loss: 127.13
 ---- batch: 050 ----
mean loss: 132.53
 ---- batch: 060 ----
mean loss: 134.29
 ---- batch: 070 ----
mean loss: 139.68
 ---- batch: 080 ----
mean loss: 140.58
 ---- batch: 090 ----
mean loss: 135.62
 ---- batch: 100 ----
mean loss: 136.65
 ---- batch: 110 ----
mean loss: 135.51
train mean loss: 133.82
epoch train time: 0:00:00.554772
elapsed time: 0:02:39.294333
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:15:59.208581
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.22
 ---- batch: 020 ----
mean loss: 131.01
 ---- batch: 030 ----
mean loss: 133.30
 ---- batch: 040 ----
mean loss: 139.26
 ---- batch: 050 ----
mean loss: 135.27
 ---- batch: 060 ----
mean loss: 132.35
 ---- batch: 070 ----
mean loss: 138.14
 ---- batch: 080 ----
mean loss: 131.85
 ---- batch: 090 ----
mean loss: 131.62
 ---- batch: 100 ----
mean loss: 131.73
 ---- batch: 110 ----
mean loss: 133.84
train mean loss: 133.68
epoch train time: 0:00:00.551706
elapsed time: 0:02:39.846172
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:15:59.760423
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.35
 ---- batch: 020 ----
mean loss: 135.95
 ---- batch: 030 ----
mean loss: 143.80
 ---- batch: 040 ----
mean loss: 139.24
 ---- batch: 050 ----
mean loss: 131.96
 ---- batch: 060 ----
mean loss: 139.73
 ---- batch: 070 ----
mean loss: 129.37
 ---- batch: 080 ----
mean loss: 120.88
 ---- batch: 090 ----
mean loss: 133.73
 ---- batch: 100 ----
mean loss: 136.20
 ---- batch: 110 ----
mean loss: 132.98
train mean loss: 133.64
epoch train time: 0:00:00.549239
elapsed time: 0:02:40.395547
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:16:00.309798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.72
 ---- batch: 020 ----
mean loss: 132.60
 ---- batch: 030 ----
mean loss: 126.69
 ---- batch: 040 ----
mean loss: 141.15
 ---- batch: 050 ----
mean loss: 131.51
 ---- batch: 060 ----
mean loss: 131.94
 ---- batch: 070 ----
mean loss: 132.58
 ---- batch: 080 ----
mean loss: 135.27
 ---- batch: 090 ----
mean loss: 132.26
 ---- batch: 100 ----
mean loss: 132.49
 ---- batch: 110 ----
mean loss: 137.51
train mean loss: 133.91
epoch train time: 0:00:00.556595
elapsed time: 0:02:40.952280
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:16:00.866549
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.49
 ---- batch: 020 ----
mean loss: 129.65
 ---- batch: 030 ----
mean loss: 127.06
 ---- batch: 040 ----
mean loss: 145.66
 ---- batch: 050 ----
mean loss: 134.22
 ---- batch: 060 ----
mean loss: 123.20
 ---- batch: 070 ----
mean loss: 133.95
 ---- batch: 080 ----
mean loss: 139.18
 ---- batch: 090 ----
mean loss: 133.00
 ---- batch: 100 ----
mean loss: 141.22
 ---- batch: 110 ----
mean loss: 139.05
train mean loss: 133.73
epoch train time: 0:00:00.552347
elapsed time: 0:02:41.504780
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:16:01.419030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.23
 ---- batch: 020 ----
mean loss: 133.87
 ---- batch: 030 ----
mean loss: 135.16
 ---- batch: 040 ----
mean loss: 134.62
 ---- batch: 050 ----
mean loss: 123.70
 ---- batch: 060 ----
mean loss: 144.96
 ---- batch: 070 ----
mean loss: 135.66
 ---- batch: 080 ----
mean loss: 133.55
 ---- batch: 090 ----
mean loss: 135.03
 ---- batch: 100 ----
mean loss: 131.95
 ---- batch: 110 ----
mean loss: 130.69
train mean loss: 133.70
epoch train time: 0:00:00.555711
elapsed time: 0:02:42.060628
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:16:01.974882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.65
 ---- batch: 020 ----
mean loss: 125.44
 ---- batch: 030 ----
mean loss: 130.78
 ---- batch: 040 ----
mean loss: 138.21
 ---- batch: 050 ----
mean loss: 130.16
 ---- batch: 060 ----
mean loss: 137.10
 ---- batch: 070 ----
mean loss: 139.26
 ---- batch: 080 ----
mean loss: 134.68
 ---- batch: 090 ----
mean loss: 133.47
 ---- batch: 100 ----
mean loss: 137.70
 ---- batch: 110 ----
mean loss: 134.87
train mean loss: 133.66
epoch train time: 0:00:00.568680
elapsed time: 0:02:42.629461
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:16:02.543711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.95
 ---- batch: 020 ----
mean loss: 131.12
 ---- batch: 030 ----
mean loss: 129.46
 ---- batch: 040 ----
mean loss: 138.23
 ---- batch: 050 ----
mean loss: 132.21
 ---- batch: 060 ----
mean loss: 137.22
 ---- batch: 070 ----
mean loss: 134.20
 ---- batch: 080 ----
mean loss: 137.64
 ---- batch: 090 ----
mean loss: 136.48
 ---- batch: 100 ----
mean loss: 134.49
 ---- batch: 110 ----
mean loss: 131.89
train mean loss: 133.54
epoch train time: 0:00:00.568059
elapsed time: 0:02:43.197657
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:16:03.111910
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.54
 ---- batch: 020 ----
mean loss: 126.52
 ---- batch: 030 ----
mean loss: 127.50
 ---- batch: 040 ----
mean loss: 141.81
 ---- batch: 050 ----
mean loss: 129.67
 ---- batch: 060 ----
mean loss: 140.33
 ---- batch: 070 ----
mean loss: 137.10
 ---- batch: 080 ----
mean loss: 129.39
 ---- batch: 090 ----
mean loss: 136.05
 ---- batch: 100 ----
mean loss: 133.19
 ---- batch: 110 ----
mean loss: 128.16
train mean loss: 133.56
epoch train time: 0:00:00.557356
elapsed time: 0:02:43.755148
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:16:03.669398
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.49
 ---- batch: 020 ----
mean loss: 135.78
 ---- batch: 030 ----
mean loss: 135.85
 ---- batch: 040 ----
mean loss: 138.67
 ---- batch: 050 ----
mean loss: 135.18
 ---- batch: 060 ----
mean loss: 144.38
 ---- batch: 070 ----
mean loss: 131.43
 ---- batch: 080 ----
mean loss: 130.87
 ---- batch: 090 ----
mean loss: 126.36
 ---- batch: 100 ----
mean loss: 131.78
 ---- batch: 110 ----
mean loss: 127.84
train mean loss: 133.53
epoch train time: 0:00:00.559538
elapsed time: 0:02:44.314865
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:16:04.229116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.73
 ---- batch: 020 ----
mean loss: 140.53
 ---- batch: 030 ----
mean loss: 132.11
 ---- batch: 040 ----
mean loss: 132.35
 ---- batch: 050 ----
mean loss: 129.41
 ---- batch: 060 ----
mean loss: 134.10
 ---- batch: 070 ----
mean loss: 129.99
 ---- batch: 080 ----
mean loss: 131.40
 ---- batch: 090 ----
mean loss: 134.44
 ---- batch: 100 ----
mean loss: 131.94
 ---- batch: 110 ----
mean loss: 132.29
train mean loss: 133.49
epoch train time: 0:00:00.554843
elapsed time: 0:02:44.869839
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:16:04.784107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.19
 ---- batch: 020 ----
mean loss: 136.72
 ---- batch: 030 ----
mean loss: 132.71
 ---- batch: 040 ----
mean loss: 135.91
 ---- batch: 050 ----
mean loss: 136.96
 ---- batch: 060 ----
mean loss: 138.65
 ---- batch: 070 ----
mean loss: 137.39
 ---- batch: 080 ----
mean loss: 125.88
 ---- batch: 090 ----
mean loss: 129.57
 ---- batch: 100 ----
mean loss: 134.92
 ---- batch: 110 ----
mean loss: 128.52
train mean loss: 133.50
epoch train time: 0:00:00.555116
elapsed time: 0:02:45.425108
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:16:05.339358
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.44
 ---- batch: 020 ----
mean loss: 134.37
 ---- batch: 030 ----
mean loss: 138.13
 ---- batch: 040 ----
mean loss: 136.36
 ---- batch: 050 ----
mean loss: 133.06
 ---- batch: 060 ----
mean loss: 141.86
 ---- batch: 070 ----
mean loss: 133.36
 ---- batch: 080 ----
mean loss: 138.84
 ---- batch: 090 ----
mean loss: 129.47
 ---- batch: 100 ----
mean loss: 130.36
 ---- batch: 110 ----
mean loss: 124.51
train mean loss: 133.57
epoch train time: 0:00:00.559016
elapsed time: 0:02:45.984263
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:16:05.898516
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.95
 ---- batch: 020 ----
mean loss: 128.87
 ---- batch: 030 ----
mean loss: 132.75
 ---- batch: 040 ----
mean loss: 128.11
 ---- batch: 050 ----
mean loss: 131.94
 ---- batch: 060 ----
mean loss: 133.64
 ---- batch: 070 ----
mean loss: 137.13
 ---- batch: 080 ----
mean loss: 135.71
 ---- batch: 090 ----
mean loss: 148.08
 ---- batch: 100 ----
mean loss: 127.68
 ---- batch: 110 ----
mean loss: 138.48
train mean loss: 133.58
epoch train time: 0:00:00.574086
elapsed time: 0:02:46.558534
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:16:06.472817
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.83
 ---- batch: 020 ----
mean loss: 127.63
 ---- batch: 030 ----
mean loss: 128.51
 ---- batch: 040 ----
mean loss: 129.63
 ---- batch: 050 ----
mean loss: 141.07
 ---- batch: 060 ----
mean loss: 137.35
 ---- batch: 070 ----
mean loss: 136.00
 ---- batch: 080 ----
mean loss: 133.14
 ---- batch: 090 ----
mean loss: 135.31
 ---- batch: 100 ----
mean loss: 138.74
 ---- batch: 110 ----
mean loss: 131.19
train mean loss: 133.38
epoch train time: 0:00:00.575994
elapsed time: 0:02:47.134708
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:16:07.048960
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.72
 ---- batch: 020 ----
mean loss: 139.96
 ---- batch: 030 ----
mean loss: 139.92
 ---- batch: 040 ----
mean loss: 128.89
 ---- batch: 050 ----
mean loss: 131.35
 ---- batch: 060 ----
mean loss: 141.15
 ---- batch: 070 ----
mean loss: 133.10
 ---- batch: 080 ----
mean loss: 120.53
 ---- batch: 090 ----
mean loss: 127.44
 ---- batch: 100 ----
mean loss: 137.65
 ---- batch: 110 ----
mean loss: 135.25
train mean loss: 133.48
epoch train time: 0:00:00.554231
elapsed time: 0:02:47.689076
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:16:07.603330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.16
 ---- batch: 020 ----
mean loss: 138.56
 ---- batch: 030 ----
mean loss: 126.03
 ---- batch: 040 ----
mean loss: 142.04
 ---- batch: 050 ----
mean loss: 138.86
 ---- batch: 060 ----
mean loss: 129.58
 ---- batch: 070 ----
mean loss: 140.26
 ---- batch: 080 ----
mean loss: 125.50
 ---- batch: 090 ----
mean loss: 130.15
 ---- batch: 100 ----
mean loss: 137.84
 ---- batch: 110 ----
mean loss: 133.50
train mean loss: 133.46
epoch train time: 0:00:00.556642
elapsed time: 0:02:48.245852
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:16:08.160126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.23
 ---- batch: 020 ----
mean loss: 129.50
 ---- batch: 030 ----
mean loss: 135.12
 ---- batch: 040 ----
mean loss: 134.83
 ---- batch: 050 ----
mean loss: 132.72
 ---- batch: 060 ----
mean loss: 133.60
 ---- batch: 070 ----
mean loss: 141.09
 ---- batch: 080 ----
mean loss: 134.74
 ---- batch: 090 ----
mean loss: 128.74
 ---- batch: 100 ----
mean loss: 133.10
 ---- batch: 110 ----
mean loss: 134.18
train mean loss: 133.41
epoch train time: 0:00:00.550674
elapsed time: 0:02:48.796684
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:16:08.710935
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.00
 ---- batch: 020 ----
mean loss: 132.26
 ---- batch: 030 ----
mean loss: 133.57
 ---- batch: 040 ----
mean loss: 132.15
 ---- batch: 050 ----
mean loss: 129.57
 ---- batch: 060 ----
mean loss: 134.86
 ---- batch: 070 ----
mean loss: 132.55
 ---- batch: 080 ----
mean loss: 128.72
 ---- batch: 090 ----
mean loss: 131.86
 ---- batch: 100 ----
mean loss: 137.80
 ---- batch: 110 ----
mean loss: 141.30
train mean loss: 133.34
epoch train time: 0:00:00.548745
elapsed time: 0:02:49.345562
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:16:09.259811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.25
 ---- batch: 020 ----
mean loss: 135.22
 ---- batch: 030 ----
mean loss: 131.79
 ---- batch: 040 ----
mean loss: 130.36
 ---- batch: 050 ----
mean loss: 133.78
 ---- batch: 060 ----
mean loss: 135.14
 ---- batch: 070 ----
mean loss: 135.62
 ---- batch: 080 ----
mean loss: 130.26
 ---- batch: 090 ----
mean loss: 130.79
 ---- batch: 100 ----
mean loss: 137.69
 ---- batch: 110 ----
mean loss: 128.02
train mean loss: 133.31
epoch train time: 0:00:00.586572
elapsed time: 0:02:49.932300
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:16:09.846560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.12
 ---- batch: 020 ----
mean loss: 133.19
 ---- batch: 030 ----
mean loss: 134.26
 ---- batch: 040 ----
mean loss: 132.70
 ---- batch: 050 ----
mean loss: 131.81
 ---- batch: 060 ----
mean loss: 133.65
 ---- batch: 070 ----
mean loss: 136.74
 ---- batch: 080 ----
mean loss: 127.06
 ---- batch: 090 ----
mean loss: 132.41
 ---- batch: 100 ----
mean loss: 138.11
 ---- batch: 110 ----
mean loss: 139.13
train mean loss: 133.16
epoch train time: 0:00:00.559180
elapsed time: 0:02:50.491626
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:16:10.405876
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.17
 ---- batch: 020 ----
mean loss: 133.05
 ---- batch: 030 ----
mean loss: 128.88
 ---- batch: 040 ----
mean loss: 133.77
 ---- batch: 050 ----
mean loss: 128.03
 ---- batch: 060 ----
mean loss: 132.89
 ---- batch: 070 ----
mean loss: 128.51
 ---- batch: 080 ----
mean loss: 132.10
 ---- batch: 090 ----
mean loss: 138.80
 ---- batch: 100 ----
mean loss: 140.23
 ---- batch: 110 ----
mean loss: 136.59
train mean loss: 133.26
epoch train time: 0:00:00.560737
elapsed time: 0:02:51.052501
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:16:10.966752
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.91
 ---- batch: 020 ----
mean loss: 134.86
 ---- batch: 030 ----
mean loss: 129.37
 ---- batch: 040 ----
mean loss: 133.44
 ---- batch: 050 ----
mean loss: 129.87
 ---- batch: 060 ----
mean loss: 136.52
 ---- batch: 070 ----
mean loss: 126.59
 ---- batch: 080 ----
mean loss: 137.77
 ---- batch: 090 ----
mean loss: 134.90
 ---- batch: 100 ----
mean loss: 136.65
 ---- batch: 110 ----
mean loss: 134.97
train mean loss: 133.25
epoch train time: 0:00:00.565449
elapsed time: 0:02:51.621318
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_3/checkpoint.pth.tar
**** end time: 2019-09-27 02:16:11.535538 ****
