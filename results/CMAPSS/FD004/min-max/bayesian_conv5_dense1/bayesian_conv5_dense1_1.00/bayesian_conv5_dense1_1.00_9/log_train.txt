Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 2784
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 09:25:56.114668 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 09:25:56.132307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3503.65
 ---- batch: 020 ----
mean loss: 1907.28
 ---- batch: 030 ----
mean loss: 1527.29
 ---- batch: 040 ----
mean loss: 1376.94
 ---- batch: 050 ----
mean loss: 1292.82
 ---- batch: 060 ----
mean loss: 1179.26
 ---- batch: 070 ----
mean loss: 1203.25
 ---- batch: 080 ----
mean loss: 1162.11
 ---- batch: 090 ----
mean loss: 1121.63
 ---- batch: 100 ----
mean loss: 1111.99
 ---- batch: 110 ----
mean loss: 1050.58
train mean loss: 1482.03
epoch train time: 0:00:45.148997
elapsed time: 0:00:45.174996
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 09:26:41.289730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1066.59
 ---- batch: 020 ----
mean loss: 1040.66
 ---- batch: 030 ----
mean loss: 1009.67
 ---- batch: 040 ----
mean loss: 1019.20
 ---- batch: 050 ----
mean loss: 1014.89
 ---- batch: 060 ----
mean loss: 1001.15
 ---- batch: 070 ----
mean loss: 1029.91
 ---- batch: 080 ----
mean loss: 1009.05
 ---- batch: 090 ----
mean loss: 1027.58
 ---- batch: 100 ----
mean loss: 1015.27
 ---- batch: 110 ----
mean loss: 1010.91
train mean loss: 1021.92
epoch train time: 0:00:15.323566
elapsed time: 0:01:00.498950
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 09:26:56.614101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 992.99
 ---- batch: 020 ----
mean loss: 988.69
 ---- batch: 030 ----
mean loss: 971.58
 ---- batch: 040 ----
mean loss: 992.44
 ---- batch: 050 ----
mean loss: 971.35
 ---- batch: 060 ----
mean loss: 943.63
 ---- batch: 070 ----
mean loss: 983.86
 ---- batch: 080 ----
mean loss: 972.32
 ---- batch: 090 ----
mean loss: 975.68
 ---- batch: 100 ----
mean loss: 961.56
 ---- batch: 110 ----
mean loss: 985.70
train mean loss: 975.57
epoch train time: 0:00:15.384084
elapsed time: 0:01:15.883937
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 09:27:11.999289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 970.37
 ---- batch: 020 ----
mean loss: 987.81
 ---- batch: 030 ----
mean loss: 969.35
 ---- batch: 040 ----
mean loss: 948.29
 ---- batch: 050 ----
mean loss: 927.78
 ---- batch: 060 ----
mean loss: 939.74
 ---- batch: 070 ----
mean loss: 946.05
 ---- batch: 080 ----
mean loss: 917.38
 ---- batch: 090 ----
mean loss: 948.69
 ---- batch: 100 ----
mean loss: 955.76
 ---- batch: 110 ----
mean loss: 958.66
train mean loss: 951.39
epoch train time: 0:00:15.464641
elapsed time: 0:01:31.349709
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 09:27:27.464957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 950.19
 ---- batch: 020 ----
mean loss: 943.57
 ---- batch: 030 ----
mean loss: 957.18
 ---- batch: 040 ----
mean loss: 960.76
 ---- batch: 050 ----
mean loss: 945.52
 ---- batch: 060 ----
mean loss: 931.95
 ---- batch: 070 ----
mean loss: 943.13
 ---- batch: 080 ----
mean loss: 930.21
 ---- batch: 090 ----
mean loss: 919.77
 ---- batch: 100 ----
mean loss: 957.60
 ---- batch: 110 ----
mean loss: 956.32
train mean loss: 945.18
epoch train time: 0:00:15.411939
elapsed time: 0:01:46.762658
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 09:27:42.877918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.00
 ---- batch: 020 ----
mean loss: 939.82
 ---- batch: 030 ----
mean loss: 933.60
 ---- batch: 040 ----
mean loss: 907.19
 ---- batch: 050 ----
mean loss: 921.49
 ---- batch: 060 ----
mean loss: 947.86
 ---- batch: 070 ----
mean loss: 942.98
 ---- batch: 080 ----
mean loss: 946.86
 ---- batch: 090 ----
mean loss: 932.22
 ---- batch: 100 ----
mean loss: 932.50
 ---- batch: 110 ----
mean loss: 932.98
train mean loss: 933.13
epoch train time: 0:00:15.349595
elapsed time: 0:02:02.113248
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 09:27:58.228492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.85
 ---- batch: 020 ----
mean loss: 921.97
 ---- batch: 030 ----
mean loss: 906.25
 ---- batch: 040 ----
mean loss: 935.63
 ---- batch: 050 ----
mean loss: 900.83
 ---- batch: 060 ----
mean loss: 938.20
 ---- batch: 070 ----
mean loss: 923.53
 ---- batch: 080 ----
mean loss: 950.03
 ---- batch: 090 ----
mean loss: 920.62
 ---- batch: 100 ----
mean loss: 947.85
 ---- batch: 110 ----
mean loss: 931.64
train mean loss: 928.61
epoch train time: 0:00:15.352061
elapsed time: 0:02:17.466296
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 09:28:13.581461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.26
 ---- batch: 020 ----
mean loss: 916.50
 ---- batch: 030 ----
mean loss: 943.49
 ---- batch: 040 ----
mean loss: 913.79
 ---- batch: 050 ----
mean loss: 934.71
 ---- batch: 060 ----
mean loss: 924.04
 ---- batch: 070 ----
mean loss: 895.31
 ---- batch: 080 ----
mean loss: 894.73
 ---- batch: 090 ----
mean loss: 921.38
 ---- batch: 100 ----
mean loss: 950.49
 ---- batch: 110 ----
mean loss: 919.43
train mean loss: 919.82
epoch train time: 0:00:15.350000
elapsed time: 0:02:32.817256
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 09:28:28.932472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.68
 ---- batch: 020 ----
mean loss: 918.38
 ---- batch: 030 ----
mean loss: 904.87
 ---- batch: 040 ----
mean loss: 897.41
 ---- batch: 050 ----
mean loss: 924.78
 ---- batch: 060 ----
mean loss: 902.62
 ---- batch: 070 ----
mean loss: 921.20
 ---- batch: 080 ----
mean loss: 908.98
 ---- batch: 090 ----
mean loss: 897.87
 ---- batch: 100 ----
mean loss: 918.16
 ---- batch: 110 ----
mean loss: 915.44
train mean loss: 912.49
epoch train time: 0:00:15.354369
elapsed time: 0:02:48.172595
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 09:28:44.287793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.47
 ---- batch: 020 ----
mean loss: 923.51
 ---- batch: 030 ----
mean loss: 896.23
 ---- batch: 040 ----
mean loss: 919.61
 ---- batch: 050 ----
mean loss: 895.52
 ---- batch: 060 ----
mean loss: 903.41
 ---- batch: 070 ----
mean loss: 894.98
 ---- batch: 080 ----
mean loss: 931.40
 ---- batch: 090 ----
mean loss: 895.94
 ---- batch: 100 ----
mean loss: 884.16
 ---- batch: 110 ----
mean loss: 908.92
train mean loss: 904.06
epoch train time: 0:00:15.315526
elapsed time: 0:03:03.489092
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 09:28:59.604264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.79
 ---- batch: 020 ----
mean loss: 874.37
 ---- batch: 030 ----
mean loss: 907.03
 ---- batch: 040 ----
mean loss: 927.29
 ---- batch: 050 ----
mean loss: 882.98
 ---- batch: 060 ----
mean loss: 901.73
 ---- batch: 070 ----
mean loss: 901.20
 ---- batch: 080 ----
mean loss: 899.65
 ---- batch: 090 ----
mean loss: 906.07
 ---- batch: 100 ----
mean loss: 890.56
 ---- batch: 110 ----
mean loss: 896.08
train mean loss: 899.99
epoch train time: 0:00:15.344103
elapsed time: 0:03:18.834024
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 09:29:14.949231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.06
 ---- batch: 020 ----
mean loss: 880.71
 ---- batch: 030 ----
mean loss: 909.01
 ---- batch: 040 ----
mean loss: 910.77
 ---- batch: 050 ----
mean loss: 890.19
 ---- batch: 060 ----
mean loss: 882.03
 ---- batch: 070 ----
mean loss: 897.98
 ---- batch: 080 ----
mean loss: 876.47
 ---- batch: 090 ----
mean loss: 898.91
 ---- batch: 100 ----
mean loss: 877.52
 ---- batch: 110 ----
mean loss: 868.36
train mean loss: 892.43
epoch train time: 0:00:15.353097
elapsed time: 0:03:34.188023
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 09:29:30.303217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.35
 ---- batch: 020 ----
mean loss: 894.70
 ---- batch: 030 ----
mean loss: 893.73
 ---- batch: 040 ----
mean loss: 885.29
 ---- batch: 050 ----
mean loss: 893.60
 ---- batch: 060 ----
mean loss: 879.76
 ---- batch: 070 ----
mean loss: 909.20
 ---- batch: 080 ----
mean loss: 852.86
 ---- batch: 090 ----
mean loss: 882.32
 ---- batch: 100 ----
mean loss: 895.26
 ---- batch: 110 ----
mean loss: 872.13
train mean loss: 887.10
epoch train time: 0:00:15.343162
elapsed time: 0:03:49.532110
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 09:29:45.647262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.99
 ---- batch: 020 ----
mean loss: 878.96
 ---- batch: 030 ----
mean loss: 876.00
 ---- batch: 040 ----
mean loss: 864.30
 ---- batch: 050 ----
mean loss: 864.79
 ---- batch: 060 ----
mean loss: 882.20
 ---- batch: 070 ----
mean loss: 887.18
 ---- batch: 080 ----
mean loss: 878.88
 ---- batch: 090 ----
mean loss: 881.45
 ---- batch: 100 ----
mean loss: 885.44
 ---- batch: 110 ----
mean loss: 907.38
train mean loss: 881.38
epoch train time: 0:00:15.401331
elapsed time: 0:04:04.934380
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 09:30:01.049568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.45
 ---- batch: 020 ----
mean loss: 867.81
 ---- batch: 030 ----
mean loss: 867.85
 ---- batch: 040 ----
mean loss: 877.35
 ---- batch: 050 ----
mean loss: 881.85
 ---- batch: 060 ----
mean loss: 866.09
 ---- batch: 070 ----
mean loss: 868.16
 ---- batch: 080 ----
mean loss: 865.65
 ---- batch: 090 ----
mean loss: 863.94
 ---- batch: 100 ----
mean loss: 872.57
 ---- batch: 110 ----
mean loss: 894.68
train mean loss: 872.71
epoch train time: 0:00:15.371607
elapsed time: 0:04:20.307008
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 09:30:16.422156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.86
 ---- batch: 020 ----
mean loss: 873.33
 ---- batch: 030 ----
mean loss: 858.45
 ---- batch: 040 ----
mean loss: 854.53
 ---- batch: 050 ----
mean loss: 866.95
 ---- batch: 060 ----
mean loss: 864.61
 ---- batch: 070 ----
mean loss: 867.68
 ---- batch: 080 ----
mean loss: 841.20
 ---- batch: 090 ----
mean loss: 852.62
 ---- batch: 100 ----
mean loss: 872.50
 ---- batch: 110 ----
mean loss: 850.24
train mean loss: 860.68
epoch train time: 0:00:15.425475
elapsed time: 0:04:35.733457
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 09:30:31.848646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.38
 ---- batch: 020 ----
mean loss: 827.75
 ---- batch: 030 ----
mean loss: 854.20
 ---- batch: 040 ----
mean loss: 864.62
 ---- batch: 050 ----
mean loss: 860.17
 ---- batch: 060 ----
mean loss: 860.15
 ---- batch: 070 ----
mean loss: 857.89
 ---- batch: 080 ----
mean loss: 834.63
 ---- batch: 090 ----
mean loss: 828.86
 ---- batch: 100 ----
mean loss: 826.46
 ---- batch: 110 ----
mean loss: 843.36
train mean loss: 845.89
epoch train time: 0:00:15.374542
elapsed time: 0:04:51.108830
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 09:30:47.224021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.24
 ---- batch: 020 ----
mean loss: 827.90
 ---- batch: 030 ----
mean loss: 801.97
 ---- batch: 040 ----
mean loss: 825.25
 ---- batch: 050 ----
mean loss: 840.25
 ---- batch: 060 ----
mean loss: 805.89
 ---- batch: 070 ----
mean loss: 828.86
 ---- batch: 080 ----
mean loss: 804.21
 ---- batch: 090 ----
mean loss: 816.12
 ---- batch: 100 ----
mean loss: 824.16
 ---- batch: 110 ----
mean loss: 800.46
train mean loss: 816.92
epoch train time: 0:00:15.411386
elapsed time: 0:05:06.521163
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 09:31:02.636377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 791.10
 ---- batch: 020 ----
mean loss: 806.23
 ---- batch: 030 ----
mean loss: 795.92
 ---- batch: 040 ----
mean loss: 752.69
 ---- batch: 050 ----
mean loss: 776.65
 ---- batch: 060 ----
mean loss: 777.94
 ---- batch: 070 ----
mean loss: 785.26
 ---- batch: 080 ----
mean loss: 775.80
 ---- batch: 090 ----
mean loss: 764.36
 ---- batch: 100 ----
mean loss: 762.69
 ---- batch: 110 ----
mean loss: 783.28
train mean loss: 778.29
epoch train time: 0:00:15.340357
elapsed time: 0:05:21.862520
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 09:31:17.977694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 749.70
 ---- batch: 020 ----
mean loss: 758.70
 ---- batch: 030 ----
mean loss: 756.73
 ---- batch: 040 ----
mean loss: 760.65
 ---- batch: 050 ----
mean loss: 731.38
 ---- batch: 060 ----
mean loss: 751.97
 ---- batch: 070 ----
mean loss: 749.48
 ---- batch: 080 ----
mean loss: 745.44
 ---- batch: 090 ----
mean loss: 726.26
 ---- batch: 100 ----
mean loss: 735.10
 ---- batch: 110 ----
mean loss: 727.60
train mean loss: 743.81
epoch train time: 0:00:15.372110
elapsed time: 0:05:37.235544
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 09:31:33.350699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 691.11
 ---- batch: 020 ----
mean loss: 728.21
 ---- batch: 030 ----
mean loss: 722.83
 ---- batch: 040 ----
mean loss: 736.30
 ---- batch: 050 ----
mean loss: 716.12
 ---- batch: 060 ----
mean loss: 693.46
 ---- batch: 070 ----
mean loss: 688.93
 ---- batch: 080 ----
mean loss: 694.00
 ---- batch: 090 ----
mean loss: 672.85
 ---- batch: 100 ----
mean loss: 682.73
 ---- batch: 110 ----
mean loss: 664.64
train mean loss: 699.23
epoch train time: 0:00:15.364361
elapsed time: 0:05:52.600886
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 09:31:48.716104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 668.85
 ---- batch: 020 ----
mean loss: 676.92
 ---- batch: 030 ----
mean loss: 659.25
 ---- batch: 040 ----
mean loss: 676.89
 ---- batch: 050 ----
mean loss: 668.57
 ---- batch: 060 ----
mean loss: 657.07
 ---- batch: 070 ----
mean loss: 659.29
 ---- batch: 080 ----
mean loss: 645.15
 ---- batch: 090 ----
mean loss: 638.31
 ---- batch: 100 ----
mean loss: 632.77
 ---- batch: 110 ----
mean loss: 639.49
train mean loss: 656.27
epoch train time: 0:00:15.395688
elapsed time: 0:06:07.997421
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 09:32:04.112588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 625.58
 ---- batch: 020 ----
mean loss: 619.95
 ---- batch: 030 ----
mean loss: 621.13
 ---- batch: 040 ----
mean loss: 633.99
 ---- batch: 050 ----
mean loss: 623.58
 ---- batch: 060 ----
mean loss: 619.32
 ---- batch: 070 ----
mean loss: 638.36
 ---- batch: 080 ----
mean loss: 599.93
 ---- batch: 090 ----
mean loss: 606.62
 ---- batch: 100 ----
mean loss: 624.11
 ---- batch: 110 ----
mean loss: 610.41
train mean loss: 619.77
epoch train time: 0:00:15.325504
elapsed time: 0:06:23.323874
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 09:32:19.439104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 590.45
 ---- batch: 020 ----
mean loss: 604.05
 ---- batch: 030 ----
mean loss: 605.76
 ---- batch: 040 ----
mean loss: 577.96
 ---- batch: 050 ----
mean loss: 606.35
 ---- batch: 060 ----
mean loss: 591.74
 ---- batch: 070 ----
mean loss: 586.64
 ---- batch: 080 ----
mean loss: 583.80
 ---- batch: 090 ----
mean loss: 577.17
 ---- batch: 100 ----
mean loss: 579.26
 ---- batch: 110 ----
mean loss: 566.29
train mean loss: 587.20
epoch train time: 0:00:15.380171
elapsed time: 0:06:38.705168
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 09:32:34.820535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 566.48
 ---- batch: 020 ----
mean loss: 572.27
 ---- batch: 030 ----
mean loss: 558.60
 ---- batch: 040 ----
mean loss: 559.38
 ---- batch: 050 ----
mean loss: 551.20
 ---- batch: 060 ----
mean loss: 547.87
 ---- batch: 070 ----
mean loss: 553.54
 ---- batch: 080 ----
mean loss: 567.50
 ---- batch: 090 ----
mean loss: 554.53
 ---- batch: 100 ----
mean loss: 538.08
 ---- batch: 110 ----
mean loss: 542.02
train mean loss: 555.39
epoch train time: 0:00:15.314905
elapsed time: 0:06:54.021217
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 09:32:50.136391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 535.86
 ---- batch: 020 ----
mean loss: 543.03
 ---- batch: 030 ----
mean loss: 525.08
 ---- batch: 040 ----
mean loss: 534.60
 ---- batch: 050 ----
mean loss: 534.88
 ---- batch: 060 ----
mean loss: 529.49
 ---- batch: 070 ----
mean loss: 523.11
 ---- batch: 080 ----
mean loss: 523.28
 ---- batch: 090 ----
mean loss: 514.49
 ---- batch: 100 ----
mean loss: 510.47
 ---- batch: 110 ----
mean loss: 513.99
train mean loss: 525.61
epoch train time: 0:00:15.291832
elapsed time: 0:07:09.313914
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 09:33:05.429100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.76
 ---- batch: 020 ----
mean loss: 508.24
 ---- batch: 030 ----
mean loss: 499.75
 ---- batch: 040 ----
mean loss: 497.90
 ---- batch: 050 ----
mean loss: 489.76
 ---- batch: 060 ----
mean loss: 491.45
 ---- batch: 070 ----
mean loss: 479.82
 ---- batch: 080 ----
mean loss: 510.39
 ---- batch: 090 ----
mean loss: 500.74
 ---- batch: 100 ----
mean loss: 478.44
 ---- batch: 110 ----
mean loss: 482.56
train mean loss: 496.38
epoch train time: 0:00:15.269890
elapsed time: 0:07:24.584794
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 09:33:20.700075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.82
 ---- batch: 020 ----
mean loss: 479.19
 ---- batch: 030 ----
mean loss: 483.91
 ---- batch: 040 ----
mean loss: 477.30
 ---- batch: 050 ----
mean loss: 478.06
 ---- batch: 060 ----
mean loss: 460.31
 ---- batch: 070 ----
mean loss: 463.06
 ---- batch: 080 ----
mean loss: 469.59
 ---- batch: 090 ----
mean loss: 464.64
 ---- batch: 100 ----
mean loss: 459.41
 ---- batch: 110 ----
mean loss: 457.24
train mean loss: 469.27
epoch train time: 0:00:15.252304
elapsed time: 0:07:39.838270
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 09:33:35.953540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.82
 ---- batch: 020 ----
mean loss: 460.72
 ---- batch: 030 ----
mean loss: 449.17
 ---- batch: 040 ----
mean loss: 469.21
 ---- batch: 050 ----
mean loss: 460.46
 ---- batch: 060 ----
mean loss: 437.10
 ---- batch: 070 ----
mean loss: 443.72
 ---- batch: 080 ----
mean loss: 441.54
 ---- batch: 090 ----
mean loss: 454.84
 ---- batch: 100 ----
mean loss: 448.62
 ---- batch: 110 ----
mean loss: 446.61
train mean loss: 450.95
epoch train time: 0:00:15.291139
elapsed time: 0:07:55.130481
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 09:33:51.245685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.07
 ---- batch: 020 ----
mean loss: 435.38
 ---- batch: 030 ----
mean loss: 432.45
 ---- batch: 040 ----
mean loss: 434.58
 ---- batch: 050 ----
mean loss: 441.75
 ---- batch: 060 ----
mean loss: 421.96
 ---- batch: 070 ----
mean loss: 421.70
 ---- batch: 080 ----
mean loss: 423.04
 ---- batch: 090 ----
mean loss: 422.20
 ---- batch: 100 ----
mean loss: 438.60
 ---- batch: 110 ----
mean loss: 428.04
train mean loss: 430.19
epoch train time: 0:00:15.252692
elapsed time: 0:08:10.384108
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 09:34:06.499287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.81
 ---- batch: 020 ----
mean loss: 431.45
 ---- batch: 030 ----
mean loss: 424.83
 ---- batch: 040 ----
mean loss: 417.00
 ---- batch: 050 ----
mean loss: 403.18
 ---- batch: 060 ----
mean loss: 420.19
 ---- batch: 070 ----
mean loss: 393.23
 ---- batch: 080 ----
mean loss: 421.09
 ---- batch: 090 ----
mean loss: 413.05
 ---- batch: 100 ----
mean loss: 411.68
 ---- batch: 110 ----
mean loss: 411.48
train mean loss: 415.65
epoch train time: 0:00:15.262610
elapsed time: 0:08:25.647645
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 09:34:21.762935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.71
 ---- batch: 020 ----
mean loss: 398.52
 ---- batch: 030 ----
mean loss: 409.54
 ---- batch: 040 ----
mean loss: 398.83
 ---- batch: 050 ----
mean loss: 400.28
 ---- batch: 060 ----
mean loss: 409.61
 ---- batch: 070 ----
mean loss: 391.56
 ---- batch: 080 ----
mean loss: 398.69
 ---- batch: 090 ----
mean loss: 390.32
 ---- batch: 100 ----
mean loss: 399.77
 ---- batch: 110 ----
mean loss: 393.29
train mean loss: 399.68
epoch train time: 0:00:15.283453
elapsed time: 0:08:40.932217
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 09:34:37.047393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.48
 ---- batch: 020 ----
mean loss: 394.13
 ---- batch: 030 ----
mean loss: 395.97
 ---- batch: 040 ----
mean loss: 381.16
 ---- batch: 050 ----
mean loss: 385.50
 ---- batch: 060 ----
mean loss: 386.40
 ---- batch: 070 ----
mean loss: 380.51
 ---- batch: 080 ----
mean loss: 405.67
 ---- batch: 090 ----
mean loss: 383.86
 ---- batch: 100 ----
mean loss: 390.30
 ---- batch: 110 ----
mean loss: 385.98
train mean loss: 389.53
epoch train time: 0:00:15.250825
elapsed time: 0:08:56.183947
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 09:34:52.299116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.46
 ---- batch: 020 ----
mean loss: 384.21
 ---- batch: 030 ----
mean loss: 381.93
 ---- batch: 040 ----
mean loss: 372.01
 ---- batch: 050 ----
mean loss: 372.23
 ---- batch: 060 ----
mean loss: 365.45
 ---- batch: 070 ----
mean loss: 391.40
 ---- batch: 080 ----
mean loss: 371.68
 ---- batch: 090 ----
mean loss: 389.38
 ---- batch: 100 ----
mean loss: 368.94
 ---- batch: 110 ----
mean loss: 372.33
train mean loss: 378.34
epoch train time: 0:00:15.255189
elapsed time: 0:09:11.440019
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 09:35:07.555230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.84
 ---- batch: 020 ----
mean loss: 362.49
 ---- batch: 030 ----
mean loss: 373.04
 ---- batch: 040 ----
mean loss: 380.30
 ---- batch: 050 ----
mean loss: 357.73
 ---- batch: 060 ----
mean loss: 370.94
 ---- batch: 070 ----
mean loss: 357.47
 ---- batch: 080 ----
mean loss: 374.01
 ---- batch: 090 ----
mean loss: 371.15
 ---- batch: 100 ----
mean loss: 373.20
 ---- batch: 110 ----
mean loss: 365.96
train mean loss: 368.55
epoch train time: 0:00:15.329463
elapsed time: 0:09:26.770562
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 09:35:22.885814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.88
 ---- batch: 020 ----
mean loss: 362.33
 ---- batch: 030 ----
mean loss: 363.90
 ---- batch: 040 ----
mean loss: 350.80
 ---- batch: 050 ----
mean loss: 361.79
 ---- batch: 060 ----
mean loss: 370.95
 ---- batch: 070 ----
mean loss: 357.90
 ---- batch: 080 ----
mean loss: 362.03
 ---- batch: 090 ----
mean loss: 358.41
 ---- batch: 100 ----
mean loss: 368.57
 ---- batch: 110 ----
mean loss: 353.14
train mean loss: 361.94
epoch train time: 0:00:15.644458
elapsed time: 0:09:42.416107
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 09:35:38.531294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.67
 ---- batch: 020 ----
mean loss: 348.84
 ---- batch: 030 ----
mean loss: 354.48
 ---- batch: 040 ----
mean loss: 350.79
 ---- batch: 050 ----
mean loss: 355.92
 ---- batch: 060 ----
mean loss: 364.07
 ---- batch: 070 ----
mean loss: 357.54
 ---- batch: 080 ----
mean loss: 341.17
 ---- batch: 090 ----
mean loss: 349.91
 ---- batch: 100 ----
mean loss: 341.69
 ---- batch: 110 ----
mean loss: 340.20
train mean loss: 350.01
epoch train time: 0:00:15.524644
elapsed time: 0:09:57.941728
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 09:35:54.056947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.27
 ---- batch: 020 ----
mean loss: 344.07
 ---- batch: 030 ----
mean loss: 354.94
 ---- batch: 040 ----
mean loss: 349.34
 ---- batch: 050 ----
mean loss: 341.90
 ---- batch: 060 ----
mean loss: 347.73
 ---- batch: 070 ----
mean loss: 334.03
 ---- batch: 080 ----
mean loss: 343.98
 ---- batch: 090 ----
mean loss: 331.62
 ---- batch: 100 ----
mean loss: 346.80
 ---- batch: 110 ----
mean loss: 348.48
train mean loss: 343.38
epoch train time: 0:00:15.562573
elapsed time: 0:10:13.505331
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 09:36:09.620541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.08
 ---- batch: 020 ----
mean loss: 341.02
 ---- batch: 030 ----
mean loss: 326.74
 ---- batch: 040 ----
mean loss: 334.60
 ---- batch: 050 ----
mean loss: 332.51
 ---- batch: 060 ----
mean loss: 334.12
 ---- batch: 070 ----
mean loss: 330.86
 ---- batch: 080 ----
mean loss: 348.82
 ---- batch: 090 ----
mean loss: 326.46
 ---- batch: 100 ----
mean loss: 333.56
 ---- batch: 110 ----
mean loss: 323.54
train mean loss: 334.59
epoch train time: 0:00:15.642552
elapsed time: 0:10:29.148835
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 09:36:25.264061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.05
 ---- batch: 020 ----
mean loss: 328.41
 ---- batch: 030 ----
mean loss: 320.96
 ---- batch: 040 ----
mean loss: 328.14
 ---- batch: 050 ----
mean loss: 324.90
 ---- batch: 060 ----
mean loss: 331.73
 ---- batch: 070 ----
mean loss: 337.75
 ---- batch: 080 ----
mean loss: 331.40
 ---- batch: 090 ----
mean loss: 332.72
 ---- batch: 100 ----
mean loss: 334.69
 ---- batch: 110 ----
mean loss: 327.43
train mean loss: 329.93
epoch train time: 0:00:15.654067
elapsed time: 0:10:44.803898
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 09:36:40.919108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.28
 ---- batch: 020 ----
mean loss: 323.40
 ---- batch: 030 ----
mean loss: 325.64
 ---- batch: 040 ----
mean loss: 330.37
 ---- batch: 050 ----
mean loss: 321.57
 ---- batch: 060 ----
mean loss: 326.38
 ---- batch: 070 ----
mean loss: 323.79
 ---- batch: 080 ----
mean loss: 319.26
 ---- batch: 090 ----
mean loss: 323.08
 ---- batch: 100 ----
mean loss: 314.98
 ---- batch: 110 ----
mean loss: 324.16
train mean loss: 322.89
epoch train time: 0:00:15.661978
elapsed time: 0:11:00.466884
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 09:36:56.582140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.59
 ---- batch: 020 ----
mean loss: 316.74
 ---- batch: 030 ----
mean loss: 324.88
 ---- batch: 040 ----
mean loss: 327.70
 ---- batch: 050 ----
mean loss: 319.25
 ---- batch: 060 ----
mean loss: 310.33
 ---- batch: 070 ----
mean loss: 325.13
 ---- batch: 080 ----
mean loss: 319.93
 ---- batch: 090 ----
mean loss: 318.12
 ---- batch: 100 ----
mean loss: 313.13
 ---- batch: 110 ----
mean loss: 319.90
train mean loss: 318.42
epoch train time: 0:00:15.629421
elapsed time: 0:11:16.097318
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 09:37:12.212536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.47
 ---- batch: 020 ----
mean loss: 314.11
 ---- batch: 030 ----
mean loss: 310.83
 ---- batch: 040 ----
mean loss: 322.53
 ---- batch: 050 ----
mean loss: 311.98
 ---- batch: 060 ----
mean loss: 310.29
 ---- batch: 070 ----
mean loss: 311.32
 ---- batch: 080 ----
mean loss: 316.66
 ---- batch: 090 ----
mean loss: 306.91
 ---- batch: 100 ----
mean loss: 310.23
 ---- batch: 110 ----
mean loss: 317.83
train mean loss: 313.48
epoch train time: 0:00:15.675949
elapsed time: 0:11:31.774298
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 09:37:27.889514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.62
 ---- batch: 020 ----
mean loss: 308.66
 ---- batch: 030 ----
mean loss: 308.74
 ---- batch: 040 ----
mean loss: 316.42
 ---- batch: 050 ----
mean loss: 303.58
 ---- batch: 060 ----
mean loss: 306.78
 ---- batch: 070 ----
mean loss: 308.56
 ---- batch: 080 ----
mean loss: 314.33
 ---- batch: 090 ----
mean loss: 303.93
 ---- batch: 100 ----
mean loss: 316.34
 ---- batch: 110 ----
mean loss: 302.95
train mean loss: 308.46
epoch train time: 0:00:15.692294
elapsed time: 0:11:47.467532
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 09:37:43.582727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.67
 ---- batch: 020 ----
mean loss: 309.59
 ---- batch: 030 ----
mean loss: 301.21
 ---- batch: 040 ----
mean loss: 301.30
 ---- batch: 050 ----
mean loss: 310.96
 ---- batch: 060 ----
mean loss: 319.45
 ---- batch: 070 ----
mean loss: 296.87
 ---- batch: 080 ----
mean loss: 299.87
 ---- batch: 090 ----
mean loss: 312.78
 ---- batch: 100 ----
mean loss: 310.96
 ---- batch: 110 ----
mean loss: 286.25
train mean loss: 303.88
epoch train time: 0:00:15.764306
elapsed time: 0:12:03.232819
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 09:37:59.348018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.13
 ---- batch: 020 ----
mean loss: 309.38
 ---- batch: 030 ----
mean loss: 306.79
 ---- batch: 040 ----
mean loss: 302.05
 ---- batch: 050 ----
mean loss: 294.78
 ---- batch: 060 ----
mean loss: 290.00
 ---- batch: 070 ----
mean loss: 300.45
 ---- batch: 080 ----
mean loss: 307.13
 ---- batch: 090 ----
mean loss: 298.64
 ---- batch: 100 ----
mean loss: 306.55
 ---- batch: 110 ----
mean loss: 294.11
train mean loss: 301.70
epoch train time: 0:00:15.793114
elapsed time: 0:12:19.026769
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 09:38:15.141948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.22
 ---- batch: 020 ----
mean loss: 297.00
 ---- batch: 030 ----
mean loss: 292.55
 ---- batch: 040 ----
mean loss: 285.70
 ---- batch: 050 ----
mean loss: 300.58
 ---- batch: 060 ----
mean loss: 290.83
 ---- batch: 070 ----
mean loss: 298.63
 ---- batch: 080 ----
mean loss: 291.30
 ---- batch: 090 ----
mean loss: 281.95
 ---- batch: 100 ----
mean loss: 298.82
 ---- batch: 110 ----
mean loss: 295.29
train mean loss: 293.18
epoch train time: 0:00:15.725472
elapsed time: 0:12:34.753222
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 09:38:30.868378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.52
 ---- batch: 020 ----
mean loss: 305.75
 ---- batch: 030 ----
mean loss: 293.16
 ---- batch: 040 ----
mean loss: 291.74
 ---- batch: 050 ----
mean loss: 296.32
 ---- batch: 060 ----
mean loss: 294.89
 ---- batch: 070 ----
mean loss: 296.04
 ---- batch: 080 ----
mean loss: 294.99
 ---- batch: 090 ----
mean loss: 290.51
 ---- batch: 100 ----
mean loss: 273.46
 ---- batch: 110 ----
mean loss: 285.86
train mean loss: 291.46
epoch train time: 0:00:15.675166
elapsed time: 0:12:50.429377
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 09:38:46.544664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.73
 ---- batch: 020 ----
mean loss: 288.50
 ---- batch: 030 ----
mean loss: 285.47
 ---- batch: 040 ----
mean loss: 282.23
 ---- batch: 050 ----
mean loss: 277.40
 ---- batch: 060 ----
mean loss: 290.47
 ---- batch: 070 ----
mean loss: 292.30
 ---- batch: 080 ----
mean loss: 296.36
 ---- batch: 090 ----
mean loss: 292.17
 ---- batch: 100 ----
mean loss: 281.86
 ---- batch: 110 ----
mean loss: 289.56
train mean loss: 288.07
epoch train time: 0:00:15.722167
elapsed time: 0:13:06.152717
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 09:39:02.267955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.58
 ---- batch: 020 ----
mean loss: 289.57
 ---- batch: 030 ----
mean loss: 277.65
 ---- batch: 040 ----
mean loss: 294.24
 ---- batch: 050 ----
mean loss: 287.55
 ---- batch: 060 ----
mean loss: 283.27
 ---- batch: 070 ----
mean loss: 282.40
 ---- batch: 080 ----
mean loss: 283.23
 ---- batch: 090 ----
mean loss: 280.34
 ---- batch: 100 ----
mean loss: 281.08
 ---- batch: 110 ----
mean loss: 272.43
train mean loss: 284.25
epoch train time: 0:00:15.723911
elapsed time: 0:13:21.877627
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 09:39:17.992786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.36
 ---- batch: 020 ----
mean loss: 277.11
 ---- batch: 030 ----
mean loss: 278.59
 ---- batch: 040 ----
mean loss: 280.06
 ---- batch: 050 ----
mean loss: 284.46
 ---- batch: 060 ----
mean loss: 279.62
 ---- batch: 070 ----
mean loss: 276.87
 ---- batch: 080 ----
mean loss: 270.67
 ---- batch: 090 ----
mean loss: 292.79
 ---- batch: 100 ----
mean loss: 280.22
 ---- batch: 110 ----
mean loss: 293.63
train mean loss: 280.89
epoch train time: 0:00:15.695033
elapsed time: 0:13:37.573644
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 09:39:33.688858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.39
 ---- batch: 020 ----
mean loss: 283.16
 ---- batch: 030 ----
mean loss: 283.61
 ---- batch: 040 ----
mean loss: 273.21
 ---- batch: 050 ----
mean loss: 274.78
 ---- batch: 060 ----
mean loss: 274.64
 ---- batch: 070 ----
mean loss: 277.88
 ---- batch: 080 ----
mean loss: 276.82
 ---- batch: 090 ----
mean loss: 275.09
 ---- batch: 100 ----
mean loss: 277.67
 ---- batch: 110 ----
mean loss: 278.07
train mean loss: 277.33
epoch train time: 0:00:15.708323
elapsed time: 0:13:53.282872
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 09:39:49.398156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.06
 ---- batch: 020 ----
mean loss: 271.26
 ---- batch: 030 ----
mean loss: 273.44
 ---- batch: 040 ----
mean loss: 274.67
 ---- batch: 050 ----
mean loss: 265.37
 ---- batch: 060 ----
mean loss: 271.08
 ---- batch: 070 ----
mean loss: 270.24
 ---- batch: 080 ----
mean loss: 272.19
 ---- batch: 090 ----
mean loss: 266.79
 ---- batch: 100 ----
mean loss: 270.35
 ---- batch: 110 ----
mean loss: 276.77
train mean loss: 272.42
epoch train time: 0:00:15.697135
elapsed time: 0:14:08.981043
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 09:40:05.096200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.52
 ---- batch: 020 ----
mean loss: 272.62
 ---- batch: 030 ----
mean loss: 269.45
 ---- batch: 040 ----
mean loss: 266.56
 ---- batch: 050 ----
mean loss: 277.05
 ---- batch: 060 ----
mean loss: 275.69
 ---- batch: 070 ----
mean loss: 267.72
 ---- batch: 080 ----
mean loss: 266.72
 ---- batch: 090 ----
mean loss: 269.27
 ---- batch: 100 ----
mean loss: 269.15
 ---- batch: 110 ----
mean loss: 272.16
train mean loss: 271.13
epoch train time: 0:00:15.699605
elapsed time: 0:14:24.681611
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 09:40:20.796861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.17
 ---- batch: 020 ----
mean loss: 271.82
 ---- batch: 030 ----
mean loss: 274.91
 ---- batch: 040 ----
mean loss: 267.72
 ---- batch: 050 ----
mean loss: 273.45
 ---- batch: 060 ----
mean loss: 263.96
 ---- batch: 070 ----
mean loss: 258.91
 ---- batch: 080 ----
mean loss: 261.69
 ---- batch: 090 ----
mean loss: 263.22
 ---- batch: 100 ----
mean loss: 277.34
 ---- batch: 110 ----
mean loss: 280.06
train mean loss: 270.24
epoch train time: 0:00:15.718789
elapsed time: 0:14:40.401409
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 09:40:36.516587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.12
 ---- batch: 020 ----
mean loss: 263.93
 ---- batch: 030 ----
mean loss: 276.92
 ---- batch: 040 ----
mean loss: 267.43
 ---- batch: 050 ----
mean loss: 270.59
 ---- batch: 060 ----
mean loss: 262.09
 ---- batch: 070 ----
mean loss: 265.23
 ---- batch: 080 ----
mean loss: 255.04
 ---- batch: 090 ----
mean loss: 270.11
 ---- batch: 100 ----
mean loss: 258.20
 ---- batch: 110 ----
mean loss: 260.54
train mean loss: 265.29
epoch train time: 0:00:15.741577
elapsed time: 0:14:56.143937
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 09:40:52.259191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.27
 ---- batch: 020 ----
mean loss: 264.48
 ---- batch: 030 ----
mean loss: 271.56
 ---- batch: 040 ----
mean loss: 260.49
 ---- batch: 050 ----
mean loss: 272.46
 ---- batch: 060 ----
mean loss: 266.12
 ---- batch: 070 ----
mean loss: 255.90
 ---- batch: 080 ----
mean loss: 250.44
 ---- batch: 090 ----
mean loss: 261.10
 ---- batch: 100 ----
mean loss: 257.03
 ---- batch: 110 ----
mean loss: 263.28
train mean loss: 262.95
epoch train time: 0:00:15.695388
elapsed time: 0:15:11.840412
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 09:41:07.955636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.16
 ---- batch: 020 ----
mean loss: 259.51
 ---- batch: 030 ----
mean loss: 259.75
 ---- batch: 040 ----
mean loss: 251.47
 ---- batch: 050 ----
mean loss: 261.01
 ---- batch: 060 ----
mean loss: 253.13
 ---- batch: 070 ----
mean loss: 260.93
 ---- batch: 080 ----
mean loss: 255.72
 ---- batch: 090 ----
mean loss: 259.72
 ---- batch: 100 ----
mean loss: 257.71
 ---- batch: 110 ----
mean loss: 265.15
train mean loss: 258.97
epoch train time: 0:00:15.824902
elapsed time: 0:15:27.666342
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 09:41:23.781580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.59
 ---- batch: 020 ----
mean loss: 263.15
 ---- batch: 030 ----
mean loss: 257.04
 ---- batch: 040 ----
mean loss: 256.63
 ---- batch: 050 ----
mean loss: 252.56
 ---- batch: 060 ----
mean loss: 247.89
 ---- batch: 070 ----
mean loss: 256.76
 ---- batch: 080 ----
mean loss: 261.71
 ---- batch: 090 ----
mean loss: 250.94
 ---- batch: 100 ----
mean loss: 257.98
 ---- batch: 110 ----
mean loss: 256.88
train mean loss: 255.51
epoch train time: 0:00:15.753876
elapsed time: 0:15:43.421240
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 09:41:39.536464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.79
 ---- batch: 020 ----
mean loss: 250.33
 ---- batch: 030 ----
mean loss: 258.36
 ---- batch: 040 ----
mean loss: 253.74
 ---- batch: 050 ----
mean loss: 245.37
 ---- batch: 060 ----
mean loss: 249.36
 ---- batch: 070 ----
mean loss: 249.24
 ---- batch: 080 ----
mean loss: 261.81
 ---- batch: 090 ----
mean loss: 260.63
 ---- batch: 100 ----
mean loss: 254.26
 ---- batch: 110 ----
mean loss: 263.25
train mean loss: 254.57
epoch train time: 0:00:15.797954
elapsed time: 0:15:59.220200
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 09:41:55.335513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.71
 ---- batch: 020 ----
mean loss: 245.50
 ---- batch: 030 ----
mean loss: 255.49
 ---- batch: 040 ----
mean loss: 250.95
 ---- batch: 050 ----
mean loss: 252.92
 ---- batch: 060 ----
mean loss: 253.06
 ---- batch: 070 ----
mean loss: 245.69
 ---- batch: 080 ----
mean loss: 259.19
 ---- batch: 090 ----
mean loss: 255.91
 ---- batch: 100 ----
mean loss: 250.62
 ---- batch: 110 ----
mean loss: 255.64
train mean loss: 252.13
epoch train time: 0:00:15.741234
elapsed time: 0:16:14.962546
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 09:42:11.077777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.30
 ---- batch: 020 ----
mean loss: 255.91
 ---- batch: 030 ----
mean loss: 250.93
 ---- batch: 040 ----
mean loss: 237.64
 ---- batch: 050 ----
mean loss: 247.84
 ---- batch: 060 ----
mean loss: 247.74
 ---- batch: 070 ----
mean loss: 262.81
 ---- batch: 080 ----
mean loss: 245.22
 ---- batch: 090 ----
mean loss: 250.09
 ---- batch: 100 ----
mean loss: 241.07
 ---- batch: 110 ----
mean loss: 251.36
train mean loss: 248.15
epoch train time: 0:00:15.770692
elapsed time: 0:16:30.734248
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 09:42:26.849637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.32
 ---- batch: 020 ----
mean loss: 240.18
 ---- batch: 030 ----
mean loss: 246.84
 ---- batch: 040 ----
mean loss: 251.16
 ---- batch: 050 ----
mean loss: 238.22
 ---- batch: 060 ----
mean loss: 249.92
 ---- batch: 070 ----
mean loss: 235.58
 ---- batch: 080 ----
mean loss: 252.34
 ---- batch: 090 ----
mean loss: 247.65
 ---- batch: 100 ----
mean loss: 246.50
 ---- batch: 110 ----
mean loss: 247.89
train mean loss: 246.23
epoch train time: 0:00:15.769547
elapsed time: 0:16:46.504975
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 09:42:42.620250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.65
 ---- batch: 020 ----
mean loss: 237.82
 ---- batch: 030 ----
mean loss: 246.53
 ---- batch: 040 ----
mean loss: 244.56
 ---- batch: 050 ----
mean loss: 236.73
 ---- batch: 060 ----
mean loss: 243.47
 ---- batch: 070 ----
mean loss: 248.05
 ---- batch: 080 ----
mean loss: 245.59
 ---- batch: 090 ----
mean loss: 244.45
 ---- batch: 100 ----
mean loss: 237.06
 ---- batch: 110 ----
mean loss: 253.62
train mean loss: 243.35
epoch train time: 0:00:15.787826
elapsed time: 0:17:02.293733
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 09:42:58.408941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.72
 ---- batch: 020 ----
mean loss: 247.55
 ---- batch: 030 ----
mean loss: 239.22
 ---- batch: 040 ----
mean loss: 229.91
 ---- batch: 050 ----
mean loss: 242.94
 ---- batch: 060 ----
mean loss: 243.91
 ---- batch: 070 ----
mean loss: 248.78
 ---- batch: 080 ----
mean loss: 244.85
 ---- batch: 090 ----
mean loss: 249.96
 ---- batch: 100 ----
mean loss: 246.83
 ---- batch: 110 ----
mean loss: 237.64
train mean loss: 243.56
epoch train time: 0:00:15.837876
elapsed time: 0:17:18.132575
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 09:43:14.247763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.75
 ---- batch: 020 ----
mean loss: 234.87
 ---- batch: 030 ----
mean loss: 247.86
 ---- batch: 040 ----
mean loss: 247.73
 ---- batch: 050 ----
mean loss: 243.56
 ---- batch: 060 ----
mean loss: 238.66
 ---- batch: 070 ----
mean loss: 245.18
 ---- batch: 080 ----
mean loss: 233.77
 ---- batch: 090 ----
mean loss: 233.68
 ---- batch: 100 ----
mean loss: 233.76
 ---- batch: 110 ----
mean loss: 237.70
train mean loss: 240.32
epoch train time: 0:00:15.754487
elapsed time: 0:17:33.888066
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 09:43:30.003265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.14
 ---- batch: 020 ----
mean loss: 237.41
 ---- batch: 030 ----
mean loss: 239.52
 ---- batch: 040 ----
mean loss: 235.47
 ---- batch: 050 ----
mean loss: 227.72
 ---- batch: 060 ----
mean loss: 244.81
 ---- batch: 070 ----
mean loss: 234.86
 ---- batch: 080 ----
mean loss: 244.22
 ---- batch: 090 ----
mean loss: 240.28
 ---- batch: 100 ----
mean loss: 243.26
 ---- batch: 110 ----
mean loss: 237.14
train mean loss: 238.67
epoch train time: 0:00:15.739131
elapsed time: 0:17:49.628237
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 09:43:45.743431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.00
 ---- batch: 020 ----
mean loss: 233.70
 ---- batch: 030 ----
mean loss: 229.51
 ---- batch: 040 ----
mean loss: 236.85
 ---- batch: 050 ----
mean loss: 243.70
 ---- batch: 060 ----
mean loss: 228.54
 ---- batch: 070 ----
mean loss: 238.09
 ---- batch: 080 ----
mean loss: 232.47
 ---- batch: 090 ----
mean loss: 241.28
 ---- batch: 100 ----
mean loss: 234.49
 ---- batch: 110 ----
mean loss: 239.52
train mean loss: 235.87
epoch train time: 0:00:15.710212
elapsed time: 0:18:05.339333
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 09:44:01.454505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.80
 ---- batch: 020 ----
mean loss: 235.55
 ---- batch: 030 ----
mean loss: 227.77
 ---- batch: 040 ----
mean loss: 237.25
 ---- batch: 050 ----
mean loss: 233.77
 ---- batch: 060 ----
mean loss: 244.12
 ---- batch: 070 ----
mean loss: 231.81
 ---- batch: 080 ----
mean loss: 228.42
 ---- batch: 090 ----
mean loss: 240.25
 ---- batch: 100 ----
mean loss: 222.18
 ---- batch: 110 ----
mean loss: 233.19
train mean loss: 233.22
epoch train time: 0:00:15.647253
elapsed time: 0:18:20.987510
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 09:44:17.102689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.37
 ---- batch: 020 ----
mean loss: 230.67
 ---- batch: 030 ----
mean loss: 238.14
 ---- batch: 040 ----
mean loss: 224.72
 ---- batch: 050 ----
mean loss: 234.24
 ---- batch: 060 ----
mean loss: 240.25
 ---- batch: 070 ----
mean loss: 230.23
 ---- batch: 080 ----
mean loss: 230.28
 ---- batch: 090 ----
mean loss: 234.66
 ---- batch: 100 ----
mean loss: 225.58
 ---- batch: 110 ----
mean loss: 235.55
train mean loss: 232.95
epoch train time: 0:00:15.548741
elapsed time: 0:18:36.537244
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 09:44:32.652465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.33
 ---- batch: 020 ----
mean loss: 229.86
 ---- batch: 030 ----
mean loss: 239.88
 ---- batch: 040 ----
mean loss: 234.42
 ---- batch: 050 ----
mean loss: 221.40
 ---- batch: 060 ----
mean loss: 223.34
 ---- batch: 070 ----
mean loss: 234.35
 ---- batch: 080 ----
mean loss: 226.58
 ---- batch: 090 ----
mean loss: 232.55
 ---- batch: 100 ----
mean loss: 229.72
 ---- batch: 110 ----
mean loss: 231.93
train mean loss: 230.00
epoch train time: 0:00:15.467846
elapsed time: 0:18:52.006141
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 09:44:48.121323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.34
 ---- batch: 020 ----
mean loss: 225.06
 ---- batch: 030 ----
mean loss: 234.37
 ---- batch: 040 ----
mean loss: 230.64
 ---- batch: 050 ----
mean loss: 214.00
 ---- batch: 060 ----
mean loss: 226.71
 ---- batch: 070 ----
mean loss: 230.03
 ---- batch: 080 ----
mean loss: 237.57
 ---- batch: 090 ----
mean loss: 228.35
 ---- batch: 100 ----
mean loss: 225.29
 ---- batch: 110 ----
mean loss: 236.01
train mean loss: 229.33
epoch train time: 0:00:15.680552
elapsed time: 0:19:07.687677
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 09:45:03.802875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.70
 ---- batch: 020 ----
mean loss: 230.16
 ---- batch: 030 ----
mean loss: 227.78
 ---- batch: 040 ----
mean loss: 231.89
 ---- batch: 050 ----
mean loss: 230.42
 ---- batch: 060 ----
mean loss: 222.94
 ---- batch: 070 ----
mean loss: 218.29
 ---- batch: 080 ----
mean loss: 225.70
 ---- batch: 090 ----
mean loss: 223.58
 ---- batch: 100 ----
mean loss: 232.20
 ---- batch: 110 ----
mean loss: 221.53
train mean loss: 226.11
epoch train time: 0:00:15.759086
elapsed time: 0:19:23.447685
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 09:45:19.562896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.33
 ---- batch: 020 ----
mean loss: 234.62
 ---- batch: 030 ----
mean loss: 224.38
 ---- batch: 040 ----
mean loss: 222.57
 ---- batch: 050 ----
mean loss: 222.81
 ---- batch: 060 ----
mean loss: 231.89
 ---- batch: 070 ----
mean loss: 221.75
 ---- batch: 080 ----
mean loss: 232.93
 ---- batch: 090 ----
mean loss: 224.52
 ---- batch: 100 ----
mean loss: 223.80
 ---- batch: 110 ----
mean loss: 219.04
train mean loss: 225.93
epoch train time: 0:00:15.793209
elapsed time: 0:19:39.241881
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 09:45:35.357109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.62
 ---- batch: 020 ----
mean loss: 223.84
 ---- batch: 030 ----
mean loss: 225.42
 ---- batch: 040 ----
mean loss: 227.32
 ---- batch: 050 ----
mean loss: 217.85
 ---- batch: 060 ----
mean loss: 227.01
 ---- batch: 070 ----
mean loss: 228.70
 ---- batch: 080 ----
mean loss: 230.09
 ---- batch: 090 ----
mean loss: 224.90
 ---- batch: 100 ----
mean loss: 226.36
 ---- batch: 110 ----
mean loss: 223.52
train mean loss: 225.69
epoch train time: 0:00:15.783131
elapsed time: 0:19:55.026090
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 09:45:51.141301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.78
 ---- batch: 020 ----
mean loss: 215.34
 ---- batch: 030 ----
mean loss: 231.05
 ---- batch: 040 ----
mean loss: 223.22
 ---- batch: 050 ----
mean loss: 225.36
 ---- batch: 060 ----
mean loss: 217.39
 ---- batch: 070 ----
mean loss: 216.82
 ---- batch: 080 ----
mean loss: 220.12
 ---- batch: 090 ----
mean loss: 224.44
 ---- batch: 100 ----
mean loss: 218.52
 ---- batch: 110 ----
mean loss: 219.80
train mean loss: 221.16
epoch train time: 0:00:15.808488
elapsed time: 0:20:10.835565
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 09:46:06.950752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.94
 ---- batch: 020 ----
mean loss: 214.24
 ---- batch: 030 ----
mean loss: 225.24
 ---- batch: 040 ----
mean loss: 223.62
 ---- batch: 050 ----
mean loss: 227.04
 ---- batch: 060 ----
mean loss: 224.16
 ---- batch: 070 ----
mean loss: 219.92
 ---- batch: 080 ----
mean loss: 214.67
 ---- batch: 090 ----
mean loss: 213.86
 ---- batch: 100 ----
mean loss: 214.55
 ---- batch: 110 ----
mean loss: 221.76
train mean loss: 220.06
epoch train time: 0:00:15.813364
elapsed time: 0:20:26.649933
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 09:46:22.765155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.18
 ---- batch: 020 ----
mean loss: 225.39
 ---- batch: 030 ----
mean loss: 215.49
 ---- batch: 040 ----
mean loss: 216.54
 ---- batch: 050 ----
mean loss: 222.01
 ---- batch: 060 ----
mean loss: 221.37
 ---- batch: 070 ----
mean loss: 216.00
 ---- batch: 080 ----
mean loss: 222.80
 ---- batch: 090 ----
mean loss: 225.95
 ---- batch: 100 ----
mean loss: 226.99
 ---- batch: 110 ----
mean loss: 212.90
train mean loss: 220.19
epoch train time: 0:00:15.770581
elapsed time: 0:20:42.421624
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 09:46:38.536864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.63
 ---- batch: 020 ----
mean loss: 214.74
 ---- batch: 030 ----
mean loss: 210.85
 ---- batch: 040 ----
mean loss: 220.10
 ---- batch: 050 ----
mean loss: 225.36
 ---- batch: 060 ----
mean loss: 228.50
 ---- batch: 070 ----
mean loss: 223.30
 ---- batch: 080 ----
mean loss: 224.40
 ---- batch: 090 ----
mean loss: 217.91
 ---- batch: 100 ----
mean loss: 212.70
 ---- batch: 110 ----
mean loss: 222.23
train mean loss: 219.43
epoch train time: 0:00:15.733120
elapsed time: 0:20:58.155764
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 09:46:54.270931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.42
 ---- batch: 020 ----
mean loss: 216.33
 ---- batch: 030 ----
mean loss: 212.29
 ---- batch: 040 ----
mean loss: 220.96
 ---- batch: 050 ----
mean loss: 216.12
 ---- batch: 060 ----
mean loss: 220.61
 ---- batch: 070 ----
mean loss: 224.69
 ---- batch: 080 ----
mean loss: 210.92
 ---- batch: 090 ----
mean loss: 220.88
 ---- batch: 100 ----
mean loss: 208.38
 ---- batch: 110 ----
mean loss: 229.89
train mean loss: 217.18
epoch train time: 0:00:15.782863
elapsed time: 0:21:13.939621
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 09:47:10.054888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.52
 ---- batch: 020 ----
mean loss: 220.77
 ---- batch: 030 ----
mean loss: 207.65
 ---- batch: 040 ----
mean loss: 212.09
 ---- batch: 050 ----
mean loss: 217.99
 ---- batch: 060 ----
mean loss: 216.27
 ---- batch: 070 ----
mean loss: 224.68
 ---- batch: 080 ----
mean loss: 211.36
 ---- batch: 090 ----
mean loss: 217.77
 ---- batch: 100 ----
mean loss: 214.87
 ---- batch: 110 ----
mean loss: 213.28
train mean loss: 215.23
epoch train time: 0:00:15.760495
elapsed time: 0:21:29.701185
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 09:47:25.816368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.27
 ---- batch: 020 ----
mean loss: 213.27
 ---- batch: 030 ----
mean loss: 210.95
 ---- batch: 040 ----
mean loss: 212.68
 ---- batch: 050 ----
mean loss: 216.70
 ---- batch: 060 ----
mean loss: 213.38
 ---- batch: 070 ----
mean loss: 208.53
 ---- batch: 080 ----
mean loss: 229.52
 ---- batch: 090 ----
mean loss: 219.17
 ---- batch: 100 ----
mean loss: 204.67
 ---- batch: 110 ----
mean loss: 221.23
train mean loss: 214.63
epoch train time: 0:00:15.766174
elapsed time: 0:21:45.468204
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 09:47:41.583416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.34
 ---- batch: 020 ----
mean loss: 215.00
 ---- batch: 030 ----
mean loss: 222.19
 ---- batch: 040 ----
mean loss: 218.61
 ---- batch: 050 ----
mean loss: 209.85
 ---- batch: 060 ----
mean loss: 214.28
 ---- batch: 070 ----
mean loss: 215.72
 ---- batch: 080 ----
mean loss: 206.00
 ---- batch: 090 ----
mean loss: 212.54
 ---- batch: 100 ----
mean loss: 207.25
 ---- batch: 110 ----
mean loss: 209.07
train mean loss: 212.79
epoch train time: 0:00:15.781625
elapsed time: 0:22:01.250835
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 09:47:57.366061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.78
 ---- batch: 020 ----
mean loss: 213.39
 ---- batch: 030 ----
mean loss: 212.31
 ---- batch: 040 ----
mean loss: 204.99
 ---- batch: 050 ----
mean loss: 213.72
 ---- batch: 060 ----
mean loss: 213.05
 ---- batch: 070 ----
mean loss: 207.33
 ---- batch: 080 ----
mean loss: 213.90
 ---- batch: 090 ----
mean loss: 217.85
 ---- batch: 100 ----
mean loss: 220.20
 ---- batch: 110 ----
mean loss: 211.08
train mean loss: 212.52
epoch train time: 0:00:15.844247
elapsed time: 0:22:17.096120
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 09:48:13.211288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.79
 ---- batch: 020 ----
mean loss: 217.46
 ---- batch: 030 ----
mean loss: 211.60
 ---- batch: 040 ----
mean loss: 206.64
 ---- batch: 050 ----
mean loss: 202.71
 ---- batch: 060 ----
mean loss: 213.62
 ---- batch: 070 ----
mean loss: 216.79
 ---- batch: 080 ----
mean loss: 220.08
 ---- batch: 090 ----
mean loss: 209.73
 ---- batch: 100 ----
mean loss: 212.00
 ---- batch: 110 ----
mean loss: 204.31
train mean loss: 211.77
epoch train time: 0:00:15.791013
elapsed time: 0:22:32.888109
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 09:48:29.003294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.17
 ---- batch: 020 ----
mean loss: 213.86
 ---- batch: 030 ----
mean loss: 199.98
 ---- batch: 040 ----
mean loss: 212.27
 ---- batch: 050 ----
mean loss: 206.35
 ---- batch: 060 ----
mean loss: 208.72
 ---- batch: 070 ----
mean loss: 212.62
 ---- batch: 080 ----
mean loss: 215.23
 ---- batch: 090 ----
mean loss: 212.36
 ---- batch: 100 ----
mean loss: 208.09
 ---- batch: 110 ----
mean loss: 211.29
train mean loss: 209.21
epoch train time: 0:00:15.707497
elapsed time: 0:22:48.596659
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 09:48:44.711886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.99
 ---- batch: 020 ----
mean loss: 222.21
 ---- batch: 030 ----
mean loss: 200.59
 ---- batch: 040 ----
mean loss: 202.27
 ---- batch: 050 ----
mean loss: 212.07
 ---- batch: 060 ----
mean loss: 211.67
 ---- batch: 070 ----
mean loss: 201.29
 ---- batch: 080 ----
mean loss: 206.64
 ---- batch: 090 ----
mean loss: 211.45
 ---- batch: 100 ----
mean loss: 210.62
 ---- batch: 110 ----
mean loss: 214.61
train mean loss: 208.72
epoch train time: 0:00:15.733349
elapsed time: 0:23:04.331049
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 09:49:00.446245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.81
 ---- batch: 020 ----
mean loss: 209.28
 ---- batch: 030 ----
mean loss: 207.31
 ---- batch: 040 ----
mean loss: 198.97
 ---- batch: 050 ----
mean loss: 204.16
 ---- batch: 060 ----
mean loss: 210.45
 ---- batch: 070 ----
mean loss: 213.28
 ---- batch: 080 ----
mean loss: 222.41
 ---- batch: 090 ----
mean loss: 207.33
 ---- batch: 100 ----
mean loss: 197.37
 ---- batch: 110 ----
mean loss: 209.12
train mean loss: 208.27
epoch train time: 0:00:15.753686
elapsed time: 0:23:20.085680
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 09:49:16.200870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.72
 ---- batch: 020 ----
mean loss: 201.82
 ---- batch: 030 ----
mean loss: 215.16
 ---- batch: 040 ----
mean loss: 206.10
 ---- batch: 050 ----
mean loss: 212.51
 ---- batch: 060 ----
mean loss: 209.25
 ---- batch: 070 ----
mean loss: 206.16
 ---- batch: 080 ----
mean loss: 208.50
 ---- batch: 090 ----
mean loss: 213.46
 ---- batch: 100 ----
mean loss: 209.78
 ---- batch: 110 ----
mean loss: 208.83
train mean loss: 207.67
epoch train time: 0:00:15.756230
elapsed time: 0:23:35.842902
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 09:49:31.958225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.18
 ---- batch: 020 ----
mean loss: 212.41
 ---- batch: 030 ----
mean loss: 205.34
 ---- batch: 040 ----
mean loss: 212.28
 ---- batch: 050 ----
mean loss: 197.49
 ---- batch: 060 ----
mean loss: 201.54
 ---- batch: 070 ----
mean loss: 208.84
 ---- batch: 080 ----
mean loss: 204.93
 ---- batch: 090 ----
mean loss: 204.36
 ---- batch: 100 ----
mean loss: 200.53
 ---- batch: 110 ----
mean loss: 204.30
train mean loss: 205.47
epoch train time: 0:00:15.738471
elapsed time: 0:23:51.582592
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 09:49:47.697825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.80
 ---- batch: 020 ----
mean loss: 202.65
 ---- batch: 030 ----
mean loss: 198.26
 ---- batch: 040 ----
mean loss: 203.15
 ---- batch: 050 ----
mean loss: 197.43
 ---- batch: 060 ----
mean loss: 209.49
 ---- batch: 070 ----
mean loss: 214.14
 ---- batch: 080 ----
mean loss: 209.00
 ---- batch: 090 ----
mean loss: 197.88
 ---- batch: 100 ----
mean loss: 208.26
 ---- batch: 110 ----
mean loss: 210.47
train mean loss: 205.11
epoch train time: 0:00:15.712576
elapsed time: 0:24:07.296163
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 09:50:03.411365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.48
 ---- batch: 020 ----
mean loss: 209.53
 ---- batch: 030 ----
mean loss: 208.28
 ---- batch: 040 ----
mean loss: 202.39
 ---- batch: 050 ----
mean loss: 200.60
 ---- batch: 060 ----
mean loss: 207.57
 ---- batch: 070 ----
mean loss: 204.39
 ---- batch: 080 ----
mean loss: 199.59
 ---- batch: 090 ----
mean loss: 203.20
 ---- batch: 100 ----
mean loss: 200.24
 ---- batch: 110 ----
mean loss: 206.99
train mean loss: 204.28
epoch train time: 0:00:15.725455
elapsed time: 0:24:23.022625
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 09:50:19.137815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.23
 ---- batch: 020 ----
mean loss: 207.41
 ---- batch: 030 ----
mean loss: 200.41
 ---- batch: 040 ----
mean loss: 212.64
 ---- batch: 050 ----
mean loss: 194.10
 ---- batch: 060 ----
mean loss: 198.58
 ---- batch: 070 ----
mean loss: 205.97
 ---- batch: 080 ----
mean loss: 211.19
 ---- batch: 090 ----
mean loss: 199.71
 ---- batch: 100 ----
mean loss: 198.61
 ---- batch: 110 ----
mean loss: 202.35
train mean loss: 203.11
epoch train time: 0:00:15.765792
elapsed time: 0:24:38.789425
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 09:50:34.904723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.66
 ---- batch: 020 ----
mean loss: 210.54
 ---- batch: 030 ----
mean loss: 204.44
 ---- batch: 040 ----
mean loss: 207.39
 ---- batch: 050 ----
mean loss: 204.22
 ---- batch: 060 ----
mean loss: 192.63
 ---- batch: 070 ----
mean loss: 203.74
 ---- batch: 080 ----
mean loss: 188.44
 ---- batch: 090 ----
mean loss: 207.48
 ---- batch: 100 ----
mean loss: 201.32
 ---- batch: 110 ----
mean loss: 204.10
train mean loss: 202.43
epoch train time: 0:00:15.690726
elapsed time: 0:24:54.481110
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 09:50:50.596285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.14
 ---- batch: 020 ----
mean loss: 207.98
 ---- batch: 030 ----
mean loss: 203.12
 ---- batch: 040 ----
mean loss: 194.93
 ---- batch: 050 ----
mean loss: 195.47
 ---- batch: 060 ----
mean loss: 208.51
 ---- batch: 070 ----
mean loss: 209.75
 ---- batch: 080 ----
mean loss: 204.09
 ---- batch: 090 ----
mean loss: 193.29
 ---- batch: 100 ----
mean loss: 202.38
 ---- batch: 110 ----
mean loss: 194.70
train mean loss: 201.12
epoch train time: 0:00:15.687362
elapsed time: 0:25:10.169445
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 09:51:06.284612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.01
 ---- batch: 020 ----
mean loss: 202.71
 ---- batch: 030 ----
mean loss: 206.08
 ---- batch: 040 ----
mean loss: 194.18
 ---- batch: 050 ----
mean loss: 202.29
 ---- batch: 060 ----
mean loss: 195.91
 ---- batch: 070 ----
mean loss: 203.61
 ---- batch: 080 ----
mean loss: 202.37
 ---- batch: 090 ----
mean loss: 198.22
 ---- batch: 100 ----
mean loss: 199.69
 ---- batch: 110 ----
mean loss: 195.67
train mean loss: 199.72
epoch train time: 0:00:15.689771
elapsed time: 0:25:25.860219
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 09:51:21.975454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.41
 ---- batch: 020 ----
mean loss: 199.73
 ---- batch: 030 ----
mean loss: 201.19
 ---- batch: 040 ----
mean loss: 210.75
 ---- batch: 050 ----
mean loss: 206.23
 ---- batch: 060 ----
mean loss: 193.77
 ---- batch: 070 ----
mean loss: 190.61
 ---- batch: 080 ----
mean loss: 194.41
 ---- batch: 090 ----
mean loss: 201.96
 ---- batch: 100 ----
mean loss: 197.50
 ---- batch: 110 ----
mean loss: 199.59
train mean loss: 200.18
epoch train time: 0:00:15.726305
elapsed time: 0:25:41.587562
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 09:51:37.702813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.59
 ---- batch: 020 ----
mean loss: 194.56
 ---- batch: 030 ----
mean loss: 188.02
 ---- batch: 040 ----
mean loss: 204.92
 ---- batch: 050 ----
mean loss: 206.88
 ---- batch: 060 ----
mean loss: 203.83
 ---- batch: 070 ----
mean loss: 202.30
 ---- batch: 080 ----
mean loss: 197.15
 ---- batch: 090 ----
mean loss: 194.84
 ---- batch: 100 ----
mean loss: 194.32
 ---- batch: 110 ----
mean loss: 201.83
train mean loss: 198.50
epoch train time: 0:00:15.716144
elapsed time: 0:25:57.304707
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 09:51:53.419875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.17
 ---- batch: 020 ----
mean loss: 197.18
 ---- batch: 030 ----
mean loss: 197.30
 ---- batch: 040 ----
mean loss: 202.83
 ---- batch: 050 ----
mean loss: 209.48
 ---- batch: 060 ----
mean loss: 192.59
 ---- batch: 070 ----
mean loss: 199.05
 ---- batch: 080 ----
mean loss: 204.56
 ---- batch: 090 ----
mean loss: 202.08
 ---- batch: 100 ----
mean loss: 201.83
 ---- batch: 110 ----
mean loss: 195.21
train mean loss: 199.30
epoch train time: 0:00:15.699053
elapsed time: 0:26:13.004727
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 09:52:09.119915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.61
 ---- batch: 020 ----
mean loss: 201.27
 ---- batch: 030 ----
mean loss: 197.41
 ---- batch: 040 ----
mean loss: 190.79
 ---- batch: 050 ----
mean loss: 199.81
 ---- batch: 060 ----
mean loss: 201.25
 ---- batch: 070 ----
mean loss: 200.21
 ---- batch: 080 ----
mean loss: 206.30
 ---- batch: 090 ----
mean loss: 202.13
 ---- batch: 100 ----
mean loss: 203.99
 ---- batch: 110 ----
mean loss: 195.63
train mean loss: 199.56
epoch train time: 0:00:15.705050
elapsed time: 0:26:28.710800
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 09:52:24.826030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.25
 ---- batch: 020 ----
mean loss: 200.72
 ---- batch: 030 ----
mean loss: 186.83
 ---- batch: 040 ----
mean loss: 200.38
 ---- batch: 050 ----
mean loss: 193.04
 ---- batch: 060 ----
mean loss: 200.25
 ---- batch: 070 ----
mean loss: 191.57
 ---- batch: 080 ----
mean loss: 185.47
 ---- batch: 090 ----
mean loss: 199.58
 ---- batch: 100 ----
mean loss: 197.25
 ---- batch: 110 ----
mean loss: 203.45
train mean loss: 196.52
epoch train time: 0:00:15.681014
elapsed time: 0:26:44.392814
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 09:52:40.507983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.39
 ---- batch: 020 ----
mean loss: 194.67
 ---- batch: 030 ----
mean loss: 203.91
 ---- batch: 040 ----
mean loss: 195.64
 ---- batch: 050 ----
mean loss: 194.96
 ---- batch: 060 ----
mean loss: 200.46
 ---- batch: 070 ----
mean loss: 193.24
 ---- batch: 080 ----
mean loss: 196.51
 ---- batch: 090 ----
mean loss: 191.20
 ---- batch: 100 ----
mean loss: 200.09
 ---- batch: 110 ----
mean loss: 198.66
train mean loss: 195.87
epoch train time: 0:00:15.692440
elapsed time: 0:27:00.086295
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 09:52:56.201469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.23
 ---- batch: 020 ----
mean loss: 197.99
 ---- batch: 030 ----
mean loss: 200.16
 ---- batch: 040 ----
mean loss: 185.71
 ---- batch: 050 ----
mean loss: 199.23
 ---- batch: 060 ----
mean loss: 191.13
 ---- batch: 070 ----
mean loss: 194.33
 ---- batch: 080 ----
mean loss: 197.66
 ---- batch: 090 ----
mean loss: 193.75
 ---- batch: 100 ----
mean loss: 187.13
 ---- batch: 110 ----
mean loss: 197.02
train mean loss: 195.21
epoch train time: 0:00:15.693083
elapsed time: 0:27:15.780375
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 09:53:11.895565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.74
 ---- batch: 020 ----
mean loss: 198.37
 ---- batch: 030 ----
mean loss: 198.53
 ---- batch: 040 ----
mean loss: 194.66
 ---- batch: 050 ----
mean loss: 195.43
 ---- batch: 060 ----
mean loss: 201.99
 ---- batch: 070 ----
mean loss: 188.76
 ---- batch: 080 ----
mean loss: 198.61
 ---- batch: 090 ----
mean loss: 190.87
 ---- batch: 100 ----
mean loss: 193.96
 ---- batch: 110 ----
mean loss: 191.12
train mean loss: 195.55
epoch train time: 0:00:15.815351
elapsed time: 0:27:31.596589
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 09:53:27.711828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.00
 ---- batch: 020 ----
mean loss: 192.56
 ---- batch: 030 ----
mean loss: 202.17
 ---- batch: 040 ----
mean loss: 191.81
 ---- batch: 050 ----
mean loss: 188.33
 ---- batch: 060 ----
mean loss: 199.58
 ---- batch: 070 ----
mean loss: 191.85
 ---- batch: 080 ----
mean loss: 192.91
 ---- batch: 090 ----
mean loss: 188.90
 ---- batch: 100 ----
mean loss: 197.80
 ---- batch: 110 ----
mean loss: 201.48
train mean loss: 195.25
epoch train time: 0:00:15.692984
elapsed time: 0:27:47.290616
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 09:53:43.405895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.89
 ---- batch: 020 ----
mean loss: 194.83
 ---- batch: 030 ----
mean loss: 193.36
 ---- batch: 040 ----
mean loss: 191.57
 ---- batch: 050 ----
mean loss: 194.62
 ---- batch: 060 ----
mean loss: 191.08
 ---- batch: 070 ----
mean loss: 192.69
 ---- batch: 080 ----
mean loss: 189.45
 ---- batch: 090 ----
mean loss: 192.91
 ---- batch: 100 ----
mean loss: 199.97
 ---- batch: 110 ----
mean loss: 206.61
train mean loss: 193.52
epoch train time: 0:00:15.675058
elapsed time: 0:28:02.966756
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 09:53:59.082072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.05
 ---- batch: 020 ----
mean loss: 188.97
 ---- batch: 030 ----
mean loss: 198.16
 ---- batch: 040 ----
mean loss: 190.65
 ---- batch: 050 ----
mean loss: 187.44
 ---- batch: 060 ----
mean loss: 197.62
 ---- batch: 070 ----
mean loss: 193.26
 ---- batch: 080 ----
mean loss: 189.16
 ---- batch: 090 ----
mean loss: 191.33
 ---- batch: 100 ----
mean loss: 196.32
 ---- batch: 110 ----
mean loss: 191.49
train mean loss: 193.14
epoch train time: 0:00:15.666691
elapsed time: 0:28:18.634765
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 09:54:14.750014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.72
 ---- batch: 020 ----
mean loss: 205.71
 ---- batch: 030 ----
mean loss: 188.28
 ---- batch: 040 ----
mean loss: 198.50
 ---- batch: 050 ----
mean loss: 198.31
 ---- batch: 060 ----
mean loss: 194.60
 ---- batch: 070 ----
mean loss: 187.45
 ---- batch: 080 ----
mean loss: 200.21
 ---- batch: 090 ----
mean loss: 204.62
 ---- batch: 100 ----
mean loss: 188.35
 ---- batch: 110 ----
mean loss: 196.78
train mean loss: 195.21
epoch train time: 0:00:15.682728
elapsed time: 0:28:34.318597
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 09:54:30.433821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.72
 ---- batch: 020 ----
mean loss: 197.38
 ---- batch: 030 ----
mean loss: 194.83
 ---- batch: 040 ----
mean loss: 189.66
 ---- batch: 050 ----
mean loss: 200.23
 ---- batch: 060 ----
mean loss: 182.58
 ---- batch: 070 ----
mean loss: 195.60
 ---- batch: 080 ----
mean loss: 195.13
 ---- batch: 090 ----
mean loss: 190.93
 ---- batch: 100 ----
mean loss: 177.56
 ---- batch: 110 ----
mean loss: 192.41
train mean loss: 191.90
epoch train time: 0:00:15.775488
elapsed time: 0:28:50.095179
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 09:54:46.210394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.31
 ---- batch: 020 ----
mean loss: 199.01
 ---- batch: 030 ----
mean loss: 182.63
 ---- batch: 040 ----
mean loss: 189.31
 ---- batch: 050 ----
mean loss: 196.11
 ---- batch: 060 ----
mean loss: 194.57
 ---- batch: 070 ----
mean loss: 192.49
 ---- batch: 080 ----
mean loss: 193.90
 ---- batch: 090 ----
mean loss: 197.12
 ---- batch: 100 ----
mean loss: 196.28
 ---- batch: 110 ----
mean loss: 192.42
train mean loss: 192.72
epoch train time: 0:00:15.749598
elapsed time: 0:29:05.845950
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 09:55:01.961188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.95
 ---- batch: 020 ----
mean loss: 185.61
 ---- batch: 030 ----
mean loss: 195.17
 ---- batch: 040 ----
mean loss: 188.98
 ---- batch: 050 ----
mean loss: 189.69
 ---- batch: 060 ----
mean loss: 191.05
 ---- batch: 070 ----
mean loss: 199.46
 ---- batch: 080 ----
mean loss: 196.48
 ---- batch: 090 ----
mean loss: 182.00
 ---- batch: 100 ----
mean loss: 191.14
 ---- batch: 110 ----
mean loss: 195.13
train mean loss: 191.18
epoch train time: 0:00:15.702390
elapsed time: 0:29:21.549410
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 09:55:17.664601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.79
 ---- batch: 020 ----
mean loss: 191.58
 ---- batch: 030 ----
mean loss: 189.49
 ---- batch: 040 ----
mean loss: 179.50
 ---- batch: 050 ----
mean loss: 196.41
 ---- batch: 060 ----
mean loss: 188.95
 ---- batch: 070 ----
mean loss: 191.87
 ---- batch: 080 ----
mean loss: 199.26
 ---- batch: 090 ----
mean loss: 187.72
 ---- batch: 100 ----
mean loss: 186.70
 ---- batch: 110 ----
mean loss: 197.32
train mean loss: 191.17
epoch train time: 0:00:15.738467
elapsed time: 0:29:37.288859
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 09:55:33.404032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.33
 ---- batch: 020 ----
mean loss: 183.79
 ---- batch: 030 ----
mean loss: 190.93
 ---- batch: 040 ----
mean loss: 192.09
 ---- batch: 050 ----
mean loss: 185.79
 ---- batch: 060 ----
mean loss: 195.64
 ---- batch: 070 ----
mean loss: 196.14
 ---- batch: 080 ----
mean loss: 194.76
 ---- batch: 090 ----
mean loss: 196.47
 ---- batch: 100 ----
mean loss: 183.48
 ---- batch: 110 ----
mean loss: 190.89
train mean loss: 190.94
epoch train time: 0:00:15.719195
elapsed time: 0:29:53.009218
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 09:55:49.124527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.47
 ---- batch: 020 ----
mean loss: 182.71
 ---- batch: 030 ----
mean loss: 190.69
 ---- batch: 040 ----
mean loss: 182.90
 ---- batch: 050 ----
mean loss: 191.04
 ---- batch: 060 ----
mean loss: 193.89
 ---- batch: 070 ----
mean loss: 182.95
 ---- batch: 080 ----
mean loss: 194.63
 ---- batch: 090 ----
mean loss: 193.55
 ---- batch: 100 ----
mean loss: 189.85
 ---- batch: 110 ----
mean loss: 199.11
train mean loss: 190.31
epoch train time: 0:00:15.732854
elapsed time: 0:30:08.743256
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 09:56:04.858477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.28
 ---- batch: 020 ----
mean loss: 200.78
 ---- batch: 030 ----
mean loss: 182.74
 ---- batch: 040 ----
mean loss: 192.47
 ---- batch: 050 ----
mean loss: 197.28
 ---- batch: 060 ----
mean loss: 202.16
 ---- batch: 070 ----
mean loss: 191.02
 ---- batch: 080 ----
mean loss: 183.16
 ---- batch: 090 ----
mean loss: 185.36
 ---- batch: 100 ----
mean loss: 185.60
 ---- batch: 110 ----
mean loss: 188.02
train mean loss: 190.40
epoch train time: 0:00:15.700505
elapsed time: 0:30:24.444731
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 09:56:20.559938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.93
 ---- batch: 020 ----
mean loss: 197.04
 ---- batch: 030 ----
mean loss: 187.31
 ---- batch: 040 ----
mean loss: 186.87
 ---- batch: 050 ----
mean loss: 191.26
 ---- batch: 060 ----
mean loss: 186.07
 ---- batch: 070 ----
mean loss: 185.93
 ---- batch: 080 ----
mean loss: 200.24
 ---- batch: 090 ----
mean loss: 193.54
 ---- batch: 100 ----
mean loss: 177.23
 ---- batch: 110 ----
mean loss: 182.89
train mean loss: 189.61
epoch train time: 0:00:15.760044
elapsed time: 0:30:40.205761
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 09:56:36.320936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.55
 ---- batch: 020 ----
mean loss: 188.13
 ---- batch: 030 ----
mean loss: 186.75
 ---- batch: 040 ----
mean loss: 185.41
 ---- batch: 050 ----
mean loss: 184.80
 ---- batch: 060 ----
mean loss: 189.58
 ---- batch: 070 ----
mean loss: 203.92
 ---- batch: 080 ----
mean loss: 198.59
 ---- batch: 090 ----
mean loss: 184.81
 ---- batch: 100 ----
mean loss: 195.34
 ---- batch: 110 ----
mean loss: 185.89
train mean loss: 189.91
epoch train time: 0:00:15.663754
elapsed time: 0:30:55.870438
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 09:56:51.985626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.80
 ---- batch: 020 ----
mean loss: 191.24
 ---- batch: 030 ----
mean loss: 186.18
 ---- batch: 040 ----
mean loss: 184.40
 ---- batch: 050 ----
mean loss: 186.31
 ---- batch: 060 ----
mean loss: 192.68
 ---- batch: 070 ----
mean loss: 194.35
 ---- batch: 080 ----
mean loss: 198.80
 ---- batch: 090 ----
mean loss: 183.60
 ---- batch: 100 ----
mean loss: 189.90
 ---- batch: 110 ----
mean loss: 189.02
train mean loss: 190.05
epoch train time: 0:00:15.659305
elapsed time: 0:31:11.530706
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 09:57:07.645898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.20
 ---- batch: 020 ----
mean loss: 187.22
 ---- batch: 030 ----
mean loss: 186.27
 ---- batch: 040 ----
mean loss: 184.46
 ---- batch: 050 ----
mean loss: 187.15
 ---- batch: 060 ----
mean loss: 185.26
 ---- batch: 070 ----
mean loss: 187.12
 ---- batch: 080 ----
mean loss: 193.95
 ---- batch: 090 ----
mean loss: 196.30
 ---- batch: 100 ----
mean loss: 193.48
 ---- batch: 110 ----
mean loss: 186.01
train mean loss: 188.34
epoch train time: 0:00:15.682351
elapsed time: 0:31:27.214027
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 09:57:23.329269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.64
 ---- batch: 020 ----
mean loss: 188.29
 ---- batch: 030 ----
mean loss: 185.30
 ---- batch: 040 ----
mean loss: 187.58
 ---- batch: 050 ----
mean loss: 191.25
 ---- batch: 060 ----
mean loss: 195.75
 ---- batch: 070 ----
mean loss: 185.75
 ---- batch: 080 ----
mean loss: 187.21
 ---- batch: 090 ----
mean loss: 192.52
 ---- batch: 100 ----
mean loss: 187.49
 ---- batch: 110 ----
mean loss: 187.19
train mean loss: 188.13
epoch train time: 0:00:15.724484
elapsed time: 0:31:42.939575
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 09:57:39.054763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.64
 ---- batch: 020 ----
mean loss: 177.41
 ---- batch: 030 ----
mean loss: 191.16
 ---- batch: 040 ----
mean loss: 187.39
 ---- batch: 050 ----
mean loss: 184.90
 ---- batch: 060 ----
mean loss: 179.83
 ---- batch: 070 ----
mean loss: 195.45
 ---- batch: 080 ----
mean loss: 178.85
 ---- batch: 090 ----
mean loss: 194.06
 ---- batch: 100 ----
mean loss: 190.29
 ---- batch: 110 ----
mean loss: 191.97
train mean loss: 187.14
epoch train time: 0:00:15.704753
elapsed time: 0:31:58.645245
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 09:57:54.760460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.17
 ---- batch: 020 ----
mean loss: 185.80
 ---- batch: 030 ----
mean loss: 184.47
 ---- batch: 040 ----
mean loss: 187.63
 ---- batch: 050 ----
mean loss: 176.77
 ---- batch: 060 ----
mean loss: 184.36
 ---- batch: 070 ----
mean loss: 190.46
 ---- batch: 080 ----
mean loss: 188.47
 ---- batch: 090 ----
mean loss: 185.35
 ---- batch: 100 ----
mean loss: 189.73
 ---- batch: 110 ----
mean loss: 188.45
train mean loss: 186.09
epoch train time: 0:00:15.680832
elapsed time: 0:32:14.326922
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 09:58:10.442156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.98
 ---- batch: 020 ----
mean loss: 196.70
 ---- batch: 030 ----
mean loss: 182.81
 ---- batch: 040 ----
mean loss: 190.98
 ---- batch: 050 ----
mean loss: 190.59
 ---- batch: 060 ----
mean loss: 186.12
 ---- batch: 070 ----
mean loss: 190.53
 ---- batch: 080 ----
mean loss: 189.23
 ---- batch: 090 ----
mean loss: 178.64
 ---- batch: 100 ----
mean loss: 190.77
 ---- batch: 110 ----
mean loss: 180.55
train mean loss: 187.95
epoch train time: 0:00:15.700893
elapsed time: 0:32:30.028814
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 09:58:26.144005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.38
 ---- batch: 020 ----
mean loss: 182.42
 ---- batch: 030 ----
mean loss: 183.18
 ---- batch: 040 ----
mean loss: 188.44
 ---- batch: 050 ----
mean loss: 184.59
 ---- batch: 060 ----
mean loss: 184.72
 ---- batch: 070 ----
mean loss: 184.75
 ---- batch: 080 ----
mean loss: 189.21
 ---- batch: 090 ----
mean loss: 194.46
 ---- batch: 100 ----
mean loss: 189.01
 ---- batch: 110 ----
mean loss: 178.28
train mean loss: 186.48
epoch train time: 0:00:15.827624
elapsed time: 0:32:45.857313
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 09:58:41.972485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.86
 ---- batch: 020 ----
mean loss: 176.13
 ---- batch: 030 ----
mean loss: 185.32
 ---- batch: 040 ----
mean loss: 189.72
 ---- batch: 050 ----
mean loss: 192.70
 ---- batch: 060 ----
mean loss: 188.28
 ---- batch: 070 ----
mean loss: 192.14
 ---- batch: 080 ----
mean loss: 180.27
 ---- batch: 090 ----
mean loss: 185.97
 ---- batch: 100 ----
mean loss: 181.28
 ---- batch: 110 ----
mean loss: 183.81
train mean loss: 185.71
epoch train time: 0:00:15.707925
elapsed time: 0:33:01.566222
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 09:58:57.681394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.18
 ---- batch: 020 ----
mean loss: 177.81
 ---- batch: 030 ----
mean loss: 186.12
 ---- batch: 040 ----
mean loss: 193.70
 ---- batch: 050 ----
mean loss: 179.53
 ---- batch: 060 ----
mean loss: 187.97
 ---- batch: 070 ----
mean loss: 193.70
 ---- batch: 080 ----
mean loss: 192.53
 ---- batch: 090 ----
mean loss: 179.54
 ---- batch: 100 ----
mean loss: 181.53
 ---- batch: 110 ----
mean loss: 186.83
train mean loss: 185.88
epoch train time: 0:00:15.667404
elapsed time: 0:33:17.234597
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 09:59:13.349975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.02
 ---- batch: 020 ----
mean loss: 182.72
 ---- batch: 030 ----
mean loss: 185.21
 ---- batch: 040 ----
mean loss: 188.55
 ---- batch: 050 ----
mean loss: 184.79
 ---- batch: 060 ----
mean loss: 189.94
 ---- batch: 070 ----
mean loss: 185.14
 ---- batch: 080 ----
mean loss: 194.35
 ---- batch: 090 ----
mean loss: 188.08
 ---- batch: 100 ----
mean loss: 179.60
 ---- batch: 110 ----
mean loss: 183.92
train mean loss: 186.24
epoch train time: 0:00:15.684710
elapsed time: 0:33:32.920707
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 09:59:29.035899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.83
 ---- batch: 020 ----
mean loss: 188.31
 ---- batch: 030 ----
mean loss: 187.57
 ---- batch: 040 ----
mean loss: 170.58
 ---- batch: 050 ----
mean loss: 198.42
 ---- batch: 060 ----
mean loss: 184.07
 ---- batch: 070 ----
mean loss: 186.43
 ---- batch: 080 ----
mean loss: 183.16
 ---- batch: 090 ----
mean loss: 190.00
 ---- batch: 100 ----
mean loss: 179.16
 ---- batch: 110 ----
mean loss: 185.74
train mean loss: 185.66
epoch train time: 0:00:15.700281
elapsed time: 0:33:48.621943
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 09:59:44.737129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.74
 ---- batch: 020 ----
mean loss: 177.72
 ---- batch: 030 ----
mean loss: 184.03
 ---- batch: 040 ----
mean loss: 184.25
 ---- batch: 050 ----
mean loss: 189.88
 ---- batch: 060 ----
mean loss: 188.00
 ---- batch: 070 ----
mean loss: 185.87
 ---- batch: 080 ----
mean loss: 187.26
 ---- batch: 090 ----
mean loss: 177.42
 ---- batch: 100 ----
mean loss: 194.16
 ---- batch: 110 ----
mean loss: 179.06
train mean loss: 185.41
epoch train time: 0:00:15.676132
elapsed time: 0:34:04.299059
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 10:00:00.414233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.93
 ---- batch: 020 ----
mean loss: 196.00
 ---- batch: 030 ----
mean loss: 189.37
 ---- batch: 040 ----
mean loss: 188.62
 ---- batch: 050 ----
mean loss: 192.32
 ---- batch: 060 ----
mean loss: 189.45
 ---- batch: 070 ----
mean loss: 180.85
 ---- batch: 080 ----
mean loss: 181.79
 ---- batch: 090 ----
mean loss: 179.20
 ---- batch: 100 ----
mean loss: 181.25
 ---- batch: 110 ----
mean loss: 186.56
train mean loss: 186.16
epoch train time: 0:00:15.633953
elapsed time: 0:34:19.933992
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 10:00:16.049229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.86
 ---- batch: 020 ----
mean loss: 183.40
 ---- batch: 030 ----
mean loss: 184.32
 ---- batch: 040 ----
mean loss: 190.19
 ---- batch: 050 ----
mean loss: 178.38
 ---- batch: 060 ----
mean loss: 182.42
 ---- batch: 070 ----
mean loss: 190.49
 ---- batch: 080 ----
mean loss: 187.31
 ---- batch: 090 ----
mean loss: 187.26
 ---- batch: 100 ----
mean loss: 183.66
 ---- batch: 110 ----
mean loss: 179.11
train mean loss: 184.07
epoch train time: 0:00:15.690921
elapsed time: 0:34:35.625928
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 10:00:31.741105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.82
 ---- batch: 020 ----
mean loss: 185.45
 ---- batch: 030 ----
mean loss: 185.96
 ---- batch: 040 ----
mean loss: 188.98
 ---- batch: 050 ----
mean loss: 184.41
 ---- batch: 060 ----
mean loss: 180.75
 ---- batch: 070 ----
mean loss: 183.40
 ---- batch: 080 ----
mean loss: 179.31
 ---- batch: 090 ----
mean loss: 177.77
 ---- batch: 100 ----
mean loss: 192.03
 ---- batch: 110 ----
mean loss: 186.07
train mean loss: 184.25
epoch train time: 0:00:15.660796
elapsed time: 0:34:51.287665
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 10:00:47.402881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.27
 ---- batch: 020 ----
mean loss: 182.94
 ---- batch: 030 ----
mean loss: 185.26
 ---- batch: 040 ----
mean loss: 186.71
 ---- batch: 050 ----
mean loss: 183.12
 ---- batch: 060 ----
mean loss: 186.27
 ---- batch: 070 ----
mean loss: 191.95
 ---- batch: 080 ----
mean loss: 176.35
 ---- batch: 090 ----
mean loss: 178.22
 ---- batch: 100 ----
mean loss: 182.38
 ---- batch: 110 ----
mean loss: 177.92
train mean loss: 183.38
epoch train time: 0:00:15.631142
elapsed time: 0:35:06.919700
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 10:01:03.034904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.78
 ---- batch: 020 ----
mean loss: 187.22
 ---- batch: 030 ----
mean loss: 190.04
 ---- batch: 040 ----
mean loss: 186.52
 ---- batch: 050 ----
mean loss: 191.56
 ---- batch: 060 ----
mean loss: 178.91
 ---- batch: 070 ----
mean loss: 178.01
 ---- batch: 080 ----
mean loss: 179.79
 ---- batch: 090 ----
mean loss: 188.92
 ---- batch: 100 ----
mean loss: 175.82
 ---- batch: 110 ----
mean loss: 188.83
train mean loss: 186.19
epoch train time: 0:00:15.709432
elapsed time: 0:35:22.630220
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 10:01:18.745403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.04
 ---- batch: 020 ----
mean loss: 193.85
 ---- batch: 030 ----
mean loss: 194.22
 ---- batch: 040 ----
mean loss: 183.80
 ---- batch: 050 ----
mean loss: 184.45
 ---- batch: 060 ----
mean loss: 184.41
 ---- batch: 070 ----
mean loss: 182.15
 ---- batch: 080 ----
mean loss: 178.52
 ---- batch: 090 ----
mean loss: 181.62
 ---- batch: 100 ----
mean loss: 187.33
 ---- batch: 110 ----
mean loss: 181.34
train mean loss: 184.53
epoch train time: 0:00:15.719036
elapsed time: 0:35:38.350266
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 10:01:34.465533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.75
 ---- batch: 020 ----
mean loss: 193.56
 ---- batch: 030 ----
mean loss: 184.05
 ---- batch: 040 ----
mean loss: 185.21
 ---- batch: 050 ----
mean loss: 176.58
 ---- batch: 060 ----
mean loss: 187.81
 ---- batch: 070 ----
mean loss: 181.21
 ---- batch: 080 ----
mean loss: 179.21
 ---- batch: 090 ----
mean loss: 184.68
 ---- batch: 100 ----
mean loss: 177.06
 ---- batch: 110 ----
mean loss: 183.59
train mean loss: 183.07
epoch train time: 0:00:15.696537
elapsed time: 0:35:54.047822
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 10:01:50.163049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.19
 ---- batch: 020 ----
mean loss: 185.06
 ---- batch: 030 ----
mean loss: 185.67
 ---- batch: 040 ----
mean loss: 178.85
 ---- batch: 050 ----
mean loss: 176.50
 ---- batch: 060 ----
mean loss: 178.88
 ---- batch: 070 ----
mean loss: 187.42
 ---- batch: 080 ----
mean loss: 186.83
 ---- batch: 090 ----
mean loss: 185.84
 ---- batch: 100 ----
mean loss: 177.06
 ---- batch: 110 ----
mean loss: 186.32
train mean loss: 183.88
epoch train time: 0:00:15.694917
elapsed time: 0:36:09.743821
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 10:02:05.858999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.38
 ---- batch: 020 ----
mean loss: 185.94
 ---- batch: 030 ----
mean loss: 183.46
 ---- batch: 040 ----
mean loss: 184.03
 ---- batch: 050 ----
mean loss: 182.31
 ---- batch: 060 ----
mean loss: 186.43
 ---- batch: 070 ----
mean loss: 180.39
 ---- batch: 080 ----
mean loss: 182.78
 ---- batch: 090 ----
mean loss: 169.76
 ---- batch: 100 ----
mean loss: 189.20
 ---- batch: 110 ----
mean loss: 189.78
train mean loss: 182.49
epoch train time: 0:00:15.629023
elapsed time: 0:36:25.373700
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 10:02:21.488939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.07
 ---- batch: 020 ----
mean loss: 193.40
 ---- batch: 030 ----
mean loss: 173.80
 ---- batch: 040 ----
mean loss: 193.71
 ---- batch: 050 ----
mean loss: 175.23
 ---- batch: 060 ----
mean loss: 187.71
 ---- batch: 070 ----
mean loss: 174.54
 ---- batch: 080 ----
mean loss: 184.28
 ---- batch: 090 ----
mean loss: 173.89
 ---- batch: 100 ----
mean loss: 186.82
 ---- batch: 110 ----
mean loss: 180.73
train mean loss: 183.45
epoch train time: 0:00:15.656255
elapsed time: 0:36:41.031087
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 10:02:37.146258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.27
 ---- batch: 020 ----
mean loss: 183.35
 ---- batch: 030 ----
mean loss: 184.26
 ---- batch: 040 ----
mean loss: 188.64
 ---- batch: 050 ----
mean loss: 187.73
 ---- batch: 060 ----
mean loss: 192.20
 ---- batch: 070 ----
mean loss: 176.02
 ---- batch: 080 ----
mean loss: 173.61
 ---- batch: 090 ----
mean loss: 181.40
 ---- batch: 100 ----
mean loss: 170.23
 ---- batch: 110 ----
mean loss: 184.97
train mean loss: 181.92
epoch train time: 0:00:15.651684
elapsed time: 0:36:56.683764
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 10:02:52.798958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.20
 ---- batch: 020 ----
mean loss: 182.23
 ---- batch: 030 ----
mean loss: 187.02
 ---- batch: 040 ----
mean loss: 190.39
 ---- batch: 050 ----
mean loss: 187.22
 ---- batch: 060 ----
mean loss: 174.24
 ---- batch: 070 ----
mean loss: 183.00
 ---- batch: 080 ----
mean loss: 183.17
 ---- batch: 090 ----
mean loss: 186.97
 ---- batch: 100 ----
mean loss: 185.88
 ---- batch: 110 ----
mean loss: 180.20
train mean loss: 183.41
epoch train time: 0:00:15.707044
elapsed time: 0:37:12.391782
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 10:03:08.506970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.28
 ---- batch: 020 ----
mean loss: 185.12
 ---- batch: 030 ----
mean loss: 177.43
 ---- batch: 040 ----
mean loss: 183.50
 ---- batch: 050 ----
mean loss: 181.29
 ---- batch: 060 ----
mean loss: 182.04
 ---- batch: 070 ----
mean loss: 182.18
 ---- batch: 080 ----
mean loss: 181.92
 ---- batch: 090 ----
mean loss: 184.36
 ---- batch: 100 ----
mean loss: 179.73
 ---- batch: 110 ----
mean loss: 177.14
train mean loss: 182.37
epoch train time: 0:00:15.747128
elapsed time: 0:37:28.139902
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 10:03:24.255099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.60
 ---- batch: 020 ----
mean loss: 192.77
 ---- batch: 030 ----
mean loss: 179.33
 ---- batch: 040 ----
mean loss: 185.39
 ---- batch: 050 ----
mean loss: 185.77
 ---- batch: 060 ----
mean loss: 188.59
 ---- batch: 070 ----
mean loss: 179.03
 ---- batch: 080 ----
mean loss: 177.72
 ---- batch: 090 ----
mean loss: 170.41
 ---- batch: 100 ----
mean loss: 188.38
 ---- batch: 110 ----
mean loss: 193.19
train mean loss: 183.62
epoch train time: 0:00:15.797627
elapsed time: 0:37:43.938553
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 10:03:40.053785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.03
 ---- batch: 020 ----
mean loss: 177.54
 ---- batch: 030 ----
mean loss: 176.18
 ---- batch: 040 ----
mean loss: 180.94
 ---- batch: 050 ----
mean loss: 186.47
 ---- batch: 060 ----
mean loss: 175.70
 ---- batch: 070 ----
mean loss: 185.28
 ---- batch: 080 ----
mean loss: 187.07
 ---- batch: 090 ----
mean loss: 189.70
 ---- batch: 100 ----
mean loss: 182.63
 ---- batch: 110 ----
mean loss: 171.63
train mean loss: 182.04
epoch train time: 0:00:15.819268
elapsed time: 0:37:59.758831
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 10:03:55.874082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.85
 ---- batch: 020 ----
mean loss: 179.32
 ---- batch: 030 ----
mean loss: 181.08
 ---- batch: 040 ----
mean loss: 183.40
 ---- batch: 050 ----
mean loss: 181.38
 ---- batch: 060 ----
mean loss: 177.84
 ---- batch: 070 ----
mean loss: 178.00
 ---- batch: 080 ----
mean loss: 184.86
 ---- batch: 090 ----
mean loss: 180.52
 ---- batch: 100 ----
mean loss: 183.32
 ---- batch: 110 ----
mean loss: 180.24
train mean loss: 181.03
epoch train time: 0:00:15.745153
elapsed time: 0:38:15.505057
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 10:04:11.620255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.42
 ---- batch: 020 ----
mean loss: 186.95
 ---- batch: 030 ----
mean loss: 187.45
 ---- batch: 040 ----
mean loss: 177.74
 ---- batch: 050 ----
mean loss: 185.50
 ---- batch: 060 ----
mean loss: 181.50
 ---- batch: 070 ----
mean loss: 182.24
 ---- batch: 080 ----
mean loss: 178.99
 ---- batch: 090 ----
mean loss: 177.37
 ---- batch: 100 ----
mean loss: 179.49
 ---- batch: 110 ----
mean loss: 180.37
train mean loss: 180.98
epoch train time: 0:00:15.774162
elapsed time: 0:38:31.280190
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 10:04:27.395363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.02
 ---- batch: 020 ----
mean loss: 187.20
 ---- batch: 030 ----
mean loss: 177.39
 ---- batch: 040 ----
mean loss: 180.85
 ---- batch: 050 ----
mean loss: 176.27
 ---- batch: 060 ----
mean loss: 179.88
 ---- batch: 070 ----
mean loss: 184.48
 ---- batch: 080 ----
mean loss: 180.68
 ---- batch: 090 ----
mean loss: 181.32
 ---- batch: 100 ----
mean loss: 176.77
 ---- batch: 110 ----
mean loss: 183.93
train mean loss: 181.73
epoch train time: 0:00:15.774698
elapsed time: 0:38:47.055818
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 10:04:43.171056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.12
 ---- batch: 020 ----
mean loss: 184.94
 ---- batch: 030 ----
mean loss: 178.02
 ---- batch: 040 ----
mean loss: 175.98
 ---- batch: 050 ----
mean loss: 186.70
 ---- batch: 060 ----
mean loss: 178.29
 ---- batch: 070 ----
mean loss: 180.42
 ---- batch: 080 ----
mean loss: 184.70
 ---- batch: 090 ----
mean loss: 184.49
 ---- batch: 100 ----
mean loss: 189.85
 ---- batch: 110 ----
mean loss: 173.46
train mean loss: 181.22
epoch train time: 0:00:15.740716
elapsed time: 0:39:02.797665
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 10:04:58.913222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.45
 ---- batch: 020 ----
mean loss: 184.20
 ---- batch: 030 ----
mean loss: 184.02
 ---- batch: 040 ----
mean loss: 180.29
 ---- batch: 050 ----
mean loss: 178.69
 ---- batch: 060 ----
mean loss: 176.88
 ---- batch: 070 ----
mean loss: 180.10
 ---- batch: 080 ----
mean loss: 186.91
 ---- batch: 090 ----
mean loss: 180.26
 ---- batch: 100 ----
mean loss: 185.61
 ---- batch: 110 ----
mean loss: 176.53
train mean loss: 180.92
epoch train time: 0:00:15.768425
elapsed time: 0:39:18.567683
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 10:05:14.682850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.21
 ---- batch: 020 ----
mean loss: 176.56
 ---- batch: 030 ----
mean loss: 176.21
 ---- batch: 040 ----
mean loss: 172.01
 ---- batch: 050 ----
mean loss: 175.61
 ---- batch: 060 ----
mean loss: 183.41
 ---- batch: 070 ----
mean loss: 184.38
 ---- batch: 080 ----
mean loss: 183.83
 ---- batch: 090 ----
mean loss: 180.90
 ---- batch: 100 ----
mean loss: 185.03
 ---- batch: 110 ----
mean loss: 176.49
train mean loss: 180.33
epoch train time: 0:00:15.764950
elapsed time: 0:39:34.333642
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 10:05:30.448808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.88
 ---- batch: 020 ----
mean loss: 181.31
 ---- batch: 030 ----
mean loss: 184.44
 ---- batch: 040 ----
mean loss: 175.13
 ---- batch: 050 ----
mean loss: 179.93
 ---- batch: 060 ----
mean loss: 189.04
 ---- batch: 070 ----
mean loss: 176.99
 ---- batch: 080 ----
mean loss: 186.59
 ---- batch: 090 ----
mean loss: 182.56
 ---- batch: 100 ----
mean loss: 173.96
 ---- batch: 110 ----
mean loss: 181.12
train mean loss: 180.96
epoch train time: 0:00:15.765735
elapsed time: 0:39:50.100339
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 10:05:46.215520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.08
 ---- batch: 020 ----
mean loss: 177.35
 ---- batch: 030 ----
mean loss: 188.59
 ---- batch: 040 ----
mean loss: 186.19
 ---- batch: 050 ----
mean loss: 175.99
 ---- batch: 060 ----
mean loss: 180.76
 ---- batch: 070 ----
mean loss: 181.29
 ---- batch: 080 ----
mean loss: 181.89
 ---- batch: 090 ----
mean loss: 178.27
 ---- batch: 100 ----
mean loss: 181.30
 ---- batch: 110 ----
mean loss: 173.10
train mean loss: 180.91
epoch train time: 0:00:15.798538
elapsed time: 0:40:05.899848
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 10:06:02.015049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.89
 ---- batch: 020 ----
mean loss: 178.88
 ---- batch: 030 ----
mean loss: 178.25
 ---- batch: 040 ----
mean loss: 177.03
 ---- batch: 050 ----
mean loss: 185.11
 ---- batch: 060 ----
mean loss: 181.31
 ---- batch: 070 ----
mean loss: 173.58
 ---- batch: 080 ----
mean loss: 183.59
 ---- batch: 090 ----
mean loss: 182.52
 ---- batch: 100 ----
mean loss: 179.20
 ---- batch: 110 ----
mean loss: 172.55
train mean loss: 180.23
epoch train time: 0:00:15.761807
elapsed time: 0:40:21.662669
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 10:06:17.777969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.98
 ---- batch: 020 ----
mean loss: 177.70
 ---- batch: 030 ----
mean loss: 180.67
 ---- batch: 040 ----
mean loss: 178.77
 ---- batch: 050 ----
mean loss: 181.31
 ---- batch: 060 ----
mean loss: 184.58
 ---- batch: 070 ----
mean loss: 186.00
 ---- batch: 080 ----
mean loss: 175.90
 ---- batch: 090 ----
mean loss: 171.42
 ---- batch: 100 ----
mean loss: 178.27
 ---- batch: 110 ----
mean loss: 185.54
train mean loss: 180.77
epoch train time: 0:00:15.807670
elapsed time: 0:40:37.471444
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 10:06:33.586694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.39
 ---- batch: 020 ----
mean loss: 176.67
 ---- batch: 030 ----
mean loss: 180.19
 ---- batch: 040 ----
mean loss: 178.99
 ---- batch: 050 ----
mean loss: 184.66
 ---- batch: 060 ----
mean loss: 179.04
 ---- batch: 070 ----
mean loss: 179.58
 ---- batch: 080 ----
mean loss: 171.29
 ---- batch: 090 ----
mean loss: 180.13
 ---- batch: 100 ----
mean loss: 176.30
 ---- batch: 110 ----
mean loss: 185.12
train mean loss: 179.16
epoch train time: 0:00:15.800555
elapsed time: 0:40:53.273075
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 10:06:49.388295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.64
 ---- batch: 020 ----
mean loss: 173.35
 ---- batch: 030 ----
mean loss: 173.94
 ---- batch: 040 ----
mean loss: 174.77
 ---- batch: 050 ----
mean loss: 174.17
 ---- batch: 060 ----
mean loss: 178.37
 ---- batch: 070 ----
mean loss: 187.54
 ---- batch: 080 ----
mean loss: 175.89
 ---- batch: 090 ----
mean loss: 182.16
 ---- batch: 100 ----
mean loss: 176.37
 ---- batch: 110 ----
mean loss: 189.43
train mean loss: 179.13
epoch train time: 0:00:15.793760
elapsed time: 0:41:09.067794
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 10:07:05.182969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.37
 ---- batch: 020 ----
mean loss: 174.42
 ---- batch: 030 ----
mean loss: 191.14
 ---- batch: 040 ----
mean loss: 187.72
 ---- batch: 050 ----
mean loss: 174.25
 ---- batch: 060 ----
mean loss: 174.95
 ---- batch: 070 ----
mean loss: 174.34
 ---- batch: 080 ----
mean loss: 176.40
 ---- batch: 090 ----
mean loss: 176.29
 ---- batch: 100 ----
mean loss: 180.48
 ---- batch: 110 ----
mean loss: 177.67
train mean loss: 179.22
epoch train time: 0:00:15.837247
elapsed time: 0:41:24.905987
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 10:07:21.021304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.98
 ---- batch: 020 ----
mean loss: 175.77
 ---- batch: 030 ----
mean loss: 181.28
 ---- batch: 040 ----
mean loss: 179.86
 ---- batch: 050 ----
mean loss: 170.40
 ---- batch: 060 ----
mean loss: 186.04
 ---- batch: 070 ----
mean loss: 169.34
 ---- batch: 080 ----
mean loss: 174.79
 ---- batch: 090 ----
mean loss: 186.95
 ---- batch: 100 ----
mean loss: 186.72
 ---- batch: 110 ----
mean loss: 183.97
train mean loss: 179.14
epoch train time: 0:00:15.860608
elapsed time: 0:41:40.767777
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 10:07:36.882970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.64
 ---- batch: 020 ----
mean loss: 175.44
 ---- batch: 030 ----
mean loss: 180.60
 ---- batch: 040 ----
mean loss: 173.64
 ---- batch: 050 ----
mean loss: 177.57
 ---- batch: 060 ----
mean loss: 176.56
 ---- batch: 070 ----
mean loss: 186.71
 ---- batch: 080 ----
mean loss: 178.90
 ---- batch: 090 ----
mean loss: 169.86
 ---- batch: 100 ----
mean loss: 183.13
 ---- batch: 110 ----
mean loss: 185.43
train mean loss: 179.30
epoch train time: 0:00:15.864590
elapsed time: 0:41:56.633347
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 10:07:52.748538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.15
 ---- batch: 020 ----
mean loss: 176.33
 ---- batch: 030 ----
mean loss: 176.68
 ---- batch: 040 ----
mean loss: 187.13
 ---- batch: 050 ----
mean loss: 178.03
 ---- batch: 060 ----
mean loss: 172.59
 ---- batch: 070 ----
mean loss: 181.49
 ---- batch: 080 ----
mean loss: 176.75
 ---- batch: 090 ----
mean loss: 172.32
 ---- batch: 100 ----
mean loss: 184.56
 ---- batch: 110 ----
mean loss: 175.70
train mean loss: 178.76
epoch train time: 0:00:15.803024
elapsed time: 0:42:12.437378
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 10:08:08.552552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.02
 ---- batch: 020 ----
mean loss: 178.19
 ---- batch: 030 ----
mean loss: 170.53
 ---- batch: 040 ----
mean loss: 185.24
 ---- batch: 050 ----
mean loss: 195.04
 ---- batch: 060 ----
mean loss: 173.63
 ---- batch: 070 ----
mean loss: 172.52
 ---- batch: 080 ----
mean loss: 183.12
 ---- batch: 090 ----
mean loss: 181.92
 ---- batch: 100 ----
mean loss: 166.93
 ---- batch: 110 ----
mean loss: 171.56
train mean loss: 178.93
epoch train time: 0:00:15.804193
elapsed time: 0:42:28.242495
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 10:08:24.357781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.61
 ---- batch: 020 ----
mean loss: 183.96
 ---- batch: 030 ----
mean loss: 179.04
 ---- batch: 040 ----
mean loss: 182.25
 ---- batch: 050 ----
mean loss: 188.75
 ---- batch: 060 ----
mean loss: 178.97
 ---- batch: 070 ----
mean loss: 178.71
 ---- batch: 080 ----
mean loss: 173.57
 ---- batch: 090 ----
mean loss: 178.25
 ---- batch: 100 ----
mean loss: 189.49
 ---- batch: 110 ----
mean loss: 175.03
train mean loss: 179.70
epoch train time: 0:00:15.780645
elapsed time: 0:42:44.024196
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 10:08:40.139385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.92
 ---- batch: 020 ----
mean loss: 182.08
 ---- batch: 030 ----
mean loss: 181.82
 ---- batch: 040 ----
mean loss: 172.59
 ---- batch: 050 ----
mean loss: 189.08
 ---- batch: 060 ----
mean loss: 179.63
 ---- batch: 070 ----
mean loss: 175.74
 ---- batch: 080 ----
mean loss: 163.30
 ---- batch: 090 ----
mean loss: 174.64
 ---- batch: 100 ----
mean loss: 184.12
 ---- batch: 110 ----
mean loss: 181.22
train mean loss: 178.84
epoch train time: 0:00:15.818261
elapsed time: 0:42:59.843474
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 10:08:55.958678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.89
 ---- batch: 020 ----
mean loss: 184.63
 ---- batch: 030 ----
mean loss: 187.28
 ---- batch: 040 ----
mean loss: 184.29
 ---- batch: 050 ----
mean loss: 180.24
 ---- batch: 060 ----
mean loss: 176.11
 ---- batch: 070 ----
mean loss: 171.96
 ---- batch: 080 ----
mean loss: 180.01
 ---- batch: 090 ----
mean loss: 170.87
 ---- batch: 100 ----
mean loss: 170.35
 ---- batch: 110 ----
mean loss: 177.95
train mean loss: 179.10
epoch train time: 0:00:15.811357
elapsed time: 0:43:15.655806
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 10:09:11.770990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.45
 ---- batch: 020 ----
mean loss: 184.81
 ---- batch: 030 ----
mean loss: 178.43
 ---- batch: 040 ----
mean loss: 182.26
 ---- batch: 050 ----
mean loss: 185.24
 ---- batch: 060 ----
mean loss: 175.66
 ---- batch: 070 ----
mean loss: 193.32
 ---- batch: 080 ----
mean loss: 173.68
 ---- batch: 090 ----
mean loss: 167.13
 ---- batch: 100 ----
mean loss: 180.86
 ---- batch: 110 ----
mean loss: 177.60
train mean loss: 179.20
epoch train time: 0:00:15.786655
elapsed time: 0:43:31.443509
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 10:09:27.558673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.72
 ---- batch: 020 ----
mean loss: 168.47
 ---- batch: 030 ----
mean loss: 179.94
 ---- batch: 040 ----
mean loss: 178.68
 ---- batch: 050 ----
mean loss: 188.52
 ---- batch: 060 ----
mean loss: 174.58
 ---- batch: 070 ----
mean loss: 174.31
 ---- batch: 080 ----
mean loss: 177.99
 ---- batch: 090 ----
mean loss: 181.64
 ---- batch: 100 ----
mean loss: 172.32
 ---- batch: 110 ----
mean loss: 165.68
train mean loss: 177.71
epoch train time: 0:00:15.771984
elapsed time: 0:43:47.216452
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 10:09:43.331702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.17
 ---- batch: 020 ----
mean loss: 178.59
 ---- batch: 030 ----
mean loss: 178.06
 ---- batch: 040 ----
mean loss: 174.07
 ---- batch: 050 ----
mean loss: 170.51
 ---- batch: 060 ----
mean loss: 175.21
 ---- batch: 070 ----
mean loss: 163.51
 ---- batch: 080 ----
mean loss: 185.98
 ---- batch: 090 ----
mean loss: 182.82
 ---- batch: 100 ----
mean loss: 192.42
 ---- batch: 110 ----
mean loss: 171.77
train mean loss: 177.30
epoch train time: 0:00:15.768601
elapsed time: 0:44:02.986141
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 10:09:59.101318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.09
 ---- batch: 020 ----
mean loss: 177.49
 ---- batch: 030 ----
mean loss: 170.96
 ---- batch: 040 ----
mean loss: 170.88
 ---- batch: 050 ----
mean loss: 182.02
 ---- batch: 060 ----
mean loss: 175.00
 ---- batch: 070 ----
mean loss: 176.44
 ---- batch: 080 ----
mean loss: 168.37
 ---- batch: 090 ----
mean loss: 187.34
 ---- batch: 100 ----
mean loss: 179.55
 ---- batch: 110 ----
mean loss: 188.93
train mean loss: 177.81
epoch train time: 0:00:15.721944
elapsed time: 0:44:18.709051
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 10:10:14.824231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.47
 ---- batch: 020 ----
mean loss: 182.85
 ---- batch: 030 ----
mean loss: 176.64
 ---- batch: 040 ----
mean loss: 185.03
 ---- batch: 050 ----
mean loss: 184.94
 ---- batch: 060 ----
mean loss: 177.15
 ---- batch: 070 ----
mean loss: 167.78
 ---- batch: 080 ----
mean loss: 178.77
 ---- batch: 090 ----
mean loss: 173.61
 ---- batch: 100 ----
mean loss: 175.14
 ---- batch: 110 ----
mean loss: 175.22
train mean loss: 178.07
epoch train time: 0:00:15.745249
elapsed time: 0:44:34.455336
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 10:10:30.570507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.99
 ---- batch: 020 ----
mean loss: 177.84
 ---- batch: 030 ----
mean loss: 168.58
 ---- batch: 040 ----
mean loss: 185.35
 ---- batch: 050 ----
mean loss: 183.66
 ---- batch: 060 ----
mean loss: 182.86
 ---- batch: 070 ----
mean loss: 174.08
 ---- batch: 080 ----
mean loss: 180.44
 ---- batch: 090 ----
mean loss: 174.77
 ---- batch: 100 ----
mean loss: 176.27
 ---- batch: 110 ----
mean loss: 168.20
train mean loss: 177.23
epoch train time: 0:00:15.738697
elapsed time: 0:44:50.195011
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 10:10:46.310224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.00
 ---- batch: 020 ----
mean loss: 184.71
 ---- batch: 030 ----
mean loss: 180.06
 ---- batch: 040 ----
mean loss: 177.16
 ---- batch: 050 ----
mean loss: 178.50
 ---- batch: 060 ----
mean loss: 184.33
 ---- batch: 070 ----
mean loss: 170.87
 ---- batch: 080 ----
mean loss: 179.90
 ---- batch: 090 ----
mean loss: 177.46
 ---- batch: 100 ----
mean loss: 169.87
 ---- batch: 110 ----
mean loss: 169.38
train mean loss: 177.63
epoch train time: 0:00:15.701506
elapsed time: 0:45:05.897504
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 10:11:02.012655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.66
 ---- batch: 020 ----
mean loss: 189.63
 ---- batch: 030 ----
mean loss: 182.20
 ---- batch: 040 ----
mean loss: 177.44
 ---- batch: 050 ----
mean loss: 173.98
 ---- batch: 060 ----
mean loss: 169.01
 ---- batch: 070 ----
mean loss: 184.22
 ---- batch: 080 ----
mean loss: 175.56
 ---- batch: 090 ----
mean loss: 173.70
 ---- batch: 100 ----
mean loss: 179.59
 ---- batch: 110 ----
mean loss: 178.61
train mean loss: 177.44
epoch train time: 0:00:15.737110
elapsed time: 0:45:21.635643
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 10:11:17.750830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.94
 ---- batch: 020 ----
mean loss: 181.15
 ---- batch: 030 ----
mean loss: 179.70
 ---- batch: 040 ----
mean loss: 169.13
 ---- batch: 050 ----
mean loss: 172.30
 ---- batch: 060 ----
mean loss: 181.08
 ---- batch: 070 ----
mean loss: 173.54
 ---- batch: 080 ----
mean loss: 178.47
 ---- batch: 090 ----
mean loss: 180.96
 ---- batch: 100 ----
mean loss: 181.99
 ---- batch: 110 ----
mean loss: 176.12
train mean loss: 177.38
epoch train time: 0:00:15.792541
elapsed time: 0:45:37.429199
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 10:11:33.544548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.73
 ---- batch: 020 ----
mean loss: 173.85
 ---- batch: 030 ----
mean loss: 181.47
 ---- batch: 040 ----
mean loss: 177.12
 ---- batch: 050 ----
mean loss: 167.33
 ---- batch: 060 ----
mean loss: 178.51
 ---- batch: 070 ----
mean loss: 186.04
 ---- batch: 080 ----
mean loss: 177.32
 ---- batch: 090 ----
mean loss: 187.14
 ---- batch: 100 ----
mean loss: 172.65
 ---- batch: 110 ----
mean loss: 186.91
train mean loss: 178.43
epoch train time: 0:00:15.787374
elapsed time: 0:45:53.218284
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 10:11:49.333163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.13
 ---- batch: 020 ----
mean loss: 182.37
 ---- batch: 030 ----
mean loss: 177.45
 ---- batch: 040 ----
mean loss: 177.85
 ---- batch: 050 ----
mean loss: 185.03
 ---- batch: 060 ----
mean loss: 178.27
 ---- batch: 070 ----
mean loss: 179.18
 ---- batch: 080 ----
mean loss: 172.80
 ---- batch: 090 ----
mean loss: 178.21
 ---- batch: 100 ----
mean loss: 180.57
 ---- batch: 110 ----
mean loss: 180.76
train mean loss: 178.46
epoch train time: 0:00:15.760565
elapsed time: 0:46:08.979516
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 10:12:05.094683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.43
 ---- batch: 020 ----
mean loss: 175.20
 ---- batch: 030 ----
mean loss: 175.62
 ---- batch: 040 ----
mean loss: 179.89
 ---- batch: 050 ----
mean loss: 177.01
 ---- batch: 060 ----
mean loss: 175.49
 ---- batch: 070 ----
mean loss: 182.27
 ---- batch: 080 ----
mean loss: 179.62
 ---- batch: 090 ----
mean loss: 179.65
 ---- batch: 100 ----
mean loss: 178.07
 ---- batch: 110 ----
mean loss: 169.93
train mean loss: 177.88
epoch train time: 0:00:15.724956
elapsed time: 0:46:24.705346
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 10:12:20.820644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.82
 ---- batch: 020 ----
mean loss: 178.95
 ---- batch: 030 ----
mean loss: 167.61
 ---- batch: 040 ----
mean loss: 189.96
 ---- batch: 050 ----
mean loss: 180.45
 ---- batch: 060 ----
mean loss: 177.80
 ---- batch: 070 ----
mean loss: 177.26
 ---- batch: 080 ----
mean loss: 178.96
 ---- batch: 090 ----
mean loss: 170.58
 ---- batch: 100 ----
mean loss: 175.46
 ---- batch: 110 ----
mean loss: 186.74
train mean loss: 178.37
epoch train time: 0:00:15.762141
elapsed time: 0:46:40.468498
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 10:12:36.583714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.62
 ---- batch: 020 ----
mean loss: 183.13
 ---- batch: 030 ----
mean loss: 179.78
 ---- batch: 040 ----
mean loss: 169.47
 ---- batch: 050 ----
mean loss: 174.84
 ---- batch: 060 ----
mean loss: 170.97
 ---- batch: 070 ----
mean loss: 170.42
 ---- batch: 080 ----
mean loss: 178.40
 ---- batch: 090 ----
mean loss: 182.79
 ---- batch: 100 ----
mean loss: 184.87
 ---- batch: 110 ----
mean loss: 167.94
train mean loss: 176.79
epoch train time: 0:00:15.750596
elapsed time: 0:46:56.220101
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 10:12:52.335312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.20
 ---- batch: 020 ----
mean loss: 189.40
 ---- batch: 030 ----
mean loss: 186.17
 ---- batch: 040 ----
mean loss: 171.51
 ---- batch: 050 ----
mean loss: 166.96
 ---- batch: 060 ----
mean loss: 179.29
 ---- batch: 070 ----
mean loss: 170.94
 ---- batch: 080 ----
mean loss: 171.67
 ---- batch: 090 ----
mean loss: 178.50
 ---- batch: 100 ----
mean loss: 171.18
 ---- batch: 110 ----
mean loss: 180.75
train mean loss: 177.12
epoch train time: 0:00:15.753438
elapsed time: 0:47:11.974561
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 10:13:08.089792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.98
 ---- batch: 020 ----
mean loss: 170.78
 ---- batch: 030 ----
mean loss: 174.86
 ---- batch: 040 ----
mean loss: 180.92
 ---- batch: 050 ----
mean loss: 177.40
 ---- batch: 060 ----
mean loss: 172.44
 ---- batch: 070 ----
mean loss: 183.46
 ---- batch: 080 ----
mean loss: 176.73
 ---- batch: 090 ----
mean loss: 173.30
 ---- batch: 100 ----
mean loss: 173.85
 ---- batch: 110 ----
mean loss: 182.52
train mean loss: 175.88
epoch train time: 0:00:15.832535
elapsed time: 0:47:27.808160
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 10:13:23.923361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.19
 ---- batch: 020 ----
mean loss: 181.79
 ---- batch: 030 ----
mean loss: 171.66
 ---- batch: 040 ----
mean loss: 168.42
 ---- batch: 050 ----
mean loss: 186.83
 ---- batch: 060 ----
mean loss: 178.37
 ---- batch: 070 ----
mean loss: 180.77
 ---- batch: 080 ----
mean loss: 179.00
 ---- batch: 090 ----
mean loss: 174.54
 ---- batch: 100 ----
mean loss: 181.71
 ---- batch: 110 ----
mean loss: 176.99
train mean loss: 177.71
epoch train time: 0:00:15.804201
elapsed time: 0:47:43.613371
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 10:13:39.728621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.14
 ---- batch: 020 ----
mean loss: 176.47
 ---- batch: 030 ----
mean loss: 185.70
 ---- batch: 040 ----
mean loss: 171.37
 ---- batch: 050 ----
mean loss: 174.32
 ---- batch: 060 ----
mean loss: 166.49
 ---- batch: 070 ----
mean loss: 174.21
 ---- batch: 080 ----
mean loss: 175.83
 ---- batch: 090 ----
mean loss: 176.05
 ---- batch: 100 ----
mean loss: 178.76
 ---- batch: 110 ----
mean loss: 180.07
train mean loss: 175.54
epoch train time: 0:00:15.795759
elapsed time: 0:47:59.410258
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 10:13:55.525505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.59
 ---- batch: 020 ----
mean loss: 179.15
 ---- batch: 030 ----
mean loss: 172.01
 ---- batch: 040 ----
mean loss: 179.26
 ---- batch: 050 ----
mean loss: 177.56
 ---- batch: 060 ----
mean loss: 182.18
 ---- batch: 070 ----
mean loss: 171.53
 ---- batch: 080 ----
mean loss: 171.90
 ---- batch: 090 ----
mean loss: 173.32
 ---- batch: 100 ----
mean loss: 176.00
 ---- batch: 110 ----
mean loss: 166.63
train mean loss: 175.38
epoch train time: 0:00:15.861933
elapsed time: 0:48:15.273260
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 10:14:11.388530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.58
 ---- batch: 020 ----
mean loss: 176.93
 ---- batch: 030 ----
mean loss: 171.22
 ---- batch: 040 ----
mean loss: 174.10
 ---- batch: 050 ----
mean loss: 170.62
 ---- batch: 060 ----
mean loss: 172.38
 ---- batch: 070 ----
mean loss: 180.96
 ---- batch: 080 ----
mean loss: 176.82
 ---- batch: 090 ----
mean loss: 189.10
 ---- batch: 100 ----
mean loss: 176.10
 ---- batch: 110 ----
mean loss: 170.94
train mean loss: 175.69
epoch train time: 0:00:15.802159
elapsed time: 0:48:31.076449
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 10:14:27.191601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.81
 ---- batch: 020 ----
mean loss: 178.23
 ---- batch: 030 ----
mean loss: 177.40
 ---- batch: 040 ----
mean loss: 173.21
 ---- batch: 050 ----
mean loss: 181.68
 ---- batch: 060 ----
mean loss: 185.16
 ---- batch: 070 ----
mean loss: 177.84
 ---- batch: 080 ----
mean loss: 175.87
 ---- batch: 090 ----
mean loss: 162.16
 ---- batch: 100 ----
mean loss: 169.52
 ---- batch: 110 ----
mean loss: 182.51
train mean loss: 176.09
epoch train time: 0:00:15.773014
elapsed time: 0:48:46.850561
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 10:14:42.965878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.84
 ---- batch: 020 ----
mean loss: 168.76
 ---- batch: 030 ----
mean loss: 180.59
 ---- batch: 040 ----
mean loss: 176.26
 ---- batch: 050 ----
mean loss: 178.63
 ---- batch: 060 ----
mean loss: 175.98
 ---- batch: 070 ----
mean loss: 178.45
 ---- batch: 080 ----
mean loss: 166.17
 ---- batch: 090 ----
mean loss: 172.84
 ---- batch: 100 ----
mean loss: 173.53
 ---- batch: 110 ----
mean loss: 177.04
train mean loss: 174.82
epoch train time: 0:00:15.792720
elapsed time: 0:49:02.644392
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 10:14:58.759570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.86
 ---- batch: 020 ----
mean loss: 180.96
 ---- batch: 030 ----
mean loss: 183.27
 ---- batch: 040 ----
mean loss: 172.51
 ---- batch: 050 ----
mean loss: 171.92
 ---- batch: 060 ----
mean loss: 175.11
 ---- batch: 070 ----
mean loss: 174.31
 ---- batch: 080 ----
mean loss: 173.52
 ---- batch: 090 ----
mean loss: 168.48
 ---- batch: 100 ----
mean loss: 165.07
 ---- batch: 110 ----
mean loss: 173.18
train mean loss: 174.83
epoch train time: 0:00:15.784543
elapsed time: 0:49:18.429986
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 10:15:14.545177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.86
 ---- batch: 020 ----
mean loss: 180.42
 ---- batch: 030 ----
mean loss: 177.77
 ---- batch: 040 ----
mean loss: 180.11
 ---- batch: 050 ----
mean loss: 181.72
 ---- batch: 060 ----
mean loss: 177.77
 ---- batch: 070 ----
mean loss: 166.65
 ---- batch: 080 ----
mean loss: 169.34
 ---- batch: 090 ----
mean loss: 178.35
 ---- batch: 100 ----
mean loss: 176.44
 ---- batch: 110 ----
mean loss: 176.70
train mean loss: 175.83
epoch train time: 0:00:15.821168
elapsed time: 0:49:34.252116
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 10:15:30.367293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.58
 ---- batch: 020 ----
mean loss: 176.21
 ---- batch: 030 ----
mean loss: 182.28
 ---- batch: 040 ----
mean loss: 164.15
 ---- batch: 050 ----
mean loss: 174.21
 ---- batch: 060 ----
mean loss: 174.48
 ---- batch: 070 ----
mean loss: 174.16
 ---- batch: 080 ----
mean loss: 172.30
 ---- batch: 090 ----
mean loss: 179.72
 ---- batch: 100 ----
mean loss: 175.25
 ---- batch: 110 ----
mean loss: 169.82
train mean loss: 174.70
epoch train time: 0:00:15.844814
elapsed time: 0:49:50.097865
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 10:15:46.213095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.12
 ---- batch: 020 ----
mean loss: 175.01
 ---- batch: 030 ----
mean loss: 190.80
 ---- batch: 040 ----
mean loss: 186.17
 ---- batch: 050 ----
mean loss: 175.72
 ---- batch: 060 ----
mean loss: 172.12
 ---- batch: 070 ----
mean loss: 168.24
 ---- batch: 080 ----
mean loss: 182.77
 ---- batch: 090 ----
mean loss: 173.44
 ---- batch: 100 ----
mean loss: 164.75
 ---- batch: 110 ----
mean loss: 171.28
train mean loss: 176.15
epoch train time: 0:00:15.793296
elapsed time: 0:50:05.892212
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 10:16:02.007378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.06
 ---- batch: 020 ----
mean loss: 171.71
 ---- batch: 030 ----
mean loss: 179.24
 ---- batch: 040 ----
mean loss: 172.21
 ---- batch: 050 ----
mean loss: 173.50
 ---- batch: 060 ----
mean loss: 180.56
 ---- batch: 070 ----
mean loss: 184.10
 ---- batch: 080 ----
mean loss: 172.45
 ---- batch: 090 ----
mean loss: 170.28
 ---- batch: 100 ----
mean loss: 174.32
 ---- batch: 110 ----
mean loss: 182.49
train mean loss: 175.26
epoch train time: 0:00:15.769940
elapsed time: 0:50:21.663161
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 10:16:17.778347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.70
 ---- batch: 020 ----
mean loss: 170.17
 ---- batch: 030 ----
mean loss: 175.01
 ---- batch: 040 ----
mean loss: 178.03
 ---- batch: 050 ----
mean loss: 184.18
 ---- batch: 060 ----
mean loss: 176.93
 ---- batch: 070 ----
mean loss: 178.29
 ---- batch: 080 ----
mean loss: 173.72
 ---- batch: 090 ----
mean loss: 179.95
 ---- batch: 100 ----
mean loss: 179.93
 ---- batch: 110 ----
mean loss: 178.22
train mean loss: 177.50
epoch train time: 0:00:15.747308
elapsed time: 0:50:37.411474
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 10:16:33.526640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.90
 ---- batch: 020 ----
mean loss: 174.19
 ---- batch: 030 ----
mean loss: 176.31
 ---- batch: 040 ----
mean loss: 174.40
 ---- batch: 050 ----
mean loss: 178.56
 ---- batch: 060 ----
mean loss: 180.31
 ---- batch: 070 ----
mean loss: 166.18
 ---- batch: 080 ----
mean loss: 174.27
 ---- batch: 090 ----
mean loss: 167.60
 ---- batch: 100 ----
mean loss: 171.64
 ---- batch: 110 ----
mean loss: 177.31
train mean loss: 175.10
epoch train time: 0:00:15.815156
elapsed time: 0:50:53.227501
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 10:16:49.342681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.56
 ---- batch: 020 ----
mean loss: 179.16
 ---- batch: 030 ----
mean loss: 175.01
 ---- batch: 040 ----
mean loss: 179.15
 ---- batch: 050 ----
mean loss: 178.24
 ---- batch: 060 ----
mean loss: 165.88
 ---- batch: 070 ----
mean loss: 167.39
 ---- batch: 080 ----
mean loss: 171.48
 ---- batch: 090 ----
mean loss: 164.52
 ---- batch: 100 ----
mean loss: 181.64
 ---- batch: 110 ----
mean loss: 177.86
train mean loss: 174.33
epoch train time: 0:00:15.837117
elapsed time: 0:51:09.065575
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 10:17:05.180781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.31
 ---- batch: 020 ----
mean loss: 188.66
 ---- batch: 030 ----
mean loss: 177.48
 ---- batch: 040 ----
mean loss: 174.23
 ---- batch: 050 ----
mean loss: 168.61
 ---- batch: 060 ----
mean loss: 170.30
 ---- batch: 070 ----
mean loss: 181.45
 ---- batch: 080 ----
mean loss: 174.18
 ---- batch: 090 ----
mean loss: 178.34
 ---- batch: 100 ----
mean loss: 175.28
 ---- batch: 110 ----
mean loss: 163.98
train mean loss: 174.59
epoch train time: 0:00:15.806740
elapsed time: 0:51:24.873341
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 10:17:20.988618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.87
 ---- batch: 020 ----
mean loss: 179.90
 ---- batch: 030 ----
mean loss: 183.48
 ---- batch: 040 ----
mean loss: 181.89
 ---- batch: 050 ----
mean loss: 170.43
 ---- batch: 060 ----
mean loss: 170.20
 ---- batch: 070 ----
mean loss: 172.77
 ---- batch: 080 ----
mean loss: 171.89
 ---- batch: 090 ----
mean loss: 178.78
 ---- batch: 100 ----
mean loss: 169.63
 ---- batch: 110 ----
mean loss: 179.37
train mean loss: 175.22
epoch train time: 0:00:15.856272
elapsed time: 0:51:40.730676
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 10:17:36.845968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.76
 ---- batch: 020 ----
mean loss: 170.95
 ---- batch: 030 ----
mean loss: 172.78
 ---- batch: 040 ----
mean loss: 188.76
 ---- batch: 050 ----
mean loss: 170.35
 ---- batch: 060 ----
mean loss: 165.88
 ---- batch: 070 ----
mean loss: 175.00
 ---- batch: 080 ----
mean loss: 177.62
 ---- batch: 090 ----
mean loss: 171.59
 ---- batch: 100 ----
mean loss: 165.21
 ---- batch: 110 ----
mean loss: 179.56
train mean loss: 174.01
epoch train time: 0:00:15.834624
elapsed time: 0:51:56.566397
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 10:17:52.681590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.11
 ---- batch: 020 ----
mean loss: 177.50
 ---- batch: 030 ----
mean loss: 179.39
 ---- batch: 040 ----
mean loss: 162.48
 ---- batch: 050 ----
mean loss: 177.83
 ---- batch: 060 ----
mean loss: 166.90
 ---- batch: 070 ----
mean loss: 188.59
 ---- batch: 080 ----
mean loss: 179.83
 ---- batch: 090 ----
mean loss: 170.58
 ---- batch: 100 ----
mean loss: 172.54
 ---- batch: 110 ----
mean loss: 171.76
train mean loss: 174.51
epoch train time: 0:00:15.804653
elapsed time: 0:52:12.372016
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 10:18:08.487191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.49
 ---- batch: 020 ----
mean loss: 170.42
 ---- batch: 030 ----
mean loss: 169.82
 ---- batch: 040 ----
mean loss: 175.50
 ---- batch: 050 ----
mean loss: 174.53
 ---- batch: 060 ----
mean loss: 181.62
 ---- batch: 070 ----
mean loss: 169.34
 ---- batch: 080 ----
mean loss: 175.77
 ---- batch: 090 ----
mean loss: 169.52
 ---- batch: 100 ----
mean loss: 166.08
 ---- batch: 110 ----
mean loss: 175.20
train mean loss: 173.43
epoch train time: 0:00:15.856847
elapsed time: 0:52:28.229866
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 10:18:24.345090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.70
 ---- batch: 020 ----
mean loss: 164.56
 ---- batch: 030 ----
mean loss: 186.48
 ---- batch: 040 ----
mean loss: 163.14
 ---- batch: 050 ----
mean loss: 176.19
 ---- batch: 060 ----
mean loss: 187.79
 ---- batch: 070 ----
mean loss: 179.95
 ---- batch: 080 ----
mean loss: 172.00
 ---- batch: 090 ----
mean loss: 164.34
 ---- batch: 100 ----
mean loss: 172.53
 ---- batch: 110 ----
mean loss: 176.29
train mean loss: 174.39
epoch train time: 0:00:15.836749
elapsed time: 0:52:44.067618
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 10:18:40.182774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.30
 ---- batch: 020 ----
mean loss: 171.56
 ---- batch: 030 ----
mean loss: 174.25
 ---- batch: 040 ----
mean loss: 174.44
 ---- batch: 050 ----
mean loss: 171.76
 ---- batch: 060 ----
mean loss: 179.25
 ---- batch: 070 ----
mean loss: 170.51
 ---- batch: 080 ----
mean loss: 163.20
 ---- batch: 090 ----
mean loss: 172.43
 ---- batch: 100 ----
mean loss: 178.07
 ---- batch: 110 ----
mean loss: 184.41
train mean loss: 173.95
epoch train time: 0:00:15.820109
elapsed time: 0:52:59.888656
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 10:18:56.003968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.68
 ---- batch: 020 ----
mean loss: 171.33
 ---- batch: 030 ----
mean loss: 180.75
 ---- batch: 040 ----
mean loss: 169.50
 ---- batch: 050 ----
mean loss: 163.89
 ---- batch: 060 ----
mean loss: 172.08
 ---- batch: 070 ----
mean loss: 167.08
 ---- batch: 080 ----
mean loss: 173.32
 ---- batch: 090 ----
mean loss: 172.42
 ---- batch: 100 ----
mean loss: 171.71
 ---- batch: 110 ----
mean loss: 166.95
train mean loss: 170.78
epoch train time: 0:00:15.843551
elapsed time: 0:53:15.733962
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 10:19:11.848861
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.37
 ---- batch: 020 ----
mean loss: 164.19
 ---- batch: 030 ----
mean loss: 164.09
 ---- batch: 040 ----
mean loss: 173.51
 ---- batch: 050 ----
mean loss: 173.07
 ---- batch: 060 ----
mean loss: 171.75
 ---- batch: 070 ----
mean loss: 164.47
 ---- batch: 080 ----
mean loss: 179.18
 ---- batch: 090 ----
mean loss: 174.25
 ---- batch: 100 ----
mean loss: 172.38
 ---- batch: 110 ----
mean loss: 164.88
train mean loss: 170.65
epoch train time: 0:00:15.882255
elapsed time: 0:53:31.616958
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 10:19:27.732179
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.83
 ---- batch: 020 ----
mean loss: 172.18
 ---- batch: 030 ----
mean loss: 177.22
 ---- batch: 040 ----
mean loss: 171.87
 ---- batch: 050 ----
mean loss: 172.50
 ---- batch: 060 ----
mean loss: 175.73
 ---- batch: 070 ----
mean loss: 166.03
 ---- batch: 080 ----
mean loss: 176.75
 ---- batch: 090 ----
mean loss: 166.72
 ---- batch: 100 ----
mean loss: 169.77
 ---- batch: 110 ----
mean loss: 168.06
train mean loss: 171.05
epoch train time: 0:00:15.776625
elapsed time: 0:53:47.394658
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 10:19:43.509903
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.42
 ---- batch: 020 ----
mean loss: 166.68
 ---- batch: 030 ----
mean loss: 165.77
 ---- batch: 040 ----
mean loss: 168.79
 ---- batch: 050 ----
mean loss: 173.21
 ---- batch: 060 ----
mean loss: 173.61
 ---- batch: 070 ----
mean loss: 175.92
 ---- batch: 080 ----
mean loss: 175.09
 ---- batch: 090 ----
mean loss: 174.84
 ---- batch: 100 ----
mean loss: 164.18
 ---- batch: 110 ----
mean loss: 169.51
train mean loss: 170.82
epoch train time: 0:00:15.791562
elapsed time: 0:54:03.187305
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 10:19:59.302543
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.26
 ---- batch: 020 ----
mean loss: 168.69
 ---- batch: 030 ----
mean loss: 173.21
 ---- batch: 040 ----
mean loss: 168.84
 ---- batch: 050 ----
mean loss: 170.82
 ---- batch: 060 ----
mean loss: 172.97
 ---- batch: 070 ----
mean loss: 172.93
 ---- batch: 080 ----
mean loss: 176.96
 ---- batch: 090 ----
mean loss: 163.02
 ---- batch: 100 ----
mean loss: 165.16
 ---- batch: 110 ----
mean loss: 174.26
train mean loss: 170.95
epoch train time: 0:00:15.736181
elapsed time: 0:54:18.924499
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 10:20:15.039671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.92
 ---- batch: 020 ----
mean loss: 169.18
 ---- batch: 030 ----
mean loss: 170.92
 ---- batch: 040 ----
mean loss: 172.93
 ---- batch: 050 ----
mean loss: 164.31
 ---- batch: 060 ----
mean loss: 174.19
 ---- batch: 070 ----
mean loss: 173.44
 ---- batch: 080 ----
mean loss: 176.11
 ---- batch: 090 ----
mean loss: 166.96
 ---- batch: 100 ----
mean loss: 163.02
 ---- batch: 110 ----
mean loss: 176.51
train mean loss: 170.83
epoch train time: 0:00:15.775694
elapsed time: 0:54:34.701206
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 10:20:30.816407
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.72
 ---- batch: 020 ----
mean loss: 175.19
 ---- batch: 030 ----
mean loss: 172.40
 ---- batch: 040 ----
mean loss: 183.21
 ---- batch: 050 ----
mean loss: 160.92
 ---- batch: 060 ----
mean loss: 171.18
 ---- batch: 070 ----
mean loss: 163.47
 ---- batch: 080 ----
mean loss: 178.95
 ---- batch: 090 ----
mean loss: 165.54
 ---- batch: 100 ----
mean loss: 176.88
 ---- batch: 110 ----
mean loss: 168.38
train mean loss: 170.79
epoch train time: 0:00:15.747059
elapsed time: 0:54:50.449275
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 10:20:46.564486
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.69
 ---- batch: 020 ----
mean loss: 169.75
 ---- batch: 030 ----
mean loss: 167.65
 ---- batch: 040 ----
mean loss: 165.95
 ---- batch: 050 ----
mean loss: 171.19
 ---- batch: 060 ----
mean loss: 178.30
 ---- batch: 070 ----
mean loss: 178.24
 ---- batch: 080 ----
mean loss: 171.58
 ---- batch: 090 ----
mean loss: 170.69
 ---- batch: 100 ----
mean loss: 164.62
 ---- batch: 110 ----
mean loss: 175.14
train mean loss: 170.82
epoch train time: 0:00:16.102456
elapsed time: 0:55:06.552767
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 10:21:02.667940
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.62
 ---- batch: 020 ----
mean loss: 156.82
 ---- batch: 030 ----
mean loss: 176.24
 ---- batch: 040 ----
mean loss: 166.05
 ---- batch: 050 ----
mean loss: 166.87
 ---- batch: 060 ----
mean loss: 173.52
 ---- batch: 070 ----
mean loss: 172.09
 ---- batch: 080 ----
mean loss: 169.17
 ---- batch: 090 ----
mean loss: 175.11
 ---- batch: 100 ----
mean loss: 169.16
 ---- batch: 110 ----
mean loss: 181.43
train mean loss: 170.75
epoch train time: 0:00:15.760348
elapsed time: 0:55:22.314070
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 10:21:18.429315
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.12
 ---- batch: 020 ----
mean loss: 170.20
 ---- batch: 030 ----
mean loss: 166.33
 ---- batch: 040 ----
mean loss: 173.95
 ---- batch: 050 ----
mean loss: 174.74
 ---- batch: 060 ----
mean loss: 172.48
 ---- batch: 070 ----
mean loss: 164.63
 ---- batch: 080 ----
mean loss: 166.88
 ---- batch: 090 ----
mean loss: 165.17
 ---- batch: 100 ----
mean loss: 168.95
 ---- batch: 110 ----
mean loss: 165.26
train mean loss: 170.52
epoch train time: 0:00:15.794489
elapsed time: 0:55:38.109639
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 10:21:34.224817
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.50
 ---- batch: 020 ----
mean loss: 173.13
 ---- batch: 030 ----
mean loss: 171.82
 ---- batch: 040 ----
mean loss: 174.00
 ---- batch: 050 ----
mean loss: 172.52
 ---- batch: 060 ----
mean loss: 172.83
 ---- batch: 070 ----
mean loss: 165.06
 ---- batch: 080 ----
mean loss: 169.18
 ---- batch: 090 ----
mean loss: 174.48
 ---- batch: 100 ----
mean loss: 163.14
 ---- batch: 110 ----
mean loss: 171.51
train mean loss: 170.95
epoch train time: 0:00:15.784915
elapsed time: 0:55:53.895625
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 10:21:50.010791
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.72
 ---- batch: 020 ----
mean loss: 169.14
 ---- batch: 030 ----
mean loss: 169.50
 ---- batch: 040 ----
mean loss: 178.20
 ---- batch: 050 ----
mean loss: 173.38
 ---- batch: 060 ----
mean loss: 173.04
 ---- batch: 070 ----
mean loss: 173.52
 ---- batch: 080 ----
mean loss: 167.48
 ---- batch: 090 ----
mean loss: 168.74
 ---- batch: 100 ----
mean loss: 171.46
 ---- batch: 110 ----
mean loss: 166.57
train mean loss: 170.61
epoch train time: 0:00:15.830325
elapsed time: 0:56:09.726918
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 10:22:05.842117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.56
 ---- batch: 020 ----
mean loss: 164.20
 ---- batch: 030 ----
mean loss: 160.97
 ---- batch: 040 ----
mean loss: 179.94
 ---- batch: 050 ----
mean loss: 172.94
 ---- batch: 060 ----
mean loss: 176.29
 ---- batch: 070 ----
mean loss: 164.14
 ---- batch: 080 ----
mean loss: 167.05
 ---- batch: 090 ----
mean loss: 163.78
 ---- batch: 100 ----
mean loss: 179.30
 ---- batch: 110 ----
mean loss: 176.58
train mean loss: 170.68
epoch train time: 0:00:15.785250
elapsed time: 0:56:25.513161
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 10:22:21.628354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.32
 ---- batch: 020 ----
mean loss: 175.62
 ---- batch: 030 ----
mean loss: 166.59
 ---- batch: 040 ----
mean loss: 162.01
 ---- batch: 050 ----
mean loss: 176.12
 ---- batch: 060 ----
mean loss: 168.09
 ---- batch: 070 ----
mean loss: 179.43
 ---- batch: 080 ----
mean loss: 172.00
 ---- batch: 090 ----
mean loss: 168.62
 ---- batch: 100 ----
mean loss: 171.74
 ---- batch: 110 ----
mean loss: 173.82
train mean loss: 170.74
epoch train time: 0:00:15.797291
elapsed time: 0:56:41.311402
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 10:22:37.426628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.53
 ---- batch: 020 ----
mean loss: 175.01
 ---- batch: 030 ----
mean loss: 175.46
 ---- batch: 040 ----
mean loss: 173.69
 ---- batch: 050 ----
mean loss: 165.23
 ---- batch: 060 ----
mean loss: 167.14
 ---- batch: 070 ----
mean loss: 177.45
 ---- batch: 080 ----
mean loss: 161.92
 ---- batch: 090 ----
mean loss: 161.63
 ---- batch: 100 ----
mean loss: 168.37
 ---- batch: 110 ----
mean loss: 172.74
train mean loss: 170.75
epoch train time: 0:00:15.758570
elapsed time: 0:56:57.071075
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 10:22:53.186309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.10
 ---- batch: 020 ----
mean loss: 172.88
 ---- batch: 030 ----
mean loss: 169.63
 ---- batch: 040 ----
mean loss: 169.61
 ---- batch: 050 ----
mean loss: 165.96
 ---- batch: 060 ----
mean loss: 168.94
 ---- batch: 070 ----
mean loss: 165.17
 ---- batch: 080 ----
mean loss: 172.54
 ---- batch: 090 ----
mean loss: 174.98
 ---- batch: 100 ----
mean loss: 173.02
 ---- batch: 110 ----
mean loss: 168.95
train mean loss: 170.72
epoch train time: 0:00:15.816335
elapsed time: 0:57:12.888500
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 10:23:09.003758
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.45
 ---- batch: 020 ----
mean loss: 166.15
 ---- batch: 030 ----
mean loss: 173.04
 ---- batch: 040 ----
mean loss: 167.70
 ---- batch: 050 ----
mean loss: 172.07
 ---- batch: 060 ----
mean loss: 173.51
 ---- batch: 070 ----
mean loss: 171.26
 ---- batch: 080 ----
mean loss: 173.27
 ---- batch: 090 ----
mean loss: 172.31
 ---- batch: 100 ----
mean loss: 177.38
 ---- batch: 110 ----
mean loss: 165.27
train mean loss: 170.42
epoch train time: 0:00:15.805270
elapsed time: 0:57:28.694797
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 10:23:24.809997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.17
 ---- batch: 020 ----
mean loss: 175.68
 ---- batch: 030 ----
mean loss: 177.49
 ---- batch: 040 ----
mean loss: 171.35
 ---- batch: 050 ----
mean loss: 168.82
 ---- batch: 060 ----
mean loss: 173.86
 ---- batch: 070 ----
mean loss: 165.58
 ---- batch: 080 ----
mean loss: 161.68
 ---- batch: 090 ----
mean loss: 165.15
 ---- batch: 100 ----
mean loss: 174.98
 ---- batch: 110 ----
mean loss: 168.15
train mean loss: 170.52
epoch train time: 0:00:15.816139
elapsed time: 0:57:44.511961
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 10:23:40.627126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.33
 ---- batch: 020 ----
mean loss: 170.08
 ---- batch: 030 ----
mean loss: 165.79
 ---- batch: 040 ----
mean loss: 167.41
 ---- batch: 050 ----
mean loss: 175.44
 ---- batch: 060 ----
mean loss: 164.18
 ---- batch: 070 ----
mean loss: 169.28
 ---- batch: 080 ----
mean loss: 181.33
 ---- batch: 090 ----
mean loss: 170.64
 ---- batch: 100 ----
mean loss: 170.03
 ---- batch: 110 ----
mean loss: 172.38
train mean loss: 170.64
epoch train time: 0:00:15.833740
elapsed time: 0:58:00.346586
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 10:23:56.461833
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.54
 ---- batch: 020 ----
mean loss: 171.39
 ---- batch: 030 ----
mean loss: 174.58
 ---- batch: 040 ----
mean loss: 172.86
 ---- batch: 050 ----
mean loss: 178.41
 ---- batch: 060 ----
mean loss: 167.91
 ---- batch: 070 ----
mean loss: 168.06
 ---- batch: 080 ----
mean loss: 167.60
 ---- batch: 090 ----
mean loss: 168.95
 ---- batch: 100 ----
mean loss: 173.93
 ---- batch: 110 ----
mean loss: 171.03
train mean loss: 170.62
epoch train time: 0:00:15.792359
elapsed time: 0:58:16.139964
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 10:24:12.255139
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.24
 ---- batch: 020 ----
mean loss: 167.40
 ---- batch: 030 ----
mean loss: 174.07
 ---- batch: 040 ----
mean loss: 176.25
 ---- batch: 050 ----
mean loss: 164.96
 ---- batch: 060 ----
mean loss: 169.81
 ---- batch: 070 ----
mean loss: 176.44
 ---- batch: 080 ----
mean loss: 177.97
 ---- batch: 090 ----
mean loss: 164.83
 ---- batch: 100 ----
mean loss: 166.69
 ---- batch: 110 ----
mean loss: 171.07
train mean loss: 170.46
epoch train time: 0:00:15.851692
elapsed time: 0:58:31.992612
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 10:24:28.107984
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.54
 ---- batch: 020 ----
mean loss: 172.61
 ---- batch: 030 ----
mean loss: 179.91
 ---- batch: 040 ----
mean loss: 168.19
 ---- batch: 050 ----
mean loss: 173.10
 ---- batch: 060 ----
mean loss: 169.91
 ---- batch: 070 ----
mean loss: 167.23
 ---- batch: 080 ----
mean loss: 167.71
 ---- batch: 090 ----
mean loss: 167.17
 ---- batch: 100 ----
mean loss: 174.07
 ---- batch: 110 ----
mean loss: 166.34
train mean loss: 170.67
epoch train time: 0:00:15.796979
elapsed time: 0:58:47.790865
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 10:24:43.906159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.69
 ---- batch: 020 ----
mean loss: 167.44
 ---- batch: 030 ----
mean loss: 164.44
 ---- batch: 040 ----
mean loss: 169.14
 ---- batch: 050 ----
mean loss: 179.87
 ---- batch: 060 ----
mean loss: 170.00
 ---- batch: 070 ----
mean loss: 173.38
 ---- batch: 080 ----
mean loss: 168.97
 ---- batch: 090 ----
mean loss: 175.07
 ---- batch: 100 ----
mean loss: 171.81
 ---- batch: 110 ----
mean loss: 164.43
train mean loss: 170.45
epoch train time: 0:00:15.776439
elapsed time: 0:59:03.568278
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 10:24:59.683464
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.80
 ---- batch: 020 ----
mean loss: 174.40
 ---- batch: 030 ----
mean loss: 173.66
 ---- batch: 040 ----
mean loss: 168.92
 ---- batch: 050 ----
mean loss: 172.04
 ---- batch: 060 ----
mean loss: 176.08
 ---- batch: 070 ----
mean loss: 169.14
 ---- batch: 080 ----
mean loss: 176.46
 ---- batch: 090 ----
mean loss: 174.55
 ---- batch: 100 ----
mean loss: 160.68
 ---- batch: 110 ----
mean loss: 167.80
train mean loss: 170.89
epoch train time: 0:00:15.812012
elapsed time: 0:59:19.381245
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 10:25:15.496483
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.33
 ---- batch: 020 ----
mean loss: 165.36
 ---- batch: 030 ----
mean loss: 178.40
 ---- batch: 040 ----
mean loss: 176.42
 ---- batch: 050 ----
mean loss: 164.08
 ---- batch: 060 ----
mean loss: 178.05
 ---- batch: 070 ----
mean loss: 162.31
 ---- batch: 080 ----
mean loss: 163.71
 ---- batch: 090 ----
mean loss: 175.54
 ---- batch: 100 ----
mean loss: 176.14
 ---- batch: 110 ----
mean loss: 170.97
train mean loss: 170.87
epoch train time: 0:00:15.795934
elapsed time: 0:59:35.178179
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 10:25:31.293343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.61
 ---- batch: 020 ----
mean loss: 178.89
 ---- batch: 030 ----
mean loss: 172.85
 ---- batch: 040 ----
mean loss: 164.99
 ---- batch: 050 ----
mean loss: 171.00
 ---- batch: 060 ----
mean loss: 156.94
 ---- batch: 070 ----
mean loss: 180.14
 ---- batch: 080 ----
mean loss: 165.83
 ---- batch: 090 ----
mean loss: 174.66
 ---- batch: 100 ----
mean loss: 172.03
 ---- batch: 110 ----
mean loss: 170.41
train mean loss: 170.32
epoch train time: 0:00:15.794125
elapsed time: 0:59:50.973267
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 10:25:47.088427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.72
 ---- batch: 020 ----
mean loss: 169.91
 ---- batch: 030 ----
mean loss: 173.77
 ---- batch: 040 ----
mean loss: 167.27
 ---- batch: 050 ----
mean loss: 169.66
 ---- batch: 060 ----
mean loss: 170.40
 ---- batch: 070 ----
mean loss: 179.60
 ---- batch: 080 ----
mean loss: 178.11
 ---- batch: 090 ----
mean loss: 168.80
 ---- batch: 100 ----
mean loss: 169.48
 ---- batch: 110 ----
mean loss: 165.84
train mean loss: 170.29
epoch train time: 0:00:15.750797
elapsed time: 1:00:06.725076
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 10:26:02.840280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.62
 ---- batch: 020 ----
mean loss: 166.72
 ---- batch: 030 ----
mean loss: 165.65
 ---- batch: 040 ----
mean loss: 174.05
 ---- batch: 050 ----
mean loss: 172.16
 ---- batch: 060 ----
mean loss: 172.76
 ---- batch: 070 ----
mean loss: 175.00
 ---- batch: 080 ----
mean loss: 174.91
 ---- batch: 090 ----
mean loss: 168.24
 ---- batch: 100 ----
mean loss: 166.14
 ---- batch: 110 ----
mean loss: 169.90
train mean loss: 170.55
epoch train time: 0:00:15.739770
elapsed time: 1:00:22.465865
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 10:26:18.581046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 161.66
 ---- batch: 020 ----
mean loss: 176.51
 ---- batch: 030 ----
mean loss: 179.89
 ---- batch: 040 ----
mean loss: 175.16
 ---- batch: 050 ----
mean loss: 171.27
 ---- batch: 060 ----
mean loss: 172.66
 ---- batch: 070 ----
mean loss: 168.73
 ---- batch: 080 ----
mean loss: 160.55
 ---- batch: 090 ----
mean loss: 173.00
 ---- batch: 100 ----
mean loss: 169.36
 ---- batch: 110 ----
mean loss: 166.65
train mean loss: 170.46
epoch train time: 0:00:15.793036
elapsed time: 1:00:38.259746
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 10:26:34.374969
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.96
 ---- batch: 020 ----
mean loss: 162.53
 ---- batch: 030 ----
mean loss: 165.98
 ---- batch: 040 ----
mean loss: 180.62
 ---- batch: 050 ----
mean loss: 169.75
 ---- batch: 060 ----
mean loss: 170.30
 ---- batch: 070 ----
mean loss: 173.85
 ---- batch: 080 ----
mean loss: 168.58
 ---- batch: 090 ----
mean loss: 170.85
 ---- batch: 100 ----
mean loss: 164.50
 ---- batch: 110 ----
mean loss: 174.05
train mean loss: 170.46
epoch train time: 0:00:15.725895
elapsed time: 1:00:53.986679
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 10:26:50.101901
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.00
 ---- batch: 020 ----
mean loss: 161.06
 ---- batch: 030 ----
mean loss: 163.77
 ---- batch: 040 ----
mean loss: 179.38
 ---- batch: 050 ----
mean loss: 177.19
 ---- batch: 060 ----
mean loss: 162.56
 ---- batch: 070 ----
mean loss: 168.83
 ---- batch: 080 ----
mean loss: 176.88
 ---- batch: 090 ----
mean loss: 167.13
 ---- batch: 100 ----
mean loss: 176.03
 ---- batch: 110 ----
mean loss: 175.31
train mean loss: 170.30
epoch train time: 0:00:15.778125
elapsed time: 1:01:09.765691
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 10:27:05.880871
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.48
 ---- batch: 020 ----
mean loss: 170.10
 ---- batch: 030 ----
mean loss: 171.52
 ---- batch: 040 ----
mean loss: 174.17
 ---- batch: 050 ----
mean loss: 162.46
 ---- batch: 060 ----
mean loss: 178.27
 ---- batch: 070 ----
mean loss: 170.40
 ---- batch: 080 ----
mean loss: 173.77
 ---- batch: 090 ----
mean loss: 171.38
 ---- batch: 100 ----
mean loss: 165.52
 ---- batch: 110 ----
mean loss: 166.78
train mean loss: 170.45
epoch train time: 0:00:15.723485
elapsed time: 1:01:25.490170
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 10:27:21.605594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.10
 ---- batch: 020 ----
mean loss: 165.80
 ---- batch: 030 ----
mean loss: 173.47
 ---- batch: 040 ----
mean loss: 173.33
 ---- batch: 050 ----
mean loss: 165.56
 ---- batch: 060 ----
mean loss: 175.05
 ---- batch: 070 ----
mean loss: 173.38
 ---- batch: 080 ----
mean loss: 172.30
 ---- batch: 090 ----
mean loss: 168.34
 ---- batch: 100 ----
mean loss: 176.27
 ---- batch: 110 ----
mean loss: 167.17
train mean loss: 170.84
epoch train time: 0:00:15.747716
elapsed time: 1:01:41.239766
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 10:27:37.354622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.15
 ---- batch: 020 ----
mean loss: 168.19
 ---- batch: 030 ----
mean loss: 161.41
 ---- batch: 040 ----
mean loss: 171.63
 ---- batch: 050 ----
mean loss: 166.41
 ---- batch: 060 ----
mean loss: 174.26
 ---- batch: 070 ----
mean loss: 166.33
 ---- batch: 080 ----
mean loss: 176.42
 ---- batch: 090 ----
mean loss: 176.05
 ---- batch: 100 ----
mean loss: 170.84
 ---- batch: 110 ----
mean loss: 172.72
train mean loss: 170.35
epoch train time: 0:00:15.760077
elapsed time: 1:01:57.000440
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 10:27:53.115737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.68
 ---- batch: 020 ----
mean loss: 162.29
 ---- batch: 030 ----
mean loss: 165.51
 ---- batch: 040 ----
mean loss: 180.55
 ---- batch: 050 ----
mean loss: 166.84
 ---- batch: 060 ----
mean loss: 176.21
 ---- batch: 070 ----
mean loss: 172.41
 ---- batch: 080 ----
mean loss: 166.19
 ---- batch: 090 ----
mean loss: 168.10
 ---- batch: 100 ----
mean loss: 166.88
 ---- batch: 110 ----
mean loss: 168.95
train mean loss: 170.28
epoch train time: 0:00:15.719470
elapsed time: 1:02:12.721065
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 10:28:08.836225
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.87
 ---- batch: 020 ----
mean loss: 173.98
 ---- batch: 030 ----
mean loss: 167.02
 ---- batch: 040 ----
mean loss: 173.70
 ---- batch: 050 ----
mean loss: 170.53
 ---- batch: 060 ----
mean loss: 181.69
 ---- batch: 070 ----
mean loss: 170.30
 ---- batch: 080 ----
mean loss: 168.96
 ---- batch: 090 ----
mean loss: 164.38
 ---- batch: 100 ----
mean loss: 167.04
 ---- batch: 110 ----
mean loss: 165.59
train mean loss: 170.06
epoch train time: 0:00:15.709833
elapsed time: 1:02:28.431837
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 10:28:24.547063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.72
 ---- batch: 020 ----
mean loss: 181.47
 ---- batch: 030 ----
mean loss: 167.58
 ---- batch: 040 ----
mean loss: 167.87
 ---- batch: 050 ----
mean loss: 171.42
 ---- batch: 060 ----
mean loss: 174.97
 ---- batch: 070 ----
mean loss: 163.67
 ---- batch: 080 ----
mean loss: 165.53
 ---- batch: 090 ----
mean loss: 173.64
 ---- batch: 100 ----
mean loss: 168.47
 ---- batch: 110 ----
mean loss: 164.43
train mean loss: 170.10
epoch train time: 0:00:15.704267
elapsed time: 1:02:44.137127
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 10:28:40.252297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.99
 ---- batch: 020 ----
mean loss: 174.08
 ---- batch: 030 ----
mean loss: 170.62
 ---- batch: 040 ----
mean loss: 175.51
 ---- batch: 050 ----
mean loss: 167.95
 ---- batch: 060 ----
mean loss: 172.97
 ---- batch: 070 ----
mean loss: 183.14
 ---- batch: 080 ----
mean loss: 160.50
 ---- batch: 090 ----
mean loss: 161.34
 ---- batch: 100 ----
mean loss: 176.06
 ---- batch: 110 ----
mean loss: 163.55
train mean loss: 170.11
epoch train time: 0:00:15.697917
elapsed time: 1:02:59.836019
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 10:28:55.951258
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.84
 ---- batch: 020 ----
mean loss: 175.07
 ---- batch: 030 ----
mean loss: 173.44
 ---- batch: 040 ----
mean loss: 168.86
 ---- batch: 050 ----
mean loss: 167.36
 ---- batch: 060 ----
mean loss: 180.85
 ---- batch: 070 ----
mean loss: 175.09
 ---- batch: 080 ----
mean loss: 176.89
 ---- batch: 090 ----
mean loss: 164.02
 ---- batch: 100 ----
mean loss: 166.87
 ---- batch: 110 ----
mean loss: 159.23
train mean loss: 170.25
epoch train time: 0:00:15.719229
elapsed time: 1:03:15.556284
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 10:29:11.671464
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.95
 ---- batch: 020 ----
mean loss: 164.46
 ---- batch: 030 ----
mean loss: 172.05
 ---- batch: 040 ----
mean loss: 163.57
 ---- batch: 050 ----
mean loss: 169.05
 ---- batch: 060 ----
mean loss: 171.44
 ---- batch: 070 ----
mean loss: 177.08
 ---- batch: 080 ----
mean loss: 168.57
 ---- batch: 090 ----
mean loss: 182.64
 ---- batch: 100 ----
mean loss: 166.03
 ---- batch: 110 ----
mean loss: 171.80
train mean loss: 170.38
epoch train time: 0:00:15.714387
elapsed time: 1:03:31.271687
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 10:29:27.386852
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.75
 ---- batch: 020 ----
mean loss: 166.76
 ---- batch: 030 ----
mean loss: 170.36
 ---- batch: 040 ----
mean loss: 168.35
 ---- batch: 050 ----
mean loss: 176.45
 ---- batch: 060 ----
mean loss: 178.43
 ---- batch: 070 ----
mean loss: 173.37
 ---- batch: 080 ----
mean loss: 166.32
 ---- batch: 090 ----
mean loss: 167.65
 ---- batch: 100 ----
mean loss: 177.81
 ---- batch: 110 ----
mean loss: 163.15
train mean loss: 170.06
epoch train time: 0:00:15.813380
elapsed time: 1:03:47.086111
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 10:29:43.201349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.08
 ---- batch: 020 ----
mean loss: 181.02
 ---- batch: 030 ----
mean loss: 179.80
 ---- batch: 040 ----
mean loss: 164.07
 ---- batch: 050 ----
mean loss: 163.42
 ---- batch: 060 ----
mean loss: 170.10
 ---- batch: 070 ----
mean loss: 171.70
 ---- batch: 080 ----
mean loss: 155.08
 ---- batch: 090 ----
mean loss: 166.57
 ---- batch: 100 ----
mean loss: 175.67
 ---- batch: 110 ----
mean loss: 172.98
train mean loss: 170.13
epoch train time: 0:00:15.761325
elapsed time: 1:04:02.848449
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 10:29:58.963622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.01
 ---- batch: 020 ----
mean loss: 179.03
 ---- batch: 030 ----
mean loss: 164.05
 ---- batch: 040 ----
mean loss: 182.24
 ---- batch: 050 ----
mean loss: 175.48
 ---- batch: 060 ----
mean loss: 165.89
 ---- batch: 070 ----
mean loss: 175.43
 ---- batch: 080 ----
mean loss: 163.25
 ---- batch: 090 ----
mean loss: 162.06
 ---- batch: 100 ----
mean loss: 175.19
 ---- batch: 110 ----
mean loss: 166.19
train mean loss: 170.07
epoch train time: 0:00:15.729157
elapsed time: 1:04:18.578606
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 10:30:14.693837
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.68
 ---- batch: 020 ----
mean loss: 168.83
 ---- batch: 030 ----
mean loss: 167.21
 ---- batch: 040 ----
mean loss: 167.79
 ---- batch: 050 ----
mean loss: 169.89
 ---- batch: 060 ----
mean loss: 168.15
 ---- batch: 070 ----
mean loss: 176.74
 ---- batch: 080 ----
mean loss: 175.97
 ---- batch: 090 ----
mean loss: 165.99
 ---- batch: 100 ----
mean loss: 173.59
 ---- batch: 110 ----
mean loss: 175.17
train mean loss: 170.03
epoch train time: 0:00:16.003074
elapsed time: 1:04:34.582692
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 10:30:30.697885
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.39
 ---- batch: 020 ----
mean loss: 167.22
 ---- batch: 030 ----
mean loss: 175.62
 ---- batch: 040 ----
mean loss: 171.22
 ---- batch: 050 ----
mean loss: 164.84
 ---- batch: 060 ----
mean loss: 174.85
 ---- batch: 070 ----
mean loss: 169.52
 ---- batch: 080 ----
mean loss: 162.10
 ---- batch: 090 ----
mean loss: 167.84
 ---- batch: 100 ----
mean loss: 173.02
 ---- batch: 110 ----
mean loss: 175.54
train mean loss: 170.12
epoch train time: 0:00:15.771681
elapsed time: 1:04:50.355408
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 10:30:46.470579
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.53
 ---- batch: 020 ----
mean loss: 173.40
 ---- batch: 030 ----
mean loss: 170.53
 ---- batch: 040 ----
mean loss: 168.50
 ---- batch: 050 ----
mean loss: 168.91
 ---- batch: 060 ----
mean loss: 167.53
 ---- batch: 070 ----
mean loss: 171.28
 ---- batch: 080 ----
mean loss: 168.49
 ---- batch: 090 ----
mean loss: 168.23
 ---- batch: 100 ----
mean loss: 168.95
 ---- batch: 110 ----
mean loss: 170.09
train mean loss: 170.45
epoch train time: 0:00:15.953301
elapsed time: 1:05:06.309701
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 10:31:02.424882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 161.58
 ---- batch: 020 ----
mean loss: 172.50
 ---- batch: 030 ----
mean loss: 172.07
 ---- batch: 040 ----
mean loss: 167.16
 ---- batch: 050 ----
mean loss: 167.48
 ---- batch: 060 ----
mean loss: 171.83
 ---- batch: 070 ----
mean loss: 171.01
 ---- batch: 080 ----
mean loss: 164.64
 ---- batch: 090 ----
mean loss: 169.37
 ---- batch: 100 ----
mean loss: 176.00
 ---- batch: 110 ----
mean loss: 179.34
train mean loss: 169.94
epoch train time: 0:00:15.957622
elapsed time: 1:05:22.268322
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 10:31:18.383564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.58
 ---- batch: 020 ----
mean loss: 168.39
 ---- batch: 030 ----
mean loss: 165.10
 ---- batch: 040 ----
mean loss: 169.66
 ---- batch: 050 ----
mean loss: 170.27
 ---- batch: 060 ----
mean loss: 166.58
 ---- batch: 070 ----
mean loss: 166.02
 ---- batch: 080 ----
mean loss: 170.08
 ---- batch: 090 ----
mean loss: 175.43
 ---- batch: 100 ----
mean loss: 179.10
 ---- batch: 110 ----
mean loss: 171.45
train mean loss: 170.07
epoch train time: 0:00:15.922280
elapsed time: 1:05:38.191624
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 10:31:34.306814
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.28
 ---- batch: 020 ----
mean loss: 170.31
 ---- batch: 030 ----
mean loss: 163.18
 ---- batch: 040 ----
mean loss: 171.75
 ---- batch: 050 ----
mean loss: 165.09
 ---- batch: 060 ----
mean loss: 173.83
 ---- batch: 070 ----
mean loss: 168.24
 ---- batch: 080 ----
mean loss: 173.40
 ---- batch: 090 ----
mean loss: 172.68
 ---- batch: 100 ----
mean loss: 173.97
 ---- batch: 110 ----
mean loss: 171.61
train mean loss: 169.95
epoch train time: 0:00:16.029862
elapsed time: 1:05:54.232179
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_9/checkpoint.pth.tar
**** end time: 2019-09-26 10:31:50.346866 ****
